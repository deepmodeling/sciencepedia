{"hands_on_practices": [{"introduction": "The integrity of train, validation, and test splits is paramount for trustworthy model evaluation. A common threat to this integrity is the presence of near-duplicates, such as slightly re-compressed images or paraphrased sentences, across different splits. This practice [@problem_id:3194863] models the detection of such pairs using perceptual hashing, where the Hamming distance between hashes serves as a similarity metric. By working through this problem, you will learn to quantify the fundamental trade-off between missing true duplicates (false negatives) and incorrectly flagging distinct items (false positives), providing a rigorous framework for making informed decisions about dataset hygiene.", "problem": "You are tasked with formalizing near-duplicate detection for split hygiene in a dataset used for deep learning. Consider that each example (for instance, an image) is summarized by a fixed-length perceptual hash of $L$ bits. For any pair of examples across different splits (training, validation, test), define the Hamming distance $D$ as the number of bit positions where the two hashes differ. A duplicate detector declares a pair as a duplicate if and only if $D \\leq T$, where $T$ is an integer threshold.\n\nAssume the following well-tested stochastic model. For two items that are truly the same underlying content (true duplicate pair) processed independently by the hashing pipeline, each bit is flipped between the two hashes independently with probability $p_d$, so that the Hamming distance $D$ for a true duplicate pair follows the binomial distribution with parameters $L$ and $p_d$. For two items that are truly different content (non-duplicate pair), each bit differs independently with probability $p_n$, so that the Hamming distance $D$ for a non-duplicate pair follows the binomial distribution with parameters $L$ and $p_n$. These independence and binomial assumptions are standard for modeling perceptual hashing noise and collision behavior.\n\nYour goals are:\n- From the fundamental definitions above, derive expressions for the false positive rate and false negative rate as functions of $L$, $T$, $p_d$, and $p_n$.\n- Use these expressions to compute the expected numbers of false positives and false negatives when the detector is evaluated on a specified number of cross-split pairs with known ground truth.\n\nDefinitions to use:\n- False positive rate is the probability that a non-duplicate pair is incorrectly declared a duplicate. Using the binomial model, this is the probability that $D \\leq T$ conditioned on the non-duplicate distribution with parameters $L$ and $p_n$.\n- False negative rate is the probability that a true duplicate pair is incorrectly declared a non-duplicate. Using the binomial model, this is the probability that $D > T$ conditioned on the duplicate distribution with parameters $L$ and $p_d$.\n- If there are $N_{\\text{non}}$ non-duplicate cross-split pairs and $N_{\\text{dup}}$ duplicate cross-split pairs, then the expected false positive count is the false positive rate multiplied by $N_{\\text{non}}$, and the expected false negative count is the false negative rate multiplied by $N_{\\text{dup}}$.\n\nYour program must implement these computations using exact binomial cumulative distribution values, not simulation, for the following test suite. Each test case specifies $(L, p_d, p_n, T, N_{\\text{dup}}, N_{\\text{non}})$:\n\n- Case A (typical setting): $L = 64$, $p_d = 0.06$, $p_n = 0.50$, $T = 10$, $N_{\\text{dup}} = 1000$, $N_{\\text{non}} = 200000$.\n- Case B (extremely strict threshold): $L = 64$, $p_d = 0.06$, $p_n = 0.50$, $T = 0$, $N_{\\text{dup}} = 1000$, $N_{\\text{non}} = 200000$.\n- Case C (extremely loose threshold): $L = 64$, $p_d = 0.06$, $p_n = 0.50$, $T = 64$, $N_{\\text{dup}} = 1000$, $N_{\\text{non}} = 200000$.\n- Case D (overlapping distributions): $L = 32$, $p_d = 0.22$, $p_n = 0.35$, $T = 12$, $N_{\\text{dup}} = 10000$, $N_{\\text{non}} = 50000$.\n- Case E (short hash): $L = 8$, $p_d = 0.10$, $p_n = 0.30$, $T = 2$, $N_{\\text{dup}} = 100$, $N_{\\text{non}} = 1000$.\n\nQuantified outputs:\n- For each test case, compute four quantities: the false positive rate as a decimal in $[0,1]$, the false negative rate as a decimal in $[0,1]$, the expected false positive count as a real number, and the expected false negative count as a real number. All four values must be rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[ \\text{FPR}, \\text{FNR}, \\text{EFP}, \\text{EFN} ]$. For example, a valid output looks like $[[0.001000,0.100000,123.456000,78.900000],[\\dots]]$ with no spaces. Do not print anything else.", "solution": "The problem requires the derivation and computation of performance metrics for a duplicate detector based on perceptual hash comparison. The core of the problem lies in the application of the binomial distribution to model the Hamming distance between hashes. We will proceed by first formalizing the random variables, then deriving the expressions for the false positive and false negative rates, and finally outlining the computation of the expected counts.\n\nLet the length of the perceptual hash be $L$. For any pair of hashes, the Hamming distance $D$ is the number of bit positions that differ. The detection rule is that a pair is flagged as a duplicate if and only if $D \\leq T$ for a given integer threshold $T$.\n\nThe problem defines two distinct stochastic models for the Hamming distance, contingent on whether a pair of items represents truly the same content (a true duplicate) or different content (a non-duplicate).\n\nLet $X_{nd}$ be the random variable for the Hamming distance $D$ between two hashes of non-duplicate items. The problem states that $X_{nd}$ follows a binomial distribution with parameters $L$ and $p_n$. We denote this as:\n$$\nX_{nd} \\sim B(L, p_n)\n$$\nThe probability mass function (PMF) is given by $P(X_{nd} = k) = \\binom{L}{k} p_n^k (1-p_n)^{L-k}$ for $k \\in \\{0, 1, \\dots, L\\}$.\n\nLet $X_{d}$ be the random variable for the Hamming distance $D$ between two hashes of a true duplicate pair. The problem states that $X_{d}$ follows a binomial distribution with parameters $L$ and $p_d$. We denote this as:\n$$\nX_{d} \\sim B(L, p_d)\n$$\nThe PMF is given by $P(X_{d} = k) = \\binom{L}{k} p_d^k (1-p_d)^{L-k}$ for $k \\in \\{0, 1, \\dots, L\\}$.\n\nWith these definitions, we can derive the required metrics.\n\n**False Positive Rate (FPR)**\nA false positive occurs when a non-duplicate pair is incorrectly classified as a duplicate. This happens when the Hamming distance for a non-duplicate pair, $X_{nd}$, satisfies the condition $X_{nd} \\leq T$.\nThe false positive rate is the probability of this event:\n$$\n\\text{FPR} = P(X_{nd} \\leq T)\n$$\nThis probability corresponds to the cumulative distribution function (CDF) of the binomial distribution $B(L, p_n)$ evaluated at the threshold $T$. Mathematically, it is the sum of probabilities for all outcomes from $0$ to $T$:\n$$\n\\text{FPR} = \\sum_{k=0}^{T} P(X_{nd} = k) = \\sum_{k=0}^{T} \\binom{L}{k} p_n^k (1-p_n)^{L-k}\n$$\n\n**False Negative Rate (FNR)**\nA false negative occurs when a true duplicate pair is incorrectly classified as a non-duplicate. This happens when the Hamming distance for a true duplicate pair, $X_{d}$, fails the detection condition, i.e., $X_{d} > T$.\nThe false negative rate is the probability of this event:\n$$\n\\text{FNR} = P(X_{d} > T)\n$$\nThis probability is the survival function (or complementary CDF) of the binomial distribution $B(L, p_d)$ evaluated at $T$. It can be calculated by summing the probabilities of all outcomes greater than $T$:\n$$\n\\text{FNR} = \\sum_{k=T+1}^{L} P(X_{d} = k) = \\sum_{k=T+1}^{L} \\binom{L}{k} p_d^k (1-p_d)^{L-k}\n$$\nAlternatively, it can be computed as $1$ minus the CDF:\n$$\n\\text{FNR} = 1 - P(X_{d} \\leq T) = 1 - \\sum_{k=0}^{T} \\binom{L}{k} p_d^k (1-p_d)^{L-k}\n$$\n\n**Expected Counts**\nGiven a set of $N_{\\text{non}}$ non-duplicate pairs and $N_{\\text{dup}}$ true duplicate pairs, the expected number of misclassifications can be calculated.\nThe expected number of false positives (EFP) is the product of the number of non-duplicate pairs and the probability of any single one being a false positive:\n$$\nE[\\text{FP}] = N_{\\text{non}} \\times \\text{FPR}\n$$\nSimilarly, the expected number of false negatives (EFN) is the product of the number of true duplicate pairs and the probability of any single one being a false negative:\n$$\nE[\\text{FN}] = N_{\\text{dup}} \\times \\text{FNR}\n$$\n\nThe computational procedure for each test case $(L, p_d, p_n, T, N_{\\text{dup}}, N_{\\text{non}})$ is as follows:\n1.  Compute $\\text{FPR} = \\sum_{k=0}^{T} \\binom{L}{k} p_n^k (1-p_n)^{L-k}$ using the CDF of $B(L, p_n)$.\n2.  Compute $\\text{FNR} = 1 - \\sum_{k=0}^{T} \\binom{L}{k} p_d^k (1-p_d)^{L-k}$ using the CDF of $B(L, p_d)$, or directly using the survival function.\n3.  Compute $E[\\text{FP}] = N_{\\text{non}} \\times \\text{FPR}$.\n4.  Compute $E[\\text{FN}] = N_{\\text{dup}} \\times \\text{FNR}$.\n5.  Round all four results to six decimal places.\n\nThese calculations are performed for each of the five specified test cases. The implementation will utilize numerical libraries that provide exact functions for the binomial CDF and survival function to avoid manual summation and ensure numerical stability.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Computes false positive/negative rates and expected counts for duplicate detection\n    based on a binomial model of Hamming distances.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (L, p_d, p_n, T, N_dup, N_non)\n    test_cases = [\n        (64, 0.06, 0.50, 10, 1000, 200000),  # Case A\n        (64, 0.06, 0.50, 0, 1000, 200000),   # Case B\n        (64, 0.06, 0.50, 64, 1000, 200000),  # Case C\n        (32, 0.22, 0.35, 12, 10000, 50000),  # Case D\n        (8, 0.10, 0.30, 2, 100, 1000),       # Case E\n    ]\n\n    # results will store the string representation of each sub-list, e.g., \"[0.1,0.2,...]\"\n    # This is done to achieve the exact output format with no spaces.\n    results = []\n    for case in test_cases:\n        L, p_d, p_n, T, N_dup, N_non = case\n\n        # False Positive Rate (FPR): P(D = T) for non-duplicates (D ~ B(L, p_n))\n        fpr = binom.cdf(T, L, p_n)\n\n        # False Negative Rate (FNR): P(D > T) for true duplicates (D ~ B(L, p_d))\n        # scipy.stats.binom.sf (survival function) computes P(X > k) directly.\n        fnr = binom.sf(T, L, p_d)\n        \n        # Expected False Positives (EFP)\n        efp = fpr * N_non\n\n        # Expected False Negatives (EFN)\n        efn = fnr * N_dup\n        \n        # Create the string for the inner list with specific formatting and no spaces,\n        # ensuring each number is rounded to six decimal places.\n        result_str = (\n            f\"[{f'{fpr:.6f}'},\"\n            f\"{f'{fnr:.6f}'},\"\n            f\"{f'{efp:.6f}'},\"\n            f\"{f'{efn:.6f}'}]\"\n        )\n        results.append(result_str)\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3194863"}, {"introduction": "In the real world, training deep learning models is always constrained by a finite computational budget, forcing a critical trade-off: should you train for more steps, or should you perform validation checks more frequently to pinpoint the best model? This exercise [@problem_id:3194810] frames this dilemma as an optimization problem, using an intuitive model of a learning curve and the associated costs of training and validation. By deriving the optimal validation frequency, you will develop a quantitative understanding of the relationship between overfitting, early stopping, and resource allocation, moving from a qualitative sense to a strategic, budget-aware training methodology.", "problem": "A team trains a deep neural network with Stochastic Gradient Descent (SGD) on a fixed training dataset and uses a separate validation dataset to select the final checkpoint by early stopping. Let the expected test risk at training step $s$ be modeled by a learning curve that strictly decreases before an optimal stopping point $S^{\\star}$ and strictly increases after it due to overfitting. Concretely, assume there exists a minimal expected test risk $R_{\\min}$ attained at $s=S^{\\star}$, and that for $sS^{\\star}$ the expected test risk increases approximately linearly with slope $\\lambda$ per additional training step, while for $sS^{\\star}$ it decreases monotonically sufficiently fast that the minimum is unique at $S^{\\star}$.\n\nValidation is performed every $k$ training steps. The final model is chosen as the checkpoint among the validated ones with the lowest validation loss. Because $S^{\\star}$ is generally not a multiple of $k$, the nearest validated checkpoint has, in expectation, a mismatch of $k/2$ steps from $S^{\\star}$, incurring an expected excess test risk of approximately $\\lambda \\cdot (k/2)$ relative to $R_{\\min}$.\n\nCompute is measured in training-step equivalents. Each training step costs $c_{t}$ units, and each full validation pass over the validation dataset costs $c_{v}$ units. With a fixed compute budget $B$, training for $N$ steps and validating every $k$ steps incurs total cost $c_{t}N + c_{v}\\cdot(N/k)$, which must not exceed $B$. To ensure availability of a checkpoint near the optimal stopping point, require $N \\geq S^{\\star}$.\n\nUnder this model, derive the optimal validation interval $k^{\\star}$ (in steps) that minimizes the expected test risk subject to the compute budget constraint and the requirement $N \\geq S^{\\star}$. Then, using the following parameters, compute $k^{\\star}$:\n- $S^{\\star} = 6.0 \\times 10^{5}$ steps,\n- $B = 8.0 \\times 10^{5}$ step-equivalents,\n- $c_{t} = 1$ step-equivalent per training step,\n- $c_{v} = 1.5224 \\times 10^{3}$ step-equivalents per validation,\n- $\\lambda = 1.0 \\times 10^{-6}$ risk units per step.\n\nRound your final answer for $k^{\\star}$ to four significant figures. Express your answer as a number of steps.", "solution": "The problem requires the derivation of the optimal validation interval, $k^{\\star}$, that minimizes the expected test risk under a fixed computational budget. The solution process begins with a formal validation of the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe givens are extracted verbatim from the problem statement:\n-   The expected test risk has a unique minimum, $R_{\\min}$, at training step $s=S^{\\star}$.\n-   For $s  S^{\\star}$, the expected test risk increases approximately linearly with slope $\\lambda$.\n-   Validation is performed every $k$ training steps.\n-   The expected mismatch between $S^{\\star}$ and the nearest validated checkpoint is $k/2$.\n-   The expected excess test risk is approximately $\\lambda \\cdot (k/2)$.\n-   The cost of one training step is $c_{t}$.\n-   The cost of one full validation pass is $c_{v}$.\n-   The total number of training steps is $N$.\n-   The total compute budget is $B$.\n-   The total cost is $c_{t}N + c_{v}\\cdot(N/k)$.\n-   Budget constraint: $c_{t}N + c_{v}\\cdot(N/k) \\leq B$.\n-   Training completion constraint: $N \\geq S^{\\star}$.\n-   Parameters:\n    -   $S^{\\star} = 6.0 \\times 10^{5}$ steps\n    -   $B = 8.0 \\times 10^{5}$ step-equivalents\n    -   $c_{t} = 1$ step-equivalent per training step\n    -   $c_{v} = 1.5224 \\times 10^{3}$ step-equivalents per validation\n    -   $\\lambda = 1.0 \\times 10^{-6}$ risk units per step\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the specified criteria:\n-   **Scientifically Grounded**: The problem is based on established concepts in machine learning, including early stopping, overfitting, and computational budget management. The model for the learning curve (a U-shape) and the linear approximation of risk increase post-optimum are standard simplifying assumptions. The cost model for training and validation is also a standard formulation. The explicit premise of an expected step mismatch of $k/2$ is a specific modeling choice provided in the problem statement; it does not violate any fundamental principles and defines the context in which the optimization must be performed.\n-   **Well-Posed**: The problem is a well-posed optimization task. It requires minimizing a clearly defined objective function (proportional to $k$) subject to a set of algebraic inequality constraints. All necessary parameters are provided to find a unique solution.\n-   **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n\nThe problem is found to be free of scientific unsoundness, ambiguity, and internal contradictions. It presents a solvable, self-contained optimization problem.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A complete solution will be provided.\n\n### Solution Derivation\n\nThe objective is to minimize the expected excess test risk, which is given as a function of the validation interval $k$:\n$$\nR_{\\text{excess}}(k) = \\frac{\\lambda k}{2}\n$$\nSince the slope $\\lambda$ is a positive constant, minimizing $R_{\\text{excess}}(k)$ is equivalent to minimizing the validation interval $k$. Our goal is to find the smallest possible value of $k$ that satisfies all constraints.\n\nThe constraints are:\n1.  The total computational cost must not exceed the budget $B$. The cost for training for $N$ steps and validating $\\frac{N}{k}$ times is:\n    $$\n    C(N, k) = c_{t}N + c_{v}\\frac{N}{k}\n    $$\n    Thus, the budget constraint is:\n    $$\n    N \\left(c_{t} + \\frac{c_{v}}{k}\\right) \\leq B\n    $$\n2.  The total number of training steps $N$ must be at least the optimal stopping point $S^{\\star}$:\n    $$\n    N \\geq S^{\\star}\n    $$\n\nFrom the budget constraint, we can express an upper bound on $N$:\n$$\nN \\leq \\frac{B}{c_{t} + \\frac{c_{v}}{k}}\n$$\nCombining this with the training duration constraint, we have:\n$$\nS^{\\star} \\leq N \\leq \\frac{B}{c_{t} + \\frac{c_{v}}{k}}\n$$\nFor a feasible solution for $N$ to exist for a given $k$, the lower bound on $N$ must be less than or equal to the upper bound:\n$$\nS^{\\star} \\leq \\frac{B}{c_{t} + \\frac{c_{v}}{k}}\n$$\nThis inequality establishes a relationship between the validation interval $k$ and the fixed parameters of the problem. We must rearrange this inequality to find the constraint on $k$.\n$$\nS^{\\star} \\left(c_{t} + \\frac{c_{v}}{k}\\right) \\leq B\n$$\n$$\nS^{\\star}c_{t} + \\frac{S^{\\star}c_{v}}{k} \\leq B\n$$\n$$\n\\frac{S^{\\star}c_{v}}{k} \\leq B - S^{\\star}c_{t}\n$$\nBefore proceeding, we verify that the term $B - S^{\\star}c_{t}$ is positive. Using the given values:\n$$\nB - S^{\\star}c_{t} = (8.0 \\times 10^{5}) - (6.0 \\times 10^{5})(1) = 2.0 \\times 10^{5} > 0\n$$\nSince the right-hand side is positive, we can safely invert the inequality (noting that $k > 0$):\n$$\nk \\geq \\frac{S^{\\star}c_{v}}{B - S^{\\star}c_{t}}\n$$\nThis inequality provides a lower bound for the validation interval $k$. Since our objective is to minimize $k$, the optimal value $k^{\\star}$ will be this minimum possible value:\n$$\nk^{\\star} = \\frac{S^{\\star}c_{v}}{B - S^{\\star}c_{t}}\n$$\nThis optimal value $k^{\\star}$ corresponds to the strategy of training for the minimum required duration, $N=S^{\\star}$, and using the entire remaining budget for validation.\n\nNow we substitute the given numerical values to compute $k^{\\star}$:\n-   $S^{\\star} = 6.0 \\times 10^{5}$\n-   $B = 8.0 \\times 10^{5}$\n-   $c_{t} = 1$\n-   $c_{v} = 1.5224 \\times 10^{3}$\n\n$$\nk^{\\star} = \\frac{(6.0 \\times 10^{5}) \\cdot (1.5224 \\times 10^{3})}{8.0 \\times 10^{5} - (1) \\cdot (6.0 \\times 10^{5})}\n$$\n$$\nk^{\\star} = \\frac{9.1344 \\times 10^{8}}{2.0 \\times 10^{5}}\n$$\n$$\nk^{\\star} = 4.5672 \\times 10^{3} = 4567.2\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures of $4567.2$ are $4$, $5$, $6$, and $7$. The fifth digit is $2$, which is less than $5$, so we round down.\n$$\nk^{\\star} \\approx 4567\n$$\nThe optimal validation interval is $4567$ steps.", "answer": "$$\n\\boxed{4567}\n$$", "id": "3194810"}, {"introduction": "Data leakage can occur in subtle ways that go beyond simple duplicate examples; the preprocessing pipeline itself can be a hidden source of contamination. This practice [@problem_id:3194860] explores this issue within the context of Natural Language Processing (NLP), demonstrating how learning a subword vocabulary on a corpus that includes the test set can lead to information leakage. You will see how this allows the tokenizer to \"memorize\" rare test-set phrases, which can artificially inflate performance metrics and invalidate your results, highlighting the critical need to freeze all data-dependent transformations based on the training set alone.", "problem": "You are asked to demonstrate, by computation, how a data-dependent subword vocabulary can cause information leakage from a test set into a model when trained naively, and how freezing the vocabulary mitigates this effect. Work in the setting of Natural Language Processing (NLP), where text is tokenized into subword units learned by an unsupervised algorithm such as Byte-Pair Encoding (BPE). You will implement a simplified BPE procedure and compute a leakage-sensitive metric across specified test cases.\n\nFundamental base and assumptions:\n- Tokenization is regarded as a deterministic preprocessing function $ \\tau $ induced by a fitted subword vocabulary. A data split into training, validation, and test is valid when the test set is not used to fit any component of the pipeline that influences the learned model, including $ \\tau $.\n- Data leakage is any flow of information from the test set into the training pipeline. A tokenizer fitted on the entire dataset, including the test portion, violates the independence assumption between training and test and can reduce the apparent complexity (e.g., token length) of rare phrases in the test set.\n- Byte-Pair Encoding (BPE) is an unsupervised procedure that starts from a character-level tokenization and repeatedly merges the most frequent adjacent token pair across a corpus. After $ M $ merges, the learned merge list determines how any input string is segmented into subwords by repeatedly applying the learned merges in order.\n\nYour task:\n1) Implement a simplified BPE as follows, with no end-of-word symbol and no continuation markers:\n   - Start with words segmented into characters. For a corpus represented as a multiset of words with integer frequencies, compute the frequency of each adjacent pair of symbols, weighted by word frequency.\n   - At each merge step, select the globally most frequent adjacent pair and merge it into a single symbol for all words. Break ties deterministically by choosing the lexicographically smallest pair as a tuple of strings. Repeat for $ M $ steps or until no pairs remain.\n   - To encode a word using the learned merges, start from its character sequence and apply the merges in the learned order, greedily replacing occurrences of the current merge pair until no more occurrences remain, then proceed to the next merge.\n\n2) Two regimes:\n   - Leaky regime: Fit the BPE merges on the combined multiset of words from both the training and the test sentences.\n   - Frozen regime: Fit the BPE merges using only the training sentences, and then keep the merge list fixed (i.e., do not refit or extend it with any information from the test sentences).\n\n3) For each test case below, compute for a set of rare target phrases the following metrics:\n   - Let $ T_{\\text{leaky}} $ be the average number of tokens per target phrase under the leaky regime, and let $ T_{\\text{frozen}} $ be the average number of tokens per target phrase under the frozen regime. Define the leakage gain\n     $$ g \\;=\\; \\frac{T_{\\text{frozen}} - T_{\\text{leaky}}}{T_{\\text{frozen}}}. $$\n     Report $ g $ as a decimal rounded to $ 3 $ decimals.\n   - Define indicator variables $ I_{\\text{leaky}} $ and $ I_{\\text{frozen}} $ which equal $ 1 $ if at least one target phrase encodes as a single token under the respective regime, and equal $ 0 $ otherwise.\n\n4) Output format:\n   - Your program should produce a single line of output containing a list of results, one per test case, where each result is itself a list $ [g, I_{\\text{leaky}}, I_{\\text{frozen}}] $ with $ g $ rounded to $ 3 $ decimals and the indicators as integers $ 0 $ or $ 1 $.\n   - The final output must be a single line of the form\n     $$ [ [g_1,I_{\\text{leaky},1},I_{\\text{frozen},1}], [g_2,I_{\\text{leaky},2},I_{\\text{frozen},2}], \\ldots ] $$\n     with no extra text.\n\nTest suite:\nConstruct corpora from sentence templates and repetition counts. Each sentence is split by spaces into words. For each case, the training corpus is the training sentence repeated $ r_{\\text{train}} $ times, and the test corpus is the test sentence repeated $ r_{\\text{test}} $ times.\n\n- Case A (happy path: leakage increases subword memorization of a rare test phrase):\n  - Training sentence: \"alpha beta gamma delta epsilon\"\n  - Test sentence: \"quantumflux\"\n  - Repetitions: $ r_{\\text{train}} = 5 $, $ r_{\\text{test}} = 20 $\n  - Target phrases: [\"quantumflux\"]\n  - Number of merges: $ M = 10 $\n\n- Case B (boundary: zero merges eliminates leakage effect):\n  - Training sentence: \"abc def\"\n  - Test sentence: \"ghij\"\n  - Repetitions: $ r_{\\text{train}} = 5 $, $ r_{\\text{test}} = 10 $\n  - Target phrases: [\"ghij\"]\n  - Number of merges: $ M = 0 $\n\n- Case C (edge: severe leakage can collapse a rare test phrase to a single token):\n  - Training sentence: \"alpha beta gamma\"\n  - Test sentence: \"quantumflux\"\n  - Repetitions: $ r_{\\text{train}} = 5 $, $ r_{\\text{test}} = 100 $\n  - Target phrases: [\"quantumflux\"]\n  - Number of merges: $ M = 12 $\n\nAnswer specification:\n- All outputs are dimensionless and must be reported as decimals or integers. Do not use percentage signs.\n- Your program should produce exactly one line containing a single list whose elements are the triplets $ [g,I_{\\text{leaky}},I_{\\text{frozen}}] $ for the cases A, B, and C, in that order, with $ g $ rounded to $ 3 $ decimals and no spaces in the printed list.", "solution": "The user's request is to demonstrate a form of data leakage in Natural Language Processing (NLP) pipelines, where a tokenizer fitted on a combination of training and test data can artificially improve performance metrics on the test set. This phenomenon will be illustrated by implementing a simplified Byte-Pair Encoding (BPE) algorithm and comparing its behavior in two distinct regimes: a \"leaky\" regime where the BPE vocabulary is learned from the combined training and test corpora, and a \"frozen\" regime where it is learned only from the training corpus. The problem is scientifically valid, well-posed, and all terms and procedures are defined with sufficient rigor for a computational solution.\n\nThe solution proceeds by first implementing the necessary components as specified:\n1.  A function to construct a corpus, represented as a multiset (or dictionary) of words and their frequencies, from input sentences and repetition counts.\n2.  A BPE fitting function, `fit_bpe`, which learns a vocabulary of subword merges.\n3.  A BPE encoding function, `encode_word`, which tokenizes a word given a learned vocabulary.\n4.  A main routine to process each test case through the two regimes and compute the required metrics.\n\n**BPE Fitting Procedure (`fit_bpe`)**\n\nThe `fit_bpe` function takes a corpus (a dictionary mapping words to their frequencies) and a number of merges, $M$, as input.\n1.  **Initialization**: The corpus is represented internally by segmenting each word into its constituent characters. For example, a word \"alpha\" with frequency $5$ is initially represented as a sequence of tokens `('a', 'l', 'p', 'h', 'a')` associated with the frequency $5$.\n2.  **Iterative Merging**: The algorithm iterates $M$ times or until no more merges are possible. In each step:\n    a.  **Pair Frequency Calculation**: It computes the frequencies of all adjacent pairs of tokens across the entire corpus, weighted by the frequency of the words in which they appear.\n    b.  **Best Pair Selection**: It identifies the most frequent pair. Ties are broken by selecting the lexicographically smallest pair, where pairs are compared as tuples of strings (e.g., `('a', 'b')` precedes `('a', 'c')`).\n    c.  **Vocabulary Update**: The best pair, say $(s_1, s_2)$, is recorded as a new merge rule.\n    d.  **Corpus Update**: All occurrences of the adjacent sequence `$s_1 s_2$` in the tokenized corpus representation are replaced by a new single token `$s_1s_2$`. This updated representation is used for the next iteration.\n3.  **Output**: The function returns an ordered list of the $M$ learned merge rules.\n\n**BPE Encoding Procedure (`encode_word`)**\n\nThe `encode_word` function takes a word string and an ordered list of merge rules to produce a sequence of subword tokens.\n1.  **Initialization**: The input word is first split into a sequence of characters.\n2.  **Applying Merges**: The function iterates through the merge rules in the order they were learned. For each rule $(s_1, s_2)$:\n    a.  The algorithm repeatedly Scans the current token sequence for the first occurrence of the adjacent pair $(s_1, s_2)$.\n    b.  Upon finding an occurrence, it is replaced by the merged token $s_1s_2$. The scan is then restarted from the beginning of the modified token sequence.\n    c.  This process continues until no more occurrences of the pair $(s_1, s_2)$ can be found in the sequence.\n    d.  The algorithm then proceeds to the next merge rule in the list.\n3.  **Output**: The final token sequence for the word is returned. The length of this sequence is the number of tokens.\n\n**Metric Calculation**\n\nFor each test case, the analysis is performed for both the frozen and leaky regimes:\n-   **Frozen Regime**: The BPE merges are learned using `fit_bpe` on the training corpus only. These merges are then used to encode the target phrases, yielding an average token count per phrase, $T_{\\text{frozen}}$.\n-   **Leaky Regime**: The BPE merges are learned from a corpus combining both training and test data. These merges are used to encode the target phrases, yielding an average token count, $T_{\\text{leaky}}$.\n\nThe leakage gain $g$ is then calculated as $g = (T_{\\text{frozen}} - T_{\\text{leaky}}) / T_{\\text{frozen}}$. A positive value of $g$ indicates that including the test data during tokenizer fitting resulted in a more compressed (shorter) tokenization for the target phrases, a direct consequence of the information leakage. The indicator variables $I_{\\text{leaky}}$ and $I_{\\text{frozen}}$ are set to $1$ if any target phrase is tokenized into a single token in the respective regime, and $0$ otherwise. This captures cases of extreme leakage where a rare word is memorized as a single unit in the vocabulary.\n\nThe results for all test cases are then aggregated and formatted into the specified list of lists structure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport collections\n\ndef get_pair_freqs(word_segs: dict[str, int]) -> collections.Counter:\n    \"\"\"\n    Calculates the frequency of adjacent token pairs in a segmented corpus representation.\n    The corpus representation is a dictionary mapping a space-separated token string to its frequency.\n    \"\"\"\n    pair_freqs = collections.Counter()\n    for seg_word, freq in word_segs.items():\n        tokens = seg_word.split(' ')\n        if len(tokens)  2:\n            continue\n        for i in range(len(tokens) - 1):\n            pair = (tokens[i], tokens[i+1])\n            pair_freqs[pair] += freq\n    return pair_freqs\n\ndef fit_bpe(corpus: dict[str, int], M: int) -> list[tuple[str, str]]:\n    \"\"\"\n    Fits a simplified BPE model by learning M merge operations from a corpus.\n    \n    Args:\n        corpus: A dictionary mapping words to their frequencies.\n        M: The number of merges to perform.\n\n    Returns:\n        An ordered list of M merge rules, where each rule is a tuple of strings.\n    \"\"\"\n    if M == 0:\n        return []\n    \n    # Initialize the corpus representation with character-level tokenization.\n    word_segs = {' '.join(word): freq for word, freq in corpus.items() if word}\n    \n    merges = []\n    for _ in range(M):\n        pair_freqs = get_pair_freqs(word_segs)\n        if not pair_freqs:\n            break\n        \n        # Find the best pair: highest frequency, with lexicographical tie-breaking.\n        max_freq = max(pair_freqs.values())\n        # Filter pairs with max frequency and sort them to find the lexicographically smallest.\n        candidate_pairs = sorted([p for p, f in pair_freqs.items() if f == max_freq])\n        best_pair = candidate_pairs[0]\n        \n        merges.append(best_pair)\n        \n        # Apply the merge to the entire corpus representation for the next iteration.\n        p1, p2 = best_pair\n        merged_token = p1 + p2\n        \n        new_word_segs = {}\n        for seg_word, freq in word_segs.items():\n            # Using string replacement on the space-separated representation.\n            new_seg_word = seg_word.replace(f'{p1} {p2}', merged_token)\n            new_word_segs[new_seg_word] = freq\n        word_segs = new_word_segs\n        \n    return merges\n\ndef encode_word(word: str, merges: list[tuple[str, str]]) -> list[str]:\n    \"\"\"\n    Encodes a word into subword tokens using a pre-computed list of BPE merges.\n    \n    Args:\n        word: The string to encode.\n        merges: An ordered list of BPE merge rules.\n\n    Returns:\n        A list of subword tokens.\n    \"\"\"\n    if not word:\n        return []\n        \n    tokens = list(word)\n    \n    for p1, p2 in merges:\n        merged_token = p1 + p2\n        while True:\n            # Repeatedly find the first occurrence of the pair and merge it.\n            # The scan restarts after each merge until no more merges for this rule are possible.\n            first_occurrence_idx = -1\n            for i in range(len(tokens) - 1):\n                if tokens[i] == p1 and tokens[i+1] == p2:\n                    first_occurrence_idx = i\n                    break\n            \n            if first_occurrence_idx != -1:\n                i = first_occurrence_idx\n                tokens = tokens[:i] + [merged_token] + tokens[i+2:]\n            else:\n                break # No more occurrences of this pair, move to the next merge rule.\n    return tokens\n\ndef process_case(train_sent: str, test_sent: str, r_train: int, r_test: int, targets: list[str], M: int) -> list:\n    \"\"\"\n    Processes a single test case, calculating metrics for leaky and frozen regimes.\n    \"\"\"\n    # Create corpora from sentence templates and repetitions\n    train_corpus = collections.Counter(train_sent.split(' '))\n    for word in list(train_corpus): train_corpus[word] *= r_train\n    \n    test_corpus = collections.Counter(test_sent.split(' '))\n    for word in list(test_corpus): test_corpus[word] *= r_test\n\n    # --- Frozen Regime ---\n    # Fit BPE on the training data only\n    frozen_merges = fit_bpe(dict(train_corpus), M)\n    # Encode target phrases and calculate metrics\n    frozen_token_counts = [len(encode_word(phrase, frozen_merges)) for phrase in targets]\n    T_frozen = np.mean(frozen_token_counts)\n    I_frozen = 1 if any(c == 1 for c in frozen_token_counts) else 0\n\n    # --- Leaky Regime ---\n    # Fit BPE on the combined training and test data\n    combined_corpus = train_corpus + test_corpus\n    leaky_merges = fit_bpe(dict(combined_corpus), M)\n    # Encode target phrases and calculate metrics\n    leaky_token_counts = [len(encode_word(phrase, leaky_merges)) for phrase in targets]\n    T_leaky = np.mean(leaky_token_counts)\n    I_leaky = 1 if any(c == 1 for c in leaky_token_counts) else 0\n\n    # Calculate final leakage gain metric\n    if T_frozen == 0:\n        g = 0.0 # Avoid division by zero\n    else:\n        g = (T_frozen - T_leaky) / T_frozen\n\n    return [g, I_leaky, I_frozen]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"train_sent\": \"alpha beta gamma delta epsilon\", \"test_sent\": \"quantumflux\",\n            \"r_train\": 5, \"r_test\": 20, \"targets\": [\"quantumflux\"], \"M\": 10\n        },\n        {\n            \"train_sent\": \"abc def\", \"test_sent\": \"ghij\",\n            \"r_train\": 5, \"r_test\": 10, \"targets\": [\"ghij\"], \"M\": 0\n        },\n        {\n            \"train_sent\": \"alpha beta gamma\", \"test_sent\": \"quantumflux\",\n            \"r_train\": 5, \"r_test\": 100, \"targets\": [\"quantumflux\"], \"M\": 12\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        g, i_leaky, i_frozen = process_case(**case)\n        \n        # Format the result as a string '[g,I_leaky,I_frozen]' with no spaces.\n        # g is rounded to 3 decimal places implicitly by the format specifier.\n        result_str = f\"[{g:.3f},{i_leaky},{i_frozen}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3194860"}]}