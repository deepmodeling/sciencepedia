{"hands_on_practices": [{"introduction": "To build a deep understanding of a neuron's behavior, we must first analyze its core component: the net input function, $z=\\mathbf{w}^{\\top}\\mathbf{x}+b$. This exercise explores the fundamental scaling properties of this linear model, investigating how a neuron's decision boundary is affected when inputs and weights are scaled. By deriving the conditions for decision invariance [@problem_id:3199772], you will uncover the crucial role the bias term $b$ plays in breaking simple symmetries and stabilizing the model's output.", "problem": "Consider a single linear unit (a neuron) in an Artificial Neural Network (ANN), whose net input is defined by the core linear model and bias, namely $z=\\mathbf{w}^{\\top}\\mathbf{x}+b$, where $\\mathbf{w}\\in\\mathbb{R}^{d}$ is the weight vector, $\\mathbf{x}\\in\\mathbb{R}^{d}$ is the input vector, and $b\\in\\mathbb{R}$ is the bias. The decision of the neuron is the sign of the net input, given by the function $\\operatorname{sign}(z)$, where $\\operatorname{sign}(t)$ returns $+1$ if $t>0$, $0$ if $t=0$, and $-1$ if $t<0$. Suppose the input and weights are simultaneously scaled by real factors $\\alpha$ and $\\beta$, respectively, while the bias is left unchanged, that is, $\\mathbf{x}\\leftarrow\\alpha\\mathbf{x}$, $\\mathbf{w}\\leftarrow\\beta\\mathbf{w}$, and $b\\leftarrow b$. \n\nStarting only from the definition of the net input and the decision function, derive the necessary and sufficient conditions on $(\\alpha,\\beta)$ under which the neuron’s decision $\\operatorname{sign}(z)$ remains invariant after the scaling. Fully characterize the cases in terms of the inner product $s=\\mathbf{w}^{\\top}\\mathbf{x}$ and the bias $b$, distinguishing between trivial and nontrivial scenarios and explaining why these distinctions arise.\n\nFinally, let $\\gamma=\\alpha\\beta$ denote the effective net scaling acting on the inner product. Determine, in closed form, the critical value $\\gamma^{\\star}$ of $\\gamma$ (as a symbolic expression in $s$ and $b$) at which the neuron’s decision flips its sign under the scaling transformation. Provide your final answer as a single analytic expression. No rounding is required.", "solution": "The neuron’s net input is defined by the linear model with bias, $z=\\mathbf{w}^{\\top}\\mathbf{x}+b$. The decision is $\\operatorname{sign}(z)$, which takes values in $\\{-1,0,+1\\}$. Under the simultaneous scaling $\\mathbf{x}\\leftarrow\\alpha\\mathbf{x}$ and $\\mathbf{w}\\leftarrow\\beta\\mathbf{w}$ while holding $b$ fixed, the transformed net input becomes\n$$\nz'=(\\beta\\mathbf{w})^{\\top}(\\alpha\\mathbf{x})+b=\\alpha\\beta\\,\\mathbf{w}^{\\top}\\mathbf{x}+b.\n$$\nIntroduce the shorthand $s=\\mathbf{w}^{\\top}\\mathbf{x}$. Then the original and transformed net inputs are\n$$\nz=s+b,\\qquad z'=\\alpha\\beta\\,s+b.\n$$\nWe require the decision to remain invariant, that is, $\\operatorname{sign}(z')=\\operatorname{sign}(z)$, which is equivalent to the condition that $z'$ remains on the same side of zero as $z$, or equals zero exactly when $z$ equals zero.\n\nWe analyze by cases determined by $s$ and $b$:\n\n1. Case $s=0$. Then $z=b$ and $z'=\\alpha\\beta\\cdot 0+b=b$. Therefore $\\operatorname{sign}(z')=\\operatorname{sign}(b)=\\operatorname{sign}(z)$ for all real $\\alpha$ and $\\beta$. This is a trivial invariance case: the inner product contributes nothing to the net input, so scaling $\\mathbf{x}$ and $\\mathbf{w}$ has no effect.\n\n2. Case $s\\neq 0$. Then $z=s+b$ and $z'=\\alpha\\beta\\,s+b$. Define $\\gamma=\\alpha\\beta$. The invariance condition $\\operatorname{sign}(z')=\\operatorname{sign}(z)$ is equivalent to requiring $z'$ and $z$ share the same sign (including zero). The boundary in $\\gamma$ at which the sign can change occurs when $z'=0$, that is,\n$$\n\\gamma\\,s+b=0.\n$$\nSolving for $\\gamma$ yields the critical value\n$$\n\\gamma^{\\star}=-\\frac{b}{s}.\n$$\nThis value separates the regions of $\\gamma$ that produce positive versus negative net input. Concretely:\n- If $s>0$, then $z'= \\gamma s + b$ is an increasing function of $\\gamma$. Therefore:\n  - If $z=s+b>0$, invariance requires $\\gamma>\\gamma^{\\star}$.\n  - If $z=s+b<0$, invariance requires $\\gamma<\\gamma^{\\star}$.\n  - If $z=s+b=0$, invariance at the boundary requires $\\gamma=\\gamma^{\\star}$ (any deviation changes the sign).\n- If $s<0$, then $z'=\\gamma s + b$ is a decreasing function of $\\gamma$. Therefore:\n  - If $z=s+b>0$, invariance requires $\\gamma<\\gamma^{\\star}$.\n  - If $z=s+b<0$, invariance requires $\\gamma>\\gamma^{\\star}$.\n  - If $z=s+b=0$, invariance at the boundary requires $\\gamma=\\gamma^{\\star}$ (any deviation changes the sign).\n\nThese results show that for $s\\neq 0$ and $z\\neq 0$ (nontrivial scenario), the set of $(\\alpha,\\beta)$ preserving the decision is the half-space in terms of $\\gamma=\\alpha\\beta$ determined by the sign of $s$ and which side of the boundary $\\gamma^{\\star}$ the original net input inhabits. The exact threshold for a decision flip is the unique scaling product that forces the transformed net input to zero.\n\nSpecial subcase $b=0$ (with $s\\neq 0$) illustrates a classical scaling invariance: $z=s$ and $z'=\\gamma s$. The decision is unchanged if and only if $\\gamma>0$ (with $\\gamma=0$ forcing $z'=0$). Here, $\\gamma^{\\star}=-b/s=0$, consistent with the boundary at which the sign can change.\n\nThus, the critical value of the effective scaling product at which the decision flips is given purely by the ratio of bias and inner product, with a negative sign:\n$$\n\\gamma^{\\star}=-\\frac{b}{s}.\n$$\nThis expression compactly characterizes the nontrivial case $s\\neq 0$; in the trivial case $s=0$, scaling has no effect on the decision and there is no meaningful finite threshold in $\\gamma$ because $z'$ is independent of $\\gamma$.", "answer": "$$\\boxed{-\\frac{b}{s}}$$", "id": "3199772"}, {"introduction": "Regularization is a cornerstone technique for preventing overfitting, but how does it distinctly affect weights and biases? This practice combines theoretical derivation with coding to demonstrate how strong $L_2$ regularization on the weights can cause them to shrink towards zero, making the bias term dominate the net input function [@problem_id:3199770]. By implementing and observing this phenomenon, you will gain a practical understanding of how an over-regularized model can underfit the data by producing nearly constant predictions.", "problem": "You are given a single-neuron model in deep learning whose net input function maps an input vector to a scalar via an affine transformation. The learning objective follows empirical risk minimization with squared error loss and an $L_2$ (Euclidean norm) penalty on the weights only. Your goal is to rigorously demonstrate, from first principles, how strong $L_2$ regularization shrinks the weight vector and thereby makes the bias term dominate the net input, resulting in nearly constant predictions and underfitting.\n\nStart from the foundational definitions of empirical risk minimization and $L_2$ regularization. Assume a dataset with $N$ samples, each input vector of dimension $d$. There is a single bias parameter. Formulate the training criterion and derive the conditions characterizing the minimizer. Using these conditions, deduce that when the regularization strength is large, the weight vector becomes small in norm and the net input values are dominated by the bias term. Explain why this leads to underfitting characterized by almost constant outputs.\n\nImplement a program that computes the trained weights and bias by solving the necessary linear system exactly for the squared-error loss with an $L_2$ penalty on the weights only. Then, for each test case below, compute the dominance ratio\n$$\nR \\equiv \\frac{\\max_{i \\in \\{1,\\dots,N\\}} \\left| \\mathbf{w}^\\top \\mathbf{x}_i \\right|}{|b| + 10^{-12}},\n$$\nwhich quantifies the amplitude of the linear (weight-dependent) contribution relative to the bias magnitude on the given dataset. A small ratio $R$ indicates that the bias dominates the net input function and predictions tend toward a constant, whereas a large ratio indicates the linear term dominates.\n\nYour program must:\n- Construct the specified datasets.\n- Train the model by minimizing the squared error with an $L_2$ penalty applied to the weights only (the bias must be left unregularized).\n- Compute the net input contributions and the ratio $R$ for each test case.\n- Output a single line containing the five $R$ values, rounded to six decimal places, as a comma-separated list enclosed in square brackets.\n\nTest suite (all vectors are row-major, and all numbers are exact integers or decimals; there are no physical units involved):\n- Case 1 (strong regularization, happy path dominance by bias): $N=5$, $d=3$, \n  $$\n  X=\\begin{bmatrix}\n  -2 & 1 & 0\\\\\n  -1 & 0 & 1\\\\\n  0 & 1 & -1\\\\\n  1 & -1 & 2\\\\\n  2 & 2 & -2\n  \\end{bmatrix},\\quad\n  \\mathbf{a}=\\begin{bmatrix}2\\\\-1\\\\0.5\\end{bmatrix},\\quad\n  c=3,\\quad\n  \\boldsymbol{\\epsilon}=\\begin{bmatrix}0\\\\-0.1\\\\0.1\\\\-0.05\\\\0.05\\end{bmatrix},\n  $$\n  and targets $y_i = \\mathbf{a}^\\top \\mathbf{x}_i + c + \\epsilon_i$. Use regularization strength $\\lambda = 10{,}000$.\n- Case 2 (no regularization, contrast case): same $X$, $\\mathbf{a}$, $c$, $\\boldsymbol{\\epsilon}$, and targets as Case 1, but $\\lambda = 0$.\n- Case 3 (boundary: zero features): $N=4$, $d=3$,\n  $$\n  X=\\begin{bmatrix}\n  0 & 0 & 0\\\\\n  0 & 0 & 0\\\\\n  0 & 0 & 0\\\\\n  0 & 0 & 0\n  \\end{bmatrix},\\quad\n  \\mathbf{y}=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\end{bmatrix},\n  $$\n  and $\\lambda = 500$.\n- Case 4 (edge: tiny feature scale yields small linear contribution even with moderate regularization): reuse the $X$ from Case 1 but scaled by $10^{-6}$ (i.e., replace $X$ with $10^{-6} X$), reuse $\\mathbf{a}$, $c$, $\\boldsymbol{\\epsilon}$, and targets defined as in Case 1 with the scaled $X$. Use $\\lambda = 1$.\n- Case 5 (features with large magnitude and moderate regularization, emphasizing linear term): reuse the $X$ from Case 1 but scaled by $10^{2}$ (i.e., replace $X$ with $100 X$), reuse $\\mathbf{a}$, $c$, $\\boldsymbol{\\epsilon}$, and targets defined as in Case 1 with the scaled $X$. Use $\\lambda = 10$.\n\nFinal output format:\n- Your program should produce a single line of output containing the five $R$ values, each rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5]\").", "solution": "The problem requires a first-principles demonstration of how strong $L_2$ regularization on the weights of a single-neuron model leads to underfitting, characterized by predictions that are nearly constant. This occurs because the weight vector $\\mathbf{w}$ is shrunk towards zero, causing the bias term $b$ to dominate the net input function. We will first derive the analytical solution for the model parameters and then use it to explain this phenomenon.\n\nThe model is a single neuron with a linear activation function, where the prediction $\\hat{y}_i$ for an input vector $\\mathbf{x}_i \\in \\mathbb{R}^d$ is given by the affine transformation (net input function):\n$$\n\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b\n$$\nHere, $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector and $b \\in \\mathbb{R}$ is the scalar bias term.\n\nThe learning objective is to find the parameters $(\\mathbf{w}, b)$ that minimize the empirical risk, defined as the sum of squared errors on a dataset of $N$ samples $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, with an added $L_2$ regularization penalty on the weights $\\mathbf{w}$. The objective function, $J(\\mathbf{w}, b)$, is:\n$$\nJ(\\mathbf{w}, b) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\mathbf{w}\\|_2^2 = \\sum_{i=1}^{N} (y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b))^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength. The bias term $b$ is not regularized, as specified.\n\nTo find the optimal parameters that minimize $J(\\mathbf{w}, b)$, we set its partial derivatives with respect to $\\mathbf{w}$ and $b$ to zero. This is a necessary condition for a minimum, and since $J$ is a strictly convex function of $(\\mathbf{w}, b)$ for $\\lambda > 0$, it is also a sufficient condition for a unique global minimum.\n\nFirst, we find the partial derivative with respect to the bias $b$:\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{N} \\frac{\\partial}{\\partial b} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b)^2 = \\sum_{i=1}^{N} 2(y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b)(-1) = -2 \\sum_{i=1}^{N} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b)\n$$\nSetting this to zero:\n$$\n\\sum_{i=1}^{N} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b) = 0 \\implies \\sum_{i=1}^{N} y_i - \\mathbf{w}^\\top \\sum_{i=1}^{N} \\mathbf{x}_i - N b = 0\n$$\nSolving for $b$ gives:\n$$\nb = \\frac{1}{N}\\sum_{i=1}^{N} y_i - \\mathbf{w}^\\top \\left(\\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{x}_i\\right)\n$$\nLet $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$ and $\\bar{\\mathbf{x}} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{x}_i$ be the means of the targets and inputs, respectively. The optimal bias is then:\n$$\nb = \\bar{y} - \\mathbf{w}^\\top \\bar{\\mathbf{x}}\n$$\nThis result shows that the optimal bias ensures the learned hyperplane passes through the centroid of the data $(\\bar{\\mathbf{x}}, \\bar{y})$.\n\nNow, we can substitute this expression for $b$ back into the objective function $J$. This decouples the problem of finding $\\mathbf{w}$ from $b$.\n$$\ny_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b) = y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + \\bar{y} - \\mathbf{w}^\\top \\bar{\\mathbf{x}}) = (y_i - \\bar{y}) - \\mathbf{w}^\\top(\\mathbf{x}_i - \\bar{\\mathbf{x}})\n$$\nLet $\\tilde{y}_i = y_i - \\bar{y}$ and $\\tilde{\\mathbf{x}}_i = \\mathbf{x}_i - \\bar{\\mathbf{x}}$ be the centered targets and inputs. The objective function becomes a function of $\\mathbf{w}$ only:\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^{N} (\\tilde{y}_i - \\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}\n$$\nIn matrix notation, let $\\tilde{X}$ be the $N \\times d$ matrix whose rows are $\\tilde{\\mathbf{x}}_i^\\top$ and $\\tilde{\\mathbf{y}}$ be the $N \\times 1$ vector of $\\tilde{y}_i$. The objective is:\n$$\nJ(\\mathbf{w}) = (\\tilde{\\mathbf{y}} - \\tilde{X}\\mathbf{w})^\\top(\\tilde{\\mathbf{y}} - \\tilde{X}\\mathbf{w}) + \\lambda \\mathbf{w}^\\top \\mathbf{w}\n$$\nTaking the gradient with respect to $\\mathbf{w}$:\n$$\n\\nabla_{\\mathbf{w}} J = -2\\tilde{X}^\\top\\tilde{\\mathbf{y}} + 2\\tilde{X}^\\top\\tilde{X}\\mathbf{w} + 2\\lambda\\mathbf{w}\n$$\nSetting the gradient to zero to find the optimal $\\mathbf{w}$:\n$$\n\\tilde{X}^\\top\\tilde{X}\\mathbf{w} + \\lambda\\mathbf{w} = \\tilde{X}^\\top\\tilde{\\mathbf{y}} \\implies (\\tilde{X}^\\top\\tilde{X} + \\lambda I_d)\\mathbf{w} = \\tilde{X}^\\top\\tilde{\\mathbf{y}}\n$$\nwhere $I_d$ is the $d \\times d$ identity matrix. The solution for the weight vector is:\n$$\n\\mathbf{w} = (\\tilde{X}^\\top\\tilde{X} + \\lambda I_d)^{-1} \\tilde{X}^\\top\\tilde{\\mathbf{y}}\n$$\nThis is the well-known solution for ridge regression.\n\nNow we analyze the effect of a large regularization strength, i.e., as $\\lambda \\to \\infty$. The term $\\lambda I_d$ in the matrix to be inverted becomes dominant.\n$$\n\\lim_{\\lambda \\to \\infty} (\\tilde{X}^\\top\\tilde{X} + \\lambda I_d) = \\lim_{\\lambda \\to \\infty} \\lambda \\left(\\frac{1}{\\lambda}\\tilde{X}^\\top\\tilde{X} + I_d\\right) \\approx \\lambda I_d\n$$\nThe inverse matrix is then approximated by:\n$$\n(\\tilde{X}^\\top\\tilde{X} + \\lambda I_d)^{-1} \\approx (\\lambda I_d)^{-1} = \\frac{1}{\\lambda}I_d\n$$\nSubstituting this back into the solution for $\\mathbf{w}$:\n$$\n\\mathbf{w} \\approx \\left(\\frac{1}{\\lambda}I_d\\right) \\tilde{X}^\\top\\tilde{\\mathbf{y}} = \\frac{1}{\\lambda} (\\tilde{X}^\\top\\tilde{\\mathbf{y}})\n$$\nAs $\\lambda \\to \\infty$, the term $\\frac{1}{\\lambda} \\to 0$, and consequently, the weight vector $\\mathbf{w}$ shrinks towards the zero vector:\n$$\n\\lim_{\\lambda \\to \\infty} \\mathbf{w} = \\mathbf{0}\n$$\nWith the weight vector approaching zero, we examine the impact on the bias term and the overall prediction. The optimal bias was $b = \\bar{y} - \\mathbf{w}^\\top \\bar{\\mathbf{x}}$. As $\\mathbf{w} \\to \\mathbf{0}$, the term $\\mathbf{w}^\\top \\bar{\\mathbf{x}} \\to 0$, and the bias converges to the mean of the targets:\n$$\n\\lim_{\\lambda \\to \\infty} b = \\bar{y}\n$$\nThe prediction for any input $\\mathbf{x}_i$ is $\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$. In the limit of large $\\lambda$:\n$$\n\\lim_{\\lambda \\to \\infty} \\hat{y}_i = (\\lim_{\\lambda \\to \\infty} \\mathbf{w})^\\top \\mathbf{x}_i + (\\lim_{\\lambda \\to \\infty} b) = \\mathbf{0}^\\top \\mathbf{x}_i + \\bar{y} = \\bar{y}\n$$\nThis demonstrates that for strong $L_2$ regularization, the model's predictions for all inputs converge to a constant value, the sample mean of the target variable $\\bar{y}$. The model effectively ignores the input features $\\mathbf{x}_i$, which is a clear case of underfitting. The net input $\\mathbf{w}^\\top \\mathbf{x}_i + b$ is dominated by the bias term $b$, while the feature-dependent part $\\mathbf{w}^\\top \\mathbf{x}_i$ vanishes. The dominance ratio $R$, which compares the magnitude of the linear term to the bias, will thus approach $0$.\n\nThe implementation will solve for the parameters by constructing and solving a single linear system for the augmented parameter vector $\\boldsymbol{\\theta} = [\\mathbf{w}^\\top, b]^\\top$. This approach is mathematically equivalent to the one derived above. Let $X_{aug} = [X, \\mathbf{1}]$ be the design matrix augmented with a column of ones. The objective function can be written as $J(\\boldsymbol{\\theta}) = (\\mathbf{y} - X_{aug}\\boldsymbol{\\theta})^\\top(\\mathbf{y} - X_{aug}\\boldsymbol{\\theta}) + \\boldsymbol{\\theta}^\\top \\Lambda_R \\boldsymbol{\\theta}$, where $\\Lambda_R$ is a $(d+1) \\times (d+1)$ matrix containing $\\lambda I_d$ in its top-left block and zeros elsewhere. Setting the gradient to zero yields the linear system $(X_{aug}^\\top X_{aug} + \\Lambda_R)\\boldsymbol{\\theta} = X_{aug}^\\top \\mathbf{y}$, which can be solved numerically for $\\boldsymbol{\\theta}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the weights and bias of a linear model with L2 regularization\n    on the weights only, and computes the dominance ratio R for several test cases.\n    \"\"\"\n\n    def train_and_compute_ratio(X, y, lambda_reg):\n        \"\"\"\n        Trains the model and computes the dominance ratio R.\n\n        The model is y_hat = X @ w + b.\n        The objective is ||y - (X @ w + b)||^2_2 + lambda * ||w||^2_2.\n        \n        This is solved by setting up an augmented linear system:\n        Let theta = [w, b]^T and X_aug = [X, 1].\n        The objective is ||y - X_aug @ theta||^2_2 + theta^T @ Lambda_mat @ theta,\n        where Lambda_mat is a diagonal matrix with lambda on the first d entries\n        and 0 for the bias term.\n        \n        The normal equation is (X_aug^T @ X_aug + Lambda_mat) @ theta = X_aug^T @ y.\n        \"\"\"\n        N, d = X.shape\n\n        # Construct the augmented design matrix [X, 1]\n        X_aug = np.hstack([X, np.ones((N, 1))])\n\n        # Construct the regularization matrix Lambda_mat\n        # It has shape (d+1)x(d+1) with lambda on the first d diagonal elements\n        # and 0 on the last diagonal element (for the unregularized bias).\n        lambda_mat = np.zeros((d + 1, d + 1))\n        lambda_mat[:d, :d] = lambda_reg * np.identity(d)\n        \n        # Form the linear system A * theta = B\n        A = X_aug.T @ X_aug + lambda_mat\n        B = X_aug.T @ y\n\n        # Solve for theta = [w, b]^T\n        try:\n            theta = np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular matrices (e.g., lambda=0 and rank-deficient X)\n            A_pinv = np.linalg.pinv(A)\n            theta = A_pinv @ B\n\n        # Extract weights w and bias b\n        w = theta[:d]\n        b = theta[d]\n\n        # Compute the dominance ratio R\n        # R = max(|w^T @ x_i|) / (|b| + epsilon)\n        linear_contribution = np.abs(X @ w)\n        max_abs_linear_contribution = np.max(linear_contribution) if N > 0 else 0.0\n        \n        dominance_ratio = max_abs_linear_contribution / (np.abs(b) + 1e-12)\n        \n        return dominance_ratio\n\n    # --- Test Case Definitions ---\n\n    # Case 1 & 2 & 4 & 5 data setup\n    X1 = np.array([\n        [-2.0, 1.0, 0.0],\n        [-1.0, 0.0, 1.0],\n        [ 0.0, 1.0, -1.0],\n        [ 1.0, -1.0, 2.0],\n        [ 2.0, 2.0, -2.0]\n    ])\n    a = np.array([2.0, -1.0, 0.5])\n    c = 3.0\n    epsilon = np.array([0.0, -0.1, 0.1, -0.05, 0.05])\n    \n    # --- Execute Test Cases ---\n    \n    results = []\n\n    # Case 1: Strong regularization\n    lambda1 = 10000.0\n    y1 = X1 @ a + c + epsilon\n    r1 = train_and_compute_ratio(X1, y1, lambda1)\n    results.append(r1)\n\n    # Case 2: No regularization\n    lambda2 = 0.0\n    y2 = y1 # Same data as case 1\n    r2 = train_and_compute_ratio(X1, y2, lambda2)\n    results.append(r2)\n\n    # Case 3: Zero features\n    X3 = np.zeros((4, 3))\n    y3 = np.array([1.0, 2.0, 3.0, 4.0])\n    lambda3 = 500.0\n    r3 = train_and_compute_ratio(X3, y3, lambda3)\n    results.append(r3)\n\n    # Case 4: Tiny feature scale\n    X4 = X1 * 1e-6\n    y4 = X4 @ a + c + epsilon\n    lambda4 = 1.0\n    r4 = train_and_compute_ratio(X4, y4, lambda4)\n    results.append(r4)\n\n    # Case 5: Large feature scale\n    X5 = X1 * 100.0\n    y5 = X5 @ a + c + epsilon\n    lambda5 = 10.0\n    r5 = train_and_compute_ratio(X5, y5, lambda5)\n    results.append(r5)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3199770"}, {"introduction": "Beyond initial training, we often need to calibrate a model's predictions to be more reliable. This practice focuses on a powerful post-hoc technique: adjusting only the bias terms of a trained classifier. You will begin by deriving the foundational gradient of the cross-entropy loss with respect to the biases [@problem_id:3199738], and then apply this result in code to optimize the biases on a validation set, improving the model's probabilistic outputs.", "problem": "You are given a multiclass linear-logit classifier whose net input function for class $k$ on input vector $\\mathbf{x} \\in \\mathbb{R}^d$ is defined by $z_k(\\mathbf{x}) = \\mathbf{w}_k^\\top \\mathbf{x} + b_k$, where $\\mathbf{w}_k \\in \\mathbb{R}^d$ are fixed weights and $b_k \\in \\mathbb{R}$ are class-specific biases. The predicted class probabilities are defined by the softmax function $p_k(\\mathbf{x}) = \\exp(z_k(\\mathbf{x}))/\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))$, and for a single labeled example $(\\mathbf{x}, y)$ with $y \\in \\{0,1,\\dots,K-1\\}$, the cross-entropy loss is $\\ell(\\mathbf{x}, y) = -\\log p_{y}(\\mathbf{x})$.\n\nTask A (derivation): Starting from the definitions of the net input function, the softmax function, and the cross-entropy loss stated above, derive the gradient of the loss with respect to the biases $b_k$ for a single labeled example. Your derivation must begin from these definitions and proceed by first principles of calculus, including the product rule and the chain rule. The final expression must be suitable for use in algorithmic optimization of the biases. Do not invoke shortcut formulas or pre-derived results.\n\nTask B (algorithmic calibration and measurement): Consider post-hoc calibration that adjusts only the biases $b_k$ while keeping all weights $\\mathbf{w}_k$ fixed. For a calibration dataset with logits matrix $L^{\\text{val}} \\in \\mathbb{R}^{N_{\\text{val}} \\times K}$ and labels $\\mathbf{y}^{\\text{val}} \\in \\{0,1,\\dots,K-1\\}^{N_{\\text{val}}}$, define the calibrated logits as $L^{\\text{val}} + \\mathbf{1} \\mathbf{b}^\\top$, where $\\mathbf{b} \\in \\mathbb{R}^K$ is the bias vector to be learned and $\\mathbf{1} \\in \\mathbb{R}^{N_{\\text{val}}}$ is the all-ones vector. The calibration objective is the average cross-entropy over the calibration dataset. After learning $\\mathbf{b}$, evaluate the effect of calibration on a separate test dataset with logits $L^{\\text{test}} \\in \\mathbb{R}^{N_{\\text{test}} \\times K}$ and labels $\\mathbf{y}^{\\text{test}} \\in \\{0,1,\\dots,K-1\\}^{N_{\\text{test}}}$ by computing the change in average negative log-likelihood (cross-entropy) on the test dataset, defined as the average cross-entropy before calibration minus the average cross-entropy after applying the learned biases to $L^{\\text{test}}$. Express this change as a real number (a float).\n\nYou must implement a complete, runnable program that:\n- Derives the gradient in Task A into an algorithmic form used to optimize $\\mathbf{b}$ for Task B.\n- Optimizes $\\mathbf{b}$ for each calibration dataset by minimizing the average cross-entropy using a gradient-based method.\n- Computes the improvement on the test dataset as described above, rounded to $6$ decimal places.\n\nUse the following test suite of datasets. Each case is defined by $(L^{\\text{val}}, \\mathbf{y}^{\\text{val}}, L^{\\text{test}}, \\mathbf{y}^{\\text{test}})$ with $K = 3$:\n\nCase $1$ (general case):\n$$\nL^{\\text{val}(1)} = \\begin{bmatrix}\n2.0 & 1.0 & 0.0\\\\\n1.5 & -0.5 & 0.0\\\\\n0.0 & 1.0 & 1.0\\\\\n1.0 & 0.5 & -1.0\\\\\n-0.5 & 0.0 & 0.5\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(1)} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\\\ 0\\\\ 2 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(1)} = \\begin{bmatrix}\n1.8 & 1.2 & 0.0\\\\\n1.4 & -0.3 & 0.1\\\\\n0.1 & 0.9 & 1.0\\\\\n0.9 & 0.4 & -0.8\\\\\n-0.6 & 0.1 & 0.6\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(1)} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\\\ 0\\\\ 2 \\end{bmatrix}.\n$$\n\nCase $2$ (near-perfect predictions):\n$$\nL^{\\text{val}(2)} = \\begin{bmatrix}\n3.0 & 0.0 & -1.0\\\\\n0.0 & 3.0 & -1.0\\\\\n-1.0 & 0.0 & 3.0\\\\\n3.5 & -0.5 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 2\\\\ 0 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(2)} = \\begin{bmatrix}\n3.0 & 0.0 & -1.0\\\\\n0.0 & 3.0 & -1.0\\\\\n-1.0 & 0.0 & 3.0\\\\\n3.5 & -0.5 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 2\\\\ 0 \\end{bmatrix}.\n$$\n\nCase $3$ (uninformative logits, imbalanced labels):\n$$\nL^{\\text{val}(3)} = \\begin{bmatrix}\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(3)} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 1\\\\ 2 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(3)} = \\begin{bmatrix}\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\\\\\n0.0 & 0.0 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(3)} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 1\\\\ 2 \\end{bmatrix}.\n$$\n\nFinal output format: Your program should produce a single line of output containing the three improvement values for Case $1$, Case $2$, and Case $3$ respectively, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets, for example $[0.012345,0.000000,0.086421]$.", "solution": "The problem requires the derivation of the gradient of the cross-entropy loss with respect to model biases and the implementation of a bias calibration procedure for a multiclass linear-logit classifier. The problem is well-posed, scientifically sound, and provides all necessary information for a unique solution.\n\n### Part A: Derivation of the Gradient\n\nThe first task is to derive the gradient of the cross-entropy loss, $\\ell$, with respect to the class-specific biases, $b_k$. The derivation will proceed from the provided first principles.\n\nThe cross-entropy loss for a single labeled example $(\\mathbf{x}, y)$ is defined as:\n$$\n\\ell(\\mathbf{x}, y) = -\\log p_{y}(\\mathbf{x})\n$$\nwhere $y$ is the true class index and $p_{y}(\\mathbf{x})$ is the predicted probability for the true class. The probabilities are given by the softmax function:\n$$\np_k(\\mathbf{x}) = \\frac{\\exp(z_k(\\mathbf{x}))}{\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))}\n$$\nThe net input function, or logit, for class $k$ is:\n$$\nz_k(\\mathbf{x}) = \\mathbf{w}_k^\\top \\mathbf{x} + b_k\n$$\nSubstituting the softmax definition into the loss function yields:\n$$\n\\ell = -\\log \\left( \\frac{\\exp(z_y)}{\\sum_{j=1}^K \\exp(z_j)} \\right)\n$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$, the loss function can be rewritten as:\n$$\n\\ell = - \\left( \\log(\\exp(z_y)) - \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right)\n$$\nSince $\\log(\\exp(u)) = u$, this simplifies to the log-sum-exp form:\n$$\n\\ell = -z_y + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)\n$$\nWe want to find the partial derivative of $\\ell$ with respect to a bias term $b_k$, for an arbitrary class $k \\in \\{0, 1, \\dots, K-1\\}$. Using the sum rule for differentiation:\n$$\n\\frac{\\partial \\ell}{\\partial b_k} = \\frac{\\partial}{\\partial b_k} \\left( -z_y + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right) = -\\frac{\\partial z_y}{\\partial b_k} + \\frac{\\partial}{\\partial b_k}\\left( \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right)\n$$\nLet's evaluate each term separately.\n\nFor the first term, we differentiate $z_y$ with respect to $b_k$. The net input $z_j = \\mathbf{w}_j^\\top \\mathbf{x} + b_j$ depends on $b_k$ only if $j=k$. Therefore:\n$$\n\\frac{\\partial z_j}{\\partial b_k} = \\delta_{jk}\n$$\nwhere $\\delta_{jk}$ is the Kronecker delta, which is $1$ if $j=k$ and $0$ otherwise. Applying this to $z_y$:\n$$\n\\frac{\\partial z_y}{\\partial b_k} = \\delta_{yk}\n$$\nFor the second term, we apply the chain rule. Let $S = \\sum_{j=1}^K \\exp(z_j)$. The term is $\\log(S)$.\n$$\n\\frac{\\partial}{\\partial b_k} \\log(S) = \\frac{1}{S} \\cdot \\frac{\\partial S}{\\partial b_k}\n$$\nNow, we find the derivative of $S$ with respect to $b_k$:\n$$\n\\frac{\\partial S}{\\partial b_k} = \\frac{\\partial}{\\partial b_k} \\left( \\sum_{j=1}^K \\exp(z_j) \\right) = \\sum_{j=1}^K \\frac{\\partial}{\\partial b_k} \\exp(z_j)\n$$\nApplying the chain rule to each term in the sum:\n$$\n\\frac{\\partial}{\\partial b_k} \\exp(z_j) = \\exp(z_j) \\cdot \\frac{\\partial z_j}{\\partial b_k} = \\exp(z_j) \\cdot \\delta_{jk}\n$$\nThe summation $\\sum_{j=1}^K \\exp(z_j) \\cdot \\delta_{jk}$ contains only one non-zero term, which occurs when $j=k$. Thus, the sum simplifies to $\\exp(z_k)$.\n$$\n\\frac{\\partial S}{\\partial b_k} = \\exp(z_k)\n$$\nSubstituting this back into the expression for the derivative of the log term:\n$$\n\\frac{\\partial}{\\partial b_k} \\log(S) = \\frac{1}{S} \\cdot \\exp(z_k) = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)} = p_k(\\mathbf{x})\n$$\nFinally, we combine the two parts to get the full gradient of the loss $\\ell$ with respect to the bias $b_k$:\n$$\n\\frac{\\partial \\ell}{\\partial b_k} = p_k(\\mathbf{x}) - \\delta_{yk}\n$$\nThis expression is the gradient for a single example. If $k$ is the correct class ($k=y$), the gradient is $p_y - 1$. If $k$ is any other class ($k \\neq y$), the gradient is $p_k$. In vector form, this is $\\nabla_{\\mathbf{b}}\\ell = \\mathbf{p} - \\mathbf{e}_y$, where $\\mathbf{p}$ is the vector of softmax probabilities and $\\mathbf{e}_y$ is the one-hot encoded vector for the true label $y$.\n\n### Part B: Algorithmic Implementation\n\nThe second task is to perform post-hoc bias calibration and measure the resulting improvement. This involves optimizing the biases $\\mathbf{b}$ to minimize the average cross-entropy loss on a calibration dataset.\n\nThe objective function to minimize is the average cross-entropy over $N_{\\text{val}}$ samples:\n$$\n\\mathcal{L}(\\mathbf{b}) = \\frac{1}{N_{\\text{val}}} \\sum_{i=1}^{N_{\\text{val}}} \\ell_i(\\mathbf{b})\n$$\nwhere $\\ell_i(\\mathbf{b})$ is the loss for the $i$-th sample, with calibrated logits $z_{ik} = L^{\\text{val}}_{ik} + b_k$. The gradient of this objective with respect to $b_k$ is the average of the individual gradients:\n$$\n\\nabla_{b_k} \\mathcal{L}(\\mathbf{b}) = \\frac{1}{N_{\\text{val}}} \\sum_{i=1}^{N_{\\text{val}}} (p_{ik} - \\delta_{y_i k})\n$$\nwhere $p_{ik}$ is the softmax probability of class $k$ for sample $i$ using the calibrated logits, and $y_i$ is the true label for sample $i$.\n\nWe will use a gradient-based optimization algorithm, specifically L-BFGS-B provided by `scipy.optimize.minimize`, to find the optimal bias vector $\\mathbf{b}^*$ that minimizes $\\mathcal{L}(\\mathbf{b})$. This is a robust and efficient method for smooth, unconstrained (or box-constrained) optimization problems like this one. The initial guess for the biases will be $\\mathbf{b} = \\mathbf{0}$.\n\nFor numerical stability, the cross-entropy loss and softmax calculations will employ the log-sum-exp trick. The cross-entropy loss for a single sample is computed as $\\ell_i = \\log\\left(\\sum_j \\exp(z_{ij})\\right) - z_{iy_i}$. This avoids floating-point overflow and underflow issues with large or small logit values.\n\nAfter determining the optimal bias vector $\\mathbf{b}^*$ using the calibration set $(L^{\\text{val}}, \\mathbf{y}^{\\text{val}})$, we evaluate its performance on the test set $(L^{\\text{test}}, \\mathbf{y}^{\\text{test}})$. The evaluation metric is the change in average negative log-likelihood (NLL):\n$$\n\\Delta_{\\text{NLL}} = \\text{NLL}_{\\text{before}} - \\text{NLL}_{\\text{after}}\n$$\nwhere:\n- $\\text{NLL}_{\\text{before}}$ is the average cross-entropy on the test set using the original logits $L^{\\text{test}}$.\n- $\\text{NLL}_{\\text{after}}$ is the average cross-entropy on the test set using the calibrated logits $L^{\\text{test}} + \\mathbf{1}(\\mathbf{b}^*)^\\top$.\n\nA positive value for $\\Delta_{\\text{NLL}}$ indicates an improvement in model calibration, meaning the predicted probabilities are more accurate after adjusting the biases. The final program will implement this procedure for each of the three provided test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax, logsumexp\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the bias calibration problem for the three given test cases.\n    \"\"\"\n    \n    # CASE 1\n    L_val_1 = np.array([\n        [2.0, 1.0, 0.0],\n        [1.5, -0.5, 0.0],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.5, -1.0],\n        [-0.5, 0.0, 0.5]\n    ])\n    y_val_1 = np.array([0, 0, 2, 0, 2])\n    L_test_1 = np.array([\n        [1.8, 1.2, 0.0],\n        [1.4, -0.3, 0.1],\n        [0.1, 0.9, 1.0],\n        [0.9, 0.4, -0.8],\n        [-0.6, 0.1, 0.6]\n    ])\n    y_test_1 = np.array([0, 0, 2, 0, 2])\n\n    # CASE 2\n    L_val_2 = np.array([\n        [3.0, 0.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-1.0, 0.0, 3.0],\n        [3.5, -0.5, 0.0]\n    ])\n    y_val_2 = np.array([0, 1, 2, 0])\n    L_test_2 = np.array([\n        [3.0, 0.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-1.0, 0.0, 3.0],\n        [3.5, -0.5, 0.0]\n    ])\n    y_test_2 = np.array([0, 1, 2, 0])\n\n    # CASE 3\n    L_val_3 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y_val_3 = np.array([0, 0, 0, 1, 1, 2])\n    L_test_3 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y_test_3 = np.array([0, 0, 0, 1, 1, 2])\n\n    test_cases = [\n        (L_val_1, y_val_1, L_test_1, y_test_1),\n        (L_val_2, y_val_2, L_test_2, y_test_2),\n        (L_val_3, y_val_3, L_test_3, y_test_3)\n    ]\n\n    results = []\n\n    def avg_cross_entropy_loss(logits, labels):\n        \"\"\"\n        Computes the average cross-entropy loss using the log-sum-exp trick\n        for numerical stability.\n        \"\"\"\n        num_samples = logits.shape[0]\n        # Log-sum-exp over classes for each sample\n        lse = logsumexp(logits, axis=1)\n        # Get the logits for the true classes\n        true_class_logits = logits[np.arange(num_samples), labels]\n        # Negative log-likelihood for each sample\n        nll = lse - true_class_logits\n        return np.mean(nll)\n\n    def objective_function(b, L, y):\n        \"\"\"\n        The objective function to minimize: average cross-entropy on the\n        calibration set with the current bias vector b.\n        \"\"\"\n        calibrated_logits = L + b\n        return avg_cross_entropy_loss(calibrated_logits, y)\n\n    def gradient_function(b, L, y):\n        \"\"\"\n        The gradient of the objective function with respect to the bias vector b.\n        \"\"\"\n        num_samples, num_classes = L.shape\n        calibrated_logits = L + b\n        \n        # Calculate softmax probabilities\n        probs = softmax(calibrated_logits, axis=1)\n        \n        # Create one-hot encoded labels\n        y_one_hot = np.zeros((num_samples, num_classes))\n        y_one_hot[np.arange(num_samples), y] = 1.0\n        \n        # Gradient is the average of (probs - one_hot_labels) over samples\n        grad = np.mean(probs - y_one_hot, axis=0)\n        return grad\n\n    for L_val, y_val, L_test, y_test in test_cases:\n        # Number of classes\n        K = L_val.shape[1]\n        \n        # Initial guess for the bias vector\n        b_initial = np.zeros(K)\n        \n        # Optimize the biases using the validation set\n        opt_result = minimize(\n            fun=objective_function,\n            x0=b_initial,\n            args=(L_val, y_val),\n            method='L-BFGS-B',\n            jac=gradient_function\n        )\n        b_optimal = opt_result.x\n        \n        # Evaluate on the test set\n        # 1. NLL before calibration\n        nll_before = avg_cross_entropy_loss(L_test, y_test)\n        \n        # 2. NLL after calibration\n        calibrated_L_test = L_test + b_optimal\n        nll_after = avg_cross_entropy_loss(calibrated_L_test, y_test)\n        \n        # 3. Compute the improvement\n        improvement = nll_before - nll_after\n        \n        results.append(f\"{improvement:.6f}\")\n\n    # Print the final results in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3199738"}]}