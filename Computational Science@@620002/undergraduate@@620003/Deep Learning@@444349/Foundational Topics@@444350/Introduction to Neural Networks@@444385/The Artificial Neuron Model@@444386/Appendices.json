{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the classical Perceptron, the historical ancestor of modern artificial neurons. This exercise [@problem_id:3190716] challenges you to see firsthand what a single neuron can and cannot represent, by first implementing a simple logic gate and then confronting the famous XOR problem. Understanding this fundamental limit of linear separability is the key motivation for developing the multi-layer networks that are central to deep learning.", "problem": "Consider a binary classification task with a perceptron model defined by the decision function $\\hat{y} = \\operatorname{sign}(\\boldsymbol{w}^{\\top} \\boldsymbol{x} + b)$, where $\\boldsymbol{x} \\in \\{0,1\\}^{d}$, $\\boldsymbol{w} \\in \\mathbb{R}^{d}$, and $b \\in \\mathbb{R}$. In all parts below, the class labels are $y \\in \\{-1,+1\\}$, with the positive class labeled $+1$.\n\nPart A (majority of three): Let $d = 3$ and define the target as the majority-vote logic: $y = +1$ if $x_1 + x_2 + x_3 \\geq 2$ and $y = -1$ otherwise. Assume the perceptron has equal positive weights, $w_1 = w_2 = w_3 = w$ with $w > 0$, and bias $b \\in \\mathbb{R}$. Among all separating hyperplanes satisfying correct classification for all $\\boldsymbol{x} \\in \\{0,1\\}^{3}$, determine the value of the ratio $b/w$ that maximizes the minimum signed distance, measured along the perceptronâ€™s normal direction, between the decision boundary and any training point.\n\nPart B (failure on Exclusive OR (XOR)): Consider $d = 2$ with the Exclusive OR (XOR) target defined by $y = +1$ if $x_1 + x_2 = 1$ and $y = -1$ if $x_1 + x_2 \\in \\{0,2\\}$. From first principles (without appealing to pre-stated theorems), explain why no choice of $(\\boldsymbol{w},b)$ yields a separating hyperplane in the original input space.\n\nPart C (restoring separability via pairwise interaction features): Augment the $d=2$ XOR input by adding the pairwise interaction to form the feature map $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2) \\in \\mathbb{R}^{3}$. Explain how an appropriate choice of weights and bias in the augmented space can correctly separate the XOR data.\n\nYour final answer should be the exact value of $b/w$ from Part A, expressed as a single reduced fraction. Do not include any units. No rounding is required.", "solution": "The problem presents a three-part analysis of the perceptron model. I will address each part systematically after validating the problem's integrity.\n\nThe problem statement is valid. It is scientifically grounded in the established theory of linear classifiers and the perceptron model. The tasks are well-posed, objective, and use precise mathematical language, providing all necessary information to derive the solutions. The problem is a standard, non-trivial exercise in machine learning fundamentals.\n\n### Part A: Majority-of-Three Function\n\nThe task is to find the ratio $b/w$ that maximizes the minimum geometric margin for a perceptron classifying the majority-of-three function.\n\nThe input space is $\\boldsymbol{x} \\in \\{0,1\\}^3$. There are $2^3=8$ possible input vectors. The target function is $y = +1$ if the sum of the inputs $S = x_1+x_2+x_3$ is at least $2$, and $y=-1$ otherwise.\nThe data points and their corresponding labels $y$ and sums $S$ are:\n- $S=0$: $\\boldsymbol{x}=(0,0,0)$, $y=-1$\n- $S=1$: $\\boldsymbol{x} \\in \\{(1,0,0), (0,1,0), (0,0,1)\\}$, $y=-1$\n- $S=2$: $\\boldsymbol{x} \\in \\{(1,1,0), (1,0,1), (0,1,1)\\}$, $y=+1$\n- $S=3$: $\\boldsymbol{x}=(1,1,1)$, $y=+1$\n\nThe perceptron model has a decision boundary defined by $\\boldsymbol{w}^{\\top}\\boldsymbol{x}+b=0$. We are given that the weights are equal and positive, $w_1=w_2=w_3=w > 0$. The activation function is therefore:\n$$\\boldsymbol{w}^{\\top}\\boldsymbol{x}+b = w_1x_1+w_2x_2+w_3x_3+b = w(x_1+x_2+x_3)+b = wS+b$$\nFor a point $\\boldsymbol{x}$ with label $y$ to be correctly classified, we must have $y(wS+b)>0$. Let's establish the conditions for the separability of all $8$ points.\n- For $S=0, y=-1$: $(-1)(w \\cdot 0 + b) > 0 \\implies -b > 0 \\implies b  0$.\n- For $S=1, y=-1$: $(-1)(w \\cdot 1 + b) > 0 \\implies -w-b > 0 \\implies w+b  0$.\n- For $S=2, y=+1$: $(+1)(w \\cdot 2 + b) > 0 \\implies 2w+b > 0$.\n- For $S=3, y=+1$: $(+1)(w \\cdot 3 + b) > 0 \\implies 3w+b > 0$.\n\nFrom $w+b0$, we get $b  -w$. From $2w+b>0$, we get $b > -2w$. Combining these, we find that any separating hyperplane of this form must satisfy $-2w  b  -w$. Since $w>0$, this implies $b0$, consistent with our first inequality. It also implies $3w+b > 3w-2w = w > 0$, satisfying the fourth inequality. Thus, the condition for separability is $-2  b/w  -1$.\n\nThe signed geometric distance (or margin) of a point $\\boldsymbol{x}_i$ from the decision boundary is given by $\\gamma_i = \\frac{y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i+b)}{\\|\\boldsymbol{w}\\|}$. We aim to maximize the minimum such distance, $\\gamma_{\\min} = \\min_{i} \\gamma_i$.\nThe norm of the weight vector is $\\|\\boldsymbol{w}\\| = \\sqrt{w_1^2+w_2^2+w_3^2} = \\sqrt{w^2+w^2+w^2} = \\sqrt{3w^2} = w\\sqrt{3}$ (since $w>0$).\n\nThe margin for each group of points, distinguished by their sum $S$, is:\n- $S=0, y=-1$: $\\gamma_0 = \\frac{(-1)(w \\cdot 0 + b)}{w\\sqrt{3}} = \\frac{-b}{w\\sqrt{3}}$\n- $S=1, y=-1$: $\\gamma_1 = \\frac{(-1)(w \\cdot 1 + b)}{w\\sqrt{3}} = \\frac{-w-b}{w\\sqrt{3}}$\n- $S=2, y=+1$: $\\gamma_2 = \\frac{(+1)(w \\cdot 2 + b)}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$\n- $S=3, y=+1$: $\\gamma_3 = \\frac{(+1)(w \\cdot 3 + b)}{w\\sqrt{3}} = \\frac{3w+b}{w\\sqrt{3}}$\n\nThe overall margin is $\\gamma_{\\min} = \\min(\\gamma_0, \\gamma_1, \\gamma_2, \\gamma_3)$. The points that constrain the margin (the support vectors) are those closest to the decision boundary. Within the region of separability ($-2w  b  -w$), the smallest positive values of $y(wS+b)$ will come from $S=1$ and $S=2$. Let's verify this. Let $f(S) = y(wS+b)$.\n- $f(0)=-b$. $f(1)=-w-b$. Since $w>0$, $-b > -w-b$. Thus $\\gamma_0 > \\gamma_1$.\n- $f(2)=2w+b$. $f(3)=3w+b$. Since $w>0$, $2w+b  3w+b$. Thus $\\gamma_2  \\gamma_3$.\nThe minimum margin is therefore $\\gamma_{\\min} = \\min(\\gamma_1, \\gamma_2)$. To maximize this minimum, we must set the two arguments to be equal. This occurs for the maximal margin hyperplane.\n$$\\gamma_1 = \\gamma_2$$\n$$\\frac{-w-b}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$$\n$$-w-b = 2w+b$$\n$$-3w = 2b$$\n$$\\frac{b}{w} = -\\frac{3}{2}$$\nThis value lies in the interval $(-2, -1)$, confirming it corresponds to a valid separating hyperplane. The ratio $b/w$ that maximizes the minimum distance is $-3/2$.\n\n### Part B: Non-separability of XOR\n\nThe Exclusive OR (XOR) function for $d=2$ is defined by the following input-output pairs:\n- $\\boldsymbol{x}^{(1)}=(0,0)$, $y^{(1)}=-1$\n- $\\boldsymbol{x}^{(2)}=(0,1)$, $y^{(2)}=+1$\n- $\\boldsymbol{x}^{(3)}=(1,0)$, $y^{(3)}=+1$\n- $\\boldsymbol{x}^{(4)}=(1,1)$, $y^{(4)}=-1$\n\nA linear separator is a hyperplane defined by $w_1 x_1 + w_2 x_2 + b = 0$. For the data to be linearly separable, there must exist parameters $(w_1, w_2, b)$ such that $y^{(i)}(w_1 x_1^{(i)} + w_2 x_2^{(i)} + b) > 0$ for all $i \\in \\{1,2,3,4\\}$. This gives rise to a system of four linear inequalities:\n1. For $(0,0), y=-1$: $(-1)(w_1 \\cdot 0 + w_2 \\cdot 0 + b) > 0 \\implies -b > 0 \\implies b  0$.\n2. For $(0,1), y=+1$: $(+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b) > 0 \\implies w_2 + b > 0$.\n3. For $(1,0), y=+1$: $(+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b) > 0 \\implies w_1 + b > 0$.\n4. For $(1,1), y=-1$: $(-1)(w_1 \\cdot 1 + w_2 \\cdot 1 + b) > 0 \\implies w_1 + w_2 + b  0$.\n\nLet's demonstrate that this system has no solution.\nFrom inequality (2), we have $w_2 > -b$.\nFrom inequality (3), we have $w_1 > -b$.\nAdding these two expressions gives:\n$$w_1 + w_2 > -2b$$\nFrom inequality (1), we know $b$ is negative. Let $c = -b$, where $c > 0$. The inequalities become:\n- $w_2 > c$\n- $w_1 > c$\n- $w_1 + w_2 > 2c$\nInequality (4) can be rewritten as $w_1 + w_2  -b$, which is $w_1 + w_2  c$.\nWe have derived two contradictory requirements: $w_1 + w_2 > 2c$ and $w_1 + w_2  c$. Since $c$ is a positive value, a number cannot be simultaneously greater than $2c$ and less than $c$. This contradiction proves that no single linear hyperplane can satisfy all four conditions. Therefore, the XOR function is not linearly separable.\n\n### Part C: Restoring Separability with a Feature Map\n\nWe augment the input space by mapping $\\boldsymbol{x}=(x_1, x_2)$ to a new feature space using $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2)$. Let's call the new coordinates $\\mathbf{z}=(z_1, z_2, z_3)$. The data points are transformed as follows:\n- $\\boldsymbol{x}=(0,0) \\implies \\mathbf{z}^{(1)}=(0,0,0)$, $y^{(1)}=-1$\n- $\\boldsymbol{x}=(0,1) \\implies \\mathbf{z}^{(2)}=(0,1,0)$, $y^{(2)}=+1$\n- $\\boldsymbol{x}=(1,0) \\implies \\mathbf{z}^{(3)}=(1,0,0)$, $y^{(3)}=+1$\n- $\\boldsymbol{x}=(1,1) \\implies \\mathbf{z}^{(4)}=(1,1,1)$, $y^{(4)}=-1$\n\nWe now seek a separating hyperplane in this new 3D space, defined by $\\boldsymbol{w}'^{\\top}\\mathbf{z} + b' = w'_1 z_1 + w'_2 z_2 + w'_3 z_3 + b' = 0$. The separability conditions are:\n1. For $\\mathbf{z}=(0,0,0), y=-1$: $-b' > 0 \\implies b'  0$.\n2. For $\\mathbf{z}=(0,1,0), y=+1$: $w'_2 + b' > 0$.\n3. For $\\mathbf{z}=(1,0,0), y=+1$: $w'_1 + b' > 0$.\n4. For $\\mathbf{z}=(1,1,1), y=-1$: $-(w'_1 + w'_2 + w'_3 + b') > 0 \\implies w'_1 + w'_2 + w'_3 + b'  0$.\n\nUnlike the original problem, this system of inequalities has solutions. Let's construct one.\nChoose $w'_1 = 1$ and $w'_2 = 1$.\nFrom conditions (2) and (3), we need $1+b' > 0$, so $b' > -1$.\nCondition (1) requires $b'  0$. Thus we must have $-1  b'  0$. Let's select $b' = -1/2$.\nNow, substitute these into condition (4):\n$1 + 1 + w'_3 - \\frac{1}{2}  0 \\implies \\frac{3}{2} + w'_3  0 \\implies w'_3  -\\frac{3}{2}$.\nLet's choose $w'_3 = -2$.\n\nA valid separator is given by the parameters $\\boldsymbol{w}'=(1, 1, -2)$ and $b'=-1/2$. Let's verify:\n- $\\mathbf{z}^{(1)}=(0,0,0), y=-1$: $1(0)+1(0)-2(0) - 1/2 = -1/2$. $\\operatorname{sign}(-1/2)=-1$. Correct.\n- $\\mathbf{z}^{(2)}=(0,1,0), y=+1$: $1(0)+1(1)-2(0) - 1/2 = 1/2$. $\\operatorname{sign}(1/2)=+1$. Correct.\n- $\\mathbf{z}^{(3)}=(1,0,0), y=+1$: $1(1)+1(0)-2(0) - 1/2 = 1/2$. $\\operatorname{sign}(1/2)=+1$. Correct.\n- $\\mathbf{z}^{(4)}=(1,1,1), y=-1$: $1(1)+1(1)-2(1) - 1/2 = -1/2$. $\\operatorname{sign}(-1/2)=-1$. Correct.\n\nThe addition of the non-linear feature $x_1x_2$ maps the data into a higher-dimensional space where it becomes linearly separable. Geometrically, the four points in the original $x_1, x_2$ plane are not separable by a line. The feature map lifts one of the points, $(1,1)$, off the $z_3=0$ plane to $(1,1,1)$. The four points in $\\mathbb{R}^3$ are $(0,0,0)$, $(0,1,0)$, $(1,0,0)$, and $(1,1,1)$. A plane (e.g., $z_1+z_2-2z_3 - 1/2 = 0$) can now be placed to separate the positive class points $\\{(0,1,0), (1,0,0)\\}$ from the negative class points $\\{(0,0,0), (1,1,1)\\}$.\n\nThe final required answer is the specific result from Part A. As derived above, this is the ratio $b/w$.", "answer": "$$\\boxed{-\\frac{3}{2}}$$", "id": "3190716"}, {"introduction": "An artificial neuron is more versatile than just a binary classifier. By changing its activation function to a simple identity mapping, it becomes a powerful tool for linear regression [@problem_id:3180363]. This practice will guide you through deriving the optimal parameters, $\\mathbf{w}$ and $b$, from the perspective of minimizing squared error, directly connecting the neuron to fundamental statistical principles and contrasting this approach with other machine learning philosophies.", "problem": "Consider a single artificial neuron with identity activation, so that its output on an input vector $\\mathbf{x} \\in \\mathbb{R}^{d}$ is $f(\\mathbf{x};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x} + b$, where $\\mathbf{w} \\in \\mathbb{R}^{d}$ are the synaptic weights and $b \\in \\mathbb{R}$ is the bias. For a labeled dataset $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ with $y_{i} \\in \\{-1, +1\\}$, define the empirical squared error objective as $J(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}$. Starting only from these definitions, derive from first principles the closed-form optimal parameters $(\\mathbf{w}^{\\star}, b^{\\star})$ that minimize $J(\\mathbf{w}, b)$.\n\nThen, apply your derivation to the specific, linearly separable dataset in $\\mathbb{R}^{2}$:\n- Positive class ($y_{i} = +1$): $\\mathbf{x}_{1} = (2, 0)$, $\\mathbf{x}_{2} = (0, 2)$.\n- Negative class ($y_{i} = -1$): $\\mathbf{x}_{3} = (-2, 0)$, $\\mathbf{x}_{4} = (0, -2)$.\n\nCompute the optimal $\\mathbf{w}^{\\star} = (w_{1}^{\\star}, w_{2}^{\\star})$ and $b^{\\star}$ for this dataset, and provide your final answer as the row vector $\\big[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}\\big]$. Round each numerical component to four significant figures.\n\nFinally, discuss qualitatively (no computation required) the conditions under which the squared-error minimizer for a single identity-activation neuron yields a decision boundary that coincides with the large-margin separating hyperplane of a Support Vector Machine (SVM), such as symmetry and covariance properties of the classes. Your discussion should be reasoned from the definitions above and any widely accepted facts about geometric margins.", "solution": "The user has provided a multi-part problem. I will address each part in sequence after validating the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Model:** A single artificial neuron with an identity activation function.\n- **Output function:** $f(\\mathbf{x};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x} + b$, where $\\mathbf{x} \\in \\mathbb{R}^{d}$, $\\mathbf{w} \\in \\mathbb{R}^{d}$, and $b \\in \\mathbb{R}$.\n- **Dataset:** A labeled dataset $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ with labels $y_{i} \\in \\{-1, +1\\}$.\n- **Objective function:** The empirical squared error, $J(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}$.\n- **Task 1:** Derive the closed-form optimal parameters $(\\mathbf{w}^{\\star}, b^{\\star})$ that minimize $J(\\mathbf{w}, b)$.\n- **Task 2:** Apply the derivation to a specific dataset in $\\mathbb{R}^{2}$ ($d=2$, $n=4$):\n  - Positive class ($y=+1$): $\\mathbf{x}_{1} = (2, 0)$, $\\mathbf{x}_{2} = (0, 2)$.\n  - Negative class ($y=-1$): $\\mathbf{x}_{3} = (-2, 0)$, $\\mathbf{x}_{4} = (0, -2)$.\n- **Task 3:** Compute the numerical values of $(\\mathbf{w}^{\\star}, b^{\\star})$ and provide the final answer as the row vector $[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}]$, with each component rounded to four significant figures.\n- **Task 4:** Qualitatively discuss the conditions under which the squared-error minimizer's decision boundary coincides with that of a large-margin Support Vector Machine (SVM).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard formulation of linear regression, a fundamental topic in statistics and machine learning. The model is a simple linear neuron, and the objective function is the sum of squared errors. This is scientifically and mathematically sound.\n- **Well-Posed:** The objective function $J(\\mathbf{w}, b)$ is a quadratic function of the parameters $\\mathbf{w}$ and $b$. It is convex and has a unique global minimum. Therefore, a unique, stable, and meaningful solution exists.\n- **Objective:** The problem is stated using precise mathematical notation and objective language.\n- **Other Flaws:** The problem is self-contained, with no missing or contradictory information. The data provided is consistent and simple. The problem is not trivial, requiring a standard derivation using multivariable calculus. It is not metaphorical, and it is verifiable.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will proceed with the solution.\n\n### Part 1: Derivation of the Optimal Parameters\n\nThe problem is to find the parameters $(\\mathbf{w}^{\\star}, b^{\\star})$ that minimize the objective function:\n$$\nJ(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}\n$$\nThis is an unconstrained optimization problem. The minimum of a convex differentiable function occurs where its gradient with respect to the parameters is the zero vector. A common way to solve this is using the normal equations, which can be derived by augmenting the input vectors.\n\nLet us define an augmented weight vector $\\hat{\\mathbf{w}} \\in \\mathbb{R}^{d+1}$ and an augmented input vector $\\hat{\\mathbf{x}}_{i} \\in \\mathbb{R}^{d+1}$ as:\n$$\n\\hat{\\mathbf{w}} = \\begin{pmatrix} \\mathbf{w} \\\\ b \\end{pmatrix}, \\quad \\hat{\\mathbf{x}}_{i} = \\begin{pmatrix} \\mathbf{x}_{i} \\\\ 1 \\end{pmatrix}\n$$\nWith these definitions, the neuron's output can be written as a single inner product:\n$$\nf(\\mathbf{x}_{i};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x}_{i} + b = \\hat{\\mathbf{w}}^{\\top}\\hat{\\mathbf{x}}_{i}\n$$\nLet $\\hat{X}$ be the $n \\times (d+1)$ design matrix where the $i$-th row is $\\hat{\\mathbf{x}}_{i}^{\\top}$, and let $\\mathbf{y}$ be the $n \\times 1$ column vector of labels $y_i$. The objective function can be written in matrix form as the squared Euclidean norm of the residual vector:\n$$\nJ(\\hat{\\mathbf{w}}) = \\|\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y}\\|_{2}^{2}\n$$\nTo find the minimum, we compute the gradient with respect to $\\hat{\\mathbf{w}}$ and set it to zero:\n$$\n\\nabla_{\\hat{\\mathbf{w}}}J(\\hat{\\mathbf{w}}) = \\nabla_{\\hat{\\mathbf{w}}} \\left( (\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y})^{\\top}(\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y}) \\right) = \\mathbf{0}\n$$\n$$\n\\nabla_{\\hat{\\mathbf{w}}} \\left( \\hat{\\mathbf{w}}^{\\top}\\hat{X}^{\\top}\\hat{X}\\hat{\\mathbf{w}} - 2\\mathbf{y}^{\\top}\\hat{X}\\hat{\\mathbf{w}} + \\mathbf{y}^{\\top}\\mathbf{y} \\right) = \\mathbf{0}\n$$\n$$\n2\\hat{X}^{\\top}\\hat{X}\\hat{\\mathbf{w}} - 2\\hat{X}^{\\top}\\mathbf{y} = \\mathbf{0}\n$$\nThis yields the normal equations:\n$$\n(\\hat{X}^{\\top}\\hat{X})\\hat{\\mathbf{w}} = \\hat{X}^{\\top}\\mathbf{y}\n$$\nAssuming the matrix $\\hat{X}^{\\top}\\hat{X}$ is invertible, the unique optimal parameter vector $\\hat{\\mathbf{w}}^{\\star}$ is given by:\n$$\n\\hat{\\mathbf{w}}^{\\star} = (\\hat{X}^{\\top}\\hat{X})^{-1}\\hat{X}^{\\top}\\mathbf{y}\n$$\nThe terms $\\hat{X}^{\\top}\\hat{X}$ and $\\hat{X}^{\\top}\\mathbf{y}$ can be expressed in terms of sums over the dataset:\n$$\n\\hat{X}^{\\top}\\hat{X} = \\sum_{i=1}^{n} \\hat{\\mathbf{x}}_{i}\\hat{\\mathbf{x}}_{i}^{\\top} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i} \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x}_{i}^{\\top}  1 \\end{pmatrix} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\mathbf{x}_{i} \\\\ \\mathbf{x}_{i}^{\\top}  1 \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\sum_{i=1}^{n} \\mathbf{x}_{i} \\\\ \\left(\\sum_{i=1}^{n} \\mathbf{x}_{i}\\right)^{\\top}  n \\end{pmatrix}\n$$\n$$\n\\hat{X}^{\\top}\\mathbf{y} = \\sum_{i=1}^{n} \\hat{\\mathbf{x}}_{i}y_{i} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i}y_{i} \\\\ y_{i} \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} y_{i}\\mathbf{x}_{i} \\\\ \\sum_{i=1}^{n} y_{i} \\end{pmatrix}\n$$\nThe optimal parameters $(\\mathbf{w}^{\\star}, b^{\\star})$ are found by solving the linear system defined by these aggregate matrices and vectors.\n\n### Part 2: Application to the Specific Dataset\n\nThe dataset is given as:\n- $\\mathbf{x}_{1} = (2, 0)^{\\top}$, $y_{1} = +1$\n- $\\mathbf{x}_{2} = (0, 2)^{\\top}$, $y_{2} = +1$\n- $\\mathbf{x}_{3} = (-2, 0)^{\\top}$, $y_{3} = -1$\n- $\\mathbf{x}_{4} = (0, -2)^{\\top}$, $y_{4} = -1$\n\nHere, $n=4$ and $d=2$. We compute the necessary sums:\n$$\nn = 4\n$$\n$$\n\\sum_{i=1}^{4} \\mathbf{x}_{i} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\sum_{i=1}^{4} y_{i} = 1 + 1 - 1 - 1 = 0\n$$\n$$\n\\sum_{i=1}^{4} y_{i}\\mathbf{x}_{i} = (1)\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + (1)\\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + (-1)\\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + (-1)\\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix}\n$$\n$$\n\\sum_{i=1}^{4} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top} = \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} + \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} = \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix}\n$$\nNow, we construct the system $(\\hat{X}^{\\top}\\hat{X})\\hat{\\mathbf{w}} = \\hat{X}^{\\top}\\mathbf{y}$:\n$$\n\\begin{pmatrix} \\sum \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\sum \\mathbf{x}_{i} \\\\ (\\sum \\mathbf{x}_{i})^{\\top}  n \\end{pmatrix} \\begin{pmatrix} \\mathbf{w} \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\sum y_{i}\\mathbf{x}_{i} \\\\ \\sum y_{i} \\end{pmatrix}\n$$\nSubstituting the computed values:\n$$\n\\begin{pmatrix} 8  0  0 \\\\ 0  8  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} w_{1} \\\\ w_{2} \\\\ b \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\end{pmatrix}\n$$\nThis corresponds to a simple system of three linear equations:\n1. $8w_{1} = 4 \\implies w_{1}^{\\star} = \\frac{4}{8} = 0.5$\n2. $8w_{2} = 4 \\implies w_{2}^{\\star} = \\frac{4}{8} = 0.5$\n3. $4b = 0 \\implies b^{\\star} = 0$\n\nThe optimal parameters are $\\mathbf{w}^{\\star} = (0.5, 0.5)^{\\top}$ and $b^{\\star} = 0$.\n\n### Part 3: Numerical Answer and Qualitative Discussion\n\nThe problem asks for the final answer as a row vector $[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}]$, with each component rounded to four significant figures.\n- $w_{1}^{\\star} = 0.5 \\implies 0.5000$\n- $w_{2}^{\\star} = 0.5 \\implies 0.5000$\n- $b^{\\star} = 0 \\implies 0.0000$\n\nThe final numerical answer vector is $[0.5000\\;\\;0.5000\\;\\;0.0000]$.\n\n**Qualitative Discussion:**\nThe decision boundary for the single neuron is the set of points $\\mathbf{x}$ where the output is zero: $\\mathbf{w}^{\\star\\top}\\mathbf{x} + b^{\\star} = 0$. The squared-error minimizer finds this boundary by performing linear regression on the class labels $y_i \\in \\{-1, +1\\}$. In contrast, a hard-margin Support Vector Machine (SVM) finds the separating hyperplane that maximizes the geometric margin between the two classes. The SVM solution depends only on the data points closest to the boundary (the support vectors), whereas the least-squares solution is influenced by every data point in the training set.\n\nThe two decision boundaries will coincide only under specific, highly symmetric conditions. These conditions ensure that the \"center of mass\" logic of the least-squares regression aligns with the \"maximum margin\" logic of the SVM. The key conditions are:\n\n1.  **Balanced Classes:** The number of data points in each class must be equal ($n_+ = n_-$). This ensures that the mean of the labels is zero ($\\bar{y}=0$) and that the overall data centroid $\\bar{\\mathbf{x}} = (\\boldsymbol{\\mu}_+ + \\boldsymbol{\\mu}_-)/2$ lies at the geometric midpoint between the class centroids $\\boldsymbol{\\mu}_+$ and $\\boldsymbol{\\mu}_-$. Under this condition, the least-squares decision boundary will pass through this midpoint, as does the SVM boundary.\n\n2.  **Symmetric Class Distributions:** The distribution of data points within each class must be symmetric in a way that does not skew the least-squares solution away from the max-margin one. The ideal case is when the class-conditional distributions, $p(\\mathbf{x}|y=+1)$ and $p(\\mathbf{x}|y=-1)$, have identical, spherical covariance matrices (e.g., proportional to the identity matrix, $cI$). In this scenario, the data points in each class are distributed isotropically around their respective means. The influence of points far from the boundary \"averages out\" perfectly, and the direction of the weight vector $\\mathbf{w}^\\star$ determined by least-squares (which is related to Fisher's Linear Discriminant Analysis) becomes parallel to the vector connecting the class means, $\\boldsymbol{\\mu}_+ - \\boldsymbol{\\mu}_-$. This is the same direction as the SVM's weight vector $\\mathbf{w}_{\\text{SVM}}$ under such symmetry.\n\nIf these conditions are not met, the solutions will typically diverge. For instance, if one class has outliers or is more spread out than the other, these points will \"pull\" the least-squares regression line towards them, shifting it away from the optimal large-margin position that an SVM would find. The provided dataset is an example where these symmetries hold, which is why the resulting boundary, $0.5x_1 + 0.5x_2 = 0$ (or $x_1+x_2=0$), is indeed the max-margin separating hyperplane.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5000  0.5000  0.0000 \\end{pmatrix}}\n$$", "id": "3180363"}, {"introduction": "To complete our journey, we turn to the workhorse of modern neural networks: the Rectified Linear Unit (ReLU). This exercise [@problem_id:3180428] moves from analyzing specific inputs to a statistical perspective, asking you to determine the probability that a ReLU neuron will be active given a distribution of random inputs. Mastering this concept is crucial for grasping the dynamics of signal propagation and the importance of initialization strategies in deep networks.", "problem": "Consider a single artificial neuron with preactivation $a=\\mathbf{w}^{\\top}\\mathbf{x}+b$ and Rectified Linear Unit (ReLU) activation $f(a)=\\max\\{0,a\\}$. Let the input be $\\mathbf{x}\\in\\mathbb{R}^{d}$ drawn from a zero-mean isotropic Gaussian, $\\mathbf{x}\\sim\\mathcal{N}(0,I_{d})$, and let the weight vector $\\mathbf{w}\\in\\mathbb{R}^{d}$ be fixed with $\\|\\mathbf{w}\\|_{2}>0$. The neuron is said to be active if $a>0$. Starting from the core definitions of the multivariate normal distribution and the properties of linear functionals of Gaussian random vectors, derive a closed-form expression for the probability $\\mathbb{P}(\\mathbf{w}^{\\top}\\mathbf{x}+b>0)$ in terms of $b$ and $\\|\\mathbf{w}\\|_{2}$. Then, to analyze sensitivity when the bias scales with the weight magnitude, set $b=\\alpha\\|\\mathbf{w}\\|_{2}$ for a scalar $\\alpha\\in\\mathbb{R}$, express $\\mathbb{P}(\\mathbf{w}^{\\top}\\mathbf{x}+b>0)$ as a function of $\\alpha$, and compute the derivative of this probability with respect to $\\alpha$ at $\\alpha=0$. You may denote by $\\Phi$ the cumulative distribution function of the standard normal distribution and by $\\phi$ its probability density function. Express your final answer as the exact value of the derivative at $\\alpha=0$. No rounding is required.", "solution": "The problem statement is critically validated before attempting a solution.\n\n### Step 1: Extract Givens\n- Neuron preactivation: $a = \\mathbf{w}^{\\top}\\mathbf{x} + b$\n- Activation function: Rectified Linear Unit (ReLU), $f(a) = \\max\\{0, a\\}$\n- Input vector: $\\mathbf{x} \\in \\mathbb{R}^{d}$, drawn from a zero-mean isotropic Gaussian distribution, $\\mathbf{x} \\sim \\mathcal{N}(0, I_d)$\n- Weight vector: $\\mathbf{w} \\in \\mathbb{R}^{d}$, fixed, with norm $\\|\\mathbf{w}\\|_2 > 0$\n- Bias term: $b \\in \\mathbb{R}$\n- Neuron active condition: $a > 0$, which is equivalent to $\\mathbf{w}^{\\top}\\mathbf{x} + b > 0$\n- Task Part 1: Derive a closed-form expression for the probability $\\mathbb{P}(\\mathbf{w}^{\\top}\\mathbf{x} + b > 0)$ in terms of $b$ and $\\|\\mathbf{w}\\|_2$.\n- Task Part 2: Set $b = \\alpha \\|\\mathbf{w}\\|_2$ for a scalar $\\alpha \\in \\mathbb{R}$ and express the probability as a function of $\\alpha$.\n- Task Part 3: Compute the derivative of this probability with respect to $\\alpha$ at $\\alpha = 0$.\n- Notation: $\\Phi$ denotes the cumulative distribution function (CDF) of the standard normal distribution, and $\\phi$ denotes its probability density function (PDF).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded:** The problem is firmly rooted in probability theory and its application to the analysis of artificial neural networks. The concepts of Gaussian random vectors, linear transformations of random variables, ReLU activation, and the statistical properties of neuron activation are standard and fundamental in machine learning theory.\n- **Well-Posed:** The problem is clearly specified. It provides all necessary information: the distributions of the inputs, the definitions of the neuron's components, and a clear objective. The condition $\\|\\mathbf{w}\\|_2 > 0$ is crucial and correctly included to prevent division by zero, ensuring a well-defined solution. A unique, stable, and meaningful solution can be derived from the premises.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in the statistical analysis of neural networks. The solution process may now commence.\n\n### Solution Derivation\nThe objective is to compute the probability that the neuron is active, which is given by $\\mathbb{P}(a > 0)$. The preactivation is $a = \\mathbf{w}^{\\top}\\mathbf{x} + b$, so we need to calculate $\\mathbb{P}(\\mathbf{w}^{\\top}\\mathbf{x} + b > 0)$.\n\nLet the random variable $Y$ be defined as the linear projection of the input vector $\\mathbf{x}$ onto the weight vector $\\mathbf{w}$:\n$$Y = \\mathbf{w}^{\\top}\\mathbf{x} = \\sum_{i=1}^{d} w_i x_i$$\nThe input vector $\\mathbf{x}$ is drawn from a multivariate normal distribution $\\mathbf{x} \\sim \\mathcal{N}(0, I_d)$, where $0$ is the zero vector and $I_d$ is the $d \\times d$ identity matrix. This means the components $x_i$ are independent and identically distributed standard normal random variables, $x_i \\sim \\mathcal{N}(0, 1)$.\n\nA linear combination of Gaussian random variables is itself a Gaussian random variable. To characterize the distribution of $Y$, we must find its mean $\\mathbb{E}[Y]$ and variance $\\text{Var}(Y)$.\n\nThe mean of $Y$ is:\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\mathbf{w}^{\\top}\\mathbf{x}] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}]$$\nSince $\\mathbb{E}[\\mathbf{x}] = 0$, the mean is:\n$$\\mathbb{E}[Y] = \\mathbf{w}^{\\top}0 = 0$$\n\nThe variance of $Y$ is given by the general formula $\\text{Var}(\\mathbf{w}^{\\top}\\mathbf{x}) = \\mathbf{w}^{\\top}\\text{Cov}(\\mathbf{x})\\mathbf{w}$. The covariance matrix of $\\mathbf{x}$ is given as $\\text{Cov}(\\mathbf{x}) = I_d$. Therefore:\n$$\\text{Var}(Y) = \\mathbf{w}^{\\top}I_d \\mathbf{w} = \\mathbf{w}^{\\top}\\mathbf{w} = \\sum_{i=1}^{d} w_i^2 = \\|\\mathbf{w}\\|_2^2$$\nThus, the random variable $Y$ follows a normal distribution with mean $0$ and variance $\\|\\mathbf{w}\\|_2^2$. We write this as $Y \\sim \\mathcal{N}(0, \\|\\mathbf{w}\\|_2^2)$.\n\nNow, we can compute the probability of activation:\n$$\\mathbb{P}(a > 0) = \\mathbb{P}(\\mathbf{w}^{\\top}\\mathbf{x} + b > 0) = \\mathbb{P}(Y > -b)$$\nTo compute this probability using the standard normal distribution, we standardize the random variable $Y$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. The standardization of $Y$ is:\n$$Z = \\frac{Y - \\mathbb{E}[Y]}{\\sqrt{\\text{Var}(Y)}} = \\frac{\\mathbf{w}^{\\top}\\mathbf{x} - 0}{\\sqrt{\\|\\mathbf{w}\\|_2^2}} = \\frac{\\mathbf{w}^{\\top}\\mathbf{x}}{\\|\\mathbf{w}\\|_2}$$\nThe inequality $Y > -b$ can be rewritten in terms of $Z$:\n$$\\frac{Y}{\\|\\mathbf{w}\\|_2} > \\frac{-b}{\\|\\mathbf{w}\\|_2} \\implies Z > -\\frac{b}{\\|\\mathbf{w}\\|_2}$$\nThe probability is then $\\mathbb{P}(Z > -\\frac{b}{\\|\\mathbf{w}\\|_2})$. By definition of the standard normal CDF $\\Phi(z) = \\mathbb{P}(Z \\le z)$, we have $\\mathbb{P}(Z > z) = 1 - \\Phi(z)$. Also, due to the symmetry of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$. This implies that $\\mathbb{P}(Z > z) = \\Phi(-z)$.\n\nApplying this property with $z = -\\frac{b}{\\|\\mathbf{w}\\|_2}$:\n$$\\mathbb{P}(Z > -\\frac{b}{\\|\\mathbf{w}\\|_2}) = \\Phi\\left(-\\left(-\\frac{b}{\\|\\mathbf{w}\\|_2}\\right)\\right) = \\Phi\\left(\\frac{b}{\\|\\mathbf{w}\\|_2}\\right)$$\nThis provides the closed-form expression for the probability in terms of $b$ and $\\|\\mathbf{w}\\|_2$.\n\nNext, we analyze the sensitivity of this probability to the bias, under the condition that the bias scales with the weight magnitude. We set $b = \\alpha \\|\\mathbf{w}\\|_2$ for some scalar $\\alpha \\in \\mathbb{R}$.\nLet $P(\\alpha)$ be the probability of activation as a function of $\\alpha$:\n$$P(\\alpha) = \\mathbb{P}(\\mathbf{w}^{\\top}\\mathbf{x} + \\alpha\\|\\mathbf{w}\\|_2 > 0) = \\Phi\\left(\\frac{\\alpha \\|\\mathbf{w}\\|_2}{\\|\\mathbf{w}\\|_2}\\right) = \\Phi(\\alpha)$$\nThe problem asks for the derivative of this probability with respect to $\\alpha$, evaluated at $\\alpha = 0$. We first find the derivative $\\frac{d}{d\\alpha}P(\\alpha)$.\nBy the Fundamental Theorem of Calculus, the derivative of the CDF of a continuous random variable is its PDF. The PDF of the standard normal distribution is denoted by $\\phi$.\n$$\\frac{d}{d\\alpha}P(\\alpha) = \\frac{d}{d\\alpha}\\Phi(\\alpha) = \\phi(\\alpha)$$\nThe PDF of the standard normal distribution is given by the formula:\n$$\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$$\nWe need to evaluate this derivative at $\\alpha = 0$:\n$$\\frac{d P}{d\\alpha}\\bigg|_{\\alpha=0} = \\phi(0)$$\nSubstituting $\\alpha=0$ into the PDF formula:\n$$\\phi(0) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{0^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(0) = \\frac{1}{\\sqrt{2\\pi}} \\cdot 1 = \\frac{1}{\\sqrt{2\\pi}}$$\nThis is the final exact value for the derivative.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi}}}$$", "id": "3180428"}]}