{"hands_on_practices": [{"introduction": "Understanding the Hessian matrix begins with the fundamentals of its construction. This exercise provides hands-on practice in calculating the Hessian for a two-variable function, a foundational skill for analyzing the local geometry of any mathematical surface. By working through a simplified cost function from robotics [@problem_id:2215313], you will compute the second-order partial derivatives and assemble them into the Hessian matrix, preparing you to use it for more advanced analyses like stability and optimization.", "problem": "In robotics, a common task is to find a stable configuration for a multi-jointed arm by minimizing a cost function. Consider a simplified two-dimensional system where the configuration is described by coordinates $(x, y)$. The cost function, which represents a combination of energy expenditure and target proximity, is given by:\n$$C(x, y) = \\ln(1 + \\alpha x^2 + \\beta y^2)$$\nHere, $\\alpha$ and $\\beta$ are positive constants representing the stiffness of the control system in the $x$ and $y$ directions, respectively. An equilibrium configuration is found at a critical point of the cost function. It has been determined that $(x, y) = (0, 0)$ is a critical point for this system.\n\nTo analyze the stability of this equilibrium, one must examine the Hessian matrix of the cost function at this point. For a system with $\\alpha = 1$ and $\\beta = 2$, calculate the determinant of the Hessian matrix of the cost function $C(x, y)$ evaluated at the critical point $(0, 0)$.", "solution": "Let $u(x,y)=1+\\alpha x^{2}+\\beta y^{2}$. Then $C(x,y)=\\ln(u)$. Using $\\partial_{x}C=\\frac{u_{x}}{u}$ and differentiating again, we have\n$$\nC_{xx}=\\frac{u_{xx}u-u_{x}^{2}}{u^{2}}, \\quad C_{yy}=\\frac{u_{yy}u-u_{y}^{2}}{u^{2}}, \\quad C_{xy}=\\frac{u_{xy}u-u_{x}u_{y}}{u^{2}}.\n$$\nCompute the derivatives of $u$:\n$$\nu_{x}=2\\alpha x, \\quad u_{xx}=2\\alpha, \\quad u_{y}=2\\beta y, \\quad u_{yy}=2\\beta, \\quad u_{xy}=0.\n$$\nEvaluate at the critical point $(0,0)$ where $u(0,0)=1$, $u_{x}(0,0)=0$, $u_{y}(0,0)=0$:\n$$\nC_{xx}(0,0)=\\frac{(2\\alpha)\\cdot 1-0}{1^{2}}=2\\alpha, \\quad C_{yy}(0,0)=2\\beta, \\quad C_{xy}(0,0)=\\frac{0-0}{1^{2}}=0.\n$$\nThus the Hessian at $(0,0)$ is\n$$\nH=\\begin{pmatrix}2\\alpha  0 \\\\ 0  2\\beta\\end{pmatrix}.\n$$\nThe determinant of the Hessian is\n$$\n\\det(H)=C_{xx}C_{yy}-C_{xy}^{2}=(2\\alpha)(2\\beta)-0=4\\alpha\\beta.\n$$\nFor $\\alpha=1$ and $\\beta=2$,\n$$\n\\det(H)=4\\cdot 1 \\cdot 2=8.\n$$", "answer": "$$\\boxed{8}$$", "id": "2215313"}, {"introduction": "In deep learning, the Hessian matrix for a network with millions of parameters is too large to compute or store explicitly. This challenge is overcome by focusing on the Hessian's *action* on a vector, known as the Hessian-vector product ($H\\mathbf{v}$). This practice [@problem_id:3186600] guides you through implementing Pearlmutter's method, a clever application of automatic differentiation that computes $H\\mathbf{v}$ without forming $H$. Mastering this technique is crucial for applying second-order optimization methods and analyzing the loss landscape of large-scale neural networks.", "problem": "You are to derive, implement, and validate the Hessian–vector product for a scalar loss function in a small feedforward neural network, using principles of automatic differentiation. Consider a single-hidden-layer Multi-Layer Perceptron (MLP) with hyperbolic tangent activation mapping inputs in $\\mathbb{R}^d$ to a scalar output. The model parameters are collected into a single vector $\\theta \\in \\mathbb{R}^p$ formed by concatenating the weights and biases from both layers. The network is defined by\n$$\n\\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1,\\quad\n\\mathbf{h}_1 = \\tanh(\\mathbf{a}_1),\\quad\n\\hat{y} = \\mathbf{W}_2^\\top \\mathbf{h}_1 + b_2,\n$$\nwhere $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times d}$, $\\mathbf{b}_1 \\in \\mathbb{R}^m$, $\\mathbf{W}_2 \\in \\mathbb{R}^m$, and $b_2 \\in \\mathbb{R}$ are the parameters, $\\mathbf{x} \\in \\mathbb{R}^d$ is the input, and $\\hat{y} \\in \\mathbb{R}$ is the predicted scalar output. For a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, the empirical loss is\n$$\nf(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2.\n$$\nLet $\\nabla f(\\theta)$ denote the gradient of $f$ with respect to $\\theta$, and let $\\mathbf{H}(\\theta)$ denote the Hessian matrix of second derivatives. The Hessian–vector product is $\\mathbf{H}(\\theta)\\mathbf{v}$ for a given direction vector $\\mathbf{v}\\in\\mathbb{R}^p$.\n\nStarting from fundamental definitions and rules of differentiation, derive Pearlmutter’s method for computing the Hessian–vector product via automatic differentiation. Specifically, use the definition of the Hessian–vector product as the directional derivative of the gradient and apply the chain rule to construct a forward directional-derivative pass and a backward pass that yields $\\mathbf{H}(\\theta)\\mathbf{v}$ without explicitly forming $\\mathbf{H}(\\theta)$.\n\nImplementation requirements:\n- Use $d=3$, $m=4$, and $n=5$. Construct the dataset deterministically as follows: draw inputs $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ and targets $\\mathbf{y}\\in\\mathbb{R}^n$ from a standard normal distribution with a fixed seed $42$. Initialize parameters $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, and $b_2$ by drawing from a standard normal distribution with seed $0$ and scaling by $0.1$. The activation function is the hyperbolic tangent $\\tanh$, which is twice differentiable.\n- Implement two functions:\n  1. A function that returns $\\mathbf{H}(\\theta)\\mathbf{v}$ using Pearlmutter’s method with automatic differentiation principles (directional derivatives propagated through the network and reverse-mode accumulation).\n  2. A function that returns a finite-difference approximation of $\\mathbf{H}(\\theta)\\mathbf{v}$ using\n     $$\n     \\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon},\n     $$\n     for a given $\\epsilon  0$, by computing gradients at $\\theta$ and $\\theta+\\epsilon\\mathbf{v}$ via standard backpropagation.\n- Use the derivative $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$ and the second derivative $\\frac{d^2}{dx^2}\\tanh(x) = -2\\,\\tanh(x)\\,\\big(1 - \\tanh^2(x)\\big)$ in your derivation and implementation.\n\nTest suite:\n- For all tests, let $\\mathbf{v}\\in\\mathbb{R}^p$ be a specified direction and $\\epsilon$ be the finite-difference step. Use the following cases:\n  1. A random unit-norm direction with $\\epsilon = 10^{-5}$ and tolerance $10^{-6}$.\n  2. The zero direction $\\mathbf{v}=\\mathbf{0}$ with $\\epsilon = 10^{-5}$ and tolerance $10^{-12}$.\n  3. A basis direction selecting the last parameter (corresponding to $b_2$), i.e., $\\mathbf{v} = \\mathbf{e}_{p}$, with $\\epsilon = 10^{-6}$ and tolerance $10^{-6}$.\n  4. The same random unit-norm direction as case 1 with $\\epsilon = 10^{-9}$ and tolerance $10^{-2}$.\n  5. The same random unit-norm direction as case 1 with $\\epsilon = 10^{-1}$ and tolerance $10^{-3}$.\n- For each test case, compute the relative error\n  $$\n  r = \\frac{\\|\\mathbf{H}(\\theta)\\mathbf{v} - \\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2}{\\max\\left(10^{-12}, \\|\\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2\\right)},\n  $$\n  where $\\text{FD}(\\theta,\\mathbf{v},\\epsilon)$ denotes the finite-difference approximation. The test result is a boolean indicating whether $r$ is less than or equal to the given tolerance.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True,False]\").\n- No physical units, angles, or percentages appear in this problem; all quantities are dimensionless real numbers.", "solution": "Pearlmutter's method computes the Hessian-vector product $\\mathbf{H}(\\theta)\\mathbf{v}$ by applying automatic differentiation to the gradient calculation itself. The key insight is that the Hessian-vector product is the directional derivative of the gradient, $\\nabla f(\\theta)$, in the direction of the vector $\\mathbf{v}$. Using the operator $D_{\\mathbf{v}}[\\cdot]$ to denote a directional derivative in direction $\\mathbf{v}$, we have:\n$$\n\\mathbf{H}(\\theta)\\mathbf{v} = D_{\\mathbf{v}}[\\nabla f(\\theta)] = \\frac{d}{d\\alpha} \\nabla f(\\theta + \\alpha\\mathbf{v}) \\bigg|_{\\alpha=0}\n$$\nThis allows us to compute $\\mathbf{H}(\\theta)\\mathbf{v}$ by propagating directional derivatives through the gradient computation graph. This involves two main phases. For clarity, we use batch notation and un-flatten the parameter vector $\\theta$ and direction vector $\\mathbf{v}$ into their component matrices and vectors (e.g., $\\mathbf{W}_1, \\mathbf{b}_1, \\dots$ and $\\mathbf{v}_{W1}, \\mathbf{v}_{b1}, \\dots$). Let $\\dot{z} = D_{\\mathbf{v}}[z]$ denote the directional derivative of a variable $z$.\n\n**Phase I: Directional Forward Pass (Forward-on-Forward)**\nFirst, we perform a standard forward pass to compute all intermediate activations, and simultaneously, we propagate the directional derivatives through the same graph.\n-   Standard forward pass:\n    -   $\\mathbf{A}_1 = \\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top$\n    -   $\\mathbf{H}_1 = \\tanh(\\mathbf{A}_1)$\n    -   $\\hat{\\mathbf{y}} = \\mathbf{H}_1 \\mathbf{W}_2 + b_2$\n-   Directional derivative forward pass:\n    -   $\\dot{\\mathbf{A}}_1 = D_{\\mathbf{v}}[\\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top] = \\mathbf{X} \\mathbf{v}_{W1}^\\top + \\mathbf{v}_{b1}^\\top$\n    -   $\\dot{\\mathbf{H}}_1 = D_{\\mathbf{v}}[\\tanh(\\mathbf{A}_1)] = \\tanh'(\\mathbf{A}_1) \\odot \\dot{\\mathbf{A}}_1 = (1 - \\mathbf{H}_1^2) \\odot \\dot{\\mathbf{A}}_1$\n    -   $\\dot{\\hat{\\mathbf{y}}} = D_{\\mathbf{v}}[\\mathbf{H}_1 \\mathbf{W}_2 + b_2] = \\dot{\\mathbf{H}}_1 \\mathbf{W}_2 + \\mathbf{H}_1 \\mathbf{v}_{W2} + v_{b2}$\n\n**Phase II: HVP Backward Pass (Forward-on-Reverse)**\nNext, we apply the directional derivative operator $D_{\\mathbf{v}}$ to the equations of the standard backpropagation algorithm. This requires applying the product rule of differentiation, $D_{\\mathbf{v}}[UV] = (\\dot{U})V + U(\\dot{V})$, to each step. Let $\\bar{z} = \\frac{\\partial f}{\\partial z}$ be the adjoint (gradient) of the loss with respect to a variable $z$. We want to compute $\\dot{\\bar{\\theta}} = D_{\\mathbf{v}}[\\bar{\\theta}]$.\n-   $\\bar{\\hat{\\mathbf{y}}} = \\frac{1}{n}(\\hat{\\mathbf{y}} - \\mathbf{y}) \\implies \\dot{\\bar{\\hat{\\mathbf{y}}}} = D_{\\mathbf{v}}[\\bar{\\hat{\\mathbf{y}}}] = \\frac{1}{n}\\dot{\\hat{\\mathbf{y}}}$\n-   $\\bar{b}_2 = \\sum_i \\bar{\\hat{y}}_i \\implies \\dot{\\bar{b}}_2 = \\sum_i \\dot{\\bar{\\hat{y}}}_i$\n-   $\\bar{\\mathbf{W}}_2 = \\mathbf{H}_1^\\top \\bar{\\hat{\\mathbf{y}}} \\implies \\dot{\\bar{\\mathbf{W}}}_2 = \\dot{\\mathbf{H}}_1^\\top \\bar{\\hat{\\mathbf{y}}} + \\mathbf{H}_1^\\top \\dot{\\bar{\\hat{\\mathbf{y}}}}$\n-   $\\bar{\\mathbf{H}}_1 = \\bar{\\hat{\\mathbf{y}}} \\mathbf{W}_2^\\top \\implies \\dot{\\bar{\\mathbf{H}}}_1 = \\dot{\\bar{\\hat{\\mathbf{y}}}} \\mathbf{W}_2^\\top + \\bar{\\hat{\\mathbf{y}}} \\mathbf{v}_{W2}^\\top$\n-   $\\bar{\\mathbf{A}}_1 = \\bar{\\mathbf{H}}_1 \\odot \\tanh'(\\mathbf{A}_1) \\implies \\dot{\\bar{\\mathbf{A}}}_1 = \\dot{\\bar{\\mathbf{H}}}_1 \\odot \\tanh'(\\mathbf{A}_1) + \\bar{\\mathbf{H}}_1 \\odot (\\tanh''(\\mathbf{A}_1) \\odot \\dot{\\mathbf{A}}_1)$\n-   $\\bar{\\mathbf{b}}_1 = \\sum_i (\\bar{\\mathbf{A}}_1)_{i,:} \\implies \\dot{\\bar{\\mathbf{b}}}_1 = \\sum_i (\\dot{\\bar{\\mathbf{A}}}_1)_{i,:}$\n-   $\\bar{\\mathbf{W}}_1 = \\bar{\\mathbf{A}}_1^\\top \\mathbf{X} \\implies \\dot{\\bar{\\mathbf{W}}}_1 = \\dot{\\bar{\\mathbf{A}}}_1^\\top \\mathbf{X}$\n\nThe concatenated results $(\\dot{\\bar{\\mathbf{W}}}_1, \\dot{\\bar{\\mathbf{b}}}_1, \\dot{\\bar{\\mathbf{W}}}_2, \\dot{\\bar{b}}_2)$ form the final Hessian-vector product $\\mathbf{H}(\\theta)\\mathbf{v}$. For validation, this result is compared against a finite-difference approximation, which requires two separate gradient computations.\n$$\n\\mathbf{H}(\\theta)\\mathbf{v} \\approx \\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon}\n$$\nThis finite-difference approach is simpler to implement but numerically less stable and computationally more expensive than Pearlmutter's method, which computes the exact product up to machine precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and validates the Hessian-vector product for a small MLP.\n    \"\"\"\n    # Define problem constants and initialize data/parameters\n    d, m, n = 3, 4, 5\n    p = d * m + m + m * 1 + 1\n\n    # Generate dataset\n    rng_data = np.random.default_rng(seed=42)\n    X = rng_data.standard_normal(size=(n, d))\n    y = rng_data.standard_normal(size=n)\n\n    # Initialize parameters\n    rng_params = np.random.default_rng(seed=0)\n    W1_init = rng_params.standard_normal(size=(m, d)) * 0.1\n    b1_init = rng_params.standard_normal(size=m) * 0.1\n    W2_init = rng_params.standard_normal(size=m) * 0.1\n    b2_init = rng_params.standard_normal() * 0.1\n\n    # --- Parameter Flattening and Unflattening Utilities ---\n    def _flatten_params(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            np.array([b2]).flatten()\n        ])\n\n    def _unflatten_params(theta):\n        idx = 0\n        W1 = theta[idx:idx + m * d].reshape(m, d)\n        idx += m * d\n        b1 = theta[idx:idx + m]\n        idx += m\n        W2 = theta[idx:idx + m]\n        idx += m\n        b2 = theta[idx]\n        return W1, b1, W2, b2\n\n    # --- Core Implementation: Gradient and HVP ---\n\n    def _compute_grad(theta, X, y):\n        \"\"\"Computes the gradient of the loss function via backpropagation.\"\"\"\n        W1, b1, W2, b2 = _unflatten_params(theta)\n        num_samples = X.shape[0]\n\n        # Forward pass\n        A1 = X @ W1.T + b1\n        H1 = np.tanh(A1)\n        y_hat = H1 @ W2 + b2\n\n        # Backward pass (gradient calculation)\n        y_hat_bar = (y_hat - y) / num_samples\n        \n        b2_bar = np.sum(y_hat_bar)\n        W2_bar = H1.T @ y_hat_bar\n        H1_bar = np.outer(y_hat_bar, W2)\n        \n        A1_bar = H1_bar * (1 - H1**2)\n        \n        b1_bar = np.sum(A1_bar, axis=0)\n        W1_bar = A1_bar.T @ X\n\n        return _flatten_params(W1_bar, b1_bar, W2_bar, b2_bar)\n\n    def _compute_hvp_pearlmutter(theta, v, X, y):\n        \"\"\"Computes the Hessian-vector product using Pearlmutter's method.\"\"\"\n        W1, b1, W2, b2 = _unflatten_params(theta)\n        v_W1, v_b1, v_W2, v_b2 = _unflatten_params(v)\n        num_samples = X.shape[0]\n\n        # --- Standard Forward Pass ---\n        A1 = X @ W1.T + b1\n        H1 = np.tanh(A1)\n        y_hat = H1 @ W2 + b2\n\n        # --- Phase I: Directional Forward Pass (Forward-on-Forward) ---\n        dot_A1 = X @ v_W1.T + v_b1\n        dot_H1 = (1 - H1**2) * dot_A1\n        dot_y_hat = np.sum(dot_H1 * W2, axis=1) + H1 @ v_W2 + v_b2\n\n        # --- Phase II: HVP Backward Pass (Forward-on-Reverse) ---\n        # Gradients from standard backprop (needed for the second-order terms)\n        y_hat_bar = (y_hat - y) / num_samples\n        H1_bar = np.outer(y_hat_bar, W2)\n\n        # Directional derivatives of gradients\n        dot_y_hat_bar = dot_y_hat / num_samples\n        \n        dot_b2_bar = np.sum(dot_y_hat_bar)\n        dot_W2_bar = dot_H1.T @ y_hat_bar + H1.T @ dot_y_hat_bar\n        \n        dot_H1_bar = np.outer(dot_y_hat_bar, W2) + np.outer(y_hat_bar, v_W2)\n        \n        tanh_prime_A1 = 1 - H1**2\n        tanh_second_A1 = -2 * H1 * tanh_prime_A1\n        \n        dot_A1_bar = dot_H1_bar * tanh_prime_A1 + H1_bar * tanh_second_A1 * dot_A1\n\n        dot_b1_bar = np.sum(dot_A1_bar, axis=0)\n        dot_W1_bar = dot_A1_bar.T @ X\n\n        return _flatten_params(dot_W1_bar, dot_b1_bar, dot_W2_bar, dot_b2_bar)\n\n    def _compute_hvp_finite_diff(theta, v, X, y, epsilon):\n        \"\"\"Approximates the Hessian-vector product using finite differences.\"\"\"\n        grad_pos = _compute_grad(theta + epsilon * v, X, y)\n        grad_neg = _compute_grad(theta, X, y)\n        return (grad_pos - grad_neg) / epsilon\n\n    # --- Test Suite ---\n    theta_init = _flatten_params(W1_init, b1_init, W2_init, b2_init)\n    \n    # Generate a reusable random unit-norm direction for cases 1, 4, 5\n    rng_v = np.random.default_rng(seed=123)\n    v_rand_raw = rng_v.standard_normal(size=p)\n    v_rand_unit = v_rand_raw / np.linalg.norm(v_rand_raw)\n\n    test_cases = [\n        # (name, direction_vector, epsilon, tolerance)\n        (\"random_unit_v_e-5\", v_rand_unit, 1e-5, 1e-6),\n        (\"zero_v\", np.zeros(p), 1e-5, 1e-12),\n        (\"basis_v\", np.eye(1, p, p - 1).flatten(), 1e-6, 1e-6),\n        (\"random_unit_v_e-9\", v_rand_unit, 1e-9, 1e-2),\n        (\"random_unit_v_e-1\", v_rand_unit, 1e-1, 1e-3),\n    ]\n\n    results = []\n    for name, v, epsilon, tolerance in test_cases:\n        # Calculate HVP using both methods\n        hvp_ad = _compute_hvp_pearlmutter(theta_init, v, X, y)\n        hvp_fd = _compute_hvp_finite_diff(theta_init, v, X, y, epsilon)\n\n        # Compute relative error\n        norm_fd = np.linalg.norm(hvp_fd)\n        # The denominator is max(1e-12, norm_fd) to avoid division by zero\n        # when hvp_fd is exactly zero (as in the v=0 case).\n        denominator = max(1e-12, norm_fd)\n        \n        rel_error = np.linalg.norm(hvp_ad - hvp_fd) / denominator\n        \n        results.append(rel_error = tolerance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186600"}, {"introduction": "Once we can efficiently compute Hessian-vector products, we can unlock powerful insights into the geometry of the loss landscape. This exercise [@problem_id:3186508] demonstrates how to use the power iteration algorithm—which relies solely on repeated Hessian-vector products—to find the largest eigenvalue of the Hessian. This value reveals the direction of maximum curvature, a critical piece of information for understanding optimization dynamics, training stability, and the sharpness of minima in deep learning models.", "problem": "Consider a differentiable scalar loss function $L(\\mathbf{w})$ used in deep learning, where $\\mathbf{w} \\in \\mathbb{R}^d$ denotes the parameter vector. The Hessian matrix $H(\\mathbf{w})$ at a point $\\mathbf{w}$ is defined as the square matrix of second-order partial derivatives, with entries $H_{ij}(\\mathbf{w}) = \\frac{\\partial^2 L(\\mathbf{w})}{\\partial w_i \\partial w_j}$. For functions with continuous second derivatives, $H(\\mathbf{w})$ is symmetric. The second-order Taylor expansion of $L$ around $\\mathbf{w}$ in a direction $\\mathbf{v} \\in \\mathbb{R}^d$ is given by $L(\\mathbf{w} + \\mathbf{v}) \\approx L(\\mathbf{w}) + \\nabla L(\\mathbf{w})^\\top \\mathbf{v} + \\frac{1}{2} \\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$. The quantity $\\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$ characterizes local curvature along $\\mathbf{v}$, and its extremal values under the constraint $\\mathbf{v}^\\top \\mathbf{v} = 1$ are attained at the eigenvectors of $H(\\mathbf{w})$. The associated extremal values are the eigenvalues, and the largest algebraic eigenvalue approximates the maximum local curvature direction at $\\mathbf{w}$.\n\nYour task is to implement a program that approximates the largest algebraic eigenvalue of a given real symmetric matrix $H$ (interpreted as a Hessian) using only repeated Hessian-vector products and normalization, together with the Rayleigh quotient. Do not form or use any closed-form eigen-decomposition. At each iteration, use the current iterate to compute the next direction by applying the matrix $H$ and normalizing the result, then evaluate the Rayleigh quotient to update the eigenvalue estimate. Iterate until the absolute change in the Rayleigh quotient is below a specified tolerance or a maximum iteration cap is reached. If at any step the Hessian-vector product is the zero vector (i.e., its Euclidean norm is zero or below a machine-safe threshold), terminate and return $0$ as the eigenvalue estimate.\n\nUse the following fixed parameters for all test cases:\n- Initial vector $\\mathbf{v}_0$ equal to the all-ones vector in the appropriate dimension, normalized to unit Euclidean norm.\n- Tolerance $10^{-10}$ on the absolute change of the Rayleigh quotient between successive iterations.\n- Maximum number of iterations $1000$.\n- Output each estimated largest algebraic eigenvalue as a float rounded to six decimal places.\n\nTest suite (each $H$ below is symmetric and should be treated as a Hessian at some parameter point):\n1. Happy path, symmetric positive definite $3 \\times 3$:\n   $$H_1 = \\begin{bmatrix}\n   4  1  0 \\\\\n   1  3  0 \\\\\n   0  0  2\n   \\end{bmatrix}.$$\n2. Indefinite $3 \\times 3$ with a dominant positive curvature:\n   $$H_2 = \\begin{bmatrix}\n   2  0.5  0 \\\\\n   0.5  -0.5  0 \\\\\n   0  0  0.2\n   \\end{bmatrix}.$$\n3. Degenerate top curvature (repeated largest eigenvalue) $3 \\times 3$:\n   $$H_3 = \\begin{bmatrix}\n   3  0  0 \\\\\n   0  3  0 \\\\\n   0  0  1\n   \\end{bmatrix}.$$\n4. Boundary case (no curvature) $3 \\times 3$:\n   $$H_4 = \\begin{bmatrix}\n   0  0  0 \\\\\n   0  0  0 \\\\\n   0  0  0\n   \\end{bmatrix}.$$\n5. Ill-conditioned symmetric positive definite $5 \\times 5$:\n   $$H_5 = \\begin{bmatrix}\n   10^{-6}  10^{-5}  0  0  0 \\\\\n   10^{-5}  10^{-2}  10^{-4}  0  0 \\\\\n   0  10^{-4}  10^{-1}  10^{-3}  0 \\\\\n   0  0  10^{-3}  1  10^{-2} \\\\\n   0  0  0  10^{-2}  10\n   \\end{bmatrix}.$$\n\nProgram requirements:\n- For each test matrix $H$, approximate the largest algebraic eigenvalue using the iterative procedure described above.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite $[H_1,H_2,H_3,H_4,H_5]$. Each entry must be the float rounded to six decimal places, with no additional text. For example, an output must look like $[x_1,x_2,x_3,x_4,x_5]$ where each $x_i$ is the rounded float for matrix $H_i$.", "solution": "The problem requires the implementation of an iterative algorithm to find the largest algebraic eigenvalue of a given real symmetric matrix $H$. The physical context provided is the Hessian matrix $H(\\mathbf{w})$ of a loss function $L(\\mathbf{w})$ in deep learning, where the largest eigenvalue corresponds to the direction of maximum local curvature. The specified algorithm is the Power Iteration method, coupled with the Rayleigh quotient for estimating the eigenvalue.\n\nThe method is based on the principle that for a diagonalizable matrix $H$, repeatedly applying it to an arbitrary non-zero vector $\\mathbf{v}_0$ will progressively align the resulting vector with the eigenvector corresponding to the eigenvalue of largest magnitude, denoted $\\lambda_{dom}$. Let the eigenvalues of $H$ be $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_d|$, with corresponding eigenvectors $\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_d$. An initial vector $\\mathbf{v}_0$ can be expressed as a linear combination of these eigenvectors: $\\mathbf{v}_0 = c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_d\\mathbf{u}_d$. Assuming $c_1 \\neq 0$, applying $H$ repeatedly gives:\n$$H^k\\mathbf{v}_0 = c_1\\lambda_1^k\\mathbf{u}_1 + c_2\\lambda_2^k\\mathbf{u}_2 + \\dots + c_d\\lambda_d^k\\mathbf{u}_d = \\lambda_1^k \\left( c_1\\mathbf{u}_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\\mathbf{u}_2 + \\dots + c_d\\left(\\frac{\\lambda_d}{\\lambda_1}\\right)^k\\mathbf{u}_d \\right)$$\nIf $|\\lambda_1| > |\\lambda_2|$ (a unique dominant eigenvalue in magnitude), the terms $(\\frac{\\lambda_i}{\\lambda_1})^k$ for $i > 1$ approach $0$ as $k \\to \\infty$. Thus, the vector $H^k\\mathbf{v}_0$ becomes increasingly parallel to the dominant eigenvector $\\mathbf{u}_1$. To prevent the magnitude of $H^k\\mathbf{v}_0$ from diverging or vanishing, the vector is normalized at each step. This leads to the iterative update rule for the vector:\n$$\\mathbf{v}_{k+1} = \\frac{H\\mathbf{v}_k}{\\|H\\mathbf{v}_k\\|_2}$$\nwhere $\\mathbf{v}_k$ is the normalized vector estimate at iteration $k$.\n\nOnce an estimate for the eigenvector $\\mathbf{v}_k$ is available, the corresponding eigenvalue can be estimated using the Rayleigh quotient, defined for a non-zero vector $\\mathbf{v}$ as:\n$$R_H(\\mathbf{v}) = \\frac{\\mathbf{v}^\\top H \\mathbf{v}}{\\mathbf{v}^\\top \\mathbf{v}}$$\nIf $\\mathbf{v}$ is an exact eigenvector, then $H\\mathbf{v} = \\lambda\\mathbf{v}$, and the Rayleigh quotient yields the exact eigenvalue $\\lambda$. Since our iterates $\\mathbf{v}_k$ are normalized such that $\\mathbf{v}_k^\\top \\mathbf{v}_k = 1$, the expression simplifies to $R_H(\\mathbf{v}_k) = \\mathbf{v}_k^\\top H \\mathbf{v}_k$. As $\\mathbf{v}_k$ converges to the dominant eigenvector $\\mathbf{u}_1$, the Rayleigh quotient $R_H(\\mathbf{v}_k)$ converges to the dominant eigenvalue $\\lambda_1$. For all the test cases provided, the largest algebraic eigenvalue is positive and also has the largest magnitude, so this method will correctly converge to the desired value.\n\nThe algorithm to be implemented, following the problem's specifications, is as follows:\n\n$1$. **Initialization**:\n   - Given a matrix $H \\in \\mathbb{R}^{d \\times d}$.\n   - The initial vector $\\mathbf{v}_0$ is the all-ones vector of dimension $d$, normalized to have a Euclidean norm of $1$: $\\mathbf{v}_0 = \\frac{1}{\\sqrt{d}}[1, 1, \\dots, 1]^\\top$.\n   - The convergence tolerance is set to $\\epsilon = 10^{-10}$.\n   - The maximum number of iterations is $N_{max} = 1000$.\n   - Initialize eigenvalue estimates, for instance $\\lambda_{current} = 0$ and $\\lambda_{previous}$ to a value like $\\infty$ to ensure the first iteration's convergence check does not pass.\n\n$2$. **Iteration**: For $k = 0, 1, 2, \\dots, N_{max}-1$:\n   a. Check for convergence: If $|\\lambda_{current} - \\lambda_{previous}|  \\epsilon$, the algorithm has converged. Terminate and return $\\lambda_{current}$.\n   b. Update the previous eigenvalue estimate: $\\lambda_{previous} \\leftarrow \\lambda_{current}$.\n   c. Compute the next unnormalized vector: $\\mathbf{w} = H \\mathbf{v}_k$.\n   d. Check for the zero-eigenvalue case: if $\\|\\mathbf{w}\\|_2$ is below a machine-safe threshold, it implies $\\mathbf{v}_k$ is in the null space of $H$ (or $H$ is the zero matrix). The corresponding eigenvalue is $0$. Per the instructions, terminate and return $0$.\n   e. Normalize the vector to obtain the next iterate: $\\mathbf{v}_{k+1} = \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2}$. Let this new vector be denoted simply as $\\mathbf{v}$ for the next step.\n   f. Update the current eigenvalue estimate by evaluating the Rayleigh quotient with the newly computed vector $\\mathbf{v}$: $\\lambda_{current} = \\mathbf{v}^\\top H \\mathbf{v}$. This requires a second matrix-vector product within the loop but strictly adheres to the sequence of operations described in the problem.\n\n$3$. **Termination**: If the loop completes without convergence (i.e., reaches $N_{max}$ iterations), return the last computed value of $\\lambda_{current}$. The final result for each test case is rounded to six decimal places.\n\nThis procedure constitutes a complete and robust method for approximating the largest algebraic eigenvalue as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, tol: float = 1e-10, max_iter: int = 1000) -> float:\n    \"\"\"\n    Approximates the largest algebraic eigenvalue of a symmetric matrix H\n    using the power iteration method with the Rayleigh quotient.\n\n    Args:\n        H: The symmetric matrix (as a numpy array).\n        tol: The tolerance for convergence, based on the absolute change\n             in the Rayleigh quotient.\n        max_iter: The maximum number of iterations.\n\n    Returns:\n        The estimated largest algebraic eigenvalue.\n    \"\"\"\n    # Get the dimension of the matrix\n    d = H.shape[0]\n\n    # 1. Initialize the vector v0 to the normalized all-ones vector.\n    v = np.ones(d, dtype=H.dtype)\n    v /= np.linalg.norm(v)\n\n    # Initialize eigenvalue estimates\n    lambda_current = 0.0\n    lambda_previous = np.inf  # Set to infinity to ensure the first diff is large\n\n    # A small number to check for zero vectors\n    machine_eps = np.finfo(H.dtype).eps\n\n    # 2. Iterate until convergence or max iterations\n    for _ in range(max_iter):\n        # Check for convergence based on the change in the eigenvalue estimate\n        if np.abs(lambda_current - lambda_previous)  tol:\n            break\n\n        # Store the previous eigenvalue estimate\n        lambda_previous = lambda_current\n\n        # Compute the Hessian-vector product\n        w = H @ v\n        \n        # Calculate the norm of the resulting vector\n        w_norm = np.linalg.norm(w)\n\n        # Check for the special case where H*v is the zero vector\n        if w_norm  machine_eps:\n            # The eigenvalue is 0. Terminate and return 0.\n            lambda_current = 0.0\n            break\n\n        # Normalize the vector to get the next iterate\n        v = w / w_norm\n\n        # Update the eigenvalue estimate using the Rayleigh quotient with the new vector v.\n        # This requires a second matrix-vector product in the loop as per a\n        # literal interpretation of the problem statement \"compute next direction...,\n        # then evaluate the Rayleigh quotient\".\n        lambda_current = v.T @ (H @ v)\n\n    return lambda_current\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Test suite as specified in the problem statement\n    H1 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 0.0],\n        [0.0, 0.0, 2.0]\n    ])\n\n    H2 = np.array([\n        [2.0, 0.5, 0.0],\n        [0.5, -0.5, 0.0],\n        [0.0, 0.0, 0.2]\n    ])\n\n    H3 = np.array([\n        [3.0, 0.0, 0.0],\n        [0.0, 3.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    H4 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    \n    H5 = np.array([\n        [1e-6, 1e-5, 0.0, 0.0, 0.0],\n        [1e-5, 1e-2, 1e-4, 0.0, 0.0],\n        [0.0, 1e-4, 1e-1, 1e-3, 0.0],\n        [0.0, 0.0, 1e-3, 1.0, 1e-2],\n        [0.0, 0.0, 0.0, 1e-2, 10.0]\n    ])\n\n    test_cases = [H1, H2, H3, H4, H5]\n    \n    results = []\n    for H in test_cases:\n        # Calculate the largest eigenvalue for the current matrix\n        largest_eigenvalue = power_iteration(H)\n        \n        # Format the result to six decimal places and append\n        results.append(f\"{largest_eigenvalue:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3186508"}]}