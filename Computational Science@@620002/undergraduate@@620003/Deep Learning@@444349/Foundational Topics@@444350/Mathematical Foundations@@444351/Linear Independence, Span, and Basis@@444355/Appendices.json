{"hands_on_practices": [{"introduction": "The power of linear algebra lies in its ability to describe structures far beyond the familiar vectors in $\\mathbb{R}^n$. Vector spaces can be composed of functions, such as polynomials, which are central to many areas of science and engineering. This exercise [@problem_id:1868582] invites you to explore this abstract concept by finding a basis for a specific subspace of polynomials. By testing sets for linear independence and the ability to span the subspace, you will gain a concrete understanding of what constitutes a basis, the fundamental framework for any vector space.", "problem": "Let $P_3$ be the vector space of all polynomials of degree at most 3 with real coefficients. Consider the subspace $W$ of $P_3$ defined by the set $W = \\{ p(x) \\in P_3 \\mid p(1) = 0 \\}$. Which of the following sets of polynomials forms a basis for the subspace $W$?\n\nA. $S_1 = \\{ x-1, x^2-1, x^3-1 \\}$\n\nB. $S_2 = \\{ x-1, x^2-1, x^2-x \\}$\n\nC. $S_3 = \\{ x-1, x^2-1 \\}$\n\nD. $S_4 = \\{ 1, x, x^2 \\}$\n\nE. $S_5 = \\{ x-1, x^2-1, x^3 \\}$", "solution": "Let $P_{3}$ be the $4$-dimensional real vector space of polynomials of degree at most $3$. Define the linear map $T:P_{3}\\to\\mathbb{R}$ by $T(p)=p(1)$. The subspace $W$ is $W=\\ker(T)$. Since $T$ is surjective (for example, $T(1)=1$), by the rank-nullity theorem,\n$$\n\\dim(W)=\\dim(P_{3})-\\dim(\\operatorname{im}T)=4-1=3.\n$$\nEquivalently, by the factor theorem, $p(1)=0$ if and only if $x-1$ divides $p(x)$, so every $p\\in W$ can be written as $p(x)=(x-1)q(x)$ with $q\\in P_{2}$, again giving $\\dim(W)=3$.\n\nWe test each candidate set.\n\nFor $S_{1}=\\{x-1,\\ x^{2}-1,\\ x^{3}-1\\}$:\n- Membership in $W$: $(x-1)(1)=0$, $(x^{2}-1)(1)=1-1=0$, $(x^{3}-1)(1)=1-1=0$, so all three are in $W$.\n- Linear independence: Suppose\n$$\na(x-1)+b(x^{2}-1)+c(x^{3}-1)=0.\n$$\nExpand and collect coefficients:\n$$\nc x^{3}+b x^{2}+a x+(-a-b-c)=0.\n$$\nEquating coefficients gives\n$$\nc=0,\\quad b=0,\\quad a=0,\\quad -a-b-c=0,\n$$\nso only the trivial solution exists. Thus the three vectors are linearly independent. Since $W$ has dimension $3$, $S_{1}$ is a basis for $W$.\n\nFor $S_{2}=\\{x-1,\\ x^{2}-1,\\ x^{2}-x\\}$:\n- Membership in $W$: $(x-1)(1)=0$, $(x^{2}-1)(1)=0$, $(x^{2}-x)(1)=0$.\n- Linear independence: Suppose\n$$\na(x-1)+b(x^{2}-1)+c(x^{2}-x)=0.\n$$\nThen\n$$\n(b+c)x^{2}+(a-c)x+(-a-b)=0,\n$$\nso\n$$\nb+c=0,\\quad a-c=0,\\quad -a-b=0.\n$$\nThese imply $a=c$ and $b=-c$, yielding nontrivial solutions (e.g., take $c\\neq 0$). Hence the set is linearly dependent and not a basis.\n\nFor $S_{3}=\\{x-1,\\ x^{2}-1\\}$:\n- It has only $2$ vectors, but $\\dim(W)=3$, so it cannot span $W$ and is not a basis.\n\nFor $S_{4}=\\{1,\\ x,\\ x^{2}\\}$:\n- Membership: $1\\notin W$ since $1(1)=1\\neq 0$. Thus it is not even a subset of $W$ and cannot be a basis of $W$.\n\nFor $S_{5}=\\{x-1,\\ x^{2}-1,\\ x^{3}\\}$:\n- Membership: $x^{3}\\notin W$ since $x^{3}(1)=1\\neq 0$. Hence not a basis of $W$.\n\nTherefore, the only valid basis among the options is $S_{1}$.", "answer": "$$\\boxed{A}$$", "id": "1868582"}, {"introduction": "While constructing a basis explicitly is a fundamental skill, it is often more efficient to determine its size—the dimension of the space—using higher-level theorems. This practice [@problem_id:1868614] demonstrates this by asking for the dimension of a vector space of matrices, which are the building blocks of neural networks. You will learn to reframe a constraint on the matrices as a linear map and apply the rank-nullity theorem, providing a powerful shortcut to understanding the degrees of freedom within the space without listing every basis vector.", "problem": "Consider the set of all $3 \\times 3$ matrices with real-valued entries. Within this set, we are interested in a special collection of matrices, which we'll call 'zero-sum matrices'. A matrix is defined as a zero-sum matrix if the sum of the elements on its main diagonal is equal to zero. Any zero-sum matrix can be constructed by taking a weighted sum (a linear combination) of a smaller, fundamental set of zero-sum matrices. What is the minimum number of matrices required in such a fundamental set, such that any $3 \\times 3$ zero-sum matrix can be represented as a weighted sum of the matrices in this set?", "solution": "The problem asks for the minimum number of matrices in a \"fundamental set\" that can be used to generate any $3 \\times 3$ \"zero-sum matrix\" through weighted sums. In the language of linear algebra, this is equivalent to finding the dimension of the subspace of traceless $3 \\times 3$ matrices.\n\nLet $V = M_{3}(\\mathbb{R})$ be the vector space of all $3 \\times 3$ matrices with real entries. The dimension of this vector space is $\\dim(V) = 3 \\times 3 = 9$. A standard basis for $V$ consists of the nine matrices $E_{ij}$ (for $i,j \\in \\{1, 2, 3\\}$), where $E_{ij}$ is the matrix with a 1 in the $i$-th row and $j$-th column and zeros elsewhere.\n\nLet $W$ be the set of \"zero-sum matrices\". A matrix $A \\in V$ is in $W$ if its trace is zero. The trace of a matrix is the sum of its diagonal elements. So, for $A = (a_{ij})$, $A \\in W$ if and only if $\\text{tr}(A) = a_{11} + a_{22} + a_{33} = 0$.\n\nFirst, we establish that $W$ is a subspace of $V$.\n1.  The zero matrix, $O$, has a trace of $0+0+0=0$, so $O \\in W$.\n2.  Let $A, B \\in W$. Then $\\text{tr}(A)=0$ and $\\text{tr}(B)=0$. The trace is a linear operation, so $\\text{tr}(A+B) = \\texttr(A) + \\text{tr}(B) = 0 + 0 = 0$. Thus, $A+B \\in W$.\n3.  Let $A \\in W$ and $c \\in \\mathbb{R}$ be a scalar. Then $\\text{tr}(cA) = c \\cdot \\text{tr}(A) = c \\cdot 0 = 0$. Thus, $cA \\in W$.\nSince $W$ is closed under addition and scalar multiplication, it is a subspace of $V$. The question asks for the dimension of this subspace, $\\dim(W)$.\n\nWe can find this dimension by using the rank-nullity theorem. Let's define a linear map $T: M_{3}(\\mathbb{R}) \\to \\mathbb{R}$ by $T(A) = \\text{tr}(A)$.\nThis map is a linear transformation (also called a linear functional) because:\n-   $T(A+B) = \\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B) = T(A) + T(B)$\n-   $T(cA) = \\text{tr}(cA) = c \\cdot \\text{tr}(A) = c \\cdot T(A)$\n\nThe set $W$ is precisely the set of matrices $A$ for which $T(A) = 0$. This means that $W$ is the kernel (or null space) of the linear transformation $T$. So, $W = \\ker(T)$.\n\nThe rank-nullity theorem states that for a linear map $T: V \\to U$, we have:\n$$ \\dim(V) = \\dim(\\ker(T)) + \\dim(\\text{Im}(T)) $$\nHere, $V = M_{3}(\\mathbb{R})$ and the codomain is $\\mathbb{R}$.\n\nWe already know $\\dim(V) = 9$. We need to find the dimension of the image (or range) of $T$, denoted $\\text{Im}(T)$. The image of $T$ is the set of all possible values that the trace can take. For any real number $r \\in \\mathbb{R}$, we can construct a matrix $A \\in M_{3}(\\mathbb{R})$ such that $\\text{tr}(A)=r$. For example, the matrix\n$$ A = \\begin{pmatrix} r & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nhas $\\text{tr}(A) = r$. Since the trace can be any real number, the image of $T$ is the entire set of real numbers, $\\mathbb{R}$. The dimension of $\\mathbb{R}$ as a vector space over itself is 1. So, $\\dim(\\text{Im}(T)) = 1$.\n\nNow, we can apply the rank-nullity theorem:\n$$ \\dim(M_{3}(\\mathbb{R})) = \\dim(\\ker(T)) + \\dim(\\text{Im}(T)) $$\n$$ 9 = \\dim(W) + 1 $$\nSolving for $\\dim(W)$, we get:\n$$ \\dim(W) = 9 - 1 = 8 $$\n\nTherefore, the minimum number of matrices required in the fundamental set is 8.", "answer": "$$\\boxed{8}$$", "id": "1868614"}, {"introduction": "Linear independence is not just an algebraic condition; it has profound geometric meaning, especially in the study of curves, surfaces, and manifolds. At any point on a smooth surface, a set of linearly independent tangent vectors forms a basis for a tangent plane, the local linear approximation of the surface. This problem [@problem_id:1651253] explores what happens when this condition fails at a singularity, like the apex of a cone. By analyzing the tangent vectors at this special point, you will discover how linear dependence manifests geometrically, signaling a point where the surface is not smooth.", "problem": "Consider a surface in three-dimensional Cartesian space parametrized by the vector function $\\vec{r}(u, v) = \\langle v \\cos(u), v \\sin(u), v \\rangle$, where the parameters are in the domain $u \\in [0, 2\\pi)$ and $v \\ge 0$. This surface describes a right circular cone with its apex located at the origin of the coordinate system. The tangent vectors associated with the parameter curves are defined as $\\vec{T}_u = \\frac{\\partial \\vec{r}}{\\partial u}$ and $\\vec{T}_v = \\frac{\\partial \\vec{r}}{\\partial v}$.\n\nEvaluate the set of tangent vectors $\\{\\vec{T}_u, \\vec{T}_v\\}$ at the apex of the cone and analyze their linear dependence. Based on your analysis, which one of the following statements is correct?\n\nA. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly independent at the apex, and thus form a valid basis for a tangent plane at that point.\n\nB. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly dependent at the apex because $\\vec{T}_u$ evaluates to the zero vector.\n\nC. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly dependent at the apex because $\\vec{T}_v$ evaluates to the zero vector.\n\nD. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly dependent at the apex because they are non-zero but are parallel (scalar multiples of one another).\n\nE. The linear independence of $\\vec{T}_u$ and $\\vec{T}_v$ at the apex cannot be determined because it depends on the specific value of the parameter $u$.", "solution": "The surface is parametrized by $\\vec{r}(u,v)=\\langle v\\cos(u),v\\sin(u),v\\rangle$ with $u\\in[0,2\\pi)$ and $v\\ge 0$. By definition of tangent vectors to a parametrized surface, \n$$\n\\vec{T}_{u}=\\frac{\\partial\\vec{r}}{\\partial u},\\qquad \\vec{T}_{v}=\\frac{\\partial\\vec{r}}{\\partial v}.\n$$\nCompute the partial derivatives componentwise:\n$$\n\\vec{T}_{u}(u,v)=\\left\\langle -v\\sin(u),\\,v\\cos(u),\\,0\\right\\rangle,\\qquad\n\\vec{T}_{v}(u,v)=\\left\\langle \\cos(u),\\,\\sin(u),\\,1\\right\\rangle.\n$$\nThe apex of the cone is the point $\\vec{r}(u,0)=\\langle 0,0,0\\rangle$ for any $u$. Evaluating the tangent vectors at $v=0$ gives\n$$\n\\vec{T}_{u}(u,0)=\\langle 0,0,0\\rangle,\\qquad \\vec{T}_{v}(u,0)=\\langle \\cos(u),\\sin(u),1\\rangle.\n$$\nThus $\\vec{T}_{u}(u,0)$ is the zero vector, while $\\vec{T}_{v}(u,0)$ is nonzero for all $u$ because its third component equals $1$. A set containing the zero vector is linearly dependent, so $\\{\\vec{T}_{u},\\vec{T}_{v}\\}$ is linearly dependent at the apex specifically because $\\vec{T}_{u}$ vanishes there. Therefore, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1651253"}]}