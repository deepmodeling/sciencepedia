{"hands_on_practices": [{"introduction": "In many deep learning optimization routines, we encounter the task of solving a linear system $Ax=b$. While one might be tempted to compute the solution as $x = A^{-1}b$, this approach is often numerically unstable, especially when the curvature matrix $A$ is ill-conditioned. This exercise will guide you through a hands-on comparison of different solution methods, allowing you to quantify the superior stability and robustness offered by direct solvers and identity-shifted systems—a technique fundamental to regularization. [@problem_id:3147728]", "problem": "You are given a sequence of linear systems that model inner iterations in deep training loops, where at each step a symmetric positive semidefinite matrix $A$ arises from local quadratic approximations and one solves $A x = b$ to compute a parameter update. Your task is to quantify, in controlled synthetic experiments, the numerical stability of three approaches and to determine how using an identity-shifted system improves robustness: explicit matrix inversion to compute $A^{-1} b$, a direct linear solve of $A x = b$, and a shifted linear solve of $(A + \\lambda I) x = b$ with $\\lambda > 0$. You must use a programmatic approach, implement all computations in double precision, and aggregate all test-case results into a single, machine-readable line.\n\nFundamental base for this problem:\n- Linear systems $A x = b$ and the identity matrix $I$.\n- The $2$-norm $\\| \\cdot \\|_2$ and the $2$-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. For symmetric positive definite $A$, $\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the largest and smallest eigenvalues of $A$.\n- For symmetric matrices, the eigenvalues of $A + \\lambda I$ are $\\lambda_i(A) + \\lambda$, hence $\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$, with $\\lambda > 0$ reducing the condition number.\n- Backward stability of solving linear systems by Gaussian elimination or Cholesky factorization versus instability of explicitly forming $A^{-1}$ in floating point arithmetic.\n\nDefinitions of metrics to compute:\n- The amplification factor $g$ for a method is defined, given a noise vector $\\eta$ and a ground truth $x^\\star$, by $g = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2}$, where $x$ is the estimate produced by the method on the noisy right-hand side $b = A x^\\star + \\eta$.\n- The condition numbers $\\kappa_2(A)$ and $\\kappa_2(A + \\lambda I)$.\n- The algorithmic discrepancy $\\delta_{\\text{inv}}$ between explicit inversion and direct solving on the clean right-hand side $b_0 = A x^\\star$, defined by $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$, where $x_{\\text{inv,clean}} = A^{-1} b_0$ and $x_{\\text{solve,clean}}$ solves $A x = b_0$ via a direct solver.\n\nExperimental setup:\n- All random numbers must be generated by a reproducible pseudorandom number generator initialized with the specified seeds.\n- For each test case, construct the matrix $A$, sample a ground truth vector $x^\\star$ with entries uniformly distributed in $[-1,1]$, form the clean right-hand side $b_0 = A x^\\star$, draw a Gaussian noise vector $\\eta$ with independent components from $\\mathcal{N}(0, \\sigma^2)$, and set $b = b_0 + \\eta$.\n- Compute $x_{\\text{inv}} = A^{-1} b$ by explicit inversion, $x_{\\text{solve}}$ by solving $A x = b$, and $x_{\\text{shift}}$ by solving $(A + \\lambda I) x = b$.\n- For methods that fail due to singularity or numerical breakdown, set the corresponding metrics to $+\\infty$.\n- For each test case, report the list $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$, where $\\text{is\\_superior}$ is the boolean value of $g_{\\text{shift}} < \\min(g_{\\text{solve}}, g_{\\text{inv}})$.\n\nTest suite with parameter values:\n- Case $1$ (random symmetric positive definite): dimension $n = 10$, construct $A = Q^\\top Q + \\alpha I$ with $Q \\in \\mathbb{R}^{n \\times n}$ having entries uniform in $[-1,1]$ using seed $7$, set $\\alpha = 10^{-2}$, ground truth seed $11$, noise seed $13$, noise standard deviation $\\sigma = 10^{-6}$, shift $\\lambda = 10^{-1}$.\n- Case $2$ (Hilbert matrix): dimension $n = 12$, construct the Hilbert matrix $A$ with entries $A_{ij} = \\frac{1}{i + j + 1}$ for indices $i, j \\in \\{0, 1, \\dots, n-1\\}$, ground truth seed $17$, noise seed $19$, noise standard deviation $\\sigma = 10^{-4}$, shift $\\lambda = 10^{-1}$.\n- Case $3$ (rank-deficient symmetric positive semidefinite): dimension $n = 10$, construct $B \\in \\mathbb{R}^{r \\times n}$ with $r = 8$ with entries uniform in $[-1,1]$ using seed $23$ and set $A = B^\\top B$, ground truth seed $29$, noise seed $31$, noise standard deviation $\\sigma = 10^{-6}$, shift $\\lambda = 1$.\n- Case $4$ (prescribed spectrum): dimension $n = 12$, construct $A = U \\operatorname{diag}(e) U^\\top$ where $U$ is obtained from the $\\operatorname{QR}$ factorization of a Gaussian matrix generated with seed $37$, and eigenvalues $e_i$ satisfy $\\log_{10}(e_i)$ linearly spaced between $-8$ and $0$, i.e., $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$ for $i \\in \\{0, 1, \\dots, n - 1\\}$, ground truth seed $41$, noise seed $43$, noise standard deviation $\\sigma = 10^{-5}$, shift $\\lambda = 10^{-2}$.\n\nYour program must:\n- Implement the above construction and metrics for each case in the specified order.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result itself being a bracketed, comma-separated list in the form $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$.", "solution": "We begin with the foundational linear algebra principles. For a linear system $A x = b$, the solution mapping is $b \\mapsto x = A^{-1} b$ in exact arithmetic when $A$ is nonsingular. The sensitivity of this mapping to perturbations is governed by the condition number $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. In floating point arithmetic, explicit inversion amplifies rounding errors because computing $A^{-1}$ is itself a numerically sensitive operation, and then multiplying by $b$ introduces further error. In contrast, direct linear solvers based on Gaussian elimination or Cholesky factorization are backward stable: they compute the exact solution to a slightly perturbed system $(A + \\Delta A) x = b$ with $\\|\\Delta A\\|$ proportional to machine precision. Consequently, forming $A^{-1}$ explicitly and using it is less stable than solving $A x = b$.\n\nFor symmetric positive definite $A$, the $2$-norm equals the largest singular value which equals the largest eigenvalue, and $\\|A^{-1}\\|_2$ equals the reciprocal of the smallest eigenvalue. Hence, $\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$. Consider the shifted system $(A + \\lambda I) x = b$ with $\\lambda > 0$. The eigenvalues of $A + \\lambda I$ are $\\lambda_i(A) + \\lambda$ for each eigenvalue $\\lambda_i(A)$ of $A$. Therefore,\n$$\n\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}.\n$$\nAssume $0 < \\lambda_{\\min}(A) \\le \\lambda_{\\max}(A)$. Define $f(\\lambda) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$ for $\\lambda > 0$. The derivative is\n$$\nf'(\\lambda) = \\frac{\\lambda_{\\min}(A) - \\lambda_{\\max}(A)}{(\\lambda_{\\min}(A) + \\lambda)^2} < 0,\n$$\nimplying $f$ is strictly decreasing. Thus, adding $\\lambda I$ reduces the condition number. Since the operator norm of $(A + \\lambda I)^{-1}$ is $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$, the shifted system reduces the potential amplification of perturbations in $b$.\n\nIn the presence of noisy right-hand sides $b = A x^\\star + \\eta$, the solution $x = A^{-1} b$ experiences error $\\|x - x^\\star\\|_2$ that, in first-order perturbation analysis, scales like $\\|A^{-1}\\|_2 \\|\\eta\\|_2$ when $A$ is well-conditioned and the algorithm is backward stable. For ill-conditioned $A$, $\\|A^{-1}\\|_2$ is large, and noise is greatly amplified. Using $(A + \\lambda I)^{-1}$ reduces $\\|A^{-1}\\|_2$ to $\\|(A + \\lambda I)^{-1}\\|_2$, dampening noise amplification, at the cost of biasing the solution away from $x^\\star$. In many deep learning contexts, this shift corresponds to Levenberg–Marquardt damping or Tikhonov-type regularization, which improves robustness by controlling step sizes. The metric we use, the amplification factor\n$$\ng = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2},\n$$\ncaptures the trade-off, with smaller $g$ indicating better robustness to noise.\n\nAlgorithm design:\n- For each test case, construct $A$ deterministically following the specified recipe:\n  - Case $1$: $A = Q^\\top Q + \\alpha I$ with $Q$ uniform in $[-1,1]$.\n  - Case $2$: Hilbert matrix $A_{ij} = \\frac{1}{i + j + 1}$.\n  - Case $3$: $A = B^\\top B$ with $B$ uniform in $[-1,1]$ and rank $r < n$.\n  - Case $4$: $A = U \\operatorname{diag}(e) U^\\top$ with orthonormal $U$ from $\\operatorname{QR}$ decomposition and eigenvalues $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$.\n- Generate $x^\\star$ uniformly in $[-1,1]$ and noise $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$ with the specified seeds. Compute $b_0 = A x^\\star$ and $b = b_0 + \\eta$.\n- Compute condition numbers $\\kappa_2(A)$ and $\\kappa_2(A + \\lambda I)$ using the $2$-norm.\n- Compute solutions:\n  - Explicit inverse $x_{\\text{inv}} = A^{-1} b$ and, on the clean right-hand side, $x_{\\text{inv,clean}} = A^{-1} b_0$.\n  - Direct solve $x_{\\text{solve}}$ for $A x = b$ and, on the clean right-hand side, $x_{\\text{solve,clean}}$.\n  - Shifted solve $x_{\\text{shift}}$ for $(A + \\lambda I) x = b$.\n- Compute metrics:\n  - Amplification factors $g_{\\text{inv}} = \\frac{\\|x_{\\text{inv}} - x^\\star\\|_2}{\\|\\eta\\|_2}$, $g_{\\text{solve}} = \\frac{\\|x_{\\text{solve}} - x^\\star\\|_2}{\\|\\eta\\|_2}$, and $g_{\\text{shift}} = \\frac{\\|x_{\\text{shift}} - x^\\star\\|_2}{\\|\\eta\\|_2}$.\n  - Algorithmic discrepancy $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$.\n- Determine superiority as the boolean value of $g_{\\text{shift}} < \\min(g_{\\text{solve}}, g_{\\text{inv}})$.\n- Handle singular or near-singular cases by catching linear algebra errors; if inversion or solve fails, set the corresponding metrics to $+\\infty$.\n- Aggregate per-case results into the specified single-line output format.\n\nWhy this demonstrates superiority:\n- For symmetric positive definite $A$, adding $\\lambda I$ increases all eigenvalues uniformly, tightening the spectrum and decreasing $\\kappa_2$. The operator norm of the inverse decreases from $\\frac{1}{\\lambda_{\\min}(A)}$ to $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$, reducing noise amplification. Empirically, $g_{\\text{shift}}$ should be smaller than $g_{\\text{inv}}$ and $g_{\\text{solve}}$, particularly in ill-conditioned or rank-deficient cases.\n- Explicit inversion tends to have larger $\\delta_{\\text{inv}}$, especially for ill-conditioned $A$, revealing its inferior numerical stability compared to direct solves.\n- The test suite covers:\n  - A moderately conditioned system where all methods perform comparably and the shift offers slight robustness.\n  - A Hilbert matrix with very large condition number where the shift significantly improves stability.\n  - A rank-deficient symmetric positive semidefinite system where the unshifted methods may fail or yield very large amplification, while the shift regularizes the problem.\n  - A system with a prescribed spectrum spanning many orders of magnitude to stress conditioning and show the benefit of shifting.\n\nThe final output format is a single line containing a bracketed, comma-separated list of the per-case lists $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$ in the order of the four cases.", "answer": "```python\n# Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used), standard library only.\nimport numpy as np\n\ndef hilbert(n: int) -> np.ndarray:\n    i = np.arange(n).reshape(-1, 1)\n    j = np.arange(n).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\ndef make_spd_via_Q(n: int, alpha: float, seed: int) -> np.ndarray:\n    rng = np.random.default_rng(seed)\n    Q = rng.uniform(-1.0, 1.0, size=(n, n))\n    A = Q.T @ Q + alpha * np.eye(n)\n    return A\n\ndef make_rank_def_spd(n: int, r: int, seed: int) -> np.ndarray:\n    rng = np.random.default_rng(seed)\n    B = rng.uniform(-1.0, 1.0, size=(r, n))\n    A = B.T @ B\n    return A\n\ndef make_spd_with_eigs(n: int, seed: int) -> np.ndarray:\n    # Create orthonormal U via QR of a Gaussian matrix\n    rng = np.random.default_rng(seed)\n    G = rng.normal(0.0, 1.0, size=(n, n))\n    Q, _ = np.linalg.qr(G)\n    # Prescribed eigenvalues: log10 spaced from -8 to 0\n    exponents = -8.0 + 8.0 * (np.arange(n) / (n - 1))\n    eigs = 10.0 ** exponents\n    A = Q @ np.diag(eigs) @ Q.T\n    return A\n\ndef amplification_factor(x_est: np.ndarray, x_true: np.ndarray, noise: np.ndarray) -> float:\n    num = np.linalg.norm(x_est - x_true, ord=2)\n    den = np.linalg.norm(noise, ord=2)\n    # Avoid division by zero; if no noise, define amplification as +inf\n    if den == 0.0:\n        return float('inf')\n    return num / den\n\ndef safe_inv_solve(A: np.ndarray, b: np.ndarray):\n    # Returns solution via explicit inverse, or raises/returns inf if fails\n    try:\n        A_inv = np.linalg.inv(A)\n        return A_inv @ b\n    except Exception:\n        return None\n\ndef safe_direct_solve(A: np.ndarray, b: np.ndarray):\n    try:\n        return np.linalg.solve(A, b)\n    except Exception:\n        return None\n\ndef compute_case(case):\n    case_type = case['type']\n    if case_type == 'spd_q':\n        A = make_spd_via_Q(case['n'], case['alpha'], case['seed_A'])\n    elif case_type == 'hilbert':\n        A = hilbert(case['n'])\n    elif case_type == 'rank_def_spd':\n        A = make_rank_def_spd(case['n'], case['rank'], case['seed_A'])\n    elif case_type == 'spd_eigs':\n        A = make_spd_with_eigs(case['n'], case['seed_A'])\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    n = case['n']\n    # Ground truth and noise\n    rng_x = np.random.default_rng(case['seed_x'])\n    x_true = rng_x.uniform(-1.0, 1.0, size=n)\n    b_clean = A @ x_true\n\n    rng_noise = np.random.default_rng(case['seed_noise'])\n    noise = rng_noise.normal(0.0, case['noise_std'], size=n)\n    b_noisy = b_clean + noise\n\n    # Condition numbers\n    try:\n        cond_A = np.linalg.cond(A)  # 2-norm condition number\n    except Exception:\n        cond_A = float('inf')\n\n    A_shift = A + case['lambda'] * np.eye(n)\n    try:\n        cond_A_shift = np.linalg.cond(A_shift)\n    except Exception:\n        cond_A_shift = float('inf')\n\n    # Solutions\n    x_inv_noisy = safe_inv_solve(A, b_noisy)\n    x_solve_noisy = safe_direct_solve(A, b_noisy)\n    x_shift_noisy = safe_direct_solve(A_shift, b_noisy)\n\n    # Clean RHS for algorithmic discrepancy\n    x_inv_clean = safe_inv_solve(A, b_clean)\n    x_solve_clean = safe_direct_solve(A, b_clean)\n\n    # Amplification factors\n    if x_inv_noisy is None:\n        g_inv = float('inf')\n    else:\n        g_inv = amplification_factor(x_inv_noisy, x_true, noise)\n\n    if x_solve_noisy is None:\n        g_solve = float('inf')\n    else:\n        g_solve = amplification_factor(x_solve_noisy, x_true, noise)\n\n    if x_shift_noisy is None:\n        g_shift = float('inf')\n    else:\n        g_shift = amplification_factor(x_shift_noisy, x_true, noise)\n\n    # Algorithmic discrepancy\n    if (x_inv_clean is None) or (x_solve_clean is None):\n        delta_inv = float('inf')\n    else:\n        denom = np.linalg.norm(x_solve_clean, ord=2)\n        if denom == 0.0:\n            delta_inv = float('inf')\n        else:\n            delta_inv = np.linalg.norm(x_inv_clean - x_solve_clean, ord=2) / denom\n\n    is_superior = g_shift < min(g_solve, g_inv)\n\n    return [g_inv, g_solve, g_shift, cond_A, cond_A_shift, delta_inv, is_superior]\n\ndef solve():\n    test_cases = [\n        {\n            'type': 'spd_q',\n            'n': 10,\n            'alpha': 1e-2,\n            'seed_A': 7,\n            'seed_x': 11,\n            'seed_noise': 13,\n            'noise_std': 1e-6,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'hilbert',\n            'n': 12,\n            'seed_A': None,  # Deterministic\n            'seed_x': 17,\n            'seed_noise': 19,\n            'noise_std': 1e-4,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'rank_def_spd',\n            'n': 10,\n            'rank': 8,\n            'seed_A': 23,\n            'seed_x': 29,\n            'seed_noise': 31,\n            'noise_std': 1e-6,\n            'lambda': 1.0,\n        },\n        {\n            'type': 'spd_eigs',\n            'n': 12,\n            'seed_A': 37,\n            'seed_x': 41,\n            'seed_noise': 43,\n            'noise_std': 1e-5,\n            'lambda': 1e-2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(compute_case(case))\n\n    # Print single-line, bracket-enclosed, comma-separated list of per-case lists\n    # Ensure booleans and floats are printed via default str\n    def format_item(item):\n        if isinstance(item, list):\n            return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n        else:\n            return str(item)\n\n    print(\"[\" + \",\".join(format_item(r) for r in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3147728"}, {"introduction": "To accelerate gradient descent, we can use preconditioning, which ideally involves multiplying the gradient by the inverse of the Hessian, $G^{-1}$. However, computing this inverse is often too expensive. This practice explores a powerful alternative: approximating the inverse using a truncated Neumann series, which is built from the identity matrix and the Hessian itself. You will derive stability conditions for this method and see how the quality of the approximation impacts the optimization process. [@problem_id:3147772]", "problem": "Consider a supervised linear model with feature matrix $X \\in \\mathbb{R}^{n \\times d}$ and target vector $y \\in \\mathbb{R}^{n}$. The squared loss is $L(w) = \\tfrac{1}{2}\\lVert X w - y \\rVert_2^2$ with gradient $\\nabla L(w) = X^\\top (X w - y)$. Define the feature Gram matrix $G = X^\\top X \\in \\mathbb{R}^{d \\times d}$. Assume $G$ is symmetric positive definite (SPD), so all its eigenvalues are strictly positive. Consider preconditioned gradient descent with preconditioner $M \\in \\mathbb{R}^{d \\times d}$:\n$$\nw_{t+1} = w_t - \\eta\\, M \\nabla L(w_t).\n$$\nThis iteration induces the linear error dynamics\n$$\ne_{t+1} = \\left(I - \\eta\\, M G\\right) e_t,\n$$\nwhere $e_t = w_t - w^\\star$ and $w^\\star$ is the unique minimizer.\n\nStart from the following fundamental base:\n- For any SPD matrix $A$, all eigenvalues are real and positive, and there exists an orthonormal eigenbasis.\n- A linear iteration $z_{t+1} = B z_t$ is stable if and only if the spectral radius of $B$ is strictly less than $1$.\n- If $A$ is diagonalizable with eigenvalues $\\{\\lambda_i\\}$ and $p$ is a polynomial, then $p(A)$ has eigenvalues $\\{p(\\lambda_i)\\}$.\n\nDefine the exact inverse preconditioner $M_{\\text{exact}} = G^{-1}$ and the truncated Neumann series preconditioner for a scalar $\\alpha > 0$ and nonnegative integer $K$:\n$$\nM_{K,\\alpha} = \\frac{1}{\\alpha}\\sum_{k=0}^{K} \\left(I - \\frac{1}{\\alpha} G\\right)^k.\n$$\nAssume throughout that $\\alpha$ is strictly larger than the largest eigenvalue of $G$ so that the Neumann series is well-defined.\n\nYour tasks are:\n1. Using only the fundamental base listed above and first principles, derive a general stability condition on the step size $\\eta$ for the preconditioned gradient descent in terms of the largest eigenvalue of the SPD matrix $M G$.\n2. Specialize your result to $M_{\\text{exact}}$ and explain what this implies about the largest stable step size for this exact inverse preconditioner.\n3. Specialize your result to $M_{K,\\alpha}$ and express the largest stable step size in terms of the eigenvalues of $G$, the truncation order $K$, and the scalar $\\alpha$. Also derive a formula for the operator norm $\\lVert I - M_{K,\\alpha} G \\rVert_2$ in terms of the eigenvalues of $G$, $K$, and $\\alpha$. Your expressions must be valid under the stated assumptions.\n\nTest suite and data construction:\n- Work in dimension $d = 5$.\n- For each test case, construct $G$ as diagonal with prescribed eigenvalues $\\{\\lambda_1,\\dots,\\lambda_5\\}$, that is, $G = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_5)$. One can take $X = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_5})$ so that $G = X^\\top X$.\n- For each test case, let $\\alpha = \\text{alpha\\_factor} \\times \\lambda_{\\max}(G)$, where $\\lambda_{\\max}(G)$ is the largest eigenvalue of $G$.\n\nUse the following test suite of five cases, each given by a triple $\\left(\\{\\lambda_i\\}_{i=1}^5,\\, \\text{alpha\\_factor},\\, K\\right)$:\n- Case $1$: $\\{\\lambda_i\\} = \\{1.0,\\, 2.5,\\, 4.0,\\, 7.0,\\, 10.0\\}$, $\\text{alpha\\_factor} = 1.5$, $K = 0$.\n- Case $2$: $\\{\\lambda_i\\} = \\{1.0,\\, 2.5,\\, 4.0,\\, 7.0,\\, 10.0\\}$, $\\text{alpha\\_factor} = 1.5$, $K = 5$.\n- Case $3$: $\\{\\lambda_i\\} = \\{1.0,\\, 10.0,\\, 50.0,\\, 200.0,\\, 1000.0\\}$, $\\text{alpha\\_factor} = 1.1$, $K = 3$.\n- Case $4$: $\\{\\lambda_i\\} = \\{1.0,\\, 1.125,\\, 1.25,\\, 1.375,\\, 1.5\\}$, $\\text{alpha\\_factor} = 1.01$, $K = 1$.\n- Case $5$: $\\{\\lambda_i\\} = \\{1.0,\\, 5.0,\\, 25.0,\\, 125.0,\\, 625.0\\}$, $\\text{alpha\\_factor} = 10.0$, $K = 0$.\n\nFor each case $j \\in \\{1,2,3,4,5\\}$, compute the following three quantities:\n- The largest stable step size for the exact preconditioner, denoted $\\eta_{\\max}^{\\text{exact}}(j)$.\n- The largest stable step size for the truncated Neumann preconditioner, denoted $\\eta_{\\max}^{\\text{neu}}(j)$.\n- The spectral operator norm of the deviation from the identity under the truncated Neumann preconditioner, $\\lVert I - M_{K,\\alpha} G \\rVert_2(j)$.\n\nFinal output format:\n- Your program must produce a single line containing a comma-separated list enclosed in square brackets with exactly $15$ floats in the following order:\n$$\n[\\eta_{\\max}^{\\text{exact}}(1),\\, \\eta_{\\max}^{\\text{neu}}(1),\\, \\lVert I - M_{K,\\alpha} G \\rVert_2(1),\\, \\eta_{\\max}^{\\text{exact}}(2),\\, \\eta_{\\max}^{\\text{neu}}(2),\\, \\lVert I - M_{K,\\alpha} G \\rVert_2(2),\\, \\dots,\\, \\eta_{\\max}^{\\text{exact}}(5),\\, \\eta_{\\max}^{\\text{neu}}(5),\\, \\lVert I - M_{K,\\alpha} G \\rVert_2(5)].\n$$\nNo inputs are read; all data are embedded exactly as specified above. All outputs are unitless real numbers. The program should round each float to $10$ decimal places before printing.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical linear algebra and optimization, well-posed with a clear objective, and internally consistent. We may therefore proceed with a formal derivation.\n\nThe analysis begins from the provided linear error dynamics for preconditioned gradient descent:\n$$\ne_{t+1} = \\left(I - \\eta M G\\right) e_t\n$$\nwhere $e_t = w_t - w^\\star$ is the error at iteration $t$, $I$ is the identity matrix, $\\eta > 0$ is the step size, $M$ is the preconditioner, and $G$ is the symmetric positive definite (SPD) feature Gram matrix.\n\nBased on the provided fundamental principles, a linear iteration $z_{t+1} = B z_t$ is stable if and only if the spectral radius of the iteration matrix $B$, denoted $\\rho(B)$, is strictly less than $1$. In our case, the iteration matrix is $B = I - \\eta M G$. The spectral radius is defined as $\\rho(B) = \\max_j |\\lambda_j(B)|$, where $\\{\\lambda_j(B)\\}$ are the eigenvalues of $B$. Therefore, the stability condition is:\n$$\n\\rho(I - \\eta M G) < 1\n$$\nLet the eigenvalues of the matrix product $MG$ be denoted by $\\{\\mu_j\\}$. Then the eigenvalues of $I - \\eta M G$ are $\\{1 - \\eta \\mu_j\\}$. The stability condition becomes $\\max_j |1 - \\eta \\mu_j| < 1$. This inequality is equivalent to $-1 < 1 - \\eta \\mu_j < 1$ for all eigenvalues $\\mu_j$ of $MG$.\n\nThe right-hand side of the inequality, $1 - \\eta \\mu_j < 1$, implies $-\\eta \\mu_j < 0$. Since $\\eta > 0$, this requires $\\mu_j > 0$ for all $j$. The left-hand side, $-1 < 1 - \\eta \\mu_j$, implies $\\eta \\mu_j < 2$. Combining these, the stability condition is $0 < \\eta \\mu_j < 2$ for all eigenvalues $\\mu_j$ of $MG$.\n\nThe problem states that $G$ is SPD. For the two specific preconditioners considered, $M_{\\text{exact}} = G^{-1}$ and $M_{K,\\alpha}$, we must verify that they are also SPD, which would ensure that the product $MG$ has strictly positive eigenvalues. Since $G$ is SPD, its inverse $G^{-1}$ is also SPD. The matrix $M_{K,\\alpha}$ is a polynomial in $G$. Since $G$ is SPD with eigenvalues $\\{\\lambda_i > 0\\}$, $M_{K,\\alpha}$ is also SPD, with eigenvalues $m_i = \\frac{1}{\\lambda_i}[1 - (1 - \\frac{\\lambda_i}{\\alpha})^{K+1}]$. The condition $\\alpha > \\lambda_{\\max}(G)$ ensures $0 < \\lambda_i/\\alpha < 1$, which makes $m_i > 0$. Because both $M$ and $G$ are SPD, the matrix $MG$ is similar to the SPD matrix $G^{1/2} M G^{1/2}$ and thus has strictly positive real eigenvalues $\\{\\mu_j\\}$.\n\nTherefore, the condition for stability simplifies to $\\eta \\max_j \\mu_j < 2$, which can be written as:\n$$\n0 < \\eta < \\frac{2}{\\lambda_{\\max}(MG)}\n$$\nThis constitutes the general stability condition on the step size $\\eta$. The largest stable step size is $\\eta_{\\max} = \\frac{2}{\\lambda_{\\max}(MG)}$.\n\nNext, we specialize this result to the two given preconditioners.\n\nFor the exact inverse preconditioner, $M = M_{\\text{exact}} = G^{-1}$. The matrix product becomes $MG = G^{-1}G = I$. The eigenvalues of the identity matrix are all equal to $1$. Thus, $\\lambda_{\\max}(MG) = 1$. The largest stable step size is:\n$$\n\\eta_{\\max}^{\\text{exact}} = \\frac{2}{1} = 2\n$$\nThis implies that for the exact inverse preconditioner, any step size in the interval $(0, 2)$ leads to a stable iteration. A step size of $\\eta = 1$ leads to one-step convergence, as $e_1 = (I - 1 \\cdot I)e_0 = 0$.\n\nFor the truncated Neumann series preconditioner, $M = M_{K,\\alpha} = \\frac{1}{\\alpha}\\sum_{k=0}^{K} \\left(I - \\frac{1}{\\alpha} G\\right)^k$. Since $M_{K,\\alpha}$ is a polynomial in $G$, the matrices $M_{K,\\alpha}$ and $G$ commute and share a common orthonormal eigenbasis. Let $v_i$ be an eigenvector of $G$ with corresponding eigenvalue $\\lambda_i > 0$. Then $v_i$ is also an eigenvector of $M_{K,\\alpha}$. The corresponding eigenvalue of $M_{K,\\alpha}$ is obtained by summing the geometric series:\n$$\nm_i = \\frac{1}{\\alpha}\\sum_{k=0}^{K} \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^k = \\frac{1}{\\alpha} \\frac{1 - (1-\\lambda_i/\\alpha)^{K+1}}{1 - (1-\\lambda_i/\\alpha)} = \\frac{1}{\\lambda_i}\\left[1 - \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\\right]\n$$\nThe eigenvalue of the product $M_{K,\\alpha}G$ corresponding to the eigenvector $v_i$ is $\\mu_i = m_i \\lambda_i$:\n$$\n\\mu_i = 1 - \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\n$$\nTo find $\\lambda_{\\max}(M_{K,\\alpha}G) = \\max_i \\mu_i$, we analyze the function $f(x) = 1 - (1 - x/\\alpha)^{K+1}$ for $x \\in [\\lambda_{\\min}(G), \\lambda_{\\max}(G)]$. Given $\\alpha > \\lambda_{\\max}(G)$, the term $1 - x/\\alpha$ is always in $(0, 1)$. The function $g(x) = 1 - x/\\alpha$ is decreasing in $x$. Since $K+1 > 0$, the function $h(y) = y^{K+1}$ is increasing for $y > 0$. Thus, $(1-x/\\alpha)^{K+1}$ is a decreasing function of $x$. Consequently, $f(x) = 1 - (1-x/\\alpha)^{K+1}$ is an increasing function of $x$. The maximum value of $\\mu_i$ is therefore achieved when $\\lambda_i = \\lambda_{\\max}(G)$.\n$$\n\\lambda_{\\max}(M_{K,\\alpha}G) = 1 - \\left(1 - \\frac{\\lambda_{\\max}(G)}{\\alpha}\\right)^{K+1}\n$$\nThe largest stable step size is then:\n$$\n\\eta_{\\max}^{\\text{neu}} = \\frac{2}{\\lambda_{\\max}(M_{K,\\alpha}G)} = \\frac{2}{1 - \\left(1 - \\frac{\\lambda_{\\max}(G)}{\\alpha}\\right)^{K+1}}\n$$\nFinally, we derive the formula for the spectral operator norm $\\lVert I - M_{K,\\alpha} G \\rVert_2$. The matrix $C = I - M_{K,\\alpha}G$ is symmetric, as it is a polynomial in the symmetric matrix $G$. For a symmetric matrix, the spectral norm (2-norm) is equal to its spectral radius. The eigenvalues of $C$ are $c_i = 1 - \\mu_i$, where $\\mu_i$ are the eigenvalues of $M_{K,\\alpha}G$.\n$$\nc_i = 1 - \\left[1 - \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\\right] = \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\n$$\nThe condition $\\alpha > \\lambda_{\\max}(G)$ ensures that $0 < 1 - \\lambda_i/\\alpha < 1$, so all eigenvalues $c_i$ are positive. The spectral norm is the maximum of these eigenvalues:\n$$\n\\lVert I - M_{K,\\alpha} G \\rVert_2 = \\max_i |c_i| = \\max_i \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\n$$\nSince the function $h(x) = (1 - x/\\alpha)^{K+1}$ is a decreasing function of $x$ on the interval of eigenvalues, its maximum value is attained at the smallest eigenvalue, $\\lambda_{\\min}(G)$.\n$$\n\\lVert I - M_{K,\\alpha} G \\rVert_2 = \\left(1 - \\frac{\\lambda_{\\min}(G)}{\\alpha}\\right)^{K+1}\n$$\nThese derived formulas are used to compute the required quantities for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating stability limits and norms for preconditioned gradient descent.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        ({'lambdas': [1.0, 2.5, 4.0, 7.0, 10.0], 'alpha_factor': 1.5, 'K': 0}),\n        # Case 2\n        ({'lambdas': [1.0, 2.5, 4.0, 7.0, 10.0], 'alpha_factor': 1.5, 'K': 5}),\n        # Case 3\n        ({'lambdas': [1.0, 10.0, 50.0, 200.0, 1000.0], 'alpha_factor': 1.1, 'K': 3}),\n        # Case 4\n        ({'lambdas': [1.0, 1.125, 1.25, 1.375, 1.5], 'alpha_factor': 1.01, 'K': 1}),\n        # Case 5\n        ({'lambdas': [1.0, 5.0, 25.0, 125.0, 625.0], 'alpha_factor': 10.0, 'K': 0}),\n    ]\n\n    results = []\n    for case in test_cases:\n        lambdas = case['lambdas']\n        alpha_factor = case['alpha_factor']\n        K = case['K']\n\n        # Extract min and max eigenvalues\n        lambda_min = min(lambdas)\n        lambda_max = max(lambdas)\n\n        # Calculate alpha\n        alpha = alpha_factor * lambda_max\n\n        # 1. Largest stable step size for the exact preconditioner\n        eta_max_exact = 2.0\n\n        # 2. Largest stable step size for the truncated Neumann preconditioner\n        # Formula: 2 / (1 - (1 - lambda_max / alpha)^(K + 1))\n        # Note: 1 - lambda_max / alpha = 1 - lambda_max / (alpha_factor * lambda_max) = 1 - 1/alpha_factor\n        term_neu = 1.0 - (1.0 - 1.0 / alpha_factor)**(K + 1)\n        eta_max_neu = 2.0 / term_neu\n\n        # 3. Spectral norm of the deviation from identity\n        # Formula: (1 - lambda_min / alpha)^(K + 1)\n        term_norm = 1.0 - lambda_min / alpha\n        norm_val = term_norm**(K + 1)\n\n        results.extend([eta_max_exact, eta_max_neu, norm_val])\n\n    # Format the final output string with rounding to 10 decimal places.\n    output_str = f\"[{','.join([f'{r:.10f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3147772"}, {"introduction": "Deep learning models, particularly in areas like attention mechanisms, often involve matrices with a special structure: an identity matrix plus a low-rank component, $A = I + U V^{\\top}$. Inverting such a matrix naively is computationally wasteful and misses a crucial opportunity for optimization. In this exercise, you will derive and implement the celebrated Sherman-Morrison formula to compute the inverse efficiently, demonstrating the immense performance gains achievable by exploiting this structure. [@problem_id:3147731]", "problem": "You are given square matrices of the form $A = I + U V^\\top$, where $I$ is the identity matrix of size $n \\times n$, and $U, V \\in \\mathbb{R}^{n \\times k}$ define a low-rank update. Such low-rank plus identity structures arise in computational linear algebra for deep learning when approximating certain kernelized or preconditioned transformations, and they are especially relevant when the structure is block-local or head-local in attention mechanisms. Your task is to implement and evaluate an algorithm that inverts $A$ using repeated rank-$1$ updates starting from $A_0 = I$ and incrementally incorporating each column pair $(u_i, v_i)$, where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively. You must not assume any formula in advance; instead, you must derive the required update from first principles, starting from the definition of the matrix inverse, and implement it.\n\nFundamental base you may use: definitions and properties of matrix inverse and identity, algebra of outer products and matrix-vector multiplication, norms, and standard block matrix manipulation rules. In particular, you may rely on the facts that if $A$ is invertible, then $A A^{-1} = I$ and $A^{-1} A = I$, and that the inverse is unique when it exists.\n\nInstructions for constructing the matrices:\n- For each test, construct $U$ by drawing entries from a standard normal distribution and then scaling by a factor $\\gamma / \\sqrt{n}$, where $\\gamma$ is provided per test case. To guarantee well-conditioned and nonsingular updates, set $V = U$, so that $A = I + U U^\\top$ is symmetric positive definite for any real $\\gamma$.\n- The matrices are purely mathematical; no physical units are involved.\n\nAlgorithmic tasks:\n1. Starting from $A_0 = I$, and its inverse $A_0^{-1} = I$, incorporate each rank-$1$ term $u_i v_i^\\top$ sequentially to obtain $A_i = A_{i-1} + u_i v_i^\\top$ and update an explicit inverse $A_i^{-1}$ using only matrix-vector products, vector outer products, scalar operations, and matrix additions. Derive the update rule you need starting from the definition of the inverse.\n2. Compute a reference inverse $A_{\\mathrm{ref}}^{-1}$ using a direct method based on a robust numerical linear algebra algorithm.\n3. For each test case, compute the relative Frobenius error between your iterative inverse and the reference inverse:\n   $$ \\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}. $$\n4. Analyze computational cost in terms of scalar multiply-add operations under the following standardized model:\n   - Assume the cost for directly forming $A^{-1}$ by solving $A X = I$ using triangular factorization with $n$ right-hand sides is\n     $$ C_{\\text{direct}}(n) = \\frac{5}{3} \\, n^3. $$\n   - Assume the cost per rank-$1$ update in your iterative inverse update (from $A_{i-1}^{-1}$ to $A_i^{-1}$) is\n     $$ C_{\\text{rank-1}}(n) = 4 n^2, $$\n     counting two matrix-vector products, one outer product, and one matrix update at leading order, and ignoring lower-order terms. For $k$ updates, take\n     $$ C_{\\text{iter}}(n,k) = 4 k n^2. $$\n   - Report the cost ratio\n     $$ \\rho = \\frac{C_{\\text{iter}}(n,k)}{C_{\\text{direct}}(n)}. $$\n5. Your program must implement the above, using $V = U$ precisely as stated.\n\nTest suite:\n- Test 1 (general small): $n = 8$, $k = 3$, $\\gamma = 0.2$, random seed $= 0$.\n- Test 2 (moderate size): $n = 64$, $k = 4$, $\\gamma = 0.05$, random seed $= 1$.\n- Test 3 (edge case identity): $n = 5$, $k = 0$, $\\gamma = 0.0$, random seed $= 2$.\n- Test 4 (larger and more challenging): $n = 50$, $k = 10$, $\\gamma = 0.5$, random seed $= 3$.\n\nRequired final output:\n- For each test case, in the given order, output two values: first the relative Frobenius error $\\varepsilon$ (a floating-point number), then the cost ratio $\\rho$ (a floating-point number).\n- Aggregate all results into a single line string formatted as a Python-style list with comma-separated values and no spaces, for example $[\\varepsilon_1,\\rho_1,\\varepsilon_2,\\rho_2,\\dots]$.\n\nYour program must produce exactly one line in this format as its only output. No physical units or angles are involved, so no special unit handling is required.", "solution": "The problem of inverting a matrix of the form $A = I + U V^\\top$ is a classical problem in numerical linear algebra. The Sherman-Morrison-Woodbury formula provides a direct expression for the inverse. However, the present task requires a first-principles derivation of the update rule for a sequence of rank-$1$ modifications, which we shall now undertake.\n\nLet $A_{i-1} \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix with a known inverse $A_{i-1}^{-1}$. We wish to find the inverse of the matrix $A_i$, which is formed by a rank-$1$ update:\n$$ A_i = A_{i-1} + u_i v_i^\\top $$\nwhere $u_i, v_i \\in \\mathbb{R}^n$ are column vectors. Let us denote the inverse we seek by $A_i^{-1}$.\n\nBy the definition of a matrix inverse, we must have $A_i A_i^{-1} = I$, where $I$ is the $n \\times n$ identity matrix. Substituting the expression for $A_i$:\n$$ (A_{i-1} + u_i v_i^\\top) A_i^{-1} = I $$\nAssuming $A_{i-1}$ is invertible, we can left-multiply by $A_{i-1}^{-1}$:\n$$ A_{i-1}^{-1} (A_{i-1} + u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} I $$\n$$ (I + A_{i-1}^{-1} u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} $$\nNow, we can isolate $A_i^{-1}$ by inverting the term in parentheses:\n$$ A_i^{-1} = (I + A_{i-1}^{-1} u_i v_i^\\top)^{-1} A_{i-1}^{-1} $$\nThe problem is now reduced to finding the inverse of the matrix $M = I + w v_i^\\top$, where we have defined the column vector $w = A_{i-1}^{-1} u_i$.\n\nWe hypothesize that the inverse of $M$ has a similar structure, namely $M^{-1} = I + \\alpha w v_i^\\top$ for some scalar $\\alpha$ to be determined. To find $\\alpha$, we enforce the condition $M M^{-1} = I$:\n$$ (I + w v_i^\\top)(I + \\alpha w v_i^\\top) = I $$\nExpanding the left-hand side using the distributive property of matrix multiplication:\n$$ I(I + \\alpha w v_i^\\top) + w v_i^\\top(I + \\alpha w v_i^\\top) = I $$\n$$ I + \\alpha w v_i^\\top + w v_i^\\top + w v_i^\\top (\\alpha w v_i^\\top) = I $$\nThe term $v_i^\\top w$ is a scalar, which is the inner product of $v_i$ and $w$. We can regroup terms:\n$$ I + (\\alpha w v_i^\\top + w v_i^\\top) + \\alpha (w v_i^\\top w v_i^\\top) = I $$\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha w (v_i^\\top w) v_i^\\top = I $$\nSince $v_i^\\top w$ is a scalar, we can commute it:\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha (v_i^\\top w) w v_i^\\top = I $$\nFactoring out the outer product $w v_i^\\top$:\n$$ I + [(\\alpha+1) + \\alpha(v_i^\\top w)] w v_i^\\top = I $$\nFor this equation to hold for general non-zero vectors $w$ and $v_i$, the scalar coefficient of the $w v_i^\\top$ term must be zero:\n$$ (\\alpha+1) + \\alpha(v_i^\\top w) = 0 $$\n$$ \\alpha(1 + v_i^\\top w) = -1 $$\nSolving for $\\alpha$, we find:\n$$ \\alpha = -\\frac{1}{1 + v_i^\\top w} $$\nThis expression is valid provided that the denominator $1 + v_i^\\top w \\neq 0$. If this condition holds, the inverse is:\n$$ (I + w v_i^\\top)^{-1} = I - \\frac{w v_i^\\top}{1 + v_i^\\top w} $$\nSubstituting this result back into our expression for $A_i^{-1}$:\n$$ A_i^{-1} = \\left(I - \\frac{w v_i^\\top}{1 + v_i^\\top w}\\right) A_{i-1}^{-1} $$\nSubstituting back $w = A_{i-1}^{-1} u_i$:\n$$ A_i^{-1} = \\left(I - \\frac{(A_{i-1}^{-1} u_i) v_i^\\top}{1 + v_i^\\top (A_{i-1}^{-1} u_i)}\\right) A_{i-1}^{-1} $$\nFinally, distributing the $A_{i-1}^{-1}$ term from the right:\n$$ A_i^{-1} = A_{i-1}^{-1} - \\frac{(A_{i-1}^{-1} u_i) (v_i^\\top A_{i-1}^{-1})}{1 + v_i^\\top A_{i-1}^{-1} u_i} $$\nThis is the Sherman-Morrison formula, which provides the sequential update rule for the inverse.\n\nFor this specific problem, we are given $V = U$, which means $v_i = u_i$ for all $i = 1, \\dots, k$. The matrices being constructed are $A_0 = I$ and $A_i = A_{i-1} + u_i u_i^\\top$ for $i > 0$. Since $A_0=I$ is symmetric positive definite (SPD), and each update $u_i u_i^\\top$ is symmetric positive semi-definite, all matrices $A_i$ are SPD. Consequently, their inverses $A_i^{-1}$ are also SPD. This ensures that for any non-zero vector $u_i$, the quadratic form $u_i^\\top A_{i-1}^{-1} u_i > 0$. Thus, the denominator $1 + u_i^\\top A_{i-1}^{-1} u_i$ is always strictly greater than $1$, precluding any division by zero.\n\nThe algorithmic implementation will thus proceed as follows:\n1. Initialize the inverse as $A_0^{-1} = I$.\n2. For each step $i = 1, \\dots, k$, update the current inverse $A_{i-1}^{-1}$ to $A_i^{-1}$ using the derived formula with $v_i=u_i$. This constitutes the iterative inverse, $A_{\\mathrm{iter}}^{-1}$.\n3. Construct the final matrix $A = I + U U^\\top$ directly and compute its inverse $A_{\\mathrm{ref}}^{-1}$ using a standard numerical library function, which typically relies on a robust factorization method like LU decomposition.\n4. Compute the relative Frobenius error $\\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}$.\n5. Compute the cost ratio $\\rho = C_{\\text{iter}}(n,k) / C_{\\text{direct}}(n)$, using the provided cost models:\n   $C_{\\text{iter}}(n,k) = 4 k n^2$\n   $C_{\\text{direct}}(n) = \\frac{5}{3} n^3$\n   This ratio simplifies to $\\rho = \\frac{4 k n^2}{(5/3) n^3} = \\frac{12k}{5n}$. This ratio indicates that the iterative method is more efficient when the number of updates $k$ is small relative to the matrix dimension $n$, specifically when $k < \\frac{5}{12}n$.\n\nThe implementation will follow these steps for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of iteratively inverting a matrix of the form A = I + UU^T\n    and compares it against a direct inversion method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, gamma, random_seed)\n        (8, 3, 0.2, 0),\n        (64, 4, 0.05, 1),\n        (5, 0, 0.0, 2),\n        (50, 10, 0.5, 3),\n    ]\n\n    results = []\n    for n, k, gamma, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # Construct matrix U. Handle the k=0 edge case.\n        if k > 0:\n            scale_factor = gamma / np.sqrt(n)\n            U = np.random.randn(n, k) * scale_factor\n        else:\n            # An n x 0 matrix\n            U = np.zeros((n, 0))\n        \n        # As per instructions, V = U\n        V = U\n\n        # 1. Iterative inverse calculation using the derived rank-1 update rule.\n        # Start with the inverse of A_0 = I, which is I.\n        A_inv_iter = np.eye(n)\n        \n        # Sequentially apply k rank-1 updates.\n        for i in range(k):\n            u_i = U[:, i:i+1] # Get i-th column as a (n, 1) vector\n            v_i = V[:, i:i+1] # v_i is u_i\n            \n            # Sherman-Morrison formula:\n            # A_inv_new = A_inv - (A_inv @ u @ v.T @ A_inv) / (1 + v.T @ A_inv @ u)\n            \n            # Pre-calculate terms for clarity and efficiency\n            w = A_inv_iter @ u_i # This is the vector (A_{i-1}^{-1} u_i)\n            \n            # Denominator: 1 + v_i^T * (A_{i-1}^{-1} u_i)\n            denominator = 1.0 + (v_i.T @ w).item()\n            \n            # Since A_inv_iter is symmetric and v_i=u_i, the numerator term\n            # (A_inv @ u) @ (v.T @ A_inv) simplifies to w @ w.T\n            numerator_outer_product = w @ w.T\n            \n            # Update the inverse\n            A_inv_iter -= numerator_outer_product / denominator\n\n        # 2. Compute the reference inverse using a direct method.\n        # First, construct the full matrix A = I + U U^T\n        A = np.eye(n) + U @ U.T\n        \n        # Use a robust direct solver\n        A_inv_ref = np.linalg.inv(A)\n\n        # 3. Compute the relative Frobenius error.\n        norm_ref = np.linalg.norm(A_inv_ref, 'fro')\n        \n        if norm_ref == 0:\n            # This is unlikely for an invertible matrix, but for completeness:\n            # if reference is zero matrix, error is 0 if iterative is also zero, else infinity\n            epsilon = 0.0 if np.linalg.norm(A_inv_iter, 'fro') == 0.0 else np.inf\n        else:\n            norm_diff = np.linalg.norm(A_inv_iter - A_inv_ref, 'fro')\n            epsilon = norm_diff / norm_ref\n\n        # 4. Compute the computational cost ratio.\n        c_direct = (5.0 / 3.0) * (n**3)\n        c_iter = 4.0 * k * (n**2)\n        \n        # The ratio rho simplifies to 12*k / (5*n)\n        if c_direct == 0:\n            # Handle n=0 case to avoid division by zero\n            rho = np.inf if c_iter > 0 else 0.0\n        else:\n            rho = c_iter / c_direct\n            \n        results.extend([epsilon, rho])\n\n    # Final print statement in the exact required format.\n    # The format is a string representing a list, with no spaces.\n    formatted_results = [f\"{val:.12g}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3147731"}]}