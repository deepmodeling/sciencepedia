{"hands_on_practices": [{"introduction": "A core tenet of deep learning is the stacking of layers to build expressive models. This exercise goes back to first principles to explore what happens in the simplest case: composing two linear layers. By analyzing the conditions under which these layers commute, you will uncover the concept of functional redundancy and gain a deeper appreciation for the essential role that non-linear activation functions play in enabling deep networks to learn complex patterns [@problem_id:3148079].", "problem": "Consider two linear layers in a feed-forward neural network, represented by real $3 \\times 3$ matrices $W_1$ and $W_2$. You will analyze when these layers commute by exhibiting a shared eigenbasis (simultaneous diagonalization), and interpret what this implies about functional redundancy in a network with and without a nonlinearity between the layers. Use only the fundamental definitions of matrix multiplication, eigenvalues and eigenvectors, and the elementwise action of nonlinearities.\n\nLet $W_1$ and $W_2$ be given by\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\nwhere $a,b,c,d,e,f \\in \\mathbb{R}$. These are already diagonal in the standard basis, which is a candidate shared eigenbasis.\n\nFirst, construct sufficient conditions under which two real matrices $W_1$ and $W_2$ commute by identifying a shared eigenbasis and explaining how simultaneous diagonalization enforces commutativity. Your construction must be based on the definitions of eigenvalues and eigenvectors, and on the properties of diagonal matrices, without assuming any specific shortcut theorems not derived from these.\n\nNext, consider the composition of the two linear layers without any nonlinearity between them, which acts as $x \\mapsto W_2 W_1 x$ for $x \\in \\mathbb{R}^3$. Explain the notion of functional redundancy in this linear case and, for the specific $W_1$ and $W_2$ given above, compute the scalar quantity $\\operatorname{tr}(W_2 W_1)$ as a closed-form analytic expression in $a,b,c,d,e,f$.\n\nFinally, insert a Rectified Linear Unit (ReLU) nonlinearity, defined as the elementwise map $\\sigma(z) = \\max\\{0,z\\}$, between the layers to form $x \\mapsto W_2 \\,\\sigma(W_1 x)$. In the shared-eigenbasis (which is the standard basis here), give the condition on $x$ under which this composition reduces to a single linear map on that input, and interpret what this means for functional redundancy of the two layers when a nonlinearity is present. You do not need to compute any numeric value for this part.\n\nYour final answer must be the single closed-form expression for $\\operatorname{tr}(W_2 W_1)$ in terms of $a,b,c,d,e,f$.", "solution": "We begin from the foundational definitions. Two linear operators represented by matrices $W_1$ and $W_2$ commute if and only if $W_1 W_2 = W_2 W_1$. A matrix $W$ is diagonalizable if there exists an invertible matrix $S$ such that $S^{-1} W S = D$, where $D$ is a diagonal matrix. A diagonal matrix $D$ has the property that $D u$ scales each coordinate of a vector $u$ independently, and diagonal matrices commute because multiplication of diagonal matrices is entrywise and therefore associative and commutative at the level of corresponding diagonal entries.\n\nConstructing sufficient conditions for commutation via simultaneous diagonalization proceeds as follows. Suppose $W_1$ and $W_2$ are diagonalizable and share a complete set of eigenvectors; that is, there exists an invertible matrix $S$ whose columns form a basis of eigenvectors common to both $W_1$ and $W_2$. Then we can write\n$$\nS^{-1} W_1 S = D_1, \\qquad S^{-1} W_2 S = D_2,\n$$\nwhere $D_1$ and $D_2$ are diagonal matrices comprising the eigenvalues of $W_1$ and $W_2$ with respect to the shared eigenbasis. Consider the products:\n$$\nW_1 W_2 = S D_1 S^{-1} S D_2 S^{-1} = S (D_1 D_2) S^{-1},\n$$\nand\n$$\nW_2 W_1 = S D_2 S^{-1} S D_1 S^{-1} = S (D_2 D_1) S^{-1}.\n$$\nBecause diagonal matrices commute, we have $D_1 D_2 = D_2 D_1$, which immediately implies $W_1 W_2 = W_2 W_1$. Hence, a sufficient condition for $W_1$ and $W_2$ to commute is that they be simultaneously diagonalizable by the same invertible matrix $S$ (equivalently, they share a complete eigenbasis). In particular, if $W_1$ and $W_2$ are both diagonal in the same basis, they commute.\n\nFor the specific matrices given,\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\nthe standard basis vectors are common eigenvectors, and both matrices are diagonal in that basis. Therefore, $W_1$ and $W_2$ commute. The composition without nonlinearity is the linear operator $W_2 W_1$, which is also diagonal with diagonal entries given by the products of corresponding entries:\n$$\nW_2 W_1 = \\begin{pmatrix}\nad  0  0 \\\\\n0  be  0 \\\\\n0  0  cf\n\\end{pmatrix}.\n$$\nFunctional redundancy in this linear case means that the two-layer linear subnetwork $x \\mapsto W_2 W_1 x$ can be replaced by a single linear layer $x \\mapsto W_{\\text{eff}} x$ with $W_{\\text{eff}} = W_2 W_1$, without changing the input-output mapping. To compute the requested scalar,\n$$\n\\operatorname{tr}(W_2 W_1) = ad + be + cf,\n$$\nby the definition of the trace as the sum of the diagonal entries.\n\nNow consider inserting a Rectified Linear Unit (ReLU) nonlinearity between the layers, forming the map $x \\mapsto W_2 \\,\\sigma(W_1 x)$ with $\\sigma$ applied elementwise: $\\sigma(z) = \\max\\{0,z\\}$. Because $W_1$ is diagonal,\n$$\nW_1 x = \\begin{pmatrix}\na x_1 \\\\ b x_2 \\\\ c x_3\n\\end{pmatrix}, \\qquad \\sigma(W_1 x) = \\begin{pmatrix}\n\\max\\{0, a x_1\\} \\\\ \\max\\{0, b x_2\\} \\\\ \\max\\{0, c x_3\\}\n\\end{pmatrix}.\n$$\nThe subsequent multiplication by $W_2$ scales each coordinate of $\\sigma(W_1 x)$ by $d$, $e$, and $f$ respectively. The composition $x \\mapsto W_2 \\,\\sigma(W_1 x)$ reduces to the single linear map $x \\mapsto W_2 W_1 x$ on a particular input $x$ exactly when $\\sigma$ acts identically on $W_1 x$, which occurs if and only if each coordinate of $W_1 x$ is nonnegative:\n$$\na x_1 \\ge 0, \\quad b x_2 \\ge 0, \\quad c x_3 \\ge 0.\n$$\nUnder these conditions, $\\sigma(W_1 x) = W_1 x$, so $W_2 \\,\\sigma(W_1 x) = W_2 W_1 x$, and the presence of the nonlinearity does not change the function on that input; the two-layer structure is functionally redundant for such $x$. If any of these coordinate-wise inequalities fail, then at least one component is truncated to zero by ReLU, and the overall mapping cannot be represented by a single fixed linear operator for all inputsâ€”functional redundancy is broken by the nonlinearity. In particular, global redundancy (for all $x$) would require that the domain of inputs be restricted to the set where $a x_1 \\ge 0$, $b x_2 \\ge 0$, and $c x_3 \\ge 0$ hold, or that the signs of $a$, $b$, and $c$ and the input coordinates be aligned so that ReLU acts as the identity everywhere in the domain of interest.\n\nTherefore, the requested scalar expression for the linear case is the trace computed above.", "answer": "$$\\boxed{ad + be + cf}$$", "id": "3148079"}, {"introduction": "Beyond simply stacking layers, we can engineer a network's behavior by imposing structural constraints on its weights. This practice explores a powerful method for doing so, using an orthogonal projection matrix $P$ to force a layer's output to reside within a specific, predefined subspace. By working through the implications of this constraint, you will investigate how it affects the model's expressive power, the geometry of its output space, and the flow of gradients during backpropagation [@problem_id:3148026].", "problem": "A fully connected linear layer in a deep network maps an input $x \\in \\mathbb{R}^{n}$ to an output $y \\in \\mathbb{R}^{m}$ via $y = W x$ with $W \\in \\mathbb{R}^{m \\times n}$. To enforce that the output lives in a prescribed subspace, training is modified so that after every parameter update the effective weight used in the forward pass is overwritten by\n$$\nW_{\\mathrm{eff}} \\leftarrow P W,\n$$\nwhere $P \\in \\mathbb{R}^{m \\times m}$ is a symmetric idempotent matrix, i.e., $P = P^{\\top}$ and $P^{2} = P$. Let $\\mathrm{range}(P)$ denote the column space of $P$, $\\mathrm{null}(P)$ its null space, and let $\\mathrm{rank}(P) = r$. Consider the function class realized by this layer under the constraint $W_{\\mathrm{eff}} = P W$ and analyze the expressivity relative to the unconstrained case.\n\nWhich of the following statements are true?\n\nA. For every target matrix $T \\in \\mathbb{R}^{m \\times n}$ satisfying $P T = T$, there exists a parameter matrix $W \\in \\mathbb{R}^{m \\times n}$ such that the effective weight equals $T$, i.e., $P W = T$.\n\nB. The realizable set $\\{P W : W \\in \\mathbb{R}^{m \\times n}\\}$ is equal to $\\{M \\in \\mathbb{R}^{m \\times n} : \\mathrm{col}(M) \\subseteq \\mathrm{null}(P)\\}$.\n\nC. The linear space of realizable effective weights has dimension $r n$.\n\nD. If this constrained layer is followed by a trainable linear layer with weight $U \\in \\mathbb{R}^{k \\times m}$ having full row rank $k$, then the composition $U (P W)$ can represent any matrix in $\\mathbb{R}^{k \\times n}$, so there is no expressivity loss.\n\nE. For any input $x \\in \\mathbb{R}^{n}$, the output $y = (P W) x$ necessarily lies in $\\mathrm{range}(P)^{\\perp}$.\n\nF. With the parameterization $W_{\\mathrm{eff}} = P W$, if $L$ is a scalar loss that depends on $W$ only through $W_{\\mathrm{eff}}$, then the gradient satisfies\n$$\n\\nabla_{W} L \\;=\\; P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L \\;=\\; P \\,\\nabla_{W_{\\mathrm{eff}}} L.\n$$\n\nSelect all that apply.", "solution": "The problem statement is analyzed and validated to be self-contained, mathematically sound, and well-posed. The concepts of linear algebra, such as idempotent and symmetric matrices, column spaces, null spaces, and rank, are standard. The application to a constrained linear layer in a neural network is a valid and specific context. The problem is therefore solvable as stated.\n\nThe core of the problem lies in understanding the properties of the matrix $P \\in \\mathbb{R}^{m \\times m}$. We are given that $P$ is symmetric ($P = P^{\\top}$) and idempotent ($P^2 = P$). A matrix with these two properties is an orthogonal projection matrix. It projects any vector in $\\mathbb{R}^m$ orthogonally onto its column space, denoted $\\mathrm{range}(P)$. The rank of this projection is given as $\\mathrm{rank}(P) = r$, which is the dimension of the subspace $\\mathrm{range}(P)$.\n\nKey properties of an orthogonal projection matrix $P$:\n1.  For any vector $v \\in \\mathrm{range}(P)$, it holds that $P v = v$.\n2.  For any vector $v \\in \\mathrm{range}(P)^{\\perp}$, it holds that $P v = 0$. Since $P$ is symmetric, its null space is the orthogonal complement of its range: $\\mathrm{null}(P) = \\mathrm{range}(P)^{\\perp}$.\n3.  Any vector $u \\in \\mathbb{R}^m$ can be uniquely decomposed as $u = v + w$, where $v \\in \\mathrm{range}(P)$ and $w \\in \\mathrm{null}(P)$. Applying $P$ gives $Pu = P(v+w) = Pv + Pw = v + 0 = v$.\n\nThe set of realizable effective weights is given by $\\mathcal{W}_{\\mathrm{eff}} = \\{P W : W \\in \\mathbb{R}^{m \\times n}\\}$.\nLet $M = PW$ be an effective weight matrix. The $j$-th column of $M$ is $M_{:,j} = P W_{:,j}$. Since $W_{:,j}$ is a vector in $\\mathbb{R}^m$, its projection $P W_{:,j}$ must lie in the column space of $P$, i.e., $M_{:,j} \\in \\mathrm{range}(P)$. This is true for all columns of $M$. Therefore, the column space of any realizable effective weight matrix is a subspace of $\\mathrm{range}(P)$: $\\mathrm{col}(M) \\subseteq \\mathrm{range}(P)$. This is equivalent to the condition $PM = M$, because applying $P$ to any column of $M$ leaves the column unchanged.\n\nNow we evaluate each statement.\n\n**A. For every target matrix $T \\in \\mathbb{R}^{m \\times n}$ satisfying $P T = T$, there exists a parameter matrix $W \\in \\mathbb{R}^{m \\times n}$ such that the effective weight equals $T$, i.e., $P W = T$.**\n\nThis statement asks if the equation $PW = T$ has a solution for $W$ given the condition $PT = T$.\nThe condition $PT=T$ means that the target matrix $T$ is in the set of feasible effective weights. We need to find if there exists a \"pre-image\" $W$ under the projection map $W \\mapsto PW$.\nLet's try a simple candidate solution: $W = T$.\nSubstituting this into the equation, we get $P W = P T$.\nBy the given condition, $P T = T$.\nTherefore, $P W = T$ is satisfied for the choice $W=T$.\nSince we have found at least one solution for $W$, the statement is true.\n\n**Verdict for A: Correct.**\n\n**B. The realizable set $\\{P W : W \\in \\mathbb{R}^{m \\times n}\\}$ is equal to $\\{M \\in \\mathbb{R}^{m \\times n} : \\mathrm{col}(M) \\subseteq \\mathrm{null}(P)\\}$.**\n\nAs established in the preamble, the realizable set is $\\mathcal{W}_{\\mathrm{eff}} = \\{PW : W \\in \\mathbb{R}^{m \\times n}\\}$. For any matrix $M \\in \\mathcal{W}_{\\mathrm{eff}}$, its columns must lie in $\\mathrm{range}(P)$.\nThe statement suggests the realizable set is $\\{M \\in \\mathbb{R}^{m \\times n} : \\mathrm{col}(M) \\subseteq \\mathrm{null}(P)\\}$.\nFor a matrix $M$ in this set, every column must be in the null space of $P$, which means $P M_{:,j} = 0$ for all $j$, or $PM = 0$.\nSo the statement claims $\\mathcal{W}_{\\mathrm{eff}} = \\{M \\in \\mathbb{R}^{m \\times n} : PM = 0\\}$.\nHowever, we know that for any $M \\in \\mathcal{W}_{\\mathrm{eff}}$, it holds that $PM = M$.\nSo the statement would imply that for any $W$, the matrix $M=PW$ must satisfy $M=0$. This is only true if $P$ is the zero matrix (i.e., $r=0$).\nIn general, for $r0$, we can construct a non-zero effective weight matrix. For example, let $v \\in \\mathrm{range}(P)$ be a non-zero vector, and let $W$ be a matrix whose first column is $v$. Then the first column of $PW$ is $Pv = v \\neq 0$, so $PW \\neq 0$.\nThe columns of a realizable matrix lie in $\\mathrm{range}(P)$, whereas the statement claims they lie in $\\mathrm{null}(P)$. Since $P$ is an orthogonal projection, $\\mathrm{range}(P) \\cap \\mathrm{null}(P) = \\{0\\}$. The two sets are fundamentally different (unless $r=0$).\n\n**Verdict for B: Incorrect.**\n\n**C. The linear space of realizable effective weights has dimension $r n$.**\n\nThe set of realizable effective weights is $\\mathcal{W}_{\\mathrm{eff}} = \\{M \\in \\mathbb{R}^{m \\times n} : PM=M\\}$. This is a vector space, as it is the kernel of the linear transformation $M \\mapsto M - PM$. We need to find its dimension.\nThe condition $PM=M$ is equivalent to requiring that each of the $n$ columns of $M$, denoted $M_{:,j}$, lies in the column space of $P$.\nThe subspace $\\mathrm{range}(P)$ has dimension $r$. So, for each column $M_{:,j}$, we must choose a vector from an $r$-dimensional space.\nLet $\\{p_1, \\dots, p_r\\}$ be a basis for $\\mathrm{range}(P)$. Then any column $M_{:,j}$ can be uniquely represented as a linear combination $M_{:,j} = \\sum_{k=1}^r c_{jk} p_k$. The coefficients $c_{j1}, \\dots, c_{jr}$ are the coordinates of $M_{:,j}$ in this basis.\nA matrix $M \\in \\mathcal{W}_{\\mathrm{eff}}$ is fully determined by the $n$ sets of $r$ coefficients, $\\{c_{jk}\\}_{k=1 \\dots r}^{j=1 \\dots n}$.\nThe total number of free parameters (or degrees of freedom) is the total number of such coefficients, which is $r \\times n$.\nThus, the dimension of the linear space $\\mathcal{W}_{\\mathrm{eff}}$ is $rn$.\n\n**Verdict for C: Correct.**\n\n**D. If this constrained layer is followed by a trainable linear layer with weight $U \\in \\mathbb{R}^{k \\times m}$ having full row rank $k$, then the composition $U (P W)$ can represent any matrix in $\\mathbb{R}^{k \\times n}$, so there is no expressivity loss.**\n\nThe matrix representing the composition of the two layers is $T_{\\mathrm{comp}} = U (PW)$. We need to determine if the set $\\{U(PW) : W \\in \\mathbb{R}^{m \\times n}, U \\in \\mathbb{R}^{k \\times m} \\text{ with rank } k\\}$ is equal to $\\mathbb{R}^{k \\times n}$.\nLet $W_{\\mathrm{eff}} = PW$. The rank of the effective weight matrix is bounded by the rank of $P$:\n$\\mathrm{rank}(W_{\\mathrm{eff}}) = \\mathrm{rank}(PW) \\le \\min(\\mathrm{rank}(P), \\mathrm{rank}(W)) \\le \\mathrm{rank}(P) = r$.\nNow, consider the rank of the composite matrix $T_{\\mathrm{comp}} = U W_{\\mathrm{eff}}$:\n$\\mathrm{rank}(T_{\\mathrm{comp}}) = \\mathrm{rank}(U W_{\\mathrm{eff}}) \\le \\min(\\mathrm{rank}(U), \\mathrm{rank}(W_{\\mathrm{eff}})) \\le \\mathrm{rank}(W_{\\mathrm{eff}})$.\nCombining these inequalities, we have $\\mathrm{rank}(T_{\\mathrm{comp}}) \\le r$.\nThe set of all matrices in $\\mathbb{R}^{k \\times n}$ contains matrices with rank up to $\\min(k, n)$. If we choose $k$ and $n$ such that $\\min(k,n)  r$, it is impossible to represent a matrix $T \\in \\mathbb{R}^{k \\times n}$ with $\\mathrm{rank}(T)  r$. For example, if $m=10$, $n=5$, $k=5$, and we have a projection with $r=2$, then the composite map can only produce matrices of rank at most $2$. It cannot produce an invertible $5 \\times 5$ matrix (if $n=k=5$), which has rank $5$.\nTherefore, the composition cannot represent any matrix in $\\mathbb{R}^{k \\times n}$ in general. There is a loss of expressivity determined by the rank $r$ of the projection $P$.\n\n**Verdict for D: Incorrect.**\n\n**E. For any input $x \\in \\mathbb{R}^{n}$, the output $y = (P W) x$ necessarily lies in $\\mathrm{range}(P)^{\\perp}$.**\n\nThe output of the layer is $y = (PW)x$.\nLet $W_{\\mathrm{eff}} = PW$. The output vector $y$ can be written as a linear combination of the columns of $W_{\\mathrm{eff}}$:\n$y = \\sum_{j=1}^n x_j (W_{\\mathrm{eff}})_{:,j}$, where $x_j$ are the components of the input vector $x$.\nAs shown before, every column $(W_{\\mathrm{eff}})_{:,j} = P W_{:,j}$ is a vector in $\\mathrm{range}(P)$.\nSince $\\mathrm{range}(P)$ is a vector subspace, any linear combination of vectors from this subspace must also be in the subspace.\nThus, $y \\in \\mathrm{range}(P)$ for any input $x$.\nThe statement claims that $y \\in \\mathrm{range}(P)^{\\perp}$. Since $P=P^\\top$, $\\mathrm{range}(P)^\\perp = \\mathrm{null}(P)$.\nA vector can be in both $\\mathrm{range}(P)$ and $\\mathrm{range}(P)^{\\perp}$ only if it is the zero vector. So the statement would imply $y=0$ for all $x$, which is not generally true. The statement is the direct opposite of the truth.\n\n**Verdict for E: Incorrect.**\n\n**F. With the parameterization $W_{\\mathrm{eff}} = P W$, if $L$ is a scalar loss that depends on $W$ only through $W_{\\mathrm{eff}}$, then the gradient satisfies $\\nabla_{W} L \\;=\\; P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L \\;=\\; P \\,\\nabla_{W_{\\mathrm{eff}}} L$.**\n\nWe are asked to compute the gradient of the loss $L$ with respect to the parameters $W$. The loss $L$ is a function of $W_{\\mathrm{eff}}$, which in turn is a function of $W$: $L(W) = L(W_{\\mathrm{eff}}(W))$.\nWe use the chain rule for matrix calculus. The differential of $L$ can be written as:\n$dL = \\mathrm{Tr}((\\nabla_{W} L)^\\top dW)$.\nAlso, using the chain rule, $dL = \\mathrm{Tr}((\\nabla_{W_{\\mathrm{eff}}} L)^\\top dW_{\\mathrm{eff}})$.\nThe relationship between the differentials $dW$ and $dW_{\\mathrm{eff}}$ is found by differentiating $W_{\\mathrm{eff}} = PW$:\n$dW_{\\mathrm{eff}} = d(PW) = P(dW)$, since $P$ is a constant matrix.\nSubstituting this into the expression for $dL$:\n$dL = \\mathrm{Tr}((\\nabla_{W_{\\mathrm{eff}}} L)^\\top P dW)$.\nUsing the cyclic property of the trace, $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB)$, we can write:\n$dL = \\mathrm{Tr}(P dW (\\nabla_{W_{\\mathrm{eff}}} L)^\\top)$. This isn't the form we want. Let's use $\\mathrm{Tr}(AB) = \\mathrm{Tr}(BA)$ and $\\mathrm{Tr}(A^\\top) = \\mathrm{Tr}(A)$.\nLet $A = (\\nabla_{W_{\\mathrm{eff}}} L)^\\top P$ and $B = dW$.\n$dL = \\mathrm{Tr}(A B)$. We want to write this as $\\mathrm{Tr}(C^\\top B)$.\nWe have $A^\\top = ((\\nabla_{W_{\\mathrm{eff}}} L)^\\top P)^\\top = P^\\top ((\\nabla_{W_{\\mathrm{eff}}} L)^\\top)^\\top = P^\\top \\nabla_{W_{\\mathrm{eff}}} L$.\nSo we can write $A = (A^\\top)^\\top = (P^\\top \\nabla_{W_{\\mathrm{eff}}} L)^\\top$.\nThen $dL = \\mathrm{Tr}((P^\\top \\nabla_{W_{\\mathrm{eff}}} L)^\\top dW)$.\nBy comparing this with $dL = \\mathrm{Tr}((\\nabla_{W} L)^\\top dW)$, we identify the gradients:\n$\\nabla_{W} L = P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L$.\nWe are given that $P$ is symmetric, so $P^{\\top} = P$.\nTherefore, $\\nabla_{W} L = P \\nabla_{W_{\\mathrm{eff}}} L$.\nThe statement claims $\\nabla_{W} L = P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L = P \\nabla_{W_{\\mathrm{eff}}} L$, which is consistent with our derivation.\n\n**Verdict for F: Correct.**\n\nFinal summary: Statements A, C, and F are true.", "answer": "$$\\boxed{ACF}$$", "id": "3148026"}, {"introduction": "To control the complexity of a model and ensure stable training, it is often necessary to measure and regularize the \"magnitude\" of its weight matrices. The spectral norm, $\\|W\\|_2$, provides a principled measure of how much a layer can \"stretch\" its inputs. This hands-on coding practice demystifies this important concept by guiding you through the implementation of the power iteration method, a classic numerical algorithm that estimates the spectral norm using only fundamental matrix-vector multiplications [@problem_id:3148029].", "problem": "You are given a real matrix $W \\in \\mathbb{R}^{m \\times n}$ representing a weight matrix from a linear layer in deep learning. The goal is to implement an algorithm that estimates the spectral norm (matrix operator $2$-norm) $\\|W\\|_2$ using only matrix multiplication and properties, grounded in fundamental definitions. The spectral norm is defined from first principles as\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm for vectors. The iterative procedure to estimate $\\|W\\|_2$ relies on the following steps: initialize a unit vector $v \\in \\mathbb{R}^{n}$, then repeat the following updates for a specified number of iterations $T$:\n$$\nu \\leftarrow \\frac{W v}{\\|W v\\|_2} \\quad \\text{if } \\|W v\\|_2 \\neq 0,\n$$\n$$\nv \\leftarrow \\frac{W^\\top u}{\\|W^\\top u\\|_2} \\quad \\text{if } \\|W^\\top u\\|_2 \\neq 0,\n$$\nand at the end report the estimate\n$$\n\\hat{\\sigma} = \\|W v\\|_2,\n$$\nwhich, under suitable conditions, approximates $\\|W\\|_2$. If either normalization denominator becomes zero during the process, replace the corresponding vector with a random unit vector of the appropriate dimension and continue.\n\nStarting only from the above definition and iterative procedure, write a complete program that:\n- Implements the described power iteration estimator for $\\|W\\|_2$, using only matrix multiplications and vector normalizations.\n- Uses a reproducible random number generator seeded with $12345$ whenever random unit vectors are needed.\n- Produces results for the following test suite, designed to cover general behavior and edge cases. In all cases, iteration counts $T$ are integers and dimensions are specified by positive integers.\n\nTest suite:\n$1.$ General rectangular case (happy path): Let $W_1 \\in \\mathbb{R}^{5 \\times 3}$ be filled with independent standard normal entries, generated with a fixed seed $12345$. Use $T = 50$. Output the estimated spectral norm $\\hat{\\sigma}_1$ as a floating-point number.\n\n$2.$ Identity boundary case: Let $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$. Use $T = 10$. Output the estimated spectral norm $\\hat{\\sigma}_2$ as a floating-point number.\n\n$3.$ Zero boundary case: Let $W_3 \\in \\mathbb{R}^{3 \\times 2}$ be the zero matrix. Use $T = 10$. Output the estimated spectral norm $\\hat{\\sigma}_3$ as a floating-point number.\n\n$4.$ Diagonal known-values case: Let $W_4 = \\mathrm{diag}(3,1,2) \\in \\mathbb{R}^{3 \\times 3}$. Use $T = 10$. Output a boolean indicating whether the estimated spectral norm is within an absolute tolerance of $10^{-6}$ of $3$, i.e., output\n$$\n\\left|\\hat{\\sigma}_4 - 3\\right| \\leq 10^{-6}.\n$$\n\n$5.$ Constructed rectangular matrix with known singular values: Construct $W_5 \\in \\mathbb{R}^{2 \\times 5}$ as $W_5 = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{2 \\times 2}$ and $V \\in \\mathbb{R}^{5 \\times 5}$ are orthonormal matrices obtained from the $\\mathrm{QR}$ decomposition of random Gaussian matrices (seed $12345$), and\n$$\n\\Sigma = \\begin{bmatrix}\n4  0  0  0  0 \\\\\n0  1  0  0  0\n\\end{bmatrix}.\n$$\nUse $T = 60$. Output the absolute difference $|\\hat{\\sigma}_5 - 4|$ as a floating-point number.\n\n$6.$ Scalar multiplication property check: Let $W_6 \\in \\mathbb{R}^{4 \\times 4}$ be random Gaussian (seed $12345$) and let $\\alpha = 0.5$. Use $T = 60$. Output a boolean indicating whether\n$$\n\\left|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\, \\hat{\\sigma}(W_6)\\right| \\leq 10^{-5},\n$$\nwhere $\\hat{\\sigma}(\\cdot)$ denotes the algorithmâ€™s estimate.\n\n$7.$ Submultiplicativity check with a vector: Using the same $W_1$ from the first test, draw a random vector $x \\in \\mathbb{R}^{3}$ (seed $12345$), and compute the exact $\\|W_1\\|_2$ using the singular value decomposition (SVD, singular value decomposition) for verification only in this property check. Output a boolean indicating whether\n$$\n\\|W_1 x\\|_2 \\leq \\|W_1\\|_2 \\cdot \\|x\\|_2.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must appear in the order of the tests $1$ through $7$, and each element must be of type float or boolean as specified. For example, your output should look like\n$[\\hat{\\sigma}_1,\\hat{\\sigma}_2,\\hat{\\sigma}_3,\\text{bool}_4,\\text{float}_5,\\text{bool}_6,\\text{bool}_7]$.", "solution": "The problem requires the implementation of an algorithm to estimate the spectral norm of a real matrix $W \\in \\mathbb{R}^{m \\times n}$. The spectral norm, or matrix $2$-norm, is defined as the maximum stretching factor the matrix applies to any unit vector:\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2\n$$\nThis norm is equivalent to the largest singular value of the matrix, denoted $\\sigma_{\\max}(W)$. The prescribed algorithm is a variant of the power iteration method, which is designed to find the eigenvector corresponding to the eigenvalue with the largest magnitude.\n\nThe algorithm's connection to the spectral norm can be understood by examining the matrix $A = W^\\top W \\in \\mathbb{R}^{n \\times n}$. This matrix is symmetric and positive semi-definite. Its eigenvalues $\\lambda_i(A)$ are real and non-negative, and they are related to the singular values of $W$ by $\\lambda_i(W^\\top W) = \\sigma_i(W)^2$. Consequently, the largest eigenvalue of $W^\\top W$ is the square of the largest singular value of $W$: $\\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2 = \\|W\\|_2^2$.\n\nThe power iteration method, when applied to $A = W^\\top W$, finds an eigenvector $v$ corresponding to $\\lambda_{\\max}(A)$. The iterative step for the power method is $v_k \\propto A v_{k-1} = (W^\\top W) v_{k-1}$. Let us analyze the procedure given in the problem statement:\n$1.$ $u_k \\leftarrow \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2}$\n$2.$ $v_k \\leftarrow \\frac{W^\\top u_k}{\\|W^\\top u_k\\|_2}$\n\nSubstituting the expression for $u_k$ into the update for $v_k$:\n$$\nv_k \\propto W^\\top u_k \\propto W^\\top \\left( \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2} \\right) \\propto W^\\top W v_{k-1}\n$$\nThis demonstrates that the sequence of vectors $\\{v_k\\}$ is generated by the power method applied to the matrix $W^\\top W$. Under suitable conditions (specifically, that the initial vector $v_0$ has a non-zero component in the direction of the eigenvector corresponding to $\\lambda_{\\max}(W^\\top W)$, and $\\lambda_{\\max}$ is strictly greater than other eigenvalues), the vector $v_k$ converges to the eigenvector of $W^\\top W$ associated with $\\lambda_{\\max}$. This eigenvector is the right-singular vector of $W$ corresponding to $\\sigma_{\\max}(W)$.\n\nThe final estimate for the spectral norm is $\\hat{\\sigma} = \\|W v_T\\|_2$, where $v_T$ is the vector after $T$ iterations. As $v_T$ approaches the dominant right-singular vector $v_{\\max}$, we have:\n$$\n\\|W v_T\\|_2^2 = v_T^\\top W^\\top W v_T \\xrightarrow{k \\to \\infty} v_{\\max}^\\top (W^\\top W) v_{\\max} = v_{\\max}^\\top (\\lambda_{\\max} v_{\\max}) = \\lambda_{\\max} \\|v_{\\max}\\|_2^2 = \\lambda_{\\max}(W^\\top W)\n$$\nSince $v_{\\max}$ is a unit vector, this gives $\\hat{\\sigma}^2 \\to \\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2$. Therefore, the estimate $\\hat{\\sigma} = \\|W v_T\\|_2$ converges to $\\sigma_{\\max}(W) = \\|W\\|_2$.\n\nThe algorithm must handle cases where a normalization factor is zero. If $\\|W v\\|_2 = 0$, it implies $Wv=0$, so $v$ is in the null space of $W$. If $\\|W^\\top u\\|_2 = 0$, then $u$ is in the null space of $W^\\top$. The problem specifies that in such an event, the vector to be normalized (i.e., $u$ or $v$) should be replaced by a random unit vector of the appropriate dimension. This prevents the iteration from getting stuck at the zero vector. A single reproducible random number generator, seeded with $12345$, must be used for all stochastic aspects of the implementation.\n\nThe solution will now proceed by implementing this algorithm and applying it to the seven specified test cases.\n\n$1.$ **General rectangular case**: A random matrix $W_1 \\in \\mathbb{R}^{5 \\times 3}$ is generated from a standard normal distribution. This is the \"happy path\" case where singular values are distinct and non-zero, for which the power method is expected to converge well. We run the iteration $T=50$ times and report the estimate $\\hat{\\sigma}_1$.\n\n$2.$ **Identity boundary case**: For $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$, the spectral norm is known to be $\\|I_4\\|_2 = 1$. For any unit vector $v$, $\\|I_4 v\\|_2 = \\|v\\|_2 = 1$. The algorithm will compute $u = \\frac{Iv}{\\|Iv\\|_2} = \\frac{v}{1} = v$, and then $v_{new} = \\frac{I^\\top u}{\\|I^\\top u\\|_2} = \\frac{I v}{\\|I v\\|_2} = v$. The vectors do not change, and the final estimate $\\|I_4 v\\|_2$ will be $1$ regardless of the initial vector. The result $\\hat{\\sigma}_2$ should be $1.0$.\n\n$3.$ **Zero boundary case**: For $W_3=0 \\in \\mathbb{R}^{3 \\times 2}$, the spectral norm is $\\|0\\|_2 = 0$. For any vector $v$, $W_3 v = 0$, so its norm is $0$. According to the specified procedure, $u$ is reset to a random unit vector. Then, $W_3^\\top u = 0$, so its norm is also $0$. The vector $v$ is then reset to a random unit vector. This process repeats for $T=10$ iterations. The final estimate is $\\hat{\\sigma}_3 = \\|W_3 v_T\\|_2 = \\|0 \\cdot v_T\\|_2 = 0$. The result should be $0.0$.\n\n$4.$ **Diagonal known-values case**: For a diagonal matrix $W_4 = \\mathrm{diag}(3,1,2)$, the spectral norm is the maximum absolute value of its diagonal entries, which is $\\max(|3|, |1|, |2|) = 3$. The algorithm is expected to converge to this value. The test checks if the estimate $\\hat{\\sigma}_4$ is within a tolerance of $10^{-6}$ of the true value $3$.\n\n$5.$ **Constructed rectangular matrix**: A matrix $W_5 \\in \\mathbb{R}^{2 \\times 5}$ is constructed from its Singular Value Decomposition (SVD), $W_5 = U \\Sigma V^\\top$. The matrices $U$ and $V$ are generated as orthonormal bases from random matrices, ensuring generality. The matrix $\\Sigma$ explicitly sets the singular values to $4$ and $1$. The spectral norm is the largest singular value, so $\\|W_5\\|_2 = 4$. The test computes the absolute difference between the estimate $\\hat{\\sigma}_5$ and the true value $4$.\n\n$6.$ **Scalar multiplication property check**: This test verifies the absolute homogeneity property of norms, $\\|\\alpha W\\|_2 = |\\alpha| \\|W\\|_2$. The algorithm is run on a random matrix $W_6$ and on $\\alpha W_6$ with $\\alpha=0.5$. The test verifies if the estimates obey this property, i.e., $|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\hat{\\sigma}(W_6)| \\leq 10^{-5}$. Our theoretical analysis above showed that this should hold up to floating-point precision, as the normalization steps cancel out the scalar factor $|\\alpha|$.\n\n$7.$ **Submultiplicativity check**: This test verifies the fundamental definition of an induced operator norm: $\\|W x\\|_2 \\leq \\|W\\|_2 \\|x\\|_2$ for any vector $x$. The check is performed using the matrix $W_1$ from the first test and a newly generated random vector $x$. Crucially, the problem requires using the *exact* spectral norm $\\|W_1\\|_2$, computed via a standard SVD library function for this verification, not the algorithm's estimate. This inequality is a mathematical truth, so the test should result in `True`, serving as a sanity check on the understanding of the concepts and their implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Master random number generator for reproducibility across all tests.\n    rng = np.random.default_rng(12345)\n    \n    # This is the core algorithm as described in the problem.\n    def estimate_spectral_norm(W, T, prng):\n        \"\"\"\n        Estimates the spectral norm of a matrix W using power iteration.\n\n        Args:\n            W (np.ndarray): The matrix for which to estimate the spectral norm.\n            T (int): The number of power iterations.\n            prng (np.random.Generator): A seeded random number generator.\n\n        Returns:\n            float: The estimated spectral norm of W.\n        \"\"\"\n        m, n = W.shape\n        \n        # Initialize v with a random unit vector.\n        v = prng.standard_normal(size=n)\n        v = v / np.linalg.norm(v)\n\n        for _ in range(T):\n            # Update u\n            Wv = W @ v\n            norm_Wv = np.linalg.norm(Wv)\n            if norm_Wv == 0:\n                u = prng.standard_normal(size=m)\n                u = u / np.linalg.norm(u)\n            else:\n                u = Wv / norm_Wv\n            \n            # Update v\n            Wtu = W.T @ u\n            norm_Wtu = np.linalg.norm(Wtu)\n            if norm_Wtu == 0:\n                v = prng.standard_normal(size=n)\n                v = v / np.linalg.norm(v)\n            else:\n                v = Wtu / norm_Wtu\n        \n        # Final estimate\n        sigma_hat = np.linalg.norm(W @ v)\n        return sigma_hat\n\n    results = []\n\n    # Test case 1: General rectangular case\n    W1 = rng.standard_normal(size=(5, 3))\n    sigma1_hat = estimate_spectral_norm(W1, 50, rng)\n    results.append(sigma1_hat)\n\n    # Test case 2: Identity boundary case\n    W2 = np.identity(4)\n    sigma2_hat = estimate_spectral_norm(W2, 10, rng)\n    results.append(sigma2_hat)\n\n    # Test case 3: Zero boundary case\n    W3 = np.zeros((3, 2))\n    sigma3_hat = estimate_spectral_norm(W3, 10, rng)\n    results.append(sigma3_hat)\n\n    # Test case 4: Diagonal known-values case\n    W4 = np.diag([3, 1, 2])\n    sigma4_hat = estimate_spectral_norm(W4, 10, rng)\n    bool4 = np.abs(sigma4_hat - 3) = 1e-6\n    results.append(bool4)\n\n    # Test case 5: Constructed rectangular matrix with known singular values\n    A_U = rng.standard_normal(size=(2, 2))\n    U, _ = np.linalg.qr(A_U)\n    A_V = rng.standard_normal(size=(5, 5))\n    V, _ = np.linalg.qr(A_V)\n    Sigma = np.zeros((2, 5))\n    Sigma[0, 0] = 4\n    Sigma[1, 1] = 1\n    W5 = U @ Sigma @ V.T\n    sigma5_hat = estimate_spectral_norm(W5, 60, rng)\n    float5 = np.abs(sigma5_hat - 4)\n    results.append(float5)\n\n    # Test case 6: Scalar multiplication property check\n    W6 = rng.standard_normal(size=(4, 4))\n    alpha = 0.5\n    sigma_W6_hat = estimate_spectral_norm(W6, 60, rng)\n    sigma_alphaW6_hat = estimate_spectral_norm(alpha * W6, 60, rng)\n    bool6 = np.abs(sigma_alphaW6_hat - np.abs(alpha) * sigma_W6_hat) = 1e-5\n    results.append(bool6)\n    \n    # Test case 7: Submultiplicativity check with a vector\n    # Using W1 from test case 1.\n    x = rng.standard_normal(size=3)\n    norm_W1x = np.linalg.norm(W1 @ x)\n    norm_x = np.linalg.norm(x)\n    # Compute the exact spectral norm using SVD\n    singular_values_W1 = np.linalg.svd(W1, compute_uv=False)\n    true_norm_W1 = singular_values_W1[0]\n    bool7 = norm_W1x = true_norm_W1 * norm_x\n    results.append(bool7)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) converts each element (float, bool) to its string representation\n    # e.g., True -> \"True\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3148029"}]}