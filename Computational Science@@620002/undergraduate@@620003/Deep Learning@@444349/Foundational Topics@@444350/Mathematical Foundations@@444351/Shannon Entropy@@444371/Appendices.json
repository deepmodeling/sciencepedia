{"hands_on_practices": [{"introduction": "Before training any deep learning model, it is crucial to understand the complexity of the data itself. Shannon entropy gives us a powerful tool to quantify this complexity, measuring the average uncertainty or information content per data point. This exercise guides you through a practical comparison of grayscale and color image data, demonstrating how adding color channels increases both the per-pixel entropy and, more dramatically, the number of samples required to reliably estimate the data distribution [@problem_id:3174049].", "problem": "You are given two classes of datasets for image classification in deep learning: grayscale images with a single channel and color images with three channels (red, green, blue). Each pixel is modeled as a discrete random variable taking values in a finite alphabet. In the grayscale case, a pixel is modeled by a single discrete random variable $X$ with a probability mass function over $m$ intensity bins. In the color case, a pixel is modeled by a triple of discrete random variables $(X_{R}, X_{G}, X_{B})$, one for each channel. Assume throughout that the channels are statistically independent at the pixel level.\n\nStarting from fundamental definitions, reason about and implement how the uncertainty in pixels differs between grayscale and color images, and how this difference influences the number of samples needed to reliably estimate the pixel distribution in a way useful for training deep classifiers. Specifically:\n\n- Use the standard definition of the Shannon entropy of a discrete random variable (expressed in bits, i.e., using logarithm base $2$) as the measure of uncertainty per pixel. For the color dataset, use only the independence property of entropy to obtain the per-pixel joint entropy from the channel entropies.\n- For sample complexity, use a well-tested fact from empirical distribution estimation: to ensure that the expected $\\ell_{1}$ error of the empirical distribution of a multinomial random variable is at most a tolerance $\\varepsilon$, the number of independent samples $n$ must satisfy an inequality of the form $n \\geq c \\cdot \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}}$, where $|\\mathcal{A}|$ is the alphabet size and $c$ is a constant that does not depend on $|\\mathcal{A}|$ or $\\varepsilon$. For the purpose of this problem, take $c = 1$ to obtain a conservative scaling law, and treat this as a lower bound proxy for the number of labeled examples a deep classifier would need to estimate pixel-level distributions to the desired tolerance. Your program should compute $n$ by rounding up to the nearest integer.\n\nYour task is to write a complete program that:\n1. Computes the per-pixel entropy in bits for a grayscale dataset and for a color dataset (assuming independence across channels).\n2. Computes the proxy sample complexity $n$ for the grayscale dataset and for the color dataset using the bound $n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil$.\n\nAll mathematical quantities must be handled precisely, with the following conventions:\n- For entropy, use base-$2$ logarithms so the unit is bits.\n- Use the convention $0 \\log 0 = 0$.\n- For the grayscale alphabet size, use $m = | \\text{bins for } X |$.\n- For the color alphabet size, use $m_{\\text{color}} = | \\text{bins for } X_{R} | \\cdot | \\text{bins for } X_{G} | \\cdot | \\text{bins for } X_{B} |$.\n\nImplement the following test suite of parameter values. Each test case specifies probability mass functions for grayscale and for each color channel, and a tolerance $\\varepsilon$:\n- Test case $1$ (happy path, uniform): Grayscale $p = [0.25, 0.25, 0.25, 0.25]$, Color channels identical to grayscale, $\\varepsilon = 0.05$.\n- Test case $2$ (boundary, degenerate grayscale): Grayscale $p = [1.0, 0.0, 0.0, 0.0]$, Color $p_{R} = p_{G} = p_{B} = [0.97, 0.01, 0.01, 0.01]$, $\\varepsilon = 0.05$.\n- Test case $3$ (heterogeneous channels): Grayscale $p = [0.10, 0.20, 0.30, 0.40]$, Color $p_{R} = [0.70, 0.10, 0.10, 0.10]$, $p_{G} = [0.50, 0.20, 0.20, 0.10]$, $p_{B} = [0.25, 0.25, 0.25, 0.25]$, $\\varepsilon = 0.02$.\n- Test case $4$ (small tolerance, identical distributions): Grayscale $p = [0.40, 0.40, 0.10, 0.10]$, Color channels identical to grayscale, $\\varepsilon = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list $[H_{\\text{gray}}, H_{\\text{color}}, n_{\\text{gray}}, n_{\\text{color}}]$, where $H_{\\text{gray}}$ and $H_{\\text{color}}$ are floats (in bits), and $n_{\\text{gray}}$ and $n_{\\text{color}}$ are integers. For example: $[[h_{1}, h_{1}^{c}, n_{1}, n_{1}^{c}], [h_{2}, h_{2}^{c}, n_{2}, n_{2}^{c}], \\dots]$. No units should be printed, but entropies must be computed in bits and $n$ is a dimensionless count of samples.", "solution": "The problem is valid as it is scientifically grounded in information theory and statistics, well-posed with all necessary information provided, and objective in its formulation. We will proceed with a step-by-step solution.\n\nThe problem asks us to compare the complexity of grayscale and color image data from an information-theoretic and statistical estimation perspective. We model a pixel's value as a discrete random variable and use Shannon entropy as the measure of uncertainty and a derived formula for sample complexity to estimate the underlying probability distribution.\n\n### 1. Per-Pixel Uncertainty: Shannon Entropy\n\nThe fundamental measure of uncertainty or \"information content\" of a discrete random variable $X$ with alphabet $\\mathcal{X}$ and probability mass function (PMF) $p(x) = P(X=x)$ is the Shannon entropy, $H(X)$. When using the logarithm to the base $2$, the entropy is measured in units of bits. The definition is:\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) $$\nBy convention, we define $0 \\log_2 0 = 0$ to handle outcomes with zero probability.\n\n#### Grayscale Image Entropy ($H_{\\text{gray}}$)\nA grayscale pixel is modeled by a single discrete random variable $X$ over an alphabet of $m$ intensity bins. Given a PMF $p_{\\text{gray}} = [p_1, p_2, \\ldots, p_m]$, the per-pixel entropy is calculated directly from the definition:\n$$ H_{\\text{gray}} = H(X) = - \\sum_{i=1}^{m} p_i \\log_2 p_i $$\n\n#### Color Image Entropy ($H_{\\text{color}}$)\nA color pixel is modeled as a vector of three random variables, $(X_R, X_G, X_B)$, one for each color channel. The problem states a critical assumption: the channels are statistically independent. For independent random variables, the joint entropy is simply the sum of their individual entropies:\n$$ H(X_R, X_G, X_B) = H(X_R) + H(X_G) + H(X_B) $$\nTherefore, the per-pixel entropy for a color image, $H_{\\text{color}}$, is given by the sum of the entropies of each channel's PMF ($p_R$, $p_G$, $p_B$):\n$$ H_{\\text{color}} = \\left( - \\sum_{i} p_{R,i} \\log_2 p_{R,i} \\right) + \\left( - \\sum_{j} p_{G,j} \\log_2 p_{G,j} \\right) + \\left( - \\sum_{k} p_{B,k} \\log_2 p_{B,k} \\right) $$\n\n### 2. Sample Complexity for Distribution Estimation\n\nThe problem provides a proxy for the number of samples $n$ required to reliably estimate the PMF of a multinomial random variable. This quantity, termed sample complexity, is given by the formula:\n$$ n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil $$\nwhere $|\\mathcal{A}|$ is the size of the alphabet (the number of possible outcomes) for the random variable, $\\varepsilon$ is the desired tolerance for the expected $\\ell_{1}$ error, and $\\lceil \\cdot \\rceil$ is the ceiling function, which rounds up to the nearest integer.\n\nThis formula reveals that the number of samples required grows linearly with the size of the alphabet, $|\\mathcal{A}|$.\n\n#### Grayscale Sample Complexity ($n_{\\text{gray}}$)\nFor a grayscale pixel, the random variable is $X$ and its alphabet size, $|\\mathcal{A}_{\\text{gray}}|$, is the number of intensity bins, $m$.\n$$ n_{\\text{gray}} = \\left\\lceil \\frac{m - 1}{\\varepsilon^{2}} \\right\\rceil $$\n\n#### Color Sample Complexity ($n_{\\text{color}}$)\nFor a color pixel, the random variable is the vector $(X_R, X_G, X_B)$. We are estimating the joint distribution of this vector. The alphabet of the joint distribution consists of all possible triples of channel values. If the channels have $m_R$, $m_G$, and $m_B$ bins respectively, the size of the joint alphabet is the product of these individual sizes:\n$$ |\\mathcal{A}_{\\text{color}}| = m_R \\cdot m_G \\cdot m_B $$\nThe sample complexity is then:\n$$ n_{\\text{color}} = \\left\\lceil \\frac{(m_R \\cdot m_G \\cdot m_B) - 1}{\\varepsilon^{2}} \\right\\rceil $$\nThis multiplicative growth in the alphabet size for higher-dimensional data is a manifestation of the \"curse of dimensionality.\" It demonstrates that estimating a joint distribution for color pixels requires a substantially larger number of samples than for a single grayscale channel, assuming the same tolerance $\\varepsilon$. The entropy, which is additive for independent variables, grows much more slowly than the sample complexity, which depends on the multiplicatively growing alphabet size. This distinction is a central point of the analysis.\n\n### Summary of Computations for Each Test Case\n1.  **Calculate $H_{\\text{gray}}$**: Apply the entropy formula to the grayscale PMF.\n2.  **Calculate $H_{\\text{color}}$**: Calculate the entropy for each color channel PMF and sum the results.\n3.  **Calculate $n_{\\text{gray}}$**: Determine the alphabet size $m$ from the length of the grayscale PMF and apply the sample complexity formula.\n4.  **Calculate $n_{\\text{color}}$**: Determine the alphabet sizes $m_R, m_G, m_B$ from the lengths of the channel PMFs, calculate the joint alphabet size $m_R \\cdot m_G \\cdot m_B$, and apply the sample complexity formula.\n\nThese steps will be systematically implemented for each test case provided.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes per-pixel entropy and sample complexity for grayscale and color image models\n    across a suite of test cases.\n    \"\"\"\n\n    # Test cases: (p_gray, p_red, p_green, p_blue, epsilon)\n    # The structure allows for cases where channel PMFs are distinct or identical to grayscale.\n    p1 = [0.25, 0.25, 0.25, 0.25]\n    p2_gray = [1.0, 0.0, 0.0, 0.0]\n    p2_color_channel = [0.97, 0.01, 0.01, 0.01]\n    p3_gray = [0.10, 0.20, 0.30, 0.40]\n    p3_r = [0.70, 0.10, 0.10, 0.10]\n    p3_g = [0.50, 0.20, 0.20, 0.10]\n    p3_b = [0.25, 0.25, 0.25, 0.25]\n    p4 = [0.40, 0.40, 0.10, 0.10]\n    \n    test_cases = [\n        (p1, p1, p1, p1, 0.05),\n        (p2_gray, p2_color_channel, p2_color_channel, p2_color_channel, 0.05),\n        (p3_gray, p3_r, p3_g, p3_b, 0.02),\n        (p4, p4, p4, p4, 0.01),\n    ]\n\n    results = []\n\n    def calculate_entropy(pmf: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Shannon entropy for a given probability mass function.\n        Handles the 0*log(0) = 0 case.\n        \"\"\"\n        # Filter out zero probabilities to avoid log(0)\n        pmf_nz = pmf[pmf > 0]\n        if pmf_nz.size == 0:\n            return 0.0\n        return -np.sum(pmf_nz * np.log2(pmf_nz))\n\n    def calculate_sample_complexity(alphabet_size: int, epsilon: float) -> int:\n        \"\"\"\n        Calculates the sample complexity n based on the given formula.\n        \"\"\"\n        if alphabet_size <= 1:\n            return 0\n        n = math.ceil((alphabet_size - 1) / (epsilon**2))\n        return int(n)\n\n    for case in test_cases:\n        p_gray_list, p_r_list, p_g_list, p_b_list, epsilon = case\n        \n        p_gray = np.array(p_gray_list, dtype=np.float64)\n        p_r = np.array(p_r_list, dtype=np.float64)\n        p_g = np.array(p_g_list, dtype=np.float64)\n        p_b = np.array(p_b_list, dtype=np.float64)\n\n        # 1. Grayscale Calculations\n        h_gray = calculate_entropy(p_gray)\n        m_gray = len(p_gray)\n        n_gray = calculate_sample_complexity(m_gray, epsilon)\n\n        # 2. Color Calculations\n        h_r = calculate_entropy(p_r)\n        h_g = calculate_entropy(p_g)\n        h_b = calculate_entropy(p_b)\n        h_color = h_r + h_g + h_b\n\n        m_r, m_g, m_b = len(p_r), len(p_g), len(p_b)\n        m_color = m_r * m_g * m_b\n        n_color = calculate_sample_complexity(m_color, epsilon)\n        \n        results.append([h_gray, h_color, n_gray, n_color])\n\n    # Format the final output string exactly as required, handling list-of-lists.\n    # The standard str() representation of a list includes spaces, which is fine here.\n    # The example f-string correctly joins string representations of the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3174049"}, {"introduction": "Real-world datasets are rarely perfectly balanced, and models trained on them can often neglect minority classes. The Shannon entropy of the class prior distribution, $H(Y)$, provides a single number summarizing the degree of this imbalance. This practice explores how entropy can be used as a diagnostic tool and tests a hypothetical 'inverse entropy' scaling method to see if it can improve a model's recognition of rare classes and its overall calibration [@problem_id:3174037].", "problem": "You are given a classification scenario inspired by ecological species identification. A fixed set of species classes is denoted by the random variable $Y$ taking values in $\\{0,1,\\dots,K-1\\}$. For each test case, you are provided with the number of samples per class, a matrix of model logits per sample, and the corresponding true labels. Your task is to compute the Shannon entropy of the class priors $H(Y)$ and to examine whether reweighting the logits by the inverse entropy improves minority class recognition and calibrates the model’s predicted probabilities.\n\nStart from the following fundamental bases:\n- The axioms of probability that define a discrete distribution over classes $Y$, where the class prior probabilities $p_k$ satisfy $p_k \\ge 0$ and $\\sum_{k=0}^{K-1} p_k = 1$.\n- The operational definition of self-information of an outcome $y$ as the logarithm of the inverse of its prior likelihood, and Shannon entropy $H(Y)$ as the expected self-information of the class label under the prior distribution.\n- The standard mapping from logits to probabilities via a normalized exponential, which yields a valid categorical distribution per sample.\n\nYou must implement the following computations, using the natural logarithm so that all information quantities are in nats:\n1. Compute the class prior probabilities $\\mathbf{p} = (p_0,\\dots,p_{K-1})$ from the provided counts $\\mathbf{c} = (c_0,\\dots,c_{K-1})$ by normalizing nonnegative counts to sum to one. Then compute the Shannon entropy $H(Y)$ of the prior distribution, in nats. Handle classes with zero counts numerically in a way that is consistent with the limiting behavior of the entropy.\n2. Using the provided logits for each sample, convert logits to predicted class probabilities via a normalized exponential function. Define the predicted class as the index of the largest probability for each sample. Compute two metrics:\n   - Minority recall: Define the set of minority classes as $\\{k : p_k < 1/K\\}$. Compute the per-class recall for each minority class $k$ as the fraction of samples with true label $k$ that are correctly predicted as class $k$, and report the macro-average over the minority classes. If the set of minority classes is empty, define minority recall to be the macro-average recall across all classes.\n   - Expected Calibration Error (ECE): Partition the interval $[0,1]$ into $B=10$ equal-width bins. For each bin, compute the fraction of samples whose predicted top-1 confidence falls in the bin, their empirical accuracy, and their average confidence. The ECE is the sum over bins of the bin fraction multiplied by the absolute difference between accuracy and average confidence.\n3. Perform “reweighting by inverse entropy” by scaling every sample’s logits by the scalar $s = 1/H(Y)$ prior to converting them to probabilities. Recompute the minority recall and ECE after this scaling. Determine whether the minority recall improved and whether the ECE improved (strictly decreased) relative to the unscaled baseline.\n\nTest Suite:\nFor each test case, $K=3$. You are provided with the class counts $\\mathbf{c}$, the logits matrix $\\mathbf{L}$ of shape $(N,K)$, and the true labels vector $\\mathbf{y}$ of length $N$, where $N = \\sum_k c_k$.\n\n- Test Case 1 (balanced priors):\n  - $\\mathbf{c} = [4,4,4]$.\n  - $\\mathbf{L}$ rows (each row is a length-$3$ vector of logits):\n    $[3.0,1.0,0.0]$, $[2.5,2.0,1.0]$, $[1.0,1.2,0.8]$, $[2.2,-0.5,0.1]$,\n    $[0.5,2.8,1.0]$, $[1.0,2.2,2.0]$, $[0.2,1.5,1.6]$, $[0.1,2.5,-0.3]$,\n    $[0.0,0.5,2.5]$, $[1.0,0.9,1.2]$, $[-0.2,1.3,1.1]$, $[0.3,-0.4,2.0]$.\n  - $\\mathbf{y}$ (true labels per row of $\\mathbf{L}$): $[0,0,0,0,1,1,1,1,2,2,2,2]$.\n\n- Test Case 2 (moderately imbalanced priors):\n  - $\\mathbf{c} = [8,3,1]$.\n  - $\\mathbf{L}$ rows:\n    $[3.4,0.1,-0.2]$, $[2.8,0.5,0.0]$, $[3.1,0.3,0.2]$, $[2.0,1.1,0.9]$,\n    $[1.8,1.5,1.2]$, $[2.5,1.6,1.6]$, $[1.7,1.9,1.6]$, $[2.6,0.8,0.7]$,\n    $[1.5,2.7,1.0]$, $[2.2,1.8,0.9]$, $[2.0,2.1,1.9]$, $[2.4,1.3,1.6]$.\n  - $\\mathbf{y}$: $[0,0,0,0,0,0,0,0,1,1,1,2]$.\n\n- Test Case 3 (highly imbalanced priors):\n  - $\\mathbf{c} = [10,1,1]$.\n  - $\\mathbf{L}$ rows:\n    $[3.5,0.2,0.1]$, $[3.0,0.5,0.4]$, $[2.9,0.1,-0.2]$, $[2.3,0.7,0.6]$,\n    $[2.6,1.0,0.9]$, $[3.2,0.8,0.7]$, $[2.8,0.4,0.3]$, $[3.1,1.2,1.0]$,\n    $[2.7,1.4,1.3]$, $[2.4,1.6,1.5]$, $[2.5,2.2,2.1]$, $[2.5,2.1,2.2]$.\n  - $\\mathbf{y}$: $[0,0,0,0,0,0,0,0,0,0,1,2]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list in the order:\n$[H(Y), \\text{minority\\_recall\\_before}, \\text{minority\\_recall\\_after}, \\text{ECE\\_before}, \\text{ECE\\_after}, \\text{improved\\_recall}, \\text{improved\\_ECE}]$,\nwhere the first five entries are floats (in nats for $H(Y)$; use the natural logarithm) and the last two entries are booleans. For example, the overall output should look like:\n$[[h_1,r^{\\text{min}}_{1,\\text{before}},r^{\\text{min}}_{1,\\text{after}},e_{1,\\text{before}},e_{1,\\text{after}},b^{\\text{recall}}_1,b^{\\text{ECE}}_1],[\\dots],[\\dots]]$.", "solution": "The problem requires an analysis of a classification model's outputs by computing Shannon entropy, minority class recall, and Expected Calibration Error (ECE), and then re-evaluating these metrics after a proposed logit scaling procedure. The solution is constructed by systematically applying the foundational principles of probability theory, information theory, and model evaluation.\n\nFirst, we define the class prior probabilities and the associated Shannon entropy. Given a set of class counts $\\mathbf{c} = (c_0, c_1, \\dots, c_{K-1})$ for $K$ classes from a total of $N$ samples, where $N = \\sum_{k=0}^{K-1} c_k$, the prior probability $p_k$ for each class $k$ is estimated as the empirical frequency:\n$$p_k = \\frac{c_k}{N}$$\nThis set of probabilities $\\mathbf{p} = (p_0, \\dots, p_{K-1})$ forms a discrete probability distribution. The Shannon entropy $H(Y)$ of the random variable $Y$ representing the class label is a measure of the uncertainty of this distribution. Using the natural logarithm (for units of nats), it is defined as the expected self-information:\n$$H(Y) = -\\sum_{k=0}^{K-1} p_k \\ln(p_k)$$\nIn this calculation, we respect the limit $\\lim_{x\\to 0^+} x \\ln(x) = 0$, so classes with zero counts do not contribute to the entropy.\n\nNext, we formalize the conversion of model logits to predictions. For a given sample, a model produces a vector of logits $\\mathbf{l} = (l_0, l_1, \\dots, l_{K-1})$. These real-valued scores are transformed into a valid probability distribution over the classes using the softmax function:\n$$P(Y=k|\\mathbf{l}) = \\frac{\\exp(l_k)}{\\sum_{j=0}^{K-1} \\exp(l_j)}$$\nThe predicted class $\\hat{y}$ is the one corresponding to the maximum probability, $\\hat{y} = \\arg\\max_k P(Y=k|\\mathbf{l})$, and the confidence of this prediction is this maximum probability, $\\text{conf} = \\max_k P(Y=k|\\mathbf{l})$.\n\nWith these predictions, we can evaluate model performance and calibration.\n1.  **Minority Recall**: This metric assesses the model's ability to correctly identify instances of under-represented classes. A class $k$ is defined as a minority class if its prior probability $p_k$ is less than the uniform prior, i.e., $p_k < 1/K$. The recall for a specific class $k$, $\\text{Recall}_k$, is the proportion of actual instances of class $k$ that are correctly predicted as class $k$. The overall minority recall is the macro-average of $\\text{Recall}_k$ over all minority classes. If the set of minority classes is empty (a balanced dataset), the metric is computed as the macro-average recall over all classes.\n\n2.  **Expected Calibration Error (ECE)**: This metric quantifies how well the model's prediction confidences reflect its actual accuracy. The confidence range $[0, 1]$ is divided into $B$ equal-width bins (here, $B=10$). For each bin $m$, we find all samples whose prediction confidence falls within that bin. We then compute the average accuracy, $\\text{acc}(b_m)$, and average confidence, $\\text{conf}(b_m)$, for the samples in that bin. The ECE is the weighted average of the absolute difference between these two quantities:\n    $$ \\text{ECE} = \\sum_{m=1}^{B} \\frac{|b_m|}{N} |\\text{acc}(b_m) - \\text{conf}(b_m)| $$\n    where $|b_m|$ is the number of samples in bin $m$ and $N$ is the total number of samples. A lower ECE implies better calibration.\n\nThe core of the problem is to test the hypothesis of \"reweighting by inverse entropy\". This involves scaling each sample's logit vector $\\mathbf{l}$ by a scalar $s = 1/H(Y)$. The new logits are $\\mathbf{l}' = s \\cdot \\mathbf{l}$. All metrics—probabilities, predictions, minority recall, and ECE—are recomputed using these scaled logits. This operation is analogous to adjusting the temperature of the softmax function to $T=H(Y)$. A high-entropy (uncertain) prior leads to a high temperature, \"softening\" the probabilities, while a low-entropy (peaked) prior leads to a low temperature, \"sharpening\" them.\n\nThe final step is to compare the performance before and after scaling. An improvement in recall is registered if $\\text{recall}_{\\text{after}} > \\text{recall}_{\\text{before}}$, and an improvement in calibration is registered if $\\text{ECE}_{\\text{after}} < \\text{ECE}_{\\text{before}}$. This procedure is applied to each test case to generate the required results.", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef calculate_metrics(logits, y_true, K, minority_classes_idx, n_bins=10):\n    \"\"\"\n    Calculates minority recall and ECE for a given set of logits and labels.\n    \"\"\"\n    N = len(y_true)\n    if N == 0:\n        return 0.0, 0.0\n\n    # 1. Get predictions and confidences\n    probs = softmax(logits, axis=1)\n    confidences = np.max(probs, axis=1)\n    y_pred = np.argmax(probs, axis=1)\n    correct = (y_pred == y_true)\n\n    # 2. Calculate Minority Recall\n    recall_set = minority_classes_idx\n    if not recall_set:  # If no minority classes, use all classes\n        recall_set = list(range(K))\n\n    recalls = []\n    for k in recall_set:\n        class_mask = (y_true == k)\n        n_class_samples = np.sum(class_mask)\n        if n_class_samples > 0:\n            tp = np.sum(correct[class_mask])\n            recall_k = tp / n_class_samples\n            recalls.append(recall_k)\n\n    minority_recall = np.mean(recalls) if recalls else 0.0\n\n    # 3. Calculate ECE\n    ece = 0.0\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    \n    for i in range(n_bins):\n        lower, upper = bin_boundaries[i], bin_boundaries[i+1]\n        \n        # The first bin is inclusive of 0.\n        if i == 0:\n             in_bin = (confidences >= lower) & (confidences <= upper)\n        else:\n             in_bin = (confidences > lower) & (confidences <= upper)\n        \n        num_in_bin = np.sum(in_bin)\n\n        if num_in_bin > 0:\n            accuracy_in_bin = np.mean(correct[in_bin])\n            confidence_in_bin = np.mean(confidences[in_bin])\n            ece += (num_in_bin / N) * np.abs(accuracy_in_bin - confidence_in_bin)\n\n    return minority_recall, ece\n\ndef process_case(c, L, y, K, n_bins=10):\n    \"\"\"\n    Processes a single test case according to the problem description.\n    \"\"\"\n    c = np.array(c, dtype=float)\n    L = np.array(L, dtype=float)\n    y = np.array(y, dtype=int)\n    \n    # 1. Compute prior probabilities and Shannon entropy H(Y)\n    N = np.sum(c)\n    p = c / N\n    p_nonzero = p[p > 0]\n    H_Y = -np.sum(p_nonzero * np.log(p_nonzero))\n\n    # 2. Identify minority classes\n    uniform_prob = 1.0 / K\n    minority_classes_idx = [k for k, pk in enumerate(p) if pk < uniform_prob]\n\n    # 3. Compute baseline metrics\n    recall_before, ece_before = calculate_metrics(L, y, K, minority_classes_idx, n_bins)\n    \n    # 4. Perform reweighting and recompute metrics\n    if H_Y == 0:\n        s = 1.0  # Avoid division by zero, effectively no scaling\n    else:\n        s = 1.0 / H_Y\n    \n    L_after = L * s\n    recall_after, ece_after = calculate_metrics(L_after, y, K, minority_classes_idx, n_bins)\n\n    # 5. Determine if metrics improved\n    improved_recall = recall_after > recall_before\n    improved_ece = ece_after < ece_before\n\n    return [H_Y, recall_before, recall_after, ece_before, ece_after, improved_recall, improved_ece]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"c\": [4, 4, 4],\n            \"L\": [\n                [3.0, 1.0, 0.0], [2.5, 2.0, 1.0], [1.0, 1.2, 0.8], [2.2, -0.5, 0.1],\n                [0.5, 2.8, 1.0], [1.0, 2.2, 2.0], [0.2, 1.5, 1.6], [0.1, 2.5, -0.3],\n                [0.0, 0.5, 2.5], [1.0, 0.9, 1.2], [-0.2, 1.3, 1.1], [0.3, -0.4, 2.0]\n            ],\n            \"y\": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\n            \"K\": 3\n        },\n        {\n            \"c\": [8, 3, 1],\n            \"L\": [\n                [3.4, 0.1, -0.2], [2.8, 0.5, 0.0], [3.1, 0.3, 0.2], [2.0, 1.1, 0.9],\n                [1.8, 1.5, 1.2], [2.5, 1.6, 1.6], [1.7, 1.9, 1.6], [2.6, 0.8, 0.7],\n                [1.5, 2.7, 1.0], [2.2, 1.8, 0.9], [2.0, 2.1, 1.9], [2.4, 1.3, 1.6]\n            ],\n            \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2],\n            \"K\": 3\n        },\n        {\n            \"c\": [10, 1, 1],\n            \"L\": [\n                [3.5, 0.2, 0.1], [3.0, 0.5, 0.4], [2.9, 0.1, -0.2], [2.3, 0.7, 0.6],\n                [2.6, 1.0, 0.9], [3.2, 0.8, 0.7], [2.8, 0.4, 0.3], [3.1, 1.2, 1.0],\n                [2.7, 1.4, 1.3], [2.4, 1.6, 1.5], [2.5, 2.2, 2.1], [2.5, 2.1, 2.2]\n            ],\n            \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n            \"K\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['c'], case['L'], case['y'], case['K'])\n        results.append(result)\n    \n    # Format the final output string exactly as specified\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{res[6]}]\" \n        for res in results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3174037"}, {"introduction": "Modern neural networks are often over-parameterized, containing redundant components that can be removed to create smaller, more efficient models. One compelling hypothesis is that channels or neurons with low-entropy activations carry little information and are thus good candidates for pruning. In this hands-on practice, you will implement an entropy-based pruning algorithm for a convolutional neural network, directly exploring the trade-off between model compression and predictive accuracy [@problem_id:3174074].", "problem": "Consider a Convolutional Neural Network (CNN) trained on the Canadian Institute for Advanced Research (CIFAR) $10$-class dataset. Let the activation of channel $c$ over a set of samples be a real-valued random variable $Z_c$. From first principles in information theory, the Shannon entropy of a discrete random variable $Z$ with probabilities $\\{p_i\\}_{i=1}^B$ is defined as $$H(Z) = -\\sum_{i=1}^{B} p_i \\log_2 p_i,$$ measured in bits. In practice, activations are continuous-valued; to estimate entropy, one may discretize $Z_c$ into $B$ bins and treat the empirical bin frequencies as probabilities. The core idea for pruning is to remove channels with low activation entropy, under the hypothesis that low-entropy channels carry little information and are therefore redundant. \n\nYour task is to implement and evaluate an entropy-based pruning criterion, purely as a computational procedure grounded in these definitions, with no external data access. Specifically:\n\n1. Given a collection of channel activations for a single layer, discretize each channel’s activation values into $B$ bins spanning the observed range for that channel, and compute the estimated Shannon entropy $H(Z_c)$ in bits using base-$2$ logarithms. Use additive smoothing to avoid zero probabilities by adding a small constant $\\varepsilon$ to each bin count before normalizing.\n\n2. Prune channel $c$ if and only if $H(Z_c) < \\tau$, where $\\tau$ is a user-specified threshold in bits. The pruning decision is strict inequality, so channels with $H(Z_c) = \\tau$ are not pruned.\n\n3. Quantify compression as the fraction of channels removed, i.e., $$\\text{compression\\_fraction} = \\frac{\\text{number of pruned channels}}{\\text{total number of channels}}.$$ This quantity is unitless.\n\n4. Estimate post-pruning accuracy on CIFAR-$10$ (expressed as a decimal between $0$ and $1$) by starting from a given baseline accuracy $A_0$ (before pruning) and subtracting a penalty proportional to the normalized information removed. For $B$ bins, the maximum possible entropy per channel is $$H_{\\max} = \\log_2(B).$$ Let $\\eta$ be a proportionality constant (unitless). Define the predicted accuracy after pruning as $$A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\\right)\\right),$$ where $\\mathcal{P}$ is the set of pruned channels. This clamps the predicted accuracy to the interval $[0,1]$. Report accuracy as a decimal, not using a percentage sign.\n\n5. To relate pruning to redundancy, also report the average entropy of pruned channels, $$\\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c), & \\text{if } |\\mathcal{P}| > 0,\\\\ 0, & \\text{otherwise.}\\end{cases}$$\n\nImplementation details:\n- Use $B$ bins per channel with equal-width binning across that channel’s activation range.\n- Use additive smoothing constant $\\varepsilon$ for all bins before normalization.\n- Use base-$2$ logarithms for entropy to report values in bits.\n\nTest suite:\nImplement and evaluate the following four test cases. In each case, channels are defined by deterministic sequences so your program produces fixed outputs.\n\nCase $1$ (happy path, moderate threshold):\n- Number of samples per channel $N = 100$.\n- Channels ($6$ total):\n  - Channel $0$: $z_n = 0$ for $n = 1, \\dots, N$.\n  - Channel $1$: periodic small values $z_n \\in \\{-0.02, 0, 0.02\\}$ repeating to length $N$.\n  - Channel $2$: uniform sequence $z_n = -1 + \\frac{2(n-1)}{N-1}$ for $n = 1, \\dots, N$.\n  - Channel $3$: bimodal sequence composed of two clusters: first $50$ samples linearly spaced in $[-0.6,-0.4]$, next $50$ samples linearly spaced in $[0.4,0.6]$.\n  - Channel $4$: constant small value $z_n = 0.1$ for all $n$.\n  - Channel $5$: sinusoidal sequence $z_n = 0.5 \\sin\\left(\\frac{8\\pi (n-1)}{N-1}\\right)$ for $n = 1, \\dots, N$.\n- Bins $B = 8$, threshold $\\tau = 0.5$ bits, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.92$, proportionality constant $\\eta = 0.03$.\n\nCase $2$ (boundary threshold, no pruning):\n- Same channels and $N$ as Case $1$.\n- Bins $B = 8$, threshold $\\tau = 0$, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.92$, proportionality constant $\\eta = 0.03$.\n\nCase $3$ (aggressive threshold, substantial pruning):\n- Same channels and $N$ as Case $1$.\n- Bins $B = 8$, threshold $\\tau = 2.5$ bits, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.92$, proportionality constant $\\eta = 0.03$.\n\nCase $4$ (different distribution shapes and bin count):\n- Number of samples per channel $N = 120$.\n- Channels ($4$ total):\n  - Channel $0$: $z_n = 0$ for all $n$.\n  - Channel $1$: uniform sequence $z_n = -2 + \\frac{4(n-1)}{N-1}$ for $n = 1, \\dots, N$.\n  - Channel $2$: heavy-tailed mixture: first $60$ samples linearly spaced in $[-0.1,0.1]$, next $30$ samples in $[-3,-2]$, and last $30$ samples in $[2,3]$.\n  - Channel $3$: near-constant with occasional variation: first $100$ samples $z_n = 1$, last $20$ samples linearly spaced in $[0.9,1.1]$.\n- Bins $B = 4$, threshold $\\tau = 1.0$ bits, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.90$, proportionality constant $\\eta = 0.025$.\n\nRequired output:\n- For each case, produce three floats in order: compression\\_fraction, predicted\\_accuracy, average\\_entropy\\_pruned.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_{12}]$ for $4$ cases with $3$ floats each, in the order of Case $1$, Case $2$, Case $3$, Case $4$).\n\nNo external files, user input, or network access are permitted. Use only the specified runtime environment.", "solution": "The problem requires the implementation of a deterministic computational procedure for channel pruning in a neural network based on the principle of Shannon entropy. The procedure involves calculating the entropy of channel activations, applying a pruning rule, and reporting several derived metrics: compression fraction, a predicted post-pruning accuracy, and the average entropy of the channels that were removed.\n\nThe validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. It is based on the standard definition of Shannon entropy from information theory. The method for estimating entropy for continuous variables—discretization via binning—is a common and valid, albeit approximate, technique. The formulas for the derived metrics are mathematically sound and unambiguous. The test cases are constructed using deterministic sequences, ensuring that the problem has a unique, verifiable solution. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe solution is implemented by following a sequence of steps for each test case provided.\n\nFirst, for each channel, we must generate the activation values as specified. The problem defines these activations not as random samples, but as deterministic sequences. For a given channel $c$, we have a set of $N$ activation values $\\{z_n\\}_{n=1}^N$.\n\nSecond, we estimate the Shannon entropy $H(Z_c)$ for each channel's activation variable $Z_c$. Since the activations are continuous-valued, we must first discretize them. The procedure specifies using $B$ equal-width bins over the observed range of activations for that specific channel. Let the minimum and maximum activation values for channel $c$ be $z_{\\min, c} = \\min_n(z_n)$ and $z_{\\max, c} = \\max_n(z_n)$. The range $[z_{\\min, c}, z_{\\max, c}]$ is divided into $B$ bins of equal width $w = (z_{\\max, c} - z_{\\min, c}) / B$. We then count the number of activations, $k_i$, that fall into each bin $i$, for $i = 1, \\dots, B$. A special case arises if all activations in a channel are constant, i.e., $z_{\\min, c} = z_{\\max, c}$. In this scenario, all $N$ activations fall into a single bin, so its count is $N$, and all other $B-1$ bins have a count of $0$.\n\nThird, to compute the probabilities required for the entropy formula, we must handle the possibility of zero counts, which would make the logarithm undefined. The problem mandates additive smoothing (also known as Laplace smoothing). A small constant $\\varepsilon$ is added to every bin count. The smoothed count for bin $i$ is $k'_i = k_i + \\varepsilon$. The total smoothed count is $N' = \\sum_{i=1}^{B} k'_i = (\\sum k_i) + B\\varepsilon = N + B\\varepsilon$. The probability for each bin is then estimated as $p_i = k'_i / N'$.\n\nWith these probabilities, the Shannon entropy for channel $c$ is calculated in bits using the base-$2$ logarithm:\n$$\nH(Z_c) = -\\sum_{i=1}^{B} p_i \\log_2 p_i\n$$\nThis calculation is performed for every channel in the layer.\n\nFourth, we apply the pruning criterion. A channel $c$ is marked for pruning if and only if its calculated entropy $H(Z_c)$ is strictly less than a given threshold $\\tau$. We identify the set of pruned channels, which we denote as $\\mathcal{P}$.\n$$\n\\mathcal{P} = \\{c \\mid H(Z_c) < \\tau\\}\n$$\n\nFifth, we compute the three required output metrics based on the set of pruned channels $\\mathcal{P}$.\n\n1.  **Compression Fraction**: This is the ratio of the number of pruned channels to the total number of channels, $C$.\n    $$\n    \\text{compression\\_fraction} = \\frac{|\\mathcal{P}|}{C}\n    $$\n\n2.  **Average Entropy of Pruned Channels**: This metric quantifies the average information content of the channels that were deemed redundant. It is defined as:\n    $$\n    \\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c), & \\text{if } |\\mathcal{P}| > 0,\\\\ 0, & \\text{otherwise.}\\end{cases}\n    $$\n    The case for $|\\mathcal{P}|=0$ ensures a well-defined output when no channels are pruned.\n\n3.  **Predicted Post-Pruning Accuracy**: This is an estimate based on a penalty model. The model starts with a baseline accuracy $A_0$ and subtracts a penalty proportional to the total normalized information removed by pruning. The maximum possible entropy for a distribution across $B$ bins is $H_{\\max} = \\log_2(B)$, which occurs when the distribution is uniform. The predicted accuracy, $A_{\\text{pred}}$, is given by:\n    $$\n    A_{\\text{pred}} = A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\n    $$\n    where $\\eta$ is a unitless proportionality constant. The final value is clamped to the interval $[0, 1]$ to ensure it remains a valid accuracy value:\n    $$\n    A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_{\\text{pred}}\\right)\\right)\n    $$\n\nBy systematically applying these steps to each of the four test cases defined in the problem statement, we can generate the required numerical results. Each step is a direct implementation of the provided formulas and rules.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the channel pruning problem for all test cases.\n    \"\"\"\n\n    def compute_metrics(activations_list, B, tau, epsilon, A0, eta):\n        \"\"\"\n        Computes entropy for each channel, prunes, and calculates metrics.\n        \"\"\"\n        num_channels = len(activations_list)\n        entropies = []\n        N = len(activations_list[0]) if num_channels > 0 else 0\n\n        for act in activations_list:\n            min_val, max_val = np.min(act), np.max(act)\n            \n            if min_val == max_val:\n                # All values are constant, they fall into a single bin.\n                counts = np.zeros(B)\n                counts[0] = N\n            else:\n                # Use numpy's histogram function for equal-width binning.\n                # The range is inclusive of min_val but exclusive of max_val,\n                # except for the last bin which is inclusive of max_val.\n                counts, _ = np.histogram(act, bins=B, range=(min_val, max_val))\n            \n            # Additive smoothing\n            smoothed_counts = counts.astype(np.float64) + epsilon\n            \n            # Normalize to get probabilities\n            total_smoothed_count = np.sum(smoothed_counts)\n            probs = smoothed_counts / total_smoothed_count\n            \n            # Calculate Shannon entropy in bits (log base 2)\n            # We filter out zero probabilities, though smoothing prevents this.\n            # This is just for robustness.\n            non_zero_probs = probs[probs > 0]\n            entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n            entropies.append(entropy)\n\n        # Identify pruned channels\n        pruned_indices = [i for i, h in enumerate(entropies) if h < tau]\n        pruned_entropies = [entropies[i] for i in pruned_indices]\n        \n        num_pruned = len(pruned_indices)\n        \n        # 1. Compression Fraction\n        compression_fraction = num_pruned / num_channels if num_channels > 0 else 0.0\n\n        # 2. Average Entropy of Pruned Channels\n        avg_entropy_pruned = np.mean(pruned_entropies) if num_pruned > 0 else 0.0\n\n        # 3. Predicted Post-Pruning Accuracy\n        H_max = np.log2(B)\n        # Avoid division by zero if B = 1, though problem B >= 4\n        if H_max == 0:\n            penalty = 0\n        else:\n            total_info_removed = np.sum(pruned_entropies)\n            penalty = eta * total_info_removed / H_max\n        \n        predicted_accuracy = A0 - penalty\n        predicted_accuracy = np.clip(predicted_accuracy, 0.0, 1.0)\n        \n        return compression_fraction, predicted_accuracy, avg_entropy_pruned\n\n    # --- Test Cases Definition ---\n    test_cases = [\n        {\n            \"id\": 1,\n            \"N\": 100, \"B\": 8, \"tau\": 0.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 2,\n            \"N\": 100, \"B\": 8, \"tau\": 0.0, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 3,\n            \"N\": 100, \"B\": 8, \"tau\": 2.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 4,\n            \"N\": 120, \"B\": 4, \"tau\": 1.0, \"epsilon\": 1e-12, \"A0\": 0.90, \"eta\": 0.025,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.linspace(-2.0, 2.0, n),\n                lambda n: np.concatenate([\n                    np.linspace(-0.1, 0.1, n // 2), \n                    np.linspace(-3.0, -2.0, n // 4), \n                    np.linspace(2.0, 3.0, n // 4)\n                ]),\n                lambda n: np.concatenate([\n                    np.full(100, 1.0),\n                    np.linspace(0.9, 1.1, 20)\n                ]),\n            ]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        activations_list = [gen(N) for gen in case[\"channels_def\"]]\n        \n        results = compute_metrics(\n            activations_list,\n            case[\"B\"],\n            case[\"tau\"],\n            case[\"epsilon\"],\n            case[\"A0\"],\n            case[\"eta\"]\n        )\n        all_results.extend(results)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "3174074"}]}