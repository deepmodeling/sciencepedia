{"hands_on_practices": [{"introduction": "The geometry of the loss landscape, particularly its curvature, is a critical factor in the performance of optimization algorithms like gradient descent. This practice explores how eigendecomposition of the Hessian matrix, which captures this curvature, provides a powerful analytical tool. You will investigate the effect of $L_2$ regularization, a ubiquitous technique in deep learning, and discover its elegant geometric interpretation: it systematically increases the curvature in all directions, making the optimization problem better-conditioned [@problem_id:3120548]. This exercise bridges theory and practice by having you first derive this property analytically and then verify it precisely through a numerical experiment you will build from scratch.", "problem": "Consider binary classification with empirical risk minimization (ERM) using the logistic loss for labels in $\\{0,1\\}$. Let $X \\in \\mathbb{R}^{n \\times d}$ be the design matrix whose rows are samples, $y \\in \\{0,1\\}^{n}$ be the label vector, and $w \\in \\mathbb{R}^{d}$ be the parameter vector. Define the logistic function $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ and the empirical risk $R(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(w; x_i, y_i)$ with $\\ell(w; x_i, y_i) = - y_i \\log \\sigma(x_i^\\top w) - (1 - y_i) \\log(1 - \\sigma(x_i^\\top w))$. Define the squared Euclidean norm penalty (L2) with strength $\\lambda \\ge 0$ by $P_\\lambda(w) = \\frac{\\lambda}{2} \\lVert w \\rVert_2^2$, and the regularized risk $R_\\lambda(w) = R(w) + P_\\lambda(w)$. The Hessian of a twice-differentiable function $F$ at $w$ is the matrix of second partial derivatives $\\nabla^2 F(w)$. Using only the definitions of $R(w)$, $P_\\lambda(w)$, and the rules of differentiation, analytically derive the Hessian of $R_\\lambda(w)$ in terms of the Hessian of $R(w)$ and identify how the eigenvalues of the Hessian are affected by the penalty strength $\\lambda$. Then, design and implement a numerical experiment that isolates this effect in training dynamics, meaning that you must evaluate the curvature effect at the same parameter iterate $w$ along an unregularized gradient descent trajectory to avoid confounding from different trajectories.\n\nYour program must implement the following steps from first principles:\n- Generate synthetic datasets $(X,y)$ by sampling $X$ with independent and identically distributed standard normal entries, sampling a ground-truth parameter $w_\\star$, computing logits $z = X w_\\star$, sampling $y_i \\sim \\mathrm{Bernoulli}(\\sigma(z_i))$, and returning $(X,y)$. Ensure reproducibility via fixed random seeds.\n- Run full-batch Gradient Descent (GD) on the unregularized empirical risk $R(w)$ for $K$ steps from initialization $w_0 = 0$, with a fixed learning rate $\\eta  0$.\n- At each GD iterate $w_k$, compute the Hessian $H_{\\text{unreg}}(w_k) = \\nabla^2 R(w_k)$, and the regularized Hessian at the same point $H_{\\text{reg}}(w_k,\\lambda) = \\nabla^2 R_\\lambda(w_k)$.\n- Compute the eigendecomposition of both Hessians to obtain their spectra. To isolate the penalty’s spectral effect, compare the spectra at the same $w_k$ by forming the differences of sorted eigenvalues. Aggregate, for each test case, the maximum absolute deviation over all eigenvalues and over all iterates:\n$$\n\\varepsilon_{\\max} = \\max_{0 \\le k  K} \\left( \\max_j \\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\Delta \\, \\right| \\right),\n$$\nwhere $\\Delta$ is the constant spectral shift you have identified analytically and $\\lambda_j(\\cdot)$ denotes the $j$-th eigenvalue in ascending order.\n- Report $\\varepsilon_{\\max}$ as a nonnegative real number for each test case.\n\nImplement the following test suite to cover a general case, boundary conditions, and a rank-deficient edge case:\n- Case A (happy path, full rank): $n = 200$, $d = 5$, seed $= 7$, $\\lambda = 0.05$, $K = 8$, $\\eta = 0.1$, full-rank $X$.\n- Case B (boundary, zero penalty): $n = 200$, $d = 5$, seed $= 7$, $\\lambda = 0$, $K = 8$, $\\eta = 0.1$, full-rank $X$.\n- Case C (edge, rank-deficient with modest penalty): $n = 150$, $d = 6$, seed $= 11$, $\\lambda = 0.05$, $K = 8$, $\\eta = 0.1$, construct $X$ with two identical columns to reduce rank by $1$.\n- Case D (edge, rank-deficient with large penalty): $n = 150$, $d = 6$, seed $= 11$, $\\lambda = 10.0$, $K = 8$, $\\eta = 0.1$, same rank-deficient construction as Case C.\n- Case E (small-scale variability check): $n = 60$, $d = 3$, seed $= 3$, $\\lambda = 0.2$, $K = 10$, $\\eta = 0.2$, full-rank $X$.\n\nAngle units and physical units are not applicable in this problem. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D, Case E]. Each entry must be the real number $\\varepsilon_{\\max}$ for that case, as defined above. For example, the format must be exactly like $[\\varepsilon_A,\\varepsilon_B,\\varepsilon_C,\\varepsilon_D,\\varepsilon_E]$.", "solution": "The problem statement is assessed to be valid as it is scientifically grounded in the principles of optimization and machine learning, is well-posed with a clear and numerically verifiable objective, and is free of contradictions or ambiguities.\n\nHerein, a complete solution is provided, comprising an analytical derivation followed by the design of a numerical experiment.\n\n### Analytical Derivation of the Hessian and its Spectral Shift\n\nThe objective is to derive the Hessian of the L2-regularized logistic regression risk, $R_\\lambda(w)$, and determine how its eigenvalues are affected by the regularization parameter $\\lambda$.\n\nThe regularized risk is defined as the sum of the empirical risk $R(w)$ and a penalty term $P_\\lambda(w)$:\n$$\nR_\\lambda(w) = R(w) + P_\\lambda(w)\n$$\nwhere $R(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(w; x_i, y_i)$ and $P_\\lambda(w) = \\frac{\\lambda}{2} \\lVert w \\rVert_2^2$.\n\nThe Hessian operator, $\\nabla^2$, is a linear operator. Therefore, the Hessian of the regularized risk is the sum of the Hessians of the empirical risk and the penalty term:\n$$\n\\nabla^2 R_\\lambda(w) = \\nabla^2 R(w) + \\nabla^2 P_\\lambda(w)\n$$\n\nLet us compute the Hessian of the penalty term, $\\nabla^2 P_\\lambda(w)$. The penalty term is $P_\\lambda(w) = \\frac{\\lambda}{2} \\sum_{j=1}^{d} w_j^2$.\nThe first partial derivative with respect to a component $w_k$ of the vector $w$ is:\n$$\n\\frac{\\partial P_\\lambda(w)}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\left( \\frac{\\lambda}{2} \\sum_{j=1}^{d} w_j^2 \\right) = \\frac{\\lambda}{2} \\cdot (2 w_k) = \\lambda w_k\n$$\nThis means the gradient is $\\nabla P_\\lambda(w) = \\lambda w$.\n\nThe second partial derivatives form the entries of the Hessian matrix. The $(j,k)$-th entry is:\n$$\n(\\nabla^2 P_\\lambda(w))_{jk} = \\frac{\\partial^2 P_\\lambda(w)}{\\partial w_j \\partial w_k} = \\frac{\\partial}{\\partial w_j} (\\lambda w_k)\n$$\nThis derivative is $\\lambda$ if $j=k$, and $0$ if $j \\neq k$. This can be written using the Kronecker delta, $\\delta_{jk}$.\n$$\n(\\nabla^2 P_\\lambda(w))_{jk} = \\lambda \\delta_{jk}\n$$\nThis is the definition of the matrix $\\lambda I_d$, where $I_d$ is the $d \\times d$ identity matrix. Thus, $\\nabla^2 P_\\lambda(w) = \\lambda I_d$.\n\nSubstituting this result back, we obtain the relationship between the regularized and unregularized Hessians:\n$$\n\\nabla^2 R_\\lambda(w) = \\nabla^2 R(w) + \\lambda I_d\n$$\nLet's denote the unregularized Hessian as $H_{\\text{unreg}}(w) = \\nabla^2 R(w)$ and the regularized Hessian as $H_{\\text{reg}}(w, \\lambda) = \\nabla^2 R_\\lambda(w)$. The relationship is:\n$$\nH_{\\text{reg}}(w, \\lambda) = H_{\\text{unreg}}(w) + \\lambda I_d\n$$\nNow, we analyze the effect of this relationship on the eigenvalues. Let $v$ be an eigenvector of the unregularized Hessian $H_{\\text{unreg}}(w)$ with a corresponding eigenvalue $\\mu$. By definition:\n$$\nH_{\\text{unreg}}(w) v = \\mu v\n$$\nApplying the regularized Hessian $H_{\\text{reg}}(w, \\lambda)$ to this eigenvector $v$:\n$$\nH_{\\text{reg}}(w, \\lambda) v = (H_{\\text{unreg}}(w) + \\lambda I_d) v = H_{\\text{unreg}}(w) v + \\lambda I_d v = \\mu v + \\lambda v = (\\mu + \\lambda) v\n$$\nThis demonstrates that $v$ is also an eigenvector of $H_{\\text{reg}}(w, \\lambda)$, and its corresponding eigenvalue is $\\mu + \\lambda$. Since the Hessians are real and symmetric, they possess a complete orthonormal basis of eigenvectors. This property holds for all eigenvectors, which implies that the entire spectrum of the regularized Hessian is shifted by a constant value $\\lambda$ relative to the spectrum of the unregularized Hessian.\n\nThe constant spectral shift identified is therefore $\\Delta = \\lambda$. The numerical experiment is designed to verify that the quantity $\\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\lambda \\, \\right|$ is close to zero, where differences from zero are attributable to floating-point precision errors.\n\n### Numerical Experiment Design\n\nThe numerical experiment is structured to isolate and verify this analytical finding.\n\n1.  **Data Generation**: Synthetic datasets $(X, y)$ are generated with fixed random seeds for reproducibility. The feature matrix $X \\in \\mathbb{R}^{n \\times d}$ is sampled from a standard normal distribution. A ground-truth parameter vector $w_\\star \\in \\mathbb{R}^d$ is also sampled. Logits $z = X w_\\star$ are computed, which are then passed through the logistic function $\\sigma(t) = (1+e^{-t})^{-1}$ to produce probabilities $p_i = \\sigma(z_i)$. The binary labels $y_i$ are then sampled from a Bernoulli distribution, $y_i \\sim \\text{Bernoulli}(p_i)$. For rank-deficient cases, two columns of $X$ are made identical, reducing its rank by at least one.\n\n2.  **Gradient Descent Trajectory**: Full-batch Gradient Descent (GD) is performed for $K$ iterations, starting from $w_0 = 0$. Crucially, the updates use the gradient of the *unregularized* risk $R(w)$, given by $\\nabla R(w) = \\frac{1}{n} X^\\top (\\sigma(Xw) - y)$. This ensures that the sequence of iterates $w_0, w_1, \\dots, w_{K-1}$ is independent of $\\lambda$, allowing for a controlled comparison.\n\n3.  **Hessian Evaluation**: At each iterate $w_k$ along this trajectory, both the unregularized and regularized Hessians are computed. The unregularized Hessian of the logistic risk is given by the formula:\n    $$\n    H_{\\text{unreg}}(w_k) = \\nabla^2 R(w_k) = \\frac{1}{n} X^\\top S_k X\n    $$\n    where $S_k$ is a diagonal matrix whose diagonal entries are $S_{ii} = p_{k,i}(1-p_{k,i})$, with $p_k = \\sigma(Xw_k)$. The regularized Hessian is then found by $H_{\\text{reg}}(w_k, \\lambda) = H_{\\text{unreg}}(w_k) + \\lambda I_d$.\n\n4.  **Spectral Analysis and Metric Calculation**: For each pair of Hessians computed at step $k$, their eigenvalues are calculated using a numerical eigensolver for symmetric matrices, which returns them sorted in ascending order. Let these sorted eigenvalues be $\\lambda_j(H_{\\text{unreg}}(w_k))$ and $\\lambda_j(H_{\\text{reg}}(w_k, \\lambda))$. The core of the analysis is to compute the deviation from the theoretical expectation:\n    $$\n    d_{k,j} = \\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\lambda \\, \\right|\n    $$\n    The final reported metric, $\\varepsilon_{\\max}$, is the maximum of these deviations over all eigenvalues $j$ and all GD steps $k$:\n    $$\n    \\varepsilon_{\\max} = \\max_{0 \\le k  K} \\left( \\max_j d_{k,j} \\right)\n    $$\n    This value quantifies the maximum discrepancy between the numerical results and the analytical theory, which should be on the order of machine precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef run_experiment(n, d, seed, lambd, K, eta, rank_deficient):\n    \"\"\"\n    Runs one instance of the numerical experiment to measure the spectral shift\n    of the Hessian due to L2 regularization.\n\n    Args:\n        n (int): Number of samples.\n        d (int): Number of features.\n        seed (int): Random seed for reproducibility.\n        lambd (float): L2 regularization strength.\n        K (int): Number of Gradient Descent steps.\n        eta (float): Learning rate for Gradient Descent.\n        rank_deficient (bool): If True, the data matrix X is made rank-deficient.\n\n    Returns:\n        float: The maximum absolute deviation, epsilon_max.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Generate synthetic dataset\n    X = np.random.randn(n, d)\n    if rank_deficient:\n        # Induce rank deficiency by making two columns identical.\n        X[:, -1] = X[:, -2]\n\n    # Sample a ground-truth parameter vector\n    w_star = np.random.randn(d, 1)\n\n    # Define logistic function with clipping for numerical stability\n    def sigma(t):\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    # Compute true probabilities and sample binary labels\n    z_true = X @ w_star\n    p_true = sigma(z_true)\n    y = (np.random.rand(n, 1)  p_true).astype(float)\n\n    # 2. Run Gradient Descent on the UNREGULARIZED risk R(w)\n    w = np.zeros((d, 1))\n    max_deviations_across_steps = []\n\n    for k in range(K):\n        # 3. At each iterate w_k, compute and analyze Hessians\n        \n        # Predictions at current iterate\n        z_k = X @ w\n        p_k = sigma(z_k)\n\n        # Compute the unregularized Hessian: H_unreg = (1/n) * X^T * S * X\n        # where S is a diagonal matrix with S_ii = p_k_i * (1 - p_k_i)\n        s_k = p_k * (1 - p_k)  # Element-wise product, shape (n, 1)\n        # Efficient computation using broadcasting: (X.T * s_k.T) is shape (d, n)\n        H_unreg = (X.T * s_k.T) @ X / n\n\n        # Compute the regularized Hessian: H_reg = H_unreg + lambda * I\n        H_reg = H_unreg + lambd * np.eye(d)\n\n        # 4. Compute eigendecompositions and find deviations\n        # linalg.eigh returns eigenvalues sorted in ascending order.\n        eigvals_unreg = linalg.eigh(H_unreg, eigvals_only=True)\n        eigvals_reg = linalg.eigh(H_reg, eigvals_only=True)\n        \n        # The theoretical spectral shift is Delta = lambda.\n        Delta = lambd\n        \n        # Deviations from the theoretical shift\n        deviations = np.abs(eigvals_reg - eigvals_unreg - Delta)\n        \n        # Store the maximum deviation for the current step k\n        if deviations.size  0:\n            max_deviations_across_steps.append(np.max(deviations))\n        else: # Handle d=0 case, though not used in test suite\n            max_deviations_across_steps.append(0.0)\n\n        # 5. Perform GD update using the gradient of the UNREGULARIZED risk\n        grad = X.T @ (p_k - y) / n\n        w = w - eta * grad\n        \n    # Aggregate result: find the maximum deviation over all steps\n    if not max_deviations_across_steps:\n        return 0.0\n    epsilon_max = np.max(max_deviations_across_steps)\n    return epsilon_max\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, d, seed, lambda, K, eta, rank_deficient_flag)\n        (200, 5, 7, 0.05, 8, 0.1, False),   # Case A: happy path, full rank\n        (200, 5, 7, 0.0, 8, 0.1, False),    # Case B: boundary, zero penalty\n        (150, 6, 11, 0.05, 8, 0.1, True),   # Case C: edge, rank-deficient\n        (150, 6, 11, 10.0, 8, 0.1, True),   # Case D: edge, rank-deficient, large lambda\n        (60, 3, 3, 0.2, 10, 0.2, False)     # Case E: small-scale check\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_experiment(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120548"}, {"introduction": "While gradient descent is excellent at finding areas of low loss, it can struggle in the complex, non-convex landscapes of neural networks, often slowing down at saddle points. These are deceptive regions with near-zero gradients but available \"escape routes\" along directions of negative curvature. This hands-on problem demonstrates how to use the eigenvalues and eigenvectors of the Hessian to build a more sophisticated optimizer [@problem_id:3120493]. You will implement an algorithm that not only detects these escape routes by identifying negative eigenvalues but also actively uses the corresponding eigenvectors to navigate away from saddle points, providing insight into the mechanics of advanced second-order optimization methods.", "problem": "You are given a deterministic toy training objective that captures a common nonconvex phenomenon found in deep learning: stationary saddle points with negative curvature. Consider the loss function in dimension $2$ defined for parameters $\\theta = (x,y) \\in \\mathbb{R}^{2}$ by\n$$\nf(\\theta) = \\frac{1}{4}\\left(x^{4}+y^{4}\\right) - \\frac{1}{2}x^{2} + \\frac{1}{2}y^{2}.\n$$\nThis objective has stationary points with both positive and negative curvature components. Your task is to design and implement a procedure that tracks negative eigenvalues $\\lambda_{i}  0$ of the Hessian $H(\\theta)$ during gradient-based training and escapes saddle behavior by moving along corresponding eigenvectors $v_{i}$ with $\\lambda_{i}  0$ when the first-order signal is weak.\n\nStart from the following fundamental base:\n- The gradient $\\nabla f(\\theta)$ is the vector of partial derivatives of $f$ with respect to $(x,y)$.\n- The Hessian $H(\\theta)$ is the $2 \\times 2$ symmetric matrix of second-order partial derivatives.\n- An eigenpair $(\\lambda, v)$ of $H(\\theta)$ satisfies $H(\\theta)\\,v=\\lambda\\,v$ with $v \\neq 0$.\n- A standard gradient descent step uses $\\theta_{t+1}=\\theta_{t}-\\alpha\\,\\nabla f(\\theta_{t})$ with step size $\\alpha0$.\n- The second-order Taylor expansion of $f$ around $\\theta$ in direction $v$ with scalar step $\\varepsilon$ is\n$$\nf(\\theta+\\varepsilon v)\\approx f(\\theta)+\\varepsilon\\,\\nabla f(\\theta)^{\\top}v+\\frac{1}{2}\\varepsilon^{2}v^{\\top}H(\\theta)v.\n$$\nWhen $v$ is an eigenvector with eigenvalue $\\lambda0$ and $\\|\\nabla f(\\theta)\\|$ is sufficiently small, the quadratic term dominates and small nonzero $\\varepsilon$ can decrease $f$.\n\nProblem requirements:\n- Implement an iterative training procedure that alternates between standard gradient descent and a principled negative-curvature escape step. At iteration $t$, compute $\\nabla f(\\theta_{t})$ and $H(\\theta_{t})$, perform an eigendecomposition of $H(\\theta_{t})$, and identify the smallest eigenvalue $\\lambda_{\\min}$ with a corresponding unit eigenvector $v_{\\min}$.\n- Use a simple decision rule: if $\\lambda_{\\min} -\\lambda_{\\mathrm{thresh}}$ and $\\|\\nabla f(\\theta_{t})\\| \\le g_{\\mathrm{thresh}}$, then perform a curvature escape $\\theta_{t+1}=\\theta_{t}+s_{\\mathrm{esc}}\\,\\tilde{v}$ where $\\tilde{v}$ is a unit eigenvector associated with $\\lambda_{\\min}$; otherwise perform a gradient descent step $\\theta_{t+1}=\\theta_{t}-\\alpha\\,\\nabla f(\\theta_{t})$.\n- The escape step size should be scaled to the local curvature magnitude using only the quantities available from the eigendecomposition and should be chosen to be small and safe according to the second-order Taylor model. You must ensure the step remains finite for all states with a small regularization if needed.\n- Track how many escape steps are taken.\n\nTest suite and hyperparameters:\n- Use the same hyperparameters for all test cases: step size $\\alpha = 0.1$, maximum iterations $T=200$, gradient-norm threshold $g_{\\mathrm{thresh}}=10^{-6}$, negative-curvature threshold $\\lambda_{\\mathrm{thresh}}=10^{-9}$, baseline escape magnitude $s_{0}=0.1$, and a small curvature regularizer $\\varepsilon=10^{-12}$ used only inside your escape-step scaling, if any.\n- The test cases are four initializations $\\theta_{0}$:\n    $$\n    \\theta_{0}^{(1)} = [0,0],\\quad\n    \\theta_{0}^{(2)} = [0.2,0],\\quad\n    \\theta_{0}^{(3)} = [0,0.5],\\quad\n    \\theta_{0}^{(4)} = [1.5,0.1].\n    $$\n- For each test case, run the iterative procedure for at most $T$ steps and report:\n    $1)$ the integer escape count,\n    $2)$ the boolean indicating whether the final Hessian has any negative eigenvalue,\n    $3)$ the final loss value $f(\\theta_{T})$ as a real number,\n    $4)$ the final parameter vector $\\theta_{T}$ as a list of real numbers.\n- Rounding: round all real-valued outputs to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-test-case result is a list in the order described above. For example:\n$$\n[\\,[\\text{escape\\_count}_{1},\\text{has\\_neg}_{1},f_{1},[\\text{theta}_{1,1},\\text{theta}_{1,2}]],\\dots,[\\text{escape\\_count}_{4},\\text{has\\_neg}_{4},f_{4},[\\text{theta}_{4,1},\\text{theta}_{4,2}]]\\,].\n$$\nNo units are involved in this problem. Angles are not used. Percentages are not used. Ensure the printed line matches the specified format with no additional text.", "solution": "The user has requested the design and implementation of an optimization procedure to handle saddle points in a nonconvex objective function, a common challenge in training deep learning models. The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution.\n\nThe objective function to be minimized is given by $f: \\mathbb{R}^{2} \\to \\mathbb{R}$, defined for parameters $\\theta = (x,y)$ as:\n$$\nf(\\theta) = f(x,y) = \\frac{1}{4}\\left(x^{4}+y^{4}\\right) - \\frac{1}{2}x^{2} + \\frac{1}{2}y^{2}\n$$\nOur task is to implement an iterative algorithm that starts from an initial point $\\theta_0$ and generates a sequence of iterates $\\theta_t$ that converge to a local minimum, effectively escaping any saddle points encountered.\n\nFirst, we must derive the first and second-order derivatives of the loss function, which are the gradient $\\nabla f(\\theta)$ and the Hessian matrix $H(\\theta)$, respectively. These are essential for the specified optimization algorithm.\n\nThe gradient is the vector of partial derivatives:\n$$\n\\nabla f(\\theta) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} x^3 - x \\\\ y^3 + y \\end{pmatrix}\n$$\nThe Hessian is the matrix of second-order partial derivatives:\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 3x^2 - 1  0 \\\\ 0  3y^2 + 1 \\end{pmatrix}\n$$\nThe Hessian is a diagonal matrix, which simplifies its analysis. Its eigenvalues are the diagonal entries: $\\lambda_1 = 3x^2 - 1$ and $\\lambda_2 = 3y^2 + 1$. The corresponding eigenvectors are the standard basis vectors, $v_1 = (1,0)^\\top$ and $v_2 = (0,1)^\\top$.\n\nStationary points are locations where the gradient is zero, i.e., $\\nabla f(\\theta) = 0$. For our function, this occurs when $x^3 - x = 0$ and $y^3 + y = 0$. The first equation gives $x \\in \\{-1, 0, 1\\}$, and the second requires $y=0$. Thus, the stationary points are $(-1,0)$, $(0,0)$, and $(1,0)$.\n\nThe nature of these stationary points is determined by the eigenvalues of the Hessian evaluated at these points:\n- At $\\theta = (\\pm 1, 0)$: $H = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$. The eigenvalues are $\\lambda_1=2$ and $\\lambda_2=1$. Both are positive, so these points are local minima. The loss at these minima is $f(\\pm 1, 0) = \\frac{1}{4} - \\frac{1}{2} = -0.25$.\n- At $\\theta = (0, 0)$: $H = \\begin{pmatrix} -1  0 \\\\ 0  1 \\end{pmatrix}$. The eigenvalues are $\\lambda_1=-1$ and $\\lambda_2=1$. Since there is one negative and one positive eigenvalue, this point is a saddle point. The direction of negative curvature is along the $x$-axis, associated with the eigenvector $v_1=(1,0)^\\top$.\n\nThe proposed algorithm is a hybrid strategy. At each iteration $t$ with current parameters $\\theta_t$:\n$1.$ We compute the gradient $\\nabla f(\\theta_t)$, its norm $\\|\\nabla f(\\theta_t)\\|$, and the Hessian $H(\\theta_t)$.\n$2.$ We perform an eigendecomposition of $H(\\theta_t)$ to find its smallest eigenvalue, $\\lambda_{\\min}$, and the corresponding unit eigenvector, $v_{\\min}$.\n$3.$ A decision is made based on two thresholds, $g_{\\mathrm{thresh}}$ for the gradient norm and $\\lambda_{\\mathrm{thresh}}$ for the negative curvature.\n\nThe update rule is as follows:\n- **If $\\lambda_{\\min}  -\\lambda_{\\mathrm{thresh}}$ and $\\|\\nabla f(\\theta_t)\\| \\le g_{\\mathrm{thresh}}$**: This condition indicates that we are near a stationary point (weak gradient) that has a significant direction of negative curvature. A standard gradient descent step would be very slow. To escape the saddle region, we perform a \"negative curvature escape\" step. This step moves the parameters along the direction of the eigenvector corresponding to the negative eigenvalue. The update rule is:\n$$\n\\theta_{t+1} = \\theta_t + s_{\\mathrm{esc}} v_{\\min}\n$$\nThe problem requires designing a step size $s_{\\mathrm{esc}}$ that is scaled by the curvature magnitude. A principled choice, inspired by trust-region methods, is to make the step size inversely proportional to the magnitude of the negative curvature, while using a baseline magnitude $s_0$ and a small regularization constant $\\varepsilon$ to prevent division by zero. Thus, we define the escape step size as:\n$$\ns_{\\mathrm{esc}} = \\frac{s_0}{|\\lambda_{\\min}| + \\varepsilon}\n$$\nThis design ensures a larger step when the curvature is nearly flat (small $|\\lambda_{\\min}|$) and a smaller, safer step when the curvature is sharp (large $|\\lambda_{\\min}|$). The direction is taken from the unit eigenvector $v_{\\min}$ returned by the eigendecomposition routine.\n\n- **Otherwise**: If the gradient is sufficiently large or if there is no significant negative curvature, the standard gradient descent update is more effective. This update is:\n$$\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n$$\nwhere $\\alpha$ is the learning rate (step size).\n\nThis iterative procedure is repeated for a maximum of $T=200$ iterations for each specified initial point $\\theta_0$. We will track the number of escape steps taken. After the final iteration, we report the required metrics: the total escape count, whether the final Hessian has a negative eigenvalue, the final loss value, and the final parameter vector, with all real values rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests a hybrid optimization algorithm that combines\n    gradient descent with a negative curvature escape mechanism to avoid\n    saddle points.\n    \"\"\"\n    # Hyperparameters\n    alpha = 0.1\n    T = 200\n    g_thresh = 1e-6\n    lambda_thresh = 1e-9\n    s0 = 0.1\n    epsilon = 1e-12\n\n    # Test cases: initial parameter vectors theta_0\n    thetas0 = [\n        np.array([0.0, 0.0]),\n        np.array([0.2, 0.0]),\n        np.array([0.0, 0.5]),\n        np.array([1.5, 0.1]),\n    ]\n\n    def f(theta):\n        \"\"\"Computes the loss function f(theta).\"\"\"\n        x, y = theta\n        return 0.25 * (x**4 + y**4) - 0.5 * x**2 + 0.5 * y**2\n\n    def grad_f(theta):\n        \"\"\"Computes the gradient of f(theta).\"\"\"\n        x, y = theta\n        return np.array([x**3 - x, y**3 + y])\n\n    def hess_f(theta):\n        \"\"\"Computes the Hessian of f(theta).\"\"\"\n        x, y = theta\n        # The Hessian is diagonal for this specific problem.\n        return np.array([[3 * x**2 - 1, 0.0], [0.0, 3 * y**2 + 1]])\n\n    all_results = []\n\n    for theta0 in thetas0:\n        theta = theta0.copy()\n        escape_count = 0\n\n        for _ in range(T):\n            # Compute gradient, Hessian, and its eigendecomposition\n            grad = grad_f(theta)\n            hess = hess_f(theta)\n            \n            # np.linalg.eigh is for symmetric matrices and returns sorted eigenvalues\n            eigenvalues, eigenvectors = np.linalg.eigh(hess)\n            lambda_min = eigenvalues[0]\n            v_min = eigenvectors[:, 0]\n            \n            grad_norm = np.linalg.norm(grad)\n\n            # Decision rule for which step to take\n            if lambda_min  -lambda_thresh and grad_norm = g_thresh:\n                # Curvature escape step\n                escape_count += 1\n                \n                # Scaled escape step size as per problem design principles\n                s_esc = s0 / (abs(lambda_min) + epsilon)\n                \n                # Update along the eigenvector of minimum eigenvalue\n                theta = theta + s_esc * v_min\n            else:\n                # Standard gradient descent step\n                theta = theta - alpha * grad\n        \n        # After T iterations, collect and format results\n        final_loss = f(theta)\n        \n        # Check final Hessian for negative eigenvalues\n        final_hess = hess_f(theta)\n        final_eigenvalues, _ = np.linalg.eigh(final_hess)\n        has_neg_eigenvalue = final_eigenvalues[0]  0\n\n        # Round all specified real-valued outputs to 6 decimal places\n        final_loss_rounded = round(final_loss, 6)\n        final_theta_rounded = [round(c, 6) for c in theta]\n        \n        result_for_case = [\n            escape_count,\n            bool(has_neg_eigenvalue), # Cast to standard Python boolean\n            final_loss_rounded,\n            final_theta_rounded\n        ]\n        all_results.append(result_for_case)\n\n    # Manually format the output string to match the exact requirement,\n    # avoiding extra spaces and using standard boolean/list representations.\n    result_strings = []\n    for res in all_results:\n        escape_count, has_neg, loss, theta_vec = res\n        theta_str = f\"[{theta_vec[0]},{theta_vec[1]}]\"\n        has_neg_str = 'True' if has_neg else 'False'\n        res_str = f\"[{escape_count},{has_neg_str},{loss},{theta_str}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3120493"}, {"introduction": "The power of curvature analysis extends beyond just optimizing a model; it can also be used to understand the model's structure and identify which components are most important. This principle is the foundation of many modern model compression and pruning techniques. In this exercise, you will explore the Rayleigh quotient, a scale-invariant measure of directional curvature, to quantify the importance of different directions in parameter space [@problem_id:3120472]. By implementing a pruning rule based on this curvature score, you will gain practical experience in how eigendecomposition-related concepts can be applied to create more efficient and compact neural networks.", "problem": "You are given a symmetric real matrix that represents the Hessian of a twice-differentiable loss function in a local quadratic approximation used in deep learning. Consider a nonzero parameter-direction vector and its associated curvature along that direction. Your tasks are to derive a scale-invariant directional curvature measure from first principles, connect it to eigendecomposition, and use it to design a pruning rule.\n\nStart from the following fundamental base:\n- For a twice-differentiable scalar loss function, the second-order Taylor expansion around a point yields, for a small step of size $\\alpha$ in direction $\\mathbf{v}$, a quadratic change that is governed by the quadratic form $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$, where $\\mathbf{H}$ is the symmetric Hessian matrix at the point of expansion.\n- The Hessian $\\mathbf{H}$ of a twice-differentiable scalar loss at a point is a real symmetric matrix, hence it admits a real eigendecomposition with an orthonormal eigenbasis and real eigenvalues.\n\nYour program must:\n- Construct a scale-invariant measure of directional curvature from the quadratic form, and use it as a curvature importance score for pruning. Explicitly justify scale-invariance from first principles in your method design.\n- Implement the pruning rule: prune a direction if and only if its curvature importance score is less than or equal to a given threshold $t$. For numerical robustness, if $\\mathbf{v}^{\\top}\\mathbf{v}  \\varepsilon$ for a prescribed $\\varepsilon$, define the curvature importance score to be $0$ by convention.\n- Use eigendecomposition to compute the smallest and largest eigenvalues of $\\mathbf{H}$ and verify, for each provided nonzero direction, that the implemented curvature importance score lies within the closed interval from the smallest eigenvalue to the largest eigenvalue. When checking interval membership, use a numerical tolerance $\\tau$ and treat a score $r$ as within bounds if $\\lambda_{\\min} - \\tau \\le r \\le \\lambda_{\\max} + \\tau$.\n\nTest suite and required outputs:\nImplement your solution on the following test cases. In each case, you are given a symmetric matrix $\\mathbf{H}$, a set of direction vectors, a threshold $t$, a small-norm cutoff $\\varepsilon$, and a bound-check tolerance $\\tau$.\n\n- Case $1$ (happy path, positive definite):\n  - $\\mathbf{H}_1 = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$.\n  - Directions: $\\mathbf{v}_{1,1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{1,2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $\\mathbf{v}_{1,3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n  - Threshold: $t_1 = 3.5$.\n  - Small-norm cutoff: $\\varepsilon = 10^{-12}$.\n  - Bound tolerance: $\\tau = 10^{-10}$.\n\n- Case $2$ (boundary equality, repeated eigenvalue):\n  - $\\mathbf{H}_2 = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}$.\n  - Directions: $\\mathbf{v}_{2,1} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\mathbf{v}_{2,2} = \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix}$.\n  - Threshold: $t_2 = 2$.\n  - Small-norm cutoff: $\\varepsilon = 10^{-12}$.\n  - Bound tolerance: $\\tau = 10^{-10}$.\n\n- Case $3$ (positive semidefinite with a nullspace and a near-zero vector):\n  - $\\mathbf{H}_3 = \\begin{bmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  3 \\end{bmatrix}$.\n  - Directions: $\\mathbf{v}_{3,1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{3,2} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{3,3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{3,4} = \\begin{bmatrix} 10^{-13} \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - Threshold: $t_3 = 0.5$.\n  - Small-norm cutoff: $\\varepsilon = 10^{-12}$.\n  - Bound tolerance: $\\tau = 10^{-10}$.\n\n- Case $4$ (indefinite curvature, saddle-like):\n  - $\\mathbf{H}_4 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$.\n  - Directions: $\\mathbf{v}_{4,1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $\\mathbf{v}_{4,2} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, $\\mathbf{v}_{4,3} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n  - Threshold: $t_4 = -0.5$.\n  - Small-norm cutoff: $\\varepsilon = 10^{-12}$.\n  - Bound tolerance: $\\tau = 10^{-10}$.\n\nFor each case $k \\in \\{1,2,3,4\\}$, your program must compute:\n- The integer count $c_k$ of pruned directions under the rule “prune if and only if curvature importance $\\le t_k$”.\n- A boolean flag $b_k$ that is true if and only if every computed curvature importance score for the case lies within the eigenvalue interval $[\\lambda_{\\min}(\\mathbf{H}_k), \\lambda_{\\max}(\\mathbf{H}_k)]$ up to the tolerance $\\tau$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$[c_1, b_1, c_2, b_2, c_3, b_3, c_4, b_4]$.", "solution": "The problem is valid. It is scientifically grounded in the principles of multivariable calculus and linear algebra as applied to optimization in deep learning, specifically concerning the analysis of the loss landscape via the Hessian matrix. The problem is well-posed, objective, and contains all necessary information to derive a unique, verifiable solution.\n\nThe core task is to develop and apply a scale-invariant measure of directional curvature. The change in a loss function $L$ for a small step of size $\\alpha$ in a direction $\\mathbf{v}$ from a point $\\mathbf{w}_0$ is approximated by the second-order Taylor expansion:\n$$ L(\\mathbf{w}_0 + \\alpha\\mathbf{v}) \\approx L(\\mathbf{w}_0) + \\alpha \\nabla L(\\mathbf{w}_0)^{\\top}\\mathbf{v} + \\frac{\\alpha^2}{2} \\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v} $$\nAt a critical point where the gradient $\\nabla L(\\mathbf{w}_0)$ is zero, the local change in loss is determined by the quadratic form $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$, where $\\mathbf{H}$ is the Hessian matrix evaluated at $\\mathbf{w}_0$. This term describes the curvature of the loss surface in the direction of $\\mathbf{v}$.\n\n**1. Derivation of the Scale-Invariant Curvature Importance Score**\n\nThe quadratic form $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ itself is not a suitable measure of directional curvature because it depends on the magnitude of the direction vector $\\mathbf{v}$. If we scale the vector by a nonzero constant $k$, the quadratic form changes:\n$$ (k\\mathbf{v})^{\\top}\\mathbf{H}(k\\mathbf{v}) = k^2(\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}) $$\nThe measure scales with $k^2$, meaning it reflects the \"length\" of the vector $\\mathbf{v}$ rather than just its direction. To create a measure that is intrinsic to the direction alone, we must normalize this quantity. A natural choice for normalization is the squared Euclidean norm of the vector, $\\|\\mathbf{v}\\|^2 = \\mathbf{v}^{\\top}\\mathbf{v}$. This leads to the **Rayleigh quotient**, which we define as the curvature importance score, $s(\\mathbf{v})$:\n$$ s(\\mathbf{v}) = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} $$\nThis score is scale-invariant for any nonzero scalar $k$:\n$$ s(k\\mathbf{v}) = \\frac{(k\\mathbf{v})^{\\top}\\mathbf{H}(k\\mathbf{v})}{(k\\mathbf{v})^{\\top}(k\\mathbf{v})} = \\frac{k^2(\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v})}{k^2(\\mathbf{v}^{\\top}\\mathbf{v})} = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} = s(\\mathbf{v}) $$\nThis property makes $s(\\mathbf{v})$ a pure measure of curvature in the direction of $\\mathbf{v}$, independent of its magnitude. For numerical stability, as per the problem statement, if the squared norm $\\mathbf{v}^{\\top}\\mathbf{v}$ is less than a small positive constant $\\varepsilon$, we define the score to be $0$. This convention treats directions of negligible magnitude as having zero curvature importance.\n\n**2. Connection to Eigendecomposition and Boundedness**\n\nThe Hessian matrix $\\mathbf{H}$ is given as real and symmetric. A fundamental property of such matrices is that they are orthogonally diagonalizable and have real eigenvalues. The Rayleigh-Ritz theorem establishes a direct connection between the Rayleigh quotient and the eigenvalues of $\\mathbf{H}$. The theorem states that the value of the Rayleigh quotient for any nonzero vector $\\mathbf{v}$ is bounded by the minimum and maximum eigenvalues of $\\mathbf{H}$:\n$$ \\lambda_{\\min}(\\mathbf{H}) \\le s(\\mathbf{v}) = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} \\le \\lambda_{\\max}(\\mathbf{H}) $$\nThe minimum value, $\\lambda_{\\min}(\\mathbf{H})$, is achieved when $\\mathbf{v}$ is an eigenvector corresponding to the smallest eigenvalue. Similarly, the maximum value, $\\lambda_{\\max}(\\mathbf{H})$, is achieved when $\\mathbf{v}$ is an eigenvector for the largest eigenvalue. This theorem provides a powerful tool for analyzing the range of possible curvatures and forms the basis for the verification step required by the problem. For each computed score $s$, we must verify that it lies within the interval $[\\lambda_{\\min}(\\mathbf{H}) - \\tau, \\lambda_{\\max}(\\mathbf{H}) + \\tau]$ for a given numerical tolerance $\\tau$.\n\n**3. Pruning Rule and Verification Procedure**\n\nThe implemented procedure for each test case adheres to the following logic:\n\n**For each case $k$ with Hessian $\\mathbf{H}_k$, directions $\\{\\mathbf{v}_{k,i}\\}$, threshold $t_k$, cutoff $\\varepsilon$, and tolerance $\\tau$:**\n1.  Initialize a counter for pruned directions, $c_k = 0$, and a boolean flag for bound verification, $b_k = \\text{true}$.\n2.  Perform an eigendecomposition of the symmetric matrix $\\mathbf{H}_k$ to find its real eigenvalues. Identify the minimum eigenvalue, $\\lambda_{\\min}$, and the maximum eigenvalue, $\\lambda_{\\max}$.\n3.  For each direction vector $\\mathbf{v}_{k,i}$:\n    a. Calculate the squared norm $n_i = \\mathbf{v}_{k,i}^{\\top}\\mathbf{v}_{k,i}$.\n    b. Compute the curvature importance score, $s_{k,i}$. If $n_i  \\varepsilon$, set $s_{k,i} = 0$. Otherwise, calculate $s_{k,i} = (\\mathbf{v}_{k,i}^{\\top}\\mathbf{H}_k\\mathbf{v}_{k,i}) / n_i$.\n    c. Apply the pruning rule: if $s_{k,i} \\le t_k$, increment the pruned count $c_k$.\n    d. Verify the score against the spectral bounds: if the condition $\\lambda_{\\min} - \\tau \\le s_{k,i} \\le \\lambda_{\\max} + \\tau$ is false, set $b_k = \\text{false}$.\n4.  The final results for the case are the pair $(c_k, b_k)$.\n\nThis procedure is applied systematically to all provided test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing each test case according to the\n    derived methodology for curvature calculation, pruning, and verification.\n    \"\"\"\n\n    def process_case(H, directions, t, eps, tau):\n        \"\"\"\n        Calculates the number of pruned directions and verifies that all curvature\n        scores lie within the eigenvalue bounds for a single test case.\n\n        Args:\n            H (np.ndarray): The symmetric Hessian matrix.\n            directions (list of np.ndarray): A list of direction vectors.\n            t (float): The pruning threshold.\n            eps (float): The small-norm cutoff for vectors.\n            tau (float): The tolerance for eigenvalue bound checking.\n\n        Returns:\n            tuple: A tuple (pruned_count, all_scores_in_bounds) containing\n                   the integer count of pruned directions and a boolean flag.\n        \"\"\"\n        pruned_count = 0\n        all_scores_in_bounds = True\n\n        # For a real symmetric matrix, eigh is preferred.\n        # It returns sorted eigenvalues and is numerically stable.\n        eigenvalues = np.linalg.eigh(H)[0]\n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[-1]\n\n        for v in directions:\n            v_dot_v = v.T @ v\n            \n            # Use single element from 1x1 array if necessary\n            if isinstance(v_dot_v, np.ndarray):\n                v_dot_v = v_dot_v.item()\n\n            score = 0.0\n            if v_dot_v = eps:\n                # Rayleigh quotient calculation\n                numerator = v.T @ H @ v\n                if isinstance(numerator, np.ndarray):\n                    numerator = numerator.item()\n                score = numerator / v_dot_v\n\n            # Apply pruning rule\n            if score = t:\n                pruned_count += 1\n\n            # Verify score is within spectral bounds (Rayleigh-Ritz theorem)\n            if not (lambda_min - tau = score = lambda_max + tau):\n                all_scores_in_bounds = False\n\n        return pruned_count, all_scores_in_bounds\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, positive definite)\n        {\n            \"H\": np.array([[4, 1], [1, 3]]),\n            \"directions\": [np.array([[1], [0]]), np.array([[0], [1]]), np.array([[1], [1]])],\n            \"t\": 3.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 2 (boundary equality, repeated eigenvalue)\n        {\n            \"H\": np.array([[2, 0], [0, 2]]),\n            \"directions\": [np.array([[1], [2]]), np.array([[-3], [4]])],\n            \"t\": 2.0,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 3 (positive semidefinite with a nullspace and a near-zero vector)\n        {\n            \"H\": np.array([[1, 0, 0], [0, 0, 0], [0, 0, 3]]),\n            \"directions\": [\n                np.array([[0], [1], [0]]),\n                np.array([[1], [0], [0]]),\n                np.array([[1], [1], [0]]),\n                np.array([[1e-13], [0], [0]])\n            ],\n            \"t\": 0.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 4 (indefinite curvature, saddle-like)\n        {\n            \"H\": np.array([[0, 1], [1, 0]]),\n            \"directions\": [np.array([[1], [1]]), np.array([[1], [-1]]), np.array([[1], [0]])],\n            \"t\": -0.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        c, b = process_case(case[\"H\"], case[\"directions\"], case[\"t\"], case[\"eps\"], case[\"tau\"])\n        results.extend([c, b])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120472"}]}