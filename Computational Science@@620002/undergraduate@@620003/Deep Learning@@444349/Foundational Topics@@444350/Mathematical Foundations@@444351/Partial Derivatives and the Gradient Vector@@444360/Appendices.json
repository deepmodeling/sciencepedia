{"hands_on_practices": [{"introduction": "The choice of a loss function is a critical design decision in building any machine learning model. This exercise explores the practical differences between two common regression losses: Mean Squared Error (MSE) and Mean Absolute Error (MAE). By calculating the gradient vectors for each under the influence of an outlier, you will gain first-hand insight into how MSE heavily penalizes large errors, potentially leading to unstable training, while MAE provides a more robust, constant gradient magnitude [@problem_id:3162520]. This practice illuminates why understanding the character of a loss function's partial derivatives is fundamental to building robust models.", "problem": "Consider a single linear neuron used for regression with two real-valued features, defined by the parametric model $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ where $\\mathbf{w} \\in \\mathbb{R}^{2}$ and $\\mathbf{x} \\in \\mathbb{R}^{2}$. You are given a mini-batch of $3$ training examples:\n- Example $1$: $\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ with target $y^{(1)} = 1$,\n- Example $2$: $\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ with target $y^{(2)} = 1$,\n- Example $3$ (an outlier in its target): $\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ with target $y^{(3)} = -10$.\n\nAssume the current parameter is $\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$. Define the mean squared error (MSE) and mean absolute error (MAE) over the mini-batch as\n$$\nL_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}, \n\\quad\nL_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|.\n$$\nUsing only the base definitions of partial derivatives and the gradient vector, compute the batch gradient with respect to $\\mathbf{w}$ under each loss at the given $\\mathbf{w}$, and then compute the angle between these two gradient vectors. Express the final answer as a single exact closed-form expression for the angle in radians (do not approximate). State your final answer in radians (no degree conversion and no numerical rounding).", "solution": "The problem statement is first validated according to the required procedure.\n\n### Step 1: Extract Givens\n- **Model**: A single linear neuron with two real-valued features, $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$, where $\\mathbf{w} \\in \\mathbb{R}^{2}$ and $\\mathbf{x} \\in \\mathbb{R}^{2}$.\n- **Training Data (Mini-batch of size 3)**:\n  - Example $1$: $\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, $y^{(1)} = 1$.\n  - Example $2$: $\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, $y^{(2)} = 1$.\n  - Example $3$: $\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $y^{(3)} = -10$.\n- **Current Parameter**: $\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n- **Loss Functions**:\n  - Mean Squared Error (MSE): $L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}$.\n  - Mean Absolute Error (MAE): $L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|$.\n- **Task**:\n  1. Compute the batch gradient of $L_{\\mathrm{MSE}}$ with respect to $\\mathbf{w}$ at the given $\\mathbf{w}$.\n  2. Compute the batch gradient of $L_{\\mathrm{MAE}}$ with respect to $\\mathbf{w}$ at the given $\\mathbf{w}$.\n  3. Compute the angle in radians between these two gradient vectors.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in the field of machine learning and optimization. It deals with fundamental concepts such as linear models, loss functions (MSE, MAE), and gradient computation, which are central to training neural networks. The setup is scientifically and mathematically sound.\n- **Well-Posed**: The problem provides all necessary data and definitions to compute a unique solution. The model, data, initial parameters, and loss functions are explicitly defined. The gradient of the MAE loss function is well-defined at the specified point because the arguments to the absolute value function are non-zero, ensuring differentiability.\n- **Objective**: The problem is stated using precise mathematical language and is free of any ambiguity, subjective claims, or opinions.\n\nThe problem is self-contained, consistent, and adheres to established scientific principles. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Solution\n\nThe model is a linear neuron, so its prediction $\\hat{y}$ for an input vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ with weights $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is given by $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x} = w_1 x_1 + w_2 x_2$.\n\nThe task requires computing the gradients of the MSE and MAE loss functions at the specific weight vector $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we compute the model's predictions for each example in the mini-batch at $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)} = \\begin{pmatrix} 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\end{pmatrix} = 0$ for all $i \\in \\{1, 2, 3\\}$.\n\nNext, we calculate the errors $(\\hat{y}^{(i)} - y^{(i)})$ for each example:\n- Error for example $1$: $e_1 = \\hat{y}^{(1)} - y^{(1)} = 0 - 1 = -1$.\n- Error for example $2$: $e_2 = \\hat{y}^{(2)} - y^{(2)} = 0 - 1 = -1$.\n- Error for example $3$: $e_3 = \\hat{y}^{(3)} - y^{(3)} = 0 - (-10) = 10$.\n\n**1. Gradient of the Mean Squared Error (MSE) Loss**\n\nThe MSE loss is $L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right)^{2}$.\nThe gradient of $L_{\\mathrm{MSE}}$ with respect to the weight vector $\\mathbf{w}$ is given by:\n$$ \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2} \\right) = \\frac{1}{3}\\sum_{i=1}^{3} 2\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} $$\nSince $\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)}$, its gradient with respect to $\\mathbf{w}$ is $\\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\mathbf{x}^{(i)}$.\nThus, the gradient of the MSE loss is:\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{2}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\nEvaluating this at $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ using the pre-calculated errors:\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (10)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (10)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}10 \\\\ 0\\end{pmatrix} \\right) = \\frac{2}{3} \\begin{pmatrix}10 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} $$\n\n**2. Gradient of the Mean Absolute Error (MAE) Loss**\n\nThe MAE loss is $L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right|$.\nThe derivative of the absolute value function $|u|$ is the sign function, $\\text{sgn}(u)$, which is defined for $u \\neq 0$. Since all errors $e_i$ are non-zero ($-1, -1, 10$), the gradient of $L_{\\mathrm{MAE}}$ is well-defined at this point.\nThe gradient of $L_{\\mathrm{MAE}}$ with respect to $\\mathbf{w}$ is:\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\nWe need the signs of the errors:\n- $\\text{sgn}(e_1) = \\text{sgn}(-1) = -1$.\n- $\\text{sgn}(e_2) = \\text{sgn}(-1) = -1$.\n- $\\text{sgn}(e_3) = \\text{sgn}(10) = 1$.\nEvaluating the gradient at $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (1)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (1)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} $$\n\n**3. Angle between the Gradient Vectors**\n\nLet $\\theta$ be the angle between the two gradient vectors $\\mathbf{g}_{\\mathrm{MSE}}$ and $\\mathbf{g}_{\\mathrm{MAE}}$. The angle is given by the formula:\n$$ \\theta = \\arccos\\left(\\frac{\\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}}}{\\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\|}\\right) $$\nWe compute the components of this formula:\n- **Dot Product**:\n$$ \\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} = \\left(\\frac{20}{3}\\right)\\left(\\frac{1}{3}\\right) + \\left(-\\frac{4}{3}\\right)\\left(-\\frac{2}{3}\\right) = \\frac{20}{9} + \\frac{8}{9} = \\frac{28}{9} $$\n- **Magnitudes**:\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| = \\sqrt{\\left(\\frac{20}{3}\\right)^2 + \\left(-\\frac{4}{3}\\right)^2} = \\sqrt{\\frac{400}{9} + \\frac{16}{9}} = \\sqrt{\\frac{416}{9}} = \\frac{\\sqrt{16 \\times 26}}{3} = \\frac{4\\sqrt{26}}{3} $$\n$$ \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\sqrt{\\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{2}{3}\\right)^2} = \\sqrt{\\frac{1}{9} + \\frac{4}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3} $$\n- **Product of Magnitudes**:\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\left(\\frac{4\\sqrt{26}}{3}\\right) \\left(\\frac{\\sqrt{5}}{3}\\right) = \\frac{4\\sqrt{26 \\times 5}}{9} = \\frac{4\\sqrt{130}}{9} $$\n- **Cosine of the angle**:\n$$ \\cos(\\theta) = \\frac{\\frac{28}{9}}{\\frac{4\\sqrt{130}}{9}} = \\frac{28}{4\\sqrt{130}} = \\frac{7}{\\sqrt{130}} $$\n- **Angle $\\theta$**:\nThe angle in radians is:\n$$ \\theta = \\arccos\\left(\\frac{7}{\\sqrt{130}}\\right) $$\nThis is the final exact closed-form expression for the angle.", "answer": "$$\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{130}}\\right)}$$", "id": "3162520"}, {"introduction": "Gradient-based optimization relies on the chain rule, but what happens when a function in our network is not differentiable? This is a common challenge in areas like model quantization, where continuous parameters are mapped to discrete values. This exercise introduces the Straight-Through Estimator (STE), a practical and widely used heuristic that makes training possible by substituting a \"proxy\" gradient during the backward pass [@problem_id:3162528]. By calculating both the true gradient (which is zero) and the biased STE gradient, you will uncover the principles and trade-offs behind this essential deep learning technique.", "problem": "Consider a single-parameter linear predictor with a pre-quantized parameter $\\tilde{w} \\in \\mathbb{R}$ and a quantized parameter $w \\in \\mathbb{Z}$ given by $w=\\mathrm{round}(\\tilde{w})$, where $\\mathrm{round}(\\cdot)$ denotes rounding to the nearest integer. The model output for input $x$ is $f(x)=w\\,x$. You train on a single deterministic training pair $(x,y)$ with $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$ using the squared-error loss\n$$\nL(\\tilde{w})=\\big(w\\,x-y\\big)^{2}.\n$$\nDuring backpropagation, adopt the straight-through estimator (STE), that is, approximate the Jacobian $\\frac{\\partial w}{\\partial \\tilde{w}}$ by $1$ wherever needed.\n\nWork at any $\\tilde{w}$ in the open interval $\\big(\\frac{1}{2},\\frac{3}{2}\\big)$, where $w$ is constant. Starting only from the definition of partial derivatives, the chain rule, and the definition of the squared-error loss, do the following:\n\n- Compute the STE-based partial derivative $\\frac{\\partial L}{\\partial \\tilde{w}}$ at such a $\\tilde{w}$.\n- Using the exact, quantized objective $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$, determine the true partial derivative $\\frac{\\partial L}{\\partial \\tilde{w}}$ at such a $\\tilde{w}$.\n- Define the bias of the STE gradient at such a $\\tilde{w}$ as $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$. Compute $B$ exactly.\n\nYour final answer must be a single exact number for $B$ with no rounding.", "solution": "The problem asks for the computation of the bias of the straight-through estimator (STE) for the gradient of a loss function with respect to a pre-quantized parameter. We are given the loss function $L(\\tilde{w})=\\big(w\\,x-y\\big)^{2}$, where $w = \\mathrm{round}(\\tilde{w})$. The bias is defined as $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$. We must evaluate this for $\\tilde{w} \\in \\big(\\frac{1}{2},\\frac{3}{2}\\big)$, with the specific training data $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$.\n\nFirst, we compute the STE-based partial derivative, $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}$.\nThe loss function $L$ is a composition of functions of $\\tilde{w}$. We apply the chain rule to find its derivative with respect to $\\tilde{w}$:\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = \\frac{\\partial L}{\\partial w} \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\nLet's compute the partial derivative of $L$ with respect to $w$:\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\big((w\\,x-y)^{2}\\big) = 2(w\\,x-y) \\cdot \\frac{\\partial}{\\partial w}(w\\,x-y) = 2(w\\,x-y)x\n$$\nSubstituting this back into the chain rule expression gives:\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = 2x(w\\,x-y) \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\nThe straight-through estimator (STE) approximates the derivative of the quantization function, $\\frac{\\partial w}{\\partial \\tilde{w}}$, as $1$.\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2x(w\\,x-y) \\cdot 1 = 2x(w\\,x-y)\n$$\nThe problem specifies that we evaluate this derivative for any $\\tilde{w}$ in the open interval $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$, which is the interval $(0.5, 1.5)$. For any value of $\\tilde{w}$ in this interval, the `round` function yields the integer $1$. Thus, $w = \\mathrm{round}(\\tilde{w}) = 1$.\nNow we substitute the given values $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$, along with $w=1$:\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2\\left(\\frac{3}{2}\\right)\\left( (1)\\left(\\frac{3}{2}\\right) - \\frac{5}{4} \\right) = 3\\left(\\frac{6}{4} - \\frac{5}{4}\\right) = 3\\left(\\frac{1}{4}\\right) = \\frac{3}{4}\n$$\nSo, the STE-based gradient is $\\frac{3}{4}$.\n\nNext, we compute the true partial derivative, $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$.\nThe loss function is $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$.\nAs established before, for any $\\tilde{w}$ in the open interval $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$, the value of $\\mathrm{round}(\\tilde{w})$ is constant and equal to $1$.\nTherefore, for $\\tilde{w} \\in \\big(\\frac{1}{2}, \\frac{3}{2}\\big)$, the loss function simplifies to:\n$$\nL(\\tilde{w}) = \\big((1)x-y\\big)^{2}\n$$\nSubstituting the constant values $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$:\n$$\nL(\\tilde{w}) = \\left(\\frac{3}{2} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{6}{4} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{16}\n$$\nIn the specified interval, $L(\\tilde{w})$ is a constant function of $\\tilde{w}$. The derivative of any constant function is $0$.\nTherefore, the true partial derivative is:\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}} = \\frac{\\partial}{\\partial \\tilde{w}}\\left(\\frac{1}{16}\\right) = 0\n$$\nThis is consistent with the fact that the derivative of the `round` function is $0$ everywhere except at the points of discontinuity (i.e., at values $\\tilde{w}=n+0.5$ for any integer $n$), which are not included in the open interval $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$.\n\nFinally, we compute the bias $B$.\nThe bias is defined as the difference between the STE-based gradient and the true gradient:\n$$\nB = \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} - \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}\n$$\nSubstituting the values we computed:\n$$\nB = \\frac{3}{4} - 0 = \\frac{3}{4}\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3162528"}, {"introduction": "Modern neural networks are composed of powerful layers, and Batch Normalization (BN) is one of the most impactful. However, its mechanism of standardizing activations across a mini-batch introduces complex dependencies that are crucial to understand for backpropagation. In this exercise, you will derive the partial derivatives for a BN layer from first principles, revealing how the gradient for a single input, $\\frac{\\partial L}{\\partial x_i}$, is influenced by all other inputs in the mini-batch [@problem_id:3162556]. Mastering this derivation provides a deep understanding of how gradients flow through architectures with non-local interactions.", "problem": "Consider a single scalar feature channel processed by Batch Normalization (BN) in training mode over a mini-batch of size $m$, with forward definitions given by the batch mean $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i$, the batch variance $\\sigma^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^{2}$, the standard deviation $\\sigma = \\sqrt{\\sigma^{2} + \\varepsilon}$, the normalized activations $\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}$, and the BN outputs $y_i = \\gamma \\hat{x}_i + \\beta$. A scalar loss is applied to this BN layer with upstream sensitivities $g_i = \\frac{\\partial L}{\\partial y_i}$.\n\nStarting only from these forward definitions and the chain rule of calculus, derive the general expression for the partial derivative $\\frac{\\partial L}{\\partial x_i}$, making explicit how the dependence of $\\mu$ and $\\sigma$ on all samples in the mini-batch creates non-local terms that couple all $\\{x_j\\}_{j=1}^{m}$ through the gradient. Then, using your derived expression, evaluate $\\frac{\\partial L}{\\partial x_1}$ for the following scientifically plausible configuration that exhibits domination of gradient contributions by a single sample via batch statistics:\n\n- Mini-batch size $m = 3$.\n- Inputs $(x_1, x_2, x_3) = (5, 0, 1)$.\n- BN parameters $\\gamma = 1$, $\\beta = 0$, and $\\varepsilon = 0$.\n- A linear loss $L = \\sum_{i=1}^{m} c_i y_i$ with coefficients $(c_1, c_2, c_3) = (1, 0, 0)$, implying upstream sensitivities $g_i = c_i$.\n\nYour final answer must be the single real value of $\\frac{\\partial L}{\\partial x_1}$ for this configuration. Round your answer to four significant figures.", "solution": "The problem statement is rigorously validated and found to be valid. It is scientifically grounded in the established principles of deep learning, specifically the Batch Normalization algorithm. It is well-posed, with all necessary parameters and functions defined unambiguously, providing a self-contained and consistent setup that permits a unique and meaningful solution. The problem requires the derivation and application of partial derivatives, which is central to the specified topic.\n\nWe begin by deriving the general expression for the partial derivative of the loss $L$ with respect to a mini-batch input $x_i$. The loss $L$ is a function of the Batch Normalization (BN) outputs $\\{y_j\\}_{j=1}^m$. The dependence of $L$ on a specific input $x_i$ is established through the chain rule:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}\n$$\nGiven the upstream sensitivities $g_j = \\frac{\\partial L}{\\partial y_j}$, this becomes:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} g_j \\frac{\\partial y_j}{\\partial x_i}\n$$\nThe BN output $y_j$ is defined as $y_j = \\gamma \\hat{x}_j + \\beta$, where $\\hat{x}_j = \\frac{x_j - \\mu}{\\sigma}$. The parameters $\\gamma$ and $\\beta$ are independent of the inputs $\\{x_k\\}$. Therefore, we must compute $\\frac{\\partial y_j}{\\partial x_i}$:\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\gamma \\frac{x_j - \\mu}{\\sigma} + \\beta \\right) = \\gamma \\frac{\\partial}{\\partial x_i} \\left( \\frac{x_j - \\mu}{\\sigma} \\right)\n$$\nUsing the quotient rule, where both the numerator $N_j = x_j - \\mu$ and the denominator $D = \\sigma$ depend on $x_i$:\n$$\n\\frac{\\partial}{\\partial x_i} \\left( \\frac{N_j}{D} \\right) = \\frac{D \\frac{\\partial N_j}{\\partial x_i} - N_j \\frac{\\partial D}{\\partial x_i}}{D^2} = \\frac{1}{\\sigma} \\frac{\\partial (x_j - \\mu)}{\\partial x_i} - \\frac{x_j - \\mu}{\\sigma^2} \\frac{\\partial \\sigma}{\\partial x_i}\n$$\nWe must find the partial derivatives of the batch statistics $\\mu$ and $\\sigma$ with respect to $x_i$.\nThe batch mean is $\\mu = \\frac{1}{m} \\sum_{k=1}^{m} x_k$. Its partial derivative is:\n$$\n\\frac{\\partial \\mu}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_k \\right) = \\frac{1}{m}\n$$\nThe derivative of the numerator term $N_j = x_j - \\mu$ is:\n$$\n\\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\frac{\\partial x_j}{\\partial x_i} - \\frac{\\partial \\mu}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta.\n\nNext, we address the derivative of the standard deviation $\\sigma = \\sqrt{\\sigma^2 + \\varepsilon}$:\n$$\n\\frac{\\partial \\sigma}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\sqrt{\\sigma^2 + \\varepsilon} = \\frac{1}{2\\sqrt{\\sigma^2 + \\varepsilon}} \\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{1}{2\\sigma} \\frac{\\partial \\sigma^2}{\\partial x_i}\n$$\nThe batch variance is $\\sigma^2 = \\frac{1}{m} \\sum_{k=1}^{m} (x_k - \\mu)^2$. Its partial derivative is:\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{1}{m} \\sum_{k=1}^{m} \\frac{\\partial}{\\partial x_i} (x_k - \\mu)^2 = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_k - \\mu) \\frac{\\partial (x_k - \\mu)}{\\partial x_i}\n$$\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{m} \\sum_{k=1}^{m} (x_k - \\mu) (\\delta_{ki} - \\frac{1}{m}) = \\frac{2}{m} \\left( (x_i - \\mu) - \\frac{1}{m} \\sum_{k=1}^{m} (x_k - \\mu) \\right)\n$$\nBy definition of the mean, $\\sum_{k=1}^{m} (x_k - \\mu) = 0$. Thus:\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{m} (x_i - \\mu)\n$$\nSubstituting this back into the derivative for $\\sigma$:\n$$\n\\frac{\\partial \\sigma}{\\partial x_i} = \\frac{1}{2\\sigma} \\left( \\frac{2}{m} (x_i - \\mu) \\right) = \\frac{x_i - \\mu}{m\\sigma}\n$$\nNow we can assemble the expression for $\\frac{\\partial y_j}{\\partial x_i}$:\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\gamma \\left( \\frac{1}{\\sigma}(\\delta_{ij} - \\frac{1}{m}) - \\frac{x_j - \\mu}{\\sigma^2} \\frac{x_i - \\mu}{m\\sigma} \\right)\n$$\nUsing the definition of normalized activations, $\\hat{x}_k = \\frac{x_k - \\mu}{\\sigma}$, we can simplify this expression:\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( (\\delta_{ij} - \\frac{1}{m}) - \\frac{\\hat{x}_j \\hat{x}_i}{m} \\right) = \\frac{\\gamma}{\\sigma} \\left( \\delta_{ij} - \\frac{1}{m}(1 + \\hat{x}_i \\hat{x}_j) \\right)\n$$\nFinally, we substitute this into the sum for $\\frac{\\partial L}{\\partial x_i}$:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} g_j \\left[ \\frac{\\gamma}{\\sigma} \\left( \\delta_{ij} - \\frac{1}{m} - \\frac{\\hat{x}_i \\hat{x}_j}{m} \\right) \\right]\n$$\n$$\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( \\sum_{j=1}^{m} g_j \\delta_{ij} - \\frac{1}{m} \\sum_{j=1}^{m} g_j - \\frac{\\hat{x}_i}{m} \\sum_{j=1}^{m} g_j \\hat{x}_j \\right)\n$$\nThis simplifies to the general expression for the gradient with respect to an input feature $x_i$:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j - \\frac{\\hat{x}_i}{m} \\sum_{j=1}^{m} g_j \\hat{x}_j \\right)\n$$\nThis expression demonstrates the non-local coupling: the gradient $\\frac{\\partial L}{\\partial x_i}$ depends not only on the local upstream gradient $g_i$ and local activation $\\hat{x}_i$, but also on terms that are averages over the entire mini-batch ($\\sum g_j$ and $\\sum g_j \\hat{x}_j$).\n\nNow, we evaluate this expression for the specific configuration provided.\nThe givens are:\n- Mini-batch size: $m = 3$\n- Inputs: $(x_1, x_2, x_3) = (5, 0, 1)$\n- BN parameters: $\\gamma = 1$, $\\beta = 0$, $\\varepsilon = 0$\n- Upstream sensitivities: $(g_1, g_2, g_3) = (1, 0, 0)$\n\nFirst, we compute the batch statistics:\n1.  Batch mean: $\\mu = \\frac{1}{3}(5 + 0 + 1) = \\frac{6}{3} = 2$.\n2.  Centered inputs: $(x_1 - \\mu, x_2 - \\mu, x_3 - \\mu) = (5-2, 0-2, 1-2) = (3, -2, -1)$.\n3.  Batch variance: $\\sigma^2 = \\frac{1}{3}((3)^2 + (-2)^2 + (-1)^2) = \\frac{1}{3}(9 + 4 + 1) = \\frac{14}{3}$.\n4.  Standard deviation: $\\sigma = \\sqrt{\\sigma^2 + \\varepsilon} = \\sqrt{\\frac{14}{3} + 0} = \\sqrt{\\frac{14}{3}}$.\n\nNext, we calculate the terms required for the gradient formula for $i=1$:\n- $\\sum_{j=1}^{3} g_j = g_1 + g_2 + g_3 = 1 + 0 + 0 = 1$.\n- We need the normalized activation $\\hat{x}_1$: $\\hat{x}_1 = \\frac{x_1 - \\mu}{\\sigma} = \\frac{3}{\\sqrt{14/3}}$.\n- We need the sum $\\sum_{j=1}^{3} g_j \\hat{x}_j = g_1\\hat{x}_1 + g_2\\hat{x}_2 + g_3\\hat{x}_3 = (1)\\hat{x}_1 + (0)\\hat{x}_2 + (0)\\hat{x}_3 = \\hat{x}_1$.\n\nSubstituting these into the general formula for $\\frac{\\partial L}{\\partial x_1}$:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{\\gamma}{\\sigma} \\left( g_1 - \\frac{1}{m} \\sum_{j=1}^{3} g_j - \\frac{\\hat{x}_1}{m} \\sum_{j=1}^{3} g_j \\hat{x}_j \\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{1}{\\sqrt{14/3}} \\left( 1 - \\frac{1}{3}(1) - \\frac{\\hat{x}_1}{3}(\\hat{x}_1) \\right) = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{\\hat{x}_1^2}{3} \\right)\n$$\nWe calculate $\\hat{x}_1^2$:\n$$\n\\hat{x}_1^2 = \\left( \\frac{3}{\\sqrt{14/3}} \\right)^2 = \\frac{9}{14/3} = \\frac{27}{14}\n$$\nNow substitute this value back:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{1}{3} \\cdot \\frac{27}{14} \\right) = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{9}{14} \\right)\n$$\nTo simplify the term in the parenthesis, we find a common denominator, which is $42$:\n$$\n\\frac{2}{3} - \\frac{9}{14} = \\frac{2 \\cdot 14}{42} - \\frac{9 \\cdot 3}{42} = \\frac{28 - 27}{42} = \\frac{1}{42}\n$$\nTherefore, the exact value of the partial derivative is:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\sqrt{\\frac{3}{14}} \\cdot \\frac{1}{42} = \\frac{\\sqrt{3}}{42\\sqrt{14}} = \\frac{\\sqrt{3}\\sqrt{14}}{42 \\cdot 14} = \\frac{\\sqrt{42}}{588}\n$$\nFinally, we compute the numerical value and round to four significant figures:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{\\sqrt{42}}{588} \\approx \\frac{6.4807407}{588} \\approx 0.01102166785\n$$\nRounding to four significant figures yields $0.01102$.", "answer": "$$\\boxed{0.01102}$$", "id": "3162556"}]}