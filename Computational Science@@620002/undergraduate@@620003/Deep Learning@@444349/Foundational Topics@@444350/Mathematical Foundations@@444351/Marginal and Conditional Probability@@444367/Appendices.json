{"hands_on_practices": [{"introduction": "We begin with one of the most fundamental questions in classification: how does the underlying data distribution dictate the optimal way to make predictions? This practice grounds the discussion in Bayes' theorem, showing how the optimal decision boundary depends on both the class-conditional distributions $p(x|y)$ and the class marginal probabilities (or priors) $p(y)$. By analyzing a hypothetical scenario involving class-balanced sampling, you will gain a concrete understanding of why this common technique can create a mismatch between training and testing conditions, and how marginal probabilities are central to a model's true generalization performance. [@problem_id:3146696]", "problem": "Consider a binary classification setting with a single real-valued input variable $x \\in \\mathbb{R}$. Let there be a latent component variable $z \\in \\{0,1\\}$ with a deterministic mapping to the class label $y \\in \\{0,1\\}$ such that $y = z$. The generative process is defined by a mixture model for the input with class-conditional densities and a deterministic label: for component $z = 0$, $x$ is drawn from a normal distribution with mean $\\mu_0$ and variance $\\sigma^2$, and for component $z = 1$, $x$ is drawn from a normal distribution with mean $\\mu_1$ and variance $\\sigma^2$. The marginal input distribution is $p(x) = \\pi_0 \\, \\mathcal{N}(x \\mid \\mu_0, \\sigma^2) + \\pi_1 \\, \\mathcal{N}(x \\mid \\mu_1, \\sigma^2)$, where $\\pi_1 \\in (0,1)$ is the mixture weight of component $z=1$ and $\\pi_0 = 1 - \\pi_1$.\n\nA learning algorithm trains a single-threshold classifier $h_t(x)$ that predicts $y = 1$ for $x$ on one side of a threshold $t$ and $y = 0$ on the other side, with the direction determined by the relative ordering of $\\mu_0$ and $\\mu_1$. Training is performed under class-balanced sampling, meaning the training distribution $q(x,y)$ satisfies $\\Pr_q(y=0) = \\Pr_q(y=1) = \\frac{1}{2}$ and $q(x \\mid y)$ matches the true class-conditional $p(x \\mid y)$. Evaluation of generalization is performed under the true marginal distribution $p(x,y)$ with class priors $\\pi_0$ and $\\pi_1$.\n\nStarting only from the core definitions of marginal probability $p(x)$, conditional probability $p(y \\mid x)$, and expected $0-1$ loss risk under a distribution, derive the decision threshold that minimizes expected $0-1$ loss under the true marginal $p(x,y)$, and the decision threshold that minimizes expected $0-1$ loss under the class-balanced training distribution $q(x,y)$. Use these thresholds to define the divergence in true marginal generalization performance caused by class-balanced training as the difference in expected $0-1$ loss under the true distribution:\n$$\nD(\\mu_0,\\mu_1,\\sigma,\\pi_1) = R_p\\big(h_{t_{\\text{bal}}}\\big) - R_p\\big(h_{t_{\\text{true}}}\\big),\n$$\nwhere $R_p(h)$ denotes the expected $0-1$ loss under $p(x,y)$ and $t_{\\text{bal}}$ and $t_{\\text{true}}$ denote the thresholds that minimize risk under $q(x,y)$ and $p(x,y)$, respectively.\n\nYour program must:\n- Implement the classifier evaluation strictly using the definitions of marginal and conditional probabilities and expected $0-1$ loss.\n- For each test case, compute $D(\\mu_0,\\mu_1,\\sigma,\\pi_1)$ as a real number.\n- Aggregate all results into a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where each $r_i$ is a floating-point number.\n\nUse the following test suite of parameter values, which covers typical, balanced, highly separable, highly overlapping, and reversed-mean cases:\n- Test case $1$: $(\\mu_0, \\mu_1, \\sigma, \\pi_1) = (0.0, 2.0, 1.0, 0.2)$.\n- Test case $2$: $(\\mu_0, \\mu_1, \\sigma, \\pi_1) = (0.0, 2.0, 1.0, 0.5)$.\n- Test case $3$: $(\\mu_0, \\mu_1, \\sigma, \\pi_1) = (0.0, 6.0, 1.0, 0.1)$.\n- Test case $4$: $(\\mu_0, \\mu_1, \\sigma, \\pi_1) = (0.0, 0.5, 1.0, 0.2)$.\n- Test case $5$: $(\\mu_0, \\mu_1, \\sigma, \\pi_1) = (2.0, -1.0, 0.8, 0.7)$.\n\nThere are no physical units or angles involved. All outputs must be real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets as described.", "solution": "We begin from the foundational definitions. The marginal probability $p(x)$ is the probability distribution over $x$ induced by the generative process, here a mixture $p(x) = \\pi_0 \\, p(x \\mid y=0) + \\pi_1 \\, p(x \\mid y=1)$, with $p(x \\mid y=c) = \\mathcal{N}(x \\mid \\mu_c, \\sigma^2)$ for $c \\in \\{0,1\\}$. The conditional probability $p(y \\mid x)$ is given by Bayes' rule:\n$$\np(y=c \\mid x) = \\frac{p(x \\mid y=c)\\, \\pi_c}{p(x)} \\quad \\text{for } c \\in \\{0,1\\}.\n$$\nThe expected $0-1$ loss risk of a classifier $h$ under a distribution $p(x,y)$ is\n$$\nR_p(h) = \\mathbb{E}_{(x,y) \\sim p}\\left[\\mathbf{1}\\{h(x) \\neq y\\}\\right],\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nFor binary classification under $0-1$ loss, the Bayes optimal decision rule that minimizes $R_p(h)$ is to predict the label with the larger posterior probability. Thus, the Bayes classifier decides $y=1$ when $p(y=1 \\mid x) \\geq p(y=0 \\mid x)$ and $y=0$ otherwise. Using Bayes' rule, this inequality reduces to comparing $\\pi_1 p(x \\mid y=1)$ and $\\pi_0 p(x \\mid y=0)$. With the class-conditionals both normal and sharing the same variance $\\sigma^2$, the decision boundary solves\n$$\n\\pi_1 \\, \\mathcal{N}(x \\mid \\mu_1,\\sigma^2) = \\pi_0 \\, \\mathcal{N}(x \\mid \\mu_0,\\sigma^2).\n$$\nTaking natural logarithms and expanding the quadratic terms yields the threshold equation\n$$\n\\ln \\pi_1 - \\frac{(x - \\mu_1)^2}{2\\sigma^2} = \\ln \\pi_0 - \\frac{(x - \\mu_0)^2}{2\\sigma^2}.\n$$\nRearranging and simplifying gives the unique threshold $t_{\\text{true}}$:\n$$\nt_{\\text{true}} = \\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right),\n$$\nwhich is valid when $\\mu_1 \\neq \\mu_0$. The classifier is monotone with respect to $x$, predicting $y=1$ on the side of the threshold closer to $\\mu_1$ (i.e., for $x \\geq t_{\\text{true}}$ when $\\mu_1 > \\mu_0$, and for $x \\leq t_{\\text{true}}$ when $\\mu_1 < \\mu_0$).\n\nUnder class-balanced sampling, the training distribution $q(x,y)$ satisfies $\\Pr_q(y=0) = \\Pr_q(y=1) = \\tfrac{1}{2}$ and $q(x \\mid y=c) = p(x \\mid y=c)$ for $c \\in \\{0,1\\}$. The Bayes decision boundary minimizing expected $0-1$ loss under $q$ solves\n$$\n\\frac{1}{2} \\, \\mathcal{N}(x \\mid \\mu_1,\\sigma^2) = \\frac{1}{2} \\, \\mathcal{N}(x \\mid \\mu_0,\\sigma^2),\n$$\nleading to the balanced threshold\n$$\nt_{\\text{bal}} = \\frac{\\mu_0 + \\mu_1}{2}.\n$$\nThus, the divergence in generalization under the true marginal due to class-balanced training arises entirely from the mismatch between the true prior odds $\\frac{\\pi_1}{\\pi_0}$ and the balanced prior odds $\\frac{1}{1}$, which shifts the Bayes decision boundary by $\\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right)$.\n\nTo compute the risks, we use the cumulative distribution function (CDF) of the normal distribution, defined as $\\Phi(u) = \\Pr(U \\leq u)$ for a standard normal variable $U \\sim \\mathcal{N}(0,1)$. For a fixed threshold $t$, the classifier $h_t$ predicts $y=1$ on one side of $t$ and $y=0$ on the other. The misclassification probabilities under each class-conditional are then tail probabilities of the normal distribution:\n- When $\\mu_1 > \\mu_0$, $h_t$ predicts $y=1$ for $x \\geq t$. The class $y=0$ misclassification probability is $\\Pr_{x \\sim \\mathcal{N}(\\mu_0,\\sigma^2)}[x \\geq t] = 1 - \\Phi\\left(\\frac{t - \\mu_0}{\\sigma}\\right)$, and the class $y=1$ misclassification probability is $\\Pr_{x \\sim \\mathcal{N}(\\mu_1,\\sigma^2)}[x < t] = \\Phi\\left(\\frac{t - \\mu_1}{\\sigma}\\right)$.\n- When $\\mu_1 < \\mu_0$, $h_t$ predicts $y=1$ for $x \\leq t$. The class $y=0$ misclassification probability is $\\Pr_{x \\sim \\mathcal{N}(\\mu_0,\\sigma^2)}[x \\leq t] = \\Phi\\left(\\frac{t - \\mu_0}{\\sigma}\\right)$, and the class $y=1$ misclassification probability is $\\Pr_{x \\sim \\mathcal{N}(\\mu_1,\\sigma^2)}[x > t] = 1 - \\Phi\\left(\\frac{t - \\mu_1}{\\sigma}\\right)$.\n\nThe expected $0-1$ loss under the true distribution is therefore\n$$\nR_p(h_t) = \\pi_0 \\cdot \\text{misclass}_{0}(t) + \\pi_1 \\cdot \\text{misclass}_{1}(t),\n$$\nwith the appropriate tail probabilities selected according to the ordering of $\\mu_1$ and $\\mu_0$ as described above.\n\nAlgorithmic design:\n- For each test case $(\\mu_0,\\mu_1,\\sigma,\\pi_1)$, compute $\\pi_0 = 1 - \\pi_1$.\n- Compute $t_{\\text{bal}} = \\frac{\\mu_0 + \\mu_1}{2}$.\n- Compute $t_{\\text{true}} = \\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_0}{\\pi_1}\\right)$.\n- Evaluate $R_p(h_{t_{\\text{bal}}})$ and $R_p(h_{t_{\\text{true}}})$ using normal CDF tail probabilities as above.\n- Compute $D = R_p(h_{t_{\\text{bal}}}) - R_p(h_{t_{\\text{true}}})$.\n\nInterpretation across the test suite:\n- In the balanced prior case with $\\pi_1 = 0.5$, we have $t_{\\text{bal}} = t_{\\text{true}}$, so $D$ is $0$ up to numerical precision.\n- In highly separable cases (large $|\\mu_1 - \\mu_0|$ relative to $\\sigma$), both risks are small and $D$ is near $0$ even when priors are imbalanced.\n- In highly overlapping cases (small $|\\mu_1 - \\mu_0|$ relative to $\\sigma$), prior mismatch matters more, and $D$ can be appreciable.\n- When $\\mu_1 < \\mu_0$, the decision direction reverses; the formulas above handle this by swapping tail probabilities accordingly.\n\nThe program implements these steps and outputs the list of divergences for the specified test suite in the required single-line format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import log\nfrom scipy.stats import norm\n\ndef bayes_threshold(mu0: float, mu1: float, sigma: float, pi1: float) -> float:\n    \"\"\"\n    Compute the Bayes-optimal threshold under true priors for shared-variance Gaussians.\n\n    t_true = (mu0 + mu1)/2 + (sigma^2 / (mu1 - mu0)) * ln(pi0/pi1), where pi0 = 1 - pi1.\n    \"\"\"\n    pi0 = 1.0 - pi1\n    if mu1 == mu0:\n        # Degenerate case: identical means. Any threshold yields same risk.\n        # Return midpoint as a convention.\n        return mu0\n    return (mu0 + mu1) / 2.0 + (sigma ** 2) / (mu1 - mu0) * log(pi0 / pi1)\n\ndef balanced_threshold(mu0: float, mu1: float) -> float:\n    \"\"\"\n    Threshold under class-balanced sampling (equal priors).\n    \"\"\"\n    return (mu0 + mu1) / 2.0\n\ndef risk_under_true_distribution(t: float, mu0: float, mu1: float, sigma: float, pi1: float) -> float:\n    \"\"\"\n    Compute expected 0-1 loss under the true mixture distribution p(x,y)\n    for a threshold classifier h_t.\n\n    Handles both cases mu1 > mu0 and mu1 < mu0 by choosing the correct tails.\n    \"\"\"\n    pi0 = 1.0 - pi1\n    # Standardize thresholds\n    z0 = (t - mu0) / sigma\n    z1 = (t - mu1) / sigma\n\n    if mu1 > mu0:\n        # Predict y=1 for x >= t\n        mis0 = 1.0 - norm.cdf(z0)      # P[x >= t | y=0]\n        mis1 = norm.cdf(z1)            # P[x < t | y=1]\n    else:\n        # Predict y=1 for x <= t\n        mis0 = norm.cdf(z0)            # P[x <= t | y=0]\n        mis1 = 1.0 - norm.cdf(z1)      # P[x > t | y=1]\n\n    return pi0 * mis0 + pi1 * mis1\n\ndef divergence(mu0: float, mu1: float, sigma: float, pi1: float) -> float:\n    \"\"\"\n    Compute D = R_p(h_t_bal) - R_p(h_t_true).\n    \"\"\"\n    t_bal = balanced_threshold(mu0, mu1)\n    t_true = bayes_threshold(mu0, mu1, sigma, pi1)\n\n    r_bal = risk_under_true_distribution(t_bal, mu0, mu1, sigma, pi1)\n    r_true = risk_under_true_distribution(t_true, mu0, mu1, sigma, pi1)\n\n    return r_bal - r_true\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu0, mu1, sigma, pi1)\n    test_cases = [\n        (0.0, 2.0, 1.0, 0.2),   # Typical imbalanced\n        (0.0, 2.0, 1.0, 0.5),   # Balanced priors\n        (0.0, 6.0, 1.0, 0.1),   # Highly separable, imbalanced\n        (0.0, 0.5, 1.0, 0.2),   # Highly overlapping, imbalanced\n        (2.0, -1.0, 0.8, 0.7),  # Reversed means, imbalanced\n    ]\n\n    results = []\n    for mu0, mu1, sigma, pi1 in test_cases:\n        d = divergence(mu0, mu1, sigma, pi1)\n        # Format to a fixed number of decimal places for stable output\n        results.append(f\"{d:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3146696"}, {"introduction": "Moving from the data's properties to the model's behavior, we now explore how to intentionally control a model's predictive uncertainty. This exercise delves into entropy regularization, a technique that encourages smoother, less confident conditional probability distributions $p_\\theta(y|x)$ by effectively adjusting the \"temperature\" of the softmax function. By observing the model's behavior across different levels of regularization, you will uncover the crucial trade-off between fitting the data and maintaining model uncertainty, and see how the model's conditional predictions aggregate to form an induced marginal class distribution $p_\\theta(y)$. [@problem_id:3146660]", "problem": "Consider a multi-class classifier in the setting of deep learning with conditional class probabilities $p_\\theta(y \\mid x)$ for $C$ classes, where $y \\in \\{0,1,\\dots,C-1\\}$ and $x \\in \\mathbb{R}^d$ is a feature vector. Entropy regularization uses the Shannon entropy $H(p) = -\\sum_{j=0}^{C-1} p_j \\log p_j$ to bias predictions toward higher uncertainty. Suppose the prediction $p_\\theta(y \\mid x)$ is obtained by solving, for each input $x$, the optimization problem\n$$\n\\max_{p \\in \\Delta^{C}} \\ \\sum_{j=0}^{C-1} p_j \\, s_j(x) + \\lambda \\, H(p),\n$$\nwhere $\\Delta^{C}$ is the probability simplex over $C$ classes, $\\lambda \\ge 0$ is the entropy regularization coefficient, and $s_j(x) = w_j^\\top x + b_j$ is a real-valued score for class $j$ given by a linear model. This formulation embodies the principle that predictions should balance the model scores with entropy-driven uncertainty.\n\nYou are given a synthetic dataset with $C=3$ classes and $N=8$ samples in $\\mathbb{R}^2$, along with fixed class-specific linear scores. The dataset and parameters are:\n- Number of classes: $C = 3$.\n- Number of samples: $N = 8$.\n- Feature dimension: $d = 2$.\n- Class weights and biases:\n  - $w_0 = (2.0, 0.0)$, $b_0 = 0.0$.\n  - $w_1 = (0.0, 2.0)$, $b_1 = 0.0$.\n  - $w_2 = (-1.0, -1.0)$, $b_2 = 1.0$.\n- Dataset samples $(x_i, y_i)$ for $i \\in \\{1,\\dots,8\\}$:\n  - $x_1 = (1.5, 0.2)$, $y_1 = 0$.\n  - $x_2 = (1.2, 0.1)$, $y_2 = 0$.\n  - $x_3 = (0.1, 1.5)$, $y_3 = 1$.\n  - $x_4 = (0.2, 1.2)$, $y_4 = 1$.\n  - $x_5 = (-0.9, -1.1)$, $y_5 = 2$.\n  - $x_6 = (-1.1, -0.8)$, $y_6 = 2$.\n  - $x_7 = (0.3, 0.3)$, $y_7 = 2$.\n  - $x_8 = (-0.05, -0.05)$, $y_8 = 2$.\n\nFor each sample $x_i$, define scores $s_j(x_i) = w_j^\\top x_i + b_j$ for $j \\in \\{0,1,2\\}$. For a given $\\lambda > 0$, the solution of the stated optimization produces a prediction\n$$\np_\\theta(y=j \\mid x_i; \\lambda) = \\operatorname{softmax}\\left(\\frac{s(x_i)}{\\lambda}\\right)_j = \\frac{\\exp\\left(\\frac{s_j(x_i)}{\\lambda}\\right)}{\\sum_{k=0}^{C-1} \\exp\\left(\\frac{s_k(x_i)}{\\lambda}\\right)}.\n$$\nFor $\\lambda = 0$, interpret the solution as the deterministic distribution that puts probability $1$ on $\\arg\\max_j s_j(x_i)$; in case of ties, choose the smallest index.\n\nDefine the empirical data distribution $p_{\\text{data}}(x)$ as the uniform distribution over the $N$ samples. The marginal class distribution induced by the model is\n$$\np_\\theta(y=j; \\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_\\theta(y=j \\mid x_i; \\lambda).\n$$\n\nDefine the expected stochastic accuracy (expressed as a decimal fraction) under one sample from $p_\\theta(y \\mid x_i; \\lambda)$ per input as\n$$\n\\text{Acc}(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_\\theta(y=y_i \\mid x_i; \\lambda).\n$$\n\nYour tasks:\n- Compute $p_\\theta(y; \\lambda)$ and $\\text{Acc}(\\lambda)$ for a specified test suite of entropy coefficients.\n- Examine how $p_\\theta(y; \\lambda)$ changes with $\\lambda$ and identify the paradoxical regime where over-regularization drives $p_\\theta(y; \\lambda)$ toward the uniform marginal distribution while simultaneously reducing $\\text{Acc}(\\lambda)$.\n\nTest suite for $\\lambda$:\n- $\\lambda = 0$ (boundary, deterministic predictions).\n- $\\lambda = 0.1$ (low regularization).\n- $\\lambda = 0.5$ (moderate regularization).\n- $\\lambda = 1.0$ (notable regularization).\n- $\\lambda = 5.0$ (high regularization).\n- $\\lambda = 50.0$ (extreme regularization, near-uniform predictions).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one $\\lambda$ in the given order and must itself be a two-element list whose first element is the marginal vector $p_\\theta(y; \\lambda)$ as a list of $C$ floats rounded to six decimal places, and whose second element is $\\text{Acc}(\\lambda)$ rounded to six decimal places. For example, the outer structure is `[ [[p_0, p_1, p_2], Acc], ..., [[p_0, p_1, p_2], Acc] ]` where each element corresponds to a value of $\\lambda$.\nNo physical units or angle units are involved. All accuracy values must be expressed as decimal fractions.", "solution": "The problem is subjected to a rigorous validation process before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Model Framework**: A multi-class classifier with conditional class probabilities $p_\\theta(y \\mid x)$ for $C$ classes, where $y \\in \\{0, 1, \\dots, C-1\\}$ and $x \\in \\mathbb{R}^d$.\n- **Prediction Origin**: The prediction $p_\\theta(y \\mid x)$ is the solution to the entropy-regularized optimization problem: $$\\max_{p \\in \\Delta^{C}} \\sum_{j=0}^{C-1} p_j \\, s_j(x) + \\lambda \\, H(p),$$ where $\\Delta^{C}$ is the probability simplex, $\\lambda \\ge 0$ is the regularization coefficient, $H(p)$ is the Shannon entropy, and $s_j(x) = w_j^\\top x + b_j$ are linear scores.\n- **Conditional Probability Formula ($\\lambda > 0$)**: $$p_\\theta(y=j \\mid x_i; \\lambda) = \\operatorname{softmax}\\left(\\frac{s(x_i)}{\\lambda}\\right)_j = \\frac{\\exp\\left(\\frac{s_j(x_i)}{\\lambda}\\right)}{\\sum_{k=0}^{C-1} \\exp\\left(\\frac{s_k(x_i)}{\\lambda}\\right)}.$$\n- **Conditional Probability Formula ($\\lambda = 0$)**: The prediction is a deterministic distribution with probability $1$ on the class corresponding to $\\arg\\max_j s_j(x_i)$, with ties broken by selecting the smallest index $j$.\n- **Constants and Parameters**:\n  - Number of classes: $C = 3$.\n  - Number of samples: $N = 8$.\n  - Feature dimension: $d = 2$.\n  - Class weights: $w_0 = (2.0, 0.0)$, $w_1 = (0.0, 2.0)$, $w_2 = (-1.0, -1.0)$.\n  - Class biases: $b_0 = 0.0$, $b_1 = 0.0$, $b_2 = 1.0$.\n- **Dataset**:\n  - $x_1 = (1.5, 0.2)$, $y_1 = 0$.\n  - $x_2 = (1.2, 0.1)$, $y_2 = 0$.\n  - $x_3 = (0.1, 1.5)$, $y_3 = 1$.\n  - $x_4 = (0.2, 1.2)$, $y_4 = 1$.\n  - $x_5 = (-0.9, -1.1)$, $y_5 = 2$.\n  - $x_6 = (-1.1, -0.8)$, $y_6 = 2$.\n  - $x_7 = (0.3, 0.3)$, $y_7 = 2$.\n  - $x_8 = (-0.05, -0.05)$, $y_8 = 2$.\n- **Quantities to Compute**:\n  - Marginal class distribution: $$p_\\theta(y=j; \\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_\\theta(y=j \\mid x_i; \\lambda).$$\n  - Expected stochastic accuracy: $$\\text{Acc}(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_\\theta(y=y_i \\mid x_i; \\lambda).$$\n- **Test Suite**: $\\lambda \\in \\{0, 0.1, 0.5, 1.0, 5.0, 50.0\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Soundness**: The formulation of an entropy-regularized linear classifier is a standard topic in statistical machine learning. The derivation of the softmax function as the solution to the stated optimization problem is a well-established result based on principles of maximum entropy and variational methods. All concepts used, such as conditional probability, marginal probability, and accuracy, are fundamental to the field.\n2.  **Well-Posedness**: The problem is fully specified. All necessary data, parameters, definitions, and formulas are provided. The tie-breaking rule for $\\lambda=0$ ensures a unique solution exists for all cases.\n3.  **Completeness and Consistency**: The provided data and constraints are complete and internally consistent. There are no contradictions or missing pieces of information required for the solution.\n4.  **Feasibility**: The numerical values for the dataset and parameters are synthetic but plausible for a small-scale machine learning problem. The computations required are feasible.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe analysis requires computing two quantities for each specified value of the entropy regularization coefficient $\\lambda$: the model's marginal class distribution $p_\\theta(y; \\lambda)$ and the expected stochastic accuracy $\\text{Acc}(\\lambda)$. The algorithm proceeds as follows.\n\nFirst, we establish the data and parameters in a computational framework. The weights $w_j$ and biases $b_j$ can be structured into a weight matrix $W \\in \\mathbb{R}^{C \\times d}$ and a bias vector $b \\in \\mathbb{R}^{C}$. The dataset is represented by a feature matrix $X \\in \\mathbb{R}^{N \\times d}$ and a label vector $Y \\in \\mathbb{Z}^{N}$.\n- $C=3$, $N=8$, $d=2$.\n- $W = \\begin{pmatrix} 2.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ -1.0 & -1.0 \\end{pmatrix}$, $b = \\begin{pmatrix} 0.0 \\\\ 0.0 \\\\ 1.0 \\end{pmatrix}$.\n- $X = \\begin{pmatrix} 1.5 & 0.2 \\\\ 1.2 & 0.1 \\\\ 0.1 & 1.5 \\\\ 0.2 & 1.2 \\\\ -0.9 & -1.1 \\\\ -1.1 & -0.8 \\\\ 0.3 & 0.3 \\\\ -0.05 & -0.05 \\end{pmatrix}$, $Y = (0, 0, 1, 1, 2, 2, 2, 2)^\\top$.\n\nThe core of the computation is to iterate through each value of $\\lambda$ from the test suite $\\{0, 0.1, 0.5, 1.0, 5.0, 50.0\\}$. For each $\\lambda$, we must average over the $N=8$ data samples.\n\nFor each sample $x_i$ (row $i$ of $X$) and its true label $y_i$ (element $i$ of $Y$):\n\n**1. Compute Scores:** The class scores $s(x_i) = (s_0(x_i), s_1(x_i), s_2(x_i))^\\top$ are calculated using the linear model $s(x_i) = W x_i + b$.\n\n**2. Compute Conditional Probabilities:** The conditional probability distribution $p_\\theta(y \\mid x_i; \\lambda)$ is determined based on the value of $\\lambda$.\n\n- **Case $\\lambda = 0$ (Deterministic Prediction):**\n  The model outputs a one-hot probability vector. The probability mass is concentrated entirely on the class with the highest score.\n  $$ j_{\\text{max}} = \\arg\\max_{j \\in \\{0, \\dots, C-1\\}} s_j(x_i) $$\n  The tie-breaking rule specifies choosing the smallest index. The conditional probability vector is then $p_\\theta(y=j \\mid x_i; 0) = \\delta_{j, j_{\\text{max}}}$, where $\\delta$ is the Kronecker delta.\n\n- **Case $\\lambda > 0$ (Stochastic Prediction):**\n  The conditional probabilities are given by the softmax function applied to the scores scaled by $\\lambda$:\n  $$ p_\\theta(y=j \\mid x_i; \\lambda) = \\frac{\\exp(s_j(x_i)/\\lambda)}{\\sum_{k=0}^{C-1} \\exp(s_k(x_i)/\\lambda)} $$\n  For numerical stability, especially with very small $\\lambda$ that can lead to large arguments in the exponential function, we use the standard identity $\\operatorname{softmax}(z)_j = \\operatorname{softmax}(z - \\max(z))_j$. Let $s'_j = s_j(x_i)/\\lambda$. We compute $p_\\theta(y=j \\mid x_i; \\lambda) = \\frac{\\exp(s'_j - \\max_k s'_k)}{\\sum_{k=0}^{C-1} \\exp(s'_k - \\max_k s'_k)}$.\n\n**3. Accumulate Results:**\nFor each sample $x_i$, we obtain a conditional probability vector $p_i = p_\\theta(y \\mid x_i; \\lambda)$. We maintain two running sums for the current $\\lambda$:\n- A vector sum for the marginal probability: $\\sum_{i=1}^{N} p_i$.\n- A scalar sum for the accuracy: $\\sum_{i=1}^{N} p_i[y_i]$, where $p_i[y_i]$ is the probability assigned to the true class $y_i$.\n\n**4. Finalize Computations for $\\lambda$:**\nAfter iterating through all $N=8$ samples, the final quantities for the given $\\lambda$ are obtained by averaging:\n- Marginal distribution: $p_\\theta(y; \\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_i$.\n- Accuracy: $\\text{Acc}(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_i[y_i]$.\n\nThis entire procedure is repeated for each $\\lambda$ in the test suite. The final results are then formatted according to the specified output structure, with all numerical values rounded to six decimal places. The \"paradoxical regime\" mentioned in the problem highlights the trade-off inherent in regularization: as $\\lambda$ increases, the model's predictions become more uniform, which smooths the marginal distribution $p_\\theta(y; \\lambda)$ towards a uniform $(1/C, \\dots, 1/C)$, but this disregard for the data-driven scores leads to a drop in accuracy.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the marginal class distribution and expected stochastic accuracy\n    for a multi-class classifier with entropy regularization over a range of\n    regularization coefficients.\n    \"\"\"\n    \n    # === Define Given Parameters and Data ===\n    \n    # Number of classes, samples, and feature dimension\n    C = 3\n    N = 8\n    d = 2\n    \n    # Class weights and biases\n    W = np.array([\n        [2.0, 0.0],\n        [0.0, 2.0],\n        [-1.0, -1.0]\n    ])\n    b = np.array([0.0, 0.0, 1.0])\n    \n    # Dataset samples (features and true labels)\n    X = np.array([\n        [1.5, 0.2],\n        [1.2, 0.1],\n        [0.1, 1.5],\n        [0.2, 1.2],\n        [-0.9, -1.1],\n        [-1.1, -0.8],\n        [0.3, 0.3],\n        [-0.05, -0.05]\n    ])\n    Y = np.array([0, 0, 1, 1, 2, 2, 2, 2])\n    \n    # Test suite for the entropy regularization coefficient lambda\n    lambdas = [0.0, 0.1, 0.5, 1.0, 5.0, 50.0]\n    \n    # List to store the final formatted results for printing\n    results_for_print = []\n    \n    # === Iterate through each lambda value ===\n    for lambda_val in lambdas:\n        \n        # Accumulators for the current lambda\n        total_cond_p = np.zeros(C)\n        total_acc_contrib = 0.0\n        \n        # === Iterate through each data sample ===\n        for i in range(N):\n            x_i = X[i]\n            y_i = Y[i]\n            \n            # 1. Compute scores for the current sample\n            scores = W @ x_i + b\n            \n            # 2. Compute conditional probabilities p(y | x_i; lambda)\n            cond_p = np.zeros(C)\n            if lambda_val == 0.0:\n                # Deterministic case: one-hot vector at the max score index\n                # np.argmax handles ties by returning the first (smallest) index\n                j_max = np.argmax(scores)\n                cond_p[j_max] = 1.0\n            else:\n                # Stochastic case: softmax of scaled scores\n                scaled_scores = scores / lambda_val\n                # Use stable softmax: subtract max score before exponentiating\n                stable_scores = scaled_scores - np.max(stable_scores)\n                exp_scores = np.exp(stable_scores)\n                cond_p = exp_scores / np.sum(exp_scores)\n            \n            # 3. Accumulate results\n            total_cond_p += cond_p\n            total_acc_contrib += cond_p[y_i]\n            \n        # 4. Finalize computations for the current lambda\n        marginal_p = total_cond_p / N\n        accuracy = total_acc_contrib / N\n        \n        # Format the result string for this lambda according to the spec\n        p_vec_str = f\"[{','.join([f'{p:.6f}' for p in marginal_p])}]\"\n        acc_str = f'{accuracy:.6f}'\n        results_for_print.append(f\"[{p_vec_str},{acc_str}]\")\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n```", "id": "3146660"}, {"introduction": "Many real-world problems involve data with inherent structure, such as sequences where labels are not independent. This final practice introduces the linear-chain Conditional Random Field (CRF), a foundational model for sequence labeling that explicitly models dependencies between adjacent outputs in its joint conditional distribution $p(y_{1:T}|x_{1:T})$. You will implement the forward-backward algorithm, a powerful application of dynamic programming, to efficiently compute the exact node marginal probabilities $p(y_t|x_{1:T})$ without the need for intractable enumeration. This hands-on implementation will solidify your understanding of how to perform exact marginal inference in graphical models and demonstrate why modeling structure is essential for respecting complex constraints. [@problem_id:3146744]", "problem": "Consider a linear-chain Conditional Random Field (CRF) for sequence labeling, where each label $y_t$ takes values in $\\{0,1\\}$. The unnormalized probability of a label sequence $y_{1:T}$ conditioned on observed inputs $x_{1:T}$ is the product of position-wise unary potentials and pairwise transition potentials along the chain. Specifically, the model defines an unnormalized score proportional to the product $\\prod_{t=1}^{T} \\psi_t(y_t; x_t) \\cdot \\prod_{t=2}^{T} \\phi_t(y_{t-1}, y_t)$, where $\\psi_t(y_t; x_t)$ is a positive potential for state $y_t$ at position $t$ and $\\phi_t(y_{t-1}, y_t)$ is a positive potential for the transition from $y_{t-1}$ to $y_t$. The normalized conditional distribution is obtained by dividing this product by the partition function, but for computing marginal probabilities you must derive and implement the forward-backward (sum-product) dynamic programming algorithm using the chain factorization, without relying on any shortcut expressions.\n\nYour tasks are:\n1. Using the chain factorization, implement the forward-backward algorithm with scaling to compute the node marginals $p(y_t = 1 \\mid x_{1:T})$ at every position $t$ for each test case.\n2. Construct a naive independent model by normalizing the unary potentials at each position, producing $q_t(1) = \\frac{\\psi_t(1; x_t)}{\\psi_t(0; x_t) + \\psi_t(1; x_t)}$ and $q_t(0) = 1 - q_t(1)$, which incorrectly assumes conditional independence of labels across positions given $x_{1:T}$.\n3. For a hard structural constraint forbidding adjacent $1$s, defined by setting $\\phi_t(1,1) = 0$ for all transitions, compute:\n   - The total probability mass that the naive independent model assigns to forbidden adjacent-$1$ events, given by the sum over consecutive pairs $t$ of the product $q_t(1) \\cdot q_{t+1}(1)$.\n   - The same quantity under the CRF, computed as the sum over $t$ of the pairwise marginals $p(y_t = 1, y_{t+1} = 1 \\mid x_{1:T})$ derived via the forward-backward messages on the chain.\n4. Determine a boolean flag for each test case indicating whether the naive independent model’s marginals are inconsistent with the structural constraint while the CRF respects it, which you must compute as $\\text{inconsistent} = \\big(\\sum_t q_t(1) q_{t+1}(1) > 0\\big) \\land \\big(\\sum_t p(y_t=1, y_{t+1}=1 \\mid x_{1:T}) = 0\\big)$ simultaneously.\n5. Based on the observed inconsistency under the naive model, propose (in your solution reasoning) adding higher-order potentials, such as a triple-clique potential over $(y_{t-2}, y_{t-1}, y_t)$, to capture constraints like “exactly one $1$ in every length-$3$ window,” and explain why pairwise potentials alone cannot enforce such a constraint.\n\nAll floating-point outputs must be rounded to six decimal places. No physical units occur in this problem. Angles are not present. Percentages must be expressed as decimals. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For each test case, your program must output a list of the form:\n[crf_node_marginals_for_p(y_t=1),naive_node_marginals_for_q_t(1),naive_adjacent_violation_mass,crf_adjacent_violation_mass,inconsistent_boolean]\nwhere the two marginal lists contain the per-position probabilities rounded to six decimals, the two violation masses are floats rounded to six decimals, and the boolean is either True or False.\n\nTest Suite:\n- Test Case A (happy path): Sequence length $T = 5$. Unary potentials $\\psi_t(y_t)$ given as\n  - $t=1$: $\\psi_1(0) = 3.0$, $\\psi_1(1) = 1.0$\n  - $t=2$: $\\psi_2(0) = 1.0$, $\\psi_2(1) = 5.0$\n  - $t=3$: $\\psi_3(0) = 1.0$, $\\psi_3(1) = 4.0$\n  - $t=4$: $\\psi_4(0) = 2.0$, $\\psi_4(1) = 1.0$\n  - $t=5$: $\\psi_5(0) = 1.0$, $\\psi_5(1) = 3.0$\n  Pairwise transition potentials for $t=2,3,4,5$: $\\phi_t(0,0) = 1.0$, $\\phi_t(0,1) = 1.0$, $\\phi_t(1,0) = 1.0$, $\\phi_t(1,1) = 0.0$.\n- Test Case B (boundary condition): Sequence length $T = 4$. Unary potentials\n  - $t=1$: $\\psi_1(0) = 1.0$, $\\psi_1(1) = 0.0$\n  - $t=2$: $\\psi_2(0) = 1.2$, $\\psi_2(1) = 0.0$\n  - $t=3$: $\\psi_3(0) = 0.9$, $\\psi_3(1) = 0.0$\n  - $t=4$: $\\psi_4(0) = 1.0$, $\\psi_4(1) = 0.0$\n  Pairwise transition potentials for $t=2,3,4$: $\\phi_t(0,0) = 1.0$, $\\phi_t(0,1) = 1.0$, $\\phi_t(1,0) = 1.0$, $\\phi_t(1,1) = 0.0$.\n- Test Case C (edge case with symmetry): Sequence length $T = 6$. Unary potentials\n  - $t=1$: $\\psi_1(0) = 1.0$, $\\psi_1(1) = 1.0$\n  - $t=2$: $\\psi_2(0) = 1.0$, $\\psi_2(1) = 1.0$\n  - $t=3$: $\\psi_3(0) = 1.0$, $\\psi_3(1) = 1.0$\n  - $t=4$: $\\psi_4(0) = 1.0$, $\\psi_4(1) = 1.0$\n  - $t=5$: $\\psi_5(0) = 1.0$, $\\psi_5(1) = 1.0$\n  - $t=6$: $\\psi_6(0) = 1.0$, $\\psi_6(1) = 1.0$\n  Pairwise transition potentials for $t=2,3,4,5,6$: $\\phi_t(0,0) = 1.0$, $\\phi_t(0,1) = 1.0$, $\\phi_t(1,0) = 1.0$, $\\phi_t(1,1) = 0.0$.\n\nYour program must:\n- Implement scaled forward-backward to compute node marginals $p(y_t=1 \\mid x_{1:T})$ for each test case.\n- Compute naive $q_t(1)$ by local normalization of unary potentials.\n- Compute the total adjacent-$1$ violation mass under the naive model as $\\sum_{t=1}^{T-1} q_t(1) \\cdot q_{t+1}(1)$, and under the CRF as $\\sum_{t=1}^{T-1} p(y_t=1, y_{t+1}=1 \\mid x_{1:T})$, using pairwise marginals derived from the forward-backward messages.\n- Output a single line with the aggregated list of the three per-test-case result lists, with no spaces, rounding all floats to six decimals.", "solution": "The problem requires the implementation and analysis of a linear-chain Conditional Random Field (CRF) and a comparison with a simpler, naive independent model. The core task is to apply the forward-backward algorithm to compute marginal probabilities under the CRF, which correctly models dependencies between adjacent labels, and contrast these with the results from a model that assumes independence.\n\nFirst, we define the linear-chain CRF model. Given a sequence of observations $x_{1:T}$, the conditional probability of a label sequence $y_{1:T}$ where each $y_t \\in \\{0, 1\\}$ is given by:\n$$\np(y_{1:T} | x_{1:T}) = \\frac{1}{Z(x_{1:T})} \\left( \\prod_{t=1}^{T} \\psi_t(y_t; x_t) \\right) \\left( \\prod_{t=2}^{T} \\phi_t(y_{t-1}, y_t) \\right)\n$$\nHere, $\\psi_t(y_t; x_t)$ is the unary potential for label $y_t$ at position $t$, derived from the input $x_t$, and $\\phi_t(y_{t-1}, y_t)$ is the pairwise transition potential from label $y_{t-1}$ to $y_t$. For notational simplicity, we will write $\\psi_t(y_t)$ as the dependency on $x_t$ is implicit in the provided values. The denominator $Z(x_{1:T})$ is the partition function, which is a sum over all possible label sequences $y'_{1:T}$ to ensure the distribution normalizes to $1$:\n$$\nZ(x_{1:T}) = \\sum_{y'_{1:T}} \\left( \\prod_{t=1}^{T} \\psi_t(y'_t) \\right) \\left( \\prod_{t=2}^{T} \\phi_t(y'_{t-1}, y'_t) \\right)\n$$\n\n### 1. The Scaled Forward-Backward Algorithm\n\nTo compute the required marginal probabilities without explicitly summing over an exponential number of sequences, we use the forward-backward (or sum-product) algorithm. To prevent numerical underflow with long sequences, a scaling procedure is employed.\n\nThe **forward pass** computes scaled messages, $\\alpha_t(y_t)$, which represent the normalized sum of scores of all partial label sequences from $1$ to $t$ ending with label $y_t$.\nThe recurrence is defined as follows:\n-   **Initialization ($t=1$):**\n    The unscaled message is $\\hat{\\alpha}_1(y_1) = \\psi_1(y_1)$.\n    The first scaling factor is $s_1 = \\sum_{y_1} \\hat{\\alpha}_1(y_1)$.\n    The scaled message is $\\alpha_1(y_1) = \\hat{\\alpha}_1(y_1) / s_1$.\n-   **Recursion ($t=2, \\dots, T$):**\n    The unscaled message is $\\hat{\\alpha}_t(y_t) = \\psi_t(y_t) \\sum_{y_{t-1}} \\alpha_{t-1}(y_{t-1}) \\phi_t(y_{t-1}, y_t)$.\n    The scaling factor is $s_t = \\sum_{y_t} \\hat{\\alpha}_t(y_t)$.\n    The scaled message is $\\alpha_t(y_t) = \\hat{\\alpha}_t(y_t) / s_t$.\n\nThe **backward pass** computes scaled messages, $\\beta_t(y_t)$, which represent the normalized sum of scores of all partial label sequences from $t+1$ to $T$, given that the label at position $t$ is $y_t$.\nThe recurrence is defined as follows:\n-   **Initialization ($t=T$):**\n    The final backward message is initialized to unity: $\\beta_T(y_T) = 1$.\n-   **Recursion ($t=T-1, \\dots, 1$):**\n    The scaled message is $\\beta_t(y_t) = \\frac{1}{s_{t+1}} \\sum_{y_{t+1}} \\beta_{t+1}(y_{t+1}) \\psi_{t+1}(y_{t+1}) \\phi_{t+1}(y_t, y_{t+1})$. Note the crucial use of the forward scaling factor $s_{t+1}$ to maintain a consistent scale.\n\nWith these scaled messages, the **node marginal probability** $p(y_t = k | x_{1:T})$ is computed elegantly as the product of the corresponding forward and backward messages:\n$$\np(y_t = k | x_{1:T}) = \\alpha_t(k) \\beta_t(k)\n$$\nThe normalization is implicitly handled by the scaling procedure, as $\\sum_k \\alpha_t(k) \\beta_t(k) = 1$ for any $t$.\n\nThe **pairwise (edge) marginal probability** $p(y_{t-1}=i, y_t=j | x_{1:T})$ is needed to evaluate the structural constraint. It is computed as:\n$$\np(y_{t-1}=i, y_t=j | x_{1:T}) = \\frac{1}{s_t} \\alpha_{t-1}(i) \\psi_t(j) \\phi_t(i, j) \\beta_t(j)\n$$\n\n### 2. The Naive Independent Model\n\nThis model serves as a baseline to illustrate the limitations of ignoring label dependencies. It assumes that each label $y_t$ is conditionally independent of all other labels given the observation $x_t$. The probability of a label at position $t$ is obtained by simply normalizing the local unary potentials:\n$$\nq_t(y_t=1) = \\frac{\\psi_t(1)}{\\psi_t(0) + \\psi_t(1)} \\quad \\text{and} \\quad q_t(y_t=0) = \\frac{\\psi_t(0)}{\\psi_t(0) + \\psi_t(1)}\n$$\nThis model is computationally trivial but fails to capture any structural constraints or correlations between labels.\n\n### 3. Structural Constraints and Violation Mass\n\nThe problem imposes a hard structural constraint: adjacent labels cannot both be $1$. This is encoded in the CRF by setting the transition potential $\\phi_t(1, 1) = 0$ for all $t$.\n\nThe total probability mass assigned to this forbidden event under the CRF is the sum of the pairwise marginals for this configuration:\n$$\n\\text{CRF Violation Mass} = \\sum_{t=1}^{T-1} p(y_t=1, y_{t+1}=1 | x_{1:T})\n$$\nFrom the edge marginal formula, each term $p(y_t=1, y_{t+1}=1 | x_{1:T})$ includes the factor $\\phi_{t+1}(1, 1)$, which is $0$. Therefore, every term in the sum is $0$, and the total CRF violation mass is exactly $0$. The CRF, by its construction, perfectly respects the hard constraint.\n\nIn contrast, the naive model is oblivious to transition potentials. It computes the probability of two adjacent $1$s as if they were independent events: $q_t(1) \\cdot q_{t+1}(1)$. The total mass it assigns to forbidden configurations is:\n$$\n\\text{Naive Violation Mass} = \\sum_{t=1}^{T-1} q_t(1) \\cdot q_{t+1}(1)\n$$\nThis sum will be non-zero as long as any two consecutive unary potentials give non-zero probability to the label $1$, exposing the model's fundamental flaw. The problem asks for a boolean flag to capture this inconsistency:\n$$\n\\text{inconsistent} = \\left( \\sum_{t=1}^{T-1} q_t(1) q_{t+1}(1) > 0 \\right) \\land \\left( \\sum_{t=1}^{T-1} p(y_t=1, y_{t+1}=1 | x_{1:T}) = 0 \\right)\n$$\n\n### 4. Higher-Order Potentials for Complex Constraints\n\nThe problem proposes a more complex constraint: \"exactly one $1$ in every length-$3$ window,\" meaning for any $t \\geq 3$, the condition $y_{t-2} + y_{t-1} + y_t = 1$ must hold. Pairwise potentials are insufficient to enforce this. A linear-chain CRF possesses the Markov property: given $y_{t-1}$, $y_t$ is conditionally independent of all previous labels $y_{1:t-2}$. The transition from $y_{t-1}$ to $y_t$, governed by $\\phi_t(y_{t-1}, y_t)$, cannot depend on the value of $y_{t-2}$. For instance, the sequence $(1, 0, 1)$ violates the proposed constraint. To forbid it, the model must penalize the transition from $y_2=0$ to $y_3=1$ specifically because $y_1=1$. A pairwise potential $\\phi_3(0,1)$ cannot make this distinction, as it must also permit the valid sequence $(0, 0, 1)$ which contains the same $(0,1)$ transition.\n\nTo enforce such a constraint, one must introduce **higher-order potentials**. A trigram or triple-clique potential, $\\xi_t(y_{t-2}, y_{t-1}, y_t)$, is required. The model's unnormalized score would be modified to include this term:\n$$\n\\text{Score} \\propto \\left( \\prod_{t=1}^{T} \\psi_t(y_t) \\right) \\left( \\prod_{t=2}^{T} \\phi_t(y_{t-1}, y_t) \\right) \\left( \\prod_{t=3}^{T} \\xi_t(y_{t-2}, y_{t-1}, y_t) \\right)\n$$\nThe constraint can be enforced by setting $\\xi_t(y_{t-2}, y_{t-1}, y_t) = 0$ for any triplet that violates the rule (i.e., whose sum is not $1$) and $\\xi_t = 1$ otherwise. Inference in such a model requires a more general belief propagation algorithm or a specialized forward-backward algorithm on an expanded state space (where each state could be a pair of consecutive labels).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef format_result(result_list):\n    \"\"\"\n    Custom formatter to produce a string representation of a list\n    containing lists, floats, and booleans with no spaces, and with\n    floats rounded to 6 decimal places.\n    \"\"\"\n    def format_item(item):\n        if isinstance(item, list):\n            return f\"[{','.join(format_item(sub_item) for sub_item in item)}]\"\n        elif isinstance(item, float):\n            return f\"{item:.6f}\"\n        elif isinstance(item, bool):\n            return str(item)\n        else:\n            return str(item)\n\n    return f\"[{','.join(format_item(i) for i in result_list)}]\"\n\ndef solve_for_case(unaries, transitions):\n    \"\"\"\n    Implements the scaled forward-backward algorithm for a linear-chain CRF.\n    \n    Args:\n        unaries (np.ndarray): A (T, K) array of unary potentials psi_t(y_t).\n        transitions (np.ndarray): A (K, K) array of transition potentials phi(y_{t-1}, y_t).\n    \n    Returns:\n        A dictionary with computed results.\n    \"\"\"\n    T, K = unaries.shape\n    \n    # --- 1. Scaled Forward Pass ---\n    alpha = np.zeros((T, K))\n    scales = np.zeros(T)\n    \n    # Initialization (t=0, corresponding to sequence position 1)\n    alpha_hat_0 = unaries[0]\n    scales[0] = np.sum(alpha_hat_0)\n    alpha[0] = alpha_hat_0 / scales[0] if scales[0] > 0 else np.zeros(K)\n    \n    # Recursion (t=1 to T-1)\n    for t in range(1, T):\n        alpha_hat_t = unaries[t] * (alpha[t-1] @ transitions)\n        scales[t] = np.sum(alpha_hat_t)\n        alpha[t] = alpha_hat_t / scales[t] if scales[t] > 0 else np.zeros(K)\n\n    # --- 2. Scaled Backward Pass ---\n    beta = np.zeros((T, K))\n    \n    # Initialization (t=T-1)\n    beta[T-1] = 1.0\n    \n    # Recursion (t=T-2 down to 0)\n    for t in range(T - 2, -1, -1):\n        if scales[t+1] > 0:\n            beta[t] = (beta[t+1] * unaries[t+1]) @ transitions.T / scales[t+1]\n        else:\n            beta[t] = np.zeros(K)\n\n    # --- 3. Compute CRF Node Marginals ---\n    # p(y_t) = alpha_t * beta_t\n    crf_node_marginals = alpha * beta\n    # We need p(y_t = 1)\n    crf_marginals_y1 = crf_node_marginals[:, 1]\n\n    # --- 4. Compute CRF Edge Marginals and Violation Mass ---\n    # p(y_{t-1}, y_t) = (1/s_t) * alpha_{t-1} . (psi_t * beta_t) . phi_t\n    crf_violation_mass = 0.0\n    for t in range(1, T):\n        # We only need the p(y_{t-1}=1, y_t=1) term\n        if scales[t] > 0:\n            p_edge_11 = (1 / scales[t]) * alpha[t-1, 1] * unaries[t, 1] * transitions[1, 1] * beta[t, 1]\n            crf_violation_mass += p_edge_11\n        \n    # --- 5. Compute Naive Model Marginals and Violation Mass ---\n    unary_sums = np.sum(unaries, axis=1, keepdims=True)\n    # Handle division by zero if a unary potential pair is (0,0)\n    safe_unary_sums = np.where(unary_sums == 0, 1, unary_sums)\n    naive_node_marginals = unaries / safe_unary_sums\n    naive_marginals_y1 = naive_node_marginals[:, 1]\n    \n    naive_violation_mass = np.sum(naive_marginals_y1[:-1] * naive_marginals_y1[1:])\n\n    # --- 6. Compute Inconsistency Flag ---\n    is_inconsistent = (naive_violation_mass > 1e-9) and (abs(crf_violation_mass)  1e-9)\n\n    return {\n        \"crf_marginals_y1\": [round(p, 6) for p in crf_marginals_y1.tolist()],\n        \"naive_marginals_y1\": [round(q, 6) for q in naive_marginals_y1.tolist()],\n        \"naive_violation_mass\": round(float(naive_violation_mass), 6),\n        \"crf_violation_mass\": round(float(crf_violation_mass), 6),\n        \"is_inconsistent\": is_inconsistent,\n    }\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"unaries\": np.array([\n                [3.0, 1.0], [1.0, 5.0], [1.0, 4.0], [2.0, 1.0], [1.0, 3.0]\n            ]),\n            \"transitions\": np.array([\n                [1.0, 1.0], [1.0, 0.0]\n            ])\n        },\n        {\n            \"unaries\": np.array([\n                [1.0, 0.0], [1.2, 0.0], [0.9, 0.0], [1.0, 0.0]\n            ]),\n            \"transitions\": np.array([\n                [1.0, 1.0], [1.0, 0.0]\n            ])\n        },\n        {\n            \"unaries\": np.array([\n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n            ]),\n            \"transitions\": np.array([\n                [1.0, 1.0], [1.0, 0.0]\n            ])\n        }\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        res_dict = solve_for_case(case[\"unaries\"], case[\"transitions\"])\n        # Assemble the list in the specified order\n        result_list = [\n            res_dict[\"crf_marginals_y1\"],\n            res_dict[\"naive_marginals_y1\"],\n            res_dict[\"naive_violation_mass\"],\n            res_dict[\"crf_violation_mass\"],\n            res_dict[\"is_inconsistent\"]\n        ]\n        # Format the list into a string with no spaces\n        res_str = format_result(result_list)\n        results_str_list.append(res_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3146744"}]}