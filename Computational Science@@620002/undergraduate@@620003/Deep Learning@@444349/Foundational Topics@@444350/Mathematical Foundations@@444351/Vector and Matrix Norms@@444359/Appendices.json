{"hands_on_practices": [{"introduction": "The first step in mastering any new mathematical concept is to apply its definitions directly. This exercise provides a foundational workout, asking you to calculate four of the most common matrix norms for a simple, structured matrix. By working through the 1-norm, $\\infty$-norm, Frobenius norm, and the more complex 2-norm (or spectral norm), you will build a strong intuition for how each of these norms quantifies the \"magnitude\" of a matrix in a distinct way [@problem_id:2449594].", "problem": "Let $m,n \\in \\mathbb{N}$ with $m \\geq 1$ and $n \\geq 1$. For a matrix $A \\in \\mathbb{R}^{m \\times n}$ and $p \\in \\{1,2,\\infty\\}$, the operator norm induced by the vector $p$-norm is defined by\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}},\n$$\nwhere, for $x \\in \\mathbb{R}^{k}$, the vector norms are $\\|x\\|_{1} = \\sum_{i=1}^{k} |x_{i}|$, $\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{k} |x_{i}|^{2}}$, and $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq k} |x_{i}|$. The Frobenius norm of $A$ is defined by\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}.\n$$\nConsider the all-ones matrix $J \\in \\mathbb{R}^{m \\times n}$ with entries $J_{ij} = 1$ for all $i,j$. Determine $\\|J\\|_{1}$, $\\|J\\|_{2}$, $\\|J\\|_{\\infty}$, and $\\|J\\|_{F}$ as explicit functions of $m$ and $n$. Provide your final answer as a single row vector in the order $\\big(\\|J\\|_{1}, \\|J\\|_{2}, \\|J\\|_{\\infty}, \\|J\\|_{F}\\big)$. The answer must be exact; do not approximate.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- $m, n \\in \\mathbb{N}$ with $m \\geq 1$ and $n \\geq 1$.\n- Matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- Operator norm: $\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$ for $p \\in \\{1, 2, \\infty\\}$.\n- Vector norms for $x \\in \\mathbb{R}^{k}$:\n  - $\\|x\\|_{1} = \\sum_{i=1}^{k} |x_{i}|$\n  - $\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{k} |x_{i}|^{2}}$\n  - $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq k} |x_{i}|$\n- Frobenius norm: $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}$.\n- The matrix under consideration is the all-ones matrix $J \\in \\mathbb{R}^{m \\times n}$ with entries $J_{ij} = 1$ for all $i,j$.\n- The objective is to determine $\\|J\\|_{1}$, $\\|J\\|_{2}$, $\\|J\\|_{\\infty}$, and $\\|J\\|_{F}$ as explicit functions of $m$ and $n$.\n- The final answer is required in the format of a row vector: $\\big(\\|J\\|_{1}, \\|J\\|_{2}, \\|J\\|_{\\infty}, \\|J\\|_{F}\\big)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded:** The problem uses standard, universally accepted definitions for vector and matrix norms from the field of linear algebra and numerical analysis. It is scientifically and mathematically sound.\n- **Well-Posed:** The problem is clearly stated. The matrix $J$ is unambiguously defined, as are the norms to be computed. A unique, meaningful solution exists for each of the four requested quantities.\n- **Objective:** The problem is formulated with precise mathematical language, devoid of any subjectivity or ambiguity.\n\nThe problem is found to be free of any flaws such as scientific unsoundness, incompleteness, or contradiction.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\nWe proceed to calculate the four specified norms for the all-ones matrix $J \\in \\mathbb{R}^{m \\times n}$.\n\n**1. Calculation of the $1$-norm, $\\|J\\|_{1}$**\nThe operator $1$-norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as the maximum absolute column sum:\n$$\n\\|A\\|_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{m} |a_{ij}|\n$$\nFor the matrix $J$, all entries are $J_{ij} = 1$. We compute the sum of the absolute values for an arbitrary column $j$:\n$$\n\\sum_{i=1}^{m} |J_{ij}| = \\sum_{i=1}^{m} |1| = \\sum_{i=1}^{m} 1 = m\n$$\nThis sum is constant for every column $j$, where $1 \\leq j \\leq n$. Therefore, the maximum of these identical sums is $m$.\n$$\n\\|J\\|_{1} = m\n$$\n\n**2. Calculation of the $\\infty$-norm, $\\|J\\|_{\\infty}$**\nThe operator $\\infty$-norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as the maximum absolute row sum:\n$$\n\\|A\\|_{\\infty} = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{n} |a_{ij}|\n$$\nFor the matrix $J$, we compute the sum of the absolute values for an arbitrary row $i$:\n$$\n\\sum_{j=1}^{n} |J_{ij}| = \\sum_{j=1}^{n} |1| = \\sum_{j=1}^{n} 1 = n\n$$\nThis sum is constant for every row $i$, where $1 \\leq i \\leq m$. The maximum of these identical sums is $n$.\n$$\n\\|J\\|_{\\infty} = n\n$$\n\n**3. Calculation of the Frobenius norm, $\\|J\\|_{F}$**\nThe Frobenius norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined by:\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}\n$$\nFor the matrix $J$, this becomes:\n$$\n\\|J\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |1|^{2}} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} 1}\n$$\nThe double summation represents the total number of entries in the matrix, which is $mn$.\n$$\n\\|J\\|_{F} = \\sqrt{mn}\n$$\n\n**4. Calculation of the $2$-norm (Spectral Norm), $\\|J\\|_{2}$**\nThe operator $2$-norm of a matrix $A$ is its largest singular value, $\\sigma_{\\max}(A)$. This is equivalent to the square root of the largest eigenvalue of the matrix $A^T A$.\n$$\n\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^T A)}\n$$\nFor our matrix $J$, we first construct the matrix $J^T J$. The matrix $J$ is an $m \\times n$ matrix of all ones, so its transpose $J^T$ is an $n \\times m$ matrix of all ones. The product $J^T J$ is an $n \\times n$ matrix. An entry $(k,l)$ of this product is:\n$$\n(J^T J)_{kl} = \\sum_{i=1}^{m} (J^T)_{ki} J_{il} = \\sum_{i=1}^{m} J_{ik} J_{il} = \\sum_{i=1}^{m} 1 \\cdot 1 = m\n$$\nThus, $J^T J$ is an $n \\times n$ matrix where every entry is equal to $m$. We can write this as $J^T J = m U_n$, where $U_n$ is the $n \\times n$ matrix of all ones. The eigenvalues of $J^T J$ are $m$ times the eigenvalues of $U_n$.\n\nThe matrix $U_n$ has rank $1$, since all its columns are identical. A matrix of rank $k$ has at least $n-k$ zero eigenvalues. Thus, $U_n$ has an eigenvalue of $0$ with multiplicity of at least $n-1$. The sum of the eigenvalues of a matrix equals its trace. The trace of $U_n$ is $\\text{Tr}(U_n) = \\sum_{i=1}^{n} 1 = n$. Since $n-1$ eigenvalues are $0$, the remaining eigenvalue must be $n$. So, the eigenvalues of $U_n$ are $n$ (with multiplicity $1$) and $0$ (with multiplicity $n-1$).\n\nThe eigenvalues of $J^T J = m U_n$ are therefore $mn$ (multiplicity $1$) and $0$ (multiplicity $n-1$). The largest eigenvalue is $\\lambda_{\\max}(J^T J) = mn$.\nThe $2$-norm is the square root of this value:\n$$\n\\|J\\|_{2} = \\sqrt{\\lambda_{\\max}(J^T J)} = \\sqrt{mn}\n$$\n\nIn summary, the four computed norms are:\n- $\\|J\\|_{1} = m$\n- $\\|J\\|_{2} = \\sqrt{mn}$\n- $\\|J\\|_{\\infty} = n$\n- $\\|J\\|_{F} = \\sqrt{mn}$\n\nThe final answer is presented as a row vector in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} m & \\sqrt{mn} & n & \\sqrt{mn} \\end{pmatrix}}\n$$", "id": "2449594"}, {"introduction": "Beyond simple calculation, the true power of norms lies in their fundamental properties. This practice moves into conceptual territory by focusing on the triangle inequality, a cornerstone of all normed spaces. You are challenged to go beyond simply accepting the inequality and instead to construct a specific scenario where it holds with perfect equality, revealing a deep and elegant connection between the geometry of vectors and the algebraic structure of eigenvectors and eigenvalues [@problem_id:2447194].", "problem": "In a linear state-transition setting often used in computational economics and finance, consider a real $2 \\times 2$ matrix $A$ and the identity matrix $I$. The Euclidean vector norm $\\|\\cdot\\|_2$ is used throughout. Construct a non-diagonal real $2 \\times 2$ matrix $A$ and find a nonzero vector $x \\in \\mathbb{R}^2$ such that the triangle inequality\n$$\n\\|(A+I)x\\|_2 \\leq \\|Ax\\|_2 + \\|x\\|_2\n$$\nholds with equality. Define the ratio\n$$\nR \\equiv \\frac{\\|(A+I)x\\|_2}{\\|Ax\\|_2 + \\|x\\|_2}.\n$$\nCompute the exact value of $R$ for your constructed pair $(A,x)$, and briefly interpret the condition on $(A,x)$ under which equality in the triangle inequality is achieved. Your final reported answer must be the exact value of $R$ (no rounding required).", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- `A`: A real `2 \\times 2` non-diagonal matrix.\n- `I`: The identity matrix.\n- `x`: A nonzero vector in `\\mathbb{R}^2`.\n- Norm: The Euclidean vector norm `\\|\\cdot\\|_2`.\n- Inequality: `\\|(A+I)x\\|_2 \\leq \\|Ax\\|_2 + \\|x\\|_2`.\n- Task: Construct `A` and `x` such that the inequality holds as an equality.\n- Definition: Ratio `R \\equiv \\frac{\\|(A+I)x\\|_2}{\\|Ax\\|_2 + \\|x\\|_2}`.\n- Task: Compute the exact value of `R` for the constructed pair `(A,x)`.\n- Task: Interpret the condition for equality.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the established principles of linear algebra and vector norm theory. It is well-posed, as a valid construction is possible and leads to a unique value for the specified ratio. The problem is stated with objective and precise mathematical language, is self-contained, and free of contradictions. The scenario is a standard exercise in the study of linear operators.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A full solution will be provided.\n\nThe problem requires constructing a matrix `A` and a vector `x` such that the triangle inequality for vectors becomes an equality. In general, for any two vectors `\\mathbf{u}` and `\\mathbf{v}` in an inner product space, the triangle inequality is `\\|\\mathbf{u}+\\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|$. Equality holds if and only if one vector is a non-negative scalar multiple of the other.\n\nIn this problem, the vectors are `\\mathbf{u} = Ax` and `\\mathbf{v} = Ix = x`. The given expression is `\\|Ax+x\\|_2 \\leq \\|Ax\\|_2 + \\|x\\|_2`. For this to hold with equality, we must satisfy the condition `Ax = kx` for some real scalar `k \\geq 0`. The case `x = k(Ax)` is also possible, but since `x` is non-zero, `A` would need to be invertible and this reduces to `A^{-1}x = kx`, which is equivalent to the first condition with eigenvalue `\\frac{1}{k}`. We proceed with the simpler form.\n\nThe condition `Ax = kx` means that `x` must be an eigenvector of the matrix `A`, and the corresponding eigenvalue `k` must be non-negative. The problem states that `x` must be a nonzero vector, which is consistent with the definition of an eigenvector.\n\nOur task is therefore to:\n$1$. Construct a real, non-diagonal `2 \\times 2` matrix `A`.\n$2$. Ensure `A` has at least one non-negative eigenvalue, `\\lambda \\geq 0`.\n$3$. Find a corresponding eigenvector `x`.\n$4$. Calculate the ratio `R` for this pair `(A,x)`.\n\nLet us construct such a matrix `A`. We can design `A` to have simple, non-negative integer eigenvalues. Let the eigenvalues be `\\lambda_1 = 1` and `\\lambda_2 = 4$. Both are non-negative.\nFor a `2 \\times 2` matrix, the sum of eigenvalues is the trace, and the product is the determinant.\n`\\mathrm{Tr}(A) = \\lambda_1 + \\lambda_2 = 1+4 = 5`.\n`\\det(A) = \\lambda_1 \\lambda_2 = 1 \\times 4 = 4`.\n\nLet `A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}`. We need `a+d=5` and `ad-bc=4`. To ensure `A` is non-diagonal, we require that `b` and `c` are not both zero.\nLet us choose `a=3`. Then `d=5-3=2`.\nThe determinant condition becomes `(3)(2) - bc = 4`, which simplifies to `6 - bc = 4`, or `bc=2`.\nWe can choose `b=2` and `c=1`. This gives the matrix:\n$$\nA = \\begin{pmatrix} 3 & 2 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThis matrix is real, `2 \\times 2`, and non-diagonal. Its eigenvalues are, by construction, `\\lambda_1 = 1` and `\\lambda_2 = 4`, which are both non-negative.\n\nNow, we find an eigenvector `x` for one of these eigenvalues. Let us choose `\\lambda = 4$. We need to solve the system `(A - \\lambda I)x = 0`, which is `(A - 4I)x = 0`.\n$$\n(A - 4I) = \\begin{pmatrix} 3-4 & 2 \\\\ 1 & 2-4 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 \\\\ 1 & -2 \\end{pmatrix}\n$$\nThe equation to solve is:\n$$\n\\begin{pmatrix} -1 & 2 \\\\ 1 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the single independent equation `-x_1 + 2x_2 = 0`, or `x_1 = 2x_2`.\nWe must choose a nonzero vector `x`. A simple choice is `x_2=1`, which gives `x_1=2`.\nSo, our constructed vector is `x = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}`.\n\nWe have now constructed the pair `(A,x)`:\n`A = \\begin{pmatrix} 3 & 2 \\\\ 1 & 2 \\end{pmatrix}` and `x = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}`.\n`A` is non-diagonal and `x` is non-zero. The corresponding eigenvalue is `\\lambda=4 \\geq 0`, so the equality condition `Ax = 4x` is met.\n\nNow we compute the ratio `R`.\n$$\nR = \\frac{\\|(A+I)x\\|_2}{\\|Ax\\|_2 + \\|x\\|_2}\n$$\nBy our theoretical deduction, since the condition for equality in the triangle inequality is satisfied, the numerator and denominator must be equal, so `R` must be `1`. We verify this by direct calculation.\n\nFirst, calculate the terms in the denominator:\n`Ax = 4x = 4 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}`.\n`\\|Ax\\|_2 = \\left\\| \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix} \\right\\|_2 = \\sqrt{8^2 + 4^2} = \\sqrt{64+16} = \\sqrt{80} = 4\\sqrt{5}`.\n`\\|x\\|_2 = \\left\\| \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\right\\|_2 = \\sqrt{2^2 + 1^2} = \\sqrt{4+1} = \\sqrt{5}`.\nThe denominator is `\\|Ax\\|_2 + \\|x\\|_2 = 4\\sqrt{5} + \\sqrt{5} = 5\\sqrt{5}`.\n\nNext, calculate the term in the numerator:\n`(A+I)x = Ax + x`.\nSince `Ax=4x`, we have `Ax+x = 4x+x=5x`.\n`(A+I)x = 5x = 5 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix}`.\n`\\|(A+I)x\\|_2 = \\left\\| \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix} \\right\\|_2 = \\sqrt{10^2 + 5^2} = \\sqrt{100+25} = \\sqrt{125} = 5\\sqrt{5}`.\n\nFinally, compute the ratio `R`.\n$$\nR = \\frac{5\\sqrt{5}}{5\\sqrt{5}} = 1\n$$\nThe calculation confirms the theoretical expectation.\n\nThe interpretation of the condition for equality is as follows: The equality `\\|(A+I)x\\|_2 = \\|Ax\\|_2 + \\|x\\|_2` holds if and only if the vector `Ax` is a non-negative scalar multiple of the vector `x`. Given that `x` must be non-zero, this is precisely the definition of `x` being an eigenvector of the matrix `A` corresponding to a non-negative eigenvalue.", "answer": "$$\n\\boxed{1}\n$$", "id": "2447194"}, {"introduction": "In many applications, especially in deep learning and stability analysis, the spectral norm ($\\|A\\|_2$) is the most important measure of a matrix's size, yet it is the least straightforward to compute by hand. This hands-on coding practice bridges the gap between abstract theory and practical implementation. You will derive and implement the power iteration method, a fundamental numerical algorithm, to estimate the spectral norm, thereby turning a theoretical definition into a tangible computational tool [@problem_id:2449590].", "problem": "You are asked to design and implement a deterministic program that estimates the induced matrix $2$-norm $\\lVert A \\rVert_2$ for real matrices $A \\in \\mathbb{R}^{m \\times n}$ using a power-iteration-based algorithm, starting only from fundamental definitions of vector and matrix norms and basic properties of eigenvalues of symmetric matrices. Your goal is to derive, justify, and code an algorithm that does not rely on explicitly forming any matrix product beyond standard matrix-vector multiplication, and that works for both square and rectangular matrices.\n\nTasks to complete:\n1. Starting from the core definition of the induced matrix $2$-norm $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$ and the fact that for any real matrix $A$ the matrix $A^\\top A$ is symmetric positive semidefinite, derive an iterative scheme that estimates $\\lVert A \\rVert_2$ by repeatedly applying matrix-vector multiplications of the form $A \\mathbf{x}$ and $A^\\top \\mathbf{y}$ without explicitly forming $A^\\top A$. Your derivation must be grounded in these definitions and properties and should include a clear stopping criterion based on the change of the estimate.\n2. Implement the derived algorithm as a complete, runnable program. The algorithm must:\n   - Initialize with a deterministic nonzero vector in $\\mathbb{R}^n$, normalize it in the $\\ell_2$ sense, and at each iteration use only the operations $A \\mathbf{x}$ and $A^\\top \\mathbf{y}$.\n   - Terminate when the relative change in the estimated norm between successive iterations is below a tolerance $\\varepsilon = 10^{-10}$, or when a maximum of $10^4$ iterations is reached, whichever occurs first.\n   - Handle the edge case $A = 0$ robustly, yielding the estimate $\\lVert A \\rVert_2 = 0$.\n   - Return a nonnegative estimate of $\\lVert A \\rVert_2$.\n3. Your program must evaluate the following test suite of matrices and report the estimated norms in the specified format:\n   - Case $1$ (square, symmetric positive definite):\n     $$A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}.$$\n   - Case $2$ (square, highly non-normal):\n     $$A_2 = \\begin{bmatrix} 1 & 10 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0.1 \\end{bmatrix}.$$\n   - Case $3$ (tall rectangular):\n     $$A_3 = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\\\ 2 & 0 \\\\ 0 & 0 \\end{bmatrix}.$$\n   - Case $4$ (wide rectangular):\n     $$A_4 = \\begin{bmatrix} 1 & 0 & 2 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}.$$\n   - Case $5$ (zero matrix):\n     $$A_5 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}.$$\n   - Case $6$ (square, nearly multiple dominant singular values):\n     $$A_6 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.999 \\end{bmatrix}.$$\n4. Final output format: Your program should produce a single line of output containing the results as a comma-separated list of the six estimated norms, in the order $A_1$ through $A_6$, enclosed in square brackets. Each value must be rounded to $8$ decimal places. For example, a valid output format is\n   $$[\\text{v}_1,\\text{v}_2,\\text{v}_3,\\text{v}_4,\\text{v}_5,\\text{v}_6],$$\n   where each $\\text{v}_i$ is a decimal rounded to $8$ places. No additional text or lines should be printed.\n\nImplementation constraints:\n- The program must be fully self-contained, require no user input, and use only the Python standard library and permitted libraries.\n- Angles are not used in this problem.\n- No physical units are involved.", "solution": "The problem requires the derivation and implementation of an iterative algorithm to estimate the induced matrix $2$-norm, $\\lVert A \\rVert_2$, for a real matrix $A \\in \\mathbb{R}^{m \\times n}$. The derivation must be based on first principles and avoid the explicit formation of matrix products such as $A^\\top A$.\n\nThe validation of the problem statement is performed first.\n\n**Step 1: Extract Givens**\n- **Definition**: The induced matrix $2$-norm is defined as $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$.\n- **Property**: For any real matrix $A$, the matrix $A^\\top A$ is symmetric positive semidefinite.\n- **Algorithm Goal**: Derive an iterative scheme using only matrix-vector multiplications of the form $A \\mathbf{x}$ and $A^\\top \\mathbf{y}$ to estimate $\\lVert A \\rVert_2$.\n- **Implementation Constraints**:\n  - **Initialization**: Use a deterministic, normalized, nonzero vector in $\\mathbb{R}^n$.\n  - **Stopping Criteria**: Terminate when the relative change in the norm estimate is less than a tolerance $\\varepsilon = 10^{-10}$, or after a maximum of $10^4$ iterations.\n  - **Edge Case**: Handle the zero matrix $A = 0$ correctly, yielding an estimate of $0$.\n  - **Return Value**: The function must return a nonnegative estimate of $\\lVert A \\rVert_2$.\n- **Test Suite**: Six matrices ($A_1$ to $A_6$) are provided for evaluation.\n- **Output Format**: A single line with a comma-separated list of the six estimated norms, rounded to $8$ decimal places, enclosed in square brackets.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded**: The problem is based on the standard definition of the induced $2$-norm and its fundamental relationship with the largest eigenvalue of $A^\\top A$. The proposed method, power iteration, is a classical and scientifically sound algorithm in numerical linear algebra for finding the dominant eigenvalue. The problem is firmly grounded in established mathematical principles.\n2.  **Well-Posed**: The problem is well-posed. It asks for an estimate of a uniquely defined mathematical quantity ($\\lVert A \\rVert_2$). The algorithm's termination is guaranteed by the maximum iteration limit and the convergence criterion.\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free of ambiguity or subjective claims.\n4.  **Flaw Analysis**:\n    - **Scientific or Factual Unsoundness**: None. The premises are correct.\n    - **Non-Formalizable or Irrelevant**: None. The problem is a standard task in computational engineering and numerical analysis, directly concerning vector and matrix norms.\n    - **Incomplete or Contradictory Setup**: None. All necessary components are specified: the goal, the constraints on the method, termination criteria, and test cases.\n    - **Unrealistic or Infeasible**: None. The algorithm is practical, and the test matrices are standard examples.\n    - **Ill-Posed or Poorly Structured**: None. The structure is clear, guiding from theoretical derivation to implementation.\n    - **Outside Scientific Verifiability**: None. The correctness of the algorithm and the accuracy of its results can be verified against known analytical solutions or standard library functions (e.g., singular value decomposition).\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A complete, reasoned solution will be provided.\n\n**Derivation and Algorithmic Design**\n\nThe starting point is the definition of the induced matrix $2$-norm for a matrix $A \\in \\mathbb{R}^{m \\times n}$:\n$$\n\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}\n$$\nSince the norm is always non-negative, we can consider its square:\n$$\n\\lVert A \\rVert_2^2 = \\left( \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2} \\right)^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2^2}{\\lVert \\mathbf{x} \\rVert_2^2}\n$$\nUsing the definition of the Euclidean norm, $\\lVert \\mathbf{v} \\rVert_2^2 = \\mathbf{v}^\\top \\mathbf{v}$, we can rewrite the expression as:\n$$\n\\lVert A \\rVert_2^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{(A \\mathbf{x})^\\top (A \\mathbf{x})}{\\mathbf{x}^\\top \\mathbf{x}} = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\mathbf{x}^\\top A^\\top A \\mathbf{x}}{\\mathbf{x}^\\top \\mathbf{x}}\n$$\nThis expression is the Rayleigh quotient for the matrix $B = A^\\top A$. A fundamental theorem in linear algebra states that the supremum of the Rayleigh quotient for a symmetric matrix is its largest eigenvalue, $\\lambda_{\\text{max}}$. The matrix $B = A^\\top A$ is indeed symmetric (since $(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A$) and positive semidefinite. Therefore, we have the crucial relationship:\n$$\n\\lVert A \\rVert_2^2 = \\lambda_{\\text{max}}(A^\\top A)\n$$\nThis implies that the induced matrix $2$-norm is the square root of the largest eigenvalue of $A^\\top A$:\n$$\n\\lVert A \\rVert_2 = \\sqrt{\\lambda_{\\text{max}}(A^\\top A)}\n$$\nThe value $\\sqrt{\\lambda_{\\text{max}}(A^\\top A)}$ is also, by definition, the largest singular value of $A$, denoted $\\sigma_1(A)$.\n\nThe problem now reduces to finding $\\lambda_{\\text{max}}(A^\\top A)$ without explicitly computing the matrix $A^\\top A$. This can be achieved using the **power iteration** method. The power method is an iterative algorithm to find the eigenvalue with the largest magnitude (the dominant eigenvalue) and its corresponding eigenvector. For a symmetric positive semidefinite matrix like $A^\\top A$, all eigenvalues are real and non-negative, so the largest magnitude eigenvalue is simply $\\lambda_{\\text{max}}$.\n\nThe standard power iteration for a matrix $B$ is:\n1.  Start with a non-zero vector $\\mathbf{v}_0$.\n2.  Iterate for $k = 1, 2, \\dots$: $\\mathbf{v}_k = \\frac{B \\mathbf{v}_{k-1}}{\\lVert B \\mathbf{v}_{k-1} \\rVert_2}$.\nThe sequence of vectors $\\{\\mathbf{v}_k\\}$ converges to the eigenvector corresponding to $\\lambda_{\\text{max}}(B)$, provided the initial vector $\\mathbf{v}_0$ has a non-zero component in the direction of this eigenvector.\n\nIn our case, $B = A^\\top A$. The iterative step is $\\mathbf{v}_k \\propto (A^\\top A) \\mathbf{v}_{k-1}$. As required, we avoid forming $A^\\top A$ by performing the multiplication as two sequential matrix-vector products:\n1.  First, compute $\\mathbf{y}_{k-1} = A \\mathbf{v}_{k-1}$.\n2.  Then, compute $\\mathbf{x}_k = A^\\top \\mathbf{y}_{k-1}$.\nSo, the core update is $\\mathbf{x}_k = A^\\top (A \\mathbf{v}_{k-1})$. The next normalized vector is $\\mathbf{v}_k = \\mathbf{x}_k / \\lVert \\mathbf{x}_k \\rVert_2$.\n\nWe also need an estimate for $\\lambda_{\\text{max}}(A^\\top A)$ at each iteration. This can be obtained from the Rayleigh quotient using the current eigenvector estimate $\\mathbf{v}_{k-1}$:\n$$\n\\lambda_k \\approx \\frac{\\mathbf{v}_{k-1}^\\top (A^\\top A) \\mathbf{v}_{k-1}}{\\mathbf{v}_{k-1}^\\top \\mathbf{v}_{k-1}}\n$$\nSince $\\mathbf{v}_{k-1}$ is a unit vector ($\\lVert \\mathbf{v}_{k-1} \\rVert_2 = 1$), its denominator is $1$. The numerator becomes:\n$$\n\\mathbf{v}_{k-1}^\\top A^\\top A \\mathbf{v}_{k-1} = (A \\mathbf{v}_{k-1})^\\top (A \\mathbf{v}_{k-1}) = \\mathbf{y}_{k-1}^\\top \\mathbf{y}_{k-1} = \\lVert \\mathbf{y}_{k-1} \\rVert_2^2\n$$\nSo, the estimate for the largest eigenvalue at iteration $k$ is $\\lambda_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2^2$.\nConsequently, the estimate for the matrix $2$-norm, $\\sigma_k = \\sqrt{\\lambda_k}$, is simply:\n$$\n\\sigma_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2 = \\lVert \\mathbf{y}_{k-1} \\rVert_2\n$$\nThis provides a simple and efficient way to update the norm estimate at each iteration.\n\n**Final Algorithm:**\nLet $A$ be an $m \\times n$ matrix. Let $\\varepsilon = 10^{-10}$ be the tolerance and $K_{\\text{max}} = 10^4$ be the maximum number of iterations.\n\n1.  **Handle trivial cases**: If $n=0$, the domain is empty, so $\\lVert A \\rVert_2 = 0$.\n2.  **Initialization**:\n    - Choose a deterministic non-zero starting vector $\\mathbf{v}_0 \\in \\mathbb{R}^n$. A standard choice is a vector of ones.\n    - Normalize it: $\\mathbf{v} \\leftarrow \\frac{\\mathbf{v}_0}{\\lVert \\mathbf{v}_0 \\rVert_2}$.\n    - Initialize the norm estimate, e.g., $\\sigma_{\\text{new}} \\leftarrow 0$.\n3.  **Iteration**: For $k=1, \\dots, K_{\\text{max}}$:\n    a.  Store the previous estimate: $\\sigma_{\\text{old}} \\leftarrow \\sigma_{\\text{new}}$.\n    b.  Apply the first matrix-vector product: $\\mathbf{y} \\leftarrow A \\mathbf{v}$.\n    c.  Update the norm estimate: $\\sigma_{\\text{new}} \\leftarrow \\lVert \\mathbf{y} \\rVert_2$.\n    d.  **Check for convergence**: If $k>1$ and $|\\sigma_{\\text{new}} - \\sigma_{\\text{old}}| < \\varepsilon \\cdot \\sigma_{\\text{new}}$, break the loop.\n    e.  **Handle zero matrix case**: If $\\sigma_{\\text{new}} = 0$, it implies $\\mathbf{y}=\\mathbf{0}$. This means $A\\mathbf{v}=\\mathbf{0}$. The matrix $A$ is singular, or possibly the zero matrix. The algorithm correctly yields $\\sigma_{\\text{new}} = 0$ and should terminate. We can break.\n    f.  Apply the second matrix-vector product: $\\mathbf{x} \\leftarrow A^\\top \\mathbf{y}$.\n    g.  Normalize the resulting vector for the next iteration: $\\mathbf{v} \\leftarrow \\frac{\\mathbf{x}}{\\lVert \\mathbf{x} \\rVert_2}$. If $\\lVert \\mathbf{x} \\rVert_2 = 0$, break. This happens if $\\mathbf{y}$ is in the null space of $A^\\top$, which for $\\mathbf{y}=A\\mathbf{v}$ implies $A\\mathbf{v}=\\mathbf{0}$, correctly yielding a norm of $0$.\n4.  **Return**: The final estimate $\\sigma_{\\text{new}}$.\n\nThis algorithm only uses matrix-vector products, adheres to all constraints, and correctly estimates $\\lVert A \\rVert_2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_induced_2_norm(A, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Estimates the induced matrix 2-norm (largest singular value) of a real matrix A\n    using the power iteration method.\n\n    The algorithm iteratively computes v_k = A^T * A * v_{k-1} without explicitly\n    forming the matrix A^T*A. The norm is estimated as ||A*v_k||_2.\n\n    Args:\n        A (np.ndarray): The input matrix, m x n.\n        tol (float): The relative tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The estimated induced 2-norm of A.\n    \"\"\"\n    # Get matrix dimensions\n    m, n = A.shape\n\n    # Handle the edge case of a matrix with zero columns.\n    if n == 0:\n        return 0.0\n\n    # Initialize with a deterministic non-zero vector in R^n.\n    # A vector of ones is a standard deterministic choice.\n    # The power method might fail if this initial vector is orthogonal\n    # to the dominant eigenvector of A^T*A. In practice, for general\n    # matrices and with finite-precision arithmetic, this is rare.\n    v = np.ones(n)\n    v /= np.linalg.norm(v)\n\n    norm_est = 0.0\n\n    for _ in range(max_iter):\n        norm_est_prev = norm_est\n\n        # First matrix-vector product: y = A*v\n        y = A @ v\n\n        # Update the norm estimate: ||A||_2 approx ||y||_2\n        norm_est = np.linalg.norm(y)\n\n        # Check for convergence using relative change.\n        # This check is safe because for a non-zero matrix, norm_est converges\n        # to a positive value.\n        if norm_est > 0 and abs(norm_est - norm_est_prev) < tol * norm_est:\n            break\n        \n        # Handle the case where A is the zero matrix or v is in the null space of A.\n        if norm_est == 0:\n            return 0.0\n\n        # Second matrix-vector product: x = A^T*y\n        x = A.T @ y\n        \n        # Normalize the vector for the next iteration.\n        norm_x = np.linalg.norm(x)\n\n        # If norm_x is zero, the iteration has converged to the null space.\n        if norm_x == 0:\n            break\n            \n        v = x / norm_x\n\n    return norm_est\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the norm estimation, and prints the results.\n    \"\"\"\n    \n    A1 = np.array([[3.0, 1.0], \n                   [1.0, 3.0]])\n\n    A2 = np.array([[1.0, 10.0, 0.0],\n                   [0.0, 1.0, 0.0],\n                   [0.0, 0.0, 0.1]])\n\n    A3 = np.array([[1.0, 2.0],\n                   [0.0, 1.0],\n                   [2.0, 0.0],\n                   [0.0, 0.0]])\n\n    A4 = np.array([[1.0, 0.0, 2.0, 0.0],\n                   [0.0, 1.0, 0.0, 1.0]])\n\n    A5 = np.array([[0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0]])\n\n    A6 = np.array([[1.0, 0.0],\n                   [0.0, 0.999]])\n                   \n    test_cases = [A1, A2, A3, A4, A5, A6]\n\n    results = []\n    for A in test_cases:\n        norm_estimate = estimate_induced_2_norm(A, tol=1e-10, max_iter=10000)\n        results.append(f\"{norm_estimate:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2449590"}]}