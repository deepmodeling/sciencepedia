{"hands_on_practices": [{"introduction": "Many real-world experiments, from clinical trials to quality control, can be modeled as a sequence of independent trials, each with a \"success\" or \"failure\" outcome. The binomial distribution is the fundamental tool for analyzing such scenarios by calculating the probability of a specific number of successes in a fixed number of trials. This practice problem [@problem_id:2381067] grounds this concept in a common molecular biology workflow, colony PCR, allowing you to apply the binomial formula to a tangible scientific question and practice using the complement rule, a crucial problem-solving technique in probability.", "problem": "In a colony Polymerase Chain Reaction (PCR) screening experiment to verify a recombinant plasmid, historical data from this protocol at the same laboratory indicates that exactly $1$ in $10$ colonies contains the correct insert. You randomly select $24$ colonies from a single transformation plate and assay them independently using the same PCR conditions. Assume that each colony is independent and has the same probability $p$ of being a true positive, with $p = \\frac{1}{10}$, and that the assay has perfect sensitivity and specificity under these conditions. Under this model, what is the probability that the number of true positives among the $24$ tested colonies is at least $2$? Express your final answer as a decimal and round your answer to four significant figures.", "solution": "The problem statement will first be subjected to rigorous validation.\n\nStep 1: Extracted Givens.\n- Historical data indicates that \"exactly $1$ in $10$ colonies contains the correct insert.\"\n- The probability $p$ of a colony being a true positive is given as $p = \\frac{1}{10}$.\n- The number of randomly selected colonies is $n = 24$.\n- The colonies are assayed \"independently\".\n- The assay is assumed to have \"perfect sensitivity and specificity\".\n- The objective is to find the probability that the number of true positives among the $24$ tested colonies is \"at least $2$\".\n- The final answer must be expressed as a decimal and rounded to four significant figures.\n\nStep 2: Validation of Givens.\nThe problem is scientifically grounded. It correctly models a standard molecular biology procedure, a colony Polymerase Chain Reaction (PCR) screening, using a foundational statistical framework. The conditions described—a fixed number of independent trials, each with a constant probability of success—are the precise assumptions underlying the binomial distribution. The idealization of \"perfect sensitivity and specificity\" is a standard and acceptable simplification for creating a well-posed problem in probability theory and does not render the problem scientifically unsound in this context. The problem is well-posed, providing all necessary parameters ($n=24$, $p=\\frac{1}{10}$) to compute a unique solution. The language is objective and unambiguous. It does not violate fundamental principles, is not incomplete or contradictory, and is not ill-posed.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A complete solution will now be derived.\n\nThe experiment constitutes a sequence of Bernoulli trials. Let $X$ be the random variable representing the number of true positive colonies (successes) in a sample of $n$ colonies.\nThe parameters of this process are given as:\n- The number of trials, $n = 24$.\n- The probability of success in a single trial, $p = \\frac{1}{10} = 0.1$.\nThe random variable $X$ therefore follows a binomial distribution, denoted as $X \\sim B(n, p)$, or specifically $X \\sim B(24, 0.1)$.\n\nThe probability mass function (PMF) for a binomial distribution, which gives the probability of observing exactly $k$ successes in $n$ trials, is:\n$$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nwhere $\\binom{n}{k}$ is the binomial coefficient, $\\frac{n!}{k!(n-k)!}$.\n\nThe problem requires the calculation of the probability that the number of true positives is at least $2$. This corresponds to the event $X \\ge 2$. A direct calculation by summing the probabilities of $k=2, 3, \\dots, 24$ is inefficient. It is more logical to apply the complement rule. The complement of the event $\\{X \\ge 2\\}$ is the event $\\{X  2\\}$, which comprises the outcomes $\\{X=0\\}$ and $\\{X=1\\}$.\nThe desired probability is thus:\n$$P(X \\ge 2) = 1 - P(X  2) = 1 - [P(X=0) + P(X=1)]$$\n\nWe proceed to calculate the probabilities for $k=0$ and $k=1$.\nFor $k=0$ (no true positives):\n$$P(X=0) = \\binom{24}{0} (0.1)^0 (1-0.1)^{24-0} = 1 \\times 1 \\times (0.9)^{24} = (0.9)^{24}$$\n\nFor $k=1$ (exactly one true positive):\n$$P(X=1) = \\binom{24}{1} (0.1)^1 (1-0.1)^{24-1} = 24 \\times (0.1)^1 \\times (0.9)^{23} = 2.4 \\times (0.9)^{23}$$\n\nSubstituting these expressions back into the equation for $P(X \\ge 2)$:\n$$P(X \\ge 2) = 1 - [(0.9)^{24} + 2.4 \\times (0.9)^{23}]$$\nTo facilitate calculation, we can factor out $(0.9)^{23}$:\n$$P(X \\ge 2) = 1 - [(0.9) \\times (0.9)^{23} + 2.4 \\times (0.9)^{23}]$$\n$$P(X \\ge 2) = 1 - [(0.9 + 2.4) \\times (0.9)^{23}]$$\n$$P(X \\ge 2) = 1 - 3.3 \\times (0.9)^{23}$$\n\nNow, we perform the numerical evaluation as stipulated by the problem.\nFirst, we compute the necessary power of $0.9$:\n$$(0.9)^{23} \\approx 0.0886293302$$\nNext, we calculate the probability of the complement event:\n$$P(X  2) = P(X=0) + P(X=1)$$\n$$P(X=0) = (0.9)^{24} \\approx 0.0797663972$$\n$$P(X=1) = 2.4 \\times (0.9)^{23} \\approx 2.4 \\times 0.0886293302 \\approx 0.2127103925$$\n$$P(X  2) \\approx 0.0797663972 + 0.2127103925 \\approx 0.2924767897$$\nFinally, we compute the desired probability:\n$$P(X \\ge 2) = 1 - P(X  2) \\approx 1 - 0.2924767897 = 0.7075232103$$\n\nThe problem demands the answer be rounded to four significant figures. The value is $0.7075232103$. The first four significant figures are $7, 0, 7, 5$. The fifth significant figure is $2$, which is less than $5$, so we do not round up.\nThe final result, rounded to four significant figures, is $0.7075$.", "answer": "$$\\boxed{0.7075}$$", "id": "2381067"}, {"introduction": "While the binomial distribution counts successes in a fixed number of trials, its close relative, the geometric distribution, answers a different question: how many trials are needed to achieve the first success? This conceptual shift is powerful for modeling waiting times. This hypothetical exercise [@problem_id:2381108] uses the geometric distribution to explore the statistical implications of \"p-hacking\"—repeatedly running experiments until a desired result is found. By calculating the expected number of trials to find a false positive, you will gain a deeper appreciation for the principles of statistical hygiene and the importance of correcting for multiple comparisons.", "problem": "A computational biology group is repeatedly analyzing independent, newly simulated null datasets in a differential expression workflow to find at least one gene that appears significantly differentially expressed at a threshold of $p0.05$. Each run of the workflow applies a correctly calibrated hypothesis test with type I error rate $\\alpha=0.05$, and the outcomes of different runs are independent. The group stops as soon as a run yields a result that crosses the significance threshold.\n\nUnder the assumption that the null hypothesis is true for every run, what is the expected number of independent workflow runs they will perform before stopping? Provide your answer as an exact number without rounding.", "solution": "The problem requires the calculation of an expected value for a process of repeated, independent trials. This is a standard problem in probability theory. First, the validity of the problem statement must be established.\n\nThe problem describes a sequence of independent workflow runs. Each run constitutes a trial. A trial is considered a \"success\" if a statistically significant result ($p  0.05$) is found. The problem states that for every run, the null hypothesis is true. This is a critical piece of information.\n\nLet us define the events and random variables.\nLet $S_i$ be the event that the $i$-th run yields a statistically significant result.\nThe probability of this event, given that the null hypothesis is true, is the probability of a Type I error. This is defined by the significance level, $\\alpha$.\nThe problem provides $\\alpha = 0.05$.\nTherefore, the probability of a \"success\" in any given trial is $p = P(S_i) = \\alpha = 0.05$.\n\nThe trials (workflow runs) are stated to be independent. The group stops as soon as the first success occurs.\nLet $X$ be the random variable representing the number of runs performed until the process stops.\nThis scenario precisely describes a sequence of Bernoulli trials, and the random variable $X$, which counts the number of trials up to and including the first success, follows a geometric distribution.\n\nThe probability mass function (PMF) for a geometric distribution is given by:\n$$P(X=k) = (1-p)^{k-1} p$$\nwhere $k \\in \\{1, 2, 3, \\ldots\\}$ is the number of trials, and $p$ is the probability of success on a single trial. In our case, $p = 0.05$.\n\nThe problem asks for the expected number of independent workflow runs, which is the expected value of the random variable $X$, denoted $E[X]$.\nFor a random variable $X$ following a geometric distribution with parameter $p$, the expected value is a well-known result:\n$$E[X] = \\sum_{k=1}^{\\infty} k \\cdot P(X=k) = \\sum_{k=1}^{\\infty} k (1-p)^{k-1} p$$\nThe value of this infinite series is:\n$$E[X] = \\frac{1}{p}$$\n\nWe are given the parameter $p$ as the Type I error rate, $\\alpha$.\n$$p = \\alpha = 0.05$$\nSubstituting this value into the formula for the expected value:\n$$E[X] = \\frac{1}{0.05}$$\n\nTo compute the final numerical answer, we can express the decimal as a fraction:\n$$0.05 = \\frac{5}{100} = \\frac{1}{20}$$\nTherefore, the expected number of runs is:\n$$E[X] = \\frac{1}{\\frac{1}{20}} = 20$$\n\nThis result means that, on average, the research group will have to perform $20$ analyses on null datasets before they incorrectly find a \"significant\" result due to random chance alone. This illustrates a fundamental problem in multiple testing without correction: if one looks enough times, a statistically significant result will eventually be found, even in the complete absence of any true effect. The problem is well-posed and its solution is a direct application of elementary probability theory.", "answer": "$$\\boxed{20}$$", "id": "2381108"}, {"introduction": "The principles of probability are not just theoretical; they provide deep insights into the mechanics of deep learning models. This problem [@problem_id:3106903] connects the continuous normal distribution and the concept of expectation directly to the behavior of neural networks. By modeling neuron pre-activations as draws from a Gaussian distribution, you will calculate the expected number of \"gradient-active\" neurons for different activation functions, uncovering the mathematical reasons behind phenomena like the \"dying ReLU\" problem and appreciating why alternative designs like the leaky ReLU were developed.", "problem": "A fully connected layer in a neural network has $n$ neurons. Before activation, the preactivation of each neuron is modeled as an independent draw $X_{i} \\sim \\mathcal{N}(0,1)$ for $i \\in \\{1,\\dots,n\\}$, due to Gaussian initialization and zero-mean inputs. Consider two activation functions: the Rectified Linear Unit (ReLU) defined by $\\varphi(x) = \\max(0,x)$, and the leaky Rectified Linear Unit with slope parameter $\\alpha \\in (0,1)$ defined by $\\varphi_{\\alpha}(x) = \\max(\\alpha x, x)$. Define a neuron to be gradient-active if its local derivative with respect to its preactivation at the realized input is nonzero. For ReLU, take the conventional derivative for $x \\neq 0$; for leaky ReLU, use the piecewise derivative for $x \\neq 0$; in both cases, ignore the event $x=0$ because it has probability zero under a continuous distribution.\n\nUsing only fundamental properties of the standard normal distribution, indicator random variables, and linearity of expectation, compute the expected number of gradient-active neurons among the $n$ neurons under ReLU and under leaky ReLU with slope parameter $\\alpha$. Express your final answer as a single row vector with the first entry equal to the expected number under ReLU and the second entry equal to the expected number under leaky ReLU, both as functions of $n$ and $\\alpha$. No rounding is required.", "solution": "The problem asks for the expected number of gradient-active neurons in a layer of size $n$. The preactivation of each neuron, $X_i$ for $i \\in \\{1, \\dots, n\\}$, is modeled as an independent and identically distributed random variable from a standard normal distribution, $X_i \\sim \\mathcal{N}(0,1)$.\n\nLet $Y$ be the random variable representing the total number of gradient-active neurons in the layer. We can express $Y$ as the sum of indicator random variables, $I_i$, for each neuron:\n$$Y = \\sum_{i=1}^{n} I_i$$\nwhere $I_i = 1$ if the $i$-th neuron is gradient-active, and $I_i = 0$ otherwise.\n\nBy the linearity of expectation, the expected number of gradient-active neurons is:\n$$E[Y] = E\\left[\\sum_{i=1}^{n} I_i\\right] = \\sum_{i=1}^{n} E[I_i]$$\nSince each $X_i$ is drawn from the same distribution, the probability that any given neuron is gradient-active is the same for all $i$. Let $p = P(I_i = 1)$ be this probability. The expectation of an indicator variable is the probability of the event it indicates, so $E[I_i] = p$. Therefore, the total expected number of gradient-active neurons is:\n$$E[Y] = \\sum_{i=1}^{n} p = n p$$\nOur task reduces to finding the probability $p$ for a single neuron to be gradient-active under the two specified activation functions. Let us consider a single preactivation $X \\sim \\mathcal{N}(0,1)$.\n\nCase 1: Rectified Linear Unit (ReLU)\nThe ReLU activation function is defined as $\\varphi(x) = \\max(0,x)$. The local derivative with respect to its preactivation $x$ is:\n$$\\varphi'(x) = \\frac{d}{dx}\\max(0,x) = \\begin{cases} 1  \\text{if } x  0 \\\\ 0  \\text{if } x  0 \\end{cases}$$\nThe problem states to ignore the point $x=0$, which is justified as $P(X=0)=0$ for a continuous random variable $X$. A neuron is defined as gradient-active if its local derivative is nonzero. For ReLU, this condition is $\\varphi'(X) \\neq 0$. Based on the derivative, this is equivalent to the condition $X  0$.\n\nThe probability $p_{\\text{ReLU}}$ that a neuron is gradient-active is therefore $P(X  0)$. Since $X$ is drawn from a standard normal distribution $\\mathcal{N}(0,1)$, which is symmetric about its mean of $0$, the probability of $X$ being positive is exactly $\\frac{1}{2}$.\n$$p_{\\text{ReLU}} = P(X  0) = \\frac{1}{2}$$\nThe expected number of gradient-active neurons for a layer with ReLU activations, denoted $E[Y_{\\text{ReLU}}]$, is:\n$$E[Y_{\\text{ReLU}}] = n \\cdot p_{\\text{ReLU}} = n \\cdot \\frac{1}{2} = \\frac{n}{2}$$\n\nCase 2: Leaky Rectified Linear Unit (Leaky ReLU)\nThe leaky ReLU activation function is defined as $\\varphi_{\\alpha}(x) = \\max(\\alpha x, x)$, with the slope parameter $\\alpha$ given to be in the interval $\\alpha \\in (0,1)$. The function can be written piecewise as:\n$$\\varphi_{\\alpha}(x) = \\begin{cases} x  \\text{if } x \\ge 0 \\\\ \\alpha x  \\text{if } x  0 \\end{cases}$$\nThe local derivative with respect to its preactivation $x$ is:\n$$\\varphi'_{\\alpha}(x) = \\frac{d}{dx}\\varphi_{\\alpha}(x) = \\begin{cases} 1  \\text{if } x  0 \\\\ \\alpha  \\text{if } x  0 \\end{cases}$$\nAgain, we ignore the point of non-differentiability at $x=0$. A neuron is gradient-active if this derivative is nonzero, i.e., $\\varphi'_{\\alpha}(X) \\neq 0$.\n\nWe analyze the possible values of the derivative. If $X  0$, the derivative is $1$. If $X  0$, the derivative is $\\alpha$. The problem specifies that $\\alpha \\in (0,1)$, which means $\\alpha$ is strictly positive and thus nonzero. Therefore, the derivative $\\varphi'_{\\alpha}(X)$ is equal to $1$ or $\\alpha$, neither of which is zero, for any $X \\neq 0$.\nThe condition for being gradient-active, $\\varphi'_{\\alpha}(X) \\neq 0$, is always satisfied as long as $X \\neq 0$.\n\nThe probability $p_{\\text{LReLU}}$ that a neuron is gradient-active is the probability that its derivative is non-zero:\n$$p_{\\text{LReLU}} = P(\\varphi'_{\\alpha}(X) \\neq 0) = P(X \\neq 0)$$\nFor any continuous random variable, the probability of it taking on any single specific value is $0$. Thus, for $X \\sim \\mathcal{N}(0,1)$, we have $P(X=0)=0$.\nThis implies:\n$$p_{\\text{LReLU}} = P(X \\neq 0) = 1 - P(X=0) = 1 - 0 = 1$$\nEvery neuron with a non-zero preactivation is gradient-active. Since the probability of a zero preactivation is zero, a neuron is gradient-active with probability $1$.\nThe expected number of gradient-active neurons for a layer with leaky ReLU activations, denoted $E[Y_{\\text{LReLU}}]$, is:\n$$E[Y_{\\text{LReLU}}] = n \\cdot p_{\\text{LReLU}} = n \\cdot 1 = n$$\n\nThe final answer is a row vector containing the expected number of gradient-active neurons for ReLU and leaky ReLU, respectively.\nFor ReLU, the expected number is $\\frac{n}{2}$.\nFor leaky ReLU, the expected number is $n$.\nThe resulting vector is $\\begin{pmatrix} \\frac{n}{2}  n \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{n}{2}  n \\end{pmatrix}}$$", "id": "3106903"}]}