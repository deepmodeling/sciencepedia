{"hands_on_practices": [{"introduction": "To truly understand Singular Value Decomposition, we must begin with its fundamental components: the singular values. This first exercise provides a direct, hands-on computation, grounding the abstract theory in a concrete calculation. By working through the steps to find the singular values of a simple $2 \\times 2$ matrix, you will practice the core algebraic procedure that connects a matrix $A$ to the eigenvalues of $A^T A$, revealing the matrix's intrinsic scaling properties [@problem_id:1071366].", "problem": "Compute the singular values of the $2 \\times 2$ matrix \n\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n$$\n\nExpress the singular values in simplified radical form and list them in decreasing order.", "solution": "1. The singular values $\\sigma_i$ of $A$ are the square roots of the eigenvalues of $A^T A$.\n\n2. Compute \n$$A^T A = \\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\n=\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}.$$\n\n3. The characteristic polynomial of $A^T A$ is\n$$\\det\\bigl(\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}-\\lambda I\\bigr)\n=(2-\\lambda)(1-\\lambda)-1\n=\\lambda^2-3\\lambda+1.$$\n\n4. Solve $\\lambda^2-3\\lambda+1=0$:\n$$\\lambda=\\frac{3\\pm\\sqrt{9-4}}{2}=\\frac{3\\pm\\sqrt5}{2}.$$\n\n5. Thus the singular values are\n$$\\sigma_1=\\sqrt{\\frac{3+\\sqrt5}{2}},\\quad\n\\sigma_2=\\sqrt{\\frac{3-\\sqrt5}{2}},$$\nlisted in decreasing order.", "answer": "$$\\boxed{\\sqrt{\\frac{3+\\sqrt{5}}{2}},\\ \\sqrt{\\frac{3-\\sqrt{5}}{2}}}$$", "id": "1071366"}, {"introduction": "With the basic computation mastered, we can now explore the deeper meaning of singular values. This exercise uses a powerful geometric analogy—the transformation of a unit circle into an ellipse—to investigate how SVD behaves under matrix inversion [@problem_id:1388918]. Understanding this relationship is key to appreciating singular values as the fundamental 'stretching factors' of a linear transformation and seeing how they elegantly describe the properties of its inverse.", "problem": "Consider a linear transformation in a two-dimensional Cartesian plane represented by an invertible $2 \\times 2$ matrix $A$. According to the geometric interpretation of the Singular Value Decomposition (SVD), this transformation maps the unit circle (the set of all vectors $\\mathbf{x}$ with $\\|\\mathbf{x}\\|_2 = 1$) into an ellipse. The lengths of the semi-major and semi-minor axes of this ellipse correspond to the singular values of the matrix $A$. Let these singular values be denoted by $\\sigma_1$ and $\\sigma_2$, satisfying the condition $\\sigma_1 \\ge \\sigma_2 > 0$.\n\nNow, consider the inverse linear transformation, which maps the ellipse back to the unit circle. This inverse transformation is represented by the matrix $A^{-1}$. Determine the singular values of the matrix $A^{-1}$ in terms of $\\sigma_1$ and $\\sigma_2$. Present your answer as a pair of expressions, ordered from largest to smallest.", "solution": "Let the Singular Value Decomposition (SVD) of the invertible $2 \\times 2$ matrix $A$ be given by $A = U\\Sigma V^T$. In this decomposition:\n- $U$ and $V$ are $2 \\times 2$ orthogonal matrices, which means $U^T U = UU^T = I$ and $V^T V = VV^T = I$, where $I$ is the identity matrix. From this property, it follows that $U^{-1} = U^T$ and $V^{-1} = V^T$.\n- $\\Sigma$ is a $2 \\times 2$ diagonal matrix containing the singular values of $A$ on its diagonal. Since the singular values are given as $\\sigma_1$ and $\\sigma_2$ in non-increasing order, we have:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix}\n$$\nThe problem states that $A$ is invertible. This is consistent with the condition $\\sigma_1 \\ge \\sigma_2 > 0$, which ensures that no singular value is zero, and thus $\\det(A) \\ne 0$.\n\nWe need to find the singular values of the inverse matrix, $A^{-1}$. First, we find an expression for $A^{-1}$ using the SVD of $A$:\n$$\nA^{-1} = (U\\Sigma V^T)^{-1}\n$$\nUsing the property of matrix inverses $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$, we get:\n$$\nA^{-1} = (V^T)^{-1} \\Sigma^{-1} U^{-1}\n$$\nNow, we use the properties of orthogonal matrices, where $(V^T)^{-1} = (V^{-1})^{-1} = V$ and $U^{-1} = U^T$:\n$$\nA^{-1} = V \\Sigma^{-1} U^T\n$$\nThis expression is in the form of an SVD for the matrix $A^{-1}$. Let's verify this. The expression is a product of three matrices, $V$, $\\Sigma^{-1}$, and $U^T$.\n- The first matrix, $V$, is an orthogonal matrix.\n- The third matrix, $U^T$, is an orthogonal matrix (the transpose of an orthogonal matrix is also orthogonal).\n- The middle matrix, $\\Sigma^{-1}$, must be a diagonal matrix with non-negative entries to be the singular value matrix for $A^{-1}$.\n\nLet's compute $\\Sigma^{-1}$. Since $\\Sigma$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal entries.\n$$\n\\Sigma^{-1} = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma_1} & 0 \\\\ 0 & \\frac{1}{\\sigma_2} \\end{pmatrix}\n$$\nThe diagonal entries of $\\Sigma^{-1}$ are $\\frac{1}{\\sigma_1}$ and $\\frac{1}{\\sigma_2}$. Since $\\sigma_1 > 0$ and $\\sigma_2 > 0$, these entries are positive. Thus, the expression $A^{-1} = V \\Sigma^{-1} U^T$ is indeed a valid SVD of $A^{-1}$.\n\nThe singular values of a matrix are the diagonal entries of the middle matrix in its SVD, arranged in non-increasing order. The diagonal entries of $\\Sigma^{-1}$ are $\\frac{1}{\\sigma_1}$ and $\\frac{1}{\\sigma_2}$.\nWe are given the condition $\\sigma_1 \\ge \\sigma_2 > 0$. Taking the reciprocal of this inequality series reverses the direction of the inequalities:\n$$\n\\frac{1}{\\sigma_1} \\le \\frac{1}{\\sigma_2}\n$$\nTherefore, the singular values of $A^{-1}$, when ordered from largest to smallest (non-increasingly), are $\\frac{1}{\\sigma_2}$ and $\\frac{1}{\\sigma_1}$. The problem asks for the pair of expressions ordered from largest to smallest.\n\nThe largest singular value of $A^{-1}$ is $\\frac{1}{\\sigma_2}$.\nThe smallest singular value of $A^{-1}$ is $\\frac{1}{\\sigma_1}$.", "answer": "$$ \\boxed{\\frac{1}{\\sigma_2}, \\frac{1}{\\sigma_1}} $$", "id": "1388918"}, {"introduction": "The true power of SVD in machine learning lies in its ability to analyze and simplify complex, high-dimensional data. This final practice session is a coding challenge that applies SVD to solve a classic computer vision problem: facial recognition using the 'eigenface' algorithm [@problem_id:3275135]. By implementing this method, you will see firsthand how SVD is used for dimensionality reduction, extracting the most significant features from a dataset of images to enable efficient and effective classification.", "problem": "Consider a collection of vectorized grayscale facial images arranged as columns of a data matrix $X \\in \\mathbb{R}^{m \\times n}$, where each image has $m$ pixels and there are $n$ images in the training set. Let $\\mu \\in \\mathbb{R}^{m}$ denote the empirical mean of the training images, defined by $\\mu = \\frac{1}{n} \\sum_{j=1}^{n} x_j$, where $x_j$ is the $j$-th training image column. Define the mean-centered training matrix $X_c = X - \\mu \\mathbf{1}^T$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. Perform the Singular Value Decomposition (SVD) $X_c = U \\Sigma V^T$ with $U \\in \\mathbb{R}^{m \\times r}$, $\\Sigma \\in \\mathbb{R}^{r \\times r}$, and $V \\in \\mathbb{R}^{n \\times r}$ where $r = \\operatorname{rank}(X_c)$ and the columns of $U$ form an orthonormal basis for the principal subspace of the centered data. The top $k$ columns of $U$ define an \"eigenface\" basis $U_k \\in \\mathbb{R}^{m \\times k}$.\n\nGiven a test image $x \\in \\mathbb{R}^{m}$, define its projection coordinates onto the eigenface basis by $z = U_k^T (x - \\mu) \\in \\mathbb{R}^{k}$. For classification, use nearest-neighbor in the projected space: for each test image, compute distances to all projected training images $z_j = U_k^T (x_j - \\mu)$, and assign the identity of the nearest projected training image. If multiple distances are exactly equal, break ties by choosing the smallest index in order of appearance of the training images.\n\nYour task is to implement a complete program that:\n- Constructs synthetic facial-image datasets with $m$ pixels per image and a specified number of identities. For each identity $i$, define a base face vector $b_i \\in \\mathbb{R}^{m}$ such that the dataset is scientifically plausible and linearly separable under low noise. Generate training and test images by adding independent Gaussian noise with specified standard deviation $\\sigma$ to the identity base vectors.\n- Computes the mean $\\mu$, performs mean centering, and computes the singular value decomposition $X_c = U \\Sigma V^T$.\n- Forms the eigenface basis $U_k$ from the first $k$ left singular vectors.\n- Projects both training and test images into the eigenface subspace and performs nearest-neighbor classification in that subspace.\n- Reports, for each test case, the integer number of correctly recognized test images across all identities.\n\nConstruct base vectors for non-degenerate datasets as follows: choose $m = 60$ and $C = 3$ identities. Partition the $m$ coordinates into $C$ contiguous blocks of equal length $m/C = 20$. For identity $i \\in \\{0,1,2\\}$, set $b_i$ to have ones on its block and zeros elsewhere. For degenerate datasets, set all identities to share the same base vector $b$ equal to the first identity’s base $b_0$.\n\nAll angles, if any were to appear, must be treated in radians, but no angles are needed in this problem. No physical units appear. The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The only outputs for each test case must be integers.\n\nUse the following test suite of parameter values, where $m = 60$, $C = 3$, the number of training images per identity is $n_{\\text{train}} = 5$, and the number of test images per identity is $n_{\\text{test}} = 3$. The Gaussian noise standard deviation is $\\sigma$, and the eigenface dimension is $k$. A single fixed pseudo-random seed must be used across all cases to ensure deterministic behavior.\n\n- Case $1$ (happy path): $k = 9$, $\\sigma = 0.1$, non-degenerate base vectors.\n- Case $2$ (low-dimensional subspace stress test): $k = 1$, $\\sigma = 0.25$, non-degenerate base vectors.\n- Case $3$ (boundary condition): $k = 0$, $\\sigma = 0.0$, non-degenerate base vectors.\n- Case $4$ (degenerate dataset): $k = 5$, $\\sigma = 0.0$, degenerate base vectors as defined above.\n\nFor each case, aggregate all test images across identities and count the total number of correct nearest-neighbor classifications in the eigenface subspace. Your program should produce a single line of output containing the four integers corresponding to Cases $1$ through $4$ in order, formatted exactly as:\n$[r_1,r_2,r_3,r_4]$,\nwhere $r_i$ is the number of correctly recognized test images in Case $i$.", "solution": "The user-provided problem statement has been meticulously validated and found to be scientifically grounded, well-posed, and objective. It describes a complete and self-contained task in numerical linear algebra and pattern recognition, specifically the application of Singular Value Decomposition (SVD) to facial recognition, a method commonly known as \"eigenfaces.\" All parameters, definitions, and procedures are specified with sufficient clarity to permit a unique and verifiable solution.\n\nThe solution proceeds by implementing the described algorithm step-by-step for each of the four specified test cases. A fixed pseudo-random seed is utilized to ensure the deterministic generation of synthetic data, making the results reproducible.\n\n### Step 1: Synthetic Data Generation\n\nFor each test case, we first generate a training set and a test set of synthetic images. The problem specifies $m=60$ pixels per image, $C=3$ distinct identities, $n_{\\text{train}}=5$ training images per identity, and $n_{\\text{test}}=3$ test images per identity. This results in a total of $n = C \\times n_{\\text{train}} = 3 \\times 5 = 15$ training images and $C \\times n_{\\text{test}} = 3 \\times 3 = 9$ test images.\n\nThe base vectors for the identities, $b_i \\in \\mathbb{R}^{m}$ for $i \\in \\{0, 1, 2\\}$, are constructed as follows:\n- **Non-degenerate case**: The $m=60$ pixel coordinates are partitioned into $C=3$ contiguous, non-overlapping blocks of size $m/C = 20$. The base vector $b_i$ is defined to have a value of $1$ for all coordinates within the $i$-th block and $0$ elsewhere. These three vectors, $b_0, b_1, b_2$, are mutually orthogonal.\n- **Degenerate case**: All identities share the same base vector, which is set to be $b_0$ from the non-degenerate case. That is, $b_0 = b_1 = b_2$.\n\nTraining and test images are generated by taking the appropriate base vector $b_i$ for a given identity and adding a vector of independent random values sampled from a Gaussian distribution with mean $0$ and standard deviation $\\sigma$. A fixed seed for the pseudo-random number generator ensures that the same noise is generated for each run.\n\nThe training images are organized as columns of a data matrix $X \\in \\mathbb{R}^{60 \\times 15}$, and their corresponding identity labels (e.g., $0, 1, 2$) are stored. Similarly, the $9$ test images and their labels are stored separately.\n\n### Step 2: Mean Centering and SVD\n\nThe core of the eigenface method lies in performing Principal Component Analysis (PCA) on the training data. This is achieved by computing the SVD of the mean-centered data matrix.\n\nFirst, the empirical mean of the training images, $\\mu \\in \\mathbb{R}^{60}$, is calculated:\n$$\n\\mu = \\frac{1}{n} \\sum_{j=1}^{n} x_j\n$$\nwhere $x_j$ are the columns of the training data matrix $X$.\n\nNext, each training image is centered by subtracting the mean, forming the centered data matrix $X_c \\in \\mathbb{R}^{60 \\times 15}$:\n$$\nX_c = X - \\mu \\mathbf{1}^T\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{15}$ is a column vector of ones.\n\nThe SVD of $X_c$ is then computed. Since $m > n$, we use an \"economy\" SVD for efficiency:\n$$\nX_c = U \\Sigma V^T\n$$\nHere, $U \\in \\mathbb{R}^{60 \\times 15}$ is the matrix of left singular vectors (the eigenfaces), $\\Sigma \\in \\mathbb{R}^{15 \\times 15}$ is a diagonal matrix of singular values, and $V \\in \\mathbb{R}^{15 \\times 15}$ is the matrix of right singular vectors. The columns of $U$ form an orthonormal basis for the subspace spanned by the centered training images.\n\n### Step 3: Projection onto the Eigenface Subspace\n\nThe next step is to reduce the dimensionality of the data by projecting it onto a lower-dimensional subspace spanned by the most significant eigenfaces. The problem specifies using the top $k$ eigenfaces, which correspond to the first $k$ columns of the matrix $U$. This forms the projection basis $U_k \\in \\mathbb{R}^{60 \\times k}$.\n\nEach centered training image $x_j - \\mu$ is projected onto this subspace to obtain its low-dimensional representation $z_j \\in \\mathbb{R}^{k}$:\n$$\nz_j = U_k^T (x_j - \\mu)\n$$\nSimilarly, for each test image $x_{\\text{test}}$, its centered version is projected to get its coordinate vector $z_{\\text{test}} \\in \\mathbb{R}^{k}$:\n$$\nz_{\\text{test}} = U_k^T (x_{\\text{test}} - \\mu)\n$$\n\n### Step 4: Nearest-Neighbor Classification\n\nClassification is performed in the low-dimensional eigenface subspace. For each projected test image $z_{\\text{test}}$, we find the projected training image $z_j$ that is closest to it. The distance is measured using the Euclidean norm:\n$$\nd_j = \\| z_{\\text{test}} - z_j \\|_2\n$$\nThe index $j^*$ of the nearest neighbor is found by minimizing this distance:\n$$\nj^* = \\arg\\min_{j \\in \\{1, \\dots, n\\}} d_j\n$$\nThe problem specifies a tie-breaking rule: if multiple training images are equidistant, the one with the smallest index $j$ is chosen. The predicted identity for the test image is the identity of the training image $x_{j^*}$.\n\nThis predicted identity is then compared to the true identity of the test image. The total count of correctly classified test images is accumulated for each test case.\n\n### Analysis of Special Cases\n\n- **Case 3 ($k=0$):** Here, the projection basis $U_0$ is an empty matrix of size $60 \\times 0$. Consequently, any projection $z = U_0^T (\\dots)$ results in a $0$-dimensional vector. All projected training and test images become the zero vector in $\\mathbb{R}^0$. The distance between any projected test image and any projected training image is $\\|0 - 0\\|_2 = 0$. According to the tie-breaking rule, every test image is assigned the identity of the first training image (index $j=0$), which is identity $0$. Since there are $3$ test images belonging to identity $0$, the number of correct classifications is $3$.\n\n- **Case 4 ($k=5$, $\\sigma=0$, degenerate):** In this case, all base vectors are identical, and there is no noise ($\\sigma=0$). Thus, all $15$ training images are identical, $x_j = b$ for all $j$. The mean image is $\\mu = b$. The centered data matrix $X_c$ becomes the zero matrix, as $x_j - \\mu = b - b = 0$. The SVD of the zero matrix yields zero singular values. All projected training vectors $z_j = U_5^T (0)$ are the zero vector in $\\mathbb{R}^5$. All test images are also equal to $b$, so their projections $z_{\\text{test}} = U_5^T(b-b)$ are also the zero vector. As in the $k=0$ case, all distances are $0$, and the tie-breaker rule assigns identity $0$ to all test images. The number of correct classifications is again $3$.\n\nThe final output is a list containing the integer counts of correct classifications for each of the four cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the eigenface recognition pipeline for a series of test cases\n    and computes the number of correctly classified images for each.\n    \"\"\"\n    \n    # Set a single fixed pseudo-random seed for deterministic data generation.\n    np.random.seed(0)\n\n    # Define problem parameters\n    m = 60  # Pixels per image\n    C = 3   # Number of identities\n    n_train_per_id = 5\n    n_test_per_id = 3\n    n_train_total = C * n_train_per_id\n    n_test_total = C * n_test_per_id\n\n    # Test suite parameters\n    test_cases = [\n        # (k, sigma, is_degenerate)\n        {'k': 9, 'sigma': 0.1, 'degenerate': False, 'label': 'Case 1'},\n        {'k': 1, 'sigma': 0.25, 'degenerate': False, 'label': 'Case 2'},\n        {'k': 0, 'sigma': 0.0, 'degenerate': False, 'label': 'Case 3'},\n        {'k': 5, 'sigma': 0.0, 'degenerate': True, 'label': 'Case 4'},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        k = case['k']\n        sigma = case['sigma']\n        is_degenerate = case['degenerate']\n\n        # Step 1: Generate synthetic data\n        \n        # Construct base vectors\n        base_vectors = np.zeros((m, C))\n        block_size = m // C\n        if not is_degenerate:\n            for i in range(C):\n                base_vectors[i * block_size : (i + 1) * block_size, i] = 1.0\n        else:\n            base_vectors[0 * block_size : 1 * block_size, 0] = 1.0\n            for i in range(1, C):\n                base_vectors[:, i] = base_vectors[:, 0]\n\n        # Generate training data\n        X_train = np.zeros((m, n_train_total))\n        y_train = np.zeros(n_train_total, dtype=int)\n        img_idx = 0\n        for i in range(C):\n            for _ in range(n_train_per_id):\n                noise = np.random.normal(0, sigma, m)\n                X_train[:, img_idx] = base_vectors[:, i] + noise\n                y_train[img_idx] = i\n                img_idx += 1\n\n        # Generate test data\n        X_test = np.zeros((m, n_test_total))\n        y_test = np.zeros(n_test_total, dtype=int)\n        img_idx = 0\n        for i in range(C):\n            for _ in range(n_test_per_id):\n                noise = np.random.normal(0, sigma, m)\n                X_test[:, img_idx] = base_vectors[:, i] + noise\n                y_test[img_idx] = i\n                img_idx += 1\n        \n        # Step 2: Mean Centering and SVD\n        mu = np.mean(X_train, axis=1, keepdims=True)\n        X_c = X_train - mu\n        \n        U, s, Vt = np.linalg.svd(X_c, full_matrices=False)\n        \n        # Step 3: Projection onto the Eigenface Subspace\n        U_k = U[:, :k]\n        \n        # Project training images\n        Z_train = U_k.T @ X_c\n        \n        # Step 4: Nearest-Neighbor Classification\n        num_correct = 0\n        for i in range(n_test_total):\n            x_test_vec = X_test[:, i]\n            \n            # Center and project the test image\n            x_test_c = x_test_vec.reshape(-1, 1) - mu\n            z_test = U_k.T @ x_test_c\n\n            # Compute distances to all projected training images\n            # The reshape of z_test ensures correct broadcasting\n            distances = np.linalg.norm(Z_train - z_test, axis=0)\n            \n            # Find the index of the nearest neighbor. np.argmin handles ties\n            # by returning the first occurrence, as required.\n            best_match_idx = np.argmin(distances)\n            \n            # Get the predicted label and compare with the true label\n            predicted_label = y_train[best_match_idx]\n            true_label = y_test[i]\n            \n            if predicted_label == true_label:\n                num_correct += 1\n        \n        results.append(num_correct)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3275135"}]}