{"hands_on_practices": [{"introduction": "A core motivation behind the DenseNet architecture is its remarkable computational efficiency. This exercise provides a hands-on opportunity to quantify this efficiency by deriving the Floating Point Operations (FLOPs) required for a dense block from first principles. By comparing this to a ResNet block of similar capacity, you will gain a deeper appreciation for how DenseNet's feature reuse mechanism translates into significant computational savings, a crucial skill for designing models for resource-constrained environments. [@problem_id:3114885]", "problem": "You are given a Densely Connected Convolutional Network (DenseNet) dense block that operates at spatial resolution $H \\times W$. The block has $L$ layers and a growth rate $k \\in \\mathbb{N}$, $k \\ge 1$. The input to the block has $C_0$ channels. Each layer $\\ell \\in \\{1,\\dots,L\\}$ receives as input the concatenation of all previous outputs in the block, so it has $C_0 + (\\ell - 1) k$ input channels. Each layer uses a bottleneck design consisting of a $1 \\times 1$ convolution that outputs $4k$ channels, followed by a $3 \\times 3$ convolution that outputs $k$ channels. The $k$ output channels from each layer are concatenated to the block’s features for subsequent layers. Assume there is no change in spatial size within the block, and ignore the computational cost of Batch Normalization (BN), Rectified Linear Unit (ReLU), concatenation, biases, and any non-convolutional operations.\n\nFor comparison, consider a Residual Network (ResNet) bottleneck block that processes the same spatial resolution $H \\times W$ and takes as input $C_0$ channels. It consists of a $1 \\times 1$ convolution that reduces channels to $4k$, a $3 \\times 3$ convolution that maps $4k$ to $4k$, and a $1 \\times 1$ convolution that expands channels to $C_0 + Lk$ so that the final channel count matches the DenseNet block’s output width. Assume no change in spatial size and ignore the costs of BN, ReLU, skip connection addition, biases, and any non-convolutional operations.\n\nUse the following foundational definition for convolutional cost: for a convolution producing an output of spatial size $H \\times W$ with $C_{\\text{in}}$ input channels, $C_{\\text{out}}$ output channels, and a $K \\times K$ kernel, the number of multiply-accumulate operations is $H W \\, C_{\\text{out}} \\, C_{\\text{in}} \\, K^2$. Counting one multiply and one add as two floating point operations (FLOPs), the FLOPs is $2 H W \\, C_{\\text{out}} \\, C_{\\text{in}} \\, K^2$.\n\nStarting only from this definition and the architectural descriptions above, derive closed-form expressions for the total Floating Point Operations (FLOPs) of:\n- the DenseNet dense block, and\n- the ResNet bottleneck block with matched output width $C_0 + Lk$ and bottleneck width $4k$.\n\nThen, form the ratio $R$ of the DenseNet block FLOPs to the ResNet block FLOPs. Provide your final answer as a single simplified analytic expression for $R$ in terms of $H$, $W$, $C_0$, $k$, and $L$. No numerical evaluation is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **DenseNet Block:**\n  - Spatial resolution: $H \\times W$\n  - Number of layers: $L$\n  - Growth rate: $k \\in \\mathbb{N}$, $k \\ge 1$\n  - Block input channels: $C_0$\n  - Layer $\\ell$ input channels: $C_0 + (\\ell - 1)k$ for $\\ell \\in \\{1,\\dots,L\\}$\n  - Layer $\\ell$ architecture: A $1 \\times 1$ convolution producing $4k$ channels, followed by a $3 \\times 3$ convolution producing $k$ channels.\n- **ResNet Block:**\n  - Spatial resolution: $H \\times W$\n  - Block input channels: $C_0$\n  - Architecture: A $1 \\times 1$ convolution reducing to $4k$ channels, a $3 \\times 3$ convolution from $4k$ to $4k$ channels, and a $1 \\times 1$ convolution expanding to $C_0 + Lk$ channels.\n- **FLOPs Definition:**\n  - For a convolution with output size $H \\times W$, $C_{\\text{in}}$ input channels, $C_{\\text{out}}$ output channels, and a $K \\times K$ kernel, the Floating Point Operations (FLOPs) count is $2 H W C_{\\text{out}} C_{\\text{in}} K^2$.\n- **Assumptions:** No change in spatial size within the blocks. Ignore costs of Batch Normalization, ReLU, concatenation, biases, and other non-convolutional operations.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing standard architectures (DenseNet, ResNet) and computational cost metrics (FLOPs) used in the field of deep learning. It is well-posed, providing all necessary parameters and definitions to derive a unique analytical solution. The language is objective and precise. The problem is self-contained and free of contradictions. The setup does not violate any fundamental principles.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full, reasoned solution will be provided.\n\n### Derivation of DenseNet Block FLOPs\n\nLet $\\text{FLOPs}_{\\text{DN}}$ be the total FLOPs for the DenseNet block. This is the sum of FLOPs from each of the $L$ layers. For a single layer $\\ell \\in \\{1, \\dots, L\\}$, the input has $C_{\\text{in}, \\ell} = C_0 + (\\ell-1)k$ channels. The layer consists of two convolutional operations.\n\n1.  **$1 \\times 1$ Bottleneck Convolution:**\n    -   Input channels: $C_{\\text{in}, \\ell} = C_0 + (\\ell - 1)k$\n    -   Output channels: $C_{\\text{out},1} = 4k$\n    -   Kernel size: $K_1 = 1$\n    -   FLOPs for this operation ($\\text{FLOPs}_{\\text{DN}, \\ell, 1}$):\n        $$ \\text{FLOPs}_{\\text{DN}, \\ell, 1} = 2 H W C_{\\text{out},1} C_{\\text{in}, \\ell} K_1^2 = 2 H W (4k) (C_0 + (\\ell-1)k) (1^2) = 8HWk(C_0 + (\\ell-1)k) $$\n\n2.  **$3 \\times 3$ Convolution:**\n    -   Input channels (output of the previous layer): $C_{\\text{in},2} = 4k$\n    -   Output channels: $C_{\\text{out},2} = k$\n    -   Kernel size: $K_2 = 3$\n    -   FLOPs for this operation ($\\text{FLOPs}_{\\text{DN}, \\ell, 2}$):\n        $$ \\text{FLOPs}_{\\text{DN}, \\ell, 2} = 2 H W C_{\\text{out},2} C_{\\text{in},2} K_2^2 = 2 H W (k) (4k) (3^2) = 72HWk^2 $$\n\nThe total FLOPs for layer $\\ell$ is the sum of these two costs:\n$$ \\text{FLOPs}_{\\text{DN}, \\ell} = \\text{FLOPs}_{\\text{DN}, \\ell, 1} + \\text{FLOPs}_{\\text{DN}, \\ell, 2} = 8HWk(C_0 + (\\ell-1)k) + 72HWk^2 $$\n\nTo find the total FLOPs for the block, we sum over all $L$ layers:\n$$ \\text{FLOPs}_{\\text{DN}} = \\sum_{\\ell=1}^{L} \\text{FLOPs}_{\\text{DN}, \\ell} = \\sum_{\\ell=1}^{L} \\left( 8HWk(C_0 + (\\ell-1)k) + 72HWk^2 \\right) $$\nWe can separate the terms in the summation:\n$$ \\text{FLOPs}_{\\text{DN}} = \\sum_{\\ell=1}^{L} 8HWkC_0 + \\sum_{\\ell=1}^{L} 8HWk^2(\\ell-1) + \\sum_{\\ell=1}^{L} 72HWk^2 $$\nEvaluating each sum:\n-   $\\sum_{\\ell=1}^{L} 8HWkC_0 = L \\cdot 8HWkC_0 = 8LHWkC_0$\n-   $\\sum_{\\ell=1}^{L} 8HWk^2(\\ell-1) = 8HWk^2 \\sum_{\\ell=1}^{L} (\\ell-1) = 8HWk^2 \\sum_{j=0}^{L-1} j$. Using the formula for the sum of an arithmetic series, $\\sum_{j=0}^{n} j = \\frac{n(n+1)}{2}$, with $n=L-1$, we get $8HWk^2 \\frac{(L-1)L}{2} = 4L(L-1)HWk^2$.\n-   $\\sum_{\\ell=1}^{L} 72HWk^2 = L \\cdot 72HWk^2 = 72LHWk^2$\n\nSumming these results:\n$$ \\text{FLOPs}_{\\text{DN}} = 8LHWkC_0 + 4L(L-1)HWk^2 + 72LHWk^2 $$\nFactoring out common terms:\n$$ \\text{FLOPs}_{\\text{DN}} = HWL \\left[ 8kC_0 + 4(L-1)k^2 + 72k^2 \\right] $$\n$$ \\text{FLOPs}_{\\text{DN}} = HWL \\left[ 8kC_0 + (4L-4)k^2 + 72k^2 \\right] $$\n$$ \\text{FLOPs}_{\\text{DN}} = HWL \\left[ 8kC_0 + (4L+68)k^2 \\right] $$\n$$ \\text{FLOPs}_{\\text{DN}} = 4HWLk \\left[ 2C_0 + (L+17)k \\right] $$\n\n### Derivation of ResNet Block FLOPs\n\nLet $\\text{FLOPs}_{\\text{RN}}$ be the total FLOPs for the ResNet block. The block has $C_0$ input channels and consists of three convolutional operations. The final output has $C_0 + Lk$ channels to match the DenseNet block's output feature map depth.\n\n1.  **$1 \\times 1$ Reduction Convolution:**\n    -   Input channels: $C_{\\text{in},1} = C_0$\n    -   Output channels: $C_{\\text{out},1} = 4k$\n    -   Kernel size: $K_1 = 1$\n    -   FLOPs: $\\text{FLOPs}_{\\text{RN}, 1} = 2 H W (4k) (C_0) (1^2) = 8HWkC_0$\n\n2.  **$3 \\times 3$ Convolution:**\n    -   Input channels: $C_{\\text{in},2} = 4k$\n    -   Output channels: $C_{\\text{out},2} = 4k$\n    -   Kernel size: $K_2 = 3$\n    -   FLOPs: $\\text{FLOPs}_{\\text{RN}, 2} = 2 H W (4k) (4k) (3^2) = 2 H W (16k^2)(9) = 288HWk^2$\n\n3.  **$1 \\times 1$ Expansion Convolution:**\n    -   Input channels: $C_{\\text{in},3} = 4k$\n    -   Output channels: $C_{\\text{out},3} = C_0 + Lk$\n    -   Kernel size: $K_3 = 1$\n    -   FLOPs: $\\text{FLOPs}_{\\text{RN}, 3} = 2 H W (C_0 + Lk) (4k) (1^2) = 8HWk(C_0 + Lk)$\n\nThe total FLOPs for the ResNet block is the sum of these three costs:\n$$ \\text{FLOPs}_{\\text{RN}} = \\text{FLOPs}_{\\text{RN}, 1} + \\text{FLOPs}_{\\text{RN}, 2} + \\text{FLOPs}_{\\text{RN}, 3} $$\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWkC_0 + 288HWk^2 + 8HWk(C_0 + Lk) $$\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWkC_0 + 288HWk^2 + 8HWkC_0 + 8HWLk^2 $$\n$$ \\text{FLOPs}_{\\text{RN}} = 16HWkC_0 + 288HWk^2 + 8HWLk^2 $$\nFactoring out common terms:\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWk \\left[ 2C_0 + 36k + Lk \\right] $$\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWk \\left[ 2C_0 + (L+36)k \\right] $$\n\n### Ratio of DenseNet to ResNet FLOPs\n\nThe ratio $R$ is defined as $R = \\frac{\\text{FLOPs}_{\\text{DN}}}{\\text{FLOPs}_{\\text{RN}}}$. Substituting the derived expressions:\n$$ R = \\frac{4HWLk \\left[ 2C_0 + (L+17)k \\right]}{8HWk \\left[ 2C_0 + (L+36)k \\right]} $$\nThe common factor $4HWk$ in the numerator and denominator can be canceled:\n$$ R = \\frac{L \\left[ 2C_0 + (L+17)k \\right]}{2 \\left[ 2C_0 + (L+36)k \\right]} $$\nThis expression cannot be simplified further. This is the final analytic expression for the ratio $R$.", "answer": "$$\n\\boxed{\\frac{L(2C_0 + (L+17)k)}{2(2C_0 + (L+36)k)}}\n$$", "id": "3114885"}, {"introduction": "Beyond computational cost, a network's effectiveness is often measured by its parameter efficiency—the accuracy gained per parameter. This practice moves from FLOPs to analyzing the number of learnable parameters, using a hypothetical but instructive model to connect network width to expected accuracy. You will explore the concept of diminishing returns, where increasing the growth rate $k$ yields progressively smaller gains due to feature redundancy, a vital consideration for balancing model size and performance. [@problem_id:3114924]", "problem": "Consider a single dense block from a Densely Connected Convolutional Network (DenseNet), where the depth is fixed to $L$ layers and the growth rate is $k$ channels per layer. Let the input to the block have $k_0$ channels. The network uses two-dimensional convolutions. You will analyze how the total parameter count scales with $k$ and, under a capacity saturation model, how the expected accuracy behaves as $k$ increases, exhibiting diminishing returns due to redundancy and limited spatial resolution.\n\nFundamental base and definitions to be used:\n- A convolution with kernel size $s \\times s$ mapping $c_{\\text{in}}$ input channels to $c_{\\text{out}}$ output channels has $s^2 \\, c_{\\text{in}} \\, c_{\\text{out}}$ learnable weights. Bias terms may be neglected for this analysis.\n- In a DenseNet block with growth rate $k$, the $l$-th layer, indexed $l \\in \\{1,\\dots,L\\}$, receives as input the concatenation of all previous outputs and the initial features, so the number of input channels to layer $l$ is $c_{\\text{in}}(l) = k_0 + k\\,(l-1)$.\n- Each layer in the block consists of a single $3 \\times 3$ convolution that produces $k$ output channels.\n\nModeling assumptions to connect parameters to expected accuracy:\n- Let $P(k)$ denote the total number of convolutional parameters in the dense block as a function of $k$.\n- Due to feature redundancy that increases with the growth rate, define the effective parameter count as $P_{\\text{eff}}(k) = \\dfrac{P(k)}{1 + r\\,(k-1)}$, where $r$ is a nonnegative redundancy coefficient.\n- Due to limited spatial resolution and finite task complexity, model the expected accuracy as a saturating response to effective capacity:\n$$\nA\\big(P_{\\text{eff}}(k)\\big) \\;=\\; A_{\\min} \\;+\\; \\big(A_{\\max} - A_{\\min}\\big)\\,\\Big(1 - e^{-\\,P_{\\text{eff}}(k)\\,/\\,P_{\\text{sat}}}\\Big),\n$$\nwhere $A_{\\min}$ and $A_{\\max}$ are the lower and upper accuracy bounds (expressed as decimals), and $P_{\\text{sat}}$ is a positive scale parameter controlling the e-fold saturation point.\n\nTasks:\n1) Starting only from the fundamental definitions above, derive a closed-form expression for the total parameter count $P(k)$ of the dense block as a function of $L$, $k_0$, and $k$.\n2) Using the redundancy-adjusted effective parameter count $P_{\\text{eff}}(k)$ and the saturation model for $A\\big(\\cdot\\big)$, compute the expected accuracy $A(k)$ for each specified $k$.\n3) Define the parameter-to-accuracy marginal efficiency between two growth rates $k_1$ and $k_2$ with $k_2 &gt; k_1$ as\n$$\n\\mathcal{E}\\big(k_1 \\rightarrow k_2\\big) \\;=\\; \\dfrac{A(k_2) - A(k_1)}{P(k_2) - P(k_1)}.\n$$\nCompute this marginal efficiency across consecutive $k$ in a provided ordered test suite.\n\nUse the following fixed constants and test suite:\n- Depth $L = 12$.\n- Initial channels $k_0 = 24$.\n- Redundancy coefficient $r = 0.02$.\n- Saturation scale $P_{\\text{sat}} = 200000$.\n- Accuracy bounds $A_{\\min} = 0.4$ and $A_{\\max} = 0.9$.\n- Ordered test suite for growth rates $k \\in \\{4, 8, 16, 32, 64\\}$.\n\nComputation requirements:\n- For the main analysis set $k \\in \\{8, 16, 32\\}$, compute the accuracies $A(8)$, $A(16)$, and $A(32)$, each rounded to six decimal places as decimals in $[0,1]$.\n- For coverage of boundary and edge behavior, also compute the marginal efficiencies for the consecutive pairs in the ordered suite $\\{4 \\rightarrow 8,\\, 8 \\rightarrow 16,\\, 16 \\rightarrow 32,\\, 32 \\rightarrow 64\\}$, each rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with two sublists:\n  - The first sublist is $[A(8), A(16), A(32)]$.\n  - The second sublist is $[\\mathcal{E}(4 \\rightarrow 8), \\mathcal{E}(8 \\rightarrow 16), \\mathcal{E}(16 \\rightarrow 32), \\mathcal{E}(32 \\rightarrow 64)]$.\n- The final printed line must be exactly of the form\n$[[a_{8},a_{16},a_{32}],[e_{4\\rightarrow 8},e_{8\\rightarrow 16},e_{16\\rightarrow 32},e_{32\\rightarrow 64}]]$\nwith each $a_{\\cdot}$ and $e_{\\cdot}$ a real number rounded to six decimal places, expressed as decimals (no percent signs).", "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed, self-contained, and scientifically grounded problem within the domain of deep learning, specifically concerning the architectural analysis of DenseNets. All definitions, constants, and objectives are clearly articulated, and no contradictions, ambiguities, or factual unsoundness were detected. The modeling of parameter redundancy and accuracy saturation, while based on simplifying assumptions, represents a valid and standard approach for theoretical analysis of neural network behavior.\n\nThe solution proceeds in two parts as required. First, we derive the closed-form expression for the total parameter count. Second, we outline the computational procedure to determine the expected accuracies and marginal efficiencies based on the provided models.\n\n**Part 1: Derivation of the Total Parameter Count $P(k)$**\n\nThe total number of parameters, $P(k)$, in the dense block is the sum of the parameters from each of its $L$ layers. We begin by defining the parameter count for a single layer, $l$.\n\nAccording to the problem statement, the number of learnable weights in a single convolutional layer is given by $s^2 \\, c_{\\text{in}} \\, c_{\\text{out}}$, where $s$ is the kernel size, $c_{\\text{in}}$ is the number of input channels, and $c_{\\text{out}}$ is the number of output channels. Bias terms are neglected.\n\nFor the $l$-th layer in our dense block, where $l \\in \\{1, 2, \\dots, L\\}$:\n- The kernel size is specified as $3 \\times 3$, so $s = 3$.\n- The number of output channels produced by each layer is the growth rate, so $c_{\\text{out}} = k$.\n- The number of input channels, $c_{\\text{in}}(l)$, is the concatenation of the initial block input (with $k_0$ channels) and the outputs of all preceding $l-1$ layers, each of which contributes $k$ channels. Therefore, $c_{\\text{in}}(l) = k_0 + k(l-1)$.\n\nThe number of parameters in layer $l$, denoted $P_l(k)$, is:\n$$\nP_l(k) = s^2 \\cdot c_{\\text{in}}(l) \\cdot c_{\\text{out}} = 3^2 \\cdot \\big(k_0 + k(l-1)\\big) \\cdot k = 9k\\big(k_0 + k(l-1)\\big)\n$$\n\nThe total parameter count for the entire block, $P(k)$, is the sum of the parameters of all $L$ layers:\n$$\nP(k) = \\sum_{l=1}^{L} P_l(k) = \\sum_{l=1}^{L} 9k\\big(k_0 + k(l-1)\\big)\n$$\n\nWe can factor out constants from the summation and separate the terms:\n$$\nP(k) = 9k \\left( \\sum_{l=1}^{L} k_0 + \\sum_{l=1}^{L} k(l-1) \\right) = 9k \\left( k_0 \\sum_{l=1}^{L} 1 + k \\sum_{l=1}^{L} (l-1) \\right)\n$$\n\nThe two sums are evaluated as follows:\n- The first sum is $\\sum_{l=1}^{L} 1 = L$.\n- The second sum is the sum of an arithmetic series: $\\sum_{l=1}^{L} (l-1) = 0 + 1 + 2 + \\dots + (L-1)$. The sum of the first $n$ non-negative integers is given by $\\frac{n(n+1)}{2}$. For $n = L-1$, this becomes $\\frac{(L-1)L}{2}$.\n\nSubstituting these results back into the expression for $P(k)$:\n$$\nP(k) = 9k \\left( k_0 L + k \\frac{L(L-1)}{2} \\right)\n$$\n\nExpanding this gives the final closed-form expression for the total parameter count as a function of $L$, $k_0$, and $k$:\n$$\nP(k) = 9Lk_0k + \\frac{9}{2}L(L-1)k^2\n$$\n\n**Part 2: Computation of Accuracy and Marginal Efficiency**\n\nUsing the derived formula for $P(k)$ and the provided models, we can compute the required metrics. The procedure for a given growth rate $k$ involves the following steps:\n1.  Calculate the total parameter count $P(k)$ using the constants $L=12$ and $k_0=24$:\n    $$ P(k) = 9(12)(24)k + \\frac{9}{2}(12)(11)k^2 = 2592k + 594k^2 $$\n2.  Calculate the effective parameter count $P_{\\text{eff}}(k)$ using the redundancy coefficient $r=0.02$:\n    $$ P_{\\text{eff}}(k) = \\frac{P(k)}{1 + r(k-1)} = \\frac{2592k + 594k^2}{1 + 0.02(k-1)} $$\n3.  Calculate the expected accuracy $A(k)$ using the saturation model with $A_{\\min}=0.4$, $A_{\\max}=0.9$, and $P_{\\text{sat}}=200000$:\n    $$ A(k) = 0.4 + (0.9 - 0.4) \\left( 1 - e^{-P_{\\text{eff}}(k) / P_{\\text{sat}}} \\right) = 0.4 + 0.5 \\left( 1 - e^{-P_{\\text{eff}}(k) / 200000} \\right) $$\n\nThese steps are first performed for all values of $k$ in the ordered suite $\\{4, 8, 16, 32, 64\\}$ to obtain the corresponding $P(k)$ and $A(k)$ values.\n\nThen, the required outputs are assembled:\n- The accuracies $A(8)$, $A(16)$, and $A(32)$ are extracted and rounded to six decimal places.\n- The parameter-to-accuracy marginal efficiency, $\\mathcal{E}(k_1 \\rightarrow k_2)$, is computed for consecutive pairs in the suite using the formula:\n    $$ \\mathcal{E}(k_1 \\rightarrow k_2) = \\frac{A(k_2) - A(k_1)}{P(k_2) - P(k_1)} $$\n    The efficiency is calculated for the pairs $4 \\rightarrow 8$, $8 \\rightarrow 16$, $16 \\rightarrow 32$, and $32 \\rightarrow 64$, with each result rounded to six decimal places.\n\nThe following Python code implements this complete computational procedure and formats the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DenseNet analysis problem by deriving parameter counts,\n    accuracies, and marginal efficiencies.\n    \"\"\"\n    \n    # Define fixed constants from the problem statement\n    L = 12\n    k0 = 24\n    r = 0.02\n    P_sat = 200000.0\n    A_min = 0.4\n    A_max = 0.9\n\n    # Ordered test suite for a growth rate k\n    k_suite = [4, 8, 16, 32, 64]\n\n    def calculate_metrics(k):\n        \"\"\"\n        Calculates P(k), P_eff(k), and A(k) for a given growth rate k.\n        \n        Args:\n            k (int): The growth rate (number of channels per layer).\n            \n        Returns:\n            tuple: A tuple containing (P(k), A(k)).\n        \"\"\"\n        # Calculate total parameters P(k) using the derived closed-form expression:\n        # P(k) = 9*L*k0*k + 4.5*L*(L-1)*k^2\n        P_k = 9 * L * k0 * k + 4.5 * L * (L - 1) * k**2\n        \n        # Calculate effective parameters P_eff(k)\n        # For k=1, denominator is 1. Prevent division by zero if k=1 is ever used.\n        if k == 1:\n            P_eff_k = P_k\n        else:\n            P_eff_k = P_k / (1 + r * (k - 1))\n        \n        # Calculate expected accuracy A(k) using the saturation model\n        exponent = -P_eff_k / P_sat\n        A_k = A_min + (A_max - A_min) * (1 - np.exp(exponent))\n        \n        return P_k, A_k\n\n    # Store results for P(k) and A(k) for each k in the test suite\n    results = {}\n    for k_val in k_suite:\n        p_val, a_val = calculate_metrics(k_val)\n        results[k_val] = {'p': p_val, 'a': a_val}\n\n    # Task 1: Compute the accuracies A(8), A(16), A(32)\n    accuracies_to_compute = [8, 16, 32]\n    accuracy_results = [round(results[k]['a'], 6) for k in accuracies_to_compute]\n    \n    # Task 2: Compute the marginal efficiencies for consecutive pairs\n    efficiency_results = []\n    for i in range(len(k_suite) - 1):\n        k1 = k_suite[i]\n        k2 = k_suite[i+1]\n        \n        A1 = results[k1]['a']\n        A2 = results[k2]['a']\n        P1 = results[k1]['p']\n        P2 = results[k2]['p']\n        \n        delta_A = A2 - A1\n        delta_P = P2 - P1\n        \n        # Marginal efficiency is not well-defined if parameters do not increase\n        if delta_P == 0:\n            efficiency = 0.0\n        else:\n            efficiency = delta_A / delta_P\n            \n        efficiency_results.append(round(efficiency, 6))\n    \n    # Format the lists for the final output string\n    acc_str = ','.join(map(str, accuracy_results))\n    \n    # The prompt requires numbers rounded to six decimal places. Using an f-string\n    # ensures that numbers like 0.000000 are displayed with the correct precision.\n    eff_str = ','.join([f\"{val:.6f}\" for val in efficiency_results])\n    \n    print(f\"[[{acc_str}],[{eff_str}]]\")\n\nsolve()\n\n```", "id": "3114924"}, {"introduction": "High-level architectural designs must be complemented by careful implementation to ensure stable training. This coding-based practice investigates a subtle but critical detail of the DenseNet architecture: the placement of normalization layers. By simulating and analyzing how Batch Normalization interacts with the concatenation of statistically diverse feature maps, you will uncover why certain design choices are superior and how they contribute to a more uniform internal data representation, a key factor in successful deep learning. [@problem_id:3114019]", "problem": "Consider a dense block that concatenates feature maps along the channel dimension, a characteristic pattern of Dense Convolutional Network (DenseNet) style architectures. Let a set of channels coming from earlier layers be denoted by $A$ with $c_A$ channels, and a set of newly produced channels be denoted by $B$ with $c_B$ channels. For each channel $j$, model its pre-normalization activation as a random variable $x_j$ with mean $\\mu_j$ and variance $\\sigma_j^2$. Assume channels are independent and that statistics are stationary over the batch. We investigate two strategies for placing Batch Normalization (BN) relative to concatenation:\n- Strategy $S_1$ (BN-before-concatenation): apply BN to the new set $B$ only, then concatenate $A$ and the normalized $B$.\n- Strategy $S_2$ (BN-after-concatenation): first concatenate $A$ and $B$, then apply a single BN to all concatenated channels.\n\nUse the following fundamental base definitions:\n- Batch Normalization (BN) on a single channel $x$ with parameters $\\gamma$ and $\\beta$ and small numerical stabilizer $\\varepsilon>0$ computes\n$$\n\\mathrm{BN}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta,\n$$\nwhere $\\mu$ and $\\sigma^2$ are the batch-estimated mean and variance for that channel. In this problem, assume $\\gamma=1$ and $\\beta=0$ so that the output is centered and scaled purely by the batch statistics and $\\varepsilon$.\n- Concatenation along channels is a purely structural operation that does not alter per-channel statistics on its own; it only aggregates channels from $A$ and $B$ into a single tensor.\n\nTasks:\n1. Derive, from the BN definition and the independence assumption, the post-normalization per-channel mean and variance for each channel under $S_1$ and $S_2$ in terms of the given pre-normalization $\\mu_j$, $\\sigma_j^2$, and $\\varepsilon$. Your derivation must use only the above fundamental base and logic about how concatenation and per-channel BN interact.\n2. Define the following two scalar metrics that summarize cross-channel heterogeneity after the normalization step:\n   - The maximum absolute mean across channels\n   $$\n   M_{\\max} \\;=\\; \\max_{j} \\left| \\mathbb{E}[y_j] \\right|,\n   $$\n   where $y_j$ is the post-normalization activation for channel $j$.\n   - The variance uniformity ratio across channels\n   $$\n   R_{\\mathrm{var}} \\;=\\; \\frac{\\max_{j} \\mathrm{Var}(y_j)}{\\min_{j} \\mathrm{Var}(y_j)}.\n   $$\n3. Implement a program that, for each test case below, computes and outputs the tuple of four floats\n$$\n\\left[ M_{\\max}^{(S_1)}, \\; R_{\\mathrm{var}}^{(S_1)}, \\; M_{\\max}^{(S_2)}, \\; R_{\\mathrm{var}}^{(S_2)} \\right],\n$$\nwhere $M_{\\max}^{(S_k)}$ and $R_{\\mathrm{var}}^{(S_k)}$ denote the metrics under strategy $S_k$.\n\nImportant implementation details:\n- Treat the BN statistics as exactly equal to the provided $\\mu_j$ and $\\sigma_j^2$ for each channel (no estimation error), and use the provided $\\varepsilon$ for all channels within a given test case.\n- Under $S_1$, apply BN only to channels in $B$; channels in $A$ remain unchanged. Under $S_2$, apply BN to both $A$ and $B$ after concatenation.\n- Your program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four floats as specified above. For example, if there are $T$ test cases and the $t$-th result is the list $r_t$, the final output format must be\n$$\n[ r_1, r_2, \\dots, r_T ].\n$$\n\nTest suite:\nUse exactly the following four test cases, each defined by $(\\mu_A, \\sigma_A^2, \\mu_B, \\sigma_B^2, \\varepsilon)$, where $\\mu_A$ and $\\sigma_A^2$ are lists of length $c_A$ and $\\mu_B$ and $\\sigma_B^2$ are lists of length $c_B$. Every number provided must be used as given.\n\n- Test case $1$ (happy path, previously normalized $A$ and unnormalized $B$):\n  - $\\mu_A = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 1.0, \\; 1.0, \\; 1.0 \\,]$\n  - $\\mu_B = [\\, 3.0, \\; -2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 4.0, \\; 0.5 \\,]$\n  - $\\varepsilon = 10^{-5}$\n\n- Test case $2$ (heterogeneous $A$ and $B$):\n  - $\\mu_A = [\\, 1.5, \\; -0.5 \\,]$\n  - $\\sigma_A^2 = [\\, 2.0, \\; 0.2 \\,]$\n  - $\\mu_B = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 1.0, \\; 5.0, \\; 0.01 \\,]$\n  - $\\varepsilon = 10^{-3}$\n\n- Test case $3$ (edge case with very small variances):\n  - $\\mu_A = [\\, 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 10^{-6} \\,]$\n  - $\\mu_B = [\\, 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 10^{-6} \\,]$\n  - $\\varepsilon = 10^{-6}$\n\n- Test case $4$ (large variance mismatch):\n  - $\\mu_A = [\\, 5.0, \\; -3.0, \\; 1.0 \\,]$\n  - $\\sigma_A^2 = [\\, 100.0, \\; 0.1, \\; 10.0 \\,]$\n  - $\\mu_B = [\\, 2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 50.0 \\,]$\n  - $\\varepsilon = 10^{-4}$\n\nYour program must compute the four metrics for each test case and print a single line in the exact format\n$$\n[ [M_{\\max}^{(S_1)}, R_{\\mathrm{var}}^{(S_1)}, M_{\\max}^{(S_2)}, R_{\\mathrm{var}}^{(S_2)}], \\; \\dots ]\n$$\nwith one bracketed list per test case, in the same order as provided above.", "solution": "The problem is deemed valid. It is scientifically grounded in the principles of deep learning and statistics, is well-posed with a unique solution for each test case, and is expressed objectively. All data and definitions required for the solution are provided and are consistent. We may proceed with the solution.\n\n### 1. Derivation of Post-Normalization Statistics\n\nThe analysis relies on the fundamental properties of expectation and variance. For a random variable $X$ and constants $a$ and $b$:\n- Linearity of Expectation: $\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b$\n- Property of Variance: $\\mathrm{Var}(aX + b) = a^2 \\mathrm{Var}(X)$\n\nThe problem provides the Batch Normalization (BN) transformation for a single channel's activation $x$ as:\n$$\n\\mathrm{BN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta\n$$\nGiven the constraints $\\gamma=1$ and $\\beta=0$, this simplifies to:\n$$\ny = \\mathrm{BN}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n$$\nwhere $y$ is the post-normalization activation, and $\\mu = \\mathbb{E}[x]$ and $\\sigma^2 = \\mathrm{Var}(x)$ are the pre-normalization mean and variance of the channel.\n\nWe analyze the two strategies, $S_1$ and $S_2$, based on this definition.\n\n#### Strategy $S_1$: BN-before-concatenation\n\nIn this strategy, only the new channels in set $B$ are normalized. The channels from set $A$ are concatenated without modification.\n\n- **For channels $j$ in set $A$**:\nThe activations are not changed. Let $x_j$ be the pre-normalization activation and $y_j$ be the post-normalization activation.\n$$\ny_j = x_j\n$$\nTherefore, the post-normalization statistics are identical to the pre-normalization statistics:\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}[x_j] = \\mu_j\n$$\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}(x_j) = \\sigma_j^2\n$$\n\n- **For channels $j$ in set $B$**:\nThe activations are transformed by BN. Let $x_j$ be the pre-normalization activation for a channel $j \\in B$ with mean $\\mu_j$ and variance $\\sigma_j^2$. The post-normalization activation is:\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\nWe derive the mean and variance of $y_j$:\nThe mean is:\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}\\left[\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j]\n$$\nBy linearity of expectation, $\\mathbb{E}[x_j - \\mu_j] = \\mathbb{E}[x_j] - \\mu_j = \\mu_j - \\mu_j = 0$.\n$$\n\\mathbb{E}[y_j] = 0\n$$\nThe variance is:\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}\\left(\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j - \\mu_j)\n$$\nSince $\\mathrm{Var}(X-c) = \\mathrm{Var}(X)$ for any constant $c$, we have $\\mathrm{Var}(x_j - \\mu_j) = \\mathrm{Var}(x_j) = \\sigma_j^2$.\n$$\n\\mathrm{Var}(y_j) = \\frac{1}{\\sigma_j^2 + \\varepsilon} \\cdot \\sigma_j^2 = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\n\n#### Strategy $S_2$: BN-after-concatenation\n\nIn this strategy, channels from sets $A$ and $B$ are first concatenated. Then, a single BN layer is applied to all channels. The problem specifies that BN is a per-channel operation, meaning each channel $j$ in the concatenated set $A \\cup B$ is normalized using its own statistics $\\mu_j$ and $\\sigma_j^2$.\n\n- **For any channel $j$ in the concatenated set $A \\cup B$**:\nThe post-normalization activation $y_j$ is given by:\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\nThe derivation of the mean and variance of $y_j$ is identical to the derivation for channels in set $B$ under strategy $S_1$.\nThe mean is:\n$$\n\\mathbb{E}[y_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} (\\mu_j - \\mu_j) = 0\n$$\nThe variance is:\n$$\n\\mathrm{Var}(y_j) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j) = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\nThis holds for all channels, whether they originated from set $A$ or set $B$.\n\n### 2. Formulas for Cross-Channel Metrics\n\nUsing the derived statistics, we can formulate expressions for the metrics $M_{\\max}$ and $R_{\\mathrm{var}}$.\n\n#### Metrics for Strategy $S_1$\n\nThe set of post-normalization means is $\\{\\mu_j\\}_{j \\in A} \\cup \\{0\\}_{j \\in B}$.\nThe maximum absolute mean $M_{\\max}^{(S_1)}$ is:\n$$\nM_{\\max}^{(S_1)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max(\\max_{j \\in A} |\\mu_j|, \\max_{j \\in B} |0|) = \\max_{j \\in A} |\\mu_j|\n$$\nIf set $A$ is empty, $M_{\\max}^{(S_1)} = 0$.\n\nThe set of post-normalization variances is $\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B}$.\nThe variance uniformity ratio $R_{\\mathrm{var}}^{(S_1)}$ is:\n$$\nR_{\\mathrm{var}}^{(S_1)} = \\frac{\\max(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}{\\min(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}\n$$\n\n#### Metrics for Strategy $S_2$\n\nThe post-normalization mean for every channel $j \\in A \\cup B$ is $0$.\nThe maximum absolute mean $M_{\\max}^{(S_2)}$ is:\n$$\nM_{\\max}^{(S_2)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max_{j \\in A \\cup B} |0| = 0\n$$\n\nThe set of post-normalization variances is $\\{\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\}_{j \\in A \\cup B}$.\nThe variance uniformity ratio $R_{\\mathrm{var}}^{(S_2)}$ is:\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\max_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}{\\min_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}\n$$\nThe function $f(v) = \\frac{v}{v + \\varepsilon}$ is monotonically increasing for $v \\ge 0$. Therefore, the maximum and minimum of the transformed variances correspond to the maximum and minimum of the original variances $\\sigma_j^2$.\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\frac{\\max_{j \\in A \\cup B} \\sigma_j^2}{\\max_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}{\\frac{\\min_{j \\in A \\cup B} \\sigma_j^2}{\\min_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}\n$$\nThese formulas are now ready for implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the metrics for cross-channel heterogeneity\n    under two different batch normalization strategies in a dense block.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            [0.0, 0.0, 0.0],  # mu_A\n            [1.0, 1.0, 1.0],  # sigma_A^2\n            [3.0, -2.0],      # mu_B\n            [4.0, 0.5],       # sigma_B^2\n            1e-5              # epsilon\n        ),\n        # Test case 2\n        (\n            [1.5, -0.5],      # mu_A\n            [2.0, 0.2],       # sigma_A^2\n            [0.0, 0.0, 0.0],  # mu_B\n            [1.0, 5.0, 0.01], # sigma_B^2\n            1e-3              # epsilon\n        ),\n        # Test case 3\n        (\n            [0.0],            # mu_A\n            [1e-6],           # sigma_A^2\n            [0.0],            # mu_B\n            [1e-6],           # sigma_B^2\n            1e-6              # epsilon\n        ),\n        # Test case 4\n        (\n            [5.0, -3.0, 1.0], # mu_A\n            [100.0, 0.1, 10.0],# sigma_A^2\n            [2.0],            # mu_B\n            [50.0],           # sigma_B^2\n            1e-4              # epsilon\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_A, sigma_A_sq, mu_B, sigma_B_sq, epsilon = case\n\n        # --- Strategy S1: BN-before-concatenation ---\n        \n        # Means: Channels from A are unchanged, channels from B are centered to 0.\n        # M_max is the max absolute mean from the un-normalized channels in A.\n        if mu_A:\n            M_max_s1 = np.max(np.abs(np.array(mu_A)))\n        else:\n            M_max_s1 = 0.0\n            \n        # Variances: Channels from A are unchanged, channels from B are scaled.\n        post_vars_A_s1 = np.array(sigma_A_sq)\n        post_vars_B_s1 = np.array([s2 / (s2 + epsilon) for s2 in sigma_B_sq])\n        \n        all_post_vars_s1 = np.concatenate((post_vars_A_s1, post_vars_B_s1))\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s1.size > 0:\n            min_var_s1 = np.min(all_post_vars_s1)\n            max_var_s1 = np.max(all_post_vars_s1)\n            # Avoid division by zero, though problem constraints ensure min_var > 0.\n            R_var_s1 = max_var_s1 / min_var_s1 if min_var_s1 > 0 else float('inf')\n        else:\n            R_var_s1 = 1.0 # No variance heterogeneity if there are no channels.\n\n        # --- Strategy S2: BN-after-concatenation ---\n\n        # Means: All channels are normalized, so all means are 0.\n        M_max_s2 = 0.0\n        \n        # Variances: All channels are normalized.\n        all_pre_vars_s2 = np.array(sigma_A_sq + sigma_B_sq)\n        \n        all_post_vars_s2 = np.array([s2 / (s2 + epsilon) for s2 in all_pre_vars_s2])\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s2.size > 0:\n            min_var_s2 = np.min(all_post_vars_s2)\n            max_var_s2 = np.max(all_post_vars_s2)\n            R_var_s2 = max_var_s2 / min_var_s2 if min_var_s2 > 0 else float('inf')\n        else:\n            R_var_s2 = 1.0\n            \n        result = [M_max_s1, R_var_s1, M_max_s2, R_var_s2]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() for a list already includes spaces, e.g., '[1.0, 2.0]'.\n    # Joining these with a comma results in '...,[...', which when wrapped\n    # in brackets becomes '[[...],[...]]', matching the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3114019"}]}