{"hands_on_practices": [{"introduction": "The power of residual networks doesn't just come from adding connections, but from *how* and *where* they are added. This exercise provides a quantitative framework for comparing different skip connection strategies, moving beyond qualitative arguments. By calculating the diversity of gradient paths and the associated computational cost, you will formalize the intuition that a series of local skips creates an 'ensemble' of effective network depths, a property not shared by simpler architectures like a single global skip connection [@problem_id:3169728].", "problem": "You are asked to formalize and compare two placements of skip connections in feedforward networks: local residual skips at each block and a single global skip from input to output. Your goal is to derive, from first principles, how these placements affect the number of distinct gradient paths and an operations-based proxy for training time, and then implement a program that computes these quantities for a set of test architectures.\n\nStart from the following fundamental base:\n- Backpropagation uses the chain rule of calculus, and when a nodeâ€™s output is the sum of branch outputs, the Jacobian of the sum equals the sum of the branch Jacobians. Let the input be $x$, the layer functions be $F_i$, and the output be $y$.\n- For a fully connected linear layer mapping $\\mathbb{R}^{d_i} \\to \\mathbb{R}^{d_{i+1}}$ with weights of shape $d_{i+1} \\times d_i$, the forward floating-point operation count (floating-point operations (FLOPs)) for the matrix-vector product scales as $2\\,d_i\\,d_{i+1}$ (one multiply and one add per entry in the output). Under standard reverse-mode automatic differentiation for linear layers, the backward pass includes computing the gradient with respect to inputs and weights, each with cost scaling as $2\\,d_i\\,d_{i+1}$, for a per-layer total of $6\\,d_i\\,d_{i+1}$ FLOPs per iteration (one forward pass and one backward pass). Bias costs are negligible here and are ignored. A scalar addition of two vectors in $\\mathbb{R}^{k}$ costs $k$ FLOPs.\n\nArchitectures to analyze:\n1. Plain chain (no skips): $L$ linear layers with dimensions $[d_0,d_1,\\dots,d_L]$; output is $y = F_{L-1}\\circ \\dots \\circ F_0(x)$.\n2. Global skip only: same chain plus a single skip added at the output $y = F_{L-1}\\circ \\dots \\circ F_0(x) + x$, requiring $d_0 = d_L$ for the sum to be well-defined.\n3. Local residual skips only: each block outputs $x_{i+1} = x_i + F_i(x_i)$, requiring $d_i = d_{i+1}$ for each addition to be well-defined.\n\nDefinitions to compute:\n- Gradient path diversity $D$: the number of algebraically distinct additive paths by which gradients from $y$ can reach $x$, counting paths made distinct by the presence or absence of traversing each residual branch.\n- Operations-based training-time proxy $C$: the total FLOPs for one training iteration (one forward plus one backward pass), computed by summing per-layer matrix multiplication FLOPs and the FLOPs of any addition operations introduced by skip connections in the forward pass. Backward FLOPs for the addition nodes are taken as negligible in this model, while backward FLOPs for linear layers are included as above.\n\nYou must implement a program that, for each test case, returns a pair $[D,C]$ as integers. No physical units are involved, and angles are not used. The architecture is specified by a string indicating the skip type and a list of dimensions indicating the layer widths.\n\nTest suite to implement (in this order):\n- Case $1$: local residual skips, $L = 3$, dimensions $[64,64,64,64]$.\n- Case $2$: global skip only, $L = 3$, dimensions $[64,64,64,64]$.\n- Case $3$: plain chain, $L = 3$, dimensions $[64,64,64,64]$.\n- Case $4$: local residual skips, $L = 1$, dimensions $[32,32]$.\n- Case $5$: global skip only, $L = 10$, dimensions $[16,16,16,16,16,16,16,16,16,16,16]$.\n- Case $6$: local residual skips, $L = 10$, dimensions $[16,16,16,16,16,16,16,16,16,16,16]$.\n\nOutput specification:\n- Your program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets, where each result is itself a two-element list $[D,C]$. For example, the format is $[[D_1,C_1],[D_2,C_2],\\dots,[D_6,C_6]]$ with no spaces anywhere in the line.", "solution": "The problem requires the formalization and analysis of three distinct feedforward network architectures: a plain sequential chain, a chain with a single global skip connection, and a network composed of local residual blocks. For each architecture, we must derive expressions for two quantities: the gradient path diversity, denoted by $D$, and an operations-based proxy for training time, denoted by $C$. These derivations must stem from the fundamental principles provided.\n\nThe number of layers is denoted by $L$, and the dimensions of the network are given by a list $[d_0, d_1, \\dots, d_L]$, where $d_0$ is the input dimension and $d_i$ for $i > 0$ is the output dimension of the $(i-1)$-th layer. The function implemented by the $i$-th layer (for $i$ from $0$ to $L-1$) is $F_i$, which maps from $\\mathbb{R}^{d_i}$ to $\\mathbb{R}^{d_{i+1}}$.\n\nThe operations cost $C$ is the total number of floating-point operations (FLOPs) for one full training iteration (one forward pass and one backward pass). The provided cost model is as follows:\n- A linear layer $F_i: \\mathbb{R}^{d_i} \\to \\mathbb{R}^{d_{i+1}}$ has a total iteration cost of $6 d_i d_{i+1}$ FLOPs.\n- The addition of two vectors in $\\mathbb{R}^k$ costs $k$ FLOPs for the forward pass, with negligible cost for the backward pass.\n\nThe gradient path diversity $D$ is the number of distinct additive paths for the gradient to flow from the output $y$ back to the input $x$. This is derived from the chain rule and the principle that the Jacobian of a sum of functions is the sum of their Jacobians.\n\nWe will now derive the expressions for $D$ and $C$ for each architecture.\n\n**1. Plain Chain Architecture**\n\nIn this configuration, the layers are composed sequentially. The output of layer $i$, denoted $x_{i+1}$, is the result of applying function $F_i$ to the previous output $x_i$.\nThe architecture is defined by:\n$$ y = x_L = F_{L-1}(x_{L-1}) = F_{L-1} \\circ F_{L-2} \\circ \\dots \\circ F_0(x_0) $$\nwhere $x_0$ is the network input.\n\nGradient Path Diversity ($D_{plain}$):\nThe Jacobian of the output $y$ with respect to the input $x_0$ is found by applying the chain rule:\n$$ \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial F_{L-1}}{\\partial x_{L-1}} \\frac{\\partial F_{L-2}}{\\partial x_{L-2}} \\dots \\frac{\\partial F_0}{\\partial x_0} $$\nThis expression is a single product of Jacobians. There are no additive terms in the functional dependence of $y$ on $x_0$. Consequently, there is only one pathway for gradients to propagate back.\n$$ D_{plain} = 1 $$\n\nOperations-based Training-time Proxy ($C_{plain}$):\nThe total cost is the sum of the costs for each of the $L$ linear layers. The cost for layer $F_i$ is $6 d_i d_{i+1}$.\n$$ C_{plain} = \\sum_{i=0}^{L-1} 6 d_i d_{i+1} $$\n\n**2. Global Skip Only Architecture**\n\nThis architecture adds a single skip connection from the input $x_0$ to the final output $y$.\nThe output is defined as:\n$$ y = (F_{L-1} \\circ F_{L-2} \\circ \\dots \\circ F_0(x_0)) + x_0 $$\nFor the vector addition to be well-defined, the dimension of the input must match the dimension of the final layer's output, i.e., $d_0 = d_L$.\n\nGradient Path Diversity ($D_{global}$):\nThe output $y$ is a sum of two functions of $x_0$: the deep path through the layers, let's call it $H(x_0)$, and the identity path $I(x_0) = x_0$. Based on the provided principle, the Jacobian of the sum is the sum of the Jacobians:\n$$ \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial H(x_0)}{\\partial x_0} + \\frac{\\partial I(x_0)}{\\partial x_0} = \\left( \\prod_{i=0}^{L-1} \\frac{\\partial F_{L-1-i}}{\\partial x_{L-1-i}} \\right) + I_{d_0} $$\nwhere $I_{d_0}$ is the identity matrix of size $d_0 \\times d_0$. The two additive terms correspond to two distinct algebraic paths for the gradient.\n$$ D_{global} = 2 $$\n\nOperations-based Training-time Proxy ($C_{global}$):\nThe cost includes the cost of the $L$ linear layers, identical to the plain chain, plus the cost of the final vector addition. The addition is between two vectors in $\\mathbb{R}^{d_L}$. The cost of this addition is $d_L$ FLOPs.\n$$ C_{global} = \\left( \\sum_{i=0}^{L-1} 6 d_i d_{i+1} \\right) + d_L $$\n\n**3. Local Residual Skips Only Architecture**\n\nThis architecture consists of a sequence of residual blocks. In each block $i$, the input $x_i$ is passed through a layer $F_i$ and the result is added back to the input $x_i$.\nThe recurrent definition is:\n$$ x_{i+1} = x_i + F_i(x_i) \\quad \\text{for } i = 0, \\dots, L-1 $$\nThe final output is $y = x_L$. This structure requires that the input and output dimensions of each block are identical, so $d_i = d_{i+1}$ for all $i = 0, \\dots, L-1$. This implies all dimensions $d_0, \\dots, d_L$ must be equal.\n\nGradient Path Diversity ($D_{local}$):\nLet's represent each block's transformation as an operator. Since the layers $F_i$ are linear, we can write $x_{i+1} = (I + F_i)(x_i)$, where $I$ is the identity operator. Unrolling the recurrence from $y=x_L$ back to $x_0$:\n$$ y = (I + F_{L-1}) (x_{L-1}) = (I + F_{L-1}) (I + F_{L-2}) \\dots (I + F_0) (x_0) $$\nExpanding this product of $L$ binomials results in a sum of $2^L$ distinct terms. Each term is a unique composition of functions, formed by choosing either the identity $I$ or the layer function $F_i$ from each block $(I+F_i)$. For example, with $L=2$, we get $(I+F_1)(I+F_0) = F_1 \\circ F_0 + F_1 + F_0 + I$. Each of these $2^L$ terms constitutes a distinct additive path from input to output.\n$$ D_{local} = 2^L $$\n\nOperations-based Training-time Proxy ($C_{local}$):\nThe total cost is the sum of costs for each of the $L$ residual blocks. For each block $i$, the computation involves one linear layer $F_i$ and one vector addition. The cost for the layer is $6 d_i d_{i+1}$. The addition $x_i + F_i(x_i)$ involves vectors in $\\mathbb{R}^{d_{i+1}}$, costing $d_{i+1}$ FLOPs. The total cost is the sum of these block costs over all $L$ blocks.\n$$ C_{local} = \\sum_{i=0}^{L-1} (6 d_i d_{i+1} + d_{i+1}) $$\n\nThese derived formulae are implemented to compute the values for the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef calculate_costs(skip_type: str, dims: list[int]) -> list[int]:\n    \"\"\"\n    Calculates gradient path diversity (D) and operations proxy (C) for a given architecture.\n\n    Args:\n        skip_type: A string indicating the architecture ('plain', 'global', 'local').\n        dims: A list of integers representing layer dimensions [d_0, d_1, ..., d_L].\n\n    Returns:\n        A list [D, C] containing the two computed integer values.\n    \"\"\"\n    L = len(dims) - 1\n    if L == 0: # A network with 0 layers is just an identity\n        if skip_type == 'plain':\n            return [1, 0]\n        elif skip_type == 'global': # x = x+x, not well defined in problem, assume plain\n             return [1, 0]\n        elif skip_type == 'local': # x_1 = x_0 + F_0(x_0) -> L must be at least 1\n            return [0, 0] # Or error, but problem cases have L >= 1.\n\n    # Calculate base cost from linear layers, common to all architectures\n    cost_layers = 0\n    for i in range(L):\n        d_i = dims[i]\n        d_i_plus_1 = dims[i+1]\n        cost_layers += 6 * d_i * d_i_plus_1\n\n    D = 0\n    C = 0\n\n    if skip_type == 'plain':\n        # D: Single path through all layers.\n        D = 1\n        # C: Sum of costs of linear layers.\n        C = cost_layers\n    elif skip_type == 'global':\n        # D: Two paths - one through layers, one through identity skip.\n        D = 2\n        # C: Cost of layers plus one vector addition at the output.\n        d_L = dims[L]\n        C = cost_layers + d_L\n    elif skip_type == 'local':\n        # D: 2 choices at each of L blocks (identity or layer) -> 2^L paths.\n        D = 2**L\n        # C: Cost of layers plus L vector additions, one for each block.\n        cost_additions = 0\n        for i in range(L):\n            d_i_plus_1 = dims[i+1]\n            cost_additions += d_i_plus_1\n        C = cost_layers + cost_additions\n    \n    return [int(D), int(C)]\n\ndef solve():\n    \"\"\"\n    Executes the analysis for the predefined test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        ('local', [64, 64, 64, 64]),\n        ('global', [64, 64, 64, 64]),\n        ('plain', [64, 64, 64, 64]),\n        ('local', [32, 32]),\n        ('global', [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]),\n        ('local', [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]),\n    ]\n\n    results = []\n    for skip_type, dims in test_cases:\n        result = calculate_costs(skip_type, dims)\n        results.append(result)\n\n    # Format the output string to be exactly as specified: [[D1,C1],[D2,C2],...]\n    # str() on a list adds spaces, so we remove them.\n    results_str = [str(r).replace(' ', '') for r in results]\n    output_str = f\"[{','.join(results_str)}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3169728"}, {"introduction": "Having established the architectural benefit of many local skips, we now zoom in to analyze the stability of a single residual block. This practice connects deep learning to the theory of dynamical systems, viewing a deep ResNet as the discretization of a continuous-time system. By deriving the stability condition for a linearized residual block, you will uncover the mathematical reason why skip connections are so effective at preventing the exploding or vanishing gradients that plague very deep networks [@problem_id:3169711].", "problem": "Consider a residual block used in deep neural networks with skip connections, where the layer update is given by $x_{l+1} = x_{l} + h F(x_{l})$ for a step size $h>0$ and layer index $l \\in \\mathbb{N}$. Suppose the residual function is linear, $F(x) = A x$, with $A \\in \\mathbb{C}^{n \\times n}$ a normal matrix (that is, $A^{*}A = A A^{*}$), so that $A$ is unitarily diagonalizable. Denote by $\\|\\cdot\\|_{2}$ the Euclidean norm on $\\mathbb{C}^{n}$ and the associated induced operator norm on matrices. To prevent exploding norms under repeated application of the residual block, we require the single-step map $B(h) \\equiv I + h A$ to be non-expansive in the Euclidean norm, i.e., $\\|B(h)\\|_{2} \\leq 1$.\n\nStarting only from the spectral theorem for normal matrices, the definition of the induced operator norm, and the property that the eigenvalues of a polynomial in a matrix are the polynomial evaluated at the eigenvalues, derive a condition on $h$ and the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{n}$ of $A$ that guarantees $\\|I + h A\\|_{2} \\leq 1$. Express this as a stability region in the complex plane for $h \\lambda$, written as an explicit inequality involving $\\operatorname{Re}(\\lambda)$ and $|\\lambda|$.\n\nThen, consider a concrete normal matrix $A$ whose spectrum is $\\{-2,\\,-0.5 + i,\\,-0.5 - i,\\,-0.2\\}$. Determine the supremum step size $h^{\\star} > 0$ such that $\\|I + h A\\|_{2} \\leq 1$ holds for all $0 \\leq h \\leq h^{\\star}$. Express your final answer as a single real number. No rounding is required.", "solution": "The problem asks for two parts: first, to derive a stability condition for a residual block with a linear residual function, and second, to apply this condition to a specific case to find a maximum allowable step size.\n\nPart 1: Derivation of the Stability Condition\n\nWe are given the layer update rule $x_{l+1} = (I + hA)x_l$ where $A$ is a normal matrix. The condition to prevent exploding norms is that the map $B(h) = I + hA$ is non-expansive, i.e., $\\|I + hA\\|_2 \\leq 1$.\n\nWe start by determining the 2-norm of the matrix $B(h) = I + hA$.\nThe problem states that $A \\in \\mathbb{C}^{n \\times n}$ is a normal matrix, meaning $A^*A = AA^*$. The matrix $B(h)$ is also normal for any real step size $h$, because:\n$$ (I + hA)^*(I + hA) = (I^* + \\overline{h}A^*)(I + hA) = (I + hA^*)(I + hA) = I + hA + hA^* + h^2 A^*A $$\n$$ (I + hA)(I + hA)^* = (I + hA)(I^* + \\overline{h}A^*) = (I + hA)(I + hA^*) = I + hA^* + hA + h^2 AA^* $$\nSince $A^*A = AA^*$, we have $(I + hA)^*(I + hA) =\n (I + hA)(I + hA)^*$, so $I + hA$ is normal.\n\nAccording to the spectral theorem, a normal matrix is unitarily diagonalizable. A key property that follows is that the induced 2-norm of a normal matrix is equal to its spectral radius (the maximum absolute value of its eigenvalues).\nLet $\\{\\lambda_i\\}_{i=1}^n$ be the eigenvalues of $A$. The problem states that the eigenvalues of a polynomial in a matrix are the polynomial evaluated at the eigenvalues. The matrix $B(h) = I + hA$ can be seen as the evaluation of the polynomial $p(z) = 1 + hz$ at $A$. Therefore, the eigenvalues of $B(h)$ are $\\{1 + h\\lambda_i\\}_{i=1}^n$.\n\nThe 2-norm of $B(h)$ is then:\n$$ \\|I + hA\\|_2 = \\rho(I + hA) = \\max_{i \\in \\{1, \\dots, n\\}} |1 + h\\lambda_i| $$\nThe non-expansive condition $\\|I + hA\\|_2 \\leq 1$ thus requires that for every eigenvalue $\\lambda_i$ of $A$, the following inequality holds:\n$$ |1 + h\\lambda_i| \\leq 1 $$\nThis is the fundamental condition. Let's analyze it further. Letting $z_i = h\\lambda_i$, the condition is that for each $i$, the complex number $z_i$ must lie within the stability region defined by $|1+z| \\leq 1$. Geometrically, this is a closed disk in the complex plane centered at $(-1, 0)$ with a radius of $1$.\n\nTo derive the explicit inequality involving $\\operatorname{Re}(\\lambda)$ and $|\\lambda|$, we square both sides of $|1 + h\\lambda| \\leq 1$:\n$$ |1 + h\\lambda|^2 \\leq 1^2 $$\nLet $\\lambda = \\operatorname{Re}(\\lambda) + i\\operatorname{Im}(\\lambda)$. The term $h\\lambda = h\\operatorname{Re}(\\lambda) + ih\\operatorname{Im}(\\lambda)$.\nThe squared magnitude is:\n$$ (1 + h\\operatorname{Re}(\\lambda))^2 + (h\\operatorname{Im}(\\lambda))^2 \\leq 1 $$\nExpanding the left side gives:\n$$ 1 + 2h\\operatorname{Re}(\\lambda) + h^2(\\operatorname{Re}(\\lambda))^2 + h^2(\\operatorname{Im}(\\lambda))^2 \\leq 1 $$\nCombining the terms with $h^2$:\n$$ 1 + 2h\\operatorname{Re}(\\lambda) + h^2((\\operatorname{Re}(\\lambda))^2 + (\\operatorname{Im}(\\lambda))^2) \\leq 1 $$\nRecognizing that $(\\operatorname{Re}(\\lambda))^2 + (\\operatorname{Im}(\\lambda))^2 = |\\lambda|^2$, we have:\n$$ 1 + 2h\\operatorname{Re}(\\lambda) + h^2|\\lambda|^2 \\leq 1 $$\nSubtracting $1$ from both sides yields:\n$$ 2h\\operatorname{Re}(\\lambda) + h^2|\\lambda|^2 \\leq 0 $$\nSince the step size $h$ is given to be positive ($h>0$), we can divide the inequality by $h$ without changing its direction:\n$$ 2\\operatorname{Re}(\\lambda) + h|\\lambda|^2 \\leq 0 $$\nThis is the condition that must be satisfied by $h$ and every eigenvalue $\\lambda$ of the matrix $A$.\n\nPart 2: Calculation of the Supremum Step Size $h^{\\star}$\n\nWe are given a matrix $A$ with the spectrum (set of eigenvalues) $\\{-2, -0.5 + i, -0.5 - i, -0.2\\}$. We must find the supremum $h^{\\star} > 0$ such that the condition $2\\operatorname{Re}(\\lambda) + h|\\lambda|^2 \\leq 0$ is satisfied for all $h \\in [0, h^{\\star}]$ and for all eigenvalues $\\lambda$ in the spectrum.\n\nFor a non-zero eigenvalue $\\lambda$, we can rearrange the inequality to find an upper bound on $h$:\n$$ h|\\lambda|^2 \\leq -2\\operatorname{Re}(\\lambda) $$\nFor a positive solution $h > 0$ to exist, we must have $-2\\operatorname{Re}(\\lambda) > 0$, which implies $\\operatorname{Re}(\\lambda)  0$. We check this for all given eigenvalues:\n*   $\\operatorname{Re}(-2) = -2  0$\n*   $\\operatorname{Re}(-0.5+i) = -0.5  0$\n*   $\\operatorname{Re}(-0.5-i) = -0.5  0$\n*   $\\operatorname{Re}(-0.2) = -0.2  0$\nSince all eigenvalues have negative real parts, a positive step size $h$ is possible.\nIf $|\\lambda|^2 \\neq 0$, the inequality for $h$ is:\n$$ h \\leq \\frac{-2\\operatorname{Re}(\\lambda)}{|\\lambda|^2} $$\nThis inequality must hold for every eigenvalue. Therefore, $h$ must be less than or equal to the minimum of these upper bounds over all eigenvalues. The supremum $h^{\\star}$ is this minimum value.\n$$ h^{\\star} = \\min_{\\lambda \\in \\text{spectrum}} \\left( \\frac{-2\\operatorname{Re}(\\lambda)}{|\\lambda|^2} \\right) $$\nLet's compute this value for each eigenvalue:\n\n1.  For $\\lambda_1 = -2$:\n    $\\operatorname{Re}(\\lambda_1) = -2$ and $|\\lambda_1|^2 = (-2)^2 = 4$.\n    The bound on $h$ is $h \\leq \\frac{-2(-2)}{4} = \\frac{4}{4} = 1$.\n\n2.  For $\\lambda_2 = -0.5 + i$:\n    $\\operatorname{Re}(\\lambda_2) = -0.5$ and $|\\lambda_2|^2 = (-0.5)^2 + 1^2 = 0.25 + 1 = 1.25$.\n    The bound on $h$ is $h \\leq \\frac{-2(-0.5)}{1.25} = \\frac{1}{1.25} = \\frac{1}{5/4} = \\frac{4}{5} = 0.8$.\n\n3.  For $\\lambda_3 = -0.5 - i$:\n    $\\operatorname{Re}(\\lambda_3) = -0.5$ and $|\\lambda_3|^2 = (-0.5)^2 + (-1)^2 = 0.25 + 1 = 1.25$.\n    The bound on $h$ is $h \\leq \\frac{-2(-0.5)}{1.25} = \\frac{1}{1.25} = 0.8$.\n\n4.  For $\\lambda_4 = -0.2$:\n    $\\operatorname{Re}(\\lambda_4) = -0.2$ and $|\\lambda_4|^2 = (-0.2)^2 = 0.04$.\n    The bound on $h$ is $h \\leq \\frac{-2(-0.2)}{0.04} = \\frac{0.4}{0.04} = 10$.\n\nThe set of upper bounds for $h$ is $\\{1, 0.8, 0.8, 10\\}$. To satisfy the condition for all eigenvalues simultaneously, $h$ must be less than or equal to the minimum of these bounds.\n$$ h^{\\star} = \\min\\{1, 0.8, 10\\} = 0.8 $$\nThus, the supremum step size for which the system remains non-expansive for all $h \\in [0, h^{\\star}]$ is $0.8$.", "answer": "$$\\boxed{0.8}$$", "id": "3169711"}, {"introduction": "While the identity skip connection is a powerful default, its effectiveness depends on the target function the network must learn. This exercise presents a hypothetical scenario where the identity mapping is fundamentally at odds with the learning objective, revealing a limitation in the standard ResNet block. By comparing an identity skip to a learnable projection, you will gain insight into important design trade-offs and understand why many modern architectures incorporate more flexible skip-path transformations [@problem_id:3169751].", "problem": "Consider the canonical residual block used in deep learning, where an input vector $x \\in \\mathbb{R}^n$ is transformed to an output $y \\in \\mathbb{R}^n$ using a skip connection and a learnable residual branch. Two variants are considered:\n\n- Identity skip: $y = x + F_\\theta(x)$.\n- Projection skip: $y = P_\\phi x + F_\\theta(x)$, where $P_\\phi$ is a learnable linear projection.\n\nAssume $F_\\theta$ is instantiated using a two-layer network with the hyperbolic tangent nonlinearity, that is $F_\\theta(x) = W_2 \\tanh(W_1 x)$ with $W_1 \\in \\mathbb{R}^{k \\times n}$ and $W_2 \\in \\mathbb{R}^{n \\times k}$, where $k \\in \\mathbb{N}$. Due to the bounded range of the hyperbolic tangent, and standard weight regularization practices, suppose that the residual output is component-wise bounded: for all $x$ and all output indices $i$, $F_\\theta(x)_i \\in [-c, c]$ for a given constant $c \\ge 0$. This models a common regime in which the residual branch cannot arbitrarily cancel large input magnitudes.\n\nWe will study a target mapping that strongly suppresses inputs, defined for a given scalar $s \\in \\mathbb{R}$ as $T(x) = s x$, with $|s| \\le 1$. For a finite dataset consisting of input vectors $\\{x^{(j)}\\}_{j=1}^N$, define the Mean Squared Error (MSE) as\n$$\n\\operatorname{MSE} = \\frac{1}{n N} \\sum_{j=1}^N \\left\\| \\hat{y}(x^{(j)}) - T\\left(x^{(j)}\\right) \\right\\|_2^2.\n$$\n\nYour task is to, from first principles and without using any machine learning libraries, compute the minimal achievable MSE for each scenario under the stated residual branch bound:\n- Identity skip: $y = x + F_\\theta(x)$ with the constraint that each component of $F_\\theta(x)$ lies in $[-c, c]$.\n- Projection skip: $y = \\alpha x + F_\\theta(x)$ where the projection is restricted to scalar scaling $P_\\phi = \\alpha I_n$ and $I_n$ is the identity matrix in $\\mathbb{R}^{n \\times n}$, with the same residual constraint on $F_\\theta(x)$.\n\nYou should reason about the optimal choice of the residual output under the bound, as well as the optimal scalar $\\alpha$ when allowed, and compute the resulting minimal MSEs.\n\nTest Suite:\nUse the following parameter sets, each consisting of a single input vector $x \\in \\mathbb{R}^n$, a scalar target factor $s$, and a residual bound $c$:\n- Case $1$: $x = [5, 5, 5, 5]$, $s = 0$, $c = 1$.\n- Case $2$: $x = [1, 1, 1, 1]$, $s = 0$, $c = 1$.\n- Case $3$: $x = [2, -3, 4, -1]$, $s = 0.2$, $c = 1$.\n- Case $4$: $x = [3, -3, 3, -3]$, $s = 0$, $c = 3$.\n- Case $5$: $x = [10, -1, 3, -7]$, $s = 0.9$, $c = 0.05$.\n\nFor each case, compute two quantities:\n- The minimal MSE achievable with identity skip.\n- The minimal MSE achievable with projection skip.\n\nFinally, report, for each case, the difference between these two minimal MSEs (identity minus projection). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the cases $1$ through $5$ (e.g., $[d_1,d_2,d_3,d_4,d_5]$), where each $d_j$ is a floating-point number giving the difference for case $j$.\n\nNo physical units or angle units are involved. All results must be numerical floats. The program must be self-contained and not rely on external files or user input.", "solution": "The problem asks for the minimal achievable Mean Squared Error (MSE) for two types of residual blocks, given a single data point $(x, T(x))$, where $T(x)=sx$ and $N=1$. The MSE is given by $\\operatorname{MSE} = \\frac{1}{n} \\left\\| \\hat{y}(x) - T(x) \\right\\|_2^2$. Minimizing this is equivalent to minimizing the squared error for each component independently. Let $E_i = (\\hat{y}_i - s x_i)^2$.\n\n### Case 1: Identity Skip\n\nThe model is $y = x + F_\\theta(x)$. Let $z = F_\\theta(x)$, where each component $z_i$ is constrained such that $z_i \\in [-c, c]$.\nThe error for the $i$-th component is:\n$$ E_i = (x_i + z_i - s x_i)^2 = ((1-s)x_i + z_i)^2 $$\nTo minimize this quadratic in $z_i$, we must choose $z_i$ to be as close as possible to the value that makes the term zero, which is $-(1-s)x_i = (s-1)x_i$.\nGiven the constraint $z_i \\in [-c, c]$, the optimal choice for $z_i$ is the value $(s-1)x_i$ projected (or clipped) onto the interval $[-c, c]$.\n$$ z_i^* = \\operatorname{clip}((s-1)x_i, -c, c) $$\nThe minimal component-wise squared error is then $E_i^* = ((1-s)x_i + z_i^*)^2$. The minimal MSE for the identity skip block is the average of these errors:\n$$ \\operatorname{MSE}_{\\text{id}} = \\frac{1}{n} \\sum_{i=1}^n ((1-s)x_i + \\operatorname{clip}((s-1)x_i, -c, c))^2 $$\n\n### Case 2: Projection Skip\n\nThe model is $y = \\alpha x + F_\\theta(x)$, where $\\alpha$ is a learnable scalar and $z = F_\\theta(x)$ has components $z_i \\in [-c, c]$.\nThe total MSE to minimize is:\n$$ \\operatorname{MSE}_{\\text{proj}} = \\min_{\\alpha, \\{z_i\\}} \\frac{1}{n} \\sum_{i=1}^n ((\\alpha - s)x_i + z_i)^2 $$\nWe have the freedom to choose both $\\alpha$ and the residual vector $z$. The expression inside the sum is a sum of squares, so it is minimized when each term is zero. We can achieve this perfectly.\nFirst, we choose the learnable scalar projection $\\alpha$ to match the target scaling factor $s$, i.e., we set $\\alpha = s$.\nThe expression for the squared error per component then simplifies to $(z_i)^2$.\n$$ \\operatorname{MSE}_{\\text{proj}} = \\min_{\\{z_i\\}} \\frac{1}{n} \\sum_{i=1}^n (z_i)^2 $$\nTo minimize this sum of squares, subject to the constraint $z_i \\in [-c, c]$, we must choose each $z_i$ to have the smallest possible magnitude. The optimal choice is therefore $z_i = 0$ for all $i$. This is a valid choice because the problem states $c \\ge 0$, so $0 \\in [-c, c]$ is always true.\nWith $\\alpha = s$ and $z_i = 0$ for all $i$, the MSE becomes exactly 0.\n$$ \\operatorname{MSE}_{\\text{proj}} = 0 $$\n\n### Difference Calculation\n\nThe problem asks for the difference between the minimal MSE of the identity skip and the projection skip.\n$$ \\Delta \\operatorname{MSE} = \\operatorname{MSE}_{\\text{id}} - \\operatorname{MSE}_{\\text{proj}} = \\operatorname{MSE}_{\\text{id}} - 0 = \\operatorname{MSE}_{\\text{id}} $$\nTherefore, the task reduces to computing $\\operatorname{MSE}_{\\text{id}}$ for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference in minimal MSE for identity-skip and projection-skip\n    residual blocks for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {'x': np.array([5, 5, 5, 5], dtype=float), 's': 0.0, 'c': 1.0},\n        {'x': np.array([1, 1, 1, 1], dtype=float), 's': 0.0, 'c': 1.0},\n        {'x': np.array([2, -3, 4, -1], dtype=float), 's': 0.2, 'c': 1.0},\n        {'x': np.array([3, -3, 3, -3], dtype=float), 's': 0.0, 'c': 3.0},\n        {'x': np.array([10, -1, 3, -7], dtype=float), 's': 0.9, 'c': 0.05}\n    ]\n\n    results = []\n    for case in test_cases:\n        x, s, c = case['x'], case['s'], case['c']\n        \n        # As derived in the solution, the required difference is simply the\n        # minimal MSE for the identity skip case, as the minimal MSE for the\n        # projection skip case is zero.\n\n        # The ideal residual F(x) to achieve the target T(x)=sx is (s-1)x.\n        target_residual = (s - 1.0) * x\n        \n        # The actual residual branch output is bounded, so the optimal choice is\n        # to clip the ideal residual to the allowed range [-c, c].\n        optimal_residual = np.clip(target_residual, -c, c)\n        \n        # The final output is y = x + optimal_residual. The error vector is y - sx.\n        # y - sx = (x + optimal_residual) - sx = (1-s)x + optimal_residual\n        # Note that (1-s)x = -target_residual.\n        # So, the error vector is (-target_residual + optimal_residual).\n        error_vector = -target_residual + optimal_residual\n        \n        # The MSE is the mean of the squared components of the error vector.\n        min_mse_id = np.mean(error_vector**2)\n\n        results.append(min_mse_id)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3169751"}]}