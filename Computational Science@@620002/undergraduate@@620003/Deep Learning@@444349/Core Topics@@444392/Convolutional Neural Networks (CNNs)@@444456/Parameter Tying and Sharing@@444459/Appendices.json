{"hands_on_practices": [{"introduction": "The most direct way to understand parameter tying is to see its consequences in action. This first practice demonstrates the fundamental link between a model's architecture and its learned symmetries. By implementing a simple neural network with and without tied parameters, you will empirically verify how enforcing parameter equality across parallel processing streams can create a model that is inherently invariant to specific transformations of its input, such as permuting coordinates. [@problem_id:3161977]", "problem": "Consider a binary classifier implemented as a feedforward neural network with one hidden layer over a two-dimensional input vector $x = (x_1, x_2)$. The hidden layer uses the Rectified Linear Unit (ReLU) activation, where the Rectified Linear Unit (ReLU) is defined as $\\mathrm{ReLU}(z) = \\max(0, z)$. The output layer is a scalar affine combination of hidden activations, and the final class label is determined by thresholding at $0$: the predicted label is $1$ if the scalar output is greater than or equal to $0$ and $0$ otherwise.\n\nYou will implement two variants of this classifier:\n\n- An untied-parameter model with hidden units\n  $$h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1), \\quad h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2),$$\n  and output\n  $$y = v_1 h_1 + v_2 h_2 + c.$$\n\n- A tied-parameter model with hidden units\n  $$h_1 = \\mathrm{ReLU}(a x_1 + b), \\quad h_2 = \\mathrm{ReLU}(a x_2 + b),$$\n  and output\n  $$y = v (h_1 + h_2) + c.$$\n\nParameter tying here means setting parameters equal across structurally symmetric components, for example $a_1 = a_2$, $b_1 = b_2$, and $v_1 = v_2$.\n\nWe define invariance under a transformation $T$ of the input space as follows: for a model $f$ mapping inputs to labels, $f$ is invariant under $T$ on a set $X$ if and only if $f(x) = f(T(x))$ for all $x \\in X$. The two transformations to be tested are:\n- The coordinate-swap transformation $S$ defined by $S(x_1, x_2) = (x_2, x_1)$.\n- The sign-flip transformation $F$ defined by $F(x_1, x_2) = (-x_1, -x_2)$.\n\nStarting from the fundamental basis that a feedforward network computes compositions of affine maps and pointwise non-linearities, and the definition of invariance above, construct a program that:\n- Implements both the untied and tied models.\n- Generates synthetic test inputs $x$ by sampling each coordinate independently from the uniform distribution on the interval $[-1, 1]$.\n- For a given transformation $T \\in \\{S, F\\}$, empirically verifies invariance by checking whether all predicted labels satisfy $f(x) = f(T(x))$ over the sampled set.\n\nYour program must use the following test suite, where $N$ is the number of samples, and “seed” specifies the pseudorandom number generator seed:\n\n- Test case $1$ (untied, expected non-invariance under $S$): parameters $a_1 = 1.0$, $b_1 = 0.1$, $a_2 = 2.0$, $b_2 = -0.3$, $v_1 = 1.0$, $v_2 = -0.5$, $c = 0.05$, transformation $T = S$, $N = 200$, seed $= 42$.\n- Test case $2$ (tied, expected invariance under $S$): parameters $a = 1.5$, $b = -0.2$, $v = 0.7$, $c = -0.1$, transformation $T = S$, $N = 200$, seed $= 42$.\n- Test case $3$ (untied but equal, expected invariance under $S$): parameters $a_1 = 0.8$, $b_1 = 0.0$, $a_2 = 0.8$, $b_2 = 0.0$, $v_1 = 1.0$, $v_2 = 1.0$, $c = 0.0$, transformation $T = S$, $N = 200$, seed $= 7$.\n- Test case $4$ (tied, expected non-invariance under $F$): parameters $a = 1.0$, $b = 0.0$, $v = 0.5$, $c = 0.05$, transformation $T = F$, $N = 200$, seed $= 42$.\n- Test case $5$ (tied with zero output weight, expected invariance under $S$ by trivial constancy): parameters $a = 1.0$, $b = 0.5$, $v = 0.0$, $c = 0.3$, transformation $T = S$, $N = 200$, seed $= 123$.\n\nFor each test case, compute a boolean indicating whether the model is invariant under the specified transformation on the sampled set. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each $result_i$ is the boolean value for test case $i$ in order.", "solution": "The problem requires an analysis and empirical verification of the relationship between parameter tying in a simple neural network and its invariance to specific input transformations. We will first establish the mathematical basis for the expected behavior and then use this framework to verify the outcomes of the prescribed test cases.\n\nA binary classifier is defined by a function $f(x)$ that maps an input vector $x$ to a label in $\\{0, 1\\}$. The classifier is said to be invariant under a transformation $T$ if $f(x) = f(T(x))$ for all inputs $x$ in a given domain. The classification rule is given by the sign of a scalar output function $y$, such that the label is $1$ if $y \\ge 0$ and $0$ otherwise. Therefore, invariance $f(x) = f(T(x))$ holds if and only if the sign of $y(x)$ is the same as the sign of $y(T(x))$, i.e., $(\\,y(x) \\ge 0 \\land y(T(x)) \\ge 0\\,) \\lor (\\,y(x) < 0 \\land y(T(x)) < 0\\,)$. A sufficient, but not necessary, condition for this is $y(x) = y(T(x))$.\n\nThe input vector is $x = (x_1, x_2)$. The activation function is the Rectified Linear Unit, $\\mathrm{ReLU}(z) = \\max(0, z)$.\n\nThe two models are:\n1.  **Untied Model**: The hidden units are $h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1)$ and $h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2)$. The scalar output is $y_{untied}(x_1, x_2) = v_1 h_1 + v_2 h_2 + c$. The structure is asymmetric, processing each input coordinate with distinct parameters.\n\n2.  **Tied Model**: The hidden units are $h_1 = \\mathrm{ReLU}(a x_1 + b)$ and $h_2 = \\mathrm{ReLU}(a x_2 + b)$. The scalar output is $y_{tied}(x_1, x_2) = v (h_1 + h_2) + c$. The parameters $(a, b)$ for the hidden layer and the weight $v$ for the output layer are shared (or \"tied\") across the two input branches.\n\nThe two transformations are:\n1.  **Coordinate-Swap ($S$)**: $S(x_1, x_2) = (x_2, x_1)$.\n2.  **Sign-Flip ($F$)**: $F(x_1, x_2) = (-x_1, -x_2)$.\n\n**Analysis of Invariance under Coordinate-Swap ($S$)**\n\nLet's analyze the output of each model for a swapped input $S(x) = (x_2, x_1)$.\n\n*   **Tied Model**:\n    The output for the original input is:\n    $$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\n    The output for the swapped input is:\n    $$y_{tied}(x_2, x_1) = v (\\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)) + c$$\n    Due to the commutative property of addition, $\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b) = \\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)$. Thus, $y_{tied}(x_1, x_2) = y_{tied}(x_2, x_1)$ for all $x$. This implies that the predicted labels will also be identical, $f_{tied}(x) = f_{tied}(S(x))$. The tied model architecture inherently enforces invariance to the coordinate-swap transformation.\n\n*   **Untied Model**:\n    The output for the original input is:\n    $$y_{untied}(x_1, x_2) = v_1 \\mathrm{ReLU}(a_1 x_1 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_2 + b_2) + c$$\n    The output for the swapped input is:\n    $$y_{untied}(x_2, x_1) = v_1 \\mathrm{ReLU}(a_1 x_2 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_1 + b_2) + c$$\n    In general, $y_{untied}(x_1, x_2) \\neq y_{untied}(x_2, x_1)$ unless the parameters possess a specific symmetry. Invariance is achieved if the function is symmetric with respect to swapping $x_1$ and $x_2$. This happens if the terms can be permuted, which requires $a_1=a_2$, $b_1=b_2$, and $v_1=v_2$. These are precisely the constraints imposed by parameter tying.\n\n**Analysis of Invariance under Sign-Flip ($F$)**\n\nLet's analyze the output of the tied model for a sign-flipped input $F(x) = (-x_1, -x_2)$.\nThe output for the original input is:\n$$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\nThe output for the flipped input is:\n$$y_{tied}(-x_1, -x_2) = v (\\mathrm{ReLU}(a(-x_1) + b) + \\mathrm{ReLU}(a(-x_2) + b)) + c = v (\\mathrm{ReLU}(-a x_1 + b) + \\mathrm{ReLU}(-a x_2 + b)) + c$$\nFor invariance, we need $y_{tied}(x_1, x_2)$ and $y_{tied}(-x_1, -x_2)$ to have the same sign. In general this is not true. The $\\mathrm{ReLU}$ function is not an even function, i.e., $\\mathrm{ReLU}(z) \\neq \\mathrm{ReLU}(-z)$ except for $z=0$. Therefore, the tied architecture does not inherently enforce invariance to the sign-flip transformation.\n\n**Empirical Verification via Test Cases**\n\nWe will now apply this reasoning to the specific test cases. For each case, we generate $N=200$ samples of $x=(x_1, x_2)$ with coordinates drawn from $U[-1, 1]$ and check if $f(x) = f(T(x))$ for all samples.\n\n*   **Test Case 1**: Untied model with asymmetric parameters ($a_1=1.0, b_1=0.1, a_2=2.0, b_2=-0.3, v_1=1.0, v_2=-0.5, c=0.05$) under transformation $S$. As predicted by our analysis, the lack of parameter symmetry will break the invariance. The result is expected to be `False`.\n\n*   **Test Case 2**: Tied model ($a=1.5, b=-0.2, v=0.7, c=-0.1$) under transformation $S$. As shown analytically, the tied model is invariant to coordinate swapping. The result is expected to be `True`.\n\n*   **Test Case 3**: Untied model with parameters that are manually set to be symmetric ($a_1=0.8, b_1=0.0, a_2=0.8, b_2=0.0, v_1=1.0, v_2=1.0, c=0.0$). This model is functionally equivalent to a tied model: $y_{untied} = \\mathrm{ReLU}(0.8 x_1) + \\mathrm{ReLU}(0.8 x_2)$. Therefore, it will be invariant to transformation $S$. This case highlights that invariance is a property of the computed function, which is determined by the parameter values, not merely the model's structural classification as \"untied\". The result is expected to be `True`.\n\n*   **Test Case 4**: Tied model ($a=1.0, b=0.0, v=0.5, c=0.05$) under transformation $F$. The problem expects non-invariance. Let's analyze the output function:\n    $$y = 0.5 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.0) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.0)) + 0.05 = 0.5 \\cdot (\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)) + 0.05$$\n    The input coordinates $x_1, x_2$ are sampled from $[-1, 1]$. The $\\mathrm{ReLU}$ function is non-negative.\n    The minimum value of $\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)$ is $0$ (when $x_1 \\le 0$ and $x_2 \\le 0$).\n    In this case, the minimum output is $y_{min} = 0.5 \\cdot 0 + 0.05 = 0.05$.\n    Since the output $y$ is always greater than or equal to $0.05$, it is always greater than the threshold of $0$. This means the predicted label is always $1$, i.e., $f(x) \\equiv 1$ for all inputs in the domain. A constant function is trivially invariant under any transformation, including $F$. Thus, contrary to the problem's informal expectation, the model will be empirically verified as invariant. The result will be `True`.\n\n*   **Test Case 5**: Tied model with a zero output weight ($a=1.0, b=0.5, v=0.0, c=0.3$) under transformation $S$. The output function is:\n    $$y = 0.0 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.5) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.5)) + 0.3 = 0.3$$\n    The output is a constant $0.3$. Since $0.3 \\ge 0$, the predicted label is always $1$. As in the previous case, the classifier is a constant function and is therefore trivially invariant to any transformation. The result is `True`.\n\nThe program will implement these models and checks to produce a list of booleans corresponding to these five cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests untied and tied neural network models for invariance\n    under specified transformations.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 1.0, \"b1\": 0.1, \"a2\": 2.0, \"b2\": -0.3, \"v1\": 1.0, \"v2\": -0.5, \"c\": 0.05},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.5, \"b\": -0.2, \"v\": 0.7, \"c\": -0.1},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 0.8, \"b1\": 0.0, \"a2\": 0.8, \"b2\": 0.0, \"v1\": 1.0, \"v2\": 1.0, \"c\": 0.0},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 7\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.0, \"v\": 0.5, \"c\": 0.05},\n            \"transformation\": \"F\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.5, \"v\": 0.0, \"c\": 0.3},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 123\n        }\n    ]\n\n    def relu(z):\n        \"\"\"Rectified Linear Unit activation function.\"\"\"\n        return np.maximum(0, z)\n\n    def untied_model(X, params):\n        \"\"\"Computes labels for the untied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a1\"] * x1 + p[\"b1\"])\n        h2 = relu(p[\"a2\"] * x2 + p[\"b2\"])\n        y = p[\"v1\"] * h1 + p[\"v2\"] * h2 + p[\"c\"]\n        return (y >= 0).astype(int)\n\n    def tied_model(X, params):\n        \"\"\"Computes labels for the tied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a\"] * x1 + p[\"b\"])\n        h2 = relu(p[\"a\"] * x2 + p[\"b\"])\n        y = p[\"v\"] * (h1 + h2) + p[\"c\"]\n        return (y >= 0).astype(int)\n\n    model_funcs = {\n        \"untied\": untied_model,\n        \"tied\": tied_model\n    }\n\n    transform_funcs = {\n        \"S\": lambda X: X[:, ::-1],  # Coordinate-swap\n        \"F\": lambda X: -X           # Sign-flip\n    }\n\n    results = []\n    for case in test_cases:\n        # 1. Set seed and generate data\n        np.random.seed(case[\"seed\"])\n        X_original = np.random.uniform(-1, 1, size=(case[\"N\"], 2))\n\n        # 2. Apply transformation\n        transform_func = transform_funcs[case[\"transformation\"]]\n        X_transformed = transform_func(X_original)\n\n        # 3. Select model and compute labels\n        model_func = model_funcs[case[\"model\"]]\n        labels_original = model_func(X_original, case[\"params\"])\n        labels_transformed = model_func(X_transformed, case[\"params\"])\n\n        # 4. Check for invariance\n        is_invariant = np.array_equal(labels_original, labels_transformed)\n        results.append(is_invariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161977"}, {"introduction": "While hard-coding parameter equality is powerful, a more flexible approach is \"soft\" parameter tying, which encourages but does not strictly enforce sharing. This exercise explores this concept using regularization, a common technique in machine learning. You will apply penalties based on the $L_1$ and $L_2$ norms to the difference between parameter sets, observing firsthand how these penalties influence both model compression and predictive performance. [@problem_id:3161931]", "problem": "You are to implement and analyze soft parameter tying via regularization on the difference of parameter vectors in a simple binary classifier. The problem explores how different norms affect both compression (how many parameters become effectively shared) and predictive accuracy. The implementation must be a complete, runnable program.\n\nThe foundational basis for this problem is Empirical Risk Minimization (ERM), Binary Cross-Entropy (BCE), gradient-based optimization, and convex norm regularization. Let $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ denote the logistic sigmoid function. BCE for a single example $(x, y)$ with predicted probability $p(x)$ is $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$, and ERM approximates expected risk by the sample average.\n\nModel specification:\n- Inputs are vectors $x \\in \\mathbb{R}^d$ and labels are $y \\in \\{0,1\\}$.\n- The model maintains two weight vectors $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$.\n- The prediction is $p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$.\n- The training objective is the average Binary Cross-Entropy (BCE) over the training set plus a soft-tying penalty applied to the parameter difference $d = w^{(a)} - w^{(b)}$.\n\nSoft-tying penalties to compare:\n- $L_1$ penalty: $\\lambda_1 \\|d\\|_1$.\n- $L_2$ penalty (squared): $\\lambda_2 \\|d\\|_2^2$.\n- Elastic net penalty: $\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$.\n\nData generation (deterministic):\n- Use pseudo-random seed $0$ for all randomness.\n- Dimension $d = 10$.\n- Training set size $n_{\\text{train}} = 512$ and validation set size $n_{\\text{val}} = 256$.\n- Draw $w^\\star \\sim \\mathcal{N}(0, I_d)$ once and fix it for both sets.\n- Draw inputs $x \\sim \\mathcal{N}(0, I_d)$ independently for all samples.\n- Draw noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma^2 = 0.25$ independently for all samples.\n- Define labels as $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon > 0\\}$.\n\nTraining procedure:\n- Initialize $w^{(a)} = 0$ and $w^{(b)} = 0$ in $\\mathbb{R}^d$.\n- Optimize the objective using full-batch gradient descent for $T = 400$ iterations with step size $\\eta = 0.05$.\n- For the $L_1$ penalty, use a subgradient $\\text{sign}(d)$.\n- For the $L_2$ penalty (squared), use gradient $2\\lambda_2 d$.\n- For elastic net, sum the respective gradients.\n\nEvaluation metrics:\n- Compression ratio: the fraction of indices $i \\in \\{1,\\dots,d\\}$ such that $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$, with tolerance $\\tau = 10^{-2}$.\n- Accuracy: on the validation set, the fraction of correctly classified examples using the decision rule $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$.\n\nTest suite:\nImplement and run the following $5$ test cases, each defined by $(\\lambda_1, \\lambda_2)$ and the penalty type:\n1. Baseline, no tying: $\\lambda_1 = 0$, $\\lambda_2 = 0$.\n2. $L_2$ moderate: $\\lambda_1 = 0$, $\\lambda_2 = 0.1$.\n3. $L_1$ moderate: $\\lambda_1 = 0.05$, $\\lambda_2 = 0$.\n4. $L_2$ strong: $\\lambda_1 = 0$, $\\lambda_2 = 1.0$.\n5. Elastic net balanced: $\\lambda_1 = 0.05$, $\\lambda_2 = 0.2$.\n\nYour program should implement the above setup and produce, for each test case in the given order, a pair $[\\text{compression\\_ratio}, \\text{accuracy}]$ with both values as floating-point numbers. The final output format must be a single line containing a list of these $5$ pairs, in order, for example:\n$[[c_1,a_1],[c_2,a_2],[c_3,a_3],[c_4,a_4],[c_5,a_5]]$,\nwith each $c_i$ and $a_i$ being floating-point numbers rounded to $4$ decimal places.", "solution": "The objective is to train a model by minimizing a composite objective function $J(w^{(a)}, w^{(b)})$ using gradient descent. This objective combines a data-fitting term (average BCE loss) and a regularization term that promotes parameter tying.\n\n**Objective Function**\n\nLet the training data be $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$. The model's prediction for a sample $x_i$ is $p_i = p(x_i) = \\sigma(z_i)$, where the logit is $z_i = \\frac{1}{2}(w^{(a)\\top}x_i + w^{(b)\\top}x_i)$. The total objective function is:\n$$\nJ(w^{(a)}, w^{(b)}) = J_{\\text{BCE}}(w^{(a)}, w^{(b)}) + R(w^{(a)} - w^{(b)})\n$$\nThe first term is the average BCE loss:\n$$\nJ_{\\text{BCE}}(w^{(a)}, w^{(b)}) = -\\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\nThe second term is the general form of the soft-tying penalty on the difference vector $d = w^{(a)} - w^{(b)}$:\n$$\nR(d) = \\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2\n$$\n\n**Gradient Calculation**\nTo implement gradient descent, we need the gradients of $J$ with respect to $w^{(a)}$ and $w^{(b)}$. We compute these using the chain rule.\n\nThe gradient of the BCE loss for a single sample $L_i$ with respect to the logit $z_i$ is well-known:\n$$\n\\frac{\\partial L_i}{\\partial z_i} = p_i - y_i\n$$\nThe gradient of the logit $z_i$ with respect to the weight vectors is:\n$$\n\\nabla_{w^{(a)}} z_i = \\frac{1}{2}x_i \\quad \\text{and} \\quad \\nabla_{w^{(b)}} z_i = \\frac{1}{2}x_i\n$$\nApplying the chain rule for the BCE term over the entire training set:\n$$\n\\nabla_{w^{(a)}} J_{\\text{BCE}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\frac{\\partial L_i}{\\partial z_i} \\nabla_{w^{(a)}} z_i = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (p_i - y_i) \\frac{1}{2}x_i = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\nBy symmetry, the gradient with respect to $w^{(b)}$ is identical:\n$$\n\\nabla_{w^{(b)}} J_{\\text{BCE}} = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\nwhere $X$ is the $n_{\\text{train}} \\times d$ data matrix, $P$ is the vector of predictions, and $Y$ is the vector of true labels.\n\nNext, we find the gradient of the regularization term $R(d) = R(w^{(a)} - w^{(b)})$.\n$$\n\\nabla_{w^{(a)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(a)}} = \\nabla_{d} R(d)\n$$\n$$\n\\nabla_{w^{(b)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(b)}} = -\\nabla_{d} R(d)\n$$\nThe (sub)gradient $\\nabla_{d} R(d)$ is given by:\n$$\n\\nabla_{d} R(d) = \\lambda_1 \\text{sign}(d) + 2\\lambda_2 d\n$$\nHere, $\\text{sign}(d)$ is the subgradient of the $L_1$ norm. For algorithmic implementation, $\\text{sign}(0)$ can be taken as $0$.\n\nCombining these terms, the full gradients are:\n$$\n\\nabla_{w^{(a)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) + (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n$$\n\\nabla_{w^{(b)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) - (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n\n**Optimization Algorithm**\nThe weights are updated using full-batch gradient descent for $T=400$ iterations with learning rate $\\eta=0.05$. Starting from $w^{(a)}_0 = 0$ and $w^{(b)}_0 = 0$, the update rules at step $t$ are:\n$$\nw^{(a)}_{t+1} = w^{(a)}_t - \\eta \\nabla_{w^{(a)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\n$$\nw^{(b)}_{t+1} = w^{(b)}_t - \\eta \\nabla_{w^{(b)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\nThis process is repeated for each of the $5$ test cases defined by the $(\\lambda_1, \\lambda_2)$ pairs.\n\n**Evaluation**\nAfter training, two metrics are computed:\n1.  **Compression Ratio**: This measures the degree of parameter sharing. It is the fraction of dimensions for which the absolute difference between corresponding weights is below a tolerance $\\tau = 10^{-2}$.\n    $$\n    \\text{Compression Ratio} = \\frac{1}{d} \\sum_{i=1}^d \\mathbf{1}\\{|w^{(a)}_i - w^{(b)}_i| \\le \\tau\\}\n    $$\n2.  **Accuracy**: This measures predictive performance on the unseen validation set. The predicted label $\\hat{y}$ is $1$ if the predicted probability $p(x) \\ge 0.5$, and $0$ otherwise. This is equivalent to checking if the logit $z = \\frac{1}{2}(w^{(a)\\top}x + w^{(b)\\top}x) \\ge 0$.\n    $$\n    \\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\mathbf{1}\\{\\hat{y}_i = y_i^{\\text{val}}\\}\n    $$\nThe implementation will follow these derivations to produce the required output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes soft parameter tying in a binary classifier.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    SEED = 0\n    D = 10  # Dimension of input vectors\n    N_TRAIN = 512\n    N_VAL = 256\n    NOISE_VAR = 0.25\n    \n    T = 400  # Number of iterations\n    ETA = 0.05  # Step size (learning rate)\n    TAU = 1e-2  # Tolerance for compression ratio\n\n    # --- Test Cases ---\n    test_cases = [\n        # (lambda_1, lambda_2), penalty_type\n        (0.0, 0.0),      # 1. Baseline, no tying\n        (0.0, 0.1),      # 2. L2 moderate\n        (0.05, 0.0),     # 3. L1 moderate\n        (0.0, 1.0),      # 4. L2 strong\n        (0.05, 0.2),     # 5. Elastic net balanced\n    ]\n\n    # --- Helper Functions ---\n    def sigmoid(u):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-u))\n\n    def generate_data(n_samples, d, w_star, noise_std, rng):\n        \"\"\"Generates synthetic data for binary classification.\"\"\"\n        X = rng.standard_normal(size=(n_samples, d))\n        epsilon = rng.normal(0, noise_std, size=n_samples)\n        logits = X @ w_star + epsilon\n        y = (logits > 0).astype(int)\n        return X, y\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(SEED)\n    w_star = rng.standard_normal(size=D)\n    noise_std = np.sqrt(NOISE_VAR)\n    \n    X_train, y_train = generate_data(N_TRAIN, D, w_star, noise_std, rng)\n    X_val, y_val = generate_data(N_VAL, D, w_star, noise_std, rng)\n\n    results = []\n\n    # --- Main Loop: Train and Evaluate for each test case ---\n    for lambda_1, lambda_2 in test_cases:\n        # Initialize weights\n        w_a = np.zeros(D)\n        w_b = np.zeros(D)\n\n        # Full-batch gradient descent\n        for _ in range(T):\n            # Forward pass\n            logits = 0.5 * (X_train @ w_a + X_train @ w_b)\n            predictions = sigmoid(logits)\n            \n            # --- Gradient Calculation ---\n            # Gradient of BCE loss term\n            error = predictions - y_train\n            grad_bce = (1 / (2 * N_TRAIN)) * X_train.T @ error\n\n            # Gradient of regularization term\n            d = w_a - w_b\n            grad_reg = lambda_1 * np.sign(d) + 2 * lambda_2 * d\n            \n            # Full gradients for w_a and w_b\n            grad_w_a = grad_bce + grad_reg\n            grad_w_b = grad_bce - grad_reg\n\n            # --- Weight Update ---\n            w_a -= ETA * grad_w_a\n            w_b -= ETA * grad_w_b\n\n        # --- Evaluation ---\n        # 1. Compression Ratio\n        final_d = w_a - w_b\n        compression_ratio = np.mean(np.abs(final_d) <= TAU)\n\n        # 2. Accuracy on validation set\n        val_logits = 0.5 * (X_val @ w_a + X_val @ w_b)\n        y_hat_val = (val_logits >= 0).astype(int)\n        accuracy = np.mean(y_hat_val == y_val)\n        \n        results.append((compression_ratio, accuracy))\n\n    # --- Format and Print Output ---\n    output_str = \"[\" + \",\".join([f\"[{c:.4f},{a:.4f}]\" for c, a in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3161931"}, {"introduction": "Parameter tying finds its most sophisticated expression in equivariant neural networks, which are foundational to modern geometric deep learning. This advanced practice guides you through the implementation of a Group Convolutional layer over the cyclic group $C_4$ (90-degree rotations). By generating rotated filters from a single, shared base kernel, you will build a network layer that respects rotational symmetry, a property known as equivariance, and quantify its advantages over naive data augmentation. [@problem_id:3161942]", "problem": "You are given a two-dimensional discrete signal and asked to implement Group Convolution (GC) over the cyclic group of order four, denoted by $C_4$, which consists of rotations by multiples of $\\pi/2$ radians. The objective is to realize parameter tying by generating rotated versions of a single base filter, use these to compute orientation-indexed feature maps, and quantify the equivariance properties of GC. You must compare these properties to a baseline that represents \"data augmentation alone,\" modeled here as applying a single unrotated filter to the input and its rotated versions without any orientation-channel parameter sharing or transformation of the filters. The comparison is to be made on rotationally structured tasks where invariance and equivariance are relevant.\n\nStart from the following foundational basis:\n- Discrete cross-correlation used in Convolutional Neural Networks (CNNs) is defined by, for an input $x \\in \\mathbb{R}^{H \\times W}$ and a kernel $k \\in \\mathbb{R}^{m \\times n}$ with zero-padding to maintain spatial size,\n$$\n(y = x \\star k)_{i,j} = \\sum_{u=0}^{m-1}\\sum_{v=0}^{n-1} x_{i+u-p, j+v-q}\\,k_{u,v},\n$$\nwhere $p = \\lfloor m/2 \\rfloor$ and $q = \\lfloor n/2 \\rfloor$, and indices outside the image are treated as zeros.\n- The cyclic group $C_4 = \\{0,1,2,3\\}$ represents rotation actions $R_r$ by $r \\cdot \\pi/2$ radians on the grid, implemented exactly by $90^{\\circ}$ increment rotations using integer grid rotation $R_r(x) = \\operatorname{rot90}(x, r)$.\n- Group Convolution (GC) over $C_4$ with parameter tying constructs rotated filters $k_g = R_g(k)$ from a single base filter $k$ and returns orientation-indexed feature maps $y[g] = x \\star k_g$ for $g \\in C_4$.\n- Equivariance for GC is the property that rotating the input by $r$ induces a predictable transformation of the GC outputs: for rotation $R_r$, the relation\n$$\nx_r = R_r(x), \\quad y_r[g] = x_r \\star k_g, \\quad \\text{and} \\quad y_r[g] \\stackrel{?}{=} R_r\\!\\Big(y\\big[(g-r) \\bmod 4\\big]\\Big)\n$$\nshould hold when using zero-padding and exact $90^{\\circ}$-grid rotations.\n- Invariance can be induced by pooling over the group, for example by averaging: $P(x) = \\frac{1}{4}\\sum_{g=0}^{3} y[g]$. Under rotation $R_r$, the prediction is $P(x_r) \\stackrel{?}{=} R_r\\big(P(x)\\big)$.\n\nYou must implement:\n1. A function to perform discrete cross-correlation with zero-padding to produce spatially \"same\"-sized outputs.\n2. A function to rotate images and kernels by $r \\in C_4$ using integer-grid rotations.\n3. Group Convolution over $C_4$ with parameter tying (rotated kernels from a single base kernel) producing four orientation-indexed feature maps.\n4. A baseline representing \"data augmentation alone\" in inference using a single unrotated filter $k$ applied to both the original and the rotated inputs, without generating rotated filters or orientation channels.\n\nYou must compute the following quantitative metrics:\n- Equivariance error for GC:\n$$\nE_{\\text{GC}}(x, k, r) = \\frac{1}{4HW}\\sum_{g=0}^{3}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} \\Big( y_r[g]_{i,j} - \\big(R_r\\!\\big(y[(g-r)\\bmod 4]\\big)\\big)_{i,j} \\Big)^2.\n$$\n- Invariance error for pooled GC:\n$$\nE_{\\text{pool}}(x, k, r) = \\frac{1}{HW}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} \\Big( P(x_r)_{i,j} - \\big(R_r(P(x))\\big)_{i,j} \\Big)^2.\n$$\n- Invariance error for the augmentation-only single-filter baseline:\n$$\nE_{\\text{aug-single}}(x, k, r) = \\frac{1}{HW}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} \\Big( S(x_r)_{i,j} - \\big(R_r(S(x))\\big)_{i,j} \\Big)^2,\n$$\nwhere $S(x) = x \\star k$ is the single-channel output of the unrotated filter.\n\nTest suite. Use images of size $9 \\times 9$ with binary entries $0$ or $1$, the base kernel\n$$\nk = \\begin{bmatrix}\n0 & -1 & 0 \\\\\n0 & \\phantom{-}2 & 0 \\\\\n0 & -1 & 0\n\\end{bmatrix},\n$$\nand rotation angle index $r=1$ (i.e., $\\pi/2$ radians).\nConstruct the following inputs:\n- Test case $1$ (happy path): A vertical bar centered in the image, defined by $x_{i,4} = 1$ for all $i \\in \\{0,\\dots,8\\}$, and $x_{i,j} = 0$ otherwise.\n- Test case $2$ (rotationally symmetric edge case): A plus shape centered in the image, defined by $x_{i,4} = 1$ for all $i$, and $x_{4,j} = 1$ for all $j$ (so both the central row and central column are ones), and $0$ otherwise.\n- Test case $3$ (nontrivial orientation edge case): A main diagonal bar, defined by $x_{i,i} = 1$ for all $i$, and $0$ otherwise.\n\nFor each test case, compute and return the triple $\\big(E_{\\text{GC}}, E_{\\text{pool}}, E_{\\text{aug-single}}\\big)$ as real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$, $2$, $3$, each contributing three numbers, so the final output contains nine comma-separated floats in total, for example, $[e_{11}, e_{12}, e_{13}, e_{21}, e_{22}, e_{23}, e_{31}, e_{32}, e_{33}]$. No physical units or angles need to be reported beyond the group index definition, and all outputs must be real numbers.", "solution": "The problem requires the implementation and quantitative analysis of Group Convolution (GC) over the cyclic group of order four, $C_4$, which mathematically represents $90$-degree rotations on a two-dimensional grid. The core of the problem is to verify the equivariance properties of GC and compare its performance to a baseline model.\n\nThe key principle behind Group Convolution is to build neural network layers that are guaranteed to respect specific symmetries, in this case, rotational symmetry. A function $f$ is said to be equivariant to a group of transformations $G$ if applying a transformation $g \\in G$ to the input results in a predictable transformation of the output. For a convolution layer with group $C_4$, this property is formally expressed as convoluting a rotated input being equivalent to rotating the output of the convolution on the original input, albeit with a corresponding shift in the orientation channels. The problem statement formulates this as checking if $y_r[g] = R_r(y[(g-r) \\bmod 4])$, where $y_r[g]$ is the $g$-th feature map for a rotated input $x_r=R_r(x)$, and $R_r$ is the rotation operator.\n\nParameter tying is central to the design of GC. Instead of learning a separate, independent kernel for each of the four desired orientations, a single base kernel, $k$, is learned. The other kernels, $k_g$, are generated by applying the group action (rotation) to this base kernel: $k_g = R_g(k)$ for $g \\in C_4 = \\{0, 1, 2, 3\\}$. This drastically reduces the number of learnable parameters and embeds the rotational symmetry directly into the architecture of the model.\n\nThe group action $R_r$ on a discrete grid is implemented by a $90$-degree rotation of the matrix of pixel values, an operation denoted as $\\operatorname{rot90}(x, r)$. The convolution operation is defined as discrete cross-correlation. A critical detail is the use of zero-padding to maintain the spatial dimensions of the input (a \"same\" convolution). This choice has profound consequences for equivariance. The convolution operator, $\\star$, as defined with zero-padding, does not commute with the rotation operator, $R_r$. That is, $R_r(x \\star k) \\neq R_r(x) \\star R_r(k)$. This is because padding is applied relative to the fixed boundaries of the array; rotating first changes which pixels are near the boundary and thus affected differently by padding. As a result, perfect equivariance is broken at the boundaries of the feature map. The equivariance error, $E_{\\text{GC}}$, is a metric designed to quantify this deviation from perfect equivariance due to these boundary effects.\n\nFrom the equivariant feature maps $y[g]$, one can construct features that are invariant to rotation. A standard method is to pool over the group dimension. The problem defines an average pooling operation, $P(x) = \\frac{1}{4}\\sum_{g=0}^{3} y[g]$. If the underlying layer were perfectly equivariant, this pooled output would be equivariant: $P(R_r(x)) = R_r(P(x))$. The invariance error $E_{\\text{pool}}$ measures the deviation from this property. A special feature of the provided base kernel $k$ is that the sum of its $C_4$ rotations is the zero matrix: $\\sum_{g \\in C_4} R_g(k) = 0$. This is because $k_0+k_2=0$ and $k_1+k_3=0$. Consequently, the pooled output $P(x) = x \\star (\\frac{1}{4}\\sum_g k_g) = x \\star 0 = 0$ is always a zero matrix, regardless of the input $x$. This makes the pooled representation $P(x)$ trivially and perfectly rotationally symmetric, meaning $P(R_r(x)) = 0$ and $R_r(P(x)) = 0$. Therefore, for this specific problem, the invariance error $E_{\\text{pool}}$ will be exactly zero.\n\nThe baseline model represents a naive approach, sometimes labeled \"data augmentation at test time,\" where a single, unrotated filter $k$ is applied to both the original input $x$ and its rotated version $x_r$. The outputs are $S(x) = x \\star k$ and $S(x_r) = x_r \\star k$. The invariance error $E_{\\text{aug-single}}$ measures if rotating the input is equivalent to rotating the output, i.e., $S(x_r) \\stackrel{?}{=} R_r(S(x))$. Since the filter $k$ is orientation-specific (it detects vertical edges) and is not adapted to the rotated input, this property is not expected to hold. For instance, applying a vertical edge detector to a horizontal bar will yield a very different result from rotating the output of the same detector applied to a vertical bar. We expect $E_{\\text{aug-single}}$ to be significantly larger than $E_{\\text{GC}}$, demonstrating the failure of this approach to properly handle rotational symmetry.\n\nThe implementation plan is as follows:\n1.  Define a function for the rotation $R_r$ using `numpy.rot90`.\n2.  Define a function for the cross-correlation $\\star$ using `scipy.signal.correlate2d` with `mode='same'` and zero-padding.\n3.  Implement the Group Convolution function, which generates rotated kernels and computes the four feature maps.\n4.  Implement a main function that, for each test case, computes the outputs for the original and rotated inputs ($y$, $y_r$, $S(x)$, $S(x_r)$), and then calculates the three error metrics $E_{\\text{GC}}$, $E_{\\text{pool}}$, and $E_{\\text{aug-single}}$ according to their specified formulas. The results are then aggregated and formatted for output.\n\nThe problem is thus a well-defined computational exercise in verifying the theoretical properties of group equivariant networks and understanding the practical implications of implementation choices like padding.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute equivariance/invariance errors.\n    \"\"\"\n\n    def rotate(array, r):\n        \"\"\"\n        Rotates a 2D numpy array counter-clockwise by r * 90 degrees.\n        r is an element of the cyclic group C4 = {0, 1, 2, 3}.\n        \"\"\"\n        # np.rot90(m, k) rotates k times by 90 degrees CCW.\n        return np.rot90(array, k=r)\n\n    def cross_correlate(x, k):\n        \"\"\"\n        Performs 2D cross-correlation with zero-padding for 'same' output size.\n        This matches the definition provided in the problem statement.\n        \"\"\"\n        return correlate2d(x, k, mode='same', boundary='fill', fillvalue=0)\n\n    def group_convolution(x, k_base):\n        \"\"\"\n        Performs Group Convolution over C4 for a given input x and base kernel k_base.\n        Returns a list of 4 orientation-indexed feature maps.\n        \"\"\"\n        C4_indices = [0, 1, 2, 3]\n        \n        # Generate the set of rotated kernels from the base kernel\n        kernels = [rotate(k_base, g) for g in C4_indices]\n        \n        # Compute the feature map for each rotated kernel\n        feature_maps = [cross_correlate(x, k_g) for k_g in kernels]\n        \n        return feature_maps\n\n    def calculate_metrics(x, k, r):\n        \"\"\"\n        Calculates the three error metrics for a given test case.\n        \"\"\"\n        H, W = x.shape\n        C4_indices = [0, 1, 2, 3]\n        \n        # --- Group Convolution (GC) Calculations ---\n        \n        # 1. Compute outputs for original input x\n        y = group_convolution(x, k)  # y is a list of 4 feature maps y[g]\n        \n        # 2. Compute outputs for rotated input x_r\n        x_r = rotate(x, r)\n        y_r = group_convolution(x_r, k)  # y_r is a list of 4 feature maps y_r[g]\n        \n        # 3. Calculate E_GC (Equivariance Error for GC)\n        total_sq_error_gc = 0.0\n        for g in C4_indices:\n            # The equivariance property states: y_r[g] should be R_r(y[(g-r) mod 4])\n            expected_y_g = rotate(y[(g - r) % 4], r)\n            diff_gc = y_r[g] - expected_y_g\n            total_sq_error_gc += np.sum(diff_gc**2)\n            \n        E_GC = total_sq_error_gc / (4 * H * W)\n        \n        # --- Pooled GC Calculations ---\n        \n        # 1. Pool the outputs for x\n        P_x = np.sum(y, axis=0) / 4.0\n        \n        # 2. Pool the outputs for x_r\n        P_xr = np.sum(y_r, axis=0) / 4.0\n        \n        # 3. Calculate E_pool (Invariance Error for Pooled GC)\n        expected_P_xr = rotate(P_x, r)\n        diff_pool = P_xr - expected_P_xr\n        E_pool = np.sum(diff_pool**2) / (H * W)\n        \n        # --- Augmentation-only Single-Filter Baseline Calculations ---\n        \n        # 1. Compute output S(x) = x * k\n        S_x = cross_correlate(x, k)\n        \n        # 2. Compute output S(x_r) = x_r * k\n        S_xr = cross_correlate(x_r, k)\n        \n        # 3. Calculate E_aug_single (Invariance Error for Baseline)\n        expected_S_xr = rotate(S_x, r)\n        diff_aug = S_xr - expected_S_xr\n        E_aug_single = np.sum(diff_aug**2) / (H * W)\n        \n        return (E_GC, E_pool, E_aug_single)\n\n    # Define common parameters for the test suite\n    H, W = 9, 9\n    r_rot = 1  # Rotation by r * pi/2, with r=1\n    base_kernel = np.array([\n        [0, -1, 0],\n        [0,  2, 0],\n        [0, -1, 0]\n    ], dtype=np.float64)\n\n    # Define the test cases from the problem statement.\n    # Test case 1 (happy path): Vertical bar\n    x1 = np.zeros((H, W), dtype=np.float64)\n    x1[:, 4] = 1.0\n\n    # Test case 2 (rotationally symmetric edge case): Plus shape\n    x2 = np.zeros((H, W), dtype=np.float64)\n    x2[:, W // 2] = 1.0\n    x2[H // 2, :] = 1.0\n\n    # Test case 3 (nontrivial orientation edge case): Main diagonal bar\n    x3 = np.eye(H, dtype=np.float64)\n\n    test_cases = [x1, x2, x3]\n\n    results = []\n    for case_input in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        e_gc, e_pool, e_aug = calculate_metrics(case_input, base_kernel, r_rot)\n        results.extend([e_gc, e_pool, e_aug])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161942"}]}