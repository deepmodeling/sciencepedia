{"hands_on_practices": [{"introduction": "The best way to truly understand a fundamental operation is to build it from the ground up. The two-dimensional convolution, despite its complex applications, is at its core an elegant and simple process: a sliding dot product. This exercise will guide you through implementing the convolution operation using basic loops, stripping away the abstractions of deep learning libraries to reveal the core arithmetic. By building the mechanism yourself and verifying it against a trusted standard, you will gain a concrete and unshakeable understanding of what happens inside a convolutional layer. [@problem_id:3180075]", "problem": "You are asked to demonstrate, from first principles, how the two-dimensional ($2\\mathrm{D}$) discrete convolution used in deep learning can be understood as sliding dot products, and to validate a naive implementation against a trusted numerical library. Begin from the fundamental definition of discrete convolution on finite arrays, and treat the image as a real-valued matrix $X \\in \\mathbb{R}^{H \\times W}$ and the kernel as a real-valued matrix $K \\in \\mathbb{R}^{R \\times S}$. Use the following constraints to ensure scientific realism and clarity: perform the convolution in the “valid” configuration (no padding, stride $1$, no dilation), reverse the kernel across both axes before sliding, and do not assume any optimizations or specialized hardware.\n\nYour program must implement the operation as sliding dot products computed by naive loops: for each spatial location, take the corresponding submatrix of $X$ of size $R \\times S$, reverse $K$ across both axes, and compute the scalar product between the two. Independently, compute the same result using Scientific Python (SciPy) to serve as an implementation oracle, and verify equality within a specified numerical tolerance. Then aggregate the verification outcomes across a set of test cases.\n\nUse the following test suite, which covers a general case, boundary cases, rectangular kernels, identity-like behavior, and mixed-sign floating-point values. In all cases below, the arrays are in $\\mathbb{R}$ and should be interpreted exactly as written.\n\n$X_1 = \\begin{bmatrix}\n1 & 2 & 0 & -1 \\\\\n3 & -1 & 2 & 0 \\\\\n0 & 1 & -2 & 3\n\\end{bmatrix},\\quad\nK_1 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix}.$\n\n$X_2 = \\begin{bmatrix}\n2 & -1 \\\\\n0 & 4\n\\end{bmatrix},\\quad\nK_2 = \\begin{bmatrix}\n-1 & 2 \\\\\n3 & 0\n\\end{bmatrix}.$\n\n$X_3 = \\begin{bmatrix}\n0.5 & -1.0 & 2.0 & 0.0 & 1.5 \\\\\n1.0 & 3.0 & -2.0 & 4.0 & -0.5 \\\\\n0.0 & -1.5 & 2.5 & -3.5 & 0.0 \\\\\n2.0 & 0.0 & 1.0 & -1.0 & 2.0\n\\end{bmatrix},\\quad\nK_3 = \\begin{bmatrix}\n1.0 & -1.0 & 0.5 \\\\\n0.0 & 2.0 & -0.5\n\\end{bmatrix}.$\n\n$X_4 = \\begin{bmatrix}\n1.0 & -2.0 & 3.0 \\\\\n-1.0 & 0.0 & 2.0 \\\\\n4.0 & -3.0 & 1.0\n\\end{bmatrix},\\quad\nK_4 = \\begin{bmatrix}\n1.0\n\\end{bmatrix}.$\n\n$X_5 = \\begin{bmatrix}\n0.1 & -0.2 & 0.3 \\\\\n1.5 & 0.0 & -0.5 \\\\\n-0.7 & 2.2 & 1.1 \\\\\n3.0 & -1.0 & 0.0 \\\\\n0.0 & 0.5 & -2.0\n\\end{bmatrix},\\quad\nK_5 = \\begin{bmatrix}\n0.5 & -0.5 \\\\\n1.0 & 0.0 \\\\\n-1.0 & 0.5\n\\end{bmatrix}.$\n\nRequirements for validation and output:\n- Compute the naive “valid” convolution between each $X_i$ and $K_i$ by reversing $K_i$ across both axes and sliding with stride $1$.\n- Compute the same “valid” convolution using Scientific Python (SciPy).\n- For each test case $i \\in \\{1,2,3,4,5\\}$, verify numerical equality using a tolerance of $10^{-12}$ in both relative and absolute terms, and return a boolean indicating whether the two results agree elementwise to within tolerance.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite given above, for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$ where each entry is a boolean.\n\nThere are no physical units or angle units involved. Express the numerical tolerance as the decimal $10^{-12}$ and ensure that all comparisons strictly adhere to this tolerance. The final output type for each test case is a boolean, and the aggregate output is a list of booleans on a single line.", "solution": "The problem requires a demonstration of two-dimensional ($2\\mathrm{D}$) discrete convolution as a series of sliding dot products, implemented from first principles and validated against a standard library function.\n\nThe fundamental operation is the $2\\mathrm{D}$ discrete convolution of a real-valued matrix $X \\in \\mathbb{R}^{H \\times W}$ (the image) with a real-valued matrix $K \\in \\mathbb{R}^{R \\times S}$ (the kernel). Let the output matrix be $Y$. For the \"valid\" convolution mode, with a stride of $1$ and no padding, the dimensions of the output matrix $Y$ are $(H - R + 1) \\times (W - S + 1)$.\n\nThe value of the output $Y$ at each location $(m, n)$ is defined by the convolution sum:\n$$\nY(m, n) = (X * K)(m, n) = \\sum_{i=0}^{R-1} \\sum_{j=0}^{S-1} X(m+i, n+j) \\cdot K(R-1-i, S-1-j)\n$$\nThis formula is applicable for output indices $m$ and $n$ such that $0 \\le m \\le H-R$ and $0 \\le n \\le W-S$. The expression $K(R-1-i, S-1-j)$ represents the kernel $K$ flipped (or reversed) along both its horizontal and vertical axes.\n\nLet us define the reversed kernel $K_r$ as:\n$$\nK_r(i, j) = K(R-1-i, S-1-j)\n$$\nWith this definition, the convolution formula simplifies to:\n$$\nY(m, n) = \\sum_{i=0}^{R-1} \\sum_{j=0}^{S-1} X(m+i, n+j) \\cdot K_r(i, j)\n$$\nThis form reveals the \"sliding dot product\" interpretation. Let $X_{m,n}$ denote the submatrix of $X$ of size $R \\times S$ whose top-left corner is at position $(m, n)$. The elements of this submatrix are $X_{m,n}(i, j) = X(m+i, n+j)$. The expression for $Y(m,n)$ is then the sum of the element-wise products of the submatrix $X_{m,n}$ and the reversed kernel $K_r$. This operation is equivalent to the Frobenius inner product of the two matrices:\n$$\nY(m, n) = \\langle X_{m,n}, K_r \\rangle_F\n$$\nConceptually, the reversed kernel $K_r$ slides across the image $X$. At each position $(m, n)$, the dot product between the kernel and the underlying image patch is calculated, and the result becomes the corresponding entry in the output matrix $Y$.\n\nThe implementation will proceed in three steps for each test case:\n1.  **Naive Implementation**: A function will be created to implement the sliding dot product from first principles. This function will first explicitly reverse the kernel $K$ to get $K_r$. Then, it will iterate through all valid top-left positions $(m, n)$ in the image $X$. For each position, it will extract the $R \\times S$ submatrix $X_{m,n}$, compute the element-wise product with $K_r$, and sum the results to produce the output value $Y(m, n)$.\n\n2.  **Library Implementation**: The `scipy.signal.convolve2d` function with the `mode='valid'` parameter will be used as an oracle. This function is a highly optimized and trusted implementation of the $2\\mathrm{D}$ convolution operation. By definition, it performs the kernel reversal internally before computing the sliding sum of products. Therefore, its output should be mathematically identical to the result of our naive implementation.\n\n3.  **Verification**: The numerical results from both implementations will be compared. The `numpy.allclose` function is suitable for this, as it performs an element-wise comparison of two arrays within a specified tolerance. As required, both the relative tolerance (`rtol`) and absolute tolerance (`atol`) will be set to $10^{-12}$. For each test case, the function will return `True` if the arrays are equal within this tolerance and `False` otherwise. The boolean results from all test cases will be collected and presented in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing a naive 2D convolution,\n    validating it against SciPy's implementation, and reporting\n    the comparison results for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[1, 2, 0, -1], [3, -1, 2, 0], [0, 1, -2, 3]], dtype=np.float64),\n         np.array([[1, 0], [0, -1]], dtype=np.float64)),\n        (np.array([[2, -1], [0, 4]], dtype=np.float64),\n         np.array([[-1, 2], [3, 0]], dtype=np.float64)),\n        (np.array([[0.5, -1.0, 2.0, 0.0, 1.5],\n                   [1.0, 3.0, -2.0, 4.0, -0.5],\n                   [0.0, -1.5, 2.5, -3.5, 0.0],\n                   [2.0, 0.0, 1.0, -1.0, 2.0]], dtype=np.float64),\n         np.array([[1.0, -1.0, 0.5], [0.0, 2.0, -0.5]], dtype=np.float64)),\n        (np.array([[1.0, -2.0, 3.0], [-1.0, 0.0, 2.0], [4.0, -3.0, 1.0]], dtype=np.float64),\n         np.array([[1.0]], dtype=np.float64)),\n        (np.array([[0.1, -0.2, 0.3],\n                   [1.5, 0.0, -0.5],\n                   [-0.7, 2.2, 1.1],\n                   [3.0, -1.0, 0.0],\n                   [0.0, 0.5, -2.0]], dtype=np.float64),\n         np.array([[0.5, -0.5], [1.0, 0.0], [-1.0, 0.5]], dtype=np.float64))\n    ]\n\n    def naive_convolution_2d(X, K):\n        \"\"\"\n        Computes the 2D \"valid\" convolution from first principles\n        as a sliding dot product.\n        \n        Args:\n            X (np.ndarray): The input matrix (image).\n            K (np.ndarray): The kernel matrix.\n            \n        Returns:\n            np.ndarray: The result of the convolution.\n        \"\"\"\n        H, W = X.shape\n        R, S = K.shape\n\n        # Dimensions of the output matrix for 'valid' convolution\n        H_out = H - R + 1\n        W_out = W - S + 1\n\n        # Handle cases where kernel is larger than image\n        if H_out <= 0 or W_out <= 0:\n            return np.array([[]], dtype=np.float64)\n\n        output = np.zeros((H_out, W_out))\n        \n        # Reverse the kernel across both axes\n        K_reversed = K[::-1, ::-1]\n\n        # Iterate over all possible top-left positions of the kernel on the image\n        for m in range(H_out):\n            for n in range(W_out):\n                # Extract the submatrix of X\n                sub_X = X[m : m + R, n : n + S]\n                \n                # Compute the dot product (sum of element-wise products)\n                # and assign it to the output matrix\n                output[m, n] = np.sum(sub_X * K_reversed)\n        \n        return output\n\n    results = []\n    # Set the numerical tolerance as specified in the problem\n    tolerance = 1e-12\n\n    for (X, K) in test_cases:\n        # 1. Compute the convolution using the naive implementation\n        result_naive = naive_convolution_2d(X, K)\n\n        # 2. Compute the convolution using the SciPy library function\n        result_scipy = convolve2d(X, K, mode='valid')\n        \n        # 3. Verify numerical equality within the specified tolerance\n        are_equal = np.allclose(result_naive, result_scipy, rtol=tolerance, atol=tolerance)\n        \n        # The Python bool 'True' will be capitalized, so we convert it to\n        # lowercase 'true' if needed, but the problem indicates the Python\n        # representation is fine.\n        results.append(str(are_equal))\n\n    # Final print statement in the exact required format.\n    # The map to str correctly handles Python's booleans (True, False).\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3180075"}, {"introduction": "Having established the mechanism of convolution, we now ask *why* this operation is so powerful and ubiquitous in deep learning. The answer lies in a transformative concept: parameter sharing. This exercise challenges you to quantify the impact of this principle by comparing a standard convolutional layer to a hypothetical \"locally connected\" counterpart that processes each image patch independently. By deriving and calculating the parameter counts for both, you will discover the staggering efficiency that weight sharing provides, which is the primary reason CNNs can scale to high-dimensional data like images without an explosion in model complexity. [@problem_id:3180099]", "problem": "A Convolutional Neural Network (CNN) layer applies a two-dimensional convolution by sliding a spatial kernel across an input feature map and reusing the same kernel weights at every spatial location. Consider an input tensor with spatial dimensions $H \\times W = 56 \\times 56$ and $C_{\\text{in}} = 64$ channels. You apply a convolutional layer with a square kernel of spatial size $k \\times k$ where $k=3$, stride $s=1$, zero padding $p=1$ (so the output spatial size is preserved), and $C_{\\text{out}} = 128$ output channels. Assume an additive bias is included for each output channel in the convolutional layer.\n\nNow define a hypothetical alternative layer that produces the same output shape by, at each output spatial location, taking the corresponding $k \\times k \\times C_{\\text{in}}$ input patch, flattening it, and applying a fully connected (affine) transformation to $C_{\\text{out}}$ outputs. Crucially, in this alternative layer there is no weight sharing across spatial locations: each spatial location has its own independent weights and its own independent biases, one bias per output channel per location.\n\nStarting only from the definition that convolution reuses the same kernel weights across all spatial locations while the alternative layer uses independent weights per location, derive the total number of trainable parameters in each case in terms of $k$, $C_{\\text{in}}$, $C_{\\text{out}}$, $H$, $W$, $p$, and $s$, and then specialize to the given numerical values. Compute the parameter savings factor $S$, defined as the ratio of the number of parameters in the no-sharing alternative to the number of parameters in the convolutional layer:\n$$\nS \\equiv \\frac{\\text{parameters in no-sharing alternative}}{\\text{parameters in convolution}}.\n$$\nGive the final value of $S$ for the stated configuration. The final answer must be a single real number. No rounding is required.", "solution": "The problem asks for a comparison of the number of trainable parameters in two types of neural network layers: a standard 2D convolutional layer and a hypothetical locally connected layer without weight sharing. We will derive the parameter count for each and then compute their ratio.\n\n### 1. Parameters in a Standard Convolutional Layer\n\nA standard convolutional layer reuses the same set of filters across all spatial locations. Let the number of input channels be $C_{\\text{in}}$, the number of output channels be $C_{\\text{out}}$, and the kernel size be $k \\times k$.\n\n-   **Weights:** To produce one output feature map, a single filter is convolved over the input. This filter must have a depth equal to the number of input channels, $C_{\\text{in}}$. Thus, its size is $k \\times k \\times C_{\\text{in}}$. Since we need to generate $C_{\\text{out}}$ output feature maps, we need $C_{\\text{out}}$ such filters. The total number of weights is the product of these terms:\n    $$ N_{\\text{weights, conv}} = k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} $$\n-   **Biases:** Typically, one bias term is added per output feature map. Since there are $C_{\\text{out}}$ output maps, there are $C_{\\text{out}}$ biases.\n    $$ N_{\\text{biases, conv}} = C_{\\text{out}} $$\n-   **Total Parameters:** The total number of trainable parameters, $N_{\\text{conv}}$, is the sum of weights and biases:\n    $$ N_{\\text{conv}} = (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}} $$\n\n### 2. Parameters in a Locally Connected Layer (No Sharing)\n\nThis layer applies an independent transformation at each spatial location of the output map. First, we determine the number of output locations. The output spatial dimensions, $H_{\\text{out}} \\times W_{\\text{out}}$, are given by the formula:\n$$ H_{\\text{out}} = \\frac{H - k + 2p}{s} + 1 \\quad \\text{and} \\quad W_{\\text{out}} = \\frac{W - k + 2p}{s} + 1 $$\nGiven $H=56, W=56, k=3, p=1, s=1$, the output dimensions are:\n$$ H_{\\text{out}} = \\frac{56 - 3 + 2(1)}{1} + 1 = 56 \\quad \\text{and} \\quad W_{\\text{out}} = \\frac{56 - 3 + 2(1)}{1} + 1 = 56 $$\nSo there are $56 \\times 56 = 3136$ output locations.\n\nAt each location, the transformation is from an input patch of size $k \\times k \\times C_{\\text{in}}$ to $C_{\\text{out}}$ output values. This is equivalent to a fully connected layer.\n\n-   **Parameters per location:** The number of weights for this local transformation is $(k \\times k \\times C_{\\text{in}}) \\times C_{\\text{out}}$, and the number of biases is $C_{\\text{out}}$.\n    $$ N_{\\text{per location}} = (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}} $$\n-   **Total Parameters:** Since there is no weight sharing, we multiply the parameters per location by the total number of output locations:\n    $$ N_{\\text{no-share}} = (H_{\\text{out}} \\cdot W_{\\text{out}}) \\times N_{\\text{per location}} $$\n    $$ N_{\\text{no-share}} = (H_{\\text{out}} \\cdot W_{\\text{out}}) \\cdot \\left( (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}} \\right) $$\n\n### 3. Parameter Savings Factor (S)\n\nThe savings factor $S$ is the ratio of the parameters in the no-sharing layer to the convolutional layer.\n$$ S = \\frac{N_{\\text{no-share}}}{N_{\\text{conv}}} = \\frac{(H_{\\text{out}} \\cdot W_{\\text{out}}) \\cdot \\left( (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}} \\right)}{(k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}}} $$\nThe term for parameters per location cancels out, leaving:\n$$ S = H_{\\text{out}} \\cdot W_{\\text{out}} $$\n\n### 4. Calculation\n\nUsing the calculated output dimensions:\n$$ S = 56 \\times 56 = 3136 $$\nThis shows that the convolutional layer is more than 3000 times more parameter-efficient than its locally connected counterpart for this configuration, highlighting the profound impact of parameter sharing.", "answer": "$$\n\\boxed{3136}\n$$", "id": "3180099"}, {"introduction": "We now understand *how* convolution works and *why* it is efficient, but what specific capabilities does it unlock? This practice tackles that question by exploring the critical role of kernel size. You will confront a specially designed \"checkerboard\" problem where the correct output for a given location depends on the values of its neighbors. Through a combination of theoretical analysis and practical implementation, you will prove that models limited to pointwise ($1 \\times 1$) operations are fundamentally incapable of solving this task, and then demonstrate how a simple kernel with a larger spatial receptive field ($2 \\times 2$) solves it perfectly. This provides a clear and powerful demonstration of why spatial kernels are the essential ingredient for detecting local patterns and textures. [@problem_id:3180076]", "problem": "You are given binary images and asked to study a target that depends on the coupling between neighboring pixels. The study must be conducted using the core definition of discrete two-dimensional convolution and basic probability of independent Bernoulli variables.\n\nBase definitions to use:\n- Discrete two-dimensional convolution of an input array $x$ with a finite kernel $k$ is defined by\n$$\n(y \\star k)[i,j] = \\sum_{u}\\sum_{v} x[i-u, j-v]\\;k[u,v],\n$$\nwith appropriate boundary conventions. Stacking convolutions corresponds to repeated applications of this linear operator, and stacking only convolutions of spatial size $1\\times 1$ with pointwise nonlinearities yields a pointwise function per spatial location.\n\nDataset construction:\n- For each image $x\\in\\{0,1\\}^{H\\times W}$, define a target $t\\in\\{0,1\\}^{(H-1)\\times (W-1)}$ on the upper-left anchored valid positions by the parity of each $2\\times 2$ block:\n$$\nt[i,j] = \\left( x[i,j] + x[i,j+1] + x[i+1,j] + x[i+1,j+1] \\right)\\bmod 2,\n$$\nfor all $i\\in\\{0,\\dots,H-2\\}$ and $j\\in\\{0,\\dots,W-2\\}$. Each input image should be drawn with entries independently and identically distributed as Bernoulli with parameter $p=\\tfrac{1}{2}$.\n\nModel families to compare:\n- Family A (pointwise-only): any architecture composed exclusively of $1\\times 1$ convolutions and pointwise nonlinearities. Such models implement a function of the form $f(x[i,j])$ at each spatial location without accessing neighbors. Under the stated data distribution, determine a provable tight upper bound (as a real number) on the achievable accuracy for Family A on the above target and report that bound.\n- Family B (neighborhood-aware): a construction using two-dimensional convolutions with spatial support that covers each $2\\times 2$ block and pointwise operations. Design such a construction that achieves perfect prediction of $t$ on the valid region and report its empirical accuracy computed over the dataset.\n\nAccuracy definition:\n- For a given dataset of $M$ images with shape $H\\times W$, aggregate the total number of correct predictions over all images and all valid positions $(i,j)$, and divide by the total number of valid positions across the dataset, which equals $M\\cdot (H-1)\\cdot (W-1)$. The result is a real number in $[0,1]$.\n\nYour task:\n- Implement a program that, for each test case below, constructs the dataset as described, computes the accuracy of Family B on the dataset, and also outputs the provable tight upper bound for Family A derived from first principles on this data distribution.\n- Important: The Family A result to output must be the provable bound (a real number), not an empirical estimate. The Family B result must be the empirical accuracy computed on the dataset.\n\nTest suite:\n- Use the following test cases, each specified as a tuple $(M,H,W,\\text{seed})$:\n    - Case $1$: $(M,H,W,\\text{seed})=(\\,$ $256$ $,$ $8$ $,$ $8$ $,$ $7$ $\\,)$.\n    - Case $2$: $(M,H,W,\\text{seed})=(\\,$ $64$ $,$ $2$ $,$ $2$ $,$ $13$ $\\,)$.\n    - Case $3$: $(M,H,W,\\text{seed})=(\\,$ $100$ $,$ $9$ $,$ $7$ $,$ $3$ $\\,)$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a two-element list $[b,a]$, where $b$ is the Family A provable upper bound and $a$ is the Family B empirical accuracy on the dataset. The final output must therefore be a single list of three two-element lists. Express all real numbers rounded to exactly three decimal places. For example:\n\"[[0.500,1.000],[0.500,1.000],[0.500,1.000]]\"", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in discrete mathematics and probability theory, directly relevant to the study of convolutional networks, and is well-posed, self-contained, and objective. We may therefore proceed with a solution.\n\nThe problem requires us to analyze the performance of two families of models on a binary image task. The target variable $t[i,j]$ for a position $(i,j)$ is the parity of the sum of pixels in the $2 \\times 2$ block starting at that position. The input pixels $x[i,j]$ are independent and identically distributed Bernoulli random variables with parameter $p=\\tfrac{1}{2}$.\n\n### Analysis of Model Family A (Pointwise-Only)\n\nModels in Family A are composed exclusively of $1 \\times 1$ convolutions and pointwise nonlinearities. A $1 \\times 1$ convolution computes an output at spatial location $(i,j)$ that is a function of only the input at the same location, $x[i,j]$. Stacking any number of such layers preserves this property. Consequently, any model in Family A implements a pointwise function, where the prediction for the target $t[i,j]$, denoted $\\hat{t}[i,j]$, is a function of only $x[i,j]$. Let this function be $f:\\{0,1\\} \\to \\{0,1\\}$, such that $\\hat{t}[i,j] = f(x[i,j])$.\n\nThe target is defined as:\n$$t[i,j] = \\left( x[i,j] + x[i,j+1] + x[i+1,j] + x[i+1,j+1] \\right) \\bmod 2$$\nWe can separate the dependence on $x[i,j]$ from its neighbors. Let $S_{i, j} = x[i,j+1] + x[i+1,j] + x[i+1,j+1]$. The target is then $t[i,j] = (x[i,j] + S_{i, j}) \\bmod 2$.\n\nThe input pixels $x[i',j']$ are independent Bernoulli random variables with parameter $p=\\tfrac{1}{2}$. That is, $P(x[i',j']=0) = P(x[i',j']=1) = \\tfrac{1}{2}$. The sum of any number of i.i.d. Bernoulli($\\tfrac{1}{2}$) variables, taken modulo $2$, results in a new Bernoulli($\\tfrac{1}{2}$) variable. Since $S_{i,j}$ is the sum of $3$ such variables, $S_{i,j} \\bmod 2$ is a random variable that takes values $0$ and $1$ with equal probability, i.e., $P(S_{i,j} \\bmod 2 = 0) = P(S_{i,j} \\bmod 2 = 1) = \\tfrac{1}{2}$.\n\nCrucially, the pixels composing $S_{i,j}$ are distinct from $x[i,j]$, so $S_{i,j}$ is statistically independent of $x[i,j]$. The model $f$ bases its prediction only on $x[i,j]$, which provides no information about the value of $S_{i, j} \\bmod 2$.\n\nThe accuracy of the model is the probability of a correct prediction, $P(\\hat{t}[i,j] = t[i,j])$. We can find the maximum possible accuracy by considering the optimal choices for $f(0)$ and $f(1)$.\n$$ Acc = P(f(x[i,j]) = (x[i,j] + S_{i,j}) \\bmod 2) $$\nBy the law of total probability, conditioning on the value of $x[i,j]$:\n$$ Acc = P(\\hat{t}=t | x[i,j]=0)P(x[i,j]=0) + P(\\hat{t}=t | x[i,j]=1)P(x[i,j]=1) $$\n$$ Acc = \\frac{1}{2} P(f(0) = (0 + S_{i,j}) \\bmod 2) + \\frac{1}{2} P(f(1) = (1 + S_{i,j}) \\bmod 2) $$\nThe first term is $P(f(0) = S_{i,j} \\bmod 2)$. Since $S_{i,j} \\bmod 2$ is a fair coin flip, regardless of whether $f(0)$ is chosen to be $0$ or $1$, the probability of matching is $\\tfrac{1}{2}$.\nThe second term is $P(f(1) = (1 + S_{i,j}) \\bmod 2)$. The variable $(1 + S_{i,j}) \\bmod 2$ is also a fair coin flip, so the probability of matching it is also $\\tfrac{1}{2}$, regardless of the choice of $f(1)$.\n\nThus, the maximum achievable accuracy is:\n$$ Acc_{max} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nThis constitutes a tight upper bound, as it is achieved by any constant predictor, for example $f(x)=0$ for all $x$, which has an accuracy of $P(t[i,j]=0) = \\tfrac{1}{2}$. The provable tight upper bound on accuracy for Family A is $0.5$.\n\n### Analysis of Model Family B (Neighborhood-Aware)\n\nModels in Family B are permitted to use two-dimensional convolutions with spatial support covering each $2 \\times 2$ block. We can construct a model that perfectly computes the target function.\n\nThe target function is $t[i,j] = \\left( \\sum_{u=0}^{1} \\sum_{v=0}^{1} x[i+u, j+v] \\right) \\bmod 2$.\nThis computation can be decomposed into two steps, both of which are permitted for Family B:\n1.  A linear spatial aggregation: Compute the sum $y[i,j] = \\sum_{u=0}^{1} \\sum_{v=0}^{1} x[i+u,j+v]$. This is a 2D cross-correlation with a $2 \\times 2$ kernel of all ones, $k = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$. Cross-correlation is a standard operation in deep learning libraries, often referred to as \"convolution,\" and fits the problem's description of a convolution with spatial support. This operation, with `'valid'` padding, produces an output of shape $(H-1) \\times (W-1)$.\n2.  A pointwise nonlinearity: Apply the function $g(z) = z \\bmod 2$ to each element of the intermediate feature map $y$.\n\nThe resulting model, $\\hat{t}[i,j] = g(y[i,j])$, is mathematically identical to the definition of the target $t[i,j]$. Therefore, this construction is a perfect predictor. When evaluated on any dataset generated according to the problem's specifications, the number of correct predictions will be equal to the total number of valid positions. The empirical accuracy will be exactly $1.0$.\n\nThe program below will implement this perfect model for Family B and compute its empirical accuracy, which is expected to be $1.0$. It will also report the derived theoretical bound of $0.5$ for Family A.", "answer": "```python\nimport numpy as np\nimport scipy.signal\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the provable bound for Family A and the\n    empirical accuracy for Family B for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # (M, H, W, seed)\n        (256, 8, 8, 7),\n        (64, 2, 2, 13),\n        (100, 9, 7, 3),\n    ]\n\n    all_results = []\n\n    # The provable tight upper bound for Family A is derived from first principles.\n    # The input pixel x[i,j] has zero mutual information with the target t[i,j].\n    # Since the target is a balanced binary variable (P(t=0)=P(t=1)=0.5),\n    # the best achievable accuracy for a model that only sees x[i,j] is 0.5.\n    family_a_bound = 0.5\n\n    for M, H, W, seed in test_cases:\n        # Set the random seed for reproducible dataset generation.\n        np.random.seed(seed)\n\n        # --- Dataset Construction ---\n        # Generate M images of size HxW with pixels i.i.d. as Bernoulli(0.5).\n        # Use a dtype that avoids overflow for the intermediate sum.\n        images = np.random.randint(0, 2, size=(M, H, W), dtype=np.int32)\n\n        # --- Target Construction ---\n        # t[i,j] = (x[i,j] + x[i,j+1] + x[i+1,j] + x[i+1,j+1]) mod 2\n        # This is computed for all images and all valid positions.\n        if H < 2 or W < 2:\n            targets = np.empty((M, 0, 0))\n        else:\n            targets = (images[:, :H-1, :W-1] +\n                       images[:, :H-1, 1:W] +\n                       images[:, 1:H, :W-1] +\n                       images[:, 1:H, 1:W]) % 2\n\n        # --- Family B: Empirical Accuracy Calculation ---\n        # A Family B model is constructed to perfectly compute the target.\n        # The model consists of a 2D convolution followed by a pointwise nonlinearity.\n        kernel = np.ones((2, 2), dtype=np.int32)\n        total_correct_predictions = 0\n\n        for m in range(M):\n            image = images[m, :, :]\n            \n            if image.shape[0] >= 2 and image.shape[1] >= 2:\n                # 1. Convolutional layer: scipy.signal.convolve2d with mode='valid'\n                # computes the sum over each 2x2 block. For a symmetric kernel,\n                # convolution and correlation are equivalent.\n                convolved_output = scipy.signal.convolve2d(image, kernel, mode='valid')\n                \n                # 2. Pointwise nonlinearity (modulo 2)\n                predictions = convolved_output % 2\n                \n                # Compare predictions with the pre-computed target for this image\n                target_m = targets[m, :, :]\n                total_correct_predictions += np.sum(predictions == target_m)\n\n        # 3. Compute empirical accuracy\n        if H > 1 and W > 1:\n            total_valid_positions = M * (H - 1) * (W - 1)\n            family_b_accuracy = total_correct_predictions / total_valid_positions\n        else:\n            # No valid positions, so accuracy is vacuously 1.0 (no errors made).\n            family_b_accuracy = 1.0\n\n        all_results.append([family_a_bound, family_b_accuracy])\n\n    # --- Format the Final Output ---\n    # The format is a string representing a list of lists, with numbers\n    # rounded to three decimal places.\n    # Example: \"[[0.500,1.000],[0.500,1.000],[0.500,1.000]]\"\n    result_strings = [f\"[{b:.3f},{a:.3f}]\" for b, a in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3180076"}]}