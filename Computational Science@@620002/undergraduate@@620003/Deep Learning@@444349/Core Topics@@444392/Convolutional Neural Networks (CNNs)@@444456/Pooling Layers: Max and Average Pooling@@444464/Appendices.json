{"hands_on_practices": [{"introduction": "To build a robust understanding of pooling layers, one must master their fundamental mechanics, including how they interact with padding and strides. This practice problem focuses on these crucial details by having you trace the output of max and average pooling on a carefully chosen input vector. By comparing the results from different padding schemes—zero padding versus reflect padding—you will gain a concrete appreciation for how boundary conditions can significantly influence a network's feature extraction process, especially when presented with challenging or \"adversarial\" inputs. [@problem_id:3163871]", "problem": "You are given a one-dimensional (1D) feature map intentionally crafted to exploit boundary effects under different padding schemes. Consider the input vector $x = [-100, -50, 3, 2]$ of length $n = 4$. A pooling layer with kernel size $k = 3$, stride $s = 2$, and symmetric padding of $p = 1$ on both sides is applied. Two padding schemes are compared:\n\n- Zero padding: For any index $j$ outside the valid range $\\{0, 1, \\dots, n-1\\}$, the padded signal value is defined as $\\tilde{x}_j = 0$.\n- Reflect padding (no edge duplication): For any index $j$ outside the valid range and given $p = 1$, the only out-of-range indices that can occur in the pooling windows are $j = -1$ on the left and $j = n$ on the right. Define the reflected values as $\\tilde{x}_{-1} = x_1$ and $\\tilde{x}_{n} = x_{n-2}$. For in-range indices, $\\tilde{x}_j = x_j$.\n\nUse only the core definitions of max pooling and average pooling over a sliding window of size $k$ and stride $s$ on the padded signal to determine the pooled outputs under zero padding and reflect padding. Then, decide which of the following statements are correct about the outputs for this adversarial input:\n\nA. Under max pooling, the first output under zero padding is larger than under reflect padding.\n\nB. Under average pooling, the first output under reflect padding is greater than under zero padding.\n\nC. For both pooling types, the second output is identical under zero padding and reflect padding.\n\nD. The magnitude of the difference between the first outputs (zero padding minus reflect padding) is larger for average pooling than for max pooling.\n\nSelect all that apply. Provide no computations in your selection; base your choice on principled reasoning from the definitions of pooling and padding.", "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following information:\n- Input vector: $x = [-100, -50, 3, 2]$\n- Input vector length: $n = 4$\n- Kernel size: $k = 3$\n- Stride: $s = 2$\n- Symmetric padding: $p = 1$\n- Zero padding definition: For an index $j$ outside the valid range $\\{0, 1, \\dots, n-1\\}$, the padded signal value is $\\tilde{x}_j = 0$.\n- Reflect padding definition: For an index $j$ outside the valid range, with $p=1$, the padded values are defined as $\\tilde{x}_{-1} = x_1$ and $\\tilde{x}_{n} = x_{n-2}$. For in-range indices, $\\tilde{x}_j = x_j$.\n- Question: Evaluate the correctness of four statements (A, B, C, D) comparing the pooling outputs under the two padding schemes.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is subjected to validation against established criteria.\n- **Scientifically Grounded:** The problem is well-grounded in the field of deep learning, specifically concerning the mechanics of convolutional neural networks. Max pooling, average pooling, and padding are standard, fundamental operations. The \"reflect padding\" described is a specific, non-standard variant, but its definition is mathematically precise and unambiguous ($\\tilde{x}_{-1} = x_1$, $\\tilde{x}_{n} = x_{n-2}$), making it a valid construct within the confines of the problem. It does not violate any mathematical or scientific laws.\n- **Well-Posed:** All parameters required to compute the output are provided: the input vector $x$, its length $n$, the kernel size $k$, the stride $s$, and the padding size $p$. The rules for both padding schemes are explicitly defined. This setup ensures that a unique and stable solution can be determined through direct computation.\n- **Objective:** The problem is phrased in objective, mathematical language. It requires the calculation and comparison of quantities based on fixed definitions. The term \"adversarial input\" is descriptive of the input's nature but does not introduce subjectivity into the required computations.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is self-contained, scientifically sound, and well-posed. The derivation of the solution can proceed.\n\n### Solution Derivation\n\nThe output size of the pooling operation is given by the formula $L_{out} = \\lfloor \\frac{n + 2p - k}{s} \\rfloor + 1$.\nSubstituting the given values:\n$$L_{out} = \\left\\lfloor \\frac{4 + 2(1) - 3}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{3}{2} \\right\\rfloor + 1 = 1 + 1 = 2$$\nThus, the output vector for each case will have a length of $2$. Let the two output values be denoted $y_0$ and $y_1$.\n\nThe pooling operation applies a sliding window of size $k=3$ with a stride of $s=2$ to the padded input. With a padding of $p=1$, the padded signal effectively ranges from index $-1$ to $n-1+p = 4$.\n- The first window (for $y_0$) starts at index $-p = -1$ and covers indices $\\{-1, 0, 1\\}$.\n- The second window (for $y_1$) starts at index $-p+s = -1+2 = 1$ and covers indices $\\{1, 2, 3\\}$.\n\nThe input vector is $x = [x_0, x_1, x_2, x_3] = [-100, -50, 3, 2]$.\n\n#### 1. Zero Padding\n\nUnder zero padding, any access outside the original input range $\\{0, 1, 2, 3\\}$ yields a value of $0$.\n\n- **First Window (for $y_0$):**\nThe window covers the padded values $\\{\\tilde{x}_{-1}, \\tilde{x}_0, \\tilde{x}_1\\}$. With zero padding, these values are $\\{0, x_0, x_1\\} = \\{0, -100, -50\\}$.\n- Max pooling: $y_{0, \\text{zero}}^{\\text{max}} = \\max(0, -100, -50) = 0$.\n- Average pooling: $y_{0, \\text{zero}}^{\\text{avg}} = \\frac{0 + (-100) + (-50)}{3} = \\frac{-150}{3} = -50$.\n\n- **Second Window (for $y_1$):**\nThe window covers the padded values $\\{\\tilde{x}_1, \\tilde{x}_2, \\tilde{x}_3\\}$. These indices are all within the original input's range, so the values are $\\{x_1, x_2, x_3\\} = \\{-50, 3, 2\\}$.\n- Max pooling: $y_{1, \\text{zero}}^{\\text{max}} = \\max(-50, 3, 2) = 3$.\n- Average pooling: $y_{1, \\text{zero}}^{\\text{avg}} = \\frac{-50 + 3 + 2}{3} = \\frac{-45}{3} = -15$.\n\n#### 2. Reflect Padding\n\nUnder the specified reflect padding, $\\tilde{x}_{-1} = x_1$ and $\\tilde{x}_4 = x_2$.\n\n- **First Window (for $y_0$):**\nThe window covers the padded values $\\{\\tilde{x}_{-1}, \\tilde{x}_0, \\tilde{x}_1\\}$. With reflect padding, these values are $\\{x_1, x_0, x_1\\} = \\{-50, -100, -50\\}$.\n- Max pooling: $y_{0, \\text{reflect}}^{\\text{max}} = \\max(-50, -100, -50) = -50$.\n- Average pooling: $y_{0, \\text{reflect}}^{\\text{avg}} = \\frac{-50 + (-100) + (-50)}{3} = \\frac{-200}{3}$.\n\n- **Second Window (for $y_1$):**\nThe window covers the values $\\{\\tilde{x}_1, \\tilde{x}_2, \\tilde{x}_3\\}$. These indices are all within the original input's range, so the values are $\\{x_1, x_2, x_3\\} = \\{-50, 3, 2\\}$. The padding scheme has no effect on this window.\n- Max pooling: $y_{1, \\text{reflect}}^{\\text{max}} = \\max(-50, 3, 2) = 3$.\n- Average pooling: $y_{1, \\text{reflect}}^{\\text{avg}} = \\frac{-50 + 3 + 2}{3} = \\frac{-45}{3} = -15$.\n\n### Option-by-Option Analysis\n\n**A. Under max pooling, the first output under zero padding is larger than under reflect padding.**\n- From our calculations, the first max pooling output under zero padding is $y_{0, \\text{zero}}^{\\text{max}} = 0$.\n- The first max pooling output under reflect padding is $y_{0, \\text{reflect}}^{\\text{max}} = -50$.\n- Comparing the two values: $0  -50$.\n- The statement is **Correct**.\n\n**B. Under average pooling, the first output under reflect padding is greater than under zero padding.**\n- From our calculations, the first average pooling output under zero padding is $y_{0, \\text{zero}}^{\\text{avg}} = -50$.\n- The first average pooling output under reflect padding is $y_{0, \\text{reflect}}^{\\text{avg}} = -\\frac{200}{3} \\approx -66.67$.\n- Comparing the two values: $-50  -\\frac{200}{3}$. Thus, the output under zero padding is greater.\n- The statement claims the opposite.\n- The statement is **Incorrect**.\n\n**C. For both pooling types, the second output is identical under zero padding and reflect padding.**\n- The second pooling window covers indices $\\{1, 2, 3\\}$. These indices correspond to elements $x_1, x_2, x_3$ of the original input vector.\n- Since this window does not include any padded elements, its contents are independent of the padding scheme.\n- The values for the window are $\\{x_1, x_2, x_3\\} = \\{-50, 3, 2\\}$ for both zero and reflect padding.\n- Consequently, applying any deterministic function, such as max or average pooling, to this identical set of values must yield an identical result.\n- Our calculations confirm this:\n  - $y_{1, \\text{zero}}^{\\text{max}} = 3$ and $y_{1, \\text{reflect}}^{\\text{max}} = 3$.\n  - $y_{1, \\text{zero}}^{\\text{avg}} = -15$ and $y_{1, \\text{reflect}}^{\\text{avg}} = -15$.\n- The statement is **Correct**.\n\n**D. The magnitude of the difference between the first outputs (zero padding minus reflect padding) is larger for average pooling than for max pooling.**\n- Let the difference for max pooling be $\\Delta_{\\text{max}} = y_{0, \\text{zero}}^{\\text{max}} - y_{0, \\text{reflect}}^{\\text{max}} = 0 - (-50) = 50$. The magnitude is $|\\Delta_{\\text{max}}| = 50$.\n- Let the difference for average pooling be $\\Delta_{\\text{avg}} = y_{0, \\text{zero}}^{\\text{avg}} - y_{0, \\text{reflect}}^{\\text{avg}} = -50 - (-\\frac{200}{3}) = -50 + \\frac{200}{3} = \\frac{-150 + 200}{3} = \\frac{50}{3}$. The magnitude is $|\\Delta_{\\text{avg}}| = \\frac{50}{3}$.\n- Comparing the magnitudes: $|\\Delta_{\\text{max}}| = 50$ and $|\\Delta_{\\text{avg}}| = \\frac{50}{3} \\approx 16.67$.\n- Clearly, $50  \\frac{50}{3}$. The magnitude of the difference is larger for max pooling.\n- The statement claims the opposite.\n- The statement is **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3163871"}, {"introduction": "Beyond the forward pass, a deep understanding of pooling requires analyzing its effect on the backward pass of gradients. Max pooling is known for creating sparse gradients, a property that has significant implications for training dynamics. This exercise moves from deterministic computation to probabilistic analysis, asking you to calculate the likelihood that any single input activation receives the gradient from a max pooling layer. By treating the inputs as random variables, you will uncover the fundamental statistical reason for this sparse gradient flow, connecting the mechanics of backpropagation to core principles of probability. [@problem_id:3163876]", "problem": "Consider a max pooling operation applied to a single window of size $3 \\times 3$ in a deep neural network. Let the pre-pooling activations in this window be random variables $X_{1}, X_{2}, \\ldots, X_{9}$ that are independent and identically distributed (i.i.d., independent and identically distributed) from a continuous distribution with probability density function $f$ and cumulative distribution function $F$. In the forward pass, max pooling outputs $\\max\\{X_{1}, X_{2}, \\ldots, X_{9}\\}$. In the backward pass under standard max pooling, an incoming scalar gradient $g$ is routed entirely to the unique index attaining the maximum (i.e., the argmax), and all other positions receive $0$. If there is a tie for the maximum, a deterministic tie-breaking rule is used, but you may assume ties occur with probability $0$ under the stated assumptions.\n\nUsing only the base principles of probability for independent and identically distributed continuous random variables and the definition of max pooling and its backward pass, compute the probability that a specific position, say the first coordinate $X_{1}$, receives a nonzero gradient during backpropagation. Express your final answer as an exact value. No rounding is required, and no physical units are involved.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. It provides a self-contained and consistent setup for a solvable problem in probability theory as applied to deep learning.\n\nThe problem asks for the probability that a specific input activation, say $X_1$, receives a nonzero gradient during the backward pass of a max pooling operation. The pooling window has a size of $3 \\times 3$, which means it covers $9$ input activations. These activations are modeled as independent and identically distributed (i.i.d.) continuous random variables, denoted $X_1, X_2, \\ldots, X_9$.\n\nAccording to the problem description, the backward pass for max pooling routes the entire incoming gradient to the unique activation that had the maximum value in the forward pass. All other activations in the window receive a gradient of $0$. Therefore, for position $X_1$ to receive a nonzero gradient, its value must be the maximum among all $9$ values in the window.\n\nLet the set of random variables be $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_9\\}$. The event of interest, which we shall call $E_1$, is that $X_1$ is the maximum value in this set. Mathematically, this is the event:\n$$ E_1 = \\{X_1 = \\max(X_1, X_2, \\ldots, X_9)\\} $$\nWe are asked to compute the probability of this event, $P(E_1)$.\n\nThe most direct way to solve this is by leveraging the statistical properties of the variables. The variables $X_1, X_2, \\ldots, X_9$ are stated to be independent and identically distributed (i.i.d.). This property implies a fundamental symmetry: each variable has the same probability of being the largest, the second largest, or any other rank in the ordered sequence of the variables.\n\nLet $E_i$ be the event that the variable $X_i$ is the maximum value in the set $\\mathcal{X}$, for $i \\in \\{1, 2, \\ldots, 9\\}$. Due to the i.i.d. nature of the random variables, the joint probability distribution of $(X_1, \\ldots, X_9)$ is symmetric with respect to the permutation of its arguments. Consequently, the probability of any specific variable being the maximum is the same for all variables:\n$$ P(E_1) = P(E_2) = \\cdots = P(E_9) $$\nLet's denote this common probability by $p$.\n\nThe problem states that the variables are drawn from a continuous distribution. For continuous random variables, the probability of a tie between any two of them is $0$. That is, $P(X_i = X_j) = 0$ for $i \\neq j$. This means that with probability $1$, there will be a unique maximum value among the $9$ variables.\n\nThe events $E_1, E_2, \\ldots, E_9$ are mutually exclusive (as there can be only one unique maximum). The union of these events, $\\bigcup_{i=1}^9 E_i$, represents the certain event that one of the variables is the maximum. Therefore, the sum of their probabilities must be $1$:\n$$ P\\left(\\bigcup_{i=1}^9 E_i\\right) = \\sum_{i=1}^9 P(E_i) = 1 $$\nSubstituting the common probability $p$ into this sum gives:\n$$ \\sum_{i=1}^9 p = 9p = 1 $$\nSolving for $p$, we find the probability that any specific variable, such as $X_1$, is the maximum:\n$$ p = P(E_1) = \\frac{1}{9} $$\n\nThis result can be rigorously confirmed through direct integration. The event $E_1$ is equivalent to the joint event $\\{X_1  X_2, X_1  X_3, \\ldots, X_1  X_9\\}$. The probability is calculated by conditioning on the value of $X_1$ and integrating over its entire range. Let $f(x)$ be the probability density function (PDF) and $F(x)$ be the cumulative distribution function (CDF) of the variables.\n$$ P(E_1) = \\int_{-\\infty}^{\\infty} P(X_1  X_2, \\ldots, X_1  X_9 | X_1 = x) f(x) \\,dx $$\nGiven $X_1 = x$, and because the variables are independent, the conditional probability becomes:\n$$ P(x  X_2, \\ldots, x  X_9) = \\prod_{i=2}^9 P(X_i  x) $$\nSince the variables are identically distributed, $P(X_i  x) = F(x)$ for all $i \\in \\{2, \\ldots, 9\\}$. There are $8$ such terms in the product:\n$$ \\prod_{i=2}^9 P(X_i  x) = [F(x)]^8 $$\nSubstituting this back into the integral, we get:\n$$ P(E_1) = \\int_{-\\infty}^{\\infty} [F(x)]^8 f(x) \\,dx $$\nWe can solve this integral using a change of variables. Let $u = F(x)$. Then, the differential is $du = F'(x)dx = f(x)dx$. The limits of integration for $u$ change from $F(-\\infty) = 0$ to $F(\\infty) = 1$. The integral transforms to:\n$$ P(E_1) = \\int_{0}^{1} u^8 \\,du $$\nEvaluating this elementary integral gives:\n$$ P(E_1) = \\left[ \\frac{u^9}{9} \\right]_{0}^{1} = \\frac{1^9}{9} - \\frac{0^9}{9} = \\frac{1}{9} $$\nBoth methods yield the same result. The probability that the specific position $X_1$ receives a nonzero gradient is $\\frac{1}{9}$.", "answer": "$$\\boxed{\\frac{1}{9}}$$", "id": "3163876"}, {"introduction": "While max and average pooling are often presented as distinct operations, they can be viewed as two endpoints of a continuous spectrum. The Log-Sum-Exp (LSE) function provides a powerful mathematical bridge between them, acting as a \"soft\" or differentiable approximation of the max function. In this advanced exercise, you will explore the properties of LSE pooling, proving how it gracefully transitions to max pooling at low \"temperatures\" ($\\tau \\to 0$) and relates to average pooling at high temperatures ($\\tau \\to \\infty$). Deriving its gradient will also reveal a deep connection to the softmax function, providing a unified perspective on these fundamental operations in deep learning. [@problem_id:3163845]", "problem": "Consider a vector $x \\in \\mathbb{R}^{n}$ with components $\\{x_{i}\\}_{i=1}^{n}$, and define Log-Sum-Exp (LSE) pooling by\n$$\nP_{\\tau}(x) \\;=\\; \\tau \\,\\log\\!\\left(\\sum_{i=1}^{n} \\exp\\!\\left(\\frac{x_{i}}{\\tau}\\right)\\right),\n$$\nfor any temperature parameter $\\tau  0$. Max pooling outputs $\\max_{i} x_{i}$, and average pooling outputs $\\frac{1}{n}\\sum_{i=1}^{n} x_{i}$. Using only the definitions of the exponential and logarithm functions, their basic algebraic properties (e.g., $\\log(ab)=\\log a + \\log b$), and standard first-order asymptotics for large and small arguments (e.g., $\\exp(u) = 1 + u + o(u)$ as $u \\to 0$ and $\\log(1+u) = u + o(u)$ as $u \\to 0$), do the following:\n\n1. Starting from the definition of $P_{\\tau}(x)$, derive the limit of $P_{\\tau}(x)$ as $\\tau \\to 0$ and explain why it coincides with max pooling. Your derivation must be justified without invoking any unproven shortcut identities.\n\n2. Show that $P_{\\tau}(x)$ approaches average pooling up to an additive scaling term as $\\tau \\to \\infty$ by computing the limit of $P_{\\tau}(x) - \\tau \\log n$ as $\\tau \\to \\infty$. Your argument must begin from the given definitions and well-tested asymptotic expansions.\n\n3. Derive the gradient of $P_{\\tau}(x)$ with respect to a single component $x_{j}$, for a fixed index $j \\in \\{1,\\dots,n\\}$, starting from the provided definition of $P_{\\tau}(x)$ and the chain rule. Express the result in closed form as a function of $x$ and $\\tau$.\n\nReport the three requested results as analytic expressions. No rounding is required. The final answer must contain exactly these three expressions.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. All necessary definitions and conditions are provided, and there are no contradictions or ambiguities. The tasks are standard exercises in calculus and limit theory applied to a function relevant in machine learning. We will proceed with the solution.\n\nThe Log-Sum-Exp function is defined as $P_{\\tau}(x) = \\tau \\log\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)$. For the purposes of calculus, the function $\\log$ is interpreted as the natural logarithm, which we will denote as $\\ln$.\n\n1. Derivation of the limit as $\\tau \\to 0^+$ (Max Pooling)\n\nLet $x_{\\max} = \\max_{i \\in \\{1, \\dots, n\\}} x_i$. We analyze the limit of $P_{\\tau}(x)$ as $\\tau \\to 0^+$. To manage the exponential terms where the argument approaches infinity, we factor out the term corresponding to the maximum value, $\\exp(x_{\\max}/\\tau)$.\n\n$$\nP_{\\tau}(x) = \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\n\nWe rewrite the sum by factoring out $\\exp(x_{\\max}/\\tau)$:\n$$\nP_{\\tau}(x) = \\tau \\ln\\left( \\exp\\left(\\frac{x_{\\max}}{\\tau}\\right) \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right) \\right)\n$$\n\nUsing the property $\\ln(ab) = \\ln(a) + \\ln(b)$, we get:\n$$\nP_{\\tau}(x) = \\tau \\left[ \\ln\\left(\\exp\\left(\\frac{x_{\\max}}{\\tau}\\right)\\right) + \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)\\right) \\right]\n$$\n\nSince $\\ln(\\exp(u)) = u$, this simplifies to:\n$$\nP_{\\tau}(x) = \\tau \\left[ \\frac{x_{\\max}}{\\tau} + \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)\\right) \\right]\n$$\n$$\nP_{\\tau}(x) = x_{\\max} + \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)\\right)\n$$\n\nNow, we examine the sum in the argument of the logarithm as $\\tau \\to 0^+$. For each term $\\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)$, the exponent's numerator $(x_i - x_{\\max})$ is non-positive.\n- If $x_i  x_{\\max}$, then $x_i - x_{\\max}  0$. As $\\tau \\to 0^+$, the argument $\\frac{x_i - x_{\\max}}{\\tau} \\to -\\infty$, and therefore $\\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right) \\to 0$.\n- If $x_i = x_{\\max}$, then $x_i - x_{\\max} = 0$. The argument is $\\frac{0}{\\tau} = 0$, and $\\exp(0) = 1$.\n\nLet $k$ be the number of indices $i$ for which $x_i = x_{\\max}$. Then, as $\\tau \\to 0^+$, the sum approaches the count of these maximal elements:\n$$\n\\lim_{\\tau \\to 0^+} \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right) = k\n$$\nwhere $k \\ge 1$. Substituting this into our expression for $P_{\\tau}(x)$:\n$$\n\\lim_{\\tau \\to 0^+} P_{\\tau}(x) = \\lim_{\\tau \\to 0^+} \\left[ x_{\\max} + \\tau \\ln(k) \\right]\n$$\nAs $\\tau \\to 0^+$, the term $\\tau \\ln(k)$ approaches $0$, since $\\ln(k)$ is a constant.\n$$\n\\lim_{\\tau \\to 0^+} P_{\\tau}(x) = x_{\\max} + 0 = x_{\\max}\n$$\nThis result, $x_{\\max} = \\max_{i} x_i$, is the definition of max pooling.\n\n2. Derivation of the limit as $\\tau \\to \\infty$ (Average Pooling)\n\nWe are asked to compute the limit of $P_{\\tau}(x) - \\tau \\ln n$ as $\\tau \\to \\infty$.\n$$\nP_{\\tau}(x) - \\tau \\ln n = \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right) - \\tau \\ln n\n$$\nUsing the property $\\ln(a) - \\ln(b) = \\ln(a/b)$, we combine the terms:\n$$\nP_{\\tau}(x) - \\tau \\ln n = \\tau \\ln\\left(\\frac{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)}{n}\\right) = \\tau \\ln\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\nAs $\\tau \\to \\infty$, the argument of each exponential function, $u_i = x_i/\\tau$, approaches $0$. We can therefore use the specified first-order asymptotic expansion $\\exp(u) = 1 + u + o(u)$ for $u \\to 0$.\n$$\n\\exp\\left(\\frac{x_{i}}{\\tau}\\right) = 1 + \\frac{x_i}{\\tau} + o\\left(\\frac{1}{\\tau}\\right)\n$$\nSubstituting this into the sum inside the logarithm:\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(1 + \\frac{x_i}{\\tau} + o\\left(\\frac{1}{\\tau}\\right)\\right) = \\frac{1}{n} \\left(\\sum_{i=1}^{n} 1 + \\frac{1}{\\tau}\\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} o\\left(\\frac{1}{\\tau}\\right)\\right)\n$$\n$$\n= \\frac{1}{n} \\left(n + \\frac{1}{\\tau}\\sum_{i=1}^{n} x_i + n \\cdot o\\left(\\frac{1}{\\tau}\\right)\\right) = 1 + \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\n$$\nNow, substitute this back into the full expression. Let $v = \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)$. As $\\tau \\to \\infty$, $v \\to 0$. We can use the expansion $\\ln(1+v) = v + o(v)$.\n$$\n\\lim_{\\tau \\to \\infty} \\left[ P_{\\tau}(x) - \\tau \\ln n \\right] = \\lim_{\\tau \\to \\infty} \\tau \\ln\\left(1 + \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\\right)\n$$\n$$\n= \\lim_{\\tau \\to \\infty} \\tau \\left[ \\left(\\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\\right) + o\\left(\\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\\right) \\right]\n$$\nThe term inside the brackets simplifies to $\\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)$.\n$$\n= \\lim_{\\tau \\to \\infty} \\tau \\left[ \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right) \\right] = \\lim_{\\tau \\to \\infty} \\left[ \\frac{1}{n}\\sum_{i=1}^{n} x_i + \\tau \\cdot o\\left(\\frac{1}{\\tau}\\right) \\right]\n$$\nThe term $\\tau \\cdot o(1/\\tau)$ represents a quantity that goes to zero faster than $1/\\tau$, so multiplying by $\\tau$ still results in a term that vanishes as $\\tau \\to \\infty$. For example, if $o(1/\\tau)$ is of the order $1/\\tau^2$, $\\tau(1/\\tau^2) = 1/\\tau \\to 0$. Thus, $\\lim_{\\tau \\to \\infty} \\tau \\cdot o(1/\\tau) = 0$.\nThe limit is therefore:\n$$\n\\lim_{\\tau \\to \\infty} \\left[ P_{\\tau}(x) - \\tau \\ln n \\right] = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n$$\nThis is the definition of average pooling. Thus, for large $\\tau$, $P_{\\tau}(x)$ approximates average pooling up to an additive term $\\tau \\ln n$.\n\n3. Derivation of the Gradient $\\frac{\\partial P_{\\tau}(x)}{\\partial x_j}$\n\nWe compute the partial derivative of $P_{\\tau}(x)$ with respect to a component $x_j$, for a fixed index $j \\in \\{1, \\dots, n\\}$. We use the chain rule.\n$$\nP_{\\tau}(x) = \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\nLet $u(x) = \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)$. Then $P_{\\tau}(x) = \\tau \\ln(u(x))$.\nBy the chain rule, $\\frac{\\partial P_{\\tau}(x)}{\\partial x_j} = \\frac{d}{du}(\\tau \\ln u) \\cdot \\frac{\\partial u}{\\partial x_j}$.\n\nFirst, we find the derivative of the outer function with respect to its argument $u$:\n$$\n\\frac{d}{du}(\\tau \\ln u) = \\frac{\\tau}{u} = \\frac{\\tau}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)}\n$$\nNext, we find the partial derivative of the inner function $u(x)$ with respect to $x_j$:\n$$\n\\frac{\\partial u}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\nThe derivative of a sum is the sum of the derivatives. For any term with index $i \\neq j$, $\\frac{\\partial}{\\partial x_j}\\exp\\left(\\frac{x_{i}}{\\tau}\\right) = 0$. The only non-zero term in the sum is for $i=j$.\n$$\n\\frac{\\partial u}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\exp\\left(\\frac{x_{j}}{\\tau}\\right) = \\exp\\left(\\frac{x_{j}}{\\tau}\\right) \\cdot \\frac{1}{\\tau}\n$$\nCombining these results:\n$$\n\\frac{\\partial P_{\\tau}(x)}{\\partial x_j} = \\left( \\frac{\\tau}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)} \\right) \\cdot \\left( \\exp\\left(\\frac{x_{j}}{\\tau}\\right) \\cdot \\frac{1}{\\tau} \\right)\n$$\nThe factor $\\tau$ in the numerator cancels with the factor $1/\\tau$.\n$$\n\\frac{\\partial P_{\\tau}(x)}{\\partial x_j} = \\frac{\\exp\\left(\\frac{x_{j}}{\\tau}\\right)}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)}\n$$\nThis expression is the softmax function applied to the vector $x/\\tau$.\nThe three requested results are:\n1. $\\max_{i} x_i$\n2. $\\frac{1}{n} \\sum_{i=1}^{n} x_i$\n3. $\\frac{\\exp(x_j/\\tau)}{\\sum_{i=1}^{n} \\exp(x_i/\\tau)}$", "answer": "$$\n\\boxed{\n\\pmatrix{\n\\max_{i} x_i\n\n\\frac{1}{n}\\sum_{i=1}^{n} x_{i}\n\n\\frac{\\exp\\left(\\frac{x_j}{\\tau}\\right)}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_i}{\\tau}\\right)}\n}\n}\n$$", "id": "3163845"}]}