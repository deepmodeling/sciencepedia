{"hands_on_practices": [{"introduction": "A network's receptive field determines its ability to integrate contextual information. This exercise challenges you to derive the receptive field size for a dense block, providing a fundamental understanding of how its unique connectivity pattern influences information flow [@problem_id:3114064]. By comparing this to an equivalent residual network, you will uncover surprising similarities and gain a clearer perspective on architectural design principles.", "problem": "Consider a dense block as in Densely Connected Convolutional Networks (DenseNet) within a Convolutional Neural Network (CNN). The block consists of $L$ sequential two-dimensional convolutional layers, each with a $3 \\times 3$ kernel, stride $1$, zero-padding of $1$ on all sides (so spatial resolution is preserved), no pooling, no dilation, and pointwise nonlinearities that do not change spatial dependency. Each layer takes as input the channelwise concatenation of the original block input and all preceding layer outputs in the block. The block output is the channelwise concatenation of all layer outputs (ignore any compression or transition layers after the block). For any layer output, define its receptive field side length as the number of input pixels along a single spatial axis of the original block input that can influence a single output pixel at that layer. Define the block receptive field side length as the maximum receptive field side length among all its output channels.\n\nStarting from the definitions of discrete convolution and receptive field propagation, derive the closed-form expression for the block receptive field side length at the end of this dense block as a function of $L$. Then, consider an “equivalent-depth” residual stack composed of $L$ sequential $3 \\times 3$ stride-$1$ convolutional layers, also with zero-padding of $1$, no pooling, and identity skip connections arranged to form standard residual pathways but without altering spatial dimensions. Using the same first-principles reasoning, derive the block receptive field side length at the end of this residual stack and compare it with that of the dense block.\n\nReport only the single common closed-form expression for the receptive field side length of the block output (measured along one spatial axis) in terms of $L$. Your final answer must be a single analytic expression. No rounding is required.", "solution": "The problem asks for a single common closed-form expression for the receptive field side length of a dense block and a residual stack, each composed of $L$ layers. We will derive this by analyzing the receptive field propagation in each architecture based on first principles.\n\nThe fundamental recurrence relation for the side length of a receptive field, $RF$, when applying a convolutional layer to an input feature map is given by:\n$$RF_{out} = RF_{in} + (k - 1) \\times S_{in}$$\nwhere $RF_{in}$ is the receptive field side length of the input feature map with respect to the original network input, $k$ is the kernel side length of the current layer, and $S_{in}$ is the product of strides of all preceding layers from the original network input to the current layer's input. The receptive field of a single pixel in the original input is defined as $RF_{original} = 1$.\n\nIn this problem, for every convolutional layer, the kernel size is $k=3$ and the stride is $s=1$. The product of strides $S_{in}$ is always $1$ because all strides are $1$. Thus, the propagation formula for a single convolutional step simplifies to:\n$$RF_{out} = RF_{in} + (3 - 1) \\times 1 = RF_{in} + 2$$\nThis formula indicates that each $3 \\times 3$ convolution with stride $1$ increases the receptive field side length by $2$.\n\nLet's now analyze each architecture. We will denote the original input to the block as $x_0$.\n\n**1. Analysis of the Dense Block (DenseNet)**\n\nIn a dense block, the input to layer $l$ (where $l \\in \\{1, 2, \\dots, L\\}$) is the channel-wise concatenation of the original block input $x_0$ and the outputs of all preceding layers, $y_1, y_2, \\dots, y_{l-1}$. The output of layer $l$ is $y_l$.\nLet $RF(Z)$ denote the receptive field side length of the tensor $Z$ with respect to the original input $x_0$. We define $RF_l \\equiv RF(y_l)$. A single output pixel in $y_l$ is influenced by a $3 \\times 3$ region of its composite input, $\\text{concat}(x_0, y_1, \\dots, y_{l-1})$. The total receptive field $RF_l$ is the union of the receptive fields from all input paths. The size of this union is determined by the path that yields the largest receptive field.\n\nFor layer $l=1$:\nThe input is $x_0$. The receptive field of the input pixels is $RF(x_0) = 1$.\n$$RF_1 = RF(x_0) + (3-1) = 1 + 2 = 3$$\n\nFor layer $l=2$:\nThe input is $\\text{concat}(x_0, y_1)$. A pixel in $y_2$ is influenced by a $3 \\times 3$ region of this input.\n- The path from $x_0$ through the convolution at layer $2$ results in a receptive field of size $k=3$.\n- The path from $y_1$ through the convolution at layer $2$ results in a receptive field of size $RF_1 + (3-1) = 3 + 2 = 5$.\nThe receptive field of $y_2$ is the maximum of these:\n$$RF_2 = \\max(3, 5) = 5$$\n\nFor layer $l=3$:\nThe input is $\\text{concat}(x_0, y_1, y_2)$. The receptive field $RF_3$ is the maximum of the receptive fields from all paths:\n- Path via $x_0$: $RF=3$.\n- Path via $y_1$: $RF = RF_1 + 2 = 3 + 2 = 5$.\n- Path via $y_2$: $RF = RF_2 + 2 = 5 + 2 = 7$.\n$$RF_3 = \\max(3, 5, 7) = 7$$\n\nBy induction, for an arbitrary layer $l$, its receptive field is:\n$$RF_l = \\max(RF(\\text{path via } x_0), RF(\\text{path via } y_1), \\dots, RF(\\text{path via } y_{l-1}))$$\n$$RF_l = \\max(3, RF_1+2, RF_2+2, \\dots, RF_{l-1}+2)$$\nSince $RF_j = 2j+1$ for $j  l$, the sequence $RF_j+2 = 2j+3$ is strictly increasing with $j$. The maximum value is determined by the path through the most recent layer, $y_{l-1}$.\n$$RF_l = RF_{l-1} + 2$$\nThis is a simple arithmetic progression. With the base case $RF_1 = 3$, the closed-form solution for the receptive field of layer $l$ is:\n$$RF_l = RF_1 + (l-1) \\times 2 = 3 + 2l - 2 = 2l + 1$$\n\nThe block output is the concatenation of all layer outputs: $[y_1, y_2, \\dots, y_L]$. The \"block receptive field side length\" is defined as the maximum receptive field side length among all its output channels, which is $\\max(RF_1, RF_2, \\dots, RF_L)$. Since $RF_l = 2l+1$ is a strictly increasing function of $l$, the maximum is achieved at $l=L$.\n$$RF_{\\text{dense block}} = RF_L = 2L + 1$$\n\n**2. Analysis of the Residual Stack (ResNet)**\n\nIn a standard residual stack, the output of block $l$, which we denote as $z_l$, is the sum of the input to the block, $z_{l-1}$, and the output of the convolutional transformation of the input, $\\text{Conv}_l(z_{l-1})$. (The problem states that pointwise nonlinearities do not alter spatial dependency, so they do not affect the receptive field calculation).\n$$z_l = \\text{Conv}_l(z_{l-1}) + z_{l-1}$$\nLet $RF_l'$ be the receptive field side length of $z_l$ with respect to the initial input $z_0 = x_0$. The receptive field of the initial input is $RF_0' = 1$.\n\nFor layer $l$, an output pixel in $z_l$ is influenced by two paths originating from $z_{l-1}$:\n- The convolutional path: $\\text{Conv}_l(z_{l-1})$. The receptive field contributed by this path is $RF_{l-1}' + (3-1) = RF_{l-1}' + 2$.\n- The identity skip connection path: $z_{l-1}$. The receptive field contributed by this path is simply $RF_{l-1}'$.\n\nThe total receptive field of $z_l$ is the union of the receptive fields from these two paths. The size of the union is the maximum of the sizes of the individual receptive fields.\n$$RF_l' = \\max(RF_{l-1}' + 2, RF_{l-1}') = RF_{l-1}' + 2$$\nThis establishes the same recurrence relation as for a simple stack of convolutional layers. We have an arithmetic progression starting from $RF_0' = 1$. After $L$ layers, the receptive field is:\n$$RF_L' = RF_0' + \\sum_{i=1}^{L} 2 = 1 + 2L$$\n\nThe final output of the residual stack is $z_L$, so the block's receptive field is $RF_L'$.\n$$RF_{\\text{residual stack}} = 2L + 1$$\n\n**Conclusion**\n\nBoth the dense block and the residual stack, under the specified conditions ($3 \\times 3$ kernel, stride $1$), exhibit the same linear growth in receptive field size. The dominant path for receptive field expansion in both architectures is the sequential chain of convolutions. Shorter paths created by dense or residual connections have smaller receptive fields that are contained within the receptive field of the longest path.\n\nThe single common closed-form expression for the block receptive field side length for both architectures is $2L+1$.", "answer": "$$\\boxed{2L+1}$$", "id": "3114064"}, {"introduction": "While dense connectivity promotes feature reuse, it can also lead to a high computational burden as the number of channels grows. This practice explores a key optimization strategy: replacing standard convolutions with depthwise separable convolutions within the bottleneck layer [@problem_id:3113990]. By deriving the exact formula for floating-point operation (FLOP) savings, you will develop the essential skill of analyzing computational efficiency from first principles.", "problem": "A single layer inside a Dense Convolutional Network (DenseNet) dense block uses a bottleneck design with two stages: a $1 \\times 1$ convolution followed by a spatial convolution. The growth rate is $k$, meaning the layer produces $k$ new feature maps, and the bottleneck expansion factor is $b$, meaning the intermediate channel count after the $1 \\times 1$ convolution is $b k$. Consider two design choices for the spatial stage: (i) a standard $3 \\times 3$ convolution from $b k$ input channels to $k$ output channels, and (ii) a depthwise separable convolution (DSC), which consists of a $3 \\times 3$ depthwise stage applied channel-wise to the $b k$ channels, followed by a $1 \\times 1$ pointwise stage mapping $b k$ channels to $k$ channels. Assume stride $1$ and that the output feature map has spatial dimensions $H \\times W$.\n\nUsing the fundamental definition of discrete convolution as a sum of products and the widely accepted convention in deep learning that each multiply and each add counts as one floating-point operation (FLOPs), and ignoring bias additions, activation functions, and boundary effects, derive the exact analytical expression for the total FLOP savings $S(k,b,H,W)$ achieved when replacing the standard $3 \\times 3$ convolution with the depthwise separable convolution in this bottleneck layer. Express $S(k,b,H,W)$ only in terms of $k$, $b$, $H$, and $W$, and simplify your final expression as much as possible. The final answer must be a single closed-form analytic expression.", "solution": "The problem asks for the analytical expression for the total floating-point operation (FLOP) savings, denoted $S(k,b,H,W)$, when replacing a standard $3 \\times 3$ spatial convolution with a $3 \\times 3$ depthwise separable convolution (DSC) within a DenseNet bottleneck layer. The savings is the difference between the FLOPs required for the standard convolution and the FLOPs for the DSC.\n\nFirst, we establish the formula for calculating FLOPs. A convolution operation is fundamentally a series of multiply-accumulate (MAC) operations. Following the problem's stated convention, each multiplication and each addition is counted as one FLOP. A single MAC operation, which computes a sum by adding a product, therefore consists of $2$ FLOPs. For a standard convolutional layer, the total number of FLOPs is given by the product of the number of MACs and $2$.\nThe number of MACs to produce an output feature map of size $H_{\\text{out}} \\times W_{\\text{out}}$ from an input with $C_{\\text{in}}$ channels using $C_{\\text{out}}$ filters of kernel size $K_H \\times K_W$ is $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W$.\nThus, the general formula for FLOPs, ignoring biases, is:\n$$\n\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W\n$$\nThe problem specifies that the stride is $1$ and the output spatial dimensions are $H \\times W$, so we set $H_{\\text{out}} = H$ and $W_{\\text{out}} = W$. The input to the spatial stage of the bottleneck layer has $b k$ channels.\n\nCase (i): Standard $3 \\times 3$ Convolution\nFor this case, the parameters are:\n- Input channels, $C_{\\text{in}} = b k$\n- Output channels, $C_{\\text{out}} = k$\n- Kernel height, $K_H = 3$\n- Kernel width, $K_W = 3$\n- Output spatial dimensions: $H \\times W$\n\nUsing the general formula, the total FLOPs for the standard convolution, $\\text{FLOPs}_{\\text{standard}}$, are:\n$$\n\\text{FLOPs}_{\\text{standard}} = 2 \\times H \\times W \\times k \\times (b k) \\times 3 \\times 3\n$$\n$$\n\\text{FLOPs}_{\\text{standard}} = 18 b H W k^2\n$$\n\nCase (ii): Depthwise Separable Convolution (DSC)\nThe DSC consists of two consecutive stages: a depthwise convolution followed by a pointwise convolution. We calculate the FLOPs for each stage and sum them.\n\nStage $1$: $3 \\times 3$ Depthwise Convolution\nA depthwise convolution applies a single spatial filter to each input channel independently. The number of output channels is therefore equal to the number of input channels.\n- Input channels, $C_{\\text{in}} = b k$\n- Kernel height, $K_H = 3$\n- Kernel width, $K_W = 3$\nThe FLOPs calculation for a depthwise convolution is $\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W$.\nThe FLOPs for this stage, $\\text{FLOPs}_{\\text{DW}}$, are:\n$$\n\\text{FLOPs}_{\\text{DW}} = 2 \\times H \\times W \\times (b k) \\times 3 \\times 3\n$$\n$$\n\\text{FLOPs}_{\\text{DW}} = 18 b H W k\n$$\nThe output of this stage has $b k$ channels and spatial dimensions $H \\times W$.\n\nStage $2$: $1 \\times 1$ Pointwise Convolution\nThis stage maps the features from the depthwise stage to the final number of output channels. It is a standard convolution with a $1 \\times 1$ kernel.\n- Input channels (from Stage $1$), $C_{\\text{in}} = b k$\n- Output channels, $C_{\\text{out}} = k$\n- Kernel height, $K_H = 1$\n- Kernel width, $K_W = 1$\nThe FLOPs for this stage, $\\text{FLOPs}_{\\text{PW}}$, are:\n$$\n\\text{FLOPs}_{\\text{PW}} = 2 \\times H \\times W \\times k \\times (b k) \\times 1 \\times 1\n$$\n$$\n\\text{FLOPs}_{\\text{PW}} = 2 b H W k^2\n$$\n\nThe total FLOPs for the DSC, $\\text{FLOPs}_{\\text{DSC}}$, is the sum of the FLOPs from both stages:\n$$\n\\text{FLOPs}_{\\text{DSC}} = \\text{FLOPs}_{\\text{DW}} + \\text{FLOPs}_{\\text{PW}} = 18 b H W k + 2 b H W k^2\n$$\n\nFinally, we calculate the FLOP savings $S(k,b,H,W)$ by subtracting the FLOPs for the DSC from the FLOPs for the standard convolution:\n$$\nS(k,b,H,W) = \\text{FLOPs}_{\\text{standard}} - \\text{FLOPs}_{\\text{DSC}}\n$$\n$$\nS(k,b,H,W) = (18 b H W k^2) - (18 b H W k + 2 b H W k^2)\n$$\n$$\nS(k,b,H,W) = 18 b H W k^2 - 18 b H W k - 2 b H W k^2\n$$\nCombine the terms with $k^2$:\n$$\nS(k,b,H,W) = (18 - 2) b H W k^2 - 18 b H W k\n$$\n$$\nS(k,b,H,W) = 16 b H W k^2 - 18 b H W k\n$$\nTo simplify the expression, we factor out the common terms $2$, $b$, $H$, $W$, and $k$:\n$$\nS(k,b,H,W) = 2 b H W k (8k - 9)\n$$\nThis is the final, simplified analytical expression for the FLOP savings.", "answer": "$$\\boxed{2bHWk(8k - 9)}$$", "id": "3113990"}, {"introduction": "The effectiveness of deep networks often hinges on subtle implementation details, and Batch Normalization (BN) is a prime example. When concatenating feature maps from different sources, as in a dense block, the question of where to place the BN layer becomes critical [@problem_id:3114019]. This practice requires you to analyze the statistical consequences of two different normalization strategies, revealing how architectural choices can create or resolve heterogeneity in feature distributions.", "problem": "Consider a dense block that concatenates feature maps along the channel dimension, a characteristic pattern of Dense Convolutional Network (DenseNet) style architectures. Let a set of channels coming from earlier layers be denoted by $A$ with $c_A$ channels, and a set of newly produced channels be denoted by $B$ with $c_B$ channels. For each channel $j$, model its pre-normalization activation as a random variable $x_j$ with mean $\\mu_j$ and variance $\\sigma_j^2$. Assume channels are independent and that statistics are stationary over the batch. We investigate two strategies for placing Batch Normalization (BN) relative to concatenation:\n- Strategy $S_1$ (BN-before-concatenation): apply BN to the new set $B$ only, then concatenate $A$ and the normalized $B$.\n- Strategy $S_2$ (BN-after-concatenation): first concatenate $A$ and $B$, then apply a single BN to all concatenated channels.\n\nUse the following fundamental base definitions:\n- Batch Normalization (BN) on a single channel $x$ with parameters $\\gamma$ and $\\beta$ and small numerical stabilizer $\\varepsilon>0$ computes\n$$\n\\mathrm{BN}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta,\n$$\nwhere $\\mu$ and $\\sigma^2$ are the batch-estimated mean and variance for that channel. In this problem, assume $\\gamma=1$ and $\\beta=0$ so that the output is centered and scaled purely by the batch statistics and $\\varepsilon$.\n- Concatenation along channels is a purely structural operation that does not alter per-channel statistics on its own; it only aggregates channels from $A$ and $B$ into a single tensor.\n\nTasks:\n1. Derive, from the BN definition and the independence assumption, the post-normalization per-channel mean and variance for each channel under $S_1$ and $S_2$ in terms of the given pre-normalization $\\mu_j$, $\\sigma_j^2$, and $\\varepsilon$. Your derivation must use only the above fundamental base and logic about how concatenation and per-channel BN interact.\n2. Define the following two scalar metrics that summarize cross-channel heterogeneity after the normalization step:\n   - The maximum absolute mean across channels\n   $$\n   M_{\\max} \\;=\\; \\max_{j} \\left| \\mathbb{E}[y_j] \\right|,\n   $$\n   where $y_j$ is the post-normalization activation for channel $j$.\n   - The variance uniformity ratio across channels\n   $$\n   R_{\\mathrm{var}} \\;=\\; \\frac{\\max_{j} \\mathrm{Var}(y_j)}{\\min_{j} \\mathrm{Var}(y_j)}.\n   $$\n3. Implement a program that, for each test case below, computes and outputs the tuple of four floats\n$$\n\\left[ M_{\\max}^{(S_1)}, \\; R_{\\mathrm{var}}^{(S_1)}, \\; M_{\\max}^{(S_2)}, \\; R_{\\mathrm{var}}^{(S_2)} \\right],\n$$\nwhere $M_{\\max}^{(S_k)}$ and $R_{\\mathrm{var}}^{(S_k)}$ denote the metrics under strategy $S_k$.\n\nImportant implementation details:\n- Treat the BN statistics as exactly equal to the provided $\\mu_j$ and $\\sigma_j^2$ for each channel (no estimation error), and use the provided $\\varepsilon$ for all channels within a given test case.\n- Under $S_1$, apply BN only to channels in $B$; channels in $A$ remain unchanged. Under $S_2$, apply BN to both $A$ and $B$ after concatenation.\n- Your program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four floats as specified above. For example, if there are $T$ test cases and the $t$-th result is the list $r_t$, the final output format must be\n$$\n[ r_1, r_2, \\dots, r_T ].\n$$\n\nTest suite:\nUse exactly the following four test cases, each defined by $(\\mu_A, \\sigma_A^2, \\mu_B, \\sigma_B^2, \\varepsilon)$, where $\\mu_A$ and $\\sigma_A^2$ are lists of length $c_A$ and $\\mu_B$ and $\\sigma_B^2$ are lists of length $c_B$. Every number provided must be used as given.\n\n- Test case $1$ (happy path, previously normalized $A$ and unnormalized $B$):\n  - $\\mu_A = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 1.0, \\; 1.0, \\; 1.0 \\,]$\n  - $\\mu_B = [\\, 3.0, \\; -2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 4.0, \\; 0.5 \\,]$\n  - $\\varepsilon = 10^{-5}$\n\n- Test case $2$ (heterogeneous $A$ and $B$):\n  - $\\mu_A = [\\, 1.5, \\; -0.5 \\,]$\n  - $\\sigma_A^2 = [\\, 2.0, \\; 0.2 \\,]$\n  - $\\mu_B = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 1.0, \\; 5.0, \\; 0.01 \\,]$\n  - $\\varepsilon = 10^{-3}$\n\n- Test case $3$ (edge case with very small variances):\n  - $\\mu_A = [\\, 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 10^{-6} \\,]$\n  - $\\mu_B = [\\, 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 10^{-6} \\,]$\n  - $\\varepsilon = 10^{-6}$\n\n- Test case $4$ (large variance mismatch):\n  - $\\mu_A = [\\, 5.0, \\; -3.0, \\; 1.0 \\,]$\n  - $\\sigma_A^2 = [\\, 100.0, \\; 0.1, \\; 10.0 \\,]$\n  - $\\mu_B = [\\, 2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 50.0 \\,]$\n  - $\\varepsilon = 10^{-4}$\n\nYour program must compute the four metrics for each test case and print a single line in the exact format\n$$\n[ [M_{\\max}^{(S_1)}, R_{\\mathrm{var}}^{(S_1)}, M_{\\max}^{(S_2)}, R_{\\mathrm{var}}^{(S_2)}], \\; \\dots ]\n$$\nwith one bracketed list per test case, in the same order as provided above.", "solution": "The problem is deemed valid. It is scientifically grounded in the principles of deep learning and statistics, is well-posed with a unique solution for each test case, and is expressed objectively. All data and definitions required for the solution are provided and are consistent. We may proceed with the solution.\n\n### 1. Derivation of Post-Normalization Statistics\n\nThe analysis relies on the fundamental properties of expectation and variance. For a random variable $X$ and constants $a$ and $b$:\n- Linearity of Expectation: $\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b$\n- Property of Variance: $\\mathrm{Var}(aX + b) = a^2 \\mathrm{Var}(X)$\n\nThe problem provides the Batch Normalization (BN) transformation for a single channel's activation $x$ as:\n$$\n\\mathrm{BN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta\n$$\nGiven the constraints $\\gamma=1$ and $\\beta=0$, this simplifies to:\n$$\ny = \\mathrm{BN}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n$$\nwhere $y$ is the post-normalization activation, and $\\mu = \\mathbb{E}[x]$ and $\\sigma^2 = \\mathrm{Var}(x)$ are the pre-normalization mean and variance of the channel.\n\nWe analyze the two strategies, $S_1$ and $S_2$, based on this definition.\n\n#### Strategy $S_1$: BN-before-concatenation\n\nIn this strategy, only the new channels in set $B$ are normalized. The channels from set $A$ are concatenated without modification.\n\n- **For channels $j$ in set $A$**:\nThe activations are not changed. Let $x_j$ be the pre-normalization activation and $y_j$ be the post-normalization activation.\n$$\ny_j = x_j\n$$\nTherefore, the post-normalization statistics are identical to the pre-normalization statistics:\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}[x_j] = \\mu_j\n$$\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}(x_j) = \\sigma_j^2\n$$\n\n- **For channels $j$ in set $B$**:\nThe activations are transformed by BN. Let $x_j$ be the pre-normalization activation for a channel $j \\in B$ with mean $\\mu_j$ and variance $\\sigma_j^2$. The post-normalization activation is:\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\nWe derive the mean and variance of $y_j$:\nThe mean is:\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}\\left[\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j]\n$$\nBy linearity of expectation, $\\mathbb{E}[x_j - \\mu_j] = \\mathbb{E}[x_j] - \\mu_j = \\mu_j - \\mu_j = 0$.\n$$\n\\mathbb{E}[y_j] = 0\n$$\nThe variance is:\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}\\left(\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j - \\mu_j)\n$$\nSince $\\mathrm{Var}(X-c) = \\mathrm{Var}(X)$ for any constant $c$, we have $\\mathrm{Var}(x_j - \\mu_j) = \\mathrm{Var}(x_j) = \\sigma_j^2$.\n$$\n\\mathrm{Var}(y_j) = \\frac{1}{\\sigma_j^2 + \\varepsilon} \\cdot \\sigma_j^2 = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\n\n#### Strategy $S_2$: BN-after-concatenation\n\nIn this strategy, channels from sets $A$ and $B$ are first concatenated. Then, a single BN layer is applied to all channels. The problem specifies that BN is a per-channel operation, meaning each channel $j$ in the concatenated set $A \\cup B$ is normalized using its own statistics $\\mu_j$ and $\\sigma_j^2$.\n\n- **For any channel $j$ in the concatenated set $A \\cup B$**:\nThe post-normalization activation $y_j$ is given by:\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\nThe derivation of the mean and variance of $y_j$ is identical to the derivation for channels in set $B$ under strategy $S_1$.\nThe mean is:\n$$\n\\mathbb{E}[y_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} (\\mu_j - \\mu_j) = 0\n$$\nThe variance is:\n$$\n\\mathrm{Var}(y_j) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j) = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\nThis holds for all channels, whether they originated from set $A$ or set $B$.\n\n### 2. Formulas for Cross-Channel Metrics\n\nUsing the derived statistics, we can formulate expressions for the metrics $M_{\\max}$ and $R_{\\mathrm{var}}$.\n\n#### Metrics for Strategy $S_1$\n\nThe set of post-normalization means is $\\{\\mu_j\\}_{j \\in A} \\cup \\{0\\}_{j \\in B}$.\nThe maximum absolute mean $M_{\\max}^{(S_1)}$ is:\n$$\nM_{\\max}^{(S_1)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max(\\max_{j \\in A} |\\mu_j|, \\max_{j \\in B} |0|) = \\max_{j \\in A} |\\mu_j|\n$$\nIf set $A$ is empty, $M_{\\max}^{(S_1)} = 0$.\n\nThe set of post-normalization variances is $\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B}$.\nThe variance uniformity ratio $R_{\\mathrm{var}}^{(S_1)}$ is:\n$$\nR_{\\mathrm{var}}^{(S_1)} = \\frac{\\max(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}{\\min(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}\n$$\n\n#### Metrics for Strategy $S_2$\n\nThe post-normalization mean for every channel $j \\in A \\cup B$ is $0$.\nThe maximum absolute mean $M_{\\max}^{(S_2)}$ is:\n$$\nM_{\\max}^{(S_2)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max_{j \\in A \\cup B} |0| = 0\n$$\n\nThe set of post-normalization variances is $\\{\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\}_{j \\in A \\cup B}$.\nThe variance uniformity ratio $R_{\\mathrm{var}}^{(S_2)}$ is:\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\max_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}{\\min_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}\n$$\nThe function $f(v) = \\frac{v}{v + \\varepsilon}$ is monotonically increasing for $v \\ge 0$. Therefore, the maximum and minimum of the transformed variances correspond to the maximum and minimum of the original variances $\\sigma_j^2$.\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\frac{\\max_{j \\in A \\cup B} \\sigma_j^2}{\\max_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}{\\frac{\\min_{j \\in A \\cup B} \\sigma_j^2}{\\min_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}\n$$\nThese formulas are now ready for implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the metrics for cross-channel heterogeneity\n    under two different batch normalization strategies in a dense block.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            [0.0, 0.0, 0.0],  # mu_A\n            [1.0, 1.0, 1.0],  # sigma_A^2\n            [3.0, -2.0],      # mu_B\n            [4.0, 0.5],       # sigma_B^2\n            1e-5              # epsilon\n        ),\n        # Test case 2\n        (\n            [1.5, -0.5],      # mu_A\n            [2.0, 0.2],       # sigma_A^2\n            [0.0, 0.0, 0.0],  # mu_B\n            [1.0, 5.0, 0.01], # sigma_B^2\n            1e-3              # epsilon\n        ),\n        # Test case 3\n        (\n            [0.0],            # mu_A\n            [1e-6],           # sigma_A^2\n            [0.0],            # mu_B\n            [1e-6],           # sigma_B^2\n            1e-6              # epsilon\n        ),\n        # Test case 4\n        (\n            [5.0, -3.0, 1.0], # mu_A\n            [100.0, 0.1, 10.0],# sigma_A^2\n            [2.0],            # mu_B\n            [50.0],           # sigma_B^2\n            1e-4              # epsilon\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_A, sigma_A_sq, mu_B, sigma_B_sq, epsilon = case\n\n        # --- Strategy S1: BN-before-concatenation ---\n        \n        # Means: Channels from A are unchanged, channels from B are centered to 0.\n        # M_max is the max absolute mean from the un-normalized channels in A.\n        if mu_A:\n            M_max_s1 = np.max(np.abs(np.array(mu_A)))\n        else:\n            M_max_s1 = 0.0\n            \n        # Variances: Channels from A are unchanged, channels from B are scaled.\n        post_vars_A_s1 = np.array(sigma_A_sq)\n        post_vars_B_s1 = np.array([s2 / (s2 + epsilon) for s2 in sigma_B_sq])\n        \n        all_post_vars_s1 = np.concatenate((post_vars_A_s1, post_vars_B_s1))\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s1.size > 0:\n            min_var_s1 = np.min(all_post_vars_s1)\n            max_var_s1 = np.max(all_post_vars_s1)\n            # Avoid division by zero, though problem constraints ensure min_var > 0.\n            R_var_s1 = max_var_s1 / min_var_s1 if min_var_s1 > 0 else float('inf')\n        else:\n            R_var_s1 = 1.0 # No variance heterogeneity if there are no channels.\n\n        # --- Strategy S2: BN-after-concatenation ---\n\n        # Means: All channels are normalized, so all means are 0.\n        M_max_s2 = 0.0\n        \n        # Variances: All channels are normalized.\n        all_pre_vars_s2 = np.array(sigma_A_sq + sigma_B_sq)\n        \n        all_post_vars_s2 = np.array([s2 / (s2 + epsilon) for s2 in all_pre_vars_s2])\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s2.size > 0:\n            min_var_s2 = np.min(all_post_vars_s2)\n            max_var_s2 = np.max(all_post_vars_s2)\n            R_var_s2 = max_var_s2 / min_var_s2 if min_var_s2 > 0 else float('inf')\n        else:\n            R_var_s2 = 1.0\n            \n        result = [M_max_s1, R_var_s1, M_max_s2, R_var_s2]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() for a list already includes spaces, e.g., '[1.0, 2.0]'.\n    # Joining these with a comma results in '...,[...', which when wrapped\n    # in brackets becomes '[[...],[...]]', matching the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3114019"}]}