{"hands_on_practices": [{"introduction": "A neuron deep within a convolutional neural network (CNN) perceives the input through many layers of processing. The set of input pixels that can influence this neuron's activation is known as its receptive field. This exercise grounds this abstract concept in a concrete mathematical derivation, challenging you to build the formula for receptive field size from the ground up based on kernel size, stride, and dilation [@problem_id:3175390]. Mastering this calculation is essential for designing architectures where receptive fields are appropriately scaled to capture features of interest.", "problem": "A one-dimensional convolutional neural network is constructed as a stack of $L$ identical layers, each a discrete, linear, shift-invariant convolution with kernel size $k$, stride $s$, and dilation $d$, with no padding and no pooling. Consider the standard definition of a dilated, strided discrete convolution: the pre-activation at output position $n$ in layer $\\ell$ is the finite sum over $k$ weights applied to inputs spaced by dilation $d$, and successive outputs are spaced by stride $s$ in the input index of the previous layer. The local receptive field of a single output position at the top layer is the number of distinct input positions from the original input layer that can affect it. Sparse connectivity means that this number is finite and does not scale with the input length.\n\nStarting only from these fundamental definitions, and without invoking any pre-memorized formula, derive a general closed-form expression for the receptive field size $R_L$ at depth $L$ as a function of $k$, $s$, $d$, and $L$, assuming that all layers share the same $(k,s,d)$ and that no padding is used anywhere. Then, verify your expression by computing the receptive field size for a synthetic network with $k=3$, $s=2$, $d=2$, and $L=4$.\n\nProvide your final answer as a single integer equal to the receptive field size for the specified synthetic network. No units are required, and no rounding is needed.", "solution": "The problem requires the derivation of a general closed-form expression for the receptive field size of a one-dimensional convolutional neural network, followed by a calculation for a specific case. The derivation must proceed from fundamental definitions.\n\nLet the layers of the network be indexed from $\\ell=1$ to $\\ell=L$. The input data is designated as layer $\\ell=0$. We define the receptive field size of a single neuron in layer $\\ell$, denoted by $R_\\ell$, as the number of distinct input positions in layer $0$ that can affect its pre-activation value. Each of the $L$ layers is identical, characterized by a kernel size $k$, a stride $s$, and a dilation $d$.\n\nTo derive the general expression for $R_L$, we first establish a recursive relationship. Consider a single neuron in layer $\\ell$. Its value is computed from $k$ inputs in layer $\\ell-1$. Due to dilation $d$, these inputs are not at adjacent positions in the layer $\\ell-1$ feature map. If the first input is at position $j$, the subsequent inputs are at positions $j+d, j+2d, \\dots, j+(k-1)d$. The total receptive field of the layer-$\\ell$ neuron is the union of the receptive fields of these $k$ input neurons from layer $\\ell-1$.\n\nTo determine the size of this union, we must understand how receptive fields from layer $\\ell-1$ are positioned relative to each other in the original input space (layer $0$). Let $J_{\\ell-1}$ represent the cumulative stride up to layer $\\ell-1$. $J_{\\ell-1}$ is the distance in the layer $0$ input space between the starting points of the receptive fields of two adjacent neurons in layer $\\ell-1$. The cumulative stride can be defined recursively. Since all layers have the same stride $s$, the cumulative stride at layer $\\ell$ is:\n$$J_\\ell = s \\cdot J_{\\ell-1}$$\nWith a base case $J_0 = 1$ (as adjacent input neurons in layer $0$ are one position apart), the solution to this recurrence is a geometric progression:\n$$J_\\ell = s^\\ell$$\n\nNow, let's analyze the span of the $k$ inputs to our neuron in layer $\\ell$. These inputs are located at relative positions $0, d, 2d, \\dots, (k-1)d$ in the layer $\\ell-1$ feature map. The first input (relative position $0$) and the last input (relative position $(k-1)d$) are separated by a distance of $(k-1)d$ in the layer $\\ell-1$ coordinate system. The corresponding shift in the original input space is the product of this separation and the cumulative stride up to layer $\\ell-1$:\n$$\\text{Shift in input space} = ((j+(k-1)d) - j) \\times J_{\\ell-1} = (k-1)d \\cdot J_{\\ell-1}$$\nThe total receptive field size at layer $\\ell$, $R_\\ell$, is the size of the receptive field of one of its constituent neurons from layer $\\ell-1$ (which is $R_{\\ell-1}$) plus the additional span covered by the shifting of the receptive fields of the other input neurons. This additional span is equal to the shift calculated above. This gives the following recurrence relation for the receptive field size:\n$$R_\\ell = R_{\\ell-1} + (k-1)d \\cdot J_{\\ell-1}$$\nSubstituting the expression for the cumulative stride, $J_{\\ell-1} = s^{\\ell-1}$, we get:\n$$R_\\ell = R_{\\ell-1} + (k-1)d \\cdot s^{\\ell-1}$$\nThe base case for this recurrence is the receptive field of a neuron in the input layer ($0$), which is just the neuron itself. Therefore, $R_0 = 1$. We can solve the recurrence for $R_L$ by telescoping the sum:\n$$R_L = R_0 + \\sum_{\\ell=1}^{L} (R_\\ell - R_{\\ell-1})$$\nSubstituting the recurrence relation and the base case $R_0=1$:\n$$R_L = 1 + \\sum_{\\ell=1}^{L} (k-1)d \\cdot s^{\\ell-1}$$\nWe can change the summation index to $i = \\ell-1$, which runs from $0$ to $L-1$:\n$$R_L = 1 + \\sum_{i=0}^{L-1} (k-1)d \\cdot s^i$$\nThe term $(k-1)d$ is constant with respect to the summation index, so it can be factored out:\n$$R_L = 1 + (k-1)d \\sum_{i=0}^{L-1} s^i$$\nThe sum is a finite geometric series, which has a well-known closed form that depends on the value of $s$.\n\nCase 1: $s \\neq 1$.\nThe sum of the geometric series is $\\sum_{i=0}^{L-1} s^i = \\frac{s^L - 1}{s-1}$.\nThe receptive field size is therefore:\n$$R_L = 1 + (k-1)d \\left(\\frac{s^L - 1}{s-1}\\right)$$\n\nCase 2: $s = 1$.\nThe sum becomes $\\sum_{i=0}^{L-1} 1^i = L$.\nThe receptive field size is therefore:\n$$R_L = 1 + (k-1)d \\cdot L$$\nThis completes the derivation of the general closed-form expression.\n\nThe second part of the problem is to compute the receptive field size for a synthetic network with the following parameters:\n- Number of layers: $L=4$\n- Kernel size: $k=3$\n- Stride: $s=2$\n- Dilation: $d=2$\n\nSince $s=2 \\neq 1$, we use the formula for Case 1:\n$$R_L = 1 + (k-1)d \\left(\\frac{s^L - 1}{s-1}\\right)$$\nSubstituting the given numerical values:\n$$R_4 = 1 + (3-1) \\cdot 2 \\cdot \\left(\\frac{2^4 - 1}{2-1}\\right)$$\n$$R_4 = 1 + 2 \\cdot 2 \\cdot \\left(\\frac{16 - 1}{1}\\right)$$\n$$R_4 = 1 + 4 \\cdot (15)$$\n$$R_4 = 1 + 60$$\n$$R_4 = 61$$\nThe receptive field size for the specified synthetic network is $61$.", "answer": "$$\\boxed{61}$$", "id": "3175390"}, {"introduction": "Knowing how to calculate a receptive field's size leads to a critical design question: what is the *right* size for a given task? This practice explores that question in a hypothetical scenario of detecting an edge in a noisy signal, a classic problem in signal processing [@problem_id:3175463]. By finding the minimal receptive field that ensures reliable detection, you will grapple with the fundamental trade-off between noise suppression (which favors larger fields) and feature localization (which favors smaller fields), a central challenge in building effective vision models.", "problem": "Consider a one-dimensional abstraction of a horizontal scanline from a binary image that contains a single vertical edge. The clean signal consists of a constant intensity $I_{\\ell}$ to the left of the edge and a constant intensity $I_{r}$ to the right of the edge, forming an ideal step with contrast $\\Delta = |I_{\\ell} - I_{r}|$. The observed signal is corrupted by independent additive Gaussian noise at each pixel with zero mean and variance $\\sigma^{2}$.\n\nA local receptive field of size $R$ (with $R$ an even integer) defines a sparse connectivity pattern that uses only $R$ contiguous samples centered at a given scan position. Consider the following linear detector that operates on this receptive field: the weights on the left half of the field are all equal to $+\\frac{1}{R/2}$, and the weights on the right half are all equal to $-\\frac{1}{R/2}$. The detector output at a given position is the weighted sum of the $R$ observed samples in the field. A scanning procedure with stride $1$ is assumed, so that there is a position where the field is exactly centered on the true edge. Define a fixed decision threshold $\\tau \\ge 0$ on the absolute value of the detector output, and define a required reliability $q \\in (0,1)$, understood as a decimal fraction.\n\nRobust detection is defined as achieving, at the position where the receptive field is centered on the true edge, a probability at least $q$ that the magnitude of the detector output exceeds $\\tau$. Assume that the edge is sufficiently far from the signal boundaries so that the aligned field is fully contained in the signal of length $N$; in particular, the receptive field size is constrained by $2 \\le R \\le N$.\n\nStarting from the fundamental definitions of linear filtering, local receptive fields, and properties of independent Gaussian noise under linear transformations, derive a method to determine, for given $(N, I_{\\ell}, I_{r}, \\sigma, \\tau, q)$, the minimal even integer $R$ that guarantees robust detection in the above sense. If no such $R$ exists under the constraint $R \\le N$, report impossibility by returning the integer $-1$.\n\nYour program must implement this derivation to compute the minimal $R$ for each parameter set in the following test suite. Each parameter set is specified as an ordered tuple $(N, I_{\\ell}, I_{r}, \\sigma, \\tau, q)$:\n\n- Test case $1$: $(N = 128, I_{\\ell} = 0.0, I_{r} = 1.0, \\sigma = 0.2, \\tau = 0.5, q = 0.95)$.\n- Test case $2$: $(N = 128, I_{\\ell} = 0.0, I_{r} = 0.6, \\sigma = 0.3, \\tau = 0.25, q = 0.90)$.\n- Test case $3$: $(N = 256, I_{\\ell} = 0.0, I_{r} = 0.4, \\sigma = 0.1, \\tau = 0.5, q = 0.90)$.\n- Test case $4$: $(N = 64, I_{\\ell} = 0.0, I_{r} = 1.0, \\sigma = 0.0, \\tau = 0.7, q = 0.99)$.\n- Test case $5$: $(N = 4, I_{\\ell} = 0.0, I_{r} = 0.6, \\sigma = 0.3, \\tau = 0.25, q = 0.90)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the same order as the test cases (for example, $[R_{1},R_{2},R_{3},R_{4},R_{5}]$). No other output is permitted. Angles do not appear in this problem, and no physical units are required in the output. All probabilities must be handled and represented as decimal fractions, not as percentages.", "solution": "The problem statement is critically evaluated for validity before attempting a solution.\n\n### Step 1: Extract Givens\n- **Signal Model**: A one-dimensional scanline from a binary image is considered.\n- **Clean Signal**: An ideal step function with constant intensity $I_{\\ell}$ to the left of an edge and $I_{r}$ to the right.\n- **Contrast**: $\\Delta = |I_{\\ell} - I_{r}|$.\n- **Noise Model**: Independent additive Gaussian noise with mean $0$ and variance $\\sigma^2$ is present at each pixel.\n- **Detector**: A local receptive field of size $R$, where $R$ is an even integer. The detector is a linear filter with weights.\n- **Detector Weights**: The weights are $+\\frac{1}{R/2}$ for the $R/2$ samples in the left half of the field and $-\\frac{1}{R/2}$ for the $R/2$ samples in the right half.\n- **Detector Output**: The output is the weighted sum of the $R$ observed samples in the field.\n- **Scanning and Alignment**: A stride of $1$ is used. A specific position is considered where the receptive field is perfectly centered on the true edge.\n- **Decision Rule**: A fixed decision threshold $\\tau \\ge 0$ is applied to the absolute value of the detector output.\n- **Reliability Requirement**: A probability $q \\in (0,1)$ that the magnitude of the detector output exceeds $\\tau$ must be achieved.\n- **Robust Detection**: The condition $P(\\text{detector output magnitude} > \\tau) \\ge q$ must hold when the field is centered on the edge.\n- **Constraints**: $2 \\le R \\le N$, where $N$ is the signal length. The receptive field is assumed to be fully contained within the signal.\n- **Objective**: Find the minimal even integer $R$ satisfying the robust detection criteria for a given parameter set $(N, I_{\\ell}, I_{r}, \\sigma, \\tau, q)$.\n- **Impossibility**: If no such $R$ exists within the constraint $R \\le N$, the result should be $-1$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in fundamental principles of signal processing, statistics, and machine learning. It models a classic edge detection scenario under additive white Gaussian noise (AWGN), using a linear filter (a simplified Haar-like feature detector) and statistical decision theory. These concepts are standard and scientifically sound.\n- **Well-Posed**: The problem is well-posed. It asks for the minimization of an integer variable $R$ subject to a well-defined inequality and boundary constraints ($R$ is an even integer, $2 \\le R \\le N$). The existence of a unique minimal solution (or the demonstrable lack thereof) can be determined.\n- **Objective**: The problem is stated in precise, objective, and mathematical language. Subjective terms are given formal definitions (e.g., \"robust detection\").\n\nThe problem does not exhibit any flaws such as scientific unsoundness, ambiguity, contradiction, or reliance on non-standard a-scientific claims. All parameters are provided, and the objective is clearly specified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A principled solution will be derived.\n\n### Derivation of the Solution\n\nLet the receptive field of size $R$ be centered on the edge. The observed samples are $s_i = \\mu_i + \\eta_i$ for $i=1, \\dots, R$, where $\\mu_i$ is the clean signal intensity and $\\eta_i$ are independent and identically distributed (i.i.d.) random variables from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$.\n\nWhen the field is centered, the clean signal values are $\\mu_i = I_{\\ell}$ for the left half ($i=1, \\dots, R/2$) and $\\mu_i = I_{r}$ for the right half ($i=R/2+1, \\dots, R$).\n\nThe detector is a linear filter with weights $w_i$:\n$$\nw_i = \\begin{cases}\n+ \\frac{1}{R/2} & \\text{for } i = 1, \\dots, R/2 \\\\\n- \\frac{1}{R/2} & \\text{for } i = R/2+1, \\dots, R\n\\end{cases}\n$$\nThe detector output, $D$, is a random variable given by the weighted sum:\n$$\nD = \\sum_{i=1}^{R} w_i s_i = \\frac{1}{R/2} \\sum_{i=1}^{R/2} s_i - \\frac{1}{R/2} \\sum_{i=R/2+1}^{R} s_i\n$$\n\nWe determine the statistical properties of $D$. As $D$ is a linear combination of independent Gaussian random variables, $D$ itself follows a Gaussian distribution. We need to find its mean $\\mu_D$ and variance $\\sigma_D^2$.\n\nThe mean (expected value) of the detector output is:\n$$\n\\mu_D = E[D] = E\\left[\\sum_{i=1}^{R} w_i (\\mu_i + \\eta_i)\\right] = \\sum_{i=1}^{R} w_i \\mu_i + \\sum_{i=1}^{R} w_i E[\\eta_i]\n$$\nSince $E[\\eta_i] = 0$, the second term vanishes.\n$$\n\\mu_D = \\sum_{i=1}^{R/2} \\left(+\\frac{1}{R/2}\\right) I_{\\ell} + \\sum_{i=R/2+1}^{R} \\left(-\\frac{1}{R/2}\\right) I_{r}\n$$\n$$\n\\mu_D = \\frac{R/2}{R/2} I_{\\ell} - \\frac{R/2}{R/2} I_{r} = I_{\\ell} - I_{r}\n$$\n\nThe variance of the detector output is:\n$$\n\\sigma_D^2 = \\text{Var}(D) = \\text{Var}\\left(\\sum_{i=1}^{R} w_i (\\mu_i + \\eta_i)\\right) = \\text{Var}\\left(\\sum_{i=1}^{R} w_i \\eta_i\\right)\n$$\nSince the noise samples $\\eta_i$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\sigma_D^2 = \\sum_{i=1}^{R} w_i^2 \\text{Var}(\\eta_i) = \\sum_{i=1}^{R} w_i^2 \\sigma^2\n$$\n$$\n\\sigma_D^2 = \\sigma^2 \\left( \\sum_{i=1}^{R/2} \\left(\\frac{1}{R/2}\\right)^2 + \\sum_{i=R/2+1}^{R} \\left(-\\frac{1}{R/2}\\right)^2 \\right)\n$$\n$$\n\\sigma_D^2 = \\sigma^2 \\left( (R/2) \\frac{1}{(R/2)^2} + (R/2) \\frac{1}{(R/2)^2} \\right) = \\sigma^2 \\left( \\frac{1}{R/2} + \\frac{1}{R/2} \\right) = \\frac{2\\sigma^2}{R/2} = \\frac{4\\sigma^2}{R}\n$$\nThe standard deviation of the output is $\\sigma_D = \\sqrt{4\\sigma^2/R} = \\frac{2\\sigma}{\\sqrt{R}}$.\n\nSo, the detector output is a Gaussian random variable $D \\sim \\mathcal{N}\\left(I_{\\ell} - I_{r}, \\frac{4\\sigma^2}{R}\\right)$.\n\nThe robust detection condition is $P(|D| > \\tau) \\ge q$. This can be written as $P(D > \\tau) + P(D < -\\tau) \\ge q$.\nLet $Z = \\frac{D-\\mu_D}{\\sigma_D}$ be a standard normal variable, $Z \\sim \\mathcal{N}(0,1)$.\nThe condition becomes:\n$$\nP\\left(Z > \\frac{\\tau - \\mu_D}{\\sigma_D}\\right) + P\\left(Z < \\frac{-\\tau - \\mu_D}{\\sigma_D}\\right) \\ge q\n$$\nLet $\\Phi(x)$ be the cumulative distribution function (CDF) of the standard normal distribution. The inequality is:\n$$\n\\left(1 - \\Phi\\left(\\frac{\\tau - \\mu_D}{\\sigma_D}\\right)\\right) + \\Phi\\left(\\frac{-\\tau - \\mu_D}{\\sigma_D}\\right) \\ge q\n$$\nLet $\\Delta = |I_{\\ell} - I_{r}|$. Then $\\mu_D = \\pm \\Delta$. The probability expression is symmetric with respect to the sign of $\\mu_D$. Using the property $\\Phi(-x) = 1-\\Phi(x)$, the expression simplifies to $\\Phi\\left(-\\frac{\\tau-\\Delta}{\\sigma_D}\\right) + \\Phi\\left(\\frac{-\\tau-\\Delta}{\\sigma_D}\\right)$ if we assume $\\mu_D=\\Delta$, and the same holds for $\\mu_D=-\\Delta$. We can write this as $\\Phi\\left(\\frac{\\Delta-\\tau}{\\sigma_D}\\right) + \\Phi\\left(\\frac{-\\Delta-\\tau}{\\sigma_D}\\right)$.\nSubstituting $\\sigma_D = \\frac{2\\sigma}{\\sqrt{R}}$:\n$$\nP(R) = \\Phi\\left(\\frac{(\\Delta - \\tau)\\sqrt{R}}{2\\sigma}\\right) + \\Phi\\left(\\frac{(-\\Delta - \\tau)\\sqrt{R}}{2\\sigma}\\right) \\ge q\n$$\nWe need to find the minimum even integer $R \\in [2, N]$ that satisfies this inequality.\n\nWe analyze the behavior of the probability function $P(R)$ with respect to $R$.\n1.  **Case $\\sigma = 0$**: The detector output is deterministic: $D = \\mu_D$. Then $|D| = \\Delta$. The probability $P(|D| > \\tau)$ is $1$ if $\\Delta > \\tau$ and $0$ if $\\Delta \\le \\tau$. If $\\Delta > \\tau$, any $R$ works, so the minimal even $R$ is $2$. If $\\Delta \\le \\tau$, no $R$ works, so the answer is $-1$.\n2.  **Case $\\sigma > 0$ and $\\Delta > \\tau$**: As $R$ increases, $\\sigma_D$ decreases. The distribution of $D$ becomes more concentrated around its mean $\\mu_D$. Since $|\\mu_D|=\\Delta > \\tau$, the mean lies outside the interval $[-\\tau, \\tau]$. Thus, concentrating the probability mass around $\\mu_D$ increases the probability of being outside $[-\\tau, \\tau]$. Therefore, $P(R)$ is a monotonically increasing function of $R$. We can find the minimal $R$ by starting with $R=2$ and incrementing by $2$ until the condition is met or $R > N$.\n3.  **Case $\\sigma > 0$ and $\\Delta \\le \\tau$**: As $R$ increases, $\\sigma_D$ decreases. Since $|\\mu_D|=\\Delta \\le \\tau$, the mean lies inside or on the boundary of the interval $[-\\tau, \\tau]$. Concentrating the probability mass around $\\mu_D$ increases the probability of being inside $[-\\tau, \\tau]$, thus decreasing $P(|D| > \\tau)$. Therefore, $P(R)$ is a non-increasing function of $R$. If the condition $P(R) \\ge q$ is not met for the smallest possible $R$ (i.e., $R=2$), it will not be met for any larger $R$. Thus, we only need to check $R=2$. If it passes, the answer is $2$; otherwise, it is $-1$.\n\nThe algorithm is as follows:\nFor each parameter set $(N, I_{\\ell}, I_{r}, \\sigma, \\tau, q)$:\n1.  Calculate $\\Delta = |I_{\\ell} - I_{r}|$.\n2.  If $\\sigma = 0$: Return $2$ if $\\Delta > \\tau$, otherwise return $-1$.\n3.  If $\\sigma > 0$ and $\\Delta > \\tau$: Iterate through even integers $R$ from $2$ to $N$. For each $R$, calculate $P(R)$. The first $R$ for which $P(R) \\ge q$ is the answer. If the loop completes without finding such an $R$, return $-1$.\n4.  If $\\sigma > 0$ and $\\Delta \\le \\tau$: Calculate $P(2)$. If $P(2) \\ge q$, return $2$. Otherwise, return $-1$.\n\nThe standard normal CDF $\\Phi(x)$ can be computed using the error function, $\\text{erf}(x)$, as $\\Phi(x) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$.", "answer": "[2,6,-1,2,-1]", "id": "3175463"}, {"introduction": "While a receptive field defines a neuron's window onto the input, the kernel weights determine *how* it processes information within that window. This exercise delves into the internal structure of a kernel, comparing two powerful compression techniques: sparse pruning and low-rank factorization [@problem_id:3175349]. By analyzing how these distinct approaches to creating \"sparse\" or efficient filters affect the kernel's energy distribution, you will gain a more nuanced understanding of structural priors and their relevance to modern, efficient deep learning architectures.", "problem": "Consider a two-dimensional (2D) discrete convolutional filter with finite spatial support. Let the filter kernel be a matrix $K \\in \\mathbb{R}^{h \\times w}$, with $h,w \\in \\mathbb{N}$, indexed by $i \\in \\{0,\\ldots,h-1\\}$ and $j \\in \\{0,\\ldots,w-1\\}$. When convolved with a discrete impulse centered at the kernel center, the output equals the kernel itself; thus the kernel acts as the effective impulse response and encodes the local receptive field.\n\nYou will analyze and compare two compression schemes for $K$:\n- Low-rank factorization by singular value truncation.\n- Sparse kernel pruning that zeros all but the largest-magnitude entries.\n\nYou will relate rank constraints to effective locality by quantifying how much of the kernel's energy is concentrated within a central window.\n\nFundamental base to use:\n- Definition of discrete convolution and the notion that convolving a filter with a centered impulse yields the filter as the output.\n- The Frobenius norm $\\|K\\|_{F} = \\sqrt{\\sum_{i=0}^{h-1} \\sum_{j=0}^{w-1} K_{ij}^{2}}$ and energy $\\|K\\|_{F}^{2}$.\n- Singular Value Decomposition (SVD): any matrix $K$ admits a factorization $K = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{h \\times h}$, $V \\in \\mathbb{R}^{w \\times w}$ are orthogonal and $\\Sigma \\in \\mathbb{R}^{h \\times w}$ is diagonal in the sense of singular values. The truncated SVD of rank $R$ is obtained by keeping the top $R$ singular values and associated singular vectors.\n- Sparse pruning: keeping only the $s$ entries of $K$ with largest absolute values and zeroing the rest.\n\nConstruct $K$ deterministically as a normalized isotropic Gaussian bump centered in the kernel:\n$$\nK_{ij} = \\exp\\left(-\\frac{(i - c_h)^2 + (j - c_w)^2}{2\\sigma^2}\\right),\n$$\nwhere $c_h = \\left\\lfloor \\frac{h}{2} \\right\\rfloor$, $c_w = \\left\\lfloor \\frac{w}{2} \\right\\rfloor$, and $\\sigma \\in \\mathbb{R}_{>0}$. Normalize $K$ so that $\\|K\\|_{F} = 1$.\n\nDefine the central window of odd side length $m \\in \\mathbb{N}$, with $m \\le \\min(h,w)$, centered at $(c_h,c_w)$, spanning indices $i \\in \\{c_h - \\lfloor m/2 \\rfloor, \\ldots, c_h + \\lfloor m/2 \\rfloor\\}$ and $j \\in \\{c_w - \\lfloor m/2 \\rfloor, \\ldots, c_w + \\lfloor m/2 \\rfloor\\}$.\n\nFor a matrix $A \\in \\mathbb{R}^{h \\times w}$, define its locality fraction with respect to the central window as\n$$\nL(A; m) = \\frac{\\sum_{i=c_h-\\lfloor m/2 \\rfloor}^{c_h+\\lfloor m/2 \\rfloor} \\sum_{j=c_w-\\lfloor m/2 \\rfloor}^{c_w+\\lfloor m/2 \\rfloor} A_{ij}^{2}}{\\sum_{i=0}^{h-1} \\sum_{j=0}^{w-1} A_{ij}^{2}}.\n$$\n\nTwo compressions:\n1. Low-rank compression: compute $K \\approx K_R = \\sum_{r=1}^{R} \\sigma_r u_r v_r^{\\top}$ using truncated singular value decomposition of rank $R \\in \\mathbb{N}$, with $\\sigma_r$ the singular values, $u_r \\in \\mathbb{R}^{h}$ and $v_r \\in \\mathbb{R}^{w}$ the singular vectors.\n2. Sparse pruning: compute $K_{\\text{sp}}$ by retaining only the $s \\in \\mathbb{N}$ entries of $K$ with largest absolute values and setting all other entries to zero.\n\nFor each test case, compute the difference in locality fraction\n$$\nD = L(K_R; m) - L(K_{\\text{sp}}; m).\n$$\n\nYour program must implement the above definitions and produce $D$ for each provided test case.\n\nTest suite:\n- Case $\\#1$: $(h,w,\\sigma,R,s,m) = (11,11,2.0,1,25,5)$.\n- Case $\\#2$: $(h,w,\\sigma,R,s,m) = (11,11,1.0,2,15,3)$.\n- Case $\\#3$ (boundary, exact reconstructions): $(h,w,\\sigma,R,s,m) = (11,11,2.5,11,121,5)$.\n- Case $\\#4$ (edge, extreme sparsity and rank): $(h,w,\\sigma,R,s,m) = (11,11,3.0,1,1,7)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the test cases in order, as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4]$, where each $r_k$ is the computed real-valued $D$ for case $k$.", "solution": "The problem requires a comparative analysis of two matrix compression techniques—low-rank factorization and sparse pruning—applied to a 2D discrete convolutional kernel. The kernel $K$ is modeled as a normalized, centered isotropic Gaussian function. The comparison is based on a metric called the \"locality fraction,\" $L(A; m)$, which quantifies the proportion of a matrix's total energy (squared Frobenius norm) contained within a central window of size $m \\times m$. For each test case, we must compute the difference $D = L(K_R; m) - L(K_{\\text{sp}}; m)$, where $K_R$ is the rank-$R$ approximation and $K_{\\text{sp}}$ is the sparse approximation with $s$ non-zero elements.\n\nThe solution proceeds systematically through the following steps for each test case $(h, w, \\sigma, R, s, m)$:\n\n1.  **Kernel Generation ($K$)**:\n    First, we construct the unnormalized kernel matrix $K' \\in \\mathbb{R}^{h \\times w}$. Its elements $K'_{ij}$ are given by the isotropic Gaussian function centered at $(c_h, c_w)$, where $c_h = \\lfloor h/2 \\rfloor$ and $c_w = \\lfloor w/2 \\rfloor$:\n    $$\n    K'_{ij} = \\exp\\left(-\\frac{(i - c_h)^2 + (j - c_w)^2}{2\\sigma^2}\\right)\n    $$\n    for $i \\in \\{0, \\ldots, h-1\\}$ and $j \\in \\{0, \\ldots, w-1\\}$.\n    This kernel is then normalized to have a unit Frobenius norm, ensuring its total energy is $1$. The normalized kernel $K$ is:\n    $$\n    K = \\frac{K'}{\\|K'\\|_{F}}\n    $$\n    where the Frobenius norm is $\\|K'\\|_{F} = \\sqrt{\\sum_{i=0}^{h-1} \\sum_{j=0}^{w-1} (K'_{ij})^2}$. After normalization, $\\|K\\|_{F}^2 = 1$.\n\n2.  **Low-Rank Approximation ($K_R$)**:\n    The rank-$R$ approximation of $K$, denoted $K_R$, is obtained via truncated Singular Value Decomposition (SVD). The SVD of $K$ is $K = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{h \\times h}$ and $V \\in \\mathbb{R}^{w \\times w}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{h \\times w}$ is a rectangular diagonal matrix containing the singular values $\\sigma_r$ in descending order. The columns of $U$ ($u_r$) and $V$ ($v_r$) are the left and right singular vectors, respectively.\n    $K_R$ is constructed by summing the first $R$ outer products of corresponding singular vectors scaled by their singular values:\n    $$\n    K_R = \\sum_{r=1}^{R} \\sigma_r u_r v_r^{\\top}\n    $$\n    This is the best rank-$R$ approximation of $K$ in the Frobenius norm sense. In implementation, this is achieved by taking the first $R$ columns of $U$, the first $R$ rows of $V^{\\top}$, and the top-left $R \\times R$ block of $\\Sigma$.\n\n3.  **Sparse Approximation ($K_{\\text{sp}}$)**:\n    The sparse kernel $K_{\\text{sp}}$ is generated by retaining only the $s$ elements of $K$ with the largest absolute values and setting all other elements to zero. Since $K$ is a centered Gaussian, its values are non-negative and monotonically decrease with distance from the center. Therefore, the $s$ largest values correspond to the $s$ grid points closest to the center $(c_h, c_w)$.\n    Let $\\mathcal{I}_s$ be the set of index pairs $(i,j)$ corresponding to the $s$ largest entries of $K$. Then,\n    $$\n    (K_{\\text{sp}})_{ij} = \\begin{cases} K_{ij} & \\text{if } (i,j) \\in \\mathcal{I}_s \\\\ 0 & \\text{otherwise} \\end{cases}\n    $$\n    This operation is also known as hard thresholding or pruning.\n\n4.  **Locality Fraction ($L(A; m)$)**:\n    The locality fraction $L(A; m)$ for any matrix $A \\in \\mathbb{R}^{h \\times w}$ measures the concentration of its energy within a central window of size $m \\times m$. The window is defined by indices $i, j$ such that $i \\in \\{c_h - \\lfloor m/2 \\rfloor, \\ldots, c_h + \\lfloor m/2 \\rfloor\\}$ and $j \\in \\{c_w - \\lfloor m/2 \\rfloor, \\ldots, c_w + \\lfloor m/2 \\rfloor\\}$. The formula is:\n    $$\n    L(A; m) = \\frac{\\text{Energy inside window}}{\\text{Total energy}} = \\frac{\\sum_{i=c_h-\\lfloor m/2 \\rfloor}^{c_h+\\lfloor m/2 \\rfloor} \\sum_{j=c_w-\\lfloor m/2 \\rfloor}^{c_w+\\lfloor m/2 \\rfloor} A_{ij}^{2}}{\\|A\\|_{F}^{2}}\n    $$\n\n5.  **Final Calculation ($D$)**:\n    For each test case, we first compute $K_R$ and $K_{\\text{sp}}$ from the base kernel $K$. Then, we calculate their respective locality fractions, $L(K_R; m)$ and $L(K_{\\text{sp}}; m)$. The final desired value is the difference between these two quantities:\n    $$\n    D = L(K_R; m) - L(K_{\\text{sp}}; m)\n    $$\n    This value $D$ provides insight into how the two different compression strategies affect the spatial localization of the kernel's energy. A negative value suggests that sparse pruning results in a more localized kernel than low-rank approximation, and vice-versa. For the special case where both approximations exactly reconstruct the original kernel (e.g., $R=\\min(h,w)$ and $s=h \\times w$), $D$ must be $0$.", "answer": "[-0.013581781643,0.016335359737,0.000000000000,-0.485981329249]", "id": "3175349"}]}