{"hands_on_practices": [{"introduction": "This problem explores a core optimization principle introduced in later versions of the Inception architecture. By analyzing the equivalence between a single large convolution and a stack of smaller ones, you will gain a first-principles understanding of how to reduce computational cost while preserving the receptive field size [@problem_id:3137618]. This exercise is crucial for appreciating the efficiency and design philosophy behind modern convolutional networks.", "problem": "Consider a branch in an Inception module that is implemented in two alternative ways on an input feature map of spatial size $H \\times W$ with $C_{\\text{in}}$ input channels and $F$ output channels. Variant $1$ uses a single two-dimensional convolution with kernel size $5 \\times 5$, stride $1$, no dilation, and \"same\" zero padding, mapping $C_{\\text{in}} \\rightarrow F$. Variant $2$ uses two stacked two-dimensional convolutions, each of kernel size $3 \\times 3$, stride $1$, no dilation, and \"same\" zero padding, where the first convolution maps $C_{\\text{in}} \\rightarrow F$ and the second maps $F \\rightarrow F$. A Rectified Linear Unit (ReLU) nonlinearity is applied immediately after each convolution. Assume standard dense convolutions (no groups and no depthwise separations), and ignore bias costs and nonlinear activation costs in the operation counting.\n\nUsing only the definition of discrete convolution, the fact that composing stride-$1$ convolutions aggregates index shifts additively, and counting Multiplyâ€“Accumulate (MAC) operations per output spatial location from the summations inherent in convolution, reason from first principles to establish the following properties:\n\n- Whether the two designs have equal receptive field size under the stated conditions.\n- The algebraic condition on $C_{\\text{in}}$ and $F$ under which the per-location MAC count of Variant $2$ equals that of Variant $1$.\n- For the top-left output spatial position under \"same\" padding, how many contributing terms in the summations involve padded inputs for Variant $1$ versus Variant $2$.\n- How many nonlinearities a signal experiences along a single input-to-output path in each variant.\n\nSelect all statements that are true:\n\nA. Under stride $1$ and no dilation, two stacked $3 \\times 3$ convolutions have the same receptive field size as one $5 \\times 5$ convolution, namely $5 \\times 5$.\n\nB. The per-location MAC counts are equal between Variant $1$ and Variant $2$ exactly when $C_{\\text{in}} = \\dfrac{9}{16} F$, given that Variant $2$ uses $C_{\\text{in}} \\rightarrow F$ then $F \\rightarrow F$ convolutions and biases are ignored.\n\nC. With \"same\" zero padding at each convolution, for the top-left output position, Variant $2$ aggregates $81$ second-order kernel contributions, of which $45$ involve padded inputs, whereas Variant $1$ aggregates $25$ contributions, of which $16$ involve padded inputs; thus Variant $2$ involves a larger absolute number of padding-influenced computations at the border.\n\nD. Because two kernels are composed, the receptive field of Variant $2$ becomes $7 \\times 7$.\n\nE. If a Rectified Linear Unit (ReLU) is placed after each convolution, the number of nonlinearities applied along an input-to-output path is the same in Variant $1$ and Variant $2$.", "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, objective, and self-contained. We proceed with the solution.\n\nThe problem asks for an analysis of two alternative convolutional branch designs (Variant $1$ and Variant $2$) based on four properties: receptive field size, computational cost (MACs), influence of padding, and the number of nonlinearities. We will derive each property from first principles.\n\nLet the input feature map have dimensions $H \\times W \\times C_{\\text{in}}$. Both variants produce an output feature map of size $H \\times W \\times F$.\n\n-   **Variant 1**: One $5 \\times 5$ convolution, mapping $C_{\\text{in}} \\rightarrow F$, with stride $1$.\n-   **Variant 2**: Two stacked $3 \\times 3$ convolutions, both with stride $1$. The first maps $C_{\\text{in}} \\rightarrow F$, and the second maps $F \\rightarrow F$.\n\n### 1. Receptive Field Size\n\nThe receptive field of a neuron is the region of the input space that affects its value. For a sequence of convolutions with stride $1$, the receptive field size grows additively. The general formula for the receptive field size $R_i$ after layer $i$ with kernel size $k_i$ is $R_i = R_{i-1} + k_i - 1$, where $R_0 = 1$ (a single input pixel).\n\n-   **Variant 1**: We have a single layer with a kernel of size $k_1 = 5$. The receptive field size is $R_1 = R_0 + k_1 - 1 = 1 + 5 - 1 = 5$. The receptive field is $5 \\times 5$.\n\n-   **Variant 2**: We have two stacked layers, both with kernel size $k=3$.\n    -   After the first convolution ($k_1 = 3$): The receptive field size is $R_1 = R_0 + k_1 - 1 = 1 + 3 - 1 = 3$. An intermediate neuron sees a $3 \\times 3$ patch of the input.\n    -   After the second convolution ($k_2 = 3$): The receptive field size of the final output neuron is $R_2 = R_1 + k_2 - 1 = 3 + 3 - 1 = 5$. An output neuron sees a $3 \\times 3$ patch of the intermediate feature map, where the receptive field of each of those intermediate neurons was already $3 \\times 3$. The total receptive field with respect to the original input is $5 \\times 5$.\n\nTherefore, both variants have the same receptive field size of $5 \\times 5$.\n\n### 2. Per-Location Multiply-Accumulate (MAC) Count\n\nThe number of MAC operations to compute a single spatial location of the output feature map is the product of the number of output channels, input channels, and the kernel's spatial size ($K \\times K$). Bias costs are ignored.\n\n-   **Variant 1**:\n    -   Kernel size: $5 \\times 5 = 25$.\n    -   Input channels: $C_{\\text{in}}$.\n    -   Output channels: $F$.\n    -   MACs per location (V1) = $C_{\\text{in}} \\times F \\times 5 \\times 5 = 25 C_{\\text{in}} F$.\n\n-   **Variant 2**: The total cost is the sum of the costs of the two convolutional layers.\n    -   **First convolution**:\n        -   Kernel size: $3 \\times 3 = 9$.\n        -   Input channels: $C_{\\text{in}}$.\n        -   Output channels: $F$.\n        -   MACs (Conv1) = $C_{\\text{in}} \\times F \\times 3 \\times 3 = 9 C_{\\text{in}} F$.\n    -   **Second convolution**:\n        -   Kernel size: $3 \\times 3 = 9$.\n        -   Input channels: $F$ (output from the first layer).\n        -   Output channels: $F$.\n        -   MACs (Conv2) = $F \\times F \\times 3 \\times 3 = 9 F^2$.\n    -   **Total MACs per location (V2)** = MACs(Conv1) + MACs(Conv2) = $9 C_{\\text{in}} F + 9 F^2$.\n\nTo find the condition for equal MAC counts, we set MACs(V1) = MACs(V2):\n$$25 C_{\\text{in}} F = 9 C_{\\text{in}} F + 9 F^2$$\nAssuming $F \\neq 0$, we can divide by $F$:\n$$25 C_{\\text{in}} = 9 C_{\\text{in}} + 9 F$$\n$$16 C_{\\text{in}} = 9 F$$\n$$C_{\\text{in}} = \\frac{9}{16} F$$\n\n### 3. Padded Input Contributions for the Top-Left Output\n\nWe analyze the computation for the output at spatial position $(0, 0)$. \"Same\" padding adds rows/columns of zeros to ensure the output spatial dimensions match the input. For a kernel of size $K \\times K$ with stride $1$, the necessary padding is $P = (K-1)/2$ on each side.\n\n-   **Variant 1**: Kernel size $K = 5$, so padding is $P = (5-1)/2 = 2$. The $5 \\times 5$ kernel, when computing the output at $(0,0)$, covers an area in the padded input that corresponds to original input indices from $-2$ to $2$.\n    -   The total number of spatial terms in the sum is $5 \\times 5 = 25$. These are the \"contributions\".\n    -   The input values with non-negative indices are from the original feature map. For the top-left position, this corresponds to indices $(i,j)$ where $i,j \\in \\{0, 1, 2\\}$. This is a $3 \\times 3$ grid, so there are $9$ non-padded contributions.\n    -   The number of contributions from padded zeros is $25 - 9 = 16$.\n\n-   **Variant 2**: This involves two $3 \\times 3$ convolutions. Let the kernels be $W_1$ (first layer) and $W_2$ (second layer). An output pixel $Y$ is a function of the input $X$ via the intermediate map $I$. Schematically, $Y \\sim W_2 \\star I = W_2 \\star (W_1 \\star X)$. Ignoring the ReLU and channel dimensions for structural analysis, this is equivalent to $Y \\sim (W_2 \\star W_1) \\star X$, where $W_{\\text{eff}} = W_2 \\star W_1$ is an effective kernel of size $(3+3-1) \\times (3+3-1) = 5 \\times 5$.\n    -   The computation of an output pixel value can be expanded as a sum of terms, each being a product of one weight from $W_1$, one from $W_2$, and one input pixel value $X$.\n    $Y_{i,j} = \\sum_{p,q} W_2(p,q) I_{i-p, j-q} = \\sum_{p,q} W_2(p,q) \\left( \\sum_{r,s} W_1(r,s) X_{i-p-r, j-q-s} \\right)$.\n    -   The total number of such \"second-order kernel contributions\" for a single output pixel is the product of the number of weights in each kernel: $3 \\times 3 \\times 3 \\times 3 = 81$.\n    -   We need to count how many of these $81$ terms use a padded input for the output at $(0,0)$. A term involves a padded input if the input index $(i,j)$ has $i<0$ or $j<0$. The input index is given by $(-p-r, -q-s)$ where $p,q,r,s \\in \\{-1, 0, 1\\}$ (if kernel indices are relative to center). It is easier to count the non-padded terms, where $-p-r \\ge 0$ and $-q-s \\ge 0$, which is equivalent to $p+r \\le 0$ and $q+s \\le 0$.\n    -   Let's count pairs $(k_1, k_2)$ from $\\{-1,0,1\\}$ where $k_1+k_2 \\le 0$:\n        -   If $k_1 = -1$, $k_2$ can be $-1, 0, 1$ ($3$ choices).\n        -   If $k_1 = 0$, $k_2$ can be $-1, 0$ ($2$ choices).\n        -   If $k_1 = 1$, $k_2$ can be $-1$ ($1$ choice).\n        -   Total choices = $3+2+1=6$.\n    -   The number of non-padded terms is where both indices are non-negative, so we have $6$ choices for the row index pair $(p,r)$ and $6$ choices for the column index pair $(q,s)$. Total non-padded terms = $6 \\times 6 = 36$.\n    -   The number of terms involving padded inputs is the total minus the non-padded: $81 - 36 = 45$.\n    -   Comparing the two variants, Variant $1$ has $16$ padding-influenced computations at the border, while Variant $2$ has $45$. Thus, $45 > 16$, and Variant $2$ involves a larger number of such computations.\n\n### 4. Number of Nonlinearities on an Input-to-Output Path\n\nA \"single input-to-output path\" refers to the sequence of operations a signal undergoes from an input neuron to an output neuron. A Rectified Linear Unit (ReLU) is applied after each convolution.\n\n-   **Variant 1**: The path is Input $\\rightarrow$ Convolution $\\rightarrow$ ReLU $\\rightarrow$ Output. The signal passes through one nonlinearity.\n\n-   **Variant 2**: The path is Input $\\rightarrow$ Conv1 $\\rightarrow$ ReLU1 $\\rightarrow$ Conv2 $\\rightarrow$ ReLU2 $\\rightarrow$ Output. The signal passes through the first ReLU, is then processed by the second convolution, and finally passes through the second ReLU. The total number of nonlinearities experienced is two.\n\nTherefore, the number of nonlinearities is not the same in the two variants.\n\n### Evaluation of Options\n\n**A. Under stride $1$ and no dilation, two stacked $3 \\times 3$ convolutions have the same receptive field size as one $5 \\times 5$ convolution, namely $5 \\times 5$.**\nOur derivation in section 1 shows that the receptive field for two stacked $3 \\times 3$ convolutions is $5 \\times 5$, which is identical to that of a single $5 \\times 5$ convolution.\n**Verdict: Correct.**\n\n**B. The per-location MAC counts are equal between Variant $1$ and Variant $2$ exactly when $C_{\\text{in}} = \\dfrac{9}{16} F$, given that Variant $2$ uses $C_{\\text{in}} \\rightarrow F$ then $F \\rightarrow F$ convolutions and biases are ignored.**\nOur derivation in section 2 confirms this exact algebraic condition. The calculation $25 C_{\\text{in}} F = 9 C_{\\text{in}} F + 9 F^2$ directly leads to $C_{\\text{in}} = \\frac{9}{16}F$.\n**Verdict: Correct.**\n\n**C. With \"same\" zero padding at each convolution, for the top-left output position, Variant $2$ aggregates $81$ second-order kernel contributions, of which $45$ involve padded inputs, whereas Variant $1$ aggregates $25$ contributions, of which $16$ involve padded inputs; thus Variant $2$ involves a larger absolute number of padding-influenced computations at the border.**\nOur analysis in section 3 confirms all numerical claims. Variant 1: $25$ total contributions, $16$ padded. Variant 2: $81$ second-order contributions, $45$ padded. The conclusion that Variant $2$ involves a larger number of padding-influenced computations ($45 > 16$) is also correct.\n**Verdict: Correct.**\n\n**D. Because two kernels are composed, the receptive field of Variant $2$ becomes $7 \\times 7$.**\nThis is incorrect. As shown in section 1, the receptive field size is $5 \\times 5$. A $7 \\times 7$ receptive field would require, for example, three stacked $3 \\times 3$ convolutions.\n**Verdict: Incorrect.**\n\n**E. If a Rectified Linear Unit (ReLU) is placed after each convolution, the number of nonlinearities applied along an input-to-output path is the same in Variant $1$ and Variant $2$.**\nThis is incorrect. As shown in section 4, Variant 1 has one nonlinearity, while Variant 2 has two.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABC}$$", "id": "3137618"}, {"introduction": "Building on the concept of multi-scale processing, this practice investigates how different convolutional branches respond to input perturbations. Through a series of controlled computational experiments, you will empirically test the hypothesis that larger spatial kernels, such as a $3 \\times 3$ filter, offer greater robustness to image blurring compared to pointwise $1 \\times 1$ convolutions [@problem_id:3137579]. This hands-on coding task will help solidify your intuition about the functional properties of different filter sizes.", "problem": "Consider an Inception-style multi-branch module consisting of two linear convolutional branches: a pointwise branch and a spatial branch. The pointwise branch uses a $1 \\times 1$ convolution and the spatial branch uses a $3 \\times 3$ convolution. Let an image be a real-valued discrete array $I \\in \\mathbb{R}^{H \\times W}$. Let discrete two-dimensional convolution be denoted by $*$, and let the $L^2$ norm of an array $A$ be $\\|A\\|_2 = \\sqrt{\\sum_{i,j} A_{i,j}^2}$. Define a Gaussian blur operator as convolution with a discrete, normalized Gaussian kernel $G_\\sigma \\in \\mathbb{R}^{m \\times m}$, where $m = 2\\lceil 3\\sigma \\rceil + 1$, the kernel entries are given by $G_\\sigma[i,j] \\propto \\exp\\left(-\\frac{i^2 + j^2}{2\\sigma^2}\\right)$ for integer coordinates $i,j$ centered at zero, and the kernel is normalized so that $\\sum_{i,j} G_\\sigma[i,j] = 1$. For the special case $\\sigma = 0$, define $G_0$ to be the discrete delta kernel of size $1 \\times 1$, that is $G_0 = [1]$.\n\nLet the pointwise branch kernel be $K_{1} = [1]$ of size $1 \\times 1$, representing an identity mapping. Let the spatial branch kernel be the normalized box filter $K_{3} \\in \\mathbb{R}^{3 \\times 3}$ with entries $K_{3}[i,j] = \\frac{1}{9}$ for all $i,j \\in \\{-1,0,1\\}$.\n\nFor a given image $I$ and blur level $\\sigma$, define the pre-branch outputs\n$$\nY_{1,\\text{clean}} = I * K_{1}, \\quad Y_{3,\\text{clean}} = I * K_{3},\n$$\nand the post-blur outputs\n$$\nI_{\\text{blur}} = I * G_\\sigma, \\quad Y_{1,\\text{blur}} = I_{\\text{blur}} * K_{1}, \\quad Y_{3,\\text{blur}} = I_{\\text{blur}} * K_{3}.\n$$\nDefine the relative degradation of branch $b \\in \\{1,3\\}$ as\n$$\nD_b(I,\\sigma) = \\frac{\\|Y_{b,\\text{blur}} - Y_{b,\\text{clean}}\\|_2}{\\|Y_{b,\\text{clean}}\\|_2},\n$$\nwith the convention that if $\\|Y_{b,\\text{clean}}\\|_2 = 0$ then $D_b(I,\\sigma) = 0$.\n\nYour task is to write a complete program that, for a fixed test suite of images and blur levels, computes for each test case the boolean\n$$\n\\text{result} = \\left( D_3(I,\\sigma) \\le D_1(I,\\sigma) + \\tau \\right),\n$$\nwhere $\\tau$ is a numerical tolerance defined as $\\tau = 10^{-8}$.\n\nThe program must implement the following scientifically controlled blur experiments, using discrete convolution with symmetric boundary handling for all convolutions:\n\n- Image definitions on a grid of size $H = 64$, $W = 64$:\n  - Checkerboard image with block period $p$, defined by\n    $$\n    I[i,j] = \\left( \\left\\lfloor \\frac{i}{p} \\right\\rfloor + \\left\\lfloor \\frac{j}{p} \\right\\rfloor \\right) \\bmod 2 \\in \\{0,1\\}, \\quad \\text{then mapped to } \\{-1,1\\} \\text{ via } I \\leftarrow 2I - 1.\n    $$\n  - Vertical step image, defined by $I[i,j] = 0$ for $j < \\left\\lfloor \\frac{W}{2} \\right\\rfloor$ and $I[i,j] = 1$ otherwise.\n  - Impulse image, defined by $I[i,j] = 0$ for all $i,j$ except $I[\\left\\lfloor \\frac{H}{2} \\right\\rfloor, \\left\\lfloor \\frac{W}{2} \\right\\rfloor] = 1$.\n  - Zero-mean random noise image, defined by $I[i,j]$ independently sampled from the standard normal distribution, using a fixed seed for reproducibility.\n\n- Test suite with distinct cases designed to cover a general case, high-frequency sensitivity, edge-like structures, impulse response, strong blur on noise, and a boundary case:\n  1. Checkerboard with $p = 4$, $\\sigma = 1.0$.\n  2. Checkerboard with $p = 2$, $\\sigma = 2.0$.\n  3. Vertical step, $\\sigma = 1.5$.\n  4. Impulse, $\\sigma = 0.75$.\n  5. Noise with seed $0$, $\\sigma = 3.0$.\n  6. Checkerboard with $p = 8$, $\\sigma = 0.0$.\n\nFor each test case, compute the boolean result as defined above, using $\\tau = 10^{-8}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example\n$$\n[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6].\n$$\nThe only acceptable outputs are the literals $\\text{True}$ or $\\text{False}$ for each entry in the list. No physical units are involved, and no angles are used. Ensure all computations are performed using precisely defined discrete convolutions and Gaussian kernels as specified.", "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and computationally tractable. All definitions, constants, and conditions are provided, forming a self-contained and consistent problem. We may thus proceed with a reasoned solution.\n\nThe task is to determine, for a set of specific images $I$ and blur levels $\\sigma$, whether the relative degradation $D_3(I, \\sigma)$ of a spatial convolutional branch is less than or equal to the relative degradation $D_1(I, \\sigma)$ of a pointwise branch, plus a small tolerance $\\tau$. The core of the problem lies in the precise implementation of discrete two-dimensional convolutions, the generation of specified images and kernels, and the calculation of $L^2$ norms.\n\nLet us first clarify the quantities involved. The pointwise branch uses a $1 \\times 1$ identity kernel $K_1 = [1]$, and the spatial branch uses a $3 \\times 3$ normalized box filter $K_3$, where $K_3[i,j] = 1/9$. An input image $I$ is blurred by convolving it with a normalized Gaussian kernel $G_\\sigma$ to produce $I_{\\text{blur}} = I * G_\\sigma$.\n\nThe outputs of the branches for the clean image $I$ are:\n$$\nY_{1,\\text{clean}} = I * K_1 = I\n$$\n$$\nY_{3,\\text{clean}} = I * K_3\n$$\n\nThe outputs for the blurred image $I_{\\text{blur}}$ are:\n$$\nY_{1,\\text{blur}} = I_{\\text{blur}} * K_1 = I_{\\text{blur}}\n$$\n$$\nY_{3,\\text{blur}} = I_{\\text{blur}} * K_3\n$$\n\nThe relative degradation for each branch $b \\in \\{1, 3\\}$ is defined as the ratio of the $L^2$ norm of the change in the branch's output to the $L^2$ norm of the original output:\n$$\nD_b(I,\\sigma) = \\frac{\\|Y_{b,\\text{blur}} - Y_{b,\\text{clean}}\\|_2}{\\|Y_{b,\\text{clean}}\\|_2}\n$$\nThe problem specifies that $D_b(I,\\sigma) = 0$ if the denominator $\\|Y_{b,\\text{clean}}\\|_2$ is zero. The final comparison to be made is $D_3(I, \\sigma) \\le D_1(I, \\sigma) + \\tau$, where $\\tau = 10^{-8}$.\n\nThe solution is implemented as a series of computational steps for each test case specified in the problem statement.\n\n1.  **Image Generation**: For each test case, the corresponding image $I$ of size $H=64, W=64$ is generated according to its mathematical definition. This includes the checkerboard pattern with period $p$, the vertical step function, the single-pixel impulse, and a zero-mean random noise field from a seeded random number generator for reproducibility.\n\n2.  **Kernel Generation**:\n    *   The kernels $K_1$ and $K_3$ are constant throughout the experiments.\n    *   The Gaussian kernel $G_\\sigma$ is generated for each specified value of $\\sigma$. The kernel size is $m = 2 \\lceil 3\\sigma \\rceil + 1$. The entries are calculated using the formula $G_\\sigma[i,j] \\propto \\exp\\left(-\\frac{i^2 + j^2}{2\\sigma^2}\\right)$ over a grid of integer coordinates centered at $(0,0)$, followed by normalization to ensure the sum of entries is $1$. The special case $\\sigma=0$ results in the $1 \\times 1$ identity kernel $G_0 = [1]$.\n\n3.  **Convolution Operations**: All convolutions are performed using discrete two-dimensional convolution with symmetric boundary handling, as specified. This is realized computationally using the `scipy.signal.convolve2d` function with parameters `mode='same'` and `boundary='symm'`. The `'same'` mode ensures the output array has the same dimensions as the input image, and `'symm'` implements the specified boundary condition. The order of operations adheres strictly to the definitions: the input image $I$ is first blurred to $I_{\\text{blur}}$, which is then passed through the branch convolutions. While for infinite signals convolution is associative (i.e., $(A*B)*C = A*(B*C)$), this property does not strictly hold for discrete signals with boundary handling, so the prescribed order of operations $Y_{3,\\text{blur}} = (I*G_\\sigma)*K_3$ is followed meticulously.\n\n4.  **Degradation Calculation**:\n    *   First, the clean outputs $Y_{1,\\text{clean}}$ and $Y_{3,\\text{clean}}$ are computed.\n    *   Then, the blurred image $I_{\\text{blur}} = I * G_\\sigma$ is computed.\n    *   The blurred outputs $Y_{1,\\text{blur}}$ and $Y_{3,\\text{blur}}$ are then computed from $I_{\\text{blur}}$.\n    *   The $L^2$ norms are calculated using `numpy.linalg.norm`.\n    *   The degradations $D_1$ and $D_3$ are computed according to their definitions, with a check to handle cases where the denominator is close to zero, in which case the degradation is set to $0$.\n\n5.  **Final Comparison**: The boolean result is obtained by evaluating the expression $D_3 \\le D_1 + \\tau$. This process is repeated for all six test cases, and the results are collected into a list.\n\nThe overall procedure constitutes a well-defined numerical experiment to investigate the robustness of different convolutional filter structures to input perturbations, a concept central to the design of robust deep neural networks.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Performs a series of scientifically controlled blur experiments on different images\n    to compare the relative degradation of a pointwise and a spatial convolutional branch.\n    \"\"\"\n    # Define global parameters from the problem statement\n    H, W = 64, 64\n    tau = 1e-8\n\n    # Define the test suite\n    # Each case is a tuple: (image_type, parameter, sigma)\n    # The parameter is 'p' for checkerboard, 'seed' for noise, or None.\n    test_cases = [\n        ('checkerboard', 4, 1.0),\n        ('checkerboard', 2, 2.0),\n        ('vertical_step', None, 1.5),\n        ('impulse', None, 0.75),\n        ('noise', 0, 3.0),\n        ('checkerboard', 8, 0.0),\n    ]\n\n    results = []\n\n    # Process each test case\n    for image_type, param, sigma in test_cases:\n        \n        # --- 1. Generate Image I ---\n        if image_type == 'checkerboard':\n            p = param\n            # Create coordinate grids\n            i_coords, j_coords = np.mgrid[0:H, 0:W]\n            # Calculate block indices\n            i_blocks = i_coords // p\n            j_blocks = j_coords // p\n            # Create checkerboard pattern {0, 1}\n            I = ((i_blocks + j_blocks) % 2).astype(float)\n            # Map to {-1, 1}\n            I = 2 * I - 1\n        elif image_type == 'vertical_step':\n            I = np.zeros((H, W), dtype=float)\n            j_mid = W // 2\n            I[:, j_mid:] = 1.0\n        elif image_type == 'impulse':\n            I = np.zeros((H, W), dtype=float)\n            i_mid, j_mid = H // 2, W // 2\n            I[i_mid, j_mid] = 1.0\n        elif image_type == 'noise':\n            seed = param\n            rng = np.random.default_rng(seed)\n            I = rng.standard_normal((H, W))\n        \n        # --- 2. Define Convolution Kernels ---\n        K1 = np.array([[1.0]], dtype=float)\n        K3 = np.ones((3, 3), dtype=float) / 9.0\n        \n        # --- 3. Generate Gaussian Kernel G_sigma ---\n        if sigma == 0:\n            G_sigma = np.array([[1.0]], dtype=float)\n        else:\n            m_half = int(np.ceil(3 * sigma))\n            x = np.arange(-m_half, m_half + 1)\n            # Use 'ij' indexing to match matrix row/column convention\n            ii, jj = np.meshgrid(x, x, indexing='ij')\n            G_sigma = np.exp(-(ii**2 + jj**2) / (2 * sigma**2))\n            G_sigma /= np.sum(G_sigma)\n            \n        # --- 4. Compute Degradations D1 and D3 ---\n\n        # Pre-blur outputs (clean)\n        Y1_clean = I # Convolution with K1 (identity)\n        Y3_clean = convolve2d(I, K3, mode='same', boundary='symm')\n\n        # Blurred image\n        I_blur = convolve2d(I, G_sigma, mode='same', boundary='symm')\n\n        # Post-blur outputs\n        Y1_blur = I_blur # Convolution with K1 (identity)\n        Y3_blur = convolve2d(I_blur, K3, mode='same', boundary='symm')\n        \n        # Calculate degradation for Branch 1\n        norm_Y1_clean = np.linalg.norm(Y1_clean)\n        if np.isclose(norm_Y1_clean, 0):\n            D1 = 0.0\n        else:\n            norm_diff_Y1 = np.linalg.norm(Y1_blur - Y1_clean)\n            D1 = norm_diff_Y1 / norm_Y1_clean\n\n        # Calculate degradation for Branch 3\n        norm_Y3_clean = np.linalg.norm(Y3_clean)\n        if np.isclose(norm_Y3_clean, 0):\n            D3 = 0.0\n        else:\n            norm_diff_Y3 = np.linalg.norm(Y3_blur - Y3_clean)\n            D3 = norm_diff_Y3 / norm_Y3_clean\n            \n        # --- 5. Compute the final boolean result ---\n        is_less_or_equal = (D3 <= D1 + tau)\n        results.append(is_less_or_equal)\n        \n    # --- 6. Print the final results in the specified format ---\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function\nsolve()\n```", "id": "3137579"}, {"introduction": "This advanced practice addresses a subtle but significant implementation detail in multi-branch architectures: boundary effects caused by padding. You will investigate how naive zero-padding creates artifacts that cause discrepancies between branches, particularly near image edges [@problem_id:3137620]. By implementing a principled, padding-aware normalization scheme, you will learn how to mitigate these effects and ensure a more uniform and fair comparison of features extracted at different scales.", "problem": "You are asked to study cross-branch boundary effects in multi-branch Inception-style modules and to design a padding-aware normalization that reduces discrepancies near image edges. Work in a purely two-dimensional, single-channel, discrete setting, with no stochastic components. The computational goal is to implement functions that quantify the cross-branch variance at each pixel location for different convolution branches and to compare the naive zero-padded response to a padding-aware normalized response.\n\nYou will model an Inception-style block as a set of branches that each compute a uniform local average over a square receptive field of size $k \\times k$, where $k \\in \\{\\,1,3,5,7\\,\\}$. The input is a single-channel image represented as a real-valued matrix of height $H$ and width $W$. Each branch uses zero-padding to produce a \"same\" output of shape $H \\times W$. The local average of a branch at location $(i,j)$ with kernel size $k$ is defined as the arithmetic mean of the values in its $k \\times k$ receptive field centered at $(i,j)$. With naive zero-padding, out-of-bounds positions contribute the value $0$ to the arithmetic mean. This naive averaging introduces a location-dependent bias near the image boundary because the number of valid in-bounds contributions is smaller than $k^2$.\n\nYour task is to derive and implement a padding-aware normalization that removes this bias using only fundamental definitions. Start from the definition of discrete two-dimensional convolution and the arithmetic mean. Treat the boundary as an absence of data rather than the presence of zeros: for each pixel location, conceptually average only over those positions within the receptive field that lie inside the image domain. Use this principle to define a normalized branch response at each location and for each $k$. For each method (naive and padding-aware), compute the variance across branches at every pixel location, where the variance is defined as the population variance over the set of branch responses at that location. Denote by $B$ the number of branches and by $y_b(i,j)$ the response of branch $b$ at $(i,j)$; then the cross-branch variance at $(i,j)$ is the arithmetic mean of squared deviations from the cross-branch mean.\n\nDefine the \"edge band\" as all pixel locations within a Chebyshev distance less than or equal to $p_{\\max}$ from the boundary, where $p_{\\max} = \\lfloor \\max(K)/2 \\rfloor$ and $K = \\{\\,1,3,5,7\\,\\}$. Formally, this edge band is the set of $(i,j)$ such that $i \\lt p_{\\max}$ or $i \\ge H - p_{\\max}$ or $j \\lt p_{\\max}$ or $j \\ge W - p_{\\max}$. The complement inside the image is the interior.\n\nProgram specification:\n- Implement the naive zero-padded local average for each $k \\in \\{\\,1,3,5,7\\,\\}$.\n- Implement the padding-aware normalized local average for each $k$ by averaging exclusively over in-bounds pixels of each receptive field at every location, as derived from first principles. Do not infer or assume any specialized \"shortcut\" formulas beyond basic counting and the definition of the arithmetic mean.\n- For each method, compute the per-pixel cross-branch variance.\n- For each test image below, compute the mean of the cross-branch variance over the edge band only (exclude the interior), for both methods.\n- For each test image, return three real numbers: the mean edge variance for the naive method, the mean edge variance for the padding-aware method, and the ratio of the latter divided by the former. If the denominator is $0$, define the ratio to be $0$.\n- Round each reported real number to exactly six digits after the decimal point.\n\nUse the following four test images, each deterministic and defined purely in mathematical terms:\n- Test case $1$ (constant field): $H = 7$, $W = 7$, and $I(i,j) = 3$ for all $0 \\le i \\lt H$, $0 \\le j \\lt W$.\n- Test case $2$ (additive gradient): $H = 8$, $W = 5$, and $I(i,j) = i + j$ for all $0 \\le i \\lt H$, $0 \\le j \\lt W$.\n- Test case $3$ (impulse): $H = 3$, $W = 3$, and $I(i,j) = 1$ if $(i,j) = (1,1)$ and $I(i,j) = 0$ otherwise.\n- Test case $4$ (checkerboard): $H = 6$, $W = 6$, and $I(i,j) = 1$ if $(i + j)$ is even and $I(i,j) = 0$ if $(i + j)$ is odd.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be in the order of the test cases, with three rounded floats per test case: $[\\text{naive}_1,\\text{norm}_1,\\text{ratio}_1,\\text{naive}_2,\\text{norm}_2,\\text{ratio}_2,\\text{naive}_3,\\text{norm}_3,\\text{ratio}_3,\\text{naive}_4,\\text{norm}_4,\\text{ratio}_4]$.\n- All numbers must be printed with exactly six digits after the decimal point.", "solution": "The problem statement is valid. It is scientifically grounded in the principles of discrete convolution and statistical variance, well-posed with specific inputs and deterministic objectives, and objectively defined.\n\nThe task is to analyze and mitigate boundary artifacts in a simplified multi-branch convolutional module, akin to an Inception block in deep learning. The module comprises four parallel branches, each performing a local averaging operation with a different square kernel size $k \\in K = \\{1, 3, 5, 7\\}$. We will derive and implement two methods for this averaging: a naive zero-padded approach and a proposed padding-aware normalized approach. Subsequently, we will quantify the discrepancy between branches by computing the cross-branch variance at each pixel and analyzing its mean value within a defined \"edge band.\"\n\n### 1. Mathematical Formulation of Local Averaging Methods\n\nLet the input be a single-channel image represented by a real-valued matrix $I$ of size $H \\times W$. The output of a branch with kernel size $k$ is a matrix $Y_k$ of the same dimensions. Let $p = \\lfloor k/2 \\rfloor$ be the padding required for a symmetric $k \\times k$ kernel to produce a \"same\" output.\n\n#### a. Naive Zero-Padded Local Average\n\nIn standard zero-padded convolution, any part of the receptive field that falls outside the image domain is treated as having a value of $0$. The local average is computed by summing the values in the $k \\times k$ receptive field and dividing by the total area of the kernel, $k^2$.\n\nLet $I_{\\text{padded}}(u,v)$ be a function that returns $I(u,v)$ if the coordinates $(u,v)$ are within the image bounds ($0 \\le u < H$, $0 \\le v < W$) and $0$ otherwise. The naive response $Y_k^{\\text{naive}}$ at location $(i,j)$ is:\n$$ Y_k^{\\text{naive}}(i,j) = \\frac{1}{k^2} \\sum_{u=i-p}^{i+p} \\sum_{v=j-p}^{j+p} I_{\\text{padded}}(u,v) $$\nThis method introduces a systematic bias near the image boundaries because the denominator $k^2$ remains constant while the number of actual image pixels contributing to the sum decreases. This leads to an artificial darkening or lightening effect at the edges, depending on the image content and fill value.\n\n#### b. Padding-Aware Normalized Local Average\n\nA more principled approach is to compute the arithmetic mean using only the valid pixel data available within the receptive field at each location. The zeros from padding represent an absence of data and should not be included in the averaging calculation.\n\nLet $\\mathcal{D} = [0, H-1] \\times [0, W-1]$ be the image domain. For a kernel of size $k$ centered at $(i,j)$, the receptive field is $\\mathcal{R}_k(i,j) = [i-p, i+p] \\times [j-p, j+p]$. The set of valid pixels is the intersection $\\mathcal{V}_k(i,j) = \\mathcal{R}_k(i,j) \\cap \\mathcal{D}$. The padding-aware normalized response $Y_k^{\\text{norm}}$ is defined by the true arithmetic mean over this valid set:\n$$ Y_k^{\\text{norm}}(i,j) = \\frac{\\sum_{(u,v) \\in \\mathcal{V}_k(i,j)} I(u,v)}{\\left| \\mathcal{V}_k(i,j) \\right|} $$\nHere, the sum is taken only over pixels within the image boundaries, and the denominator is the exact count of such pixels. This definition ensures that the local average is not biased by out-of-bounds regions, correctly reflecting the mean of the available data. For example, on a constant image $I(i,j)=C$, this method yields $Y_k^{\\text{norm}}(i,j)=C$ everywhere, unlike the naive method.\n\n### 2. Implementation via Integral Images\n\nTo implement these calculations efficiently without specialized libraries, we use the integral image, or summed-area table (SAT). A SAT allows the sum over any rectangular region to be computed in constant time. We construct a SAT, denoted $\\text{sat}(I)$, for an image $I$.\n\nFor the **naive method**, we first create a zero-padded image $I_{\\text{pad}}$ with a border of width $p_{\\max} = \\lfloor \\max(K)/2 \\rfloor = 3$. We then compute its SAT. The sum over any $k \\times k$ window can be queried from this SAT to calculate $Y_k^{\\text{naive}}$.\n\nFor the **padding-aware method**, we compute two SATs from the original image $I$: one for the image itself, $\\text{sat}(I)$, and one for a matrix of ones of the same size, $\\text{sat}(\\mathbf{1}_{H \\times W})$. For each pixel $(i,j)$ and kernel $k$, we determine the valid rectangular sub-window $\\mathcal{V}_k(i,j)$ and use $\\text{sat}(I)$ to find the sum of its elements and $\\text{sat}(\\mathbf{1}_{H \\times W})$ to find its area (pixel count).\n\n### 3. Cross-Branch Variance and Edge Analysis\n\nAfter computing the responses $\\{Y_k^{\\text{naive}}\\}$ and $\\{Y_k^{\\text{norm}}\\}$ for all $k \\in K$, we quantify the disagreement between branches. At each pixel $(i,j)$, we have a set of four responses, $\\mathcal{Y}(i,j) = \\{Y_k(i,j)\\}_{k \\in K}$. The cross-branch variance is the population variance of this set:\n$$ V(i,j) = \\frac{1}{4} \\sum_{k \\in K} \\left( Y_k(i,j) - \\mu_{\\mathcal{Y}}(i,j) \\right)^2, \\quad \\text{where} \\quad \\mu_{\\mathcal{Y}}(i,j) = \\frac{1}{4} \\sum_{k \\in K} Y_k(i,j) $$\nThis calculation yields two variance maps, $V^{\\text{naive}}$ and $V^{\\text{norm}}$.\n\nFinally, we compute the mean of these variance values over the specified \"edge band,\" which includes all pixels $(i,j)$ such that $i < p_{\\max}$ or $i \\ge H - p_{\\max}$ or $j < p_{\\max}$ or $j \\ge W - p_{\\max}$, with $p_{\\max} = 3$. This allows us to aggregate the boundary effect into a single scalar metric for each method and compare their performance via their ratio.", "answer": "```python\nimport numpy as np\n\ndef _compute_sat(matrix):\n    \"\"\"Computes the summed-area table (integral image) for a 2D matrix.\"\"\"\n    H, W = matrix.shape\n    sat = np.zeros((H + 1, W + 1), dtype=np.float64)\n    sat[1:, 1:] = np.cumsum(np.cumsum(matrix, axis=0), axis=1)\n    return sat\n\ndef _query_sat(sat, r1, c1, r2, c2):\n    \"\"\"Queries the sum of a rectangle (r1, c1) to (r2, c2) inclusive.\"\"\"\n    # The SAT is padded, so coordinates correspond directly.\n    # sat[r, c] stores sum of matrix[0:r, 0:c]\n    if r1 > r2 or c1 > c2:\n        return 0.0\n    # Correct indexing for the padded SAT\n    # sum = D - B - C + A\n    # A = sat[r1, c1], B = sat[r1, c2+1], C = sat[r2+1, c1], D = sat[r2+1, c2+1]\n    return sat[r2 + 1, c2 + 1] - sat[r1, c2 + 1] - sat[r2 + 1, c1] + sat[r1, c1]\n\ndef _process_image(I):\n    \"\"\"\n    Computes naive and normalized edge variances for a given image.\n    \"\"\"\n    H, W = I.shape\n    kernels = [1, 3, 5, 7]\n    p_max = max(kernels) // 2\n\n    # --- 1. Naive Zero-Padded Method ---\n    I_padded = np.pad(I, pad_width=p_max, mode='constant', constant_values=0)\n    sat_padded = _compute_sat(I_padded)\n    \n    naive_responses = []\n    for k in kernels:\n        p_k = k // 2\n        Y_k_naive = np.zeros((H, W), dtype=np.float64)\n        for i in range(H):\n            for j in range(W):\n                # Window coordinates in the padded image\n                r1_pad, c1_pad = i + p_max - p_k, j + p_max - p_k\n                r2_pad, c2_pad = r1_pad + k - 1, c1_pad + k - 1\n                \n                window_sum = _query_sat(sat_padded, r1_pad, c1_pad, r2_pad, c2_pad)\n                Y_k_naive[i, j] = window_sum / (k * k)\n        naive_responses.append(Y_k_naive)\n    \n    # --- 2. Padding-Aware Normalized Method ---\n    sat_I = _compute_sat(I)\n    sat_ones = _compute_sat(np.ones_like(I, dtype=np.float64))\n\n    norm_responses = []\n    for k in kernels:\n        p_k = k // 2\n        Y_k_norm = np.zeros((H, W), dtype=np.float64)\n        for i in range(H):\n            for j in range(W):\n                # Window coordinates in the original image, need to be clipped\n                r1, c1 = i - p_k, j - p_k\n                r2, c2 = i + p_k, j + p_k\n                \n                # Clipped coordinates for valid region\n                r1_clip, c1_clip = max(0, r1), max(0, c1)\n                r2_clip, c2_clip = min(H - 1, r2), min(W - 1, c2)\n                \n                window_sum = _query_sat(sat_I, r1_clip, c1_clip, r2_clip, c2_clip)\n                pixel_count = _query_sat(sat_ones, r1_clip, c1_clip, r2_clip, c2_clip)\n                \n                if pixel_count > 0:\n                    Y_k_norm[i, j] = window_sum / pixel_count\n        norm_responses.append(Y_k_norm)\n\n    # --- 3. Variance and Edge Analysis ---\n    naive_stack = np.stack(naive_responses, axis=0)\n    norm_stack = np.stack(norm_responses, axis=0)\n\n    V_naive = np.var(naive_stack, axis=0)\n    V_norm = np.var(norm_stack, axis=0)\n\n    rows, cols = np.ogrid[:H, :W]\n    edge_mask = (rows < p_max) | (rows >= H - p_max) | \\\n                (cols < p_max) | (cols >= W - p_max)\n    \n    if not np.any(edge_mask): # Should not happen with given test cases\n        mean_V_naive_edge = 0.0\n        mean_V_norm_edge = 0.0\n    else:\n        mean_V_naive_edge = np.mean(V_naive[edge_mask])\n        mean_V_norm_edge = np.mean(V_norm[edge_mask])\n\n    ratio = 0.0\n    if mean_V_naive_edge != 0:\n        ratio = mean_V_norm_edge / mean_V_naive_edge\n\n    return mean_V_naive_edge, mean_V_norm_edge, ratio\n\ndef solve():\n    test_cases_defs = [\n        {'H': 7, 'W': 7, 'func': lambda i, j: 3.0},\n        {'H': 8, 'W': 5, 'func': lambda i, j: i + j},\n        {'H': 3, 'W': 3, 'func': lambda i, j: 1.0 * ((i == 1) & (j == 1))},\n        {'H': 6, 'W': 6, 'func': lambda i, j: 1.0 * ((i + j) % 2 == 0)}\n    ]\n\n    results = []\n    for case_def in test_cases_defs:\n        H, W = case_def['H'], case_def['W']\n        image_func = case_def['func']\n        image = np.fromfunction(np.vectorize(image_func), (H, W), dtype=int).astype(np.float64)\n        \n        naive_var, norm_var, ratio = _process_image(image)\n        \n        results.append(naive_var)\n        results.append(norm_var)\n        results.append(ratio)\n        \n    # Format output as specified: comma-separated list in brackets, 6 decimal places.\n    # The f-string formatting handles rounding appropriately for display.\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3137620"}]}