{"hands_on_practices": [{"introduction": "The relationship between input size, kernel size, padding, and stride is fundamental to designing any convolutional neural network. While a standard formula is often provided, true mastery comes from understanding how it arises from the mechanics of kernel placement. This first practice challenges you to derive the constraints on padding from first principles, ensuring you can precisely control the output dimensions of a convolutional layer. By working through this derivation [@problem_id:3177669], you will build a robust intuition for how these parameters interact, a skill essential for debugging network architectures and designing custom layers.", "problem": "A one-dimensional (1D) discrete convolution layer in deep learning takes an input of length $n$, applies a kernel of width $k$, uses a stride $s \\in \\mathbb{N}$, and symmetric zero padding of $p \\in \\mathbb{Z}_{\\ge 0}$ elements on both the left and right. Assume standard, contiguous sampling with stride (no dilation), and that an output is produced only when the entire kernel window lies within the padded input.\n\nStarting only from the indexing definition of valid kernel placements under stride, derive constraints on $p$ that guarantee the output length is exactly a prescribed target $m \\in \\mathbb{N}$. Express these constraints explicitly as inequalities in terms of $n$, $k$, $s$, $p$, and $m$, and then solve those inequalities for $p$ to obtain a closed-form characterization (involving integer bounds) of all feasible $p$. Discuss feasibility subtleties for $s>1$, including the parity implicit in $2p$, and explain the boundary edge case where the final stride-aligned kernel placement lands exactly at the end of the padded input.\n\nThen, for the specific configuration $n=37$, $k=7$, $s=3$, and target $m=12$, determine whether a feasible nonnegative integer $p$ exists and, if so, compute the smallest such $p$. If no feasible $p$ exists, state that there is no solution. Your final reported answer must be the single smallest integer $p$ (no units and no rounding instructions are needed for this integer quantity).", "solution": "The problem asks for a derivation of the constraints on symmetric zero padding $p$ to achieve a specific output length $m$ for a one-dimensional convolution, and then to apply this derivation to a specific numerical case.\n\nFirst, we formalize the convolution operation based on the provided givens.\nAn input of length $n$ is padded with $p$ zeros on each side. The resulting padded input has a total length of $n_{padded} = n + 2p$. Let us use $0$-based indexing for the elements of this padded sequence, from index $0$ to $n+2p-1$.\n\nA kernel of width $k$ is applied to this padded input. An output is produced by placing the kernel at different positions. The stride $s$ dictates the step size between consecutive kernel placements. Let the output elements be indexed by $j$, where $j \\in \\{0, 1, 2, \\ldots, m-1\\}$, since the target output length is $m$.\n\nThe starting position of the kernel for the $j$-th output (where $j=0$ corresponds to the first output) is given by $j \\cdot s$. The kernel window then covers the indices from $j \\cdot s$ to $j \\cdot s + k - 1$ in the padded input.\n\nThe problem states that an output is produced only when the entire kernel window lies within the padded input. This imposes two boundary conditions on any valid starting position $i_{start}$:\n1. The start of the kernel, $i_{start}$, must be at or after the beginning of the padded input: $i_{start} \\ge 0$.\n2. The end of the kernel, $i_{start} + k - 1$, must be at or before the end of the padded input: $i_{start} + k - 1 \\le n+2p-1$.\n\nThe second condition can be rewritten as $i_{start} \\le n+2p-k$.\nSince the starting positions are given by $j \\cdot s$ where $j \\ge 0$ and $s \\in \\mathbb{N}$, the first condition $j \\cdot s \\ge 0$ is always satisfied. Thus, the sole constraint on a valid placement for the $j$-th output is:\n$$ j \\cdot s \\le n+2p-k $$\n\nFor the output length to be exactly $m$, two conditions must be met simultaneously:\n1. An output must be produced for index $j=m-1$. This means the placement for the $m$-th output is valid.\n   $$ (m-1)s \\le n+2p-k $$\n2. An output must *not* be produced for index $j=m$. This means the placement for a hypothetical $(m+1)$-th output is invalid.\n   $$ m \\cdot s > n+2p-k $$\n\nCombining these two inequalities gives a single compound inequality that constrains the effective length of the feature map available for convolution, $n+2p-k$:\n$$ (m-1)s \\le n+2p-k < ms $$\n\nTo find the constraints on $p$, we solve this compound inequality for $p$.\nFrom the left side:\n$(m-1)s \\le n+2p-k \\implies (m-1)s - n + k \\le 2p \\implies p \\ge \\frac{(m-1)s - n + k}{2}$.\nFrom the right side:\n$n+2p-k < ms \\implies 2p < ms - n + k \\implies p < \\frac{ms - n + k}{2}$.\n\nThus, the closed-form characterization of all feasible values of $p$ is given by:\n$$ \\frac{(m-1)s - n + k}{2} \\le p < \\frac{ms - n + k}{2} $$\nSince $p$ must be a non-negative integer ($p \\in \\mathbb{Z}_{\\ge 0}$), a valid solution exists only if this interval $[\\frac{(m-1)s - n + k}{2}, \\frac{ms - n + k}{2})$ contains at least one non-negative integer.\n\nNow, we discuss feasibility subtleties for $s>1$.\nLet's analyze the interval for $2p$:\n$$ (m-1)s - n + k \\le 2p < ms - n + k $$\nLet $A = (m-1)s - n + k$. The condition is $A \\le 2p < A+s$. We are seeking a non-negative even integer, $2p$, within the interval $[A, A+s)$. The length of this interval is $s$.\nIf $s=1$, the interval is $[A, A+1)$, which contains exactly one integer, $A$. For a solution to exist, $A$ must be an even, non-negative integer. This is a very restrictive condition.\nIf $s>1$, the interval $[A, A+s)$ has length $s \\ge 2$. For any integer $A$, the interval $[A, A+s)$ is guaranteed to contain at least one even integer. For example, if $A$ is even, $A$ itself is a candidate. if $A$ is odd, $A+1$ is even, and since $s \\ge 2$, $A+1 < A+s$, so $A+1$ is in the interval.\nTherefore, for $s > 1$, an integer $p$ satisfying the bounds is always guaranteed to exist. The only remaining condition is that $p$ must be non-negative. This means the interval $[A, A+s)$ must contain at least one non-negative even integer. This is assured as long as the upper bound of the interval for $2p$ is positive, i.e., $A+s > 0$, which is $ms - n + k > 0$.\n\nThe parity of $2p$ being even is implicitly handled by the formulation. The term $2p$ must be an even integer. The expression $(m-1)s - n + k$ can be either even or odd. If it is even, then the boundary edge case is possible.\n\nThe boundary edge case, where the final stride-aligned kernel placement lands exactly at the end of the padded input, occurs when the inequality $(m-1)s \\le n+2p-k$ is satisfied with equality:\n$$ (m-1)s = n+2p-k $$\nSolving for $p$, we get $p = \\frac{(m-1)s - n + k}{2}$. For this precise alignment to be possible, the value of $p$ must be a non-negative integer. This requires the numerator, $(m-1)s - n + k$, to be a non-negative even integer.\n\nFinally, we apply these results to the specific configuration: $n=37$, $k=7$, $s=3$, and target $m=12$.\nWe need to find the smallest non-negative integer $p$ that satisfies the derived inequalities.\nSubstituting the given values into the interval for $p$:\nLower bound:\n$$ p \\ge \\frac{(12-1) \\cdot 3 - 37 + 7}{2} = \\frac{11 \\cdot 3 - 37 + 7}{2} = \\frac{33 - 37 + 7}{2} = \\frac{-4 + 7}{2} = \\frac{3}{2} = 1.5 $$\nUpper bound:\n$$ p < \\frac{12 \\cdot 3 - 37 + 7}{2} = \\frac{36 - 37 + 7}{2} = \\frac{-1 + 7}{2} = \\frac{6}{2} = 3 $$\nSo, the condition on $p$ is:\n$$ 1.5 \\le p < 3 $$\nWe are looking for the smallest integer $p$ that satisfies this condition. The integers in the half-open interval $[1.5, 3)$ are $\\{2\\}$. The only integer solution is $p=2$. Since $p=2$ is a non-negative integer, it is a feasible padding value. As it is the only integer solution in the valid range, it is also the smallest.\n\nTherefore, for the given configuration, a feasible non-negative integer $p$ exists, and the smallest such value is $2$.", "answer": "$$\\boxed{2}$$", "id": "3177669"}, {"introduction": "Beyond simply controlling feature map dimensions, stride and padding choices have profound implications for spatial alignment, especially in architectures like U-Nets that rely on skip connections. In tasks like semantic segmentation, precise correspondence between encoder and decoder features is critical for high-quality results. This exercise [@problem_id:3180119] introduces a formal coordinate mapping system to track how receptive field centers shift through a network, allowing you to quantify the pixel-level misalignment caused by seemingly innocuous choices like using an even-sized kernel. Mastering this analysis is key to designing robust and accurate segmentation models.", "problem": "Consider a Two-Dimensional (2D) U-shaped Convolutional Neural Network (U-Net) that performs one downsampling stage and one upsampling stage. The network uses the cross-correlation form of the two-dimensional discrete convolution, where for an input feature map $X$ and a kernel $W$ of size $k_{x} \\times k_{y}$, the output feature map $Y$ is given (along each spatial axis independently) by\n$$\nY[i,j] \\;=\\; \\sum_{u=0}^{k_{x}-1} \\sum_{v=0}^{k_{y}-1} W[u,v] \\, X[s_{x} i - p_{x} + u, \\, s_{y} j - p_{y} + v].\n$$\nAssume the following conventions and parameters:\n- The stride along each axis for standard convolutions is $s_{x} = s_{y} = 1$.\n- The padding implements the commonly used “same” rule $p_{x} = \\left\\lfloor \\frac{k_{x}}{2} \\right\\rfloor$ and $p_{y} = \\left\\lfloor \\frac{k_{y}}{2} \\right\\rfloor$.\n- The index of the kernel’s discrete center along each axis is $c_{x} = \\left\\lfloor \\frac{k_{x}-1}{2} \\right\\rfloor$ and $c_{y} = \\left\\lfloor \\frac{k_{y}-1}{2} \\right\\rfloor$.\n- Two-dimensional max pooling uses a window of size $2 \\times 2$, stride $2$, and zero padding along both axes. Treat the representative location of a pooled output as the center-of-window determined by $(c_{x}, c_{y})$ defined above.\n- Two-dimensional transposed convolution (fractionally strided convolution) with kernel size $2 \\times 2$ and stride $2$ along each axis is implemented by the standard deep learning definition equivalent to inserting zeros between inputs and then applying a cross-correlation with the given kernel; its padding is $p_{x} = p_{y} = 0$, and its center indices are $c_{x} = c_{y} = \\left\\lfloor \\frac{2-1}{2} \\right\\rfloor$.\n\nDefine a coordinate mapping from the index $(i,j)$ of any intermediate feature map back to the input image’s coordinate system by a pair $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$, where $S_{x}$ and $S_{y}$ are the cumulative strides and $(\\Delta_{x}, \\Delta_{y})$ are the cumulative offsets in input pixels. For a standard convolution or pooling layer with stride $(s_{x}, s_{y})$, padding $(p_{x}, p_{y})$, and center indices $(c_{x}, c_{y})$ applied to an input with mapping $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$, the output mapping is\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; S_{x}^{\\mathrm{in}} \\, s_{x}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; S_{y}^{\\mathrm{in}} \\, s_{y}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + S_{y}^{\\mathrm{in}} (c_{y} - p_{y}).\n$$\nFor a transposed convolution with stride $(s_{x}, s_{y})$, padding $(p_{x}, p_{y})$, and center indices $(c_{x}, c_{y})$, applied to an input with mapping $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$, the output mapping is\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; \\frac{S_{x}^{\\mathrm{in}}}{s_{x}}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; \\frac{S_{y}^{\\mathrm{in}}}{s_{y}}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + \\frac{S_{y}^{\\mathrm{in}}}{s_{y}} (p_{y} - c_{y}).\n$$\n\nConsider the following minimal U-Net path:\n1. Encoder stage: a $3 \\times 3$ standard convolution with “same” padding, followed by $2 \\times 2$ max pooling with stride $2$ and zero padding.\n2. Decoder stage: a $2 \\times 2$ transposed convolution with stride $2$ and zero padding, followed by a standard convolution whose kernel is either $3 \\times 3$ (odd) or $2 \\times 2$ (even), with “same” padding.\n\nA skip connection concatenates the encoder’s feature map immediately after the first $3 \\times 3$ convolution with the decoder’s feature map immediately after the final standard convolution. Using the coordinate mapping rules above, derive the cumulative $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$ for both the encoder feature and the decoder feature, and use them to compute the spatial misalignment vector between these two features at the skip connection when the decoder’s final standard convolution uses a $2 \\times 2$ kernel with “same” padding. Then, report the Euclidean magnitude of this misalignment vector, measured in pixels at the skip-connection resolution, as a single real-valued number. If no misalignment arises, the magnitude is zero. Provide only the magnitude as your final answer.", "solution": "This problem requires us to track the spatial coordinate system of feature maps through a simplified U-Net to determine the misalignment at a skip connection. A coordinate mapping $(S_x, S_y, \\Delta_x, \\Delta_y)$ allows us to find the corresponding center of the receptive field in the original input image for any pixel $(i,j)$ in a feature map: $(x, y) = (i \\cdot S_x + \\Delta_x, j \\cdot S_y + \\Delta_y)$.\n\nThe initial input image has an identity mapping: $(S^{(0)}, \\Delta^{(0)}) = (1, 1, 0, 0)$.\n\n**1. Encoder Feature Map (for the Skip Connection)**\n\nThe first feature map in the skip connection is the output of the first encoder layer: a $3 \\times 3$ standard convolution.\n- Input map: $(1, 1, 0, 0)$.\n- Operation: Standard convolution with kernel $k=3$, stride $s=1$, and \"same\" padding $p = \\lfloor 3/2 \\rfloor = 1$. The kernel center is $c = \\lfloor (3-1)/2 \\rfloor = 1$.\n- We apply the update rule for standard convolutions (the logic is identical for both x and y axes):\n  - $S_{out} = S_{in} \\cdot s = 1 \\cdot 1 = 1$.\n  - $\\Delta_{out} = \\Delta_{in} + S_{in} (c - p) = 0 + 1(1 - 1) = 0$.\nThe coordinate map for the encoder feature is $(S_E, \\Delta_E) = (1, 1, 0, 0)$.\n\n**2. Decoder Feature Map (for the Skip Connection)**\n\nThis feature map is generated by passing the data through the rest of the U-Net path. We trace the coordinate map transformations step-by-step.\n\n**Step 2a: Max Pooling (Bottleneck)**\nThe input is the encoder feature map we just calculated: $(1, 1, 0, 0)$.\n- Operation: Max pooling with window $k=2$, stride $s=2$, and padding $p=0$. The window center is $c = \\lfloor (2-1)/2 \\rfloor = 0$.\n  - $S_{out} = S_{in} \\cdot s = 1 \\cdot 2 = 2$.\n  - $\\Delta_{out} = \\Delta_{in} + S_{in} (c - p) = 0 + 1(0 - 0) = 0$.\nThe map at the bottleneck is $(2, 2, 0, 0)$.\n\n**Step 2b: Transposed Convolution (Upsampling)**\nThe input is the bottleneck map: $(2, 2, 0, 0)$.\n- Operation: Transposed convolution with kernel $k=2$, stride $s=2$, and padding $p=0$. The kernel center is $c = \\lfloor (2-1)/2 \\rfloor = 0$.\n- We apply the update rule for transposed convolutions:\n  - $S_{out} = S_{in} / s = 2 / 2 = 1$.\n  - $\\Delta_{out} = \\Delta_{in} + (S_{in} / s) \\cdot (p - c) = 0 + (2/2) \\cdot (0 - 0) = 0$.\nThe map after upsampling is $(1, 1, 0, 0)$.\n\n**Step 2c: Final Decoder Convolution**\nThe input is the upsampled map: $(1, 1, 0, 0)$.\n- Operation: Standard convolution with an even-sized kernel $k=2$, stride $s=1$, and \"same\" padding $p = \\lfloor 2/2 \\rfloor = 1$. The kernel center is $c = \\lfloor (2-1)/2 \\rfloor = 0$.\n  - $S_{out} = S_{in} \\cdot s = 1 \\cdot 1 = 1$.\n  - $\\Delta_{out} = \\Delta_{in} + S_{in} (c - p) = 0 + 1(0 - 1) = -1$.\nThe final coordinate map for the decoder feature is $(S_D, \\Delta_D) = (1, 1, -1, -1)$.\n\n**3. Calculating the Misalignment**\n\nWe have the two coordinate maps at the point of concatenation:\n- Encoder feature: $(S_E, \\Delta_E) = (1, 1, 0, 0)$.\n- Decoder feature: $(S_D, \\Delta_D) = (1, 1, -1, -1)$.\n\nFor a given pixel at index $(i,j)$ in the feature maps to be concatenated:\n- The encoder pixel corresponds to input location $(x_E, y_E) = (i \\cdot 1 + 0, j \\cdot 1 + 0) = (i, j)$.\n- The decoder pixel corresponds to input location $(x_D, y_D) = (i \\cdot 1 - 1, j \\cdot 1 - 1) = (i-1, j-1)$.\n\nThe spatial misalignment vector in the input's pixel grid is the difference:\n$\\vec{v} = (x_E - x_D, y_E - y_D) = (i - (i-1), j - (j-1)) = (1, 1)$.\n\nThe problem asks for the magnitude \"at the skip-connection resolution\". At this point, the cumulative stride for both features is $S_x=S_y=1$, meaning one pixel in the feature map corresponds to one pixel in the input image. Thus, the misalignment vector in this resolution is also $(1, 1)$.\n\nThe Euclidean magnitude is $|\\vec{v}| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$. This misalignment is a direct consequence of using an even-sized ($2 \\times 2$) kernel in the decoder, which creates a half-pixel shift that is not present in the odd-kerneled encoder path.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3180119"}, {"introduction": "Upsampling is as crucial as downsampling in many modern CNNs, but the standard method—transposed convolution—is notorious for producing 'checkerboard' visual artifacts. These patterns are not random noise; they are a direct symptom of how the operation distributes information across the output grid. This practice [@problem_id:3177691] provides a diagnostic framework to investigate the root cause of these artifacts: the relationship between the kernel size $k$ and the stride $s$. By analyzing the uniformity of kernel overlap, you will gain the insight needed to diagnose, mitigate, and design upsampling paths that avoid this common pitfall.", "problem": "You are analyzing why checkerboard artifacts appear when using a transposed convolution (often called a deconvolution) to upsample intermediate feature maps in a deep Convolutional Neural Network (CNN). Consider a one-dimensional ($1$D) transposed convolution layer with stride $s>1$, kernel length $k$, no bias, and a single input and output channel. Focus on the interior (ignore boundaries). To isolate pure coverage effects, consider the following diagnostic: set every input activation to $1$ and every kernel weight to $1$.\n\nIn this setting, the output at a location equals the total number of kernel taps that land on that location when shifted copies of the kernel are placed on an output grid spaced by the stride $s$. This total can vary with position depending on $s$, $k$, and the padding alignment.\n\nBased on first principles reasoning about how transposed convolution distributes kernel support over the output grid, and using only the above diagnostic setup, choose all statements that are correct:\n\nA. For stride $s=2$ and kernel length $k=3$, with any interior-aligned padding, the interior output exhibits a periodic alternation (for example, $\\ldots,2,1,2,1,\\ldots$) under the all-ones diagnostic, indicating uneven coverage that explains checkerboard artifacts.\n\nB. If $k$ is a multiple of $s$ and padding is chosen so that kernel footprints are aligned to the output grid without fractional offsets, then, away from boundaries, each output position receives exactly the same number of contributions under the all-ones diagnostic; the interior output becomes spatially uniform, mitigating checkerboard artifacts.\n\nC. Replacing the transposed convolution by explicit upsampling by a factor of $s$ using nearest-neighbor or bilinear interpolation, followed by a standard convolution with stride $1$, eliminates interior checkerboard artifacts for any $k$ because the convolution then has uniform coverage.\n\nD. Increasing the zero-padding width in the transposed convolution is sufficient to eliminate checkerboard artifacts everywhere, including the interior, regardless of $k$ and $s$.\n\nE. Adjusting the “output padding” parameter (which only changes the output length by at most $s-1$) can remove the interior periodicity when $k$ is not divisible by $s$.\n\nSelect all that apply. Then briefly justify your choice by linking the phenomenon to the interaction between stride $s$, kernel support $k$, and padding choices. You may also comment on how this extends to the two-dimensional ($2$D) case and suggest practical fixes consistent with your choices.", "solution": "This problem analyzes the root cause of checkerboard artifacts in transposed convolutions. The key insight is that these artifacts are a direct result of **uneven kernel overlap**. The provided diagnostic—setting all inputs and weights to 1—allows us to visualize this: the output value at any location is simply the number of times it is covered by a kernel footprint. Artifacts are eliminated if and only if this coverage is uniform across all interior positions.\n\nA transposed convolution with stride $s$ and kernel size $k$ works by placing a copy of the kernel for each input, with each placement shifted by $s$ positions on the output grid. An output position is covered by multiple kernels if their footprints overlap. The pattern of this overlap is determined by the relationship between $k$ and $s$.\n\nA rigorous analysis shows that coverage is uniform **if and only if the kernel size $k$ is a multiple of the stride $s$**. If this condition holds, every interior output position receives exactly $k/s$ contributions. If $k$ is not a multiple of $s$, the number of contributions varies periodically with period $s$, creating the checkerboard pattern.\n\nWith this principle, we can evaluate the options:\n\n*   **A. Correct.** For $s=2$ and $k=3$, $k$ is not a multiple of $s$. A direct calculation shows that output positions with an even index are covered by two kernel footprints, while positions with an odd index are covered by only one. This creates a periodic alternation of values ($\\dots, 2, 1, 2, 1, \\dots$), which is the 1D checkerboard artifact.\n\n*   **B. Correct.** This statement directly expresses the condition for uniform coverage. When $k$ is a multiple of $s$, the kernel footprints tile perfectly across the output grid (away from boundaries), ensuring every position is covered an equal number of times ($k/s$). This removes the source of the artifacts.\n\n*   **C. Correct.** This describes the \"upsample-then-convolve\" strategy, a common and effective alternative. First, the feature map is upsampled (e.g., via nearest-neighbor), which simply repeats values. Then, a **standard convolution with stride 1** is applied. A stride-1 convolution has inherently uniform coverage; its kernel slides one step at a time, applying the same operation to every position. This two-step process avoids the uneven overlap problem of transposed convolution entirely.\n\n*   **D. Incorrect.** Padding affects the size of the output and how boundaries are handled. It does not change the fundamental relationship between $k$ and $s$, and therefore cannot fix the periodic non-uniformity of kernel overlap in the interior of the output.\n\n*   **E. Incorrect.** The \"output padding\" parameter is a fine-tuning mechanism to resolve output size ambiguity. Like standard padding, it can shift the alignment of the output grid but cannot alter the intrinsic pattern of uneven coverage in the interior caused when $k$ is not a multiple of $s$.\n\nTherefore, the correct options are A, B, and C. In 2D, this principle applies along each axis independently. Practical fixes include ensuring kernel dimensions are divisible by strides or, more robustly, replacing transposed convolutions with an upsampling layer followed by a standard convolution.", "answer": "$$\\boxed{ABC}$$", "id": "3177691"}]}