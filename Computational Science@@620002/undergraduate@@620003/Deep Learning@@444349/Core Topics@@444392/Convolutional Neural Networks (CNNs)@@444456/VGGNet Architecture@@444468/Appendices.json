{"hands_on_practices": [{"introduction": "Deploying deep learning models on devices with limited resources, such as mobile phones or embedded systems, requires a careful analysis of computational and memory costs. This practice guides you through a first-principles derivation of the floating-point operations (FLOPs) and memory traffic for convolutional layers, the core components of VGGNet. By applying these formulas to several VGG-like variants, you will develop the essential skill of evaluating architectural trade-offs to meet specific hardware budgets [@problem_id:3198589].", "problem": "A team is sizing simplified variants of the Visual Geometry Group (VGG) network architecture for deployment on a device with limited computational and memory bandwidth resources. Each variant consists of stacks of two-dimensional convolution layers with kernel size $K$, stride $1$, and zero-padding that preserves spatial dimensions. Ignore bias terms, batch normalization, nonlinearities, and pooling compute or traffic; average pooling only changes $(H, W)$ for subsequent layers. Every tensor element is stored in single-precision floating point, so each element occupies $4$ bytes.\n\nStarting from the definition of discrete two-dimensional convolution, derive the forward-pass floating-point operation count per convolution layer by counting multiplications and additions from first principles. Use the standard benchmarking convention that one multiply-add contributes two floating-point operations. Also derive a per-layer main-memory traffic model assuming that, for each convolution layer, the input activation tensor and the filter tensor are read once from main memory, and the output activation tensor is written once to main memory.\n\nThree candidate variants are given. For all convolution layers, the kernel size is $K=3$ and padding preserves $(H, W)$. The input image has spatial size $64 \\times 64$ with $3$ channels. The variants are:\n\n- Variant A:\n  - Layer $1$: $(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$.\n  - Layer $2$: $(64, 64, 16, 16)$.\n  - Average pooling halves spatial dimensions: $(H, W) \\mapsto (32, 32)$.\n  - Layer $3$: $(32, 32, 16, 32)$.\n  - Layer $4$: $(32, 32, 32, 32)$.\n  - Average pooling halves spatial dimensions: $(H, W) \\mapsto (16, 16)$.\n  - Layer $5$: $(16, 16, 32, 64)$.\n  - Layer $6$: $(16, 16, 64, 64)$.\n\n- Variant B:\n  - Layer $1$: $(64, 64, 3, 16)$.\n  - Layer $2$: $(64, 64, 16, 16)$.\n  - Average pooling: $(H, W) \\mapsto (32, 32)$.\n  - Layer $3$: $(32, 32, 16, 32)$.\n  - Layer $4$: $(32, 32, 32, 32)$.\n  - Layer $5$: $(32, 32, 32, 32)$.\n  - Average pooling: $(H, W) \\mapsto (16, 16)$.\n  - Layer $6$: $(16, 16, 32, 64)$.\n  - Layer $7$: $(16, 16, 64, 64)$.\n  - Layer $8$: $(16, 16, 64, 64)$.\n\n- Variant C:\n  - Layer $1$: $(64, 64, 3, 32)$.\n  - Layer $2$: $(64, 64, 32, 32)$.\n  - Average pooling: $(H, W) \\mapsto (32, 32)$.\n  - Layer $3$: $(32, 32, 32, 64)$.\n  - Layer $4$: $(32, 32, 64, 64)$.\n  - Average pooling: $(H, W) \\mapsto (16, 16)$.\n  - Layer $5$: $(16, 16, 64, 128)$.\n  - Layer $6$: $(16, 16, 128, 128)$.\n\nLet the compute budget be $2.0 \\times 10^{8}$ floating-point operations per forward pass, and the main-memory bandwidth budget be $3.0 \\times 10^{6}$ bytes per forward pass. Among the variants that satisfy both budgets, select the one with the largest total floating-point operation count. Report, as your final answer, the exact total floating-point operation count of the selected variant as a single integer. Do not include units in the final answer.", "solution": "First, we derive the general expressions for the forward-pass floating-point operation (FLOP) count and main-memory traffic for a single two-dimensional convolution layer, as specified in the problem statement.\n\nLet the input activation tensor have dimensions $(H, W, C_{\\text{in}})$, representing height, width, and number of input channels, respectively.\nLet the output activation tensor have dimensions $(H_{\\text{out}}, W_{\\text{out}}, C_{\\text{out}})$.\nLet the convolution kernel size be $K \\times K$.\nThe problem states that padding is used to preserve spatial dimensions, so $H_{\\text{out}} = H$ and $W_{\\text{out}} = W$. The stride is given as $1$.\n\nDerivation of Floating-Point Operations (FLOPs):\nTo compute a single value in one output feature map (one channel of the output tensor), a $K \\times K \\times C_{\\text{in}}$ filter is applied to a corresponding patch of the input tensor. This involves $K \\times K \\times C_{\\text{in}}$ multiplication operations and $(K \\times K \\times C_{\\text{in}} - 1)$ addition operations. Since bias is ignored, there are no further additions.\nThe total number of operations to produce one output value is $(K^2 C_{\\text{in}}) + (K^2 C_{\\text{in}} - 1)$.\nUsing the standard benchmarking convention that one multiply-add operation contributes two FLOPs, we can approximate the cost for one output value as $2 \\times K^2 \\times C_{\\text{in}}$.\nThis computation must be performed for every spatial location in the output tensor, of which there are $H \\times W$.\nFurthermore, this must be done for each of the $C_{\\text{out}}$ output channels.\nTherefore, the total FLOP count, $F$, for a single convolution layer is:\n$$F = (2 \\cdot K^2 \\cdot C_{\\text{in}}) \\cdot (H \\cdot W \\cdot C_{\\text{out}})$$\n$$F = 2 \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$$\n\nDerivation of Main-Memory Traffic:\nThe memory traffic, $M$, is the sum of bytes read from and written to main memory. Each tensor element is a single-precision float, occupying $4$ bytes.\n$1$. Input tensor read: The entire input tensor of size $H \\times W \\times C_{\\text{in}}$ is read once. Traffic is $4 \\cdot H \\cdot W \\cdot C_{\\text{in}}$ bytes.\n$2$. Filter tensor read: The convolution filters (weights) are read once. There are $C_{\\text{out}}$ filters, each of size $K \\times K \\times C_{\\text{in}}$. Total size is $K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$. Traffic is $4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$ bytes.\n$3$. Output tensor written: The resulting output tensor of size $H \\times W \\times C_{\\text{out}}$ is written once. Traffic is $4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$ bytes.\nThe total memory traffic per layer is the sum of these three components:\n$$M = 4 \\cdot H \\cdot W \\cdot C_{\\text{in}} + 4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} + 4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$$\n$$M = 4 \\cdot (H \\cdot W \\cdot (C_{\\text{in}} + C_{\\text{out}}) + K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}})$$\n\nNow, we evaluate each variant using $K=3$ (so $K^2=9$).\nThe budgets are: FLOPs $\\le 2.0 \\times 10^8$ and Memory $\\le 3.0 \\times 10^6$ bytes.\n\nVariant A:\n- Layer $1$: $(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 16 \\cdot 9 = 3,538,944$\n  $M_1 = 4 \\cdot (64 \\cdot 64 \\cdot (3+16) + 9 \\cdot 3 \\cdot 16) = 4 \\cdot (4096 \\cdot 19 + 432) = 313,024$\n- Layer $2$: $(64, 64, 16, 16)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 16 \\cdot 16 \\cdot 9 = 18,874,368$\n  $M_2 = 4 \\cdot (4096 \\cdot (16+16) + 9 \\cdot 16 \\cdot 16) = 4 \\cdot (4096 \\cdot 32 + 2304) = 533,504$\n- Layer $3$: $(32, 32, 16, 32)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 16 \\cdot 32 \\cdot 9 = 9,437,184$\n  $M_3 = 4 \\cdot (1024 \\cdot (16+32) + 9 \\cdot 16 \\cdot 32) = 4 \\cdot (1024 \\cdot 48 + 4608) = 215,040$\n- Layer $4$: $(32, 32, 32, 32)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 9 = 18,874,368$\n  $M_4 = 4 \\cdot (1024 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 4 \\cdot (1024 \\cdot 64 + 9216) = 299,008$\n- Layer $5$: $(16, 16, 32, 64)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$\n  $M_5 = 4 \\cdot (256 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 4 \\cdot (256 \\cdot 96 + 18432) = 172,032$\n- Layer $6$: $(16, 16, 64, 64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$\n  $M_6 = 4 \\cdot (256 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 4 \\cdot (256 \\cdot 128 + 36864) = 278,224$\n\n- Total for Variant A:\n  $F_A = 3538944 + 18874368 + 9437184 + 18874368 + 9437184 + 18874368 = 79,036,416$\n  $M_A = 313024 + 533504 + 215040 + 299008 + 172032 + 278224 = 1,810,832$\n- Budget Check: $F_A = 7.90 \\times 10^7 \\le 2.0 \\times 10^8$ (Pass). $M_A = 1.81 \\times 10^6 \\le 3.0 \\times 10^6$ (Pass).\nVariant A is a valid candidate.\n\nVariant B:\n- Layers $1-4$ are a subset of B's layers. Layers 1-2, pooling, Layer 3-5, pooling, Layers 6-8.\n- Layers $1, 2, 3, 4, 5$: $(64,64,3,16), (64,64,16,16), (32,32,16,32), (32,32,32,32), (32,32,32,32)$\n  $F_1=3,538,944, M_1=313,024$\n  $F_2=18,874,368, M_2=533,504$\n  $F_3=9,437,184, M_3=215,040$\n  $F_4=18,874,368, M_4=299,008$\n  $F_5=18,874,368, M_5=299,008$\n- Layers $6, 7, 8$: $(16,16,32,64), (16,16,64,64), (16,16,64,64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$, $M_6=172,032$\n  $F_7 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$, $M_7=278,224$\n  $F_8 = 18,874,368$, $M_8=278,224$\n- Total for Variant B:\n  $F_B = 3538944 + 18874368 + 9437184 + 18874368 + 18874368 + 9437184 + 18874368 + 18874368 = 116,785,152$\n  $M_B = 313024 + 533504 + 215040 + 299008 + 299008 + 172032 + 278224 + 278224 = 2,388,064$\n- Budget Check: $F_B = 1.17 \\times 10^8 \\le 2.0 \\times 10^8$ (Pass). $M_B = 2.39 \\times 10^6 \\le 3.0 \\times 10^6$ (Pass).\nVariant B is a valid candidate.\n\nVariant C:\n- Layer $1$: $(64, 64, 3, 32)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 32 \\cdot 9 = 7,077,888$\n  $M_1 = 4 \\cdot (4096 \\cdot (3+32) + 9 \\cdot 3 \\cdot 32) = 576,896$\n- Layer $2$: $(64, 64, 32, 32)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 32 \\cdot 32 \\cdot 9 = 37,748,736$\n  $M_2 = 4 \\cdot (4096 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 1,085,440$\n- Layer $3$: $(32, 32, 32, 64)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 9 = 37,748,736$\n  $M_3 = 4 \\cdot (1024 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 466,944$\n- Layer $4$: $(32, 32, 64, 64)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 64 \\cdot 9 = 75,497,472$\n  $M_4 = 4 \\cdot (1024 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 671,744$\n- Layer $5$: $(16, 16, 64, 128)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 128 \\cdot 9 = 37,748,736$\n  $M_5 = 4 \\cdot (256 \\cdot (64+128) + 9 \\cdot 64 \\cdot 128) = 491,520$\n- Layer $6$: $(16, 16, 128, 128)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 128 \\cdot 128 \\cdot 9 = 75,497,472$\n  $M_6 = 4 \\cdot (256 \\cdot (128+128) + 9 \\cdot 128 \\cdot 128) = 851,968$\n\n- Total for Variant C:\n  $F_C = 7077888 + 37748736 + 37748736 + 75497472 + 37748736 + 75497472 = 271,319,040$\n  $M_C = 576896 + 1085440 + 466944 + 671744 + 491520 + 851968 = 4,144,512$\n- Budget Check: $F_C = 2.71 \\times 10^8 > 2.0 \\times 10^8$ (Fail). $M_C = 4.14 \\times 10^6 > 3.0 \\times 10^6$ (Fail).\nVariant C is not a valid candidate.\n\nComparison and Selection:\nThe valid candidates are Variant A and Variant B. We must select the one with the largest total FLOP count.\n$F_A = 79,036,416$\n$F_B = 116,785,152$\nSince $F_B > F_A$, Variant B is the selected model. The problem asks for the exact total floating-point operation count of this selected variant.\nThe required value is $F_B = 116,785,152$.", "answer": "$$\\boxed{116785152}$$", "id": "3198589"}, {"introduction": "Pretrained models like VGGNet are powerful tools, but they expect inputs to be formatted exactly as they were during training, including the order of color channels (e.g., RGB vs. BGR). This exercise explores the consequences of this common mismatch and demonstrates how the issue can be resolved by leveraging the linear properties of the convolution operation. By analyzing how to adjust the first layer's weights, you will gain a deeper insight into the inner workings of convolutional filters and the importance of data preprocessing [@problem_id:3198592].", "problem": "A pretrained Visual Geometry Group Network (VGGNet) expects input images with channels in a particular order (for example, Blue-Green-Red (BGR)). You intend to feed Red-Green-Blue (RGB) inputs. Let the input tensor be denoted by $x \\in \\mathbb{R}^{3 \\times H \\times W}$, the first convolutional layer weights by $W \\in \\mathbb{R}^{C_{\\text{out}} \\times 3 \\times k \\times k}$, and the first-layer biases by $b \\in \\mathbb{R}^{C_{\\text{out}}}$. Model the channel-order change as applying a fixed $3 \\times 3$ permutation matrix $P$ to the $3$ input channels before the first convolution, so the network sees $x' = P x$ instead of $x$. Assume the remainder of the network is unchanged and behaves deterministically given its inputs.\n\nChoose all statements that are true about how to obtain outputs identical to those of the original pretrained model that expects the $P$-ordered channels (for example, BGR) when given RGB inputs:\n\nA. It is sufficient to modify only the first convolutional layer weights by permuting the in-channel dimension according to $P$ (that is, reindex $W$ along its input-channel axis according to the permutation induced by $P$), leaving $b$ and all subsequent layers unchanged.\n\nB. It is sufficient to permute only the out-channel dimension of the first convolutional layer according to $P$, leaving all subsequent layers unchanged.\n\nC. No weight changes are required; the network is invariant to input-channel permutations due to the presence of Rectified Linear Unit (ReLU) nonlinearities.\n\nD. If the RGB input is pre-permuted by applying $P$ before the first convolution so that the first layer receives channels in the order it was trained on, then keeping the pretrained weights and biases unchanged yields outputs identical to the original pretrained model.\n\nE. Adjusting only the first-layer biases $b$ (and not the weights $W$) is sufficient to compensate for the change in channel order.", "solution": "We start from the fundamental definition of multi-channel convolution and linearity. For the first convolutional layer, the output feature map for output channel $o \\in \\{1, \\dots, C_{\\text{out}}\\}$ is\n$$\ny_o = \\sum_{c=1}^{3} \\left( W_{o,c} * x_c \\right) + b_o,\n$$\nwhere $*$ denotes two-dimensional spatial convolution and $x_c$ is the $c$-th input channel. This operation is linear both in the input channels and in the kernel weights.\n\nAn input-channel permutation can be represented by a permutation matrix $P \\in \\mathbb{R}^{3 \\times 3}$ acting on the channel vector, so the permuted input $x'$ satisfies\n$$\nx'_c = \\sum_{d=1}^{3} P_{c,d}\\, x_d.\n$$\nApplying the first layer to $x'$ yields\n$$\ny'_o = \\sum_{c=1}^{3} \\left( W_{o,c} * x'_c \\right) + b_o \n= \\sum_{c=1}^{3} \\left( W_{o,c} * \\sum_{d=1}^{3} P_{c,d} x_d \\right) + b_o \n= \\sum_{d=1}^{3} \\left( \\sum_{c=1}^{3} P_{c,d} W_{o,c} \\right) * x_d + b_o.\n$$\nDefine $W'_{o,d} := \\sum_{c=1}^{3} P_{c,d} W_{o,c}$. Then\n$$\ny'_o = \\sum_{d=1}^{3} \\left( W'_{o,d} * x_d \\right) + b_o.\n$$\nThis shows that pre-permuting the input channels by $P$ is algebraically equivalent, at the first layer, to replacing the weights by an in-channel permutation $W'$ induced by $P$ while leaving the bias $b$ unchanged. After this adjustment, the remainder of the network receives the same $y'$ as in the pretrained configuration and thus produces identical outputs.\n\nWe now analyze each option:\n\nA. “It is sufficient to modify only the first convolutional layer weights by permuting the in-channel dimension according to $P$, leaving $b$ and all subsequent layers unchanged.” From the derivation, setting $W'_{o,d} := \\sum_{c=1}^{3} P_{c,d} W_{o,c}$, which is a reindexing (permutation) along the input-channel axis of $W$, yields $y'_o = \\sum_{d=1}^{3} ( W'_{o,d} * x_d ) + b_o$, matching the pretrained layer’s response to permuted inputs. Because the remainder of the network sees the same first-layer outputs it was trained on, no further changes are required. Bias $b$ is unaffected because the permutation acts on the input channels before the convolution. Verdict: Correct.\n\nB. “It is sufficient to permute only the out-channel dimension of the first convolutional layer according to $P$, leaving all subsequent layers unchanged.” Permuting the out-channel dimension of $W$ reorders $y_o$ across $o$. Subsequent layers are parameterized to expect a specific ordering of feature channels; changing this order without correspondingly permuting the weights of all downstream layers would not preserve the function of the network. There is no reason that an input-channel permutation should be counteracted by an output-channel permutation in the first layer alone. Verdict: Incorrect.\n\nC. “No weight changes are required; the network is invariant to input-channel permutations due to the presence of Rectified Linear Unit (ReLU) nonlinearities.” ReLU is applied elementwise and does not randomize or average over channels; network invariance to channel permutations does not hold in general. The first-layer filters are learned to respond to specific channel combinations and statistics; permuting channels changes the inputs to these filters and thus changes activations. Verdict: Incorrect.\n\nD. “If the RGB input is pre-permuted by applying $P$ before the first convolution so that the first layer receives channels in the order it was trained on, then keeping the pretrained weights and biases unchanged yields outputs identical to the original pretrained model.” This follows directly from the definition of the pretrained model: if it was trained on inputs in the $P$-ordered channel space (for example, BGR), then presenting RGB inputs transformed by $P$ recreates the exact input distribution it expects. Consequently, the first layer, and hence the entire network, produces identical outputs. Verdict: Correct.\n\nE. “Adjusting only the first-layer biases $b$ (and not the weights $W$) is sufficient to compensate for the change in channel order.” The bias $b$ is added after the linear combination across channels and does not modify how channels are combined. A permutation of input channels alters the linear combination in a way that cannot be undone by adding a constant offset per output channel. Therefore, changing only $b$ cannot restore equivalence. Verdict: Incorrect.\n\nIn summary, the correct strategies are to either permute the input channels by $P$ before the first convolution (option D), or equivalently to permute the in-channel dimension of the first-layer weights according to $P$ while leaving everything else unchanged (option A).", "answer": "$$\\boxed{AD}$$", "id": "3198592"}, {"introduction": "Training large networks like VGG is computationally intensive, and modern hardware offers significant speedups through lower-precision arithmetic, such as half-precision floating-point numbers (FP16). However, this efficiency comes with the risk of numerical instability, particularly gradient underflow, which can stall training. This advanced hands-on simulation tasks you with implementing mixed-precision training from scratch, demonstrating how a technique called loss scaling can effectively mitigate underflow and unlock performance gains without sacrificing accuracy [@problem_id:3198711].", "problem": "You are asked to implement a principled simulation of mixed-precision training for a small Visual Geometry Group Network (VGG)-like convolutional neural network, with explicit loss scaling to mitigate gradient underflow in half-precision arithmetic. Your program must be a complete, runnable implementation that trains a tiny model from first principles using only array operations, simulates mixed-precision arithmetic by casting to half-precision where appropriate, and computes a theoretical speedup and any change in final accuracy relative to a full-precision baseline. All mathematical definitions and targets are expressed below in a form suitable for algorithmic derivation.\n\nFundamental base to use:\n- Convolutional layer operation: a two-dimensional convolution with stride $1$ and zero padding that preserves spatial size, using a kernel of size $3 \\times 3$. A multiply-accumulate (MAC) operation is the repeated pattern of multiplication followed by addition. For a single convolutional layer with input shape $\\left(C_{\\text{in}}, H, W\\right)$, output channels $C_{\\text{out}}$, and kernel size $K \\times K$ with stride $1$ and padding, the total forward-pass MAC count is $C_{\\text{out}} \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot K \\cdot K$. For training, assume the backward-pass cost for gradients with respect to inputs and weights each has a MAC count on the same order as the forward pass, so the total MACs per training step per convolution is approximately $3$ times the forward-pass MACs. For a fully connected (linear) layer with input dimension $d_{\\text{in}}$ and output dimension $d_{\\text{out}}$, the forward-pass MAC count is $d_{\\text{in}} \\cdot d_{\\text{out}}$, and similarly assume a factor of approximately $3$ for one training step including backward.\n- Mixed-precision with loss scaling: use Institute of Electrical and Electronics Engineers (IEEE) $754$ binary $16$ (half precision) for compute, and store a master copy of parameters in binary $32$ (single precision). Let $\\alpha$ be a positive scaling factor. Scaling the scalar loss by $\\alpha$ scales every gradient by $\\alpha$ during backpropagation: if $\\nabla_{\\theta} \\mathcal{L}$ denotes the gradient in full precision, then the gradient computed in half precision after scaling is approximately $\\operatorname{fp16}\\!\\left(\\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}\\right)$, which is then unscaled by dividing by $\\alpha$ in full precision before applying the update. The purpose is to reduce underflow of small gradients in half precision. IEEE $754$ binary $16$ has a smallest positive normal value approximately $2^{-14} \\approx 6.1035 \\times 10^{-5}$ and a smallest positive subnormal value approximately $2^{-24} \\approx 5.9605 \\times 10^{-8}$; values below the subnormal minimum underflow to zero in half precision.\n- Throughput model: Let $T_{32}$ denote the effective throughput for binary $32$ computations and $T_{16}$ denote the effective throughput for binary $16$ computations. Assume $T_{16} = s \\cdot T_{32}$ with $s = 2.0$. For a fixed number of floating point operations (FLOPs), the idealized speedup of mixed precision over full precision is $$S = \\frac{\\text{time}_{32}}{\\text{time}_{16}} = \\frac{\\text{FLOPs} / T_{32}}{\\text{FLOPs} / T_{16}} = \\frac{T_{16}}{T_{32}} = s.$$\n\nModel and training setup to implement:\n- Architecture (a minimal VGG-like stack): two convolutional layers with kernel size $3 \\times 3$, stride $1$, and zero padding to maintain $8 \\times 8$ spatial resolution, followed by Rectified Linear Unit (ReLU) nonlinearity after each convolution, then global average pooling (channel-wise mean over spatial positions), and a final fully connected (linear) classifier. The channel configuration is $1 \\rightarrow 2 \\rightarrow 2$ for the convolutional layers, then a linear map $\\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{2}$ for two classes. No bias terms are required. The input dimension is $1 \\times 8 \\times 8$.\n- Loss: softmax cross-entropy. Use a numerically stable softmax by subtracting the maximum logit per sample.\n- Optimizer: stochastic gradient descent with learning rate $\\eta$ applied to the master $32$-bit parameters after unscaling gradients (if any scaling was used). Use a constant learning rate $\\eta = 10^{8}$.\n- Data: construct a deterministic two-class dataset of $8$ samples with input scale $\\sigma = 10^{-4}$. For class $0$, the image has a centered $2 \\times 2$ block of ones scaled by $\\sigma$ and zeros elsewhere. For class $1$, the image has ones scaled by $\\sigma$ along the four borders (top row, bottom row, left column, right column) and zeros elsewhere. Create $4$ copies of each class to obtain $8$ samples. Labels are one-hot vectors in $\\mathbb{R}^{2}$. The batch size is $1$; at each step use the sample with index equal to the step modulo $8$.\n- Initialization: initialize all weights with independent Gaussian samples with zero mean and standard deviation $\\tau = 10^{-3}$, using a fixed random seed to ensure determinism.\n- Mixed-precision simulation: for full precision, perform all computations in binary $32$. For mixed precision, on each step:\n  - cast the current master weights to binary $16$ for the forward and backward computations;\n  - compute the loss and its gradient with respect to logits; multiply the loss gradient by the loss-scaling factor $\\alpha$; cast it to binary $16$ before backpropagation;\n  - compute gradients with respect to the cast weights in binary $16$;\n  - unscale the gradients by dividing by $\\alpha$ in binary $32$ and apply the stochastic gradient descent update to the master weights.\n  If any intermediate or parameter value becomes not-a-number or infinite at any point, treat the training run as failed and set the resulting accuracy to $0$ for that run.\n- Training length: run exactly $E = 20$ update steps.\n\nQuantities to compute and report:\n- Let $A_{\\text{fp32}}$ be the final top-$1$ accuracy (as a decimal fraction in $[0,1]$) of the full-precision run evaluated on the training set after $E$ steps, using forward propagation in binary $32$.\n- For each mixed-precision run with a given $\\alpha$, let $A_{\\text{mixed}}(\\alpha)$ be the final top-$1$ accuracy of the mixed-precision trained model evaluated on the training set in binary $32$ after $E$ steps.\n- For each mixed-precision run, report the theoretical speedup $S = s = 2.0$ and the accuracy change $\\Delta A(\\alpha) = A_{\\text{mixed}}(\\alpha) - A_{\\text{fp32}}$.\n\nTest suite:\n- Use the following three loss-scaling factors:\n  - $\\alpha_{1} = 1$\n  - $\\alpha_{2} = 8192$ (that is, $2^{13}$)\n  - $\\alpha_{3} = 1048576$ (that is, $2^{20}$)\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the same order $\\left[\\alpha_{1}, \\alpha_{2}, \\alpha_{3}\\right]$. Each element must itself be a list of two floats in the order $\\left[S, \\Delta A(\\alpha)\\right]$. For example, the printed line must look like a list of lists: $\\left[\\left[S_{1}, \\Delta A\\left(\\alpha_{1}\\right)\\right], \\left[S_{2}, \\Delta A\\left(\\alpha_{2}\\right)\\right], \\left[S_{3}, \\Delta A\\left(\\alpha_{3}\\right)\\right]\\right]$. No additional text should be printed.", "solution": "This problem requires implementing a simulation of full-precision and mixed-precision training to study the effect of loss scaling.\n\n### Principled Solution\nThe core of the problem revolves around the limited range of half-precision floating-point numbers (`binary16`) and how this can cause training to fail.\n\n**1. The Problem: Gradient Underflow**\nThe dynamic range of `binary16` is much smaller than `binary32`. The smallest positive `binary16` subnormal value is roughly $6 \\times 10^{-8}$. Gradients smaller than this will be flushed to zero (underflow). In this problem, the small input scale ($\\sigma=10^{-4}$) and small initial weights ($\\tau=10^{-3}$) are deliberately chosen to produce extremely small gradients that will underflow in a `binary16` backward pass, preventing the model from learning.\n\n**2. The Solution: Loss Scaling**\nLoss scaling prevents underflow by artificially boosting the magnitude of the loss, and therefore all its gradients, before the backward pass.\n1.  After the forward pass, calculate the loss $\\mathcal{L}$.\n2.  Scale it by a large factor $\\alpha$: $\\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\mathcal{L}$.\n3.  Backpropagate from $\\mathcal{L}_{\\text{scaled}}$. By the chain rule, all gradients are now scaled by $\\alpha$: $\\nabla_{\\theta} \\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}$.\n4.  These larger, scaled gradients are computed in `binary16` and are less likely to underflow.\n5.  Before the weight update, the gradients are unscaled by dividing by $\\alpha$. The update remains mathematically correct: $\\theta_{t+1} = \\theta_t - \\eta \\cdot (\\frac{1}{\\alpha} (\\nabla_{\\theta_t} \\mathcal{L}_{\\text{scaled}})) = \\theta_t - \\eta \\cdot \\nabla_{\\theta_t} \\mathcal{L}$.\n\nThe choice of $\\alpha$ is a trade-off: it must be large enough to prevent underflow but not so large that the scaled gradients overflow (exceed the `binary16` max value of 65504).\n\n### Implementation\nThe provided Python code is a complete, from-scratch implementation of the required simulation. It defines all necessary components: layer operations (convolution, ReLU, pooling) and their gradients, the loss function, the data generation process, and the training loop which simulates both full-precision and mixed-precision modes. It correctly implements the logic for casting to `binary16`, scaling the loss gradient, and unscaling the final weight gradients for the mixed-precision case. The code is deterministic due to a fixed random seed.\n\n```python\nimport numpy as np\n\n# This program simulates mixed-precision training for a small CNN from first principles.\n# It is designed to be fully deterministic and self-contained.\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the simulation, training, and evaluation.\n    \"\"\"\n    # --- Problem Parameters ---\n    ETA = 1e8\n    SIGMA = 1e-4\n    TAU = 1e-3\n    E = 20  # Number of training steps\n    NUM_SAMPLES = 8\n    SPEEDUP_FACTOR = 2.0\n    SEED = 42\n    ALPHAS = [1.0, 8192.0, 1048576.0]\n\n    # --- Layer Implementations ---\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def relu_grad(x):\n        return (x > 0).astype(x.dtype)\n\n    def convolve_forward(x_in, W, stride=1, padding=1):\n        \"\"\"A simplified 2D convolution forward pass.\"\"\"\n        C_out, C_in, K, _ = W.shape\n        _, H_in, W_in = x_in.shape\n        \n        H_out = (H_in - K + 2 * padding) // stride + 1\n        W_out = (W_in - K + 2 * padding) // stride + 1\n\n        x_pad = np.pad(x_in, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n        out = np.zeros((C_out, H_out, W_out), dtype=x_in.dtype)\n\n        for c_o in range(C_out):\n            for h in range(H_out):\n                for w in range(W_out):\n                    h_start, w_start = h * stride, w * stride\n                    region = x_pad[:, h_start:h_start + K, w_start:w_start + K]\n                    out[c_o, h, w] = np.sum(region * W[c_o, :, :, :])\n        return out\n\n    def convolve_backward(d_out, x_in, W, stride=1, padding=1):\n        \"\"\"A simplified 2D convolution backward pass.\"\"\"\n        C_out, C_in, K, _ = W.shape\n        _, H_in, W_in = x_in.shape\n        \n        x_pad = np.pad(x_in, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n        d_x = np.zeros_like(x_pad, dtype=x_in.dtype)\n        d_W = np.zeros_like(W, dtype=x_in.dtype)\n        \n        H_out = (H_in - K + 2 * padding) // stride + 1\n        W_out = (W_in - K + 2 * padding) // stride + 1\n\n        for c_o in range(C_out):\n            for h in range(H_out):\n                for w in range(W_out):\n                    h_start, w_start = h * stride, w * stride\n                    region = x_pad[:, h_start:h_start + K, w_start:w_start + K]\n                    \n                    d_W[c_o, :, :, :] += d_out[c_o, h, w] * region\n                    d_x[:, h_start:h_start + K, w_start:w_start + K] += d_out[c_o, h, w] * W[c_o, :, :, :]\n        \n        if padding > 0:\n            d_x = d_x[:, padding:-padding, padding:-padding]\n        \n        return d_x, d_W\n\n    def global_avg_pool_forward(x):\n        return np.mean(x, axis=(1, 2))\n\n    def global_avg_pool_backward(d_out, x_shape):\n        C, H, W = x_shape\n        return d_out[:, np.newaxis, np.newaxis] / (H * W) * np.ones(x_shape, dtype=d_out.dtype)\n    \n    def stable_softmax_cross_entropy(logits, y_true):\n        # Logits and y_true are 1D arrays for a single sample.\n        stable_logits = logits - np.max(logits)\n        exp_logits = np.exp(stable_logits)\n        probs = exp_logits / np.sum(exp_logits)\n        loss = -np.sum(y_true * np.log(probs))\n        d_logits = probs - y_true\n        return loss, d_logits\n\n    # --- Data Generation ---\n    def generate_data():\n        X = np.zeros((NUM_SAMPLES, 1, 8, 8), dtype=np.float32)\n        Y = np.zeros((NUM_SAMPLES, 2), dtype=np.float32)\n        \n        # Class 0: centered 2x2 block\n        img0 = np.zeros((8, 8), dtype=np.float32)\n        img0[3:5, 3:5] = SIGMA\n        \n        # Class 1: border\n        img1 = np.zeros((8, 8), dtype=np.float32)\n        img1[[0, 7], :] = SIGMA\n        img1[:, [0, 7]] = SIGMA\n\n        for i in range(NUM_SAMPLES):\n            if i  4:\n                X[i, 0, :, :] = img0\n                Y[i, :] = [1, 0]\n            else:\n                X[i, 0, :, :] = img1\n                Y[i, :] = [0, 1]\n        return X, Y\n\n    X_train, Y_train = generate_data()\n\n    # --- Model Initialization ---\n    def init_weights(seed):\n        rng = np.random.default_rng(seed)\n        weights = {\n            'W1': rng.normal(0, TAU, (2, 1, 3, 3)).astype(np.float32),\n            'W2': rng.normal(0, TAU, (2, 2, 3, 3)).astype(np.float32),\n            'W_lin': rng.normal(0, TAU, (2, 2)).astype(np.float32),\n        }\n        return weights\n\n    # --- Training and Evaluation ---\n    def train(X_data, Y_data, mode, alpha, seed):\n        weights = init_weights(seed)\n        fp_master_weights = {k: v.copy() for k, v in weights.items()}\n\n        for step in range(E):\n            idx = step % NUM_SAMPLES\n            x, y_true = X_data[idx], Y_data[idx]\n\n            # Set precision for this run\n            compute_dtype = np.float16 if mode == 'mixed' else np.float32\n            \n            # --- FORWARD PASS ---\n            W1 = fp_master_weights['W1'].astype(compute_dtype)\n            W2 = fp_master_weights['W2'].astype(compute_dtype)\n            W_lin = fp_master_weights['W_lin'].astype(compute_dtype)\n            x_compute = x.astype(compute_dtype)\n\n            z1 = convolve_forward(x_compute, W1)\n            a1 = relu(z1)\n            z2 = convolve_forward(a1, W2)\n            a2 = relu(z2)\n            p = global_avg_pool_forward(a2)\n            logits = p @ W_lin\n\n            # --- LOSS  BACKWARD PASS ---\n            # Loss computed in FP32 for stability\n            _, d_logits_fp32 = stable_softmax_cross_entropy(logits.astype(np.float32), y_true)\n\n            if mode == 'mixed':\n                d_logits = (d_logits_fp32 * alpha).astype(compute_dtype)\n            else:\n                d_logits = d_logits_fp32.astype(compute_dtype)\n            \n            d_p = d_logits @ W_lin.T\n            d_W_lin = np.outer(p, d_logits)\n            \n            d_a2 = global_avg_pool_backward(d_p, a2.shape)\n            d_z2 = d_a2 * relu_grad(z2)\n            d_a1, d_W2 = convolve_backward(d_z2, a1, W2)\n            \n            d_z1 = d_a1 * relu_grad(z1)\n            _, d_W1 = convolve_backward(d_z1, x_compute, W1)\n            \n            # --- WEIGHT UPDATE ---\n            grads = {\n                'W1': d_W1.astype(np.float32),\n                'W2': d_W2.astype(np.float32),\n                'W_lin': d_W_lin.astype(np.float32),\n            }\n\n            if mode == 'mixed':\n                for k in grads:\n                    grads[k] /= alpha\n\n            for k in fp_master_weights:\n                fp_master_weights[k] -= ETA * grads[k]\n            \n            # Check for failure\n            is_invalid = any(np.any(np.isinf(w)) or np.any(np.isnan(w)) for w in fp_master_weights.values())\n            if is_invalid:\n                return None  # Failed run\n\n        return fp_master_weights\n\n    def evaluate(X_data, Y_data, weights):\n        correct_predictions = 0\n        for i in range(NUM_SAMPLES):\n            x = X_data[i].astype(np.float32)\n            \n            z1 = convolve_forward(x, weights['W1'])\n            a1 = relu(z1)\n            z2 = convolve_forward(a1, weights['W2'])\n            a2 = relu(z2)\n            p = global_avg_pool_forward(a2)\n            logits = p @ weights['W_lin']\n            \n            predicted_class = np.argmax(logits)\n            true_class = np.argmax(Y_data[i])\n            \n            if predicted_class == true_class:\n                correct_predictions += 1\n        \n        return correct_predictions / NUM_SAMPLES\n\n    # --- Main Execution Logic ---\n    \n    # FP32 baseline run\n    final_weights_fp32 = train(X_train, Y_train, mode='fp32', alpha=1.0, seed=SEED)\n    acc_fp32 = evaluate(X_train, Y_train, final_weights_fp32)\n\n    results = []\n    for alpha in ALPHAS:\n        final_weights_mixed = train(X_train, Y_train, mode='mixed', alpha=alpha, seed=SEED)\n        \n        if final_weights_mixed is None:\n            acc_mixed = 0.0\n        else:\n            acc_mixed = evaluate(X_train, Y_train, final_weights_mixed)\n            \n        delta_A = acc_mixed - acc_fp32\n        results.append([SPEEDUP_FACTOR, delta_A])\n    \n    # Format and print the final output\n    formatted_results = \",\".join([f\"[{s},{da}]\" for s, da in results])\n    return f\"[{formatted_results}]\"\n\n# The final answer is the output of this deterministic simulation.\n# Running solve() yields: \"[[2.0,-0.5],[2.0,0.0],[2.0,-1.0]]\"\n```", "answer": "[[2.0,-0.5],[2.0,0.0],[2.0,-1.0]]", "id": "3198711"}]}