{"hands_on_practices": [{"introduction": "One of the primary motivations for data augmentation is to improve a model's robustness to real-world variations not fully captured in the training set. This practice explores this concept using Cutout, an augmentation technique that randomly masks rectangular regions of an input image. By forcing the model to make predictions even when parts of the input are missing, Cutout discourages overfitting to specific local features and promotes learning more global, resilient patterns [@problem_id:3151871]. You will implement a controlled experiment to quantitatively measure how training with Cutout enhances a model's performance on images with simulated occlusions.", "problem": "Consider a binary classification problem where each input is a synthetic satellite image represented as a two-dimensional array of pixel intensities and the label indicates the presence or absence of a target land feature. The objective is to study how the \"cutout\" augmentation affects robustness to simulated cloud occlusion at test-time. Begin from empirical risk minimization for binary logistic regression: the classifier is a linear function with parameters $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, mapping an input vector $x \\in \\mathbb{R}^d$ to a probability via the logistic function $\\sigma(z) = 1/(1 + e^{-z})$ where $z = w^\\top x + b$. The empirical risk under binary cross-entropy with $\\ell_2$ regularization is\n$$\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log(1 - \\sigma(w^\\top x_i + b))\\right] + \\frac{\\lambda}{2}\\|w\\|_2^2.\n$$\nData augmentation by cutout is modeled as a stochastic transformation $T_f$ that, given an image $X \\in \\mathbb{R}^{H \\times W}$ and a target area fraction $f \\in [0,1]$, masks a single axis-aligned rectangular region of area approximately $f \\cdot H \\cdot W$ by setting its pixel intensities to zero. The rectangle has a uniformly random aspect ratio in $[0.3, 3.0]$ and a uniformly random location subject to full containment in the image. Training with cutout applies $T_f$ independently to each training sample with probability $p \\in [0,1]$. At test-time, occlusion is simulated by applying $T_{f_{\\text{test}}}$ deterministically to every test image (with independent randomness in rectangle shape and location per image).\n\nConstruct a synthetic dataset as follows. Let image height $H = 16$ and width $W = 16$, so $d = H \\cdot W = 256$. For each sample index $i \\in \\{1,\\dots,n\\}$, draw a label $y_i \\in \\{0,1\\}$ uniformly at random. Draw base noise $N_i \\in \\mathbb{R}^{H \\times W}$ with independent entries from a zero-mean normal distribution with standard deviation $\\sigma = 0.1$. Impose a weak global class-dependent bias: for $y_i = 1$ add a constant $g = 0.1$ to all pixels of $N_i$, and for $y_i = 0$ subtract $g$ from all pixels. Impose a strong local discriminative patch by adding a constant $a_p = 1.0$ to the top-left $s \\times s$ square with $s = 6$ for $y_i = 1$, and subtracting $a_p$ from that square for $y_i = 0$. Flatten each image to a vector in $\\mathbb{R}^{256}$. This construction yields two redundant cues: a strong local patch and a weak global bias. A model trained without occlusion may heavily rely on the local patch, while a model trained with cutout experiences occasional occlusion during training and may allocate non-zero weight to the global bias, potentially improving robustness when the patch is occluded at test-time.\n\nImplement a program that:\n- Generates a training set of $N_{\\text{train}} = 2000$ samples and a test set of $N_{\\text{test}} = 2000$ samples using the procedure above, with independent random seeds per test case to ensure reproducibility.\n- Trains two binary logistic regression models using full-batch gradient descent with learning rate $\\eta = 0.1$, $\\ell_2$ regularization coefficient $\\lambda = 0.001$, and $E = 300$ optimization steps: \n  - A baseline model trained without augmentation (i.e., $p = 0$).\n  - A cutout-augmented model trained with a specified augmentation probability $p$ and augmentation area fraction equal to the test-time occlusion fraction $f_{\\text{test}}$ for that case.\n- Evaluates both models on a test set that has been occluded by applying $T_{f_{\\text{test}}}$ once to every test sample. The same occluded test set must be used to evaluate both models so that performance is comparable.\n- Computes the accuracy of each model, where accuracy is the fraction of correctly classified test samples using a threshold of $0.5$ on the predicted probability, and reports the improvement as the difference $A_{\\text{aug}} - A_{\\text{base}}$.\n\nUse the following test suite of parameter values, where each tuple is $(p, f_{\\text{test}}, \\text{seed})$ and each mathematical entity is specified:\n- Case $1$: $(p = 0.5, f_{\\text{test}} = 0.0, \\text{seed} = 42)$.\n- Case $2$: $(p = 0.5, f_{\\text{test}} = 0.3, \\text{seed} = 123)$.\n- Case $3$: $(p = 0.9, f_{\\text{test}} = 0.5, \\text{seed} = 7)$.\n- Case $4$: $(p = 0.0, f_{\\text{test}} = 0.3, \\text{seed} = 99)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_1,r_2,r_3,r_4$]\"). Each $r_i$ must be a float rounded to $4$ decimals representing $A_{\\text{aug}} - A_{\\text{base}}$ for the corresponding test case. No other output should be printed.", "solution": "The user wants to solve a problem about the effect of cutout augmentation on model robustness.\n\n### Step 1: Extract Givens\n- **Problem Type**: Binary classification with a logistic regression model.\n- **Input Data**: Synthetic satellite images of size $H \\times W$, where $H=16$ and $W=16$. The flattened input dimension is $d=256$.\n- **Classifier**: $\\sigma(w^\\top x + b)$, where $\\sigma(z) = 1/(1+e^{-z})$, $w \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$.\n- **Loss Function**: Binary cross-entropy with $\\ell_2$ regularization:\n$$\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log(1 - \\sigma(w^\\top x_i + b))\\right] + \\frac{\\lambda}{2}\\|w\\|_2^2.\n$$\n- **Data Augmentation (Cutout, $T_f$)**: Masks a single axis-aligned rectangular region with zeros.\n    - Target area fraction: $f \\in [0,1]$.\n    - Approximate area: $f \\cdot H \\cdot W$.\n    - Rectangle aspect ratio: Uniformly random in $[0.3, 3.0]$.\n    - Rectangle location: Uniformly random, fully contained in the image.\n- **Training Procedure**:\n    - Optimizer: Full-batch gradient descent.\n    - Learning rate: $\\eta = 0.1$.\n    - $\\ell_2$ regularization coefficient: $\\lambda = 0.001$.\n    - Optimization steps (epochs): $E = 300$.\n    - Augmented model training: Apply $T_f$ with probability $p$ to each sample at each training step. Training augmentation fraction $f$ is set to the test-time occlusion fraction $f_{\\text{test}}$.\n    - Baseline model training: No augmentation ($p=0$).\n- **Synthetic Data Generation**:\n    - Samples: $N_{\\text{train}} = 2000$, $N_{\\text{test}} = 2000$.\n    - Labels $y_i \\in \\{0,1\\}$: Uniformly random.\n    - Base noise $N_i$: Pixel-wise independent from a normal distribution $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.1$.\n    - Global bias: Add $g=0.1$ for $y_i=1$, subtract $g=0.1$ for $y_i=0$.\n    - Local discriminative patch: A $s \\times s$ square with $s=6$ in the top-left corner. Add $a_p=1.0$ for $y_i=1$, subtract $a_p=1.0$ for $y_i=0$.\n- **Test-Time Evaluation**:\n    - Occlusion: Apply $T_{f_{\\text{test}}}$ deterministically to every test image (i.e., with probability $p=1.0$).\n    - Metric: Accuracy improvement, defined as $A_{\\text{aug}} - A_{\\text{base}}$. Accuracy is the fraction of correct predictions with a decision threshold of $0.5$.\n- **Test Cases**: Tuples of $(p, f_{\\text{test}}, \\text{seed})$.\n    - Case $1$: $(0.5, 0.0, 42)$.\n    - Case $2$: $(0.5, 0.3, 123)$.\n    - Case $3$: $(0.9, 0.5, 7)$.\n    - Case $4$: $(0.0, 0.3, 99)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is well-grounded in standard machine learning principles, including logistic regression, gradient descent, regularization, and data augmentation (cutout). The experimental setup to test the hypothesis that cutout improves robustness to occlusion is a common and valid research methodology in the field. The problem is scientifically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly related to the specified topic and is entirely formalizable into a computational task.\n3.  **Incomplete or Contradictory Setup**: All required parameters ($H, W, n, \\lambda, \\eta, E$, data generation parameters, augmentation parameters) are explicitly defined. The procedure is described in sufficient detail to be implemented. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The parameters and setup are computationally feasible and constitute a standard, small-scale machine learning experiment.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The detailed specification of the data generation, training, and evaluation processes ensures that a unique and meaningful numerical result can be computed for each test case.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a non-trivial concept—the interplay between data augmentation and model robustness—through a concrete, computational experiment. While the setup is synthetic, it is designed to isolate and study a specific phenomenon, which is a valid scientific approach.\n7.  **Outside Scientific Verifiability**: The results are verifiable by implementing the described algorithm and running it with the specified seeds.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with providing a complete solution.\n\n### Algorithmic Solution\nThe solution involves implementing the specified data generation, model training, and evaluation pipeline.\n\n#### 1. Data Generation\nFor a set of $n$ samples, we generate images $X \\in \\mathbb{R}^{n \\times H \\times W}$ and labels $y \\in \\{0, 1\\}^n$.\nFor each sample $i \\in \\{1, \\dots, n\\}$:\n- A label $y_i$ is drawn from a Bernoulli distribution with $p=0.5$.\n- A base noise image $N_i \\in \\mathbb{R}^{H \\times W}$ is drawn, with each pixel $N_{i,jk} \\sim \\mathcal{N}(0, \\sigma^2)$, where $\\sigma = 0.1$.\n- A class-conditional multiplier $c_i = 2y_i - 1$ is defined, which is $+1$ for $y_i=1$ and $-1$ for $y_i=0$.\n- The image $X_i$ is constructed by adding a global bias and a local patch to the noise:\n$$ X_i = N_i + g \\cdot c_i \\cdot \\mathbf{1}_{H \\times W} + a_p \\cdot c_i \\cdot P_s $$\nwhere $\\mathbf{1}_{H \\times W}$ is an $H \\times W$ matrix of ones, and $P_s$ is an $H \\times W$ matrix with ones in the top-left $s \\times s$ block (with $s=6$) and zeros elsewhere. Constants are $g=0.1$ and $a_p=1.0$.\n- Finally, each $X_i$ is flattened into a vector $x_i \\in \\mathbb{R}^d$, where $d=H \\cdot W = 256$.\n\n#### 2. Cutout Augmentation\nThe cutout transformation $T_f$ takes an image and masks a random rectangular region. For an image of size $H \\times W$ and area fraction $f$:\n- The target area is $A = f \\cdot H \\cdot W$.\n- An aspect ratio $r$ is sampled uniformly from $[0.3, 3.0]$.\n- The height and width of the rectangle are calculated as $h = \\sqrt{A/r}$ and $w = \\sqrt{Ar}$. These are rounded to the nearest integer and clipped to be at most $H$ and $W$, respectively.\n- A top-left corner $(y_0, x_0)$ is sampled uniformly from the valid range, $[0, H-h] \\times [0, W-w]$.\n- The pixel intensities in the rectangle $[y_0:y_0+h, x_0:x_0+w]$ are set to $0$.\n\n#### 3. Model Training\nThe logistic regression parameters $w$ and $b$ are optimized using full-batch gradient descent. The gradients of the loss function $\\mathcal{L}(w,b)$ are:\n$$ \\nabla_w \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) x_i + \\lambda w $$\n$$ \\nabla_b \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) $$\nwhere $z_i = w^\\top x_i + b$. The parameters are initialized to zero and updated for $E=300$ steps:\n$$ w \\leftarrow w - \\eta \\nabla_w \\mathcal{L} $$\n$$ b \\leftarrow b - \\eta \\nabla_b \\mathcal{L} $$\n- For the **baseline model**, the training data $\\{x_i, y_i\\}$ is used directly.\n- For the **augmented model**, at each of the $E$ steps, a new augmented training set is created. For each sample $x_i$ in the original training batch, the cutout transformation $T_{f_{\\text{test}}}$ is applied with probability $p$. The gradients are then computed using this stochastically augmented batch.\n\n#### 4. Evaluation\n- First, a single occluded test set is created by applying the cutout transformation $T_{f_{\\text{test}}}$ once to every image in the generated test set.\n- Both the baseline model ($w_{\\text{base}}, b_{\\text{base}}$) and the augmented model ($w_{\\text{aug}}, b_{\\text{aug}}$) are used to make predictions on this same occluded test set.\n- A prediction $\\hat{y}_i$ for a test sample $x_i$ is $1$ if $\\sigma(w^\\top x_i + b) \\ge 0.5$, and $0$ otherwise.\n- Accuracy is computed as the fraction of correct predictions: $A = \\frac{1}{N_{\\text{test}}}\\sum_{i=1}^{N_{\\text{test}}} \\mathbb{I}(\\hat{y}_i = y_i)$.\n- The final result for each case is the accuracy improvement, $A_{\\text{aug}} - A_{\\text{base}}$, rounded to $4$ decimal places.\n\nThis entire procedure is repeated for each of the four test cases, with the random number generator re-seeded for each case to ensure reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment for all test cases.\n    \"\"\"\n    H, W = 16, 16\n    d = H * W\n    N_train = 2000\n    N_test = 2000\n    noise_std = 0.1\n    g = 0.1\n    s = 6\n    a_p = 1.0\n    eta = 0.1\n    lambda_reg = 0.001\n    epochs = 300\n    aspect_ratio_range = (0.3, 3.0)\n\n    test_cases = [\n        # (p, f_test, seed)\n        (0.5, 0.0, 42),\n        (0.5, 0.3, 123),\n        (0.9, 0.5, 7),\n        (0.0, 0.3, 99),\n    ]\n\n    results = []\n\n    def generate_data(n_samples, rng):\n        \"\"\"Generates synthetic image data.\"\"\"\n        labels = rng.integers(0, 2, size=n_samples)\n        noise = rng.normal(0, noise_std, size=(n_samples, H, W))\n        \n        # c_i = +1 for label 1, -1 for label 0\n        class_multiplier = 2 * labels - 1\n        \n        # Add global bias\n        images = noise + g * class_multiplier[:, np.newaxis, np.newaxis]\n        \n        # Add local patch\n        patch = np.zeros((H, W))\n        patch[0:s, 0:s] = a_p\n        images += class_multiplier[:, np.newaxis, np.newaxis] * patch\n        \n        return images.reshape(n_samples, d), labels\n\n    def apply_cutout_on_batch(images_flat, f, p_aug, rng):\n        \"\"\"Applies cutout augmentation to a batch of images.\"\"\"\n        if p_aug == 0 or f == 0:\n            return images_flat.copy()\n\n        n_samples = images_flat.shape[0]\n        images = images_flat.reshape(n_samples, H, W).copy()\n        \n        target_area = f * H * W\n        \n        # Determine which images to augment\n        augment_indices = rng.random(n_samples)  p_aug\n        \n        for i in range(n_samples):\n            if not augment_indices[i]:\n                continue\n\n            aspect_ratio = rng.uniform(aspect_ratio_range[0], aspect_ratio_range[1])\n            \n            h = int(np.round(np.sqrt(target_area / aspect_ratio)))\n            w = int(np.round(np.sqrt(target_area * aspect_ratio)))\n            \n            h = min(h, H)\n            w = min(w, W)\n\n            if h > 0 and w > 0:\n                y0 = rng.integers(0, H - h + 1)\n                x0 = rng.integers(0, W - w + 1)\n                images[i, y0:y0+h, x0:x0+w] = 0.0\n\n        return images.reshape(n_samples, d)\n        \n    def logistic_sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n\n    def train_model(X_train, y_train, p_aug, f_aug, rng):\n        \"\"\"Trains a logistic regression model.\"\"\"\n        n_samples, n_features = X_train.shape\n        w = np.zeros(n_features)\n        b = 0.0\n\n        for _ in range(epochs):\n            X_epoch = apply_cutout_on_batch(X_train, f_aug, p_aug, rng)\n            \n            z = X_epoch @ w + b\n            sigma = logistic_sigmoid(z)\n            \n            grad_w = (1/n_samples) * X_epoch.T @ (sigma - y_train) + lambda_reg * w\n            grad_b = (1/n_samples) * np.sum(sigma - y_train)\n            \n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n        return w, b\n\n    def predict(X, w, b):\n        \"\"\"Makes predictions with a trained model.\"\"\"\n        probas = logistic_sigmoid(X @ w + b)\n        return (probas >= 0.5).astype(int)\n\n    def calculate_accuracy(y_true, y_pred):\n        \"\"\"Calculates prediction accuracy.\"\"\"\n        return np.mean(y_true == y_pred)\n\n    for p_case, f_test_case, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X_train, y_train = generate_data(N_train, rng)\n        X_test, y_test = generate_data(N_test, rng)\n\n        # Train baseline model (no augmentation)\n        w_base, b_base = train_model(X_train, y_train, p_aug=0.0, f_aug=0.0, rng=rng)\n\n        # Train augmented model\n        # Augmentation fraction f is set to the test occlusion fraction f_test\n        w_aug, b_aug = train_model(X_train, y_train, p_aug=p_case, f_aug=f_test_case, rng=rng)\n\n        # Create occluded test set (deterministic application, p=1.0)\n        X_test_occluded = apply_cutout_on_batch(X_test, f_test_case, p_aug=1.0, rng=rng)\n\n        # Evaluate both models on the same occluded test set\n        preds_base = predict(X_test_occluded, w_base, b_base)\n        acc_base = calculate_accuracy(y_test, preds_base)\n        \n        preds_aug = predict(X_test_occluded, w_aug, b_aug)\n        acc_aug = calculate_accuracy(y_test, preds_aug)\n\n        improvement = round(acc_aug - acc_base, 4)\n        results.append(improvement)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3151871"}, {"introduction": "While augmentations like Cutout modify single images, Mixup creates new virtual samples by forming convex combinations of pairs of images and their labels, using a mixing coefficient $\\lambda$. This next exercise challenges you to move beyond a 'one-size-fits-all' approach and investigate an adaptive mixing strategy where $\\lambda$ is determined by the semantic similarity between the two images being combined [@problem_id:3151945]. You will quantify the 'harm' of different mixing policies by comparing their resulting label distributions to an ideal 'teacher' distribution using the Kullback-Leibler divergence, $D_{\\mathrm{KL}}(p \\| q)$, thereby gaining insight into designing more principled, data-aware augmentations.", "problem": "You are given a proposal to modify the standard linear mixup augmentation by choosing the mixing coefficient as a deterministic function of a data-dependent semantic similarity. Your task is to formalize and test whether an adaptive rule that sets the mixup coefficient equal to the semantic similarity can avoid harmful mixing on semantically distant pairs. The problem is to be solved purely in mathematical and algorithmic terms and implemented as a complete program. The context is deep learning data augmentations: mixup, cutout, and cutmix. Although only mixup is directly simulated here, your reasoning should be anchored in the core definitions used in deep learning classification.\n\nFundamental base and definitions:\n- Consider a classification problem with $C$ classes. Let an input sample be represented by a vector in $\\mathbb{R}^d$ and its class label be represented by a one-hot vector in $\\{0,1\\}^C$.\n- Standard mixup produces a virtual training sample from a pair $(x_i, y_i)$ and $(x_j, y_j)$ using a mixing coefficient $\\lambda \\in [0,1]$. The mixed input is $x_{\\text{mix}} = \\lambda x_i + (1-\\lambda) x_j$, and the mixed target is the convex combination $y_{\\text{mix}} = \\lambda y_i + (1-\\lambda) y_j$.\n- Let semantic similarity $s \\in [0,1]$ between two samples be defined by the cosine similarity mapped to $[0,1]$ using $s(x_i, x_j) = \\frac{1 + \\cos(\\theta(x_i,x_j))}{2}$, where $\\cos(\\theta(x_i,x_j)) = \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$.\n- Assume a semantics-consistent “teacher” target distribution for a mixed pair that places mass $s$ on the class of $x_i$ and mass $(1-s)$ on the class of $x_j$ when $y_i \\neq y_j$, and places all mass on the single class when $y_i=y_j$. This reflects the well-tested principle in classification that targets should follow the data’s semantic composition, with $s$ as the composition proxy.\n- Harm of a mixed target $y_{\\text{mix}}$ is quantified by the Kullback–Leibler divergence $D_{\\mathrm{KL}}(p \\| q)$ from the teacher distribution $p$ to the mixup target $q$, computed with a small positive constant added to avoid division by zero, in accordance with the standard definition of $D_{\\mathrm{KL}}$ for discrete distributions.\n\nGoal:\n- Define and evaluate three mixing policies for $\\lambda$ on a synthetic dataset:\n  - Policy A (fixed): $\\lambda = 0.5$.\n  - Policy B (stochastic Beta): $\\lambda \\sim \\mathrm{Beta}(\\alpha,\\alpha)$ with a given $\\alpha$.\n  - Policy C (adaptive): $\\lambda = s$.\n- A pair $(x_i,y_i),(x_j,y_j)$ is deemed semantically distant if $y_i \\neq y_j$ and $s(x_i,x_j) \\le s_{\\mathrm{cut}}$, for a given $s_{\\mathrm{cut}} \\in [0,1]$.\n- A mixed pair is harmful under a policy if its $D_{\\mathrm{KL}}(p \\| q)$ exceeds a given threshold $\\delta  0$.\n- For each test case, estimate the fraction of harmful pairs on the subset of semantically distant pairs, separately for each policy, using $M$ randomly sampled pairs. If no semantically distant pairs are found for a test case (the set is empty), define the harmful fraction as $0$ for all policies and treat the adaptive policy as having successfully avoided harmful mixing in that case.\n\nDataset construction for each test case:\n- Choose $C$ class prototypes $\\mu_c \\in \\mathbb{R}^d$ for $c \\in \\{1,\\dots,C\\}$. Place them as scaled orthonormal vectors so that $\\mu_c$ is a scaled basis vector with scale given by a “separation” parameter.\n- For each class $c$, draw $n$ samples $x$ from a multivariate normal distribution with mean $\\mu_c$ and isotropic covariance $\\sigma^2 I_d$. Assign one-hot labels $y$ accordingly.\n\nNumerical instructions:\n- All angles are dimensionless and use radians inside trigonometric functions.\n- There are no physical units in this problem.\n- All probabilities and fractions must be computed as real numbers in $[0,1]$.\n\nTest suite:\nFor each test case, produce a boolean result defined as “True if the harmful fraction of the adaptive policy (Policy C) on semantically distant pairs is strictly lower than that of both Policy A and Policy B, or if there are zero semantically distant pairs,” and “False” otherwise. Use a single fixed random seed for reproducibility. Evaluate the following four test cases:\n\n- Test case $1$ (general “happy path”):\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=2.5$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “all pairs,” $\\alpha=0.4$.\n- Test case $2$ (boundary: low inter-class separation, likely few or no distant pairs):\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=0.5$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “all pairs,” $\\alpha=0.4$.\n- Test case $3$ (boundary: high inter-class separation):\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.3$, separation $=4.0$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “all pairs,” $\\alpha=0.4$.\n- Test case $4$ (edge: same-class pairing only):\n  - $d=16$, $C=3$, $n=50$, $\\sigma=0.6$, separation $=2.0$, $s_{\\mathrm{cut}}=0.3$, $\\delta=0.02$, $M=4000$, pair mode = “same-class only,” $\\alpha=0.4$.\n\nOutput format:\n- Your program should produce a single line of output containing the four boolean results in order for the four test cases, as a comma-separated Python-style list enclosed in square brackets. For example, an acceptable output format is of the form $[b_1,b_2,b_3,b_4]$ where each $b_k$ is either True or False.\n- The program must be fully self-contained and must not require any user input, external files, or network access. Use a single fixed random seed so that your results are deterministic.\n\nNotes:\n- You should base your derivation on the standard definitions stated above. Do not use any shortcut formulas beyond the established definitions of convex combinations, cosine similarity, one-hot encoding, and Kullback–Leibler divergence.\n- While the broader topic includes cutout and cutmix, this computational experiment focuses on mixup. You should still frame your reasoning with awareness that cutout masks out regions (setting pixels to zero) and cutmix pastes patches between images; both differ from mixup in that the input mixing is not a global convex combination, but the target mixing remains a convex combination in cutmix, whereas cutout often keeps the original label.", "solution": "We proceed from the foundational definitions of classification targets, convex combinations, cosine similarity, and the Kullback–Leibler divergence. The objective is to formalize adaptive mixup in which the mixing coefficient for labels and inputs depends on a measurable semantic similarity between two samples, and then to evaluate whether this adaptivity reduces harmful mixing for semantically distant pairs.\n\nStep $1$: Data model and prototypes\n- We consider $C$ classes in $\\mathbb{R}^d$. A simple, well-tested way to synthesize class structure is to place prototypes $\\mu_c$ on orthonormal axes, scaled by a scalar “separation” to control inter-class distance. Explicitly, let $e_c$ denote the standard basis in $\\mathbb{R}^d$, and define $\\mu_c = \\text{separation} \\cdot e_c$ for $c \\in \\{1,\\dots,C\\}$ so that $\\mu_c^\\top \\mu_{c'} = 0$ for $c \\neq c'$ and $\\|\\mu_c\\|_2 = \\text{separation}$. This ensures controlled class separation under isotropic noise.\n- For each class $c$, we draw $n$ samples $x \\sim \\mathcal{N}(\\mu_c, \\sigma^2 I_d)$, assign one-hot labels $y$ with $y_c=1$ and zeros elsewhere, and collect the dataset $\\{(x_i,y_i)\\}$.\n\nStep $2$: Semantic similarity\n- For a pair $(x_i,y_i)$ and $(x_j,y_j)$, define the cosine similarity $\\cos(\\theta(x_i,x_j)) = \\frac{x_i^\\top x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$. Map this to the unit interval to obtain semantic similarity $s \\in [0,1]$ via $s(x_i,x_j) = \\frac{1 + \\cos(\\theta(x_i,x_j))}{2}$. This mapping is a monotone transformation that preserves order and bounds similarity, which is a widely accepted practice in metric learning.\n\nStep $3$: Teacher distribution aligned with semantics\n- We define a semantics-aligned teacher distribution $p$ for a pair as follows. If $y_i \\neq y_j$, then $p$ assigns probability mass $s$ to class $y_i$ and $(1-s)$ to class $y_j$, with zero mass to all other classes. If $y_i = y_j$, then $p$ is the one-hot distribution on that class (because mixing two samples of the same class should retain the same target).\n- This aligns with the fundamental idea that, for a target distribution in classification, the correctness probability should follow the data’s semantic content. The variable $s$ encodes the fractional contribution of one sample’s semantics to the mixed input.\n\nStep $4$: Mixup policies and mixed targets\n- Standard mixup forms the mixed target as the convex combination $q_\\lambda = \\lambda y_i + (1-\\lambda) y_j$. We evaluate three policies:\n  - Policy A: $\\lambda = 0.5$ (fixed half-and-half).\n  - Policy B: $\\lambda \\sim \\mathrm{Beta}(\\alpha,\\alpha)$, which is a common choice in practice to randomize mixing weights; we fix $\\alpha$ per test case and use a fixed random seed for reproducibility.\n  - Policy C: $\\lambda = s$ (adaptive), using the measurable semantic similarity to set the mixing proportion.\n- For same-class pairs, since $y_i=y_j$, all policies produce the same $q_\\lambda$, which equals the underlying one-hot label.\n\nStep $5$: Harm quantification using Kullback–Leibler divergence\n- To measure alignment between the semantics-informed teacher $p$ and the mixed target $q_\\lambda$, we use the Kullback–Leibler divergence\n$$\nD_{\\mathrm{KL}}(p \\| q_\\lambda) = \\sum_{c=1}^{C} p_c \\log\\left(\\frac{p_c}{q_{\\lambda,c}}\\right),\n$$\nwith the convention that a small $\\varepsilon0$ is added to both $p_c$ and $q_{\\lambda,c}$ inside the logarithm to avoid numerical issues when probabilities are zero. This matches the standard definition of $D_{\\mathrm{KL}}$ for discrete distributions and provides a quantitative measure of misalignment.\n- A mixed pair is deemed harmful if $D_{\\mathrm{KL}}(p \\| q_\\lambda)  \\delta$ for a given threshold $\\delta0$.\n\nStep $6$: Focus on semantically distant pairs\n- Harmful mixing is particularly concerning when samples are semantically distant. We therefore restrict our evaluation to pairs with $y_i \\neq y_j$ and $s(x_i,x_j) \\le s_{\\mathrm{cut}}$. The cutoff $s_{\\mathrm{cut}}$ formalizes “distant pairs” and is specified per test case.\n- If no such pairs are found in a test case (for instance, when classes overlap heavily and similarities are high), we define the harmful fraction as $0$ for all policies and declare the adaptive policy to have avoided harmful mixing by convention in such vacuous cases.\n\nStep $7$: Test cases and expected behavior\n- We design four test cases to cover different regimes:\n  - Test case $1$: Moderate separation and noise. We expect many cross-class pairs with a range of similarities. Policy C sets $\\lambda=s$ so that $q_\\lambda$ matches $p$ exactly on cross-class pairs, yielding $D_{\\mathrm{KL}}=0$, while Policies A and B generally incur positive divergence when $s \\neq \\lambda$.\n  - Test case $2$: Low separation. Cross-class similarities tend to be high, likely producing few or no distant pairs ($s \\le s_{\\mathrm{cut}}$). By our rule, the adaptive policy is credited with having no harmful distant mixing in this boundary scenario.\n  - Test case $3$: High separation with low noise. Many cross-class pairs are distant. Policy C remains aligned with the semantics ($\\lambda=s$), while Policies A and B generate mismatches for most distant pairs, resulting in larger divergences.\n  - Test case $4$: Same-class only pairing. All policies coincide because $y_i = y_j$, and all have zero harmful fraction on the empty set of distant cross-class pairs, qualifying as a success for the adaptive policy.\n\nStep $8$: Computation and output\n- For each test case, we sample $M$ pairs according to the specified pair mode (all pairs or same-class only), compute $s$, identify the distant subset, evaluate $D_{\\mathrm{KL}}(p\\|q_\\lambda)$ per policy, and compute the harmful fraction. We then output a boolean indicating whether the adaptive policy’s harmful fraction is strictly lower than both baselines (or there are zero distant pairs).\n- The final program prints the results for the four test cases as a single line: a list of four booleans in order.\n\nDiscussion of cutout and cutmix:\n- Cutout masks out contiguous regions of an image input by setting them to zero while keeping the original label; it does not perform a convex combination of inputs or labels. Cutmix replaces a region of one image with a patch from another and mixes the labels proportionally to the patched area. In contrast, mixup forms a global convex combination of the entire input and performs label interpolation accordingly. The adaptive idea explored here naturally extends to cutmix by setting the label mixing coefficient to a semantics-dependent weight (instead of or in addition to area), whereas cutout typically retains the original label and thus would not use $\\lambda$ at all.\n\nBy grounding the evaluation on $D_{\\mathrm{KL}}$ between a semantics-informed teacher distribution and the mixed target, we adhere to core definitions in statistical learning without introducing shortcut formulas, and we provide a principled and testable way to check whether the adaptive policy avoids harmful mixing on distant pairs.", "answer": "```python\nimport numpy as np\n\n# Environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used here)\n\ndef generate_prototypes(C, d, separation):\n    \"\"\"\n    Generate class prototypes as scaled standard basis vectors in R^d.\n    \"\"\"\n    prototypes = np.zeros((C, d), dtype=float)\n    for c in range(C):\n        prototypes[c, c] = separation\n    return prototypes\n\ndef generate_dataset(C, d, n_per_class, sigma, separation, rng):\n    \"\"\"\n    Generate synthetic dataset: for each class, sample n_per_class points from N(mu_c, sigma^2 I).\n    \"\"\"\n    mus = generate_prototypes(C, d, separation)\n    X_list = []\n    y_list = []\n    for c in range(C):\n        Xc = rng.normal(loc=0.0, scale=sigma, size=(n_per_class, d)) + mus[c]\n        yc = np.zeros((n_per_class, C), dtype=float)\n        yc[:, c] = 1.0\n        X_list.append(Xc)\n        y_list.append(yc)\n    X = np.vstack(X_list)\n    Y = np.vstack(y_list)\n    labels = np.argmax(Y, axis=1)\n    return X, Y, labels\n\ndef cosine_similarity(x, y):\n    \"\"\"\n    Cosine similarity between two vectors.\n    \"\"\"\n    nx = np.linalg.norm(x)\n    ny = np.linalg.norm(y)\n    if nx == 0.0 or ny == 0.0:\n        return 0.0\n    return float(np.dot(x, y) / (nx * ny))\n\ndef semantic_similarity(x, y):\n    \"\"\"\n    Map cosine similarity to [0,1]: s = (1 + cos) / 2.\n    \"\"\"\n    cos = cosine_similarity(x, y)\n    return (1.0 + cos) / 2.0\n\ndef teacher_distribution(li, lj, s, C):\n    \"\"\"\n    Build teacher distribution p over C classes:\n    - If li != lj: mass s on li and (1-s) on lj.\n    - If li == lj: one-hot on li.\n    \"\"\"\n    p = np.zeros(C, dtype=float)\n    if li == lj:\n        p[li] = 1.0\n    else:\n        p[li] = s\n        p[lj] = 1.0 - s\n    return p\n\ndef mixup_target(li, lj, lam, C):\n    \"\"\"\n    Mixed target q over C classes using mixup coefficient lam.\n    \"\"\"\n    q = np.zeros(C, dtype=float)\n    if li == lj:\n        q[li] = 1.0\n    else:\n        q[li] = lam\n        q[lj] = 1.0 - lam\n    return q\n\ndef kl_divergence(p, q, eps=1e-12):\n    \"\"\"\n    D_KL(p || q) for discrete distributions with epsilon smoothing.\n    \"\"\"\n    p_safe = p + eps\n    q_safe = q + eps\n    p_safe /= p_safe.sum()\n    q_safe /= q_safe.sum()\n    return float(np.sum(p_safe * (np.log(p_safe) - np.log(q_safe))))\n\ndef sample_pairs(labels, M, mode, rng):\n    \"\"\"\n    Sample M pairs of indices according to mode:\n    - 'all': any pairs uniformly at random with replacement\n    - 'same': pairs restricted to same-class\n    - 'cross': pairs restricted to different classes\n    Returns arrays of indices i_idx, j_idx.\n    \"\"\"\n    N = len(labels)\n    i_idx = []\n    j_idx = []\n    # Precompute class indices\n    class_to_indices = {}\n    for c in np.unique(labels):\n        class_to_indices[c] = np.where(labels == c)[0]\n    if mode == 'all':\n        i_idx = rng.integers(0, N, size=M)\n        j_idx = rng.integers(0, N, size=M)\n        return i_idx, j_idx\n    elif mode == 'same':\n        # For each sample, choose j from same class uniformly\n        for _ in range(M):\n            i = int(rng.integers(0, N))\n            ci = labels[i]\n            choices = class_to_indices[ci]\n            j = int(choices[rng.integers(0, len(choices))])\n            i_idx.append(i)\n            j_idx.append(j)\n        return np.array(i_idx, dtype=int), np.array(j_idx, dtype=int)\n    elif mode == 'cross':\n        # For each sample, choose i and j with different classes\n        classes = list(class_to_indices.keys())\n        for _ in range(M):\n            ci = int(classes[rng.integers(0, len(classes))])\n            # choose a different class\n            cj_choices = [c for c in classes if c != ci]\n            cj = int(cj_choices[rng.integers(0, len(cj_choices))])\n            i = int(class_to_indices[ci][rng.integers(0, len(class_to_indices[ci]))])\n            j = int(class_to_indices[cj][rng.integers(0, len(class_to_indices[cj]))])\n            i_idx.append(i)\n            j_idx.append(j)\n        return np.array(i_idx, dtype=int), np.array(j_idx, dtype=int)\n    else:\n        raise ValueError(\"Unknown mode\")\n\ndef evaluate_case(d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha, rng):\n    \"\"\"\n    Evaluate one test case and return boolean result as specified.\n    \"\"\"\n    X, Y, labels = generate_dataset(C=C, d=d, n_per_class=n, sigma=sigma, separation=separation, rng=rng)\n    # Determine sampling mode\n    if pair_mode == 'all':\n        mode = 'all'\n    elif pair_mode == 'same-class only':\n        mode = 'same'\n    elif pair_mode == 'cross-class only':\n        mode = 'cross'\n    else:\n        raise ValueError(\"pair_mode must be one of: all, same-class only, cross-class only\")\n\n    i_idx, j_idx = sample_pairs(labels, M, mode, rng)\n    # For policy A: fixed lambda=0.5\n    # For policy B: lambda ~ Beta(alpha,alpha)\n    # For policy C: lambda = s\n    harm_counts = {'A': 0, 'B': 0, 'C': 0}\n    total_distant = 0\n\n    # Pre-sample Beta for policy B\n    if alpha = 0:\n        raise ValueError(\"alpha must be positive\")\n    lam_beta = rng.beta(alpha, alpha, size=M)\n\n    for k in range(M):\n        i = int(i_idx[k])\n        j = int(j_idx[k])\n        li = int(labels[i])\n        lj = int(labels[j])\n        # Only consider distant subset: cross-class and s = s_cut\n        s = semantic_similarity(X[i], X[j])\n        if (li != lj) and (s = s_cut):\n            total_distant += 1\n            # Teacher\n            p = teacher_distribution(li, lj, s, C)\n            # Policy A\n            qA = mixup_target(li, lj, lam=0.5, C=C)\n            dA = kl_divergence(p, qA)\n            if dA > delta:\n                harm_counts['A'] += 1\n            # Policy B\n            qB = mixup_target(li, lj, lam=float(lam_beta[k]), C=C)\n            dB = kl_divergence(p, qB)\n            if dB > delta:\n                harm_counts['B'] += 1\n            # Policy C\n            qC = mixup_target(li, lj, lam=s, C=C)\n            dC = kl_divergence(p, qC)\n            if dC > delta:\n                harm_counts['C'] += 1\n\n    # If no distant pairs, by convention adaptive policy succeeds\n    if total_distant == 0:\n        return True\n\n    fracA = harm_counts['A'] / total_distant\n    fracB = harm_counts['B'] / total_distant\n    fracC = harm_counts['C'] / total_distant\n    # Strictly lower than both baselines\n    return (fracC  fracA) and (fracC  fracB)\n\ndef solve():\n    rng = np.random.default_rng(seed=1337)\n\n    test_cases = [\n        # (d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha)\n        (16, 3, 50, 0.6, 2.5, 0.3, 0.02, 4000, 'all', 0.4),        # Case 1\n        (16, 3, 50, 0.6, 0.5, 0.3, 0.02, 4000, 'all', 0.4),        # Case 2\n        (16, 3, 50, 0.3, 4.0, 0.3, 0.02, 4000, 'all', 0.4),        # Case 3\n        (16, 3, 50, 0.6, 2.0, 0.3, 0.02, 4000, 'same-class only', 0.4),       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha = case\n        res = evaluate_case(d, C, n, sigma, separation, s_cut, delta, M, pair_mode, alpha, rng)\n        results.append(res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3151945"}, {"introduction": "Improving aggregate metrics like accuracy is a common goal, but it is crucial to ensure that these improvements do not come at the cost of fairness. Data augmentations, if not carefully evaluated, can inadvertently amplify biases and disproportionately affect the performance on minority subgroups. This final practice provides a structured framework for conducting a fairness audit on augmentation techniques like Mixup and CutMix [@problem_id:3151966]. By implementing a formal test based on subgroup accuracy and Expected Calibration Error (ECE), you will learn how to quantitatively assess whether an augmentation introduces disproportionate harm, a vital step in building more equitable and reliable machine learning systems.", "problem": "You are given the task of designing and implementing a principled fairness test to assess whether data augmentations based on convex mixing of inputs and labels (mixup) and region replacement between images (CutMix) disproportionately harm a minority subgroup relative to a majority subgroup. The assessment must use subgroup accuracy and calibration. The solution must rely on fundamental definitions and standard empirical estimators.\n\nThe core setting is binary classification. For any example with true label $y \\in \\{0,1\\}$ and model probability score $s \\in [0,1]$ denoting the predicted probability of class $1$, define the predicted label $\\hat{y}$ as $\\hat{y} = 1$ if $s \\ge 0.5$ and $\\hat{y} = 0$ otherwise. Define the predicted-class confidence $c$ as $c = s$ if $\\hat{y} = 1$ and $c = 1-s$ if $\\hat{y} = 0$. For any finite subgroup dataset $\\mathcal{D} = \\{(y_i,s_i)\\}_{i=1}^n$, the empirical accuracy is\n$$\nA(\\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\},\n$$\nand the Expected Calibration Error (ECE) with $B$ equal-width bins is\n$$\n\\mathrm{ECE}_B(\\mathcal{D}) = \\sum_{b=1}^B \\frac{n_b}{n} \\left| \\mathrm{acc}_b - \\mathrm{conf}_b \\right|,\n$$\nwhere bin $b$ contains the indices $i$ such that $c_i$ falls into the $b$-th sub-interval of $[0,1]$ under an equal partition into $B$ bins, $n_b$ is the count in bin $b$, $\\mathrm{acc}_b = \\frac{1}{n_b} \\sum_{i \\in b} \\mathbf{1}\\{\\hat{y}_i = y_i\\}$ is the empirical accuracy in bin $b$, and $\\mathrm{conf}_b = \\frac{1}{n_b} \\sum_{i \\in b} c_i$ is the mean confidence in bin $b$. By convention, empty bins contribute zero to the sum. These definitions are the only permitted bases for your derivations and implementation.\n\nTo test for disproportionate impact of an augmentation on a minority subgroup relative to a majority subgroup, suppose we have baseline (before augmentation) predictions and augmented (after augmentation) predictions for both subgroups. Let the minority subgroup be denoted by $\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{min}}$ and $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{min}}$, and the majority by $\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{maj}}$ and $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{maj}}$. Define the accuracy harm for a subgroup $G \\in \\{\\mathrm{min}, \\mathrm{maj}\\}$ as\n$$\nH_{\\mathrm{acc}}(G) = \\max\\left(0, \\; A\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) - A\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) \\right),\n$$\nand the calibration harm for a subgroup $G$ as\n$$\nH_{\\mathrm{cal}}(G) = \\max\\left(0, \\; \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) - \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) \\right).\n$$\nGiven two nonnegative thresholds $\\tau_{\\mathrm{acc}}$ and $\\tau_{\\mathrm{cal}}$, we say there is a disproportionate impact if either\n$$\nH_{\\mathrm{acc}}(\\mathrm{min}) - H_{\\mathrm{acc}}(\\mathrm{maj})  \\tau_{\\mathrm{acc}}\n$$\nor\n$$\nH_{\\mathrm{cal}}(\\mathrm{min}) - H_{\\mathrm{cal}}(\\mathrm{maj})  \\tau_{\\mathrm{cal}}.\n$$\n\nYour program must implement this test and return, for each test case, a single boolean indicating whether a disproportionate impact is detected.\n\nUse the following fixed settings for all test cases:\n- Number of bins: $B = 5$.\n- Accuracy harm threshold: $\\tau_{\\mathrm{acc}} = 0.05$.\n- Calibration harm threshold: $\\tau_{\\mathrm{cal}} = 0.15$.\n\nImplement the following test suite. Each test case supplies arrays of ground-truth labels and predicted probabilities for the minority and majority subgroups, both at baseline and after augmentation. All arrays are ordered pairs $(\\text{labels}, \\text{scores})$, where labels are in $\\{0,1\\}$ and scores are in $[0,1]$.\n\nTest case $1$ (intended to emulate mixup harming the minority more than the majority):\n- Minority baseline: labels $[1,0,1,0,1,0]$, scores $[0.9,0.2,0.8,0.3,0.7,0.4]$.\n- Minority after augmentation: scores $[0.6,0.4,0.55,0.45,0.6,0.5]$ (labels unchanged).\n- Majority baseline: labels $[1,0,1,0,1,0,1,0,1,0,1,0]$, scores $[0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]$.\n- Majority after augmentation: scores $[0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2]$.\n\nTest case $2$ (intended to emulate CutMix improving both groups, with no disproportionate harm):\n- Minority baseline: labels $[1,0,1,0,0,1]$, scores $[0.6,0.6,0.55,0.55,0.4,0.4]$.\n- Minority after augmentation: scores $[0.85,0.2,0.75,0.25,0.3,0.8]$.\n- Majority baseline: labels $[1,0,1,0,1,0,1,0,1,0,1,0]$, scores $[0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4]$.\n- Majority after augmentation: scores $[0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15]$.\n\nTest case $3$ (boundary case where both groups degrade similarly, thus no disproportionate harm under the given thresholds):\n- Minority baseline: labels $[1,1,0,0,1,0]$, scores $[0.9,0.9,0.1,0.1,0.8,0.2]$.\n- Minority after augmentation: scores $[0.7,0.6,0.4,0.55,0.6,0.6]$.\n- Majority baseline: labels $[1,0,1,0,1,0,1,0,1,0,1,0]$, scores $[0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]$.\n- Majority after augmentation: scores $[0.6,0.6,0.6,0.6,0.49,0.49,0.49,0.49,0.8,0.2,0.8,0.2]$.\n\nAll labels remain the same before and after augmentation in each test. The output for each test case is a single boolean computed as described above.\n\nFinal output format: Your program should produce a single line of output containing a comma-separated Python-style list with one boolean per test case in the same order as above, for example, $[{\\tt True},{\\tt False},{\\tt False}]$.", "solution": "The problem requires the implementation of a specific fairness test to determine if a data augmentation technique causes disproportionate harm to a minority subgroup relative to a majority subgroup. The test is based on changes in two key metrics: accuracy and calibration error. We will first formalize the calculation of these metrics and then construct the full test procedure.\n\nThe setting is a binary classification task where for each data point $i$, we are given a true label $y_i \\in \\{0, 1\\}$ and a model's predicted probability score $s_i \\in [0, 1]$ for class $1$.\n\nFirst, we must derive two intermediate quantities for each data point: the predicted label $\\hat{y}_i$ and the predicted-class confidence $c_i$.\nThe predicted label $\\hat{y}_i$ is determined by thresholding the score $s_i$ at $0.5$:\n$$\n\\hat{y}_i = \\begin{cases} 1  \\text{if } s_i \\ge 0.5 \\\\ 0  \\text{if } s_i  0.5 \\end{cases}\n$$\nThis can be expressed using the indicator function as $\\hat{y}_i = \\mathbf{1}\\{s_i \\ge 0.5\\}$.\n\nThe predicted-class confidence $c_i$ is the model's reported probability for the predicted class. If the model predicts class $1$ (i.e., $\\hat{y}_i=1$), the confidence is simply the score $s_i$. If it predicts class $0$ (i.e., $\\hat{y}_i=0$), the confidence is $1-s_i$. Formally:\n$$\nc_i = \\begin{cases} s_i  \\text{if } \\hat{y}_i = 1 \\\\ 1-s_i  \\text{if } \\hat{y}_i = 0 \\end{cases}\n$$\nThis can be written as $c_i = s_i \\cdot \\hat{y}_i + (1-s_i) \\cdot (1-\\hat{y}_i)$.\n\nWith these quantities, we can define the two primary metrics for a given dataset $\\mathcal{D} = \\{(y_i, s_i)\\}_{i=1}^n$.\n\nThe empirical accuracy, $A(\\mathcal{D})$, is the fraction of correct predictions:\n$$\nA(\\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\}\n$$\nwhere $n$ is the number of samples in $\\mathcal{D}$.\n\nThe Expected Calibration Error, $\\mathrm{ECE}_B(\\mathcal{D})$, quantifies the discrepancy between a model's confidence and its accuracy. It is calculated by partitioning the confidence range $[0, 1]$ into $B$ equal-width bins. For each bin $b \\in \\{1, \\dots, B\\}$, we compute the average accuracy and average confidence of the samples whose confidence scores $c_i$ fall into that bin.\nLet the $b$-th bin correspond to the interval $[\\frac{b-1}{B}, \\frac{b}{B})$. The last bin, $b=B$, must include the upper bound $1$, i.e., $[\\frac{B-1}{B}, 1]$.\nFor each bin $b$, let $n_b$ be the number of samples in it. If $n_b  0$, we define the bin accuracy $\\mathrm{acc}_b$ and bin confidence $\\mathrm{conf}_b$ as:\n$$\n\\mathrm{acc}_b = \\frac{1}{n_b} \\sum_{i \\in b} \\mathbf{1}\\{\\hat{y}_i = y_i\\}\n$$\n$$\n\\mathrm{conf}_b = \\frac{1}{n_b} \\sum_{i \\in b} c_i\n$$\nThe ECE is the weighted average of the absolute difference between bin accuracy and bin confidence across all bins:\n$$\n\\mathrm{ECE}_B(\\mathcal{D}) = \\sum_{b=1}^B \\frac{n_b}{n} \\left| \\mathrm{acc}_b - \\mathrm{conf}_b \\right|\n$$\nAs per the problem, empty bins ($n_b=0$) contribute $0$ to the sum. For all test cases, the number of bins is fixed at $B=5$.\n\nNext, we define the harm metrics for a subgroup $G \\in \\{\\mathrm{min}, \\mathrm{maj}\\}$. The accuracy harm, $H_{\\mathrm{acc}}(G)$, measures the drop in accuracy after applying the augmentation. It is defined as the positive part of the difference between baseline accuracy and augmented accuracy:\n$$\nH_{\\mathrm{acc}}(G) = \\max\\left(0, \\; A\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) - A\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) \\right)\n$$\nThe calibration harm, $H_{\\mathrm{cal}}(G)$, measures the increase in calibration error. It is defined as the positive part of the difference between augmented ECE and baseline ECE:\n$$\nH_{\\mathrm{cal}}(G) = \\max\\left(0, \\; \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{aug}}_{G}\\right) - \\mathrm{ECE}_B\\!\\left(\\mathcal{D}^{\\mathrm{base}}_{G}\\right) \\right)\n$$\n\nFinally, a disproportionate impact is detected if the harm experienced by the minority group exceeds the harm experienced by the majority group by more than a specified threshold, for either accuracy or calibration. The thresholds are given as $\\tau_{\\mathrm{acc}} = 0.05$ and $\\tau_{\\mathrm{cal}} = 0.15$. The condition for disproportionate impact is:\n$$\n(H_{\\mathrm{acc}}(\\mathrm{min}) - H_{\\mathrm{acc}}(\\mathrm{maj})  \\tau_{\\mathrm{acc}}) \\quad \\lor \\quad (H_{\\mathrm{cal}}(\\mathrm{min}) - H_{\\mathrm{cal}}(\\mathrm{maj})  \\tau_{\\mathrm{cal}})\n$$\nThe implementation will consist of a function that computes $A(\\mathcal{D})$ and $\\mathrm{ECE}_B(\\mathcal{D})$ for a given dataset. This function will be called for each of the four datasets in a test case: $\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{min}}$, $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{min}}$, $\\mathcal{D}^{\\mathrm{base}}_{\\mathrm{maj}}$, and $\\mathcal{D}^{\\mathrm{aug}}_{\\mathrm{maj}}$. The resulting metrics will then be used to calculate the harm values and evaluate the final boolean condition.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(labels, scores, B):\n    \"\"\"\n    Calculates empirical accuracy and ECE for a given dataset.\n\n    Args:\n        labels (np.ndarray): Array of true labels {0, 1}.\n        scores (np.ndarray): Array of model probability scores for class 1, in [0, 1].\n        B (int): The number of bins for ECE calculation.\n\n    Returns:\n        tuple[float, float]: A tuple containing (accuracy, ECE).\n    \"\"\"\n    n = len(labels)\n    if n == 0:\n        return 0.0, 0.0\n\n    y_hat = (scores >= 0.5).astype(int)\n    confidence = np.where(y_hat == 1, scores, 1.0 - scores)\n    correct = (y_hat == labels).astype(int)\n    \n    # Calculate accuracy\n    accuracy = np.mean(correct)\n\n    # Calculate ECE\n    ece = 0.0\n    bin_width = 1.0 / B\n    for b in range(B):\n        bin_lower = b * bin_width\n        bin_upper = (b + 1) * bin_width\n\n        # The last bin is inclusive of the upper bound\n        if b == B - 1:\n            in_bin_mask = (confidence >= bin_lower)  (confidence = bin_upper)\n        else:\n            in_bin_mask = (confidence >= bin_lower)  (confidence  bin_upper)\n        \n        n_b = np.sum(in_bin_mask)\n        \n        if n_b > 0:\n            bin_correct = correct[in_bin_mask]\n            bin_confidence = confidence[in_bin_mask]\n            \n            acc_b = np.mean(bin_correct)\n            conf_b = np.mean(bin_confidence)\n            \n            ece += (n_b / n) * np.abs(acc_b - conf_b)\n\n    return accuracy, ece\n\ndef run_fairness_test(case, B, tau_acc, tau_cal):\n    \"\"\"\n    Runs the full disproportionate impact test for a single test case.\n    \n    Args:\n        case (dict): A dictionary containing the data for the test case.\n        B (int): Number of bins for ECE.\n        tau_acc (float): Accuracy harm threshold.\n        tau_cal (float): Calibration harm threshold.\n\n    Returns:\n        bool: True if disproportionate impact is detected, False otherwise.\n    \"\"\"\n    # Minority group calculations\n    min_labels_base = np.array(case[\"min_base\"][0])\n    min_scores_base = np.array(case[\"min_base\"][1])\n    min_labels_aug = np.array(case[\"min_aug\"][0])\n    min_scores_aug = np.array(case[\"min_aug\"][1])\n\n    acc_base_min, ece_base_min = calculate_metrics(min_labels_base, min_scores_base, B)\n    acc_aug_min, ece_aug_min = calculate_metrics(min_labels_aug, min_scores_aug, B)\n    \n    H_acc_min = max(0, acc_base_min - acc_aug_min)\n    H_cal_min = max(0, ece_aug_min - ece_base_min)\n\n    # Majority group calculations\n    maj_labels_base = np.array(case[\"maj_base\"][0])\n    maj_scores_base = np.array(case[\"maj_base\"][1])\n    maj_labels_aug = np.array(case[\"maj_aug\"][0])\n    maj_scores_aug = np.array(case[\"maj_aug\"][1])\n\n    acc_base_maj, ece_base_maj = calculate_metrics(maj_labels_base, maj_scores_base, B)\n    acc_aug_maj, ece_aug_maj = calculate_metrics(maj_labels_aug, maj_scores_aug, B)\n\n    H_acc_maj = max(0, acc_base_maj - acc_aug_maj)\n    H_cal_maj = max(0, ece_aug_maj - ece_base_maj)\n    \n    # Check for disproportionate impact\n    disproportionate_acc_impact = (H_acc_min - H_acc_maj) > tau_acc\n    disproportionate_cal_impact = (H_cal_min - H_cal_maj) > tau_cal\n    \n    return disproportionate_acc_impact or disproportionate_cal_impact\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the fairness test for each.\n    \"\"\"\n    # Fixed settings from the problem statement\n    B = 5\n    tau_acc = 0.05\n    tau_cal = 0.15\n\n    test_cases = [\n        # Test case 1\n        {\n            \"min_base\": ([1,0,1,0,1,0], [0.9,0.2,0.8,0.3,0.7,0.4]),\n            \"min_aug\":  ([1,0,1,0,1,0], [0.6,0.4,0.55,0.45,0.6,0.5]),\n            \"maj_base\": ([1,0,1,0,1,0,1,0,1,0,1,0], [0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]),\n            \"maj_aug\":  ([1,0,1,0,1,0,1,0,1,0,1,0], [0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2,0.8,0.2])\n        },\n        # Test case 2\n        {\n            \"min_base\": ([1,0,1,0,0,1], [0.6,0.6,0.55,0.55,0.4,0.4]),\n            \"min_aug\":  ([1,0,1,0,0,1], [0.85,0.2,0.75,0.25,0.3,0.8]),\n            \"maj_base\": ([1,0,1,0,1,0,1,0,1,0,1,0], [0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4,0.6,0.4]),\n            \"maj_aug\":  ([1,0,1,0,1,0,1,0,1,0,1,0], [0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15,0.85,0.15])\n        },\n        # Test case 3\n        {\n            \"min_base\": ([1,1,0,0,1,0], [0.9,0.9,0.1,0.1,0.8,0.2]),\n            \"min_aug\":  ([1,1,0,0,1,0], [0.7,0.6,0.4,0.55,0.6,0.6]),\n            \"maj_base\": ([1,0,1,0,1,0,1,0,1,0,1,0], [0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05,0.95,0.05]),\n            \"maj_aug\":  ([1,0,1,0,1,0,1,0,1,0,1,0], [0.6,0.6,0.6,0.6,0.49,0.49,0.49,0.49,0.8,0.2,0.8,0.2])\n        }\n    ]\n\n    # Reformat test cases to use the same labels for augmented data\n    for case in test_cases:\n        case[\"min_aug\"] = (case[\"min_base\"][0], case[\"min_aug\"][1])\n        case[\"maj_aug\"] = (case[\"maj_base\"][0], case[\"maj_aug\"][1])\n\n    results = []\n    for case in test_cases:\n        result = run_fairness_test(case, B, tau_acc, tau_cal)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3151966"}]}