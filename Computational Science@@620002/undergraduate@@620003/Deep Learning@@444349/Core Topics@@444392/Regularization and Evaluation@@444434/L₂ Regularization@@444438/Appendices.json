{"hands_on_practices": [{"introduction": "To truly grasp $L_2$ regularization, we begin with its mathematical foundation. This first practice connects the concept of weight decay, common in neural networks, to its statistical counterpart, Ridge Regression. By deriving the closed-form solution for a regularized linear model from first principles, you will gain a solid understanding of how the penalty term directly alters the optimization landscape to produce a more stable solution.", "problem": "Consider a single-layer linear neural network with parameter vector $w \\in \\mathbb{R}^{d}$ that maps an input $x \\in \\mathbb{R}^{d}$ to an output via $f_{w}(x) = w^{\\top} x$. You are given training data $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$, stacked into a design matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^{n}$. The training objective is the sum of squared errors with a weight decay penalty (Euclidean norm (L2) regularization), defined by\n$$\nJ(w) = \\lVert y - X w\\rVert_{2}^{2} + \\lambda \\lVert w\\rVert_{2}^{2},\n$$\nwhere $\\lambda > 0$ is the regularization strength.\n\nStarting only from the definition of $J(w)$ and the basic rules of differentiation and matrix algebra, derive the closed-form optimizer $w^{\\star}$ that minimizes $J(w)$, and explain why this optimizer establishes the equivalence between ridge regression and training a single-layer linear neural network with weight decay.\n\nThen, for the concrete dataset\n$$\nX = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{pmatrix},\n$$\nwith regularization parameter $\\lambda = 1$, compute the optimized weight vector $w^{\\star}$. Express your final answer as a row matrix with exact rational entries (no rounding).", "solution": "The problem asks for two main tasks: first, to derive the closed-form solution for the weights of a single-layer linear neural network trained with L2 regularization (weight decay) and explain its equivalence to ridge regression; second, to compute the specific weight vector for a given dataset.\n\nThe problem is valid as it is scientifically grounded in standard machine learning theory, well-posed with sufficient and consistent information, and phrased objectively.\n\nFirst, we derive the optimal weight vector $w^{\\star}$. The objective function to minimize is given by:\n$$\nJ(w) = \\lVert y - X w\\rVert_{2}^{2} + \\lambda \\lVert w\\rVert_{2}^{2}\n$$\nwhere $w \\in \\mathbb{R}^{d}$, $X \\in \\mathbb{R}^{n \\times d}$, $y \\in \\mathbb{R}^{n}$, and $\\lambda > 0$.\n\nTo find the minimum of $J(w)$, we must first compute its gradient with respect to $w$, denoted as $\\nabla_{w} J(w)$, and set it to zero. Let's expand the terms in $J(w)$. The squared Euclidean norm $\\lVert v\\rVert_{2}^{2}$ is equivalent to the dot product $v^{\\top}v$.\n\nThe first term is the sum of squared errors:\n$$\n\\lVert y - X w\\rVert_{2}^{2} = (y - X w)^{\\top} (y - X w) = (y^{\\top} - w^{\\top} X^{\\top})(y - X w) = y^{\\top}y - y^{\\top}Xw - w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw\n$$\nSince $y^{\\top}Xw$ is a scalar, it is equal to its transpose $(y^{\\top}Xw)^{\\top} = w^{\\top}X^{\\top}y$. Thus, we can combine the cross-terms:\n$$\n\\lVert y - X w\\rVert_{2}^{2} = y^{\\top}y - 2w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw\n$$\n\nThe second term is the weight decay penalty:\n$$\n\\lambda \\lVert w\\rVert_{2}^{2} = \\lambda w^{\\top}w\n$$\n\nCombining these, the full objective function is:\n$$\nJ(w) = y^{\\top}y - 2w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw + \\lambda w^{\\top}w\n$$\n\nNow we differentiate $J(w)$ with respect to the vector $w$. We use the following standard rules of matrix calculus:\n1.  $\\nabla_{w} (c) = 0$ for a constant $c$.\n2.  $\\nabla_{w} (w^{\\top}a) = \\nabla_{w} (a^{\\top}w) = a$ for a vector $a$.\n3.  $\\nabla_{w} (w^{\\top}Mw) = (M + M^{\\top})w$. If $M$ is symmetric, this simplifies to $2Mw$.\n\nApplying these rules to $J(w)$:\n- The term $y^{\\top}y$ is constant with respect to $w$, so its gradient is $0$.\n- For the term $-2w^{\\top}X^{\\top}y$, the vector $X^{\\top}y$ is constant with respect to $w$. So, $\\nabla_{w}(-2w^{\\top}X^{\\top}y) = -2X^{\\top}y$.\n- For the term $w^{\\top}X^{\\top}Xw$, the matrix $M = X^{\\top}X$ is symmetric. Thus, $\\nabla_{w}(w^{\\top}X^{\\top}Xw) = 2(X^{\\top}X)w$.\n- For the term $\\lambda w^{\\top}w$, we can write it as $\\lambda w^{\\top}Iw$ where $I$ is the identity matrix. Since $I$ is symmetric, $\\nabla_{w}(\\lambda w^{\\top}Iw) = \\lambda(2Iw) = 2\\lambda w$.\n\nSumming these gradients, we get:\n$$\n\\nabla_{w} J(w) = -2X^{\\top}y + 2(X^{\\top}X)w + 2\\lambda w\n$$\n\nTo find the optimal weights $w^{\\star}$ that minimize $J(w)$, we set the gradient to the zero vector:\n$$\n\\nabla_{w} J(w^{\\star}) = -2X^{\\top}y + 2(X^{\\top}X)w^{\\star} + 2\\lambda w^{\\star} = 0\n$$\nDividing by $2$ and rearranging the terms:\n$$\n(X^{\\top}X)w^{\\star} + \\lambda w^{\\star} = X^{\\top}y\n$$\nFactoring out $w^{\\star}$ on the left side (note that $\\lambda w^{\\star} = \\lambda I w^{\\star}$ where $I$ is the $d \\times d$ identity matrix):\n$$\n(X^{\\top}X + \\lambda I) w^{\\star} = X^{\\top}y\n$$\nTo isolate $w^{\\star}$, we multiply by the inverse of the matrix $(X^{\\top}X + \\lambda I)$. This inverse exists because $X^{\\top}X$ is a positive semi-definite matrix, and for $\\lambda > 0$, $\\lambda I$ is a positive definite matrix. The sum of a positive semi-definite and a positive definite matrix is always positive definite, and hence invertible.\n$$\nw^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y\n$$\nThis is the closed-form solution for the optimizer $w^{\\star}$.\n\nThis result establishes the equivalence with ridge regression. The standard formulation of ridge regression is to find the coefficient vector $w$ that minimizes the penalized sum of squared errors, which is precisely the objective function $J(w)$. The derived solution $w^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y$ is the well-known closed-form solution for ridge regression. Therefore, training a single-layer linear neural network with a sum of squared errors loss and L2 weight decay is mathematically identical to performing ridge regression.\n\nNext, we compute $w^{\\star}$ for the given dataset:\n$$\nX = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\lambda = 1\n$$\nThe weight vector $w$ will be in $\\mathbb{R}^{2}$. First, we compute the required matrices.\nThe transpose of $X$ is:\n$$\nX^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nNext, we compute $X^{\\top}X$:\n$$\nX^{\\top}X = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(0)+1(1) & 1(0)+0(1)+1(1) \\\\ 0(1)+1(0)+1(1) & 0(0)+1(1)+1(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nNow we compute the term $(X^{\\top}X + \\lambda I)$. Since $X^{\\top}X$ is a $2 \\times 2$ matrix, $I$ is the $2 \\times 2$ identity matrix.\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + 1 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nNext, we find the inverse of this matrix. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant is $\\det(X^{\\top}X + \\lambda I) = 3(3) - 1(1) = 9 - 1 = 8$.\nThe inverse is:\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}\n$$\nNow, we compute the term $X^{\\top}y$:\n$$\nX^{\\top}y = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(2)+1(3) \\\\ 0(1)+1(2)+1(3) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n$$\nFinally, we compute $w^{\\star}$ by multiplying the two results:\n$$\nw^{\\star} = (X^{\\top}X + \\lambda I)^{-1} (X^{\\top}y) = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3(4) - 1(5) \\\\ -1(4) + 3(5) \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 12 - 5 \\\\ -4 + 15 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 7 \\\\ 11 \\end{pmatrix}\n$$\nSo, the optimized weight vector is:\n$$\nw^{\\star} = \\begin{pmatrix} \\frac{7}{8} \\\\ \\frac{11}{8} \\end{pmatrix}\n$$\nThe problem asks for the final answer as a row matrix.\n$$\n(w^{\\star})^{\\top} = \\begin{pmatrix} \\frac{7}{8} & \\frac{11}{8} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{8} & \\frac{11}{8}\n\\end{pmatrix}\n}\n$$", "id": "3169526"}, {"introduction": "Having established the mathematical basis of $L_2$ regularization [@problem_id:3169526], we now turn to observing its effects empirically. This hands-on coding exercise allows you to design a controlled experiment that vividly demonstrates regularization's primary role in preventing overfitting. You will witness firsthand how a model, without regularization, memorizes random noise in the labels, and how by tuning the strength $\\lambda$, you can guide it toward genuine generalization.", "problem": "You are asked to implement a complete, runnable program that constructs a controlled synthetic binary classification experiment to study how squared Euclidean norm regularization (also called $L_{2}$ regularization or weight decay) can prevent memorization of random labels and thereby improve generalization. The program must produce a single line of output in a precisely specified format.\n\nThe program must adhere to the following formal design.\n\n1. Data generation. For each test case, generate a training design matrix $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ and a test design matrix $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$, where $d = d_{\\text{sig}} + d_{\\text{noi}}$. Each row of $X_{\\text{train}}$ and $X_{\\text{test}}$ must be independently sampled from a zero-mean, identity-covariance multivariate normal distribution. Construct a ground-truth weight vector $w^{\\star} \\in \\mathbb{R}^{d}$ whose first $d_{\\text{sig}}$ entries are nonzero and whose last $d_{\\text{noi}}$ entries are exactly zero; ensure that $w^{\\star}$ has unit Euclidean norm. Define the clean labels by $y^{\\text{clean}} = \\operatorname{sign}(X w^{\\star})$ elementwise, taking values in $\\{-1, +1\\}$. For the training set, construct the observed (possibly corrupted) labels $y_{\\text{train}}$ by independently replacing each $y^{\\text{clean}}_{i}$ with a random draw from $\\{-1, +1\\}$ with probability $p$ (and leaving it unchanged with probability $1 - p$). For the test set, use $y_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}} w^{\\star})$ without any corruption. The labels are therefore balanced in expectation, and the chance-level accuracy on the test set is $0.5$ by symmetry.\n\n2. Model class and learning objective. Consider linear predictors $f_{w}(x) = x^{\\top} w$ with $w \\in \\mathbb{R}^{d}$ and no bias term. Train $w$ by minimizing the empirical risk with $L_{2}$ regularization: minimize, over $w \\in \\mathbb{R}^{d}$,\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i}^{\\top} w - y_{\\text{train},i}\\right)^{2} \\;+\\; \\lambda \\,\\lVert w \\rVert_{2}^{2},\n$$\nfor multiple values of the regularization strength $\\lambda \\ge 0$ provided in a grid. After training, classify by $\\hat{y} = \\operatorname{sign}(X_{\\text{test}} w)$ and compute the test accuracy as the fraction of correct predictions with respect to $y_{\\text{test}}$.\n\n3. Transition criterion and critical $\\lambda$. Define the improvement threshold $\\tau$ (strictly greater than the chance level $0.5$). For a given test case and a given ascending grid of $\\lambda$ values, define the critical $\\lambda$ as the smallest $\\lambda$ in the grid such that the corresponding test accuracy is at least $\\tau$. If no $\\lambda$ in the provided grid achieves test accuracy at least $\\tau$, then define the critical $\\lambda$ to be $-1.0$.\n\n4. Determinism. Use a fixed pseudorandom number generator seed per test case. When computing a solution for $\\lambda = 0$, handle potential singularity in the normal equations in a deterministic manner so that the training rule is well-defined and reproducible.\n\n5. Test suite. Your program must execute the following three test cases and process each independently:\n   - Case A (random-label memorization discouraged by $L_{2}$): $n = 120$, $m = 4000$, $d_{\\text{sig}} = 5$, $d_{\\text{noi}} = 300$, $p = 0.4$, improvement threshold $\\tau = 0.6$, regularization grid\n     $$\n     \\Lambda_{A} = [\\,0.0,\\; 10^{-6},\\; 10^{-5},\\; 3\\cdot 10^{-5},\\; 10^{-4},\\; 3\\cdot 10^{-4},\\; 10^{-3},\\; 3\\cdot 10^{-3},\\; 10^{-2},\\; 3\\cdot 10^{-2},\\; 10^{-1},\\; 3\\cdot 10^{-1},\\; 1.0,\\; 3.0,\\; 10.0\\,].\n     $$\n   - Case B (fully random labels; no improvement possible): $n = 120$, $m = 4000$, $d_{\\text{sig}} = 5$, $d_{\\text{noi}} = 300$, $p = 1.0$, improvement threshold $\\tau = 0.6$, regularization grid\n     $$\n     \\Lambda_{B} = \\Lambda_{A}.\n     $$\n   - Case C (clean, low-dimensional true signal; improvement immediate): $n = 200$, $m = 2000$, $d_{\\text{sig}} = 1$, $d_{\\text{noi}} = 0$, $p = 0.0$, improvement threshold $\\tau = 0.6$, regularization grid\n     $$\n     \\Lambda_{C} = \\Lambda_{A}.\n     $$\n\n   For reproducibility, use the per-case random seeds $s_{A} = 12345$, $s_{B} = 13345$, and $s_{C} = 14345$.\n\n6. Output specification. Your program must produce a single line of output containing a list with exactly three floating-point values in the order of cases A, B, C, where each value is the critical regularization strength for that case as defined above, and with the value $-1.0$ used when no transition to improved generalization is observed. The line must be in the exact format\n$$\n[\\ell_{A},\\ell_{B},\\ell_{C}]\n$$\nwith the values separated by commas and no additional text. No physical units are involved.\n\nYour goal is to implement this experiment in a way that directly tests whether and when $L_{2}$ regularization prevents memorization of random labels by shrinking the weight vector, and to measure the smallest regularization strength at which test accuracy transitions from chance to improved generalization for each of the three cases.", "solution": "The problem requires the implementation of a numerical experiment to investigate the role of $L_2$ regularization in a binary classification context. The experiment is designed to show how regularization can prevent a model from memorizing noise in the training labels, thereby improving its ability to generalize to unseen data. The core of the task is to find the \"critical\" regularization strength $\\lambda$ at which the model's performance on a test set transitions from chance-level to a specified threshold of improvement. This will be done for three distinct cases designed to highlight different aspects of regularization.\n\nThe model is a linear predictor $f_w(x) = x^{\\top} w$, where $x \\in \\mathbb{R}^{d}$ is a feature vector and $w \\in \\mathbb{R}^{d}$ is the weight vector. The weights are learned by minimizing a regularized empirical risk objective function based on the squared error loss. The objective function $L(w)$ for a training set of $n$ samples $\\{(x_i, y_i)\\}_{i=1}^{n}$ is given by:\n$$\nL(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i}^{\\top} w - y_i\\right)^{2} + \\lambda \\lVert w \\rVert_{2}^{2}\n$$\nHere, $y_i \\in \\{-1, +1\\}$ are the training labels, and $\\lambda \\ge 0$ is the regularization parameter that controls the penalty on the squared Euclidean norm of the weight vector, $\\lVert w \\rVert_{2}^{2}$.\n\nTo find the optimal weight vector $\\hat{w}$ that minimizes $L(w)$, we compute the gradient of $L(w)$ with respect to $w$ and set it to the zero vector. First, we express the objective function in matrix notation. Let $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ be the design matrix where the $i$-th row is $x_i^{\\top}$, and let $y_{\\text{train}} \\in \\mathbb{R}^{n}$ be the vector of training labels. The objective function becomes:\n$$\nL(w) = \\frac{1}{n} \\lVert X_{\\text{train}}w - y_{\\text{train}} \\rVert_{2}^{2} + \\lambda \\lVert w \\rVert_{2}^{2} = \\frac{1}{n} (X_{\\text{train}}w - y_{\\text{train}})^{\\top}(X_{\\text{train}}w - y_{\\text{train}}) + \\lambda w^{\\top}w\n$$\nThe gradient of $L(w)$ with respect to $w$ is:\n$$\n\\nabla_w L(w) = \\frac{1}{n} \\nabla_w (w^{\\top}X_{\\text{train}}^{\\top}X_{\\text{train}}w - 2y_{\\text{train}}^{\\top}X_{\\text{train}}w + y_{\\text{train}}^{\\top}y_{\\text{train}}) + \\lambda \\nabla_w (w^{\\top}w)\n$$\nUsing standard matrix calculus identities, and noting that $X_{\\text{train}}^{\\top}X_{\\text{train}}$ is a symmetric matrix, we get:\n$$\n\\nabla_w L(w) = \\frac{1}{n} (2X_{\\text{train}}^{\\top}X_{\\text{train}}w - 2X_{\\text{train}}^{\\top}y_{\\text{train}}) + 2\\lambda w\n$$\nSetting the gradient to zero to find the minimum:\n$$\n\\frac{2}{n} X_{\\text{train}}^{\\top}X_{\\text{train}}w - \\frac{2}{n} X_{\\text{train}}^{\\top}y_{\\text{train}} + 2\\lambda w = 0\n$$\n$$\n(X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I) w = X_{\\text{train}}^{\\top}y_{\\text{train}}\n$$\nwhere $I$ is the $d \\times d$ identity matrix.\n\nThis system of linear equations provides the solution for the optimal weight vector $\\hat{w}$.\n\nFor $\\lambda > 0$, the matrix $(X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I)$ is guaranteed to be invertible. This is because $X_{\\text{train}}^{\\top}X_{\\text{train}}$ is positive semi-definite, and $n\\lambda I$ is positive definite, making their sum positive definite and thus invertible. The unique solution is:\n$$\n\\hat{w} = (X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I)^{-1} X_{\\text{train}}^{\\top} y_{\\text{train}}\n$$\nNumerically, it is more stable and efficient to solve the linear system directly rather than computing the matrix inverse.\n\nFor the special case $\\lambda = 0$, the objective becomes the standard unregularized least-squares problem, and the normal equations are $X_{\\text{train}}^{\\top}X_{\\text{train}}w = X_{\\text{train}}^{\\top}y_{\\text{train}}$. In high-dimensional settings where the number of features $d$ is greater than the number of samples $n$ (as in Test Cases A and B), the matrix $X_{\\text{train}}^{\\top}X_{\\text{train}}$ is singular, and the system has infinitely many solutions. The problem requires a deterministic handling of this case. The standard approach, which yields the solution with the minimum Euclidean norm, is to use the Moore-Penrose pseudoinverse. This is equivalent to solving the least-squares problem $\\min_{w} \\lVert X_{\\text{train}}w - y_{\\text{train}} \\rVert_2^2$, which can be readily done using numerical linear algebra libraries.\n\nThe experimental procedure for each test case is as follows:\n1.  Initialize a pseudorandom number generator with the specified seed for reproducibility.\n2.  Construct the ground-truth weight vector $w^{\\star} \\in \\mathbb{R}^{d}$ by generating its first $d_{\\text{sig}}$ components from a standard normal distribution, normalizing this sub-vector to have unit norm, and setting the remaining $d_{\\text{noi}}$ components to zero. This ensures $\\lVert w^{\\star} \\rVert_{2} = 1$.\n3.  Generate the training data $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ and test data $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$ by drawing each entry independently from a standard normal distribution $\\mathcal{N}(0, 1)$.\n4.  Generate the labels. The true test labels are $y_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}} w^{\\star})$. The training labels $y_{\\text{train}}$ are generated by first computing the clean labels $y^{\\text{clean}} = \\operatorname{sign}(X_{\\text{train}} w^{\\star})$ and then, for each label, replacing it with a random value from $\\{-1, +1\\}$ with probability $p$. The `sign` function's output for an input of $0$ will be deterministically mapped to $+1$.\n5.  Iterate through the provided ascending grid of $\\lambda$ values. For each $\\lambda$, compute the corresponding optimal weight vector $\\hat{w}$.\n6.  Use the learned $\\hat{w}$ to make predictions on the test set: $\\hat{y}_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}}\\hat{w})$.\n7.  Calculate the test accuracy as the fraction of matching predictions: $\\frac{1}{m} \\sum_{j=1}^{m} \\mathbb{I}(\\hat{y}_{\\text{test},j} = y_{\\text{test},j})$.\n8.  The critical $\\lambda$ is the first value in the grid for which the test accuracy is greater than or equal to the improvement threshold $\\tau$. If no such $\\lambda$ is found, the critical value is defined as $-1.0$.\n\nThis procedure is applied independently to each of the three test cases, and the resulting critical $\\lambda$ values are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes a synthetic experiment to study L2 regularization.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    lambda_grid = [0.0, 1e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 0.1, 0.3, 1.0, 3.0, 10.0]\n    \n    test_cases = [\n        # Case A: High-dimensional, noisy labels\n        {'n': 120, 'm': 4000, 'd_sig': 5, 'd_noi': 300, 'p': 0.4, 'tau': 0.6, 'seed': 12345, 'lambda_grid': lambda_grid},\n        # Case B: Fully random labels\n        {'n': 120, 'm': 4000, 'd_sig': 5, 'd_noi': 300, 'p': 1.0, 'tau': 0.6, 'seed': 13345, 'lambda_grid': lambda_grid},\n        # Case C: Clean, low-dimensional signal\n        {'n': 200, 'm': 2000, 'd_sig': 1, 'd_noi': 0, 'p': 0.0, 'tau': 0.6, 'seed': 14345, 'lambda_grid': lambda_grid},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, m, d_sig, d_noi, p, tau, seed, current_lambda_grid = \\\n            case['n'], case['m'], case['d_sig'], case['d_noi'], case['p'], case['tau'], case['seed'], case['lambda_grid']\n        \n        d = d_sig + d_noi\n        \n        # 1. Deterministic Setup\n        rng = np.random.default_rng(seed)\n\n        # 2. Data Generation\n        # Construct ground-truth weight vector w_star with unit norm\n        w_star_sig = rng.standard_normal(size=d_sig)\n        w_star_sig /= np.linalg.norm(w_star_sig)\n        w_star = np.concatenate((w_star_sig, np.zeros(d_noi)))\n        \n        # Generate design matrices\n        X_train = rng.standard_normal(size=(n, d))\n        X_test = rng.standard_normal(size=(m, d))\n        \n        # Generate labels\n        # Test labels (clean)\n        y_test = np.sign(X_test @ w_star)\n        y_test[y_test == 0] = 1 # Deterministic handling of sign(0)\n\n        # Training labels (potentially corrupted)\n        y_clean_train = np.sign(X_train @ w_star)\n        y_clean_train[y_clean_train == 0] = 1\n        \n        y_train = y_clean_train.copy()\n        corruption_mask = rng.random(size=n) < p\n        num_corrupt = np.sum(corruption_mask)\n        if num_corrupt > 0:\n            random_labels = rng.choice([-1, 1], size=num_corrupt)\n            y_train[corruption_mask] = random_labels\n\n        # 3. Model Training and Evaluation\n        critical_lambda = -1.0\n        \n        # Pre-compute parts of the normal equations\n        XtX = X_train.T @ X_train\n        Xty = X_train.T @ y_train\n        identity_d = np.identity(d)\n\n        for lam in current_lambda_grid:\n            # Solve for the weight vector w_hat\n            if lam == 0.0:\n                # For lambda=0, use lstsq for the min-norm solution, robust to singularity\n                w_hat = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n            else:\n                # For lambda > 0, solve the regularized normal equations\n                A = XtX + n * lam * identity_d\n                w_hat = np.linalg.solve(A, Xty)\n            \n            # Predict on test set\n            y_pred = np.sign(X_test @ w_hat)\n            y_pred[y_pred == 0] = 1 # Deterministic handling of sign(0)\n            \n            # Compute test accuracy\n            accuracy = np.mean(y_pred == y_test)\n            \n            # Check for transition\n            if accuracy >= tau:\n                critical_lambda = lam\n                break\n        \n        results.append(critical_lambda)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141360"}, {"introduction": "Effective application of regularization involves more nuance than simply adding a penalty to the loss function. This practice delves into a crucial, practical question: should we apply $L_2$ regularization to the bias (or intercept) terms in a model? By analyzing the specific role biases play in capturing a model's baseline predictions, you will discover why excluding them from weight decay is often a superior strategy for achieving better model calibration and performance.", "problem": "Consider multiclass linear classification with $K$ classes, where each input $\\mathbf{x} \\in \\mathbb{R}^d$ is mapped to logits $\\mathbf{z}(\\mathbf{x}) \\in \\mathbb{R}^K$ via $\\mathbf{z}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$ with weight matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times d}$ and bias vector $\\mathbf{b} \\in \\mathbb{R}^K$. Predicted class probabilities are $p_k(\\mathbf{x}) = \\exp(z_k(\\mathbf{x}))/\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))$. For a dataset $\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N$ with one-hot labels $\\mathbf{y}_i \\in \\{0,1\\}^K$ and empirical class frequencies $\\hat{\\pi}_k = \\frac{1}{N}\\sum_{i=1}^N y_{ik}$, the average cross-entropy is\n$$\n\\mathcal{L}_{\\text{CE}}(\\mathbf{W},\\mathbf{b}) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K y_{ik}\\,\\log p_k(\\mathbf{x}_i).\n$$\nA common modification is to add an $L_2$ penalty (also called weight decay), yielding\n$$\n\\mathcal{L}_{\\lambda}(\\mathbf{W},\\mathbf{b}) = \\mathcal{L}_{\\text{CE}}(\\mathbf{W},\\mathbf{b}) + \\frac{\\lambda}{2}\\left(\\lVert \\mathbf{W}\\rVert_F^2 + \\alpha \\lVert \\mathbf{b}\\rVert_2^2\\right),\n$$\nwhere $\\lambda > 0$ is fixed and $\\alpha \\in \\{0,1\\}$ indicates whether biases are penalized ($\\alpha = 1$) or excluded from decay ($\\alpha = 0$). Assume inputs are centered so that $\\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i = \\mathbf{0}$. Using only fundamental definitions of the softmax, cross-entropy, and $L_2$ regularization, reason about the effect of setting $\\alpha = 1$ versus $\\alpha = 0$ on probability calibration and on the position and margin of the decision boundary.\n\nSelect all statements that are correct.\n\nA. Penalizing $\\mathbf{b}$ can harm probability calibration under class imbalance because the intercepts encode base rates; shrinking $\\mathbf{b}$ toward $\\mathbf{0}$ pulls predicted probabilities toward a uniform distribution across classes, potentially misrepresenting true class priors.\n\nB. Because the softmax is invariant to adding the same constant to all logits for each input, regularizing $\\mathbf{b}$ cannot affect predicted probabilities; therefore it cannot affect calibration or margins.\n\nC. Including an $L_2$ penalty on $\\mathbf{b}$ can move the decision boundary $\\{\\mathbf{x} : \\mathbf{w}_k^{\\top}\\mathbf{x} + b_k = \\mathbf{w}_{\\ell}^{\\top}\\mathbf{x} + b_{\\ell}\\}$ between any pair of classes $k,\\ell$, and for fixed $\\lVert \\mathbf{W}\\rVert_F$ it can reduce the minimal geometric margin $\\min_i \\frac{z_{y_i}(\\mathbf{x}_i) - \\max_{k \\neq y_i} z_k(\\mathbf{x}_i)}{\\lVert \\mathbf{W}\\rVert_F}$.\n\nD. It is always preferable to decay $\\mathbf{b}$ exactly as strongly as $\\mathbf{W}$ because bias parameters overfit as easily as weights; excluding biases from weight decay cannot be justified.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: Multiclass linear classification with $K$ classes.\n- **Input**: $\\mathbf{x} \\in \\mathbb{R}^d$.\n- **Parameters**: Weight matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times d}$ and bias vector $\\mathbf{b} \\in \\mathbb{R}^K$.\n- **Logits**: $\\mathbf{z}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$. The $k$-th logit is $z_k(\\mathbf{x}) = \\mathbf{w}_k^{\\top}\\mathbf{x} + b_k$, where $\\mathbf{w}_k^{\\top}$ is the $k$-th row of $\\mathbf{W}$.\n- **Probabilities (Softmax)**: $p_k(\\mathbf{x}) = \\frac{\\exp(z_k(\\mathbf{x}))}{\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))}$.\n- **Dataset**: $\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N$, where labels $\\mathbf{y}_i \\in \\{0,1\\}^K$ are one-hot encoded vectors.\n- **Empirical Class Frequencies**: $\\hat{\\pi}_k = \\frac{1}{N}\\sum_{i=1}^N y_{ik}$.\n- **Cross-Entropy Loss**: $\\mathcal{L}_{\\text{CE}}(\\mathbf{W},\\mathbf{b}) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K y_{ik}\\,\\log p_k(\\mathbf{x}_i)$.\n- **Regularized Loss**: $\\mathcal{L}_{\\lambda}(\\mathbf{W},\\mathbf{b}) = \\mathcal{L}_{\\text{CE}}(\\mathbf{W},\\mathbf{b}) + \\frac{\\lambda}{2}\\left(\\lVert \\mathbf{W}\\rVert_F^2 + \\alpha \\lVert \\mathbf{b}\\rVert_2^2\\right)$, with $\\lambda > 0$ and $\\alpha \\in \\{0,1\\}$.\n- **Assumption**: Inputs are centered: $\\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i = \\mathbf{0}$.\n- **Question**: Analyze the effect of penalizing biases ($\\alpha=1$) versus not penalizing them ($\\alpha=0$) on probability calibration, decision boundary position, and margin.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is set within the standard framework of multiclass logistic regression with L2 regularization, a fundamental topic in machine learning. All definitions are standard and correct.\n- **Well-Posed**: The problem asks for a conceptual analysis of the role of a hyperparameter, which is a well-defined task with a determinable outcome based on the provided mathematical structure.\n- **Objective**: The language is precise, and the definitions are formal.\n- **Conclusion**: The problem is scientifically sound, well-posed, and objective. It contains no inconsistencies or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be derived.\n\n### Analysis\nThe core of the problem is to understand the role of the bias vector $\\mathbf{b}$ and the consequences of applying an $L_2$ penalty to it.\n\n**The Role of the Bias Vector $\\mathbf{b}$**\n\nLet's first analyze the model without regularization on the bias term (i.e., $\\alpha=0$). The parameters are found by minimizing the cross-entropy loss $\\mathcal{L}_{\\text{CE}}$. The gradient of $\\mathcal{L}_{\\text{CE}}$ with respect to the bias $b_k$ is:\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial b_k} = \\sum_{i=1}^N \\sum_{j=1}^K \\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial p_j(\\mathbf{x}_i)} \\frac{\\partial p_j(\\mathbf{x}_i)}{\\partial z_k(\\mathbf{x}_i)} \\frac{\\partial z_k(\\mathbf{x}_i)}{\\partial b_k}\n$$\nA more direct calculation using the chain rule on the log-softmax shows that the gradient with respect to the logit $z_k(\\mathbf{x}_i)$ is $\\frac{1}{N}(p_k(\\mathbf{x}_i) - y_{ik})$. Since $\\frac{\\partial z_k(\\mathbf{x}_i)}{\\partial b_k} = 1$, the gradient with respect to $b_k$ is:\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{CE}}}{\\partial b_k} = \\frac{1}{N} \\sum_{i=1}^N (p_k(\\mathbf{x}_i) - y_{ik})\n$$\nAt the minimum of the loss function, this gradient must be zero. Thus, for the optimal parameters, we have:\n$$\n\\sum_{i=1}^N p_k(\\mathbf{x}_i) = \\sum_{i=1}^N y_{ik} = N \\hat{\\pi}_k\n$$\nThis equation shows that the sum of predicted probabilities for class $k$ over the training set must equal the total number of examples of class $k$. This is a fundamental property of maximum likelihood estimation for the exponential family, of which logistic regression is a member.\n\nNow, consider the average logit for class $k$ over the dataset, using the given assumption that inputs are centered ($\\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i = \\mathbf{0}$):\n$$\n\\frac{1}{N}\\sum_{i=1}^N z_k(\\mathbf{x}_i) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{w}_k^{\\top}\\mathbf{x}_i + b_k) = \\mathbf{w}_k^{\\top} \\left(\\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i\\right) + b_k = \\mathbf{w}_k^{\\top}\\mathbf{0} + b_k = b_k\n$$\nSo, the optimal bias term $b_k$ is precisely the average logit for class $k$ over the training data. The bias terms absorb the \"base rate\" information for each class. For an input $\\mathbf{x} = \\mathbf{0}$ (the mean of the data), the logits are simply $\\mathbf{z}(\\mathbf{0}) = \\mathbf{b}$, and the probabilities $p_k(\\mathbf{0}) = \\exp(b_k)/\\sum_j \\exp(b_j)$ reflect the model's estimate of class priors. If classes are imbalanced, the optimal $b_k$ values will be different to reflect this.\n\n**The Effect of Penalizing the Bias Vector $\\mathbf{b}$ ($\\alpha=1$)**\n\nWhen we add the penalty $\\frac{\\lambda}{2} \\lVert \\mathbf{b} \\rVert_2^2$, the loss function becomes $\\mathcal{L}_{\\lambda} = \\mathcal{L}_{\\text{CE}} + \\frac{\\lambda}{2}(\\lVert \\mathbf{W}\\rVert_F^2 + \\lVert \\mathbf{b}\\rVert_2^2)$. The gradient with respect to $b_k$ is now:\n$$\n\\frac{\\partial \\mathcal{L}_{\\lambda}}{\\partial b_k} = \\frac{1}{N} \\sum_{i=1}^N (p_k(\\mathbf{x}_i) - y_{ik}) + \\lambda b_k\n$$\nSetting this to zero at the minimum yields $\\lambda b_k = -\\frac{1}{N} \\sum_{i=1}^N (p_k(\\mathbf{x}_i) - y_{ik})$, or $b_k = \\frac{1}{\\lambda}(\\hat{\\pi}_k - \\bar{p}_k)$, where $\\bar{p}_k = \\frac{1}{N}\\sum_i p_k(\\mathbf{x}_i)$. This regularization term pulls each $b_k$ towards $0$. As all $b_k$ components are shrunk towards zero, they become more similar. For $\\mathbf{x}=\\mathbf{0}$, the probabilities $p_k(\\mathbf{0})$ will tend towards $\\frac{1}{K}$, representing a uniform distribution.\n\n### Option-by-Option Analysis\n\n**A. Penalizing $\\mathbf{b}$ can harm probability calibration under class imbalance because the intercepts encode base rates; shrinking $\\mathbf{b}$ toward $\\mathbf{0}$ pulls predicted probabilities toward a uniform distribution across classes, potentially misrepresenting true class priors.**\n\nAs derived above, the bias terms $b_k$ capture the average class logits, which are closely related to the class base rates or priors $\\hat{\\pi}_k$. If the classes are imbalanced (e.g., $\\hat{\\pi}_1 \\gg \\hat{\\pi}_2$), the unpenalized optimal biases $b_k$ would reflect this imbalance. Applying an $L_2$ penalty on $\\mathbf{b}$ shrinks the biases towards the origin. This forces the model's baseline prediction (e.g., at the mean input $\\mathbf{x}=\\mathbf{0}$) to be closer to a uniform distribution $(1/K, 1/K, \\ldots, 1/K)$, which does not reflect the true imbalanced priors. This distortion of the baseline class probabilities can negatively impact the model's calibration, as the predicted probabilities no longer accurately represent the true likelihoods across the full range of inputs.\n\n**Verdict: Correct.**\n\n**B. Because the softmax is invariant to adding the same constant to all logits for each input, regularizing $\\mathbf{b}$ cannot affect predicted probabilities; therefore it cannot affect calibration or margins.**\n\nThe softmax function is indeed invariant to a common shift in its inputs: $p_k(\\mathbf{z}+c\\mathbf{1}) = \\frac{\\exp(z_k+c)}{\\sum_j \\exp(z_j+c)} = p_k(\\mathbf{z})$. This means that if we change $\\mathbf{b}$ to $\\mathbf{b}' = \\mathbf{b} + c\\mathbf{1}$ (where $\\mathbf{1}$ is a vector of ones), the probabilities $p_k(\\mathbf{x})$ and thus the cross-entropy loss $\\mathcal{L}_{\\text{CE}}$ remain unchanged for a fixed $\\mathbf{W}$. However, the $L_2$ penalty on the bias, $\\lVert \\mathbf{b} \\rVert_2^2 = \\sum_k b_k^2$, is *not* invariant to this shift:\n$$\n\\lVert \\mathbf{b}' \\rVert_2^2 = \\sum_{k=1}^K (b_k+c)^2 = \\sum_{k=1}^K (b_k^2 + 2cb_k + c^2) = \\lVert \\mathbf{b} \\rVert_2^2 + 2c \\sum_{k=1}^K b_k + K c^2 \\neq \\lVert \\mathbf{b} \\rVert_2^2 \\quad (\\text{for } c \\neq 0)\n$$\nBecause the regularized loss function $\\mathcal{L}_{\\lambda}$ includes this non-invariant term, the optimization process is no longer indifferent to such shifts. The penalty term breaks the symmetry and forces the optimizer to select a specific $\\mathbf{b}$ (one with a small $L_2$ norm), which in turn affects the learned model and its predicted probabilities. The premise that regularizing $\\mathbf{b}$ cannot affect probabilities is false.\n\n**Verdict: Incorrect.**\n\n**C. Including an $L_2$ penalty on $\\mathbf{b}$ can move the decision boundary $\\{\\mathbf{x} : \\mathbf{w}_k^{\\top}\\mathbf{x} + b_k = \\mathbf{w}_{\\ell}^{\\top}\\mathbf{x} + b_{\\ell}\\}$ between any pair of classes $k,\\ell$, and for fixed $\\lVert \\mathbf{W}\\rVert_F$ it can reduce the minimal geometric margin $\\min_i \\frac{z_{y_i}(\\mathbf{x}_i) - \\max_{k \\neq y_i} z_k(\\mathbf{x}_i)}{\\lVert \\mathbf{W}\\rVert_F}$.**\n\nLet's analyze the two parts of this statement.\n1.  **Decision Boundary**: The decision boundary between classes $k$ and $\\ell$ is the set of points $\\mathbf{x}$ where the logits are equal: $z_k(\\mathbf{x}) = z_\\ell(\\mathbf{x})$. This gives the equation of a hyperplane:\n    $$\n    \\mathbf{w}_k^{\\top}\\mathbf{x} + b_k = \\mathbf{w}_{\\ell}^{\\top}\\mathbf{x} + b_{\\ell} \\implies (\\mathbf{w}_k - \\mathbf{w}_{\\ell})^{\\top}\\mathbf{x} + (b_k - b_{\\ell}) = 0\n    $$\n    The term $(b_k - b_{\\ell})$ acts as an offset, shifting the hyperplane. Since including the penalty on $\\mathbf{b}$ changes the optimization problem, it will lead to a different optimal solution $(\\mathbf{W}^*, \\mathbf{b}^*)$. The resulting $b_k^*$ and $b_\\ell^*$ will generally be different from the unpenalized solution, thus altering the offset $(b_k^* - b_\\ell^*)$ and moving the decision boundary. This part is correct.\n2.  **Margin**: The problem defines a margin for sample $i$ as $M_i = \\frac{z_{y_i}(\\mathbf{x}_i) - \\max_{k \\neq y_i} z_k(\\mathbf{x}_i)}{\\lVert \\mathbf{W}\\rVert_F}$. The numerator is the functional margin. Let $k^* = \\operatorname{argmax}_{k \\neq y_i} z_k(\\mathbf{x}_i)$. The functional margin is $m_i = (\\mathbf{w}_{y_i}^\\top \\mathbf{x}_i + b_{y_i}) - (\\mathbf{w}_{k^*}^\\top \\mathbf{x}_i + b_{k^*})$. Without a penalty on $\\mathbf{b}$, the optimizer is free to make the difference $|b_{y_i} - b_{k^*}|$ large if it helps increase the margin and reduce the loss. When we penalize $\\mathbf{b}$, we add a cost for large bias values, shrinking them toward zero. This constrains the optimizer. It may no longer be optimal to use large biases to help separate the classes, and the resulting functional margin $m_i$ might be smaller. Since the statement considers a fixed $\\lVert \\mathbf{W}\\rVert_F$, a smaller functional margin directly implies a smaller $M_i$. Thus, penalizing $\\mathbf{b}$ can indeed lead to a reduction in the margin.\n\nBoth parts of the statement are logically sound.\n\n**Verdict: Correct.**\n\n**D. It is always preferable to decay $\\mathbf{b}$ exactly as strongly as $\\mathbf{W}$ because bias parameters overfit as easily as weights; excluding biases from weight decay cannot be justified.**\n\nThis statement makes two strong, universal claims that are factually incorrect in the context of machine learning.\n1.  \"*bias parameters overfit as easily as weights*\": In a typical model, the number of weight parameters is $K \\times d$, while the number of bias parameters is only $K$. If the input dimension $d$ is large (as is common), then $K \\times d \\gg K$. The complexity and capacity of the model are dominated by the weights $\\mathbf{W}$, not the biases $\\mathbf{b}$. Therefore, the risk of overfitting is substantially higher for the weights than for the biases. The claim is false.\n2.  \"*excluding biases from weight decay cannot be justified*\": This is also false. As established in the analysis of option A, there is a very strong justification for excluding biases from decay: it allows the model to correctly learn the class base rates from the data, which is crucial for probability calibration, especially in cases of class imbalance. In practice, it is a common and often recommended strategy to not regularize bias terms.\n\nThe statement is overly strong and based on faulty reasoning.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AC}$$", "id": "3141394"}]}