{"hands_on_practices": [{"introduction": "The choice between Mean Squared Error ($MSE$) and Mean Absolute Error ($MAE$) is fundamental in regression, hinging on how they treat residuals. This first practice explores their core behavioral differences by examining how they respond to simple scaling of the target variable [@problem_id:3168883]. By training a model with an $MSE$ loss on scaled data but evaluating with $MAE$ on the original data, you will gain direct, hands-on insight into how their respective gradient properties influence the training process and a model's final performance.", "problem": "You are given a one-dimensional regression model with a single input feature and an affine predictor defined by $\\hat{y}_i = w x_i + b$ for sample index $i \\in \\{1,\\dots,n\\}$. Training is performed on labels scaled by a multiplicative constant $\\alpha \\in \\mathbb{R}$, namely $y_i' = \\alpha y_i$. The training loss is Mean Squared Error (MSE) defined as the average of squared residuals, and evaluation is performed using Mean Absolute Error (MAE) defined as the average of absolute residuals with respect to the original (unscaled) labels. Specifically, use the following core definitions as the fundamental base:\n\n- Mean Squared Error (MSE): $L_{\\text{MSE}}(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\hat{y}_i - y_i'\\right)^2$.\n- Mean Absolute Error (MAE): $L_{\\text{MAE}}(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left|\\hat{y}_i - y_i\\right|$.\n- Chain rule for differentiation, linearity of expectation over sums, and the standard gradient descent update $[w,b] \\leftarrow [w,b] - \\eta \\nabla L$ with learning rate $\\eta$.\n\nFor the purposes of derivative computations involving $L_{\\text{MAE}}$, adopt the subgradient definition of the sign function: $\\operatorname{sign}(z) = 1$ if $z > 0$, $\\operatorname{sign}(z) = -1$ if $z  0$, and $\\operatorname{sign}(0) = 0$.\n\nYour tasks are as follows, relying only on the fundamental definitions above and the chain rule:\n\n1. Derive, from first principles, the parameter-space gradients $\\nabla L_{\\text{MSE}}(w,b)$ and a valid subgradient $\\partial L_{\\text{MAE}}(w,b)$ with respect to the parameters $w$ and $b$. Using these, compute the magnitude of each gradient vector (use the Euclidean norm) at the initialization $(w_0,b_0)$ for each $\\alpha$ in the test suite below, where the MAE subgradient should be taken with respect to the scaled labels $y_i'$ to isolate the effect of label scaling on gradient magnitudes for the two losses. Explicitly, for each $\\alpha$, compute\n$$\n\\left\\|\\nabla L_{\\text{MSE}}(w_0,b_0; \\alpha y)\\right\\|_2\n\\quad\\text{and}\\quad\n\\left\\|\\partial L_{\\text{MAE}}(w_0,b_0; \\alpha y)\\right\\|_2.\n$$\n\n2. For each $\\alpha$ in the test suite, perform gradient descent training using $L_{\\text{MSE}}(w,b)$ with labels $y_i' = \\alpha y_i$ and learning rate $\\eta$, starting from $(w_0,b_0)$. After each update, evaluate the MAE on the original labels $y_i$ (not scaled). Stop when the evaluated MAE is less than or equal to the threshold $\\tau$, or when the number of iterations reaches $\\text{max\\_iters}$. Record the number of iterations performed and the final evaluated MAE. Use the subgradient convention above wherever needed for absolute values.\n\nUse the following scientifically sound dataset and parameters:\n\n- Number of samples: $n = 6$.\n- Features: $x = [-2,-1,0,1,2,3]$.\n- Labels: $y = [ -3.4,-1.7,0.5,2.7,4.4,6.8 ]$, which follow $y_i = 2 x_i + 0.5 + \\varepsilon_i$ with deterministic noise $\\varepsilon = [0.1,-0.2,0.0,0.2,-0.1,0.3]$.\n- Initialization: $(w_0,b_0) = (0.0,0.0)$.\n- Learning rate: $\\eta = 0.01$.\n- Evaluation threshold: $\\tau = 0.25$.\n- Maximum iterations: $\\text{max\\_iters} = 3000$.\n\nTest suite (covering a general case, boundary conditions, and edge cases): $\\alpha \\in [1.0, 0.0, -2.0, 10.0]$.\n\nYour program must:\n\n- For each $\\alpha$ in the test suite, compute:\n    1. The gradient magnitude at initialization for MSE on $y' = \\alpha y$.\n    2. The gradient magnitude at initialization for MAE on $y' = \\alpha y$ (using the subgradient sign convention).\n    3. The number of gradient descent iterations required to reach $L_{\\text{MAE}}(w,b) \\le \\tau$ when training with $L_{\\text{MSE}}(w,b)$ on $y' = \\alpha y$, or $\\text{max\\_iters}$ if the threshold is not reached.\n    4. The final evaluated $L_{\\text{MAE}}(w,b)$ after stopping.\n\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each inner list formatted likewise and containing no spaces. Each inner list must be ordered as $[\\alpha,\\text{grad\\_mse\\_norm\\_init},\\text{grad\\_mae\\_norm\\_init},\\text{iterations},\\text{final\\_mae}]$. For example: $[[\\alpha_1,g_1,h_1,i_1,m_1],[\\alpha_2,g_2,h_2,i_2,m_2],\\dots]$. All numeric entries must be printed as raw decimals or integers (no units, no percentage signs).", "solution": "The problem requires a two-part analysis of a simple linear regression model. First, we must derive the gradients of the Mean Squared Error (MSE) loss function and subgradients of the Mean Absolute Error (MAE) loss function from first principles. Second, we must simulate a training process using gradient descent on the MSE loss and evaluate performance using MAE under different label scaling conditions.\n\n### Part 1: Gradient and Subgradient Derivation\n\nWe are given an affine predictor $\\hat{y}_i = w x_i + b$. The training is performed on scaled labels $y_i' = \\alpha y_i$.\n\n**Gradient of Mean Squared Error (MSE) Loss**\n\nThe MSE loss function is defined as:\n$$\nL_{\\text{MSE}}(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i')^2 = \\frac{1}{n} \\sum_{i=1}^{n} (w x_i + b - y_i')^2\n$$\nTo find the gradient $\\nabla L_{\\text{MSE}}(w,b)$, we compute the partial derivatives with respect to the parameters $w$ and $b$.\n\nThe partial derivative with respect to $w$ is found using the linearity of the derivative and the chain rule:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (w x_i + b - y_i')^2 \\right] = \\frac{1}{n} \\sum_{i=1}^{n} 2(w x_i + b - y_i') \\cdot \\frac{\\partial}{\\partial w}(w x_i + b)\n$$\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i') x_i\n$$\n\nSimilarly, the partial derivative with respect to $b$ is:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (w x_i + b - y_i')^2 \\right] = \\frac{1}{n} \\sum_{i=1}^{n} 2(w x_i + b - y_i') \\cdot \\frac{\\partial}{\\partial b}(w x_i + b)\n$$\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i')\n$$\nThus, the gradient vector is:\n$$\n\\nabla L_{\\text{MSE}}(w,b) = \\begin{bmatrix} \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i') x_i \\\\ \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i') \\end{bmatrix}\n$$\n\n**Subgradient of Mean Absolute Error (MAE) Loss**\n\nThe problem asks us to compute the magnitude of the MAE subgradient with respect to the scaled labels $y_i'$. The MAE loss is:\n$$\nL_{\\text{MAE}}(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i'| = \\frac{1}{n} \\sum_{i=1}^{n} |w x_i + b - y_i'|\n$$\nThe derivative of the absolute value function $|z|$ is not defined at $z=0$. We use the subgradient, which is the set of valid slopes. For $|z|$, the subgradient $\\partial|z|$ is $\\operatorname{sign}(z)$ if $z \\neq 0$ and the interval $[-1, 1]$ if $z=0$. The problem specifies using the convention $\\operatorname{sign}(0) = 0$, which selects a particular element from the subgradient set at $z=0$.\n\nUsing the chain rule for subgradients, a valid subgradient of $L_{\\text{MAE}}$ with respect to its parameters is:\n$$\n\\partial L_{\\text{MAE}}(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(w x_i + b - y_i') \\cdot \\nabla(w x_i + b)\n$$\nThe partial subderivative with respect to $w$ is:\n$$\n\\frac{\\partial L_{\\text{MAE}}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(\\hat{y}_i - y_i') \\cdot x_i\n$$\nThe partial subderivative with respect to $b$ is:\n$$\n\\frac{\\partial L_{\\text{MAE}}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(\\hat{y}_i - y_i')\n$$\nThus, a subgradient vector is:\n$$\n\\partial L_{\\text{MAE}}(w,b) = \\begin{bmatrix} \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(\\hat{y}_i - y_i') x_i \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(\\hat{y}_i - y_i') \\end{bmatrix}\n$$\n\n**Initial Gradient Magnitudes**\n\nAt initialization, $(w_0, b_0) = (0.0, 0.0)$. The predictions are $\\hat{y}_i = 0 \\cdot x_i + 0 = 0$. The residuals used for calculating gradients are $\\hat{y}_i - y_i' = -\\alpha y_i$.\n\nFor MSE, the gradient components at initialization are:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial w}\\bigg|_{(w_0,b_0)} = \\frac{2}{n} \\sum_{i=1}^{n} (-\\alpha y_i) x_i = -\\frac{2\\alpha}{n} \\sum_{i=1}^{n} y_i x_i\n$$\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial b}\\bigg|_{(w_0,b_0)} = \\frac{2}{n} \\sum_{i=1}^{n} (-\\alpha y_i) = -\\frac{2\\alpha}{n} \\sum_{i=1}^{n} y_i\n$$\nThe Euclidean norm of the gradient is:\n$$\n\\|\\nabla L_{\\text{MSE}}(w_0,b_0)\\|_2 = \\sqrt{\\left(-\\frac{2\\alpha}{n} \\sum y_i x_i\\right)^2 + \\left(-\\frac{2\\alpha}{n} \\sum y_i\\right)^2} = \\frac{2|\\alpha|}{n} \\sqrt{\\left(\\sum y_i x_i\\right)^2 + \\left(\\sum y_i\\right)^2}\n$$\nThis shows that the MSE gradient magnitude at initialization is directly proportional to the absolute value of the scaling factor, $|\\alpha|$.\n\nFor MAE, the subgradient components at initialization are:\n$$\n\\frac{\\partial L_{\\text{MAE}}}{\\partial w}\\bigg|_{(w_0,b_0)} = \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(-\\alpha y_i) x_i = -\\frac{\\operatorname{sign}(\\alpha)}{n} \\sum_{i=1}^{n} \\operatorname{sign}(y_i) x_i\n$$\n$$\n\\frac{\\partial L_{\\text{MAE}}}{\\partial b}\\bigg|_{(w_0,b_0)} = \\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{sign}(-\\alpha y_i) = -\\frac{\\operatorname{sign}(\\alpha)}{n} \\sum_{i=1}^{n} \\operatorname{sign}(y_i)\n$$\n(assuming no $y_i=0$ and $\\alpha \\neq 0$). The norm is:\n$$\n\\|\\partial L_{\\text{MAE}}(w_0,b_0)\\|_2 = \\frac{|\\operatorname{sign}(\\alpha)|}{n} \\sqrt{\\left(\\sum \\operatorname{sign}(y_i) x_i\\right)^2 + \\left(\\sum \\operatorname{sign}(y_i)\\right)^2}\n$$\nFor $\\alpha \\neq 0$, $|\\operatorname{sign}(\\alpha)|=1$, so the magnitude is constant regardless of the value of $\\alpha$. For $\\alpha=0$, the gradient is the zero vector, and its magnitude is $0$. This insensitivity of the MAE gradient magnitude to the scale of the errors (as long as the sign is preserved) is a key difference from MSE.\n\n### Part 2: Gradient Descent Simulation Logic\n\nThe simulation will perform gradient descent for each value of $\\alpha$ in the test suite. The core algorithm is as follows:\n1. Initialize parameters $(w,b) = (0.0, 0.0)$.\n2. For each iteration up to a maximum of $\\text{max\\_iters}$:\n   a. Compute the predictions $\\hat{y} = w x + b$.\n   b. Compute the gradient of the MSE loss, $\\nabla L_{\\text{MSE}}$, using the scaled labels $y' = \\alpha y$.\n   c. Update the parameters using the gradient descent rule: $[w,b] \\leftarrow [w,b] - \\eta \\nabla L_{\\text{MSE}}$.\n   d. Evaluate the MAE, $L_{\\text{MAE}} = \\frac{1}{n} \\sum |\\hat{y}_i - y_i|$, using the original (unscaled) labels $y$.\n   e. If $L_{\\text{MAE}} \\le \\tau$, terminate the process for the current $\\alpha$.\n3. Record the number of iterations and the final MAE.\n\nWe expect the following behaviors:\n- For $\\alpha=1.0$, the training objective aligns with the evaluation metric (up to the difference between MSE and MAE minima), so we anticipate convergence to a low MAE.\n- For $\\alpha=0.0$, the scaled labels are all zero. The initial gradient is zero, so no learning occurs. The process will run for $\\text{max\\_iters}$ iterations.\n- For $\\alpha=-2.0$ and $\\alpha=10.0$, the model is trained to fit a target that is incorrectly scaled relative to the true data. The learned parameters $(w,b)$ will be optimal for the scaled problem but will produce large errors when evaluated against the original labels. Thus, the MAE is not expected to fall below the threshold $\\tau$, and the simulation will run for $\\text{max\\_iters}$.\n\nThe following program implements this logic to compute the required metrics for each $\\alpha$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression problem by deriving gradients, running simulations,\n    and reporting the results as specified.\n    \"\"\"\n    # Dataset and parameters\n    x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    y = np.array([-3.4, -1.7, 0.5, 2.7, 4.4, 6.8])\n    n = len(x)\n    w0, b0 = 0.0, 0.0\n    eta = 0.01\n    tau = 0.25\n    max_iters = 3000\n    \n    test_suite_alpha = [1.0, 0.0, -2.0, 10.0]\n    \n    results = []\n\n    for alpha in test_suite_alpha:\n        # --- Task 1: Compute initial gradient magnitudes ---\n        \n        y_prime = alpha * y\n        \n        # Initial predictions\n        y_hat_init = w0 * x + b0\n        \n        # Residuals for gradient calculation\n        residuals_init = y_hat_init - y_prime\n        \n        # MSE gradient at initialization\n        grad_mse_w_init = (2 / n) * np.sum(residuals_init * x)\n        grad_mse_b_init = (2 / n) * np.sum(residuals_init)\n        grad_mse_norm_init = np.sqrt(grad_mse_w_init**2 + grad_mse_b_init**2)\n        \n        # MAE subgradient at initialization (with sign(0)=0 convention)\n        signs = np.sign(residuals_init)\n        grad_mae_w_init = (1 / n) * np.sum(signs * x)\n        grad_mae_b_init = (1 / n) * np.sum(signs)\n        grad_mae_norm_init = np.sqrt(grad_mae_w_init**2 + grad_mae_b_init**2)\n\n        # --- Task 2: Gradient Descent Training ---\n        \n        w, b = w0, b0\n        iterations_performed = 0\n        \n        for i in range(1, max_iters + 1):\n            iterations_performed = i\n            \n            # Predictions\n            y_hat = w * x + b\n            \n            # MSE Gradients using scaled labels y_prime\n            residuals_mse = y_hat - y_prime\n            grad_w = (2 / n) * np.sum(residuals_mse * x)\n            grad_b = (2 / n) * np.sum(residuals_mse)\n            \n            # Update parameters\n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n            # Evaluate MAE on original labels y\n            current_mae = np.mean(np.abs((w * x + b) - y))\n            \n            # Check stopping condition\n            if current_mae = tau:\n                break\n        \n        # Final evaluated MAE on original labels\n        final_mae = np.mean(np.abs((w * x + b) - y))\n        \n        results.append([\n            alpha,\n            grad_mse_norm_init,\n            grad_mae_norm_init,\n            iterations_performed,\n            final_mae\n        ])\n    \n    # Format output as specified\n    output_str = \"[\" + \",\".join(\n        f\"[{','.join(map(str, row))}]\" for row in results\n    ) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3168883"}, {"introduction": "Building on the insights from the previous exercise, we now explore a practical consequence of $MSE$'s sensitivity to large errors: its vulnerability to outliers. This practice asks you to design a controlled experiment [@problem_id:3168805] to quantify how outliers can degrade the performance of an $MSE$-trained model when the goal is to achieve a low $MAE$. You will then implement and train a model with a robust, differentiable surrogate for $MAE$, the log-cosh loss, demonstrating a powerful technique for aligning the training objective with the evaluation metric in the presence of noisy data.", "problem": "You are given a supervised regression setting under the empirical risk minimization framework. Let a dataset be $D = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ with $\\mathbf{x}_i \\in \\mathbb{R}^d$. For a linear model with parameters $(\\mathbf{w}, b)$ and predictions $\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$, define the residuals $r_i = y_i - \\hat{y}_i$. The task is to design and implement a controlled experiment that quantifies the mismatch between the optimization metric and the evaluation metric when training with Mean Squared Error (MSE) but evaluating with Mean Absolute Error (MAE), and to compare this against training with a differentiable surrogate of MAE based on the logarithm of the hyperbolic cosine.\n\nFundamental base and definitions to use:\n- Empirical Risk Minimization (ERM) minimizes the empirical average of a nonnegative loss on residuals $r_i$.\n- Mean Squared Error (MSE) is the empirical risk given by the average of $r_i^2$ over the training set.\n- Mean Absolute Error (MAE) is the empirical risk given by the average of $|r_i|$ over a dataset.\n- The proposed differentiable surrogate of MAE is the logarithm of the hyperbolic cosine, namely the function $r \\mapsto \\log(\\cosh(r / \\tau))$ with a positive scale parameter $\\tau$ that controls the transition between quadratic and linear behavior.\n\nExperiment design and required steps:\n1. Data generation from first principles:\n   - Fix a true linear data-generating process in $d=3$ dimensions with a deterministic parameter vector $\\mathbf{w}^\\star = [2.0, -3.0, 0.5]^\\top$ and intercept $b^\\star = 0.7$. For each sample, draw $\\mathbf{x}_i$ independently with independent standard normal coordinates. Compute clean targets $y_i^{\\text{clean}} = (\\mathbf{w}^\\star)^\\top \\mathbf{x}_i + b^\\star$.\n   - Add independent noise $\\epsilon_i$ to each clean target to produce $y_i = y_i^{\\text{clean}} + \\epsilon_i$. The noise is drawn from a mixture that models outliers: with probability $1 - p_{\\text{out}}$, draw $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$; with probability $p_{\\text{out}}$, draw $\\epsilon_i \\sim \\mathcal{N}(0, (\\sigma \\cdot s_{\\text{out}})^2)$. Here $p_{\\text{out}} \\in [0, 1)$ is the outlier rate, $\\sigma > 0$ is the base noise scale, and $s_{\\text{out}} \\ge 1$ is the outlier scale factor.\n   - Split independently generated data into a training set of size $n_{\\text{train}}$ and a test set of size $n_{\\text{test}}$, each generated from the same process with the same parameters.\n2. Training objectives to implement and minimize by gradient descent:\n   - Train a linear model $(\\mathbf{w}, b)$ to minimize the empirical MSE on the training set by gradient descent from zero initialization using a fixed learning rate and a fixed number of full-batch epochs. Use only the ERM principle and calculus to derive the necessary gradients for parameter updates.\n   - Train another linear model $(\\tilde{\\mathbf{w}}, \\tilde{b})$ to minimize the empirical average of $\\log(\\cosh(r / \\tau))$ on the training set by gradient descent with the same initialization and the same optimization hyperparameters, using a given positive $\\tau$. Again, use only the ERM principle and calculus to derive the necessary gradients for parameter updates.\n3. Evaluation metric and mismatch quantification:\n   - For each trained model, compute the MAE on the test set, that is, the empirical average of $|r_i|$ on the test set. Denote these as $\\text{MAE}_{\\text{MSE}}$ and $\\text{MAE}_{\\log\\cosh}$ for the model trained with Mean Squared Error and for the model trained with the $\\log(\\cosh(\\cdot))$ surrogate, respectively.\n   - Quantify the optimization–evaluation mismatch in two ways: the absolute gap $\\Delta = \\text{MAE}_{\\text{MSE}} - \\text{MAE}_{\\log\\cosh}$ and the ratio $\\rho = \\text{MAE}_{\\text{MSE}} / \\text{MAE}_{\\log\\cosh}$. If $\\text{MAE}_{\\log\\cosh} = 0$, define $\\rho = 1.0$ to avoid division by zero.\n4. Implementation constraints:\n   - Implement the entire experiment as a deterministic program using pseudorandom number generators seeded as specified below. Use full-batch gradient descent, not stochastic methods. Use only basic linear algebra and calculus implied by ERM; do not rely on external optimization libraries.\n   - Output for each test case a list of $4$ floats $[\\text{MAE}_{\\text{MSE}}, \\text{MAE}_{\\log\\cosh}, \\Delta, \\rho]$, each rounded to $6$ decimal places.\n\nTest suite:\nRun your program on the following parameter sets, each with independently generated training and test data as described:\n- Case A (happy path, no outliers): random seed $0$, $n_{\\text{train}} = 512$, $n_{\\text{test}} = 2048$, base noise $\\sigma = 0.5$, outlier rate $p_{\\text{out}} = 0.0$, outlier scale $s_{\\text{out}} = 10.0$, learning rate $\\eta = 0.05$, epochs $300$, surrogate scale $\\tau = 1.0$.\n- Case B (moderate outliers): random seed $1$, $n_{\\text{train}} = 512$, $n_{\\text{test}} = 2048$, base noise $\\sigma = 0.5$, outlier rate $p_{\\text{out}} = 0.2$, outlier scale $s_{\\text{out}} = 8.0$, learning rate $\\eta = 0.05$, epochs $300$, surrogate scale $\\tau = 1.0$.\n- Case C (heavier outliers, different scale): random seed $2$, $n_{\\text{train}} = 256$, $n_{\\text{test}} = 4096$, base noise $\\sigma = 0.2$, outlier rate $p_{\\text{out}} = 0.3$, outlier scale $s_{\\text{out}} = 12.0$, learning rate $\\eta = 0.05$, epochs $400$, surrogate scale $\\tau = 0.5$.\n- Case D (small-sample edge case): random seed $3$, $n_{\\text{train}} = 16$, $n_{\\text{test}} = 4096$, base noise $\\sigma = 0.5$, outlier rate $p_{\\text{out}} = 0.25$, outlier scale $s_{\\text{out}} = 10.0$, learning rate $\\eta = 0.03$, epochs $600$, surrogate scale $\\tau = 1.0$.\n\nFinal output format:\nYour program should produce a single line containing a list of the per-case results, where each case contributes a list of four rounded floats in the order $[\\text{MAE}_{\\text{MSE}}, \\text{MAE}_{\\log\\cosh}, \\Delta, \\rho]$. The top-level list must concatenate the cases in the order A, B, C, D and use commas with no spaces. For example, a valid shape and formatting would be\n$[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],[c_1,c_2,c_3,c_4],[d_1,d_2,d_3,d_4]]$\nwith each $a_j, b_j, c_j, d_j$ a float rounded to $6$ decimal places.", "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and objective. It outlines a detailed and formalizable numerical experiment in the domain of supervised machine learning, with all necessary parameters and conditions specified. The problem is a substantive task requiring the implementation of data generation, gradient-based optimization, and evaluation based on first principles of empirical risk minimization.\n\nThe problem requires a controlled experiment to investigate the mismatch between an optimization metric (Mean Squared Error, MSE) and an evaluation metric (Mean Absolute Error, MAE), particularly in the presence of outliers. This is contrasted with using a differentiable surrogate for MAE, the logarithm of the hyperbolic cosine ($\\log(\\cosh)$), as the optimization metric. The core principle being tested is that aligning the optimization objective more closely with the evaluation metric can lead to better performance, especially when the data's noise characteristics violate the implicit assumptions of the standard objective.\n\n### 1. Experimental Design and Data Generation\n\nThe experiment is based on a synthetic dataset generated from a known linear process, which allows for a controlled analysis. The ground truth model is defined by parameters $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ and $b^\\star \\in \\mathbb{R}$ with dimension $d=3$. Data points $(\\mathbf{x}_i, y_i)$ are generated as follows:\n- The features $\\mathbf{x}_i \\in \\mathbb{R}^3$ are drawn from a standard normal distribution, i.e., each component $x_{ij} \\sim \\mathcal{N}(0, 1)$.\n- The true target values are computed as $y_i^{\\text{clean}} = (\\mathbf{w}^\\star)^\\top \\mathbf{x}_i + b^\\star$.\n- The observed targets $y_i$ are created by adding noise $\\epsilon_i$ to the clean targets: $y_i = y_i^{\\text{clean}} + \\epsilon_i$.\n- The noise $\\epsilon_i$ is drawn from a mixture model to simulate outliers. With probability $1 - p_{\\text{out}}$, the noise is from a baseline normal distribution $\\mathcal{N}(0, \\sigma^2)$. With probability $p_{\\text{out}}$, the noise is from a high-variance normal distribution $\\mathcal{N}(0, (\\sigma \\cdot s_{\\text{out}})^2)$, representing outliers. The parameters $p_{\\text{out}}$, $\\sigma$, and $s_{\\text{out}}$ control the rate, base scale, and magnitude of these outliers, respectively.\n\nThis process is used to generate an independent training set of size $n_{\\text{train}}$ and a test set of size $n_{\\text{test}}$.\n\n### 2. Optimization via Empirical Risk Minimization\n\nA linear model with parameters $(\\mathbf{w}, b)$ provides predictions $\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$. The goal is to find the optimal parameters by minimizing the empirical average of a chosen loss function over the training set. This is done using full-batch gradient descent, where parameters are updated iteratively based on the gradient of the total loss.\n\n#### Scenario 1: Training with Mean Squared Error (MSE)\nThe MSE loss is defined by the squared residual, $L_{\\text{MSE}}(r_i) = r_i^2 = (y_i - \\hat{y}_i)^2$. The total empirical risk is:\n$$J_{\\text{MSE}}(\\mathbf{w}, b) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b))^2$$\nTo minimize this using gradient descent, we need its partial derivatives with respect to the parameters $\\mathbf{w}$ and $b$. Let $\\mathbf{r} = \\mathbf{y} - (\\mathbf{X}\\mathbf{w} + b)$ be the vector of residuals.\nThe gradients are:\n$$ \\nabla_{\\mathbf{w}} J_{\\text{MSE}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} 2(y_i - \\hat{y}_i)(-\\mathbf{x}_i) = -\\frac{2}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}} r_i \\mathbf{x}_i = -\\frac{2}{n_{\\text{train}}} \\mathbf{X}^\\top \\mathbf{r} $$\n$$ \\nabla_{b} J_{\\text{MSE}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} 2(y_i - \\hat{y}_i)(-1) = -\\frac{2}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}} r_i $$\nThe squared term in the MSE loss function makes it highly sensitive to large residuals, meaning outliers can disproportionately influence the resulting model parameters.\n\n#### Scenario 2: Training with the Log-Cosh Surrogate\nThe log-cosh loss is a smooth approximation of the MAE loss, defined as $L_{\\log\\cosh}(r_i) = \\log(\\cosh(r_i / \\tau))$, where $\\tau > 0$ is a scale parameter. The empirical risk is:\n$$J_{\\log\\cosh}(\\tilde{\\mathbf{w}}, \\tilde{b}) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\log\\left(\\cosh\\left(\\frac{y_i - (\\tilde{\\mathbf{w}}^\\top \\mathbf{x}_i + \\tilde{b})}{\\tau}\\right)\\right)$$\nThis function behaves quadratically (like MSE) for small residuals ($|r_i| \\ll \\tau$) and linearly (like MAE) for large residuals ($|r_i| \\gg \\tau$), making it more robust to outliers. The gradients are derived using the chain rule and the identity $\\frac{d}{dz}\\log(\\cosh(z)) = \\tanh(z)$:\n$$ \\nabla_{\\tilde{\\mathbf{w}}} J_{\\log\\cosh} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\tanh\\left(\\frac{r_i}{\\tau}\\right) \\frac{1}{\\tau} (-\\mathbf{x}_i) = -\\frac{1}{n_{\\text{train}}\\tau} \\sum_{i=1}^{n_{\\text{train}}} \\tanh\\left(\\frac{r_i}{\\tau}\\right) \\mathbf{x}_i $$\n$$ \\nabla_{\\tilde{b}} J_{\\log\\cosh} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\tanh\\left(\\frac{r_i}{\\tau}\\right) \\frac{1}{\\tau} (-1) = -\\frac{1}{n_{\\text{train}}\\tau} \\sum_{i=1}^{n_{\\text{train}}} \\tanh\\left(\\frac{r_i}{\\tau}\\right) $$\nThe use of the hyperbolic tangent function, $\\tanh(\\cdot)$, \"squashes\" the influence of large residuals, providing the desired robustness.\n\nFor both scenarios, the parameters are initialized to zero and updated for a fixed number of epochs using the gradient descent rule with a learning rate $\\eta$: $\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} J$, where $\\boldsymbol{\\theta}$ represents the pair $(\\mathbf{w}, b)$ or $(\\tilde{\\mathbf{w}}, \\tilde{b})$.\n\n### 3. Evaluation and Mismatch Quantification\n\nAfter training, both models are evaluated on the independently generated test set. The performance metric is the Mean Absolute Error (MAE), which is the ultimate metric of interest.\n$$ \\text{MAE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} |y_i - \\hat{y}_i| $$\nWe compute $\\text{MAE}_{\\text{MSE}}$ for the model trained with MSE and $\\text{MAE}_{\\log\\cosh}$ for the model trained with the log-cosh surrogate. The mismatch between the optimization and evaluation objectives is then quantified by two metrics:\n1.  Absolute Gap: $\\Delta = \\text{MAE}_{\\text{MSE}} - \\text{MAE}_{\\log\\cosh}$\n2.  Ratio: $\\rho = \\text{MAE}_{\\text{MSE}} / \\text{MAE}_{\\log\\cosh}$\n\nA positive $\\Delta$ and a ratio $\\rho > 1$ indicate that training with the robust log-cosh surrogate yields a model that performs better on the MAE evaluation metric, thereby mitigating the optimization-evaluation mismatch. This effect is expected to be more pronounced in the presence of outliers (i.e., for $p_{\\text{out}} > 0$).\n\nThe experiment will be implemented programmatically, adhering to the specified parameters for each test case, ensuring deterministic and reproducible results through the use of seeded pseudorandom number generators.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the series of experiments and print the final results.\n    \"\"\"\n    # Define the true parameters for data generation\n    w_star = np.array([2.0, -3.0, 0.5])\n    b_star = 0.7\n    d = 3\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path, no outliers\n        {'seed': 0, 'n_train': 512, 'n_test': 2048, 'sigma': 0.5, 'p_out': 0.0, 's_out': 10.0, 'eta': 0.05, 'epochs': 300, 'tau': 1.0},\n        # Case B: moderate outliers\n        {'seed': 1, 'n_train': 512, 'n_test': 2048, 'sigma': 0.5, 'p_out': 0.2, 's_out': 8.0, 'eta': 0.05, 'epochs': 300, 'tau': 1.0},\n        # Case C: heavier outliers, different scale\n        {'seed': 2, 'n_train': 256, 'n_test': 4096, 'sigma': 0.2, 'p_out': 0.3, 's_out': 12.0, 'eta': 0.05, 'epochs': 400, 'tau': 0.5},\n        # Case D: small-sample edge case\n        {'seed': 3, 'n_train': 16, 'n_test': 4096, 'sigma': 0.5, 'p_out': 0.25, 's_out': 10.0, 'eta': 0.03, 'epochs': 600, 'tau': 1.0},\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = run_experiment(w_star, b_star, d, **params)\n        all_results.append(result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[val1,val2,val3,val4],[val1,val2,val3,val4],...]\n    results_str = ','.join([f\"[{','.join(map(str, [round(v, 6) for v in res]))}]\" for res in all_results])\n    print(f\"[{results_str}]\")\n\ndef generate_data(n_samples, d, w_star, b_star, sigma, p_out, s_out, rng):\n    \"\"\"\n    Generates synthetic data from a linear model with mixed Gaussian noise.\n    \"\"\"\n    X = rng.standard_normal(size=(n_samples, d))\n    y_clean = X @ w_star + b_star\n\n    # Generate noise from the mixture model\n    is_outlier = rng.uniform(size=n_samples)  p_out\n    noise_std = np.where(is_outlier, sigma * s_out, sigma)\n    noise = rng.normal(loc=0.0, scale=noise_std)\n    \n    y = y_clean + noise\n    return X, y\n\ndef train(X_train, y_train, loss_type, eta, epochs, tau=None):\n    \"\"\"\n    Trains a linear model using full-batch gradient descent.\n    \"\"\"\n    n_train, d = X_train.shape\n    w = np.zeros(d)\n    b = 0.0\n\n    for _ in range(epochs):\n        predictions = X_train @ w + b\n        residuals = y_train - predictions\n\n        if loss_type == 'mse':\n            grad_w = (-2 / n_train) * (X_train.T @ residuals)\n            grad_b = (-2 / n_train) * np.sum(residuals)\n        elif loss_type == 'logcosh':\n            if tau is None:\n                raise ValueError(\"tau must be provided for logcosh loss\")\n            tanh_term = np.tanh(residuals / tau)\n            grad_w = (-1 / (n_train * tau)) * (X_train.T @ tanh_term)\n            grad_b = (-1 / (n_train * tau)) * np.sum(tanh_term)\n        else:\n            raise ValueError(f\"Unknown loss type: {loss_type}\")\n\n        w -= eta * grad_w\n        b -= eta * grad_b\n    \n    return w, b\n\ndef evaluate_mae(X_test, y_test, w, b):\n    \"\"\"\n    Evaluates the Mean Absolute Error of a model on the test set.\n    \"\"\"\n    y_pred = X_test @ w + b\n    mae = np.mean(np.abs(y_test - y_pred))\n    return mae\n\ndef run_experiment(w_star, b_star, d, seed, n_train, n_test, sigma, p_out, s_out, eta, epochs, tau):\n    \"\"\"\n    Runs a single experimental case: data generation, training, and evaluation.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate training and test data\n    X_train, y_train = generate_data(n_train, d, w_star, b_star, sigma, p_out, s_out, rng)\n    X_test, y_test = generate_data(n_test, d, w_star, b_star, sigma, p_out, s_out, rng)\n\n    # Train model 1: Minimize MSE\n    w_mse, b_mse = train(X_train, y_train, loss_type='mse', eta=eta, epochs=epochs)\n    \n    # Train model 2: Minimize LogCosh loss\n    w_logcosh, b_logcosh = train(X_train, y_train, loss_type='logcosh', eta=eta, epochs=epochs, tau=tau)\n\n    # Evaluate both models on the test set using MAE\n    mae_mse = evaluate_mae(X_test, y_test, w_mse, b_mse)\n    mae_logcosh = evaluate_mae(X_test, y_test, w_logcosh, b_logcosh)\n\n    # Quantify the mismatch\n    delta = mae_mse - mae_logcosh\n    \n    if mae_logcosh == 0:\n        rho = 1.0\n    else:\n        rho = mae_mse / mae_logcosh\n        \n    return [mae_mse, mae_logcosh, delta, rho]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3168805"}, {"introduction": "Many real-world applications require more than a single point prediction; they demand an understanding of the forecast's uncertainty. This is where quantile regression excels, by predicting a range of possible outcomes to form a prediction interval. This final practice [@problem_id:3168892] moves beyond evaluating point estimates and teaches you how to assess the quality of probabilistic forecasts, requiring you to implement the proper evaluation metric, the asymmetric pinball loss, from its first principles. You will learn to evaluate not only the accuracy of specific quantiles but also the reliability and sharpness of the prediction intervals they form.", "problem": "You are given predicted conditional quantiles at levels $\\tau \\in \\{0.1, 0.5, 0.9\\}$ for multiple observations. Your task is to construct a complete, runnable program that evaluates quantile regression predictions using the asymmetric linear loss principle and interval-based coverage diagnostics.\n\nFundamental base to use:\n- The $\\tau$-quantile is characterized as a minimizer of the expected value of an additive, positively homogeneous, asymmetric linear loss that penalizes positive residuals and negative residuals with different slopes determined by $\\tau$. Specifically, underestimation and overestimation incur different linear costs whose ratio depends on $\\tau$.\n- The empirical risk aggregates per-observation losses additively over a dataset.\n\nFrom this base, you must derive the exact per-sample loss expression for a given residual $u = y - \\hat{q}_\\tau(x)$ and implement it. Do not assume or use any pre-packaged formula; derive it from the stated asymmetry principle and implement the derived expression directly.\n\nDefinitions to implement from first principles:\n- For each quantile level $\\tau$, the per-sample loss is an asymmetric linear function of $u$ with a slope that depends on the sign of $u$ and the parameter $\\tau$.\n- The mean loss for a given $\\tau$ is the arithmetic mean of the per-sample losses over all samples.\n- To study asymmetric costs, decompose the total loss at a given $\\tau$ into contributions from positive residuals and from negative residuals, and compute the share contributed by positive residuals. If the total loss equals zero, define this share to be $0.0$.\n- A central prediction interval is defined from the lower quantile at $\\tau = 0.1$ and the upper quantile at $\\tau = 0.9$. If the predicted quantiles are not ordered for a sample, construct the interval by taking the minimum of the two as the lower endpoint and the maximum as the upper endpoint. The empirical coverage rate is the fraction of targets $y$ that lie within the constructed interval (inclusive of the endpoints). The mean interval width is the mean of the absolute difference between the two endpoints. The crossing count is the number of samples for which the originally predicted lower quantile exceeds the originally predicted upper quantile.\n- The aggregate quantile score is defined as the per-sample sum of the losses across $\\tau \\in \\{0.1, 0.5, 0.9\\}$, averaged over all samples.\n\nProgram requirements:\n1. Derive the explicit per-sample asymmetric linear loss as a function of residual $u$ and parameter $\\tau$ from the asymmetry principle, and implement it.\n2. For each test case and each $\\tau \\in \\{0.1, 0.5, 0.9\\}$, compute:\n   - The mean loss.\n   - The share of loss contributed by positive residuals (with the convention $0.0$ if the total loss is $0$).\n3. For each test case, using the predicted $\\tau = 0.1$ and $\\tau = 0.9$ quantiles, compute:\n   - The empirical coverage rate of the central prediction interval.\n   - The mean interval width.\n   - The crossing count (how many samples have the lower quantile strictly greater than the upper quantile before reordering).\n4. For each test case, compute the aggregate quantile score, defined as the mean over samples of the sum of the per-sample losses across $\\tau \\in \\{0.1, 0.5, 0.9\\}$.\n5. Round all floating-point outputs to $6$ decimal places.\n\nTest suite (implement exactly the following three cases; each case provides arrays of targets and predicted quantiles):\n\n- Test case $1$ (happy path, ordered quantiles):\n  - Targets $y$: $[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]$.\n  - Predicted $\\hat{q}_{0.1}$: $[0.5, 1.5, 2.7, 3.0, 4.2, 5.5]$.\n  - Predicted $\\hat{q}_{0.5}$: $[0.9, 1.8, 3.1, 3.9, 4.9, 6.2]$.\n  - Predicted $\\hat{q}_{0.9}$: $[1.6, 2.5, 3.6, 4.8, 5.7, 6.8]$.\n\n- Test case $2$ (boundary, perfect predictions, zero width):\n  - Targets $y$: $[2.5, -1.0, 0.0, 3.3]$.\n  - Predicted $\\hat{q}_{0.1}$: $[2.5, -1.0, 0.0, 3.3]$.\n  - Predicted $\\hat{q}_{0.5}$: $[2.5, -1.0, 0.0, 3.3]$.\n  - Predicted $\\hat{q}_{0.9}$: $[2.5, -1.0, 0.0, 3.3]$.\n\n- Test case $3$ (edge case, quantile crossing):\n  - Targets $y$: $[10.0, 0.0, -2.0, 5.0, 1.0]$.\n  - Predicted $\\hat{q}_{0.1}$: $[12.0, 1.0, -1.0, 7.0, 3.0]$.\n  - Predicted $\\hat{q}_{0.5}$: $[11.0, 0.5, -2.5, 6.0, 1.5]$.\n  - Predicted $\\hat{q}_{0.9}$: $[9.0, -1.0, -3.0, 4.0, 0.0]$.\n\nFinal output format:\n- For each test case, output a list of ten values in the following order:\n  $[\\text{mean\\_loss\\_0.1}, \\text{mean\\_loss\\_0.5}, \\text{mean\\_loss\\_0.9}, \\text{aggregate\\_quantile\\_score}, \\text{coverage\\_rate}, \\text{mean\\_interval\\_width}, \\text{crossing\\_count}, \\text{pos\\_loss\\_share\\_0.1}, \\text{pos\\_loss\\_share\\_0.5}, \\text{pos\\_loss\\_share\\_0.9}]$ where all entries except the crossing count are floating-point numbers rounded to $6$ decimals and the crossing count is an integer.\n- Your program should produce a single line of output containing the results for all three test cases as a comma-separated list enclosed in square brackets, where each element is the list for one test case. For example: $[[\\dots],[\\dots],[\\dots]]$.\n- There are no physical units or angles involved in this problem.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically sound, self-contained, and well-posed, providing a clear and objective task based on established principles of quantile regression evaluation. All necessary data and definitions for a unique solution are provided.\n\nThe core of the problem is the derivation and implementation of the quantile loss function from first principles.\n\n**1. Derivation of the Per-Sample Quantile Loss Function**\n\nThe problem specifies that the $\\tau$-quantile loss is an asymmetric linear function of the residual $u = y - \\hat{q}_\\tau$, where $y$ is the true target value and $\\hat{q}_\\tau$ is the predicted quantile at level $\\tau$. Let this loss function be denoted by $\\rho_\\tau(u)$.\n\nThe asymmetry means the penalty for underestimation ($u > 0$, i.e., $y > \\hat{q}_\\tau$) differs from the penalty for overestimation ($u  0$, i.e., $y  \\hat{q}_\\tau$). The loss is linear, so we can write it as:\n$$\n\\rho_\\tau(u) =\n\\begin{cases}\na_{pos} \\cdot u  \\text{if } u > 0 \\\\\n0  \\text{if } u = 0 \\\\\na_{neg} \\cdot (-u)  \\text{if } u  0\n\\end{cases}\n$$\nwhere $a_{pos}$ and $a_{neg}$ are non-negative slopes.\n\nA fundamental property of a quantile forecast is that it should balance the costs of overestimation and underestimation. The $\\tau$-quantile is the value that is expected to be greater than the observation with probability $\\tau$. The first-order condition for minimizing the expected loss $E[\\rho_\\tau(y - \\hat{q}_\\tau)]$ with respect to the forecast $\\hat{q}_\\tau$ requires that the expected value of the derivative of the loss with respect to $\\hat{q}_\\tau$ is zero. The derivative is:\n$$\n\\frac{\\partial \\rho_\\tau(y - \\hat{q}_\\tau)}{\\partial \\hat{q}_\\tau} =\n\\begin{cases}\n-a_{pos}  \\text{if } y > \\hat{q}_\\tau \\\\\na_{neg}  \\text{if } y  \\hat{q}_\\tau\n\\end{cases}\n$$\nSetting the expectation to zero yields:\n$$ E\\left[\\frac{\\partial \\rho_\\tau(y - \\hat{q}_\\tau)}{\\partial \\hat{q}_\\tau}\\right] = (-a_{pos}) \\cdot P(y > \\hat{q}_\\tau) + (a_{neg}) \\cdot P(y  \\hat{q}_\\tau) = 0 $$\n$$ a_{neg} \\cdot P(y  \\hat{q}_\\tau) = a_{pos} \\cdot P(y > \\hat{q}_\\tau) $$\nFor an ideal forecast where $\\hat{q}_\\tau$ is the true $\\tau$-quantile of the conditional distribution of $y$, we have $P(y  \\hat{q}_\\tau) = \\tau$ and $P(y > \\hat{q}_\\tau) = 1 - \\tau$. Substituting these into the balance condition gives:\n$$ a_{neg} \\cdot \\tau = a_{pos} \\cdot (1 - \\tau) $$\nThis equation determines the ratio of the slopes. A standard and convenient choice that satisfies this relation is to set $a_{pos} = \\tau$ and $a_{neg} = 1 - \\tau$. This leads to the widely used quantile loss (or pinball loss) function.\n\nSubstituting these slopes back into the piecewise definition, and including the $u=0$ case (where loss is $0$) into the $u>0$ case, we arrive at the anaytical expression for the per-sample loss:\n$$\n\\rho_\\tau(u) =\n\\begin{cases}\n\\tau \\cdot u  \\text{if } u \\ge 0 \\\\\n(1-\\tau) \\cdot (-u)  \\text{if } u  0\n\\end{cases}\n$$\nwhere $u = y - \\hat{q}_\\tau$. This formula is derived directly from the asymmetry principle as required and forms the basis for all loss-based calculations.\n\n**2. Definitions of Evaluation Metrics**\n\nGiven a dataset of $N$ observations, indexed by $i=1, \\dots, N$, with targets $y_i$ and quantile predictions $\\hat{q}_{i, \\tau}$, the evaluation metrics are defined as follows.\n\n- **Mean Loss**: The arithmetic mean of the per-sample losses for a given quantile level $\\tau$:\n$$ \\text{MeanLoss}_\\tau = \\frac{1}{N} \\sum_{i=1}^N \\rho_\\tau(y_i - \\hat{q}_{i, \\tau}) $$\n\n- **Share of Loss from Positive Residuals**: The proportion of the total loss that comes from samples where $y_i > \\hat{q}_{i, \\tau}$. Let $u_{i, \\tau} = y_i - \\hat{q}_{i, \\tau}$.\n$$ \\text{PositiveLossShare}_\\tau = \\frac{\\sum_{i=1}^N \\rho_\\tau(u_{i, \\tau}) \\cdot \\mathbb{I}(u_{i, \\tau} > 0)}{\\sum_{i=1}^N \\rho_\\tau(u_{i, \\tau})} $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. If the total loss (denominator) is $0$, the share is defined as $0.0$.\n\n- **Interval-Based Metrics**: Using the quantile predictions for $\\tau=0.1$ and $\\tau=0.9$:\n  - **Crossing Count**: The number of samples where the predicted lower quantile is greater than the predicted upper quantile.\n  $$ \\text{CrossingCount} = \\sum_{i=1}^N \\mathbb{I}(\\hat{q}_{i, 0.1} > \\hat{q}_{i, 0.9}) $$\n  - **Prediction Interval**: For each sample $i$, the interval is $[l_i, u_i]$ where $l_i = \\min(\\hat{q}_{i, 0.1}, \\hat{q}_{i, 0.9})$ and $u_i = \\max(\\hat{q}_{i, 0.1}, \\hat{q}_{i, 0.9})$.\n  - **Empirical Coverage Rate**: The fraction of observations $y_i$ falling within the constructed interval $[l_i, u_i]$.\n  $$ \\text{CoverageRate} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(l_i \\le y_i \\le u_i) $$\n  - **Mean Interval Width**: The average width of the constructed intervals.\n  $$ \\text{MeanWidth} = \\frac{1}{N} \\sum_{i=1}^N (u_i - l_i) = \\frac{1}{N} \\sum_{i=1}^N |\\hat{q}_{i, 0.9} - \\hat{q}_{i, 0.1}| $$\n\n- **Aggregate Quantile Score**: The mean over samples of the sum of the per-sample losses across all specified quantile levels.\n$$ \\text{AggregateScore} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\rho_{0.1}(u_{i, 0.1}) + \\rho_{0.5}(u_{i, 0.5}) + \\rho_{0.9}(u_{i, 0.9}) \\right) $$\nThis is equivalent to the sum of the mean losses: $\\text{MeanLoss}_{0.1} + \\text{MeanLoss}_{0.5} + \\text{MeanLoss}_{0.9}$.\n\nThe implementation will proceed by applying these derived formulas to the provided test data.", "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(y_true, q_pred_01, q_pred_05, q_pred_09):\n    \"\"\"\n    Computes quantile regression evaluation metrics from first principles.\n\n    Args:\n        y_true (np.ndarray): Array of true target values.\n        q_pred_01 (np.ndarray): Array of predicted 0.1-quantiles.\n        q_pred_05 (np.ndarray): Array of predicted 0.5-quantiles.\n        q_pred_09 (np.ndarray): Array of predicted 0.9-quantiles.\n\n    Returns:\n        list: A list of 10 computed metrics, rounded as required.\n    \"\"\"\n    y = np.array(y_true, dtype=float)\n    q01 = np.array(q_pred_01, dtype=float)\n    q05 = np.array(q_pred_05, dtype=float)\n    q09 = np.array(q_pred_09, dtype=float)\n\n    taus = [0.1, 0.5, 0.9]\n    quantiles = [q01, q05, q09]\n    \n    mean_losses = []\n    pos_loss_shares = []\n\n    for tau, q_pred in zip(taus, quantiles):\n        # Calculate per-sample quantile loss from the derived formula\n        u = y - q_pred\n        # rho(u) = tau * u if u >= 0 else (1 - tau) * (-u)\n        losses = np.where(u >= 0, tau * u, (1 - tau) * -u)\n        \n        # Calculate mean loss\n        mean_loss = np.mean(losses)\n        mean_losses.append(mean_loss)\n        \n        # Calculate share of loss from positive residuals\n        total_loss = np.sum(losses)\n        if total_loss == 0.0:\n            pos_share = 0.0\n        else:\n            # Positive residuals are strictly u > 0.\n            # Loss for u > 0 is tau * u.\n            pos_losses_sum = np.sum(np.where(u > 0, tau * u, 0.0))\n            pos_share = pos_losses_sum / total_loss\n        pos_loss_shares.append(pos_share)\n\n    # Interval-based metrics for tau=0.1 and tau=0.9\n    \n    # Crossing count\n    crossing_count = np.sum(q01 > q09)\n\n    # Construct intervals, handling crossings\n    interval_lower = np.minimum(q01, q09)\n    interval_upper = np.maximum(q01, q09)\n    \n    # Empirical coverage rate\n    covered = (y >= interval_lower)  (y = interval_upper)\n    coverage_rate = np.mean(covered)\n    \n    # Mean interval width\n    # Width is max - min, which is equivalent to |q09 - q01|\n    widths = interval_upper - interval_lower\n    mean_interval_width = np.mean(widths)\n    \n    # Aggregate quantile score\n    aggregate_score = sum(mean_losses)\n    \n    # Assemble results in the specified order and format\n    results = [\n        round(mean_losses[0], 6),\n        round(mean_losses[1], 6),\n        round(mean_losses[2], 6),\n        round(aggregate_score, 6),\n        round(coverage_rate, 6),\n        round(mean_interval_width, 6),\n        int(crossing_count),\n        round(pos_loss_shares[0], 6),\n        round(pos_loss_shares[1], 6),\n        round(pos_loss_shares[2], 6),\n    ]\n    \n    return results\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the metric calculations, and prints the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"y\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n            \"q_0.1\": [0.5, 1.5, 2.7, 3.0, 4.2, 5.5],\n            \"q_0.5\": [0.9, 1.8, 3.1, 3.9, 4.9, 6.2],\n            \"q_0.9\": [1.6, 2.5, 3.6, 4.8, 5.7, 6.8],\n        },\n        {\n            \"y\": [2.5, -1.0, 0.0, 3.3],\n            \"q_0.1\": [2.5, -1.0, 0.0, 3.3],\n            \"q_0.5\": [2.5, -1.0, 0.0, 3.3],\n            \"q_0.9\": [2.5, -1.0, 0.0, 3.3],\n        },\n        {\n            \"y\": [10.0, 0.0, -2.0, 5.0, 1.0],\n            \"q_0.1\": [12.0, 1.0, -1.0, 7.0, 3.0],\n            \"q_0.5\": [11.0, 0.5, -2.5, 6.0, 1.5],\n            \"q_0.9\": [9.0, -1.0, -3.0, 4.0, 0.0],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y_true = case[\"y\"]\n        q_pred_01 = case[\"q_0.1\"]\n        q_pred_05 = case[\"q_0.5\"]\n        q_pred_09 = case[\"q_0.9\"]\n        \n        result = calculate_metrics(y_true, q_pred_01, q_pred_05, q_pred_09)\n        all_results.append(result)\n\n    # Format the final output string exactly as required.\n    # The default str() for floats might not show trailing zeros,\n    # so we use a format specifier.\n    result_str = \"[\"\n    for i, res_list in enumerate(all_results):\n        res_list_str = \"[\"\n        for j, item in enumerate(res_list):\n            if isinstance(item, float):\n                res_list_str += f\"{item:.6f}\"\n            else:\n                res_list_str += str(item)\n            if j  len(res_list) - 1:\n                res_list_str += \",\"\n        res_list_str += \"]\"\n        result_str += res_list_str\n        if i  len(all_results) - 1:\n            result_str += \",\"\n    result_str += \"]\"\n    \n    print(result_str)\n\nsolve()\n```", "id": "3168892"}]}