{"hands_on_practices": [{"introduction": "A core principle of a deployed machine learning model is that its predictions should be deterministic for a given input. This practice demonstrates a critical pitfall of Batch Normalization: using the training-time logic during inference. Through a series of pathological examples [@problem_id:3101625], you will see how a model's prediction for a single data point can change based on the other unrelated inputs it is batched with, highlighting the fundamental need for a distinct inference-time procedure.", "problem": "Construct a program that demonstrates why using Batch Normalization (BN) batch statistics at inference time is dangerous by producing batch-dependent predictions. Work in one spatial dimension so that every input is a scalar. Use the following fundamental base.\n\nBatch Normalization (BN) definition for a batch $\\mathcal{B} = \\{x_i\\}_{i=1}^m$ with batch mean $\\mu_{\\mathcal{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$ and batch variance $\\sigma^2_{\\mathcal{B}} = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathcal{B}})^2$ transforms a sample $x$ into\n$$\n\\operatorname{BN}_{\\mathcal{B}}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}} \\;+\\; \\beta,\n$$\nwhere $\\gamma$ and $\\beta$ are learned affine parameters and $\\epsilon$ is a small positive constant to ensure numerical stability. Correct inference replaces $\\mu_{\\mathcal{B}}$ and $\\sigma^2_{\\mathcal{B}}$ by running estimates $\\mu_r$ and $\\sigma_r^2$ accumulated during training:\n$$\n\\operatorname{BN}_{\\mathrm{run}}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} \\;+\\; \\beta.\n$$\nA simple classifier uses the BN output $z$ and predicts the binary label $\\hat{y}$ by the threshold rule\n$$\n\\hat{y} \\;=\\; \n\\begin{cases}\n1, & z \\ge 0,\\\\\n0, & z < 0.\n\\end{cases}\n$$\nYou must implement both inference modes for a fixed target input $x^\\star$ and show that, if one incorrectly uses per-batch statistics at inference, the predicted label for $x^\\star$ can flip depending on the other samples in the batch.\n\nUse the following fixed parameters and definitions for all computations:\n- Target input: $x^\\star = 0.2$.\n- BN parameters: $\\gamma = 1.0$, $\\beta = -0.1$, $\\epsilon = 10^{-5}$.\n- Running statistics (the correct inference statistics): $\\mu_r = 0.0$, $\\sigma_r^2 = 1.0$.\n- Classification rule as given above.\n\nFor each batch $\\mathcal{B}$ below, compute:\n1. The correct-inference prediction for $x^\\star$ using $z_{\\mathrm{run}} = \\operatorname{BN}_{\\mathrm{run}}(x^\\star)$.\n2. The incorrect batch-dependent prediction for $x^\\star$ using $z_{\\mathcal{B}} = \\operatorname{BN}_{\\mathcal{B}}(x^\\star)$ with $\\mu_{\\mathcal{B}}$ and $\\sigma^2_{\\mathcal{B}}$ computed over all elements of the batch including $x^\\star$.\n3. A boolean indicating whether the incorrect batch-dependent prediction differs from the correct-inference prediction.\n\nTest suite (each batch is a multiset that always contains $x^\\star$):\n- Happy path with roughly standardized peers: $\\mathcal{B}_1 = [x^\\star, -1.0, 0.0, 1.0]$.\n- Pathological mean shift with large positive peers: $\\mathcal{B}_2 = [x^\\star, 10.0, 10.0, 10.0]$.\n- Small variance around a mean below $x^\\star$: $\\mathcal{B}_3 = [x^\\star, 0.0, 0.3, -0.3]$.\n- Large variance around the same mean: $\\mathcal{B}_4 = [x^\\star, 0.0, 3.0, -3.0]$.\n- Boundary case of batch size one: $\\mathcal{B}_5 = [x^\\star]$.\n\nYour program must compute the five booleans, in the order of $\\mathcal{B}_1$ through $\\mathcal{B}_5$, where each boolean is true if and only if the incorrect batch-dependent prediction for $x^\\star$ differs from the correct-inference prediction using running statistics.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, `[b_1,b_2,b_3,b_4,b_5]`, where each $b_i$ is either True or False.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of Batch Normalization in deep learning, is well-posed with all necessary parameters and definitions provided, and is objective in its formulation. It requests a computational demonstration of a well-known concept, which is a suitable and verifiable task.\n\nThe central principle to be demonstrated is that Batch Normalization (BN) must be handled differently during model training and inference. During training, BN normalizes layer inputs using the mean and variance of the current mini-batch of data. This makes the training process more stable. For inference, however, a single input is processed, not a batch. Using statistics from a \"batch\" of one or whatever ad-hoc batch of inputs happens to be available at inference time would make the model's prediction for a given input dependent on other, unrelated inputs processed concurrently. This violates the desideratum that a trained model should be a deterministic function. The correct procedure for inference is to use fixed, population-level statisticsâ€”approximated by aggregating statistics (e.g., via an exponential moving average) over all mini-batches seen during training.\n\nWe will first compute the correct, deterministic prediction for the target input $x^\\star$ using the provided running statistics. Then, for each test batch, we will compute the incorrect, batch-dependent prediction and compare the two.\n\nThe fixed parameters are:\n- Target input: $x^\\star = 0.2$\n- BN affine parameters: $\\gamma = 1.0$, $\\beta = -0.1$\n- Numerical stability constant: $\\epsilon = 10^{-5}$\n- Running statistics for correct inference: $\\mu_r = 0.0$, $\\sigma_r^2 = 1.0$\n\nThe classification rule is:\n$$\n\\hat{y} = \\begin{cases} 1, & z \\ge 0 \\\\ 0, & z < 0 \\end{cases}\n$$\nwhere $z$ is the output of the BN layer.\n\n### Step 1: Correct Inference Prediction\n\nThe correct inference uses the running statistics $\\mu_r$ and $\\sigma_r^2$. The BN output for $x^\\star$ is given by:\n$$\nz_{\\mathrm{run}} = \\operatorname{BN}_{\\mathrm{run}}(x^\\star) = \\gamma \\cdot \\frac{x^\\star - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} + \\beta\n$$\nSubstituting the given values:\n$$\nz_{\\mathrm{run}} = 1.0 \\cdot \\frac{0.2 - 0.0}{\\sqrt{1.0 + 10^{-5}}} - 0.1 \\approx \\frac{0.2}{1.000005} - 0.1 \\approx 0.199999 - 0.1 = 0.099999\n$$\nSince $z_{\\mathrm{run}} = 0.099999 \\ge 0$, the correct prediction is $\\hat{y}_{\\mathrm{run}} = 1$. This is the single, constant baseline against which all other predictions for $x^\\star$ will be compared.\n\n### Step 2: Incorrect Batch-Dependent Inference\n\nWe now compute the prediction for $x^\\star$ for each test batch $\\mathcal{B}_i$, incorrectly using the statistics of that batch. The BN output is:\n$$\nz_{\\mathcal{B}_i} = \\operatorname{BN}_{\\mathcal{B}_i}(x^\\star) = \\gamma \\cdot \\frac{x^\\star - \\mu_{\\mathcal{B}_i}}{\\sqrt{\\sigma^2_{\\mathcal{B}_i} + \\epsilon}} + \\beta\n$$\nwhere $\\mu_{\\mathcal{B}_i}$ and $\\sigma^2_{\\mathcal{B}_i}$ are computed from the elements of each batch $\\mathcal{B}_i$.\n\n**Case 1: Batch $\\mathcal{B}_1 = [0.2, -1.0, 0.0, 1.0]$**\n- Batch mean: $\\mu_{\\mathcal{B}_1} = \\frac{1}{4}(0.2 - 1.0 + 0.0 + 1.0) = \\frac{0.2}{4} = 0.05$\n- Batch variance: $\\sigma^2_{\\mathcal{B}_1} = \\frac{1}{4}((0.2-0.05)^2 + (-1.0-0.05)^2 + (0.0-0.05)^2 + (1.0-0.05)^2) = \\frac{1}{4}(0.0225+1.1025+0.0025+0.9025) = \\frac{2.03}{4} = 0.5075$\n- BN output: $z_{\\mathcal{B}_1} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{0.5075 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{0.7124} - 0.1 \\approx 0.2105 - 0.1 = 0.1105$\n- Prediction: $\\hat{y}_{\\mathcal{B}_1} = 1$ since $z_{\\mathcal{B}_1} \\ge 0$.\n- Comparison: $\\hat{y}_{\\mathcal{B}_1} = \\hat{y}_{\\mathrm{run}}$. The boolean is **False**.\n\n**Case 2: Batch $\\mathcal{B}_2 = [0.2, 10.0, 10.0, 10.0]$**\n- Batch mean: $\\mu_{\\mathcal{B}_2} = \\frac{1}{4}(0.2 + 10.0 + 10.0 + 10.0) = \\frac{30.2}{4} = 7.55$\n- Batch variance: $\\sigma^2_{\\mathcal{B}_2} = \\frac{1}{4}((0.2-7.55)^2 + 3 \\cdot (10.0-7.55)^2) = \\frac{1}{4}(54.0225 + 3 \\cdot 6.0025) = \\frac{72.03}{4} = 18.0075$\n- BN output: $z_{\\mathcal{B}_2} = 1.0 \\cdot \\frac{0.2 - 7.55}{\\sqrt{18.0075 + 10^{-5}}} - 0.1 \\approx \\frac{-7.35}{4.2435} - 0.1 \\approx -1.732 - 0.1 = -1.832$\n- Prediction: $\\hat{y}_{\\mathcal{B}_2} = 0$ since $z_{\\mathcal{B}_2} < 0$.\n- Comparison: $\\hat{y}_{\\mathcal{B}_2} \\neq \\hat{y}_{\\mathrm{run}}$. The boolean is **True**. This demonstrates how a large mean shift in the batch drastically alters the normalized value of $x^\\star$.\n\n**Case 3: Batch $\\mathcal{B}_3 = [0.2, 0.0, 0.3, -0.3]$**\n- Batch mean: $\\mu_{\\mathcal{B}_3} = \\frac{1}{4}(0.2 + 0.0 + 0.3 - 0.3) = \\frac{0.2}{4} = 0.05$\n- Batch variance: $\\sigma^2_{\\mathcal{B}_3} = \\frac{1}{4}((0.2-0.05)^2 + (0.0-0.05)^2 + (0.3-0.05)^2 + (-0.3-0.05)^2) = \\frac{1}{4}(0.0225+0.0025+0.0625+0.1225) = \\frac{0.21}{4} = 0.0525$\n- BN output: $z_{\\mathcal{B}_3} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{0.0525 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{0.22915} - 0.1 \\approx 0.6546 - 0.1 = 0.5546$\n- Prediction: $\\hat{y}_{\\mathcal{B}_3} = 1$ since $z_{\\mathcal{B}_3} \\ge 0$.\n- Comparison: $\\hat{y}_{\\mathcal{B}_3} = \\hat{y}_{\\mathrm{run}}$. The boolean is **False**.\n\n**Case 4: Batch $\\mathcal{B}_4 = [0.2, 0.0, 3.0, -3.0]$**\n- Batch mean: $\\mu_{\\mathcal{B}_4} = \\frac{1}{4}(0.2 + 0.0 + 3.0 - 3.0) = \\frac{0.2}{4} = 0.05$\n- Batch variance: $\\sigma^2_{\\mathcal{B}_4} = \\frac{1}{4}((0.2-0.05)^2 + (0.0-0.05)^2 + (3.0-0.05)^2 + (-3.0-0.05)^2) = \\frac{1}{4}(0.0225+0.0025+8.7025+9.3025) = \\frac{18.03}{4} = 4.5075$\n- BN output: $z_{\\mathcal{B}_4} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{4.5075 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{2.1231} - 0.1 \\approx 0.0706 - 0.1 = -0.0294$\n- Prediction: $\\hat{y}_{\\mathcal{B}_4} = 0$ since $z_{\\mathcal{B}_4} < 0$.\n- Comparison: $\\hat{y}_{\\mathcal{B}_4} \\neq \\hat{y}_{\\mathrm{run}}$. The boolean is **True**. Here, the batch mean is the same as in Cases 1 and 3, but the large variance reduces the magnitude of the normalized value, pushing it below the classification threshold after the $\\beta$ shift.\n\n**Case 5: Batch $\\mathcal{B}_5 = [0.2]$**\n- Batch mean: $\\mu_{\\mathcal{B}_5} = \\frac{1}{1}(0.2) = 0.2$\n- Batch variance: $\\sigma^2_{\\mathcal{B}_5} = \\frac{1}{1}((0.2 - 0.2)^2) = 0.0$\n- BN output: $z_{\\mathcal{B}_5} = 1.0 \\cdot \\frac{0.2 - 0.2}{\\sqrt{0.0 + 10^{-5}}} - 0.1 = \\frac{0.0}{\\sqrt{10^{-5}}} - 0.1 = 0.0 - 0.1 = -0.1$\n- Prediction: $\\hat{y}_{\\mathcal{B}_5} = 0$ since $z_{\\mathcal{B}_5} < 0$.\n- Comparison: $\\hat{y}_{\\mathcal{B}_5} \\neq \\hat{y}_{\\mathrm{run}}$. The boolean is **True**. This is an extreme case where the numerator $x^\\star - \\mu_{\\mathcal{B}}$ becomes zero. The output is simply $\\beta$, highlighting the critical role of $\\epsilon$ in preventing division by zero, but still leading to a flipped prediction.\n\nThese calculations clearly demonstrate that using batch statistics at inference time makes predictions for a fixed input $x^\\star$ unstable and dependent on the context of other inputs in the batch, leading to erroneous outcomes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Demonstrates the danger of using batch statistics for Batch Normalization\n    at inference time by showing that predictions can become batch-dependent.\n    \"\"\"\n    # Define fixed parameters and the target input.\n    x_star = 0.2\n    gamma = 1.0\n    beta = -0.1\n    epsilon = 1e-5\n\n    # Define running statistics for correct inference.\n    mu_r = 0.0\n    sigma_r_sq = 1.0\n\n    # Define the test cases (batches).\n    # Each batch is a list of floats and always contains x_star.\n    test_cases = [\n        # Happy path with roughly standardized peers\n        [x_star, -1.0, 0.0, 1.0],\n        # Pathological mean shift with large positive peers\n        [x_star, 10.0, 10.0, 10.0],\n        # Small variance around a mean below x_star\n        [x_star, 0.0, 0.3, -0.3],\n        # Large variance around the same mean\n        [x_star, 0.0, 3.0, -3.0],\n        # Boundary case of batch size one\n        [x_star],\n    ]\n\n    # --- Step 1: Compute the correct-inference prediction ---\n    # This prediction is deterministic and serves as the ground truth.\n    z_run = gamma * (x_star - mu_r) / np.sqrt(sigma_r_sq + epsilon) + beta\n    y_hat_run = 1 if z_run >= 0 else 0\n\n    results = []\n    # --- Step 2: Compute incorrect predictions for each batch ---\n    for batch_data in test_cases:\n        batch = np.array(batch_data)\n        \n        # Calculate batch-specific statistics.\n        # The variance is calculated using the population formula (1/m),\n        # as is standard in BN implementations.\n        mu_b = np.mean(batch)\n        sigma_b_sq = np.var(batch)\n\n        # Calculate the batch-dependent BN output for x_star.\n        z_b = gamma * (x_star - mu_b) / np.sqrt(sigma_b_sq + epsilon) + beta\n\n        # Determine the batch-dependent prediction.\n        y_hat_b = 1 if z_b >= 0 else 0\n\n        # Compare the incorrect prediction to the correct one.\n        # The result is True if the predictions differ.\n        prediction_differs = (y_hat_b != y_hat_run)\n        results.append(prediction_differs)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3101625"}, {"introduction": "Having established why a separate inference procedure is necessary, this exercise focuses on the correct implementation. You will build a Batch Normalization layer from the ground up, managing both the training and evaluation modes [@problem_id:3101661]. The key task is to implement the logic for updating running statistics ($\\mu_{\\text{run}}$, $\\sigma_{\\text{run}}^2$) during training and then using these frozen statistics for deterministic predictions during evaluation, verifying the numerical equivalence of the correct inference-time model.", "problem": "You are given the task of implementing a minimal feedforward model with a single affine layer followed by Batch Normalization (BN) and demonstrating the conversion from training mode to evaluation mode. The model must be implemented purely with arrays and operations as found in numerical programming environments and must not rely on external deep learning frameworks. The demonstration must verify numerical equivalence under frozen statistics with a specified tolerance bound.\n\nBegin from the following fundamental base, using only widely accepted definitions:\n\n- Given a mini-batch of feature vectors $x \\in \\mathbb{R}^{B \\times d}$, define the empirical mini-batch mean and variance along the batch dimension as\n$$\n\\mu_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} x_i, \\quad\n\\sigma_{\\text{batch}}^2 = \\frac{1}{B} \\sum_{i=1}^{B} (x_i - \\mu_{\\text{batch}})^2,\n$$\nboth computed elementwise for each feature dimension. A small constant $\\epsilon > 0$ is added to stabilize variance inversion.\n- Maintain running statistics with momentum $\\rho \\in [0,1]$ by exponential moving averages:\n$$\n\\mu_{\\text{run}} \\leftarrow (1 - \\rho)\\, \\mu_{\\text{run}} + \\rho\\, \\mu_{\\text{batch}}, \\quad\n\\sigma_{\\text{run}}^2 \\leftarrow (1 - \\rho)\\, \\sigma_{\\text{run}}^2 + \\rho\\, \\sigma_{\\text{batch}}^2.\n$$\nInitialize $\\mu_{\\text{run}}$ to the zero vector and $\\sigma_{\\text{run}}^2$ to the one vector of appropriate dimension.\n- Let the affine transformation be defined by weights $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ and bias $b \\in \\mathbb{R}^{d_{\\text{out}}}$ as\n$$\ny = x W + b,\n$$\nwith Batch Normalization applied featurewise to $y$ with learnable scale and shift parameters $\\gamma, \\beta \\in \\mathbb{R}^{d_{\\text{out}}}$:\n$$\n\\hat{y} = \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad\n\\text{BN}(y; \\gamma, \\beta, \\mu, \\sigma^2, \\epsilon) = \\gamma \\odot \\hat{y} + \\beta,\n$$\nwhere $\\odot$ denotes elementwise multiplication.\n\nThe goals are:\n\n1. Implement training-mode BN that computes $\\mu_{\\text{batch}}$ and $\\sigma_{\\text{batch}}^2$ on the current mini-batch, updates the running statistics $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ using momentum $\\rho$, and outputs normalized activations using the current mini-batch statistics.\n2. Implement evaluation-mode BN that uses the frozen running statistics $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ without updating them and outputs normalized activations using those frozen statistics.\n3. Implement a \"frozen-stats training forward\" that does not update $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ and uses $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ for normalization, even if the global model mode is training. This is the operational definition of \"frozen stats.\"\n4. After simulating training by processing a sequence of mini-batches to update $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$, convert the model to evaluation mode and verify numerical equivalence between evaluation-mode BN and frozen-stats training forward by checking that the maximum absolute difference between their outputs on the same input is less than or equal to a given tolerance bound $\\tau$.\n\nYour program must implement the above and evaluate the following test suite. For each case, construct the model parameters and data exactly as specified. All random numbers must be deterministically generated using the stated seeds. Use unbiasedness degree-of-freedom $0$ for variance (that is, the divisor is $B$). Every numerical value in this statement is dimensionless.\n\nFor each test case, define:\n- $d_{\\text{in}}$: input feature dimension.\n- $d_{\\text{out}}$: output feature dimension.\n- $B$: mini-batch size.\n- $T$: number of training mini-batches to process.\n- $\\rho$: momentum for running statistics.\n- $\\epsilon$: stabilizer added to variance.\n- Seeds: $s_{\\text{model}}$ to initialize $W$, $b$, $\\gamma$, $\\beta$; $s_{\\text{train}}$ to generate training mini-batches; $s_{\\text{test}}$ to generate the test mini-batch.\n- Training data generation rule and test data generation rule:\n  - If a constant value $c$ is specified, all entries in the training mini-batch are equal to $c$.\n  - Otherwise, for a normal distribution, draw entries independently from $\\mathcal{N}(\\mu, \\sigma^2)$ with specified mean $\\mu$ and standard deviation $\\sigma$.\n\nVerification criterion for each case:\n- After simulating training for $T$ mini-batches, compute the model output on a test mini-batch twice: once in evaluation mode and once in training mode with frozen statistics. Let $\\Delta$ be the maximum absolute difference between these two outputs. The result is True if $\\Delta \\le \\tau$ and False otherwise.\n\nUse the following test suite, each with tolerance bound $\\tau = 10^{-12}$:\n\n- Case $1$ (general \"happy path\"):\n  - $d_{\\text{in}} = 8$, $d_{\\text{out}} = 8$, $B = 32$, $T = 50$, $\\rho = 0.1$, $\\epsilon = 10^{-5}$,\n  - Seeds: $s_{\\text{model}} = 0$, $s_{\\text{train}} = 1$, $s_{\\text{test}} = 2$,\n  - Training data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$,\n  - Test data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$.\n\n- Case $2$ (boundary with batch size $1$ and zero-variance training batches):\n  - $d_{\\text{in}} = 4$, $d_{\\text{out}} = 4$, $B = 1$, $T = 3$, $\\rho = 0.5$, $\\epsilon = 10^{-3}$,\n  - Seeds: $s_{\\text{model}} = 10$, $s_{\\text{train}} = 11$, $s_{\\text{test}} = 12$,\n  - Training data: constant $c = 3.3$,\n  - Test data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$.\n\n- Case $3$ (large $\\epsilon$ and potentially negative scale parameters):\n  - $d_{\\text{in}} = 16$, $d_{\\text{out}} = 16$, $B = 64$, $T = 10$, $\\rho = 0.9$, $\\epsilon = 10^{-1}$,\n  - Seeds: $s_{\\text{model}} = 21$, $s_{\\text{train}} = 22$, $s_{\\text{test}} = 23$,\n  - Training data: $\\mathcal{N}(\\mu = 1.5, \\sigma = 2.0)$,\n  - Test data: $\\mathcal{N}(\\mu = -0.5, \\sigma = 1.0)$.\n\n- Case $4$ (momentum $\\rho = 0$, no running-statistics update):\n  - $d_{\\text{in}} = 5$, $d_{\\text{out}} = 5$, $B = 20$, $T = 25$, $\\rho = 0.0$, $\\epsilon = 10^{-6}$,\n  - Seeds: $s_{\\text{model}} = 31$, $s_{\\text{train}} = 32$, $s_{\\text{test}} = 33$,\n  - Training data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$,\n  - Test data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$.\n\n- Case $5$ (momentum $\\rho = 1$, running statistics equal last batch statistics):\n  - $d_{\\text{in}} = 7$, $d_{\\text{out}} = 7$, $B = 10$, $T = 7$, $\\rho = 1.0$, $\\epsilon = 10^{-6}$,\n  - Seeds: $s_{\\text{model}} = 41$, $s_{\\text{train}} = 42$, $s_{\\text{test}} = 43$,\n  - Training data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$,\n  - Test data: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$.\n\nConstruction of model parameters:\n- For each case, initialize the affine parameters $W$ and $b$, and BN parameters $\\gamma$ and $\\beta$ by drawing independent standard normal entries using the seed $s_{\\text{model}}$.\n\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True,True]\"). The entries must be the boolean values computed by the verification criterion in the exact order of the cases above. No physical units or angle units apply in this problem; report booleans only.", "solution": "The problem statement has been analyzed and is deemed valid. It constitutes a well-posed, self-contained, and computationally verifiable task that is scientifically grounded in the principles of deep learning. The definitions for the affine layer, Batch Normalization (BN), and the update rules for running statistics are standard and mathematically sound. The objective is to implement these components and verify the numerical equivalence between two operational modes of the BN layer, which is a test of implementation correctness.\n\nThe solution is constructed by following the architectural and procedural specifications laid out in the problem.\n\n**Model Architecture**\n\nThe model is a feedforward sequence of two layers: an affine layer and a Batch Normalization layer.\n\n$1$. **Affine Layer**: This layer executes a linear transformation. For an input mini-batch $x \\in \\mathbb{R}^{B \\times d_{\\text{in}}}$, where $B$ is the batch size and $d_{\\text{in}}$ is the input feature dimension, the layer computes an output $y \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$. This is governed by a weight matrix $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ and a bias vector $b \\in \\mathbb{R}^{d_{\\text{out}}}$. The transformation is:\n$$y = x W + b$$\nThe bias $b$ is broadcasted such that it is added to each row of the matrix product $xW$.\n\n$2$. **Batch Normalization (BN) Layer**: This layer normalizes its input, $y$, feature by feature. It operates in two distinct modes: training and evaluation.\n\n**BN in Training Mode**\n\nDuring the training phase, the BN layer normalizes its inputs using statistics calculated from the current mini-batch. It simultaneously updates a set of running statistics that approximate the global dataset statistics.\n\n- **Step $1$: Mini-Batch Statistics Computation**. For the input $y \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$, the element-wise mean $\\mu_{\\text{batch}} \\in \\mathbb{R}^{d_{\\text{out}}}$ and variance $\\sigma_{\\text{batch}}^2 \\in \\mathbb{R}^{d_{\\text{out}}}$ are computed across the batch dimension (of size $B$):\n$$\n\\mu_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} y_i\n$$\n$$\n\\sigma_{\\text{batch}}^2 = \\frac{1}{B} \\sum_{i=1}^{B} (y_i - \\mu_{\\text{batch}})^2\n$$\nThe problem explicitly states to use an unbiasedness degree-of-freedom of $0$, which corresponds to division by $B$.\n\n- **Step $2$: Normalization**. The input $y$ is normalized using these batch-specific statistics. A small constant $\\epsilon > 0$ is incorporated into the denominator to ensure numerical stability:\n$$\n\\hat{y} = \\frac{y - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n$$\nBoth $\\mu_{\\text{batch}}$ and the denominator term are broadcasted across the batch dimension for this element-wise operation.\n\n- **Step $3$: Scale and Shift Transformation**. The normalized activation $\\hat{y}$ is subsequently subjected to a learned affine transformation, defined by a scale parameter $\\gamma \\in \\mathbb{R}^{d_{\\text{out}}}$ and a shift parameter $\\beta \\in \\mathbb{R}^{d_{\\text{out}}}$:\n$$\n\\text{output} = \\gamma \\odot \\hat{y} + \\beta\n$$\nwhere $\\odot$ signifies element-wise multiplication.\n\n- **Step $4$: Running Statistics Update**. The layer maintains long-term running estimates of mean ($\\mu_{\\text{run}}$) and variance ($\\sigma_{\\text{run}}^2$). These are updated via an exponential moving average, controlled by a momentum parameter $\\rho \\in [0, 1]$:\n$$\n\\mu_{\\text{run}} \\leftarrow (1 - \\rho)\\mu_{\\text{run}} + \\rho\\mu_{\\text{batch}}\n$$\n$$\n\\sigma_{\\text{run}}^2 \\leftarrow (1 - \\rho)\\sigma_{\\text{run}}^2 + \\rho\\sigma_{\\text{batch}}^2\n$$\nAs specified, $\\mu_{\\text{run}}$ is initialized to a vector of zeros, and $\\sigma_{\\text{run}}^2$ is initialized to a vector of ones.\n\n**BN in Evaluation Mode**\n\nDuring evaluation or inference, using statistics from a single mini-batch may be undesirable or impossible (e.g., if $B=1$). Instead, the \"frozen\" running statistics, $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$, which were aggregated over the training process, are employed.\n\n- **Step $1$: Normalization with Running Statistics**. The input $y$ is normalized using the fixed $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$:\n$$\n\\hat{y} = \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\n$$\n\n- **Step $2$: Scale and Shift Transformation**. This step is identical to the one in training mode:\n$$\n\\text{output} = \\gamma \\odot \\hat{y} + \\beta\n$$\n\nCritically, the running statistics $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$ are not modified during evaluation mode.\n\n**Verification Procedure**\n\nThe core task is to confirm that a correct implementation of the BN layer produces numerically identical outputs for two conceptually equivalent scenarios after a simulated training phase.\n\n$1$. **Training Simulation**. For each of the five test cases, the process begins by initializing the model parameters $W, b, \\gamma, \\beta$ by drawing from a standard normal distribution, using the specified seed $s_{\\text{model}}$. The running statistics $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$ are initialized to $\\vec{0}$ and $\\vec{1}$, respectively. The model is then set to its training configuration. A simulation loop executes for $T$ steps. In each step, a training mini-batch is generated using a random number generator seeded with $s_{\\text{train}}$. A forward pass is performed, which uses the mini-batch statistics for normalization and updates the running statistics.\n\n$2$. **Equivalence Test**. Upon completion of the training simulation, the accumulated running statistics are considered final or \"frozen\". A new test mini-batch is generated using seed $s_{\\text{test}}$. The model's output for this test batch is then computed in two prescribed ways:\n    - **Scenario A (Evaluation Mode)**: The model is switched to its evaluation configuration (e.g., via a flag `training=False`). A forward pass on the test data is performed. In this mode, the layer must use the frozen $\\mu_{\\text{run}}$ and $\\sigma_{\\text{run}}^2$ for normalization.\n    - **Scenario B (Frozen-Stats Training Forward)**: The model remains in its nominal training configuration (e.g., `training=True`), but is instructed via a special flag (e.g., `freeze_stats=True`) to perform a forward pass that uses the frozen running statistics for normalization, bypassing the computation of batch statistics and the update of running statistics.\n\nBy their definitions, these two scenarios describe the same computation. The test is designed to validate that the implementation correctly reflects this identity.\n\n$3$. **Numerical Verification**. The maximum absolute element-wise difference, $\\Delta$, between the output matrices from Scenario A and Scenario B is calculated. The result for the test case is `True` if this difference is less than or equal to the specified tolerance, $\\Delta \\le \\tau$, where $\\tau = 10^{-12}$. This boolean result is recorded for each of the five test cases, yielding the final output list.", "answer": "```python\nimport numpy as np\n\nclass Affine:\n    \"\"\"Implements a single affine (fully connected) layer.\"\"\"\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n\n    def forward(self, x):\n        \"\"\"Performs the forward pass: y = xW + b.\"\"\"\n        return x @ self.W + self.b\n\nclass BatchNorm1d:\n    \"\"\"Implements a Batch Normalization layer for 1D features.\"\"\"\n    def __init__(self, d_out, gamma, beta, eps, momentum):\n        self.gamma = gamma  # scale\n        self.beta = beta    # shift\n        self.eps = eps\n        self.momentum = momentum\n        \n        self.running_mean = np.zeros(d_out)\n        self.running_var = np.ones(d_out)\n        \n        self.training = True\n\n    def forward(self, y, freeze_stats=False):\n        \"\"\"Performs the forward pass for Batch Normalization.\"\"\"\n        if not self.training or freeze_stats:\n            # Evaluation mode or frozen-stats training mode: use running stats\n            mean_to_use = self.running_mean\n            var_to_use = self.running_var\n        else:\n            # Standard training mode: use batch stats and update running stats\n            if y.shape[0] > 1:\n                batch_mean = np.mean(y, axis=0)\n                batch_var = np.var(y, axis=0, ddof=0)\n            else: # B=1 case\n                batch_mean = y[0]\n                batch_var = np.zeros_like(y[0])\n\n            mean_to_use = batch_mean\n            var_to_use = batch_var\n\n            # Update running statistics\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n            \n        # Normalize\n        y_hat = (y - mean_to_use) / np.sqrt(var_to_use + self.eps)\n        \n        # Scale and shift\n        out = self.gamma * y_hat + self.beta\n        return out\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n\nclass Model:\n    \"\"\"Combines an Affine layer and a BatchNorm1d layer.\"\"\"\n    def __init__(self, affine_layer, bn_layer):\n        self.affine = affine_layer\n        self.bn = bn_layer\n\n    def forward(self, x, freeze_stats=False):\n        y = self.affine.forward(x)\n        return self.bn.forward(y, freeze_stats=freeze_stats)\n\n    def train(self):\n        self.bn.train()\n\n    def eval(self):\n        self.bn.eval()\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (general \"happy path\")\n        {\n            \"d_in\": 8, \"d_out\": 8, \"B\": 32, \"T\": 50, \"rho\": 0.1, \"epsilon\": 1e-5,\n            \"s_model\": 0, \"s_train\": 1, \"s_test\": 2,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 2 (boundary with batch size 1 and zero-variance training batches)\n        {\n            \"d_in\": 4, \"d_out\": 4, \"B\": 1, \"T\": 3, \"rho\": 0.5, \"epsilon\": 1e-3,\n            \"s_model\": 10, \"s_train\": 11, \"s_test\": 12,\n            \"train_gen\": lambda rng, B, d: np.full((B, d), 3.3),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 3 (large epsilon and potentially negative scale parameters)\n        {\n            \"d_in\": 16, \"d_out\": 16, \"B\": 64, \"T\": 10, \"rho\": 0.9, \"epsilon\": 1e-1,\n            \"s_model\": 21, \"s_train\": 22, \"s_test\": 23,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=1.5, scale=2.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=-0.5, scale=1.0, size=(B, d)),\n        },\n        # Case 4 (momentum rho = 0, no running-statistics update)\n        {\n            \"d_in\": 5, \"d_out\": 5, \"B\": 20, \"T\": 25, \"rho\": 0.0, \"epsilon\": 1e-6,\n            \"s_model\": 31, \"s_train\": 32, \"s_test\": 33,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 5 (momentum rho = 1, running statistics equal last batch statistics)\n        {\n            \"d_in\": 7, \"d_out\": 7, \"B\": 10, \"T\": 7, \"rho\": 1.0, \"epsilon\": 1e-6,\n            \"s_model\": 41, \"s_train\": 42, \"s_test\": 43,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n    ]\n\n    tau = 1e-12\n    results = []\n\n    for case in test_cases:\n        d_in, d_out, B, T = case[\"d_in\"], case[\"d_out\"], case[\"B\"], case[\"T\"]\n        \n        # 1. Initialize Model\n        rng_model = np.random.default_rng(case[\"s_model\"])\n        W = rng_model.standard_normal(size=(d_in, d_out))\n        b = rng_model.standard_normal(size=(d_out,))\n        gamma = rng_model.standard_normal(size=(d_out,))\n        beta = rng_model.standard_normal(size=(d_out,))\n        \n        affine_layer = Affine(W, b)\n        bn_layer = BatchNorm1d(d_out, gamma, beta, case[\"epsilon\"], case[\"rho\"])\n        model = Model(affine_layer, bn_layer)\n        \n        # 2. Simulate Training\n        model.train()\n        rng_train = np.random.default_rng(case[\"s_train\"])\n        for _ in range(T):\n            train_batch = case[\"train_gen\"](rng_train, B, d_in)\n            model.forward(train_batch) # Forward pass updates running stats\n\n        # 3. Verification Step\n        rng_test = np.random.default_rng(case[\"s_test\"])\n        test_batch = case[\"test_gen\"](rng_test, B, d_in)\n        \n        # Scenario A: Evaluation mode\n        model.eval()\n        output_eval = model.forward(test_batch)\n\n        # Scenario B: Frozen-stats training forward\n        model.train() # Set global mode to training\n        output_frozen = model.forward(test_batch, freeze_stats=True)\n\n        # 4. Compare and record result\n        max_abs_diff = np.max(np.abs(output_eval - output_frozen))\n        results.append(max_abs_diff = tau)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3101661"}, {"introduction": "To truly master a technique, we must understand its limitations. This final practice is a thought experiment that explores the behavior of Batch Normalization at its boundary: a mini-batch of size one ($m=1$) [@problem_id:3101723]. By deriving the gradient of the loss with respect to the input, you will uncover a surprising result that explains why training with a batch size of one is often problematic and why gradient signals may fail to propagate backward through the network in this specific scenario.", "problem": "Consider a single-layer scalar network in training mode with Batch Normalization (BN), where the mini-batch size is $m=1$. Let the pre-normalization scalar activation be $x \\in \\mathbb{R}$. Batch Normalization (BN) proceeds by standardizing the activation using the empirical batch mean and variance, then applying an affine transformation. The empirical mean of a mini-batch $\\mathcal{B}$ of size $m$ is defined by\n$$\n\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i,\n$$\nand the empirical variance is defined by\n$$\n\\sigma_{\\mathcal{B}}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(x_i - \\mu_{\\mathcal{B}}\\right)^{2}.\n$$\nStandardization divides by the square root of the empirical variance with a positive stabilizing constant $\\epsilon  0$. After standardization, BN applies an affine map with learnable parameters $\\gamma \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$. Let the scalar target be $t \\in \\mathbb{R}$, and consider the squared-error loss\n$$\nL = \\left(y - t\\right)^{2},\n$$\nwhere $y$ is the post-BN scalar output.\n\nStarting from the definitions of empirical mean and variance, and using only the standardization and affine transformation steps described above, derive an explicit closed-form expression for the gradient $\\frac{dL}{dx}$ when $m=1$ during training. Your final answer must be a single real number or a single analytic expression. No rounding is required.", "solution": "The user wants to find the gradient $\\frac{dL}{dx}$ of the loss function $L$ with respect to the input $x$ for a Batch Normalization (BN) layer with a mini-batch of size $m=1$. The loss function is given by $L = (y - t)^2$, where $y$ is the output of the BN layer.\n\nFirst, we will express the output $y$ as a function of the input $x$ by applying the steps of Batch Normalization as defined in the problem statement for the specific case where the mini-batch size is $m=1$.\n\nThe mini-batch consists of a single scalar activation, which we denote as $\\mathcal{B} = \\{x\\}$.\n\n1.  **Compute the empirical batch mean $\\mu_{\\mathcal{B}}$:**\n    The formula for the mean is $\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$.\n    For $m=1$ and $\\mathcal{B} = \\{x\\}$, this becomes:\n    $$\n    \\mu_{\\mathcal{B}} = \\frac{1}{1} \\sum_{i=1}^{1} x_i = x\n    $$\n\n2.  **Compute the empirical batch variance $\\sigma_{\\mathcal{B}}^2$:**\n    The formula for the variance is $\\sigma_{\\mathcal{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$.\n    Substituting $m=1$, $\\mathcal{B} = \\{x\\}$, and $\\mu_{\\mathcal{B}} = x$, we get:\n    $$\n    \\sigma_{\\mathcal{B}}^2 = \\frac{1}{1} (x - \\mu_{\\mathcal{B}})^2 = (x - x)^2 = 0\n    $$\n\n3.  **Perform standardization:**\n    The standardized activation, let's call it $\\hat{x}$, is calculated as:\n    $$\n    \\hat{x} = \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}}\n    $$\n    where $\\epsilon  0$ is a small positive constant to ensure numerical stability.\n    Substituting the values for $\\mu_{\\mathcal{B}}$ and $\\sigma_{\\mathcal{B}}^2$ we just found:\n    $$\n    \\hat{x} = \\frac{x - x}{\\sqrt{0 + \\epsilon}} = \\frac{0}{\\sqrt{\\epsilon}} = 0\n    $$\n    The standardization step, for a batch size of $m=1$, results in an output of exactly $0$, irrespective of the input value $x$. The condition $\\epsilon  0$ is critical here, as it prevents division by zero.\n\n4.  **Apply the affine transformation:**\n    The final output of the BN layer, $y$, is obtained by scaling and shifting the standardized activation $\\hat{x}$ using the learnable parameters $\\gamma$ and $\\beta$:\n    $$\n    y = \\gamma \\hat{x} + \\beta\n    $$\n    Substituting the value $\\hat{x}=0$:\n    $$\n    y = \\gamma(0) + \\beta = \\beta\n    $$\n    This result shows that for a mini-batch size of $m=1$, the output of the Batch Normalization layer, $y$, is simply equal to the learnable bias parameter $\\beta$. The output $y$ has no dependence on the input activation $x$.\n\nNow, we can compute the gradient $\\frac{dL}{dx}$. The loss function is $L = (y-t)^2$. We can use the chain rule:\n$$\n\\frac{dL}{dx} = \\frac{dL}{dy} \\frac{dy}{dx}\n$$\nThe first term is the derivative of the loss with respect to the BN layer's output:\n$$\n\\frac{dL}{dy} = \\frac{d}{dy}(y-t)^2 = 2(y-t)\n$$\nSubstituting $y=\\beta$, this becomes $\\frac{dL}{dy} = 2(\\beta-t)$.\n\nThe second term is the derivative of the BN layer's output with respect to its input:\n$$\n\\frac{dy}{dx}\n$$\nAs we derived, for $m=1$, the function relating the output $y$ to the input $x$ is $y(x) = \\beta$. The parameter $\\beta$ is a learnable parameter of the model, but it is treated as a constant with respect to the input $x$ of the layer. Therefore, its derivative with respect to $x$ is zero:\n$$\n\\frac{dy}{dx} = \\frac{d}{dx}(\\beta) = 0\n$$\nFinally, we can compute the full gradient:\n$$\n\\frac{dL}{dx} = \\frac{dL}{dy} \\frac{dy}{dx} = 2(\\beta-t) \\cdot 0 = 0\n$$\nThe gradient of the loss with respect to the input activation $x$ is zero. This implies that for a mini-batch of size $1$, no gradient signal will be propagated backward through the BN layer to the layers that precede it. The normalization process effectively nullifies the input information for the purpose of gradient computation with respect to that input.", "answer": "$$\n\\boxed{0}\n$$", "id": "3101723"}]}