{"hands_on_practices": [{"introduction": "We begin with the classic problem where $L_1$ regularization first made its mark: sparse linear regression. In many scientific and engineering domains, we believe that out of a vast number of potential factors, only a few are truly influential. This exercise demonstrates how $L_1$ regularization, as implemented in the LASSO algorithm, is exceptionally skilled at identifying this sparse \"ground truth\" from data, a task where traditional methods like Ordinary Least Squares often fall short [@problem_id:3140969]. By implementing this from scratch, you will develop a core intuition for how the unique geometry of the $L_1$ norm naturally produces sparse solutions.", "problem": "You are given a synthetic sparse linear regression setting in which only a small subset of input dimensions influences the output. The goal is to implement and compare training with and without $\\ell_{1}$ regularization, and then evaluate how accurately the true sparse support is recovered as the observation noise varies.\n\nStart from the following foundational base:\n- The linear model assumption: the observed response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ is generated by $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{p}$ is the true parameter vector, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is noise.\n- The concept of sparsity: the true parameter vector $\\mathbf{w}^{\\star}$ has only $k$ nonzero entries (the support size), with all other entries equal to zero.\n- The $\\ell_{1}$ norm: for any vector $\\mathbf{w} \\in \\mathbb{R}^{p}$, the $\\ell_{1}$ norm is $\\|\\mathbf{w}\\|_{1} = \\sum_{j=1}^{p} |w_{j}|$.\n- The $\\ell_{2}$ norm: for any vector $\\mathbf{r} \\in \\mathbb{R}^{n}$, the $\\ell_{2}$ norm is $\\|\\mathbf{r}\\|_{2} = \\left(\\sum_{i=1}^{n} r_{i}^{2}\\right)^{1/2}$.\n\nTask specification:\n1. Data generation:\n   - Use $n = 120$ samples and $p = 100$ features.\n   - Set the true sparsity level to $k = 5$.\n   - Use a fixed random seed for reproducibility so that the true support is constant across all test cases.\n   - Draw $\\mathbf{X}$ with entries independently from a standard normal distribution, then center each column to have zero mean and scale each column so that its empirical second moment equals $1$ (that is, for each column $j$, enforce $(1/n)\\sum_{i=1}^{n} X_{ij}^{2} = 1$).\n   - Define the true parameter $\\mathbf{w}^{\\star}$ by selecting a support of size $k$ uniformly at random and setting those entries to $+1$, with all other entries set to $0$.\n   - Generate noise $\\boldsymbol{\\varepsilon}$ with independent entries from a normal distribution with zero mean and standard deviation $\\sigma$, and set $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}$.\n2. Training procedures to implement:\n   - With $\\ell_{1}$ regularization: minimize the objective $(1/(2n))\\|\\mathbf{y}-\\mathbf{X}\\mathbf{w}\\|_{2}^{2} + \\lambda \\|\\mathbf{w}\\|_{1}$ to estimate $\\widehat{\\mathbf{w}}_{\\ell_{1}}$. Derive the optimization steps from first principles using optimality conditions and implement a correct algorithm (for example, cyclic coordinate descent using soft-thresholding) that converges to a minimizer.\n   - Without $\\ell_{1}$ regularization: minimize $(1/(2n))\\|\\mathbf{y}-\\mathbf{X}\\mathbf{w}\\|_{2}^{2}$ to estimate $\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}}$ using Ordinary Least Squares (OLS), that is, solve the least-squares problem for $\\mathbf{w}$.\n   - For the $\\ell_{1}$ method, set the regularization parameter according to $\\lambda = \\alpha \\cdot \\sigma \\cdot \\sqrt{(2 \\log p)/n}$ with $\\alpha = 1$. This ties the regularization strength to the noise level in a principled way.\n3. Support recovery accuracy:\n   - Define the true support $S^{\\star} \\subset \\{1,\\dots,p\\}$ as the index set of nonzero entries of $\\mathbf{w}^{\\star}$, with $|S^{\\star}| = k$.\n   - For any estimated parameter vector $\\widehat{\\mathbf{w}}$, define its predicted support $\\widehat{S}$ as the index set of the $k$ largest entries of $\\widehat{\\mathbf{w}}$ in absolute value.\n   - Define the support recovery accuracy as the fraction of indices whose support membership is correctly classified: $\\frac{1}{p}\\sum_{j=1}^{p} \\mathbf{1}\\{(j \\in \\widehat{S}) \\Leftrightarrow (j \\in S^{\\star})\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function and $\\Leftrightarrow$ denotes logical equivalence.\n4. Test suite:\n   - Use the noise standard deviation values $\\sigma \\in \\{0.0, 0.1, 0.5, 1.0\\}$.\n   - For each $\\sigma$, train both models and compute the support recovery accuracy for each.\n5. Required final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{acc}_{\\ell_{1}}(\\sigma_{1}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{1}), \\text{acc}_{\\ell_{1}}(\\sigma_{2}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{2}), \\text{acc}_{\\ell_{1}}(\\sigma_{3}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{3}), \\text{acc}_{\\ell_{1}}(\\sigma_{4}), \\text{acc}_{\\mathrm{no}\\ \\ell_{1}}(\\sigma_{4})]$, where each $\\text{acc}$ is a float in $[0,1]$.\n\nNo user input is allowed; all parameters must be set internally according to the above specification. The program must be fully deterministic given the fixed random seed, and must implement the training and evaluation exactly as specified.", "solution": "The user wants to compare the performance of Ordinary Least Squares (OLS) and LASSO ($\\ell_1$ regularized regression) in recovering the true sparse support of a parameter vector in a synthetic linear regression problem. The comparison will be performed across varying levels of observation noise.\n\n### 1. Problem Formulation\n\nThe underlying generative model is a linear relationship perturbed by additive Gaussian noise:\n$$\n\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the vector of observations, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{p}$ is the true sparse parameter vector with only $k$ non-zero entries, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is a vector of i.i.d. noise components drawn from $\\mathcal{N}(0, \\sigma^2)$.\n\nWe aim to estimate $\\mathbf{w}^{\\star}$ using two methods:\n\n1.  **Ordinary Least Squares (OLS, no $\\ell_1$)**: This method minimizes the sum of squared residuals. The objective function is:\n    $$\n    L_{\\mathrm{no}\\ \\ell_{1}}(\\mathbf{w}) = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_{2}^{2}\n    $$\n2.  **LASSO ($\\ell_1$ regularization)**: This method adds an $\\ell_1$ penalty term to the OLS objective, which encourages sparse solutions. The objective function is:\n    $$\n    L_{\\ell_{1}}(\\mathbf{w}) = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_{2}^{2} + \\lambda \\|\\mathbf{w}\\|_{1}\n    $$\nwhere $\\|\\mathbf{w}\\|_{1} = \\sum_{j=1}^{p} |w_j|$ is the $\\ell_1$ norm of $\\mathbf{w}$.\n\n### 2. Data Generation\n\nThe synthetic data is generated as specified:\n-   **Dimensions**: Number of samples $n=120$, number of features $p=100$.\n-   **Sparsity**: The true support size is $k=5$.\n-   **Design Matrix $\\mathbf{X}$**: First, a matrix $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$ is drawn with entries from a standard normal distribution $\\mathcal{N}(0, 1)$. Each column $\\mathbf{z}_j$ is then centered to have zero mean and scaled to have an empirical second moment of $1$. That is, for each column $j$, the final column $\\mathbf{x}_j$ is computed as:\n    $$\n    \\mathbf{x}_j = \\frac{\\mathbf{z}_j - \\bar{z}_j \\mathbf{1}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (z_{ij} - \\bar{z}_j)^2}}\n    $$\n    where $\\bar{z}_j = \\frac{1}{n} \\sum_{i=1}^n z_{ij}$ and $\\mathbf{1}$ is a vector of ones. This ensures that for the final matrix $\\mathbf{X}$, $\\frac{1}{n}\\sum_{i=1}^{n} X_{ij} = 0$ and $\\frac{1}{n}\\sum_{i=1}^{n} X_{ij}^2 = 1$ for all $j \\in \\{1,\\dots,p\\}$. Note that the second property implies $\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{x}_j = 1$.\n-   **True Parameter Vector $\\mathbf{w}^{\\star}$**: A set of $k=5$ indices is chosen uniformly at random from $\\{1, \\dots, p\\}$ to form the true support $S^{\\star}$. The corresponding entries in $\\mathbf{w}^{\\star}$ are set to $1$, while all other $p-k$ entries are set to $0$.\n-   **Observation Vector $\\mathbf{y}$**: For a given noise level $\\sigma$, the noise vector $\\boldsymbol{\\varepsilon}$ is generated with i.i.d. entries from $\\mathcal{N}(0, \\sigma^2)$. The final observations are $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^{\\star} + \\boldsymbol{\\varepsilon}$.\n-   **Reproducibility**: A fixed random seed ensures that $\\mathbf{X}$ and $\\mathbf{w}^{\\star}$ are identical across all test cases (i.e., for all values of $\\sigma$).\n\n### 3. Model Training\n\n#### Ordinary Least Squares (OLS) Solution\nThe OLS estimator $\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}}$ is found by minimizing $L_{\\mathrm{no}\\ \\ell_{1}}(\\mathbf{w})$. This objective is a convex quadratic function, and its minimum can be found by setting its gradient with respect to $\\mathbf{w}$ to zero.\n$$\n\\nabla_{\\mathbf{w}} L_{\\mathrm{no}\\ \\ell_{1}}(\\mathbf{w}) = \\frac{1}{n} \\nabla_{\\mathbf{w}} \\left( \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \\right) = \\frac{1}{n} (-\\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\mathbf{w})\n$$\nSetting the gradient to zero yields the normal equations:\n$$\n\\mathbf{X}^T\\mathbf{X}\\widehat{\\mathbf{w}} = \\mathbf{X}^T\\mathbf{y}\n$$\nSince $n=120 > p=100$ and $\\mathbf{X}$ is generated from a continuous distribution, the matrix $\\mathbf{X}^T\\mathbf{X}$ is invertible with probability $1$. The unique OLS solution is:\n$$\n\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n#### LASSO Solution via Cyclic Coordinate Descent\nThe LASSO objective $L_{\\ell_{1}}(\\mathbf{w})$ is non-differentiable at points where any $w_j=0$ due to the $\\ell_1$ norm. We use an iterative optimization method called Cyclic Coordinate Descent (CCD). In CCD, we optimize the objective with respect to one coordinate $w_j$ at a time, holding all other coordinates fixed.\n\nLet's fix all coordinates $w_k$ for $k \\ne j$. The objective as a function of $w_j$ is:\n$$\nL(w_j) = \\frac{1}{2n} \\left\\|\\mathbf{y} - \\sum_{k \\ne j} \\mathbf{x}_k w_k - \\mathbf{x}_j w_j \\right\\|_2^2 + \\lambda |w_j| + \\text{const}\n$$\nLet $\\mathbf{r}_j = \\mathbf{y} - \\sum_{k \\ne j} \\mathbf{x}_k w_k$ be the partial residual. The objective simplifies to:\n$$\nL(w_j) = \\frac{1}{2n} \\|\\mathbf{r}_j - \\mathbf{x}_j w_j\\|_2^2 + \\lambda |w_j| = \\frac{1}{2n}(\\mathbf{r}_j^T\\mathbf{r}_j - 2w_j\\mathbf{x}_j^T\\mathbf{r}_j + w_j^2\\mathbf{x}_j^T\\mathbf{x}_j) + \\lambda |w_j|\n$$\nBecause of our column normalization, we have $\\mathbf{x}_j^T\\mathbf{x}_j = n$. The objective becomes:\n$$\nL(w_j) = \\frac{1}{2}w_j^2 - (\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j)w_j + \\lambda|w_j| + \\text{const}\n$$\nThe subgradient of this with respect to $w_j$ must contain zero at the minimum. This condition leads to the update rule defined by the soft-thresholding operator $S_{\\tau}(\\cdot)$:\n$$\nw_j^{\\text{new}} = S_{\\lambda}\\left(\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j\\right)\n$$\nwhere $S_{\\tau}(z) = \\text{sign}(z)\\max(|z|-\\tau, 0)$. The term $\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j$ can be expressed in a computationally efficient way for an iterative algorithm. Let $\\mathbf{w}^{\\text{old}}$ be the coefficient vector before the update of $w_j$.\n$$\n\\frac{1}{n}\\mathbf{x}_j^T\\mathbf{r}_j = \\frac{1}{n}\\mathbf{x}_j^T\\left(\\mathbf{y} - \\sum_{k \\ne j} \\mathbf{x}_k w_k^{\\text{old}}\\right) = \\frac{1}{n}\\mathbf{x}_j^T\\left(\\mathbf{y} - \\mathbf{X}\\mathbf{w}^{\\text{old}} + \\mathbf{x}_j w_j^{\\text{old}}\\right) = \\frac{1}{n}\\mathbf{x}_j^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}^{\\text{old}}) + w_j^{\\text{old}}\n$$\nThe term $\\mathbf{y} - \\mathbf{X}\\mathbf{w}^{\\text{old}}$ is the current full residual. This leads to an efficient CCD algorithm:\n1.  Initialize $\\widehat{\\mathbf{w}}_{\\ell_1} = \\mathbf{0}$.\n2.  Repeat for a fixed number of epochs (e.g., $100$):\n    For each coordinate $j=1, \\dots, p$:\n    a. Let $w_j^{\\text{old}}$ be the current value of the $j$-th coefficient.\n    b. Calculate the correlation of the $j$-th feature with the current partial residual: $\\rho_j = \\frac{1}{n}\\mathbf{x}_j^T(\\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{w}}_{\\ell_1}) + w_j^{\\text{old}}$. Note that this is efficient if we maintain the full residual $\\mathbf{r} = \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{w}}_{\\ell_1}$ and update it.\n    c. Apply the soft-thresholding operator to update the coefficient: $w_j^{\\text{new}} = S_{\\lambda}(\\rho_j)$.\n    d. Update the coefficient vector: $\\widehat{w}_{\\ell_1, j} = w_j^{\\text{new}}$.\n\nThe regularization parameter is set according to the theoretically motivated choice $\\lambda = \\alpha \\sigma \\sqrt{\\frac{2 \\log p}{n}}$, with $\\alpha=1$.\n\n### 4. Support Recovery Accuracy\n\nThe performance is measured by how well the estimated support $\\widehat{S}$ matches the true support $S^{\\star}$.\n-   $S^{\\star} = \\{ j \\mid w_j^{\\star} \\ne 0 \\}$, the set of indices of true non-zero coefficients. $|S^{\\star}| = k$.\n-   $\\widehat{S} = \\{ j \\mid \\widehat{w}_j \\text{ is among the } k \\text{ largest in absolute value} \\}$, the estimated support. $|\\widehat{S}| = k$.\n\nThe accuracy is the fraction of indices whose support membership is correctly identified. An index $j$ is classified correctly if it is either in both $S^{\\star}$ and $\\widehat{S}$ (a true positive) or in neither (a true negative).\n-   True Positives: $TP = |S^{\\star} \\cap \\widehat{S}|$.\n-   True Negatives: $TN = |\\{1,\\dots,p\\} \\setminus (S^{\\star} \\cup \\widehat{S})|$.\nSince $|S^{\\star}|=k$ and $|\\widehat{S}|=k$, we have $|S^{\\star} \\cup \\widehat{S}| = |S^{\\star}| + |\\widehat{S}| - |S^{\\star} \\cap \\widehat{S}| = 2k - TP$.\nTherefore, $TN = p - (2k - TP) = p - 2k + TP$.\nThe accuracy is defined as:\n$$\n\\text{Accuracy} = \\frac{TP + TN}{p} = \\frac{TP + (p - 2k + TP)}{p} = \\frac{p - 2k + 2TP}{p}\n$$\nThis formula is used to evaluate the accuracy for both $\\widehat{\\mathbf{w}}_{\\mathrm{no}\\ \\ell_{1}}$ and $\\widehat{\\mathbf{w}}_{\\ell_{1}}$ for each noise level $\\sigma$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares OLS and LASSO for sparse support recovery.\n    \"\"\"\n    # 1. Problem parameters\n    n = 120  # Number of samples\n    p = 100  # Number of features\n    k = 5    # Sparsity level (number of non-zero coefficients)\n    fixed_seed = 42 # Seed for reproducibility of X and w_star\n\n    # Test suite: noise standard deviations\n    sigmas = [0.0, 0.1, 0.5, 1.0]\n\n    # --- Data Generation (fixed across noise levels) ---\n    rng = np.random.default_rng(seed=fixed_seed)\n    \n    # Generate design matrix X\n    X_raw = rng.standard_normal(size=(n, p))\n    # Center each column\n    X_centered = X_raw - X_raw.mean(axis=0)\n    # Scale each column so its empirical second moment is 1\n    # (equivalent to empirical variance since columns are centered)\n    col_norms = np.sqrt(np.mean(X_centered**2, axis=0))\n    X = X_centered / col_norms\n\n    # Generate true sparse parameter vector w_star\n    w_star = np.zeros(p)\n    support_indices = rng.choice(p, size=k, replace=False)\n    w_star[support_indices] = 1.0\n    S_star = set(support_indices)\n\n    results = []\n\n    # Loop over each noise level\n    for sigma in sigmas:\n        # --- Generate response vector y for the current noise level ---\n        if sigma == 0.0:\n            epsilon = np.zeros(n)\n        else:\n            # Use a different seed for noise generation to ensure it's independent\n            # but still reproducible for a given sigma.\n            noise_rng = np.random.default_rng(seed=int(sigma * 1000))\n            epsilon = noise_rng.normal(0, sigma, n)\n        \n        y = X @ w_star + epsilon\n\n        # --- 2. Training Procedures ---\n\n        # A. Without L1 regularization (Ordinary Least Squares)\n        # Solve the normal equations: (X.T @ X) @ w = X.T @ y\n        XTX = X.T @ X\n        XTy = X.T @ y\n        w_no_l1 = np.linalg.solve(XTX, XTy)\n\n        # B. With L1 regularization (LASSO via Cyclic Coordinate Descent)\n        alpha = 1.0\n        lambda_val = alpha * sigma * np.sqrt(2 * np.log(p) / n)\n        \n        def soft_threshold(rho, lam):\n            \"\"\"Soft-thresholding operator.\"\"\"\n            if rho > lam:\n                return rho - lam\n            elif rho  -lam:\n                return rho + lam\n            else:\n                return 0.0\n\n        w_l1 = np.zeros(p)\n        num_epochs = 100  # Number of CCD cycles\n\n        # Efficient CCD implementation with residual updates\n        residual = y - X @ w_l1\n        for _ in range(num_epochs):\n            for j in range(p):\n                w_old_j = w_l1[j]\n                \n                # The argument to the soft-thresholding operator is (1/n) * X_j^T * r_j,\n                # where r_j is the partial residual y - sum_{k!=j} X_k w_k.\n                # This can be calculated efficiently as (1/n) * X_j^T * (residual + X_j * w_old_j).\n                rho = (X[:, j].T @ (residual + X[:, j] * w_old_j))\n                w_new_j = soft_threshold(rho / n, lambda_val)\n                \n                # Update residual with the change in w_j\n                if w_new_j != w_old_j:\n                    residual -= X[:, j] * (w_new_j - w_old_j)\n                    w_l1[j] = w_new_j\n\n\n        # --- 3. Support Recovery Accuracy ---\n        def calculate_accuracy(w_est, S_star_set, p_dim, k_dim):\n            \"\"\"Calculates support recovery accuracy.\"\"\"\n            # Get predicted support: indices of k largest absolute values\n            if np.all(w_est == 0):\n                S_hat = set()\n            else:\n                S_hat = set(np.argsort(np.abs(w_est))[-k_dim:])\n            \n            # True Positives: intersection of true and predicted supports\n            tp = len(S_hat.intersection(S_star_set))\n            \n            # Accuracy = (TP + TN) / p\n            # TN = p - |S* U S_hat| = p - (|S*|+|S_hat|-|S* intersect S_hat|)\n            # TN = p - (k + k - tp) = p - 2k + tp\n            # Accuracy = (tp + p - 2k + tp) / p\n            accuracy = (p_dim - 2 * k_dim + 2 * tp) / p_dim\n            return accuracy\n        \n        acc_l1 = calculate_accuracy(w_l1, S_star, p, k)\n        acc_no_l1 = calculate_accuracy(w_no_l1, S_star, p, k)\n        \n        results.extend([acc_l1, acc_no_l1])\n\n    # --- 4. Final Output ---\n    # Format: [acc_l1(s1), acc_no_l1(s1), acc_l1(s2), acc_no_l1(s2), ...]\n    output_str = \",\".join([f\"{res:.10f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3140969"}, {"introduction": "Having established how $L_1$ regularization works in linear models, we now transition this powerful concept into the realm of neural networks. Here, the idea of sparsity can be applied in more nuanced ways, offering different avenues for model compression and interpretation. This practice challenges you to compare two distinct strategies: the standard approach of penalizing network weights, and a more subtle technique of penalizing the neuron activations themselves [@problem_id:3140987]. This comparison highlights that sparsity is not a monolithic concept in deep learning; how and where you encourage it can lead to different trade-offs between model size, computational patterns, and predictive power.", "problem": "You are asked to implement and compare two forms of $\\ell_1$ regularization in a simple deep learning classifier: $\\ell_1$ applied to the weights versus an $\\ell_1$ penalty applied to the hidden activations. The comparison focuses on which method yields more compressible networks while maintaining accuracy close to an unregularized baseline. The program must be complete and runnable, and it must produce the specified single-line output format.\n\nThe fundamental base is Empirical Risk Minimization (ERM) and standard gradient-based training for a single-hidden-layer neural network. The network uses a Rectified Linear Unit (ReLU) hidden activation and a logistic sigmoid output for binary classification. The loss is Binary Cross-Entropy (BCE). You will implement two versions of ERM that differ only in the regularization term:\n\n- $\\ell_1$ on weights: add an $\\ell_1$ penalty on all weight matrices.\n- $\\ell_1$ on activations: add an $\\ell_1$ penalty on the hidden layer activations.\n\nDefinitions and setting:\n- The model is a single-hidden-layer network with input dimension $d_{in} = 2$, hidden width $m$, and output dimension $1$. For an input matrix $X \\in \\mathbb{R}^{N \\times d_{in}}$, hidden parameters $(W_1 \\in \\mathbb{R}^{d_{in} \\times m}, b_1 \\in \\mathbb{R}^{m})$, output parameters $(W_2 \\in \\mathbb{R}^{m \\times 1}, b_2 \\in \\mathbb{R}^{1})$, and hidden preactivation $Z_1 = X W_1 + \\mathbf{1} b_1^\\top$, the hidden activation is $H = \\max(0, Z_1)$ where the maximum is elementwise (Rectified Linear Unit (ReLU)), the output preactivation is $Z_2 = H W_2 + \\mathbf{1} b_2$, and the prediction is $\\hat{y} = \\sigma(Z_2)$ where $\\sigma(a) = \\frac{1}{1 + e^{-a}}$ is the logistic sigmoid applied elementwise.\n- The unregularized empirical risk uses Binary Cross-Entropy (BCE): for labels $y \\in \\{0,1\\}^N$, $$\\text{BCE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( -y_i \\log(\\hat{y}_i) - (1 - y_i) \\log(1 - \\hat{y}_i) \\right).$$\n- The $\\ell_1$ norm for a matrix $A$ is $\\|A\\|_1 = \\sum_{i,j} |A_{ij}|$.\n\nTwo regularization variants to implement in training:\n1. $\\ell_1$ on weights (no activation penalty): \n   $$L_{\\text{weights}} = \\text{BCE} + \\lambda_w \\left(\\|W_1\\|_1 + \\|W_2\\|_1\\right).$$\n   Biases $b_1$ and $b_2$ are not regularized.\n2. $\\ell_1$ on activations (no weight penalty):\n   $$L_{\\text{activations}} = \\text{BCE} + \\lambda_h \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\|H_{i,\\cdot}\\|_1 \\right).$$\n   This activation penalty is averaged per sample. Biases are not regularized.\n\nTraining protocol:\n- Use full-batch gradient descent for $E$ epochs with constant step size $\\eta$.\n- Initialize weights with independent and identically distributed Gaussian entries $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.1$, and biases to $0$. Use a fixed random seed for reproducibility.\n- Use the chain rule and subgradients. For the $\\ell_1$ on weights, use the subgradient $\\partial |w| = \\text{sign}(w)$ with $\\text{sign}(0) = 0$. For the $\\ell_1$ on activations, add $\\lambda_h \\cdot \\text{sign}(H)$ to the gradient with respect to $H$ before backpropagating through ReLU; at $H=0$ use $0$.\n\nDataset generation:\n- Create a binary classification dataset in $\\mathbb{R}^2$ with $N_{\\text{train}}$ training samples and $N_{\\text{test}}$ test samples. Class $0$ is drawn from a Gaussian with mean $(-d, 0)$ and identity covariance; class $1$ from a Gaussian with mean $(+d, 0)$ and identity covariance. Classes are balanced, and a fixed random seed is used for sampling.\n\nEvaluation metrics:\n- Test accuracy: the fraction of correct predictions on the test set using a threshold of $0.5$ on $\\hat{y}$.\n- Weight sparsity fraction $s_W$: the fraction of entries in $W_1$ and $W_2$ with absolute value at most a threshold $\\tau$. Formally, if $n_W$ is the total number of entries across $W_1$ and $W_2$, $$s_W = \\frac{1}{n_W} \\left|\\{ \\text{entries of } W_1, W_2 \\text{ with } |w| \\le \\tau \\} \\right|.$$\n- Activation sparsity fraction $s_H$: evaluated on the test set, the fraction of hidden activations with magnitude at most $\\tau$. If $H_{\\text{test}} \\in \\mathbb{R}^{N_{\\text{test}} \\times m}$ is the hidden activation on test inputs, $$s_H = \\frac{1}{N_{\\text{test}} \\cdot m} \\left|\\{ \\text{entries of } H_{\\text{test}} \\text{ with } |h| \\le \\tau \\} \\right|.$$\n- Combined compressibility score $S = s_W + s_H$.\n\nDecision rule per test case:\n- Compute a baseline accuracy $A_0$ by training with $\\lambda_w = 0$ and $\\lambda_h = 0$.\n- Train the network with $\\ell_1$ on weights and with $\\ell_1$ on activations using the specified $\\lambda_w$ and $\\lambda_h$ respectively, yielding accuracies $A_W$ and $A_H$ and compressibility scores $S_W$ and $S_H$.\n- If neither method achieves accuracy at least $A_0 - \\Delta$ (where $\\Delta$ is the allowed accuracy drop), output $-1$.\n- Otherwise, among methods with accuracy at least $A_0 - \\Delta$, choose the method with larger $S$. If $|S_W - S_H| \\le \\varepsilon$ (a tie tolerance), output $2$. If $S_W  S_H$ by more than $\\varepsilon$, output $0$ (weights better). If $S_H  S_W$ by more than $\\varepsilon$, output $1$ (activations better).\n\nAngle units are not applicable. There are no physical units involved. Percentages must be expressed as decimals; accuracy is a fraction in $[0,1]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the integer code for the decision of the $i$-th test case.\n\nUse the following global hyperparameters (fixed across test cases): $N_{\\text{train}} = 200$, $N_{\\text{test}} = 200$, $E = 300$, $\\eta = 0.1$, $\\sigma = 0.1$, tie tolerance $\\varepsilon = 10^{-3}$, and a fixed random seed $s = 0$ for all randomness.\n\nTest suite (each test case is a tuple $(m, \\lambda_w, \\lambda_h, \\Delta, \\tau, d)$):\n- Case $1$: $(16, 0.01, 0.01, 0.05, 10^{-3}, 2.5)$\n- Case $2$: $(16, 0.30, 0.01, 0.03, 10^{-3}, 2.5)$\n- Case $3$: $(16, 0.01, 0.30, 0.03, 10^{-3}, 2.5)$\n- Case $4$: $(8, 0.02, 0.02, 0.08, 10^{-3}, 0.8)$", "solution": "The problem requires the implementation and comparison of two types of $\\ell_1$ regularization for a simple binary classifier: one penalizing the network's weights and the other penalizing its hidden layer activations. The goal is to determine which method provides better network compressibility while maintaining classification accuracy.\n\nThe model is a single-hidden-layer neural network with input dimension $d_{in}=2$, a hidden layer of width $m$, and a single output neuron. The hidden layer uses the Rectified Linear Unit (ReLU) activation function, and the output layer uses a logistic sigmoid function to produce a probability for binary classification.\n\nFor a batch of $N$ input samples $X \\in \\mathbb{R}^{N \\times d_{in}}$, the forward propagation through the network is defined as follows:\nThe pre-activation of the hidden layer is calculated as:\n$$ Z_1 = X W_1 + \\mathbf{1} b_1^\\top $$\nwhere $W_1 \\in \\mathbb{R}^{d_{in} \\times m}$ and $b_1 \\in \\mathbb{R}^{m}$ are the weights and biases of the hidden layer, respectively, and $\\mathbf{1}$ is an $N$-dimensional column vector of ones. The hidden layer activations $H \\in \\mathbb{R}^{N \\times m}$ are obtained by applying the ReLU function element-wise:\n$$ H = \\text{ReLU}(Z_1) = \\max(0, Z_1) $$\nThe pre-activation of the output layer is then:\n$$ Z_2 = H W_2 + \\mathbf{1} b_2 $$\nwhere $W_2 \\in \\mathbb{R}^{m \\times 1}$ and $b_2 \\in \\mathbb{R}^{1}$ are the output layer weights and bias. Finally, the predicted probabilities $\\hat{y} \\in \\mathbb{R}^{N \\times 1}$ are computed using the sigmoid function $\\sigma(\\cdot)$:\n$$ \\hat{y} = \\sigma(Z_2) = \\frac{1}{1 + e^{-Z_2}} $$\n\nThe training process is based on full-batch gradient descent to minimize an empirical risk function. The base loss function is the Binary Cross-Entropy (BCE) loss, averaged over the batch:\n$$ L_{\\text{BCE}}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\nwhere $y \\in \\{0, 1\\}^N$ are the true labels.\n\nTwo regularization schemes are added to this base loss:\n1.  **$\\ell_1$ on weights**: This scheme adds a penalty proportional to the $\\ell_1$ norm of the weight matrices $W_1$ and $W_2$. The total loss is:\n    $$ L_{\\text{weights}} = L_{\\text{BCE}} + \\lambda_w \\left( \\|W_1\\|_1 + \\|W_2\\|_1 \\right) $$\n    where $\\|A\\|_1 = \\sum_{i,j} |A_{ij}|$. Biases $b_1$ and $b_2$ are not regularized.\n2.  **$\\ell_1$ on activations**: This scheme penalizes the $\\ell_1$ norm of the hidden layer activations, averaged over all samples in the batch. The total loss is:\n    $$ L_{\\text{activations}} = L_{\\text{BCE}} + \\lambda_h \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\|H_{i,\\cdot}\\|_1 \\right) $$\n    where $H_{i,\\cdot}$ is the activation vector for the $i$-th sample.\n\nThe network parameters $\\boldsymbol{\\theta} = \\{W_1, b_1, W_2, b_2\\}$ are updated using the gradient descent rule $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}} L$, where $\\eta$ is the learning rate. The gradients are computed via backpropagation.\n\nThe gradient of the BCE loss with respect to the output pre-activation $Z_2$ is:\n$$ \\nabla_{Z_2} L_{\\text{BCE}} = \\frac{1}{N} (\\hat{y} - y) $$\nLet's denote this gradient as $dZ_2$. The gradients for the output layer parameters are:\n$$ \\nabla_{W_2} L = H^\\top dZ_2 + \\nabla_{W_2} L_{\\text{reg}} $$\n$$ \\nabla_{b_2} L = \\sum_{i=1}^{N} (dZ_2)_i $$\nThe gradient is then propagated back to the hidden activations $H$:\n$$ \\nabla_{H} L = dZ_2 W_2^\\top + \\nabla_{H} L_{\\text{reg}} $$\nLet's denote this as $dH$. This gradient is then passed through the derivative of the ReLU function:\n$$ \\nabla_{Z_1} L = dH \\odot \\mathbb{I}(Z_1  0) $$\nwhere $\\odot$ denotes element-wise multiplication and $\\mathbb{I}(\\cdot)$ is the indicator function. Let this be $dZ_1$. Finally, the gradients for the hidden layer parameters are:\n$$ \\nabla_{W_1} L = X^\\top dZ_1 + \\nabla_{W_1} L_{\\text{reg}} $$\n$$ \\nabla_{b_1} L = \\sum_{i=1}^{N} (dZ_1)_i $$\n\nThe regularization-specific gradient terms are derived from the subgradient of the $\\ell_1$ norm, using $\\partial|x| / \\partial x = \\text{sign}(x)$ with $\\text{sign}(0)=0$:\n-   For $L_{\\text{weights}}$:\n    $$ \\nabla_{W_1} L_{\\text{reg}} = \\lambda_w \\text{sign}(W_1) $$\n    $$ \\nabla_{W_2} L_{\\text{reg}} = \\lambda_w \\text{sign}(W_2) $$\n    $$ \\nabla_{H} L_{\\text{reg}} = 0 $$\n-   For $L_{\\text{activations}}$:\n    $$ \\nabla_{W_1} L_{\\text{reg}} = 0 $$\n    $$ \\nabla_{W_2} L_{\\text{reg}} = 0 $$\n    $$ \\nabla_{H} L_{\\text{reg}} = \\frac{\\lambda_h}{N} \\text{sign}(H) $$\n\nThe implementation will follow this mathematical framework. For each test case, three models are trained from the same initial state: a baseline model ($\\lambda_w=0, \\lambda_h=0$), a weight-regularized model, and an activation-regularized model. Their performance is evaluated on a test set based on accuracy and a combined sparsity score $S = s_W + s_H$, where $s_W$ is the fraction of near-zero weights and $s_H$ is the fraction of near-zero hidden activations on the test set. A decision rule is then applied to select the superior regularization method based on which one achieves the highest sparsity score $S$ while not dropping in accuracy by more than a specified tolerance $\\Delta$ from the baseline. Reproducibility is ensured by using a fixed random seed for both data generation and weight initialization.", "answer": "```python\nimport numpy as np\n\n# Global hyperparameters\nN_TRAIN = 200\nN_TEST = 200\nEPOCHS = 300\nETA = 0.1\nINIT_SIGMA = 0.1\nTIE_EPSILON = 1e-3\nSEED = 0\n\ndef sigmoid(z):\n    \"\"\"Numerically stable sigmoid function.\"\"\"\n    z = np.clip(z, -500, 500)\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef generate_data(n_samples, d, rng):\n    \"\"\"Generates a balanced binary classification dataset.\"\"\"\n    n_class_0 = n_samples // 2\n    n_class_1 = n_samples - n_class_0\n    \n    mean0 = [-d, 0]\n    mean1 = [d, 0]\n    cov = [[1, 0], [0, 1]]\n    \n    class0_data = rng.multivariate_normal(mean0, cov, n_class_0)\n    class1_data = rng.multivariate_normal(mean1, cov, n_class_1)\n    \n    X = np.vstack((class0_data, class1_data))\n    y = np.vstack((np.zeros((n_class_0, 1)), np.ones((n_class_1, 1))))\n    \n    permutation = rng.permutation(n_samples)\n    return X[permutation], y[permutation]\n\nclass NeuralNetwork:\n    \"\"\"A single hidden layer neural network for binary classification.\"\"\"\n    def __init__(self, d_in, m, init_sigma, rng):\n        self.W1 = rng.normal(0, init_sigma, (d_in, m))\n        self.b1 = np.zeros((1, m))\n        self.W2 = rng.normal(0, init_sigma, (m, 1))\n        self.b2 = np.zeros((1, 1))\n        self.m = m\n\n    def forward(self, X):\n        \"\"\"Performs the forward pass.\"\"\"\n        Z1 = X @ self.W1 + self.b1\n        H = np.maximum(0, Z1)  # ReLU\n        Z2 = H @ self.W2 + self.b2\n        y_hat = sigmoid(Z2)\n        return y_hat, H\n\n    def accuracy(self, X, y):\n        \"\"\"Calculates prediction accuracy.\"\"\"\n        y_hat, _ = self.forward(X)\n        predictions = (y_hat > 0.5).astype(int)\n        return np.mean(predictions == y)\n\n    def get_sparsity_scores(self, X_test, tau):\n        \"\"\"Calculates weight and activation sparsity fractions.\"\"\"\n        # Weight sparsity\n        n_w1 = self.W1.size\n        n_w2 = self.W2.size\n        n_w_total = n_w1 + n_w2\n        sparse_w1 = np.sum(np.abs(self.W1) = tau)\n        sparse_w2 = np.sum(np.abs(self.W2) = tau)\n        s_W = (sparse_w1 + sparse_w2) / n_w_total\n\n        # Activation sparsity\n        _, H_test = self.forward(X_test)\n        n_h_total = H_test.size\n        sparse_h = np.sum(np.abs(H_test) = tau)\n        s_H = sparse_h / n_h_total\n\n        return s_W, s_H\n\ndef train(network, X, y, epochs, eta, lambda_w, lambda_h):\n    \"\"\"Trains the neural network using full-batch gradient descent.\"\"\"\n    N = X.shape[0]\n\n    for _ in range(epochs):\n        # Forward pass\n        y_hat, H = network.forward(X)\n        \n        # Backward pass\n        # Gradient of BCE loss w.r.t. Z2\n        dZ2 = (y_hat - y) / N\n\n        # Gradients for output layer\n        dW2 = H.T @ dZ2\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n\n        # Gradient w.r.t. H\n        dH = dZ2 @ network.W2.T\n        \n        # Add activation regularization gradient if applicable\n        if lambda_h > 0:\n            dH += (lambda_h / N) * np.sign(H)\n\n        # Backprop through ReLU\n        dZ1 = dH * (H > 0)\n\n        # Gradients for hidden layer\n        dW1 = X.T @ dZ1\n        db1 = np.sum(dZ1, axis=0, keepdims=True)\n        \n        # Add weight regularization gradients if applicable\n        if lambda_w > 0:\n            dW1 += lambda_w * np.sign(network.W1)\n            dW2 += lambda_w * np.sign(network.W2)\n            \n        # Update parameters\n        network.W1 -= eta * dW1\n        network.b1 -= eta * db1\n        network.W2 -= eta * dW2\n        network.b2 -= eta * db2\n\ndef solve():\n    \"\"\"Main function to run test cases and generate output.\"\"\"\n    test_cases = [\n        (16, 0.01, 0.01, 0.05, 1e-3, 2.5),\n        (16, 0.30, 0.01, 0.03, 1e-3, 2.5),\n        (16, 0.01, 0.30, 0.03, 1e-3, 2.5),\n        (8, 0.02, 0.02, 0.08, 1e-3, 0.8),\n    ]\n\n    results = []\n\n    for m, lambda_w_case, lambda_h_case, delta, tau, d in test_cases:\n        \n        # Ensure reproducibility for each test case\n        data_rng = np.random.default_rng(SEED)\n        \n        X_train, y_train = generate_data(N_TRAIN, d, data_rng)\n        X_test, y_test = generate_data(N_TEST, d, data_rng)\n\n        # 1. Baseline run\n        model_rng = np.random.default_rng(SEED)\n        net_base = NeuralNetwork(d_in=2, m=m, init_sigma=INIT_SIGMA, rng=model_rng)\n        train(net_base, X_train, y_train, EPOCHS, ETA, 0.0, 0.0)\n        A0 = net_base.accuracy(X_test, y_test)\n\n        # 2. L1 on weights run\n        model_rng = np.random.default_rng(SEED)\n        net_w = NeuralNetwork(d_in=2, m=m, init_sigma=INIT_SIGMA, rng=model_rng)\n        train(net_w, X_train, y_train, EPOCHS, ETA, lambda_w_case, 0.0)\n        A_W = net_w.accuracy(X_test, y_test)\n        s_W_w, s_H_w = net_w.get_sparsity_scores(X_test, tau)\n        S_W = s_W_w + s_H_w\n\n        # 3. L1 on activations run\n        model_rng = np.random.default_rng(SEED)\n        net_h = NeuralNetwork(d_in=2, m=m, init_sigma=INIT_SIGMA, rng=model_rng)\n        train(net_h, X_train, y_train, EPOCHS, ETA, 0.0, lambda_h_case)\n        A_H = net_h.accuracy(X_test, y_test)\n        s_W_h, s_H_h = net_h.get_sparsity_scores(X_test, tau)\n        S_H = s_W_h + s_H_h\n        \n        # 4. Decision logic\n        w_valid = A_W >= A0 - delta\n        h_valid = A_H >= A0 - delta\n        \n        decision = -1\n        if not w_valid and not h_valid:\n            decision = -1\n        elif w_valid and not h_valid:\n            decision = 0\n        elif not w_valid and h_valid:\n            decision = 1\n        else:  # both are valid\n            if S_W > S_H + TIE_EPSILON:\n                decision = 0\n            elif S_H > S_W + TIE_EPSILON:\n                decision = 1\n            else:\n                decision = 2\n        \n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3140987"}, {"introduction": "Our final practice advances to a cutting-edge application of sparsity in deep learning: creating efficient models through structured pruning. Simply zeroing out individual weights can result in a model that is \"sparse\" on paper but fails to deliver significant speed-ups on modern hardware. This exercise introduces a more sophisticated technique where an $L_1$ penalty is applied to the scaling parameters, $\\gamma$, within Batch Normalization layers [@problem_id:3140994]. Driving a $\\gamma$ parameter to zero effectively deactivates an entire feature map, achieving a form of structured sparsity that translates into tangible gains in computational efficiency. This practice demonstrates how a fundamental mathematical tool can be cleverly adapted to solve critical engineering challenges in deploying lean and fast deep learning models.", "problem": "You are given a simplified deep learning model with Batch Normalization (BN) that maps an input matrix $X \\in \\mathbb{R}^{N \\times d}$ to a scalar output via one hidden layer with $C$ channels. The hidden pre-activations are $H = XW + \\mathbf{1}b^\\top$ where $W \\in \\mathbb{R}^{d \\times C}$, $b \\in \\mathbb{R}^{C}$, and $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the all-ones vector. For each channel, Batch Normalization (BN) computes a batch mean $\\mu \\in \\mathbb{R}^{C}$ and variance $\\sigma^2 \\in \\mathbb{R}^{C}$, produces normalized activations $Z = (H - \\mu)/\\sqrt{\\sigma^2 + \\epsilon}$, and then scales and shifts with parameters $\\Gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$ to yield $Y = \\Gamma \\odot Z + \\beta$, where $\\odot$ denotes elementwise multiplication. The scalar model output for the $i$-th sample is $s_i = \\sum_{k=1}^{C} Y_{i,k} v_k$ with $v \\in \\mathbb{R}^{C}$. During training, the running mean and variance used at test time are updated with momentum $m \\in (0,1)$ as $\\mu_{\\text{run}} \\leftarrow m \\mu_{\\text{run}} + (1-m)\\mu$ and $\\sigma^2_{\\text{run}} \\leftarrow m \\sigma^2_{\\text{run}} + (1-m)\\sigma^2$.\n\nThe empirical risk is defined using the logistic loss with labels $y_i \\in \\{-1,+1\\}$:\n$$\n\\mathcal{L}_{\\text{logistic}} = \\frac{1}{N}\\sum_{i=1}^{N} \\log\\left(1 + \\exp(-y_i s_i)\\right).\n$$\nWe consider two distinct regularized objectives via empirical risk minimization:\n- Case A (weight sparsity): add an $L_1$ penalty on the weights $W$ with strength $\\alpha$, giving the objective $J_A = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|W\\|_1$.\n- Case B (channel sparsity): add an $L_1$ penalty on the BN scale parameters $\\Gamma$ with the same strength $\\alpha$, giving the objective $J_B = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|\\Gamma\\|_1$.\n\nFor optimization, use gradient descent on the smooth part of the objective with learning rate $\\eta$, and apply the proximal operator associated with the $L_1$ norm to the penalized parameters after each gradient step. The Batch Normalization (BN) gradients must be derived from the BN definitions given above; you must not assume any unproven or ad hoc formulas.\n\nDefine an “inactive channel at test time” as any channel index $k \\in \\{1,\\dots,C\\}$ whose trained BN scale parameter satisfies $|\\Gamma_k| \\le \\tau$, where $\\tau  0$ is a small threshold. The test-time BN transformation uses the running mean $\\mu_{\\text{run}}$ and running variance $\\sigma^2_{\\text{run}}$, but the inactivity criterion depends only on $\\Gamma$.\n\nYour program must:\n- Generate a synthetic binary classification dataset in $\\mathbb{R}^d$ with $N$ samples by drawing $N/2$ samples for $y=+1$ from a multivariate normal distribution with mean vector $\\mu_{+} \\in \\mathbb{R}^d$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, and $N/2$ samples for $y=-1$ from a multivariate normal distribution with mean vector $\\mu_{-} \\in \\mathbb{R}^d$ and the same covariance $\\Sigma$. All random draws must be reproducible by fixing a seed.\n- Train the model twice for each test case: once with Case A ($L_1$ penalty on $W$) and once with Case B ($L_1$ penalty on $\\Gamma$), using identical initial parameters and hyperparameters except for which parameter receives the $L_1$ penalty.\n- After training, count how many channels are inactive at test time for each case, i.e., the number of $k$ such that $|\\Gamma_k| \\le \\tau$.\n\nStart from the following foundations exclusively:\n- Empirical Risk Minimization with logistic loss as defined above.\n- Batch Normalization (BN) as defined above, including the use of running averages for test-time statistics.\n- Gradient descent for the smooth part of the objective and the proximal operator corresponding to the $L_1$ norm (without assuming any shortcut formulas not derived from the $L_1$ norm definition).\n\nYour program must implement these steps with clear, principled computations.\n\nUse the following fixed hyperparameters unless otherwise stated in the test cases:\n- Input dimension $d = 4$.\n- Number of channels $C = 8$.\n- Training set size $N = 256$.\n- Learning rate $\\eta = 0.05$.\n- BN epsilon $\\epsilon = 10^{-5}$.\n- BN momentum $m = 0.9$.\n- Initial parameters: $W$ initialized with independent normal draws of standard deviation $0.1$, $b$ initialized to $0$, $\\Gamma$ initialized to $1$, $\\beta$ initialized to $0$, $v$ initialized with independent normal draws of standard deviation $0.1$.\n\nImplement a single-epoch full-batch gradient descent loop repeated for the specified number of steps in each test case. You must ensure that both Case A and Case B start from identical initial parameters in each test case to make the comparison fair.\n\nTest Suite:\nProvide the following four test cases, each given as a tuple $(\\alpha, \\tau, S)$ where $\\alpha$ is the $L_1$ strength, $\\tau$ is the inactivity threshold, and $S$ is the number of gradient descent steps:\n1. $(0.0, 10^{-3}, 200)$: No $L_1$ penalty baseline.\n2. $(10^{-3}, 10^{-3}, 400)$: Weak $L_1$ to compare effects.\n3. $(5 \\times 10^{-3}, 10^{-3}, 400)$: Moderate $L_1$ to induce noticeable sparsity.\n4. $(2 \\times 10^{-2}, 10^{-3}, 400)$: Strong $L_1$ to test near-collapse behavior.\n\nRequired Final Output Format:\nYour program should produce a single line of output containing a list of results for the four test cases. Each result is itself a list of two integers $[n_{\\Gamma}, n_{W}]$, where $n_{\\Gamma}$ is the number of inactive channels when penalizing $\\Gamma$ (Case B) and $n_{W}$ is the number of inactive channels when penalizing $W$ (Case A), in that order. The final line must therefore look like:\n$[ [n_{\\Gamma}^{(1)}, n_{W}^{(1)}], [n_{\\Gamma}^{(2)}, n_{W}^{(2)}], [n_{\\Gamma}^{(3)}, n_{W}^{(3)}], [n_{\\Gamma}^{(4)}, n_{W}^{(4)}] ]$,\nwith the four inner lists corresponding to the four test cases in the order listed above.", "solution": "The user's request is to analyze and compare the sparsity-inducing effects of applying an $L_1$ penalty to either the weights of a linear layer or the scaling parameters of a subsequent Batch Normalization (BN) layer. This is a well-defined problem in computational science, grounded in the principles of empirical risk minimization and neural network optimization. The problem is valid and will be solved by deriving the necessary gradients and implementing the specified proximal gradient descent algorithm.\n\n### 1. Model and Objective Functions\n\nThe model maps an input matrix $X \\in \\mathbb{R}^{N \\times d}$ to a vector of scalar outputs $s \\in \\mathbb{R}^{N}$ through a single hidden layer with $C$ channels.\n\n**Forward Pass:**\n1.  **Linear Transformation**: The pre-activation for the hidden layer is computed as $H = XW + \\mathbf{1}b^\\top$, where $W \\in \\mathbb{R}^{d \\times C}$ are the weights, $b \\in \\mathbb{R}^{C}$ is the bias vector, and $\\mathbf{1} \\in \\mathbb{R}^{N}$ is a vector of ones.\n2.  **Batch Normalization (BN)**: For each channel $k \\in \\{1, \\dots, C\\}$, the batch mean $\\mu_k$ and variance $\\sigma^2_k$ are calculated over the $N$ samples:\n    $$\n    \\mu_k = \\frac{1}{N} \\sum_{i=1}^{N} H_{i,k} \\quad \\text{and} \\quad \\sigma_k^2 = \\frac{1}{N} \\sum_{i=1}^{N} (H_{i,k} - \\mu_k)^2\n    $$\n    The pre-activations are then normalized:\n    $$\n    Z_{i,k} = \\frac{H_{i,k} - \\mu_k}{\\sqrt{\\sigma_k^2 + \\epsilon}}\n    $$\n    where $\\epsilon$ is a small constant for numerical stability. Finally, a learned affine transformation is applied:\n    $$\n    Y_{i,k} = \\Gamma_k Z_{i,k} + \\beta_k\n    $$\n    using the channel-wise scale parameter $\\Gamma_k$ and shift parameter $\\beta_k$.\n3.  **Output Layer**: The final scalar score for the $i$-th sample is a linear combination of the channel outputs:\n    $$\n    s_i = \\sum_{k=1}^{C} Y_{i,k} v_k\n    $$\n    where $v \\in \\mathbb{R}^{C}$ is the output weight vector.\n\n**Objective Functions:**\nThe base loss is the empirical risk using the logistic loss for binary classification with labels $y_i \\in \\{-1, +1\\}$:\n$$\n\\mathcal{L}_{\\text{logistic}} = \\frac{1}{N} \\sum_{i=1}^{N} \\log(1 + \\exp(-y_i s_i))\n$$\nTwo regularized objectives are considered:\n-   **Case A (Weight Sparsity)**: An $L_1$ penalty is applied to the weight matrix $W$:\n    $$\n    J_A = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|W\\|_1 = \\mathcal{L}_{\\text{logistic}} + \\alpha \\sum_{j=1}^{d} \\sum_{k=1}^{C} |W_{j,k}|\n    $$\n-   **Case B (Channel Sparsity)**: An $L_1$ penalty is applied to the BN scale vector $\\Gamma$:\n    $$\n    J_B = \\mathcal{L}_{\\text{logistic}} + \\alpha \\|\\Gamma\\|_1 = \\mathcal{L}_{\\text{logistic}} + \\alpha \\sum_{k=1}^{C} |\\Gamma_k|\n    $$\n\n### 2. Optimization by Proximal Gradient Descent\n\nThe objectives $J_A$ and $J_B$ are composite, consisting of a smooth part ($\\mathcal{L}_{\\text{logistic}}$) and a non-smooth part (the $L_1$ norm). Such problems are effectively solved using proximal gradient descent. For a general objective $J(\\theta) = f(\\theta) + g(\\theta)$ where $f$ is smooth and $g$ is non-smooth, the update rule is:\n$$\n\\theta_{t+1} = \\text{prox}_{\\eta g}(\\theta_t - \\eta \\nabla f(\\theta_t))\n$$\nThe proximal operator for the scaled $L_1$ norm, $g(\\theta) = \\alpha \\|\\theta\\|_1$, is the soft-thresholding function, applied element-wise:\n$$\n\\text{prox}_{\\eta \\alpha \\|\\cdot\\|_1}(x)_i = \\text{sign}(x_i) \\max(|x_i| - \\eta\\alpha, 0)\n$$\nIn our procedure, we first perform a standard gradient descent step on the parameters using the gradients of $\\mathcal{L}_{\\text{logistic}}$, and then apply the soft-thresholding operator to the specific parameter being penalized ($W$ in Case A, $\\Gamma$ in Case B).\n\n### 3. Gradient Derivation\n\nTo implement the gradient descent step, we need the gradients of $\\mathcal{L}_{\\text{logistic}}$ with respect to all trainable parameters ($W, b, \\Gamma, \\beta, v$). We use the chain rule, starting from the output and propagating backwards. Let $\\nabla_{\\theta}\\mathcal{L} \\equiv \\frac{\\partial \\mathcal{L}_{\\text{logistic}}}{\\partial \\theta}$.\n\n1.  **Gradient w.r.t. scores $s$**:\n    $\\frac{\\partial \\mathcal{L}_{\\text{logistic}}}{\\partial s_i} = \\frac{1}{N} \\frac{-y_i \\exp(-y_i s_i)}{1+\\exp(-y_i s_i)} = \\frac{1}{N} \\frac{-y_i}{1+\\exp(y_i s_i)}$. Let this gradient vector be $\\nabla_s \\mathcal{L} \\in \\mathbb{R}^N$.\n\n2.  **Gradients w.r.t. output layer ($v$) and BN output ($Y$)**:\n    $$\n    \\nabla_v \\mathcal{L} = Y^\\top (\\nabla_s \\mathcal{L}) \\quad \\in \\mathbb{R}^C\n    $$\n    $$\n    \\nabla_Y \\mathcal{L} = (\\nabla_s \\mathcal{L}) v^\\top \\quad \\in \\mathbb{R}^{N \\times C}\n    $$\n    where $\\nabla_s \\mathcal{L}$ is treated as a column vector.\n\n3.  **Gradients w.r.t. BN parameters ($\\Gamma, \\beta$) and BN input ($Z$)**:\n    $$\n    \\nabla_\\Gamma \\mathcal{L} = \\text{sum}(\\nabla_Y \\mathcal{L} \\odot Z, \\text{axis}=0) \\quad \\in \\mathbb{R}^C\n    $$\n    $$\n    \\nabla_\\beta \\mathcal{L} = \\text{sum}(\\nabla_Y \\mathcal{L}, \\text{axis}=0) \\quad \\in \\mathbb{R}^C\n    $$\n    $$\n    \\nabla_Z \\mathcal{L} = \\nabla_Y \\mathcal{L} \\odot \\Gamma^\\top \\quad \\in \\mathbb{R}^{N \\times C}\n    $$\n    where $\\odot$ is element-wise multiplication and $\\Gamma^\\top$ is broadcast across the batch dimension.\n\n4.  **Gradient w.r.t. pre-normalization activations $H$**: This is the most involved step, as each $H_{i,k}$ affects all $Z_{j,k}$ for a given channel $k$ through the batch statistics $\\mu_k$ and $\\sigma_k^2$. The final expression for the gradient with respect to the input matrix $H$ (for a single channel $k$, then generalized) is:\n    $$\n    (\\nabla_H \\mathcal{L})_{i,k} = \\frac{1}{\\sqrt{\\sigma_k^2+\\epsilon}} \\left[ (\\nabla_Z \\mathcal{L})_{i,k} - \\frac{1}{N}\\sum_{j=1}^N (\\nabla_Z \\mathcal{L})_{j,k} - Z_{i,k} \\frac{1}{N}\\sum_{j=1}^N \\left((\\nabla_Z \\mathcal{L})_{j,k} Z_{j,k}\\right) \\right]\n    $$\n    This can be implemented efficiently using vectorized operations.\n\n5.  **Gradients w.r.t. input weights ($W$) and biases ($b$)**:\n    $$\n    \\nabla_W \\mathcal{L} = X^\\top (\\nabla_H \\mathcal{L}) \\quad \\in \\mathbb{R}^{d \\times C}\n    $$\n    $$\n    \\nabla_b \\mathcal{L} = \\text{sum}(\\nabla_H \\mathcal{L}, \\text{axis}=0) \\quad \\in \\mathbb{R}^C\n    $$\n\n### 4. Implementation and Evaluation\n\nThe program implements this process for the specified test cases. A synthetic dataset is generated from two multivariate normal distributions. For each test case, two training runs are performed, starting from identical initial parameters: one for Case A and one for Case B. A full-batch gradient descent loop is executed for the specified number of steps, applying the updates derived above. After training, the number of \"inactive channels\" is counted for each case, defined as channels where the final learned scale parameter $|\\Gamma_k|$ falls below a threshold $\\tau$. The results are then collated and printed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment and print the results.\n    \"\"\"\n    RNG_SEED = 42\n    np.random.seed(RNG_SEED)\n\n    # Fixed hyperparameters\n    CONSTS = {\n        'd': 4,\n        'C': 8,\n        'N': 256,\n        'eta': 0.05,\n        'epsilon': 1e-5,\n        'm': 0.9,\n    }\n\n    # Test cases: (alpha, tau, S)\n    test_cases = [\n        (0.0, 1e-3, 200),\n        (1e-3, 1e-3, 400),\n        (5e-3, 1e-3, 400),\n        (2e-2, 1e-3, 400),\n    ]\n\n    # Data generation parameters\n    mu_plus = np.full(CONSTS['d'], 0.5)\n    mu_minus = np.full(CONSTS['d'], -0.5)\n    Sigma = np.eye(CONSTS['d'])\n    \n    # Generate dataset\n    N_half = CONSTS['N'] // 2\n    X_plus = np.random.multivariate_normal(mu_plus, Sigma, N_half)\n    X_minus = np.random.multivariate_normal(mu_minus, Sigma, N_half)\n    X = np.vstack([X_plus, X_minus])\n    y = np.hstack([np.ones(N_half), -np.ones(N_half)])\n\n    all_results = []\n    \n    for alpha, tau, S in test_cases:\n        # Generate initial parameters for this test case\n        # Ensures both Case A and B start from the exact same point\n        initial_params = {\n            'W': np.random.normal(0, 0.1, (CONSTS['d'], CONSTS['C'])),\n            'b': np.zeros(CONSTS['C']),\n            'Gamma': np.ones(CONSTS['C']),\n            'beta': np.zeros(CONSTS['C']),\n            'v': np.random.normal(0, 0.1, CONSTS['C']),\n            'mu_run': np.zeros(CONSTS['C']),\n            'sigma2_run': np.ones(CONSTS['C']),\n        }\n\n        # Case A: Penalize W\n        final_params_A = run_training(initial_params, X, y, alpha, S, penalize_W=True, consts=CONSTS)\n        n_W = np.sum(np.abs(final_params_A['Gamma']) = tau)\n\n        # Case B: Penalize Gamma\n        final_params_B = run_training(initial_params, X, y, alpha, S, penalize_W=False, consts=CONSTS)\n        n_Gamma = np.sum(np.abs(final_params_B['Gamma']) = tau)\n        \n        all_results.append([n_Gamma, n_W])\n\n    # Format and print the final output\n    print(str(all_results).replace(' ', ''))\n\n\ndef soft_threshold(x, threshold):\n    \"\"\"Soft-thresholding operator for L1 proximal step.\"\"\"\n    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n\n\ndef run_training(initial_params, X, y, alpha, S, penalize_W, consts):\n    \"\"\"\n    Runs the training loop for S steps.\n    \"\"\"\n    # Make a deep copy to avoid modifying the initial parameters dict\n    params = {k: np.copy(v) for k, v in initial_params.items()}\n\n    for _ in range(S):\n        # 1. Forward pass\n        s, cache = forward_pass(X, params, consts)\n\n        # 2. Update running statistics\n        params['mu_run'] = consts['m'] * params['mu_run'] + (1 - consts['m']) * cache['mu']\n        params['sigma2_run'] = consts['m'] * params['sigma2_run'] + (1 - consts['m']) * cache['var']\n\n        # 3. Backward pass (compute gradients of the smooth part)\n        grads = backward_pass(X, y, s, cache, consts)\n\n        # 4. Parameter update (gradient descent + proximal step)\n        eta_alpha = consts['eta'] * alpha\n\n        # Update non-penalized parameters with standard gradient descent\n        params['v'] -= consts['eta'] * grads['dL_dv']\n        params['beta'] -= consts['eta'] * grads['dL_dbeta']\n        params['b'] -= consts['eta'] * grads['dL_db']\n\n        if penalize_W:\n            # Penalize W (Case A)\n            params['Gamma'] -= consts['eta'] * grads['dL_dGamma']\n            W_temp = params['W'] - consts['eta'] * grads['dL_dW']\n            params['W'] = soft_threshold(W_temp, eta_alpha)\n        else:\n            # Penalize Gamma (Case B)\n            params['W'] -= consts['eta'] * grads['dL_dW']\n            G_temp = params['Gamma'] - consts['eta'] * grads['dL_dGamma']\n            params['Gamma'] = soft_threshold(G_temp, eta_alpha)\n\n    return params\n\n\ndef forward_pass(X, params, consts):\n    \"\"\"Computes the forward pass of the model.\"\"\"\n    W, b = params['W'], params['b']\n    Gamma, beta = params['Gamma'], params['beta']\n    v = params['v']\n    \n    # Linear layer\n    H = X @ W + b\n    \n    # Batch Normalization\n    mu = H.mean(axis=0)\n    var = H.var(axis=0)\n    sigma_eff = np.sqrt(var + consts['epsilon'])\n    H_centered = H - mu\n    Z = H_centered / sigma_eff\n    Y = Gamma * Z + beta\n    \n    # Output layer\n    s = Y @ v\n    \n    cache = {'X': X, 'H': H, 'Z': Z, 'mu': mu, 'var': var, 'sigma_eff': sigma_eff, 'Gamma': Gamma, 'Y': Y, 'v': v}\n    return s, cache\n\n\ndef backward_pass(X, y, s, cache, consts):\n    \"\"\"Computes the gradients for all parameters.\"\"\"\n    N = consts['N']\n    \n    # Gradient of loss w.r.t. scores s\n    g_s = -y / (1 + np.exp(y * s))\n    dL_ds = (1 / N) * g_s\n    \n    # Backprop to v and Y\n    dL_dv = cache['Y'].T @ dL_ds\n    dL_dY = np.outer(dL_ds, cache['v'])\n    \n    # Backprop through BN affine transform to Gamma, beta, and Z\n    dL_dGamma = np.sum(dL_dY * cache['Z'], axis=0)\n    dL_dbeta = np.sum(dL_dY, axis=0)\n    dL_dZ = dL_dY * cache['Gamma']\n    \n    # Backprop through BN normalization to H\n    dLdZ_sum = np.sum(dL_dZ, axis=0)\n    dLdZ_Z_sum = np.sum(dL_dZ * cache['Z'], axis=0)\n    \n    term1 = dL_dZ\n    term2 = (1 / N) * dLdZ_sum\n    term3 = (1 / N) * cache['Z'] * dLdZ_Z_sum\n    \n    dL_dH = (1 / cache['sigma_eff']) * (term1 - term2 - term3)\n    \n    # Backprop through linear layer to W and b\n    dL_dW = X.T @ dL_dH\n    dL_db = np.sum(dL_dH, axis=0)\n    \n    grads = {'dL_dv': dL_dv, 'dL_dGamma': dL_dGamma, 'dL_dbeta': dL_dbeta, 'dL_dW': dL_dW, 'dL_db': dL_db}\n    return grads\n\nsolve()\n\n```", "id": "3140994"}]}