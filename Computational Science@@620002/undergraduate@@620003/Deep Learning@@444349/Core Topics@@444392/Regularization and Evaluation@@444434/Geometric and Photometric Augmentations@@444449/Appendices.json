{"hands_on_practices": [{"introduction": "Images are more than just grids of pixel values; they are captures of physical light. Most standard image formats, like JPEG or PNG, have undergone non-linear transformations, such as gamma correction, which can distort the underlying physics. This exercise explores a critical question for building robust models: should we apply photometric augmentations like brightness and contrast changes in the raw, linear sensor space before these transformations, or in the processed, non-linear space afterward? By simulating a simplified camera pipeline, you will discover which approach leads to more consistent representations across different devices, a key property for generalization [@problem_id:3129353].", "problem": "Consider a simplified camera imaging pipeline to study photometric augmentations. Let a synthetic one-dimensional image be represented by a vector of $N$ samples $x \\in [0,1]^N$ that approximates normalized linear sensor measurements in the camera's raw domain. Assume $x$ is constructed as a uniform ramp $x_i = \\frac{i}{N-1}$ for $i \\in \\{0,1,\\dots,N-1\\}$, so that $x_0 = 0$ and $x_{N-1} = 1$. Define two augmentation operators: brightness jitter and contrast jitter. Brightness jitter with factor $\\beta > 0$ is the operator $B_{\\beta}(x) = \\operatorname{clip}_{[0,1]}(\\beta x)$, where $\\operatorname{clip}_{[0,1]}(z)$ denotes component-wise clipping of $z$ into the interval $[0,1]$. Contrast jitter with factor $\\kappa > 0$ is the operator $C_{\\kappa}(z) = \\operatorname{clip}_{[0,1]}\\big((z - \\mu_z)\\kappa + \\mu_z\\big)$, where $\\mu_z = \\frac{1}{N}\\sum_{i=0}^{N-1} z_i$ is the mean intensity of $z$. Define the combined jitter operator $J_{\\beta,\\kappa}(x) = C_{\\kappa}(B_{\\beta}(x))$.\n\nA camera pipeline maps raw measurements $x$ to a display-referred domain via a tone mapping function. For camera $c$ with gamma parameter $g_c > 0$, define the tone map as $T_{g_c}(x) = \\operatorname{clip}_{[0,1]}(x)^{1/g_c}$ applied component-wise. This is a simplified model of the gamma encoding used in Standard Red Green Blue (sRGB), which is commonly approximated by a power-law with exponent near $1/2.2$. Real Image Signal Processing (ISP) pipelines include additional steps, but for this problem, the tone map $T_{g_c}$ suffices to capture key nonlinearities.\n\nWe compare two augmentation stages:\n- Pre-ISP jitter: $y^{\\text{pre}}_{c} = T_{g_c}\\big(J_{\\beta,\\kappa}(x)\\big)$.\n- Post-ISP jitter: $y^{\\text{post}}_{c} = J_{\\beta,\\kappa}\\big(T_{g_c}(x)\\big)$.\n\nTo quantify cross-camera invariance, consider a set of cameras with gamma values $\\{g_c\\}_{c=1}^{M}$. For a stage $s \\in \\{\\text{pre}, \\text{post}\\}$ and pixel index $i$, let $\\sigma^{(s)}_i$ be the standard deviation across cameras at pixel $i$:\n$$\n\\sigma^{(s)}_i = \\sqrt{\\frac{1}{M}\\sum_{c=1}^{M}\\left(y^{(s)}_{c,i} - \\bar{y}^{(s)}_{i}\\right)^2}, \\quad \\text{where } \\bar{y}^{(s)}_{i} = \\frac{1}{M}\\sum_{c=1}^{M} y^{(s)}_{c,i}.\n$$\nDefine the discrepancy for stage $s$ as the average of these per-pixel standard deviations:\n$$\nD^{(s)} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\sigma^{(s)}_i.\n$$\nA smaller $D^{(s)}$ indicates greater invariance across cameras. For each test case, determine whether the pre-ISP stage yields lower discrepancy than the post-ISP stage, that is, whether $D^{(\\text{pre})} < D^{(\\text{post})}$.\n\nUse $N = 1024$. Your program must implement the above definitions exactly and evaluate the following test suite, each specified by the set of camera gamma parameters and the jitter parameters $(\\beta,\\kappa)$:\n- Case $1$: $\\{g_c\\} = \\{2.0, 2.2, 2.4\\}$, $(\\beta,\\kappa) = (1.1, 1.2)$.\n- Case $2$: $\\{g_c\\} = \\{2.0, 2.4, 2.8\\}$, $(\\beta,\\kappa) = (1.0, 1.0)$.\n- Case $3$: $\\{g_c\\} = \\{1.6, 2.2, 2.8\\}$, $(\\beta,\\kappa) = (1.5, 1.5)$.\n- Case $4$: $\\{g_c\\} = \\{2.2, 2.2, 2.2\\}$, $(\\beta,\\kappa) = (1.3, 0.7)$.\n\nFor each case, compute $D^{(\\text{pre})}$ and $D^{(\\text{post})}$ as defined, then produce a boolean result $\\mathsf{pre\\_better}$ that is $\\text{True}$ if $D^{(\\text{pre})} < D^{(\\text{post})}$ and $\\text{False}$ otherwise. No physical units are involved; intensities are dimensionless and constrained to $[0,1]$. Angles are not used.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{True},\\text{False},\\text{True},\\text{False}]$), in the order of the test cases from Case $1$ to Case $4$.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of digital image processing, well-posed with a unique and computable solution, and is expressed with objective mathematical formalism. All definitions, constants, and conditions required for a solution are provided and are mutually consistent.\n\nThe task is to determine, for four distinct scenarios, whether applying photometric augmentations before a non-linear tone mapping operation (pre-ISP) results in less variability across different camera models than applying them after (post-ISP). The measure of variability is a discrepancy metric $D^{(s)}$, defined as the average per-pixel standard deviation of the final image intensities across a set of cameras. A lower discrepancy indicates greater invariance.\n\nThe solution proceeds by first algorithmically defining the components of the imaging pipeline and augmentation process, and then assembling them to compute the discrepancy for both pre-ISP and post-ISP stages.\n\n**1. Foundational Definitions and Structures**\n\nThe initial input is a one-dimensional synthetic image $x$, a vector of $N$ samples. With $N=1024$, this vector is defined as a linear ramp:\n$$\nx_i = \\frac{i}{N-1} \\quad \\text{for } i \\in \\{0, 1, \\dots, N-1\\}\n$$\nThis ensures the input intensity values span the full normalized range $[0, 1]$.\n\nAll operations are defined on vectors and are clipped to the valid normalized intensity range $[0, 1]$. The component-wise clipping function is denoted as $\\operatorname{clip}_{[0,1]}(z)$.\n\n**2. Photometric Augmentation Operators**\n\nTwo jitter operators are defined:\n\n- **Brightness Jitter:** This operator, $B_{\\beta}$, scales the image intensities by a factor $\\beta > 0$.\n$$\nB_{\\beta}(z) = \\operatorname{clip}_{[0,1]}(\\beta z)\n$$\n- **Contrast Jitter:** This operator, $C_{\\kappa}$, adjusts the spread of intensities around the image mean, $\\mu_z$, by a factor $\\kappa > 0$.\n$$\nC_{\\kappa}(z) = \\operatorname{clip}_{[0,1]}\\big((z - \\mu_z)\\kappa + \\mu_z\\big), \\quad \\text{where } \\mu_z = \\frac{1}{N}\\sum_{i=0}^{N-1} z_i\n$$\nThe combined jitter operator, $J_{\\beta,\\kappa}$, applies brightness jitter followed by contrast jitter:\n$$\nJ_{\\beta,\\kappa}(x) = C_{\\kappa}(B_{\\beta}(x))\n$$\n\n**3. Camera Tone Mapping**\n\nThe camera's image signal processor (ISP) is modeled by a non-linear tone mapping function, $T_{g_c}$, which performs gamma correction with a camera-specific parameter $g_c > 0$.\n$$\nT_{g_c}(x) = \\operatorname{clip}_{[0,1]}(x)^{1/g_c}\n$$\nThe operation is applied component-wise. The clipping on the input $x$ ensures that the base of the power function is non-negative, though in this problem, the inputs to $T_{g_c}$ are always within the $[0, 1]$ range by prior operations.\n\n**4. Processing Pipelines**\n\nWe evaluate two distinct pipelines for applying augmentations:\n\n- **Pre-ISP Jitter:** Augmentations are performed on the linear raw image $x$. The resulting image is then processed by each camera's tone map. For a camera $c$ with gamma $g_c$, the output image $y^{\\text{pre}}_c$ is:\n$$\ny^{\\text{pre}}_{c} = T_{g_c}\\big(J_{\\beta,\\kappa}(x)\\big)\n$$\n- **Post-ISP Jitter:** The linear raw image $x$ is first processed by each camera's tone map. The augmentations are then applied to each of the resulting display-referred images.\n$$\ny^{\\text{post}}_{c} = J_{\\beta,\\kappa}\\big(T_{g_c}(x)\\big)\n$$\n\nThe core of the problem lies in the non-commutativity of the jitter operator $J_{\\beta,\\kappa}$ and the non-linear tone map $T_{g_c}$.\n\n**5. Discrepancy Calculation**\n\nTo quantify cross-camera invariance for each pipeline stage $s \\in \\{\\text{pre}, \\text{post}\\}$, we first compute the per-pixel standard deviation, $\\sigma^{(s)}_i$, across the set of $M$ cameras.\n$$\n\\sigma^{(s)}_i = \\sqrt{\\frac{1}{M}\\sum_{c=1}^{M}\\left(y^{(s)}_{c,i} - \\bar{y}^{(s)}_{i}\\right)^2}\n$$\nwhere $\\bar{y}^{(s)}_{i} = \\frac{1}{M}\\sum_{c=1}^{M} y^{(s)}_{c,i}$ is the average intensity at pixel $i$ across all cameras. This is the population standard deviation.\n\nThe total discrepancy for stage $s$, $D^{(s)}$, is the average of these standard deviations over all $N$ pixels:\n$$\nD^{(s)} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\sigma^{(s)}_i\n$$\n\n**6. Algorithmic Implementation Plan**\n\nFor each test case, defined by a set of gamma values $\\{g_c\\}_{c=1}^{M}$ and jitter parameters $(\\beta, \\kappa)$:\n\n1.  Generate the initial vector $x$ of size $N=1024$.\n2.  **Calculate $D^{(\\text{pre})}$:**\n    a.  Compute the jittered image $x' = J_{\\beta,\\kappa}(x)$.\n    b.  For each $g_c$, compute $y^{\\text{pre}}_c = T_{g_c}(x')$. This yields a set of $M$ vectors, $\\{y^{\\text{pre}}_c\\}$.\n    c.  Form a matrix of shape $(M, N)$ with these vectors.\n    d.  Compute the standard deviation along the camera axis (axis $0$) to get a vector of $\\sigma^{(\\text{pre})}_i$ values.\n    e.  Compute the mean of this vector to find $D^{(\\text{pre})}$.\n\n3.  **Calculate $D^{(\\text{post})}$:**\n    a.  For each $g_c$, compute the tone-mapped image $x'_c = T_{g_c}(x)$. This yields a set of $M$ vectors, $\\{x'_c\\}$.\n    b.  For each $x'_c$, compute the final image $y^{\\text{post}}_c = J_{\\beta,\\kappa}(x'_c)$.\n    c.  Form a matrix of shape $(M, N)$ with these final vectors, $\\{y^{\\text{post}}_c\\}$.\n    d.  Compute the standard deviation along the camera axis (axis $0$) to obtain $\\sigma^{(\\text{post})}_i$.\n    e.  Compute the mean of this vector to find $D^{(\\text{post})}$.\n\n4.  **Compare and Conclude:** The final result for the test case is the boolean value of the expression $D^{(\\text{pre})} < D^{(\\text{post})}$. The special case where all $g_c$ are identical (Case 4) will yield $D^{(\\text{pre})} = 0$ and $D^{(\\text{post})} = 0$, making the comparison $0 < 0$, which is $\\text{False}$.\n\nThis procedure will be executed for each of the four test cases specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating and comparing discrepancy for pre-ISP and post-ISP augmentations\n    for a given set of test cases.\n    \"\"\"\n\n    N = 1024\n    test_cases = [\n        # Case 1: {g_c} = {2.0, 2.2, 2.4}, (beta, kappa) = (1.1, 1.2)\n        ({'gammas': [2.0, 2.2, 2.4], 'beta': 1.1, 'kappa': 1.2}),\n        # Case 2: {g_c} = {2.0, 2.4, 2.8}, (beta, kappa) = (1.0, 1.0)\n        ({'gammas': [2.0, 2.4, 2.8], 'beta': 1.0, 'kappa': 1.0}),\n        # Case 3: {g_c} = {1.6, 2.2, 2.8}, (beta, kappa) = (1.5, 1.5)\n        ({'gammas': [1.6, 2.2, 2.8], 'beta': 1.5, 'kappa': 1.5}),\n        # Case 4: {g_c} = {2.2, 2.2, 2.2}, (beta, kappa) = (1.3, 0.7)\n        ({'gammas': [2.2, 2.2, 2.2], 'beta': 1.3, 'kappa': 0.7}),\n    ]\n\n    results = []\n\n    # Create the base synthetic image x\n    # x_i = i / (N - 1) for i in {0, ..., N-1}\n    x = np.linspace(0.0, 1.0, N)\n\n    def brightness_jitter(img, beta):\n        \"\"\"Applies brightness jitter B_beta(z) = clip_01(beta * z).\"\"\"\n        return np.clip(beta * img, 0.0, 1.0)\n\n    def contrast_jitter(img, kappa):\n        \"\"\"Applies contrast jitter C_kappa(z).\"\"\"\n        # For a single image (1D array)\n        if img.ndim == 1:\n            mu_z = np.mean(img)\n            return np.clip((img - mu_z) * kappa + mu_z, 0.0, 1.0)\n        # For a batch of images (2D array), operating on each row\n        elif img.ndim == 2:\n            mu_z = np.mean(img, axis=1, keepdims=True)\n            return np.clip((img - mu_z) * kappa + mu_z, 0.0, 1.0)\n        else:\n            raise ValueError(\"Input must be 1D or 2D array.\")\n\n    def combined_jitter(img, beta, kappa):\n        \"\"\"Applies combined jitter J_{beta,kappa}(z) = C_kappa(B_beta(z)).\"\"\"\n        img_bright = brightness_jitter(img, beta)\n        img_contrast = contrast_jitter(img_bright, kappa)\n        return img_contrast\n\n    def tone_map(img, gammas):\n        \"\"\"Applies tone mapping T_g(z) = clip_01(z)^(1/g).\"\"\"\n        # Ensure gammas is a column vector for broadcasting\n        g_vec = np.array(gammas).reshape(-1, 1)\n        # Definition is clip(x)^(1/g), but inputs are already in [0,1].\n        # We clip for strict adherence to the formula.\n        img_clipped = np.clip(img, 0.0, 1.0)\n        return np.power(img_clipped, 1.0 / g_vec)\n\n    for case in test_cases:\n        gammas = case['gammas']\n        beta = case['beta']\n        kappa = case['kappa']\n        M = len(gammas)\n\n        # --- Pre-ISP Jitter Pipeline ---\n        # y_pre = T_g(J(x))\n        # 1. Apply combined jitter to the single raw image x.\n        x_jittered = combined_jitter(x, beta, kappa)\n\n        # 2. Apply tone mapping for all cameras. Broadcasting handles this efficiently.\n        Y_pre = tone_map(x_jittered, gammas) # Shape (M, N)\n\n        # 3. Calculate discrepancy D_pre\n        # Population standard deviation across cameras (axis=0), ddof=0 is default\n        sigma_pre_i = np.std(Y_pre, axis=0, ddof=0)\n        D_pre = np.mean(sigma_pre_i)\n\n        # --- Post-ISP Jitter Pipeline ---\n        # y_post = J(T_g(x))\n        # 1. Apply tone mapping to raw image x for each camera.\n        X_tone_mapped = tone_map(x, gammas) # Shape (M, N)\n        \n        # 2. Apply combined jitter to the batch of M tone-mapped images.\n        Y_post = combined_jitter(X_tone_mapped, beta, kappa) # Shape (M, N)\n\n        # 3. Calculate discrepancy D_post\n        sigma_post_i = np.std(Y_post, axis=0, ddof=0)\n        D_post = np.mean(sigma_post_i)\n\n        # --- Comparison ---\n        pre_is_better = D_pre  D_post\n        results.append(pre_is_better)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3129353"}, {"introduction": "Not all augmentations are created equal; their effectiveness often depends on how well they mimic the variations a model will encounter in the real world. This practice contrasts a generic augmentation, random erasing (or \"cutout\"), with a more structured approach designed to simulate occlusion from other objects whose edges align with our target. Using morphological operations as a simplified proxy for a segmentation model's reasoning, you will quantify how semantically-aware augmentations can lead to superior robustness when reconstructing occluded objects [@problem_id:3129324].", "problem": "You are given a purely synthetic instance segmentation robustness evaluation problem that compares two augmentation-driven proxy models on binary images under controlled occlusions. The task is to implement a program that constructs a ground-truth object mask, applies a specified occluder, simulates two proxy predictors corresponding to two augmentation strategies, and then measures a robustness improvement metric across a test suite of cases.\n\nFundamental base and definitions to use:\n- A binary image is a subset of a discrete grid of pixel coordinates. Let the image size be $H \\times W$ with $H \\in \\mathbb{N}$ and $W \\in \\mathbb{N}$. A foreground mask is represented as a set $S \\subseteq \\{0,1,\\dots,H-1\\} \\times \\{0,1,\\dots,W-1\\}$.\n- The ground-truth object mask $G$ will be an axis-aligned filled rectangle defined by top-left and bottom-right coordinates. All pixels inside that axis-aligned rectangle belong to $G$.\n- An occluder is an axis-aligned filled rectangle $R$ that zeroes out pixels where it overlaps with the object. The observed mask is $M = G \\setminus R$, which corresponds to the visible portion of the object after occlusion.\n- Morphological closing is defined by the standard set operation with a structuring element $B$ as $\\mathrm{Close}(A,B) = (A \\oplus B) \\ominus B$, where $\\oplus$ denotes dilation and $\\ominus$ denotes erosion on binary sets using the footprint $B$. You must implement binary dilation and binary erosion semantics consistent with the discrete grid and footprint convolution.\n- Two proxy predictors simulate models trained with different augmentation distributions:\n  1. Proxy for Random Erasing (cutout-like), denoted as $P_R$: use a square structuring element of side length $k_R$ pixels, $B_R \\in \\{0,1\\}^{k_R \\times k_R}$ with all ones, and compute $P_R = \\mathrm{Close}(M, B_R)$.\n  2. Proxy for Semantic-Aligned Occluders (geometric occlusions whose edges align with object edges), denoted as $P_S$: use two line structuring elements, horizontal $B_{S,h} \\in \\{0,1\\}^{1 \\times k_S}$ and vertical $B_{S,v} \\in \\{0,1\\}^{k_S \\times 1}$ with all ones, and compute $P_S = \\mathrm{Close}(M, B_{S,h}) \\cup \\mathrm{Close}(M, B_{S,v})$.\n- The Intersection over Union (IoU) between any prediction $P$ and $G$ is\n$$\n\\mathrm{IoU}(P,G) = \\frac{|P \\cap G|}{|P \\cup G|},\n$$\nwhere $|\\cdot|$ denotes cardinality of a finite set. The robustness improvement is defined as\n$$\n\\Delta = \\mathrm{IoU}(P_S,G) - \\mathrm{IoU}(P_R,G).\n$$\n\nAngle units: All rectangles are axis-aligned; there are no non-axis angles. No physical units are involved in this problem.\n\nImplement the following procedure for each test case:\n1. Construct a binary image of size $H \\times W$ with $H = 64$ and $W = 64$.\n2. Construct the ground-truth rectangle $G$ given by top-left $(y_0, x_0) = (16,16)$ and bottom-right $(y_1, x_1) = (48,48)$, where indexing is zero-based and ranges are half-open in the natural array sense, that is, all integer coordinates $(y, x)$ with $16 \\le y  48$ and $16 \\le x  48$ belong to $G$.\n3. Apply the specified occluder rectangle $R$ by zeroing the overlapping region to form $M = G \\setminus R$. If an occluder does not intersect $G$, then $M=G$. If an occluder completely covers $G$, then $M = \\varnothing$.\n4. Compute $P_R$ and $P_S$ as above with $k_R = 5$ and $k_S = 7$.\n5. Compute $\\mathrm{IoU}(P_R,G)$ and $\\mathrm{IoU}(P_S,G)$ and then compute $\\Delta$ for the case.\n6. Report $\\Delta$ rounded to $4$ decimal places.\n\nTest suite of occlusion scenarios:\n- Case A (happy path, aligned vertical strip narrower than the semantic line length): occluder rectangle with top-left $(y_0^o, x_0^o) = (20,16)$ and bottom-right $(y_1^o, x_1^o) = (44,22)$, which overlaps the left edge of the object with width $6$ pixels.\n- Case B (central random erasing, hole wider than both structuring elements): occluder rectangle with top-left $(y_0^o, x_0^o) = (27,27)$ and bottom-right $(y_1^o, x_1^o) = (37,37)$, a $10 \\times 10$ central hole.\n- Case C (aligned horizontal strip wider than the semantic line length): occluder rectangle with top-left $(y_0^o, x_0^o) = (16,24)$ and bottom-right $(y_1^o, x_1^o) = (24,40)$, which overlaps the top edge of the object with height $8$ pixels.\n- Case D (no occlusion edge case): no occluder is applied; equivalently, the occluder rectangle is empty.\n- Case E (extreme occlusion, full coverage): occluder rectangle identical to $G$, that is $(16,16)$ to $(48,48)$.\n\nYour program should produce a single line of output containing the robustness improvements for Cases A through E, in that order, as a comma-separated list of floats rounded to $4$ decimal places, enclosed in square brackets. For example, an output line of the correct format is $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D,\\Delta_E]$ with each $\\Delta$ shown to $4$ decimal places and no spaces.", "solution": "The problem requires a computational evaluation of two proxy models for instance segmentation robustness under different occlusion scenarios. The evaluation is performed on a synthetic binary image. The core of the task is to implement the specified image generation pipeline, including morphological operations, and calculate a robustness improvement metric, $\\Delta$.\n\nFirst, we establish the mathematical and computational framework for the problem. The image space is a discrete grid of pixels of size $H \\times W$, where $H=64$ and $W=64$. Binary masks are represented as subsets of the pixel grid $\\{0, 1, \\dots, H-1\\} \\times \\{0, 1, \\dots, W-1\\}$, which are computationally handled as $H \\times W$ boolean arrays.\n\nThe ground-truth object mask, $G$, is an axis-aligned filled rectangle defined by its top-left coordinate $(y_0, x_0) = (16, 16)$ and bottom-right coordinate $(y_1, x_1) = (48, 48)$. The set of pixels $(y, x)$ in $G$ is given by $G = \\{(y,x) \\in \\mathbb{Z}^2 \\mid 16 \\le y  48 \\text{ and } 16 \\le x  48\\}$. The total number of pixels in $G$ is $|G| = (48-16) \\times (48-16) = 32 \\times 32 = 1024$.\n\nAn occlusion is modeled by another axis-aligned rectangle, $R$. The observed mask, $M$, which represents the visible portion of the object, is the set difference $M = G \\setminus R$. In terms of boolean arrays, this corresponds to a logical AND operation between the ground-truth mask array and the negation of the occluder mask array.\n\nThe core of the predictive models lies in morphological operations. For a binary image $A$ and a structuring element $B$, both represented as sets of pixel coordinates, binary dilation and erosion are defined as:\n- Dilation: $A \\oplus B = \\bigcup_{a \\in A, b \\in B} \\{a+b\\}$. This operation expands the boundaries of foreground regions.\n- Erosion: $A \\ominus B = \\{z \\mid z+b \\in A \\text{ for all } b \\in B\\}$. This operation shrinks the boundaries of foreground regions.\nThese operations will be implemented using the `scipy.ndimage.binary_dilation` and `scipy.ndimage.binary_erosion` functions, which provide efficient and correct implementations.\n\nThe morphological closing of a set $A$ with a structuring element $B$ is defined as a dilation followed by an erosion: $\\mathrm{Close}(A,B) = (A \\oplus B) \\ominus B$. This operation is effective at filling small holes and closing narrow gaps in the foreground object.\n\nTwo proxy models for predictors trained with different augmentation strategies are defined:\n\n1.  Predictor $P_R$, proxy for Random Erasing: This model uses a morphological closing with a square structuring element $B_R$. $B_R$ is an all-ones matrix of size $k_R \\times k_R$, where $k_R=5$. The prediction is $P_R = \\mathrm{Close}(M, B_R)$. This model is expected to be effective at in-painting small, isotropic (compact) holes.\n\n2.  Predictor $P_S$, proxy for Semantic-Aligned Occluders: This model combines closings using two linear structuring elements: a horizontal line $B_{S,h}$ of size $1 \\times k_S$ and a vertical line $B_{S,v}$ of size $k_S \\times 1$, with $k_S=7$. The prediction is the union of the results from these two closing operations: $P_S = \\mathrm{Close}(M, B_{S,h}) \\cup \\mathrm{Close}(M, B_{S,v})$. This model is hypothesized to be better at reconstructing parts of an object occluded by thin, elongated shapes that align with the object's primary axes.\n\nThe performance of these predictors is quantified using the Intersection over Union (IoU) metric, calculated with respect to the ground-truth mask $G$:\n$$\n\\mathrm{IoU}(P,G) = \\frac{|P \\cap G|}{|P \\cup G|}\n$$\nFor boolean arrays `P_arr` and `G_arr`, this is computed as `np.sum(P_arr  G_arr) / np.sum(P_arr | G_arr)`.\n\nThe final metric to be reported is the robustness improvement, $\\Delta$, defined as the difference in IoU scores between the two predictors:\n$$\n\\Delta = \\mathrm{IoU}(P_S,G) - \\mathrm{IoU}(P_R,G)\n$$\nA positive $\\Delta$ indicates that the semantic-aligned predictor $P_S$ provides a better reconstruction of the occluded object than the random-erasing predictor $P_R$.\n\nThe procedure for each test case is as follows:\n1.  Define the image dimensions $H=64, W=64$. Create the ground-truth mask $G$ as a $64 \\times 64$ boolean array.\n2.  For the specific case, create the occluder mask $R$ as a boolean array. Handle the no-occlusion case by using an empty mask for $R$.\n3.  Compute the observed mask $M$ by applying the occlusion: `M = G  (~R)`.\n4.  Define the structuring elements $B_R$, $B_{S,h}$, and $B_{S,v}$ as boolean NumPy arrays of the specified dimensions.\n5.  Compute the prediction $P_R$ by dilating $M$ with $B_R$, then eroding the result with $B_R$.\n6.  Compute the prediction $P_S$ by first calculating $P_{S,h} = \\mathrm{Close}(M, B_{S,h})$ and $P_{S,v} = \\mathrm{Close}(M, B_{S,v})$, and then taking their logical union `P_Sh | P_Sv`.\n7.  Calculate $\\mathrm{IoU}(P_R,G)$ and $\\mathrm{IoU}(P_S,G)$.\n8.  Compute $\\Delta = \\mathrm{IoU}(P_S,G) - \\mathrm{IoU}(P_R,G)$.\n9.  Round the final value of $\\Delta$ to $4$ decimal places.\n\nThis process is repeated for each of the five specified occlusion scenarios (Cases A-E), and the results are aggregated into the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import binary_erosion, binary_dilation\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline to evaluate robustness of two proxy models\n    under various occlusion scenarios and computes the robustness improvement metric.\n    \"\"\"\n    H, W = 64, 64\n    k_R, k_S = 5, 7\n\n    # Define the ground-truth object mask G\n    G = np.zeros((H, W), dtype=bool)\n    G[16:48, 16:48] = True\n\n    # Define structuring elements for morphological operations\n    se_R = np.ones((k_R, k_R), dtype=bool)\n    se_S_h = np.ones((1, k_S), dtype=bool)\n    se_S_v = np.ones((k_S, 1), dtype=bool)\n\n    # Test suite of occlusion scenarios\n    test_cases = [\n        # Case A: Aligned vertical strip\n        {'name': 'A', 'rect': (20, 16, 44, 22)},\n        # Case B: Central random erasing\n        {'name': 'B', 'rect': (27, 27, 37, 37)},\n        # Case C: Aligned horizontal strip\n        {'name': 'C', 'rect': (16, 24, 24, 40)},\n        # Case D: No occlusion\n        {'name': 'D', 'rect': None},\n        # Case E: Extreme occlusion (full coverage)\n        {'name': 'E', 'rect': (16, 16, 48, 48)},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # 1. Construct the occluder mask R\n        R = np.zeros((H, W), dtype=bool)\n        if case['rect']:\n            y0, x0, y1, x1 = case['rect']\n            R[y0:y1, x0:x1] = True\n\n        # 2. Compute the observed mask M\n        M = G  (~R)\n\n        # 3. Compute P_R prediction\n        # P_R = Close(M, B_R)\n        dilated_M_R = binary_dilation(M, structure=se_R)\n        P_R = binary_erosion(dilated_M_R, structure=se_R)\n\n        # 4. Compute P_S prediction\n        # P_S = Close(M, B_S_h) U Close(M, B_S_v)\n        dilated_M_Sh = binary_dilation(M, structure=se_S_h)\n        closed_M_Sh = binary_erosion(dilated_M_Sh, structure=se_S_h)\n        \n        dilated_M_Sv = binary_dilation(M, structure=se_S_v)\n        closed_M_Sv = binary_erosion(dilated_M_Sv, structure=se_S_v)\n        \n        P_S = closed_M_Sh | closed_M_Sv\n\n        # 5. Compute IoU for both predictors\n        def iou(P, G_mask):\n            intersection = np.sum(P  G_mask)\n            union = np.sum(P | G_mask)\n            if union == 0:\n                # This case implies both P and G are empty.\n                # In this problem, G is never empty, so union is only 0 if P is also G,\n                # which isn't the case for empty masks. However, for robustness:\n                return 1.0 if intersection == 0 else 0.0\n            return intersection / union\n\n        iou_R = iou(P_R, G)\n        iou_S = iou(P_S, G)\n        \n        # 6. Compute robustness improvement Delta\n        delta = iou_S - iou_R\n        \n        # 7. Round and store the result\n        results.append(round(delta, 4))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3129324"}, {"introduction": "Manually designing an optimal augmentation strategy can be challenging, leading to a powerful idea: can the model learn its own augmentations? In this advanced practice, you will implement a system where a small neural network, trained jointly with the main classifier, learns to generate custom photometric transformations for each input image. This exercise provides hands-on experience with data-dependent augmentation policies and reveals both their potential to improve generalization and the risks of failure modes like \"policy collapse\" [@problem_id:3129310].", "problem": "You are given a binary classification task over synthetic images with photometric transformations. Each input image is a matrix of size $H \\times W$ with pixel intensities in $[0,1]$, flattened to a vector $x \\in \\mathbb{R}^{D}$ with $D = H \\cdot W$. The label is $y \\in \\{0,1\\}$. The fundamental base for this task consists of the following well-tested facts and core definitions:\n- Empirical Risk Minimization (ERM): given a model with parameters, the empirical risk over a dataset is the average loss over the samples and the training procedure aims to minimize this risk.\n- Binary logistic regression: for an input $x$, the classifier uses parameters $(w,b)$ with $w \\in \\mathbb{R}^{D}$ and $b \\in \\mathbb{R}$, computes the logit $z = w^{\\top} x + b$, and predicts $\\hat{p} = \\sigma(z)$ where $\\sigma(\\cdot)$ is the logistic sigmoid function $\\sigma(t) = \\frac{1}{1 + e^{-t}}$. The binary cross-entropy loss for a label $y \\in \\{0,1\\}$ is $\\ell(\\hat{p}, y) = -\\left(y \\log(\\hat{p}) + (1-y)\\log(1-\\hat{p})\\right)$.\n- Photometric augmentation: a brightness and contrast transformation of an image is modeled as $x \\mapsto \\mathrm{clip}(\\alpha x + \\beta, 0, 1)$, where $\\alpha \\in \\mathbb{R}$ is a contrast scaling and $\\beta \\in \\mathbb{R}$ is a brightness shift, and $\\mathrm{clip}$ clamps each component to the interval $[0,1]$.\n- Data-dependent augmentation via a learnable Multilayer Perceptron (MLP): for an input $x$, a small Multilayer Perceptron (MLP) with parameters $\\phi$ outputs photometric parameters $\\theta(x) = (\\alpha(x), \\beta(x))$. The MLP consists of one hidden layer with hyperbolic tangent activation. The outputs are passed through hyperbolic tangent functions and scaled to respect target ranges.\n\nThe program you will implement must:\n1. Construct a synthetic dataset. Class $0$ images contain a Gaussian blob centered near the left side of the image and class $1$ images contain a Gaussian blob near the right side, each with small random shifts and additive noise. Use $H = 16$, $W = 16$, and generate $N_{\\mathrm{train}} = 120$ training samples and $N_{\\mathrm{val}} = 120$ validation samples. Pixel intensities must lie in $[0,1]$.\n2. Define three augmentation modes for training:\n   - No augmentation: $x \\mapsto x$.\n   - Data-independent random photometric augmentation: $x \\mapsto \\mathrm{clip}(\\alpha x + \\beta, 0, 1)$ with $\\alpha$ sampled uniformly from $[1 - a_{\\mathrm{max}}, 1 + a_{\\mathrm{max}}]$ and $\\beta$ sampled uniformly from $[-b_{\\mathrm{max}}, b_{\\mathrm{max}}]$ independently per sample and per training step.\n   - Data-dependent learned augmentation: an MLP maps $x$ to parameters $\\theta(x) = (\\alpha(x), \\beta(x))$. The MLP has input dimension $D$, hidden dimension $h$, and output dimension $2$. Let the hidden pre-activation be $u = W_1 x + b_1$, hidden activation $h(x) = \\tanh(u)$, output pre-activation $o = W_2 h(x) + b_2$, and define\n   $$\n   \\alpha(x) = 1 + a_{\\mathrm{scale}} \\tanh(o_0), \\quad \\beta(x) = b_{\\mathrm{scale}} \\tanh(o_1).\n   $$\n   The transformed input is $x' = \\mathrm{clip}(\\alpha(x)\\, x + \\beta(x), 0, 1)$. To discourage extreme transformations, include a regularization term in the objective proportional to $((\\alpha(x) - 1)^2 + \\beta(x)^2)$ with coefficient $\\lambda_{\\mathrm{reg}} \\geq 0$.\n3. Train a binary logistic regression classifier $(w,b)$ using full-batch gradient descent for a fixed number of steps $T$. In the learned augmentation mode, jointly update the MLP parameters by differentiating through the photometric transformation via the chain rule. Treat $\\mathrm{clip}$ as having zero derivative outside the open interval $(0,1)$ and standard derivative inside.\n4. After training, evaluate the classifier without augmentation on the validation set to measure generalization. Compute:\n   - Training accuracy as the fraction of correctly classified training samples with a decision threshold of $0.5$ on $\\hat{p}$.\n   - Validation accuracy similarly on the validation set without augmentation.\n   - Generalization gap defined as training accuracy minus validation accuracy (a float).\n   - Collapse indicator defined as follows: for the learned augmentation mode, compute $x'_i$ for each training sample under the final MLP parameters. Let $s_i$ be the standard deviation across pixels of $x'_i$. Compute the mean $\\bar{s}$ over all training samples. Declare collapse if $\\bar{s}  s_{\\mathrm{thr}}$, where $s_{\\mathrm{thr}}$ is a fixed threshold. For non-learned modes, set collapse to $0$. The collapse indicator must be an integer: $1$ if collapsed, $0$ otherwise. No physical units apply; all quantities are unitless.\n\nYour implementation must use the following test suite with four parameter settings to explore happy path, boundary, and edge cases:\n- Case $1$ (Baseline, no augmentation):\n  - Mode: none.\n  - $a_{\\mathrm{scale}} = 0$, $b_{\\mathrm{scale}} = 0$, $\\lambda_{\\mathrm{reg}} = 0$, $T = 100$, hidden dimension $h = 8$, $s_{\\mathrm{thr}} = 0.05$.\n- Case $2$ (Random augmentation, moderate ranges):\n  - Mode: random.\n  - $a_{\\mathrm{max}} = 0.2$, $b_{\\mathrm{max}} = 0.1$, $T = 100$, $s_{\\mathrm{thr}} = 0.05$.\n- Case $3$ (Learned augmentation, strong regularization):\n  - Mode: learned.\n  - $a_{\\mathrm{scale}} = 0.3$, $b_{\\mathrm{scale}} = 0.15$, $\\lambda_{\\mathrm{reg}} = 1.0$, $T = 100$, hidden dimension $h = 8$, $s_{\\mathrm{thr}} = 0.05$.\n- Case $4$ (Learned augmentation, weak regularization and large ranges — edge case for collapse):\n  - Mode: learned.\n  - $a_{\\mathrm{scale}} = 1.0$, $b_{\\mathrm{scale}} = 0.5$, $\\lambda_{\\mathrm{reg}} = 0.0$, $T = 100$, hidden dimension $h = 8$, $s_{\\mathrm{thr}} = 0.05$.\n\nFor each case, your program must output a list containing four values in the following order: validation accuracy (float), training accuracy (float), generalization gap (float), collapse indicator (integer). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[val_acc1,train_acc1,gen_gap1,collapse1],[val_acc2,train_acc2,gen_gap2,collapse2],...]\"). No other text should be printed.", "solution": "The problem is valid as it is scientifically grounded in established machine learning principles, well-posed with all necessary parameters defined, and objective in its formulation. The task is to implement and evaluate three different data augmentation strategies for a binary logistic regression classifier on a synthetic dataset. The solution is designed as a self-contained numerical experiment structured into four main components: dataset generation, model implementation, training via gradient descent, and evaluation.\n\n### 1. Synthetic Dataset Generation\nA synthetic dataset is constructed to represent a simple binary classification problem. Images are of size $H \\times W = 16 \\times 16$, flattened into vectors $x \\in \\mathbb{R}^{256}$. Pixel intensities are in $[0, 1]$. We generate $N_{\\mathrm{train}} = 120$ training samples and $N_{\\mathrm{val}} = 120$ validation samples, with balanced classes.\n- **Class $0$ ($y=0$)**: An image consists of a two-dimensional Gaussian blob centered near the left side of the image, at approximately $(c_x, c_y) = (W/4, H/2) = (4, 8)$.\n- **Class $1$ ($y=1$)**: An image consists of a Gaussian blob centered near the right side, at approximately $(c_x, c_y) = (3W/4, H/2) = (12, 8)$.\nEach blob's center is perturbed by small random shifts to introduce variability. The intensity at a pixel with coordinates $(i, j)$ for a blob centered at $(c_x, c_y)$ is given by $I(i,j) = \\exp\\left(-\\frac{(i-c_y)^2 + (j-c_x)^2}{2\\sigma^2}\\right)$, with a fixed standard deviation $\\sigma$. Additive Gaussian noise is then applied, and the final pixel values are clipped to the range $[0, 1]$.\n\n### 2. Model Architectures\nTwo main models are used: a logistic regression classifier and a learnable augmentation network.\n\n- **Logistic Regression Classifier**: The classifier has parameters $(w, b)$, where $w \\in \\mathbb{R}^{D}$ and $b \\in \\mathbb{R}$. For an input vector $x'$, it computes a logit $z = x'^{\\top}w + b$ and predicts the class probability $\\hat{p} = \\sigma(z)$, where $\\sigma(t) = 1 / (1+e^{-t})$ is the sigmoid function. The parameters are initialized to zero.\n\n- **Learnable Augmentation Network**: For the `learned` mode, a Multi-Layer Perceptron (MLP) with one hidden layer generates the photometric transformation parameters $(\\alpha(x), \\beta(x))$ for a given input image $x$.\n    - The MLP has an input layer of size $D=256$, a hidden layer of size $h=8$ with a hyperbolic tangent ($\\tanh$) activation function, and an output layer of size $2$. Let the MLP parameters be $\\phi = \\{W_1, b_1, W_2, b_2\\}$.\n    - Hidden layer: $h(x) = \\tanh(W_1^{\\top}x + b_1)$.\n    - Output logits: $o(x) = W_2^{\\top}h(x) + b_2$.\n    - The transformation parameters are derived from the output logits $o = (o_0, o_1)$:\n      $$\n      \\alpha(x) = 1 + a_{\\mathrm{scale}} \\tanh(o_0)\n      $$\n      $$\n      \\beta(x) = b_{\\mathrm{scale}} \\tanh(o_1)\n      $$\n    - The transformed image is then $x' = \\mathrm{clip}(\\alpha(x)x + \\beta(x), 0, 1)$. MLP weights are initialized using Glorot (Xavier) initialization.\n\n### 3. Training by Full-Batch Gradient Descent\nThe models are trained for $T=100$ steps using full-batch gradient descent. The objective function depends on the augmentation mode. For all modes, the primary loss component is the mean binary cross-entropy (BCE) over the training batch:\n$$\n\\mathcal{L}_{\\mathrm{BCE}} = -\\frac{1}{N_{\\mathrm{train}}} \\sum_{i=1}^{N_{\\mathrm{train}}} \\left[ y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i) \\right]\n$$\nwhere $\\hat{p}_i$ is the prediction for the $i$-th (potentially augmented) sample $x'_i$.\n\n- **Mode `none`**: $x' = x$. The classifier parameters $(w, b)$ are updated to minimize $\\mathcal{L}_{\\mathrm{BCE}}$.\n- **Mode `random`**: For each sample $x_i$ at each training step, a new transformation $x'_i = \\mathrm{clip}(\\alpha_i x_i + \\beta_i, 0, 1)$ is applied, with $\\alpha_i \\sim U(1-a_{\\mathrm{max}}, 1+a_{\\mathrm{max}})$ and $\\beta_i \\sim U(-b_{\\mathrm{max}}, b_{\\mathrm{max}})$. The classifier parameters are updated to minimize $\\mathcal{L}_{\\mathrm{BCE}}$ on the augmented batch.\n- **Mode `learned`**: The objective function includes a regularization term to penalize extreme transformations:\n$$\n\\mathcal{L} = \\mathcal{L}_{\\mathrm{BCE}} + \\mathcal{L}_{\\mathrm{reg}} = \\mathcal{L}_{\\mathrm{BCE}} + \\frac{\\lambda_{\\mathrm{reg}}}{N_{\\mathrm{train}}} \\sum_{i=1}^{N_{\\mathrm{train}}} \\left( (\\alpha(x_i) - 1)^2 + \\beta(x_i)^2 \\right)\n$$\nBoth the classifier parameters $(w, b)$ and the MLP parameters $\\phi = \\{W_1, b_1, W_2, b_2\\}$ are updated jointly by descending the gradient of $\\mathcal{L}$. The gradients are computed using the chain rule (backpropagation), differentiating through the entire computational graph, including the clipping function (treated as having a derivative of $1$ on $(0, 1)$ and $0$ otherwise).\n\nThe gradient of the total loss $\\mathcal{L}$ with respect to the classifier parameters $(w, b)$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{N_{\\mathrm{train}}} X'^{\\top}(\\hat{p} - Y) \\quad\\quad \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{N_{\\mathrm{train}}} \\sum_{i=1}^{N_{\\mathrm{train}}} (\\hat{p}_i - y_i)\n$$\nwhere $X'$ is the matrix of augmented training samples. For the learned mode, the gradients with respect to the MLP parameters $\\phi$ are calculated by continuing the backpropagation from the gradients on $\\alpha$ and $\\beta$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial x'_i} \\cdot \\frac{\\partial x'_i}{\\partial \\alpha_i} \\right) + \\frac{\\partial \\mathcal{L}_{\\mathrm{reg}}}{\\partial \\alpha_i} \\quad\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\beta_i} = \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial x'_i} \\cdot \\frac{\\partial x'_i}{\\partial \\beta_i} \\right) + \\frac{\\partial \\mathcal{L}_{\\mathrm{reg}}}{\\partial \\beta_i}\n$$\nThese gradients are then propagated back through the MLP to find $\\frac{\\partial \\mathcal{L}}{\\partial \\phi}$.\n\n### 4. Evaluation\nAfter training, the classifier's performance is assessed using four metrics. All accuracy evaluations are performed on the original, unaugmented data.\n\n- **Training Accuracy**: The fraction of correctly classified samples in the training set $X_{\\mathrm{train}}$, using a decision threshold of $0.5$.\n- **Validation Accuracy**: The fraction of correctly classified samples in the validation set $X_{\\mathrm{val}}$.\n- **Generalization Gap**: The difference between training accuracy and validation accuracy. A smaller gap suggests better generalization.\n- **Collapse Indicator**: For the `learned` mode, this metric quantifies a failure mode where the augmentation learns to \"erase\" the input image features, typically to minimize training loss. For each training sample $x_i$, the final learned transformation is applied to get $x'_i$. The standard deviation of pixels $s_i = \\mathrm{std}(x'_i)$ is computed. If the mean of these standard deviations, $\\bar{s} = \\frac{1}{N_{\\mathrm{train}}} \\sum_i s_i$, falls below a threshold $s_{\\mathrm{thr}}=0.05$, the collapse indicator is set to $1$; otherwise, it is $0$. For `none` and `random` modes, the collapse indicator is always $0$.\n\nThe implementation will execute these steps for each of the four specified test cases and report the resulting metrics in the required format. A fixed random seed ensures reproducibility.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified experiments and print results.\n    \"\"\"\n    # Define constants\n    H, W = 16, 16\n    D = H * W\n    N_TRAIN, N_VAL = 120, 120\n    RANDOM_SEED = 42\n\n    def generate_dataset(n_samples, h_dim, w_dim, seed_offset=0):\n        \"\"\"Generates a synthetic dataset of Gaussian blobs.\"\"\"\n        np.random.seed(RANDOM_SEED + seed_offset)\n        d_dim = h_dim * w_dim\n        x_data = np.zeros((n_samples, d_dim))\n        y_data = np.zeros((n_samples, 1))\n        \n        xx, yy = np.meshgrid(np.arange(w_dim), np.arange(h_dim))\n        \n        for i in range(n_samples):\n            y_data[i] = i % 2\n            if y_data[i] == 0: # Class 0: blob on the left\n                center_x = w_dim / 4.0\n                center_y = h_dim / 2.0\n            else: # Class 1: blob on the right\n                center_x = 3 * w_dim / 4.0\n                center_y = h_dim / 2.0\n            \n            shift_x = np.random.uniform(-w_dim / 8.0, w_dim / 8.0)\n            shift_y = np.random.uniform(-h_dim / 8.0, h_dim / 8.0)\n            \n            cx, cy = center_x + shift_x, center_y + shift_y\n            \n            sigma = 2.0\n            img = np.exp(-((xx - cx)**2 + (yy - cy)**2) / (2 * sigma**2))\n            noise = np.random.normal(0, 0.1, img.shape)\n            img += noise\n            \n            x_data[i, :] = np.clip(img, 0, 1).flatten()\n            \n        return x_data, y_data\n\n    def sigmoid(z):\n        \"\"\"Computes the sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def evaluate(X, Y, w, b):\n        \"\"\"Computes classification accuracy.\"\"\"\n        z = X @ w + b\n        p_hat = sigmoid(z)\n        preds = (p_hat > 0.5).astype(int)\n        accuracy = np.mean(preds == Y)\n        return accuracy\n\n    def run_experiment(params):\n        \"\"\"Runs a single experiment based on the given parameters.\"\"\"\n        np.random.seed(RANDOM_SEED)\n\n        # Generate data\n        X_train, Y_train = generate_dataset(N_TRAIN, H, W, seed_offset=0)\n        X_val, Y_val = generate_dataset(N_VAL, H, W, seed_offset=1)\n\n        # Unpack params\n        mode = params['mode']\n        T = params.get('T', 100)\n        h_dim = params.get('h', 8)\n        s_thr = params.get('s_thr', 0.05)\n        \n        # Hyperparameters\n        lr_clf = 0.5\n        lr_aug = 0.01\n\n        # Initialize models\n        w = np.zeros((D, 1))\n        b = 0.0\n        phi = {}\n        if mode == 'learned':\n            # Glorot initialization\n            phi['W1'] = np.random.randn(D, h_dim) * np.sqrt(1. / D)\n            phi['b1'] = np.zeros(h_dim)\n            phi['W2'] = np.random.randn(h_dim, 2) * np.sqrt(1. / h_dim)\n            phi['b2'] = np.zeros(2)\n\n        # Training loop\n        for _ in range(T):\n            X_train_aug = X_train\n            alpha, beta, pre_clip, h_act = None, None, None, None\n\n            if mode == 'random':\n                a_max = params['a_max']\n                b_max = params['b_max']\n                alpha_rand = np.random.uniform(1 - a_max, 1 + a_max, (N_TRAIN, 1))\n                beta_rand = np.random.uniform(-b_max, b_max, (N_TRAIN, 1))\n                X_train_aug = np.clip(alpha_rand * X_train + beta_rand, 0, 1)\n            \n            elif mode == 'learned':\n                a_scale = params['a_scale']\n                b_scale = params['b_scale']\n                lambda_reg = params['lambda_reg']\n\n                # Augmenter forward pass\n                u = X_train @ phi['W1'] + phi['b1']\n                h_act = np.tanh(u)\n                o = h_act @ phi['W2'] + phi['b2']\n                o0, o1 = o[:, 0:1], o[:, 1:2]\n                tanh_o0, tanh_o1 = np.tanh(o0), np.tanh(o1)\n                \n                alpha = 1 + a_scale * tanh_o0\n                beta = b_scale * tanh_o1\n                \n                pre_clip = alpha * X_train + beta\n                X_train_aug = np.clip(pre_clip, 0, 1)\n\n            # Classifier forward pass\n            z = X_train_aug @ w + b\n            p_hat = sigmoid(z)\n            eps = 1e-9\n\n            # Loss\n            bce_loss = -np.mean(Y_train * np.log(p_hat + eps) + (1 - Y_train) * np.log(1 - p_hat + eps))\n            \n            # Gradients for classifier\n            grad_z = (p_hat - Y_train) / N_TRAIN\n            grad_w = X_train_aug.T @ grad_z\n            grad_b = np.sum(grad_z)\n            \n            # Update classifier\n            w -= lr_clf * grad_w\n            b -= lr_clf * grad_b\n\n            if mode == 'learned':\n                # Gradients for augmenter\n                grad_X_aug = grad_z @ w.T\n                dclip_mask = (pre_clip > 0)  (pre_clip  1)\n                grad_pre_clip = grad_X_aug * dclip_mask\n\n                grad_alpha_bce = np.sum(grad_pre_clip * X_train, axis=1, keepdims=True)\n                grad_beta_bce = np.sum(grad_pre_clip, axis=1, keepdims=True)\n\n                grad_alpha_reg = lambda_reg * (2 * (alpha - 1)) / N_TRAIN\n                grad_beta_reg = lambda_reg * (2 * beta) / N_TRAIN\n\n                grad_alpha = grad_alpha_bce + grad_alpha_reg\n                grad_beta = grad_beta_bce + grad_beta_reg\n\n                # Backprop through MLP\n                grad_o0 = grad_alpha * a_scale * (1 - tanh_o0**2)\n                grad_o1 = grad_beta * b_scale * (1 - tanh_o1**2)\n                grad_o = np.hstack([grad_o0, grad_o1])\n\n                grad_W2 = h_act.T @ grad_o\n                grad_b2 = np.sum(grad_o, axis=0)\n                grad_h_act = grad_o @ phi['W2'].T\n\n                grad_u = grad_h_act * (1 - h_act**2)\n\n                grad_W1 = X_train.T @ grad_u\n                grad_b1 = np.sum(grad_u, axis=0)\n                \n                # Update augmenter\n                phi['W1'] -= lr_aug * grad_W1\n                phi['b1'] -= lr_aug * grad_b1\n                phi['W2'] -= lr_aug * grad_W2\n                phi['b2'] -= lr_aug * grad_b2\n                \n        # Evaluation\n        train_acc = evaluate(X_train, Y_train, w, b)\n        val_acc = evaluate(X_val, Y_val, w, b)\n        gen_gap = train_acc - val_acc\n        \n        collapse_indicator = 0\n        if mode == 'learned':\n            u = X_train @ phi['W1'] + phi['b1']\n            h_act = np.tanh(u)\n            o = h_act @ phi['W2'] + phi['b2']\n            alpha = 1 + params['a_scale'] * np.tanh(o[:, 0:1])\n            beta = params['b_scale'] * np.tanh(o[:, 1:2])\n            X_train_final_aug = np.clip(alpha * X_train + beta, 0, 1)\n            \n            s = np.std(X_train_final_aug, axis=1)\n            s_mean = np.mean(s)\n            \n            if s_mean  s_thr:\n                collapse_indicator = 1\n\n        return [val_acc, train_acc, gen_gap, collapse_indicator]\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'mode': 'none', 'T': 100, 'h': 8, 's_thr': 0.05},\n        {'mode': 'random', 'a_max': 0.2, 'b_max': 0.1, 'T': 100, 's_thr': 0.05},\n        {'mode': 'learned', 'a_scale': 0.3, 'b_scale': 0.15, 'lambda_reg': 1.0, 'T': 100, 'h': 8, 's_thr': 0.05},\n        {'mode': 'learned', 'a_scale': 1.0, 'b_scale': 0.5, 'lambda_reg': 0.0, 'T': 100, 'h': 8, 's_thr': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_experiment(params)\n        results.append(result)\n\n    # Format output\n    output_str = ','.join([f\"[{','.join(map(str, r))}]\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3129310"}]}