{"hands_on_practices": [{"introduction": "A critical aspect of implementing normalization layers is ensuring numerical stability during training. As the variance of a group of activations approaches zero, the standard normalization formula involves division by a very small number, which can lead to exploding gradients during backpropagation. This exercise guides you through a rigorous mathematical analysis to understand this phenomenon, revealing the crucial role of the small constant $\\epsilon$ in stabilizing the learning process. Mastering this concept is essential for diagnosing training issues and building robust deep learning models.", "problem": "Consider a single training sample and a single group in Group Normalization (GN) with group size $m \\geq 2$. Let the group activations be $x_{1}, \\dots, x_{m} \\in \\mathbb{R}$, their group mean be $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}$, and their group variance be $\\sigma_{g}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_{i} - \\mu)^{2}$. GN produces normalized activations $\\hat{x}_{i} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$ and outputs $y_{i} = \\gamma \\hat{x}_{i} + \\beta$, where $\\gamma \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are learnable parameters shared across the $m$ elements of the group. Let the scalar loss be $L(y_{1}, \\dots, y_{m})$. Assume the incoming gradients satisfy a uniform bound $| \\frac{\\partial L}{\\partial y_{i}} | \\leq G$ for all $i \\in \\{1,\\dots,m\\}$, where $G > 0$ is a given constant.\n\nUsing only the definitions above and basic multivariable calculus (chain rule) as the fundamental base:\n\n1) Derive the limiting form of the gradient $\\frac{\\partial L}{\\partial x_{i}}$ as $\\sigma_{g}^{2} \\to 0$, expressed in terms of $\\gamma$, $\\epsilon$, and the incoming gradients.\n\n2) From that limiting form, derive the tightest possible upper bound on $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$ in terms of $m$, $\\gamma$, $\\epsilon$, and $G$ when $\\sigma_{g}^{2} \\to 0$.\n\n3) Given a target gradient magnitude threshold $\\tau > 0$, solve for the smallest $\\epsilon$ that guarantees $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$ in the limit $\\sigma_{g}^{2} \\to 0$.\n\n4) Propose one adaptive strategy for selecting $\\epsilon$ as a function of $\\sigma_{g}^{2}$ and other problem parameters that preserves numerical stability as $\\sigma_{g}^{2} \\to 0$, and explain why it works.\n\nYour final answer must be only the closed-form expression for the minimal $\\epsilon$ from part 3). No units are needed. If your answer involves constants, express them symbolically. Do not round.", "solution": "The solution is a four-part derivation and analysis based on the principles of multivariable calculus applied to the Group Normalization (GN) function.\n\n### Part 1: Derivation of the limiting form of the gradient $\\frac{\\partial L}{\\partial x_{i}}$\n\nThe total derivative of the loss $L$ with respect to an input activation $x_i$ is found using the multivariable chain rule, summing over all outputs $y_j$ that depend on $x_i$:\n$$ \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{i}} $$\nThe output $y_j$ is defined as $y_{j} = \\gamma \\hat{x}_{j} + \\beta$, where $\\hat{x}_{j} = \\frac{x_{j} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$. Therefore, $\\frac{\\partial y_{j}}{\\partial x_{i}} = \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$.\n\nTo find $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$, we must consider that both $\\mu$ and $\\sigma_g^2$ are functions of all $x_k$ in the group. The partial derivatives of the mean $\\mu$ and variance $\\sigma_g^2$ with respect to $x_i$ are:\n$$ \\frac{\\partial \\mu}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_{k} \\right) = \\frac{1}{m} $$\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} (x_{k} - \\mu)^{2} \\right) = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_{k} - \\mu) \\left( \\frac{\\partial x_{k}}{\\partial x_{i}} - \\frac{\\partial \\mu}{\\partial x_{i}} \\right) $$\n$$ = \\frac{2}{m} \\sum_{k=1}^{m} (x_{k} - \\mu) ( \\delta_{ki} - \\frac{1}{m} ) = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) - \\frac{1}{m}\\sum_{k \\neq i} (x_k - \\mu) \\right) $$\nUsing the property $\\sum_{k=1}^{m} (x_{k} - \\mu) = 0$, we have $\\sum_{k \\neq i} (x_{k} - \\mu) = -(x_i - \\mu)$. Substituting this gives:\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) + \\frac{1}{m}(x_i - \\mu) \\right) = \\frac{2}{m} (x_i - \\mu) $$\nNow, we apply the quotient rule to $\\hat{x}_j = \\frac{u}{v}$ where $u = x_j - \\mu$ and $v = \\sqrt{\\sigma_g^2 + \\epsilon}$.\n$$ \\frac{\\partial u}{\\partial x_i} = \\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m} $$\n$$ \\frac{\\partial v}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{\\partial \\sigma_g^2}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{2}{m}(x_i - \\mu) = \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}} $$\nThe derivative $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{v \\frac{\\partial u}{\\partial x_i} - u \\frac{\\partial v}{\\partial x_i}}{v^2}$ is:\n$$ \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{\\sqrt{\\sigma_g^2 + \\epsilon}(\\delta_{ij} - \\frac{1}{m}) - (x_j-\\mu) \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}}}{\\sigma_g^2 + \\epsilon} = \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) - \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} $$\nWe are interested in the limit as $\\sigma_g^2 \\to 0$. This condition implies that $(x_k - \\mu) \\to 0$ for all $k \\in \\{1, \\dots, m\\}$. The term $(x_i-\\mu)(x_j-\\mu)$ is of order $O(\\sigma_g^2)$. Let us analyze the second term in the expression for $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$:\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{O(\\sigma_g^2)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\frac{0}{m \\epsilon^{3/2}} = 0 $$\nTherefore, in the limit $\\sigma_g^2 \\to 0$, the derivative simplifies to:\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) = \\frac{1}{\\sqrt{\\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) $$\nSubstituting this back into the expression for $\\frac{\\partial L}{\\partial x_i}$, and using $g_j = \\frac{\\partial L}{\\partial y_j}$ to denote the incoming gradients:\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} g_j \\left( \\lim_{\\sigma_g^2 \\to 0} \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} \\right) = \\sum_{j=1}^{m} g_j \\frac{\\gamma}{\\sqrt{\\epsilon}} (\\delta_{ij} - \\frac{1}{m}) $$\n$$ = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j \\right) $$\nThis is the limiting form of the gradient $\\frac{\\partial L}{\\partial x_i}$.\n\n### Part 2: Tightest upper bound on $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$\n\nFrom Part 1, we must find the tightest upper bound on $\\max_i \\left| \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\bar{g} \\right) \\right|$, where $\\bar{g} = \\frac{1}{m} \\sum_j g_j$ and we are given $|g_j| \\leq G$ for all $j$. This is equivalent to maximizing $\\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\max_i |g_i - \\bar{g}|$. We focus on maximizing $|g_i - \\bar{g}|$.\n$$ |g_i - \\bar{g}| = \\left| g_i - \\frac{1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| = \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| $$\nBy the triangle inequality:\n$$ \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| \\leq \\frac{m-1}{m} |g_i| + \\frac{1}{m} \\sum_{j \\neq i} |g_j| $$\nUsing the bound $|g_j| \\leq G$:\n$$ \\leq \\frac{m-1}{m} G + \\frac{1}{m} (m-1) G = 2G \\frac{m-1}{m} $$\nThis bound is tight. It can be achieved by setting one gradient to an extreme and the others to the opposite extreme. For a fixed $i$, let $g_i = G$ and $g_j = -G$ for all $j \\neq i$.\nThe mean gradient is $\\bar{g} = \\frac{1}{m} (G + (m-1)(-G)) = \\frac{G(1 - m + 1)}{m} = \\frac{G(2-m)}{m}$.\nThen, $g_i - \\bar{g} = G - G\\frac{2-m}{m} = G \\left( \\frac{m - 2 + m}{m} \\right) = G \\frac{2m-2}{m} = 2G \\frac{m-1}{m}$.\nThe magnitude is $|g_i - \\bar{g}| = 2G \\frac{m-1}{m}$. For any $j \\neq i$, $|g_j - \\bar{g}| = |-G - G\\frac{2-m}{m}| = |-G \\frac{m+2-m}{m}| = \\frac{2G}{m}$.\nSince $m \\geq 2$, it follows that $m-1 \\geq 1$, so $2G\\frac{m-1}{m} \\geq \\frac{2G}{m}$.\nThe maximum deviation from the mean is indeed $2G \\frac{m-1}{m}$.\nThus, the tightest upper bound on the gradient magnitude is:\n$$ \\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\left( 2G \\frac{m-1}{m} \\right) $$\n\n### Part 3: Solving for the smallest $\\epsilon$\n\nWe are given the requirement that $\\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$ in the limit $\\sigma_g^2 \\to 0$. Using the tightest bound from Part 2, we establish the inequality:\n$$ \\frac{2G |\\gamma|}{\\sqrt{\\epsilon}} \\frac{m-1}{m} \\leq \\tau $$\nTo find the smallest $\\epsilon > 0$ that guarantees this condition for any valid configuration of incoming gradients, we solve for $\\epsilon$:\n$$ \\sqrt{\\epsilon} \\geq \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} $$\nThe inequality defines a lower bound for $\\sqrt{\\epsilon}$. The smallest value of $\\epsilon$ is obtained when this inequality is an equality. Squaring both sides yields the minimum required $\\epsilon$:\n$$ \\epsilon = \\left( \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} \\right)^2 = \\frac{4 G^2 \\gamma^2}{\\tau^2} \\left( 1 - \\frac{1}{m} \\right)^2 $$\n\n### Part 4: Proposing an adaptive strategy for selecting $\\epsilon$\n\nThe instability arises from the term $\\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}}$ in the gradient expression, which can blow up if both $\\sigma_g^2$ and $\\epsilon$ are close to zero. The fixed $\\epsilon$ from Part 3 provides a worst-case guarantee but may be overly conservative. An adaptive strategy can set $\\epsilon$ based on current conditions to be just large enough to ensure stability.\n\nA principled adaptive strategy is to enforce the gradient constraint from Part 3 dynamically. At each backward pass for a given group, we have access to the incoming gradients $g_j = \\frac{\\partial L}{\\partial y_j}$ and the parameter $\\gamma$. We can compute the term that drives the gradient magnitude: $C_{\\max} = |\\gamma| \\max_j |g_j - \\bar{g}|$.\nThe dominant part of the gradient magnitude is approximately $\\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}}$. To cap this at a threshold $\\tau$, we require:\n$$ \\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\leq \\tau \\implies \\sigma_g^2 + \\epsilon \\geq \\left( \\frac{C_{\\max}}{\\tau} \\right)^2 $$\nThis leads to the strategy of setting $\\epsilon$ to satisfy this condition with equality, while ensuring it does not become negative and including a small floor for robustness:\n$$ \\epsilon = \\max\\left(\\epsilon_{\\text{floor}}, \\left(\\frac{C_{\\max}}{\\tau}\\right)^2 - [\\sigma_g^2]_{\\text{sg}}\\right) $$\nHere, $\\epsilon_{\\text{floor}}$ is a small positive constant (e.g., $10^{-8}$), $\\tau$ is a hyperparameter for the desired gradient norm cap, and $[\\sigma_g^2]_{\\text{sg}}$ denotes that the value of the batch variance $\\sigma_g^2$ is used without propagating gradients through it in the computation of $\\epsilon$ (a stop-gradient operation).\n\nThis strategy works because:\n1. If $\\sigma_g^2$ is large (i.e., $(\\frac{C_{\\max}}{\\tau})^2 \\leq \\sigma_g^2$), $\\epsilon$ defaults to $\\epsilon_{\\text{floor}}$. The gradient denominator $\\sqrt{\\sigma_g^2+\\epsilon}$ is large, so gradients are small and stable.\n2. If $\\sigma_g^2$ is small (i.e., $(\\frac{C_{\\max}}{\\tau})^2 > \\sigma_g^2$), $\\epsilon$ is chosen such that $\\sigma_g^2 + \\epsilon = (C_{\\max}/\\tau)^2$. The gradient denominator becomes $\\sqrt{(C_{\\max}/\\tau)^2} = C_{\\max}/\\tau$. The gradient magnitude is thus capped at approximately $\\frac{C_{\\max}}{C_{\\max}/\\tau} = \\tau$, preventing explosion.\nThis adaptive selection of $\\epsilon$ directly targets the source of numerical instability.", "answer": "$$\n\\boxed{\\frac{4G^2\\gamma^2}{\\tau^2}\\left(\\frac{m-1}{m}\\right)^2}\n$$", "id": "3133999"}, {"introduction": "While Group Normalization effectively standardizes activations, its reliance on classical mean and variance makes it sensitive to outliers—extreme values that can corrupt statistical estimates. This hands-on coding practice challenges you to implement and compare the standard GN with a more robust alternative based on the Median-of-Means estimator. By constructing pathological test cases, you will empirically investigate how these different statistical foundations affect the network's behavior, offering valuable insights into designing models that are resilient to noisy or unusual data.", "problem": "Consider a four-dimensional tensor representing a mini-batch of feature maps in deep neural networks. Let the tensor be denoted by $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$, with batch size $N$, number of channels $C$, and spatial dimensions $H$ and $W$. The task is to implement, analyze, and compare two normalization schemes applied per sample and per group of channels: Group Normalization (GN) and a robust alternative that replaces classical estimators with the Median-of-Means (MoM) for both the mean and the second moment.\n\nYou must derive the normalization transformations starting from the following fundamental base:\n\n- The arithmetic mean of a finite set of real numbers $\\{x_i\\}_{i=1}^m$ is defined as $\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i$.\n- The (population) variance is defined as $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2$.\n- The second moment about zero is $M_2 = \\frac{1}{m} \\sum_{i=1}^m x_i^2$ and satisfies $\\sigma^2 = M_2 - \\mu^2$.\n\nA group configuration partitions the $C$ channels into $G$ non-overlapping groups of equal size $\\frac{C}{G}$ (assume $C$ is divisible by $G$). For each sample $n \\in \\{1,\\dots,N\\}$ and each group $g \\in \\{1,\\dots,G\\}$, collect all elements across the channels in that group and their spatial locations; denote this multiset by $\\mathcal{S}_{n,g}$ and its cardinality by $m = |\\mathcal{S}_{n,g}|$. Group Normalization uses the arithmetic mean and variance computed over $\\mathcal{S}_{n,g}$ to map elements of $X$ to a dimensionless representation whose group-wise mean is $0$ and group-wise second central moment equals $1$, up to a numerical stability constant $\\varepsilon > 0$. The robust alternative replaces the arithmetic mean with the Median-of-Means estimator and the second moment with a Median-of-Means estimate of $M_2$, and then constructs a robust variance via $\\widehat{\\sigma}^2_{\\text{robust}} = \\max\\{ \\widehat{M}_2 - \\widehat{\\mu}^2, 0 \\}$, also using the same numerical stability constant $\\varepsilon$.\n\nThe Median-of-Means (MoM) procedure on $\\mathcal{S}_{n,g}$ is defined as follows. Flatten $\\mathcal{S}_{n,g}$ into a sequence $(x_1,\\dots,x_m)$ in a fixed, deterministic order. Choose an integer number of blocks $k$ satisfying $k = \\min\\{3, m\\}$, and partition the sequence into $k$ contiguous blocks with sizes as equal as possible. For the mean, compute the block means and take their median to obtain $\\widehat{\\mu}$. For the second moment, compute the block averages of $x_i^2$ and take their median to obtain $\\widehat{M}_2$.\n\nImplement both normalization methods without any affine re-scaling or bias (i.e., use $\\gamma = 1$ and $\\beta = 0$) and use a fixed numerical stability constant $\\varepsilon = 10^{-5}$.\n\nConstruct the following deterministic test suite. In all cases, $N = 1$, and the ordering of elements within each group for the MoM partition is the default row-major flattening with channels varying slowest and spatial dimensions varying fastest.\n\n- Test Case $1$ (Pathological dominance within a group): $C = 4$, $H = 2$, $W = 2$, $G = 2$. Channels $\\{0,1\\}$ form group $0$, channels $\\{2,3\\}$ form group $1$. Define the spatial sign pattern $s(h,w) = 1$ if $(h + w)$ is even and $s(h,w) = -1$ otherwise, for $h \\in \\{0,1\\}$ and $w \\in \\{0,1\\}$. Set channel values as:\n  - Channel $0$: $x_{0,h,w} = \\mathbf{1}\\{(h + w) \\text{ is odd}\\}$,\n  - Channel $1$: $x_{1,h,w} = 1000 \\cdot s(h,w)$,\n  - Channel $2$: $x_{2,h,w} = 0.5 \\cdot s(h,w)$,\n  - Channel $3$: $x_{3,h,w} = -1 \\cdot s(h,w)$.\n  Use outlier channel index $c_{\\text{out}} = 1$ and small-variance channel index $c_{\\text{small}} = 0$.\n\n- Test Case $2$ (Balanced variances, no extreme outliers): $C = 4$, $H = 2$, $W = 2$, $G = 2$. Use the same $s(h,w)$ and set amplitudes $a_0 = 1$, $a_1 = 1.2$, $a_2 = 0.8$, $a_3 = 1.5$ with\n  - Channel $c$: $x_{c,h,w} = a_c \\cdot s(h,w)$ for $c \\in \\{0,1,2,3\\}$.\n  Use $c_{\\text{out}} = 1$ and $c_{\\text{small}} = 2$.\n\n- Test Case $3$ (Boundary: group size $1$): $C = 4$, $H = 2$, $W = 2$, $G = 4$. Use the same channel definitions as in Test Case $1$. Here each group contains exactly one channel. Use $c_{\\text{out}} = 1$ and $c_{\\text{small}} = 0$.\n\n- Test Case $4$ (Minimal elements per group): $C = 2$, $H = 1$, $W = 1$, $G = 1$. Set\n  - Channel $0$: $x_{0,0,0} = 0.1$,\n  - Channel $1$: $x_{1,0,0} = 1000$.\n  Use $c_{\\text{out}} = 1$ and $c_{\\text{small}} = 0$.\n\nFor each test case, compute four quantities:\n- $r_{\\text{den}}$: the ratio of denominators $\\frac{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}}$ for the group containing $c_{\\text{out}}$,\n- $r_{\\text{out}}$: the ratio of maximum absolute normalized values for the outlier channel $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{out}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{out}})|}$,\n- $r_{\\text{small}}$: the ratio of maximum absolute normalized values for the small-variance channel $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{small}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{small}})|}$,\n- $d_{\\mu}$: the absolute difference of the group mean estimators $| \\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}} |$ for the group containing $c_{\\text{out}}$.\n\nAll four quantities must be real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the four values in the order $[r_{\\text{den}}, r_{\\text{out}}, r_{\\text{small}}, d_{\\mu}]$. For example, the output format must be like $[[v_{1,1}, v_{1,2}, v_{1,3}, v_{1,4}], [v_{2,1}, v_{2,2}, v_{2,3}, v_{2,4}], [v_{3,1}, v_{3,2}, v_{3,3}, v_{3,4}], [v_{4,1}, v_{4,2}, v_{4,3}, v_{4,4}]]$ with exact numeric values produced by your implementation.", "solution": "The present task requires the implementation and comparative analysis of two normalization schemes for deep learning feature maps: the standard Group Normalization (GN) and a robust variant based on the Median-of-Means (MoM) estimator. The analysis will be conducted on a deterministic test suite designed to evaluate the behavior of these methods, particularly in the presence of outliers.\n\n### 1. Theoretical Foundation\n\nLet $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$ be a tensor representing a mini-batch of feature maps. Both normalization schemes operate on groups of channels. The $C$ channels are partitioned into $G$ groups. For each sample $n$ and group $g$, we define a multiset $\\mathcal{S}_{n,g}$ containing all activation values from the channels in that group and their spatial dimensions. The size of this set is $m = (C/G) \\cdot H \\cdot W$. Normalization aims to standardize the moments (e.g., mean and variance) of the distribution of values within each set $\\mathcal{S}_{n,g}$.\n\n#### 1.1. Standard Group Normalization (GN)\n\nGroup Normalization, as a standard technique, uses classical statistical estimators which are known to be efficient but sensitive to outliers.\n\nThe mean and variance for the set $\\mathcal{S}_{n,g}$ (with elements denoted by $x_i$) are computed as:\n-   **Arithmetic Mean**: The estimator for the first raw moment (the mean) is the sample mean:\n    $$ \\mu_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m x_i $$\n-   **Population Variance**: The estimator for the second central moment (the variance) is:\n    $$ \\sigma^2_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_{\\text{GN}})^2 $$\n    For computational stability and efficiency, this is often calculated using the second raw moment, $M_{2, \\text{GN}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i^2$, via the identity $\\sigma^2_{\\text{GN}} = M_{2, \\text{GN}} - \\mu_{\\text{GN}}^2$.\n\nThe normalization transformation for each element $x_i \\in \\mathcal{S}_{n,g}$ is then:\n$$ y_i = \\frac{x_i - \\mu_{\\text{GN}}}{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}} $$\nwhere $\\varepsilon$ is a small constant ($10^{-5}$ in this problem) added for numerical stability to prevent division by zero. This transformation rescales the activations in each group to have a mean of approximately $0$ and a variance of approximately $1$.\n\n#### 1.2. Robust Group Normalization with Median-of-Means (MoM)\n\nTo improve robustness against outliers, one can replace the classical mean and variance estimators with robust alternatives. The Median-of-Means (MoM) is a simple and effective robust estimator for the mean.\n\nThe MoM procedure for a set of $m$ values is as follows:\n1.  Partition the set of $m$ values into $k$ non-overlapping blocks, where $k$ is specified as $k = \\min\\{3, m\\}$. The partitioning is done to make block sizes as equal as possible.\n2.  Compute the arithmetic mean of the values within each of the $k$ blocks.\n3.  The MoM estimate is the median of these $k$ block means.\n\nApplying this principle, we define robust estimators for the mean and second moment:\n-   **MoM Mean Estimator ($\\widehat{\\mu}_{\\text{MoM}}$)**: The data $\\{x_i\\}_{i=1}^m$ are partitioned into $k$ blocks. Let $B_1, \\dots, B_k$ be these blocks. The MoM estimate of the mean is:\n    $$ \\widehat{\\mu}_{\\text{MoM}} = \\text{median}\\left( \\left\\{ \\text{mean}(B_j) \\right\\}_{j=1}^k \\right) $$\n-   **MoM Second Moment Estimator ($\\widehat{M}_{2, \\text{MoM}}$)**: The same procedure is applied to the squared data $\\{x_i^2\\}_{i=1}^m$. These are partitioned into $k$ blocks, and the MoM estimate of the second moment is the median of the block averages of the squared data.\n-   **Robust Variance Estimator ($\\widehat{\\sigma}^2_{\\text{MoM}}$)**: A robust variance estimate is constructed using the plug-in principle with the robust estimators for the moments:\n    $$ \\widehat{\\sigma}^2_{\\text{MoM}} = \\max\\left( \\widehat{M}_{2, \\text{MoM}} - \\widehat{\\mu}_{\\text{MoM}}^2, 0 \\right) $$\n    The $\\max(\\cdot, 0)$ operation ensures non-negativity, which could be violated due to estimation errors.\n\nThe robust normalization transformation for each element $x_i \\in \\mathcal{S}_{n,g}$ is analogous to GN:\n$$ y_i = \\frac{x_i - \\widehat{\\mu}_{\\text{MoM}}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}} $$\n\n### 2. Algorithmic Implementation and Analysis\n\nThe implementation follows the definitions above. For each test case, an input tensor $X$ is constructed. Two separate functions apply the GN and MoM-based normalization schemes. Each function iterates through the $G$ channel groups. For a given group, it extracts the corresponding data slice from $X$, flattens it into a one-dimensional array, and computes the required statistics (either $\\mu_{\\text{GN}}, \\sigma^2_{\\text{GN}}$ or $\\widehat{\\mu}_{\\text{MoM}}, \\widehat{\\sigma}^2_{\\text{MoM}}$). These statistics are then used to normalize the data in that group.\n\nA key detail in the MoM implementation is the block partitioning. For a flattened array of size $m$ and $k$ blocks, `numpy.array_split` is used, which creates blocks of sizes as equal as possible, matching the problem specification. The median is computed using `numpy.median`, which correctly handles both odd and even numbers of block statistics.\n\nThe analysis is performed by computing four metrics for each test case:\n-   $d_{\\mu} = |\\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}}|$: This directly measures the difference between the classical and robust mean estimates for the group containing the outlier channel. A large value indicates that the outlier significantly skewed the classical mean.\n-   $r_{\\text{den}} = \\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon} / \\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}$: This compares the scale estimates (the normalization denominators). The ratio indicates whether the robust method perceived a larger or smaller spread in the data compared to the classical method.\n-   $r_{\\text{out}} = \\max |Y_{\\text{GN}}(c_{\\text{out}})| / \\max |Y_{\\text{MoM}}(c_{\\text{out}})|$: This ratio measures the relative post-normalization magnitude of the outlier channel. A value greater than $1$ suggests that the MoM-based method was more effective at suppressing the outlier's magnitude.\n-   $r_{\\text{small}} = \\max |Y_{\\text{GN}}(c_{\\text{small}})| / \\max |Y_{\\text{MoM}}(c_{\\text{small}})|$: This assesses the impact of the normalization scheme on a \"regular\" channel. An ideal robust method would have minimal distorting impact on non-outlying data.\n\nThe test cases are designed to probe different behaviors:\n-   **Test Case 1** introduces a strong outlier to highlight the core differences in robustness.\n-   **Test Case 2** uses balanced data without extreme outliers, where both methods are expected to yield similar results.\n-   **Test Cases 3 and 4** explore boundary conditions related to the number of elements per group ($m$) and its effect on the MoM estimator, specifically when $k < 3$. For $m=2$, $k=2$, MoM estimators for mean and second moment become identical to their classical counterparts because the median of two values is their average. For $m=4$, $k=3$, the difference between methods depends on the data distribution and how the partitions are formed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    test_cases_params = [\n        # (C, H, W, G, c_out, c_small, data_generator_id)\n        (4, 2, 2, 2, 1, 0, 1),\n        (4, 2, 2, 2, 1, 2, 2),\n        (4, 2, 2, 4, 1, 0, 1),\n        (2, 1, 1, 1, 1, 0, 3)\n    ]\n\n    EPSILON = 1e-5\n\n    def get_mom_stats(data: np.ndarray):\n        \"\"\"Computes Median-of-Means for mean and second moment.\"\"\"\n        m = data.shape[0]\n        k = min(3, m)\n        \n        # Partition data for mean estimation\n        blocks = np.array_split(data, k)\n        block_means = [np.mean(b) for b in blocks]\n        mu_mom = np.median(block_means)\n        \n        # Partition squared data for M2 estimation\n        squared_data = np.square(data)\n        sq_blocks = np.array_split(squared_data, k)\n        block_m2s = [np.mean(b) for b in sq_blocks]\n        m2_mom = np.median(block_m2s)\n        \n        return mu_mom, m2_mom\n\n    def apply_norm(X: np.ndarray, G: int, method: str):\n        \"\"\"Applies Group Normalization using the specified method.\"\"\"\n        N, C, H, W = X.shape\n        Y = np.zeros_like(X, dtype=np.float64)\n        stats = {}\n        \n        channels_per_group = C // G\n        if C % G != 0:\n            raise ValueError(\"C must be divisible by G.\")\n        \n        for g in range(G):\n            ch_start = g * channels_per_group\n            ch_end = (g + 1) * channels_per_group\n            \n            group_data = X[:, ch_start:ch_end, :, :].flatten()\n            \n            if method == 'gn':\n                mu = np.mean(group_data)\n                # Population variance\n                var = np.var(group_data)\n            elif method == 'mom':\n                mu, m2 = get_mom_stats(group_data)\n                var = max(m2 - mu**2, 0)\n            else:\n                raise ValueError(\"Unknown method.\")\n            \n            stats[g] = {'mean': mu, 'var': var}\n            \n            group_slice = X[:, ch_start:ch_end, :, :]\n            Y[:, ch_start:ch_end, :, :] = (group_slice - mu) / np.sqrt(var + EPSILON)\n            \n        return Y, stats\n\n    def create_tensor(C, H, W, gen_id):\n        \"\"\"Generates the input tensor for a test case.\"\"\"\n        X = np.zeros((1, C, H, W), dtype=np.float64)\n        \n        if gen_id == 1 or gen_id == 2:\n            s = np.zeros((H, W))\n            for h in range(H):\n                for w in range(W):\n                    s[h,w] = 1 if (h + w) % 2 == 0 else -1\n\n            if gen_id == 1:\n                # Test Case 1 & 3 data\n                X[0, 0, :, :] = np.fromfunction(lambda h, w: (h + w) % 2, (H, W))\n                X[0, 1, :, :] = 1000.0 * s\n                X[0, 2, :, :] = 0.5 * s\n                X[0, 3, :, :] = -1.0 * s\n            elif gen_id == 2:\n                # Test Case 2 data\n                amplitudes = [1.0, 1.2, 0.8, 1.5]\n                for c in range(C):\n                    X[0, c, :, :] = amplitudes[c] * s\n        elif gen_id == 3:\n            # Test Case 4 data\n            X[0, 0, 0, 0] = 0.1\n            X[0, 1, 0, 0] = 1000.0\n            \n        return X\n\n    final_results = []\n    for C, H, W, G, c_out, c_small, gen_id in test_cases_params:\n        X = create_tensor(C, H, W, gen_id)\n        \n        channels_per_group = C // G\n        group_out_idx = c_out // channels_per_group\n        \n        # Apply both normalization methods\n        Y_gn, stats_gn = apply_norm(X, G, 'gn')\n        Y_mom, stats_mom = apply_norm(X, G, 'mom')\n        \n        # Extract stats for the outlier group\n        mu_gn = stats_gn[group_out_idx]['mean']\n        var_gn = stats_gn[group_out_idx]['var']\n        mu_mom = stats_mom[group_out_idx]['mean']\n        var_mom = stats_mom[group_out_idx]['var']\n\n        # Calculate metrics\n        d_mu = np.abs(mu_gn - mu_mom)\n        \n        denom_gn = np.sqrt(var_gn + EPSILON)\n        denom_mom = np.sqrt(var_mom + EPSILON)\n        r_den = denom_gn / denom_mom\n        \n        max_abs_gn_out = np.max(np.abs(Y_gn[0, c_out, :, :]))\n        max_abs_mom_out = np.max(np.abs(Y_mom[0, c_out, :, :]))\n        r_out = max_abs_gn_out / max_abs_mom_out if max_abs_mom_out != 0 else np.inf\n\n        max_abs_gn_small = np.max(np.abs(Y_gn[0, c_small, :, :]))\n        max_abs_mom_small = np.max(np.abs(Y_mom[0, c_small, :, :]))\n        r_small = max_abs_gn_small / max_abs_mom_small if max_abs_mom_small != 0 else np.inf\n        \n        case_results = [r_den, r_out, r_small, d_mu]\n        final_results.append(case_results)\n\n    # Convert results to a string in the required list-of-lists format\n    result_str = \"[\" + \", \".join([f\"[{', '.join(map(str, res))}]\" for res in final_results]) + \"]\"\n    print(result_str)\n\nsolve()\n\n```", "id": "3134016"}, {"introduction": "In modern deep learning architectures, layers are rarely used in isolation; their composition and interaction define the network's function. A fundamental question is whether the order of operations matters, for instance, applying convolution before or after normalization. This advanced theoretical exercise delves into the algebraic relationship between Group Normalization and convolution, asking you to derive the precise conditions under which these two operations commute. This exploration provides a deeper understanding of the symmetries within neural networks and the principles governing the design of effective layer blocks.", "problem": "Consider a single input sample with tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, split into $G$ disjoint groups of equal size $C/G$ channels. For each group $g \\in \\{1,\\dots,G\\}$, define the vectorized group $z_g \\in \\mathbb{R}^{m}$ by stacking all $(C/G)\\cdot H \\cdot W$ entries of that group, where $m = (C/G)\\,H\\,W$. Let $1 \\in \\mathbb{R}^{m}$ denote the all-ones vector and let the centering projector be $P_g = I_m - \\frac{1}{m} 1 1^{\\top}$. The group mean and variance are\n$$\n\\mu_g(z_g) = \\frac{1}{m} 1^{\\top} z_g, \n\\qquad \n\\sigma_g^{2}(z_g) = \\frac{1}{m} \\| P_g z_g \\|_2^{2}.\n$$\nGroup Normalization (GN) with per-channel affine parameters $\\{\\gamma_c, \\beta_c\\}_{c=1}^{C}$ acts on each group $g$ as\n$$\n\\mathrm{GN}_g(z_g) \\;=\\; \\Gamma_g \\frac{P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} \\;+\\; \\beta_g,\n$$\nwhere $\\varepsilon \\ge 0$ is a fixed constant, $\\Gamma_g \\in \\mathbb{R}^{m\\times m}$ is diagonal collecting the per-channel $\\gamma_c$ replicated across spatial locations for channels in group $g$, and $\\beta_g \\in \\mathbb{R}^{m}$ stacks the per-channel $\\beta_c$ replicated across spatial locations for channels in group $g$. The full output is the concatenation over groups.\n\nLet a convolutional layer (Conv) be represented as a block-diagonal linear operator across groups with per-group linear maps $L_g \\in \\mathbb{R}^{m\\times m}$ and a per-group bias $b_g \\in \\mathbb{R}^{m}$, so that\n$$\n\\mathrm{Conv}_g(z_g) \\;=\\; L_g z_g \\;+\\; b_g,\n$$\nwith no mixing of channels across groups (that is, the overall $L$ is block-diagonal with blocks $L_g$).\n\nUsing only the definitions above, and the facts that convolution is linear and $P_g$ is an orthogonal projector onto the zero-mean subspace, do the following:\n\n1) Derive necessary and sufficient algebraic conditions on the per-group convolution maps $L_g$ and biases $b_g$, the Group Normalization affine parameters $\\Gamma_g$ and $\\beta_g$, and the stabilizer $\\varepsilon$, under which\n$$\n\\mathrm{Conv}(\\mathrm{GN}(x)) \\;=\\; \\mathrm{GN}(\\mathrm{Conv}(x))\n$$\nholds for every input $x$. Your derivation must be explicit in terms of $P_g$ and $1$ and should make precise the structure that $L_g$ must have on the subspace $\\mathrm{range}(P_g)$ and on $\\mathrm{span}\\{1\\}$.\n\n2) Define the commutator residual\n$$\nC(x) \\;=\\; \\mathrm{Conv}(\\mathrm{GN}(x)) \\;-\\; \\mathrm{GN}(\\mathrm{Conv}(x)),\n$$\nand the scalar\n$$\nJ(x) \\;=\\; \\sum_{g=1}^{G} \\| C_g(x) \\|_2^{2},\n$$\nwhere $C_g(x)$ is the $g$-th group block of $C(x)$ viewed in $\\mathbb{R}^{m}$. Under the conditions you derived in part 1, determine the exact value of $J(x)$ for arbitrary nonzero $x$. Provide your final answer as a single real number with no units. No rounding is required.", "solution": "We begin from the definitions of Group Normalization (GN) and convolution as stated. For each group $g$, we work with a vectorized representation $z_g \\in \\mathbb{R}^{m}$, where $m = (C/G)\\,H\\,W$. The centering projector $P_g = I_m - \\frac{1}{m} 1 1^{\\top}$ satisfies $P_g^{2} = P_g$, $P_g^{\\top} = P_g$, $\\mathrm{range}(P_g) = \\{v : 1^{\\top} v = 0\\}$, and $\\ker(P_g) = \\mathrm{span}\\{1\\}$. The variance can be written as $\\sigma_g^{2}(z_g) = \\frac{1}{m} \\|P_g z_g\\|_2^{2}$.\n\nPart 1: Derivation of commutation conditions.\n\nWe write the two compositions on group $g$:\n\nLeft (Conv after GN):\n$$\n\\mathrm{Conv}_g(\\mathrm{GN}_g(z_g)) \n= L_g \\left( \\Gamma_g \\frac{P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} + \\beta_g \\right) + b_g \n= \\frac{L_g \\Gamma_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} + L_g \\beta_g + b_g.\n$$\n\nRight (GN after Conv):\nFirst compute $\\mathrm{Conv}_g(z_g) = L_g z_g + b_g$. Its group mean and variance are\n$$\n\\mu_g(L_g z_g + b_g) = \\frac{1}{m} 1^{\\top} (L_g z_g + b_g), \n\\qquad \n\\sigma_g^{2}(L_g z_g + b_g) = \\frac{1}{m} \\| P_g (L_g z_g + b_g)\\|_2^{2}.\n$$\nThen\n$$\n\\mathrm{GN}_g(\\mathrm{Conv}_g(z_g))\n= \\Gamma_g \\frac{P_g (L_g z_g + b_g)}{\\sqrt{\\sigma_g^{2}(L_g z_g + b_g) + \\varepsilon}} + \\beta_g.\n$$\n\nEquality for all $z_g$ requires\n$$\n\\frac{L_g \\Gamma_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} + L_g \\beta_g + b_g\n\\;=\\;\n\\Gamma_g \\frac{P_g (L_g z_g + b_g)}{\\sqrt{\\sigma_g^{2}(L_g z_g + b_g) + \\varepsilon}} + \\beta_g\n\\quad \\text{for all } z_g.\n$$\n\nWe separate terms that depend on $z_g$ from those that do not.\n\nA) Bias terms. Comparing terms independent of $z_g$, we have\n$$\nL_g \\beta_g + b_g \\;=\\; \\Gamma_g \\frac{P_g b_g}{\\sqrt{\\sigma_g^{2}(b_g) + \\varepsilon}} + \\beta_g.\n$$\nFor this to hold for every configuration, and in particular to avoid input-dependent denominators on the right-hand side, a sufficient and necessary way is to eliminate $b_g$ and also prevent the left-hand term $L_g \\beta_g$ from introducing a mismatch. A clean necessary condition is\n$$\nb_g = 0.\n$$\nWith $b_g=0$, the condition reduces to $L_g \\beta_g = \\beta_g$. To guarantee commutation independently of any further structure of $L_g$ on the centered subspace, the robust choice is\n$$\n\\beta_g = 0.\n$$\nIf one wishes to allow $\\beta_g \\neq 0$, it must satisfy $L_g \\beta_g = \\beta_g$, i.e., $\\beta_g$ lies in the fixed-point subspace of $L_g$. Since we seek conditions that ensure equality for every input $x$ and are uniform across admissible $L_g$, we adopt $\\beta_g=0$ and $b_g=0$ as necessary and sufficient for the bias/shift terms not to break commutation.\n\nHenceforth, we assume\n$$\nb_g = 0, \\qquad \\beta_g = 0.\n$$\n\nB) Scale and centering interaction. With $b_g=\\beta_g=0$, the equality reduces to\n$$\n\\frac{L_g \\Gamma_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}}\n\\;=\\;\n\\Gamma_g \\frac{P_g L_g z_g}{\\sqrt{\\sigma_g^{2}(L_g z_g) + \\varepsilon}}\n\\quad \\text{for all } z_g.\n$$\nThis can be rearranged as\n$$\nL_g \\Gamma_g P_g z_g \\cdot \\sqrt{\\sigma_g^{2}(L_g z_g) + \\varepsilon}\n\\;=\\;\n\\Gamma_g P_g L_g z_g \\cdot \\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}\n\\quad \\text{for all } z_g.\n$$\n\nWe now impose commutation of $L_g$ with $P_g$ to avoid mean leakage: we need $P_g L_g z_g$ to depend only on $P_g z_g$ (and not on $\\mu_g(z_g)$). This is equivalent to requiring\n$$\nP_g L_g = L_g P_g,\n$$\nwhich expands to $L_g 1 \\in \\mathrm{span}\\{1\\}$ and $1^{\\top} L_g \\in \\mathrm{span}\\{1^{\\top}\\}$. Concretely, there must exist a scalar $\\alpha_g \\in \\mathbb{R}$ such that\n$$\nL_g 1 = \\alpha_g 1, \n\\qquad \n1^{\\top} L_g = \\alpha_g 1^{\\top}.\n$$\nUnder this bi-eigenvector condition, $P_g L_g = L_g P_g$ holds.\n\nOn the centered subspace $\\mathrm{range}(P_g)$, write the restriction of $L_g$ as\n$$\nL_g \\big|_{\\mathrm{range}(P_g)} = A_g,\n$$\nwhere $A_g:\\mathrm{range}(P_g) \\to \\mathrm{range}(P_g)$ is linear. Then for any $z_g$,\n$$\nP_g L_g z_g = A_g (P_g z_g),\n\\qquad \n\\sigma_g^{2}(L_g z_g) = \\frac{1}{m} \\| A_g (P_g z_g)\\|_2^{2}.\n$$\n\nC) Equalization of denominators. For the normalized magnitudes to match for all $z_g$, we require that on the centered subspace the transformation scales norms uniformly by a factor independent of $z_g$ (up to an orthogonal action). That is, there exists $\\alpha_g \\in \\mathbb{R}$ and an orthogonal map $U_g$ on $\\mathrm{range}(P_g)$ such that\n$$\nA_g = \\alpha_g U_g,\n\\qquad \nU_g^{\\top} U_g = U_g U_g^{\\top} = I \\text{ on } \\mathrm{range}(P_g).\n$$\nThen\n$$\n\\|A_g (P_g z_g)\\|_2 = |\\alpha_g| \\, \\| P_g z_g \\|_2 \\;\\Rightarrow\\; \\sigma_g^{2}(L_g z_g) = \\alpha_g^{2}\\, \\sigma_g^{2}(z_g).\n$$\nWith $\\sigma_g^{2}(L_g z_g) = \\alpha_g^{2} \\sigma_g^{2}(z_g)$, the denominators become\n$$\n\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}\n\\quad \\text{and} \\quad\n\\sqrt{\\alpha_g^{2}\\,\\sigma_g^{2}(z_g) + \\varepsilon}.\n$$\nFor equality for every $z_g$ (and hence for every $\\sigma_g^{2}(z_g) \\ge 0$), we must have\n$$\n\\sqrt{\\alpha_g^{2}\\,\\sigma_g^{2}(z_g) + \\varepsilon} \\;=\\; \\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon} \\quad \\text{for all } \\sigma_g^{2}(z_g) \\ge 0,\n$$\nwhich forces\n$$\n\\alpha_g^{2} = 1.\n$$\nThus $\\alpha_g \\in \\{+1,-1\\}$. This is the only way to reconcile the $\\varepsilon$-stabilized denominators for all inputs.\n\nD) Commutation with the GN affine scaling. The diagonal $\\Gamma_g$ multiplies each coordinate within group $g$. For the equality\n$$\nL_g \\Gamma_g P_g z_g \\;=\\; \\Gamma_g P_g L_g z_g\n$$\nto hold for all $z_g$, we need $\\Gamma_g$ to commute with $L_g$ on $\\mathrm{range}(P_g)$. Since $L_g$ may mix coordinates within the group via $U_g$ on $\\mathrm{range}(P_g)$, the only diagonal $\\Gamma_g$ that commutes with all such $U_g$ is a scalar multiple of the identity on the group:\n$$\n\\Gamma_g = \\gamma_g I_m \\quad \\text{for some } \\gamma_g \\in \\mathbb{R}.\n$$\nIn words, the per-channel scale must be constant within each group. Together with $\\beta_g=0$, this amounts to GN’s affine being a per-group scalar without shift.\n\nCollecting A–D, the necessary and sufficient conditions for commutation on each group $g$ are:\n- No cross-group mixing and zero biases: $b_g = 0$.\n- GN affine parameters satisfy $\\beta_g = 0$ and $\\Gamma_g = \\gamma_g I_m$ for some $\\gamma_g \\in \\mathbb{R}$ (i.e., per-channel $\\gamma_c$ is constant within the group; per-channel $\\beta_c$ is zero).\n- The convolution map respects means and is an isometry on the centered subspace up to a possible sign:\nThere exists $\\alpha_g \\in \\{+1,-1\\}$ and an orthogonal $U_g$ on $\\mathrm{range}(P_g)$ such that\n$$\nL_g 1 = \\alpha_g 1, \n\\qquad \n1^{\\top} L_g = \\alpha_g 1^{\\top}, \n\\qquad \nL_g P_g = \\alpha_g U_g P_g.\n$$\nEquivalently, $P_g L_g = L_g P_g$, $L_g$ acts as $\\alpha_g$ on $\\mathrm{span}\\{1\\}$, and as $\\alpha_g U_g$ on $\\mathrm{range}(P_g)$.\n\nUnder these conditions, for all $z_g$,\n$$\n\\mathrm{Conv}_g(\\mathrm{GN}_g(z_g)) \n= \\frac{\\gamma_g \\alpha_g U_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}}\n= \\gamma_g \\frac{P_g L_g z_g}{\\sqrt{\\sigma_g^{2}(L_g z_g) + \\varepsilon}}\n= \\mathrm{GN}_g(\\mathrm{Conv}_g(z_g)).\n$$\nHence $\\mathrm{Conv}(\\mathrm{GN}(x)) = \\mathrm{GN}(\\mathrm{Conv}(x))$ for every input $x$.\n\nPart 2: Value of $J(x)$ under the conditions.\n\nBy definition,\n$$\nC(x) = \\mathrm{Conv}(\\mathrm{GN}(x)) - \\mathrm{GN}(\\mathrm{Conv}(x)),\n$$\nand $J(x) = \\sum_{g=1}^{G} \\| C_g(x) \\|_2^{2}$, where $C_g(x)$ is the $g$-th group block.\n\nUnder the conditions derived in Part 1, we have $\\mathrm{Conv}(\\mathrm{GN}(x)) = \\mathrm{GN}(\\mathrm{Conv}(x))$ identically for all $x$, hence $C_g(x) = 0$ for all $g$ and $x$. Therefore,\n$$\nJ(x) = \\sum_{g=1}^{G} \\| 0 \\|_2^{2} = 0.\n$$\nThis exact value holds for arbitrary nonzero $x$ as well as $x=0$.", "answer": "$$\\boxed{0}$$", "id": "3134023"}]}