{"hands_on_practices": [{"introduction": "Weight decay, or $L_2$ regularization, is one of the most common tools for preventing overfitting, but its necessity can be subtle. This exercise peels back the layers on why it is so effective by comparing an unregularized optimizer with a regularized one on a simple, analyzable dataset. By solving this problem [@problem_id:3169285], you will discover the \"implicit bias\" of gradient descent which can drive weights to infinity, and see how an explicit $L_2$ penalty tames this behavior to find a stable, finite solution.", "problem": "Consider a binary linear classifier with parameter vector $w \\in \\mathbb{R}^d$ trained on a linearly separable dataset consisting of exactly two examples $(x_1,y_1)$ and $(x_2,y_2)$ given by $(x_1,y_1)=(x,+1)$ and $(x_2,y_2)=(-x,-1)$, where $x \\in \\mathbb{R}^d$ and $x \\neq 0$. Let $r=\\|x\\|$. The prediction is $f_w(x)=w^\\top x$, and the training loss for a single example is the exponential loss $\\ell(z)=\\exp(-z)$. The empirical risk without regularization is $L(w)=\\sum_{i=1}^{2}\\ell\\!\\left(y_i w^\\top x_i\\right)$. To study the effect of weight decay, also consider the regularized objective $F_\\lambda(w)=L(w)+\\frac{\\lambda}{2}\\|w\\|^2$ with $\\lambda0$. Define the signed margin of a parameter $w$ as $m(w)=\\min_{i \\in \\{1,2\\}} y_i w^\\top x_i$.\n\nAssume stochastic gradient descent (SGD) with a sufficiently small constant step size and standard unbiased gradient estimates is run on $L(w)$ (no weight decay) and on $F_\\lambda(w)$ (with weight decay), and that as $t \\to \\infty$ the iterates track the continuous-time gradient flow and converge in direction to stationary solutions where applicable. Using only the definitions of empirical risk minimization, gradients of smooth functions, and properties of convex symmetric problems, analyze and compare the implicit bias of SGD in the two cases as $t \\to \\infty$ by studying the behavior of the norm $\\|w(t)\\|$ and the signed margin $m\\!\\left(w(t)\\right)$.\n\nThen, for the regularized case with $\\lambda0$, reduce the optimization to a single scalar by exploiting the symmetry of the dataset, derive the first-order optimality condition for the minimizer $w_\\lambda^\\star$ of $F_\\lambda(w)$, and solve it exactly to obtain the limiting signed margin $m_\\lambda^\\star = m\\!\\left(w_\\lambda^\\star\\right)$ in closed form as a function of $r$ and $\\lambda$.\n\nGive your final answer as a single closed-form analytic expression for $m_\\lambda^\\star$ in terms of $r$ and $\\lambda$. No numerical approximation is required or allowed.", "solution": "The problem statement has been validated and is determined to be a valid, well-posed problem in the field of machine learning theory. It is scientifically grounded, self-contained, and free of ambiguities or contradictions. We may therefore proceed with a full solution.\n\nThe problem asks for an analysis of the behavior of stochastic gradient descent (SGD) on a binary linear classification task with and without L2 regularization (weight decay), and for the derivation of the optimal margin in the regularized case.\n\nLet us first define the key quantities based on the provided information. The dataset consists of two points, $(x_1, y_1) = (x, +1)$ and $(x_2, y_2) = (-x, -1)$, where $x \\in \\mathbb{R}^d$ is a non-zero vector. Let $r = \\|x\\|$. The loss function is the exponential loss, $\\ell(z) = \\exp(-z)$.\n\nThe unregularized empirical risk is given by $L(w) = \\sum_{i=1}^{2} \\ell(y_i w^\\top x_i)$. Substituting the data points:\n$$L(w) = \\ell( (+1) w^\\top x ) + \\ell( (-1) w^\\top (-x) ) = \\exp(-w^\\top x) + \\exp(-(w^\\top x)) = 2 \\exp(-w^\\top x)$$\nThe signed margin for this dataset is $m(w) = \\min_{i \\in \\{1,2\\}} y_i w^\\top x_i = \\min(w^\\top x, (-1)w^\\top(-x)) = \\min(w^\\top x, w^\\top x) = w^\\top x$.\nTherefore, the unregularized risk can be expressed purely in terms of the margin:\n$$L(w) = 2 \\exp(-m(w))$$\nThe regularized objective function is $F_\\lambda(w) = L(w) + \\frac{\\lambda}{2} \\|w\\|^2$ for a regularization parameter $\\lambda  0$.\n\n**Analysis of SGD Dynamics**\n\nWe analyze the behavior of the parameter vector $w(t)$ under gradient flow, which SGD is assumed to track for a small step size.\n\n**Case 1: Unregularized Objective ($L(w)$, $\\lambda=0$)**\nThe gradient flow for the unregularized objective $L(w)$ is described by the differential equation $\\frac{dw}{dt} = -\\nabla L(w)$.\nThe gradient of $L(w)$ is:\n$$\\nabla L(w) = \\nabla_w \\left( 2 \\exp(-w^\\top x) \\right) = 2 \\exp(-w^\\top x) \\cdot \\nabla_w(-w^\\top x) = -2x \\exp(-w^\\top x)$$\nThus, the gradient flow is:\n$$\\frac{dw}{dt} = -(-2x \\exp(-w^\\top x)) = 2x \\exp(-w^\\top x)$$\nThis equation reveals that the change in $w$ at any time $t$ is always in the direction of the vector $x$. If we initialize $w(0)=0$, then $w(t)$ will always be parallel to $x$. More generally, any component of $w(0)$ orthogonal to $x$ will remain constant, while the component parallel to $x$ evolves. The dynamics of interest are captured by assuming $w(t)$ is collinear with $x$, so we can write $w(t) = c(t)x$ for some scalar function $c(t)$.\n\nMinimizing $L(w) = 2\\exp(-m(w))$ is equivalent to maximizing the margin $m(w) = w^\\top x$. For a linearly separable dataset with exponential loss, the loss can be driven arbitrarily close to $0$ by making the margin arbitrarily large. This implies that the norm of the weight vector, $\\|w\\|$, must tend to infinity. Let's verify this with the flow equation. Substituting $w(t) = c(t)x$:\n$$\\frac{d}{dt}(c(t)x) = 2x \\exp(-(c(t)x)^\\top x)$$\n$$x \\frac{dc}{dt} = 2x \\exp(-c(t) \\|x\\|^2) = 2x \\exp(-c(t) r^2)$$\n$$\\frac{dc}{dt} = 2 \\exp(-c(t) r^2)$$\nSince the right-hand side is always positive, $c(t)$ is a strictly increasing function of $t$. As $t \\to \\infty$, $c(t)$ grows without bound, so $c(t) \\to \\infty$.\nConsequently, the norm of the weights diverges: $\\|w(t)\\| = \\|c(t)x\\| = |c(t)| \\|x\\| = c(t)r \\to \\infty$.\nThe margin also diverges: $m(w(t)) = w(t)^\\top x = c(t)r^2 \\to \\infty$.\n\nThis behavior is known as the **implicit bias** of gradient descent. For separable data with exponential loss, SGD does not converge to a finite weight vector. Instead, it finds a solution with ever-increasing norm that continuously improves the margin. The direction of $w(t)$ converges to the direction of $x$, which is the max-margin direction for this simple problem.\n\n**Case 2: Regularized Objective ($F_\\lambda(w)$, $\\lambda0$)**\nThe gradient flow for the regularized objective $F_\\lambda(w) = L(w) + \\frac{\\lambda}{2} \\|w\\|^2$ is given by $\\frac{dw}{dt} = -\\nabla F_\\lambda(w)$.\nThe gradient is:\n$$\\nabla F_\\lambda(w) = \\nabla L(w) + \\nabla \\left( \\frac{\\lambda}{2} \\|w\\|^2 \\right) = -2x \\exp(-w^\\top x) + \\lambda w$$\nThe gradient flow is:\n$$\\frac{dw}{dt} = 2x \\exp(-w^\\top x) - \\lambda w$$\nFor $\\lambda  0$, the objective $F_\\lambda(w)$ is strictly convex, as it is the sum of a convex function $L(w)$ and a strictly convex function $\\frac{\\lambda}{2}\\|w\\|^2$. Therefore, a unique finite minimizer $w_\\lambda^\\star$ exists, which is the stationary point of the flow where $\\frac{dw}{dt}=0$.\nAt this minimizer, the gradient is zero:\n$$\\nabla F_\\lambda(w_\\lambda^\\star) = -2x \\exp(-(w_\\lambda^\\star)^\\top x) + \\lambda w_\\lambda^\\star = 0$$\nThis implies that SGD converges to a finite solution: $w(t) \\to w_\\lambda^\\star$ as $t \\to \\infty$. Consequently, both the norm $\\|w(t)\\|$ and the margin $m(w(t))$ converge to finite values, $\\|w_\\lambda^\\star\\|$ and $m(w_\\lambda^\\star)$ respectively.\n\nThe L2 regularization term, or weight decay, introduces a penalty on large weights, preventing the norm from diverging to infinity. It counteracts the implicit margin-maximization pressure from the exponential loss, resulting in a **finite** optimal weight vector $w_\\lambda^\\star$ that balances minimizing the classification loss and minimizing the weight norm.\n\n**Derivation of the Optimal Margin $m_\\lambda^\\star$**\n\nWe must now solve for the limiting signed margin $m_\\lambda^\\star = m(w_\\lambda^\\star)$ in the regularized case. The first-order optimality condition is:\n$$\\lambda w_\\lambda^\\star = 2x \\exp(-(w_\\lambda^\\star)^\\top x)$$\nFrom this equation, it is evident that the optimal vector $w_\\lambda^\\star$ must be parallel to the vector $x$. Thus, we can express $w_\\lambda^\\star$ as $w_\\lambda^\\star = c^\\star x$ for some scalar constant $c^\\star$. To ensure the margin $w^\\top x$ is positive (for low loss), we must have $c^\\star  0$.\n\nLet's substitute $w_\\lambda^\\star = c^\\star x$ back into the optimality condition:\n$$\\lambda (c^\\star x) = 2x \\exp(-(c^\\star x)^\\top x)$$\nSince $x \\neq 0$, we can cancel it from both sides:\n$$\\lambda c^\\star = 2 \\exp(-c^\\star \\|x\\|^2) = 2 \\exp(-c^\\star r^2)$$\nThis is the optimality condition for the scalar coefficient $c^\\star$.\n\nThe problem asks for the optimal margin, $m_\\lambda^\\star$, not $c^\\star$. The margin is related to $c^\\star$ as follows:\n$$m_\\lambda^\\star = m(w_\\lambda^\\star) = (w_\\lambda^\\star)^\\top x = (c^\\star x)^\\top x = c^\\star \\|x\\|^2 = c^\\star r^2$$\nFrom this relationship, we can express $c^\\star$ in terms of the margin we seek:\n$$c^\\star = \\frac{m_\\lambda^\\star}{r^2}$$\nNow, we substitute this expression for $c^\\star$ back into its scalar optimality condition:\n$$\\lambda \\left(\\frac{m_\\lambda^\\star}{r^2}\\right) = 2 \\exp\\left(-\\left(\\frac{m_\\lambda^\\star}{r^2}\\right) r^2\\right)$$\n$$\\frac{\\lambda m_\\lambda^\\star}{r^2} = 2 \\exp(-m_\\lambda^\\star)$$\nTo solve for $m_\\lambda^\\star$, we rearrange the equation to isolate it. We bring all terms involving $m_\\lambda^\\star$ to one side:\n$$m_\\lambda^\\star \\exp(m_\\lambda^\\star) = \\frac{2 r^2}{\\lambda}$$\nThis equation is of the form $u \\exp(u) = v$, where $u = m_\\lambda^\\star$ and $v = \\frac{2 r^2}{\\lambda}$. The solution to such an equation is given by the Lambert W function, $W(v)$. The Lambert W function is defined as the inverse of the function $f(u) = u \\exp(u)$.\nSince $r^2  0$ and $\\lambda  0$, the argument $\\frac{2 r^2}{\\lambda}$ is positive. For positive arguments, the principal branch of the Lambert W function, denoted $W_0$ or simply $W$, provides a unique positive real solution.\nTherefore, the optimal margin is:\n$$m_\\lambda^\\star = W\\left(\\frac{2 r^2}{\\lambda}\\right)$$\nThis is the exact, closed-form analytical expression for the limiting signed margin as a function of $r$ and $\\lambda$.", "answer": "$$\\boxed{W\\left(\\frac{2r^2}{\\lambda}\\right)}$$", "id": "3169285"}, {"introduction": "Data augmentation is a powerful regularization technique that artificially expands the training set, but its effectiveness is not unlimited. This practice provides a quantitative framework for understanding the returns on your augmentation effort by modeling the correlation $\\rho$ between generated samples. You will derive the 'effective sample size' $N_{\\text{eff}}$, a concept that reveals how statistical dependence limits the benefit of creating more data and provides a concrete way to reason about the power of augmentation [@problem_id:3169320].", "problem": "You are training a deep neural network classifier under the Empirical Risk Minimization (ERM) principle, where the target population risk is the expected loss $L = \\mathbb{E}[\\ell(f_{\\theta}(x), y)]$ and the empirical risk estimator is constructed from the training data. To reduce overfitting, you apply label-preserving data augmentation to each original example.\n\nAssume there are $n$ original examples $(x_{i}, y_{i})$ drawn independently and identically distributed (i.i.d.) from an unknown distribution. For each original example $i \\in \\{1, \\ldots, n\\}$, you generate $K$ augmented variants via transformations $T_{i,1}, \\ldots, T_{i,K}$ from a fixed augmentation pool, and you evaluate the loss random variables\n$$\nZ_{i,k} = \\ell\\big(f_{\\theta}(T_{i,k}(x_{i})), y_{i}\\big)\n$$\nfor $k \\in \\{1, \\ldots, K\\}$. Model these losses with the following properties:\n- For all $i$ and $k$, $\\mathbb{E}[Z_{i,k}] = \\mu$ and $\\operatorname{Var}(Z_{i,k}) = \\sigma^{2}$.\n- For a fixed $i$, the correlation coefficient between $Z_{i,k}$ and $Z_{i,k'}$ for $k \\neq k'$ is a constant $\\rho$ with $0 \\leq \\rho  1$. For $i \\neq j$, the random variables $Z_{i,k}$ and $Z_{j,k'}$ are independent.\n\nThe empirical risk used for training is the average over all augmented losses:\n$$\n\\hat{L} = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}.\n$$\n\nDefine the effective sample size $N_{\\mathrm{eff}}(K)$ as the number of independent loss samples, each with variance $\\sigma^{2}$, whose empirical mean would have the same variance as $\\hat{L}$. Using only the assumptions provided and standard properties of variance and covariance, derive a closed-form analytic expression for $N_{\\mathrm{eff}}(K)$ in terms of $n$, $K$, and $\\rho$. Provide your final answer as a single analytic expression. No rounding is required.", "solution": "The goal is to derive the effective sample size $N_{\\mathrm{eff}}(K)$, which is defined by the relation $\\operatorname{Var}(\\hat{L}) = \\frac{\\sigma^2}{N_{\\mathrm{eff}}(K)}$. To find $N_{\\mathrm{eff}}(K)$, we must first compute the variance of the empirical risk estimator, $\\operatorname{Var}(\\hat{L})$.\n\nThe estimator is $\\hat{L} = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}$.\nUsing the property of variance, $\\operatorname{Var}(c X) = c^2 \\operatorname{Var}(X)$, we get:\n$$\n\\operatorname{Var}(\\hat{L}) = \\frac{1}{(nK)^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}\\right)\n$$\nLet's define $S_i = \\sum_{k=1}^{K} Z_{i,k}$ as the sum of losses for each original example $i$. The problem states that for $i \\neq j$, $Z_{i,k}$ and $Z_{j,k'}$ are independent. This implies that $S_i$ and $S_j$ are also independent for $i \\neq j$. Therefore, the variance of the total sum is the sum of the variances:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} S_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(S_i) = n \\operatorname{Var}(S_1)\n$$\nwhere the last equality holds because all $S_i$ are identically distributed.\n\nNow we compute the variance of $S_i$ for a fixed $i$. The variables $Z_{i,1}, \\ldots, Z_{i,K}$ are correlated. The variance of their sum is the sum of all elements in their covariance matrix:\n$$\n\\operatorname{Var}(S_i) = \\operatorname{Var}\\left(\\sum_{k=1}^{K} Z_{i,k}\\right) = \\sum_{k=1}^{K} \\sum_{k'=1}^{K} \\operatorname{Cov}(Z_{i,k}, Z_{i,k'})\n$$\nThis sum consists of $K$ variance terms (where $k=k'$) and $K(K-1)$ covariance terms (where $k \\neq k'$).\n- For $k=k'$, $\\operatorname{Cov}(Z_{i,k}, Z_{i,k}) = \\operatorname{Var}(Z_{i,k}) = \\sigma^2$.\n- For $k \\neq k'$, $\\operatorname{Cov}(Z_{i,k}, Z_{i,k'}) = \\rho \\sqrt{\\operatorname{Var}(Z_{i,k})\\operatorname{Var}(Z_{i,k'})} = \\rho \\sigma^2$.\n\nCombining these parts, we get:\n$$\n\\operatorname{Var}(S_i) = K \\sigma^2 + K(K-1) \\rho \\sigma^2 = K\\sigma^2 \\big(1 + (K-1)\\rho\\big)\n$$\nSubstituting this back into the expression for $\\operatorname{Var}(\\hat{L})$:\n$$\n\\operatorname{Var}(\\hat{L}) = \\frac{1}{(nK)^2} \\left[ n \\cdot K\\sigma^2 \\big(1 + (K-1)\\rho\\big) \\right] = \\frac{\\sigma^2 \\big(1 + (K-1)\\rho\\big)}{nK}\n$$\nFinally, we solve for the effective sample size $N_{\\mathrm{eff}}(K)$:\n$$\nN_{\\mathrm{eff}}(K) = \\frac{\\sigma^2}{\\operatorname{Var}(\\hat{L})} = \\frac{\\sigma^2}{\\frac{\\sigma^2 \\big(1 + (K-1)\\rho\\big)}{nK}} = \\frac{nK}{1 + (K-1)\\rho}\n$$\nThis expression quantifies the diminishing returns of adding highly correlated augmented data.", "answer": "$$ \\boxed{\\frac{nK}{1 + (K-1)\\rho}} $$", "id": "3169320"}, {"introduction": "In practice, deep learning models often benefit from combining multiple regularization techniques, such as Cutout and Mixup, but their interactions can be complex. This hands-on coding challenge asks you to move from theory to practice by designing an experiment to investigate whether these two popular augmentation methods work together in a complementary fashion or introduce competing biases. By implementing the full experimental pipeline from data generation to evaluation [@problem_id:3169254], you will build crucial skills in empirical validation and gain insight into the practical art of combining regularizers.", "problem": "You are given a synthetic binary classification task and asked to analyze, from first principles, whether combining two data augmentation strategies—cutout and Mixup—introduces competing biases or complementary smoothing of decision boundaries. You must implement a complete program that generates the dataset, applies augmentations, trains a linear model under empirical risk minimization, and returns a boolean decision for each test case according to a precisely defined criterion.\n\nBase definitions and setup:\n- Consider Empirical Risk Minimization (ERM), which selects parameters to minimize the empirical loss over training samples. Use squared loss with ridge regularization for a linear predictor.\n- Let the input dimension be $d = 16$. Let the number of training samples be $n_{\\text{train}} = 240$ and the number of test samples be $n_{\\text{test}} = 240$.\n- Let the class labels be $y \\in \\{-1, +1\\}$.\n- Let the two classes be generated from multivariate normal distributions with means $\\mu_{+}$ and $\\mu_{-}$ and isotropic covariance. Specifically, set $\\sigma = 0.8$ and define $\\mu_{+}$ to have its first $k = 6$ components equal to $\\delta = 1.2$ and the remaining components equal to $0$, i.e., $\\mu_{+} = (\\underbrace{\\delta, \\dots, \\delta}_{k}, \\underbrace{0, \\dots, 0}_{d-k})$. Set $\\mu_{-} = -\\mu_{+}$. For each class, draw $n_{\\text{train}}/2$ training samples and $n_{\\text{test}}/2$ test samples from $\\mathcal{N}(\\mu_{\\pm}, \\sigma^{2} I_{d})$, where $I_{d}$ is the $d \\times d$ identity matrix.\n\nModel and training:\n- Use a linear predictor $f(x) = w^{\\top} x$ trained by ridge regression. Given a design matrix $X \\in \\mathbb{R}^{n \\times d}$ and targets $Y \\in \\mathbb{R}^{n}$, select $w \\in \\mathbb{R}^{d}$ to minimize the regularized squared empirical loss\n$$\n\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w^{\\top} x_{i} - y_{i}\\right)^{2} + \\lambda \\|w\\|_{2}^{2},\n$$\nwhere $\\lambda = 0.1$. Use the closed-form solution to the normal equations for ridge regression:\n$$\nw = \\left(X^{\\top} X + \\lambda I_{d}\\right)^{-1} X^{\\top} Y.\n$$\n\nAugmentations:\n- Cutout with length $c$: For each training sample $x \\in \\mathbb{R}^{d}$, choose a random start index $s$ uniformly from $\\{0, 1, \\dots, d-c\\}$ and set components $x_{s}, x_{s+1}, \\dots, x_{s+c-1}$ to $0$. If $c = 0$, cutout is disabled.\n- Mixup with parameter $\\alpha$: For each training sample, draw $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ (only if $\\alpha  0$), choose a random partner sample $(x_{j}, y_{j})$, and form a mixed sample\n$$\nx' = \\lambda x_{i} + (1 - \\lambda) x_{j}, \\quad y' = \\lambda y_{i} + (1 - \\lambda) y_{j}.\n$$\nConstruct a mixed dataset of the same size $n_{\\text{train}}$ as the original training set. If $\\alpha \\leq 0$, Mixup is disabled. When both augmentations are active, apply cutout first to the original features and then perform Mixup on the cutout-transformed features.\n\nEvaluation metrics:\n- Decision performance: Compute the misclassification rate on the test set as a decimal in $[0,1]$. Use the classifier $\\hat{y} = \\text{sign}(w^{\\top} x)$ and compare to true labels $y \\in \\{-1, +1\\}$.\n- Smoothing proxy: Use the Euclidean norm $\\|w\\|_{2}$ as a proxy for the Lipschitz constant of the linear predictor; smaller $\\|w\\|_{2}$ indicates a smoother decision function.\n\nComplementarity decision rule:\n- For a given pair $(c, \\alpha)$, compute results for three training configurations:\n    1. Cutout only: cutout length $c$, Mixup disabled.\n    2. Mixup only: Mixup with parameter $\\alpha$, cutout disabled.\n    3. Both: cutout length $c$ and Mixup with parameter $\\alpha$ applied in sequence (cutout then Mixup).\n- Let $e_{\\text{cut}}$, $e_{\\text{mix}}$, $e_{\\text{both}}$ denote the test misclassification rates, and $n_{\\text{cut}}$, $n_{\\text{mix}}$, $n_{\\text{both}}$ denote the corresponding $\\|w\\|_{2}$ norms. Let $e_{\\text{single}}^{\\min} = \\min(e_{\\text{cut}}, e_{\\text{mix}})$ and $n_{\\text{single}}^{\\min} = \\min(n_{\\text{cut}}, n_{\\text{mix}})$. With tolerances $\\tau_{e} = 10^{-3}$ and $\\tau_{n} = 10^{-6}$, declare the boolean\n$$\n\\mathcal{C} = \\left( e_{\\text{both}} \\leq e_{\\text{single}}^{\\min} + \\tau_{e} \\right) \\wedge \\left( n_{\\text{both}} \\leq n_{\\text{single}}^{\\min} + \\tau_{n} \\right).\n$$\nIf $\\mathcal{C}$ is true, interpret this as complementary smoothing; otherwise interpret it as competing biases.\n\nTest suite:\n- Use the following $(c, \\alpha)$ pairs:\n    1. $(0, 0.0)$,\n    2. $(4, 0.0)$,\n    3. $(0, 0.4)$,\n    4. $(4, 0.4)$,\n    5. $(10, 0.4)$,\n    6. $(4, 4.0)$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\"[result1,result2,result3]\"$), where each element is a boolean corresponding to $\\mathcal{C}$ for a test case, in the same order as listed above. No additional text should be printed.", "solution": "The solution requires implementing a computational experiment to test the hypothesis of complementarity between cutout and Mixup augmentations. The implementation follows the problem specification precisely.\n\n**1. Data Generation:** A synthetic dataset is created with two classes drawn from multivariate normal distributions with specified means and covariance. A fixed random seed ensures reproducibility.\n\n**2. Augmentation Functions:** Two functions are implemented:\n- **Cutout:** A function that takes a data matrix and cutout length `c`. For each sample, it randomly selects a contiguous block of `c` features and sets them to zero.\n- **Mixup:** A function that takes a data matrix, labels, and Mixup parameter `α`. It creates a new dataset of the same size by forming convex combinations of pairs of samples and their labels, with the mixing coefficient drawn from a Beta distribution `Beta(α, α)`.\n\n**3. Training and Evaluation:** A linear model is trained using the closed-form solution for ridge regression: $w = (X^{\\top}X + \\lambda I_d)^{-1} X^{\\top}Y$. The model's performance is evaluated on an un-augmented test set by computing two metrics: the misclassification rate and the Euclidean norm of the weight vector, $\\|w\\|_2$.\n\n**4. Experimental Protocol:** For each `(c, α)` pair in the test suite, three models are trained and evaluated:\n- **Cutout-only:** Using data with only cutout applied (`c > 0`, `α = 0`).\n- **Mixup-only:** Using data with only Mixup applied (`c = 0`, `α > 0`).\n- **Both:** Using data with cutout applied first, followed by Mixup.\n\n**5. Complementarity Decision:** The results are compared using the specified rule: the combination is \"complementary\" if both its test error and weight norm are less than or equal to the minimum of the corresponding metrics from the single-augmentation models (within specified tolerances). The program computes and outputs a boolean value for each test case based on this rule.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import special\n\ndef solve():\n    # Define problem parameters\n    D = 16\n    N_TRAIN = 240\n    N_TEST = 240\n    K_FEATURES = 6\n    DELTA = 1.2\n    SIGMA = 0.8\n    LAMBDA_REG = 0.1\n    TAU_E = 1e-3\n    TAU_N = 1e-6\n    SEED = 42\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 0.0),\n        (4, 0.0),\n        (0, 0.4),\n        (4, 0.4),\n        (10, 0.4),\n        (4, 4.0),\n    ]\n\n    rng = np.random.default_rng(seed=SEED)\n\n    def generate_data():\n        \"\"\"Generates the base synthetic dataset.\"\"\"\n        mu_plus = np.zeros(D)\n        mu_plus[:K_FEATURES] = DELTA\n        mu_minus = -mu_plus\n        cov = (SIGMA**2) * np.identity(D)\n\n        n_per_class_train = N_TRAIN // 2\n        n_per_class_test = N_TEST // 2\n\n        X_train_plus = rng.multivariate_normal(mu_plus, cov, size=n_per_class_train)\n        X_train_minus = rng.multivariate_normal(mu_minus, cov, size=n_per_class_train)\n        X_train = np.vstack((X_train_plus, X_train_minus))\n        Y_train = np.array([1] * n_per_class_train + [-1] * n_per_class_train)\n\n        X_test_plus = rng.multivariate_normal(mu_plus, cov, size=n_per_class_test)\n        X_test_minus = rng.multivariate_normal(mu_minus, cov, size=n_per_class_test)\n        X_test = np.vstack((X_test_plus, X_test_minus))\n        Y_test = np.array([1] * n_per_class_test + [-1] * n_per_class_test)\n\n        return X_train, Y_train, X_test, Y_test\n\n    def apply_cutout(X, c, local_rng):\n        \"\"\"Applies cutout augmentation.\"\"\"\n        if c == 0:\n            return X\n        X_aug = X.copy()\n        for i in range(X_aug.shape[0]):\n            s = local_rng.integers(0, D - c + 1)\n            X_aug[i, s:s+c] = 0\n        return X_aug\n\n    def apply_mixup(X, Y, alpha, local_rng):\n        \"\"\"Applies Mixup augmentation.\"\"\"\n        if alpha = 0:\n            return X, Y\n        n_samples = X.shape[0]\n        X_aug = np.zeros_like(X)\n        Y_aug = np.zeros_like(Y, dtype=float)\n        \n        partner_indices = local_rng.permutation(n_samples)\n        \n        for i in range(n_samples):\n            lambda_mix = local_rng.beta(alpha, alpha)\n            j = partner_indices[i]\n            \n            X_aug[i] = lambda_mix * X[i] + (1 - lambda_mix) * X[j]\n            Y_aug[i] = lambda_mix * Y[i] + (1 - lambda_mix) * Y[j]\n            \n        return X_aug, Y_aug\n\n    def train_ridge(X, Y, lambda_reg):\n        \"\"\"Trains a ridge regression model using the closed-form solution.\"\"\"\n        d = X.shape[1]\n        I = np.identity(d)\n        # Using the formula w = (X^T X + lambda * I_d)^-1 X^T Y\n        term1 = np.linalg.inv(X.T @ X + lambda_reg * I)\n        term2 = X.T @ Y\n        w = term1 @ term2\n        return w\n\n    def evaluate_model(w, X_test, Y_test):\n        \"\"\"Evaluates the model's performance and weight norm.\"\"\"\n        # Misclassification rate\n        Y_pred_scores = X_test @ w\n        Y_pred_labels = np.sign(Y_pred_scores)\n        Y_pred_labels[Y_pred_labels == 0] = 1 # Handle zeros\n        misclassification_rate = np.mean(Y_pred_labels != Y_test)\n        \n        # L2 norm of weights\n        weight_norm = np.linalg.norm(w)\n        \n        return misclassification_rate, weight_norm\n\n    def run_experiment(c, alpha, base_data, local_rng):\n        \"\"\"Runs a single experiment for a given (c, alpha) configuration.\"\"\"\n        X_train_base, Y_train_base, X_test, Y_test = base_data\n        \n        # Apply cutout\n        X_cut = apply_cutout(X_train_base, c, local_rng)\n        \n        # Apply mixup (on cutout-transformed data)\n        X_aug, Y_aug = apply_mixup(X_cut, Y_train_base, alpha, local_rng)\n        \n        # Train model\n        w = train_ridge(X_aug, Y_aug, LAMBDA_REG)\n        \n        # Evaluate\n        error, norm = evaluate_model(w, X_test, Y_test)\n        \n        return error, norm\n\n    # Generate the base dataset once for all experiments\n    base_dataset = generate_data()\n    \n    results = []\n    for c_val, alpha_val in test_cases:\n        # 1. Cutout only\n        e_cut, n_cut = run_experiment(c=c_val, alpha=0.0, base_data=base_dataset, local_rng=rng)\n        \n        # 2. Mixup only\n        e_mix, n_mix = run_experiment(c=0, alpha=alpha_val, base_data=base_dataset, local_rng=rng)\n        \n        # 3. Both augmentations\n        e_both, n_both = run_experiment(c=c_val, alpha=alpha_val, base_data=base_dataset, local_rng=rng)\n        \n        # Complementarity decision rule\n        e_single_min = min(e_cut, e_mix)\n        n_single_min = min(n_cut, n_mix)\n        \n        is_complementary = (e_both = e_single_min + TAU_E) and (n_both = n_single_min + TAU_N)\n        results.append(is_complementary)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3169254"}]}