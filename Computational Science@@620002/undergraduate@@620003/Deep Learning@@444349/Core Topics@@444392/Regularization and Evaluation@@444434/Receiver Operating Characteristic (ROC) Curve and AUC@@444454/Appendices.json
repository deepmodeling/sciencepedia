{"hands_on_practices": [{"introduction": "Before we compute the Area Under the Curve (AUC) from data, it is crucial to understand what this metric fundamentally represents. This exercise guides you through a theoretical derivation to reveal the probabilistic meaning of AUC in an idealized setting, where a classifier's scores for positive and negative classes are modeled by Gaussian distributions [@problem_id:3167175]. By working through this, you will see how the AUC, often presented as a geometric area, is equivalent to the probability that the classifier correctly ranks a randomly chosen positive example above a randomly chosen negative one, providing a powerful and intuitive interpretation of model performance.", "problem": "A binary classifier from a deep neural network outputs a real-valued score $s$ used for threshold-based decisions. Assume the class-conditional score distributions are Gaussian but have different variances (heteroscedastic): $s \\mid y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$ and $s \\mid y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, with independent draws across examples and between classes. Starting from core definitions of the Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC), derive a closed-form expression for the AUC in terms of the standard normal cumulative distribution function and show how an intermediate representation using Owen’s $T$ function arises when one evaluates the ROC integral. Then, numerically validate the closed-form by evaluating the AUC for parameters $\\mu_{1} = 1.5$, $\\sigma_{1} = 0.6$, $\\mu_{0} = 0.5$, and $\\sigma_{0} = 0.8$. Round your final numerical answer to four significant figures.\n\nUse only the following foundations:\n- The definition of the ROC curve: for threshold $t$, the true positive rate is $\\mathrm{TPR}(t) = \\mathbb{P}(s \\ge t \\mid y=1)$ and the false positive rate is $\\mathrm{FPR}(t) = \\mathbb{P}(s \\ge t \\mid y=0)$.\n- The definition of AUC as the area under $\\mathrm{TPR}$ versus $\\mathrm{FPR}$.\n- Well-tested facts about Gaussian distributions and the standard normal cumulative distribution function $\\Phi(\\cdot)$ and density $\\phi(\\cdot)$.\n- The definition of Owen’s $T$ function as an integral transform related to bivariate normal probabilities.\n\nExpress the final AUC as a single real number. No units are required. Round your answer to four significant figures.", "solution": "The problem asks for a closed-form expression for the Area Under the Curve (AUC) of a Receiver Operating Characteristic (ROC) curve, given that the classifier scores for the two classes follow heteroscedastic Gaussian distributions. It also requires an explanation of how Owen's $T$ function arises in an intermediate step of the derivation, and finally, a numerical evaluation of the AUC for a given set of parameters.\n\nFirst, we formalize the problem based on the provided definitions. The classifier score is a random variable $s$. The class-conditional distributions are given as:\nFor the positive class ($y=1$): $S_1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$\nFor the negative class ($y=0$): $S_0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$\n\nThe ROC curve is parameterized by a threshold $t$. The True Positive Rate (TPR) and False Positive Rate (FPR) are defined as:\n$\\mathrm{TPR}(t) = \\mathbb{P}(s \\ge t \\mid y=1) = \\mathbb{P}(S_1 \\ge t)$\n$\\mathrm{FPR}(t) = \\mathbb{P}(s \\ge t \\mid y=0) = \\mathbb{P}(S_0 \\ge t)$\n\nWe can express these probabilities using the standard normal cumulative distribution function (CDF), $\\Phi(z) = \\mathbb{P}(Z \\le z)$ where $Z \\sim \\mathcal{N}(0,1)$.\nFor a general Gaussian variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the probability $\\mathbb{P}(X \\ge t)$ is:\n$\\mathbb{P}(X \\ge t) = 1 - \\mathbb{P}(X < t) = 1 - \\mathbb{P}\\left(\\frac{X-\\mu}{\\sigma} < \\frac{t-\\mu}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{t-\\mu}{\\sigma}\\right)$.\nUsing the symmetry property of the normal distribution, $1 - \\Phi(z) = \\Phi(-z)$, we get:\n$\\mathbb{P}(X \\ge t) = \\Phi\\left(-\\frac{t-\\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu-t}{\\sigma}\\right)$.\n\nApplying this to our specific distributions:\n$\\mathrm{TPR}(t) = \\Phi\\left(\\frac{\\mu_{1}-t}{\\sigma_{1}}\\right)$\n$\\mathrm{FPR}(t) = \\Phi\\left(\\frac{\\mu_{0}-t}{\\sigma_{0}}\\right)$\n\nThe AUC is the area under the curve formed by plotting $\\mathrm{TPR}$ against $\\mathrm{FPR}$. As the threshold $t$ sweeps from $+\\infty$ to $-\\infty$, $\\mathrm{FPR}(t)$ sweeps from $0$ to $1$. The AUC can be calculated by integrating $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$:\n$$ \\mathrm{AUC} = \\int_0^1 \\mathrm{TPR}(\\mathrm{FPR}) \\, d(\\mathrm{FPR}) $$\nWe can evaluate this integral by parameterizing it with the threshold $t$:\n$$ \\mathrm{AUC} = \\int_{t=+\\infty}^{t=-\\infty} \\mathrm{TPR}(t) \\frac{d(\\mathrm{FPR}(t))}{dt} dt = - \\int_{-\\infty}^{+\\infty} \\mathrm{TPR}(t) \\frac{d(\\mathrm{FPR}(t))}{dt} dt $$\nThe derivative of $\\mathrm{FPR}(t)$ with respect to $t$ is, using the chain rule $\\frac{d}{dx}\\Phi(f(x)) = \\phi(f(x))f'(x)$:\n$$ \\frac{d}{dt}\\mathrm{FPR}(t) = \\frac{d}{dt}\\Phi\\left(\\frac{\\mu_{0}-t}{\\sigma_{0}}\\right) = \\phi\\left(\\frac{\\mu_{0}-t}{\\sigma_{0}}\\right) \\cdot \\left(-\\frac{1}{\\sigma_{0}}\\right) = -\\frac{1}{\\sigma_{0}}\\phi\\left(\\frac{t-\\mu_{0}}{\\sigma_{0}}\\right) $$\nwhere $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ is the standard normal probability density function (PDF).\n\nSubstituting these expressions into the integral for AUC:\n$$ \\mathrm{AUC} = - \\int_{-\\infty}^{+\\infty} \\Phi\\left(\\frac{\\mu_{1}-t}{\\sigma_{1}}\\right) \\left(-\\frac{1}{\\sigma_{0}}\\phi\\left(\\frac{t-\\mu_{0}}{\\sigma_{0}}\\right)\\right) dt $$\n$$ \\mathrm{AUC} = \\int_{-\\infty}^{+\\infty} \\Phi\\left(\\frac{\\mu_{1}-t}{\\sigma_{1}}\\right) \\frac{1}{\\sigma_{0}}\\phi\\left(\\frac{t-\\mu_{0}}{\\sigma_{0}}\\right) dt $$\nTo solve this integral, we perform a change of variables. Let $u = \\frac{t-\\mu_{0}}{\\sigma_{0}}$. This implies $t = \\sigma_{0}u + \\mu_{0}$ and $dt = \\sigma_{0}du$. The limits of integration remain $(-\\infty, +\\infty)$.\nThe argument of the $\\Phi$ function becomes:\n$$ \\frac{\\mu_{1}-t}{\\sigma_{1}} = \\frac{\\mu_{1}-(\\sigma_{0}u + \\mu_{0})}{\\sigma_{1}} = \\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}} - \\frac{\\sigma_{0}}{\\sigma_{1}}u $$\nThe integral for AUC transforms into:\n$$ \\mathrm{AUC} = \\int_{-\\infty}^{+\\infty} \\Phi\\left(\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}} - \\frac{\\sigma_{0}}{\\sigma_{1}}u\\right) \\frac{1}{\\sigma_{0}}\\phi(u) (\\sigma_{0}du) = \\int_{-\\infty}^{+\\infty} \\Phi(a + bu) \\phi(u) du $$\nwhere we have defined the constants $a = \\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}}$ and $b = -\\frac{\\sigma_{0}}{\\sigma_{1}}$.\n\nThis integral has a well-known closed form. A direct way to evaluate it is through a probabilistic interpretation. The integral represents the probability $\\mathbb{P}(Z_1 \\le a + b Z_0)$, where $Z_0, Z_1$ are independent standard normal random variables. This is because by conditioning on $Z_0=u$, the probability is $\\mathbb{P}(Z_1 \\le a+bu) = \\Phi(a+bu)$, and integrating over all possible values of $u$ weighted by its PDF $\\phi(u)$ gives the total probability.\nWe can rearrange the inequality as $\\mathbb{P}(Z_1 - bZ_0 \\le a)$.\nLet $W = Z_1 - bZ_0$. Since $Z_1$ and $Z_0$ are independent Gaussian variables, $W$ is also Gaussian.\nThe mean of $W$ is $\\mathbb{E}[W] = \\mathbb{E}[Z_1] - b\\mathbb{E}[Z_0] = 0 - b \\cdot 0 = 0$.\nThe variance of $W$ is $\\mathrm{Var}(W) = \\mathrm{Var}(Z_1) + (-b)^2\\mathrm{Var}(Z_0) = 1 + b^2$.\nSo, $W \\sim \\mathcal{N}(0, 1+b^2)$. The probability becomes:\n$$ \\mathbb{P}(W \\le a) = \\Phi\\left(\\frac{a - \\mathbb{E}[W]}{\\sqrt{\\mathrm{Var}(W)}}\\right) = \\Phi\\left(\\frac{a}{\\sqrt{1+b^2}}\\right) $$\nSubstituting back the expressions for $a$ and $b$:\n$$ \\frac{a}{\\sqrt{1+b^2}} = \\frac{(\\mu_1-\\mu_0)/\\sigma_1}{\\sqrt{1 + (-\\sigma_0/\\sigma_1)^2}} = \\frac{(\\mu_1-\\mu_0)/\\sigma_1}{\\sqrt{1 + \\sigma_0^2/\\sigma_1^2}} = \\frac{(\\mu_1-\\mu_0)/\\sigma_1}{\\sqrt{(\\sigma_1^2+\\sigma_0^2)/\\sigma_1^2}} = \\frac{(\\mu_1-\\mu_0)/\\sigma_1}{(\\sqrt{\\sigma_1^2+\\sigma_0^2})/\\sigma_1} $$\n$$ \\frac{a}{\\sqrt{1+b^2}} = \\frac{\\mu_1-\\mu_0}{\\sqrt{\\sigma_1^2+\\sigma_0^2}} $$\nThus, the closed-form expression for the AUC is:\n$$ \\mathrm{AUC} = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sqrt{\\sigma_1^2 + \\sigma_0^2}}\\right) $$\nThis result has a direct interpretation: the AUC is the probability that a score drawn from the positive class is greater than a score drawn from the negative class, $\\mathbb{P}(S_1 > S_0)$. The difference $D = S_1 - S_0$ is distributed as $\\mathcal{N}(\\mu_1-\\mu_0, \\sigma_1^2+\\sigma_0^2)$, and $\\mathbb{P}(D>0)$ yields the same expression.\n\nNow, we address the part of the problem concerning Owen's $T$ function. This function arises when the integral $\\int_{-\\infty}^{+\\infty} \\Phi(a + bu) \\phi(u) du$ is split at $u=0$:\n$$ \\mathrm{AUC} = \\int_{-\\infty}^{0} \\Phi(a + bu) \\phi(u) du + \\int_{0}^{+\\infty} \\Phi(a + bu) \\phi(u) du $$\nThe second term, an integral over the positive half-axis, is directly related to Owen's $T$ function, typically defined as $T(h,k) = \\frac{1}{2\\pi}\\int_0^k \\frac{\\exp(-\\frac{1}{2}h^2(1+x^2))}{1+x^2} dx$. A known identity states:\n$$ \\int_0^\\infty \\phi(u)\\Phi(k+hu)du = \\frac{1}{2}\\Phi(k) + T(k,h) $$\nApplying this to our integral part (with $k=a, h=b$), we get $\\int_{0}^{+\\infty} \\Phi(a + bu) \\phi(u) du = \\frac{1}{2}\\Phi(a) + T(a,b)$.\nFor the first term, we change variables $v=-u$:\n$$ \\int_{-\\infty}^{0} \\Phi(a + bu) \\phi(u) du = \\int_{+\\infty}^{0} \\Phi(a - bv) \\phi(-v) (-dv) = \\int_{0}^{+\\infty} \\Phi(a - bv) \\phi(v) dv $$\nApplying the identity again (with $k=a, h=-b$) yields $\\frac{1}{2}\\Phi(a) + T(a,-b)$.\nSumming these two parts gives an intermediate representation for the AUC:\n$$ \\mathrm{AUC} = \\left(\\frac{1}{2}\\Phi(a)+T(a,b)\\right) + \\left(\\frac{1}{2}\\Phi(a)+T(a,-b)\\right) = \\Phi(a) + T(a,b) + T(a,-b) $$\nThis shows how the $T$ function arises. While simplifying this expression requires careful application of identities relating $T(a,b)$ and $T(a,-b)$, the summation ultimately resolves to the same closed-form result, $\\Phi\\left(\\frac{a}{\\sqrt{1+b^2}}\\right)$, consistent with the more direct derivation.\n\nFinally, we numerically validate the closed-form expression with the given parameters:\n$\\mu_{1} = 1.5$\n$\\sigma_{1} = 0.6 \\implies \\sigma_1^2 = 0.36$\n$\\mu_{0} = 0.5$\n$\\sigma_{0} = 0.8 \\implies \\sigma_0^2 = 0.64$\n\nThe argument of $\\Phi$ is:\n$$ \\frac{\\mu_1 - \\mu_0}{\\sqrt{\\sigma_1^2 + \\sigma_0^2}} = \\frac{1.5 - 0.5}{\\sqrt{0.36 + 0.64}} = \\frac{1.0}{\\sqrt{1.0}} = 1.0 $$\nTherefore, the AUC is:\n$$ \\mathrm{AUC} = \\Phi(1.0) $$\nThe value of the standard normal CDF at $z=1.0$ is approximately $0.8413447$. Rounding to four significant figures gives $0.8413$.", "answer": "$$\\boxed{0.8413}$$", "id": "3167175"}, {"introduction": "While theoretical derivations provide insight, real-world machine learning involves working with finite datasets that yield a discrete set of points on the Receiver Operating Characteristic (ROC) curve. This practice provides the essential hands-on experience of computing the AUC from these discrete points using the trapezoidal rule [@problem_id:3284361]. You will implement a robust algorithm that includes crucial preprocessing steps, such as sorting the points and constructing the curve's upper envelope, to accurately approximate the area under the piecewise-linear curve.", "problem": "You are given discrete receiver operating characteristic (ROC) outputs as a finite set of points $\\{(\\mathrm{FPR}_i,\\mathrm{TPR}_i)\\}_{i=1}^m$, where $\\mathrm{FPR}$ denotes the false positive rate and $\\mathrm{TPR}$ denotes the true positive rate. The Area Under the ROC Curve (AUC) is the area under the graph of the function $\\mathrm{TPR}(\\mathrm{FPR})$ over the interval $[0,1]$, which can be formalized as the Riemann integral $\\int_{0}^{1} \\mathrm{TPR}(x)\\,dx$ when $\\mathrm{TPR}$ is regarded as a function of $x=\\mathrm{FPR}$. In practice, only discrete samples are available.\n\nYour task is to write a complete program that approximates this area using the trapezoidal rule derived from first principles of Riemann sums and piecewise-linear approximation, applied to a preprocessed version of the given points. The preprocessing must be as follows:\n- Include endpoints: ensure that $(0,0)$ and $(1,1)$ are included. If either endpoint is absent, it must be added.\n- Resolve duplicate abscissae: if multiple points share the same false positive rate $x$ value, retain only the point with the maximum true positive rate at that $x$ (this constructs the upper envelope at repeated abscissae).\n- Sort by abscissa: sort the remaining points by $\\mathrm{FPR}$ in nondecreasing order so that the abscissae form a valid partition of $[0,1]$.\n\nAssume that all provided coordinates satisfy $0 \\le \\mathrm{FPR} \\le 1$ and $0 \\le \\mathrm{TPR} \\le 1$. Use the trapezoidal rule on the sorted sequence to approximate $\\int_{0}^{1} \\mathrm{TPR}(x)\\,dx$ by summing the areas of trapezoids under consecutive line segments. The final AUC for each test case must be rounded to six decimal places.\n\nTest suite (each test case is a list of points $(\\mathrm{FPR},\\mathrm{TPR})$):\n- Case $1$ (general monotone case): $[(0,0),(0.2,0.5),(0.6,0.8),(1,1)]$.\n- Case $2$ (unsorted inputs): $[(1,1),(0.3,0.6),(0,0),(0.7,0.9)]$.\n- Case $3$ (duplicate abscissae, take maximum $\\mathrm{TPR}$): $[(0,0),(0.5,0.4),(0.5,0.6),(1,1)]$.\n- Case $4$ (vertical rise at zero, perfect classifier envelope): $[(0,0),(0,1),(1,1)]$.\n- Case $5$ (degenerate classifier): $[(0,0),(1,0)]$.\n- Case $6$ (missing endpoints, require augmentation): $[(0.2,0.3),(0.4,0.6),(0.9,0.95)]$.\n\nYour program must compute the AUC for each case using only the procedure described above, then print a single line containing a list of the six AUC values in the same order as the cases above, rounded to six decimal places. The output format must be exactly a single line containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,\\dots,r_6]$ where each $r_i$ is a floating-point number rounded to six decimal places. No units are involved in this problem; the output is unitless.", "solution": "The problem requires the computation of the Area Under the Receiver Operating Characteristic Curve (AUC) from a discrete set of points $\\{(\\mathrm{FPR}_i, \\mathrm{TPR}_i)\\}$. The AUC is defined as the definite integral of the True Positive Rate ($\\mathrm{TPR}$) with respect to the False Positive Rate ($\\mathrm{FPR}$) over the interval $[0, 1]$.\n$$\n\\mathrm{AUC} = \\int_{0}^{1} \\mathrm{TPR}(x) \\,dx\n$$\nwhere $x$ represents the $\\mathrm{FPR}$. Since the function $\\mathrm{TPR}(x)$ is only known at a finite number of points, the integral must be approximated numerically. The problem specifies the use of the trapezoidal rule, which is derived from a piecewise-linear approximation of the function.\n\nThe solution proceeds in two main stages: data preprocessing followed by numerical integration.\n\n**1. Data Preprocessing**\n\nBefore applying the trapezoidal rule, the raw data points must be processed to form a well-defined, monotonic function on a partition of the interval $[0, 1]$. This involves three sequential steps as stipulated.\n\n**Step 1.1: Endpoint Augmentation**\nThe integral is defined over the domain $[0, 1]$. To ensure the entire domain is covered, the standard endpoints of an ROC curve, $(0, 0)$ and $(1, 1)$, must be part of the dataset. The point $(0, 0)$ corresponds to a classifier threshold that never classifies an instance as positive, resulting in zero true positives and zero false positives. The point $(1, 1)$ corresponds to a threshold that always classifies an instance as positive, resulting in all true positives and all false positives. Therefore, we augment the initial set of points by including $(0, 0)$ and $(1, 1)$.\n\n**Step 1.2: Resolution of Duplicate Abscissae**\nA function must have a single value for each input. If multiple points share the same $\\mathrm{FPR}$ value, they represent different classifier performances at the same false positive cost. The ROC curve itself is defined as the upper envelope of all possible $(\\mathrm{FPR}, \\mathrm{TPR})$ pairs. Consequently, for any given $\\mathrm{FPR}$ value $x_i$, we must select the point with the maximum corresponding $\\mathrm{TPR}$ value. This is achieved by grouping points by their $\\mathrm{FPR}$ coordinate and retaining only the maximum $\\mathrm{TPR}$ for each group. For instance, if the points $(0.5, 0.4)$ and $(0.5, 0.6)$ are present, we retain $(0.5, 0.6)$ and discard $(0.5, 0.4)$.\n\n**Step 1.3: Sorting by Abscissa**\nThe trapezoidal rule sums the areas of trapezoids formed by consecutive points. This requires the points to be ordered by their x-coordinate ($\\mathrm{FPR}$). After the previous steps, the resulting unique points $\\{ (x_j, y_j) \\}_{j=0}^N$ are sorted in nondecreasing order of their $x_j$ values. This yields an ordered sequence of points $(x_0, y_0), (x_1, y_1), \\dots, (x_N, y_N)$ such that $0 = x_0 < x_1 < \\dots < x_N = 1$.\n\n**2. Numerical Integration using the Trapezoidal Rule**\n\nWith the preprocessed and sorted points $(x_i, y_i)$ for $i=0, \\dots, N$, we approximate the function $\\mathrm{TPR}(x)$ by connecting consecutive points with straight line segments. The area under this piecewise-linear curve is the sum of the areas of the trapezoids formed by each segment and the x-axis.\n\nFor any two consecutive points, $P_{i-1} = (x_{i-1}, y_{i-1})$ and $P_i = (x_i, y_i)$, they form a trapezoid with vertices at $(x_{i-1}, 0)$, $(x_i, 0)$, $(x_i, y_i)$, and $(x_{i-1}, y_{i-1})$. The area of this $i$-th trapezoid, $A_i$, is given by the formula:\n$$\nA_i = \\frac{1}{2} (y_{i-1} + y_i) (x_i - x_{i-1})\n$$\nThis formula represents the average height of the two parallel sides ($y_{i-1}$ and $y_i$) multiplied by the width of the base ($x_i - x_{i-1}$).\n\nThe total AUC is the sum of the areas of all such trapezoids from $i=1$ to $N$:\n$$\n\\mathrm{AUC} \\approx \\sum_{i=1}^{N} A_i = \\sum_{i=1}^{N} \\frac{1}{2} (y_{i-1} + y_i) (x_i - x_{i-1})\n$$\nThis summation provides the numerical approximation of $\\int_{0}^{1} \\mathrm{TPR}(x) \\,dx$. The final result for each test case is rounded to six decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef calculate_auc(points_list):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC) from a list of (FPR, TPR) points.\n\n    The process involves three preprocessing steps followed by the trapezoidal rule application:\n    1. Augment points with (0,0) and (1,1).\n    2. Resolve duplicate FPRs by taking the maximum TPR.\n    3. Sort the points by FPR.\n    4. Apply the trapezoidal rule to the processed points.\n\n    Args:\n        points_list (list of tuples): A list where each tuple is an (FPR, TPR) point.\n\n    Returns:\n        float: The calculated AUC.\n    \"\"\"\n    # Step 1: Augment points with the definitional endpoints (0,0) and (1,1).\n    # This creates a comprehensive list to start with.\n    all_points = list(points_list)\n    all_points.append((0, 0))\n    all_points.append((1, 1))\n\n    # Step 2: Resolve duplicate abscissae (FPRs).\n    # We use a dictionary to store the maximum TPR for each unique FPR.\n    # The ROC curve is the upper envelope of performance, justifying taking the max TPR.\n    roc_map = {}\n    for fpr, tpr in all_points:\n        # If the FPR is already in the map, update it only if the new TPR is higher.\n        # Otherwise, add the new (FPR, TPR) pair.\n        roc_map[fpr] = max(roc_map.get(fpr, -1.0), tpr)\n    \n    # Convert the map back to a list of points.\n    processed_points = list(roc_map.items())\n\n    # Step 3: Sort the points by FPR in nondecreasing order.\n    # This prepares the points for the trapezoidal rule, ensuring a valid partition.\n    processed_points.sort(key=lambda p: p[0])\n\n    # Convert the list of points to a NumPy array for efficient computation.\n    roc_array = np.array(processed_points)\n    x_coords = roc_array[:, 0]  # FPR values\n    y_coords = roc_array[:, 1]  # TPR values\n\n    # Step 4: Apply the trapezoidal rule.\n    # np.trapz(y, x) computes the integral of y(x) using the trapezoidal rule.\n    # This is equivalent to sum(0.5 * (y_i + y_{i-1}) * (x_i - x_{i-1})).\n    auc = np.trapz(y_coords, x_coords)\n    \n    return auc\n\n\ndef solve():\n    \"\"\"\n    Main function to execute the AUC calculation for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General monotone case\n        [(0,0),(0.2,0.5),(0.6,0.8),(1,1)],\n        # Case 2: Unsorted inputs\n        [(1,1),(0.3,0.6),(0,0),(0.7,0.9)],\n        # Case 3: Duplicate abscissae, take maximum TPR\n        [(0,0),(0.5,0.4),(0.5,0.6),(1,1)],\n        # Case 4: Vertical rise at zero, perfect classifier envelope\n        [(0,0),(0,1),(1,1)],\n        # Case 5: Degenerate classifier\n        [(0,0),(1,0)],\n        # Case 6: Missing endpoints, require augmentation\n        [(0.2,0.3),(0.4,0.6),(0.9,0.95)]\n    ]\n\n    results = []\n    for case in test_cases:\n        auc_value = calculate_auc(case)\n        # Format the result to exactly six decimal places as a string.\n        results.append(f\"{auc_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3284361"}, {"introduction": "A single, overall AUC score can sometimes be misleading, especially when comparing models whose ROC curves cross. This advanced practice addresses such a scenario, where one model may excel at low false positive rates while another performs better elsewhere [@problem_id:3167178]. You will learn to implement and apply the partial AUC ($\\mathrm{pAUC}$), a metric that focuses evaluation on a specific, mission-critical region of the ROC curve, enabling a more nuanced and context-aware model selection for applications with strict performance constraints.", "problem": "You are given two binary classifiers (for example, two deep neural network models) that output real-valued scores for each example. From first principles, the Receiver Operating Characteristic (ROC) curve is the set of points of the form $\\left(\\mathrm{FPR}(t), \\mathrm{TPR}(t)\\right)$ obtained by varying a decision threshold $t$ on the scores, where the False Positive Rate (FPR) and True Positive Rate (TPR) at threshold $t$ are defined by\n$$\n\\mathrm{FPR}(t) \\equiv \\frac{\\mathrm{FP}(t)}{\\mathrm{FP}(t)+\\mathrm{TN}(t)}, \\quad\n\\mathrm{TPR}(t) \\equiv \\frac{\\mathrm{TP}(t)}{\\mathrm{TP}(t)+\\mathrm{FN}(t)}.\n$$\nHere, $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, $\\mathrm{TN}(t)$, and $\\mathrm{FN}(t)$ are the counts of true positives, false positives, true negatives, and false negatives induced by thresholding the scores at $t$ on a dataset with binary labels. The Area Under the Curve (AUC) is the integral of $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ along the ROC curve over the full range $\\mathrm{FPR}\\in[0,1]$. For applications requiring very low false alarm rates, a relevant metric is the partial AUC over a restricted false positive rate regime, defined as\n$$\n\\mathrm{pAUC}(\\alpha) \\equiv \\int_{0}^{\\alpha} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\,\\mathrm{FPR},\n$$\nwhere $\\alpha \\in [0,1]$ specifies the maximum allowable false positive rate.\n\nYour task is to implement, from these definitions, an algorithm that:\n- Computes the ROC points by sweeping thresholds across the sorted scores.\n- Computes the partial AUC $\\mathrm{pAUC}(\\alpha)$ by integrating $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ over the interval $[0,\\alpha]$ using a piecewise-linear interpolation between successive ROC points and the trapezoidal rule.\n- Computes $\\mathrm{TPR}$ at an exact $\\mathrm{FPR}=\\alpha$ by linear interpolation along the ROC curve; when the ROC has vertical segments (i.e., $\\mathrm{FPR}$ does not change while $\\mathrm{TPR}$ increases), define $\\mathrm{TPR}$ at a given $\\mathrm{FPR}$ as the maximum $\\mathrm{TPR}$ achieved at that $\\mathrm{FPR}$.\n- Picks the better model for low-false-alarm applications by the following deterministic rule: prefer the model with larger $\\mathrm{pAUC}(\\alpha)$; if the $\\mathrm{pAUC}(\\alpha)$ values are equal within a numerical tolerance, prefer the model with larger $\\mathrm{TPR}$ at $\\mathrm{FPR}=\\alpha$; if still equal, prefer the model with larger full AUC; if still equal, choose Model A.\n\nConstruct and evaluate the following test suite of parameter values. Each test case specifies the binary label vector and the score vectors from each model aligned by index, followed by the value of $\\alpha$. All values are unitless real numbers.\n\n- Test case $1$ (crossing ROC curves; low $\\alpha$):\n  - Labels $y^{(1)}$: $[\\,1,\\,0,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$.\n  - Model A scores $s_A^{(1)}$: $[\\,0.98,\\,0.62,\\,0.61,\\,0.95,\\,0.59,\\,0.60,\\,0.57,\\,0.58,\\,0.55,\\,0.45,\\,0.50,\\,0.49,\\,0.42,\\,0.47,\\,0.43,\\,0.40,\\,0.41,\\,0.37,\\,0.38,\\,0.36\\,]$.\n  - Model B scores $s_B^{(1)}$: $[\\,0.80,\\,0.60,\\,0.82,\\,0.78,\\,0.58,\\,0.76,\\,0.56,\\,0.74,\\,0.54,\\,0.72,\\,0.52,\\,0.79,\\,0.70,\\,0.50,\\,0.48,\\,0.68,\\,0.46,\\,0.44,\\,0.66,\\,0.42\\,]$.\n  - $\\alpha^{(1)} = 0.10$.\n\n- Test case $2$ (same data; full AUC comparison):\n  - Labels $y^{(2)}$: identical to $y^{(1)}$.\n  - Model A scores $s_A^{(2)}$: identical to $s_A^{(1)}$.\n  - Model B scores $s_B^{(2)}$: identical to $s_B^{(1)}$.\n  - $\\alpha^{(2)} = 1.00$.\n\n- Test case $3$ (same data; zero false positive rate boundary):\n  - Labels $y^{(3)}$: identical to $y^{(1)}$.\n  - Model A scores $s_A^{(3)}$: identical to $s_A^{(1)}$.\n  - Model B scores $s_B^{(3)}$: identical to $s_B^{(1)}$.\n  - $\\alpha^{(3)} = 0.00$.\n\n- Test case $4$ (identical models; tie-breaking to Model A):\n  - Labels $y^{(4)}$: $[\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0\\,]$.\n  - Model A scores $s_A^{(4)}$: $[\\,0.90,\\,0.80,\\,0.65,\\,0.55,\\,0.53,\\,0.70,\\,0.51,\\,0.49,\\,0.47,\\,0.45,\\,0.43,\\,0.41,\\,0.60,\\,0.39,\\,0.37\\,]$.\n  - Model B scores $s_B^{(4)}$: identical to $s_A^{(4)}$.\n  - $\\alpha^{(4)} = 0.20$.\n\nYour program must implement the above algorithm without using any external datasets or user input and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case $i \\in \\{1,2,3,4\\}$, output an integer where $0$ means Model A is preferred and $1$ means Model B is preferred under the specified decision rule, so the final output format is exactly of the form $[\\,r_1, r_2, r_3, r_4\\,]$ with each $r_i \\in \\{0,1\\}$ expressed as integers.", "solution": "The problem requires the implementation of an algorithm to compare two binary classifiers, Model A and Model B, based on their performance in a low-false-alarm regime. The comparison is based on the partial Area Under the Receiver Operating Characteristic (ROC) curve, denoted as $\\mathrm{pAUC}(\\alpha)$, where $\\alpha$ is the maximum acceptable False Positive Rate (FPR). The solution involves several steps: generating the ROC curve, calculating the full and partial AUC, determining the True Positive Rate (TPR) at a specific FPR, and applying a deterministic decision rule.\n\n### Step 1: ROC Curve Generation\n\nThe Receiver Operating Characteristic (ROC) curve is a plot of the True Positive Rate ($\\mathrm{TPR}$) against the False Positive Rate ($\\mathrm{FPR}$) at various decision thresholds. The $\\mathrm{TPR}$ is the fraction of positive instances correctly classified, and the $\\mathrm{FPR}$ is the fraction of negative instances incorrectly classified.\n$$\n\\mathrm{TPR}(t) = \\frac{\\mathrm{TP}(t)}{P}, \\quad \\mathrm{FPR}(t) = \\frac{\\mathrm{FP}(t)}{N}\n$$\nwhere $P$ is the total number of positive instances and $N$ is the total number of negative instances. $\\mathrm{TP}(t)$ and $\\mathrm{FP}(t)$ are the counts of true positives and false positives when a threshold $t$ is applied to the classifier's scores.\n\nAn efficient algorithm to generate the ROC curve points is as follows:\n1.  Combine the true labels $y \\in \\{0, 1\\}$ and the classifier scores $s$ into pairs.\n2.  Sort these pairs in descending order based on the scores. A stable sort (such as `mergesort`) is used to handle ties in scores consistently.\n3.  Iterate through the sorted list, calculating the cumulative sum of true positives ($\\mathrm{TP}$) and false positives ($\\mathrm{FP}$). The points of the ROC curve correspond to the unique thresholds, which are the unique score values.\n4.  To correctly handle tied scores, we identify the indices where the score value changes. These indices mark the end of a group of instances with the same score, which are processed as a single threshold step. The cumulative $\\mathrm{TP}$ and $\\mathrm{FP}$ counts at these indices give the coordinates of the ROC curve's \"corners\".\n5.  The counts are normalized by $P$ and $N$ to get $\\mathrm{TPR}$ and $\\mathrm{FPR}$ values. The point $(0, 0)$ is prepended to the lists, representing a threshold higher than any score (classifying all instances as negative).\n\nThe result is a set of coordinates $(f_i, t_i)$ representing the ROC curve. The array of $f_i$ values is monotonically non-decreasing.\n\n### Step 2: Defining the Functional ROC Curve\n\nThe problem states that for interpolation and integration, the ROC curve should be treated as a function $\\mathrm{TPR}(\\mathrm{FPR})$. In cases where the curve has vertical segments (multiple $\\mathrm{TPR}$ values for a single $\\mathrm{FPR}$), the $\\mathrm{TPR}$ at that $\\mathrm{FPR}$ is defined as the *maximum* $\\mathrm{TPR}$ achieved. This creates the upper envelope of the ROC curve.\n\n1.  From the generated sequence of ROC points $(f_i, t_i)$, we identify the set of unique $\\mathrm{FPR}$ values, let's call them $f'_j$.\n2.  For each unique $f'_j$, we find the maximum corresponding $\\mathrm{TPR}$ value: $t'_j = \\max \\{ t_i \\mid f_i = f'_j \\}$.\n3.  The resulting points $(f'_j, t'_j)$ form a well-defined function $\\mathrm{TPR} = g(\\mathrm{FPR})$, where the $f'_j$ values are strictly increasing. This well-behaved representation is suitable for both interpolation and numerical integration.\n\n### Step 3: Calculation of Metrics\n\nUsing the functional ROC curve representation $(f'_j, t'_j)$, we compute the required metrics:\n\n1.  **Full Area Under the Curve (AUC):** The full AUC is the integral of the functional ROC curve from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=1$. This is computed using the trapezoidal rule on the points $(f'_j, t'_j)$.\n    $$\n    \\mathrm{AUC} = \\int_{0}^{1} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR} \\approx \\sum_{j=1}^{m} \\frac{t'_{j-1} + t'_j}{2} (f'_j - f'_{j-1})\n    $$\n\n2.  **$\\mathrm{TPR}$ at $\\mathrm{FPR}=\\alpha$:** This value is found by performing linear interpolation on the functional ROC curve points $(f'_j, t'_j)$ at the point $\\mathrm{FPR}=\\alpha$.\n\n3.  **Partial AUC ($\\mathrm{pAUC}(\\alpha)$):** This is the integral of the functional ROC curve from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=\\alpha$.\n    $$\n    \\mathrm{pAUC}(\\alpha) = \\int_{0}^{\\alpha} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR}\n    $$\n    To compute this, we select the subset of points $(f'_j, t'_j)$ where $f'_j \\le \\alpha$. If $\\alpha$ itself is not one of the $f'_j$ values, we add a new point $(\\alpha, \\mathrm{TPR}(\\alpha))$ to this subset, where $\\mathrm{TPR}(\\alpha)$ is the value obtained from linear interpolation. The trapezoidal rule is then applied to this subset of points.\n\n### Step 4: Deterministic Decision Rule\n\nThe final step is to select the better model based on the following hierarchical rule, using a small numerical tolerance (e.g., $10^{-9}$) for equality comparisons:\n1.  Prefer the model with the larger $\\mathrm{pAUC}(\\alpha)$.\n2.  If the $\\mathrm{pAUC}(\\alpha)$ values are equal, prefer the model with the larger $\\mathrm{TPR}$ at $\\mathrm{FPR} = \\alpha$.\n3.  If these are also equal, prefer the model with the larger full AUC.\n4.  If all three metrics are equal, the tie is broken by choosing Model A by default.\n\nThis entire procedure is implemented for each test case to determine the preferred model, outputting $0$ for Model A and $1$ for Model B.", "answer": "```python\nimport numpy as np\n\ndef compute_metrics(y_true, y_scores, alpha):\n    \"\"\"\n    Computes Receiver Operating Characteristic (ROC) curve-based metrics from first principles.\n\n    This function calculates the partial Area Under the Curve (pAUC), the True Positive Rate (TPR)\n    at a specific False Positive Rate (FPR), and the full AUC.\n\n    The ROC curve is first generated by sorting scores and calculating cumulative TP/FP counts.\n    To satisfy the problem's requirements for interpolation and integration, a functional\n    representation of the curve (its upper envelope) is created, ensuring a unique TPR for each FPR.\n    Metrics are then calculated on this well-defined function.\n\n    Args:\n        y_true (list or np.ndarray): True binary labels (0 or 1).\n        y_scores (list or np.ndarray): Target scores from a classifier.\n        alpha (float): The maximum False Positive Rate for pAUC calculation.\n\n    Returns:\n        tuple: A tuple containing (pAUC, TPR_at_alpha, full_AUC). Returns (0.0, 0.0, 0.0)\n               if the data contains only one class.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=np.int32)\n    y_scores = np.asarray(y_scores, dtype=np.float64)\n\n    # Sort scores and corresponding truth values in descending order.\n    # 'mergesort' is a stable sort, which is important for reproducibility.\n    desc_score_indices = np.argsort(y_scores, kind=\"mergesort\")[::-1]\n    y_scores = y_scores[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n\n    # Calculate cumulative true positives (TPs) and false positives (FPs).\n    tps_cum = np.cumsum(y_true)\n    fps_cum = np.cumsum(1 - y_true)\n    \n    # Get total positives (P) and negatives (N).\n    P = tps_cum[-1] if len(tps_cum) > 0 else 0\n    N = fps_cum[-1] if len(fps_cum) > 0 else 0\n\n    if P == 0 or N == 0:\n        # ROC is undefined for single-class data.\n        return 0.0, 0.0, 0.0\n\n    # Find the indices corresponding to unique thresholds. A new threshold step\n    # is taken for each unique score value.\n    distinct_value_indices = np.where(np.diff(y_scores))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    # Extract the (TP, FP) counts at each threshold step.\n    tps = tps_cum[threshold_idxs]\n    fps = fps_cum[threshold_idxs]\n\n    # Convert counts to rates and prepend the (0,0) origin point.\n    fpr_raw = fps / N\n    tpr_raw = tps / P\n    fpr = np.r_[0.0, fpr_raw]\n    tpr = np.r_[0.0, tpr_raw]\n\n    # The problem defines TPR at a given FPR as the maximum TPR achieved.\n    # This forms the upper envelope of the ROC curve, making it a well-defined function.\n    # This also ensures the `fpr_u` array is strictly increasing for interpolation.\n    fpr_u = np.unique(fpr)\n    tpr_u = np.array([tpr[fpr == f].max() for f in fpr_u])\n\n    # 1. Calculate full AUC using the trapezoidal rule on the functional curve.\n    full_auc = np.trapz(tpr_u, fpr_u)\n    \n    # 2. Calculate TPR at FPR=alpha via linear interpolation on the functional curve.\n    tpr_at_alpha = np.interp(alpha, fpr_u, tpr_u)\n\n    # 3. Calculate partial AUC up to alpha.\n    pauc = 0.0\n    if alpha > 0.0:\n        # Select the part of the functional curve up to the FPR limit `alpha`.\n        fpr_pauc = fpr_u[fpr_u <= alpha]\n        tpr_pauc = tpr_u[fpr_u <= alpha]\n        \n        # If alpha is not one of the existing FPR points, add it via interpolation\n        # to correctly calculate the area of the final trapezoid.\n        if not np.isclose(fpr_pauc[-1], alpha):\n            fpr_pauc = np.append(fpr_pauc, alpha)\n            tpr_pauc = np.append(tpr_pauc, tpr_at_alpha)\n\n        pauc = np.trapz(tpr_pauc, fpr_pauc)\n        \n    return pauc, tpr_at_alpha, full_auc\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and determine the preferred model for each case.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"y\": [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n            \"s_A\": [0.98, 0.62, 0.61, 0.95, 0.59, 0.60, 0.57, 0.58, 0.55, 0.45, 0.50, 0.49, 0.42, 0.47, 0.43, 0.40, 0.41, 0.37, 0.38, 0.36],\n            \"s_B\": [0.80, 0.60, 0.82, 0.78, 0.58, 0.76, 0.56, 0.74, 0.54, 0.72, 0.52, 0.79, 0.70, 0.50, 0.48, 0.68, 0.46, 0.44, 0.66, 0.42],\n            \"alpha\": 0.10\n        },\n        # Test case 2\n        {\n            \"y\": [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n            \"s_A\": [0.98, 0.62, 0.61, 0.95, 0.59, 0.60, 0.57, 0.58, 0.55, 0.45, 0.50, 0.49, 0.42, 0.47, 0.43, 0.40, 0.41, 0.37, 0.38, 0.36],\n            \"s_B\": [0.80, 0.60, 0.82, 0.78, 0.58, 0.76, 0.56, 0.74, 0.54, 0.72, 0.52, 0.79, 0.70, 0.50, 0.48, 0.68, 0.46, 0.44, 0.66, 0.42],\n            \"alpha\": 1.00\n        },\n        # Test case 3\n        {\n            \"y\": [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n            \"s_A\": [0.98, 0.62, 0.61, 0.95, 0.59, 0.60, 0.57, 0.58, 0.55, 0.45, 0.50, 0.49, 0.42, 0.47, 0.43, 0.40, 0.41, 0.37, 0.38, 0.36],\n            \"s_B\": [0.80, 0.60, 0.82, 0.78, 0.58, 0.76, 0.56, 0.74, 0.54, 0.72, 0.52, 0.79, 0.70, 0.50, 0.48, 0.68, 0.46, 0.44, 0.66, 0.42],\n            \"alpha\": 0.00\n        },\n        # Test case 4\n        {\n            \"y\": [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            \"s_A\": [0.90, 0.80, 0.65, 0.55, 0.53, 0.70, 0.51, 0.49, 0.47, 0.45, 0.43, 0.41, 0.60, 0.39, 0.37],\n            \"s_B\": [0.90, 0.80, 0.65, 0.55, 0.53, 0.70, 0.51, 0.49, 0.47, 0.45, 0.43, 0.41, 0.60, 0.39, 0.37],\n            \"alpha\": 0.20\n        },\n    ]\n\n    results = []\n    TOLERANCE = 1e-9\n\n    for case in test_cases:\n        y, s_A, s_B, alpha = case[\"y\"], case[\"s_A\"], case[\"s_B\"], case[\"alpha\"]\n        \n        pauc_A, tpr_A, auc_A = compute_metrics(y, s_A, alpha)\n        pauc_B, tpr_B, auc_B = compute_metrics(y, s_B, alpha)\n\n        # Apply the deterministic decision rule with tie-breaking logic.\n        # Prefer the model with the larger pAUC(alpha).\n        if pauc_A - pauc_B > TOLERANCE:\n            results.append(0)  # Model A is better\n        elif pauc_B - pauc_A > TOLERANCE:\n            results.append(1)  # Model B is better\n        # If pAUCs are equal, tie-break with TPR at alpha.\n        elif tpr_A - tpr_B > TOLERANCE:\n            results.append(0)\n        elif tpr_B - tpr_A > TOLERANCE:\n            results.append(1)\n        # If TPRs are equal, tie-break with full AUC.\n        elif auc_A - auc_B > TOLERANCE:\n            results.append(0)\n        elif auc_B - auc_A > TOLERANCE:\n            results.append(1)\n        # If all are equal, choose Model A by default.\n        else:\n            results.append(0)\n            \n    # Format and print the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167178"}]}