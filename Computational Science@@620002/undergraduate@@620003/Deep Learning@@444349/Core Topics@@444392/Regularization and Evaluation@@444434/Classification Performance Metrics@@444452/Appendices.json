{"hands_on_practices": [{"introduction": "A model's performance is not an intrinsic constant; it is context-dependent and sensitive to the characteristics of the data it evaluates. A frequent challenge in deploying machine learning systems is \"prevalence shift,\" where the proportion of positive and negative classes in the real world differs from the balanced environment of a validation set. This exercise [@problem_id:3105734] guides you through a rigorous, first-principles derivation to reveal how and why Precision is highly sensitive to class prevalence, while Recall remains robust. By working through this scenario, you will develop a deeper analytical understanding of metric behavior and the critical need to re-optimize decision thresholds for new environments.", "problem": "A binary classifier used in a deep learning pipeline outputs a nonnegative score $s$ where larger values indicate a higher likelihood of belonging to the positive class. On a validation set, the classifierâ€™s decision threshold $\\tau$ was selected to maximize the F1-score (F1), defined by the harmonic mean of precision and recall. After deployment, the class prevalence changes. Starting only from fundamental definitions of the confusion matrix and the standard classification metrics, derive how precision and recall depend on the class prevalence when the score threshold is fixed, and then re-tune the threshold to maximize the F1-score under the new prevalence.\n\nAssume the following scientifically plausible model for the score $s$:\n- The conditional score distribution for the positive class is exponential with rate $\\lambda_{1}$, namely $s \\mid y=1 \\sim \\text{Exp}(\\lambda_{1})$.\n- The conditional score distribution for the negative class is exponential with rate $\\lambda_{0}$, namely $s \\mid y=0 \\sim \\text{Exp}(\\lambda_{0})$.\n\nA decision rule predicts $y=1$ if and only if $s \\geq \\tau$ and predicts $y=0$ otherwise. Let the validation prevalence be $\\pi_{\\text{val}}$, and the deployment prevalence be $\\pi_{\\text{dep}}$. Use only the core definitions of true positive rate, false positive rate, precision, recall, and F1-score to derive the required expressions.\n\nConcretely, work with parameters $\\lambda_{1}=1$, $\\lambda_{0}=2$, $\\pi_{\\text{val}}=0.4$, and $\\pi_{\\text{dep}}=0.2$:\n- First, using only the definitions, derive expressions for true positive rate and false positive rate as functions of $\\tau$.\n- Then, derive expressions for precision and recall as functions of $\\tau$ and the prevalence $\\pi$.\n- Explain qualitatively which of precision or recall changes when the prevalence shifts from $\\pi_{\\text{val}}$ to $\\pi_{\\text{dep}}$, and why.\n- Determine the validation threshold $\\tau_{\\text{val}}$ that maximizes the F1-score under $\\pi_{\\text{val}}$.\n- Finally, compute the deployment threshold $\\tau_{\\text{dep}}$ that maximizes the F1-score under $\\pi_{\\text{dep}}$.\n\nRound your final numerical answer for the deployment threshold $\\tau_{\\text{dep}}$ to four significant figures. Provide only the numerical value of $\\tau_{\\text{dep}}$ as your final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in statistical decision theory applied to classifier evaluation, using common definitions and a plausible, formalizable model for classifier scores. All necessary parameters are provided. Therefore, the problem is valid, and a solution will be derived.\n\nThe solution proceeds in five parts as requested.\n\nFirst, we derive the expressions for the True Positive Rate (TPR) and False Positive Rate (FPR) as functions of the decision threshold $\\tau$. By definition, the TPR, also known as Recall or Sensitivity, is the probability that a positive instance is correctly classified as positive. The FPR is the probability that a negative instance is incorrectly classified as positive. The decision rule states that a sample is classified as positive ($\\hat{y}=1$) if its score $s$ is greater than or equal to the threshold $\\tau$.\n\nGiven a true class $y$, the TPR and FPR are:\n$$TPR(\\tau) = P(\\hat{y}=1 | y=1) = P(s \\geq \\tau | y=1)$$\n$$FPR(\\tau) = P(\\hat{y}=1 | y=0) = P(s \\geq \\tau | y=0)$$\nThe conditional score distributions are given as exponential. For a random variable $X \\sim \\text{Exp}(\\lambda)$, its probability density function is $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$, and its survival function is $P(X \\geq x) = e^{-\\lambda x}$.\nFor the positive class, $s \\mid y=1 \\sim \\text{Exp}(\\lambda_1)$. The TPR is therefore:\n$$TPR(\\tau) = \\exp(-\\lambda_1 \\tau)$$\nFor the negative class, $s \\mid y=0 \\sim \\text{Exp}(\\lambda_0)$. The FPR is therefore:\n$$FPR(\\tau) = \\exp(-\\lambda_0 \\tau)$$\n\nSecond, we derive the expressions for Precision (PREC) and Recall (REC) as functions of $\\tau$ and the class prevalence $\\pi = P(y=1)$.\nRecall is, by definition, the same as the True Positive Rate:\n$$REC(\\tau) = TPR(\\tau) = \\exp(-\\lambda_1 \\tau)$$\nPrecision, or Positive Predictive Value (PPV), is the probability that a sample is truly positive, given that it was classified as positive. Using Bayes' theorem:\n$$PREC(\\tau, \\pi) = P(y=1 | \\hat{y}=1) = \\frac{P(\\hat{y}=1 | y=1)P(y=1)}{P(\\hat{y}=1)}$$\nThe numerator is $TPR(\\tau) \\cdot \\pi$. The denominator can be expanded using the law of total probability:\n$$P(\\hat{y}=1) = P(\\hat{y}=1 | y=1)P(y=1) + P(\\hat{y}=1 | y=0)P(y=0)$$\n$$P(\\hat{y}=1) = TPR(\\tau) \\cdot \\pi + FPR(\\tau) \\cdot (1-\\pi)$$\nSubstituting these into the expression for precision gives:\n$$PREC(\\tau, \\pi) = \\frac{TPR(\\tau) \\cdot \\pi}{TPR(\\tau) \\cdot \\pi + FPR(\\tau) \\cdot (1-\\pi)}$$\nSubstituting the exponential forms for TPR and FPR, we get the final expressions:\n$$REC(\\tau) = \\exp(-\\lambda_1 \\tau)$$\n$$PREC(\\tau, \\pi) = \\frac{\\pi \\exp(-\\lambda_1 \\tau)}{\\pi \\exp(-\\lambda_1 \\tau) + (1-\\pi) \\exp(-\\lambda_0 \\tau)}$$\n\nThird, we explain the effect of changing prevalence.\nThe expression for Recall, $REC(\\tau) = \\exp(-\\lambda_1 \\tau)$, does not depend on the prevalence $\\pi$. Recall is a measure of a classifier's ability to identify all positive instances, conditioned on the ground truth being positive. This property is intrinsic to the classifier's performance on the positive class and is not affected by the proportion of positive instances in the population.\nThe expression for Precision, conversely, clearly shows a dependence on $\\pi$. Precision measures the proportion of correct predictions among all positive predictions. The set of positive predictions is composed of True Positives (from the positive class) and False Positives (from the negative class). A change in prevalence $\\pi$ alters the balance between the number of available positive and negative samples. If $\\pi$ decreases (as from $\\pi_{\\text{val}}=0.4$ to $\\pi_{\\text{dep}}=0.2$), the negative class becomes more dominant. For a fixed FPR, this leads to a larger absolute number of False Positives, which dilutes the set of predicted positives and consequently lowers the precision.\n\nFourth, we determine the optimal threshold $\\tau$ that maximizes the F1-score. The F1-score is the harmonic mean of Precision and Recall:\n$$F1 = 2 \\frac{PREC \\cdot REC}{PREC + REC}$$\nMaximizing the F1-score is equivalent to minimizing its reciprocal, $\\frac{1}{F1}$:\n$$\\frac{1}{F1} = \\frac{1}{2}\\left(\\frac{1}{PREC} + \\frac{1}{REC}\\right)$$\nLet's define a function $f(\\tau) = \\frac{2}{F1(\\tau, \\pi)}$ which we want to minimize.\n$$\\frac{1}{REC(\\tau)} = \\frac{1}{\\exp(-\\lambda_1 \\tau)} = \\exp(\\lambda_1 \\tau)$$\n$$\\frac{1}{PREC(\\tau, \\pi)} = \\frac{\\pi \\exp(-\\lambda_1 \\tau) + (1-\\pi) \\exp(-\\lambda_0 \\tau)}{\\pi \\exp(-\\lambda_1 \\tau)} = 1 + \\frac{1-\\pi}{\\pi} \\frac{\\exp(-\\lambda_0 \\tau)}{\\exp(-\\lambda_1 \\tau)} = 1 + \\frac{1-\\pi}{\\pi} \\exp((\\lambda_1 - \\lambda_0)\\tau)$$\nSo, the function to minimize is:\n$$f(\\tau) = \\exp(\\lambda_1 \\tau) + 1 + \\frac{1-\\pi}{\\pi} \\exp((\\lambda_1 - \\lambda_0)\\tau)$$\nTo find the minimum, we compute the derivative with respect to $\\tau$ and set it to zero:\n$$\\frac{df}{d\\tau} = \\lambda_1 \\exp(\\lambda_1 \\tau) + \\frac{1-\\pi}{\\pi}(\\lambda_1 - \\lambda_0) \\exp((\\lambda_1 - \\lambda_0)\\tau) = 0$$\n$$\\lambda_1 \\exp(\\lambda_1 \\tau) = -\\frac{1-\\pi}{\\pi}(\\lambda_1 - \\lambda_0) \\exp((\\lambda_1 - \\lambda_0)\\tau)$$\n$$\\lambda_1 \\exp(\\lambda_1 \\tau) = \\frac{1-\\pi}{\\pi}(\\lambda_0 - \\lambda_1) \\exp(\\lambda_1 \\tau)\\exp(-\\lambda_0 \\tau)$$\nSince $\\tau \\geq 0$, we know $\\exp(\\lambda_1 \\tau) > 0$ and can divide by it:\n$$\\lambda_1 = \\frac{1-\\pi}{\\pi}(\\lambda_0 - \\lambda_1) \\exp(-\\lambda_0 \\tau)$$\nNow, we solve for $\\tau$:\n$$\\exp(-\\lambda_0 \\tau) = \\frac{\\pi \\lambda_1}{(1-\\pi)(\\lambda_0 - \\lambda_1)}$$\n$$-\\lambda_0 \\tau = \\ln\\left(\\frac{\\pi \\lambda_1}{(1-\\pi)(\\lambda_0 - \\lambda_1)}\\right)$$\n$$\\tau = -\\frac{1}{\\lambda_0} \\ln\\left(\\frac{\\pi \\lambda_1}{(1-\\pi)(\\lambda_0 - \\lambda_1)}\\right) = \\frac{1}{\\lambda_0} \\ln\\left(\\frac{(1-\\pi)(\\lambda_0 - \\lambda_1)}{\\pi \\lambda_1}\\right)$$\nThis is the general expression for the F1-maximizing threshold $\\tau$. The validation threshold $\\tau_{\\text{val}}$ is obtained by substituting $\\pi = \\pi_{\\text{val}} = 0.4$, $\\lambda_1 = 1$, and $\\lambda_0 = 2$:\n$$\\tau_{\\text{val}} = \\frac{1}{2} \\ln\\left(\\frac{(1-0.4)(2-1)}{0.4 \\cdot 1}\\right) = \\frac{1}{2} \\ln\\left(\\frac{0.6}{0.4}\\right) = \\frac{1}{2} \\ln(1.5)$$\n\nFinally, we compute the deployment threshold $\\tau_{\\text{dep}}$ by substituting the deployment prevalence $\\pi = \\pi_{\\text{dep}} = 0.2$ into the general expression for $\\tau$:\n$$\\tau_{\\text{dep}} = \\frac{1}{2} \\ln\\left(\\frac{(1-0.2)(2-1)}{0.2 \\cdot 1}\\right)$$\n$$\\tau_{\\text{dep}} = \\frac{1}{2} \\ln\\left(\\frac{0.8}{0.2}\\right) = \\frac{1}{2} \\ln(4)$$\nSince $\\ln(4) = \\ln(2^2) = 2\\ln(2)$, the expression simplifies to:\n$$\\tau_{\\text{dep}} = \\frac{1}{2} (2\\ln(2)) = \\ln(2)$$\nThe numerical value is $\\tau_{\\text{dep}} \\approx 0.693147...$. Rounding to four significant figures gives $0.6931$.", "answer": "$$\\boxed{0.6931}$$", "id": "3105734"}, {"introduction": "In deep learning, we use a validation set to monitor training progress and decide when to stopâ€”a technique known as early stopping. The choice of which metric to monitor is a crucial decision that can significantly impact the final model's utility, especially when dealing with imbalanced datasets. This hands-on practice [@problem_id:3105763] simulates a training trajectory to directly compare the outcomes of stopping based on overall Accuracy versus the $F_1$-score. It is designed to build practical intuition, demonstrating how a naive focus on Accuracy can lead to a model that neglects the minority class, whereas the $F_1$-score promotes a more balanced and often more useful outcome.", "problem": "You are given a binary classification validation setting with imbalanced classes and two early stopping strategies. The goal is to construct a deterministic validation trajectory across epochs using class-conditional correctness probabilities, derive performance metrics from first principles, and decide for each test case whether stopping on validation $\\text{Accuracy}$ yields a worse minority-class $\\text{Recall}$ than stopping on validation $\\text{F1}$.\n\nFundamental base definitions to use:\n- A binary classifier induces counts of True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) on a dataset; these are the building blocks from which performance metrics are defined.\n- Performance metrics to derive and implement from these counts are: $\\text{Accuracy}$, $\\text{Precision}$, $\\text{Recall}$, and the $\\text{F1}$-score (commonly denoted as $\\text{F1}$).\n\nValidation trajectory generation:\n- Let the total number of validation samples be $N$.\n- Let the minority-class (positive) fraction be $r$, so the majority-class (negative) fraction is $1 - r$.\n- The trajectory spans $T$ epochs indexed by $t \\in \\{1,2,\\ldots,T\\}$.\n- For each epoch $t$, define two class-conditional correctness probabilities:\n  - The negative-class correctness probability $p_n(t)$.\n  - The positive-class correctness probability $p_p(t)$.\n- The trajectory is generated by linear trends with clipping:\n  - $p_n(t)$ follows a decreasing linear trend: starting at $p_{n,0}$ and changing by a nonnegative rate $\\delta_n$ per epoch step, i.e., $p_n(t) = \\mathrm{clip}\\big(p_{n,0} - \\delta_n \\cdot (t - 1), 0, 1\\big)$.\n  - $p_p(t)$ follows an increasing linear trend: starting at $p_{p,0}$ and changing by a nonnegative rate $\\delta_p$ per epoch step, i.e., $p_p(t) = \\mathrm{clip}\\big(p_{p,0} + \\delta_p \\cdot (t - 1), 0, 1\\big)$.\n- Here $\\mathrm{clip}(x,0,1)$ denotes restricting $x$ to the interval $[0,1]$.\n\nComputation requirements:\n- For each epoch $t$, treat $\\{p_n(t), p_p(t)\\}$ as the probabilities of correct classification for negative and positive classes, respectively, and derive the expected confusion counts $\\{TP(t), FP(t), TN(t), FN(t)\\}$ from first principles, starting with $N_+ = r N$ and $N_- = (1 - r) N$ as the expected positive and negative sample counts.\n- Using only these definitions, derive and implement $\\text{Accuracy}(t)$, $\\text{Precision}(t)$, $\\text{Recall}(t)$, and $\\text{F1}(t)$ as decimals in $[0,1]$ for each epoch $t$.\n- Define two early stopping strategies:\n  - Strategy $\\mathcal{A}$: stop at the epoch $t$ that maximizes validation $\\text{Accuracy}(t)$.\n  - Strategy $\\mathcal{F}$: stop at the epoch $t$ that maximizes validation $\\text{F1}(t)$.\n- In case of ties for the maximum value across epochs, select the earliest epoch (smallest $t$).\n- For each strategy, report the minority-class $\\text{Recall}$ at the chosen epoch, and compare them.\n\nOutput specification:\n- For each test case, output a boolean indicating whether the minority-class $\\text{Recall}$ under Strategy $\\mathcal{A}$ is strictly lower than under Strategy $\\mathcal{F}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\ldots]$). Each $result_i$ must be a boolean.\n\nAll metrics must be expressed as decimals in $[0,1]$; do not use percentages.\n\nTest suite:\nUse the following four parameter sets to instantiate $(N, r, T, p_{n,0}, \\delta_n, p_{p,0}, \\delta_p)$.\n\n- Case $1$ (happy path, strong imbalance and diverging trends):\n  - $N = 10000$, $r = 0.05$, $T = 12$, $p_{n,0} = 0.995$, $\\delta_n = 0.01$, $p_{p,0} = 0.20$, $\\delta_p = 0.06$.\n- Case $2$ (balanced classes):\n  - $N = 10000$, $r = 0.50$, $T = 12$, $p_{n,0} = 0.995$, $\\delta_n = 0.01$, $p_{p,0} = 0.20$, $\\delta_p = 0.06$.\n- Case $3$ (mild imbalance, gentler trends):\n  - $N = 10000$, $r = 0.20$, $T = 12$, $p_{n,0} = 0.99$, $\\delta_n = 0.005$, $p_{p,0} = 0.30$, $\\delta_p = 0.03$.\n- Case $4$ (edge case, constant performance):\n  - $N = 10000$, $r = 10$, $T = 12$, $p_{n,0} = 0.99$, $\\delta_n = 0.0$, $p_{p,0} = 0.10$, $\\delta_p = 0.0$.\n\nYour task:\n- Implement the above trajectory generator and metric computations.\n- For each case, determine the early stopping epoch for $\\mathcal{A}$ and $\\mathcal{F}$, extract the corresponding minority-class $\\text{Recall}$ values, and output whether $\\text{Recall}_{\\mathcal{A}} < \\text{Recall}_{\\mathcal{F}}$ as a boolean.\n- Produce the final output in the exact single-line format described above.", "solution": "The problem requires an analysis of two early stopping strategies for a binary classifier, based on validation Accuracy and validation F1-score, respectively. We are asked to determine whether stopping on Accuracy leads to a worse minority-class Recall compared to stopping on F1-score. The analysis is performed on a deterministic validation trajectory generated from class-conditional correctness probabilities.\n\nFirst, we establish the theoretical foundations by deriving the necessary metrics from first principles as specified.\n\nLet $N$ be the total number of samples in the validation set.\nLet $r$ be the fraction of the minority (positive) class.\nThe number of positive samples is $N_+ = rN$.\nThe number of negative samples is $N_- = (1 - r)N$.\n\nThe validation trajectory spans $T$ epochs, indexed by $t \\in \\{1, 2, ..., T\\}$. For each epoch $t$, we are given the probabilities of correct classification for each class:\n- $p_p(t)$: probability of correctly classifying a positive sample.\n- $p_n(t)$: probability of correctly classifying a negative sample.\n\nThese probabilities evolve over epochs according to the following clipped linear functions:\n$$p_p(t) = \\mathrm{clip}(p_{p,0} + \\delta_p \\cdot (t - 1), 0, 1)$$\n$$p_n(t) = \\mathrm{clip}(p_{n,0} - \\delta_n \\cdot (t - 1), 0, 1)$$\nwhere $p_{p,0}$ and $p_{n,0}$ are initial probabilities, and $\\delta_p \\ge 0$ and $\\delta_n \\ge 0$ are the rates of change. The $\\mathrm{clip}(x, a, b)$ function constrains $x$ to the interval $[a, b]$.\n\nFrom these probabilities, we can derive the expected counts for the confusion matrix components at each epoch $t$:\n- True Positives ($TP(t)$): A positive sample is correctly classified.\n  $$TP(t) = N_+ \\cdot p_p(t) = rN \\cdot p_p(t)$$\n- False Negatives ($FN(t)$): A positive sample is incorrectly classified. The probability is $1 - p_p(t)$.\n  $$FN(t) = N_+ \\cdot (1 - p_p(t)) = rN \\cdot (1 - p_p(t))$$\n- True Negatives ($TN(t)$): A negative sample is correctly classified.\n  $$TN(t) = N_- \\cdot p_n(t) = (1 - r)N \\cdot p_n(t)$$\n- False Positives ($FP(t)$): A negative sample is incorrectly classified. The probability is $1 - p_n(t)$.\n  $$FP(t) = N_- \\cdot (1 - p_n(t)) = (1 - r)N \\cdot (1 - p_n(t))$$\n\nThe sum of all components is $TP(t) + FN(t) + TN(t) + FP(t) = N_+ + N_- = N$, as expected.\n\nNext, we derive the performance metrics for each epoch $t$ using these expected counts.\n1.  **Accuracy**: The fraction of correctly classified samples.\n    $$\\text{Accuracy}(t) = \\frac{TP(t) + TN(t)}{N} = \\frac{rN \\cdot p_p(t) + (1-r)N \\cdot p_n(t)}{N} = r \\cdot p_p(t) + (1-r) \\cdot p_n(t)$$\n    This shows that Accuracy is a weighted average of the class-wise correctness probabilities, with weights corresponding to class prevalence.\n\n2.  **Recall (Sensitivity or True Positive Rate)**: The fraction of actual positive samples that are correctly identified. This is the minority-class recall.\n    $$\\text{Recall}(t) = \\frac{TP(t)}{TP(t) + FN(t)} = \\frac{TP(t)}{N_+} = \\frac{rN \\cdot p_p(t)}{rN} = p_p(t)$$\n    This is a crucial result: the minority-class Recall at epoch $t$ is simply the correctness probability for the positive class, $p_p(t)$.\n\n3.  **Precision (Positive Predictive Value)**: The fraction of positive predictions that are correct.\n    $$\\text{Precision}(t) = \\frac{TP(t)}{TP(t) + FP(t)} = \\frac{rN \\cdot p_p(t)}{rN \\cdot p_p(t) + (1-r)N \\cdot (1-p_n(t))}$$\n    If the denominator $TP(t) + FP(t)$ is $0$, Precision is conventionally defined as $0$.\n\n4.  **F1-Score**: The harmonic mean of Precision and Recall.\n    $$\\text{F1}(t) = 2 \\cdot \\frac{\\text{Precision}(t) \\cdot \\text{Recall}(t)}{\\text{Precision}(t) + \\text{Recall}(t)}$$\n    If Precision and Recall are both $0$, the F1-score is also $0$.\n\nThe problem defines two early stopping strategies:\n- Strategy $\\mathcal{A}$: Stop at epoch $t_\\mathcal{A} = \\arg\\max_{t} \\text{Accuracy}(t)$.\n- Strategy $\\mathcal{F}$: Stop at epoch $t_\\mathcal{F} = \\arg\\max_{t} \\text{F1}(t)$.\nIn case of ties for the maximum value, the earliest epoch (smallest $t$) is chosen.\n\nWe must determine if the minority-class Recall under strategy $\\mathcal{A}$ is strictly lower than under strategy $\\mathcal{F}$. This is the condition:\n$$\\text{Recall}(t_\\mathcal{A}) < \\text{Recall}(t_\\mathcal{F})$$\nUsing our derived formula for Recall, this is equivalent to:\n$$p_p(t_\\mathcal{A}) < p_p(t_\\mathcal{F})$$\nThe function $p_p(t)$ is defined as a monotonically non-decreasing function of $t$ (since $\\delta_p \\ge 0$). Therefore, $p_p(t_1) < p_p(t_2)$ if and only if $t_1 < t_2$, assuming $p_p(t)$ has not been clipped at its maximum value of $1$. If it has been clipped, it is possible for $t_1 < t_2$ but $p_p(t_1) = p_p(t_2) = 1$. However, the strict inequality $p_p(t_\\mathcal{A}) < p_p(t_\\mathcal{F})$ can only hold if $t_\\mathcal{A} < t_\\mathcal{F}$. If $t_\\mathcal{A} \\ge t_\\mathcal{F}$, then $p_p(t_\\mathcal{A}) \\ge p_p(t_\\mathcal{F})$.\nThus, the problem reduces to determining if $t_\\mathcal{A} < t_\\mathcal{F}$.\n\nThe algorithm to solve the problem for each test case is as follows:\n1.  For the given parameters $(N, r, T, p_{n,0}, \\delta_n, p_{p,0}, \\delta_p)$, initialize arrays to store the values of $\\text{Accuracy}(t)$ and $\\text{F1}(t)$ for $t=1, \\dots, T$.\n2.  Loop for $t$ from $1$ to $T$:\n    a. Calculate $p_n(t)$ and $p_p(t)$ using the provided clipped linear formulas.\n    b. Calculate the confusion matrix components $TP(t)$, $FP(t)$, $TN(t)$, $FN(t)$.\n    c. Calculate $\\text{Accuracy}(t)$, $\\text{Recall}(t)$, and $\\text{Precision}(t)$, handling potential divisions by zero.\n    d. Calculate $\\text{F1}(t)$.\n    e. Store $\\text{Accuracy}(t)$ and $\\text{F1}(t)$ in their respective arrays.\n3.  Find the epoch $t_\\mathcal{A}$ that maximizes the $\\text{Accuracy}(t)$ array. The index of the first maximum element corresponds to the earliest epoch rule.\n4.  Find the epoch $t_\\mathcal{F}$ that maximizes the $\\text{F1}(t)$ array, again using the index of the first maximum.\n5.  Evaluate the boolean condition $t_\\mathcal{A} < t_\\mathcal{F}$.\n6.  Repeat for all test cases and collect the boolean results.\nThis procedure will be implemented to generate the final answer.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the early stopping comparison problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, strong imbalance and diverging trends)\n        (10000, 0.05, 12, 0.995, 0.01, 0.20, 0.06),\n        # Case 2 (balanced classes)\n        (10000, 0.50, 12, 0.995, 0.01, 0.20, 0.06),\n        # Case 3 (mild imbalance, gentler trends)\n        (10000, 0.20, 12, 0.99, 0.005, 0.30, 0.03),\n        # Case 4 (edge case, constant performance)\n        (10000, 0.10, 12, 0.99, 0.0, 0.10, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, r, T, p_n0, delta_n, p_p0, delta_p = case\n        \n        epochs = np.arange(1, T + 1)\n        \n        # --- Trajectory Generation ---\n        # p_n(t) = clip(p_n0 - delta_n * (t - 1), 0, 1)\n        # p_p(t) = clip(p_p0 + delta_p * (t - 1), 0, 1)\n        p_n_t = np.clip(p_n0 - delta_n * (epochs - 1), 0, 1)\n        p_p_t = np.clip(p_p0 + delta_p * (epochs - 1), 0, 1)\n\n        # --- Confusion Matrix and Metrics Calculation ---\n        N_pos = r * N\n        N_neg = (1 - r) * N\n\n        TP_t = N_pos * p_p_t\n        FN_t = N_pos * (1 - p_p_t)\n        TN_t = N_neg * p_n_t\n        FP_t = N_neg * (1 - p_n_t)\n\n        # Accuracy(t) = (TP(t) + TN(t)) / N\n        accuracy_t = (TP_t + TN_t) / N\n        \n        # Recall(t) = TP(t) / (TP(t) + FN(t)) = p_p(t)\n        recall_t = p_p_t\n        \n        # Precision(t) = TP(t) / (TP(t) + FP(t))\n        precision_denominator = TP_t + FP_t\n        # Handle division by zero, though unlikely with given parameters.\n        precision_t = np.divide(TP_t, precision_denominator, \n                                out=np.zeros_like(precision_denominator), \n                                where=precision_denominator!=0)\n        \n        # F1(t) = 2 * (Precision * Recall) / (Precision + Recall)\n        f1_denominator = precision_t + recall_t\n        # Handle division by zero.\n        f1_t = np.divide(2 * precision_t * recall_t, f1_denominator,\n                         out=np.zeros_like(f1_denominator),\n                         where=f1_denominator!=0)\n\n        # --- Early Stopping Decision ---\n        # Strategy A: stop at max validation Accuracy\n        # np.argmax returns the index of the first occurrence of the maximum value.\n        # Epoch index is 1-based, array index is 0-based.\n        t_A_idx = np.argmax(accuracy_t)\n        \n        # Strategy F: stop at max validation F1\n        t_F_idx = np.argmax(f1_t)\n\n        # The question is if Recall_A < Recall_F.\n        # Since Recall(t) = p_p(t) is monotonic non-decreasing with t,\n        # Recall(t_A) < Recall(t_F) is equivalent to p_p(t_A) < p_p(t_F).\n        # This is true if and only if t_A < t_F, assuming p_p(t) is not yet\n        # clipped at 1 for both epochs. The direct comparison of recall values\n        # is the most robust way to check.\n        \n        recall_A = recall_t[t_A_idx]\n        recall_F = recall_t[t_F_idx]\n        \n        results.append(recall_A < recall_F)\n\n    # Format output as specified: a string representation of a list of booleans.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105763"}, {"introduction": "Beyond initial training, we often need to fine-tune a classifier to meet specific performance targets, which involves navigating the inherent trade-off between different types of errors. This exercise [@problem_id:3105654] presents a concrete model of a \"hard example mining\" strategy, a process that actively works to correct False Negatives to improve Recall. You will simulate this intervention step-by-step, observing how efforts to boost Recall can inadvertently introduce new False Positives and harm Precision. Your goal is to analyze this dynamic and determine the optimal intervention schedule that maximizes the harmonic mean of these two metrics, the $F_1$-score.", "problem": "You are given a binary classification model and a post-training intervention called hard example mining that targets correcting False Negatives (FN). Hard example mining proceeds in discrete steps, where each step attempts to convert some number of False Negatives into True Positives (TP). However, emphasizing corrections can also degrade Precision by increasing False Positives (FP). Your task is to formally model this intervention and quantify its effects on Precision, Recall, and the F1-score, then select the mining schedule that maximizes the F1-score.\n\nFundamental base definitions to use:\n- Precision ($P$) is defined as $P = \\dfrac{TP}{TP + FP}$, with the convention that if $TP + FP = 0$ then $P = 0$.\n- Recall ($R$) is defined as $R = \\dfrac{TP}{TP + FN}$, with the convention that if $TP + FN = 0$ then $R = 0$.\n- F1-score ($F_1$) is defined as $F_1 = \\dfrac{2PR}{P + R}$, with the convention that if $P + R = 0$ then $F_1 = 0$.\n\nHard example mining model:\n- Start from a baseline confusion matrix with counts $\\left(TP_0, FP_0, TN_0, FN_0\\right)$, all nonnegative integers, with total $N = TP_0 + FP_0 + TN_0 + FN_0$.\n- A mining schedule is a finite sequence of steps. Each step $i$ has two parameters $(m_i, b_i)$, where $m_i$ is the maximum number of False Negatives the step attempts to correct, and $b_i \\in [0,1]$ is a side-effect coefficient modeling how many additional False Positives may be induced per correction attempt.\n- At step $i$, let $c_i = \\min(m_i, FN)$ be the number of actual corrections applied given the remaining False Negatives $FN$. Update counts as follows:\n    1. Convert $c_i$ False Negatives to True Positives: $TP \\leftarrow TP + c_i$, $FN \\leftarrow FN - c_i$.\n    2. Increase False Positives due to side-effects: $\\Delta FP_i = \\min\\!\\left(TN, \\left\\lfloor b_i \\cdot c_i \\right\\rfloor\\right)$, then update $FP \\leftarrow FP + \\Delta FP_i$, $TN \\leftarrow TN - \\Delta FP_i$. The floor function ensures integer counts and the minimum caps by the available True Negatives $TN$.\n- After all steps in a schedule, compute the new $P$, $R$, and $F_1$ from the updated confusion matrix using the definitions above.\n\nComputational objectives for each test case:\n1. Compute the resulting increase in Recall $\\Delta R = R_{\\text{final}} - R_{\\text{baseline}}$ for the schedule that maximizes the final $F_1$.\n2. Determine whether Precision suffers, defined as the boolean statement $P_{\\text{final}} < P_{\\text{baseline}}$, for that maximizing schedule.\n3. Find the mining schedule (among the provided candidates) that maximizes the final $F_1$; in case of ties in $F_1$, select the schedule with the smallest index.\n\nAngle units are not applicable. No physical units are involved. All metric values must be expressed as decimals rounded to six places; do not use a percentage sign.\n\nYour program must evaluate the following test suite. For each test case, you are given a baseline confusion matrix $\\left(TP_0, FP_0, TN_0, FN_0\\right)$ and a list of candidate schedules. A schedule is a list of steps, and each step is a pair $(m_i, b_i)$.\n\nTest case $1$:\n- Baseline: $\\left(TP_0, FP_0, TN_0, FN_0\\right) = \\left(60, 10, 110, 20\\right)$.\n- Candidate schedules (indexed $0,1,2$):\n    - Index $0$: $\\left[(10, 0.2), (5, 0.1)\\right]$.\n    - Index $1$: $\\left[(15, 0.05)\\right]$.\n    - Index $2$: $\\left[(20, 0.3)\\right]$.\n\nTest case $2$:\n- Baseline: $\\left(TP_0, FP_0, TN_0, FN_0\\right) = \\left(30, 20, 150, 0\\right)$.\n- Candidate schedules (indexed $0,1,2$):\n    - Index $0$: $\\left[]\\right]$ (the empty schedule).\n    - Index $1$: $\\left[(5, 0.2)\\right]$.\n    - Index $2$: $\\left[(1, 1.0), (2, 1.0)\\right]$.\n\nTest case $3$:\n- Baseline: $\\left(TP_0, FP_0, TN_0, FN_0\\right) = \\left(50, 5, 10, 35\\right)$.\n- Candidate schedules (indexed $0,1,2$):\n    - Index $0$: $\\left[(10, 1.0), (10, 1.0)\\right]$.\n    - Index $1$: $\\left[(35, 0.6)\\right]$.\n    - Index $2$: $\\left[(20, 0.1), (15, 0.1)\\right]$.\n\nFinal output format:\n- For each test case, report a list containing four elements in the following order: $[\\Delta R, \\text{precision\\_decreased}, F_{1,\\text{max}}, \\text{schedule\\_index}]$, where $\\Delta R$ and $F_{1,\\text{max}}$ are decimals rounded to six places, $\\text{precision\\_decreased}$ is a boolean, and $\\text{schedule\\_index}$ is an integer.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets with no spaces, for example: $\\left[[0.123456,\\text{True},0.654321,1],[\\dots]\\right]$.\n\nAssumptions and conventions:\n- At each step, counts remain integers; the floor and cap by $TN$ are mandatory.\n- If any denominator in the metric definitions is zero, the corresponding metric is defined to be zero.\n- Ties in the maximizing $F_1$ are broken by choosing the smallest schedule index.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of classification performance evaluation, well-posed with a clear and deterministic set of rules, and formulated with objective, unambiguous language. The problem provides a self-contained model for simulating a \"hard example mining\" intervention and asks for a quantitative analysis of its effects, which is a formalizable and solvable task.\n\nThe core of the problem is to simulate the evolution of a confusion matrix, represented by the counts $(TP, FP, TN, FN)$, under various \"mining schedules\". For each schedule, we must apply a sequence of transformations and then evaluate the final performance metrics to find the optimal schedule.\n\nFirst, we define the performance metrics as specified. Given a confusion matrix with counts for True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$):\n- Precision, $P = \\dfrac{TP}{TP + FP}$. By convention, if $TP + FP = 0$, then $P = 0$.\n- Recall, $R = \\dfrac{TP}{TP + FN}$. By convention, if $TP + FN = 0$, then $R = 0$.\n- F1-score, $F_1 = \\dfrac{2PR}{P + R}$. By convention, if $P + R = 0$, then $F_1 = 0$.\n\nThe simulation proceeds for each candidate schedule. A schedule is a list of steps, with each step $i$ defined by a pair of parameters $(m_i, b_i)$. We begin with a baseline confusion matrix $(TP_0, FP_0, TN_0, FN_0)$. For each schedule, we initialize a temporary state $(TP, FP, TN, FN)$ to these baseline values. Then, for each step $i$ in the schedule, we update the state according to the following deterministic rules:\n\n1.  Determine the number of False Negatives to be corrected. This is the minimum of the number of corrections attempted, $m_i$, and the number of currently available False Negatives, $FN$. Let this number be $c_i = \\min(m_i, FN)$.\n\n2.  Update the counts related to positive instances. The $c_i$ corrected False Negatives become True Positives.\n    $$TP \\leftarrow TP + c_i$$\n    $$FN \\leftarrow FN - c_i$$\n    The total number of actual positive instances, $TP + FN$, remains constant throughout this process, which is a logically consistent property.\n\n3.  Calculate the side-effect, which is an increase in False Positives. This is modeled as a fraction $b_i$ of the correction attempts $c_i$, with the result converted to an integer using the floor function. This increase is also capped by the number of available True Negatives, $TN$. The increase in False Positives is $\\Delta FP_i = \\min(TN, \\lfloor b_i \\cdot c_i \\rfloor)$.\n\n4.  Update the counts related to negative instances. The $\\Delta FP_i$ new False Positives are converted from True Negatives.\n    $$FP \\leftarrow FP + \\Delta FP_i$$\n    $$TN \\leftarrow TN - \\Delta FP_i$$\n    Similarly, the total number of actual negative instances, $FP + TN$, remains constant.\n\nAfter applying all steps in a given schedule, we arrive at a final confusion matrix, from which we compute the final metrics $P_{\\text{final}}$, $R_{\\text{final}}$, and $F_{1,\\text{final}}$.\n\nThis simulation must be performed for all candidate schedules within a test case. We then compare the resulting $F_{1,\\text{final}}$ scores. The schedule that yields the highest $F_{1,\\text{final}}$ is chosen as the optimal one. If multiple schedules result in the same maximum $F_1$-score, the one with the smallest index is selected as per the problem's tie-breaking rule.\n\nOnce the optimal schedule is identified, we compute the final required outputs for that schedule:\n1.  The change in Recall: $\\Delta R = R_{\\text{final}} - R_{\\text{baseline}}$.\n2.  The boolean indicating if Precision decreased: $P_{\\text{final}} < P_{\\text{baseline}}$.\n3.  The maximum F1-score achieved: $F_{1,\\text{max}} = F_{1,\\text{final}}$.\n4.  The index of the optimal schedule.\n\nLet us demonstrate the procedure with the first step of the first schedule in Test Case 1.\n- Baseline: $(TP_0, FP_0, TN_0, FN_0) = (60, 10, 110, 20)$.\n- Schedule 0: $[(10, 0.2), (5, 0.1)]$.\n- Initial state for simulation: $(TP, FP, TN, FN) = (60, 10, 110, 20)$.\n- First step is $(m_1, b_1) = (10, 0.2)$.\n- Number of corrections: $c_1 = \\min(10, FN) = \\min(10, 20) = 10$.\n- Update $TP$ and $FN$: $TP \\leftarrow 60 + 10 = 70$, $FN \\leftarrow 20 - 10 = 10$.\n- Induced False Positives: $\\Delta FP_1 = \\min(TN, \\lfloor b_1 \\cdot c_1 \\rfloor) = \\min(110, \\lfloor 0.2 \\cdot 10 \\rfloor) = \\min(110, 2) = 2$.\n- Update $FP$ and $TN$: $FP \\leftarrow 10 + 2 = 12$, $TN \\leftarrow 110 - 2 = 108$.\n- The state after step 1 is $(70, 12, 108, 10)$. This becomes the input state for the second step of the schedule. This iterative process is repeated for all steps in all schedules to determine the final results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hard example mining problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"baseline\": (60, 10, 110, 20),\n            \"schedules\": [\n                [(10, 0.2), (5, 0.1)],\n                [(15, 0.05)],\n                [(20, 0.3)],\n            ]\n        },\n        {\n            \"baseline\": (30, 20, 150, 0),\n            \"schedules\": [\n                [],\n                [(5, 0.2)],\n                [(1, 1.0), (2, 1.0)],\n            ]\n        },\n        {\n            \"baseline\": (50, 5, 10, 35),\n            \"schedules\": [\n                [(10, 1.0), (10, 1.0)],\n                [(35, 0.6)],\n                [(20, 0.1), (15, 0.1)],\n            ]\n        }\n    ]\n\n    def calculate_metrics(tp, fp, fn):\n        \"\"\"\n        Calculates Precision, Recall, and F1-score from confusion matrix counts.\n        \"\"\"\n        # Precision\n        p_denom = tp + fp\n        p = tp / p_denom if p_denom > 0 else 0.0\n        \n        # Recall\n        r_denom = tp + fn\n        r = tp / r_denom if r_denom > 0 else 0.0\n        \n        # F1-score\n        f1_denom = p + r\n        f1 = (2 * p * r) / f1_denom if f1_denom > 0 else 0.0\n        \n        return p, r, f1\n\n    all_results = []\n    for case in test_cases:\n        tp0, fp0, tn0, fn0 = case[\"baseline\"]\n        schedules = case[\"schedules\"]\n        \n        p_baseline, r_baseline, _ = calculate_metrics(tp0, fp0, fn0)\n        \n        schedule_results = []\n        for schedule in schedules:\n            tp, fp, tn, fn = tp0, fp0, tn0, fn0\n            \n            for m_i, b_i in schedule:\n                c_i = min(m_i, fn)\n                if c_i == 0:\n                    continue # No change if no FNs to correct\n                \n                # Update TP and FN\n                tp += c_i\n                fn -= c_i\n                \n                # Update FP and TN\n                delta_fp = min(tn, int(np.floor(b_i * c_i)))\n                fp += delta_fp\n                tn -= delta_fp\n            \n            p_final, r_final, f1_final = calculate_metrics(tp, fp, fn)\n            schedule_results.append({\n                \"p\": p_final,\n                \"r\": r_final,\n                \"f1\": f1_final\n            })\n\n        # Find the best schedule\n        best_f1 = -1.0\n        best_schedule_index = -1\n        \n        for i, res in enumerate(schedule_results):\n            if res[\"f1\"] > best_f1:\n                best_f1 = res[\"f1\"]\n                best_schedule_index = i\n        \n        # In case all schedules result in F1=0 or there are no schedules\n        if not schedules:\n            best_schedule_index = 0 # Hypothetical, not used in tests\n            p_final, r_final, f1_final = calculate_metrics(tp0, fp0, fn0)\n        elif best_schedule_index == -1: # Handles all-zero F1 scores\n             best_schedule_index = 0\n             best_f1 = schedule_results[0]['f1']\n             p_final = schedule_results[0]['p']\n             r_final = schedule_results[0]['r']\n        else:\n             p_final = schedule_results[best_schedule_index][\"p\"]\n             r_final = schedule_results[best_schedule_index][\"r\"]\n             f1_final = best_f1\n\n        # Calculate final outputs\n        delta_r = r_final - r_baseline\n        precision_decreased = p_final  p_baseline\n        f1_max = f1_final\n        \n        # Format the result string for this test case\n        result_str = (\n            f\"[\"\n            f\"{delta_r:.6f},\"\n            f\"{'True' if precision_decreased else 'False'},\"\n            f\"{f1_max:.6f},\"\n            f\"{best_schedule_index}\"\n            f\"]\"\n        )\n        all_results.append(result_str)\n\n    # Print the final output for all test cases in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3105654"}]}