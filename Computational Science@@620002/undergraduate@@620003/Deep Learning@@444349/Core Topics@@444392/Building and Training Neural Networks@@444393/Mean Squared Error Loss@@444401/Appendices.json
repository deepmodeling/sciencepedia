{"hands_on_practices": [{"introduction": "The choice of a loss function is a critical design decision that directly influences a model's learning behavior. This first exercise provides a foundational comparison between Mean Squared Error (MSE) and Mean Absolute Error (MAE), two of the most common loss functions for regression. By calculating the penalty each function assigns to the same error, you will gain a concrete understanding of how MSE's quadratic nature amplifies the impact of larger prediction mistakes [@problem_id:1931773].", "problem": "A data science team at a meteorological institute is evaluating a new weather prediction model. The model's accuracy is assessed using loss functions, which quantify the penalty for incorrect predictions. Let the true measured temperature on a given day be denoted by $y$, and the model's predicted temperature be $\\hat{y}$.\n\nTwo common loss functions are being considered:\n1.  The Squared Error Loss, defined as $L_2(y, \\hat{y}) = (y - \\hat{y})^2$.\n2.  The Absolute Error Loss, defined as $L_1(y, \\hat{y}) = |y - \\hat{y}|$.\n\nOn a specific test day in the Antarctic, the model's temperature prediction is off by exactly $3.5$ Kelvin. Calculate the numerical value of the ratio of the penalty assigned by the squared error loss to the penalty assigned by the absolute error loss for this specific prediction.", "solution": "The problem asks for the ratio of the squared error loss to the absolute error loss for a given prediction error. Let the true temperature be $y$ and the predicted temperature be $\\hat{y}$.\n\nThe squared error loss is given by the formula:\n$$L_2(y, \\hat{y}) = (y - \\hat{y})^2$$\n\nThe absolute error loss is given by the formula:\n$$L_1(y, \\hat{y}) = |y - \\hat{y}|$$\n\nWe are given that the magnitude of the prediction error is $3.5$ Kelvin. This means that the absolute difference between the true value and the predicted value is $3.5$. Mathematically, this can be written as:\n$$|y - \\hat{y}| = 3.5$$\n\nNow, we can calculate the value of each loss function for this specific error.\n\nFor the absolute error loss, we can directly substitute the given error magnitude:\n$$L_1 = |y - \\hat{y}| = 3.5$$\nThe penalty from the absolute error loss is $3.5$.\n\nFor the squared error loss, we use the fact that $(y-\\hat{y})^2 = (|y-\\hat{y}|)^2$. Substituting the given error magnitude:\n$$L_2 = (y - \\hat{y})^2 = (|y - \\hat{y}|)^2 = (3.5)^2$$\nCalculating the value:\n$$L_2 = 3.5 \\times 3.5 = 12.25$$\nThe penalty from the squared error loss is $12.25$.\n\nThe problem asks for the ratio of the squared error loss to the absolute error loss. Let's call this ratio $R$.\n$$R = \\frac{L_2(y, \\hat{y})}{L_1(y, \\hat{y})}$$\n\nSubstituting the calculated values for the losses:\n$$R = \\frac{12.25}{3.5}$$\n\nTo simplify this fraction, we can express $12.25$ as $(3.5)^2$:\n$$R = \\frac{(3.5)^2}{3.5} = 3.5$$\n\nThus, for a prediction error of $3.5$ Kelvin, the penalty from the squared error loss is $3.5$ times larger than the penalty from the absolute error loss. The units of temperature (Kelvin for the error, Kelvin$^2$ for $L_2$, Kelvin for $L_1$) cancel out in the ratio, resulting in a dimensionless quantity.", "answer": "$$\\boxed{3.5}$$", "id": "1931773"}, {"introduction": "Beyond penalizing errors, minimizing the MSE loss over a dataset has a profound statistical interpretation. This practice guides you through deriving and demonstrating that the optimal constant prediction for a set of data points is their arithmetic mean, the value that minimizes the squared error. You will then extend this principle to a weighted scenario, revealing how MSE naturally accommodates sample importance and provides the foundation for understanding its role in conditional mean prediction [@problem_id:3148476].", "problem": "Consider a supervised learning setup in which a model predicts a scalar target given a discrete input. Let the dataset be $\\{(x_i,y_i)\\}_{i=1}^n$ with $x_i \\in \\mathcal{X}$ and $y_i \\in \\mathbb{R}$. Define the per-sample loss as the squared error $\\ell(\\hat{y},y) = (\\hat{y}-y)^2$, and define the Mean Squared Error (MSE) empirical risk over a subset $\\mathcal{D} \\subseteq \\{1,\\dots,n\\}$ as $L(\\theta \\mid \\mathcal{D}) = \\sum_{i \\in \\mathcal{D}} (\\theta - y_i)^2$ for a scalar predictor $\\theta$ that is constant within the subset. In the conditional prediction setting with discrete inputs, consider a model that predicts a separate constant for each input value: for each $x \\in \\mathcal{X}$, the model returns $f(x) = \\theta_x$, and the empirical risk decomposes by input value as $L_{\\text{emp}}(\\{\\theta_x\\}) = \\sum_{x \\in \\mathcal{X}} \\sum_{i : x_i = x} (\\theta_x - y_i)^2$.\n\nStarting from the definitions above and first principles of convex optimization (for example, minimizing a differentiable convex quadratic by setting the derivative to zero), derive what scalar $\\theta_x$ minimizes the empirical risk restricted to a single input value $x$ when samples for that input may be duplicated. Let each sample index $i$ carry a nonnegative integer duplication count $c_i \\in \\{0,1,2,\\dots\\}$, and suppose the empirical risk for input $x$ is $L_x(\\theta_x) = \\sum_{i : x_i = x} c_i \\, (\\theta_x - y_i)^2$. Express the minimizer in terms of $\\{y_i\\}$ and $\\{c_i\\}$, and explain why duplication counts effectively act as weights that can bias the minimizer away from the unweighted conditional mean.\n\nThen, implement a program that performs the following deterministic simulation. Use a dataset with two input values and the following labels:\n- For $x=0$, let the labels be $y$ values $[1.0, 3.0]$.\n- For $x=1$, let the labels be $y$ values $[0.0, 4.0]$.\n\nFor each test case below, let the duplication counts per sample be specified as arrays aligned with the label order of each input value. For example, for $x=0$ the first count applies to $y=1.0$ and the second to $y=3.0$, and for $x=1$ the first count applies to $y=0.0$ and the second to $y=4.0$. For each test case:\n- Compute the original per-input empirical minimizers (with all duplication counts equal to $1$).\n- Compute the duplicated per-input empirical minimizers using the provided counts.\n- Output, for each input value $x \\in \\{0,1\\}$, the difference $\\Delta_x = \\theta_x^{\\text{dup}} - \\theta_x^{\\text{orig}}$.\n\nDesign a test suite that covers varied duplication scenarios:\n- Test case $1$ (happy path): $x=0$ counts $[1,1]$, $x=1$ counts $[1,3]$.\n- Test case $2$ (boundary, no duplication): $x=0$ counts $[1,1]$, $x=1$ counts $[1,1]$.\n- Test case $3$ (edge, heavy duplication in $x=0$): $x=0$ counts $[5,1]$, $x=1$ counts $[1,1]$.\n- Test case $4$ (edge, balanced duplication in both inputs): $x=0$ counts $[2,2]$, $x=1$ counts $[2,2]$.\n- Test case $5$ (edge, removal of one label in $x=1$ by zero duplication): $x=0$ counts $[1,1]$, $x=1$ counts $[0,2]$.\n\nThe required final output format is a single line containing the list of results, one per test case, where each result is the list $[\\Delta_0,\\Delta_1]$ for that case. The format must be a comma-separated list enclosed in square brackets, such as $[[\\Delta_0^{(1)},\\Delta_1^{(1)}],[\\Delta_0^{(2)},\\Delta_1^{(2)}],\\dots]$, with numeric values represented as standard decimal floats. No physical units or angle units apply; all numeric outputs should be real numbers. Your program should produce exactly one line with this aggregate list and no other text.", "solution": "The problem asks for the derivation of the optimal scalar predictor $\\theta_x$ that minimizes a weighted empirical risk for a specific input value $x$. The risk function is defined as the weighted sum of squared errors.\n\nLet the set of sample indices corresponding to a discrete input value $x \\in \\mathcal{X}$ be denoted by $I_x = \\{i \\mid x_i = x\\}$. The empirical risk for this input value, incorporating nonnegative integer duplication counts $c_i$, is given by:\n$$\nL_x(\\theta_x) = \\sum_{i \\in I_x} c_i (\\theta_x - y_i)^2\n$$\nHere, $\\theta_x$ is the constant prediction for input $x$, $\\{y_i\\}_{i \\in I_x}$ are the corresponding target values, and $\\{c_i\\}_{i \\in I_x}$ are the duplication counts.\n\nThe function $L_x(\\theta_x)$ is a quadratic function of $\\theta_x$. It can be expanded as:\n$$\nL_x(\\theta_x) = \\sum_{i \\in I_x} c_i (\\theta_x^2 - 2\\theta_x y_i + y_i^2) = \\left(\\sum_{i \\in I_x} c_i\\right)\\theta_x^2 - \\left(2\\sum_{i \\in I_x} c_i y_i\\right)\\theta_x + \\left(\\sum_{i \\in I_x} c_i y_i^2\\right)\n$$\nThis is an upward-opening parabola in $\\theta_x$ as long as the coefficient of the $\\theta_x^2$ term, $\\sum_{i \\in I_x} c_i$, is positive. This condition holds if at least one sample for input $x$ has a count $c_i > 0$. Under this condition, the function is strictly convex and has a unique global minimum.\n\nTo find the value of $\\theta_x$ that minimizes $L_x(\\theta_x)$, we apply the first-order necessary condition for optimality by taking the derivative of $L_x(\\theta_x)$ with respect to $\\theta_x$ and setting it to zero.\n\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\frac{d}{d\\theta_x} \\left( \\sum_{i \\in I_x} c_i (\\theta_x - y_i)^2 \\right)\n$$\n\nBy linearity of differentiation, we can move the derivative inside the summation:\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\sum_{i \\in I_x} \\frac{d}{d\\theta_x} \\left( c_i (\\theta_x - y_i)^2 \\right)\n$$\n\nApplying the chain rule, where $\\frac{d}{du}(u^2) = 2u$:\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\sum_{i \\in I_x} c_i \\cdot 2(\\theta_x - y_i) \\cdot \\frac{d}{d\\theta_x}(\\theta_x - y_i) = \\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i)\n$$\n\nSetting the derivative to zero to find the critical point:\n$$\n\\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i) = 0\n$$\n\nWe can divide by the constant $2$ and distribute the summation:\n$$\n\\sum_{i \\in I_x} c_i \\theta_x - \\sum_{i \\in I_x} c_i y_i = 0\n$$\n\nSince $\\theta_x$ is a constant with respect to the index $i$, it can be factored out of the first sum:\n$$\n\\theta_x \\left( \\sum_{i \\in I_x} c_i \\right) = \\sum_{i \\in I_x} c_i y_i\n$$\n\nSolving for $\\theta_x$, we obtain the minimizer, which we will denote as $\\theta_x^*$:\n$$\n\\theta_x^* = \\frac{\\sum_{i \\in I_x} c_i y_i}{\\sum_{i \\in I_x} c_i}\n$$\nThis assumes that $\\sum_{i \\in I_x} c_i \\neq 0$. If all counts were zero, $L_x(\\theta_x)$ would be identically zero, and any $\\theta_x$ would be a minimizer. The problem context, however, ensures this denominator is non-zero in all test cases.\n\nTo confirm this critical point is a minimum, we examine the second derivative:\n$$\n\\frac{d^2L_x(\\theta_x)}{d\\theta_x^2} = \\frac{d}{d\\theta_x} \\left( \\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i) \\right) = \\sum_{i \\in I_x} 2 c_i\n$$\nSince the counts $c_i$ are nonnegative integers and at least one is positive, the second derivative $\\sum_{i \\in I_x} 2 c_i > 0$. A positive second derivative confirms that the function $L_x(\\theta_x)$ is strictly convex and that the critical point $\\theta_x^*$ is indeed a unique global minimum.\n\nThe derived expression for $\\theta_x^*$ is a weighted average of the target values $\\{y_i\\}_{i \\in I_x}$, where the weights are the duplication counts $\\{c_i\\}_{i \\in I_x}$. In the absence of duplication, all counts are $c_i=1$. The expression then simplifies to:\n$$\n\\theta_x^* \\text{ (unweighted)} = \\frac{\\sum_{i \\in I_x} y_i}{\\sum_{i \\in I_x} 1} = \\frac{\\sum_{i \\in I_x} y_i}{|I_x|}\n$$\nThis is the unweighted conditional mean of the target values for the given input $x$.\n\nWhen the counts $c_i$ are not uniform, they act as weights that give more importance to samples with higher duplication. A sample $(x_i, y_i)$ with a large count $c_i$ will contribute more significantly to both the numerator and the denominator of the weighted average. Consequently, the optimal predictor $\\theta_x^*$ is \"pulled\" towards those $y_i$ values that are associated with higher counts. This demonstrates how duplication counts, or more generally sample weights, can bias the minimizer of the squared error loss away from the unweighted conditional mean and toward specific data points deemed more important.\n\nThe simulation will apply this formula to compute the minimizers under different duplication scenarios and quantify the resulting bias as the difference $\\Delta_x = \\theta_x^{\\text{dup}} - \\theta_x^{\\text{orig}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and simulates the effect of sample duplication on the minimizer\n    of the Mean Squared Error loss for a conditional model.\n    \"\"\"\n    \n    # Dataset of labels for each discrete input value x.\n    y_values = {\n        0: np.array([1.0, 3.0]),\n        1: np.array([0.0, 4.0])\n    }\n\n    # Test cases defining the duplication counts for each label in y_values.\n    test_cases = [\n        # Test case 1 (happy path)\n        {'counts_x0': [1, 1], 'counts_x1': [1, 3]},\n        # Test case 2 (boundary, no duplication)\n        {'counts_x0': [1, 1], 'counts_x1': [1, 1]},\n        # Test case 3 (edge, heavy duplication in x=0)\n        {'counts_x0': [5, 1], 'counts_x1': [1, 1]},\n        # Test case 4 (edge, balanced duplication in both inputs)\n        {'counts_x0': [2, 2], 'counts_x1': [2, 2]},\n        # Test case 5 (edge, removal of one label in x=1)\n        {'counts_x0': [1, 1], 'counts_x1': [0, 2]},\n    ]\n    \n    def compute_minimizer(y, counts):\n        \"\"\"\n        Computes the weighted average that minimizes the squared error.\n        theta_x = (sum(c_i * y_i)) / (sum(c_i))\n        \"\"\"\n        c = np.array(counts, dtype=float)\n        numerator = np.sum(c * y)\n        denominator = np.sum(c)\n        if denominator == 0:\n            # This case will not be reached in this problem but is good practice.\n            return np.nan \n        return numerator / denominator\n\n    # Compute the original minimizers (all duplication counts are 1).\n    orig_counts = [1, 1]\n    theta0_orig = compute_minimizer(y_values[0], orig_counts)\n    theta1_orig = compute_minimizer(y_values[1], orig_counts)\n\n    results = []\n    \n    for case in test_cases:\n        counts0 = case['counts_x0']\n        counts1 = case['counts_x1']\n        \n        # Compute the duplicated (weighted) minimizers.\n        theta0_dup = compute_minimizer(y_values[0], counts0)\n        theta1_dup = compute_minimizer(y_values[1], counts1)\n        \n        # Calculate the differences.\n        delta0 = theta0_dup - theta0_orig\n        delta1 = theta1_dup - theta1_orig\n        \n        results.append([delta0, delta1])\n        \n    # Format the final output string as a list of lists.\n    # e.g., [[d0,d1],[d0,d1],...]\n    output_str = f\"[{','.join(f'[{d[0]},{d[1]}]' for d in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3148476"}, {"introduction": "To train a deep neural network, we must calculate how a change in each parameter affects the final loss—a process powered by backpropagation. This exercise connects the theory of MSE to its practical application in deep learning by asking you to perform a key step in this process. You will derive the partial derivatives of the MSE loss with respect to the learnable scale ($\\gamma$) and shift ($\\beta$) parameters of a Layer Normalization block, a common component in modern network architectures [@problem_id:3148553].", "problem": "Consider a supervised learning setting in deep learning with mini-batch size $B$ and per-sample output dimensionality $d$. For each sample $b \\in \\{1,\\dots,B\\}$, the network produces a raw output vector $\\hat{\\mathbf{y}}^{(b)} \\in \\mathbb{R}^{d}$ and is trained against a target vector $\\mathbf{t}^{(b)} \\in \\mathbb{R}^{d}$. The model applies layer normalization at the output using a single scalar scale parameter $\\gamma \\in \\mathbb{R}$ and a single scalar shift parameter $\\beta \\in \\mathbb{R}$, shared across all features. For each sample $b$, define the sample mean\n$$\n\\mu^{(b)} \\equiv \\frac{1}{d} \\sum_{i=1}^{d} \\hat{y}^{(b)}_{i},\n$$\nand the sample standard deviation\n$$\n\\sigma^{(b)} \\equiv \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} \\left(\\hat{y}^{(b)}_{i} - \\mu^{(b)}\\right)^{2} + \\epsilon},\n$$\nwhere $\\epsilon > 0$ is a fixed constant. The normalized output components are\n$$\ny^{(b)}_{\\text{norm},i} \\equiv \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\quad \\text{for } i \\in \\{1,\\dots,d\\}.\n$$\nTraining uses the Mean Squared Error (MSE) loss averaged over the batch and features,\n$$\nL \\equiv \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}.\n$$\nStarting only from the definition of $L$, the definitions of $\\mu^{(b)}$ and $\\sigma^{(b)}$, and standard rules of differential calculus (in particular, the chain rule and linearity), derive closed-form expressions for the two partial derivatives $\\frac{\\partial L}{\\partial \\gamma}$ and $\\frac{\\partial L}{\\partial \\beta}$ in terms of $\\gamma$, $\\beta$, $\\hat{\\mathbf{y}}^{(b)}$, $\\mathbf{t}^{(b)}$, $\\mu^{(b)}$, $\\sigma^{(b)}$, $B$, $d$, and $\\epsilon$. Express the final answer as two explicit analytic sums with no undefined symbols. No numerical approximation is required, and no rounding is needed. Your final answer must consist of these two derivatives only.", "solution": "The problem requires the derivation of the partial derivatives of the Mean Squared Error (MSE) loss function, $L$, with respect to the scalar layer normalization parameters $\\gamma$ and $\\beta$. The derivation will proceed from the provided definitions using the standard rules of differential calculus, most notably the chain rule.\n\nThe loss function $L$ is defined as:\n$$L \\equiv \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}$$\nThe normalized output for sample $b$ and feature $i$, $y^{(b)}_{\\text{norm},i}$, is given by:\n$$y^{(b)}_{\\text{norm},i} \\equiv \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta$$\nThe sample mean $\\mu^{(b)}$ and sample standard deviation $\\sigma^{(b)}$ are computed from the raw network outputs $\\hat{\\mathbf{y}}^{(b)}$. As such, they do not depend on the parameters $\\gamma$ and $\\beta$. This is a crucial observation for the subsequent differentiation.\n\nWe can establish a general formula for the derivative of $L$ with respect to a generic parameter $\\theta$ by applying the chain rule.\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2} \\right]$$\nBy linearity of differentiation, we can move the derivative operator inside the sums:\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\frac{\\partial}{\\partial \\theta} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}$$\nApplying the chain rule to the squared term yields:\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} 2 \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\frac{\\partial}{\\partial \\theta}\\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)$$\nThe target values $t^{(b)}_{i}$ are fixed constants with respect to the model parameters, so their derivative is zero. The expression simplifies to:\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\theta}$$\nThis general form provides a template for finding the required partial derivatives.\n\n**1. Derivation of the partial derivative with respect to $\\gamma$ ($\\frac{\\partial L}{\\partial \\gamma}$)**\n\nWe set $\\theta = \\gamma$ in the general formula. The first step is to compute the partial derivative of $y^{(b)}_{\\text{norm},i}$ with respect to $\\gamma$:\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\right)$$\nSince $\\beta$, $\\mu^{(b)}$, and $\\sigma^{(b)}$ are constant with respect to $\\gamma$, this derivative is:\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\gamma} = \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}}$$\nSubstituting this result into the general formula for $\\frac{\\partial L}{\\partial \\theta}$:\n$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\left(\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}}\\right)$$\nFinally, to express the result solely in terms of the specified input variables, we substitute the definition of $y^{(b)}_{\\text{norm},i}$:\n$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right) \\left( \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} \\right)$$\n\n**2. Derivation of the partial derivative with respect to $\\beta$ ($\\frac{\\partial L}{\\partial \\beta}$)**\n\nWe set $\\theta = \\beta$ in the general formula. We first compute the partial derivative of $y^{(b)}_{\\text{norm},i}$ with respect to $\\beta$:\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\right)$$\nThe first term is constant with respect to $\\beta$, so the derivative is simply:\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\beta} = 1$$\nSubstituting this result into the general formula for $\\frac{\\partial L}{\\partial \\theta}$:\n$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) (1)$$\nAgain, we substitute the definition of $y^{(b)}_{\\text{norm},i}$ to obtain the final expression:\n$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right)$$\n\nThe two derived expressions are closed-form analytic sums expressed in terms of the variables specified in the problem statement.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right) \\left( \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} \\right) & \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right)\n\\end{pmatrix}\n}\n$$", "id": "3148553"}]}