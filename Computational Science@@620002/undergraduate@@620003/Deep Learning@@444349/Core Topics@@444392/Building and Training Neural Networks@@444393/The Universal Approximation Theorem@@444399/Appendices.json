{"hands_on_practices": [{"introduction": "The Universal Approximation Theorem famously tells us that a neural network *can* approximate any continuous function, but it often feels like a magic trick. This first exercise demystifies the theorem by putting you in the designer's seat. We will move from an abstract guarantee to a concrete construction, building a network that approximates a piecewise constant function, which is a fundamental building block for more complex signals [@problem_id:3194212]. By using sigmoid neurons as smooth \"switches,\" you will see precisely how to combine simple components to create a specified output and determine the minimal number of neurons required, directly linking network size to the complexity of the function.", "problem": "Consider a single-hidden-layer Artificial Neural Network (ANN) with a logistic sigmoid activation $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ in the hidden layer and a linear output. The network takes a scalar input $x \\in [0,1]$ and outputs a scalar. Let $f:[0,1] \\to \\mathbb{R}$ be a piecewise constant function with $k$ plateaus, with breakpoints $0 = s_{0} < s_{1} < \\dots < s_{k-1} < s_{k} = 1$ and plateau levels $h_{1}, h_{2}, \\dots, h_{k}$, where each $h_{i} \\in [0,1]$. Assume the true transitions of $f$ occur only in the $\\delta$-neighborhoods of the internal breakpoints, i.e., $f$ is constant on each $[s_{i-1}+\\frac{\\delta}{2},\\,s_{i}-\\frac{\\delta}{2}]$, and changes only within each $(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2})$ for $i \\in \\{1,\\dots,k-1\\}$.\n\nYou are asked to construct an explicit approximation $\\hat{f}$ by a single-hidden-layer ANN of the form\n$$\n\\hat{f}(x) = c_{0} + \\sum_{j=1}^{N} c_{j}\\,\\sigma\\!\\big(\\alpha(x - b_{j})\\big),\n$$\nwhere $N$ is the number of hidden neurons, $c_{0}, c_{1}, \\dots, c_{N} \\in \\mathbb{R}$ are output-layer weights (with $c_{0}$ being the bias), $b_{1},\\dots,b_{N} \\in [0,1]$ are hidden-layer biases that locate transitions, and $\\alpha > 0$ is a common steepness parameter controlling the width of the smooth transition.\n\nSubject to choosing $b_{j}$ at the internal breakpoints and appropriate $c_{j}$ and $c_{0}$ to match the plateau levels, enforce the uniform error constraint\n$$\n\\sup_{x \\in [0,1]\\setminus \\bigcup_{i=1}^{k-1}\\left(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2}\\right)}\\big|\\,\\hat{f}(x)-f(x)\\,\\big| \\le \\epsilon,\n$$\nby an appropriate choice of $\\alpha$ that yields smooth transitions of width at most $\\delta$.\n\nStarting from the definitions of the logistic sigmoid and the Heaviside step function, and using only well-tested facts about single-hidden-layer ANN function composition and linear combinations, derive an explicit construction and error bound, and determine the minimal number $N$ of hidden neurons needed to realize $k$ plateaus with the above uniform error constraint away from the transition neighborhoods. Provide $N$ as a function of $k$.\n\nYour final answer must be a single closed-form expression for $N$ in terms of $k$. No rounding is required, and no physical units are involved.", "solution": "We begin with the logistic sigmoid definition $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$. The Heaviside step function $H(x - s)$, which equals $0$ for $x < s$ and $1$ for $x \\ge s$, can be approximated by the smooth sigmoid $\\sigma(\\alpha(x - s))$, where $\\alpha > 0$ controls the steepness and hence the transition width. As $\\alpha \\to \\infty$, $\\sigma(\\alpha(x - s))$ converges pointwise to $H(x - s)$ for all $x \\neq s$.\n\nA piecewise constant function with breakpoints $s_{1},\\dots,s_{k-1}$ and plateau heights $h_{1},\\dots,h_{k}$ can be represented exactly using the Heaviside step function as\n$$\nf(x) = h_{1} + \\sum_{i=1}^{k-1}\\big(h_{i+1}-h_{i}\\big)\\,H(x - s_{i}).\n$$\nTo verify this identity, note that for any $x \\in [s_{m-1}, s_{m})$ with $m \\in \\{1,\\dots,k\\}$, we have $H(x - s_{i}) = 0$ for all $i \\ge m$ and $H(x - s_{i}) = 1$ for all $i < m$, so\n$$\nf(x) = h_{1} + \\sum_{i=1}^{m-1}\\big(h_{i+1}-h_{i}\\big) = h_{m},\n$$\nby telescoping.\n\nTo obtain a single-hidden-layer ANN approximation, we replace $H(x - s_{i})$ by $\\sigma\\!\\big(\\alpha(x - s_{i})\\big)$ and define\n$$\n\\hat{f}(x) = c_{0} + \\sum_{i=1}^{k-1} c_{i}\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big),\n$$\nwith coefficients chosen as $c_{0} = h_{1}$ and $c_{i} = h_{i+1} - h_{i}$ for $i \\in \\{1,\\dots,k-1\\}$. This yields\n$$\n\\hat{f}(x) = h_{1} + \\sum_{i=1}^{k-1}\\big(h_{i+1}-h_{i}\\big)\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big).\n$$\nTo enforce the uniform error bound away from transitions, we must bound the deviation of each sigmoid from its ideal step value outside $(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2})$.\n\nFix an internal breakpoint $s$, and consider $x \\le s - \\frac{\\delta}{2}$. Then $\\alpha(x - s) \\le -\\alpha\\frac{\\delta}{2}$, so\n$$\n\\sigma\\!\\big(\\alpha(x - s)\\big) \\le \\sigma\\!\\big(-\\alpha\\frac{\\delta}{2}\\big) = \\frac{1}{1+\\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big)}.\n$$\nSimilarly, for $x \\ge s + \\frac{\\delta}{2}$, we have\n$$\n\\sigma\\!\\big(\\alpha(x - s)\\big) \\ge \\sigma\\!\\big(\\alpha\\frac{\\delta}{2}\\big) = \\frac{1}{1+\\exp\\!\\big(-\\alpha\\frac{\\delta}{2}\\big)}.\n$$\nTherefore, if we require that for each transition\n$$\n\\sigma\\!\\big(-\\alpha\\frac{\\delta}{2}\\big) \\le \\varepsilon' \\quad\\text{and}\\quad \\sigma\\!\\big(\\alpha\\frac{\\delta}{2}\\big) \\ge 1 - \\varepsilon',\n$$\nthen outside the $\\delta$-neighborhood of $s$ the sigmoid approximates the step to within $\\varepsilon'$ uniformly. Both inequalities are equivalent to the same constraint on $\\alpha$:\n\n$$\n\\frac{1}{1+\\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big)} \\le \\varepsilon' \\quad\\Longleftrightarrow\\quad \\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big) \\ge \\frac{1-\\varepsilon'}{\\varepsilon'} \\quad\\Longleftrightarrow\\quad \\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\varepsilon'}{\\varepsilon'}\\Big).\n$$\n\nHence choosing\n$$\n\\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\varepsilon'}{\\varepsilon'}\\Big)\n$$\nguarantees each sigmoid matches its ideal step to within $\\varepsilon'$ outside the corresponding $\\delta$-neighborhood.\n\nWe now bound the uniform error of $\\hat{f}$ away from the union of all transition neighborhoods. For any $x$ away from transitions, each $\\sigma\\!\\big(\\alpha(x - s_{i})\\big)$ differs from its ideal $H(x - s_{i})$ value by at most $\\varepsilon'$. Using the triangle inequality,\n$$\n\\big|\\,\\hat{f}(x) - f(x)\\,\\big| \\le \\sum_{i=1}^{k-1} |c_{i}|\\,\\big|\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big) - H(x - s_{i})\\,\\big| \\le \\varepsilon' \\sum_{i=1}^{k-1} |h_{i+1}-h_{i}|.\n$$\nLet the total variation across breakpoints be\n$$\nV \\equiv \\sum_{i=1}^{k-1} |h_{i+1}-h_{i}|.\n$$\nTo enforce the target uniform error $\\epsilon$, it suffices to set\n$$\n\\varepsilon' = \\frac{\\epsilon}{V} \\quad \\text{if } V > 0,\n$$\nwith the convention that if $V = 0$ (the function is constant, i.e., $k=1$), then no hidden neurons are needed and the approximation is exact with $c_{0} = h_{1}$.\n\nBecause each $h_{i} \\in [0,1]$, a worst-case bound is $V \\le k-1$, attained when the plateaus alternate between $0$ and $1$. Thus a conservative choice is $\\varepsilon' = \\frac{\\epsilon}{k-1}$ for $k \\ge 2$, which leads to a sufficient steepness\n$$\n\\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\frac{\\epsilon}{k-1}}{\\frac{\\epsilon}{k-1}}\\Big) = \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{k-1-\\epsilon}{\\epsilon}\\Big).\n$$\nThis guarantees the uniform error constraint away from the transition neighborhoods.\n\nWe now count the minimal number of hidden neurons $N$ required. The constructive representation above uses one sigmoid per internal breakpoint $s_{i}$, i.e., $N = k-1$, and the output bias $c_{0}=h_{1}$. To argue minimality, note that with a single-hidden-layer network using a monotone activation per neuron, the derivative of $\\hat{f}$ with respect to $x$ can be written as\n$$\n\\frac{d\\hat{f}}{dx}(x) = \\alpha \\sum_{i=1}^{N} c_{i}\\,\\sigma\\!\\big(\\alpha(x - b_{i})\\big)\\,\\big(1 - \\sigma\\!\\big(\\alpha(x - b_{i})\\big)\\big),\n$$\nwhich is a sum of $N$ localized, unimodal bumps centered near the $b_{i}$. A function with $k$ plateaus that changes only within the $(k-1)$ disjoint $\\delta$-neighborhoods must exhibit $(k-1)$ localized changes in value, hence at least $(k-1)$ localized bumps in its derivative away from flat regions. Since each hidden neuron contributes at most one such localized bump, any realization must satisfy $N \\ge k-1$. Combining this lower bound with the explicit construction that achieves $N = k-1$, we conclude that the minimal number of hidden neurons is\n$$\nN(k) = k - 1.\n$$\n\nTherefore, the minimal number of hidden neurons needed to build $k$ plateaus on $[0,1]$ with smooth transitions of width $\\delta$ and uniform error at most $\\epsilon$ away from transitions, using a single-hidden-layer ANN with logistic sigmoid activation and linear output, is $k-1$.", "answer": "$$\\boxed{k-1}$$", "id": "3194212"}, {"introduction": "Now that we've seen *that* a network can be constructed to approximate a function, a critical practical question arises: what is the cost? Specifically, how many neurons do we need to achieve a desired accuracy $\\epsilon$? This practice explores the efficiency of approximation by focusing on a function with a region of high curvature [@problem_id:3194159]. You will analyze how a network built with Rectified Linear Unit (ReLU) activations, which create piecewise-linear functions, must allocate its resources to capture sharp changes, deriving a fundamental relationship between network width, approximation error, and the target function's properties.", "problem": "Consider the one-dimensional target function $f:[-1,1] \\to \\mathbb{R}$ defined by the parameters $\\alpha>0$ and $\\delta\\in(0,1)$ as follows: \n$$\nf(x) \\;=\\; \n\\begin{cases}\n\\alpha x^{2}, & |x|\\le \\delta,\\\\\n\\alpha \\delta^{2} \\;+\\; 2\\alpha \\delta\\,(|x|-\\delta), & \\delta<|x|\\le 1.\n\\end{cases}\n$$\nThus $f$ has a localized region of high curvature of width $2\\delta$ around $x=0$, and is linear outside that region. Let $\\epsilon \\in (0,1)$ be a target uniform error. You will approximate $f$ on $[-1,1]$ by a one-hidden-layer Rectified Linear Unit (ReLU) network, where the ReLU activation is $\\sigma(t)=\\max\\{0,t\\}$, so the network realizes continuous piecewise-linear functions on $[-1,1]$ whose number of linear pieces is at most the number of hidden neurons plus $1$.\n\nUsing only fundamental facts about linear spline interpolation error for twice differentiable functions and the representational property of one-hidden-layer ReLU networks as continuous piecewise-linear functions, derive an explicit function $U(\\epsilon,\\alpha,\\delta)$ such that there exists a one-hidden-layer ReLU network of width at most $U(\\epsilon,\\alpha,\\delta)$ whose output $g$ satisfies \n$$\n\\sup_{x\\in[-1,1]} |f(x)-g(x)| \\le \\epsilon.\n$$\nYour answer must be a single closed-form expression for $U(\\epsilon,\\alpha,\\delta)$ (do not use inequalities or rounding functions in your final expression). No numerical rounding is required.", "solution": "To find the upper bound on the network width, we will construct a continuous piecewise-linear (CPWL) function $g(x)$ that approximates the target function $f(x)$ with the desired uniform error $\\epsilon$. The number of neurons required to represent this CPWL function will give us our upper bound $U(\\epsilon, \\alpha, \\delta)$.\n\nThe function $f(x)$ is already piecewise-linear for $|x| \\in (\\delta, 1]$, so no approximation error occurs in this region. We only need to approximate the non-linear part, $f(x) = \\alpha x^{2}$, on the interval $[-\\delta, \\delta]$. We will use linear spline interpolation for this task.\n\nThe error bound for linear interpolation on an interval $[a,b]$ for a twice-differentiable function $h(x)$ is given by $\\sup |h(x) - L(x)| \\le \\frac{1}{8}(b-a)^2 \\sup |h''(t)|$. For $h(x)=\\alpha x^2$, the second derivative is constant: $h''(x)=2\\alpha$.\n\nLet's partition $[-\\delta, \\delta]$ into $N$ equal subintervals, each of width $h_s = 2\\delta/N$. The error on each subinterval is bounded by:\n$$ \\frac{1}{8}h_s^2 (2\\alpha) = \\frac{1}{8} \\left(\\frac{2\\delta}{N}\\right)^2 (2\\alpha) = \\frac{\\alpha\\delta^2}{N^2} $$\nTo ensure the uniform error over $[-\\delta, \\delta]$ is at most $\\epsilon$, we set $\\frac{\\alpha\\delta^2}{N^2} \\le \\epsilon$, which implies:\n$$ N \\ge \\sqrt{\\frac{\\alpha \\delta^2}{\\epsilon}} = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} $$\nA CPWL function with $N$ linear segments on $[-\\delta, \\delta]$ requires $N-1$ interior knots. The full approximation $g(x)$ over $[-1, 1]$ also has knots at $x=\\pm\\delta$, where the quadratic approximation meets the linear parts of the original function. This gives a total of $(N-1)+2 = N+1$ knots. A one-hidden-layer ReLU network can represent any such CPWL function using a number of neurons equal to the number of knots. Thus, the required width is $W = N+1$.\n\nSubstituting the condition for $N$, we get:\n$$ W \\ge \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1 $$\nSince the width $W$ must be an integer, the minimum required width is $W_{\\text{min}} = \\left\\lceil \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1 \\right\\rceil$.\nThe problem asks for a simple closed-form upper bound $U$ without rounding functions. We can use the property $\\lceil y \\rceil \\le y + 1$. This gives:\n$$ W \\le \\left(\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1\\right) + 1 = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2 $$\nTherefore, a valid upper bound for the network width is $U(\\epsilon,\\alpha,\\delta) = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2$.", "answer": "$$\\boxed{\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2}$$", "id": "3194159"}, {"introduction": "The power of the Universal Approximation Theorem relies on certain assumptions about the network's components. But what happens if we impose constraints, for instance, requiring all weights to be non-negative? This practice challenges you to explore the boundaries of universality by analyzing a special class of networks [@problem_id:3194145]. You will discover that such constraints can fundamentally limit the types of functions a network can represent, revealing that universality is not guaranteed and depends critically on the flexibility of the network's parameters. This exercise highlights the importance of understanding not just what neural networks can do, but also what they cannot.", "problem": "Let $K \\subset \\mathbb{R}^d$ be a nonempty compact set and let $f: K \\to \\mathbb{R}$ be continuous with $f(x) < 0$ for all $x \\in K$. Consider feedforward neural networks with the Rectified Linear Unit (ReLU) activation, where Rectified Linear Unit (ReLU) is defined by $\\sigma(t) = \\max\\{0, t\\}$. Assume that every weight matrix $W^{(\\ell)}$ at layer $\\ell \\in \\{1, \\dots, L\\}$ has nonnegative entries (elementwise), while biases $b^{(\\ell)}$ are unconstrained real numbers. The network outputs a scalar $N(x) \\in \\mathbb{R}$.\n\nStarting only from the definitions that: \n- $\\sigma$ is coordinatewise nondecreasing, \n- affine maps with nonnegative coefficients are coordinatewise nondecreasing, and \n- compositions of coordinatewise nondecreasing maps are coordinatewise nondecreasing,\nanswer the following:\n\nWhich of the following statements are correct?\n\nA. Because biases $b^{(\\ell)}$ can be negative, the class of such ReLU networks with nonnegative weights is dense in $C(K)$, the space of continuous real-valued functions on $K$ with the uniform norm. Therefore, any continuous negative-valued $f$ on $K$ can be uniformly approximated.\n\nB. Any function computed by such a network is coordinatewise nondecreasing on $K$. Hence, if $f$ is not coordinatewise nondecreasing, it cannot be uniformly approximated by these networks, regardless of how negative the output bias is chosen. A practical workaround to produce negative outputs is to select a constant $c > -\\inf_{x \\in K} f(x)$ so that $f + c \\ge 0$ on $K$, attempt to approximate $f + c$ with a nonnegatively weighted ReLU network, and then set the final-layer bias to $-c$; however, this shift does not remove the fundamental monotonicity limitation.\n\nC. Picking $c = -\\min_{x \\in K} f(x)$ guarantees that $f + c$ is convex on $K$, so a nonnegatively weighted ReLU network can approximate $f + c$ and the final negative bias recovers $f$ exactly.\n\nD. If we precenter the inputs by subtracting a sufficiently large constant vector so that all preactivations are negative, every hidden ReLU outputs zero, and the network reduces to a tunable negative bias that can match any negative-valued $f$ on $K$ by selecting biases appropriately for each input.\n\nChoose all that apply.", "solution": "The problem defines a class of feedforward ReLU networks where all weight matrices have nonnegative entries. We analyze the properties of functions that can be computed by such networks.\n\nA layer's transformation is a composition of an affine map $x \\mapsto Wx+b$ and a coordinatewise ReLU activation $\\sigma$. The problem states that affine maps with nonnegative weights are coordinatewise nondecreasing, the ReLU activation is nondecreasing, and the composition of nondecreasing functions is nondecreasing. By applying these properties inductively through the layers, any function $N(x)$ computed by such a network must be coordinatewise nondecreasing. This means that if we have two input vectors $x_1$ and $x_2$ in the domain $K$ such that $x_1 \\le x_2$ (elementwise), then it must be that $N(x_1) \\le N(x_2)$.\n\nThis is a very strong constraint. If a sequence of functions $\\{N_k\\}$ (from our restricted class) converges uniformly to a function $f$, then the limit function $f$ must also be coordinatewise nondecreasing. Therefore, this class of networks can only approximate functions that are themselves coordinatewise nondecreasing.\n\nWith this understanding, we evaluate the options:\n\n**A.** This is incorrect. The class of networks is not dense in $C(K)$ because it cannot approximate any function that is not coordinatewise nondecreasing (e.g., $f(x)=-x$ on $[0,1]$). The flexibility of biases allows for vertical shifting but does not alter the monotonicity.\n\n**B.** This is correct. It correctly identifies that any computable function must be coordinatewise nondecreasing, and therefore functions lacking this property cannot be approximated. It also correctly notes that adding a constant bias (shifting the function vertically) does not change its monotonicity, so the proposed workaround of approximating $f+c$ is only possible if $f+c$ (and thus $f$) is already monotonic.\n\n**C.** This is incorrect. Shifting a function vertically does not guarantee convexity. For example, adding a constant to $\\sin(x)$ does not make it convex. The premise is false.\n\n**D.** This is incorrect. If inputs are shifted such that all first-layer preactivations are negative, the first hidden layer outputs a zero vector. Consequently, the output of the entire network becomes a constant, independent of the input $x$. A constant function cannot uniformly approximate an arbitrary non-constant function.\n\nTherefore, only statement B is correct.", "answer": "$$\\boxed{B}$$", "id": "3194145"}]}