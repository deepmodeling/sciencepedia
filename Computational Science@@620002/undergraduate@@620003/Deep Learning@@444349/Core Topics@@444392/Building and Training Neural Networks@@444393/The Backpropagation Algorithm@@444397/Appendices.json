{"hands_on_practices": [{"introduction": "To truly grasp backpropagation, it is essential to connect it to the fundamental principles of calculus and optimization. This first practice strips away the complexity of a large network and focuses on a single linear neuron, allowing for a complete analysis by hand. By deriving the gradient and Hessian, you will see how backpropagation is used to find critical points of the loss function and determine their nature, providing a concrete link between the algorithm and the underlying theory of optimization. [@problem_id:3099996]", "problem": "You are given a single-neuron model with a linear activation, defined by the parametric function $f(x; \\theta) = W x + b$, where $\\theta = (W, b)$, $W \\in \\mathbb{R}$, and $b \\in \\mathbb{R}$. The training set consists of three input-output pairs $(x_i, y_i)$ for $i = 1, 2, 3$, specifically $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The empirical risk is the half-sum of squared errors, defined by\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\nStarting from the fundamental definition of the chain rule of calculus and the definition of the gradient and Hessian (the matrix of second-order partial derivatives), do the following:\n1. Choose parameters $W$ and $b$ that exactly fit the three data points, meaning that $f(x_i; \\theta) = y_i$ for every $i \\in \\{1, 2, 3\\}$.\n2. Using backpropagation (that is, the chain rule applied to the computational graph of the model), derive the gradient $\\nabla_{\\theta} J(\\theta)$ and evaluate it at the exact-fit parameters you chose in part $1$.\n3. Derive the Hessian $H(\\theta)$ of $J(\\theta)$ with respect to $\\theta$ and evaluate it at the exact-fit parameters. Compute the minimum eigenvalue $\\lambda_{\\min}(H)$.\n4. Based on the sign of $\\lambda_{\\min}(H)$, briefly state whether the exact-fit point is a local minimum or a saddle point for $J(\\theta)$.\n\nProvide your final answer as the exact value of $\\lambda_{\\min}(H)$ at the solution. No rounding is required.", "solution": "The problem has been validated and is scientifically sound, well-posed, objective, and contains sufficient information for a unique solution.\n\nThe task is to analyze the empirical risk function $J(\\theta)$ for a single linear neuron model $f(x; \\theta) = W x + b$ with parameters $\\theta = (W, b)$. The risk is defined as the half-sum of squared errors over three data points: $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The risk function is:\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\nSubstituting the given data points:\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. Find the exact-fit parameters $\\theta^* = (W, b)$**\n\nFor an exact fit, the model must satisfy $f(x_i; \\theta) = y_i$ for all $i \\in \\{1, 2, 3\\}$. This yields a system of linear equations for $W$ and $b$:\n\\begin{enumerate}\n    \\item For $(x_1, y_1) = (0, 1)$: $W(0) + b = 1 \\implies b = 1$.\n    \\item For $(x_2, y_2) = (1, 3)$: $W(1) + b = 3 \\implies W + b = 3$.\n    \\item For $(x_3, y_3) = (2, 5)$: $W(2) + b = 5 \\implies 2W + b = 5$.\n\\end{enumerate}\nSubstituting $b = 1$ from the first equation into the second gives $W + 1 = 3$, which implies $W = 2$.\nWe must verify that these values satisfy the third equation: $2W + b = 2(2) + 1 = 4 + 1 = 5$, which is consistent with $y_3 = 5$.\nThus, the parameters for an exact fit are $W = 2$ and $b = 1$. Let us denote this point as $\\theta^* = (2, 1)$.\n\n**2. Derive and evaluate the gradient $\\nabla_{\\theta} J(\\theta)$ at $\\theta^*$**\n\nThe gradient of $J(\\theta)$ with respect to $\\theta = (W, b)$ is $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$.\nUsing the chain rule, as specified by the backpropagation methodology, we define the error for each point as $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$. The loss is $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$.\nThe partial derivatives are:\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\nAt the exact-fit point $\\theta^* = (2, 1)$, by definition, the error terms are zero: $e_i(\\theta^*) = Wx_i + b - y_i = 0$ for all $i$.\nTherefore, evaluating the gradient at $\\theta^*$:\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\nThe gradient at the exact-fit point is the zero vector: $\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This confirms that $\\theta^*$ is a critical point of the loss function $J(\\theta)$.\n\n**3. Derive the Hessian $H(\\theta)$ and compute its minimum eigenvalue**\n\nThe Hessian matrix $H(\\theta)$ contains the second-order partial derivatives of $J(\\theta)$:\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2} & \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W} & \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\nWe compute these by differentiating the first-order partial derivatives:\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\nNote that $\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$, as expected. The Hessian is constant and does not depend on $W$ or $b$. We evaluate the sums using the given inputs $x_1=0$, $x_2=1$, $x_3=2$:\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\nThe Hessian matrix is:\n$$\nH = \\begin{pmatrix} 5 & 3 \\\\ 3 & 3 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $H$ are the roots of the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 5-\\lambda & 3 \\\\ 3 & 3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\nUsing the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\nSimplifying $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$:\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\nThe two eigenvalues are $\\lambda_1 = 4 + \\sqrt{10}$ and $\\lambda_2 = 4 - \\sqrt{10}$. The minimum eigenvalue is $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$.\n\n**4. Classify the critical point $\\theta^*$**\n\nTo classify the critical point $\\theta^*$, we examine the signs of the eigenvalues of the Hessian matrix evaluated at that point. Since $H$ is constant, we use the eigenvalues just computed.\nWe know that $3 = \\sqrt{9} < \\sqrt{10} < \\sqrt{16} = 4$.\nTherefore, the minimum eigenvalue $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ is positive, as $4 > \\sqrt{10}$.\nThe maximum eigenvalue $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ is also clearly positive.\nSince both eigenvalues of the Hessian are positive, the Hessian matrix is positive definite. According to the second partial derivative test, a critical point at which the Hessian is positive definite is a local minimum. For this quadratic loss function, it is the unique global minimum. The exact-fit point is a local minimum.\nThe final answer is the value of the minimum eigenvalue.", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}, {"introduction": "While backpropagation provides an exact analytical gradient, implementing it for complex models can be error-prone. A crucial engineering practice is to verify your implementation using a numerical approximation, a technique known as gradient checking. This exercise guides you through comparing the gradient from backpropagation with a finite-difference approximation and confirming that the error between them shrinks as expected, thereby validating the correctness of your code. [@problem_id:3100954]", "problem": "Construct a verification of the backpropagation algorithm by comparing an analytically derived gradient of a two-layer neural network to a finite-difference approximation. The objective is to numerically confirm that the discrepancy between the two gradients behaves as an order-$\\epsilon$ truncation error when using a forward finite difference, that is, the error is $\\mathcal{O}(\\epsilon)$.\n\nUse the following purely mathematical setup.\n\n- Network architecture and data:\n  - Inputs have dimension $d = 3$, the hidden layer has $h = 3$ units with hyperbolic tangent activation, and the output layer has dimension $o = 1$ with a linear output.\n  - Given a mini-batch of size $n = 4$, the input matrix $X \\in \\mathbb{R}^{4 \\times 3}$ and target vector $y \\in \\mathbb{R}^{4 \\times 1}$ are:\n    $$\n    X =\n    \\begin{bmatrix}\n    0.2 & -0.1 & 0.4 \\\\\n    -0.5 & 0.3 & 0.1 \\\\\n    0.0 & -0.2 & 0.2 \\\\\n    0.1 & 0.4 & -0.3\n    \\end{bmatrix}, \\quad\n    y =\n    \\begin{bmatrix}\n    0.5 \\\\ -0.1 \\\\ 0.2 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - Parameters for Test Case A are fixed as:\n    $$\n    W_1 =\n    \\begin{bmatrix}\n    0.3 & -0.1 & 0.2 \\\\\n    -0.4 & 0.5 & 0.1 \\\\\n    0.2 & 0.3 & -0.2\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    0.05 \\\\ -0.02 \\\\ 0.01\n    \\end{bmatrix}, \\quad\n    W_2 =\n    \\begin{bmatrix}\n    0.6 & -0.7 & 0.2\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0.03\n    \\end{bmatrix}.\n    $$\n    For Test Case B, use the same shapes but scale every entry of $W_1, b_1, W_2, b_2$ by a factor of $0.1$.\n\n- Forward model and loss:\n  - For each row $x_i^\\top$ of $X$, define the hidden pre-activation $z_{1,i}^\\top = x_i^\\top W_1^\\top + b_1^\\top$, hidden activation $h_i^\\top = \\tanh(z_{1,i}^\\top)$, output pre-activation $z_{2,i} = h_i^\\top W_2^\\top + b_2$, and prediction $\\hat{y}_i = z_{2,i}$.\n  - Define the mean squared error loss\n    $$\n    L(W_1,b_1,W_2,b_2) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2.\n    $$\n\n- Analytical gradient via backpropagation:\n  - Using multivariable calculus, the chain rule, and the derivative identity $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$, derive the gradient of $L$ with respect to all parameters and implement it.\n  - Flatten the parameter set into a single vector $\\theta \\in \\mathbb{R}^{p}$ with $p = 16$ using the following order and memory layout:\n    1. Flatten $W_1 \\in \\mathbb{R}^{3 \\times 3}$ in row-major order.\n    2. Append $b_1 \\in \\mathbb{R}^{3}$.\n    3. Flatten $W_2 \\in \\mathbb{R}^{1 \\times 3}$ in row-major order.\n    4. Append $b_2 \\in \\mathbb{R}^{1}$.\n\n- Finite-difference approximation:\n  - For a given $\\epsilon > 0$ and the standard basis vector $e_k$ in $\\mathbb{R}^p$, approximate the $k$-th component of $\\nabla_{\\theta} L$ by the forward difference\n    $$\n    \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}.\n    $$\n  - Use the list of step sizes\n    $$\n    \\mathcal{E} = \\left[ 10^{-1}, \\; 3 \\cdot 10^{-2}, \\; 10^{-2}, \\; 3 \\cdot 10^{-3}, \\; 10^{-3}, \\; 3 \\cdot 10^{-4}, \\; 10^{-4} \\right].\n    $$\n\n- Error metric and order verification:\n  - For each $\\epsilon \\in \\mathcal{E}$, compute the Euclidean norm of the difference between the analytical gradient and the finite-difference gradient,\n    $$\n    \\mathrm{err}(\\epsilon) = \\left\\| \\nabla_{\\theta} L - g_{\\mathrm{FD}}(\\epsilon) \\right\\|_2.\n    $$\n  - For consecutive $\\epsilon_i > \\epsilon_{i+1}$, compute the empirical order\n    $$\n    s_i = \\frac{\\log\\left( \\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}) \\right)}{\\log\\left( \\epsilon_i / \\epsilon_{i+1} \\right)}.\n    $$\n  - Define two boolean checks per test case:\n    1. Let $s_{\\mathrm{med}}$ be the median of $\\{ s_i \\}$. Define $\\mathrm{pass\\_order}$ to be true if $0.8 \\le s_{\\mathrm{med}} \\le 1.2$.\n    2. Define $\\mathrm{pass\\_mono}$ to be true if $\\mathrm{err}(\\epsilon)$ is strictly decreasing over the first $5$ values of $\\mathcal{E}$, that is, for $\\epsilon \\in \\{ 10^{-1}, 3\\cdot 10^{-2}, 10^{-2}, 3\\cdot 10^{-3}, 10^{-3} \\}$.\n\n- Test suite:\n  - Two test cases are specified by the parameter sets:\n    - Test Case A: the parameters exactly as given above.\n    - Test Case B: the same parameter shapes with every entry scaled by $0.1$ relative to Test Case A.\n  - In both cases, use the same $X$, $y$, and the same $\\mathcal{E}$.\n\n- Required program behavior and final output format:\n  - Your program must implement the forward model, derive and compute the analytical gradient via backpropagation from first principles, compute the finite-difference gradients for each $\\epsilon \\in \\mathcal{E}$, and evaluate the error norms and empirical orders.\n  - For each test case, produce a list with four entries: \n    $[ \\mathrm{err}(\\min \\mathcal{E}), \\; s_{\\mathrm{med}}, \\; \\mathrm{pass\\_order}, \\; \\mathrm{pass\\_mono} ]$.\n  - The final program output must be a single line containing a list with the two per-case lists, formatted exactly as a comma-separated list enclosed in square brackets, for example:\n    $$\n    \\left[ [a_1, a_2, a_3, a_4], [b_1, b_2, b_3, b_4] \\right]\n    $$\n    where each $a_j$ and $b_j$ is a boolean or a floating-point number. No other text must be printed.\n  - There are no physical units involved in this problem.\n\nYour implementation must be self-contained and must not read input. It must use the specified numerical values exactly as provided above.", "solution": "The objective is to numerically verify the correctness of the backpropagation algorithm for a two-layer neural network. This is achieved by comparing the analytically computed gradient with a numerical approximation obtained via the finite-difference method. The primary verification criterion is to confirm that the error between the analytical and numerical gradients decreases linearly with the finite-difference step size $\\epsilon$, characteristic of a forward-difference scheme's first-order truncation error, $\\mathcal{O}(\\epsilon)$.\n\n### Mathematical Model and Loss Function\n\nThe neural network architecture consists of an input layer, one hidden layer, and an output layer.\n- Input $X \\in \\mathbb{R}^{n \\times d}$ ($n=4, d=3$)\n- Hidden layer with $h=3$ units and $\\tanh$ activation.\n- Output layer with $o=1$ unit and linear activation.\n- Parameters: $W_1 \\in \\mathbb{R}^{h \\times d}$, $b_1 \\in \\mathbb{R}^{h \\times 1}$, $W_2 \\in \\mathbb{R}^{o \\times h}$, $b_2 \\in \\mathbb{R}^{o \\times 1}$.\n\nThe forward propagation of a mini-batch $X$ is defined by the following matrix operations:\n1.  **Hidden Layer Pre-activation**: The linear transformation for the hidden layer is given by $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$ is a vector of ones and its product with $b_1^\\top$ is handled via broadcasting. The resulting matrix $Z_1 \\in \\mathbb{R}^{n \\times h}$.\n2.  **Hidden Layer Activation**: The hyperbolic tangent activation function is applied element-wise: $H = \\tanh(Z_1)$, where $H \\in \\mathbb{R}^{n \\times h}$.\n3.  **Output Layer Pre-activation**: A second linear transformation produces the output pre-activations: $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$. The resulting matrix $Z_2 \\in \\mathbb{R}^{n \\times o}$.\n4.  **Prediction**: The network output is linear, so the prediction $\\hat{Y}$ is equal to the pre-activation: $\\hat{Y} = Z_2 \\in \\mathbb{R}^{n \\times o}$.\n\nThe performance of the network is quantified by the mean squared error (MSE) loss function, averaged over the mini-batch:\n$$\nL = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\|\\hat{Y} - Y\\|_F^2\n$$\nwhere $Y \\in \\mathbb{R}^{n \\times o}$ is the matrix of true target values.\n\n### Analytical Gradient via Backpropagation\n\nThe core of this task is to derive the gradient of the loss $L$ with respect to each parameter ($W_1, b_1, W_2, b_2$) using the multivariable chain rule. This process is known as backpropagation. We denote the gradient of the loss with respect to a matrix $M$ as $\\delta_M = \\frac{\\partial L}{\\partial M}$.\n\n1.  **Gradient at the Output**: The gradient of the loss with respect to the network's predictions $\\hat{Y}$ is:\n    $$\n    \\delta_{\\hat{Y}} = \\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{1}{n} (\\hat{Y} - Y)\n    $$\n    Since $\\hat{Y} =Z_2$, we have $\\delta_{Z_2} = \\delta_{\\hat{Y}}$.\n\n2.  **Gradients for the Output Layer ($W_2, b_2$)**:\n    Using the chain rule on $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$:\n    $$\n    \\delta_{W_2} = \\frac{\\partial L}{\\partial W_2} = \\delta_{Z_2}^\\top H\n    $$\n    $$\n    \\delta_{b_2} = \\frac{\\partial L}{\\partial b_2} = (\\mathrm{sum}(\\delta_{Z_2}, \\text{axis}=0))^\\top = \\delta_{Z_2}^\\top \\mathbf{1}\n    $$\n\n3.  **Propagating the Gradient to the Hidden Layer**:\n    The gradient is propagated back to the hidden layer's activations $H$:\n    $$\n    \\delta_H = \\frac{\\partial L}{\\partial H} = \\delta_{Z_2} W_2\n    $$\n    Next, the gradient is propagated through the $\\tanh$ activation function, using $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$:\n    $$\n    \\delta_{Z_1} = \\frac{\\partial L}{\\partial Z_1} = \\delta_H \\odot (1 - H^2)\n    $$\n    where $\\odot$ denotes the element-wise (Hadamard) product.\n\n4.  **Gradients for the Hidden Layer ($W_1, b_1$)**:\n    Finally, using the chain rule on $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$:\n    $$\n    \\delta_{W_1} = \\frac{\\partial L}{\\partial W_1} = \\delta_{Z_1}^\\top X\n    $$\n    $$\n    \\delta_{b_1} = \\frac{\\partial L}{\\partial b_1} = (\\mathrm{sum}(\\delta_{Z_1}, \\text{axis}=0))^\\top = \\delta_{Z_1}^\\top \\mathbf{1}\n    $$\n\nThese matrix-form equations provide a complete algorithm for computing the analytical gradient.\n\n### Numerical Verification\n\nTo verify the analytical gradient, we compare it against a numerical approximation.\n\n- **Parameter Vectorization**: All network parameters ($W_1, b_1, W_2, b_2$) are flattened and concatenated into a single vector $\\theta \\in \\mathbb{R}^{p}$, with $p=16$. The specified order is $W_1$ (row-major), $b_1$, $W_2$ (row-major), and $b_2$.\n\n- **Finite-Difference Approximation**: The gradient is approximated using the first-order forward-difference formula. The $k$-th component of the gradient vector is estimated as:\n  $$\n  g_{\\mathrm{FD},k}(\\epsilon) = \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}\n  $$\n  where $e_k$ is the $k$-th standard basis vector and $\\epsilon$ is a small step size.\n\n- **Error Analysis and Order Verification**:\n  The discrepancy between the analytical gradient $\\nabla_\\theta L$ and the finite-difference approximation $g_{\\mathrm{FD}}(\\epsilon)$ is measured by the Euclidean norm of their difference:\n  $$\n  \\mathrm{err}(\\epsilon) = \\| \\nabla_\\theta L - g_{\\mathrm{FD}}(\\epsilon) \\|_2\n  $$\n  For a first-order method, this error is expected to be proportional to $\\epsilon$, i.e., $\\mathrm{err}(\\epsilon) \\approx C\\epsilon$. This implies that the ratio of errors for two step sizes $\\epsilon_i$ and $\\epsilon_{i+1}$ should be approximately equal to the ratio of the step sizes themselves. To quantify this relationship, we compute the empirical order of convergence $s_i$:\n  $$\n  s_i = \\frac{\\log(\\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}))}{\\log(\\epsilon_i / \\epsilon_{i+1})}\n  $$\n  A value of $s_i \\approx 1$ confirms the expected first-order convergence, thus validating the analytical gradient implementation. We use the median of the computed $s_i$ values for robustness. The verification is considered successful if this median order $s_{\\mathrm{med}}$ is within the range $[0.8, 1.2]$ and if the error is monotonically decreasing for the initial, larger values of $\\epsilon$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the backpropagation verification problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n    X = np.array([\n        [0.2, -0.1, 0.4],\n        [-0.5, 0.3, 0.1],\n        [0.0, -0.2, 0.2],\n        [0.1, 0.4, -0.3]\n    ])\n    y = np.array([\n        [0.5],\n        [-0.1],\n        [0.2],\n        [0.0]\n    ])\n    n = X.shape[0]  # Mini-batch size, n=4\n\n    # Parameters for Test Case A\n    W1_a = np.array([\n        [0.3, -0.1, 0.2],\n        [-0.4, 0.5, 0.1],\n        [0.2, 0.3, -0.2]\n    ])\n    b1_a = np.array([[0.05], [-0.02], [0.01]])\n    W2_a = np.array([[0.6, -0.7, 0.2]])\n    b2_a = np.array([[0.03]])\n    \n    test_cases_params = [\n        (W1_a, b1_a, W2_a, b2_a),\n        (W1_a * 0.1, b1_a * 0.1, W2_a * 0.1, b2_a * 0.1) # Test Case B\n    ]\n\n    epsilons = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n    \n    # --- Helper functions for parameter manipulation ---\n    def params_to_vec(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            b2.flatten()\n        ])\n\n    def vec_to_params(theta):\n        W1 = theta[0:9].reshape(3, 3)\n        b1 = theta[9:12].reshape(3, 1)\n        W2 = theta[12:15].reshape(1, 3)\n        b2 = theta[15:16].reshape(1, 1)\n        return W1, b1, W2, b2\n        \n    # --- Core functions for forward/backward pass ---\n    def forward_pass(W1, b1, W2, b2):\n        Z1 = X @ W1.T + b1.T\n        H = np.tanh(Z1)\n        Z2 = H @ W2.T + b2\n        Y_hat = Z2\n        loss = (1 / (2 * n)) * np.sum((Y_hat - y)**2)\n        return loss, Z1, H, Y_hat\n\n    def analytical_gradient(W1, b1, W2, b2):\n        _loss, _Z1, H, Y_hat = forward_pass(W1, b1, W2, b2)\n        \n        # Backward pass\n        dZ2 = (1 / n) * (Y_hat - y)\n        dW2 = dZ2.T @ H\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dH = dZ2 @ W2\n        dZ1 = dH * (1 - H**2)\n        dW1 = dZ1.T @ X\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n        \n        return params_to_vec(dW1, db1, dW2, db2)\n\n    def run_verification(params):\n        W1, b1, W2, b2 = params\n        \n        # 1. Compute analytical gradient\n        grad_analytic = analytical_gradient(W1, b1, W2, b2)\n        \n        # 2. Compute finite-difference gradient for each epsilon\n        theta_base = params_to_vec(W1, b1, W2, b2)\n        loss_base, _, _, _ = forward_pass(W1, b1, W2, b2)\n        \n        errors = []\n        for eps in epsilons:\n            grad_fd = np.zeros_like(theta_base)\n            for k in range(len(theta_base)):\n                theta_p = np.copy(theta_base)\n                theta_p[k] += eps\n                \n                W1_p, b1_p, W2_p, b2_p = vec_to_params(theta_p)\n                loss_p, _, _, _ = forward_pass(W1_p, b1_p, W2_p, b2_p)\n                \n                grad_fd[k] = (loss_p - loss_base) / eps\n            \n            error = np.linalg.norm(grad_analytic - grad_fd)\n            errors.append(error)\n\n        # 3. Compute empirical orders\n        orders = []\n        for i in range(len(epsilons) - 1):\n            s_i = np.log(errors[i] / errors[i+1]) / np.log(epsilons[i] / epsilons[i+1])\n            orders.append(s_i)\n        \n        s_med = np.median(orders)\n        \n        # 4. Perform boolean checks\n        pass_order = 0.8 <= s_med <= 1.2\n        pass_mono = all(errors[i] > errors[i+1] for i in range(4))\n\n        # 5. Collect results\n        err_min_eps = errors[-1]\n        \n        return [err_min_eps, s_med, bool(pass_order), bool(pass_mono)]\n\n    # --- Main execution loop ---\n    final_results = []\n    for case_params in test_cases_params:\n        result = run_verification(case_params)\n        final_results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3100954"}, {"introduction": "A correct analytical implementation of backpropagation can still fail in practice due to the limitations of floating-point arithmetic. This problem explores the critical concept of numerical stability, focusing on the Log-Sum-Exp function, which is a core component of the widely used softmax and cross-entropy loss. You will implement a stabilized version of its backward pass and see firsthand why a naive implementation fails with large-magnitude inputs, a crucial lesson for building robust deep learning models. [@problem_id:3181541]", "problem": "Consider the function $f:\\mathbb{R}^n\\to\\mathbb{R}$ defined by $f(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$, where $\\mathbf{z}=(z_1,z_2,\\dots,z_n)\\in\\mathbb{R}^n$. This function is widely used in deep learning as a building block for models and loss functions. Your task is to implement a numerically stable backward pass for $f(\\mathbf{z})$ that computes the gradient $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ and to explore numerical stability for large magnitudes of $\\mathbf{z}$. Start from fundamental calculus principles (the chain rule, the derivative of the exponential function, and the derivative of the natural logarithm), and do not use any pre-derived \"shortcut\" formulas. Derive the mathematical expression for $\\frac{\\partial f}{\\partial z_k}$ in terms of $\\mathbf{z}$ using only these principles. Then, design a numerically stable algorithm to compute $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ that avoids overflow and underflow when $\\lvert z_i\\rvert$ is large. In particular, implement the Log-Sum-Exp (LSE) trick: compute $m=\\max_i z_i$, and use the identity $\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)=m+\\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$ to stabilize both the forward and backward computations.\n\nYour program must:\n- Implement a numerically stable forward function $F(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$ using the identity above, with $m=\\max_i z_i$.\n- Implement the backward function that returns $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ using your derived expression, computed in a numerically stable way via shifting by $m$.\n- Implement a naive (non-stable) backward function that directly uses $e^{z_i}$ without shifting by $m$.\n- Implement a central finite-difference approximation to $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ by perturbing one coordinate at a time with a step size $\\varepsilon$, using a numerically stable forward computation. Use $\\varepsilon=10^{-6}$.\n\nFor each test case vector $\\mathbf{z}$ in the test suite below, compute:\n$1.$ The maximum absolute difference $d_{\\text{stable}}=\\max_k \\left\\lvert \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)_k - \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k \\right\\rvert$ between your numerically stable analytical gradient and the central finite-difference approximation.\n$2.$ A boolean $b_{\\text{naive}}$ that is $\\text{True}$ if the naive gradient contains any non-finite values (that is, $\\infty$ or $\\mathrm{NaN}$), and $\\text{False}$ otherwise.\n\nTest suite (each $\\mathbf{z}$ is a row vector):\n- Case $1$: $\\mathbf{z}=[1000,1000,1000]$.\n- Case $2$: $\\mathbf{z}=[1000,-1000,0]$.\n- Case $3$: $\\mathbf{z}=[-1000,-1000,-999]$.\n- Case $4$: $\\mathbf{z}=[800,-800,800,-800]$.\n\nDesign for coverage rationale:\n- Case $1$ tests symmetry and large equal values (the gradient components are expected to be equal).\n- Case $2$ tests extreme disparity with both very large positive and very large negative entries.\n- Case $3$ tests near-cancellation and underflow risks when all entries are very negative but close to each other.\n- Case $4$ tests multiple large-magnitude entries with repetition.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered as $[d_{\\text{stable}}^{(1)},b_{\\text{naive}}^{(1)},d_{\\text{stable}}^{(2)},b_{\\text{naive}}^{(2)},d_{\\text{stable}}^{(3)},b_{\\text{naive}}^{(3)},d_{\\text{stable}}^{(4)},b_{\\text{naive}}^{(4)}]$, where the superscript indicates the case number. All floating-point quantities must be output in default floating-point notation, and booleans must be output as $\\text{True}$ or $\\text{False}$. No physical units, angle units, or percentages are involved in this problem.", "solution": "The problem is valid. It is scientifically grounded in the principles of calculus and numerical analysis, well-posed with a clear objective, and provides all necessary information and constraints for a unique solution.\n\nWe are asked to derive and implement a numerically stable backward pass for the function $f:\\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$, where $\\mathbf{z} = (z_1, z_2, \\dots, z_n)$.\n\n### 1. Mathematical Derivation of the Gradient\n\nThe primary task is to find the gradient of $f(\\mathbf{z})$, which is the vector of partial derivatives $\\nabla_{\\mathbf{z}} f(\\mathbf{z}) = \\left(\\frac{\\partial f}{\\partial z_1}, \\frac{\\partial f}{\\partial z_2}, \\dots, \\frac{\\partial f}{\\partial z_n}\\right)$. We will derive the expression for a single component, $\\frac{\\partial f}{\\partial z_k}$, using fundamental calculus principles.\n\nLet's decompose the function $f(\\mathbf{z})$ using the chain rule. Define an intermediate variable $S(\\mathbf{z})$ and a function $g(S)$:\n$$ S(\\mathbf{z}) = \\sum_{i=1}^{n} e^{z_i} $$\n$$ g(S) = \\log(S) $$\nThus, $f(\\mathbf{z}) = g(S(\\mathbf{z}))$.\n\nAccording to the multivariate chain rule, the partial derivative of $f$ with respect to $z_k$ is:\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{d g}{d S} \\cdot \\frac{\\partial S}{\\partial z_k} $$\n\nFirst, we compute the derivative of $g(S)$ with respect to $S$:\n$$ \\frac{d g}{d S} = \\frac{d}{dS} \\log(S) = \\frac{1}{S} $$\n\nNext, we compute the partial derivative of $S(\\mathbf{z})$ with respect to $z_k$:\n$$ \\frac{\\partial S}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left(\\sum_{i=1}^{n} e^{z_i}\\right) $$\nDue to the linearity of the differentiation operator, we can move the derivative inside the sum:\n$$ \\frac{\\partial S}{\\partial z_k} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial z_k} (e^{z_i}) $$\nThe derivative $\\frac{\\partial}{\\partial z_k} (e^{z_i})$ depends on whether $i=k$.\n- If $i=k$, then $\\frac{\\partial}{\\partial z_k} (e^{z_k}) = e^{z_k}$.\n- If $i \\neq k$, then $z_i$ is treated as a constant with respect to $z_k$, so $\\frac{\\partial}{\\partial z_k} (e^{z_i}) = 0$.\n\nTherefore, the sum collapses to a single term where $i=k$:\n$$ \\frac{\\partial S}{\\partial z_k} = e^{z_k} $$\n\nSubstituting these results back into the chain rule equation, we get:\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{1}{S} \\cdot e^{z_k} = \\frac{1}{\\sum_{i=1}^{n} e^{z_i}} \\cdot e^{z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}} $$\nThis expression gives the $k$-th component of the gradient $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$. The gradient vector is thus the result of applying the softmax function to the input vector $\\mathbf{z}$.\n\n### 2. Numerical Stability and the Log-Sum-Exp Trick\n\nThe derived expression $\\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$ is numerically unstable.\n- **Overflow**: If any $z_i$ is a large positive number (e.g., $1000$), $e^{z_i}$ will exceed the maximum value representable by standard floating-point types, resulting in infinity (`inf`). The subsequent division `inf/inf` results in `NaN` (Not a Number).\n- **Underflow**: If all $z_i$ are large negative numbers (e.g., $-1000$), each $e^{z_i}$ will underflow to $0$. The sum in the denominator becomes $0$, leading to division by zero, which also results in `inf` or `NaN`.\n\nTo address this, we use the \"Log-Sum-Exp\" (LSE) trick. Let $m = \\max_{i} z_i$. We can multiply the numerator and denominator of the gradient expression by the non-zero constant $e^{-m}$:\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k} \\cdot e^{-m}}{\\left(\\sum_{i=1}^{n} e^{z_i}\\right) \\cdot e^{-m}} = \\frac{e^{z_k - m}}{\\sum_{i=1}^{n} e^{z_i - m}} $$\nThis stabilized formula avoids overflow because the argument of each exponential, $z_i - m$, is always less than or equal to $0$. Consequently, each term $e^{z_i - m}$ is in the range $(0, 1]$. The maximum value is $1$, which occurs when $z_i = m$. The stable formula also avoids underflow-induced division by zero because at least one term in the denominator sum is $e^0 = 1$, ensuring the sum is always at least $1$.\n\nThe same trick is applied to the forward pass computation of $f(\\mathbf{z})$ itself:\n$$ f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right) = \\log\\left(e^m \\sum_{i=1}^{n} e^{z_i-m}\\right) = \\log(e^m) + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) = m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) $$\nThis form is used for the numerically stable forward computation.\n\n### 3. Algorithmic Implementation\n\nThe problem requires implementing four functions and comparing their results.\n\n1.  **Numerically Stable Forward Function, $F(\\mathbf{z})$**: This function first computes $m = \\max_i z_i$ and then returns the value $m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$.\n\n2.  **Numerically Stable Backward Function, $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$**: This function computes the gradient using the stabilized softmax formula. It first finds $m = \\max_i z_i$. Then it computes the vector of numerators $[e^{z_1-m}, e^{z_2-m}, \\dots, e^{z_n-m}]$ and the denominator sum $\\sum_{i=1}^{n} e^{z_i-m}$. The final gradient vector is obtained by dividing each numerator by the sum.\n\n3.  **Naive Backward Function**: This function directly implements the original formula $\\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$ for each component $k$. It is expected to produce non-finite values (`inf` or `NaN`) for the given test cases due to overflow or underflow.\n\n4.  **Central Finite-Difference Approximation, $(\\nabla_{\\mathbf{z}} f(\\mathbf{z}))^{\\text{FD}}$**: This serves as a numerical benchmark to verify the correctness of the analytical gradient implementation. For each component $k$, it approximates the partial derivative using the central difference formula with a small step size $\\varepsilon = 10^{-6}$:\n    $$ \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k = \\frac{F(\\mathbf{z} + \\varepsilon \\mathbf{e}_k) - F(\\mathbf{z} - \\varepsilon \\mathbf{e}_k)}{2\\varepsilon} $$\n    where $\\mathbf{e}_k$ is the standard basis vector with a $1$ at index $k$ and $0$s elsewhere. The numerically stable forward function $F(\\mathbf{z})$ must be used for this calculation to obtain meaningful results for large-magnitude inputs.\n\nThe program will execute these functions on the provided test vectors and compute the maximum absolute difference $d_{\\text{stable}}$ between the stable analytical gradient and the finite-difference approximation, as well as a boolean flag $b_{\\text{naive}}$ indicating if the naive gradient computation resulted in any non-finite numbers.", "answer": "```python\nimport numpy as np\n\ndef stable_forward_lse(z: np.ndarray) -> float:\n    \"\"\"\n    Computes the Log-Sum-Exp function in a numerically stable way.\n    f(z) = log(sum(exp(z_i)))\n    \"\"\"\n    m = np.max(z)\n    # The Log-Sum-Exp identity: m + log(sum(exp(z_i - m)))\n    return m + np.log(np.sum(np.exp(z - m)))\n\ndef backward_stable(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function in a stable manner.\n    This is equivalent to the softmax function.\n    grad_k = exp(z_k - m) / sum(exp(z_i - m))\n    \"\"\"\n    m = np.max(z)\n    shifted_exp_z = np.exp(z - m)\n    return shifted_exp_z / np.sum(shifted_exp_z)\n\ndef backward_naive(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function naively.\n    This is prone to overflow/underflow.\n    grad_k = exp(z_k) / sum(exp(z_i))\n    \"\"\"\n    # This is expected to fail for large inputs\n    exp_z = np.exp(z)\n    sum_exp_z = np.sum(exp_z)\n    return exp_z / sum_exp_z\n\ndef gradient_finite_difference(z: np.ndarray, epsilon: float = 1e-6) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function using central finite differences.\n    \"\"\"\n    n = z.shape[0]\n    grad_fd = np.zeros(n, dtype=np.float64)\n    \n    # Use a copy of z to avoid modifying the original\n    z_temp = z.astype(np.float64)\n\n    for i in range(n):\n        # Store original value\n        original_zi = z_temp[i]\n        \n        # Calculate f(z + epsilon * e_i)\n        z_temp[i] = original_zi + epsilon\n        f_plus = stable_forward_lse(z_temp)\n        \n        # Calculate f(z - epsilon * e_i)\n        z_temp[i] = original_zi - epsilon\n        f_minus = stable_forward_lse(z_temp)\n        \n        # Restore original value\n        z_temp[i] = original_zi\n        \n        # Central difference formula\n        grad_fd[i] = (f_plus - f_minus) / (2 * epsilon)\n        \n    return grad_fd\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        np.array([1000.0, 1000.0, 1000.0]),\n        np.array([1000.0, -1000.0, 0.0]),\n        np.array([-1000.0, -1000.0, -999.0]),\n        np.array([800.0, -800.0, 800.0, -800.0])\n    ]\n    \n    epsilon = 1e-6\n    results = []\n\n    for z in test_cases:\n        # 1. Compute stable analytical gradient and finite difference approximation\n        grad_stable = backward_stable(z)\n        grad_fd = gradient_finite_difference(z, epsilon)\n        \n        # Calculate the maximum absolute difference\n        d_stable = np.max(np.abs(grad_stable - grad_fd))\n        \n        # 2. Compute naive gradient and check for non-finite values\n        # Suppress RuntimeWarning for overflow in exp and invalid value in divide\n        with np.errstate(over='ignore', invalid='ignore'):\n             grad_naive = backward_naive(z)\n        \n        b_naive = np.any(~np.isfinite(grad_naive))\n        \n        results.append(d_stable)\n        results.append(b_naive)\n        \n    output_parts = []\n    for item in results:\n        if isinstance(item, (bool, np.bool_)):\n            output_parts.append(str(item))\n        else:\n            output_parts.append(f\"{item}\")\n            \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3181541"}]}