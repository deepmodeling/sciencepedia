{"hands_on_practices": [{"introduction": "The mathematical definition of cross-entropy loss appears simple, but translating it directly into code can lead to significant numerical errors like overflow or underflow, especially when dealing with the large range of values in neural network logits. This practice guides you through the derivation of a numerically stable version of the categorical cross-entropy loss using the \"log-sum-exp\" trick. Mastering this technique [@problem_id:3103417] is a fundamental step in building robust deep learning models that train reliably.", "problem": "You are given a classification setting with a finite number of classes modeled by a categorical distribution. For an input with unnormalized log-probabilities (called logits) $\\mathbf{z} \\in \\mathbb{R}^K$ and a one-hot target vector $\\mathbf{y} \\in \\{0,1\\}^K$ with the correct class index $c$ such that $y_c = 1$, the negative log-likelihood of the correct class under the categorical model defines the Categorical Cross-Entropy (CCE). The probability vector is obtained by normalizing exponentials of logits, which is the standard definition of the softmax transformation. From the core definitions of a categorical distribution, negative log-likelihood, properties of the exponential and logarithm, and the invariance of probability ratios under adding a constant to all logits, derive a numerically stable expression for the CCE that does not require forming probabilities via exponentials at the original scale. Your derivation should make explicit use of the identity that expresses a logarithm of a sum of exponentials in a way that avoids overflow or underflow by shifting with a data-dependent constant. Explain why this identity avoids arithmetic overflow when logits have large positive magnitude and avoids underflow when logits have large negative magnitude, using the well-tested fact that double-precision floating-point overflows occur for $\\exp(x)$ when $x \\gtrsim 709$ and underflows to zero when $x \\lesssim -745$.\n\nAdditionally, in the special case of $K=2$ classes, show from first principles that the CCE reduces to the Binary Cross-Entropy (BCE) between a binary label $y \\in \\{0,1\\}$ and a Bernoulli model with logit parameter $a \\in \\mathbb{R}$ equal to the log-odds induced by the two logits. Express the BCE in a numerically stable form using the SoftPlus function, which is defined solely in terms of elementary functions.\n\nYour program must:\n- Implement a naive CCE that computes the softmax probabilities directly by exponentiating logits and normalizing, then computes the loss for a single example. It should detect failure (overflow or underflow) by checking whether any intermediate or final value becomes non-finite.\n- Implement a numerically stable CCE using an appropriate shift and a stable computation of the logarithm of a sum of exponentials, without ever forming probabilities at the original scale.\n- Implement a numerically stable BCE with logits for the $K=2$ case using the SoftPlus function in a way that prevents overflow and underflow.\n- Apply these implementations to the test suite below and return the requested outputs.\n\nTest suite (each case is a single input with logits $\\mathbf{z}$ and a correct class index $c$):\n- Case $1$: $K=3$, $\\mathbf{z} = [1.0, 0.0, -1.0]$, $c=0$.\n- Case $2$: $K=3$, $\\mathbf{z} = [120.0, 0.0, -120.0]$, $c=1$.\n- Case $3$: $K=3$, $\\mathbf{z} = [1000.0, 999.0, 998.0]$, $c=0$.\n- Case $4$: $K=3$, $\\mathbf{z} = [-1000.0, -1001.0, -1002.0]$, $c=2$.\n- Case $5$ (equivalence check when $K=2$): $\\mathbf{z} = [5.0, -5.0]$, $c=0$. Let $a$ be the log-odds implied by $\\mathbf{z}$ for the $c=0$ class and $y=1$ the corresponding binary label. Compute the absolute difference between the stable CCE and the stable BCE for this example.\n\nFor Cases $1$ through $4$, for each case you must output two values in order:\n- A boolean indicating whether the naive softmax-based CCE computation failed (non-finite loss), where failure is defined as producing a value that is not finite.\n- The numerically stable CCE loss for that case, rounded to $6$ decimal places.\n\nFor Case $5$, you must output two values:\n- A boolean indicating whether the naive softmax-based CCE computation failed for this case.\n- The absolute difference between the stable CCE and the stable BCE for this example, rounded to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing all the results for Cases $1$ through $5$ concatenated in order as a comma-separated list enclosed in square brackets. The order is:\n  $[\\text{fail}_1, \\text{stable\\_loss}_1, \\text{fail}_2, \\text{stable\\_loss}_2, \\text{fail}_3, \\text{stable\\_loss}_3, \\text{fail}_4, \\text{stable\\_loss}_4, \\text{fail}_5, \\text{abs\\_diff}_{5}]$.\n- All floats must be printed rounded as specified above. No other text should be printed.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a rigorous solution. We proceed with the derivation and implementation.\n\n### Part 1: Derivation of a Numerically Stable Categorical Cross-Entropy (CCE)\n\nThe Categorical Cross-Entropy (CCE) loss is the negative log-likelihood of the true class under a model. For a single data point, given a vector of unnormalized log-probabilities (logits) $\\mathbf{z} \\in \\mathbb{R}^K$ and the correct class index $c$, we first compute the probability vector $\\mathbf{p}$ using the softmax function:\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} \\quad \\text{for } i=1, \\dots, K\n$$\nThe likelihood of observing the correct class $c$ is $p_c$. The CCE loss is the negative natural logarithm of this likelihood:\n$$\nL_{\\text{CCE}} = -\\ln(p_c) = -\\ln\\left(\\frac{\\exp(z_c)}{\\sum_{j=1}^K \\exp(z_j)}\\right)\n$$\nUsing the property of logarithms $\\ln(a/b) = \\ln(a) - \\ln(b)$, and that $\\ln(\\exp(x)) = x$, we can rewrite the loss as:\n$$\nL_{\\text{CCE}} = -(\\ln(\\exp(z_c)) - \\ln\\left(\\sum_{j=1}^K \\exp(z_j)\\right)) = -z_c + \\ln\\left(\\sum_{j=1}^K \\exp(z_j)\\right)\n$$\nThis expression, while mathematically correct, is numerically unstable. The term $\\sum_j \\exp(z_j)$ is prone to floating-point errors.\n- **Overflow**: If any logit $z_j$ is large and positive (e.g., $z_j \\gtrsim 709$ for double-precision), $\\exp(z_j)$ will evaluate to infinity, causing the entire expression to become non-finite.\n- **Underflow**: If all logits $z_j$ are large and negative (e.g., $z_j \\lesssim -745$), every $\\exp(z_j)$ will underflow to $0$. The sum will be $0$, and its natural logarithm will be $-\\infty$, again resulting in a non-finite loss.\n\nTo stabilize this computation, we use the \"log-sum-exp\" trick. This relies on the identity that for any constant $m$, the probability ratios are invariant:\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} = \\frac{\\exp(z_i) \\cdot \\exp(-m)}{\\sum_{j=1}^K \\exp(z_j) \\cdot \\exp(-m)} = \\frac{\\exp(z_i - m)}{\\sum_{j=1}^K \\exp(z_j - m)}\n$$\nThis invariance allows us to shift the logits without changing the final probabilities. We apply this idea to the term $\\ln(\\sum_j \\exp(z_j))$:\n$$\n\\ln\\left(\\sum_{j=1}^K \\exp(z_j)\\right) = \\ln\\left(\\exp(m) \\sum_{j=1}^K \\exp(z_j - m)\\right) = \\ln(\\exp(m)) + \\ln\\left(\\sum_{j=1}^K \\exp(z_j - m)\\right) = m + \\ln\\left(\\sum_{j=1}^K \\exp(z_j - m)\\right)\n$$\nBy choosing $m = \\max_j(z_j)$, we ensure numerical stability:\n1.  **Overflow Prevention**: The argument to the exponential function, $z_j - m$, is now always less than or equal to $0$. The largest value it takes is $z_k - m = m - m = 0$ for the index $k$ where $z_k$ is the maximum. Thus, the largest term in the sum is $\\exp(0) = 1$. This prevents any term from overflowing.\n2.  **Underflow Prevention**: Since at least one term in the sum $\\sum_j \\exp(z_j - m)$ is $\\exp(0)=1$, the sum is guaranteed to be at least $1$. This prevents the argument of the logarithm from underflowing to zero, thereby avoiding a result of $-\\infty$.\n\nSubstituting this stable log-sum-exp expression back into the loss formula, we get the numerically stable CCE:\n$$\nL_{\\text{CCE, stable}} = -z_c + \\left( \\max_j(z_j) + \\ln\\left(\\sum_{j=1}^K \\exp(z_j - \\max_j(z_j))\\right) \\right)\n$$\n\n### Part 2: Equivalence of CCE and BCE for $K=2$\n\nFor the special case of a binary classification problem ($K=2$), we will show that CCE is equivalent to Binary Cross-Entropy (BCE). Let the logits be $\\mathbf{z}=[z_0, z_1]$. Let's define a binary random variable with label $y \\in \\{0, 1\\}$, where $y=1$ corresponds to class $c=1$ and $y=0$ to class $c=0$.\n\nThe probability of class $1$ is given by the sigmoid (logistic) function of the log-odds. The log-odds for class $1$ versus class $0$ is $a = z_1 - z_0$. The probability $p_1$ is:\n$$\np_1 = P(y=1) = \\frac{\\exp(z_1)}{\\exp(z_0) + \\exp(z_1)} = \\frac{\\exp(z_1-z_0)}{\\exp(z_0-z_0) + \\exp(z_1-z_0)} = \\frac{\\exp(a)}{1 + \\exp(a)} = \\sigma(a)\n$$\nThe probability of class $0$ is $p_0 = 1 - p_1 = 1 - \\sigma(a) = \\sigma(-a)$. The BCE loss for a binary label $y$ and logit $a$ is:\n$$\nL_{\\text{BCE}} = -[y \\ln(p_1) + (1-y) \\ln(p_0)] = -[y \\ln(\\sigma(a)) + (1-y) \\ln(\\sigma(-a))]\n$$\nNow, let's analyze the CCE loss for $K=2$: $L_{\\text{CCE}} = -z_c + \\ln(\\exp(z_0) + \\exp(z_1))$.\n- If the true class is $c=1$ (so $y=1$):\n$$\nL_{\\text{CCE}}(c=1) = -z_1 + \\ln(\\exp(z_0) + \\exp(z_1)) = -z_1 + \\ln(\\exp(z_1)(\\exp(z_0-z_1) + 1)) = -z_1 + z_1 + \\ln(\\exp(-a) + 1) = \\ln(1 + \\exp(-a))\n$$\nThe BCE loss for $y=1$ is $-\\ln(\\sigma(a)) = -\\ln(\\frac{1}{1+\\exp(-a)}) = \\ln(1+\\exp(-a))$. The expressions match.\n\n- If the true class is $c=0$ (so $y=0$):\n$$\nL_{\\text{CCE}}(c=0) = -z_0 + \\ln(\\exp(z_0) + \\exp(z_1)) = -z_0 + \\ln(\\exp(z_0)(1 + \\exp(z_1-z_0))) = -z_0 + z_0 + \\ln(1 + \\exp(a)) = \\ln(1 + \\exp(a))\n$$\nThe BCE loss for $y=0$ is $-\\ln(\\sigma(-a)) = -\\ln(\\frac{1}{1+\\exp(a)}) = \\ln(1+\\exp(a))$. The expressions match again.\nThus, CCE for $K=2$ is equivalent to BCE.\n\n### Part 3: Numerically Stable BCE with SoftPlus\n\nThe SoftPlus function is defined as $\\text{SoftPlus}(x) = \\ln(1 + \\exp(x))$. Our derived BCE expressions are $\\text{SoftPlus}(-a)$ for $y=1$ and $\\text{SoftPlus}(a)$ for $y=0$. A unified, more convenient expression for BCE loss is:\n$$\nL_{\\text{BCE}} = -ya + \\text{SoftPlus}(a) = -ya + \\ln(1 + \\exp(a))\n$$\nLet's verify this form:\n- For $y=1$: $L = -a + \\ln(1+\\exp(a)) = \\ln(\\exp(-a)) + \\ln(1+\\exp(a)) = \\ln(\\exp(-a)(1+\\exp(a))) = \\ln(1+\\exp(-a))$. Correct.\n- For $y=0$: $L = \\ln(1+\\exp(a))$. Correct.\n\nThis form $L_{\\text{BCE}} = -ya + \\text{SoftPlus}(a)$ is stable, provided that $\\text{SoftPlus}(a)$ is computed stably. The naive computation of $\\text{SoftPlus}(a) = \\ln(1+\\exp(a))$ overflows if $a$ is large and positive. We can use the identity $\\ln(1+\\exp(a)) = a + \\ln(1+\\exp(-a))$. This gives a stable implementation:\n$$\n\\text{SoftPlus}(a) =\n\\begin{cases}\n    \\ln(1 + \\exp(a)) & \\text{if } a \\le 0 \\\\\n    a + \\ln(1 + \\exp(-a)) & \\text{if } a > 0\n\\end{cases}\n$$\nThis can be written compactly as $\\max(0, a) + \\ln(1 + \\exp(-|a|))$. Substituting this into the unified BCE expression yields a fully stable computation for any values of $y$ and $a$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests naive and stable cross-entropy loss functions.\n    \"\"\"\n\n    def naive_cce(z, c):\n        \"\"\"\n        Computes CCE using a naive implementation of softmax. Detects numerical failure.\n        A failure is defined as any intermediate or final value being non-finite (inf, -inf, or nan).\n        \n        Returns:\n            (bool, float): A tuple (failure_detected, loss_value).\n        \"\"\"\n        try:\n            # Use high precision floats to be robust, but still subject to machine limits\n            z_np = np.array(z, dtype=np.float64)\n\n            # Python's `math.exp` raises OverflowError, numpy's `np.exp` returns `inf`\n            # We catch both possibilities using numpy's behavior and checking for inf.\n            with np.errstate(over='ignore'):\n                exps = np.exp(z_np)\n            \n            # Check for overflow\n            if np.isinf(exps).any():\n                return True, 0.0\n\n            sum_exps = np.sum(exps)\n            \n            # Check for underflow of all terms, or overflow of sum\n            if sum_exps == 0.0 or not np.isfinite(sum_exps):\n                return True, 0.0\n\n            probs = exps / sum_exps\n            \n            # This check is mostly for division by zero (sum_exps=0) resulting in nan\n            if not np.all(np.isfinite(probs)):\n                return True, 0.0\n\n            # The probability of the correct class might be zero due to underflow\n            prob_c = probs[c]\n            if prob_c <= 0:\n                return True, 0.0\n\n            loss = -np.log(prob_c)\n\n            if not np.isfinite(loss):\n                return True, 0.0\n\n            return False, loss\n        except Exception:\n            return True, 0.0\n\n    def stable_cce(z, c):\n        \"\"\"\n        Computes CCE using the numerically stable log-sum-exp trick.\n        \"\"\"\n        z_np = np.array(z, dtype=np.float64)\n        m = np.max(z_np)\n        log_sum_exp = m + np.log(np.sum(np.exp(z_np - m)))\n        loss = -z_np[c] + log_sum_exp\n        return loss\n\n    def stable_bce(z, c):\n        \"\"\"\n        Computes numerically stable BCE for a K=2 case.\n        The problem statement specifies to model the provided correct class `c`\n        as the positive class with a binary label y=1.\n        \"\"\"\n        z_np = np.array(z, dtype=np.float64)\n        \n        # The logit 'a' is the log-odds of the positive class (class `c`)\n        if c == 0:\n            a = z_np[0] - z_np[1]\n        else: # c == 1\n            a = z_np[1] - z_np[0]\n            \n        y = 1.0 # The label for the positive class is 1.\n\n        # Stable SoftPlus(x) = max(0, x) + log(1 + exp(-|x|))\n        stable_softplus_a = np.maximum(0.0, a) + np.log(1.0 + np.exp(-np.abs(a)))\n        \n        # Stable BCE loss L = -y*a + SoftPlus(a)\n        loss = -y * a + stable_softplus_a\n        return loss\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'z': [1.0, 0.0, -1.0], 'c': 0},\n        {'z': [120.0, 0.0, -120.0], 'c': 1},\n        {'z': [1000.0, 999.0, 998.0], 'c': 0},\n        {'z': [-1000.0, -1001.0, -1002.0], 'c': 2},\n        {'z': [5.0, -5.0], 'c': 0},\n    ]\n\n    results = []\n\n    # Process Cases 1-4\n    for i in range(4):\n        case = test_cases[i]\n        fail, _ = naive_cce(case['z'], case['c'])\n        s_loss = stable_cce(case['z'], case['c'])\n        results.append(fail)\n        results.append(f\"{s_loss:.6f}\")\n    \n    # Process Case 5\n    case5 = test_cases[4]\n    fail5, _ = naive_cce(case5['z'], case5['c'])\n    cce_loss5 = stable_cce(case5['z'], case5['c'])\n    bce_loss5 = stable_bce(case5['z'], case5['c'])\n    abs_diff = np.abs(cce_loss5 - bce_loss5)\n    \n    results.append(fail5)\n    results.append(f\"{abs_diff:.12f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3103417"}, {"introduction": "A single loss value averaged over an entire dataset gives a useful but incomplete picture of a model's performance, as it can easily mask poor performance on rare classes or systematic biases. This exercise [@problem_id:3103440] teaches you to look deeper by decomposing the total categorical cross-entropy loss into per-class contributions. By calculating these components, you can create powerful diagnostics to identify and analyze issues like class imbalance, leading to more targeted model improvements.", "problem": "You are given a classification dataset with $K=3$ classes and $N=12$ labeled examples. For each example $i \\in \\{1,\\dots,N\\}$, the model outputs a predictive probability vector $\\mathbf{p}_i = (p_{i1}, p_{i2}, p_{i3})$ with $\\sum_{k=1}^{3} p_{ik} = 1$, and the true label is represented as a one-hot vector $\\mathbf{y}_i = (y_{i1}, y_{i2}, y_{i3})$, where $y_{ik} \\in \\{0,1\\}$ and $\\sum_{k=1}^{3} y_{ik} = 1$. Assume the logarithm base is the natural logarithm $\\ln$. The categorical cross-entropy (CCE) loss over the dataset is defined as the sum of per-example negative log-likelihoods.\n\nTask:\n1) Starting from the definition of the negative log-likelihood for a multinomial model with one-hot labels, derive a decomposition of the total categorical cross-entropy (CCE) loss into a sum of per-class contributions. Then, define the per-class mean negative log-likelihood as well as the overall mean negative log-likelihood across all examples.\n2) Using your decomposition, compute the following class-imbalance miscalibration diagnostic index:\n$$\nI \\;=\\; \\sum_{k=1}^{3} \\frac{n_k}{N} \\left(\\bar{\\ell}_k - \\bar{\\ell}\\right)^{2}\n$$ ,\nwhere $n_k$ is the number of examples whose true class is $k$, $\\bar{\\ell}_k$ is the mean negative log-likelihood for class $k$, and $\\bar{\\ell}$ is the overall mean negative log-likelihood across all $N$ examples.\n\nUse the following dataset of $12$ examples, each given as a true class and the predicted probability vector:\n- Example $1$: true class $1$, $\\mathbf{p}_1=(0.8, 0.1, 0.1)$.\n- Example $2$: true class $1$, $\\mathbf{p}_2=(0.7, 0.2, 0.1)$.\n- Example $3$: true class $1$, $\\mathbf{p}_3=(0.9, 0.05, 0.05)$.\n- Example $4$: true class $1$, $\\mathbf{p}_4=(0.6, 0.3, 0.1)$.\n- Example $5$: true class $1$, $\\mathbf{p}_5=(0.8, 0.15, 0.05)$.\n- Example $6$: true class $1$, $\\mathbf{p}_6=(0.7, 0.15, 0.15)$.\n- Example $7$: true class $2$, $\\mathbf{p}_7=(0.3, 0.5, 0.2)$.\n- Example $8$: true class $2$, $\\mathbf{p}_8=(0.2, 0.6, 0.2)$.\n- Example $9$: true class $2$, $\\mathbf{p}_9=(0.4, 0.4, 0.2)$.\n- Example $10$: true class $2$, $\\mathbf{p}_{10}=(0.25, 0.5, 0.25)$.\n- Example $11$: true class $3$, $\\mathbf{p}_{11}=(0.4, 0.3, 0.3)$.\n- Example $12$: true class $3$, $\\mathbf{p}_{12}=(0.6, 0.2, 0.2)$.\n\nCompute $I$ from first principles and report a single numerical value. Round your answer to four significant figures.", "solution": "The user has provided a problem that is well-posed, scientifically grounded, and contains all necessary information for a solution. The problem is therefore deemed **valid**.\n\n### Part 1: Derivation of CCE Decomposition and Definitions\n\nThe starting point is the likelihood of a single observation $i$ under a multinomial classification model. For an observation with a true class label represented by a one-hot vector $\\mathbf{y}_i = (y_{i1}, y_{i2}, \\dots, y_{iK})$ and a model prediction vector $\\mathbf{p}_i = (p_{i1}, p_{i2}, \\dots, p_{iK})$, the likelihood is the probability of the true outcome:\n$$\nL(\\mathbf{p}_i | \\mathbf{y}_i) = \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\nwhere $K$ is the number of classes.\n\nThe negative log-likelihood (NLL) for this single observation, denoted as $\\ell_i$, is defined as:\n$$\n\\ell_i = -\\ln(L(\\mathbf{p}_i | \\mathbf{y}_i)) = -\\ln\\left(\\prod_{k=1}^{K} p_{ik}^{y_{ik}}\\right) = -\\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\n$$\nSince $\\mathbf{y}_i$ is a one-hot vector, there is exactly one index $k=c_i$ for which $y_{ic_i}=1$ and $y_{ik}=0$ for all $k \\neq c_i$, where $c_i$ is the true class of example $i$. The sum therefore simplifies to a single term:\n$$\n\\ell_i = -y_{ic_i} \\ln(p_{ic_i}) = -\\ln(p_{ic_i})\n$$\nThe total categorical cross-entropy (CCE) loss over a dataset of $N$ examples is the sum of the individual NLLs:\n$$\nL_{CCE} = \\sum_{i=1}^{N} \\ell_i = \\sum_{i=1}^{N} \\left(-\\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\\right) = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln(p_{ik})\n$$\nTo decompose this total loss into per-class contributions, we swap the order of summation:\n$$\nL_{CCE} = -\\sum_{k=1}^{K} \\sum_{i=1}^{N} y_{ik} \\ln(p_{ik})\n$$\nLet $S_k$ be the set of indices of all examples whose true class is $k$. For a given class $k$, the term $y_{ik}$ is $1$ if $i \\in S_k$ and $0$ otherwise. The inner sum can thus be restricted to examples belonging to class $k$:\n$$\nL_{CCE} = \\sum_{k=1}^{K} \\left(-\\sum_{i \\in S_k} \\ln(p_{ik})\\right)\n$$\nWe define the total loss contribution from class $k$, denoted $L_k$, as:\n$$\nL_k = -\\sum_{i \\in S_k} \\ln(p_{ik})\n$$\nThe total CCE loss is then the sum of these per-class contributions:\n$$\nL_{CCE} = \\sum_{k=1}^{K} L_k\n$$\nLet $n_k = |S_k|$ be the number of examples in class $k$. The per-class mean negative log-likelihood, $\\bar{\\ell}_k$, is the average loss for examples in class $k$:\n$$\n\\bar{\\ell}_k = \\frac{L_k}{n_k} = \\frac{1}{n_k} \\sum_{i \\in S_k} (-\\ln(p_{ik}))\n$$\nThe overall mean negative log-likelihood, $\\bar{\\ell}$, is the average loss over all $N$ examples:\n$$\n\\bar{\\ell} = \\frac{L_{CCE}}{N} = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i = \\frac{1}{N} \\sum_{k=1}^{K} L_k = \\frac{1}{N} \\sum_{k=1}^{K} n_k \\bar{\\ell}_k = \\sum_{k=1}^{K} \\frac{n_k}{N} \\bar{\\ell}_k\n$$\nThis completes the required derivation and definitions.\n\n### Part 2: Computation of the Diagnostic Index $I$\n\nFirst, we process the given dataset. We have $N=12$ examples and $K=3$ classes. We identify the examples belonging to each class and calculate their counts.\n- **Class 1**: Examples $1, 2, 3, 4, 5, 6$. The count is $n_1=6$.\n- **Class 2**: Examples $7, 8, 9, 10$. The count is $n_2=4$.\n- **Class 3**: Examples $11, 12$. The count is $n_3=2$.\nThe total count is $n_1+n_2+n_3 = 6+4+2=12=N$, as expected.\n\nNext, we calculate the per-class mean NLL, $\\bar{\\ell}_k$.\n\nFor **Class 1** ($k=1$):\nThe true class is $1$, so we use $p_{i1}$ for $i \\in \\{1, \\dots, 6\\}$.\n$$\n\\bar{\\ell}_1 = \\frac{1}{6} \\left[ -\\ln(0.8) - \\ln(0.7) - \\ln(0.9) - \\ln(0.6) - \\ln(0.8) - \\ln(0.7) \\right]\n$$\n$$\n\\bar{\\ell}_1 = \\frac{1}{6} \\left[ -2\\ln(0.8) - 2\\ln(0.7) - \\ln(0.9) - \\ln(0.6) \\right]\n$$\n$$\n\\bar{\\ell}_1 = -\\frac{1}{6} \\ln(0.8^2 \\cdot 0.7^2 \\cdot 0.9 \\cdot 0.6) = -\\frac{1}{6} \\ln(0.169344) \\approx \\frac{1.775823}{6} \\approx 0.295971\n$$\nFor **Class 2** ($k=2$):\nThe true class is $2$, so we use $p_{i2}$ for $i \\in \\{7, \\dots, 10\\}$.\n$$\n\\bar{\\ell}_2 = \\frac{1}{4} \\left[ -\\ln(0.5) - \\ln(0.6) - \\ln(0.4) - \\ln(0.5) \\right]\n$$\n$$\n\\bar{\\ell}_2 = \\frac{1}{4} \\left[ -2\\ln(0.5) - \\ln(0.6) - \\ln(0.4) \\right]\n$$\n$$\n\\bar{\\ell}_2 = -\\frac{1}{4} \\ln(0.5^2 \\cdot 0.6 \\cdot 0.4) = -\\frac{1}{4} \\ln(0.06) \\approx \\frac{2.813411}{4} \\approx 0.703353\n$$\nFor **Class 3** ($k=3$):\nThe true class is $3$, so we use $p_{i3}$ for $i \\in \\{11, 12\\}$.\n$$\n\\bar{\\ell}_3 = \\frac{1}{2} \\left[ -\\ln(0.3) - \\ln(0.2) \\right]\n$$\n$$\n\\bar{\\ell}_3 = -\\frac{1}{2} \\ln(0.3 \\cdot 0.2) = -\\frac{1}{2} \\ln(0.06) \\approx \\frac{2.813411}{2} \\approx 1.406705\n$$\nNext, we compute the overall mean NLL, $\\bar{\\ell}$:\n$$\n\\bar{\\ell} = \\sum_{k=1}^{3} \\frac{n_k}{N} \\bar{\\ell}_k = \\frac{6}{12}\\bar{\\ell}_1 + \\frac{4}{12}\\bar{\\ell}_2 + \\frac{2}{12}\\bar{\\ell}_3\n$$\n$$\n\\bar{\\ell} \\approx \\frac{1}{2}(0.295971) + \\frac{1}{3}(0.703353) + \\frac{1}{6}(1.406705)\n$$\n$$\n\\bar{\\ell} \\approx 0.1479855 + 0.234451 + 0.234451 = 0.6168875\n$$\nFinally, we compute the class-imbalance miscalibration diagnostic index $I$:\n$$\nI = \\sum_{k=1}^{3} \\frac{n_k}{N} \\left(\\bar{\\ell}_k - \\bar{\\ell}\\right)^{2}\n$$\nWe calculate the individual terms:\n- For $k=1$:\n$$\n\\frac{n_1}{N}\\left(\\bar{\\ell}_1 - \\bar{\\ell}\\right)^2 \\approx \\frac{6}{12} (0.295971 - 0.6168875)^2 = 0.5 \\times (-0.3209165)^2 \\approx 0.5 \\times 0.1029874 \\approx 0.051494\n$$\n- For $k=2$:\n$$\n\\frac{n_2}{N}\\left(\\bar{\\ell}_2 - \\bar{\\ell}\\right)^2 \\approx \\frac{4}{12} (0.703353 - 0.6168875)^2 = \\frac{1}{3} \\times (0.0864655)^2 \\approx \\frac{1}{3} \\times 0.0074763 \\approx 0.002492\n$$\n- For $k=3$:\n$$\n\\frac{n_3}{N}\\left(\\bar{\\ell}_3 - \\bar{\\ell}\\right)^2 \\approx \\frac{2}{12} (1.406705 - 0.6168875)^2 = \\frac{1}{6} \\times (0.7898175)^2 \\approx \\frac{1}{6} \\times 0.6238117 \\approx 0.103969\n$$\nSumming the terms gives the value of $I$:\n$$\nI \\approx 0.051494 + 0.002492 + 0.103969 = 0.157955\n$$\nRounding the result to four significant figures, we get $0.1580$.", "answer": "$$\\boxed{0.1580}$$", "id": "3103440"}, {"introduction": "While minimizing cross-entropy loss often improves accuracy, it can sometimes lead to an undesirable side effect: overconfidence, where a model's predicted probabilities do not reflect its true correctness likelihood. This property, known as calibration, is critical for reliable decision-making. This practice [@problem_id:3103445] presents a case study where improving accuracy worsens calibration and introduces temperature scaling as a technique to recalibrate the model's confidence without altering its predictions.", "problem": "You are given a classification scenario in which a model produces unnormalized scores (logits) for $K$ classes on $N$ samples. Starting from the principle of maximum likelihood for a multinomial model with the softmax link, derive the average negative log-likelihood per sample as a loss functional for multi-class classification. Then, demonstrate a case study in which lowering this loss increases accuracy but worsens calibration due to overconfident predictions, and apply temperature scaling to correct calibration while preserving predicted class rankings.\n\nYour program must implement the following steps without using any shortcuts that skip derivations:\n\n- From the principle of maximum likelihood for a multinomial classification and the softmax construction, derive the expression for the categorical negative log-likelihood loss (commonly known as categorical cross-entropy) as an average over samples.\n- Define and compute classification accuracy as the fraction of samples with predicted class equal to the true class.\n- Define and compute calibration via the Expected Calibration Error (ECE) using confidence binning: partition the interval $[0,1]$ into $B$ equal-width bins, compute for each bin the difference between average confidence and average accuracy, and aggregate a weighted average by the fraction of samples in the bin. All quantities must be expressed as decimals in $[0,1]$.\n\nUse the following explicit test suite consisting of three cases. For each case, the ground-truth labels are the same vector of length $N = 8$ with $K = 3$ classes. The labels are encoded as integers in $\\{0,1,2\\}$:\n\n- Labels $y$: $[0,1,2,0,1,2,0,2]$.\n\n- Case $1$ (baseline logits $L^{(1)}$ with moderate confidence and mixed correctness): a matrix of shape $N \\times K$,\n  $$\n  L^{(1)} =\n  \\begin{bmatrix}\n  0.8 & 0.9 & 0.7 \\\\\n  0.4 & 0.6 & 0.5 \\\\\n  0.3 & 0.4 & 0.5 \\\\\n  0.6 & 0.5 & 0.4 \\\\\n  0.7 & 0.6 & 0.5 \\\\\n  0.5 & 0.6 & 0.7 \\\\\n  0.2 & 0.3 & 0.1 \\\\\n  0.4 & 0.5 & 0.45\n  \\end{bmatrix}.\n  $$\n  Use $B = 5$ confidence bins for ECE.\n\n- Case $2$ (overconfident logits $L^{(2)}$ with higher accuracy but worse calibration):\n  $$\n  L^{(2)} =\n  \\begin{bmatrix}\n  3.0 & 1.0 & -1.0 \\\\\n  0.0 & 3.0 & -1.0 \\\\\n  -0.5 & 0.5 & 2.5 \\\\\n  2.5 & 0.0 & -1.5 \\\\\n  3.0 & 0.5 & -0.5 \\\\\n  -1.0 & 0.0 & 3.0 \\\\\n  -1.0 & 3.0 & -2.0 \\\\\n  -1.5 & 0.0 & 2.5\n  \\end{bmatrix}.\n  $$\n  Use $B = 5$ confidence bins for ECE.\n\n- Case $3$ (temperature scaling for case $2$): apply temperature scaling with temperature $T \\in S$, where $S = \\{1.0, 1.5, 2.0, 3.0\\}$, by dividing each logit in $L^{(2)}$ by $T$ before softmax. Select the temperature $T^\\star \\in S$ that minimizes the Expected Calibration Error on the given data in case $2$ (express ECE as a decimal). Compute accuracy, average categorical loss, and ECE under $T^\\star$.\n\nScientific realism constraints:\n\n- The derivation must start from maximum likelihood for multinomial classification and the softmax function as the link, not from pre-stated target formulas.\n- Confidence is defined as the maximum predicted class probability per sample.\n- Temperature scaling must preserve class ranking because dividing by a positive scalar $T$ does not alter the order of logits.\n\nFinal output requirements:\n\n- For each case, compute the following in order: accuracy, average categorical negative log-likelihood loss, and Expected Calibration Error (ECE), each as a decimal in $[0,1]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  $$\n  [\\text{Acc}^{(1)}, \\text{Loss}^{(1)}, \\text{ECE}^{(1)}, \\text{Acc}^{(2)}, \\text{Loss}^{(2)}, \\text{ECE}^{(2)}, \\text{Acc}^{(2,T^\\star)}, \\text{Loss}^{(2,T^\\star)}, \\text{ECE}^{(2,T^\\star)}].\n  $$\n- Round each output to $6$ decimal places. All quantities must be decimals; do not use a percentage sign. No physical units are involved; do not include any units.\n\nYour implementation must be a complete, runnable program that takes no input and prints exactly the required single-line output.", "solution": "The problem is valid as it is a self-contained, scientifically grounded, and well-posed exercise in machine learning. It asks for the derivation of a fundamental loss function from first principles and its application in a concrete numerical scenario to illustrate the important concepts of model accuracy, confidence, and calibration. All necessary data, definitions, and constraints are provided. We may now proceed with the solution.\n\nThe solution is structured as follows:\n1.  Derivation of the average negative log-likelihood loss for multi-class classification from the principle of maximum likelihood.\n2.  Formal definitions of the performance metrics: accuracy, confidence, and Expected Calibration Error (ECE).\n3.  Application of these principles to the three test cases provided.\n\n**1. Derivation of the Negative Log-Likelihood Loss**\n\nWe consider a multi-class classification problem with $N$ independent samples and $K$ classes. For each sample $i$ (where $i \\in \\{1, \\dots, N\\}$), a model takes an input vector $x_i$ and produces a vector of unnormalized scores, or logits, $l_i = (l_{i1}, l_{i2}, \\dots, l_{iK})$.\n\nThe model's predicted probability for each class $k \\in \\{1, \\dots, K\\}$ for sample $i$ is obtained by applying the softmax function to the logits:\n$$\np_{ik} = P(\\text{class}=k | x_i) = \\frac{\\exp(l_{ik})}{\\sum_{j=1}^{K} \\exp(l_{ij})}\n$$\nThe set of probabilities for sample $i$ is the vector $p_i = (p_{i1}, p_{i2}, \\dots, p_{iK})$. Note that $\\sum_{k=1}^K p_{ik} = 1$ and $p_{ik} > 0$ for all $i, k$.\n\nThe ground truth label for sample $i$ is denoted by $c_i \\in \\{1, \\dots, K\\}$. We can represent this label using a one-hot encoded vector $y_i$ of length $K$, where its $k$-th component, $y_{ik}$, is given by the indicator function:\n$$\ny_{ik} = \\mathbb{I}(k=c_i) = \\begin{cases} 1 & \\text{if } k = c_i \\\\ 0 & \\text{if } k \\neq c_i \\end{cases}\n$$\nThe outcome of the classification for a single sample can be modeled as a draw from a Categorical distribution, which is a specific case of the Multinomial distribution with one trial ($n=1$). The probability mass function for observing the outcome $y_i$ given the predicted probabilities $p_i$ is:\n$$\nP(Y_i=y_i) = \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\nSince $y_{ik}$ is $1$ only for the true class $c_i$ and $0$ otherwise, this expression simplifies to $p_{i, c_i}$, the predicted probability of the true class.\n\nThe principle of maximum likelihood estimation (MLE) states that we should choose the model parameters (which determine the logits $l_i$) that maximize the likelihood of observing the given dataset. Assuming the $N$ samples are independent and identically distributed (i.i.d.), the total likelihood of the dataset is the product of the individual sample likelihoods:\n$$\n\\mathcal{L} = \\prod_{i=1}^{N} P(Y_i=y_i) = \\prod_{i=1}^{N} \\prod_{k=1}^{K} p_{ik}^{y_{ik}}\n$$\nFor numerical stability and mathematical convenience, we work with the log-likelihood:\n$$\n\\ln \\mathcal{L} = \\ln \\left( \\prod_{i=1}^{N} \\prod_{k=1}^{K} p_{ik}^{y_{ik}} \\right) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln p_{ik}\n$$\nIn machine learning optimization, it is standard to minimize a loss function rather than maximize a likelihood. Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood (NLL):\n$$\n\\text{NLL} = - \\ln \\mathcal{L} = - \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln p_{ik}\n$$\nThis expression is the definition of the categorical cross-entropy between the true distributions (the one-hot vectors $y_i$) and the predicted distributions ($p_i$). As a loss function for training, we typically use the average NLL per sample:\n$$\nJ = \\frac{1}{N} \\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\ln p_{ik}\n$$\nAgain, since $y_{ik}$ is non-zero only for the true class $c_i$, the inner sum collapses, and the loss function simplifies to:\n$$\nJ = -\\frac{1}{N} \\sum_{i=1}^{N} \\ln p_{i, c_i}\n$$\nThis is the average negative log-likelihood, or categorical cross-entropy loss. By substituting the softmax formula, we can express the loss for a single sample $i$ in terms of its logits $l_i$:\n$$\nJ_i = -\\ln p_{i, c_i} = -\\ln \\left( \\frac{\\exp(l_{i, c_i})}{\\sum_{j=1}^{K} \\exp(l_{ij})} \\right) = \\ln\\left(\\sum_{j=1}^{K} \\exp(l_{ij})\\right) - l_{i, c_i}\n$$\nThis completes the derivation.\n\n**2. Performance Metrics**\n\nWe define the required metrics as follows:\n\n-   **Accuracy**: The predicted class for sample $i$ is $\\hat{y}_i = \\arg\\max_{k} p_{ik} = \\arg\\max_{k} l_{ik}$. Accuracy is the fraction of correctly classified samples:\n    $$\n    \\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(\\hat{y}_i = c_i)\n    $$\n-   **Confidence**: The confidence of a prediction for sample $i$ is the model's probability assigned to its predicted class, which is simply the maximum probability in the output vector $p_i$:\n    $$\n    \\text{conf}_i = \\max_{k} p_{ik}\n    $$\n-   **Expected Calibration Error (ECE)**: ECE measures the discrepancy between confidence and accuracy. The interval $[0,1]$ is partitioned into $B$ equal-width bins, $M_1, M_2, \\dots, M_B$. For each bin $M_b$, we compute the average accuracy and average confidence of all samples whose prediction confidence falls within that bin. Let $I_b$ be the set of sample indices $i$ such that $\\text{conf}_i \\in M_b$, and let $n_b = |I_b|$. If $n_b > 0$:\n    $$\n    \\text{acc}(b) = \\frac{1}{n_b} \\sum_{i \\in I_b} \\mathbb{I}(\\hat{y}_i = c_i)\n    $$\n    $$\n    \\text{conf}(b) = \\frac{1}{n_b} \\sum_{i \\in I_b} \\text{conf}_i\n    $$\n    The ECE is the weighted average of the absolute difference between these quantities over all bins:\n    $$\n    \\text{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{N} \\left| \\text{acc}(b) - \\text{conf}(b) \\right|\n    $$\n    A perfectly calibrated model has an ECE of $0$.\n\n-   **Temperature Scaling**: This is a post-processing technique to improve model calibration. It works by scaling the logits by a positive temperature parameter $T > 0$ before the softmax function:\n    $$\n    p_{ik}(T) = \\frac{\\exp(l_{ik}/T)}{\\sum_{j=1}^{K} \\exp(l_{ij}/T)}\n    $$\n    A value of $T>1$ \"softens\" the probabilities, pushing them closer to a uniform distribution and reducing overconfidence. A key property is that for any $T > 0$, temperature scaling does not change the $\\arg\\max$ of the logits, so it does not alter the model's predictions $\\hat{y}_i$ or its accuracy. The optimal temperature $T^\\star$ is found by minimizing a calibration metric, such as ECE, on a validation set.\n\n**3. Application to Test Cases**\n\nWe will now compute the required metrics (accuracy, loss, ECE) for the three specified cases. For all cases, $N=8$, $K=3$, and the ground-truth labels are $y = [0, 1, 2, 0, 1, 2, 0, 2]$.\n\n-   **Case 1**: Baseline logits $L^{(1)}$, using $B=5$ bins for ECE.\n-   **Case 2**: Overconfident logits $L^{(2)}$, using $B=5$ bins for ECE. We expect to see higher accuracy, lower loss, but a higher ECE compared to Case 1, demonstrating the classic accuracy-calibration trade-off.\n-   **Case 3**: Temperature scaling applied to Case 2. We will find the optimal temperature $T^\\star$ from the set $S = \\{1.0, 1.5, 2.0, 3.0\\}$ that minimizes ECE for the data from Case 2. As noted, the accuracy for Case 3 will be identical to that of Case 2. We will then report all three metrics for the model calibrated with $T^\\star$.\n\nThe computations are performed by the program provided in the final answer. The program implements the softmax function, the loss function, accuracy calculation, and the ECE calculation with binning as described above. It then iterates through the cases and computes the required values.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and calculating classification metrics\n    for the provided test cases.\n    \"\"\"\n\n    # --- Data Definition ---\n    y_true = np.array([0, 1, 2, 0, 1, 2, 0, 2])\n    \n    L1 = np.array([\n        [0.8, 0.9, 0.7],\n        [0.4, 0.6, 0.5],\n        [0.3, 0.4, 0.5],\n        [0.6, 0.5, 0.4],\n        [0.7, 0.6, 0.5],\n        [0.5, 0.6, 0.7],\n        [0.2, 0.3, 0.1],\n        [0.4, 0.5, 0.45]\n    ])\n\n    L2 = np.array([\n        [3.0, 1.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-0.5, 0.5, 2.5],\n        [2.5, 0.0, -1.5],\n        [3.0, 0.5, -0.5],\n        [-1.0, 0.0, 3.0],\n        [-1.0, 3.0, -2.0],\n        [-1.5, 0.0, 2.5]\n    ])\n\n    T_values = [1.0, 1.5, 2.0, 3.0]\n    n_bins = 5\n\n    # --- Helper Functions ---\n    def stable_softmax(logits, T=1.0):\n        \"\"\"\n        Computes softmax probabilities in a numerically stable way.\n        Optionally applies temperature scaling.\n        \"\"\"\n        scaled_logits = logits / T\n        # Subtract max for numerical stability\n        exps = np.exp(scaled_logits - np.max(scaled_logits, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def calculate_metrics(logits, y, n_bins_ece, T=1.0):\n        \"\"\"\n        Calculates accuracy, average NLL loss, and ECE.\n        \"\"\"\n        N = logits.shape[0]\n        probs = stable_softmax(logits, T)\n\n        # Accuracy\n        y_pred = np.argmax(probs, axis=1)\n        correctness = (y_pred == y)\n        accuracy = np.mean(correctness)\n\n        # Average Categorical Negative Log-Likelihood (Cross-Entropy)\n        # We need the probabilities corresponding to the true classes\n        true_class_probs = probs[np.arange(N), y]\n        # To avoid log(0), add a small epsilon\n        loss = -np.mean(np.log(true_class_probs + 1e-9))\n        \n        # Expected Calibration Error (ECE)\n        confidences = np.max(probs, axis=1)\n        ece = 0.0\n        bin_boundaries = np.linspace(0, 1, n_bins_ece + 1)\n        \n        for b in range(n_bins_ece):\n            bin_lower = bin_boundaries[b]\n            bin_upper = bin_boundaries[b+1]\n            \n            # Define samples in the current bin\n            # The first bin [0, 1/B] is inclusive of 0.\n            if b == 0:\n                in_bin = (confidences >= bin_lower) & (confidences <= bin_upper)\n            else:\n                # Subsequent bins ( (k-1)/B, k/B ] are exclusive of the lower bound\n                in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n            \n            n_in_bin = np.sum(in_bin)\n            \n            if n_in_bin > 0:\n                acc_in_bin = np.mean(correctness[in_bin])\n                conf_in_bin = np.mean(confidences[in_bin])\n                ece += (n_in_bin / N) * np.abs(acc_in_bin - conf_in_bin)\n                \n        return accuracy, loss, ece\n\n    # --- Calculations for each case ---\n    results = []\n\n    # Case 1\n    acc1, loss1, ece1 = calculate_metrics(L1, y_true, n_bins)\n    results.extend([acc1, loss1, ece1])\n\n    # Case 2\n    acc2, loss2, ece2 = calculate_metrics(L2, y_true, n_bins)\n    results.extend([acc2, loss2, ece2])\n\n    # Case 3\n    ece_by_T = []\n    metrics_by_T = []\n    for T in T_values:\n        metrics = calculate_metrics(L2, y_true, n_bins, T=T)\n        metrics_by_T.append(metrics)\n        ece_by_T.append(metrics[2])\n\n    # Find the temperature T* that minimizes ECE\n    # np.argmin returns the index of the first minimum in case of a tie.\n    best_T_idx = np.argmin(ece_by_T)\n    acc3, loss3, ece3 = metrics_by_T[best_T_idx]\n    results.extend([acc3, loss3, ece3])\n    \n    # --- Format and Print Output ---\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3103445"}]}