{"hands_on_practices": [{"introduction": "To truly master automatic differentiation, it's essential to look under the hood and trace the flow of gradients yourself. This exercise guides you through simulating a reverse-mode AD engine for a simple scalar function. By constructing a computational graph (or \"tape\") and manually performing the sequence of Vector-Jacobian Product (VJP) pulls, you will gain a concrete intuition for how AD mechanically applies the chain rule to compute gradients efficiently, a process that is fundamental to training any deep learning model [@problem_id:3100431].", "problem": "Consider reverse-mode automatic differentiation (AD) in the context of deep learning, where gradients of scalar loss functions with respect to parameters are computed efficiently by traversing a computational graph backward using the chain rule. The function of interest is the scalar map $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ with $y\\neq 0$. Using only elementary operations compatible with a computational graph (multiplication, sine, exponential, and division), construct a minimal set of intermediate variables that evaluates $f(x,y)$ and a tape that records the parent-child relationships for these operations. Then, using the principle of the chain rule for composite functions and the concept of Vector-Jacobian Product (VJP), derive the exact sequence of backward passes (VJP pulls) needed to obtain the gradient $\\nabla f(x,y)$ by hand. Your derivation should clearly identify the order of traversing the tape in reverse and the local contributions to the adjoints of the inputs at each step. Provide the final analytical expression for $\\nabla f(x,y)$ as a row vector. Do not round; the final answer must be an exact symbolic expression.", "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique and meaningful solution. The task concerns the application of reverse-mode automatic differentiation (AD), a cornerstone algorithm in computational calculus and deep learning, to a differentiable function. The procedure is formalizable and aligns with established principles.\n\nWe are tasked with computing the gradient $\\nabla f(x,y)$ of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ for $y \\neq 0$, using the principles of reverse-mode AD. This involves a forward pass to construct a computational graph and evaluate the function, followed by a backward pass to propagate gradients.\n\nFirst, we decompose the function into a sequence of elementary operations. This sequence defines the computational graph, or \"tape\". Let the inputs be $v_1 = x$ and $v_2 = y$.\n\n**Forward Pass: Constructing the Computational Graph**\n\nThe evaluation of $f(x,y)$ can be represented by the following minimal set of intermediate variables:\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\nThis sequence constitutes the forward pass. The tape records these operations and their dependencies: $(v_3, \\text{mul}, v_1, v_2)$, $(v_4, \\sin, v_3)$, $(v_5, \\exp, v_1)$, $(v_6, \\text{div}, v_5, v_2)$, $(v_7, \\text{add}, v_4, v_6)$.\n\n**Backward Pass: Gradient Computation using the Chain Rule**\n\nThe backward pass computes the partial derivatives of the final output $v_7$ with respect to each intermediate variable $v_i$, which are denoted as adjoints, $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$. The process begins by initializing the adjoint of the output node to $1$, i.e., $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$. All other adjoints are initialized to $0$. We then traverse the graph in reverse topological order.\n\nThe core principle is the chain rule. For an operation $v_k = g(v_i, v_j, \\dots)$, the adjoints of its parents are updated by accumulating the child's adjoint multiplied by the local partial derivative:\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n... and so on. This operation is effectively a Vector-Jacobian Product (VJP) pull.\n\nLet's compute the adjoints in reverse order of the forward pass:\n\n1.  **Start:** Initialize adjoints: $\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$.\n    Set the seed for the backward pass: $\\bar{v}_7 = 1$.\n\n2.  **Node $v_7 = v_4 + v_6$:**\n    The parents are $v_4$ and $v_6$.\n    Local partial derivatives: $\\frac{\\partial v_7}{\\partial v_4} = 1$, $\\frac{\\partial v_7}{\\partial v_6} = 1$.\n    Update parent adjoints:\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$.\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$.\n    Current state: $\\bar{v}_4=1, \\bar{v}_6=1$.\n\n3.  **Node $v_6 = \\frac{v_5}{v_2}$:**\n    The parents are $v_5$ and $v_2$.\n    Local partial derivatives: $\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$, $\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$.\n    Update parent adjoints:\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$.\n    Current state: $\\bar{v}_5 = \\frac{1}{y}$, $\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$.\n\n4.  **Node $v_5 = \\exp(v_1)$:**\n    The parent is $v_1$.\n    Local partial derivative: $\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$.\n    Update parent adjoint:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$.\n    Current state: $\\bar{v}_1 = \\frac{\\exp(x)}{y}$.\n\n5.  **Node $v_4 = \\sin(v_3)$:**\n    The parent is $v_3$.\n    Local partial derivative: $\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$.\n    Update parent adjoint:\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$.\n    Current state: $\\bar{v}_3 = \\cos(xy)$.\n\n6.  **Node $v_3 = v_1 \\cdot v_2$:**\n    The parents are $v_1$ and $v_2$. Note that $v_1$ and $v_2$ have already received gradients from other paths; we accumulate the new contributions.\n    Local partial derivatives: $\\frac{\\partial v_3}{\\partial v_1} = v_2$, $\\frac{\\partial v_3}{\\partial v_2} = v_1$.\n    Update parent adjoints:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$.\n\nThe process terminates as we have computed the adjoints for all input nodes.\nThe final gradients are the final values of the adjoints of the input variables:\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\nThe gradient vector $\\nabla f(x,y)$ is the row vector of these partial derivatives:\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\nThis derivation rigorously follows the mechanical steps of reverse-mode automatic differentiation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$", "id": "3100431"}, {"introduction": "While AD frameworks are powerful, they are not immune to the pitfalls of floating-point arithmetic. A naive implementation of a function can lead to numerical instability, such as overflow or loss of precision, causing training to fail. This practice challenges you to implement a custom, numerically stable AD primitive for the `softplus` function, a common component in neural networks [@problem_id:3100433]. By comparing your robust implementation against a naive one, you will learn why and how to build reliable custom operations for your models.", "problem": "You are tasked with constructing a numerically stable reverse-mode Automatic Differentiation (AD) primitive for the function $y=\\mathrm{softplus}(x)$ with $y=\\log(1+e^{x})$ that supplies a custom Vector-Jacobian Product (VJP). The Vector-Jacobian Product (VJP) maps a cotangent (an upstream sensitivity) $v$ to $v$ multiplied by the gradient of $y$ with respect to $x$. Your implementation must be robust in the regime where $|x|\\gg 0$, specifically near $x=100$ and $x=-100$, and must be tested on a broader set of values.\n\nStarting from the mathematical definition of derivative and the chain rule, implement two versions of the computation:\n- A baseline \"Naive AD\" version that evaluates $y=\\log(1+e^{x})$ and its gradient using direct application of rules to this exact expression without any algebraic rearrangements for numerical stability.\n- A custom VJP version that returns $y$ and a callable that, given $v$, returns the VJP $v\\cdot \\frac{dy}{dx}$, with both the primal $y$ and the gradient $\\frac{dy}{dx}$ computed in a numerically stable manner for large positive and large negative $x$.\n\nDesign a test suite with the following input scalar values $x$: $100$, $-100$, $0$, $10000$, $-10000$. For each test case, compute:\n1. Whether the Naive AD primal and the custom VJP primal agree within an absolute tolerance of $10^{-12}$ and are both finite.\n2. Whether the Naive AD gradient and the custom VJP gradient agree within an absolute tolerance of $10^{-12}$ and are both finite.\n\nThe final output of your program must be a single line containing a comma-separated list enclosed in square brackets, consisting of booleans in the order\n$[\\text{agree\\_y}(x_1),\\text{agree\\_grad}(x_1),\\text{agree\\_y}(x_2),\\text{agree\\_grad}(x_2),\\dots]$,\nwhere $\\text{agree\\_y}(x)$ is true if and only if both primal values are finite and their absolute difference is less than or equal to $10^{-12}$, and $\\text{agree\\_grad}(x)$ is defined analogously for gradients. No physical units are involved and all angles, if any, are measured in radians, but this problem does not use angles.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\dots]$).", "solution": "The problem requires the implementation and comparison of two reverse-mode Automatic Differentiation (AD) primitives for the softplus function, $y(x) = \\log(1+e^x)$. One implementation is a \"naive\" direct translation of the formula, and the other is a \"custom VJP\" designed for numerical stability.\n\nFirst, we define the function and its derivative. The softplus function is given by:\n$$y(x) = \\log(1+e^x)$$\nUsing the chain rule, its derivative with respect to $x$ is:\n$$\\frac{dy}{dx} = \\frac{1}{1+e^x} \\cdot \\frac{d}{dx}(1+e^x) = \\frac{e^x}{1+e^x}$$\nThis derivative is the logistic sigmoid function, often denoted as $\\sigma(x)$.\n\nA reverse-mode AD primitive for a function $y=f(x)$ generally consists of two parts: a forward pass that computes the primal output value $y$, and a function that provides the Vector-Jacobian Product (VJP). The VJP maps a vector from the cotangent space of the output, $\\bar{y}$, to a vector in the cotangent space of the input, $\\bar{x}$, via the relation $\\bar{x} = J_f^T \\bar{y}$, where $J_f$ is the Jacobian matrix of $f$. For a scalar function $y=f(x)$, the Jacobian is simply the scalar derivative $\\frac{dy}{dx}$. The cotangent \"vector\" $\\bar{y}$ is a scalar, denoted as $v$ in the problem statement. Therefore, the VJP is the scalar product $v \\cdot \\frac{dy}{dx}$. A standard method to recover the gradient itself is to evaluate the VJP with $v=1$.\n\nWe will now analyze the numerical stability of the naive implementation and derive a stable alternative.\n\n**Naive AD Implementation**\nA direct implementation uses the formulas as written:\n- Primal: $y = \\log(1+e^x)$\n- Gradient: $\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$\n\nLet us analyze the behavior of these expressions in floating-point arithmetic for extreme values of $x$.\n1.  **Large Positive $x$ (e.g., $x \\to \\infty$):** The term $e^x$ grows exponentially. For sufficiently large $x$ (e.g., $x > 709.78$ in standard $64$-bit floats), $e^x$ overflows, yielding an infinite value (`inf`).\n    - Naive Primal: $y = \\log(1 + \\text{inf}) = \\log(\\text{inf}) = \\text{inf}$. This is numerically incorrect. Asymptotically, for large $x$, $y(x) \\approx \\log(e^x) = x$.\n    - Naive Gradient: $\\frac{dy}{dx} = \\frac{\\text{inf}}{1+\\text{inf}} = \\frac{\\text{inf}}{\\text{inf}}$, which evaluates to `NaN` (Not a Number). This is also incorrect. Asymptotically, the gradient should approach $1$.\n\n2.  **Large Negative $x$ (e.g., $x \\to -\\infty$):** The term $e^x$ underflows to $0$.\n    - Naive Primal: $y = \\log(1+0) = 0$. This is numerically stable and correct.\n    - Naive Gradient: $\\frac{dy}{dx} = \\frac{0}{1+0} = 0$. This is also a stable and correct evaluation.\n\nThe naive implementation is clearly flawed due to overflow for large positive inputs.\n\n**Custom VJP: A Numerically Stable Implementation**\nTo create a stable implementation, we must algebraically rearrange the expressions to avoid computing $e^x$ for large positive $x$.\n\nFor the primal value $y(x)$, when $x > 0$:\n$$y(x) = \\log(1+e^x) = \\log\\left(e^x \\cdot (e^{-x} + 1)\\right) = \\log(e^x) + \\log(1+e^{-x}) = x + \\log(1+e^{-x})$$\nIn this form, for large positive $x$, we compute $e^{-x}$, which safely underflows to $0$. The expression $x + \\log(1+e^{-x})$ correctly evaluates to approximately $x$, avoiding overflow. For $x \\le 0$, the original expression $y(x) = \\log(1+e^x)$ is stable.\n\nFor the gradient $\\frac{dy}{dx}$, when $x > 0$:\n$$\\frac{dy}{dx} = \\frac{e^x}{1+e^x} = \\frac{e^x \\cdot e^{-x}}{(1+e^x) \\cdot e^{-x}} = \\frac{1}{e^{-x} + 1}$$\nThis form also avoids overflow for large positive $x$ by using $e^{-x}$, and correctly evaluates to approximately $1$. For $x \\le 0$, the original expression $\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$ is stable.\n\nCombining these observations, we can define a piecewise function for stable computation:\n- **Stable Primal $y(x)$**:\n$$\ny_{stable}(x) = \n\\begin{cases} \nx + \\log(1+e^{-x}) & \\text{if } x > 0 \\\\\n\\log(1+e^x) & \\text{if } x \\le 0 \n\\end{cases}\n$$\n\n- **Stable Gradient $\\frac{dy}{dx}(x)$**:\n$$\ng_{stable}(x) = \n\\begin{cases} \n\\frac{1}{1+e^{-x}} & \\text{if } x \\ge 0 \\\\\n\\frac{e^x}{1+e^x} & \\text{if } x < 0 \n\\end{cases}\n$$\nThe split point for the gradient is chosen as $x=0$, where both expressions evaluate to $\\frac{1}{2}$, ensuring continuity. Using $x \\ge 0$ for the first case ensures the point $x=0$ is handled correctly.\n\nThe custom VJP primitive will implement these stable computations. It will compute $y_{stable}$ and $g_{stable}$, then return $y_{stable}$ and a callable function (a closure) that takes $v$ and returns the product $v \\cdot g_{stable}$.\n\nThe test suite will then compare the outputs of the naive and stable primitives for a set of input values, checking for finiteness and agreement within a specified tolerance, thereby demonstrating the failure of the naive approach and the correctness of the stable one.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares naive vs. numerically stable reverse-mode AD\n    primitives for the softplus function.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [100.0, -100.0, 0.0, 10000.0, -10000.0]\n    tolerance = 1e-12\n\n    def naive_ad_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and gradient using a direct,\n        numerically unstable implementation.\n        \"\"\"\n        # Catch numpy warnings about overflow/invalid values during naive computation\n        with np.errstate(over='ignore', invalid='ignore'):\n            # Primal: y = log(1 + exp(x))\n            primal_y = np.log(1.0 + np.exp(x))\n            \n            # Gradient: dy/dx = exp(x) / (1 + exp(x))\n            grad = np.exp(x) / (1.0 + np.exp(x))\n        \n        return primal_y, grad\n\n    def custom_vjp_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and a VJP callable using a\n        numerically stable implementation.\n        \"\"\"\n        # Stable primal computation\n        if x > 0:\n            primal_y = x + np.log(1.0 + np.exp(-x))\n        else:\n            primal_y = np.log(1.0 + np.exp(x))\n        \n        # Stable gradient computation\n        if x >= 0:\n            grad = 1.0 / (1.0 + np.exp(-x))\n        else:\n            grad = np.exp(x) / (1.0 + np.exp(x))\n\n        # The VJP is a function that takes an upstream gradient v\n        # and multiplies it by the local gradient.\n        def vjp_callable(v: float):\n            return v * grad\n            \n        return primal_y, vjp_callable\n\n    results = []\n    for x_val in test_cases:\n        # Get results from the naive implementation\n        y_naive, grad_naive = naive_ad_primitive(x_val)\n        \n        # Get results from the custom stable implementation\n        # The gradient is recovered by calling the VJP with v=1.0\n        y_custom, vjp_fn = custom_vjp_primitive(x_val)\n        grad_custom = vjp_fn(1.0)\n        \n        # 1. Compare primal values (y)\n        y_are_finite = np.isfinite(y_naive) and np.isfinite(y_custom)\n        if y_are_finite:\n            y_agree = np.abs(y_naive - y_custom) = tolerance\n        else:\n            y_agree = False\n        results.append(y_agree)\n        \n        # 2. Compare gradient values (dy/dx)\n        grad_are_finite = np.isfinite(grad_naive) and np.isfinite(grad_custom)\n        if grad_are_finite:\n            grad_agree = np.abs(grad_naive - grad_custom) = tolerance\n        else:\n            grad_agree = False\n        results.append(grad_agree)\n\n    # Final print statement in the exact required format.\n    # str(bool) gives 'True'/'False', which is the required format.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "3100433"}, {"introduction": "Modern machine learning is built on linear algebra, and many advanced models require differentiating through complex matrix operations. This exercise scales up our understanding of AD from scalar functions to matrix calculus, focusing on the gradient of a loss function involving a `log-determinant` term, which frequently appears in models involving Gaussian distributions or covariance matrices [@problem_id:3100414]. You will derive the gradient from first principles and implement two computational strategies, contrasting a generic but potentially unstable method with a superior approach using Cholesky factorization, reinforcing the importance of numerical stability in high-dimensional settings.", "problem": "You are given the scalar loss on a symmetric positive definite matrix $\\,\\Sigma \\in \\mathbb{R}^{n \\times n}\\,$ defined by\n$$\n\\mathcal{L}(\\Sigma;\\alpha) \\;=\\; \\tfrac{1}{2}\\,\\|\\Sigma\\|_F^2 \\;-\\; \\alpha\\,\\log\\det(\\Sigma),\n$$\nwhere $\\,\\alpha \\in \\mathbb{R}_{0}\\,$ is a positive scalar and $\\,\\|\\cdot\\|_F\\,$ is the Frobenius norm. The matrix $\\,\\Sigma\\,$ is guaranteed to be symmetric positive definite so that the Cholesky factorization exists. Your tasks are:\n- Derive the gradient $\\,\\nabla_{\\Sigma}\\mathcal{L}(\\Sigma;\\alpha)\\,$ from first principles using the definition of the matrix differential and standard identities of matrix calculus and linear algebra (you may rely on well-tested facts such as Jacobi’s identity for the determinant and basic properties of the trace and Cholesky factorization, but do not assume any pre-derived gradient formulas for $\\,\\log\\det(\\Sigma)\\,$).\n- Design two algorithmic backward passes to compute $\\,\\nabla_{\\Sigma}\\mathcal{L}(\\Sigma;\\alpha)\\,$:\n  1. A numerically stable backward using the Cholesky factorization of $\\,\\Sigma\\,$. This should avoid forming any explicit matrix inverse by instead solving triangular systems derived from the Cholesky factors.\n  2. A “generic-determinant” automatic differentiation simulation that treats $\\,\\log\\det(\\Sigma)\\,$ as composed of the determinant and a scalar logarithm and computes the gradient via the conventional matrix inverse pathway corresponding to a generic automatic differentiation rule for determinants.\n- Compare these two gradients numerically and verify the correctness of the stable Cholesky-based gradient with a finite-difference directional derivative check.\n\nFundamental base for derivation. You must start from the definition of the differential $\\,\\mathrm{d}f(X)\\,$, the Frobenius inner product $\\,\\langle A,B\\rangle = \\operatorname{tr}(A^\\top B)\\,$, and well-tested identities, including but not limited to the product rule for determinants (Jacobi’s identity), properties of the trace, and properties of the Cholesky factorization $\\,\\Sigma = L L^\\top\\,$ for $\\,\\Sigma \\succ 0\\,$. Do not assume any ready-made gradient expression for $\\,\\log\\det(\\Sigma)\\,$; derive it.\n\nAlgorithmic requirements. Implement both backward passes as described:\n- The stable Cholesky-based backward must compute the action equivalent to $\\,\\Sigma^{-1}\\,$ using a Cholesky factorization and triangular solves (no explicit inverse).\n- The “generic-determinant” backward must emulate an automatic differentiation path that would arise from $\\,\\log(\\det(\\cdot))\\,$ via the determinant rule and a chain rule, which, for symmetric positive definite inputs, leads to the conventional use of a matrix inverse.\n\nNumerical verification. For each test case below:\n- Compute the relative Frobenius-norm difference between the two gradients.\n- Perform a symmetric finite-difference directional derivative check in a random symmetric direction $\\,H\\,$ with $\\,\\|H\\|_F = 1\\,$:\n  - Define $\\,\\phi(t) = \\mathcal{L}(\\Sigma + t H; \\alpha)\\,$ and approximate the directional derivative at $\\,t=0\\,$ as $\\,\\frac{\\phi(t)-\\phi(-t)}{2t}\\,$ for a sufficiently small scalar step $\\,t\\,$ chosen to preserve positive definiteness.\n  - Compare this approximation to the analytic directional derivative $\\,\\langle \\nabla_{\\Sigma}\\mathcal{L}(\\Sigma;\\alpha), H \\rangle\\,$ and report a relative error.\n\nTest suite. Use the following parameterized symmetric positive definite matrices constructed as $\\,\\Sigma = A^\\top A + \\varepsilon I\\,$, where $\\,A\\,$ has independent standard normal entries generated by the listed random seed, and $\\,I\\,$ is the identity. For each case, also generate an independent random direction $\\,H\\,$ from the same seed plus $\\,10\\,$, symmetrize it as $\\,H \\leftarrow \\tfrac{1}{2}(H + H^\\top)\\,$, and scale it to satisfy $\\,\\|H\\|_F = 1\\,$.\n- Case 1: $\\,n=2\\,$, seed $\\,0\\,$, $\\,\\varepsilon = 10^{-2}\\,$, $\\,\\alpha = 0.7\\,$.\n- Case 2: $\\,n=3\\,$, seed $\\,1\\,$, $\\,\\varepsilon = 10^{-6}\\,$, $\\,\\alpha = 1.5\\,$.\n- Case 3: $\\,n=5\\,$, seed $\\,2\\,$, $\\,\\varepsilon = 10^{-3}\\,$, $\\,\\alpha = 3.0\\,$.\n- Case 4: $\\,n=8\\,$, seed $\\,3\\,$, $\\,\\varepsilon = 10^{-6}\\,$, $\\,\\alpha = 0.1\\,$.\n\nAngle units and physical units. There are no physical quantities or angles in this problem, so no unit specification is required.\n\nFinal output format. Your program must produce a single line containing a comma-separated list enclosed in square brackets with eight floating-point numbers in the following order:\n$$\n[\\mathrm{err\\_grad}_1,\\mathrm{err\\_fd}_1,\\mathrm{err\\_grad}_2,\\mathrm{err\\_fd}_2,\\mathrm{err\\_grad}_3,\\mathrm{err\\_fd}_3,\\mathrm{err\\_grad}_4,\\mathrm{err\\_fd}_4],\n$$\nwhere $\\,\\mathrm{err\\_grad}_k\\,$ is the relative Frobenius-norm difference between the two gradients for case $\\,k\\,$, and $\\,\\mathrm{err\\_fd}_k\\,$ is the relative error of the finite-difference directional derivative check for case $\\,k\\,$. All numbers must be printed in full precision as produced by the language’s default string conversion for floating-point numbers.", "solution": "The problem asks for the derivation and implementation of the gradient of the scalar loss function $\\mathcal{L}(\\Sigma;\\alpha)$ with respect to a symmetric positive definite matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$. The loss function is given by:\n$$\n\\mathcal{L}(\\Sigma;\\alpha) \\;=\\; \\tfrac{1}{2}\\,\\|\\Sigma\\|_F^2 \\;-\\; \\alpha\\,\\log\\det(\\Sigma)\n$$\nwhere $\\alpha \\in \\mathbb{R}_{0}$ is a positive scalar, $\\|\\cdot\\|_F$ is the Frobenius norm, and $\\det(\\cdot)$ is the determinant. The matrix $\\Sigma$ is guaranteed to be symmetric positive definite ($\\Sigma \\succ 0$), which ensures that $\\det(\\Sigma)  0$.\n\nOur derivation will be based on first principles, starting from the definition of the matrix differential. The gradient $\\nabla_{\\Sigma}\\mathcal{L}$ is the unique symmetric matrix that satisfies the relation $\\mathrm{d}\\mathcal{L} = \\operatorname{tr}((\\nabla_{\\Sigma}\\mathcal{L})^\\top \\mathrm{d}\\Sigma) = \\langle \\nabla_{\\Sigma}\\mathcal{L}, \\mathrm{d}\\Sigma \\rangle$ for any symmetric perturbation $\\mathrm{d}\\Sigma$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the Frobenius inner product.\n\nThe loss function consists of two terms. We will find the differential of each term separately.\n\n**1. Differential of the Frobenius norm term**\n\nLet the first term be $f_1(\\Sigma) = \\tfrac{1}{2}\\|\\Sigma\\|_F^2$. The Frobenius norm is defined as $\\|\\Sigma\\|_F = \\sqrt{\\operatorname{tr}(\\Sigma^\\top \\Sigma)}$. Thus, the term is $f_1(\\Sigma) = \\tfrac{1}{2}\\operatorname{tr}(\\Sigma^\\top \\Sigma)$. Since $\\Sigma$ is symmetric, $\\Sigma^\\top = \\Sigma$, and we can write $f_1(\\Sigma) = \\tfrac{1}{2}\\operatorname{tr}(\\Sigma^2)$.\n\nTo find its differential, we apply the product rule for differentials:\n$$\n\\mathrm{d}f_1 = \\tfrac{1}{2}\\,\\mathrm{d}\\operatorname{tr}(\\Sigma^2) = \\tfrac{1}{2}\\operatorname{tr}(\\mathrm{d}(\\Sigma^2)) = \\tfrac{1}{2}\\operatorname{tr}((\\mathrm{d}\\Sigma)\\Sigma + \\Sigma(\\mathrm{d}\\Sigma))\n$$\nUsing the linearity and cyclic property of the trace operator ($\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$):\n$$\n\\mathrm{d}f_1 = \\tfrac{1}{2}(\\operatorname{tr}((\\mathrm{d}\\Sigma)\\Sigma) + \\operatorname{tr}(\\Sigma(\\mathrm{d}\\Sigma))) = \\tfrac{1}{2}(\\operatorname{tr}(\\Sigma(\\mathrm{d}\\Sigma)) + \\operatorname{tr}(\\Sigma(\\mathrm{d}\\Sigma))) = \\operatorname{tr}(\\Sigma\\,\\mathrm{d}\\Sigma)\n$$\nSince $\\Sigma$ is symmetric, this is $\\operatorname{tr}(\\Sigma^\\top \\mathrm{d}\\Sigma)$. This is in the form $\\operatorname{tr}(G_1^\\top \\mathrm{d}\\Sigma)$ where $G_1 = \\Sigma$. Therefore, the gradient of the first term is $\\nabla_{\\Sigma}f_1 = \\Sigma$.\n\n**2. Differential of the log-determinant term**\n\nLet the second term be $f_2(\\Sigma) = -\\alpha\\,\\log\\det(\\Sigma)$. Using the chain rule for differentials, where $\\mathrm{d}(\\log u) = u^{-1}\\mathrm{d}u$, we have:\n$$\n\\mathrm{d}f_2 = -\\alpha\\,\\frac{1}{\\det(\\Sigma)}\\,\\mathrm{d}(\\det(\\Sigma))\n$$\nFor the differential of the determinant, we use Jacobi's identity, which states that for an invertible matrix $\\Sigma$, $\\mathrm{d}\\det(\\Sigma) = \\det(\\Sigma)\\operatorname{tr}(\\Sigma^{-1}\\mathrm{d}\\Sigma)$. This identity is a standard result in matrix calculus. Substituting this into our expression for $\\mathrm{d}f_2$:\n$$\n\\mathrm{d}f_2 = -\\alpha\\,\\frac{1}{\\det(\\Sigma)}\\,\\left(\\det(\\Sigma)\\operatorname{tr}(\\Sigma^{-1}\\mathrm{d}\\Sigma)\\right) = -\\alpha\\,\\operatorname{tr}(\\Sigma^{-1}\\mathrm{d}\\Sigma)\n$$\nSince $\\Sigma$ is symmetric, its inverse $\\Sigma^{-1}$ is also symmetric. Thus, we can write this as $\\operatorname{tr}((-\\alpha\\,\\Sigma^{-1})^\\top\\mathrm{d}\\Sigma)$. This is in the form $\\operatorname{tr}(G_2^\\top \\mathrm{d}\\Sigma)$, where $G_2 = -\\alpha\\,\\Sigma^{-1}$. Therefore, the gradient of the second term is $\\nabla_{\\Sigma}f_2 = -\\alpha\\,\\Sigma^{-1}$.\n\n**3. Total Gradient**\n\nCombining the differentials of the two terms, we get the total differential of the loss function:\n$$\n\\mathrm{d}\\mathcal{L} = \\mathrm{d}f_1 + \\mathrm{d}f_2 = \\operatorname{tr}(\\Sigma\\,\\mathrm{d}\\Sigma) - \\alpha\\,\\operatorname{tr}(\\Sigma^{-1}\\mathrm{d}\\Sigma) = \\operatorname{tr}((\\Sigma-\\alpha\\,\\Sigma^{-1})\\,\\mathrm{d}\\Sigma)\n$$\nThe matrix $G = \\Sigma - \\alpha\\,\\Sigma^{-1}$ is symmetric because both $\\Sigma$ and $\\Sigma^{-1}$ are symmetric. Thus, $G^\\top = G$. The differential can be written as $\\mathrm{d}\\mathcal{L} = \\operatorname{tr}(G^\\top \\mathrm{d}\\Sigma)$. By definition, the gradient of $\\mathcal{L}$ with respect to the symmetric matrix $\\Sigma$ is:\n$$\n\\nabla_{\\Sigma}\\mathcal{L}(\\Sigma;\\alpha) = \\Sigma - \\alpha\\,\\Sigma^{-1}\n$$\n\n**4. Algorithmic Backward Passes**\n\nThe core of the gradient computation is evaluating the term $\\Sigma^{-1}$. We implement two approaches.\n\n**Algorithm 1 (Stable Cholesky-based backward):** This method avoids explicit computation of the matrix inverse, which is prone to numerical instability for ill-conditioned matrices. Since $\\Sigma$ is symmetric positive definite, it admits a unique Cholesky factorization $\\Sigma = LL^\\top$, where $L$ is a lower triangular matrix with positive diagonal entries. To compute $X = \\Sigma^{-1}$, we can solve the linear system $\\Sigma X = I$ for $X$, where $I$ is the identity matrix. Substituting the Cholesky factorization gives $LL^\\top X = I$. This can be solved in two stages using triangular solves, which are numerically stable and efficient:\n1.  Solve $LY = I$ for $Y$ using forward substitution. This yields $Y=L^{-1}$.\n2.  Solve $L^\\top X = Y$ for $X$ using backward substitution. This yields $X = (L^\\top)^{-1}Y = (L^\\top)^{-1}L^{-1} = (LL^\\top)^{-1} = \\Sigma^{-1}$.\nThe gradient is then computed as $\\nabla_{\\Sigma}\\mathcal{L} = \\Sigma - \\alpha X$.\n\n**Algorithm 2 (Generic-determinant backward):** This approach simulates the pathway a generic automatic differentiation (AD) system might take. If the AD system has a rule for the matrix inverse but not a specialized rule for the $\\log\\det$ of an SPD matrix, it would compute the gradient via the explicit inverse. This involves directly calculating $\\Sigma^{-1}$ using a general-purpose inversion algorithm (e.g., based on LU decomposition). The gradient is then computed as $\\nabla_{\\Sigma}\\mathcal{L} = \\Sigma - \\alpha\\,\\Sigma^{-1}$. This method can be less accurate and stable than the Cholesky-based approach.\n\n**5. Numerical Verification**\n\nThe correctness of the derived gradient and its implementation is verified through two numerical checks.\n\n**Gradient Comparison:** We compute the relative Frobenius-norm difference between the gradient computed by the stable method ($\\nabla_1$) and the generic method ($\\nabla_2$):\n$$\n\\mathrm{err\\_grad} = \\frac{\\|\\nabla_1 - \\nabla_2\\|_F}{\\|\\nabla_1\\|_F}\n$$\nThis measures the numerical discrepancy between the two computational paths.\n\n**Finite-Difference Check:** We validate the analytical gradient $\\nabla_{\\Sigma}\\mathcal{L}$ (computed via the stable method) by comparing its projection onto a random symmetric direction $H$ (with $\\|H\\|_F=1$) with a finite-difference approximation. The analytical directional derivative is $\\langle \\nabla_{\\Sigma}\\mathcal{L}, H \\rangle = \\operatorname{tr}((\\nabla_{\\Sigma}\\mathcal{L})^\\top H)$. The central difference approximation is given by:\n$$\n\\frac{\\mathcal{L}(\\Sigma+tH;\\alpha) - \\mathcal{L}(\\Sigma-tH;\\alpha)}{2t}\n$$\nfor a small step size $t$ (e.g., $t=10^{-7}$). The relative error between the analytical and numerical directional derivatives is calculated as:\n$$\n\\mathrm{err\\_fd} = \\frac{|\\langle \\nabla_{\\Sigma}\\mathcal{L}, H \\rangle - \\text{FD approx}|}{|\\langle \\nabla_{\\Sigma}\\mathcal{L}, H \\rangle|}\n$$\nThis check provides strong evidence for the correctness of our derived gradient formula.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef solve():\n    \"\"\"\n    Computes and verifies gradients of a loss function on SPD matrices.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, seed, epsilon, alpha)\n        (2, 0, 1e-2, 0.7),\n        (3, 1, 1e-6, 1.5),\n        (5, 2, 1e-3, 3.0),\n        (8, 3, 1e-6, 0.1),\n    ]\n\n    results = []\n    # Small step size for finite difference calculation\n    t = 1e-7\n\n    def loss_fn(S, a):\n        \"\"\"Computes the loss L(S; a) = 0.5 * ||S||_F^2 - a * log(det(S)).\"\"\"\n        # np.linalg.slogdet is more stable than log(det(...))\n        _sign, logdet = np.linalg.slogdet(S)\n        # For an SPD matrix, sign is always 1.\n        if _sign = 0:\n            return np.inf\n        \n        # Frobenius norm squared can be computed as sum of squares of all elements\n        norm_sq = np.sum(S**2)\n        return 0.5 * norm_sq - a * logdet\n\n    for n, seed, epsilon, alpha in test_cases:\n        # 1. Generate the symmetric positive definite matrix Sigma\n        np.random.seed(seed)\n        A = np.random.randn(n, n)\n        # Sigma = A.T @ A + epsilon * I ensures Sigma is SPD\n        Sigma = A.T @ A + epsilon * np.identity(n)\n\n        # 2. Generate a random symmetric direction matrix H with unit Frobenius norm\n        np.random.seed(seed + 10)\n        H_rand = np.random.randn(n, n)\n        # Symmetrize H\n        H = 0.5 * (H_rand + H_rand.T)\n        # Normalize H\n        H = H / np.linalg.norm(H, 'fro')\n\n        # 3. Compute the gradient using the stable Cholesky-based method\n        # This is Algorithm 1.\n        # Compute Sigma^{-1} via Cholesky factorization: Sigma = L L.T\n        # Solve Sigma X = I, which becomes L L.T X = I\n        c, low = cho_factor(Sigma, lower=True)\n        Sigma_inv_cho = cho_solve((c, low), np.identity(n))\n        grad_cho = Sigma - alpha * Sigma_inv_cho\n        \n        # 4. Compute the gradient using the generic inverse method\n        # This is Algorithm 2, simulating a generic AD path.\n        Sigma_inv_np = np.linalg.inv(Sigma)\n        grad_inv = Sigma - alpha * Sigma_inv_np\n        \n        # 5. Compute the relative Frobenius-norm difference between the two gradients\n        grad_diff_norm = np.linalg.norm(grad_cho - grad_inv, 'fro')\n        grad_cho_norm = np.linalg.norm(grad_cho, 'fro')\n        err_grad = grad_diff_norm / grad_cho_norm\n        results.append(err_grad)\n\n        # 6. Perform the finite-difference directional derivative check\n        # Analytical directional derivative: grad, H = tr(grad.T @ H)\n        # Since both grad_cho and H are symmetric, this simplifies to sum(grad_cho * H)\n        analytic_dd = np.sum(grad_cho * H)\n\n        # Numerical approximation using central differences\n        loss_p = loss_fn(Sigma + t * H, alpha)\n        loss_m = loss_fn(Sigma - t * H, alpha)\n        numeric_dd = (loss_p - loss_m) / (2.0 * t)\n        \n        # Relative error of the finite difference check\n        err_fd = np.abs(analytic_dd - numeric_dd) / np.abs(analytic_dd)\n        results.append(err_fd)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3100414"}]}