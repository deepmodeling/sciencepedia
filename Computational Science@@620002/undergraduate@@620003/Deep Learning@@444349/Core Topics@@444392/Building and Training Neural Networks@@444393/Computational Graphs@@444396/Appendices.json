{"hands_on_practices": [{"introduction": "To truly understand backpropagation, it's helpful to move beyond seeing it as a black box and instead view it as a direct application of the multivariable chain rule on a graph. This exercise demystifies the process by asking you to manually trace every path through which an input variable influences the final output. By calculating the contribution of each path and summing them, you will see how the overall gradient is an aggregation of all these influences, providing a foundational intuition for how automatic differentiation works [@problem_id:3108040].", "problem": "Consider a scalar input $x$ measured in radians flowing through a computational graph that is a Directed Acyclic Graph (DAG). The nodes compute the following deterministic transformations:\n$$a = x^{2}, \\quad b = \\sin(x), \\quad c = a \\cdot b, \\quad d = \\exp(a), \\quad y = c + d, \\quad L = \\ln\\!\\big(1 + y\\big).$$\nAssume all functions are differentiable for the values of $x$ considered. Using only the chain rule and the definition of a computational graph (where gradients propagate along edges via local partial derivatives), examine the linearized gradient propagation on this DAG from $L$ back to $x$.\n\nTasks:\n- Enumerate the distinct simple directed paths from $x$ to $L$ in the linearized gradient graph obtained by placing the local derivative factor on each edge. Count the number $N$ of such paths.\n- Derive the exact symbolic expression for the total gradient $\\frac{\\partial L}{\\partial x}$ by summing the contributions of all paths, where each path contributes the product of its local derivative factors.\n- Present the final simplified expression for $\\frac{\\partial L}{\\partial x}$ in terms of $x$.\n\nProvide the final answer as a row matrix containing $N$ and the exact symbolic expression for $\\frac{\\partial L}{\\partial x}$. No rounding is required. Angles are in radians, and no physical units are involved. Do not use any shortcut formulas beyond the chain rule and the basic definitions of the computational graph and backpropagation (backward propagation of gradients).", "solution": "The problem requires us to compute the total derivative $\\frac{\\partial L}{\\partial x}$ by conceptualizing the computation as a graph and applying the chain rule along all paths from the input $x$ to the output $L$.\n\n**Task 1: Path Enumeration**\n\nFirst, we identify the structure of the computational graph based on the given dependencies. The dependencies define the directed edges of the graph, where an edge $(u, v)$ means $v$ is computed from $u$.\n- $x \\rightarrow a$\n- $x \\rightarrow b$\n- $a \\rightarrow c$\n- $b \\rightarrow c$\n- $a \\rightarrow d$\n- $c \\rightarrow y$\n- $d \\rightarrow y$\n- $y \\rightarrow L$\n\nWe must find all distinct simple directed paths from the input node $x$ to the final output node $L$. By tracing the connections from $x$ to $L$:\n1. A path can go from $x$ to $a$. From $a$, it can proceed to $c$ or $d$.\n   - If it proceeds to $c$, the path must then go to $y$ and finally to $L$. This gives the path: $x \\rightarrow a \\rightarrow c \\rightarrow y \\rightarrow L$.\n   - If it proceeds to $d$, the path must then go to $y$ and finally to $L$. This gives the path: $x \\rightarrow a \\rightarrow d \\rightarrow y \\rightarrow L$.\n2. A path can go from $x$ to $b$. From $b$, it can only proceed to $c$. From $c$, it must go to $y$ and then to $L$. This gives the path: $x \\rightarrow b \\rightarrow c \\rightarrow y \\rightarrow L$.\n\nThese are the three unique simple directed paths from $x$ to $L$.\nTherefore, the number of such paths is $N = 3$.\n\n**Task 2: Derivation of the Total Gradient**\n\nThe total derivative $\\frac{\\partial L}{\\partial x}$ is the sum of the contributions from each of these $3$ paths. The contribution of a single path is the product of the local partial derivatives along its edges.\n\nFirst, we compute all the necessary local partial derivatives:\n- $\\frac{\\partial L}{\\partial y} = \\frac{\\partial}{\\partial y} \\ln(1 + y) = \\frac{1}{1 + y}$\n- $\\frac{\\partial y}{\\partial c} = \\frac{\\partial}{\\partial c} (c + d) = 1$\n- $\\frac{\\partial y}{\\partial d} = \\frac{\\partial}{\\partial d} (c + d) = 1$\n- $\\frac{\\partial c}{\\partial a} = \\frac{\\partial}{\\partial a} (a \\cdot b) = b$\n- $\\frac{\\partial c}{\\partial b} = \\frac{\\partial}{\\partial b} (a \\cdot b) = a$\n- $\\frac{\\partial d}{\\partial a} = \\frac{\\partial}{\\partial a} \\exp(a) = \\exp(a)$\n- $\\frac{\\partial a}{\\partial x} = \\frac{\\partial}{\\partial x} (x^{2}) = 2x$\n- $\\frac{\\partial b}{\\partial x} = \\frac{\\partial}{\\partial x} (\\sin(x)) = \\cos(x)$\n\nNext, we calculate the contribution of each path by multiplying the derivatives along it:\n- **Path 1 contribution ($\\mathcal{P}_1$):** $x \\rightarrow a \\rightarrow c \\rightarrow y \\rightarrow L$\n$$ \\mathcal{P}_1 = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial c} \\frac{\\partial c}{\\partial a} \\frac{\\partial a}{\\partial x} = \\left(\\frac{1}{1 + y}\\right) \\cdot (1) \\cdot (b) \\cdot (2x) = \\frac{2xb}{1 + y} $$\n- **Path 2 contribution ($\\mathcal{P}_2$):** $x \\rightarrow b \\rightarrow c \\rightarrow y \\rightarrow L$\n$$ \\mathcal{P}_2 = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial x} = \\left(\\frac{1}{1 + y}\\right) \\cdot (1) \\cdot (a) \\cdot (\\cos(x)) = \\frac{a \\cos(x)}{1 + y} $$\n- **Path 3 contribution ($\\mathcal{P}_3$):** $x \\rightarrow a \\rightarrow d \\rightarrow y \\rightarrow L$\n$$ \\mathcal{P}_3 = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial d} \\frac{\\partial d}{\\partial a} \\frac{\\partial a}{\\partial x} = \\left(\\frac{1}{1 + y}\\right) \\cdot (1) \\cdot (\\exp(a)) \\cdot (2x) = \\frac{2x \\exp(a)}{1 + y} $$\n\nThe total derivative is the sum of these path contributions:\n$$ \\frac{\\partial L}{\\partial x} = \\mathcal{P}_1 + \\mathcal{P}_2 + \\mathcal{P}_3 = \\frac{2xb}{1 + y} + \\frac{a \\cos(x)}{1 + y} + \\frac{2x \\exp(a)}{1 + y} $$\n$$ \\frac{\\partial L}{\\partial x} = \\frac{2xb + a \\cos(x) + 2x \\exp(a)}{1 + y} $$\n\n**Task 3: Final Simplified Expression**\n\nTo obtain the final expression in terms of $x$, we substitute the definitions of the intermediate variables ($a, b, y$) into the expression for $\\frac{\\partial L}{\\partial x}$.\n- $a = x^{2}$\n- $b = \\sin(x)$\n- $y = c + d = (a \\cdot b) + \\exp(a) = x^{2}\\sin(x) + \\exp(x^{2})$\n\nSubstituting these into the numerator:\n$$ 2xb + a \\cos(x) + 2x \\exp(a) = 2x(\\sin(x)) + (x^{2})\\cos(x) + 2x \\exp(x^{2}) $$\nSubstituting into the denominator:\n$$ 1 + y = 1 + x^{2}\\sin(x) + \\exp(x^{2}) $$\n\nCombining these yields the final symbolic expression for the total gradient:\n$$ \\frac{\\partial L}{\\partial x} = \\frac{2x \\sin(x) + x^{2} \\cos(x) + 2x \\exp(x^{2})}{1 + x^{2} \\sin(x) + \\exp(x^{2})} $$\nThis expression cannot be further simplified in a meaningful way.\n\nThe two requested results are $N=3$ and the expression for $\\frac{\\partial L}{\\partial x}$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 3 & \\frac{2x \\sin(x) + x^{2} \\cos(x) + 2x \\exp(x^{2})}{1 + x^{2} \\sin(x) + \\exp(x^{2})} \\end{pmatrix} } $$", "id": "3108040"}, {"introduction": "After grasping the theoretical underpinnings of gradient flow, the next logical step is to implement it. This practice guides you through building a miniature reverse-mode Automatic Differentiation (AD) engine, the core technology behind modern deep learning frameworks. You will also implement gradient checking, a crucial debugging technique that validates your analytical gradients against numerical approximations, ensuring your engine is not just built, but built correctly [@problem_id:3107983].", "problem": "You are to implement a minimal reverse-mode Automatic Differentiation (AD) engine using computational graphs and use it to validate gradients by comparing analytic derivatives against numerical central differences. The fundamental base to use is the definition of a computational graph and the chain rule from differential calculus. Specifically, inject a known function node $f(x)=x^2$ into a scalar model graph and verify gradient propagation is correct at neighboring nodes.\n\nConstruct a scalar computational graph with input $x$ and constants $w$ and $c$ as follows, using standard mathematical operations and functions: $f(x)=x^2$, $u(x)=\\sin(f(x))$, $v(x)=w\\cdot f(x)+c$, and $y(x)=u(x)+\\exp(v(x))$. The trigonometric function $\\sin$ must be evaluated in radians. Your AD engine must compute analytic derivatives via reverse-mode for the following four node outputs with respect to $x$: $f(x)$, $u(x)$, $v(x)$, and $y(x)$. Independently, compute numerical central-difference approximations for each of these four scalar functions using step size $h=10^{-6}$:\n$$\n\\frac{d}{dx} s(x) \\approx \\frac{s(x+h) - s(x-h)}{2h}.\n$$\n\nCompare analytic versus numerical derivatives using an absolute tolerance $\\tau_{\\mathrm{abs}}=10^{-7}$ and relative tolerance $\\tau_{\\mathrm{rel}}=10^{-7}$. A comparison for a single derivative is considered acceptable if the values are close under these tolerances. Perform this comparison for each of the four node outputs $f(x)$, $u(x)$, $v(x)$, and $y(x)$ at each test value of $x$ in the test suite below.\n\nUse the following fixed constants throughout: $w=10^{-6}$ and $c=0.7$. Evaluate the program on the following test suite of input values: $x\\in\\{-3.0,\\,-10^{-8},\\,0.0,\\,0.5,\\,1000.0\\}$.\n\nYour program must be a complete, runnable program that:\n- Builds the described computational graph.\n- Implements reverse-mode AD on this graph to compute analytic derivatives with respect to $x$ for $f(x)$, $u(x)$, $v(x)$, and $y(x)$.\n- Computes numerical central-difference derivatives for the same functions.\n- Compares analytic versus numerical derivatives using the stated tolerances and produces a boolean for each comparison at each $x$.\n\nFinal output format:\n- The program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, where each inner list corresponds to one test input $x$ in the given order and contains four booleans in the order $\\big[f,u,v,y\\big]$ indicating whether the gradients matched within tolerance. For example, the format should look like $[[\\text{True},\\text{True},\\text{True},\\text{True}],[\\text{True},\\text{False},\\text{True},\\text{True}],\\dots]$ with no spaces.\n- There are no physical units involved in this problem, and angles must be in radians as stated above.\n\nNo user input is allowed; the program must run as-is and print exactly one line in the specified format.", "solution": "The task is to implement a reverse-mode Automatic Differentiation (AD) engine for a scalar computational graph and validate its results against numerical differentiation. The solution proceeds in several steps:\n\n1.  **Computational Graph Representation**: The core of the solution is a `Var` class, which represents a node in the computational graph. Each `Var` object stores a scalar `value` and its `grad` (gradient), which is the derivative of the final output with respect to this node's value. The graph is constructed dynamically by overloading standard Python operators (`+`, `*`, `**`) and defining methods for mathematical functions (`sin`, `exp`). When an operation is performed on `Var` objects, a new `Var` object is created, storing the result of the operation in its `value`. Crucially, this new node also retains references to its parent nodes and a `_backward` function that knows how to propagate its gradient back to its parents according to the chain rule.\n\n2.  **Forward Pass**: For a given input value $x$, we instantiate the leaf nodes of the graph, $x$, $w$, and $c$, as `Var` objects.\n    -   $x_{val} \\in \\{-3.0, -10^{-8}, 0.0, 0.5, 1000.0\\}$\n    -   $w = 10^{-6}$\n    -   $c = 0.7$\n    The graph is then built by applying the specified sequence of operations:\n    -   $f = x^2$\n    -   $u = \\sin(f)$\n    -   $v = w \\cdot f + c$\n    -   $y = u + \\exp(v)$\n    The creation of each node computes its `value`, effectively performing a forward pass through the graph.\n\n3.  **Reverse Pass (Backpropagation)**: Reverse-mode AD computes gradients by propagating them backward from an output node. To find the derivative of a node $s$ with respect to an input $x$, denoted $\\frac{ds}{dx}$, we initiate a backward pass starting from $s$. This is achieved through the following algorithm for each node of interest $s \\in \\{f, u, v, y\\}$:\n    a.  **Topological Sort**: A topological sort of the computational graph is performed, starting from the final node $y$ and traversing backward to all its ancestors. This ensures that when we backpropagate, a node's gradient is fully computed before it's used to compute the gradients of its parents.\n    b.  **Gradient Reset**: The `grad` attribute of all nodes in the graph is reset to $0.0$.\n    c.  **Seeding**: The gradient of the node $s$ (for which we want to compute the derivative) is set to $1.0$. This is the base case, as $\\frac{ds}{ds} = 1$.\n    d.  **Propagation**: We iterate through the nodes in reverse topological order. For each node, its `_backward` method is called. This method uses the node's own `grad` and the `value`s of its parents to update the parents' `grad`s based on the local partial derivatives (chain rule). For example, for a node $z = \\text{op}(a, b)$, the `_backward` call will perform `a.grad += z.grad * \\frac{\\partial z}{\\partial a}` and `b.grad += z.grad * \\frac{\\partial z}{\\partial b}`.\n    e.  **Result**: After the backward pass is complete, the `grad` attribute of the input node `x` will contain the final desired derivative, $\\frac{ds}{dx}$. This process is repeated for $f, u, v,$ and $y$.\n\n4.  **Numerical Differentiation**: For validation, the derivatives are approximated using the central difference formula for each function $s(x) \\in \\{f(x), u(x), v(x), y(x)\\}$:\n    $$\n    \\frac{d}{dx} s(x) \\approx \\frac{s(x+h) - s(x-h)}{2h}\n    $$\n    This is calculated using a small step size $h = 10^{-6}$.\n\n5.  **Comparison**: The analytic gradients from AD are compared against the numerical approximations. A match is confirmed if the absolute difference is within a tolerance defined by an absolute tolerance $\\tau_{\\mathrm{abs}} = 10^{-7}$ and a relative tolerance $\\tau_{\\mathrm{rel}} = 10^{-7}$. The check is:\n    $$\n    |\\text{analytic} - \\text{numerical}| \\le (\\tau_{\\mathrm{abs}} + \\tau_{\\mathrm{rel}} \\cdot |\\text{numerical}|)\n    $$\n    The `numpy.isclose` function implements this logic directly. For each test value of $x$, four boolean results are generated, corresponding to the comparisons for $\\frac{df}{dx}, \\frac{du}{dx}, \\frac{dv}{dx},$ and $\\frac{dy}{dx}$.\n\n6.  **Final Output**: The boolean results are collected into lists, with one inner list for each test value of $x$. These lists are then formatted into a single string as specified by the problem, e.g., `[[True,True,True,True],[...]]`, and printed to standard output.", "answer": "```python\nimport numpy as np\n\nclass Var:\n    \"\"\"\n    A variable class for reverse-mode automatic differentiation.\n    Each Var instance represents a node in the computational graph, holding a scalar\n    value and its gradient with respect to the final output.\n    \"\"\"\n\n    def __init__(self, value, _parents=()):\n        self.value = float(value)\n        self.grad = 0.0\n        # A function to propagate gradient to parents\n        self._backward = lambda: None\n        # The set of parent nodes that created this node\n        self._parents = set(_parents)\n\n    def __repr__(self):\n        return f\"Var(value={self.value}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Var) else Var(other)\n        out = Var(self.value + other.value, (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Var) else Var(other)\n        out = Var(self.value * other.value, (self, other))\n\n        def _backward():\n            self.grad += other.value * out.grad\n            other.grad += self.value * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __pow__(self, p):\n        assert isinstance(p, (int, float)), \"Only supporting scalar powers\"\n        out = Var(self.value ** p, (self,))\n\n        def _backward():\n            self.grad += p * (self.value ** (p - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def sin(self):\n        out = Var(np.sin(self.value), (self,))\n\n        def _backward():\n            self.grad += np.cos(self.value) * out.grad\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        out = Var(np.exp(self.value), (self,))\n\n        def _backward():\n            self.grad += np.exp(self.value) * out.grad\n        out._backward = _backward\n        return out\n\ndef solve():\n    \"\"\"\n    Main function to run the AD engine and validation.\n    \"\"\"\n    # Define fixed constants and test parameters\n    W_VAL = 1e-6\n    C_VAL = 0.7\n    H = 1e-6\n    TAU_ABS = 1e-7\n    TAU_REL = 1e-7\n    TEST_CASES = [-3.0, -1e-8, 0.0, 0.5, 1000.0]\n\n    # Define the scalar functions for numerical differentiation\n    def s_f(x_val): return x_val**2\n    def s_u(x_val): return np.sin(s_f(x_val))\n    def s_v(x_val): return W_VAL * s_f(x_val) + C_VAL\n    def s_y(x_val): return s_u(x_val) + np.exp(s_v(x_val))\n    \n    scalar_funcs = [s_f, s_u, s_v, s_y]\n    \n    all_case_results = []\n    \n    for x_val in TEST_CASES:\n        # ---- Analytic Derivatives via Reverse-Mode AD ----\n\n        # 1. Build the computational graph (forward pass)\n        x = Var(x_val)\n        w = Var(W_VAL)\n        c = Var(C_VAL)\n        \n        f = x**2\n        u = f.sin()\n        v = w * f + c\n        y = u + v.exp()\n        \n        nodes_of_interest = [f, u, v, y]\n\n        # 2. Prepare for backward pass: Get all nodes via topological sort\n        topo_order = []\n        visited = set()\n        def build_topo(node):\n            if node not in visited:\n                visited.add(node)\n                for parent in node._parents:\n                    build_topo(parent)\n                topo_order.append(node)\n        \n        build_topo(y)\n        all_graph_nodes = visited\n\n        analytic_grads = []\n        for node_to_diff in nodes_of_interest:\n            # 3. Reset gradients for all nodes in the graph\n            for node in all_graph_nodes:\n                node.grad = 0.0\n            \n            # 4. Seed the gradient of the node we're differentiating\n            node_to_diff.grad = 1.0\n            \n            # 5. Propagate gradients backward through the sorted graph\n            for node in reversed(topo_order):\n                node._backward()\n            \n            # 6. The gradient is now in the input variable `x`\n            analytic_grads.append(x.grad)\n\n        # ---- Numerical Derivatives via Central Differences ----\n        numerical_grads = []\n        for func in scalar_funcs:\n            grad = (func(x_val + H) - func(x_val - H)) / (2 * H)\n            numerical_grads.append(grad)\n            \n        # ---- Comparison ----\n        current_case_results = []\n        for a_grad, n_grad in zip(analytic_grads, numerical_grads):\n            is_match = np.isclose(a_grad, n_grad, rtol=TAU_REL, atol=TAU_ABS)\n            current_case_results.append(bool(is_match))\n        \n        all_case_results.append(current_case_results)\n\n    # Format the final output string\n    inner_strings = [f\"[{','.join(map(str, res)).replace(' ','')}]\" for res in all_case_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output.replace('True', 'True').replace('False', 'False'))\n\nsolve()\n```", "id": "3107983"}, {"introduction": "In the world of deep learning, mathematical correctness is only half the battle; numerical stability is equally important. This practice explores how the very structure of a computational graph can impact its stability, using the common `softplus` function, $f(x) = \\ln(1+\\exp(x))$, as a case study. By comparing a naive implementation with a rearranged, stabilized version, you will learn why certain graph structures are preferred to prevent issues like overflow and underflow, a vital lesson for building robust and reliable models [@problem_id:3108012].", "problem": "Consider the function $f(x)=\\ln\\!\\big(1+\\exp(x)\\big)$ represented by a computational graph, where a computational graph is a directed acyclic graph whose nodes are primitive operations and whose edges carry intermediate values. Reverse-Mode Automatic Differentiation (AD) is an algorithm that applies the chain rule of calculus on such graphs to propagate sensitivities from outputs back to inputs. You will analyze two graphs for $f(x)$: a naive graph and a numerically stabilized graph. All derivations must start from the chain rule and basic properties of the elementary functions involved.\n\nNaive graph:\n1. $a=\\exp(x)$,\n2. $b=1+a$,\n3. $y=\\ln(b)$.\n\nStabilized graph using a reparameterization that avoids overflow when $x$ is large and positive:\n1. $m=\\max(x,0)$,\n2. $s=-|x|$,\n3. $c=\\exp(s)$,\n4. $d=\\ln(1+c)$,\n5. $y=m+d$.\n\nTasks:\n1. Using Reverse-Mode Automatic Differentiation (AD), derive the symbolic expression for $\\frac{dy}{dx}$ from the naive graph by composing local derivatives via the chain rule. Express your result entirely in terms of $x$.\n2. Using Reverse-Mode Automatic Differentiation (AD), derive the symbolic expression for $\\frac{dy}{dx}$ from the stabilized graph. Because $m=\\max(x,0)$ and $|x|$ are piecewise-defined, carry out the derivation separately for $x\\ge 0$ and $x<0$, and then simplify to a single analytic expression valid for all real $x$.\n3. Briefly explain, by inspecting the local derivatives on each graph, why the stabilized graph mitigates numerical overflow or underflow relative to the naive graph for large $|x|$ without changing the exact gradient.\n4. Provide your final simplified gradient expression for $\\frac{dy}{dx}$ as a single closed-form analytic expression. This final expression is the only item that will be graded. No numerical rounding is required, and no physical units apply.", "solution": "The function under consideration is $f(x) = \\ln(1+\\exp(x))$. We are tasked with deriving its derivative, $\\frac{dy}{dx}$, using two different computational graphs and explaining the numerical benefits of the second graph. In the context of Reverse-Mode Automatic Differentiation (AD), we use the notation $\\bar{v}$ to represent the derivative of the final output $y$ with respect to an intermediate variable $v$, i.e., $\\bar{v} = \\frac{dy}{dv}$. The process starts with $\\bar{y}=1$ and propagates gradients backwards through the graph using the chain rule.\n\n**Task 1: Derivative from the Naive Graph**\n\nThe naive computational graph is defined by the following sequence of operations:\n1. $a = \\exp(x)$\n2. $b = 1 + a$\n3. $y = \\ln(b)$\n\nWe apply Reverse-Mode AD to find $\\frac{dy}{dx} = \\bar{x}$.\n\n1.  The initial gradient is $\\bar{y} = \\frac{dy}{dy} = 1$.\n2.  Propagate the gradient from $y$ to $b$. The local derivative is $\\frac{dy}{db} = \\frac{d}{db}(\\ln(b)) = \\frac{1}{b}$. The adjoint for $b$ is:\n    $$ \\bar{b} = \\bar{y} \\frac{dy}{db} = 1 \\cdot \\frac{1}{b} = \\frac{1}{b} $$\n3.  Propagate the gradient from $b$ to $a$. The local derivative is $\\frac{db}{da} = \\frac{d}{da}(1+a) = 1$. The adjoint for $a$ is:\n    $$ \\bar{a} = \\bar{b} \\frac{db}{da} = \\frac{1}{b} \\cdot 1 = \\frac{1}{b} $$\n4.  Propagate the gradient from $a$ to $x$. The local derivative is $\\frac{da}{dx} = \\frac{d}{dx}(\\exp(x)) = \\exp(x)$. The adjoint for $x$ is:\n    $$ \\bar{x} = \\bar{a} \\frac{da}{dx} = \\frac{1}{b} \\cdot \\exp(x) $$\n5.  To express the final result in terms of $x$, we substitute back the intermediate variables: $b = 1+a = 1+\\exp(x)$.\n    $$ \\frac{dy}{dx} = \\bar{x} = \\frac{\\exp(x)}{1+\\exp(x)} $$\n\n**Task 2: Derivative from the Stabilized Graph**\n\nThe stabilized graph is given by:\n1. $m = \\max(x, 0)$\n2. $s = -|x|$\n3. $c = \\exp(s)$\n4. $d = \\ln(1+c)$\n5. $y = m + d$\n\nFirst, we verify that this formulation is mathematically equivalent to the original function $f(x) = \\ln(1+\\exp(x))$.\nFor $x \\ge 0$: $|x|=x$, so $m=x$ and $s=-x$. Then $y = x + \\ln(1+\\exp(-x)) = \\ln(\\exp(x)) + \\ln(1+\\exp(-x)) = \\ln(\\exp(x)(1+\\exp(-x))) = \\ln(\\exp(x)+1)$.\nFor $x < 0$: $|x|=-x$, so $m=0$ and $s=x$. Then $y = 0 + \\ln(1+\\exp(x)) = \\ln(1+\\exp(x))$.\nThe formulation is indeed correct for all $x \\in \\mathbb{R}$.\n\nWe derive $\\frac{dy}{dx}$ by considering the two cases for the piecewise functions $m(x)$ and $s(x)$. In general, using the chain rule on the graph structure $y=m(x)+d(c(s(x)))$, we have:\n$$ \\frac{dy}{dx} = \\frac{dm}{dx} + \\frac{dd}{dc} \\frac{dc}{ds} \\frac{ds}{dx} $$\nThe local derivatives are:\n$\\frac{dd}{dc} = \\frac{1}{1+c}$\n$\\frac{dc}{ds} = \\exp(s)$\n\nCase 1: $x > 0$\nFor $x>0$, we have $m(x)=x$ and $s(x)=-|x|=-x$. The derivatives with respect to $x$ are:\n$\\frac{dm}{dx} = 1$\n$\\frac{ds}{dx} = -1$\nSubstituting these into the chain rule expression:\n$$ \\frac{dy}{dx} = 1 + \\left(\\frac{1}{1+c}\\right)(\\exp(s))(-1) = 1 - \\frac{\\exp(s)}{1+c} $$\nNow, we express this in terms of $x$. For $x>0$, $s=-x$ and $c=\\exp(-x)$.\n$$ \\frac{dy}{dx} = 1 - \\frac{\\exp(-x)}{1+\\exp(-x)} = \\frac{(1+\\exp(-x)) - \\exp(-x)}{1+\\exp(-x)} = \\frac{1}{1+\\exp(-x)} $$\nTo show this is equivalent to the previous result, we multiply the numerator and denominator by $\\exp(x)$:\n$$ \\frac{dy}{dx} = \\frac{1 \\cdot \\exp(x)}{(1+\\exp(-x))\\exp(x)} = \\frac{\\exp(x)}{\\exp(x)+1} $$\n\nCase 2: $x < 0$\nFor $x<0$, we have $m(x)=0$ and $s(x)=-|x|=-(-x)=x$. The derivatives with respect to $x$ are:\n$\\frac{dm}{dx} = 0$\n$\\frac{ds}{dx} = 1$\nSubstituting these into the chain rule expression:\n$$ \\frac{dy}{dx} = 0 + \\left(\\frac{1}{1+c}\\right)(\\exp(s))(1) = \\frac{\\exp(s)}{1+c} $$\nNow, we express this in terms of $x$. For $x<0$, $s=x$ and $c=\\exp(x)$.\n$$ \\frac{dy}{dx} = \\frac{\\exp(x)}{1+\\exp(x)} $$\n\nBoth cases yield the same analytic expression. The expression $\\frac{\\exp(x)}{1+\\exp(x)}$ is continuous for all $x \\in \\mathbb{R}$. At $x=0$, the value is $\\frac{\\exp(0)}{1+\\exp(0)} = \\frac{1}{2}$, which is the correct derivative. Thus, this single expression is the derivative of $f(x)$ for all real $x$.\n\n**Task 3: Explanation of Numerical Stability**\n\nThe stabilized graph mitigates numerical overflow, which is a critical issue in the naive graph for large positive values of $x$.\n\n- **Naive Graph Analysis**: The first operation is $a = \\exp(x)$. If $x$ is a large positive number (e.g., $x=1000$), $\\exp(x)$ will exceed the maximum representable value for standard floating-point types, causing an overflow error. The computation fails. For the gradient, $\\frac{\\exp(x)}{1+\\exp(x)}$, a naive floating-point evaluation would result in $\\frac{\\infty}{\\infty}$, which evaluates to NaN (Not a Number).\n\n- **Stabilized Graph Analysis**: The stabilized graph is constructed such that the argument of the exponential function, $s = -|x|$, is always non-positive ($s \\le 0$). Consequently, the intermediate value $c = \\exp(s)$ is always in the range $(0, 1]$. This prevents any possibility of overflow in the exponential calculation.\n  - For large positive $x$: The computation proceeds as $y = x + \\ln(1+\\exp(-x))$. The term $\\exp(-x)$ correctly underflows to $0$ without error, leading to $y \\approx x$, which is the correct asymptotic behavior. The gradient is computed from the expression corresponding to the $x>0$ case, $\\frac{dy}{dx} = \\frac{1}{1+\\exp(-x)}$. For large positive $x$, this evaluates to $\\frac{1}{1+0}=1$, which is the correct limit, avoiding the `NaN` result of the naive approach.\n  - For large negative $x$: Both the naive graph and the stabilized graph compute $\\exp(x)$, which underflows to $0$. In this regime, both forms are numerically stable. The primary advantage of the stabilized graph is specifically for large positive $x$.\n\nIn summary, the stabilized graph maintains mathematical equivalence while restructuring the computation to ensure that the argument to the exponential function is never large and positive, thereby avoiding overflow and producing numerically stable results for both the function value and its gradient across the entire domain of $x$.\n\n**Task 4: Final Simplified Gradient Expression**\n\nAs derived in both Task 1 and Task 2, the simplified, closed-form analytic expression for the gradient $\\frac{dy}{dx}$ that is valid for all real $x$ is the sigmoid function.\n$$ \\frac{dy}{dx} = \\frac{\\exp(x)}{1+\\exp(x)} $$\nThis can also be written as $\\frac{1}{1+\\exp(-x)}$, which is the more numerically stable form for computation when $x$ is large and positive. Both expressions are mathematically identical.", "answer": "$$\\boxed{\\frac{\\exp(x)}{1+\\exp(x)}}$$", "id": "3108012"}]}