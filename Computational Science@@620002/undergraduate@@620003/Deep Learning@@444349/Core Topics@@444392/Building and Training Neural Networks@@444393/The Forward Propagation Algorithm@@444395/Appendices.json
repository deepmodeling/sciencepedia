{"hands_on_practices": [{"introduction": "To truly understand forward propagation, we must look beyond the equations and develop an intuition for what a layer is actually doing. This first practice focuses on the Rectified Linear Unit (ReLU), the most common activation function. By constructing specific inputs and weights, you will explore how a single layer, defined by $a = \\mathrm{ReLU}(W x + b)$, acts as a geometric partitioner, dividing the input space into distinct regions of activation [@problem_id:3185431]. This exercise connects the algebra of matrix multiplication with the geometric interpretation of a neural network layer, a crucial step in building a deeper understanding of how models learn to represent complex functions.", "problem": "Consider a single-layer feed-forward network with Rectified Linear Unit (ReLU) activation, where the forward pass is defined by $a = \\mathrm{ReLU}(W x + b)$ applied elementwise. The Rectified Linear Unit (ReLU) function is defined by $\\mathrm{ReLU}(z) = \\max(0, z)$. Let the input dimension be $n = 2$ and the number of units be $m = 4$. Throughout, denote the rows of $W$ by $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$.\n\nYou will analyze how to force exactly $k = 2$ nonzero activations with carefully chosen $x$, $W$, and $b$, and then reason from first principles about how the induced partition of $\\mathbb{R}^2$ into regions depends on $W$ and $b$.\n\nSelect all options that are correct.\n\nA. Let $W$ have rows $w_1 = (1, 0)$, $w_2 = (-1, 0)$, $w_3 = (0, 1)$, $w_4 = (0, -1)$ and let $b = \\mathbf{0} \\in \\mathbb{R}^4$. For $x = (1, 1)$, the forward pass yields exactly $k = 2$ nonzero activations. In this configuration, the induced region boundaries are the hyperplanes $x_1 = 0$ and $x_2 = 0$.\n\nB. With the same $W$ and $b = \\mathbf{0}$ as in option A, choosing $x = (0, 0)$ yields exactly $k = 2$ nonzero activations.\n\nC. With the same $W$ as in option A, choose $b = (0.5, 0.5, -0.5, -0.5)$ and $x = (0, 0)$. Then the forward pass yields exactly $k = 2$ nonzero activations.\n\nD. For $b = \\mathbf{0}$, scaling the entire matrix $W$ by any positive scalar $\\alpha > 0$ does not change the partition of $\\mathbb{R}^2$ into regions of constant activation patterns (i.e., the support of $a$ across $x \\in \\mathbb{R}^2$ is unchanged).\n\nE. For $b = \\mathbf{0}$, replacing any single row $w_i$ by $-w_i$ leaves the geometric hyperplane boundary $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ unchanged but flips which side of that boundary is active for unit $i$, thereby changing the activation pattern assigned to each region.\n\nF. For any choice of $W \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, as $x$ varies over $\\mathbb{R}^n$ the number of distinct activation patterns realized is exactly $2^m$.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The neural network is a single-layer feed-forward network.\n- The activation function is the Rectified Linear Unit (ReLU), applied elementwise: $\\mathrm{ReLU}(z) = \\max(0, z)$.\n- The forward pass is defined by the equation $a = \\mathrm{ReLU}(W x + b)$.\n- The input vector dimension is $n = 2$, so $x \\in \\mathbb{R}^2$.\n- The number of units (output dimension) is $m = 4$, so $a, b \\in \\mathbb{R}^4$ and $W \\in \\mathbb{R}^{4 \\times 2}$.\n- The rows of the weight matrix $W$ are denoted by $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard single-neuron-layer network, a fundamental building block in deep learning. The components—input vector, weight matrix, bias vector, ReLU activation, and the forward propagation equation—are all standard and well-defined in the field. The questions posed in the options are specific, testable mathematical claims about the behavior of this system under different parameterizations.\n\n- **Scientifically Grounded:** The problem is based on the established mathematical framework of artificial neural networks. All definitions and concepts are standard.\n- **Well-Posed:** The problem provides all necessary information to evaluate the claims made in the options. Each option presents a clear, falsifiable hypothesis.\n- **Objective:** The problem is stated in precise mathematical language, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full solution and evaluation of options will be performed.\n\nThe core of the analysis involves the pre-activation vector $z = Wx + b$, whose components are $z_i = w_i^\\top x + b_i$ for $i \\in \\{1, 2, 3, 4\\}$. The $i$-th activation is $a_i = \\mathrm{ReLU}(z_i)$. An activation $a_i$ is non-zero if and only if its corresponding pre-activation $z_i$ is positive, i.e., $a_i > 0 \\iff w_i^\\top x + b_i > 0$. The boundary between the active and inactive regions for the $i$-th unit is the hyperplane (a line in $\\mathbb{R}^2$) defined by the equation $w_i^\\top x + b_i = 0$.\n\n### Option-by-Option Analysis\n\n**A. Let $W$ have rows $w_1 = (1, 0)$, $w_2 = (-1, 0)$, $w_3 = (0, 1)$, $w_4 = (0, -1)$ and let $b = \\mathbf{0} \\in \\mathbb{R}^4$. For $x = (1, 1)$, the forward pass yields exactly $k = 2$ nonzero activations. In this configuration, the induced region boundaries are the hyperplanes $x_1 = 0$ and $x_2 = 0$.**\n\nFirst, we compute the pre-activation vector $z = Wx + b$ with the given values.\n$W = \\begin{pmatrix} 1 & 0 \\\\ -1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{pmatrix}$, $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n$z = Wx = \\begin{pmatrix} 1 & 0 \\\\ -1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ -1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\n\nNext, we apply the ReLU function to get the activation vector $a$:\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 1) \\\\ \\max(0, -1) \\\\ \\max(0, 1) \\\\ \\max(0, -1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nThe number of nonzero activations is $k=2$ ($a_1$ and $a_3$). This part of the statement is correct.\n\nNow, we analyze the region boundaries. The boundaries are defined by $w_i^\\top x + b_i = 0$. Since $b = \\mathbf{0}$, this simplifies to $w_i^\\top x = 0$.\nLet $x = (x_1, x_2)^\\top$.\n- For $i=1$: $w_1^\\top x = (1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1 = 0$.\n- For $i=2$: $w_2^\\top x = (-1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_1 = 0 \\implies x_1 = 0$.\n- For $i=3$: $w_3^\\top x = (0, 1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_2 = 0$.\n- For $i=4$: $w_4^\\top x = (0, -1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_2 = 0 \\implies x_2 = 0$.\nThe set of distinct boundary hyperplanes is $\\{x_1 = 0\\}$ and $\\{x_2 = 0\\}$. These are the coordinate axes. This part of the statement is also correct.\n\nVerdict: **Correct**.\n\n**B. With the same $W$ and $b = \\mathbf{0}$ as in option A, choosing $x = (0, 0)$ yields exactly $k = 2$ nonzero activations.**\n\nUsing $W$ from option A, $b = \\mathbf{0}$, and $x = (0, 0)^\\top$.\n$z = Wx + b = W\\mathbf{0} + \\mathbf{0} = \\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}(\\mathbf{0}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe number of nonzero activations is $k=0$. The statement claims $k=2$.\n\nVerdict: **Incorrect**.\n\n**C. With the same $W$ as in option A, choose $b = (0.5, 0.5, -0.5, -0.5)$ and $x = (0, 0)$. Then the forward pass yields exactly $k = 2$ nonzero activations.**\n\nUsing $W$ from option A, $x = (0, 0)^\\top$, and $b = (0.5, 0.5, -0.5, -0.5)^\\top$.\n$z = Wx + b = W\\mathbf{0} + b = b = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}$.\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 0.5) \\\\ \\max(0, 0.5) \\\\ \\max(0, -0.5) \\\\ \\max(0, -0.5) \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe number of nonzero activations is $k=2$ ($a_1$ and $a_2$).\n\nVerdict: **Correct**.\n\n**D. For $b = \\mathbf{0}$, scaling the entire matrix $W$ by any positive scalar $\\alpha > 0$ does not change the partition of $\\mathbb{R}^2$ into regions of constant activation patterns (i.e., the support of $a$ across $x \\in \\mathbb{R}^2$ is unchanged).**\n\nThe activation pattern is determined by the signs of the pre-activations $z_i$. When $b = \\mathbf{0}$, we have $z_i = w_i^\\top x$. The $i$-th unit is active if $w_i^\\top x > 0$.\nLet the scaled weight matrix be $W' = \\alpha W$, where $\\alpha > 0$. The new weight vectors are $w'_i = \\alpha w_i$.\nThe new pre-activations are $z'_i = (w'_i)^\\top x = (\\alpha w_i)^\\top x = \\alpha(w_i^\\top x)$.\nSince $\\alpha > 0$, the sign of $z'_i$ is the same as the sign of $z_i = w_i^\\top x$.\n- $z'_i > 0 \\iff \\alpha(w_i^\\top x) > 0 \\iff w_i^\\top x > 0$.\n- $z'_i \\le 0 \\iff \\alpha(w_i^\\top x) \\le 0 \\iff w_i^\\top x \\le 0$.\nFor any given input $x$, the set of active units (the activation pattern) remains identical. The boundary hyperplanes are defined by $w_i^\\top x = 0$. For the scaled system, they are defined by $(\\alpha w_i)^\\top x = 0$, which is equivalent to $w_i^\\top x = 0$. The geometric locations of the boundaries are unchanged. Thus, the partition of the input space into regions of constant activation patterns is unchanged.\n\nVerdict: **Correct**.\n\n**E. For $b = \\mathbf{0}$, replacing any single row $w_i$ by $-w_i$ leaves the geometric hyperplane boundary $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ unchanged but flips which side of that boundary is active for unit $i$, thereby changing the activation pattern assigned to each region.**\n\nLet $w'_i = -w_i$.\nThe original boundary for unit $i$ is the set of points $x$ such that $w_i^\\top x = 0$.\nThe new boundary is the set of points $x$ such that $(w'_i)^\\top x = 0$. This is $(-w_i)^\\top x = - (w_i^\\top x) = 0$, which is equivalent to $w_i^\\top x = 0$. The geometric boundary is indeed unchanged.\n\nThe original active region for unit $i$ is where $w_i^\\top x > 0$.\nThe new active region for unit $i$ is where $(w'_i)^\\top x > 0$, which means $(-w_i)^\\top x > 0$, or $-(w_i^\\top x) > 0$. This simplifies to $w_i^\\top x < 0$.\nThe region of activation for unit $i$ has been flipped from one side of the hyperplane to the other. Consequently, for any point $x$ not on the boundary itself, the activation status of the $i$-th unit will be flipped compared to the original configuration. This changes the activation pattern (the vector of $0$s and $1$s indicating which units are active) for every region in the partition.\n\nVerdict: **Correct**.\n\n**F. For any choice of $W \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, as $x$ varies over $\\mathbb{R}^n$ the number of distinct activation patterns realized is exactly $2^m$.**\n\nAn activation pattern is a binary vector of length $m$, so there are $2^m$ possible patterns in total. The question is whether all of them can be realized for *any* choice of $W$ and $b$.\nThe hyperplanes $w_i^\\top x + b_i = 0$ for $i=1, \\dots, m$ partition the input space $\\mathbb{R}^n$ into a number of regions. Within each open, connected region, the signs of all $w_i^\\top x + b_i$ are constant, so the activation pattern is constant. The number of distinct activation patterns is therefore bounded by the number of regions created by these hyperplanes.\nThe maximum number of regions that $m$ hyperplanes can partition $\\mathbb{R}^n$ into is given by Zaslavsky's formula, which for hyperplanes in general position is $\\sum_{j=0}^{n} \\binom{m}{j}$.\nIn our case, $n=2$ and $m=4$. The maximum number of regions is:\n$$ \\sum_{j=0}^{2} \\binom{4}{j} = \\binom{4}{0} + \\binom{4}{1} + \\binom{4}{2} = 1 + 4 + \\frac{4 \\cdot 3}{2} = 1 + 4 + 6 = 11 $$\nThe total number of possible activation patterns is $2^m = 2^4 = 16$.\nSince the number of regions ($11$) is less than the number of possible patterns ($16$), it is impossible to realize all $2^m$ patterns. The statement is universal (\"For any choice...\"), so a single counterexample is sufficient to disprove it. The geometric argument above shows that no choice of $W, b$ can achieve this for $n=2, m=4$ (even without considering degenerate cases like parallel or coincident hyperplanes which would reduce the number of regions further). For example, if we choose all $w_i$ to be identical, say $w_i = (1,0)$ for all $i$, and $b=\\mathbf{0}$, we get only one distinct hyperplane $x_1=0$. This creates two regions ($x_1>0$ and $x_1<0$), realizing only two patterns: all units on or all units off.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACDE}$$", "id": "3185431"}, {"introduction": "Modern neural networks, such as Transformers, are built upon sophisticated layers that can seem intimidating at first. However, their forward pass is still just a sequence of well-defined matrix operations. This exercise demystifies the core of the Transformer, the Scaled Dot-Product Attention mechanism, by guiding you through a manual calculation for a small, synthetic example [@problem_id:3185352]. By computing the attention scores, weights, and final output from scratch, you will gain a concrete understanding of how inputs interact to create context-aware representations, seeing firsthand how the formula $\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ works in practice.", "problem": "Consider the forward propagation through a single-head self-attention layer, which is based on dot products between query and key vectors, scaling by the square root of the key dimensionality, applying the softmax function row-wise to obtain attention weights, and then a matrix multiplication with the value vectors to produce the output. Use the following small synthetic example with three tokens and key dimensionality $d_k=2$. The query matrix $Q$, key matrix $K$, and value matrix $V$ are all equal and given by\n$$\nQ=K=V=\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}.\n$$\nYou must proceed only from fundamental definitions: the dot product of vectors, the square-root scaling by $\\sqrt{d_k}$, row-wise application of the softmax function defined by $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$, and standard matrix multiplication. Without invoking any shortcut formulas, compute the forward pass of Scaled Dot-Product Attention (SDPA) for this example. By analyzing the structure of $Q$, $K$, and $V$, identify which rows of the output are identical due to symmetry. Then, compute the common value of the first component of those identical output rows. Express your final numerical result rounded to four significant figures and report it as a dimensionless quantity.", "solution": "The problem asks for the computation of the forward pass of a single-head Scaled Dot-Product Attention (SDPA) layer for a given set of input matrices $Q$, $K$, and $V$. The final output should be a specific numerical value derived from the resulting attention output matrix. The process is governed by the formula:\n$$Z = \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nwhere $Z$ is the output matrix. We are instructed to proceed from fundamental definitions, breaking down the computation into sequential steps.\n\nThe given inputs are:\nThe query, key, and value matrices are identical:\n$$Q=K=V=\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\nThe key dimensionality is given as $d_k=2$.\n\nStep 1: Compute the matrix of dot product scores, $QK^T$.\nFirst, we find the transpose of the key matrix $K$:\n$$K^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\nNext, we perform the matrix multiplication $QK^T$:\n$$QK^T = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\nThis product yields a $3 \\times 3$ matrix of attention scores. The elements are calculated as follows:\n$$(QK^T)_{11} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{12} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{13} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{21} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{22} = (0)(0) + (1)(1) = 1$$\n$$(QK^T)_{23} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{31} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{32} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{33} = (1)(1) + (0)(0) = 1$$\nThe resulting scores matrix is:\n$$S_{raw} = QK^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}$$\n\nStep 2: Scale the scores matrix by $1/\\sqrt{d_k}$.\nWith $d_k=2$, the scaling factor is $1/\\sqrt{2}$. The scaled scores matrix, which we denote as $S$, is:\n$$S = \\frac{QK^T}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n\nStep 3: Compute the attention weight matrix $A_w$ by applying the softmax function row-wise to $S$.\nThe softmax function for a vector $x$ is defined as $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$.\n\nFor the first row of $S$, $s_1 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$:\nThe denominator for the softmax is $\\sum_{j=1}^3 \\exp(s_{1j}) = \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) = 2\\exp(\\frac{1}{\\sqrt{2}}) + 1$.\nThe components of the first row of $A_w$ are:\n$$A_{w,11} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,12} = \\frac{\\exp(0)}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} = \\frac{1}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,13} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\nFor the second row of $S$, $s_2 = (0, \\frac{1}{\\sqrt{2}}, 0)$:\nThe denominator for the softmax is $\\sum_{j=1}^3 \\exp(s_{2j}) = \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) = 2 + \\exp(\\frac{1}{\\sqrt{2}})$.\nThe components of the second row of $A_w$ are:\n$$A_{w,21} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,22} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,23} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}$$\n\nThe third row of $S$, $s_3 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$, is identical to the first row $s_1$. Thus, the third row of the attention matrix $A_w$ is identical to its first row: $A_{w,3j} = A_{w,1j}$ for $j \\in \\{1, 2, 3\\}$.\n\nStep 4: Compute the final output matrix $Z = A_w V$.\n$$Z = A_w V = \\begin{pmatrix} A_{w,11} & A_{w,12} & A_{w,13} \\\\ A_{w,21} & A_{w,22} & A_{w,23} \\\\ A_{w,31} & A_{w,32} & A_{w,33} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\nThe rows of the output matrix $Z$ are linear combinations of the rows of $V$. Let $v_1, v_2, v_3$ be the rows of $V$, so $v_1 = (1, 0)$, $v_2 = (0, 1)$, and $v_3 = (1, 0)$. Note that $v_1=v_3$.\n\nThe problem asks to identify which rows of the output are identical due to symmetry. The input query vectors are $q_1=(1,0)$, $q_2=(0,1)$, and $q_3=(1,0)$. Because $q_1=q_3$ and the key matrix $K$ is shared, the attention scores for the first and third queries will be identical. As established in Step 2, the first and third rows of the scaled scores matrix $S$ are the same. This leads to the first and third rows of the attention weight matrix $A_w$ being identical, as shown in Step 3.\nThe first and third rows of the output $Z$ are:\n$$Z_1 = A_{w,11}v_1 + A_{w,12}v_2 + A_{w,13}v_3$$\n$$Z_3 = A_{w,31}v_1 + A_{w,32}v_2 + A_{w,33}v_3$$\nSince $A_{w,1j} = A_{w,3j}$ for all $j$, we have $Z_1 = Z_3$. The first and third rows of the output are identical.\n\nThe problem then asks for the common value of the first component of these identical output rows. We will compute the first component of the first output row, $Z_{11}$.\n$$Z_1 = (Z_{11}, Z_{12}) = A_{w,11}(1, 0) + A_{w,12}(0, 1) + A_{w,13}(1, 0) = (A_{w,11} + A_{w,13}, A_{w,12})$$\nThe first component is $Z_{11} = A_{w,11} + A_{w,13}$. From Step 3, we know $A_{w,11} = A_{w,13}$. Therefore:\n$$Z_{11} = 2A_{w,11} = 2 \\left( \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} \\right) = \\frac{2\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\nFinally, we compute the numerical value and round it to four significant figures.\nThe value of the exponent is $\\frac{1}{\\sqrt{2}} \\approx 0.70710678$.\nThe exponential term is $\\exp(\\frac{1}{\\sqrt{2}}) \\approx 2.02813039$.\nSubstituting this into the expression for $Z_{11}$:\n$$Z_{11} = \\frac{2 \\times 2.02813039}{2 \\times 2.02813039 + 1} = \\frac{4.05626078}{5.05626078} \\approx 0.80218349$$\nRounding this result to four significant figures gives $0.8022$.", "answer": "$$\\boxed{0.8022}$$", "id": "3185352"}, {"introduction": "The forward propagation algorithm does more than just compute a final prediction; it also dictates the internal dynamics and computational efficiency of a model. This practice explores this concept within a Mixture-of-Experts (MoE) layer, a powerful architecture used in large-scale models. Here, the forward pass includes a gating network that routes inputs to different \"expert\" sub-networks [@problem_id:3185355]. You will implement the MoE forward pass and analyze a critical performance characteristic: load balancing. This exercise highlights how the design and scaling of parameters in the forward pass directly impact the model's practical viability and performance.", "problem": "Consider a Mixture-of-Experts (MoE) forward propagation for a batch of inputs, where the gate assigns routing weights using the softmax mapping of linear scores. Let the number of experts be $m$, the input dimension be $d$, and the output dimension be $p$. For an input vector $x \\in \\mathbb{R}^{d}$, define the gate as $g(x) = \\mathrm{softmax}(U x)$, where $U \\in \\mathbb{R}^{m \\times d}$ is a parameter matrix. Each expert is an affine map $E_i(x) = A_i x + c_i$ with $A_i \\in \\mathbb{R}^{p \\times d}$ and $c_i \\in \\mathbb{R}^{p}$. The MoE output is $y(x) = \\sum_{i=1}^{m} g_i(x) E_i(x)$, where $g_i(x)$ is the $i$-th component of $g(x)$. The task is to implement the forward propagation from first principles: linear scoring, softmax normalization, affine expert outputs, and the weighted aggregation.\n\nBegin from the following fundamental base definitions:\n- A linear map is represented by a matrix-vector multiplication, that is, for $M \\in \\mathbb{R}^{a \\times b}$ and $v \\in \\mathbb{R}^{b}$, the output is $M v \\in \\mathbb{R}^{a}$.\n- The softmax function maps a real vector to a probability simplex: for $z \\in \\mathbb{R}^{m}$, $\\mathrm{softmax}(z)_i = \\exp(z_i)\\big/\\sum_{j=1}^{m}\\exp(z_j)$, producing nonnegative components that sum to $1$.\n\nTo quantify routing load imbalance, for a batch matrix $X \\in \\mathbb{R}^{n \\times d}$, define the expected load of expert $i$ as $S_i = \\sum_{k=1}^{n} g_i(x_k)$, where $x_k$ is the $k$-th row of $X$. Define the imbalance ratio $r = \\max_{1 \\leq j \\leq m} S_j \\big/ \\min_{1 \\leq j \\leq m} S_j$. A high value of $r$ indicates severe imbalance; a value of $r = 1$ indicates perfectly balanced routing.\n\nImplement the forward propagation algorithm and compute $r$ for each of the following test cases. Use the same base matrices and vectors for all cases:\n\n- Dimensions: $m = 4$, $d = 3$, $p = 2$, $n = 5$.\n- Base gate matrix:\n$$\nB = \\begin{bmatrix}\n1.0 & -2.0 & 0.5 \\\\\n-0.5 & 3.0 & 1.5 \\\\\n2.0 & 0.0 & -1.0 \\\\\n-1.0 & 1.0 & 2.0\n\\end{bmatrix}.\n$$\n- Batch inputs:\n$$\nX = \\begin{bmatrix}\n0.1 & -0.2 & 0.3 \\\\\n1.0 & 0.0 & -1.0 \\\\\n-0.5 & 2.0 & 1.0 \\\\\n0.0 & 0.0 & 0.0 \\\\\n3.0 & -1.0 & 0.5\n\\end{bmatrix}.\n$$\n- Expert parameters:\n$$\nA_1 = \\begin{bmatrix} 0.2 & -0.1 & 0.0 \\\\ 1.0 & 0.5 & -0.3 \\end{bmatrix}, \\quad c_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix},\n$$\n$$\nA_2 = \\begin{bmatrix} -0.3 & 0.7 & 0.2 \\\\ 0.0 & -0.2 & 0.5 \\end{bmatrix}, \\quad c_2 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix},\n$$\n$$\nA_3 = \\begin{bmatrix} 0.5 & 0.0 & -0.4 \\\\ -0.6 & 0.9 & 0.0 \\end{bmatrix}, \\quad c_3 = \\begin{bmatrix} -0.2 \\\\ 0.3 \\end{bmatrix},\n$$\n$$\nA_4 = \\begin{bmatrix} 0.1 & 0.1 & 0.1 \\\\ -0.1 & -0.2 & -0.3 \\end{bmatrix}, \\quad c_4 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}.\n$$\n\nTest suite of gate matrices $U$:\n1. Happy path: $U = 1.0 \\cdot B$.\n2. Poorly scaled large: $U = 50.0 \\cdot B$.\n3. Boundary condition (uniform routing): $U = 0.0 \\cdot B$.\n4. Ill-conditioned per-row scaling: $U = D B$, where $D = \\mathrm{diag}(10.0, 0.1, 5.0, 1.0)$.\n\nFor each case:\n- Implement the forward pass $y(x_k)$ for all $k \\in \\{1,\\dots,n\\}$.\n- Compute the gate probabilities $g(x_k)$ and the expected loads $S_i$.\n- Compute the imbalance ratio $r$.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the imbalance ratios for the four test cases, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets, in the order of the cases above (for example, $[r_1,r_2,r_3,r_4]$).", "solution": "The problem as stated is subjected to rigorous validation.\n\n### Step 1: Extract Givens\n- **Model Definition**: A Mixture-of-Experts (MoE) model.\n  - Gate function: $g(x) = \\mathrm{softmax}(U x)$, where $U \\in \\mathbb{R}^{m \\times d}$ and $x \\in \\mathbb{R}^{d}$.\n  - Expert function: $E_i(x) = A_i x + c_i$, where $A_i \\in \\mathbb{R}^{p \\times d}$ and $c_i \\in \\mathbb{R}^{p}$.\n  - MoE output: $y(x) = \\sum_{i=1}^{m} g_i(x) E_i(x)$, where $g_i(x)$ is the $i$-th component of $g(x)$.\n- **Base Definitions**:\n  - Linear map: Matrix-vector multiplication $Mv$.\n  - Softmax: $\\mathrm{softmax}(z)_i = \\exp(z_i)\\big/\\sum_{j=1}^{m}\\exp(z_j)$.\n- **Imbalance Quantification**:\n  - Expected load of expert $i$: $S_i = \\sum_{k=1}^{n} g_i(x_k)$ for a batch $X \\in \\mathbb{R}^{n \\times d}$ with rows $x_k$.\n  - Imbalance ratio: $r = \\max_{j} S_j \\big/ \\min_{j} S_j$.\n- **Dimensions**:\n  - Number of experts, $m = 4$.\n  - Input dimension, $d = 3$.\n  - Output dimension, $p = 2$.\n  - Batch size, $n = 5$.\n- **Data**:\n  - Base gate matrix, $B = \\begin{bmatrix} 1.0 & -2.0 & 0.5 \\\\ -0.5 & 3.0 & 1.5 \\\\ 2.0 & 0.0 & -1.0 \\\\ -1.0 & 1.0 & 2.0 \\end{bmatrix}$.\n  - Batch inputs, $X = \\begin{bmatrix} 0.1 & -0.2 & 0.3 \\\\ 1.0 & 0.0 & -1.0 \\\\ -0.5 & 2.0 & 1.0 \\\\ 0.0 & 0.0 & 0.0 \\\\ 3.0 & -1.0 & 0.5 \\end{bmatrix}$.\n  - Expert parameters:\n    - $A_1 = \\begin{bmatrix} 0.2 & -0.1 & 0.0 \\\\ 1.0 & 0.5 & -0.3 \\end{bmatrix}, \\quad c_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}$.\n    - $A_2 = \\begin{bmatrix} -0.3 & 0.7 & 0.2 \\\\ 0.0 & -0.2 & 0.5 \\end{bmatrix}, \\quad c_2 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$.\n    - $A_3 = \\begin{bmatrix} 0.5 & 0.0 & -0.4 \\\\ -0.6 & 0.9 & 0.0 \\end{bmatrix}, \\quad c_3 = \\begin{bmatrix} -0.2 \\\\ 0.3 \\end{bmatrix}$.\n    - $A_4 = \\begin{bmatrix} 0.1 & 0.1 & 0.1 \\\\ -0.1 & -0.2 & -0.3 \\end{bmatrix}, \\quad c_4 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}$.\n- **Test Cases for Gate Matrix $U$**:\n  1. $U = 1.0 \\cdot B$.\n  2. $U = 50.0 \\cdot B$.\n  3. $U = 0.0 \\cdot B$.\n  4. $U = D B$, where $D = \\mathrm{diag}(10.0, 0.1, 5.0, 1.0)$.\n- **Task**: For each case, implement the forward pass, compute the expected loads $S_i$, and the imbalance ratio $r$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the forward pass of a standard Mixture-of-Experts layer, a well-established component in neural network architectures. The mathematical operations (matrix multiplication, softmax) and definitions (load balancing) are standard in the field of deep learning. The problem is scientifically sound.\n- **Well-Posed**: All required constants ($m, d, p, n$), matrices ($B, X, A_i$), and vectors ($c_i$) are provided. The dimensions are consistent for all specified operations. For example, for the expression $Ux$, $U \\in \\mathbb{R}^{4 \\times 3}$ and $x \\in \\mathbb{R}^{3}$, yielding a vector in $\\mathbb{R}^{4}$ as required by the softmax function for $m=4$ experts. The tasks are defined unambiguously, leading to a unique, computable solution for each test case.\n- **Objective**: The problem is stated in precise, formal mathematical and computational terms, free of subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is self-contained, scientifically grounded, and well-posed. A complete solution will be provided.\n\n### Solution\nThe forward propagation through a Mixture-of-Experts (MoE) layer involves three primary stages: gating, expert evaluation, and aggregation. The objective is to compute the routing imbalance ratio $r$, which depends solely on the gating mechanism's outputs.\n\nLet the batch of inputs be $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the batch size and $d$ is the input feature dimension. Each row $x_k$ of $X$ represents a single input vector. The gate matrix is $U \\in \\mathbb{R}^{m \\times d}$, where $m$ is the number of experts.\n\n**1. Gating Network: Linear Scores and Softmax Normalization**\n\nThe first step is to calculate the \"routing logits\" or linear scores for each input-expert pair. For the entire batch, this is efficiently computed as a matrix multiplication:\n$Z = X U^T$\nThe resulting matrix $Z \\in \\mathbb{R}^{n \\times m}$ contains the logits, where $Z_{ki}$ is the score assigning input $x_k$ to expert $i$.\n\nNext, these scores are converted into a probability distribution over the experts for each input using the softmax function. This is applied row-wise to the logit matrix $Z$. The gate probability matrix $G \\in \\mathbb{R}^{n \\times m}$ is computed as:\n$$G_{ki} = g_i(x_k) = \\frac{\\exp(Z_{ki})}{\\sum_{j=1}^{m} \\exp(Z_{kj})}$$\nEach row of $G$ is a probability vector that sums to $1$, representing the weights for combining the expert outputs for a given input. For numerical stability, a common technique is to subtract the maximum logit from each row before exponentiation: $Z'_{ki} = Z_{ki} - \\max_j(Z_{kj})$.\n\n**2. Expert Network: Affine Transformation**\n\nWhile not required for computing the imbalance ratio $r$, a complete forward pass also involves evaluating each expert's output. Every expert $i$ is an affine function $E_i(x) = A_i x + c_i$. For an input vector $x_k$ (represented as a column vector), the output of expert $i$ is a vector $y_{ki} \\in \\mathbb{R}^p$.\n$$y_{ki} = A_i x_k^T + c_i$$\nThis computation would be performed for each of the $n$ inputs and $m$ experts.\n\n**3. Aggregation**\n\nThe final output for a single input $x_k$ is the weighted sum of the individual expert outputs, using the gate probabilities as weights:\n$$y(x_k) = \\sum_{i=1}^{m} G_{ki} \\cdot y_{ki} = \\sum_{i=1}^{m} g_i(x_k) E_i(x_k)$$\nThe result $y(x_k)$ is a vector in $\\mathbb{R}^p$.\n\n**4. Imbalance Ratio Calculation**\n\nThe core task is to compute the imbalance ratio $r$. This requires the expected load on each expert, $S_i$, which is the sum of the gate probabilities assigned to that expert over the entire batch:\n$$S_i = \\sum_{k=1}^{n} G_{ki}$$\nThis corresponds to computing the column sums of the gate probability matrix $G$. After obtaining the load vector $S = [S_1, S_2, \\dots, S_m]$, the imbalance ratio $r$ is calculated as the ratio of the maximum load to the minimum load:\n$$r = \\frac{\\max_{j} S_j}{\\min_{j} S_j}$$\n\n**Analysis of Test Cases**\n\nWe apply this procedure to the four specified cases for the gate matrix $U$:\n\n- **Case 1: $U = 1.0 \\cdot B$**. This is the baseline case. We expect some moderate level of routing imbalance depending on the alignment of the input data $X$ with the weight vectors in $B$.\n\n- **Case 2: $U = 50.0 \\cdot B$**. Scaling the logits by a large positive factor ($50.0$) pushes the softmax function towards a \"hard\" max, where the probability for the expert with the highest logit approaches $1$ and all others approach $0$. This typically leads to sparse routing, where each input is sent to only one expert. Such specialization often results in significant load imbalance, so we expect a high value of $r$.\n\n- **Case 3: $U = 0.0 \\cdot B$**. Here, the gate matrix $U$ is the zero matrix. Consequently, all logits $Z_{ki}$ are $0$. The softmax of a vector of zeros is a uniform distribution: $g_i(x_k) = \\exp(0) / \\sum_j \\exp(0) = 1/m$. Every input is routed to every expert with equal probability ($1/4 = 0.25$). This results in perfect load balancing. The load on each expert will be identical: $S_i = n \\cdot (1/m) = 5/4 = 1.25$. Therefore, $r = S_{\\max}/S_{\\min} = 1.0$.\n\n- **Case 4: $U = D B$ with $D = \\mathrm{diag}(10.0, 0.1, 5.0, 1.0)$**. This corresponds to scaling the rows of the base gate matrix $B$. The weights for expert $1$ are magnified by $10.0$, while those for expert $2$ are attenuated by a factor of $0.1$. This will strongly bias the routing. Inputs are more likely to be routed to experts with large scaling factors (expert $1$ and $3$) and less likely to expert $2$. We expect this to create a significant imbalance, resulting in a large $r$.\n\nThe calculation of these four ratios is performed programmatically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Implements the Mixture-of-Experts forward propagation to compute routing imbalance.\n    \"\"\"\n    # Define problem dimensions and constants\n    m = 4  # number of experts\n    d = 3  # input dimension\n    p = 2  # output dimension\n    n = 5  # batch size\n\n    # Define base matrices and vectors as given in the problem statement\n    B = np.array([\n        [1.0, -2.0, 0.5],\n        [-0.5, 3.0, 1.5],\n        [2.0, 0.0, -1.0],\n        [-1.0, 1.0, 2.0]\n    ])\n\n    X = np.array([\n        [0.1, -0.2, 0.3],\n        [1.0, 0.0, -1.0],\n        [-0.5, 2.0, 1.0],\n        [0.0, 0.0, 0.0],\n        [3.0, -1.0, 0.5]\n    ])\n\n    # Expert parameters (A_i, c_i) are defined for completeness of the MoE model,\n    # but are not used in the calculation of the imbalance ratio `r`.\n    A1 = np.array([[0.2, -0.1, 0.0], [1.0, 0.5, -0.3]])\n    c1 = np.array([[0.0], [0.1]])\n    A2 = np.array([[-0.3, 0.7, 0.2], [0.0, -0.2, 0.5]])\n    c2 = np.array([[0.2], [-0.1]])\n    A3 = np.array([[0.5, 0.0, -0.4], [-0.6, 0.9, 0.0]])\n    c3 = np.array([[-0.2], [0.3]])\n    A4 = np.array([[0.1, 0.1, 0.1], [-0.1, -0.2, -0.3]])\n    c4 = np.array([[0.05], [-0.05]])\n    \n    expert_params = [(A1, c1), (A2, c2), (A3, c3), (A4, c4)]\n\n    # Define the 4 test cases for the gate matrix U\n    D = np.diag([10.0, 0.1, 5.0, 1.0])\n    test_cases = [\n        (\"Happy path\", 1.0 * B),\n        (\"Poorly scaled large\", 50.0 * B),\n        (\"Boundary condition (uniform routing)\", 0.0 * B),\n        (\"Ill-conditioned per-row scaling\", D @ B)\n    ]\n\n    imbalance_ratios = []\n\n    for name, U in test_cases:\n        # Step 1: Compute linear scores (logits) for the batch\n        # Z = X @ U.T\n        # Z will have shape (n, m) = (5, 4)\n        Z = X @ U.T\n\n        # Step 2: Apply softmax row-wise to get gate probabilities\n        # G will have shape (n, m) = (5, 4)\n        # Using scipy.special.softmax for robust computation\n        G = softmax(Z, axis=1)\n\n        # Step 3: Compute expected load S_i for each expert\n        # S is the sum of probabilities for each expert over the batch\n        # S will have shape (m,) = (4,)\n        S = np.sum(G, axis=0)\n\n        # Step 4: Compute the imbalance ratio r\n        # r = max(S) / min(S)\n        s_max = np.max(S)\n        s_min = np.min(S)\n\n        # Handle potential division by zero, though not expected in these cases.\n        if s_min == 0:\n            if s_max == 0:\n                r = 1.0  # All loads are zero, so perfectly balanced\n            else:\n                r = np.inf # At least one expert has zero load, others don't\n        else:\n            r = s_max / s_min\n        \n        imbalance_ratios.append(r)\n\n    # Format the final output string as specified\n    formatted_results = [f\"{r:.6f}\" for r in imbalance_ratios]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3185355"}]}