{"hands_on_practices": [{"introduction": "The stability of a deep network depends heavily on ensuring that activations do not explode or vanish as they propagate through layers. This practice invites you to explore, from first principles, how a careful choice of bias initialization can help maintain zero-mean activations, a key factor in stable training [@problem_id:3098917]. By working through the statistics of a single neuron, you will build a foundational understanding of the interplay between input data, network parameters, and activation functions.", "problem": "Consider a single hidden unit within a deep feedforward network layer. The unit receives a scalar random input $X$ with mean $\\mu$ and a distribution that is symmetric about $\\mu$ in the sense that the distribution of $X - \\mu$ is identical to the distribution of $-(X - \\mu)$. The unit computes a pre-activation $z$ and an activation $a$ via a linear transformation followed by a nonlinearity:\n$$\nz = w X + b, \\quad a = \\phi(z),\n$$\nwhere $w \\in \\mathbb{R}$ is a weight, $b \\in \\mathbb{R}$ is a bias, and $\\phi:\\mathbb{R}\\to\\mathbb{R}$ is a fixed activation function. The bias is initialized using the heuristic $b = -\\mu$.\n\nUsing only the definitions of expectation, linearity of expectation, and the definition of a symmetric distribution, and without invoking any shortcut formulas, do the following:\n1. Derive $\\mathbb{E}[z]$ as a function of $w$ and $\\mu$ under the given bias initialization and input mean.\n2. From first principles, determine the value of $w$ that ensures $\\mathbb{E}[z]=0$ under $b=-\\mu$.\n3. Identify a sufficient and explicit functional property of $\\phi$ that guarantees the distribution of $a$ is symmetric about $0$ whenever the distribution of $z$ is symmetric about $0$.\n4. Define the symmetry score $S$ for the activation as $S := \\mathbb{E}[a]$. Under the conditions you derived, compute $S$ exactly.\n\nExpress your final answer as the exact value of $S$. No rounding is required.", "solution": "The user wants me to solve a problem about the statistical properties of a single hidden unit in a neural network.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified procedure.\n\n#### Step 1: Extract Givens\n\n- A single hidden unit receives a scalar random input $X$.\n- The mean of the input is $\\mathbb{E}[X] = \\mu$.\n- The distribution of $X$ is symmetric about $\\mu$, meaning the distribution of $X - \\mu$ is identical to the distribution of $-(X - \\mu)$.\n- The pre-activation is $z = w X + b$, where $w \\in \\mathbb{R}$ is a weight and $b \\in \\mathbb{R}$ is a bias.\n- The activation is $a = \\phi(z)$, where $\\phi: \\mathbb{R} \\to \\mathbb{R}$ is a fixed activation function.\n- The bias is initialized with the heuristic $b = -\\mu$.\n- The task consists of four parts:\n    1. Derive $\\mathbb{E}[z]$ as a function of $w$ and $\\mu$.\n    2. Determine the value of $w$ that ensures $\\mathbb{E}[z]=0$.\n    3. Identify a sufficient functional property of $\\phi$ that grantees the distribution of $a$ is symmetric about $0$ when the distribution of $z$ is symmetric about $0$.\n    4. Define the symmetry score $S := \\mathbb{E}[a]$ and compute its exact value under the derived conditions.\n\n#### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is firmly based on the fundamental principles of probability theory (expectation, symmetric distributions) and their application to a standard model of a neuron in a feedforward network. All concepts are well-established within mathematics and machine learning.\n- **Well-Posed:** The problem is presented as a sequence of logically connected questions. Each question is clearly stated and has a well-defined goal. The information provided is sufficient to derive a unique solution for each part, leading to a final answer.\n- **Objective:** The problem statement uses precise, formal mathematical language and definitions. There are no subjective or ambiguous terms.\n- **Flaw Checklist:**\n    1.  **Scientific or Factual Unsoundness:** None. The definitions and relationships are standard in the field.\n    2.  **Non-Formalizable or Irrelevant:** None. The problem is a formal mathematical exercise directly relevant to the analysis of deep learning architectures.\n    3.  **Incomplete or Contradictory Setup:** None. All necessary variables, definitions ($z$, $a$), and conditions ($b=-\\mu$, symmetry of $X$) are provided.\n    4.  **Unrealistic or Infeasible:** None. The setup is a common theoretical simplification used to analyze network dynamics.\n    5.  **Ill-Posed or Poorly Structured:** None. The definition of symmetry is explicitly provided. The problem structure guides the derivation logically.\n    6.  **Pseudo-Profound, Trivial, or Tautological:** None. The problem requires a solid understanding and application of first principles, and the result is not immediately obvious.\n    7.  **Outside Scientific Verifiability:** None. The derivations are mathematical and fully verifiable.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-posed, scientifically sound, and objective exercise in applied mathematics. I will now proceed with the solution.\n\n### Solution\n\nThe problem asks for a four-part derivation concerning the expected value of the activation of a hidden unit. I will address each part in sequence using first principles.\n\n**1. Derive $\\mathbb{E}[z]$ as a function of $w$ and $\\mu$**\n\nThe pre-activation $z$ is defined as $z = w X + b$. To find its expectation, $\\mathbb{E}[z]$, we apply the expectation operator to this definition:\n$$\n\\mathbb{E}[z] = \\mathbb{E}[w X + b]\n$$\nBy the linearity of expectation, which states that $\\mathbb{E}[c_1 Y_1 + c_2 Y_2] = c_1 \\mathbb{E}[Y_1] + c_2 \\mathbb{E}[Y_2]$ for constants $c_1, c_2$ and random variables $Y_1, Y_2$, we can separate the terms. Here, $w$ and $b$ are constants (non-random parameters).\n$$\n\\mathbb{E}[z] = \\mathbb{E}[w X] + \\mathbb{E}[b] = w \\mathbb{E}[X] + b\n$$\nWe are given that $\\mathbb{E}[X] = \\mu$ and the bias initialization is $b = -\\mu$. Substituting these values into the expression for $\\mathbb{E}[z]$:\n$$\n\\mathbb{E}[z] = w \\mu + (-\\mu) = (w - 1)\\mu\n$$\n\n**2. Determine the value of $w$ that ensures $\\mathbb{E}[z]=0$**\n\nUsing the result from the previous step, we set $\\mathbb{E}[z]$ to $0$:\n$$\n(w - 1)\\mu = 0\n$$\nFor this equation to hold, either $w-1=0$ or $\\mu=0$. If $\\mu=0$, then $\\mathbb{E}[z]=0$ for any value of $w$. However, the problem asks for \"the value of $w$\", which implies a unique solution that should hold for a general input distribution, not just one with a zero mean. Therefore, we assume $\\mu \\neq 0$. Under this non-trivial assumption, the only way to satisfy the equation is to have:\n$$\nw - 1 = 0 \\implies w = 1\n$$\n\n**3. Identify a sufficient property of $\\phi$ for symmetry of $a$**\n\nWe are given that the distribution of $z$ is symmetric about $0$. This means that the random variables $z$ and $-z$ have identical distributions. We need to find a property of the function $\\phi$ such that the distribution of $a = \\phi(z)$ is also symmetric about $0$.\n\nFor the distribution of $a$ to be symmetric about $0$, the random variables $a$ and $-a$ must have identical distributions.\nLet's express this using the random variable $z$. We need $\\phi(z)$ and $-\\phi(z)$ to have identical distributions.\nSince $z$ and $-z$ have identical distributions, this means that for any well-behaved test function $g$, we have $\\mathbb{E}[g(z)] = \\mathbb{E}[g(-z)]$.\n\nWe want the distribution of $\\phi(z)$ to be the same as the distribution of $-\\phi(z)$. A sufficient condition to ensure this is that the function mapping $z \\to \\phi(z)$ and the function mapping $z \\to -\\phi(z)$ are linked in a specific way.\nConsider the relationship between $\\phi(z)$ and $\\phi(-z)$.\nIf we impose the condition that $\\phi(-z) = -\\phi(z)$ for all $z \\in \\mathbb{R}$, this means $\\phi$ is an **odd function**.\nLet's see if this property is sufficient. The distribution of $-\\phi(z)$ is what we are interested in. If $\\phi$ is odd, the distribution of $\\phi(-z)$ is the same as the distribution of $-\\phi(z)$.\nSince $z$ and $-z$ have the same distribution, the distribution of $\\phi(z)$ is the same as the distribution of $\\phi(-z)$.\nCombining these facts:\nDistribution of $\\phi(z)$ = Distribution of $\\phi(-z)$ = Distribution of $-\\phi(z)$.\nThis shows that the distribution of $a = \\phi(z)$ is the same as the distribution of $-a = -\\phi(z)$, which is the definition of a distribution symmetric about $0$.\nTherefore, a sufficient property is that $\\phi$ is an **odd function**.\n\n**4. Compute the symmetry score $S = \\mathbb{E}[a]$**\n\nWe must compute $S = \\mathbb{E}[a]$ under the conditions derived: $w=1$ and $\\phi$ is an odd function. The bias is given as $b=-\\mu$.\nThe pre-activation is $z = wX+b = (1)X + (-\\mu) = X - \\mu$.\nThe activation is $a = \\phi(z) = \\phi(X - \\mu)$.\nThe symmetry score is $S = \\mathbb{E}[a] = \\mathbb{E}[\\phi(X - \\mu)]$.\n\nLet's define a new random variable $Z' = X - \\mu$.\nThe problem states that the distribution of $X$ is symmetric about $\\mu$, which is formally defined as the distribution of $X - \\mu$ being identical to the distribution of $-(X - \\mu)$.\nThis means the distribution of $Z'$ is identical to the distribution of $-Z'$. In other words, the distribution of $Z'$ is symmetric about $0$.\n\nWe need to compute $S = \\mathbb{E}[\\phi(Z')]$.\nBy the definition of a distribution symmetric about $0$, for any measurable function $g$, we have $\\mathbb{E}[g(Z')] = \\mathbb{E}[g(-Z')]$.\nLet's choose our function $g$ to be the activation function $\\phi$. Thus,\n$$\n\\mathbb{E}[\\phi(Z')] = \\mathbb{E}[\\phi(-Z')]\n$$\nFrom part 3, we have the condition that $\\phi$ is an odd function, meaning $\\phi(-y) = -\\phi(y)$ for any input $y$. Applying this to the random variable $Z'$:\n$$\n\\phi(-Z') = -\\phi(Z')\n$$\nNow substitute this back into our expectation equality:\n$$\n\\mathbb{E}[\\phi(Z')] = \\mathbb{E}[-\\phi(Z')]\n$$\nUsing the linearity of expectation, $\\mathbb{E}[-Y] = -\\mathbb{E}[Y]$, we can write:\n$$\n\\mathbb{E}[\\phi(Z')] = -\\mathbb{E}[\\phi(Z')]\n$$\nThe quantity we want to compute is $S = \\mathbb{E}[\\phi(Z')]$. The equation becomes:\n$$\nS = -S\n$$\nAdding $S$ to both sides gives $2S = 0$, which implies $S=0$.\n\nThe symmetry score $S$, which is the expected activation under the specified conditions, is exactly $0$. This holds true for any input distribution symmetric about its mean $\\mu$ and any odd activation function $\\phi$, given the specific weight and bias initializations $w=1$ and $b=-\\mu$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3098917"}, {"introduction": "As deep learning models grow, so does the demand for parameter-efficient architectures. This exercise introduces low-rank factorization, a powerful technique to compress large weight matrices and reduce computational cost, and explores its consequences on the network's representational capacity [@problem_id:3098849]. You will calculate the parameter savings and analyze how nonlinear activation functions like the Rectified Linear Unit (ReLU) enable the network to assemble complex, high-rank functions from low-rank building blocks.", "problem": "Consider a fully connected deep feedforward network with input dimension $n_0 = 600$, one hidden layer of width $n_1 = 400$, and output dimension $n_2 = 200$. Each layer uses a bias vector. For the first layer, the weight matrix is denoted by $W_1 \\in \\mathbb{R}^{n_1 \\times n_0}$ and the bias by $b_1 \\in \\mathbb{R}^{n_1}$; for the second layer, the weight matrix is $W_2 \\in \\mathbb{R}^{n_2 \\times n_1}$ and the bias is $b_2 \\in \\mathbb{R}^{n_2}$. The activation function between layers is the Rectified Linear Unit (ReLU), defined elementwise by $\\mathrm{ReLU}(z) = \\max(0, z)$.\n\nThe network is trained under two design choices:\n- Design $A$: unfactorized weights $W_1$ and $W_2$.\n- Design $B$: each weight matrix is constrained to be low-rank via a factorization $W_i = U_i V_i^\\top$ with $U_1 \\in \\mathbb{R}^{n_1 \\times r_1}$, $V_1 \\in \\mathbb{R}^{n_0 \\times r_1}$, $U_2 \\in \\mathbb{R}^{n_2 \\times r_2}$, and $V_2 \\in \\mathbb{R}^{n_1 \\times r_2}$, where $r_1 = 50$ and $r_2 = 40$.\n\nUse only fundamental definitions of affine maps, matrix rank, and the behavior of linear compositions and pointwise nonlinearities to reason about parameter counts and representational capacity. Select all statements that are correct:\n\nA. The total number of trainable parameters in Design $A$ is $320600$, whereas in Design $B$ it is $74600$.\n\nB. Under the given dimensions and ranks, Design $B$ reduces the number of parameters by more than $70\\%$ compared to Design $A$.\n\nC. If the activation function is removed (so the network becomes purely linear), the overall linear map $x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$ has matrix part $W_2 W_1$ of rank at most $\\min(r_1, r_2) = 40$.\n\nD. With the Rectified Linear Unit (ReLU) between layers, increasing depth can compensate for rank limitations by partitioning the input space into multiple regions, so that the network implements different low-rank linear maps in different regions; although the Jacobian at any single input has rank at most $\\min(r_1, r_2)$, the overall piecewise linear function can approximate mappings that a single rank-$r$ linear layer cannot.\n\nE. With the Rectified Linear Unit (ReLU) between layers, there exist inputs at which the Jacobian of the network has rank strictly greater than $\\min(r_1, r_2)$.", "solution": "We begin from first principles. A fully connected layer computes an affine map $x \\mapsto W x + b$, where $W$ is a matrix of free parameters and $b$ is a vector of free parameters. The number of parameters in such a layer equals the number of free entries in $W$ plus the number of entries in $b$. If a matrix $W \\in \\mathbb{R}^{m \\times n}$ is unconstrained, it has $m n$ free entries. If $W$ is constrained to factorize as $U V^\\top$ with $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$, then the number of free entries is $m r + n r$, and the matrix rank is at most $r$. For compositions of linear maps, the rank of a product satisfies $\\mathrm{rank}(A B) \\le \\min(\\mathrm{rank}(A), \\mathrm{rank}(B))$. For a network with a pointwise nonlinearity such as the Rectified Linear Unit (ReLU), the function becomes piecewise linear; at any input $x$, the Jacobian of the network equals the product of weight matrices and diagonal activation masks, and its rank is bounded by the minimal rank bottleneck in that product. However, different activation masks across different regions yield different local linear maps, enabling richer function classes than a single global linear map.\n\nCompute parameter counts.\n\n- Design $A$ (unfactorized):\n\nFirst layer parameters: $W_1$ has $n_1 n_0 = 400 \\cdot 600 = 240000$ entries; $b_1$ has $n_1 = 400$ entries. Second layer parameters: $W_2$ has $n_2 n_1 = 200 \\cdot 400 = 80000$ entries; $b_2$ has $n_2 = 200$ entries. Total parameters in Design $A$:\n$$240000 + 400 + 80000 + 200 = 320600.$$\n\n- Design $B$ (factorized):\n\nFirst layer parameters: $U_1$ has $n_1 r_1 = 400 \\cdot 50 = 20000$ entries; $V_1$ has $n_0 r_1 = 600 \\cdot 50 = 30000$ entries; $b_1$ has $n_1 = 400$ entries. Sum: $$20000 + 30000 + 400 = 50400.$$\nSecond layer parameters: $U_2$ has $n_2 r_2 = 200 \\cdot 40 = 8000$ entries; $V_2$ has $n_1 r_2 = 400 \\cdot 40 = 16000$ entries; $b_2$ has $n_2 = 200$ entries. Sum: $$8000 + 16000 + 200 = 24200.$$\nTotal parameters in Design $B$:\n$$50400 + 24200 = 74600.$$\n\nCompute relative reduction:\n$$\\text{reduction fraction} = 1 - \\frac{74600}{320600} \\approx 1 - 0.2326 \\approx 0.7674,$$\nwhich is a reduction of approximately $76.74\\%$, i.e., greater than $70\\%$.\n\nRank properties.\n\nIf the activation is removed, the overall map is linear: $x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$. The matrix part is $W_2 W_1$. Under factorization constraints, $\\mathrm{rank}(W_1) \\le r_1$ and $\\mathrm{rank}(W_2) \\le r_2$. Hence\n$$\\mathrm{rank}(W_2 W_1) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\n\nWith the Rectified Linear Unit (ReLU), the network function is piecewise linear. At any particular input $x$, the activation pattern induces a diagonal matrix $D$ with diagonal entries in $\\{0,1\\}$, so the local Jacobian equals $J(x) = W_2 D W_1$. Therefore, for any fixed $x$,\n$$\\mathrm{rank}(J(x)) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(D), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\nHowever, as $x$ varies, $D$ changes, yielding different local linear maps $W_2 D W_1$ across regions. This partitioning allows the network to implement a large number of distinct low-rank linear pieces glued together, thereby approximating functions that a single global low-rank linear layer cannot represent. This is a central mechanism by which depth with nonlinearities compensates for narrow or low-rank layers: it increases the number of linear regions and enables compositional structure, not the pointwise rank of the Jacobian at a given input.\n\nOption-by-option analysis:\n\nA. The computation above shows Design $A$ has $320600$ parameters and Design $B$ has $74600$ parameters. Verdict: Correct.\n\nB. The reduction is approximately $76.74\\%$, which is indeed more than $70\\%$. Verdict: Correct.\n\nC. Without nonlinearities, the rank of $W_2 W_1$ is at most $\\min(r_1, r_2) = 40$ by the rank submultiplicative property. Verdict: Correct.\n\nD. With the Rectified Linear Unit (ReLU), the network becomes piecewise linear with different activation masks $D$ across regions. Each region’s Jacobian rank is bounded by $\\min(r_1, r_2)$, yet the network can approximate mappings beyond what a single rank-$r$ linear layer can represent by stitching together many low-rank pieces. This accurately captures how depth compensates for rank limitations. Verdict: Correct.\n\nE. The Jacobian at any input $x$ equals $W_2 D W_1$, whose rank is bounded above by $\\min(r_1, r_2)$. Thus, it cannot exceed $\\min(r_1, r_2)$. Verdict: Incorrect.", "answer": "$$\\boxed{ABCD}$$", "id": "3098849"}, {"introduction": "Standard feedforward networks often struggle to learn functions with high-frequency details, a common feature in images, audio, and physical signals. This computational practice demonstrates how a simple but powerful architectural trick—positional encoding—can overcome this limitation by mapping inputs to a higher-dimensional feature space [@problem_id:3098829]. By implementing and testing this technique, you will gain hands-on insight into how modern architectures like Transformers and Neural Radiance Fields (NeRFs) achieve their remarkable expressivity.", "problem": "You are asked to formalize and quantify how a fixed nonlinear feature mapping known as positional encoding can enhance the effective expressivity of a shallow feedforward architecture, relative to a baseline model operating directly on raw input. Consider the following components and definitions on the domain $[0,1]$.\n\n1. Define the positional encoding $\\gamma_K(x)$ of order $K$ as the concatenation of sine and cosine features at exponentially increasing angular frequencies:\n$$\n\\gamma_K(x) = \\Big[ \\sin(2^0 \\pi x), \\cos(2^0 \\pi x), \\sin(2^1 \\pi x), \\cos(2^1 \\pi x), \\ldots, \\sin(2^K \\pi x), \\cos(2^K \\pi x) \\Big].\n$$\n\n2. Define a shallow model class built on the positional encoding, namely the linear readout on $\\gamma_K(x)$,\n$$\n\\mathcal{F}_{\\text{PE},K} = \\left\\{ f(x) = \\sum_{k=0}^{K} \\big(a_k \\sin(2^k \\pi x) + b_k \\cos(2^k \\pi x)\\big) + c \\,:\\, a_k, b_k, c \\in \\mathbb{R} \\right\\}.\n$$\nThis is a single-layer linear model on fixed nonlinear features, which is a standard shallow architecture with a prescribed feature map in deep learning practice.\n\n3. Define a baseline shallow model operating on raw input without positional encoding,\n$$\n\\mathcal{F}_{\\text{plain}} = \\left\\{ f(x) = w_1 x + w_0 \\,:\\, w_1, w_0 \\in \\mathbb{R} \\right\\}.\n$$\n\n4. Define the target functions to be approximated on $[0,1]$:\n   - A single high-frequency target: $g_1(x) = \\sin(2^m \\pi x)$ with $m = 3$.\n   - A mixed-frequency target: $g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$.\n   - A smooth polynomial target: $g_3(x) = x^2$.\n\n5. Let the empirical least-squares fit of a model class to a target $g(x)$ over a uniform grid $x_i = \\frac{i}{N-1}$ for $i \\in \\{0,1,\\ldots,N-1\\}$ be defined by minimizing\n$$\n\\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\nIn this question, use a grid size of $N = 4096$ points, and obtain the minimizer by linear least squares over the model’s features.\n\n6. Define the empirical mean squared error (MSE) of the fit $f$ to target $g$ as\n$$\nE(f, g) = \\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\n\n7. Define the expressivity boost ratio for a given target $g$ and positional encoding order $K$ as\n$$\nR(g, K) = \\frac{E\\big(f_{\\text{plain}}^\\star, g\\big)}{E\\big(f_{\\text{PE},K}^\\star, g\\big)},\n$$\nwhere $f_{\\text{plain}}^\\star \\in \\mathcal{F}_{\\text{plain}}$ and $f_{\\text{PE},K}^\\star \\in \\mathcal{F}_{\\text{PE},K}$ denote the least-squares minimizers in their respective classes. To avoid numerical division by zero in perfectly representable cases, use a floor $10^{-12}$ in the denominator, i.e., divide by $\\max\\big(E(f_{\\text{PE},K}^\\star, g), 10^{-12}\\big)$.\n\n8. Define a simple capacity proxy based on the maximum number of sign changes (zero-crossings) on $[0,1]$:\n   - For the baseline model $\\mathcal{F}_{\\text{plain}}$, the maximum number of sign changes is $S_{\\text{plain}} = 1$.\n   - For the positional encoding model $\\mathcal{F}_{\\text{PE},K}$ (a trigonometric polynomial of maximum angular frequency $2^K \\pi$), use the upper bound $S_{\\text{PE}}(K) = 2 \\cdot 2^K$.\n\nYour tasks:\n\nA) Implement the least-squares fits for $\\mathcal{F}_{\\text{plain}}$ and $\\mathcal{F}_{\\text{PE},K}$ to each target $g_1, g_2, g_3$ for $K \\in \\{0,1,3\\}$, and compute $R(g,K)$.\n\nB) Compute the sign-change capacity proxy $S_{\\text{plain}}$ and $S_{\\text{PE}}(K)$ for $K \\in \\{0,1,3\\}$.\n\nTest suite and required outputs:\n\n- Use $N = 4096$, $m = 3$, $K \\in \\{0,1,3\\}$, and the three targets $g_1, g_2, g_3$ as specified above.\n- Produce the single-line output containing the following $13$ values in this exact order:\n$\\big[ S_{\\text{plain}}, S_{\\text{PE}}(0), S_{\\text{PE}}(1), S_{\\text{PE}}(3), R(g_1,0), R(g_1,1), R(g_1,3), R(g_2,0), R(g_2,1), R(g_2,3), R(g_3,0), R(g_3,1), R(g_3,3) \\big]$.\nAll outputs are real numbers or integers without units. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[result_1, result_2, ...]` ).", "solution": "This problem requires us to quantify the increase in expressivity afforded by positional encoding features compared to a simple linear model. The core of the problem lies in function approximation using linear least squares. We will solve for the optimal parameters of two different model classes, $\\mathcal{F}_{\\text{plain}}$ and $\\mathcal{F}_{\\text{PE},K}$, to best fit three distinct target functions, and then compare their resulting approximation errors.\n\nThe fundamental principle is that both model classes, $\\mathcal{F}_{\\text{plain}}$ and $\\mathcal{F}_{\\text{PE},K}$, are linear in their parameters. A function $f(x; \\mathbf{w})$ is linear in its parameters $\\mathbf{w} = (w_1, \\ldots, w_p)^T$ if it can be written as a linear combination of basis functions $\\phi_j(x)$:\n$$\nf(x; \\mathbf{w}) = \\sum_{j=1}^{p} w_j \\phi_j(x) = \\mathbf{w}^T \\mathbf{\\phi}(x)\n$$\nThe goal of linear least squares is to find the parameter vector $\\mathbf{w}^\\star$ that minimizes the Mean Squared Error (MSE) over a set of $N$ data points $(x_i, g_i)$:\n$$\nE(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j=1}^{p} w_j \\phi_j(x_i) - g(x_i) \\right)^2\n$$\nIn matrix form, let $\\mathbf{g}$ be the vector of target values $g(x_i)$ and $\\mathbf{\\Phi}$ be the $N \\times p$ design matrix where $\\mathbf{\\Phi}_{ij} = \\phi_j(x_i)$. The MSE is $E(\\mathbf{w}) = \\frac{1}{N} \\| \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{g} \\|_2^2$. The optimal $\\mathbf{w}^\\star$ that minimizes this error can be found by solving the linear system known as the normal equations, $\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{g}$, or more robustly through methods like QR decomposition or Singular Value Decomposition (SVD), which are standard in numerical libraries.\n\nThe capacity proxies, $S_{\\text{plain}}$ and $S_{\\text{PE}}(K)$, are calculated directly from the provided formulae. $S_{\\text{plain}} = 1$ is fixed. For $K \\in \\{0, 1, 3\\}$, we calculate $S_{\\text{PE}}(K) = 2 \\cdot 2^K$.\n\nThe main computational task is to find the minimized errors $E(f^\\star, g)$ for each model and target.\n\n1.  **Define Grid and Targets**: We begin by creating the uniform grid of $N=4096$ points $x_i = \\frac{i}{N-1}$ on the interval $[0,1]$. Then, we evaluate the target functions $g_1(x) = \\sin(8\\pi x)$, $g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$, and $g_3(x) = x^2$ on this grid to obtain the target vectors $\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3$.\n\n2.  **Fit Baseline Model $\\mathcal{F}_{\\text{plain}}$**: The model is $f(x) = w_1 x + w_0$. The basis functions are $\\phi_1(x) = x$ and $\\phi_2(x) = 1$. The design matrix $\\mathbf{\\Phi}_{\\text{plain}}$ is an $N \\times 2$ matrix with the first column being the $x_i$ values and the second column being all ones. We solve the least-squares problem for each target $\\mathbf{g}_j$ to find the minimized MSE, $E(f_{\\text{plain}}^\\star, g_j)$.\n\n3.  **Fit Positional Encoding Model $\\mathcal{F}_{\\text{PE},K}$**: For each order $K \\in \\{0, 1, 3\\}$, the model is a trigonometric polynomial. The basis functions are $\\{\\sin(2^k\\pi x), \\cos(2^k\\pi x)\\}_{k=0}^K$ plus a constant basis function $\\phi(x)=1$. The total number of parameters (and basis functions) is $p = 2(K+1) + 1$. For each $K$, we construct the corresponding $N \\times p$ design matrix $\\mathbf{\\Phi}_{\\text{PE},K}$ by evaluating these basis functions on the grid. For each target $\\mathbf{g}_j$, we solve the least-squares problem to find the minimized MSE, $E(f_{\\text{PE},K}^\\star, g_j)$.\n\n4.  **Compute Expressivity Boost Ratios**: With the minimal MSEs computed for both model classes, we calculate the ratio $R(g, K)$ for each target $g_j$ and order $K$:\n    $$\n    R(g_j, K) = \\frac{E(f_{\\text{plain}}^\\star, g_j)}{\\max\\big(E(f_{\\text{PE},K}^\\star, g_j), 10^{-12}\\big)}\n    $$\n    This ratio measures how many times smaller the error of the positional encoding model is compared to the baseline linear model. A large ratio indicates a significant expressivity advantage. The denominator is floored to prevent division by zero, which is likely to occur when the model class $\\mathcal{F}_{\\text{PE},K}$ can perfectly represent the target function (e.g., for $g_1, g_2$ when $K=3$).\n\nThe algorithm proceeds by first calculating the capacity proxies. Then, for each of the three target functions, it calculates the baseline error $E(f_{\\text{plain}}^\\star, g)$. Subsequently, for each $K \\in \\{0, 1, 3\\}$, it calculates the positional encoding model error $E(f_{\\text{PE},K}^\\star, g)$ and the corresponding ratio $R(g, K)$. Finally, all computed values are assembled into a single list in the specified order.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes capacity proxies and expressivity boost ratios for shallow models\n    with and without positional encoding.\n    \"\"\"\n    \n    # --- Problem Parameters ---\n    N = 4096\n    m = 3\n    K_values = [0, 1, 3]\n\n    # --- Step 1: Define Grid and Target Functions ---\n    x_grid = np.linspace(0, 1, N, dtype=np.float64)\n    \n    # g1(x) = sin(2^3 * pi * x) = sin(8 * pi * x)\n    g1 = np.sin(2**m * np.pi * x_grid)\n    \n    # g2(x) = 0.5*sin(2*pi*x) + 0.25*cos(4*pi*x) + 0.2*sin(8*pi*x)\n    g2 = (0.5 * np.sin(2 * np.pi * x_grid) + \n          0.25 * np.cos(4 * np.pi * x_grid) + \n          0.2 * np.sin(8 * np.pi * x_grid))\n          \n    # g3(x) = x^2\n    g3 = x_grid**2\n    \n    targets = {\n        'g1': g1,\n        'g2': g2,\n        'g3': g3\n    }\n    target_names = ['g1', 'g2', 'g3']\n\n    # --- Task B: Compute Capacity Proxies ---\n    S_plain = 1\n    S_PE = {K: 2 * (2**K) for K in K_values}\n\n    def get_mse(X, y):\n        \"\"\"\n        Solves the linear least-squares problem and returns the mean squared error.\n        \n        A robust method for calculating MSE is used by finding the optimal weights `w`\n        and then explicitly computing the error `mean((X@w - y)**2)`. This handles\n        cases of perfect fits where the 'residuals' output of np.linalg.lstsq\n        can be an empty array.\n        \"\"\"\n        w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        y_pred = X @ w\n        mse = np.mean((y - y_pred)**2)\n        return mse\n\n    # --- Fit Baseline Model (F_plain) ---\n    X_plain = np.vstack([x_grid, np.ones(N)]).T\n    mse_plain_results = {}\n    for name in target_names:\n        mse_plain_results[name] = get_mse(X_plain, targets[name])\n        \n    # --- Fit Positional Encoding Model (F_PE,K) ---\n    mse_pe_results = {name: {} for name in target_names}\n    for K in K_values:\n        # Construct the design matrix X_PE for the given K\n        features = []\n        for k in range(K + 1):\n            freq = 2**k\n            features.append(np.sin(freq * np.pi * x_grid))\n            features.append(np.cos(freq * np.pi * x_grid))\n        features.append(np.ones(N)) # Bias term\n        X_pe = np.vstack(features).T\n        \n        # Calculate MSE for each target\n        for name in target_names:\n            mse_pe_results[name][K] = get_mse(X_pe, targets[name])\n            \n    # --- Task A: Compute Expressivity Boost Ratios (R) ---\n    R_results = {name: {} for name in target_names}\n    for name in target_names:\n        for K in K_values:\n            numerator = mse_plain_results[name]\n            denominator = max(mse_pe_results[name][K], 1e-12)\n            R_results[name][K] = numerator / denominator\n\n    # --- Assemble Final Output in the specified order ---\n    final_output = []\n    # Capacity Proxies\n    final_output.append(S_plain)\n    final_output.append(S_PE[0])\n    final_output.append(S_PE[1])\n    final_output.append(S_PE[3])\n    \n    # Ratios for g1, g2, g3\n    for name in target_names:\n        for K in K_values:\n            final_output.append(R_results[name][K])\n            \n    # Print the formatted output\n    print(f\"[{','.join(f'{v:.6f}' for v in final_output)}]\")\n\nsolve()\n```", "id": "3098829"}]}