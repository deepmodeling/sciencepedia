{"hands_on_practices": [{"introduction": "The cross-entropy loss is a cornerstone of modern classification models, yet its geometric properties are often taken for granted. This exercise moves beyond its surface-level application, inviting you to analyze how a simple transformation—logit normalization—can fundamentally alter the loss landscape. By deriving the loss's invariance to the magnitude of the logit vector [@problem_id:3145403], you will gain a deeper intuition for concepts like model calibration and loss sharpness, revealing how seemingly minor design choices can have profound implications for a model's training dynamics.", "problem": "Consider a multiclass classifier with $K$ classes that produces a logit vector $z \\in \\mathbb{R}^{K}$. The predicted class probabilities are given by the softmax function, defined by $p_{i}(z) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}$ for $i \\in \\{1, \\dots, K\\}$. Given a true class index $y \\in \\{1, \\dots, K\\}$ and a one-hot target, the cross-entropy (CE) loss is defined as $L(z,y) = -\\ln\\big(p_{y}(z)\\big)$. Let $\\|\\cdot\\|_{2}$ denote the Euclidean norm (L2 norm). Define the normalized logits $\\hat{z} = \\frac{z}{\\|z\\|_{2}}$ for any nonzero $z$, and the corresponding normalized cross-entropy loss $\\hat{L}(z,y) = -\\ln\\left(\\frac{\\exp\\left(\\hat{z}_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left(\\hat{z}_{j}\\right)}\\right)$. Starting from the definitions of the softmax function and cross-entropy loss, and the basic property $\\|c z\\|_{2} = |c| \\|z\\|_{2}$ for any scalar $c \\in \\mathbb{R}$, analyze the effect of replacing $z$ by $\\frac{z}{\\|z\\|_{2}}$ on loss invariance to positive logit scaling. Specifically, for $c>0$, derive from first principles whether $\\hat{L}(c z,y)$ equals $\\hat{L}(z,y)$ and explain the implications of this invariance for a standard notion of sharpness along the scaling direction. To make the sharpness concrete, define the scaling-direction sharpness of the normalized loss as $S(z,y) = \\left.\\frac{d^{2}}{dc^{2}} \\hat{L}(c z,y)\\right|_{c=1}$.\n\nUsing the setup above, take $K=3$, $y=1$, and $z = (2, 0, -1)^{\\top}$ (a nonzero vector). Based on your derivation, compute the value of $S(z,y)$. Your final answer must be a single real number. No rounding is required.", "solution": "The problem statement is first validated according to the required procedure.\n\n### Step 1: Extract Givens\n- Number of classes: $K$.\n- Logit vector: $z \\in \\mathbb{R}^{K}$.\n- Softmax function: $p_{i}(z) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}$ for $i \\in \\{1, \\dots, K\\}$.\n- True class index: $y \\in \\{1, \\dots, K\\}$.\n- Cross-entropy (CE) loss: $L(z,y) = -\\ln\\big(p_{y}(z)\\big)$.\n- Euclidean norm (L2 norm): $\\|\\cdot\\|_{2}$.\n- Normalized logits: $\\hat{z} = \\frac{z}{\\|z\\|_{2}}$ for any nonzero $z$.\n- Normalized cross-entropy loss: $\\hat{L}(z,y) = -\\ln\\left(\\frac{\\exp\\left(\\hat{z}_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left(\\hat{z}_{j}\\right)}\\right)$.\n- Property of norm: $\\|c z\\|_{2} = |c| \\|z\\|_{2}$ for any scalar $c \\in \\mathbb{R}$.\n- Scaling factor: $c > 0$.\n- Scaling-direction sharpness: $S(z,y) = \\left.\\frac{d^{2}}{dc^{2}} \\hat{L}(c z,y)\\right|_{c=1}$.\n- Specific values for computation: $K=3$, $y=1$, $z = (2, 0, -1)^{\\top}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically grounded. It uses standard definitions from the field of deep learning, including the softmax function, cross-entropy loss, and L2 normalization. The premises and definitions are mathematically consistent and free of contradictions. The objective is clearly stated: to perform a specific derivation and a subsequent calculation. All information required for the derivation and calculation is provided. The vector $z$ is specified as nonzero, which is required for the normalization $\\frac{z}{\\|z\\|_{2}}$ to be defined. The condition $c>0$ is important for the identity $\\|cz\\|_2 = c\\|z\\|_2$. The problem is objective, formal, and poses a non-trivial but solvable question about the properties of a loss function, which is a standard topic of study.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution Derivation\nThe problem requires us to analyze the invariance of the normalized cross-entropy loss $\\hat{L}(z,y)$ with respect to positive scaling of the logits $z$, and then to compute a measure of sharpness in the scaling direction.\n\nFirst, let us analyze the effect of scaling the logit vector $z$ by a positive constant $c > 0$ on the normalized logits. Let the scaled logit vector be $z' = cz$. The normalized version of $z'$ is, by definition:\n$$\n\\widehat{z'} = \\frac{z'}{\\|z'\\|_{2}}\n$$\nSubstituting $z' = cz$ and using the provided property of the Euclidean norm, $\\|cz\\|_{2} = |c|\\|z\\|_{2}$:\n$$\n\\widehat{z'} = \\frac{cz}{\\|cz\\|_{2}} = \\frac{cz}{|c|\\|z\\|_{2}}\n$$\nSince the problem states that $c > 0$, we have $|c|=c$. The expression simplifies to:\n$$\n\\widehat{z'} = \\frac{cz}{c\\|z\\|_{2}} = \\frac{z}{\\|z\\|_{2}}\n$$\nThe right-hand side is the definition of the normalized logits $\\hat{z}$ for the original vector $z$. Thus, we have shown that for any nonzero $z \\in \\mathbb{R}^K$ and any $c>0$:\n$$\n\\widehat{cz} = \\hat{z}\n$$\nThis means that the normalization step makes the resulting vector invariant to positive scaling of the input vector. All vectors on the ray $\\{cz \\mid c > 0\\}$ are mapped to the same point $\\hat{z}$ on the unit hypersphere.\n\nNext, we examine the normalized cross-entropy loss, $\\hat{L}(cz,y)$. The definition of the function $\\hat{L}(\\cdot, y)$ states that it first normalizes its input vector and then computes the cross-entropy loss. So, for the input vector $cz$, the loss is computed using the normalized vector $\\widehat{cz}$.\n$$\n\\hat{L}(cz,y) = -\\ln\\left(\\frac{\\exp\\left((\\widehat{cz})_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left((\\widehat{cz})_{j}\\right)}\\right)\n$$\nSince we found that $\\widehat{cz} = \\hat{z}$, we can substitute this into the loss function:\n$$\n\\hat{L}(cz,y) = -\\ln\\left(\\frac{\\exp\\left(\\hat{z}_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left(\\hat{z}_{j}\\right)}\\right)\n$$\nThe expression on the right is precisely the definition of $\\hat{L}(z,y)$. Therefore, we conclude that:\n$$\n\\hat{L}(cz,y) = \\hat{L}(z,y)\n$$\nThe normalized cross-entropy loss is invariant to positive scaling of the logits.\n\nThe implication for sharpness is direct. The scaling-direction sharpness $S(z,y)$ is defined as the second derivative of the loss with respect to the scaling factor $c$, evaluated at $c=1$. Let us define a function $g(c) = \\hat{L}(cz, y)$ for $c>0$. From our derivation, we know that $g(c) = \\hat{L}(z, y)$, which is a constant value for fixed $z$ and $y$. The function $g(c)$ has no dependence on $c$.\n\nThe first derivative of a constant function is zero:\n$$\n\\frac{d}{dc} g(c) = \\frac{d}{dc} [\\hat{L}(z,y)] = 0\n$$\nThe second derivative is also zero:\n$$\n\\frac{d^2}{dc^2} g(c) = \\frac{d}{dc} [0] = 0\n$$\nThis holds for any $c>0$. The sharpness is this second derivative evaluated at $c=1$:\n$$\nS(z,y) = \\left.\\frac{d^{2}}{dc^{2}} \\hat{L}(c z,y)\\right|_{c=1} = 0\n$$\nThe implication is that the loss landscape of the normalized loss function is perfectly flat along any direction of scaling from the origin. The sharpness, a measure of curvature, is zero in this direction. This is a direct consequence of the normalization step, which removes any information about the magnitude of the logit vector.\n\nFinally, we are asked to compute the value of $S(z,y)$ for the specific case where $K=3$, $y=1$, and $z = (2, 0, -1)^{\\top}$.\nOur derivation that $S(z,y)=0$ is general and holds for any number of classes $K$, any true class index $y$, and any nonzero logit vector $z$. The given vector $z = (2, 0, -1)^{\\top}$ is nonzero, since its norm is $\\|z\\|_2 = \\sqrt{2^2 + 0^2 + (-1)^2} = \\sqrt{4+0+1} = \\sqrt{5} \\neq 0$.\nTherefore, the general result applies directly to this specific case without needing to substitute the numerical values of $z$, $K$, or $y$ into any complex expression. The result is determined entirely by the functional form of the normalized loss.\n\nThus, the value of the scaling-direction sharpness is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3145403"}, {"introduction": "In many real-world applications, the ultimate metric of success, such as the $F_1$-score, is non-differentiable and cannot be optimized directly with gradient-based methods. This practice tackles this common challenge by guiding you through the principled design of a *smooth surrogate loss*. You will learn to construct the Tversky loss, a differentiable proxy for $F_1$-like metrics [@problem_id:3145404], providing you with a powerful technique to align your training objective more closely with the true evaluation criteria of your task.", "problem": "You are training a binary classifier with network outputs that produce scores $s_{i} \\in (0,1)$ interpreted as the probability that the $i$-th example is positive. The training set consists of $n$ labeled examples with ground-truth labels $y_{i} \\in \\{0,1\\}$. The goal is to understand why directly optimizing the $F_{1}$-score is difficult, and to construct and analyze a smooth surrogate loss based on the Tversky index. Throughout, use only the following fundamental base: the definitions of true positives, false positives, and false negatives in terms of a decision threshold; the definition of precision and recall as ratios of these counts; and the definition of $F_{1}$ as the harmonic mean of precision and recall.\n\n1) Using only the definitions of precision, recall, and $F_{1}$, argue from first principles why treating $s_{i}$ as continuous network outputs and converting them to hard decisions with a threshold $t \\in (0,1)$ makes the empirical $F_{1}$-score a non-decomposable and almost-everywhere non-differentiable function of the score vector $\\mathbf{s} = (s_{1},\\dots,s_{n})$. Your argument must begin from the definitions of the confusion-matrix counts in terms of $\\mathbf{1}[s_{i} \\ge t]$ and avoid any special-purpose optimization formulas.\n\n2) To obtain a smooth surrogate, construct a differentiable replacement for the confusion-matrix counts by replacing the indicator functions with their soft counterparts built from $s_{i}$ and $1 - s_{i}$. Introduce nonnegative weights $\\alpha$ and $\\beta$ that scale the relative contributions of false positives and false negatives, respectively. From these ingredients, derive a differentiable expression for a soft Tversky index over a mini-batch and define the associated Tversky loss as one minus this index. Introduce a small $\\epsilon \\ge 0$ to ensure the denominator is nonzero, but do not substitute numerical values at this step.\n\n3) Differentiate your Tversky loss with respect to a single score $s_{k}$ to obtain a closed-form expression for $\\frac{\\partial \\mathcal{L}}{\\partial s_{k}}$ in terms of the batchwise soft counts, $\\alpha$, $\\beta$, and $\\epsilon$. Simplify your expression to show explicitly how the sign and magnitude depend on whether $y_{k} = 1$ or $y_{k} = 0$.\n\n4) Let $n = 4$, the labels be $y = (1,0,1,0)$, and the scores be $s = (0.8,0.6,0.4,0.3)$. Evaluate your gradient at this batch for $\\alpha = 0.3$, $\\beta = 0.7$, and $\\epsilon = 0$, and report the four components $\\left(\\frac{\\partial \\mathcal{L}}{\\partial s_{1}},\\frac{\\partial \\mathcal{L}}{\\partial s_{2}},\\frac{\\partial \\mathcal{L}}{\\partial s_{3}},\\frac{\\partial \\mathcal{L}}{\\partial s_{4}}\\right)$ in exact form (no rounding).\n\n5) Now suppose the scores are calibrated probabilities in the sense that the ground-truth positivity probability for example $i$ equals $q_{i} = s_{i}$, and assume statistical independence across examples. For a fixed threshold $t \\in (0,1)$, define the surrogate expected counts by replacing each hard decision with its indicator $\\mathbf{1}[s_{i} \\ge t]$ and each unobserved label with its probability $q_{i}$. Using only these substitutions and the definitions of the counts, write the surrogate expected $F_{1}$ as the harmonic mean of the surrogate expected precision and recall, and reduce it to a single ratio of linear functions of the surrogate expected counts. For the given scores $s = (0.8,0.6,0.4,0.3)$, determine the threshold $t^{\\star}$ that maximizes this surrogate expected $F_{1}$ over all $t \\in (0,1)$; in the event of multiple maximizers, report the largest threshold among them. Your final answer must be a single row matrix containing the four gradient components from part 4 followed by this threshold, written in exact form. No rounding is required.", "solution": "We proceed step by step from the stated definitions.\n\n1) By definition, for a threshold $t \\in (0,1)$ and scores $s_{i}$, the hard prediction is $\\hat{y}_{i} = \\mathbf{1}[s_{i} \\ge t]$. The confusion-matrix counts are given by\n$$\nTP = \\sum_{i=1}^{n} \\mathbf{1}[y_{i} = 1] \\, \\mathbf{1}[s_{i} \\ge t], \\quad FP = \\sum_{i=1}^{n} \\mathbf{1}[y_{i} = 0] \\, \\mathbf{1}[s_{i} \\ge t], \\quad FN = \\sum_{i=1}^{n} \\mathbf{1}[y_{i} = 1] \\, \\mathbf{1}[s_{i} < t].\n$$\nPrecision is $P = \\frac{TP}{TP + FP}$ (defined when $TP + FP > 0$), recall is $R = \\frac{TP}{TP + FN}$ (defined when $TP + FN > 0$), and the $F_{1}$-score is the harmonic mean $F_{1} = \\frac{2PR}{P + R}$, equivalently $F_{1} = \\frac{2TP}{2TP + FP + FN}$ when denominators are nonzero. As a function of $\\mathbf{s}$, each count is a sum of indicator functions $\\mathbf{1}[s_{i} \\ge t]$, which are discontinuous step functions of $s_{i}$ and are constant except at $s_{i} = t$. Consequently, $TP$, $FP$, and $FN$ are piecewise constant functions of $\\mathbf{s}$, and so is $F_{1}$, with jump discontinuities occurring on the hyperplanes $s_{i} = t$. Therefore, with respect to the continuous outputs $\\mathbf{s}$, the empirical $F_{1}$-score is non-decomposable across examples (due to dependence on global counts) and almost everywhere non-differentiable (the gradient is zero wherever it is defined and undefined on the decision boundaries), making direct gradient-based optimization ill-posed.\n\n2) To build a differentiable surrogate, replace the hard contributions with soft ones. Define soft counterparts of the confusion-matrix counts over a mini-batch as\n$$\n\\widetilde{TP} = \\sum_{i=1}^{n} y_{i} s_{i}, \\quad \\widetilde{FP} = \\sum_{i=1}^{n} (1 - y_{i}) s_{i}, \\quad \\widetilde{FN} = \\sum_{i=1}^{n} y_{i} (1 - s_{i}).\n$$\nIntroduce nonnegative weights $\\alpha \\ge 0$ and $\\beta \\ge 0$ to control the relative penalties for $\\widetilde{FP}$ and $\\widetilde{FN}$, and define a soft Tversky index as\n$$\n\\mathrm{TI}(\\mathbf{s},\\mathbf{y};\\alpha,\\beta,\\epsilon) = \\frac{\\widetilde{TP}}{\\widetilde{TP} + \\alpha \\, \\widetilde{FP} + \\beta \\, \\widetilde{FN} + \\epsilon},\n$$\nwhere $\\epsilon \\ge 0$ is a small constant ensuring the denominator is nonzero. The associated Tversky loss is\n$$\n\\mathcal{L}(\\mathbf{s},\\mathbf{y};\\alpha,\\beta,\\epsilon) = 1 - \\mathrm{TI}(\\mathbf{s},\\mathbf{y};\\alpha,\\beta,\\epsilon) = 1 - \\frac{\\widetilde{TP}}{\\widetilde{TP} + \\alpha \\, \\widetilde{FP} + \\beta \\, \\widetilde{FN} + \\epsilon}.\n$$\nThis construction is differentiable in $\\mathbf{s}$ for $\\epsilon > 0$, and remains well-defined even for $\\epsilon = 0$ provided the denominator is nonzero.\n\n3) Let\n$$\nN = \\widetilde{TP} = \\sum_{i=1}^{n} y_{i} s_{i}, \\quad A = \\widetilde{FP} = \\sum_{i=1}^{n} (1 - y_{i}) s_{i}, \\quad B = \\widetilde{FN} = \\sum_{i=1}^{n} y_{i} (1 - s_{i}),\n$$\nand denote the denominator by\n$$\nD = N + \\alpha A + \\beta B + \\epsilon.\n$$\nThen the loss is $\\mathcal{L} = 1 - \\frac{N}{D}$. Differentiate with respect to $s_{k}$. First note the partial derivatives of the soft counts:\n$$\n\\frac{\\partial N}{\\partial s_{k}} = y_{k}, \\quad \\frac{\\partial A}{\\partial s_{k}} = 1 - y_{k}, \\quad \\frac{\\partial B}{\\partial s_{k}} = - y_{k}.\n$$\nBy the chain rule,\n$$\n\\frac{\\partial D}{\\partial s_{k}} = \\frac{\\partial N}{\\partial s_{k}} + \\alpha \\frac{\\partial A}{\\partial s_{k}} + \\beta \\frac{\\partial B}{\\partial s_{k}} = y_{k} + \\alpha (1 - y_{k}) - \\beta y_{k} = y_{k} (1 - \\beta) + \\alpha (1 - y_{k}).\n$$\nUsing the quotient rule for $-N/D$, we have\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} = - \\frac{ \\left(\\frac{\\partial N}{\\partial s_{k}}\\right) D - N \\left(\\frac{\\partial D}{\\partial s_{k}}\\right)}{D^{2}} = - \\frac{ y_{k} D - N \\left( y_{k} (1 - \\beta) + \\alpha (1 - y_{k}) \\right)}{D^{2}}.\n$$\nThis simplifies casewise as\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} =\n\\begin{cases}\n- \\dfrac{ D - N (1 - \\beta) }{ D^{2} }, & \\text{if } y_{k} = 1, \\\\\n\\dfrac{ N \\alpha }{ D^{2} }, & \\text{if } y_{k} = 0.\n\\end{cases}\n$$\nThus, for positive labels ($y_{k} = 1$) the gradient component is negative when $D > N (1 - \\beta)$, pushing $s_{k}$ upward to increase overlap; for negative labels ($y_{k} = 0$) the component is nonnegative and proportional to $N \\alpha$, pushing $s_{k}$ downward to reduce false positives.\n\n4) For $y = (1,0,1,0)$ and $s = (0.8,0.6,0.4,0.3)$ with $\\alpha = 0.3$, $\\beta = 0.7$, and $\\epsilon = 0$, compute\n$$\nN = \\sum_{i=1}^{4} y_{i} s_{i} = 0.8 + 0.4 = 1.2 = \\frac{6}{5},\n$$\n$$\nA = \\sum_{i=1}^{4} (1 - y_{i}) s_{i} = 0.6 + 0.3 = 0.9 = \\frac{9}{10},\n$$\n$$\nB = \\sum_{i=1}^{4} y_{i} (1 - s_{i}) = (1 - 0.8) + (1 - 0.4) = 0.2 + 0.6 = 0.8 = \\frac{4}{5}.\n$$\nThe denominator is\n$$\nD = N + \\alpha A + \\beta B = \\frac{6}{5} + \\frac{3}{10} \\cdot \\frac{9}{10} + \\frac{7}{10} \\cdot \\frac{4}{5} = \\frac{120}{100} + \\frac{27}{100} + \\frac{56}{100} = \\frac{203}{100}.\n$$\nFor indices with $y_{k} = 1$ (namely $k = 1,3$),\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} = - \\frac{ D - N (1 - \\beta) }{ D^{2} } = - \\frac{ \\frac{203}{100} - \\frac{6}{5} \\cdot \\frac{3}{10} }{ \\left( \\frac{203}{100} \\right)^{2} } = - \\frac{ \\frac{203}{100} - \\frac{18}{50} }{ \\frac{41209}{10000} } = - \\frac{ \\frac{203}{100} - \\frac{36}{100} }{ \\frac{41209}{10000} } = - \\frac{ \\frac{167}{100} }{ \\frac{41209}{10000} } = - \\frac{16700}{41209}.\n$$\nFor indices with $y_{k} = 0$ (namely $k = 2,4$),\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} = \\frac{ N \\alpha }{ D^{2} } = \\frac{ \\frac{6}{5} \\cdot \\frac{3}{10} }{ \\left( \\frac{203}{100} \\right)^{2} } = \\frac{ \\frac{18}{50} }{ \\frac{41209}{10000} } = \\frac{ \\frac{36}{100} }{ \\frac{41209}{10000} } = \\frac{3600}{41209}.\n$$\nTherefore,\n$$\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial s_{1}}, \\frac{\\partial \\mathcal{L}}{\\partial s_{2}}, \\frac{\\partial \\mathcal{L}}{\\partial s_{3}}, \\frac{\\partial \\mathcal{L}}{\\partial s_{4}} \\right) = \\left( - \\frac{16700}{41209}, \\frac{3600}{41209}, - \\frac{16700}{41209}, \\frac{3600}{41209} \\right).\n$$\n\n5) Under calibration, take $q_{i} = s_{i}$ and assume independence. For a threshold $t \\in (0,1)$, the surrogate expected counts are\n$$\n\\mathbb{E}[\\mathrm{TP}](t) = \\sum_{i=1}^{n} q_{i} \\, \\mathbf{1}[s_{i} \\ge t], \\quad \\mathbb{E}[\\mathrm{FP}](t) = \\sum_{i=1}^{n} (1 - q_{i}) \\, \\mathbf{1}[s_{i} \\ge t], \\quad \\mathbb{E}[\\mathrm{FN}](t) = \\sum_{i=1}^{n} q_{i} \\, \\mathbf{1}[s_{i} < t].\n$$\nDefine the surrogate expected $F_{1}$ by replacing the counts in the ratio with these expectations:\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\, \\mathbb{E}[\\mathrm{TP}](t)}{2 \\, \\mathbb{E}[\\mathrm{TP}](t) + \\mathbb{E}[\\mathrm{FP}](t) + \\mathbb{E}[\\mathrm{FN}](t)}.\n$$\nFor $s = (0.8,0.6,0.4,0.3)$ and $q = s$, $\\widetilde{F}_{1}(t)$ is piecewise constant between the distinct score values, changing only when $t$ crosses one of $\\{0.8, 0.6, 0.4, 0.3\\}$. Evaluate on the five regions determined by these breakpoints:\n\n(i) $t > 0.8$: no predicted positives. Then $\\mathbb{E}[\\mathrm{TP}] = 0$, $\\mathbb{E}[\\mathrm{FP}] = 0$, $\\mathbb{E}[\\mathrm{FN}] = 0.8 + 0.6 + 0.4 + 0.3 = 2.1$, so $\\widetilde{F}_{1}(t) = 0$.\n\n(ii) $0.8 \\ge t > 0.6$: predicted positives $\\{0.8\\}$. Then $\\mathbb{E}[\\mathrm{TP}] = 0.8$, $\\mathbb{E}[\\mathrm{FP}] = 1 - 0.8 = 0.2$, $\\mathbb{E}[\\mathrm{FN}] = 0.6 + 0.4 + 0.3 = 1.3$, so\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 0.8}{2 \\cdot 0.8 + 0.2 + 1.3} = \\frac{1.6}{3.1}.\n$$\n\n(iii) $0.6 \\ge t > 0.4$: predicted positives $\\{0.8, 0.6\\}$. Then $\\mathbb{E}[\\mathrm{TP}] = 0.8 + 0.6 = 1.4$, $\\mathbb{E}[\\mathrm{FP}] = (1 - 0.8) + (1 - 0.6) = 0.2 + 0.4 = 0.6$, $\\mathbb{E}[\\mathrm{FN}] = 0.4 + 0.3 = 0.7$, so\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 1.4}{2 \\cdot 1.4 + 0.6 + 0.7} = \\frac{2.8}{4.1}.\n$$\n\n(iv) $0.4 \\ge t > 0.3$: predicted positives $\\{0.8, 0.6, 0.4\\}$. Then $\\mathbb{E}[\\mathrm{TP}] = 0.8 + 0.6 + 0.4 = 1.8$, $\\mathbb{E}[\\mathrm{FP}] = (1 - 0.8) + (1 - 0.6) + (1 - 0.4) = 0.2 + 0.4 + 0.6 = 1.2$, $\\mathbb{E}[\\mathrm{FN}] = 0.3$, so\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 1.8}{2 \\cdot 1.8 + 1.2 + 0.3} = \\frac{3.6}{5.1}.\n$$\n\n(v) $0.3 \\ge t > 0$: predicted positives $\\{0.8, 0.6, 0.4, 0.3\\}$. Then $\\mathbb{E}[\\mathrm{TP}] = 0.8 + 0.6 + 0.4 + 0.3 = 2.1$, $\\mathbb{E}[\\mathrm{FP}] = (1 - 0.8) + (1 - 0.6) + (1 - 0.4) + (1 - 0.3) = 0.2 + 0.4 + 0.6 + 0.7 = 1.9$, $\\mathbb{E}[\\mathrm{FN}] = 0$, so\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 2.1}{2 \\cdot 2.1 + 1.9 + 0} = \\frac{4.2}{6.1}.\n$$\nComparing the exact values,\n$$\n\\frac{1.6}{3.1} \\approx 0.516129, \\quad \\frac{2.8}{4.1} \\approx 0.682927, \\quad \\frac{3.6}{5.1} \\approx 0.705882, \\quad \\frac{4.2}{6.1} \\approx 0.688525,\n$$\nthe maximum is attained on region (iv), i.e., for any threshold $t$ with $0.4 \\ge t > 0.3$. By the tie-breaking rule selecting the largest maximizing threshold, we report $t^{\\star} = 0.4 = \\frac{2}{5}$.\n\nCollecting the requested quantities in the specified order, the final answer is the row matrix consisting of the four gradient components followed by $t^{\\star}$, all in exact form.", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{16700}{41209} & \\frac{3600}{41209} & -\\frac{16700}{41209} & \\frac{3600}{41209} & \\frac{2}{5}\\end{pmatrix}}$$", "id": "3145404"}, {"introduction": "Deep learning is increasingly being applied to tasks that go beyond simple classification or regression, such as predicting unordered sets of objects. This exercise introduces the challenge of permutation invariance, where the loss function must be insensitive to the ordering of the model's outputs. You will learn to build such a loss by combining a bipartite matching procedure with standard loss components like Cross-Entropy and MSE [@problem_id:3145459], a powerful principle that underlies state-of-the-art models in domains like object detection.", "problem": "Consider a set prediction task in which a model outputs an unordered set of two elements. Each element consists of a scalar coordinate and a binary class probability. The unordered target set also contains two elements, each with a scalar coordinate and a binary class label. The training objective must be permutation-invariant with respect to the order of elements in the set of predictions. Starting from the principle that a set has no inherent ordering and the assignment problem formulation that seeks to pair predictions to targets by minimizing a total cost, derive a permutation-invariant loss by combining the binary Cross Entropy (CE) for classification and a Mean Squared Error (MSE) for the coordinate regression.\n\nUse the following concrete instance:\n- Predicted element $1$ has probability $\\hat{p}_1 = 0.1$ for class $1$ and coordinate $\\hat{z}_1(t) = t$, where $t \\in \\mathbb{R}$ is a scalar parameter.\n- Predicted element $2$ has probability $\\hat{p}_2 = 0.8$ for class $1$ and coordinate $\\hat{z}_2(t) = \\frac{6}{5}$.\n- Target element $a$ has class label $c_a = 0$ and coordinate $z_a^{\\star} = 0$.\n- Target element $b$ has class label $c_b = 1$ and coordinate $z_b^{\\star} = 1$.\n\nDefine the per-pair loss as the sum of a binary Cross Entropy (CE) classification term and a coordinate regression term given by a Mean Squared Error (MSE) with a factor of $\\frac{1}{2}$:\n- For a predicted probability $\\hat{p}$ and target label $c \\in \\{0,1\\}$, the binary Cross Entropy is $-\\big(c \\ln(\\hat{p}) + (1-c) \\ln(1-\\hat{p})\\big)$.\n- For a predicted coordinate $\\hat{z}$ and target coordinate $z^{\\star}$, the regression term is $\\frac{1}{2}(\\hat{z} - z^{\\star})^{2}$.\n\nTasks:\n1. Using the assignment problem perspective and the definition of permutation invariance for sets, derive the permutation-invariant total loss $L(t)$ as the minimum, over all assignments of the two predictions to the two targets, of the sum of the two per-pair losses.\n2. In the $2 \\times 2$ case described, write $L(t)$ explicitly as the minimum of two candidate totals (the two possible one-to-one assignments).\n3. Determine the critical value $t^{\\star}$ at which the optimal assignment changes.\n4. Compute the one-sided derivatives $\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}-0}$ and $\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}+0}$, and then report the magnitude of the discontinuity in the gradient at $t^{\\star}$, namely $\\left|\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}+0} - \\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}-0}\\right|$.\n\nExpress the final requested quantity as an exact value. No rounding is required. The final answer must be a single real number.", "solution": "The problem requires constructing a permutation-invariant loss function for a set prediction task and analyzing its derivative at a point where the optimal assignment between predictions and targets changes. The total loss $L(t)$ is formulated as the solution to an assignment problem, specifically by taking the minimum of the total costs over all possible permutations (one-to-one assignments) of predictions to targets.\n\nLet the set of predictions be $\\hat{Y} = \\{\\hat{y}_1, \\hat{y}_2\\}$ and the set of targets be $Y^{\\star} = \\{y_a^{\\star}, y_b^{\\star}\\}$.\nThe predictions are given as:\n$\\hat{y}_1 = (\\hat{z}_1, \\hat{p}_1) = (t, 0.1)$\n$\\hat{y}_2 = (\\hat{z}_2, \\hat{p}_2) = (\\frac{6}{5}, 0.8)$\n\nThe targets are given as:\n$y_a^{\\star} = (z_a^{\\star}, c_a) = (0, 0)$\n$y_b^{\\star} = (z_b^{\\star}, c_b) = (1, 1)$\n\nThe per-pair loss, or cost, of assigning a prediction $\\hat{y} = (\\hat{z}, \\hat{p})$ to a target $y^{\\star} = (z^{\\star}, c)$ is the sum of a binary Cross Entropy (CE) term and a Mean Squared Error (MSE) term:\n$C(\\hat{y}, y^{\\star}) = L_{cls}(\\hat{p}, c) + L_{reg}(\\hat{z}, z^{\\star})$\nwhere $L_{cls}(\\hat{p}, c) = -\\big(c \\ln(\\hat{p}) + (1-c) \\ln(1-\\hat{p})\\big)$ and $L_{reg}(\\hat{z}, z^{\\star}) = \\frac{1}{2}(\\hat{z} - z^{\\star})^{2}$.\n\nWe first compute the four possible per-pair costs, denoted by $C_{ij}$ for assigning prediction $i$ to target $j$.\n\n1.  Cost of assigning $\\hat{y}_1$ to $y_a^{\\star}$ ($C_{1a}$):\n    $L_{cls}(\\hat{p}_1, c_a) = -(0 \\cdot \\ln(0.1) + (1-0) \\cdot \\ln(1-0.1)) = -\\ln(0.9) = \\ln(\\frac{1}{0.9}) = \\ln(\\frac{10}{9})$.\n    $L_{reg}(\\hat{z}_1, z_a^{\\star}) = \\frac{1}{2}(t - 0)^2 = \\frac{1}{2}t^2$.\n    $C_{1a}(t) = \\ln(\\frac{10}{9}) + \\frac{1}{2}t^2$.\n\n2.  Cost of assigning $\\hat{y}_2$ to $y_b^{\\star}$ ($C_{2b}$):\n    $L_{cls}(\\hat{p}_2, c_b) = -(1 \\cdot \\ln(0.8) + (1-1) \\cdot \\ln(1-0.8)) = -\\ln(0.8) = \\ln(\\frac{1}{0.8}) = \\ln(\\frac{5}{4})$.\n    $L_{reg}(\\hat{z}_2, z_b^{\\star}) = \\frac{1}{2}(\\frac{6}{5} - 1)^2 = \\frac{1}{2}(\\frac{1}{5})^2 = \\frac{1}{50}$.\n    $C_{2b} = \\ln(\\frac{5}{4}) + \\frac{1}{50}$.\n\n3.  Cost of assigning $\\hat{y}_1$ to $y_b^{\\star}$ ($C_{1b}$):\n    $L_{cls}(\\hat{p}_1, c_b) = -(1 \\cdot \\ln(0.1) + (1-1) \\cdot \\ln(1-0.1)) = -\\ln(0.1) = \\ln(10)$.\n    $L_{reg}(\\hat{z}_1, z_b^{\\star}) = \\frac{1}{2}(t - 1)^2$.\n    $C_{1b}(t) = \\ln(10) + \\frac{1}{2}(t - 1)^2$.\n\n4.  Cost of assigning $\\hat{y}_2$ to $y_a^{\\star}$ ($C_{2a}$):\n    $L_{cls}(\\hat{p}_2, c_a) = -(0 \\cdot \\ln(0.8) + (1-0) \\cdot \\ln(1-0.8)) = -\\ln(0.2) = \\ln(\\frac{1}{0.2}) = \\ln(5)$.\n    $L_{reg}(\\hat{z}_2, z_a^{\\star}) = \\frac{1}{2}(\\frac{6}{5} - 0)^2 = \\frac{1}{2} \\cdot \\frac{36}{25} = \\frac{18}{25}$.\n    $C_{2a} = \\ln(5) + \\frac{18}{25}$.\n\nFor a $2 \\times 2$ problem, there are two possible one-to-one assignments (permutations).\nAssignment 1: $(\\hat{y}_1 \\to y_a^{\\star}, \\hat{y}_2 \\to y_b^{\\star})$. The total loss is $L_1(t) = C_{1a}(t) + C_{2b}$.\n$L_1(t) = \\left(\\ln(\\frac{10}{9}) + \\frac{1}{2}t^2\\right) + \\left(\\ln(\\frac{5}{4}) + \\frac{1}{50}\\right) = \\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50}$.\n\nAssignment 2: $(\\hat{y}_1 \\to y_b^{\\star}, \\hat{y}_2 \\to y_a^{\\star})$. The total loss is $L_2(t) = C_{1b}(t) + C_{2a}$.\n$L_2(t) = \\left(\\ln(10) + \\frac{1}{2}(t-1)^2\\right) + \\left(\\ln(5) + \\frac{18}{25}\\right) = \\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}$.\n\nThe permutation-invariant total loss $L(t)$ is the minimum of the losses over all possible assignments.\n$L(t) = \\min(L_1(t), L_2(t))$. This completes Tasks 1 and 2.\n\nTo find the critical value $t^{\\star}$ where the optimal assignment changes (Task 3), we set $L_1(t) = L_2(t)$:\n$\\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50} = \\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}$\n$\\frac{1}{2}t^2 + \\ln(10) - \\ln(9) + \\ln(5) - \\ln(4) + \\frac{1}{50} = \\frac{1}{2}(t^2 - 2t + 1) + \\ln(10) + \\ln(5) + \\frac{36}{50}$\nCanceling common terms ($\\frac{1}{2}t^2, \\ln(10), \\ln(5)$) from both sides gives:\n$-\\ln(9) - \\ln(4) + \\frac{1}{50} = -t + \\frac{1}{2} + \\frac{36}{50}$\n$-\\ln(36) + \\frac{1}{50} = -t + \\frac{25}{50} + \\frac{36}{50}$\n$-\\ln(36) + \\frac{1}{50} = -t + \\frac{61}{50}$\nSolving for $t$:\n$t^{\\star} = \\ln(36) + \\frac{61}{50} - \\frac{1}{50} = \\ln(36) + \\frac{60}{50} = \\ln(36) + \\frac{6}{5}$.\n\nFor Task 4, we need to compute the one-sided derivatives of $L(t)$ at $t^{\\star}$ and the magnitude of the discontinuity in the gradient.\nThe derivatives of the two candidate losses with respect to $t$ are:\n$\\frac{\\partial L_1}{\\partial t} = \\frac{\\partial}{\\partial t} \\left(\\frac{1}{2}t^2 + \\text{constant}\\right) = t$.\n$\\frac{\\partial L_2}{\\partial t} = \\frac{\\partial}{\\partial t} \\left(\\frac{1}{2}(t-1)^2 + \\text{constant}\\right) = \\frac{1}{2} \\cdot 2(t-1) \\cdot 1 = t-1$.\n\nTo determine which branch of the minimum function is active, we analyze the sign of the difference $L_1(t) - L_2(t)$.\n$L_1(t) - L_2(t) = \\left(\\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50}\\right) - \\left(\\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}\\right)$\n$= \\frac{1}{2}t^2 - \\frac{1}{2}(t^2-2t+1) + (\\ln(10)-\\ln(9)+\\ln(5)-\\ln(4)+\\frac{1}{50}) - (\\ln(10)+\\ln(5)+\\frac{36}{50})$\n$= t - \\frac{1}{2} - \\ln(9) - \\ln(4) - \\frac{35}{50} = t - \\frac{1}{2} - \\ln(36) - \\frac{7}{10}$\n$= t - \\left(\\ln(36) + \\frac{5}{10} + \\frac{7}{10}\\right) = t - \\left(\\ln(36) + \\frac{12}{10}\\right) = t - \\left(\\ln(36) + \\frac{6}{5}\\right)$.\nSince $t^{\\star} = \\ln(36) + \\frac{6}{5}$, we have $L_1(t) - L_2(t) = t - t^{\\star}$.\n- If $t  t^{\\star}$, then $L_1(t) - L_2(t)  0$, which means $L_1(t)  L_2(t)$. The minimum is $L(t) = L_1(t)$.\n- If $t > t^{\\star}$, then $L_1(t) - L_2(t) > 0$, which means $L_2(t)  L_1(t)$. The minimum is $L(t) = L_2(t)$.\n\nThe total loss function is piecewise:\n$L(t) = \\begin{cases} L_1(t)  \\text{if } t \\le t^{\\star} \\\\ L_2(t)  \\text{if } t  t^{\\star} \\end{cases}$\n\nThe one-sided derivatives at $t=t^{\\star}$ are:\nThe left-sided derivative is the derivative of the active function for $t \\le t^{\\star}$:\n$\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}-0} = \\lim_{t \\to t^{\\star-}} \\frac{\\partial L}{\\partial t} = \\lim_{t \\to t^{\\star-}} \\frac{\\partial L_1}{\\partial t} = \\lim_{t \\to t^{\\star-}} t = t^{\\star}$.\n\nThe right-sided derivative is the derivative of the active function for $t  t^{\\star}$:\n$\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}+0} = \\lim_{t \\to t^{\\star+}} \\frac{\\partial L}{\\partial t} = \\lim_{t \\to t^{\\star+}} \\frac{\\partial L_2}{\\partial t} = \\lim_{t \\to t^{\\star+}} (t-1) = t^{\\star}-1$.\n\nThe magnitude of the discontinuity in the gradient at $t^{\\star}$ is the absolute difference between the right-sided and left-sided derivatives:\n$\\left|\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}+0} - \\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}-0}\\right| = \\left| (t^{\\star}-1) - t^{\\star} \\right| = |-1| = 1$.", "answer": "$$\\boxed{1}$$", "id": "3145459"}]}