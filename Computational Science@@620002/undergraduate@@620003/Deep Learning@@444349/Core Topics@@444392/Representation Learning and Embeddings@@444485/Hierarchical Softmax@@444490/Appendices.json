{"hands_on_practices": [{"introduction": "To truly grasp the implications of a tree-based structure, we must first understand that not all decisions are created equal. In hierarchical softmax, an incorrect turn near the root of the tree sends the prediction down an entirely wrong branch, leading to a large increase in loss. This exercise provides a clear, quantitative look at this phenomenon by comparing the impact of an error at the root versus an error deep within the tree, revealing a fundamental property of the hierarchical approach. [@problem_id:3134876]", "problem": "Consider a hierarchical softmax classifier whose label space is organized as a full binary tree of depth $D$, so that every leaf is exactly $D$ decisions away from the root. For a given training example whose true class corresponds to a specific leaf, the model’s prediction at each internal node on the true path outputs a probability $q \\in (0.5, 1)$ for the branch that moves toward the true leaf, and therefore a probability $1 - q$ for the opposite branch. The probability assigned to the true class by hierarchical softmax is the product of the conditional branch probabilities along the path from the root to the true leaf. The training loss is the Negative Log-Likelihood (NLL), defined as the negative natural logarithm of the probability assigned to the target class.\n\nAssume a single label error occurs at level $k$ along the path ($k=1$ for the root, $k=D$ for the deepest decision). A label error at level $k$ flips the branch decision at level $k$, and the target path then follows the corresponding wrong subtree to some leaf in that subtree. For the purpose of quantifying the expected loss increase caused by such a label error, assume that at every node in the wrong subtree, the model continues to assign probability $q$ to the branch that would move toward the true leaf (which is not the target under the label error) and probability $1 - q$ to the branch that moves away from the true leaf (which is the target under the label error). Under these assumptions, the NLL for the correctly labeled example uses $q$ at every level, whereas the NLL under the label error at level $k$ uses $q$ for the first $k-1$ levels and $1 - q$ for the remaining $D - k + 1$ levels.\n\nWith $D = 7$ and $q = 0.8$, compute the ratio of the expected NLL increase for a label error at the root ($k = 1$) to the expected NLL increase for a label error at the deepest level ($k = D$). Provide your final answer as a pure number; no rounding is required.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded within the field of deep learning, describing a theoretical model of hierarchical softmax. The problem is well-posed, providing a self-contained and consistent set of definitions, assumptions, and data ($D=7$, $q=0.8$) that allow for the calculation of a unique solution. The language is objective and precise, and the setup, while simplified, is not scientifically unsound, unrealistic, or trivial.\n\nLet $D$ be the depth of the full binary tree of the hierarchical softmax classifier. Let $q$ be the probability of selecting the correct branch at any internal node along the path to the true leaf class. The problem defines the training loss as the Negative Log-Likelihood (NLL), which is $L = -\\ln(P)$, where $P$ is the probability assigned by the model to a target class.\n\nFirst, we calculate the NLL for the correctly labeled example, which we denote as $L_{\\text{true}}$. The path from the root to the true leaf consists of $D$ decision steps. At each step, the model assigns a probability $q$ to the correct branch. The total probability for the true class, $P_{\\text{true}}$, is the product of these probabilities:\n$$P_{\\text{true}} = \\prod_{i=1}^{D} q = q^D$$\nThe corresponding NLL is thus:\n$$L_{\\text{true}} = -\\ln(P_{\\text{true}}) = -\\ln(q^D) = -D \\ln(q)$$\n\nNext, we consider the case where a label error occurs at level $k$, where $k=1$ corresponds to the root and $k=D$ corresponds to the deepest decision. For such an error, the target path follows the correct sequence of branches for the first $k-1$ levels, takes the wrong branch at level $k$, and then continues to a leaf in that incorrect subtree.\n\nThe problem provides a specific assumption for calculating the probability of the new, erroneous target leaf, $P_k$: the NLL calculation uses a probability of $q$ for the first $k-1$ levels and a probability of $1-q$ for the remaining $D-k+1$ levels. This implies that the probability $P_k$ is given by the product:\n$$P_k = \\left( \\prod_{i=1}^{k-1} q \\right) \\left( \\prod_{j=k}^{D} (1-q) \\right) = q^{k-1} (1-q)^{D-k+1}$$\nThe NLL for this erroneous label, $L_k$, is:\n$$L_k = -\\ln(P_k) = -\\ln\\left(q^{k-1} (1-q)^{D-k+1}\\right)$$\nUsing the properties of the natural logarithm, we can expand this expression:\n$$L_k = - \\left[ \\ln(q^{k-1}) + \\ln((1-q)^{D-k+1}) \\right] = - \\left[ (k-1)\\ln(q) + (D-k+1)\\ln(1-q) \\right]$$\n\nThe increase in NLL due to a label error at level $k$, denoted $\\Delta L_k$, is the difference between the NLL with the error and the NLL for the true label: $\\Delta L_k = L_k - L_{\\text{true}}$.\n$$\\Delta L_k = \\left( - (k-1)\\ln(q) - (D-k+1)\\ln(1-q) \\right) - \\left( -D\\ln(q) \\right)$$\n$$\\Delta L_k = D\\ln(q) - (k-1)\\ln(q) - (D-k+1)\\ln(1-q)$$\nWe can group the terms containing $\\ln(q)$:\n$$\\Delta L_k = (D - (k-1))\\ln(q) - (D-k+1)\\ln(1-q)$$\n$$\\Delta L_k = (D-k+1)\\ln(q) - (D-k+1)\\ln(1-q)$$\nFactoring out the common term $(D-k+1)$:\n$$\\Delta L_k = (D-k+1) \\left[ \\ln(q) - \\ln(1-q) \\right]$$\nThis simplifies to:\n$$\\Delta L_k = (D-k+1) \\ln\\left(\\frac{q}{1-q}\\right)$$\n\nThe problem asks for the ratio of the expected NLL increase for a label error at the root ($k=1$) to that for an error at the deepest level ($k=D$).\n\nFor a label error at the root, we set $k=1$:\n$$\\Delta L_{k=1} = (D-1+1) \\ln\\left(\\frac{q}{1-q}\\right) = D \\ln\\left(\\frac{q}{1-q}\\right)$$\n\nFor a label error at the deepest level, we set $k=D$:\n$$\\Delta L_{k=D} = (D-D+1) \\ln\\left(\\frac{q}{1-q}\\right) = (1) \\ln\\left(\\frac{q}{1-q}\\right) = \\ln\\left(\\frac{q}{1-q}\\right)$$\n\nNow, we compute the required ratio:\n$$\\text{Ratio} = \\frac{\\Delta L_{k=1}}{\\Delta L_{k=D}} = \\frac{D \\ln\\left(\\frac{q}{1-q}\\right)}{\\ln\\left(\\frac{q}{1-q}\\right)}$$\nThe problem specifies that $q \\in (0.5, 1)$, which ensures that $q \\neq 1-q$ and $\\frac{q}{1-q} > 1$. Therefore, the term $\\ln\\left(\\frac{q}{1-q}\\right)$ is a well-defined positive number and can be canceled from the numerator and denominator.\n$$\\text{Ratio} = D$$\n\nThe problem provides the specific values $D=7$ and $q=0.8$. The value of $q$ is not necessary to compute the ratio, but it serves to confirm that the logarithmic term is well-defined. Substituting the value of $D$:\n$$\\text{Ratio} = 7$$", "answer": "$$\\boxed{7}$$", "id": "3134876"}, {"introduction": "The structure of a hierarchical softmax tree is not just a computational shortcut; it encodes assumptions about the relationships between classes. By placing similar classes as siblings, we hypothesize that confusion is more likely to occur between them. This exercise explores the benefit of this structure by analyzing the model's resilience to label noise, comparing how hierarchical softmax localizes the impact of an error between sibling classes versus how a standard flat softmax handles the same error. [@problem_id:3134801]", "problem": "Consider a multi-class classification task trained with cross-entropy loss, comparing a flat softmax parameterization to a hierarchical softmax parameterization over a binary tree. Focus on a particular internal node whose two children are leaf classes, denoted by $y$ and $y'$, which are siblings. In the hierarchical softmax, let the model’s probability of reaching this internal node along the path from the root be $q \\in (0,1)$, and let the conditional probability of choosing $y$ versus $y'$ at this node be $s \\in (0,1)$ (so $y$ is chosen with probability $s$ and $y'$ with probability $1-s$). Hence the hierarchical softmax assigns leaf probabilities $p_{\\mathrm{hs}}(y)=q s$ and $p_{\\mathrm{hs}}(y')=q(1-s)$. In the flat softmax parameterization over all classes, let the model’s predicted probabilities for these same two leaves be $p_{\\mathrm{flat}}(y)=\\pi$ and $p_{\\mathrm{flat}}(y')=\\pi'$, with $\\pi,\\pi' \\in (0,1)$.\n\nAssume that at training time the observed label undergoes label noise: with probability $1-\\eta$ the true label $y$ is observed, and with probability $\\eta \\in (0,1)$ the label is replaced by its sibling $y'$. Training uses the negative log-likelihood (cross-entropy) loss for the observed label, namely $L(\\ell)=-\\ln p(\\ell)$, where $p(\\ell)$ denotes the model probability assigned to the observed label $\\ell$ under the given parameterization.\n\nDefine the loss increase due to label noise, for each parameterization, as the expected loss under the noisy labeling minus the clean loss for the true label $y$. Derive a closed-form analytic expression for the difference between these increases,\n$$\n\\Delta \\equiv \\big(\\mathbb{E}[L_{\\mathrm{hs}}(\\text{noisy})]-L_{\\mathrm{hs}}(\\text{clean }y)\\big)\\;-\\;\\big(\\mathbb{E}[L_{\\mathrm{flat}}(\\text{noisy})]-L_{\\mathrm{flat}}(\\text{clean }y)\\big),\n$$\nexpressed only in terms of $\\eta$, $s$, $\\pi$, and $\\pi'$. Provide your final answer as an exact expression without numerical approximation.", "solution": "The problem asks for the difference in the expected loss increase due to label noise between a hierarchical softmax (HS) and a flat softmax model. Let this difference be $\\Delta$. We can write $\\Delta = \\Delta_{\\text{hs}} - \\Delta_{\\text{flat}}$, where $\\Delta_{\\text{hs}}$ and $\\Delta_{\\text{flat}}$ are the respective loss increases.\n\n### 1. Loss Increase for Hierarchical Softmax\n\nThe loss for the clean (true) label $y$ is given by the negative log-likelihood:\n$$L_{\\text{hs}}(\\text{clean }y) = -\\ln(p_{\\text{hs}}(y)) = -\\ln(qs)$$\n\nUnder label noise, the observed label is $y$ with probability $1-\\eta$ and $y'$ with probability $\\eta$. The expected loss is:\n$$\n\\mathbb{E}[L_{\\text{hs}}(\\text{noisy})] = (1-\\eta) L_{\\text{hs}}(y) + \\eta L_{\\text{hs}}(y')\n= (1-\\eta) [-\\ln(qs)] + \\eta [-\\ln(q(1-s))]\n$$\n\nThe loss increase, $\\Delta_{\\text{hs}}$, is the difference between the expected noisy loss and the clean loss:\n$$\n\\Delta_{\\text{hs}} = \\mathbb{E}[L_{\\text{hs}}(\\text{noisy})] - L_{\\text{hs}}(\\text{clean }y)\n= \\big( (1-\\eta) [-\\ln(qs)] + \\eta [-\\ln(q(1-s))] \\big) - [-\\ln(qs)]\n$$\n$$\n\\Delta_{\\text{hs}} = -\\ln(qs) + \\eta\\ln(qs) - \\eta\\ln(q(1-s)) + \\ln(qs)\n= \\eta (\\ln(qs) - \\ln(q(1-s)))\n$$\nUsing the properties of logarithms, this simplifies to:\n$$\n\\Delta_{\\text{hs}} = \\eta \\ln\\left(\\frac{qs}{q(1-s)}\\right) = \\eta \\ln\\left(\\frac{s}{1-s}\\right)\n$$\n\n### 2. Loss Increase for Flat Softmax\n\nSimilarly, for the flat softmax model, the loss for the clean label $y$ is:\n$$L_{\\text{flat}}(\\text{clean }y) = -\\ln(p_{\\text{flat}}(y)) = -\\ln(\\pi)$$\n\nThe expected loss under the same noisy labeling scheme is:\n$$\n\\mathbb{E}[L_{\\text{flat}}(\\text{noisy})] = (1-\\eta) L_{\\text{flat}}(y) + \\eta L_{\\text{flat}}(y')\n= (1-\\eta) [-\\ln(\\pi)] + \\eta [-\\ln(\\pi')]\n$$\nThe loss increase for the flat softmax, $\\Delta_{\\text{flat}}$, is:\n$$\n\\Delta_{\\text{flat}} = \\mathbb{E}[L_{\\text{flat}}(\\text{noisy})] - L_{\\text{flat}}(\\text{clean }y)\n= \\big( (1-\\eta) [-\\ln(\\pi)] + \\eta [-\\ln(\\pi')] \\big) - [-\\ln(\\pi)]\n$$\n$$\n\\Delta_{\\text{flat}} = \\eta (\\ln(\\pi) - \\ln(\\pi')) = \\eta \\ln\\left(\\frac{\\pi}{\\pi'}\\right)\n$$\n\n### 3. Final Difference in Loss Increases\n\nThe final quantity $\\Delta$ is the difference between these two increases:\n$$\n\\Delta = \\Delta_{\\text{hs}} - \\Delta_{\\text{flat}} = \\eta \\ln\\left(\\frac{s}{1-s}\\right) - \\eta \\ln\\left(\\frac{\\pi}{\\pi'}\\right)\n$$\nFactoring out $\\eta$ and combining the logarithmic terms gives the final result:\n$$\n\\Delta = \\eta \\left[ \\ln\\left(\\frac{s}{1-s}\\right) - \\ln\\left(\\frac{\\pi}{\\pi'}\\right) \\right] = \\eta \\ln\\left( \\frac{s\\pi'}{(1-s)\\pi} \\right)\n$$", "answer": "$$\n\\boxed{\\eta \\ln\\left(\\frac{s \\pi'}{(1-s)\\pi}\\right)}\n$$", "id": "3134801"}, {"introduction": "Having explored the properties of a given hierarchy, we now address the most critical design question: how should the tree be constructed in the first place? The architecture of the tree itself dictates the model's performance, balancing computational speed against classification accuracy. This comprehensive practice guides you through implementing and evaluating two canonical tree-building strategies: one based on class frequency to optimize for speed (Huffman coding), and another based on semantic similarity to optimize for classification ease (k-means clustering), forcing a direct confrontation with this fundamental trade-off. [@problem_id:3134847]", "problem": "You are given a binary-tree-based hierarchical softmax classifier for multiclass prediction. A hierarchical softmax organizes a finite set of classes into the leaves of a binary tree, where each internal node makes a binary decision that routes an input to its left or right child until it reaches a leaf. For a fixed input feature vector $x \\in \\mathbb{R}^d$ and a fixed class $c$, the model defines the class probability as the product of the conditional probabilities encountered along the root-to-leaf path for class $c$. At each internal node $n$, a differentiable binary classifier computes a scalar score $z_n(x)$ and maps it to a probability of going to a specific child via the logistic function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The negative log-likelihood for one labeled example $(x, c)$ is the negative logarithm of the product of these routing probabilities, and the perplexity over a dataset is the exponential of the average negative log-likelihood.\n\nYou must compare two different heuristics for constructing the tree structure:\n- Frequency-based Huffman coding: Building a binary tree that minimizes the expected leaf depth under the class frequency distribution.\n- Clustering-based top-down $k$-means: Recursively cluster class embeddings into $k = 2$ clusters to form a binary tree, splitting until each leaf contains a single class.\n\nYour program must implement the following, starting from fundamental definitions of probability, expected values, and logistic binary classification:\n\n1) Tree-building heuristics:\n- Frequency-based Huffman tree: Given a discrete distribution over classes $\\{p(c)\\}_{c=1}^V$ where $V$ is the vocabulary size, build a binary tree using Huffman’s algorithm that greedily merges the two least probable nodes until a single root remains. Each merge creates a parent node whose children are the two merged nodes.\n- Clustering-based tree via top-down $2$-means: Given class embeddings $\\{e_c \\in \\mathbb{R}^d\\}_{c=1}^V$, recursively partition the current set of classes into two non-empty clusters using $k$-means with $k = 2$ and squared Euclidean distance, until each leaf contains exactly one class. For reproducibility, initialize the $k$-means centers deterministically at the classes with the minimum and maximum values along the first coordinate in the current subset, update by assignment and cluster mean recomputation, and stop when assignments do not change or after a fixed number of iterations. If a cluster becomes empty, reassign the farthest point from the non-empty cluster’s center to the empty cluster to maintain non-emptiness.\n\n2) Internal node classifiers:\n- For any internal node that splits its set of descendant classes into disjoint left and right subsets, compute a linear classifier from first principles by modeling a separating hyperplane halfway between the means of the class embeddings on each side. Let $m_L$ be the arithmetic mean of $\\{e_c: c \\in \\text{left}\\}$ and $m_R$ be the arithmetic mean of $\\{e_c: c \\in \\text{right}\\}$. Define $w = \\frac{m_R - m_L}{\\lVert m_R - m_L \\rVert_2}$ if $\\lVert m_R - m_L \\rVert_2 \\neq 0$ and $w = 0$ otherwise, and define $b = -\\frac{1}{2} w^\\top (m_L + m_R)$. For an input $x$, the probability of routing to the right child is $\\sigma(w^\\top x + b)$ and to the left child is $1 - \\sigma(w^\\top x + b)$.\n\n3) Class probability and negative log-likelihood:\n- For a labeled sample $(x, c)$, let the root-to-leaf path for class $c$ be the ordered sequence of internal nodes $\\{n_1, n_2, \\dots, n_{L(c)}\\}$, where at each node $n_\\ell$ the routing decision is either left or right depending on which child lies on the path to class $c$. The model probability is\n$$\nP(c \\mid x) \\;=\\; \\prod_{\\ell = 1}^{L(c)} P(\\text{child}_\\ell \\mid x, n_\\ell),\n$$\nwhere $P(\\text{right} \\mid x, n) = \\sigma(w_n^\\top x + b_n)$ and $P(\\text{left} \\mid x, n) = 1 - \\sigma(w_n^\\top x + b_n)$, with $(w_n, b_n)$ defined as above. The negative log-likelihood for $(x, c)$ is $-\\log P(c \\mid x)$, and the perplexity over a dataset $\\{(x_i, c_i)\\}_{i=1}^N$ is\n$$\n\\operatorname{ppl} \\;=\\; \\exp\\!\\left(\\frac{1}{N} \\sum_{i=1}^N \\big(-\\log P(c_i \\mid x_i)\\big)\\right).\n$$\n\n4) Latency proxy:\n- Define a latency proxy as the expected number of node evaluations per prediction, computed as the expected leaf depth under the true class distribution,\n$$\n\\mathbb{E}\\big[L(C)\\big] \\;=\\; \\sum_{c=1}^{V} p(c)\\,L(c),\n$$\nwhere $L(c)$ is the depth (number of internal nodes along the path) of the leaf for class $c$.\n\n5) Data generation:\n- Simulate labeled data by sampling class labels $c \\sim \\text{Categorical}(p)$ and then sampling feature vectors according to a Gaussian model $x \\sim \\mathcal{N}(e_c, \\sigma^2 I_d)$, where $I_d$ is the $d \\times d$ identity matrix and $\\sigma > 0$ is the standard deviation.\n\nYou must implement both trees, construct linear classifiers at internal nodes using the mean-difference hyperplane described above, generate the synthetic dataset, compute the average negative log-likelihood and perplexity for each tree, and compute the latency proxy (expected depth) for each tree.\n\nTest Suite and Answer Specification:\nImplement your program to run the following three test cases, each fully specified by $(V, d, E, p, \\sigma, N, \\text{seed})$. In all cases, $V = 8$.\n\n- Test Case $1$ (non-uniform frequencies, two-dimensional embeddings):\n    - $d = 2$\n    - Embeddings $E \\in \\mathbb{R}^{8 \\times 2}$ with rows $e_0$ through $e_7$:\n        - $e_0 = [0.0, 0.0]$\n        - $e_1 = [1.0, 0.0]$\n        - $e_2 = [0.0, 1.0]$\n        - $e_3 = [1.0, 1.0]$\n        - $e_4 = [2.0, 0.0]$\n        - $e_5 = [2.0, 1.0]$\n        - $e_6 = [3.0, 0.5]$\n        - $e_7 = [-1.0, 0.5]$\n    - Class probabilities $p = [0.30, 0.20, 0.15, 0.12, 0.10, 0.06, 0.04, 0.03]$.\n    - Gaussian noise standard deviation $\\sigma = 0.20$.\n    - Number of samples $N = 3000$.\n    - Random seed for reproducibility $\\text{seed} = 7$.\n\n- Test Case $2$ (uniform frequencies, circular arrangement in two dimensions):\n    - $d = 2$\n    - Embeddings $E$:\n        - $e_0 = [1.000, 0.000]$\n        - $e_1 = [0.707, 0.707]$\n        - $e_2 = [0.000, 1.000]$\n        - $e_3 = [-0.707, 0.707]$\n        - $e_4 = [-1.000, 0.000]$\n        - $e_5 = [-0.707, -0.707]$\n        - $e_6 = [0.000, -1.000]$\n        - $e_7 = [0.707, -0.707]$\n    - Class probabilities $p = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]$.\n    - $\\sigma = 0.15$.\n    - $N = 3000$.\n    - $\\text{seed} = 13$.\n\n- Test Case $3$ (highly skewed frequencies, three-dimensional embeddings):\n    - $d = 3$\n    - Embeddings $E$:\n        - $e_0 = [0.0, 0.0, 0.0]$\n        - $e_1 = [1.0, 0.5, -0.5]$\n        - $e_2 = [-0.5, 1.0, 0.5]$\n        - $e_3 = [0.5, -1.0, 0.5]$\n        - $e_4 = [1.5, 1.5, -1.0]$\n        - $e_5 = [-1.5, 1.0, 1.0]$\n        - $e_6 = [0.0, -1.5, -1.0]$\n        - $e_7 = [2.0, 0.0, 1.0]$\n    - Class probabilities $p = [0.6, 0.1, 0.1, 0.05, 0.05, 0.04, 0.03, 0.03]$.\n    - $\\sigma = 0.25$.\n    - $N = 4000$.\n    - $\\text{seed} = 23$.\n\nRequired outputs per test case:\n- For each test case, compute:\n    - Perplexity under the clustering-based $2$-means tree, as a float.\n    - Perplexity under the frequency-based Huffman tree, as a float.\n    - Expected depth under the clustering-based $2$-means tree, as a float.\n    - Expected depth under the frequency-based Huffman tree, as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a sublist in the order given above. Each sublist must be of the form $[\\text{ppl\\_kmeans}, \\text{ppl\\_huffman}, \\text{lat\\_kmeans}, \\text{lat\\_huffman}]$, with all values rounded to $6$ decimal places. For example, the overall output format is\n\"[[a11,a12,a13,a14],[a21,a22,a23,a24],[a31,a32,a33,a34]]\".", "solution": "The user-provided problem statement has been analyzed and is determined to be **valid**. It is scientifically grounded in the principles of machine learning and probability theory, self-contained, and well-posed. All components required for a unique, computable solution are specified, including deterministic procedures for tree construction and data generation.\n\nThe task is to implement and compare two heuristics for building a binary tree for a hierarchical softmax classifier: a frequency-based Huffman tree and a clustering-based top-down $2$-means tree. The comparison will be based on two metrics: perplexity on a synthetic dataset, which measures model fit, and a latency proxy defined as the expected class depth, which measures computational efficiency.\n\nThe solution will be implemented by following these core steps:\n\n1.  **Tree Construction**: Two distinct algorithms will be implemented to construct the binary tree hierarchies.\n    -   **Huffman Tree**: Following the problem specification, we will implement the classic Huffman coding algorithm. This involves using a min-priority queue to greedily merge the two nodes (classes or sub-trees) with the lowest probability. The probability of a new internal node is the sum of its children's probabilities. This algorithm is known to construct a prefix code that minimizes the expected codeword length, which in this context translates to minimizing the expected leaf depth $\\mathbb{E}[L(C)]$ given the class frequencies $p(c)$. Each leaf in the tree represents a unique class, and each internal node represents a binary decision.\n    -   **Clustering Tree**: A tree will be constructed by recursively partitioning the set of class embeddings. At each step, for a given set of classes, we use the $k$-means algorithm with $k=2$ to split them into two clusters. The process is deterministic: initial centers for the $2$-means algorithm are chosen as the class embeddings with the minimum and maximum values along the first coordinate. The clustering criterion is the squared Euclidean distance. The recursion continues until each leaf node contains a single class. This heuristic aims to group similar classes together, potentially making the classification task at each internal node easier.\n\n2.  **Internal Node Classifier Formulation**: For each internal node in a constructed tree, a linear binary classifier is defined. This classifier's purpose is to decide whether to route an input vector $x$ to its left or right child. Given the set of classes in the left subtree, $\\text{classes}_L$, and the right subtree, $\\text{classes}_R$, we first compute the mean of their respective class embeddings, $m_L = \\text{mean}\\{e_c : c \\in \\text{classes}_L\\}$ and $m_R = \\text{mean}\\{e_c : c \\in \\text{classes}_R\\}$. The classifier is a hyperplane positioned halfway between these two means. Its parameters, a weight vector $w$ and a bias term $b$, are defined as:\n    $$\n    w = \\frac{m_R - m_L}{\\lVert m_R - m_L \\rVert_2}, \\quad b = -\\frac{1}{2} w^\\top (m_L + m_R)\n    $$\n    The score for an input $x$ is $z(x) = w^\\top x + b$. This score represents the projected distance of $x$ from the separating hyperplane. The probability of routing to the right child is given by the logistic function $P(\\text{right} \\mid x) = \\sigma(z(x))$, and to the left is $P(\\text{left} \\mid x) = 1 - \\sigma(z(x))$.\n\n3.  **Probabilistic Model and Perplexity**: The probability of a class $c$ given an input $x$, $P(c \\mid x)$, is the product of the probabilities of the routing decisions made at each internal node along the path from the root to the leaf for class $c$. The negative log-likelihood (NLL) is $-\\log P(c \\mid x)$. To avoid numerical instability when computing logarithms of probabilities, which can be very small, we sum the log-probabilities at each step of the path. The log-probabilities for left and right turns are $\\log(1-\\sigma(z))$ and $\\log(\\sigma(z))$ respectively, which can be computed robustly using NumPy's `logaddexp` function, equivalent to a stable `log(1+exp(x))` calculation. The perplexity over a dataset of $N$ samples is then calculated as the exponential of the average NLL:\n    $$\n    \\operatorname{ppl} = \\exp\\left(\\frac{1}{N} \\sum_{i=1}^N \\text{NLL}(x_i, c_i)\\right)\n    $$\n\n4.  **Latency Proxy**: A proxy for the computational latency of a prediction is the expected number of internal node evaluations required. This is equivalent to the expected depth of a class, weighted by the class probabilities $p(c)$:\n    $$\n    \\mathbb{E}[L(C)] = \\sum_{c=1}^{V} p(c) L(c)\n    $$\n    where $L(c)$ is the depth of the leaf node for class $c$.\n\n5.  **Data Simulation and Evaluation**: For each test case, a synthetic dataset will be generated as specified. A set of class labels $\\{c_i\\}$ is drawn from the categorical distribution defined by probabilities $p$. For each label $c_i$, a feature vector $x_i$ is sampled from a multivariate Gaussian distribution $\\mathcal{N}(e_{c_i}, \\sigma^2 I_d)$, where $e_{c_i}$ is the embedding for class $c_i$. The evaluation metrics (perplexity and latency) will be computed for both tree structures on this dataset. The entire process, from data generation to evaluation, is made reproducible by using the specified random seed.\n\nThe final implementation will process each test case, build both tree types, compute the classifiers, and evaluate the required metrics, formatting the output as a list of lists according to the problem specification.", "answer": "```python\nimport numpy as np\nimport heapq\nfrom collections import deque, defaultdict\n\nclass Node:\n    \"\"\"Represents a node in the binary tree for hierarchical softmax.\"\"\"\n    def __init__(self, is_leaf, class_id=None, left=None, right=None):\n        self.is_leaf = is_leaf\n        self.class_id = class_id\n        self.left = left\n        self.right = right\n        self.w = None\n        self.b = None\n        self.classes = set([class_id]) if is_leaf else set()\n\ndef generate_data(N, p, E, sigma, rng):\n    \"\"\"Generates synthetic data from a Gaussian mixture model.\"\"\"\n    d = E.shape[1]\n    class_labels = rng.choice(len(p), size=N, p=p)\n    means = E[class_labels]\n    noise = rng.normal(scale=sigma, size=(N, d))\n    X = means + noise\n    C = class_labels\n    return X, C\n\ndef build_huffman_tree(p):\n    \"\"\"Constructs a Huffman tree based on class frequencies.\"\"\"\n    pq = [(prob, i, Node(is_leaf=True, class_id=i)) for i, prob in enumerate(p)]\n    heapq.heapify(pq)\n    \n    counter = len(p)\n    while len(pq) > 1:\n        prob1, _, left_node = heapq.heappop(pq)\n        prob2, _, right_node = heapq.heappop(pq)\n        \n        parent_prob = prob1 + prob2\n        parent_node = Node(is_leaf=False, left=left_node, right=right_node)\n        parent_node.classes = left_node.classes.union(right_node.classes)\n        \n        heapq.heappush(pq, (parent_prob, counter, parent_node))\n        counter += 1\n        \n    return pq[0][2]\n\ndef build_kmeans_tree(class_indices, E, max_iter=100):\n    \"\"\"Recursively constructs a tree using top-down 2-means clustering.\"\"\"\n    if len(class_indices) == 1:\n        return Node(is_leaf=True, class_id=class_indices[0])\n\n    sub_E = E[class_indices, :]\n    \n    # Deterministic initialization of centers\n    min_idx_in_subset = np.argmin(sub_E[:, 0])\n    max_idx_in_subset = np.argmax(sub_E[:, 0])\n    \n    if min_idx_in_subset == max_idx_in_subset:\n        if E.shape[1] > 1:\n            min_idx_in_subset = np.argmin(sub_E[:, 1])\n            max_idx_in_subset = np.argmax(sub_E[:, 1])\n        if min_idx_in_subset == max_idx_in_subset:\n             min_idx_in_subset, max_idx_in_subset = 0, 1\n    \n    center1 = sub_E[min_idx_in_subset]\n    center2 = sub_E[max_idx_in_subset]\n\n    # Handle identical initial centers\n    if np.array_equal(center1, center2):\n      center1 = sub_E[0]\n      center2 = sub_E[1]\n\n    last_assignments = None\n    \n    for _ in range(max_iter):\n        assignments = defaultdict(list)\n        \n        # Assignment step\n        for i, class_idx in enumerate(class_indices):\n            emb = sub_E[i]\n            dist1 = np.sum((emb - center1)**2)\n            dist2 = np.sum((emb - center2)**2)\n            assignments[0 if dist1 = dist2 else 1].append(class_idx)\n        \n        # Empty cluster handling\n        if not assignments[0] or not assignments[1]:\n            non_empty_key = 1 if not assignments[0] else 0\n            empty_key = 1 - non_empty_key\n            non_empty_indices = assignments[non_empty_key]\n            non_empty_center = center1 if non_empty_key == 0 else center2\n            \n            dists = np.sum((E[non_empty_indices] - non_empty_center)**2, axis=1)\n            farthest_idx_in_list = np.argmax(dists)\n            farthest_class_id = non_empty_indices[farthest_idx_in_list]\n            \n            assignments[non_empty_key].remove(farthest_class_id)\n            assignments[empty_key].append(farthest_class_id)\n        \n        # Convergence check\n        current_assignments_tuple = (tuple(sorted(assignments[0])), tuple(sorted(assignments[1])))\n        if current_assignments_tuple == last_assignments:\n            break\n        last_assignments = current_assignments_tuple\n        \n        # Update step\n        if assignments[0]: center1 = np.mean(E[assignments[0]], axis=0)\n        if assignments[1]: center2 = np.mean(E[assignments[1]], axis=0)\n    \n    # Ensure deterministic child assignment\n    left_indices, right_indices = assignments[0], assignments[1]\n    if min(left_indices) > min(right_indices):\n        left_indices, right_indices = right_indices, left_indices\n\n    left_child = build_kmeans_tree(left_indices, E, max_iter)\n    right_child = build_kmeans_tree(right_indices, E, max_iter)\n    \n    node = Node(is_leaf=False, left=left_child, right=right_child)\n    node.classes = left_child.classes.union(right_child.classes)\n    return node\n\ndef compute_classifiers(node, E):\n    \"\"\"Computes and sets classifier parameters (w, b) for all internal nodes.\"\"\"\n    if node.is_leaf:\n        return\n    \n    compute_classifiers(node.left, E)\n    compute_classifiers(node.right, E)\n    \n    m_L = np.mean(E[list(node.left.classes)], axis=0)\n    m_R = np.mean(E[list(node.right.classes)], axis=0)\n    \n    diff = m_R - m_L\n    norm = np.linalg.norm(diff)\n    \n    w = np.zeros_like(m_R) if norm == 0 else diff / norm\n    b = -0.5 * w.T @ (m_L + m_R)\n    \n    node.w, node.b = w, b\n\ndef find_paths_and_depths(root):\n    \"\"\"Traverses the tree to find the path and depth for each class leaf.\"\"\"\n    paths, depths = {}, {}\n    q = deque([(root, [])]) # (node, path_so_far)\n    \n    while q:\n        node, path = q.popleft()\n        \n        if node.is_leaf:\n            paths[node.class_id] = path\n            depths[node.class_id] = len(path)\n            continue\n            \n        q.append((node.left, path + [(node, 'L')]))\n        q.append((node.right, path + [(node, 'R')]))\n        \n    return paths, depths\n\ndef calculate_perplexity_and_latency(tree_root, E, p, X, C):\n    \"\"\"Computes perplexity and latency for a given tree and dataset.\"\"\"\n    compute_classifiers(tree_root, E)\n    paths, depths = find_paths_and_depths(tree_root)\n    \n    # Latency Proxy\n    V = len(p)\n    latency = sum(p[c] * depths[c] for c in range(V) if c in depths)\n    \n    # Perplexity\n    total_nll = 0.0\n    for i in range(X.shape[0]):\n        x_i, c_i = X[i], C[i]\n        path = paths[c_i]\n        \n        log_prob = 0.0\n        for node, direction in path:\n            z = np.dot(node.w, x_i) + node.b\n            # log P(right) = -log(1+exp(-z))\n            # log P(left) = -log(1+exp(z))\n            log_prob -= np.logaddexp(0, -z) if direction == 'R' else np.logaddexp(0, z)\n        \n        total_nll -= log_prob\n    \n    perplexity = np.exp(total_nll / X.shape[0])\n    \n    return perplexity, latency\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {\"V\": 8, \"d\": 2, \"E\": np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [2.0, 0.0], [2.0, 1.0], [3.0, 0.5], [-1.0, 0.5]]), \"p\": np.array([0.30, 0.20, 0.15, 0.12, 0.10, 0.06, 0.04, 0.03]), \"sigma\": 0.20, \"N\": 3000, \"seed\": 7},\n        {\"V\": 8, \"d\": 2, \"E\": np.array([[1.000, 0.000], [0.707, 0.707], [0.000, 1.000], [-0.707, 0.707], [-1.000, 0.000], [-0.707, -0.707], [0.000, -1.000], [0.707, -0.707]]), \"p\": np.array([0.125] * 8), \"sigma\": 0.15, \"N\": 3000, \"seed\": 13},\n        {\"V\": 8, \"d\": 3, \"E\": np.array([[0.0, 0.0, 0.0], [1.0, 0.5, -0.5], [-0.5, 1.0, 0.5], [0.5, -1.0, 0.5], [1.5, 1.5, -1.0], [-1.5, 1.0, 1.0], [0.0, -1.5, -1.0], [2.0, 0.0, 1.0]]), \"p\": np.array([0.6, 0.1, 0.1, 0.05, 0.05, 0.04, 0.03, 0.03]), \"sigma\": 0.25, \"N\": 4000, \"seed\": 23}\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        V, E, p, sigma, N, seed = case[\"V\"], case[\"E\"], case[\"p\"], case[\"sigma\"], case[\"N\"], case[\"seed\"]\n        rng = np.random.default_rng(seed)\n        \n        X, C = generate_data(N, p, E, sigma, rng)\n        \n        # Clustering-based (k-means) tree\n        kmeans_root = build_kmeans_tree(list(range(V)), E)\n        ppl_kmeans, lat_kmeans = calculate_perplexity_and_latency(kmeans_root, E, p, X, C)\n        \n        # Frequency-based (Huffman) tree\n        huffman_root = build_huffman_tree(p)\n        ppl_huffman, lat_huffman = calculate_perplexity_and_latency(huffman_root, E, p, X, C)\n        \n        all_results.append([\n            f\"{ppl_kmeans:.6f}\", f\"{ppl_huffman:.6f}\",\n            f\"{lat_kmeans:.6f}\", f\"{lat_huffman:.6f}\"\n        ])\n\n    result_str = \"[\" + \",\".join([f\"[{','.join(res)}]\" for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3134847"}]}