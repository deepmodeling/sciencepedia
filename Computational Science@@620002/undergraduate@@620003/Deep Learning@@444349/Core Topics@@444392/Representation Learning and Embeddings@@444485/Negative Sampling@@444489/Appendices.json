{"hands_on_practices": [{"introduction": "To truly master negative sampling, we must first understand its theoretical underpinnings. This exercise guides you through deriving the Bayes-optimal score function for the classic negative sampling objective, revealing the precise relationship between the learned scores and the underlying data and noise distributions. By applying this knowledge, you will quantify the \"mismatch cost\" that arises from distribution shift—a critical and common challenge where training and testing data differ—providing a concrete understanding of how model performance can degrade in the wild. [@problem_id:3156698]", "problem": "Consider a discrete vocabulary with a finite set of outcomes. Let there be a training distribution over outcomes denoted by $p_{\\text{train}}$ and a test distribution denoted by $p_{\\text{test}}$. In a negative sampling setup, for each real (positive) sample, $k$ noise (negative) samples are drawn independently from a noise distribution $q$. The negative sampling classifier uses a scalar score function $s(w)$ for an outcome $w$, and its surrogate loss for a pair $(w, \\text{label})$ is based on the logistic function. Specifically, the loss for a real draw is defined as the negative logarithm of the logistic function of $s(w)$, and the loss for a noise draw is defined as the negative logarithm of the logistic function of $-s(w)$.\n\nNoise distributions are tied to data frequencies via an exponent $\\alpha \\in [0,1]$. For any distribution $r$ over the vocabulary, define the noise distribution $q^{(\\alpha)}(r)$ by raising each probability to the power $\\alpha$ and normalizing, i.e., for each outcome $w$, $q^{(\\alpha)}(r)(w) \\propto r(w)^{\\alpha}$ with normalization to ensure $\\sum_{w} q^{(\\alpha)}(r)(w) = 1$.\n\nAssume a model is trained to minimize the expected surrogate loss under the training mixture formed by $p_{\\text{train}}$ for real samples and $q_{\\text{train}} = q^{(\\alpha)}(p_{\\text{train}})$ for noise samples. At test time, the true real distribution is $p_{\\text{test}}$, and two evaluation regimes are considered:\n- Regime A: Negatives continue to be drawn from $q_{\\text{train}}$.\n- Regime B: Negatives are drawn from $q_{\\text{test}} = q^{(\\alpha)}(p_{\\text{test}})$.\n\nStarting from fundamental definitions of the logistic function $\\sigma(x)$ and the Bernoulli cross-entropy, and the expected negative sampling surrogate risk over the mixture of real and noise draws, derive from first principles:\n1. The expected surrogate risk functional for a general score function $s(w)$ when real outcomes follow a distribution $p$ and noise outcomes follow a distribution $q$ with $k$ negatives per real.\n2. The Bayes-optimal score function $s^{\\star}(w)$ that minimizes the expected surrogate risk for given $(p,q,k)$.\n3. A quantitative mismatch cost defined as the difference between the expected surrogate risk achieved by the trained score function $s_{\\text{train}}$ and the minimal expected surrogate risk under the test mixture. Compute this mismatch cost for both Regime A (where the evaluation noise is $q_{\\text{train}}$) and Regime B (where the evaluation noise is $q_{\\text{test}}$).\n\nImplement a program that, for each test case specified below, computes two floating-point numbers:\n- The Regime A mismatch cost, defined as the expected surrogate risk under $(p_{\\text{test}}, q_{\\text{train}}, k)$ when using $s_{\\text{train}}$, minus the minimal expected surrogate risk under $(p_{\\text{test}}, q_{\\text{train}}, k)$.\n- The Regime B mismatch cost, defined analogously but with $q_{\\text{test}}$ in place of $q_{\\text{train}}$.\n\nUse the following test suite, where each probability vector is over a vocabulary of five outcomes and sums to $1$, and where $q^{(\\alpha)}(\\cdot)$ is computed as described. All probabilities and parameters are strictly positive and scientifically plausible.\n\n- Test Case $1$ (general shift, moderate $k$):\n  - $p_{\\text{train}} = (0.40, 0.25, 0.20, 0.10, 0.05)$\n  - $p_{\\text{test}} = (0.10, 0.15, 0.25, 0.30, 0.20)$\n  - $\\alpha = 0.75$\n  - $k = 5$\n\n- Test Case $2$ (no shift, identical train and test):\n  - $p_{\\text{train}} = (0.20, 0.20, 0.20, 0.20, 0.20)$\n  - $p_{\\text{test}} = (0.20, 0.20, 0.20, 0.20, 0.20)$\n  - $\\alpha = 0.50$\n  - $k = 5$\n\n- Test Case $3$ (uniform noise, large $k$):\n  - $p_{\\text{train}} = (0.55, 0.15, 0.10, 0.10, 0.10)$\n  - $p_{\\text{test}} = (0.10, 0.25, 0.25, 0.20, 0.20)$\n  - $\\alpha = 0.00$\n  - $k = 10$\n\n- Test Case $4$ (strong shift, small $k$):\n  - $p_{\\text{train}} = (0.70, 0.10, 0.10, 0.05, 0.05)$\n  - $p_{\\text{test}} = (0.05, 0.65, 0.10, 0.10, 0.10)$\n  - $\\alpha = 1.00$\n  - $k = 1$\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the Regime A mismatch cost followed by the Regime B mismatch cost. The final output must therefore contain eight numbers corresponding to the four test cases, in order. Express each number as a floating-point value rounded to six decimal places, with no units, and print exactly the format $[r_1,r_2,\\dots,r_8]$.", "solution": "The problem asks for the derivation of the expected surrogate risk in a negative sampling setup, the corresponding Bayes-optimal score function, and the computation of a mismatch cost under two different evaluation regimes.\n\nThe analysis proceeds in three steps as requested. First, we derive the expected surrogate risk functional. Second, we find the score function that minimizes this risk. Third, we use these results to define and subsequently compute the mismatch costs for the specified test cases.\n\nLet $\\sigma(x) = (1 + e^{-x})^{-1}$ be the logistic function. The loss for a positive sample $w$ is given as $L_{+} = -\\log(\\sigma(s(w)))$, and the loss for a negative (noise) sample $w$ is $L_{-} = -\\log(\\sigma(-s(w)))$. We can rewrite these using the identity $\\log(\\sigma(x)) = -\\log(1+e^{-x})$:\n$L_{+} = \\log(1+e^{-s(w)})$\n$L_{-} = \\log(1+e^{s(w)})$\n\n### 1. Expected Surrogate Risk Functional\n\nThe negative sampling procedure involves, for each real outcome $w_{\\text{real}}$ drawn from a data distribution $p$, drawing $k$ noise outcomes $w_{\\text{noise},1}, \\dots, w_{\\text{noise},k}$ independently from a noise distribution $q$. The total loss for this \"event\" (one real sample and its $k$ associated negative samples) is the sum of the individual losses:\n$$\nL_{\\text{event}}(w_{\\text{real}}, w_{\\text{noise},1}, \\dots, w_{\\text{noise},k}) = -\\log(\\sigma(s(w_{\\text{real}}))) - \\sum_{i=1}^{k} \\log(\\sigma(-s(w_{\\text{noise},i})))\n$$\nThe expected surrogate risk, denoted $R(s; p, q, k)$, is the expectation of this total loss over all possible choices of real and noise samples, drawn according to their respective distributions. By the linearity of expectation, we can separate the contributions from the real and noise samples:\n$$\nR(s; p, q, k) = \\mathbb{E}_{w_{\\text{real}} \\sim p, \\{w_{\\text{noise},i}\\}_{i=1}^k \\sim q} \\left[ -\\log(\\sigma(s(w_{\\text{real}}))) - \\sum_{i=1}^{k} \\log(\\sigma(-s(w_{\\text{noise},i}))) \\right]\n$$\n$$\nR(s; p, q, k) = \\mathbb{E}_{w_{\\text{real}} \\sim p} [-\\log(\\sigma(s(w_{\\text{real}})))] + \\sum_{i=1}^{k} \\mathbb{E}_{w_{\\text{noise},i} \\sim q} [-\\log(\\sigma(-s(w_{\\text{noise},i})))]\n$$\nSince the noise samples are identically distributed, their expected losses are the same. Thus, we can simplify the sum:\n$$\nR(s; p, q, k) = \\mathbb{E}_{w \\sim p} [-\\log(\\sigma(s(w)))] + k \\cdot \\mathbb{E}_{w' \\sim q} [-\\log(\\sigma(-s(w')))]\n$$\nFor a discrete vocabulary $V$, this expectation is expressed as a sum over all outcomes $w \\in V$:\n$$\nR(s; p, q, k) = \\sum_{w \\in V} p(w) [-\\log(\\sigma(s(w)))] + k \\sum_{w \\in V} q(w) [-\\log(\\sigma(-s(w)))]\n$$\nUsing the numerically stable form with logarithms, this is:\n$$\nR(s; p, q, k) = \\sum_{w \\in V} \\left[ p(w)\\log(1+e^{-s(w)}) + k \\, q(w)\\log(1+e^{s(w)}) \\right]\n$$\nThis is the desired expected surrogate risk functional.\n\n### 2. Bayes-Optimal Score Function\n\nTo find the Bayes-optimal score function $s^{\\star}(w)$ that minimizes $R(s; p, q, k)$, we can optimize the risk with respect to $s(w)$ for each outcome $w$ independently, as the total risk is a sum of terms, each depending on only one $s(w)$. Let us consider the component of the risk associated with a single outcome $w_j \\in V$:\n$$\nR_j(s(w_j)) = p(w_j)\\log(1+e^{-s(w_j)}) + k \\, q(w_j)\\log(1+e^{s(w_j)})\n$$\nWe find the minimum by setting the derivative with respect to $s(w_j)$ to zero. We use the derivatives $\\frac{d}{dx}\\log(1+e^{-x}) = \\frac{-e^{-x}}{1+e^{-x}} = -\\sigma(-x)$ and $\\frac{d}{dx}\\log(1+e^{x}) = \\frac{e^{x}}{1+e^{x}} = \\sigma(x)$.\n$$\n\\frac{\\partial R_j}{\\partial s(w_j)} = p(w_j) (-\\sigma(-s(w_j))) + k \\, q(w_j) (\\sigma(s(w_j))) = 0\n$$\nAt the optimum $s^{\\star}(w_j)$, this gives:\n$$\np(w_j) \\sigma(-s^{\\star}(w_j)) = k \\, q(w_j) \\sigma(s^{\\star}(w_j))\n$$\nUsing $\\sigma(-x) = 1 - \\sigma(x)$:\n$$\np(w_j) (1 - \\sigma(s^{\\star}(w_j))) = k \\, q(w_j) \\sigma(s^{\\star}(w_j))\n$$\nRearranging to solve for $\\sigma(s^{\\star}(w_j))$:\n$$\np(w_j) = p(w_j)\\sigma(s^{\\star}(w_j)) + k \\, q(w_j)\\sigma(s^{\\star}(w_j)) = \\sigma(s^{\\star}(w_j)) (p(w_j) + k \\, q(w_j))\n$$\n$$\n\\sigma(s^{\\star}(w_j)) = \\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}\n$$\nTo find $s^{\\star}(w_j)$ itself, we invert the logistic function using the logit function, $\\text{logit}(y) = \\log(y/(1-y))$:\n$$\ns^{\\star}(w_j) = \\text{logit}\\left(\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}\\right) = \\log\\left( \\frac{\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}}{1 - \\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}} \\right)\n$$\n$$\ns^{\\star}(w_j) = \\log\\left( \\frac{\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}}{\\frac{k \\, q(w_j)}{p(w_j) + k \\, q(w_j)}} \\right) = \\log\\left(\\frac{p(w_j)}{k \\, q(w_j)}\\right)\n$$\nThus, the Bayes-optimal score function for a given set of distributions $(p,q)$ and parameter $k$ is $s^{\\star}(w) = \\log(p(w)) - \\log(k) - \\log(q(w))$.\n\n### 3. Mismatch Cost Computation\n\nThe mismatch cost is the excess risk incurred by using a non-optimal score function. A model trained on $(p_{\\text{train}}, q_{\\text{train}}, k)$ learns the score function:\n$$\ns_{\\text{train}}(w) = s^{\\star}(w; p_{\\text{train}}, q_{\\text{train}}, k) = \\log\\left(\\frac{p_{\\text{train}}(w)}{k \\, q_{\\text{train}}(w)}\\right)\n$$\nwhere $q_{\\text{train}}(w) \\propto p_{\\text{train}}(w)^{\\alpha}$.\n\n**Regime A:** The evaluation setting is $(p_{\\text{test}}, q_{\\text{train}}, k)$.\nThe risk incurred by the trained model is $R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{train}}, k)$.\nThe minimal possible risk in this regime is achieved by the optimal score function for this regime:\n$$\ns^{\\star}_{\\text{test},A}(w) = s^{\\star}(w; p_{\\text{test}}, q_{\\text{train}}, k) = \\log\\left(\\frac{p_{\\text{test}}(w)}{k \\, q_{\\text{train}}(w)}\\right)\n$$\nThe corresponding minimal risk is $R(s^{\\star}_{\\text{test},A}; p_{\\text{test}}, q_{\\text{train}}, k)$.\nThe mismatch cost for Regime A is the difference:\n$$\n\\text{Cost}_A = R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{train}}, k) - R(s^{\\star}_{\\text{test},A}; p_{\\text{test}}, q_{\\text{train}}, k)\n$$\n\n**Regime B:** The evaluation setting is $(p_{\\text{test}}, q_{\\text{test}}, k)$, where $q_{\\text{test}}(w) \\propto p_{\\text{test}}(w)^{\\alpha}$.\nThe risk incurred by the trained model is $R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{test}}, k)$.\nThe minimal possible risk in this regime is achieved by the optimal score function for this regime:\n$$\ns^{\\star}_{\\text{test},B}(w) = s^{\\star}(w; p_{\\text{test}}, q_{\\text{test}}, k) = \\log\\left(\\frac{p_{\\text{test}}(w)}{k \\, q_{\\text{test}}(w)}\\right)\n$$\nThe corresponding minimal risk is $R(s^{\\star}_{\\text{test},B}; p_{\\text{test}}, q_{\\text{test}}, k)$.\nThe mismatch cost for Regime B is the difference:\n$$\n\\text{Cost}_B = R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{test}}, k) - R(s^{\\star}_{\\text{test},B}; p_{\\text{test}}, q_{\\text{test}}, k)\n$$\n\nThe algorithm for the computation is as follows:\n1.  For each test case, given $p_{\\text{train}}$, $p_{\\text{test}}$, $\\alpha$, and $k$.\n2.  Construct the noise distributions $q_{\\text{train}}(w) = \\frac{p_{\\text{train}}(w)^\\alpha}{\\sum_{w'} p_{\\text{train}}(w')^\\alpha}$ and $q_{\\text{test}}(w) = \\frac{p_{\\text{test}}(w)^\\alpha}{\\sum_{w'} p_{\\text{test}}(w')^\\alpha}$.\n3.  Calculate the score functions $s_{\\text{train}}$, $s^{\\star}_{\\text{test},A}$, and $s^{\\star}_{\\text{test},B}$ using the derived formula.\n4.  Calculate the four necessary risk values using the risk functional $R(s; p, q, k)$.\n5.  Compute the differences $\\text{Cost}_A$ and $\\text{Cost}_B$.\n\nThis procedure is implemented for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the negative sampling mismatch cost problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (general shift, moderate k)\n        {\n            \"p_train\": np.array([0.40, 0.25, 0.20, 0.10, 0.05]),\n            \"p_test\": np.array([0.10, 0.15, 0.25, 0.30, 0.20]),\n            \"alpha\": 0.75,\n            \"k\": 5\n        },\n        # Test Case 2 (no shift, identical train and test)\n        {\n            \"p_train\": np.array([0.20, 0.20, 0.20, 0.20, 0.20]),\n            \"p_test\": np.array([0.20, 0.20, 0.20, 0.20, 0.20]),\n            \"alpha\": 0.50,\n            \"k\": 5\n        },\n        # Test Case 3 (uniform noise, large k)\n        {\n            \"p_train\": np.array([0.55, 0.15, 0.10, 0.10, 0.10]),\n            \"p_test\": np.array([0.10, 0.25, 0.25, 0.20, 0.20]),\n            \"alpha\": 0.00,\n            \"k\": 10\n        },\n        # Test Case 4 (strong shift, small k)\n        {\n            \"p_train\": np.array([0.70, 0.10, 0.10, 0.05, 0.05]),\n            \"p_test\": np.array([0.05, 0.65, 0.10, 0.10, 0.10]),\n            \"alpha\": 1.00,\n            \"k\": 1\n        },\n    ]\n\n    results = []\n    \n    def compute_q(p, alpha):\n        \"\"\"Computes the noise distribution q from a data distribution p and exponent alpha.\"\"\"\n        p_pow_alpha = np.power(p, alpha)\n        normalization_constant = np.sum(p_pow_alpha)\n        return p_pow_alpha / normalization_constant\n\n    def risk_functional(s, p, q, k):\n        \"\"\"Computes the expected surrogate risk R(s; p, q, k).\"\"\"\n        # Uses the numerically stable form: R = sum(p*log(1+exp(-s)) + k*q*log(1+exp(s)))\n        # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x))\n        term1 = p * np.logaddexp(0, -s)\n        term2 = k * q * np.logaddexp(0, s)\n        return np.sum(term1 + term2)\n\n    def optimal_score(p, q, k):\n        \"\"\"Computes the Bayes-optimal score function s_star.\"\"\"\n        # s_star(w) = log(p(w) / (k * q(w)))\n        # Probabilities are strictly positive, so no log(0) issues.\n        return np.log(p) - np.log(k) - np.log(q)\n\n    for case in test_cases:\n        p_train = case[\"p_train\"]\n        p_test = case[\"p_test\"]\n        alpha = case[\"alpha\"]\n        k = case[\"k\"]\n\n        # Compute noise distributions\n        q_train = compute_q(p_train, alpha)\n        q_test = compute_q(p_test, alpha)\n        \n        # Compute the score function learned during training\n        s_train = optimal_score(p_train, q_train, k)\n\n        # --- Regime A ---\n        # Evaluation with test data distribution but training noise distribution\n        \n        # Optimal score for Regime A's setting (p_test, q_train)\n        s_star_test_A = optimal_score(p_test, q_train, k)\n        \n        # Risk of the trained model in Regime A\n        risk_A_trained = risk_functional(s_train, p_test, q_train, k)\n        \n        # Minimal possible risk in Regime A\n        risk_A_optimal = risk_functional(s_star_test_A, p_test, q_train, k)\n        \n        cost_A = risk_A_trained - risk_A_optimal\n        results.append(cost_A)\n        \n        # --- Regime B ---\n        # Evaluation with test data distribution and test noise distribution\n        \n        # Optimal score for Regime B's setting (p_test, q_test)\n        s_star_test_B = optimal_score(p_test, q_test, k)\n        \n        # Risk of the trained model in Regime B\n        risk_B_trained = risk_functional(s_train, p_test, q_test, k)\n        \n        # Minimal possible risk in Regime B\n        risk_B_optimal = risk_functional(s_star_test_B, p_test, q_test, k)\n        \n        cost_B = risk_B_trained - risk_B_optimal\n        results.append(cost_B)\n\n    # Format output to 6 decimal places as requested\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3156698"}, {"introduction": "A core assumption of negative sampling is that all \"negative\" samples are truly dissimilar to the anchor. In practice, this is often violated by \"false negatives\"—semantically similar items that are accidentally sampled. This practice challenges you to implement a modern debiased contrastive loss function that elegantly handles this problem by re-weighting each negative's contribution based on its estimated probability of being a false negative, giving you hands-on experience with a key technique for improving model robustness. [@problem_id:3156689]", "problem": "You are given an embedding function $f$ that maps objects to a Euclidean vector space $\\mathbb{R}^d$, and you consider a contrastive learning setting where, for an anchor $x$, there is one positive sample $x^{+}$ and a set of negatives $\\{y_i\\}$. In this setting, negative sampling approximates the denominator of a categorical probability by a finite set of sampled negatives. Begin from the following foundations: (i) the definition of the softmax function and its temperature scaling, where for a set of scores $\\{s_j\\}$ the normalized probability for index $j$ is $\\exp(s_j/\\tau) / \\sum_k \\exp(s_k/\\tau)$ with temperature $\\tau > 0$, and (ii) the principle that Maximum Likelihood Estimation via Noise-Contrastive Estimation (NCE) trains by maximizing the log-probability of the correct class under such a normalized probability. Using these, derive and implement the standard Info Noise-Contrastive Estimation (InfoNCE) loss and a debiased variant that reweights negative contributions to mitigate false negatives.\n\nDefinitions for this problem:\n- All embeddings must be $\\ell_2$-normalized before computing similarities.\n- The similarity between two embeddings is defined as the dot product $s(u,v) = f(u)^\\top f(v)$ after normalization.\n- The standard contrastive loss for an anchor $x$ with one positive $x^{+}$ and negatives $\\{y_i\\}$ is based on the softmax probability over the set $\\{x^{+}\\} \\cup \\{y_i\\}$ of similarities scaled by the temperature $\\tau$; the loss is the negative logarithm of the probability assigned to $x^{+}$.\n- To debias the loss for potential false negatives, the expected contribution of each negative $y$ is reduced by a factor derived from the estimated probability that $y$ is a false negative for $x$. Specifically, use the logistic sigmoid estimate $p_{\\text{FN}}(y\\mid x) \\approx \\sigma\\!\\left(\\frac{f(x)^\\top f(y)}{\\tau}\\right)$, where $\\sigma(a) = \\frac{1}{1+\\exp(-a)}$, and subtract the expected contribution of false negatives by weighting each negative term with $1 - p_{\\text{FN}}(y\\mid x)$ inside the softmax denominator. The debiased loss is then the negative logarithm of the reweighted probability assigned to $x^{+}$.\n\nYour task:\n- Implement two losses: the standard Info Noise-Contrastive Estimation (InfoNCE) loss and the debiased contrastive loss described above.\n- Use $\\ell_2$ normalization of all embeddings prior to similarity computation.\n- For each test case defined below, compute the difference between the standard and debiased losses, i.e., output $L_{\\text{std}} - L_{\\text{debiased}}$ for that case. A positive value indicates that the debiased loss is smaller than the standard loss.\n\nTest suite (use exactly these vectors, in this order):\n- Case $1$ (happy path): dimension $d = 3$, temperature $\\tau = 0.5$, anchor $x = [1.0, 0.0, 0.0]$, positive $x^{+} = [1.0, 0.1, 0.0]$, negatives $\\{y_i\\} = \\{[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0]\\}$.\n- Case $2$ (significant false negative among negatives): dimension $d = 3$, temperature $\\tau = 0.5$, anchor $x = [1.0, 0.0, 0.0]$, positive $x^{+} = [1.0, 0.0, 0.05]$, negatives $\\{y_i\\} = \\{[0.9, 0.05, 0.0], [0.0, 0.0, 1.0]\\}$.\n- Case $3$ (boundary condition: high temperature): dimension $d = 3$, temperature $\\tau = 5.0$, anchor $x = [1.0, 0.0, 0.0]$, positive $x^{+} = [0.95, 0.0, 0.0]$, negatives $\\{y_i\\} = \\{[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]\\}$.\n- Case $4$ (edge case: no negatives): dimension $d = 3$, temperature $\\tau = 0.5$, anchor $x = [1.0, 0.0, 0.0]$, positive $x^{+} = [1.0, 0.0, 0.0]$, negatives $\\{y_i\\} = \\emptyset$.\n\nOutput specification:\n- Your program should produce a single line of output containing the loss differences for the four cases in the exact order provided above, as a comma-separated list enclosed in square brackets, i.e., $[r_1,r_2,r_3,r_4]$, where each $r_i$ is a real number (float) representing $L_{\\text{std}} - L_{\\text{debiased}}$ for case $i$.", "solution": "The user has provided a well-posed problem in the domain of deep learning, specifically concerning contrastive loss functions. The task is to derive, implement, and compare the standard Info Noise-Contrastive Estimation (InfoNCE) loss with a debiased variant designed to mitigate the effects of false negatives. The solution proceeds by first formalizing the mathematical expressions for both loss functions based on the provided definitions, and then applying these formulas to the given test cases.\n\nFirst, let us establish the foundational components. We are given an embedding function $f$ that maps objects to $\\mathbb{R}^d$. For a given anchor sample $x$, we have a positive sample $x^{+}$ and a set of $N$ negative samples $\\{y_i\\}_{i=1}^N$. All embedding vectors are $\\ell_2$-normalized prior to any computation. Let $\\tilde{v}$ denote the normalized vector $\\tilde{v} = v / \\|v\\|_2$. The similarity between two objects is the dot product of their normalized embeddings. For an anchor $x$ and another object $z$, the similarity score is $s(x, z) = \\tilde{f}(x)^\\top \\tilde{f}(z)$.\n\nThe positive score is $s_{pos} = s(x, x^{+})$, and the negative scores are $s_{neg, i} = s(x, y_i)$ for $i \\in \\{1, \\dots, N\\}$.\n\n**1. Standard InfoNCE Loss ($L_{\\text{std}}$)**\n\nThe standard InfoNCE loss is derived from the softmax-normalized probability of correctly identifying the positive sample $x^{+}$ from the set containing it and all negative samples $\\{y_i\\}$. Given a temperature parameter $\\tau > 0$, the probability of selecting the positive sample is:\n$$ P_{\\text{std}}(x^{+}) = \\frac{\\exp(s_{pos}/\\tau)}{\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)} $$\nThe loss is the negative log-likelihood of this probability, which aligns with the principle of Noise-Contrastive Estimation (NCE) aiming to maximize this value.\n$$ L_{\\text{std}} = -\\log\\left(P_{\\text{std}}(x^{+})\\right) = -\\log\\left(\\frac{\\exp(s_{pos}/\\tau)}{\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)}\\right) $$\nUsing the property of logarithms $\\log(a/b) = \\log(a) - \\log(b)$, we can rewrite the loss as:\n$$ L_{\\text{std}} = -\\left(s_{pos}/\\tau - \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)\\right)\\right) $$\n$$ L_{\\text{std}} = -s_{pos}/\\tau + \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)\\right) $$\n\n**2. Debiased Contrastive Loss ($L_{\\text{debiased}}$)**\n\nThe debiased loss addresses the issue of \"false negatives,\" where a sample $y_i$ labeled as negative is semantically similar to the anchor $x$. Such samples can unduly penalize the model during training. The specified debiasing strategy re-weights the contribution of each negative sample in the denominator of the softmax.\n\nThe probability that a negative sample $y_i$ is a false negative is estimated using the logistic sigmoid function $\\sigma(a) = (1 + \\exp(-a))^{-1}$:\n$$ p_{\\text{FN}}(y_i \\mid x) = \\sigma\\left(\\frac{s_{neg, i}}{\\tau}\\right) $$\nEach negative term in the denominator is then weighted by $w_i = 1 - p_{\\text{FN}}(y_i \\mid x)$. We can simplify this weight:\n$$ w_i = 1 - \\sigma\\left(\\frac{s_{neg, i}}{\\tau}\\right) = 1 - \\frac{1}{1 + \\exp(-s_{neg, i}/\\tau)} = \\frac{1 + \\exp(-s_{neg, i}/\\tau) - 1}{1 + \\exp(-s_{neg, i}/\\tau)} = \\frac{\\exp(-s_{neg, i}/\\tau)}{1 + \\exp(-s_{neg, i}/\\tau)} $$\nThis is equivalent to $\\sigma(-s_{neg, i}/\\tau)$.\n$$ w_i = \\sigma\\left(-\\frac{s_{neg, i}}{\\tau}\\right) = \\frac{1}{1 + \\exp(s_{neg, i}/\\tau)} $$\nThe reweighted probability for the positive sample becomes:\n$$ P_{\\text{debiased}}(x^{+}) = \\frac{\\exp(s_{pos}/\\tau)}{\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} w_i \\exp(s_{neg, i}/\\tau)} $$\nThe debiased loss is the negative log-likelihood of this reweighted probability:\n$$ L_{\\text{debiased}} = -\\log\\left(P_{\\text{debiased}}(x^{+})\\right) = -s_{pos}/\\tau + \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\sigma\\left(-\\frac{s_{neg, i}}{\\tau}\\right) \\exp(s_{neg, i}/\\tau)\\right) $$\n\n**3. Loss Difference Calculation**\n\nThe task is to compute the difference $L_{\\text{std}} - L_{\\text{debiased}}$.\n$$ L_{\\text{std}} - L_{\\text{debiased}} = \\left( -s_{pos}/\\tau + \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)\\right) \\right) - \\left( -s_{pos}/\\tau + \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} w_i \\exp(s_{neg, i}/\\tau)\\right) \\right) $$\nThe $-s_{pos}/\\tau$ terms cancel, yielding:\n$$ L_{\\text{std}} - L_{\\text{debiased}} = \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)\\right) - \\log\\left(\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} w_i \\exp(s_{neg, i}/\\tau)\\right) $$\nUsing the property $\\log(A) - \\log(B) = \\log(A/B)$, the final expression for the difference is:\n$$ L_{\\text{std}} - L_{\\text{debiased}} = \\log\\left( \\frac{\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\exp(s_{neg, i}/\\tau)}{\\exp(s_{pos}/\\tau) + \\sum_{i=1}^{N} \\sigma(-s_{neg, i}/\\tau) \\exp(s_{neg, i}/\\tau)} \\right) $$\nThis expression will be implemented for each test case.\n\n**Edge Case: No Negatives**\nFor Case $4$, the set of negatives $\\{y_i\\}$ is empty, so $N=0$. A sum over an empty set is $0$.\nThe denominators for both losses become identical:\n$D_{\\text{std}} = D_{\\text{debiased}} = \\exp(s_{pos}/\\tau)$.\nThus, $L_{\\text{std}} = L_{\\text{debiased}} = -\\log(\\exp(s_{pos}/\\tau)/\\exp(s_{pos}/\\tau)) = -\\log(1) = 0$. The difference is $0$.\n\nThe implementation will compute this final expression for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference between standard and debiased InfoNCE loss for a suite of test cases.\n    \"\"\"\n    # Test suite (use exactly these vectors, in this order):\n    # Case 1 (happy path): dimension d = 3, temperature τ = 0.5, anchor x = [1.0, 0.0, 0.0], positive x⁺ = [1.0, 0.1, 0.0], negatives {yᵢ} = {[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0]}.\n    # Case 2 (significant false negative): dimension d = 3, temperature τ = 0.5, anchor x = [1.0, 0.0, 0.0], positive x⁺ = [1.0, 0.0, 0.05], negatives {yᵢ} = {[0.9, 0.05, 0.0], [0.0, 0.0, 1.0]}.\n    # Case 3 (boundary condition: high temperature): dimension d = 3, temperature τ = 5.0, anchor x = [1.0, 0.0, 0.0], positive x⁺ = [0.95, 0.0, 0.0], negatives {yᵢ} = {[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]}.\n    # Case 4 (edge case: no negatives): dimension d = 3, temperature τ = 0.5, anchor x = [1.0, 0.0, 0.0], positive x⁺ = [1.0, 0.0, 0.0], negatives {yᵢ} = ∅.\n    \n    test_cases = [\n        (0.5, [1.0, 0.0, 0.0], [1.0, 0.1, 0.0], [[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0]]),\n        (0.5, [1.0, 0.0, 0.0], [1.0, 0.0, 0.05], [[0.9, 0.05, 0.0], [0.0, 0.0, 1.0]]),\n        (5.0, [1.0, 0.0, 0.0], [0.95, 0.0, 0.0], [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n        (0.5, [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], []),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        tau, anchor_vec, positive_vec, negative_vecs_list = case\n\n        # Handle the edge case where there are no negative samples.\n        if not negative_vecs_list:\n            results.append(0.0)\n            continue\n\n        # Convert lists to NumPy arrays for vectorized operations.\n        anchor = np.array(anchor_vec, dtype=float)\n        positive = np.array(positive_vec, dtype=float)\n        negatives = np.array(negative_vecs_list, dtype=float)\n\n        # L2-normalize all embedding vectors.\n        norm_anchor = anchor / np.linalg.norm(anchor)\n        norm_positive = positive / np.linalg.norm(positive)\n        # Normalize each row (negative vector) of the matrix. keepdims is important for broadcasting.\n        norm_negatives = negatives / np.linalg.norm(negatives, axis=1, keepdims=True)\n\n        # Compute similarity scores (dot products).\n        s_pos = np.dot(norm_anchor, norm_positive)\n        s_neg = np.dot(norm_negatives, norm_anchor) # Results in a vector of scores\n\n        # Scale scores by temperature.\n        s_pos_scaled = s_pos / tau\n        s_neg_scaled = s_neg / tau\n\n        # Denominator for the standard InfoNCE loss.\n        exp_pos = np.exp(s_pos_scaled)\n        exp_neg = np.exp(s_neg_scaled)\n        D_std = exp_pos + np.sum(exp_neg)\n\n        # Denominator for the debiased contrastive loss.\n        # The weight is w = 1 - sigma(s_neg/tau) = sigma(-s_neg/tau).\n        # sigma(z) = 1 / (1 + exp(-z)), so sigma(-z) = 1 / (1 + exp(z)).\n        weights = 1.0 / (1.0 + np.exp(s_neg_scaled))\n        debiased_neg_terms = weights * exp_neg\n        D_debiased = exp_pos + np.sum(debiased_neg_terms)\n\n        # The difference L_std - L_debiased simplifies to log(D_std) - log(D_debiased).\n        loss_difference = np.log(D_std) - np.log(D_debiased)\n        results.append(loss_difference)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3156689"}, {"introduction": "Beyond refining the loss function, we can dramatically improve training efficiency by optimizing the negative sampling distribution itself. This advanced problem frames the design of a sampling strategy as a variance reduction task, a powerful concept borrowed from statistics. You will derive and implement a method to find the optimal mixture between a frequency-based sampler and a hardness-aware sampler, minimizing gradient variance and thereby creating a more stable and effective training signal for a fixed computational budget. [@problem_id:3156767]", "problem": "You are given a finite set of candidate negative items indexed by $i \\in \\{1,\\dots,n\\}$ with a base data distribution $p(i)$ over items. Consider the task of estimating an expectation of the form $\\mu = \\mathbb{E}_{i \\sim p}[h(i)]$ where $h(i) \\ge 0$ quantifies the magnitude of a per-item contribution to a gradient term in a sampled objective. A standard unbiased importance sampling estimator using $k$ independent negative samples drawn from a proposal distribution $q$ is\n$$\n\\widehat{\\mu} \\;=\\; \\frac{1}{k}\\sum_{j=1}^{k} \\frac{p(i_j)}{q(i_j)}\\,h(i_j), \\quad i_j \\sim q \\text{ independently}.\n$$\nThis estimator has variance\n$$\n\\mathrm{Var}[\\widehat{\\mu}] \\;=\\; \\frac{1}{k}\\left(\\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{q(i)} \\;-\\; \\mu^{2}\\right),\n$$\nwhich is minimized, for a fixed $k$, by appropriately choosing $q$. In negative sampling practice, one often uses a mixture sampler that combines a frequency-based distribution with a hardness-based distribution. Define the mixture sampler\n$$\nq_{\\lambda}(i) \\;=\\; \\lambda\\, q_{\\mathrm{freq}}(i) \\;+\\; (1-\\lambda)\\, q_{\\mathrm{hard}}(i),\n$$\nwhere $q_{\\mathrm{freq}}(i) = p(i)$ and $q_{\\mathrm{hard}}(i) \\propto h(i)$, that is,\n$$\nq_{\\mathrm{hard}}(i) \\;=\\; \\frac{h(i)}{\\sum_{j=1}^{n} h(j)}.\n$$\nFor a fixed sampling budget $k \\in \\mathbb{N}$, the generalization-error proxy in this setting is taken to be the variance $\\mathrm{Var}[\\widehat{\\mu}]$, because lower estimator variance for the negative term yields lower noise in training signals and improved generalization. Your task is to choose the mixture weight $\\lambda \\in [0,1]$ that minimizes $\\mathrm{Var}[\\widehat{\\mu}]$ under the mixture family $q_{\\lambda}$.\n\nStarting from the fundamental definition of importance sampling and the variance formula above, derive an algorithm that, given a discrete distribution $p$ and nonnegative weights $h$, finds the optimal $\\lambda^{\\star} \\in [0,1]$ minimizing $\\mathrm{Var}[\\widehat{\\mu}]$. Assume $h(i) > 0$ for all $i$ in the provided test cases so that $q_{\\mathrm{hard}}(i)$ is well-defined and strictly positive.\n\nImplementation requirements:\n- Your program must implement a numerically stable solver for $\\lambda^{\\star}$ using only the principle that the variance is minimized when the function\n$$\nJ(\\lambda) \\;=\\; \\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{\\lambda\\,p(i) + (1-\\lambda)\\,q_{\\mathrm{hard}}(i)}\n$$\nis minimized over $\\lambda \\in [0,1]$.\n- In case multiple $\\lambda$ achieve the same minimum within a numerical tolerance of $\\varepsilon = 10^{-12}$, return the smallest such $\\lambda$.\n- For each test case, output only the optimal $\\lambda^{\\star}$ rounded to four decimal places.\n\nTest suite:\n- Case A (interior optimum expected): $n=5$, $p = [\\,0.40,\\,0.25,\\,0.20,\\,0.10,\\,0.05\\,]$, $h = [\\,1.0,\\,2.0,\\,2.5,\\,3.0,\\,5.0\\,]$, $k = 10$.\n- Case B (boundary at $\\lambda = 1$ expected): $n=4$, $p = [\\,0.50,\\,0.30,\\,0.15,\\,0.05\\,]$, $h = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$, $k = 7$.\n- Case C (boundary at $\\lambda = 0$ likely): $n=5$, $p = [\\,0.40,\\,0.25,\\,0.20,\\,0.10,\\,0.05\\,]$, $h = [\\,0.1,\\,0.1,\\,0.5,\\,4.0,\\,10.0\\,]$, $k = 3$.\n- Case D (degenerate equality $q_{\\mathrm{hard}} = p$): $n=4$, $p = [\\,0.40,\\,0.30,\\,0.20,\\,0.10\\,]$, $h = [\\,4.0,\\,3.0,\\,2.0,\\,1.0\\,]$, $k = 5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four optimal values $\\lambda^{\\star}$ for Cases A, B, C, and D, as a comma-separated list enclosed in square brackets, for example, $[\\lambda_{A},\\lambda_{B},\\lambda_{C},\\lambda_{D}]$, where each entry is rounded to four decimal places.", "solution": "The objective is to find the mixture weight $\\lambda \\in [0,1]$ that minimizes the variance of the importance sampling estimator, $\\mathrm{Var}[\\widehat{\\mu}]$. The variance is given by:\n$$\n\\mathrm{Var}[\\widehat{\\mu}] = \\frac{1}{k}\\left(\\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{q(i)} - \\mu^{2}\\right)\n$$\nTo minimize $\\mathrm{Var}[\\widehat{\\mu}]$ with respect to the choice of sampling distribution $q$, we only need to consider the term that depends on $q$. Since $k$, $p(i)$, $h(i)$, and $\\mu$ are fixed for this optimization problem, the task is equivalent to minimizing the summation term. The proposal distribution is a mixture model $q_{\\lambda}(i) = \\lambda\\, q_{\\mathrm{freq}}(i) + (1-\\lambda)\\, q_{\\mathrm{hard}}(i)$, where $q_{\\mathrm{freq}}(i) = p(i)$ and $q_{\\mathrm{hard}}(i) = h(i) / \\sum_{j=1}^{n} h(j)$. Substituting $q_{\\lambda}(i)$ for $q(i)$, we obtain an objective function $J(\\lambda)$ to be minimized over the interval $\\lambda \\in [0,1]$:\n$$\nJ(\\lambda) = \\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{\\lambda\\,p(i) + (1-\\lambda)\\,q_{\\mathrm{hard}}(i)}\n$$\nThis function $J(\\lambda)$ is a sum of individual functions, one for each item $i \\in \\{1, \\dots, n\\}$. Let's denote each term by $f_i(\\lambda)$:\n$$\nf_i(\\lambda) = \\frac{c_i}{\\ell_i(\\lambda)}\n$$\nwhere $c_i = p(i)^2 h(i)^2$ is a non-negative constant, and $\\ell_i(\\lambda) = \\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i)$ is the denominator, which is a linear function of $\\lambda$. The problem assumes that $h(i) > 0$ for all $i$, which implies $q_{\\mathrm{hard}}(i) > 0$. As $p(i)$ is a probability distribution over the $n$ items, we can assume $p(i)>0$ for all $i$ in the support. Therefore, for $\\lambda \\in [0,1]$, $\\ell_i(\\lambda)$ is a convex combination of two strictly positive numbers, $p(i)$ and $q_{\\mathrm{hard}}(i)$, and is thus strictly positive.\n\nTo determine the nature of the optimization problem, we analyze the convexity of $J(\\lambda)$. The second derivative of each term $f_i(\\lambda)$ with respect to $\\lambda$ is:\n$$\n\\frac{d^2 f_i}{d\\lambda^2} = \\frac{d^2}{d\\lambda^2} \\left( c_i (\\ell_i(\\lambda))^{-1} \\right) = \\frac{d}{d\\lambda} \\left( -c_i (\\ell_i(\\lambda))^{-2} \\frac{d\\ell_i}{d\\lambda} \\right)\n$$\nThe derivative of the denominator is $\\frac{d\\ell_i}{d\\lambda} = p(i) - q_{\\mathrm{hard}}(i)$. So,\n$$\n\\frac{d^2 f_i}{d\\lambda^2} = -c_i \\left( -2 (\\ell_i(\\lambda))^{-3} \\left(\\frac{d\\ell_i}{d\\lambda}\\right)^2 \\right) = \\frac{2c_i(p(i) - q_{\\mathrm{hard}}(i))^2}{(\\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i))^3}\n$$\nSince $c_i \\ge 0$ and the denominator is positive, the second derivative $\\frac{d^2 f_i}{d\\lambda^2}$ is non-negative for all $\\lambda \\in [0,1]$. This proves that each function $f_i(\\lambda)$ is convex. The sum of convex functions is also convex, so $J(\\lambda)$ is a convex function on the interval $[0,1]$.\n\nThe minimum of a convex function on a closed interval occurs either at one of the boundaries ($\\lambda=0$ or $\\lambda=1$) or at an interior point $\\lambda^{\\star} \\in (0,1)$ where the first derivative is zero. The first derivative of $J(\\lambda)$ is:\n$$\nJ'(\\lambda) = \\frac{dJ}{d\\lambda} = \\sum_{i=1}^{n} \\frac{-c_i(p(i) - q_{\\mathrm{hard}}(i))}{(\\ell_i(\\lambda))^2} = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{(\\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i))^2}\n$$\nSince $J(\\lambda)$ is convex, its derivative $J'(\\lambda)$ is a monotonically non-decreasing function. This property allows for an efficient search for the optimal $\\lambda^{\\star}$:\n1.  If $J'(0) \\ge 0$, the function is non-decreasing on $[0,1]$, so the minimum is at $\\lambda^{\\star}=0$.\n2.  If $J'(1) \\le 0$, the function is non-increasing on $[0,1]$, so the minimum is at $\\lambda^{\\star}=1$.\n3.  If $J'(0) < 0$ and $J'(1) > 0$, the continuity and monotonicity of $J'(\\lambda)$ guarantee a unique root $\\lambda^{\\star} \\in (0,1)$ where $J'(\\lambda^{\\star})=0$. This root corresponds to the unique minimum of $J(\\lambda)$.\n\nThis leads to the following algorithm:\nFirst, compute the derivatives at the boundaries, $J'(0)$ and $J'(1)$.\n$$\nJ'(0) = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{q_{\\mathrm{hard}}(i)^2}\n$$\n$$\nJ'(1) = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{p(i)^2} = - \\sum_{i=1}^{n} h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))\n$$\nBased on their signs, we either identify $\\lambda^{\\star}$ as $0$ or $1$, or we proceed to find the root of $J'(\\lambda)=0$ in the interval $(0,1)$.\nFor the interior case, the equation $J'(\\lambda)=0$ is a non-linear equation that can be solved numerically. Given that $J'(\\lambda)$ is monotonic and we have bracketed a root between $\\lambda=0$ and $\\lambda=1$, the bisection method is a simple and robust choice for finding $\\lambda^{\\star}$. The bisection algorithm iteratively halves the search interval $[low, high]$ by evaluating $J'(mid)$ at the midpoint and updating the interval based on the sign, guaranteeing convergence to the unique root. In the special case where $p(i) = q_{\\mathrm{hard}}(i)$ for all $i$, $J'(\\lambda)=0$ for all $\\lambda \\in [0,1]$, making all values of $\\lambda$ minimizers. The problem requires returning the smallest one, which is $\\lambda^{\\star}=0$. This case is correctly handled by the condition $J'(0) \\ge 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the optimization problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'p': [0.40, 0.25, 0.20, 0.10, 0.05], 'h': [1.0, 2.0, 2.5, 3.0, 5.0]},\n        # Case B\n        {'p': [0.50, 0.30, 0.15, 0.05], 'h': [1.0, 1.0, 1.0, 1.0]},\n        # Case C\n        {'p': [0.40, 0.25, 0.20, 0.10, 0.05], 'h': [0.1, 0.1, 0.5, 4.0, 10.0]},\n        # Case D\n        {'p': [0.40, 0.30, 0.20, 0.10], 'h': [4.0, 3.0, 2.0, 1.0]},\n    ]\n\n    def find_optimal_lambda(p, h):\n        \"\"\"\n        Calculates the optimal lambda that minimizes the variance estimator.\n\n        Args:\n            p (list or np.ndarray): The base data distribution.\n            h (list or np.ndarray): The per-item contribution weights.\n\n        Returns:\n            float: The optimal lambda value.\n        \"\"\"\n        p_vec = np.array(p, dtype=np.float64)\n        h_vec = np.array(h, dtype=np.float64)\n\n        sum_h = np.sum(h_vec)\n        q_hard_vec = h_vec / sum_h if sum_h > 0 else np.full_like(p_vec, 1.0/len(p_vec))\n\n        def J_prime(lam, p, h, q_hard):\n            \"\"\"\n            Computes the derivative of the objective function J with respect to lambda.\n            \"\"\"\n            numerator = p**2 * h**2 * (p - q_hard)\n            denominator = (lam * p + (1.0 - lam) * q_hard)**2\n            \n            # To handle potential division by zero in theory, though not \n            # expected with p(i)>0, h(i)>0.\n            # Using np.divide with a where clause is safer than adding an epsilon.\n            terms = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n            \n            return -np.sum(terms)\n\n        # A small tolerance for floating-point comparisons near zero.\n        # This is based on the logic that if the derivative is extremely\n        # close to zero at a boundary, we can consider the optimum to be there.\n        tol = 1e-12\n\n        # 1. Check boundary at lambda = 0\n        J_prime_at_0 = J_prime(0.0, p_vec, h_vec, q_hard_vec)\n        if J_prime_at_0 >= -tol:\n            return 0.0\n\n        # 2. Check boundary at lambda = 1\n        J_prime_at_1 = J_prime(1.0, p_vec, h_vec, q_hard_vec)\n        if J_prime_at_1 = tol:\n            return 1.0\n\n        # 3. Find interior minimum using bisection\n        low = 0.0\n        high = 1.0\n        # 100 iterations are sufficient for double-precision floating-point accuracy.\n        for _ in range(100):\n            mid = low + 0.5 * (high - low)\n            # If mid is indistinguishable from low or high, stop.\n            if mid == low or mid == high:\n                break\n            val = J_prime(mid, p_vec, h_vec, q_hard_vec)\n            if val  0:\n                low = mid\n            else:\n                high = mid\n        \n        return (low + high) / 2.0\n\n    results = []\n    for case in test_cases:\n        p_dist = case['p']\n        h_weights = case['h']\n        lambda_star = find_optimal_lambda(p_dist, h_weights)\n        results.append(lambda_star)\n\n    # Format results to four decimal places for the final output string.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3156767"}]}