{"hands_on_practices": [{"introduction": "This first exercise guides you through the end-to-end process of creating and evaluating distributional word embeddings from a raw text corpus. You will learn how practical decisions, such as cleaning the text by removing stopwords or duplicate sentences, directly impact the statistical properties of word contexts and the stability of the final embeddings. This practice builds a foundational understanding of the empirical pipeline that underpins the distributional hypothesis [@problem_id:3182871].", "problem": "You are given a small text corpus and asked to empirically test the distributional hypothesis in deep learning: \"You shall know a word by the company it keeps.\" Starting from the core definitions of conditional distributions, entropy, and similarity, you must write a complete program that quantifies how corpus cleaning operations affect conditional context distributions and distributional embeddings. The cleaning operations are stopword removal and deduplication of duplicate sentences. You must compute the following for a raw corpus and its cleaned version, and then report the changes (cleaned minus raw):\n\n- Sparsity of the conditional context distribution for words, defined over the union of context types appearing in either corpus.\n- Average context entropy, which captures the uncertainty in the conditional distribution of contexts given a word.\n- Hubness in the embedding space, which measures the skewness of how often a word appears in other words' top-$k$ nearest-neighbor lists.\n- Embedding stability, quantified as the average cosine similarity between corresponding word embeddings before and after cleaning.\n\nUse the following fundamental base:\n- The distributional hypothesis states that words are characterized by their contexts. Represent this through word-context co-occurrence counts in a symmetric window of size $w$. For a target word $w$, the conditional context distribution is $p(c \\mid w) = \\frac{N(w,c)}{\\sum_{c'} N(w,c')}$, where $N(w,c)$ is the co-occurrence count.\n- Shannon entropy of a discrete distribution is $H(p) = -\\sum_i p_i \\log p_i$ using the natural logarithm.\n- Positive Pointwise Mutual Information (PPMI) is defined for a word-context pair as $\\max\\left(0, \\log \\frac{N(w,c)\\cdot N_{\\mathrm{tot}}}{\\left(\\sum_{c'} N(w,c')\\right)\\left(\\sum_{w'} N(w',c)\\right)}\\right)$, where $N_{\\mathrm{tot}} = \\sum_{w,c} N(w,c)$ is the total count. Use the PPMI row vector for a word as its distributional embedding and apply row-wise $\\ell_2$ normalization to enable cosine similarity comparisons.\n- Nearest neighbors are computed by cosine similarity. For each word, take its top-$k$ nearest neighbors (excluding itself). Hubness is quantified by the skewness of the distribution of $k$-occurrence counts across the vocabulary: if $x_i$ is the number of times word $i$ appears in others' top-$k$ lists, and $\\mu$ and $\\sigma$ are the mean and standard deviation of $\\{x_i\\}$, then the skewness is $\\frac{1}{n}\\sum_i \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^3$, where $n$ is the number of words.\n\nImplement the following steps:\n1. Tokenize sentences to lowercase words, stripping basic punctuation. Define a set of stopwords and remove them only if requested by the test case. Deduplicate sentences only if requested. Deduplication means removing exact duplicate token sequences.\n2. Build co-occurrence counts $N(w,c)$ using a symmetric window of size $w$ around each target word. Define contexts as the union of all tokens from the raw and cleaned corpora. Define target words as non-stopwords that appear with at least one context in both the raw and cleaned corpora.\n3. Compute $p(c \\mid w)$ for all target words on the shared context set. Define sparsity as the average proportion of zero entries in $p(c \\mid w)$ across target words. Define average context entropy as the average of $H\\left(p(\\cdot \\mid w)\\right)$ across target words, treating $0\\log 0$ as $0$.\n4. Compute PPMI matrices for raw and cleaned corpora on the same target words and context set. Normalize each row to unit $\\ell_2$ norm to obtain word embeddings. Compute:\n   - Embedding stability: for each target word, compute cosine similarity between its raw and cleaned embeddings, then average across words.\n   - Hubness: for raw and cleaned embeddings separately, compute the top-$k$ nearest neighbors (cosine similarity) for each target word and count how many times each word appears in others' lists. Compute the skewness of this $k$-occurrence distribution. Report the change as skewness(cleaned) minus skewness(raw).\n\nTest suite:\nUse the following fixed raw corpus (ten sentences, with duplicates included), stopwords, and three parameter cases.\n\nRaw corpus sentences:\n- \"The cat sat on the mat and the cat slept\"\n- \"Dogs bark and cats meow\"\n- \"A cat and a dog in the house\"\n- \"The house has a mat\"\n- \"The cat sat on the mat and the cat slept\"\n- \"Dogs bark and cats meow\"\n- \"The cat chased a mouse in the house\"\n- \"Cats and dogs in the yard\"\n- \"The yard has a house and a mat\"\n- \"A dog sleeps on the mat\"\n\nStopwords set: {\"the\",\"and\",\"a\",\"of\",\"to\",\"in\",\"on\",\"has\",\"is\",\"was\",\"are\"}\n\nParameter cases (each is one test case):\n- Case 1: window size $w=2$, top-$k=2$, remove stopwords = True, deduplicate = False.\n- Case 2: window size $w=2$, top-$k=2$, remove stopwords = False, deduplicate = True.\n- Case 3: window size $w=1$, top-$k=3$, remove stopwords = True, deduplicate = True.\n\nFor each case, compute and return a list of four floats:\n- $\\Delta S$, the change in sparsity (cleaned minus raw).\n- $\\Delta H$, the change in average context entropy (cleaned minus raw).\n- $\\Delta \\mathrm{hub}$, the change in hubness skewness (cleaned minus raw).\n- $\\mathrm{stab}$, the embedding stability (average cosine similarity between corresponding raw and cleaned embeddings).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of three inner lists (one per case), each inner list being [ΔS,ΔH,Δhub,stab], rounded to six decimal places, enclosed in square brackets. For example, \"[[0.0,0.1,-0.2,0.95],[...],[...]]\".", "solution": "The problem requires an empirical investigation of the distributional hypothesis by quantifying the effects of corpus cleaning operations (stopword removal and sentence deduplication) on several linguistic and geometric properties of word representations. The analysis will be performed on a raw corpus and a cleaned version, comparing four key metrics: distributional sparsity, average context entropy, embedding hubness, and embedding stability. The solution proceeds by first formalizing the processing pipeline and then implementing the required metrics.\n\n### Principle-Based Design\n\nThe solution is founded on the distributional hypothesis, which posits that the meaning of a word is determined by the contexts in which it appears. We will model this principle by constructing co-occurrence statistics and deriving distributional representations from them.\n\n**Step 1: Corpus Preprocessing and Vocabulary Definition**\n\nFirst, we define a standardized procedure to process the text corpus. For each test case, we generate two versions of the corpus: a \"raw\" version and a \"cleaned\" version.\n- **Tokenization**: Each sentence is converted to lowercase, and non-alphanumeric characters are removed. The sentence is then split into a sequence of tokens.\n- **Cleaning**: The specified cleaning operations are applied. If `deduplicate` is true, we remove duplicate token sequences, preserving the first occurrence. If `remove_stopwords` is true, we filter out tokens present in the provided stopword set.\n\nTo ensure valid comparisons between the raw and cleaned corpora, we establish a common ground for analysis:\n- **`context_vocab`**: The set of all unique tokens that appear in either the raw or the cleaned corpus. This forms the basis for our co-occurrence and embedding dimensions.\n- **`target_words`**: The set of words whose properties we will analyze. As specified, a word is included as a target if and only if it is not a stopword and it co-occurs with at least one context word in *both* the raw and the cleaned corpora. This ensures that every word we analyze has a defined representation before and after cleaning, making comparisons meaningful.\n\n**Step 2: Co-occurrence Matrix Construction**\n\nThe core of the distributional analysis is the word-context co-occurrence matrix, $N$. For each target word $w$ (a row) and each context word $c$ (a column), the entry $N(w,c)$ stores the number of times $c$ appears within a symmetric window of size $w_{size}$ around $w$. We construct two such matrices, $N_{\\mathrm{raw}}$ and $N_{\\mathrm{cleaned}}$, using the respective processed corpora but defined over the common `target_words` and `context_vocab`.\n\n**Step 3: Distributional Sparsity and Context Entropy**\n\nFrom the co-occurrence matrix $N$, we derive the conditional probability distribution of a context $c$ given a target word $w$:\n$$ p(c \\mid w) = \\frac{N(w,c)}{\\sum_{c'} N(w,c')} $$\nThis distribution embodies the \"company\" a word keeps. We quantify its properties as follows:\n- **Sparsity ($S$)**: The average fraction of zero entries in the conditional probability vectors $p(\\cdot \\mid w)$ across all target words. A higher sparsity indicates that a word is associated with a smaller, more specific set of contexts.\n- **Average Context Entropy ($H$)**: The uncertainty or \"spread\" of the context distribution, averaged across all target words. The entropy for a single word $w$ is given by Shannon's formula using the natural logarithm:\n$$ H(p(\\cdot \\mid w)) = -\\sum_{c \\in \\text{context\\_vocab}} p(c \\mid w) \\log p(c \\mid w) $$\nwhere we define $0 \\log 0 = 0$. A lower entropy implies a more predictable, less uniform context distribution.\n\n**Step 4: PPMI Embeddings and Stability**\n\nTo create vector representations (embeddings) for words, we use Positive Pointwise Mutual Information (PPMI). PPMI re-weights the co-occurrence counts to emphasize surprising or informative pairings. The PPMI for a pair $(w,c)$ is:\n$$ \\mathrm{PPMI}(w,c) = \\max\\left(0, \\log \\frac{N(w,c) \\cdot N_{\\mathrm{tot}}}{N(w) \\cdot N(c)}\\right) $$\nwhere $N(w) = \\sum_{c'} N(w,c')$, $N(c) = \\sum_{w'} N(w',c)$, and $N_{\\mathrm{tot}} = \\sum_{w,c} N(w,c)$. The embedding for each word $w$ is its corresponding row vector in the PPMI matrix. These vectors are then $\\ell_2$-normalized to facilitate cosine similarity comparisons.\n\n- **Embedding Stability ($\\mathrm{stab}$)**: This metric measures how much the cleaning process alters the word embeddings. It is calculated as the average cosine similarity between a word's raw embedding ($v_{\\mathrm{raw}}$) and its cleaned embedding ($v_{\\mathrm{cleaned}}$) across all target words. Since the vectors are $\\ell_2$-normalized, this is simply the average of their dot products.\n\n**Step 5: Hubness Calculation**\n\nHubness is a phenomenon in high-dimensional spaces where some points (hubs) appear as nearest neighbors to many other points. We quantify this by analyzing the distribution of \"k-occurrence\" counts.\n1. For a set of embeddings, compute the pairwise cosine similarity matrix.\n2. For each word, identify its top-$k$ nearest neighbors (excluding itself).\n3. For each word $w_i$, count how many times it appears in the top-$k$ lists of other words. This is its k-occurrence count, $x_i$.\n4. The hubness is defined as the skewness of the distribution of these counts $\\{x_i\\}$. A positive skew indicates the presence of hubs. The population skewness is computed as:\n$$ \\mathrm{skew} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^3 $$\nwhere $n$ is the number of target words, and $\\mu$ and $\\sigma$ are the mean and standard deviation of the k-occurrence counts. If $\\sigma=0$, the skewness is defined as $0$.\n\nBy computing these four metrics for both the raw and cleaned corpora, we can calculate the changes ($\\Delta S$, $\\Delta H$, $\\Delta \\mathrm{hub}$) and stability ($\\mathrm{stab}$) to provide a quantitative answer for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport re\nfrom collections import Counter\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    raw_corpus_sentences = [\n        \"The cat sat on the mat and the cat slept\", \"Dogs bark and cats meow\",\n        \"A cat and a dog in the house\", \"The house has a mat\",\n        \"The cat sat on the mat and the cat slept\", \"Dogs bark and cats meow\",\n        \"The cat chased a mouse in the house\", \"Cats and dogs in the yard\",\n        \"The yard has a house and a mat\", \"A dog sleeps on the mat\"\n    ]\n    stopwords = {\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"on\", \"has\", \"is\", \"was\", \"are\"}\n    test_cases = [\n        # (window_size, k, remove_stopwords, deduplicate)\n        (2, 2, True, False),\n        (2, 2, False, True),\n        (1, 3, True, True),\n    ]\n\n    all_results = []\n    for window_size, k, remove_stops, dedup in test_cases:\n        # Step 1: Preprocess corpora\n        raw_processed = preprocess_corpus(raw_corpus_sentences, stopwords, remove_stopwords=False, deduplicate=False)\n        cleaned_processed = preprocess_corpus(raw_corpus_sentences, stopwords, remove_stops, dedup)\n        \n        # Step 2: Define vocabularies and target words\n        raw_tokens = {token for sent in raw_processed for token in sent}\n        cleaned_tokens = {token for sent in cleaned_processed for token in sent}\n        \n        def get_words_with_contexts(corpus, w_size):\n            words = set()\n            for sent in corpus:\n                if len(sent) > 1:\n                    for i, word in enumerate(sent):\n                        start = max(0, i - w_size)\n                        end = min(len(sent), i + w_size + 1)\n                        context = sent[start:i] + sent[i + 1:end]\n                        if context:\n                            words.add(word)\n            return words\n\n        raw_active_words = get_words_with_contexts(raw_processed, window_size)\n        cleaned_active_words = get_words_with_contexts(cleaned_processed, window_size)\n        \n        potential_targets = raw_active_words  cleaned_active_words\n        target_words = sorted([w for w in potential_targets if w not in stopwords])\n        \n        context_vocab = sorted(list(raw_tokens | cleaned_tokens))\n        \n        if not target_words:\n            all_results.append([0.0, 0.0, 0.0, 0.0])\n            continue\n\n        target_map = {word: i for i, word in enumerate(target_words)}\n        context_map = {word: i for i, word in enumerate(context_vocab)}\n\n        # Step 3  4: Run analysis for both corpora\n        raw_metrics = run_analysis(raw_processed, target_words, target_map, context_vocab, context_map, window_size, k)\n        cleaned_metrics = run_analysis(cleaned_processed, target_words, target_map, context_vocab, context_map, window_size, k)\n        \n        # Step 5: Compute final results\n        delta_sparsity = cleaned_metrics[\"sparsity\"] - raw_metrics[\"sparsity\"]\n        delta_entropy = cleaned_metrics[\"avg_entropy\"] - raw_metrics[\"avg_entropy\"]\n        delta_hubness = cleaned_metrics[\"hubness\"] - raw_metrics[\"hubness\"]\n        \n        raw_embeds = raw_metrics[\"embeddings\"]\n        cleaned_embeds = cleaned_metrics[\"embeddings\"]\n        # Cosine similarity for normalized vectors is their dot product\n        stability = np.mean(np.sum(raw_embeds * cleaned_embeds, axis=1))\n\n        all_results.append([delta_sparsity, delta_entropy, delta_hubness, stability])\n\n    # Format the final output string\n    formatted_lists = [f\"[{','.join([f'{v:.6f}' for v in sublist])}]\" for sublist in all_results]\n    final_output = f\"[{','.join(formatted_lists)}]\"\n    print(final_output)\n\ndef preprocess_corpus(sentences, stopwords, remove_stopwords, deduplicate):\n    \"\"\"Tokenizes, cleans, and optionally deduplicates sentences.\"\"\"\n    tokenized_sents = []\n    for s in sentences:\n        s_clean = re.sub(r'[^\\w\\s]', '', s.lower())\n        tokens = s_clean.split()\n        if remove_stopwords:\n            tokens = [token for token in tokens if token not in stopwords]\n        if tokens:\n            tokenized_sents.append(tokens)\n\n    if deduplicate:\n        unique_sents = []\n        seen = set()\n        for sent in tokenized_sents:\n            sent_tuple = tuple(sent)\n            if sent_tuple not in seen:\n                unique_sents.append(sent)\n                seen.add(sent_tuple)\n        return unique_sents\n    return tokenized_sents\n\ndef build_cooccurrence(corpus, target_map, context_map, window_size):\n    \"\"\"Builds the word-context co-occurrence matrix.\"\"\"\n    num_targets = len(target_map)\n    num_contexts = len(context_map)\n    cooc_matrix = np.zeros((num_targets, num_contexts), dtype=float)\n\n    for sent in corpus:\n        for i, token in enumerate(sent):\n            if token in target_map:\n                target_idx = target_map[token]\n                start = max(0, i - window_size)\n                end = min(len(sent), i + window_size + 1)\n                for j in range(start, end):\n                    if i == j:\n                        continue\n                    context_word = sent[j]\n                    if context_word in context_map:\n                        context_idx = context_map[context_word]\n                        cooc_matrix[target_idx, context_idx] += 1\n    return cooc_matrix\n\ndef calculate_ppmi(N):\n    \"\"\"Calculates the Positive Pointwise Mutual Information matrix from a co-occurrence matrix N.\"\"\"\n    N_w = N.sum(axis=1, keepdims=True)\n    N_c = N.sum(axis=0, keepdims=True)\n    N_tot = N.sum()\n\n    if N_tot == 0:\n        return np.zeros_like(N)\n\n    # Denominator: N(w) * N(c)\n    denominator = N_w @ N_c\n    \n    # Ratio inside the log\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = (N * N_tot) / denominator\n\n    # log(ratio), handling -inf and nan\n    with np.errstate(all='ignore'):\n        pmi = np.log(ratio)\n    \n    pmi[np.isneginf(pmi)] = 0\n    pmi[np.isnan(pmi)] = 0\n    \n    ppmi = np.maximum(0, pmi)\n    return ppmi\n\ndef calculate_hubness(embeddings, k):\n    \"\"\"Calculates hubness skewness for a set of embeddings.\"\"\"\n    num_words = embeddings.shape[0]\n    if num_words = k:\n        return 0.0\n\n    sim_matrix = embeddings @ embeddings.T\n    np.fill_diagonal(sim_matrix, -1)\n    \n    top_k_indices = np.argsort(sim_matrix, axis=1)[:, -k:]\n    \n    k_occurrences = np.bincount(top_k_indices.flatten(), minlength=num_words)\n    \n    std_dev = k_occurrences.std()\n    if std_dev == 0:\n        return 0.0\n    \n    mean_val = k_occurrences.mean()\n    skewness = np.mean(((k_occurrences - mean_val) / std_dev) ** 3)\n    return skewness\n\ndef run_analysis(corpus, target_words, target_map, context_vocab, context_map, window_size, k):\n    \"\"\"Performs the full analysis pipeline for a given corpus.\"\"\"\n    cooc_matrix = build_cooccurrence(corpus, target_map, context_map, window_size)\n    \n    # Sparsity and Entropy\n    row_sums = cooc_matrix.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1 # Avoid division by zero\n    p_c_given_w = cooc_matrix / row_sums\n    \n    sparsity = np.mean(p_c_given_w == 0)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        term = p_c_given_w * np.log(p_c_given_w)\n        term[np.isnan(term)] = 0\n    avg_entropy = np.mean(-np.sum(term, axis=1))\n    \n    # PPMI and Embeddings\n    ppmi_matrix = calculate_ppmi(cooc_matrix)\n    norms = np.linalg.norm(ppmi_matrix, axis=1, keepdims=True)\n    norms[norms == 0] = 1\n    embeddings = ppmi_matrix / norms\n    \n    # Hubness\n    hubness = calculate_hubness(embeddings, k)\n    \n    return {\n        \"sparsity\": sparsity,\n        \"avg_entropy\": avg_entropy,\n        \"hubness\": hubness,\n        \"embeddings\": embeddings\n    }\n\nsolve()\n```", "id": "3182871"}, {"introduction": "The distributional hypothesis can be operationalized in two primary ways: predicting a word from its context, or predicting the context from a word. This exercise contrasts these two fundamental views by building embeddings from both conditional probabilities, $p(\\text{word}|\\text{context})$ and $p(\\text{context}|\\text{word})$, which are analogous to the objectives in Masked Language Modeling and Skip-gram models. By comparing the geometric similarity of words in these two resulting spaces, you will gain a deeper intuition for how this core modeling choice shapes the nature of the learned representations [@problem_id:3182958].", "problem": "You are given a fixed, small corpus and asked to operationalize the distributional hypothesis in two canonical deep learning objectives, Masked Language Modeling (MLM) and Skip-gram (SG), using principled count-based estimators and to compare the resulting geometries of word embeddings. The distributional hypothesis states that words occurring in similar contexts tend to have similar meanings. Your task is to start from core probabilistic definitions and well-tested statistical procedures and derive, implement, and compare conditional distributions and the induced vector-space geometry.\n\nCorpus (tokenized, all lowercase, punctuation removed; each line is a sentence): \nthe cat sat on the mat\nthe dog sat on the rug\na cat chased a mouse\na dog chased a ball\nmusic and song fill the hall\nthe song played music softly\nquantum theory explains physics\ntheory of music and physics\nthe quantum cat thought of physics\ndogs and cats share a home\na theory about a song\n\nDefinitions to use as fundamental base:\n- Conditional probability: for events $A$ and $B$, $p(A \\mid B) = \\frac{p(A, B)}{p(B)}$ whenever $p(B) > 0$.\n- Maximum Likelihood Estimation (MLE) for a multinomial distribution: given counts $\\{n_i\\}$ over outcomes $\\{i\\}$, $\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$.\n- Additive (Laplace) smoothing: for counts $\\{n_i\\}$ over $V$ outcomes and smoothing parameter $\\alpha > 0$, define $\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$.\n- Cosine similarity: for vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^d$, $\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$.\n\nTasks to perform:\n1. Construct a symmetric context window of size $K$ around each target word $w$ in the corpus. For each occurrence at position $i$, include all words at positions $j$ with $|i - j| \\le K$ and $j \\ne i$ as context words $c$. Accumulate co-occurrence counts $N(w, c)$ over the entire corpus. Use the entire token vocabulary as the set of possible $w$ and $c$.\n2. Define Masked Language Modeling (MLM): predict $w$ from $c$. From the counts $N(w, c)$, derive the conditional distribution $p_{\\mathrm{MLM}}(w \\mid c)$ using Maximum Likelihood Estimation with additive smoothing parameter $\\alpha$, consistent with the above definitions.\n3. Define Skip-gram (SG): predict $c$ from $w$. From the same counts $N(w, c)$, derive $p_{\\mathrm{SG}}(c \\mid w)$ using Maximum Likelihood Estimation with additive smoothing parameter $\\alpha$, consistent with the above definitions.\n4. Embed each word $w$ in two spaces:\n   - MLM embedding $\\mathbf{v}_{\\mathrm{MLM}}(w)$: interpret $p_{\\mathrm{MLM}}(w \\mid c)$ as a function of $c$ and create a vector whose components are indexed by contexts $c$.\n   - SG embedding $\\mathbf{v}_{\\mathrm{SG}}(w)$: interpret $p_{\\mathrm{SG}}(c \\mid w)$ as a function of $c$ and create a vector whose components are indexed by contexts $c$.\n   Ensure both embeddings have the same dimension equal to the vocabulary size $V$, ordered by a fixed, deterministic ordering of the vocabulary.\n5. For each test case below, compute the cosine similarity between two specified words under both geometries, that is, compute $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$ and $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$, and report their difference $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$.\n6. Use additive smoothing with parameter $\\alpha > 0$ and symmetric window size $K \\in \\mathbb{N}$ as specified in the test suite. Angles are not required; report only cosine similarities as real numbers.\n\nTest suite (each case is of the form $(K, \\alpha, w_1, w_2)$):\n- Case $1$: $K = 2$, $\\alpha = 0.5$, $w_1 =$ \"cat\", $w_2 =$ \"dog\".\n- Case $2$: $K = 1$, $\\alpha = 1.0$, $w_1 =$ \"music\", $w_2 =$ \"song\".\n- Case $3$: $K = 3$, $\\alpha = 10^{-6}$, $w_1 =$ \"the\", $w_2 =$ \"a\".\n- Case $4$: $K = 2$, $\\alpha = 0.2$, $w_1 =$ \"quantum\", $w_2 =$ \"theory\".\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist of three floats in the order $[\\mathrm{cos}_{\\mathrm{MLM}}, \\mathrm{cos}_{\\mathrm{SG}}, \\Delta]$. For example, an output with two hypothetical cases should look like \"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\". No extra text should be printed.\n\nScientific realism and constraints:\n- Use only the provided corpus, symmetric windowing, and additive smoothing to estimate the conditional distributions.\n- Ensure that the vocabulary is the set of all unique tokens from the corpus and that contexts are tokens from the same set.\n- Use deterministic tokenization and vocabulary ordering so that vectors are comparable across test cases.\n- All reported values must be real numbers with no physical units.", "solution": "The problem is subjected to validation against the specified criteria.\n\n### Step 1: Extract Givens\n- **Corpus**: A fixed set of 11 tokenized sentences.\n  - `the cat sat on the mat`\n  - `the dog sat on the rug`\n  - `a cat chased a mouse`\n  - `a dog chased a ball`\n  - `music and song fill the hall`\n  - `the song played music softly`\n  - `quantum theory explains physics`\n  - `theory of music and physics`\n  - `the quantum cat thought of physics`\n  - `dogs and cats share a home`\n  - `a theory about a song`\n- **Fundamental Definitions**:\n  - Conditional probability: $p(A \\mid B) = \\frac{p(A, B)}{p(B)}$ for $p(B) > 0$.\n  - MLE for multinomial: $\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$.\n  - Additive (Laplace) smoothing: $\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$ for smoothing parameter $\\alpha > 0$ and $V$ outcomes.\n  - Cosine similarity: $\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$.\n- **Tasks**:\n  1. Construct a co-occurrence count matrix $N(w, c)$ using a symmetric context window of size $K$.\n  2. Define and derive the conditional distribution $p_{\\mathrm{MLM}}(w \\mid c)$ for Masked Language Modeling (predicting target word $w$ from context word $c$) using MLE and additive smoothing.\n  3. Define and derive the conditional distribution $p_{\\mathrm{SG}}(c \\mid w)$ for Skip-gram (predicting context word $c$ from target word $w$) using MLE and additive smoothing.\n  4. Construct embedding vectors $\\mathbf{v}_{\\mathrm{MLM}}(w)$ and $\\mathbf{v}_{\\mathrm{SG}}(w)$ for each word $w$, where vector components are indexed by context words $c$.\n  5. Compute $\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$, $\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$, and the difference $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$.\n  6. Use specified parameters $K$ and $\\alpha$ for each test case.\n- **Test Suite**:\n  - Case 1: $(K=2, \\alpha=0.5, w_1=\\text{\"cat\"}, w_2=\\text{\"dog\"})$\n  - Case 2: $(K=1, \\alpha=1.0, w_1=\\text{\"music\"}, w_2=\\text{\"song\"})$\n  - Case 3: $(K=3, \\alpha=10^{-6}, w_1=\\text{\"the\"}, w_2=\\text{\"a\"})$\n  - Case 4: $(K=2, \\alpha=0.2, w_1=\\text{\"quantum\"}, w_2=\\text{\"theory\"})$\n- **Output Format**: A single-line string `[[cos_mlm1,cos_sg1,delta1],[cos_mlm2,cos_sg2,delta2],...]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the principles of computational linguistics and machine learning. The distributional hypothesis, Masked Language Modeling (MLM), Skip-gram (SG), Maximum Likelihood Estimation (MLE), additive smoothing, and cosine similarity are all standard, well-established concepts. The task operationalizes these abstract concepts using concrete, principled count-based methods, which is a valid and insightful exercise.\n- **Well-Posed**: The problem provides a specific corpus, explicit definitions, and a clear algorithmic procedure. All parameters ($K, \\alpha$) and target words ($w_1, w_2$) are specified for each test case. The required outputs are uniquely determined by the inputs and the prescribed methodology.\n- **Objective**: The problem is stated using precise, unambiguous mathematical and procedural language. There are no subjective or opinion-based elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and objective. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution operationalizes the distributional hypothesis by constructing word embeddings from co-occurrence statistics and comparing their geometric properties under two different probabilistic models, Masked Language Modeling (MLM) and Skip-gram (SG).\n\n**1. Vocabulary and Co-occurrence Matrix Construction**\n\nFirst, we establish a fixed vocabulary $\\mathcal{V}$ by collecting all unique tokens from the provided corpus and sorting them alphabetically to ensure a deterministic ordering. Let $V = |\\mathcal{V}|$ be the size of the vocabulary. We create a word-to-index mapping for computational convenience.\n\nFor a given symmetric window size $K$, we populate a $V \\times V$ co-occurrence matrix $N$. An entry $N_{ij}$ stores the count $N(w_i, c_j)$, representing the number of times word $c_j$ appears in the context of target word $w_i$. We iterate through each word $w_i$ at position $p$ in each sentence and increment the counts for all context words $c_j$ found at positions $q$ such that $1 \\le |p - q| \\le K$.\n\n**2. Skip-gram (SG) Model and Embeddings**\n\nThe Skip-gram model aims to predict context words $c$ given a target word $w$. This corresponds to estimating the conditional probability $p_{\\mathrm{SG}}(c \\mid w)$.\nFollowing the principle of Maximum Likelihood Estimation (MLE) on our counts, the probability of observing a specific context word $c_j$ given the target word $w_i$ is estimated by the ratio of their co-occurrence count to the total count of all context words for $w_i$.\nThe total number of context words observed for a target word $w_i$ is the sum of the $i$-th row of the matrix $N$: $S_i^{\\text{row}} = \\sum_{k=1}^V N_{ik}$.\nApplying additive smoothing with parameter $\\alpha$ to handle data sparsity and avoid zero probabilities, the conditional probability is:\n$$p_{\\mathrm{SG}}(c_j \\mid w_i) = \\frac{N_{ij} + \\alpha}{S_i^{\\text{row}} + \\alpha V}$$\nThe Skip-gram embedding for a word $w_i$, denoted $\\mathbf{v}_{\\mathrm{SG}}(w_i)$, is a $V$-dimensional vector whose components are these probabilities, indexed by the context words $c_j \\in \\mathcal{V}$:\n$$\\mathbf{v}_{\\mathrm{SG}}(w_i) = [p_{\\mathrm{SG}}(c_1 \\mid w_i), p_{\\mathrm{SG}}(c_2 \\mid w_i), \\ldots, p_{\\mathrm{SG}}(c_V \\mid w_i)]^\\top$$\nThis vector corresponds to the $i$-th row of the smoothed conditional probability matrix $P_{\\mathrm{SG}}$, where $(P_{\\mathrm{SG}})_{ij} = p_{\\mathrm{SG}}(c_j \\mid w_i)$.\n\n**3. Masked Language Modeling (MLM) Model and Embeddings**\n\nThe Masked Language Modeling objective, in this context, is to predict the target word $w$ given a context word $c$. This requires estimating the conditional probability $p_{\\mathrm{MLM}}(w \\mid c)$.\nAnalogously to the SG model, we use the co-occurrence counts. For a given context word $c_j$, the count of co-occurring target words $w_i$ is $N_{ij}$. The total number of times $c_j$ appears as a context word for any target is the sum of the $j$-th column of the matrix $N$: $S_j^{\\text{col}} = \\sum_{k=1}^V N_{kj}$.\nApplying additive smoothing, the conditional probability is:\n$$p_{\\mathrm{MLM}}(w_i \\mid c_j) = \\frac{N_{ij} + \\alpha}{S_j^{\\text{col}} + \\alpha V}$$\nThe MLM embedding for a word $w_i$, denoted $\\mathbf{v}_{\\mathrm{MLM}}(w_i)$, is a $V$-dimensional vector. As per the problem description, its components are indexed by the context word $c_j$. Therefore, the $j$-th component of this vector is $p_{\\mathrm{MLM}}(w_i \\mid c_j)$.\n$$\\mathbf{v}_{\\mathrm{MLM}}(w_i) = [p_{\\mathrm{MLM}}(w_i \\mid c_1), p_{\\mathrm{MLM}}(w_i \\mid c_2), \\ldots, p_{\\mathrm{MLM}}(w_i \\mid c_V)]^\\top$$\nThis vector corresponds to the $i$-th row of a matrix $P_{\\mathrm{MLM}}$, where $(P_{\\mathrm{MLM}})_{ij} = p_{\\mathrm{MLM}}(w_i \\mid c_j)$.\n\n**4. Geometric Comparison via Cosine Similarity**\n\nTo compare the geometries induced by the two models, we compute the cosine similarity between the embedding vectors of two words, $w_1$ and $w_2$. The cosine similarity provides a measure of the angle between two vectors, and thus their orientation-based similarity, which is a common way to quantify semantic similarity in vector spaces.\n\nFor a given pair of words $(w_1, w_2)$, we compute:\n- The MLM similarity: $\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{MLM}}(w_1)^\\top \\mathbf{v}_{\\mathrm{MLM}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_2) \\rVert_2}$\n- The SG similarity: $\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{SG}}(w_1)^\\top \\mathbf{v}_{\\mathrm{SG}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{SG}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{SG}}(w_2) \\rVert_2}$\n\nFinally, we calculate the difference $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$ to directly compare the similarity measures produced by the two models for each test case. This entire process is repeated for each set of parameters $(K, \\alpha, w_1, w_2)$ in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the word embedding comparison problem by following these steps:\n    1. Processes a fixed corpus to build a vocabulary.\n    2. For each test case:\n        a. Constructs a word-context co-occurrence matrix `N` for a given window size `K`.\n        b. Derives conditional probability matrices `P_MLM` and `P_SG` using additive smoothing.\n        c. Extracts word embedding vectors for the specified words from these matrices.\n        d. Computes the cosine similarity between the word vectors in both embedding spaces (MLM and SG).\n        e. Calculates the difference between the two similarities.\n    3. Formats and prints the final results as specified.\n    \"\"\"\n    corpus = [\n        \"the cat sat on the mat\",\n        \"the dog sat on the rug\",\n        \"a cat chased a mouse\",\n        \"a dog chased a ball\",\n        \"music and song fill the hall\",\n        \"the song played music softly\",\n        \"quantum theory explains physics\",\n        \"theory of music and physics\",\n        \"the quantum cat thought of physics\",\n        \"dogs and cats share a home\",\n        \"a theory about a song\"\n    ]\n\n    test_cases = [\n        (2, 0.5, \"cat\", \"dog\"),\n        (1, 1.0, \"music\", \"song\"),\n        (3, 1e-6, \"the\", \"a\"),\n        (2, 0.2, \"quantum\", \"theory\"),\n    ]\n\n    # Pre-processing: tokenize corpus and build a deterministic vocabulary\n    corpus_tokens = [line.split() for line in corpus]\n    \n    all_tokens = set()\n    for sentence in corpus_tokens:\n        all_tokens.update(sentence)\n    \n    vocabulary = sorted(list(all_tokens))\n    V = len(vocabulary)\n    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n\n    final_results = []\n    cooccurrence_matrices_cache = {}\n\n    for K, alpha, w1_str, w2_str in test_cases:\n        \n        # Task 1: Construct co-occurrence count matrix N(w, c)\n        # We cache the matrix as it can be expensive to recompute.\n        if K not in cooccurrence_matrices_cache:\n            N = np.zeros((V, V), dtype=np.float64)\n            for sentence in corpus_tokens:\n                for i, target_word in enumerate(sentence):\n                    target_idx = word_to_idx[target_word]\n                    \n                    start = max(0, i - K)\n                    end = min(len(sentence), i + K + 1)\n                    \n                    for j in range(start, end):\n                        if i == j:\n                            continue\n                        context_word = sentence[j]\n                        context_idx = word_to_idx[context_word]\n                        N[target_idx, context_idx] += 1\n            cooccurrence_matrices_cache[K] = N\n        \n        N = cooccurrence_matrices_cache[K]\n            \n        w1_idx = word_to_idx[w1_str]\n        w2_idx = word_to_idx[w2_str]\n\n        # Task 2  4: Derive P_MLM(w|c) and MLM embeddings\n        # P_MLM(w_i | c_j) = (N[i, j] + alpha) / (sum_k N[k, j] + alpha * V)\n        col_sums = N.sum(axis=0)\n        P_mlm_denom = col_sums + alpha * V\n        P_mlm = (N + alpha) / P_mlm_denom[np.newaxis, :]\n        \n        v_mlm_w1 = P_mlm[w1_idx, :]\n        v_mlm_w2 = P_mlm[w2_idx, :]\n        \n        # Task 3  4: Derive P_SG(c|w) and SG embeddings\n        # P_SG(c_j | w_i) = (N[i, j] + alpha) / (sum_k N[i, k] + alpha * V)\n        row_sums = N.sum(axis=1)\n        # Use [:, np.newaxis] to ensure correct broadcasting for row-wise division\n        P_sg_denom = row_sums + alpha * V\n        P_sg = (N + alpha) / P_sg_denom[:, np.newaxis]\n        \n        v_sg_w1 = P_sg[w1_idx, :]\n        v_sg_w2 = P_sg[w2_idx, :]\n\n        # Task 5: Compute cosine similarities\n        def cosine_similarity(u, v):\n            dot_product = np.dot(u, v)\n            norm_u = np.linalg.norm(u)\n            norm_v = np.linalg.norm(v)\n            # Denominator is guaranteed non-zero due to alpha > 0 smoothing\n            return dot_product / (norm_u * norm_v)\n\n        cos_mlm = cosine_similarity(v_mlm_w1, v_mlm_w2)\n        cos_sg = cosine_similarity(v_sg_w1, v_sg_w2)\n        \n        # Task 5: Compute difference\n        delta = cos_sg - cos_mlm\n        \n        final_results.append([cos_mlm, cos_sg, delta])\n        \n    # Final print statement in the exact required format.\n    # e.g., \"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\"\n    output_str = \"[\" + \",\".join([f\"[{c1:.6f},{c2:.6f},{d:.6f}]\" for c1, c2, d in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3182958"}, {"introduction": "Preprocessing decisions can significantly alter the \"company a word keeps.\" This practice focuses on lemmatization—grouping different word forms (e.g., \"run\", \"runs\", \"running\") into a single lemma—and asks you to quantify its effect using information-theoretic tools. By measuring the divergence between the context distributions of individual surface forms and their shared lemma, you will develop a rigorous method for analyzing how such preprocessing choices can either clarify or conflate word meanings [@problem_id:3182931].", "problem": "Consider the Distributional Hypothesis in Natural Language Processing, which states that words occurring in similar contexts tend to have similar meanings. In deep learning models that learn word representations, preprocessing decisions such as lemmatization (mapping surface forms to a common base form) can alter the empirical context distributions and thus affect learned embeddings. Formalize and quantify this effect by comparing context distributions conditioned on a lemma versus distributions conditioned on each surface form. Your task is to implement a program that, for each test case, computes two information-theoretic quantities derived from first principles.\n\nDefinitions and setup: Let a finite set of discrete contexts be denoted by $\\mathcal{C} = \\{c_1, c_2, \\ldots, c_K\\}$ and let a lemma $\\ell$ have surface forms $s_1, s_2, \\ldots, s_M$. For each surface form $s_m$, you are given nonnegative integer context counts $\\{n(c_k, s_m)\\}_{k=1}^K$. You must estimate the conditional context distribution $p(c \\mid s_m)$ from these counts using a scientifically justified estimator that avoids zero-probability issues for unseen contexts. Then, estimate the lemma-level context distribution $p(c \\mid \\ell)$ by aggregating counts across surface forms and applying the same estimation principle. Using these estimated distributions, quantify:\n- The average pairwise divergence among the surface-form distributions $p(c \\mid s_m)$, interpreted as how distinct the surface forms’ contextual usage is.\n- The average divergence between each surface-form distribution $p(c \\mid s_m)$ and the lemma-level distribution $p(c \\mid \\ell)$, interpreted as how much lemmatization conflates or separates the distinct contextual usages of the surface forms.\n\nEstimation requirement: Use add-$\\alpha$ smoothing (a special case of a symmetric Dirichlet prior) with smoothing parameter $\\alpha = 0.5$ to estimate the conditional distributions from counts. This means each context probability must be computed from the counts with additive smoothing and normalized over $\\mathcal{C}$, ensuring strictly positive probabilities for all contexts.\n\nDivergence requirement: Use Kullback-Leibler (KL) divergence and Jensen-Shannon (JS) divergence from information theory to define the quantities. For any two discrete distributions $P$ and $Q$ on the same support, KL divergence measures the expected log-likelihood ratio under $P$, and Jensen-Shannon divergence symmetrizes and smooths KL divergence by mixing distributions before comparison. All logarithms must be taken as natural logarithms. For the purposes of this task, the pairwise divergence among surface forms must be computed using Jensen-Shannon divergence between $p(c \\mid s_i)$ and $p(c \\mid s_j)$ for all unordered pairs $(i,j)$ with $i  j$, averaged over pairs. The divergence between surface forms and the lemma must be computed using Jensen-Shannon divergence between $p(c \\mid s_m)$ and $p(c \\mid \\ell)$ for all $m=1,\\ldots,M$, averaged over $m$.\n\nTest suite: Implement your program to process the following three test cases. Each case specifies $\\mathcal{C}$ by name (names are descriptive only; your computation is purely numeric) and provides the surface-form counts. You must treat these names as labels for indices $1,\\ldots,K$ and only use the counts.\n\n- Test Case $1$ (context set size $K = 4$): lemma $\\ell = \\text{\"run\"}$ with surface forms $s_1 = \\text{\"run\"}$, $s_2 = \\text{\"runs\"}$, $s_3 = \\text{\"running\"}$. Contexts are $\\mathcal{C} = [\\text{\"sports\"}, \\text{\"execute\"}, \\text{\"finance\"}, \\text{\"river\"}]$ with counts:\n  - $n(c, s_1) = [40, 10, 0, 0]$\n  - $n(c, s_2) = [20, 5, 0, 0]$\n  - $n(c, s_3) = [35, 8, 0, 0]$\n\n- Test Case $2$ (context set size $K = 4$): lemma $\\ell = \\text{\"bank\"}$ with surface forms $s_1 = \\text{\"bank\"}$, $s_2 = \\text{\"banks\"}$, $s_3 = \\text{\"banking\"}$. Contexts are $\\mathcal{C} = [\\text{\"finance\"}, \\text{\"river\"}, \\text{\"law\"}, \\text{\"sports\"}]$ with counts:\n  - $n(c, s_1) = [45, 5, 10, 0]$\n  - $n(c, s_2) = [10, 40, 0, 0]$\n  - $n(c, s_3) = [30, 0, 5, 0]$\n\n- Test Case $3$ (context set size $K = 4$): lemma $\\ell = \\text{\"bear\"}$ with surface forms $s_1 = \\text{\"bear\"}$, $s_2 = \\text{\"bears\"}$, $s_3 = \\text{\"bearing\"}$. Contexts are $\\mathcal{C} = [\\text{\"animal\"}, \\text{\"finance\"}, \\text{\"support\"}, \\text{\"weather\"}]$ with counts:\n  - $n(c, s_1) = [2, 1, 0, 0]$\n  - $n(c, s_2) = [0, 0, 1, 0]$\n  - $n(c, s_3) = [0, 0, 0, 1]$\n\nComputational details:\n- Estimate $p(c \\mid s_m)$ for each surface form with add-$\\alpha$ smoothing using $\\alpha = 0.5$ across the $K$ contexts.\n- Estimate $p(c \\mid \\ell)$ by aggregating raw counts across all surface forms and applying the same add-$\\alpha$ smoothing with $\\alpha = 0.5$.\n- Compute the average pairwise Jensen-Shannon divergence among the surface-form distributions, and compute the average Jensen-Shannon divergence between each surface form and the lemma distribution. Use natural logarithms for all divergence computations.\n\nFinal output format: Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of two floating-point numbers in the order $[\\text{mean\\_JS\\_among\\_forms}, \\text{mean\\_JS\\_form\\_vs\\_lemma}]$, with each number rounded to $6$ decimal places. For example, the final printed string must look like $[[x_1,y_1],[x_2,y_2],[x_3,y_3]]$ where each $x_i$ and $y_i$ are floats rounded to $6$ decimal places. No additional text should be printed.", "solution": "The problem requires the quantification of the contextual difference between a lemma's various surface forms and the aggregated lemma itself, based on the principles of the Distributional Hypothesis. This is accomplished by computing two information-theoretic quantities for three given test cases. The entire process rests upon a rigorous mathematical and statistical framework, which we will now detail.\n\nFirst, we must estimate the conditional context probability distributions from raw counts. Let $\\mathcal{C} = \\{c_1, \\ldots, c_K\\}$ be the set of $K$ contexts and $\\{s_1, \\ldots, s_M\\}$ be the set of $M$ surface forms for a lemma $\\ell$. We are given counts $n(c_k, s_m)$ for each context $c_k$ and surface form $s_m$. A direct maximum likelihood estimate $p(c_k \\mid s_m) = n(c_k, s_m) / \\sum_j n(c_j, s_m)$ is prone to issues with zero counts, which would incorrectly assign zero probability to unobserved events and lead to mathematical problems (e.g., logarithms of zero) in subsequent calculations.\n\nTo address this, the problem mandates the use of add-$\\alpha$ smoothing (also known as Laplace smoothing or a symmetric Dirichlet prior in a Bayesian framework). With a smoothing parameter $\\alpha = 0.5$, the estimated probability of context $c_k$ given surface form $s_m$ is:\n$$\np(c_k \\mid s_m) = \\frac{n(c_k, s_m) + \\alpha}{\\sum_{j=1}^K n(c_j, s_m) + K\\alpha}\n$$\nThis estimator ensures that every context is assigned a non-zero probability.\n\nNext, we estimate the context distribution for the lemma $\\ell$. This is achieved by first aggregating the counts for each context across all surface forms:\n$$\nn(c_k, \\ell) = \\sum_{m=1}^M n(c_k, s_m)\n$$\nThen, we apply the same add-$\\alpha$ smoothing rule to these aggregated counts to obtain the lemma's context distribution:\n$$\np(c_k \\mid \\ell) = \\frac{n(c_k, \\ell) + \\alpha}{\\sum_{j=1}^K n(c_j, \\ell) + K\\alpha}\n$$\n\nWith these probability distributions established, we can quantify the \"distance\" or \"divergence\" between them using tools from information theory. The problem specifies the use of Jensen-Shannon (JS) divergence. The JS divergence is a symmetrized and smoothed version of the more fundamental Kullback-Leibler (KL) divergence.\n\nFor two discrete probability distributions $P = \\{p_k\\}_{k=1}^K$ and $Q = \\{q_k\\}_{k=1}^K$ defined on the same support, the KL divergence from $Q$ to $P$ is given by:\n$$\nD_{KL}(P \\| Q) = \\sum_{k=1}^K p_k \\ln\\left(\\frac{p_k}{q_k}\\right)\n$$\nThe KL divergence is asymmetric, i.e., $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$ in general. The Jensen-Shannon divergence resolves this by comparing both distributions to their average. Let $M = \\frac{1}{2}(P+Q)$ be the mixture distribution. The JS divergence is defined as:\n$$\nD_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)\n$$\n$D_{JS}(P \\| Q)$ is symmetric, non-negative, and bounded (specifically, $0 \\le D_{JS}(P \\| Q) \\le \\ln 2$).\n\nWe must compute two specific quantities for each test case:\n1.  **The average pairwise divergence among surface forms**: This metric, $\\text{mean\\_JS\\_among\\_forms}$, quantifies the degree of contextual variation among the different surface forms of the lemma. A high value suggests that the surface forms are used in distinct contexts, and lemmatizing them might conflate different meanings. It is calculated by averaging the JS divergence over all unique pairs of surface forms:\n    $$\n    \\text{mean\\_JS\\_among\\_forms} = \\frac{1}{\\binom{M}{2}} \\sum_{1 \\le i  j \\le M} D_{JS}\\big(p(c \\mid s_i) \\,\\|\\, p(c \\mid s_j)\\big)\n    $$\n    For $M=3$, this is an average over $\\binom{3}{2}=3$ pairs.\n\n2.  **The average divergence between each surface form and the lemma**: This metric, $\\text{mean\\_JS\\_form\\_vs\\_lemma}$, measures how well the aggregated lemma distribution represents the context of each individual surface form. A high value indicates that the lemma's \"average\" context is a poor representative for one or more of its constituent forms. It is calculated by averaging the JS divergence between each surface form's distribution and the lemma's distribution:\n    $$\n    \\text{mean\\_JS\\_form\\_vs\\_lemma} = \\frac{1}{M} \\sum_{m=1}^M D_{JS}\\big(p(c \\mid s_m) \\,\\|\\, p(c \\mid \\ell)\\big)\n    $$\n    For $M=3$, this is an average over $3$ pairs.\n\nThe algorithm for each test case proceeds as follows:\na. For each of the $M$ surface forms, take the $K$-dimensional vector of counts and compute the smoothed probability distribution $p(c \\mid s_m)$ using $\\alpha=0.5$.\nb. Sum the count vectors across all $M$ surface forms to get the aggregated count vector for the lemma, $n(c, \\ell)$.\nc. Compute the smoothed probability distribution for the lemma, $p(c \\mid \\ell)$, from its aggregated counts.\nd. Calculate the pairwise JS divergences, $D_{JS}(p(c \\mid s_i) \\| p(c \\mid s_j))$, for all $1 \\le i  j \\le M$. Average these values to find the first required quantity.\ne. Calculate the JS divergences between each surface form and the lemma, $D_{JS}(p(c \\mid s_m) \\| p(c \\mid \\ell))$, for all $m=1, \\ldots, M$. Average these values to find the second required quantity.\nf. Report the two averaged quantities, rounded to six decimal places.\n\nThis procedure is applied to each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes information-theoretic quantities to measure the effect of lemmatization\n    on context distributions for a given set of test cases.\n    \"\"\"\n\n    def calculate_distribution(counts: np.ndarray, alpha: float) -> np.ndarray:\n        \"\"\"\n        Estimates a probability distribution from counts using add-alpha smoothing.\n\n        Args:\n            counts: A NumPy array of non-negative integer counts.\n            alpha: The smoothing parameter.\n\n        Returns:\n            A NumPy array representing the smoothed probability distribution.\n        \"\"\"\n        if not isinstance(counts, np.ndarray):\n            counts = np.array(counts, dtype=float)\n        \n        k = len(counts)\n        total_counts = np.sum(counts)\n        numerator = counts + alpha\n        denominator = total_counts + k * alpha\n        return numerator / denominator\n\n    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Jensen-Shannon divergence between two probability distributions.\n        All logarithms are natural logarithms.\n\n        Args:\n            p: A NumPy array for the first probability distribution.\n            q: A NumPy array for the second probability distribution.\n\n        Returns:\n            The Jensen-Shannon divergence as a float.\n        \"\"\"\n        m = 0.5 * (p + q)\n        # scipy.stats.entropy(pk, qk) calculates KL divergence D_KL(pk || qk)\n        kl_p_m = entropy(p, m)\n        kl_q_m = entropy(q, m)\n        return 0.5 * (kl_p_m + kl_q_m)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: lemma \"run\"\n        {\n            \"surface_form_counts\": [\n                [40, 10, 0, 0],\n                [20, 5, 0, 0],\n                [35, 8, 0, 0],\n            ]\n        },\n        # Test Case 2: lemma \"bank\"\n        {\n            \"surface_form_counts\": [\n                [45, 5, 10, 0],\n                [10, 40, 0, 0],\n                [30, 0, 5, 0],\n            ]\n        },\n        # Test Case 3: lemma \"bear\"\n        {\n            \"surface_form_counts\": [\n                [2, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1],\n            ]\n        },\n    ]\n\n    alpha = 0.5\n    all_results = []\n\n    for case in test_cases:\n        counts_per_form = np.array(case[\"surface_form_counts\"], dtype=float)\n        m, k = counts_per_form.shape\n\n        # Estimate distributions for each surface form\n        dists_s_m = [calculate_distribution(counts, alpha) for counts in counts_per_form]\n\n        # Aggregate counts and estimate distribution for the lemma\n        lemma_counts = np.sum(counts_per_form, axis=0)\n        dist_lemma = calculate_distribution(lemma_counts, alpha)\n\n        # 1. Calculate average pairwise JS divergence among surface forms\n        pairwise_js_divs = []\n        if m > 1:\n            for i in range(m):\n                for j in range(i + 1, m):\n                    div = js_divergence(dists_s_m[i], dists_s_m[j])\n                    pairwise_js_divs.append(div)\n        \n        mean_js_among_forms = np.mean(pairwise_js_divs) if pairwise_js_divs else 0.0\n\n        # 2. Calculate average JS divergence between each surface form and the lemma\n        form_lemma_js_divs = []\n        for i in range(m):\n            div = js_divergence(dists_s_m[i], dist_lemma)\n            form_lemma_js_divs.append(div)\n        \n        mean_js_form_vs_lemma = np.mean(form_lemma_js_divs) if form_lemma_js_divs else 0.0\n        \n        all_results.append([mean_js_among_forms, mean_js_form_vs_lemma])\n\n    # Format the final output string as specified: [[x1,y1],[x2,y2],[x3,y3]]\n    result_strings = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3182931"}]}