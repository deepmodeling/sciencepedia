{"hands_on_practices": [{"introduction": "To train the GloVe model, we use gradient-based optimization, which requires computing the derivative of the objective function with respect to the word vectors and biases. This first exercise guides you through the essential task of deriving these gradients from first principles of multivariable calculus. By implementing them, you will gain a deep, practical understanding of how the model learns and see firsthand how the weighting function $f(X_{ij})$ plays a crucial role in ensuring numerical stability during training [@problem_id:3130245].", "problem": "You are given the Global Vectors (GloVe) objective that learns word vectors from a co-occurrence matrix. Let there be a vocabulary of size $V$ and an embedding dimension $d$. For each word index $i \\in \\{1,\\dots,V\\}$ and context index $j \\in \\{1,\\dots,V\\}$, let $w_i \\in \\mathbb{R}^d$ denote the word vector, $\\tilde{w}_j \\in \\mathbb{R}^d$ the context vector, $b_i \\in \\mathbb{R}$ the word bias, $\\tilde{b}_j \\in \\mathbb{R}$ the context bias, and $X_{ij} \\in \\mathbb{R}_{\\ge 0}$ the co-occurrence count. Consider the objective\n$$\nJ = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2,\n$$\nwith the understanding that the summation is effectively restricted to indices with $X_{ij}  0$ since $\\log X_{ij}$ is defined only for strictly positive arguments. The function $f: \\mathbb{R}_{\\ge 0} \\to \\mathbb{R}_{\\ge 0}$ is a nonnegative weighting function.\n\nTask A (derivation). Starting only from fundamental rules of multivariable calculus (linearity of summation, chain rule, and the gradient of an inner product), derive the gradients $\\nabla_{w_i} J$ and $\\nabla_{\\tilde{w}_j} J$ for arbitrary $i$ and $j$. Your derivation must explicitly justify the restriction to indices with $X_{ij}  0$ and the role of the weighting function $f(\\cdot)$ within the gradients.\n\nTask B (implementation). Implement the derived gradients in a program that computes them exactly for given parameter values. Use two weighting functions:\n- The GloVe weighting defined by\n$$\nf_{\\mathrm{std}}(x) = \\begin{cases}\n\\left(\\dfrac{x}{x_{\\max}}\\right)^\\alpha,  \\text{if } x  x_{\\max},\\\\\n1,  \\text{otherwise},\n\\end{cases}\n$$\nwith hyperparameters $x_{\\max} = 100$ and $\\alpha = 0.75$.\n- A uniform weighting that includes only positive co-occurrences,\n$$\nf_{\\mathrm{uni}}(x) = \\begin{cases}\n1,  \\text{if } x  0,\\\\\n0,  \\text{if } x = 0.\n\\end{cases}\n$$\n\nTask C (stability analysis under sparsity). Using the derived gradients and implementations, quantitatively analyze stability under sparse $X_{ij}$ by comparing the maximum absolute gradient component (the $\\ell_\\infty$-norm over all components of all word and context gradients) under $f_{\\mathrm{uni}}$ and under $f_{\\mathrm{std}}$. For each test case below, compute the ratio\n$$\n\\rho = \\frac{\\max\\{|\\nabla_{w} J|_\\infty,\\,|\\nabla_{\\tilde{w}} J|_\\infty\\}\\text{ under } f_{\\mathrm{uni}}}{\\max\\{|\\nabla_{w} J|_\\infty,\\,|\\nabla_{\\tilde{w}} J|_\\infty\\}\\text{ under } f_{\\mathrm{std}}},\n$$\nwhere $|\\nabla_{w} J|_\\infty$ denotes the maximum absolute value over all entries of all $\\nabla_{w_i} J$, and similarly for $|\\nabla_{\\tilde{w}} J|_\\infty$. Also report whether the computed gradients are finite (contain no NaN or infinite values) under each weighting.\n\nTest suite. Use $V = 4$ and $d = 3$. Initialize all word and context parameters deterministically by a fixed seed $42$ so that results are reproducible:\n- Initialize all entries of $w_i$ and $\\tilde{w}_j$ independently from a zero-mean normal distribution with standard deviation $0.5$ using the fixed seed $42$.\n- Initialize all biases $b_i$ and $\\tilde{b}_j$ to $0$.\n\nUse the following three co-occurrence matrices, each of size $4 \\times 4$:\n- Case A (moderately sparse, small counts):\n$$\nX^{(A)} =\n\\begin{bmatrix}\n0  3  1  0\\\\\n2  0  0  5\\\\\n0  1  0  0\\\\\n4  0  0  0\n\\end{bmatrix}.\n$$\n- Case B (extremely sparse, single co-occurrence):\n$$\nX^{(B)} =\n\\begin{bmatrix}\n0  1  0  0\\\\\n0  0  0  0\\\\\n0  0  0  0\\\\\n0  0  0  0\n\\end{bmatrix}.\n$$\n- Case C (large counts only, above $x_{\\max}$):\n$$\nX^{(C)} =\n\\begin{bmatrix}\n1000  0  0  0\\\\\n0  500  0  0\\\\\n0  0  200  0\\\\\n0  0  0  800\n\\end{bmatrix}.\n$$\n\nNumerical output specification. For each case, compute a list consisting of:\n- the ratio $\\rho$ defined above, rounded to $6$ decimal places,\n- a boolean indicating whether the gradients under $f_{\\mathrm{std}}$ are all finite,\n- a boolean indicating whether the gradients under $f_{\\mathrm{uni}}$ are all finite.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these three-element lists, enclosed in square brackets, for the three cases in the order A, B, C. For example, the printed output must have the form\n$[\\,[\\rho_A,\\mathrm{finite}^{(A)}_{\\mathrm{std}},\\mathrm{finite}^{(A)}_{\\mathrm{uni}}],[\\rho_B,\\mathrm{finite}^{(B)}_{\\mathrm{std}},\\mathrm{finite}^{(B)}_{\\mathrm{uni}}],[\\rho_C,\\mathrm{finite}^{(C)}_{\\mathrm{std}},\\mathrm{finite}^{(C)}_{\\mathrm{uni}}]\\,]$,\nwhere each $\\rho$ is a decimal rounded to $6$ places and booleans are unquoted literal booleans. No physical units or angles are involved in this problem. All real numbers must be standard decimal numerals in the output. The program must be self-contained and require no input.", "solution": "The problem requires the derivation of gradients for the GloVe objective function, the implementation of these gradients for two different weighting schemes, and a quantitative analysis of their stability under different data sparsity conditions.\n\n### Task A (Derivation)\nThe objective function is given by:\n$$\nJ = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2\n$$\nwhere the summation is understood to be restricted to pairs $(i, j)$ for which the co-occurrence count $X_{ij}  0$. This restriction is naturally handled by the weighting function $f(x)$, which is defined such that $f(0)=0$. For any pair $(i, j)$ with $X_{ij}=0$, the corresponding term in the sum is $f(0) \\times (\\dots)^2 = 0 \\times (\\dots)^2 = 0$, thus gracefully obviating the issue of computing $\\log(0)$.\n\n**1. Gradient with respect to the word vector $w_k$**\n\nWe seek to compute the gradient $\\nabla_{w_k} J$ for an arbitrary word index $k \\in \\{1, \\dots, V\\}$. We begin by applying the linearity of the gradient operator:\n$$\n\\nabla_{w_k} J = \\nabla_{w_k} \\left( \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2 \\right) = \\sum_{i=1}^V \\sum_{j=1}^V \\nabla_{w_k} \\left( f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2 \\right)\n$$\nThe gradient $\\nabla_{w_k}$ acts on the parameters of the model. The term inside the summation depends on $w_k$ only when the summation index $i$ is equal to $k$. For $i \\ne k$, the vector $w_i$ is independent of $w_k$, so the derivative is the zero vector. Therefore, we can simplify the expression by retaining only the terms where $i=k$:\n$$\n\\nabla_{w_k} J = \\sum_{j=1}^V \\nabla_{w_k} \\left( f(X_{kj}) \\,\\big(w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj}\\big)^2 \\right)\n$$\nFor each term in this sum, the weighting factor $f(X_{kj})$ is a scalar constant with respect to $w_k$. We can move it outside the gradient operator. Let's define the scalar error term for a pair $(k, j)$ as:\n$$\nS_{kj} = w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj}\n$$\nThe expression becomes:\n$$\n\\nabla_{w_k} J = \\sum_{j=1}^V f(X_{kj}) \\nabla_{w_k} (S_{kj}^2)\n$$\nUsing the chain rule for vector derivatives, $\\nabla_v g(h(v)) = g'(h(v)) \\nabla_v h(v)$, with $g(h) = h^2$ and $h(w_k) = S_{kj}$, we get:\n$$\n\\nabla_{w_k} (S_{kj}^2) = 2 S_{kj} \\nabla_{w_k} S_{kj}\n$$\nNow, we must compute the gradient of $S_{kj}$ with respect to $w_k$:\n$$\n\\nabla_{w_k} S_{kj} = \\nabla_{w_k} (w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj})\n$$\nThe terms $b_k$, $\\tilde{b}_j$, and $\\log X_{kj}$ are scalars that do not depend on the vector $w_k$, so their gradients are zero. We are left with the gradient of the inner product:\n$$\n\\nabla_{w_k} (w_k^\\top \\tilde{w}_j)\n$$\nUsing the fundamental identity for the gradient of an inner product, $\\nabla_v (v^\\top a) = a$, where $v = w_k$ and $a = \\tilde{w}_j$, we find:\n$$\n\\nabla_{w_k} (w_k^\\top \\tilde{w}_j) = \\tilde{w}_j\n$$\nSubstituting this back, we get $\\nabla_{w_k} S_{kj} = \\tilde{w}_j$. Assembling all the pieces, the gradient with respect to $w_k$ is:\n$$\n\\nabla_{w_k} J = \\sum_{j=1}^V f(X_{kj}) \\cdot 2 \\left( w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj} \\right) \\tilde{w}_j\n$$\nAs established earlier, since $f(X_{kj}) = 0$ if $X_{kj}=0$, this sum is effectively taken over all $j$ for which $X_{kj}  0$.\n\n**2. Gradient with respect to the context vector $\\tilde{w}_k$**\n\nThe derivation for $\\nabla_{\\tilde{w}_k} J$ follows a symmetric argument. The term in the original double summation depends on $\\tilde{w}_k$ only when the index $j$ is equal to $k$.\n$$\n\\nabla_{\\tilde{w}_k} J = \\sum_{i=1}^V \\nabla_{\\tilde{w}_k} \\left( f(X_{ik}) \\,\\big(w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik}\\big)^2 \\right)\n$$\nLet $S_{ik} = w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik}$. Applying the chain rule as before:\n$$\n\\nabla_{\\tilde{w}_k} J = \\sum_{i=1}^V f(X_{ik}) \\cdot 2 S_{ik} \\cdot \\nabla_{\\tilde{w}_k} S_{ik}\n$$\nThe gradient of the scalar error term $S_{ik}$ with respect to $\\tilde{w}_k$ is:\n$$\n\\nabla_{\\tilde{w}_k} S_{ik} = \\nabla_{\\tilde{w}_k} (w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik}) = \\nabla_{\\tilde{w}_k} (w_i^\\top \\tilde{w}_k)\n$$\nUsing the identity $\\nabla_v (a^\\top v) = a$, where $v = \\tilde{w}_k$ and $a = w_i$, we obtain:\n$$\n\\nabla_{\\tilde{w}_k} (w_i^\\top \\tilde{w}_k) = w_i\n$$\nThus, the final expression for the gradient with respect to $\\tilde{w}_k$ is:\n$$\n\\nabla_{\\tilde{w}_k} J = \\sum_{i=1}^V f(X_{ik}) \\cdot 2 \\left( w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik} \\right) w_i\n$$\nAgain, this sum is effectively over all $i$ for which $X_{ik}  0$.\n\n### Task B  C (Algorithmic Design and Stability Analysis)\n\nThe derived gradients form the basis for the implementation. The algorithm computes the gradients for all word and context vectors for a given co-occurrence matrix $X$, model parameters, and a specified weighting function.\n\n**Algorithmic Steps:**\n1.  Initialize parameter matrices $W \\in \\mathbb{R}^{V \\times d}$, $\\tilde{W} \\in \\mathbb{R}^{V \\times d}$ and bias vectors $b \\in \\mathbb{R}^V$, $\\tilde{b} \\in \\mathbb{R}^V$ according to the specified deterministic procedure (zero-mean normal distribution with standard deviation $0.5$ using seed $42$ for vectors, zeros for biases).\n2.  Initialize gradient matrices $\\nabla_W J$ and $\\nabla_{\\tilde{W}} J$ to zeros.\n3.  Iterate through each pair of indices $(i, j)$ where the co-occurrence count $X_{ij}$ is non-zero.\n4.  For each such pair, calculate the scalar error term $S_{ij} = w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}$.\n5.  Calculate the weight $f(X_{ij})$ using one of the two specified functions:\n    - **Standard GloVe weighting ($f_{\\mathrm{std}}$):** $f_{\\mathrm{std}}(x) = \\min(1, (x/x_{\\max})^\\alpha)$. This function down-weights co-occurrences with low counts and caps the weight at $1$ for very frequent co-occurrences (those with counts $\\ge x_{\\max}$).\n    - **Uniform weighting ($f_{\\mathrm{uni}}$):** $f_{\\mathrm{uni}}(x) = 1$ if $x  0$ and $0$ otherwise. This function treats all observed co-occurrences equally, regardless of their frequency.\n6.  Compute the scalar pre-factor $p_{ij} = 2 \\cdot f(X_{ij}) \\cdot S_{ij}$.\n7.  Update the gradients by adding the contribution from the pair $(i, j)$:\n    $$\n    \\nabla_{w_i} J \\leftarrow \\nabla_{w_i} J + p_{ij} \\tilde{w}_j \\\\\n    \\nabla_{\\tilde{w}_j} J \\leftarrow \\nabla_{\\tilde{w}_j} J + p_{ij} w_i\n    $$\n\n**Stability Analysis:**\nThe stability analysis compares the magnitude of the gradients produced by the two weighting functions. The metric is the ratio $\\rho$ of the maximum absolute gradient components ($\\ell_\\infty$-norm over all gradient entries):\n$$\n\\rho = \\frac{\\max\\{|\\nabla_{W} J|_{\\infty}, |\\nabla_{\\tilde{W}} J|_{\\infty}\\}\\text{ under } f_{\\mathrm{uni}}}{\\max\\{|\\nabla_{W} J|_{\\infty}, |\\nabla_{\\tilde{W}} J|_{\\infty}\\}\\text{ under } f_{\\mathrm{std}}}\n$$\n- For co-occurrence pairs $(i, j)$ with low counts (i.e., $X_{ij}  x_{\\max}$), we have $f_{\\mathrm{std}}(X_{ij}) = (X_{ij}/x_{\\max})^\\alpha  1$, while $f_{\\mathrm{uni}}(X_{ij}) = 1$. Consequently, the gradients from $f_{\\mathrm{uni}}$ will be larger than those from $f_{\\mathrm{std}}$ for these pairs. This suggests $\\rho  1$ for sparse matrices with small counts (Cases A, B). The down-weighting property of $f_{\\mathrm{std}}$ is a key feature for model stability, preventing rare but potentially noisy co-occurrences from generating excessively large gradients.\n- For co-occurrence pairs with very high counts ($X_{ij} \\ge x_{\\max}$), $f_{\\mathrm{std}}(X_{ij}) = 1$, which is identical to $f_{\\mathrm{uni}}(X_{ij})$. If all non-zero counts in a matrix are above $x_{\\max}$ (Case C), the two weighting schemes become equivalent, and we expect the gradients to be identical, yielding $\\rho = 1$.\n- The program also checks if all computed gradient components are finite numbers. Given that the calculation explicitly avoids $\\log(0)$ and all input counts are non-negative, non-finite values (NaN/Inf) are not expected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GloVe gradient derivation, implementation, and analysis problem.\n    \"\"\"\n    # ------------------\n    # --- PARAMETERS ---\n    # ------------------\n    V = 4\n    d = 3\n    seed = 42\n    x_max = 100.0\n    alpha = 0.75\n\n    # ------------------\n    # --- INITIALIZATION ---\n    # ------------------\n    # Initialize parameters deterministically as specified.\n    rng = np.random.default_rng(seed)\n    W = rng.normal(loc=0, scale=0.5, size=(V, d))\n    tilde_W = rng.normal(loc=0, scale=0.5, size=(V, d))\n    b = np.zeros(V)\n    tilde_b = np.zeros(V)\n\n    # ------------------\n    # --- TEST CASES ---\n    # ------------------\n    test_cases = [\n        # Case A: Moderately sparse, small counts\n        np.array([\n            [0, 3, 1, 0],\n            [2, 0, 0, 5],\n            [0, 1, 0, 0],\n            [4, 0, 0, 0]\n        ], dtype=float),\n        # Case B: Extremely sparse, single co-occurrence\n        np.array([\n            [0, 1, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        # Case C: Large counts only, above x_max\n        np.array([\n            [1000, 0, 0, 0],\n            [0, 500, 0, 0],\n            [0, 0, 200, 0],\n            [0, 0, 0, 800]\n        ], dtype=float),\n    ]\n\n    # ------------------\n    # --- WEIGHTING FUNCTIONS ---\n    # ------------------\n    def f_std(x):\n        \"\"\"Standard GloVe weighting function.\"\"\"\n        if x  x_max:\n            return (x / x_max)**alpha\n        else:\n            return 1.0\n\n    def f_uni(x):\n        \"\"\"Uniform weighting function for positive co-occurrences.\"\"\"\n        return 1.0 if x  0 else 0.0\n\n    # ------------------\n    # --- GRADIENT COMPUTATION ---\n    # ------------------\n    def compute_gradients(X, f_func, W_in, tilde_W_in, b_in, tilde_b_in):\n        \"\"\"\n        Computes gradients for word and context vectors based on derived formula.\n        \"\"\"\n        V_dim, d_dim = W_in.shape\n        grad_W = np.zeros_like(W_in)\n        grad_tilde_W = np.zeros_like(tilde_W_in)\n        \n        # Iterate only over non-zero entries of the co-occurrence matrix X\n        rows, cols = np.nonzero(X)\n        \n        for i, j in zip(rows, cols):\n            X_ij = X[i, j]\n            \n            # The inner term of the GloVe objective\n            inner_term = (W_in[i] @ tilde_W_in[j] + b_in[i] + tilde_b_in[j] \n                          - np.log(X_ij))\n            \n            # Weight from the weighting function\n            weight = f_func(X_ij)\n            \n            # Scalar pre-factor common to both gradient updates\n            prefactor = 2 * weight * inner_term\n            \n            # Update gradients according to the derived formulas\n            grad_W[i, :] += prefactor * tilde_W_in[j, :]\n            grad_tilde_W[j, :] += prefactor * W_in[i, :]\n            \n        return grad_W, grad_tilde_W\n\n    # ------------------\n    # --- ANALYSIS  OUTPUT ---\n    # ------------------\n    results_list = []\n    \n    for X_case in test_cases:\n        # --- Compute gradients and metrics for f_std ---\n        grad_W_std, grad_tilde_W_std = compute_gradients(X_case, f_std, W, tilde_W, b, tilde_b)\n        \n        finite_std = np.all(np.isfinite(grad_W_std)) and np.all(np.isfinite(grad_tilde_W_std))\n        \n        max_abs_grad_std = 0.0\n        if grad_W_std.size  0:\n            max_abs_grad_std = max(np.max(np.abs(grad_W_std)), np.max(np.abs(grad_tilde_W_std)))\n        \n        # --- Compute gradients and metrics for f_uni ---\n        grad_W_uni, grad_tilde_W_uni = compute_gradients(X_case, f_uni, W, tilde_W, b, tilde_b)\n        \n        finite_uni = np.all(np.isfinite(grad_W_uni)) and np.all(np.isfinite(grad_tilde_W_uni))\n        \n        max_abs_grad_uni = 0.0\n        if grad_W_uni.size  0:\n            max_abs_grad_uni = max(np.max(np.abs(grad_W_uni)), np.max(np.abs(grad_tilde_W_uni)))\n        \n        # --- Compute the ratio rho ---\n        # If std grad is 0, uni grad must also be 0 (since f_std(x)0 = f_uni(x)0).\n        # In this 0/0 case, the gradients are identical, so the ratio is 1.\n        if max_abs_grad_std  1e-9: # Use tolerance for float comparison\n            rho = max_abs_grad_uni / max_abs_grad_std\n        else:\n            rho = 1.0\n            \n        # Format the result for this case\n        case_result = f\"[{round(rho, 6)},{str(finite_std).lower()},{str(finite_uni).lower()}]\"\n        results_list.append(case_result)\n        \n    # --- Final Print ---\n    # Print the final result in the exact required single-line format.\n    print(f\"[{','.join(results_list)}]\")\n\nsolve()\n```", "id": "3130245"}, {"introduction": "A silent bug in your gradient calculation can prevent a model from learning, a notoriously difficult problem to debug. This practice introduces gradient checking, a powerful and universal technique to verify the correctness of your hand-derived gradients by comparing them to a numerical approximation. Completing this exercise [@problem_id:3130312] will not only ensure your GloVe implementation is correct but also equip you with an indispensable tool for developing any gradient-based machine learning model.", "problem": "Given a small co-occurrence matrix $X \\in \\mathbb{R}_{\\ge 0}^{n_w \\times n_c}$ for a vocabulary of $n_w$ target words and $n_c$ context words, consider the weighted least squares objective used in Global Vectors (GloVe), defined as follows. Let $W \\in \\mathbb{R}^{n_w \\times d}$ be target word embeddings, $C \\in \\mathbb{R}^{n_c \\times d}$ be context embeddings, $b \\in \\mathbb{R}^{n_w}$ be target biases, and $d_{\\text{biases}} \\in \\mathbb{R}^{n_c}$ be context biases. Define the weighting function $f(x)$ by $f(x) = \\min\\{(x / x_{\\max})^\\alpha, 1\\}$ for $x > 0$ and $f(0) = 0$, where $\\alpha \\in (0,1]$ and $x_{\\max} > 0$ are hyperparameters. The GloVe objective is\n$$\nJ(W, C, b, d_{\\text{biases}}; X) = \\sum_{i=1}^{n_w} \\sum_{j=1}^{n_c} f(X_{ij}) \\left( W_i^\\top C_j + b_i + d_{\\text{biases},j} - \\log X_{ij} \\right)^2,\n$$\nwith the convention that terms for which $X_{ij} = 0$ contribute zero because $f(0) = 0$. All logarithms are the natural logarithm.\n\nYou are to implement analytic gradients with respect to $W$, $C$, $b$, and $d_{\\text{biases}}$, and then verify their correctness using gradient checking via central finite differences derived from first principles. The fundamental base to use includes: the definition of the gradient as the vector of partial derivatives, the first-order Taylor expansion of a differentiable function, and the central difference approximation for a scalar function $J(\\theta)$ at parameter vector $\\theta$:\n$$\n\\frac{\\partial J}{\\partial \\theta_k} \\approx \\frac{J(\\theta + h e_k) - J(\\theta - h e_k)}{2h},\n$$\nwhere $e_k$ is the $k$-th coordinate basis vector and $h  0$ is a small step size.\n\nRequirements:\n- Implement a function that computes $J(W, C, b, d_{\\text{biases}}; X)$ without producing undefined values when $X_{ij} = 0$. You must avoid evaluating $\\log 0$ by ensuring that any expression involving $\\log X_{ij}$ is not used when $X_{ij} = 0$; equivalently, you may use a masked computation that sets $\\log X_{ij}$ to $0$ wherever $X_{ij} = 0$, relying on $f(0) = 0$ to nullify those terms.\n- Derive and implement the analytic gradients with respect to $W$, $C$, $b$, and $d_{\\text{biases}}$ based on the objective $J$. Use only standard rules of multivariable calculus and linear algebra. Your implementation must be fully vectorized over indices $i$ and $j$.\n- Implement gradient checking using central finite differences with a small step size $h$ to approximate the gradient of $J$ with respect to all parameters $(W, C, b, d_{\\text{biases}})$ packed into a single vector $\\theta$. Compute, for each test case, the maximum absolute discrepancy between the analytic gradient and the numerical gradient over all coordinates of $\\theta$.\n\nTest Suite:\nUse the following four test cases. For all random generations, you must use the specified pseudo-random seed to ensure deterministic outputs. For each test case, initialize parameters $W$, $C$, $b$, and $d_{\\text{biases}}$ with independent samples from a normal distribution with mean $0$ and standard deviation $0.1$.\n\n1. Happy path with mixed counts and zeros:\n   - $n_w = 3$, $n_c = 4$, $d = 2$, seed $0$.\n   - Generate $X$ by drawing independent integers uniformly from $\\{0,1,2,3,4,5,6,7\\}$.\n   - $\\alpha = 0.75$, $x_{\\max} = 10$.\n2. Boundary case with all zeros:\n   - $n_w = 2$, $n_c = 2$, $d = 3$, seed $1$.\n   - $X$ is the all-zero matrix (every entry is $0$).\n   - $\\alpha = 0.75$, $x_{\\max} = 10$.\n3. Corner case with all ones:\n   - $n_w = 5$, $n_c = 3$, $d = 2$, seed $2$.\n   - $X$ is the all-ones matrix (every entry is $1$).\n   - $\\alpha = 0.5$, $x_{\\max} = 1$.\n4. Mixed extremes:\n   - $n_w = 4$, $n_c = 4$, $d = 1$, seed $42$.\n   - Generate $X$ by sampling each entry independently from the set $\\{0, 1, 50\\}$ with probabilities $\\{0.4, 0.5, 0.1\\}$.\n   - $\\alpha = 0.75$, $x_{\\max} = 100$.\n\nNumerical Differentiation:\n- Use central differences with step size $h = 10^{-6}$.\n- Pack all parameters $(W, C, b, d_{\\text{biases}})$ into a single parameter vector $\\theta$ in the following order: row-major flattening of $W$, then row-major flattening of $C$, then $b$, then $d_{\\text{biases}}$.\n\nOutput:\n- For each test case, compute the maximum absolute discrepancy between the analytic gradient and the numerical gradient over all coordinates in $\\theta$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_k$ is the discrepancy for test case $k$, rounded to six decimal places.\n- There are no physical units or angles in this problem. All quantities are dimensionless real numbers.\n\nYour solution must be a complete, runnable program that performs all computations and prints the final result in the exact specified format.", "solution": "The user has provided a problem that requires the derivation and implementation of the gradients for the GloVe objective function, followed by verification using numerical gradient checking.\n\nFirst, a critical analysis of the problem statement is required. The problem uses the symbol $d$ for two distinct mathematical objects: first, as an integer representing the dimensionality of the word embeddings, where $W \\in \\mathbb{R}^{n_w \\times d}$ and $C \\in \\mathbb{R}^{n_c \\times d}$; and second, as a vector of context biases, $d \\in \\mathbb{R}^{n_c}$. This notational overload is a significant flaw that creates ambiguity. A rigorous mathematical text would use distinct symbols. For clarity in this explanation, let us denote the embedding dimension by $d_{dim}$ and the context bias vector by $\\vec{\\beta} \\in \\mathbb{R}^{n_c}$. The problem is otherwise well-defined, scientifically grounded in the principles of machine learning and calculus, and can be solved upon resolving this ambiguity. The parameters of the model are therefore the target word embeddings $W \\in \\mathbb{R}^{n_w \\times d_{dim}}$, the context word embeddings $C \\in \\mathbb{R}^{n_c \\times d_{dim}}$, the target biases $b \\in \\mathbb{R}^{n_w}$, and the context biases $\\vec{\\beta} \\in \\mathbb{R}^{n_c}$.\n\nThe objective function to minimize is:\n$$\nJ(W, C, b, \\vec{\\beta}; X) = \\sum_{i=1}^{n_w} \\sum_{j=1}^{n_c} f(X_{ij}) \\left( W_i^\\top C_j + b_i + \\beta_j - \\log X_{ij} \\right)^2\n$$\nwhere $W_i$ is the $i$-th row of $W$, $C_j$ is the $j$-th row of $C$, $b_i$ is the $i$-th component of $b$, and $\\beta_j$ is the $j$-th component of $\\vec{\\beta}$. The weighting function is $f(x) = \\min\\{(x/x_{\\max})^\\alpha, 1\\}$ for $x > 0$ and $f(0)=0$. The term involving $\\log X_{ij}$ is only computed when $X_{ij} > 0$; otherwise, the entire term for the $(i,j)$ pair is zero due to $f(0)=0$.\n\nTo implement gradient checking, we must first derive the analytic gradients of $J$ with respect to each parameter. We apply the chain rule of multivariable calculus. Let the error term for an entry $(i,j)$ be:\n$$\nE_{ij} = W_i^\\top C_j + b_i + \\beta_j - \\log X_{ij}\n$$\nThe objective function can be written as $J = \\sum_{i,j} f(X_{ij}) E_{ij}^2$. The partial derivative with respect to any parameter $\\theta$ is:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i,j} \\frac{\\partial}{\\partial \\theta} \\left[ f(X_{ij}) E_{ij}^2 \\right] = \\sum_{i,j} f(X_{ij}) \\cdot 2 E_{ij} \\cdot \\frac{\\partial E_{ij}}{\\partial \\theta}\n$$\nThe terms $f(X_{ij})$ are constant with respect to the model parameters. Let's define the intermediate matrix $S \\in \\mathbb{R}^{n_w \\times n_c}$ with entries $S_{ij} = 2 f(X_{ij}) E_{ij}$. The expression for the gradient becomes:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i,j} S_{ij} \\frac{\\partial E_{ij}}{\\partial \\theta}\n$$\n\n1.  **Gradient with respect to target embeddings $W$**:\n    We compute the partial derivative with respect to a single component $W_{ik}$ (row $i$, column $k$) of the matrix $W$.\n    $$\n    \\frac{\\partial E_{mn}}{\\partial W_{ik}} = \\frac{\\partial}{\\partial W_{ik}} \\left( \\sum_{l=1}^{d_{dim}} W_{ml} C_{nl} + b_m + \\beta_n - \\log X_{mn} \\right) = \\delta_{mi} \\delta_{lk} C_{nl} \\implies \\frac{\\partial W_m^\\top C_n}{\\partial W_{ik}} = \\delta_{mi} C_{nk}\n    $$\n    Here, $\\delta$ is the Kronecker delta. The derivative is non-zero only when $m=i$.\n    $$\n    \\frac{\\partial J}{\\partial W_{ik}} = \\sum_{j=1}^{n_c} S_{ij} \\frac{\\partial E_{ij}}{\\partial W_{ik}} = \\sum_{j=1}^{n_c} S_{ij} C_{jk}\n    $$\n    This expression represents the $(i,k)$-th element of a matrix product. The gradient matrix $\\nabla_W J$ is therefore given by:\n    $$\n    \\nabla_W J = [S_{ij}]_{i,j} \\cdot C\n    $$\n    where $[S_{ij}]_{i,j}$ is the matrix with elements $S_{ij}$.\n\n2.  **Gradient with respect to context embeddings $C$**:\n    Similarly, for a component $C_{jk}$ of the matrix $C$:\n    $$\n    \\frac{\\partial E_{mn}}{\\partial C_{jk}} = \\frac{\\partial}{\\partial C_{jk}} \\left( \\sum_{l=1}^{d_{dim}} W_{ml} C_{nl} \\right) = \\delta_{nj} \\delta_{lk} W_{ml} \\implies \\frac{\\partial W_m^\\top C_n}{\\partial C_{jk}} = \\delta_{nj} W_{mk}\n    $$\n    The derivative is non-zero only when $n=j$.\n    $$\n    \\frac{\\partial J}{\\partial C_{jk}} = \\sum_{i=1}^{n_w} S_{ij} \\frac{\\partial E_{ij}}{\\partial C_{jk}} = \\sum_{i=1}^{n_w} S_{ij} W_{ik}\n    $$\n    This is the $(j,k)$-th element of the matrix product $S^\\top W$. The gradient matrix $\\nabla_C J$ is:\n    $$\n    \\nabla_C J = [S_{ij}]_{i,j}^\\top \\cdot W\n    $$\n\n3.  **Gradient with respect to target biases $b$**:\n    For a component $b_k$ of the vector $b$:\n    $$\n    \\frac{\\partial E_{ij}}{\\partial b_k} = \\delta_{ik}\n    $$\n    The derivative is non-zero only when $i=k$.\n    $$\n    \\frac{\\partial J}{\\partial b_k} = \\sum_{i,j} S_{ij} \\delta_{ik} = \\sum_{j=1}^{n_c} S_{kj}\n    $$\n    The gradient vector $\\nabla_b J$ is obtained by summing the rows of the matrix $S$.\n\n4.  **Gradient with respect to context biases $\\vec{\\beta}$**:\n    For a component $\\beta_k$ of the vector $\\vec{\\beta}$:\n    $$\n    \\frac{\\partial E_{ij}}{\\partial \\beta_k} = \\delta_{jk}\n    $$\n    The derivative is non-zero only when $j=k$.\n    $$\n    \\frac{\\partial J}{\\partial \\beta_k} = \\sum_{i,j} S_{ij} \\delta_{jk} = \\sum_{i=1}^{n_w} S_{ik}\n    $$\n    The gradient vector $\\nabla_{\\vec{\\beta}} J$ is obtained by summing the columns of the matrix $S$.\n\nThese analytic gradients must be verified numerically. Gradient checking is based on the first-order Taylor expansion of a function. For a scalar function $J(\\vec{\\theta})$ of a vector parameter $\\vec{\\theta}$, the central difference formula provides a second-order accurate approximation of the partial derivative with respect to its $k$-th component $\\theta_k$:\n$$\n\\frac{\\partial J}{\\partial \\theta_k} \\approx \\frac{J(\\vec{\\theta} + h \\vec{e}_k) - J(\\vec{\\theta} - h \\vec{e}_k)}{2h}\n$$\nwhere $\\vec{e}_k$ is the standard basis vector with a $1$ at the $k$-th position and zeros elsewhere, and $h$ is a small step size (e.g., $h=10^{-6}$).\n\nTo apply this, we first \"unroll\" all model parameters $(W, C, b, \\vec{\\beta})$ into a single, large parameter vector $\\vec{\\theta}$ by concatenating them in the specified order: $W$ (flattened row-major), $C$ (flattened row-major), $b$, and $\\vec{\\beta}$. We then iterate through each component $\\theta_k$ of $\\vec{\\theta}$, compute the numerical derivative using the formula above, and compare it to the corresponding component of the analytic gradient vector (which is also unrolled in the same order). The maximum absolute difference between the analytic and numerical gradients across all components serves as the final measure of correctness.\n\nThe implementation will consist of:\n1.  A function to compute the cost $J$, safely handling cases where $X_{ij}=0$.\n2.  A function to compute the analytic gradients $\\nabla_W J, \\nabla_C J, \\nabla_b J, \\nabla_{\\vec{\\beta}} J$ using vectorized operations.\n3.  A main procedure that, for each test case:\n    a. Initializes parameters.\n    b. Computes the analytic gradient vector, $\\vec{g}_{analytic}$.\n    c. Iteratively computes the numerical gradient vector, $\\vec{g}_{numeric}$, using the central difference method.\n    d. Calculates the discrepancy $\\max|\\vec{g}_{analytic} - \\vec{g}_{numeric}|$.\nThis rigorous process ensures the correctness of the gradient implementation, a foundational step in training machine learning models.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GloVe gradient checking for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"n_w\": 3, \"n_c\": 4, \"dim\": 2, \"seed\": 0, \"alpha\": 0.75, \"x_max\": 10,\n            \"X_gen\": lambda n_w, n_c: np.random.randint(0, 8, size=(n_w, n_c)).astype(float)\n        },\n        {\n            \"n_w\": 2, \"n_c\": 2, \"dim\": 3, \"seed\": 1, \"alpha\": 0.75, \"x_max\": 10,\n            \"X_gen\": lambda n_w, n_c: np.zeros((n_w, n_c), dtype=float)\n        },\n        {\n            \"n_w\": 5, \"n_c\": 3, \"dim\": 2, \"seed\": 2, \"alpha\": 0.5, \"x_max\": 1,\n            \"X_gen\": lambda n_w, n_c: np.ones((n_w, n_c), dtype=float)\n        },\n        {\n            \"n_w\": 4, \"n_c\": 4, \"dim\": 1, \"seed\": 42, \"alpha\": 0.75, \"x_max\": 100,\n            \"X_gen\": lambda n_w, n_c: np.random.choice([0, 1, 50], size=(n_w, n_c), p=[0.4, 0.5, 0.1]).astype(float)\n        },\n    ]\n\n    h = 1e-6\n    results = []\n\n    for case in test_cases:\n        n_w, n_c, dim, seed = case[\"n_w\"], case[\"n_c\"], case[\"dim\"], case[\"seed\"]\n        alpha, x_max = case[\"alpha\"], case[\"x_max\"]\n        \n        np.random.seed(seed)\n        \n        X = case[\"X_gen\"](n_w, n_c)\n        W = np.random.normal(0, 0.1, size=(n_w, dim))\n        C = np.random.normal(0, 0.1, size=(n_c, dim))\n        b = np.random.normal(0, 0.1, size=(n_w,))\n        d_biases = np.random.normal(0, 0.1, size=(n_c,))\n\n        # --- Helper functions for parameter packing/unpacking ---\n        param_sizes = {\n            'W': W.size, 'C': C.size, 'b': b.size, 'd_biases': d_biases.size\n        }\n        \n        def pack_params(W, C, b, d_biases):\n            return np.concatenate([W.flatten(), C.flatten(), b.flatten(), d_biases.flatten()])\n\n        def unpack_params(theta):\n            idx = 0\n            W_flat = theta[idx : idx + param_sizes['W']]\n            W_unpacked = W_flat.reshape(n_w, dim)\n            idx += param_sizes['W']\n            \n            C_flat = theta[idx : idx + param_sizes['C']]\n            C_unpacked = C_flat.reshape(n_c, dim)\n            idx += param_sizes['C']\n            \n            b_unpacked = theta[idx : idx + param_sizes['b']]\n            idx += param_sizes['b']\n\n            d_biases_unpacked = theta[idx:]\n            return W_unpacked, C_unpacked, b_unpacked, d_biases_unpacked\n        \n        # --- Cost and Analytic Gradient Calculation ---\n        def compute_cost_and_grads(W, C, b, d_biases, X, alpha, x_max):\n            # Mask for non-zero co-occurrences\n            non_zero_mask = X  0\n            \n            # Weighting function f(X_ij)\n            weights = np.zeros_like(X, dtype=float)\n            if np.any(non_zero_mask):\n                x_scaled = X[non_zero_mask] / x_max\n                f_vals = np.minimum(1.0, np.power(x_scaled, alpha))\n                weights[non_zero_mask] = f_vals\n\n            # Safely compute log(X_ij)\n            log_X = np.zeros_like(X, dtype=float)\n            if np.any(non_zero_mask):\n                log_X[non_zero_mask] = np.log(X[non_zero_mask])\n\n            # Error term: W_i^T C_j + b_i + d_j - log(X_ij)\n            errors = W @ C.T + b[:, np.newaxis] + d_biases[np.newaxis, :] - log_X\n            \n            # Cost J\n            cost = np.sum(weights * (errors ** 2))\n            \n            # Common term for gradients\n            s_term = weights * errors\n            \n            # Analytic gradients\n            grad_W = 2 * s_term @ C\n            grad_C = 2 * s_term.T @ W\n            grad_b = 2 * np.sum(s_term, axis=1)\n            grad_d_biases = 2 * np.sum(s_term, axis=0)\n            \n            return cost, grad_W, grad_C, grad_b, grad_d_biases\n        \n        # --- Numerical Gradient Calculation (Gradient Checking) ---\n        theta = pack_params(W, C, b, d_biases)\n        num_grads = np.zeros_like(theta)\n        \n        for i in range(len(theta)):\n            # J(theta + h*e_i)\n            theta_plus = theta.copy()\n            theta_plus[i] += h\n            W_p, C_p, b_p, d_p = unpack_params(theta_plus)\n            cost_plus, _, _, _, _ = compute_cost_and_grads(W_p, C_p, b_p, d_p, X, alpha, x_max)\n            \n            # J(theta - h*e_i)\n            theta_minus = theta.copy()\n            theta_minus[i] -= h\n            W_m, C_m, b_m, d_m = unpack_params(theta_minus)\n            cost_minus, _, _, _, _ = compute_cost_and_grads(W_m, C_m, b_m, d_m, X, alpha, x_max)\n            \n            # Central difference\n            num_grads[i] = (cost_plus - cost_minus) / (2 * h)\n\n        # --- Comparison ---\n        _, grad_W, grad_C, grad_b, grad_d_biases = compute_cost_and_grads(W, C, b, d_biases, X, alpha, x_max)\n        analytic_grads = pack_params(grad_W, grad_C, grad_b, grad_d_biases)\n        \n        discrepancy = np.max(np.abs(analytic_grads - num_grads))\n        results.append(discrepancy)\n\n    # Final formatted output\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3130312"}, {"introduction": "A trained model is more than just a set of final parameters; it's a new lens through which to view your data. This practice moves from building the model to interpreting its behavior by developing a diagnostic tool to identify word pairs that are poorly explained by the trained embeddings. By calculating and analyzing standardized residuals, you will learn to pinpoint specific semantic relationships the model struggles with, providing crucial insights into its limitations and the structure of the underlying data [@problem_id:3130256].", "problem": "You are given a finite vocabulary of size $n$ and a symmetric co-occurrence matrix $X \\in \\mathbb{R}^{n \\times n}$, where $X_{ij}$ counts how often word $i$ appears near word $j$ in a corpus. Assume the Global Vectors for Word Representation (GloVe) model, where each word is assigned a word vector $w_i \\in \\mathbb{R}^d$, a context vector $\\tilde{w}_i \\in \\mathbb{R}^d$, a word bias $b_i \\in \\mathbb{R}$, and a context bias $\\tilde{b}_i \\in \\mathbb{R}$. The GloVe objective is a weighted least-squares fit to the logarithm of co-occurrence, with weighting function $f(x)$ given by a well-tested empirical design:\n$$\nJ = \\sum_{i=1}^n \\sum_{j=1}^n f(X_{ij})\\left(\\, w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\,\\right)^2,\n$$\nwhere\n$$\nf(x) = \\begin{cases}\n\\left(\\dfrac{x}{x_{\\max}}\\right)^{\\alpha},  x  x_{\\max},\\\\\n1,  x \\ge x_{\\max}.\n\\end{cases}\n$$\nUsing this principle-based formulation, derive a diagnostic to identify misfit word pairs by constructing a standardized residual from the model implied by $J$. Explain how the sign of the residual connects to semantic interpretation, and implement an algorithm that:\n- For all ordered pairs with $ij$, computes a standardized residual only if $X_{ij}0$.\n- Excludes pairs with $X_{ij}=0$ because $\\log X_{ij}$ is undefined and these pairs are not included in the GloVe loss.\n- Uses the weighting function $f$ to standardize residuals so that large magnitude values indicate significant misfit under heteroscedastic noise.\n- Selects the top $k$ pairs by descending absolute standardized residual magnitude among those whose magnitude exceeds a threshold $\\tau$.\n- Returns each selected pair as a list $[i,j,s_{ij}]$ with $i$ and $j$ as zero-based indices and $s_{ij}$ rounded to three decimals.\n\nYour program must implement this diagnostic and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$), where each $\\text{result}_t$ is a list of misfit triples $[i,j,s_{ij}]$ for test case $t$.\n\nTest Suite:\n- Test Case $1$ (happy path, diverse co-occurrences):\n  - Vocabulary size $n=5$, embedding dimension $d=3$.\n  - Co-occurrence matrix $X$ (symmetric, zeros on diagonal):\n    $$\n    X = \\begin{bmatrix}\n    0  50  60  30  1\\\\\n    50  0  25  55  1\\\\\n    60  25  0  40  5\\\\\n    30  55  40  0  5\\\\\n    1  1  5  5  0\n    \\end{bmatrix}.\n    $$\n  - Word vectors $w_0=[0.8,0.2,0.05],\\, w_1=[0.75,0.25,0.1],\\, w_2=[0.6,-0.1,0.0],\\, w_3=[0.55,0.05,0.1],\\, w_4=[-0.3,0.2,-0.05]$.\n  - Context vectors $\\tilde{w}_0=[0.82,0.18,0.0],\\, \\tilde{w}_1=[0.7,0.3,0.05],\\, \\tilde{w}_2=[0.58,-0.12,0.02],\\, \\tilde{w}_3=[0.5,0.02,0.08],\\, \\tilde{w}_4=[-0.28,0.18,0.0]$.\n  - Biases $b=[0.5,\\,0.45,\\,0.4,\\,0.35,\\,0.1]$, $\\tilde{b}=[0.4,\\,0.35,\\,0.3,\\,0.25,\\,0.05]$.\n  - Weight parameters $x_{\\max}=100$, $\\alpha=0.75$.\n  - Threshold $\\tau=1.0$, top count $k=4$.\n\n- Test Case $2$ (boundary condition with zeros):\n  - Vocabulary size $n=3$, embedding dimension $d=2$.\n  - Co-occurrence matrix $X$:\n    $$\n    X = \\begin{bmatrix}\n    0  2  0\\\\\n    2  0  0\\\\\n    0  0  0\n    \\end{bmatrix}.\n    $$\n  - Word vectors $w_0=[0.1,0.0],\\, w_1=[0.0,0.1],\\, w_2=[0.05,-0.05]$.\n  - Context vectors $\\tilde{w}_0=[0.0,0.1],\\, \\tilde{w}_1=[0.1,0.0],\\, \\tilde{w}_2=[0.06,-0.04]$.\n  - Biases $b=[0.0,\\,0.0,\\,0.0]$, $\\tilde{b}=[0.0,\\,0.0,\\,0.0]$.\n  - Weight parameters $x_{\\max}=10$, $\\alpha=0.75$.\n  - Threshold $\\tau=0.1$, top count $k=3$.\n\n- Test Case $3$ (edge case with very high counts to test weighting saturation):\n  - Vocabulary size $n=4$, embedding dimension $d=2$.\n  - Co-occurrence matrix $X$:\n    $$\n    X = \\begin{bmatrix}\n    0  1000  900  700\\\\\n    1000  0  150  140\\\\\n    900  150  0  160\\\\\n    700  140  160  0\n    \\end{bmatrix}.\n    $$\n  - Word vectors $w_0=[0.9,0.1],\\, w_1=[0.2,0.7],\\, w_2=[0.25,0.65],\\, w_3=[0.3,0.6]$.\n  - Context vectors $\\tilde{w}_0=[1.0,0.0],\\, \\tilde{w}_1=[0.15,0.8],\\, \\tilde{w}_2=[0.2,0.7],\\, \\tilde{w}_3=[0.35,0.55]$.\n  - Biases $b=[1.0,\\,0.2,\\,0.25,\\,0.3]$, $\\tilde{b}=[0.8,\\,0.15,\\,0.2,\\,0.25]$.\n  - Weight parameters $x_{\\max}=100$, $\\alpha=0.75$.\n  - Threshold $\\tau=2.0$, top count $k=5$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the three results for the test suite as a comma-separated list enclosed in square brackets.\n- Each result is a list of triples $[i,j,s_{ij}]$ for the selected misfit pairs, where $i$ and $j$ are indices in $\\{0,1,\\dots,n-1\\}$, and $s_{ij}$ is the standardized residual rounded to three decimals.\n- For example, the output should look like $[[i_1,j_1,s_1],\\dots],[\\dots],[\\dots]]$ aggregated into one line as $[[\\dots],[\\dots],[\\dots]]$.", "solution": "The starting point is the Global Vectors for Word Representation (GloVe) weighted least-squares objective:\n$$\nJ = \\sum_{i=1}^n \\sum_{j=1}^n f(X_{ij})\\left(\\, w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\,\\right)^2,\n$$\nwith the empirical weighting function\n$$\nf(x) = \\begin{cases}\n\\left(\\dfrac{x}{x_{\\max}}\\right)^{\\alpha},  x  x_{\\max},\\\\\n1,  x \\ge x_{\\max}.\n\\end{cases}\n$$\nThis is a standard weighted least-squares formulation, where the inner parentheses represent the model error or residual in predicting $\\log X_{ij}$. Denote the raw residual by\n$$\nr_{ij} = w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}.\n$$\nIn classical weighted least squares under heteroscedastic noise, weights are chosen proportional to the inverse of the variance of the error. Here, $f(X_{ij})$ plays the role of a weight that increases with $X_{ij}$ up to saturation at $x_{\\max}$, capturing the idea that higher counts are more reliable. A principled diagnostic compares residuals across pairs on a common scale by standardizing with the square root of the weight, yielding\n$$\ns_{ij} = \\sqrt{f(X_{ij})}\\, r_{ij}.\n$$\nThis standardized residual has the interpretation of a z-like score under the assumption that $f(X_{ij})$ approximates inverse variance. Large magnitude $|s_{ij}|$ indicates a misfit pair relative to the model's heteroscedastic error structure.\n\nSemantic interpretation via sign follows directly from the definition of $r_{ij}$:\n- If $r_{ij}  0$, then $w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j  \\log X_{ij}$, meaning the model predicts a larger log co-occurrence than observed. This is an overprediction, suggestive of a spurious association in the embedding space.\n- If $r_{ij}  0$, then the model predicts a smaller log co-occurrence than observed. This is an underprediction, suggesting the embedding under-represents a genuine association present in the data.\n\nAlgorithmic design:\n1. For each test case, read $X \\in \\mathbb{R}^{n \\times n}$, $w \\in \\mathbb{R}^{n \\times d}$, $\\tilde{w} \\in \\mathbb{R}^{n \\times d}$, $b \\in \\mathbb{R}^n$, $\\tilde{b} \\in \\mathbb{R}^n$, $x_{\\max} \\in \\mathbb{R}$, $\\alpha \\in \\mathbb{R}$, a threshold $\\tau \\in \\mathbb{R}$, and an integer $k$.\n2. Iterate over pairs with $ij$. For each pair:\n   - If $X_{ij}=0$, skip the pair because $\\log X_{ij}$ is undefined and such pairs are excluded by the objective $J$.\n   - Compute the weight $f_{ij} = f(X_{ij})$ using the given $x_{\\max}$ and $\\alpha$.\n   - Compute the raw residual\n     $$\n     r_{ij} = w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}.\n     $$\n   - Compute the standardized residual\n     $$\n     s_{ij} = \\sqrt{f_{ij}}\\; r_{ij}.\n     $$\n3. Filter the set of computed standardized residuals to retain those with $|s_{ij}| \\ge \\tau$.\n4. Sort the filtered pairs by descending $|s_{ij}|$. In the event of ties, a deterministic secondary sort by indices $(i,j)$ ensures reproducibility.\n5. Select the first $k$ pairs from the sorted list.\n6. Return for each selected pair the triple $[i,j,s_{ij}]$ with $s_{ij}$ rounded to three decimals.\n\nEdge case handling:\n- Pairs with $X_{ij}=0$ are excluded to avoid undefined $\\log X_{ij}$ and because they are not part of the training loss.\n- Symmetry: Since $X$ is symmetric, considering only $ij$ avoids duplicate reporting of $(i,j)$ and $(j,i)$.\n\nComputational considerations:\n- The algorithm is $\\mathcal{O}(n^2 d)$ to compute dot products across pairs in the worst case, but here $n$ is small and the test matrices are sparse enough that skipping zeros reduces work.\n- Rounding $s_{ij}$ to three decimals is applied only for output formatting; sorting uses the full-precision values to preserve correct ordering.\n\nApplying to the test suite:\n- Test Case $1$ exercises typical mid-range counts; $x_{\\max}=100$ with $\\alpha=0.75$ produces weights below $1$ for small counts and $1$ for larger ones. Threshold $\\tau=1.0$ captures notable misfits while ignoring minor variations.\n- Test Case $2$ validates proper exclusion of zero entries and operation on minimal nonzero structure; with $\\tau=0.1$, the algorithm should report any evident misfit in the single nonzero pair.\n- Test Case $3$ includes very high counts for which $f(X_{ij})=1$ due to saturation ($X_{ij} \\ge x_{\\max}$), stressing the standardized residuals to reflect raw misfit magnitudes directly. A higher threshold $\\tau=2.0$ focuses the diagnostic on the most significant deviations.\n\nThe final program implements these steps and prints a single line containing a list of three results, one per test case, where each result is a list of $[i,j,s_{ij}]$ triples sorted by descending absolute standardized residual and truncated to $k$ entries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef weight_function(x, x_max, alpha):\n    # f(x) = (x/x_max)^alpha if x  x_max else 1\n    if x  x_max:\n        return (x / x_max) ** alpha\n    else:\n        return 1.0\n\ndef standardized_residuals(X, W, WT, b, bt, x_max, alpha, tau, k):\n    n = X.shape[0]\n    results = []\n    # Iterate over upper triangle ij to avoid duplicates\n    for i in range(n):\n        for j in range(i + 1, n):\n            x_ij = X[i, j]\n            if x_ij = 0:\n                continue  # exclude zeros (log undefined and not in GloVe loss)\n            f_ij = weight_function(float(x_ij), x_max, alpha)\n            # Compute residual r_ij = w_i^T wtilde_j + b_i + bt_j - log(X_ij)\n            pred = float(np.dot(W[i], WT[j])) + float(b[i]) + float(bt[j])\n            r_ij = pred - np.log(float(x_ij))\n            s_ij = np.sqrt(f_ij) * r_ij\n            results.append((i, j, s_ij))\n    # Filter by threshold |s_ij| = tau\n    filtered = [(i, j, s) for (i, j, s) in results if abs(s) = tau]\n    # Sort by descending absolute standardized residual, then by indices for determinism\n    filtered.sort(key=lambda t: (-abs(t[2]), t[0], t[1]))\n    # Take top k\n    topk = filtered[:k]\n    # Round s_ij to three decimals for output\n    as_lists = [[i, j, round(float(s), 3)] for (i, j, s) in topk]\n    return as_lists\n\ndef solve():\n    test_cases = []\n\n    # Test Case 1\n    X1 = np.array([\n        [0, 50, 60, 30, 1],\n        [50, 0, 25, 55, 1],\n        [60, 25, 0, 40, 5],\n        [30, 55, 40, 0, 5],\n        [1, 1, 5, 5, 0]\n    ], dtype=float)\n    W1 = np.array([\n        [0.8, 0.2, 0.05],\n        [0.75, 0.25, 0.1],\n        [0.6, -0.1, 0.0],\n        [0.55, 0.05, 0.1],\n        [-0.3, 0.2, -0.05]\n    ], dtype=float)\n    WT1 = np.array([\n        [0.82, 0.18, 0.0],\n        [0.7, 0.3, 0.05],\n        [0.58, -0.12, 0.02],\n        [0.5, 0.02, 0.08],\n        [-0.28, 0.18, 0.0]\n    ], dtype=float)\n    b1 = np.array([0.5, 0.45, 0.4, 0.35, 0.1], dtype=float)\n    bt1 = np.array([0.4, 0.35, 0.3, 0.25, 0.05], dtype=float)\n    params1 = (X1, W1, WT1, b1, bt1, 100.0, 0.75, 1.0, 4)\n    test_cases.append(params1)\n\n    # Test Case 2\n    X2 = np.array([\n        [0, 2, 0],\n        [2, 0, 0],\n        [0, 0, 0]\n    ], dtype=float)\n    W2 = np.array([\n        [0.1, 0.0],\n        [0.0, 0.1],\n        [0.05, -0.05]\n    ], dtype=float)\n    WT2 = np.array([\n        [0.0, 0.1],\n        [0.1, 0.0],\n        [0.06, -0.04]\n    ], dtype=float)\n    b2 = np.array([0.0, 0.0, 0.0], dtype=float)\n    bt2 = np.array([0.0, 0.0, 0.0], dtype=float)\n    params2 = (X2, W2, WT2, b2, bt2, 10.0, 0.75, 0.1, 3)\n    test_cases.append(params2)\n\n    # Test Case 3\n    X3 = np.array([\n        [0, 1000, 900, 700],\n        [1000, 0, 150, 140],\n        [900, 150, 0, 160],\n        [700, 140, 160, 0]\n    ], dtype=float)\n    W3 = np.array([\n        [0.9, 0.1],\n        [0.2, 0.7],\n        [0.25, 0.65],\n        [0.3, 0.6]\n    ], dtype=float)\n    WT3 = np.array([\n        [1.0, 0.0],\n        [0.15, 0.8],\n        [0.2, 0.7],\n        [0.35, 0.55]\n    ], dtype=float)\n    b3 = np.array([1.0, 0.2, 0.25, 0.3], dtype=float)\n    bt3 = np.array([0.8, 0.15, 0.2, 0.25], dtype=float)\n    params3 = (X3, W3, WT3, b3, bt3, 100.0, 0.75, 2.0, 5)\n    test_cases.append(params3)\n\n    results = []\n    for case in test_cases:\n        X, W, WT, b, bt, x_max, alpha, tau, k = case\n        res = standardized_residuals(X, W, WT, b, bt, x_max, alpha, tau, k)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3130256"}]}