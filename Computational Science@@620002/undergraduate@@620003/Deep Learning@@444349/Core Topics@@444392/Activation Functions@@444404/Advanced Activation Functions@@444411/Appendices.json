{"hands_on_practices": [{"introduction": "An activation function's primary role is to introduce nonlinearity, but its impact is far more profound. It directly reshapes the statistical distribution of signals passing through a neural network. This foundational exercise guides you through a first-principles derivation of how two key activations, ReLU and GELU, transform a standard Gaussian input, allowing you to analytically compare their output statistics and understand their implications for signal propagation and normalization layers. [@problem_id:3097811]", "problem": "Let $X$ denote a scalar pre-activation drawn independently and identically distributed from the standard Gaussian distribution $X \\sim \\mathcal{N}(0,1)$. Consider two activation functions: the Rectified Linear Unit (ReLU) defined by $\\phi_{\\mathrm{ReLU}}(x) = \\max(0,x)$ and the Gaussian Error Linear Unit (GELU) defined by $\\phi_{\\mathrm{GELU}}(x) = x \\, \\Phi(x)$, where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution. Using only first principles from probability and integration, derive the output distributions under these activations by computing the mean $E[\\phi(X)]$ and the variance $\\operatorname{Var}[\\phi(X)]$ for each activation. Then, for Batch Normalization (BN), defined as $y = \\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta$ with population statistics $\\mu = E[\\phi(X)]$ and $\\sigma^{2} = \\operatorname{Var}[\\phi(X)]$, determine the affine parameters $(\\gamma^{\\ast}, \\beta^{\\ast})$ that yield an output $y$ with zero mean and unit variance in the infinite-batch limit. Your derivation must start from the definitions of expectation and variance, standard normal probability density function and cumulative distribution function, and fundamental properties of jointly Gaussian random variables. Express your final answer as closed-form analytic expressions. No rounding is required. Provide the final tuple in the order $\\big(E[\\phi_{\\mathrm{ReLU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)], E[\\phi_{\\mathrm{GELU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)], \\gamma^{\\ast}, \\beta^{\\ast}\\big)$.", "solution": "Let $X$ be a random variable following the standard normal distribution, $X \\sim \\mathcal{N}(0,1)$. Its probability density function (PDF) is $p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$, and its cumulative distribution function (CDF) is $\\Phi(x) = \\int_{-\\infty}^{x} p(t) dt$. For a function $g(X)$, the expectation is defined as $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) p(x) dx$, and the variance is $\\operatorname{Var}[g(X)] = E[g(X)^2] - (E[g(X)])^2$.\n\n**Part 1: Statistics of the ReLU Activation**\n\nThe Rectified Linear Unit (ReLU) is defined as $\\phi_{\\mathrm{ReLU}}(x) = \\max(0,x)$.\n\n**1.1. Mean of ReLU output:**\nThe expectation $E[\\phi_{\\mathrm{ReLU}}(X)]$ is calculated as follows:\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\int_{-\\infty}^{\\infty} \\max(0,x) p(x) dx = \\int_{0}^{\\infty} x p(x) dx$$\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx$$\nWe perform a substitution with $u = \\frac{x^2}{2}$, which implies $du = x dx$. The limits of integration remain from $0$ to $\\infty$.\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\exp(-u) du = \\frac{1}{\\sqrt{2\\pi}} [-\\exp(-u)]_{0}^{\\infty} = \\frac{1}{\\sqrt{2\\pi}} (-0 - (-1)) = \\frac{1}{\\sqrt{2\\pi}}$$\n\n**1.2. Variance of ReLU output:**\nFirst, we compute the second moment $E[(\\phi_{\\mathrm{ReLU}}(X))^2]$.\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\int_{-\\infty}^{\\infty} (\\max(0,x))^2 p(x) dx = \\int_{0}^{\\infty} x^2 p(x) dx$$\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\int_{0}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx$$\nThe full integral $\\int_{-\\infty}^{\\infty} x^2 p(x) dx$ corresponds to the second moment of $X$, which is $E[X^2] = \\operatorname{Var}[X] + (E[X])^2 = 1 + 0^2 = 1$. The integrand $x^2 p(x)$ is an even function, so the integral from $0$ to $\\infty$ is half of the integral from $-\\infty$ to $\\infty$.\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\frac{1}{2} \\int_{-\\infty}^{\\infty} x^2 p(x) dx = \\frac{1}{2} E[X^2] = \\frac{1}{2}$$\nThe variance is then:\n$$\\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)] = E[(\\phi_{\\mathrm{ReLU}}(X))^2] - (E[\\phi_{\\mathrm{ReLU}}(X)])^2 = \\frac{1}{2} - \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2 = \\frac{1}{2} - \\frac{1}{2\\pi} = \\frac{\\pi - 1}{2\\pi}$$\n\n**Part 2: Statistics of the GELU Activation**\n\nThe Gaussian Error Linear Unit (GELU) is defined as $\\phi_{\\mathrm{GELU}}(x) = x \\Phi(x)$. We will use Stein's Lemma, a fundamental property of Gaussian variables, which states that for $X \\sim \\mathcal{N}(0,1)$ and a differentiable function $g$, $E[X g(X)] = E[g'(X)]$.\n\n**2.1. Mean of GELU output:**\nWe want to compute $E[\\phi_{\\mathrm{GELU}}(X)] = E[X \\Phi(X)]$. Let $g(x) = \\Phi(x)$. Its derivative is $g'(x) = p(x)$. Applying Stein's Lemma:\n$$E[X \\Phi(X)] = E[p(X)] = \\int_{-\\infty}^{\\infty} p(x) p(x) dx = \\int_{-\\infty}^{\\infty} p(x)^2 dx$$\n$$E[\\phi_{\\mathrm{GELU}}(X)] = \\int_{-\\infty}^{\\infty} \\left(\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\right)^2 dx = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-x^2) dx$$\nUsing the Gaussian integral result $\\int_{-\\infty}^{\\infty} \\exp(-ax^2) dx = \\sqrt{\\frac{\\pi}{a}}$ with $a=1$:\n$$E[\\phi_{\\mathrm{GELU}}(X)] = \\frac{1}{2\\pi} \\sqrt{\\pi} = \\frac{1}{2\\sqrt{\\pi}}$$\n\n**2.2. Variance of GELU output:**\nWe first compute the second moment $E[(\\phi_{\\mathrm{GELU}}(X))^2] = E[X^2 \\Phi(X)^2]$. We apply Stein's Lemma with $g(x) = x \\Phi(x)^2$. The derivative is $g'(x) = \\frac{d}{dx}(x \\Phi(x)^2) = \\Phi(x)^2 + x(2\\Phi(x)p(x)) = \\Phi(x)^2 + 2x\\Phi(x)p(x)$.\n$$E[X^2 \\Phi(X)^2] = E[g'(X)] = E[\\Phi(X)^2 + 2X\\Phi(X)p(X)] = E[\\Phi(X)^2] + 2E[X\\Phi(X)p(X)]$$\nWe must compute two expectations:\n\n(a) $E[\\Phi(X)^2] = \\int_{-\\infty}^{\\infty} \\Phi(x)^2 p(x) dx$. This can be interpreted as $P(\\max(Y,Z) \\le X)$ where $Y, Z, X$ are i.i.d. $\\mathcal{N}(0,1)$. We solve it using integration by parts. Let $u = \\Phi(x)^2$ and $dv = p(x) dx$. Then $du = 2\\Phi(x)p(x) dx$ and $v = \\Phi(x)$.\n$$J = \\int_{-\\infty}^{\\infty} \\Phi(x)^2 p(x) dx = [\\Phi(x)^3]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\Phi(x) (2\\Phi(x)p(x)) dx = (1^3 - 0^3) - 2J$$\n$$J = 1 - 2J \\implies 3J = 1 \\implies J = E[\\Phi(X)^2] = \\frac{1}{3}$$\n\n(b) $E[X\\Phi(X)p(X)] = \\int_{-\\infty}^{\\infty} x \\Phi(x) p(x)^2 dx$.\n$$p(x)^2 = \\frac{1}{2\\pi}\\exp(-x^2)$$\nSo, $E[X\\Phi(X)p(X)] = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} x \\Phi(x) \\exp(-x^2) dx$.\nIntegration by parts: let $u=\\Phi(x)$ and $dv = x \\exp(-x^2) dx$. Then $du=p(x)dx$ and $v = -\\frac{1}{2}\\exp(-x^2)$.\n$$\\int x \\Phi(x) e^{-x^2} dx = \\left[-\\frac{1}{2} e^{-x^2} \\Phi(x)\\right]_{-\\infty}^\\infty - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{2}e^{-x^2}\\right) p(x) dx$$\nThe boundary term is $0$. The integral is $\\frac{1}{2} \\int_{-\\infty}^{\\infty} e^{-x^2} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx = \\frac{1}{2\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-3x^2/2} dx$.\nUsing $\\int_{-\\infty}^{\\infty} e^{-ax^2} dx = \\sqrt{\\pi/a}$ with $a=3/2$:\n$$\\frac{1}{2\\sqrt{2\\pi}} \\sqrt{\\frac{2\\pi}{3}} = \\frac{1}{2\\sqrt{3}}$$\nThus, $E[X\\Phi(X)p(X)] = \\frac{1}{2\\pi} \\frac{1}{2\\sqrt{3}} = \\frac{1}{4\\pi\\sqrt{3}}$.\n\nCombining these results for the second moment of GELU:\n$$E[(\\phi_{\\mathrm{GELU}}(X))^2] = \\frac{1}{3} + 2 \\left( \\frac{1}{4\\pi\\sqrt{3}} \\right) = \\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}}$$\nThe variance of GELU is:\n$$\\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)] = E[(\\phi_{\\mathrm{GELU}}(X))^2] - (E[\\phi_{\\mathrm{GELU}}(X)])^2 = \\left(\\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}}\\right) - \\left(\\frac{1}{2\\sqrt{\\pi}}\\right)^2$$\n$$\\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)] = \\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}} - \\frac{1}{4\\pi} = \\frac{1}{3} + \\frac{\\sqrt{3}}{6\\pi} - \\frac{1}{4\\pi} = \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}$$\n\n**Part 3: Batch Normalization Parameters**\n\nThe Batch Normalization transformation is given by $y = \\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta$, where $\\mu = E[\\phi(X)]$ and $\\sigma^2 = \\operatorname{Var}[\\phi(X)]$. We need to find the parameters $(\\gamma^{\\ast}, \\beta^{\\ast})$ that result in an output $y$ with $E[y]=0$ and $\\operatorname{Var}[y]=1$.\n\n**3.1. Mean of y:**\nUsing the linearity of the expectation operator:\n$$E[y] = E\\left[\\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta\\right] = \\frac{\\gamma}{\\sigma} E[\\phi(X) - \\mu] + E[\\beta] = \\frac{\\gamma}{\\sigma} (E[\\phi(X)] - \\mu) + \\beta$$\nSince $\\mu=E[\\phi(X)]$, the term $(E[\\phi(X)] - \\mu)$ is $0$.\n$$E[y] = \\beta$$\nFor $E[y]=0$, we must have $\\beta^{\\ast} = 0$.\n\n**3.2. Variance of y:**\nUsing the variance property $\\operatorname{Var}[aZ+b] = a^2 \\operatorname{Var}[Z]$:\n$$\\operatorname{Var}[y] = \\operatorname{Var}\\left[\\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta\\right] = \\operatorname{Var}\\left[\\frac{\\gamma}{\\sigma}\\phi(X)\\right] = \\left(\\frac{\\gamma}{\\sigma}\\right)^2 \\operatorname{Var}[\\phi(X)]$$\nSince $\\sigma^2 = \\operatorname{Var}[\\phi(X)]$:\n$$\\operatorname{Var}[y] = \\frac{\\gamma^2}{\\sigma^2} \\sigma^2 = \\gamma^2$$\nFor $\\operatorname{Var}[y]=1$, we must have $(\\gamma^{\\ast})^2=1$. This yields $\\gamma^{\\ast} = 1$ or $\\gamma^{\\ast} = -1$. In the context of deep learning, $\\gamma$ is a scaling factor conventionally initialized to $1$ and often constrained to be positive. We adopt the standard choice $\\gamma^{\\ast}=1$.\n\nThe affine parameters are $(\\gamma^{\\ast}, \\beta^{\\ast}) = (1, 0)$. These are independent of the specific activation function $\\phi$.\n\nThe final tuple is $\\big(E[\\phi_{\\mathrm{ReLU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)], E[\\phi_{\\mathrm{GELU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)], \\gamma^{\\ast}, \\beta^{\\ast}\\big)$.\nSubstituting the derived expressions:\n$\\left( \\frac{1}{\\sqrt{2\\pi}}, \\frac{\\pi-1}{2\\pi}, \\frac{1}{2\\sqrt{\\pi}}, \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}, 1, 0 \\right)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\sqrt{2\\pi}} & \\frac{\\pi-1}{2\\pi} & \\frac{1}{2\\sqrt{\\pi}} & \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi} & 1 & 0 \\end{pmatrix}}\n$$", "id": "3097811"}, {"introduction": "The choice of activation function has direct consequences for gradient-based learning. While sharp, non-differentiable activations like ReLU and its multi-input generalization, the max operator, are computationally efficient, they can lead to a critical training pathology known as 'gradient starvation.' This practice will have you diagnose this issue, where gradients for certain features are completely zeroed out, and implement a solution by deriving and applying a temperature-smoothed version of the activation. [@problem_id:3097871]", "problem": "Consider a toy one-layer regression model with $d$ scalar features. Let the input vector be $x \\in \\mathbb{R}^d$, the parameter vector be $w \\in \\mathbb{R}^d$, and define preactivations $u \\in \\mathbb{R}^d$ by $u_i = w_i x_i$ for each index $i \\in \\{1,2,\\dots,d\\}$. The model produces a scalar output $s$ from $u$ via an activation aggregator $\\phi$, and is trained against a scalar target $y \\in \\mathbb{R}$ using the squared loss $L = \\tfrac{1}{2} (s - y)^2$. Two activation aggregators are considered:\n- A sharp-kink aggregator $\\phi_{\\text{sharp}}$ defined by $\\phi_{\\text{sharp}}(u) = \\max_{i} u_i$, which is non-differentiable at ties.\n- A temperature-smoothed aggregator $\\phi_{\\tau}$ defined by $\\phi_{\\tau}(u)$ that smoothly approximates the maximum for a positive temperature parameter $\\tau \\in \\mathbb{R}_{>0}$. The specific functional form of $\\phi_{\\tau}$ is to be derived starting from first principles of smooth approximations to the maximum using exponentials.\n\nStarting only from the definition of the loss $L$ and the chain rule for derivatives, and without using any pre-given derivative formulas for $\\phi_{\\text{sharp}}$ or $\\phi_{\\tau}$, you must:\n1. Derive $\\,\\dfrac{\\partial L}{\\partial w_i}\\,$ for $\\phi_{\\text{sharp}}$ in terms of $x$, $w$, and $y$, making a justified subgradient choice at ties. If multiple indices attain the maximum, you must choose a uniform subgradient among those indices (i.e., distribute gradient equally over all indices $i$ that achieve the maximum), and ensure your choice is consistent with the chain rule and the definition of $L$.\n2. Derive $\\,\\dfrac{\\partial L}{\\partial w_i}\\,$ for a temperature-smoothed aggregator $\\phi_{\\tau}$ constructed as a smooth maximum based on exponentials and a positive temperature parameter $\\tau$. Your derivation must clearly indicate how smoothness yields nonzero gradients for all coordinates and how the distribution of gradient mass depends on $\\tau$.\n3. Define the \"gradient starvation index\" for a given $(x,w,y,\\tau)$ as follows. Let $A \\subseteq \\{1,\\dots,d\\}$ be the set of indices that achieve the maximum of the vector $u$ (with ties included), and $A^c$ be its complement. Let $g^{\\text{sharp}} \\in \\mathbb{R}^d$ and $g^{\\tau} \\in \\mathbb{R}^d$ be the respective gradients $\\,\\dfrac{\\partial L}{\\partial w}\\,$ under $\\phi_{\\text{sharp}}$ and $\\phi_{\\tau}$. The gradient starvation index for non-argmax coordinates is\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} \\left| g^{\\text{sharp}}_i \\right|, \\quad\n\\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} \\left| g^{\\tau}_i \\right|.\n$$\nCompute both indices and the \"alleviation amount\"\n$$\n\\Delta_{\\tau} = \\operatorname{SI}_{\\tau} - \\operatorname{SI}_{\\text{sharp}}.\n$$\n\nImplement a complete program that, for each test case specified below, computes the three floating-point quantities $\\,\\operatorname{SI}_{\\text{sharp}},\\,\\operatorname{SI}_{\\tau},\\,\\Delta_{\\tau}\\,$ using the derived formulas and the uniform subgradient choice at ties for $\\phi_{\\text{sharp}}$. Use a numerically stable implementation for the smooth aggregator that remains well-behaved for very small $\\tau$.\n\nTest suite (each case is $(d,x,w,y,\\tau)$):\n- Case $1$: $d = 4$, $x = [1.0, 0.9, 0.8, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 2.0$, $\\tau = 0.5$.\n- Case $2$: $d = 4$, $x = [1.0, 0.9, 0.8, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 2.0$, $\\tau = 0.001$.\n- Case $3$: $d = 4$, $x = [1.0, 1.0, 0.1, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 3.0$, $\\tau = 0.5$.\n- Case $4$: $d = 3$, $x = [-0.5, 0.4, 0.39]$, $w = [-2.0, 1.0, 1.0]$, $y = 0.0$, $\\tau = 0.2$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of inner lists, each inner list being $[\\operatorname{SI}_{\\text{sharp}}, \\operatorname{SI}_{\\tau}, \\Delta_{\\tau}]$, enclosed in square brackets. For example: $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$. No other text should be printed.", "solution": "The problem requires the derivation and computation of gradients for a one-layer regression model under two different activation aggregation schemes, and the subsequent analysis of a \"gradient starvation index.\"\n\nThe model is defined by an input $x \\in \\mathbb{R}^d$, parameters $w \\in \\mathbb{R}^d$, and preactivations $u \\in \\mathbb{R}^d$ where $u_i = w_i x_i$. The model output is $s = \\phi(u)$, and the loss against a target $y \\in \\mathbb{R}$ is $L = \\frac{1}{2}(s - y)^2$. Our objective is to find the partial derivative of the loss with respect to each weight, $\\frac{\\partial L}{\\partial w_i}$.\n\nWe begin by applying the chain rule. The derivative of $L$ with respect to $w_i$ can be decomposed as follows:\n$$\n\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial s} \\frac{\\partial s}{\\partial w_i}\n$$\nThe first term, the derivative of the loss with respect to the model output $s$, is straightforward:\n$$\n\\frac{\\partial L}{\\partial s} = \\frac{\\partial}{\\partial s} \\left( \\frac{1}{2}(s - y)^2 \\right) = s - y\n$$\nThe second term, $\\frac{\\partial s}{\\partial w_i}$, requires a further application of the chain rule, since $s = \\phi(u)$ and $u$ is a function of $w$. The output $s$ depends on all components of $u$, which in turn depend on $w$.\n$$\n\\frac{\\partial s}{\\partial w_i} = \\sum_{j=1}^{d} \\frac{\\partial s}{\\partial u_j} \\frac{\\partial u_j}{\\partial w_i}\n$$\nThe preactivation $u_j$ is defined as $u_j = w_j x_j$. Its partial derivative with respect to $w_i$ is non-zero only when $j=i$:\n$$\n\\frac{\\partial u_j}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} (w_j x_j) = \\begin{cases} x_i & \\text{if } j=i \\\\ 0 & \\text{if } j \\neq i \\end{cases}\n$$\nSubstituting this into the sum, all terms where $j \\neq i$ vanish, leaving only the $j=i$ term:\n$$\n\\frac{\\partial s}{\\partial w_i} = \\frac{\\partial s}{\\partial u_i} \\frac{\\partial u_i}{\\partial w_i} = \\frac{\\partial s}{\\partial u_i} x_i\n$$\nCombining these results, we obtain the general formula for the gradient component $g_i = \\frac{\\partial L}{\\partial w_i}$:\n$$\ng_i = \\frac{\\partial L}{\\partial w_i} = (s - y) \\frac{\\partial s}{\\partial u_i} x_i\n$$\nThe specific form of the gradient therefore depends on the term $\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial \\phi}{\\partial u_i}$, which we will now derive for the two specified aggregators.\n\n**1. Gradient for the Sharp-Kink Aggregator $\\phi_{\\text{sharp}}$**\n\nThe sharp-kink aggregator is defined as the maximum function: $s = \\phi_{\\text{sharp}}(u) = \\max_{j} u_j$.\nThe maximum function is differentiable everywhere except at points where two or more components of $u$ are equal to the maximum value.\nLet $A = \\{k \\in \\{1,\\dots,d\\} \\mid u_k = \\max_j u_j\\}$ be the set of indices corresponding to the maximal value of $u$. Let $|A|$ be the cardinality of this set.\n\nCase 1: Unique maximum ($|A|=1$). Let $A = \\{k\\}$. Then for $u$ in a neighborhood of the current value, $s = u_k$. The partial derivative is:\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial u_k}{\\partial u_i} = \\delta_{ik} = \\begin{cases} 1 & \\text{if } i=k \\\\ 0 & \\text{if } i \\neq k \\end{cases}\n$$\nCase 2: Tie for the maximum ($|A|>1$). The function is not differentiable at this point. We must select a subgradient. The subdifferential of the max function, $\\partial(\\max)(u)$, is the convex hull of the standard basis vectors corresponding to the indices in $A$: $\\partial(\\max)(u) = \\text{conv}\\{e_k \\mid k \\in A\\}$. A vector $v \\in \\mathbb{R}^d$ is a subgradient if its components are $v_i$ such that $v_i \\ge 0$, $\\sum_{i \\in A} v_i = 1$, and $v_i = 0$ for $i \\notin A$.\n\nThe problem mandates a \"uniform subgradient choice,\" which corresponds to distributing the gradient equally among all maximal components. This is a specific, justified choice of subgradient where we set the coefficients to be uniform:\n$$\n\\frac{\\partial s}{\\partial u_i} = \\begin{cases} 1/|A| & \\text{if } i \\in A \\\\ 0 & \\text{if } i \\notin A \\end{cases}\n$$\nThis single formula correctly handles both the unique maximum case (where $|A|=1$) and the tie case.\n\nThe gradient component for the sharp aggregator, $g^{\\text{sharp}}_i$, is therefore:\n$$\ng^{\\text{sharp}}_i = (\\max_j u_j - y) \\cdot x_i \\cdot \\begin{cases} 1/|A| & \\text{if } i \\in A \\\\ 0 & \\text{if } i \\notin A \\end{cases}\n$$\n\n**2. Gradient for the Temperature-Smoothed Aggregator $\\phi_{\\tau}$**\n\nWe are tasked to construct a smooth approximation of the maximum function based on exponentials. A standard construction is the LogSumExp function, scaled by a temperature parameter $\\tau > 0$. We define $\\phi_{\\tau}(u)$ as:\n$$\ns = \\phi_{\\tau}(u) = \\tau \\ln \\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right)\n$$\nTo see that this approximates the maximum, let $u_k = \\max_j u_j$. We can rewrite the expression as:\n$$\n\\phi_{\\tau}(u) = \\tau \\ln \\left( e^{u_k/\\tau} \\sum_{j=1}^{d} e^{(u_j - u_k)/\\tau} \\right) = u_k + \\tau \\ln \\left( 1 + \\sum_{j \\neq k} e^{(u_j - u_k)/\\tau} \\right)\n$$\nAs $\\tau \\to 0^+$, for any $j \\neq k$, the term $(u_j - u_k)/\\tau \\to -\\infty$, so $e^{(u_j - u_k)/\\tau} \\to 0$. The sum vanishes, the logarithm approaches $\\ln(1)=0$, and thus $\\phi_{\\tau}(u) \\to u_k = \\max_j u_j$.\n\nThe partial derivative $\\frac{\\partial s}{\\partial u_i}$ is found by differentiating $\\phi_{\\tau}(u)$:\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial}{\\partial u_i} \\left[ \\tau \\ln \\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right) \\right] = \\tau \\cdot \\frac{1}{\\sum_{j=1}^{d} e^{u_j/\\tau}} \\cdot \\frac{\\partial}{\\partial u_i}\\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right)\n$$\nThe derivative of the sum with respect to $u_i$ is simply the derivative of the $i$-th term:\n$$\n\\frac{\\partial}{\\partial u_i}\\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right) = e^{u_i/\\tau} \\cdot \\frac{1}{\\tau}\n$$\nSubstituting back, we get:\n$$\n\\frac{\\partial s}{\\partial u_i} = \\tau \\cdot \\frac{1}{\\sum_j e^{u_j/\\tau}} \\cdot \\frac{e^{u_i/\\tau}}{\\tau} = \\frac{e^{u_i/\\tau}}{\\sum_j e^{u_j/\\tau}}\n$$\nThis is the softmax function applied to the vector $u/\\tau$. This expression is strictly positive for all $i$, demonstrating how smoothness yields non-zero partial derivatives (and thus non-zero gradients) for all weight coordinates, unlike the sharp aggregator which yields zero gradients for non-maximal coordinates. The magnitude of the gradient depends on the relative value of $u_i$ and the temperature $\\tau$. A smaller $\\tau$ leads to a 'sharper' softmax distribution, concentrating the gradient mass on the component with the largest $u_i$, whereas a larger $\\tau$ results in a 'softer', more uniform distribution.\n\nFor numerical stability, especially when some $u_j/\\tau$ are large, we can factor out the maximum value $u_{\\max} = \\max_j u_j$:\n$$\n\\phi_{\\tau}(u) = u_{\\max} + \\tau \\ln \\left( \\sum_j e^{(u_j-u_{\\max})/\\tau} \\right)\n$$\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{e^{(u_i-u_{\\max})/\\tau}}{\\sum_j e^{(u_j-u_{\\max})/\\tau}}\n$$\nThe gradient component for the smooth aggregator, $g^{\\tau}_i$, is therefore:\n$$\ng^{\\tau}_i = \\left(\\phi_{\\tau}(u) - y\\right) \\cdot \\frac{e^{u_i/\\tau}}{\\sum_j e^{u_j/\\tau}} \\cdot x_i\n$$\n\n**3. Gradient Starvation Indices**\n\nThe set of maximizing indices is $A = \\{i \\mid u_i = \\max_j u_j\\}$ and its complement is $A^c$. The gradient starvation indices are defined as:\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} \\left| g^{\\text{sharp}}_i \\right|, \\quad \\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} \\left| g^{\\tau}_i \\right|\n$$\nFrom our derivation of $g^{\\text{sharp}}_i$, the term $\\frac{\\partial s}{\\partial u_i}$ is zero if $i \\notin A$ (i.e., $i \\in A^c$). Consequently, $g^{\\text{sharp}}_i=0$ for all $i \\in A^c$. This leads to the conclusion that the gradient starvation for the sharp aggregator is absolute for non-maximal coordinates:\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} |0| = 0\n$$\nThe term \"gradient starvation\" is thus literal for the sharp-kink case.\n\nFor the smooth aggregator, $g^{\\tau}_i$ is generally non-zero for all $i$, so $\\operatorname{SI}_{\\tau}$ will be a positive value representing the total magnitude of gradient \"leaked\" to the non-maximal components.\nThe alleviation amount is defined as $\\Delta_{\\tau} = \\operatorname{SI}_{\\tau} - \\operatorname{SI}_{\\text{sharp}}$. Given our finding, this simplifies to:\n$$\n\\Delta_{\\tau} = \\operatorname{SI}_{\\tau}\n$$\nThe calculation for each test case proceeds by first computing the preactivations $u$, identifying the set $A^c$, and then calculating $g^{\\tau}_i$ for all $i \\in A^c$ using the numerically stable formulas. The required quantities are then $\\operatorname{SI}_{\\text{sharp}}=0$, $\\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} |g^{\\tau}_i|$, and $\\Delta_{\\tau} = \\operatorname{SI}_{\\tau}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating gradient starvation indices for sharp and\n    smooth max aggregators in a simple regression model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: d=4, x=[1.0, 0.9, 0.8, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=2.0, tau=0.5\n        (4, [1.0, 0.9, 0.8, 0.1], [1.0, 1.0, 1.0, 1.0], 2.0, 0.5),\n        # Case 2: d=4, x=[1.0, 0.9, 0.8, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=2.0, tau=0.001\n        (4, [1.0, 0.9, 0.8, 0.1], [1.0, 1.0, 1.0, 1.0], 2.0, 0.001),\n        # Case 3: d=4, x=[1.0, 1.0, 0.1, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=3.0, tau=0.5\n        (4, [1.0, 1.0, 0.1, 0.1], [1.0, 1.0, 1.0, 1.0], 3.0, 0.5),\n        # Case 4: d=3, x=[-0.5, 0.4, 0.39], w=[-2.0, 1.0, 1.0], y=0.0, tau=0.2\n        (3, [-0.5, 0.4, 0.39], [-2.0, 1.0, 1.0], 0.0, 0.2),\n    ]\n\n    results = []\n    for d, x_list, w_list, y, tau in test_cases:\n        x = np.array(x_list, dtype=float)\n        w = np.array(w_list, dtype=float)\n        \n        # 1. Compute preactivations u\n        u = w * x\n        \n        # 2. Identify maximal and non-maximal indices\n        u_max = np.max(u)\n        # Using a small tolerance for float comparison, though not strictly necessary for given inputs\n        # but is good practice. In this case, direct comparison works.\n        is_max = (u == u_max)\n        is_not_max = ~is_max\n        \n        # 3. Calculate SI_sharp\n        # As derived, the gradient for non-maximal elements is always 0 for phi_sharp.\n        si_sharp = 0.0\n        \n        # 4. Calculate SI_tau\n        # Numerically stable calculation for s_tau and softmax probabilities\n        u_shifted = u - u_max\n        exp_terms = np.exp(u_shifted / tau)\n        sum_exp_terms = np.sum(exp_terms)\n        \n        # s_tau = u_max + tau * np.log(sum_exp_terms)\n        s_tau = u_max + tau * np.log(np.sum(np.exp((u - u_max) / tau)))\n        \n        # Common loss derivative term\n        dL_ds = s_tau - y\n        \n        # Softmax probabilities\n        p = exp_terms / sum_exp_terms\n        \n        # Gradient g_tau\n        g_tau = dL_ds * p * x\n        \n        # Sum of absolute gradients for non-maximal components\n        si_tau = np.sum(np.abs(g_tau[is_not_max]))\n        \n        # 5. Calculate Delta_tau\n        delta_tau = si_tau - si_sharp\n        \n        results.append([si_sharp, si_tau, delta_tau])\n\n    # Final print statement in the exact required format.\n    # The format [[a1,b1,c1],[a2,b2,c2]] is a string representation of a list of lists.\n    # We construct this string manually to avoid spaces and ensure exact format.\n    inner_lists_str = [','.join(map(str, r)) for r in results]\n    result_str = f\"[[{'],['.join(inner_lists_str)}]]\"\n    \n    print(result_str)\n\nsolve()\n```", "id": "3097871"}, {"introduction": "Beyond just transforming signals, some activation functions can be understood as performing a principled optimization step within the network's forward pass. This advanced practice uncovers a deep connection between activation functions and sparse feature learning. You will derive the soft-thresholding operator from a regularized objective function and see how its application is equivalent to a step in the proximal gradient method for enforcing an $L_1$ penalty, thereby linking an architectural component to a powerful regularization technique. [@problem_id:3097828]", "problem": "Consider a single linear layer that produces pre-activations $v \\in \\mathbb{R}^n$ before a nonlinearity, and suppose learned features $a \\in \\mathbb{R}^n$ are obtained by solving a local empirical risk with a squared error term and a one-norm regularizer. The objective for given $v$ and regularization parameter $\\lambda \\ge 0$ is\n$$\nJ(a; v, \\lambda) = \\frac{1}{2}\\|a - v\\|_2^2 + \\lambda \\|a\\|_1.\n$$\nAssume $J$ is minimized coordinate-wise due to separability. From first principles (convexity, separability, and subgradient optimality), derive the activation mapping that produces the unique minimizer of $J$ for any coordinate, and explain why this mapping promotes sparsity in $a$ relative to $v$. Then, using the Proximal Gradient Method (PGM), justify how applying this activation in the forward pass is equivalent to performing a single proximal step for one-norm regularization on the features, thereby relating the activation to learned features with one-norm penalties.\n\nImplementation task: Write a complete, runnable program that, for each test case specified below, computes the decrease in the regularized objective achieved by applying the derived activation mapping to $v$ versus leaving $a = v$ unchanged. For each test case, compute the scalar\n$$\n\\Delta(v,\\lambda) = J(v; v, \\lambda) - J(\\phi(v;\\lambda); v, \\lambda),\n$$\nwhere $\\phi(\\cdot;\\lambda)$ denotes the derived activation mapping. Your program must output all $\\Delta$ values for the test suite as a single line containing a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3]$). No physical units or angle units are involved, and all outputs are real numbers.\n\nTest suite:\n- Case $1$: $v = [3.0,-1.5,0.2,-0.05]$, $\\lambda = 0.5$.\n- Case $2$: $v = [0.5,-0.5,0.49,-0.51]$, $\\lambda = 0.5$.\n- Case $3$: $v = [0.1,-0.2,0.0,0.05]$, $\\lambda = 10.0$.\n- Case $4$: $v = [-2.0,2.0]$, $\\lambda = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3,x_4]$), where each $x_i$ is the computed $\\Delta(v,\\lambda)$ for the corresponding test case, represented as a floating-point number.", "solution": "### Part 1: Derivation of the Activation Mapping from First Principles\n\nThe objective function to minimize is given by:\n$$\nJ(a; v, \\lambda) = \\frac{1}{2}\\|a - v\\|_2^2 + \\lambda \\|a\\|_1\n$$\nwhere $a, v \\in \\mathbb{R}^n$ and $\\lambda \\ge 0$. The squared L2-norm term is $\\|a - v\\|_2^2 = \\sum_{i=1}^n (a_i - v_i)^2$, and the L1-norm term is $\\|a\\|_1 = \\sum_{i=1}^n |a_i|$. The objective function is separable, meaning it can be written as a sum of functions of individual coordinates:\n$$\nJ(a; v, \\lambda) = \\sum_{i=1}^n \\left( \\frac{1}{2}(a_i - v_i)^2 + \\lambda |a_i| \\right) = \\sum_{i=1}^n J_i(a_i; v_i, \\lambda)\n$$\nTherefore, we can minimize $J$ by minimizing each $J_i$ with respect to $a_i$ independently. The objective for a single coordinate $a_i$ is:\n$$\nJ_i(a_i; v_i, \\lambda) = \\frac{1}{2}(a_i - v_i)^2 + \\lambda |a_i|\n$$\nThis function is the sum of a strictly convex differentiable function, $\\frac{1}{2}(a_i - v_i)^2$, and a convex non-differentiable function, $\\lambda |a_i|$. The sum is strictly convex, guaranteeing a unique minimizer.\n\nTo find the minimizer $a_i^*$, we use the first-order optimality condition from convex analysis, which states that $0$ must be in the subgradient of $J_i$ at the minimum, i.e., $0 \\in \\partial J_i(a_i^*)$.\n\nThe subgradient of $J_i(a_i)$ is given by:\n$$\n\\partial J_i(a_i) = \\frac{d}{d a_i} \\left(\\frac{1}{2}(a_i - v_i)^2\\right) + \\lambda \\partial |a_i| = (a_i - v_i) + \\lambda \\partial |a_i|\n$$\nThe subgradient of the absolute value function, $\\partial|x|$, is defined as:\n$$\n\\partial |x| = \\begin{cases} \\{1\\} & \\text{if } x > 0 \\\\ \\{-1\\} & \\text{if } x < 0 \\\\ [-1, 1] & \\text{if } x = 0 \\end{cases}\n$$\nThis can be written compactly as $\\text{sgn}(x)$, provided we define $\\text{sgn}(0) = [-1, 1]$. The optimality condition $0 \\in \\partial J_i(a_i^*)$ becomes:\n$$\n0 \\in (a_i^* - v_i) + \\lambda \\cdot \\partial |a_i^*| \\implies v_i - a_i^* \\in \\lambda \\cdot \\partial|a_i^*|\n$$\nWe analyze this condition in three cases for the value of $a_i^*$:\n1.  **Case 1: $a_i^* > 0$**. The subgradient $\\partial|a_i^*|$ is $\\{1\\}$. The condition becomes $v_i - a_i^* = \\lambda$, which yields $a_i^* = v_i - \\lambda$. For this solution to be consistent with the assumption $a_i^* > 0$, we must have $v_i - \\lambda > 0$, or $v_i > \\lambda$.\n2.  **Case 2: $a_i^* < 0$**. The subgradient $\\partial|a_i^*|$ is $\\{-1\\}$. The condition becomes $v_i - a_i^* = -\\lambda$, which yields $a_i^* = v_i + \\lambda$. For this to be consistent with $a_i^* < 0$, we must have $v_i + \\lambda < 0$, or $v_i < -\\lambda$.\n3.  **Case 3: $a_i^* = 0$**. The subgradient $\\partial|a_i^*|$ is the interval $[-1, 1]$. The condition becomes $v_i - 0 \\in \\lambda [-1, 1]$, which simplifies to $v_i \\in [-\\lambda, \\lambda]$, or $|v_i| \\le \\lambda$.\n\nCombining these three mutually exclusive cases gives the complete solution for the minimizer $a_i^*$:\n$$\na_i^* = \\phi(v_i; \\lambda) = \\begin{cases} v_i - \\lambda & \\text{if } v_i > \\lambda \\\\ 0 & \\text{if } |v_i| \\le \\lambda \\\\ v_i + \\lambda & \\text{if } v_i < -\\lambda \\end{cases}\n$$\nThis activation mapping, $\\phi$, is known as the **soft-thresholding operator**. It can be written more compactly as:\n$$\na_i^* = \\phi(v_i; \\lambda) = \\text{sgn}(v_i) \\max(0, |v_i| - \\lambda)\n$$\n\n### Part 2: Promotion of Sparsity\n\nThe soft-thresholding operator $\\phi(v_i; \\lambda)$ promotes sparsity in the learned feature vector $a$ relative to the pre-activation vector $v$. Sparsity means that many components of the vector are exactly zero. The mechanism is clear from the derived formula:\n-   Any coordinate $v_i$ of the input vector whose absolute value $|v_i|$ does not exceed the threshold $\\lambda$ is mapped to $a_i^* = 0$. This creates a \"dead zone\" for activations, effectively zeroing out small signals.\n-   For any coordinate $v_i$ whose absolute value $|v_i|$ is greater than $\\lambda$, the resulting activation $a_i^*$ is shrunk towards zero by an amount $\\lambda$. For example, if $v_i > \\lambda$, then $a_i^* = v_i - \\lambda < v_i$.\n\nBy setting to zero all features whose initial magnitude is below a certain threshold $\\lambda$, the mapping increases the number of zero entries in $a$ compared to $v$, thereby promoting a sparse representation. The parameter $\\lambda$ directly controls the degree of sparsity: a larger $\\lambda$ results in a wider dead zone $[-\\lambda, \\lambda]$ and more aggressive shrinkage, leading to a sparser output vector $a$.\n\n### Part 3: Relation to Proximal Gradient Method (PGM)\n\nThe Proximal Gradient Method (PGM) is an iterative algorithm for minimizing composite objective functions of the form $F(x) = f(x) + g(x)$, where $f$ is convex and differentiable (smooth) and $g$ is convex but may be non-differentiable. The update rule for PGM is:\n$$\nx_{k+1} = \\text{prox}_{\\eta g}(x_k - \\eta \\nabla f(x_k))\n$$\nwhere $\\eta > 0$ is a step size. The key component is the proximal operator of a function $h$, defined as:\n$$\n\\text{prox}_{h}(y) = \\arg\\min_x \\left( h(x) + \\frac{1}{2}\\|x - y\\|_2^2 \\right)\n$$\nLet us compare this definition with the original problem statement. Our objective was to find:\n$$\na^* = \\arg\\min_a J(a; v, \\lambda) = \\arg\\min_a \\left( \\lambda \\|a\\|_1 + \\frac{1}{2}\\|a - v\\|_2^2 \\right)\n$$\nBy direct comparison with the definition of the proximal operator, we see that finding the minimizer $a^*$ of $J(a; v, \\lambda)$ is equivalent to computing the proximal operator of the function $h(a) = \\lambda \\|a\\|_1$ evaluated at the point $y=v$.\nThus, the derived activation function $\\phi(v; \\lambda)$ is precisely the proximal operator of the L1-norm regularizer:\n$$\n\\phi(v; \\lambda) = \\text{prox}_{\\lambda \\|\\cdot\\|_1}(v)\n$$\nThis establishes a direct link between the activation function and PGM. In a typical machine learning context, we might minimize a regularized loss, e.g., $\\min_w \\mathcal{L}(w) + \\gamma \\|w\\|_1$, where $\\mathcal{L}$ is a smooth data-fitting loss term (like mean squared error) and $\\gamma \\|w\\|_1$ is the non-smooth regularization term. A single PGM step to update the weights $w$ would be:\n1.  Perform a gradient descent step on the smooth part: $w' = w_k - \\eta \\nabla \\mathcal{L}(w_k)$.\n2.  Apply the proximal operator to the result: $w_{k+1} = \\text{prox}_{\\eta \\gamma \\|\\cdot\\|_1}(w')$.\n\nIf we identify the pre-activation $v$ with the result of the gradient step, $v = w'$, and set $\\lambda = \\eta \\gamma$, then applying the activation function $\\phi(v; \\lambda)$ is mathematically identical to performing the proximal update step in PGM. This shows how an activation function in a neural network's forward pass can be interpreted as executing one iteration of an optimization algorithm for learning features under a sparsity-inducing L1-penalty.\n\n### Part 4: Calculation of the Objective Decrease\n\nThe quantity to compute is $\\Delta(v, \\lambda) = J(v; v, \\lambda) - J(\\phi(v; \\lambda); v, \\lambda)$. Let $a^* = \\phi(v; \\lambda)$.\nThe first term is:\n$$\nJ(v; v, \\lambda) = \\frac{1}{2}\\|v - v\\|_2^2 + \\lambda \\|v\\|_1 = \\lambda \\|v\\|_1\n$$\nThe second term is:\n$$\nJ(a^*; v, \\lambda) = \\frac{1}{2}\\|a^* - v\\|_2^2 + \\lambda \\|a^*\\|_1\n$$\nSo, the decrease in the objective is:\n$$\n\\Delta(v, \\lambda) = \\lambda \\|v\\|_1 - \\left( \\frac{1}{2}\\|a^* - v\\|_2^2 + \\lambda \\|a^*\\|_1 \\right)\n$$\nThis value will be computed for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the decrease in the regularized objective J(a; v, lambda)\n    by applying the soft-thresholding activation versus leaving a=v.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([3.0, -1.5, 0.2, -0.05]), 0.5),\n        (np.array([0.5, -0.5, 0.49, -0.51]), 0.5),\n        (np.array([0.1, -0.2, 0.0, 0.05]), 10.0),\n        (np.array([-2.0, 2.0]), 0.0),\n    ]\n\n    results = []\n    \n    for v, lmbda in test_cases:\n        # Define the objective function J(a; v, lambda)\n        def objective_J(a, v_in, lambda_in):\n            \"\"\"\n            Computes J(a; v, lambda) = 0.5 * ||a - v||_2^2 + lambda * ||a||_1\n            \"\"\"\n            l2_term = 0.5 * np.sum((a - v_in)**2)\n            l1_term = lambda_in * np.sum(np.abs(a))\n            return l2_term + l1_term\n\n        # Define the soft-thresholding activation function phi(v; lambda)\n        def soft_thresholding(v_in, lambda_in):\n            \"\"\"\n            Computes a* = sgn(v) * max(0, |v| - lambda) element-wise.\n            \"\"\"\n            return np.sign(v_in) * np.maximum(0, np.abs(v_in) - lambda_in)\n\n        # 1. Compute the optimal activation a_star = phi(v; lambda)\n        a_star = soft_thresholding(v, lmbda)\n\n        # 2. Compute J(v; v, lambda). The L2 term is zero.\n        # J(v; v, lambda) = 0.5 * ||v-v||_2^2 + lambda * ||v||_1 = lambda * ||v||_1\n        J_v = lmbda * np.sum(np.abs(v))\n        \n        # 3. Compute J(a_star; v, lambda)\n        J_a_star = objective_J(a_star, v, lmbda)\n\n        # 4. Compute the decrease in the objective\n        # delta = J(v; v, lambda) - J(phi(v;lambda); v, lambda)\n        delta = J_v - J_a_star\n        \n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097828"}]}