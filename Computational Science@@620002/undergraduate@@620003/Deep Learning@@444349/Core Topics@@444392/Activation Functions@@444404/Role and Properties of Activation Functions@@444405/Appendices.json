{"hands_on_practices": [{"introduction": "The fundamental purpose of an activation function is to introduce non-linearity, which allows neural networks to approximate arbitrarily complex functions. This exercise provides a striking and practical demonstration of this principle. By constructing a specific scenario where the popular ReLU activation is forced to operate only in its linear region, you will see firsthand how the network's expressive power collapses, rendering it unable to learn a simple sinusoidal function. This practice will solidify your understanding of why effective non-linearity is the cornerstone of a neural network's ability to model the world [@problem_id:3171992].", "problem": "You will study how the choice of activation function shapes the hypothesis class and thereby affects approximation error for a target function with high curvature. Work in one dimension. Consider the Rectified Linear Unit (ReLU) activation $g_{\\mathrm{ReLU}}(z)=\\max(0,z)$ and the hyperbolic tangent activation $g_{\\tanh}(z)=\\tanh(z)$. Let the target be $f^\\ast(x)=\\sin(\\omega x)$ with angles in radians. You will construct a dataset and two families of single-hidden-layer models of equal width $m$, then compare their empirical mean squared errors (MSE) when fit by least squares.\n\nBase definitions to use:\n- A single-hidden-layer model with fixed hidden parameters $\\{a_i,b_i\\}_{i=1}^m$ and activation $g$ has the form $h(x)=c+\\sum_{i=1}^m w_i\\,g(a_i x+b_i)$, where $c$ and $\\{w_i\\}$ are the trainable output layer parameters. For a dataset $\\{(x_j,y_j)\\}_{j=1}^N$, the empirical mean squared error is $\\frac{1}{N}\\sum_{j=1}^N\\left(h(x_j)-y_j\\right)^2$.\n- Fitting by least squares over the output layer means choosing $(c,\\{w_i\\})$ to minimize the empirical mean squared error, equivalently solving a linear least-squares problem over the feature matrix formed by $g(a_i x_j+b_i)$ augmented with a bias feature.\n\nConstruct the dataset and models as follows.\n1) Dataset: Let $N=1001$ and $x_j$ be $N$ equally spaced points in $[-1,1]$. Let $y_j=f^\\ast(x_j)=\\sin(\\omega x_j)$, with $\\omega$ specified per test case and interpreted in radians.\n2) ReLU model family $H_{\\mathrm{ReLU}}$: Fix width $m$. Choose slopes $a_i$ as $m$ equally spaced values in $[-\\alpha,\\alpha]$ with $\\alpha=0.5$. Choose a shared bias $b_i^{\\mathrm{ReLU}}=1.0$ for all hidden units. On $x\\in[-1,1]$, these choices guarantee $a_i x + b_i^{\\mathrm{ReLU}} \\ge 0.5 > 0$ for all $i$, so $g_{\\mathrm{ReLU}}(a_i x+b_i^{\\mathrm{ReLU}})=a_i x+b_i^{\\mathrm{ReLU}}$. Consequently, any $h \\in H_{\\mathrm{ReLU}}$ reduces to an affine function of $x$.\n3) Tanh model family $H_{\\tanh}$: Fix width $m$. Choose slopes $a_i$ as $m$ equally spaced values in $[-A,A]$ with $A=\\max(1.0,0.7\\,\\omega)$ and set $b_i^{\\tanh}=0$ for all hidden units. This places the hidden pre-activations near the high-slope region of $g_{\\tanh}$ so that $H_{\\tanh}$ spans a richer, smooth, nonlinear family.\n\nFor each model family, construct a feature matrix $\\Phi \\in \\mathbb{R}^{N \\times (m+1)}$ whose first column is the bias feature $\\phi_0(x)\\equiv 1$, and whose remaining $m$ columns are $\\phi_i(x)=g(a_i x+b_i)$ evaluated at the $N$ inputs. Compute the least-squares solution $\\hat{\\theta} = \\arg\\min_{\\theta} \\|\\Phi\\theta - y\\|_2^2$ and report the empirical mean squared error $E = \\frac{1}{N} \\|\\Phi\\hat{\\theta}-y\\|_2^2$.\n\nAngle unit requirement: All trigonometric evaluations use radians. The output must not use a percentage sign; any rates must be in decimal form.\n\nTest suite and required outputs:\n- Use the following parameter sets $(\\omega,m)$: $(0.0,5)$, $(3.0,5)$, $(9.0,5)$, $(9.0,2)$, $(15.0,10)$.\n- For each parameter set, compute three floats: $E_{\\mathrm{ReLU}}$, $E_{\\tanh}$, and the difference $E_{\\mathrm{ReLU}}-E_{\\tanh}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the triple for one test case, itself formatted as a comma-separated list enclosed in square brackets. For example: $[[e_{1,\\mathrm{ReLU}},e_{1,\\tanh},d_1],[e_{2,\\mathrm{ReLU}},e_{2,\\tanh},d_2],\\ldots]$. Round all floats to $6$ decimal places in the final output string.\n\nDeliverable: Provide a complete, runnable program that constructs the dataset, builds both feature matrices per test, solves the least-squares problems, computes the requested errors, and prints exactly one line in the required format. No user input or external files are allowed.", "solution": "The user-provided problem is a valid, well-posed, and scientifically grounded exercise in computational machine learning. It asks for a comparison of the approximation capabilities of two single-hidden-layer neural network models, one using the Rectified Linear Unit (ReLU) activation function and the other using the hyperbolic tangent (tanh) activation function. The analysis hinges on how the choice of fixed hidden-layer parameters affects the richness of the function class (hypothesis space) that each model can represent.\n\nThe solution proceeds by first formalizing the problem as a linear least-squares task. Subsequently, we construct the feature matrices corresponding to each model family for the given test parameters. The least-squares solution is then computed to find the best-fit model in each family, and their respective mean squared errors are calculated and compared.\n\n**1. Problem Formulation as Linear Least-Squares**\n\nA single-hidden-layer model is defined as $h(x) = c + \\sum_{i=1}^m w_i\\,g(a_i x+b_i)$. The hidden layer parameters, slopes $\\{a_i\\}_{i=1}^m$ and biases $\\{b_i\\}_{i=1}^m$, are fixed according to the problem specification for each model family. The trainable parameters are the output-layer bias $c$ and weights $\\{w_i\\}_{i=1}^m$.\n\nFor a dataset of $N$ points $\\{(x_j, y_j)\\}_{j=1}^N$, we can express the model's predictions as a matrix-vector product. Let $\\theta = [c, w_1, \\ldots, w_m]^T$ be the column vector of trainable parameters. We define a feature matrix $\\Phi \\in \\mathbb{R}^{N \\times (m+1)}$ where each row corresponds to an input point $x_j$ and each column corresponds to a basis function. The first column is a constant bias term, $\\phi_0(x_j) = 1$ for all $j$. The subsequent $m$ columns are the outputs of the hidden neurons, $\\phi_i(x_j) = g(a_i x_j + b_i)$ for $i=1, \\ldots, m$.\n\nThe vector of model predictions for all $x_j$ is given by $\\hat{y} = \\Phi\\theta$. The objective is to find the parameter vector $\\theta$ that minimizes the empirical mean squared error (MSE), which is proportional to the squared Euclidean norm of the residual vector $r = y - \\hat{y}$:\n$$\nE = \\frac{1}{N} \\sum_{j=1}^N (h(x_j) - y_j)^2 = \\frac{1}{N} \\| \\Phi\\theta - y \\|_2^2\n$$\nThis is a standard linear least-squares problem. The optimal parameter vector $\\hat{\\theta}$ that minimizes this error is given by the solution to the normal equations:\n$$\n(\\Phi^T \\Phi) \\hat{\\theta} = \\Phi^T y\n$$\nNumerically, it is more stable to solve for $\\hat{\\theta}$ using methods like QR decomposition or Singular Value Decomposition (SVD), which are implemented in standard numerical libraries. Once $\\hat{\\theta}$ is found, the minimum MSE is calculated as $E = \\frac{1}{N} \\| \\Phi\\hat{\\theta} - y \\|_2^2$.\n\n**2. Analysis of the ReLU Model Family ($H_{\\mathrm{ReLU}}$)**\n\nThe ReLU model family is defined with the activation function $g_{\\mathrm{ReLU}}(z) = \\max(0, z)$. The hidden parameters are chosen as follows: width $m$, slopes $a_i$ as $m$ equally spaced values in $[-\\alpha, \\alpha]$ with $\\alpha=0.5$, and a shared bias $b_i^{\\mathrm{ReLU}} = 1.0$ for all units.\n\nA critical insight is the behavior of the pre-activation term $z_i(x) = a_i x + b_i^{\\mathrm{ReLU}}$ for inputs $x \\in [-1, 1]$. Since $|a_i| \\le 0.5$ and $|x| \\le 1$, the term $a_i x$ is bounded: $|a_i x| \\le 0.5$. The pre-activation is therefore bounded as:\n$$\nz_i(x) = a_i x + 1.0 \\ge -0.5 + 1.0 = 0.5\n$$\nSince $z_i(x) \\ge 0.5$ for all $i$ and all $x$ in the domain, the ReLU activation function always operates in its linear region, i.e., $g_{\\mathrm{ReLU}}(z_i(x)) = z_i(x) = a_i x + 1.0$.\n\nConsequently, the model function $h(x) \\in H_{\\mathrm{ReLU}}$ simplifies to:\n$$\nh(x) = c + \\sum_{i=1}^m w_i (a_i x + 1.0) = \\left(c + \\sum_{i=1}^m w_i\\right) + \\left(\\sum_{i=1}^m w_i a_i\\right) x\n$$\nThis is an affine function of the form $C_0 + C_1 x$. This means that despite being formulated as a neural network, this specific construction of the ReLU model can only represent linear functions. Its ability to approximate a non-linear target function like $f^\\ast(x) = \\sin(\\omega x)$ is fundamentally limited to the best possible linear approximation. For $m \\ge 2$, the hypothesis space does not expand with increasing width $m$, as the basis functions $\\{1, a_1 x+1, \\ldots, a_m x+1\\}$ all lie within the span of $\\{1, x\\}$.\n\n**3. Analysis of the Tanh Model Family ($H_{\\tanh}$)**\n\nThe Tanh model family uses the activation $g_{\\tanh}(z) = \\tanh(z)$. The hidden parameters are: width $m$, slopes $a_i$ as $m$ equally spaced values in $[-A, A]$ with $A = \\max(1.0, 0.7\\,\\omega)$, and biases $b_i^{\\tanh} = 0$.\n\nThe model function is $h(x) = c + \\sum_{i=1}^m w_i \\tanh(a_i x)$. Unlike the ReLU case, the $\\tanh$ function is non-linear across its entire domain (except at $z=0$). The basis functions $\\phi_i(x) = \\tanh(a_i x)$ are smooth, sigmoidal functions. The range of slopes $[-A, A]$ is adapted to the frequency $\\omega$ of the target function, ensuring that the basis functions have varying degrees of steepness appropriate for capturing the oscillations of $\\sin(\\omega x)$. A linear combination of these non-linear basis functions can form a rich class of smooth, non-linear approximations, which is far more powerful than the affine class of $H_{\\mathrm{ReLU}}$.\n\n**4. Computational Algorithm**\n\nFor each parameter set $(\\omega, m)$ provided in the test suite, we execute the following steps:\n1.  **Generate Dataset**: Create a vector of $N=1001$ input points $x$ uniformly spaced in $[-1, 1]$ and the corresponding target vector $y = \\sin(\\omega x)$.\n2.  **Compute $E_{\\mathrm{ReLU}}$**:\n    -   Determine the ReLU slopes $a_i$ and bias $b_i^{\\mathrm{ReLU}}$.\n    -   Construct the $N \\times (m+1)$ feature matrix $\\Phi_{\\mathrm{ReLU}}$ with columns $[1, g_{\\mathrm{ReLU}}(a_1 x + b_1^{\\mathrm{ReLU}}), \\ldots, g_{\\mathrm{ReLU}}(a_m x + b_m^{\\mathrm{ReLU}})]$.\n    -   Solve the least-squares problem for $\\hat{\\theta}_{\\mathrm{ReLU}} = \\arg\\min_{\\theta} \\|\\Phi_{\\mathrm{ReLU}}\\theta - y\\|_2^2$.\n    -   Calculate the error $E_{\\mathrm{ReLU}} = \\frac{1}{N} \\| \\Phi_{\\mathrm{ReLU}}\\hat{\\theta}_{\\mathrm{ReLU}} - y \\|_2^2$.\n3.  **Compute $E_{\\tanh}$**:\n    -   Determine the Tanh slopes $a_i$ (based on $\\omega$) and bias $b_i^{\\tanh}$.\n    -   Construct the $N \\times (m+1)$ feature matrix $\\Phi_{\\tanh}$ with columns $[1, g_{\\tanh}(a_1 x + b_1^{\\tanh}), \\ldots, g_{\\tanh}(a_m x + b_m^{\\tanh})]$.\n    -   Solve for $\\hat{\\theta}_{\\tanh} = \\arg\\min_{\\theta} \\|\\Phi_{\\tanh}\\theta - y\\|_2^2$.\n    -   Calculate the error $E_{\\tanh} = \\frac{1}{N} \\| \\Phi_{\\tanh}\\hat{\\theta}_{\\tanh} - y \\|_2^2$.\n4.  **Store Results**: The triplet $(E_{\\mathrm{ReLU}}, E_{\\tanh}, E_{\\mathrm{ReLU}} - E_{\\tanh})$ is computed and stored.\n\nThis procedure is repeated for all test cases, and the final results are formatted into a single string as specified. The expected outcome is that for any non-trivial target ($\\omega > 0$), $E_{\\mathrm{ReLU}}$ will be significantly larger than $E_{\\tanh}$, demonstrating the critical role of non-linearity in the activation function for approximating complex functions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the approximation errors of two single-hidden-layer\n    neural network models (ReLU and Tanh) on a sinusoidal target function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 5),\n        (3.0, 5),\n        (9.0, 5),\n        (9.0, 2),\n        (15.0, 10),\n    ]\n\n    all_results = []\n    \n    # Constants\n    N = 1001\n\n    for omega, m in test_cases:\n        # 1. Construct the dataset\n        x = np.linspace(-1.0, 1.0, N)\n        y = np.sin(omega * x)\n\n        # 2. ReLU Model Family Calculation\n        alpha = 0.5\n        a_relu = np.linspace(-alpha, alpha, m)\n        b_relu = 1.0\n\n        # Construct ReLU feature matrix Phi_relu\n        Phi_relu = np.ones((N, m + 1))\n        # Vectorized computation of hidden unit outputs\n        # x is (N,), a_relu is (m,). We need an (N, m) matrix.\n        # x[:, np.newaxis] is (N, 1), a_relu[np.newaxis, :] is (1, m)\n        # Broadcasting (N, 1) * (1, m) -> (N, m)\n        pre_acts_relu = x[:, np.newaxis] * a_relu[np.newaxis, :] + b_relu\n        # Per problem statement logic, pre_acts_relu is always > 0,\n        # so np.maximum(0, a) is equivalent to a. We use the formal definition.\n        Phi_relu[:, 1:] = np.maximum(0, pre_acts_relu)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_relu, _, _, _ = np.linalg.lstsq(Phi_relu, y, rcond=None)\n            y_pred_relu = Phi_relu @ theta_relu\n            E_relu = np.mean((y - y_pred_relu)**2)\n        except np.linalg.LinAlgError:\n            # Should not happen with np.linalg.lstsq, but as a safeguard\n            E_relu = np.nan\n\n        # 3. Tanh Model Family Calculation\n        A = max(1.0, 0.7 * omega)\n        a_tanh = np.linspace(-A, A, m)\n        b_tanh = 0.0\n\n        # Construct Tanh feature matrix Phi_tanh\n        Phi_tanh = np.ones((N, m + 1))\n        pre_acts_tanh = x[:, np.newaxis] * a_tanh[np.newaxis, :] + b_tanh\n        Phi_tanh[:, 1:] = np.tanh(pre_acts_tanh)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_tanh, _, _, _ = np.linalg.lstsq(Phi_tanh, y, rcond=None)\n            y_pred_tanh = Phi_tanh @ theta_tanh\n            E_tanh = np.mean((y - y_pred_tanh)**2)\n        except np.linalg.LinAlgError:\n            E_tanh = np.nan\n            \n        # 4. Difference\n        diff = E_relu - E_tanh\n        \n        all_results.append((E_relu, E_tanh, diff))\n\n    # 5. Format the final output string\n    output_parts = []\n    for e_relu, e_tanh, d in all_results:\n        part = f\"[{e_relu:.6f},{e_tanh:.6f},{d:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3171992"}, {"introduction": "Beyond enabling function approximation, the properties of an activation function's derivative are critical for successful training via gradient-based optimization. This practice explores the infamous \"dying ReLU\" problem, where neurons can become permanently inactive, halting learning. Using a simplified probabilistic model where pre-activations follow a standard normal distribution, you will calculate the likelihood of a neuron \"dying\" and explore how a simple modification—the Leaky ReLU—mitigates this issue by ensuring a non-zero gradient is always present [@problem_id:3171941].", "problem": "Consider the Rectified Linear Unit (ReLU) activation function and its leaky variant. Let the Rectified Linear Unit (ReLU) be defined by $f(z) = \\max(0, z)$ and the Leaky Rectified Linear Unit (Leaky ReLU) be defined by $f_{\\alpha}(z) = \\max(\\alpha z, z)$, where $\\alpha$ is the negative-side slope with $0 \\le \\alpha \\le 1$. The derivative $f'(z)$ for the Rectified Linear Unit (ReLU) is $1$ when $z > 0$ and $0$ when $z  0$, with the derivative at $z = 0$ considered undefined but occurring with probability $0$ under a continuous distribution. Similarly, the derivative of the Leaky Rectified Linear Unit (Leaky ReLU) is $1$ when $z \\ge 0$ and $\\alpha$ when $z  0$. Assume the pre-activation $z$ is a random variable distributed as the Standard Normal Distribution (SND), $z \\sim \\mathcal{N}(0,1)$, at every training step.\n\nStarting from fundamental definitions of derivatives, probability under the Standard Normal Distribution (SND), and the activation functions specified above, derive the following quantities:\n\n1. The probability $P(f'(z) = 0)$ for the Rectified Linear Unit (ReLU) under $z \\sim \\mathcal{N}(0,1)$.\n2. The probability $P(f'(z) = 0)$ for the Leaky Rectified Linear Unit (Leaky ReLU) with a given $\\alpha$ under $z \\sim \\mathcal{N}(0,1)$.\n3. An $\\alpha$ threshold that guarantees avoidance of neuron inactivity by imposing a lower bound on the expected magnitude of the derivative. Specifically, select the minimal $\\alpha \\in [0,1]$ such that the expected absolute derivative $\\mathbb{E}[|f'_{\\alpha}(z)|]$ under $z \\sim \\mathcal{N}(0,1)$ satisfies $\\mathbb{E}[|f'_{\\alpha}(z)|] \\ge \\gamma$, and simultaneously enforces strictly positive slope $\\alpha \\ge \\varepsilon$ to make $P(f'(z) = 0)$ zero under $z \\sim \\mathcal{N}(0,1)$. Your task is to compute $\\alpha_{\\text{threshold}} = \\max(2\\gamma - 1, \\varepsilon)$, clipped to the interval $[0,1]$, based on the fundamental definitions and properties of the Standard Normal Distribution (SND) and the activation functions.\n\nFor scientific realism, assume the distribution $z \\sim \\mathcal{N}(0,1)$ remains unchanged across training steps. Express all probabilities as real numbers in $[0,1]$.\n\nTest Suite:\nProvide computations for the following parameter sets, each denoted by $(T, \\alpha, \\gamma, \\varepsilon)$, where $T$ is the number of training steps (used to show the time series across steps), $\\alpha$ is the given negative-side slope for the Leaky Rectified Linear Unit (Leaky ReLU), $\\gamma$ is the required lower bound, and $\\varepsilon$ is the minimal strictly positive slope:\n\n- Test case $1$: $(T=\\;3,\\;\\alpha=\\;0.0,\\;\\gamma=\\;0.5,\\;\\varepsilon=\\;0.001)$.\n- Test case $2$: $(T=\\;4,\\;\\alpha=\\;0.05,\\;\\gamma=\\;0.6,\\;\\varepsilon=\\;0.001)$.\n- Test case $3$: $(T=\\;2,\\;\\alpha=\\;0.0,\\;\\gamma=\\;1.0,\\;\\varepsilon=\\;0.01)$.\n\nRequired outputs per test case:\n- A list of $T$ probabilities $[P_1^{\\text{ReLU}}, P_2^{\\text{ReLU}}, \\dots, P_T^{\\text{ReLU}}]$, where $P_t^{\\text{ReLU}} = P(f'(z)=0)$ at training step $t$ for the Rectified Linear Unit (ReLU).\n- A list of $T$ probabilities $[P_1^{\\text{Leaky}}, P_2^{\\text{Leaky}}, \\dots, P_T^{\\text{Leaky}}]$, where $P_t^{\\text{Leaky}} = P(f'(z)=0)$ at training step $t$ for the Leaky Rectified Linear Unit (Leaky ReLU) with the given $\\alpha$.\n- The single value $\\alpha_{\\text{threshold}}$ computed from $\\gamma$ and $\\varepsilon$ using the principle described above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list structured as $[\\text{ReLU\\_series},\\;\\text{Leaky\\_series},\\;\\alpha_{\\text{threshold}}]$. For example, the overall output format should be $[[\\dots],[\\dots],[\\dots]]$ where each inner list holds the three required outputs for a test case.", "solution": "The problem requires the calculation of specific probabilistic quantities related to the ReLU and Leaky ReLU activation functions, under the simplifying assumption that the pre-activation input $z$ is drawn from a standard normal distribution, $z \\sim \\mathcal{N}(0,1)$. The problem is deemed valid as its premises are scientifically grounded in the fundamentals of probability theory and calculus, and it is well-posed, leading to a unique and meaningful solution. The definitions and constraints are self-contained and consistent. Although the assumption of a static distribution for $z$ across training steps is a simplification, and the presentation of the formula for $\\alpha_{\\text{threshold}}$ as a derivation task is slightly circular, these aspects do not invalidate the core mathematical exercise. We will proceed with a rigorous, step-by-step derivation of the required quantities.\n\nThe analysis is structured as follows:\n1.  Calculation of the probability of a zero-gradient event for the standard Rectified Linear Unit (ReLU).\n2.  Calculation of the same probability for the Leaky Rectified Linear Unit (Leaky ReLU).\n3.  Derivation and computation of the threshold parameter $\\alpha_{\\text{threshold}}$.\n4.  Application of these results to the specific test cases provided.\n\n### 1. Probability of Zero Gradient for ReLU\n\nThe Rectified Linear Unit (ReLU) function is defined as $f(z) = \\max(0, z)$. Its derivative, for the purpose of this analysis, is given by:\n$$\nf'(z) = \\begin{cases} \n1  \\text{if } z > 0 \\\\\n0  \\text{if } z  0 \n\\end{cases}\n$$\nThe derivative at $z=0$ is undefined, but for a continuous random variable like $z \\sim \\mathcal{N}(0,1)$, the probability of any single point is zero, i.e., $P(z=0) = 0$. Thus, this point of non-differentiability does not affect our probability calculations.\n\nWe need to find the probability $P(f'(z) = 0)$. From the definition of the derivative, the event $f'(z) = 0$ is equivalent to the event $z  0$. Therefore, we must compute $P(z  0)$ where $z$ follows a standard normal distribution.\n\nThe standard normal distribution $\\mathcal{N}(0,1)$ has a probability density function (PDF) that is symmetric about the mean $z=0$. Its cumulative distribution function (CDF) is typically denoted by $\\Phi(z) = P(Z \\le z)$. By symmetry, the probability of the random variable being less than the mean is exactly $0.5$.\n$$\nP(f'(z) = 0) = P(z  0) = \\Phi(0) = 0.5\n$$\nThe problem states that the distribution of $z$ remains unchanged across training steps. Consequently, this probability is constant for all training steps $t=1, \\dots, T$. For a given $T$, the series of probabilities is a list of $T$ identical values of $0.5$.\n\n### 2. Probability of Zero Gradient for Leaky ReLU\n\nThe Leaky ReLU function is defined as $f_{\\alpha}(z) = \\max(\\alpha z, z)$ for a slope parameter $0 \\le \\alpha \\le 1$. Its derivative is given as:\n$$\nf'_{\\alpha}(z) = \\begin{cases} \n1  \\text{if } z \\ge 0 \\\\\n\\alpha  \\text{if } z  0 \n\\end{cases}\n$$\nWe seek the probability $P(f'_{\\alpha}(z) = 0)$. The derivative $f'_{\\alpha}(z)$ can take only two values: $1$ and $\\alpha$. The event $f'_{\\alpha}(z) = 0$ is possible if and only if one of these values is $0$. Since the problem specifies $0 \\le \\alpha \\le 1$, we consider two cases for the given parameter $\\alpha$:\n\n-   **Case 1: $\\alpha > 0$**. In this case, the possible values for the derivative are $1$ and $\\alpha$, both of which are strictly positive. Thus, the derivative can never be zero. The event $f'_{\\alpha}(z) = 0$ is an impossible event, and its probability is $0$.\n    $$\n    P(f'_{\\alpha}(z) = 0) = 0 \\quad \\text{if } \\alpha > 0\n    $$\n-   **Case 2: $\\alpha = 0$**. In this case, the Leaky ReLU function becomes the standard ReLU function, $f_{0}(z) = \\max(0, z) = f(z)$. The derivative becomes $0$ for $z  0$. As calculated in the previous section, the probability of this event is $0.5$.\n    $$\n    P(f'_{0}(z) = 0) = P(z  0) = 0.5 \\quad \\text{if } \\alpha = 0\n    $$\nIn summary, $P(f'_{\\alpha}(z)=0)$ is $0.5$ if $\\alpha=0$ and $0$ if $\\alpha>0$. As before, this probability is constant across all training steps $T$.\n\n### 3. Derivation of the Threshold $\\alpha_{\\text{threshold}}$\n\nThe problem requires finding the minimal $\\alpha \\in [0,1]$ that satisfies two conditions:\n1.  The expected absolute derivative meets a lower bound: $\\mathbb{E}[|f'_{\\alpha}(z)|] \\ge \\gamma$.\n2.  The slope $\\alpha$ is strictly positive by a margin: $\\alpha \\ge \\varepsilon$.\n\nFirst, we derive the expression for the expectation $\\mathbb{E}[|f'_{\\alpha}(z)|]$. The absolute value of the derivative, $|f'_{\\alpha}(z)|$, is a random variable determined by $z$.\nSince $0 \\le \\alpha \\le 1$, we have $|\\alpha| = \\alpha$.\n$$\n|f'_{\\alpha}(z)| = \\begin{cases} \n|1| = 1  \\text{if } z \\ge 0 \\\\\n|\\alpha| = \\alpha  \\text{if } z  0 \n\\end{cases}\n$$\nThe expectation of this discrete random variable is the sum of its possible values weighted by their respective probabilities:\n$$\n\\mathbb{E}[|f'_{\\alpha}(z)|] = 1 \\cdot P(z \\ge 0) + \\alpha \\cdot P(z  0)\n$$\nFor $z \\sim \\mathcal{N}(0,1)$, we have $P(z \\ge 0) = 0.5$ and $P(z  0) = 0.5$. Substituting these values:\n$$\n\\mathbb{E}[|f'_{\\alpha}(z)|] = 1 \\cdot (0.5) + \\alpha \\cdot (0.5) = 0.5(1 + \\alpha)\n$$\nNow, we apply the first condition, $\\mathbb{E}[|f'_{\\alpha}(z)|] \\ge \\gamma$:\n$$\n0.5(1 + \\alpha) \\ge \\gamma \\implies 1 + \\alpha \\ge 2\\gamma \\implies \\alpha \\ge 2\\gamma - 1\n$$\nThis derivation confirms the expression provided in the problem statement. To satisfy both conditions simultaneously, $\\alpha$ must be greater than or equal to both $2\\gamma - 1$ and $\\varepsilon$. The minimal value of $\\alpha$ that satisfies this is the maximum of the two lower bounds:\n$$\n\\alpha \\ge \\max(2\\gamma - 1, \\varepsilon)\n$$\nFinally, the problem requires that $\\alpha_{\\text{threshold}}$ be within the valid range for the Leaky ReLU slope, which is $[0, 1]$. Therefore, we must clip the computed value to this interval.\n$$\n\\alpha_{\\text{threshold}} = \\min(1.0, \\max(0.0, \\max(2\\gamma - 1, \\varepsilon)))\n$$\nSince $\\max(0.0, \\cdot)$ is applied, the expression simplifies to:\n$$\n\\alpha_{\\text{threshold}} = \\min(1.0, \\max(2\\gamma - 1, \\varepsilon))\n$$\nThis formula will be used for the test cases.\n\n### 4. Application to Test Cases\n\nWe now compute the required outputs for each test case.\n\n**Test Case 1:** $(T=3, \\alpha=0.0, \\gamma=0.5, \\varepsilon=0.001)$\n-   **ReLU Series**: $P(f'(z) = 0) = 0.5$. The list of $T=3$ probabilities is $[0.5, 0.5, 0.5]$.\n-   **Leaky ReLU Series**: The given slope is $\\alpha = 0.0$. Thus, $P(f'_{\\alpha}(z) = 0) = 0.5$. The list of $T=3$ probabilities is $[0.5, 0.5, 0.5]$.\n-   **$\\alpha_{\\text{threshold}}$**: We compute $\\max(2\\gamma - 1, \\varepsilon) = \\max(2(0.5) - 1, 0.001) = \\max(0, 0.001) = 0.001$. Clipping this to $[0,1]$ yields $\\alpha_{\\text{threshold}} = 0.001$.\n\n**Test Case 2:** $(T=4, \\alpha=0.05, \\gamma=0.6, \\varepsilon=0.001)$\n-   **ReLU Series**: $P(f'(z) = 0) = 0.5$. The list of $T=4$ probabilities is $[0.5, 0.5, 0.5, 0.5]$.\n-   **Leaky ReLU Series**: The given slope is $\\alpha = 0.05 > 0$. Thus, $P(f'_{\\alpha}(z) = 0) = 0$. The list of $T=4$ probabilities is $[0.0, 0.0, 0.0, 0.0]$.\n-   **$\\alpha_{\\text{threshold}}$**: We compute $\\max(2\\gamma - 1, \\varepsilon) = \\max(2(0.6) - 1, 0.001) = \\max(0.2, 0.001) = 0.2$. Clipping this to $[0,1]$ yields $\\alpha_{\\text{threshold}} = 0.2$.\n\n**Test Case 3:** $(T=2, \\alpha=0.0, \\gamma=1.0, \\varepsilon=0.01)$\n-   **ReLU Series**: $P(f'(z) = 0) = 0.5$. The list of $T=2$ probabilities is $[0.5, 0.5]$.\n-   **Leaky ReLU Series**: The given slope is $\\alpha = 0.0$. Thus, $P(f'_{\\alpha}(z) = 0) = 0.5$. The list of $T=2$ probabilities is $[0.5, 0.5]$.\n-   **$\\alpha_{\\text{threshold}}$**: We compute $\\max(2\\gamma - 1, \\varepsilon) = \\max(2(1.0) - 1, 0.01) = \\max(1.0, 0.01) = 1.0$. Clipping this to $[0,1]$ yields $\\alpha_{\\text{threshold}} = 1.0$.\n\nThese calculations are implemented in the following program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating probabilities and thresholds for\n    ReLU and Leaky ReLU activation functions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, alpha, gamma, epsilon).\n    test_cases = [\n        (3, 0.0, 0.5, 0.001),\n        (4, 0.05, 0.6, 0.001),\n        (2, 0.0, 1.0, 0.01),\n    ]\n\n    all_results = []\n    \n    for T, alpha, gamma, epsilon in test_cases:\n        # Part 1: Calculate the series for ReLU.\n        # The probability of the derivative being zero for ReLU is P(z  0), where\n        # z ~ N(0,1). This probability is 0.5. Since the distribution is static\n        # over training steps T, the probability is constant.\n        prob_relu = 0.5\n        relu_series = [prob_relu] * T\n\n        # Part 2: Calculate the series for Leaky ReLU.\n        # The derivative is alpha for z  0 and 1 for z = 0.\n        # The derivative is zero if and only if alpha is zero.\n        # If alpha = 0, the probability is P(z  0) = 0.5.\n        # If alpha  0, the probability is 0.\n        prob_leaky = 0.5 if alpha == 0.0 else 0.0\n        leaky_series = [prob_leaky] * T\n\n        # Part 3: Calculate alpha_threshold.\n        # Derivation shows alpha_threshold = min(1, max(0, max(2*gamma - 1, epsilon))).\n        # The equation from the problem simplifies this, as `max` with `epsilon` (=0)\n        # handles the lower bound of 0.\n        # alpha_threshold = max(2*gamma - 1, epsilon), clipped to the interval [0,1].\n        val = max(2 * gamma - 1, epsilon)\n        alpha_threshold = min(1.0, max(0.0, val)) # Clipping ensures value is in [0,1]\n        \n        case_result = [relu_series, leaky_series, alpha_threshold]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, [[series1, series2, num1], [seriesA, seriesB, numA], ...]\n    # We construct the string representation manually to ensure no unwanted spaces.\n    result_strings = []\n    for res in all_results:\n        # res = [relu_series, leaky_series, alpha_threshold]\n        s_relu = f\"[{','.join(map(str, res[0]))}]\"\n        s_leaky = f\"[{','.join(map(str, res[1]))}]\"\n        s_alpha = str(res[2])\n        result_strings.append(f\"[{s_relu},{s_leaky},{s_alpha}]\")\n\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3171941"}, {"introduction": "In deep networks, the challenge of maintaining a stable flow of information across many layers becomes paramount. This advanced exercise delves into the theory of signal propagation, analyzing how the statistical properties of activations evolve from one layer to the next. By deriving and comparing the mean-variance propagation maps for `tanh`, `ReLU`, and `SELU` activations, you will discover why some functions can lead to vanishing signals while others, like SELU, are specifically engineered to be \"self-normalizing.\" This practice illuminates the sophisticated design principles required for building and initializing truly deep neural networks [@problem_id:3171987].", "problem": "A fully connected neural layer is defined by the preactivation $z = W x + b$ and postactivation $a = f(z)$, where $x \\in \\mathbb{R}^{n}$ is the input vector to the layer, $W \\in \\mathbb{R}^{m \\times n}$ is the weight matrix, and $b \\in \\mathbb{R}^{m}$ is the bias vector. Assume the following statistical model for forward signal propagation:\n\n1. The entries of $W$ are independent and identically distributed (i.i.d.) Gaussian random variables with $W_{ji} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^{2}}{n}\\right)$.\n2. The components $\\{x_{i}\\}_{i=1}^{n}$ are i.i.d. and independent of $W$, each with mean $\\mu$ and variance $\\sigma^{2}$.\n3. The bias is set to $b = 0$.\n\nConsider three activation functions: the hyperbolic tangent $f(z) = \\tanh(z)$, the Rectified Linear Unit (ReLU) $f(z) = \\max\\{0, z\\}$, and the Scaled Exponential Linear Unit (SELU),\n$$\nf(z) =\n\\begin{cases}\n\\lambda z,  z > 0, \\\\\n\\lambda \\alpha \\left(\\exp(z) - 1\\right),  z \\leq 0,\n\\end{cases}\n$$\nwith $\\lambda = 1.050700987355480$ and $\\alpha = 1.673263242354377$.\n\nStarting from the core definitions of expectation and variance, the independence assumptions, and the Gaussian approximation implied by the Central Limit Theorem for large $n$, derive the mean/variance propagation map $(\\mu, \\sigma^{2}) \\mapsto (\\mu', \\sigma'^{2})$ across the layer for a general activation $f$ by expressing $\\mu' = \\mathbb{E}[f(Z)]$ and $\\sigma'^{2} = \\mathrm{Var}[f(Z)]$ in terms of the Gaussian law of $Z$. Specialize your derivation for each activation $f = \\tanh$, $f = \\mathrm{ReLU}$, and $f = \\mathrm{SELU}$.\n\nThen, restrict to the zero-mean case $\\mu = 0$, gain $g = 1$, and $b = 0$, so that the preactivation is approximately Gaussian with zero mean and variance equal to the input variance. Under this restriction, determine the fixed-point variances $\\sigma_{\\star}^{2}$ for each activation by solving $\\sigma'^{2} = \\sigma^{2}$ and comparing the solutions in the three cases. Report your final answer as the row matrix of the three fixed-point variances, in the order $(\\tanh, \\mathrm{ReLU}, \\mathrm{SELU})$. If any numerical evaluation is required, use the provided values of $\\lambda$ and $\\alpha$ exactly and the standard normal cumulative distribution function $\\Phi(\\cdot)$ and probability density function $\\varphi(\\cdot)$ to express results analytically without rounding. The final answer must be a single row matrix containing the three fixed-point variances, with no units.", "solution": "The problem asks for the derivation of the mean/variance propagation map for a fully connected neural layer under specific statistical assumptions and for the determination of the fixed-point variances for three activation functions: hyperbolic tangent ($\\tanh$), Rectified Linear Unit (ReLU), and Scaled Exponential Linear Unit (SELU).\n\nFirst, we characterize the distribution of the preactivation $z_j$ for a neuron $j$ in the layer. The preactivation is given by $z_j = \\sum_{i=1}^{n} W_{ji} x_i$, as the bias $b$ is set to $0$. The input components $x_i$ are i.i.d. with mean $\\mathbb{E}[x_i] = \\mu$ and variance $\\mathrm{Var}[x_i] = \\sigma^2$. The weights $W_{ji}$ are i.i.d. Gaussian random variables, $W_{ji} \\sim \\mathcal{N}(0, g^2/n)$, and are independent of the inputs $x_i$.\n\nThe mean of the preactivation $z_j$ is:\n$$\n\\mathbb{E}[z_j] = \\mathbb{E}\\left[\\sum_{i=1}^{n} W_{ji} x_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[W_{ji} x_i]\n$$\nDue to the independence of $W_{ji}$ and $x_i$, we have $\\mathbb{E}[W_{ji} x_i] = \\mathbb{E}[W_{ji}] \\mathbb{E}[x_i]$.\n$$\n\\mathbb{E}[z_j] = \\sum_{i=1}^{n} (0) \\cdot \\mu = 0\n$$\nThe mean of the preactivation is always $0$.\n\nThe variance of the preactivation $z_j$ is:\n$$\n\\mathrm{Var}[z_j] = \\mathrm{Var}\\left[\\sum_{i=1}^{n} W_{ji} x_i\\right]\n$$\nSince the terms $W_{ji} x_i$ are independent for different indices $i$, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}[z_j] = \\sum_{i=1}^{n} \\mathrm{Var}[W_{ji} x_i]\n$$\nFor two independent random variables $A$ and $B$, $\\mathrm{Var}[AB] = \\mathbb{E}[A^2]\\mathbb{E}[B^2] - (\\mathbb{E}[A]\\mathbb{E}[B])^2$.\nWe have $\\mathbb{E}[W_{ji}] = 0$ and $\\mathrm{Var}[W_{ji}] = g^2/n$, so $\\mathbb{E}[W_{ji}^2] = \\mathrm{Var}[W_{ji}] + (\\mathbb{E}[W_{ji}])^2 = g^2/n$.\nFor the input, $\\mathbb{E}[x_i] = \\mu$ and $\\mathrm{Var}[x_i] = \\sigma^2$, so $\\mathbb{E}[x_i^2] = \\mathrm{Var}[x_i] + (\\mathbb{E}[x_i])^2 = \\sigma^2 + \\mu^2$.\nThus, the variance of a single term is:\n$$\n\\mathrm{Var}[W_{ji} x_i] = \\left(\\frac{g^2}{n}\\right)(\\sigma^2 + \\mu^2) - (0 \\cdot \\mu)^2 = \\frac{g^2}{n}(\\sigma^2 + \\mu^2)\n$$\nSumming over the $n$ terms, we get the total variance of the preactivation:\n$$\n\\sigma_z^2 = \\mathrm{Var}[z_j] = \\sum_{i=1}^{n} \\frac{g^2}{n}(\\sigma^2 + \\mu^2) = n \\cdot \\frac{g^2}{n}(\\sigma^2 + \\mu^2) = g^2(\\sigma^2 + \\mu^2)\n$$\nFor large $n$, by the Central Limit Theorem, the preactivation $z_j$ is approximately Gaussian. We denote this random variable by $Z$:\n$$\nZ \\sim \\mathcal{N}(0, g^2(\\sigma^2 + \\mu^2))\n$$\nThe output of the activation function is $a = f(Z)$. The mean and variance of the output activations are $\\mu' = \\mathbb{E}[a]$ and $\\sigma'^2 = \\mathrm{Var}[a]$. We are asked to find the fixed-point variances $\\sigma_\\star^2$ by solving $\\sigma'^2 = \\sigma^2$ under the specific conditions $\\mu=0$ and $g=1$.\n\nUnder these restrictions, the preactivation distribution simplifies to:\n$$\nZ \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe variance propagation map is $\\sigma^2 \\mapsto \\sigma'^2 = \\mathrm{Var}[f(Z)]$. We now solve $\\sigma'^2 = \\sigma^2$ for each activation function.\n\n**1. Hyperbolic Tangent: $f(z) = \\tanh(z)$**\n\nSince $f(z) = \\tanh(z)$ is an odd function and the distribution of $Z \\sim \\mathcal{N}(0, \\sigma^2)$ is symmetric about $0$, the mean of the output is $\\mu' = \\mathbb{E}[\\tanh(Z)] = 0$.\nThe variance of the output is $\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)] - (\\mu')^2 = \\mathbb{E}[\\tanh^2(Z)]$.\nThe fixed-point equation is $\\sigma^2 = \\mathbb{E}[\\tanh^2(Z)]$.\nA trivial solution is $\\sigma^2 = 0$. If $\\sigma^2 = 0$, then $Z \\equiv 0$, so $\\tanh(Z) \\equiv 0$, and $\\sigma'^2 = 0$.\nFor any $\\sigma^2 > 0$, we have $|\\tanh(z)|  1$ for any finite $z$, which implies $\\tanh^2(z)  1$. Thus, $\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)]  \\mathbb{E}[1] = 1$. This means any non-zero fixed point must satisfy $\\sigma^2  1$.\nTo analyze the behavior near the origin, we use the Taylor expansion of $\\tanh(z)$ around $z=0$: $\\tanh(z) = z - \\frac{z^3}{3} + O(z^5)$.\nThen $\\tanh^2(z) = \\left(z - \\frac{z^3}{3} + \\dots\\right)^2 = z^2 - \\frac{2z^4}{3} + O(z^6)$.\nTaking the expectation with respect to $Z \\sim \\mathcal{N}(0, \\sigma^2)$:\n$$\n\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)] \\approx \\mathbb{E}[Z^2] - \\frac{2}{3}\\mathbb{E}[Z^4]\n$$\nFor a zero-mean Gaussian, $\\mathbb{E}[Z^2] = \\sigma^2$ and $\\mathbb{E}[Z^4] = 3\\sigma^4$.\n$$\n\\sigma'^2 \\approx \\sigma^2 - \\frac{2}{3}(3\\sigma^4) = \\sigma^2 - 2\\sigma^4\n$$\nThe fixed-point equation $\\sigma^2 = \\sigma^2 - 2\\sigma^4$ yields $2\\sigma^4 = 0$, whose only real solution is $\\sigma=0$. For any small $\\sigma^2 > 0$, we have $\\sigma'^2  \\sigma^2$, indicating that the mapping is contractive towards the origin. Thus, the only fixed point is $\\sigma_{\\star, \\tanh}^2 = 0$.\n\n**2. Rectified Linear Unit: $f(z) = \\max\\{0, z\\}$**\n\nFor $Z \\sim \\mathcal{N}(0, \\sigma^2)$ with PDF $p(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{z^2}{2\\sigma^2})$, we first compute the output mean $\\mu'$.\n$$\n\\mu' = \\mathbb{E}[\\max\\{0, Z\\}] = \\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right) dz = \\left[-\\frac{\\sigma^2}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)\\right]_{0}^{\\infty} = \\frac{\\sigma}{\\sqrt{2\\pi}}\n$$\nNext, we compute the second moment of the output.\n$$\n\\mathbb{E}[f(Z)^2] = \\mathbb{E}[\\max\\{0, Z\\}^2] = \\int_{0}^{\\infty} z^2 p(z) dz = \\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[Z^2] = \\frac{\\sigma^2}{2}\n$$\nThe output variance is:\n$$\n\\sigma'^2 = \\mathbb{E}[f(Z)^2] - (\\mu')^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\frac{\\sigma^2}{2} - \\frac{\\sigma^2}{2\\pi} = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\n$$\nThe fixed-point equation is $\\sigma^2 = \\sigma'^2$:\n$$\n\\sigma^2 = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\n$$\nThis equation can be rewritten as $\\sigma^2\\left(1 - \\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\\right) = 0$, or $\\sigma^2\\left(\\frac{1}{2} + \\frac{1}{2\\pi}\\right)=0$. Since the term in the parentheses is non-zero, the only solution is $\\sigma^2=0$.\nThus, the fixed-point variance is $\\sigma_{\\star, \\mathrm{ReLU}}^2 = 0$.\n\n**3. Scaled Exponential Linear Unit (SELU)**\n\nThe SELU activation function is given by:\n$$\nf(z) =\n\\begin{cases}\n\\lambda z,  z > 0, \\\\\n\\lambda \\alpha \\left(\\exp(z) - 1\\right),  z \\leq 0,\n\\end{cases}\n$$\nwith specific constants $\\lambda = 1.050700987355480$ and $\\alpha = 1.673263242354377$. These constants are not arbitrary; they are precisely chosen to ensure that the activation function yields self-normalizing properties. Specifically, if the input to a layer consists of activations with mean $0$ and variance $1$, the output activations will also have a mean of approximately $0$ and a variance of approximately $1$.\nIn our framework, this means that if the input to the activation function $Z$ is a standard normal variable, $Z \\sim \\mathcal{N}(0, 1)$, then the output $a=f(Z)$ will have $\\mathbb{E}[a] = 0$ and $\\mathrm{Var}[a]=1$.\n\nLet's test this in our fixed-point problem. We are looking for $\\sigma^2$ such that if $Z \\sim \\mathcal{N}(0, \\sigma^2)$, then $\\sigma'^2 = \\mathrm{Var}[f(Z)] = \\sigma^2$.\nConsider the case where the input variance is $\\sigma^2=1$. The preactivation is then $Z \\sim \\mathcal{N}(0, 1)$.\nBy the design property of SELU, the output activations $a = f(Z)$ will have mean $\\mu' = \\mathbb{E}[f(Z)] = 0$ and variance $\\sigma'^2 = \\mathrm{Var}[f(Z)] = 1$.\nSince the input variance $\\sigma^2=1$ results in an output variance $\\sigma'^2=1$, the condition $\\sigma'^2=\\sigma^2$ is met.\nTherefore, $\\sigma^2=1$ is a fixed point of the variance propagation map for SELU. This non-trivial fixed point is crucial for maintaining a stable signal flow in deep networks.\nThus, the fixed-point variance is $\\sigma_{\\star, \\mathrm{SELU}}^2 = 1$.\n\nIn summary, the fixed-point variances for the three activation functions under the given conditions are:\n- For tanh: $\\sigma_{\\star}^2 = 0$\n- For ReLU: $\\sigma_{\\star}^2 = 0$\n- For SELU: $\\sigma_{\\star}^2 = 1$\n\nThe final answer is the row matrix of these three values in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  0  1\n\\end{pmatrix}\n}\n$$", "id": "3171987"}]}