{"hands_on_practices": [{"introduction": "Understanding how an activation function transforms the statistical properties of signals is key to designing stable deep networks. This practice provides a first-principles derivation of the output mean and variance for ReLU, ELU, and SELU, revealing why variants were developed to control activation statistics. By working through these calculations, you will gain a foundational insight into the motivation behind concepts like self-normalizing networks. [@problem_id:3197614]", "problem": "Let a scalar preactivation $x$ be distributed as a normal random variable $x \\sim \\mathcal{N}(0,\\sigma^{2})$. Consider three activation functions commonly used in deep learning: the Rectified Linear Unit (ReLU), the Exponential Linear Unit (ELU), and the Scaled Exponential Linear Unit (SELU), defined by\n$$\n\\mathrm{ReLU}(x) = \\max\\{0,x\\},\n$$\n$$\n\\mathrm{ELU}(x) = \n\\begin{cases}\nx,  x  0,\\\\\n\\alpha\\left(\\exp(x) - 1\\right),  x \\leq 0,\n\\end{cases}\n\\quad \\text{with } \\alpha  0,\n$$\n$$\n\\mathrm{SELU}(x) = \\lambda \\cdot \\mathrm{ELU}(x) \\quad \\text{with } \\lambda  0 \\text{ and } \\alpha  0.\n$$\nStarting only from the definitions of expectation and variance for real-valued random variables and the probability density function of the normal distribution, derive closed-form expressions (in terms of $\\sigma$, $\\alpha$, $\\lambda$, and standard functions such as the standard normal cumulative distribution function) for the mean $\\mathbb{E}[\\mathrm{ReLU}(x)]$ and variance $\\mathrm{Var}[\\mathrm{ReLU}(x)]$. Then, derive the corresponding formulae for $\\mathbb{E}[\\mathrm{ELU}(x)]$ and $\\mathrm{Var}[\\mathrm{ELU}(x)]$, and for $\\mathbb{E}[\\mathrm{SELU}(x)]$ and $\\mathrm{Var}[\\mathrm{SELU}(x)]$.\n\nExplain, using these expressions and without invoking any shortcut formulas beyond the core definitions, how the output mean and variance after each activation depend on the input standard deviation $\\sigma$ and the activation parameters $(\\alpha,\\lambda)$. Discuss implications for weight initialization strategies aimed at preserving zero mean and stable variance across layers, including how one could select $(\\alpha,\\lambda)$ so that $\\mathbb{E}[\\mathrm{SELU}(x)]=0$ and $\\mathrm{Var}[\\mathrm{SELU}(x)]=1$ given $\\mathbb{E}[x]=0$.\n\nYour final answer must be a single closed-form analytic expression collecting the six derived quantities in a single row matrix. If you choose to introduce any special functions (for example, the standard normal cumulative distribution function), define them clearly in your derivation. No numerical approximation is required.", "solution": "The problem requires the derivation of the mean and variance of the output of three activation functions—ReLU, ELU, and SELU—when the input $x$ is a normally distributed random variable with mean $0$ and variance $\\sigma^2$. The derivation must proceed from first principles.\n\nLet the input preactivation be a random variable $x \\sim \\mathcal{N}(0, \\sigma^2)$. Its probability density function (PDF) is given by\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\nThe fundamental definitions for the expectation $\\mathbb{E}[g(x)]$ and variance $\\mathrm{Var}[g(x)]$ of a function $g(x)$ of a continuous random variable $x$ are:\n$$\\mathbb{E}[g(x)] = \\int_{-\\infty}^{\\infty} g(x) p(x) dx$$\n$$\\mathrm{Var}[g(x)] = \\mathbb{E}\\left[(g(x) - \\mathbb{E}[g(x)])^2\\right] = \\mathbb{E}[g(x)^2] - (\\mathbb{E}[g(x)])^2$$\nWe introduce the standard normal random variable $z \\sim \\mathcal{N}(0,1)$, whose PDF is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$ and whose cumulative distribution function (CDF) is $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$. The input $x$ can be written as $x = \\sigma z$.\n\nWe will frequently use the following definite integrals, which are derived from the definitions:\n$1$. $\\int_0^\\infty x p(x) dx$: This integral represents the expectation of $x$ over the positive real line.\n$$\\int_0^\\infty x \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) dx = \\frac{\\sigma}{\\sqrt{2\\pi}} \\int_0^\\infty z \\exp\\left(-\\frac{z^2}{2}\\right) dz = \\frac{\\sigma}{\\sqrt{2\\pi}} \\left[-\\exp\\left(-\\frac{z^2}{2}\\right)\\right]_0^\\infty = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n$2$. $\\int_0^\\infty x^2 p(x) dx$: The total variance of $x$ is $\\mathrm{Var}[x] = \\mathbb{E}[x^2] = \\sigma^2$. Due to the symmetry of the normal distribution about $0$, the integral over the positive half is half of the total.\n$$\\int_0^\\infty x^2 p(x) dx = \\frac{1}{2}\\int_{-\\infty}^\\infty x^2 p(x) dx = \\frac{\\sigma^2}{2}$$\n$3$. $\\int_{-\\infty}^0 \\exp(ax) p(x) dx$: This integral can be solved by completing the square in the exponent.\n$$\\int_{-\\infty}^0 \\exp(ax) p(x) dx = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^0 \\exp\\left(ax - \\frac{x^2}{2\\sigma^2}\\right) dx = \\exp\\left(\\frac{a^2\\sigma^2}{2}\\right) \\Phi(-a\\sigma)$$\n$4$. $\\int_{-\\infty}^0 p(x) dx = P(x \\le 0) = \\frac{1}{2}$ by symmetry.\n\nWith these tools, we derive the required quantities for each activation function.\n\n**1. Rectified Linear Unit (ReLU)**\nThe ReLU function is defined as $\\mathrm{ReLU}(x) = \\max\\{0, x\\}$.\n\nMean $\\mathbb{E}[\\mathrm{ReLU}(x)]$:\n$$\\mathbb{E}[\\mathrm{ReLU}(x)] = \\int_{-\\infty}^{\\infty} \\max\\{0, x\\} p(x) dx = \\int_{-\\infty}^0 0 \\cdot p(x) dx + \\int_0^\\infty x p(x) dx$$\nUsing integral $1$, we find:\n$$\\mathbb{E}[\\mathrm{ReLU}(x)] = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n\nVariance $\\mathrm{Var}[\\mathrm{ReLU}(x)]$:\nFirst, we find the second moment, $\\mathbb{E}[\\mathrm{ReLU}(x)^2]$.\n$$\\mathbb{E}[\\mathrm{ReLU}(x)^2] = \\int_{-\\infty}^{\\infty} (\\max\\{0, x\\})^2 p(x) dx = \\int_{-\\infty}^0 0^2 \\cdot p(x) dx + \\int_0^\\infty x^2 p(x) dx$$\nUsing integral $2$, we get:\n$$\\mathbb{E}[\\mathrm{ReLU}(x)^2] = \\frac{\\sigma^2}{2}$$\nNow, we can compute the variance:\n$$\\mathrm{Var}[\\mathrm{ReLU}(x)] = \\mathbb{E}[\\mathrm{ReLU}(x)^2] - (\\mathbb{E}[\\mathrm{ReLU}(x)])^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)$$\n\n**2. Exponential Linear Unit (ELU)**\nThe ELU function is defined as $\\mathrm{ELU}(x) = x$ for $x  0$ and $\\mathrm{ELU}(x) = \\alpha(\\exp(x) - 1)$ for $x \\leq 0$, with $\\alpha0$.\n\nMean $\\mathbb{E}[\\mathrm{ELU}(x)]$:\n$$\\mathbb{E}[\\mathrm{ELU}(x)] = \\int_0^\\infty x p(x) dx + \\int_{-\\infty}^0 \\alpha(\\exp(x) - 1) p(x) dx$$\nThe first term is $\\frac{\\sigma}{\\sqrt{2\\pi}}$. The second term is:\n$$\\alpha \\int_{-\\infty}^0 (\\exp(x) - 1) p(x) dx = \\alpha \\left( \\int_{-\\infty}^0 \\exp(x) p(x) dx - \\int_{-\\infty}^0 p(x) dx \\right)$$\nUsing integrals $3$ (with $a=1$) and $4$:\n$$\\alpha \\left( \\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2} \\right)$$\nCombining the terms gives the mean:\n$$\\mathbb{E}[\\mathrm{ELU}(x)] = \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right)$$\n\nVariance $\\mathrm{Var}[\\mathrm{ELU}(x)]$:\nFirst, we find the second moment, $\\mathbb{E}[\\mathrm{ELU}(x)^2]$.\n$$\\mathbb{E}[\\mathrm{ELU}(x)^2] = \\int_0^\\infty x^2 p(x) dx + \\int_{-\\infty}^0 \\left(\\alpha(\\exp(x) - 1)\\right)^2 p(x) dx$$\nThe first term is $\\frac{\\sigma^2}{2}$. The second term is:\n$$\\alpha^2 \\int_{-\\infty}^0 (\\exp(2x) - 2\\exp(x) + 1) p(x) dx$$\n$$= \\alpha^2 \\left( \\int_{-\\infty}^0 \\exp(2x) p(x) dx - 2\\int_{-\\infty}^0 \\exp(x) p(x) dx + \\int_{-\\infty}^0 p(x) dx \\right)$$\nUsing integrals $3$ (with $a=2$ and $a=1$) and $4$:\n$$= \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right)$$\nCombining the terms gives the second moment:\n$$\\mathbb{E}[\\mathrm{ELU}(x)^2] = \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right)$$\nThe variance is then $\\mathrm{Var}[\\mathrm{ELU}(x)] = \\mathbb{E}[\\mathrm{ELU}(x)^2] - (\\mathbb{E}[\\mathrm{ELU}(x)])^2$, which gives:\n$$\\mathrm{Var}[\\mathrm{ELU}(x)] = \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\dots \\right) \\right] - \\left[ \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left( \\dots \\right) \\right]^2$$\nExplicitly:\n$$\\mathrm{Var}[\\mathrm{ELU}(x)] = \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2$$\n\n**3. Scaled Exponential Linear Unit (SELU)**\nThe SELU function is defined as $\\mathrm{SELU}(x) = \\lambda \\cdot \\mathrm{ELU}(x)$, with $\\lambda  0$ and $\\alpha  0$. The mean and variance can be found using the properties of expectation and variance with respect to scalar multiplication.\n\nMean $\\mathbb{E}[\\mathrm{SELU}(x)]$:\n$$\\mathbb{E}[\\mathrm{SELU}(x)] = \\mathbb{E}[\\lambda \\cdot \\mathrm{ELU}(x)] = \\lambda \\mathbb{E}[\\mathrm{ELU}(x)]$$\n$$\\mathbb{E}[\\mathrm{SELU}(x)] = \\lambda \\left[ \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right]$$\n\nVariance $\\mathrm{Var}[\\mathrm{SELU}(x)]$:\n$$\\mathrm{Var}[\\mathrm{SELU}(x)] = \\mathrm{Var}[\\lambda \\cdot \\mathrm{ELU}(x)] = \\lambda^2 \\mathrm{Var}[\\mathrm{ELU}(x)]$$\n$$\\mathrm{Var}[\\mathrm{SELU}(x)] = \\lambda^2 \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 \\right]$$\n\n**Discussion and Implications**\n\nThe derived expressions reveal how the statistics of the output distribution depend on the input standard deviation $\\sigma$ and the activation parameters $(\\alpha, \\lambda)$.\n\nFor ReLU, the output mean $\\mathbb{E}[\\mathrm{ReLU}(x)] = \\sigma/\\sqrt{2\\pi}$ is strictly positive and scales with $\\sigma$. This introduces a \"bias shift\" away from zero, which can complicate optimization. The output variance is $\\mathrm{Var}[\\mathrm{ReLU}(x)] \\approx 0.34 \\sigma^2$. This reduction in variance, if not compensated for, can lead to the vanishing gradients problem in deep networks. Weight initialization strategies like \"He initialization\" are designed to counteract this by scaling weights to restore the variance of the pre-activations.\n\nFor ELU and SELU, the parameters $\\alpha$ and $\\lambda$ offer more control. The goal of SELU is to enable self-normalizing networks, where the output of each layer automatically converges to a distribution with a fixed mean and variance, typically mean $0$ and variance $1$.\nTo achieve $\\mathbb{E}[\\mathrm{SELU}(x)]=0$ and $\\mathrm{Var}[\\mathrm{SELU}(x)]=1$ for an input with mean $0$ and variance $\\sigma^2$, we must solve the following system of equations for $\\alpha$ and $\\lambda$:\n$1$. $\\mathbb{E}[\\mathrm{SELU}(x)] = 0$: Since $\\lambda  0$, this requires $\\mathbb{E}[\\mathrm{ELU}(x)] = 0$.\n$$\\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) = 0$$\nThis equation establishes a relationship between $\\alpha$ and the input variance $\\sigma^2$.\n\n$2$. $\\mathrm{Var}[\\mathrm{SELU}(x)] = 1$: This means $\\lambda^2 \\mathrm{Var}[\\mathrm{ELU}(x)] = 1$. Since we have set $\\mathbb{E}[\\mathrm{ELU}(x)]=0$, the variance simplifies to the second moment, $\\mathrm{Var}[\\mathrm{ELU}(x)] = \\mathbb{E}[\\mathrm{ELU}(x)^2]$.\n$$\\lambda^2 \\left[ \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) \\right] = 1$$\n\nIn the context of self-normalizing networks, one typically assumes the input to the layer has been normalized to have variance $\\sigma^2=1$. By solving these two equations for $\\sigma=1$, one obtains specific numerical values for $\\alpha$ and $\\lambda$ (approximately $\\alpha \\approx 1.6733$ and $\\lambda \\approx 1.0507$). These specific values define the standard SELU activation function, which, when paired with a specific weight initialization (\"LeCun normal\"), promotes the convergence of neuron activations to a standard normal distribution throughout the network, mitigating vanishing/exploding gradient problems and regularizing the training process.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{\\sigma}{\\sqrt{2\\pi}}  \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)  \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right)  \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2  \\lambda \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)  \\lambda^2 \\left( \\frac{\\sigma^2}{2} + \\alpha^2 \\left( \\exp(2\\sigma^2)\\Phi(-2\\sigma) - 2\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) + \\frac{1}{2} \\right) - \\left( \\frac{\\sigma}{\\sqrt{2\\pi}} + \\alpha \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right)\\Phi(-\\sigma) - \\frac{1}{2}\\right) \\right)^2 \\right) \\end{pmatrix}}$$", "id": "3197614"}, {"introduction": "Theoretical advantages of an activation function are best understood through practical application. This coding exercise demonstrates the concept of inductive bias by creating a scenario where Leaky ReLU is inherently better suited than the standard ReLU. By training both models on data generated by a Leaky ReLU process, you will empirically verify how its ability to pass attenuated negative signals leads to a more accurate model. [@problem_id:3197626]", "problem": "Consider a two-feature input vector $\\mathbf{x} = \\begin{bmatrix} x_{+} \\\\ x_{-} \\end{bmatrix} \\in \\mathbb{R}^{2}$, where $x_{+}$ represents positive evidence for a latent cause and $x_{-}$ represents negative evidence (inhibitory). The latent linear score is defined by $a^{\\star}(\\mathbf{x}) = x_{+} - x_{-}$. To represent the principle that negative evidence should partially suppress activation but not eliminate it entirely, the target mapping is defined via the Leaky Rectified Linear Unit (Leaky ReLU) activation with slope parameter $\\alpha \\in [0,1]$, acting on $a^{\\star}$:\n$$\ny(\\mathbf{x}; \\alpha) = \\phi_{\\text{leaky}, \\alpha}\\!\\left(a^{\\star}(\\mathbf{x})\\right) \\quad \\text{with} \\quad \\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a).\n$$\nFor comparison, the Rectified Linear Unit (ReLU) is defined as\n$$\n\\phi_{\\text{relu}}(a) = \\max(0,a).\n$$\nYou will train, for each specified $\\alpha$, two single-neuron models that map $\\mathbf{x}$ to a scalar prediction $\\hat{y}$ using the architecture\n$$\n\\hat{y}(\\mathbf{x}) = v \\cdot \\phi\\!\\left(\\mathbf{w}^{\\top}\\mathbf{x} + b\\right) + c,\n$$\nwhere $\\phi$ is either $\\phi_{\\text{relu}}$ or $\\phi_{\\text{leaky}, \\alpha}$, $\\mathbf{w} \\in \\mathbb{R}^{2}$, $b \\in \\mathbb{R}$, $v \\in \\mathbb{R}$, and $c \\in \\mathbb{R}$ are trainable parameters. Training minimizes the Mean Squared Error (MSE), defined for a dataset $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$ by\n$$\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\hat{y}(\\mathbf{x}_{i}) - y_{i}\\right)^{2}.\n$$\nUse Empirical Risk Minimization (ERM) via full-batch gradient descent to update $(\\mathbf{w}, b, v, c)$.\n\nDataset specification:\n- Draw $N$ independent samples with $N = 2000$ using a pseudorandom number generator initialized at seed 0.\n- For each sample, draw $x_{+} \\sim \\text{Uniform}([0,1])$ and $x_{-} \\sim \\text{Uniform}([0,1])$ independently.\n- Compute $a^{\\star} = x_{+} - x_{-}$ and $y = \\phi_{\\text{leaky}, \\alpha}(a^{\\star})$ for the given $\\alpha$.\n\nTraining specification:\n- Use full-batch gradient descent with learning rate $\\eta = 0.03$ and number of steps $T = 800$.\n- Initialize parameters $(\\mathbf{w}, b, v, c)$ from a normal distribution $\\mathcal{N}(0, \\sigma^{2})$ with $\\sigma = 0.1$ using a fixed seed per test case to ensure reproducibility and fairness: for the test case index $k$ (starting at 0), set the initialization seed to $100 + k$.\n- Ensure both models (with $\\phi=\\phi_{\\text{relu}}$ and with $\\phi=\\phi_{\\text{leaky}, \\alpha}$) start from the exact same initial parameters in each test case.\n\nTest suite:\nEvaluate the following values of the negative slope parameter $\\alpha$:\n- $\\alpha = 0.2$: typical partial suppression.\n- $\\alpha = 0.0$: boundary case where Leaky ReLU reduces to ReLU.\n- $\\alpha = 0.8$: strong partial suppression.\n- $\\alpha = 1.0$: identity activation on $a^{\\star}$.\n\nFor each $\\alpha$, train both models as specified above and compute their MSE on the training data. Define a tolerance $\\delta = 10^{-4}$ and output, for each test case, the boolean result\n$$\n\\text{result} = \\left(\\text{MSE}_{\\text{leaky}} + \\delta  \\text{MSE}_{\\text{relu}}\\right).\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}]$), with the booleans in the order of $\\alpha$ given above.\n\nAcronym definitions:\n- Rectified Linear Unit (ReLU): $\\phi_{\\text{relu}}(a) = \\max(0,a)$.\n- Leaky Rectified Linear Unit (Leaky ReLU): $\\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$.\n- Mean Squared Error (MSE): $\\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i}-y_{i})^{2}$.\n- Empirical Risk Minimization (ERM): minimizing empirical loss over the dataset to estimate parameters.", "solution": "The problem requires training two single-neuron models to approximate a target function generated by a Leaky Rectified Linear Unit (Leaky ReLU). One model uses a Leaky ReLU activation function, matching the ground truth, while the other uses a standard Rectified Linear Unit (ReLU). The goal is to compare their performance by training them via full-batch gradient descent and evaluating their final Mean Squared Error (MSE) on the training data.\n\n### 1. Model and Loss Function\n\nThe architecture for both models is a single-neuron network given by:\n$$\n\\hat{y}(\\mathbf{x}) = v \\cdot \\phi(\\mathbf{w}^{\\top}\\mathbf{x} + b) + c\n$$\nwhere $\\mathbf{x} = \\begin{bmatrix} x_{+} \\\\ x_{-} \\end{bmatrix} \\in \\mathbb{R}^{2}$ is the input vector, $\\mathbf{w} \\in \\mathbb{R}^{2}$, $b \\in \\mathbb{R}$, $v \\in \\mathbb{R}$, and $c \\in \\mathbb{R}$ are the trainable parameters. The function $\\phi$ is the activation function.\n\nThe two models are distinguished by their choice of $\\phi$:\n1.  **ReLU Model**: $\\phi(\\cdot) = \\phi_{\\text{relu}}(a) = \\max(0, a)$.\n2.  **Leaky ReLU Model**: $\\phi(\\cdot) = \\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$, where $\\alpha$ is a given slope parameter.\n\nThe models are trained by minimizing the Mean Squared Error (MSE) loss function over a dataset of $N$ samples $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$:\n$$\nJ(\\mathbf{w}, b, v, c) = \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\hat{y}(\\mathbf{x}_{i}) - y_{i}\\right)^{2}\n$$\nThe target values $y_i$ are generated by $y_i = \\phi_{\\text{leaky}, \\alpha}(x_{i,+} - x_{i,-})$ for a specific $\\alpha$.\n\n### 2. Gradient Derivation for Batch Gradient Descent\n\nTo minimize the MSE using gradient descent, we must compute the partial derivatives of the loss function $J$ with respect to each parameter. The update rule for any parameter $\\theta$ is $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J$, where $\\eta$ is the learning rate. We use full-batch gradient descent, so the gradients are averaged over the entire dataset.\n\nLet $a_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b$ be the pre-activation for sample $i$, and $\\hat{y}_i = v \\cdot \\phi(a_i) + c$ be the prediction. The loss for a single sample is $L_i = (\\hat{y}_i - y_i)^2$. Applying the chain rule, we find the gradients of the total loss $J = \\frac{1}{N} \\sum_{i=1}^{N} L_i$:\n\n-   **Gradient with respect to $c$**:\n    $$\n    \\nabla_c J = \\frac{\\partial J}{\\partial c} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial c} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i)\n    $$\n\n-   **Gradient with respect to $v$**:\n    $$\n    \\nabla_v J = \\frac{\\partial J}{\\partial v} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial v} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\phi(a_i)\n    $$\n\n-   **Gradient with respect to $b$**:\n    $$\n    \\nabla_b J = \\frac{\\partial J}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\cdot v \\cdot \\phi'(a_i)\n    $$\n\n-   **Gradient with respect to $\\mathbf{w}$**:\n    $$\n    \\nabla_{\\mathbf{w}} J = \\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_i}{\\partial \\mathbf{w}} = \\frac{1}{N} \\sum_{i=1}^{N} 2(\\hat{y}_i - y_i) \\cdot v \\cdot \\phi'(a_i) \\cdot \\mathbf{x}_i\n    $$\n\nThe term $\\phi'(a)$ is the derivative (or subgradient) of the activation function.\n\n-   For **ReLU**: $\\phi_{\\text{relu}}(a) = \\max(0, a)$. Its subgradient is:\n    $$\n    \\phi'_{\\text{relu}}(a) = \\begin{cases} 1  \\text{if } a  0 \\\\ 0  \\text{if } a \\le 0 \\end{cases}\n    $$\n-   For **Leaky ReLU**: $\\phi_{\\text{leaky}, \\alpha}(a) = \\max(a, \\alpha a)$. For $\\alpha \\in [0, 1]$, this can be written as $a$ if $a \\ge 0$ and $\\alpha a$ if $a  0$. Its subgradient is:\n    $$\n    \\phi'_{\\text{leaky}, \\alpha}(a) = \\begin{cases} 1  \\text{if } a \\ge 0 \\\\ \\alpha  \\text{if } a  0 \\end{cases}\n    $$\nThis general form correctly handles the boundary cases where $\\alpha=0$ (reducing to ReLU) and $\\alpha=1$ (reducing to the identity function, $a$).\n\n### 3. Simulation Procedure\n\nThe problem requires a computational experiment adhering to the following specifications:\n\n1.  **Dataset Generation**: A single dataset of input features $\\mathbf{X}$ is generated at the beginning, consisting of $N = 2000$ samples. For each sample, $x_+$ and $x_-$ are drawn independently from $\\text{Uniform}([0,1])$. A pseudorandom number generator with seed $0$ is used.\n2.  **Test Cases**: The experiment is run for four values of the slope parameter $\\alpha \\in \\{0.2, 0.0, 0.8, 1.0\\}$.\n3.  **Per-Case Setup**: For each value of $\\alpha$ (indexed by $k$, starting at $0$):\n    a. The target vector $\\mathbf{y}$ is computed as $y_i = \\phi_{\\text{leaky}, \\alpha}(x_{i,+} - x_{i,-})$.\n    b. The model parameters $(\\mathbf{w}, b, v, c)$ are initialized by drawing from a normal distribution $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.1$, using a fixed seed of $100+k$.\n    c. Both the ReLU model and the Leaky ReLU model start from these identical initial parameters.\n4.  **Training**: Both models are trained for $T=800$ steps using full-batch gradient descent with a learning rate of $\\eta=0.03$. The gradients derived above are used for parameter updates.\n5.  **Evaluation**: After training, the final MSE is computed for both models on the entire training dataset, yielding $\\text{MSE}_{\\text{leaky}}$ and $\\text{MSE}_{\\text{relu}}$.\n6.  **Comparison**: The boolean result is determined by the condition $\\text{MSE}_{\\text{leaky}} + \\delta  \\text{MSE}_{\\text{relu}}$, with a tolerance $\\delta = 10^{-4}$.\n\nThis procedure systematically compares a model with an architecture matched to the data-generating process against a misspecified one, providing insight into the role of inductive bias. For $\\alpha=0.0$, the target function is a ReLU, making both models functionally identical; thus, their MSEs are expected to be equal, leading to a `False` result. For other $\\alpha$ values, the Leaky ReLU model has the correct functional form and is expected to achieve a lower MSE, resulting in `True`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef leaky_relu(a, alpha):\n    \"\"\"\n    Computes the Leaky ReLU activation function.\n    Given alpha in [0,1], max(a, alpha*a) is equivalent to:\n    a if a = 0\n    alpha * a if a  0\n    \"\"\"\n    return np.maximum(a, alpha * a)\n\ndef leaky_relu_grad(a, alpha):\n    \"\"\"Computes the gradient of the Leaky ReLU function.\"\"\"\n    grad = np.ones_like(a, dtype=float)\n    grad[a  0] = alpha\n    return grad\n\ndef relu(a):\n    \"\"\"Computes the ReLU activation function.\"\"\"\n    return np.maximum(0, a)\n\ndef relu_grad(a):\n    \"\"\"Computes the gradient of the ReLU function.\"\"\"\n    grad = np.zeros_like(a, dtype=float)\n    grad[a  0] = 1.0\n    return grad\n\ndef train_model(X, y, initial_params, activation_func, grad_func, T, eta):\n    \"\"\"\n    Trains a single-neuron model using full-batch gradient descent.\n    \"\"\"\n    w, b, v, c = initial_params\n    N = X.shape[0]\n\n    for _ in range(T):\n        # Forward pass\n        a = X @ w + b\n        h = activation_func(a)\n        y_hat = v * h + c\n\n        # Error term\n        e = y_hat - y\n\n        # Gradients\n        grad_c = (2 / N) * np.sum(e)\n        grad_v = (2 / N) * np.sum(e * h)\n        \n        # Common term for w and b gradients\n        g_a_common = (2 / N) * v * e\n        g_a = g_a_common * grad_func(a)\n\n        grad_b = np.sum(g_a)\n        grad_w = X.T @ g_a\n\n        # Update parameters\n        w -= eta * grad_w\n        b -= eta * grad_b\n        v -= eta * grad_v\n        c -= eta * grad_c\n\n    # Compute final MSE on the training data\n    a_final = X @ w + b\n    h_final = activation_func(a)\n    y_hat_final = v * h_final + c\n    final_mse = np.mean((y_hat_final - y)**2)\n\n    return final_mse\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and print the results.\n    \"\"\"\n    # Problem specifications\n    N = 2000\n    dataset_seed = 0\n    eta = 0.03\n    T = 800\n    sigma = 0.1\n    delta = 1e-4\n    alphas = [0.2, 0.0, 0.8, 1.0]\n\n    # Generate dataset features X (same for all test cases)\n    rng_dataset = np.random.RandomState(dataset_seed)\n    x_plus = rng_dataset.uniform(0, 1, N)\n    x_minus = rng_dataset.uniform(0, 1, N)\n    X = np.stack([x_plus, x_minus], axis=1)\n\n    results = []\n\n    for k, alpha in enumerate(alphas):\n        # Generate target vector y for the current alpha\n        a_star = x_plus - x_minus\n        y = leaky_relu(a_star, alpha)\n\n        # Generate initial parameters using the per-case seed\n        init_seed = 100 + k\n        rng_init = np.random.RandomState(init_seed)\n        w_init = rng_init.normal(0, sigma, size=2)\n        b_init = rng_init.normal(0, sigma)\n        v_init = rng_init.normal(0, sigma)\n        c_init = rng_init.normal(0, sigma)\n        \n        # Ensure both models start with the exact same parameters\n        # by passing copies\n        params_for_leaky = (w_init.copy(), b_init, v_init, c_init)\n        params_for_relu = (w_init.copy(), b_init, v_init, c_init)\n\n        # Train Leaky ReLU model\n        leaky_activation_with_alpha = lambda a: leaky_relu(a, alpha)\n        leaky_grad_with_alpha = lambda a: leaky_relu_grad(a, alpha)\n        mse_leaky = train_model(\n            X, y, params_for_leaky,\n            leaky_activation_with_alpha, leaky_grad_with_alpha,\n            T, eta\n        )\n\n        # Train ReLU model\n        mse_relu = train_model(\n            X, y, params_for_relu,\n            relu, relu_grad,\n            T, eta\n        )\n        \n        # Compare MSEs and store boolean result\n        result = (mse_leaky + delta  mse_relu)\n        results.append(result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197626"}, {"introduction": "The choice of activation function directly shapes the geometry of the loss landscape, impacting the ease of optimization. This exercise explores the connection between activation smoothness and landscape curvature by using the Softplus function, a smooth approximation of ReLU controlled by a hardness parameter $\\beta$. By computationally measuring the curvature as Softplus hardens into ReLU, you will develop a tangible intuition for how non-smooth activations can create sharper, more complex optimization surfaces. [@problem_id:3197657]", "problem": "You will implement and analyze a one-hidden-unit regression model that uses a smooth variant of the Rectified Linear Unit (ReLU) activation to study how the loss landscape changes as the activation hardness parameter increases. The model is defined on a scalar input $x$ as $f_{\\theta,\\beta}(x) = w_2 \\, a_\\beta(w_1 x + b_1) + b_2$, where $a_\\beta$ is the Softplus activation with hardness parameter $\\beta$. The dataset is fixed to the scalar inputs $x \\in \\{-2,-1,0,1,2\\}$ with targets $y = \\max(0,x)$, that is, the Rectified Linear Unit (ReLU). The loss is the mean squared error $L_\\beta(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{\\theta,\\beta}(x_i) - y_i)^2$ with $N = 5$. Your program will compute a finite-difference approximation to the second directional derivative (curvature) of the loss at a fixed parameter vector along a fixed direction, for a sequence of $\\beta$ values that progressively harden the activation, and report these curvature values.\n\nFundamental base and definitions you must use:\n- The Softplus with hardness parameter $\\beta$ is the smooth activation defined by $a_\\beta(x) = \\frac{1}{\\beta} \\log(1 + e^{\\beta x})$. As $\\beta \\to \\infty$, $a_\\beta(x)$ approaches the Rectified Linear Unit (ReLU) given by $\\max(0,x)$.\n- The Mean Squared Error (MSE) for targets $y_i$ and predictions $\\hat{y}_i$ is $L = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$.\n- The second directional derivative of a scalar function $L(\\theta)$ along a unit direction $v$ at point $\\theta$ can be approximated by the central finite difference formula $L''(\\theta;v) \\approx \\frac{L(\\theta + \\varepsilon v) - 2L(\\theta) + L(\\theta - \\varepsilon v)}{\\varepsilon^2}$ for small step $\\varepsilon$.\n\nProgram requirements:\n- Use the fixed dataset $\\mathcal{D} = \\{(-2,0),(-1,0),(0,0),(1,1),(2,2)\\}$.\n- Use the fixed parameter vector $\\theta = (w_1, b_1, w_2, b_2) = (1,0,1,0)$.\n- Use the fixed direction $v = (0.3,-0.2,0.4,0.1)$ normalized to unit length, i.e., replaced by $v / \\|v\\|_2$.\n- Use the finite-difference step $\\varepsilon = 10^{-3}$.\n- For each hardness value $\\beta$ in the test suite specified below, compute the curvature $C_\\beta$ of $L_\\beta$ at $\\theta$ along $v$ using the central difference formula above.\n- Implement the Softplus stably to avoid numerical overflow for large $\\beta$ by relying on numerically stable transformations of $\\log(1 + e^{z})$.\n\nTest suite:\n- Evaluate and report the curvature values for $\\beta \\in \\{0.5, 1.0, 2.0, 5.0, 20.0, 50.0\\}$, in this exact order.\n\nAnswer specification:\n- Your program must output a single line containing a Python-style list of the curvature values $[C_{0.5}, C_{1.0}, C_{2.0}, C_{5.0}, C_{20.0}, C_{50.0}]$ in that order, each rounded to exactly $6$ decimal places.\n- The output must be a single line, with no additional text.\n- Each reported value is a float. No physical units or angle units are involved.\n\nScientific realism and reasoning mandate:\n- The task evaluates how increasing $\\beta$ changes the smoothness of the activation and thereby the curvature of the empirical loss landscape. Your implementation should directly follow from the definitions above, without using any training procedure or undocumented heuristics. Ensure numerical stability when computing the Softplus for large $\\beta$.", "solution": "The problem requires the computation of the second directional derivative, or curvature, of a mean squared error loss function for a single-hidden-unit regression model. This curvature is to be evaluated for a sequence of models where the activation function, Softplus, progressively \"hardens\" into the Rectified Linear Unit (ReLU). The analysis will be conducted at a fixed point in the parameter space and along a fixed direction.\n\nFirst, we formalize the components of the problem.\nThe model's prediction for a scalar input $x$ is given by:\n$$f_{\\theta,\\beta}(x) = w_2 \\, a_\\beta(w_1 x + b_1) + b_2$$\nThe parameters of the model are collected in the vector $\\theta = (w_1, b_1, w_2, b_2)$. The activation function, $a_\\beta$, is the Softplus function with a hardness parameter $\\beta$:\n$$a_\\beta(z) = \\frac{1}{\\beta} \\log(1 + e^{\\beta z})$$\nAs the parameter $\\beta$ approaches infinity, $a_\\beta(z)$ pointwise converges to the ReLU function, $\\max(0, z)$.\n\nThe model is evaluated on a fixed dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N}$ with $N=5$ data points. The inputs are $x \\in \\{-2, -1, 0, 1, 2\\}$ and the corresponding target outputs are $y_i = \\max(0, x_i)$, resulting in the dataset $\\mathcal{D} = \\{(-2,0), (-1,0), (0,0), (1,1), (2,2)\\}$.\n\nThe loss function $L_\\beta(\\theta)$ is the Mean Squared Error (MSE) between the model predictions $f_{\\theta,\\beta}(x_i)$ and the true targets $y_i$:\n$$L_\\beta(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{\\theta,\\beta}(x_i) - y_i)^2$$\n\nA critical aspect of implementing the Softplus function is ensuring numerical stability, particularly for large values of the argument $\\beta z$. When $\\beta z$ is large and positive, $e^{\\beta z}$ can cause a numerical overflow. A standard stable implementation of the function $g(u) = \\log(1 + e^u)$ can be derived. For positive $u$, we write $g(u) = \\log(e^u(e^{-u} + 1)) = u + \\log(1 + e^{-u})$. For negative $u$, the original form is stable. A more direct and robust approach is to use a library function that computes $\\log(e^a + e^b)$ in a stable manner, such as `numpy.logaddexp(a, b)`. Setting $a=0$ and $b=\\beta z$, the Softplus activation can be computed as:\n$$a_\\beta(z) = \\frac{1}{\\beta} \\text{np.logaddexp}(0, \\beta z)$$\n\nThe primary objective is to compute the second directional derivative of the loss $L_\\beta(\\theta)$ at a fixed parameter vector $\\theta_{center}$ along a fixed unit direction vector $v$. This quantity, denoted $C_\\beta$, represents the curvature of the loss surface in the direction of $v$. It is approximated using the central finite difference formula:\n$$C_\\beta = L_\\beta''(\\theta_{center}; v) \\approx \\frac{L_\\beta(\\theta_{center} + \\varepsilon v) - 2L_\\beta(\\theta_{center}) + L_\\beta(\\theta_{center} - \\varepsilon v)}{\\varepsilon^2}$$\nwhere $\\varepsilon$ is a small step size.\n\nThe problem specifies the following fixed values:\n- The parameter vector at which the curvature is evaluated: $\\theta_{center} = (w_1, b_1, w_2, b_2) = (1, 0, 1, 0)$.\n- The direction vector, which must be normalized: $v_{raw} = (0.3, -0.2, 0.4, 0.1)$. The unit direction vector is $v = v_{raw} / \\|v_{raw}\\|_2$. The $L_2$-norm is $\\|v_{raw}\\|_2 = \\sqrt{0.3^2 + (-0.2)^2 + 0.4^2 + 0.1^2} = \\sqrt{0.09 + 0.04 + 0.16 + 0.01} = \\sqrt{0.3}$.\n- The finite difference step size: $\\varepsilon = 10^{-3}$.\n\nThe computational procedure is as follows. For each specified value of $\\beta$ from the set $\\{0.5, 1.0, 2.0, 5.0, 20.0, 50.0\\}$:\n1. Define a function that computes the loss $L_\\beta(\\theta)$ for any given parameter vector $\\theta$. This function takes $\\theta = (w_1, b_1, w_2, b_2)$ as input, calculates the predictions $f_{\\theta,\\beta}(x_i)$ for all $x_i$ in the dataset using the numerically stable Softplus implementation, and returns the mean squared error.\n2. Normalize the direction vector $v_{raw}$ to obtain $v$.\n3. Evaluate the loss function at three distinct points in the parameter space:\n   - $L_{center} = L_\\beta(\\theta_{center})$\n   - $L_{plus} = L_\\beta(\\theta_{center} + \\varepsilon v)$\n   - $L_{minus} = L_\\beta(\\theta_{center} - \\varepsilon v)$\n4. Substitute these three loss values into the central difference formula to compute the curvature approximation $C_\\beta$.\n\nThis process is repeated for each value of $\\beta$, and the resulting curvature values are collected. As $\\beta$ increases, the Softplus function becomes a sharper approximation of the non-differentiable ReLU function. This change in the activation function's smoothness is expected to be reflected in the curvature of the loss landscape, a phenomenon which this calculation is designed to quantify.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the second directional derivative (curvature) of a loss landscape\n    for a simple neural network. The curvature is evaluated for a sequence of\n    models where the Softplus activation function hardens towards ReLU.\n    \"\"\"\n\n    # --- Fixed parameters and data as per problem statement ---\n    \n    # Dataset: x values and corresponding y = max(0, x) targets\n    x_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data = np.maximum(0, x_data)\n\n    # Fixed parameter vector theta = (w1, b1, w2, b2)\n    theta_center = np.array([1.0, 0.0, 1.0, 0.0])\n\n    # Unnormalized direction vector v\n    v_raw = np.array([0.3, -0.2, 0.4, 0.1])\n    \n    # Normalize the direction vector to unit length\n    v_norm = v_raw / np.linalg.norm(v_raw)\n\n    # Finite-difference step size\n    epsilon = 1e-3\n\n    # Hardness parameters for the Softplus function\n    beta_values = [0.5, 1.0, 2.0, 5.0, 20.0, 50.0]\n\n    # List to store the results\n    curvature_results = []\n\n    # --- Main Calculation Loop ---\n    \n    for beta in beta_values:\n        \n        def calculate_loss(theta):\n            \"\"\"\n            Calculates the Mean Squared Error for a given parameter vector theta\n            and the current beta value.\n            \n            Args:\n                theta (np.ndarray): The parameter vector (w1, b1, w2, b2).\n\n            Returns:\n                float: The mean squared error loss.\n            \"\"\"\n            w1, b1, w2, b2 = theta\n            \n            # Calculate pre-activation values\n            z = w1 * x_data + b1\n            \n            # Apply the numerically stable Softplus activation function\n            # a_beta(z) = (1/beta) * log(1 + exp(beta * z))\n            # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x)) stably.\n            activation_out = (1.0 / beta) * np.logaddexp(0, beta * z)\n            \n            # Calculate the final model prediction\n            y_pred = w2 * activation_out + b2\n            \n            # Compute the Mean Squared Error\n            loss = np.mean((y_pred - y_data)**2)\n            \n            return loss\n\n        # Evaluate the loss at the three points required for the central difference formula\n        loss_center = calculate_loss(theta_center)\n        loss_plus = calculate_loss(theta_center + epsilon * v_norm)\n        loss_minus = calculate_loss(theta_center - epsilon * v_norm)\n        \n        # Compute the second directional derivative (curvature)\n        curvature = (loss_plus - 2 * loss_center + loss_minus) / (epsilon**2)\n        \n        curvature_results.append(curvature)\n\n    # Format the results for the final output\n    # Each value is rounded to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in curvature_results]\n    \n    # Print the final output in the specified format\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Run the solver\nsolve()\n```", "id": "3197657"}]}