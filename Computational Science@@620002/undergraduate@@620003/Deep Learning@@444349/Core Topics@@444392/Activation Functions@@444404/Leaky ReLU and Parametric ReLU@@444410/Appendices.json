{"hands_on_practices": [{"introduction": "The key innovation of Parametric ReLU (PReLU) is making the negative slope, $\\alpha$, a learnable parameter. To train this parameter using gradient descent, we must first derive the gradient of the loss function with respect to $\\alpha$. This foundational exercise will guide you through applying the chain rule to find this gradient, providing essential practice in the mechanics of backpropagation for custom activation functions [@problem_id:3101068].", "problem": "Consider a single neuron in a feedforward network whose activation is the Parametric Rectified Linear Unit (PReLU), formally, the parametric activation function $f_{\\alpha}(z)$ defined by\n$$\nf_{\\alpha}(z) \\;=\\; \\max(0, z) \\;+\\; \\alpha\\,\\min(0, z).\n$$\nLet the neuron's pre-activation be $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$, and its output be $y = f_{\\alpha}(z)$. For a scalar target $t$, define the training objective as the sum of a data-fitting term and an $\\ell_{2}$ regularization term on $\\alpha$:\n$$\nL \\;=\\; \\frac{1}{2}\\,\\big(y - t\\big)^{2} \\;+\\; \\frac{\\lambda}{2}\\,\\alpha^{2}.\n$$\nStarting from the chain rule of calculus and the definitions of $\\max(\\cdot,\\cdot)$ and $\\min(\\cdot,\\cdot)$, derive an explicit expression for $\\frac{\\partial L}{\\partial \\alpha}$ in terms of $z$, $y$, $t$, $\\alpha$, and $\\lambda$. Then, using the specific values $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$, $\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$, $b = 0.1$, $\\alpha = 0.25$, $t = -0.3$, and $\\lambda = 0.1$, compute the numerical value of $\\frac{\\partial L}{\\partial \\alpha}$.\n\nFinally, based on your derived expression, explain briefly in one or two sentences how the regularization on $\\alpha$ influences the gradient direction and magnitude during gradient descent.\n\nRound your final numeric answer for $\\frac{\\partial L}{\\partial \\alpha}$ to $3$ significant figures. No units are required.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe following data and definitions are provided:\n-   Parametric Rectified Linear Unit (PReLU) activation function: $f_{\\alpha}(z) = \\max(0, z) + \\alpha\\,\\min(0, z)$.\n-   Neuron pre-activation: $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$.\n-   Neuron output: $y = f_{\\alpha}(z)$.\n-   Loss function (objective): $L = \\frac{1}{2}\\,(y - t)^{2} + \\frac{\\lambda}{2}\\,\\alpha^{2}$.\n-   Input vector: $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$.\n-   Weight vector: $\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$.\n-   Bias: $b = 0.1$.\n-   PReLU parameter: $\\alpha = 0.25$.\n-   Target value: $t = -0.3$.\n-   Regularization hyperparameter: $\\lambda = 0.1$.\n-   Tasks:\n    1.  Derive an explicit expression for $\\frac{\\partial L}{\\partial \\alpha}$ in terms of $z, y, t, \\alpha, \\lambda$.\n    2.  Compute the numerical value of $\\frac{\\partial L}{\\partial \\alpha}$ for the given values, rounded to $3$ significant figures.\n    3.  Explain the effect of the regularization term on the gradient.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on standard, well-established concepts in machine learning and computational neuroscience, including neural network activation functions (PReLU), loss functions with $\\ell_2$ regularization, and gradient-based optimization (backpropagation). The mathematical framework is sound.\n-   **Well-Posed**: The problem is well-posed. The functions involved are continuous and differentiable almost everywhere. The provided numerical values allow for a unique and stable solution. All necessary information is provided, and there are no contradictions. The function $f_{\\alpha}(z)$ and its derivatives are well-defined for the case that will be encountered ($z \\neq 0$).\n-   **Objective**: The problem is stated using precise mathematical language and is free of subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and contains a complete and consistent setup. Therefore, I will proceed with a full solution.\n\n### Part 1: Derivation of the Gradient Expression\n\nThe objective is to find the partial derivative of the loss function $L$ with respect to the parameter $\\alpha$, denoted as $\\frac{\\partial L}{\\partial \\alpha}$. The loss function is given by:\n$$\nL(\\alpha, y) \\;=\\; \\frac{1}{2}\\,(y - t)^{2} \\;+\\; \\frac{\\lambda}{2}\\,\\alpha^{2}\n$$\nThe neuron's output $y$ is a function of $\\alpha$ and $z$, where $y = f_{\\alpha}(z)$. The pre-activation $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$ does not depend on $\\alpha$.\nWe apply the chain rule for partial differentiation. Since $L$ depends on $\\alpha$ both directly through the regularization term and indirectly through $y$, we have:\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; \\frac{\\partial L}{\\partial y}\\,\\frac{\\partial y}{\\partial \\alpha} \\;+\\; \\frac{\\partial}{\\partial \\alpha}\\left( \\frac{\\lambda}{2}\\,\\alpha^{2} \\right)\n$$\nWe compute each term separately.\nFirst, the derivative of the loss with respect to the output $y$:\n$$\n\\frac{\\partial L}{\\partial y} \\;=\\; \\frac{\\partial}{\\partial y}\\left( \\frac{1}{2}\\,(y - t)^{2} \\right) \\;=\\; y - t\n$$\nSecond, the derivative of the regularization term with respect to $\\alpha$:\n$$\n\\frac{\\partial}{\\partial \\alpha}\\left( \\frac{\\lambda}{2}\\,\\alpha^{2} \\right) \\;=\\; \\lambda\\alpha\n$$\nThird, the derivative of the neuron output $y$ with respect to $\\alpha$. The output is $y = f_{\\alpha}(z) = \\max(0, z) + \\alpha\\,\\min(0, z)$. Since $z$ is not a function of $\\alpha$, we treat it as a constant for this partial derivative:\n$$\n\\frac{\\partial y}{\\partial \\alpha} \\;=\\; \\frac{\\partial}{\\partial \\alpha}\\left( \\max(0, z) + \\alpha\\,\\min(0, z) \\right) \\;=\\; 0 + 1 \\cdot \\min(0, z) \\;=\\; \\min(0, z)\n$$\nWe can verify this by examining the piecewise definition of $f_\\alpha(z)$.\nIf $z  0$, then $y = z$, so $\\frac{\\partial y}{\\partial \\alpha} = 0$. In this case, $\\min(0, z) = 0$.\nIf $z \\le 0$, then $y = \\alpha z$, so $\\frac{\\partial y}{\\partial \\alpha} = z$. In this case, $\\min(0, z) = z$.\nThus, the expression $\\frac{\\partial y}{\\partial \\alpha} = \\min(0, z)$ is correct.\n\nSubstituting these components back into the chain rule expression:\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; (y - t)\\,\\min(0, z) \\;+\\; \\lambda\\alpha\n$$\nThis is the explicit expression for the gradient in terms of $z, y, t, \\alpha$, and $\\lambda$.\n\n### Part 2: Numerical Computation\n\nWe now compute the numerical value of $\\frac{\\partial L}{\\partial \\alpha}$ using the provided data.\n-   $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$, $\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$, $b = 0.1$.\n-   $\\alpha = 0.25$, $t = -0.3$, $\\lambda = 0.1$.\n\nFirst, we compute the pre-activation $z$:\n$$\nz \\;=\\; \\mathbf{w}^{\\top}\\mathbf{x} + b \\;=\\; (-0.8)(1.2) + (0.4)(-0.5) + (0.6)(0.3) + 0.1\n$$\n$$\nz \\;=\\; -0.96 - 0.20 + 0.18 + 0.1 \\;=\\; -1.16 + 0.28 \\;=\\; -0.88\n$$\nSince $z = -0.88  0$, the neuron operates in the \"leaky\" regime. We now compute the output $y$:\n$$\ny \\;=\\; f_{\\alpha}(z) \\;=\\; \\max(0, -0.88) + (0.25)\\,\\min(0, -0.88)\n$$\n$$\ny \\;=\\; 0 + (0.25)(-0.88) \\;=\\; -0.22\n$$\nNow we have all the components to calculate $\\frac{\\partial L}{\\partial \\alpha}$:\n-   $y - t = -0.22 - (-0.3) = 0.08$\n-   $\\min(0, z) = \\min(0, -0.88) = -0.88$\n-   $\\lambda\\alpha = (0.1)(0.25) = 0.025$\n\nSubstituting these values into the derived gradient expression:\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; (0.08)(-0.88) + 0.025\n$$\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; -0.0704 + 0.025 \\;=\\; -0.0454\n$$\nThe problem requires the answer to be rounded to $3$ significant figures. The calculated value $-0.0454$ already has exactly three significant figures (the digits $4$, $5$, and $4$).\n\n### Part 3: Influence of Regularization\n\nThe derived gradient is $\\frac{\\partial L}{\\partial \\alpha} = (y - t)\\,\\min(0, z) + \\lambda\\alpha$. The term $\\lambda\\alpha$ is the direct contribution from the $\\ell_2$ regularization penalty. In a gradient descent update step, $\\alpha$ would be updated as $\\alpha_{new} = \\alpha_{old} - \\eta \\frac{\\partial L}{\\partial \\alpha}$, which means the regularization term $\\lambda\\alpha$ always contributes a component to the gradient that acts to push the value of $\\alpha$ toward zero, irrespective of the data-fitting term. This effect, known as weight decay, penalizes large values of $\\alpha$, which helps to control model complexity and prevent overfitting.", "answer": "$$\n\\boxed{-0.0454}\n$$", "id": "3101068"}, {"introduction": "Now that we understand how to update $\\alpha$, we must consider the valid range for this parameter. This thought experiment explores the consequences of an unconstrained $\\alpha$ by investigating what happens when it becomes negative. By constructing a concrete counterexample, you will see how a negative slope violates the function's monotonicity, a property that is generally crucial for stable and efficient optimization [@problem_id:3142552].", "problem": "Consider a scalar regression model with a single neuron. The preactivation is $z = w x + b$, where $x \\in \\mathbb{R}$ is the input, $w \\in \\mathbb{R}$ is the weight, and $b \\in \\mathbb{R}$ is the bias. The activation is the Parametric Rectified Linear Unit (PReLU), defined by the piecewise function $f(z)$ with parameter $\\alpha \\in \\mathbb{R}$ such that $f(z) = z$ for $z \\geq 0$ and $f(z) = \\alpha z$ for $z  0$. The output of the neuron is $y = f(z)$. The training objective is the squared error $L(y) = (y - t)^{2}$ with target $t \\in \\mathbb{R}$. Use the chain rule and the given definitions as the only starting point.\n\nInvestigate the effect of a negative slope parameter $\\alpha  0$ on monotonicity and gradient behavior by considering the following specific setting:\n- Choose $b = 0$, $w = 1$, $\\alpha = -\\frac{1}{3}$.\n- Choose two inputs $x_{1} = -2$ and $x_{2} = -1$ and a target $t = 1$.\n\nTasks:\n1. Using only the definition of $f(z)$ and the sign of $\\alpha$, construct a concrete counterexample that shows $y$ as a function of $x$ fails to be monotonically non-decreasing when restricted to negative inputs. Explicitly evaluate $y$ at $x_{1}$ and $x_{2}$ and compare.\n2. For the input $x = x_{1} = -2$, compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial \\alpha}$ via the chain rule and the piecewise definition of $f(z)$.\n3. Explain how the signs of these gradients relate to potential training instability when $\\alpha  0$.\n\nExpress your final answer as the row matrix containing the two gradient values $\\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial \\alpha}\\right)$. No rounding is required.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Model**: Single neuron regression.\n- **Preactivation**: $z = w x + b$, with $x, w, b \\in \\mathbb{R}$.\n- **Activation Function (PReLU)**: $y = f(z)$, where $f(z) = z$ if $z \\geq 0$ and $f(z) = \\alpha z$ if $z  0$. $\\alpha \\in \\mathbb{R}$.\n- **Loss Function**: Squared error $L(y) = (y - t)^{2}$, with target $t \\in \\mathbb{R}$.\n- **Specific Setting**:\n    - Weight $w = 1$.\n    - Bias $b = 0$.\n    - PReLU parameter $\\alpha = -\\frac{1}{3}$.\n    - Inputs $x_{1} = -2$ and $x_{2} = -1$.\n    - Target $t = 1$.\n- **Tasks**:\n    1.  Provide a counterexample to show $y(x)$ is not monotonically non-decreasing for negative inputs using $x_1$ and $x_2$.\n    2.  Compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial \\alpha}$ at $x = x_1 = -2$.\n    3.  Explain how the signs of these gradients relate to potential training instability when $\\alpha  0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the domain of deep learning, specifically concerning activation functions and backpropagation. The definitions for the model, PReLU activation, and loss function are standard. The problem is well-posed, providing all necessary constants and variables for the required calculations and analysis. The language is precise and objective. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe model is defined by the following equations:\nPreactivation: $z = w x + b$\nOutput: $y = f(z) = \\begin{cases} z  \\text{if } z \\geq 0 \\\\ \\alpha z  \\text{if } z  0 \\end{cases}$\nLoss: $L = (y - t)^2$\n\nWe are given the specific parameters $w=1$, $b=0$, and $\\alpha = -1/3$. The model simplifies to:\n$z = x$\n$y = f(x) = \\begin{cases} x  \\text{if } x \\geq 0 \\\\ -\\frac{1}{3} x  \\text{if } x  0 \\end{cases}$\n\n#### 1. Counterexample for Monotonicity\n\nTo show that $y$ as a function of $x$ is not monotonically non-decreasing for negative inputs, we must find $x_a$ and $x_b$ such that $x_a  x_b  0$ but $y(x_a)  y(x_b)$. We use the provided inputs $x_1 = -2$ and $x_2 = -1$.\nWe have $x_1  x_2$.\n\n- Evaluate $y$ at $x_1 = -2$:\nSince $x_1 = -2  0$, we use the second case of the piecewise function for $y$.\n$y_1 = f(x_1) = \\alpha x_1 = \\left(-\\frac{1}{3}\\right)(-2) = \\frac{2}{3}$.\n\n- Evaluate $y$ at $x_2 = -1$:\nSince $x_2 = -1  0$, we again use the second case.\n$y_2 = f(x_2) = \\alpha x_2 = \\left(-\\frac{1}{3}\\right)(-1) = \\frac{1}{3}$.\n\n- Compare the outputs:\nWe have $x_1 = -2  x_2 = -1$, but we find that $y_1 = \\frac{2}{3}  y_2 = \\frac{1}{3}$. This violates the condition for a non-decreasing function, $y(x_1) \\leq y(x_2)$. Thus, we have demonstrated with a concrete counterexample that for $\\alpha  0$, the PReLU activation function is not monotonic in the negative domain; it is, in fact, decreasing.\n\n#### 2. Gradient Computation\n\nWe need to compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial \\alpha}$ for the input $x = x_1 = -2$, with $w=1$, $b=0$, $\\alpha = -1/3$, and $t=1$.\n\nFirst, we calculate the intermediate values for this input:\n- Preactivation: $z = w x + b = (1)(-2) + 0 = -2$.\n- Output: Since $z = -2  0$, $y = \\alpha z = \\left(-\\frac{1}{3}\\right)(-2) = \\frac{2}{3}$.\n\nNext, we use the chain rule to find the gradients. The required partial derivatives are:\n- $\\frac{\\partial L}{\\partial y} = 2(y - t)$\n- $\\frac{\\partial y}{\\partial z} = f'(z) = \\begin{cases} 1  \\text{if } z  0 \\\\ \\alpha  \\text{if } z  0 \\end{cases}$\n- $\\frac{\\partial z}{\\partial w} = x$\n- $\\frac{\\partial y}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(\\alpha z) = z$ (since $z0$)\n\nNow we evaluate these partial derivatives at the given point:\n- $\\frac{\\partial L}{\\partial y} = 2\\left(\\frac{2}{3} - 1\\right) = 2\\left(-\\frac{1}{3}\\right) = -\\frac{2}{3}$.\n- Since $z = -2  0$, $\\frac{\\partial y}{\\partial z} = \\alpha = -\\frac{1}{3}$.\n- $\\frac{\\partial z}{\\partial w} = x = -2$.\n- Since $z = -2  0$, $\\frac{\\partial y}{\\partial \\alpha} = z = -2$.\n\nNow we combine these using the chain rule to find the final gradients.\n\n- **Gradient with respect to $w$**:\n$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w} $$\n$$ \\frac{\\partial L}{\\partial w} = \\left(-\\frac{2}{3}\\right) \\left(-\\frac{1}{3}\\right) (-2) = \\left(\\frac{2}{9}\\right) (-2) = -\\frac{4}{9} $$\n\n- **Gradient with respect to $\\alpha$**:\n$$ \\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\alpha} $$\n$$ \\frac{\\partial L}{\\partial \\alpha} = \\left(-\\frac{2}{3}\\right) (-2) = \\frac{4}{3} $$\n\nThe requested gradients are $\\frac{\\partial L}{\\partial w} = -\\frac{4}{9}$ and $\\frac{\\partial L}{\\partial \\alpha} = \\frac{4}{3}$.\n\n#### 3. Relation to Training Instability\n\nThe signs of the gradients $\\frac{\\partial L}{\\partial w}  0$ and $\\frac{\\partial L}{\\partial \\alpha}  0$ reveal potential issues in training when $\\alpha  0$.\n\n1.  **Gradient Sign Flipping**: The gradient of the loss with respect to the preactivation is $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} = \\frac{\\partial L}{\\partial y} \\alpha$. In our case, the output $y = 2/3$ is less than the target $t=1$, so the output error gradient $\\frac{\\partial L}{\\partial y} = -2/3$ is negative. Because $\\alpha = -1/3$ is also negative, the backpropagated gradient is $\\frac{\\partial L}{\\partial z} = (-2/3)(-1/3) = 2/9$, which is positive. This means the sign of the error signal is flipped when passing backward through the activation function for negative inputs. While mathematically correct, this behavior is unconventional. A standard Leaky ReLU with $\\alpha  0$ would preserve the sign of the error gradient. This sign flip in the error signal for the pre-activation directly leads to the sign of our weight gradient: $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} x = (2/9)(-2) = -4/9$. A positive $\\alpha$ would have resulted in a positive weight gradient. This dependence of the weight gradient's sign on the sign of $\\alpha$ can lead to complex and potentially unstable dynamics during training.\n\n2.  **Conflicting Updates from Non-Monotonicity**: The core issue with $\\alpha  0$ is the non-monotonic nature of the activation function, as shown in Task 1. Consider training with a batch of data containing multiple negative inputs. Because the function is decreasing for $x  0$, it's possible for some data points to require an increase in $\\alpha$ to reduce their loss, while other data points require a decrease. For example, if we have a point $(x_a, t_a)$ where the output $y_a = \\alpha x_a$ is too high, and another point $(x_b, t_b)$ where $y_b = \\alpha x_b$ is too low, the gradients $\\frac{\\partial L_a}{\\partial \\alpha}$ and $\\frac{\\partial L_b}{\\partial \\alpha}$ may have opposite signs. These conflicting update signals for the shared parameter $\\alpha$ can cause training to oscillate or stagnate, which is a form of training instability. The opposite signs of the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial \\alpha}$ for our single data point are a symptom of the unusual, coupled optimization landscape created by the non-monotonic activation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{4}{9}  \\frac{4}{3}\n\\end{pmatrix}\n}\n$$", "id": "3142552"}, {"introduction": "The previous practice demonstrated why we should constrain $\\alpha$ to be non-negative. This final exercise bridges that theoretical insight with practical implementation, challenging you to enforce the constraint $\\alpha \\ge 0$ in code. You will use reparameterization, a standard deep learning technique, to build a PReLU unit that maintains monotonicity by design, contrasting its behavior with an unconstrained version [@problem_id:3142517].", "problem": "Consider the piecewise-linear activation families used in deep learning: leaky rectified linear unit (leaky ReLU) and parametric rectified linear unit (PReLU). A single-unit PReLU applied to a scalar pre-activation $z$ is defined by the function $f_{\\alpha}(z)$ where $f_{\\alpha}(z) = \\max(0,z) + \\alpha \\min(0,z)$ and the scalar parameter $\\alpha \\in \\mathbb{R}$. The leaky rectified linear unit is recovered when $\\alpha$ is a fixed constant, whereas PReLU learns $\\alpha$ from data.\n\nFrom the core definitions of piecewise-linear functions and the definition of derivative, a function $g(z)$ is monotonically nondecreasing in its argument $z$ if its derivative $g'(z)$ is greater than or equal to $0$ for all $z$. For $f_{\\alpha}(z)$, monotonicity in each unit (as a function of its input $z$) requires a constraint on $\\alpha$.\n\nYour task is to:\n- Propose and implement a parameterization that enforces the monotonicity of each unit by ensuring $\\alpha \\ge 0$ throughout training (for example, by an appropriate reparameterization).\n- Show violations when $\\alpha$ is unconstrained and can become negative, and quantify their empirical impact.\n\nUse the following fundamental base:\n- Mean squared error (MSE) loss: for predictions $\\hat{y}_i$ and targets $y_i$, the average loss is $L = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$.\n- Gradient descent update: for a parameter $\\theta$, the update is $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L$, where $\\eta  0$ is the learning rate.\n- Chain rule for differentiation: if $\\alpha = h(\\beta)$, then $\\nabla_{\\beta} L = \\nabla_{\\alpha} L \\cdot h'(\\beta)$.\n\nExperimental setup to be implemented:\n- Construct a synthetic dataset of $N = 2001$ evenly spaced scalars $z_i$ over the interval $[-2,2]$.\n- Define the target as the standard rectified linear unit output $y_i = \\max(0,z_i)$.\n- Use a single-unit model with output $\\hat{y}_i = f_{\\alpha}(z_i)$, where the only trainable parameter is $\\alpha$ and the linear pre-activation is simply $z$ (no additional weights or biases).\n- Train with plain gradient descent for a specified number of epochs, computing exact analytical gradients from first principles.\n\nMonotonicity assessment and violation count:\n- A unit is monotonically nondecreasing if and only if $\\alpha \\ge 0$, because $f'_{\\alpha}(z) = 1$ for $z  0$ and $f'_{\\alpha}(z) = \\alpha$ for $z  0$. Report a boolean indicating monotonicity and an integer count of strict violations over the dataset grid, defined as the number of $z_i  0$ points where the local slope is negative (this equals $0$ when $\\alpha \\ge 0$ and equals the count of negative $z_i$ when $\\alpha  0$).\n\nConstraint design:\n- Implement a constrained variant by reparameterizing $\\alpha$ with a nonnegative mapping (for example, the softplus function), such that $\\alpha = \\log(1 + e^{\\beta})$ for a free parameter $\\beta \\in \\mathbb{R}$. This enforces $\\alpha \\ge 0$ for all $\\beta$. The derivative needed for the chain rule is $d\\alpha/d\\beta = \\frac{1}{1 + e^{-\\beta}}$.\n\nTest suite:\nRun the following four test cases, each specified as a tuple $(\\text{mode}, \\text{init}, \\eta, E)$, where $\\text{mode}$ is either \"unconstrained\" (directly optimizing $\\alpha$) or \"softplus\" (optimizing $\\beta$ with $\\alpha=\\log(1+e^{\\beta})$), $\\text{init}$ is the initial value ($\\alpha_0$ for \"unconstrained\" or $\\beta_0$ for \"softplus\"), $\\eta$ is the learning rate, and $E$ is the number of epochs:\n1. Unconstrained with negative initialization: $(\\text{\"unconstrained\"}, \\alpha_0=-0.5, \\eta=0.05, E=10)$.\n2. Unconstrained with positive initialization: $(\\text{\"unconstrained\"}, \\alpha_0=0.5, \\eta=0.05, E=10)$.\n3. Constrained softplus with moderate negative initialization: $(\\text{\"softplus\"}, \\beta_0=-1.0, \\eta=0.10, E=10)$.\n4. Constrained softplus near the boundary: $(\\text{\"softplus\"}, \\beta_0=-10.0, \\eta=0.10, E=10)$.\n\nFor each test case, after training, compute and return:\n- The final value of $\\alpha$ as a float.\n- The monotonicity boolean for the unit, defined as $(\\alpha \\ge 0)$.\n- The final mean squared error $L$ over the entire dataset as a float.\n- The integer violation count over the grid, as defined above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list containing $[\\alpha_{\\text{final}}, \\text{is\\_monotone}, L_{\\text{final}}, \\text{violation\\_count}]$. For example, a valid output format is $[[0.0,\\text{True},0.123,0],[\\dots]]$.", "solution": "The problem requires an analysis of the PReLU activation function, $f_{\\alpha}(z) = \\max(0,z) + \\alpha \\min(0,z)$, specifically concerning the enforcement of its monotonicity. A function is monotonically nondecreasing if its derivative is non-negative everywhere. The derivative of $f_{\\alpha}(z)$ with respect to its input $z$ is piecewise constant:\n$$\nf'_{\\alpha}(z) = \\frac{d}{dz} f_{\\alpha}(z) =\n\\begin{cases}\n1  \\text{if } z  0 \\\\\n\\alpha  \\text{if } z  0\n\\end{cases}\n$$\nFor the function to be monotonically nondecreasing, its derivative must be greater than or equal to $0$ for all $z$ where it is defined. This imposes the constraint $\\alpha \\ge 0$. The task is to implement and compare two optimization strategies for a single PReLU unit tasked with learning a standard ReLU function: an unconstrained optimization of $\\alpha$ and a constrained optimization that guarantees $\\alpha \\ge 0$.\n\nThe experimental setup involves a synthetic dataset of $N=2001$ points $z_i$ uniformly spaced in $[-2, 2]$. The target output is $y_i = \\max(0, z_i)$, and the model's prediction is $\\hat{y}_i = f_{\\alpha}(z_i)$. The objective is to find the parameter $\\alpha$ that minimizes the Mean Squared Error (MSE) loss function:\n$$\nL(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n$$\nSubstituting the expressions for $y_i$ and $\\hat{y}_i$:\n$$\ny_i - \\hat{y}_i = \\max(0, z_i) - (\\max(0, z_i) + \\alpha \\min(0, z_i)) = -\\alpha \\min(0, z_i)\n$$\nThe loss function simplifies to:\n$$\nL(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N} (-\\alpha \\min(0, z_i))^2 = \\frac{\\alpha^2}{N} \\sum_{i=1}^{N} (\\min(0, z_i))^2\n$$\nThe term $(\\min(0, z_i))^2$ is non-zero only when $z_i  0$, in which case it equals $z_i^2$. Let the set of indices for which $z_i  0$ be $\\mathcal{I}^- = \\{i \\mid z_i  0\\}$. The loss can be written as:\n$$\nL(\\alpha) = \\frac{\\alpha^2}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2\n$$\nThis simplified form reveals that the loss is a simple quadratic function of $\\alpha$, with a global minimum at $\\alpha=0$. This is expected, as $\\alpha=0$ recovers the target function $y=\\max(0,z)$.\n\nWe will now derive the gradients for the two optimization modes.\n\n**Unconstrained Optimization**\nIn this mode, we directly optimize $\\alpha \\in \\mathbb{R}$ using gradient descent. The gradient of the loss function with respect to $\\alpha$ is:\n$$\n\\nabla_{\\alpha} L = \\frac{\\partial L}{\\partial \\alpha} = \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^2}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right) = \\frac{2\\alpha}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2\n$$\nThe gradient descent update rule for $\\alpha$ at each epoch is:\n$$\n\\alpha \\leftarrow \\alpha - \\eta \\, \\nabla_{\\alpha} L = \\alpha - \\eta \\left( \\frac{2\\alpha}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right)\n$$\nwhere $\\eta$ is the learning rate. If the initial value $\\alpha_0$ is negative, $\\alpha$ will approach $0$ from the negative side, thus violating the monotonicity condition $\\alpha \\ge 0$ during training.\n\n**Constrained Optimization via Softplus Reparameterization**\nTo enforce the constraint $\\alpha \\ge 0$, we reparameterize $\\alpha$ using a function whose range is non-negative. We choose the softplus function, defining $\\alpha$ in terms of a new unconstrained parameter $\\beta \\in \\mathbb{R}$:\n$$\n\\alpha(\\beta) = \\log(1 + e^{\\beta})\n$$\nThis ensures $\\alpha(\\beta) \\ge 0$ for any real-valued $\\beta$. We now perform gradient descent on $\\beta$. The gradient $\\nabla_{\\beta} L$ is found using the chain rule:\n$$\n\\nabla_{\\beta} L = \\frac{\\partial L}{\\partial \\beta} = \\frac{\\partial L}{\\partial \\alpha} \\frac{d\\alpha}{d\\beta}\n$$\nWe have already computed $\\frac{\\partial L}{\\partial \\alpha}$. The derivative of $\\alpha$ with respect to $\\beta$ is:\n$$\n\\frac{d\\alpha}{d\\beta} = \\frac{d}{d\\beta} \\log(1 + e^{\\beta}) = \\frac{e^{\\beta}}{1 + e^{\\beta}} = \\frac{1}{1 + e^{-\\beta}}\n$$\nThis is the logistic sigmoid function, often denoted $\\sigma(\\beta)$.\nCombining these parts, the gradient with respect to $\\beta$ is:\n$$\n\\nabla_{\\beta} L = \\left( \\frac{2\\alpha(\\beta)}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right) \\cdot \\sigma(\\beta)\n$$\nThe gradient descent update rule for $\\beta$ is:\n$$\n\\beta \\leftarrow \\beta - \\eta \\, \\nabla_{\\beta} L\n$$\nIn this scheme, regardless of the value of $\\beta$, the resulting parameter $\\alpha$ will always be non-negative, thereby preserving the monotonicity of the activation function throughout training. The optimization process will drive $\\beta$ towards $-\\infty$, which in turn drives $\\alpha$ towards its optimal value of $0$.\n\n**Evaluation Metrics**\nFor each test case, we compute four values after the specified number of training epochs:\n1.  **Final $\\alpha$**: The value of the PReLU parameter. For the \"softplus\" mode, this is computed from the final $\\beta$.\n2.  **Monotonicity `is_monotone`**: A boolean value, `True` if $\\alpha_{\\text{final}} \\ge 0$, and `False` otherwise.\n3.  **Final MSE Loss $L_{\\text{final}}$**: The loss calculated with the final $\\alpha$.\n4.  **Violation Count**: The number of points $z_i  0$ in the dataset for which the local slope $f'_{\\alpha}(z_i)$ is negative. This count is equal to the number of negative $z_i$ points if $\\alpha  0$, and $0$ otherwise. For the given dataset, there are $1000$ points with $z_i  0$.\n\nBy running the specified test cases, we can empirically observe the consequence of the unconstrained approach (potential for non-monotonicity) and the guaranteed enforcement of monotonicity by the softplus reparameterization.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates unconstrained and constrained optimization\n    for a single PReLU unit learning a ReLU function.\n    \"\"\"\n    # Define dataset parameters\n    N = 2001\n    z_min, z_max = -2.0, 2.0\n    z = np.linspace(z_min, z_max, N, dtype=np.float64)\n\n    # Define target output (standard ReLU)\n    y = np.maximum(0, z)\n\n    # Pre-calculate components that are constant throughout training\n    z_neg = z[z  0]\n    sum_z_neg_sq = np.sum(z_neg**2)\n    num_z_neg = len(z_neg)\n\n    # Test suite: (mode, initial_value, learning_rate, epochs)\n    test_cases = [\n        (\"unconstrained\", -0.5, 0.05, 10),\n        (\"unconstrained\", 0.5, 0.05, 10),\n        (\"softplus\", -1.0, 0.10, 10),\n        (\"softplus\", -10.0, 0.10, 10),\n    ]\n\n    results = []\n\n    for mode, init_val, eta, epochs in test_cases:\n        if mode == \"unconstrained\":\n            # Directly optimize alpha\n            alpha = float(init_val)\n            \n            for _ in range(epochs):\n                # Gradient of Loss w.r.t. alpha\n                # L(alpha) = (alpha^2 / N) * sum(z_i^2 for z_i  0)\n                # dL/d_alpha = (2 * alpha / N) * sum(z_i^2 for z_i  0)\n                grad_alpha = (2.0 * alpha / N) * sum_z_neg_sq\n                \n                # Gradient descent update\n                alpha -= eta * grad_alpha\n                \n            final_alpha = alpha\n\n        elif mode == \"softplus\":\n            # Optimize beta, where alpha = log(1 + exp(beta))\n            beta = float(init_val)\n\n            for _ in range(epochs):\n                # Calculate alpha from beta using a numerically stable method\n                # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x))\n                alpha = np.logaddexp(0, beta)\n                \n                # Derivative of alpha w.r.t. beta (sigmoid function)\n                # d_alpha/d_beta = exp(beta)/(1+exp(beta)) = 1/(1+exp(-beta))\n                # This is numerically stable.\n                d_alpha_d_beta = 1.0 / (1.0 + np.exp(-beta))\n\n                # Gradient of Loss w.r.t. alpha\n                grad_alpha = (2.0 * alpha / N) * sum_z_neg_sq\n                \n                # Gradient of Loss w.r.t. beta (using chain rule)\n                grad_beta = grad_alpha * d_alpha_d_beta\n                \n                # Gradient descent update\n                beta -= eta * grad_beta\n            \n            final_alpha = np.logaddexp(0, beta)\n\n        # Calculate final metrics after training\n        is_monotone = (final_alpha = 0.0)\n        \n        # Final loss: L = (alpha^2 / N) * sum_z_neg_sq\n        final_loss = (final_alpha**2 / N) * sum_z_neg_sq\n        \n        # Violation count: number of negative slopes for z  0\n        violation_count = num_z_neg if final_alpha  0.0 else 0\n        \n        results.append([\n            float(final_alpha),\n            bool(is_monotone),\n            float(final_loss),\n            int(violation_count)\n        ])\n\n    # The final print statement must follow the specified format:\n    # A string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3142517"}]}