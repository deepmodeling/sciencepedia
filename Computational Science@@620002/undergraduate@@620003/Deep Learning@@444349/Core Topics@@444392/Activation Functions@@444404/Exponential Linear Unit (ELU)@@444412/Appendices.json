{"hands_on_practices": [{"introduction": "Before using any new tool in deep learning, it's crucial to build a strong intuition for its behavior. This first practice invites you to explore the fundamental properties of the Exponential Linear Unit (ELU) function through a thought experiment. By considering how to test for negative saturation and continuity, you will gain a clear mental model of the ELU's shape and its response to a range of inputs [@problem_id:3185350].", "problem": "A single-neuron layer is used to study forward propagation with the Exponential Linear Unit (ELU) activation. The neuron takes a scalar input $x$, computes the pre-activation $z$ by the standard forward rule $z = w x + b$, and outputs $y = \\phi(z)$, where $\\phi$ is the ELU with parameter $\\alpha > 0$ defined by\n$$\n\\phi(z) = \\begin{cases}\nz, & z > 0,\\\\\n\\alpha\\left(\\exp(z) - 1\\right), & z \\le 0.\n\\end{cases}\n$$\nAssume $w = 1$ and $b = 0$, so that $z = x$ for all inputs. You want to design a small set of test inputs to empirically probe two distinct forward behaviors when $\\alpha$ is large (for example $\\alpha = 100$): (i) negative-region saturation, and (ii) continuity of the forward map at $x = 0$. Using only the forward propagation definitions above and the definition of continuity (the limit equals the function value at the point), choose the option that correctly proposes a suitable input set and correctly states the analytical truth about continuity at $x = 0$ for large $\\alpha$.\n\nA. Use inputs $\\{ -10^{1}, -10^{-6}, 0, 10^{-6}, 10^{1} \\}$ to probe both the extreme negative regime and a tight neighborhood around $0$ on both sides; the ELU forward map is continuous at $x = 0$ for any $\\alpha > 0$, including large $\\alpha$.\n\nB. Use inputs $\\{ 10^{3}, 10^{4} \\}$ because very large positive inputs are sufficient to observe negative saturation; the ELU forward map is not continuous at $x = 0$ when $\\alpha$ is large.\n\nC. Use inputs $\\{ -10^{2}, -10^{1}, -1 \\}$ to reveal negative saturation and $\\{ -10^{-6}, 10^{-6} \\}$ to check behavior near $0$ (no need to include $0$ itself); the ELU forward map has a removable discontinuity at $x = 0$ whenever $\\alpha \\ne 1$.\n\nD. Use input $\\{ -10^{6} \\}$ to see that ELU saturates to $-\\infty$ as $x \\to -\\infty$; the ELU forward map is continuous at $x = 0$ only if $\\alpha$ is sufficiently small.", "solution": "The user wants a critical analysis of a problem concerning the forward propagation behavior of a single neuron using the Exponential Linear Unit (ELU) activation function.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- A single-neuron layer with a scalar input $x$.\n- Pre-activation function: $z = w x + b$.\n- Activation function (output): $y = \\phi(z)$, where $\\phi$ is the ELU function.\n- ELU function definition:\n$$\n\\phi(z) = \\begin{cases}\nz, & z > 0,\\\\\n\\alpha\\left(\\exp(z) - 1\\right), & z \\le 0.\n\\end{cases}\n$$\n- Parameter constraint: $\\alpha > 0$.\n- Specific neuron parameters: weight $w = 1$ and bias $b = 0$.\n- Implication of parameters: $z = x$ for all inputs. The output is $y = \\phi(x)$.\n- A specific condition to consider: $\\alpha$ is large (e.g., $\\alpha = 100$).\n- The task is to choose an option that correctly proposes a set of test inputs to empirically probe two behaviors: (i) negative-region saturation, and (ii) continuity of the forward map at $x = 0$. The option must also correctly state the analytical truth about continuity at $x = 0$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is scientifically sound. The ELU function is a standard activation function in deep learning and machine learning. Its definition is provided correctly. The concepts of forward propagation, saturation, and mathematical continuity are fundamental and correctly invoked.\n2.  **Well-Posed**: The problem is well-posed. The function $\\phi(x)$ is explicitly defined, and the properties to be investigated (saturation and continuity) are standard mathematical concepts. The question asks for an appropriate set of test points and a correct analytical statement, which allows for a unique, correct answer among the options.\n3.  **Objective**: The problem is stated in objective, mathematical language. There are no subjective or ambiguous terms.\n4.  **Completeness and Consistency**: The problem statement is self-contained. All necessary definitions and parameters ($w$, $b$, $\\phi(z)$, $\\alpha>0$) are provided. The simplification $z = x$ is clearly stated. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-defined exercise in analyzing the properties of a standard activation function. I will now proceed to solve the problem and evaluate the options.\n\n### Solution Derivation\n\nGiven $w=1$ and $b=0$, the pre-activation is $z=x$ and the output is $y = \\phi(x)$. The function is:\n$$\ny = \\phi(x) = \\begin{cases}\nx, & x > 0,\\\\\n\\alpha\\left(\\exp(x) - 1\\right), & x \\le 0.\n\\end{cases}\n$$\nWe need to analyze two properties: negative-region saturation and continuity at $x=0$.\n\n**1. Negative-Region Saturation**\nSaturation in the negative region refers to the behavior of the function as its input approaches $-\\infty$. We need to compute the limit of $\\phi(x)$ as $x \\to -\\infty$.\nFor $x < 0$, the function is $\\phi(x) = \\alpha(\\exp(x) - 1)$.\nThe limit is:\n$$\n\\lim_{x \\to -\\infty} \\phi(x) = \\lim_{x \\to -\\infty} \\alpha\\left(\\exp(x) - 1\\right)\n$$\nSince $\\lim_{x \\to -\\infty} \\exp(x) = 0$, we have:\n$$\n\\lim_{x \\to -\\infty} \\phi(x) = \\alpha(0 - 1) = -\\alpha\n$$\nThe function $\\phi(x)$ saturates to the constant value $-\\alpha$ as $x$ becomes very large in the negative direction. To empirically probe this, we need to choose input values of $x$ that are large and negative, for example, $x = -10$ or $x = -100$. For such inputs, $\\exp(x)$ will be very close to $0$, and $\\phi(x)$ will be very close to $-\\alpha$.\n\n**2. Continuity at $x=0$**\nFor a function to be continuous at a point $c$, three conditions must be met:\n(i) $f(c)$ is defined.\n(ii) $\\lim_{x \\to c} f(x)$ exists.\n(iii) $\\lim_{x \\to c} f(x) = f(c)$.\n\nLet's check these conditions for $\\phi(x)$ at $x=0$.\n\n(i) The value of the function at $x=0$ is defined by the second case of the piecewise function:\n$$\n\\phi(0) = \\alpha\\left(\\exp(0) - 1\\right) = \\alpha(1 - 1) = 0\n$$\n\n(ii) To check if the limit exists, we must evaluate the left-hand and right-hand limits and see if they are equal.\nThe right-hand limit (as $x$ approaches $0$ from the positive side, $x > 0$):\n$$\n\\lim_{x \\to 0^+} \\phi(x) = \\lim_{x \\to 0^+} x = 0\n$$\nThe left-hand limit (as $x$ approaches $0$ from the negative side, $x < 0$):\n$$\n\\lim_{x \\to 0^-} \\phi(x) = \\lim_{x \\to 0^-} \\alpha\\left(\\exp(x) - 1\\right) = \\alpha\\left(\\exp(0) - 1\\right) = \\alpha(1 - 1) = 0\n$$\nSince the left-hand limit ($0$) equals the right-hand limit ($0$), the limit exists and is equal to $0$.\n$$\n\\lim_{x \\to 0} \\phi(x) = 0\n$$\n\n(iii) We compare the limit with the function's value:\n$$\n\\lim_{x \\to 0} \\phi(x) = 0 \\quad \\text{and} \\quad \\phi(0) = 0\n$$\nSince $\\lim_{x \\to 0} \\phi(x) = \\phi(0)$, the function $\\phi(x)$ is continuous at $x=0$.\nCrucially, this result holds for **any** value of $\\alpha > 0$. The size of $\\alpha$ does not affect the continuity of the function at $x=0$, although it does affect the function's derivative at that point.\n\nTo empirically probe continuity at $x=0$, a suitable set of inputs would include points very close to $0$ from both sides, such as $-10^{-6}$ and $10^{-6}$, as well as the point $0$ itself.\n\n**Summary for a suitable test set:**\n- To test saturation: at least one large negative number (e.g., $-10^1$).\n- To test continuity at $0$: a tight cluster of points around $0$ (e.g., $-10^{-6}, 0, 10^{-6}$).\n- To test the positive linear region: a positive number (e.g., $10^1$).\nA comprehensive set would be $\\{ -10^1, -10^{-6}, 0, 10^{-6}, 10^1 \\}$.\n\n### Option-by-Option Analysis\n\n**A. Use inputs $\\{ -10^{1}, -10^{-6}, 0, 10^{-6}, 10^{1} \\}$ to probe both the extreme negative regime and a tight neighborhood around $0$ on both sides; the ELU forward map is continuous at $x = 0$ for any $\\alpha > 0$, including large $\\alpha$.**\n- **Input Set:** The proposed input set is excellent. The input $-10^1$ probes the negative saturation region. The inputs $-10^{-6}$, $0$, and $10^{-6}$ form a tight neighborhood around $0$ to empirically check for continuity. The input $10^1$ tests the linear behavior for positive inputs. This set is well-designed for the stated goals.\n- **Continuity Statement:** The statement that \"the ELU forward map is continuous at $x = 0$ for any $\\alpha > 0$, including large $\\alpha$\" is analytically correct, as proven above.\n- **Verdict:** Correct.\n\n**B. Use inputs $\\{ 10^{3}, 10^{4} \\}$ because very large positive inputs are sufficient to observe negative saturation; the ELU forward map is not continuous at $x = 0$ when $\\alpha$ is large.**\n- **Input Set:** The proposed inputs are large and positive. These inputs lie in the region where $\\phi(x)=x$. They provide no information whatsoever about the negative region ($x \\le 0$) or the saturation behavior. The justification is nonsensical.\n- **Continuity Statement:** The statement that the function is \"not continuous at $x = 0$ when $\\alpha$ is large\" is false. We proved continuity for all $\\alpha > 0$.\n- **Verdict:** Incorrect.\n\n**C. Use inputs $\\{ -10^{2}, -10^{1}, -1 \\}$ to reveal negative saturation and $\\{ -10^{-6}, 10^{-6} \\}$ to check behavior near $0$ (no need to include $0$ itself); the ELU forward map has a removable discontinuity at $x = 0$ whenever $\\alpha \\ne 1$.**\n- **Input Set:** The suggested inputs are reasonable for probing the specified behaviors.\n- **Continuity Statement:** The statement that the function \"has a removable discontinuity at $x = 0$ whenever $\\alpha \\ne 1$\" is false. A removable discontinuity occurs if the limit exists but does not equal the function's value. Here, the limit exists and equals the function's value ($0$), meaning the function is continuous. The condition $\\alpha \\ne 1$ is relevant for the continuity of the *derivative* $\\phi'(x)$ at $x=0$, but the question is about the continuity of the function $\\phi(x)$ itself.\n- **Verdict:** Incorrect.\n\n**D. Use input $\\{ -10^{6} \\}$ to see that ELU saturates to $-\\infty$ as $x \\to -\\infty$; the ELU forward map is continuous at $x = 0$ only if $\\alpha$ is sufficiently small.**\n- **Input Set:** A single input point is insufficient to test both saturation and continuity at $0$.\n- **Saturation Statement:** The claim that \"ELU saturates to $-\\infty$\" is false. As derived, the function saturates to the finite value $-\\alpha$.\n- **Continuity Statement:** The claim that continuity at $x = 0$ holds \"only if $\\alpha$ is sufficiently small\" is false. We proved continuity for all $\\alpha > 0$.\n- **Verdict:** Incorrect.\n\nBased on the detailed analysis, option A is the only one that provides both a suitable experimental design (input set) and a correct analytical statement.", "answer": "$$\\boxed{A}$$", "id": "3185350"}, {"introduction": "A key requirement for any activation function used in gradient-based learning is the ability to backpropagate error signals through it. This exercise transitions from the ELU's forward-pass characteristics to its role in the backward pass. You will apply the multivariate chain rule to compute the gradient of a loss function with respect to the input, paying special attention to the non-differentiable \"kink\" at the origin [@problem_id:3190277].", "problem": "Consider a scalar input $x \\in \\mathbb{R}$ that feeds into an Exponential Linear Unit (ELU) activation $g(x)$ with parameter $\\alpha \\in (0, \\infty)$, defined by\n$$\ng(x) = \n\\begin{cases}\nx & \\text{if } x > 0, \\\\\n\\alpha\\left(\\exp(x) - 1\\right) & \\text{if } x \\le 0.\n\\end{cases}\n$$\nThis activation fans out to a two-branch network:\n- Branch 1 computes $y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$, where $\\sigma(z)$ is the logistic sigmoid, $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$, and $w_{1}, b_{1} \\in \\mathbb{R}$ are fixed parameters.\n- Branch 2 computes $y_{2} = \\left(g(x)\\right)^{2}$.\n\nThe scalar loss is the half-sum of squared errors\n$$\nL = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2},\n$$\nwith fixed targets $t_{1}, t_{2} \\in \\mathbb{R}$.\n\nUsing only foundational definitions of derivatives and the multivariate chain rule, derive the backpropagation expressions for $\\frac{\\partial L}{\\partial x}$ for $x > 0$ and $x < 0$ by propagating derivatives through both branches. Then, evaluate $\\frac{\\partial L}{\\partial x}$ at the kink $x = 0$ using subgradients. For the nondifferentiable point $x = 0$, adopt the convention that backpropagation selects the right-hand derivative of the ELU (i.e., take $\\frac{d g}{d x}\\big|_{x=0}$ equal to the derivative from $x > 0$). Provide your final expression for $\\frac{\\partial L}{\\partial x}$ at $x = 0$ in terms of $w_{1}$, $b_{1}$, $t_{1}$, and standard functions. No numerical rounding is required.", "solution": "The problem is valid. It is a well-posed problem in calculus and its application to neural networks, a standard topic in deep learning. The problem is scientifically grounded, self-contained, and objective. The provided convention for handling the non-differentiable point of the ELU activation function makes the problem unambiguous.\n\nThe objective is to find the derivative of the scalar loss $L$ with respect to the scalar input $x$, denoted as $\\frac{\\partial L}{\\partial x}$. The loss $L$ depends on $x$ through a series of intermediate computations involving the activation function $g(x)$. The overall dependency graph is:\n$x \\rightarrow g(x)$\n$g(x) \\rightarrow y_{1} \\rightarrow L$\n$g(x) \\rightarrow y_{2} \\rightarrow L$\n\nThe total derivative $\\frac{\\partial L}{\\partial x}$ can be found using the multivariate chain rule. Since the input $x$ influences the loss $L$ through a single intermediate variable $g(x)$, which then branches out, the chain rule can be structured as:\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial g(x)} \\frac{d g(x)}{d x}\n$$\nThe first term, $\\frac{\\partial L}{\\partial g(x)}$, accounts for the two branches originating from $g(x)$. It is the sum of the partial derivatives along each path:\n$$\n\\frac{\\partial L}{\\partial g(x)} = \\frac{\\partial L}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial g(x)} + \\frac{\\partial L}{\\partial y_{2}} \\frac{\\partial y_{2}}{\\partial g(x)}\n$$\nWe will compute each component of this expression.\n\nFirst, we compute the derivatives of the loss $L$ with respect to its direct inputs $y_{1}$ and $y_{2}$.\nThe loss is $L = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2}$.\nThe partial derivatives are:\n$$\n\\frac{\\partial L}{\\partial y_{1}} = y_{1} - t_{1}\n$$\n$$\n\\frac{\\partial L}{\\partial y_{2}} = y_{2} - t_{2}\n$$\n\nNext, we compute the derivatives of $y_{1}$ and $y_{2}$ with respect to $g(x)$.\nFor Branch 1, $y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$. Let $z_{1} = w_{1}\\,g(x) + b_{1}$. The derivative of the logistic sigmoid function $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ is $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$.\nUsing the chain rule for $y_{1}$:\n$$\n\\frac{\\partial y_{1}}{\\partial g(x)} = \\frac{d\\sigma(z_{1})}{dz_{1}} \\frac{\\partial z_{1}}{\\partial g(x)} = \\sigma(z_{1})(1 - \\sigma(z_{1})) \\cdot w_{1} = y_{1}(1 - y_{1})w_{1}\n$$\nFor Branch 2, $y_{2} = \\left(g(x)\\right)^{2}$. The derivative is:\n$$\n\\frac{\\partial y_{2}}{\\partial g(x)} = 2 g(x)\n$$\n\nNow we combine these parts to find $\\frac{\\partial L}{\\partial g(x)}$:\n$$\n\\frac{\\partial L}{\\partial g(x)} = (y_{1} - t_{1}) \\cdot [y_{1}(1 - y_{1})w_{1}] + (y_{2} - t_{2}) \\cdot [2 g(x)]\n$$\n$$\n\\frac{\\partial L}{\\partial g(x)} = w_{1}(y_{1} - t_{1})y_{1}(1 - y_{1}) + 2(y_{2} - t_{2})g(x)\n$$\n\nThe final step is to determine the derivative of the ELU activation, $\\frac{d g(x)}{d x}$. This depends on the value of $x$.\nThe ELU function is defined as:\n$$\ng(x) = \n\\begin{cases}\nx & \\text{if } x > 0, \\\\\n\\alpha\\left(\\exp(x) - 1\\right) & \\text{if } x \\le 0.\n\\end{cases}\n$$\n- For $x > 0$: $g(x) = x$, so $\\frac{d g}{d x} = 1$.\n- For $x < 0$: $g(x) = \\alpha(\\exp(x) - 1)$, so $\\frac{d g}{d x} = \\alpha \\exp(x)$.\n- For $x = 0$: The function is non-differentiable at this point (a \"kink\"), as the left-hand derivative is $\\alpha \\exp(0) = \\alpha$ and the right-hand derivative is $1$. The problem specifies a convention: \"adopt the convention that backpropagation selects the right-hand derivative\". Therefore, we must use:\n$$\n\\frac{d g}{d x}\\bigg|_{x=0} = 1\n$$\n\nWe are asked to find the final expression for $\\frac{\\partial L}{\\partial x}$ at $x = 0$. We evaluate all intermediate quantities at $x=0$:\nFirst, at $x=0$, the value of the activation function is:\n$$\ng(0) = \\alpha(\\exp(0) - 1) = \\alpha(1-1) = 0\n$$\nNext, we evaluate $y_{1}$ and $y_{2}$ at $x=0$:\n$$\ny_{1}\\big|_{x=0} = \\sigma(w_{1} g(0) + b_{1}) = \\sigma(w_{1}(0) + b_{1}) = \\sigma(b_{1})\n$$\n$$\ny_{2}\\big|_{x=0} = (g(0))^{2} = 0^{2} = 0\n$$\nNow we can evaluate the gradient term $\\frac{\\partial L}{\\partial g(x)}$ at $x=0$:\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(y_{1}\\big|_{x=0} - t_{1})y_{1}\\big|_{x=0}(1 - y_{1}\\big|_{x=0}) + 2(y_{2}\\big|_{x=0} - t_{2})g(0)\n$$\nSubstituting the values we just found:\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) + 2(0 - t_{2})(0)\n$$\nThe second term vanishes:\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\nFinally, we compute $\\frac{\\partial L}{\\partial x}$ at $x=0$ using the main chain rule formula and the specified convention for $\\frac{d g}{d x}$:\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left(\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0}\\right) \\left(\\frac{d g}{d x}\\bigg|_{x=0}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left( w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) \\right) \\cdot 1\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\nThis expression is in terms of the required parameters $w_{1}$, $b_{1}$, $t_{1}$, and the standard sigmoid function $\\sigma$, as requested. Note that $t_{2}$ and $\\alpha$ do not appear in the final expression for the derivative at $x=0$.", "answer": "$$\n\\boxed{w_{1}\\left(\\sigma(b_{1}) - t_{1}\\right)\\sigma(b_{1})\\left(1 - \\sigma(b_{1})\\right)}\n$$", "id": "3190277"}, {"introduction": "To unlock the full potential of an architecture, we can sometimes treat hyperparameters like ELU's $\\alpha$ as learnable parameters. This final practice bridges theory and implementation, guiding you to derive the gradient for a learnable $\\alpha$ and verify it numerically. You will also investigate the important practical concept of parameter identifiability, which can affect model training and interpretation [@problem_id:3123807].", "problem": "You are asked to implement and analyze a single hidden layer using the Exponential Linear Unit (ELU), where the ELU parameter is a learnable scalar per layer. The analysis must be grounded in the fundamental definitions of the ELU function and the chain rule for derivatives.\n\nConsider a single hidden layer with pre-activations $z \\in \\mathbb{R}^n$, elapsed through an Exponential Linear Unit (ELU) activation $f(x;\\alpha)$ with a layer-specific learnable parameter $\\alpha \\in \\mathbb{R}$. The ELU function is defined piecewise by $f(x;\\alpha) = x$ for $x > 0$ and $f(x;\\alpha) = \\alpha (\\exp(x) - 1)$ for $x \\le 0$. The layer output is then passed to a one-dimensional linear output $y_{\\text{hat}} = w^\\top f(z;\\alpha) + b$ with weights $w \\in \\mathbb{R}^n$ and bias $b \\in \\mathbb{R}$. The loss is the Mean Squared Error (MSE) $L = \\frac{1}{2}(y_{\\text{hat}} - y)^2$ with target $y \\in \\mathbb{R}$.\n\nTasks:\n- Derive from first principles using the chain rule for derivatives a closed-form expression for the gradient $\\partial L / \\partial \\alpha$ that depends on $z$, $w$, $y_{\\text{hat}}$, $y$, and $\\alpha$. The derivation must start from the definitions of $f(x;\\alpha)$, $y_{\\text{hat}}$, and $L$, and apply the chain rule without using any shortcut formulas.\n- Implement a complete program that:\n  1. Computes the forward pass $y_{\\text{hat}}$, the loss $L$, and the analytical gradient $\\partial L / \\partial \\alpha$ for given inputs.\n  2. Validates the analytical gradient against a numerical gradient computed via central finite differences on $\\alpha$.\n  3. Examines identifiability of $\\alpha$ under layer scaling: discuss and test when scaling $\\alpha$ by a factor $s$ and simultaneously scaling the subsequent linear layer weights $w$ by $1/s$ yields the same output, and when it does not. Identifiability refers to whether different parameter settings produce indistinguishable model outputs for given inputs.\n  4. Covers boundary behavior at $x=0$ and the case of all-positive pre-activations.\n\nUse the following specific test suite:\n- Mixed-sign case for gradient check:\n  - $z_{\\text{mixed}} = [-1.2, 0.8, -0.3]$, $\\alpha = 1.3$, $w = [0.9, -1.1, 0.5]$, $b = 0.2$, $y = 0.7$, finite-difference step $\\varepsilon = 10^{-6}$.\n  - Report the relative error between the analytical and numerical gradients as a float computed as $\\frac{|\\text{analytical} - \\text{numerical}|}{\\max(10^{-12}, |\\text{analytical}| + |\\text{numerical}|)}$.\n- Boundary case at zero:\n  - $z_{0} = [0.0]$, $\\alpha = 1.0$, $w = [1.0]$, $b = 0.0$, $y = 0.0$.\n  - Report the analytical gradient $\\partial L / \\partial \\alpha$ as a float.\n- Identifiability with all-negative pre-activations:\n  - $z_{\\text{neg}} = [-1.0, -0.5]$, $\\alpha = 0.7$, $w = [1.2, -0.8]$, $b = 0.3$, scale factor $s = 2.5$.\n  - Report a boolean indicating whether $w^\\top f(z_{\\text{neg}};\\alpha) + b$ equals $(w/s)^\\top f(z_{\\text{neg}}; s\\alpha) + b$ within a tolerance of $10^{-12}$.\n- Identifiability fails for mixed signs:\n  - $z_{\\text{mix2}} = [-1.0, 0.5]$, use the same $\\alpha$, $w$, $b$, and $s$ as above.\n  - Report a boolean indicating whether $w^\\top f(z_{\\text{mix2}};\\alpha) + b$ equals $(w/s)^\\top f(z_{\\text{mix2}}; s\\alpha) + b$ within a tolerance of $10^{-12}$.\n- All-positive pre-activations:\n  - $z_{\\text{pos}} = [0.4, 0.2]$, $\\alpha = 1.7$, $w = [0.6, -0.3]$, $b = -0.1$, $y = 0.5$.\n  - Report the analytical gradient $\\partial L / \\partial \\alpha$ as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4,result_5]$), in the exact order specified by the test suite above: mixed-case relative error (float), boundary-case gradient at zero (float), all-negative identifiability boolean, mixed-sign identifiability boolean, all-positive-case gradient (float). No other text should be printed.", "solution": "The problem is valid. It is scientifically grounded in the principles of neural network theory, mathematically well-posed, and objectively stated. All necessary data and definitions for a unique solution are provided.\n\n### 1. Derivation of the Gradient $\\partial L / \\partial \\alpha$\n\nThe primary task is to derive a closed-form expression for the gradient of the loss function $L$ with respect to the Exponential Linear Unit (ELU) parameter $\\alpha$. This derivation starts from the provided definitions and applies the chain rule of calculus.\n\nThe relevant quantities are:\n- The ELU activation function, defined for a scalar input $x$ and parameter $\\alpha$:\n$$\nf(x; \\alpha) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha (\\exp(x) - 1) & \\text{if } x \\le 0\n\\end{cases}\n$$\n- The model's output prediction $y_{\\text{hat}}$, for a pre-activation vector $z \\in \\mathbb{R}^n$, weight vector $w \\in \\mathbb{R}^n$, and scalar bias $b$:\n$$\ny_{\\text{hat}} = w^\\top f(z; \\alpha) + b = \\sum_{i=1}^n w_i f(z_i; \\alpha) + b\n$$\nwhere $f(z; \\alpha)$ is applied element-wise to the vector $z$.\n- The Mean Squared Error (MSE) loss function, for a target value $y$:\n$$\nL = \\frac{1}{2}(y_{\\text{hat}} - y)^2\n$$\n\nWe seek to compute $\\frac{\\partial L}{\\partial \\alpha}$ by applying the chain rule:\n$$\n\\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y_{\\text{hat}}} \\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}\n$$\n\nFirst, we compute the derivative of the loss $L$ with respect to the prediction $y_{\\text{hat}}$:\n$$\n\\frac{\\partial L}{\\partial y_{\\text{hat}}} = \\frac{\\partial}{\\partial y_{\\text{hat}}} \\left( \\frac{1}{2}(y_{\\text{hat}} - y)^2 \\right) = y_{\\text{hat}} - y\n$$\n\nNext, we compute the derivative of the prediction $y_{\\text{hat}}$ with respect to the parameter $\\alpha$.\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\sum_{i=1}^n w_i f(z_i; \\alpha) + b \\right)\n$$\nSince differentiation is a linear operator, and $b$ is not a function of $\\alpha$:\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i \\frac{\\partial f(z_i; \\alpha)}{\\partial \\alpha}\n$$\nNow, we must find the derivative of the ELU function $f(x; \\alpha)$ with respect to $\\alpha$. We consider the two cases from its definition:\n- If $x > 0$, then $f(x; \\alpha) = x$. The derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(x) = 0\n$$\n- If $x \\le 0$, then $f(x; \\alpha) = \\alpha (\\exp(x) - 1)$. The derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (\\alpha (\\exp(x) - 1)) = \\exp(x) - 1\n$$\nThis can be expressed compactly using an indicator function, $I(c)$, which is $1$ if condition $c$ is true and $0$ otherwise:\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = (\\exp(x) - 1) \\cdot I(x \\le 0)\n$$\nNote that this derivative is well-defined and continuous at $x=0$, as $\\lim_{x\\to0^-}(\\exp(x)-1) = 0$, which matches the derivative for $x > 0$.\n\nSubstituting this back into the expression for $\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}$:\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i \\le 0)\n$$\nThis sum includes only the terms for which the pre-activation $z_i$ is non-positive.\n\nFinally, we combine the parts to obtain the full expression for $\\frac{\\partial L}{\\partial \\alpha}$:\n$$\n\\frac{\\partial L}{\\partial \\alpha} = (y_{\\text{hat}} - y) \\left( \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i \\le 0) \\right)\n$$\nThis is the closed-form expression for the gradient, which depends only on the given variables $z, w, y_{\\text{hat}}, y$, and $\\alpha$.\n\n### 2. Analysis of Special Cases and Identifiability\n\n- **All-positive pre-activations ($z_i > 0$ for all $i$)**: In this case, $I(z_i \\le 0) = 0$ for all $i$. The sum in the gradient expression becomes zero. Consequently, $\\frac{\\partial L}{\\partial \\alpha} = 0$. This is logical, as for positive inputs, the ELU function behaves as the identity function, $f(z_i; \\alpha) = z_i$, which is independent of $\\alpha$. Therefore, the model's output $y_{\\text{hat}}$ and the loss $L$ do not depend on $\\alpha$, and its gradient must be zero. This is tested in the boundary case ($z=[0.0]$) and the all-positive case.\n\n- **Identifiability under scaling**: We analyze whether scaling $\\alpha$ by a factor $s$ and $w$ by $1/s$ leaves the model output unchanged. Let the new parameters be $\\alpha' = s\\alpha$ and $w' = w/s$. The new output is $y'_{\\text{hat}} = (w')^\\top f(z; \\alpha') + b$.\n\n- **Case A: All-negative pre-activations ($z_i \\le 0$ for all $i$)**:\nFor any component $i$, $f(z_i; \\alpha) = \\alpha(\\exp(z_i)-1)$. The original output is:\n$$y_{\\text{hat}} = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\nThe new output is:\n$$y'_{\\text{hat}} = \\sum_i w'_i f(z_i; \\alpha') + b = \\sum_i \\frac{w_i}{s} \\left( (s\\alpha)(\\exp(z_i)-1) \\right) + b = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\nHere, $y'_{\\text{hat}} = y_{\\text{hat}}$. The model is not identifiable, as different parameter sets $(\\alpha, w)$ and $(s\\alpha, w/s)$ produce the same output for any input $z$ with all-non-positive components.\n\n- **Case B: Mixed-sign pre-activations**:\nSuppose some $z_j > 0$ and some $z_k \\le 0$. The original output is:\n$$y_{\\text{hat}} = \\sum_{i \\mid z_i > 0} w_i z_i + \\sum_{i \\mid z_i \\le 0} w_i \\alpha(\\exp(z_i)-1) + b$$\nThe new output is:\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i > 0} w'_i z_i + \\sum_{i \\mid z_i \\le 0} w'_i \\alpha'(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i > 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i \\le 0} \\frac{w_i}{s} (s\\alpha)(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i > 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i \\le 0} w_i \\alpha(\\exp(z_i)-1) + b$$\nComparing $y_{\\text{hat}}$ and $y'_{\\text{hat}}$, the terms corresponding to non-positive $z_i$ are identical, but the terms for positive $z_i$ are scaled by $1/s$. Unless $s=1$ or all $w_i z_i=0$ for the positive part, $y'_{\\text{hat}} \\neq y_{\\text{hat}}$. In this case, the parameters are identifiable.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes results for all test cases as specified in the problem statement.\n    \"\"\"\n    \n    def elu(z, alpha):\n        \"\"\"Computes the element-wise ELU activation.\"\"\"\n        return np.where(z >= 0, z, alpha * (np.exp(z) - 1))\n\n    def compute_forward_loss(z, alpha, w, b, y):\n        \"\"\"Computes the forward pass and loss.\"\"\"\n        activations = elu(z, alpha)\n        y_hat = np.dot(w, activations) + b\n        loss = 0.5 * (y_hat - y)**2\n        return y_hat, loss\n\n    def analytical_gradient(z, alpha, w, y_hat, y):\n        \"\"\"Computes the analytical gradient dL/d(alpha).\"\"\"\n        # Derivative of ELU with respect to alpha\n        d_elu_d_alpha = np.where(z >= 0, 0.0, np.exp(z) - 1)\n        \n        # Derivative of y_hat with respect to alpha\n        d_yhat_d_alpha = np.dot(w, d_elu_d_alpha)\n        \n        # Derivative of Loss with respect to y_hat\n        d_L_d_yhat = y_hat - y\n        \n        # Final gradient using the chain rule\n        d_L_d_alpha = d_L_d_yhat * d_yhat_d_alpha\n        return d_L_d_alpha\n\n    def numerical_gradient(z, w, b, y, alpha, epsilon):\n        \"\"\"Computes the numerical gradient using central finite differences.\"\"\"\n        _, loss_plus = compute_forward_loss(z, alpha + epsilon, w, b, y)\n        _, loss_minus = compute_forward_loss(z, alpha - epsilon, w, b, y)\n        return (loss_plus - loss_minus) / (2 * epsilon)\n\n    results = []\n\n    # Test Case 1: Mixed-sign case for gradient check\n    z_mixed = np.array([-1.2, 0.8, -0.3])\n    alpha_mixed = 1.3\n    w_mixed = np.array([0.9, -1.1, 0.5])\n    b_mixed = 0.2\n    y_mixed = 0.7\n    epsilon = 1e-6\n    \n    y_hat_mixed, _ = compute_forward_loss(z_mixed, alpha_mixed, w_mixed, b_mixed, y_mixed)\n    grad_analyt = analytical_gradient(z_mixed, alpha_mixed, w_mixed, y_hat_mixed, y_mixed)\n    grad_numer = numerical_gradient(z_mixed, w_mixed, b_mixed, y_mixed, alpha_mixed, epsilon)\n    \n    rel_error_num = np.abs(grad_analyt - grad_numer)\n    rel_error_den = np.maximum(1e-12, np.abs(grad_analyt) + np.abs(grad_numer))\n    relative_error = rel_error_num / rel_error_den\n    results.append(relative_error)\n    \n    # Test Case 2: Boundary case at zero\n    z_0 = np.array([0.0])\n    alpha_0 = 1.0\n    w_0 = np.array([1.0])\n    b_0 = 0.0\n    y_0 = 0.0\n    \n    y_hat_0, _ = compute_forward_loss(z_0, alpha_0, w_0, b_0, y_0)\n    grad_at_zero = analytical_gradient(z_0, alpha_0, w_0, y_hat_0, y_0)\n    results.append(grad_at_zero)\n    \n    # Test Case 3: Identifiability with all-negative pre-activations\n    z_neg = np.array([-1.0, -0.5])\n    alpha_neg = 0.7\n    w_neg = np.array([1.2, -0.8])\n    b_neg = 0.3\n    s_neg = 2.5\n    \n    y_hat_orig_neg, _ = compute_forward_loss(z_neg, alpha_neg, w_neg, b_neg, 0.0)\n    \n    alpha_scaled_neg = s_neg * alpha_neg\n    w_scaled_neg = w_neg / s_neg\n    y_hat_scaled_neg, _ = compute_forward_loss(z_neg, alpha_scaled_neg, w_scaled_neg, b_neg, 0.0)\n    \n    is_identifiable_neg = np.isclose(y_hat_orig_neg, y_hat_scaled_neg, atol=1e-12)\n    results.append(bool(is_identifiable_neg))\n    \n    # Test Case 4: Identifiability fails for mixed signs\n    z_mix2 = np.array([-1.0, 0.5])\n    # The problem specifies to use the same alpha, w, b, s as in the previous case.\n    alpha_mix2 = alpha_neg #0.7\n    w_mix2 = w_neg #[1.2, -0.8]\n    b_mix2 = b_neg #0.3\n    s_mix2 = s_neg # 2.5\n    \n    y_hat_orig_mix2, _ = compute_forward_loss(z_mix2, alpha_mix2, w_mix2, b_mix2, 0.0)\n    \n    alpha_scaled_mix2 = s_mix2 * alpha_mix2\n    w_scaled_mix2 = w_mix2 / s_mix2\n    y_hat_scaled_mix2, _ = compute_forward_loss(z_mix2, alpha_scaled_mix2, w_scaled_mix2, b_mix2, 0.0)\n    \n    is_identifiable_mix2 = np.isclose(y_hat_orig_mix2, y_hat_scaled_mix2, atol=1e-12)\n    # The question is whether they are equal, so non-identifiability means the outputs are different.\n    # The problem asks for a boolean indicating whether the outputs are equal.\n    results.append(bool(is_identifiable_mix2))\n\n    # Test Case 5: All-positive pre-activations\n    z_pos = np.array([0.4, 0.2])\n    alpha_pos = 1.7\n    w_pos = np.array([0.6, -0.3])\n    b_pos = -0.1\n    y_pos = 0.5\n\n    y_hat_pos, _ = compute_forward_loss(z_pos, alpha_pos, w_pos, b_pos, y_pos)\n    grad_all_pos = analytical_gradient(z_pos, alpha_pos, w_pos, y_hat_pos, y_pos)\n    results.append(grad_all_pos)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3123807"}]}