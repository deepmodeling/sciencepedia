{"hands_on_practices": [{"introduction": "Before we can build complex models, it's crucial to understand their fundamental mechanics. This first exercise flips the script: instead of running a known RNN forward, you are given its observed behavior—a series of inputs and the corresponding hidden states. Your task is to play the role of a detective and deduce the internal rules of the system, a process known as system identification. By applying the principle of least squares, you will recover the underlying linear dynamics, solidifying your intuition for how the state transition and input matrices govern the evolution of the hidden state [@problem_id:3192180].", "problem": "Consider a linear Recurrent Neural Network (RNN), where the hidden state update uses the identity activation function. The hidden state $\\mathbf{h}_t \\in \\mathbb{R}^2$ evolves according to the recurrence relation\n$$\n\\mathbf{h}_t = A \\mathbf{h}_{t-1} + B x_t,\n$$\nwith unknown state transition matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and unknown input matrix $B \\in \\mathbb{R}^{2 \\times 1}$. You are given the initial hidden state $\\mathbf{h}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the following observed input–state pairs $(x_t, \\mathbf{h}_t)$ for $t = 1, 2, 3, 4, 5$:\n$$\n\\begin{aligned}\nx_1 &= 1, \\quad \\mathbf{h}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\\\\nx_2 &= 2, \\quad \\mathbf{h}_2 = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix}, \\\\\nx_3 &= 1, \\quad \\mathbf{h}_3 = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix}, \\\\\nx_4 &= 0, \\quad \\mathbf{h}_4 = \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix}, \\\\\nx_5 &= -1, \\quad \\mathbf{h}_5 = \\begin{pmatrix} 27 \\\\ 3 \\end{pmatrix}.\n\\end{aligned}\n$$\nStarting from the foundational definition of least squares estimation (minimizing the sum of squared residuals), derive an estimator for $A$ and $B$ by explicitly constructing a linear regression problem from the recurrence and solving the associated normal equations without appealing to any pre-tabulated formulas. Then, apply your derivation to the given data to recover the numerical values of $A$ and $B$.\n\nAdditionally, justify, using first principles from linear systems, when the recovery is unique by discussing the role of block Hankel matrices constructed from time series data and the observability of the state when the output is the hidden state itself. Your justification should make clear how rank conditions arise from these constructions and why they matter for identifiability of $A$ and $B$.\n\nYour final answer must be a single composite analytical expression containing all entries of $A$ and $B$ in a single row matrix in the order $(a_{11}, a_{12}, a_{21}, a_{22}, b_1, b_2)$. No rounding is required.", "solution": "The problem is valid. It is a well-posed problem in the field of system identification, a core topic in engineering and machine learning. It is scientifically grounded, objective, and contains sufficient information to determine a unique solution. The task requires deriving an estimator from first principles and justifying its uniqueness, which are standard and appropriate questions in this context.\n\nThe hidden state evolution is given by the linear recurrence relation:\n$$\n\\mathbf{h}_t = A \\mathbf{h}_{t-1} + B x_t\n$$\nwhere $\\mathbf{h}_t \\in \\mathbb{R}^2$, $A \\in \\mathbb{R}^{2 \\times 2}$, and $B \\in \\mathbb{R}^{2 \\times 1}$. The parameters to be estimated are the entries of matrices $A$ and $B$.\nLet us define the matrices and vectors as:\n$$\nA = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}, \\quad B = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix}, \\quad \\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}\n$$\nThe recurrence relation can be separated into two independent scalar equations for each row:\n$$\nh_{t,1} = a_{11} h_{t-1,1} + a_{12} h_{t-1,2} + b_1 x_t \\\\\nh_{t,2} = a_{21} h_{t-1,1} + a_{22} h_{t-1,2} + b_2 x_t\n$$\nThis structure reveals two independent linear regression problems. For the first row, the parameters are $\\boldsymbol{\\theta}_1 = \\begin{pmatrix} a_{11} & a_{12} & b_1 \\end{pmatrix}^T$, and the target variable is $h_{t,1}$. For the second row, the parameters are $\\boldsymbol{\\theta}_2 = \\begin{pmatrix} a_{21} & a_{22} & b_2 \\end{pmatrix}^T$, and the target is $h_{t,2}$. Both regressions share the same set of regressors, which can be arranged into a vector $\\mathbf{z}_t = \\begin{pmatrix} h_{t-1,1} & h_{t-1,2} & x_t \\end{pmatrix}^T$. The equations thus become:\n$$\nh_{t,1} = \\mathbf{z}_t^T \\boldsymbol{\\theta}_1 \\\\\nh_{t,2} = \\mathbf{z}_t^T \\boldsymbol{\\theta}_2\n$$\nTo estimate the parameters using the given data for $t = 1, \\dots, 5$, we assemble the data into matrices. Let $N=5$ be the number of observations. We define a design matrix $Z$ where each row corresponds to the regressors at a given time step, and target vectors $Y_1$ and $Y_2$ for the two scalar sub-problems.\n$$\nZ = \\begin{pmatrix} \\mathbf{z}_1^T \\\\ \\mathbf{z}_2^T \\\\ \\vdots \\\\ \\mathbf{z}_N^T \\end{pmatrix} = \\begin{pmatrix} \\mathbf{h}_0^T & x_1 \\\\ \\mathbf{h}_1^T & x_2 \\\\ \\vdots \\\\ \\mathbf{h}_{N-1}^T & x_N \\end{pmatrix}, \\quad Y_1 = \\begin{pmatrix} h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{N,1} \\end{pmatrix}, \\quad Y_2 = \\begin{pmatrix} h_{1,2} \\\\ h_{2,2} \\\\ \\vdots \\\\ h_{N,2} \\end{pmatrix}\n$$\nThe entire system of equations for all time steps can be written as $Y_1 \\approx Z \\boldsymbol{\\theta}_1$ and $Y_2 \\approx Z \\boldsymbol{\\theta}_2$.\n\nThe least squares principle requires us to find the parameter vector $\\boldsymbol{\\theta}$ that minimizes the sum of squared residuals (errors). For the first sub-problem, the objective function is the squared Euclidean norm of the residual vector:\n$$\nS(\\boldsymbol{\\theta}_1) = || Y_1 - Z \\boldsymbol{\\theta}_1 ||_2^2 = (Y_1 - Z \\boldsymbol{\\theta}_1)^T (Y_1 - Z \\boldsymbol{\\theta}_1) = Y_1^T Y_1 - 2 Y_1^T Z \\boldsymbol{\\theta}_1 + \\boldsymbol{\\theta}_1^T Z^T Z \\boldsymbol{\\theta}_1\n$$\nTo find the minimum, we compute the gradient of $S(\\boldsymbol{\\theta}_1)$ with respect to $\\boldsymbol{\\theta}_1$ and set it to zero. Using standard matrix calculus identities, we have:\n$$\n\\nabla_{\\boldsymbol{\\theta}_1} S(\\boldsymbol{\\theta}_1) = -2 Z^T Y_1 + 2 Z^T Z \\boldsymbol{\\theta}_1 = 0\n$$\nThis yields the celebrated normal equations:\n$$\n(Z^T Z) \\boldsymbol{\\theta}_1 = Z^T Y_1\n$$\nAssuming that the matrix $Z^T Z$ is invertible, the unique least squares estimator for $\\boldsymbol{\\theta}_1$ is:\n$$\n\\hat{\\boldsymbol{\\theta}}_1 = (Z^T Z)^{-1} Z^T Y_1\n$$\nAn identical derivation holds for $\\boldsymbol{\\theta}_2$, leading to $\\hat{\\boldsymbol{\\theta}}_2 = (Z^T Z)^{-1} Z^T Y_2$.\n\nNow, we apply this derivation to the given data. The known values are $\\mathbf{h}_0 = \\begin{pmatrix} 0 & 0 \\end{pmatrix}^T$ and:\n$\\mathbf{h}_1 = \\begin{pmatrix} 1 & 1 \\end{pmatrix}^T$ for $x_1=1$;\n$\\mathbf{h}_2 = \\begin{pmatrix} 5 & 3 \\end{pmatrix}^T$ for $x_2=2$;\n$\\mathbf{h}_3 = \\begin{pmatrix} 12 & 4 \\end{pmatrix}^T$ for $x_3=1$;\n$\\mathbf{h}_4 = \\begin{pmatrix} 20 & 4 \\end{pmatrix}^T$ for $x_4=0$;\n$\\mathbf{h}_5 = \\begin{pmatrix} 27 & 3 \\end{pmatrix}^T$ for $x_5=-1$.\n\nWe construct the matrices $Z$, $Y_1$, and $Y_2$:\n$$\nZ = \\begin{pmatrix} \\mathbf{h}_0^T & x_1 \\\\ \\mathbf{h}_1^T & x_2 \\\\ \\mathbf{h}_2^T & x_3 \\\\ \\mathbf{h}_3^T & x_4 \\\\ \\mathbf{h}_4^T & x_5 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 1 & 2 \\\\ 5 & 3 & 1 \\\\ 12 & 4 & 0 \\\\ 20 & 4 & -1 \\end{pmatrix}, \\quad Y_1 = \\begin{pmatrix} 1 \\\\ 5 \\\\ 12 \\\\ 20 \\\\ 27 \\end{pmatrix}, \\quad Y_2 = \\begin{pmatrix} 1 \\\\ 3 \\\\ 4 \\\\ 4 \\\\ 3 \\end{pmatrix}\n$$\nRather than directly computing $(Z^T Z)^{-1}$, we can test if a perfect solution exists. A perfect solution would yield zero error, which is the absolute minimum of the sum of squared residuals, and thus would be the unique least squares solution.\n\nFrom $\\mathbf{h}_1 = A \\mathbf{h}_0 + B x_1$:\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + B (1) \\implies B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nFrom $\\mathbf{h}_2 = A \\mathbf{h}_1 + B x_2$:\n$$\n\\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (2) \\implies A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nThis gives the linear equations: $a_{11} + a_{12} = 3$ and $a_{21} + a_{22} = 1$.\n\nFrom $\\mathbf{h}_3 = A \\mathbf{h}_2 + B x_3$:\n$$\n\\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} = A \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (1) \\implies A \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\nThis gives the linear equations: $5a_{11} + 3a_{12} = 11$ and $5a_{21} + 3a_{22} = 3$.\n\nWe now solve the two systems of equations for the rows of $A$:\nFor the first row of $A$:\n1) $a_{11} + a_{12} = 3 \\implies a_{12} = 3 - a_{11}$\n2) $5a_{11} + 3a_{12} = 11 \\implies 5a_{11} + 3(3 - a_{11}) = 11 \\implies 2a_{11} + 9 = 11 \\implies 2a_{11}=2 \\implies a_{11}=1$.\nThus, $a_{12} = 3 - 1 = 2$.\n\nFor the second row of $A$:\n1) $a_{21} + a_{22} = 1 \\implies a_{22} = 1 - a_{21}$\n2) $5a_{21} + 3a_{22} = 3 \\implies 5a_{21} + 3(1 - a_{21}) = 3 \\implies 2a_{21} + 3 = 3 \\implies 2a_{21}=0 \\implies a_{21}=0$.\nThus, $a_{22} = 1 - 0 = 1$.\n\nThis gives the candidate solution: $A = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$ and $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. We must verify this solution against the remaining data points.\nFor $t=4$:\n$$ \\mathbf{h}_4 = A \\mathbf{h}_3 + B x_4 = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (0) = \\begin{pmatrix} 12+8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix} $$\nThis matches the given $\\mathbf{h}_4$.\nFor $t=5$:\n$$ \\mathbf{h}_5 = A \\mathbf{h}_4 + B x_5 = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (-1) = \\begin{pmatrix} 20+8 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 28-1 \\\\ 4-1 \\end{pmatrix} = \\begin{pmatrix} 27 \\\\ 3 \\end{pmatrix} $$\nThis matches the given $\\mathbf{h}_5$.\nSince the proposed solution fits all data points exactly, the sum of squared residuals is zero. This is the global minimum of the non-negative objective function $S(\\boldsymbol{\\theta})$. Hence, this is the exact and unique solution provided by the least squares method.\n\nThe recovered matrices are $A = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$ and $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Justification for Uniqueness of Recovery**\nThe uniqueness of the least squares solution $\\hat{\\boldsymbol{\\theta}} = (Z^T Z)^{-1} Z^T Y$ hinges on the invertibility of the matrix $Z^T Z$. This matrix is invertible if and only if the design matrix $Z$ has full column rank. In our case, $Z$ is a $5 \\times 3$ matrix, so it must have rank $3$. This means its three columns must be linearly independent.\n\nFrom a first-principles linear systems perspective, the ability to uniquely identify the system parameters $(A, B)$ from data relies on two crucial properties: the richness of the input signal and the structural properties of the system itself, namely controllability and observability.\n\n1.  **Observability**: A system is observable if its internal state can be uniquely determined from its outputs. The standard observability matrix for a system with output equation $\\mathbf{y}_t = C \\mathbf{h}_t$ is $\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ \\dots \\\\ CA^{n-1} \\end{pmatrix}$. In this problem, the output is the hidden state itself, which corresponds to setting the output matrix $C=I$ (the identity matrix). The observability matrix is $\\mathcal{O} = \\begin{pmatrix} I \\\\ A \\\\ \\dots \\\\ A^{n-1} \\end{pmatrix}$. Since this matrix contains the identity block $I$, its columns are guaranteed to be linearly independent, and it has rank $n=2$. Therefore, any linear system where the state is directly measured is trivially observable.\n\n2.  **Controllability and Persistent Excitation**: Controllability ensures that the input $x_t$ can drive the state vector $\\mathbf{h}_t$ to any point in the state space. It is determined by the rank of the controllability matrix $\\mathcal{C}_n = \\begin{pmatrix} B & AB & \\dots & A^{n-1}B \\end{pmatrix}$. If a system is not controllable, some of its internal dynamics are immune to the input, and the state vector $\\mathbf{h}_t$ will be confined to a subspace. Even with direct state observation, if we cannot excite all modes of the system, we cannot learn the parameters governing those modes. Concurrently, the input sequence $\\{x_t\\}$ must be \"persistently exciting,\" meaning it must be rich enough to prevent degeneracies. For instance, a constant input might not be sufficient to distinguish the effect of $A$ from that of $B$. Together, controllability and a persistently exciting input ensure that the regressor vectors $\\mathbf{z}_t^T = (\\mathbf{h}_{t-1}^T, x_t)$ sufficiently span the regressor space, making the columns of the design matrix $Z$ linearly independent.\n\nThe construction of block Hankel matrices of input and output data is a cornerstone of subspace identification methods (e.g., N4SID), which are designed for the more general problem of identifying $(A, B, C)$ from input-output pairs $(x_t, y_t)$. The rank of a certain projection of these Hankel matrices reveals the system order $n$, and this rank condition is fundamentally linked to the product of the observability and controllability matrices. In our specialized case, since observability is guaranteed ($C=I$), the identifiability conditions simplify and are more directly expressed by the requirement that the matrix $Z$ has full rank.\n\nFor our specific problem, we have found $A = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$ and $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The controllability matrix is $\\mathcal{C}_2 = \\begin{pmatrix} B & AB \\end{pmatrix} = \\begin{pmatrix} 1 & 3 \\\\ 1 & 1 \\end{pmatrix}$, which has determinant $1-3 = -2 \\neq 0$. The system is controllable. The input sequence $\\{x_t\\}$ and the resulting states $\\{ \\mathbf{h}_t \\}$ produce a design matrix $Z$ whose columns are linearly independent, thus making $Z^T Z$ invertible and guaranteeing a unique solution for $(A, B)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2 & 0 & 1 & 1 & 1 \\end{pmatrix}}\n$$", "id": "3192180"}, {"introduction": "Real-world RNNs rely on non-linear activation functions to capture complex patterns, but these introduce rich and sometimes unexpected dynamics. This practice moves beyond simple linear systems to explore the behavior of an RNN with a Rectified Linear Unit (ReLU) activation. You will analyze how the network's state can become 'trapped' in a low-dimensional subspace, a phenomenon driven by the sparsity that ReLU induces. This exercise provides a concrete look at how the interplay between the recurrence and the activation function shapes the representational geometry of the hidden state [@problem_id:3192158].", "problem": "Consider a discrete-time Recurrent Neural Network (RNN) with a two-dimensional hidden state and Rectified Linear Unit (ReLU) nonlinearity. Let the recurrence be defined by the elementwise application of the ReLU (Rectified Linear Unit) function to the preactivation, so that the hidden state update is given by\n$$\n\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right),\n$$\nwhere $\\phi$ acts componentwise as $\\phi(z)=\\max(0,z)$, $\\mathbf{h}_{t}\\in\\mathbb{R}^{2}$ is the hidden state at time $t$, $\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$, $\\mathbf{U}\\in\\mathbb{R}^{2\\times 1}$, $\\mathbf{b}\\in\\mathbb{R}^{2}$, and the input is a constant scalar $u\\in\\mathbb{R}$ applied at every time step. The parameters and initial hidden state are\n$$\n\\mathbf{W}=\\begin{pmatrix}0.6 & -0.4\\\\ -0.5 & 0.2\\end{pmatrix},\\quad\n\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix},\\quad\n\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}.\n$$\nDefine the one-dimensional cone\n$$\n\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}>0,\\ h_{2}=0\\right\\}.\n$$\nStarting from the fundamental definitions of the RNN recurrence and the ReLU nonlinearity, analyze how activation sparsity (zeros created by the ReLU) can persist in time and derive conditions under which the hidden state becomes trapped in the low-dimensional cone $\\mathcal{C}$ for all $t\\geq 1$. Then, compute the largest real $u$ such that, with the constant input $u$ applied at every step, the hidden state satisfies $\\mathbf{h}_{t}\\in\\mathcal{C}$ for all $t\\geq 1$. The final answer must be a single real number. No rounding is required.", "solution": "The user wants me to find the largest real number $u$ for which the hidden state $\\mathbf{h}_t$ of a given Recurrent Neural Network (RNN) is constrained to a specific cone $\\mathcal{C}$ for all time steps $t \\geq 1$.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Recurrence Relation**: $\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right)$, where $\\phi(z)=\\max(0,z)$ (ReLU) is applied elementwise.\n- **State Vector**: $\\mathbf{h}_{t} = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix} \\in\\mathbb{R}^{2}$.\n- **Weight Matrix**: $\\mathbf{W}=\\begin{pmatrix}0.6 & -0.4\\\\ -0.5 & 0.2\\end{pmatrix}$.\n- **Input Matrix**: $\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}$.\n- **Bias Vector**: $\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}$.\n- **Input**: $u \\in \\mathbb{R}$, constant for all $t$.\n- **Initial State**: $\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}$.\n- **Target Cone**: $\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}>0,\\ h_{2}=0\\right\\}$.\n- **Condition**: Find the largest $u$ such that $\\mathbf{h}_{t} \\in \\mathcal{C}$ for all $t \\geq 1$.\n\n**1.2. Validate Using Extracted Givens**\nThe problem is a well-defined exercise in analyzing the dynamics of a discrete-time switched linear system, which is the model for an RNN with a ReLU activation function.\n- **Scientifically Grounded**: The problem is based on standard definitions from deep learning and dynamical systems theory.\n- **Well-Posed**: It asks for the maximum value of a parameter subject to a set of derivable constraints. The problem statement is self-contained and provides all necessary information to determine a unique solution.\n- **Objective**: All terms are defined mathematically, and the objective is to compute a single numerical value.\n\nThe problem is valid as it does not violate any of the specified criteria. I will proceed with the solution.\n\n### Step 2: Deriving Conditions for State Confinement\n\nThe condition that $\\mathbf{h}_{t} \\in \\mathcal{C}$ for all $t \\geq 1$ means that for every $t \\geq 1$, the hidden state vector $\\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}$ must satisfy:\n1.  $h_{t,1} > 0$\n2.  $h_{t,2} = 0$\n\nLet the pre-activation vector at time $t$ be $\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}$. Then $\\mathbf{h}_t = \\phi(\\mathbf{z}_t)$.\nThe elementwise ReLU function implies:\n$h_{t,1} = \\max(0, z_{t,1})$\n$h_{t,2} = \\max(0, z_{t,2})$\n\nFor $\\mathbf{h}_t$ to be in $\\mathcal{C}$, we must have:\n1.  $\\max(0, z_{t,1}) > 0 \\implies z_{t,1} > 0$. In this case, $h_{t,1} = z_{t,1}$.\n2.  $\\max(0, z_{t,2}) = 0 \\implies z_{t,2} \\leq 0$.\n\nSo, for all $t \\geq 1$, we must satisfy $z_{t,1} > 0$ and $z_{t,2} \\leq 0$.\n\n### Step 3: Analysis for Time Step $t=1$\n\nWe begin with the initial state $\\mathbf{h}_0 = \\begin{pmatrix} 0.1 \\\\ 0 \\end{pmatrix}$. We compute the pre-activation $\\mathbf{z}_1$:\n$$\n\\mathbf{z}_1 = \\mathbf{W}\\mathbf{h}_0 + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6 & -0.4\\\\ -0.5 & 0.2\\end{pmatrix} \\begin{pmatrix}0.1\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}u + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_1 = \\begin{pmatrix}0.6 \\times 0.1\\\\ -0.5 \\times 0.1\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix} = \\begin{pmatrix}0.06 + 0.3u - 0.2\\\\ -0.05 + 0.1u - 0.05\\end{pmatrix} = \\begin{pmatrix}0.3u - 0.14\\\\ 0.1u - 0.1\\end{pmatrix}\n$$\nFor $\\mathbf{h}_1 \\in \\mathcal{C}$, we need $z_{1,1} > 0$ and $z_{1,2} \\leq 0$:\n1.  $z_{1,1} = 0.3u - 0.14 > 0 \\implies 0.3u > 0.14 \\implies u > \\frac{0.14}{0.3} = \\frac{14}{30} = \\frac{7}{15}$.\n2.  $z_{1,2} = 0.1u - 0.1 \\leq 0 \\implies 0.1u \\leq 0.1 \\implies u \\leq 1$.\n\nThus, a necessary condition on $u$ is $\\frac{7}{15} < u \\leq 1$.\nIf this condition holds, the state at $t=1$ is $\\mathbf{h}_1 = \\begin{pmatrix} z_{1,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.3u - 0.14 \\\\ 0 \\end{pmatrix}$.\n\n### Step 4: Inductive Analysis for $t \\geq 2$\n\nWe proceed by induction. Assume that for some $t-1 \\geq 1$, the state is in the cone, i.e., $\\mathbf{h}_{t-1} = \\begin{pmatrix} h_{t-1,1} \\\\ 0 \\end{pmatrix}$ with $h_{t-1,1} > 0$. We need to find the conditions on $u$ such that $\\mathbf{h}_t$ also lies in $\\mathcal{C}$.\n\nLet's compute $\\mathbf{z}_t$:\n$$\n\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1} + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6 & -0.4\\\\ -0.5 & 0.2\\end{pmatrix} \\begin{pmatrix}h_{t-1,1}\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_t = \\begin{pmatrix} 0.6 h_{t-1,1} + 0.3u - 0.2 \\\\ -0.5 h_{t-1,1} + 0.1u - 0.05 \\end{pmatrix}\n$$\nFor $\\mathbf{h}_t \\in \\mathcal{C}$, we need $z_{t,1} > 0$ and $z_{t,2} \\leq 0$:\n1.  $z_{t,1} > 0 \\implies h_{t,1} = 0.6 h_{t-1,1} + 0.3u - 0.2 > 0$.\n2.  $z_{t,2} \\leq 0 \\implies -0.5 h_{t-1,1} + 0.1u - 0.05 \\leq 0$.\n\nThe second inequality provides a lower bound on $h_{t-1,1}$:\n$$\n0.5 h_{t-1,1} \\geq 0.1u - 0.05 \\implies h_{t-1,1} \\geq \\frac{0.1u - 0.05}{0.5} \\implies h_{t-1,1} \\geq 0.2u - 0.1\n$$\nThis inequality must hold for all $t-1 \\geq 1$. Thus, the sequence of the first hidden state component, let's call it $x_t = h_{t,1}$, must satisfy $x_t \\geq 0.2u - 0.1$ for all $t \\geq 1$.\n\nThe dynamics of $x_t$ are given by the linear recurrence from the first condition:\n$x_t = 0.6 x_{t-1} + (0.3u - 0.2)$ for $t \\geq 2$, with $x_1 = 0.3u - 0.14$.\n\nFirst, consider the case where the lower bound is non-positive: $0.2u - 0.1 \\leq 0 \\implies u \\leq 0.5$. The full range on $u$ so far is $\\frac{7}{15} < u \\leq 1$. If $u \\in (\\frac{7}{15}, 0.5]$, then $0.3u - 0.2$ is negative. The fixed point of the recurrence for $x_t$ is $x_{fp} = \\frac{0.3u - 0.2}{1 - 0.6} = \\frac{0.3u - 0.2}{0.4} = 0.75u - 0.5$, which is also negative for $u \\leq 0.5$. The sequence $x_t$ starts at a positive value $x_1 > 0$ and converges to a negative fixed point $x_{fp} < 0$. Therefore, $x_t$ must eventually become negative, violating the condition $x_t>0$. Thus, there are no solutions for $u \\leq 0.5$.\n\nWe must have $u > 0.5$. In this case, $0.2u - 0.1 > 0$. The condition $x_t \\geq 0.2u-0.1$ is stronger than and implies $x_t > 0$. So we only need to ensure $x_t \\geq 0.2u - 0.1$ for all $t \\geq 1$. This is equivalent to requiring that the minimum of the sequence $\\{x_t\\}_{t\\geq 1}$ satisfies this bound: $\\min_{t \\geq 1} x_t \\geq 0.2u - 0.1$.\n\nThe sequence $x_t$ is monotonic, as it's a first-order linear recurrence. Its behavior depends on the relation between the start-point $x_1$ and the fixed-point $x_{fp}$.\n- $x_1 = 0.3u - 0.14$\n- $x_{fp} = 0.75u - 0.5$\nThe difference is $x_1 - x_{fp} = (0.3u - 0.14) - (0.75u - 0.5) = -0.45u + 0.36 = 0.09(4-5u)$.\nThe sign of the difference depends on whether $u$ is smaller or larger than $\\frac{4}{5} = 0.8$.\n\n**Case 1: $u < 0.8$**\nIn this case, $x_1 - x_{fp} > 0$, so $x_1 > x_{fp}$. The sequence $\\{x_t\\}$ is monotonically decreasing and converges to $x_{fp}$ from above.\nThe minimum value is the infimum, $\\min_{t \\geq 1} x_t = \\inf_{t \\geq 1} x_t = x_{fp}$.\nThe condition becomes $x_{fp} \\geq 0.2u - 0.1$.\n$0.75u - 0.5 \\geq 0.2u - 0.1 \\implies 0.55u \\geq 0.4 \\implies u \\geq \\frac{0.4}{0.55} = \\frac{40}{55} = \\frac{8}{11}$.\nSo for this case we need $u \\in [\\frac{8}{11}, 0.8)$. Note that $\\frac{8}{11} \\approx 0.727$ which is consistent with $u > 0.5$ and $u > 7/15 \\approx 0.467$.\n\n**Case 2: $u > 0.8$**\nIn this case, $x_1 - x_{fp} < 0$, so $x_1 < x_{fp}$. The sequence $\\{x_t\\}$ is monotonically increasing and converges to $x_{fp}$ from below.\nThe minimum value is the first term, $\\min_{t \\geq 1} x_t = x_1$.\nThe condition becomes $x_1 \\geq 0.2u - 0.1$.\n$0.3u - 0.14 \\geq 0.2u - 0.1 \\implies 0.1u \\geq 0.04 \\implies u \\geq 0.4$.\nFor this case, we have $u > 0.8$. The condition $u \\geq 0.4$ is automatically satisfied. We must also satisfy the initial constraint $u \\leq 1$. So this case gives the range $u \\in (0.8, 1]$.\n\n**Case 3: $u = 0.8$**\nIn this case, $x_1 = x_{fp}$. The sequence is constant: $x_t = x_1$ for all $t\\geq 1$.\n$x_1 = 0.3(0.8) - 0.14 = 0.24 - 0.14 = 0.1$.\nThe condition is $x_t \\geq 0.2u - 0.1$.\n$0.1 \\geq 0.2(0.8) - 0.1 = 0.16 - 0.1 = 0.06$.\nThis is true. The initial constraint $u \\in (\\frac{7}{15}, 1]$ is also satisfied for $u=0.8$. So $u=0.8$ is a valid solution.\n\n### Step 5: Final Solution\n\nCombining the intervals from all cases:\n- From Case 1: $u \\in [\\frac{8}{11}, 0.8)$\n- From Case 3: $u = 0.8$\n- From Case 2: $u \\in (0.8, 1]$\n\nThe union of these sets gives the total range of valid values for $u$:\n$$\nu \\in \\left[\\frac{8}{11}, 0.8\\right) \\cup \\{0.8\\} \\cup (0.8, 1] = \\left[\\frac{8}{11}, 1\\right]\n$$\nThe problem asks for the largest real number $u$ that satisfies the condition. The largest value in the interval $[\\frac{8}{11}, 1]$ is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3192158"}, {"introduction": "An RNN is only as good as its learned parameters. The final and most critical piece of the puzzle is understanding how these parameters are optimized, which hinges on calculating gradients. This exercise guides you through the foundational derivation of the gradient of a loss function with respect to the recurrent weights in a linear RNN. By deriving the closed-form solution, you will see how the gradient is a sum of correlations across time, providing a deep insight into the mechanics of Backpropagation Through Time (BPTT) and the computational challenges of learning from sequential data [@problem_id:3192146].", "problem": "Consider a linear Recurrent Neural Network (RNN) with hidden state dimension $d_h$, input dimension $d_x$, and output dimension $d_y$, driven by a single sequence of length $T$. The recurrence and readout are given by the core definitions\n$h_t = W_h h_{t-1} + W_x x_t$ with $h_0 = 0$, and $y_t = W_y h_t$,\nfor $t \\in \\{1,2,\\dots,T\\}$, where $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The input vectors are $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$ and the targets are $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$. Consider the quadratic loss\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$.\nLet the output residuals be $e_t = y_t - s_t$ for each $t$. Using only the recurrence definition, the readout definition, the chain rule from multivariable calculus, and standard linear algebraic facts, derive a closed-form analytic expression for the gradient $\\nabla_{W_h} L$ that makes explicit its decomposition as a sum of time-shifted correlation terms of the form $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$. Then, briefly discuss the computational complexity (in Big–$O$ notation) of evaluating this gradient when implemented (i) directly from your closed-form expression by summing the time-shifted terms, and (ii) via Backpropagation Through Time (BPTT). Your final answer must be a single closed-form analytic matrix expression for $\\nabla_{W_h} L$ written in terms of $W_h$, $W_y$, $\\{e_t\\}$, and $\\{h_t\\}$ only. Do not include any numerical evaluation.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem statement from the field of deep learning theory. It requires the derivation of a gradient for a standard linear Recurrent Neural Network (RNN), which is a formal and solvable task based on established principles of multivariable calculus and linear algebra. All necessary definitions and conditions are provided.\n\nWe begin by stating the core definitions provided in the problem:\nRecurrence relation: $h_t = W_h h_{t-1} + W_x x_t$ for $t \\in \\{1, 2, \\dots, T\\}$, with initial state $h_0 = 0$.\nReadout equation: $y_t = W_y h_t$.\nLoss function: $L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$, where $e_t = y_t - s_t$.\nThe parameters are matrices $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The inputs are vectors $x_t \\in \\mathbb{R}^{d_x}$ and targets $s_t \\in \\mathbb{R}^{d_y}$.\n\nOur goal is to derive a closed-form expression for the gradient of the loss function $L$ with respect to the recurrent weight matrix $W_h$, denoted as $\\nabla_{W_h} L$. We will use the chain rule for matrix calculus.\n\nThe loss $L$ is a function of $W_h$ through its influence on the hidden states $\\{h_t\\}_{t=1}^T$. The gradient can be expressed as a sum of contributions from each timestep where $W_h$ is used in the forward pass:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\nwhere the \"local\" gradient refers to the gradient with respect to the instance of $W_h$ used in the computation $h_t = W_h h_{t-1} + W_x x_t$. Using the chain rule, this contribution is given by the outer product of the gradient of the loss with respect to the state $h_t$, and the gradient of the state-update term with respect to $W_h$. Let $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ be the gradient of the total loss with respect to the hidden state $h_t$. The gradient of the pre-activation $a_t = W_h h_{t-1} + W_x x_t$ with respect to $W_h$ is $h_{t-1}^T$. Thus, the contribution to the gradient from timestep $t$ is $\\delta_t h_{t-1}^T$. Summing over all timesteps gives the total gradient:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\nThis is the standard formula for the recurrent weight gradient in BPTT. However, the problem requires a closed-form expression in terms of the given quantities $W_h, W_y, \\{e_t\\}, \\{h_t\\}$. To achieve this, we must derive a closed form for $\\delta_t$.\n\nThe vector $\\delta_t$ represents the total influence of $h_t$ on the loss $L$. The state $h_t$ influences the loss through the outputs $y_k$ for all subsequent timesteps $k \\ge t$.\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\nHere, we apply the chain rule, summing over all paths from $h_t$ to the final loss. The partial derivatives are (using numerator-layout matrix calculus conventions where the gradient of a scalar w.r.t. a column vector is a column vector):\n$1.$ $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$. This is a column vector of size $d_y$.\n$2.$ $\\frac{\\partial y_k}{\\partial h_k} = W_y$. This is the Jacobian matrix of size $d_y \\times d_h$. The term in the chain rule is its transpose, $(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$.\n$3.$ $\\frac{\\partial h_k}{\\partial h_t}$. We find this by unrolling the recurrence relation:\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\nThe dependence of $h_k$ on $h_t$ for $k > t$ is given by:\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$.\nThe Jacobian matrix is therefore $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$ for $k \\ge t$. The term in the chain rule composition is its transpose, $(W_h^{k-t})^T = (W_h^T)^{k-t}$.\n\nSubstituting these derivatives back into the expression for $\\delta_t$:\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\nThis provides the closed-form expression for the gradient signal $\\delta_t$ propagated back from all future timesteps $k \\ge t$.\n\nFinally, we substitute this expression for $\\delta_t$ into our equation for the total gradient $\\nabla_{W_h} L$:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\nThis is the required closed-form analytic expression. It decomposes the gradient into a double summation over time. Each term $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ can be interpreted as a \"correlation\" between the error $e_k$ at a future time $k$ and the hidden state $h_{t-1}$ at a past time $t-1$, mediated by the matrix $(W_h^T)^{k-t} W_y^T$ which accounts for the propagation of influence through the network's dynamics and readout layer.\n\nNext, we discuss the computational complexity.\n(i) Direct evaluation from the closed-form expression:\nThe formula is $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$.\nAssume the forward pass has been run, so all $e_t \\in \\mathbb{R}^{d_y}$ and $h_t \\in \\mathbb{R}^{d_h}$ are known.\nThe double loop involves $O(T^2)$ terms. For each term $(t, k)$, the computation involves:\n- A matrix power $(W_h^T)^{k-t}$, which, if not pre-computed, takes $O((k-t) d_h^3)$. A pre-computation of all necessary powers of $W_h^T$ up to $T-1$ takes $O(T d_h^3)$.\n- A matrix-vector product $W_y^T e_k$, costing $O(d_h d_y)$.\n- A matrix-vector product $(W_h^T)^{k-t} (W_y^T e_k)$, costing $O(d_h^2)$.\n- An outer product with $h_{t-1}^T$, costing $O(d_h^2)$.\nA naive implementation would be very expensive. A more optimized direct evaluation would proceed as:\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\nThe most expensive part is the double loop structure. There are approximately $T^2/2$ pairs of $(t,k)$. For each pair, the dominant cost is computing the matrix-vector product with the matrix power, which is $O(d_h^2)$ assuming powers are precomputed. The precomputation costs $O(T d_h^3)$. Therefore, the total complexity is dominated by the nested loops and the precomputation, leading to a complexity of $O(T d_h^3 + T^2 d_h^2)$. The quadratic dependence on the sequence length $T$ makes this method computationally prohibitive for long sequences.\n\n(ii) Evaluation via Backpropagation Through Time (BPTT):\nBPTT avoids the explicit calculation of the double summation by using dynamic programming. It computes the gradients via a single backward pass through time. The key is the recursive relationship for $\\delta_t$:\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$ for $t = T-1, \\dots, 1$.\nThe BPTT algorithm for computing $\\nabla_{W_h} L$ is:\n$1.$ Perform a forward pass from $t=1$ to $T$ to compute and store all $h_t$ and $e_t$. Cost: $O(T(d_h^2 + d_h d_x + d_y d_h))$.\n$2.$ Initialize $\\nabla_{W_h} L = 0$ and $\\delta_{T+1} = 0$.\n$3.$ Perform a backward pass from $t=T$ down to $1$:\n   a. Compute $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$. This involves two matrix-vector products and a vector addition, costing $O(d_h d_y + d_h^2)$.\n   b. Add the contribution to the gradient: $\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$. This is an outer product and matrix addition, costing $O(d_h^2)$.\nThe backward pass consists of a single loop of length $T$, with each step costing $O(d_h^2 + d_h d_y)$. The total complexity of the backward pass is $O(T(d_h^2 + d_h d_y))$.\nThe total complexity of BPTT is therefore $O(T(d_h^2 + d_h d_x + d_y d_h))$, which is linear in the sequence length $T$. This is significantly more efficient than the direct evaluation of the closed-form expression.", "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$", "id": "3192146"}]}