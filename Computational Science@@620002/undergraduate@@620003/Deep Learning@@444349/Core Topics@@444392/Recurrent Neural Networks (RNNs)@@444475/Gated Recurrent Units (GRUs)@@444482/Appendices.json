{"hands_on_practices": [{"introduction": "The ability of a neural network to learn rests on understanding how to adjust its parameters to reduce error. This process, governed by the chain rule of calculus, can seem complex in recurrent architectures like the GRU. This first exercise demystifies the learning mechanism by asking you to derive the gradients for the update gate's parameters [@problem_id:3128076]. By working through this derivation, you will gain a concrete understanding of how learning signals are formed and what factors—such as the input $x_t$ and previous state $h_{t-1}$—influence the training of this crucial gate.", "problem": "Consider a single hidden-unit ($1$-dimensional) Gated Recurrent Unit (GRU). Let the input at time $t$ be the scalar $x_t \\in \\mathbb{R}$ and the previous hidden state be the scalar $h_{t-1} \\in \\mathbb{R}$. The GRU has scalar parameters for the update gate, reset gate, and candidate state. Denote the update-gate parameters by $w_z, u_z, b_z \\in \\mathbb{R}$, the reset-gate parameters by $w_r, u_r, b_r \\in \\mathbb{R}$, and the candidate-state parameters by $w_n, u_n, b_n \\in \\mathbb{R}$. The GRU update is defined by the following equations:\n$$z_t = \\sigma\\!\\left(w_z x_t + u_z h_{t-1} + b_z\\right),$$\n$$r_t = \\sigma\\!\\left(w_r x_t + u_r h_{t-1} + b_r\\right),$$\n$$\\tilde{h}_t = \\tanh\\!\\left(w_n x_t + u_n \\left(r_t h_{t-1}\\right) + b_n\\right),$$\n$$h_t = \\left(1 - z_t\\right)\\tilde{h}_t + z_t h_{t-1},$$\nwhere $\\sigma(a) = \\frac{1}{1 + \\exp(-a)}$ is the logistic sigmoid function and $\\tanh(a) = \\frac{\\exp(a) - \\exp(-a)}{\\exp(a) + \\exp(-a)}$ is the hyperbolic tangent. All symbols denote scalars. Treat $x_t$ and $h_{t-1}$ as given inputs at time $t$, and treat all parameters other than $w_z, u_z, b_z$ as constants for the purpose of differentiation.\n\nStarting from the core definitions above and fundamental calculus rules (chain rule and the derivatives of $\\sigma$ and $\\tanh$), derive explicit expressions for the partial derivatives $\\frac{\\partial h_t}{\\partial w_z}$, $\\frac{\\partial h_t}{\\partial u_z}$, and $\\frac{\\partial h_t}{\\partial b_z}$ in closed form, expressed in terms of $x_t$, $h_{t-1}$, $z_t$, and $\\tilde{h}_t$ (and elementary functions as needed). Your derivation should make clear how learning signals from $x_t$ and $h_{t-1}$ enter these gradients.\n\nProvide your final answer as three closed-form analytic expressions, in the order $\\frac{\\partial h_t}{\\partial w_z}$, $\\frac{\\partial h_t}{\\partial u_z}$, $\\frac{\\partial h_t}{\\partial b_z}$, formatted as a single row matrix. No numerical evaluation or rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It is a standard derivation in the field of deep learning, requiring the application of fundamental calculus rules to the defined equations of a Gated Recurrent Unit (GRU).\n\nThe objective is to compute the partial derivatives of the hidden state $h_t$ with respect to the parameters of the update gate, $w_z$, $u_z$, and $b_z$. The hidden state $h_t$ is given by the equation:\n$$h_t = (1 - z_t)\\tilde{h}_t + z_t h_{t-1}$$\nHere, $h_{t-1}$ is the previous hidden state, $\\tilde{h}_t$ is the candidate hidden state, and $z_t$ is the output of the update gate. The parameters $w_z$, $u_z$, and $b_z$ influence $h_t$ exclusively through their role in calculating $z_t$. As per the problem statement, all other quantities, including $\\tilde{h}_t$, $r_t$, and their parameters ($w_r, u_r, b_r, w_n, u_n, b_n$), as well as the inputs $x_t$ and $h_{t-1}$, are treated as constants for the purpose of this differentiation.\n\nWe will use the chain rule for differentiation. For a generic parameter $p \\in \\{w_z, u_z, b_z\\}$, the partial derivative of $h_t$ with respect to $p$ is:\n$$\\frac{\\partial h_t}{\\partial p} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial p}$$\n\nFirst, we compute the partial derivative of $h_t$ with respect to $z_t$. The equation for $h_t$ can be rewritten as $h_t = \\tilde{h}_t - z_t \\tilde{h}_t + z_t h_{t-1}$. Since $\\tilde{h}_t$ and $h_{t-1}$ are treated as constants in this context, the differentiation is straightforward:\n$$\\frac{\\partial h_t}{\\partial z_t} = \\frac{\\partial}{\\partial z_t} \\left( \\tilde{h}_t - z_t \\tilde{h}_t + z_t h_{t-1} \\right) = -\\tilde{h}_t + h_{t-1} = h_{t-1} - \\tilde{h}_t$$\n\nNext, we compute the term $\\frac{\\partial z_t}{\\partial p}$. The update gate $z_t$ is defined as:\n$$z_t = \\sigma(w_z x_t + u_z h_{t-1} + b_z)$$\nLet the argument of the sigmoid function be $a_z = w_z x_t + u_z h_{t-1} + b_z$. Thus, $z_t = \\sigma(a_z)$. Using the chain rule again:\n$$\\frac{\\partial z_t}{\\partial p} = \\frac{\\partial \\sigma(a_z)}{\\partial a_z} \\frac{\\partial a_z}{\\partial p} = \\sigma'(a_z) \\frac{\\partial a_z}{\\partial p}$$\nThe derivative of the logistic sigmoid function $\\sigma(a) = \\frac{1}{1 + \\exp(-a)}$ is $\\sigma'(a) = \\sigma(a)(1 - \\sigma(a))$. Therefore, $\\sigma'(a_z) = z_t(1 - z_t)$. This gives us:\n$$\\frac{\\partial z_t}{\\partial p} = z_t(1 - z_t) \\frac{\\partial a_z}{\\partial p}$$\n\nNow, we must compute $\\frac{\\partial a_z}{\\partial p}$ for each of the three parameters $p \\in \\{w_z, u_z, b_z\\}$.\n1. For $p = w_z$:\n$$\\frac{\\partial a_z}{\\partial w_z} = \\frac{\\partial}{\\partial w_z}(w_z x_t + u_z h_{t-1} + b_z) = x_t$$\n2. For $p = u_z$:\n$$\\frac{\\partial a_z}{\\partial u_z} = \\frac{\\partial}{\\partial u_z}(w_z x_t + u_z h_{t-1} + b_z) = h_{t-1}$$\n3. For $p = b_z$:\n$$\\frac{\\partial a_z}{\\partial b_z} = \\frac{\\partial}{\\partial b_z}(w_z x_t + u_z h_{t-1} + b_z) = 1$$\n\nFinally, we combine these results to obtain the final expressions for the required partial derivatives.\n\nFor $\\frac{\\partial h_t}{\\partial w_z}$:\n$$\\frac{\\partial h_t}{\\partial w_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_z} = (h_{t-1} - \\tilde{h}_t) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial w_z} \\right) = (h_{t-1} - \\tilde{h}_t) z_t(1 - z_t) x_t$$\nThis can be written as $x_t z_t (1 - z_t) (h_{t-1} - \\tilde{h}_t)$. This expression shows that the gradient with respect to the input weight $w_z$ is proportional to the input $x_t$.\n\nFor $\\frac{\\partial h_t}{\\partial u_z}$:\n$$\\frac{\\partial h_t}{\\partial u_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial u_z} = (h_{t-1} - \\tilde{h}_t) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial u_z} \\right) = (h_{t-1} - \\tilde{h}_t) z_t(1 - z_t) h_{t-1}$$\nThis can be written as $h_{t-1} z_t (1 - z_t) (h_{t-1} - \\tilde{h}_t)$. This expression shows that the gradient with respect to the recurrent weight $u_z$ is proportional to the previous hidden state $h_{t-1}$.\n\nFor $\\frac{\\partial h_t}{\\partial b_z}$:\n$$\\frac{\\partial h_t}{\\partial b_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial b_z} = (h_{t-1} - \\tilde{h}_t) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial b_z} \\right) = (h_{t-1} - \\tilde{h}_t) z_t(1 - z_t) (1)$$\nThis can be written as $z_t (1 - z_t) (h_{t-1} - \\tilde{h}_t)$. The gradient with respect to the bias term $b_z$ does not depend directly on the specific values of $x_t$ or $h_{t-1}$ apart from their influence on $z_t$.\n\nIn all three cases, the learning signal is modulated by two key factors:\n- The term $z_t(1 - z_t)$, which is the derivative of the sigmoid gate. This term is maximal when $z_t=0.5$ and approaches $0$ as $z_t$ approaches $0$ or $1$. This means learning is most active when the gate is \"uncertain\" and slows down when the gate is saturated (fully open or closed), a characteristic that can contribute to the vanishing gradient problem.\n- The term $(h_{t-1} - \\tilde{h}_t)$, which represents the difference between the previous state and the new candidate state. The gradient is proportional to this difference. If the candidate state is very different from the previous state, the update gate's parameters have a larger gradient, reflecting their heightened importance in deciding the final state $h_t$. Conversely, if $h_{t-1}$ and $\\tilde{h}_t$ are nearly identical, the update gate's value has little effect, and its associated gradients are small.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_t z_t (1 - z_t) (h_{t-1} - \\tilde{h}_t) & h_{t-1} z_t (1 - z_t) (h_{t-1} - \\tilde{h}_t) & z_t (1 - z_t) (h_{t-1} - \\tilde{h}_t)\n\\end{pmatrix}\n}\n$$", "id": "3128076"}, {"introduction": "Now that we have seen how a GRU's gates are trained, let's explore their functional purpose. The update gate, $z_t$, is often described as controlling how much of the past is remembered. In this practice, we make this concept precise by analyzing a synthetic 'copy' task [@problem_id:3128117]. You will derive the direct mathematical relationship between the update gate's average activation and the characteristic timescale over which the network can reliably retain information, providing a quantitative link between a network component and its memory capacity.", "problem": "You are asked to design and analyze a synthetic \"copy\" task for a Gated Recurrent Unit (GRU) that isolates and quantifies how the update gate controls memory timescales. Work in a purely mathematical setting with a scalar hidden state. Use the following core definitions and assumptions as the foundational base for your reasoning:\n\n- A GRU updates its hidden state using the update gate. For a scalar hidden state, the recurrence at step $t$ is modeled as $h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$, where $z_t \\in (0,1)$ is the update gate and $\\tilde{h}_t$ is the candidate state.\n- In the synthetic copy task, an input sequence of length $T$ is written into the hidden state over $T$ steps, then the network receives $T$ steps of neutral input (during which the candidate state equals zero, i.e., $\\tilde{h}_t = 0$), followed by a readout step. The readout is considered reliable if the retained magnitude of the originally stored hidden component at the readout step is at least a fraction $\\rho \\in (0,1)$ of its magnitude immediately after the write phase.\n- During the neutral-input delay phase of $T$ steps, the update gate variables $z_t$ are identically distributed and independent with mean $\\bar{z}$, and the multiplicative retention factor per step equals $(1 - z_t)$.\n\nTask:\n- Derive from first principles the requirement that guarantees reliable retention at the readout step and determine the minimal mean update gate $\\bar{z}_{\\min}$ needed to meet the reliability threshold $\\rho$ for a sequence of length $T$. Express $\\bar{z}_{\\min}$ in terms of $T$ and $\\rho$ only.\n- Relate your result to a characteristic timescale $\\tau$ controlled by the update gate by defining the exact mapping between the per-step retention factor and $\\tau$ for constant gating. Give the minimal timescale $\\tau_{\\min}$ implied by your derived $\\bar{z}_{\\min}$ and provide its small-$\\bar{z}$ approximation in terms of $T$ and $\\rho$.\n\nTest suite:\nCompute $\\bar{z}_{\\min}$ and $\\tau_{\\min}$ for the following parameter pairs $(T,\\rho)$:\n- Case $1$: $T = 10$, $\\rho = 0.5$.\n- Case $2$: $T = 50$, $\\rho = 0.9$.\n- Case $3$ (boundary): $T = 1$, $\\rho = 0.99$.\n- Case $4$ (edge): $T = 100$, $\\rho = 0.01$.\n- Case $5$: $T = 8$, $\\rho = 0.8$.\n\nOutput specification:\n- Your program must produce a single line containing the results for all five cases as a comma-separated list of per-case results, enclosed in square brackets.\n- Each per-case result must be the two-element list $[\\bar{z}_{\\min}, \\tau_{\\min}]$, with both values rounded to $6$ decimal places.\n- The final output must therefore have the form $[[\\bar{z}_{\\min}^{(1)}, \\tau_{\\min}^{(1)}],[\\bar{z}_{\\min}^{(2)}, \\tau_{\\min}^{(2)}],\\dots,[\\bar{z}_{\\min}^{(5)}, \\tau_{\\min}^{(5)}]]$.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the established mathematical formulation of a Gated Recurrent Unit (GRU) and poses a well-defined question about its memory properties. The synthetic \"copy\" task is a standard method for analyzing the behavior of recurrent neural networks. The language is objective and the parameters are specified, allowing for a unique solution.\n\nA minor ambiguity exists in the phrasing \"minimal mean update gate $\\bar{z}_{\\min}$\". As will be shown, the derived requirement for reliable retention establishes an upper bound on the mean update gate, $\\bar{z} \\le C$, where $C$ is a constant determined by the problem parameters. In this context, a \"minimal\" value is not uniquely defined, as any value approaching $0$ would satisfy the condition. The most logical and scientifically sound interpretation is that the problem asks for the critical boundary value of this inequality, i.e., the maximum permissible mean update gate. This value represents the fastest rate of \"forgetting\" that still meets the retention criterion. We will therefore interpret $\\bar{z}_{\\min}$ as this boundary value, which we shall derive.\n\nThe solution proceeds in three parts. First, we derive the requirement on the mean update gate $\\bar{z}$ from first principles. Second, we establish the relationship between $\\bar{z}$ and the memory timescale $\\tau$. Third, we use these results to compute the required quantities.\n\n**1. Derivation of the Mean Update Gate Requirement**\n\nThe problem defines the update of the scalar hidden state $h_t$ at time step $t$ as:\n$$h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$$\nHere, $z_t \\in (0,1)$ is the update gate activation and $\\tilde{h}_t$ is the candidate state.\n\nThe task involves a write phase of $T$ steps, followed by a neutral-input delay phase of $T$ steps. Let $h_T$ be the hidden state at the end of the write phase. During the subsequent delay phase, from step $t=T+1$ to $t=2T$, the input is neutral, which is specified to mean the candidate state is zero: $\\tilde{h}_t = 0$.\n\nFor $t \\in \\{T+1, \\dots, 2T\\}$, the recurrence simplifies to:\n$$h_t = (1 - z_t) h_{t-1}$$\nWe can unroll this recurrence relationship over the $T$ steps of the delay phase to express the final state $h_{2T}$ in terms of the initial state $h_T$:\n$$h_{T+1} = (1 - z_{T+1}) h_T$$\n$$h_{T+2} = (1 - z_{T+2}) h_{T+1} = (1 - z_{T+2})(1 - z_{T+1}) h_T$$\n$$\\vdots$$\n$$h_{2T} = \\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) h_T$$\nThe total retention factor after $T$ delay steps is the product $\\prod_{i=1}^{T} (1 - z_{T+i})$.\n\nThe reliability condition states that the magnitude of the hidden state at the readout step, $|h_{2T}|$, must be at least a fraction $\\rho \\in (0,1)$ of its magnitude immediately after the write phase, $|h_T|$:\n$$|h_{2T}| \\ge \\rho |h_T|$$\nSubstituting the expression for $h_{2T}$:\n$$\\left| \\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) h_T \\right| \\ge \\rho |h_T|$$\nSince $z_t \\in (0,1)$, the term $(1 - z_t)$ is always positive, so their product is also positive. We can simplify the expression (assuming $h_T \\neq 0$):\n$$\\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) \\ge \\rho$$\nThe problem states that the update gate variables $z_t$ during the delay phase are independent and identically distributed (i.i.d.) random variables with mean $\\bar{z}$. Therefore, the condition on the retention factor must be interpreted in expectation. We take the expectation of both sides of the inequality:\n$$E\\left[ \\prod_{i=1}^{T} (1 - z_{T+i}) \\right] \\ge \\rho$$\nDue to the independence of the $z_t$ variables, the expectation of the product is the product of the expectations:\n$$\\prod_{i=1}^{T} E[1 - z_{T+i}] \\ge \\rho$$\nSince the variables are identically distributed with $E[z_t] = \\bar{z}$, we have $E[1 - z_t] = 1 - E[z_t] = 1 - \\bar{z}$. The inequality becomes:\n$$\\prod_{i=1}^{T} (1 - \\bar{z}) \\ge \\rho$$\n$$(1 - \\bar{z})^T \\ge \\rho$$\nTo find the boundary value for $\\bar{z}$, we solve this inequality. Since $1 - \\bar{z} > 0$ and $\\rho > 0$, we can take the $T$-th root of both sides:\n$$1 - \\bar{z} \\ge \\rho^{1/T}$$\n$$\\bar{z} \\le 1 - \\rho^{1/T}$$\nThis inequality specifies the requirement for reliable retention. As discussed, we define $\\bar{z}_{\\min}$ to be the boundary value of this condition, which is the maximum permissible value for $\\bar{z}$:\n$$\\bar{z}_{\\min} = 1 - \\rho^{1/T}$$\n\n**2. Derivation of the Minimal Timescale $\\tau_{\\min}$**\n\nThe characteristic timescale $\\tau$ is defined by relating the discrete per-step retention factor to a continuous exponential decay process, $e^{-1/\\tau}$. For a constant gating value $z_t = \\bar{z}$, the per-step retention factor is $(1 - \\bar{z})$. The mapping is:\n$$1 - \\bar{z} = e^{-1/\\tau}$$\nSolving for $\\tau$ gives its relationship with $\\bar{z}$:\n$$\\ln(1 - \\bar{z}) = -1/\\tau$$\n$$\\tau = -\\frac{1}{\\ln(1 - \\bar{z})}$$\nA smaller $\\bar{z}$ corresponds to a larger retention factor $(1 - \\bar{z})$ and thus a longer memory timescale $\\tau$. The requirement $\\bar{z} \\le \\bar{z}_{\\min}$ translates to a requirement on the minimum timescale needed. The minimal timescale, $\\tau_{\\min}$, corresponds to the largest allowed value of $\\bar{z}$, which is $\\bar{z}_{\\min}$.\n$$\\tau_{\\min} = -\\frac{1}{\\ln(1 - \\bar{z}_{\\min})}$$\nSubstituting our expression for $\\bar{z}_{\\min}$:\n$$\\tau_{\\min} = -\\frac{1}{\\ln(1 - (1 - \\rho^{1/T}))} = -\\frac{1}{\\ln(\\rho^{1/T})}$$\nUsing the logarithm property $\\ln(a^b) = b \\ln(a)$:\n$$\\tau_{\\min} = -\\frac{1}{\\frac{1}{T} \\ln(\\rho)} = -\\frac{T}{\\ln(\\rho)}$$\n\n**3. Small-$\\bar{z}$ Approximation**\n\nThe problem also asks for the small-$\\bar{z}$ approximation for the timescale. The general relationship is $\\tau = -1/\\ln(1-\\bar{z})$. For small values of $x$, the Taylor series expansion of $\\ln(1-x)$ is $\\ln(1-x) \\approx -x - x^2/2 - \\dots$. The first-order approximation is $\\ln(1-x) \\approx -x$.\nApplying this to the timescale equation yields the small-$\\bar{z}$ approximation:\n$$\\tau \\approx -\\frac{1}{-\\bar{z}} = \\frac{1}{\\bar{z}}$$\nThis shows that for small update gate values, the memory timescale is approximately the reciprocal of the mean gate activation. The approximation for $\\tau_{\\min}$ in terms of $T$ and $\\rho$ is obtained by using this relation with our derived expression for $\\bar{z}_{\\min}$:\n$$\\tau_{\\min, \\text{approx}} = \\frac{1}{\\bar{z}_{\\min}} = \\frac{1}{1 - \\rho^{1/T}}$$\nThe final calculations will use the exact formulas for $\\bar{z}_{\\min}$ and $\\tau_{\\min}$.\n\nThe formulas for computation are:\n$$ \\bar{z}_{\\min} = 1 - \\rho^{1/T} $$\n$$ \\tau_{\\min} = -\\frac{T}{\\ln(\\rho)} $$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GRU memory timescale problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, rho).\n    test_cases = [\n        (10, 0.5),   # Case 1\n        (50, 0.9),   # Case 2\n        (1, 0.99),   # Case 3\n        (100, 0.01), # Case 4\n        (8, 0.8),    # Case 5\n    ]\n\n    results_as_strings = []\n    for T, rho in test_cases:\n        # T is the sequence length (and delay duration).\n        # rho is the reliability threshold for retention.\n\n        # From first principles, the condition on the mean update gate z_bar is:\n        #   (1 - z_bar)^T >= rho\n        # This implies z_bar <= 1 - rho^(1/T).\n        # The problem asks for z_min, which is interpreted as the boundary value\n        # (the maximum permissible value) of this inequality.\n        z_min = 1 - np.power(rho, 1/T)\n\n        # The timescale tau is related to z_bar by (1 - z_bar) = exp(-1/tau).\n        #   tau = -1 / ln(1 - z_bar)\n        # The minimal required timescale tau_min corresponds to the maximal z_bar.\n        #   tau_min = -1 / ln(1 - z_min) = -1 / ln(rho^(1/T)) = -T / ln(rho).\n        tau_min = -T / np.log(rho)\n\n        # Format the results as a string \"[z_min_val,tau_min_val]\" with 6 decimal places.\n        case_result_str = f\"[{z_min:.6f},{tau_min:.6f}]\"\n        results_as_strings.append(case_result_str)\n\n    # The final output must be a single line in the specified format:\n    # [[z_min_1,tau_min_1],[z_min_2,tau_min_2],...]\n    final_output = f\"[{','.join(results_as_strings)}]\"\n    print(final_output)\n\n# Run the solver.\nsolve()\n```", "id": "3128117"}, {"introduction": "In practice, choosing the right model architecture is critical. While LSTMs are powerful, the GRU's simpler design offers a key advantage in certain scenarios. This exercise explores the principle of capacity control to explain why a GRU can outperform an LSTM on smaller datasets [@problem_id:3128080]. By deriving the parameter counts for both models and applying a simplified model of test error, you will quantify how the GRU's greater parsimony can lead to better generalization by reducing the risk of overfitting.", "problem": "You are asked to formalize and compute a principled comparison between two recurrent sequence models: Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTM) networks. The goal is to demonstrate cases where GRUs outperform LSTMs on small datasets due to fewer parameters, to quantify the improvement in predicted test error, and to relate this improvement to capacity control.\n\nStarting point: Use Empirical Risk Minimization and capacity control via Structural Risk Minimization as the fundamental base. Assume that the predicted test mean squared error is the sum of an irreducible noise term and a capacity penalty that grows with model size and decreases with sample size. Concretely, model the predicted test mean squared error as $$\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N},$$ where $\\sigma^2$ is the irreducible noise variance, $\\lambda$ is a positive constant capturing problem complexity and training regimen, $p$ is the total number of trainable parameters in the network, and $N$ is the number of training sequences.\n\nDefinition of architecture components to derive parameter counts from first principles:\n- A Gated Recurrent Unit (GRU) has $3$ distinct affine transformations per time step: one for the update gate, one for the reset gate, and one for the candidate hidden state. Each transformation connects an input of dimension $x$ and the previous hidden state of dimension $h$ to a current hidden state of dimension $h$, and includes biases of dimension $h$.\n- A Long Short-Term Memory (LSTM) network has $4$ distinct affine transformations per time step: one each for the input gate, forget gate, cell candidate, and output gate. Each transformation connects an input of dimension $x$ and the previous hidden state of dimension $h$ to a current hidden state of dimension $h$, and includes biases of dimension $h$.\n- Both models are followed by a linear output layer mapping a hidden state of dimension $h$ to an output of dimension $y$ with a bias of dimension $y$.\n\nFrom these definitions, derive the total number of trainable parameters $p_{\\text{GRU}}$ and $p_{\\text{LSTM}}$ as functions of $x$, $h$, and $y$; then use the predicted test mean squared error model $\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N}$ to compute, for each test case, the difference in predicted test error between LSTM and GRU, defined as $$\\Delta = \\mathcal{E}_{\\text{LSTM}} - \\mathcal{E}_{\\text{GRU}}.$$ Report $\\Delta$ for each test case, rounded to six decimal places. A positive $\\Delta$ indicates that the GRU is predicted to have a lower test error than the LSTM under capacity control for the given small dataset.\n\nImplement a program that:\n- Takes the following fixed test suite of parameter values (provided below).\n- For each test case, computes $p_{\\text{GRU}}$, $p_{\\text{LSTM}}$, $\\mathcal{E}_{\\text{GRU}}$, $\\mathcal{E}_{\\text{LSTM}}$, and $\\Delta$.\n- Produces a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$), with each $\\Delta$ rounded to six decimal places.\n\nTest suite (each tuple is $(x,h,y,N,\\sigma^2,\\lambda)$):\n- Case $1$ (happy path, small dataset): $(x=\\;5,\\; h=\\;16,\\; y=\\;1,\\; N=\\;64,\\; \\sigma^2=\\;0.05,\\; \\lambda=\\;0.05)$\n- Case $2$ (boundary with larger dataset where capacity penalty diminishes): $(x=\\;5,\\; h=\\;16,\\; y=\\;1,\\; N=\\;2048,\\; \\sigma^2=\\;0.05,\\; \\lambda=\\;0.05)$\n- Case $3$ (edge case with minimal hidden size): $(x=\\;1,\\; h=\\;1,\\; y=\\;1,\\; N=\\;32,\\; \\sigma^2=\\;0.01,\\; \\lambda=\\;0.05)$\n- Case $4$ (small dataset with larger hidden size amplifying capacity differences): $(x=\\;8,\\; h=\\;32,\\; y=\\;2,\\; N=\\;16,\\; \\sigma^2=\\;0.02,\\; \\lambda=\\;0.05)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, with each value rounded to six decimal places and no spaces.", "solution": "The problem has been validated and found to be self-contained, scientifically grounded in statistical learning theory, and well-posed. All required variables and definitions are provided, and no contradictions exist. The problem is a formal exercise in deriving and comparing the parameter complexity of two common recurrent neural network architectures, GRU and LSTM, and analyzing the impact of this complexity on predicted test error using a simplified capacity control model.\n\nThe solution proceeds in two stages. First, we derive the general formulas for the number of trainable parameters for both a Gated Recurrent Unit (GRU) and a Long Short-Term Memory (LSTM) network based on the provided definitions. Second, we use these formulas to compute the difference in predicted test error, $\\Delta$, for each test case.\n\nThe core of both GRU and LSTM cells consists of affine transformations. An affine transformation mapping an input vector of dimension $d_{\\text{in}}$ to an output vector of dimension $d_{\\text{out}}$ is of the form $\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}$. The weight matrix $W$ has dimensions $d_{\\text{out}} \\times d_{\\text{in}}$, contributing $d_{\\text{out}} \\cdot d_{\\text{in}}$ parameters. The bias vector $\\mathbf{b}$ has dimension $d_{\\text{out}}$, contributing $d_{\\text{out}}$ parameters. The total number of parameters for one such transformation is $d_{\\text{out}} \\cdot d_{\\text{in}} + d_{\\text{out}}$.\n\nFor both GRU and LSTM cells, the input to each internal gate at a given time step is the concatenation of the external input vector $\\mathbf{x}_t$ (dimension $x$) and the previous hidden state $\\mathbf{h}_{t-1}$ (dimension $h$). Therefore, the input dimension for the affine transformations within the recurrent cell is $d_{\\text{in}} = x + h$. The output of these transformations is a vector of dimension $h$, so $d_{\\text{out}} = h$. The number of parameters for a single gate's transformation is thus $h \\cdot (x + h) + h = hx + h^2 + h$.\n\nAccording to the problem definition:\nA GRU cell has $3$ such affine transformations (for the update gate, reset gate, and candidate activation). The total number of parameters in the GRU cell, $p_{\\text{GRU,cell}}$, is:\n$$p_{\\text{GRU,cell}} = 3 \\cdot (h^2 + hx + h)$$\nAn LSTM cell has $4$ such affine transformations (for the input gate, forget gate, cell candidate, and output gate). The total number of parameters in the LSTM cell, $p_{\\text{LSTM,cell}}$, is:\n$$p_{\\text{LSTM,cell}} = 4 \\cdot (h^2 + hx + h)$$\n\nBoth architectures are followed by an identical linear output layer. This layer maps the final hidden state of dimension $h$ to an output of dimension $y$. For this transformation, $d_{\\text{in}} = h$ and $d_{\\text{out}} = y$. The number of parameters in the output layer, $p_{\\text{out}}$, is:\n$$p_{\\text{out}} = yh + y$$\n\nThe total number of trainable parameters, $p$, for each model is the sum of the parameters in the recurrent cell and the output layer.\n$$p_{\\text{GRU}} = p_{\\text{GRU,cell}} + p_{\\text{out}} = 3(h^2 + hx + h) + (hy + y)$$\n$$p_{\\text{LSTM}} = p_{\\text{LSTM,cell}} + p_{\\text{out}} = 4(h^2 + hx + h) + (hy + y)$$\n\nThe problem asks for the difference in predicted test error, $\\Delta = \\mathcal{E}_{\\text{LSTM}} - \\mathcal{E}_{\\text{GRU}}$. The predicted test error is given by the model $\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N}$.\nSubstituting the expressions for the errors:\n$$\\Delta = \\left(\\sigma^2 + \\lambda \\frac{p_{\\text{LSTM}}}{N}\\right) - \\left(\\sigma^2 + \\lambda \\frac{p_{\\text{GRU}}}{N}\\right)$$\nThe irreducible error term $\\sigma^2$ cancels out, simplifying the expression to:\n$$\\Delta = \\frac{\\lambda}{N} (p_{\\text{LSTM}} - p_{\\text{GRU}})$$\nNow, we compute the difference in the number of parameters:\n$$p_{\\text{LSTM}} - p_{\\text{GRU}} = \\left(4(h^2 + hx + h) + (hy + y)\\right) - \\left(3(h^2 + hx + h) + (hy + y)\\right)$$\nThe parameters from the identical output layer, $(hy + y)$, cancel out, leaving:\n$$p_{\\text{LSTM}} - p_{\\text{GRU}} = (4 - 3)(h^2 + hx + h) = h^2 + hx + h$$\nSubstituting this back into the expression for $\\Delta$, we obtain the final formula for the calculation:\n$$\\Delta = \\frac{\\lambda}{N} (h^2 + hx + h)$$\nThis elegantly demonstrates that the difference in predicted error, under this model, is solely a function of the increased parameter count of the LSTM cell, scaled by the problem complexity $\\lambda$ and inversely by the number of samples $N$. A positive $\\Delta$ indicates a performance advantage for the GRU due to its greater parsimony.\n\nWe now apply this formula to each test case:\n\nCase 1: $(x=5, h=16, y=1, N=64, \\sigma^2=0.05, \\lambda=0.05)$\n$$\\Delta_1 = \\frac{0.05}{64} (16^2 + 16 \\cdot 5 + 16) = \\frac{0.05}{64} (256 + 80 + 16) = \\frac{0.05}{64} (352) = 0.275$$\n\nCase 2: $(x=5, h=16, y=1, N=2048, \\sigma^2=0.05, \\lambda=0.05)$\n$$\\Delta_2 = \\frac{0.05}{2048} (16^2 + 16 \\cdot 5 + 16) = \\frac{0.05}{2048} (352) \\approx 0.00859375$$\n\nCase 3: $(x=1, h=1, y=1, N=32, \\sigma^2=0.01, \\lambda=0.05)$\n$$\\Delta_3 = \\frac{0.05}{32} (1^2 + 1 \\cdot 1 + 1) = \\frac{0.05}{32} (1 + 1 + 1) = \\frac{0.05}{32} (3) = 0.0046875$$\n\nCase 4: $(x=8, h=32, y=2, N=16, \\sigma^2=0.02, \\lambda=0.05)$\n$$\\Delta_4 = \\frac{0.05}{16} (32^2 + 32 \\cdot 8 + 32) = \\frac{0.05}{16} (1024 + 256 + 32) = \\frac{0.05}{16} (1312) = 4.1$$\n\nThese results are then rounded to six decimal places for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference in predicted test error between LSTM and GRU models\n    based on a simplified capacity control model.\n    \"\"\"\n    # Test suite: (x, h, y, N, sigma_sq, lambda_val)\n    test_cases = [\n        # Case 1 (happy path, small dataset)\n        (5, 16, 1, 64, 0.05, 0.05),\n        # Case 2 (boundary with larger dataset where capacity penalty diminishes)\n        (5, 16, 1, 2048, 0.05, 0.05),\n        # Case 3 (edge case with minimal hidden size)\n        (1, 1, 1, 32, 0.01, 0.05),\n        # Case 4 (small dataset with larger hidden size amplifying capacity differences)\n        (8, 32, 2, 16, 0.02, 0.05)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x, h, y, N, sigma_sq, lambda_val = case\n\n        # The difference in the number of parameters between an LSTM and a GRU cell is\n        # p_diff = (4 * (h^2 + h*x + h)) - (3 * (h^2 + h*x + h))\n        # p_diff = h^2 + h*x + h\n        p_diff = h**2 + h * x + h\n        \n        # The difference in predicted test error is Delta = (lambda/N) * p_diff\n        delta = (lambda_val / N) * p_diff\n        \n        # The problem statement requires rounding to six decimal places.\n        # Using an f-string with a format specifier handles both rounding and formatting.\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3128080"}]}