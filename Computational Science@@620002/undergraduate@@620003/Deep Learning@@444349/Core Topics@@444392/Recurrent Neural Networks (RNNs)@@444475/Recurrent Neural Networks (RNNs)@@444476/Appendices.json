{"hands_on_practices": [{"introduction": "To truly master Recurrent Neural Networks, we must first understand their fundamental mechanics and potential pitfalls. This first practice dives into the heart of the vanilla RNN, exploring how gradients flow backward through time during training. By implementing a simple RNN from scratch, you will investigate how the choice of activation function—hyperbolic tangent ($\\tanh$) versus the Rectified Linear Unit (ReLU)—and weight initialization directly impact gradient stability and can lead to common issues like vanishing gradients or \"dying\" neurons [@problem_id:3168374].", "problem": "You are asked to implement and analyze a simple, fully connected, vanilla Recurrent Neural Network (RNN) under two activation functions: hyperbolic tangent ($\\tanh$) and Rectified Linear Unit (ReLU). Your analysis must be grounded in the fundamental definition of a vanilla RNN and the chain rule for gradients in discrete-time dynamical systems. The goal is to compare gradient flow and dead-unit frequency as the sequence length grows.\n\nFundamental base (definitions to use):\n- The RNN hidden state follows the recurrence $h_t = \\phi\\!\\left(W_{hh}\\,h_{t-1} + W_{xh}\\,x_t + b\\right)$ for $t \\in \\{1,\\dots,T\\}$, with $h_0 = 0$.\n- The loss is defined as $L = \\frac{1}{2}\\left\\|h_T\\right\\|_2^2$.\n- By the chain rule, the gradient propagates backwards in time according to $\\delta_t = \\frac{\\partial L}{\\partial h_t}$ with $\\delta_T = h_T$ and, for $t \\ge 1$, $\\delta_{t-1} = W_{hh}^\\top \\left(\\phi'(a_t) \\odot \\delta_t\\right)$ where $a_t = W_{hh}\\,h_{t-1} + W_{xh}\\,x_t + b$ and $\\odot$ denotes elementwise multiplication.\n\nActivation functions:\n- For $\\tanh$, use $\\phi(z) = \\tanh(z)$ and $\\phi'(z) = 1 - \\tanh^2(z)$.\n- For ReLU, use $\\phi(z) = \\max(0,z)$ and $\\phi'(z) = \\mathbf{1}_{\\{z>0\\}}$.\n\nInitialization and inputs:\n- Hidden size is $n$, input size is $d$, and sequence length is $T$.\n- Initialize $W_{hh} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^2}{n}\\right)$ elementwise and $W_{xh} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^2}{d}\\right)$ elementwise, with bias $b=0$.\n- For a batch of $N$ independent sequences, draw inputs $x_t \\in \\mathbb{R}^d$ i.i.d. as standard normal for each time step and each sequence.\n- Use $N = 64$ sequences per test, $n = 64$, $d = 32$, and $h_0 = 0$ for all sequences.\n- Use the same fixed random seed $0$ per test case to ensure reproducibility.\n\nQuantities to compute per test case:\n- Gradient flow metric: For each sequence, compute $\\delta_0 = \\frac{\\partial L}{\\partial h_0}$ via the recurrence above, then compute its Euclidean norm and record $\\log_{10}\\!\\left(\\|\\delta_0\\|_2 + 10^{-12}\\right)$. Report the batch average of this quantity as a single float.\n- Dead-unit frequency:\n  - For ReLU, declare a hidden unit “dead” if its derivative is zero at all time steps for all sequences, that is, if $\\phi'(a_t)=0$ for all $t \\in \\{1,\\dots,T\\}$ and across all $N$ sequences. Report the fraction of dead units among the $n$ hidden units as a float in $[0,1]$.\n  - For $\\tanh$, declare a hidden unit “near-dead” if its derivative magnitude remains below a small threshold $\\epsilon$ at all time steps for all sequences, i.e., if $\\left|\\phi'(a_t)\\right| < \\epsilon$ for all $t$ and all $N$ sequences. Use $\\epsilon = 10^{-3}$. Report the fraction of near-dead units among the $n$ hidden units as a float in $[0,1]$.\n\nNumerical units:\n- There are no physical units in this problem. Angles, if any, are in radians by definition of $\\tanh$ and ReLU which do not require angle units. All reported fractions must be in decimal form.\n\nTest suite:\nFor each test case, the parameters are $(\\text{activation}, n, d, T, g)$ in the following order:\n- Case $1$: $(\\tanh, 64, 32, 5, 0.5)$.\n- Case $2$: $(\\tanh, 64, 32, 50, 0.5)$.\n- Case $3$: $(\\tanh, 64, 32, 50, 1.5)$.\n- Case $4$: $(\\text{ReLU}, 64, 32, 5, 0.5)$.\n- Case $5$: $(\\text{ReLU}, 64, 32, 50, 0.5)$.\n- Case $6$: $(\\text{ReLU}, 64, 32, 50, 1.5)$.\n- Case $7$ (boundary): $(\\text{ReLU}, 64, 32, 20, 0.0)$.\n\nProgram requirements:\n- Implement the RNN forward and backward passes exactly as stated above.\n- For each test case, compute:\n  - The average gradient flow metric over the $N$ sequences.\n  - The dead-unit (or near-dead) fraction as defined.\n- Your program should produce a single line of output containing a comma-separated list of pairs, one pair per test case, in the same order as above. Each pair must be of the form $[\\text{avg\\_log10\\_grad\\_norm}, \\text{dead\\_fraction}]$ and the entire output must be enclosed in a single top-level pair of square brackets. For example: $[[x_1,y_1],[x_2,y_2],\\dots]$. The floats should be printed with exactly $6$ digits after the decimal point.", "solution": "The problem is valid. It presents a well-defined computational experiment to analyze the behavior of vanilla Recurrent Neural Networks (RNNs). The definitions, equations, initial conditions, and parameters are all specified with sufficient precision to permit a unique, verifiable solution. The problem is scientifically grounded in the established principles of neural network theory, specifically backpropagation through time (BPTT), and investigates recognized phenomena such as the vanishing/exploding gradient problem and the \"dying neuron\" issue.\n\nThe analysis proceeds by implementing the specified RNN dynamics and then computing the required metrics for each test case.\n\n### Mathematical Formulation\n\nThe core of the problem lies in the discrete-time dynamics of the RNN.\n\n**1. Forward Pass:**\nThe hidden state $h_t \\in \\mathbb{R}^n$ at time step $t$ is computed based on the previous hidden state $h_{t-1}$ and the current input $x_t \\in \\mathbb{R}^d$. For a batch of $N$ sequences, we can write this in matrix form. Let $H_{t-1}$ be the $(N \\times n)$ matrix of hidden states for the batch at time $t-1$, and $X_t$ be the $(N \\times d)$ matrix of inputs. The pre-activation matrix $A_t \\in \\mathbb{R}^{N \\times n}$ and the new hidden state matrix $H_t \\in \\mathbb{R}^{N \\times n}$ are given by:\n$$A_t = H_{t-1} W_{hh}^\\top + X_t W_{xh}^\\top + \\mathbf{1}b^\\top$$\n$$H_t = \\phi(A_t)$$\nwhere $\\phi$ is the element-wise activation function, $W_{hh} \\in \\mathbb{R}^{n \\times n}$ and $W_{xh} \\in \\mathbb{R}^{n \\times d}$ are the weight matrices, $b \\in \\mathbb{R}^n$ is the bias vector (given as $b=0$), and $\\mathbf{1}$ is a column vector of ones of size $N$. The process starts with an initial hidden state $H_0$, which is a zero matrix of size $(N \\times n)$. This recurrence is applied for $t = 1, \\dots, T$.\n\n**2. Loss Function:**\nThe loss $L$ is a scalar value computed from the final hidden state $H_T$ of the batch.\n$$L = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2} \\|h_{T,i}\\|_2^2 = \\frac{1}{2N} \\text{Tr}(H_T H_T^\\top)$$\nThe problem statement defines the loss for a single sequence as $L_i = \\frac{1}{2}\\|h_{T,i}\\|_2^2$. The gradient derivation that follows is for this single-sequence loss.\n\n**3. Backward Pass (Backpropagation Through Time):**\nThe goal is to compute the gradient of the loss with respect to the initial state, $\\frac{\\partial L}{\\partial h_0}$. We define the gradient of the loss with respect to the hidden state at time $t$ as $\\delta_t = \\frac{\\partial L}{\\partial h_t}$.\nThe chain rule gives a backward recurrence for this gradient. The process starts at the final time step $T$:\n$$\\delta_T = \\frac{\\partial L}{\\partial h_T} = \\frac{\\partial}{\\partial h_T} \\left( \\frac{1}{2}h_T^\\top h_T \\right) = h_T$$\nFor time steps $t < T$, the gradient $\\delta_t$ depends on $\\delta_{t+1}$. The total derivative of $L$ with respect to $h_t$ is:\n$$\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial h_{t+1}}{\\partial h_t}^\\top \\frac{\\partial L}{\\partial h_{t+1}} = \\frac{\\partial h_{t+1}}{\\partial h_t}^\\top \\delta_{t+1}$$\nThe Jacobian matrix $\\frac{\\partial h_{t+1}}{\\partial h_t}$ is derived from the forward pass equation $h_{t+1} = \\phi(W_{hh}h_t + W_{xh}x_{t+1} + b)$.\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = \\text{diag}(\\phi'(a_{t+1})) W_{hh}$$\nwhere $a_{t+1} = W_{hh}h_t + W_{xh}x_{t+1} + b$ and $\\phi'$ is the derivative of the activation function. The notation $\\text{diag}(v)$ creates a diagonal matrix from a vector $v$.\nSubstituting this back, we get the gradient recurrence:\n$$\\delta_t = W_{hh}^\\top \\text{diag}(\\phi'(a_{t+1})) \\delta_{t+1} = W_{hh}^\\top (\\phi'(a_{t+1}) \\odot \\delta_{t+1})$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product. This matches the equation provided in the problem. The recurrence is executed for $t = T-1, \\dots, 0$.\n\n### Algorithmic Procedure\n\nFor each test case $(\\text{activation}, n, d, T, g)$:\n\n1.  **Initialization:**\n    -   Set the random seed to $0$ for reproducibility.\n    -   Set parameters $N=64$ and $\\epsilon=10^{-3}$.\n    -   Initialize weights $W_{hh} \\in \\mathbb{R}^{n \\times n}$ and $W_{xh} \\in \\mathbb{R}^{n \\times d}$ by drawing from Gaussian distributions $\\mathcal{N}(0, \\sigma^2)$, where the standard deviations are $\\sigma_{hh} = g / \\sqrt{n}$ and $\\sigma_{xh} = g / \\sqrt{d}$, respectively.\n    -   Initialize inputs $X \\in \\mathbb{R}^{N \\times T \\times d}$ from a standard normal distribution $\\mathcal{N}(0, 1)$.\n    -   Initialize the first hidden state $H_0 \\in \\mathbb{R}^{N \\times n}$ to a matrix of zeros.\n\n2.  **Forward Pass:**\n    -   Store all hidden states $H_t$ and pre-activations $A_t$ for $t=1, \\dots, T$.\n    -   Iterate from $t=1$ to $T$:\n        -   Compute $A_t = H_{t-1} W_{hh}^\\top + X_{:, t-1, :} W_{xh}^\\top$.\n        -   Compute $H_t = \\phi(A_t)$, using either $\\tanh$ or $\\text{ReLU}$ as specified.\n        -   Store $A_t$ and $H_t$.\n\n3.  **Backward Pass:**\n    -   Initialize the batch of gradients $\\Delta_T = H_T$.\n    -   Create a container `All_Phi_Primes` of shape $(T \\times N \\times n)$ to store derivatives.\n    -   Iterate backwards from $t=T$ down to $1$:\n        -   Retrieve the pre-activation $A_t$ and hidden state $H_t$.\n        -   Compute the derivative matrix $\\Phi'_t$:\n            -   For $\\tanh$: $\\Phi'_t = 1 - H_t \\odot H_t$.\n            -   For $\\text{ReLU}$: $\\Phi'_t = \\mathbf{1}_{\\{A_t > 0\\}}$.\n        -   Store $\\Phi'_t$ in `All_Phi_Primes`.\n        -   Update the gradient: $\\Delta_{t-1} = (\\Delta_t \\odot \\Phi'_t) W_{hh}$. Let $\\Delta_t$ be the updated gradient variable. After this step, the variable we call $\\Delta_t$ now holds the value for $\\Delta_{t-1}$, ready for the next iteration.\n    -   The final value of the gradient variable after the loop is $\\Delta_0$.\n\n4.  **Metric Computation:**\n    -   **Gradient Flow Metric:**\n        -   Compute the Euclidean norm of each row of $\\Delta_0$ (i.e., for each sequence in the batch). This gives a vector of $N$ norms, $\\|\\delta_{0,i}\\|_2$.\n        -   Apply the transformation $\\log_{10}(\\|\\delta_{0,i}\\|_2 + 10^{-12})$ to each norm.\n        -   Calculate the average of these $N$ values.\n    -   **Dead-Unit Frequency:**\n        -   For each hidden unit $j \\in \\{1, \\dots, n\\}$:\n            -   Check if the unit's derivative is \"dead\" across all $T$ time steps and all $N$ sequences.\n            -   For $\\text{ReLU}$: Check if $(\\text{All\\_Phi\\_Primes})_{t,i,j} == 0$ for all $t, i$.\n            -   For $\\tanh$: Check if $|\\text{All\\_Phi\\_Primes}_{t,i,j}| < \\epsilon$ for all $t, i$.\n        -   Count the number of dead units and divide by the total number of hidden units, $n$.\n\nThis procedure is systematically applied to all test cases, with results formatted as specified. The special case where $g=0$ results in zero weights, leading to zero activations and hidden states throughout the forward pass. Consequently, the initial gradient $\\delta_T$ is zero, and all subsequent gradients $\\delta_t$ are zero, yielding a gradient flow metric of $\\log_{10}(10^{-12}) = -12$. For ReLU with $g=0$, all pre-activations are zero, so their derivatives are zero, making all units \"dead\" and yielding a frequency of $1.0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the RNN analysis for all test cases and print the results.\n    \"\"\"\n\n    def run_rnn_analysis(activation, n, d, T, g, N=64, seed=0):\n        \"\"\"\n        Implements a single run of the RNN forward and backward pass for analysis.\n        \n        Args:\n            activation (str): 'tanh' or 'ReLU'.\n            n (int): Hidden size.\n            d (int): Input size.\n            T (int): Sequence length.\n            g (float): Gain parameter for weight initialization.\n            N (int): Batch size.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            tuple: A pair containing (avg_log10_grad_norm, dead_fraction).\n        \"\"\"\n        np.random.seed(seed)\n        \n        # 1. Initialization\n        # Weight matrices\n        # std_dev = g / sqrt(fan_in)\n        w_hh = (g / np.sqrt(n)) * np.random.randn(n, n)\n        w_xh = (g / np.sqrt(d)) * np.random.randn(n, d)\n        \n        # Input data: (N, T, d) for batch-first processing\n        x = np.random.randn(N, T, d)\n        \n        # Initial hidden state (batch of zero vectors)\n        h0 = np.zeros((N, n))\n\n        # 2. Forward Pass\n        h_states = [h0]\n        a_states = []\n        \n        h_prev = h0\n        for t in range(T):\n            # a_t = h_{t-1} W_hh^T + x_t W_xh^T\n            # Dimensions: (N, n) @ (n, n) + (N, d) @ (d, n) -> (N, n)\n            a = h_prev @ w_hh.T + x[:, t, :] @ w_xh.T\n            \n            if activation == 'tanh':\n                h_curr = np.tanh(a)\n            elif activation == 'ReLU':\n                h_curr = np.maximum(0, a)\n            else:\n                raise ValueError(\"Unsupported activation function\")\n\n            a_states.append(a)\n            h_states.append(h_curr)\n            h_prev = h_curr\n\n        # 3. Backward Pass (BPTT)\n        # delta_t = dL/dh_t. Start with delta_T = h_T\n        delta = h_states[-1]  # H_T, shape (N, n)\n        \n        # Store phi'(a_t) for all t and sequences\n        all_phi_primes = np.zeros((T, N, n))\n\n        # Loop from t=T down to t=1\n        for t in range(T - 1, -1, -1):\n            a_t = a_states[t] # Pre-activation a_{t+1} from problem notation\n            h_t = h_states[t+1] # Hidden state h_{t+1}\n\n            if activation == 'tanh':\n                # phi'(z) = 1 - tanh^2(z) = 1 - h^2\n                phi_prime = 1.0 - h_t**2\n            elif activation == 'ReLU':\n                # phi'(z) = 1 if z > 0, 0 otherwise\n                phi_prime = (a_t > 0).astype(float)\n            \n            all_phi_primes[t, :, :] = phi_prime\n            \n            # delta_{t-1} = (delta_t * phi'(a_t)) @ W_hh\n            # Dimensions: ((N, n) * (N, n)) @ (n, n) -> (N, n)\n            delta = (delta * phi_prime) @ w_hh\n\n        # After loop, delta is delta_0 = dL/dh_0\n\n        # 4. Metric Computation\n        # 4.1 Gradient Flow Metric\n        grad_norms = np.linalg.norm(delta, axis=1)\n        log_grad_norms = np.log10(grad_norms + 1e-12)\n        avg_log10_grad_norm = np.mean(log_grad_norms)\n\n        # 4.2 Dead-Unit Frequency\n        if activation == 'tanh':\n            # Near-dead if |phi'(a_t)| < epsilon for all t, N\n            epsilon = 1e-3\n            is_near_dead_condition = np.abs(all_phi_primes) < epsilon\n            # Check if condition holds for all t (axis 0) and all N (axis 1)\n            unit_is_dead_overall = np.all(is_near_dead_condition, axis=(0, 1))\n        elif activation == 'ReLU':\n            # Dead if phi'(a_t) == 0 for all t, N\n            is_dead_condition = (all_phi_primes == 0)\n            unit_is_dead_overall = np.all(is_dead_condition, axis=(0, 1))\n\n        dead_fraction = np.mean(unit_is_dead_overall)\n\n        return avg_log10_grad_norm, dead_fraction\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('tanh', 64, 32, 5, 0.5),\n        ('tanh', 64, 32, 50, 0.5),\n        ('tanh', 64, 32, 50, 1.5),\n        ('ReLU', 64, 32, 5, 0.5),\n        ('ReLU', 64, 32, 50, 0.5),\n        ('ReLU', 64, 32, 50, 1.5),\n        ('ReLU', 64, 32, 20, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        activation, n, d, T, g = case\n        result_pair = run_rnn_analysis(activation, n, d, T, g)\n        results.append(result_pair)\n\n    # Final print statement in the exact required format.\n    output_str = ','.join([f'[{r[0]:.6f},{r[1]:.6f}]' for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3168374"}, {"introduction": "While vanilla RNNs struggle with long sequences, gated architectures like LSTMs and GRUs were specifically designed to maintain information over extended periods. This practice provides a powerful, quantitative lens through which to understand their effectiveness on a memory-intensive copy task. You will derive an analytical model for memory decay and noise accumulation, directly linking an architecture's average gate values—such as the LSTM's forget gate—to its probability of successfully recalling information after a long delay [@problem_id:3168420].", "problem": "You are given three canonical recurrent sequence models: the Recurrent Neural Network (RNN), the Gated Recurrent Unit (GRU), and the Long Short-Term Memory (LSTM). Consider the following simplified, scalar, linearized memory channel that represents the evolution of a single latent memory component under these architectures during a delayed copy/repeat task. Let the memory variable be denoted by $m_t \\in \\mathbb{R}$, and suppose it evolves according to a first-order autoregressive recursion with independent additive noise,\n$$\nm_t = r \\, m_{t-1} + \\epsilon_t,\n$$\nwhere $r \\in (0,1]$ is a fixed retention coefficient and $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and identically distributed Gaussian perturbations. The initial memory amplitude $m_0$ is given. The retention coefficient $r$ must be instantiated from model-specific gating statistics as follows:\n- For Long Short-Term Memory (LSTM), use the expected forget-gate value $r = \\mathbb{E}[f_t]$.\n- For Gated Recurrent Unit (GRU), use the expected update-gate value $r = \\mathbb{E}[z_t]$.\n- For Recurrent Neural Network (RNN), in the scalar linearization $h_t = \\alpha h_{t-1} + \\epsilon_t$, use $r = \\alpha$.\n\nIn the copy/repeat task, the model reads an initial binary pattern of length $L$ to be remembered, waits for a gap of $G$ timesteps with no informative input, and then is required to output the pattern repeated $R$ times. Define success to mean that the absolute magnitude of the memory variable at the decoding time of the last symbol of the last repetition exceeds a fixed threshold $\\tau > 0$. Under this definition, the decoding time offset $D$ relative to the end of the pattern read-in is\n$$\nD = G + L \\cdot R - 1,\n$$\nso that the last symbol’s decoding uses $m_D$.\n\nStarting only from the above recursion and standard properties of the Gaussian distribution, derive an analytic expression for the probability that the task succeeds as a function of the parameters $(r, D, m_0, \\sigma, \\tau)$, and then implement a program that computes this success probability for each of the following test cases. In all cases, use the architecture-specific mapping for $r$ described above, treat $r$ as time-invariant and equal to the provided average gate value, and assume independent Gaussian noise $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ at each timestep. If $r=1$, use the scientifically consistent limit of the recursion’s variance. No physical units apply, and angles are not involved.\n\nTest suite (each item is $(\\text{architecture}, \\text{gate average}, G, L, R, m_0, \\sigma, \\tau)$):\n- $(\\text{LSTM}, 0.95, 20, 5, 2, 1.0, 0.05, 0.5)$\n- $(\\text{GRU}, 0.90, 20, 5, 2, 1.0, 0.05, 0.5)$\n- $(\\text{RNN}, 0.85, 20, 5, 2, 1.0, 0.05, 0.5)$\n- $(\\text{LSTM}, 0.999, 200, 3, 3, 1.0, 0.02, 0.5)$\n- $(\\text{GRU}, 0.50, 10, 4, 1, 1.0, 0.10, 0.5)$\n- $(\\text{LSTM}, 1.00, 50, 2, 5, 1.0, 0.05, 0.5)$\n- $(\\text{GRU}, 0.95, 30, 5, 1, 1.0, 0.50, 0.5)$\n\nYour program should produce a single line of output containing the success probabilities for these seven test cases, in the same order, as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\ldots,\\text{result}_7]$). Each result must be a real number in standard decimal notation (a Python float).", "solution": "The problem requires the derivation and computation of the success probability for a delayed copy task, modeled by a scalar, linearized recurrent memory channel. The success of the task is defined by the memory variable $m_t \\in \\mathbb{R}$ at a specific future time step.\n\nThe evolution of the memory variable is described by the first-order autoregressive process:\n$$\nm_t = r \\, m_{t-1} + \\epsilon_t\n$$\nwhere $m_0$ is the initial memory state, $r \\in (0,1]$ is the retention coefficient, and $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ are i.i.d. Gaussian noise terms. Success is achieved if at the final decoding time step $D$, the condition $|m_D| > \\tau$ is met, for a given threshold $\\tau > 0$. The decoding time offset is given by $D = G + L \\cdot R - 1$.\n\nOur objective is to find an analytic expression for the success probability, $P(|m_D| > \\tau)$. This requires determining the probability distribution of the random variable $m_D$. We begin by unrolling the recursion to express $m_D$ in terms of the initial state $m_0$ and the sequence of noise perturbations $\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_D$.\n\nStarting from $m_1 = r m_0 + \\epsilon_1$, we can write:\n$m_2 = r m_1 + \\epsilon_2 = r(r m_0 + \\epsilon_1) + \\epsilon_2 = r^2 m_0 + r \\epsilon_1 + \\epsilon_2$.\nBy induction, the expression for $m_t$ at any time step $t>0$ is:\n$$\nm_t = r^t m_0 + \\sum_{i=1}^{t} r^{t-i} \\epsilon_i\n$$\nFor the final time step $D$, we have:\n$$\nm_D = r^D m_0 + \\sum_{i=1}^{D} r^{D-i} \\epsilon_i\n$$\nThe term $r^D m_0$ is a constant. The summation term is a linear combination of $D$ independent Gaussian random variables $\\epsilon_i$. A fundamental property of the Gaussian distribution is that any linear combination of Gaussian random variables is also a Gaussian random variable. Therefore, $m_D$ follows a Gaussian distribution, $m_D \\sim \\mathcal{N}(\\mu_D, \\sigma_D^2)$. We must now find its mean $\\mu_D$ and variance $\\sigma_D^2$.\n\nThe mean $\\mu_D$ is the expectation of $m_D$:\n$$\n\\mu_D = \\mathbb{E}[m_D] = \\mathbb{E}\\left[r^D m_0 + \\sum_{i=1}^{D} r^{D-i} \\epsilon_i\\right]\n$$\nBy linearity of expectation, and since $\\mathbb{E}[\\epsilon_i] = 0$ for all $i$:\n$$\n\\mu_D = r^D m_0 + \\sum_{i=1}^{D} r^{D-i} \\mathbb{E}[\\epsilon_i] = r^D m_0 + 0\n$$\n$$\n\\mu_D = r^D m_0\n$$\nThe variance $\\sigma_D^2$ is the variance of $m_D$. The constant term $r^D m_0$ does not affect the variance. As the noise terms $\\epsilon_i$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\sigma_D^2 = \\text{Var}(m_D) = \\text{Var}\\left(\\sum_{i=1}^{D} r^{D-i} \\epsilon_i\\right) = \\sum_{i=1}^{D} \\text{Var}(r^{D-i} \\epsilon_i)\n$$\nUsing the property $\\text{Var}(aX) = a^2\\text{Var}(X)$ and that $\\text{Var}(\\epsilon_i) = \\sigma^2$:\n$$\n\\sigma_D^2 = \\sum_{i=1}^{D} (r^{D-i})^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^{D} (r^2)^{D-i}\n$$\nThis is a geometric series. By substituting the index $j = D-i$, the sum runs from $j=0$ to $j=D-1$:\n$$\n\\sigma_D^2 = \\sigma^2 \\sum_{j=0}^{D-1} (r^2)^j\n$$\nWe evaluate this sum based on the value of $r$:\nCase 1: $r < 1$. The sum of the finite geometric series is $\\frac{1 - (r^2)^D}{1 - r^2}$.\n$$\n\\sigma_D^2 = \\sigma^2 \\frac{1 - r^{2D}}{1 - r^2}\n$$\nCase 2: $r = 1$. The ratio of the geometric series is $1$. The sum is simply the number of terms, which is $D$.\n$$\n\\sigma_D^2 = \\sigma^2 D\n$$\nThis result for $r=1$ is also obtainable by taking the limit of the expression for $r<1$ as $r \\to 1$ via L'Hôpital's rule, ensuring consistency as required.\n\nWith the distribution of $m_D$ fully characterized as $m_D \\sim \\mathcal{N}(r^D m_0, \\sigma_D^2)$, we can compute the success probability $P(|m_D| > \\tau)$. This is equivalent to $P(m_D > \\tau \\text{ or } m_D < -\\tau)$. Since these are mutually exclusive events:\n$$\nP(\\text{success}) = P(m_D > \\tau) + P(m_D < -\\tau)\n$$\nTo evaluate these probabilities, we standardize the variable $m_D$. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal variable, and let $\\Phi(z) = P(Z \\le z)$ be its cumulative distribution function (CDF). The standard deviation is $\\sigma_D = \\sqrt{\\sigma_D^2}$.\n$$\nP(m_D < -\\tau) = P\\left(\\frac{m_D - \\mu_D}{\\sigma_D} < \\frac{-\\tau - \\mu_D}{\\sigma_D}\\right) = \\Phi\\left(\\frac{-\\tau - \\mu_D}{\\sigma_D}\\right)\n$$\n$$\nP(m_D > \\tau) = 1 - P(m_D \\le \\tau) = 1 - P\\left(\\frac{m_D - \\mu_D}{\\sigma_D} \\le \\frac{\\tau - \\mu_D}{\\sigma_D}\\right) = 1 - \\Phi\\left(\\frac{\\tau - \\mu_D}{\\sigma_D}\\right)\n$$\nUsing the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, we can rewrite the second term:\n$$\n1 - \\Phi\\left(\\frac{\\tau - \\mu_D}{\\sigma_D}\\right) = \\Phi\\left(-\\left(\\frac{\\tau - \\mu_D}{\\sigma_D}\\right)\\right) = \\Phi\\left(\\frac{\\mu_D - \\tau}{\\sigma_D}\\right)\n$$\nCombining the terms, the final expression for the success probability is:\n$$\nP(\\text{success}) = \\Phi\\left(\\frac{\\mu_D - \\tau}{\\sigma_D}\\right) + \\Phi\\left(\\frac{-\\mu_D - \\tau}{\\sigma_D}\\right)\n$$\nThis expression will be implemented to compute the results for the given test cases. The values for $\\mu_D$ and $\\sigma_D^2$ are determined using the parameters from each test case, and the CDF $\\Phi(z)$ is computed numerically, for which the error function $\\text{erf}(x)$ can be used via the identity $\\Phi(z) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Calculates the success probability for a memory task in simplified recurrent models.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (architecture, gate_average, G, L, R, m0, sigma, tau)\n    test_cases = [\n        ('LSTM', 0.95, 20, 5, 2, 1.0, 0.05, 0.5),\n        ('GRU', 0.90, 20, 5, 2, 1.0, 0.05, 0.5),\n        ('RNN', 0.85, 20, 5, 2, 1.0, 0.05, 0.5),\n        ('LSTM', 0.999, 200, 3, 3, 1.0, 0.02, 0.5),\n        ('GRU', 0.50, 10, 4, 1, 1.0, 0.10, 0.5),\n        ('LSTM', 1.00, 50, 2, 5, 1.0, 0.05, 0.5),\n        ('GRU', 0.95, 30, 5, 1, 1.0, 0.50, 0.5)\n    ]\n\n    results = []\n\n    def standard_normal_cdf(z):\n        \"\"\"\n        Computes the standard normal CDF using the error function.\n        Phi(z) = 0.5 * (1 + erf(z / sqrt(2)))\n        \"\"\"\n        return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        architecture, r, G, L, R, m0, sigma, tau = case\n\n        # Calculate the decoding time offset D\n        D = G + L * R - 1\n\n        # Calculate the mean of the memory state at time D\n        mu_D = (r**D) * m0\n\n        # Calculate the variance of the memory state at time D\n        # Handle the special case where r = 1\n        if r == 1.0:\n            # For r=1, the variance is sigma^2 * D\n            sigma_D_sq = (sigma**2) * D\n        else:\n            # For r<1, use the geometric series sum formula\n            sigma_D_sq = (sigma**2) * (1.0 - r**(2.0 * D)) / (1.0 - r**2)\n\n        # Handle the deterministic case where sigma is zero\n        if sigma_D_sq <= 1e-12:  # Use a small tolerance for floating point\n            prob = 1.0 if np.abs(mu_D) > tau else 0.0\n        else:\n            # Calculate the standard deviation\n            sigma_D = np.sqrt(sigma_D_sq)\n\n            # Standardize the thresholds\n            z1 = (mu_D - tau) / sigma_D\n            z2 = (-mu_D - tau) / sigma_D\n            \n            # Calculate the success probability using the CDF\n            # P(success) = P(m_D > tau) + P(m_D < -tau)\n            # which is Phi((mu_D - tau) / sigma_D) + Phi((-mu_D - tau) / sigma_D)\n            prob = standard_normal_cdf(z1) + standard_normal_cdf(z2)\n        \n        results.append(prob)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3168420"}, {"introduction": "Beyond simple sequence modeling, can RNNs learn to execute algorithms? This final practice challenges you to train an RNN to perform multi-bit binary addition, a task that requires carrying information over potentially long dependencies. You will not only train the network but also analyze its fundamental limits by enforcing a constraint on its recurrent weights' spectral radius, a key factor governing memory. This exercise provides a fascinating link between the mathematical properties of a network and its capacity to learn structured, procedural tasks [@problem_id:3167589].", "problem": "You are asked to design, analyze, and implement a complete program that constructs and trains a Recurrent Neural Network (RNN) to emulate a simple algorithmic task (binary addition) and to diagnose principled failure cases arising from limited memory. The task must be framed in purely mathematical terms and solved using first principles of statistical learning and recurrent neural networks. The final output of your program must aggregate results from a fixed test suite into a single line in a specified format.\n\nThe fundamental base of this problem is the definition of a standard Elman Recurrent Neural Network and the causal structure of binary addition. Consider a Recurrent Neural Network (RNN) with hyperbolic tangent activation where the hidden state update is defined by the recurrence\n$$\n\\mathbf{h}_t = \\tanh\\!\\Big(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h\\Big),\n$$\nwith output logits\n$$\n\\mathbf{o}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y,\n$$\nand probabilistic outputs\n$$\n\\mathbf{y}_t = \\sigma(\\mathbf{o}_t),\n$$\nwhere $\\sigma(\\cdot)$ denotes the elementwise logistic sigmoid. The input at time $t$ is the pair of bits $\\mathbf{x}_t = (a_t, b_t)$ with $a_t \\in \\{0,1\\}$ and $b_t \\in \\{0,1\\}$, scanned from the least significant bit ($t = 0$) to the most significant bit ($t = T-1$). The network must output at each time step two bits: the sum bit $s_t$ and the carry-out bit $c_t$. The true targets $(s_t, c_t)$ are generated by the causal addition rule from first principles: initialize $c_{-1} = 0$ and for each time step $t$ compute\n$$\nu_t = a_t + b_t + c_{t-1}, \\quad s_t = u_t \\bmod 2, \\quad c_t = \\left\\lfloor \\frac{u_t}{2} \\right\\rfloor.\n$$\nAfter $T$ steps, the full sum is recovered as\n$$\nS = \\sum_{t=0}^{T-1} s_t 2^t + c_{T-1} \\cdot 2^T.\n$$\n\nYour program must do the following:\n\n- Implement and train the above RNN on randomly generated pairs of $T_{\\text{train}}$-bit integers to emulate the bitwise addition algorithm, with $T_{\\text{train}} = 8$. Use a binary cross-entropy loss summed over time and outputs. Use stochastic gradient descent with gradient clipping. Initialize $\\mathbf{h}_{-1} = \\mathbf{0}$. After every parameter update, enforce the spectral radius constraint\n$$\n\\rho\\big(\\mathbf{W}_{hh}\\big) \\le \\rho_{\\max},\n$$\nwith $\\rho_{\\max} = 0.7$, by rescaling $\\mathbf{W}_{hh}$ as needed. This constraint is mandatory and serves to induce a finite memory length.\n\n- From fundamental definitions, derive and implement a principled estimator for the effective memory length $L_{\\varepsilon}$ of the trained network, expressed as the smallest integer $L$ such that the influence of information $L$ steps in the past on the current hidden state is bounded by a tolerance $\\varepsilon$. Your derivation must begin with the RNN recurrence, express the hidden-to-hidden influence through Jacobian products, and use a norm or spectral-radius-based bound to obtain a computable estimator $L_{\\varepsilon}$ in terms of the learned parameters and average activation derivatives. In code, set $\\varepsilon = 0.05$.\n\n- Define the carry chain length for a specific input pair $(\\{a_t\\}, \\{b_t\\})$ as the maximum number of consecutive time steps during which the running carry bit equals $1$ when computed causally from least significant to most significant bit. That is, if $\\{c_t\\}$ is the carry sequence induced by the true addition rule above, the carry chain length is\n$$\n\\max_{i \\le j} \\Big\\{ (j-i+1) \\; \\text{such that} \\; c_i = c_{i+1} = \\cdots = c_j = 1 \\Big\\}.\n$$\n\n- Predict sufficiency versus failure for a given input pair by comparing the estimated memory length $L_{\\varepsilon}$ with the input’s carry chain length. If $L_{\\varepsilon}$ is greater than or equal to the carry chain length, predict “sufficient”; otherwise, predict “insufficient.” Then empirically evaluate the trained RNN on that input pair by computing the entire predicted sum from the per-time-step outputs and comparing it to the true integer sum.\n\n- Test Suite. Use a fixed sequence length of $T = 12$ bits for all test inputs and evaluate the trained network on the following five input pairs of nonnegative integers, each represented in $12$ bits (least significant bit first in the computational procedure):\n    - Case $1$ (happy path): $(A,B) = (25, 6)$.\n    - Case $2$ (boundary proximity): $(A,B) = (255, 1)$.\n    - Case $3$ (beyond boundary): $(A,B) = (1023, 1)$.\n    - Case $4$ (edge case with zero): $(A,B) = (0, 0)$.\n    - Case $5$ (extreme carry propagation): $(A,B) = (4095, 1)$.\n  These inputs are selected to exercise typical behavior, boundary conditions, and significant edge cases regarding carry propagation.\n\n- Final Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order, output two booleans:\n    1. The first boolean indicates whether the estimated memory length $L_{\\varepsilon}$ is sufficient for the maximum carry chain length of that case.\n    2. The second boolean indicates whether the trained RNN produces the exact correct integer sum for that case.\n  The final line must therefore contain $10$ boolean values in total, in the form\n$$\n[\\text{p}_1,\\text{a}_1,\\text{p}_2,\\text{a}_2,\\text{p}_3,\\text{a}_3,\\text{p}_4,\\text{a}_4,\\text{p}_5,\\text{a}_5],\n$$\nwhere $\\text{p}_i$ is the predicted sufficiency and $\\text{a}_i$ is the actual correctness for case $i$.\n\nAll mathematical quantities and numbers in this specification must be interpreted in their standard mathematical sense and, where applicable, answers must be expressed as booleans and evaluated exactly as defined. No physical units or angle units are involved in this problem.", "solution": "The problem requires the design, implementation, and analysis of a Recurrent Neural Network (RNN) to perform binary addition. A critical aspect of the problem is to theoretically and empirically investigate the network's memory limitations, which are intentionally induced by a spectral radius constraint on the recurrent weight matrix. The solution proceeds in three main stages: first, defining the model and training procedure; second, deriving a principled estimator for the network's effective memory length; and third, implementing the full system to test the hypothesis that network failure on long-carry additions is predictable by comparing this memory length to the task's required carry propagation length.\n\n### 1. RNN Model and Binary Addition Task\n\nThe RNN architecture is a standard Elman network. The hidden state $\\mathbf{h}_t \\in \\mathbb{R}^{d_h}$ evolves according to the recurrence:\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h)\n$$\nwhere $\\mathbf{x}_t \\in \\{0,1\\}^2$ is the input vector representing two bits $(a_t, b_t)$, $\\mathbf{W}_{hh} \\in \\mathbb{R}^{d_h \\times d_h}$, $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d_h \\times 2}$, and $\\mathbf{b}_h \\in \\mathbb{R}^{d_h}$ are parameters. The initial hidden state is $\\mathbf{h}_{-1} = \\mathbf{0}$.\n\nAt each time step $t$, the network produces output logits $\\mathbf{o}_t \\in \\mathbb{R}^2$ and probabilistic outputs $\\mathbf{y}_t \\in (0,1)^2$:\n$$\n\\mathbf{o}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y\n$$\n$$\n\\mathbf{y}_t = \\sigma(\\mathbf{o}_t)\n$$\nwhere $\\mathbf{W}_{hy} \\in \\mathbb{R}^{2 \\times d_h}$, $\\mathbf{b}_y \\in \\mathbb{R}^2$, and $\\sigma(\\cdot)$ is the element-wise logistic sigmoid function. The two components of $\\mathbf{y}_t$ are the network's probabilistic predictions for the sum bit $s_t$ and the carry-out bit $c_t$.\n\nThe ground truth for training is generated by the rules of binary addition. Given input bits $(a_t, b_t)$ and the carry-in $c_{t-1}$ (with $c_{-1}=0$), the true sum bit $s_t$ and carry-out bit $c_t$ are:\n$$\nu_t = a_t + b_t + c_{t-1}\n$$\n$$\ns_t = u_t \\pmod 2\n$$\n$$\nc_t = \\left\\lfloor \\frac{u_t}{2} \\right\\rfloor\n$$\n\n### 2. Training Procedure\n\nThe network is trained to minimize the total binary cross-entropy (BCE) loss between its predictions $\\mathbf{y}_t = (y_{t,s}, y_{t,c})$ and the true targets $(s_t, c_t)$ over a sequence of length $T$:\n$$\nL = -\\sum_{t=0}^{T-1} \\left[ s_t \\log(y_{t,s}) + (1-s_t) \\log(1-y_{t,s}) + c_t \\log(y_{t,c}) + (1-c_t) \\log(1-y_{t,c}) \\right]\n$$\nParameters are updated using Stochastic Gradient Descent (SGD) with gradients computed via Backpropagation Through Time (BPTT). To prevent exploding gradients, all gradients are clipped to a maximum norm. A key feature of the training is the enforcement of a spectral radius constraint on the recurrent weight matrix, $\\rho(\\mathbf{W}_{hh}) \\le \\rho_{\\max}$, where $\\rho(\\cdot)$ denotes the spectral radius (maximum absolute eigenvalue). This is achieved by rescaling $\\mathbf{W}_{hh}$ after each gradient update if the constraint is violated:\n$$\n\\text{if } \\rho(\\mathbf{W}_{hh}) > \\rho_{\\max}, \\quad \\mathbf{W}_{hh} \\leftarrow \\mathbf{W}_{hh} \\frac{\\rho_{\\max}}{\\rho(\\mathbf{W}_{hh})}\n$$\nThis constraint limits the long-term memory capacity of the RNN, as it forces the linear part of the recurrent dynamics to be contractive. For this problem, we use $\\rho_{\\max} = 0.7$ and train on $T_{\\text{train}}=8$ bit numbers. Hyperparameters such as hidden dimension, learning rate, and training epochs must be chosen appropriately; we select $d_h=4$, a learning rate of $\\eta=0.05$, and $3000$ training epochs.\n\n### 3. Derivation of Effective Memory Length $L_{\\varepsilon}$\n\nThe effective memory length, $L_{\\varepsilon}$, is the number of time steps over which information can propagate before its influence decays below a threshold $\\varepsilon$. We can formalize this by analyzing the Jacobian of the hidden state at time $t$ with respect to the hidden state at a past time $t-L$, $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-L}}$.\n\nUsing the chain rule on the recurrence relation:\n$$\n\\frac{\\partial \\mathbf{h}_k}{\\partial \\mathbf{h}_{k-1}} = \\frac{\\partial \\tanh(\\mathbf{z}_k)}{\\partial \\mathbf{z}_k} \\frac{\\partial \\mathbf{z}_k}{\\partial \\mathbf{h}_{k-1}} = \\text{diag}(1-\\tanh^2(\\mathbf{z}_k)) \\cdot \\mathbf{W}_{hh} = \\mathbf{D}_k \\mathbf{W}_{hh}\n$$\nwhere $\\mathbf{z}_k = \\mathbf{W}_{hh} \\mathbf{h}_{k-1} + \\dots$ is the pre-activation at time $k$, and $\\mathbf{D}_k$ is a diagonal matrix of activation derivatives. The full Jacobian is a product of these terms:\n$$\n\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-L}} = \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} \\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{h}_{t-2}} \\cdots \\frac{\\partial \\mathbf{h}_{t-L+1}}{\\partial \\mathbf{h}_{t-L}} = \\prod_{k=t-L+1}^{t} (\\mathbf{D}_k \\mathbf{W}_{hh})\n$$\nThe norm of this Jacobian bounds the magnitude of the influence. To obtain a tractable estimator, we make two approximations. First, we replace the time-varying matrix $\\mathbf{D}_k$ with a scalar-matrix approximation $\\bar{d} \\mathbf{I}$, where $\\bar{d}$ is the average value of the derivative $1-\\tanh^2(z)$ across all hidden units and time steps, estimated from a sample of training data. Second, we use the property that for large $L$, the norm of a matrix power $\\|(\\bar{d}\\mathbf{W}_{hh})^L\\|$ is dominated by its spectral radius, $\\rho(\\bar{d}\\mathbf{W}_{hh})^L$.\nThe influence decay factor over $L$ steps is thus approximated by $(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))^L$. We seek the smallest integer $L$ for which this value is less than or equal to $\\varepsilon$:\n$$\n(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))^L \\le \\varepsilon\n$$\nSolving for $L$ by taking the logarithm of both sides yields:\n$$\nL \\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh})) \\le \\log(\\varepsilon)\n$$\nSince $\\rho(\\mathbf{W}_{hh}) \\le \\rho_{\\max} = 0.7 < 1$ and $\\bar{d} \\le 1$, the term $\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh})$ is less than $1$, making its logarithm negative. Dividing by this negative number reverses the inequality:\n$$\nL \\ge \\frac{\\log(\\varepsilon)}{\\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))}\n$$\nThe an estimator for the effective memory length $L_\\varepsilon$ is the smallest integer satisfying this:\n$$\nL_{\\varepsilon} = \\left\\lceil \\frac{\\log(\\varepsilon)}{\\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))} \\right\\rceil\n$$\nFor the implementation, we use $\\varepsilon = 0.05$. A larger $L_\\varepsilon$ implies the network can maintain information for longer durations.\n\n### 4. Failure Prediction and Evaluation\n\nThe binary addition task's memory requirement is determined by its longest carry chain. The \"carry chain length\" for an input pair is the maximum number of consecutive time steps for which the true carry-out bit $c_t$ is $1$. For the network to correctly perform the addition, its effective memory length $L_{\\varepsilon}$ must be at least as long as the required carry chain length, $L_{\\text{carry}}$. This gives us a prediction rule:\n- Predict \"sufficient\" if $L_{\\varepsilon} \\ge L_{\\text{carry}}$.\n- Predict \"insufficient\" if $L_{\\varepsilon} < L_{\\text{carry}}$.\n\nThis prediction is then compared against the actual performance of the trained RNN on the test cases. The empirical correctness is determined by feeding the $T=12$ bit representations of the test integers into the network, binarizing its outputs $(y_{t,s}, y_{t,c})$ to get predicted bits $(s'_t, c'_t)$, calculating the predicted integer sum $S_{\\text{pred}}$, and comparing it to the true sum $S_{\\text{true}}$.\n$$\nS_{\\text{pred}} = \\sum_{t=0}^{T-1} s'_t 2^t + c'_{T-1} 2^T\n$$\nThe network's performance is deemed correct if and only if $S_{\\text{pred}} = S_{\\text{true}}$. The final output aggregates these comparisons for the given test suite.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\nimport math\n\n# Use a fixed random seed for reproducibility of training.\nnp.random.seed(42)\n\ndef solve():\n    \"\"\"\n    Main function to construct, train, and evaluate the RNN for binary addition.\n    \"\"\"\n\n    # --- Configuration ---\n    # RNN architecture\n    INPUT_DIM = 2\n    HIDDEN_DIM = 4  # A small hidden size is sufficient for this task.\n    OUTPUT_DIM = 2\n\n    # Training parameters\n    LEARNING_RATE = 0.05\n    N_EPOCHS = 3000\n    GRAD_CLIP_THRESHOLD = 1.0\n    T_TRAIN = 8  # Train on 8-bit numbers.\n\n    # Memory analysis parameters\n    RHO_MAX = 0.7\n    EPSILON = 0.05\n\n    # Testing parameters\n    T_TEST = 12 # Test on 12-bit numbers.\n\n    # --- Helper Functions ---\n    def int_to_binary_array(n, num_bits):\n        binary_str = format(n, f'0{num_bits}b')\n        return np.array([int(bit) for bit in reversed(binary_str)], dtype=np.float64)\n\n    def get_true_targets(a_bits, b_bits):\n        seq_len = len(a_bits)\n        s_bits = np.zeros(seq_len, dtype=np.float64)\n        c_bits = np.zeros(seq_len, dtype=np.float64)\n        carry_in = 0\n        for t in range(seq_len):\n            u_t = a_bits[t] + b_bits[t] + carry_in\n            s_bits[t] = u_t % 2\n            c_bits[t] = u_t // 2\n            carry_in = c_bits[t]\n        return s_bits, c_bits\n\n    class RNN:\n        \"\"\"A simple Elman Recurrent Neural Network.\"\"\"\n        def __init__(self, input_dim, hidden_dim, output_dim):\n            # Xavier/Glorot initialization for weights\n            limit_xh = np.sqrt(6.0 / (input_dim + hidden_dim))\n            limit_hh = np.sqrt(6.0 / (hidden_dim + hidden_dim))\n            limit_hy = np.sqrt(6.0 / (hidden_dim + output_dim))\n            \n            self.W_xh = np.random.uniform(-limit_xh, limit_xh, (hidden_dim, input_dim))\n            self.W_hh = np.random.uniform(-limit_hh, limit_hh, (hidden_dim, hidden_dim))\n            self.W_hy = np.random.uniform(-limit_hy, limit_hy, (output_dim, hidden_dim))\n            \n            self.b_h = np.zeros((hidden_dim, 1))\n            self.b_y = np.zeros((output_dim, 1))\n\n        def _sigmoid(self, x):\n            return 1.0 / (1.0 + np.exp(-x))\n\n        def forward(self, x_seq):\n            seq_len = x_seq.shape[0]\n            h = np.zeros((self.W_hh.shape[0], 1))\n            \n            history = {\n                'h': { -1: h }, # Store h_t values, key is time index\n                'y': [],\n                'deriv_activations': []\n            }\n            \n            for t in range(seq_len):\n                x_t = x_seq[t].reshape(-1, 1)\n                h_pre_act = self.W_xh @ x_t + self.W_hh @ h + self.b_h\n                h = np.tanh(h_pre_act)\n                o_t = self.W_hy @ h + self.b_y\n                y_t = self._sigmoid(o_t)\n                \n                history['h'][t] = h\n                history['y'].append(y_t)\n                history['deriv_activations'].append(1 - h**2)\n                \n            return history\n\n        def rescale_W_hh(self, rho_max):\n            eigenvalues = scipy.linalg.eigvals(self.W_hh)\n            rho = np.max(np.abs(eigenvalues))\n            if rho > rho_max:\n                self.W_hh *= rho_max / rho\n\n    def train_rnn(model, num_epochs, seq_len, learning_rate, clip_threshold, rho_max):\n        \"\"\"Train the RNN model using SGD and BPTT.\"\"\"\n        for epoch in range(num_epochs):\n            # Generate a random training sample\n            a_int = np.random.randint(0, 2**seq_len)\n            b_int = np.random.randint(0, 2**seq_len)\n            a_bits = int_to_binary_array(a_int, seq_len)\n            b_bits = int_to_binary_array(b_int, seq_len)\n            x_seq = np.vstack((a_bits, b_bits)).T\n            s_true, c_true = get_true_targets(a_bits, b_bits)\n            y_true_seq = np.vstack((s_true, c_true)).T\n\n            # Forward pass\n            history = model.forward(x_seq)\n            \n            # --- Backward Pass (BPTT) ---\n            dW_xh, dW_hh, dW_hy = np.zeros_like(model.W_xh), np.zeros_like(model.W_hh), np.zeros_like(model.W_hy)\n            db_h, db_y = np.zeros_like(model.b_h), np.zeros_like(model.b_y)\n            \n            delta_h_future = np.zeros_like(model.b_h)\n            \n            for t in reversed(range(seq_len)):\n                y_pred_t = history['y'][t]\n                y_true_t = y_true_seq[t].reshape(-1, 1)\n                h_t = history['h'][t]\n                h_prev = history['h'][t - 1]\n                x_t = x_seq[t].reshape(-1, 1)\n                deriv_act_t = history['deriv_activations'][t]\n\n                delta_o_t = y_pred_t - y_true_t\n                dW_hy += delta_o_t @ h_t.T\n                db_y += delta_o_t\n                \n                delta_h_from_output = model.W_hy.T @ delta_o_t\n                delta_h_total = delta_h_from_output + delta_h_future\n                \n                delta_pre_act_h = delta_h_total * deriv_act_t\n                \n                dW_hh += delta_pre_act_h @ h_prev.T\n                dW_xh += delta_pre_act_h @ x_t.T\n                db_h += delta_pre_act_h\n                \n                delta_h_future = model.W_hh.T @ delta_pre_act_h\n            \n            # Gradient clipping\n            grads = [dW_xh, dW_hh, dW_hy, db_h, db_y]\n            total_norm = np.sqrt(sum(np.sum(g**2) for g in grads))\n            if total_norm > clip_threshold:\n                for g in grads:\n                    g *= clip_threshold / total_norm\n\n            # SGD update\n            model.W_xh -= learning_rate * dW_xh\n            model.W_hh -= learning_rate * dW_hh\n            model.W_hy -= learning_rate * dW_hy\n            model.b_h -= learning_rate * db_h\n            model.b_y -= learning_rate * db_y\n            \n            # Enforce spectral radius constraint\n            model.rescale_W_hh(rho_max)\n        \n        return model\n\n    def calculate_memory_length(model, rho_max, epsilon):\n        # Estimate average activation derivative\n        num_samples = 100\n        all_deriv_activations = []\n        for _ in range(num_samples):\n            a_int = np.random.randint(0, 2**T_TRAIN)\n            b_int = np.random.randint(0, 2**T_TRAIN)\n            a_bits = int_to_binary_array(a_int, T_TRAIN)\n            b_bits = int_to_binary_array(b_int, T_TRAIN)\n            x_seq = np.vstack((a_bits, b_bits)).T\n            history = model.forward(x_seq)\n            all_deriv_activations.extend([d.flatten() for d in history['deriv_activations']])\n        \n        d_avg_scalar = np.mean(np.concatenate(all_deriv_activations))\n        \n        rho_W_hh = np.max(np.abs(scipy.linalg.eigvals(model.W_hh)))\n        \n        # Calculate L_epsilon\n        log_numerator = np.log(epsilon)\n        decay_factor = d_avg_scalar * rho_W_hh\n        \n        if decay_factor >= 1.0:\n            return float('inf')\n        if decay_factor <= 0:\n            return 1\n            \n        log_denominator = np.log(decay_factor)\n        L_eps = math.ceil(log_numerator / log_denominator)\n        return L_eps\n\n    def calculate_carry_chain_length(A, B, num_bits):\n        a_bits = int_to_binary_array(A, num_bits)\n        b_bits = int_to_binary_array(B, num_bits)\n        _, c_bits = get_true_targets(a_bits, b_bits)\n        \n        max_len = 0\n        current_len = 0\n        for bit in c_bits:\n            if bit == 1:\n                current_len += 1\n            else:\n                max_len = max(max_len, current_len)\n                current_len = 0\n        max_len = max(max_len, current_len)\n        return max_len\n\n    # --- Main Execution ---\n    rnn = RNN(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n    \n    # Train the RNN\n    trained_rnn = train_rnn(rnn, N_EPOCHS, T_TRAIN, LEARNING_RATE, GRAD_CLIP_THRESHOLD, RHO_MAX)\n    \n    # Analyze the trained network's memory\n    L_epsilon = calculate_memory_length(trained_rnn, RHO_MAX, EPSILON)\n    \n    # Test Suite\n    test_cases = [\n        (25, 6),\n        (255, 1),\n        (1023, 1),\n        (0, 0),\n        (4095, 1),\n    ]\n\n    results = []\n    for A, B in test_cases:\n        # 1. Predict sufficiency based on memory length\n        L_carry = calculate_carry_chain_length(A, B, T_TEST)\n        p_sufficient = (L_epsilon >= L_carry)\n        \n        # 2. Evaluate actual performance\n        a_bits = int_to_binary_array(A, T_TEST)\n        b_bits = int_to_binary_array(B, T_TEST)\n        x_seq = np.vstack((a_bits, b_bits)).T\n        \n        history = trained_rnn.forward(x_seq)\n        \n        s_pred_bits = np.round([y[0,0] for y in history['y']])\n        c_pred_bits = np.round([y[1,0] for y in history['y']])\n        \n        S_pred = 0\n        for t in range(T_TEST):\n            S_pred += s_pred_bits[t] * (2**t)\n        # Add final carry-out\n        S_pred += c_pred_bits[T_TEST - 1] * (2**T_TEST)\n        \n        S_true = A + B\n        a_correct = (S_pred == S_true)\n        \n        results.extend([p_sufficient, a_correct])\n        \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167589"}]}