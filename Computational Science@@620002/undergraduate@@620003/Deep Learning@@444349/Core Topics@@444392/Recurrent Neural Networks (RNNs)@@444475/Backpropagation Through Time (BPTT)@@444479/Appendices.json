{"hands_on_practices": [{"introduction": "Why don't we just write down the final mathematical equation for the gradient and compute it directly? This exercise answers that very question by guiding you through the process for a simple linear RNN. By deriving the closed-form expression for the gradient, you will uncover its underlying structure and, more importantly, discover why its direct computation is prohibitively expensive for long sequences, thereby motivating the need for an efficient recursive algorithm like Backpropagation Through Time. [@problem_id:3192146]", "problem": "Consider a linear Recurrent Neural Network (RNN) with hidden state dimension $d_h$, input dimension $d_x$, and output dimension $d_y$, driven by a single sequence of length $T$. The recurrence and readout are given by the core definitions\n$h_t = W_h h_{t-1} + W_x x_t$ with $h_0 = 0$, and $y_t = W_y h_t$,\nfor $t \\in \\{1,2,\\dots,T\\}$, where $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The input vectors are $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$ and the targets are $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$. Consider the quadratic loss\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$.\nLet the output residuals be $e_t = y_t - s_t$ for each $t$. Using only the recurrence definition, the readout definition, the chain rule from multivariable calculus, and standard linear algebraic facts, derive a closed-form analytic expression for the gradient $\\nabla_{W_h} L$ that makes explicit its decomposition as a sum of time-shifted correlation terms of the form $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$. Then, briefly discuss the computational complexity (in Bigâ€“$O$ notation) of evaluating this gradient when implemented (i) directly from your closed-form expression by summing the time-shifted terms, and (ii) via Backpropagation Through Time (BPTT) using the first-order recursion for the adjoint signals. Your final answer must be a single closed-form analytic matrix expression for $\\nabla_{W_h} L$ written in terms of $W_h$, $W_y$, $\\{e_t\\}$, and $\\{h_t\\}$ only. Do not include any numerical evaluation.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem statement from the field of deep learning theory. It requires the derivation of a gradient for a standard linear Recurrent Neural Network (RNN), which is a formal and solvable task based on established principles of multivariable calculus and linear algebra. All necessary definitions and conditions are provided.\n\nWe begin by stating the core definitions provided in the problem:\nRecurrence relation: $h_t = W_h h_{t-1} + W_x x_t$ for $t \\in \\{1, 2, \\dots, T\\}$, with initial state $h_0 = 0$.\nReadout equation: $y_t = W_y h_t$.\nLoss function: $L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$, where $e_t = y_t - s_t$.\nThe parameters are matrices $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The inputs are vectors $x_t \\in \\mathbb{R}^{d_x}$ and targets $s_t \\in \\mathbb{R}^{d_y}$.\n\nOur goal is to derive a closed-form expression for the gradient of the loss function $L$ with respect to the recurrent weight matrix $W_h$, denoted as $\\nabla_{W_h} L$. We will use the chain rule for matrix calculus.\n\nThe loss $L$ is a function of $W_h$ through its influence on the hidden states $\\{h_t\\}_{t=1}^T$. The gradient can be expressed as a sum of contributions from each timestep where $W_h$ is used in the forward pass:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\nwhere the \"local\" gradient refers to the gradient with respect to the instance of $W_h$ used in the computation $h_t = W_h h_{t-1} + W_x x_t$. Using the chain rule, this contribution is given by the outer product of the gradient of the loss with respect to the state $h_t$, and the gradient of the state-update term with respect to $W_h$. Let $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ be the gradient of the total loss with respect to the hidden state $h_t$. The gradient of the pre-activation $a_t = W_h h_{t-1} + W_x x_t$ with respect to $W_h$ is $h_{t-1}^T$. Thus, the contribution to the gradient from timestep $t$ is $\\delta_t h_{t-1}^T$. Summing over all timesteps gives the total gradient:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\nThis is the standard formula for the recurrent weight gradient in BPTT. However, the problem requires a closed-form expression in terms of the given quantities $W_h, W_y, \\{e_t\\}, \\{h_t\\}$. To achieve this, we must derive a closed form for $\\delta_t$.\n\nThe vector $\\delta_t$ represents the total influence of $h_t$ on the loss $L$. The state $h_t$ influences the loss through the outputs $y_k$ for all subsequent timesteps $k \\ge t$.\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\nHere, we apply the chain rule, summing over all paths from $h_t$ to the final loss. The partial derivatives are (using numerator-layout matrix calculus conventions where the gradient of a scalar w.r.t. a column vector is a column vector):\n$1.$ $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$. This is a column vector of size $d_y$.\n$2.$ $\\frac{\\partial y_k}{\\partial h_k} = W_y$. This is the Jacobian matrix of size $d_y \\times d_h$. The term in the chain rule is its transpose, $(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$.\n$3.$ $\\frac{\\partial h_k}{\\partial h_t}$. We find this by unrolling the recurrence relation:\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\nThe dependence of $h_k$ on $h_t$ for $k  t$ is given by:\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$.\nThe Jacobian matrix is therefore $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$ for $k \\ge t$. The term in the chain rule composition is its transpose, $(W_h^{k-t})^T = (W_h^T)^{k-t}$.\n\nSubstituting these derivatives back into the expression for $\\delta_t$:\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\nThis provides the closed-form expression for the gradient signal $\\delta_t$ propagated back from all future timesteps $k \\ge t$.\n\nFinally, we substitute this expression for $\\delta_t$ into our equation for the total gradient $\\nabla_{W_h} L$:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\nThis is the required closed-form analytic expression. It decomposes the gradient into a double summation over time. Each term $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ can be interpreted as a \"correlation\" between the error $e_k$ at a future time $k$ and the hidden state $h_{t-1}$ at a past time $t-1$, mediated by the matrix $(W_h^T)^{k-t} W_y^T$ which accounts for the propagation of influence through the network's dynamics and readout layer.\n\nNext, we discuss the computational complexity.\n(i) Direct evaluation from the closed-form expression:\nThe formula is $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$.\nAssume the forward pass has been run, so all $e_t \\in \\mathbb{R}^{d_y}$ and $h_t \\in \\mathbb{R}^{d_h}$ are known.\nThe double loop involves $O(T^2)$ terms. For each term $(t, k)$, the computation involves:\n- A matrix power $(W_h^T)^{k-t}$, which, if not pre-computed, takes $O((k-t) d_h^3)$. A pre-computation of all necessary powers of $W_h^T$ up to $T-1$ takes $O(T d_h^3)$.\n- A matrix-vector product $W_y^T e_k$, costing $O(d_h d_y)$.\n- A matrix-vector product $(W_h^T)^{k-t} (W_y^T e_k)$, costing $O(d_h^2)$.\n- An outer product with $h_{t-1}^T$, costing $O(d_h^2)$.\nA naive implementation would be very expensive. A more optimized direct evaluation would proceed as:\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\nThe most expensive part is the double loop structure. There are approximately $T^2/2$ pairs of $(t,k)$. For each pair, the dominant cost is computing the matrix-vector product with the matrix power, which is $O(d_h^2)$ assuming powers are precomputed. The precomputation costs $O(T d_h^3)$. Therefore, the total complexity is dominated by the nested loops and the precomputation, leading to a complexity of $O(T d_h^3 + T^2 d_h^2)$. The quadratic dependence on the sequence length $T$ makes this method computationally prohibitive for long sequences.\n\n(ii) Evaluation via Backpropagation Through Time (BPTT):\nBPTT avoids the explicit calculation of the double summation by using dynamic programming. It computes the gradients via a single backward pass through time. The key is the recursive relationship for $\\delta_t$:\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$ for $t = T-1, \\dots, 1$.\nThe BPTT algorithm for computing $\\nabla_{W_h} L$ is:\n$1.$ Perform a forward pass from $t=1$ to $T$ to compute and store all $h_t$ and $e_t$. Cost: $O(T(d_h^2 + d_h d_x + d_y d_h))$.\n$2.$ Initialize $\\nabla_{W_h} L = 0$ and $\\delta_{T+1} = 0$.\n$3.$ Perform a backward pass from $t=T$ down to $1$:\n   a. Compute $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$. This involves two matrix-vector products and a vector addition, costing $O(d_h d_y + d_h^2)$.\n   b. Add the contribution to the gradient: $\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$. This is an outer product and matrix addition, costing $O(d_h^2)$.\nThe backward pass consists of a single loop of length $T$, with each step costing $O(d_h^2 + d_h d_y)$. The total complexity of the backward pass is $O(T(d_h^2 + d_h d_y))$.\nThe total complexity of BPTT is therefore $O(T(d_h^2 + d_h d_x + d_y d_h))$, which is linear in the sequence length $T$. This is significantly more efficient than the direct evaluation of the closed-form expression.", "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$", "id": "3192146"}, {"introduction": "The way gradients propagate backward through time is central to training RNNs, yet it is also the source of major challenges like vanishing or exploding gradients. This practice utilizes a simplified scalar RNN to provide a clear, analytical window into how architectural choices influence this gradient flow. By comparing a loss applied only at the final timestep to one distributed across the entire sequence, you will derive a concrete result that demonstrates how intermediate losses create valuable \"shortcuts\" for the gradient, a crucial technique for stabilizing the training of deep recurrent models. [@problem_id:3101225]", "problem": "Consider a scalar linear recurrent neural network defined for discrete time indices by the recurrence\n$$h_{t+1} = w\\,h_{t}, \\quad t = 0, 1, \\dots, T-1,$$\nwith initial hidden state $$h_{0} \\neq 0,$$ scalar parameter $$w \\in \\mathbb{R}$$ satisfying $$0  |w| \\neq 1,$$ and an integer horizon $$T \\in \\mathbb{N}$$ with $$T \\geq 1.$$ The output at each time is identified with the hidden state, $$y_{t} = h_{t}.$$ The per-time-step loss is\n$$\\ell_{t} = \\frac{1}{2}\\,h_{t}^{2}.$$\nTwo overall loss placements are considered:\n- a temporally distributed loss,\n$$L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t},$$\n- and a final-time-only loss,\n$$L_{\\mathrm{final}} = \\ell_{T}.$$\n\nUsing only the chain rule of calculus, the definition of backpropagation through time (BPTT) as repeated application of the chain rule along the unfolded computational graph, and the model equations above, derive explicit symbolic expressions for $$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$$ and $$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$$ for arbitrary $$t \\in \\{0,1,\\dots,T\\}.$$ Then, as a quantitative summary of how loss placement affects gradient propagation back to the start of the sequence, compute the ratio\n$$R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$$\nas a single closed-form analytic expression in terms of $$w$$ and $$T.$$\n\nYour final answer must be the simplified closed-form expression for $$R(w,T)$$. No numerical evaluation is required and no rounding is needed. Do not include units, and express all quantities using standard mathematical notation in LaTeX.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and objective.\n\n### Step 1: Extract Givens\n- Recurrence relation: $h_{t+1} = w h_{t}$ for $t = 0, 1, \\dots, T-1$.\n- Initial condition: $h_{0} \\neq 0$.\n- Parameter constraints: $w \\in \\mathbb{R}$, $0  |w| \\neq 1$.\n- Horizon: $T \\in \\mathbb{N}$, $T \\geq 1$.\n- Output definition: $y_{t} = h_{t}$.\n- Per-time-step loss: $\\ell_{t} = \\frac{1}{2} h_{t}^{2}$.\n- Temporally distributed loss: $L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t}$.\n- Final-time-only loss: $L_{\\mathrm{final}} = \\ell_{T}$.\n- Quantities to derive:\n    1. Explicit symbolic expression for $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$ for $t \\in \\{0, 1, \\dots, T\\}$.\n    2. Explicit symbolic expression for $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$ for $t \\in \\{0, 1, \\dots, T\\}$.\n    3. The ratio $R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a simplified linear recurrent neural network and asks for the computation of gradients using backpropagation through time (BPTT). This is a standard and fundamental topic in deep learning and is based on the chain rule of calculus. All definitions and relationships are mathematically sound.\n- **Well-Posed**: All variables, constants, and functions are clearly defined. The constraints $h_0 \\neq 0$ and $0  |w| \\neq 1$ ensure that denominators in gradient expressions and the final ratio are non-zero, leading to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise mathematical language without any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nFirst, we establish the closed-form expression for the hidden state $h_t$. The recurrence $h_{t+1} = w h_t$ is a geometric progression. Unrolling it from the initial state $h_0$ gives:\n$$h_t = w^t h_0$$\nThis expression holds for all $t \\in \\{0, 1, \\dots, T\\}$.\n\n#### Derivation of $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$\nThe final-time loss is $L_{\\mathrm{final}} = \\ell_T = \\frac{1}{2} h_T^2$.\nThe gradient of the loss with respect to an intermediate hidden state $h_t$ is found by applying the chain rule through the computational graph from $h_t$ to $h_T$. The state $h_t$ only influences $L_{\\mathrm{final}}$ through its effect on $h_T$.\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = \\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{t}}$$\nThe first term is the derivative of the loss function with respect to its direct input:\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} = \\frac{\\partial}{\\partial h_T} \\left( \\frac{1}{2} h_T^2 \\right) = h_T$$\nThe second term is the derivative of a future state with respect to a past state. Using $h_T = w^{T-t} h_t$, we get:\n$$\\frac{\\partial h_T}{\\partial h_{t}} = \\frac{\\partial}{\\partial h_{t}} (w^{T-t} h_t) = w^{T-t}$$\nCombining these results, the gradient for any $t \\in \\{0, 1, \\dots, T\\}$ is:\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = h_T w^{T-t}$$\nThis is one of the requested explicit expressions. We can also express this in terms of $h_0$ by substituting $h_T = w^T h_0$:\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = (w^T h_0) w^{T-t} = h_0 w^{2T-t}$$\n\n#### Derivation of $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$\nThe distributed loss is $L_{\\mathrm{sum}} = \\sum_{k=1}^{T} \\ell_k = \\sum_{k=1}^{T} \\frac{1}{2} h_k^2$.\nThe hidden state $h_t$ influences all losses $\\ell_k$ for $k \\ge t$. We must sum over all paths from $h_t$ to the total loss.\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\frac{\\partial}{\\partial h_t} \\left( \\sum_{k=1}^{T} \\frac{1}{2} h_k^2 \\right)$$\nSince $h_k$ for $k  t$ does not depend on $h_t$, we can restrict the sum:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial}{\\partial h_t} \\left( \\frac{1}{2} h_k^2 \\right) = \\sum_{k=t}^{T} \\frac{\\partial(\\frac{1}{2} h_k^2)}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t} = \\sum_{k=t}^{T} h_k \\frac{\\partial h_k}{\\partial h_t}$$\nNote that the summation starts from $k=t$, but the loss $L_{\\mathrm{sum}}$ is defined from $t=1$. We must consider two cases for the lower bound of the sum.\n\n**Case 1: $t \\in \\{1, 2, \\dots, T\\}$**\nThe sum is valid as written. We have $\\frac{\\partial h_k}{\\partial h_t} = w^{k-t}$.\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} h_k w^{k-t}$$\nSubstitute $h_k = w^{k-t} h_t$:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} (w^{k-t} h_t) w^{k-t} = h_t \\sum_{k=t}^{T} w^{2(k-t)}$$\nLet $j = k-t$. The sum becomes a geometric series:\n$$h_t \\sum_{j=0}^{T-t} (w^2)^j = h_t \\left( \\frac{(w^2)^{T-t+1} - 1}{w^2 - 1} \\right) = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\nSo, for $t \\in \\{1, \\dots, T\\}$, the explicit expression is:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}} = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\n\n**Case 2: $t = 0$**\nThe loss sum starts at $k=1$, so $h_0$ has no direct contribution to any $\\ell_k$. Its influence is entirely through $h_k$ for $k \\ge 1$.\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} \\frac{\\partial \\ell_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_0} = \\sum_{k=1}^{T} h_k w^k$$\nSubstitute $h_k = w^k h_0$:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} (w^k h_0) w^k = h_0 \\sum_{k=1}^{T} w^{2k}$$\nThis is a geometric series with first term $w^2$, ratio $w^2$, and $T$ terms.\n$$\\sum_{k=1}^{T} (w^2)^k = w^2 \\frac{(w^2)^T - 1}{w^2 - 1} = \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\nThus, for $t=0$, the explicit expression is:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\n\n#### Calculation of the Ratio $R(w,T)$\nThe ratio is defined as $R(w,T) = \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$.\nFrom the derivations above, we have:\n- Numerator: $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$\n- Denominator: For $t=0$, $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_0} = h_0 w^{2T-0} = h_0 w^{2T}$.\n\nNow we compute the ratio. The condition $h_0 \\neq 0$ allows cancellation.\n$$R(w,T) = \\frac{h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}}{h_0 w^{2T}} = \\frac{w^2(w^{2T} - 1)}{(w^2 - 1)w^{2T}}$$\nWe can simplify this expression.\n$$R(w,T) = \\frac{w^2}{w^{2T}} \\cdot \\frac{w^{2T} - 1}{w^2 - 1} = w^{2-2T} \\frac{w^{2T} - 1}{w^2 - 1}$$\nAn alternative, more symmetric form is obtained by factoring out powers of $w$:\n$$R(w,T) = \\frac{w^{2T} - 1}{w^{2T}} \\cdot \\frac{w^2}{w^2 - 1} = \\left(1 - \\frac{1}{w^{2T}}\\right) \\cdot \\frac{1}{\\frac{w^2-1}{w^2}} = \\frac{1 - w^{-2T}}{1 - w^{-2}}$$\nThis is the final simplified closed-form expression for the ratio. The conditions $0  |w| \\neq 1$ ensure the denominator $1 - w^{-2}$ is non-zero.", "answer": "$$\\boxed{\\frac{1 - w^{-2T}}{1 - w^{-2}}}$$", "id": "3101225"}, {"introduction": "Modern deep learning frameworks compute gradients automatically, so why should we study the BPTT algorithm in detail? This final exercise bridges the gap between the specific algorithm and its general foundation: reverse-mode automatic differentiation (AD). You are tasked with manually deriving the step-by-step adjoint update rules for an RNN, which effectively recreates the logic an AD engine would follow. This process reveals that BPTT is not a unique trick but rather a systematic and efficient application of the chain rule to a recurrent computational graph. [@problem_id:3101263]", "problem": "You are asked to connect Backpropagation Through Time (BPTT) to Automatic Differentiation (AD) as implemented in compilers, by manually annotating adjoints for a simple recurrent neural network and cross-validating your results against an automatically differentiated computation. Work in purely mathematical terms with the following specification.\n\nConsider a simple recurrent neural network with hyperbolic tangent activation. For time steps indexed by $t \\in \\{1,2,\\dots,T\\}$, define the recurrence and output as follows:\n- Hidden pre-activation: $$\\mathbf{a}_t = \\mathbf{W}_{hh}\\,\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\,\\mathbf{x}_t + \\mathbf{b}_h.$$\n- Hidden state: $$\\mathbf{h}_t = \\tanh(\\mathbf{a}_t).$$\n- Output: $$\\mathbf{y}_t = \\mathbf{W}_{hy}\\,\\mathbf{h}_t + \\mathbf{b}_y.$$\n\nLet the loss be the sum of squared errors over the sequence:\n$$L = \\sum_{t=1}^{T} \\frac{1}{2} \\left\\|\\mathbf{y}_t - \\mathbf{z}_t\\right\\|_2^2.$$\n\nStart from fundamental base definitions:\n- The chain rule of calculus, which states for differentiable functions that the derivative of a composition is the product of derivatives at appropriately mapped points.\n- The definition of reverse-mode Automatic Differentiation (AD), which accumulates sensitivities (adjoints) from outputs back to inputs by traversing the computational graph in reverse topological order.\n- The definition of the hyperbolic tangent function $\\tanh$ and the well-tested identity for its derivative $\\frac{d}{du}\\tanh(u) = 1 - \\tanh^2(u)$.\n\nTask requirements:\n1. Implement manual Backpropagation Through Time (BPTT) by explicitly annotating adjoints for the parameters $\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_h$, and $\\mathbf{b}_y$ based solely on the above base definitions. Do not use any shortcut formulas beyond these principles.\n2. Implement a minimal reverse-mode Automatic Differentiation (AD) engine sufficient to handle the operations used in the above recurrent computation: matrix multiplication, addition, transpose, and elementwise $\\tanh$. The engine should construct a computational graph and perform a single reverse pass to produce gradients with respect to the same parameters for the same computation and loss.\n3. For each test case specified below, compute both sets of gradients and return the maximum absolute difference across all corresponding gradient elements for the parameters $\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_h$, and $\\mathbf{b}_y$. No physical units apply.\n\nTest suite:\nProvide four test cases, each defined by dimensions, seeds, and scales. In all cases, the initial hidden state is $\\mathbf{h}_0 = \\mathbf{0}$.\n\n- Test case $1$ (general case):\n  - Dimensions: hidden size $H = 2$, input size $D = 2$, output size $O = 1$, sequence length $T = 3$.\n  - Parameter initialization: use seed $42$; draw standard normal entries and scale by $0.1$ for $\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$; set $\\mathbf{b}_h = \\mathbf{0}$, $\\mathbf{b}_y = \\mathbf{0}$.\n  - Inputs: use seed $100$; draw standard normal entries for each $\\mathbf{x}_t$ and reshape to column vectors; scale by $1.0$.\n  - Targets: use seed $200$; draw standard normal entries for each $\\mathbf{z}_t$ and reshape to column vectors; scale by $0.5$.\n\n- Test case $2$ (zero-weights boundary):\n  - Dimensions: $H = 3$, $D = 1$, $O = 2$, $T = 5$.\n  - Parameters: all zeros for $\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_h$, $\\mathbf{b}_y$.\n  - Inputs: use seed $7$; standard normal, scale $1.0$.\n  - Targets: use seed $8$; standard normal, scale $1.0$.\n\n- Test case $3$ (longer sequence):\n  - Dimensions: $H = 3$, $D = 2$, $O = 2$, $T = 10$.\n  - Parameter initialization: use seed $123$; standard normal scaled by $0.2$ for $\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$. Biases: $\\mathbf{b}_h$ entries set to constant $0.05$, $\\mathbf{b}_y$ entries set to constant $-0.03$.\n  - Inputs: use seed $321$; standard normal scaled by $0.3$.\n  - Targets: use seed $654$; standard normal scaled by $0.4$.\n\n- Test case $4$ (saturated $\\tanh$ regime):\n  - Dimensions: $H = 2$, $D = 2$, $O = 1$, $T = 4$.\n  - Parameter initialization: use seed $11$; standard normal scaled by $5.0$ for $\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$. Biases: $\\mathbf{b}_h$ entries set to constant $0.5$, $\\mathbf{b}_y$ entries set to constant $0.1$.\n  - Inputs: use seed $22$; standard normal scaled by $2.0$.\n  - Targets: use seed $33$; standard normal scaled by $1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the maximum absolute gradient differences for the four test cases as a comma-separated list enclosed in square brackets, for example, $$[\\delta_1,\\delta_2,\\delta_3,\\delta_4]$$ where each $\\delta_i$ is a floating-point number representing the maximum absolute difference for test case $i$.", "solution": "The user-provided problem is assessed to be valid. It is scientifically sound, well-posed, objective, and provides all necessary information for a solution. It is a standard numerical exercise in computational differentiation, comparing the specific algorithm of Backpropagation Through Time (BPTT) with a general-purpose reverse-mode Automatic Differentiation (AD) engine.\n\nThe solution will be presented in three parts:\n1.  **Derivation and Implementation of Manual Backpropagation Through Time (BPTT)**: We will derive the gradient update rules for each parameter by explicitly applying the chain rule to the recurrent computation, unrolled over time.\n2.  **Implementation of a Reverse-Mode Automatic Differentiation (AD) Engine**: We will construct a minimal AD engine that builds a computational graph and propagates gradients (adjoints) backwards from the loss to the parameters.\n3.  **Validation and Comparison**: For each test case, we will compute gradients using both methods and find the maximum absolute difference between them, verifying that BPTT is a specific instance of the general reverse-mode AD principle.\n\n### 1. Manual Backpropagation Through Time (BPTT)\n\nBPTT computes gradients in a recurrent neural network by unrolling the network through time, creating a large, non-recurrent computational graph, and then applying the standard backpropagation algorithm. This is equivalent to accumulating gradients by iterating backwards through time.\n\nLet the adjoint of a variable $V$ be denoted as $\\bar{V} = \\frac{\\partial L}{\\partial V}$. The loss is $L = \\sum_{t=1}^{T} L_t$, where $L_t = \\frac{1}{2} \\left\\|\\mathbf{y}_t - \\mathbf{z}_t\\right\\|_2^2$.\n\n**Forward Pass**: First, we compute and store all intermediate values for $t = 1, \\dots, T$, starting with $\\mathbf{h}_0 = \\mathbf{0}$.\n$$ \\mathbf{a}_t = \\mathbf{W}_{hh}\\,\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\,\\mathbf{x}_t + \\mathbf{b}_h $$\n$$ \\mathbf{h}_t = \\tanh(\\mathbf{a}_t) $$\n$$ \\mathbf{y}_t = \\mathbf{W}_{hy}\\,\\mathbf{h}_t + \\mathbf{b}_y $$\n\n**Backward Pass**: We iterate backwards from $t=T$ down to $t=1$. Parameter gradients are accumulated over all timesteps. We initialize all parameter gradients to zero (e.g., $\\bar{\\mathbf{W}}_{hy} = \\mathbf{0}$). The gradient with respect to the hidden state that flows from the future, $\\bar{\\mathbf{h}}_{t, \\text{next}} = \\frac{\\partial L}{\\partial \\mathbf{h}_t}|_{\\text{from } t+1, \\dots, T}$, is initialized to $\\bar{\\mathbf{h}}_{T, \\text{next}} = \\mathbf{0}$.\n\nFor each timestep $t = T, \\dots, 1$:\n1.  **Adjoint of the output $\\mathbf{y}_t$**: The gradient of the loss with respect to the output at timestep $t$ is:\n    $$ \\bar{\\mathbf{y}}_t = \\frac{\\partial L}{\\partial \\mathbf{y}_t} = \\frac{\\partial L_t}{\\partial \\mathbf{y}_t} = \\mathbf{y}_t - \\mathbf{z}_t $$\n\n2.  **Adjoints of the output layer parameters ($\\mathbf{W}_{hy}, \\mathbf{b}_y$)**: Using the chain rule on $\\mathbf{y}_t = \\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y$:\n    $$ \\bar{\\mathbf{W}}_{hy} \\mathrel{+}= \\frac{\\partial L}{\\partial \\mathbf{W}_{hy}}\\bigg|_{\\text{from }t} = \\bar{\\mathbf{y}}_t \\mathbf{h}_t^T $$\n    $$ \\bar{\\mathbf{b}}_y \\mathrel{+}= \\frac{\\partial L}{\\partial \\mathbf{b}_y}\\bigg|_{\\text{from }t} = \\bar{\\mathbf{y}}_t $$\n    The `$\\mathrel{+}=$` denotes accumulation.\n\n3.  **Adjoint of the hidden state $\\mathbf{h}_t$**: The hidden state $\\mathbf{h}_t$ influences the loss through the current output $\\mathbf{y}_t$ and all subsequent hidden states starting from $\\mathbf{h}_{t+1}$. The gradient contribution from the future is already contained in $\\bar{\\mathbf{h}}_{t, \\text{next}}$.\n    $$ \\bar{\\mathbf{h}}_t = \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{h}_t}^T \\bar{\\mathbf{y}}_t + \\bar{\\mathbf{h}}_{t, \\text{next}} = \\mathbf{W}_{hy}^T \\bar{\\mathbf{y}}_t + \\bar{\\mathbf{h}}_{t, \\text{next}} $$\n\n4.  **Adjoint of the pre-activation $\\mathbf{a}_t$**: Using the chain rule on $\\mathbf{h}_t = \\tanh(\\mathbf{a}_t)$ and the identity $\\frac{d}{du}\\tanh(u) = 1-\\tanh^2(u)$:\n    $$ \\bar{\\mathbf{a}}_t = \\left(\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{a}_t}\\right)^T \\bar{\\mathbf{h}}_t = \\text{diag}(1 - \\tanh^2(\\mathbf{a}_t)) \\bar{\\mathbf{h}}_t = (1 - \\mathbf{h}_t \\odot \\mathbf{h}_t) \\odot \\bar{\\mathbf{h}}_t $$\n    where $\\odot$ denotes the element-wise (Hadamard) product.\n\n5.  **Adjoints of the recurrent layer parameters ($\\mathbf{W}_{hh}, \\mathbf{W}_{xh}, \\mathbf{b}_h$)**: Using the chain rule on $\\mathbf{a}_t = \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h$:\n    $$ \\bar{\\mathbf{W}}_{hh} \\mathrel{+}= \\frac{\\partial L}{\\partial \\mathbf{W}_{hh}}\\bigg|_{\\text{from }t} = \\bar{\\mathbf{a}}_t \\mathbf{h}_{t-1}^T $$\n    $$ \\bar{\\mathbf{W}}_{xh} \\mathrel{+}= \\frac{\\partial L}{\\partial \\mathbf{W}_{xh}}\\bigg|_{\\text{from }t} = \\bar{\\mathbf{a}}_t \\mathbf{x}_t^T $$\n    $$ \\bar{\\mathbf{b}}_h \\mathrel{+}= \\frac{\\partial L}{\\partial \\mathbf{b}_h}\\bigg|_{\\text{from }t} = \\bar{\\mathbf{a}}_t $$\n\n6.  **Adjoint for the previous hidden state $\\mathbf{h}_{t-1}$**: Finally, we compute the gradient to be passed back to the previous timestep. This becomes the `$\\bar{\\mathbf{h}}_{\\text{next}}$` for step $t-1$.\n    $$ \\bar{\\mathbf{h}}_{(t-1), \\text{next}} = \\left(\\frac{\\partial \\mathbf{a}_t}{\\partial \\mathbf{h}_{t-1}}\\right)^T \\bar{\\mathbf{a}}_t = \\mathbf{W}_{hh}^T \\bar{\\mathbf{a}}_t $$\n\nThis sequence of steps, iterated from $t=T$ to $1$, yields the total gradients for all parameters.\n\n### 2. Reverse-Mode Automatic Differentiation (AD)\n\nReverse-mode AD is a general technique for computing gradients. It operates in two phases:\n1.  **Forward Pass**: The computation is executed, and a computational graph is built, recording each operation and its input variables (parents).\n2.  **Backward Pass**: Gradients (adjoints) are propagated backward from the final output (the loss) to all input variables by traversing the graph in reverse topological order. For each node in the graph, its adjoint is used to compute the adjoints of its parents, according to the local partial derivative of the operation.\n\nWe will implement a `Variable` class to represent nodes in the graph. Each `Variable` will store its numerical `value`, its `grad` (adjoint), and a `_backward` function that encapsulates the logic for propagating its gradient to its parents. Overloading operators like `+`, `@` (matrix multiplication), etc., allows the graph to be constructed implicitly as the forward pass is written.\n\nThe backward pass starts by setting the gradient of the final loss `Variable` to $1.0$. It then performs a topological sort on the graph and iterates through the sorted nodes in reverse, calling the `_backward` function for each. This process accumulates the gradients in the `.grad` attribute of each parameter `Variable`.\n\n### 3. Implementation and Validation\n\nWe will implement both approaches in Python. The manual BPTT method will follow the derived equations directly. The AD method will use our `Variable` class to build and backpropagate through the RNN computation graph. For each of the four specified test cases, we will:\n1.  Generate parameters, inputs, and targets using the specified dimensions, seeds, and scales.\n2.  Compute the gradients of the five parameter sets ($\\mathbf{W}_{hh}$, $\\mathbf{W}_{xh}$, $\\mathbf{W}_{hy}$, $\\mathbf{b}_h$, $\\mathbf{b}_y$) using both methods.\n3.  Calculate the maximum absolute difference between the gradients obtained from the two methods across all elements of all parameters. This difference should be close to zero, limited only by floating-point precision, thus cross-validating the implementations.\nThe final output will be a list of these maximum differences for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# --- Part 2: Automatic Differentiation Engine ---\n\nclass Variable:\n    \"\"\"\n    A node in the computational graph for reverse-mode automatic differentiation.\n    \"\"\"\n    def __init__(self, value, name=None, parents=None, op=None):\n        self.value = np.asarray(value, dtype=float)\n        self.name = name\n        self._parents = set(parents) if parents else set()\n        self._backward_fn = lambda: None\n        self.grad = np.zeros_like(self.value)\n\n    def backward(self):\n        \"\"\"Performs backpropagation starting from this variable.\"\"\"\n        topo_order = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for p in v._parents:\n                    build_topo(p)\n                topo_order.append(v)\n        \n        build_topo(self)\n        \n        # Initialize gradients for all nodes in this specific graph\n        for v in topo_order:\n            v.grad = np.zeros_like(v.value)\n        self.grad = np.ones_like(self.value)\n        \n        for v in reversed(topo_order):\n            v._backward_fn()\n\n    def __add__(self, other):\n        other = other if isinstance(other, Variable) else Variable(other)\n        out = Variable(self.value + other.value, parents=[self, other], op='add')\n        \n        def _backward_fn():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward_fn = _backward_fn\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        other = other if isinstance(other, Variable) else Variable(other)\n        out = Variable(self.value - other.value, parents=[self, other], op='sub')\n        \n        def _backward_fn():\n            self.grad += out.grad\n            other.grad -= out.grad\n        out._backward_fn = _backward_fn\n        return out\n    \n    def __rsub__(self, other):\n        return Variable(other) - self\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Variable) else Variable(other)\n        out = Variable(self.value * other.value, parents=[self, other], op='mul')\n        \n        def _backward_fn():\n            self.grad += other.value * out.grad\n            other.grad += self.value * out.grad\n        out._backward_fn = _backward_fn\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __matmul__(self, other):\n        other = other if isinstance(other, Variable) else Variable(other)\n        out = Variable(self.value @ other.value, parents=[self, other], op='matmul')\n\n        def _backward_fn():\n            self.grad += out.grad @ other.value.T\n            other.grad += self.value.T @ out.grad\n        out._backward_fn = _backward_fn\n        return out\n\n    def __pow__(self, power):\n        out = Variable(self.value ** power, parents=[self], op='pow')\n\n        def _backward_fn():\n            self.grad += (power * self.value**(power - 1)) * out.grad\n        out._backward_fn = _backward_fn\n        return out\n\ndef ad_tanh(x):\n    val = np.tanh(x.value)\n    out = Variable(val, parents=[x], op='tanh')\n    \n    def _backward_fn():\n        x.grad += (1 - val**2) * out.grad\n    out._backward_fn = _backward_fn\n    return out\n\ndef ad_sum(x):\n    out = Variable(np.sum(x.value), parents=[x], op='sum')\n    \n    def _backward_fn():\n        x.grad += np.ones_like(x.value) * out.grad\n    out._backward_fn = _backward_fn\n    return out\n\ndef ad_bptt_gradients(params, inputs, targets, h0):\n    W_hh_v, W_xh_v, W_hy_v, b_h_v, b_y_v = [Variable(p) for p in params]\n    \n    h_prev = Variable(h0)\n    total_loss = Variable(0.0)\n    \n    T = len(inputs)\n    for t in range(T):\n        x_t = Variable(inputs[t])\n        z_t = Variable(targets[t]) # Treat as constant\n        \n        a_t = W_hh_v @ h_prev + W_xh_v @ x_t + b_h_v\n        h_t = ad_tanh(a_t)\n        y_t = W_hy_v @ h_t + b_y_v\n        \n        loss_t = Variable(0.5) * ad_sum((y_t - z_t)**2)\n        total_loss = total_loss + loss_t\n        \n        h_prev = h_t\n        \n    total_loss.backward()\n    \n    return [p.grad for p in [W_hh_v, W_xh_v, W_hy_v, b_h_v, b_y_v]]\n\n# --- Part 1: Manual BPTT Implementation ---\n\ndef manual_bptt_gradients(params, inputs, targets, h0):\n    W_hh, W_xh, W_hy, b_h, b_y = params\n    H, _ = W_xh.shape\n    T = len(inputs)\n\n    # Store forward pass values\n    hs = {0: h0}\n    ats = {}\n    ys = {}\n\n    # Forward pass\n    for t in range(1, T + 1):\n        x_t = inputs[t-1]\n        h_prev = hs[t-1]\n        a_t = W_hh @ h_prev + W_xh @ x_t + b_h\n        h_t = np.tanh(a_t)\n        y_t = W_hy @ h_t + b_y\n        ats[t] = a_t\n        hs[t] = h_t\n        ys[t] = y_t\n\n    # Initialize gradients\n    dW_hh = np.zeros_like(W_hh)\n    dW_xh = np.zeros_like(W_xh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n\n    dh_next = np.zeros((H, 1))\n\n    # Backward pass\n    for t in reversed(range(1, T + 1)):\n        z_t = targets[t-1]\n        dy = ys[t] - z_t\n        \n        dW_hy += dy @ hs[t].T\n        db_y += dy\n        \n        dh = W_hy.T @ dy + dh_next\n        da = (1 - hs[t]**2) * dh\n        \n        dW_hh += da @ hs[t-1].T\n        dW_xh += da @ inputs[t-1].T\n        db_h += da\n        \n        dh_next = W_hh.T @ da\n\n    return dW_hh, dW_xh, dW_hy, db_h, db_y\n\n# --- Test Case Generation and Main Logic ---\n\ndef generate_test_case(case_num):\n    if case_num == 1:\n        H, D, O, T = 2, 2, 1, 3\n        seed_p, scale_p = 42, 0.1\n        seed_x, scale_x = 100, 1.0\n        seed_z, scale_z = 200, 0.5\n        b_h_val, b_y_val = 0.0, 0.0\n    elif case_num == 2:\n        H, D, O, T = 3, 1, 2, 5\n        seed_p, scale_p = -1, 0.0 # seed unused for zero weights but kept for structure\n        seed_x, scale_x = 7, 1.0\n        seed_z, scale_z = 8, 1.0\n        b_h_val, b_y_val = 0.0, 0.0\n    elif case_num == 3:\n        H, D, O, T = 3, 2, 2, 10\n        seed_p, scale_p = 123, 0.2\n        seed_x, scale_x = 321, 0.3\n        seed_z, scale_z = 654, 0.4\n        b_h_val, b_y_val = 0.05, -0.03\n    elif case_num == 4:\n        H, D, O, T = 2, 2, 1, 4\n        seed_p, scale_p = 11, 5.0\n        seed_x, scale_x = 22, 2.0\n        seed_z, scale_z = 33, 1.0\n        b_h_val, b_y_val = 0.5, 0.1\n    else:\n        raise ValueError(\"Invalid case number\")\n\n    if scale_p > 0.0:\n      rng_p = np.random.RandomState(seed_p)\n      W_hh = rng_p.randn(H, H) * scale_p\n      W_xh = rng_p.randn(H, D) * scale_p\n      W_hy = rng_p.randn(O, H) * scale_p\n    else: # Case 2\n      W_hh = np.zeros((H, H))\n      W_xh = np.zeros((H, D))\n      W_hy = np.zeros((O, H))\n      \n    b_h = np.full((H, 1), b_h_val)\n    b_y = np.full((O, 1), b_y_val)\n\n    rng_x = np.random.RandomState(seed_x)\n    inputs = [rng_x.randn(D, 1) * scale_x for _ in range(T)]\n    \n    rng_z = np.random.RandomState(seed_z)\n    targets = [rng_z.randn(O, 1) * scale_z for _ in range(T)]\n    \n    h0 = np.zeros((H, 1))\n    \n    params = (W_hh, W_xh, W_hy, b_h, b_y)\n    \n    return params, inputs, targets, h0\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and compare gradient computation methods.\n    \"\"\"\n    test_cases = [1, 2, 3, 4]\n    results = []\n\n    for case_num in test_cases:\n        params, inputs, targets, h0 = generate_test_case(case_num)\n        \n        # Compute gradients using manual BPTT\n        manual_grads = manual_bptt_gradients(params, inputs, targets, h0)\n        \n        # Compute gradients using automatic differentiation\n        ad_grads = ad_bptt_gradients(params, inputs, targets, h0)\n        \n        # Compare results\n        max_diff = 0.0\n        for m_grad, ad_grad in zip(manual_grads, ad_grads):\n            diff = np.abs(m_grad - ad_grad)\n            if diff.size > 0:\n                max_diff = max(max_diff, np.max(diff))\n        \n        results.append(max_diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3101263"}]}