{"hands_on_practices": [{"introduction": "The core issue with teacher forcing is \"exposure bias\"—the model is never exposed to its own errors during training. This practice [@problem_id:3179348] provides a hands-on method to quantify a key consequence of this bias: miscalibration. By comparing the model's confidence against its actual accuracy under both teacher-forced and free-running conditions, you will gain a concrete understanding of why a model can become overconfident or underconfident when generating sequences on its own.", "problem": "You are given a synthetic sequence generation setting designed to isolate the effects of teacher forcing on calibration. Let a model define, for each prefix, a categorical distribution over the next token. For each position $t$ in a sequence, define two conditioning regimes: teacher forcing, which uses the ground-truth prefix $y_{<t}$, and free-running greedy decoding, which uses the model’s previously generated tokens $\\hat{y}_{<t}$. At each position $t$, you will measure calibration via Expected Calibration Error (ECE) under both regimes and compare them.\n\nFundamental basis for the setting:\n- Sequences are drawn from a fixed dataset. The vocabulary consists of tokens indexed $0,1,2,3$ and a distinguished start token indexed $4$, denoted $\\langle \\mathrm{START} \\rangle$. There are no physical units or angles involved.\n- The model’s next-token distribution is specified by a row-stochastic bigram table $P$, where each row corresponds to the previous token index and each column corresponds to the next token index. The base table $P_{\\mathrm{base}}$ is\n$$\nP_{\\mathrm{base}}(4,\\cdot) = [0.7,\\,0.1,\\,0.1,\\,0.1],\\quad\nP_{\\mathrm{base}}(0,\\cdot) = [0.1,\\,0.3,\\,0.4,\\,0.2],\\quad\nP_{\\mathrm{base}}(1,\\cdot) = [0.1,\\,0.2,\\,0.6,\\,0.1],\\quad\nP_{\\mathrm{base}}(2,\\cdot) = [0.1,\\,0.1,\\,0.1,\\,0.7],\\quad\nP_{\\mathrm{base}}(3,\\cdot) = [0.5,\\,0.2,\\,0.2,\\,0.1].\n$$\nA second “perfect” table $P_{\\mathrm{perfect}}$ is:\n$$\nP_{\\mathrm{perfect}}(4,\\cdot) = [1.0,\\,0.0,\\,0.0,\\,0.0],\\quad\nP_{\\mathrm{perfect}}(0,\\cdot) = [0.0,\\,1.0,\\,0.0,\\,0.0],\\quad\nP_{\\mathrm{perfect}}(1,\\cdot) = [0.0,\\,0.0,\\,1.0,\\,0.0],\\quad\nP_{\\mathrm{perfect}}(2,\\cdot) = [0.0,\\,0.0,\\,0.0,\\,1.0],\\quad\nP_{\\mathrm{perfect}}(3,\\cdot) = [0.0,\\,0.0,\\,0.0,\\,1.0].\n$$\n- Temperature scaling: given a temperature $T>0$, transform each row $p$ of a probability table to a new row $p^{(T)}$ by\n$$\np^{(T)}_j = \\frac{p_j^{\\,\\alpha}}{\\sum_k p_k^{\\,\\alpha}},\\quad \\alpha = \\frac{1}{T},\n$$\nwhich preserves the $\\arg\\max$ class while modulating confidence.\n\nDataset:\n- Ground-truth sequences are\n$$\ny^{(1)} = [0,\\,1,\\,2,\\,3],\\quad\ny^{(2)} = [0,\\,1,\\,2],\\quad\ny^{(3)} = [0,\\,1],\\quad\ny^{(4)} = [0,\\,1,\\,2,\\,3].\n$$\nLet the maximum sequence length be $T_{\\max} = 4$. For any position $t$, consider only sequences whose length is at least $t$.\n\nTeacher forcing and free-running definitions:\n- Teacher forcing uses $p_\\theta(\\cdot\\mid y_{<t})$ to form the predictive distribution at position $t$.\n- Free-running greedy decoding starts from the start token index $4$ and repeatedly selects $\\hat{y}_t = \\arg\\max_j p_\\theta(j\\mid \\hat{y}_{<t})$, thereby forming its own prefix $\\hat{y}_{<t}$.\n\nCalibration measurement:\n- At any position $t$ and conditioning regime, define the confidence for an example as $c_t = \\max_j p_\\theta(j\\mid \\text{prefix})$, and define correctness for that example as $o_t = 1$ if the predicted token equals the ground-truth $y_t$, and $o_t = 0$ otherwise.\n- Expected Calibration Error (ECE) for position $t$ is computed by partitioning confidence values into $B$ equal-width bins on $[0,1]$, then aggregating bin-level absolute differences between empirical accuracy and mean confidence, weighted by the bin’s relative frequency. You must implement the binning and aggregation precisely and handle empty bins by contributing zero weight.\n\nYour tasks:\n1. For each test case below, construct $p_\\theta$ from the specified table (either $P_{\\mathrm{base}}$ or $P_{\\mathrm{perfect}}$) and temperature $T$, apply teacher forcing and free-running as defined, and compute per-position ECE for $t=1,2,3,4$ in both regimes.\n2. For each position $t$, compute the scalar difference $\\Delta_t = \\mathrm{ECE}_{\\mathrm{free}}(t) - \\mathrm{ECE}_{\\mathrm{teacher}}(t)$.\n3. Aggregate all $\\Delta_t$ values across all test cases into a single flat list, in the order of test cases and within each test case in increasing $t$.\n\nTest suite:\n- Case A (happy path): $B=5$, $T=1.0$, table $P_{\\mathrm{base}}$.\n- Case B (boundary bins): $B=1$, $T=1.0$, table $P_{\\mathrm{base}}$.\n- Case C (overconfident): $B=10$, $T=0.1$, table $P_{\\mathrm{base}}$.\n- Case D (underconfident): $B=10$, $T=10.0$, table $P_{\\mathrm{base}}$.\n- Case E (perfect model): $B=10$, $T=1.0$, table $P_{\\mathrm{perfect}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,\\dots,x_{20}]$, where the $20$ entries are the $4$ position-wise differences $\\Delta_t$ for each of the $5$ test cases, concatenated in order. Each entry must be a floating-point number. No other text should be printed.", "solution": "This problem requires analyzing the difference in Expected Calibration Error (ECE) between two conditioning regimes—teacher forcing and free-running greedy decoding—in a controlled, synthetic environment. The solution involves a systematic computation for each test case, where the core of the problem lies in carefully tracking the context token for each prediction, which differs between the two regimes.\n\n### 1. Model and Temperature Scaling\nFirst, for each test case, the model's transition probability matrix, $p_\\theta$, is constructed. This is done by taking the specified base table ($P_{\\mathrm{base}}$ or $P_{\\mathrm{perfect}}$) and applying temperature scaling with the given temperature $T$. The transformation for each row $p$ (a probability distribution) is:\n$$p_j^{(T)} = \\frac{p_j^{\\,1/T}}{\\sum_k p_k^{\\,1/T}}$$\nA temperature $T < 1$ sharpens the distribution, increasing the confidence of the most likely token, while $T > 1$ flattens it, decreasing confidence. For $T=1$, the table remains unchanged.\n\n### 2. Algorithmic Procedure for a Single Time Step\nFor each time step $t \\in \\{1, 2, 3, 4\\}$, we calculate the ECE under both regimes.\n\n-   **Data Filtering**: We consider only the ground-truth sequences from the dataset with length at least $t$. Let this set of sequences be $D_t$.\n\n-   **Teacher Forcing (TF) Regime**: The model is always conditioned on the true previous token.\n    1.  **Context**: For each sequence $y^{(i)} \\in D_t$, the context for predicting the token at position $t$ is the ground-truth token at position $t-1$, i.e., $y_{t-1}^{(i)}$. For $t=1$, the context is the fixed start token, index $4$.\n    2.  **Prediction**: The next-token distribution is given by the corresponding row in $p_\\theta$: $d^{(i)} = p_\\theta( \\cdot \\mid y_{t-1}^{(i)})$.\n    3.  **Confidence & Correctness**: For each sequence, confidence is $c^{(i)} = \\max_j d^{(i)}_j$. The predicted token is $\\hat{y}_t^{(i)} = \\arg\\max_j d^{(i)}_j$. Correctness is $o^{(i)} = 1$ if $\\hat{y}_t^{(i)} = y_t^{(i)}$, and $0$ otherwise.\n    4.  **ECE Calculation**: The pairs $(c^{(i)}, o^{(i)})$ for all $y^{(i)} \\in D_t$ are collected to compute $\\mathrm{ECE}_{\\mathrm{teacher}}(t)$.\n\n-   **Free-Running (FR) Regime**: The model is conditioned on its own previously generated tokens. A single greedily-decoded sequence $\\hat{y}$ is generated once per test case.\n    1.  **Context**: The context for predicting the token at position $t$ is the token $\\hat{y}_{t-1}$ from the model's own generated sequence. This context is the *same* for all ground-truth sequences being evaluated at step $t$.\n    2.  **Prediction**: The single next-token distribution is $d = p_\\theta(\\cdot \\mid \\hat{y}_{t-1})$.\n    3.  **Confidence & Correctness**: The confidence $c = \\max_j d_j$ and predicted token $\\hat{y}_t = \\arg\\max_j d_j$ are constant for all samples at step $t$. For each ground-truth sequence $y^{(i)} \\in D_t$, we determine correctness $o^{(i)} = 1$ if $\\hat{y}_t = y_t^{(i)}$, and $0$ otherwise.\n    4.  **ECE Calculation**: The pairs $(c, o^{(i)})$ for all $y^{(i)} \\in D_t$ are collected to compute $\\mathrm{ECE}_{\\mathrm{free}}(t)$.\n\n### 3. ECE Computation\nThe Expected Calibration Error is computed by binning the collected confidence scores. For a set of $N$ pairs of (confidence, outcome) and $B$ bins:\n1.  Partition the interval $[0, 1]$ into $B$ equal-width bins.\n2.  For each bin $b_m$, calculate the average accuracy $\\mathrm{acc}(b_m)$ and average confidence $\\mathrm{conf}(b_m)$ of the samples that fall into it.\n3.  The ECE is the weighted average of the absolute difference between accuracy and confidence across all non-empty bins:\n    $$\\mathrm{ECE} = \\sum_{m=1}^{B} \\frac{|S_m|}{N} |\\mathrm{acc}(b_m) - \\mathrm{conf}(b_m)|$$\n    where $S_m$ is the set of samples in bin $m$.\n\n### 4. Final Aggregation\nAfter computing $\\mathrm{ECE}_{\\mathrm{teacher}}(t)$ and $\\mathrm{ECE}_{\\mathrm{free}}(t)$ for each $t \\in \\{1, 2, 3, 4\\}$, the difference $\\Delta_t = \\mathrm{ECE}_{\\mathrm{free}}(t) - \\mathrm{ECE}_{\\mathrm{teacher}}(t)$ is calculated. These four $\\Delta_t$ values are computed for each of the five test cases. The final output is a single flat list containing all $20$ $\\Delta_t$ values.\n\nAn interesting property of this specific problem setup is that for any given time step $t$ and regime, the context token is the same across all samples because all ground-truth sequences share the same prefix. Consequently, all samples within a (regime, time step) pair have the identical confidence score, simplifying the ECE calculation to $|\\text{mean accuracy} - \\text{confidence}|$, as all samples fall into a single bin.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the teacher forcing vs. free-running ECE problem.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    P_base_def = {\n        4: [0.7, 0.1, 0.1, 0.1],\n        0: [0.1, 0.3, 0.4, 0.2],\n        1: [0.1, 0.2, 0.6, 0.1],\n        2: [0.1, 0.1, 0.1, 0.7],\n        3: [0.5, 0.2, 0.2, 0.1],\n    }\n    \n    P_perfect_def = {\n        4: [1.0, 0.0, 0.0, 0.0],\n        0: [0.0, 1.0, 0.0, 0.0],\n        1: [0.0, 0.0, 1.0, 0.0],\n        2: [0.0, 0.0, 0.0, 1.0],\n        3: [0.0, 0.0, 0.0, 1.0],\n    }\n\n    # Convert definitions to numpy arrays with consistent indexing 0-4\n    P_base = np.zeros((5, 4))\n    for k, v in P_base_def.items():\n        P_base[k] = v\n        \n    P_perfect = np.zeros((5, 4))\n    for k, v in P_perfect_def.items():\n        P_perfect[k] = v\n\n    dataset = [\n        [0, 1, 2, 3],\n        [0, 1, 2],\n        [0, 1],\n        [0, 1, 2, 3],\n    ]\n    \n    T_max = 4\n\n    test_cases = [\n        {'name': 'A', 'B': 5,  'T': 1.0, 'P': P_base},\n        {'name': 'B', 'B': 1,  'T': 1.0, 'P': P_base},\n        {'name': 'C', 'B': 10, 'T': 0.1, 'P': P_base},\n        {'name': 'D', 'B': 10, 'T': 10.0,'P': P_base},\n        {'name': 'E', 'B': 10, 'T': 1.0, 'P': P_perfect},\n    ]\n\n    all_deltas = []\n\n    # --- Helper Functions ---\n\n    def calculate_ece(confidences, outcomes, B):\n        \"\"\"Calculates Expected Calibration Error.\"\"\"\n        if not confidences:\n            return 0.0\n\n        confidences = np.array(confidences)\n        outcomes = np.array(outcomes)\n        \n        total_samples = len(confidences)\n        \n        # Assign each sample to a bin\n        # A confidence of 1.0 must go into the last bin, B-1.\n        bin_indices = np.floor(confidences * B).astype(int)\n        bin_indices = np.minimum(bin_indices, B - 1)\n\n        ece = 0.0\n        for b in range(B):\n            in_bin = (bin_indices == b)\n            bin_count = np.sum(in_bin)\n            \n            if bin_count > 0:\n                bin_accuracy = np.mean(outcomes[in_bin])\n                bin_confidence = np.mean(confidences[in_bin])\n                weight = bin_count / total_samples\n                ece += weight * np.abs(bin_accuracy - bin_confidence)\n                \n        return ece\n\n    # --- Main Loop ---\n\n    for case in test_cases:\n        B, T, P_table = case['B'], case['T'], case['P']\n        \n        # 1. Apply temperature scaling\n        if T == 1.0:\n            p_theta = P_table\n        else:\n            alpha = 1.0 / T\n            # Add a small epsilon to avoid 0^alpha where alpha is non-integer\n            p_theta = np.power(P_table + 1e-12, alpha)\n            p_theta = p_theta / p_theta.sum(axis=1, keepdims=True)\n\n        # 2. Generate the single free-running sequence\n        y_hat = [4]  # Start with START token (index 4)\n        for _ in range(T_max):\n            prev_token = y_hat[-1]\n            dist = p_theta[prev_token]\n            next_token = np.argmax(dist).item()\n            y_hat.append(next_token)\n\n        case_deltas = []\n        for t in range(1, T_max + 1):\n            # Filter dataset for sequences with length >= t\n            relevant_sequences = [seq for seq in dataset if len(seq) >= t]\n            if not relevant_sequences:\n                case_deltas.append(0.0)\n                continue\n            \n            ground_truth_tokens = [seq[t - 1] for seq in relevant_sequences]\n            \n            # --- Teacher Forcing Regime ---\n            if t == 1:\n                context_token_tf = 4\n            else:\n                # In this dataset, all sequences have the same prefix, so context is uniform\n                context_token_tf = relevant_sequences[0][t - 2]\n            \n            dist_tf = p_theta[context_token_tf]\n            conf_tf = np.max(dist_tf)\n            pred_tf = np.argmax(dist_tf).item()\n            \n            outcomes_tf = [(1 if pred_tf == gt else 0) for gt in ground_truth_tokens]\n            confidences_tf = [conf_tf] * len(outcomes_tf)\n            ece_tf = calculate_ece(confidences_tf, outcomes_tf, B)\n            \n            # --- Free-Running Regime ---\n            context_token_fr = y_hat[t - 1] # y_hat[0] == START token idx\n            \n            dist_fr = p_theta[context_token_fr]\n            conf_fr = np.max(dist_fr)\n            pred_fr = np.argmax(dist_fr).item()\n\n            outcomes_fr = [(1 if pred_fr == gt else 0) for gt in ground_truth_tokens]\n            confidences_fr = [conf_fr] * len(outcomes_fr)\n            ece_fr = calculate_ece(confidences_fr, outcomes_fr, B)\n\n            # --- Calculate and store difference ---\n            delta = ece_fr - ece_tf\n            case_deltas.append(delta)\n        \n        all_deltas.extend(case_deltas)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{d:.15f}' for d in all_deltas)}]\")\n\nsolve()\n```", "id": "3179348"}, {"introduction": "Exposure bias can lead to \"cascading failures,\" where a single mistake leads to a sequence of subsequent errors. This exercise [@problem_id:3179290] asks you to build a simplified probabilistic model to investigate one common failure mode: premature termination. You will explore how an initial token error can increase the probability of emitting an End-Of-Sequence (EOS) token, allowing you to quantify the \"omission rate\" and understand the dynamics of error propagation.", "problem": "You will model how teacher forcing versus free-running affects premature emission of the End Of Sequence (EOS) token in auto-regressive sequence generation, and you will quantify the connection between token omission errors and an increased conditional probability of EOS under self-generated prefixes. Work from the base definitions of auto-regressive modeling and conditional probability, and design a program that computes quantitative outcomes for a set of specified parameter values.\n\nAssume the following foundational setting and definitions, which you must use as the starting point for your derivation and algorithmic design:\n\n- Auto-regressive factorization: for any sequence of discrete tokens, the model parameterized by parameters $\\theta$ defines a distribution $p_{\\theta}$ such that\n  $$p_{\\theta}(y_{1:T}) = \\prod_{t=1}^{T} p_{\\theta}(y_{t}\\mid y_{<t}).$$\n- Teacher forcing: during training, the model conditions on gold prefixes $y_{<t}$ when optimizing the negative log-likelihood objective, that is, the cross-entropy between the empirical data distribution and $p_{\\theta}$.\n- Free-running (also called open-loop decoding): during generation, the model conditions on its own previously sampled tokens, denoted $\\hat{y}_{<t}$, so the distribution at time step $t$ is $p_{\\theta}(\\cdot \\mid \\hat{y}_{<t})$.\n- Define End Of Sequence (EOS) as a special token marking termination. Let the baseline probability of emitting EOS before the true sequence end under gold prefixes be a constant $q \\in [0,1]$ at each pre-target step $t \\in \\{1,\\dots,L-1\\}$, where $L$ is the ground-truth length.\n- Define a token error event at a time step as emitting a non-EOS token that does not match the ground truth. Let the per-step probability of making such a token error, conditional on not emitting EOS, be a constant $p_{\\mathrm{err}} \\in [0,1]$ when the generator has not yet made any error on previous steps.\n- Exposure bias model: in free-running, once at least one token error has occurred anywhere in the prefix, the model’s conditional EOS probability is inflated relative to the baseline. Specifically, when conditioning on a self-generated prefix that contains at least one error, the EOS probability at subsequent steps is\n  $$q' = \\min\\{1,\\, q \\cdot (1+\\alpha)\\},$$\n  where $\\alpha \\ge 0$ is a given EOS inflation factor. When no error has yet occurred in the prefix, the EOS probability remains $q$.\n- State description: at each step before termination, the generator is in one of two mutually exclusive states: the “good” state $\\mathsf{G}$ (no token error has occurred so far) or the “bad” state $\\mathsf{B}$ (at least one token error has occurred). The process starts in state $\\mathsf{G}$ at step $t=1$. If EOS is emitted, generation terminates immediately. If a non-EOS token is emitted while in $\\mathsf{G}$, then with probability $p_{\\mathrm{err}}$ the state switches to $\\mathsf{B}$ at the next step; otherwise it remains in $\\mathsf{G}$. If a non-EOS token is emitted while in $\\mathsf{B}$, the state remains in $\\mathsf{B}$.\n\nYour tasks:\n\n1) From the above base definitions and assumptions only, derive a logically consistent method to compute the omission rate, defined as the probability that EOS is emitted strictly before the ground-truth length, that is, at any step $t \\in \\{1,\\dots,L-1\\}$ while free-running. You must treat the temporal evolution of the two-state process and properly account for termination.\n\n2) Using the same model, formally define and compute an average conditional EOS probability under self-generated prefixes, and quantify its increase relative to the teacher-forced baseline. Specifically, for each pre-target step $t \\in \\{1,\\dots,L-1\\}$, consider the conditional EOS probability under free-running given that generation has not yet terminated at the start of step $t$. Average this conditional probability across steps $t \\in \\{1,\\dots,L-1\\}$ using an equal-weight average over steps for which the process has not yet terminated at that step. Let this average be denoted $\\overline{p}_{\\mathrm{EOS}}^{\\mathrm{free}}$. The baseline teacher-forced EOS probability per pre-target step is $q$, so report the increase $\\Delta = \\overline{p}_{\\mathrm{EOS}}^{\\mathrm{free}} - q$. If there are no pre-target steps, that is, if $L \\le 1$, define both the omission rate and $\\Delta$ to be $0$.\n\nProgram requirements:\n\n- Implement your derived method as a complete program that takes no input and computes results for the following test suite of parameter quadruples $(L, q, \\alpha, p_{\\mathrm{err}})$:\n  - Test $1$: $(L, q, \\alpha, p_{\\mathrm{err}}) = (\\,10,\\, 0.02,\\, 3.0,\\, 0.1\\,)$.\n  - Test $2$: $(L, q, \\alpha, p_{\\mathrm{err}}) = (\\,10,\\, 0.0,\\, 3.0,\\, 0.5\\,)$.\n  - Test $3$: $(L, q, \\alpha, p_{\\mathrm{err}}) = (\\,10,\\, 0.05,\\, 0.0,\\, 0.3\\,)$.\n  - Test $4$: $(L, q, \\alpha, p_{\\mathrm{err}}) = (\\,1,\\, 0.1,\\, 10.0,\\, 0.9\\,)$.\n  - Test $5$: $(L, q, \\alpha, p_{\\mathrm{err}}) = (\\,50,\\, 0.01,\\, 4.0,\\, 0.2\\,)$.\n- For each test case, compute:\n  - The omission rate as a real number in $[0,1]$.\n  - The increase $\\Delta$ as a real number (which can be nonnegative or zero).\n- Output specification: Your program should produce a single line of output containing a list of results, one per test case, in the same order as listed above. Each result must be a two-element list of floats $[\\text{omission\\_rate}, \\Delta]$, where both floats are rounded to exactly $6$ decimal places using standard rounding to nearest, with ties to the nearest even representable value.\n- The final output format is: a single line printing the Python-style list of lists, for example,\n  $$[\\,[x_{1},y_{1}],\\,[x_{2},y_{2}],\\,\\dots\\,]$$\n  where each $x_{i}$ is the omission rate and each $y_{i}$ is the increase $\\Delta$ for the $i$-th test case.\n\nScientific realism and consistency requirements:\n\n- Your derivation and algorithm must start only from the core definitions above (auto-regressive factorization, conditional probabilities, and the two-state exposure-bias model) and standard probability rules. You must not assume any form of oracle access or provide any shortcut formulas not derivable from the stated assumptions.\n- Ensure your computation is internally consistent: EOS emission immediately terminates generation; state transitions happen only on non-EOS emissions; and all probabilities remain within $[0,1]$ by construction. If at any stage you must clamp a value, use the given definition of $q'$ above.", "solution": "We begin from the auto-regressive factorization $p_{\\theta}(y_{1:T}) = \\prod_{t=1}^{T} p_{\\theta}(y_{t}\\mid y_{<t})$ and the distinction between teacher forcing, which conditions on gold prefixes, and free-running, which conditions on self-generated prefixes $\\hat{y}_{<t}$. The End Of Sequence (EOS) token is a special symbol that terminates generation. Under teacher forcing at pre-target steps, the EOS probability is the baseline $q$. Under free-running, exposure bias implies that the conditional input distribution differs once an error occurs, and we model this by increasing the EOS probability to $q' = \\min\\{1,\\, q(1+\\alpha)\\}$ whenever the current prefix contains at least one error; otherwise, it remains at $q$.\n\nTo connect omissions with conditional EOS probabilities, we model decoding as a two-state stochastic process indexed by discrete time steps $t \\in \\{1,2,\\dots\\}$ prior to termination:\n\n- State $\\mathsf{G}$ (good): no token error has occurred in the prefix so far.\n- State $\\mathsf{B}$ (bad): at least one token error has occurred in the prefix so far.\n\nThe process starts at step $t=1$ in state $\\mathsf{G}$. At each step, if the process is alive (i.e., has not terminated via EOS), the following occurs:\n\n1) With probability equal to the state-dependent EOS probability, the process emits EOS and terminates immediately. In state $\\mathsf{G}$ this probability is $q$, and in state $\\mathsf{B}$ it is $q'$.\n\n2) Otherwise, a non-EOS token is emitted. If the current state is $\\mathsf{G}$, then with probability $p_{\\mathrm{err}}$ the emitted token is incorrect and the next state becomes $\\mathsf{B}$; with probability $1 - p_{\\mathrm{err}}$ the emitted token is correct and the next state remains $\\mathsf{G}$. If the current state is $\\mathsf{B}$, the next state remains $\\mathsf{B}$.\n\nThis specification is derived from the base principles: the chain rule of probability, immediate termination upon emitting EOS, and the definition of exposure-bias-induced increase in EOS probability once the prefix contains an error. There is no need for any shortcut formula; instead we construct a recurrence by tracking the probability mass over states at the start of each step.\n\nDefine $g_{t}$ as the probability that the process is alive in state $\\mathsf{G}$ at the start of step $t$, and $b_{t}$ as the probability that the process is alive in state $\\mathsf{B}$ at the start of step $t$. The initial condition is $g_{1} = 1$ and $b_{1} = 0$. At any step $t$, given the process is alive with state mass $(g_{t}, b_{t})$, the termination (EOS) probability mass at step $t$ is\n$$m_{t}^{\\mathrm{EOS}} = q \\cdot g_{t} + q' \\cdot b_{t}.$$\nThe surviving (non-EOS) mass that goes on to emit a non-EOS token is\n$$s_{t}^{\\mathrm{nonEOS}} = (1 - q) \\cdot g_{t} + (1 - q') \\cdot b_{t}.$$\nFrom the state-transition rules, the recurrences for the alive state mass at the next step are obtained by conditioning on non-EOS emission:\n- The mass that remains in $\\mathsf{G}$ is the non-EOS mass from $\\mathsf{G}$ times the probability of no token error:\n  $$g_{t+1} = (1 - q)\\, g_{t}\\, (1 - p_{\\mathrm{err}}).$$\n- The mass that is in $\\mathsf{B}$ at the next step is the non-EOS mass from $\\mathsf{B}$ (which stays in $\\mathsf{B}$) plus the non-EOS mass from $\\mathsf{G}$ that experiences a token error:\n  $$b_{t+1} = (1 - q')\\, b_{t} + (1 - q)\\, g_{t}\\, p_{\\mathrm{err}}.$$\n\nThese recurrences are direct consequences of the law of total probability applied to mutually exclusive ways of being alive at the next step, together with immediate termination on EOS and the memoryless per-step behavior specified by the model. Note that $q' = \\min\\{1, q (1+\\alpha)\\}$ ensures that probabilities remain within $[0,1]$.\n\nTask 1 (omission rate): An omission occurs if EOS is emitted strictly before reaching the ground-truth length $L$, that is, at any step $t \\in \\{1,\\dots,L-1\\}$. Because termination is absorbing, the total omission probability is the sum of the EOS termination masses over these steps:\n$$\\text{OmissionRate}(L, q, \\alpha, p_{\\mathrm{err}}) = \\sum_{t=1}^{L-1} \\left(q \\cdot g_{t} + q' \\cdot b_{t}\\right),$$\nwith $(g_{t}, b_{t})$ evolved by the above recurrences and initialized at $(1,0)$.\n\nThis expression follows from summing the disjoint probabilities of termination at each step $t$ prior to $L$.\n\nTask 2 (average conditional EOS probability boost): At each pre-target step $t \\in \\{1,\\dots,L-1\\}$, the conditional EOS probability under free-running given that the process is alive at that step is\n$$p_{t}^{\\mathrm{free}} = \\frac{q \\cdot g_{t} + q' \\cdot b_{t}}{g_{t} + b_{t}} \\quad \\text{whenever } g_{t}+b_{t} > 0.$$\nWe are asked to average this conditional probability across pre-target steps using an equal-weight average over steps, restricted to those steps where the process is alive. Define the index set of steps with nonzero alive mass as\n$$\\mathcal{T} = \\{\\, t \\in \\{1,\\dots,L-1\\} \\mid g_{t} + b_{t} > 0 \\,\\}.$$\nThen the average conditional EOS probability is\n$$\\overline{p}_{\\mathrm{EOS}}^{\\mathrm{free}} = \\begin{cases}\n\\frac{1}{|\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} p_{t}^{\\mathrm{free}}, & \\text{if } |\\mathcal{T}| > 0, \\\\\n0, & \\text{if } |\\mathcal{T}| = 0,\n\\end{cases}$$\nand the requested increase relative to the teacher-forced baseline $q$ is\n$$\\Delta = \\overline{p}_{\\mathrm{EOS}}^{\\mathrm{free}} - q.$$\nWhen $L \\le 1$, there are no pre-target steps, so by definition both the omission rate and $\\Delta$ are set to $0$.\n\nAlgorithmic design:\n\n- Initialize $g_{1} = 1$ and $b_{1} = 0$. Compute $q' = \\min\\{1, q(1+\\alpha)\\}$.\n- For $t$ from $1$ to $L-1$:\n  - Accumulate omission mass $m_{t}^{\\mathrm{EOS}} = q g_{t} + q' b_{t}$.\n  - If $g_{t} + b_{t} > 0$, compute $p_{t}^{\\mathrm{free}} = m_{t}^{\\mathrm{EOS}} / (g_{t} + b_{t})$ and include it in the step-average.\n  - Update $(g_{t+1}, b_{t+1})$ via the recurrences.\n- The omission rate is the sum of $m_{t}^{\\mathrm{EOS}}$ over $t \\in \\{1,\\dots,L-1\\}$. The increase $\\Delta$ is the difference between the step-average of $p_{t}^{\\mathrm{free}}$ and $q$ (or $0$ if $L \\le 1$).\n\nThis computation is fully determined by the base model and uses only standard probability rules. It makes the relationship explicit: increasing $\\alpha$ or $p_{\\mathrm{err}}$ raises the share of alive mass in state $\\mathsf{B}$ and increases $q'$, thereby increasing both the average conditional EOS probability under free-running and, through the per-step termination masses, the omission rate. Edge cases are handled as follows: if $q=0$ then $q'=0$ and omissions are impossible; if $\\alpha=0$ then $q'=q$ so free-running equals the teacher-forced baseline with respect to EOS, and the boost $\\Delta$ is $0$; if $L \\le 1$ there are no pre-target opportunities and both metrics are $0$ by definition.\n\nImplementation details:\n\n- Real arithmetic is sufficient; probabilities remain in $[0,1]$ by construction, given the clamping of $q'$ and the update rules.\n- The program will implement the loop exactly and round the two outputs per test case to $6$ decimal places, returning a single line containing the list of results for the prescribed test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_omission_and_boost(L: int, q: float, alpha: float, p_err: float):\n    \"\"\"\n    Compute:\n      - omission rate: probability EOS occurs before step L (i.e., among steps 1..L-1)\n      - average conditional EOS probability boost: average over t=1..L-1 of\n        p(EOS | alive at t, free-running) minus q (teacher-forced baseline),\n        where p(EOS | alive at t) = (q*g_t + q'*b_t)/(g_t + b_t), if alive mass>0.\n    Model:\n      - Two-state process: G (no error so far), B (error has occurred).\n      - Start: g1=1, b1=0.\n      - At each step t:\n          terminate mass m_t = q*g_t + q'*b_t.\n          surviving non-EOS mass from G: (1 - q) * g_t\n          surviving non-EOS mass from B: (1 - q') * b_t\n          next g_{t+1} = (1 - q) * g_t * (1 - p_err)\n          next b_{t+1} = (1 - q') * b_t + (1 - q) * g_t * p_err\n      - q' = min(1, q*(1+alpha))\n    \"\"\"\n    if L = 1:\n        return 0.0, 0.0\n\n    q_prime = min(1.0, q * (1.0 + alpha))\n\n    g = 1.0  # g_1\n    b = 0.0  # b_1\n\n    omission = 0.0\n    # For average conditional EOS probability across pre-target steps:\n    p_eos_cond_sum = 0.0\n    alive_steps = 0  # count of steps where alive mass > 0\n\n    for t in range(1, L):  # steps 1..L-1\n        alive_mass = g + b\n        # If there's alive mass, compute conditional p(EOS | alive)\n        if alive_mass > 0.0:\n            p_eos_cond = (q * g + q_prime * b) / alive_mass\n            p_eos_cond_sum += p_eos_cond\n            alive_steps += 1\n        # Termination mass this step contributes to omission\n        m_eos = q * g + q_prime * b\n        omission += m_eos\n\n        # Update to next step's alive masses via non-EOS emission and transitions\n        g_next = (1.0 - q) * g * (1.0 - p_err)\n        b_next = (1.0 - q_prime) * b + (1.0 - q) * g * p_err\n\n        g, b = g_next, b_next\n\n    avg_p_eos_free = (p_eos_cond_sum / alive_steps) if alive_steps > 0 else 0.0\n    delta = avg_p_eos_free - q\n    # Clamp minor numerical drift for omission into [0,1]\n    if omission  0.0:\n        omission = 0.0\n    if omission > 1.0:\n        omission = 1.0\n    return omission, delta\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (L, q, alpha, p_err)\n    test_cases = [\n        (10, 0.02, 3.0, 0.1),\n        (10, 0.0, 3.0, 0.5),\n        (10, 0.05, 0.0, 0.3),\n        (1, 0.1, 10.0, 0.9),\n        (50, 0.01, 4.0, 0.2),\n    ]\n\n    results = []\n    for L, q, alpha, p_err in test_cases:\n        omission, delta = compute_omission_and_boost(L, q, alpha, p_err)\n        # Round to exactly 6 decimal places as per problem spec\n        omission_rounded = round(omission, 6)\n        delta_rounded = round(delta, 6)\n        results.append([omission_rounded, delta_rounded])\n\n    # Final print statement in the exact required format.\n    # Single line with Python-style list of lists.\n    print(f\"{results}\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3179290"}, {"introduction": "To combat exposure bias, techniques like Scheduled Sampling slowly introduce the model to its own predictions during the training process. This practice [@problem_id:3179283] allows you to analyze the benefits of this strategy in a synthetic environment where errors are guaranteed to cascade. You will implement a formal model of how scheduled sampling can improve model robustness and quantify its impact on the expected number of correct predictions.", "problem": "You are to design and analyze an autoregressive sequence generation process in which early prediction errors cause inevitable failure of later predictions, and to quantify how Scheduled Sampling with a decaying teacher-forcing probability mitigates this cascading failure. The analysis must be implemented as a complete, runnable program that computes specified metrics over a given test suite and prints the results in the required format.\n\nConsider an alphabet of size $K$ with tokens $\\{0,1,\\dots,K-1\\}$. Define a synthetic grammar $G$ that generates sequences of length $L$ by the rule\n$$\nx_{t+1} = (x_t + 1) \\bmod K \\quad \\text{for } t \\in \\{0,1,\\dots,L-1\\},\n$$\nwith $x_0$ drawn uniformly from $\\{0,1,\\dots,K-1\\}$. An autoregressive model learns to predict the next token given its previous outputs. At inference, the model conditions exclusively on its own previous predictions. Under this grammar, if the model makes its first error at step $s$, then for all $t  s$, the conditioning token is incorrect and the deterministic transition forces all subsequent predictions to be inconsistent with the ground-truth sequence $x_{1:L}$, i.e., failure cascades deterministically after the first error.\n\nDuring training, we compare two regimes:\n- Teacher Forcing (TF): the model is always fed the ground-truth previous token.\n- Scheduled Sampling (SS): the model is fed the ground-truth previous token with probability $\\alpha_t$ and its own previous prediction with probability $1 - \\alpha_t$, where the schedule satisfies\n$$\n\\alpha_t = \\exp(-\\beta t),\n$$\nwith $\\beta \\ge 0$.\n\nTo make the analysis numerically testable without explicit model training, adopt the following scientifically plausible abstraction, grounded in autoregressive modeling and exposure bias. Let the instantaneous error probability at step $t$ be modeled as a time-dependent quantity\n$$\np_t(\\beta) = \\min\\left\\{1, \\max\\left\\{0,\\, p_0 + \\gamma t - \\lambda\\left(1 - \\alpha_t\\right)\\right\\}\\right\\},\n$$\nwhere $p_0 \\in [0,1)$ is a base error level at $t=0$, $\\gamma \\ge 0$ models error growth due to exposure bias as time progresses, and $\\lambda \\ge 0$ quantifies mitigation strength achieved by exposure to the model’s own predictions during training. Under pure Teacher Forcing, set $\\beta = 0$ so that $\\alpha_t = 1$ and\n$$\np_t(\\text{TF}) = \\min\\left\\{1, \\max\\left\\{0,\\, p_0 + \\gamma t\\right\\}\\right\\}.\n$$\n\nUnder the cascading-failure grammar $G$, the probability that the model has not yet failed by step $t$ equals\n$$\nS_t(\\beta) = \\prod_{s=1}^{t} \\left(1 - p_s(\\beta)\\right).\n$$\nHence, the expected number of correctly generated tokens in the prefix is\n$$\n\\mathbb{E}[N_{\\text{correct}}(\\beta)] = \\sum_{t=1}^{L} S_t(\\beta),\n$$\nand the expected fraction of correct tokens is\n$$\n\\mathrm{Frac}_{\\text{correct}}(\\beta) = \\frac{1}{L}\\sum_{t=1}^{L} S_t(\\beta).\n$$\nThe probability of fully correct sequence generation (no failure across the entire length) is\n$$\n\\mathrm{SuccessProb}(\\beta) = \\prod_{t=1}^{L} \\left(1 - p_t(\\beta)\\right).\n$$\n\nYour program must:\n1. Implement the grammar $G$ for completeness, even if not used directly in the metric computation.\n2. Implement the schedule $\\alpha_t = \\exp(-\\beta t)$ and the error model $p_t(\\beta)$ described above.\n3. Compute, for each test case, the pair of improvements achieved by Scheduled Sampling over Teacher Forcing:\n   - The improvement in expected fraction of correct tokens:\n     $$\n     \\Delta \\mathrm{Frac} = \\mathrm{Frac}_{\\text{correct}}(\\beta) - \\mathrm{Frac}_{\\text{correct}}(0).\n     $$\n   - The improvement in full-sequence success probability:\n     $$\n     \\Delta \\mathrm{Success} = \\mathrm{SuccessProb}(\\beta) - \\mathrm{SuccessProb}(0).\n     $$\n   Both improvements must be reported as decimals in $[0,1]$.\n\nUse the following test suite of parameter values $(L, K, p_0, \\gamma, \\lambda, \\beta)$:\n- Case A (general case): $(20, 7, 0.02, 0.01, 0.10, 0.20)$.\n- Case B (boundary, no scheduled sampling): $(20, 7, 0.02, 0.01, 0.10, 0.00)$.\n- Case C (strong exposure bias, longer sequence): $(50, 10, 0.01, 0.02, 0.30, 0.40)$.\n- Case D (higher base error, moderate mitigation): $(30, 5, 0.20, 0.02, 0.50, 0.30)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing a comma-separated list of four items, one per test case, each item being a list of two floats $[\\Delta \\mathrm{Frac}, \\Delta \\mathrm{Success}]$. For example:\n- Output must look like\n$$\n[[\\Delta \\mathrm{Frac}_A,\\Delta \\mathrm{Success}_A],[\\Delta \\mathrm{Frac}_B,\\Delta \\mathrm{Success}_B],[\\Delta \\mathrm{Frac}_C,\\Delta \\mathrm{Success}_C],[\\Delta \\mathrm{Frac}_D,\\Delta \\mathrm{Success}_D]].\n$$\nNo other text should be printed.", "solution": "The objective of this problem is to quantify the improvement offered by Scheduled Sampling (SS) over a baseline of pure Teacher Forcing (TF) for a synthetic sequence generation task. The improvements are measured in terms of the expected fraction of correctly generated tokens and the probability of generating the entire sequence without error.\n\nThe analysis hinges on a model for the instantaneous error probability, $p_t(\\beta)$, at each time step $t \\in \\{1, 2, \\dots, L\\}$. This model captures three phenomena: a base error level $p_0$, a degradation effect over time due to exposure bias modeled by $\\gamma t$, and a mitigation effect proportional to the model's exposure to its own outputs during training, modeled by $\\lambda(1-\\alpha_t)$. The parameter $\\beta$ controls the decay of the teacher-forcing probability $\\alpha_t = \\exp(-\\beta t)$. The case $\\beta=0$ corresponds to pure Teacher Forcing, where $\\alpha_t = 1$ for all $t$.\n\nThe complete analytical procedure is as follows. The prediction time steps are indexed $t = 1, 2, \\dots, L$. The teacher-forcing schedule is given by:\n$$\n\\alpha_t = \\exp(-\\beta t)\n$$\nThe instantaneous error probability at step $t$ is defined as:\n$$\np_t(\\beta) = \\min\\left\\{1, \\max\\left\\{0, p_0 + \\gamma t - \\lambda\\left(1 - \\exp(-\\beta t)\\right)\\right\\}\\right\\}\n$$\n\nGiven the cascading-failure property of the grammar $G$, the model generates a correct prefix up to step $t$ only if it has not made any errors at any prior step $s \\in \\{1, \\dots, t\\}$. The probability of this event, the survival probability, is:\n$$\nS_t(\\beta) = \\prod_{s=1}^{t} \\left(1 - p_s(\\beta)\\right)\n$$\n\nFrom the survival probability, we derive the two primary performance metrics:\n1.  The probability of successfully generating the entire sequence of length $L$, which is the survival probability at the final step, $t=L$:\n    $$\n    \\mathrm{SuccessProb}(\\beta) = S_L(\\beta) = \\prod_{t=1}^{L} \\left(1 - p_t(\\beta)\\right)\n    $$\n2.  The expected fraction of correctly generated tokens. The expected number of correct tokens is $\\sum_{t=1}^{L} S_t(\\beta)$, so the expected fraction is:\n    $$\n    \\mathrm{Frac}_{\\text{correct}}(\\beta) = \\frac{1}{L}\\sum_{t=1}^{L} S_t(\\beta)\n    $$\n\nTo quantify the improvement, we compute these metrics for the baseline Teacher Forcing (TF) case ($\\beta=0$). The error probability under TF is:\n$$\np_t(0) = \\min\\left\\{1, \\max\\left\\{0, p_0 + \\gamma t\\right\\}\\right\\}\n$$\nThe corresponding TF metrics, $\\mathrm{Frac}_{\\text{correct}}(0)$ and $\\mathrm{SuccessProb}(0)$, are calculated using $p_t(0)$.\n\nFinally, the improvements due to Scheduled Sampling are the differences:\n$$\n\\Delta \\mathrm{Frac} = \\mathrm{Frac}_{\\text{correct}}(\\beta) - \\mathrm{Frac}_{\\text{correct}}(0)\n$$\n$$\n\\Delta \\mathrm{Success} = \\mathrm{SuccessProb}(\\beta) - \\mathrm{SuccessProb}(0)\n$$\n\nThe implementation will systematically compute these values for each test case. The grammar $G$ and alphabet size $K$ inform the problem's conceptual setup but do not enter the numerical computation, which is based on the abstract error model.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n\n    def generate_sequence_from_grammar(K, L, x0):\n        \"\"\"\n        Implements the grammar G to generate a sequence of length L.\n        The rule is x_{t+1} = (x_t + 1) mod K, starting from x_0.\n        This function is included for completeness as required by the problem statement,\n        but it is not used in the main metric computation.\n        \n        Args:\n            K (int): The size of the alphabet {0, 1, ..., K-1}.\n            L (int): The length of the sequence to generate (x_1, ..., x_L).\n            x0 (int): The initial token x_0.\n        \n        Returns:\n            list: The generated sequence [x_1, x_2, ..., x_L].\n        \"\"\"\n        if not (isinstance(K, int) and K > 0 and \n                isinstance(L, int) and L > 0 and \n                isinstance(x0, int) and 0 = x0  K):\n            raise ValueError(\"Invalid arguments for grammar sequence generation.\")\n        \n        sequence = []\n        current_x = x0\n        for _ in range(L):\n            current_x = (current_x + 1) % K\n            sequence.append(current_x)\n        return sequence\n\n    def calculate_metrics(L, p0, gamma, lambda_param, beta):\n        \"\"\"\n        Computes the performance metrics for a given set of parameters.\n        \n        Args:\n            L (int): Sequence length.\n            p0 (float): Base error probability.\n            gamma (float): Error growth rate.\n            lambda_param (float): Mitigation strength.\n            beta (float): Scheduled sampling decay rate.\n            \n        Returns:\n            tuple: A tuple containing (Frac_correct, SuccessProb).\n        \"\"\"\n        t = np.arange(1, L + 1)\n\n        # Scheduled Sampling (SS) case\n        if beta > 0:\n            alpha_t = np.exp(-beta * t)\n            p_t_ss = np.clip(p0 + gamma * t - lambda_param * (1 - alpha_t), 0, 1)\n        # Teacher Forcing (TF) case can be computed by setting beta = 0\n        else:\n            p_t_ss = np.clip(p0 + gamma * t, 0, 1)\n\n        one_minus_p_t_ss = 1 - p_t_ss\n        S_t_ss = np.cumprod(one_minus_p_t_ss)\n        \n        frac_correct_ss = np.sum(S_t_ss) / L\n        success_prob_ss = S_t_ss[-1]\n        \n        return frac_correct_ss, success_prob_ss\n\n    # Test suite of parameter values (L, K, p0, gamma, lambda, beta)\n    test_cases = [\n        # Case A (general case)\n        (20, 7, 0.02, 0.01, 0.10, 0.20),\n        # Case B (boundary, no scheduled sampling)\n        (20, 7, 0.02, 0.01, 0.10, 0.00),\n        # Case C (strong exposure bias, longer sequence)\n        (50, 10, 0.01, 0.02, 0.30, 0.40),\n        # Case D (higher base error, moderate mitigation)\n        (30, 5, 0.20, 0.02, 0.50, 0.30),\n    ]\n\n    results = []\n    for case in test_cases:\n        L, K, p0, gamma, lambda_param, beta = case\n\n        # Calculate metrics for the Scheduled Sampling (SS) case with the given beta\n        frac_correct_ss, success_prob_ss = calculate_metrics(L, p0, gamma, lambda_param, beta)\n\n        # Calculate metrics for the Teacher Forcing (TF) case (beta = 0)\n        frac_correct_tf, success_prob_tf = calculate_metrics(L, p0, gamma, lambda_param, 0)\n        \n        # Calculate the improvements\n        delta_frac = frac_correct_ss - frac_correct_tf\n        delta_success = success_prob_ss - success_prob_tf\n        \n        results.append([delta_frac, delta_success])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3179283"}]}