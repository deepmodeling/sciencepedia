{"hands_on_practices": [{"introduction": "To truly understand how an LSTM maintains memory, we can start by analyzing its behavior in a simplified, controlled setting. This first practice invites you to derive a closed-form solution for the cell state, assuming the gates are constant over time [@problem_id:3188449]. This mathematical exercise is more than just algebra; it uncovers the core mechanism of the LSTM, revealing its direct connection to the concept of an exponentially weighted moving average and providing a clear intuition for how the forget gate shapes the influence of past information.", "problem": "Consider a single scalar unit of a Long Short-Term Memory (LSTM) cell. By definition, the cell state update at time step $t$ is given by $c_t = f_t c_{t-1} + i_t \\tilde{c}_t$, where $f_t$ is the forget gate value, $i_t$ is the input gate value, and $\\tilde{c}_t$ is the candidate cell state, each produced by the corresponding gating and candidate mechanisms (for example, logistic sigmoid for gates and hyperbolic tangent for the candidate). Assume that the gates are time-invariant with $f_t \\equiv f$ and $i_t \\equiv i$, where $0 < f < 1$ and $0 < i \\leq 1$, and that the initial cell state $c_0$ is given. The sequence $\\{\\tilde{c}_k\\}_{k=1}^t$ of candidate cell states is arbitrary but bounded and known.\n\nStarting from the core update definition above and without introducing any additional shortcut formulas, derive a closed-form analytic expression for $c_t$ in terms of $f$, $i$, $c_0$, and the sequence $\\tilde{c}_1, \\tilde{c}_2, \\dots, \\tilde{c}_t$. Then, interpret the resulting expression as a form of exponential smoothing applied to the sequence of candidates, identifying the weight assigned to each $\\tilde{c}_k$ at time $t$ and explaining the role of the initial condition $c_0$ in this smoothing view.\n\nYour final answer must be the single closed-form analytic expression you derive for $c_t$. No numerical approximation is required.", "solution": "The problem is valid as it is scientifically grounded in the principles of recurrent neural networks, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. We may proceed with the derivation.\n\nThe starting point is the core update definition for the LSTM cell state, $c_t$, at time step $t$. Under the given assumptions of time-invariant gates, the recurrence relation is:\n$$c_t = f c_{t-1} + i \\tilde{c}_t$$\nwhere $f$ is the constant forget gate value, $i$ is the constant input gate value, $c_{t-1}$ is the cell state at the previous time step, and $\\tilde{c}_t$ is the candidate cell state at the current time step. The initial cell state is $c_0$.\n\nOur objective is to derive a closed-form analytic expression for $c_t$ by unrolling this recurrence relation. Let us expand the expression for the first few time steps to identify a pattern.\n\nFor $t=1$:\n$$c_1 = f c_0 + i \\tilde{c}_1$$\n\nFor $t=2$:\n$$c_2 = f c_1 + i \\tilde{c}_2$$\nSubstituting the expression for $c_1$:\n$$c_2 = f(f c_0 + i \\tilde{c}_1) + i \\tilde{c}_2 = f^2 c_0 + f i \\tilde{c}_1 + i \\tilde{c}_2$$\n\nFor $t=3$:\n$$c_3 = f c_2 + i \\tilde{c}_3$$\nSubstituting the expression for $c_2$:\n$$c_3 = f(f^2 c_0 + f i \\tilde{c}_1 + i \\tilde{c}_2) + i \\tilde{c}_3 = f^3 c_0 + f^2 i \\tilde{c}_1 + f i \\tilde{c}_2 + i \\tilde{c}_3$$\n\nFrom these expansions, a general pattern emerges. The expression for $c_t$ appears to be composed of two parts: a term involving the initial state $c_0$ and a summation over the sequence of candidate states $\\{\\tilde{c}_k\\}$. We hypothesize the following closed-form expression:\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$$\n\nTo rigorously establish this formula, we use proof by mathematical induction on the time step $t$.\n\n**Base Case:** For $t=1$, the hypothesized formula yields:\n$$c_1 = f^1 c_0 + i \\sum_{k=1}^{1} f^{1-k} \\tilde{c}_k = f c_0 + i (f^{1-1} \\tilde{c}_1) = f c_0 + i f^0 \\tilde{c}_1 = f c_0 + i \\tilde{c}_1$$\nThis result matches the expression derived directly from the recurrence relation for $t=1$. The base case holds.\n\n**Inductive Step:** Assume the formula is true for an arbitrary time step $t-1$, where $t > 1$. This is our inductive hypothesis:\n$$c_{t-1} = f^{t-1} c_0 + i \\sum_{k=1}^{t-1} f^{(t-1)-k} \\tilde{c}_k$$\nWe must show that the formula also holds for time step $t$. We start with the recurrence relation for $c_t$:\n$$c_t = f c_{t-1} + i \\tilde{c}_t$$\nSubstituting the inductive hypothesis for $c_{t-1}$:\n$$c_t = f \\left( f^{t-1} c_0 + i \\sum_{k=1}^{t-1} f^{t-1-k} \\tilde{c}_k \\right) + i \\tilde{c}_t$$\nDistributing the factor $f$:\n$$c_t = f \\cdot f^{t-1} c_0 + f \\cdot i \\sum_{k=1}^{t-1} f^{t-1-k} \\tilde{c}_k + i \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t-1} f^{1 + (t-1-k)} \\tilde{c}_k + i \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t-1} f^{t-k} \\tilde{c}_k + i \\tilde{c}_t$$\nThe last term, $i\\tilde{c}_t$, can be written as $i f^0 \\tilde{c}_t = i f^{t-t} \\tilde{c}_t$. This allows us to incorporate it into the summation, extending the upper limit from $t-1$ to $t$:\n$$c_t = f^t c_0 + \\left( i \\sum_{k=1}^{t-1} f^{t-k} \\tilde{c}_k \\right) + i f^{t-t} \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$$\nThis is precisely the hypothesized formula for time step $t$. Thus, by the principle of mathematical induction, the closed-form expression is valid for all $t \\ge 1$.\n\n**Interpretation as Exponential Smoothing:**\nThe derived expression, $c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$, can be interpreted as a form of exponential smoothing.\n\nThe expression for $c_t$ is a sum of two components:\n$1$. The initial state contribution: $f^t c_0$.\n$2$. The weighted sum of candidate states: $i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$.\n\nThe first term, $f^t c_0$, represents the persisting influence of the initial cell state $c_0$. Given the constraint $0 < f < 1$, the factor $f^t$ decays exponentially towards zero as $t \\to \\infty$. This demonstrates the \"forgetting\" mechanism: the memory of the initial state gradually vanishes over time.\n\nThe second term is a weighted sum of the sequence of candidate states from $\\tilde{c}_1$ to $\\tilde{c}_t$. The weight assigned to each candidate state $\\tilde{c}_k$ (where $1 \\le k \\le t$) is $w_k = i f^{t-k}$. Let us examine these weights:\n- For the most recent candidate $\\tilde{c}_t$, the weight is $w_t = i f^{t-t} = i$.\n- For the previous candidate $\\tilde{c}_{t-1}$, the weight is $w_{t-1} = i f^{t-(t-1)} = i f$.\n- For a past candidate $\\tilde{c}_{t-j}$, the weight is $w_{t-j} = i f^{j}$.\n\nSince $0 < f < 1$, the weights $w_k$ decrease geometrically as we move further into the past (i.e., as $k$ decreases, $t-k$ increases, and $f^{t-k}$ decreases). This unequal weighting, where recent inputs receive exponentially more weight than older inputs, is the defining characteristic of an exponentially weighted moving average, also known as exponential smoothing. The forget gate value $f$ acts as the decay rate or smoothing factor.\n\nIn summary, the role of the initial condition $c_0$ is to provide a starting value for this smoothing process. Its influence is weighted by $f^t$ and decays exponentially. The cell state $c_t$ is thus a combination of the fading memory of its initial state and an exponentially smoothed average of all candidate inputs encountered up to time $t$. The weight assigned to a specific candidate $\\tilde{c}_k$ at time $t$ is precisely $i f^{t-k}$. The input gate value $i$ acts as a scaling factor for the influence of all new information.", "answer": "$$\\boxed{c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k}$$", "id": "3188449"}, {"introduction": "Building on the concept of exponential memory decay, how can we quantify the \"forgetfulness\" of an LSTM in a practical scenario? This exercise challenges you to derive and compute an \"effective memory half-life\" based on a sequence of forget gate activations [@problem_id:3188446]. By connecting this metric to the initial bias of the forget gate, you will gain hands-on experience with one of the most important heuristics for training LSTMs to handle long-term dependencies.", "problem": "You are asked to formalize and compute an effective memory measure for the Long Short-Term Memory (LSTM) cell forget gate. Consider an LSTM (Long Short-Term Memory) cell whose cell state update is defined fundamentally by a multiplicative retention mechanism. Let the forget gate activation at discrete time step $t$ be $f_t \\in (0,1)$, obtained from a pre-activation $z_t$ via the logistic sigmoid $\\sigma$, namely $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The cell state $c_t$ evolves according to a retention-only recurrence $c_t = f_t \\cdot c_{t-1}$ when no new input is injected (this is the fundamental definition of multiplicative retention in LSTM cells). Over $k$ steps, the retained fraction of the initial state $c_0$ is the product $\\prod_{t=1}^{k} f_t$.\n\nDefine the effective memory half-life $h$ (in discrete steps) as the unique positive real $k$ for which the expected retained fraction equals $0.5$, under the assumption that the empirical statistics of $\\{f_t\\}$ are representative and stationary enough to approximate the expectation by a time average. Use only the preceding fundamental definitions to derive a computable expression for $h$ in terms of empirical statistics of $\\{f_t\\}$, without introducing any extraneous assumptions unrelated to the multiplicative retention mechanism.\n\nYou will compare the effect of the forget-gate bias initialization $b_f \\in \\{0,1,2\\}$ on the half-life, using a fixed time series of baseline pre-activations $\\{s_t\\}$, and forming $z_t = b_f + s_t$, then $f_t = \\sigma(z_t)$.\n\nFundamental base to use:\n- The LSTM retention-only update $c_t = f_t \\cdot c_{t-1}$.\n- Multiplicativity of retention over steps, $\\prod_{t=1}^k f_t$.\n- The logistic sigmoid $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n- Laws of logarithms for products.\n\nYour task:\n1. For each $b_f \\in \\{0,1,2\\}$, compute $z_t = b_f + s_t$ and $f_t = \\sigma(z_t)$ for $t = 1, \\dots, 20$ using the test suite below.\n2. Using only the preceding fundamental base, derive a computable estimator for the effective memory half-life $h$ in steps based on the empirical statistics of $\\{f_t\\}_{t=1}^{20}$.\n3. Implement a program that computes $h$ for each $b_f \\in \\{0,1,2\\}$ from the supplied data and outputs the three values in order $b_f = 0$, $b_f = 1$, $b_f = 2$.\n4. Round each reported $h$ to $6$ decimal places. If the computation implies a non-decaying or increasing retention in expectation (i.e., a non-negative empirical average decay rate), output $+\\infty$ for that case as the literal $inf$.\n\nTest suite:\n- Use the fixed baseline pre-activations sequence of length $20$:\n$$\n\\{s_t\\}_{t=1}^{20} = [-1.5,\\ 0.0,\\ 1.0,\\ -0.5,\\ 3.0,\\ -3.0,\\ 0.5,\\ 2.0,\\ -2.0,\\ 4.0,\\ -4.0,\\ 1.5,\\ -1.0,\\ 0.25,\\ -0.25,\\ 2.5,\\ -2.5,\\ 0.75,\\ -0.75,\\ 5.0].\n$$\n- For each $b_f \\in \\{0,1,2\\}$, form $z_t = b_f + s_t$ and $f_t = \\sigma(z_t)$, and compute the corresponding effective half-life $h$.\n\nAngle units are not applicable. No physical units are involved. All outputs are pure numbers in units of discrete steps.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[h_0,h_1,h_2]$), where $h_0$ corresponds to $b_f = 0$, $h_1$ to $b_f = 1$, and $h_2$ to $b_f = 2$. Each $h$ must be rounded to $6$ decimal places as specified.", "solution": "The user wants to compute an effective memory half-life for an LSTM forget gate under a simplified retention-only model.\n\n### Step 1: Extract Givens\n\n-   **LSTM Cell State Update (Retention-Only):** $c_t = f_t \\cdot c_{t-1}$, where $c_t$ is the cell state at time $t$.\n-   **Forget Gate Activation:** $f_t = \\sigma(z_t)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid function and $f_t \\in (0,1)$.\n-   **Pre-activation:** $z_t = b_f + s_t$, where $b_f$ is a bias term.\n-   **Retained Fraction over $k$ steps:** $R_k = \\prod_{i=1}^{k} f_i$.\n-   **Effective Memory Half-Life ($h$):** The unique positive real $k$ for which the expected retained fraction equals $0.5$. The expectation is to be approximated by a time average based on the empirical statistics of $\\{f_t\\}$.\n-   **Fundamental Base:** Use only the given definitions and the laws of logarithms.\n-   **Forget-gate bias values:** $b_f \\in \\{0, 1, 2\\}$.\n-   **Baseline pre-activations sequence ($N=20$):** $\\{s_t\\}_{t=1}^{20} = [-1.5, 0.0, 1.0, -0.5, 3.0, -3.0, 0.5, 2.0, -2.0, 4.0, -4.0, 1.5, -1.0, 0.25, -0.25, 2.5, -2.5, 0.75, -0.75, 5.0]$.\n-   **Special Condition:** If the empirical average decay rate is non-positive (implying non-decaying retention), the half-life $h$ is to be reported as $+\\infty$ (literal `inf`).\n-   **Rounding:** Final values for $h$ must be rounded to $6$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is based on the fundamental mathematical formulation of an LSTM cell, a core concept in deep learning. The model is a simplification but is mathematically and conceptually sound for analyzing the role of the forget gate. It is free of pseudoscience. (Valid)\n-   **Well-Posed:** The problem provides all necessary data ($s_t$, $b_f$), definitions ($\\sigma(z)$, $c_t$ recurrence), and a clear objective (compute $h$). The path to a solution is derivable from the given principles. (Valid)\n-   **Objective:** The problem is stated using precise mathematical language and definitions. It is free of subjectivity or opinion. (Valid)\n-   **Completeness:** The problem is self-contained. All constants, data sequences, and formulas are provided. (Valid)\n-   **Consistency:** The provided definitions and constraints are consistent with one another. (Valid)\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived and implemented.\n\n### Derivation of the Half-Life Estimator\n\nThe objective is to find the half-life $h$, defined as the number of time steps $k$ at which the expected retained fraction of the memory cell state is $0.5$. The retained fraction after $k$ steps is given by the product $R_k = \\prod_{i=1}^{k} f_i$.\n\nThe core of the problem lies in interpreting \"expected retained fraction\" under the directive to use \"empirical statistics\" and \"time average\". A multiplicative process such as this is most effectively analyzed in the logarithmic domain, a path suggested by the \"Laws of logarithms for products\" hint.\n\n1.  We model the retention process using an effective, constant forget factor per step, $f_{\\text{eff}}$. This $f_{\\text{eff}}$ should be representative of the sequence $\\{f_t\\}_{t=1}^{N}$. For a multiplicative sequence, the most appropriate measure of central tendency is the geometric mean.\n    $$\n    f_{\\text{eff}} = \\left( \\prod_{t=1}^{N} f_t \\right)^{1/N}\n    $$\n    where $N$ is the length of the sample sequence, in this case $N=20$.\n\n2.  With this effective factor, the retained fraction after $h$ steps is modeled as $(f_{\\text{eff}})^h$. The half-life $h$ is defined by the condition that this fraction equals $0.5$:\n    $$\n    (f_{\\text{eff}})^h = 0.5\n    $$\n\n3.  To solve for $h$, we take the natural logarithm of both sides of the equation:\n    $$\n    \\ln\\left((f_{\\text{eff}})^h\\right) = \\ln(0.5)\n    $$\n    $$\n    h \\cdot \\ln(f_{\\text{eff}}) = \\ln(0.5)\n    $$\n\n4.  Solving for $h$ gives:\n    $$\n    h = \\frac{\\ln(0.5)}{\\ln(f_{\\text{eff}})}\n    $$\n\n5.  The term $\\ln(f_{\\text{eff}})$ can be expressed using the laws of logarithms. It is the empirical average of the logarithm of the forget gate activations:\n    $$\n    \\ln(f_{\\text{eff}}) = \\ln\\left( \\left( \\prod_{t=1}^{N} f_t \\right)^{1/N} \\right) = \\frac{1}{N} \\ln\\left( \\prod_{t=1}^{N} f_t \\right) = \\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t)\n    $$\n    This quantity, let's call it $\\lambda_{\\log} = \\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t)$, represents the average logarithmic change per step.\n\n6.  Substituting this back into the expression for $h$, we get the final computable formula:\n    $$\n    h = \\frac{\\ln(0.5)}{\\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t)}\n    $$\n\n7.  The problem states that if the \"empirical average decay rate\" is non-positive, we should output `inf`. The decay rate in the log domain is $-\\lambda_{\\log} = -(\\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t))$. A non-positive decay rate means $-\\lambda_{\\log} \\le 0$, which is equivalent to $\\lambda_{\\log} \\ge 0$. In this case, the denominator of the expression for $h$ would be non-negative. Since $\\ln(0.5) < 0$, a non-negative denominator means $h$ would be negative or infinite. As $h$ must be a positive real number, we conclude that if $\\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t) \\ge 0$, the half-life is infinite, $h = +\\infty$.\n\n### Computational Steps\n\nFor each bias $b_f \\in \\{0, 1, 2\\}$:\n1.  Compute the pre-activation sequence $z_t = b_f + s_t$ for $t=1, \\dots, 20$.\n2.  Compute the forget gate activation sequence $f_t = \\sigma(z_t) = \\frac{1}{1 + e^{-z_t}}$.\n3.  Compute the log-activations $\\ln(f_t)$.\n4.  Calculate the average log-activation $\\lambda_{\\log} = \\frac{1}{20} \\sum_{t=1}^{20} \\ln(f_t)$.\n5.  If $\\lambda_{\\log} \\ge 0$, the result is `inf`.\n6.  Otherwise, compute $h = \\frac{\\ln(0.5)}{\\lambda_{\\log}}$.\n7.  Round the finite result to $6$ decimal places.\n8.  Collect the results for $b_f=0, 1, 2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective memory half-life of an LSTM's forget gate\n    for different bias initializations.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Baseline pre-activations sequence of length 20\n    s_t = np.array([\n        -1.5, 0.0, 1.0, -0.5, 3.0, -3.0, 0.5, 2.0, -2.0, 4.0, \n        -4.0, 1.5, -1.0, 0.25, -0.25, 2.5, -2.5, 0.75, -0.75, 5.0\n    ])\n\n    # Forget-gate bias values to test\n    biases = [0, 1, 2]\n    \n    # Sigmoid function\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    results = []\n    \n    # Loop through each bias value\n    for b_f in biases:\n        # Step 1: Compute pre-activations z_t\n        z_t = b_f + s_t\n        \n        # Step 2: Compute forget gate activations f_t\n        f_t = sigmoid(z_t)\n        \n        # Step 3: Compute log-activations\n        log_f_t = np.log(f_t)\n        \n        # Step 4: Calculate the average log-activation\n        avg_log_f = np.mean(log_f_t)\n        \n        # Step 5: Check for non-decaying case\n        if avg_log_f >= 0:\n            h = 'inf'\n        else:\n            # Step 6: Compute the half-life h\n            h_val = np.log(0.5) / avg_log_f\n            \n            # Step 7: Round to 6 decimal places\n            h = round(h_val, 6)\n            \n        results.append(h)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3188446"}, {"introduction": "An LSTM's power lies not only in storing information but also in controlling when to reveal it. This final practice delves into the critical distinction between the internal cell state ($c_t$) and the external hidden state ($h_t$), which is modulated by the output gate [@problem_id:3188501]. Through a conceptual analysis, you will explore how an LSTM can maintain a stable long-term memory internally while its observable output can be dynamically adjusted, a key feature for tackling complex sequential tasks.", "problem": "Consider the standard Long Short-Term Memory (LSTM) cell, where the cell state is denoted by $c_t$ and the hidden state by $h_t$. The gates are the forget gate $f_t$, the input gate $i_t$, and the output gate $o_t$. Each gate takes values in $(0,1)$ via a sigmoid nonlinearity, and the candidate update (sometimes called the input modulation) is produced by a bounded nonlinearity such as the hyperbolic tangent, yielding values in $[-1,1]$. Assume inputs are bounded and parameters are fixed. Focus on a controlled regime over a contiguous block of steps $t \\in \\{t_0, t_0+1, \\dots, t_0+k\\}$ where $f_t \\to 1$ and $i_t \\to 0$, while $o_t$ may vary over $(0,1)$. In this regime, analyze what must hold about the retention of information in $c_t$ versus its observability through $h_t$, and propose methods, if any, to recover the latent content of $c_t$ when $o_t$ is small.\n\nSelect all statements that are necessarily correct in this regime.\n\nA. In the limit $f_t \\to 1$ and $i_t \\to 0$, the cell state $c_t$ remains approximately constant across the block, effectively retaining prior information, while the observable hidden state $h_t$ becomes directly controlled by $o_t$ through $h_t \\approx o_t \\,\\tanh(c_t)$; decreasing $o_t$ reduces the externally visible memory without erasing $c_t$.\n\nB. If $o_t = 0$ for all steps in the block, the latent $c_t$ can still be exactly recovered at those times by computing $\\operatorname{arctanh}(h_t)$ from the hidden state.\n\nC. Practical probes to recover the latent cell content include: temporarily setting $o_t$ to $1$ for a single step while keeping $i_t \\approx 0$ and $f_t \\approx 1$ to read out $\\tanh(c_t)$ without altering $c_t$, or, if peephole connections are present, using gate pre-activations that depend on $c_t$ to infer $c_t$.\n\nD. In the limit $f_t \\to 1$ and $i_t \\to 0$, the cell state $c_t$ necessarily decays to $0$ over time because the hyperbolic tangent saturates.\n\nE. The sensitivity of the hidden state $h_t$ to changes in $c_t$ is proportional to $o_t$ and the derivative of the hyperbolic tangent at $c_t$, implying that when $o_t$ is small, gradients from $h_t$ to $c_t$ are attenuated; therefore, increasing $o_t$ to $1$ for a single step while maintaining $i_t \\approx 0$ and $f_t \\approx 1$ is a principled way to expose $c_t$ without modifying it, enabling probing via direct observation or via Backpropagation Through Time (BPTT).", "solution": "This problem requires an analysis of the LSTM equations in a specific operational regime. The core LSTM equations are:\n$$\n\\begin{aligned}\nc_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\nh_t &= o_t \\odot \\tanh(c_t)\n\\end{aligned}\n$$\nThe problem specifies a regime where the forget gate activation $f_t \\to 1$ and the input gate activation $i_t \\to 0$.\n\n**Analysis of the Regime**\nIn this regime, the cell state update equation simplifies to:\n$$c_t \\approx (1) \\odot c_{t-1} + (0) \\odot \\tilde{c}_t \\approx c_{t-1}$$\nThis demonstrates that the cell state $c_t$ remains approximately constant, acting as a memory register that preserves its value over time. The hidden state equation, $h_t = o_t \\odot \\tanh(c_t)$, shows that the observable output $h_t$ is a dynamically scaled version of the transformed, stable cell state. The output gate $o_t$ directly controls this scaling.\n\nWe will now evaluate each option based on this understanding.\n\n**A. In the limit $f_t \\to 1$ and $i_t \\to 0$, the cell state $c_t$ remains approximately constant across the block, effectively retaining prior information, while the observable hidden state $h_t$ becomes directly controlled by $o_t$ through $h_t \\approx o_t \\,\\tanh(c_t)$; decreasing $o_t$ reduces the externally visible memory without erasing $c_t$.**\n*   **Analysis:** This statement is a direct and accurate consequence of our analysis. The cell state $c_t$ is stable ($c_t \\approx c_{t-1}$), and the hidden state $h_t$ is a product of the output gate $o_t$ and the stable value $\\tanh(c_t)$. Reducing $o_t$ scales down $h_t$ but does not affect the update rule for $c_t$, so the internal memory is not erased.\n*   **Verdict: Correct.**\n\n**B. If $o_t = 0$ for all steps in the block, the latent $c_t$ can still be exactly recovered at those times by computing $\\operatorname{arctanh}(h_t)$ from the hidden state.**\n*   **Analysis:** If $o_t = 0$, then $h_t = 0 \\odot \\tanh(c_t) = 0$. Attempting to recover $c_t$ by computing $\\operatorname{arctanh}(h_t)$ yields $\\operatorname{arctanh}(0) = 0$. This would incorrectly imply $c_t=0$, regardless of its actual stored value. Information about $c_t$ is completely masked when $o_t=0$.\n*   **Verdict: Incorrect.**\n\n**C. Practical probes to recover the latent cell content include: temporarily setting $o_t$ to $1$ for a single step while keeping $i_t \\approx 0$ and $f_t \\approx 1$ to read out $\\tanh(c_t)$ without altering $c_t$, or, if peephole connections are present, using gate pre-activations that depend on $c_t$ to infer $c_t$.**\n*   **Analysis:** This describes valid methods for probing the internal state. Setting $o_t=1$ makes $h_t = \\tanh(c_t)$, directly exposing the transformed cell state. Since $i_t$ and $f_t$ are maintained in the memory-preserving regime, this probing does not corrupt the memory. Peephole connections, where gates are functions of the cell state (e.g., $o_t = \\sigma(W_o x_t + U_o h_{t-1} + V_o \\odot c_t + b_o)$), mean that gate activations carry information about $c_t$, which could be used for inference.\n*   **Verdict: Correct.**\n\n**D. In the limit $f_t \\to 1$ and $i_t \\to 0$, the cell state $c_t$ necessarily decays to $0$ over time because the hyperbolic tangent saturates.**\n*   **Analysis:** This statement is fundamentally flawed. The $\\tanh$ function is not applied to $c_t$ in its own update loop. The cell state update is $c_t \\approx c_{t-1}$. The cell state's value is preserved, not driven to zero. The saturation of $\\tanh$ affects the hidden state $h_t$ and the candidate state $\\tilde{c}_t$, not the primary cell state recurrence itself.\n*   **Verdict: Incorrect.**\n\n**E. The sensitivity of the hidden state $h_t$ to changes in $c_t$ is proportional to $o_t$ and the derivative of the hyperbolic tangent at $c_t$, implying that when $o_t$ is small, gradients from $h_t$ to $c_t$ are attenuated; therefore, increasing $o_t$ to $1$ for a single step while maintaining $i_t \\approx 0$ and $f_t \\approx 1$ is a principled way to expose $c_t$ without modifying it, enabling probing via direct observation or via Backpropagation Through Time (BPTT).**\n*   **Analysis:** The sensitivity is the gradient $\\frac{\\partial h_t}{\\partial c_t}$. For scalar values, this is $\\frac{\\partial}{\\partial c_t} (o_t \\tanh(c_t)) = o_t \\cdot \\text{sech}^2(c_t)$. This gradient is directly proportional to $o_t$. A small $o_t$ attenuates the gradient flowing from $h_t$ back to $c_t$ during BPTT. Setting $o_t=1$ maximizes this gradient, allowing the value of $c_t$ to have maximum influence on the output and any associated loss function, thus \"exposing\" it to the learning process without altering its stored value.\n*   **Verdict: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "3188501"}]}