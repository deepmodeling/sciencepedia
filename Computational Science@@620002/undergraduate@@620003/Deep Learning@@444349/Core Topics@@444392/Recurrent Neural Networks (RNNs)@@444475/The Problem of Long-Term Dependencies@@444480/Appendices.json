{"hands_on_practices": [{"introduction": "Long Short-Term Memory (LSTM) networks were specifically designed to combat the vanishing gradient problem that plagues simple Recurrent Neural Networks (RNNs). The key innovation is the gating mechanism, particularly the \"forget gate,\" which adaptively controls the flow of information through time. This exercise provides a focused, quantitative exploration of this critical component. By simplifying the LSTM dynamics to isolate the forget gate, you will derive and observe how its initial bias setting directly governs the rate of memory decay, making the abstract concept of long-term dependency tangible and controllable. [@problem_id:3191179]", "problem": "Consider a single-cell Long Short-Term Memory (LSTM) recurrence initialized to study the effect of the forget-gate bias on retaining information over long sequences. The standard cell update is defined by the following well-tested equations: the cell state $c_t$ evolves as $c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$, where the forget gate $f_t = \\sigma(z_f(t))$, the input gate $i_t = \\sigma(z_i(t))$, and the candidate update $g_t = \\tanh(z_g(t))$. The logistic sigmoid function is $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$, and the hyperbolic tangent is $\\tanh(x)$. At random initialization with zero-mean weights, one commonly controls the initial tendency of the forget gate to retain information by setting the forget-gate bias $b_f$.\n\nWe define a controlled, purely mathematical variant of the initialization regime to isolate retention dynamics:\n- For all time steps $t \\geq 1$, the pre-activations for the gates and candidate satisfy $z_f(t) = b_f$, $z_i(t) = 0$, and $z_g(t) = 0$, which implies $f_t = \\sigma(b_f)$, $i_t = \\sigma(0)$, and $g_t = \\tanh(0)$.\n- At time $t = 0$, a one-time write sets an initial memory $c_0$ with $c_0 = 1$ (dimensionless), and for $t \\geq 1$ there are no further writes, so $i_t \\cdot g_t = 0$ for $t \\geq 1$.\n\nStarting only from the above definitions and facts, derive the retention after $L$ steps, defined as the ratio $R(L, b_f) = \\dfrac{c_L}{c_0}$, and implement a program that computes this retention for a sweep of forget-gate bias values $b_f$ and dependency lengths $L$.\n\nTest Suite:\n- Forget-gate bias values $b_f \\in \\{-3, -1, 0, 1, 3\\}$, i.e., $b_f$ equals $-3$, $-1$, $0$, $1$, and $3$.\n- Dependency lengths $L \\in \\{0, 5, 20, 100, 200\\}$, i.e., $L$ equals $0$, $5$, $20$, $100$, and $200$.\n\nFor each ordered pair $(b_f, L)$, with $b_f$ iterated in ascending order followed by $L$ in ascending order (lexicographic order on the pair), compute the retention $R(L, b_f)$ as a real number and report it as a float rounded to $6$ decimal places. There are no physical units in this problem; all quantities are dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact specified order of pairs: first $b_f = -3$ with all $L$, then $b_f = -1$ with all $L$, then $b_f = 0$ with all $L$, then $b_f = 1$ with all $L$, then $b_f = 3$ with all $L$. For example, the output format must be like `[r_1,r_2,...,r_25]` where each $r_k$ is the computed float for the corresponding test case. The answers for each test case are floats.", "solution": "The user wants me to solve a problem about LSTM cells.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n#### Step 1.1: Extract Givens\n\nThe givens are:\n- LSTM cell state update equation: $c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$\n- Forget gate activation: $f_t = \\sigma(z_f(t))$\n- Input gate activation: $i_t = \\sigma(z_i(t))$\n- Candidate update: $g_t = \\tanh(z_g(t))$\n- Logistic sigmoid function: $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n- Hyperbolic tangent function: $\\tanh(x)$\n- Controlled model conditions for all time steps $t \\geq 1$:\n    - Pre-activation for forget gate: $z_f(t) = b_f$\n    - Pre-activation for input gate: $z_i(t) = 0$\n    - Pre-activation for candidate update: $z_g(t) = 0$\n- Condition on writes for $t \\geq 1$: $i_t \\cdot g_t = 0$\n- Initial condition at $t = 0$: $c_0 = 1$\n- Definition of retention: $R(L, b_f) = \\dfrac{c_L}{c_0}$\n- Test suite parameters:\n    - Forget-gate bias values $b_f \\in \\{-3, -1, 0, 1, 3\\}$\n    - Dependency lengths $L \\in \\{0, 5, 20, 100, 200\\}$\n- Output format: A comma-separated list of float values rounded to $6$ decimal places, for each pair $(b_f, L)$ in lexicographic order.\n\n#### Step 1.2: Validate Using Extracted Givens\n\nThe problem is subjected to validation against the established criteria.\n\n1.  **Scientific or Factual Soundness**: The problem is scientifically grounded. It uses the standard, universally accepted equations for a Long Short-Term Memory (LSTM) cell. The introduced simplifications ($z_f(t) = b_f$, $z_i(t) = 0$, $z_g(t) = 0$) are a well-defined mathematical construct designed to isolate and study the effect of the forget gate's bias on memory retention. This is a common and valid technique in scientific modeling to understand complex systems by analyzing simplified cases. The statement that for $t \\geq 1$, $i_t \\cdot g_t = 0$ is consistent with the given pre-activations, as $g_t = \\tanh(z_g(t)) = \\tanh(0) = 0$, which makes the product zero regardless of the value of $i_t$.\n\n2.  **Well-Posedness and Completeness**: The problem is well-posed and self-contained. It provides a clear initial condition ($c_0=1$), a deterministic recurrence relation for the cell state $c_t$, and a precise definition of the quantity to be calculated ($R(L, b_f)$). All necessary variables and functions are defined explicitly. The test cases are specified, and the required output format is unambiguous. This structure guarantees that a unique, stable, and meaningful solution exists and can be computed.\n\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language. It is free from ambiguity, subjective claims, or opinion-based statements.\n\n#### Step 1.3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-defined mathematical problem rooted in the established principles of deep learning. I will now proceed to derive the solution.\n\n### Step 2: Derivation and Solution\n\nThe objective is to derive an expression for the retention $R(L, b_f) = \\dfrac{c_L}{c_0}$ and compute its value for the given test suite.\n\nThe fundamental recurrence relation for the LSTM cell state is given as:\n$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n\nThe problem defines a simplified regime where for all time steps $t \\geq 1$, the input-write term is zero:\n$$i_t \\cdot g_t = 0 \\quad \\text{for } t \\geq 1$$\nThis simplification is consistent with the provided pre-activations, since $z_g(t) = 0$ implies $g_t = \\tanh(0) = 0$.\n\nSubstituting this condition into the cell state equation, the recurrence for $t \\geq 1$ simplifies to a purely multiplicative update:\n$$c_t = f_t \\cdot c_{t-1}$$\n\nThe forget gate activation, $f_t$, is determined by its pre-activation, $z_f(t)$. For $t \\geq 1$, this is held constant:\n$$z_f(t) = b_f$$\nTherefore, the forget gate's output $f_t$ is also constant for all $t \\geq 1$. Let us denote this constant value by $f$:\n$$f = f_t = \\sigma(z_f(t)) = \\sigma(b_f) = \\frac{1}{1 + e^{-b_f}}$$\n\nThe recurrence relation for $t \\geq 1$ now becomes a simple geometric progression with a constant factor $f$:\n$$c_t = f \\cdot c_{t-1}$$\n\nWe can unroll this recurrence starting from the initial state $c_0$:\n- For $t=1$: $c_1 = f \\cdot c_0$\n- For $t=2$: $c_2 = f \\cdot c_1 = f \\cdot (f \\cdot c_0) = f^2 \\cdot c_0$\n- For $t=3$: $c_3 = f \\cdot c_2 = f \\cdot (f^2 \\cdot c_0) = f^3 \\cdot c_0$\n\nBy induction, for any time step $L \\geq 1$, the cell state $c_L$ is given by:\n$$c_L = f^L \\cdot c_0$$\n\nThe retention $R(L, b_f)$ is defined as the ratio $\\dfrac{c_L}{c_0}$.\nFor $L \\geq 1$, we have:\n$$R(L, b_f) = \\frac{c_L}{c_0} = \\frac{f^L \\cdot c_0}{c_0} = f^L$$\nSubstituting the expression for $f$, we get the retention formula for $L \\geq 1$:\n$$R(L, b_f) = \\left(\\sigma(b_f)\\right)^L = \\left(\\frac{1}{1 + e^{-b_f}}\\right)^L$$\n\nA special case is at $L=0$. The retention after $0$ steps is, by definition:\n$$R(0, b_f) = \\frac{c_0}{c_0} = 1$$\nThis result is also consistent with the general formula if we adopt the convention that $x^0 = 1$.\n\nSo, the complete formula for retention is:\n$$\nR(L, b_f) =\n\\begin{cases}\n1 & \\text{if } L = 0 \\\\\n\\left(\\dfrac{1}{1 + e^{-b_f}}\\right)^L & \\text{if } L > 0\n\\end{cases}\n$$\n\nThe implementation will consist of a program that iterates through the specified pairs of $(b_f, L)$ in lexicographic order. For each pair, it will compute $R(L, b_f)$ using the derived formula, handling the $L=0$ case as a base condition. The results will be collected, formatted to $6$ decimal places, and printed as a single comma-separated string enclosed in square brackets. `numpy` is suitable for the exponential calculation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the memory retention in a simplified LSTM cell for a sweep of\n    forget-gate biases and dependency lengths.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    b_f_values = [-3, -1, 0, 1, 3]\n    L_values = [0, 5, 20, 100, 200]\n\n    # This structure ensures lexicographic ordering on the pair (b_f, L).\n    test_cases = [(b_f, L) for b_f in b_f_values for L in L_values]\n\n    results = []\n    \n    for b_f, L in test_cases:\n        # The retention R(L, b_f) is defined as c_L / c_0.\n        # The recurrence relation c_t = f * c_{t-1} for t >= 1 with c_0 given.\n        # This leads to c_L = f^L * c_0, so R(L, b_f) = f^L.\n        # The forget gate activation f is constant: f = sigma(b_f).\n\n        # Case 1: L = 0 (0 steps)\n        # By definition, R(0, b_f) = c_0 / c_0 = 1.\n        if L == 0:\n            retention = 1.0\n        # Case 2: L > 0\n        else:\n            # Calculate the sigmoid function for the given bias b_f.\n            # sigma(x) = 1 / (1 + e^(-x))\n            sigma_b_f = 1.0 / (1.0 + np.exp(-b_f))\n            \n            # Calculate retention as sigma(b_f) raised to the power of L.\n            retention = sigma_b_f ** L\n        \n        results.append(retention)\n\n    # Format the final output string as a comma-separated list of floats\n    # rounded to 6 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # The final print statement must produce only the specified output format.\n    print(output_string)\n\nsolve()\n\n```", "id": "3191179"}, {"introduction": "While gated RNNs like LSTMs improve information flow, the self-attention mechanism, the core of the Transformer architecture, offers a fundamentally different solution by creating direct pathways between any two positions in a sequence. This practice sets up a conceptual duel between these two architectural families on a task that is notoriously difficult for sequential models: recognizing nested structures. By analyzing simplified models based on first principles, you will discover the inherent limitations of RNNs due to sequential gradient propagation and contrast them with the strengths and weaknesses of attention, which depend on receptive field size and the ability to distinguish positions. [@problem_id:3191175]", "problem": "Consider the canonical nested-parentheses recognition task, which requires a stack-like memory to process hierarchical structure. Let the input be a sequence of length $T$ over the alphabet $\\{ \"(\", \")\" \\}$, encoded as $x_t \\in \\{+1, -1\\}$ for $t \\in \\{1,\\dots,T\\}$, where $+1$ denotes an opening parenthesis `(` and $-1$ denotes a closing parenthesis `)`. Define the cumulative sum prefix process $s_t = \\sum_{i=1}^t x_i$, with $s_0 = 0$. A string is balanced if and only if $s_T = 0$ and $s_t \\ge 0$ for all $t \\in \\{0,\\dots,T\\}$. The maximum nesting depth equals $\\max_{t} s_t$. The hardest balanced string of depth $D$ is the length-$T$ string with $T = 2D$ given by $D$ opening parentheses followed by $D$ closing parentheses, for which the first opening parenthesis must be matched to the last closing parenthesis, creating a dependency distance of $\\delta = 2D - 1$.\n\nYou will compare two abstracted model families on this task:\n1. A vanilla Recurrent Neural Network (RNN) with hidden state update $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$ and a final classifier that reads $h_T$. Assume the per-step Jacobian operator norm is bounded by a constant $\\lambda > 0$, i.e., $\\| J_t \\|_2 \\le \\lambda$ for all steps, where $J_t = \\partial h_{t+1}/\\partial h_t$. This is a standard, widely used bound with $\\lambda = \\| W_h \\|_2 L_\\phi$ for activation function Lipschitz constant $L_\\phi$.\n2. A single-layer dot-product self-attention mechanism with a local attention window of size $w \\in \\mathbb{N}$, meaning each position can directly attend only to positions within distance at most $w$. Let $p \\in \\{0, 1\\}$ indicate whether absolute positional encodings are present ($p=1$) or absent ($p=0$). Absence of positional encodings implies tokens with identical content are indistinguishable by position in a single attention layer.\n\nFrom first principles, use the following reasoning to decide whether each model can, in principle, succeed on the worst-case balanced string of depth $D$ without training, under the stated abstractions:\n- For the RNN, the chain rule with submultiplicativity of operator norms gives an upper bound on the influence of the earliest token on $h_T$ as\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2 \\le \\lambda^T\n$$.\nUsing a tolerance threshold $\\varepsilon = 10^{-3}$, declare the RNN as succeeding if and only if\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1}\n$$,\nwhich rules out vanishing ($\\lambda^T < \\varepsilon$) and exploding ($\\lambda^T > \\varepsilon^{-1}$) regimes as failure modes for learning long-term dependencies.\n- For the single-layer local self-attention, the model can only directly relate tokens within distance $w$. The worst-case match distance is $\\delta = 2D - 1$. Therefore, the model succeeds only if $p = 1$ (positions are distinguishable) and $w \\ge \\delta$. Otherwise, it fails due to either locality ($w < \\delta$) or position-indistinguishability ($p=0$).\n\nYour task is to implement a program that, for each test case, applies the above criteria and returns a categorical integer code per case:\n- Return $3$ if both the RNN and attention succeed.\n- Return $2$ if only the RNN succeeds.\n- Return $1$ if only the attention model succeeds.\n- Return $0$ if both fail.\n\nUse the following test suite, which covers a general case, locality-limited attention, vanishing-gradient RNN, trade-offs at larger depth, lack of positional encoding, and a shallow edge case:\n- Case 1: $D = 5$, $\\lambda = 0.95$, $w = 10$, $p = 1$.\n- Case 2: $D = 5$, $\\lambda = 0.95$, $w = 5$, $p = 1$.\n- Case 3: $D = 50$, $\\lambda = 0.9$, $w = 200$, $p = 1$.\n- Case 4: $D = 100$, $\\lambda = 0.99$, $w = 50$, $p = 1$.\n- Case 5: $D = 100$, $\\lambda = 1.05$, $w = 1000$, $p = 0$.\n- Case 6: $D = 1$, $\\lambda = 0.5$, $w = 1$, $p = 1$.\n\nImplementation details:\n- For each case, compute $T = 2D$, $\\delta = 2D - 1$, and use $\\varepsilon = 10^{-3}$.\n- Determine the RNN success by the inequality on $\\lambda^T$ relative to $\\varepsilon$.\n- Determine attention success by checking $p = 1$ and $w \\ge \\delta$.\n- Encode the result per case into a single integer as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,\\dots,r_6]$, where each $r_i$ is the integer code for test case $i$ in the order listed above.", "solution": "The problem requires a comparative analysis of a vanilla Recurrent Neural Network (RNN) and a single-layer local self-attention model on the task of recognizing a specific class of balanced parenthesis strings. The analysis is based on abstract, first-principle criteria for handling long-term dependencies.\n\nFirst, we formalize the problem context. The input is a sequence $x_1, \\dots, x_T$ of length $T$ where $x_t=+1$ represents an opening parenthesis `(` and $x_t=-1$ represents a closing parenthesis `)`. The \"hardest\" balanced string of nesting depth $D$ is defined as $D$ opening parentheses followed by $D$ closing parentheses. This structure implies the sequence length is $T = 2D$. The critical dependency is between the first opening parenthesis at time $t=1$ and the last closing parenthesis at time $t=T=2D$. The distance of this dependency is $\\delta = T-1 = 2D-1$.\n\nThe success of each model is determined by a set of predefined criteria. We will analyze each model class according to these rules.\n\n**Model 1: Vanilla Recurrent Neural Network (RNN)**\n\nThe RNN processes the sequence step-by-step, updating its hidden state $h_t$ via the recurrence relation $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$. The ability of the network to learn long-range dependencies is fundamentally related to how information from early inputs propagates through this recurrence to influence the final state $h_T$. This propagation is mathematically described by the gradient of the final state with respect to an initial state, $\\frac{\\partial h_T}{\\partial h_0}$. Using the chain rule, this can be expressed as a product of Jacobians for each time step:\n$$\n\\frac{\\partial h_T}{\\partial h_0} = \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0} = \\prod_{t=0}^{T-1} J_t\n$$\nwhere $J_t = \\frac{\\partial h_{t+1}}{\\partial h_t}$. Taking the operator norm and using its submultiplicative property ($\\|AB\\| \\le \\|A\\|\\|B\\|$), we get an upper bound on the norm of this gradient:\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2\n$$\nThe problem provides a uniform bound $\\|J_t\\|_2 \\le \\lambda$ for all $t$. This simplifies the expression to:\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\lambda^T\n$$\nThis bound is central to the concepts of vanishing and exploding gradients. If $\\lambda < 1$, the bound $\\lambda^T$ decays exponentially to $0$ as the sequence length $T$ increases. This means the influence of early inputs can become vanishingly small, making it impossible for the model to learn long-term dependencies. This is the vanishing gradient problem. Conversely, if $\\lambda > 1$, the bound grows exponentially, which can lead to unstable training. This is the exploding gradient problem.\n\nThe success criterion for the RNN is defined to avoid these pathological regimes. Using a tolerance threshold $\\varepsilon = 10^{-3}$, the RNN is considered successful if and only if the gradient bound is within a stable range:\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1}\n$$\n\n**Model 2: Single-Layer Local Self-Attention**\n\nThe self-attention mechanism computes the representation of each token as a weighted average of other tokens' representations. In a local self-attention model with a window of size $w$, each token at position $i$ can only directly attend to (i.e., exchange information with) tokens in the range $[i-w, i+w]$. To solve the given task, the model must be able to associate the first opening parenthesis at $t=1$ with the last closing parenthesis at $t=T$. The distance between these tokens is $\\delta = T-1 = 2D-1$. For a single layer of local attention to create a direct path between these tokens, its window size $w$ must be large enough to span this distance, i.e., $w \\ge \\delta$.\n\nFurthermore, the problem specifies the hardest-case string consists of $D$ identical opening parentheses followed by $D$ identical closing parentheses. Without a mechanism to distinguish tokens by their position, a self-attention layer cannot differentiate between the first `(` and the second `(`, making it impossible to learn the correct nested structure. Absolute positional encodings provide this crucial information. The parameter $p=1$ indicates their presence, while $p=0$ indicates their absence.\n\nTherefore, the success of the single-layer local self-attention model is contingent on two conditions:\n$1$. Positional information must be available: $p=1$.\n$2$. The attention window must be wide enough to capture the longest dependency: $w \\ge \\delta$.\nThe model succeeds if and only if both conditions are met.\n\n**Case-by-Case Evaluation**\n\nWe now apply these criteria to each test case, using $\\varepsilon = 10^{-3}$.\n\n**Case 1:** $D=5$, $\\lambda = 0.95$, $w=10$, $p=1$.\n- Derived values: $T = 2D = 10$, $\\delta = 2D-1 = 9$.\n- RNN: We evaluate $\\lambda^T = 0.95^{10} \\approx 0.5987$. The condition is $0.001 \\le 0.5987 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $10 \\ge 9$ (true). **Attention succeeds.**\n- Result: Both succeed, Code = $3$.\n\n**Case 2:** $D=5$, $\\lambda = 0.95$, $w=5$, $p=1$.\n- Derived values: $T = 10$, $\\delta = 9$.\n- RNN: We evaluate $\\lambda^T = 0.95^{10} \\approx 0.5987$. The condition is $0.001 \\le 0.5987 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $5 \\ge 9$ (false). **Attention fails.**\n- Result: Only RNN succeeds, Code = $2$.\n\n**Case 3:** $D=50$, $\\lambda = 0.9$, $w=200$, $p=1$.\n- Derived values: $T = 2D = 100$, $\\delta = 2D-1 = 99$.\n- RNN: We evaluate $\\lambda^T = 0.9^{100} \\approx 2.656 \\times 10^{-5}$. This value is less than $\\varepsilon = 0.001$. The gradient vanishes. **RNN fails.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $200 \\ge 99$ (true). **Attention succeeds.**\n- Result: Only Attention succeeds, Code = $1$.\n\n**Case 4:** $D=100$, $\\lambda = 0.99$, $w=50$, $p=1$.\n- Derived values: $T = 2D = 200$, $\\delta = 2D-1 = 199$.\n- RNN: We evaluate $\\lambda^T = 0.99^{200} \\approx 0.134$. The condition is $0.001 \\le 0.134 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $50 \\ge 199$ (false). **Attention fails.**\n- Result: Only RNN succeeds, Code = $2$.\n\n**Case 5:** $D=100$, $\\lambda = 1.05$, $w=1000$, $p=0$.\n- Derived values: $T = 200$, $\\delta = 199$.\n- RNN: We evaluate $\\lambda^T = 1.05^{200} \\approx 17293$. This value is greater than $\\varepsilon^{-1} = 1000$. The gradient explodes. **RNN fails.**\n- Attention: We check $p=1$. This is false ($p=0$). The model cannot distinguish token positions. **Attention fails.**\n- Result: Both fail, Code = $0$.\n\n**Case 6:** $D=1$, $\\lambda = 0.5$, $w=1$, $p=1$.\n- Derived values: $T = 2D = 2$, $\\delta = 2D-1 = 1$. The string is `()`.\n- RNN: We evaluate $\\lambda^T = 0.5^2 = 0.25$. The condition is $0.001 \\le 0.25 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $1 \\ge 1$ (true). **Attention succeeds.**\n- Result: Both succeed, Code = $3$.\n\nThese results are synthesized into a final list of integer codes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies success criteria to an RNN and a local self-attention model\n    for a series of test cases on a parenthesis-matching task.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (D, lambda, w, p).\n    test_cases = [\n        (5, 0.95, 10, 1),  # Case 1\n        (5, 0.95, 5, 1),   # Case 2\n        (50, 0.9, 200, 1), # Case 3\n        (100, 0.99, 50, 1),# Case 4\n        (100, 1.05, 1000, 0),# Case 5\n        (1, 0.5, 1, 1),    # Case 6\n    ]\n\n    # The tolerance threshold epsilon.\n    epsilon = 1e-3\n\n    results = []\n    for D, lam, w, p in test_cases:\n        # Calculate derived problem parameters.\n        T = 2 * D\n        delta = 2 * D - 1\n\n        # --- RNN Success Criterion ---\n        # The RNN succeeds if its gradient norm bound lambda^T is within a stable range.\n        # This prevents vanishing (lambda^T < epsilon) and exploding (lambda^T > 1/epsilon) gradients.\n        lambda_T = np.power(lam, T)\n        rnn_succeeds = (epsilon <= lambda_T <= 1 / epsilon)\n\n        # --- Attention Success Criterion ---\n        # The single-layer local attention model succeeds if:\n        # 1. Positional encodings are present (p=1) to distinguish identical tokens.\n        # 2. The local window size (w) is large enough to span the maximum dependency distance (delta).\n        attention_succeeds = (p == 1 and w >= delta)\n\n        # --- Determine the final result code ---\n        # 3: both succeed\n        # 2: only RNN succeeds\n        # 1: only attention succeeds\n        # 0: both fail\n        if rnn_succeeds and attention_succeeds:\n            code = 3\n        elif rnn_succeeds and not attention_succeeds:\n            code = 2\n        elif not rnn_succeeds and attention_succeeds:\n            code = 1\n        else: # both fail\n            code = 0\n            \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191175"}, {"introduction": "Theoretically, a global self-attention mechanism can model any dependency regardless of distance. In practice, however, its power comes at a steep price: the computational and memory costs scale quadratically with the sequence length, $n$, as $O(n^2)$. This final practice moves from theory to engineering reality, placing you in the role of a machine learning engineer tasked with training a model for very long sequences under a strict hardware budget. You will analyze and choose among various strategies—including efficient alternatives like local and linear attention, as well as memory-saving techniques like gradient checkpointing—to find the optimal balance between computational feasibility and modeling capability. [@problem_id:3191168]", "problem": "You are asked to model and optimize training-time compute–memory trade-offs for sequence models on Long Range Arena (LRA) tasks under strict Graphics Processing Unit (GPU) memory budgets. Your program must select, for each given case, the training strategy that minimizes total floating-point operation count (a proxy for time) while satisfying a hard memory budget. The selection must be made among a specified subset of strategies for each case.\n\nBase definitions and assumptions (derive all formulas from these only):\n- Let batch size be $b$, sequence length be $n$, model (embedding) dimension be $d$, number of attention heads be $h$, number of layers be $L$, and head dimension $d_h = d / h$ (assume $d$ divisible by $h$).\n- Use $32$-bit floating point for all activations and intermediate tensors. Each stored scalar consumes $4$ bytes. Denote this constant as $\\beta_b = 4$ bytes per scalar.\n- During training, activations are stored for the backward pass. For a single transformer layer with dense self-attention, base activations to store include:\n  1. Queries $Q$, Keys $K$, Values $V$, and the output of the attention projection $O$, each with shape $(b, h, n, d_h)$, which equals $(b, n, d)$ scalars when multiplied out across heads.\n  2. Two feed-forward (multi-layer perceptron) activations of shape $(b, n, d)$ each (pre- and post-nonlinearity), modeled as a simplified pair to capture dominant storage.\n  3. The attention logits and the attention probabilities, each with shape $(b, h, n, n)$.\n- From these, for one layer with dense attention, the total count of stored scalars to leading order is:\n  - A “token-linear” part: $Q$, $K$, $V$, $O$ give $4 \\cdot b \\cdot n \\cdot d$ scalars; the two feed-forward activations add $2 \\cdot b \\cdot n \\cdot d$. Hence token-linear scalars per layer equal $\\alpha_1 \\cdot b \\cdot n \\cdot d$ with $\\alpha_1 = 6$.\n  - A “quadratic-attention” part: attention logits and probabilities together give $\\alpha_2 \\cdot b \\cdot h \\cdot n^2$ scalars with $\\alpha_2 = 2$.\n- For a model with $L$ layers, if a memory term is “per layer,” multiply it by $L$ to obtain the total activation memory.\n- Ignore parameter and optimizer state memory, which is dominated by long-sequence activation memory under LRA-like settings, and assume no additional overhead. All memory accounting must be expressed in bytes via scalar counts times $\\beta_b$.\n- For compute, count floating-point operations (treat a multiply–add as a single unit for simplicity and consistency across strategies). Per layer:\n  - Projections $Q$, $K$, $V$, $O$ cost $4 \\cdot b \\cdot n \\cdot d^2$ operation units (matrix–matrix products at each token).\n  - Dense attention costs a term proportional to forming $QK^\\top$ and multiplying by $V$, both with contraction dimension $d_h$. Model this as $2 \\cdot b \\cdot h \\cdot n^2 \\cdot d_h$ operation units per layer.\n  - Local (windowed) attention with window $w$ replaces $n^2$ by $n \\cdot w$ in both attention multiplications, for $2 \\cdot b \\cdot h \\cdot n \\cdot w \\cdot d_h$ operation units per layer.\n  - Linear attention (kernelized or equivalent) removes the explicit attention matrix and uses streaming accumulations. Model this attention component as $k_\\ell \\cdot b \\cdot h \\cdot n \\cdot d_h$ operation units per layer with constant $k_\\ell = 4$.\n- Gradient checkpointing across each layer trades memory for compute by recomputing intermediates in the backward pass. Use the following model for one layer when checkpointing is enabled:\n  - Activation memory is multiplied by $r_{\\mathrm{ckpt}} = 1/2$ (store roughly only layer inputs/outputs).\n  - Compute is multiplied by $g_{\\mathrm{ckpt}} = 2$ (each layer’s forward intermediates are recomputed in backward).\n- Strategies:\n  - Strategy $0$: Dense attention (no checkpointing).\n  - Strategy $1$: Dense attention with checkpointing (memory scaled by $r_{\\mathrm{ckpt}}$, compute scaled by $g_{\\mathrm{ckpt}}$).\n  - Strategy $2$: Local attention with window $w$ (no checkpointing).\n  - Strategy $3$: Linear attention (no checkpointing).\n- A strategy is feasible if and only if its total activation memory in bytes is less than or equal to the given memory budget for the case. Among feasible and allowed strategies for the case, choose the one with the smallest total operation count. If there is a tie, select the smallest strategy index. If no allowed strategy is feasible, output $-1$ for that case.\n\nTasks:\n1. Starting only from the shapes and definitions above, derive the per-layer activation memory in scalar counts for each strategy, then convert to bytes using $\\beta_b = 4$. Aggregate across $L$ layers to obtain total activation memory for a case–strategy pair.\n2. From the base matrix operations, derive the per-layer operation counts for each strategy, aggregate across $L$ layers, and incorporate the checkpointing multiplier $g_{\\mathrm{ckpt}}$ where applicable.\n3. Implement a program that, for each test case below, enumerates the allowed strategies, filters by the memory budget, and selects the feasible strategy with the minimum total operation count, breaking ties by the smallest index. Return the selected strategy index as an integer for each case. No physical units are required in the output, only the integer indices.\n\nTest suite:\nEach case is a tuple ($n, d, h, L, b, M_{\\mathrm{bytes}}, w$, allowed) where $M_{\\mathrm{bytes}}$ is the memory budget in bytes, $w$ is the local attention window size (only used by Strategy $2$), and $\\text{allowed}$ is the list of allowed strategy indices.\n- Case $1$: ($n = 1024, d = 512, h = 8, L = 6, b = 4, M_{\\mathrm{bytes}} = 2147483648, w = 256$, allowed = $[0, 1, 2, 3]$).\n- Case $2$: ($n = 4096, d = 512, h = 8, L = 6, b = 1, M_{\\mathrm{bytes}} = 6000000000, w = 256$, allowed = $[0, 1, 2, 3]$).\n- Case $3$: ($n = 16384, d = 512, h = 8, L = 6, b = 1, M_{\\mathrm{bytes}} = 2000000000, w = 256$, allowed = $[0, 2, 3]$).\n- Case $4$: ($n = 4096, d = 512, h = 8, L = 6, b = 2, M_{\\mathrm{bytes}} = 1409286144, w = 256$, allowed = $[2]$). This case is a boundary test where feasibility holds exactly when total memory equals the budget.\n- Case $5$: ($n = 2048, d = 512, h = 8, L = 6, b = 4, M_{\\mathrm{bytes}} = 3523215360, w = 1024$, allowed = $[0, 1, 2]$). This case ensures that checkpointing can make a dense model feasible while dense without checkpointing and local attention are infeasible under the budget.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), where each $r_i$ is the selected integer strategy index for case $i$ in the order presented above.", "solution": "The task is to determine the optimal training strategy for a sequence model under a given memory budget, by minimizing the total floating-point operation count (compute). The selection is made from a set of allowed strategies for several test cases. The first step, problem validation, has been completed and the problem is deemed valid and well-posed. The solution proceeds by first deriving the mathematical models for memory usage and computational cost for each strategy, and then implementing an algorithm to find the optimal strategy for each case.\n\n### 1. Derivation of Memory and Compute Formulas\n\nWe derive the total activation memory ($M$) in bytes and total computational cost ($C$) in floating-point operations for each of the four strategies. The formulas are derived based on the provided parameters: batch size $b$, sequence length $n$, model dimension $d$, number of heads $h$, head dimension $d_h = d/h$, number of layers $L$, and local attention window size $w$. The constants are $\\beta_b = 4$ bytes/scalar, token-linear scalar coefficient $\\alpha_1 = 6$, quadratic-attention scalar coefficient $\\alpha_2 = 2$, linear attention compute constant $k_\\ell=4$, checkpointing memory ratio $r_{\\text{ckpt}}=1/2$, and checkpointing compute multiplier $g_{\\text{ckpt}}=2$.\n\nA crucial interpretation must be made for local attention (Strategy $2$). The problem states that for compute, the quadratic dependency $n^2$ is replaced by a linear dependency $nw$. It is a standard assumption and necessary for creating a meaningful model that this replacement also applies to the memory term associated with storing attention matrices. Without this, local attention would not provide a memory advantage over dense attention. This interpretation is confirmed by the problem's description of Test Case $4$ as a boundary case where feasibility holds exactly, which is only true if the memory for local attention scales with $nw$.\n\n**Strategy 0: Dense Attention**\n-   **Activation Memory ($M_0$)**: The total scalar count is the sum of the token-linear part and the quadratic-attention part, multiplied by the number of layers $L$.\n    $$ \\text{scalars}_0 = L \\cdot (\\alpha_1 bnd + \\alpha_2 bhn^2) = L \\cdot (6bnd + 2bhn^2) $$\n    Converting to bytes using $\\beta_b = 4$:\n    $$ M_0 = \\beta_b \\cdot L \\cdot (6bnd + 2bhn^2) = 4L(6bnd + 2bhn^2) $$\n-   **Computational Cost ($C_0$)**: The cost per layer is the sum of projection costs and attention computation costs. We use the identity $h \\cdot d_h = d$.\n    $$ c_{\\text{layer},0} = 4bnd^2 + 2bhn^2d_h = 4bnd^2 + 2bn^2d $$\n    Total cost for $L$ layers:\n    $$ C_0 = L(4bnd^2 + 2bn^2d) $$\n\n**Strategy 1: Dense Attention with Gradient Checkpointing**\nThis strategy modifies Strategy $0$ by applying the checkpointing multipliers.\n-   **Activation Memory ($M_1$)**: Memory is scaled by $r_{\\text{ckpt}} = 1/2$.\n    $$ M_1 = M_0 \\cdot r_{\\text{ckpt}} = \\frac{1}{2} \\cdot 4L(6bnd + 2bhn^2) = 2L(6bnd + 2bhn^2) $$\n-   **Computational Cost ($C_1$)**: Compute is scaled by $g_{\\text{ckpt}} = 2$.\n    $$ C_1 = C_0 \\cdot g_{\\text{ckpt}} = 2 \\cdot L(4bnd^2 + 2bn^2d) $$\n\n**Strategy 2: Local Attention**\nThis strategy replaces the quadratic dependency on $n$ with a dependency on $n \\cdot w$.\n-   **Activation Memory ($M_2$)**: The $n^2$ term in the memory model is replaced with $nw$.\n    $$ M_2 = \\beta_b \\cdot L \\cdot (\\alpha_1 bnd + \\alpha_2 bhnw) = 4L(6bnd + 2bhnw) $$\n-   **Computational Cost ($C_2$)**: The $n^2$ term in the compute model is replaced with $nw$.\n    $$ C_2 = L(4bnd^2 + 2bhnwd_h) = L(4bnd^2 + 2bnwd) $$\n\n**Strategy 3: Linear Attention**\nThis strategy eliminates the explicit $n \\times n$ attention matrix, thereby removing the quadratic memory term.\n-   **Activation Memory ($M_3$)**: Only the token-linear part of activation memory remains.\n    $$ M_3 = \\beta_b \\cdot L \\cdot (\\alpha_1 bnd) = 4L(6bnd) = 24Lbnd $$\n-   **Computational Cost ($C_3$)**: The attention compute cost is replaced by a linear-time equivalent.\n    $$ C_3 = L(4bnd^2 + k_\\ell bhnd_h) = L(4bnd^2 + 4bnd) $$\n\n### 2. Algorithmic Design\n\nFor each test case, specified by $(n, d, h, L, b, M_{\\mathrm{bytes}}, w, \\text{allowed})$, the following algorithm is executed:\n\n1.  Initialize an empty list, `feasible_strategies`, to store strategies that meet the memory budget.\n2.  For each strategy index `s` in the provided `allowed` list:\n    a. Calculate the total activation memory $M_s$ and total compute $C_s$ using the formulas derived above with the given case parameters.\n    b. Check for feasibility: if $M_s \\le M_{\\mathrm{bytes}}$, add the tuple $(C_s, s)$ to the `feasible_strategies` list.\n3.  After evaluating all allowed strategies:\n    a. If `feasible_strategies` is empty, no allowed strategy is feasible. The result for this case is $-1$.\n    b. If `feasible_strategies` is not empty, sort it. The primary sorting key is the compute cost $C_s$ (ascending), and the secondary sorting key is the strategy index $s$ (ascending) to break ties as required.\n    c. The optimal strategy is the one corresponding to the first element of the sorted list. The result is the strategy index from this element.\n4.  The results from all test cases are collected and formatted into a single string `[r_1,r_2,...]` for the final output. This procedure guarantees finding the strategy with the minimum compute cost among all feasible options, respecting the tie-breaking rule.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Models and optimizes training-time compute-memory trade-offs for sequence models.\n\n    For each test case, this function selects the training strategy that minimizes\n    the total floating-point operation count while satisfying a hard memory budget.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, d, h, L, b, M_bytes, w, allowed)\n        (1024, 512, 8, 6, 4, 2147483648, 256, [0, 1, 2, 3]),\n        (4096, 512, 8, 6, 1, 6000000000, 256, [0, 1, 2, 3]),\n        (16384, 512, 8, 6, 1, 2000000000, 256, [0, 2, 3]),\n        (4096, 512, 8, 6, 2, 1409286144, 256, [2]),\n        (2048, 512, 8, 6, 4, 3523215360, 1024, [0, 1, 2]),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, d, h, L, b, M_bytes, w, allowed = case\n        d_h = d // h\n\n        # Constants from problem\n        beta_b = 4  # bytes per scalar\n        k_ell = 4   # linear attention compute constant\n\n        # --- Memory and Compute Formulas ---\n\n        # Strategy 0: Dense attention\n        mem0 = beta_b * L * (6 * b * n * d + 2 * b * h * n**2)\n        comp0 = L * (4 * b * n * d**2 + 2 * b * n**2 * d)\n\n        # Strategy 1: Dense attention with checkpointing\n        mem1 = mem0 / 2\n        comp1 = comp0 * 2\n\n        # Strategy 2: Local attention\n        mem2 = beta_b * L * (6 * b * n * d + 2 * b * h * n * w)\n        comp2 = L * (4 * b * n * d**2 + 2 * b * n * w * d)\n\n        # Strategy 3: Linear attention\n        mem3 = beta_b * L * (6 * b * n * d)\n        comp3 = L * (4 * b * n * d**2 + k_ell * b * n * d)\n\n        # Store strategy performance in a list of tuples: (memory, compute)\n        strategies_perf = [\n            (mem0, comp0),\n            (mem1, comp1),\n            (mem2, comp2),\n            (mem3, comp3)\n        ]\n\n        feasible_options = []\n        for strat_idx in allowed:\n            memory, compute = strategies_perf[strat_idx]\n            if memory <= M_bytes:\n                # Store as (compute, strategy_index) for sorting\n                feasible_options.append((compute, strat_idx))\n\n        if not feasible_options:\n            results.append(-1)\n        else:\n            # Sort by compute cost (primary key), then by strategy index (secondary key)\n            feasible_options.sort()\n            best_strategy_index = feasible_options[0][1]\n            results.append(best_strategy_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191168"}]}