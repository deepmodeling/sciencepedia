{"hands_on_practices": [{"introduction": "The best way to truly grasp an algorithm is to walk through its calculations by hand. This first exercise breaks down the Adam optimizer into its fundamental components, guiding you through a single update step for a simple quadratic function [@problem_id:2152250]. By manually computing the moment estimates, applying bias correction, and calculating the final parameter update, you will build a concrete foundation for understanding how Adam works under the hood.", "problem": "In the field of numerical optimization, the Adam algorithm is a widely used method for finding the minimum of a function. Consider the one-dimensional cost function $f(x) = 5x^2$. We wish to find the value of $x$ that minimizes this function, starting from an initial guess of $x_0 = 2$.\n\nCalculate the value of the parameter after one update step, denoted as $x_1$, using the Adam algorithm. The algorithm is configured with the following hyperparameters:\n- Learning rate, $\\alpha = 0.1$\n- Exponential decay rate for the first moment estimate, $\\beta_1 = 0.9$\n- Exponential decay rate for the second moment estimate, $\\beta_2 = 0.999$\n- A small constant for numerical stability, $\\epsilon = 10^{-8}$\n\nThe initial first and second moment estimates, $m_0$ and $v_0$, are both initialized to zero.\n\nRound your final answer to five significant figures.", "solution": "We minimize $f(x)=5x^{2}$ using Adam starting from $x_{0}=2$. The gradient is $\\nabla f(x)=10x$. At the first step ($t=1$), the gradient evaluated at $x_{0}$ is\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam moment updates are\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\nWith $m_{0}=0$ and $v_{0}=0$, $\\beta_{1}=0.9$, and $\\beta_{2}=0.999$,\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\nBias-corrected estimates are\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nThe Adam update is\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\nEvaluating the fraction,\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\nso\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\nRounded to five significant figures, this is $1.9000$.", "answer": "$$\\boxed{1.9000}$$", "id": "2152250"}, {"introduction": "Having mastered the mechanics of a single update, let's explore Adam's dynamic behavior in a specific optimization landscape. This practice uses a thought experiment involving a long, flat plateau—a common challenge in training neural networks—to reveal how Adam's second moment estimate, $\\hat{v}_t$, adjusts the effective learning rate [@problem_id:2152254]. You will see how this adaptive mechanism can help the optimizer make more substantial progress in regions with persistently small gradients.", "problem": "An engineer is using the Adam (Adaptive Moment Estimation) optimizer to train a neural network. The training process has entered a phase where the optimizer is traversing a long, flat plateau in the loss landscape. On this plateau, the gradient of the loss function with respect to a particular scalar parameter, $\\theta$, is observed to be a small, constant value for several consecutive timesteps.\n\nAssume the optimizer starts at timestep $t=0$ with its moment estimates initialized to zero: the first moment estimate $m_0 = 0$ and the second moment estimate $v_0 = 0$. For all subsequent timesteps $t \\geq 1$, the gradient is constant, $g_t = g = 1.0 \\times 10^{-5}$.\n\nThe Adam update rules are given by:\n1.  Update the biased first moment estimate: $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n2.  Update the biased second moment estimate: $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n3.  Compute the bias-corrected first moment estimate: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n4.  Compute the bias-corrected second moment estimate: $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n\nThe parameter update itself uses an effective learning rate that adapts based on the history of gradients. This effective learning rate, which modulates the base learning rate $\\alpha$, can be defined as $\\alpha_{\\text{eff}, t} = \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}$.\n\nGiven the following hyperparameters:\n- Base learning rate: $\\alpha = 0.001$\n- Exponential decay rate for the first moment: $\\beta_1 = 0.9$\n- Exponential decay rate for the second moment: $\\beta_2 = 0.999$\n- Epsilon for numerical stability: $\\epsilon = 1.0 \\times 10^{-8}$\n\nCalculate the effective learning rate, $\\alpha_{\\text{eff}, t}$, at timestep $t=3$. Round your final answer to four significant figures.", "solution": "We are given the Adam recursions with $m_{0}=0$, $v_{0}=0$ and a constant gradient $g_{t}=g$ for all $t \\geq 1$. The updates are\n$$\nm_{t}=\\beta_{1} m_{t-1}+(1-\\beta_{1}) g,\\qquad\nv_{t}=\\beta_{2} v_{t-1}+(1-\\beta_{2}) g^{2}.\n$$\nSolving the linear recursions with $m_{0}=0$ and $v_{0}=0$ yields geometric sums:\n$$\nm_{t}=\\beta_{1}^{t} m_{0}+(1-\\beta_{1}) g \\sum_{k=0}^{t-1} \\beta_{1}^{k}=(1-\\beta_{1}^{t}) g,\n$$\n$$\nv_{t}=\\beta_{2}^{t} v_{0}+(1-\\beta_{2}) g^{2} \\sum_{k=0}^{t-1} \\beta_{2}^{k}=(1-\\beta_{2}^{t}) g^{2}.\n$$\nThe bias-corrected estimates are then\n$$\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}=g,\\qquad\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}=g^{2}.\n$$\nTherefore, for any $t \\geq 1$, and in particular at $t=3$, the effective learning rate is\n$$\n\\alpha_{\\text{eff},t}=\\frac{\\alpha}{\\sqrt{\\hat{v}_{t}}+\\epsilon}=\\frac{\\alpha}{\\sqrt{g^{2}}+\\epsilon}=\\frac{\\alpha}{|g|+\\epsilon}.\n$$\nWith the given values $g=1.0 \\times 10^{-5}$, $\\alpha=0.001$, and $\\epsilon=1.0 \\times 10^{-8}$, we have $|g|+\\epsilon=1.0 \\times 10^{-5}+1.0 \\times 10^{-8}=1.001 \\times 10^{-5}$. Hence\n$$\n\\alpha_{\\text{eff},3}=\\frac{0.001}{1.001 \\times 10^{-5}}=\\frac{1.0 \\times 10^{-3}}{1.001 \\times 10^{-5}}=\\frac{1.0}{1.001} \\times 10^{2}\\approx 99.9000999\\ldots\n$$\nRounding to four significant figures gives $99.90$.", "answer": "$$\\boxed{99.90}$$", "id": "2152254"}, {"introduction": "Now it's time to bridge the gap between pen-and-paper analysis and real-world application by putting on your programmer's hat. This advanced exercise challenges you to implement Adam and its more stable variant, AMSGrad, to investigate their behavior on carefully constructed gradient sequences that expose a known instability in the original Adam algorithm [@problem_id:3095752]. By coding the optimizers and quantifying their performance differences, you will gain a profound, practical understanding of the subtleties of adaptive optimization.", "problem": "You are asked to reason from first principles to compare Adaptive Moment Estimation (Adam) and Adaptive Moment Estimation with Maximum Tracking (AMSGrad) in a controlled, one-dimensional stochastic optimization setting. The goal is to construct and analyze a gradient sequence that causes Adaptive Moment Estimation (Adam) to exhibit oscillatory or increasing step magnitudes due to a decreasing second-moment estimate, while Adaptive Moment Estimation with Maximum Tracking (AMSGrad) stabilizes the updates by ensuring that the effective second-moment denominator is monotone non-decreasing in time. You will implement both methods, run them on a small test suite of gradient sequences, and quantitatively report stability metrics.\n\nStart from the following fundamental basis:\n- Stochastic optimization updates a parameter $x_t \\in \\mathbb{R}$ at discrete time $t$ by forming a descent direction from a gradient estimate $g_t$ and applying a step-size $s_t$ to obtain $x_{t+1} = x_t - s_t$.\n- An exponential moving average (EMA) of a sequence $(z_t)$ with coefficient $\\beta \\in (0,1)$ is the recursion $u_t = \\beta u_{t-1} + (1-\\beta) z_t$ with $u_0 = 0$. Bias correction divides $u_t$ by $(1-\\beta^t)$.\n- Adaptive methods use EMA of $g_t$ and $g_t^2$ to shape $s_t$ per time step through elementwise normalization.\n\nYou must implement two optimizers that share the same hyperparameters $(\\beta_1,\\beta_2)$, learning rate $\\alpha$, and numerical stabilizer $\\varepsilon$:\n- Method Adam: uses EMA of $g_t$ and $g_t^2$ with bias correction for both moment estimates to produce a step-size $s_t$ proportional to the ratio of the bias-corrected first moment to the square root of the bias-corrected second moment.\n- Method AMSGrad: identical to Method Adam except that it replaces the bias-corrected second moment with an elementwise running maximum over time, which enforces a non-decreasing denominator and therefore non-increasing effective per-coordinate step sizes.\n\nStability should be quantified using the following metrics computed over a run of length $T$:\n- Let $\\Delta x_t$ denote the signed update at time $t$ and $|\\Delta x_t|$ its magnitude.\n- Define the maximum update magnitudes $$M_{\\text{Adam}} = \\max_{1 \\le t \\le T} |\\Delta x_t^{\\text{Adam}}|$$ and $$M_{\\text{AMS}} = \\max_{1 \\le t \\le T} |\\Delta x_t^{\\text{AMS}}|$$.\n- Define the ratio $R = M_{\\text{Adam}} / M_{\\text{AMS}}$.\n- Define the standard deviation of update magnitudes $\\sigma_{\\text{Adam}}$ and $\\sigma_{\\text{AMS}}$ across $t=1,\\dots,T$, and the stability gain $\\Delta \\sigma = \\sigma_{\\text{Adam}} - \\sigma_{\\text{AMS}}$.\n- Let $N_{\\uparrow}^{\\text{Adam}}$ count how many times $|\\Delta x_t^{\\text{Adam}}| > |\\Delta x_{t-1}^{\\text{Adam}}|$ for $t=2,\\dots,T$, and similarly $N_{\\uparrow}^{\\text{AMS}}$ for AMSGrad. Larger $N_{\\uparrow}$ means more frequent increases in step magnitude.\n\nYou will run both methods on the following test suite of gradient sequences, each with specified hyperparameters. In all cases, use initial parameter $x_0 = 0$, stabilizer $\\varepsilon = 10^{-8}$, and learning rate $\\alpha = 10^{-2}$.\n\nTest Suite:\n1. Alternating large-small gradients (oscillatory magnitude case):\n   - Sequence: $g_t = 10$ for odd $t$, $g_t = 0.1$ for even $t$, for $t = 1,\\dots,T$ with $T = 200$.\n   - Hyperparameters: $\\beta_1 = 0.99$, $\\beta_2 = 0.2$.\n2. Constant gradients (steady case):\n   - Sequence: $g_t = 1$ for all $t = 1,\\dots,T$ with $T = 200$.\n   - Hyperparameters: $\\beta_1 = 0.99$, $\\beta_2 = 0.2$.\n3. Single spike then small gradients (transient instability case):\n   - Sequence: $g_1 = 50$ and $g_t = 0.1$ for $t = 2,\\dots,T$ with $T = 200$.\n   - Hyperparameters: $\\beta_1 = 0.99$, $\\beta_2 = 0.2$.\n4. Short alternating sequence (bias correction boundary condition):\n   - Sequence: $g = [10, 0.1, 0.1]$ so $T = 3$.\n   - Hyperparameters: $\\beta_1 = 0.99$, $\\beta_2 = 0.2$.\n\nFor each test case, compute and return a list with the seven quantities in the following order:\n- $R$, $M_{\\text{Adam}}$, $M_{\\text{AMS}}$, $|x_T^{\\text{Adam}}|$, $|x_T^{\\text{AMS}}|$, $N_{\\uparrow}^{\\text{Adam}}$, $N_{\\uparrow}^{\\text{AMS}}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of these per-test-case lists, enclosed in square brackets. For example, the format must be like:\n[[case1_values],[case2_values],[case3_values],[case4_values]]\nNo units are involved, and all outputs must be real numbers or integers. Round nothing; print full precision as produced by standard Python formatting.", "solution": "We begin from the definition of a stochastic optimization update. A scalar parameter $x_t \\in \\mathbb{R}$ is updated via\n$$\nx_{t+1} \\;=\\; x_t - s_t,\n$$\nwhere $s_t$ is a step formed from a gradient-like signal $g_t$. Adaptive methods compute $s_t$ by normalizing the first moment of $g_t$ by the square root of a second moment estimate of $g_t^2$, thereby creating a dimensionless step scaled by a learning rate.\n\nThe exponential moving average (EMA) of a sequence $(z_t)$ with coefficient $\\beta \\in (0,1)$ is defined by\n$$\nu_t \\;=\\; \\beta u_{t-1} + (1-\\beta) z_t, \\quad u_0 = 0.\n$$\nBias correction compensates for the initialization at zero and is given by\n$$\n\\hat{u}_t \\;=\\; \\frac{u_t}{1-\\beta^t}.\n$$\n\nAdaptive Moment Estimation (Adam) uses two EMAs:\n- The first moment (momentum-like) estimate $m_t$ of $g_t$ with coefficient $\\beta_1$, and\n- The second moment estimate $v_t$ of $g_t^2$ with coefficient $\\beta_2$.\n\nWith bias-corrected estimates $\\hat{m}_t$ and $\\hat{v}_t$, Adam’s per-step update magnitude for the scalar case is\n$$\n|\\Delta x_t^{\\text{Adam}}| \\;=\\; \\alpha \\cdot \\frac{|\\hat{m}_t|}{\\sqrt{\\hat{v}_t} + \\varepsilon},\n$$\nwhere $\\alpha$ is the learning rate and $\\varepsilon$ is a small positive constant for numerical stability.\n\nAdaptive Moment Estimation with Maximum Tracking (AMSGrad) keeps the same $\\hat{m}_t$ but enforces a monotone non-decreasing second-moment denominator by defining\n$$\n\\tilde{v}_t \\;=\\; \\max(\\tilde{v}_{t-1}, \\hat{v}_t), \\quad \\tilde{v}_0 = 0,\n$$\nand uses $\\tilde{v}_t$ in place of $\\hat{v}_t$ in the denominator. The per-step update magnitude is\n$$\n|\\Delta x_t^{\\text{AMS}}| \\;=\\; \\alpha \\cdot \\frac{|\\hat{m}_t|}{\\sqrt{\\tilde{v}_t} + \\varepsilon}.\n$$\nBy construction, $\\tilde{v}_t$ is elementwise non-decreasing, so $(\\sqrt{\\tilde{v}_t} + \\varepsilon)$ is also non-decreasing. This ensures that the effective per-coordinate normalization does not become smaller over time. In contrast, Adam’s $\\hat{v}_t$ may decrease when the recent $g_t^2$ are smaller, causing possible increases in the normalized step magnitude even if $|\\hat{m}_t|$ remains relatively large due to its slower decay governed by $\\beta_1$.\n\nWe now derive why the AMSGrad modification improves stability in the constructed cases. Consider a sequence where $g_t$ alternates between a large value and a small value, or where there is a single early spike followed by persistently small values. For $\\beta_1$ close to $1$ (for example $\\beta_1 = 0.99$), the first moment $m_t$ retains substantial memory of the large gradients because\n$$\nm_t \\approx \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n$$\ndecays slowly. For a smaller $\\beta_2$ (for example $\\beta_2 = 0.2$), the second moment $v_t$ responds quickly to recent $g_t^2$, so when $g_t$ becomes small, $v_t$ (and thus $\\hat{v}_t$) may drop significantly. Therefore, under Adam,\n$$\n\\frac{|\\hat{m}_t|}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n$$\ncan increase after a large-to-small gradient transition because the denominator shrinks faster than the numerator, producing larger $|\\Delta x_t^{\\text{Adam}}|$ than earlier steps. This manifests as oscillatory or increasing step magnitudes and reduced stability.\n\nBy comparison, AMSGrad enforces\n$$\n\\tilde{v}_t \\ge \\tilde{v}_{t-1} \\ge \\hat{v}_\\tau \\quad \\text{for all} \\; \\tau \\le t,\n$$\nso the denominator never decreases below its historical maximum. Consequently,\n$$\n|\\Delta x_t^{\\text{AMS}}| \\;=\\; \\alpha \\cdot \\frac{|\\hat{m}_t|}{\\sqrt{\\tilde{v}_t} + \\varepsilon} \\;\\le\\; \\alpha \\cdot \\frac{|\\hat{m}_t|}{\\sqrt{\\hat{v}_t} + \\varepsilon} \\;=\\; |\\Delta x_t^{\\text{Adam}}|\n$$\nwhenever $\\tilde{v}_t \\ge \\hat{v}_t$ and the same $\\hat{m}_t$ is used. This shows per-step updates in AMSGrad are never larger than in Adam for the same $\\hat{m}_t$, and because the denominator is non-decreasing, AMSGrad’s step magnitudes do not exhibit upward spikes caused solely by a shrinking $\\hat{v}_t$.\n\nTo quantify the stability difference, we compute:\n- The maximum update magnitudes $M_{\\text{Adam}}$ and $M_{\\text{AMS}}$,\n- The ratio $R = M_{\\text{Adam}} / M_{\\text{AMS}}$,\n- The standard deviations $\\sigma_{\\text{Adam}}$ and $\\sigma_{\\text{AMS}}$ of $|\\Delta x_t|$ across the run, and the stability gain $\\Delta \\sigma = \\sigma_{\\text{Adam}} - \\sigma_{\\text{AMS}}$,\n- The counts $N_{\\uparrow}^{\\text{Adam}}$ and $N_{\\uparrow}^{\\text{AMS}}$ of steps where the magnitude increases compared to the previous time step.\n\nWe construct four test cases:\n1. Alternating large-small gradients with $g_t = 10$ for odd $t$ and $g_t = 0.1$ for even $t$, with $T = 200$, $\\beta_1 = 0.99$, $\\beta_2 = 0.2$, $\\alpha = 10^{-2}$, $\\varepsilon = 10^{-8}$. This creates frequent large-to-small transitions that decrease $\\hat{v}_t$ quickly in Adam, while AMSGrad stabilizes via $\\tilde{v}_t$.\n2. Constant gradients with $g_t = 1$ for $T = 200$ under the same hyperparameters. Here, both methods should behave similarly, so $R$ should be close to $1$ and $\\Delta \\sigma$ small.\n3. Single spike then small gradients with $g_1 = 50$, $g_t = 0.1$ for $t \\ge 2$, with $T = 200$. Adam’s $\\hat{v}_t$ will decay after the spike, potentially increasing later step magnitudes, while AMSGrad maintains the maximum denominator from the spike, preventing increases.\n4. Short alternating sequence $g = [10, 0.1, 0.1]$ with $T = 3$ to examine bias correction effects at small $t$.\n\nAlgorithmic design for the program:\n- Implement Adam by maintaining $m_t$ and $v_t$ EMAs with bias correction and computing $|\\Delta x_t| = \\alpha |\\hat{m}_t| / (\\sqrt{\\hat{v}_t} + \\varepsilon)$.\n- Implement AMSGrad by computing $\\hat{v}_t$ as in Adam and tracking $\\tilde{v}_t = \\max(\\tilde{v}_{t-1}, \\hat{v}_t)$ for the denominator.\n- For each test case, simulate updates for $t = 1,\\dots,T$, record $|\\Delta x_t|$, update the parameter $x_{t+1} = x_t - \\Delta x_t$ with the signed $\\Delta x_t$ using the sign of $\\hat{m}_t$.\n- Compute the requested stability metrics and aggregate them into the specified output format.\n\nThis approach connects the fundamental EMA definitions to the adaptive update rules, explains why a non-decreasing second-moment denominator in AMSGrad yields more stable updates, and provides a quantitative comparison across carefully designed scenarios that stress the difference in behavior.", "answer": "```python\nimport numpy as np\n\ndef adam_updates(g_seq, lr, beta1, beta2, eps):\n    \"\"\"\n    Compute Adam updates for a scalar parameter given a gradient sequence.\n    Returns:\n        x_T: final parameter value\n        updates: array of signed updates Delta x_t\n    \"\"\"\n    m = 0.0\n    v = 0.0\n    x = 0.0\n    updates = []\n    for t, g in enumerate(g_seq, start=1):\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g * g)\n        # Bias corrections\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        # Signed update uses sign of m_hat\n        step = lr * m_hat / (np.sqrt(v_hat) + eps)\n        # Apply update\n        x = x - step\n        updates.append(step)\n    return x, np.array(updates)\n\n\ndef amsgrad_updates(g_seq, lr, beta1, beta2, eps):\n    \"\"\"\n    Compute AMSGrad updates for a scalar parameter given a gradient sequence.\n    Uses bias-corrected v_hat tracked by a running max for the denominator.\n    Returns:\n        x_T: final parameter value\n        updates: array of signed updates Delta x_t\n    \"\"\"\n    m = 0.0\n    v = 0.0\n    v_hat_max = 0.0\n    x = 0.0\n    updates = []\n    for t, g in enumerate(g_seq, start=1):\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g * g)\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        v_hat_max = max(v_hat_max, v_hat)\n        step = lr * m_hat / (np.sqrt(v_hat_max) + eps)\n        x = x - step\n        updates.append(step)\n    return x, np.array(updates)\n\n\ndef stability_metrics(upd_adam, upd_ams, xT_adam, xT_ams):\n    \"\"\"\n    Compute stability metrics:\n    - R = max(|adam|)/max(|ams|)\n    - M_adam, M_ams (max magnitudes)\n    - |xT_adam|, |xT_ams|\n    - N_increase_adam, N_increase_ams (count of times |update_t| > |update_{t-1}|)\n    - Delta sigma = std(|adam|) - std(|ams|)\n    Returns the list in order:\n    [R, M_adam, M_ams, |xT_adam|, |xT_ams|, N_inc_adam, N_inc_ams, Delta_sigma]\n    Note: The problem statement requests the first seven quantities; we will include\n    Delta_sigma as the seventh element by replacing it in place of counts if needed.\n    But to exactly match the requested seven values, we will not include Delta_sigma\n    in the final list; instead we include both counts. We compute Delta_sigma but do not return it.\n    \"\"\"\n    mag_adam = np.abs(upd_adam)\n    mag_ams = np.abs(upd_ams)\n    M_adam = float(np.max(mag_adam)) if mag_adam.size > 0 else 0.0\n    M_ams = float(np.max(mag_ams)) if mag_ams.size > 0 else 0.0\n    R = float(M_adam / M_ams) if M_ams != 0.0 else float('inf')\n    # Count increases\n    N_inc_adam = int(np.sum(mag_adam[1:] > mag_adam[:-1])) if mag_adam.size > 1 else 0\n    N_inc_ams = int(np.sum(mag_ams[1:] > mag_ams[:-1])) if mag_ams.size > 1 else 0\n    # Final parameter magnitudes\n    xTa = float(abs(xT_adam))\n    xTm = float(abs(xT_ams))\n    # Stability gain (not returned, but computed)\n    sigma_adam = float(np.std(mag_adam)) if mag_adam.size > 0 else 0.0\n    sigma_ams = float(np.std(mag_ams)) if mag_ams.size > 0 else 0.0\n    delta_sigma = sigma_adam - sigma_ams  # computed but not part of final output list per spec\n    return [R, M_adam, M_ams, xTa, xTm, N_inc_adam, N_inc_ams]\n\n\ndef run_case(g_seq, lr=1e-2, beta1=0.99, beta2=0.2, eps=1e-8):\n    xA, updA = adam_updates(g_seq, lr, beta1, beta2, eps)\n    xM, updM = amsgrad_updates(g_seq, lr, beta1, beta2, eps)\n    return stability_metrics(updA, updM, xA, xM)\n\n\ndef build_sequences():\n    # Case 1: Alternating large-small, T=200, g_t = 10 for odd t, 0.1 for even t\n    T1 = 200\n    seq1 = [10.0 if (t % 2 == 1) else 0.1 for t in range(1, T1 + 1)]\n\n    # Case 2: Constant gradients, T=200, g_t = 1\n    T2 = 200\n    seq2 = [1.0] * T2\n\n    # Case 3: Single spike then small, T=200, g_1 = 50, then 0.1\n    T3 = 200\n    seq3 = [50.0] + [0.1] * (T3 - 1)\n\n    # Case 4: Short alternating sequence, g = [10, 0.1, 0.1]\n    seq4 = [10.0, 0.1, 0.1]\n\n    return [\n        (\"case1\", seq1, 1e-2, 0.99, 0.2, 1e-8),\n        (\"case2\", seq2, 1e-2, 0.99, 0.2, 1e-8),\n        (\"case3\", seq3, 1e-2, 0.99, 0.2, 1e-8),\n        (\"case4\", seq4, 1e-2, 0.99, 0.2, 1e-8),\n    ]\n\n\ndef solve():\n    test_cases = build_sequences()\n    results = []\n    for name, seq, lr, b1, b2, eps in test_cases:\n        res = run_case(seq, lr=lr, beta1=b1, beta2=b2, eps=eps)\n        results.append(res)\n    # Print single line with the list of lists\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3095752"}]}