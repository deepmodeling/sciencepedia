{"hands_on_practices": [{"introduction": "The gradient descent algorithm's simplicity is one of its greatest strengths, but it hides subtle complexities. The choice of a constant learning rate, $\\eta$, is critical; too large, and the algorithm can diverge. This exercise explores a more insidious failure mode: a learning rate that is not large enough to cause divergence but is perfectly wrong, trapping the optimizer in a non-convergent limit cycle. By analyzing a simple quadratic potential, you will derive the exact step size that causes the iterates to bounce between two points forever, never reaching the minimum, providing a stark illustration of the importance of learning rate selection [@problem_id:2463052].", "problem": "In computational chemistry, local geometry optimization is often performed by minimizing a potential energy surface (PES). Consider the two-dimensional potential energy surface (PES) that approximates a local quadratic well, given in nondimensionalized units by\n$$\nE(x,y) = \\tfrac{1}{2}\\big(x^{2} + 3\\,y^{2}\\big).\n$$\nApply the steepest descent algorithm (SD) with a constant step size parameter $s$ to minimize $E(x,y)$. By definition, the SD update for the coordinates $\\mathbf{r}_{k} = (x_{k},y_{k})$ is\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s\\,\\nabla E(\\mathbf{r}_{k}).\n$$\nAssume the initial condition $\\mathbf{r}_{0} = (0,y_{0})$ with $y_{0} \\neq 0$.\n\nDetermine the value of the constant step size $s$ (expressed as an exact, simplified fraction) for which the SD iterates are trapped in a non-convergent two-point limit cycle, alternating forever between $(0,y_{0})$ and $(0,-y_{0})$. Use nondimensionalized units and do not include units in your answer. No rounding is required.", "solution": "The problem as stated is well-posed and scientifically sound. We shall proceed with the derivation.\n\nThe potential energy surface (PES) is given by the function:\n$$\nE(x,y) = \\frac{1}{2}(x^{2} + 3y^{2})\n$$\nThe steepest descent (SD) algorithm updates the coordinates $\\mathbf{r}_{k} = (x_{k}, y_{k})$ according to the rule:\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s \\nabla E(\\mathbf{r}_{k})\n$$\nwhere $s$ is the constant step size.\n\nFirst, we must compute the gradient of the energy function, $\\nabla E(x,y)$. The partial derivatives are:\n$$\n\\frac{\\partial E}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = x\n$$\n$$\n\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = 3y\n$$\nThus, the gradient vector is:\n$$\n\\nabla E(x,y) = \\begin{pmatrix} x \\\\ 3y \\end{pmatrix}\n$$\nSubstituting the gradient into the SD update rule, we obtain the component-wise recurrence relations for the coordinates $(x_{k}, y_{k})$:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_{k} \\\\ y_{k} \\end{pmatrix} - s \\begin{pmatrix} x_{k} \\\\ 3y_{k} \\end{pmatrix}\n$$\nThis yields two independent equations:\n$$\nx_{k+1} = x_{k} - s x_{k} = (1 - s) x_{k}\n$$\n$$\ny_{k+1} = y_{k} - s (3y_{k}) = (1 - 3s) y_{k}\n$$\nThe initial condition is given as $\\mathbf{r}_{0} = (0, y_{0})$, where $y_{0} \\neq 0$.\n\nLet us analyze the behavior of the $x$-coordinate. With $x_{0} = 0$, the recurrence relation for $x$ gives:\n$$\nx_{1} = (1-s)x_{0} = (1-s)(0) = 0\n$$\nBy induction, it is trivial to show that $x_{k} = 0$ for all integers $k \\ge 0$. This is consistent with the specified limit cycle points $(0, y_{0})$ and $(0, -y_{0})$, which lie on the $y$-axis.\n\nNow, we analyze the behavior of the $y$-coordinate. The problem states that the system enters a two-point limit cycle, alternating between $(0, y_{0})$ and $(0, -y_{0})$. This implies that for even $k$, $\\mathbf{r}_{k} = (0, y_{0})$, and for odd $k$, $\\mathbf{r}_{k} = (0, -y_{0})$.\n\nLet's consider the first step of the iteration, from $k=0$ to $k=1$.\nWe start at $\\mathbf{r}_{0} = (0, y_{0})$. The next point must be $\\mathbf{r}_{1} = (0, -y_{0})$.\nUsing the recurrence relation for $y$ at $k=0$:\n$$\ny_{1} = (1 - 3s) y_{0}\n$$\nFor the iteration to match the limit cycle, we require $y_{1} = -y_{0}$. Therefore:\n$$\n-y_{0} = (1 - 3s) y_{0}\n$$\nSince it is given that $y_{0} \\neq 0$, we can divide both sides by $y_{0}$:\n$$\n-1 = 1 - 3s\n$$\nSolving this equation for the step size $s$:\n$$\n3s = 1 - (-1) = 2\n$$\n$$\ns = \\frac{2}{3}\n$$\nWe must verify that this value of $s$ sustains the limit cycle indefinitely. If $s = \\frac{2}{3}$, the recurrence for $y$ becomes:\n$$\ny_{k+1} = \\left(1 - 3 \\cdot \\frac{2}{3}\\right) y_{k} = (1 - 2) y_{k} = -y_{k}\n$$\nLet's examine the sequence of iterates:\nFor $k=0$: $\\mathbf{r}_{0} = (0, y_{0})$.\nFor $k=1$: $y_{1} = -y_{0}$, so $\\mathbf{r}_{1} = (0, -y_{0})$.\nFor $k=2$: $y_{2} = -y_{1} = -(-y_{0}) = y_{0}$, so $\\mathbf{r}_{2} = (0, y_{0})$.\nFor $k=3$: $y_{3} = -y_{2} = -y_{0}$, so $\\mathbf{r}_{3} = (0, -y_{0})$.\nThe sequence of points is $(0, y_{0}), (0, -y_{0}), (0, y_{0}), (0, -y_{0}), \\dots$, which is precisely the specified two-point limit cycle. The algorithm never converges to the minimum at $(0,0)$.\n\nThe value of the constant step size $s$ that produces this behavior is $\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "2463052"}, {"introduction": "In many real-world optimization problems, particularly in deep learning, the loss landscape features long, narrow valleys. Standard gradient descent struggles in this \"ill-conditioned\" terrain, taking tiny steps along the valley floor while oscillating across its steep walls. This practice introduces the Heavy-Ball momentum method, an enhancement designed to accelerate progress in such scenarios. You will analytically derive the optimal parameters for both standard gradient descent and the momentum-based method, quantitatively demonstrating the significant convergence speed-up that momentum provides [@problem_id:3186112].", "problem": "Consider the strictly convex quadratic objective $f(x,y) = \\frac{1}{2}\\left(a x^{2} + b y^{2}\\right)$ with $a  b  0$ and $a \\gg b$. The gradient is $\\nabla f(x,y) = \\left(a x, b y\\right)$ and the Hessian is diagonal with eigenvalues $a$ and $b$. You will compare Gradient Descent (GD) and the Heavy-Ball momentum method (HB) on this objective from first principles.\n\nStarting only from the definitions of the iterative algorithms\n$$(\\text{GD})\\quad \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}),$$\n$$(\\text{HB})\\quad \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) + \\beta\\left(\\mathbf{z}_{t} - \\mathbf{z}_{t-1}\\right),$$\nwhere $\\mathbf{z}_{t} = (x_{t}, y_{t})$, derive and analyze the one-dimensional recursions induced along each eigen-direction of the Hessian.\n\nYour tasks are:\n1. For GD, derive the scalar contraction factor along an eigenvalue $\\lambda \\in \\{a,b\\}$ and determine the learning rate $\\eta$ that minimizes the worst-case spectral radius over the set $\\{a,b\\}$. Express your result in terms of $a$ and $b$.\n2. For HB, derive the scalar second-order recursion along an eigenvalue $\\lambda \\in \\{a,b\\}$, and then select $\\eta$ and $\\beta$ to minimize the worst-case spectral radius across the set $\\{a,b\\}$ subject to stability. Use only the algorithm definitions above and fundamental linear dynamical system reasoning. Express your optimal $\\eta$ and $\\beta$ as closed-form analytic expressions in terms of $a$ and $b$, and state the corresponding minimized worst-case spectral radius achieved by HB as a function of $a$ and $b$.\n3. Discuss, in words, the overshoot and oscillation tradeoffs that arise when $a \\gg b$, including how $\\beta$ and $\\eta$ influence trajectory behavior in a narrow valley, and propose a concrete experiment: specify $(a,b)$ with $a \\gg b$, an initialization $(x_{0},y_{0})$ and $(x_{-1},y_{-1})$, and what metrics you would record to compare GD versus HB under your derived optimal parameters.\n\nReport your final answer as the optimal Heavy-Ball parameter pair $(\\eta^{\\star}, \\beta^{\\star})$ in terms of $a$ and $b$. No rounding is required. Format the pair as a $1 \\times 2$ row matrix using the LaTeX $\\texttt{pmatrix}$ environment.", "solution": "The problem asks for an analysis of Gradient Descent (GD) and the Heavy-Ball (HB) momentum method on a strictly convex quadratic objective function $f(x,y) = \\frac{1}{2}(ax^2 + by^2)$ where $a  b  0$. The analysis is to be performed from first principles by studying the one-dimensional recursions along the eigen-directions of the Hessian matrix.\n\nThe objective function can be written in vector form as $f(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^T H \\mathbf{z}$, where $\\mathbf{z} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ and $H = \\begin{pmatrix} a  0 \\\\ 0  b \\end{pmatrix}$. The gradient is $\\nabla f(\\mathbf{z}) = H\\mathbf{z}$. The eigenvalues of the Hessian $H$ are $\\lambda_1 = a$ and $\\lambda_2 = b$, with corresponding eigenvectors along the coordinate axes. The dynamics of the optimization algorithms decouple along these axes. We can analyze the convergence by studying the scalar recursion for a generic eigenvalue $\\lambda \\in \\{a,b\\}$.\n\n### 1. Gradient Descent (GD) Analysis\n\nThe GD update rule is given by:\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) $$\nSubstituting $\\nabla f(\\mathbf{z}_t) = H\\mathbf{z}_t$, we get:\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_t - \\eta H \\mathbf{z}_t = (I - \\eta H) \\mathbf{z}_t $$\nThis is a linear dynamical system. For a component of $\\mathbf{z}_t$ along an eigenvector corresponding to an eigenvalue $\\lambda$ of $H$, the scalar update rule is:\n$$ z_{t+1}^{(\\lambda)} = (1 - \\eta \\lambda) z_t^{(\\lambda)} $$\nThe term $c(\\eta, \\lambda) = 1 - \\eta\\lambda$ is the contraction factor for this component. For an iterative method to converge, the magnitude of its contraction factor must be less than $1$ for all modes. The overall convergence rate is determined by the largest magnitude, which is the spectral radius of the iteration matrix $I - \\eta H$:\n$$ \\rho(I - \\eta H) = \\max_{\\lambda \\in \\{a, b\\}} |1 - \\eta \\lambda| $$\nFor convergence, we require $\\rho  1$, which implies $|1 - \\eta\\lambda|  1$ for both $\\lambda=a$ and $\\lambda=b$. This means $-1  1 - \\eta\\lambda  1$, which simplifies to $0  \\eta\\lambda  2$. Since $ab0$, this must hold for $\\lambda=a$, giving the stability condition $0  \\eta  \\frac{2}{a}$.\n\nOur goal is to find the learning rate $\\eta$ that minimizes the worst-case spectral radius:\n$$ \\eta^{\\star} = \\arg\\min_{\\eta} \\max(|1 - \\eta a|, |1 - \\eta b|) $$\nThe minimum of the maximum of two linear functions of $\\eta$ occurs when their absolute values are equal. That is, $|1 - \\eta a| = |1 - \\eta b|$. Since $\\eta  0$ and $a  b$, $1-\\eta a  1-\\eta b$. For their magnitudes to be equal, we must have:\n$$ 1 - \\eta b = -(1 - \\eta a) = \\eta a - 1 $$\nSolving for $\\eta$:\n$$ 2 = \\eta a + \\eta b = \\eta(a+b) $$\n$$ \\eta_{GD}^{\\star} = \\frac{2}{a+b} $$\nThis value of $\\eta$ is within the stability range since $a+b  a$, so $\\frac{2}{a+b}  \\frac{2}{a}$. The corresponding minimal spectral radius is:\n$$ \\rho_{GD} = \\left|1 - b \\left(\\frac{2}{a+b}\\right)\\right| = \\left|\\frac{a+b-2b}{a+b}\\right| = \\frac{a-b}{a+b} $$\nIn terms of the condition number $\\kappa = a/b$, this is $\\rho_{GD} = \\frac{\\kappa-1}{\\kappa+1}$.\n\n### 2. Heavy-Ball (HB) Momentum Analysis\n\nThe HB update rule is:\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) + \\beta(\\mathbf{z}_{t} - \\mathbf{z}_{t-1}) $$\nSubstituting $\\nabla f(\\mathbf{z}_t) = H\\mathbf{z}_t$ and considering the scalar recursion for a mode corresponding to eigenvalue $\\lambda$:\n$$ z_{t+1} = z_t - \\eta \\lambda z_t + \\beta(z_t - z_{t-1}) $$\n$$ z_{t+1} = (1 - \\eta\\lambda + \\beta)z_t - \\beta z_{t-1} $$\nThis is a second-order linear homogeneous recurrence relation. Its dynamics are governed by the roots of its characteristic polynomial:\n$$ \\mu^2 - (1 - \\eta\\lambda + \\beta)\\mu + \\beta = 0 $$\nFor the system to be stable, the magnitudes of both roots, $\\mu_1$ and $\\mu_2$, must be less than $1$. The convergence rate is determined by $\\rho_\\lambda = \\max(|\\mu_1|, |\\mu_2|)$. The roots are:\n$$ \\mu = \\frac{1 - \\eta\\lambda + \\beta \\pm \\sqrt{(1 - \\eta\\lambda + \\beta)^2 - 4\\beta}}{2} $$\nThe nature of the roots depends on the discriminant $D = (1 - \\eta\\lambda + \\beta)^2 - 4\\beta$.\nIf $D \\ge 0$, the roots are real, and the spectral radius is $\\rho_\\lambda = \\frac{|1 - \\eta\\lambda + \\beta| + \\sqrt{D}}{2}$.\nIf $D  0$, the roots are a complex conjugate pair, and their magnitude is $|\\mu| = \\sqrt{\\text{product of roots}} = \\sqrt{\\beta}$. The spectral radius is $\\rho_\\lambda = \\sqrt{\\beta}$.\n\nTo minimize the worst-case spectral radius $\\max_{\\lambda \\in \\{a,b\\}} \\rho_\\lambda$, we seek to make the spectral radii for both $\\lambda=a$ and $\\lambda=b$ equal and as small as possible. The optimal solution occurs when the dynamics for both eigenvalues are in the complex root regime and on the boundary of the real root regime, i.e., when the discriminant is non-positive, and the spectral radius is $\\sqrt{\\beta}$ for both. This requires:\n$$ (1 - \\eta\\lambda + \\beta)^2 - 4\\beta \\le 0 \\quad \\text{for } \\lambda \\in \\{a,b\\} $$\n$$ |1 - \\eta\\lambda + \\beta| \\le 2\\sqrt{\\beta} $$\nThis is equivalent to $-2\\sqrt{\\beta} \\le 1 - \\eta\\lambda + \\beta \\le 2\\sqrt{\\beta}$, which rearranges to:\n$$ (1-\\sqrt{\\beta})^2 \\le \\eta\\lambda \\le (1+\\sqrt{\\beta})^2 $$\nTo minimize $\\beta$ (and thus the spectral radius $\\sqrt{\\beta}$), we want this interval to be as small as possible while still containing $[\\eta b, \\eta a]$. This is achieved by setting the endpoints of the intervals to match:\n$$ \\eta b = (1 - \\sqrt{\\beta})^2 $$\n$$ \\eta a = (1 + \\sqrt{\\beta})^2 $$\nDividing the second equation by the first gives:\n$$ \\frac{a}{b} = \\left(\\frac{1 + \\sqrt{\\beta}}{1 - \\sqrt{\\beta}}\\right)^2 $$\nTaking the square root and solving for $\\sqrt{\\beta}$:\n$$ \\sqrt{\\frac{a}{b}} = \\frac{1 + \\sqrt{\\beta}}{1 - \\sqrt{\\beta}} \\implies \\sqrt{a}(1-\\sqrt{\\beta}) = \\sqrt{b}(1+\\sqrt{\\beta}) $$\n$$ \\sqrt{a} - \\sqrt{a}\\sqrt{\\beta} = \\sqrt{b} + \\sqrt{b}\\sqrt{\\beta} $$\n$$ \\sqrt{a} - \\sqrt{b} = (\\sqrt{a} + \\sqrt{b})\\sqrt{\\beta} $$\nThis yields $\\sqrt{\\beta^{\\star}} = \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}$. Since $ab0$, we have $0  \\sqrt{\\beta^{\\star}}  1$, which is a valid parameter. The optimal momentum parameter is:\n$$ \\beta^{\\star} = \\left(\\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}\\right)^2 $$\nNow, we find the optimal learning rate $\\eta^{\\star}$ using $\\eta a = (1+\\sqrt{\\beta})^2$. First, we compute $1+\\sqrt{\\beta^{\\star}}$:\n$$ 1+\\sqrt{\\beta^{\\star}} = 1 + \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}} = \\frac{(\\sqrt{a}+\\sqrt{b}) + (\\sqrt{a}-\\sqrt{b})}{\\sqrt{a}+\\sqrt{b}} = \\frac{2\\sqrt{a}}{\\sqrt{a}+\\sqrt{b}} $$\nSubstituting this into the equation for $\\eta$:\n$$ \\eta^{\\star} a = \\left(\\frac{2\\sqrt{a}}{\\sqrt{a}+\\sqrt{b}}\\right)^2 = \\frac{4a}{(\\sqrt{a}+\\sqrt{b})^2} $$\n$$ \\eta^{\\star} = \\frac{4}{(\\sqrt{a}+\\sqrt{b})^2} $$\nWith these optimal parameters, the spectral radius for both modes is $\\sqrt{\\beta^{\\star}}$, so the worst-case spectral radius for HB is:\n$$ \\rho_{HB} = \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}} $$\nIn terms of the condition number $\\kappa=a/b$, this is $\\rho_{HB} = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$. This rate is significantly better than GD's rate of $\\frac{\\kappa-1}{\\kappa+1}$, especially for large $\\kappa$.\n\n### 3. Discussion and Experiment Proposal\n\nWhen $a \\gg b$, the objective function $f(x,y)$ has a long, narrow valley shape. The condition number $\\kappa = a/b$ is large. The contours of $f$ are highly elongated ellipses. The gradient is steep in the $x$-direction (eigenvalue $a$) and shallow in the $y$-direction (eigenvalue $b$).\n\nFor GD, the optimal learning rate $\\eta_{GD}^{\\star} = \\frac{2}{a+b} \\approx \\frac{2}{a}$ is very small, constrained by the largest eigenvalue $a$ to ensure stability. This small learning rate leads to very slow progress along the flat valley ($y$-direction), as the update $- \\eta \\nabla_y f = -\\eta b y$ is tiny. The algorithm exhibits a characteristic zig-zagging behavior, oscillating across the narrow valley while slowly crawling along its floor toward the minimum.\n\nFor HB, the momentum term $\\beta(\\mathbf{z}_t - \\mathbf{z}_{t-1})$ acts as a memory of past updates. For the optimal parameters with $a \\gg b$, we have $\\beta^{\\star} \\approx (1 - 2\\sqrt{b/a})^2 \\approx 1 - 4\\sqrt{b/a}$, which is close to $1$. This high value of $\\beta$ ensures that updates are averaged over many steps. The oscillating gradient components across the valley (in the $x$-direction) tend to cancel out over time, while the consistent, small gradient components along the valley (in the $y$-direction) accumulate, building up velocity and accelerating progress. The optimal learning rate $\\eta_{HB}^{\\star} = \\frac{4}{(\\sqrt{a}+\\sqrt{b})^2} \\approx \\frac{4}{a}$ is roughly twice that of GD. This larger learning rate is stabilized by the momentum term. The trade-off is potential overshoot; the momentum can cause the iterate to \"fly past\" the minimum and oscillate around it with a larger amplitude than GD, but the overall trajectory is smoother and convergence is much faster.\n\nA concrete experiment to compare the methods would be as follows:\n- **Problem Setup**: Let $a=100$ and $b=1$. This gives a condition number of $\\kappa=100$. The objective is $f(x,y) = 50x^2 + 0.5y^2$.\n- **Algorithm Parameters**:\n  - GD: $\\eta_{GD}^{\\star} = \\frac{2}{100+1} = \\frac{2}{101} \\approx 0.0198$.\n  - HB:\n    - $\\beta^{\\star} = \\left(\\frac{\\sqrt{100}-\\sqrt{1}}{\\sqrt{100}+\\sqrt{1}}\\right)^2 = \\left(\\frac{9}{11}\\right)^2 = \\frac{81}{121} \\approx 0.6694$.\n    - $\\eta^{\\star} = \\frac{4}{(\\sqrt{100}+\\sqrt{1})^2} = \\frac{4}{11^2} = \\frac{4}{121} \\approx 0.0331$.\n- **Initialization**: Start at a point that is off-center in the steep direction and far from the minimum in the shallow direction, e.g., $\\mathbf{z}_0 = (x_0, y_0) = (1, 10)$. For HB, initialize with zero velocity by setting $\\mathbf{z}_{-1} = \\mathbf{z}_0$.\n- **Metrics**: Run both algorithms for a fixed number of iterations (e.g., $N=100$). For each iteration $t=0, 1, \\dots, N$, record:\n  1. The iterate position $\\mathbf{z}_t = (x_t, y_t)$.\n  2. The objective function value $f(\\mathbf{z}_t)$.\n  3. The norm of the gradient $\\|\\nabla f(\\mathbf{z}_t)\\|_2$.\n- **Analysis**:\n  - Plot the trajectories $(\\mathbf{z}_0, \\mathbf{z}_1, \\dots, \\mathbf{z}_N)$ for both GD and HB on a 2D contour plot of $f(x,y)$. This will visualize the zig-zagging of GD versus the smoother, faster path of HB.\n  - Plot $f(\\mathbf{z}_t)$ versus $t$ for both methods on a semi-logarithmic scale (logarithm of $f$ vs. linear $t$). This will clearly show the linear convergence rates and demonstrate that HB's convergence slope is much steeper, indicating faster convergence.\n  - Plot $\\|\\nabla f(\\mathbf{z}_t)\\|_2$ versus $t$ on a semi-log scale, which provides another view of the convergence to the critical point.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{(\\sqrt{a}+\\sqrt{b})^{2}}  \\left(\\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}\\right)^{2} \\end{pmatrix}}\n$$", "id": "3186112"}, {"introduction": "The loss landscapes of modern neural networks are complex, high-dimensional surfaces. It's now understood that these landscapes are dominated not by local minima, but by saddle points, which can stall deterministic optimization algorithms. This exercise investigates how Stochastic Gradient Descent (SGD) overcomes this challenge. You will analyze a toy model of a saddle point and discover how the inherent noise in SGD provides a natural mechanism for escape, a crucial property for successful training in deep learning. By deriving the expected \"escape time,\" you will quantify the relationship between noise, learning rate, and the geometry of the saddle point [@problem_id:3186081].", "problem": "Consider the quadratic toy loss function in two dimensions defined by $L(x,y) = \\frac{1}{2}\\big(\\lambda_{p} x^{2} - \\lambda_{n} y^{2}\\big)$, where $\\lambda_{p}  0$ and $\\lambda_{n}  0$ are constants. The Hessian matrix at the origin is $\\nabla^{2}L(0,0) = \\operatorname{diag}(\\lambda_{p}, -\\lambda_{n})$, which has both positive and negative eigenvalues, making $(0,0)$ a saddle point. Let the optimization dynamics be discrete-time Stochastic Gradient Descent (SGD), defined by the update\n$$\n\\begin{pmatrix}\nx_{t+1} \\\\\ny_{t+1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}\n- \\eta \\left( \\nabla L(x_{t}, y_{t}) + \\xi_{t} \\right),\n$$\nwhere $\\eta  0$ is the step size, and $\\xi_{t} \\in \\mathbb{R}^{2}$ is an independent and identically distributed zero-mean Gaussian noise vector at each step with covariance matrix $\\sigma^{2} I$, that is, $\\xi_{t} \\sim \\mathcal{N}(0, \\sigma^{2} I)$ with variance parameter $\\sigma^{2} \\ge 0$. Assume the process starts exactly at the saddle point, $(x_{0}, y_{0}) = (0,0)$.\n\nYour tasks are:\n- Starting from core definitions of gradient, Hessian, and the SGD update, linearize the dynamics around $(0,0)$ (which is exact for this quadratic $L$), and use the resulting linear stochastic recurrence to derive how the expected squared Euclidean norm $\\mathbb{E}\\big[\\| (x_{t}, y_{t}) \\|^{2}\\big]$ evolves with $t$.\n- Define the escape time $T$ to be the smallest nonnegative integer such that $\\mathbb{E}\\big[\\| (x_{T}, y_{T}) \\|^{2}\\big] \\ge r_{*}^{2}$ for a given threshold $r_{*}  0$. Explain the dependence of $T$ on the noise variance $\\sigma^{2}$, the step size $\\eta$, and the curvature parameters $\\lambda_{p}$ and $\\lambda_{n}$.\n- Clarify how $T$ scales with $\\eta$ when $\\eta$ is small but positive, and the negative-curvature direction drives the escape from the saddle.\n\nImplement a complete, runnable program that, for each parameter set in the test suite below, computes the escape time $T$ under the exact linearized SGD dynamics specified above, using the definition of $T$ in terms of the expected squared norm. The program must perform no random sampling; it must compute the expected quantity deterministically from the derived recurrences. If $\\sigma^{2} = 0$ and the initialization is at $(0,0)$, the process does not leave the saddle deterministically, so in that case the program must output $-1$ for $T$. If $r_{*} \\le 0$, define $T = 0$.\n\nUse the following test suite of parameter values (each tuple is $(\\lambda_{p}, \\lambda_{n}, \\eta, \\sigma^{2}, r_{*})$):\n- Test $1$: $(\\lambda_{p} = 1.0, \\lambda_{n} = 1.0, \\eta = 0.05, \\sigma^{2} = 0.1, r_{*} = 0.5)$.\n- Test $2$: $(\\lambda_{p} = 1.0, \\lambda_{n} = 1.0, \\eta = 0.01, \\sigma^{2} = 0.1, r_{*} = 0.5)$.\n- Test $3$: $(\\lambda_{p} = 1.0, \\lambda_{n} = 2.0, \\eta = 0.05, \\sigma^{2} = 0.1, r_{*} = 0.5)$.\n- Test $4$ (edge case, no noise): $(\\lambda_{p} = 1.0, \\lambda_{n} = 1.0, \\eta = 0.05, \\sigma^{2} = 0.0, r_{*} = 0.5)$.\n- Test $5$ (near stability boundary in the positive-curvature direction): $(\\lambda_{p} = 10.0, \\lambda_{n} = 0.5, \\eta = 0.19, \\sigma^{2} = 0.05, r_{*} = 0.4)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests above, for example, $[T_{1},T_{2},T_{3},T_{4},T_{5}]$. Each $T_{i}$ must be an integer. No other text should be printed.", "solution": "The problem is valid. It is scientifically grounded, well-posed, and objective. It presents a canonical model for analyzing the behavior of Stochastic Gradient Descent (SGD) near saddle points, a fundamental topic in the theory of deep learning and non-convex optimization. All parameters and conditions are formally defined, allowing for a unique and meaningful solution.\n\nWe begin by deriving the dynamics of the system and the evolution of the expected squared norm of the state vector.\n\nThe loss function is given by $L(x,y) = \\frac{1}{2}(\\lambda_{p} x^{2} - \\lambda_{n} y^{2})$, where $\\lambda_{p}  0$ and $\\lambda_{n}  0$.\nThe gradient of the loss function, $\\nabla L(x, y)$, is:\n$$\n\\nabla L(x, y) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x} \\\\ \\frac{\\partial L}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{p} x \\\\ -\\lambda_{n} y \\end{pmatrix} = \\begin{pmatrix} \\lambda_{p}  0 \\\\ 0  -\\lambda_{n} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n$$\nThis is the Hessian matrix at the origin, $\\nabla^2 L(0,0) = \\operatorname{diag}(\\lambda_p, -\\lambda_n)$, multiplied by the state vector. As the loss function is quadratic, the gradient is linear, and no approximation is needed.\n\nThe SGD update rule is given by:\n$$\nz_{t+1} = z_{t} - \\eta (\\nabla L(z_{t}) + \\xi_{t})\n$$\nwhere $z_t = \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix}$ and $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2 I)$. Substituting the gradient, we get:\n$$\nz_{t+1} = z_{t} - \\eta \\left( \\begin{pmatrix} \\lambda_{p}  0 \\\\ 0  -\\lambda_{n} \\end{pmatrix} z_t + \\xi_t \\right) = \\left( I - \\eta \\begin{pmatrix} \\lambda_{p}  0 \\\\ 0  -\\lambda_{n} \\end{pmatrix} \\right) z_t - \\eta \\xi_t\n$$\nThis results in the following decoupled linear recurrences for the components $x_t$ and $y_t$:\n$$\nx_{t+1} = (1 - \\eta \\lambda_{p}) x_t - \\eta \\xi_{x,t}\n$$\n$$\ny_{t+1} = (1 + \\eta \\lambda_{n}) y_t - \\eta \\xi_{y,t}\n$$\nwhere $\\xi_t = \\begin{pmatrix} \\xi_{x,t} \\\\ \\xi_{y,t} \\end{pmatrix}$, and $\\xi_{x,t}$ and $\\xi_{y,t}$ are independent zero-mean Gaussian random variables with variance $\\sigma^2$.\n\nWe are interested in the evolution of the expected squared Euclidean norm, $E_t = \\mathbb{E}[\\|z_t\\|^2] = \\mathbb{E}[x_t^2 + y_t^2]$. By linearity of expectation, this is $E_t = \\mathbb{E}[x_t^2] + \\mathbb{E}[y_t^2]$. Let us define $u_t = \\mathbb{E}[x_t^2]$ and $v_t = \\mathbb{E}[y_t^2]$.\n\nFor $u_t$, we have:\n$$\nu_{t+1} = \\mathbb{E}[x_{t+1}^2] = \\mathbb{E}[((1 - \\eta \\lambda_{p}) x_t - \\eta \\xi_{x,t})^2]\n$$\n$$\nu_{t+1} = \\mathbb{E}[(1 - \\eta \\lambda_{p})^2 x_t^2 - 2\\eta(1 - \\eta \\lambda_{p}) x_t \\xi_{x,t} + \\eta^2 \\xi_{x,t}^2]\n$$\nUsing the linearity of expectation and the fact that $\\xi_{x,t}$ is independent of $x_t$ with $\\mathbb{E}[\\xi_{x,t}] = 0$: $\\mathbb{E}[x_t \\xi_{x,t}] = \\mathbb{E}[x_t]\\mathbb{E}[\\xi_{x,t}] = 0$. Also, $\\mathbb{E}[\\xi_{x,t}^2] = \\sigma^2$. This yields the recurrence for $u_t$:\n$$\nu_{t+1} = (1 - \\eta \\lambda_{p})^2 u_t + \\eta^2 \\sigma^2\n$$\nBy an identical argument for the $y$ component, we find the recurrence for $v_t$:\n$$\nv_{t+1} = (1 + \\eta \\lambda_{n})^2 v_t + \\eta^2 \\sigma^2\n$$\nThe process starts at the origin, $(x_0, y_0) = (0,0)$, so the initial conditions are $u_0 = \\mathbb{E}[x_0^2] = 0$ and $v_0 = \\mathbb{E}[y_0^2] = 0$.\n\nThe escape time $T$ is the smallest non-negative integer $t$ such that $E_t = u_t + v_t \\ge r_*^2$. We can compute $T$ by iterating these recurrences starting from $t=0$ until the condition is met.\n\nNow, we analyze the behavior of $T$. The recurrence for $u_t$ is associated with the positive curvature direction $\\lambda_p$. For typical learning rates satisfying the stability condition $\\eta  2/\\lambda_p$, the multiplicative factor $c_x = (1 - \\eta \\lambda_p)^2$ is less than $1$. Consequently, $u_t$ converges to a finite steady-state value: $u_\\infty = \\frac{\\eta^2 \\sigma^2}{1 - (1-\\eta\\lambda_p)^2}$.\nIn contrast, the recurrence for $v_t$ corresponds to the negative curvature direction $-\\lambda_n$. The factor $c_y = (1 + \\eta \\lambda_n)^2$ is always greater than $1$ for $\\eta  0$ and $\\lambda_n  0$. This means $v_t$ grows exponentially without bound, driving the escape from the saddle point.\n\nThe escape dynamics are therefore dominated by the $y$ component. For large $t$, $E_t \\approx v_t$. The closed-form solution for the recurrence $v_{t+1} = c_y v_t + d$ with $v_0=0$ is $v_t = d \\frac{c_y^t - 1}{c_y - 1}$. Substituting $c_y=(1+\\eta\\lambda_n)^2$ and $d=\\eta^2\\sigma^2$, we have:\n$$\nE_T \\approx v_T = \\eta^2\\sigma^2 \\frac{((1+\\eta\\lambda_n)^2)^T - 1}{(1+\\eta\\lambda_n)^2 - 1} \\ge r_*^2\n$$\nSolving for $T$ provides insight into its dependencies:\n$$\nT \\approx \\frac{1}{2\\ln(1 + \\eta \\lambda_n)} \\ln\\left( \\frac{r_*^2 ((1+\\eta\\lambda_n)^2-1)}{\\eta^2\\sigma^2} + 1 \\right)\n$$\n- **Dependence on $\\sigma^2$**: $T$ is approximately proportional to $\\ln(1/\\sigma^2)$. Lower noise variance leads to a logarithmically longer escape time, as the random perturbations that initiate the escape are smaller.\n- **Dependence on $\\eta$**: For small $\\eta  0$, we can use the approximations $\\ln(1 + \\eta\\lambda_n) \\approx \\eta\\lambda_n$ and $(1+\\eta\\lambda_n)^2-1 \\approx 2\\eta\\lambda_n$. The escape time scales as:\n$$\nT \\propto \\frac{1}{\\eta\\lambda_n} \\ln\\left(\\frac{1}{\\eta\\sigma^2}\\right)\n$$\nThis shows that as the step size $\\eta$ becomes smaller, the escape time increases, primarily due to the $\\frac{1}{\\eta}$ prefactor.\n- **Dependence on curvatures**: The escape is driven by the negative curvature $\\lambda_n$. A larger magnitude of negative curvature (larger $\\lambda_n$) leads to faster exponential growth and thus a smaller escape time $T$. The positive curvature $\\lambda_p$ only influences a bounded component of the expected norm and has a negligible effect on escape for sufficiently large $r_*$.\n\nThe algorithm to compute $T$ for a given parameter set is:\n1. Handle edge cases: if $r_* \\le 0$, return $T=0$. If $\\sigma^2 = 0$, the particle never leaves the origin, so return $T=-1$.\n2. Initialize time $t=0$, and expected squared components $u=0$ and $v=0$.\n3. Pre-compute constants: $c_x = (1 - \\eta \\lambda_p)^2$, $c_y = (1 + \\eta \\lambda_n)^2$, and the noise term $d = \\eta^2 \\sigma^2$. Also compute the threshold $r_*^2$.\n4. Loop through time steps:\n   a. At step $t$, check if $u+v \\ge r_*^2$. If so, $T=t$ is found; terminate and return $t$.\n   b. Otherwise, update the expected squared components for the next step: $u \\leftarrow c_x u + d$ and $v \\leftarrow c_y v + d$.\n   c. Increment time $t \\leftarrow t+1$.\n\nThis iterative approach is numerically stable and directly implements the derived dynamics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the escape time from a saddle point for SGD on a quadratic potential.\n    \"\"\"\n\n    # Each tuple is (lambda_p, lambda_n, eta, sigma_sq, r_star)\n    test_cases = [\n        # Test 1\n        (1.0, 1.0, 0.05, 0.1, 0.5),\n        # Test 2\n        (1.0, 1.0, 0.01, 0.1, 0.5),\n        # Test 3\n        (1.0, 2.0, 0.05, 0.1, 0.5),\n        # Test 4 (edge case, no noise)\n        (1.0, 1.0, 0.05, 0.0, 0.5),\n        # Test 5 (near stability boundary)\n        (10.0, 0.5, 0.19, 0.05, 0.4),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        lambda_p, lambda_n, eta, sigma_sq, r_star = case\n\n        # Per problem specification, if r_* = 0, T = 0.\n        if r_star = 0:\n            results.append(0)\n            continue\n\n        # Per problem specification, with no noise and starting at the origin,\n        # the system never escapes. T is -1.\n        if sigma_sq == 0.0:\n            results.append(-1)\n            continue\n\n        r_star_sq = r_star**2\n        \n        # Expected squared value for the x-component\n        u_t = 0.0\n        # Expected squared value for the y-component\n        v_t = 0.0\n        \n        # Multiplicative factors for the recurrences\n        c_x = (1.0 - eta * lambda_p)**2\n        c_y = (1.0 + eta * lambda_n)**2\n        \n        # Additive noise term in the recurrences\n        noise_term = eta**2 * sigma_sq\n        \n        t = 0\n        while True:\n            # Check the escape condition at the beginning of the loop for t=0\n            if u_t + v_t = r_star_sq:\n                results.append(t)\n                break\n            \n            # Update for the next time step\n            u_t = c_x * u_t + noise_term\n            v_t = c_y * v_t + noise_term\n            t += 1\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186081"}]}