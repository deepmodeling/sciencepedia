{"hands_on_practices": [{"introduction": "This first exercise provides a hands-on demonstration of the core principle behind cyclical learning rates with warm restarts. By using a simple, one-dimensional non-convex loss function, you will directly observe how periodically increasing the learning rate can provide the necessary \"kick\" to escape a shallow local minimum, a feat often impossible for optimizers using traditional, monotonically decreasing learning rates. This practice builds the fundamental intuition for why techniques like Stochastic Gradient Descent with Restarts (SGDR) are so effective in navigating the complex landscapes of modern neural networks [@problem_id:3110220].", "problem": "You are given a synthetic one-dimensional nonconvex loss defined by the quartic polynomial $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$ for real parameters $a$ and $b$. Consider deterministic gradient descent with parameter updates governed by the rule $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$, where the scalar learning rate at step $t$ is $\\eta_t0$ and the gradient is $\\nabla f(\\theta)=\\frac{d}{d\\theta}f(\\theta)$. The aim is to examine, from first principles, how the choice of learning rate schedule affects the ability of gradient descent to make progress on a nonconvex landscape that can exhibit both shallow and deep minima.\n\nYour tasks:\n\n1. Starting from the definition of the loss and elementary calculus, derive the analytical gradient $\\nabla f(\\theta)$ needed for deterministic gradient descent.\n\n2. Starting from the definition of a learning rate sequence $\\{\\eta_t\\}_{t=0}^{T-1}$ over a period of length $T$, and the constraints that it be smooth, have zero slope at the boundaries (to avoid abrupt changes), attain a maximum $\\eta_{\\max}$ at the start of each period, and attain a minimum $\\eta_{\\min}$ at the end of each period, derive a learning rate schedule over one period that satisfies these constraints and then define a warm-restart mechanism that repeats this schedule every $T$ steps. Angles for any trigonometric functions must be expressed in radians.\n\n3. Define a contrasting monotone learning rate schedule $\\eta_t$ that strictly decreases over time according to a well-tested rule. Use this to perform deterministic gradient descent without restarts.\n\n4. Implement both schedules and run deterministic gradient descent for a fixed number of epochs. For each test case in the suite below, compute the final losses obtained under the warm-restart schedule and under the monotone schedule. For each case, return a boolean that is true if and only if the final loss under warm restarts is strictly smaller than the final loss under monotone decay by at least a margin $\\varepsilon$ (use $\\varepsilon=10^{-2}$), and false otherwise. This boolean is intended to operationalize the statement that warm restarts can enable escape from shallow minima compared to monotone decay, in the sense of achieving a strictly lower final objective value after the same number of steps.\n\nUse the following test suite. Each test case is a tuple specifying $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)$, where $a$ and $b$ parameterize the loss $f(\\theta)$, $\\theta_0$ is the initial parameter value, $E$ is the number of gradient descent steps (epochs), $(\\eta_{\\max},\\eta_{\\min},T)$ specify the warm-restart schedule’s maximum, minimum, and period, and $(\\eta_0,\\gamma)$ specify the monotone schedule’s initial learning rate and multiplicative decay factor:\n\n- Case 1 (general nonconvex, warm restarts have room to help): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.20,0.0005,30,0.005,0.90)$.\n- Case 2 (boundary-like: one long cycle approximates monotone): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.050,0.049,120,0.050,0.99)$.\n- Case 3 (nearly convex regime with a strong linear term): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(0.25,1.20,0.00,100,0.10,0.001,25,0.10,0.97)$.\n- Case 4 (frequent restarts): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.05,1.30,100,0.20,0.0005,10,0.010,0.95)$.\n\nAngle unit requirement: any use of trigonometric functions must use radians.\n\nOutput specification: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the boolean for the corresponding test case in the order listed above.", "solution": "We begin with the synthetic nonconvex loss $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$. From the definition of the derivative and elementary calculus, the analytical gradient is obtained by differentiating term-by-term:\n$$\n\\nabla f(\\theta)=\\frac{d}{d\\theta}\\left(\\theta^4-2a\\theta^2+b\\theta\\right)=4\\theta^3-4a\\theta+b.\n$$\nDeterministic gradient descent updates the parameter using the rule:\n$$\n\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)=\\theta_t-\\eta_t\\left(4\\theta_t^3-4a\\theta_t+b\\right),\n$$\nwhere $\\eta_t$ is the learning rate at iteration $t$. This rule follows directly from the steepest descent principle: move in the direction of the negative gradient scaled by a positive step size.\n\nWe next derive a warm-restart learning rate schedule over a period of length $T$ that satisfies smoothness and boundary constraints. We require a function $g:[0,T]\\to\\mathbb{R}$ such that:\n1. $g(0)=\\eta_{\\max}$,\n2. $g(T)=\\eta_{\\min}$,\n3. $g'(0)=0$ and $g'(T)=0$ for boundary smoothness,\n4. $g$ is sufficiently smooth within $(0,T)$,\n5. The schedule restarts every $T$ steps, so that the effective $\\eta_t=g(t\\bmod T)$.\n\nA minimal smooth function achieving these constraints can be constructed from a shifted and scaled cosine over a half period. Consider the cosine function, which is smooth, even, and has zero slope at its extrema. On the interval $[0,T]$, the function $\\cos\\left(\\pi t/T\\right)$ has $\\cos(0)=1$, $\\cos(\\pi)=-1$, and zero slope at both ends. Scaling and shifting to match the boundary values yields:\n$$\ng(t)=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t}{T}\\right)\\right).\n$$\nThis satisfies all stated conditions, with angles in radians as required. To incorporate warm restarts (Stochastic Gradient Descent with Restarts (SGDR) in this deterministic setting), we repeat $g$ every $T$ steps by setting:\n$$\n\\eta_t=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t\\bmod T}{T}\\right)\\right).\n$$\nEach restart resets the learning rate to $\\eta_{\\max}$ and then smoothly anneals to $\\eta_{\\min}$ over one period, encouraging periodic exploration and escape from shallow regions by intermittently increasing the step size.\n\nFor contrast, we define a monotone learning rate schedule that decreases over time according to a well-tested exponential rule:\n$$\n\\eta_t=\\eta_0\\gamma^t,\n$$\nwith $\\eta_00$ and $0\\gamma1$, providing a strictly decreasing sequence. This monotone decay is widely used in practice and reduces step sizes over time, which can help with convergence but may impede escaping shallow minima due to diminishing step sizes.\n\nAlgorithmic design:\n- For each test case, compute the final loss under two regimes: cosine annealing with warm restarts and monotone exponential decay.\n- Initialize $\\theta_0$ and iterate $E$ steps of deterministic gradient descent using the respective schedules. At each step $t$, compute the gradient $\\nabla f(\\theta_t)=4\\theta_t^3-4a\\theta_t+b$ and update $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$.\n- After $E$ steps, compute $f(\\theta_E)$ for both schedules. Return a boolean indicating whether the final loss under warm restarts is strictly lower than the final loss under monotone decay by at least $\\varepsilon=10^{-2}$:\n$$\n\\text{result}=\\left(f_{\\text{warm}}+ \\varepsilon  f_{\\text{mono}}\\right)\n$$\n\nPrinciple-based interpretation:\n- In a nonconvex landscape like the quartic $f$, shallow minima and plateaus can cause small gradients. Under monotone decay, step sizes shrink continuously, which can trap the iterates near suboptimal shallow regions because the product $\\eta_t\\lVert\\nabla f(\\theta_t)\\rVert$ becomes too small to make meaningful progress.\n- The cosine annealing with warm restarts periodically increases the learning rate to $\\eta_{\\max}$, producing larger steps that can move the iterate out of shallow valleys or plateaus. Because the schedule is smooth and has zero slope at period boundaries, it avoids sudden jumps that may destabilize training while still injecting sufficient exploratory capacity at restarts.\n\nTest suite rationale:\n- Case 1 provides a nonconvex setting with parameters that make shallow regions discernible; warm restarts are expected to outperform monotone decay, yielding true.\n- Case 2 uses a single long period with near-constant learning rate, approximating monotone behavior; the advantage of restarts diminishes, often yielding false.\n- Case 3 has a strong linear term, making the loss closer to convex-like behavior (effectively one dominant basin), where both schedules tend to perform similarly, yielding false.\n- Case 4 uses frequent restarts to induce substantial periodic exploration, which can improve final loss relative to monotone decay, potentially yielding true.\n\nThe final output is a single line containing a list of four booleans in the order of the test cases, formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(theta, a, b):\n    # Quartic nonconvex loss f(theta) = theta^4 - 2 a theta^2 + b theta\n    return theta**4 - 2.0*a*theta**2 + b*theta\n\ndef grad_f(theta, a, b):\n    # Analytical gradient: df/dtheta = 4 theta^3 - 4 a theta + b\n    return 4.0*theta**3 - 4.0*a*theta + b\n\ndef cosine_annealing_lr(t, eta_max, eta_min, T):\n    # Cosine schedule over one period T with warm restarts.\n    # Angle in radians; uses a half-cosine anneal from eta_max to eta_min.\n    # t_mod cycles every T steps.\n    t_mod = t % T\n    return eta_min + 0.5*(eta_max - eta_min)*(1.0 + np.cos(np.pi * (t_mod / T)))\n\ndef monotone_exp_lr(t, eta0, gamma):\n    # Monotone exponential decay: eta_t = eta0 * gamma^t\n    return eta0 * (gamma ** t)\n\ndef run_descent(a, b, theta0, E, lr_schedule_fn, lr_params):\n    theta = theta0\n    for t in range(E):\n        eta_t = lr_schedule_fn(t, *lr_params)\n        g = grad_f(theta, a, b)\n        theta = theta - eta_t * g\n    return f(theta, a, b), theta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, b, theta0, E, eta_max, eta_min, T, eta0, gamma)\n    test_cases = [\n        (1.0, 0.01, 1.30, 120, 0.20, 0.0005, 30, 0.005, 0.90),   # Case 1\n        (1.0, 0.01, 1.30, 120, 0.050, 0.049, 120, 0.050, 0.99),  # Case 2\n        (0.25, 1.20, 0.00, 100, 0.10, 0.001, 25, 0.10, 0.97),    # Case 3\n        (1.0, 0.05, 1.30, 100, 0.20, 0.0005, 10, 0.010, 0.95),   # Case 4\n    ]\n\n    epsilon = 1e-2  # Margin for declaring warm restarts strictly better\n\n    results = []\n    for case in test_cases:\n        a, b, theta0, E, eta_max, eta_min, T, eta0, gamma = case\n\n        # Warm restarts via cosine annealing with restarts\n        warm_loss, warm_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=cosine_annealing_lr,\n            lr_params=(eta_max, eta_min, T)\n        )\n\n        # Monotone exponential decay\n        mono_loss, mono_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=monotone_exp_lr,\n            lr_params=(eta0, gamma)\n        )\n\n        # Boolean result: True if warm_loss + epsilon  mono_loss\n        result = (warm_loss + epsilon)  mono_loss\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3110220"}, {"introduction": "Building on the intuition from non-convex optimization, this practice explores the behavior of cyclical learning rates in the context of structured convex problems, specifically those involving L1 regularization (a technique common in feature selection, like LASSO). You will implement proximal gradient descent, a standard algorithm for such problems, and compare the performance of a cosine cyclical learning rate against the classic fixed-step approach. This exercise bridges the gap between the conceptual benefits of cyclical schedules and their practical application in well-understood machine learning optimization tasks [@problem_id:3110151].", "problem": "Consider convex composite optimization in finite dimensions. Let $F(\\mathbf{x}) = f(\\mathbf{x}) + g(\\mathbf{x})$ where $f$ is differentiable with a Lipschitz-continuous gradient and $g$ is proper, lower semicontinuous, and convex. You will study the effect of cyclical step sizes based on a cosine schedule in the proximal gradient method and compare against a fixed-step proximal gradient method.\n\nWork with the following instance class, which is separable by coordinates:\n- The smooth part is $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$ where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is diagonal with strictly positive diagonal entries, and $\\mathbf{b} \\in \\mathbb{R}^n$ is a given vector. For such $\\mathbf{A}$, the gradient $\\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} - \\mathbf{b}$ is $L$-Lipschitz with $L$ equal to the largest diagonal entry of $\\mathbf{A}$.\n- The nonsmooth part is $g(\\mathbf{x}) = \\lambda \\lVert \\mathbf{x} \\rVert_1$ with $\\lambda \\ge 0$.\n\nThe proximal operator of $g$ at a point $\\mathbf{v}$ with parameter $\\alpha  0$ is defined by $\\mathrm{prox}_{\\alpha g}(\\mathbf{v}) = \\arg\\min_{\\mathbf{y}} \\left\\{ g(\\mathbf{y}) + \\tfrac{1}{2\\alpha}\\lVert \\mathbf{y} - \\mathbf{v} \\rVert_2^2 \\right\\}$. Recall that the proximal gradient step for a step size $\\alpha_t$ updates $\\mathbf{x}^{t+1}$ by minimizing a quadratic upper bound of $f$ around $\\mathbf{x}^t$ plus $g$.\n\nFor the class above with diagonal $\\mathbf{A}$, the unique minimizer $\\mathbf{x}^\\star$ of $F$ can be obtained coordinatewise from first principles by solving the scalar convex problems $\\min_{x_i} \\left\\{ \\tfrac{1}{2} a_i x_i^2 - b_i x_i + \\lambda |x_i| \\right\\}$ where $a_i$ are the diagonal entries of $\\mathbf{A}$ and $b_i$ are the entries of $\\mathbf{b}$. Use this closed-form minimizer to compute the optimal value $F(\\mathbf{x}^\\star)$ for each test case.\n\nImplement and compare two step-size strategies in proximal gradient descent starting from $\\mathbf{x}^0 = \\mathbf{0}$ and running for $T$ iterations:\n- Fixed step: use a constant step $\\alpha_{\\text{fixed}} = 1/L$.\n- Cosine cyclical step: use a periodic schedule that, at iteration $t \\in \\{0,1,2,\\dots\\}$, selects a step size $\\alpha_t$ that starts a new cycle every $P$ iterations, begins each cycle at a maximum value $\\alpha_{\\max}$ and decreases smoothly to a minimum value $\\alpha_{\\min}$ by following a half-period cosine profile over one cycle. Express the cosine argument in radians. The values $\\alpha_{\\min}$ and $\\alpha_{\\max}$ will be specified as fractions of $1/L$ for each test case, and $P$ will be given as a positive integer.\n\nAngle unit requirement: all trigonometric function arguments must be in radians.\n\nFor each test case below:\n- Construct $\\mathbf{A}$ and $\\mathbf{b}$ exactly as specified.\n- Compute $L$ as the maximum diagonal entry of $\\mathbf{A}$.\n- Compute the exact optimizer $\\mathbf{x}^\\star$ coordinatewise.\n- Run proximal gradient with the fixed step for $T$ iterations to obtain $\\mathbf{x}^{T}_{\\text{fixed}}$ and with the cosine cyclical step for $T$ iterations to obtain $\\mathbf{x}^{T}_{\\text{cyc}}$.\n- Compute $F(\\mathbf{x}^{T}_{\\text{fixed}}) - F(\\mathbf{x}^\\star)$ and $F(\\mathbf{x}^{T}_{\\text{cyc}}) - F(\\mathbf{x}^\\star)$.\n- Output a boolean that is $\\mathrm{True}$ if the cyclical schedule attains a final objective value no larger than the fixed-step method, i.e., $F(\\mathbf{x}^{T}_{\\text{cyc}}) \\le F(\\mathbf{x}^{T}_{\\text{fixed}})$ up to a numerical tolerance of $10^{-12}$, and $\\mathrm{False}$ otherwise.\n\nTest suite (angles in radians, all quantities to be interpreted exactly as real numbers):\n- Test $1$ (happy path):\n  - Dimension $n = 20$.\n  - Diagonal entries $a_i = 1 + \\tfrac{i}{10}$ for $i \\in \\{1,2,\\dots,20\\}$.\n  - Entries $b_i = \\tfrac{1}{i}$ for $i \\in \\{1,2,\\dots,20\\}$.\n  - Regularization $\\lambda = 0.05$.\n  - Iterations $T = 200$.\n  - Cosine cycle period $P = 40$.\n  - Cosine bounds as fractions of $1/L$: $\\alpha_{\\min}$ factor $= 0.0$, $\\alpha_{\\max}$ factor $= 1.0$.\n- Test $2$ (high regularization and short period):\n  - Dimension $n = 20$.\n  - Diagonal entries $a_i = 0.5 + \\tfrac{i}{20}$ for $i \\in \\{1,2,\\dots,20\\}$.\n  - Entries $b_i = (-1)^i \\tfrac{1}{i}$ for $i \\in \\{1,2,\\dots,20\\}$.\n  - Regularization $\\lambda = 0.5$.\n  - Iterations $T = 120$.\n  - Cosine cycle period $P = 12$.\n  - Cosine bounds as fractions of $1/L$: $\\alpha_{\\min}$ factor $= 0.2$, $\\alpha_{\\max}$ factor $= 0.9$.\n- Test $3$ (nonsmooth data with trigonometric pattern, tighter range):\n  - Dimension $n = 30$.\n  - Diagonal entries $a_i = 0.2 + \\tfrac{i}{30}$ for $i \\in \\{1,2,\\dots,30\\}$.\n  - Entries $b_i = \\sin(i)$ for $i \\in \\{1,2,\\dots,30\\}$, with $i$ in radians.\n  - Regularization $\\lambda = 0.1$.\n  - Iterations $T = 150$.\n  - Cosine cycle period $P = 25$.\n  - Cosine bounds as fractions of $1/L$: $\\alpha_{\\min}$ factor $= 0.5$, $\\alpha_{\\max}$ factor $= 0.99$.\n- Test $4$ (boundary condition: constant step as a degenerate cycle):\n  - Dimension $n = 10$.\n  - Diagonal entries $a_i = 1 + \\tfrac{i}{5}$ for $i \\in \\{1,2,\\dots,10\\}$.\n  - Entries $b_i = \\tfrac{1}{i}$ for $i \\in \\{1,2,\\dots,10\\}$.\n  - Regularization $\\lambda = 0.05$.\n  - Iterations $T = 150$.\n  - Cosine cycle period $P = 1$.\n  - Cosine bounds as fractions of $1/L$: $\\alpha_{\\min}$ factor $= 1.0$, $\\alpha_{\\max}$ factor $= 1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets (for example, [`True`, `False`, `True`, `True`]). No other text should be printed.", "solution": "We begin from the composite convex objective $F(\\mathbf{x}) = f(\\mathbf{x}) + g(\\mathbf{x})$ where $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$ and $g(\\mathbf{x}) = \\lambda \\lVert \\mathbf{x} \\rVert_1$. For a diagonal matrix $\\mathbf{A}$ with positive diagonal entries $(a_i)_{i=1}^n$, the gradient is $\\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} - \\mathbf{b}$ and is $L$-Lipschitz with $L = \\max_i a_i$, a direct consequence of the largest eigenvalue of $\\mathbf{A}$ equaling its largest diagonal entry when $\\mathbf{A}$ is diagonal.\n\nThe proximal gradient method follows from minimizing a quadratic upper bound of $f$ at the current point $\\mathbf{x}^t$ plus $g$. Specifically, by the $L$-smoothness of $f$, for any step size $\\alpha_t \\in (0, 1/L]$,\n$$\nf(\\mathbf{y}) \\le f(\\mathbf{x}^t) + \\nabla f(\\mathbf{x}^t)^{\\top}(\\mathbf{y} - \\mathbf{x}^t) + \\frac{1}{2\\alpha_t} \\lVert \\mathbf{y} - \\mathbf{x}^t \\rVert_2^2.\n$$\nA proximal gradient step chooses $\\mathbf{x}^{t+1}$ as the minimizer of the right-hand side plus $g(\\mathbf{y})$:\n$$\n\\mathbf{x}^{t+1} = \\arg\\min_{\\mathbf{y}} \\left\\{ \\nabla f(\\mathbf{x}^t)^{\\top}(\\mathbf{y} - \\mathbf{x}^t) + \\frac{1}{2\\alpha_t} \\lVert \\mathbf{y} - \\mathbf{x}^t \\rVert_2^2 + g(\\mathbf{y}) \\right\\}.\n$$\nCompleting the square shows that\n$$\n\\mathbf{x}^{t+1} = \\mathrm{prox}_{\\alpha_t g}\\!\\left(\\mathbf{x}^t - \\alpha_t \\nabla f(\\mathbf{x}^t)\\right).\n$$\n\nFor $g(\\mathbf{x}) = \\lambda \\lVert \\mathbf{x} \\rVert_1$, the proximal operator is the coordinatewise soft-thresholding map, defined for any $\\tau  0$ by\n$$\n\\left[\\mathrm{prox}_{\\tau \\lambda \\lVert \\cdot \\rVert_1}(\\mathbf{v})\\right]_i = \\mathrm{sign}(v_i)\\,\\max\\{|v_i| - \\tau \\lambda, 0\\}.\n$$\nThus a proximal gradient step is computable in closed form as a gradient step on $f$ followed by soft-thresholding with threshold $\\alpha_t \\lambda$.\n\nThe exact minimizer $\\mathbf{x}^\\star$ for the specified diagonal $\\mathbf{A}$ can be derived independently, coordinatewise. Writing $F(\\mathbf{x}) = \\sum_{i=1}^n \\left( \\tfrac{1}{2} a_i x_i^2 - b_i x_i + \\lambda |x_i| \\right)$ reduces the problem to $n$ decoupled scalar convex problems. For a fixed coordinate $i$, define $\\phi_i(x_i) = \\tfrac{1}{2} a_i x_i^2 - b_i x_i + \\lambda |x_i|$. Completing the square yields\n$$\n\\phi_i(x_i) = \\tfrac{1}{2} a_i \\left(x_i - \\tfrac{b_i}{a_i}\\right)^2 + \\lambda |x_i| + \\mathrm{constant}.\n$$\nMinimizing this in $x_i$ is equivalent to evaluating the proximal operator of $\\lambda |\\cdot|$ with parameter $1/a_i$ at $b_i/a_i$, which gives the closed form\n$$\nx_i^\\star = \\mathrm{sign}\\!\\left(\\tfrac{b_i}{a_i}\\right)\\,\\max\\!\\left\\{ \\left|\\tfrac{b_i}{a_i}\\right| - \\tfrac{\\lambda}{a_i}, 0 \\right\\} = \\mathcal{S}_{\\lambda/a_i}\\!\\left( \\tfrac{b_i}{a_i} \\right),\n$$\nwhere $\\mathcal{S}_\\theta(u) = \\mathrm{sign}(u)\\,\\max\\{|u| - \\theta, 0\\}$ denotes soft-thresholding at level $\\theta$.\n\nRegarding step-size strategies:\n- Fixed step uses $\\alpha_{\\text{fixed}} = 1/L$. For $f$ with $L$-Lipschitz gradient, proximal gradient with any $\\alpha \\in (0, 1/L]$ yields a nonincreasing objective sequence and converges to an optimizer.\n- Cosine cyclical step uses a periodic step size $\\alpha_t$ that restarts every $P$ iterations, beginning each cycle at $\\alpha_{\\max}$ and decreasing smoothly to $\\alpha_{\\min}$ via a half-cosine profile. Let the local phase be $u_t = (t \\bmod P)/P \\in [0,1)$; a half-period cosine map $c(u_t) = \\tfrac{1}{2}(1 + \\cos(\\pi u_t))$ in radians satisfies $c(0) = 1$ and $c(1) = 0$. Mapping $c(u_t)$ from $[0,1]$ to $[\\alpha_{\\min}, \\alpha_{\\max}]$ defines $\\alpha_t \\in [\\alpha_{\\min}, \\alpha_{\\max}]$ within each cycle. Choosing $\\alpha_{\\max} \\le 1/L$ ensures that all steps remain within the standard convergence-safe range, preserving stability while allowing within-cycle variation.\n\nAlgorithmic design for the program:\n- For each test case, construct $\\mathbf{A}$ and $\\mathbf{b}$ deterministically as specified. Compute $L = \\max_i a_i$.\n- Compute $\\mathbf{x}^\\star$ coordinatewise using $x_i^\\star = \\mathcal{S}_{\\lambda/a_i}\\!\\left( b_i/a_i \\right)$ and evaluate $F(\\mathbf{x}^\\star)$.\n- Implement proximal gradient:\n  - Initialize $\\mathbf{x}^0 = \\mathbf{0}$.\n  - For $t = 0,1,\\dots,T-1$, compute $\\mathbf{v}^t = \\mathbf{x}^t - \\alpha_t (\\mathbf{A}\\mathbf{x}^t - \\mathbf{b})$ and then $\\mathbf{x}^{t+1} = \\mathcal{S}_{\\alpha_t \\lambda}(\\mathbf{v}^t)$.\n- Use $\\alpha_t \\equiv \\alpha_{\\text{fixed}}$ for the fixed-step run, and the cosine cyclical schedule for the cyclical run, with $\\alpha_{\\min} = \\text{factor}_{\\min} \\cdot (1/L)$ and $\\alpha_{\\max} = \\text{factor}_{\\max} \\cdot (1/L)$, and period $P$ as specified. All cosine arguments are in radians.\n- After $T$ iterations, compute the objective gaps for both strategies and return a boolean indicating whether the cyclical gap is less than or equal to the fixed-step gap within a tolerance of $10^{-12}$.\n\nEdge-case coverage:\n- The first test checks a typical case where $\\alpha_{\\max} = 1/L$ and $\\alpha_{\\min} = 0$, highlighting potential within-cycle stalls near zero step size and their impact over a longer horizon $T = 200$.\n- The second test uses high regularization $\\lambda = 0.5$ and a short period $P = 12$ with $\\alpha_{\\min}$ strictly positive, examining sparse regimes.\n- The third test introduces oscillatory data $\\mathbf{b}$ with $b_i = \\sin(i)$ (angles in radians) and a tighter step-size range, probing stability under sign-changing gradients.\n- The fourth test sets $P = 1$ with $\\alpha_{\\min} = \\alpha_{\\max} = 1/L$, which degenerates to a constant-step method, serving as a boundary consistency check.\n\nThe final program aggregates the booleans for all tests into a single list and prints it in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(x, tau):\n    # Soft-thresholding operator applied elementwise\n    return np.sign(x) * np.maximum(np.abs(x) - tau, 0.0)\n\ndef objective_value(diag_a, b, lam, x):\n    # F(x) = 0.5 * x^T A x - b^T x + lam * ||x||_1\n    return 0.5 * np.sum(diag_a * x * x) - float(b @ x) + lam * np.sum(np.abs(x))\n\ndef exact_optimum_diag(diag_a, b, lam):\n    # x*_i = soft_threshold(b_i / a_i, lam / a_i)\n    ratio = b / diag_a\n    thresh = lam / diag_a\n    return soft_threshold(ratio, thresh)\n\ndef proximal_gradient_fixed(diag_a, b, lam, alpha, T, x0=None):\n    n = b.shape[0]\n    x = np.zeros(n) if x0 is None else x0.copy()\n    for _ in range(T):\n        grad = diag_a * x - b\n        v = x - alpha * grad\n        x = soft_threshold(v, alpha * lam)\n    return x\n\ndef proximal_gradient_cyclic_cosine(diag_a, b, lam, alpha_min, alpha_max, period, T, x0=None):\n    n = b.shape[0]\n    x = np.zeros(n) if x0 is None else x0.copy()\n    # Cosine annealing with warm restarts every 'period' steps\n    for t in range(T):\n        if period = 0:\n            # Fallback to constant alpha_max if period invalid\n            alpha_t = alpha_max\n        else:\n            u = (t % period) / period  # progress in [0,1)\n            alpha_t = alpha_min + 0.5 * (alpha_max - alpha_min) * (1.0 + np.cos(np.pi * u))\n        grad = diag_a * x - b\n        v = x - alpha_t * grad\n        x = soft_threshold(v, alpha_t * lam)\n    return x\n\ndef build_test_cases():\n    tests = []\n\n    # Test 1\n    n1 = 20\n    diag_a1 = np.array([1.0 + i/10.0 for i in range(1, n1+1)], dtype=float)\n    b1 = np.array([1.0/i for i in range(1, n1+1)], dtype=float)\n    lam1 = 0.05\n    T1 = 200\n    P1 = 40\n    # factors relative to 1/L\n    amin_factor1, amax_factor1 = 0.0, 1.0\n    tests.append((diag_a1, b1, lam1, T1, P1, amin_factor1, amax_factor1))\n\n    # Test 2\n    n2 = 20\n    diag_a2 = np.array([0.5 + i/20.0 for i in range(1, n2+1)], dtype=float)\n    b2 = np.array([(((-1)**i) * 1.0/i) for i in range(1, n2+1)], dtype=float)\n    lam2 = 0.5\n    T2 = 120\n    P2 = 12\n    amin_factor2, amax_factor2 = 0.2, 0.9\n    tests.append((diag_a2, b2, lam2, T2, P2, amin_factor2, amax_factor2))\n\n    # Test 3\n    n3 = 30\n    diag_a3 = np.array([0.2 + i/30.0 for i in range(1, n3+1)], dtype=float)\n    # sin(i) with i in radians\n    b3 = np.sin(np.arange(1, n3+1, dtype=float))\n    lam3 = 0.1\n    T3 = 150\n    P3 = 25\n    amin_factor3, amax_factor3 = 0.5, 0.99\n    tests.append((diag_a3, b3, lam3, T3, P3, amin_factor3, amax_factor3))\n\n    # Test 4\n    n4 = 10\n    diag_a4 = np.array([1.0 + i/5.0 for i in range(1, n4+1)], dtype=float)\n    b4 = np.array([1.0/i for i in range(1, n4+1)], dtype=float)\n    lam4 = 0.05\n    T4 = 150\n    P4 = 1\n    amin_factor4, amax_factor4 = 1.0, 1.0\n    tests.append((diag_a4, b4, lam4, T4, P4, amin_factor4, amax_factor4))\n\n    return tests\n\ndef solve():\n    test_cases = build_test_cases()\n    results = []\n\n    for diag_a, b, lam, T, period, amin_factor, amax_factor in test_cases:\n        L = float(np.max(diag_a))\n        alpha_fixed = 1.0 / L\n        alpha_min = amin_factor * (1.0 / L)\n        alpha_max = amax_factor * (1.0 / L)\n\n        # Exact optimum and optimal objective\n        x_star = exact_optimum_diag(diag_a, b, lam)\n        F_star = objective_value(diag_a, b, lam, x_star)\n\n        # Fixed-step proximal gradient\n        x_fixed_T = proximal_gradient_fixed(diag_a, b, lam, alpha_fixed, T)\n        F_fixed_T = objective_value(diag_a, b, lam, x_fixed_T)\n\n        # Cyclical cosine proximal gradient\n        x_cyc_T = proximal_gradient_cyclic_cosine(diag_a, b, lam, alpha_min, alpha_max, period, T)\n        F_cyc_T = objective_value(diag_a, b, lam, x_cyc_T)\n\n        # Compare with tolerance\n        tol = 1e-12\n        results.append(F_cyc_T = F_fixed_T + tol)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(['True' if r else 'False' for r in results])}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3110151"}, {"introduction": "This final practice moves towards a more advanced and adaptive application of cyclical learning rates, connecting the learning rate schedule directly to the local geometry of the loss function. You will implement a controller that estimates the maximum local curvature using the power iteration method, a technique for finding the largest eigenvalue of the Hessian matrix. This estimate is then used to enforce a stability constraint, $\\eta_{\\max} \\le 2/\\hat{\\lambda}_{\\max}$, dynamically capping the peak learning rate to prevent divergence while still allowing for aggressive exploration. This exercise demonstrates how to create more robust and theoretically grounded optimization strategies [@problem_id:3110195].", "problem": "You are asked to implement a complete, runnable program that constructs a controller for a cyclical learning rate schedule using Cosine Annealing (CA) within the context of Gradient Descent (GD) in deep learning. The controller must adjust the maximum learning rate for each cycle based on a curvature estimate obtained via power iteration on a symmetric positive semidefinite matrix, and must enforce the constraint that the maximum learning rate is no larger than the reciprocal stability threshold derived from the largest curvature estimate. The implementation must follow standard mathematical and algorithmic principles without relying on shortcut formulas supplied in the problem statement.\n\nThe fundamental base you must use is the following widely accepted facts:\n- For a local quadratic approximation of a smooth loss function near a point, the loss can be written as $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$ where $\\mathbf{H}$ is a symmetric positive semidefinite matrix representing curvature.\n- The Gradient Descent (GD) iteration is $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k})$ and, in the quadratic case, it reduces to a linear system driven by the matrix $\\mathbf{I} - \\eta \\mathbf{H}$.\n- Stability of GD on a quadratic objective is controlled by the spectral radius of the update matrix, which is bounded by the largest eigenvalue of $\\mathbf{H}$.\n- The largest eigenvalue of a symmetric matrix can be approximated from first principles via power iteration that leverages the Rayleigh quotient, without needing explicit eigendecomposition.\n\nYou must implement the following components in your program:\n- A power iteration routine that, given a symmetric positive semidefinite matrix $\\mathbf{H}$ and an iteration budget $K$, returns an estimate $\\hat{\\lambda}_{\\max}$ of the largest eigenvalue of $\\mathbf{H}$.\n- A controller that, at the start of each cycle, sets the maximum learning rate $\\eta_{\\max}$ to $\\min\\left(\\eta_{\\text{target}}, \\tfrac{2}{\\hat{\\lambda}_{\\max}}\\right)$, where $\\eta_{\\text{target}}$ is a user-specified target amplitude and $\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ is the stability threshold derived from the quadratic GD dynamics. When $\\hat{\\lambda}_{\\max} \\le 0$, interpret $\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ as $+\\infty$.\n- A cyclical schedule per cycle based on Cosine Annealing (CA), with angles measured in radians. In each cycle of length $T$ steps, the learning rate must start at $\\eta_{\\max}$ and end at $\\eta_{\\min}$, where $\\eta_{\\min}$ is a user-specified lower bound. For the boundary case $T = 1$, the schedule must contain a single value equal to $\\eta_{\\max}$.\n- A verification routine that checks, for every cycle, that all learning rate values $\\eta_t$ for that cycle satisfy $\\eta_{\\min} \\le \\eta_t \\le \\eta_{\\max}$ and $\\eta_t \\le \\tfrac{2}{\\hat{\\lambda}_{\\max}}$, and that the schedule starts at $\\eta_{\\max}$ and ends at $\\eta_{\\min}$ when $T  1$, or is constant at $\\eta_{\\max}$ when $T = 1$.\n\nAngle units must be in radians. No physical quantities with units are involved.\n\nTest Suite:\nImplement and evaluate your controller on the following test cases. Each test case specifies the curvature matrix $\\mathbf{H}$, the cycle configuration, and controller parameters. For reproducibility, use independent random seeds for power iteration per cycle as given. All matrices are symmetric, and all lengths and counts are integers.\n\n- Test Case $1$ (happy path, moderate curvature):\n  - $\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.0$,\n  - $\\eta_{\\text{target}} = 0.9$,\n  - number of cycles $C = 2$,\n  - cycle length $T = 10$,\n  - power iteration steps $K = 50$,\n  - base seed $s = 101$.\n\n- Test Case $2$ (high curvature requiring capping):\n  - $\\mathbf{H} = \\begin{bmatrix} 5.0  0.0 \\\\ 0.0  3.0 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.05$,\n  - $\\eta_{\\text{target}} = 0.9$,\n  - number of cycles $C = 3$,\n  - cycle length $T = 8$,\n  - power iteration steps $K = 30$,\n  - base seed $s = 202$.\n\n- Test Case $3$ (near-zero curvature, large stability threshold):\n  - $\\mathbf{H} = 0.001 \\cdot \\mathbf{I}_{3}$,\n  - $\\eta_{\\min} = 0.0$,\n  - $\\eta_{\\text{target}} = 1.5$,\n  - number of cycles $C = 2$,\n  - cycle length $T = 4$,\n  - power iteration steps $K = 20$,\n  - base seed $s = 303$.\n\n- Test Case $4$ (ill-conditioned curvature):\n  - $\\mathbf{H} = \\begin{bmatrix} 10.0  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.0$,\n  - $\\eta_{\\text{target}} = 0.25$,\n  - number of cycles $C = 3$,\n  - cycle length $T = 7$,\n  - power iteration steps $K = 25$,\n  - base seed $s = 404$.\n\n- Test Case $5$ (boundary case $T = 1$):\n  - $\\mathbf{H} = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.1$,\n  - $\\eta_{\\text{target}} = 1.0$,\n  - number of cycles $C = 1$,\n  - cycle length $T = 1$,\n  - power iteration steps $K = 40$,\n  - base seed $s = 505$.\n\nYour program must produce a single line of output containing the verification results for the five test cases as a comma-separated list enclosed in square brackets, with each item being a boolean indicating whether all constraints were satisfied for that test case across all cycles, for example, [`True`, `False`, ..., `True`]. Angles must be in radians throughout. No user input is allowed; the program must be fully self-contained and runnable as is.", "solution": "We start from the local quadratic approximation of a smooth deep learning loss near a point, which is a well-tested fact. Let the loss be $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$, where $\\mathbf{H}$ is a symmetric positive semidefinite matrix capturing curvature. The Gradient Descent (GD) iteration with learning rate $\\eta$ is\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k}) = \\mathbf{x}_{k} - \\eta \\mathbf{H} \\mathbf{x}_{k} = \\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)\\mathbf{x}_{k}.\n$$\nConvergence and stability of this linear system depends on the spectral radius $\\rho\\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)$. Because $\\mathbf{H}$ is symmetric, it is diagonalizable with real, nonnegative eigenvalues $\\lambda_{1}, \\dots, \\lambda_{n}$. The eigenvalues of $\\mathbf{I} - \\eta \\mathbf{H}$ are $1 - \\eta \\lambda_{i}$. Stability requires\n$$\n\\max_{i} \\left|1 - \\eta \\lambda_{i}\\right|  1,\n$$\nwhich implies for every $i$,\n$$\n-1  1 - \\eta \\lambda_{i}  1 \\quad \\Rightarrow \\quad 0  \\eta \\lambda_{i}  2.\n$$\nTo satisfy this simultaneously for all eigenvalues, it suffices to enforce\n$$\n0  \\eta  \\frac{2}{\\lambda_{\\max}},\n$$\nwhere $\\lambda_{\\max} = \\max_{i} \\lambda_{i}$. This inequality gives a principled upper bound on the stable learning rate in the quadratic regime.\n\nWe do not directly compute $\\lambda_{\\max}$ via explicit eigendecomposition; instead, we estimate it using the power iteration method, a first-principles algorithm. Let $\\mathbf{v}_{0}$ be a nonzero initial vector. The power iteration updates are\n$$\n\\mathbf{w}_{k} = \\mathbf{H} \\mathbf{v}_{k}, \\quad \\mathbf{v}_{k+1} = \\frac{\\mathbf{w}_{k}}{\\lVert \\mathbf{w}_{k} \\rVert}.\n$$\nUnder standard conditions for symmetric matrices with a unique largest eigenvalue in magnitude, $\\mathbf{v}_{k}$ converges to the eigenvector associated with $\\lambda_{\\max}$, and the Rayleigh quotient\n$$\n\\hat{\\lambda}_{\\max}^{(k)} = \\mathbf{v}_{k}^{\\top} \\mathbf{H} \\mathbf{v}_{k}\n$$\nconverges to $\\lambda_{\\max}$. If $\\mathbf{H}$ is the zero matrix, then $\\mathbf{H}\\mathbf{v}_{k} = \\mathbf{0}$ and the Rayleigh quotient is $0$, which we interpret as yielding an infinite stability threshold $\\frac{2}{\\hat{\\lambda}_{\\max}} = +\\infty$.\n\nWe now design the cyclical learning rate controller. In each cycle, before constructing the learning rate schedule, we:\n- Run $K$ steps of power iteration on $\\mathbf{H}$ to obtain $\\hat{\\lambda}_{\\max}$.\n- Compute the stability cap $c = \\frac{2}{\\hat{\\lambda}_{\\max}}$ if $\\hat{\\lambda}_{\\max}  0$, and $c = +\\infty$ otherwise.\n- Set the maximum learning rate in the cycle to\n$$\n\\eta_{\\max} = \\min\\left(\\eta_{\\text{target}}, c\\right).\n$$\n- Use Cosine Annealing (CA) with angles in radians to construct a schedule of length $T$ that starts at $\\eta_{\\max}$ and ends at $\\eta_{\\min}$. The CA schedule is the standard practice in Cyclical Learning Rate (CLR) designs and is derived by mapping discrete step indices $t \\in \\{0, 1, \\dots, T-1\\}$ to points on a cosine curve in $[0, \\pi]$. This mapping ensures a smooth transition from $\\eta_{\\max}$ to $\\eta_{\\min}$. For the boundary case $T = 1$, the schedule must be the single value $\\eta_{\\max}$, which is the limiting behavior as the cosine path degenerates to a point.\n\nWe verify, for each cycle, the following properties:\n- Bounds: $\\eta_{\\min} \\le \\eta_{t} \\le \\eta_{\\max}$ for all steps $t$.\n- Stability: $\\eta_{t} \\le \\frac{2}{\\hat{\\lambda}_{\\max}}$ for all steps $t$ in that cycle.\n- Endpoints: If $T  1$, the first value equals $\\eta_{\\max}$ and the last value equals $\\eta_{\\min}$. If $T = 1$, the single value equals $\\eta_{\\max}$.\n\nFinally, we aggregate the verification results for the entire test suite into a single list of booleans. Each boolean indicates whether all cycles of the corresponding test case satisfy the constraints. Angles are consistently in radians.\n\nAlgorithmic steps implemented in code:\n- Construct the given test matrices $\\mathbf{H}$ and parameters $(\\eta_{\\min}, \\eta_{\\text{target}}, C, T, K, s)$ for each test case.\n- For each cycle $c \\in \\{0, \\dots, C-1\\}$, run power iteration with a reproducible random seed derived from $s + c$, compute $\\eta_{\\max}$ via the controller, and build the CA schedule.\n- Check constraints and accumulate a pass/fail boolean per test case.\n- Print the list of five booleans as a single line in the specified format.\n\nThis design aligns with the fundamental stability condition of GD in the quadratic regime, leverages a principled eigenvalue estimation, and implements a well-established CLR mechanism using CA with radians while correctly handling edge cases such as $T = 1$ and near-zero curvature.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, num_iters: int, seed: int) -> float:\n    \"\"\"\n    Estimate the largest eigenvalue of a symmetric matrix H using power iteration.\n    Uses the Rayleigh quotient on the normalized iterate.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = H.shape[0]\n    v = rng.normal(size=(n,))\n    # Handle potential zero vector initialization, enforce nonzero\n    if np.linalg.norm(v) == 0.0:\n        v = np.ones(n)\n    v = v / np.linalg.norm(v)\n\n    lambda_hat = 0.0\n    for _ in range(num_iters):\n        w = H @ v\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0.0:\n            # H might be (near) zero; lambda_hat remains 0\n            lambda_hat = 0.0\n            break\n        v = w / norm_w\n        # Rayleigh quotient on the new direction\n        lambda_hat = float(v.T @ (H @ v))\n    return lambda_hat\n\ndef cosine_cycle_schedule(eta_min: float, eta_max: float, T: int) -> np.ndarray:\n    \"\"\"\n    Construct a cosine annealing schedule over T steps in radians,\n    starting at eta_max and ending at eta_min. For T == 1, return [eta_max].\n    \"\"\"\n    if T  1:\n        raise ValueError(\"Cycle length T must be a positive integer.\")\n    if T == 1:\n        return np.array([eta_max], dtype=float)\n    t = np.arange(T, dtype=float)\n    # Cosine in radians: angle = pi * t / (T - 1)\n    lrs = eta_min + 0.5 * (eta_max - eta_min) * (1.0 + np.cos(np.pi * t / (T - 1)))\n    return lrs\n\ndef verify_cycle(lrs: np.ndarray, eta_min: float, eta_max: float, lambda_hat: float, T: int) -> bool:\n    \"\"\"\n    Verify that the schedule respects [eta_min, eta_max], is capped by 2/lambda_hat,\n    and has correct endpoints (start at eta_max, end at eta_min for T>1; constant for T==1).\n    \"\"\"\n    tol = 1e-12\n    # Range checks\n    cond_range = (lrs.min() >= eta_min - tol) and (lrs.max() = eta_max + tol)\n    # Stability cap\n    cap = np.inf if lambda_hat = 0.0 else (2.0 / lambda_hat) + tol\n    cond_cap = np.all(lrs = cap)\n    # Endpoint checks\n    if T > 1:\n        cond_start = abs(lrs[0] - eta_max) = tol\n        cond_end = abs(lrs[-1] - eta_min) = tol\n        cond_endpoints = cond_start and cond_end\n    else:\n        cond_endpoints = abs(lrs[0] - eta_max) = tol\n    return cond_range and cond_cap and cond_endpoints\n\ndef controller_and_verify(H: np.ndarray, eta_min: float, eta_target: float,\n                          cycles: int, T: int, K: int, base_seed: int) -> bool:\n    \"\"\"\n    For each cycle, estimate curvature via power iteration, set eta_max = min(eta_target, 2/lambda_hat),\n    build the cosine annealing schedule, and verify constraints. Return True iff all cycles pass.\n    \"\"\"\n    all_ok = True\n    for c in range(cycles):\n        # Per-cycle seed to vary the initial direction deterministically\n        seed = base_seed + c\n        lambda_hat = power_iteration(H, K, seed)\n        eta_cap = np.inf if lambda_hat = 0.0 else 2.0 / lambda_hat\n        eta_max = min(eta_target, eta_cap)\n        lrs = cosine_cycle_schedule(eta_min, eta_max, T)\n        ok = verify_cycle(lrs, eta_min, eta_max, lambda_hat, T)\n        all_ok = all_ok and ok\n    return all_ok\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"H\": np.array([[1.0, 0.0],\n                           [0.0, 0.5]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.9,\n            \"cycles\": 2,\n            \"T\": 10,\n            \"K\": 50,\n            \"seed\": 101,\n        },\n        # Test Case 2\n        {\n            \"H\": np.array([[5.0, 0.0],\n                           [0.0, 3.0]], dtype=float),\n            \"eta_min\": 0.05,\n            \"eta_target\": 0.9,\n            \"cycles\": 3,\n            \"T\": 8,\n            \"K\": 30,\n            \"seed\": 202,\n        },\n        # Test Case 3\n        {\n            \"H\": 0.001 * np.eye(3, dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 1.5,\n            \"cycles\": 2,\n            \"T\": 4,\n            \"K\": 20,\n            \"seed\": 303,\n        },\n        # Test Case 4\n        {\n            \"H\": np.array([[10.0, 0.0],\n                           [0.0, 0.01]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.25,\n            \"cycles\": 3,\n            \"T\": 7,\n            \"K\": 25,\n            \"seed\": 404,\n        },\n        # Test Case 5\n        {\n            \"H\": np.array([[2.0, 0.0],\n                           [0.0, 1.0]], dtype=float),\n            \"eta_min\": 0.1,\n            \"eta_target\": 1.0,\n            \"cycles\": 1,\n            \"T\": 1,\n            \"K\": 40,\n            \"seed\": 505,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = controller_and_verify(\n            H=case[\"H\"],\n            eta_min=case[\"eta_min\"],\n            eta_target=case[\"eta_target\"],\n            cycles=case[\"cycles\"],\n            T=case[\"T\"],\n            K=case[\"K\"],\n            base_seed=case[\"seed\"]\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3110195"}]}