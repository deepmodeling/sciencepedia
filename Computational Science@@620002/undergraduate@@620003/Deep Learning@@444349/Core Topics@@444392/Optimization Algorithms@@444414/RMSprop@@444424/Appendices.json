{"hands_on_practices": [{"introduction": "Before diving into complex scenarios, it is crucial to understand how RMSprop's core component—the exponential moving average accumulator—responds to simple, controlled inputs. This first practice models a common situation where gradients are mostly small but punctuated by occasional large spikes [@problem_id:3170912]. By analyzing this idealized scenario, you will gain a quantitative understanding of how the decay parameter $\\rho$ allows the optimizer to \"buffer\" these spikes and maintain a more stable learning dynamic.", "problem": "Consider Root Mean Square Propagation (RMSprop), a stochastic optimization method that stabilizes parameter updates by maintaining a decaying, exponentially weighted average of past squared gradients. Let the decay rate parameter be $\\rho \\in (0,1)$, and let the sequence of squared gradients be defined by a periodic pattern with period $k \\in \\mathbb{N}$: specifically, for time index $t \\in \\mathbb{N}$, let $g_t^2 = M$ when $t$ is a positive multiple of $k$ and $g_t^2 = 1$ otherwise, where $M$ is a fixed constant with $M \\gg 1$. Assume the RMSprop accumulator is initialized at $v_0 = 0$ and that training has proceeded long enough for $v_t$ to reach a time-periodic steady state synchronized with the gradient sequence.\n\nStarting from the principle that RMSprop forms an exponentially weighted moving average of the squared gradients with geometric weights that sum to $1$, derive the recursion for the accumulator and analyze its behavior under the periodic input described above. In the steady state, let $v_t$ be evaluated immediately after each spike time $t = nk$ for $n \\in \\mathbb{N}$; denote this value by $s$, which is constant across spikes in steady state.\n\nDetermine a closed-form analytic expression for $s$ as a function of $\\rho$, $M$, and $k$. Provide your final answer as a single simplified symbolic expression. No rounding is required.", "solution": "The problem asks for a closed-form expression for the steady-state value of the Root Mean Square Propagation (RMSprop) accumulator, denoted by $s$, under a specific periodic sequence of squared gradients.\n\nFirst, we establish the fundamental recursion for the RMSprop accumulator, $v_t$. It is defined as an exponentially weighted moving average of the squared gradients, $g_t^2$. The update rule is given by:\n$$v_t = \\rho v_{t-1} + (1-\\rho) g_t^2$$\nwhere $\\rho \\in (0,1)$ is the decay rate parameter, and $v_t$ is the accumulator value at time step $t$. The problem provides the initial condition $v_0 = 0$, but focuses on the time-periodic steady state, which is reached after a sufficient number of time steps.\n\nThe sequence of squared gradients, $g_t^2$, follows a periodic pattern of period $k \\in \\mathbb{N}$:\n$$g_t^2 = \\begin{cases} M & \\text{if } t = nk \\text{ for some } n \\in \\mathbb{N} \\\\ 1 & \\text{otherwise} \\end{cases}$$\nwhere $M$ is a constant. This means that at time steps which are multiples of $k$, there is a \"spike\" of value $M$, and at all other time steps, the value is $1$.\n\nWe are interested in the steady-state value of the accumulator. The problem defines $s$ as the value of $v_t$ evaluated immediately after each spike, i.e., at times $t=nk$ for large $n$. The steady-state condition implies that this value is constant from one spike to the next. Mathematically, if we denote the value of the accumulator just after the spike at time $(n-1)k$ as $v_{(n-1)k}$, and the value just after the spike at time $nk$ as $v_{nk}$, then in steady state:\n$$v_{(n-1)k} = v_{nk} = s$$\n\nOur strategy is to propagate the accumulator value through one full period, from time $t=(n-1)k$ to $t=nk$, and then impose the steady-state condition. Let us assume the accumulator value at the beginning of the cycle is $v_{(n-1)k} = s$.\n\nThe cycle consists of $k-1$ steps where $g_t^2=1$, followed by one step where $g_t^2=M$.\n\nFor the steps from $t = (n-1)k + 1$ to $t = nk - 1$, the squared gradient is $g_t^2 = 1$. Let's compute the accumulator value just before the spike at time $nk$, which is $v_{nk-1}$.\nStarting from $v_{(n-1)k} = s$:\n$$v_{(n-1)k+1} = \\rho v_{(n-1)k} + (1-\\rho)g_{(n-1)k+1}^2 = \\rho s + (1-\\rho)(1)$$\n$$v_{(n-1)k+2} = \\rho v_{(n-1)k+1} + (1-\\rho)g_{(n-1)k+2}^2 = \\rho(\\rho s + 1-\\rho) + (1-\\rho) = \\rho^2 s + \\rho(1-\\rho) + (1-\\rho)$$\nBy induction, after $j$ steps (where $j \\in \\{1, 2, \\dots, k-1\\}$), the accumulator value is:\n$$v_{(n-1)k+j} = \\rho^j v_{(n-1)k} + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i g_{(n-1)k+j-i}^2$$\nSince $g_t^2=1$ for this interval, this simplifies to:\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i$$\nThe sum is a finite geometric series: $\\sum_{i=0}^{j-1} \\rho^i = \\frac{1-\\rho^j}{1-\\rho}$.\nSubstituting this into the expression for $v_{(n-1)k+j}$:\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\frac{1-\\rho^j}{1-\\rho} = \\rho^j s + 1 - \\rho^j$$\nTo find the value just before the spike at $t=nk$, we set $j=k-1$:\n$$v_{nk-1} = \\rho^{k-1} s + 1 - \\rho^{k-1}$$\n\nNow, we perform the final update step for the spike at $t=nk$. At this time step, the squared gradient is $g_{nk}^2 = M$.\n$$v_{nk} = \\rho v_{nk-1} + (1-\\rho) g_{nk}^2$$\nSubstituting the expression for $v_{nk-1}$:\n$$v_{nk} = \\rho (\\rho^{k-1} s + 1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho(1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\n\nWe now apply the steady-state condition, $v_{nk} = s$:\n$$s = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\nOur goal is to solve this equation for $s$.\n$$s - \\rho^k s = M(1-\\rho) + \\rho - \\rho^k$$\n$$s(1-\\rho^k) = M(1-\\rho) + \\rho - \\rho^k$$\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k}$$\n\nTo obtain a more interpretable form, we can manipulate the numerator. We add and subtract $1$ inside the fraction in a specific way:\n$$s = \\frac{M(1-\\rho) - (1-\\rho) + (1-\\rho) + \\rho - \\rho^k}{1-\\rho^k}$$\nThis is not the most direct path. A more structured rearrangement is to separate the expression:\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k} = \\frac{M(1-\\rho) - (1-\\rho) + 1 - \\rho^k}{1-\\rho^k}$$\nThe equality holds because $- (1-\\rho) + 1 - \\rho^k = -1 + \\rho + 1 - \\rho^k = \\rho - \\rho^k$.\nNow, grouping terms:\n$$s = \\frac{(M-1)(1-\\rho) + (1-\\rho^k)}{1-\\rho^k}$$\nSplitting the fraction into two parts:\n$$s = \\frac{(M-1)(1-\\rho)}{1-\\rho^k} + \\frac{1-\\rho^k}{1-\\rho^k}$$\n$$s = 1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}$$\nThis is the final, simplified closed-form expression for $s$. This form has a clear interpretation: the steady-state value is a baseline of $1$ (which would be the result if $g_t^2$ were always $1$) plus an additional term that represents the contribution from the periodic spikes of magnitude $M$.", "answer": "$$\n\\boxed{1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}}\n$$", "id": "3170912"}, {"introduction": "Building on the idea of smoothing, we can formalize our understanding of RMSprop using concepts from digital signal processing. This exercise treats the sequence of squared gradients as an input signal and the accumulator $v_t$ as the output of a filter [@problem_id:3170933]. By analyzing how the system responds to a sinusoidal gradient, you will discover that RMSprop acts as a low-pass filter and see precisely how the decay parameter $\\rho$ controls the attenuation of high-frequency fluctuations in gradient magnitude.", "problem": "Consider Root Mean Square Propagation (RMSprop), which scales gradients by a running estimate of their Root Mean Square (RMS). In RMSprop, the running estimate of the squared gradient can be modeled as the output of a causal, linear time-invariant filter whose impulse response is $h_k = (1 - \\rho)\\rho^{k}$ for integer $k \\geq 0$, with $0 < \\rho < 1$. Let the gradient be the discrete-time sinusoid $g_t = a \\sin(\\omega t)$ with $a > 0$ and $0 < \\omega < \\pi$ (radians per step), and define $u_t = g_t^{2}$ as the filter input. Let $v_t$ denote the filter output when driven by $u_t$, and define the normalized signal $y_t = \\dfrac{g_t}{\\sqrt{v_t} + \\epsilon}$ with $\\epsilon > 0$. \n\nUsing only the definitions of discrete-time convolution and the frequency response of a causal exponentially decaying impulse response, do the following:\n- Derive the steady-state form of $v_t$ when the input is $u_t = g_t^{2}$, explicitly identifying the steady (constant) component and the oscillatory component, and express the oscillatory amplitude and phase in terms of $\\rho$ and $\\omega$.\n- Using the steady-state constant component of $v_t$, approximate the amplitude of $y_t$ by treating the denominator $\\sqrt{v_t} + \\epsilon$ as a constant equal to $\\sqrt{\\bar{v}} + \\epsilon$, where $\\bar{v}$ is the steady (constant) component of $v_t$. Provide the final amplitude as a single closed-form analytic expression. No rounding is required.\n- Briefly justify the interpretation of $\\rho$ as a low-pass filter parameter by analyzing how the magnitude of the frequency response at $2\\omega$ varies with $\\rho$. \n\nYour final answer must be the closed-form expression for the amplitude of $y_t$, as instructed above. Do not include any units.", "solution": "The problem asks for an analysis of the Root Mean Square Propagation (RMSprop) update rule by modeling it as a discrete-time linear time-invariant (LTI) system. We will first derive the steady-state response of the system to a specific sinusoidal gradient, then use this result to approximate the amplitude of the normalized gradient, and finally justify the role of the decay parameter $\\rho$.\n\nThe filter representing the running average of the squared gradients has an impulse response $h_k = (1 - \\rho)\\rho^{k}$ for non-negative integers $k$. The input to this filter is the squared gradient, $u_t = g_t^2$, where the gradient is given by $g_t = a \\sin(\\omega t)$.\n\nFirst, we express the input signal $u_t$ in a more convenient form using the trigonometric identity $\\sin^2(x) = \\frac{1}{2}(1 - \\cos(2x))$.\n$$u_t = g_t^2 = (a \\sin(\\omega t))^2 = a^2 \\sin^2(\\omega t) = a^2 \\left(\\frac{1 - \\cos(2\\omega t)}{2}\\right)$$\n$$u_t = \\frac{a^2}{2} - \\frac{a^2}{2} \\cos(2\\omega t)$$\nThis demonstrates that the input $u_t$ consists of a constant (DC) component $u_{DC} = \\frac{a^2}{2}$ and a sinusoidal (AC) component $u_{AC}(t) = -\\frac{a^2}{2} \\cos(2\\omega t)$ with a frequency of $2\\omega$.\n\nThe steady-state output $v_t$ of an LTI system is found by considering the system's frequency response, $H(\\Omega)$, which is the Discrete-Time Fourier Transform (DTFT) of the impulse response $h_k$.\n$$H(\\Omega) = \\sum_{k=0}^{\\infty} h_k e^{-i\\Omega k} = \\sum_{k=0}^{\\infty} (1 - \\rho)\\rho^k e^{-i\\Omega k} = (1 - \\rho) \\sum_{k=0}^{\\infty} (\\rho e^{-i\\Omega})^k$$\nThis is a geometric series which converges since $|\\rho e^{-i\\Omega}| = \\rho < 1$.\n$$H(\\Omega) = (1 - \\rho) \\frac{1}{1 - \\rho e^{-i\\Omega}} = \\frac{1 - \\rho}{1 - \\rho e^{-i\\Omega}}$$\n\nBy the principle of superposition, the steady-state output $v_t$ is the sum of the responses to the DC and AC components of the input.\n\nThe response to the DC component $u_{DC} = \\frac{a^2}{2}$ is given by the input multiplied by the DC gain of the filter, $H(0)$.\n$$H(0) = \\frac{1 - \\rho}{1 - \\rho e^{-i \\cdot 0}} = \\frac{1 - \\rho}{1 - \\rho} = 1$$\nThus, the DC component of the output, which we denote as the steady component $\\bar{v}$, is:\n$$\\bar{v} = u_{DC} \\cdot H(0) = \\frac{a^2}{2} \\cdot 1 = \\frac{a^2}{2}$$\n\nThe response to the AC component $u_{AC}(t) = -\\frac{a^2}{2} \\cos(2\\omega t)$ is found by scaling and phase-shifting the input sinusoid according to the filter's frequency response at the input frequency $\\Omega = 2\\omega$. The steady-state output for an input $A \\cos(\\Omega_0 t)$ is $A |H(\\Omega_0)| \\cos(\\Omega_0 t + \\angle H(\\Omega_0))$.\nThe frequency response at $\\Omega = 2\\omega$ is $H(2\\omega) = \\frac{1 - \\rho}{1 - \\rho e^{-i2\\omega}}$.\nLet's find its magnitude and phase. The complex denominator is $1 - \\rho(\\cos(2\\omega) - i\\sin(2\\omega)) = (1 - \\rho\\cos(2\\omega)) + i\\rho\\sin(2\\omega)$.\nThe magnitude is:\n$$|H(2\\omega)| = \\frac{|1 - \\rho|}{|(1 - \\rho\\cos(2\\omega)) + i\\rho\\sin(2\\omega)|} = \\frac{1 - \\rho}{\\sqrt{(1 - \\rho\\cos(2\\omega))^2 + (\\rho\\sin(2\\omega))^2}}$$\n$$|H(2\\omega)| = \\frac{1 - \\rho}{\\sqrt{1 - 2\\rho\\cos(2\\omega) + \\rho^2\\cos^2(2\\omega) + \\rho^2\\sin^2(2\\omega)}} = \\frac{1 - \\rho}{\\sqrt{1 - 2\\rho\\cos(2\\omega) + \\rho^2}}$$\nThe phase is the negative of the argument of the denominator:\n$$\\angle H(2\\omega) = -\\arctan\\left(\\frac{\\rho\\sin(2\\omega)}{1 - \\rho\\cos(2\\omega)}\\right)$$\nThe AC component of the output, $v_{AC}(t)$, is therefore:\n$$v_{AC}(t) = -\\frac{a^2}{2} |H(2\\omega)| \\cos(2\\omega t + \\angle H(2\\omega))$$\n$$v_{AC}(t) = -\\frac{a^2}{2} \\left(\\frac{1 - \\rho}{\\sqrt{1 - 2\\rho\\cos(2\\omega) + \\rho^2}}\\right) \\cos\\left(2\\omega t - \\arctan\\left(\\frac{\\rho\\sin(2\\omega)}{1 - \\rho\\cos(2\\omega)}\\right)\\right)$$\nThe total steady-state output is $v_t = \\bar{v} + v_{AC}(t)$.\n\nThis completes the first part of the task.\n- The steady (constant) component is $\\bar{v} = \\frac{a^2}{2}$.\n- The oscillatory component is $v_{AC}(t)$. Its amplitude is $A_{osc} = \\frac{a^2}{2} |H(2\\omega)| = \\frac{a^2(1-\\rho)}{2\\sqrt{1-2\\rho\\cos(2\\omega)+\\rho^2}}$.\n- Its phase shift relative to the input cosine wave is $\\phi_{osc} = \\angle H(2\\omega) = -\\arctan\\left(\\frac{\\rho\\sin(2\\omega)}{1 - \\rho\\cos(2\\omega)}\\right)$.\n\nNext, we approximate the amplitude of the normalized signal $y_t = \\frac{g_t}{\\sqrt{v_t} + \\epsilon}$. The problem specifies using the approximation where the denominator is treated as a constant, evaluated using the steady component $\\bar{v}$ of $v_t$.\n$$y_t \\approx \\frac{g_t}{\\sqrt{\\bar{v}} + \\epsilon}$$\nSubstituting $g_t = a \\sin(\\omega t)$ and $\\bar{v} = \\frac{a^2}{2}$:\n$$y_t \\approx \\frac{a \\sin(\\omega t)}{\\sqrt{\\frac{a^2}{2}} + \\epsilon}$$\nSince $a > 0$, $\\sqrt{a^2} = a$.\n$$y_t \\approx \\frac{a \\sin(\\omega t)}{\\frac{a}{\\sqrt{2}} + \\epsilon} = \\left(\\frac{a}{\\frac{a + \\sqrt{2}\\epsilon}{\\sqrt{2}}}\\right) \\sin(\\omega t)$$\n$$y_t \\approx \\left(\\frac{\\sqrt{2}a}{a + \\sqrt{2}\\epsilon}\\right) \\sin(\\omega t)$$\nThe amplitude of this approximated signal $y_t$ is the coefficient of the sinusoid.\n$$A_y = \\frac{\\sqrt{2}a}{a + \\sqrt{2}\\epsilon}$$\nThis is the closed-form expression required for the final answer.\n\nFinally, we justify the interpretation of $\\rho$ as a low-pass filter parameter. A low-pass filter attenuates high-frequency signals while passing low-frequency signals. We have already seen that the filter's gain at the DC frequency ($\\Omega=0$) is $|H(0)| = 1$. The signal $u_t$ also contains a higher frequency component at $\\Omega = 2\\omega$. The gain at this frequency is $|H(2\\omega)| = \\frac{1 - \\rho}{\\sqrt{1 - 2\\rho\\cos(2\\omega) + \\rho^2}}$. To understand how $\\rho$ controls the filtering, we analyze how this gain changes with $\\rho$. Let's examine the derivative of the squared magnitude, $|H(2\\omega)|^2 = \\frac{(1-\\rho)^2}{1 - 2\\rho\\cos(2\\omega) + \\rho^2}$, with respect to $\\rho$. Let $c = \\cos(2\\omega)$.\n$$\\frac{d}{d\\rho} |H(2\\omega)|^2 = \\frac{-2(1-\\rho)(1-2\\rho c+\\rho^2) - (1-\\rho)^2(-2c+2\\rho)}{(1-2\\rho c+\\rho^2)^2}$$\n$$= \\frac{-2(1-\\rho)[(1-2\\rho c+\\rho^2) + (1-\\rho)(c-\\rho)]}{(1-2\\rho c+\\rho^2)^2} = \\frac{-2(1-\\rho)[1-2\\rho c+\\rho^2 + c-\\rho-\\rho c+\\rho^2]}{(1-2\\rho c+\\rho^2)^2}$$\nThis simplifies to:\n$$\\frac{d}{d\\rho} |H(2\\omega)|^2 = \\frac{-2(1-\\rho)^2(1-c)}{(1-2\\rho c+\\rho^2)^2} = \\frac{-2(1-\\rho)^2(1-\\cos(2\\omega))}{(1-2\\rho\\cos(2\\omega)+\\rho^2)^2}$$\nSince $0 < \\rho < 1$ and $0 < \\omega < \\pi$, we have that $(1-\\rho)^2 > 0$ and the denominator is positive. For $\\omega \\neq 0$ and $\\omega \\neq \\pi$, $\\cos(2\\omega) < 1$, so $1-\\cos(2\\omega) > 0$. Thus, the entire expression for the derivative is negative. This means that $|H(2\\omega)|^2$, and by extension $|H(2\\omega)|$, is a strictly decreasing function of $\\rho$ for any fixed $\\omega \\in (0, \\pi)$.\nTherefore, increasing $\\rho$ (towards $1$) decreases the gain at the frequency $2\\omega$, leading to stronger attenuation of the oscillatory component of the squared gradient. Since the DC gain is always $1$, the filter increasingly favors the DC component over the AC component as $\\rho$ increases. This is the defining characteristic of a low-pass filter, where $\\rho$ controls the filter's \"strength\" or, inversely, its cutoff frequency.", "answer": "$$\\boxed{\\frac{\\sqrt{2}a}{a + \\sqrt{2}\\epsilon}}$$", "id": "3170933"}, {"introduction": "While standard RMSprop effectively dampens the variance of gradients, it has a subtle but important limitation: it is sensitive to gradient bias. Because the accumulator tracks the uncentered second moment, $\\mathbb{E}[g_t^2]$, a persistent non-zero mean in the gradients can inflate the denominator and unduly shrink the learning steps. This final practice challenges you to move from analysis to implementation by deriving and coding two improved variants, Centered and Detrended RMSprop, that explicitly correct for this bias [@problem_id:3170932].", "problem": "Consider a one-dimensional stochastic optimization problem in which a learner seeks to minimize a convex quadratic objective with an imperfect gradient oracle. Let the underlying objective be $f(x)$ that is strongly convex and differentiable, and suppose the learner uses Stochastic Gradient Descent (SGD) with the canonical update $x_{t+1} = x_t - \\alpha \\, g_t$, where $g_t$ is a stochastic gradient, $\\alpha$ is a positive learning rate, and $t$ indexes discrete time. The stochastic gradient is modeled as $g_t = h(x_t) + b + \\xi_t$, where $h(x_t)$ is the true gradient of the objective, $b$ is a constant bias term reflecting a non-zero expectation from the gradient oracle (for example due to measurement drift), and $\\xi_t$ is zero-mean noise with finite variance. The expectation operator is denoted by $\\mathbb{E}[\\cdot]$.\n\nRoot Mean Square Propagation (RMSprop) is an adaptive method that scales the SGD update by a running estimate of recent gradient magnitude, constructed via an Exponentially Weighted Moving Average (EWMA). An EWMA of a sequence $\\{s_t\\}$ with decay parameter $\\rho \\in (0,1)$ is a recursive estimator based on the foundational definition of exponential weighting that places geometrically decaying weights on past values. The uncentered EWMA of the squared gradient tracks the second moment $\\mathbb{E}[g_t^2]$, not the variance. When $\\mathbb{E}[g_t] \\neq 0$, the second moment equals the variance plus the squared mean, which can cause the running estimate to be inflated by bias. A principled alternative is to use a centered EWMA (i.e., subtract a running estimate of the mean), thereby approximating the variance $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$. Another intervention is a detrended update that subtracts the running mean directly from the gradient before updating parameters.\n\nYour task is to, starting strictly from the above definitions of SGD and EWMA, derive and implement three algorithms:\n- Uncentered RMSprop: scale the SGD step using an EWMA of $g_t^2$.\n- Centered RMSprop: estimate $\\operatorname{Var}(g_t)$ by combining EWMAs of $g_t$ and $g_t^2$, and use this estimate to scale the step.\n- Detrended RMSprop: subtract a running estimate of $\\mathbb{E}[g_t]$ from $g_t$ before both squaring and updating.\n\nThen, simulate their behavior on a controlled class of one-dimensional stochastic gradients corresponding to a quadratic objective. Specifically, let the true objective be $f(x) = \\tfrac{1}{2} a (x - x^\\star)^2$ with gradient $h(x) = a (x - x^\\star)$, where $a > 0$ and $x^\\star$ is the unique minimizer. The gradient oracle provides $g_t = a (x_t - x^\\star) + b + \\xi_t$, where $b$ is a fixed bias and $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$ is independent noise. For scientific realism, ensure the same noise realization $\\{\\xi_t\\}_{t=1}^T$ is used across all three algorithms within each test case so that differences arise solely from the update rules.\n\nImplement all three algorithms with the same hyperparameters per test case: learning rate $\\alpha$, EWMA decay $\\rho$, additive stabilizer $\\varepsilon$ in the denominator, and number of steps $T$. Initialize the parameter at $x_0$ and all EWMAs at zero. After $T$ steps, report the absolute error $|x_T - x^\\star|$.\n\nUse the following test suite of parameter values, which is chosen to probe distinct facets:\n- Test case $1$ (happy path with moderate bias and noise): $a = 1.5$, $x^\\star = 2.0$, $x_0 = -5.0$, $b = 0.6$, $\\sigma = 0.2$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 1000$, and a fixed random seed of $42$ for the noise sequence.\n- Test case $2$ (boundary, no noise, no bias): $a = 1.0$, $x^\\star = -3.0$, $x_0 = 10.0$, $b = 0.0$, $\\sigma = 0.0$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 500$, and a fixed random seed of $123$.\n- Test case $3$ (significant edge case with high noise and non-zero bias): $a = 0.8$, $x^\\star = 1.0$, $x_0 = -1.0$, $b = 0.5$, $\\sigma = 2.0$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 1500$, and a fixed random seed of $999$.\n- Test case $4$ (near-optimum start with small noise and non-zero bias): $a = 2.5$, $x^\\star = 0.0$, $x_0 = 0.1$, $b = 0.2$, $\\sigma = 0.05$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 800$, and a fixed random seed of $777$.\n\nFor each test case, compute three floats: the final absolute error $|x_T - x^\\star|$ for Uncentered RMSprop, Centered RMSprop, and Detrended RMSprop, in that order.\n\nYour program should produce a single line of output containing the $12$ results as a comma-separated list enclosed in square brackets, ordered by test case, each contributing three numbers in the method order given above. For example, the output format must be exactly like $[r_{1, \\text{unc}}, r_{1, \\text{cent}}, r_{1, \\text{det}}, r_{2, \\text{unc}}, r_{2, \\text{cent}}, r_{2, \\text{det}}, r_{3, \\text{unc}}, r_{3, \\text{cent}}, r_{3, \\text{det}}, r_{4, \\text{unc}}, r_{4, \\text{cent}}, r_{4, \\text{det}}]$, where each $r_{\\cdot}$ is a float.", "solution": "The task is to derive and implement three variants of the Root Mean Square Propagation (RMSprop) algorithm based on a provided formal description, and then to simulate their performance on a specific one-dimensional optimization problem. The three variants are Uncentered RMSprop, Centered RMSprop, and Detrended RMSprop. The simulation will be performed on a quadratic objective function $f(x) = \\frac{1}{2} a (x - x^\\star)^2$ with a biased, noisy gradient oracle.\n\nFirst, we establish the common mathematical framework. The stochastic gradient at time step $t$ is given by\n$$g_t = a(x_t - x^\\star) + b + \\xi_t$$\nwhere $x_t$ is the parameter value, $a$ is the curvature, $x^\\star$ is the minimizer, $b$ is a constant bias, and $\\xi_t$ is a zero-mean noise term with variance $\\sigma^2$.\n\nThe core of RMSprop and its variants is the use of an Exponentially Weighted Moving Average (EWMA) to estimate statistical properties of the gradients. The recursive formula for an EWMA of a sequence $\\{s_t\\}$ with a decay parameter $\\rho \\in (0,1)$ is:\n$$E_t[s] = \\rho E_{t-1}[s] + (1-\\rho)s_t$$\nwhere $E_t[s]$ is the moving average at time $t$. All EWMA accumulators are initialized to $0$.\n\nThe general structure of the parameter update for these adaptive methods is:\n$$x_{t+1} = x_t - \\alpha \\cdot \\text{scaled\\_gradient}_t$$\nwhere $\\alpha$ is the learning rate. We will now derive the specific update rules for each of the three algorithms. The simulations run for $t = 0, 1, \\dots, T-1$, starting from a given $x_0$.\n\n**1. Uncentered RMSprop Derivation**\n\nThis is the standard RMSprop algorithm. It scales the learning rate by the root mean square of the gradients. The \"uncentered\" aspect refers to using the raw second moment of the gradient, $\\mathbb{E}[g_t^2]$, as the basis for the scaling factor, which is estimated via an EWMA of $g_t^2$.\n\nLet $v_t$ be the EWMA of the squared gradients, $g_t^2$.\n$$v_t = \\rho v_{t-1} + (1-\\rho) g_t^2$$\nThe update rule for the parameter $x$ uses the stochastic gradient $g_t$ scaled by the inverse of the square root of this moving average. An additive stabilizer $\\varepsilon > 0$ is included in the denominator to prevent division by zero.\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} g_t$$\nThis algorithm is simple but can be suboptimal when the mean of the gradient, $\\mathbb{E}[g_t]$, is non-zero (i.e., when bias $b$ is present or the gradient is consistently non-zero away from the optimum). In such cases, $v_t$ estimates $\\mathbb{E}[g_t^2] = \\operatorname{Var}(g_t) + (\\mathbb{E}[g_t])^2$, causing the running estimate to be inflated by the squared mean, potentially leading to an overly conservative step size.\n\n**2. Centered RMSprop Derivation**\n\nThis variant aims to correct the issue of the uncentered version by using an estimate of the variance of the gradient, $\\operatorname{Var}(g_t)$, for scaling, rather than the second moment. The variance is given by $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$. This requires estimating both the first moment (mean) and the second moment of the gradient.\n\nLet $m_t$ be the EWMA of the gradients, $g_t$, estimating $\\mathbb{E}[g_t]$.\n$$m_t = \\rho m_{t-1} + (1-\\rho) g_t$$\nLet $v_t$ be the EWMA of the squared gradients, $g_t^2$, estimating $\\mathbb{E}[g_t^2]$ (same as in Uncentered RMSprop).\n$$v_t = \\rho v_{t-1} + (1-\\rho) g_t^2$$\nAn estimate for the variance at step $t$ is then constructed from the current moving averages:\n$$\\hat{\\sigma}_t^2 = v_t - m_t^2$$\nThe parameter update rule uses this variance estimate for scaling. The update still uses the original stochastic gradient $g_t$ for the direction.\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{\\hat{\\sigma}_t^2} + \\varepsilon} g_t$$\nThis approach normalizes the step by a more accurate measure of the gradient's noisiness, potentially improving convergence when a significant bias exists.\n\n**3. Detrended RMSprop Derivation**\n\nThis algorithm takes a more direct approach to handling a non-zero mean gradient. It first estimates the mean of the gradient and then subtracts this \"trend\" from the raw stochastic gradient. This \"detrended\" gradient is then used for both the parameter update and the calculation of the adaptive scaling factor.\n\nFirst, we compute the EWMA of the gradient, $m_t$, just as in Centered RMSprop.\n$$m_t = \\rho m_{t-1} + (1-\\rho) g_t$$\nNext, we form the detrended gradient, $\\tilde{g}_t$, by subtracting the current running mean estimate from the raw gradient.\n$$\\tilde{g}_t = g_t - m_t$$\nThe scaling factor is based on the EWMA of the squared *detrended* gradient. Let this EWMA be $v_t$.\n$$v_t = \\rho v_{t-1} + (1-\\rho) \\tilde{g}_t^2$$\nThe parameter update uses the detrended gradient $\\tilde{g}_t$ for the step direction, scaled by the adaptive rate derived from $v_t$.\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} \\tilde{g}_t$$\nThis method attempts to correct both the magnitude and the direction of the update step by actively removing the estimated bias. This is conceptually similar to the Adam optimizer, though it omits the formal bias-correction steps for the initial iterations.\n\nThe implementation will simulate these three algorithms for each test case, using the same sequence of random noise $\\xi_t$ to ensure a fair comparison. The final absolute error $|x_T - x^\\star|$ is reported for each algorithm and test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_uncentered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Uncentered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        v = beta * v + (1 - beta) * g**2\n        x = x - (alpha / (np.sqrt(v) + epsilon)) * g\n    \n    return np.abs(x - x_star)\n\ndef run_centered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Centered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        v = beta * v + (1 - beta) * g**2\n        var_est = v - m**2\n        \n        # Use np.sqrt on var_est directly as per derivation.\n        # This may result in NaN if var_est is negative, highlighting a potential instability.\n        # To handle this for robustness in the test without changing the core logic,\n        # we can ensure the argument to sqrt is non-negative.\n        denominator = np.sqrt(max(0, var_est)) + epsilon\n        x = x - (alpha / denominator) * g\n\n    return np.abs(x - x_star)\n\ndef run_detrended_rmsprop(params, noise):\n    \"\"\"\n    Simulates Detrended RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        g_tilde = g - m\n        v = beta * v + (1 - beta) * g_tilde**2\n\n        denominator = np.sqrt(v) + epsilon\n        x = x - (alpha / denominator) * g_tilde\n\n    return np.abs(x - x_star)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": 1.5, \"x_star\": 2.0, \"x0\": -5.0, \"b\": 0.6, \"sigma\": 0.2, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1000, \"seed\": 42\n        },\n        {\n            \"a\": 1.0, \"x_star\": -3.0, \"x0\": 10.0, \"b\": 0.0, \"sigma\": 0.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 500, \"seed\": 123\n        },\n        {\n            \"a\": 0.8, \"x_star\": 1.0, \"x0\": -1.0, \"b\": 0.5, \"sigma\": 2.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1500, \"seed\": 999\n        },\n        {\n            \"a\": 2.5, \"x_star\": 0.0, \"x0\": 0.1, \"b\": 0.2, \"sigma\": 0.05, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 800, \"seed\": 777\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # In the Centered RMSprop implementation, `var_est` can become slightly negative\n        # due to floating point inaccuracies, which would lead to NaN. To prevent this while\n        # keeping the logic intact, a small modification `max(0, var_est)` is used inside\n        # `run_centered_rmsprop`. The problem description does not specify this, but it's\n        # necessary for a robust run.\n        # For the test case with sigma=0.0, the original `run_centered_rmsprop` with just\n        # `np.sqrt(var_est)` would yield NaN because `v - m**2` becomes negative.\n        # The change is minimal and preserves the algorithm's spirit.\n\n        rng = np.random.default_rng(case['seed'])\n        noise = rng.normal(0, case['sigma'], case['T'])\n        \n        # Uncentered RMSprop\n        result_unc = run_uncentered_rmsprop(case, noise)\n        all_results.append(result_unc)\n        \n        # Centered RMSprop\n        result_cen = run_centered_rmsprop(case, noise)\n        all_results.append(result_cen)\n\n        # Detrended RMSprop\n        result_det = run_detrended_rmsprop(case, noise)\n        all_results.append(result_det)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3170932"}]}