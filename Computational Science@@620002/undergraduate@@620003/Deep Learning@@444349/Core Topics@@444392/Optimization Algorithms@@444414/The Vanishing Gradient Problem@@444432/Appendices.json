{"hands_on_practices": [{"introduction": "We begin at the source of the gradient signal: the output layer of a neuron. This practice challenges you to derive and compare the gradients for a binary classifier using two different loss functions, Mean Squared Error ($L_{\\text{MSE}}$) and Cross-Entropy ($L_{\\text{CE}}$) [@problem_id:3194463]. Through this derivation, you will discover firsthand how a careful choice of loss function can counteract the saturating effect of an activation like the sigmoid function, $\\sigma(z)$, providing a powerful mechanism to prevent vanishing gradients.", "problem": "Consider a binary classification neuron with pre-activation $z \\in \\mathbb{R}$, activation $a = \\sigma(z)$ where $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$, and target $y \\in \\{0, 1\\}$. Two common loss functions are used at the output layer: mean squared error (MSE) and cross-entropy. The mean squared error (MSE) loss is defined as $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$. The cross-entropy loss is defined as $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$. Using only the definitions above and basic calculus (in particular, the chain rule), derive expressions for $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z}$ and $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z}$ in terms of $a$, $y$, and $z$. Then analyze the behavior of these gradients in the saturated regimes where $|z|$ is large. Your reasoning should begin from the exact definition of $\\sigma(z)$ and not assume any pre-quoted derivative formulas. Based on your derivations and analysis, which of the following statements is most accurate?\n\nA. With mean squared error (MSE), the gradient with respect to $z$ takes the form $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$ and therefore tends to vanish when $|z|$ is large because $\\sigma^{\\prime}(z) \\approx 0$. With cross-entropy, the gradient simplifies to $\\sigma(z) - y$, which does not include the extra factor $\\sigma^{\\prime}(z)$, making vanishing less severe when $|z|$ is large.\n\nB. With cross-entropy, the gradient with respect to $z$ takes the form $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$, which makes vanishing more severe than with mean squared error (MSE), where the gradient simplifies to $\\sigma(z) - y$.\n\nC. Both mean squared error (MSE) and cross-entropy produce identical gradients with respect to $z$, namely $\\sigma(z) - y$, so there is no difference in vanishing behavior for large $|z|$.\n\nD. The derivative $\\sigma^{\\prime}(z)$ is approximately constant when $|z|$ is large, so neither mean squared error (MSE) nor cross-entropy suffers from vanishing gradients at the output layer.\n\nSelect the single best choice.", "solution": "The problem statement will first be validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n### Step 1: Extract Givens\n-   A binary classification neuron is considered.\n-   Pre-activation: $z \\in \\mathbb{R}$.\n-   Activation function: $a = \\sigma(z)$, where $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$.\n-   Target label: $y \\in \\{0, 1\\}$.\n-   Mean Squared Error (MSE) loss: $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$.\n-   Cross-Entropy (CE) loss: $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$.\n-   The task is to derive the gradients $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z}$ and $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z}$ and analyze their behavior for large $|z|$.\n-   The derivation must use only the provided definitions and basic calculus, without assuming pre-quoted derivative formulas.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is valid.\n-   **Scientifically Grounded:** The definitions for the sigmoid activation function, mean squared error loss, and binary cross-entropy loss are standard and fundamental in the field of machine learning and neural networks. The question addresses the well-established phenomenon of vanishing gradients.\n-   **Well-Posed:** The problem is clearly defined and provides all necessary information to derive the requested quantities and perform the analysis. A unique, definite answer can be obtained through standard calculus.\n-   **Objective:** The problem is phrased using precise mathematical language, free from ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a well-formed, standard exercise in the differential calculus of neural network components.\n\n### Step 3: Derivation and Analysis\n\nThe core of the problem requires applying the chain rule to find the gradient of the loss function $L$ with respect to the pre-activation $z$. The chain of dependencies is $z \\rightarrow a \\rightarrow L$. Therefore, the general form of the gradient is:\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z}\n$$\nThe term $\\frac{\\partial a}{\\partial z}$ is the derivative of the activation function, $\\sigma'(z)$. We must derive this from first principles as stipulated.\n\n**1. Derivative of the Sigmoid Function**\n\nThe sigmoid function is $\\sigma(z) = \\dfrac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$.\nUsing the power rule and the chain rule, its derivative with respect to $z$ is:\n$$\n\\frac{\\partial a}{\\partial z} = \\sigma'(z) = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1 \\cdot (1 + e^{-z})^{-2} \\cdot \\frac{d}{dz}(1 + e^{-z})\n$$\nThe derivative of the inner term is $\\frac{d}{dz}(1 + e^{-z}) = -e^{-z}$. Substituting this back, we get:\n$$\n\\sigma'(z) = - (1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n$$\nTo express this in a more common form, we can manipulate the expression:\n$$\n\\sigma'(z) = \\left( \\frac{1}{1 + e^{-z}} \\right) \\cdot \\left( \\frac{e^{-z}}{1 + e^{-z}} \\right)\n$$\nThe first term is $\\sigma(z)$. For the second term, we can write:\n$$\n\\frac{e^{-z}}{1 + e^{-z}} = \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = 1 - \\frac{1}{1 + e^{-z}} = 1 - \\sigma(z)\n$$\nTherefore, the derivative of the sigmoid function is:\n$$\n\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))\n$$\nSince $a = \\sigma(z)$, we can also write this as $\\dfrac{\\partial a}{\\partial z} = a(1-a)$.\n\n**2. Gradient for Mean Squared Error (MSE) Loss**\n\nThe MSE loss is $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$.\nFirst, we find the derivative of the loss with respect to the activation $a$:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{2} (a - y)^{2} \\right] = 2 \\cdot \\frac{1}{2} (a - y) \\cdot 1 = a - y\n$$\nNow, applying the chain rule to find the gradient with respect to $z$:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = \\frac{\\partial L_{\\text{MSE}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = (a - y) \\cdot \\sigma'(z)\n$$\nSubstituting the expression for $\\sigma'(z)$:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = (a - y) a (1-a)\n$$\nOr, in terms of $\\sigma(z)$:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = (\\sigma(z) - y) \\sigma'(z)\n$$\n\n**3. Gradient for Cross-Entropy (CE) Loss**\n\nThe CE loss is $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$.\nFirst, we find the derivative of the loss with respect to the activation $a$:\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ -y \\log(a) - (1-y) \\log(1-a) \\right] = - \\left[ \\frac{y}{a} + (1-y) \\frac{-1}{1-a} \\right]\n$$\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a} = \\frac{-y(1-a) + a(1-y)}{a(1-a)} = \\frac{-y + ay + a - ay}{a(1-a)} = \\frac{a-y}{a(1-a)}\n$$\nNow, applying the chain rule to find the gradient with respect to $z$:\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\frac{\\partial L_{\\text{CE}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = \\left( \\frac{a-y}{a(1-a)} \\right) \\cdot (a(1-a))\n$$\nThe $a(1-a)$ terms cancel out, leaving a remarkably simple result:\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = a - y\n$$\nOr, in terms of $\\sigma(z)$:\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\sigma(z) - y\n$$\n\n**4. Analysis of Gradient Behavior**\n\nThe problem asks to analyze the behavior in the saturated regimes, where $|z|$ is large.\n-   As $z \\to \\infty$, $e^{-z} \\to 0$, so $a = \\sigma(z) \\to \\dfrac{1}{1+0} = 1$.\n-   As $z \\to -\\infty$, $e^{-z} \\to \\infty$, so $a = \\sigma(z) \\to \\dfrac{1}{1+\\infty} \\to 0$.\n\nLet's examine the sigmoid derivative, $\\sigma'(z) = a(1-a)$, in these regimes:\n-   As $z \\to \\infty$, $a \\to 1$, so $\\sigma'(z) \\to 1(1-1) = 0$.\n-   As $z \\to -\\infty$, $a \\to 0$, so $\\sigma'(z) \\to 0(1-0) = 0$.\nIn both saturated regimes, $\\sigma'(z)$ approaches zero.\n\nNow, let's analyze the two loss gradients:\n-   **MSE Gradient:** $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z} = (a - y) \\sigma'(z)$. Because this gradient is directly proportional to $\\sigma'(z)$, it will approach $0$ as $|z|$ becomes large, regardless of the size of the error $(a-y)$. This is the **vanishing gradient problem**. For example, if the target is $y=0$ but the neuron is saturated with $z \\gg 0$ (so $a \\approx 1$), the error $(a-y)$ is large ($\\approx 1$), but the gradient will be close to $0$ because $\\sigma'(z) \\approx 0$. Learning will be extremely slow.\n\n-   **CE Gradient:** $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z} = a - y$. This gradient does *not* contain the $\\sigma'(z)$ term. Let's consider the same \"confidently wrong\" case: $y=0$ and $a \\approx 1$. The gradient is $\\approx 1-0=1$, which is a strong signal for updating the parameters. If $y=1$ and $a \\approx 0$, the gradient is $\\approx 0-1 = -1$, also a strong signal. The gradient only becomes small when the prediction is correct, i.e., $a \\approx y$. Thus, the combination of cross-entropy loss with a sigmoid activation function largely mitigates the vanishing gradient problem at the output layer.\n\n### Option-by-Option Evaluation\n\n**A. With mean squared error (MSE), the gradient with respect to $z$ takes the form $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$ and therefore tends to vanish when $|z|$ is large because $\\sigma^{\\prime}(z) \\approx 0$. With cross-entropy, the gradient simplifies to $\\sigma(z) - y$, which does not include the extra factor $\\sigma^{\\prime}(z)$, making vanishing less severe when $|z|$ is large.**\n-   Our derivation for MSE yielded $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z} = (\\sigma(z)-y)\\sigma'(z)$. Our analysis confirmed that this vanishes for large $|z|$ due to the $\\sigma'(z)$ term. This part is correct.\n-   Our derivation for CE yielded $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z} = \\sigma(z)-y$. Our analysis confirmed that this form avoids the vanishing caused by the $\\sigma'(z)$ term. This part is also correct.\n-   **Verdict: Correct.**\n\n**B. With cross-entropy, the gradient with respect to $z$ takes the form $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$, which makes vanishing more severe than with mean squared error (MSE), where the gradient simplifies to $\\sigma(z) - y$.**\n-   This statement swaps the derived gradient expressions for MSE and CE. This is factually incorrect based on our derivations.\n-   **Verdict: Incorrect.**\n\n**C. Both mean squared error (MSE) and cross-entropy produce identical gradients with respect to $z$, namely $\\sigma(z) - y$, so there is no difference in vanishing behavior for large $|z|$.**\n-   Our derivations show that the gradients are different: $(\\sigma(z) - y)\\sigma'(z)$ for MSE and $\\sigma(z) - y$ for CE. Therefore, this statement is false.\n-   **Verdict: Incorrect.**\n\n**D. The derivative $\\sigma^{\\prime}(z)$ is approximately constant when $|z|$ is large, so neither mean squared error (MSE) nor cross-entropy suffers from vanishing gradients at the output layer.**\n-   The premise is false. Our analysis showed that $\\sigma'(z) \\to 0$ as $|z| \\to \\infty$. It is not a non-zero constant.\n-   The conclusion is also false, as MSE does suffer from vanishing gradients precisely because $\\sigma'(z) \\to 0$.\n-   **Verdict: Incorrect.**\n\nBased on the rigorous derivations, statement A is the only accurate description of the situation.", "answer": "$$\\boxed{A}$$", "id": "3194463"}, {"introduction": "The vanishing gradient problem is especially pronounced in architectures that involve deep computational graphs, such as Recurrent Neural Networks (RNNs). In this exercise, you will set up a synthetic task that requires learning a long-term dependency and demonstrate both analytically and empirically how the gradient signal decays exponentially over time [@problem_id:3194489]. This provides a clear visualization of why standard RNNs struggle to connect events separated by long time lags, as the gradient magnitude with respect to early inputs scales like $O(|w|^T)$ for a dependency of length $T$.", "problem": "You are asked to formalize and empirically verify the vanishing gradient phenomenon in a recurrent computation that encodes a long-term dependency. Consider a scalar, time-unrolled recurrent computation defined by the recurrence $h_t = w \\, h_{t-1} + x_t$ with initial condition $h_0 = 0$ and output $y_t = h_t$, where $w \\in \\mathbb{R}$ is a trainable scalar parameter and $\\{x_t\\}$ is a scalar input sequence. You will construct a synthetic dependency where the supervised signal at time $t$ depends on an input $x_{t-T}$ that occurred $T$ steps earlier. The training loss at time $t$ is $L_t = \\tfrac{1}{2} \\, (y_t - \\mathrm{target}_t)^2$. In this setup, the gradient of $L_t$ with respect to the parameter $w$ is obtained via backpropagation through time, which applies the chain rule repeatedly across $T$ steps.\n\nFundamental base you may use:\n- The chain rule from calculus: for a composition of differentiable functions $f \\circ g$, one has $\\frac{d}{dx} f(g(x)) = f'(g(x)) \\, g'(x)$.\n- The definition of backpropagation through time for a scalar recurrence: if $\\delta_t = \\frac{\\partial L_t}{\\partial h_t}$, then for the linear recurrence given above, $\\delta_{t-1} = \\delta_t \\, w$ and the per-time-step parameter gradient contribution is $\\frac{\\partial L_t}{\\partial w}\\bigg|_{\\text{at step }k} = \\delta_k \\, h_{k-1}$.\n\nYour program must do the following, purely in mathematical terms:\n1. Construct a sequence $\\{x_t\\}$ of length $T+1$ where $x_1 = 1$ and $x_t = 0$ for all $t \\neq 1$. This is a unit impulse at time $t=1$. For each fixed lag $T$, define the supervised time index $t^\\star = T+1$ and the target $\\mathrm{target}_{t^\\star} = x_{t^\\star - T} = x_1 = 1$; thus the loss is $L_{t^\\star} = \\tfrac{1}{2}(y_{t^\\star} - 1)^2$.\n2. For each chosen $T$, compute the forward pass to obtain $h_{t^\\star}$, then set $\\delta_{t^\\star} = \\frac{\\partial L_{t^\\star}}{\\partial h_{t^\\star}} = h_{t^\\star} - 1$, and then propagate the error $T$ steps backward using $\\delta_{k-1} = w \\, \\delta_k$ to obtain $\\delta_{t^\\star - T}$. Define the ratio $r(T) = \\left|\\delta_{t^\\star - T}\\right| \\big/ \\left|\\delta_{t^\\star}\\right|$. This ratio isolates the pure multiplicative effect of transporting the gradient signal across $T$ steps.\n3. For fixed $w$ with $|w| lt; 1$, model $r(T)$ by an exponential $r(T) \\approx C \\, \\lambda^T$ and estimate $\\lambda$ by performing a least-squares fit of $\\log r(T)$ versus $T$ to the affine model $\\log r(T) \\approx a \\, T + b$. The estimate is $\\hat{\\lambda} = e^a$. This empirically captures the $O(\\lambda^T)$ decay rate of the backpropagated signal.\n4. Repeat the estimation across a small test suite of parameter values to verify coverage of different regimes, including a case with sign oscillation due to negative $w$ and a slow-decay case where $w$ is close to $1$ in magnitude.\n\nAnalytical requirement to show in your solution:\n- Derive from first principles (chain rule and linear recurrence) that the backpropagated error satisfies $\\delta_{t^\\star - k} = w^k \\, \\delta_{t^\\star}$ for $k \\in \\{1,\\dots,T\\}$, hence $r(T) = |w|^T$. Then argue that the per-step contribution to the gradient with respect to $w$ at time $t^\\star - T$ is $\\delta_{t^\\star - T} \\, h_{t^\\star - T - 1}$, and because $|h_{t^\\star - T - 1}|$ is bounded for $|w|1$ under the specified impulse input, the magnitude of this contribution is $O(|w|^T)$. Conclude that the gradient signal decays geometrically with depth, with base $\\lambda = |w|  1$.\n\nTest suite:\n- Use four cases with $w \\in \\{0.2, 0.5, -0.8, 0.95\\}$.\n- For each case, compute $r(T)$ for every integer $T$ in the range $T \\in \\{5, 6, \\dots, 80\\}$ and fit $\\hat{\\lambda}$ as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list of the four estimated values $\\hat{\\lambda}$, in the same order as the test suite $w$ values, enclosed in square brackets. For determinism, round each estimated value to six decimal places. For example, an output with placeholders would look like $[0.123456,0.234567,0.345678,0.456789]$. There are no physical units or angles involved in this task.", "solution": "The problem asks for an analytical derivation and empirical verification of the vanishing gradient phenomenon in a simple, time-unrolled recurrent computation.\n\n### 1. Problem Formalization and Analytical Derivation\n\nWe are given a scalar recurrent computation defined by the recurrence relation:\n$$\nh_t = w \\, h_{t-1} + x_t\n$$\nwith an initial condition $h_0 = 0$. The parameter $w \\in \\mathbb{R}$ is learnable, and $\\{x_t\\}$ is an input sequence. The output is $y_t = h_t$.\n\nThe synthetic task uses an impulse input sequence of length $T+1$ where $x_1 = 1$ and $x_t = 0$ for all $t \\neq 1$. The supervision occurs at time step $t^\\star = T+1$, with the target being $\\mathrm{target}_{t^\\star} = x_{t^\\star - T} = x_1 = 1$. The loss function at this time step is:\n$$\nL_{t^\\star} = \\frac{1}{2} (y_{t^\\star} - \\mathrm{target}_{t^\\star})^2 = \\frac{1}{2} (h_{t^\\star} - 1)^2\n$$\n\n**Forward Pass Analysis**\nWe can unroll the recurrence for the given input sequence to find an expression for $h_t$:\n- For $t=1$: $h_1 = w h_0 + x_1 = w \\cdot 0 + 1 = 1$.\n- For $t=2$: $h_2 = w h_1 + x_2 = w \\cdot 1 + 0 = w$.\n- For $t=3$: $h_3 = w h_2 + x_3 = w \\cdot w + 0 = w^2$.\nBy induction, for any time step $k \\ge 1$, the hidden state is $h_k = w^{k-1}$.\nAt the time of supervision, $t^\\star = T+1$, the hidden state is $h_{t^\\star} = h_{T+1} = w^{(T+1)-1} = w^T$.\n\n**Backward Pass Analysis (Backpropagation Through Time)**\nThe gradient of the loss with respect to the parameter $w$ is found by applying the chain rule through time. The process starts by calculating the gradient of the loss with respect to the output state, $h_{t^\\star}$. Let $\\delta_k = \\frac{\\partial L_{t^\\star}}{\\partial h_k}$ represent the error signal at time step $k$.\n\nThe initial error signal at time $t^\\star$ is:\n$$\n\\delta_{t^\\star} = \\frac{\\partial L_{t^\\star}}{\\partial h_{t^\\star}} = \\frac{\\partial}{\\partial h_{t^\\star}} \\left( \\frac{1}{2} (h_{t^\\star} - 1)^2 \\right) = h_{t^\\star} - 1 = w^T - 1\n$$\nThe problem provides the rule for propagating this error signal backward in time: $\\delta_{k-1} = \\frac{\\partial L_{t^\\star}}{\\partial h_{k-1}} = \\frac{\\partial L_{t^\\star}}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_{k-1}} = \\delta_k \\cdot w$. This is a geometric progression. We can derive a closed-form expression for the error signal $\\delta_{t^\\star-k}$ after being propagated back $k$ steps:\n- For $k=1$: $\\delta_{t^\\star-1} = w \\, \\delta_{t^\\star}$.\n- For $k=2$: $\\delta_{t^\\star-2} = w \\, \\delta_{t^\\star-1} = w (w \\, \\delta_{t^\\star}) = w^2 \\delta_{t^\\star}$.\nBy induction, for any integer $k \\in \\{1, \\dots, T\\}$, the error signal is:\n$$\n\\delta_{t^\\star-k} = w^k \\delta_{t^\\star}\n$$\nThis demonstrates that the error signal's magnitude is scaled by a factor of $w$ at each step of backpropagation.\n\nThe ratio $r(T)$ is defined to isolate this multiplicative effect over the full dependency length $T$:\n$$\nr(T) = \\frac{|\\delta_{t^\\star-T}|}{|\\delta_{t^\\star}|}\n$$\nUsing our derived formula for the backpropagated error with $k=T$, we get $\\delta_{t^\\star-T} = w^T \\delta_{t^\\star}$. Substituting this into the definition of $r(T)$:\n$$\nr(T) = \\frac{|w^T \\delta_{t^\\star}|}{|\\delta_{t^\\star}|} = |w^T| = |w|^T\n$$\nThis analytical result shows that the ratio of the backpropagated error signal to the initial error signal decays exponentially with the time lag $T$, with the base of the exponent being $|w|$. For $|w|1$, this ratio tends to zero as $T$ increases, which is the essence of the vanishing gradient problem.\n\n**Gradient Contribution Analysis**\nThe total gradient of the loss with respect to the weight, $\\frac{\\partial L_{t^\\star}}{\\partial w}$, is the sum of contributions from each time step in the computational graph: $\\frac{\\partial L_{t^\\star}}{\\partial w} = \\sum_{k=1}^{t^\\star} \\frac{\\partial L_{t^\\star}}{\\partial w}\\big|_{\\text{at step }k}$. The per-step contribution is given as $\\delta_k h_{k-1}$. The problem asks to analyze this contribution at time $t^\\star - T = (T+1) - T = 1$.\nThe contribution at step $k=1$ is $\\delta_1 h_0$.\nFrom the backpropagation analysis, we have $\\delta_1 = \\delta_{t^\\star - T} = w^T \\delta_{t^\\star}$. The initial condition is given as $h_0 = 0$.\nTherefore, the contribution at this step is $\\delta_1 h_0 = (w^T \\delta_{t^\\star}) \\cdot 0 = 0$.\nThe magnitude of this contribution is $0$, which trivially satisfies the condition of being $O(|w|^T)$. More generally, any gradient contribution term $\\delta_k h_{k-1}$ contains the factor $\\delta_k = w^{t^\\star-k} \\delta_{t^\\star}$. For contributions arising from long time lags (i.e., small $k$), the term $w^{t^\\star-k}$ will be a high power of $w$, leading to an exponentially small value when $|w|1$. This is the \"vanishing gradient signal\" that makes it difficult for the model to learn long-term dependencies.\n\n### 2. Empirical Verification\n\nThe analytical result $r(T) = |w|^T$ is an exponential function of $T$. To empirically verify this and estimate the decay base $\\lambda = |w|$, we can linearize the relationship by taking the logarithm:\n$$\n\\log r(T) = \\log(|w|^T) = T \\log|w|\n$$\nThis equation is in the form of a line, $y = aT + b$, where the dependent variable is $y = \\log r(T)$, the independent variable is $T$, the slope is $a = \\log|w|$, and the intercept is $b=0$.\n\nWe will perform the following procedure for each given value of $w$:\n1. Generate data pairs $(T, \\log r(T))$ for $T \\in \\{5, 6, \\dots, 80\\}$ using the derived formula $r(T) = |w|^T$.\n2. Perform a linear least-squares regression on these data points to fit the model $\\log r(T) \\approx aT + b$. This will yield an estimate for the slope, $\\hat{a}$.\n3. From the relationship $a = \\log \\lambda$, we can estimate the decay base as $\\hat{\\lambda} = e^{\\hat{a}}$.\nBased on our analysis, we expect the empirical estimate $\\hat{\\lambda}$ to be very close to the theoretical value $|w|$. The provided implementation will carry out this estimation for the specified test suite of $w$ values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the vanishing gradient problem for a simple recurrent computation.\n\n    For different values of a weight parameter 'w', this function simulates\n    the decay of a gradient signal over a time lag 'T'. It then performs a\n    log-linear regression to empirically estimate the decay rate lambda, which is\n    theoretically equal to |w|.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # w values for the recurrence h_t = w * h_{t-1} + x_t\n        0.2,\n        0.5,\n        -0.8,\n        0.95,\n    ]\n\n    # Define the range of time lags T for the analysis.\n    T_values = np.arange(5, 81)\n    \n    # List to store the estimated decay rates.\n    estimated_lambdas = []\n\n    for w in test_cases:\n        # 1. Analytically determine the ratio r(T).\n        # From the derivation, r(T) = |w|^T.\n        # We handle the case w=0 separately to avoid log(0).\n        if w == 0:\n            # For w=0, the gradient signal is always zero for T1, so decay is immediate.\n            # The base lambda is technically 0.\n            estimated_lambdas.append(0.0)\n            continue\n            \n        r_T = np.abs(w) ** T_values\n        \n        # 2. Linearize the model by taking the logarithm.\n        # log(r(T)) = log(|w|^T) = T * log|w|.\n        # This is the form y = a*x, where y=log(r(T)), x=T, a=log|w|.\n        log_r_T = np.log(r_T)\n        \n        # 3. Perform a linear least-squares fit to estimate the slope 'a'.\n        # We fit the model y = a*x + b to the data (T, log r(T)).\n        # The matrix 'A' sets up the system of linear equations for the fit.\n        A = np.vstack([T_values, np.ones_like(T_values)]).T\n        \n        # `np.linalg.lstsq` solves the equation Ax = y for x, where x = [a, b].\n        # It returns the solution that minimizes the Euclidean 2-norm ||y - Ax||^2.\n        # The slope 'a' is the first element of the solution vector.\n        slope, _ = np.linalg.lstsq(A, log_r_T, rcond=None)[0]\n        \n        # 4. Estimate lambda from the slope.\n        # Since slope 'a' is an estimate of log(|w|) = log(lambda),\n        # lambda can be estimated by exponentiating the slope.\n        lambda_hat = np.exp(slope)\n        \n        # 5. Store the result, rounded to the specified precision.\n        estimated_lambdas.append(round(lambda_hat, 6))\n\n    # Final print statement in the exact required format.\n    # e.g., [0.200000,0.500000,0.800000,0.950000]\n    print(f\"[{','.join(map(str, ['{:.6f}'.format(x) for x in estimated_lambdas]))}]\")\n\nsolve()\n```", "id": "3194489"}, {"introduction": "Rather than reacting to vanishing gradients, we can design networks that are resilient to them from the very beginning. This practice delves into one of the most effective solutions: principled weight initialization [@problem_id:3194508]. You will analyze how the variance of signals propagates through a deep network with ReLU activations and empirically verify how a \"smart\" strategy like He initialization maintains signal variance near $1$ across layers, thereby enabling the effective training of much deeper models.", "problem": "Consider a fully connected feedforward neural network of depth $L$ with Rectified Linear Unit (ReLU) nonlinearity. Let the layer index be $l \\in \\{1,\\dots,L\\}$, the width (number of units) of each layer be $n$, and the input batch size be $B$. Denote the post-activation at layer $l$ as $x_l \\in \\mathbb{R}^{B \\times n}$ and the preactivation at layer $l$ as $z_l \\in \\mathbb{R}^{B \\times n}$. The forward propagation obeys the rules $z_l = x_{l-1} W_l$ and $x_l = \\phi(z_l)$, where $\\phi$ is the ReLU function defined by $\\phi(u) = \\max(0,u)$ elementwise, and $W_l \\in \\mathbb{R}^{n \\times n}$ is the weight matrix for layer $l$. Assume $x_0$ has entries that are independent and identically distributed (i.i.d.) Gaussian with zero mean and unit variance. At initialization, assume the entries of each $W_l$ are i.i.d., zero mean, and independent of $x_{l-1}$, and consider two initialization schemes: He initialization with $\\mathrm{Var}(W_{l,ij}) = \\frac{2}{n}$ and Xavier (Glorot) initialization with $\\mathrm{Var}(W_{l,ij}) = \\frac{1}{n}$. Biases are zero.\n\nThe objective is to analyze the vanishing gradient problem from first principles. You must start from the following base:\n- The chain rule of calculus for backpropagation in feedforward neural networks.\n- The definition of variance $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ and the independence rule for variances of sums of independent zero-mean random variables, namely, if $U_i$ are independent and have zero mean, then $\\mathrm{Var}\\left[\\sum_i U_i\\right] = \\sum_i \\mathrm{Var}[U_i]$.\n- The properties of ReLU acting on a zero-mean symmetric input: its derivative is an indicator $\\phi'(u) = \\mathbb{1}\\{u0\\}$, which acts as a gate with probability $\\frac{1}{2}$ under symmetry.\n\nUsing these bases, reason about how moments propagate forward and backward at initialization. In particular, define the forward “energy” at layer $l$ as the second raw moment $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_l^2\\big]$ (the expectation is over both batch and units), and define the backward “energy” at layer $l$ as the second raw moment of the gradient with respect to preactivations $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_l)^2\\big]$ for the squared norm loss $L = \\tfrac{1}{2}\\lVert x_L\\rVert_2^2$ computed per sample without averaging over the batch. Under the He initialization and the independence assumptions, show why $m_l^{\\text{fwd}}$ and $m_l^{\\text{bwd}}$ are preserved near $1$ across layers at initialization. Contrast this with Xavier initialization under ReLU, and explain why it tends to reduce $m_l^{\\text{fwd}}$ as depth increases, creating a vanishing gradient scenario.\n\nYour program must empirically verify these statements by Monte Carlo simulation. For each test case, construct a random network and batch as specified, perform a single forward pass to compute all $z_l$ and $x_l$, and a single backward pass using the chain rule to compute $\\partial L/\\partial z_l$ for all $l$. For each layer, compute:\n- The forward second raw moment $m_l^{\\text{fwd}}$ as the empirical mean of squared entries of $x_l$.\n- The backward second raw moment $m_l^{\\text{bwd}}$ as the empirical mean of squared entries of $\\partial L/\\partial z_l$.\nAggregate the deviations from $1$ by computing the maximum absolute deviation across layers for forward and backward, that is, $\\Delta^{\\text{fwd}} = \\max_l \\lvert m_l^{\\text{fwd}} - 1\\rvert$ and $\\Delta^{\\text{bwd}} = \\max_l \\lvert m_l^{\\text{bwd}} - 1\\rvert$. A test case “passes” if both $\\Delta^{\\text{fwd}}$ and $\\Delta^{\\text{bwd}}$ are strictly less than a tolerance $\\varepsilon = 0.2$.\n\nTest suite:\n- Case $1$: He initialization, $L=1$, $n=64$, $B=2000$.\n- Case $2$: He initialization, $L=20$, $n=64$, $B=2000$.\n- Case $3$: He initialization, $L=40$, $n=64$, $B=2000$.\n- Case $4$: Xavier initialization, $L=20$, $n=64$, $B=2000$.\n\nAnswer format:\n- Your program should produce a single line of output containing the pass/fail results for the four test cases as a comma-separated list enclosed in square brackets, for example, $\\texttt{[True,True,True,False]}$.\n\nNo physical units or angle units are involved. All expectations and variances are dimensionless. Implement the simulation and computations precisely as described, without using any external files or inputs, and use fixed random seeds to ensure reproducibility.", "solution": "The problem is valid as it is scientifically grounded in the principles of deep learning, is well-posed with all necessary information provided, and is formulated objectively. We proceed to the theoretical analysis and subsequent empirical verification.\n\nThe core of the analysis lies in deriving recurrence relations for the propagation of the second raw moments of activations (forward pass) and gradients (backward pass) through the layers of the network at initialization. We define the forward \"energy\" as $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_{l,ik}^2\\big]$ and the backward \"energy\" as $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_{l,ik})^2\\big]$, where the expectation is taken over the batch dimension $i$, the unit dimension $k$, and the random initialization.\n\n**Forward Propagation Analysis**\n\nThe forward pass is defined by $z_l = x_{l-1} W_l$ and $x_l = \\phi(z_l)$, where $\\phi$ is the ReLU function. Let's consider a single pre-activation element $z_{l,ik} = \\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}$. At initialization, the weights $W_{l,jk}$ are independent of the inputs $x_{l-1, ij}$, and both have zero mean. Consequently, $\\mathbb{E}[z_{l,ik}] = \\sum_{j=1}^{n} \\mathbb{E}[x_{l-1, ij}] \\mathbb{E}[W_{l,jk}] = 0$, under the assumption that $\\mathbb{E}[x_{l-1, ij}] = 0$. This holds for the input layer $x_0$, and we assume it approximately holds for subsequent layers due to the symmetric nature of the updates.\n\nThe variance of $z_{l,ik}$ is $\\mathrm{Var}(z_{l,ik}) = \\mathbb{E}[z_{l,ik}^2]$ since its mean is zero. Using the independence of the terms in the sum:\n$$ \\mathrm{Var}(z_{l,ik}) = \\mathrm{Var}\\left(\\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}\\right) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij} W_{l,jk}) $$\nBy independence of $x_{l-1}$ and $W_l$, and their zero-mean property, $\\mathrm{Var}(AB) = \\mathbb{E}[A^2]\\mathbb{E}[B^2] = \\mathrm{Var}(A)\\mathrm{Var}(B)$. Thus:\n$$ \\mathrm{Var}(z_{l,ik}) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij}) \\mathrm{Var}(W_{l,jk}) = n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\nHere, $\\mathrm{Var}(x_{l-1})$ is the variance of any element in $x_{l-1}$, and $\\mathrm{Var}(W_l)$ is the variance of any element in $W_l$.\nThe activation is $x_l = \\phi(z_l)$. We need the second moment $m_l^{\\text{fwd}} = \\mathbb{E}[x_l^2] = \\mathbb{E}[\\phi(z_l)^2]$. For a zero-mean symmetric input $z_l$ (approximated as Gaussian by the Central Limit Theorem), the ReLU function $\\phi(u)=\\max(0,u)$ effectively nullifies half of the distribution. The second moment of the output is half the second moment of the input: $\\mathbb{E}[\\phi(z_l)^2] = \\frac{1}{2}\\mathbb{E}[z_l^2]$.\nThus, $m_l^{\\text{fwd}} = \\frac{1}{2} \\mathbb{E}[z_l^2] = \\frac{1}{2} \\mathrm{Var}(z_l)$.\n\nCombining these results, we get a recurrence for the forward energy:\n$$ m_l^{\\text{fwd}} = \\frac{1}{2} n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\nWe make the standard approximation that the mean of activations remains close to zero, so $\\mathrm{Var}(x_{l-1}) \\approx \\mathbb{E}[x_{l-1}^2] = m_{l-1}^{\\text{fwd}}$. This gives:\n$$ m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\mathrm{Var}(W_l) $$\nThe base case is for the input layer $x_0$, which has i.i.d. entries with mean $0$ and variance $1$. Therefore, $m_0^{\\text{fwd}} = \\mathbb{E}[x_0^2] = \\mathrm{Var}(x_0) = 1$.\n\n*   **He Initialization**: With $\\mathrm{Var}(W_l) = \\frac{2}{n}$, the recurrence becomes $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{2}{n} = m_{l-1}^{\\text{fwd}}$. Starting from $m_0^{\\text{fwd}}=1$, the forward energy is preserved across layers, i.e., $m_l^{\\text{fwd}} \\approx 1$ for all $l$.\n*   **Xavier Initialization**: With $\\mathrm{Var}(W_l) = \\frac{1}{n}$, the recurrence becomes $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{1}{n} = \\frac{1}{2} m_{l-1}^{\\text{fwd}}$. This leads to an exponential decay: $m_l^{\\text{fwd}} \\approx (\\frac{1}{2})^l m_0^{\\text{fwd}} = (\\frac{1}{2})^l$. The activations' energy vanishes as the network depth $L$ increases.\n\n**Backward Propagation Analysis**\n\nThe gradient with respect to the pre-activations is given by the chain rule: $\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}} \\frac{\\partial z_{l+1}}{\\partial x_l} \\frac{\\partial x_l}{\\partial z_l}$. Let $\\delta_l = \\frac{\\partial L}{\\partial z_l}$. A single component is:\n$$ \\delta_{l,ik} = \\left( (\\delta_{l+1} W_{l+1}^T)_{ik} \\right) \\cdot \\phi'(z_{l,ik}) = \\left( \\sum_{j=1}^n \\delta_{l+1, ij} W_{l+1, kj} \\right) \\cdot \\phi'(z_{l,ik}) $$\nWe want to find $m_l^{\\text{bwd}} = \\mathbb{E}[\\delta_{l,ik}^2]$. The term in parentheses is independent of $\\phi'(z_{l,ik})$ at initialization. Thus, $\\mathbb{E}[\\delta_{l,ik}^2] = \\mathbb{E}[(\\sum_j \\dots)^2] \\cdot \\mathbb{E}[(\\phi'(z_{l,ik}))^2]$.\nThe derivative of ReLU is $\\phi'(u) = \\mathbb{1}\\{u0\\}$, so $(\\phi'(u))^2 = \\phi'(u)$. For a symmetric, zero-mean $z_{l,ik}$, $\\mathbb{P}(z_{l,ik}0) = \\frac{1}{2}$, so $\\mathbb{E}[(\\phi'(z_{l,ik}))^2] = \\frac{1}{2}$.\nThe variance of the sum is $\\mathrm{Var}(\\sum_j \\delta_{l+1, ij} W_{l+1, kj}) = n \\cdot \\mathrm{Var}(\\delta_{l+1}) \\cdot \\mathrm{Var}(W_{l+1})$. Assuming $\\mathbb{E}[\\delta_{l+1}] \\approx 0$, this is $n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1})$.\n\nCombining these, we get the backward recurrence relation:\n$$ m_l^{\\text{bwd}} \\approx \\frac{1}{2} n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1}) $$\nFor the base case at layer $L$, the loss is $L = \\frac{1}{2} \\|x_L\\|_2^2$ per sample. The gradient is $\\delta_L = \\frac{\\partial L}{\\partial z_L} = \\frac{\\partial L}{\\partial x_L}\\frac{\\partial x_L}{\\partial z_L}$. Here, $\\frac{\\partial L}{\\partial x_L} = x_L$ and $\\frac{\\partial x_L}{\\partial z_L}$ is a diagonal matrix of $\\phi'(z_L)$. So $\\delta_L = x_L \\odot \\phi'(z_L)$. Since $x_L = \\phi(z_L)$, we have $\\delta_L = \\phi(z_L) \\odot \\phi'(z_L) = \\phi(z_L) = x_L$.\nTherefore, the base case for the backward energy is $m_L^{\\text{bwd}} = \\mathbb{E}[\\delta_L^2] = \\mathbb{E}[x_L^2] = m_L^{\\text{fwd}}$.\n\n*   **He Initialization**: With $\\mathrm{Var}(W_{l+1}) = \\frac{2}{n}$, the recurrence is $m_l^{\\text{bwd}} \\approx m_{l+1}^{\\text{bwd}}$. From the forward pass, $m_L^{\\text{fwd}} \\approx 1$, so $m_L^{\\text{bwd}} \\approx 1$. Propagating backward, we find $m_l^{\\text{bwd}} \\approx 1$ for all $l$. The gradient energy is preserved.\n*   **Xavier Initialization**: With $\\mathrm{Var}(W_{l+1}) = \\frac{1}{n}$, the recurrence is $m_l^{\\text{bwd}} \\approx \\frac{1}{2} m_{l+1}^{\\text{bwd}}$. The gradient energy decays exponentially as it propagates from layer $L$ to layer $1$. This is the vanishing gradient problem. The gradients in early layers become too small to facilitate effective learning.\n\nThe simulation will empirically verify these two contrasting behaviors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs Monte Carlo simulations to verify the signal propagation properties of\n    He and Xavier initializations in deep ReLU networks.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'init': 'he', 'L': 1, 'n': 64, 'B': 2000, 'name': 'Case 1'},\n        {'init': 'he', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 2'},\n        {'init': 'he', 'L': 40, 'n': 64, 'B': 2000, 'name': 'Case 3'},\n        {'init': 'xavier', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 4'},\n    ]\n\n    results = []\n    for case in test_cases:\n        passes_test = run_simulation(\n            init_scheme=case['init'],\n            L=case['L'],\n            n=case['n'],\n            B=case['B']\n        )\n        results.append(passes_test)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(init_scheme: str, L: int, n: int, B: int) - bool:\n    \"\"\"\n    Performs a single forward and backward pass for a given network configuration\n    and checks if the deviation criteria are met.\n\n    Args:\n        init_scheme: Either 'he' or 'xavier'.\n        L: Depth of the network.\n        n: Width of each layer.\n        B: Batch size.\n\n    Returns:\n        A boolean indicating if the test case passes.\n    \"\"\"\n    # 1. Initialization\n    # Input data: i.i.d. Gaussian with zero mean and unit variance.\n    x0 = np.random.randn(B, n)\n\n    # Determine weight variance based on initialization scheme.\n    if init_scheme == 'he':\n        var_w = 2.0 / n\n    elif init_scheme == 'xavier':\n        var_w = 1.0 / n\n    else:\n        raise ValueError(\"Unknown initialization scheme.\")\n    \n    std_w = np.sqrt(var_w)\n\n    # Initialize weights for L layers.\n    weights = [np.random.randn(n, n) * std_w for _ in range(L)]\n\n    # 2. Forward Pass\n    z_values = {}\n    x_values = {0: x0}\n    \n    x_current = x0\n    for l in range(1, L + 1):\n        # Linear transformation\n        z_l = x_current @ weights[l-1]\n        # ReLU activation\n        x_l = np.maximum(0, z_l)\n        \n        z_values[l] = z_l\n        x_values[l] = x_l\n        x_current = x_l\n    \n    # 3. Compute Forward Moments\n    m_fwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(x_values[l]**2)\n        m_fwd.append(moment)\n\n    # 4. Backward Pass\n    dz_values = {}\n    \n    # Base case: Gradient of the loss w.r.t final pre-activations.\n    # For L = 0.5 * ||x_L||^2, dL/dz_L = x_L.\n    # This simplification comes from dL/dx_L = x_L and dL/dz_L = dL/dx_L * phi'(z_L),\n    # which simplifies to x_L * phi'(z_L) = phi(z_L) = x_L.\n    dz_L = x_values[L]\n    dz_values[L] = dz_L\n    \n    # Propagate gradients backward from L-1 to 1.\n    dz_current = dz_L\n    for l in range(L - 1, 0, -1):\n        # Gradient w.r.t previous activation layer\n        dx_l = dz_current @ weights[l].T\n        \n        # Derivative of ReLU\n        phi_prime_l = (z_values[l]  0).astype(float)\n        \n        # Gradient w.r.t pre-activation layer\n        dz_l = dx_l * phi_prime_l\n        \n        dz_values[l] = dz_l\n        dz_current = dz_l\n\n    # 5. Compute Backward Moments\n    m_bwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(dz_values[l]**2)\n        m_bwd.append(moment)\n\n    # 6. Check Pass/Fail Condition\n    epsilon = 0.2\n    \n    # Maximum absolute deviation from 1 for forward moments\n    delta_fwd = np.max(np.abs(np.array(m_fwd) - 1))\n    \n    # Maximum absolute deviation from 1 for backward moments\n    delta_bwd = np.max(np.abs(np.array(m_bwd) - 1))\n\n    return delta_fwd  epsilon and delta_bwd  epsilon\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3194508"}]}