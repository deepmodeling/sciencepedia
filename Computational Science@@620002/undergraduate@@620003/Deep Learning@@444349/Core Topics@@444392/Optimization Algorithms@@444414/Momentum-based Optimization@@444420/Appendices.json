{"hands_on_practices": [{"introduction": "At its core, the momentum update relies on an Exponential Moving Average (EMA) to accumulate information from past gradients. The momentum coefficient, $\\beta$, directly controls the \"memory\" of this averaging process. This practice [@problem_id:3154008] challenges you to move beyond qualitative intuition by formalizing and quantifying this memory as an \"effective averaging window,\" giving you a more concrete grasp of how $\\beta$ shapes the optimizer's update step.", "problem": "You are asked to formalize and validate the notion of the effective averaging window length of an Exponential Moving Average (EMA) used in momentum-based optimization for deep learning. Consider the standard EMA estimator used for gradients in methods such as momentum and adaptive moment estimation, defined recursively as follows: at discrete time step index $t$, the estimate $m_t$ updates using a decay parameter $\\beta \\in [0,1)$ and a scalar gradient signal $g_t$ according to a linear recursion that combines the previous estimate $m_{t-1}$ with the current $g_t$. Assume $m_0 = 0$.\n\nYour tasks are:\n\n- Starting from the given recursion and the superposition principle for linear time-invariant systems, define the impulse response weights $w_k$ that quantify how much a gradient from $k$ steps in the past contributes to the current EMA estimate. Argue that these weights form a nonnegative sequence that sums to $1$, and interpret them as a probability mass function over the nonnegative integers.\n\n- Using only the foundational facts about geometric series, define two quantitative notions of an effective averaging window length:\n  1. The cumulative-mass window length $L_p$, defined for a fixed threshold $p \\in (0,1)$ as the smallest integer $L$ such that $\\sum_{k=0}^{L-1} w_k \\ge p$. For this problem, use $p = 0.95$ and treat this as a unitless decimal threshold.\n  2. The effective sample size $N_{\\mathrm{eff}}$ defined by $N_{\\mathrm{eff}} = 1/\\sum_{k=0}^{\\infty} w_k^2$, which corresponds to the number of equally weighted samples that would yield the same variance reduction as the geometrically weighted average under independent and identically distributed noise.\n\n- Derive a simple closed-form baseline for an “effective averaging window length” that one obtains by treating the normalized weights as a probability mass function and using the first moment to define a characteristic window length. This baseline emerges directly from the sum of a geometric series. Do not assume any external results beyond geometric-series properties.\n\n- Design a computational validation task: for a set of decay parameters $\\beta$, compute the ratios of the two effective window length metrics to the simple geometric-series-based baseline. Concretely, for each $\\beta$, compute\n  - $r_L = L_{0.95} \\,/\\, \\text{baseline}$,\n  - $r_N = N_{\\mathrm{eff}} \\,/\\, \\text{baseline}$.\n\nYour program must implement the above quantities and produce results for the following test suite of decay parameters, which probes a happy path and boundary-like regimes:\n- $\\beta \\in \\{\\, 0.0,\\, 0.5,\\, 0.9,\\, 0.99,\\, 0.999 \\,\\}$.\n\nEdge cases to consider:\n- When $\\beta = 0$, the EMA reduces to the current gradient only.\n- As $\\beta \\to 1^{-}$, the EMA retains very long memory and the effective window length grows large.\n\nImplementation details and output format requirements:\n- Your program must be self-contained and produce no output other than the final line described below.\n- Use $p = 0.95$ expressed as a decimal in all computations.\n- For each $\\beta$ in the given test suite, compute $r_L$ and $r_N$ as defined above.\n- Report the final results as a single line containing a list of lists, one inner list per $\\beta$, in the same order as listed above, where each inner list contains the two ratios $[r_L, r_N]$ as floating-point numbers rounded to three decimal places.\n- The required final output format is exactly:\n  - A single line: a string representation of a Python list of lists, for example, $[[a_{11},a_{12}],[a_{21},a_{22}],\\dots]$, with each $a_{ij}$ being a decimal rounded to three places and without any additional text.\n\nNo physical units are involved in this problem. Angles are not used. Percentages must be expressed as decimals, not with a percentage sign. The entire problem is purely mathematical and algorithmic, grounded in the definition of the EMA and properties of geometric series. Ensure scientific realism and numerical stability by accounting for $\\beta$ values close to $1$ using exact or provably convergent series manipulations rather than naive truncation when appropriate.", "solution": "The problem asks for a formalization and validation of the concept of an effective averaging window length for an Exponential Moving Average (EMA) as used in deep learning. We will proceed by first deriving the impulse response of the EMA filter, then defining three distinct metrics for the effective window length based on fundamental principles, and finally computing the ratios of these metrics for a given set of parameters.\n\nThe EMA update rule is a linear recursion that computes the new estimate $m_t$ at time step $t$ from the previous estimate $m_{t-1}$ and the current input signal $g_t$ (e.g., a gradient). The parameter $\\beta \\in [0,1)$ controls the decay rate. The problem states that the weights on past gradients must sum to $1$, which uniquely specifies the standard form of the recursion:\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\nGiven the initial condition $m_0 = 0$, we can unroll this recursion to express $m_t$ as a weighted sum of all past gradients:\n$$m_t = (1-\\beta)g_t + \\beta m_{t-1} = (1-\\beta)g_t + \\beta((1-\\beta)g_{t-1} + \\beta m_{t-2})$$\n$$m_t = (1-\\beta)g_t + (1-\\beta)\\beta g_{t-1} + (1-\\beta)\\beta^2 g_{t-2} + \\dots$$\nIn the steady-state limit (or for a process that has run for many steps, $t \\to \\infty$), the current estimate $m_t$ is a superposition of all past gradients, extending to $k \\to \\infty$:\n$$m_t = \\sum_{k=0}^{\\infty} (1-\\beta)\\beta^k g_{t-k}$$\nThe impulse response weights, $w_k$, which quantify the contribution of the gradient from $k$ steps in the past ($g_{t-k}$), are thus given by:\n$$w_k = (1-\\beta)\\beta^k, \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}$$\nThese weights form a valid probability mass function over the non-negative integers. First, since $\\beta \\in [0, 1)$, both $(1-\\beta)$ and $\\beta^k$ are non-negative, so $w_k \\ge 0$ for all $k$. Second, the sum of the weights is a geometric series:\n$$\\sum_{k=0}^{\\infty} w_k = \\sum_{k=0}^{\\infty} (1-\\beta)\\beta^k = (1-\\beta) \\sum_{k=0}^{\\infty} \\beta^k = (1-\\beta) \\left( \\frac{1}{1-\\beta} \\right) = 1$$\nThis sequence $w_k$ represents a geometric distribution over the \"age\" of the gradients, measured in time steps.\n\nThe problem requires deriving a baseline window length from the first moment of this distribution. The first moment, or mean $\\mu$, of this probability distribution is the expected age of the information:\n$$\\mu = \\mathbb{E}[k] = \\sum_{k=0}^{\\infty} k \\, w_k = \\sum_{k=0}^{\\infty} k (1-\\beta) \\beta^k = (1-\\beta) \\sum_{k=0}^{\\infty} k \\beta^k$$\nThe sum can be evaluated using the derivative of the geometric series formula: $\\sum_{k=0}^{\\infty} k x^k = x/(1-x)^2$.\n$$\\mu = (1-\\beta) \\frac{\\beta}{(1-\\beta)^2} = \\frac{\\beta}{1-\\beta}$$\nThis mean age $\\mu$ can be used to define a characteristic window length. Simply using $\\mu$ is problematic, as for $\\beta=0$, $\\mu=0$, which is not a meaningful length and would lead to division by zero in the required ratio calculations. A more robust definition, which also aligns with common heuristics, is to define the baseline length as $L_{\\text{baseline}} = \\mu + 1$. This represents the number of samples from index $0$ to the mean index $\\mu$.\n$$L_{\\text{baseline}} = \\mu + 1 = \\frac{\\beta}{1-\\beta} + 1 = \\frac{\\beta + 1 - \\beta}{1-\\beta} = \\frac{1}{1-\\beta}$$\nThis definition yields a length of $1$ for $\\beta=0$ and grows to infinity as $\\beta \\to 1^-$, which is a sensible behavior for a window length.\n\nNext, we formalize the two specified metrics for effective window length.\n\n1.  The cumulative-mass window length, $L_p$, is the smallest integer $L$ such that the sum of the first $L$ weights reaches a threshold $p$. We are given $p=0.95$.\n    $$\\sum_{k=0}^{L-1} w_k = \\sum_{k=0}^{L-1} (1-\\beta)\\beta^k = (1-\\beta) \\frac{1-\\beta^L}{1-\\beta} = 1-\\beta^L$$\n    We seek the smallest integer $L$ satisfying $1-\\beta^L \\ge p$.\n    $$1 - p \\ge \\beta^L$$\n    For $\\beta \\in (0,1)$, we take the logarithm, which reverses the inequality since $\\log(\\beta)$ is negative:\n    $$\\log(1-p) \\le L \\log(\\beta) \\implies L \\ge \\frac{\\log(1-p)}{\\log(\\beta)}$$\n    Since $L$ must be an integer, we take the ceiling of this expression. For the special case $\\beta=0$, $w_0=1$ and all other weights are $0$, so the sum is $1$ for any $L \\ge 1$; thus, the smallest $L$ is $1$.\n    $$L_p = \\begin{cases} 1 & \\text{if } \\beta=0 \\\\ \\left\\lceil \\frac{\\log(1-p)}{\\log(\\beta)} \\right\\rceil & \\text{if } \\beta \\in (0,1) \\end{cases}$$\n    With $p=0.95$, this becomes $L_{0.95} = \\lceil \\log(0.05)/\\log(\\beta) \\rceil$ for $\\beta > 0$.\n\n2.  The effective sample size, $N_{\\text{eff}}$, is defined as $N_{\\text{eff}} = 1/\\sum_{k=0}^{\\infty} w_k^2$. This metric measures the equivalent number of independent samples that would give the same variance reduction.\n    $$\\sum_{k=0}^{\\infty} w_k^2 = \\sum_{k=0}^{\\infty} ((1-\\beta)\\beta^k)^2 = (1-\\beta)^2 \\sum_{k=0}^{\\infty} (\\beta^2)^k$$\n    This is another geometric series with ratio $\\beta^2$, which converges to $1/(1-\\beta^2)$.\n    $$\\sum_{k=0}^{\\infty} w_k^2 = (1-\\beta)^2 \\frac{1}{1-\\beta^2} = (1-\\beta)^2 \\frac{1}{(1-\\beta)(1+\\beta)} = \\frac{1-\\beta}{1+\\beta}$$\n    Therefore, the effective sample size is:\n    $$N_{\\text{eff}} = \\frac{1}{(1-\\beta)/(1+\\beta)} = \\frac{1+\\beta}{1-\\beta}$$\n    This formula is valid for all $\\beta \\in [0,1)$.\n\nFinally, we compute the required ratios for the computational task.\nThe first ratio is $r_L = L_{0.95} / L_{\\text{baseline}}$:\n$$r_L = \\frac{L_{0.95}}{L_{\\text{baseline}}} = (1-\\beta) L_{0.95} = (1-\\beta) \\left\\lceil \\frac{\\log(0.05)}{\\log(\\beta)} \\right\\rceil \\quad (\\text{for } \\beta > 0)$$\nIn the limit as $\\beta \\to 1^-$, let $\\beta = 1-\\epsilon$ with $\\epsilon \\to 0^+$. Then $\\log(\\beta) \\approx -\\epsilon$. The term inside the ceiling becomes $\\log(0.05)/(-\\epsilon) \\approx 2.9957/\\epsilon$. The ratio approaches $r_L \\approx \\epsilon (\\lceil 2.9957/\\epsilon \\rceil) \\to 2.9957 = \\log(20)$.\n\nThe second ratio is $r_N = N_{\\text{eff}} / L_{\\text{baseline}}$:\n$$r_N = \\frac{N_{\\text{eff}}}{L_{\\text{baseline}}} = \\frac{(1+\\beta)/(1-\\beta)}{1/(1-\\beta)} = 1+\\beta$$\nThis remarkably simple result is valid for all $\\beta \\in [0,1)$. As $\\beta \\to 1^-$, this ratio approaches $2$.\n\nThe Python code will implement these derived formulas to compute the ratios for the specified test suite of $\\beta$ values. For $\\beta=0$, special handling is used as derived: $L_{0.95}(0)=1$, $N_{\\text{eff}}(0)=1$, and $L_{\\text{baseline}}(0)=1$, yielding $r_L=1$ and $r_N=1$.", "answer": "```python\nimport numpy as np\nimport math\n\n# The problem is valid. Proceeding with the solution.\n\ndef solve():\n    \"\"\"\n    Computes effective window length ratios for an Exponential Moving Average (EMA).\n    \n    This function calculates two ratios, r_L and r_N, for a given set of EMA\n    decay parameters (beta).\n    \n    r_L = L_0.95 / L_baseline\n    r_N = N_eff / L_baseline\n    \n    where:\n    - L_0.95 is the cumulative-mass window length for p=0.95.\n    - N_eff is the effective sample size.\n    - L_baseline is a baseline window length derived from the first moment.\n    \"\"\"\n    \n    # Test suite of decay parameters\n    betas = [0.0, 0.5, 0.9, 0.99, 0.999]\n    \n    # Threshold for cumulative-mass window length\n    p = 0.95\n    log_1_minus_p = np.log(1.0 - p)\n\n    results = []\n    for beta in betas:\n        # Handle the edge case beta = 0 separately to avoid log(0)\n        if beta == 0.0:\n            # For beta=0, EMA is just the current gradient. All window lengths are 1.\n            # L_baseline = 1/(1-0) = 1\n            # L_0.95 = 1 (since w_0 = 1 >= 0.95)\n            # N_eff = (1+0)/(1-0) = 1\n            r_L = 1.0\n            r_N = 1.0\n            results.append([round(r_L, 3), round(r_N, 3)])\n            continue\n\n        # Derived formulas from the solution section\n        \n        # 1. Baseline window length: L_baseline = 1 / (1 - beta)\n        L_baseline = 1.0 / (1.0 - beta)\n        \n        # 2. Cumulative-mass window length: L_0.95 = ceil(log(1-p) / log(beta))\n        L_095 = np.ceil(log_1_minus_p / np.log(beta))\n        \n        # 3. Effective sample size: N_eff = (1 + beta) / (1 - beta)\n        N_eff = (1.0 + beta) / (1.0 - beta)\n        \n        # Compute the ratios\n        r_L = L_095 / L_baseline\n        # r_N can be computed directly as 1 + beta, but we compute it from\n        # the definitions as a cross-check, which is more robust.\n        r_N = N_eff / L_baseline\n\n        # Round results to three decimal places before storing\n        results.append([round(r_L, 3), round(r_N, 3)])\n        \n    # Format the output into the required single-line string representation\n    # of a list of lists, with no spaces inside the inner lists.\n    # e.g., [[1.0,1.0],[2.5,1.5],...]\n    inner_strings = [str(item).replace(' ', '') for item in results]\n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "3154008"}, {"introduction": "Having quantified momentum's memory, we now turn to its behavior over time. The momentum update rule forms a second-order dynamical system, and this exercise [@problem_id:3154030] reveals its deep connection to the physics of a damped harmonic oscillator. By analyzing the conditions for stable descent versus oscillation on a simple but illustrative energy landscape, you will gain a powerful mental model for why momentum can both accelerate convergence and introduce unwanted overshooting.", "problem": "Consider the one-dimensional quartic objective $f(\\theta)=\\frac{1}{4}\\theta^4$ and the classical momentum update defined by a learning rate $\\eta>0$ and a momentum coefficient $\\beta\\ge 0$. The classical momentum iteration is given by two coupled equations: a velocity update and a parameter update. In symbols, for $t\\in\\{0,1,2,\\dots\\}$,\n$$\nv_{t+1}=\\beta\\,v_t+\\nabla f(\\theta_t)\\quad\\text{and}\\quad \\theta_{t+1}=\\theta_t-\\eta\\,v_{t+1}.\n$$\nStart from these core definitions and, with no additional assumptions, derive the equivalent second-order difference equation in $\\theta_t$ only. Then, connect this discrete-time dynamic to the behavior of a damped oscillator by performing a local linearization of the nonlinearity around a reference amplitude $a>0$. Specifically, use the approximation $\\theta_t^3\\approx a^2\\,\\theta_t$ in the derived second-order difference equation to obtain a linear, constant-coefficient recursion, and analyze its characteristic equation. From first principles, characterize how the pair $(\\eta,\\beta)$ controls the qualitative behavior near the reference amplitude $a$: monotone descent without overshoot, overshoot with damped oscillations, sustained oscillations on the stability boundary, and divergence. Your characterization must be expressed using inequalities on $(\\eta,\\beta)$ and the reference amplitude $a$, and must be linked to the root structure of the characteristic polynomial and the discrete-time analog of the damping regimes (overdamped, critically damped, underdamped).\n\nYour program must implement this characterization rule to classify several test cases. For each test case, treat the reference amplitude $a$ as the linearization point and compute the classification by analyzing the characteristic polynomial of the linearized second-order recursion. The classification integers must follow this encoding:\n- $0$: monotone descent (no overshoot), which corresponds to non-oscillatory decay with real, nonnegative roots strictly inside the unit disk.\n- $1$: damped oscillations (overshoot), which corresponds to oscillatory decay; this occurs when the spectral radius is strictly less than one and the root structure yields sign alternation or complex conjugate roots.\n- $2$: sustained oscillations (stability boundary), which corresponds to unit-modulus dominant root(s), i.e., spectral radius equal to one.\n- $3$: divergence, which corresponds to spectral radius strictly greater than one.\n\nYour program should evaluate the following test suite, where each test case is a triplet $(\\eta,\\beta,a)$ with $a>0$:\n- $(\\eta,\\beta,a)=\\left(0.05,\\,0.5,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(0.3,\\,0.9,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(3.0,\\,0.5,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(4.0,\\,0.2,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(0.5,\\,0.0,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(1.2,\\,0.0,\\,1.0\\right)$\n\nNo physical units are involved. Angles are not used. Results must be purely numerical. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite. For example, if your classifications are $r_1,\\dots,r_6$ in order, print the line $\\left[\\text{r}_1,\\text{r}_2,\\text{r}_3,\\text{r}_4,\\text{r}_5,\\text{r}_6\\right]$ with no additional text.", "solution": "The problem requires a characterization of the dynamics of the classical momentum optimization algorithm for a one-dimensional quartic objective function, $f(\\theta) = \\frac{1}{4}\\theta^4$. This involves deriving a second-order difference equation, linearizing it, and analyzing its stability and behavior based on its parameters.\n\n### Step 1: Derivation of the Second-Order Difference Equation\n\nWe start with the given coupled equations for classical momentum:\n$$v_{t+1} = \\beta v_t + \\nabla f(\\theta_t)$$\n$$\\theta_{t+1} = \\theta_t - \\eta v_{t+1}$$\nThe objective function is $f(\\theta) = \\frac{1}{4}\\theta^4$, so its gradient is $\\nabla f(\\theta) = \\frac{d}{d\\theta}\\left(\\frac{1}{4}\\theta^4\\right) = \\theta^3$. The update equations become:\n$$v_{t+1} = \\beta v_t + \\theta_t^3 \\quad (1)$$\n$$\\theta_{t+1} = \\theta_t - \\eta v_{t+1} \\quad (2)$$\n\nTo obtain a single difference equation for $\\theta_t$, we must eliminate the velocity terms $v_t$ and $v_{t+1}$. From equation (2), we can express $v_{t+1}$ in terms of the parameters $\\theta_t$ and $\\theta_{t+1}$:\n$$v_{t+1} = \\frac{1}{\\eta}(\\theta_t - \\theta_{t+1})$$\nBy shifting the time index from $t+1$ to $t$, we can also express $v_t$:\n$$v_t = \\frac{1}{\\eta}(\\theta_{t-1} - \\theta_t)$$\nSubstituting these expressions for $v_t$ and $v_{t+1}$ into the velocity update equation (1):\n$$\\frac{1}{\\eta}(\\theta_t - \\theta_{t+1}) = \\beta \\left( \\frac{1}{\\eta}(\\theta_{t-1} - \\theta_t) \\right) + \\theta_t^3$$\nMultiplying the entire equation by $\\eta$ to clear the denominator:\n$$\\theta_t - \\theta_{t+1} = \\beta(\\theta_{t-1} - \\theta_t) + \\eta \\theta_t^3$$\nFinally, we rearrange the equation to isolate $\\theta_{t+1}$, which represents the next state as a function of previous states:\n$$\\theta_{t+1} = \\theta_t - \\beta(\\theta_{t-1} - \\theta_t) - \\eta \\theta_t^3$$\n$$\\theta_{t+1} = (1+\\beta)\\theta_t - \\beta\\theta_{t-1} - \\eta \\theta_t^3$$\nThis is the desired second-order nonlinear difference equation governing the sequence $\\{\\theta_t\\}$.\n\n### Step 2: Linearization and the Characteristic Equation\n\nThe behavior of the nonlinear system can be locally approximated by linearizing it around a reference point. The problem specifies a local linearization of the nonlinearity $\\theta_t^3$ around a reference amplitude $a > 0$ using the approximation $\\theta_t^3 \\approx a^2 \\theta_t$. Substituting this into the difference equation:\n$$\\theta_{t+1} \\approx (1+\\beta)\\theta_t - \\beta\\theta_{t-1} - \\eta (a^2 \\theta_t)$$\nGrouping terms involving $\\theta_t$:\n$$\\theta_{t+1} = (1+\\beta - \\eta a^2)\\theta_t - \\beta\\theta_{t-1}$$\nTo analyze this linear, homogeneous, constant-coefficient difference equation, we seek solutions of the form $\\theta_t = \\lambda^t$. Substituting this ansatz yields the characteristic equation:\n$$\\lambda^{t+1} = (1+\\beta - \\eta a^2)\\lambda^t - \\beta\\lambda^{t-1}$$\nDividing by $\\lambda^{t-1}$ (for $\\lambda \\neq 0$):\n$$\\lambda^2 - (1+\\beta - \\eta a^2)\\lambda + \\beta = 0$$\nThe roots $\\lambda_{1,2}$ of this quadratic characteristic polynomial determine the qualitative behavior of the linearized system.\n\n### Step 3: Stability Analysis and Classification\n\nThe stability of a discrete-time system described by the characteristic polynomial $P(\\lambda) = \\lambda^2 + A\\lambda + B = 0$ is determined by the Jury stability criteria, which require all roots to have a magnitude less than $1$. For a second-order system, these criteria are:\n1. $P(1) > 0$\n2. $P(-1) > 0$\n3. $|B| < 1$\n\nIn our case, the polynomial is $P(\\lambda) = \\lambda^2 - (1+\\beta - \\eta a^2)\\lambda + \\beta$, so $A = -(1+\\beta - \\eta a^2)$ and $B = \\beta$.\n1. $P(1) = 1 - (1+\\beta - \\eta a^2) + \\beta = \\eta a^2$. Since $\\eta > 0$ and $a > 0$, $P(1) > 0$ is always satisfied. This means a root at $\\lambda=1$ is not possible, so the system will not converge to a non-zero constant.\n2. $P(-1) = 1 + (1+\\beta - \\eta a^2) + \\beta = 2(1+\\beta) - \\eta a^2$. The stability condition is $2(1+\\beta) - \\eta a^2 > 0$, or $\\eta a^2 < 2(1+\\beta)$.\n3. $|B| = |\\beta| < 1$. Since the problem states $\\beta \\ge 0$, this simplifies to $0 \\le \\beta < 1$.\n\nStable decay to the origin ($\\rho = \\max(|\\lambda_1|, |\\lambda_2|) < 1$) requires $0 \\le \\beta < 1$ and $0 < \\eta a^2 < 2(1+\\beta)$.\n\nWe can now define the categories of behavior:\n\n- **Category 3 (Divergence):** The spectral radius $\\rho > 1$. This occurs if any stability condition is violated.\n    - If $\\beta > 1$, the product of roots $\\lambda_1\\lambda_2 = \\beta > 1$, so at least one root must lie outside the unit circle.\n    - If $\\eta a^2 > 2(1+\\beta)$, then $P(-1) < 0$. Since $P(1) > 0$, there must be a real root $\\lambda < -1$, causing divergence.\n    - A special case is $\\beta=1$ and $\\eta a^2 \\ge 4$, which leads to real roots with at least one root satisfying $|\\lambda| \\ge 1$ (with a double root at $\\lambda=-1$ for $\\eta a^2=4$, leading to linear growth in amplitude).\n\n- **Category 2 (Sustained Oscillations):** The spectral radius $\\rho = 1$, with no roots outside the unit circle. This occurs on the stability boundary.\n    - If $\\beta = 1$ and $0 < \\eta a^2 < 4$. The roots are complex conjugates with modulus $\\sqrt{\\beta}=1$, lying on the unit circle.\n    - If $\\eta a^2 = 2(1+\\beta)$ and $0 \\le \\beta < 1$. This implies $P(-1)=0$, so one root is $\\lambda_1 = -1$. The other root is $\\lambda_2 = \\beta/\\lambda_1 = -\\beta$. Since $|\\beta|<1$, the dominant root has modulus $1$.\n \n- **Categories 0 & 1 (Stable Decay):** The system is stable ($0 \\le \\beta < 1$ and $0 < \\eta a^2 < 2(1+\\beta)$). The behavior is distinguished by the nature of the roots, which depends on the discriminant of the characteristic equation: $\\Delta = (1+\\beta - \\eta a^2)^2 - 4\\beta$.\n    - If $\\Delta < 0$, the roots are complex conjugates. The solution is a decaying sinusoid, which is an oscillation. This falls into **Category 1 (Damped Oscillations)**.\n    - If $\\Delta \\ge 0$, the roots are real. The behavior depends on their signs. The product of roots is $\\beta \\ge 0$, so they share the same sign. Their sign is determined by their sum, $C_1 = 1+\\beta - \\eta a^2$.\n        - If $C_1 = 1+\\beta - \\eta a^2 < 0$, both roots are negative. The solution involves terms like $(-\\lambda)^t$, producing sign alternation (overshoot). This is **Category 1 (Damped Oscillations)**.\n        - If $C_1 = 1+\\beta - \\eta a^2 \\ge 0$, both roots are non-negative. The solution is a sum of decaying exponentials with non-negative bases, resulting in **Category 0 (Monotone Descent)**.\n\n### Summary of Classification Logic\n\nFor given $(\\eta, \\beta, a)$:\n1. Define $C_0 = \\eta a^2$.\n2. **Category 3 (Divergence):**\n   If $\\beta > 1$ or $C_0 > 2(1+\\beta)$ or ($\\beta = 1$ and $C_0 \\ge 4$).\n3. **Category 2 (Sustained Oscillations):**\n   If ($\\beta = 1$ and $C_0 < 4$) or ($C_0 = 2(1+\\beta)$ and $\\beta < 1$).\n4. **Stable Cases:** Otherwise, the system is stable.\n   - Let $C_1 = 1+\\beta-C_0$ and $\\Delta = C_1^2 - 4\\beta$.\n   - **Category 1 (Damped Oscillations):** If $\\Delta < 0$ or $C_1 < 0$.\n   - **Category 0 (Monotone Descent):** If $\\Delta \\ge 0$ and $C_1 \\ge 0$.\nNote: Floating point comparisons for equality should be handled with a small tolerance.", "answer": "```python\nimport numpy as np\n\ndef classify_dynamics(eta, beta, a):\n    \"\"\"\n    Classifies the behavior of the linearized momentum dynamics.\n\n    Args:\n        eta (float): Learning rate.\n        beta (float): Momentum coefficient.\n        a (float): Reference amplitude for linearization.\n\n    Returns:\n        int: Classification code (0, 1, 2, or 3).\n    \"\"\"\n    if eta <= 0 or a <= 0 or beta < 0:\n        # Invalid parameters based on problem constraints.\n        # This case is not expected for the given test suite.\n        raise ValueError(\"Parameters eta and a must be > 0, and beta must be >= 0.\")\n    \n    c0 = eta * a**2\n    \n    # Use a tolerance for floating-point equality checks.\n    tol = 1e-9\n\n    # Category 3: Divergence (spectral radius > 1)\n    # Corresponds to violation of Jury stability conditions.\n    if beta > 1 + tol:\n        return 3\n    if c0 > 2 * (1 + beta) + tol:\n        return 3\n    if np.isclose(beta, 1.0, atol=tol) and c0 >= 4.0 - tol:\n        return 3\n\n    # Category 2: Sustained Oscillations (spectral radius = 1)\n    # Corresponds to being on the boundary of the stability region.\n    if np.isclose(beta, 1.0, atol=tol) and c0 < 4.0 - tol:\n        # Ensure c0 > 0, which is guaranteed by eta > 0, a > 0\n        return 2\n    if np.isclose(c0, 2 * (1 + beta), atol=tol) and beta < 1.0 - tol:\n        return 2\n\n    # If not divergent or sustained, the system is stable (spectral radius < 1).\n    # We now distinguish between monotone and oscillatory decay.\n    c1 = 1 + beta - c0\n    discriminant = c1**2 - 4 * beta\n\n    # Category 1: Damped Oscillations\n    # Complex roots or real negative roots (causing sign alternation).\n    if discriminant < 0 or c1 < 0:\n        return 1\n    \n    # Category 0: Monotone Descent\n    # Real, non-negative roots.\n    # This is the remaining case: discriminant >= 0 and c1 >= 0.\n    return 0\n\ndef solve():\n    \"\"\"\n    Solves the problem by classifying the dynamics for each test case.\n    \"\"\"\n    test_cases = [\n        (0.05, 0.5, 1.0),\n        (0.3, 0.9, 1.0),\n        (3.0, 0.5, 1.0),\n        (4.0, 0.2, 1.0),\n        (0.5, 0.0, 1.0),\n        (1.2, 0.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        eta, beta, a = case\n        classification = classify_dynamics(eta, beta, a)\n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3154030"}, {"introduction": "Optimization in the wild is rarely as simple as applying a single algorithm. This final practice [@problem_id:3154076] explores the critical, and often problematic, interaction between momentum and gradient clipping. You will diagnose a pathological behavior known as \"momentum wind-up\"—where large, accumulated velocity causes severe oscillations—and then engineer a principled correction, developing the skills needed to build robust training pipelines.", "problem": "Consider a one-dimensional optimization problem with a differentiable objective function defined as a piecewise quadratic: for a positive radius $r$ and positive curvatures $a_{\\mathrm{hi}}$ and $a_{\\mathrm{lo}}$,\n$$\nL(x) = \n\\begin{cases}\n\\dfrac{1}{2} a_{\\mathrm{hi}} x^2 & \\text{if } |x| \\le r, \\\\\n\\dfrac{1}{2} a_{\\mathrm{lo}} x^2 & \\text{if } |x| > r.\n\\end{cases}\n$$\nThe gradient is therefore\n$$\n\\nabla L(x) = \n\\begin{cases}\na_{\\mathrm{hi}} x & \\text{if } |x| \\le r, \\\\\na_{\\mathrm{lo}} x & \\text{if } |x| > r.\n\\end{cases}\n$$\nConsider optimization using Stochastic Gradient Descent (SGD) with classical momentum (also known as the heavy-ball method), subject to norm-based gradient clipping at threshold $c > 0$ in one dimension. The clipping rule is\n$$\ng_t = \\nabla L(x_t), \\quad g_t^{\\mathrm{clip}} = \\operatorname{sign}(g_t) \\cdot \\min\\!\\big(|g_t|, c\\big),\n$$\nand the momentum update is\n$$\nv_t = \\mu v_{t-1} + g_t^{\\mathrm{clip}}, \\quad x_{t+1} = x_t - \\eta v_t,\n$$\nwith step size $\\eta > 0$ and momentum coefficient $\\mu \\in [0,1)$.\n\nTask 1 (derivation): Starting from fundamental discrete-time linear system analysis, treat the unclipped regime $|g_t| \\le c$ as a linear recursion around a fixed curvature $a$ (that is, $g_t = a x_t$ for some fixed $a > 0$). Show that when there is no clipping the recursion can be written as a second-order difference equation in $x_t$ whose characteristic polynomial is\n$$\np(r) = r^2 - \\big(1 + \\mu - \\eta a\\big) r + \\mu,\n$$\nand recall that underdamped behavior corresponds to complex conjugate characteristic roots, which occurs when the discriminant\n$$\n\\Delta = \\big(1 + \\mu - \\eta a\\big)^2 - 4 \\mu\n$$\nis negative. Using this base, reason about how entering and leaving the clipped regime ($|g_t| > c$) systematically alters the effective stiffness and the accumulation of $v_t$ through time, thereby affecting the tendency for oscillation (sign changes of $x_t$).\n\nTask 2 (design and proposal): Design a scenario, using the above $L(x)$, with parameters $a_{\\mathrm{hi}}$, $a_{\\mathrm{lo}}$, $r$, $x_0$, $\\eta$, $\\mu$, and $c$ such that clipping occurs early (for large $|x|$) and momentum accumulates significantly while the restoring force is capped by $c$. Explain why this can produce repeated sign changes of $x_t$ (systematic underdamping in the trajectory sense) once the iterates approach the higher-curvature region near $|x| \\le r$ where clipping ceases. Propose a corrective scaling that modulates the momentum carry-over when clipping is active, using only the instantaneous quantities available at time $t$, without additional information such as second derivatives. Your proposal must be a multiplicative adjustment applied to the momentum coefficient in the update, expressed as a function of the instantaneous clipping factor\n$$\n\\gamma_t = \\frac{|g_t^{\\mathrm{clip}}|}{|g_t| + \\varepsilon},\n$$\nwhere $\\varepsilon$ is a small positive constant used only to avoid division by zero.\n\nTask 3 (implementation and test suite): Implement two optimizers for the one-dimensional $L(x)$:\n- Baseline optimizer: the classical momentum update with gradient clipping as defined above.\n- Corrected optimizer: the same, but replacing $\\mu$ with a time-varying $\\mu_t$ that depends on $\\gamma_t$ according to your proposed corrective scaling.\n\nRun both optimizers for a fixed number of steps $T$ from the same initial $x_0$ and $v_0 = 0$. Quantify underdamping by counting the number of sign changes in the trajectory $x_t$ over the final window of $W$ steps; that is, the number of indices $t \\in \\{T-W, \\dots, T-1\\}$ such that $x_t x_{t+1} < 0$. For each test case, return a boolean indicating whether the corrected optimizer reduces the underdamping measure compared to the baseline optimizer, that is, whether the corrected count is strictly less than the baseline count.\n\nUse the following test suite of parameter sets, which is designed to cover a standard case, boundary conditions, and edge cases:\n1. $a_{\\mathrm{hi}} = 20.0$, $a_{\\mathrm{lo}} = 5.0$, $r = 0.5$, $x_0 = 12.0$, $\\eta = 0.08$, $\\mu = 0.95$, $c = 1.0$, $T = 600$, $W = 200$.\n2. $a_{\\mathrm{hi}} = 20.0$, $a_{\\mathrm{lo}} = 5.0$, $r = 0.5$, $x_0 = 12.0$, $\\eta = 0.08$, $\\mu = 0.95$, $c = 10^9$, $T = 600$, $W = 200$.\n3. $a_{\\mathrm{hi}} = 20.0$, $a_{\\mathrm{lo}} = 5.0$, $r = 0.5$, $x_0 = 12.0$, $\\eta = 0.08$, $\\mu = 0.0$, $c = 1.0$, $T = 600$, $W = 200$.\n4. $a_{\\mathrm{hi}} = 30.0$, $a_{\\mathrm{lo}} = 6.0$, $r = 0.4$, $x_0 = 20.0$, $\\eta = 0.05$, $\\mu = 0.98$, $c = 0.1$, $T = 800$, $W = 300$.\n5. $a_{\\mathrm{hi}} = 25.0$, $a_{\\mathrm{lo}} = 4.0$, $r = 0.6$, $x_0 = 10.0$, $\\eta = 0.01$, $\\mu = 0.9$, $c = 0.5$, $T = 800$, $W = 400$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets (for example, \"[True,False,True,False,True]\"). No additional text should be printed.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of numerical optimization, is well-posed, objective, and contains all necessary information for a solution.\n\n### Task 1: Derivation and Analysis\n\nThe optimization process is described by a system of two coupled first-order difference equations for the position $x_t$ and the velocity $v_t$. The update rules are given as:\n$$\nv_t = \\mu v_{t-1} + g_t^{\\mathrm{clip}} \\quad (1)\n$$\n$$\nx_{t+1} = x_t - \\eta v_t \\quad (2)\n$$\nwhere $g_t^{\\mathrm{clip}}$ is the clipped gradient of the loss function $L(x)$ at $x_t$, $\\eta > 0$ is the step size, and $\\mu \\in [0,1)$ is the momentum coefficient.\n\nIn the unclipped regime, $|g_t| \\le c$, we have $g_t^{\\mathrm{clip}} = g_t$. The problem asks to analyze this regime as a linear recursion by setting $g_t = a x_t$ for some fixed positive curvature $a$. Substituting this into equation $(1)$ yields:\n$$\nv_t = \\mu v_{t-1} + a x_t\n$$\nTo derive a single second-order difference equation for $x_t$, we express $v_t$ and $v_{t-1}$ in terms of the sequence $x_t$. From equation $(2)$, we have:\n$$\nv_t = \\frac{1}{\\eta} (x_t - x_{t+1})\n$$\nand by shifting the index back by one:\n$$\nv_{t-1} = \\frac{1}{\\eta} (x_{t-1} - x_t)\n$$\nSubstituting these expressions for $v_t$ and $v_{t-1}$ into the velocity update equation gives:\n$$\n\\frac{1}{\\eta} (x_t - x_{t+1}) = \\mu \\left( \\frac{1}{\\eta} (x_{t-1} - x_t) \\right) + a x_t\n$$\nMultiplying by $\\eta$ and rearranging the terms to solve for $x_{t+1}$:\n$$\nx_t - x_{t+1} = \\mu (x_{t-1} - x_t) + \\eta a x_t\n$$\n$$\nx_{t+1} = x_t - \\mu x_{t-1} + \\mu x_t - \\eta a x_t\n$$\n$$\nx_{t+1} = (1 + \\mu - \\eta a) x_t - \\mu x_{t-1}\n$$\nThis can be written in the standard form for a homogeneous linear difference equation:\n$$\nx_{t+1} - (1 + \\mu - \\eta a) x_t + \\mu x_{t-1} = 0\n$$\nThe behavior of the solutions $x_t$ is determined by the roots of the characteristic polynomial. Assuming a solution of the form $x_t \\propto R^t$, we substitute this into the equation, which yields the characteristic polynomial in $R$ (denoted by $r$ in the problem statement, but we use $R$ to avoid confusion with the radius parameter):\n$$\np(R) = R^2 - (1 + \\mu - \\eta a) R + \\mu = 0\n$$\nThis matches the polynomial specified in the problem. The roots of this quadratic equation dictate the system's dynamics. The system exhibits underdamped behavior, characterized by oscillations, when the roots are a pair of complex conjugates. This occurs if and only if the discriminant $\\Delta$ is negative:\n$$\n\\Delta = (1 + \\mu - \\eta a)^2 - 4\\mu < 0\n$$\nThis is the condition for oscillations in the linear, unclipped regime.\n\nNow, we analyze the effect of gradient clipping. When the magnitude of the gradient, $|g_t| = |a x_t|$, exceeds the clipping threshold $c$, the update becomes nonlinear. The clipped gradient is $g_t^{\\mathrm{clip}} = c \\cdot \\operatorname{sign}(g_t)$. The velocity update is:\n$$\nv_t = \\mu v_{t-1} + c \\cdot \\operatorname{sign}(g_t)\n$$\nIn this regime, the corrective term added to the velocity is constant in magnitude, regardless of how large $|x_t|$ (and thus $|g_t|$) becomes. The feedback from the state $x_t$ to the change in velocity is effectively saturated. The system acts like an integrator, accumulating velocity from a constant-magnitude force. If the optimizer remains in this clipped regime for multiple steps, $v_t$ can grow to a large magnitude.\n\nWhen the trajectory eventually enters the high-curvature region near the origin ($|x| \\le r$), the curvature jumps from $a_{\\mathrm{lo}}$ to $a_{\\mathrm{hi}}$. Concurrently, as $|x_t|$ becomes small, the unclipped gradient $|g_t| = |a_{\\mathrm{hi}} x_t|$ may fall below the threshold $c$, causing clipping to cease. At this point, two phenomena combine to cause problems:\n$1$. The large velocity $v_t$ accumulated during the clipped phase is \"injected\" into the central region.\n$2$. The central region itself is often configured to be underdamped (i.e., $\\Delta < 0$ for $a=a_{\\mathrm{hi}}$), meaning it is inherently oscillatory.\n\nThe combination of a large initial velocity and an underdamped system leads to a significant overshoot of the minimum at $x=0$. This large overshoot propels $x_t$ far into the opposite side of the loss landscape, likely re-entering the clipped regime. This cycle can repeat, causing large, sustained oscillations, which is a form of systematic underdamping in the trajectory.\n\n### Task 2: Scenario Design and Corrective Proposal\n\nA scenario prone to this pathological behavior is one where:\n- The initial position $x_0$ is large, ensuring the dynamics start in the low-curvature region ($|x_0| > r$) where $|g_0| = |a_{\\mathrm{lo}}x_0|$ is large.\n- The clipping threshold $c$ is small compared to the initial gradient $|g_0|$, ensuring clipping is active from the start.\n- The momentum coefficient $\\mu$ is high (close to $1$), promoting significant accumulation of velocity over time.\n- The high-curvature region ($|x| \\le r$) is strongly underdamped, i.e., $(1+\\mu-\\eta a_{\\mathrm{hi}})^2 - 4\\mu < 0$. This amplifies the effect of any large velocity entering this region.\n\nThe core issue is the mismatch between the full gradient magnitude, which informs our intuition about momentum, and the clipped gradient magnitude used in the update. During clipping, momentum accumulates as if responding to a strong, consistent force, while the position update is based on a smaller, capped force. This leads to the \"wind-up\" effect described above.\n\nTo counteract this, we propose a corrective scaling that modulates the momentum coefficient $\\mu$ based on the severity of clipping. The proposed adjustment makes the momentum carry-over, $\\mu_t$, dependent on the instantaneous clipping factor $\\gamma_t$:\n$$\n\\gamma_t = \\frac{|g_t^{\\mathrm{clip}}|}{|g_t| + \\varepsilon}\n$$\nwhere $\\varepsilon > 0$ is a small constant for numerical stability. The factor $\\gamma_t$ is close to $1$ when there is no clipping ($|g_t| \\approx |g_t^{\\mathrm{clip}}|$), and it approaches $0$ when the gradient is heavily clipped ($|g_t| \\gg |g_t^{\\mathrm{clip}}| = c$).\n\nOur proposal is to define a time-varying momentum coefficient $\\mu_t$ as:\n$$\n\\mu_t = \\mu \\cdot \\gamma_t\n$$\nThe corrected velocity update becomes:\n$$\nv_t = (\\mu \\cdot \\gamma_t) v_{t-1} + g_t^{\\mathrm{clip}}\n$$\nThis correction has the desired properties:\n- **No clipping ($\\gamma_t \\approx 1$):** The update becomes $v_t \\approx \\mu v_{t-1} + g_t$, recovering the standard momentum dynamics.\n- **Heavy clipping ($\\gamma_t \\ll 1$):** The momentum carry-over term $(\\mu \\cdot \\gamma_t) v_{t-1}$ is strongly attenuated. The update approximates $v_t \\approx g_t^{\\mathrm{clip}}$, which resembles simple (clipped) gradient descent. This prevents the excessive accumulation of velocity (integrator wind-up) precisely when the feedback from $x_t$ is lost due to clipping.\n\nThis dynamic adjustment of the momentum term based on locally available information should mitigate the overshoot and reduce the resulting oscillations as the optimizer transitions between clipped and unclipped regimes.\n\n### Task 3: Implementation\n\nThe implementation and test involve two optimizers. The baseline optimizer uses the standard clipped momentum update with a fixed $\\mu$. The corrected optimizer replaces $\\mu$ with the proposed time-varying $\\mu_t = \\mu \\gamma_t$. Both are run on a suite of test cases, and their performance is measured by counting the number of sign changes in the trajectory $x_t$ over the final $W$ steps. A boolean result indicates if the corrected optimizer strictly reduces this count. The Python code in the final answer implements this procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef grad_L(x: float, a_hi: float, a_lo: float, r: float) -> float:\n    \"\"\"Computes the gradient of the piecewise quadratic function L(x).\"\"\"\n    if np.abs(x) <= r:\n        return a_hi * x\n    else:\n        return a_lo * x\n\ndef run_optimizer(params: tuple, corrected: bool) -> int:\n    \"\"\"\n    Runs an optimizer on the given problem and returns the number of sign changes.\n\n    Args:\n        params: A tuple containing (a_hi, a_lo, r, x0, eta, mu, c, T, W).\n        corrected: A boolean indicating whether to use the corrected momentum update.\n\n    Returns:\n        The number of sign changes in the trajectory's final W steps.\n    \"\"\"\n    a_hi, a_lo, r, x0, eta, mu, c, T, W = params\n    \n    x_traj = np.zeros(T + 1)\n    x_traj[0] = x0\n    \n    # v_prev corresponds to v_{t-1} in the update rule for v_t.\n    # Initialize v_{-1} = 0, consistent with v_0 = g_0^clip.\n    v_prev = 0.0\n    x_curr = x0\n    \n    # Epsilon for numerical stability in gamma_t calculation.\n    epsilon = 1e-8\n\n    # Main optimization loop\n    for t in range(T):\n        # Calculate gradient and clip it\n        g_t = grad_L(x_curr, a_hi, a_lo, r)\n        g_t_abs = np.abs(g_t)\n        g_t_clip = np.sign(g_t) * np.minimum(g_t_abs, c)\n        \n        current_mu = mu\n        if corrected:\n            gamma_t = np.abs(g_t_clip) / (g_t_abs + epsilon)\n            current_mu = mu * gamma_t\n            \n        # Update velocity and position based on the problem statement's equations:\n        # v_t = mu * v_{t-1} + g_t^clip\n        # x_{t+1} = x_t - eta * v_t\n        v_curr = current_mu * v_prev + g_t_clip\n        x_next = x_curr - eta * v_curr\n        \n        # Store trajectory and update states for the next iteration\n        x_traj[t + 1] = x_next\n        x_curr = x_next\n        v_prev = v_curr\n        \n    # Count sign changes in the final window of W steps.\n    # The window covers pairs (x_t, x_{t+1}) for t in [T-W, T-1].\n    sign_changes = 0\n    for i in range(T - W, T):\n        if x_traj[i] * x_traj[i + 1] < 0:\n            sign_changes += 1\n            \n    return sign_changes\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        # a_hi, a_lo, r, x0, eta, mu, c, T, W\n        (20.0, 5.0, 0.5, 12.0, 0.08, 0.95, 1.0, 600, 200),\n        (20.0, 5.0, 0.5, 12.0, 0.08, 0.95, 1e9, 600, 200),\n        (20.0, 5.0, 0.5, 12.0, 0.08, 0.0, 1.0, 600, 200),\n        (30.0, 6.0, 0.4, 20.0, 0.05, 0.98, 0.1, 800, 300),\n        (25.0, 4.0, 0.6, 10.0, 0.01, 0.9, 0.5, 800, 400),\n    ]\n\n    results = []\n    for case in test_cases:\n        baseline_count = run_optimizer(case, corrected=False)\n        corrected_count = run_optimizer(case, corrected=True)\n        # Append True if the corrected optimizer strictly reduces underdamping\n        results.append(corrected_count < baseline_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3154076"}]}