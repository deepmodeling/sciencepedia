{"hands_on_practices": [{"introduction": "Adaptive optimizers like Adam adjust the learning rate based on an estimate of the gradient's second moment. This exercise provides a hands-on way to understand how the hyperparameter $\\beta_2$ controls the \"memory\" of this estimate, influencing how quickly the optimizer adapts to sudden changes in the loss landscape's curvature. By simulating this dynamic in a controlled setting, you will gain a deeper intuition for tuning this critical parameter.", "problem": "You are asked to design and analyze a simplified training scenario to study how the second raw moment estimator in Adaptive Moment Estimation (Adam) reacts to abrupt changes in loss curvature. Consider a one-dimensional parameter with a loss locally approximated by a quadratic function whose curvature changes abruptly at specified iteration indices. In this setting, the squared gradientâ€™s true second moment is modeled as a piecewise-constant sequence that undergoes sudden regime shifts (phase transitions).\n\nFundamental base and setup:\n- Let the per-iteration stochastic gradient be denoted by $g_t$. The second raw moment estimator used by Adaptive Moment Estimation (Adam) is an Exponential Moving Average (EMA) defined by\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, g_t^2,\n$$\nwhere $v_t$ is the estimator at iteration $t$ and $\\beta_2 \\in [0,1)$ is the decay parameter for the second moment.\n- To isolate responsiveness to curvature changes without confounding optimization dynamics, replace the random input $g_t^2$ by its true regime-dependent second moment $m_t = \\mathbb{E}[g_t^2]$, which is assumed piecewise constant over time and changes abruptly at specified iteration indices. Under this substitution, the estimator evolves deterministically via\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t,\n$$\nwith $v_0 = 0$.\n- After each abrupt change in $m_t$, define the responsiveness metric as the smallest nonnegative integer lag $\\ell$ (measured in iterations) such that $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta \\, m_{\\text{new}}$, where $t_c$ is the iteration index of the change, $m_{\\text{new}}$ is the new constant value of $m_t$ after the change, and $\\delta \\in (0,1)$ is a specified tolerance. If the tolerance is not met within the provided horizon $T$, record the lag as $T - t_c$. The final score for a test case is the average of these lags over all changes in that case, rounded to the nearest integer.\n\nYour task:\n- Implement the deterministic update $v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t$ over a fixed horizon $T$ for several test scenarios, each defined by $(\\beta_2, T, \\{t_c\\}, \\{m\\}, \\delta)$, where $\\{t_c\\}$ is an increasing list of change indices and $\\{m\\}$ is a list of regime values of $m_t$ whose length is $|\\{t_c\\}| + 1$.\n- For each scenario, compute the mean settling time across all changes and round to the nearest integer.\n\nTest suite:\nUse the following six scenarios that explore a range of behaviors including a general case, large upward and downward phase transitions, multiple transitions, and extreme slow adaptation. In each item below, all mathematical entities are given and must be used exactly.\n\n1. Scenario $1$: $\\beta_2 = 0.0$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n2. Scenario $2$: $\\beta_2 = 0.9$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n3. Scenario $3$: $\\beta_2 = 0.99$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n4. Scenario $4$: $\\beta_2 = 0.999$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n5. Scenario $5$: $\\beta_2 = 0.99$, $T = 200$, changes at $\\{20, 40, 80, 120\\}$, regimes $\\{1.0, 4.0, 0.5, 2.0, 1.5\\}$, $\\delta = 0.05$.\n6. Scenario $6$: $\\beta_2 = 0.9999$, $T = 1000$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n\nFinal output format:\nYour program should produce a single line containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to the rounded mean settling time for one scenario, in the order $1$ through $6$ (e.g., $[r_1,r_2,r_3,r_4,r_5,r_6]$). All entries must be integers. No physical units or angles are involved in this problem; all quantities are dimensionless.", "solution": "The user has provided a problem that requires analyzing the response of a simplified second moment estimator from the Adaptive Moment Estimation (Adam) algorithm to sudden changes in the loss landscape's curvature. This analysis is to be performed under a deterministic model where the stochastic squared gradient is replaced by its piecewise-constant expectation.\n\nThe core of the problem is the deterministic, first-order linear recurrence relation governing the evolution of the second moment estimate, $v_t$:\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t\n$$\nwith the initial condition $v_0 = 0$. Here, $v_t$ is the estimator at iteration $t$, $\\beta_2 \\in [0, 1)$ is the exponential decay rate, and $m_t$ is the true second raw moment of the gradient, which is given as a piecewise-constant function of $t$.\n\nTo understand the dynamics, consider a period where $m_t$ is constant, say $m_t = m_{\\text{const}}$. The fixed point of the recurrence is $v = m_{\\text{const}}$. Let's define the error or deviation from this fixed point as $\\epsilon_t = v_t - m_{\\text{const}}$. Substituting this into the update rule gives:\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 (\\epsilon_{t-1} + m_{\\text{const}}) + (1 - \\beta_2) m_{\\text{const}}\n$$\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 \\epsilon_{t-1} + \\beta_2 m_{\\text{const}} + m_{\\text{const}} - \\beta_2 m_{\\text{const}}\n$$\n$$\n\\epsilon_t = \\beta_2 \\epsilon_{t-1}\n$$\nThis shows that the deviation from the target moment decays exponentially with a factor of $\\beta_2$ at each iteration. Consequently, after $\\ell$ iterations into a new constant regime, the initial deviation $\\epsilon_0$ will have been reduced by a factor of $\\beta_2^\\ell$. A value of $\\beta_2$ close to $1$ implies a slow decay and thus slow adaptation to a new moment, as the estimator retains a long memory of past values. Conversely, a $\\beta_2$ of $0$ implies instant adaptation, as $v_t = m_t$.\n\nThe problem asks for a responsiveness metric: the lag $\\ell$, which is the number of iterations required for $v_t$ to settle within a tolerance band $\\delta$ of a new target moment $m_{\\text{new}}$ following a change at iteration $t_c$. Specifically, we must find the smallest non-negative integer $\\ell$ such that $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$.\n\nWhile an analytical expression for $\\ell$ can be derived for an isolated regime change, the problem specifies scenarios with multiple, successive changes. The evolution of $v_t$ after a change at $t_c$ is affected not only by the new moment $m_{\\text{new}}$ but also by any subsequent changes to $m_t$ that occur before $v_t$ has fully converged. This complexity makes a direct simulation of the process the most robust and faithful method for solving the problem.\n\nThe algorithm proceeds as follows:\n1.  **Construct the Moment Sequence**: For each scenario, we first construct the complete sequence of true moments, $m_t$, for all iterations $t$ from $1$ to the horizon $T$, based on the given change points $\\{t_c\\}$ and regime values $\\{m\\}$.\n2.  **Simulate the Estimator's Evolution**: We then simulate the evolution of the estimator $v_t$ over the entire horizon. Starting with $v_0 = 0$, we iteratively compute each $v_t$ for $t=1, \\dots, T$ using the update rule $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) m_t$ and the pre-computed moment sequence.\n3.  **Calculate Settling Lags**: For each specified change point $t_c$ in a scenario, we analyze the resulting $v_t$ sequence to find the settling lag.\n    - The new target moment is $m_{\\text{new}}$, which is the value of the $m_t$ sequence for the regime starting at $t_c$.\n    - The convergence criterion is $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$.\n    - We search for the smallest non-negative integer $\\ell$ (from $\\ell=0$ up to $T-t_c$) for which this condition is met.\n    - If the condition is not satisfied for any $\\ell$ within the search window (i.e., up to a time of $t=T$), the lag is recorded as the maximum possible value, $T-t_c$, as per the problem specification.\n4.  **Compute the Final Score**: The final score for each scenario is the arithmetic mean of all calculated lags, rounded to the nearest integer.\n\nThis computational procedure accurately models the specified deterministic system and allows for the precise calculation of the responsiveness metric under complex, multi-stage regime shifts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_mean_lag(beta2, T, changes, regimes, delta):\n    \"\"\"\n    Calculates the rounded mean settling time for a single scenario.\n    \"\"\"\n    # Step 1: Construct the piecewise-constant moment sequence m_t\n    # m_seq is 1-indexed for problem clarity, so size is T+1\n    m_seq = np.zeros(T + 1)\n    \n    # Define the start and end of each regime\n    regime_starts = [1] + changes\n    regime_ends = changes + [T + 1]\n    \n    for i, m_val in enumerate(regimes):\n        start_idx = regime_starts[i]\n        end_idx = regime_ends[i]\n        if start_idx  end_idx: # Ensure the interval is valid\n            m_seq[start_idx:end_idx] = m_val\n\n    # Step 2: Simulate the evolution of the estimator v_t\n    # v_seq is 0-indexed for v_0, so size is T+1\n    v_seq = np.zeros(T + 1)  # v_seq[0] is v_0 = 0\n    \n    for t in range(1, T + 1):\n        v_seq[t] = beta2 * v_seq[t-1] + (1 - beta2) * m_seq[t]\n        \n    # Step 3: Calculate settling lags for each change\n    all_lags = []\n    for i, tc in enumerate(changes):\n        m_new = regimes[i + 1]\n        threshold = delta * m_new\n        lag_found = False\n        \n        # Search for the smallest non-negative lag l\n        max_l = T - tc\n        for l in range(max_l + 1):\n            t = tc + l\n            if abs(v_seq[t] - m_new) = threshold:\n                all_lags.append(l)\n                lag_found = True\n                break\n        \n        # If tolerance is not met, use the capped value\n        if not lag_found:\n            all_lags.append(max_l)\n            \n    # Step 4: Compute the final score (rounded mean lag)\n    if not all_lags:\n        return 0\n        \n    mean_lag = np.mean(all_lags)\n    return int(round(mean_lag))\n\ndef solve():\n    \"\"\"\n    Main function to run all test scenarios and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (beta2, T, changes, regimes, delta)\n        (0.0, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.9, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.999, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [20, 40, 80, 120], [1.0, 4.0, 0.5, 2.0, 1.5], 0.05),\n        (0.9999, 1000, [50, 120], [1.0, 10.0, 0.1], 0.05)\n    ]\n\n    results = []\n    for case in test_cases:\n        beta2, T, changes, regimes, delta = case\n        result = calculate_mean_lag(beta2, T, changes, regimes, delta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3096921"}, {"introduction": "After exploring how the step size is scaled, we now turn to its direction, which is guided by the momentum vector $m_t$. This practice allows you to diagnose the alignment between the momentum vector and the instantaneous gradient $g_t$, a key factor in ensuring effective descent. By computing metrics for alignment and step effectiveness on simple, illustrative gradient sequences, you will see firsthand how momentum can either help or hinder progress. [@problem_id:3096928]", "problem": "You are asked to design and implement a program that computes a diagnostic of alignment between the momentum vector and the instantaneous gradient under varying momentum parameters and to relate this alignment to the step effectiveness when using adaptive coordinate-wise scaling, in the spirit of Adaptive Moment Estimation (ADAM).\n\nBase definitions and assumptions:\n- Consider a differentiable scalar loss function $L(\\theta)$ with parameter vector $\\theta \\in \\mathbb{R}^d$ and instantaneous gradient $g_t = \\nabla_{\\theta} L(\\theta_t)$ at discrete time index $t$.\n- The first-order Taylor approximation gives $L(\\theta_t + \\Delta \\theta_t) \\approx L(\\theta_t) + \\langle g_t, \\Delta \\theta_t \\rangle$, so a step that reduces the loss should make $\\langle g_t, \\Delta \\theta_t \\rangle$ negative.\n- Define the momentum vector $m_t$ as an exponential moving average of gradients with parameter $\\beta_1 \\in [0,1)$ and initial condition $m_0 = 0$.\n- Define the second-moment accumulator $v_t$ as an exponential moving average of squared gradients with parameter $\\beta_2 \\in [0,1)$ and initial condition $v_0 = 0$, with coordinate-wise operations.\n- Define a coordinate-wise adaptive scaling using $v_t$ with a small stabilizer $\\epsilon  0$ to form the step $\\Delta \\theta_t$ from $m_t$.\n- No bias correction should be applied in any moving average.\n\nYour task is to compute for each time step $t$ two quantities:\n1. The cosine similarity between $m_t$ and $g_t$, defined as $\\cos_t = \\frac{\\langle m_t, g_t \\rangle}{\\|m_t\\|_2 \\, \\|g_t\\|_2}$.\n2. The normalized first-order step effectiveness under adaptive scaling, defined as $r_t = \\frac{-\\langle \\Delta \\theta_t, g_t \\rangle}{\\|\\Delta \\theta_t\\|_2 \\, \\|g_t\\|_2}$, which lies in $[0,1]$ when $\\Delta \\theta_t \\neq 0$ and $g_t \\neq 0$.\n\nConventions for degenerate cases:\n- If $\\|m_t\\|_2 = 0$ or $\\|g_t\\|_2 = 0$, define $\\cos_t = 0$.\n- If $\\|\\Delta \\theta_t\\|_2 = 0$ or $\\|g_t\\|_2 = 0$, define $r_t = 0$.\n\nImplementation details to be used in all test cases:\n- Momentum update with parameter $\\beta_1$: $m_t \\leftarrow \\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, g_t$ with $m_0 = 0$.\n- Second moment update with parameter $\\beta_2$: $v_t \\leftarrow \\beta_2 \\, v_{t-1} + (1 - \\beta_2) \\, (g_t \\odot g_t)$ with $v_0 = 0$ and coordinate-wise square, where $\\odot$ indicates Hadamard product.\n- Adaptive step using learning rate $\\alpha$: $\\Delta \\theta_t \\leftarrow - \\alpha \\, \\left( m_t \\oslash (\\sqrt{v_t} + \\epsilon) \\right)$, where division and square root are coordinate-wise and $\\alpha  0$ is the base learning rate.\n- Use $\\alpha = 1$, $\\beta_2 = 0.9$, and $\\epsilon = 10^{-8}$ in all test cases.\n\nAggregate outputs per test case:\n- Compute the arithmetic mean of $\\cos_t$ over all time steps in the sequence to obtain $\\rho$.\n- Compute the arithmetic mean of $r_t$ over all time steps in the sequence to obtain $\\eta$.\n- Report the pair $[\\rho, \\eta]$ for each test case, with each value rounded to $6$ decimal places.\n\nTest suite:\n- Use sequences in $\\mathbb{R}^2$ defined below. Each ordered pair is a vector in $\\mathbb{R}^2$.\n- Let the sequences be:\n  - $S_1$ (consistent then flip, length $6$): $(1,0),(1,0),(1,0),(-1,0),(-1,0),(-1,0)$.\n  - $S_2$ (orthogonal oscillation, length $6$): $(1,0),(0,1),(1,0),(0,1),(1,0),(0,1)$.\n  - $S_3$ (decay in magnitude, length $5$): $(3,0),(2,0),(1,0),(0.5,0),(0.25,0)$.\n  - $S_4$ (includes a zero gradient, length $3$): $(1,0),(0,0),(1,0)$.\n\nYou must evaluate the following five test cases:\n1. $\\beta_1 = 0.9$, sequence $S_1$.\n2. $\\beta_1 = 0$, sequence $S_1$.\n3. $\\beta_1 = 0.99$, sequence $S_1$.\n4. $\\beta_1 = 0.9$, sequence $S_2$.\n5. $\\beta_1 = 0.9$, sequence $S_3$.\n6. $\\beta_1 = 0.9$, sequence $S_4$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a list of lists, each inner list being $[\\rho, \\eta]$ for one test case, in the same order as listed above.\n- Each number must be rounded to $6$ decimal places.\n- For example, a valid format is $[[0.123456,0.654321],[0.000000,1.000000],\\dots]$ with no spaces.\n\nAngle units do not apply. There are no physical units involved. All reported values must be dimensionless floating-point numbers rounded to six decimal places as specified.", "solution": "The problem requires the design and implementation of a program to compute two diagnostic metrics, cosine similarity ($\\cos_t$) and normalized step effectiveness ($r_t$), for a simplified momentum-based adaptive learning rate algorithm. The algorithm's behavior is analyzed across several test cases involving different gradient sequences and momentum parameters.\n\nThe algorithm at each time step $t$, for a given sequence of gradients $\\{g_1, g_2, \\dots, g_T\\}$, is governed by the following update rules. The parameter vector is denoted by $\\theta \\in \\mathbb{R}^d$.\n\nFirst, we define the state variables:\n1.  The momentum vector, $m_t \\in \\mathbb{R}^d$, which is an exponential moving average of the gradients.\n2.  The second-moment accumulator, $v_t \\in \\mathbb{R}^d$, which is an exponential moving average of the element-wise square of the gradients.\n\nThe initial conditions at $t=0$ are $m_0 = 0$ and $v_0 = 0$, where $0$ is the zero vector in $\\mathbb{R}^d$.\n\nFor each time step $t=1, 2, \\dots, T$, given the gradient $g_t = \\nabla_{\\theta} L(\\theta_t)$, the updates proceed as follows:\n\n1.  **Momentum Update**: The momentum vector $m_t$ is updated using the previous momentum $m_{t-1}$ and the current gradient $g_t$. The update rule is given by:\n    $$m_t = \\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, g_t$$\n    where $\\beta_1 \\in [0, 1)$ is the momentum decay parameter. No bias correction is applied.\n\n2.  **Second-Moment Update**: The second-moment accumulator $v_t$ is updated using the previous accumulator $v_{t-1}$ and the current gradient $g_t$. This operation is performed element-wise.\n    $$v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2) \\, (g_t \\odot g_t)$$\n    where $\\beta_2 \\in [0, 1)$ is the second-moment decay parameter, and $\\odot$ denotes the Hadamard (element-wise) product. The problem specifies $\\beta_2 = 0.9$.\n\n3.  **Parameter Update Step Calculation**: The step vector $\\Delta \\theta_t$ is calculated using the momentum $m_t$ and the second-moment accumulator $v_t$. This step is adaptively scaled for each coordinate.\n    $$\\Delta \\theta_t = - \\alpha \\, \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$\n    where the division and square root are element-wise operations. The parameter $\\alpha  0$ is the learning rate (specified as $\\alpha = 1$), and $\\epsilon  0$ is a small constant to prevent division by zero (specified as $\\epsilon = 10^{-8}$).\n\nAfter calculating the state variables, we compute the two diagnostic metrics for each time step $t$:\n\n1.  **Cosine Similarity**: This metric, $\\cos_t$, measures the alignment between the momentum vector $m_t$ and the current gradient $g_t$. It is defined as:\n    $$\\cos_t = \\frac{\\langle m_t, g_t \\rangle}{\\|m_t\\|_2 \\, \\|g_t\\|_2}$$\n    where $\\langle \\cdot, \\cdot \\rangle$ is the dot product and $\\|\\cdot\\|_2$ is the L2-norm. By convention, if $\\|m_t\\|_2 = 0$ or $\\|g_t\\|_2 = 0$, then $\\cos_t = 0$.\n\n2.  **Normalized Step Effectiveness**: This metric, $r_t$, measures the alignment of the step direction with the ideal steepest descent direction ($-g_t$). A value of $r_t = 1$ indicates perfect alignment, while negative values indicate a counter-productive step that increases the loss based on a first-order approximation. It is defined as:\n    $$r_t = \\frac{-\\langle \\Delta \\theta_t, g_t \\rangle}{\\|\\Delta \\theta_t\\|_2 \\, \\|g_t\\|_2}$$\n    By convention, if $\\|\\Delta \\theta_t\\|_2 = 0$ or $\\|g_t\\|_2 = 0$, then $r_t = 0$.\n\nFor each test case, we are given a sequence of gradients $G = \\{g_1, g_2, \\dots, g_T\\}$ and a value for $\\beta_1$. We compute the sequences $\\{\\cos_1, \\dots, \\cos_T\\}$ and $\\{r_1, \\dots, r_T\\}$. The final output for the test case is a pair $[\\rho, \\eta]$, where $\\rho$ and $\\eta$ are the arithmetic means of the respective metric sequences:\n$$\\rho = \\frac{1}{T} \\sum_{t=1}^{T} \\cos_t$$\n$$\\eta = \\frac{1}{T} \\sum_{t=1}^{T} r_t$$\n\nThe implementation will proceed by defining a function that takes a gradient sequence and the parameter $\\beta_1$ as input. Inside this function, it initializes $m_0$ and $v_0$ as zero vectors of the appropriate dimension ($d=2$). It then iterates from $t=1$ to $T$, applying the update and metric calculation rules at each step. The lists of $\\cos_t$ and $r_t$ values are collected, and their means are computed and returned after rounding. This process is repeated for each of the six specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, simulating the adaptive\n    optimizer dynamics, and calculating the required diagnostic metrics.\n    \"\"\"\n\n    # Define the constants to be used in all test cases.\n    alpha = 1.0\n    beta_2 = 0.9\n    epsilon = 1e-8\n\n    # Define the gradient sequences from the problem statement.\n    sequences = {\n        \"S1\": np.array([\n            [1.0, 0.0], [1.0, 0.0], [1.0, 0.0],\n            [-1.0, 0.0], [-1.0, 0.0], [-1.0, 0.0]\n        ]),\n        \"S2\": np.array([\n            [1.0, 0.0], [0.0, 1.0], [1.0, 0.0],\n            [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]\n        ]),\n        \"S3\": np.array([\n            [3.0, 0.0], [2.0, 0.0], [1.0, 0.0],\n            [0.5, 0.0], [0.25, 0.0]\n        ]),\n        \"S4\": np.array([\n            [1.0, 0.0], [0.0, 0.0], [1.0, 0.0]\n        ]),\n    }\n\n    # Define the test cases with their specific parameters.\n    test_cases = [\n        {\"beta1\": 0.9, \"sequence\": sequences[\"S1\"]},\n        {\"beta1\": 0.0, \"sequence\": sequences[\"S1\"]},\n        {\"beta1\": 0.99, \"sequence\": sequences[\"S1\"]},\n        {\"beta1\": 0.9, \"sequence\": sequences[\"S2\"]},\n        {\"beta1\": 0.9, \"sequence\": sequences[\"S3\"]},\n        {\"beta1\": 0.9, \"sequence\": sequences[\"S4\"]},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        beta1 = case[\"beta1\"]\n        gradients = case[\"sequence\"]\n        \n        num_steps, dim = gradients.shape\n        m = np.zeros(dim)\n        v = np.zeros(dim)\n        \n        cos_t_list = []\n        r_t_list = []\n\n        for t in range(num_steps):\n            g_t = gradients[t]\n            \n            # 1. Update momentum and second moment vectors\n            m = beta1 * m + (1 - beta1) * g_t\n            v = beta_2 * v + (1 - beta_2) * (g_t * g_t)\n            \n            # 2. Calculate the adaptive step\n            delta_theta_t = -alpha * (m / (np.sqrt(v) + epsilon))\n\n            # 3. Calculate norms\n            norm_m_t = np.linalg.norm(m)\n            norm_g_t = np.linalg.norm(g_t)\n            norm_delta_theta_t = np.linalg.norm(delta_theta_t)\n\n            # 4. Calculate cos_t\n            if norm_m_t == 0 or norm_g_t == 0:\n                cos_t = 0.0\n            else:\n                cos_t = np.dot(m, g_t) / (norm_m_t * norm_g_t)\n            \n            # 5. Calculate r_t\n            if norm_delta_theta_t == 0 or norm_g_t == 0:\n                r_t = 0.0\n            else:\n                r_t = -np.dot(delta_theta_t, g_t) / (norm_delta_theta_t * norm_g_t)\n            \n            cos_t_list.append(cos_t)\n            r_t_list.append(r_t)\n\n        # 6. Compute the arithmetic mean of the metrics for the case\n        rho = np.mean(cos_t_list)\n        eta = np.mean(r_t_list)\n        \n        results.append([round(rho, 6), round(eta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3096928"}, {"introduction": "A major challenge in training deep neural networks is instability caused by large, sudden gradient spikes. This exercise moves from analysis to engineering by tasking you with implementing and testing a robust solution: scale-aware gradient clipping. You will derive how this technique leverages the optimizer's own scale estimate, $\\sqrt{v_{i,t}}$, to set an adaptive clipping bound, thereby preventing exploding updates while preserving the core benefits of adaptive rescaling. [@problem_id:3097005]", "problem": "You are asked to reason from first principles about scale-aware clipping in adaptive learning rate algorithms and to implement and test your conclusions in a complete, runnable program. Begin from the standard Root Mean Square Propagation (RMSProp) update, an adaptive method that rescales per-parameter gradients by an exponential moving average of squared gradients. Specifically, let $i$ index parameters and $t$ index steps. Let $g_{i,t}$ denote the stochastic gradient of a differentiable objective at step $t$, and define the second-moment accumulator $v_{i,t}$ by the exponential recursion $v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$ with $v_{i,0} = 0$ and $\\beta \\in [0,1)$. The baseline RMSProp step for parameter $i$ at time $t$ is $\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\, g_{i,t} / \\left(\\sqrt{v_{i,t-1}} + \\varepsilon\\right)$ with learning rate $\\alpha  0$ and stabilizer $\\varepsilon \\ge 0$, followed by $\\theta_{i,t} \\leftarrow \\theta_{i,t-1} + \\Delta \\theta_{i,t}$. Note that the preconditioning uses $v_{i,t-1}$ to avoid circular dependence within step $t$.\n\nTask A (derivation). Propose and justify a per-parameter clipping rule that is proportional to the square root of the second-moment accumulator, parameterized by a constant $\\lambda  0$ and the same stabilizer $\\varepsilon \\ge 0$. Your rule should be stated as an explicit inequality on a clipped gradient $\\tilde{g}_{i,t}$ in terms of $v_{i,t-1}$, $\\lambda$, and $\\varepsilon$. From the RMSProp definitions above and your clipping rule, derive a step-size bound of the form $\\lvert \\Delta \\theta_{i,t} \\rvert \\le \\text{(expression independent of $g_{i,t}$)}$ that does not depend on the unbounded magnitude of $g_{i,t}$, and explain why this is expected to reduce exploding updates while preserving the adaptive rescaling benefits of RMSProp.\n\nTask B (implementation and test). Implement two optimizers over a synthetic convex quadratic objective $f(\\theta) = \\tfrac{1}{2} \\sum_{i=1}^{d} c_i \\, \\theta_i^2$, where $d$ is the dimensionality and the curvature vector $c \\in \\mathbb{R}^{d}_{0}$ is provided per test. The exact gradient is $g_{i,t}^{\\text{base}} = c_i \\, \\theta_{i,t-1}$. To simulate bursty or heavy-tailed stochasticity, the observed gradient used for updates at step $t$ is $g_{i,t} = g_{i,t}^{\\text{base}} + s_{i,t}$, where the spike schedule $s_{i,t}$ is a sparse vector with nonzero entries only at specified $(t,i)$ pairs. Implement:\n- A baseline RMSProp variant (no clipping): use $g_{i,t}$ in the step and update $v_{i,t}$ by the standard recursion.\n- A scale-aware clipped variant: apply your proposed per-parameter clipping to $g_{i,t}$ to obtain $\\tilde{g}_{i,t}$, then use $\\tilde{g}_{i,t}$ in the step while still updating $v_{i,t}$ by the standard recursion with the unmodified $g_{i,t}$.\n\nFor each run, track the maximum absolute per-parameter step magnitude $M = \\max_{t,i} \\lvert \\Delta \\theta_{i,t} \\rvert$ and the final objective value $f(\\theta_T)$ at the end of $T$ steps.\n\nDefine two evaluation predicates per test case $j$:\n- Exploding-update reduction predicate $E_j$: true if the baseline exhibits an oversized step relative to a supplied threshold $\\tau$ while the clipped variantâ€™s maximum step is bounded by your derived bound. Formally, $E_j$ is true if $\\left(M^{\\text{base}}  \\tau\\right)$ and $\\left(M^{\\text{clip}} \\le \\alpha \\lambda + \\text{tol}\\right)$, where $\\text{tol}  0$ is a small numerical tolerance.\n- Adaptive-benefit preservation predicate $P_j$: true if the clipped final loss is not worse than the baseline by more than a supplied relative tolerance $\\delta  0$, i.e., $f^{\\text{clip}}(\\theta_T) \\le (1+\\delta) \\, f^{\\text{base}}(\\theta_T)$.\n\nFor each test case $j$, output $r_j = 1$ if both $E_j$ and $P_j$ are true, and $r_j = 0$ otherwise.\n\nTest suite. Your program must run exactly the following $3$ test cases, each specified by a tuple containing $(d, c, \\theta_0, T, \\alpha, \\beta, \\varepsilon, \\lambda, \\text{spikes}, \\tau, \\delta, \\text{tol})$:\n- Case $1$ (happy path with a large spike on a low-curvature coordinate):\n  - $d = 3$\n  - $c = [1.0, 0.1, 10.0]$\n  - $\\theta_0 = [1.0, 1.0, 1.0]$\n  - $T = 60$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 2.0$\n  - spikes: at step $t=10$, add $+200.0$ to coordinate $i=2$ (zero-based index), and at step $t=20$, add $-150.0$ to coordinate $i=2$\n  - $\\tau = 50.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n- Case $2$ (no spikes, boundary condition where clipping should be inactive):\n  - $d = 3$\n  - $c = [1.0, 0.3, 3.0]$\n  - $\\theta_0 = [1.5, -0.5, 0.75]$\n  - $T = 100$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 5.0$\n  - spikes: none\n  - $\\tau = 1.0$\n  - $\\delta = 0.05$\n  - $\\text{tol} = 10^{-12}$\n- Case $3$ (multiple spikes on a very low-curvature coordinate and one moderate spike elsewhere):\n  - $d = 4$\n  - $c = [1.0, 0.2, 5.0, 0.05]$\n  - $\\theta_0 = [1.0, 1.0, 1.0, 1.0]$\n  - $T = 120$\n  - $\\alpha = 0.04$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 1.5$\n  - spikes: at step $t=5$ add $+300.0$ to coordinate $i=3$; at step $t=6$ add $-300.0$ to coordinate $i=3$; at step $t=30$ add $+150.0$ to coordinate $i=0$\n  - $\\tau = 30.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n\nAngle units are not applicable. No physical units are used. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the test cases. For example, if $r_1 = 1$, $r_2 = 0$, and $r_3 = 1$, the required output is \"[1,0,1]\".", "solution": "### Task A: Derivation of Scale-Aware Clipping and Step-Size Bound\n\nThe objective is to propose and justify a per-parameter clipping rule for the RMSProp optimizer that is proportional to the estimated scale of the gradients. The RMSProp algorithm maintains a per-parameter second-moment accumulator, $v_{i,t}$, which tracks an exponential moving average of the squared gradients:\n$$v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$$\nHere, $g_{i,t}$ is the gradient of the objective with respect to parameter $\\theta_i$ at step $t$, and $\\beta \\in [0,1)$ is a decay factor. The magnitude of recent gradients for parameter $i$ is estimated by $\\sqrt{v_{i,t-1}}$. The standard RMSProp update normalizes the gradient by this scale estimate (plus a small stabilizer $\\varepsilon \\ge 0$):\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{g_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n\n**Proposed Clipping Rule**\n\nTo make the clipping \"scale-aware,\" we define a clipping threshold that is itself proportional to the gradient scale estimate. The problem specifies this should be parameterized by a constant $\\lambda  0$. A natural choice for the clipping threshold for parameter $i$ at step $t$ is $C_{i,t} = \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$. This directly ties the clipping bound to the same adaptive scaling factor used in the RMSProp update itself.\n\nThe clipping rule transforms the raw gradient $g_{i,t}$ into a clipped gradient $\\tilde{g}_{i,t}$. We enforce that the magnitude of the clipped gradient does not exceed this threshold. This is stated as the following inequality on the clipped gradient $\\tilde{g}_{i,t}$:\n$$|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$$\nThis rule can be implemented by clipping the value of $g_{i,t}$ to the range $[-C_{i,t}, C_{i,t}]$.\n\n**Derivation of the Step-Size Bound**\n\nWith the scale-aware clipping rule established, we now consider the modified parameter update which uses the clipped gradient $\\tilde{g}_{i,t}$:\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\nTo derive a bound on the magnitude of this update step, $|\\Delta \\theta_{i,t}|$, we take the absolute value of the expression:\n$$|\\Delta \\theta_{i,t}| = \\left| -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon} \\right| = \\alpha \\frac{|\\tilde{g}_{i,t}|}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\nNow, we substitute the inequality from our proposed clipping rule, $|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$:\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\frac{\\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\nThe term $(\\sqrt{v_{i,t-1}} + \\varepsilon)$ cancels out, yielding the final step-size bound:\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$$\nThis bound is an elegant result: the magnitude of the parameter update is guaranteed to be no larger than the product of the learning rate $\\alpha$ and the clipping parameter $\\lambda$. Critically, this bound is completely independent of the magnitude of the raw, unclipped gradient $g_{i,t}$.\n\n**Justification and Benefits**\n\nThis scale-aware clipping mechanism provides two principal advantages:\n\n$1$. **Reduction of Exploding Updates**: The derived bound $|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$ demonstrates that large, anomalous gradients (spikes) cannot cause arbitrarily large parameter updates. By clipping the gradient before it is used in the update step, the algorithm prevents such events from destabilizing the training process and pushing the parameters into poor regions of the loss landscape.\n\n$2$. **Preservation of Adaptive Rescaling**: The clipping is not performed with a single, global threshold. Instead, the threshold $C_{i,t}$ is adaptive and parameter-specific, as it depends on $v_{i,t-1}$. For parameters that naturally have large gradients, $v_{i,t-1}$ will be large, resulting in a more lenient clipping threshold. Conversely, for parameters with small gradients, the threshold will be tighter. This preserves the fundamental benefit of RMSProp, which is to adapt the effective learning rate on a per-parameter basis according to its historical gradient scale. Furthermore, the problem specifies that the second-moment accumulator $v_{i,t}$ is updated using the *unmodified* gradient $g_{i,t}$. This is a crucial design choice. It ensures that $v_{i,t}$ remains an accurate, unbiased estimator of the true second moment of the gradients, including any spikes. If $v_{i,t}$ were updated with the clipped gradient $\\tilde{g}_{i,t}$, it would systematically underestimate the gradient variance, thereby compromising the integrity of the adaptive scaling itself over time.\n\n### Task B: Implementation and Test\n\nWe now proceed to the implementation and testing phase. Two optimizers, a baseline RMSProp and our scale-aware clipped variant, will be simulated on a synthetic convex quadratic objective. Their performance will be evaluated according to the specified predicates for exploding-update reduction and adaptive-benefit preservation across three test cases. The final program will encapsulate this entire simulation and evaluation process.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the complete simulation and print the final result.\n    It defines test cases and calls helper functions to perform the optimization\n    and evaluation logic.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Spike on the highest-curvature coordinate.\n        (3, [1.0, 0.1, 10.0], [1.0, 1.0, 1.0], 60, 0.05, 0.9, 1e-3, 2.0, {(10, 2): 200.0, (20, 2): -150.0}, 50.0, 0.5, 1e-12),\n        # Case 2: No spikes, testing behavior from initial state.\n        (3, [1.0, 0.3, 3.0], [1.5, -0.5, 0.75], 100, 0.05, 0.9, 1e-3, 5.0, {}, 1.0, 0.05, 1e-12),\n        # Case 3: Multiple spikes on different coordinates.\n        (4, [1.0, 0.2, 5.0, 0.05], [1.0, 1.0, 1.0, 1.0], 120, 0.04, 0.9, 1e-3, 1.5, {(5, 3): 300.0, (6, 3): -300.0, (30, 0): 150.0}, 30.0, 0.5, 1e-12),\n    ]\n\n    def _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, mode):\n        \"\"\"\n        Runs a single optimization trajectory for either baseline or clipped RMSProp.\n        \n        Returns:\n            - max_abs_step (float): The maximum absolute step magnitude over the trajectory.\n            - final_loss (float): The final value of the objective function.\n        \"\"\"\n        # Use np.float64 for higher numerical precision and stability.\n        theta = np.array(theta0, dtype=np.float64)\n        c_arr = np.array(c, dtype=np.float64)\n        v = np.zeros(d, dtype=np.float64)\n        max_abs_step = 0.0\n\n        for t_step in range(1, T + 1):\n            # Calculate the base gradient from the convex quadratic objective\n            grad_base = c_arr * theta\n            \n            # Construct the sparse spike vector for the current step\n            spikes = np.zeros(d, dtype=np.float64)\n            for i in range(d):\n                if (t_step, i) in spikes_dict:\n                    spikes[i] = spikes_dict.get((t_step, i), 0.0)\n            \n            # The observed gradient is the sum of the base gradient and the spike\n            grad_observed = grad_base + spikes\n\n            # Denominator for the RMSProp update\n            denominator = np.sqrt(v) + epsilon\n\n            if mode == 'baseline':\n                # Standard RMSProp update uses the observed gradient\n                step = -alpha * grad_observed / denominator\n            elif mode == 'clipped':\n                # Scale-aware clipping: threshold is proportional to the scale estimate\n                clip_threshold = lam * denominator\n                grad_clipped = np.clip(grad_observed, -clip_threshold, clip_threshold)\n                # The step uses the clipped gradient\n                step = -alpha * grad_clipped / denominator\n            else:\n                raise ValueError(\"Invalid optimizer mode specified.\")\n\n            # Track the maximum absolute per-parameter step magnitude\n            current_max_abs_step = np.max(np.abs(step))\n            if current_max_abs_step > max_abs_step:\n                max_abs_step = current_max_abs_step\n            \n            # Update parameters\n            theta += step\n            \n            # Update the second-moment accumulator using the unclipped gradient\n            v = beta * v + (1 - beta) * np.square(grad_observed)\n\n        # Calculate the final objective value\n        final_loss = 0.5 * np.sum(c_arr * np.square(theta))\n        \n        return max_abs_step, final_loss\n\n    def _run_simulation(case_params):\n        \"\"\"\n        Manages a single test case, running both optimizers and evaluating predicates.\n        \n        Returns:\n            - 1 if both predicates (E_j and P_j) are true, 0 otherwise.\n        \"\"\"\n        d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, tau, delta, tol = case_params\n\n        # Run both baseline and clipped optimizers\n        M_base, f_final_base = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'baseline')\n        M_clip, f_final_clip = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'clipped')\n\n        # Evaluate Predicate E_j: Exploding-update reduction\n        # Checks if the baseline step exploded while the clipped step remained bounded as derived.\n        e_j = (M_base > tau) and (M_clip = alpha * lam + tol)\n        \n        # Evaluate Predicate P_j: Adaptive-benefit preservation\n        # Checks if the clipped version's final loss is not substantially worse than the baseline.\n        p_j = f_final_clip = (1 + delta) * f_final_base\n\n        return 1 if e_j and p_j else 0\n\n    results = []\n    for case in test_cases:\n        result = _run_simulation(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097005"}]}