{"hands_on_practices": [{"introduction": "To truly grasp Stochastic Gradient Descent (SGD), let's begin with the fundamentals. This first exercise strips the algorithm down to its core: performing a single update step on a simple, one-dimensional function. By manually calculating this update, you will build a concrete intuition for how SGD uses a single piece of information to make progress towards a minimum. [@problem_id:2206637]", "problem": "An iterative optimization algorithm is used to find a parameter $x$ that minimizes a cost function. The total cost function is an average of several component functions: $F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$. In this specific case, the component functions are quadratic and given by $f_i(x) = (x - c_i)^2$, where the constants are $c_i = i$ for $i = 1, 2, \\dots, 10$, and thus $N=10$.\n\nThe optimization process starts with an initial guess for the parameter, $x_0$. At each step, a new estimate, $x_{k+1}$, is calculated from the current estimate, $x_k$, by using only a single, randomly chosen component function, $f_j(x)$. The update rule is defined as:\n$$x_{k+1} = x_k - \\eta \\left( \\left.\\frac{d f_j(x)}{dx}\\right|_{x=x_k} \\right)$$\nwhere $\\eta$ is a constant known as the learning rate.\n\nGiven an initial parameter value of $x_0 = 10.0$ and a learning rate of $\\eta = 0.1$, compute the value of the parameter $x_1$ after one update step. For this first step, the component function used is $f_j(x)$ with the index $j=5$.", "solution": "We are given component functions of the form $f_{i}(x) = (x - c_{i})^{2}$ with $c_{i} = i$. For the first update, the chosen index is $j=5$, so $f_{5}(x) = (x - 5)^{2}$.\n\nThe update rule is\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\nUsing the power rule and chain rule, the derivative of the chosen component is\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\nEvaluating at the current iterate $x_{0} = 10$ gives\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\nWith learning rate $\\eta = 0.1$, the update becomes\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\nThus, after one update step using $f_{5}$, the parameter value is $x_{1} = 9$.", "answer": "$$\\boxed{9}$$", "id": "2206637"}, {"introduction": "Having mastered a single step, we now move to a more realistic machine learning context. This practice challenges you to derive the SGD update rule for a non-linear model with a vector of parameters, a common task when building custom models. This exercise will solidify your understanding of how to apply vector calculus to find the gradient that guides the learning process for more complex functions. [@problem_id:2206657]", "problem": "In a machine learning model for predicting a positive quantity, the predicted value $\\hat{y}$ is related to a $d$-dimensional feature vector $x$ by the formula $\\hat{y} = \\exp(w^T x)$, where $w$ is a $d$-dimensional weight vector. The goal is to adjust the weights $w$ to make the predictions as close as possible to the true observed values.\n\nThe performance of the model is measured by a loss function. For a dataset of $m$ samples $\\{(x_i, y_i)\\}_{i=1}^m$, the total loss is defined as the sum of squared differences between the model's prediction and the true value:\n$$\nL(w) = \\sum_{i=1}^{m} \\left( \\exp(w^T x_i) - y_i \\right)^2\n$$\nWe use Stochastic Gradient Descent (SGD), an iterative optimization algorithm, to minimize this loss. In each iteration, SGD approximates the total loss gradient using only a single data sample.\n\nSuppose the current weight vector is $w$. A single data sample $(x_k, y_k)$ is selected. Using this sample, a single update step of SGD is performed with a learning rate $\\eta  0$. Derive the expression for the updated weight vector, $w_{new}$. Your final expression should be in terms of $w$, $\\eta$, $x_k$, and $y_k$.", "solution": "We consider the single-sample loss for the selected sample $(x_{k}, y_{k})$,\n$$\n\\ell_{k}(w) = \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right)^{2}.\n$$\nTo perform an SGD update, we need the gradient of $\\ell_{k}(w)$ with respect to $w$. Let $f(w) = \\exp(w^{T} x_{k}) - y_{k}$. Then $\\ell_{k}(w) = f(w)^{2}$, so by the chain rule,\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 f(w)\\, \\nabla_{w} f(w).\n$$\nNext, compute $\\nabla_{w} f(w)$. Since $f(w) = \\exp(w^{T} x_{k}) - y_{k}$ and $\\nabla_{w}(w^{T} x_{k}) = x_{k}$, we have\n$$\n\\nabla_{w} f(w) = \\exp\\!\\left(w^{T} x_{k}\\right) \\, x_{k}.\n$$\nTherefore,\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\!\\left(w^{T} x_{k}\\right) x_{k}.\n$$\nThe SGD update with learning rate $\\eta0$ is\n$$\nw_{\\text{new}} = w - \\eta \\nabla_{w} \\ell_{k}(w),\n$$\nwhich yields\n$$\nw_{\\text{new}} = w - 2 \\eta \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\!\\left(w^{T} x_{k}\\right) x_{k}.\n$$", "answer": "$$\\boxed{w - 2 \\eta \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right)\\exp\\!\\left(w^{T} x_{k}\\right) x_{k}}$$", "id": "2206657"}, {"introduction": "In SGD, not all steps lead to progress; the size of each step, controlled by the learning rate $\\eta$, is crucial. This final exercise provides a cautionary tale, demonstrating how an improperly chosen learning rate can cause the optimization process to become unstable and diverge from the solution. Understanding this failure mode by observing the parameter \"overshoot\" the target is just as important as knowing how the algorithm works when it succeeds. [@problem_id:2206673]", "problem": "In a machine learning context, we often optimize a model's parameters by minimizing a loss function. Consider a simplified model with a single scalar parameter, $w$. The loss function associated with a single data point is given by $L(w) = \\frac{1}{2} c w^2$, where the minimum loss occurs at $w=0$. The parameter is updated using the Stochastic Gradient Descent (SGD) algorithm. The update rule for the parameter at step $k$ is given by $w_{k+1} = w_k - \\eta \\nabla L(w_k)$, where $\\nabla L(w_k)$ is the gradient of the loss function evaluated at $w_k$, and $\\eta$ is the learning rate.\n\nSuppose the initial value of the parameter is $w_0 = 4.0$. The model parameters are set as $c = 0.75$ and the learning rate is $\\eta = 3.2$. Calculate the value of the parameter $w$ after 3 update steps (i.e., find $w_3$).\n\nRound your final answer to three significant figures.", "solution": "The loss is $L(w)=\\frac{1}{2} c w^{2}$. Its gradient is obtained by differentiation:\n$$\n\\nabla L(w)=\\frac{\\mathrm{d}}{\\mathrm{d}w}\\left(\\frac{1}{2} c w^{2}\\right)=c w.\n$$\nThe SGD update rule is\n$$\nw_{k+1}=w_{k}-\\eta \\nabla L(w_{k})=w_{k}-\\eta c w_{k}=(1-\\eta c)\\,w_{k}.\n$$\nThis linear recurrence solves to\n$$\nw_{k}=(1-\\eta c)^{k} w_{0}.\n$$\nSubstituting $c=0.75$, $\\eta=3.2$, and $w_{0}=4.0$,\n$$\n1-\\eta c=1-(3.2)(0.75)=1-2.4=-1.4,\n$$\nso\n$$\nw_{3}=(-1.4)^{3}\\cdot 4.0=-10.976.\n$$\nRounding to three significant figures gives $-11.0$.", "answer": "$$\\boxed{-11.0}$$", "id": "2206673"}]}