{"hands_on_practices": [{"introduction": "In autoregressive models like the decoder of a Transformer, it is crucial to prevent a token at a given position from attending to subsequent, \"future\" tokens. This is accomplished using an attention mask. This practice will guide you through the mathematical underpinnings of masking, showing how adding $-\\infty$ to logits in log-space is equivalent to multiplication by zero in the standard probability space. Through a hands-on implementation, you will also demonstrate the critical consequences of an improperly constructed mask, which can lead to \"information leakage\" from the future and break the model's causal structure [@problem_id:3193602].", "problem": "You are to reason from first principles about masked attention in the Transformer architecture. Use only the following foundational bases: the definition of the SoftMax function, basic matrix multiplication, the property of the exponential and logarithm functions, and the scaled dot-product attention definition. Your task is to derive and then empirically verify that applying a binary mask before SoftMax is equivalent to an elementwise multiplication by the mask in the exponential domain, and to demonstrate how incorrect masks cause information leakage from future tokens to past outputs.\n\nDefinitions to be used as the starting point:\n- The SoftMax function over a vector $x \\in \\mathbb{R}^n$ is $$\n\\operatorname{SoftMax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}} \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\n- The Scaled Dot-Product Attention (SDPA) logits for queries $Q \\in \\mathbb{R}^{L \\times d_k}$ and keys $K \\in \\mathbb{R}^{L \\times d_k}$ are $$\nZ = \\frac{QK^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{L \\times L}.\n$$\n- A binary mask $M \\in \\{0,1\\}^{L \\times L}$ indicates disallowed positions by $0$ and allowed positions by $1$. The Hadamard (elementwise) product is denoted by $Z \\odot M$.\n\nYour program must:\n- Derive (in your own reasoning, not inside the program) from the above definitions that adding an additive mask $A$ with entries $A_{ij} = \\log M_{ij}$ to the logits $Z$ before SoftMax is equivalent to performing an elementwise multiplication by $M$ in the exponential domain before normalization, i.e., that the masked SoftMax equals $$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} e^{Z_{:,j}} \\odot M_{:,j}} \\quad \\text{row-wise}.\n$$\n- Explain why using a finite large negative constant $-C$ in place of $-\\infty$ for masked entries causes small but nonzero probability mass on masked positions, which can lead to leakage if the mask is incorrect.\n\nThen implement a single program that computes the following four test cases with the exact numerical parameters given below and outputs their results as specified.\n\nGiven constants and matrices:\n- Use sequence length $L = 4$, key dimension $d_k = 3$, and value dimension $d_v = 2$.\n- Use\n$$\nQ = \\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n1 & 1 & 1\n\\end{bmatrix}, \\quad\nK = \\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n1 & 1 & 1\n\\end{bmatrix}, \\quad\nV = \\begin{bmatrix}\n1 & 2\\\\\n3 & 4\\\\\n5 & 6\\\\\n7 & 8\n\\end{bmatrix}.\n$$\n- Define the correct causal binary mask $M^{\\mathrm{causal}} \\in \\{0,1\\}^{4 \\times 4}$ by $M^{\\mathrm{causal}}_{ij} = 1$ if $j \\le i$ and $M^{\\mathrm{causal}}_{ij} = 0$ otherwise.\n- Define the incorrect off-by-one binary mask $M^{\\mathrm{off}} \\in \\{0,1\\}^{4 \\times 4}$ by $M^{\\mathrm{off}}_{ij} = 1$ if $j \\le i+1$ and $M^{\\mathrm{off}}_{ij} = 0$ otherwise.\n- For demonstrative modification, define $V^{\\mathrm{mod}}$ to be equal to $V$ except its last row is changed to $[700, 800]$.\n\nAdditionally, for the SoftMax masking equivalence tests, use the following smaller logits and mask:\n$$\nZ^{(a)} = \\begin{bmatrix}\n0.2 & -0.1 & 0.4\\\\\n1.0 & -1.0 & 0.0\n\\end{bmatrix}, \\quad\nM^{(a)} = \\begin{bmatrix}\n1 & 0 & 1\\\\\n0 & 1 & 1\n\\end{bmatrix}.\n$$\n\nTest suite to implement and compute:\n- Case $1$ (Equivalence of masking as multiplication before SoftMax): Compute row-wise SoftMax on $Z^{(a)}$ with mask $M^{(a)}$ in two ways:\n  - Ideal multiplicative form: numerator $e^{Z^{(a)}} \\odot M^{(a)}$ and denominator the row-wise sum of that numerator.\n  - Additive approximation with a large negative constant $-C$ in logits for masked positions, with $C = 10^9$.\n  Report the maximum absolute difference between the two resulting probability matrices as a floating-point number.\n- Case $2$ (No leakage with correct causal mask): Compute attention outputs $Y$ using $Q$, $K$, $V$, and $M^{\\mathrm{causal}}$. Then recompute using $Q$, $K$, $V^{\\mathrm{mod}}$, and $M^{\\mathrm{causal}}$. Check whether the first $3$ rows (positions $0$, $1$, and $2$) of the outputs are unchanged within an absolute tolerance of $10^{-12}$. Report a boolean that is true if unchanged and false otherwise.\n- Case $3$ (Leakage with incorrect mask): Compute attention outputs using $Q$, $K$, $V$ with $M^{\\mathrm{off}}$ and then with $V^{\\mathrm{mod}}$ with $M^{\\mathrm{off}}$. Check whether any of the first $3$ rows changed by more than an absolute tolerance of $10^{-6}$. Report a boolean that is true if leakage is detected and false otherwise.\n- Case $4$ (Finite-mask boundary effect): Repeat Case $1$ but use $C = 50$ instead of $C = 10^9$ in the additive approximation. Report the maximum absolute difference as a floating-point number.\n\nAngle units are not involved. No physical units are involved. All reported numeric answers must be the specified booleans or floating-point numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{case1\\_float}, \\text{case2\\_bool}, \\text{case3\\_bool}, \\text{case4\\_float}]$. For example, a syntactically valid format is $[0.0,True,False,1e-12]$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the mathematical principles of the Transformer architecture, well-posed with a complete and consistent set of definitions and parameters, and objective in its formulation. The tasks require a rigorous derivation from first principles and an empirical verification, both of which are central to scientific and engineering disciplines. All conditions for a valid problem are met.\n\nThe core of the problem lies in understanding how masking is implemented in the SoftMax function within the scaled dot-product attention mechanism and the consequences of imperfect implementations.\n\n**Part 1: Derivation of Masking Equivalence**\n\nWe are tasked to derive that adding an additive mask $A$ where $A_{ij} = \\log M_{ij}$ to the logits $Z$ before applying the SoftMax function is equivalent to an elementwise multiplication by the binary mask $M$ in the exponential domain.\n\nLet $Z \\in \\mathbb{R}^{L \\times L}$ be the matrix of attention logits, and let $M \\in \\{0, 1\\}^{L \\times L}$ be a binary mask where $M_{ij}=1$ indicates an allowed attention connection and $M_{ij}=0$ indicates a disallowed one. We define an additive mask matrix $A \\in (\\mathbb{R} \\cup \\{-\\infty\\})^{L \\times L}$ with entries $A_{ij} = \\log M_{ij}$.\n\nThe SoftMax function is applied row-wise to the logits matrix. For a given row $i$, the masked SoftMax probability for the connection to column $j$ is computed on the additively masked logits $Z'_{ij} = Z_{ij} + A_{ij}$.\n\nAccording to the provided definition of the SoftMax function, for row $i$:\n$$\n\\operatorname{SoftMax}(Z'_i)_j = \\frac{e^{Z'_{ij}}}{\\sum_{k=1}^L e^{Z'_{ik}}}\n$$\nSubstitute $Z'_{ij} = Z_{ij} + A_{ij} = Z_{ij} + \\log M_{ij}$:\n$$\n\\operatorname{SoftMax}(Z_i + \\log M_i)_j = \\frac{e^{Z_{ij} + \\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik} + \\log M_{ik}}}\n$$\nUsing the property of the exponential function, $e^{a+b} = e^a e^b$, we can rewrite the expression as:\n$$\n\\frac{e^{Z_{ij}} e^{\\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik}} e^{\\log M_{ik}}}\n$$\nNow, we analyze the term $e^{\\log M_{ij}}$. The binary mask $M$ has entries that are either $1$ or $0$.\n- If $M_{ij} = 1$, then $\\log M_{ij} = \\log 1 = 0$. Consequently, $e^{\\log M_{ij}} = e^0 = 1$. So, $e^{\\log M_{ij}} = M_{ij}$.\n- If $M_{ij} = 0$, then $\\log M_{ij} = \\log 0$. In the limiting sense required for this context, $\\log 0 \\to -\\infty$. Consequently, $e^{\\log M_{ij}} \\to e^{-\\infty} = 0$. So, $e^{\\log M_{ij}} = M_{ij}$.\n\nIn both cases, $e^{\\log M_{ij}} = M_{ij}$. Substituting this identity back into the main expression:\n$$\n\\frac{e^{Z_{ij}} M_{ij}}{\\sum_{k=1}^L e^{Z_{ik}} M_{ik}}\n$$\nThis expression can be written using Hadamard (elementwise) product notation. Let $E$ be the matrix with entries $E_{ij}=e^{Z_{ij}}$. Then the numerator is $(E \\odot M)_{ij}$. The denominator is the sum of the $i$-th row of the matrix $E \\odot M$. This proves the equivalence:\n$$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} (e^{Z} \\odot M)_{:,j}} \\quad \\text{(row-wise)}\n$$\nThis derivation formally shows that adding the logarithm of the mask to the logits is mathematically equivalent to multiplying by the mask after exponentiation but before the normalization step of SoftMax.\n\n**Part 2: Information Leakage with Incorrect or Finite Masks**\n\nIn a practical floating-point implementation, we cannot represent $-\\infty$. Instead, we approximate $\\log 0$ with a large-magnitude negative number, $-C$, where $C$ is a large positive constant (e.g., $10^9$).\n\nThe additive mask is applied as follows: for a disallowed connection ($M_{ij}=0$), we add $-C$ to the logit $Z_{ij}$. The corresponding term in the SoftMax calculation becomes:\n$$\ne^{Z_{ij} - C} = e^{Z_{ij}} e^{-C}\n$$\nFor a large $C$, the value $e^{-C}$ is extremely small but crucially non-zero. This means that disallowed positions in the attention matrix will be assigned a tiny, non-zero probability mass, whereas the ideal mathematical formulation would assign them exactly zero probability.\n\nInformation leakage occurs when this non-zero probability is combined with an **incorrect mask**. A correct causal mask for a decoder, $M^{\\mathrm{causal}}$, ensures that for any query position $i$, the attention weights $P_{ij}$ are zero for all key positions $j > i$. This prevents a token from \"seeing\" future tokens.\n\nThe given incorrect mask, $M^{\\mathrm{off}}$, is defined such that $M^{\\mathrm{off}}_{ij} = 1$ if $j \\le i+1$. This allows a query at position $i$ to attend to key/value at position $i+1$, which is one step into the future. For example, for $i=2$, it can attend to $j=0, 1, 2, 3$. Because it can attend to position $j=3$, if the value vector $V_3$ is changed, the output at position $Y_2$ will also change.\n\nThe attention output for query $i$ is a weighted sum:\n$$\nY_i = \\sum_{j=1}^L P_{ij} V_j\n$$\nIf $M^{\\mathrm{off}}$ is used, the attention probability $P_{2,3}$ will be non-zero. Let's compare the output $Y_2$ using the original values $V$ with the output $Y'_2$ using the modified values $V^{\\mathrm{mod}}$. The change in the output at position $2$ is:\n$$\nY'_2 - Y_2 = \\sum_{j=1}^L P_{2,j} V^{\\mathrm{mod}}_j - \\sum_{j=1}^L P_{2,j} V_j = \\sum_{j=1}^L P_{2,j} (V^{\\mathrm{mod}}_j - V_j)\n$$\nSince $V^{\\mathrm{mod}}$ only differs from $V$ in the last row (index $3$), this simplifies to:\n$$\nY'_2 - Y_2 = P_{2,3} (V^{\\mathrm{mod}}_3 - V_3)\n$$\nAs $M^{\\mathrm{off}}$ allows attention from $i=2$ to $j=3$, $P_{2,3}$ is non-zero. The problem defines a very large change in $V_3$, so $(V^{\\mathrm{mod}}_3 - V_3)$ is large. The product, which represents the change in the output for a \"past\" token, becomes significant. This is a direct demonstration of information leaking from the future (the value at step $3$) to the past (the output at step $2$).\n\nWith a correct causal mask, $M^{\\mathrm{causal}}$, the weight $P_{2,3}$ would be exactly zero, and thus $Y_2$ would be completely unaffected by any changes to $V_3$.\n\nThe magnitude of the constant $C$ in the finite approximation determines how close the masked probabilities are to zero. A smaller $C$ (like $50$) results in $e^{-50}$ being a larger number than $e^{-10^9}$, leading to a greater deviation from the ideal zero-probability case and more pronounced \"leakage\" or numerical error. This is what Case $4$ demonstrates.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs all calculations for the four test cases regarding\n    masked attention in Transformers.\n    \"\"\"\n\n    # --- Given Constants and Matrices ---\n    L = 4\n    dk = 3\n    dv = 2\n\n    Q = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    K = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    V = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]\n    ], dtype=np.float64)\n\n    V_mod = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [700, 800]\n    ], dtype=np.float64)\n\n    # M_causal: j <= i\n    M_causal = np.tril(np.ones((L, L), dtype=np.float64))\n\n    # M_off: j <= i + 1\n    i_indices = np.arange(L).reshape(-1, 1)\n    j_indices = np.arange(L).reshape(1, -1)\n    M_off = (j_indices <= i_indices + 1).astype(np.float64)\n\n    # Matrices for smaller test cases\n    Z_a = np.array([\n        [0.2, -0.1, 0.4],\n        [1.0, -1.0, 0.0]\n    ], dtype=np.float64)\n\n    M_a = np.array([\n        [1, 0, 1],\n        [0, 1, 1]\n    ], dtype=np.float64)\n\n    # --- Helper Function for Attention Calculation ---\n    def compute_attention(q_mat, k_mat, v_mat, mask, C=1e9):\n        \"\"\"\n        Computes Scaled Dot-Product Attention with an additive mask.\n        Uses a numerically stable softmax.\n        \"\"\"\n        dk_val = q_mat.shape[1]\n        logits = (q_mat @ k_mat.T) / np.sqrt(dk_val)\n        \n        # Apply the additive mask\n        additive_mask = (mask - 1) * C\n        masked_logits = logits + additive_mask\n        \n        # Numerically stable softmax\n        shifted_logits = masked_logits - np.max(masked_logits, axis=1, keepdims=True)\n        attention_weights = np.exp(shifted_logits)\n        attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n        \n        output = attention_weights @ v_mat\n        return output\n\n    # --- Test Cases ---\n    results = []\n\n    # Case 1: Equivalence of masking with C = 1e9\n    # Ideal multiplicative form\n    exp_Z_a = np.exp(Z_a)\n    numerator_ideal = exp_Z_a * M_a\n    denominator_ideal = np.sum(numerator_ideal, axis=1, keepdims=True)\n    # Handle cases where a whole row is masked, to avoid division by zero\n    denominator_ideal[denominator_ideal == 0] = 1.0 \n    P_ideal = numerator_ideal / denominator_ideal\n    \n    # Additive approximation with C = 1e9\n    C1 = 1e9\n    additive_mask_1 = (M_a - 1) * C1\n    Z_masked_1 = Z_a + additive_mask_1\n    # Stable softmax for additive form\n    shifted_Z_masked_1 = Z_masked_1 - np.max(Z_masked_1, axis=1, keepdims=True)\n    exp_Z_masked_1 = np.exp(shifted_Z_masked_1)\n    P_approx_1 = exp_Z_masked_1 / np.sum(exp_Z_masked_1, axis=1, keepdims=True)\n    \n    case1_diff = np.max(np.abs(P_ideal - P_approx_1))\n    results.append(case1_diff)\n\n    # Case 2: No leakage with correct causal mask\n    Y1 = compute_attention(Q, K, V, M_causal)\n    Y2 = compute_attention(Q, K, V_mod, M_causal)\n    case2_unchanged = bool(np.allclose(Y1[:3, :], Y2[:3, :], atol=1e-12, rtol=0))\n    results.append(case2_unchanged)\n\n    # Case 3: Leakage with incorrect mask\n    Y3 = compute_attention(Q, K, V, M_off)\n    Y4 = compute_attention(Q, K, V_mod, M_off)\n    diff = np.abs(Y3[:3, :] - Y4[:3, :])\n    case3_leakage_detected = bool(np.any(diff > 1e-6))\n    results.append(case3_leakage_detected)\n\n    # Case 4: Finite-mask boundary effect with C = 50\n    C4 = 50.0\n    additive_mask_4 = (M_a - 1) * C4\n    Z_masked_4 = Z_a + additive_mask_4\n    # Stable softmax\n    shifted_Z_masked_4 = Z_masked_4 - np.max(Z_masked_4, axis=1, keepdims=True)\n    exp_Z_masked_4 = np.exp(shifted_Z_masked_4)\n    P_approx_4 = exp_Z_masked_4 / np.sum(exp_Z_masked_4, axis=1, keepdims=True)\n    \n    case4_diff = np.max(np.abs(P_ideal - P_approx_4))\n    results.append(case4_diff)\n    \n    # Final print statement\n    # Explicitly format booleans to be lowercase 'true'/'false' if needed, but str() is fine.\n    print(f\"[{results[0]},{str(results[1])},{str(results[2])},{results[3]}]\")\n\nsolve()\n```", "id": "3193602"}, {"introduction": "The scaled dot-product formulation of attention, while powerful, has properties that can lead to unexpected behavior. This exercise explores a robustness issue known as \"attention hijacking,\" where an input token with an unusually large vector norm can disproportionately capture the attention of a query, regardless of its semantic relevance. By constructing this adversarial scenario and implementing two common mitigation strategies—norm clipping and cosine similarity—you will gain a deeper intuition for the dynamics of dot products within the softmax function and the importance of ensuring numerical stability [@problem_id:3193536].", "problem": "Consider the Scaled Dot-Product Attention (SDPA) mechanism used in the transformer architecture. Given a set of Query (Q), Key (K), and Value (V) vectors, attention weights are formed by applying a softmax function to a set of attention logits. The attention logits arise from inner products between the query and each key, scaled by the square root of the key dimensionality. The softmax function converts arbitrary real-valued scores to a probability distribution over keys. Assume the following well-tested definitions: the inner product between vectors is the sum of element-wise products, the Euclidean norm is the square root of the sum of squared elements, and the softmax function maps a vector of real numbers to a vector of non-negative real numbers summing to one by exponentiating each element and normalizing by the sum of exponentials.\n\nAdversarial “attention hijacking” can occur when an attacker adds a new key token with unusually large norm, thereby producing a large inner product with the query and dominating the softmax distribution. This problem asks you to demonstrate, through computation, how such an attack can dominate attention and to implement two mitigation strategies: norm clipping of keys and cosine similarity normalization of logits.\n\nYou must implement the following from first principles:\n- Compute scaled dot-product attention logits for a single query $Q$ and a set of keys $\\{K_i\\}$ in dimension $d_k$, scale each logit by $1/\\sqrt{d_k}$, and apply the softmax function to obtain attention weights.\n- Demonstrate adversarial hijacking by appending a single attack key $K_{\\text{attack}}$ whose direction is specified and whose norm is set to a large value.\n- Implement two mitigations:\n    1. Norm clipping: replace each key $K_i$ by $\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i$ for a clip cap $c$, applied to all keys including $K_{\\text{attack}}$.\n    2. Cosine similarity normalization: replace each logit by $\\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$ for a small $\\varepsilon > 0$ before applying the softmax function.\n\nFor each test case below, compute four quantities:\n- The attack token’s attention weight under baseline (attack token norm set to $1$).\n- The attack token’s attention weight under adversarial conditions (attack token norm set to a large value).\n- The attack token’s attention weight with norm clipping applied to adversarial keys (clip cap $c$).\n- The attack token’s attention weight with cosine similarity normalization applied to adversarial keys (with a small $\\varepsilon$ to avoid division by zero).\n\nYour program must use the following test suite. In each case, the attack token is appended as the last key. Use the provided vectors exactly as given.\n\nTest Case $1$ (happy path: aligned query and attack direction):\n- $d_k = 4$\n- $Q = [0.9, 0.1, 0.0, 0.0]$\n- Normal keys: $K_1 = [1, 0, 0, 0]$, $K_2 = [0, 1, 0, 0]$, $K_3 = [0, 0, 1, 0]$\n- Attack direction: $K_{\\text{dir}} = [1, 0, 0, 0]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $100$\n- Clip cap: $2$\n\nTest Case $2$ (boundary case: zero query vector):\n- $d_k = 4$\n- $Q = [0.0, 0.0, 0.0, 0.0]$\n- Normal keys: $K_1 = [1, 0, 0, 0]$, $K_2 = [0, 1, 0, 0]$, $K_3 = [0, 0, 1, 0]$\n- Attack direction: $K_{\\text{dir}} = [0, 0, 0, 1]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $100$\n- Clip cap: $2$\n\nTest Case $3$ (orthogonal query and attack direction):\n- $d_k = 4$\n- $Q = [0.0, 1.0, 0.0, 0.0]$\n- Normal keys: $K_1 = [0, 1, 0, 0]$, $K_2 = [0, 0, 1, 0]$, $K_3 = [0, 0, 0, 1]$\n- Attack direction: $K_{\\text{dir}} = [1, 0, 0, 0]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $100$\n- Clip cap: $2$\n\nTest Case $4$ (extreme adversarial norm):\n- $d_k = 8$\n- $Q = [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0]$\n- Normal keys: $K_1 = [0, 1, 0, 0, 0, 0, 0, 0]$, $K_2 = [0, 0, 1, 0, 0, 0, 0, 0]$, $K_3 = [0, 0, 0, 1, 0, 0, 0, 0]$\n- Attack direction: $K_{\\text{dir}} = [1, 0, 0, 0, 0, 0, 0, 0]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $10^6$\n- Clip cap: $10$\n\nYour program must compute, for each test case, a list of four floating-point numbers in the order described above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the four-number list for the corresponding test case. For example: $[[w_{1,\\text{base}}, w_{1,\\text{adv}}, w_{1,\\text{clip}}, w_{1,\\text{cos}}],[w_{2,\\text{base}}, w_{2,\\text{adv}}, w_{2,\\text{clip}}, w_{2,\\text{cos}}],\\ldots]$.", "solution": "The problem requires an implementation and demonstration of an adversarial attack on the Scaled Dot-Product Attention (SDPA) mechanism, along with two mitigation strategies. I will first formalize the operations, then outline the computational steps for each part of the problem.\n\n### 1. Fundamental Concepts\n\n**Scaled Dot-Product Attention:**\nThe attention mechanism computes a distribution of weights, $\\alpha_i$, over a set of key-value pairs, based on a query vector $Q$. For a single query $Q \\in \\mathbb{R}^{d_k}$ and a set of keys $\\{K_1, K_2, \\ldots, K_N\\}$, where each $K_i \\in \\mathbb{R}^{d_k}$, the attention logits $e_i$ are computed as:\n$$\ne_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}\n$$\nHere, $d_k$ is the dimensionality of the key vectors. The attention weights $\\alpha_i$ are then obtained by applying the softmax function to the vector of logits:\n$$\n\\alpha_i = \\text{softmax}(e)_i = \\frac{\\exp(e_i)}{\\sum_{j=1}^{N} \\exp(e_j)}\n$$\nThe output of the attention layer is a weighted sum of value vectors, $\\sum_i \\alpha_i V_i$, which is not part of this problem.\n\n**Adversarial Hijacking:**\nThe inner product $Q^\\top K_i$ is directly proportional to the norm of the key vector, $\\|K_i\\|$. An attacker can exploit this by introducing an adversarial key, $K_{\\text{attack}}$, with an exceptionally large norm. If $K_{\\text{attack}}$ is not orthogonal to $Q$, the resulting dot product $Q^\\top K_{\\text{attack}}$ will be large, leading to a large logit $e_{\\text{attack}}$. Consequently, after exponentiation, $\\exp(e_{\\text{attack}})$ will dominate the sum in the softmax denominator, causing the attention weight $\\alpha_{\\text{attack}}$ to approach $1$. This effectively forces the model to attend almost exclusively to the adversarial token, \"hijacking\" the attention mechanism.\n\n### 2. Mitigation Strategies\n\nTwo mitigation strategies are to be implemented.\n\n**Mitigation 1: Norm Clipping**\nThis method enforces a maximum norm, $c$, on all key vectors. Each key $K_i$ is replaced by a clipped version $\\tilde{K}_i$:\n$$\n\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i\n$$\nIf a key's norm $\\|K_i\\|$ is already less than or equal to the clip cap $c$, it remains unchanged. If $\\|K_i\\| > c$, it is scaled down such that its new norm becomes exactly $c$. This prevents any single key from having an arbitrarily large norm and thus an excessively large influence on the logits. For this calculation, a potential division by zero for a zero-norm key is noted, but not present in the test data.\n\n**Mitigation 2: Cosine Similarity Normalization**\nThis strategy replaces the scaled dot-product logit with the cosine similarity between the query and key vectors. The logit $e_i$ is redefined as:\n$$\ne'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}\n$$\nCosine similarity is inherently normalized by the magnitudes of the vectors, making it insensitive to their norms. The value of the resulting logit is bounded in $[-1, 1]$. The small constant $\\varepsilon > 0$ is added for numerical stability to prevent division by zero if either $Q$ or $K_i$ is a zero vector. For this implementation, a standard value of $\\varepsilon = 10^{-8}$ will be used. Note that this formulation replaces the entire scaled dot-product, including the $1/\\sqrt{d_k}$ scaling factor.\n\n### 3. Computational Procedure\n\nFor each test case, we must compute the attention weight of the attack token under four distinct scenarios. The attack key, $K_{\\text{attack}}$, is constructed by scaling a given direction vector $K_{\\text{dir}}$ to a specified norm. If $\\|K_{\\text{dir}}\\| \\neq 0$, then $K_{\\text{attack}} = (\\text{norm} / \\|K_{\\text{dir}}\\| ) \\cdot K_{\\text{dir}}$. The attack key is always appended as the last key in the sequence.\n\nLet the set of normal keys be $\\{K_1, \\ldots, K_{N-1}\\}$ and the attack key be $K_N = K_{\\text{attack}}$.\n\n**A. Baseline Attention:**\n1. Construct $K_{\\text{attack}}$ using the specified `baseline_attack_norm`.\n2. Form the complete set of keys $\\{K_1, \\ldots, K_{N-1}, K_{\\text{attack}}\\}$.\n3. Compute the scaled dot-product logits $e_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}$ for all $i=1, \\ldots, N$.\n4. Apply the softmax function to obtain weights $\\{\\alpha_1, \\ldots, \\alpha_N\\}$.\n5. The result is $\\alpha_N$.\n\n**B. Adversarial Attention:**\n1. Construct $K_{\\text{attack}}$ using the `adversarial_attack_norm`.\n2. Form the complete set of keys using this new adversarial key.\n3. Repeat steps 3-5 from the baseline procedure.\n4. The result is the new $\\alpha_N$.\n\n**C. Norm Clipping Mitigation:**\n1. Use the set of keys from the adversarial scenario, including the high-norm $K_{\\text{attack}}$.\n2. Apply the norm clipping formula to every key $K_i$ to produce a set of clipped keys $\\{\\tilde{K}_1, \\ldots, \\tilde{K}_N\\}$, using the specified clip cap $c$.\n3. Compute the scaled dot-product logits using these clipped keys: $\\tilde{e}_i = \\frac{Q^\\top \\tilde{K}_i}{\\sqrt{d_k}}$.\n4. Apply the softmax function to obtain weights $\\{\\tilde{\\alpha}_1, \\ldots, \\tilde{\\alpha}_N\\}$.\n5. The result is $\\tilde{\\alpha}_N$.\n\n**D. Cosine Similarity Mitigation:**\n1. Use the set of keys from the adversarial scenario.\n2. Compute the logits using the cosine similarity formula for each key: $e'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$.\n3. Apply the softmax function to these new logits $\\{e'_1, \\ldots, e'_N\\}$ to get weights $\\{\\alpha'_1, \\ldots, \\alpha'_N\\}$.\n4. The result is $\\alpha'_N$.\n\nThese four values, computed in order, constitute the solution for a single test case. The final output aggregates the results from all test cases as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes attentional hijacking and mitigation effects for SDPA.\n    \"\"\"\n    # A small constant for numerical stability in cosine similarity.\n    EPSILON = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d_k\": 4,\n            \"Q\": [0.9, 0.1, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [0, 0, 0, 1],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 1.0, 0.0, 0.0],\n            \"normal_keys\": [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 8,\n            \"Q\": [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [\n                [0, 1, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0],\n            ],\n            \"attack_direction\": [1, 0, 0, 0, 0, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 1e6,\n            \"clip_cap\": 10,\n        },\n    ]\n\n    def softmax(x):\n        \"\"\"Computes softmax of vector x for numerical stability.\"\"\"\n        if x.size == 0:\n            return np.array([])\n        e_x = np.exp(x - np.max(x))\n        return e_x / np.sum(e_x)\n\n    def create_attack_key(direction, norm):\n        \"\"\"Creates a key vector from a direction and a norm.\"\"\"\n        direction_vec = np.array(direction, dtype=float)\n        dir_norm = np.linalg.norm(direction_vec)\n        if dir_norm == 0:\n            return np.zeros_like(direction_vec)\n        return norm * (direction_vec / dir_norm)\n\n    def calculate_sdpa_weights(Q, keys, d_k):\n        \"\"\"Computes weights using scaled dot-product attention.\"\"\"\n        logits = np.array([np.dot(Q, k) for k in keys]) / np.sqrt(d_k)\n        return softmax(logits)\n\n    def calculate_cosine_sim_weights(Q, keys):\n        \"\"\"Computes weights using cosine similarity logits.\"\"\"\n        Q_norm = np.linalg.norm(Q)\n        logits = []\n        for k in keys:\n            k_norm = np.linalg.norm(k)\n            dot_product = np.dot(Q, k)\n            logit = dot_product / (Q_norm * k_norm + EPSILON)\n            logits.append(logit)\n        return softmax(np.array(logits))\n\n    results = []\n    for case in test_cases:\n        Q_vec = np.array(case[\"Q\"], dtype=float)\n        normal_keys_vecs = [np.array(k, dtype=float) for k in case[\"normal_keys\"]]\n        d_k = case[\"d_k\"]\n        clip_cap = case[\"clip_cap\"]\n\n        # 1. Baseline calculation\n        K_attack_base = create_attack_key(case[\"attack_direction\"], case[\"baseline_attack_norm\"])\n        all_keys_base = normal_keys_vecs + [K_attack_base]\n        weights_base = calculate_sdpa_weights(Q_vec, all_keys_base, d_k)\n        w_base = weights_base[-1]\n\n        # 2. Adversarial calculation\n        K_attack_adv = create_attack_key(case[\"attack_direction\"], case[\"adversarial_attack_norm\"])\n        all_keys_adv = normal_keys_vecs + [K_attack_adv]\n        weights_adv = calculate_sdpa_weights(Q_vec, all_keys_adv, d_k)\n        w_adv = weights_adv[-1]\n\n        # 3. Norm clipping mitigation\n        all_keys_clipped = []\n        for k in all_keys_adv:\n            k_norm = np.linalg.norm(k)\n            # Use min(1, c/||K||) * K formulation\n            scale_factor = min(1.0, clip_cap / k_norm if k_norm > 0 else 1.0)\n            all_keys_clipped.append(k * scale_factor)\n        weights_clip = calculate_sdpa_weights(Q_vec, all_keys_clipped, d_k)\n        w_clip = weights_clip[-1]\n\n        # 4. Cosine similarity mitigation\n        weights_cos = calculate_cosine_sim_weights(Q_vec, all_keys_adv)\n        w_cos = weights_cos[-1]\n\n        results.append([w_base, w_adv, w_clip, w_cos])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3193536"}, {"introduction": "A major computational bottleneck in the standard attention mechanism is the need to materialize a full $N \\times M$ attention score matrix, where $N$ and $M$ are sequence lengths. This becomes prohibitive for long sequences. This advanced practice introduces the core algorithmic insight behind memory-efficient attention methods like FlashAttention, which reorder the computation to avoid this bottleneck. You will derive and implement a blockwise, \"online\" softmax computation that produces a numerically identical output to the standard method, providing a foundational understanding of how modern Transformers achieve remarkable efficiency [@problem_id:3193562].", "problem": "Consider Scaled Dot-Product Attention (SDPA) in the Transformer architecture, where, for query matrix $Q \\in \\mathbb{R}^{N \\times d}$, key matrix $K \\in \\mathbb{R}^{M \\times d}$, and value matrix $V \\in \\mathbb{R}^{M \\times d_v}$, the attention scores matrix is $S = \\frac{QK^\\top}{\\sqrt{d}}$. Attention probabilities are computed by applying a row-wise softmax to $S$, and the output is the matrix $O = \\operatorname{softmax}(S)V$. FlashAttention is an algorithmic technique that reorders computation to reduce memory footprint by computing $QK^\\top$ in blocks and performing online normalization so that full-resolution attention probabilities never need to be materialized. In this problem, you will rigorously derive why blockwise softmax accumulation preserves numerical equivalence, and you will implement a Central Processing Unit (CPU) toy version that demonstrates chunked $QK^\\top$ and online normalization.\n\nStarting from only fundamental bases suitable for deep learning at the intermediate undergraduate level:\n- The definition of softmax applied row-wise: for a row vector $x \\in \\mathbb{R}^{M}$, $\\operatorname{softmax}(x)_j = \\frac{e^{x_j}}{\\sum_{k=1}^{M} e^{x_k}}$ for $j \\in \\{1,\\dots,M\\}$.\n- The distributive and associative properties of addition and multiplication over real numbers and the rule $e^{a+b} = e^{a} e^{b}$ for all real $a$ and $b$.\n- The identity that for any real $m$, $e^{x} = e^{x-m} e^{m}$.\n\nYou must:\n1. Derive, from first principles, why processing attention scores in blocks along the key dimension with a running row-wise maximum and running sums leads to exactly the same result as the full softmax, assuming exact real arithmetic. Your derivation must use only the above core facts and definitions, and it must explain how online normalization can be performed without ever storing the full attention matrix.\n2. Provide a robust plan to handle masking, including causal masking, where disallowed positions have scores treated as $-\\infty$ so their softmax contributions are exactly $0$.\n3. Implement a complete and runnable program that:\n   - Computes reference SDPA by forming the full scores $S$, applying a numerically stable row-wise softmax with masking, and then multiplying by $V$.\n   - Computes blockwise SDPA by iterating over contiguous key-value chunks of size $c$, updating a running per-row maximum, a running per-row sum of exponentiated scores (after stabilization by the current maximum), and a running per-row accumulation of the weighted values, then normalizes at the end. This must use only CPU and must not materialize the full attention probabilities at any point.\n   - Demonstrates equivalence by producing, for each test case, the maximum absolute difference between the reference output and the blockwise output as a floating-point number.\n\nUse the following test suite of parameter values. All matrices must be generated with a fixed random seed per test case, using independent standard normal entries, and then adjusted exactly as described. The angle unit does not apply. There are no physical units to report.\n\n- Test case $1$ (general \"happy path\"):\n  - $N = 5$, $M = 7$, $d = 4$, $d_v = 3$, chunk size $c = 3$, no mask.\n- Test case $2$ (boundary condition: single block equals full computation):\n  - $N = 5$, $M = 7$, $d = 4$, $d_v = 3$, chunk size $c = 7$, no mask.\n- Test case $3$ (edge case: elementwise blocks with causal mask):\n  - $N = 4$, $M = 4$, $d = 8$, $d_v = 2$, chunk size $c = 1$, causal mask where, for query row index $i$, only keys with index $j \\le i$ are allowed.\n- Test case $4$ (stress numerical stability: large score magnitudes):\n  - $N = 3$, $M = 6$, $d = 16$, $d_v = 5$, chunk size $c = 2$, no mask. Multiply the query matrix $Q$ by the scalar $30$ before computing attention to magnify dot products.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one floating-point number per test case representing the maximum absolute difference between the reference and blockwise outputs (for example, \"[$\\text{result}_1$, $\\text{result}_2$, $\\text{result}_3$, $\\text{result}_4$]\"). The actual printed string must follow the pattern \"[result1,result2,result3,result4]\".\n\nYour derivation and implementation must ensure scientific realism and numerical soundness. Do not provide shortcut formulas in the problem statement; instead, build the argument from the listed foundational definitions and properties. The final numerical answers must be floats, and your code must be self-contained and require no user input or external files.", "solution": "The problem requires a derivation of the online update rule for blockwise softmax computation, a plan for incorporating masking, and an implementation demonstrating its numerical equivalence to standard Scaled Dot-Product Attention (SDPA).\n\n### 1. Derivation of Blockwise Softmax with Online Normalization\n\nThe core task is to compute the output of an attention layer without materializing the full $N \\times M$ attention score matrix $S$. We focus on a single row of the output matrix $O$, which corresponds to a single query vector. For a query $q_i$ (the $i$-th row of $Q$), the corresponding scores against all key vectors are given by the row vector $s_i \\in \\mathbb{R}^M$, where the $j$-th element is $s_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}$. The $i$-th row of the output matrix $O$, denoted $o_i \\in \\mathbb{R}^{d_v}$, is a weighted sum of the value vectors $v_j \\in \\mathbb{R}^{d_v}$ (rows of $V$).\n\nStarting from the provided definition of row-wise softmax, the output for row $i$ is:\n$$ o_i = \\sum_{j=1}^{M} \\operatorname{softmax}(s_i)_j v_j = \\sum_{j=1}^{M} \\frac{\\exp(s_{ij})}{\\sum_{k=1}^{M} \\exp(s_{ik})} v_j $$\nUsing the distributive property, we can rewrite this as:\n$$ o_i = \\frac{\\sum_{j=1}^{M} \\exp(s_{ij}) v_j}{\\sum_{k=1}^{M} \\exp(s_{ik})} $$\n\nTo prevent numerical overflow from large positive values of $s_{ij}$, we use the provided identity $e^x = e^{x-m}e^m$. Let $m_i = \\max_{j=1, \\dots, M} s_{ij}$. Multiplying the numerator and denominator by $e^{-m_i}$ yields:\n$$ o_i = \\frac{\\sum_{j=1}^{M} \\exp(s_{ij} - m_i) v_j}{\\sum_{k=1}^{M} \\exp(s_{ik} - m_i)} $$\nIn this form, the arguments to $\\exp(\\cdot)$ are less than or equal to $0$, preventing overflow and improving numerical stability.\n\nThe core idea of FlashAttention is to compute this expression in blocks without ever storing the full vector $s_i$. We partition the key/value indices $\\{1, \\dots, M\\}$ into $T$ contiguous blocks $B_1, B_2, \\dots, B_T$. Using the associativity of addition, the numerator and denominator can be written as sums over these blocks:\n$$ o_i = \\frac{\\sum_{t=1}^{T} \\left( \\sum_{j \\in B_t} \\exp(s_{ij}) v_j \\right)}{\\sum_{t=1}^{T} \\left( \\sum_{k \\in B_t} \\exp(s_{ik}) \\right)} $$\n\nWe can compute this iteratively. Let's define the state of the computation for query $i$ after processing the first $t$ blocks. This state consists of three quantities:\n1.  $m_i^{(t)}$: The running maximum of scores encountered so far, i.e., $m_i^{(t)} = \\max_{j \\in B_1 \\cup \\dots \\cup B_t} s_{ij}$.\n2.  $\\mathcal{O}_i^{(t)}$: The running (stabilized) numerator, $\\mathcal{O}_i^{(t)} = \\sum_{j \\in B_1 \\cup \\dots \\cup B_t} \\exp(s_{ij} - m_i^{(t)}) v_j$.\n3.  $l_i^{(t)}$: The running (stabilized) denominator, $l_i^{(t)} = \\sum_{j \\in B_1 \\cup \\dots \\cup B_t} \\exp(s_{ij} - m_i^{(t)})$.\n\nThe final output for query $i$ after all $T$ blocks are processed will be $o_i = \\mathcal{O}_i^{(T)} / l_i^{(T)}$.\n\nLet's derive the update rule to transition from state $(t-1)$ to state $(t)$. Suppose we have computed $m_i^{(t-1)}$, $\\mathcal{O}_i^{(t-1)}$, and $l_i^{(t-1)}$ for the first $t-1$ blocks. Now, we process block $B_t$.\n\nFirst, we compute the scores for the current block, $s_{ij}$ for all $j \\in B_t$. Let $m_{i,t}^{\\text{block}} = \\max_{j \\in B_t} s_{ij}$.\nThe new running maximum is $m_i^{(t)} = \\max(m_i^{(t-1)}, m_{i,t}^{\\text{block}})$.\n\nNext, we calculate the numerator and denominator contributions from the current block $B_t$, stabilized by the new maximum $m_i^{(t)}$:\n- Block numerator: $\\mathcal{O}_{i,t}^{\\text{block}} = \\sum_{j \\in B_t} \\exp(s_{ij} - m_i^{(t)}) v_j$.\n- Block denominator: $l_{i,t}^{\\text{block}} = \\sum_{j \\in B_t} \\exp(s_{ij} - m_i^{(t)})$.\n\nThe new total stabilized numerator $\\mathcal{O}_i^{(t)}$ is the sum of the contribution from the previous $t-1$ blocks and the new block $t$. The previous running numerator $\\mathcal{O}_i^{(t-1)}$ was stabilized with the old maximum $m_i^{(t-1)}$, so it must be rescaled to the new maximum $m_i^{(t)}$.\n\nUsing the identities $e^{a+b} = e^a e^b$ and associativity:\n$$ \\mathcal{O}_i^{(t-1)} = \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t-1)}) v_j $$\nTo rescale this sum, we multiply by a factor of $1 = \\exp(m_i^{(t-1)} - m_i^{(t)}) \\exp(m_i^{(t)} - m_i^{(t-1)})$:\n$$ \\mathcal{O}_i^{(t-1)} \\cdot \\exp(m_i^{(t-1)} - m_i^{(t)}) = \\left( \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t-1)}) v_j \\right) \\exp(m_i^{(t-1)} - m_i^{(t)}) $$\n$$ = \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t-1)} + m_i^{(t-1)} - m_i^{(t)}) v_j = \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t)}) v_j $$\nThis is the previous numerator, correctly stabilized by the new maximum $m_i^{(t)}$.\n\nThus, the update rule for the running numerator is:\n$$ \\mathcal{O}_i^{(t)} = \\mathcal{O}_i^{(t-1)} \\cdot \\exp(m_i^{(t-1)} - m_i^{(t)}) + \\mathcal{O}_{i,t}^{\\text{block}} $$\nSimilarly, the update rule for the running denominator is:\n$$ l_i^{(t)} = l_i^{(t-1)} \\cdot \\exp(m_i^{(t-1)} - m_i^{(t)}) + l_{i,t}^{\\text{block}} $$\n\nThese update rules allow for a single pass over the key-value pairs. We initialize with $m_i^{(0)} = -\\infty$, $\\mathcal{O}_i^{(0)} = \\vec{0}$, and $l_i^{(0)} = 0$. After iterating through all blocks $t=1, \\dots, T$, the final output for query $i$ is calculated as $o_i = \\mathcal{O}_i^{(T)} / l_i^{(T)}$. This process computes the exact same result as the full softmax (assuming exact arithmetic) without ever storing the full $N \\times M$ score matrix.\n\n### 2. Plan for Handling Masking\n\nMasking is used to prevent attention to certain key positions. This is commonly implemented by setting the scores of disallowed positions to $-\\infty$. Since $\\lim_{x \\to -\\infty} \\exp(x) = 0$, these positions have zero weight in the softmax computation and thus do not contribute to the output.\n\nThis principle integrates seamlessly into the blockwise algorithm. When computing the scores for a block $B_t$, for any disallowed query-key pair $(i,j)$, we simply set $s_{ij} = -\\infty$.\n\n- **General Masking**: A boolean mask can specify which $(i,j)$ pairs are disallowed. When computing `S_block` for keys in block $B_t$, the mask for the corresponding columns is applied, setting the appropriate scores to $-\\infty$.\n- **Causal Masking**: In causal attention (e.g., for decoder-only Transformers), a query at position $i$ can only attend to keys at positions $j \\le i$. When computing attention for query $i$ over a block of keys with indices from $j_{\\text{start}}$ to $j_{\\text{end}}$, we set $s_{ij} = -\\infty$ for all $j$ in the block such that $j > i$.\n\nThese masked scores are then processed by the standard online update mechanism. Since $s_{ij} = -\\infty$ will never be the maximum score (unless all scores in the block are $-\\infty$), it will not affect the running maximum $m_i^{(t)}$ inappropriately. Its contribution to the running sums $\\mathcal{O}_i^{(t)}$ and $l_i^{(t)}$ will be $\\exp(-\\infty) = 0$, correctly excluding it from the final average.", "answer": "```python\nimport numpy as np\n\ndef sdpa_reference(Q, K, V, mask_type):\n    \"\"\"\n    Computes standard Scaled Dot-Product Attention with full matrix materialization.\n    \"\"\"\n    N, d = Q.shape\n    M, _ = K.shape\n    \n    # 1. Compute scores\n    scores = (Q @ K.T) / np.sqrt(d)\n    \n    # 2. Apply mask\n    if mask_type == \"causal\":\n        # Create a mask where entries for j > i are -inf\n        mask = np.full((N, M), -np.inf)\n        # tril_indices provides lower triangle including diagonal\n        row_indices, col_indices = np.tril_indices(min(N, M))\n        # Where N != M, need to be careful with indexing\n        if N <= M:\n            mask[row_indices, col_indices] = 0.0\n        else: # N > M, all keys are valid for queries i >= M-1\n            mask[:M, :][row_indices, col_indices] = 0.0\n        \n        scores += mask\n\n    # 3. Numerically stable softmax\n    # Subtract max for stability\n    scores_max = np.max(scores, axis=1, keepdims=True)\n    # When a row is all -inf, max is -inf. exp(-inf - (-inf)) = exp(0) = 1.\n    # We should avoid this by replacing -inf max with 0.\n    scores_max = np.where(np.isneginf(scores_max), 0, scores_max)\n    \n    exp_scores = np.exp(scores - scores_max)\n    \n    # Handle fully masked rows where sum would be 0\n    # exp_scores will be all 0s for such rows.\n    sum_exp_scores = np.sum(exp_scores, axis=1, keepdims=True)\n    \n    attention_probs = np.divide(exp_scores, sum_exp_scores, where=sum_exp_scores!=0)\n    \n    # 4. Compute output\n    output = attention_probs @ V\n    return output\n\ndef sdpa_blockwise(Q, K, V, c, mask_type):\n    \"\"\"\n    Computes Scaled Dot-Product Attention in blocks without materializing full score matrix.\n    \"\"\"\n    N, d = Q.shape\n    M, dv = V.shape\n    \n    # Initialize running statistics\n    output_acc = np.zeros((N, dv), dtype=np.float64)\n    log_sum_exp_acc = np.zeros(N, dtype=np.float64)\n    max_score_acc = np.full(N, -np.inf, dtype=np.float64)\n    \n    # Iterate over key/value blocks\n    for j_start in range(0, M, c):\n        j_end = min(j_start + c, M)\n        \n        # Get blocks\n        K_block = K[j_start:j_end, :]\n        V_block = V[j_start:j_end, :]\n        \n        # 1. Compute block scores\n        scores_block = (Q @ K_block.T) / np.sqrt(d)\n\n        # 2. Apply mask for the current block\n        if mask_type == \"causal\":\n            key_indices = np.arange(j_start, j_end)\n            query_indices = np.arange(N)[:, np.newaxis]\n            causal_mask = key_indices > query_indices\n            scores_block[causal_mask] = -np.inf\n        \n        # 3. Compute new running maximum\n        block_max_scores = np.max(scores_block, axis=1)\n        new_max_scores = np.maximum(max_score_acc, block_max_scores)\n        \n        # Replace -inf with 0 to prevent issues in exp(x - m) if all scores are -inf\n        new_max_scores_safe = np.where(np.isneginf(new_max_scores), 0, new_max_scores)\n\n        # 4. Rescale previous accumulated values\n        rescale_factor = np.exp(max_score_acc - new_max_scores_safe)\n        output_acc *= rescale_factor[:, np.newaxis]\n        log_sum_exp_acc *= rescale_factor\n        \n        # 5. Compute and accumulate current block's values\n        exp_scores_block = np.exp(scores_block - new_max_scores_safe[:, np.newaxis])\n        \n        # We must ensure that exp_scores_block is float64 for precision in accumulation\n        exp_scores_block = exp_scores_block.astype(np.float64)\n\n        block_log_sum_exp = np.sum(exp_scores_block, axis=1)\n        block_output = exp_scores_block @ V_block.astype(np.float64)\n        \n        log_sum_exp_acc += block_log_sum_exp\n        output_acc += block_output\n        \n        # 6. Update running max\n        max_score_acc = new_max_scores\n\n    # Final normalization\n    # Handle rows that are fully masked (log_sum_exp_acc will be 0)\n    final_output = np.divide(\n        output_acc, \n        log_sum_exp_acc[:, np.newaxis], \n        out=np.zeros_like(output_acc), \n        where=(log_sum_exp_acc[:, np.newaxis] != 0)\n    )\n\n    return final_output\n\ndef run_test_case(N, M, d, dv, c, mask_type, scale_Q, seed):\n    \"\"\"\n    Runs a single test case, comparing reference and blockwise implementations.\n    \"\"\"\n    np.random.seed(seed)\n    \n    Q = np.random.standard_normal((N, d)).astype(np.float64)\n    K = np.random.standard_normal((M, d)).astype(np.float64)\n    V = np.random.standard_normal((M, dv)).astype(np.float64)\n\n    if scale_Q != 1.0:\n        Q *= scale_Q\n    \n    # Compute using reference implementation\n    output_ref = sdpa_reference(Q, K, V, mask_type)\n    \n    # Compute using blockwise implementation\n    output_blockwise = sdpa_blockwise(Q, K, V, c, mask_type)\n    \n    # Calculate maximum absolute difference\n    max_abs_diff = np.max(np.abs(output_ref - output_blockwise))\n    \n    return max_abs_diff\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (N, M, d, dv, c, mask_type, scale_Q)\n        (5, 7, 4, 3, 3, None, 1.0),\n        (5, 7, 4, 3, 7, None, 1.0),\n        (4, 4, 8, 2, 1, \"causal\", 1.0),\n        (3, 6, 16, 5, 2, None, 30.0),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        N, M, d, dv, c, mask_type, scale_Q = params\n        diff = run_test_case(N, M, d, dv, c, mask_type, scale_Q, seed=i)\n        results.append(diff)\n    \n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3193562"}]}