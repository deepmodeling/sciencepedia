{"hands_on_practices": [{"introduction": "To build robust and stable neural networks, it's crucial to understand how they behave at initialization. This theoretical exercise guides you through a probabilistic analysis of additive and multiplicative attention scores. By deriving their expectation and variance, you will uncover a key reason why multiplicative attention mechanisms often require scaling to prevent unstable training dynamics [@problem_id:3097429].", "problem": "Consider a sequence-to-sequence model with attention, where the decoder state at time $t$ is a vector $s_t \\in \\mathbb{R}^{d_h}$ and an encoder hidden state for position $i$ is a vector $h_i \\in \\mathbb{R}^{d_h}$. The multiplicative (Luong) energy is defined as $e_i^{\\text{mult}} = s_t^{\\top} W h_i$ for a weight matrix $W \\in \\mathbb{R}^{d_h \\times d_h}$. The additive (Bahdanau) energy is defined as $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$ for a weight vector $v \\in \\mathbb{R}^{d_h}$ and matrices $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$. Assume the following random initialization and distributional assumptions:\n- The entries of $s_t$ are independent and identically distributed (i.i.d.) with $s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$ for all $a \\in \\{1, \\dots, d_h\\}$.\n- The entries of $h_i$ are i.i.d. with $h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$ for all $b \\in \\{1, \\dots, d_h\\}$.\n- The entries of $W$ are i.i.d. with $W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$ and are independent of $s_t$ and $h_i$.\n- The entries of $v$ are i.i.d. with $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$ and are independent of all other variables.\n- The entries of $W_s$ and $W_h$ are i.i.d. with $W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ and $W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$, independent of $s_t$, $h_i$, and $v$.\n- All random variables mentioned are mutually independent across indices and across parameter groups, and the hyperbolic tangent function $\\tanh(\\cdot)$ is applied elementwise.\nStarting from the fundamental definitions of expectation and variance, properties of independence, and symmetry of zero-mean Gaussian distributions, derive the expectation $\\mathbb{E}[e_i^{\\text{mult}}]$ and the variance $\\operatorname{Var}(e_i^{\\text{mult}})$ of the multiplicative energy. Then, specialize your variance expression to the case of Xavier-normal initialization for $W$, where $\\sigma_W^2 = 1/d_h$. Next, derive the expectation $\\mathbb{E}[e_i^{\\text{add}}]$ of the additive energy under the same independence and symmetry assumptions. Explicitly quantify how the variance of the multiplicative energy depends on the key dimensionality $d_h$ under the Xavier-normal choice. Your final answer must be a single analytic expression collecting the three quantities $\\mathbb{E}[e_i^{\\text{mult}}]$, $\\operatorname{Var}(e_i^{\\text{mult}})$ under $\\sigma_W^2 = 1/d_h$, and $\\mathbb{E}[e_i^{\\text{add}}]$, presented in that order. No numerical evaluation is required.", "solution": "The problem statement is first subjected to a rigorous validation procedure.\n\n### Step 1: Extract Givens\n- **Decoder state:** $s_t \\in \\mathbb{R}^{d_h}$\n- **Encoder state:** $h_i \\in \\mathbb{R}^{d_h}$\n- **Multiplicative energy:** $e_i^{\\text{mult}} = s_t^{\\top} W h_i$, where $W \\in \\mathbb{R}^{d_h \\times d_h}$\n- **Additive energy:** $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$, where $v \\in \\mathbb{R}^{d_h}$ and $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$\n- **Distributional assumptions:**\n  - Entries of $s_t$: $s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$ are independent and identically distributed (i.i.d.).\n  - Entries of $h_i$: $h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$ are i.i.d.\n  - Entries of $W$: $W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$ are i.i.d.\n  - Entries of $v$: $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$ are i.i.d.\n  - Entries of $W_s$: $W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ are i.i.d.\n  - Entries of $W_h$: $W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$ are i.i.d.\n- **Independence:** All specified random variables are mutually independent.\n- **Special condition:** For one part of the analysis, Xavier-normal initialization is assumed for $W$, meaning $\\sigma_W^2 = 1/d_h$.\n- **Objective:** Derive $\\mathbb{E}[e_i^{\\text{mult}}]$, $\\operatorname{Var}(e_i^{\\text{mult}})$ (and its specialized form for Xavier initialization), and $\\mathbb{E}[e_i^{\\text{add}}]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n- **Scientifically Grounded:** The problem is firmly situated within the theoretical analysis of neural networks, specifically attention mechanisms. The definitions for multiplicative (Luong) and additive (Bahdanau) attention are standard. The assumptions of zero-mean Gaussian priors for weights and states are common for analyzing network behavior at initialization. The problem is a standard exercise in probability theory and mathematical statistics applied to a well-established machine learning concept.\n- **Well-Posed:** The problem provides a complete set of definitions, distributional assumptions, and independence criteria necessary to derive the requested quantities (expectation and variance). The objectives are clear and mathematically unambiguous, guaranteeing a unique and meaningful solution.\n- **Objective:** The problem is stated using formal mathematical language, free from any subjective or biased phrasing.\n\nThe problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, ambiguity). All terms are well-defined, and the premises are mutually consistent.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will now be provided.\n\n### Solution Derivation\n\n**Part 1: Expectation of Multiplicative Energy $\\mathbb{E}[e_i^{\\text{mult}}]$**\n\nThe multiplicative energy is defined as $e_i^{\\text{mult}} = s_t^{\\top} W h_i$. This can be written in summation form:\n$$e_i^{\\text{mult}} = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}$$\nTo find its expectation, we apply the expectation operator and use its linearity property:\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\mathbb{E}\\left[\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a} W_{ab} h_{i,b}]$$\nThe problem states that all random variables $s_{t,a}$, $W_{ab}$, and $h_{i,b}$ are mutually independent. Therefore, the expectation of their product is the product of their expectations:\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = \\mathbb{E}[s_{t,a}] \\mathbb{E}[W_{ab}] \\mathbb{E}[h_{i,b}]$$\nAccording to the given distributional assumptions, all these variables are drawn from zero-mean Gaussian distributions:\n- $\\mathbb{E}[s_{t,a}] = 0$\n- $\\mathbb{E}[W_{ab}] = 0$\n- $\\mathbb{E}[h_{i,b}] = 0$\nConsequently, for every triplet of indices $(a,b)$, we have:\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = 0 \\cdot 0 \\cdot 0 = 0$$\nSubstituting this back into the sum, we find the total expectation:\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} 0 = 0$$\n\n**Part 2: Variance of Multiplicative Energy $\\operatorname{Var}(e_i^{\\text{mult}}]$**\n\nThe variance of a random variable $X$ is defined as $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Since we have shown that $\\mathbb{E}[e_i^{\\text{mult}}] = 0$, the variance simplifies to the expectation of the square:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\mathbb{E}\\left[ (e_i^{\\text{mult}})^2 \\right] = \\mathbb{E}\\left[ \\left(\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right)^2 \\right]$$\nLet us expand the square of the summation:\n$$\\left(\\sum_{a,b} s_{t,a} W_{ab} h_{i,b}\\right)^2 = \\sum_{a,b} \\sum_{c,d} (s_{t,a} W_{ab} h_{i,b}) (s_{t,c} W_{cd} h_{i,d})$$\nwhere the sums over $a, b, c, d$ all run from $1$ to $d_h$. Taking the expectation:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a,b,c,d} \\mathbb{E}[s_{t,a} s_{t,c} W_{ab} W_{cd} h_{i,b} h_{i,d}]$$\nDue to independence, the expectation of the product can be separated. The expectation of any term will be zero if any of its constituent random variables has an exponent of $1$, as all variables are zero-mean. For the expectation to be non-zero, every random variable in the product must be paired with itself. This occurs only when the indices match:\n- $s_{t,a}$ must be paired with $s_{t,c}$, which requires $a=c$.\n- $W_{ab}$ must be paired with $W_{cd}$, which requires $(a,b)=(c,d)$.\n- $h_{i,b}$ must be paired with $h_{i,d}$, which requires $b=d$.\nThe condition $(a,b)=(c,d)$ subsumes the other two. Thus, the cross-terms where $(a,b) \\neq (c,d)$ have an expectation of zero. We only need to consider the terms where $a=c$ and $b=d$:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[(s_{t,a} W_{ab} h_{i,b})^2] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2 W_{ab}^2 h_{i,b}^2]$$\nBy independence, this becomes:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2] \\mathbb{E}[W_{ab}^2] \\mathbb{E}[h_{i,b}^2]$$\nFor any zero-mean random variable $Z$ with variance $\\sigma^2$, we have $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2 = \\sigma^2 + 0^2 = \\sigma^2$. Applying this:\n- $\\mathbb{E}[s_{t,a}^2] = \\sigma_s^2$\n- $\\mathbb{E}[W_{ab}^2] = \\sigma_W^2$\n- $\\mathbb{E}[h_{i,b}^2] = \\sigma_h^2$\nSubstituting these into the sum:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} (\\sigma_s^2 \\sigma_W^2 \\sigma_h^2)$$\nThe term inside the summation is constant with respect to the indices $a$ and $b$. The sum has $d_h \\times d_h = d_h^2$ identical terms.\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = d_h^2 \\sigma_s^2 \\sigma_W^2 \\sigma_h^2$$\n\n**Part 3: Specialization for Xavier Initialization and Dependence on $d_h$**\n\nThe problem requires specializing this result for the case of Xavier-normal initialization for the matrix $W$, where $\\sigma_W^2 = 1/d_h$. Substituting this into our variance expression:\n$$\\operatorname{Var}(e_i^{\\text{mult}})_{\\text{Xavier}} = d_h^2 \\sigma_s^2 \\left(\\frac{1}{d_h}\\right) \\sigma_h^2 = d_h \\sigma_s^2 \\sigma_h^2$$\nThis result explicitly quantifies the dependence of the energy's variance on the hidden dimension $d_h$. Under Xavier initialization, the variance of the multiplicative attention energy grows linearly with $d_h$. This scaling behavior is a primary motivation for the \"scaled dot-product attention\" mechanism, which divides the energy by $\\sqrt{d_k}$ (where $d_k$ is the key dimension, equivalent to $d_h$ here) to counteract this growth and stabilize gradients during training.\n\n**Part 4: Expectation of Additive Energy $\\mathbb{E}[e_i^{\\text{add}}]$**\n\nThe additive energy is defined as $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$. In summation form:\n$$e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k \\tanh((W_s s_t + W_h h_i)_k)$$\nwhere the subscript $k$ denotes the $k$-th element of the resulting vector. Let $z_k = \\tanh((W_s s_t + W_h h_i)_k)$. The expression becomes $e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k z_k$.\nTo find the expectation, we again use linearity:\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\mathbb{E}\\left[\\sum_{k=1}^{d_h} v_k z_k\\right] = \\sum_{k=1}^{d_h} \\mathbb{E}[v_k z_k]$$\nThe term $z_k$ is a function of the random variables in $s_t$, $h_i$, $W_s$, and $W_h$. The problem states that the entries of the vector $v$ are independent of all other variables. Therefore, for each $k$, $v_k$ is independent of $z_k$. This allows us to separate the expectation:\n$$\\mathbb{E}[v_k z_k] = \\mathbb{E}[v_k] \\mathbb{E}[z_k]$$\nWe are given that $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$, which means $\\mathbb{E}[v_k] = 0$.\nTherefore, for each $k$:\n$$\\mathbb{E}[v_k z_k] = 0 \\cdot \\mathbb{E}[z_k] = 0$$\nThis holds regardless of the value of $\\mathbb{E}[z_k]$. The total expectation is:\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\sum_{k=1}^{d_h} 0 = 0$$\nIt is worth noting that $\\mathbb{E}[z_k]$ is also zero. The argument of the $\\tanh$ function, let's call it $u_k = (W_s s_t + W_h h_i)_k$, is a sum of products of independent, zero-mean random variables. Its distribution is therefore symmetric about zero. Since $\\tanh(x)$ is an odd function ($\\tanh(-x) = -\\tanh(x)$), the expectation $\\mathbb{E}[\\tanh(u_k)]$ over a symmetric distribution is necessarily zero. However, the independence of $v$ and its zero mean provide the most direct path to the result.\n\n### Summary of Results\n1.  **Expectation of Multiplicative Energy:** $\\mathbb{E}[e_i^{\\text{mult}}] = 0$\n2.  **Variance of Multiplicative Energy (Xavier Init):** $\\operatorname{Var}(e_i^{\\text{mult}}) = d_h \\sigma_s^2 \\sigma_h^2$\n3.  **Expectation of Additive Energy:** $\\mathbb{E}[e_i^{\\text{add}}] = 0$\nThese three quantities are collected in a row matrix for the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & d_h \\sigma_s^2 \\sigma_h^2 & 0\n\\end{pmatrix}\n}\n$$", "id": "3097429"}, {"introduction": "Theory provides the \"why,\" but a concrete example demonstrates the \"how.\" In this coding practice, you will implement simplified versions of both attention mechanisms to observe a fundamental behavioral difference firsthand. You will investigate how additive attention's use of a saturating nonlinearity like $\\tanh$ contrasts with the linear scaling of multiplicative attention, revealing a critical trade-off in their ability to handle inputs of varying magnitudes [@problem_id:3097423].", "problem": "Consider a sequence-to-sequence model with an attention mechanism over encoder states, focusing on a single scalar dimension to isolate the role of magnitude in the decoder state. Let the encoder hidden states be three scalars $h_{1}$, $h_{2}$, and $h_{3}$, each in $\\mathbb{R}$, and let the decoder state at time $t$ be a scalar $s_{t} \\in \\mathbb{R}$. The magnitude of $s_{t}$ encodes importance: larger $|s_{t}|$ should lead to more selective attention toward inputs aligned with the sign of $s_{t}$. We compare two attention score constructions: additive attention (Bahdanau) and multiplicative attention (Luong).\n\nUse the following foundational definitions and parameterizations:\n\n- The Softmax function with temperature $\\tau$ for a vector of real-valued energies $e \\in \\mathbb{R}^{n}$ is defined by\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}.\n$$\nIn all computations, set $\\tau = 1$.\n\n- Additive attention energy uses a one-hidden-layer feed-forward construction with hyperbolic tangent nonlinearity:\n$$\ne_{i}^{\\mathrm{add}} = v \\cdot \\tanh\\left(w_{h} h_{i} + w_{s} s_{t} + b\\right),\n$$\nwith fixed scalar parameters $v = 1$, $w_{h} = 1$, $w_{s} = 10$, and $b = 0$.\n\n- Multiplicative attention energy uses a bilinear (scaled dot-product) form:\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i},\n$$\nwith fixed scalar parameter $W = 1$.\n\nLet the encoder states be\n$$\nh_{1} = 1, \\quad h_{2} = -1, \\quad h_{3} = 0.\n$$\nInterpret $h_{1}$ as the aligned state for positive $s_{t}$ because it shares sign alignment.\n\nFor each energy construction, convert energies $\\{e_{i}\\}$ to attention weights $\\{\\alpha_{i}\\}$ using the Softmax definition above. The attention weight assigned to $h_{1}$ under a given construction is the corresponding $\\alpha_{1}$.\n\nTest suite specification (each test case specifies a value for $s_{t}$):\n- Case $1$: $s_{t} = 0$ (boundary case with no magnitude).\n- Case $2$: $s_{t} = 0.1$ (small magnitude).\n- Case $3$: $s_{t} = 1$ (moderate magnitude).\n- Case $4$: $s_{t} = 5$ (large magnitude).\n- Case $5$: $s_{t} = 50$ (extreme magnitude).\n\nFor each case, compute:\n- The multiplicative attention weight on $h_{1}$, denoted $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$.\n- The additive attention weight on $h_{1}$, denoted $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$.\n\nThen, evaluate the following boolean properties across the entire test suite:\n- $B_{\\mathrm{mul}}$: $w^{\\mathrm{mul}}(s_{t})$ is nondecreasing as $s_{t}$ increases across the test suite. Formally, for the ordered set $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$, check $w^{\\mathrm{mul}}(0) \\le w^{\\mathrm{mul}}(0.1) \\le w^{\\mathrm{mul}}(1) \\le w^{\\mathrm{mul}}(5) \\le w^{\\mathrm{mul}}(50)$.\n- $B_{\\mathrm{add}}$: additive attention saturates and loses magnitude information at high $|s_{t}|$, quantified by near-constancy of the weight at large magnitudes. Check that the change between the large and extreme cases is negligible:\n$$\n\\left| w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5) \\right| \\le \\epsilon,\n$$\nwith $\\epsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order:\n- The five values $w^{\\mathrm{mul}}(s_{t})$ for $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$.\n- The five values $w^{\\mathrm{add}}(s_{t})$ for $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$.\n- The boolean $B_{\\mathrm{mul}}$.\n- The boolean $B_{\\mathrm{add}}$.\n\nFor example, an output line has the form\n$$\n[\\underbrace{w^{\\mathrm{mul}}(0), w^{\\mathrm{mul}}(0.1), w^{\\mathrm{mul}}(1), w^{\\mathrm{mul}}(5), w^{\\mathrm{mul}}(50)}_{\\text{five multiplicative weights}}, \\underbrace{w^{\\mathrm{add}}(0), w^{\\mathrm{add}}(0.1), w^{\\mathrm{add}}(1), w^{\\mathrm{add}}(5), w^{\\mathrm{add}}(50)}_{\\text{five additive weights}}, B_{\\mathrm{mul}}, B_{\\mathrm{add}} ].\n$$\nAll values are unitless, and no physical units are involved. The answers are floats for the weights and booleans for the final two properties.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of neural attention mechanisms, well-posed with all necessary definitions and parameters provided, and objective in its formulation. The problem asks for a quantitative comparison between additive (Bahdanau-style) and multiplicative (Luong-style) attention, focusing on how each mechanism responds to changes in the magnitude of a decoder state. We will now proceed with a complete, reasoned solution.\n\nThe core of the problem lies in calculating attention weights, which are derived from a set of energy scores using the Softmax function. For a set of energies $\\{e_{1}, e_{2}, \\dots, e_{n}\\}$, the corresponding attention weights $\\{\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{n}\\}$ are given by:\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}\n$$\nThe problem specifies a temperature $\\tau = 1$. The encoder hidden states are the scalars $h_{1} = 1$, $h_{2} = -1$, and $h_{3} = 0$. We will compute the attention weight $\\alpha_{1}$ for each specified value of the decoder state $s_{t}$ in the test suite $\\{0, 0.1, 1, 5, 50\\}$.\n\n**1. Multiplicative Attention Analysis**\n\nThe multiplicative attention energy is defined by the bilinear form $e_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i}$. With the scalar parameter $W = 1$, this simplifies to:\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} h_{i}\n$$\nFor the given encoder states, the energies are:\n- $e_{1}^{\\mathrm{mul}} = s_{t} \\cdot (1) = s_{t}$\n- $e_{2}^{\\mathrm{mul}} = s_{t} \\cdot (-1) = -s_{t}$\n- $e_{3}^{\\mathrm{mul}} = s_{t} \\cdot (0) = 0$\n\nThe attention weight on the first encoder state, $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$, is calculated using the Softmax function:\n$$\nw^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}} = \\frac{\\exp(e_{1}^{\\mathrm{mul}})}{\\exp(e_{1}^{\\mathrm{mul}}) + \\exp(e_{2}^{\\mathrm{mul}}) + \\exp(e_{3}^{\\mathrm{mul}})} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + \\exp(0)} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + 1}\n$$\nWe compute this value for each $s_{t}$ in the test suite:\n- For $s_{t} = 0$: $w^{\\mathrm{mul}}(0) = \\frac{\\exp(0)}{\\exp(0) + \\exp(0) + 1} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} \\approx 0.3333333333333333$\n- For $s_{t} = 0.1$: $w^{\\mathrm{mul}}(0.1) = \\frac{\\exp(0.1)}{\\exp(0.1) + \\exp(-0.1) + 1} \\approx 0.3670997184200147$\n- For $s_{t} = 1$: $w^{\\mathrm{mul}}(1) = \\frac{\\exp(1)}{\\exp(1) + \\exp(-1) + 1} \\approx 0.6652409557224216$\n- For $s_{t} = 5$: $w^{\\mathrm{mul}}(5) = \\frac{\\exp(5)}{\\exp(5) + \\exp(-5) + 1} \\approx 0.9932620531633535$\n- For $s_{t} = 50$: $w^{\\mathrm{mul}}(50) = \\frac{\\exp(50)}{\\exp(50) + \\exp(-50) + 1} \\approx 0.9999999999999999$\n\n**2. Additive Attention Analysis**\n\nThe additive attention energy is defined by $e_{i}^{\\mathrm{add}} = v \\cdot \\tanh(w_{h} h_{i} + w_{s} s_{t} + b)$. With parameters $v = 1$, $w_{h} = 1$, $w_{s} = 10$, and $b = 0$, this becomes:\n$$\ne_{i}^{\\mathrm{add}} = \\tanh(h_{i} + 10s_{t})\n$$\nThe energies for the given encoder states are:\n- $e_{1}^{\\mathrm{add}} = \\tanh(1 + 10s_{t})$\n- $e_{2}^{\\mathrm{add}} = \\tanh(-1 + 10s_{t})$\n- $e_{3}^{\\mathrm{add}} = \\tanh(0 + 10s_{t}) = \\tanh(10s_{t})$\n\nThe attention weight $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$ is:\n$$\nw^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}} = \\frac{\\exp(\\tanh(1 + 10s_{t}))}{\\exp(\\tanh(1 + 10s_{t})) + \\exp(\\tanh(-1 + 10s_{t})) + \\exp(\\tanh(10s_{t}))}\n$$\nWe compute this for each $s_{t}$:\n- For $s_{t} = 0$: $e_{1} = \\tanh(1)$, $e_{2} = \\tanh(-1)$, $e_{3} = \\tanh(0) = 0$.\n  $w^{\\mathrm{add}}(0) = \\frac{\\exp(\\tanh(1))}{\\exp(\\tanh(1)) + \\exp(-\\tanh(1)) + 1} \\approx 0.5052857441183307$\n- For $s_{t} = 0.1$: $e_{1} = \\tanh(2)$, $e_{2} = \\tanh(0) = 0$, $e_{3} = \\tanh(1)$.\n  $w^{\\mathrm{add}}(0.1) = \\frac{\\exp(\\tanh(2))}{\\exp(\\tanh(2)) + \\exp(0) + \\exp(\\tanh(1))} \\approx 0.44917951253013896$\n- For $s_{t} = 1$: $e_{1} = \\tanh(11)$, $e_{2} = \\tanh(9)$, $e_{3} = \\tanh(10)$.\n  $w^{\\mathrm{add}}(1) = \\frac{\\exp(\\tanh(11))}{\\exp(\\tanh(11)) + \\exp(\\tanh(9)) + \\exp(\\tanh(10))} \\approx 0.3333333333333333$\n- For $s_{t} = 5$: $e_{1} = \\tanh(51)$, $e_{2} = \\tanh(49)$, $e_{3} = \\tanh(50)$.\n  $w^{\\mathrm{add}}(5) \\approx 0.3333333333333333$\n- For $s_{t} = 50$: $e_{1} = \\tanh(501)$, $e_{2} = \\tanh(499)$, $e_{3} = \\tanh(500)$.\n  $w^{\\mathrm{add}}(50) \\approx 0.3333333333333333$\n\nThe behavior for large $s_{t}$ is explained by the saturation of the hyperbolic tangent function. For a large positive argument $x$, $\\tanh(x) \\approx 1$. For $s_{t} = 5$ and $s_{t} = 50$, the arguments to $\\tanh$ are large positive numbers ($49, 50, 51$ and $499, 500, 501$, respectively). Consequently, all three energies $e_{1}^{\\mathrm{add}}, e_{2}^{\\mathrm{add}}, e_{3}^{\\mathrm{add}}$ are extremely close to $1$. When all energies are nearly identical, the Softmax function distributes the probability mass almost uniformly, causing each weight to approach $\\frac{1}{3}$. This demonstrates the loss of sensitivity to input magnitude in additive attention at large scales, a direct result of the saturating nonlinearity.\n\n**3. Boolean Property Evaluation**\n\n- **$B_{\\mathrm{mul}}$**: This property checks if $w^{\\mathrm{mul}}(s_{t})$ is nondecreasing for $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$. The function $f(x) = \\frac{\\exp(x)}{\\exp(x) + \\exp(-x) + 1}$ has the derivative $f'(x) = \\frac{\\exp(x) + 2}{(\\exp(x) + \\exp(-x) + 1)^2}$, which is strictly positive for all real $x$. Thus, $w^{\\mathrm{mul}}(s_{t})$ is a strictly increasing function of $s_{t}$, which satisfies the nondecreasing condition. The calculated values confirm this: $0.333... \\le 0.367... \\le 0.665... \\le 0.993... \\le 0.999...$. Therefore, $B_{\\mathrm{mul}}$ is true.\n\n- **$B_{\\mathrm{add}}$**: This property checks if $|w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5)| \\le \\epsilon$ with $\\epsilon = 10^{-12}$. As explained, for $s_t=5$ and $s_t=50$, all arguments to the $\\tanh$ function are large, causing the energies to saturate to values extremely close to $1$. Minor differences in these arguments (e.g., between $\\tanh(49)$ and $\\tanh(499)$) result in differences in the energies that are exponentially small. The subsequent application of the $\\exp$ function and Softmax normalization results in attention weights for $s_{t}=5$ and $s_{t}=50$ that are practically identical. The numerical calculation confirms their difference is well below the threshold $\\epsilon = 10^{-12}$. Therefore, $B_{\\mathrm{add}}$ is true.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and verifies properties of additive and multiplicative attention\n    for a given set of encoder and decoder states.\n    \"\"\"\n    # Define givens from the problem statement\n    h = np.array([1, -1, 0])\n    s_t_values = [0, 0.1, 1, 5, 50]\n    tau = 1.0\n\n    # Parameters for additive attention\n    v_add = 1.0\n    w_h_add = 1.0\n    w_s_add = 10.0\n    b_add = 0.0\n\n    # Parameter for multiplicative attention\n    W_mul = 1.0\n    \n    epsilon = 1e-12\n\n    # Lists to store results\n    w_mul_results = []\n    w_add_results = []\n\n    def softmax(energies, temperature):\n        \"\"\"Computes softmax probabilities for a vector of energies.\"\"\"\n        # The temperature is given as 1, so division is nominal.\n        # Stabilize by subtracting the max energy.\n        e_stable = energies / temperature\n        e_stable = e_stable - np.max(e_stable)\n        exps = np.exp(e_stable)\n        return exps / np.sum(exps)\n\n    # Loop through each test case for s_t\n    for s_t in s_t_values:\n        # 1. Multiplicative Attention Calculation\n        energies_mul = s_t * W_mul * h\n        alphas_mul = softmax(energies_mul, tau)\n        w_mul = alphas_mul[0]\n        w_mul_results.append(w_mul)\n\n        # 2. Additive Attention Calculation\n        args_add = w_h_add * h + w_s_add * s_t + b_add\n        energies_add = v_add * np.tanh(args_add)\n        alphas_add = softmax(energies_add, tau)\n        w_add = alphas_add[0]\n        w_add_results.append(w_add)\n\n    # 3. Boolean Property Evaluation\n    # B_mul: Check if w_mul is nondecreasing\n    B_mul = all(w_mul_results[i] <= w_mul_results[i+1] for i in range(len(w_mul_results) - 1))\n\n    # B_add: Check for saturation at large magnitudes\n    B_add = abs(w_add_results[4] - w_add_results[3]) <= epsilon\n    \n    # Combine all results into a single list\n    final_results = w_mul_results + w_add_results + [B_mul, B_add]\n\n    # Format the output as specified\n    formatted_results = []\n    for r in final_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Format floats to a high precision to show stability\n            formatted_results.append(f\"{r:.16f}\")\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3097423"}, {"introduction": "The standard formulations of attention mechanisms are not the only options available. This final practice challenges you to think like a network architect by considering a modification to additive attention: replacing the standard $\\tanh$ activation with a Rectified Linear Unit ($\\operatorname{ReLU}$). You will reason about the deep implications of this change on the model's internal representations and learning dynamics, such as activation symmetry and gradient flow [@problem_id:3097395].", "problem": "You are given an encoder-decoder sequence model with additive (Bahdanau) attention. Let the encoder hidden state at source index $i$ be $h_i \\in \\mathbb{R}^{d_h}$ and the decoder state at target time $t$ be $s_{t-1} \\in \\mathbb{R}^{d_s}$. The additive attention score is defined by\n$$\ne_{t,i} \\;=\\; v^{\\top}\\,\\phi\\!\\left(W_h\\,h_i \\;+\\; W_s\\,s_{t-1} \\;+\\; b\\right),\n$$\nwhere $W_h \\in \\mathbb{R}^{d_a \\times d_h}$, $W_s \\in \\mathbb{R}^{d_a \\times d_s}$, $v \\in \\mathbb{R}^{d_a}$, $b \\in \\mathbb{R}^{d_a}$ are trainable parameters, and $\\phi$ is a pointwise nonlinearity. The attention weights are $\\alpha_{t,i} = \\exp(e_{t,i}) / \\sum_j \\exp(e_{t,j})$, and the context vector is $c_t = \\sum_i \\alpha_{t,i} h_i$. In the standard formulation, $\\phi(x) = \\tanh(x)$. Consider replacing $\\phi$ with the Rectified Linear Unit $\\phi(x) = \\operatorname{ReLU}(x) = \\max\\{0, x\\}$.\n\nAssume that at initialization the following hold:\n- The input projections yield pre-activations $z_{t,i} = W_h h_i + W_s s_{t-1} + b$ whose coordinates are approximately independent, zero-mean, symmetric about $0$ and have nonzero variance (for example, approximately Gaussian with mean $0$), due to zero-mean random initialization of $W_h$, $W_s$, and $b = 0$ and zero-mean $h_i$, $s_{t-1}$.\n- Gradients are propagated by the chain rule, and $\\tanh$ and $\\operatorname{ReLU}$ derivatives are the standard ones.\n\nUsing only the definitions above and basic properties of $\\tanh$ and $\\operatorname{ReLU}$, reason about how swapping $\\tanh$ for $\\operatorname{ReLU}$ in the additive attention scoring network changes (i) symmetry of the intermediate representation and its effect on the sign balance of contributions to the score, (ii) the distribution of backpropagated gradients through $\\phi$, and (iii) whether a bias term before $\\phi$ is potentially needed to avoid degenerate learning dynamics.\n\nWhich of the following statements are correct under the stated assumptions?\n\nA. With zero-mean symmetric pre-activations $z_{t,i}$, replacing $\\tanh$ by $\\operatorname{ReLU}$ makes each coordinate of $\\phi(z_{t,i})$ have strictly positive mean and eliminates the odd symmetry $\\phi(-x) = -\\phi(x)$. In contrast, with $\\tanh$, the activation is approximately zero-mean and odd-symmetric. This removes sign symmetry of contributions to $e_{t,i}$ unless parameters or biases re-center the pre-activations.\n\nB. Compared to $\\tanh$, $\\operatorname{ReLU}$ removes vanishing gradients everywhere because its derivative is either $0$ or $1$, so gradients backpropagate unchanged through all units.\n\nC. Without a bias term before $\\operatorname{ReLU}$ and with symmetric zero-mean pre-activations at initialization, approximately $1/2$ of the units will be inactive with zero derivative, which can slow learning; introducing a positive bias can shift the pre-activation distribution so that a larger fraction of units are in the linear region and receive nonzero gradients.\n\nD. Because $\\mathrm{softmax}$ is invariant to adding the same scalar to all scores at a given time step, the non-zero-centeredness introduced by $\\operatorname{ReLU}$ can always be perfectly neutralized by adding a scalar bias after computing $e_{t,i}$; thus there is no need for any bias inside the scoring network.\n\nE. Switching from $\\tanh$ to $\\operatorname{ReLU}$ in the additive attention scoring network makes it equivalent to multiplicative (Luong) attention for suitable choices of parameters, so both mechanisms compute the same scores for all inputs.", "solution": "The user's problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model:** Encoder-decoder with additive (Bahdanau) attention.\n- **Encoder hidden state:** $h_i \\in \\mathbb{R}^{d_h}$ at source index $i$.\n- **Decoder state:** $s_{t-1} \\in \\mathbb{R}^{d_s}$ at target time $t$.\n- **Attention score function:** $e_{t,i} = v^{\\top}\\,\\phi\\!\\left(W_h\\,h_i + W_s\\,s_{t-1} + b\\right)$.\n- **Parameters:** $W_h \\in \\mathbb{R}^{d_a \\times d_h}$, $W_s \\in \\mathbb{R}^{d_a \\times d_s}$, $v \\in \\mathbb{R}^{d_a}$, $b \\in \\mathbb{R}^{d_a}$.\n- **Activation function:** $\\phi$ is a pointwise nonlinearity.\n- **Reference activation:** $\\phi(x) = \\tanh(x)$.\n- **Proposed activation:** $\\phi(x) = \\operatorname{ReLU}(x) = \\max\\{0, x\\}$.\n- **Attention weights:** $\\alpha_{t,i} = \\exp(e_{t,i}) / \\sum_j \\exp(e_{t,j})$.\n- **Context vector:** $c_t = \\sum_i \\alpha_{t,i} h_i$.\n- **Initialization assumptions:**\n    - The pre-activations $z_{t,i} = W_h h_i + W_s s_{t-1} + b$ have coordinates that are approximately independent, zero-mean, symmetric about $0$, and have nonzero variance.\n    - This results from zero-mean random initialization of $W_h$, $W_s$, with $b = 0$, and zero-mean $h_i$, $s_{t-1}$.\n    - Gradients are computed via the chain rule with standard derivatives for $\\tanh$ and $\\operatorname{ReLU}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective. It provides a clear and formal description of the Bahdanau attention mechanism and poses specific questions about substituting the standard $\\tanh$ activation with $\\operatorname{ReLU}$. The assumptions regarding the distribution of pre-activations at initialization are standard in the analysis of neural networks. The problem does not violate any fundamental principles and is free of ambiguity or contradiction. It is a valid theoretical problem in the field of deep learning.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full derivation and evaluation of options will be performed.\n\n### Solution Derivation\n\nLet $z = z_{t,i}$ be the pre-activation vector. By assumption, at initialization $b=0$, and each component of $z$, denoted $z_k$, is a random variable drawn from a distribution that is symmetric about $0$ and has a mean of $0$. Let this generic random variable be denoted by $Z$. Its probability density function $f_Z(x)$ satisfies $E[Z] = 0$ and $f_Z(x) = f_Z(-x)$. We analyze the properties of the activation $\\phi(z)$ and its impact on the network.\n\n**Analysis of Statement A: Symmetry and Mean of the Activation**\n\n- **With $\\phi(x) = \\tanh(x)$:** The $\\tanh$ function is odd, i.e., $\\tanh(-x) = -\\tanh(x)$. For a symmetric, zero-mean input random variable $Z$, the output $Y = \\tanh(Z)$ is also symmetric and zero-mean. The expectation is $E[Y] = \\int_{-\\infty}^{\\infty} \\tanh(x) f_Z(x) dx$. Since $\\tanh(x)$ is an odd function and $f_Z(x)$ is an even function, their product is an odd function, and the integral over a symmetric domain is $0$. Thus, each coordinate of the activation vector $\\phi(z)$ has a mean of approximately $0$. The activations can be positive or negative, maintaining a sign symmetry.\n\n- **With $\\phi(x) = \\operatorname{ReLU}(x)$:** The $\\operatorname{ReLU}$ function is $\\max\\{0, x\\}$. It is not an odd function. For an input $Z$ as described, the output $Y = \\operatorname{ReLU}(Z)$ is always non-negative. Its expectation is $E[Y] = \\int_{-\\infty}^{\\infty} \\max\\{0, x\\} f_Z(x) dx = \\int_{0}^{\\infty} x f_Z(x) dx$. Since $Z$ has non-zero variance, $f_Z(x)$ is not concentrated at $x=0$, so this integral is strictly positive. Therefore, each coordinate of $\\phi(z)$ has a strictly positive mean. The odd symmetry is lost. The activations are all non-negative, which removes the sign symmetry that was present in the intermediate representation with $\\tanh$. While the final contributions to the score $e_{t,i} = \\sum_k v_k \\phi(z_k)$ might be balanced if the weights $v_k$ have mixed signs, the activations $\\phi(z_k)$ themselves have lost this property. Without a bias to shift the pre-activations, this systematic positive offset in the activations persists.\n\nStatement A accurately captures this fundamental change in the statistical properties of the intermediate representation.\n\n**Analysis of Statement B: Gradient Propagation**\n\n- **With $\\phi(x) = \\tanh(x)$:** The derivative is $\\phi'(x) = 1 - \\tanh^2(x)$. This derivative is always in the range $(0, 1]$. For $|x| \\gg 0$, the neuron saturates, $\\tanh^2(x) \\to 1$, and the gradient $\\phi'(x) \\to 0$. This is the classic vanishing gradient problem.\n\n- **With $\\phi(x) = \\operatorname{ReLU}(x)$:** The derivative is $\\phi'(x) = 1$ for $x > 0$ and $\\phi'(x) = 0$ for $x  0$. Gradients are passed through unchanged for active units ($x>0$), which mitigates the vanishing gradient problem in those pathways. However, for inactive units ($x0$), the gradient is completely zeroed out. This is often called the \"dying ReLU\" problem. The statement that $\\operatorname{ReLU}$ \"removes vanishing gradients everywhere\" is false; it introduces an even more severe gradient-blocking issue for inactive units. The claim that \"gradients backpropagate unchanged through all units\" is also false, as gradients are blocked for any unit with a negative pre-activation.\n\nStatement B makes an overly strong and incorrect claim about gradient propagation through $\\operatorname{ReLU}$ networks.\n\n**Analysis of Statement C: Role of the Bias Term with ReLU**\n\n- As per the initialization assumptions, the pre-activations $z_k$ are drawn from a symmetric distribution with mean $0$. This implies that $P(z_k  0) = 1/2$.\n- For every unit where the pre-activation $z_k$ is negative, the $\\operatorname{ReLU}$ activation is $0$, and its derivative is $0$. Thus, at initialization, approximately $1/2$ of the units in the hidden layer of the attention mechanism will be \"inactive\" and will have a zero gradient with respect to their inputs. This means the weights associated with these units ($W_h, W_s$) will not be updated for the given input, which can slow down the learning process.\n- If a positive bias term $b_k  0$ is introduced to each unit (or if the shared bias vector $b$ has positive components), the pre-activation becomes $z'_k = (W_h h_i + W_s s_{t-1})_k + b_k$. The mean of this new pre-activation is $E[z'_k] = b_k  0$. For a distribution with a positive mean, the probability of a draw being positive is greater than $1/2$ (i.e., $P(z'_k  0)  1/2$). This increases the fraction of active units, allowing more gradients to propagate and more weights to be updated, potentially accelerating learning, particularly at the start.\n\nStatement C correctly diagnoses the issue of inactive neurons at initialization and accurately describes the utility of a positive bias term as a remedy.\n\n**Analysis of Statement D: Neutralizing ReLU effects with a Post-Score Bias**\n\n- The $\\mathrm{softmax}$ function is indeed invariant to a constant shift in its inputs: $\\mathrm{softmax}(e_{t,1}+C, e_{t,2}+C, \\ldots) = \\mathrm{softmax}(e_{t,1}, e_{t,2}, \\ldots)$.\n- The issues with using $\\operatorname{ReLU}$ discussed in A and C are not about the final values of $\\alpha_{t,i}$, but about the internal dynamics of the scoring network, specifically the statistical properties of the activations (mean shift, loss of symmetry) and the gradient flow (dying neurons).\n- Adding a bias *after* computing the score $e_{t,i}$ has no effect whatsoever on the computation of the gradients of $e_{t,i}$ with respect to the network parameters $W_h, W_s, b$. The gradient pathway, which is pruned whenever a $\\operatorname{ReLU}$ unit is inactive, is determined *inside* the scoring network. A post-hoc adjustment to the final scores cannot repair these broken gradient paths.\n- Therefore, the claim that an external bias negates the need for a bias *inside* the network (the parameter $b$) is false. The internal bias $b$ is crucial for controlling the operating point of the $\\operatorname{ReLU}$ non-linearities and ensuring healthy gradient flow, a completely different function from any bias added to the final scores.\n\nStatement D confuses the properties of the final softmax layer with the internal learning dynamics of the scoring network.\n\n**Analysis of Statement E: Equivalence to Multiplicative Attention**\n\n- The additive attention score with $\\operatorname{ReLU}$ is $e_{t,i} = v^{\\top}\\,\\operatorname{ReLU}(W_h h_i + W_s s_{t-1} + b)$. This is a non-linear function of $h_i$ and $s_{t-1}$ due to the piecewise-linear nature of $\\operatorname{ReLU}$. Specifically, it represents the composition of an affine transformation and a non-linear activation, followed by another linear transformation. This is the structure of a small, one-hidden-layer feed-forward network.\n- The \"general\" multiplicative (Luong) attention score is $e_{t,i} = s_{t-1}^{\\top} W_a h_i$. This is a bilinear function of $s_{t-1}$ and $h_i$.\n- A bilinear form cannot in general be represented by the additive structure with a $\\operatorname{ReLU}$ activation. For a fixed $s_{t-1}$, the multiplicative score is a linear function of $h_i$. In contrast, for a fixed $s_{t-1}$, the additive score with $\\operatorname{ReLU}$ is a piecewise-linear, convex function of $h_i$. These are fundamentally different functional forms. There is no choice of parameters $v, W_h, W_s, b$ that can make the additive form equivalent to the multiplicative form for all inputs $h_i$ and $s_{t-1}$. The two mechanisms are distinct by design.\n\nStatement E makes an incorrect claim about functional equivalence between two distinct attention mechanisms.\n\n### Option-by-Option Analysis\n\n- **A. With zero-mean symmetric pre-activations $z_{t,i}$, replacing $\\tanh$ by $\\operatorname{ReLU}$ makes each coordinate of $\\phi(z_{t,i})$ have strictly positive mean and eliminates the odd symmetry $\\phi(-x) = -\\phi(x)$. In contrast, with $\\tanh$, the activation is approximately zero-mean and odd-symmetric. This removes sign symmetry of contributions to $e_{t,i}$ unless parameters or biases re-center the pre-activations.**\n  - **Verdict: Correct.** The analysis shows that $\\operatorname{ReLU}$ induces a positive mean in the activations and breaks the symmetry inherent to $\\tanh$ with symmetric zero-mean inputs.\n\n- **B. Compared to $\\tanh$, $\\operatorname{ReLU}$ removes vanishing gradients everywhere because its derivative is either $0$ or $1$, so gradients backpropagate unchanged through all units.**\n  - **Verdict: Incorrect.** The derivative is $0$ for negative inputs, which blocks gradients completely for those units. This is a form of vanishing gradient, not its elimination. Gradients only propagate through active units.\n\n- **C. Without a bias term before $\\operatorname{ReLU}$ and with symmetric zero-mean pre-activations at initialization, approximately $1/2$ of the units will be inactive with zero derivative, which can slow learning; introducing a positive bias can shift the pre-activation distribution so that a larger fraction of units are in the linear region and receive nonzero gradients.**\n  - **Verdict: Correct.** This accurately describes the \"dying ReLU\" problem in this context and the standard solution of using a positive bias to increase the initial proportion of active units.\n\n- **D. Because $\\mathrm{softmax}$ is invariant to adding the same scalar to all scores at a given time step, the non-zero-centeredness introduced by $\\operatorname{ReLU}$ can always be perfectly neutralized by adding a scalar bias after computing $e_{t,i}$; thus there is no need for any bias inside the scoring network.**\n  - **Verdict: Incorrect.** This conflates the output properties of $\\mathrm{softmax}$ with the internal learning dynamics of the network. A bias added after the score computation cannot fix gradient flow issues within the network.\n\n- **E. Switching from $\\tanh$ to $\\operatorname{ReLU}$ in the additive attention scoring network makes it equivalent to multiplicative (Luong) attention for suitable choices of parameters, so both mechanisms compute the same scores for all inputs.**\n  - **Verdict: Incorrect.** The functional forms of additive attention (a small neural network) and multiplicative attention (a bilinear form) are fundamentally different and not equivalent.\n\nBased on the analysis, statements A and C are correct.", "answer": "$$\\boxed{AC}$$", "id": "3097395"}]}