{"hands_on_practices": [{"introduction": "To truly grasp the scaled dot-product attention mechanism, it's helpful to first examine it in its simplest form. This exercise strips away the complexity of high-dimensional vectors and asks you to compute the attention output when queries, keys, and values are all single numbers ($d_k = 1$). By working through the calculation from first principles, you will solidify your understanding of how similarity scores, the softmax function, and value weighting combine to produce a final output, revealing the mechanism's core as a sophisticated form of weighted averaging. [@problem_id:3172468]", "problem": "A system uses the mechanism commonly called attention to combine a set of scalar values based on their similarity to a scalar query. In the edge case where the key dimension equals one, the similarity between a query and a key reduces to their scalar product. Let the query be $q$, the keys be $k_1, k_2, k_3$, and the values be $v_1, v_2, v_3$, all real numbers. Assume that the conventional scaling by the square-root of the key dimension is applied to the similarity scores before transforming them by a normalized exponential mapping. Starting from the definition of the dot product for scalars and the definition of the normalized exponential mapping, derive the weights assigned to each value and use them to compute the single scalar output of the attention operation for the following data:\n$q = 2$, $k_1 = 0.1$, $k_2 = 0.2$, $k_3 = 0.5$, $v_1 = 1$, $v_2 = -1$, $v_3 = 3$, with key dimension $d_k = 1$.\nRound your final numeric result to four significant figures and express it as a pure number without units.", "solution": "The goal is to construct the attention output from first principles in the case where the key dimension equals one. The fundamental bases we use are the definition of a dot product and the definition of the normalized exponential mapping, typically referred to as the softmax function.\n1. Compatibility scores from scalar dot products and conventional scaling. With key dimension $d_k = 1$, the square-root scaling factor is $\\sqrt{d_k} = \\sqrt{1} = 1$. For a scalar query $q$ and scalar keys $k_i$, the similarity (compatibility) scores are\n$$\ns_i = \\frac{q \\cdot k_i}{\\sqrt{d_k}} = q\\,k_i.\n$$\n2. Normalized exponential mapping (softmax). Given a set of scores $(s_1, s_2, s_3)$, the attention weights are obtained by exponentiating the scores and normalizing so they sum to one:\n$$\n\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{3} \\exp(s_j)}.\n$$\n3. Attention output as a convex combination of values. The attention output $y$ is the weighted sum of the values:\n$$\ny = \\sum_{i=1}^{3} \\alpha_i v_i.\n$$\nWe now apply these steps to the given numerical data with $q = 2$, $k_1 = 0.1$, $k_2 = 0.2$, $k_3 = 0.5$, $v_1 = 1$, $v_2 = -1$, $v_3 = 3$, and $d_k = 1$.\n- Compute the scores using $s_i = q k_i$:\n$$\ns_1 = 2 \\cdot 0.1 = 0.2,\\quad s_2 = 2 \\cdot 0.2 = 0.4,\\quad s_3 = 2 \\cdot 0.5 = 1.0.\n$$\n- Exponentiate the scores:\n$$\n\\exp(s_1) = \\exp(0.2),\\quad \\exp(s_2) = \\exp(0.4),\\quad \\exp(s_3) = \\exp(1.0).\n$$\nKeep these symbolic until the final numeric step. The normalization constant is\n$$\nZ = \\exp(0.2) + \\exp(0.4) + \\exp(1.0).\n$$\n- The weights are\n$$\n\\alpha_1 = \\frac{\\exp(0.2)}{Z},\\quad \\alpha_2 = \\frac{\\exp(0.4)}{Z},\\quad \\alpha_3 = \\frac{\\exp(1.0)}{Z}.\n$$\n- The output is\n$$\ny = \\alpha_1 \\cdot 1 + \\alpha_2 \\cdot (-1) + \\alpha_3 \\cdot 3 = \\frac{\\exp(0.2) - \\exp(0.4) + 3\\exp(1.0)}{Z}.\n$$\nSubstitute numerical evaluations for the exponentials to produce the final number:\n$$\n\\exp(0.2) \\approx 1.221402758160170,\\quad \\exp(0.4) \\approx 1.491824697641270,\\quad \\exp(1.0) \\approx 2.718281828459045.\n$$\nThen\n$$\nZ \\approx 1.221402758160170 + 1.491824697641270 + 2.718281828459045 \\approx 5.431509284260485,\n$$\nand\n$$\n\\text{numerator} \\approx 1.221402758160170 - 1.491824697641270 + 3 \\cdot 2.718281828459045 \\approx 7.884423545896035.\n$$\nThus\n$$\ny \\approx \\frac{7.884423545896035}{5.431509284260485} \\approx 1.45160822\\ldots\n$$\nRounded to four significant figures, this yields $1.452$.\nInterpretation in relation to scalar correlation models: When $d_k = 1$, the compatibility $s_i = q k_i$ is simply the product of two scalars. For a positive query $q$, larger (more positive) keys $k_i$ produce larger $s_i$, hence larger weights after the normalized exponential mapping. This behavior mirrors a simple scalar correlation model where alignment is captured by the product of two scalar signals, and the softmax provides a normalized, positive weighting that emphasizes higher correlations while maintaining a convex combination of the values.", "answer": "$$\\boxed{1.452}$$", "id": "3172468"}, {"introduction": "While attention weights tell us *where* a model should look, they don't tell the whole story. This problem explores a critical stability issue: how a value vector $v_j$ with an extremely large norm can dominate the attention output, even when its corresponding attention weight $a_j$ is very small. You will analyze this phenomenon and evaluate several architectural interventions designed to enforce robustness, gaining a deeper appreciation for why normalization layers are essential components of the Transformer architecture. [@problem_id:3172404]", "problem": "A single-head attention module in a transformer computes a convex combination of value vectors. Concretely, given value vectors $v_1,\\dots,v_n \\in \\mathbb{R}^{d_v}$ and nonnegative attention weights $a_1,\\dots,a_n$ that sum to $1$ (from a softmax over scaled dot products of queries and keys), the head output is the linear mixture $o = \\sum_{j=1}^n a_j v_j$. Consider a setting where the attention is peaky: $a_1 = 0.99$, $a_2 = 0.01$, and $a_j = 0$ for all other $j$. Suppose further that $\\|v_1\\|_2 = 1$ and $\\|v_2\\|_2 = B$, where $B$ is a large positive real number. Using only the linearity of the mixture and basic properties of the Euclidean norm, first reason about how the magnitude of $v_2$ can influence $\\|o\\|_2$ even when $a_2 \\ll a_1$. Then, to mitigate such value-dominance irrespective of the scale $B$, you are asked to evaluate several design interventions.\n\nWe say an intervention enforces the following robustness property $\\mathcal{P}$ at the value-mixing stage if there exists a constant $C$ that does not depend on any particular $\\|v_j\\|_2$ such that for all tokens $j$, the per-token contribution magnitude satisfies $a_j \\|v_j\\|_2 \\le C$. In words, no single token can dominate the mixture solely by having an extremely large value norm.\n\nAssume the usual transformer pre-normalization in which inputs to the value projection are Layer Normalized, so that each token embedding $x_j$ satisfies $\\|x_j\\|_2 \\le R$ for some fixed $R > 0$, where Layer Normalization (LayerNorm) denotes per-token affine-normalization of features.\n\nWhich of the following interventions provably enforce $\\mathcal{P}$ under these assumptions? Select all that apply.\n\nA. Constrain the spectral norm of the value projection matrix $W_V \\in \\mathbb{R}^{d_v \\times d_{\\text{model}}}$ to be at most $\\sigma_{\\max}$ (for example, by spectral norm regularization), and do not apply any per-token scaling after $W_V$.\n\nB. Replace each value $v_j$ by the rescaled value $v_j' = g \\, v_j / (\\|v_j\\|_2 + \\epsilon)$ with a learned global gain $g > 0$ shared across tokens and a small $\\epsilon > 0$, and then mix $v_j'$ using the same attention weights.\n\nC. Reduce the softmax temperature by dividing the attention logits by an additional constant $c > 1$ beyond the usual scaling, thereby making the attention distribution less peaky.\n\nD. Clip each attention weight so that $a_j \\le \\alpha$ for a fixed $\\alpha \\in (0,1)$, and then renormalize the weights to sum to $1$ before mixing.\n\nE. Apply LayerNorm with learned scale $\\gamma$ to the output $o$ after the value mixture $\\sum_{j=1}^n a_j v_j$ and before passing to the next layer.\n\nYour reasoning should begin from the definition of $o$ as a convex combination and the basic properties of norms. You should establish a quantitative lower bound in the peaky case to show the potential for dominance by large $\\|v_j\\|_2$, and then determine which interventions guarantee a bound of the form $a_j \\|v_j\\|_2 \\le C$ that is independent of $\\|v_j\\|_2$.", "solution": "Start from the definition of the attention head output as a convex combination of the values: $o = \\sum_{j=1}^n a_j v_j$, where $a_j \\ge 0$ and $\\sum_{j=1}^n a_j = 1$. Consider the peaky case $a_1 = 0.99$, $a_2 = 0.01$, $a_j = 0$ for all other $j$, with $\\|v_1\\|_2 = 1$ and $\\|v_2\\|_2 = B$.\n\nBy the reverse triangle inequality, for any vectors $u$ and $w$, $\\|u + w\\|_2 \\ge \\big|\\|u\\|_2 - \\|w\\|_2\\big|$. Apply this to $u = a_2 v_2$ and $w = a_1 v_1$ to obtain\n$$\n\\|o\\|_2 \\;=\\; \\|a_1 v_1 + a_2 v_2\\|_2 \\;\\ge\\; \\big| \\|a_2 v_2\\|_2 - \\|a_1 v_1\\|_2 \\big| \\;=\\; \\big| 0.01\\,B - 0.99 \\big|.\n$$\nTherefore, whenever $B > 99$, the lower bound becomes $0.01\\,B - 0.99$, which grows linearly in $B$, demonstrating that even a small attention weight $a_2 = 0.01$ cannot prevent the magnitude of $v_2$ from dominating the mixed output for sufficiently large $B$. In the best-alignment case (when $v_1$ and $v_2$ point in the same direction), the magnitude is even larger:\n$$\n\\|o\\|_2 \\;=\\; \\|0.99\\,v_1 + 0.01\\,v_2\\|_2 \\;=\\; 0.99\\,\\|v_1\\|_2 + 0.01\\,\\|v_2\\|_2 \\;=\\; 0.99 + 0.01\\,B,\n$$\nagain growing without bound as $B \\to \\infty$. This shows the phenomenon: a small weight on a very large-norm value can significantly influence the output.\n\nWe now analyze which interventions enforce the robustness property $\\mathcal{P}$: there exists a constant $C$ independent of any particular $\\|v_j\\|_2$ such that $a_j \\|v_j\\|_2 \\le C$ for all tokens $j$ at the mixing stage.\n\nOption A: Constrain the spectral norm of $W_V$ and assume bounded inputs.\nLet $v_j = W_V x_j$, where each $x_j$ is the LayerNormed token embedding with $\\|x_j\\|_2 \\le R$. The spectral norm constraint $\\|W_V\\|_2 \\le \\sigma_{\\max}$ guarantees\n$$\n\\|v_j\\|_2 \\;=\\; \\|W_V x_j\\|_2 \\;\\le\\; \\|W_V\\|_2 \\,\\|x_j\\|_2 \\;\\le\\; \\sigma_{\\max} R.\n$$\nTherefore, for each $j$,\n$$\na_j \\|v_j\\|_2 \\;\\le\\; a_j \\, (\\sigma_{\\max} R) \\;\\le\\; \\sigma_{\\max} R,\n$$\nsince $a_j \\le 1$. This satisfies $\\mathcal{P}$ with $C = \\sigma_{\\max} R$, a constant independent of any particular $\\|v_j\\|_2$. Verdict: Correct.\n\nOption B: Per-token value $\\ell_2$-normalization with a shared gain.\nDefine $v_j' = g \\, v_j / (\\|v_j\\|_2 + \\epsilon)$, with $g > 0$ shared across tokens and a small $\\epsilon > 0$. Then\n$$\n\\|v_j'\\|_2 \\;=\\; g \\, \\frac{\\|v_j\\|_2}{\\|v_j\\|_2 + \\epsilon} \\;\\le\\; g.\n$$\nMixing these rescaled values yields per-token contributions bounded by\n$$\na_j \\|v_j'\\|_2 \\;\\le\\; a_j \\, g \\;\\le\\; g,\n$$\nso $\\mathcal{P}$ holds with $C = g$. This bound is independent of the original $\\|v_j\\|_2$. Verdict: Correct.\n\nOption C: Reduce softmax temperature by dividing logits by an additional constant $c > 1$.\nChanging the temperature modifies the distribution of $a_j$, typically making it less peaky. However, $\\mathcal{P}$ requires a bound independent of $\\|v_j\\|_2$. For any $c$, one can choose a configuration where a token with large $\\|v_j\\|_2$ retains a nontrivial weight $a_j$ (recall $\\sum_j a_j = 1$), yielding $a_j \\|v_j\\|_2$ that scales without bound as $\\|v_j\\|_2 \\to \\infty$. Thus, temperature adjustment does not produce a constant $C$ independent of $\\|v_j\\|_2$. Verdict: Incorrect.\n\nOption D: Clip $a_j \\le \\alpha$ and renormalize to sum to $1$.\nClipping ensures $a_j \\le \\alpha$, but after renormalization the weights still sum to $1$. For any fixed $\\alpha \\in (0,1)$, the per-token contribution can be as large as $a_j \\|v_j\\|_2 \\le \\alpha \\, \\|v_j\\|_2$, which is unbounded as $\\|v_j\\|_2 \\to \\infty$. Hence there is no constant $C$ independent of $\\|v_j\\|_2$ that satisfies $\\mathcal{P}$. Verdict: Incorrect.\n\nOption E: Apply LayerNorm with learned scale $\\gamma$ after the mixture.\nLayer Normalization (LayerNorm) applied to $o$ can bound the outgoing magnitude after normalization and scaling, but $\\mathcal{P}$ is explicitly a constraint on the value-mixing stage contributions $a_j \\|v_j\\|_2$ prior to any post-mixing normalization. Post-hoc LayerNorm does not bound $a_j \\|v_j\\|_2$ itself; the pre-normalization mixture can still be dominated by a large-norm $v_j$ in magnitude and direction. Therefore, $\\mathcal{P}$ is not enforced. Verdict: Incorrect.\n\nIn summary, interventions that directly bound the value norms before mixing, either by constraining the operator mapping into values (Option A) or by normalizing each value to a fixed norm scale (Option B), provably enforce the robustness property $\\mathcal{P}$. Strategies that only reshape attention weights (Options C and D) or normalize after mixing (Option E) do not enforce $\\mathcal{P}$ as defined.", "answer": "$$\\boxed{AB}$$", "id": "3172404"}, {"introduction": "In real-world applications, the efficiency of a model is as important as its accuracy. This practice challenges you to implement and analyze an optimization technique called certainty-based pruning, which can significantly speed up attention computations during inference. By exploiting cases where the attention distribution is highly peaked (i.e., 'certain'), you can approximate the full attention output, creating a trade-off between computational savings and a small loss in accuracy. [@problem_id:3172461]", "problem": "Consider a single-head Scaled Dot-Product Attention (SDPA) mechanism operating on a set of queries and keys in a real vector space of dimension $d$, with corresponding value vectors. Let there be $n_q$ query vectors $\\{q_i\\}_{i=1}^{n_q}$, $n_k$ key vectors $\\{k_j\\}_{j=1}^{n_k}$, and $n_k$ value vectors $\\{v_j\\}_{j=1}^{n_k}$, all in $\\mathbb{R}^d$. The keys are assumed to be unit-norm. Using the fundamental definitions of the Euclidean inner product and the softmax function, SDPA maps each query $q_i$ to an attention distribution over keys and then to an output vector constructed from the values. Define the certainty-based pruning rule: if the largest attention probability for a query $q_i$ exceeds a threshold $\\tau$ (that is, if $\\max_j \\alpha^{(i)}_j > \\tau$), then approximate the output of SDPA for that query by the single value vector associated with the maximizing key, and skip the downstream per-token computation. Otherwise, compute the full attention output and keep the downstream computation.\n\nYou must produce a program that:\n- Computes the baseline SDPA output for each $q_i$ without pruning.\n- Applies the certainty-based pruning rule per token for a given $\\tau$, replacing the output with the maximizing value vector when the rule triggers, and otherwise using the baseline SDPA output.\n- Quantifies the accuracy loss as the average squared Euclidean distance between the pruned outputs and the baseline outputs across all queries, expressed as a decimal number (no percentage sign).\n- Quantifies the speedup as the ratio of baseline total operation count to pruned total operation count, under the following cost model:\n    - Computing all scaled inner products $s_j$ for one query has cost $n_k \\cdot d$.\n    - Applying the softmax for one query has cost $2 \\cdot n_k$.\n    - Computing the full weighted sum of values for one query has cost $n_k \\cdot d$.\n    - Copying a single value vector when pruning has cost $d$.\n    - A downstream per-token cost $C_{\\mathrm{tail}}$ is incurred when no pruning occurs and is fully skipped when pruning occurs.\n  Under this model, for each query, the baseline cost is $(n_k \\cdot d) + (2 \\cdot n_k) + (n_k \\cdot d) + C_{\\mathrm{tail}}$, while the pruned-path cost is $(n_k \\cdot d) + (2 \\cdot n_k) + d$.\n\nUse the following fixed data for all test cases:\n- Dimensionality $d = 3$.\n- Number of queries $n_q = 3$ and number of keys $n_k = 3$.\n- Keys (already unit-norm): $k_1 = [1, 0, 0]$, $k_2 = [0, 1, 0]$, $k_3 = [0, 0, 1]$.\n- Values: $v_1 = [1, 0, 0]$, $v_2 = [0, 1, 0]$, $v_3 = [0, 0, 1]$.\n- Queries: $q_1 = [2.0, -2.0, -2.0]$, $q_2 = [0.0, 1.8, -1.8]$, $q_3 = [-1.8, 0.0, 1.8]$.\n\nFor softmax stability, subtract $\\max_j s_j$ from the logits before exponentiation; this stabilizes the computation without changing the probabilities.\n\nTest suite:\n- Case 1: threshold $\\tau = 0.7$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 2: threshold $\\tau = 0.95$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 3: threshold $\\tau = 0.0$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 4: threshold $\\tau = 1.0$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 5: threshold $\\tau = 0.6$, downstream cost $C_{\\mathrm{tail}} = 50$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list $[S, L]$ with $S$ the speedup (as a decimal float) and $L$ the accuracy loss (as a decimal float). For example: $[[S_1, L_1],[S_2, L_2],[S_3, L_3],[S_4, L_4],[S_5, L_5]]$.", "solution": "The problem requires an analysis of a certainty-based pruning heuristic for the Scaled Dot-Product Attention (SDPA) mechanism. We will first establish the baseline computation for SDPA, then apply the specified pruning rule, and finally quantify the trade-off between computational speedup and accuracy loss for several test cases.\n\nThe problem provides the following data:\n- The dimension of the vector space is $d=3$.\n- The number of query vectors is $n_q=3$.\n- The number of key vectors is $n_k=3$.\n- The query matrix $Q \\in \\mathbb{R}^{n_q \\times d}$ is given by:\n$$\nQ = \\begin{pmatrix} 2.0 & -2.0 & -2.0 \\\\ 0.0 & 1.8 & -1.8 \\\\ -1.8 & 0.0 & 1.8 \\end{pmatrix}\n$$\n- The key matrix $K \\in \\mathbb{R}^{n_k \\times d}$ is given by:\n$$\nK = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n- The value matrix $V \\in \\mathbb{R}^{n_k \\times d}$ is given by:\n$$\nV = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nIt is noted that the key vectors (rows of $K$) are unit-norm.\n\nThe core operation of SDPA is defined as:\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n$$\n\nLet's dissect this computation for a single query vector $q_i$ (the $i$-th row of $Q$).\n\n1.  **Scaled Dot-Product Scores**: For each query $q_i$, we compute its dot product with every key vector $k_j$ (the $j$-th row of $K$) and scale by the inverse square root of the dimension $d$. These are the attention scores or logits, $s^{(i)}_j$.\n    $$\n    s^{(i)}_j = \\frac{q_i \\cdot k_j}{\\sqrt{d}}\n    $$\n    In matrix form, we compute the matrix of scores $M \\in \\mathbb{R}^{n_q \\times n_k}$ as $M = \\frac{QK^T}{\\sqrt{d}}$. With $d=3$, we have:\n    $$\n    M = \\frac{1}{\\sqrt{3}} QK^T = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2.0 & -2.0 & -2.0 \\\\ 0.0 & 1.8 & -1.8 \\\\ -1.8 & 0.0 & 1.8 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}^T \\approx \\begin{pmatrix} 1.1547 & -1.1547 & -1.1547 \\\\ 0.0 & 1.0392 & -1.0392 \\\\ -1.0392 & 0.0 & 1.0392 \\end{pmatrix}\n    $$\n\n2.  **Attention Probabilities**: The scores are converted into probabilities $\\alpha^{(i)}_j$ using the softmax function, applied row-wise to the matrix $M$. For numerical stability, we subtract the maximum score of each row before exponentiation. For the $i$-th query:\n    $$\n    \\alpha^{(i)}_j = \\frac{\\exp(s^{(i)}_j - \\max_l s^{(i)}_l)}{\\sum_{l=1}^{n_k} \\exp(s^{(i)}_l - \\max_l s^{(i)}_l)}\n    $$\n    Let $A \\in \\mathbb{R}^{n_q \\times n_k}$ be the matrix of attention probabilities. Its rows $A_{i,:}$ are the probability distributions $\\{\\alpha^{(i)}_j\\}_{j=1}^{n_k}$.\n    Applying this to $M$, we obtain the attention matrix $A$:\n    $$\n    A \\approx \\begin{pmatrix} 0.8343 & 0.0828 & 0.0828 \\\\ 0.2392 & 0.6762 & 0.0846 \\\\ 0.0846 & 0.2392 & 0.6762 \\end{pmatrix}\n    $$\n\n3.  **Baseline Output Vectors**: The output vector $o_i$ for query $q_i$ is the weighted sum of the value vectors, using the attention probabilities as weights.\n    $$\n    o_i = \\sum_{j=1}^{n_k} \\alpha^{(i)}_j v_j\n    $$\n    In matrix form, the baseline output matrix $O \\in \\mathbb{R}^{n_q \\times d}$ is $O = AV$. Since in this problem $V$ is the identity matrix, the baseline output matrix is simply the attention matrix, $O = A$.\n\n4.  **Certainty-Based Pruning**: The pruning rule is applied to each query $q_i$.\n    - Let $j^* = \\arg\\max_j \\alpha^{(i)}_j$ be the index of the key with the highest attention probability.\n    - If $\\alpha^{(i)}_{j^*} > \\tau$, the rule triggers. The pruned output $o'_i$ is approximated as the single value vector corresponding to the most-attended key: $o'_i = v_{j^*}$.\n    - Otherwise, the full attention output is used: $o'_i = o_i$.\n\n5.  **Metric Calculation**:\n    - **Accuracy Loss ($L$)**: The accuracy loss is the average squared Euclidean distance between the baseline and pruned outputs:\n      $$\n      L = \\frac{1}{n_q} \\sum_{i=1}^{n_q} \\|o_i - o'_i\\|^2\n      $$\n    - **Speedup ($S$)**: The speedup is the ratio of total baseline cost to total pruned path cost. The costs per query are defined as:\n      - Baseline cost: $C_{\\text{base}} = 2n_k d + 2n_k + C_{\\text{tail}} = 2(3)(3) + 2(3) + C_{\\text{tail}} = 24 + C_{\\text{tail}}$.\n      - Pruned-path cost: $C_{\\text{pruned}} = n_k d + 2n_k + d = (3)(3) + 2(3) + 3 = 18$.\n      - Let $P$ be the set of indices of pruned queries. The total pruned cost is $\\sum_{i \\in P} C_{\\text{pruned}} + \\sum_{i \\notin P} C_{\\text{base}}$.\n      - The speedup is $S = \\frac{n_q \\cdot C_{\\text{base}}}{\\sum_{i \\in P} C_{\\text{pruned}} + \\sum_{i \\notin P} C_{\\text{base}}}$.\n\nWe now apply this procedure to each test case.\n\n**General Calculations:**\nThe maximum attention probabilities for each query are:\n- For $q_1$: $\\max_j \\alpha^{(1)}_j \\approx 0.8343$.\n- For $q_2$: $\\max_j \\alpha^{(2)}_j \\approx 0.6762$.\n- For $q_3$: $\\max_j \\alpha^{(3)}_j \\approx 0.6762$.\n\n**Case 1: $\\tau = 0.7$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 24 + 500 = 524$. $C_{\\text{pruned}} = 18$.\n- For $q_1$: $0.8343 > 0.7$, so we prune. $j^*=1$. $o'_1 = v_1 = [1,0,0]$. Cost is $18$.\n- For $q_2$: $0.6762 \\le 0.7$, no pruning. $o'_2 = o_2$. Cost is $524$.\n- For $q_3$: $0.6762 \\le 0.7$, no pruning. $o'_3 = o_3$. Cost is $524$.\n- Total pruned cost = $18 + 524 + 524 = 1066$.\n- Total baseline cost = $3 \\times 524 = 1572$.\n- $S_1 = 1572 / 1066 \\approx 1.47467$.\n- Loss: $L_1 = \\frac{1}{3} (\\|o_1 - v_1\\|^2 + \\|o_2 - o_2\\|^2 + \\|o_3 - o_3\\|^2) = \\frac{1}{3} \\|o_1 - v_1\\|^2 \\approx \\frac{1}{3} (0.04120) \\approx 0.01373$.\n\n**Case 2: $\\tau = 0.95$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$.\n- For all queries, $\\max_j \\alpha^{(i)}_j \\le 0.95$. No queries are pruned.\n- Total pruned cost = $3 \\times 524 = 1572$.\n- Total baseline cost = $1572$.\n- $S_2 = 1572 / 1572 = 1.0$.\n- Loss: No pruning occurs, so $o'_i = o_i$ for all $i$. $L_2 = 0.0$.\n\n**Case 3: $\\tau = 0.0$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$, $C_{\\text{pruned}} = 18$.\n- For all queries, $\\max_j \\alpha^{(i)}_j > 0.0$. All queries are pruned.\n- $j^*$ for $q_1$ is $1$; for $q_2$ is $2$; for $q_3$ is $3$. $o'_1=v_1, o'_2=v_2, o'_3=v_3$.\n- Total pruned cost = $3 \\times 18 = 54$.\n- Total baseline cost = $1572$.\n- $S_3 = 1572 / 54 \\approx 29.11111$.\n- Loss: $L_3 = \\frac{1}{3} (\\|o_1 - v_1\\|^2 + \\|o_2 - v_2\\|^2 + \\|o_3 - v_3\\|^2) \\approx \\frac{1}{3}(0.04120 + 0.16921 + 0.16921) \\approx 0.12654$.\n\n**Case 4: $\\tau = 1.0$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$.\n- Since $\\sum_j \\alpha^{(i)}_j = 1$, $\\max_j \\alpha^{(i)}_j \\le 1$. The rule $\\max > 1.0$ can never trigger. No queries are pruned.\n- Result is identical to Case 2: $S_4 = 1.0$, $L_4 = 0.0$.\n\n**Case 5: $\\tau = 0.6$, $C_{\\text{tail}} = 50$**\n- $C_{\\text{base}} = 24 + 50 = 74$. $C_{\\text{pruned}} = 18$.\n- For all queries, $\\max_j \\alpha^{(i)}_j > 0.6$. All queries are pruned.\n- Total pruned cost = $3 \\times 18 = 54$.\n- Total baseline cost = $3 \\times 74 = 222$.\n- $S_5 = 222 / 54 \\approx 4.11111$.\n- Loss: The loss calculation is independent of $C_{\\text{tail}}$. All queries are pruned, similar to Case 3. $L_5 = L_3 \\approx 0.12654$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes speedup and accuracy loss for a certainty-based pruning heuristic\n    in Scaled Dot-Product Attention.\n    \"\"\"\n    \n    # Define fixed data from the problem statement.\n    d = 3.0\n    nq = 3\n    nk = 3\n    \n    Q = np.array([\n        [2.0, -2.0, -2.0],\n        [0.0, 1.8, -1.8],\n        [-1.8, 0.0, 1.8]\n    ])\n    \n    K = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    \n    V = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    # Step 1: Compute baseline SDPA output.\n    # Scaled dot-product scores\n    sqrt_d = np.sqrt(d)\n    scaled_scores = (Q @ K.T) / sqrt_d\n    \n    # Numerically stable softmax to get attention probabilities\n    max_scores = scaled_scores.max(axis=1, keepdims=True)\n    exp_scores = np.exp(scaled_scores - max_scores)\n    attention_probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    \n    # Baseline output vectors\n    baseline_outputs = attention_probs @ V\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.7, 500),  # Case 1\n        (0.95, 500), # Case 2\n        (0.0, 500),  # Case 3\n        (1.0, 500),  # Case 4\n        (0.6, 50),   # Case 5\n    ]\n    \n    results = []\n    \n    for tau, C_tail in test_cases:\n        # Define costs based on the model\n        cost_base = 2 * nk * d + 2 * nk + C_tail\n        cost_pruned = nk * d + 2 * nk + d\n        \n        total_pruned_cost = 0.0\n        total_squared_error = 0.0\n        \n        pruned_outputs = np.zeros_like(baseline_outputs)\n        \n        for i in range(nq):\n            probs_i = attention_probs[i]\n            max_prob = np.max(probs_i)\n            \n            is_pruned = max_prob > tau\n            \n            baseline_output_i = baseline_outputs[i]\n            \n            if is_pruned:\n                # Pruning rule triggers\n                max_idx = np.argmax(probs_i)\n                pruned_output_i = V[max_idx]\n                total_pruned_cost += cost_pruned\n            else:\n                # Full computation\n                pruned_output_i = baseline_output_i\n                total_pruned_cost += cost_base\n                \n            # Calculate squared Euclidean distance for this query\n            squared_error_i = np.sum((baseline_output_i - pruned_output_i)**2)\n            total_squared_error += squared_error_i\n            \n        # Calculate final metrics for the test case\n        # Accuracy Loss\n        accuracy_loss = total_squared_error / nq\n        \n        # Speedup\n        total_baseline_cost = nq * cost_base\n        speedup = total_baseline_cost / total_pruned_cost if total_pruned_cost > 0 else float('inf')\n        \n        results.append([speedup, accuracy_loss])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{s},{l}]\" for s, l in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3172461"}]}