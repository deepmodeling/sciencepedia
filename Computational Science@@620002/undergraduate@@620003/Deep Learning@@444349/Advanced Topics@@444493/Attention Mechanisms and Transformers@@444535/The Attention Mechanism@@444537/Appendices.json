{"hands_on_practices": [{"introduction": "At its heart, self-attention is a powerful mechanism for routing information based on content-based similarity. This exercise strips away the typical complexities of a full Transformer architecture to reveal this core function. By analyzing a simplified scenario where queries, keys, and values are identical ($Q=K=V=X$) and the input sequence has a uniform similarity structure, you will derive the conditions under which attention learns to either copy inputs or average them, providing fundamental intuition about its behavior [@problem_id:3180941].", "problem": "Consider a single-head self-attention layer using the scaled dot-product mechanism with no positional encodings and no residual connections. Let the input sequence be $X = (x_{1}, x_{2}, \\dots, x_{n})$, where each token embedding $x_{i} \\in \\mathbb{R}^{d}$. The queries, keys, and values are defined by $Q = K = X$ and $V = X$. The attention logits are constructed by scaling pairwise dot products with a nonnegative inverse-temperature parameter $s \\geq 0$, so that for each pair $\\{i,j\\}$ the logit is $L_{ij} = s \\, \\langle x_{i}, x_{j} \\rangle$, and the row-wise attention weights are $A_{ij} = \\exp(L_{ij}) \\big/ \\sum_{k=1}^{n} \\exp(L_{ik})$. The output of the attention layer is $Y = A V$, where $A$ is the matrix of row-wise normalized weights.\n\nAssume the following low-complexity similarity structure for the sequence: there exist real numbers $m$ and $c$ such that $\\langle x_{i}, x_{i} \\rangle = m$ for all $i$, and $\\langle x_{i}, x_{j} \\rangle = c$ for all $i \\neq j$, with $m > c$. In this setting, an identity-like behavior is characterized by each query placing almost-all mass on its own key, quantified by the diagonal attention weights satisfying $A_{ii} \\geq 1 - \\tau$ for a prescribed tolerance $0  \\tau  1$. Derive, from the definitions stated above and without invoking any additional architectural shortcuts, an explicit closed-form lower bound on the inverse-temperature $s$ that guarantees $A_{ii} \\geq 1 - \\tau$ for all rows when $n \\geq 2$, expressed in terms of $n$, $m$, $c$, and $\\tau$.\n\nThen, instantiate your bound for the concrete case $n = 6$, $m = 1$, $c = 0.1$, and $\\tau = 0.02$. Express the final answer as a single closed-form expression, and do not round.\n\nFinally, briefly justify, using only the same base definitions, what the attention weights become when the data are maximally low-entropy in the sense that $x_{1} = x_{2} = \\dots = x_{n}$ (you are not required to provide a numerical value for this justification).", "solution": "The problem requires the derivation of a lower bound on the inverse-temperature scaling parameter $s$ that guarantees a specified level of self-attention, and an analysis of a degenerate case. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. We may therefore proceed with a formal solution.\n\nThe primary task is to find the minimum value of $s$ such that the diagonal attention weights $A_{ii}$ are at least $1 - \\tau$ for a given tolerance $\\tau \\in (0, 1)$. The attention weights are defined by the softmax function applied row-wise to the matrix of logits $L_{ij}$. For any row $i$, the diagonal weight $A_{ii}$ is given by:\n$$\nA_{ii} = \\frac{\\exp(L_{ii})}{\\sum_{k=1}^{n} \\exp(L_{ik})}\n$$\nThe logits are $L_{ij} = s \\langle x_{i}, x_{j} \\rangle$, with $s \\geq 0$. Using the provided similarity structure, we have $\\langle x_{i}, x_{i} \\rangle = m$ and $\\langle x_{i}, x_{j} \\rangle = c$ for all $i \\neq j$. This allows us to express the logits as $L_{ii} = sm$ and $L_{ij} = sc$ for $i \\neq j$.\n\nSubstituting these into the expression for $A_{ii}$, we find that the denominator can be separated into the term for $k=i$ and the $n-1$ terms for $k \\neq i$:\n$$\n\\sum_{k=1}^{n} \\exp(L_{ik}) = \\exp(L_{ii}) + \\sum_{k=1, k \\neq i}^{n} \\exp(L_{ik}) = \\exp(sm) + (n-1)\\exp(sc)\n$$\nThe expression for $A_{ii}$ thus becomes independent of the index $i$:\n$$\nA_{ii} = \\frac{\\exp(sm)}{\\exp(sm) + (n-1)\\exp(sc)}\n$$\nThe condition to be satisfied is $A_{ii} \\geq 1 - \\tau$. We can directly form the inequality:\n$$\n\\frac{\\exp(sm)}{\\exp(sm) + (n-1)\\exp(sc)} \\geq 1 - \\tau\n$$\nSince the denominator is a sum of positive terms, and $1-\\tau  0$, we can multiply both sides by the denominator without altering the inequality's direction:\n$$\n\\exp(sm) \\geq (1 - \\tau) \\left( \\exp(sm) + (n-1)\\exp(sc) \\right)\n$$\nDistributing the $(1-\\tau)$ term:\n$$\n\\exp(sm) \\geq (1-\\tau)\\exp(sm) + (n-1)(1-\\tau)\\exp(sc)\n$$\nCollecting the terms involving $\\exp(sm)$:\n$$\n\\exp(sm) - (1-\\tau)\\exp(sm) \\geq (n-1)(1-\\tau)\\exp(sc)\n$$\n$$\n\\tau \\exp(sm) \\geq (n-1)(1-\\tau)\\exp(sc)\n$$\nTo isolate the dependence on $s$, we can divide by $\\tau \\exp(sc)$. Since $\\tau  0$ and $\\exp(sc)  0$, this is a valid operation that preserves the inequality:\n$$\n\\frac{\\exp(sm)}{\\exp(sc)} \\geq \\frac{(n-1)(1-\\tau)}{\\tau}\n$$\nUsing the property of exponents, $\\exp(a)/\\exp(b) = \\exp(a-b)$:\n$$\n\\exp(s(m-c)) \\geq \\frac{(n-1)(1-\\tau)}{\\tau}\n$$\nThe argument of the logarithm on the right side is positive because $n \\geq 2$ implies $n-1  0$, and $0  \\tau  1$ implies $1-\\tau  0$. We can take the natural logarithm of both sides. As $\\ln(x)$ is a monotonically increasing function, the inequality is preserved:\n$$\n\\ln\\left(\\exp(s(m-c))\\right) \\geq \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n$$\ns(m-c) \\geq \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\nThe problem states $m  c$, which implies $m - c  0$. We can therefore divide by $(m-c)$ to solve for $s$:\n$$\ns \\geq \\frac{1}{m-c} \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\nThis inequality provides the explicit closed-form lower bound on $s$ that guarantees the desired identity-like behavior.\n\nNext, we instantiate this bound for the concrete case where $n = 6$, $m = 1$, $c = 0.1$, and $\\tau = 0.02$.\nThe components of the expression are:\n- $m-c = 1 - 0.1 = 0.9 = \\frac{9}{10}$\n- $n-1 = 6-1 = 5$\n- $1-\\tau = 1 - 0.02 = 0.98 = \\frac{98}{100}$\n- $\\tau = 0.02 = \\frac{2}{100}$\n\nSubstituting these into the argument of the logarithm:\n$$\n\\frac{(n-1)(1-\\tau)}{\\tau} = \\frac{5 \\times 0.98}{0.02} = \\frac{5 \\times 98}{2} = 5 \\times 49 = 245\n$$\nThe lower bound for $s$ is therefore:\n$$\ns \\geq \\frac{1}{0.9} \\ln(245) = \\frac{10}{9} \\ln(245)\n$$\nThis is the required closed-form expression for the instantiated bound.\n\nFinally, we are asked to justify what the attention weights become when the data are maximally low-entropy, i.e., $x_{1} = x_{2} = \\dots = x_{n}$.\nIn this scenario, all input vectors are identical. Let $x_i = x_0$ for all $i \\in \\{1, \\dots, n\\}$. The pairwise dot products become uniform:\n$$\n\\langle x_{i}, x_{j} \\rangle = \\langle x_{0}, x_{0} \\rangle \\quad \\forall i, j\n$$\nLet this constant dot product value be $M = \\langle x_{0}, x_{0} \\rangle$. The logits are then all equal:\n$$\nL_{ij} = s \\langle x_{i}, x_{j} \\rangle = sM \\quad \\forall i, j\n$$\nThe attention weight $A_{ij}$ is calculated as:\n$$\nA_{ij} = \\frac{\\exp(L_{ij})}{\\sum_{k=1}^{n} \\exp(L_{ik})} = \\frac{\\exp(sM)}{\\sum_{k=1}^{n} \\exp(sM)}\n$$\nThe denominator is a sum of $n$ identical terms, so $\\sum_{k=1}^{n} \\exp(sM) = n \\exp(sM)$.\nSubstituting this into the expression for $A_{ij}$ yields:\n$$\nA_{ij} = \\frac{\\exp(sM)}{n \\exp(sM)} = \\frac{1}{n}\n$$\nThus, when all input tokens are identical, the attention mechanism cannot distinguish between them. The resulting attention matrix $A$ has every entry equal to $1/n$, indicating a uniform distribution of attention from each query to all keys. This represents a state of maximal uncertainty, which is the logical antithesis of the identity-like concentration sought in the main part of the problem.", "answer": "$$\\boxed{\\frac{10}{9}\\ln(245)}$$", "id": "3180941"}, {"introduction": "The softmax function is more than just a normalization step; its temperature parameter, $\\tau$, is a crucial dial for controlling the \"sharpness\" of the attention distribution. This practice explores the mathematical relationship between the smooth, differentiable softmax attention and a hard, discrete argmax selection. You will demonstrate how, as $\\tau \\to 0$, softmax attention converges to selecting the single best item, and why the smoother behavior at $\\tau > 0$ is critical for building robust models that are less brittle to small input perturbations [@problem_id:3100390].", "problem": "Consider an attention mechanism used in Transformer models based on Scaled Dot-Product Attention (SDPA). Let there be a query vector $q \\in \\mathbb{R}^d$, a set of key vectors $\\{k_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, and a corresponding set of value vectors $\\{v_i\\}_{i=1}^n \\subset \\mathbb{R}^m$. Define the score of key $k_i$ as $s_i = q^\\top k_i$. For a temperature parameter $\\tau  0$, define the softmax weights $w_i(\\tau)$ by normalizing exponentials of the scores, and define the attention output $a(\\tau)$ as the weighted sum of values using these weights. Define the argmax attention output $a^\\star$ by selecting the value corresponding to the index $i^\\star$ that maximizes the score. Starting only from the fundamental definitions of the exponential function, limits, and normalization by sums, derive the limiting relationship between $a(\\tau)$ and $a^\\star$ as $\\tau \\to 0$, and analyze the brittleness of argmax selection under small perturbations of $q$.\n\nYour task is to write a complete, runnable program that performs the following computations on a specified test suite and produces a single line of output in the exact format described below. No physical units, angle units, or percentages are involved; all outputs must be numeric floats, integers, booleans, or lists thereof.\n\nDefinitions to use:\n- Softmax weights: $w_i(\\tau) = \\dfrac{\\exp\\!\\left(s_i/\\tau\\right)}{\\sum_{j=1}^n \\exp\\!\\left(s_j/\\tau\\right)}$, where $s_i = q^\\top k_i$.\n- Softmax attention output: $a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i$.\n- Argmax index: $i^\\star = \\operatorname{argmax}_{1 \\leq i \\leq n} s_i$ (assume an arbitrary but fixed tie-breaking rule).\n- Argmax attention output: $a^\\star = v_{i^\\star}$.\n\nTest suite specification:\n\n- Test Case $1$ (unique maximum, approximation quality across temperatures):\n  - Dimension $d = 2$, value dimension $m = 2$.\n  - Query $q = (1.0, 0.2)$.\n  - Keys $k_1 = (1.0, 0.0)$, $k_2 = (0.5, 0.7)$, $k_3 = (-0.5, 0.3)$, $k_4 = (0.1, 0.2)$.\n  - Values $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$, $v_4 = (-1.0, 1.0)$.\n  - Temperatures $\\tau \\in \\{1.0, 0.5, 0.1, 0.01, 10^{-6}\\}$.\n  - For each $\\tau$, compute the Euclidean norm $\\|a(\\tau) - a^\\star\\|_2$ and return these as a list of floats ordered by $\\tau$.\n\n- Test Case $2$ (near-tie brittleness under small query perturbations and smoothing by temperature):\n  - Dimension $d = 2$, value dimension $m = 2$.\n  - Base query $q_0 = (1.0, 0.0)$, perturbed queries $q_+ = (1.0, \\epsilon)$ and $q_- = (1.0, -\\epsilon)$ with $\\epsilon = 10^{-3}$.\n  - Keys $k_1 = (1.0, 0.0)$, $k_2 = (1.0, \\eta)$ with $\\eta = 1000.0$.\n  - Values $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$.\n  - Temperatures for comparison $\\tau \\in \\{0.5, 0.1, 0.01\\}$.\n  - Compute:\n    - The integer indicator of argmax flip under perturbation, defined as $I = 1$ if $\\operatorname{argmax}_i q_+^\\top k_i \\neq \\operatorname{argmax}_i q_-^\\top k_i$, and $I = 0$ otherwise.\n    - For each specified $\\tau$, compute the Euclidean distance $\\|a_{+}(\\tau) - a_{-}(\\tau)\\|_2$ between the two softmax attention outputs for $q_+$ and $q_-$.\n  - Return a list containing $I$ followed by the three distances ordered by the listed $\\tau$ values.\n\n- Test Case $3$ (exact tie, symmetry of softmax weights):\n  - Dimension $d = 2$, value dimension $m = 2$.\n  - Query $q = (1.0, 0.0)$.\n  - Keys $k_1 = (1.0, 0.0)$, $k_2 = (1.0, 0.0)$, $k_3 = (0.0, 1.0)$ (so $k_1$ and $k_2$ tie exactly).\n  - Values $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$.\n  - Temperature $\\tau = 0.1$.\n  - Compute the softmax weights $w_1(\\tau)$ and $w_2(\\tau)$ and return the float $|w_1(\\tau) - w_2(\\tau)|$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result appearing in order as described:\n  - The first element is the list of floats from Test Case $1$.\n  - The second element is the list from Test Case $2$ containing one integer followed by three floats.\n  - The third element is the single float from Test Case $3$.\n- For example, the final line should look like: $[[x_1,x_2,x_3,x_4,x_5],[i,d_1,d_2,d_3],y]$, where each $x_j$, $i$, $d_j$, and $y$ are the computed numeric results for the specified test suite.", "solution": "The problem is well-posed, scientifically grounded, and internally consistent. It provides all necessary definitions and data for a unique and meaningful solution. We proceed with the derivation and computational implementation.\n\nThe analysis is structured in two parts. First, we derive the limiting behavior of the softmax attention output, $a(\\tau)$, as the temperature parameter $\\tau$ approaches zero. Second, we analyze the stability of the attention mechanism under small perturbations, contrasting the argmax selection with the temperate softmax.\n\n**1. Limiting Behavior of Softmax Attention as $\\tau \\to 0^+$**\n\nThe softmax attention output is defined as a weighted sum of value vectors:\n$$ a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i $$\nwhere the weights $w_i(\\tau)$ are given by the temperature-scaled softmax function applied to the scores $s_i = q^\\top k_i$:\n$$ w_i(\\tau) = \\frac{\\exp(s_i/\\tau)}{\\sum_{j=1}^n \\exp(s_j/\\tau)} $$\nWe wish to compute the limit $\\lim_{\\tau \\to 0^+} a(\\tau)$. The analysis hinges on the behavior of the weights $w_i(\\tau)$ in this limit.\n\nLet $s_{\\max} = \\max_{1 \\leq j \\leq n} s_j$ be the maximum score. To avoid numerical overflow and facilitate the limit calculation, we can factor out the term $\\exp(s_{\\max}/\\tau)$ from the numerator and denominator:\n$$ w_i(\\tau) = \\frac{\\exp(s_{\\max}/\\tau) \\exp((s_i - s_{\\max})/\\tau)}{\\exp(s_{\\max}/\\tau) \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} = \\frac{\\exp((s_i - s_{\\max})/\\tau)}{\\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} $$\nNow, we examine the limit of the exponential term $\\exp((s_j - s_{\\max})/\\tau)$ as $\\tau \\to 0^+$. The behavior depends on whether $s_j$ is equal to $s_{\\max}$.\n\nCase 1: $s_j = s_{\\max}$. In this case, the exponent is $(s_j - s_{\\max})/\\tau = 0/\\tau = 0$. Thus, $\\exp((s_j - s_{\\max})/\\tau) = \\exp(0) = 1$.\n\nCase 2: $s_j  s_{\\max}$. Here, the difference $s_j - s_{\\max}$ is a strictly negative constant. As $\\tau \\to 0^+$, the exponent $(s_j - s_{\\max})/\\tau \\to -\\infty$. Consequently, $\\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) = 0$.\n\nLet $I_{\\max} = \\{i \\mid s_i = s_{\\max}\\}$ be the set of indices corresponding to the maximum score, and let $M = |I_{\\max}|$ be the number of such indices. We can now evaluate the limit of the denominator of $w_i(\\tau)$:\n$$ \\lim_{\\tau \\to 0^+} \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau) = \\sum_{j \\in I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) + \\sum_{j \\notin I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) $$\n$$ = \\sum_{j \\in I_{\\max}} 1 + \\sum_{j \\notin I_{\\max}} 0 = M $$\nThe limit of the numerator $\\exp((s_i - s_{\\max})/\\tau)$ is $1$ if $i \\in I_{\\max}$ and $0$ if $i \\notin I_{\\max}$.\n\nCombining these results, the limit of the weight $w_i(\\tau)$ is:\n$$ \\lim_{\\tau \\to 0^+} w_i(\\tau) = \\begin{cases} 1/M  \\text{if } i \\in I_{\\max} \\\\ 0  \\text{if } i \\notin I_{\\max} \\end{cases} $$\nThis demonstrates that as $\\tau \\to 0^+$, the softmax function concentrates all of its probability mass on the indices that achieve the maximum score, distributing it uniformly among them. It effectively becomes a \"soft argmax\".\n\nNow, we can find the limit of the attention output $a(\\tau)$:\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\lim_{\\tau \\to 0^+} \\sum_{i=1}^n w_i(\\tau) v_i = \\sum_{i=1}^n \\left(\\lim_{\\tau \\to 0^+} w_i(\\tau)\\right) v_i $$\nSubstituting the limiting weights:\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\sum_{i \\in I_{\\max}} \\frac{1}{M} v_i + \\sum_{i \\notin I_{\\max}} 0 \\cdot v_i = \\frac{1}{M} \\sum_{i \\in I_{\\max}} v_i $$\nThe limiting attention output is the arithmetic mean of the value vectors whose corresponding keys achieved the maximum score.\n\nThe problem defines the argmax attention output as $a^\\star = v_{i^\\star}$, where $i^\\star = \\operatorname{argmax}_{i} s_i$ with a fixed tie-breaking rule. If the maximum score is unique, then $I_{\\max} = \\{i^\\star\\}$ and $M=1$. In this common scenario, the limit simplifies to:\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\frac{1}{1} v_{i^\\star} = v_{i^\\star} = a^\\star $$\nThus, when the maximum score is unique, the softmax attention output converges to the argmax attention output.\n\n**2. Brittleness of Argmax Selection and the Regularizing Role of Temperature**\n\nThe argmax selection is inherently discontinuous. The choice of $i^\\star$ depends on the ordering of the scores $s_i(q) = q^\\top k_i$. A small perturbation to the query, $q' = q + \\delta q$, induces a change in the scores: $s_i(q') = s_i(q) + (\\delta q)^\\top k_i$. If two scores, say $s_j(q)$ and $s_k(q)$, are very close, even an infinitesimal perturbation $\\delta q$ can alter their relative order, causing $i^\\star$ to jump from one index to another. This results in a discrete change in the output $a^\\star$, from $v_j$ to $v_k$, which can be substantial. This instability is a form of \"brittleness\".\n\nIn contrast, for any strictly positive temperature $\\tau  0$, the softmax attention output $a(\\tau)$ is a continuous and differentiable function of the query $q$. The weights $w_i(\\tau)$ are smooth functions of the scores $s_i$, which are linear in $q$. Consequently, a small change $\\delta q$ in the query leads to a correspondingly small change in the output $a(\\tau)$. The change is governed by the gradient: $a(q+\\delta q, \\tau) \\approx a(q, \\tau) + (\\nabla_q a(q, \\tau)) \\cdot \\delta q$.\n\nA higher temperature $\\tau$ has a smoothing or regularizing effect. It \"flattens\" the softmax distribution, meaning the weights $w_i(\\tau)$ are less sensitive to small differences in scores. This makes the output $a(\\tau)$ more robust to minor perturbations in $q$. Conversely, as $\\tau \\to 0$, the softmax function becomes steeper, and the gradient of $a(\\tau)$ with respect to $q$ can become very large, particularly in directions that affect the difference between near-maximum scores. In this regime, the smooth softmax output $a(\\tau)$ closely approximates the brittle, discontinuous behavior of the argmax output $a^\\star$, exhibiting high sensitivity to small perturbations that can flip the ordering of top scores, as demonstrated in Test Case $2$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three test cases specified in the problem statement regarding\n    Scaled Dot-Product Attention.\n    \"\"\"\n\n    def softmax_attention_with_weights(q, K, V, tau):\n        \"\"\"\n        Computes the softmax attention output and corresponding weights.\n\n        Args:\n            q (np.ndarray): Query vector of shape (d,).\n            K (np.ndarray): Key matrix of shape (n, d).\n            V (np.ndarray): Value matrix of shape (n, m).\n            tau (float): Temperature parameter.\n\n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: Attention output vector of shape (m,).\n                - np.ndarray: Softmax weights vector of shape (n,).\n        \"\"\"\n        scores = K @ q\n        # Scale scores by temperature as per the definition.\n        scaled_scores = scores / tau\n        # Use the max-subtraction trick for numerical stability of exp.\n        stable_scores = scaled_scores - np.max(scaled_scores)\n        exps = np.exp(stable_scores)\n        weights = exps / np.sum(exps)\n        # Attention output is the weighted sum of value vectors.\n        # V.T is (m, n), weights is (n,). Result is (m,).\n        attention_output = V.T @ weights\n        return attention_output, weights\n\n    # --- Test Case 1: Unique maximum, approximation quality ---\n    q1 = np.array([1.0, 0.2])\n    K1 = np.array([[1.0, 0.0], [0.5, 0.7], [-0.5, 0.3], [0.1, 0.2]])\n    V1 = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [-1.0, 1.0]])\n    taus1 = [1.0, 0.5, 0.1, 0.01, 1e-6]\n    \n    scores1 = K1 @ q1\n    # np.argmax respects the \"fixed tie-breaking rule\" by taking the first occurrence.\n    i_star1 = np.argmax(scores1)\n    a_star1 = V1[i_star1]\n    \n    results1 = []\n    for tau in taus1:\n        a_tau, _ = softmax_attention_with_weights(q1, K1, V1, tau)\n        error = np.linalg.norm(a_tau - a_star1)\n        results1.append(error)\n\n    # --- Test Case 2: Near-tie brittleness ---\n    epsilon = 1e-3\n    q_plus = np.array([1.0, epsilon])\n    q_minus = np.array([1.0, -epsilon])\n    eta = 1000.0\n    K2 = np.array([[1.0, 0.0], [1.0, eta]])\n    V2 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    taus2 = [0.5, 0.1, 0.01]\n    \n    scores_plus = K2 @ q_plus\n    scores_minus = K2 @ q_minus\n    argmax_plus = np.argmax(scores_plus)\n    argmax_minus = np.argmax(scores_minus)\n    I = 1 if argmax_plus != argmax_minus else 0\n    \n    results2 = [I]\n    for tau in taus2:\n        a_plus_tau, _ = softmax_attention_with_weights(q_plus, K2, V2, tau)\n        a_minus_tau, _ = softmax_attention_with_weights(q_minus, K2, V2, tau)\n        dist = np.linalg.norm(a_plus_tau - a_minus_tau)\n        results2.append(dist)\n        \n    # --- Test Case 3: Exact tie, symmetry of weights ---\n    q3 = np.array([1.0, 0.0])\n    K3 = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n    # V3 is not strictly needed as we only compute weights.\n    # Pass a dummy V matrix of the correct shape.\n    V3 = np.zeros((3, 2))\n    tau3 = 0.1\n    \n    _, weights3 = softmax_attention_with_weights(q3, K3, V3, tau3)\n    w1_tau = weights3[0]\n    w2_tau = weights3[1]\n    result3 = abs(w1_tau - w2_tau)\n    \n    # --- Format and print the final output ---\n    # The output format is a single line string `[[...],[...],...]`.\n    str_res1 = f\"[{','.join(map(str, results1))}]\"\n    str_res2 = f\"[{','.join(map(str, results2))}]\"\n    str_res3 = str(result3)\n    \n    print(f\"[{str_res1},{str_res2},{str_res3}]\")\n\nsolve()\n```", "id": "3100390"}, {"introduction": "In autoregressive models like large language models, attention must be causal: a token at a given position cannot attend to future tokens. While a mask can enforce this during the forward pass, the true test lies in the backward pass during training. This advanced practice challenges you to implement the reverse-mode automatic differentiation (the Vector-Jacobian Product) from first principles. By doing so, you will verify and prove to yourself how causal masking guarantees that gradients do not flow \"from the future,\" solidifying your understanding of how these powerful models are trained effectively [@problem_id:3100434].", "problem": "Implement a reverse-mode automatic differentiation (AD) Vector–Jacobian Product (VJP) for a single-head scaled dot-product attention mechanism with causal masking. The goal is to demonstrate, from first principles, that backpropagation through the attention with a causal mask does not propagate gradients from a token at time index $t$ into parameters associated with future tokens at indices $j$ with $j > t$. Use the following formal setting and requirements.\n\nFundamental base and definitions:\n- Let $L$ denote the sequence length and let $d$ denote the dimensionality of queries and keys. Let $d_v$ denote the dimensionality of values.\n- Let $\\mathbf{Q} \\in \\mathbb{R}^{L \\times d}$, $\\mathbf{K} \\in \\mathbb{R}^{L \\times d}$, and $\\mathbf{V} \\in \\mathbb{R}^{L \\times d_v}$ denote the query, key, and value matrices, respectively.\n- Define the scaled dot-product score matrix by\n$$\n\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\, \\mathbf{Q} \\, \\mathbf{K}^\\top \\in \\mathbb{R}^{L \\times L}.\n$$\n- Define the causal mask $\\mathbf{M} \\in \\{0,1\\}^{L \\times L}$ by $\\mathbf{M}_{i,j} = 1$ if and only if $j \\le i$, and $\\mathbf{M}_{i,j} = 0$ otherwise.\n- Construct the masked scores $\\widetilde{\\mathbf{S}}$ by setting $\\widetilde{\\mathbf{S}}_{i,j} = \\mathbf{S}_{i,j}$ for $j \\le i$ and $\\widetilde{\\mathbf{S}}_{i,j} = -\\infty$ for $j > i$.\n- Let $\\operatorname{softmax}$ denote the softmax function applied row-wise. Define the attention weights by\n$$\n\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}}) \\in \\mathbb{R}^{L \\times L},\n$$\nand the attention output by\n$$\n\\mathbf{Y} = \\mathbf{A} \\, \\mathbf{V} \\in \\mathbb{R}^{L \\times d_v}.\n$$\n\nTask and constraints:\n- Implement a complete, runnable program that:\n  - Computes the forward attention $\\mathbf{Y}$ with the causal mask as defined.\n  - Implements the reverse-mode AD Vector–Jacobian Product (VJP) that maps an upstream cotangent $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\in \\mathbb{R}^{L \\times d_v}$ to downstream cotangents $(\\mathbf{G_Q}, \\mathbf{G_K}, \\mathbf{G_V}) = \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}} \\right)$, using only the chain rule, the definition of the softmax function, and linear algebra identities. Do not use any external automatic differentiation frameworks; rely only on the fundamental rules of differentiation and linear algebra to derive and implement the VJP formulas.\n  - Uses numerically stable softmax computations by subtracting the row-wise maximum before exponentiation.\n- Testing objective:\n  - For each test, set the upstream cotangent $\\mathbf{G_Y}$ to be all zeros except a single nonzero entry at a chosen time index $t$ and value channel $c$, namely $\\mathbf{G_Y}_{t, c} = 1$ and all other entries equal to $0$.\n  - Verify that the VJP computed gradients $\\mathbf{G_K}$ and $\\mathbf{G_V}$ are zero for all future token rows $j$ with $j > t$ under causal masking. Use a numerical tolerance of $\\varepsilon = 10^{-12}$ for the zero check.\n- Angle units are not applicable to this task.\n- There are no physical units in this problem.\n\nTest suite and parameter values:\n- Use the following deterministic test cases, each specified by $(L, d, d_v, t, c, s, \\text{causal})$, where $s$ is a random seed for constructing inputs, and `causal` is a boolean indicating whether to use the causal mask.\n  - Test $1$: $(L, d, d_v, t, c, s, \\text{causal}) = (4, 3, 2, 1, 0, 0, \\text{True})$.\n  - Test $2$: $(L, d, d_v, t, c, s, \\text{causal}) = (1, 3, 2, 0, 1, 1, \\text{True})$.\n  - Test $3$: $(L, d, d_v, t, c, s, \\text{causal}) = (5, 4, 3, 4, 2, 2, \\text{True})$.\n  - Test $4$ (control without masking): $(L, d, d_v, t, c, s, \\text{causal}) = (4, 3, 2, 1, 1, 3, \\text{False})$.\n- For each test case, construct $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ by sampling independent standard normal entries using the given seed $s$ for reproducibility, and then scaling by a factor of $0.5$ for numerical moderation. Formally, set the pseudorandom number generator seed to $s$, sample $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ with independent entries from $\\mathcal{N}(0,1)$, and then replace each sampled matrix by $0.5$ times that matrix.\n- For each test, define the upstream cotangent $\\mathbf{G_Y}$ as the zero matrix in $\\mathbb{R}^{L \\times d_v}$ except for $\\mathbf{G_Y}_{t,c} = 1$.\n- For each test, the program must evaluate whether, under causal masking, gradients to future tokens are zero by computing the boolean predicate\n$$\n\\max_{j > t} \\left( \\max\\left( \\left| \\mathbf{G_K}[j,:] \\right| \\right), \\, \\max\\left( \\left| \\mathbf{G_V}[j,:] \\right| \\right) \\right) \\le \\varepsilon,\n$$\nwhere $\\varepsilon = 10^{-12}$. If the set $\\{ j : j > t \\}$ is empty (for example, when $t = L - 1$), treat the predicate as true.\n- The final output format must be a single line containing the boolean results for the $4$ tests as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{False},\\text{True},\\text{True}]$.\n\nYour program must produce exactly one line of output in the specified format and must not read any input.", "solution": "The problem requires the implementation of a reverse-mode automatic differentiation (AD) Vector-Jacobian Product (VJP) for a single-head scaled dot-product attention mechanism, with a specific focus on demonstrating the effect of causal masking on gradient flow. We must validate that gradients from an output token at time $t$ do not propagate to input parameters associated with future tokens $j > t$.\n\nFirst, we will derive the VJP equations from first principles by applying the chain rule to the sequence of operations in the forward pass. Let $\\mathcal{L}$ be a scalar loss function. The VJP for a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ maps the cotangent (gradient) with respect to its output, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\in \\mathbb{R}^m$, to the cotangent with respect to its input, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^n$. We are given the upstream cotangent $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$ and must compute $\\mathbf{G_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}}$, $\\mathbf{G_K} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}}$, and $\\mathbf{G_V} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}}$.\n\nThe forward pass is defined by the following sequence of operations:\n1.  Scaled dot-product scores: $\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\mathbf{Q} \\mathbf{K}^\\top$\n2.  Masking: $\\widetilde{\\mathbf{S}} = \\operatorname{mask}(\\mathbf{S}, \\mathbf{M})$ (where masked elements are set to $-\\infty$)\n3.  Row-wise softmax: $\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}})$\n4.  Output computation: $\\mathbf{Y} = \\mathbf{A} \\mathbf{V}$\n\nWe will derive the VJPs by reversing this sequence.\n\n**Step 1: VJP for Output Computation $\\mathbf{Y} = \\mathbf{A} \\mathbf{V}$**\n\nThis operation is a standard matrix multiplication. We are given $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$. Using the VJP rules for matrix multiplication ($C = AB \\implies \\mathbf{G_A} = \\mathbf{G_C} B^\\top, \\mathbf{G_B} = A^\\top \\mathbf{G_C}$), we find the cotangents for $\\mathbf{A}$ and $\\mathbf{V}$:\n$$\n\\mathbf{G_A} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}} = \\mathbf{G_Y} \\mathbf{V}^\\top\n$$\n$$\n\\mathbf{G_V} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}} = \\mathbf{A}^\\top \\mathbf{G_Y}\n$$\n\n**Step 2: VJP for Row-wise Softmax $\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}})$**\n\nThe softmax function is applied to each row of $\\widetilde{\\mathbf{S}}$. For a single row vector $\\mathbf{a} = \\operatorname{softmax}(\\mathbf{s})$, the VJP mapping $\\mathbf{g_a}$ to $\\mathbf{g_s}$ is derived from the Jacobian of the softmax function, $J_{jk} = \\frac{\\partial a_j}{\\partial s_k} = a_j(\\delta_{jk} - a_k)$. The VJP is:\n$$\n(\\mathbf{g_s})_k = \\sum_j (\\mathbf{g_a})_j J_{jk} = \\sum_j (\\mathbf{g_a})_j a_j(\\delta_{jk} - a_k) = (\\mathbf{g_a})_k a_k - a_k \\sum_j (\\mathbf{g_a})_j a_j\n$$\nIn vector notation, this is $\\mathbf{g_s} = \\mathbf{a} \\odot (\\mathbf{g_a} - (\\mathbf{g_a} \\cdot \\mathbf{a}))$, where $\\odot$ is the element-wise product and $\\cdot$ is the dot product. Applying this to each row $i$ of the matrices $\\mathbf{A}$ and $\\mathbf{G_A}$:\n$$\n(\\mathbf{G}_{\\widetilde{\\mathbf{S}}})_{i,:} = \\mathbf{A}_{i,:} \\odot \\left( \\mathbf{G_A}_{i,:} - \\left( \\sum_{k=1}^L \\mathbf{A}_{i,k} \\mathbf{G_A}_{i,k} \\right) \\right)\n$$\nThis computes $\\mathbf{G}_{\\widetilde{\\mathbf{S}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\mathbf{S}}}$ from $\\mathbf{G_A}$.\n\n**Step 3: VJP for Masking $\\widetilde{\\mathbf{S}} = \\operatorname{mask}(\\mathbf{S}, \\mathbf{M})$**\n\nIn the forward pass, if $\\mathbf{M}_{i,j} = 0$ (i.e., $j  i$), then $\\widetilde{\\mathbf{S}}_{i,j}$ is set to the constant $-\\infty$. If $\\mathbf{M}_{i,j} = 1$ (i.e., $j \\le i$), then $\\widetilde{\\mathbf{S}}_{i,j} = \\mathbf{S}_{i,j}$. The gradient is passed backward accordingly:\n$$\n(\\mathbf{G_S})_{i,j} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{S}_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\mathbf{S}}_{i,j}} \\frac{\\partial \\widetilde{\\mathbf{S}}_{i,j}}{\\partial \\mathbf{S}_{i,j}}\n$$\nThe derivative $\\frac{\\partial \\widetilde{\\mathbf{S}}_{i,j}}{\\partial \\mathbf{S}_{i,j}}$ is $1$ if $j \\le i$ and $0$ if $j  i$. Therefore, the gradient only flows through the unmasked elements:\n$$\n(\\mathbf{G_S})_{i,j} = \\begin{cases} (\\mathbf{G}_{\\widetilde{\\mathbf{S}}})_{i,j}  \\text{if } j \\le i \\\\ 0  \\text{if } j  i \\end{cases}\n$$\nIf causal masking is disabled, $\\mathbf{G_S} = \\mathbf{G}_{\\widetilde{\\mathbf{S}}}$.\n\n**Step 4: VJP for Scaled Dot-Product $\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\mathbf{Q} \\mathbf{K}^\\top$**\n\nThis step is again a matrix multiplication, scaled by a constant $\\frac{1}{\\sqrt{d}}$. Applying the VJP rules:\n$$\n\\mathbf{G_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}} = \\frac{1}{\\sqrt{d}} \\mathbf{G_S} (\\mathbf{K}^\\top)^\\top = \\frac{1}{\\sqrt{d}} \\mathbf{G_S} \\mathbf{K}\n$$\nFor $\\mathbf{K}$, we first find the gradient with respect to $\\mathbf{K}^\\top$ and then transpose the result.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}^\\top} = \\frac{1}{\\sqrt{d}} \\mathbf{Q}^\\top \\mathbf{G_S}\n$$\n$$\n\\mathbf{G_K} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}^\\top} \\right)^\\top = \\frac{1}{\\sqrt{d}} (\\mathbf{Q}^\\top \\mathbf{G_S})^\\top = \\frac{1}{\\sqrt{d}} \\mathbf{G_S}^\\top \\mathbf{Q}\n$$\n\n**Analysis of Causal Gradient Flow**\nThe testing objective is to verify that for an upstream cotangent $\\mathbf{G_Y}$ that is non-zero only at row $t$, the downstream cotangents $\\mathbf{G_K}$ and $\\mathbf{G_V}$ are zero for all rows $j > t$.\n\n1.  **Gradient to $\\mathbf{V}$**: $\\mathbf{G_V} = \\mathbf{A}^\\top \\mathbf{G_Y}$. The $j$-th row of $\\mathbf{G_V}$ is $(\\mathbf{G_V})_{j,:} = \\sum_{i=0}^{L-1} (\\mathbf{A}^\\top)_{j,i} (\\mathbf{G_Y})_{i,:} = \\sum_{i=0}^{L-1} \\mathbf{A}_{i,j} (\\mathbf{G_Y})_{i,:}$. Since $\\mathbf{G_Y}$ is non-zero only at row $t$, this sum reduces to $(\\mathbf{G_V})_{j,:} = \\mathbf{A}_{t,j} (\\mathbf{G_Y})_{t,:}$. With causal masking, the attention weights matrix $\\mathbf{A}$ is lower triangular, meaning $\\mathbf{A}_{i,j} = 0$ for $j  i$. Thus, for our case where $i=t$, we have $\\mathbf{A}_{t,j} = 0$ for all $j  t$. Consequently, $(\\mathbf{G_V})_{j,:}$ is a zero vector for all $j  t$.\n\n2.  **Gradient to $\\mathbf{K}$**: Tracing the gradient propagation to $\\mathbf{K}$ reveals a similar logic.\n    - $\\mathbf{G_A} = \\mathbf{G_Y} \\mathbf{V}^\\top$. Since $\\mathbf{G_Y}$ has only row $t$ non-zero, $\\mathbf{G_A}$ will also have only row $t$ non-zero.\n    - $\\mathbf{G}_{\\widetilde{\\mathbf{S}}}$ is computed from $\\mathbf{G_A}$, so it also will have only row $t$ non-zero.\n    - $\\mathbf{G_S}$ is computed from $\\mathbf{G}_{\\widetilde{\\mathbf{S}}}$ by applying the causal mask. This means $(\\mathbf{G_S})_{i,j} = 0$ for all $i \\ne t$, and also $(\\mathbf{G_S})_{t,j} = 0$ for all $j  t$. In summary, $\\mathbf{G_S}$ is a matrix with non-zero entries existing only at indices $(t,j)$ where $j \\le t$.\n    - Finally, $\\mathbf{G_K} = \\frac{1}{\\sqrt{d}} \\mathbf{G_S}^\\top \\mathbf{Q}$. The $j$-th row of $\\mathbf{G_K}$ is given by $(\\mathbf{G_K})_{j,:} = \\frac{1}{\\sqrt{d}} \\sum_{i=0}^{L-1} (\\mathbf{G_S}^\\top)_{j,i} \\mathbf{Q}_{i,:} = \\frac{1}{\\sqrt{d}} \\sum_{i=0}^{L-1} (\\mathbf{G_S})_{i,j} \\mathbf{Q}_{i,:}$.\n    - Since $(\\mathbf{G_S})_{i,j}$ is non-zero only for $i=t$, this becomes $(\\mathbf{G_K})_{j,:} = \\frac{1}{\\sqrt{d}} (\\mathbf{G_S})_{t,j} \\mathbf{Q}_{t,:}$.\n    - For any future token $j  t$, we know from the structure of $\\mathbf{G_S}$ that $(\\mathbf{G_S})_{t,j} = 0$. Therefore, $(\\mathbf{G_K})_{j,:}$ must be a zero vector for all $j  t$.\n\nThis confirms from first principles that causal masking correctly prevents information from a given time step from flowing backward to influence the parameters of future time steps. The implementation will codify these derived VJP equations and verify this property numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests the Vector-Jacobian Product (VJP) for scaled\n    dot-product attention with and without causal masking, verifying the\n    gradient flow properties of causal attention.\n    \"\"\"\n    \n    # Test suite format: (L, d, d_v, t, c, s, causal)\n    # L: sequence length, d: query/key dim, d_v: value dim,\n    # t: time index for gradient, c: channel index for gradient,\n    # s: random seed, causal: boolean for causal mask.\n    test_cases = [\n        (4, 3, 2, 1, 0, 0, True),\n        (1, 3, 2, 0, 1, 1, True),\n        (5, 4, 3, 4, 2, 2, True),\n        (4, 3, 2, 1, 1, 3, False),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        L, d, d_v, t, c, s, causal = case\n\n        # 1. Initialize inputs\n        rng = np.random.default_rng(s)\n        Q = rng.standard_normal((L, d), dtype=np.float64) * 0.5\n        K = rng.standard_normal((L, d), dtype=np.float64) * 0.5\n        V = rng.standard_normal((L, d_v), dtype=np.float64) * 0.5\n\n        # 2. Forward Pass\n        \n        # S = (Q @ K.T) / sqrt(d)\n        scores = (Q @ K.T) / np.sqrt(d)\n        \n        # Apply causal mask if specified\n        masked_scores = np.copy(scores)\n        if causal:\n            mask = np.triu(np.ones((L, L), dtype=bool), k=1)\n            masked_scores[mask] = -np.inf\n\n        # Numerically stable softmax\n        stable_scores = masked_scores - np.max(masked_scores, axis=1, keepdims=True)\n        A = np.exp(stable_scores)\n        A /= np.sum(A, axis=1, keepdims=True)\n        \n        # Y = A @ V\n        _Y = A @ V\n\n        # 3. Backward Pass (VJP)\n\n        # Initialize upstream gradient G_Y\n        G_Y = np.zeros((L, d_v), dtype=np.float64)\n        G_Y[t, c] = 1.0\n\n        # Backprop through Y = A @ V\n        # G_A = G_Y @ V.T\n        # G_V = A.T @ G_Y\n        G_A = G_Y @ V.T\n        G_V = A.T @ G_Y\n\n        # Backprop through A = softmax(S_tilde)\n        # G_S_tilde = A * (G_A - sum(G_A * A, axis=1))\n        row_wise_dot = np.sum(G_A * A, axis=1, keepdims=True)\n        G_S_tilde = A * (G_A - row_wise_dot)\n\n        # Backprop through masking\n        G_S = np.copy(G_S_tilde)\n        if causal:\n            # Gradient is zero for masked-out elements\n            mask = np.triu(np.ones((L, L), dtype=bool), k=1)\n            G_S[mask] = 0.0\n\n        # Backprop through S = (Q @ K.T) / sqrt(d)\n        # G_Q = (G_S @ K) / sqrt(d)\n        # G_K = (G_S.T @ Q) / sqrt(d)\n        sqrt_d = np.sqrt(d)\n        G_Q = (G_S @ K) / sqrt_d\n        G_K = (G_S.T @ Q) / sqrt_d\n\n        # 4. Verification\n        \n        # Check if gradients to future tokens are zero.\n        # This is vacuously true if t is the last token index.\n        if t >= L - 1:\n            predicate_holds = True\n        else:\n            # Extract gradients corresponding to future tokens (j > t)\n            G_K_future = G_K[t + 1 :, :]\n            G_V_future = G_V[t + 1 :, :]\n\n            # Compute max absolute value of these future gradients\n            max_grad_k = np.max(np.abs(G_K_future)) if G_K_future.size > 0 else 0\n            max_grad_v = np.max(np.abs(G_V_future)) if G_V_future.size > 0 else 0\n            \n            # Check if the maximum is below the tolerance\n            tolerance = 1e-12\n            predicate_holds = max(max_grad_k, max_grad_v) = tolerance\n            \n        results.append(predicate_holds)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results).lower()}]\")\n\nsolve()\n\n```", "id": "3100434"}]}