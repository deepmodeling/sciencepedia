{"hands_on_practices": [{"introduction": "The softmax function is the heart of the attention mechanism, converting raw scores into a probability distribution. While its mathematical form is elegant, implementing it on a computer reveals challenges with finite-precision arithmetic. This first practice [@problem_id:3192585] dives into the crucial concept of numerical stability, guiding you to derive and implement the `log-sum-exp` trick to prevent the overflow and underflow errors that can plague naive implementations, especially when using lower-precision formats like fp16.", "problem": "Consider a self-attention scoring setup where a set of energies $e_{ij}$ is computed for queries indexed by $i$ against keys indexed by $j$, and attention weights are obtained by applying the softmax function row-wise to the energies. The softmax function for a row vector $\\mathbf{e}_i$ is defined from first principles as $softmax(\\mathbf{e}_i)_j = \\exp(e_{ij}) \\big/ \\sum_{k} \\exp(e_{ik})$, where $\\exp$ denotes the exponential function and $\\sum$ denotes finite summation. The computation of $\\log \\sum_{j} \\exp(e_{ij})$ is central to numerical analysis of softmax since $\\log$ is the inverse of $\\exp$ and converts products into sums. The Institute of Electrical and Electronics Engineers (IEEE) $754$ standard floating-point formats fp16 (binary $16$) and fp32 (binary $32$) differ in precision and dynamic range, which affects numerical stability and rounding error when evaluating $\\exp$, $\\log$, and $softmax$.\n\nStarting only from the definitions of the exponential function $\\exp$, the natural logarithm $\\log$, and the softmax function $softmax$, derive a numerically stable transformation for computing $\\log \\sum_{j} \\exp(e_{ij})$ for each row $i$ and explain why the stable transformation reduces overflow and underflow without changing the mathematical value. Then, using that stable transformation, design an algorithm that computes row-wise $softmax(\\mathbf{e}_i)$ in a specified floating-point precision while mitigating catastrophic numerical issues.\n\nImplement the algorithm to quantify numerical error across floating-point precisions by comparing fp16 and fp32 stabilized softmax outputs to a high-precision fp64 (binary $64$) stabilized reference. For each test case below, produce the following four quantities:\n- The maximum absolute difference between fp16 stabilized softmax and the fp64 stabilized reference across all entries of the test matrix (a real number).\n- The maximum absolute difference between fp32 stabilized softmax and the fp64 stabilized reference across all entries of the test matrix (a real number).\n- The maximum absolute difference, across rows, between the naive direct computation $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ and the stabilized computation of the same quantity in fp64 (a real number; if the naive value is not finite for a row, treat its contribution to this maximum as $nan$).\n- A boolean indicating whether any row’s naive $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ was not finite in fp64 (i.e., overflow to $+\\infty$ or propagation of $nan$).\n\nUse the following test suite of energy matrices, each understood as row-wise inputs $\\mathbf{e}_i$ to $softmax$. All entries are in unitless real values. You must treat each matrix row independently in computations. The matrices are:\n- Case $1$ (moderate values): $$E^{(1)} = \\begin{bmatrix} -1 & 0 & 1 & 2 \\\\ 0.5 & -0.5 & 3 & -3 \\end{bmatrix}.$$\n- Case $2$ (extreme range): $$E^{(2)} = \\begin{bmatrix} 1000 & -1000 & 0 & 1 \\\\ 88 & 87 & 86 & 85 \\end{bmatrix}.$$\n- Case $3$ (uniform inputs): $$E^{(3)} = \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}.$$\n- Case $4$ (very negative values): $$E^{(4)} = \\begin{bmatrix} -1000 & -1001 & -999 & -1200 \\\\ -50 & -60 & -70 & -80 \\end{bmatrix}.$$\n\nAlgorithmic requirements:\n- For each matrix $E^{(k)}$, compute a stabilized fp64 reference softmax row-wise from the definition of $softmax$ and your derived stable transformation.\n- For each matrix $E^{(k)}$, compute stabilized $softmax$ row-wise in fp16 and fp32 using the same algorithm but performing the arithmetic in the specified precision.\n- For each matrix $E^{(k)}$, compute the naive fp64 values $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ row-wise directly from the definitions, and compare to the stabilized fp64 values of the same quantity.\n\nAnswer format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $k$-th element is a list of four items corresponding to Case $k$ in the order described above. For example, the final output should look like $$[ [a_1,b_1,c_1,d_1], [a_2,b_2,c_2,d_2], [a_3,b_3,c_3,d_4], [a_4,b_4,c_4,d_4] ],$$ with no spaces inserted beyond what is necessary to delimit numbers and booleans.\n\nNo external input is required. All computations are unitless real numbers. Angles are not involved. Express all difference quantities as real numbers. The final line must be the only output.", "solution": "The goal is to compute row-wise attention weights by applying the softmax function to energies $e_{ij}$ while maintaining numerical stability and then to measure precision-dependent numerical error. We begin with the core definitions and properties:\n\n$1.$ Definition of softmax for a row vector $\\mathbf{e}_i$:\n$$softmax(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}.$$\nSoftmax maps real-valued scores $e_{ij}$ to nonnegative weights that sum to $1$, a requirement for attention distributions in self-attention.\n\n$2.$ Fundamental properties of the exponential and logarithm:\nThe exponential function $\\exp$ is strictly increasing and maps $\\mathbb{R}$ to $(0,\\infty)$. The natural logarithm $\\log$ is its inverse on $(0,\\infty)$ and satisfies $\\log(ab) = \\log(a) + \\log(b)$ for $a>0$ and $b>0$. These properties allow controlled manipulation of sums of exponentials.\n\n$3.$ Numerical issue:\nDirectly computing $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ can overflow if any $e_{ij}$ is very large (making $\\exp(e_{ij})$ exceed representable range) or underflow if $e_{ij}$ is very negative (making $\\exp(e_{ij})$ round to $0$). Such overflow or underflow compromises the correctness of the softmax denominator and thereby of $softmax(\\mathbf{e}_i)$.\n\nTo derive a numerically stable transformation for $\\log \\sum_{j} \\exp(e_{ij})$, we use factorization based on the logarithm-exponential relationship. Let $m_i$ be the maximum element of the row $\\mathbf{e}_i$, i.e., $m_i = \\max_j e_{ij}$. Then:\n\n$$\n\\sum_{j} \\exp(e_{ij}) = \\sum_{j} \\exp\\left((e_{ij} - m_i) + m_i\\right) = \\sum_{j} \\left[\\exp(e_{ij} - m_i)\\cdot \\exp(m_i)\\right] = \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i).\n$$\n\nApplying $\\log$ and the property $\\log(ab) = \\log(a) + \\log(b)$ yields\n\n$$\n\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right) = \\log\\left( \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i) \\right) = m_i + \\log \\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right).\n$$\n\nThis transformation is numerically stable because:\n- The term $e_{ij} - m_i \\le 0$ for all $j$, so the largest shifted value is $0$ and its exponential is exactly $1$, avoiding overflow even if $m_i$ is large.\n- Very negative values of $e_{ij}$ produce $e_{ij} - m_i \\ll 0$ whose exponentials are very small; underflowing these contributions to $0$ has only a small effect on the sum because they are already negligible relative to the largest term.\n- The overall mathematical value is preserved by algebraic identity; the transformation does not alter $\\log \\sum_{j} \\exp(e_{ij})$.\n\nFor stabilized softmax, we use the same shift. For a row $\\mathbf{e}_i$, define $m_i = \\max_j e_{ij}$ and compute\n\n$$\nsoftmax(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij} - m_i)}{\\sum_{k} \\exp(e_{ik} - m_i)}.\n$$\n\nThis leaves the softmax values unchanged mathematically because the common multiplicative factor $\\exp(m_i)$ cancels in numerator and denominator, but it dramatically improves numerical stability by preventing overflow in the numerator and denominator.\n\nFloating-point considerations:\n- Under the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard, fp16 (binary $16$) has a smaller dynamic range and fewer fraction bits than fp32 (binary $32$), which in turn is less precise than fp64 (binary $64$). Smaller precision increases rounding error and increases the likelihood of underflow in very small exponentials and overflow in very large exponentials if unshifted.\n- By computing softmax with the stabilized algorithm and performing arithmetic in fp16, fp32, and fp64, we can quantify the impact of precision on the final attention weights by measuring the maximum absolute difference from a high-precision fp64 reference.\n\nAlgorithmic design:\n- For each test matrix $E^{(k)}$, compute a stabilized fp64 reference softmax row-wise using the shift-by-maximum technique described above.\n- For the same inputs, compute stabilized softmax in fp16 and fp32 by performing the same operations with arrays in the specified dtype. Convert the outputs to fp64 for comparison.\n- Separately, for each row, compute the naive fp64 value $\\log\\left( \\sum_{j} \\exp(e_{ij}) \\right)$ directly. This can produce non-finite values (such as $+\\infty$) when there is overflow in $\\exp(e_{ij})$, particularly for very large positive $e_{ij}$. Compare this naive value to the stabilized fp64 value $m_i + \\log\\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right)$ and report the maximum absolute difference across rows. If the naive computation is not finite for a row, that row’s contribution to the maximum is $nan$, and the boolean overflow indicator is set to true if any such non-finite values occur.\n\nTest suite coverage rationale:\n- Case $1$ uses moderate values, representing a typical regime where naive computations are safe and rounding errors are small.\n- Case $2$ includes a row with an extremely large positive value $1000$ mixed with very negative values $-1000$ and a moderate range $0$ to $1$, which causes overflow in the naive $\\exp$ and tests the necessity of stabilization; the second row uses values $88$ to $85$, large yet finite in $fp64$.\n- Case $3$ uses uniform zeros, producing exactly uniform softmax weights and testing precision effects on symmetry.\n- Case $4$ uses very negative values to study underflow behavior in lower precision formats and sensitivity of softmax to tiny exponentials.\n\nOutput specification:\n- For each matrix $E^{(k)}$, output a list $[a_k, b_k, c_k, d_k]$ where $a_k$ is the maximum absolute difference for fp16 stabilized softmax against the fp64 reference, $b_k$ is the corresponding quantity for fp32, $c_k$ is the maximum absolute difference between naive and stabilized $\\log \\sum_{j} \\exp(e_{ij})$ in fp64 aggregated over rows (with non-finite naive values contributing $nan$), and $d_k$ is a boolean indicating whether any row’s naive computation was not finite. Aggregate the four case results into a single comma-separated list enclosed in square brackets.\n\nThis principled design links the mathematical identities of $\\exp$ and $\\log$ to the algorithmic choices used in self-attention softmax computations, demonstrating both the theoretical stability and the empirical precision-dependent error characteristics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logsumexp_naive(row: np.ndarray) -> float:\n    \"\"\"Compute naive log(sum(exp(row))) in float64.\"\"\"\n    row64 = row.astype(np.float64)\n    s = np.sum(np.exp(row64))\n    return float(np.log(s))\n\ndef logsumexp_stable(row: np.ndarray) -> float:\n    \"\"\"Compute stable logsumexp in float64: m + log(sum(exp(row - m))).\"\"\"\n    row64 = row.astype(np.float64)\n    m = np.max(row64)\n    shifted = row64 - m\n    s = np.sum(np.exp(shifted))\n    return float(m + np.log(s))\n\ndef softmax_stable(matrix: np.ndarray, dtype: np.dtype) -> np.ndarray:\n    \"\"\"\n    Compute row-wise stabilized softmax in the specified dtype.\n    Returns results as float64 for comparison.\n    \"\"\"\n    x = matrix.astype(dtype)\n    # Row-wise max\n    m = np.max(x, axis=1, keepdims=True)\n    # Shift and exponentiate in dtype; cast back to dtype explicitly\n    shifted = (x - m).astype(dtype)\n    exps = np.exp(shifted).astype(dtype)\n    sums = np.sum(exps, axis=1, keepdims=True).astype(dtype)\n    # Avoid division by zero: if sum is zero (underflow), result remains zero\n    soft = exps / sums\n    return soft.astype(np.float64)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef any_nonfinite(arr: np.ndarray) -> bool:\n    \"\"\"Check if any element is non-finite.\"\"\"\n    return bool(np.any(~np.isfinite(arr)))\n\ndef format_item(item):\n    \"\"\"Format item without spaces, recursively for lists.\"\"\"\n    if isinstance(item, list):\n        return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n    elif isinstance(item, float):\n        # Ensure Python's default float string is used (includes 'nan' or 'inf' if present)\n        return str(item)\n    elif isinstance(item, (int, np.integer)):\n        return str(int(item))\n    elif isinstance(item, (bool, np.bool_)):\n        return \"True\" if bool(item) else \"False\"\n    else:\n        return str(item)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([[-1.0, 0.0, 1.0, 2.0],\n                  [0.5, -0.5, 3.0, -3.0]], dtype=np.float64),\n        np.array([[1000.0, -1000.0, 0.0, 1.0],\n                  [88.0, 87.0, 86.0, 85.0]], dtype=np.float64),\n        np.array([[0.0, 0.0, 0.0, 0.0],\n                  [0.0, 0.0, 0.0, 0.0]], dtype=np.float64),\n        np.array([[-1000.0, -1001.0, -999.0, -1200.0],\n                  [-50.0, -60.0, -70.0, -80.0]], dtype=np.float64),\n    ]\n\n    results = []\n    for E in test_cases:\n        # Reference stabilized softmax in float64\n        soft_ref = softmax_stable(E, np.float64)\n\n        # Stabilized softmax in fp16 and fp32\n        soft16 = softmax_stable(E, np.float16)\n        soft32 = softmax_stable(E, np.float32)\n\n        # Error metrics (max absolute difference w.r.t. fp64 reference)\n        err16 = max_abs_diff(soft16, soft_ref)\n        err32 = max_abs_diff(soft32, soft_ref)\n\n        # Naive vs stabilized logsumexp in float64, row-wise\n        naive_vals = []\n        stable_vals = []\n        nonfinite_flag = False\n        diffs = []\n        for i in range(E.shape[0]):\n            row = E[i, :]\n            naive = logsumexp_naive(row)\n            stable = logsumexp_stable(row)\n            naive_vals.append(naive)\n            stable_vals.append(stable)\n            if not np.isfinite(naive):\n                nonfinite_flag = True\n                diffs.append(np.nan)\n            else:\n                diffs.append(abs(naive - stable))\n\n        # Max absolute difference (nan propagated if present)\n        # If any nan present, np.nanmax ignores nan and returns max of finite values.\n        # If all are nan, result is nan.\n        diffs_arr = np.array(diffs, dtype=np.float64)\n        if np.all(np.isnan(diffs_arr)):\n            max_logsumexp_diff = float(np.nan)\n        else:\n            max_logsumexp_diff = float(np.nanmax(diffs_arr))\n\n        results.append([err16, err32, max_logsumexp_diff, nonfinite_flag])\n\n    # Final print statement in the exact required format: nested list without extra spaces.\n    print(format_item(results))\n\nsolve()\n```", "id": "3192585"}, {"introduction": "Beyond numerical correctness, the efficiency of computation is paramount in deep learning. The core of self-attention involves the matrix product $(QK^{\\top})V$, but the associative property of matrix multiplication allows an alternative grouping: $Q(K^{\\top}V)$. In this exercise [@problem_id:3192615], you will analyze why these mathematically equivalent forms have vastly different performance characteristics, particularly when the sequence length $n$ is much larger than the head dimension $d_h$. By modeling floating-point operations (FLOPs) and memory traffic, you will gain insight into the computational bottlenecks of attention and the principles behind modern optimizations like FlashAttention.", "problem": "In a single-head scaled dot-product self-attention block, the core linear algebraic chain before any nonlinearity can be written as a product of three matrices with compatible dimensions. Consider a batched, multi-head setting where queries, keys, and values are stored as tensors with shape $B \\times H \\times n \\times d_h$, where $B$ is the batch size, $H$ is the number of heads, $n$ is the sequence length, and $d_h$ is the per-head embedding dimension. For each batch-head pair, define matrices $Q, K, V \\in \\mathbb{R}^{n \\times d_h}$. Ignore any scaling factors and nonlinearities, and focus solely on the associative product of three matrices.\n\nYou are tasked with analyzing two mathematically equivalent orders of evaluation for the batched product:\n- Baseline order: $(QK^{\\top})V$.\n- Reordered (associative) order: $Q(K^{\\top}V)$.\n\nAssume the following foundational facts:\n- A General Matrix–Matrix Multiply (GEMM) of shape $(m \\times k)$ times $(k \\times n)$ performs $2mkn$ floating-point operations.\n- For each GEMM, every input element is read exactly once and every output element is written exactly once. Any intermediate result that is produced by one GEMM and consumed by another must be written to and then read from main memory exactly once. Do not assume any further caching or fusion.\n- The roofline execution time for a computation is the larger of the compute time and the memory time, where compute time equals total floating-point operations divided by the peak floating-point rate, and memory time equals total bytes moved divided by the sustained memory bandwidth.\n\nConsider half-precision floating-point (FP16) tensors with element size $s = 2$ bytes. Let the hardware be a Graphics Processing Unit (GPU) with peak FP16 throughput $P = 1.0 \\times 10^{14}$ floating-point operations per second and sustained memory bandwidth $W = 1.0 \\times 10^{12}$ bytes per second. Take $B = 2$, $H = 8$, $n = 4096$, and $d_h = 64$.\n\nUsing only the principles stated above and standard matrix dimensions, do the following across all $B \\times H$ heads:\n- Derive the total floating-point operations for each order.\n- Derive the total memory traffic in bytes for each order.\n- Compute the roofline-predicted execution time for each order as the maximum of its compute-bound and memory-bound times.\n- Finally, compute the speedup factor $S$, defined as the baseline time divided by the reordered time, as a pure number.\n\nRound your final answer $S$ to four significant figures. Express the final answer as a pure number without units.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard roofline model analysis of two computational strategies for the self-attention mechanism, using clearly defined parameters and an unambiguous model of computation and memory access. The problem is valid, and a solution will be derived.\n\nThe core task is to compare the performance of two evaluation orders for a batched, multi-head self-attention computation, specifically $(QK^{\\top})V$ (baseline) and $Q(K^{\\top}V)$ (reordered). We will use the provided roofline model, which defines execution time as the maximum of compute time and memory time.\n\nFirst, we define the given parameters symbolically:\n- Batch size: $B = 2$\n- Number of heads: $H = 8$\n- Sequence length: $n = 4096$\n- Per-head embedding dimension: $d_h = 64$\n- Element size for FP16: $s = 2$ bytes\n- Peak FP16 throughput: $P = 1.0 \\times 10^{14}$ FLOPS\n- Sustained memory bandwidth: $W = 1.0 \\times 10^{12}$ bytes/s\n\nThe total number of independent attention calculations is $N_{ops} = B \\times H = 2 \\times 8 = 16$.\nFor each calculation, the matrices $Q, K, V$ have dimensions $\\mathbb{R}^{n \\times d_h}$.\nA General Matrix-Matrix Multiply (GEMM) of a matrix of shape $(m \\times k)$ with a matrix of shape $(k \\times p)$ requires $2mkp$ floating-point operations (FLOPs). The total memory traffic for such an operation, accounting for reading two input matrices and writing one output matrix, is $s(mk + kp + mp)$ bytes.\n\n### Baseline Order Analysis: $(QK^{\\top})V$\n\nThis evaluation proceeds in two steps across all $N_{ops}$ heads.\n\n**Step 1: Compute the attention matrix $A = QK^{\\top}$**\nFor each head, this is a product of $Q \\in \\mathbb{R}^{n \\times d_h}$ and $K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$. The resulting matrix $A$ has shape $(n \\times n)$.\n- FLOPs per head: With $m=n, k=d_h, p=n$, the FLOPs are $2 \\times n \\times d_h \\times n = 2n^2d_h$.\n- Memory traffic per head: The traffic is $s(nd_h + d_h n + n^2) = s(2nd_h + n^2)$ bytes.\n\n**Step 2: Compute the output $C = AV$**\nFor each head, this is a product of $A \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times d_h}$. The resulting matrix $C$ has shape $(n \\times d_h)$.\n- FLOPs per head: With $m=n, k=n, p=d_h$, the FLOPs are $2 \\times n \\times n \\times d_h = 2n^2d_h$.\n- Memory traffic per head: The traffic is $s(n^2 + nd_h + nd_h) = s(n^2 + 2nd_h)$ bytes.\n\n**Total for Baseline Order**\nThe total FLOPs and memory traffic are the sum over the two steps, multiplied by the number of heads $N_{ops}$.\n- Total FLOPs, $F_{base}$:\n$$F_{base} = N_{ops} (2n^2d_h + 2n^2d_h) = 4BHn^2d_h$$\n$$F_{base} = 4 \\times 2 \\times 8 \\times (4096)^2 \\times 64 = 64 \\times (2^{12})^2 \\times 2^6 = 2^6 \\times 2^{24} \\times 2^6 = 2^{36} \\approx 6.872 \\times 10^{10} \\text{ FLOPs}$$\n- Total Memory Traffic, $M_{base}$:\n$$M_{base} = N_{ops} [s(2nd_h + n^2) + s(n^2 + 2nd_h)] = BHs(4nd_h + 2n^2)$$\n$$M_{base} = 16 \\times 2 \\times [4(4096)(64) + 2(4096)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^{12})^2]$$\n$$M_{base} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{24}] = 2^5 [2^{20} + 2^{25}] = 2^{25}(1+2^5) = 33 \\times 2^{25} \\approx 1.107 \\times 10^9 \\text{ bytes}$$\n\n**Baseline Roofline Time, $T_{base}$**\n- Compute Time: $T_{compute, base} = F_{base} / P = 2^{36} / 10^{14} \\approx 6.872 \\times 10^{-4}$ s.\n- Memory Time: $T_{memory, base} = M_{base} / W = (33 \\times 2^{25}) / 10^{12} \\approx 1.107 \\times 10^{-3}$ s.\n- Execution Time: $T_{base} = \\max(T_{compute, base}, T_{memory, base}) = T_{memory, base} \\approx 1.107 \\times 10^{-3}$ s.\n\n### Reordered Order Analysis: $Q(K^{\\top}V)$\n\nThis evaluation also proceeds in two steps across all $N_{ops}$ heads.\n\n**Step 1: Compute the context matrix $B = K^{\\top}V$**\nFor each head, this is a product of $K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$ and $V \\in \\mathbb{R}^{n \\times d_h}$. The resulting matrix $B$ has shape $(d_h \\times d_h)$.\n- FLOPs per head: With $m=d_h, k=n, p=d_h$, the FLOPs are $2 \\times d_h \\times n \\times d_h = 2nd_h^2$.\n- Memory traffic per head: The traffic is $s(d_h n + nd_h + d_h^2) = s(2nd_h + d_h^2)$ bytes.\n\n**Step 2: Compute the output $C = QB$**\nFor each head, this is a product of $Q \\in \\mathbb{R}^{n \\times d_h}$ and $B \\in \\mathbb{R}^{d_h \\times d_h}$. The resulting matrix $C$ has shape $(n \\times d_h)$.\n- FLOPs per head: With $m=n, k=d_h, p=d_h$, the FLOPs are $2 \\times n \\times d_h \\times d_h = 2nd_h^2$.\n- Memory traffic per head: The traffic is $s(nd_h + d_h^2 + nd_h) = s(2nd_h + d_h^2)$ bytes.\n\n**Total for Reordered Order**\n- Total FLOPs, $F_{reord}$:\n$$F_{reord} = N_{ops}(2nd_h^2 + 2nd_h^2) = 4BHnd_h^2$$\n$$F_{reord} = 4 \\times 2 \\times 8 \\times 4096 \\times (64)^2 = 64 \\times 2^{12} \\times (2^6)^2 = 2^6 \\times 2^{12} \\times 2^{12} = 2^{30} \\approx 1.074 \\times 10^9 \\text{ FLOPs}$$\n- Total Memory Traffic, $M_{reord}$:\n$$M_{reord} = N_{ops} [s(2nd_h + d_h^2) + s(2nd_h + d_h^2)] = BHs(4nd_h + 2d_h^2)$$\n$$M_{reord} = 16 \\times 2 \\times [4(4096)(64) + 2(64)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^6)^2]$$\n$$M_{reord} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{12}] = 2^5 [2^{20} + 2^{13}] = 2^{18}(2^7+1) = 129 \\times 2^{18} \\approx 3.382 \\times 10^7 \\text{ bytes}$$\n\n**Reordered Roofline Time, $T_{reord}$**\n- Compute Time: $T_{compute, reord} = F_{reord} / P = 2^{30} / 10^{14} \\approx 1.074 \\times 10^{-5}$ s.\n- Memory Time: $T_{memory, reord} = M_{reord} / W = (129 \\times 2^{18}) / 10^{12} \\approx 3.382 \\times 10^{-5}$ s.\n- Execution Time: $T_{reord} = \\max(T_{compute, reord}, T_{memory, reord}) = T_{memory, reord} \\approx 3.382 \\times 10^{-5}$ s.\n\n### Speedup Factor $S$\n\nBoth evaluation orders are memory-bound, as $T_{memory} > T_{compute}$ in both cases. The speedup factor $S$ is the ratio of their execution times.\n$$S = \\frac{T_{base}}{T_{reord}} = \\frac{T_{memory, base}}{T_{memory, reord}} = \\frac{M_{base} / W}{M_{reord} / W} = \\frac{M_{base}}{M_{reord}}$$\nSubstituting the symbolic expressions for memory traffic:\n$$S = \\frac{BHs(4nd_h + 2n^2)}{BHs(4nd_h + 2d_h^2)} = \\frac{2n(2d_h + n)}{2d_h(2n + d_h)}$$\nUsing the calculated values:\n$$S = \\frac{33 \\times 2^{25}}{129 \\times 2^{18}} = \\frac{33}{129} \\times 2^{25-18} = \\frac{3 \\times 11}{3 \\times 43} \\times 2^7 = \\frac{11}{43} \\times 128 = \\frac{1408}{43}$$\n$$S \\approx 32.744186...$$\nRounding to four significant figures, the speedup factor is $32.74$.", "answer": "$$\\boxed{32.74}$$", "id": "3192615"}, {"introduction": "Having explored the numerical and computational foundations, we now investigate the expressive power of self-attention. This practice [@problem_id:3192620] frames the classic task of sorting as a soft-alignment problem that can be solved with attention. You will construct a mechanism where queries representing sorted positions attend to keys representing original items, forming a \"soft\" permutation matrix. By quantifying the error of this soft alignment, you will build a powerful intuition for how attention performs differentiable routing and pointing, a capability that underpins its success in complex reasoning and generation tasks.", "problem": "You are given a toy sorting-by-key task designed to probe the behavior of the self-attention mechanism when used as a soft alignment that approximates permutation matrices. Consider a set of $n$ items indexed by $i \\in \\{1,\\dots,n\\}$, each with a sortable scalar key $s_i \\in \\mathbb{R}$. The goal is to express sorting as a soft alignment from target sorted positions (treated as queries) to original items (treated as keys), and to quantify how closely this soft alignment approximates the ideal permutation matrix that sorts the sequence.\n\nFundamental base and definitions to use:\n- A row-stochastic matrix is a matrix whose rows each sum to $1$ and contain nonnegative entries. Such a matrix can express a soft assignment of each query to the set of keys.\n- The Softmax function over a vector $x \\in \\mathbb{R}^m$ is defined by $\\operatorname{Softmax}(x)_j = \\dfrac{e^{x_j}}{\\sum_{\\ell=1}^m e^{x_\\ell}}$, producing a probability distribution over indices $j \\in \\{1,\\dots,m\\}$.\n- Similarity-based attention can be constructed by first computing a compatibility score between each query and key, and then applying a Softmax over keys for each query to obtain a row-stochastic alignment matrix.\n\nTask setup:\n1. Let the keys be the original scalars $k_i = s_i$.\n2. Let the queries be the sorted scalars $q_j$, obtained by sorting the original $s_i$ in nondecreasing order using a stable sort (ties broken by original index ascending). That is, if $\\pi$ is the stable sort permutation such that $s_{\\pi(1)} \\le s_{\\pi(2)} \\le \\dots \\le s_{\\pi(n)}$, then $q_j = s_{\\pi(j)}$.\n3. Define the compatibility score between a query $q_j$ and a key $k_i$ by a Gaussian-like similarity\n$$\nC_{j,i} \\;=\\; -\\frac{(q_j - k_i)^2}{2\\,\\sigma^2},\n$$\nwhere $\\sigma > 0$ is a temperature-like scale parameter that controls the sharpness of the alignment.\n4. Define the attention alignment matrix $A \\in \\mathbb{R}^{n \\times n}$ row-wise by applying the Softmax over keys for each query,\n$$\nA_{j,i} \\;=\\; \\frac{\\exp(C_{j,i})}{\\sum_{r=1}^{n} \\exp(C_{j,r})}.\n$$\n5. Define the ideal permutation matrix $P \\in \\mathbb{R}^{n \\times n}$ that sorts the original sequence into nondecreasing order (stable), by setting $P_{j,\\pi(j)} = 1$ and $P_{j,i} = 0$ for $i \\ne \\pi(j)$.\n6. Define the soft alignment error as the complement of the average probability mass placed on the correct columns:\n$$\nE \\;=\\; 1 \\;-\\; \\frac{1}{n}\\sum_{j=1}^{n} A_{j,\\pi(j)}.\n$$\nThis error satisfies $E \\in [0,1]$. Smaller $E$ indicates a closer approximation to the ideal permutation.\n\nYour program must:\n- Implement the above construction for each test case.\n- Use a numerically stable Softmax implementation (for example, by subtracting the maximum score in each row before exponentiation).\n- Use a stable sort to compute $\\pi$ as specified. In ties, the earlier index in the original sequence must come first.\n\nTest suite:\nFor each test case, you are given the list of scalars $\\{s_i\\}_{i=1}^n$ and the value of $\\sigma$. Compute and report the soft alignment error $E$ as a floating-point number.\n\n- Test case $1$: $s = [3.1,\\, 0.2,\\, 1.7,\\, 2.4]$, $\\sigma = 0.2$.\n- Test case $2$: $s = [0.5,\\, 1.0,\\, 1.5,\\, 2.0]$, $\\sigma = 0.5$.\n- Test case $3$: $s = [1.0,\\, 1.0,\\, 1.2,\\, 0.8]$, $\\sigma = 0.1$.\n- Test case $4$: $s = [2.0]$, $\\sigma = 0.3$.\n- Test case $5$: $s = [0.0,\\, 2.0,\\, 4.0,\\, 6.0]$, $\\sigma = 10.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases.\n- Each result must be a floating-point number rounded to exactly $6$ decimal places.\n- For example, a valid output line has the form $[x_1,x_2,x_3,x_4,x_5]$ where each $x_i$ is rounded to $6$ decimal places.", "solution": "The problem requires the calculation of a soft alignment error for a sorting-by-key task, modeling the process using a self-attention mechanism. The solution involves several well-defined mathematical and algorithmic steps, which I will elaborate upon before presenting the implementation.\n\nFirst, we formalize the problem. We are given a sequence of $n$ scalar keys, $S = \\{s_i\\}_{i=1}^n$, where $s_i \\in \\mathbb{R}$. The goal is to sort this sequence and represent the sorting operation as a soft alignment from target sorted positions to the original items. In the attention framework, the items to be attended to are designated as 'keys', and the items performing the attention are 'queries'.\n\n1.  **Keys and Queries Definition**: We define the 'keys' as the original scalar values, $k_i = s_i$ for $i \\in \\{1,\\dots,n\\}$. The 'queries' are defined as the target sorted values. Let $\\pi$ be the permutation that stably sorts the sequence $S$ in non-decreasing order. This means that if we apply $\\pi$ to the indices of $S$, we get a sorted sequence: $s_{\\pi(1)} \\le s_{\\pi(2)} \\le \\dots \\le s_{\\pi(n)}$. The stability of the sort implies that if $s_a = s_b$ and $a < b$, then the item originally at index $a$ will appear before the item at index $b$ in the sorted sequence. The queries are then $q_j = s_{\\pi(j)}$ for $j \\in \\{1,\\dots,n\\}$. Thus, the sequence $\\{q_j\\}_{j=1}^n$ is the sorted version of $\\{s_i\\}_{i=1}^n$.\n\n2.  **Compatibility Score**: The core of the attention mechanism is a compatibility function that scores the similarity between each query and key. The problem specifies a Gaussian-like similarity score:\n    $$\n    C_{j,i} = -\\frac{(q_j - k_i)^2}{2\\sigma^2}\n    $$\n    Here, $C_{j,i}$ measures the compatibility between the $j$-th query (the $j$-th value in the sorted sequence) and the $i$-th key (the $i$-th value in the original sequence). The parameter $\\sigma > 0$ acts as a temperature or scale. A small $\\sigma$ results in a sharply peaked score, where only keys $k_i$ very close to the query $q_j$ receive a high (close to $0$) score. A large $\\sigma$ leads to a flatter score distribution, making dissimilar keys more competitive. The maximum possible score is $0$, achieved when $q_j = k_i$.\n\n3.  **Attention Matrix Construction**: From the compatibility scores, we construct an $n \\times n$ attention matrix $A$. Each row of $A$ represents the attention distribution for a single query over all keys. This is achieved by applying the Softmax function row-wise to the compatibility matrix $C$:\n    $$\n    A_{j,i} = \\frac{\\exp(C_{j,i})}{\\sum_{r=1}^{n} \\exp(C_{j,r})}\n    $$\n    The matrix $A$ is row-stochastic, meaning its entries are non-negative and each row sums to $1$. $A_{j,i}$ can be interpreted as the probability that the $j$-th sorted position (query $q_j$) aligns with the $i$-th original item (key $k_i$). For numerical stability during computation, especially when scores in $C$ are large and negative, we subtract the maximum value of each row from all its elements before exponentiation: $A_{j,i} = \\frac{\\exp(C_{j,i} - \\max_r C_{j,r})}{\\sum_{k=1}^{n} \\exp(C_{j,k} - \\max_r C_{j,r})}$. This transformation does not alter the final probabilities but prevents floating-point overflow/underflow.\n\n4.  **Ideal Permutation and Error Metric**: The ideal sorting operation can be represented by a permutation matrix $P$, where $P_{j,i}=1$ if the $j$-th sorted element was originally at index $i$, and $P_{j,i}=0$ otherwise. Using our permutation $\\pi$, this is $P_{j, \\pi(j)} = 1$ and $P_{j,i} = 0$ for $i \\ne \\pi(j)$. Our attention matrix $A$ is a \"soft\" version of this ideal permutation matrix $P$.\n    The soft alignment error, $E$, quantifies how much $A$ deviates from $P$. It is defined as the complement of the average attention probability placed on the correct original items:\n    $$\n    E = 1 - \\frac{1}{n}\\sum_{j=1}^{n} A_{j,\\pi(j)}\n    $$\n    The term $A_{j,\\pi(j)}$ is the attention weight that the $j$-th query $q_j$ gives to its correct source key $k_{\\pi(j)}$. Since $q_j = s_{\\pi(j)} = k_{\\pi(j)}$, the compatibility score for this pair is $C_{j,\\pi(j)} = 0$, which is the maximum possible score in row $j$. The sum $\\frac{1}{n}\\sum_{j=1}^{n} A_{j,\\pi(j)}$ is thus the average correctness of the alignment. An error $E=0$ implies a perfect alignment ($A=P$), while an error approaching $1$ indicates a completely incorrect or diffuse alignment.\n\n5.  **Algorithmic Procedure**:\n    - For a given sequence $s = \\{s_i\\}_{i=1}^n$ and parameter $\\sigma$:\n    - Perform a stable sort on pairs of $(s_i, i)$ to determine the permutation $\\pi$ and the sorted sequence of queries $q = \\{q_j\\}_{j=1}^n$.\n    - Initialize a variable for the sum of correct attention probabilities to $0$.\n    - For each target position $j$ from $1$ to $n$:\n        - Compute the compatibility scores $C_{j,i}$ for the query $q_j$ against all keys $k_i=s_i$.\n        - Apply the numerically stable Softmax function to this row of scores to get the attention weights $\\{A_{j,i}\\}_{i=1}^n$.\n        - Identify the correct source index $\\pi(j)$.\n        - Add the attention weight $A_{j,\\pi(j)}$ to the running sum.\n    - Calculate the error $E = 1 - (\\text{sum} / n)$.\n    This procedure is repeated for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the soft alignment error for a series of test cases based on a\n    self-attention sorting model.\n    \"\"\"\n\n    test_cases = [\n        ([3.1, 0.2, 1.7, 2.4], 0.2),\n        ([0.5, 1.0, 1.5, 2.0], 0.5),\n        ([1.0, 1.0, 1.2, 0.8], 0.1),\n        ([2.0], 0.3),\n        ([0.0, 2.0, 4.0, 6.0], 10.0),\n    ]\n\n    results = []\n    for s_list, sigma in test_cases:\n        s = np.array(s_list, dtype=np.float64)\n        n = len(s)\n\n        if n == 0:\n            results.append(0.0)\n            continue\n            \n        # Step 1: Perform a stable sort to find the permutation pi and queries q.\n        # sorted() is stable in Python. It maintains the original order for equal elements.\n        # enumerate(s) provides (original_index, value) tuples.\n        sorted_s_with_indices = sorted(enumerate(s), key=lambda x: x[1])\n        \n        # pi[j] is the original index of the j-th element in the sorted sequence.\n        pi = np.array([item[0] for item in sorted_s_with_indices])\n        \n        # q is the sequence of sorted scalar values, which are the queries.\n        q = np.array([item[1] for item in sorted_s_with_indices])\n\n        # The keys are the original scalar values.\n        k = s\n        \n        # Variable to accumulate the attention probabilities on the correct items.\n        sum_of_correct_attentions = 0.0\n        \n        two_sigma_sq = 2.0 * sigma**2\n\n        # Step 2: Iterate through each query to compute its attention distribution.\n        for j in range(n):\n            # The j-th query is q[j].\n            \n            # Step 3: Compute compatibility scores for the j-th query with all keys.\n            # C_{j,i} = -(q_j - k_i)^2 / (2 * sigma^2)\n            compat_scores = -((q[j] - k)**2) / two_sigma_sq\n\n            # Step 4: Compute the attention weights for the j-th query using a\n            # numerically stable Softmax.\n            # Subtracting the max score prevents overflow/underflow.\n            max_score = np.max(compat_scores)\n            exp_scores = np.exp(compat_scores - max_score)\n            sum_exp_scores = np.sum(exp_scores)\n            \n            # The full j-th row of the attention matrix A would be:\n            # attention_row = exp_scores / sum_exp_scores\n            \n            # We only need the attention weight on the correct key.\n            # The correct key for query q[j] is k[pi[j]].\n            correct_key_idx = pi[j]\n            \n            # This is the value of A_{j, pi(j)}.\n            correct_attention_prob = exp_scores[correct_key_idx] / sum_exp_scores\n            \n            sum_of_correct_attentions += correct_attention_prob\n\n        # Step 5: Compute the final soft alignment error E.\n        # E = 1 - (1/n) * sum(A_{j, pi(j)})\n        error = 1.0 - (sum_of_correct_attentions / n)\n        \n        results.append(error)\n\n    # Format the results to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3192620"}]}