{"hands_on_practices": [{"introduction": "Before building complex models, it's essential to grasp the fundamental mechanics of the training process itself. This exercise focuses on the probabilistic nature of dynamic masking, a core component of models like BERT. By applying basic probability theory, you will calculate key statistics about the training procedure, such as the expected number of times a token is masked and its likelihood of being used for learning at all, providing a solid mathematical foundation for analyzing MLM pre-training [@problem_id:3164748].", "problem": "A Transformer encoder is pre-trained using Masked Language Modeling (MLM), where, during each pass over the data, every token position independently becomes a prediction target with probability $p$, regardless of whether it is replaced by a special mask symbol, a random token, or left unchanged. The training corpus contains a fixed set of token positions, and each epoch iterates over every token position exactly once. Masking is dynamically re-sampled at each epoch so that the selection decision for a given token position in one epoch is independent of its selection in any other epoch. Training runs for $E$ epochs. Stochastic Gradient Descent (SGD) is used, but for this problem, focus only on per-token contributions: each time a token position is selected as a prediction target, it contributes a single loss term and thus one gradient-contributing event for that token position during that epoch.\n\nFor a fixed token position $i$, define the random variable $S$ as the total number of epochs in which this position is selected as a prediction target. Using only the axioms of probability for independent Bernoulli trials and linearity of expectation as the fundamental base, derive, in closed form, the following quantities in terms of $E$ and $p$:\n- The expected number of gradient-contributing events per token position, $\\mathbb{E}[S]$.\n- The coverage probability that the token position is selected at least once over the $E$ epochs, $\\Pr(S \\geq 1)$.\n\nExpress your final answer as a single row vector $\\begin{pmatrix} \\mathbb{E}[S] & \\Pr(S \\geq 1) \\end{pmatrix}$. No rounding is required, and no physical units are involved.", "solution": "The problem statement is evaluated for validity before attempting a solution.\n\n### Step 1: Extract Givens\n- A Transformer encoder is pre-trained using Masked Language Modeling (MLM).\n- During each pass (epoch), every token position independently becomes a prediction target with probability $p$.\n- The selection decision for a given token position in one epoch is independent of its selection in any other epoch.\n- The total number of epochs for training is $E$.\n- Each time a token position is selected as a prediction target, it contributes a single gradient-contributing event.\n- For a fixed token position $i$, the random variable $S$ is defined as the total number of epochs in which this position is selected as a prediction target.\n- The derivation must use only the axioms of probability for independent Bernoulli trials and linearity of expectation.\n- The required quantities to be derived are the expected number of gradient-contributing events per token position, $\\mathbb{E}[S]$, and the coverage probability that the token position is selected at least once, $\\Pr(S \\geq 1)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem describes a simplified but fundamentally correct probabilistic model of the Masked Language Modeling (MLM) pre-training objective, a cornerstone of modern natural language processing models like BERT. The use of Bernoulli trials to model the independent masking events is standard and mathematically sound.\n- **Well-Posed:** The problem is clearly defined with all necessary parameters ($E$, $p$) and explicit definitions of the random variable $S$ and the target quantities ($\\mathbb{E}[S]$, $\\Pr(S \\geq 1)$). The conditions lead to a unique and meaningful solution.\n- **Objective:** The language is formal, precise, and devoid of any subjective or ambiguous terminology.\n- **Flaw Checklist:**\n    1.  **Scientific or Factual Unsoundness:** None. The setup is a valid abstraction of a real-world machine learning process.\n    2.  **Non-Formalizable or Irrelevant:** The problem is entirely formalizable within probability theory and is directly relevant to the field of deep learning.\n    3.  **Incomplete or Contradictory Setup:** The problem is self-contained and consistent. The independence of events across epochs is explicitly stated.\n    4.  **Unrealistic or Infeasible:** The scenario is a standard theoretical model used in the analysis of such algorithms. It is not unrealistic.\n    5.  **Ill-Posed or Poorly Structured:** The problem is well-structured and leads to a unique solution.\n    6.  **Pseudo-Profound, Trivial, or Tautological:** The problem requires the correct application of fundamental principles of probability theory (linearity of expectation, properties of independent events) and is a non-trivial exercise in formal derivation.\n    7.  **Outside Scientific Verifiability:** The derivation is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid** and well-posed. A solution will be derived.\n\n### Derivation\n\nLet us consider a single, fixed token position. The training process runs for $E$ epochs.\n\nFirst, we define a set of indicator random variables, $X_j$, for each epoch $j \\in \\{1, 2, \\dots, E\\}$.\nLet $X_j = 1$ if the token position is selected as a prediction target in epoch $j$, and $X_j = 0$ otherwise.\n\nAccording to the problem statement, the selection in any given epoch is an independent event with probability $p$. Therefore, each $X_j$ is an independent Bernoulli random variable with parameter $p$. The probability mass function for each $X_j$ is:\n$$ \\Pr(X_j = 1) = p $$\n$$ \\Pr(X_j = 0) = 1 - p $$\n\nThe random variable $S$ represents the total number of epochs in which the position is selected. This is the sum of the indicator variables over all epochs:\n$$ S = \\sum_{j=1}^{E} X_j $$\n\n**1. Derivation of the Expected Value $\\mathbb{E}[S]$**\n\nWe need to find the expected total number of gradient-contributing events, which is $\\mathbb{E}[S]$. The problem requires using the principle of linearity of expectation.\n\nThe expectation of a single Bernoulli random variable $X_j$ is:\n$$ \\mathbb{E}[X_j] = 1 \\cdot \\Pr(X_j=1) + 0 \\cdot \\Pr(X_j=0) = 1 \\cdot p + 0 \\cdot (1-p) = p $$\n\nUsing the linearity of expectation, which states that the expectation of a sum of random variables is the sum of their individual expectations, we have:\n$$ \\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{j=1}^{E} X_j\\right] = \\sum_{j=1}^{E} \\mathbb{E}[X_j] $$\n\nSince $\\mathbb{E}[X_j] = p$ for every epoch $j$, the sum becomes:\n$$ \\mathbb{E}[S] = \\sum_{j=1}^{E} p = Ep $$\n\n**2. Derivation of the Coverage Probability $\\Pr(S \\geq 1)$**\n\nWe need to find the probability that the token position is selected at least once over the $E$ epochs. This is the probability $\\Pr(S \\geq 1)$.\n\nIt is more direct to calculate this probability using the complement rule. The event \"$S \\geq 1$\" (the token is selected at least once) is the complement of the event \"$S = 0$\" (the token is never selected).\n$$ \\Pr(S \\geq 1) = 1 - \\Pr(S = 0) $$\n\nThe event \"$S=0$\" occurs if and only if the token is not selected in any of the $E$ epochs. This means $X_1=0$ AND $X_2=0$ AND ... AND $X_E=0$.\n$$ \\Pr(S=0) = \\Pr(X_1=0, X_2=0, \\dots, X_E=0) $$\n\nThe problem states that selection decisions across epochs are independent. Therefore, the probability of the joint event is the product of the probabilities of the individual events:\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} \\Pr(X_j=0) $$\n\nFor each epoch $j$, the probability of the token not being selected is $\\Pr(X_j=0) = 1-p$.\nSubstituting this into the product, we get:\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} (1-p) = (1-p)^E $$\n\nFinally, substituting this result back into the complement rule equation gives the coverage probability:\n$$ \\Pr(S \\geq 1) = 1 - (1-p)^E $$\n\nThe derived quantities are $\\mathbb{E}[S] = Ep$ and $\\Pr(S \\geq 1) = 1 - (1-p)^E$. These are expressed in closed form in terms of $E$ and $p$ as required.", "answer": "$$\\boxed{\\begin{pmatrix} Ep & 1 - (1-p)^{E} \\end{pmatrix}}$$", "id": "3164748"}, {"introduction": "Standard Masked Language Modeling often treats all tokens as equally important, masking them with uniform probability. This practice challenges that assumption by exploring a more nuanced, data-driven approach. You will design and implement a masking strategy that preferentially targets tokens with high information content, estimated using the classic Term Frequency–Inverse Document Frequency (TF-IDF) metric. This hands-on coding exercise [@problem_id:3147230] bridges concepts from traditional information retrieval and modern neural networks, allowing you to test whether forcing a model to predict more \"important\" words leads to more effective learning.", "problem": "You are tasked with designing and evaluating a masked language modeling corruption scheme that preferentially masks tokens with high Term Frequency–Inverse Document Frequency (TF-IDF). The goal is to analyze whether such a scheme focuses learning on informative content. Ground your reasoning and algorithm in foundational definitions from information theory and standard masked language modeling.\n\nUse the following corpus of documents, tokenized by lowercasing and splitting on whitespace, and treat the Beginning Of Sentence (BOS) token $\\langle \\text{BOS} \\rangle$ as the previous token for the first token in each document but never as a candidate for masking:\n- Document $1$: \"deep learning models learn representations of data\"\n- Document $2$: \"language modeling with transformers uses attention\"\n- Document $3$: \"masked language modeling predicts masked tokens from context\"\n- Document $4$: \"information theory defines entropy and mutual information\"\n- Document $5$: \"tf idf weights highlight terms that are rare across documents\"\n- Document $6$: \"attention mechanisms capture long range dependencies in sequences\"\n\nBase your derivation on:\n- The masked language modeling objective optimized via cross-entropy, which for masked tokens reduces to minimizing expected negative log-probability under the model.\n- Shannon self-information defined for a random variable with probability mass function $p(\\cdot)$ as $-\\ln p(x)$, measured in nats using the natural logarithm.\n- Maximum likelihood estimates for unigram and bigram distributions with appropriate smoothing.\n\nDefine the following quantities precisely:\n- Let $D$ be the number of documents and $V$ the vocabulary of tokens (excluding $\\langle \\text{BOS} \\rangle$). Let the total number of token positions eligible for masking across all documents be $L$.\n- For a token $w \\in V$ in document $d$, define the term frequency $tf_{d}(w)$ as the count of $w$ in $d$ and the document frequency $df(w)$ as the number of documents in which $w$ appears. Use smoothed inverse document frequency\n$$idf(w) = \\ln\\left(\\frac{1 + D}{1 + df(w)}\\right) + 1.$$\n- For each position $i$ containing token $x_i$ in its document $d_i$, define the per-position TF-IDF weight\n$$\\mathrm{tfidf}_i = tf_{d_i}(x_i)\\cdot idf(x_i).$$\n- Define a weighted corruption scheme parameterized by a nonnegative exponent $\\alpha \\ge 0$ and a mask rate $m \\in (0,1)$ that assigns each position $i$ an independent inclusion probability (under a Poisson sampling approximation) proportional to $\\mathrm{tfidf}_i^\\alpha$:\n$$q_i = m \\cdot \\frac{L \\,\\mathrm{tfidf}_i^\\alpha}{\\sum_{j=1}^{L} \\mathrm{tfidf}_j^\\alpha}.$$\nThis ensures $\\sum_i q_i = mL$ in expectation. The uniform baseline has $q_i^{\\mathrm{uni}} = m$ for all positions.\n- Define the unigram distribution by maximum likelihood\n$$p_{\\mathrm{uni}}(w) = \\frac{c(w)}{\\sum_{\\tilde{w}\\in V} c(\\tilde{w})},$$\nwhere $c(w)$ is the total count of $w$ across all documents. The per-position unigram self-information is\n$$s_{\\mathrm{uni}}(i) = -\\ln p_{\\mathrm{uni}}(x_i).$$\n- Define the bigram distribution with Laplace (add-one) smoothing. Let $y_i$ be the previous token for position $i$ (with $y_i = \\langle \\text{BOS} \\rangle$ for the first token in each document). Let $c(y,w)$ be the count of occurrences of bigram $(y,w)$ across the corpus and $c(y) = \\sum_{w\\in V} c(y,w)$. With $|V|$ the vocabulary size,\n$$p_{\\mathrm{bi}}(x_i\\mid y_i) = \\frac{c(y_i, x_i) + 1}{c(y_i) + |V|},\\quad s_{\\mathrm{bi}}(i) = -\\ln p_{\\mathrm{bi}}(x_i\\mid y_i).$$\n- Compute the expected masked averages under the weighted scheme:\n$$\\bar{s}_{\\mathrm{uni}}^{\\mathrm{tfidf}} = \\frac{\\sum_{i=1}^{L} q_i\\, s_{\\mathrm{uni}}(i)}{\\sum_{i=1}^{L} q_i},\\quad \\bar{s}_{\\mathrm{bi}}^{\\mathrm{tfidf}} = \\frac{\\sum_{i=1}^{L} q_i\\, s_{\\mathrm{bi}}(i)}{\\sum_{i=1}^{L} q_i}.$$\n- Compute the uniform (baseline) masked averages, which equal the overall averages because $q_i^{\\mathrm{uni}}$ is constant:\n$$\\bar{s}_{\\mathrm{uni}}^{\\mathrm{uni}} = \\frac{1}{L}\\sum_{i=1}^{L} s_{\\mathrm{uni}}(i),\\quad \\bar{s}_{\\mathrm{bi}}^{\\mathrm{uni}} = \\frac{1}{L}\\sum_{i=1}^{L} s_{\\mathrm{bi}}(i).$$\n- Report the differences\n$$\\Delta_{\\mathrm{uni}} = \\bar{s}_{\\mathrm{uni}}^{\\mathrm{tfidf}} - \\bar{s}_{\\mathrm{uni}}^{\\mathrm{uni}},\\quad \\Delta_{\\mathrm{bi}} = \\bar{s}_{\\mathrm{bi}}^{\\mathrm{tfidf}} - \\bar{s}_{\\mathrm{bi}}^{\\mathrm{uni}}.$$\n\nInterpretation: Positive $\\Delta_{\\mathrm{uni}}$ indicates that the scheme focuses on rarer tokens under the unigram model (higher self-information), while positive $\\Delta_{\\mathrm{bi}}$ indicates that masked tokens are less predictable from immediate context under the smoothed bigram model.\n\nImplement a complete, runnable program that:\n- Constructs the corpus and tokenization exactly as described.\n- Computes $tf_{d}(w)$, $df(w)$, $idf(w)$, $\\mathrm{tfidf}_i$, $p_{\\mathrm{uni}}(w)$, $p_{\\mathrm{bi}}(x_i\\mid y_i)$ with add-one smoothing, and the self-information values $s_{\\mathrm{uni}}(i)$ and $s_{\\mathrm{bi}}(i)$.\n- For each parameter set in the test suite below, computes $q_i$, the masked averages, and the differences $\\Delta_{\\mathrm{uni}}$ and $\\Delta_{\\mathrm{bi}}$.\n\nTest suite (each case is $(\\alpha,m)$):\n- Case $1$: $(\\alpha = 1.0, m = 0.15)$\n- Case $2$: $(\\alpha = 0.0, m = 0.15)$\n- Case $3$: $(\\alpha = 2.0, m = 0.50)$\n- Case $4$: $(\\alpha = 1.0, m = 0.05)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one two-element list per test case in the same order as above:\n- Each inner list must be of the form $[\\Delta_{\\mathrm{uni}}, \\Delta_{\\mathrm{bi}}]$ with both values as floats rounded to six decimal places.\n- For example, a valid output may look like $[[0.123456,0.234567],[0.000000,0.000000],[0.345678,0.456789],[0.123456,0.234567]]$.", "solution": "The user has provided a well-defined computational problem in the domain of natural language processing and information theory. The task is to analyze a TF-IDF-weighted token masking scheme for masked language modeling.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n\n*   **Corpus**: A set of $6$ documents, $D=6$.\n    *   $d_1$: \"deep learning models learn representations of data\"\n    *   $d_2$: \"language modeling with transformers uses attention\"\n    *   $d_3$: \"masked language modeling predicts masked tokens from context\"\n    *   $d_4$: \"information theory defines entropy and mutual information\"\n    *   $d_5$: \"tf idf weights highlight terms that are rare across documents\"\n    *   $d_6$: \"attention mechanisms capture long range dependencies in sequences\"\n*   **Tokenization**: Lowercase and split on whitespace. The special token $\\langle \\text{BOS} \\rangle$ is used as context for the first token of each document but is not part of the vocabulary $V$ or eligible for masking.\n*   **Variables**:\n    *   $D$: number of documents.\n    *   $V$: vocabulary of unique tokens.\n    *   $L$: total number of token positions eligible for masking.\n*   **Formulas**:\n    *   Term Frequency, $tf_d(w)$: Count of token $w$ in document $d$.\n    *   Document Frequency, $df(w)$: Number of documents containing token $w$.\n    *   Smoothed Inverse Document Frequency: $idf(w) = \\ln\\left(\\frac{1 + D}{1 + df(w)}\\right) + 1$.\n    *   Per-position TF-IDF weight: $\\mathrm{tfidf}_i = tf_{d_i}(x_i) \\cdot idf(x_i)$.\n    *   Weighted masking probability: $q_i = m \\cdot \\frac{L \\,\\mathrm{tfidf}_i^\\alpha}{\\sum_{j=1}^{L} \\mathrm{tfidf}_j^\\alpha}$.\n    *   Unigram probability: $p_{\\mathrm{uni}}(w) = \\frac{c(w)}{\\sum_{\\tilde{w}\\in V} c(\\tilde{w})}$.\n    *   Unigram self-information: $s_{\\mathrm{uni}}(i) = -\\ln p_{\\mathrm{uni}}(x_i)$.\n    *   Smoothed bigram probability: $p_{\\mathrm{bi}}(x_i\\mid y_i) = \\frac{c(y_i, x_i) + 1}{c(y_i) + |V|}$.\n    *   Bigram self-information: $s_{\\mathrm{bi}}(i) = -\\ln p_{\\mathrm{bi}}(x_i\\mid y_i)$.\n    *   Weighted average self-information: $\\bar{s}^{\\mathrm{tfidf}} = \\frac{\\sum_{i=1}^{L} q_i\\, s(i)}{\\sum_{i=1}^{L} q_i}$.\n    *   Uniform average self-information: $\\bar{s}^{\\mathrm{uni}} = \\frac{1}{L}\\sum_{i=1}^{L} s(i)$.\n    *   Differences: $\\Delta = \\bar{s}^{\\mathrm{tfidf}} - \\bar{s}^{\\mathrm{uni}}$.\n*   **Test Suite**: $(\\alpha, m)$ tuples: $(1.0, 0.15)$, $(0.0, 0.15)$, $(2.0, 0.50)$, $(1.0, 0.05)$.\n*   **Output Format**: A string representing a list of lists, e.g., `[[d_uni1, d_bi1], [d_uni2, d_bi2]]`, with values rounded to six decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded**: The problem is well-grounded in established principles of information theory (Shannon self-information), information retrieval (TF-IDF), and deep learning (masked language modeling). The formulas are standard or explicitly defined variations.\n*   **Well-Posed**: The problem is fully specified. It provides the exact corpus, tokenization rules, mathematical formulas, and test parameters. This ensures that a unique and deterministic solution can be computed.\n*   **Objective**: The problem is stated in precise, formal language, free of any subjectivity or ambiguity.\n*   **Flaw Checklist**: The problem does not exhibit any of the listed flaws.\n    1.  **Unsoundness**: No scientific or factual errors are present. The definitions are consistent with their use in the respective fields.\n    2.  **Irrelevant**: The problem is directly relevant to masked language modeling.\n    3.  **Incomplete/Contradictory**: All necessary information is provided, and there are no contradictions. The use of the $\\langle \\text{BOS} \\rangle$ token is clearly defined. The condition $\\alpha \\ge 0$ is handled, as $\\mathrm{tfidf}_i$ must be positive for any token present in the corpus, preventing issues like $0^0$. The case $\\alpha=0$ correctly reduces the weighted scheme to a uniform one, providing an internal consistency check.\n    4.  **Unrealistic/Infeasible**: The scale of the problem is small and computationally tractable.\n    5.  **Ill-Posed**: The structure is logical and leads to a unique, stable solution.\n    6.  **Trivial/Tautological**: The problem requires a non-trivial synthesis of multiple concepts and careful implementation. The investigation of how TF-IDF weighting affects the information content of masked tokens is a meaningful question.\n    7.  **Unverifiable**: The results are deterministic and can be independently verified by implementing the described algorithm.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed to provide a complete solution.\n\n### Algorithm and Implementation\n\nThe solution will be implemented by following these steps:\n\n1.  **Corpus Preparation**: The provided documents are tokenized. We establish the complete vocabulary $V$, the total number of tokens $L$, and the number of documents $D$. We also create a flat list representing each token position in the corpus, storing its value, document index, and preceding token for later use.\n\n2.  **TF-IDF Calculation**:\n    *   For each token $w \\in V$, we compute its document frequency $df(w)$ and then its smoothed inverse document frequency $idf(w)$ using the given formula.\n    *   For each token position $i$ in the corpus, we find its term frequency $tf_{d_i}(x_i)$ within its document and compute the final $\\mathrm{tfidf}_i$ value. This results in a list of $L$ TF-IDF scores, one for each token position.\n\n3.  **Probability and Self-Information Models**:\n    *   **Unigram Model**: We compute the corpus-wide frequency $c(w)$ for each token $w \\in V$. The unigram probability $p_{\\mathrm{uni}}(w)$ is $c(w)/L$. The unigram self-information $s_{\\mathrm{uni}}(i)$ for each position $i$ is derived from the probability of the token $x_i$ at that position.\n    *   **Bigram Model**: We iterate through the corpus to count all bigram occurrences $c(y, w)$ and context occurrences $c(y)$, including the special $\\langle \\text{BOS} \\rangle$ context. Using these counts and the vocabulary size $|V|$, we compute the smoothed bigram probability $p_{\\mathrm{bi}}(x_i|y_i)$ and the corresponding self-information $s_{\\mathrm{bi}}(i)$ for each position $i$.\n\n4.  **Baseline Averages**: We compute the uniform baseline averages, $\\bar{s}_{\\mathrm{uni}}^{\\mathrm{uni}}$ and $\\bar{s}_{\\mathrm{bi}}^{\\mathrm{uni}}$, by taking the arithmetic mean of the self-information values across all $L$ positions.\n\n5.  **Weighted Scheme Analysis**: For each $(\\alpha, m)$ pair in the test suite:\n    *   We compute the unnormalized weights $\\mathrm{tfidf}_i^\\alpha$ for all positions.\n    *   We calculate the normalized masking probability $q_i$ for each position using the provided formula.\n    *   The TF-IDF weighted averages, $\\bar{s}_{\\mathrm{uni}}^{\\mathrm{tfidf}}$ and $\\bar{s}_{\\mathrm{bi}}^{\\mathrm{tfidf}}$, are computed as weighted averages of the self-information values, using the $q_i$ values as weights.\n    *   The differences, $\\Delta_{\\mathrm{uni}}$ and $\\Delta_{\\mathrm{bi}}$, are calculated by subtracting the uniform averages from the weighted averages.\n\n6.  **Output Formatting**: The final results for all test cases are collected and formatted into the specified single-line string format. When $\\alpha=0$, the TF-IDF weighting becomes uniform, so the weighted averages equal the uniform averages, and the deltas correctly evaluate to $0$.\n\nThis structured approach ensures that all quantities are computed correctly according to the problem's specifications.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Implements the full analysis of the TF-IDF weighted masking scheme.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.15),\n        (0.0, 0.15),\n        (2.0, 0.50),\n        (1.0, 0.05),\n    ]\n\n    # Step 1: Corpus Preparation\n    docs_raw = [\n        \"deep learning models learn representations of data\",\n        \"language modeling with transformers uses attention\",\n        \"masked language modeling predicts masked tokens from context\",\n        \"information theory defines entropy and mutual information\",\n        \"tf idf weights highlight terms that are rare across documents\",\n        \"attention mechanisms capture long range dependencies in sequences\"\n    ]\n    docs_tokenized = [doc.lower().split() for doc in docs_raw]\n    D = len(docs_tokenized)\n\n    all_tokens_flat = [token for doc in docs_tokenized for token in doc]\n    V = sorted(list(set(all_tokens_flat)))\n    V_size = len(V)\n    L = len(all_tokens_flat)\n\n    # Create a list of dictionaries, one for each token position\n    positions = []\n    for doc_id, doc in enumerate(docs_tokenized):\n        for token_idx, token in enumerate(doc):\n            prev_token = docs_tokenized[doc_id][token_idx-1] if token_idx > 0 else '<BOS>'\n            positions.append({\n                'token': token,\n                'doc_id': doc_id,\n                'prev_token': prev_token\n            })\n\n    # Step 2: TF-IDF Calculation\n    df = {token: sum(1 for doc in docs_tokenized if token in doc) for token in V}\n    idf = {token: np.log((1 + D) / (1 + df[token])) + 1 for token in V}\n\n    doc_tf_counts = [{token: doc.count(token) for token in set(doc)} for doc in docs_tokenized]\n    \n    tfidf_values = np.array([\n        doc_tf_counts[pos['doc_id']][pos['token']] * idf[pos['token']]\n        for pos in positions\n    ])\n\n    # Step 3: Probability and Self-Information Models\n    # Unigram model\n    token_counts = {token: all_tokens_flat.count(token) for token in V}\n    p_uni = {token: count / L for token, count in token_counts.items()}\n    s_uni_values = np.array([-np.log(p_uni[pos['token']]) for pos in positions])\n\n    # Bigram model\n    bigram_counts = defaultdict(int)\n    context_counts = defaultdict(int)\n    for doc in docs_tokenized:\n        prev_token = '<BOS>'\n        for token in doc:\n            bigram_counts[(prev_token, token)] += 1\n            context_counts[prev_token] += 1\n            prev_token = token\n\n    s_bi_values = []\n    for pos in positions:\n        token = pos['token']\n        prev_token = pos['prev_token']\n        c_yw = bigram_counts.get((prev_token, token), 0)\n        c_y = context_counts.get(prev_token, 0)\n        \n        prob_bi = (c_yw + 1) / (c_y + V_size)\n        s_bi_values.append(-np.log(prob_bi))\n    s_bi_values = np.array(s_bi_values)\n\n    # Step 4: Baseline Averages (Uniform Scheme)\n    s_uni_uni_bar = np.mean(s_uni_values)\n    s_bi_uni_bar = np.mean(s_bi_values)\n\n    results = []\n    # Step 5: Weighted Scheme Analysis for each test case\n    for alpha, m in test_cases:\n        # For alpha = 0, tfidf_i^alpha = 1, making q_i = m, which is uniform.\n        # Deltas should be zero (up to float precision).\n        # We process it generally without a special case.\n        tfidf_alpha = tfidf_values ** alpha\n        sum_tfidf_alpha = np.sum(tfidf_alpha)\n\n        if sum_tfidf_alpha == 0:\n            # Under the problem constraints (tf>=1, idf>0), this should not be reached.\n            # If it did, it would imply all tfidf values are zero.\n            q_values = np.zeros(L)\n        else:\n            q_values = m * L * tfidf_alpha / sum_tfidf_alpha\n        \n        # Denominator for averging is sum of weights, which is sum(q_i)\n        sum_q = np.sum(q_values)\n        if sum_q == 0:\n            # If all weights are zero, the average is undefined.\n            # For this problem, we'd expect this only if m=0, which is not a test case.\n            s_uni_tfidf_bar = s_uni_uni_bar\n            s_bi_tfidf_bar = s_bi_uni_bar\n        else:\n            s_uni_tfidf_bar = np.average(s_uni_values, weights=q_values)\n            s_bi_tfidf_bar = np.average(s_bi_values, weights=q_values)\n            \n        delta_uni = s_uni_tfidf_bar - s_uni_uni_bar\n        delta_bi = s_bi_tfidf_bar - s_bi_uni_bar\n        \n        results.append([delta_uni, delta_bi])\n\n    # Step 6: Final print statement in the exact required format.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3147230"}, {"introduction": "Moving beyond static, data-informed masking, this exercise explores a dynamic strategy that adapts to the model's state during training. You will implement a simplified MLM training loop from scratch to investigate gradient-based saliency, where the decision to mask a token depends on its influence on the model's loss. This advanced practice [@problem_id:3147244] provides deep insight into the mechanics of MLM by having you build the core components and evaluate whether an adaptive masking strategy can focus the model's attention on its most challenging examples to accelerate learning.", "problem": "You are asked to design, implement, and evaluate a simplified masked language modeling procedure that selects which tokens to mask using gradient-based saliency, and to compare it against random masking in terms of learning speed. The setting is purely mathematical and algorithmic, and all data are synthetic. Your final output must be produced by a complete, runnable program as specified.\n\nA model is defined over a fixed vocabulary of size $V$. Each token $v \\in \\{0,1,\\dots,V-1\\}$ has an embedding vector $\\mathbf{e}_v \\in \\mathbb{R}^d$. Given a sequence $\\mathbf{s} = (s_0,\\dots,s_{L-1})$ of length $L$, let $\\mathbf{x}_j = \\mathbf{e}_{s_j}$ be the embedding of the token at position $j$. For predicting the token at position $i$, the model forms a context vector by averaging all other positions’ embeddings,\n$$\n\\mathbf{c}_i \\triangleq \\frac{1}{L-1}\\sum_{\\substack{j=0 \\\\ j\\neq i}}^{L-1}\\mathbf{x}_j.\n$$\nThe classifier parameters are a weight matrix $\\mathbf{W}\\in\\mathbb{R}^{V\\times d}$ and a bias vector $\\mathbf{b}\\in\\mathbb{R}^{V}$. The prediction logits for position $i$ are\n$$\n\\mathbf{z}_i \\triangleq \\mathbf{W}\\,\\mathbf{c}_i + \\mathbf{b} \\in \\mathbb{R}^V,\n$$\nand the predicted distribution is the softmax\n$$\n\\mathbf{p}_i \\triangleq \\mathrm{softmax}(\\mathbf{z}_i),\\quad \\text{that is }\\, [\\mathbf{p}_i]_v=\\frac{\\exp([\\mathbf{z}_i]_v)}{\\sum_{u=0}^{V-1}\\exp([\\mathbf{z}_i]_u)}.\n$$\nFor a set of masked positions $\\mathcal{M}\\subset\\{0,\\dots,L-1\\}$ within a sequence, the masked language modeling loss is the average negative log-likelihood over the masked tokens:\n$$\n\\mathcal{L}(\\mathbf{s};\\mathcal{M}) \\triangleq \\frac{1}{|\\mathcal{M}|}\\sum_{i\\in\\mathcal{M}} -\\log\\big([\\mathbf{p}_i]_{s_i}\\big).\n$$\nNote that when predicting position $i$, the context $\\mathbf{c}_i$ includes all tokens except the target token at $i$; there is no special “mask token” in this simplified setup.\n\nFor gradient-based saliency, define for a given sequence $\\mathbf{s}$ and a probing mask set $\\mathcal{M}_{\\mathrm{probe}}$ the per-position gradient with respect to the input embedding vector at position $j$ as\n$$\n\\mathbf{g}_j \\triangleq \\frac{\\partial \\mathcal{L}(\\mathbf{s};\\mathcal{M}_{\\mathrm{probe}})}{\\partial \\mathbf{x}_j}\\in\\mathbb{R}^d,\n$$\nand the saliency score as the Euclidean norm $s_j \\triangleq \\|\\mathbf{g}_j\\|_2$. A saliency-based masking policy selects the $k$ positions with the largest $s_j$ in each sequence to form $\\mathcal{M}$ on that step; a random masking baseline selects $k$ positions uniformly at random without replacement in each sequence.\n\nTraining proceeds by stochastic gradient descent on the parameters $(\\mathbf{E},\\mathbf{W},\\mathbf{b})$, where $\\mathbf{E}\\in\\mathbb{R}^{V\\times d}$ stacks the embeddings $\\mathbf{e}_v$ as rows. On each step, the selected mask sets $\\mathcal{M}$ are used to compute the loss and its gradients, which are averaged over all masked positions across the batch and then used for a parameter update with a fixed learning rate $\\eta$.\n\nYou must implement two training strategies over the same synthetic dataset and from identical initial parameters:\n- Strategy A (saliency): on each step, compute $s_j$ using $\\mathcal{M}_{\\mathrm{probe}}=\\{0,1,\\dots,L-1\\}$ and select the top $k$ positions per sequence.\n- Strategy B (random): on each step, select $k$ positions per sequence uniformly at random.\n\nAfter $T$ training steps, compute an evaluation loss by setting $\\mathcal{M}_{\\mathrm{eval}}=\\{0,1,\\dots,L-1\\}$ and averaging the negative log-likelihood over all positions and all sequences.\n\nSynthetic data generation must be deterministic from a given seed and follow one of the following schemes:\n- Topic-coherent sequences: choose a number of topics $K$ such that $K$ evenly partitions $V$ into groups of size $G=V/K$. For topic $t\\in\\{0,\\dots,K-1\\}$, define a categorical distribution over the vocabulary that assigns probability $(1-\\epsilon)/G$ to tokens in the $t$-th group and probability $\\epsilon/(V-G)$ to tokens outside the group, with a fixed mixing parameter $\\epsilon\\in(0,1)$. To sample a sequence, first choose a topic uniformly and then sample $L$ independent tokens from that topic’s distribution.\n- Uniform sequences: sample each of the $L$ tokens uniformly and independently from the $V$-sized vocabulary.\n\nAll randomness must be controlled by a specified seed so that the two strategies are compared fairly and results are reproducible.\n\nYour program must implement the gradients exactly for the described model. The following facts are permitted as the foundational base:\n- Cross-entropy for a softmax output admits the standard gradient $\\frac{\\partial}{\\partial \\mathbf{z}_i}\\big(-\\log([\\mathbf{p}_i]_{s_i})\\big)=\\mathbf{p}_i - \\mathbf{y}_i$, where $\\mathbf{y}_i$ is the one-hot vector of the target $s_i$.\n- The gradient of an average is the average of the gradients, and the gradient of an affine function is the corresponding linear map.\n\nDesign a test suite composed of three cases with the following parameters, each case being evaluated independently:\n- Case $1$ (happy path, structured data): $V=12$, $d=8$, number of sequences $B=6$, sequence length $L=8$, training steps $T=60$, top-$k$ per sequence $k=3$, learning rate $\\eta=0.3$, topics $K=3$, mixing $\\epsilon=0.05$, dataset seed $s=1$.\n- Case $2$ (edge case, no structure): $V=12$, $d=8$, number of sequences $B=8$, sequence length $L=10$, training steps $T=60$, top-$k$ per sequence $k=3$, learning rate $\\eta=0.3$, uniform dataset, dataset seed $s=2$.\n- Case $3$ (boundary case, minimal context): $V=12$, $d=8$, number of sequences $B=12$, sequence length $L=3$, training steps $T=80$, top-$k$ per sequence $k=1$, learning rate $\\eta=0.3$, topics $K=3$, mixing $\\epsilon=0.05$, dataset seed $s=3$.\n\nFor all cases, initialize model parameters $(\\mathbf{E},\\mathbf{W},\\mathbf{b})$ identically using a fixed parameter seed of $p=999$ and small random values for $\\mathbf{E}$, with $\\mathbf{W}$ and $\\mathbf{b}$ initialized to zeros. Use the same dataset for both strategies within each case.\n\nYour program must output a single line with a Python list of three booleans $[r_1,r_2,r_3]$ computed as follows:\n- For Case $1$, set $r_1$ to $\\mathrm{True}$ if the final evaluation loss from Strategy A is strictly smaller than that from Strategy B by at least a margin $\\delta_1=0.005$, i.e., $\\mathcal{L}_{\\mathrm{A}} \\le \\mathcal{L}_{\\mathrm{B}} - \\delta_1$, and $\\mathrm{False}$ otherwise.\n- For Case $2$, set $r_2$ to $\\mathrm{True}$ if the absolute difference between the final evaluation losses is at most $\\tau_2=0.01$, i.e., $|\\mathcal{L}_{\\mathrm{A}} - \\mathcal{L}_{\\mathrm{B}}| \\le \\tau_2$, and $\\mathrm{False}$ otherwise.\n- For Case $3$, set $r_3$ to $\\mathrm{True}$ if the absolute difference between the final evaluation losses is at most $\\tau_3=0.015$, i.e., $|\\mathcal{L}_{\\mathrm{A}} - \\mathcal{L}_{\\mathrm{B}}| \\le \\tau_3$, and $\\mathrm{False}$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\"). No other output must be printed. No physical units or angles are involved. All quantities are dimensionless real numbers or booleans. The program must be self-contained, require no user input, use only the Python standard library and the specified libraries, and be fully deterministic given the stated seeds.", "solution": "The problem requires the implementation and comparison of two distinct masking strategies for a simplified masked language model (MLM): a random masking strategy and a saliency-based masking strategy. The comparison is performed over a series of training steps on synthetically generated data, with the objective of determining which strategy leads to a lower evaluation loss, a proxy for faster learning. The entire process, from data generation to parameter updates, must be deterministic and reproducible.\n\nThe model operates on a vocabulary of size $V$. A sequence $\\mathbf{s} = (s_0, s_1, \\dots, s_{L-1})$ of length $L$ is represented by a sequence of embedding vectors $\\mathbf{x}_j = \\mathbf{e}_{s_j} \\in \\mathbb{R}^d$, where $\\mathbf{E} \\in \\mathbb{R}^{V \\times d}$ is the master embedding matrix. To predict the token at position $i$, a context vector $\\mathbf{c}_i$ is computed by averaging the embeddings of all other tokens in the sequence:\n$$\n\\mathbf{c}_i = \\frac{1}{L-1}\\sum_{\\substack{j=0 \\\\ j\\neq i}}^{L-1}\\mathbf{x}_j.\n$$\nThe model's classifier, with parameters $\\mathbf{W} \\in \\mathbb{R}^{V \\times d}$ and $\\mathbf{b} \\in \\mathbb{R}^V$, produces logits $\\mathbf{z}_i = \\mathbf{W}\\mathbf{c}_i + \\mathbf{b}$. The predicted probability distribution over the vocabulary is given by the softmax function, $\\mathbf{p}_i = \\mathrm{softmax}(\\mathbf{z}_i)$.\n\nThe training objective is to minimize the masked language modeling loss, which is the average cross-entropy loss over a set of masked positions $\\mathcal{M} \\subset \\{0, \\dots, L-1\\}$:\n$$\n\\mathcal{L}(\\mathbf{s};\\mathcal{M}) = \\frac{1}{|\\mathcal{M}|}\\sum_{i\\in\\mathcal{M}} -\\log\\big([\\mathbf{p}_i]_{s_i}\\big),\n$$\nwhere $s_i$ is the true token at position $i$.\n\nTraining is performed using stochastic gradient descent (SGD). This requires computing the gradients of the loss function with respect to the model parameters $(\\mathbf{E}, \\mathbf{W}, \\mathbf{b})$. Let $\\mathbf{y}_i \\in \\mathbb{R}^V$ be the one-hot encoding of the target token $s_i$. The gradient of the loss for a single masked token $i$ with respect to the logits $\\mathbf{z}_i$ is $\\delta_{\\mathbf{z}_i} = \\mathbf{p}_i - \\mathbf{y}_i$. Using the chain rule, we derive the gradients for the parameters. For a loss calculated over a mask set $\\mathcal{M}$ of size $k=|\\mathcal{M}|$, the gradients for a single sequence are:\n\nThe gradient with respect to the bias vector $\\mathbf{b}$ is the average of the logit gradients over the masked positions:\n$$\n\\nabla_{\\mathbf{b}}\\mathcal{L} = \\frac{1}{k}\\sum_{i\\in\\mathcal{M}} (\\mathbf{p}_i - \\mathbf{y}_i).\n$$\nThe gradient with respect to the weight matrix $\\mathbf{W}$ is the average of the outer products of the logit gradients and the corresponding context vectors:\n$$\n\\nabla_{\\mathbf{W}}\\mathcal{L} = \\frac{1}{k}\\sum_{i\\in\\mathcal{M}} (\\mathbf{p}_i - \\mathbf{y}_i)\\mathbf{c}_i^T.\n$$\nThe gradient with respect to the embedding matrix $\\mathbf{E}$ is more complex. It's assembled from gradients with respect to individual input embeddings $\\mathbf{x}_j$. The gradient of the loss $\\mathcal{L}(\\mathbf{s};\\mathcal{M})$ with respect to an input embedding $\\mathbf{x}_j$ is determined by how $\\mathbf{x}_j$ influences the context vectors $\\mathbf{c}_i$ for the masked positions $i \\in \\mathcal{M}$. An embedding $\\mathbf{x}_j$ contributes to every context vector $\\mathbf{c}_i$ where $i \\neq j$. The derivative is:\n$$\n\\frac{\\partial \\mathcal{L}(\\mathbf{s};\\mathcal{M})}{\\partial \\mathbf{x}_j} = \\sum_{i \\in \\mathcal{M}} \\frac{1}{k} \\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{c}_i} \\frac{\\partial \\mathbf{c}_i}{\\partial \\mathbf{x}_j} = \\sum_{i \\in \\mathcal{M}, i \\neq j} \\frac{1}{k} (\\mathbf{W}^T(\\mathbf{p}_i - \\mathbf{y}_i)) \\frac{1}{L-1}.\n$$\nThis can be written as:\n$$\n\\nabla_{\\mathbf{x}_j}\\mathcal{L} = \\frac{\\mathbf{W}^T}{k(L-1)} \\sum_{i \\in \\mathcal{M}, i \\neq j} (\\mathbf{p}_i - \\mathbf{y}_i).\n$$\nThe gradient for the entire embedding matrix $\\mathbf{E}$ is found by accumulating these vector gradients. For each vocabulary entry $v$, the gradient for its embedding $\\mathbf{e}_v$ is the sum of gradients for all positions $j$ where token $s_j$ equals $v$:\n$$\n\\nabla_{\\mathbf{e}_v}\\mathcal{L} = \\sum_{j: s_j=v} \\nabla_{\\mathbf{x}_j}\\mathcal{L}.\n$$\nThe two training strategies differ only in how the mask set $\\mathcal{M}$ is chosen for each sequence at each training step.\n\nStrategy B (Random): $k$ positions are selected uniformly at random without replacement from $\\{0, \\dots, L-1\\}$. This serves as a standard baseline.\n\nStrategy A (Saliency): This strategy aims to mask more \"informative\" tokens. The informativeness of a token at position $j$ is quantified by a saliency score $s_j$. This score is defined as the Euclidean norm of the gradient of a probe loss with respect to the input embedding $\\mathbf{x}_j$. The probe loss is the full, unmasked loss, i.e., using $\\mathcal{M}_{\\mathrm{probe}}=\\{0, \\dots, L-1\\}$. The gradient is:\n$$\n\\mathbf{g}_j = \\frac{\\partial \\mathcal{L(s; \\mathcal{M}_{\\mathrm{probe}})}}{\\partial \\mathbf{x}_j} = \\frac{\\mathbf{W}^T}{L(L-1)} \\sum_{i=0, i \\neq j}^{L-1} (\\mathbf{p}_i - \\mathbf{y}_i).\n$$\nThe saliency score is $s_j = \\|\\mathbf{g}_j\\|_2$. For each sequence, the $k$ positions with the highest saliency scores are chosen to form the mask set $\\mathcal{M}$ for the SGD update step.\n\nThe implementation involves two main phases for each test case. First, a synthetic dataset is generated according to the specified scheme (topic-coherent or uniform) using a deterministic seed. Second, two separate training runs are executed, one for each strategy, starting from identical initial parameters. After the prescribed number of training steps, each trained model is evaluated by computing the average loss over all positions of all sequences in the dataset. The final evaluation losses from the two strategies are then compared based on the criteria specific to each test case to yield a boolean result. The entire procedure is encapsulated in a Python program that uses `numpy` for efficient vectorized computation of the forward pass and gradient calculations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"V\": 12, \"d\": 8, \"B\": 6, \"L\": 8, \"T\": 60, \"k\": 3, \"eta\": 0.3,\n            \"data_mode\": \"topic\", \"K\": 3, \"epsilon\": 0.05, \"s\": 1,\n            \"p\": 999, \"comparison\": \"better\", \"margin\": 0.005\n        },\n        {\n            \"V\": 12, \"d\": 8, \"B\": 8, \"L\": 10, \"T\": 60, \"k\": 3, \"eta\": 0.3,\n            \"data_mode\": \"uniform\", \"K\": None, \"epsilon\": None, \"s\": 2,\n            \"p\": 999, \"comparison\": \"similar\", \"margin\": 0.01\n        },\n        {\n            \"V\": 12, \"d\": 8, \"B\": 12, \"L\": 3, \"T\": 80, \"k\": 1, \"eta\": 0.3,\n            \"data_mode\": \"topic\", \"K\": 3, \"epsilon\": 0.05, \"s\": 3,\n            \"p\": 999, \"comparison\": \"similar\", \"margin\": 0.015\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        results.append(run_case(case_params))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_case(params):\n    \"\"\"\n    Runs a single test case, comparing saliency and random masking strategies.\n    \"\"\"\n    dataset = generate_dataset(\n        B=params[\"B\"], L=params[\"L\"], V=params[\"V\"], mode=params[\"data_mode\"],\n        K=params[\"K\"], epsilon=params[\"epsilon\"], seed=params[\"s\"]\n    )\n\n    loss_A = train_and_evaluate(dataset, 'saliency', params)\n    loss_B = train_and_evaluate(dataset, 'random', params)\n    \n    if params[\"comparison\"] == \"better\":\n        return loss_A <= loss_B - params[\"margin\"]\n    elif params[\"comparison\"] == \"similar\":\n        return abs(loss_A - loss_B) <= params[\"margin\"]\n    return False\n\ndef generate_dataset(B, L, V, mode, K, epsilon, seed):\n    \"\"\"\n    Generates a synthetic dataset.\n    B: batch size, L: sequence length, V: vocab size.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    dataset = []\n    if mode == \"topic\":\n        G = V // K\n        topic_probs = np.full((K, V), epsilon / (V - G))\n        for t in range(K):\n            topic_probs[t, t * G:(t + 1) * G] = (1 - epsilon) / G\n        \n        for _ in range(B):\n            topic_idx = rng.choice(K)\n            sequence = rng.choice(V, size=L, p=topic_probs[topic_idx])\n            dataset.append(sequence)\n    else: # uniform\n        for _ in range(B):\n            sequence = rng.integers(0, V, size=L)\n            dataset.append(sequence)\n    return [np.array(s) for s in dataset]\n\ndef stable_softmax(z, axis=-1):\n    \"\"\"Numerically stable softmax function.\"\"\"\n    z_max = np.max(z, axis=axis, keepdims=True)\n    exp_z = np.exp(z - z_max)\n    return exp_z / np.sum(exp_z, axis=axis, keepdims=True)\n\ndef train_and_evaluate(dataset, strategy, params):\n    \"\"\"\n    Initializes model, runs training loop for a given strategy, and evaluates.\n    \"\"\"\n    V, d, L, T, k, eta, p = params[\"V\"], params[\"d\"], params[\"L\"], \\\n                            params[\"T\"], params[\"k\"], params[\"eta\"], params[\"p\"]\n    B = len(dataset)\n    \n    # Initialize parameters\n    param_rng = np.random.default_rng(p)\n    E = param_rng.normal(scale=0.01, size=(V, d))\n    W = np.zeros((V, d))\n    b = np.zeros(V)\n    \n    # RNG for random masking strategy\n    mask_rng = np.random.default_rng(p) \n\n    # Training loop\n    for _ in range(T):\n        grad_E_batch = np.zeros_like(E)\n        grad_W_batch = np.zeros_like(W)\n        grad_b_batch = np.zeros_like(b)\n\n        for s in dataset:\n            # --- Forward Pass ---\n            x = E[s]  # (L, d)\n            c = (np.sum(x, axis=0) - x) / (L - 1)  # (L, d)\n            z = c @ W.T + b  # (L, V)\n            probs = stable_softmax(z, axis=1)  # (L, V)\n            \n            y_one_hot = np.zeros_like(probs)\n            y_one_hot[np.arange(L), s] = 1\n            delta_z = probs - y_one_hot  # (L, V)\n\n            # --- Mask Selection ---\n            if strategy == 'saliency':\n                # Saliency is based on gradient of full loss\n                # grad_x = (W.T @ (sum(delta_z) - delta_z_i)) / (L * (L-1))\n                sum_delta_z = np.sum(delta_z, axis=0) # (V,)\n                grad_x_saliency = ((sum_delta_z - delta_z) @ W) / (L * (L - 1)) # (L, d)\n                saliency_scores = np.linalg.norm(grad_x_saliency, axis=1)\n                mask_indices = np.argsort(saliency_scores)[-k:]\n            else:  # random\n                mask_indices = mask_rng.choice(L, k, replace=False)\n\n            # --- Gradient Calculation for Update ---\n            grad_b = np.sum(delta_z[mask_indices], axis=0) / k\n            grad_W = (delta_z[mask_indices].T @ c[mask_indices]) / k\n            \n            grad_x = np.zeros_like(x)\n            sum_masked_delta_z = np.sum(delta_z[mask_indices], axis=0) # (V,)\n            \n            common_grad_term = (sum_masked_delta_z @ W) / (k * (L-1)) # (d,)\n            grad_x[:] = common_grad_term\n            \n            # For positions j that are in the mask, subtract its own contribution\n            for j in mask_indices:\n                correction = (delta_z[j] @ W) / (k*(L-1))\n                grad_x[j] -= correction\n                \n            grad_E = np.zeros_like(E)\n            np.add.at(grad_E, s, grad_x)\n            \n            grad_E_batch += grad_E\n            grad_W_batch += grad_W\n            grad_b_batch += grad_b\n\n        # --- SGD Update ---\n        E -= eta * (grad_E_batch / B)\n        W -= eta * (grad_W_batch / B)\n        b -= eta * (grad_b_batch / B)\n\n    # --- Final Evaluation ---\n    total_loss = 0\n    for s in dataset:\n        x = E[s]\n        c = (np.sum(x, axis=0) - x) / (L - 1)\n        z = c @ W.T + b\n        probs = stable_softmax(z, axis=1)\n        \n        # Loss over all positions\n        log_probs = np.log(probs[np.arange(L), s])\n        seq_loss = -np.mean(log_probs)\n        total_loss += seq_loss\n        \n    return total_loss / B\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3147244"}]}