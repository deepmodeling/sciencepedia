{"hands_on_practices": [{"introduction": "The most straightforward way to generate a sequence is greedy decoding, which picks the most likely token at each step. However, this \"myopic\" approach can be easily trapped by locally optimal choices that lead to a globally suboptimal result. This exercise [@problem_id:3173711] places you in a controlled environment to see this failure firsthand, demonstrating how a simple prefix bias can fool a greedy decoder. By implementing both greedy and beam search, you will directly compare their outputs and use the log-probability advantage to quantify how beam search can overcome such search errors.", "problem": "You are to implement and analyze a synthetic sequence-to-sequence (seq2seq) decoding setup that reveals the behavior of greedy and beam search decoding when faced with structured biases. The implementation must be a complete, runnable program that constructs a reversible cipher, defines a token-level probabilistic next-token model with controlled biases, decodes using greedy and beam search, and reports diagnostic metrics over a fixed test suite.\n\nFundamental base: Use the chain rule of probability for sequences. For an input sequence $x = (x_1, x_2, \\dots, x_T)$ over a vocabulary $\\mathcal{V}$, and an output sequence $y = (y_1, y_2, \\dots, y_{T'}, \\text{EOS})$ over the extended vocabulary $\\mathcal{V}_e = \\mathcal{V} \\cup \\{\\text{EOS}\\}$, the conditional probability is\n$$\nP(y \\mid x) = \\prod_{t=1}^{T'+1} P(y_t \\mid y_{<t}, x),\n$$\nwhere $y_{<t} = (y_1, \\dots, y_{t-1})$ and $\\text{EOS}$ denotes End Of Sequence (EOS). The decoderâ€™s objective is to find a sequence $\\hat{y}$ that maximizes the log-probability\n$$\n\\log P(\\hat{y} \\mid x) = \\sum_{t=1}^{T'+1} \\log P(\\hat{y}_t \\mid \\hat{y}_{<t}, x).\n$$\n\nModel specification:\n- Vocabulary: $\\mathcal{V} = \\{\\text{a}, \\text{b}, \\text{c}, \\text{d}, \\text{e}, \\text{f}\\}$ and EOS is a special token $\\text{<e>}$ with $\\mathcal{V}_e = \\mathcal{V} \\cup \\{\\text{<e>}\\}$.\n- Reversible cipher mapping: Define a fixed bijection $M: \\mathcal{V} \\to \\mathcal{V}$, which is used to construct the ground-truth target sequence $y^\\star$ by mapping each input token and appending EOS:\n$$\ny^\\star_t = \n\\begin{cases}\nM(x_t), & 1 \\le t \\le T, \\\\\n\\text{<e>}, & t = T+1.\n\\end{cases}\n$$\n- Next-token model: At each decoding step $t$, define logits $\\ell_t(v)$ for $v \\in \\mathcal{V}_e$ as a sum of independent components:\n  1. Content alignment: a weight $\\alpha_{\\text{content}} \\ge 0$ added to the logit of the correct target token $y^\\star_t$,\n  2. Prefix bias: for a distractor prefix token $d$ and length $L_{\\text{prefix}}$, add a weight $\\alpha_{\\text{prefix}} \\ge 0$ to the logit of $d$ if $t \\le L_{\\text{prefix}}$,\n  3. Repetition bias: add a weight $\\alpha_{\\text{repeat}} \\ge 0$ to the logit of the previously generated token $\\hat{y}_{t-1}$ if $t > 1$,\n  4. EOS encouragement: add a weight $\\alpha_{\\text{eos}} \\ge 0$ to the logit of $\\text{<e>}$ when $t = T+1$,\n  5. Zero-mean Gaussian noise: add $\\beta_{v,t} \\sim \\mathcal{N}(0, \\sigma^2)$ independently across $v$ and $t$.\n  The conditional distribution is the softmax over logits:\n  $$\n  P(y_t = v \\mid y_{<t}, x) = \\frac{\\exp(\\ell_t(v))}{\\sum_{u \\in \\mathcal{V}_e} \\exp(\\ell_t(u))}.\n  $$\n\nDecoders:\n- Greedy decoding: At each step $t$, choose $\\hat{y}_t = \\arg\\max_{v \\in \\mathcal{V}_e} P(y_t = v \\mid \\hat{y}_{<t}, x)$ and stop when EOS is generated or when $t = T+1$.\n- Beam search decoding: Maintain the top-$B$ partial hypotheses ranked by cumulative log-probability. At each step, expand each non-ended hypothesis by all tokens in $\\mathcal{V}_e$, accumulate log-probabilities, keep the best $B$ hypotheses, and stop when either all beams have ended or $t = T+1$. Return the highest-probability ended hypothesis if any, otherwise the highest-probability partial hypothesis.\n\nDiagnostics to compute per test case:\n1. Greedy correctness: a boolean indicating whether the greedy output exactly matches $y^\\star$ token-by-token, including EOS.\n2. Beam correctness: a boolean indicating whether the beam output exactly matches $y^\\star$ token-by-token, including EOS.\n3. Greedy earliest divergence index: the smallest $t \\in \\{1, \\dots, T+1\\}$ where the greedy output differs from $y^\\star$; return $-1$ if there is no divergence.\n4. Greedy prefix distractor match count: the number of initial positions $t \\in \\{1, \\dots, \\min(L_{\\text{prefix}}, T+1)\\}$ where the greedy output equals the distractor token $d$.\n5. Log-probability advantage: the float value $\\Delta = \\log P_{\\text{beam}} - \\log P_{\\text{greedy}}$ computed under the same model and noise grid.\n\nTest suite:\nUse the following fixed test cases, with the distractor token $d$ set to $\\text{a}$ in all cases and the cipher mapping $M$ defined as $M(\\text{a})=\\text{d}$, $M(\\text{b})=\\text{f}$, $M(\\text{c})=\\text{e}$, $M(\\text{d})=\\text{b}$, $M(\\text{e})=\\text{c}$, $M(\\text{f})=\\text{a}$.\n- Case $1$: $x=\\text{\"abc\"}$, $\\alpha_{\\text{content}}=3.0$, $\\alpha_{\\text{prefix}}=1.0$, $L_{\\text{prefix}}=2$, $\\alpha_{\\text{repeat}}=0.5$, $\\alpha_{\\text{eos}}=4.0$, $\\sigma=0.1$, $B=2$, random seed $=1$.\n- Case $2$: $x=\\text{\"fed\"}$, $\\alpha_{\\text{content}}=2.5$, $\\alpha_{\\text{prefix}}=4.0$, $L_{\\text{prefix}}=3$, $\\alpha_{\\text{repeat}}=0.2$, $\\alpha_{\\text{eos}}=4.0$, $\\sigma=0.0$, $B=3$, random seed $=2$.\n- Case $3$: $x=\\text{\"b\"}$, $\\alpha_{\\text{content}}=2.0$, $\\alpha_{\\text{prefix}}=2.0$, $L_{\\text{prefix}}=1$, $\\alpha_{\\text{repeat}}=0.0$, $\\alpha_{\\text{eos}}=2.0$, $\\sigma=0.0$, $B=2$, random seed $=3$.\n- Case $4$: $x=\\text{\"ace\"}$, $\\alpha_{\\text{content}}=2.0$, $\\alpha_{\\text{prefix}}=0.5$, $L_{\\text{prefix}}=2$, $\\alpha_{\\text{repeat}}=2.5$, $\\alpha_{\\text{eos}}=3.0$, $\\sigma=0.1$, $B=2$, random seed $=4$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a list of the form $[\\text{g\\_correct}, \\text{b\\_correct}, \\text{greedy\\_div\\_idx}, \\text{greedy\\_prefix\\_match\\_count}, \\Delta]$, where booleans are printed as $\\text{True}$ or $\\text{False}$, integers are printed in base-$10$, and the float $\\Delta$ is rounded to six decimal places. For example, the outer structure must look like\n$$\n[\\,[\\text{True},\\text{False},2,1,0.123456],\\dots]\\,.\n$$", "solution": "The user's request is to implement and analyze a synthetic sequence-to-sequence (seq2seq) decoding model. This involves creating a probabilistic model with specific biases, implementing greedy and beam search decoding algorithms, and computing a set of diagnostic metrics for a given test suite.\n\nThe problem is computationally well-defined, scientifically grounded in the principles of probabilistic modeling and search algorithms common in deep learning, and all necessary parameters and procedures are specified. Therefore, the problem is valid and a solution can be constructed.\n\nThe implementation proceeds in a structured manner, first establishing the foundational components of the model and then building the decoding and analysis logic upon them.\n\n**1. Model and Environment Formalization**\n\nThe core of the problem is a probabilistic model for generating an output sequence $y$ given an input sequence $x$. The vocabulary $\\mathcal{V} = \\{\\text{a}, \\text{b}, \\text{c}, \\text{d}, \\text{e}, \\text{f}\\}$ is extended with a special End-Of-Sequence token, $\\text{<e>}$, to form $\\mathcal{V}_e$. For computational convenience, these tokens are mapped to integer indices from $0$ to $6$.\n\nThe ground-truth target sequence, $y^\\star$, is defined by a fixed reversible cipher $M: \\mathcal{V} \\to \\mathcal{V}$ applied to the input sequence $x$, followed by the $\\text{<e>}$ token. For an input $x$ of length $T$, $y^\\star$ will have length $T+1$.\n\nThe conditional probability $P(y_t \\mid y_{<t}, x)$ is the central component of the model. It is defined via the softmax function applied to a vector of logits, $\\ell_t$. Each logit $\\ell_t(v)$ for a token $v \\in \\mathcal{V}_e$ is calculated as a sum of several bias terms and a noise term, as specified in the problem statement:\n$$\n\\ell_t(v) = \\delta_{v, y^\\star_t}\\alpha_{\\text{content}} + \\mathbb{I}(t \\le L_{\\text{prefix}})\\delta_{v,d}\\alpha_{\\text{prefix}} + \\mathbb{I}(t > 1)\\delta_{v, \\hat{y}_{t-1}}\\alpha_{\\text{repeat}} + \\mathbb{I}(t=T+1)\\delta_{v,\\text{<e>}}\\alpha_{\\text{eos}} + \\beta_{v,t}\n$$\nwhere $\\delta_{i,j}$ is the Kronecker delta, $\\mathbb{I}(\\cdot)$ is the indicator function, and $\\beta_{v,t} \\sim \\mathcal{N}(0, \\sigma^2)$ is a noise term. A single function, `get_logits`, is implemented to compute this sum based on the current decoding step $t$, the previously generated prefix $\\hat{y}_{<t}$, the ground-truth sequence $y^\\star$, and the model parameters. To ensure reproducibility, the noise terms $\\beta_{v,t}$ are pre-sampled from a normal distribution for all steps and tokens using a fixed random seed for each test case and stored in a `noise_grid`. The log-probabilities are calculated using the numerically stable log-softmax function.\n\n**2. Decoding Algorithms**\n\nThe objective is to find the sequence $\\hat{y}$ that maximizes the total log-probability, $\\log P(\\hat{y} \\mid x) = \\sum_t \\log P(\\hat{y}_t \\mid \\hat{y}_{<t}, x)$. Two standard algorithms are implemented to approximate this maximization.\n\n*   **Greedy Decoding**: This is a direct, deterministic approach. At each time step $t$, the algorithm selects the token with the highest conditional probability (or log-probability):\n    $$ \\hat{y}_t = \\arg\\max_{v \\in \\mathcal{V}_e} P(y_t = v \\mid \\hat{y}_{<t}, x) $$\n    The process is repeated until the $\\text{<e>}$ token is generated or the maximum sequence length of $T+1$ is reached. While computationally efficient, this \"myopic\" strategy can lead to sub-optimal overall sequences, as an early, locally-optimal choice may preclude a better sequence of choices later on. The implementation tracks the cumulative log-probability of the generated sequence.\n\n*   **Beam Search Decoding**: This algorithm mitigates the risk of greedy search by keeping track of the $B$ most probable partial sequences (hypotheses) at each step, where $B$ is the beam width. The process is as follows:\n    1.  At step $t$, each of the $B$ hypotheses from step $t-1$ that has not yet terminated is expanded by considering all possible next tokens from $\\mathcal{V}_e$. This generates $B \\times |\\mathcal{V}_e|$ new candidate sequences.\n    2.  The cumulative log-probability of each new candidate is calculated by adding the log-probability of the new token to that of its parent hypothesis.\n    3.  All candidates, including hypotheses that had already terminated by generating an $\\text{<e>}$ token in previous steps, are collected and ranked by their cumulative log-probability.\n    4.  The top $B$ candidates are selected to form the new set of hypotheses for the next step.\n    The search terminates when all $B$ hypotheses have generated an $\\text{<e>}$ token or the maximum length $T+1$ is reached. The final output is chosen as the highest-probability hypothesis that has generated an $\\text{<e>}$ token; if no such hypothesis exists, the highest-probability partial hypothesis is returned, as per the problem specification.\n\n**3. Diagnostic Metrics**\n\nTo analyze the behavior of the decoders, five specific metrics are computed for each test case.\n1.  **Correctness (Greedy and Beam)**: A boolean indicating if the generated sequence is identical to the ground-truth sequence $y^\\star$.\n2.  **Greedy Earliest Divergence Index**: The first time step $t$ at which the greedy sequence differs from $y^\\star$. This helps pinpoint where the greedy strategy fails.\n3.  **Greedy Prefix Distractor Match Count**: This measures the influence of the `prefix_bias` by counting how many times the greedy decoder chose the distractor token within its active prefix length.\n4.  **Log-probability Advantage**: The value $\\Delta = \\log P(\\hat{y}_{\\text{beam}}) - \\log P(\\hat{y}_{\\text{greedy}})$ is a crucial metric. A positive $\\Delta$ indicates that beam search found a sequence that is more probable under the model's own distribution than the one found by greedy search, demonstrating its superior search capability. This comparison is performed using the same `noise_grid` for both decoders to ensure fairness.\n\nThese components are integrated into a single program that iterates through the specified test suite, performs the decoding and analysis for each case, and formats the results into the required output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import log_softmax\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # --- Constants and Mappings ---\n    VOCAB = ['a', 'b', 'c', 'd', 'e', 'f']\n    EOS = '<e>'\n    FULL_VOCAB = VOCAB + [EOS]\n    TOKEN_TO_IDX = {token: i for i, token in enumerate(FULL_VOCAB)}\n    IDX_TO_TOKEN = {i: token for i, token in enumerate(FULL_VOCAB)}\n    VOCAB_SIZE = len(FULL_VOCAB)\n\n    CIPHER = {'a': 'd', 'b': 'f', 'c': 'e', 'd': 'b', 'e': 'c', 'f': 'a'}\n    DISTRACTOR_TOKEN = 'a'\n\n    def get_ground_truth(x_str: str) -> list[str]:\n        \"\"\"Generates the ground-truth target sequence y*.\"\"\"\n        y_star = [CIPHER[token] for token in x_str]\n        y_star.append(EOS)\n        return y_star\n\n    def get_logits(t: int, T: int, y_prefix: list[str], y_star: list[str], params: dict, noise_grid: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the logits for the next token distribution.\"\"\"\n        logits = np.zeros(VOCAB_SIZE)\n\n        # 1. Content alignment\n        y_star_t_token = y_star[t - 1]\n        y_star_t_idx = TOKEN_TO_IDX[y_star_t_token]\n        logits[y_star_t_idx] += params['alpha_content']\n\n        # 2. Prefix bias\n        if t <= params['L_prefix']:\n            d_idx = TOKEN_TO_IDX[DISTRACTOR_TOKEN]\n            logits[d_idx] += params['alpha_prefix']\n\n        # 3. Repetition bias\n        if t > 1 and y_prefix:\n            prev_token = y_prefix[-1]\n            if prev_token in TOKEN_TO_IDX:\n                prev_token_idx = TOKEN_TO_IDX[prev_token]\n                logits[prev_token_idx] += params['alpha_repeat']\n\n        # 4. EOS encouragement\n        if t == T + 1:\n            eos_idx = TOKEN_TO_IDX[EOS]\n            logits[eos_idx] += params['alpha_eos']\n\n        # 5. Gaussian noise\n        logits += noise_grid[t - 1]\n\n        return logits\n\n    def greedy_decode(x_str: str, y_star: list[str], params: dict, noise_grid: np.ndarray) -> tuple[list[str], float]:\n        \"\"\"Performs greedy decoding.\"\"\"\n        T = len(x_str)\n        y_greedy = []\n        total_log_prob = 0.0\n\n        for t in range(1, T + 2):\n            logits = get_logits(t, T, y_greedy, y_star, params, noise_grid)\n            log_probs = log_softmax(logits)\n            \n            best_token_idx = np.argmax(log_probs)\n            best_token = IDX_TO_TOKEN[best_token_idx]\n            \n            y_greedy.append(best_token)\n            total_log_prob += log_probs[best_token_idx]\n            \n            if best_token == EOS:\n                break\n                \n        return y_greedy, total_log_prob\n\n    def beam_search_decode(x_str: str, y_star: list[str], params: dict, noise_grid: np.ndarray) -> tuple[list[str], float]:\n        \"\"\"Performs beam search decoding.\"\"\"\n        T = len(x_str)\n        B = params['B']\n        \n        # Each beam is a tuple: (sequence, log_probability, is_ended)\n        beams = [([], 0.0, False)]\n\n        for t in range(1, T + 2):\n            all_candidates = []\n\n            for seq, log_prob, ended in beams:\n                if ended:\n                    all_candidates.append((seq, log_prob, True))\n                    continue\n                \n                logits = get_logits(t, T, seq, y_star, params, noise_grid)\n                log_probs = log_softmax(logits)\n\n                for i in range(VOCAB_SIZE):\n                    token = IDX_TO_TOKEN[i]\n                    new_seq = seq + [token]\n                    new_log_prob = log_prob + log_probs[i]\n                    is_beam_ended = (token == EOS)\n                    all_candidates.append((new_seq, new_log_prob, is_beam_ended))\n            \n            sorted_candidates = sorted(all_candidates, key=lambda item: item[1], reverse=True)\n            beams = sorted_candidates[:B]\n\n            if all(b[2] for b in beams):\n                break\n\n        ended_hypotheses = [b for b in beams if b[2]]\n        \n        if ended_hypotheses:\n            best_hyp = max(ended_hypotheses, key=lambda x: x[1])\n        else:\n            best_hyp = max(beams, key=lambda x: x[1])\n\n        return best_hyp[0], best_hyp[1]\n\n    def calculate_diagnostics(greedy_seq, beam_seq, y_star, greedy_log_p, beam_log_p, params):\n        # 1. Greedy correctness\n        greedy_correct = (greedy_seq == y_star)\n        \n        # 2. Beam correctness\n        beam_correct = (beam_seq == y_star)\n\n        # 3. Greedy earliest divergence index\n        greedy_div_idx = -1\n        min_len = min(len(greedy_seq), len(y_star))\n        for i in range(min_len):\n            if greedy_seq[i] != y_star[i]:\n                greedy_div_idx = i + 1\n                break\n        if greedy_div_idx == -1 and len(greedy_seq) != len(y_star):\n            greedy_div_idx = min_len + 1\n\n        # 4. Greedy prefix distractor match count\n        greedy_prefix_match_count = 0\n        limit = min(params['L_prefix'], len(greedy_seq))\n        for i in range(limit):\n            if greedy_seq[i] == DISTRACTOR_TOKEN:\n                greedy_prefix_match_count += 1\n                \n        # 5. Log-probability advantage\n        delta = beam_log_p - greedy_log_p\n        \n        return [greedy_correct, beam_correct, greedy_div_idx, greedy_prefix_match_count, delta]\n\n    def process_case(params):\n        \"\"\"Processes a single test case.\"\"\"\n        np.random.seed(params['seed'])\n        x_str = params['x']\n        T = len(x_str)\n        \n        y_star = get_ground_truth(x_str)\n        max_len = T + 1\n        noise_grid = np.random.normal(0, params['sigma'], size=(max_len, VOCAB_SIZE))\n\n        greedy_seq, greedy_log_p = greedy_decode(x_str, y_star, params, noise_grid)\n        beam_seq, beam_log_p = beam_search_decode(x_str, y_star, params, noise_grid)\n        \n        return calculate_diagnostics(greedy_seq, beam_seq, y_star, greedy_log_p, beam_log_p, params)\n\n    # --- Test Suite ---\n    test_cases = [\n        {'x': \"abc\", 'alpha_content': 3.0, 'alpha_prefix': 1.0, 'L_prefix': 2, 'alpha_repeat': 0.5, 'alpha_eos': 4.0, 'sigma': 0.1, 'B': 2, 'seed': 1},\n        {'x': \"fed\", 'alpha_content': 2.5, 'alpha_prefix': 4.0, 'L_prefix': 3, 'alpha_repeat': 0.2, 'alpha_eos': 4.0, 'sigma': 0.0, 'B': 3, 'seed': 2},\n        {'x': \"b\", 'alpha_content': 2.0, 'alpha_prefix': 2.0, 'L_prefix': 1, 'alpha_repeat': 0.0, 'alpha_eos': 2.0, 'sigma': 0.0, 'B': 2, 'seed': 3},\n        {'x': \"ace\", 'alpha_content': 2.0, 'alpha_prefix': 0.5, 'L_prefix': 2, 'alpha_repeat': 2.5, 'alpha_eos': 3.0, 'sigma': 0.1, 'B': 2, 'seed': 4},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        results.append(result)\n\n    def format_result(res_list):\n        g_c, b_c, g_div, g_pre, delta = res_list\n        return f'[{g_c},{b_c},{g_div},{g_pre},{delta:.6f}]'\n\n    formatted_results = [format_result(r) for r in results]\n    print(f'[{\",\".join(formatted_results)}]')\n\nsolve()\n```", "id": "3173711"}, {"introduction": "While beam search is a significant improvement over greedy decoding, it isn't without its own quirks. Because sequence log-probability is a sum of negative numbers, the score naturally decreases with every token added, creating an inherent bias towards shorter sequences. This practice [@problem_id:3132476] tackles this fundamental issue head-on by introducing length normalization. You will experiment with the normalization exponent $\\beta$ to see how it systematically favors longer or shorter outputs, and discover how to tune it to achieve a desired length based on an external evaluation metric.", "problem": "You are to construct and analyze a toy Sequence-to-Sequence (seq2seq) model for conditional text generation and implement both greedy decoding and beam search decoding with a tunable length normalization exponent. The goal is to understand how the length normalization exponent affects decoding preference for short versus long outputs and to analyze the optimal exponent under a Bilingual Evaluation Understudy (BLEU)-like brevity penalty.\n\nFundamental base:\n- Let the conditional generative model define, for each input sequence $x$, a distribution over output tokens via a next-token conditional probability $p(y_t \\mid y_{&lt;t}, x)$, where $y_t$ is the token at time index $t$ and $y_{&lt;t}$ denotes the previously generated tokens.\n- The probability of a full output sequence including an End Of Sequence (EOS) token is the product of conditional probabilities; equivalently, the log-probability is the sum of the log of next-token probabilities.\n- Greedy decoding selects $\\arg\\max_{y_t} p(y_t \\mid y_{&lt;t}, x)$ at each step until the EOS token is emitted or a maximum length is reached. If $L_{\\max}$ is reached without emitting $\\text{\"EOS\"}$, force $\\text{\"EOS\"}$ to end the sequence.\n- Beam search decoding maintains the top $k$ partial sequences according to a scoring function. The score we will use is\n$$\ns(y \\mid x, \\beta) \\;=\\; \\frac{1}{L(y)^{\\beta}} \\sum_{t=1}^{T} \\log p(y_t \\mid y_{&lt;t}, x),\n$$\nwhere $L(y)$ is the number of non-EOS tokens in the candidate sequence $y$, $T = L(y)+1$ includes the EOS emission, and $\\beta \\ge 0$ is the length normalization exponent. When $L(y)=0$, interpret the denominator $L(y)^{\\beta}$ as $1$ for scoring purposes to avoid division by zero. Beam width is denoted by $k$.\n\nBLEU-like score:\n- For a reference sequence of length $r$ and a candidate sequence of length $c$, define clipped unigram precision $P_1$ as\n$$\nP_1 \\;=\\; \\frac{\\sum_{w \\in V} \\min\\!\\left(\\text{count}_\\text{cand}(w), \\text{count}_\\text{ref}(w)\\right)}{c},\n$$\nwith the convention $P_1 = 0$ if $c = 0$. The brevity penalty is\n$$\n\\text{BP} \\;=\\;\n\\begin{cases}\n1 & \\text{if } c \\ge r,\\\\\n\\exp\\!\\left(1 - \\frac{r}{c}\\right) & \\text{if } 0 < c < r,\\\\\n0 & \\text{if } c = 0.\n\\end{cases}\n$$\nThe BLEU-like score is $S = \\text{BP} \\cdot P_1$.\n\nToy model specification:\n- Vocabulary is $V = \\{\\text{\"a\"}, \\text{\"b\"}, \\text{\"c\"}, \\text{\"d\"}, \\text{\"EOS\"}\\}$.\n- Inputs are to be categorized into three types $x \\in \\{\\text{S}, \\text{M}, \\text{L}\\}$ with the following conditional next-token distributions (all probabilities are in $[0,1]$ and sum to $1$ for each conditioning context):\n  - Input type $\\text{S}$ (short-preferring dynamics):\n    - $p(\\cdot \\mid \\langle s \\rangle, \\text{S})$: $\\{\\text{\"a\"}: 0.55, \\text{\"b\"}: 0.45\\}$.\n    - $p(\\cdot \\mid \\text{\"a\"}, \\text{S})$: $\\{\\text{\"EOS\"}: 0.70, \\text{\"b\"}: 0.30\\}$.\n    - $p(\\cdot \\mid \\text{\"b\"}, \\text{S})$: $\\{\\text{\"c\"}: 0.80, \\text{\"EOS\"}: 0.20\\}$.\n    - $p(\\cdot \\mid \\text{\"c\"}, \\text{S})$: $\\{\\text{\"EOS\"}: 0.80, \\text{\"d\"}: 0.20\\}$.\n    - $p(\\cdot \\mid \\text{\"d\"}, \\text{S})$: $\\{\\text{\"EOS\"}: 0.90\\}$.\n  - Input type $\\text{M}$ (moderate continuation but reference is short):\n    - $p(\\cdot \\mid \\langle s \\rangle, \\text{M})$: $\\{\\text{\"a\"}: 0.60, \\text{\"b\"}: 0.40\\}$.\n    - $p(\\cdot \\mid \\text{\"a\"}, \\text{M})$: $\\{\\text{\"EOS\"}: 0.80, \\text{\"b\"}: 0.20\\}$.\n    - $p(\\cdot \\mid \\text{\"b\"}, \\text{M})$: $\\{\\text{\"c\"}: 0.70, \\text{\"EOS\"}: 0.30\\}$.\n    - $p(\\cdot \\mid \\text{\"c\"}, \\text{M})$: $\\{\\text{\"EOS\"}: 0.90\\}$.\n    - $p(\\cdot \\mid \\text{\"d\"}, \\text{M})$: $\\{\\text{\"EOS\"}: 1.00\\}$.\n  - Input type $\\text{L}$ (long-preferring dynamics):\n    - $p(\\cdot \\mid \\langle s \\rangle, \\text{L})$: $\\{\\text{\"a\"}: 1.00\\}$.\n    - $p(\\cdot \\mid \\text{\"a\"}, \\text{L})$: $\\{\\text{\"b\"}: 0.90, \\text{\"EOS\"}: 0.10\\}$.\n    - $p(\\cdot \\mid \\text{\"b\"}, \\text{L})$: $\\{\\text{\"c\"}: 0.90, \\text{\"EOS\"}: 0.10\\}$.\n    - $p(\\cdot \\mid \\text{\"c\"}, \\text{L})$: $\\{\\text{\"d\"}: 0.90, \\text{\"EOS\"}: 0.10\\}$.\n    - $p(\\cdot \\mid \\text{\"d\"}, \\text{L})$: $\\{\\text{\"EOS\"}: 0.90, \\text{\"a\"}: 0.10\\}$.\n\nDecoding details:\n- Greedy decoding: at each step select the token $y_t$ with maximum $p(y_t \\mid y_{&lt;t}, x)$ and stop when $\\text{\"EOS\"}$ is emitted or a fixed maximum length $L_{\\max}$ is reached. If $L_{\\max}$ is reached without emitting $\\text{\"EOS\"}$, force $\\text{\"EOS\"}$ to end the sequence.\n- Beam search decoding with beam width $k$: maintain top $k$ beams ranked by $s(y \\mid x, \\beta)$, expand non-ended beams by one token per iteration, and include ended beams (those that emitted $\\text{\"EOS\"}$) in ranking without further expansion. If all beams have ended or the iteration reaches $L_{\\max}$, stop. If no beam has ended at termination, append $\\text{\"EOS\"}$ to the top-ranked beam. In case of ties on $s(y \\mid x, \\beta)$, prefer the shorter $L(y)$.\n\nBLEU-like evaluation and $\\beta$ tuning:\n- For a fixed grid $\\beta \\in \\{0.00, 0.50, 1.00\\}$, decode the best beam under each $\\beta$ and compute the BLEU-like score $S$ against the given reference. Select\n$$\n\\beta^{\\star} \\;=\\; \\arg\\max_{\\beta \\in \\{0.00, 0.50, 1.00\\}} S(\\text{decode}(x; \\beta), \\text{ref}),\n$$\nbreaking ties by choosing the smallest $\\beta$.\n\nTest suite and required output:\n- Use the following three test cases, each specified as $(x, k, L_{\\max}, \\text{reference})$:\n  1. $(\\text{S}, 2, 5, [\\text{\"a\"}, \\text{\"b\"}, \\text{\"c\"}])$.\n  2. $(\\text{M}, 1, 5, [\\text{\"a\"}])$.\n  3. $(\\text{L}, 3, 6, [\\text{\"a\"}, \\text{\"b\"}, \\text{\"c\"}, \\text{\"d\"}])$.\n- For each test case, compute four quantities:\n  - $L_\\text{greedy}$: the length (number of non-EOS tokens) produced by greedy decoding.\n  - $L_{\\text{beam}, \\beta=0}$: the length from beam search decoding with $\\beta = 0$.\n  - $L_{\\text{beam}, \\beta=1}$: the length from beam search decoding with $\\beta = 1$.\n  - $\\beta^{\\star}$: the optimal $\\beta$ from the grid under the BLEU-like brevity penalty.\n- Final output format: Your program should produce a single line of output containing a list of three lists, where each inner list corresponds to a test case and contains $[L_\\text{greedy}, L_{\\text{beam}, \\beta=0}, L_{\\text{beam}, \\beta=1}, \\beta^{\\star}]$. The outer list and each inner list must be printed using square brackets, and numbers must be in their standard textual representation (e.g., $[1,1,2,1.0]$). For example, the output should look like $[[\\cdots],[\\cdots],[\\cdots]]$.", "solution": "The user has provided a problem requiring the implementation and analysis of greedy decoding and beam search decoding algorithms for a specified toy sequence-to-sequence model. The analysis focuses on the effect of a length normalization exponent, $\\beta$, on the generated sequence length and its optimality with respect to a Bilingual Evaluation Understudy (BLEU)-like scoring metric.\n\n### **Problem Validation**\nThe problem statement has been validated and is deemed sound. It is scientifically grounded in established principles of natural language processing and deep learning, specifically sequence decoding strategies. The problem is well-posed, with all model parameters, algorithms, and evaluation metrics defined formally and unambiguously. The inclusion of explicit tie-breaking rules and edge-case handling ensures that a unique, verifiable solution exists. The setup is self-contained and objective. A minor inconsistency was noted where the problem text states all conditional probability distributions sum to $1$, but two provided distributions in the toy model (for input types $\\text{S}$ and $\\text{M}$) do not. This is considered a non-critical flaw, as the missing probability mass can be assumed to be zero for other tokens without affecting the specified algorithms, which rely on the relative probabilities of specified successor tokens. Therefore, the problem is solvable as stated.\n\n### **Algorithmic Principles and Design**\n\n**1. Greedy Decoding**\nThis is a deterministic decoding strategy that makes a locally optimal choice at each time step. For a given partial sequence $y_{<t}$, greedy decoding selects the next token $y_t$ that maximizes the conditional probability $p(y_t \\mid y_{<t}, x)$.\n$$\ny_t = \\arg\\max_{y \\in V} p(y \\mid y_{<t}, x)\n$$\nThis process is myopic; it does not account for the downstream consequences of a decision. While computationally efficient, it can lead to suboptimal overall sequences, often producing sequences that are too short because it may choose a high-probability path to an early End-Of-Sequence (EOS) token.\n\n**2. Beam Search Decoding**\nBeam search is a heuristic search algorithm that explores a larger portion of the search space than greedy decoding. It mitigates the risk of myopic decisions by maintaining a \"beam\" of the top $k$ most promising candidate sequences (hypotheses) at each time step. The score for a candidate sequence $y$ is given by a length-normalized log-probability:\n$$\ns(y \\mid x, \\beta) = \\frac{1}{L(y)^{\\beta}} \\sum_{t=1}^{T} \\log p(y_t \\mid y_{<t}, x)\n$$\nwhere $L(y)$ is the sequence length (excluding EOS) and $\\beta$ is the length normalization exponent.\n\n- **Effect of $\\beta$**:\n  - A value of $\\beta=0$ means the score is simply the sum of log-probabilities. Since log-probabilities are negative (for probabilities less than $1$), this score is always decreasing with sequence length, creating a natural bias towards shorter sequences.\n  - A value of $\\beta=1$ corresponds to the average log-probability per token. This normalization counteracts the bias towards short sequences by penalizing hypotheses that terminate early with a low-probability EOS token.\n  - Higher values of $\\beta > 1$ more strongly favor longer sequences.\n\nThe algorithm proceeds by expanding each of the $k$ active hypotheses with all possible next tokens, creating a new set of candidates. These candidates, along with previously completed (EOS-terminated) hypotheses, are re-ranked according to the scoring function $s(y \\mid x, \\beta)$. The top $k$ are retained for the next step. This process continues until a maximum length $L_{\\max}$ is reached or all hypotheses in the beam have terminated with an EOS token.\n\n**3. BLEU-like Score and Optimal $\\beta$ Tuning**\nThe performance of different decoding strategies is evaluated using a custom metric, $S = \\text{BP} \\cdot P_1$, which mimics the principles of the BLEU score.\n- **Clipped Unigram Precision ($P_1$)**: This measures how many tokens in the candidate sequence also appear in the reference, clipped by the reference count to prevent reward for over-generating a correct token.\n- **Brevity Penalty (BP)**: This component, $\\text{BP} = \\exp(1 - r/c)$ for candidate length $c < r$ (reference length), severely penalizes sequences that are shorter than the reference.\n\nThe optimal length normalization exponent, $\\beta^{\\star}$, is the one that produces a sequence yielding the highest score $S$ against a given reference. This tuning process finds the value of $\\beta$ that best aligns the model's sequence-scoring preference with the length preference inherent in the external evaluation metric (i.e., the brevity penalty).\n\n### **Analysis of Test Cases**\n\n**Test Case 1: $(x=\\text{S}, k=2, L_{\\max}=5, \\text{ref}=[\\text{\"a\"}, \\text{\"b\"}, \\text{\"c\"}])$**\n- **Greedy**: The path is $\\langle s \\rangle \\to \\text{\"a\"}$ ($p=0.55$) $\\to \\text{\"EOS\"}$ ($p=0.70$). The algorithm greedily takes the high-probability path to termination. Result: length $L_\\text{greedy} = 1$.\n- **Beam Search ($\\beta=0$)**: The score is the raw sum of log-probabilities. The sequence `[\"a\", \"EOS\"]` has a log-probability of $\\log(0.55) + \\log(0.70) \\approx -0.954$. Any longer sequence, such as `[\"b\", \"c\", \"EOS\"]` ($\\log(0.45)+\\log(0.8)+\\log(0.8) \\approx -1.245$), accumulates more negative log-probability terms and thus scores lower. The algorithm prefers the shortest high-probability path. Result: length $L_{\\text{beam}, \\beta=0} = 1$.\n- **Beam Search ($\\beta=1$)**: The score is the average log-probability. The sequence `[\"a\", \"EOS\"]` has an average log-prob of $\\approx -0.954/1^1 = -0.954$. The sequence `[\"b\", \"c\", \"EOS\"]` has a score of $\\approx -1.245/2^1 = -0.6225$. The normalization by length makes the longer sequence `[\"b\", \"c\"]` preferable. Result: length $L_{\\text{beam}, \\beta=1} = 2$.\n- **Optimal $\\beta^{\\star}$**: The reference length is $r=3$. The sequence of length $c=1$ (`[\"a\"]`) gets a severe brevity penalty $\\exp(1 - 3/1) \\approx 0.135$. The sequence of length $c=2$ (`[\"b\", \"c\"]`) gets a milder penalty $\\exp(1 - 3/2) \\approx 0.607$. This results in a much higher BLEU-like score for the longer sequence. The optimal $\\beta$ is the one that produced this longer sequence. Both $\\beta=0.50$ and $\\beta=1.00$ produce length $2$. By the tie-breaking rule, $\\beta^{\\star}=0.50$.\n\n**Test Case 2: $(x=\\text{M}, k=1, L_{\\max}=5, \\text{ref}=[\\text{\"a\"}])$**\n- With a beam width of $k=1$, beam search behaves similarly to greedy search, but optimizes the global sequence score $s(y|x, \\beta)$ rather than the local next-token probability. In this model, the path $\\langle s \\rangle \\to \\text{\"a\"} \\to \\text{\"EOS\"}$ is overwhelmingly probable. Any deviation results in a significantly lower score, regardless of the $\\beta$ value. Consequently, all decoding methods produce the sequence `[\"a\"]` of length $1$.\n- For $\\beta^{\\star}$ tuning, since all values of $\\beta$ produce the same sequence of length $c=1$ which perfectly matches the reference ($r=1$), the BLEU-like score is consistently $1$. The tie-breaking rule selects the smallest $\\beta$, so $\\beta^{\\star}=0.00$.\n\n**Test Case 3: $(x=\\text{L}, k=3, L_{\\max}=6, \\text{ref}=[\\text{\"a\"}, \\text{\"b\"}, \\text{\"c\"}, \\text{\"d\"}])$**\n- The model dynamics for input type $\\text{L}$ are strongly biased towards generating the sequence `[\"a\", \"b\", \"c\", \"d\"]`. At each step, the probability of continuing the sequence is $0.90$, while the probability of early termination is only $0.10$. This large probability difference means that the path `[\"a\", \"b\", \"c\", \"d\", \"EOS\"]` will dominate the search, regardless of the decoding algorithm or the value of $\\beta$. The penalty for taking a low-probability EOS token is too large for any length normalization to overcome. All methods produce the sequence of length $4$.\n- As in case 2, since all $\\beta$ values produce the same optimal sequence, the tie-breaking rule selects $\\beta^{\\star}=0.00$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import Counter\n\ndef solve():\n    \"\"\"\n    Main function to run the decoding and analysis for all test cases.\n    \"\"\"\n    PROB_MODELS = {\n        'S': {\n            '<s>': {'a': 0.55, 'b': 0.45},\n            'a': {'EOS': 0.70, 'b': 0.30},\n            'b': {'c': 0.80, 'EOS': 0.20},\n            'c': {'EOS': 0.80, 'd': 0.20},\n            'd': {'EOS': 0.90},\n        },\n        'M': {\n            '<s>': {'a': 0.60, 'b': 0.40},\n            'a': {'EOS': 0.80, 'b': 0.20},\n            'b': {'c': 0.70, 'EOS': 0.30},\n            'c': {'EOS': 0.90},\n            'd': {'EOS': 1.00},\n        },\n        'L': {\n            '<s>': {'a': 1.00},\n            'a': {'b': 0.90, 'EOS': 0.10},\n            'b': {'c': 0.90, 'EOS': 0.10},\n            'c': {'d': 0.90, 'EOS': 0.10},\n            'd': {'EOS': 0.90, 'a': 0.10},\n        }\n    }\n    \n    test_cases = [\n        ('S', 2, 5, ['a', 'b', 'c']),\n        ('M', 1, 5, ['a']),\n        ('L', 3, 6, ['a', 'b', 'c', 'd']),\n    ]\n\n    beta_grid = [0.00, 0.50, 1.00]\n    \n    final_results = []\n\n    def greedy_decode(model_type, l_max):\n        model = PROB_MODELS[model_type]\n        sequence = []\n        current_token = '<s>'\n        for _ in range(l_max):\n            if current_token not in model or not model[current_token]:\n                break\n            next_token = max(model[current_token], key=model[current_token].get)\n            if next_token == 'EOS':\n                break\n            sequence.append(next_token)\n            current_token = next_token\n        return sequence\n\n    def beam_search_decode(model_type, k, l_max, beta):\n        model = PROB_MODELS[model_type]\n        \n        # A beam is a tuple: (sequence, log_prob, has_ended_flag)\n        beams = [(['<s>'], 0.0, False)]\n\n        for _ in range(l_max):\n            candidates = []\n            all_ended = True\n            for seq, logp, ended in beams:\n                if ended:\n                    candidates.append((seq, logp, ended))\n                    continue\n                \n                all_ended = False\n                last_tok = seq[-1]\n\n                if last_tok not in model: continue\n\n                for next_tok, prob in model[last_tok].items():\n                    new_seq = seq + [next_tok]\n                    new_logp = logp + np.log(prob)\n                    is_eos = next_tok == 'EOS'\n                    candidates.append((new_seq, new_logp, is_eos))\n\n            if all_ended:\n                break\n\n            def get_score(beam_tuple):\n                seq, logp, _ = beam_tuple\n                non_eos_tokens = [tok for tok in seq if tok not in ['<s>', 'EOS']]\n                length = len(non_eos_tokens)\n                \n                denominator = length**beta if length > 0 else 1.0\n                score_val = logp / denominator\n                return score_val, length\n            \n            # Sort by score (desc), then by length (asc) for tie-breaking\n            candidates.sort(key=lambda b: (get_score(b)[0], -get_score(b)[1]), reverse=True)\n            beams = candidates[:k]\n        \n        # Select best beam and post-process\n        best_seq, _, best_ended = beams[0]\n\n        if not best_ended:\n            best_seq.append('EOS')\n\n        final_tokens = [tok for tok in best_seq if tok not in ['<s>', 'EOS']]\n        return final_tokens\n\n    def calculate_bleu_like(candidate, reference):\n        c = len(candidate)\n        r = len(reference)\n\n        if c == 0:\n            return 0.0\n\n        bp = 1.0\n        if 0 < c < r:\n            bp = np.exp(1 - r / c)\n\n        cand_counts = Counter(candidate)\n        ref_counts = Counter(reference)\n        \n        clipped_sum = sum(min(cand_counts[w], ref_counts[w]) for w in cand_counts)\n        \n        p1 = clipped_sum / c\n        return bp * p1\n\n\n    for model_type, k, l_max, reference in test_cases:\n        # 1. Greedy decoding\n        greedy_seq = greedy_decode(model_type, l_max)\n        l_greedy = len(greedy_seq)\n\n        # 2. Beam search for beta=0 and beta=1\n        beam_seq_b0 = beam_search_decode(model_type, k, l_max, 0.0)\n        l_beam_b0 = len(beam_seq_b0)\n        \n        beam_seq_b1 = beam_search_decode(model_type, k, l_max, 1.0)\n        l_beam_b1 = len(beam_seq_b1)\n\n        # 3. Find beta_star\n        scores = {}\n        for beta in beta_grid:\n            decoded_seq = beam_search_decode(model_type, k, l_max, beta)\n            scores[beta] = calculate_bleu_like(decoded_seq, reference)\n            \n        beta_star = max(scores, key=lambda b: (scores[b], -b))\n\n        case_result = [l_greedy, l_beam_b0, l_beam_b1, beta_star]\n        final_results.append(case_result)\n        \n    # Format output as specified\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in final_results]\n    print(f\"[{','.join(inner_strings)}]\")\n\nsolve()\n```", "id": "3132476"}, {"introduction": "One of the most common failure modes of sequence-to-sequence models is \"degenerate repetition,\" where the decoder gets stuck in a loop and outputs repetitive text. This advanced practice [@problem_id:3173641] moves beyond simple search algorithms to an active intervention strategy. You will implement a penalty based on the similarity of decoder hidden states, learning to discourage the model from re-entering areas of the state space it has already visited. This provides a powerful tool for promoting novelty and preventing your model from getting stuck.", "problem": "You are asked to implement a deterministic simulation of a Sequence-to-Sequence (Seq2Seq) decoder that exhibits degenerate repetition loops and to mitigate them by adding a state-loop penalty to the pre-softmax scores during greedy decoding. The goal is to demonstrate, from first principles, why such a penalty reduces degeneracy and to quantify the reduction via explicit metrics. All mathematical symbols must be interpreted as real-valued vectors or matrices with matching shapes, unless stated otherwise.\n\nBackground and fundamental base:\n- A Sequence-to-Sequence (Seq2Seq) decoder produces a sequence of tokens by iteratively updating a hidden state $h_t$ of dimension $H$ based on the previously emitted token and emitting the next token by scoring candidates. A common recurrence is a Recurrent Neural Network (RNN), where one uses an update of the form $h_{t+1} = \\tanh(A h_t + C e_{y_t})$, with $A \\in \\mathbb{R}^{H \\times H}$, $C \\in \\mathbb{R}^{H \\times D}$, $e_{y_t} \\in \\mathbb{R}^{D}$ being the embedding of token $y_t$, and $\\tanh(\\cdot)$ the elementwise hyperbolic tangent.\n- At each time step $t$, the decoder produces raw scores $z_t \\in \\mathbb{R}^{V}$ over a vocabulary of size $V$ using $z_t = W h_t + b$, with $W \\in \\mathbb{R}^{V \\times H}$ and $b \\in \\mathbb{R}^{V}$. The Softmax function converts scores into probabilities, but greedy decoding simply selects $\\arg\\max_v z_{t,v}$, which is equivalent to maximizing the pre-Softmax score due to Softmax monotonicity.\n- Cosine similarity between two nonzero vectors $u, v \\in \\mathbb{R}^{H}$ is defined as $\\cos(u, v) = \\dfrac{u \\cdot v}{\\|u\\| \\, \\|v\\|}$, where $u \\cdot v$ denotes the dot product and $\\|\\cdot\\|$ the Euclidean norm. To avoid division by zero, use a small $\\epsilon = 10^{-8}$ added to denominators as needed.\n- Degenerate loops arise when the decoder hidden state revisits previous regions in state space, often yielding repetitive tokens.\n\nYour tasks:\n- Implement a greedy decoder with a penalty that detects and discourages state-space loops before choosing a token.\n- Use a fixed, reproducible random initialization for all neural parameters to produce a controlled tendency to repeat. Do not train any parameters.\n\nDecoder dynamics and penalty:\n- Embeddings: let the embedding matrix be $E \\in \\mathbb{R}^{D \\times V}$ and denote column $v$ by $e_v \\in \\mathbb{R}^{D}$.\n- State update: for a chosen token $y_t \\in \\{0, 1, \\dots, V-1\\}$, update $h_{t+1} = \\tanh(A h_t + C e_{y_t})$.\n- Scores: compute raw scores $z_t = W h_t + b$ at time $t$.\n- Hypothetical next state for candidate $v$: $\\tilde{h}_{t+1}^{(v)} = \\tanh(A h_t + C e_v)$.\n- Let the stored history of previous states be $\\{h_0, h_1, \\dots, h_t\\}$.\n- Define the maximum cosine similarity for candidate $v$ as $s_v = \\max_{0 \\le j \\le t} \\cos\\big(\\tilde{h}_{t+1}^{(v)}, h_j\\big)$.\n- Given a threshold $\\tau \\in \\mathbb{R}$ and penalty strength $\\lambda \\in \\mathbb{R}_{\\ge 0}$, define the penalty for candidate $v$ as\n  $$\n  \\Delta_v = \n  \\begin{cases}\n  \\lambda \\, s_v & \\text{if } s_v > \\tau, \\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n  $$\n- Adjusted scores are $\\hat{z}_{t,v} = z_{t,v} - \\Delta_v$.\n- Greedy choice at step $t$ is $y_t = \\arg\\max_{v \\in \\{0,\\dots,V-1\\}} \\hat{z}_{t,v}$.\n\nLoop metrics to compute:\n- Let the decoding run for $T$ steps, producing tokens $\\{y_1, y_2, \\dots, y_T\\}$ and states $\\{h_1, h_2, \\dots, h_T\\}$ from an initial $h_0$.\n- State-loop fraction: for each $t \\in \\{1, 2, \\dots, T\\}$, compute $s^{\\text{actual}}_t = \\max_{0 \\le j < t} \\cos(h_t, h_j)$. Count it as a loop event if $s^{\\text{actual}}_t > \\tau$. The state-loop fraction is the number of loop events divided by $T$.\n- Immediate-repeat fraction: for each $t \\in \\{2, 3, \\dots, T\\}$, check whether $y_t = y_{t-1}$. The immediate-repeat fraction is the number of such events divided by $(T-1)$.\n\nFixed parameter initialization (random but reproducible):\n- Set vocabulary size $V = 6$, hidden dimension $H = 4$, embedding dimension $D = 4$, and decoding length $T = 25$.\n- Initialize a pseudo-random number generator with seed $s = 123$.\n- Draw $E \\in \\mathbb{R}^{D \\times V}$ with entries sampled independently from a normal distribution with mean $0$ and standard deviation $0.6$. Denote columns by $e_v$.\n- Draw $A \\in \\mathbb{R}^{H \\times H}$ as $A = 0.9 I_H + 0.1 M_A$, where $I_H$ is the $H \\times H$ identity and $M_A$ has entries sampled independently from a normal distribution with mean $0$ and standard deviation $1$.\n- Draw $C \\in \\mathbb{R}^{H \\times D}$ as $C = 0.5 I_{H,D} + 0.05 M_C$, where $I_{H,D}$ is the $H \\times D$ matrix with ones on the main diagonal and zeros elsewhere (when $H=D$, this is the usual identity; otherwise this is the rectangular identity), and $M_C$ has entries sampled independently from a normal distribution with mean $0$ and standard deviation $1$.\n- Draw $W \\in \\mathbb{R}^{V \\times H}$ with entries sampled independently from a normal distribution with mean $0$ and standard deviation $0.7$.\n- Draw $b \\in \\mathbb{R}^{V}$ with entries sampled independently from a normal distribution with mean $0$ and standard deviation $0.1$, then add a bias boost to encourage a particular token at baseline: set $b_0 \\leftarrow b_0 + 0.7$ and $b_1 \\leftarrow b_1 + 0.3$.\n- Initialize $h_0 \\in \\mathbb{R}^{H}$ by sampling entries independently from a normal distribution with mean $0$ and standard deviation $0.05$.\n\nPenalty hyperparameters test suite:\n- Use five test cases, each specified by $(\\lambda, \\tau)$:\n  - Case $1$: $(\\lambda, \\tau) = (0.0, 0.95)$.\n  - Case $2$: $(\\lambda, \\tau) = (1.0, 0.95)$.\n  - Case $3$: $(\\lambda, \\tau) = (2.0, 0.90)$.\n  - Case $4$: $(\\lambda, \\tau) = (3.0, 0.80)$.\n  - Case $5$: $(\\lambda, \\tau) = (2.0, 1.10)$.\n\nImplementation requirements:\n- Implement the decoder and penalty exactly as described.\n- Use $\\epsilon = 10^{-8}$ to stabilize cosine similarity computations in denominators.\n- Decode for exactly $T = 25$ steps for each test case.\n- For each test case, compute and record two floats: the state-loop fraction and the immediate-repeat fraction, in that order.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the two floats for one test case. For example, the output should look like:\n  - \"[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]\"\n- No extra text or lines should be printed.\n\nYour code must be a complete, runnable program in Python that follows the specified initialization, decoding, penalty, metrics, and output format. No external input is required and no physical units or angle units apply in this task. All numeric answers must be expressed as decimal numbers.", "solution": "The supplied problem is valid. It is a well-posed, scientifically grounded computational task within the domain of deep learning, specifically concerning the dynamics of sequence-to-sequence (Seq2Seq) models. The problem provides a complete and consistent set of definitions, parameters, and initial conditions, allowing for a deterministic and unique solution.\n\nThe objective is to simulate a recurrent neural network (RNN) based decoder and analyze a method for mitigating output degeneracy, a common failure mode where the model produces repetitive sequences. This is achieved by implementing a penalty mechanism that discourages the decoder's hidden state from revisiting regions of the state space it has previously occupied. The analysis involves executing this simulation across a suite of hyperparameter configurations and quantifying the impact on two specific metrics: the state-loop fraction and the immediate-repeat fraction.\n\nThe simulation is defined by the following components and dynamics:\n\nFirst, we establish the model's parameters, dimensions, and initial state. The vocabulary size is $V=6$, the hidden state dimension is $H=4$, the token embedding dimension is $D=4$, and the decoder will run for $T=25$ steps. A pseudo-random number generator is seeded with $s=123$ to ensure reproducibility. The model parameters are initialized as follows:\n- The token embedding matrix $E \\in \\mathbb{R}^{D \\times V}$ has entries drawn from a normal distribution $\\mathcal{N}(0, 0.6^2)$. Each column $e_v$ of $E$ represents the embedding for token $v$.\n- The state transition matrix $A \\in \\mathbb{R}^{H \\times H}$ is initialized as $A = 0.9 I_H + 0.1 M_A$, where $I_H$ is the $H \\times H$ identity matrix and $M_A$ contains entries from $\\mathcal{N}(0, 1)$. This structure biases the network towards preserving its hidden state, a condition that can contribute to repetitive behavior.\n- The input projection matrix $C \\in \\mathbb{R}^{H \\times D}$ is initialized as $C = 0.5 I_{H,D} + 0.05 M_C$, where $I_{H,D}$ is the rectangular identity matrix (which is the standard identity since $H=D=4$) and $M_C$ has entries from $\\mathcal{N}(0, 1)$.\n- The output projection matrix $W \\in \\mathbb{R}^{V \\times H}$ has entries from $\\mathcal{N}(0, 0.7^2)$.\n- The output bias vector $b \\in \\mathbb{R}^{V}$ has entries from $\\mathcal{N}(0, 0.1^2)$, with an additional manual boost applied to two tokens ($b_0 \\leftarrow b_0 + 0.7$ and $b_1 \\leftarrow b_1 + 0.3$) to increase the baseline probability of generating them.\n- The initial hidden state $h_0 \\in \\mathbb{R}^{H}$ is initialized with entries from $\\mathcal{N}(0, 0.05^2)$.\n\nThe decoding process is iterative. Starting with the initial state $h_0$, the decoder generates a sequence of tokens $y_1, y_2, \\dots, y_T$ and a corresponding sequence of hidden states $h_1, h_2, \\dots, h_T$. The procedure for generating token $y_{t+1}$ and state $h_{t+1}$ from state $h_t$ (for $t=0, 1, \\dots, T-1$) is as follows:\n\n1.  **Score Calculation**: Raw scores (logits) for all vocabulary tokens are computed from the current hidden state $h_t$: $z_t = W h_t + b$.\n\n2.  **Penalty Evaluation**: To prevent the decoder from entering a state-space loop, a penalty is calculated for each candidate token $v \\in \\{0, \\dots, V-1\\}$.\n    - A hypothetical next state $\\tilde{h}_{t+1}^{(v)}$ is computed: $\\tilde{h}_{t+1}^{(v)} = \\tanh(A h_t + C e_v)$.\n    - The similarity of this hypothetical state to all previously visited states $\\{h_0, h_1, \\dots, h_t\\}$ is measured using the cosine similarity, $\\cos(u, v) = \\frac{u \\cdot v}{\\|u\\|\\|v\\| + \\epsilon}$, where $\\epsilon = 10^{-8}$ ensures numerical stability. The maximum similarity is found: $s_v = \\max_{0 \\le j \\le t} \\cos(\\tilde{h}_{t+1}^{(v)}, h_j)$.\n    - The penalty $\\Delta_v$ for candidate $v$ is determined by a strength parameter $\\lambda \\ge 0$ and a threshold $\\tau$. If $s_v > \\tau$, then $\\Delta_v = \\lambda s_v$; otherwise, $\\Delta_v = 0$.\n\n3.  **Token Selection**: The raw scores are adjusted by the penalty: $\\hat{z}_{t,v} = z_{t,v} - \\Delta_v$. The next token $y_{t+1}$ is chosen via greedy decoding, selecting the token with the highest adjusted score: $y_{t+1} = \\arg\\max_{v} \\hat{z}_{t,v}$.\n\n4.  **State Update**: The actual next hidden state $h_{t+1}$ is computed using the chosen token $y_{t+1}$: $h_{t+1} = \\tanh(A h_t + C e_{y_{t+1}})$.\n\nThis loop is repeated $T=25$ times. After the full sequence is generated, two metrics are computed to quantify the level of repetition:\n- **State-loop fraction**: For each generated state $h_t$ where $t \\in \\{1, \\dots, T\\}$, we compute its maximum cosine similarity with all strictly preceding states, $s^{\\text{actual}}_t = \\max_{0 \\le j < t} \\cos(h_t, h_j)$. The fraction of these states for which $s^{\\text{actual}}_t > \\tau$ is the state-loop fraction.\n- **Immediate-repeat fraction**: This is the fraction of generated tokens that are identical to the one immediately preceding them. It is calculated as the number of times $y_t = y_{t-1}$ for $t \\in \\{2, \\dots, T\\}$, divided by $T-1$.\n\nThe simulation is run for five test cases defined by $(\\lambda, \\tau)$ pairs. Case 1, $(\\lambda=0.0, \\tau=0.95)$, serves as a baseline with no penalty. Cases 2-4 introduce and strengthen the penalty. Case 5, $(\\lambda=2.0, \\tau=1.10)$, serves as a control; since cosine similarity cannot exceed $1.0$, the condition $s_v > 1.10$ is never met, making the penalty inactive and the results equivalent to the baseline. This setup allows for a principled analysis of how the penalty mitigates repetition.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a Seq2Seq decoder with a state-loop penalty and calculates repetition metrics.\n    \"\"\"\n    \n    # -------- 1. Parameter Initialization --------\n    V = 6  # Vocabulary size\n    H = 4  # Hidden dimension\n    D = 4  # Embedding dimension\n    T = 25 # Decoding length\n    seed = 123\n    epsilon = 1e-8\n\n    rng = np.random.default_rng(seed)\n\n    # Initialize model parameters\n    E = rng.normal(0, 0.6, size=(D, V))\n    \n    M_A = rng.normal(0, 1, size=(H, H))\n    A = 0.9 * np.eye(H) + 0.1 * M_A\n    \n    # Since H=D, I_HD is the standard identity matrix\n    M_C = rng.normal(0, 1, size=(H, D))\n    C = 0.5 * np.eye(H, D) + 0.05 * M_C\n    \n    W = rng.normal(0, 0.7, size=(V, H))\n    \n    b = rng.normal(0, 0.1, size=V)\n    b[0] += 0.7\n    b[1] += 0.3\n    \n    h0 = rng.normal(0, 0.05, size=H)\n\n    # Test cases for penalty hyperparameters\n    test_cases = [\n        (0.0, 0.95),  # Case 1: Baseline (no penalty)\n        (1.0, 0.95),  # Case 2: Moderate penalty\n        (2.0, 0.90),  # Case 3: Stronger penalty, lower threshold\n        (3.0, 0.80),  # Case 4: Strongest penalty, lowest threshold\n        (2.0, 1.10),  # Case 5: Inactive penalty (tau > 1)\n    ]\n\n    all_results = []\n\n    def cosine_similarity(u, v_matrix):\n        \"\"\"Computes cosine similarity between a vector u and each row of v_matrix.\"\"\"\n        u_norm = np.linalg.norm(u)\n        v_norms = np.linalg.norm(v_matrix, axis=1)\n        \n        # Denominator should not be zero\n        denominators = u_norm * v_norms + epsilon\n        \n        # If a vector in v_matrix is the zero vector, its norm will be 0.\n        # This can lead to division by a near-zero number (epsilon).\n        # Cosine similarity with a zero vector is undefined. We'll treat it as 0.\n        # A more robust check:\n        # denominators = np.maximum(denominators, epsilon)\n\n        dot_products = v_matrix @ u\n        \n        similarities = dot_products / denominators\n        return similarities\n\n    # -------- 2. Run Simulation for Each Test Case --------\n    for lambda_param, tau in test_cases:\n        \n        # Reset state for each test case\n        h_current = np.copy(h0)\n        h_history = [np.copy(h0)]\n        y_generated = []\n\n        # Decoding loop\n        for _ in range(T):\n            h_t = h_current\n            \n            # 2a. Calculate base scores\n            scores_t = W @ h_t + b\n            adjusted_scores = np.copy(scores_t)\n\n            # 2b. Calculate penalty for each candidate token\n            h_history_array = np.array(h_history)\n            \n            for v in range(V):\n                e_v = E[:, v]\n                \n                # Hypothetical next state\n                h_hypothetical = np.tanh(A @ h_t + C @ e_v)\n                \n                # Max cosine similarity with past states\n                similarities = cosine_similarity(h_hypothetical, h_history_array)\n                s_v = np.max(similarities)\n                \n                # Apply penalty if applicable\n                if s_v > tau:\n                    penalty = lambda_param * s_v\n                    adjusted_scores[v] -= penalty\n            \n            # 2c. Choose next token via greedy decoding\n            y_next = np.argmax(adjusted_scores)\n            \n            # 2d. Update actual state and history\n            e_next = E[:, y_next]\n            h_next = np.tanh(A @ h_t + C @ e_next)\n            \n            y_generated.append(y_next)\n            h_history.append(h_next)\n            h_current = h_next\n\n        # -------- 3. Calculate Metrics --------\n        \n        # State-loop fraction\n        loop_events = 0\n        h_generated_states = h_history[1:] # h_1 to h_T\n        for t in range(1, T + 1):\n            h_t = h_history[t]\n            h_past_history = np.array(h_history[:t]) # h_0 to h_{t-1}\n            \n            s_actual_t = np.max(cosine_similarity(h_t, h_past_history))\n            \n            if s_actual_t > tau:\n                loop_events += 1\n        \n        state_loop_fraction = loop_events / T\n        \n        # Immediate-repeat fraction\n        repeat_events = 0\n        for t in range(1, T):\n            if y_generated[t] == y_generated[t-1]:\n                repeat_events += 1\n        \n        immediate_repeat_fraction = repeat_events / (T - 1) if T > 1 else 0.0\n        \n        all_results.append([state_loop_fraction, immediate_repeat_fraction])\n\n    # -------- 4. Format and Print Final Output --------\n    # The output format requires a string representation of a list of lists.\n    # e.g., \"[[0.1, 0.2], [0.3, 0.4]]\"\n    result_str = \"[\" + \",\".join([f\"[{res[0]},{res[1]}]\" for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3173641"}]}