{"hands_on_practices": [{"introduction": "A direct and intuitive approach to prevent catastrophic forgetting is to identify the dimensions of the weight space crucial for previous tasks and simply forbid any changes along those directions. This hands-on practice explores such a projection-based method, where you will use Singular Value Decomposition (SVD) to find the subspace of weights most important to an old task. You will then implement a gradient projection that forces updates for a new task to lie in the orthogonal complement of this protected subspace, thereby preserving the old knowledge with surgical precision [@problem_id:3109216].", "problem": "You are asked to implement and analyze a strategy for continual learning in a linear model that protects weight directions important to previously learned tasks by projecting new task updates onto a null space identified via Singular Value Decomposition (SVD). The goal is to reason from the base definitions of linear least squares, gradients of the squared loss, and orthogonal projections derived from Singular Value Decomposition (SVD). You will quantify how much this protection reduces forgetting, measured as the increase in the old task loss after taking one gradient step on a new task.\n\nDefinitions and framework:\n- Consider a linear model with weights $w \\in \\mathbb{R}^{d}$ and squared loss $L(X,y;w) = \\frac{1}{2n}\\lVert Xw - y \\rVert_{2}^{2}$, where $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^{n}$.\n- For the old task, denote data as $(X_{\\mathrm{old}}, y_{\\mathrm{old}})$ with $X_{\\mathrm{old}} \\in \\mathbb{R}^{n_{\\mathrm{old}} \\times d}$ and $y_{\\mathrm{old}} \\in \\mathbb{R}^{n_{\\mathrm{old}}}$. Pretrain by minimizing the ridge-regularized objective $\\frac{1}{2n_{\\mathrm{old}}}\\lVert X_{\\mathrm{old}} w - y_{\\mathrm{old}} \\rVert_{2}^{2} + \\frac{\\lambda}{2}\\lVert w \\rVert_{2}^{2}$ to obtain $w_{\\mathrm{old}}$. This has the closed-form solution $w_{\\mathrm{old}} = \\left(\\frac{1}{n_{\\mathrm{old}}}X_{\\mathrm{old}}^{\\top}X_{\\mathrm{old}} + \\lambda I\\right)^{-1}\\left(\\frac{1}{n_{\\mathrm{old}}}X_{\\mathrm{old}}^{\\top}y_{\\mathrm{old}}\\right)$ for $\\lambda > 0$.\n- For the new task with data $(X_{\\mathrm{new}}, y_{\\mathrm{new}})$, the gradient of the new loss at $w_{\\mathrm{old}}$ is $g = \\nabla_{w} \\left(\\frac{1}{2n_{\\mathrm{new}}}\\lVert X_{\\mathrm{new}}w - y_{\\mathrm{new}} \\rVert^{2}\\right)\\big\\rvert_{w=w_{\\mathrm{old}}} = \\frac{1}{n_{\\mathrm{new}}}X_{\\mathrm{new}}^{\\top}\\left(X_{\\mathrm{new}}w_{\\mathrm{old}} - y_{\\mathrm{new}}\\right)$.\n\nIdentifying important subspaces via Singular Value Decomposition (SVD) and protecting them:\n- Compute the Singular Value Decomposition (SVD) of $X_{\\mathrm{old}}$ as $X_{\\mathrm{old}} = U\\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{n_{\\mathrm{old}} \\times d}$ having orthonormal columns, $\\Sigma \\in \\mathbb{R}^{d \\times d}$ diagonal with nonnegative entries, and $V \\in \\mathbb{R}^{d \\times d}$ orthogonal. The right singular vectors (columns of $V$) are directions in weight space, and the corresponding singular values quantify sensitivity of old-task outputs to movement along these directions. The span of the top $k$ right singular vectors $V_{k} \\in \\mathbb{R}^{d \\times k}$ defines the important subspace to protect.\n- The orthogonal projector onto the subspace orthogonal to $\\mathrm{span}(V_{k})$ is $P = I - V_{k}V_{k}^{\\top}$. Project the new-task gradient $g$ to $g_{\\perp} = Pg$ and update $w$ by $w_{\\mathrm{proj}} = w_{\\mathrm{old}} - \\eta g_{\\perp}$, where $\\eta > 0$ is a learning rate. For comparison, the naive unprotected update is $w_{\\mathrm{naive}} = w_{\\mathrm{old}} - \\eta g$.\n\nQuantifying forgetting:\n- Define the old-task loss as $L_{\\mathrm{old}}(w) = \\frac{1}{2n_{\\mathrm{old}}}\\lVert X_{\\mathrm{old}}w - y_{\\mathrm{old}} \\rVert_{2}^{2}$. The forgetting induced by an update $w \\mapsto w'$ is $\\Delta L_{\\mathrm{old}} = L_{\\mathrm{old}}(w') - L_{\\mathrm{old}}(w_{\\mathrm{old}})$.\n- For each test case, compute $\\Delta L_{\\mathrm{naive}}$ and $\\Delta L_{\\mathrm{proj}}$ from the respective updates and report the ratio $r = \\frac{\\Delta L_{\\mathrm{proj}}}{\\Delta L_{\\mathrm{naive}}}$. If $\\Delta L_{\\mathrm{naive}}$ is numerically zero (less than a small tolerance), define $r = 0.0$.\n\nData generation for deterministic, unitless experiments:\n- Use a pseudo-random number generator initialized by a specified integer seed $s$ to generate all random quantities in a test case. For each test case, use the following fixed hyperparameters: dimension $d = 6$, old-task sample size $n_{\\mathrm{old}} = 60$, new-task sample size $n_{\\mathrm{new}} = 50$, ridge coefficient $\\lambda = 10^{-6}$, and learning rate $\\eta = 0.5$.\n- Old task: draw $X_{\\mathrm{old}} \\in \\mathbb{R}^{60 \\times 6}$ with independent standard normal entries and $w_{\\ast,\\mathrm{old}} \\in \\mathbb{R}^{6}$ with independent standard normal entries. Set $y_{\\mathrm{old}} = X_{\\mathrm{old}} w_{\\ast,\\mathrm{old}}$ (no observation noise).\n- New task: draw $X_{\\mathrm{new}} \\in \\mathbb{R}^{50 \\times 6}$ with independent standard normal entries. To precisely control the new-task gradient direction, first compute the Singular Value Decomposition (SVD) $X_{\\mathrm{old}} = U\\Sigma V^{\\top}$. Then choose a target direction $a \\in \\mathbb{R}^{6}$ according to the mode of the test case (defined below). Solve $(X_{\\mathrm{new}}^{\\top}X_{\\mathrm{new}})v = a$ for $v \\in \\mathbb{R}^{6}$, and set $y_{\\mathrm{new}} = X_{\\mathrm{new}} w_{\\mathrm{old}} - X_{\\mathrm{new}} v$. This construction yields $g = \\frac{1}{n_{\\mathrm{new}}}a$ exactly, ensuring deterministic alignment of the new-task gradient with $a$.\n- Modes for selecting $a$:\n  - top: set $a$ to the first right singular vector $V[:,1]$ of $X_{\\mathrm{old}}$ (note the indexing is mathematical; in code, use zero-based indexing).\n  - orth: set $a$ to $V[:,k+1]$ if $k < d$, which is guaranteed to be orthogonal to the protected subspace $\\mathrm{span}(V_{k})$.\n  - mix: set $a$ to a normalized convex combination of a protected and an unprotected direction, specifically $a \\propto \\alpha V[:,1] + (1-\\alpha) V[:,j]$, where $j = k+1$ when $k < d$, otherwise $j = 2$, with $\\alpha = 0.6$.\n  - random: draw a standard normal vector and normalize it to unit length.\n\nTest suite:\nImplement the above pipeline for the following five test cases, each specified by a triple $(s,k,\\text{mode})$:\n- $(7, 0, \\text{top})$\n- $(11, 6, \\text{top})$\n- $(5, 2, \\text{top})$\n- $(13, 2, \\text{orth})$\n- $(17, 3, \\text{mix})$\n\nRequired outputs:\n- For each test case, compute the ratio $r = \\frac{\\Delta L_{\\mathrm{proj}}}{\\Delta L_{\\mathrm{naive}}}$ as defined above. The outputs are unitless real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.5,0.0,1.0]\") in the order of the test cases given above.", "solution": "The problem requires an implementation and analysis of a continual learning strategy designed to mitigate catastrophic forgetting in a linear model. The core principle is to protect the knowledge acquired from a previous task (the \"old task\") while learning a new one. This protection is achieved by identifying a subspace of the model's weights that is critical for the old task's performance and then restricting the updates for the new task to be orthogonal to this subspace. The method leverages Singular Value Decomposition (SVD) to identify the critical subspace and orthogonal projection to enforce the learning constraint.\n\nFirst, we establish the mathematical framework. We consider a linear model $f(x; w) = x^{\\top}w$, with weights $w \\in \\mathbb{R}^{d}$. For a dataset $(X, y)$, where $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^{n}$, the performance is measured by the mean squared error, or squared loss, $L(X,y;w) = \\frac{1}{2n}\\lVert Xw - y \\rVert_{2}^{2}$.\n\nThe process begins by training the model on an \"old task,\" defined by data $(X_{\\mathrm{old}}, y_{\\mathrm{old}})$. To obtain the initial weights $w_{\\mathrm{old}}$, we minimize a ridge-regularized objective to ensure a well-posed problem and prevent excessively large weights:\n$$\n\\min_{w} \\left( \\frac{1}{2n_{\\mathrm{old}}}\\lVert X_{\\mathrm{old}} w - y_{\\mathrm{old}} \\rVert_{2}^{2} + \\frac{\\lambda}{2}\\lVert w \\rVert_{2}^{2} \\right)\n$$\nThe solution to this convex optimization problem is given in closed form:\n$$\nw_{\\mathrm{old}} = \\left(\\frac{1}{n_{\\mathrm{old}}}X_{\\mathrm{old}}^{\\top}X_{\\mathrm{old}} + \\lambda I\\right)^{-1}\\left(\\frac{1}{n_{\\mathrm{old}}}X_{\\mathrm{old}}^{\\top}y_{\\mathrm{old}}\\right)\n$$\nwhere $I$ is the $d \\times d$ identity matrix and $\\lambda > 0$ is the regularization coefficient.\n\nNext, to identify the directions in weight space most important to the old task, we perform a Singular Value Decomposition (SVD) on the data matrix $X_{\\mathrm{old}} = U\\Sigma V^{\\top}$. Here, the columns of $V \\in \\mathbb{R}^{d \\times d}$ are the right singular vectors, which form an orthonormal basis for the weight space $\\mathbb{R}^d$. The corresponding singular values on the diagonal of $\\Sigma$ indicate the sensitivity of the old task's predictions to perturbations in the weights along these directions. The subspace spanned by the top $k$ right singular vectors, $\\mathrm{span}(V_{k})$, where $V_k \\in \\mathbb{R}^{d \\times k}$ contains the first $k$ columns of $V$, is considered the \"important subspace\" to be protected.\n\nWhen learning a \"new task\" from data $(X_{\\mathrm{new}}, y_{\\mathrm{new}})$, the standard approach would be to update the weights using a gradient step, $w_{\\mathrm{naive}} = w_{\\mathrm{old}} - \\eta g$, where $\\eta$ is the learning rate and $g$ is the gradient of the new task's loss evaluated at $w_{\\mathrm{old}}$:\n$$\ng = \\nabla_{w} L_{\\mathrm{new}}(w)\\rvert_{w=w_{\\mathrm{old}}} = \\frac{1}{n_{\\mathrm{new}}}X_{\\mathrm{new}}^{\\top}\\left(X_{\\mathrm{new}}w_{\\mathrm{old}} - y_{\\mathrm{new}}\\right)\n$$\nTo protect the old task, we project this gradient onto the subspace orthogonal to the important subspace. The orthogonal projector onto this \"safe\" null space is $P = I - V_{k}V_{k}^{\\top}$. The projected gradient is $g_{\\perp} = Pg$, and the corresponding weight update is $w_{\\mathrm{proj}} = w_{\\mathrm{old}} - \\eta g_{\\perp}$. By construction, the update step $-\\eta g_{\\perp}$ has no component within the protected subspace, minimizing interference with the old task's knowledge.\n\nTo quantify the effectiveness of this strategy, we measure the \"forgetting\" induced by each type of update. Forgetting is defined as the increase in the old task's loss after the update, $\\Delta L_{\\mathrm{old}} = L_{\\mathrm{old}}(w') - L_{\\mathrm{old}}(w_{\\mathrm{old}})$, where $w'$ is the updated weight vector. We can derive an analytical expression for this change. Let $w' = w_{\\mathrm{old}} + \\delta w$.\n$$\nL_{\\mathrm{old}}(w') = \\frac{1}{2n_{\\mathrm{old}}}\\lVert X_{\\mathrm{old}}(w_{\\mathrm{old}} + \\delta w) - y_{\\mathrm{old}} \\rVert_{2}^{2} = \\frac{1}{2n_{\\mathrm{old}}}\\lVert (X_{\\mathrm{old}}w_{\\mathrm{old}} - y_{\\mathrm{old}}) + X_{\\mathrm{old}}\\delta w \\rVert_{2}^{2}\n$$\nExpanding the norm and subtracting $L_{\\mathrm{old}}(w_{\\mathrm{old}})$ gives:\n$$\n\\Delta L_{\\mathrm{old}} = \\frac{1}{n_{\\mathrm{old}}}(X_{\\mathrm{old}}w_{\\mathrm{old}} - y_{\\mathrm{old}})^{\\top}X_{\\mathrm{old}}\\delta w + \\frac{1}{2n_{\\mathrm{old}}}\\delta w^{\\top}X_{\\mathrm{old}}^{\\top}X_{\\mathrm{old}}\\delta w\n$$\nFrom the optimality condition of the ridge regression for $w_{\\mathrm{old}}$, we know $\\nabla_{w}L_{\\mathrm{old}}(w_{\\mathrm{old}}) + \\lambda w_{\\mathrm{old}} = 0$, which implies $\\frac{1}{n_{\\mathrm{old}}}X_{\\mathrm{old}}^{\\top}(X_{\\mathrm{old}}w_{\\mathrm{old}} - y_{\\mathrm{old}}) = -\\lambda w_{\\mathrm{old}}$. Substituting this into the above expression and setting $\\delta w = -\\eta \\Delta w$ (where $\\Delta w$ is $g$ or $g_{\\perp}$), we obtain a computationally efficient formula:\n$$\n\\Delta L_{\\mathrm{old}} = \\eta \\lambda w_{\\mathrm{old}}^{\\top}\\Delta w + \\frac{\\eta^2}{2n_{\\mathrm{old}}} \\Delta w^{\\top}X_{\\mathrm{old}}^{\\top}X_{\\mathrm{old}}\\Delta w\n$$\nUsing this formula, we compute $\\Delta L_{\\mathrm{naive}}$ (with $\\Delta w = g$) and $\\Delta L_{\\mathrm{proj}}$ (with $\\Delta w = g_{\\perp}$). The final metric is the ratio $r = \\frac{\\Delta L_{\\mathrm{proj}}}{\\Delta L_{\\mathrm{naive}}}$, which measures the reduction in forgetting.\n\nThe experimental setup is carefully designed to probe the method's behavior. The new task's data $(X_{\\mathrm{new}}, y_{\\mathrm{new}})$ is constructed such that the resulting gradient $g$ is precisely aligned with a chosen direction $a$, ensuring a deterministic analysis of how the gradient's alignment with the protected subspace affects forgetting. The different modes (`top`, `orth`, `mix`) represent scenarios where the new task is maximally conflicting, completely non-conflicting, or a mixture of both, with respect to the old task's critical directions.\n\nThe implementation proceeds by:\n1.  Initializing a random number generator with a given seed for reproducibility.\n2.  Generating the old task data and computing $w_{\\mathrm{old}}$ via the closed-form ridge regression solution.\n3.  Performing SVD on $X_{\\mathrm{old}}$ to obtain the basis $V$.\n4.  Constructing the new task's gradient direction $a$ based on the test case's mode and the columns of $V$. The gradient is then $g=a/n_{\\mathrm{new}}$.\n5.  Calculating the projected gradient $g_{\\perp} = (I - V_k V_k^{\\top})g$, where $V_k$ consists of the first $k$ columns of $V$.\n6.  Using the derived analytical formula to calculate $\\Delta L_{\\mathrm{naive}}$ and $\\Delta L_{\\mathrm{proj}}$.\n7.  Computing the final ratio $r$.\nThis procedure is repeated for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd, solve\n\ndef solve_problem():\n    \"\"\"\n    Implements and analyzes a continual learning strategy based on SVD and gradient projection.\n    \"\"\"\n\n    def run_one_case(s: int, k: int, mode: str) -> float:\n        \"\"\"\n        Runs a single test case for the continual learning simulation.\n\n        Args:\n            s: The seed for the pseudo-random number generator.\n            k: The number of top singular vectors to protect.\n            mode: The mode for generating the new task's gradient direction.\n\n        Returns:\n            The ratio of forgetting with projection to naive forgetting.\n        \"\"\"\n        # Initialize PRNG with the specified seed for reproducibility\n        rng = np.random.default_rng(s)\n\n        # Fixed hyperparameters from the problem statement\n        d = 6\n        n_old = 60\n        n_new = 50\n        lambda_reg = 1e-6\n        eta = 0.5\n\n        # 1. Generate data for the old task.\n        X_old = rng.standard_normal((n_old, d))\n        w_star_old = rng.standard_normal((d, 1))\n        y_old = X_old @ w_star_old\n\n        # 2. Train on the old task to find w_old.\n        # This is the solution to the ridge regression problem.\n        C = (1 / n_old) * (X_old.T @ X_old) + lambda_reg * np.identity(d)\n        b = (1 / n_old) * (X_old.T @ y_old)\n        # C is symmetric positive-definite, so we can use a specialized solver.\n        w_old = solve(C, b, assume_a='pos')\n\n        # 3. Identify the important subspace for the old task via SVD.\n        # Use thin SVD as n_old > d.\n        # V will have shape (d, d), its columns are the right singular vectors.\n        _, _, Vt = svd(X_old, full_matrices=False)\n        V = Vt.T\n\n        # 4. Generate the new task's gradient direction 'a'.\n        # The new task data is constructed to yield a gradient g = a / n_new.\n        if mode == 'top':\n            # Direction is the most important singular vector (math index 1).\n            a = V[:, 0]\n        elif mode == 'orth':\n            # Direction is orthogonal to the protected k-dimensional subspace (math index k+1).\n            a = V[:, k]\n        elif mode == 'mix':\n            # Direction is a normalized mixture of a protected and an unprotected vector.\n            # alpha * V[:,1] + (1-alpha) * V[:,j]\n            # V[:,1] is code index 0.\n            # j=k+1 (if k<d) is code index k. j=2 (if k>=d) is code index 1.\n            alpha = 0.6\n            v1 = V[:, 0]\n            vj_idx = k if k < d else 1\n            vj = V[:, vj_idx]\n            \n            a_unnormalized = alpha * v1 + (1 - alpha) * vj\n            a = a_unnormalized / np.linalg.norm(a_unnormalized)\n        else:\n            raise ValueError(f\"Unknown mode: {mode}\")\n\n        # Reshape for consistent matrix operations and compute the gradient.\n        a = a.reshape(-1, 1)\n        g = (1 / n_new) * a\n\n        # 5. Project the gradient to be orthogonal to the protected subspace.\n        if k == 0:\n            # No protection, projector is identity.\n            g_perp = g\n        elif k >= d:\n            # Full protection, projector is zero.\n            g_perp = np.zeros_like(g)\n        else:\n            Vk = V[:, :k]\n            # Projector P = I - Vk @ Vk.T\n            P = np.identity(d) - (Vk @ Vk.T)\n            g_perp = P @ g\n\n        # 6. Quantify forgetting (change in old task loss) for both updates.\n        # We use the derived analytical formula for efficiency:\n        # delta_L = eta * lambda * w_old.T @ delta_w + (eta**2 / (2*n_old)) * delta_w.T @ X_old.T @ X_old @ delta_w\n        # where delta_w is either g or g_perp.\n        \n        def calculate_delta_L(delta_w):\n            term1 = eta * lambda_reg * (w_old.T @ delta_w)\n            term2 = (eta**2 / (2 * n_old)) * (delta_w.T @ X_old.T @ X_old @ delta_w)\n            return (term1 + term2).item()\n\n        delta_L_naive = calculate_delta_L(g)\n        delta_L_proj = calculate_delta_L(g_perp)\n\n        # 7. Compute the final ratio r.\n        # Handle the case where the naive update causes no forgetting.\n        if abs(delta_L_naive) < 1e-12:\n            return 0.0\n        else:\n            return delta_L_proj / delta_L_naive\n\n    # The test suite provided in the problem statement.\n    test_cases = [\n        (7, 0, 'top'),\n        (11, 6, 'top'),\n        (5, 2, 'top'),\n        (13, 2, 'orth'),\n        (17, 3, 'mix'),\n    ]\n\n    results = []\n    for case in test_cases:\n        s, k, mode = case\n        result = run_one_case(s, k, mode)\n        results.append(result)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve_problem()\n```", "id": "3109216"}, {"introduction": "Moving from hard constraints to soft penalties, regularization-based methods offer a more flexible way to mitigate forgetting. This exercise delves into Elastic Weight Consolidation (EWC), a seminal strategy that adds a quadratic penalty to the loss function, discouraging changes to parameters deemed important for past tasks. You will implement the core components of EWC, including the calculation of the Fisher Information Matrix to quantify parameter importance, and analyze how this regularization effectively dampens updates within subspaces critical to old tasks [@problem_id:3109267].", "problem": "You must write a complete program that constructs and analyzes a synthetic continual learning scenario for a linear neural network with an embedding layer and a classifier. The network maps an input vector $\\mathbf{x} \\in \\mathbb{R}^d$ through an embedding matrix $\\mathbf{E} \\in \\mathbb{R}^{m \\times d}$ to an embedding $\\mathbf{z} = \\mathbf{E}\\mathbf{x}$, and then applies a classifier matrix $\\mathbf{C} \\in \\mathbb{R}^{K \\times m}$ to produce logits $\\mathbf{s} = \\mathbf{C}\\mathbf{z}$. The predictive distribution over $K$ classes is the softmax $\\mathbf{p} = \\operatorname{softmax}(\\mathbf{s})$ where $p_k = \\exp(s_k)/\\sum_{j=1}^K \\exp(s_j)$. The training loss on a single example $(\\mathbf{x}, \\mathbf{y})$ with one-hot label $\\mathbf{y} \\in \\{0,1\\}^K$ is the cross-entropy $L(\\mathbf{C};\\mathbf{z},\\mathbf{y}) = -\\sum_{k=1}^K y_k \\log p_k$. The gradient with respect to the classifier is $\\nabla_{\\mathbf{C}} L = (\\mathbf{p}-\\mathbf{y})\\mathbf{z}^\\top$. We will use full-batch gradient descent throughout.\n\nIn continual learning, after training on an old task and obtaining parameters $\\mathbf{C}^\\star$, the Elastic Weight Consolidation (EWC) strategy adds a quadratic penalty to discourage movement in directions important for the old task. Let the empirical Fisher Information (diagonal approximation) for the old task be defined as\n$$\n\\mathbf{F}_{\\text{diag}} = \\frac{1}{N} \\sum_{i=1}^N \\operatorname{vec}\\!\\left(\\nabla_{\\mathbf{C}} \\ell_i\\right) \\odot \\operatorname{vec}\\!\\left(\\nabla_{\\mathbf{C}} \\ell_i\\right),\n$$\nwhere $\\ell_i = L(\\mathbf{C}^\\star;\\mathbf{z}_i,\\mathbf{y}_i)$, $\\operatorname{vec}(\\cdot)$ flattens a matrix into a vector, and $\\odot$ denotes elementwise multiplication. The EWC penalty is\n$$\nR(\\mathbf{C}) = \\frac{\\lambda}{2} \\sum_{j} F_{\\text{diag},j}\\left(\\theta_j - \\theta_j^\\star\\right)^2,\n$$\nwhere $\\theta = \\operatorname{vec}(\\mathbf{C})$ and $\\theta^\\star = \\operatorname{vec}(\\mathbf{C}^\\star)$. During training on the new task, the baseline objective is $J_{\\text{base}}(\\mathbf{C}) = \\frac{1}{N'}\\sum_{i=1}^{N'} L(\\mathbf{C};\\mathbf{z}_i',\\mathbf{y}_i')$, and the EWC objective is $J_{\\text{ewc}}(\\mathbf{C}) = J_{\\text{base}}(\\mathbf{C}) + R(\\mathbf{C})$. The gradient of the penalty term with respect to $\\theta$ is $\\nabla_\\theta R(\\mathbf{C}) = \\lambda\\,\\mathbf{F}_{\\text{diag}} \\odot (\\theta - \\theta^\\star)$.\n\nDefine the old-task subspace $\\mathcal{S}_r$ as the span of the $r$ coordinates of $\\theta$ corresponding to the top $r$ entries of $\\mathbf{F}_{\\text{diag}}$ (largest values). Let $\\mathcal{P}_{\\mathcal{S}_r}$ denote the projector that zeroes all coordinates outside these $r$ indices. For gradient descent with learning rate $\\eta$, the update vector at step $t$ is $\\Delta_t = -\\eta \\nabla_\\theta J(\\mathbf{C}_t)$, where $J$ is either $J_{\\text{base}}$ or $J_{\\text{ewc}}$. The cumulative gradient-flow magnitude into $\\mathcal{S}_r$ over $T$ steps is\n$$\nM = \\sum_{t=1}^T \\left\\|\\mathcal{P}_{\\mathcal{S}_r}(\\Delta_t)\\right\\|_2.\n$$\n\nYour program must:\n- Construct a synthetic old task and a synthetic new task entirely in the embedding space by setting $\\mathbf{E} = \\mathbf{I}_m$ (the $m \\times m$ identity). Let the old task generate examples with class-dependent Gaussian means $\\boldsymbol{\\mu}_k^{\\text{old}} \\in \\mathbb{R}^m$ and covariance $\\sigma^2 \\mathbf{I}_m$. Let the new task means be obtained via a random orthogonal rotation $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ applied to the old means: $\\boldsymbol{\\mu}_k^{\\text{new}} = \\mathbf{R}\\boldsymbol{\\mu}_k^{\\text{old}}$. Labels remain one-hot for $K$ classes.\n- Train $\\mathbf{C}$ on the old task with full-batch gradient descent to obtain $\\mathbf{C}^\\star$ while keeping $\\mathbf{E}$ fixed, then compute $\\mathbf{F}_{\\text{diag}}$ at $\\mathbf{C}^\\star$ on the old task.\n- For the new task, compare two strategies: (i) freeze embeddings and fine-tune the classifier using $J_{\\text{base}}$, and (ii) freeze embeddings and fine-tune the classifier using $J_{\\text{ewc}}$ with the EWC penalty applied only to the classifier. In both strategies, start from $\\mathbf{C}^\\star$ and use the same number of steps and learning rate.\n- For each strategy, compute the cumulative gradient-flow magnitude $M$ into the old-task subspace $\\mathcal{S}_r$ and output a boolean indicating whether the EWC strategy reduces this magnitude compared to the baseline, that is, whether $M_{\\text{ewc}} < M_{\\text{base}}$.\n\nAll computations are purely mathematical and must use the definitions above. No physical units or angles are involved. Percentages must not be used.\n\nTest suite:\n- Use $d=m=6$, $K=3$, $N_{\\text{old}}=120$ samples per old class, $N_{\\text{new}}=120$ samples per new class, $\\sigma=0.6$, old-task learning rate $\\eta_{\\text{old}}=0.1$ for $200$ steps, and new-task learning rate $\\eta_{\\text{new}}$ and steps as specified per case below. The class means for the old task must be $\\boldsymbol{\\mu}_k^{\\text{old}} = a\\,\\mathbf{e}_k$ for $k=1,2,3$ where $\\mathbf{e}_k$ is the $k$-th standard basis vector in $\\mathbb{R}^m$ and $a=2.5$, with remaining coordinates zero for each $\\boldsymbol{\\mu}_k^{\\text{old}}$.\n- Define four test cases as tuples $(\\text{seed}, r, \\lambda, \\eta_{\\text{new}}, T)$:\n    1. $(42, 8, 20.0, 0.08, 100)$, general case.\n    2. $(7, 0, 20.0, 0.08, 100)$, boundary case with $r=0$ (empty subspace).\n    3. $(123, 8, 0.0, 0.08, 100)$, edge case with $\\lambda=0$ (no EWC effect).\n    4. $(2024, 18, 50.0, 0.06, 80)$, full-dimension subspace $r=K\\cdot m$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, for the test cases in the order listed (for example, \"[True,False,False,True]\").", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of continual learning within deep learning, well-posed with all necessary constants and definitions provided, and objective in its formulation. The task is to implement a numerical simulation to compare two continual learning strategies, baseline fine-tuning and Elastic Weight Consolidation (EWC), based on a specified metric.\n\nThe solution proceeds through a sequence of computational steps:\n\n### 1. Synthetic Task Generation\n\nThe problem is defined in the embedding space, where the embedding matrix is the identity, $\\mathbf{E} = \\mathbf{I}_m$. The input vectors $\\mathbf{x}$ are thus equivalent to the embedding vectors $\\mathbf{z} \\in \\mathbb{R}^m$.\n\n**Old Task:** The old task consists of $K=3$ classes. For each class $k \\in \\{0, 1, 2\\}$, we generate $N_{\\text{old}}=120$ data points $\\mathbf{z}_i$ by sampling from a multivariate Gaussian distribution $\\mathcal{N}(\\boldsymbol{\\mu}_k^{\\text{old}}, \\sigma^2 \\mathbf{I}_m)$. The specified parameters are $m=6$, $\\sigma=0.6$, and the means are given by $\\boldsymbol{\\mu}_k^{\\text{old}} = a\\,\\mathbf{e}_{k+1}$ with $a=2.5$, where $\\mathbf{e}_{j}$ is the $j$-th standard basis vector in $\\mathbb{R}^m$. The total number of old task samples is $N = K \\times N_{\\text{old}} = 360$. Each sample $(\\mathbf{z}_i, \\mathbf{y}_i)$ includes a one-hot encoded label vector $\\mathbf{y}_i \\in \\{0,1\\}^K$.\n\n**New Task:** A random orthogonal matrix $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ is generated. The means for the new task are created by rotating the old task means: $\\boldsymbol{\\mu}_k^{\\text{new}} = \\mathbf{R}\\boldsymbol{\\mu}_k^{\\text{old}}$. The new task data is then generated by sampling $N_{\\text{new}}=120$ points per class from $\\mathcal{N}(\\boldsymbol{\\mu}_k^{\\text{new}}, \\sigma^2 \\mathbf{I}_m)$. This construction ensures that the intrinsic structure of the class separation is preserved but oriented differently in the embedding space, creating a scenario where the model must adapt.\n\n### 2. Training on the Old Task\n\nThe classifier matrix $\\mathbf{C} \\in \\mathbb{R}^{K \\times m}$ is trained on the full batch of old task data. Starting from an initial matrix $\\mathbf{C}_0 = \\mathbf{0}$, we perform $T_{\\text{old}}=200$ steps of gradient descent with a learning rate of $\\eta_{\\text{old}}=0.1$. The objective function is the average cross-entropy loss over the dataset. The update rule for the classifier at each step is:\n$$\n\\mathbf{C}_{t+1} = \\mathbf{C}_t - \\eta_{\\text{old}} \\nabla_{\\mathbf{C}} J(\\mathbf{C}_t)\n$$\nwhere the gradient of the full-batch loss $J(\\mathbf{C}) = \\frac{1}{N} \\sum_{i=1}^N L(\\mathbf{C}; \\mathbf{z}_i, \\mathbf{y}_i)$ is given by:\n$$\n\\nabla_{\\mathbf{C}} J(\\mathbf{C}) = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{p}_i - \\mathbf{y}_i)\\mathbf{z}_i^\\top = \\frac{1}{N}(\\mathbf{P} - \\mathbf{Y})\\mathbf{Z}^\\top\n$$\nHere, $\\mathbf{P}$ is the matrix of softmax probabilities, $\\mathbf{Y}$ is the matrix of one-hot labels, and $\\mathbf{Z}$ is the matrix of data points, with each sample as a column. The final trained classifier is denoted $\\mathbf{C}^\\star$.\n\n### 3. Fisher Information Calculation\n\nThe diagonal of the empirical Fisher Information matrix, $\\mathbf{F}_{\\text{diag}}$, quantifies the sensitivity of the model's output to changes in its parameters, evaluated on the old task. It is computed at the optimal parameters $\\mathbf{C}^\\star$:\n$$\n\\mathbf{F}_{\\text{diag}} = \\frac{1}{N} \\sum_{i=1}^N \\operatorname{vec}\\!\\left(\\nabla_{\\mathbf{C}} \\ell_i\\right) \\odot \\operatorname{vec}\\!\\left(\\nabla_{\\mathbf{C}} \\ell_i\\right)\n$$\nwhere $\\ell_i = L(\\mathbf{C}^\\star;\\mathbf{z}_i,\\mathbf{y}_i)$ is the loss for the $i$-th old task sample, $\\operatorname{vec}(\\cdot)$ flattens the $K \\times m$ gradient matrix into a vector of size $Km$, and $\\odot$ is the elementwise product. This vector $\\mathbf{F}_{\\text{diag}}$ identifies which parameters are most important for the old task's performance.\n\n### 4. Fine-Tuning on the New Task\n\nWe compare two strategies for adapting the classifier to the new task, both starting from $\\mathbf{C}^\\star$.\n\n**Old-Task Subspace:** We first define the \"important\" old-task subspace $\\mathcal{S}_r$. This subspace is spanned by the $r$ parameter coordinates corresponding to the $r$ largest values in $\\mathbf{F}_{\\text{diag}}$. We define a projector $\\mathcal{P}_{\\mathcal{S}_r}$ which, when applied to a vector, preserves its components in $\\mathcal{S}_r$ and sets all other components to zero.\n\n**Strategies and Metric:**\nFor both baseline and EWC strategies, we run $T$ steps of gradient descent on the new task data with learning rate $\\eta_{\\text{new}}$. Let $\\theta_t = \\operatorname{vec}(\\mathbf{C}_t)$ be the vectorized classifier parameters at step $t$. The update vector is $\\Delta_t = -\\eta_{\\text{new}} \\nabla_\\theta J(\\mathbf{C}_t)$. We track the cumulative magnitude of the updates projected onto the old-task subspace:\n$$\nM = \\sum_{t=1}^T \\left\\|\\mathcal{P}_{\\mathcal{S}_r}(\\Delta_t)\\right\\|_2\n$$\n\n- **Baseline Fine-Tuning:** The objective is simply the new task loss, $J_{\\text{base}}$. The gradient is $\\nabla_\\theta J_{\\text{base}} = \\operatorname{vec}(\\frac{1}{N'}\\sum_{i=1}^{N'} (\\mathbf{p}'_i - \\mathbf{y}'_i)(\\mathbf{z}'_i)^\\top)$. The cumulative flow is denoted $M_{\\text{base}}$.\n\n- **EWC Fine-Tuning:** The objective includes the EWC penalty, $J_{\\text{ewc}}(\\mathbf{C}) = J_{\\text{base}}(\\mathbf{C}) + R(\\mathbf{C})$. The gradient is the sum of the baseline gradient and the penalty gradient:\n$$\n\\nabla_\\theta J_{\\text{ewc}} = \\nabla_\\theta J_{\\text{base}} + \\nabla_\\theta R(\\mathbf{C})\n$$\nwhere $\\nabla_\\theta R(\\mathbf{C}) = \\lambda\\,\\mathbf{F}_{\\text{diag}} \\odot (\\theta - \\theta^\\star)$. The penalty term pulls the parameters $\\theta$ back toward their optimal values for the old task, $\\theta^\\star$, with a strength proportional to their importance ($\\mathbf{F}_{\\text{diag}}$) and the hyperparameter $\\lambda$. The cumulative flow is denoted $M_{\\text{ewc}}$.\n\n### 5. Evaluation\n\nFor each test case, we compute $M_{\\text{base}}$ and $M_{\\text{ewc}}$. The final output is a boolean value indicating whether EWC reduced the parameter updates in the important old-task subspace compared to the baseline, i.e., whether $M_{\\text{ewc}} < M_{\\text{base}}$. The expectation is that EWC, by design, will penalize changes to important old-task parameters, thus reducing this metric. The boundary cases ($r=0$ and $\\lambda=0$) serve as sanity checks: if the subspace is empty or the penalty is zero, no reduction is expected, and the two strategies should yield equal magnitudes.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import ortho_group\n\ndef solve():\n    \"\"\"\n    Main function to run the continual learning simulation for all test cases.\n    \"\"\"\n\n    # --- Problem Constants ---\n    d_dim = 6\n    m_dim = 6\n    K_classes = 3\n    N_old_per_class = 120\n    N_new_per_class = 120\n    sigma = 0.6\n    a_mean_scale = 2.5\n    eta_old = 0.1\n    T_old = 200\n\n    test_cases = [\n        (42, 8, 20.0, 0.08, 100),\n        (7, 0, 20.0, 0.08, 100),\n        (123, 8, 0.0, 0.08, 100),\n        (2024, 18, 50.0, 0.06, 80),\n    ]\n\n    results = []\n\n    def stable_softmax(x, axis=None):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        s = x - np.max(x, axis=axis, keepdims=True)\n        e_s = np.exp(s)\n        return e_s / np.sum(e_s, axis=axis, keepdims=True)\n\n    def generate_data(means, n_per_class, sigma_val, rng_gen):\n        \"\"\"Generates synthetic data for a task.\"\"\"\n        k, m = means.shape\n        n_total = k * n_per_class\n        Z_list = []\n        Y_list = []\n        for class_idx in range(k):\n            # Data: (m, n_per_class)\n            Z_class = rng_gen.normal(loc=means[class_idx, :], scale=sigma_val, size=(n_per_class, m)).T\n            Z_list.append(Z_class)\n            # Labels: (k, n_per_class)\n            y_vec = np.zeros((k, 1))\n            y_vec[class_idx] = 1\n            Y_class = np.tile(y_vec, (1, n_per_class))\n            Y_list.append(Y_class)\n        Z = np.hstack(Z_list)\n        Y = np.hstack(Y_list)\n        return Z, Y\n\n    def run_finetuning(C_init, Z_new, Y_new, eta, T, F_diag, lamb, projector_mask):\n        \"\"\"Runs the fine-tuning loop for a given strategy and computes M.\"\"\"\n        C = C_init.copy()\n        theta_star = C_init.ravel()\n        N_new = Z_new.shape[1]\n        M = 0.0\n        param_shape = C.shape\n\n        for _ in range(T):\n            theta = C.ravel()\n            \n            # Gradients\n            S_new = C @ Z_new\n            P_new = stable_softmax(S_new, axis=0)\n            grad_C_base = (P_new - Y_new) @ Z_new.T / N_new\n            grad_theta_base = grad_C_base.ravel()\n            \n            grad_theta_penalty = lamb * F_diag * (theta - theta_star)\n            grad_theta_total = grad_theta_base + grad_theta_penalty\n            \n            # Cumulative magnitude calculation\n            delta_t = -eta * grad_theta_total\n            delta_t_proj = delta_t * projector_mask\n            M += np.linalg.norm(delta_t_proj)\n            \n            # Parameter update\n            C -= eta * grad_theta_total.reshape(param_shape)\n        \n        return M\n\n    for case in test_cases:\n        seed, r_dim, lamb, eta_new, T_new = case\n        rng = np.random.default_rng(seed)\n\n        # --- 1. Old Task ---\n        N_old_total = K_classes * N_old_per_class\n        \n        # Generate data\n        mus_old = np.zeros((K_classes, m_dim))\n        for k in range(K_classes):\n            mus_old[k, k] = a_mean_scale\n        Z_old, Y_old = generate_data(mus_old, N_old_per_class, sigma, rng)\n\n        # Train on old task to find C_star\n        C = np.zeros((K_classes, m_dim))\n        for _ in range(T_old):\n            S_old = C @ Z_old\n            P_old = stable_softmax(S_old, axis=0)\n            grad_C = (P_old - Y_old) @ Z_old.T / N_old_total\n            C -= eta_old * grad_C\n        C_star = C\n\n        # --- 2. Calculate Fisher Information ---\n        vec_grads_sq_sum = np.zeros(K_classes * m_dim)\n        for i in range(N_old_total):\n            z_i = Z_old[:, i:i+1]\n            y_i = Y_old[:, i:i+1]\n            s_i = C_star @ z_i\n            p_i = stable_softmax(s_i, axis=0)\n            grad_C_i = (p_i - y_i) @ z_i.T\n            vec_grad_i = grad_C_i.ravel()\n            vec_grads_sq_sum += vec_grad_i**2\n        F_diag = vec_grads_sq_sum / N_old_total\n\n        # --- 3. New Task ---\n        N_new_total = K_classes * N_new_per_class\n        \n        # Generate Data\n        R_ortho = ortho_group.rvs(m_dim, random_state=rng)\n        mus_new = (R_ortho @ mus_old.T).T\n        Z_new, Y_new = generate_data(mus_new, N_new_per_class, sigma, rng)\n\n        # --- 4. Define Subspace Projector ---\n        # Get indices of the r largest values in F_diag\n        if r_dim > 0:\n            top_r_indices = np.argsort(F_diag)[-r_dim:]\n        else:\n            top_r_indices = []\n        \n        projector_mask = np.zeros_like(F_diag)\n        projector_mask[top_r_indices] = 1.0\n\n        # --- 5. Run and Compare Strategies ---\n        # Baseline (lambda = 0)\n        M_base = run_finetuning(C_star, Z_new, Y_new, eta_new, T_new, F_diag, 0.0, projector_mask)\n        \n        # EWC\n        M_ewc = run_finetuning(C_star, Z_new, Y_new, eta_new, T_new, F_diag, lamb, projector_mask)\n\n        results.append(M_ewc < M_base)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3109267"}, {"introduction": "An alternative to parameter-based constraints is to preserve the learned *function* itself through knowledge distillation. In this paradigm, the model trained on an old task acts as a \"teacher,\" guiding the \"student\" model as it learns a new task. This practice guides you through a comparison of two powerful distillation strategies: one that transfers knowledge from the teacher's final output probabilities and another that uses its internal feature representations, offering valuable insights into how different levels of guidance can preserve the fine-grained decision boundaries of previous tasks [@problem_id:3109241].", "problem": "You are given a binary-class continual learning scenario in deep learning where a previously trained model on an old task must be preserved while learning a new task. The model architecture is a one-hidden-layer neural network with hyperbolic tangent activation and a softmax output layer. Two continual learning strategies are to be implemented and compared: output-space distillation and feature-space distillation. The goal is to determine which strategy better preserves fine-grained decision boundaries of the old task while learning the new task.\n\nThe fundamental base you must use consists of the following definitions and facts:\n- The softmax function for logits $\\mathbf{z} \\in \\mathbb{R}^C$ is $p_k(\\mathbf{z}) = \\dfrac{\\exp(z_k)}{\\sum_{j=1}^C \\exp(z_j)}$, where $k \\in \\{1,\\dots,C\\}$ and $C$ is the number of classes.\n- The cross-entropy loss between a one-hot target $\\mathbf{y} \\in \\{0,1\\}^C$ and predicted probabilities $\\mathbf{p} \\in [0,1]^C$ is $\\mathcal{L}_{\\mathrm{CE}}(\\mathbf{y},\\mathbf{p}) = -\\sum_{k=1}^C y_k \\log p_k$.\n- The Kullback–Leibler divergence from distribution $\\mathbf{q}$ to $\\mathbf{p}$ is $D_{\\mathrm{KL}}(\\mathbf{q}\\|\\mathbf{p}) = \\sum_{k=1}^C q_k \\log\\left(\\dfrac{q_k}{p_k}\\right)$ and acts as a measure of dissimilarity.\n- For feature-space distillation, a common alignment objective is the mean squared error $\\mathcal{L}_{\\mathrm{MSE}}(\\mathbf{h}^{\\mathrm{old}},\\mathbf{h}^{\\mathrm{new}}) = \\|\\mathbf{h}^{\\mathrm{old}}-\\mathbf{h}^{\\mathrm{new}}\\|_2^2$ between teacher and student hidden representations.\n- The hyperbolic tangent activation is $\\tanh(a) = \\dfrac{e^a - e^{-a}}{e^a + e^{-a}}$ with derivative $\\dfrac{d}{da}\\tanh(a) = 1 - \\tanh^2(a)$.\n\nThe network has input dimension $d=2$, hidden dimension $H$, and output dimension $C=2$. Let the hidden representation be $\\mathbf{h} = \\tanh(\\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)$, logits be $\\mathbf{z} = \\mathbf{h}\\mathbf{W}_2 + \\mathbf{b}_2$, and probabilities $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$. For binary classification, the fine-grained decision boundary can be characterized by the signed margin $m(\\mathbf{x}) = z_1(\\mathbf{x}) - z_0(\\mathbf{x})$, with the boundary at $m(\\mathbf{x})=0$. Preservation of fine-grained decision boundaries is measured by the average absolute change in margin at probe points near the old task boundary.\n\nYou must implement both continual learning strategies:\n- Output-space distillation: while learning the new task, penalize divergence between the teacher’s old-task softened outputs and the student’s outputs at temperature $T$. Use the Kullback–Leibler divergence with softened targets $q^{(T)}(\\mathbf{z}/T)$ and include the standard temperature scaling factor to ensure appropriately scaled gradients.\n- Feature-space distillation: while learning the new task, penalize differences between the teacher’s hidden features on old data and the student’s hidden features on the same data, using the mean squared error.\n\nExperiment protocol to follow:\n1. Train a teacher (old-task model) using cross-entropy on the old task data.\n2. Initialize a student with the same architecture and identical initial parameters for both strategies to ensure a fair comparison.\n3. Train the student on the new task under each strategy separately:\n   - Output-space distillation: minimize the sum of new-task cross-entropy and output-space distillation Kullback–Leibler divergence on old-task data, scaled by a distillation weight $\\lambda_{\\mathrm{out}}$ and the temperature $T$.\n   - Feature-space distillation: minimize the sum of new-task cross-entropy and feature-space mean squared error on old-task data, scaled by a distillation weight $\\lambda_{\\mathrm{feat}}$.\n4. Construct probe points by selecting old-task samples for which the teacher’s signed margin $m_{\\mathrm{old}}(\\mathbf{x})$ is closest to $0$, indicating proximity to the decision boundary. At these probe points, compute the average absolute margin change $\\Delta = \\dfrac{1}{K} \\sum_{i=1}^K \\left| m_{\\mathrm{student}}(\\mathbf{x}_i) - m_{\\mathrm{old}}(\\mathbf{x}_i) \\right|$ for each strategy.\n5. For each test case, output a boolean indicating whether output-space distillation yields a strictly smaller $\\Delta$ than feature-space distillation.\n\nData generation details:\n- Old task: binary classification with class-conditional distributions $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0^{\\mathrm{old}}, \\sigma_{\\mathrm{old}}^2 \\mathbf{I})$ for label $0$ and $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1^{\\mathrm{old}}, \\sigma_{\\mathrm{old}}^2 \\mathbf{I})$ for label $1$.\n- New task: binary classification with class-conditional distributions $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0^{\\mathrm{new}}, \\sigma_{\\mathrm{new}}^2 \\mathbf{I})$ for label $0$ and $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1^{\\mathrm{new}}, \\sigma_{\\mathrm{new}}^2 \\mathbf{I})$ for label $1$.\n\nTest suite:\n- Case $1$: seed $0$, $n_{\\mathrm{old}}=200$, $n_{\\mathrm{new}}=200$, $\\boldsymbol{\\mu}_0^{\\mathrm{old}}=(-1.0,0.0)$, $\\boldsymbol{\\mu}_1^{\\mathrm{old}}=(1.0,0.0)$, $\\sigma_{\\mathrm{old}}=0.4$, $\\boldsymbol{\\mu}_0^{\\mathrm{new}}=(0.0,-1.0)$, $\\boldsymbol{\\mu}_1^{\\mathrm{new}}=(0.0,1.0)$, $\\sigma_{\\mathrm{new}}=0.4$, hidden size $H=16$, learning rate $\\eta=0.05$, teacher iterations $300$, student iterations $300$, distillation weights $\\lambda_{\\mathrm{out}}=0.5$, $\\lambda_{\\mathrm{feat}}=0.5$, temperature $T=2.0$, number of probes $K=40$.\n- Case $2$: seed $1$, $n_{\\mathrm{old}}=240$, $n_{\\mathrm{new}}=240$, $\\boldsymbol{\\mu}_0^{\\mathrm{old}}=(-0.3,0.0)$, $\\boldsymbol{\\mu}_1^{\\mathrm{old}}=(0.3,0.0)$, $\\sigma_{\\mathrm{old}}=0.35$, $\\boldsymbol{\\mu}_0^{\\mathrm{new}}=(0.0,-0.6)$, $\\boldsymbol{\\mu}_1^{\\mathrm{new}}=(0.0,0.6)$, $\\sigma_{\\mathrm{new}}=0.35$, hidden size $H=24$, learning rate $\\eta=0.05$, teacher iterations $400$, student iterations $350$, distillation weights $\\lambda_{\\mathrm{out}}=0.4$, $\\lambda_{\\mathrm{feat}}=0.8$, temperature $T=2.5$, number of probes $K=60$.\n- Case $3$: seed $2$, $n_{\\mathrm{old}}=220$, $n_{\\mathrm{new}}=220$, $\\boldsymbol{\\mu}_0^{\\mathrm{old}}=(-0.8,-0.2)$, $\\boldsymbol{\\mu}_1^{\\mathrm{old}}=(0.8,0.2)$, $\\sigma_{\\mathrm{old}}=0.45$, $\\boldsymbol{\\mu}_0^{\\mathrm{new}}=(0.2,-0.8)$, $\\boldsymbol{\\mu}_1^{\\mathrm{new}}=(-0.2,0.8)$, $\\sigma_{\\mathrm{new}}=0.45$, hidden size $H=20$, learning rate $\\eta=0.05$, teacher iterations $350$, student iterations $320$, distillation weights $\\lambda_{\\mathrm{out}}=0.0$, $\\lambda_{\\mathrm{feat}}=0.6$, temperature $T=2.0$, number of probes $K=50$.\n\nYour program must:\n- Implement the training and evaluation procedure above.\n- For each test case, compute whether output-space distillation achieves a strictly smaller average absolute margin change $\\Delta$ than feature-space distillation on the probe points, returning a boolean.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3\\right]$). No physical units, angle units, or percentages are required in this problem.", "solution": "The user has provided a problem statement that requires the implementation and comparison of two continual learning strategies—output-space distillation and feature-space distillation—in the context of a two-layer neural network. The problem is well-defined, scientifically grounded in the principles of deep learning, and computationally feasible. All necessary parameters, data generation procedures, and evaluation metrics are specified, rendering the problem valid.\n\nThe solution will be developed by first outlining the mathematical framework, including the network architecture and loss functions, followed by a description of the training and evaluation protocol.\n\n### 1. Model and Data Specification\n\nThe model is a one-hidden-layer neural network designed for binary classification ($C=2$). The input $\\mathbf{x} \\in \\mathbb{R}^d$ (with $d=2$) is processed as follows:\n- The pre-activation of the hidden layer is $\\mathbf{a}_1 = \\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1$, where $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times H}$ and $\\mathbf{b}_1 \\in \\mathbb{R}^{H}$ are the weights and biases of the first layer, and $H$ is the hidden dimension.\n- The hidden representation is obtained by applying the hyperbolic tangent activation function: $\\mathbf{h} = \\tanh(\\mathbf{a}_1)$.\n- The logits (pre-activation of the output layer) are $\\mathbf{z} = \\mathbf{h}\\mathbf{W}_2 + \\mathbf{b}_2$, where $\\mathbf{W}_2 \\in \\mathbb{R}^{H \\times C}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^{C}$.\n- The final class probabilities are computed using the softmax function: $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$, where $p_k(\\mathbf{z}) = \\frac{\\exp(z_k)}{\\sum_{j=1}^C \\exp(z_j)}$.\n\nData for both the old and new tasks are generated from 2D Gaussian distributions. For a given task, class $0$ samples are drawn from $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I})$ and class $1$ samples from $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I})$.\n\n### 2. Teacher Model Training\n\nA \"teacher\" model is first trained exclusively on the old task data. The training objective is to minimize the standard cross-entropy loss, $\\mathcal{L}_{\\mathrm{CE}}$, between the one-hot encoded true labels $\\mathbf{y}$ and the model's predicted probabilities $\\mathbf{p}$:\n$$\n\\mathcal{L}_{\\mathrm{CE}}(\\mathbf{y}, \\mathbf{p}) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^C y_{ik} \\log p_{ik}\n$$\nwhere $N$ is the number of samples. The parameters $\\theta_{\\text{old}} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2\\}$ are updated using gradient descent. The gradient of the loss with respect to the logits $\\mathbf{z}$ for a single sample is $\\mathbf{p} - \\mathbf{y}$. This gradient is then backpropagated through the network to compute gradients for all parameters.\n\n### 3. Student Model Training Strategies\n\nA \"student\" model, with the same architecture and a fresh set of initial random weights, is then trained on the new task. To prevent catastrophic forgetting of the old task, a distillation loss is added to the training objective. Two strategies are compared.\n\n#### 3.1. Output-Space Distillation\n\nIn this strategy, the student model is trained to match the softened outputs of the teacher model on the old task data. The total loss function is a weighted sum of the cross-entropy loss on the new task data and a distillation loss on the old task data:\n$$\n\\mathcal{L}_{\\text{out}} = \\mathcal{L}_{\\mathrm{CE}}(\\text{new data}) + \\lambda_{\\mathrm{out}} \\mathcal{L}_{\\mathrm{distill}}(\\text{old data})\n$$\nThe distillation loss $\\mathcal{L}_{\\mathrm{distill}}$ is the Kullback-Leibler divergence between the teacher's and student's softened probability distributions, scaled by the temperature factor $T^2$:\n$$\n\\mathcal{L}_{\\mathrm{distill}} = T^2 D_{\\mathrm{KL}}(\\mathbf{q}^{(T)} \\| \\mathbf{p}^{(T)})\n$$\nwhere $\\mathbf{p}^{(T)} = \\mathrm{softmax}(\\mathbf{z}_{\\text{student}}/T)$ and $\\mathbf{q}^{(T)} = \\mathrm{softmax}(\\mathbf{z}_{\\text{teacher}}/T)$ are the student and teacher probabilities computed with temperature $T$. Minimizing $D_{\\mathrm{KL}}$ is equivalent to minimizing the cross-entropy between $\\mathbf{q}^{(T)}$ and $\\mathbf{p}^{(T)}$, since the entropy of $\\mathbf{q}^{(T)}$ is constant with respect to the student's parameters.\nThe gradient of $\\mathcal{L}_{\\mathrm{distill}}$ with respect to the student's logits $\\mathbf{z}_{\\text{student}}$ is:\n$$\n\\frac{\\partial \\mathcal{L}_{\\mathrm{distill}}}{\\partial \\mathbf{z}_{\\text{student}}} = \\mathbf{p}^{(T)} - \\mathbf{q}^{(T)}\n$$\nThe total gradient for the student's parameters $\\theta_{\\text{student}}$ is the sum of gradients derived from $\\mathcal{L}_{\\mathrm{CE}}$ on new data and from $\\lambda_{\\mathrm{out}} \\mathcal{L}_{\\mathrm{distill}}$ on old data.\n\n#### 3.2. Feature-Space Distillation\n\nHere, the student is encouraged to learn hidden representations similar to those of the teacher. The loss function is:\n$$\n\\mathcal{L}_{\\text{feat}} = \\mathcal{L}_{\\mathrm{CE}}(\\text{new data}) + \\lambda_{\\mathrm{feat}} \\mathcal{L}_{\\mathrm{MSE}}(\\text{old data})\n$$\nThe feature-space distillation loss $\\mathcal{L}_{\\mathrm{MSE}}$ is the mean squared error between the teacher's and student's hidden-layer activations ($\\mathbf{h}_{\\text{teacher}}$ and $\\mathbf{h}_{\\text{student}}$) on the old task data:\n$$\n\\mathcal{L}_{\\mathrm{MSE}} = \\|\\mathbf{h}_{\\text{student}} - \\mathbf{h}_{\\text{teacher}}\\|_2^2 = \\sum_{j=1}^H (h_{\\text{student}, j} - h_{\\text{teacher}, j})^2\n$$\nThe gradient of this loss with respect to the student's hidden activation $\\mathbf{h}_{\\text{student}}$ is $2(\\mathbf{h}_{\\text{student}} - \\mathbf{h}_{\\text{teacher}})$. This gradient is backpropagated through the first layer's activation function and weights. The final layer's parameters $(\\mathbf{W}_2, \\mathbf{b}_2)$ are unaffected by this distillation loss and are updated only based on the new task's $\\mathcal{L}_{\\mathrm{CE}}$. The first layer's parameters $(\\mathbf{W}_1, \\mathbf{b}_1)$ are updated using a combined gradient from both loss components.\n\n### 4. Evaluation Methodology\n\nThe preservation of the old task's decision boundary is quantified by measuring the change in the model's signed margin at specific \"probe points\". The signed margin is defined as $m(\\mathbf{x}) = z_1(\\mathbf{x}) - z_0(\\mathbf{x})$, where the decision boundary lies at $m(\\mathbf{x}) = 0$.\n\nThe probe points are the $K$ samples from the old task training set for which the teacher model's absolute margin $|m_{\\text{old}}(\\mathbf{x})|$ is smallest. These points are, by definition, closest to the teacher's learned decision boundary.\n\nFor each student model (one for each strategy), the average absolute margin change $\\Delta$ is calculated over these $K$ probe points:\n$$\n\\Delta = \\frac{1}{K} \\sum_{i=1}^K |m_{\\text{student}}(\\mathbf{x}_i) - m_{\\text{old}}(\\mathbf{x}_i)|\n$$\nA smaller $\\Delta$ indicates better preservation of the fine-grained decision boundary. The final output for each test case is a boolean value indicating whether output-space distillation achieves a strictly smaller $\\Delta$ than feature-space distillation ($\\Delta_{\\text{out}} < \\Delta_{\\text{feat}}$).", "answer": "```python\nimport numpy as np\n\ndef softmax(z):\n    \"\"\"Computes softmax for a batch of logits.\"\"\"\n    # Subtract max for numerical stability\n    exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n\ndef tanh(a):\n    \"\"\"Hyperbolic tangent activation.\"\"\"\n    return np.tanh(a)\n\ndef one_hot(y, C):\n    \"\"\"Converts a vector of labels to one-hot encoding.\"\"\"\n    return np.eye(C)[y]\n\nclass NeuralNetwork:\n    \"\"\"A one-hidden-layer neural network.\"\"\"\n\n    def __init__(self, d, H, C, seed):\n        \"\"\"Initializes network parameters.\"\"\"\n        rng = np.random.default_rng(seed)\n        # Xavier/Glorot initialization for tanh activation\n        self.W1 = rng.normal(0, np.sqrt(1.0 / d), (d, H))\n        self.b1 = np.zeros((1, H))\n        self.W2 = rng.normal(0, np.sqrt(1.0 / H), (H, C))\n        self.b2 = np.zeros((1, C))\n\n    def get_params(self):\n        \"\"\"Returns a copy of the model parameters.\"\"\"\n        return self.W1.copy(), self.b1.copy(), self.W2.copy(), self.b2.copy()\n\n    def set_params(self, W1, b1, W2, b2):\n        \"\"\"Sets the model parameters.\"\"\"\n        self.W1, self.b1, self.W2, self.b2 = W1, b1, W2, b2\n\n    def forward(self, X):\n        \"\"\"Performs a forward pass.\"\"\"\n        a1 = X @ self.W1 + self.b1\n        h = tanh(a1)\n        z = h @ self.W2 + self.b2\n        p = softmax(z)\n        return z, p, h, a1\n\n    def get_margin(self, X):\n        \"\"\"Computes the signed margin for binary classification.\"\"\"\n        z, _, _, _ = self.forward(X)\n        return z[:, 1] - z[:, 0]\n\n    def train_teacher(self, X_old, y_old_one_hot, eta, iterations):\n        \"\"\"Trains the teacher model using standard gradient descent.\"\"\"\n        n_samples = X_old.shape[0]\n        for _ in range(iterations):\n            z, p, h, a1 = self.forward(X_old)\n            \n            # Gradient of CE loss w.r.t logits\n            dz = (p - y_old_one_hot) / n_samples\n            \n            # Backpropagate gradients\n            dW2 = h.T @ dz\n            db2 = np.sum(dz, axis=0, keepdims=True)\n            \n            dh = dz @ self.W2.T\n            da1 = dh * (1 - h**2)\n            \n            dW1 = X_old.T @ da1\n            db1 = np.sum(da1, axis=0, keepdims=True)\n            \n            # Update parameters\n            self.W1 -= eta * dW1\n            self.b1 -= eta * db1\n            self.W2 -= eta * dW2\n            self.b2 -= eta * db2\n\n    def train_student_output_distill(self, X_new, y_new_one_hot, X_old, teacher_model, eta, iterations, lambda_out, T):\n        \"\"\"Trains a student using output-space distillation.\"\"\"\n        n_new, n_old = X_new.shape[0], X_old.shape[0]\n        for _ in range(iterations):\n            # Gradients from new task (Cross-Entropy)\n            z_new, p_new, h_new, _ = self.forward(X_new)\n            dz_ce = (p_new - y_new_one_hot) / n_new\n            dW2_ce = h_new.T @ dz_ce\n            db2_ce = np.sum(dz_ce, axis=0, keepdims=True)\n            dh_ce = dz_ce @ self.W2.T\n            da1_ce = dh_ce * (1 - h_new**2)\n            dW1_ce = X_new.T @ da1_ce\n            db1_ce = np.sum(da1_ce, axis=0, keepdims=True)\n            \n            # Gradients from old task (Distillation)\n            if lambda_out > 0:\n                z_old_student, _, h_old_student, _ = self.forward(X_old)\n                z_old_teacher, _, _, _ = teacher_model.forward(X_old)\n                \n                p_student_soft = softmax(z_old_student / T)\n                p_teacher_soft = softmax(z_old_teacher / T)\n                \n                # Gradient of T^2 * D_KL w.r.t logits\n                dz_kl = (p_student_soft - p_teacher_soft) / n_old\n                \n                dW2_kl = h_old_student.T @ dz_kl\n                db2_kl = np.sum(dz_kl, axis=0, keepdims=True)\n                dh_kl = dz_kl @ self.W2.T\n                da1_kl = dh_kl * (1 - h_old_student**2)\n                dW1_kl = X_old.T @ da1_kl\n                db1_kl = np.sum(da1_kl, axis=0, keepdims=True)\n            else:\n                dW1_kl, db1_kl, dW2_kl, db2_kl = 0, 0, 0, 0\n                \n            # Combine gradients and update\n            self.W1 -= eta * (dW1_ce + lambda_out * dW1_kl)\n            self.b1 -= eta * (db1_ce + lambda_out * db1_kl)\n            self.W2 -= eta * (dW2_ce + lambda_out * dW2_kl)\n            self.b2 -= eta * (db2_ce + lambda_out * db2_kl)\n\n    def train_student_feature_distill(self, X_new, y_new_one_hot, X_old, teacher_model, eta, iterations, lambda_feat):\n        \"\"\"Trains a student using feature-space distillation.\"\"\"\n        n_new, n_old = X_new.shape[0], X_old.shape[0]\n        for _ in range(iterations):\n            # Gradients from new task (Cross-Entropy)\n            z_new, p_new, h_new, _ = self.forward(X_new)\n            dz_ce = (p_new - y_new_one_hot) / n_new\n            dW2_ce = h_new.T @ dz_ce\n            db2_ce = np.sum(dz_ce, axis=0, keepdims=True)\n            dh_ce = dz_ce @ self.W2.T\n            da1_ce = dh_ce * (1 - h_new**2)\n            dW1_ce = X_new.T @ da1_ce\n            db1_ce = np.sum(da1_ce, axis=0, keepdims=True)\n            \n            # Gradients from old task (Feature MSE)\n            if lambda_feat > 0:\n                _, _, h_old_student, _ = self.forward(X_old)\n                _, _, h_old_teacher, _ = teacher_model.forward(X_old)\n                \n                # Gradient of MSE loss w.r.t student's hidden activations\n                dh_mse = 2 * (h_old_student - h_old_teacher) / n_old\n                da1_mse = dh_mse * (1 - h_old_student**2)\n                dW1_mse = X_old.T @ da1_mse\n                db1_mse = np.sum(da1_mse, axis=0, keepdims=True)\n                dW2_mse, db2_mse = 0, 0\n            else:\n                dW1_mse, db1_mse, dW2_mse, db2_mse = 0, 0, 0, 0\n\n            # Combine gradients and update\n            self.W1 -= eta * (dW1_ce + lambda_feat * dW1_mse)\n            self.b1 -= eta * (db1_ce + lambda_feat * db1_mse)\n            self.W2 -= eta * (dW2_ce + lambda_feat * dW2_mse)\n            self.b2 -= eta * (db2_ce + lambda_feat * db2_mse)\n\ndef generate_data(n_samples, mu0, mu1, sigma, seed):\n    \"\"\"Generates 2D Gaussian data for binary classification.\"\"\"\n    rng = np.random.default_rng(seed)\n    n0 = n_samples // 2\n    n1 = n_samples - n0\n    X0 = rng.normal(loc=mu0, scale=sigma, size=(n0, 2))\n    X1 = rng.normal(loc=mu1, scale=sigma, size=(n1, 2))\n    X = np.vstack((X0, X1))\n    y = np.array([0] * n0 + [1] * n1)\n    return X, y\n\ndef solve():\n    \"\"\"Main function to run the experiment for all test cases.\"\"\"\n    test_cases = [\n        {'seed': 0, 'n_old': 200, 'n_new': 200, 'mu0_old': (-1.0, 0.0), 'mu1_old': (1.0, 0.0), 'sigma_old': 0.4, 'mu0_new': (0.0, -1.0), 'mu1_new': (0.0, 1.0), 'sigma_new': 0.4, 'H': 16, 'eta': 0.05, 'teacher_iterations': 300, 'student_iterations': 300, 'lambda_out': 0.5, 'lambda_feat': 0.5, 'T': 2.0, 'K': 40},\n        {'seed': 1, 'n_old': 240, 'n_new': 240, 'mu0_old': (-0.3, 0.0), 'mu1_old': (0.3, 0.0), 'sigma_old': 0.35, 'mu0_new': (0.0, -0.6), 'mu1_new': (0.0, 0.6), 'sigma_new': 0.35, 'H': 24, 'eta': 0.05, 'teacher_iterations': 400, 'student_iterations': 350, 'lambda_out': 0.4, 'lambda_feat': 0.8, 'T': 2.5, 'K': 60},\n        {'seed': 2, 'n_old': 220, 'n_new': 220, 'mu0_old': (-0.8, -0.2), 'mu1_old': (0.8, 0.2), 'sigma_old': 0.45, 'mu0_new': (0.2, -0.8), 'mu1_new': (-0.2, 0.8), 'sigma_new': 0.45, 'H': 20, 'eta': 0.05, 'teacher_iterations': 350, 'student_iterations': 320, 'lambda_out': 0.0, 'lambda_feat': 0.6, 'T': 2.0, 'K': 50}\n    ]\n\n    results = []\n    for case in test_cases:\n        p = case # Use shorter alias for params\n        \n        # 1. Generate seeds and data\n        rng = np.random.default_rng(p['seed'])\n        seed_data_old = rng.integers(1e9)\n        seed_data_new = rng.integers(1e9)\n        seed_model = rng.integers(1e9)\n\n        X_old, y_old = generate_data(p['n_old'], p['mu0_old'], p['mu1_old'], p['sigma_old'], seed_data_old)\n        X_new, y_new = generate_data(p['n_new'], p['mu0_new'], p['mu1_new'], p['sigma_new'], seed_data_new)\n        y_old_one_hot = one_hot(y_old, 2)\n        y_new_one_hot = one_hot(y_new, 2)\n\n        # 2. Train teacher model\n        teacher_model = NeuralNetwork(d=2, H=p['H'], C=2, seed=seed_model)\n        teacher_model.train_teacher(X_old, y_old_one_hot, p['eta'], p['teacher_iterations'])\n\n        # 3. Identify probe points and get teacher margins\n        teacher_margins = teacher_model.get_margin(X_old)\n        probe_indices = np.argsort(np.abs(teacher_margins))[:p['K']]\n        X_probe = X_old[probe_indices]\n        old_margins_at_probes = teacher_model.get_margin(X_probe)\n        \n        # 4. Train and evaluate student with output-space distillation\n        student_out = NeuralNetwork(d=2, H=p['H'], C=2, seed=seed_model) # Re-use seed for identical init\n        student_out.train_student_output_distill(X_new, y_new_one_hot, X_old, teacher_model, p['eta'], p['student_iterations'], p['lambda_out'], p['T'])\n        student_out_margins = student_out.get_margin(X_probe)\n        delta_out = np.mean(np.abs(student_out_margins - old_margins_at_probes))\n\n        # 5. Train and evaluate student with feature-space distillation\n        student_feat = NeuralNetwork(d=2, H=p['H'], C=2, seed=seed_model) # Re-use seed for identical init\n        student_feat.train_student_feature_distill(X_new, y_new_one_hot, X_old, teacher_model, p['eta'], p['student_iterations'], p['lambda_feat'])\n        student_feat_margins = student_feat.get_margin(X_probe)\n        delta_feat = np.mean(np.abs(student_feat_margins - old_margins_at_probes))\n\n        # 6. Compare and store result\n        results.append(delta_out < delta_feat)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3109241"}]}