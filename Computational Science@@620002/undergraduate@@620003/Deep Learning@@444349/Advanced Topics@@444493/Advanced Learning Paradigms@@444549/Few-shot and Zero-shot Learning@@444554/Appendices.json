{"hands_on_practices": [{"introduction": "Prototype-based classifiers are a cornerstone of few-shot learning, operating on the intuitive principle of comparing a new example to a representative \"prototype\" for each class. Often, a softmax function with a tunable \"temperature\" parameter, $\\tau$, is used to convert these comparisons into class probabilities. This practice [@problem_id:3125741] moves beyond heuristic tuning by revealing the deep connection between this model and fundamental principles of Bayesian inference. By working through the derivation, you will see how, under a Gaussian assumption for the data, the temperature parameter is not arbitrary but is directly determined by the data's variance, grounding this common technique in solid probabilistic theory.", "problem": "Consider a few-shot classification scenario in deep learning where each class is represented by a prototype in an embedding space. Let the embedding map be denoted by $\\phi(\\cdot)$, and assume there are $C$ classes with class prototypes $\\mu_{1}, \\mu_{2}, \\dots, \\mu_{C}$ in $\\mathbb{R}^{m}$. The classifier assigns a label to an input $x$ by computing a softmax over negative distances to the class prototypes with a temperature parameter $\\tau > 0$, specifically\n$$\np_{\\tau}(y=c \\mid x) \\;=\\; \\frac{\\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{k}\\big)\\big)},\n$$\nwhere $d\\!\\big(\\phi(x), \\mu_{c}\\big) = \\|\\phi(x) - \\mu_{c}\\|_{2}^{2}$ is the squared Euclidean distance. Suppose the class-conditional distribution of embeddings is Gaussian with shared isotropic covariance across classes, namely for each class $c$, the random vector $z = \\phi(x)$ given $y=c$ is distributed as a multivariate normal with mean $\\mu_{c}$ and covariance matrix $\\sigma^{2} I_{m}$, written as $z \\mid y=c \\sim \\mathcal{N}\\!\\big(\\mu_{c}, \\sigma^{2} I_{m}\\big)$, where $\\sigma^{2} > 0$ and $I_{m}$ is the $m \\times m$ identity matrix. Assume equal class priors $p(y=c) = \\frac{1}{C}$ for all $c \\in \\{1, 2, \\dots, C\\}$.\n\nStarting from the definition of the Gaussian probability density function and Bayes' rule, derive the value of the temperature parameter $\\tau$ for the above softmax classifier that makes $p_{\\tau}(y=c \\mid x)$ match the Bayes-optimal posterior $p(y=c \\mid x)$ under the stated assumptions. Express your final answer as a single closed-form symbolic expression in terms of $\\sigma^{2}$ only. No numerical approximation or rounding is required.", "solution": "The objective is to determine the value of the temperature parameter $\\tau$ that makes the given softmax classifier's posterior probability distribution $p_{\\tau}(y=c \\mid x)$ identical to the Bayes-optimal posterior probability $p(y=c \\mid x)$ under the specified modeling assumptions.\n\nFirst, let's derive the Bayes-optimal posterior probability $p(y=c \\mid x)$. Let $z = \\phi(x)$ be the embedding of the input $x$ in $\\mathbb{R}^{m}$. According to Bayes' rule, the posterior probability of class $c$ given the embedding $z$ is:\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{p(z)}$$\nThe denominator $p(z)$ is the marginal probability density of $z$, which can be expressed using the law of total probability by summing over all possible classes:\n$$p(z) = \\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)$$\nSubstituting this into the Bayes' rule expression gives:\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{\\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)}$$\n\nThe problem provides the following information:\n1.  The class-conditional distribution of the embeddings $z = \\phi(x)$ for any class $c$ is a multivariate normal distribution with mean $\\mu_{c}$ and covariance matrix $\\sigma^{2} I_{m}$. This is denoted as $z \\mid y=c \\sim \\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$.\n2.  The class priors are uniform, i.e., $p(y=c) = \\frac{1}{C}$ for all $c \\in \\{1, 2, \\dots, C\\}$.\n\nThe probability density function (PDF) of the multivariate normal distribution $\\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$ for a vector $z \\in \\mathbb{R}^{m}$ is:\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi)^{m/2} \\det(\\sigma^{2} I_{m})^{1/2}} \\exp\\left(-\\frac{1}{2} (z - \\mu_{c})^{\\top} (\\sigma^{2} I_{m})^{-1} (z - \\mu_{c})\\right)$$\nLet's simplify the terms in this PDF. The determinant of the covariance matrix is $\\det(\\sigma^{2} I_{m}) = (\\sigma^{2})^{m} \\det(I_{m}) = (\\sigma^{2})^{m}$. The inverse of the covariance matrix is $(\\sigma^{2} I_{m})^{-1} = \\frac{1}{\\sigma^{2}} I_{m}^{-1} = \\frac{1}{\\sigma^{2}} I_{m}$.\nThe quadratic form in the exponent becomes:\n$$(z - \\mu_{c})^{\\top} \\left(\\frac{1}{\\sigma^{2}} I_{m}\\right) (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} (z - \\mu_{c})^{\\top} (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\nSubstituting these back into the PDF expression, we get:\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)$$\n\nNow, we substitute both the PDF and the uniform prior $p(y=c) = \\frac{1}{C}$ into the expression for the Bayes-optimal posterior:\n$$p(y=c \\mid z) = \\frac{\\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}{\\sum_{k=1}^{C} \\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}$$\nThe constant terms $\\frac{1}{(2\\pi\\sigma^{2})^{m/2}}$ and $\\frac{1}{C}$ are common to all terms in the numerator and the denominator's sum, so they cancel out. The simplified expression for the Bayes-optimal posterior is:\n$$p(y=c \\mid z) = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\nNext, we compare this derived posterior with the classifier model given in the problem statement. The model's probability distribution is:\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{k}\\big)\\big)}$$\nUsing $z = \\phi(x)$ and the definition of the distance metric $d(z, \\mu_c) = \\|z - \\mu_{c}\\|_{2}^{2}$, we can rewrite the classifier's probability as:\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\nFor the classifier to be Bayes-optimal, we must have $p_{\\tau}(y=c \\mid x) = p(y=c \\mid z)$ for all $c$ and $z$.\n$$\\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)} = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\nBoth expressions are in the form of a softmax function applied to a set of scores. For the outputs to be identical for any set of prototypes $\\{\\mu_{k}\\}$ and any embedding $z$, the arguments of the corresponding exponential functions in the numerator and denominator must be equal (up to an additive constant that is independent of the class index $c$, which is zero in this case).\nBy comparing the exponents for class $c$, we obtain the equality:\n$$-\\tau \\|z - \\mu_{c}\\|_{2}^{2} = -\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\nSince this must hold for any $z$, and we can choose $z$ such that $\\|z - \\mu_{c}\\|_{2}^{2} \\neq 0$, we can divide both sides by $-\\|z - \\mu_{c}\\|_{2}^{2}$:\n$$\\tau = \\frac{1}{2\\sigma^{2}}$$\nThis provides the value of the temperature parameter $\\tau$ in terms of the variance $\\sigma^{2}$ of the class-conditional embedding distributions. The condition $\\sigma^{2} > 0$ ensures that $\\tau$ is well-defined and positive, as required.", "answer": "$$\\boxed{\\frac{1}{2\\sigma^{2}}}$$", "id": "3125741"}, {"introduction": "While conceptually simple, the performance of a prototype-based classifier hinges on the quality of its prototypes. In few-shot learning, where prototypes are computed from a small number of examples, a single outlier or mislabeled sample can severely corrupt the standard sample mean, leading to poor generalization. This exercise [@problem_id:3125745] introduces a formal way to quantify an estimator's resilience to such contamination using the \"breakdown point,\" a core concept from robust statistics. By theoretically analyzing and comparing the breakdown points of the sample mean and the more robust median-of-means estimator, you will develop a critical understanding of the vulnerabilities inherent in simple estimators and the need for more robust alternatives.", "problem": "Consider a two-class few-shot classification scenario in deep learning with a fixed embedding function that maps each input to a one-dimensional feature in $\\mathbb{R}$. Class $\\mathcal{C}_{+}$ has true embedding value $+\\mu$ and class $\\mathcal{C}_{-}$ has true embedding value $-\\mu$, where $\\mu>0$. Each class provides a support set of $m$ labeled examples, and a query is classified by the nearest-prototype rule in the Euclidean metric. Assume the following simplified but scientifically consistent setting to isolate the effect of label noise:\n- Every support feature from $\\mathcal{C}_{+}$ is exactly $+\\mu$, and every support feature from $\\mathcal{C}_{-}$ is exactly $-\\mu$.\n- Each support label is independently flipped with probability $\\eta \\in (0,1/2)$, but for robustness analysis you will consider the worst-case allocation of flips rather than their stochastic nature.\n- The prototype of class $\\mathcal{C}_{+}$ is computed only from those support samples assigned to $\\mathcal{C}_{+}$ by their (possibly noisy) labels; similarly for class $\\mathcal{C}_{-}$.\n\nYou will compare two prototype estimators for class $\\mathcal{C}_{+}$:\n1. The sample mean.\n2. The median-of-means (MoM), defined as follows: partition the $m$ samples assigned to $\\mathcal{C}_{+}$ into $k$ disjoint blocks of equal size and take the median of the $k$ block means. Assume $k$ is an odd integer and $m$ is divisible by $2k$, specifically $m=2bk$ with integer $b\\geq 1$, so each block has size $2b$.\n\nDefine the breakdown point for the class $\\mathcal{C}_{+}$ prototype estimator as the smallest fraction of adversarially flipped labels within the $m$ samples assigned to $\\mathcal{C}_{+}$ that forces the estimator to become non-positive, i.e., to output a prototype $\\leq 0$. This definition captures the minimal contamination level that flips the sign of the $\\mathcal{C}_{+}$ prototype in this setting, which in turn shifts the decision boundary to misrepresent the true class geometry.\n\nStarting from core definitions of the sample mean and the median, and the above construction of the median-of-means estimator, derive the breakdown point for:\n- The sample mean prototype.\n- The median-of-means prototype, as an explicit function of $k$ under the constraint $m=2bk$ and odd $k$.\n\nExpress your final answers as exact fractions in simplest form. No rounding is required. Provide both breakdown points as a single row matrix in the order: sample mean first, median-of-means second.", "solution": "We begin from the fundamental definitions of the sample mean and the median, applied to a simplified few-shot prototype estimation problem. The support features are fixed at $+\\mu$ for class $\\mathcal{C}_{+}$ and $-\\mu$ for class $\\mathcal{C}_{-}$ with $\\mu>0$. Label flips move a $-\\mu$ point into the set assigned to $\\mathcal{C}_{+}$ (and similarly for the other class), producing contamination in the prototype estimation for $\\mathcal{C}_{+}$. Let $r$ denote the number of flipped labels among the $m$ points assigned to $\\mathcal{C}_{+}$.\n\nBy construction, the set used to estimate the $\\mathcal{C}_{+}$ prototype contains $(m-r)$ copies of $+\\mu$ and $r$ copies of $-\\mu$.\n\nSample mean breakdown point derivation:\n- The sample mean estimator for the $\\mathcal{C}_{+}$ prototype is\n$$\n\\hat{\\mu}_{\\text{mean}} = \\frac{(m-r)(+\\mu) + r(-\\mu)}{m} = \\mu \\cdot \\frac{m-2r}{m} = \\mu \\left(1 - \\frac{2r}{m}\\right).\n$$\n- The breakdown point is defined as the smallest fraction $r/m$ such that the estimator becomes non-positive:\n$$\n\\hat{\\mu}_{\\text{mean}} \\leq 0 \\quad \\Longleftrightarrow \\quad \\mu \\left(1 - \\frac{2r}{m}\\right) \\leq 0 \\quad \\Longleftrightarrow \\quad 1 - \\frac{2r}{m} \\leq 0 \\quad \\Longleftrightarrow \\quad \\frac{r}{m} \\geq \\frac{1}{2}.\n$$\n- Therefore, the breakdown point for the sample mean is\n$$\n\\epsilon_{\\text{mean}}^{\\star} = \\frac{1}{2}.\n$$\n\nMedian-of-means breakdown point derivation:\n- The median-of-means (MoM) prototype partitions the $m$ assigned samples into $k$ disjoint blocks of equal size and then takes the median of the $k$ block means. Under the constraint $m=2bk$ with integer $b\\geq 1$ and odd $k$, each block has $2b$ points. Let $r_{i}$ denote the number of flipped labels (i.e., $-\\mu$ points) in block $i$, with $i \\in \\{1,2,\\dots,k\\}$. The block mean for block $i$ is\n$$\n\\hat{\\mu}_{i} = \\frac{(2b-r_{i})(+\\mu) + r_{i}(-\\mu)}{2b} = \\mu \\cdot \\frac{2b - 2r_{i}}{2b} = \\mu \\left(1 - \\frac{r_{i}}{b}\\right).\n$$\n- A block mean is non-positive if and only if\n$$\n\\hat{\\mu}_{i} \\leq 0 \\quad \\Longleftrightarrow \\quad \\mu \\left(1 - \\frac{r_{i}}{b}\\right) \\leq 0 \\quad \\Longleftrightarrow \\quad \\frac{r_{i}}{b} \\geq 1 \\quad \\Longleftrightarrow \\quad r_{i} \\geq b.\n$$\n- Because $k$ is odd, the median of the $k$ block means is the $(k+1)/2$-th order statistic. To force the median to be non-positive, at least $\\frac{k+1}{2}$ block means must be non-positive. Under adversarial allocation of flips, the minimal total number of flips $r$ needed is obtained by setting $r_{i}=b$ in exactly $\\frac{k+1}{2}$ blocks and $r_{j}=0$ in the others. This yields\n$$\nr_{\\text{min}} = b \\cdot \\frac{k+1}{2}.\n$$\n- The breakdown point fraction is then\n$$\n\\epsilon_{\\text{MoM}}^{\\star}(k) = \\frac{r_{\\text{min}}}{m} = \\frac{b \\cdot \\frac{k+1}{2}}{2bk} = \\frac{k+1}{4k}.\n$$\n\nSummary:\n- Sample mean breakdown point: $\\epsilon_{\\text{mean}}^{\\star}=\\frac{1}{2}$.\n- Median-of-means breakdown point as a function of $k$ (odd) under $m=2bk$: $\\epsilon_{\\text{MoM}}^{\\star}(k)=\\frac{k+1}{4k}$.\n\nThese results follow directly from the core definitions of the sample mean and the median, together with the structure of the median-of-means estimator and the worst-case adversarial arrangement of flipped labels across blocks. Note that while label flips occur independently with probability $\\eta$, the breakdown point is a worst-case robustness characteristic that depends only on the minimal fraction of contamination required to flip the estimator’s sign, not on the stochastic mechanism generating the flips.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{k+1}{4k}\\end{pmatrix}}$$", "id": "3125745"}, {"introduction": "Having theoretically explored the fragility of simple prototypes, this practice [@problem_id:3125726] transitions from analysis to application by tasking you with building a more resilient classifier. You will derive and implement a robust prototype estimator using a weighted mean, where the influence of each support example is inversely proportional to its distance from the class's initial center. This principled approach automatically down-weights outliers, providing a practical solution to the problem of data contamination highlighted in the previous exercise. By coding this solution and evaluating it on challenging test cases with outliers and class imbalance, you will gain hands-on experience in translating robust estimation theory into an effective and practical machine learning model.", "problem": "Consider a two-class few-shot classification task in which a metric-based classifier predicts the class of a query by comparing its embedding to class prototypes. Let the embedding function be denoted by $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$. For a class $c\\in\\{0,1\\}$ with support embeddings $\\{y_{c,i}\\}_{i=1}^{k_c}$ where $y_{c,i}=\\phi(x_{c,i})$, a standard prototype uses the unweighted mean $\\mu_c=\\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$. In class-imbalanced few-shot settings where $k_1\\ll k_2$, the unweighted mean can be biased by dispersed points in the majority class. To mitigate this, consider a weighted prototype of the form $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$, where $\\alpha_{c,i}\\ge 0$ and $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$.\n\nStarting from foundational probabilistic modeling and the principle of empirical risk minimization, derive a principled rule for choosing the weights $\\alpha_{c,i}$ that reduces the influence of outliers while respecting the constraints $\\alpha_{c,i}\\ge 0$ and $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$. Your derivation must begin with a well-posed assumption about the data-generating process in the embedding space and proceed step by step to a concrete weighting mechanism without invoking ad hoc heuristics. Then implement the resulting classifier and evaluate its performance under the following conditions.\n\nUse a two-dimensional embedding with a fixed linear embedding function $\\phi(x)=W x$, where\n$$\nW=\\begin{pmatrix}\n1.2 & 0.3\\\\\n-0.4 & 0.8\n\\end{pmatrix}.\n$$\nClassification must be performed by nearest prototype under squared Euclidean distance in the embedding space, with ties broken in favor of the numerically smaller class index. For each test case below, compute two prototypes per class: the unweighted mean and the derived weighted prototype. For each query, predict its class using each prototype set and compute accuracy as the fraction of correctly classified queries expressed as a decimal. For numerical stability in any variance or scale estimate required by your weighting rule, add a small constant $\\varepsilon=10^{-6}$ wherever division by zero might otherwise occur.\n\nTest Suite. For each test case, the support sets and queries are specified in $\\mathbb{R}^2$. All coordinates are in unitless numerical values. The task is to to output, for each test case, a single float equal to the accuracy of the weighted-prototype classifier minus the accuracy of the unweighted-mean classifier, rounded to three decimal places.\n\n- Test Case A (moderate imbalance with a dispersed majority cluster):\n    - Class $0$ support ($k_0=3$): $(-0.1,\\,0.2)$, $(0.05,\\,-0.05)$, $(0.1,\\,0.0)$.\n    - Class $1$ support ($k_1=7$): $(3.8,\\,0.1)$, $(4.1,\\,-0.2)$, $(4.0,\\,0.0)$, $(5.5,\\,2.0)$, $(3.9,\\,0.2)$, $(4.2,\\,0.1)$, $(4.1,\\,0.0)$.\n    - Queries and ground-truth labels:\n        - $(-0.05,\\,0.0)\\rightarrow 0$, $(0.2,\\,-0.1)\\rightarrow 0$, $(4.05,\\,0.0)\\rightarrow 1$, $(3.7,\\,0.1)\\rightarrow 1$, $(2.1,\\,0.4)\\rightarrow 1$, $(2.0,\\,0.0)\\rightarrow 0$.\n\n- Test Case B (extreme imbalance $k_0=1\\ll k_1=10$):\n    - Class $0$ support ($k_0=1$): $(0.0,\\,0.0)$.\n    - Class $1$ support ($k_1=10$): $(4.0,\\,0.0)$, $(4.1,\\,0.1)$, $(3.9,\\,-0.1)$, $(4.2,\\,0.0)$, $(3.8,\\,0.2)$, $(4.3,\\,-0.2)$, $(4.05,\\,0.05)$, $(4.2,\\,0.2)$, $(5.0,\\,1.5)$, $(3.7,\\,-0.3)$.\n    - Queries and ground-truth labels:\n        - $(0.1,\\,0.0)\\rightarrow 0$, $(-0.2,\\,0.05)\\rightarrow 0$, $(4.05,\\,0.1)\\rightarrow 1$, $(3.9,\\,-0.05)\\rightarrow 1$, $(2.2,\\,0.2)\\rightarrow 1$, $(-1.0,\\,0.0)\\rightarrow 0$.\n\n- Test Case C (balanced but both classes have a strong outlier):\n    - Class $0$ support ($k_0=5$): $(0.0,\\,0.0)$, $(0.1,\\,-0.1)$, $(-0.05,\\,0.1)$, $(0.2,\\,0.05)$, $(-1.8,\\,-1.2)$.\n    - Class $1$ support ($k_1=5$): $(3.9,\\,0.0)$, $(4.1,\\,0.1)$, $(4.0,\\,-0.1)$, $(4.2,\\,0.0)$, $(5.2,\\,2.5)$.\n    - Queries and ground-truth labels:\n        - $(0.1,\\,0.0)\\rightarrow 0$, $(-0.2,\\,0.15)\\rightarrow 0$, $(4.1,\\,-0.05)\\rightarrow 1$, $(3.95,\\,0.2)\\rightarrow 1$, $(2.3,\\,0.5)\\rightarrow 1$, $(1.0,\\,-0.7)\\rightarrow 0$.\n\nYour program must:\n- Implement the derived weighting rule to compute $\\alpha_{c,i}$ and construct weighted prototypes $\\mu_c$.\n- Compute unweighted prototypes by simple means for comparison.\n- Classify each query by nearest prototype in the embedding space under squared Euclidean distance.\n- Compute the accuracy for the weighted and unweighted cases per test case.\n- Output a single line containing a comma-separated list with brackets of the per-test-case accuracy differences (weighted minus unweighted), each rounded to three decimal places, for Test Cases A, B, and C in that order. For example, the final output format must be exactly of the form $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C\\right]$.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Task**: Two-class few-shot classification.\n- **Classifier**: Metric-based, comparing query embedding to class prototypes.\n- **Classes**: $c \\in \\{0, 1\\}$.\n- **Embedding Function**: $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$.\n- **Support Set**: For class $c$, $\\{x_{c,i}\\}_{i=1}^{k_c}$ are the input points.\n- **Support Embeddings**: $y_{c,i} = \\phi(x_{c,i})$.\n- **Unweighted Prototype**: $\\mu_c = \\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$.\n- **Weighted Prototype**: $\\mu_c = \\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$, with constraints $\\alpha_{c,i} \\ge 0$ and $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$.\n- **Derivation Goal**: Derive a principled rule for weights $\\alpha_{c,i}$ starting from probabilistic modeling and empirical risk minimization.\n- **Embedding Matrix**: For the implementation, the embedding is linear, $\\phi(x) = Wx$, with $d=2$ and\n$$\nW=\\begin{pmatrix}\n1.2 & 0.3\\\\\n-0.4 & 0.8\n\\end{pmatrix}.\n$$\n- **Classification Metric**: Nearest prototype under squared Euclidean distance.\n- **Tie-Breaking Rule**: Ties are broken in favor of the numerically smaller class index (i.e., class $0$).\n- **Numerical Stability**: Add $\\varepsilon=10^{-6}$ to denominators to prevent division by zero.\n- **Test Cases**: Three test cases (A, B, C) are provided, each with specific support sets for class $0$ and class $1$, and a set of queries with ground-truth labels.\n- **Output**: For each test case, compute the accuracy of the weighted-prototype classifier minus the accuracy of the unweighted-mean classifier, rounded to three decimal places. The final output must be a single line: `[result_A,result_B,result_C]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is firmly rooted in the established principles of machine learning and statistics. It asks for a derivation based on probabilistic modeling and empirical risk minimization, which are standard, rigorous frameworks for developing algorithms. The use of a weighted mean to create robust estimators is a well-studied concept in robust statistics. The problem is scientifically sound.\n-   **Well-Posed**: The problem is clearly defined and self-contained. It provides all necessary information: the mathematical formulation, the specific embedding function, the classification rule, the tie-breaking rule, a stability constant, and a complete set of data for testing. The objective is unambiguous.\n-   **Objective**: The problem is stated in precise, formal language, free of any subjective or opinion-based claims. All data and conditions are numerical and objective.\n\nThe problem does not exhibit any of the invalidity flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, trivial, or outside the realm of scientific verifiability.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n## Derivation of the Weighting Rule\n\nThe objective is to derive a principled rule for the weights $\\alpha_{c,i}$ in the prototype definition $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$, starting from a probabilistic model and the principle of empirical risk minimization. The rule should mitigate the influence of outliers.\n\n1.  **Probabilistic Model**: We begin by modeling the support embeddings $\\{y_{c,i}\\}_{i=1}^{k_c}$ of a class $c$ as noisy observations of an unknown true class prototype $\\theta_c$. To account for outliers, we assume that each observation $y_{c,i}$ is generated from a Gaussian distribution centered at $\\theta_c$ but with its own individual variance $\\sigma_{c,i}^2 I$, where $I$ is the identity matrix. A large variance $\\sigma_{c,i}^2$ for a point $y_{c,i}$ signifies low confidence that this point is a representative sample of the true center $\\theta_c$, which provides a natural mechanism for handling outliers.\n\n2.  **Empirical Risk Minimization via Maximum Likelihood**: Under this model, the principle of empirical risk minimization corresponds to finding the prototype $\\theta_c$ that maximizes the log-likelihood of observing the support set. Assuming the observations are independent, the log-likelihood is:\n    $$ \\mathcal{L}(\\theta_c) = \\ln \\prod_{i=1}^{k_c} p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) = \\sum_{i=1}^{k_c} \\ln p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) $$\n    For our Gaussian model in a $d$-dimensional space, this becomes:\n    $$ \\mathcal{L}(\\theta_c) = \\sum_{i=1}^{k_c} \\left( -\\frac{\\|y_{c,i} - \\theta_c\\|^2}{2\\sigma_{c,i}^2} - \\frac{d}{2}\\ln(2\\pi\\sigma_{c,i}^2) \\right) $$\n    Maximizing $\\mathcal{L}(\\theta_c)$ with respect to $\\theta_c$ is equivalent to minimizing the negative log-likelihood. Dropping terms that do not depend on $\\theta_c$, we are left with minimizing the following risk function $R(\\theta_c)$:\n    $$ R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\|y_{c,i} - \\theta_c\\|^2 $$\n    This is a weighted least-squares problem where each point's contribution to the loss is inversely proportional to its variance.\n\n3.  **Solving for the Prototype**: To find the optimal prototype (our estimate of $\\theta_c$), we set the gradient of $R(\\theta_c)$ with respect to $\\theta_c$ to zero:\n    $$ \\nabla_{\\theta_c} R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\cdot 2(\\theta_c - y_{c,i}) = 0 $$\n    Solving for $\\theta_c$ yields:\n    $$ \\left( \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\right) \\theta_c = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} y_{c,i} $$\n    The resulting estimate for the prototype, which we denote $\\mu_c$, is a weighted average of the support embeddings:\n    $$ \\mu_c = \\frac{\\sum_{i=1}^{k_c} (1/\\sigma_{c,i}^2) y_{c,i}}{\\sum_{j=1}^{k_c} (1/\\sigma_{c,j}^2)} $$\n    This corresponds to the desired form $\\mu_c = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$, where the weights are $\\alpha_{c,i} = \\frac{w_{c,i}}{\\sum_{j=1}^{k_c} w_{c,j}}$ with $w_{c,i} = 1/\\sigma_{c,i}^2$. These weights automatically satisfy the constraints $\\alpha_{c,i} \\ge 0$ and $\\sum_i \\alpha_{c,i}=1$.\n\n4.  **Deriving the Weighting Rule**: The final step is to define the individual variances $\\sigma_{c,i}^2$, which are unknown. A principled, non-iterative approach is to use a data-driven proxy for the reliability of each point. A point's deviation from a preliminary estimate of the class center is a good measure of its potential \"outlierness\". The standard unweighted mean, $\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_j y_{c,j}$, is the simplest and most natural choice for this preliminary estimate (it is the maximum likelihood estimate if all variances are assumed to be equal).\n\n    We therefore model each point's variance $\\sigma_{c,i}^2$ as being proportional to its squared Euclidean distance to this unweighted mean:\n    $$ \\sigma_{c,i}^2 \\propto \\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 $$\n    This provides a concrete rule: points far from the initial \"center of mass\" are deemed less reliable (higher variance) and are consequently assigned smaller weights in the calculation of the final prototype. The unnormalized weights are inversely proportional to this variance:\n    $$ w_{c,i} \\propto \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2} $$\n    To handle the case where a point may land exactly on the mean and to ensure numerical stability, we add the small constant $\\varepsilon=10^{-6}$ as specified. The final, concrete form for the unnormalized weights is:\n    $$ w_{c,i} = \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon} $$\n    These weights are then normalized to obtain $\\alpha_{c,i}$, which are used to compute the final weighted prototype. For the special case $k_c=1$, the distance is $0$, the unnormalized weight is $1/\\varepsilon$, and the normalized weight is $1$, correctly yielding the point itself as the prototype. This completes the derivation.\n\n## Algorithm and Implementation\nThe derived method is implemented as follows:\n1.  For each class $c \\in \\{0, 1\\}$, take the support points $\\{x_{c,i}\\}$ and apply the embedding function $\\phi(x) = Wx$ to get the embeddings $\\{y_{c,i}\\}$.\n2.  **Unweighted Prototypes**: For each class, compute the standard mean of the embeddings: $\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_{i=1}^{k_c} y_{c,i}$.\n3.  **Weighted Prototypes**: For each class:\n    a. If $k_c = 1$, the weighted prototype is simply the single support embedding.\n    b. If $k_c > 1$, first compute the unweighted prototype $\\mu_{c, \\text{unw}}$.\n    c. For each support embedding $y_{c,i}$, compute an unnormalized weight $w_{c,i} = 1 / (\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon)$.\n    d. Normalize the weights: $\\alpha_{c,i} = w_{c,i} / \\sum_{j=1}^{k_c} w_{c,j}$.\n    e. Compute the weighted prototype: $\\mu_{c, \\text{w}} = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$.\n4.  **Classification and Evaluation**: For each test case:\n    a. Embed the query points $\\{x_q\\}$.\n    b. For each query embedding $y_q$, calculate its squared Euclidean distance to the prototypes $\\{\\mu_{0, \\text{unw}}, \\mu_{1, \\text{unw}}\\}$ and $\\{\\mu_{0, \\text{w}}, \\mu_{1, \\text{w}}\\}$.\n    c. Assign the class label of the nearest prototype for each method, breaking ties in favor of class $0$.\n    d. Calculate the accuracy for the unweighted and weighted methods by comparing predictions to the ground-truth labels.\n    e. Compute the difference: (weighted accuracy) - (unweighted accuracy).\n5.  Collect the accuracy differences for all test cases and format the output as requested.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the few-shot classification problem by deriving and implementing a\n    principled weighting scheme for robust prototypes.\n    \"\"\"\n\n    # ---- Global parameters from the problem statement ----\n    W = np.array([[1.2, 0.3], [-0.4, 0.8]])\n    epsilon = 1e-6\n\n    # ---- Test cases ----\n    # Each case is a tuple: (support_set_dict, query_points_array, query_labels_array)\n    test_cases = [\n        (\n            {  # Support set for Test Case A\n                0: np.array([[-0.1, 0.2], [0.05, -0.05], [0.1, 0.0]]),\n                1: np.array([(3.8, 0.1), (4.1, -0.2), (4.0, 0.0), (5.5, 2.0), (3.9, 0.2), (4.2, 0.1), (4.1, 0.0)])\n            },\n            np.array([(-0.05, 0.0), (0.2, -0.1), (4.05, 0.0), (3.7, 0.1), (2.1, 0.4), (2.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case B\n                0: np.array([[0.0, 0.0]]),\n                1: np.array([(4.0, 0.0), (4.1, 0.1), (3.9, -0.1), (4.2, 0.0), (3.8, 0.2), (4.3, -0.2), (4.05, 0.05), (4.2, 0.2), (5.0, 1.5), (3.7, -0.3)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.05), (4.05, 0.1), (3.9, -0.05), (2.2, 0.2), (-1.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case C\n                0: np.array([(0.0, 0.0), (0.1, -0.1), (-0.05, 0.1), (0.2, 0.05), (-1.8, -1.2)]),\n                1: np.array([(3.9, 0.0), (4.1, 0.1), (4.0, -0.1), (4.2, 0.0), (5.2, 2.5)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.15), (4.1, -0.05), (3.95, 0.2), (2.3, 0.5), (1.0, -0.7)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        )\n    ]\n\n    results = []\n\n    # ---- Helper functions ----\n    def embed_points(points, W_matrix):\n        if points.ndim == 1:\n            points = points.reshape(1, -1)\n        return (W_matrix @ points.T).T\n\n    def get_prototypes(support_embeddings, eps):\n        prototypes_unweighted = {}\n        prototypes_weighted = {}\n        \n        for c in sorted(support_embeddings.keys()):\n            points = support_embeddings[c]\n            k_c = len(points)\n            \n            # Unweighted prototype: simple mean\n            mu_unweighted = np.mean(points, axis=0)\n            prototypes_unweighted[c] = mu_unweighted\n            \n            # Weighted prototype: derived from the robust estimation principle\n            if k_c == 1:\n                mu_weighted = points[0]\n            else:\n                dists_sq = np.sum((points - mu_unweighted)**2, axis=1)\n                raw_weights = 1.0 / (dists_sq + eps)\n                normalized_weights = raw_weights / np.sum(raw_weights)\n                mu_weighted = np.sum(points * normalized_weights[:, np.newaxis], axis=0)\n            \n            prototypes_weighted[c] = mu_weighted\n            \n        return prototypes_unweighted, prototypes_weighted\n\n    def classify_queries(prototypes_dict, query_embeddings):\n        # Prototypes are already created for sorted class indices\n        prototypes = [prototypes_dict[i] for i in sorted(prototypes_dict.keys())]\n        predictions = []\n        for q in query_embeddings:\n            dists_sq = [np.sum((q - p)**2) for p in prototypes]\n            # np.argmin breaks ties by choosing the first occurrence, which\n            # corresponds to the smaller class index as required.\n            predictions.append(np.argmin(dists_sq))\n        return np.array(predictions)\n\n    # ---- Main loop for test case evaluation ----\n    for case in test_cases:\n        support_x, query_x, labels = case\n        \n        # 1. Embed all points using the given matrix W\n        support_y = {c: embed_points(pts, W) for c, pts in support_x.items()}\n        query_y = embed_points(query_x, W)\n        \n        # 2. Get both unweighted and weighted prototypes for each class\n        protos_unw, protos_w = get_prototypes(support_y, epsilon)\n        \n        # 3. Classify queries and compute accuracy for the unweighted method\n        preds_unw = classify_queries(protos_unw, query_y)\n        acc_unw = np.mean(preds_unw == labels)\n\n        # 4. Classify queries and compute accuracy for the weighted method\n        preds_w = classify_queries(protos_w, query_y)\n        acc_w = np.mean(preds_w == labels)\n        \n        # 5. Calculate and store the accuracy difference, rounded to 3 decimal places\n        accuracy_diff = acc_w - acc_unw\n        results.append(round(accuracy_diff, 3))\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125726"}]}