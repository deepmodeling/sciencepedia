{"hands_on_practices": [{"introduction": "A primary challenge in multi-task learning arises when tasks have conflicting objectives, causing their gradients to point in opposing directions. Simply summing these gradients can lead to inefficient updates that slow down or stall training. This exercise lets you implement \"gradient surgery,\" a technique that mitigates this conflict by projecting one task's gradient to be orthogonal to another's, ensuring that updates for one task do not directly oppose the other. Through this practice, you will directly observe how managing gradient conflict can significantly accelerate convergence [@problem_id:3155105].", "problem": "Consider a two-task multi-task learning problem in deep learning where both tasks share a parameter vector $w \\in \\mathbb{R}^d$. Each task $i \\in \\{1,2\\}$ has a Mean Squared Error (MSE) loss $L_i(w)$ defined on a dataset $(X_i, y_i)$ with $n_i$ samples, where $X_i \\in \\mathbb{R}^{n_i \\times d}$ and $y_i \\in \\mathbb{R}^{n_i}$. The MSE loss is given by\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2.\n$$\nGradient descent updates the parameter vector using an aggregated gradient and a fixed learning rate $\\eta > 0$. In the vanilla aggregated regime, the algorithm uses the sum of per-task gradients, while in the orthogonalized regime, the algorithm replaces one task's gradient by its component orthogonal to the other task's gradient, and then aggregates.\n\nYour task is to implement the following two training regimens and compare their convergence speeds:\n\n- Vanilla aggregated gradient descent: at each iteration $t$, compute the per-task gradients $\\nabla L_1(w_t)$ and $\\nabla L_2(w_t)$ and update\n$$\nw_{t+1} = w_t - \\eta \\left( \\nabla L_1(w_t) + \\nabla L_2(w_t) \\right).\n$$\n\n- Orthogonalization-based gradient surgery: at each iteration $t$, compute $\\nabla L_1(w_t)$ and $\\nabla L_2(w_t)$, replace the gradient of task $1$ by its component orthogonal to the gradient of task $2$, then update using the sum of the modified gradient for task $1$ and the unmodified gradient for task $2$. If $\\left\\|\\nabla L_2(w_t)\\right\\|_2 = 0$, leave $\\nabla L_1(w_t)$ unchanged before aggregation.\n\nFor both regimens, start from the same initial parameter vector $w_0$ and run iterations until the stopping criterion\n$$\nL_1(w_t) + L_2(w_t) \\le \\varepsilon\n$$\nis first satisfied, or a maximum number of iterations $T_{\\max}$ is reached. Record the number of iterations taken to satisfy the stopping criterion (or record $T_{\\max}$ if it is not satisfied within that limit).\n\nImplement the program to evaluate the following five test cases, each specified by $(X_1, y_1)$ and $(X_2, y_2)$ with shared dimension $d = 2$, initial parameter $w_0$, learning rate $\\eta$, tolerance $\\varepsilon$, and maximum iterations $T_{\\max}$:\n\n- Test Case $1$ (moderate conflict):\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 2  1 \\\\ 1  2 \\\\ 1  -1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}$.\n\n- Test Case $2$ (aligned per-task gradients at $w_0$):\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n\n- Test Case $3$ (orthogonal per-task gradients at $w_0$):\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\n- Test Case $4$ (anti-parallel per-task gradients at $w_0$):\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix}$.\n\n- Test Case $5$ (zero gradient for task $2$ at $w_0$):\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  1 \\\\ 1  -1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 1  2 \\\\ 3  4 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\nCommon hyperparameters for all tests:\n- $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n- $\\eta = 0.05$,\n- $\\varepsilon = 10^{-6}$,\n- $T_{\\max} = 10000$.\n\nYour program must, for each test case $k \\in \\{1,2,3,4,5\\}$:\n- Run vanilla aggregated gradient descent and record $N^{(k)}_{\\mathrm{vanilla}}$, the number of iterations to reach $L_1(w_t) + L_2(w_t) \\le \\varepsilon$, or $T_{\\max}$ if not reached.\n- Run orthogonalization-based gradient surgery (orthogonalizing task $1$’s gradient with respect to task $2$’s gradient) and record $N^{(k)}_{\\mathrm{ortho}}$, similarly defined.\n- Compute the integer difference $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case index $k = 1,2,3,4,5$, that is, print\n$$\n\\left[ \\Delta^{(1)}, \\Delta^{(2)}, \\Delta^{(3)}, \\Delta^{(4)}, \\Delta^{(5)} \\right].\n$$", "solution": "The problem is scientifically and mathematically well-posed, providing all necessary information to proceed with a solution. It is grounded in the established principles of numerical optimization and linear algebra as applied to multi-task learning.\n\nThe objective is to compare the convergence rates of two gradient descent algorithms for a two-task linear regression problem. The comparison is quantified by the number of iterations required to reach a specific tolerance on the total loss.\n\nFirst, we formalize the mathematical components required for both algorithms. The loss function for each task, $i \\in \\{1, 2\\}$, is the Mean Squared Error (MSE):\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2\n$$\nwhere $w \\in \\mathbb{R}^d$ is the shared parameter vector, $X_i \\in \\mathbb{R}^{n_i \\times d}$ is the data matrix, and $y_i \\in \\mathbb{R}^{n_i}$ is the target vector for task $i$.\n\nThe core of any gradient descent method is the gradient of the loss function. We derive the gradient of $L_i(w)$ with respect to $w$. Rewriting the squared norm as a dot product, $L_i(w) = \\frac{1}{2 n_i} (X_i w - y_i)^T (X_i w - y_i)$. Expanding this gives:\n$$\nL_i(w) = \\frac{1}{2 n_i} (w^T X_i^T X_i w - 2 y_i^T X_i w + y_i^T y_i)\n$$\nTaking the gradient with respect to $w$ using standard matrix calculus identities, we obtain:\n$$\n\\nabla_w L_i(w) = \\frac{1}{2 n_i} (2 X_i^T X_i w - 2 X_i^T y_i) = \\frac{1}{n_i} X_i^T (X_i w - y_i)\n$$\nLet us denote the per-task gradients at iteration $t$ as $g_1(w_t) = \\nabla_w L_1(w_t)$ and $g_2(w_t) = \\nabla_w L_2(w_t)$.\n\nWith the gradient defined, we specify the two algorithmic regimens.\n\n**1. Vanilla Aggregated Gradient Descent**\nThis is the standard approach in multi-task learning, where the gradients from all tasks are summed to form the update direction. The update rule is:\n$$\nw_{t+1} = w_t - \\eta \\left( g_1(w_t) + g_2(w_t) \\right)\n$$\nThis method is simple but can suffer from slow convergence if the task gradients are conflicting (i.e., point in opposing directions), as the aggregated gradient magnitude may be small.\n\n**2. Orthogonalization-Based Gradient Surgery**\nThis method aims to mitigate the issue of conflicting gradients. The gradient for one task (task $1$) is modified by removing its component that is parallel to the gradient of the other task (task $2$). This ensures that the update for task $1$ does not interfere with the gradient-indicated direction for task $2$.\n\nThe component of $g_1(w_t)$ that is parallel to $g_2(w_t)$ is found by projecting $g_1(w_t)$ onto $g_2(w_t)$:\n$$\n\\text{proj}_{g_2(w_t)}(g_1(w_t)) = \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\nThe modified gradient for task $1$, denoted $g_1^{\\perp}(w_t)$, is the component of $g_1(w_t)$ orthogonal to $g_2(w_t)$:\n$$\ng_1^{\\perp}(w_t) = g_1(w_t) - \\text{proj}_{g_2(w_t)}(g_1(w_t)) = g_1(w_t) - \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\nAs specified, if $\\|g_2(w_t)\\|_2 = 0$, the projection is undefined. In this case, we do not modify the task $1$ gradient, setting $g_1^{\\perp}(w_t) = g_1(w_t)$. The update rule then becomes:\n$$\nw_{t+1} = w_t - \\eta \\left( g_1^{\\perp}(w_t) + g_2(w_t) \\right)\n$$\n\n**Computational Procedure**\nFor each of the five test cases and for each of the two algorithms, an iterative optimization is performed.\n1. Initialize the parameters $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n2. For each iteration $t$ from $0$ to $T_{\\max} - 1$:\n   a. Calculate the total loss $L_{total}(w_t) = L_1(w_t) + L_2(w_t)$.\n   b. Check the stopping criterion: If $L_{total}(w_t) \\le \\varepsilon = 10^{-6}$, the loop terminates, and the number of iterations taken is recorded as $t$.\n   c. Calculate the per-task gradients $g_1(w_t)$ and $g_2(w_t)$.\n   d. Compute the parameter update $w_{t+1}$ according to the rules of the specific algorithm (Vanilla or Orthogonalized).\n3. If the loop completes without meeting the stopping criterion, the number of iterations is recorded as $T_{\\max} = 10000$.\n\nLet $N^{(k)}_{\\mathrm{vanilla}}$ and $N^{(k)}_{\\mathrm{ortho}}$ be the number of iterations recorded for the vanilla and orthogonalization algorithms, respectively, for test case $k \\in \\{1, 2, 3, 4, 5\\}$. The final output for each case is the integer difference $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$. This value provides a direct comparison of their convergence speeds.", "answer": "```python\nimport numpy as np\n\ndef loss_fn(X, y, w):\n    \"\"\"Computes the Mean Squared Error loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    return (1.0 / (2.0 * n)) * np.dot(error, error)\n\ndef grad_fn(X, y, w):\n    \"\"\"Computes the gradient of the MSE loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return np.zeros_like(w)\n    error = X @ w - y\n    return (1.0 / n) * X.T @ error\n\ndef solve():\n    \"\"\"\n    Implements and compares vanilla and orthogonalized gradient descent for\n    several multi-task learning test cases.\n    \"\"\"\n    \n    # Common hyperparameters\n    w0 = np.array([0.0, 0.0])\n    eta = 0.05\n    epsilon = 1e-6\n    T_max = 10000\n\n    # Test cases definition\n    test_cases = [\n        # Test Case 1 (moderate conflict)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[2.0, 1.0], [1.0, 2.0], [1.0, -1.0]]),\n            \"y2\": np.array([0.0, 1.0, -1.0]),\n        },\n        # Test Case 2 (aligned gradients)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Test Case 3 (orthogonal gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[0.0, 1.0], [0.0, 1.0]]),\n            \"y2\": np.array([1.0, 1.0]),\n        },\n        # Test Case 4 (anti-parallel gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([-1.0, -2.0, -3.0]),\n        },\n        # Test Case 5 (zero gradient for task 2 at w0)\n        {\n            \"X1\": np.array([[1.0, 1.0], [1.0, -1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[1.0, 2.0], [3.0, 4.0]]),\n            \"y2\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X1, y1 = case[\"X1\"], case[\"y1\"]\n        X2, y2 = case[\"X2\"], case[\"y2\"]\n\n        # Run Vanilla Aggregated Gradient Descent\n        w = np.copy(w0)\n        n_vanilla = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_vanilla = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            w -= eta * (g1 + g2)\n\n        # Run Orthogonalization-based Gradient Surgery\n        w = np.copy(w0)\n        n_ortho = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_ortho = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            \n            g2_norm_sq = np.dot(g2, g2)\n            \n            # Use a small tolerance for the zero-norm check for floating point stability\n            if g2_norm_sq > 1e-12:\n                g1_ortho = g1 - (np.dot(g1, g2) / g2_norm_sq) * g2\n            else:\n                g1_ortho = g1\n\n            w -= eta * (g1_ortho + g2)\n            \n        delta = n_vanilla - n_ortho\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3155105"}, {"introduction": "Building on the idea of modifying gradients, we can ask a more fundamental question: what is the *best* way to combine task gradients at each step? This practice introduces the powerful concept of Pareto optimality, the theoretical foundation for multi-objective optimization. You will implement the Multiple Gradient Descent Algorithm (MGDA), which finds the optimal set of task weights $\\boldsymbol{\\alpha}$ that produces the minimum-norm combined gradient, representing a principled compromise that seeks to improve all tasks simultaneously or identifies a Pareto stationary point [@problem_id:3155072].", "problem": "You are given a differentiable multi-task learning problem with $T$ tasks, each task $t \\in \\{1,\\dots,T\\}$ associated with a scalar loss $L_t(\\mathbf{w})$, where $\\mathbf{w} \\in \\mathbb{R}^d$ is a shared parameter vector. The gradient of each task at $\\mathbf{w}$ is denoted by $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) \\in \\mathbb{R}^d$. A point $\\mathbf{w}^\\star$ is called Pareto optimal if there is no $\\mathbf{w}$ such that $L_t(\\mathbf{w}) \\le L_t(\\mathbf{w}^\\star)$ for all $t$ and $L_{t'}(\\mathbf{w})  L_{t'}(\\mathbf{w}^\\star)$ for at least one $t'$. Fundamental optimization theory states that for unconstrained, differentiable optimization, any local minimizer of a scalar objective $s(\\mathbf{w})$ must satisfy the first-order necessary condition $\\nabla s(\\mathbf{w}) = \\mathbf{0}$. Use this and other well-tested facts from multi-objective optimization to reason about Pareto stationarity.\n\nTask A (theory). Starting from the definitions above and using only foundational principles such as separation theorems for convex cones and first-order necessary conditions for unconstrained differentiable optimization, derive the following Pareto stationarity statement: if $\\mathbf{w}^\\star$ is Pareto optimal, then there exist coefficients $\\alpha_t \\ge 0$ with $\\sum_{t=1}^T \\alpha_t = 1$ such that\n$$\n\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n$$\nExplain why this condition follows from supporting hyperplane/separation arguments on the image of $\\{L_t(\\mathbf{w})\\}_{t=1}^T$ and the first-order necessary condition applied to an appropriate scalarization.\n\nTask B (algorithm). The Multiple Gradient Descent Algorithm (MGDA) at a given $\\mathbf{w}$ selects coefficients $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_T)$ by solving the quadratic program\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^T} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}) \\right\\|_2^2 \\quad \\text{subject to} \\quad \\alpha_t \\ge 0 \\; \\text{for all } t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\nThis produces the minimum-norm convex combination of task gradients. Implement a program that, for each test case below, computes $\\boldsymbol{\\alpha}$ by solving this problem exactly for $T \\in \\{2,3\\}$ using closed-form and small-case reasoning, without numeric instability or reliance on external data.\n\nSmall model and gradients. Consider a shared linear model with parameter $\\mathbf{w} \\in \\mathbb{R}^2$, and task-specific squared losses\n$$\nL_t(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t \\|_2^2,\n$$\nwith $\\mathbf{A}_t \\in \\mathbb{R}^{2 \\times 2}$ and $\\mathbf{b}_t \\in \\mathbb{R}^2$. The gradient is\n$$\n\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) = \\mathbf{A}_t^\\top (\\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t).\n$$\nIn all test cases below, use $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{A}_t = \\mathbf{I}_2$ for every task, so that $\\mathbf{g}_t(\\mathbf{w}) = -\\mathbf{b}_t$.\n\nTest suite. For each test case, you are given the list of $\\mathbf{b}_t$ vectors, one per task, and must compute the MGDA coefficients $\\boldsymbol{\\alpha}$.\n\n- Case $1$ (two tasks, interior solution exists): $T=2$, $\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{b}_2 = \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix}$.\n\n- Case $2$ (two tasks, colinear same direction, boundary solution): $T=2$, $\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{b}_2 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$.\n\n- Case $3$ (three tasks, strictly interior solution with all coefficients positive): $T=3$, $\\mathbf{b}_1 = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$, $\\mathbf{b}_3 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\nRequirements for your program.\n\n- Your program must compute $\\boldsymbol{\\alpha}$ for each case by solving the MGDA quadratic program exactly for $T \\in \\{2,3\\}$, using closed-form formulas for $T=2$ and small-case enumeration for $T=3$ (interior barycentric solution if feasible; otherwise minimize along edges and vertices).\n\n- The final numeric answers must be rounded to $6$ decimal places.\n\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of coefficients for the corresponding test case. For example, an output could look like $[[a_{1},a_{2}],[b_{1},b_{2}],[c_{1},c_{2},c_{3}]]$, with no spaces. Replace $a_i,b_i,c_i$ by your computed floats rounded to $6$ decimals.\n\n- The only acceptable outputs are lists of floats; no booleans or strings in the list.\n\nYour program must be self-contained and require no inputs. It must not read from or write to files. It must implement the MGDA solution logic as described and compute the coefficients for the three test cases above.", "solution": "The analysis of the problem is divided into two parts as requested: a theoretical derivation of the Pareto stationarity condition (Task A) and the implementation of an algorithm to solve for the Multiple Gradient Descent Algorithm (MGDA) coefficients (Task B).\n\n### Task A: Derivation of the Pareto Stationarity Condition\n\nThis task requires deriving the first-order necessary condition for Pareto optimality. This condition is also known as Pareto stationarity. The derivation relies on the definition of Pareto optimality and fundamental results from convex analysis, specifically separation theorems.\n\nLet the set of $T$ differentiable task-specific loss functions be $\\{L_t(\\mathbf{w})\\}_{t=1}^T$, where $\\mathbf{w} \\in \\mathbb{R}^d$ is the shared parameter vector. The gradient of each loss is $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w})$.\n\n1.  **Condition for a Descent Direction**:\n    A vector $\\mathbf{d} \\in \\mathbb{R}^d$ is a descent direction for task $t$ at point $\\mathbf{w}$ if a small step in this direction reduces the loss $L_t$. Based on a first-order Taylor expansion, this condition is $\\mathbf{g}_t(\\mathbf{w})^\\top \\mathbf{d}  0$.\n\n2.  **Pareto Optimality and Descent Directions**:\n    By definition, a point $\\mathbf{w}^\\star$ is Pareto optimal if no other point $\\mathbf{w}$ exists that is strictly better for at least one task without being worse for any other task. In terms of local improvements, this implies that there cannot be a descent direction $\\mathbf{d}$ from $\\mathbf{w}^\\star$ that improves at least one task loss without worsening any other.\n    Formally, if $\\mathbf{w}^\\star$ is a Pareto optimal point, then there exists no vector $\\mathbf{d} \\in \\mathbb{R}^d$ such that:\n    $$\n    \\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d} \\le 0 \\quad \\text{for all } t \\in \\{1, \\dots, T\\}\n    $$\n    and\n    $$\n    \\mathbf{g}_{t'}(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0 \\quad \\text{for at least one } t' \\in \\{1, \\dots, T\\}.\n    $$\n    If such a direction $\\mathbf{d}$ existed, one could take a small step $\\epsilon\\mathbf{d}$ (with $\\epsilon  0$) from $\\mathbf{w}^\\star$ to obtain a new point $\\mathbf{w}' = \\mathbf{w}^\\star + \\epsilon\\mathbf{d}$ that would Pareto-dominate $\\mathbf{w}^\\star$, contradicting its optimality. The simpler condition that there is no $\\mathbf{d}$ such that $\\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0$ for all $t$ is a direct consequence.\n\n3.  **Application of a Theorem of the Alternative (Gordan's Lemma)**:\n    The non-existence of such a common descent direction can be formally established using a theorem of the alternative. Gordan's Lemma, a result from convex analysis, states that for any set of vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_T\\}$, exactly one of the following two statements must be true:\n    (i) There exists a vector $\\mathbf{d}$ such that $\\mathbf{v}_t^\\top \\mathbf{d}  0$ for all $t=1, \\dots, T$.\n    (ii) The zero vector $\\mathbf{0}$ is a non-trivial non-negative combination of the vectors, i.e., there exist coefficients $\\lambda_t \\ge 0$, not all zero, such that $\\sum_{t=1}^T \\lambda_t \\mathbf{v}_t = \\mathbf{0}$.\n    Equivalently, (ii) states that $\\mathbf{0}$ is in the convex cone of $\\{\\mathbf{v}_t\\}$.\n    A slightly more general version of this theorem covers the mixed inequalities ($\\le$ and $$) from step 2 and leads to the same conclusion.\n\n4.  **Deriving the Pareto Stationarity Condition**:\n    Applying this theorem to our set of gradients $\\{\\mathbf{g}_t(\\mathbf{w}^\\star)\\}$, the established non-existence of a common descent direction (ruling out statement (i)) implies that statement (ii) must be true. Therefore, there must exist coefficients $\\lambda_t \\ge 0$ for $t=1, \\dots, T$, with at least one $\\lambda_t  0$, such that:\n    $$\n    \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n    $$\n    Since the coefficients $\\lambda_t$ are not all zero, their sum $S = \\sum_{j=1}^T \\lambda_j$ is strictly positive. We can normalize these coefficients by defining $\\alpha_t = \\lambda_t / S$. These new coefficients satisfy:\n    - $\\alpha_t \\ge 0$ for all $t$.\n    - $\\sum_{t=1}^T \\alpha_t = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t = \\frac{S}{S} = 1$.\n    - $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\mathbf{0} = \\mathbf{0}$.\n    This concludes the derivation of the Pareto stationarity condition:\n    $$\n    \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}, \\quad \\text{with } \\alpha_t \\ge 0, \\sum_{t=1}^T \\alpha_t = 1.\n    $$\n\n5.  **Connection to Scalarization and Supporting Hyperplanes**:\n    The condition $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}$ is precisely the first-order necessary condition for a point $\\mathbf{w}^\\star$ to be a local minimum of the scalarized objective function $S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t L_t(\\mathbf{w})$. The gradient of this scalarized loss is $\\nabla S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t \\nabla L_t(\\mathbf{w})$.\n    Geometrically, in the loss space spanned by $(L_1, \\dots, L_T)$, the vector $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_T)$ defines a hyperplane. The Pareto stationarity condition implies that this hyperplane is a supporting hyperplane to the set of achievable loss vectors at the point $(L_1(\\mathbf{w}^\\star), \\dots, L_T(\\mathbf{w}^\\star))$.\n\n### Task B: MGDA Algorithm Implementation\n\nThe goal is to solve the quadratic program (QP) for $\\boldsymbol{\\alpha}$:\n$$\n\\min_{\\boldsymbol{\\alpha}} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t \\right\\|_2^2 \\quad \\text{s.t.} \\quad \\alpha_t \\ge 0 \\; \\forall t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\nThe gradients are given by $\\mathbf{g}_t = -\\mathbf{b}_t$.\n\n#### Case $T=2$\nThe problem is to minimize $f(\\alpha_1, \\alpha_2) = \\frac{1}{2}\\|\\alpha_1 \\mathbf{g}_1 + \\alpha_2 \\mathbf{g}_2\\|_2^2$ subject to $\\alpha_1 + \\alpha_2 = 1$ and $\\alpha_1, \\alpha_2 \\ge 0$. Substituting $\\alpha_2 = 1-\\alpha_1$ reduces the problem to minimizing a quadratic function of $\\alpha_1 \\in [0, 1]$. Taking the derivative with respect to $\\alpha_1$ and setting it to zero yields the unconstrained minimizer:\n$$\n\\alpha_1^* = \\frac{\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1)}{\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2}.\n$$\nThe solution to the constrained problem is found by projecting $\\alpha_1^*$ onto the interval $[0, 1]$. Thus, $\\alpha_1 = \\text{max}(0, \\text{min}(1, \\alpha_1^*))$, and $\\alpha_2 = 1 - \\alpha_1$.\n\n- **Case 1:** $\\mathbf{g}_1 = [-1, 0]^\\top$, $\\mathbf{g}_2 = [0.5, 0]^\\top$.\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[-1.5, 0]^\\top\\|_2^2 = 2.25$.\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [0.5, 0] \\cdot [1.5, 0]^\\top = 0.75$.\n  $\\alpha_1^* = 0.75 / 2.25 = 1/3$. Since $0 \\le 1/3 \\le 1$, we have $\\alpha_1 = 1/3$ and $\\alpha_2 = 2/3$.\n\n- **Case 2:** $\\mathbf{g}_1 = [-1, 0]^\\top$, $\\mathbf{g}_2 = [-2, 0]^\\top$.\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[1, 0]^\\top\\|_2^2 = 1$.\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [-2, 0] \\cdot [-1, 0]^\\top = 2$.\n  $\\alpha_1^* = 2 / 1 = 2$. Since $2 > 1$, we project to the boundary: $\\alpha_1 = 1$ and $\\alpha_2 = 0$.\n\n#### Case $T=3$\nFor $T=3$ and gradients in $\\mathbb{R}^2$, the minimum norm convex combination of gradients is zero if and only if the origin is in the convex hull of the gradients (the triangle they form). This can be tested by finding the barycentric coordinates $(\\alpha_1, \\alpha_2, \\alpha_3)$ of the origin. If all $\\alpha_t \\ge 0$, this is the solution. This requires solving the system:\n$$\n\\begin{pmatrix} g_{1x}  g_{2x}  g_{3x} \\\\ g_{1y}  g_{2y}  g_{3y} \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nIf this fails (e.g., origin is outside the triangle), the solution lies on one of the edges of the simplex, which reduces to solving three separate $T=2$ problems and picking the best one.\n\n- **Case 3:** $\\mathbf{g}_1 = [1, 0]^\\top$, $\\mathbf{g}_2 = [0, 1]^\\top$, $\\mathbf{g}_3 = [-1, -1]^\\top$.\nWe observe that $\\mathbf{g}_1 + \\mathbf{g}_2 + \\mathbf{g}_3 = \\mathbf{0}$. A convex combination that yields $\\mathbf{0}$ is $\\frac{1}{3}\\mathbf{g}_1 + \\frac{1}{3}\\mathbf{g}_2 + \\frac{1}{3}\\mathbf{g}_3 = \\mathbf{0}$. Since the objective function is a squared norm, its minimum value is $0$, which is achieved by this combination. The coefficients are $\\alpha_1 = 1/3$, $\\alpha_2 = 1/3$, $\\alpha_3 = 1/3$. All are non-negative and sum to $1$, so this is the unique valid solution. Solving the linear system above confirms this:\n$$\n\\begin{pmatrix} 1  0  -1 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\implies \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}.\n$$\n\nThe results are:\n- Case 1: $(\\alpha_1, \\alpha_2) = (1/3, 2/3)$\n- Case 2: $(\\alpha_1, \\alpha_2) = (1, 0)$\n- Case 3: $(\\alpha_1, \\alpha_2, \\alpha_3) = (1/3, 1/3, 1/3)$\nThese are implemented below to produce the final formatted output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_t2(g1: np.ndarray, g2: np.ndarray) -> tuple[float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=2 tasks exactly.\n    \"\"\"\n    g1_minus_g2 = g1 - g2\n    g1_minus_g2_sq_norm = np.dot(g1_minus_g2, g1_minus_g2)\n\n    # Handle the degenerate case where g1 == g2\n    if np.isclose(g1_minus_g2_sq_norm, 0):\n        return 0.5, 0.5\n\n    # Compute unconstrained minimizer for alpha_1\n    alpha1_star = np.dot(g2, g2 - g1) / g1_minus_g2_sq_norm\n\n    # Project onto the interval [0, 1]\n    alpha1 = max(0.0, min(1.0, alpha1_star))\n    alpha2 = 1.0 - alpha1\n    \n    return alpha1, alpha2\n\ndef solve_t3(g1: np.ndarray, g2: np.ndarray, g3: np.ndarray) -> tuple[float, float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=3 tasks exactly, as per problem specification.\n    \"\"\"\n    # First, check for an interior solution where the combination of gradients is zero.\n    # This happens if the origin is in the convex hull of the gradients.\n    # We solve for barycentric coordinates of the origin.\n    G_ext = np.array([\n        [g1[0], g2[0], g3[0]],\n        [g1[1], g2[1], g3[1]],\n        [1.0,   1.0,   1.0]\n    ])\n    \n    b_target = np.array([0.0, 0.0, 1.0])\n\n    # Check if a unique solution exists by checking if the matrix is invertible.\n    if abs(np.linalg.det(G_ext)) > 1e-9:\n        try:\n            alphas_int = np.linalg.solve(G_ext, b_target)\n            # If all coefficients are non-negative, this is the solution.\n            if np.all(alphas_int >= -1e-9):\n                return tuple(alphas_int)\n        except np.linalg.LinAlgError:\n            # Should not happen if det is non-zero, but for robustness.\n            pass\n\n    # If no interior solution exists, solve for solutions on the edges.\n    # Edge 1-2\n    a12 = solve_t2(g1, g2)\n    alpha_A = np.array([a12[0], a12[1], 0.0])\n    grad_A = alpha_A[0] * g1 + alpha_A[1] * g2\n    obj_A = 0.5 * np.dot(grad_A, grad_A)\n    \n    # Edge 1-3\n    a13 = solve_t2(g1, g3)\n    alpha_B = np.array([a13[0], 0.0, a13[1]])\n    grad_B = alpha_B[0] * g1 + alpha_B[2] * g3\n    obj_B = 0.5 * np.dot(grad_B, grad_B)\n\n    # Edge 2-3\n    a23 = solve_t2(g2, g3)\n    alpha_C = np.array([0.0, a23[0], a23[1]])\n    grad_C = alpha_C[1] * g2 + alpha_C[2] * g3\n    obj_C = 0.5 * np.dot(grad_C, grad_C)\n    \n    # Find the edge solution with the minimum objective value.\n    objectives = [obj_A, obj_B, obj_C]\n    candidates = [alpha_A, alpha_B, alpha_C]\n    best_alpha = candidates[np.argmin(objectives)]\n    \n    return tuple(best_alpha)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Gradients are g_t = -b_t.\n    test_cases = [\n        # Case 1 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([-0.5, 0.0])]},\n        # Case 2 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([2.0, 0.0])]},\n        # Case 3 (T=3)\n        {\"b_vectors\": [np.array([-1.0, 0.0]), np.array([0.0, -1.0]), np.array([1.0, 1.0])]},\n    ]\n\n    results = []\n    \n    # Case 1\n    case1 = test_cases[0]\n    g_vectors1 = [-b for b in case1[\"b_vectors\"]]\n    result1 = solve_t2(g_vectors1[0], g_vectors1[1])\n    results.append(list(result1))\n    \n    # Case 2\n    case2 = test_cases[1]\n    g_vectors2 = [-b for b in case2[\"b_vectors\"]]\n    result2 = solve_t2(g_vectors2[0], g_vectors2[1])\n    results.append(list(result2))\n\n    # Case 3\n    case3 = test_cases[2]\n    g_vectors3 = [-b for b in case3[\"b_vectors\"]]\n    result3 = solve_t3(g_vectors3[0], g_vectors3[1], g_vectors3[2])\n    results.append(list(result3))\n\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_list in results:\n        # Format each float to 6 decimal places, then join into a string like \"[f1,f2,...]\"\n        formatted_list = \"[\" + \",\".join([f\"{x:.6f}\" for x in res_list]) + \"]\"\n        formatted_results.append(formatted_list)\n    \n    # Join all case results into the final output string.\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3155072"}, {"introduction": "A critical risk in multi-task learning is \"negative transfer,\" where training on an auxiliary task hurts the performance of a primary task. This exercise simulates a realistic scenario where a domain alignment technique, intended to help a task facing a data distribution shift, may inadvertently harm a stable, co-trained task by over-regularizing the shared model components. By implementing this simulation, you will gain practical insight into the delicate trade-offs involved in designing shared representations and avoiding the unintended consequences of task interference [@problem_id:3155049].", "problem": "Consider a two-task supervised learning setup in Multi-Task Learning (MTL) with a shared linear encoder and task-specific linear heads. A single input vector $x \\in \\mathbb{R}^d$ is observed for both tasks. Task $\\mathrm{A}$ experiences a domain shift between training and test in its input distribution, while Task $\\mathrm{B}$ is stationary (its train and test input distributions are the same). We study whether adding a domain alignment regularizer that targets Task $\\mathrm{A}$ can harm Task $\\mathrm{B}$ through over-regularization of the shared encoder.\n\nFundamental base definitions to use:\n- Empirical Risk Minimization (ERM) for supervised learning seeks to minimize empirical loss over training data.\n- Linear models with squared error are a well-tested and convex setting. For inputs $x \\in \\mathbb{R}^d$, a linear model predicts $\\hat{y} = \\theta^\\top x$ and minimizes mean squared error.\n- Ridge regression (linear least squares with $\\ell_2$ regularization) for features $Z \\in \\mathbb{R}^{n \\times d}$ and targets $y \\in \\mathbb{R}^n$ minimizes $\\|Z \\theta - y\\|_2^2 + \\alpha \\|\\theta\\|_2^2$ with closed-form solution $\\theta^\\star = (Z^\\top Z + \\alpha I)^{-1} Z^\\top y$.\n- In MTL, a shared representation can be parameterized by a linear encoder $W$ that maps $x$ to $z = W x$, while task-specific heads map $z$ to task outputs.\n\nModel specification:\n- Let the shared encoder be a diagonal linear map $W = \\operatorname{diag}(s)$ with $s \\in \\mathbb{R}^d_{\\ge 0}$. The shared representation is $z = s \\odot x$, where $\\odot$ denotes elementwise multiplication.\n- Task $\\mathrm{A}$ and Task $\\mathrm{B}$ heads are vectors $a \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}^d$, giving predictions $\\hat{y}_{\\mathrm{A}} = a^\\top (s \\odot x)$ and $\\hat{y}_{\\mathrm{B}} = b^\\top (s \\odot x)$.\n- Training data for both tasks are drawn from a common source domain $\\mathcal{D}_{\\mathrm{S}}$ with $x \\sim \\mathcal{N}(\\mu_{\\mathrm{S}}, \\Sigma)$; Task $\\mathrm{A}$ has additional unlabeled target-domain inputs from $\\mathcal{D}_{\\mathrm{T}}$ with $x \\sim \\mathcal{N}(\\mu_{\\mathrm{T}}, \\Sigma)$, where $\\mu_{\\mathrm{T}} \\ne \\mu_{\\mathrm{S}}$ only in one coordinate.\n- Labels follow linear ground-truths with additive noise: $y_{\\mathrm{A}} = \\beta_{\\mathrm{A}}^\\top x + \\epsilon_{\\mathrm{A}}$ and $y_{\\mathrm{B}} = \\beta_{\\mathrm{B}}^\\top x + \\epsilon_{\\mathrm{B}}$, where $\\epsilon_{\\mathrm{A}}$ and $\\epsilon_{\\mathrm{B}}$ are independent zero-mean noise terms.\n\nTraining objective:\n- The total objective is the sum of Task $\\mathrm{A}$ and Task $\\mathrm{B}$ empirical squared losses on source-domain training data, plus ridge penalties for $a$ and $b$, plus an $\\ell_2$ penalty on $s$, and a domain alignment term that penalizes the squared difference of encoded means between source and target for Task $\\mathrm{A}$:\n$$\n\\mathcal{L}(s,a,b) = \\underbrace{\\|Z_{\\mathrm{S}} a - y_{\\mathrm{A}}\\|_2^2}_{\\text{Task A loss}} + \\underbrace{\\|Z_{\\mathrm{S}} b - y_{\\mathrm{B}}\\|_2^2}_{\\text{Task B loss}} + \\alpha_{\\mathrm{head}} (\\|a\\|_2^2 + \\|b\\|_2^2) + \\alpha_s \\|s\\|_2^2 + \\lambda \\|\\mu_{Z}^{\\mathrm{S}} - \\mu_{Z}^{\\mathrm{T}}\\|_2^2,\n$$\nwhere $Z_{\\mathrm{S}} = X_{\\mathrm{S}} \\odot s$ is the source-domain encoded design matrix, $\\mu_{Z}^{\\mathrm{S}} = s \\odot \\mu_{X}^{\\mathrm{S}}$ and $\\mu_{Z}^{\\mathrm{T}} = s \\odot \\mu_{X}^{\\mathrm{T}}$, with $\\mu_{X}^{\\mathrm{S}}$ and $\\mu_{X}^{\\mathrm{T}}$ being the input means of source and target domains, respectively, and $\\lambda \\ge 0$ is the alignment weight. The alignment term reduces to\n$$\n\\|\\mu_{Z}^{\\mathrm{S}} - \\mu_{Z}^{\\mathrm{T}}\\|_2^2 = \\sum_{j=1}^d \\left(s_j (\\mu_{X,j}^{\\mathrm{S}} - \\mu_{X,j}^{\\mathrm{T}})\\right)^2 = \\sum_{j=1}^d s_j^2 \\Delta\\mu_j^2,\n$$\nwith $\\Delta\\mu = \\mu_{X}^{\\mathrm{S}} - \\mu_{X}^{\\mathrm{T}}$.\n\nOptimization strategy:\n- For fixed $s$, set $a$ and $b$ to their ridge regression optima on $Z_{\\mathrm{S}}$:\n$$\na^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{A}}, \\quad b^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{B}}.\n$$\n- Update $s$ by gradient descent using the envelope theorem to take the partial derivative of $\\mathcal{L}$ with $a = a^\\star(s)$ and $b = b^\\star(s)$ held fixed:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_j} = 2 \\sum_{i=1}^n \\left((\\hat{y}_{\\mathrm{A},i} - y_{\\mathrm{A},i}) a_j x_{ij} + (\\hat{y}_{\\mathrm{B},i} - y_{\\mathrm{B},i}) b_j x_{ij}\\right) + 2 \\lambda s_j \\Delta \\mu_j^2 + 2 \\alpha_s s_j.\n$$\n- Nonnegativity constraint on $s$ can be enforced by elementwise clipping $s_j \\ge 0$ after each update.\n\nEvaluation target:\n- Harm to Task $\\mathrm{B}$ is defined as an increase in Task $\\mathrm{B}$ test mean squared error (MSE) when training with $\\lambda > 0$ relative to $\\lambda = 0$, on a held-out source-domain test set for Task $\\mathrm{B}$. Formally, harm is deemed present if $\\mathrm{MSE}_{\\mathrm{B}}^{\\lambda} > \\mathrm{MSE}_{\\mathrm{B}}^{0}$ by more than a small tolerance.\n\nYou must implement a complete, runnable program that:\n1. Synthesizes data according to the Gaussian model with specified parameters.\n2. Trains the shared-diagonal-encoder MTL model with ridge regression heads and gradient descent on $s$.\n3. Computes Task $\\mathrm{B}$ test MSE with and without domain alignment.\n4. Outputs, for each test case, a boolean indicating whether the aligned training harmed Task $\\mathrm{B}$ according to the definition above.\n\nUse the following fixed generative specifications:\n- Input dimension $d = 5$, covariance $\\Sigma = I_d$.\n- Ground-truth coefficients $\\beta_{\\mathrm{A}} = [0.2, 1.0, -0.5, 0.3, 0.0]$ and $\\beta_{\\mathrm{B}} = [3.0, 0.5, 0.0, 0.0, 0.0]$.\n- Source-domain mean $\\mu_{\\mathrm{S}} = [0,0,0,0,0]^\\top$, and target-domain mean $\\mu_{\\mathrm{T}}$ differs from $\\mu_{\\mathrm{S}}$ only at a single index specified by the test case.\n- Noise terms $\\epsilon_{\\mathrm{A}}$ and $\\epsilon_{\\mathrm{B}}$ are independent and distributed as $\\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.1$.\n\nUse the following training hyperparameters for all test cases:\n- Encoder regularization $\\alpha_s = 10^{-3}$.\n- Head regularization $\\alpha_{\\mathrm{head}} = 1.0$.\n- Gradient descent learning rate $\\eta = 0.05$.\n- Number of gradient iterations $T = 500$.\n- Initialize $s$ to the vector of ones.\n\nTest suite:\nProvide results on the following five parameter settings. Each test case specifies the target-domain shift magnitude, the alignment weight, the number of labeled source samples, the number of unlabeled target samples for alignment, and the index of the shifted dimension. All numbers are real-valued scalars; indices are integers:\n- Case $1$: $(\\text{shift\\_mag} = 2.0, \\ \\lambda = 1.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$.\n- Case $2$: $(\\text{shift\\_mag} = 3.0, \\ \\lambda = 10.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$.\n- Case $3$: $(\\text{shift\\_mag} = 0.0, \\ \\lambda = 10.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$.\n- Case $4$: $(\\text{shift\\_mag} = 2.0, \\ \\lambda = 2.0, \\ n_{\\mathrm{train}} = 50, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$.\n- Case $5$: $(\\text{shift\\_mag} = 3.0, \\ \\lambda = 10.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 4)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean for the corresponding test case indicating whether Task $\\mathrm{B}$ test MSE with alignment exceeds the baseline by more than a tolerance of $10^{-3}$.", "solution": "The problem asks us to investigate whether a domain alignment regularizer, applied to assist a single task (Task $\\mathrm{A}$) under domain shift, can inadvertently harm a second, stationary task (Task $\\mathrm{B}$) within a Multi-Task Learning (MTL) framework. The analysis is conducted within a simplified, yet illustrative, linear setting.\n\nThe problem statement has been rigorously validated and is deemed scientifically grounded, well-posed, and objective. It provides a clear, formalizable model and a concrete set of computational experiments. The only minor ambiguity is the size of the test set for evaluation, which is not explicitly provided. For robust and stable estimation of Mean Squared Error (MSE), a large test set size of $n_{\\mathrm{test}} = 10000$ will be used. This choice does not alter the core logic of the problem and is a standard practice in such simulations.\n\nThe solution proceeds by implementing a simulation that adheres to the specified model, training procedure, and evaluation protocol.\n\n**1. Data Generation**\nFor each test case, we synthesize the necessary datasets. Data generation follows the specified probabilistic model.\n- An input vector $x$ has dimension $d=5$.\n- The source domain inputs $X_{\\mathrm{S}}$ are drawn from a multivariate normal distribution $\\mathcal{N}(\\mu_{\\mathrm{S}}, \\Sigma)$, where the mean is $\\mu_{\\mathrm{S}} = \\mathbf{0} \\in \\mathbb{R}^5$ and the covariance is the identity matrix $\\Sigma = I_5$. This means the features are independent and standard normal. $n_{\\mathrm{train}}$ labeled samples are drawn from this distribution.\n- The target domain inputs $X_{\\mathrm{T}}$ for Task $\\mathrm{A}$'s alignment regularizer are drawn from $\\mathcal{N}(\\mu_{\\mathrm{T}}, \\Sigma)$. The mean $\\mu_{\\mathrm{T}}$ is a zero vector except for a single entry at index `shift_dim`, which has a value of `shift_mag`. $n_{\\mathrm{target}}$ unlabeled samples are drawn.\n- A held-out test set of size $n_{\\mathrm{test}} = 10000$ is also drawn from the source distribution $\\mathcal{N}(\\mu_{\\mathrm{S}}, \\Sigma)$ to evaluate the final models.\n- The labels for both tasks are generated according to the linear models $y_{\\mathrm{A}} = \\beta_{\\mathrm{A}}^\\top x + \\epsilon_{\\mathrm{A}}$ and $y_{\\mathrm{B}} = \\beta_{\\mathrm{B}}^\\top x + \\epsilon_{\\mathrm{B}}$, with ground-truth vectors $\\beta_{\\mathrm{A}} = [0.2, 1.0, -0.5, 0.3, 0.0]$ and $\\beta_{\\mathrm{B}} = [3.0, 0.5, 0.0, 0.0, 0.0]$. The noise terms $\\epsilon_{\\mathrm{A}}, \\epsilon_{\\mathrm{B}}$ are drawn independently from $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.1$.\n- To ensure a fair comparison between the models trained with and without alignment, the same set of generated data (training, target, and test) is used for both runs within each test case.\n\n**2. Model and Training Procedure**\nThe model consists of a shared diagonal linear encoder $W = \\operatorname{diag}(s)$ and two task-specific linear heads, $a$ and $b$. An input $x$ is encoded as $z = s \\odot x$, and predictions are $\\hat{y}_{\\mathrm{A}} = a^\\top z$ and $\\hat{y}_{\\mathrm{B}} = b^\\top z$.\n\nThe training objective to be minimized is:\n$$\n\\mathcal{L}(s,a,b) = \\|(X_{\\mathrm{S}} \\odot s) a - y_{\\mathrm{A}}\\|_2^2 + \\|(X_{\\mathrm{S}} \\odot s) b - y_{\\mathrm{B}}\\|_2^2 + \\alpha_{\\mathrm{head}} (\\|a\\|_2^2 + \\|b\\|_2^2) + \\alpha_s \\|s\\|_2^2 + \\lambda \\|s \\odot (\\mu_{X}^{\\mathrm{S}} - \\mu_{X}^{\\mathrm{T}})\\|_2^2\n$$\nwhere $X_{\\mathrm{S}} \\odot s$ denotes element-wise multiplication of the vector $s$ across the rows of matrix $X_{\\mathrm{S}}$, and $\\mu_{X}^{\\mathrm{S}}, \\mu_{X}^{\\mathrm{T}}$ are the empirical means of the generated source and target datasets, respectively.\n\nWe employ an alternating optimization scheme:\n- **For a fixed encoder scale vector $s$**: The optimization for the heads $a$ and $b$ decouples into two standard ridge regression problems. The optimal heads, $a^\\star(s)$ and $b^\\star(s)$, are found using the closed-form solution:\n$$\na^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{A}}\n$$\n$$\nb^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{B}}\n$$\nwhere $Z_{\\mathrm{S}} = X_{\\mathrm{S}} \\odot s$. These linear systems are solved numerically for stability.\n\n- **For fixed heads $a$ and $b$**: The encoder scale vector $s$ is updated via gradient descent. The gradient of the objective with respect to $s_j$ is given by the problem statement and derived using the envelope theorem:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_j} = 2 \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( (\\hat{y}_{\\mathrm{A},i} - y_{\\mathrm{A},i}) a_j x_{ij} + (\\hat{y}_{\\mathrm{B},i} - y_{\\mathrm{B},i}) b_j x_{ij} \\right) + 2 \\alpha_s s_j + 2 \\lambda s_j (\\mu_{X,j}^{\\mathrm{S}} - \\mu_{X,j}^{\\mathrm{T}})^2\n$$\nThe update rule is $s \\leftarrow s - \\eta \\nabla_s \\mathcal{L}$. After each step, the non-negativity constraint $s_j \\ge 0$ is enforced by element-wise clipping.\n\nThis two-step process is repeated for $T=500$ iterations, starting with $s$ initialized to a vector of ones.\n\n**3. Evaluation of Harm**\nFor each test case, we perform two full training runs:\n1.  **Baseline Model**: Train the MTL model with the alignment regularizer turned off, i.e., $\\lambda = 0$. This yields parameters $(s^0, a^0, b^0)$.\n2.  **Aligned Model**: Train the MTL model with the specified alignment weight $\\lambda > 0$. This yields parameters $(s^\\lambda, a^\\lambda, b^\\lambda)$.\n\nAfter training, we compute the MSE for Task $\\mathrm{B}$ on the held-out test set for both models:\n- $\\mathrm{MSE}_{\\mathrm{B}}^{0} = \\frac{1}{n_{\\mathrm{test}}} \\|(X_{\\mathrm{test}} \\odot s^0) b^0 - y_{\\mathrm{B,test}}\\|_2^2$\n- $\\mathrm{MSE}_{\\mathrm{B}}^{\\lambda} = \\frac{1}{n_{\\mathrm{test}}} \\|(X_{\\mathrm{test}} \\odot s^\\lambda) b^\\lambda - y_{\\mathrm{B,test}}\\|_2^2$\n\nHarm to Task $\\mathrm{B}$ is determined to be present if the MSE of the aligned model is greater than the baseline MSE by more than a specified tolerance of $10^{-3}$. That is, if $\\mathrm{MSE}_{\\mathrm{B}}^{\\lambda} > \\mathrm{MSE}_{\\mathrm{B}}^{0} + 10^{-3}$. The result for each case is a boolean value indicating the presence of such harm.\n\nThe entire simulation is run for the five test cases specified, and the boolean results are collected and formatted as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire experiment.\n    np.random.seed(0)\n\n    # --- Fixed Generative Specifications ---\n    D = 5\n    BETA_A = np.array([0.2, 1.0, -0.5, 0.3, 0.0])\n    BETA_B = np.array([3.0, 0.5, 0.0, 0.0, 0.0])\n    MU_S_TRUE = np.zeros(D)\n    SIGMA = np.identity(D)\n    NOISE_STD = 0.1\n    N_TEST = 10000  # A large test set for stable MSE evaluation.\n\n    # --- Fixed Training Hyperparameters ---\n    ALPHA_S = 1e-3\n    ALPHA_HEAD = 1.0\n    LR = 0.05\n    N_ITER = 500\n\n    # --- Test Suite ---\n    test_cases = [\n        {'shift_mag': 2.0, 'lambda_align': 1.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 3.0, 'lambda_align': 10.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 0.0, 'lambda_align': 10.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 2.0, 'lambda_align': 2.0, 'n_train': 50, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 3.0, 'lambda_align': 10.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 4},\n    ]\n\n    results = []\n    for case in test_cases:\n        harm = train_and_eval_case(\n            case, D, BETA_A, BETA_B, MU_S_TRUE, SIGMA, NOISE_STD, N_TEST,\n            ALPHA_S, ALPHA_HEAD, LR, N_ITER\n        )\n        results.append(harm)\n\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\n\ndef generate_data(n, d, mu, sigma, beta_a, beta_b, noise_std):\n    \"\"\"Generates synthetic data for the MTL tasks.\"\"\"\n    X = np.random.multivariate_normal(mu, sigma, n)\n    epsilon_a = np.random.normal(0, noise_std, n)\n    epsilon_b = np.random.normal(0, noise_std, n)\n    y_a = X @ beta_a + epsilon_a\n    y_b = X @ beta_b + epsilon_b\n    return X, y_a, y_b\n\n\ndef train_mtl(X_s, y_a, y_b, X_t, lambda_align, d, alpha_s, alpha_head, lr, n_iter):\n    \"\"\"Trains the MTL model and returns the learned parameters.\"\"\"\n    n_train = X_s.shape[0]\n    s = np.ones(d)  # Initialize s\n\n    # Empirical means for the alignment penalty\n    mu_x_s = np.mean(X_s, axis=0)\n    mu_x_t = np.mean(X_t, axis=0)\n    delta_mu_sq = (mu_x_s - mu_x_t)**2\n    \n    # Pre-computation for ridge regression\n    I_d = np.identity(d)\n\n    for _ in range(n_iter):\n        # 1. Compute encoded features\n        Z_s = X_s * s\n\n        # 2. Solve for optimal heads a and b (Ridge Regression)\n        # For a\n        A_a = Z_s.T @ Z_s + alpha_head * I_d\n        b_a = Z_s.T @ y_a\n        a = np.linalg.solve(A_a, b_a)\n        \n        # For b\n        A_b = Z_s.T @ Z_s + alpha_head * I_d\n        b_b = Z_s.T @ y_b\n        b = np.linalg.solve(A_b, b_b)\n\n        # 3. Compute gradient w.r.t s using envelope theorem\n        y_a_hat = Z_s @ a\n        y_b_hat = Z_s @ b\n        res_a = y_a_hat - y_a\n        res_b = y_b_hat - y_b\n\n        grad_res_a = np.sum((res_a[:, np.newaxis] * a) * X_s, axis=0)\n        grad_res_b = np.sum((res_b[:, np.newaxis] * b) * X_s, axis=0)\n        \n        grad_s = 2 * (grad_res_a + grad_res_b) + 2 * alpha_s * s + 2 * lambda_align * s * delta_mu_sq\n\n        # 4. Update s with gradient descent and projection\n        s = s - lr * grad_s\n        s[s  0] = 0.0\n\n    return s, a, b\n\n\ndef train_and_eval_case(case_params, d, beta_a, beta_b, mu_s_true, sigma, noise_std, n_test,\n                        alpha_s, alpha_head, lr, n_iter):\n    \"\"\"\n    Runs the full experiment for a single test case, comparing models with and without alignment.\n    \"\"\"\n    n_train = case_params['n_train']\n    n_target = case_params['n_target']\n    shift_dim = case_params['shift_dim']\n    shift_mag = case_params['shift_mag']\n    lambda_align = case_params['lambda_align']\n    \n    # --- Generate common datasets for fair comparison ---\n    X_s_train, y_a_train, y_b_train = generate_data(n_train, d, mu_s_true, sigma, beta_a, beta_b, noise_std)\n\n    mu_t_true = np.copy(mu_s_true)\n    mu_t_true[shift_dim] = shift_mag\n    X_t_unlabeled, _, _ = generate_data(n_target, d, mu_t_true, sigma, beta_a, beta_b, noise_std)\n\n    X_test, _, y_b_test = generate_data(n_test, d, mu_s_true, sigma, beta_a, beta_b, noise_std)\n    \n    # --- Train and evaluate baseline model (lambda = 0) ---\n    s0, a0, b0 = train_mtl(X_s_train, y_a_train, y_b_train, X_t_unlabeled, 0.0, \n                           d, alpha_s, alpha_head, lr, n_iter)\n    \n    Z_test_0 = X_test * s0\n    y_b_pred_0 = Z_test_0 @ b0\n    mse_b_0 = np.mean((y_b_pred_0 - y_b_test)**2)\n\n    # --- Train and evaluate aligned model (lambda > 0) ---\n    s_lambda, a_lambda, b_lambda = train_mtl(X_s_train, y_a_train, y_b_train, X_t_unlabeled, lambda_align,\n                                             d, alpha_s, alpha_head, lr, n_iter)\n\n    Z_test_lambda = X_test * s_lambda\n    y_b_pred_lambda = Z_test_lambda @ b_lambda\n    mse_b_lambda = np.mean((y_b_pred_lambda - y_b_test)**2)\n\n    # --- Determine if harm occurred ---\n    harm_tolerance = 1e-3\n    is_harm = mse_b_lambda > mse_b_0 + harm_tolerance\n    \n    return is_harm\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3155049"}]}