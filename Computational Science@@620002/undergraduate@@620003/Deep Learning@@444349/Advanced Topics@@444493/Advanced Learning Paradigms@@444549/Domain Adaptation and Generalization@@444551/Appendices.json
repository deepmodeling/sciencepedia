{"hands_on_practices": [{"introduction": "Domain shift often causes performance degradation, but its effects can be subtle, occurring deep within a network's layers. A prime example is Batch Normalization (BN), whose effectiveness relies on running statistics (mean and variance) that are estimated from the training data. This first practice provides a clear, quantitative look at how a mismatch between source and target domain statistics directly impacts model performance [@problem_id:3117605]. By working through a simplified analytical model, you will isolate and measure the harm caused by this statistical shift and explore a simple mitigation strategy.", "problem": "You are asked to write a complete and runnable program that quantifies how using different Batch Normalization (BN) statistics at test time affects the population risk on a shifted target domain, and whether an input whitening transform mitigates the impact. Batch Normalization (BN) stands for Batch Normalization, which at inference applies the feature-wise affine transform $$y_i = \\gamma_i \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\varepsilon}} + \\beta_i,$$ where $x_i$ is the input feature, $\\mu_i$ and $\\sigma_i$ are the normalization mean and standard deviation, $\\gamma_i$ and $\\beta_i$ are learned scale and shift. The population risk on the target domain is defined as $$R_T(f) = \\mathbb{E}_{(x,y) \\sim T}\\left[\\ell(f(x),y)\\right],$$ with the $0$-$1$ loss $\\ell(\\hat{y},y)=\\mathbf{1}\\{\\hat{y}\\neq y\\}$.\n\nFundamental setting and assumptions:\n- The input $x \\in \\mathbb{R}^d$ is drawn from a multivariate normal distribution with independent coordinates, and only the first coordinate drives the label. Concretely, $$x_1 \\sim \\mathcal{N}(\\mu,\\sigma^2), \\quad x_j \\sim \\mathcal{N}(0,1) \\text{ for } j \\in \\{2,\\dots,d\\},$$ with independence across coordinates.\n- The true label is $$y = \\mathrm{sign}(x_1)$$ flipped independently with probability $p \\in [0,1)$ (label noise), so the clean label is $y_{\\mathrm{clean}}=\\mathrm{sign}(x_1)$ and the observed label is $y=y_{\\mathrm{clean}}$ with probability $1-p$ and $y=-y_{\\mathrm{clean}}$ with probability $p$.\n- The trained predictor is a one-layer model that first applies BN on each feature with parameters $(\\mu_{\\mathrm{used}},\\sigma_{\\mathrm{used}},\\gamma,\\beta)$ and then a unit-weight, zero-bias classifier on the first normalized coordinate, $$f(x) = \\mathrm{sign}\\!\\left(\\gamma \\frac{x_1 - \\mu_{\\mathrm{used}}}{\\sigma_{\\mathrm{used}}} + \\beta\\right).$$ We consider the special case $\\gamma=1$ and $\\beta=0$, so the classifier reduces to $$f(x)=\\mathrm{sign}\\!\\left(\\frac{x_1 - \\mu_{\\mathrm{used}}}{\\sigma_{\\mathrm{used}}}\\right) = \\mathrm{sign}(x_1 - \\mu_{\\mathrm{used}}),$$ where the sign is unaffected by any positive scaling. Thus, the effective decision threshold is $\\tau=\\mu_{\\mathrm{used}}$ on the original feature axis.\n\n- A source domain $S$ has parameters $(\\mu_S,\\sigma_S)$. A target domain $T$ has parameters $(\\mu_T,\\sigma_T)$. At test time on the target domain, we compare three inference pipelines:\n  1. Frozen BN: Use $(\\mu_{\\mathrm{used}},\\sigma_{\\mathrm{used}}) = (\\mu_S,\\sigma_S)$, yielding decision $f_{\\mathrm{frozen}}(x) = \\mathrm{sign}(x_1 - \\mu_S)$.\n  2. Updated BN on $T$: Use $(\\mu_{\\mathrm{used}},\\sigma_{\\mathrm{used}}) = (\\mu_T,\\sigma_T)$, yielding decision $f_{\\mathrm{updated}}(x) = \\mathrm{sign}(x_1 - \\mu_T)$.\n  3. Whitening then Frozen BN: Apply a per-feature affine map that aligns target first-coordinate moments to the source, $$x_1' = \\mu_S + \\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T),$$ and then feed $x'$ through Frozen BN with $(\\mu_S,\\sigma_S)$ and the same $(\\gamma,\\beta)=(1,0)$. This composition yields the same effective decision as Updated BN, because $$f_{\\mathrm{whiten}\\to\\mathrm{frozen}}(x) = \\mathrm{sign}\\!\\left(\\frac{x_1' - \\mu_S}{\\sigma_S}\\right) = \\mathrm{sign}\\!\\left(\\frac{x_1 - \\mu_T}{\\sigma_T}\\right) = \\mathrm{sign}(x_1 - \\mu_T).$$\n\nFrom the definitions above and the Gaussian model for $x_1$, the population risk with label noise $p$ and a threshold $\\tau$ on $x_1$ can be expressed as $$R_T(\\tau) = p + (1-2p)\\,\\mathbb{P}_{x_1 \\sim \\mathcal{N}(\\mu_T,\\sigma_T^2)}\\!\\left[\\mathrm{sign}(x_1)\\neq \\mathrm{sign}(x_1 - \\tau)\\right].$$ The disagreement event $\\{\\mathrm{sign}(x_1)\\neq \\mathrm{sign}(x_1 - \\tau)\\}$ occurs exactly when $x_1$ lies between $0$ and $\\tau$. Therefore, letting $\\Phi$ denote the standard normal cumulative distribution function, $$\\mathbb{P}\\!\\left[\\mathrm{sign}(x_1)\\neq \\mathrm{sign}(x_1 - \\tau)\\right] = \\left|\\Phi\\!\\left(\\frac{\\max\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right) - \\Phi\\!\\left(\\frac{\\min\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right)\\right|.$$\n\nYour program must:\n- Implement the exact computation of $R_T(f)$ for the three pipelines by plugging in $\\tau=\\mu_S$ for Frozen BN and $\\tau=\\mu_T$ for Updated BN and Whitening then Frozen BN, using the formula above and the Gaussian cumulative distribution function.\n- Use the test suite below and compute the three risks for each case, rounding each risk to $6$ decimal places.\n\nParameters held fixed across all tests:\n- Source statistics $(\\mu_S,\\sigma_S) = (\\,0.5,\\,1.0\\,)$.\n- Label noise $p = 0.1$.\n- Dimension $d = 5$ (only the first coordinate affects labels; other coordinates are nuisance and independent standard normal, which do not affect the calculation).\n\nTest suite (each item specifies $(\\mu_T,\\sigma_T)$ for the target domain):\n- Case A: $(\\,0.5,\\,1.0\\,)$.\n- Case B: $(\\,0.0,\\,1.0\\,)$.\n- Case C: $(\\,0.5,\\,0.5\\,)$.\n- Case D: $(\\,-1.0,\\,2.0\\,)$.\n\nFor each case, output a list of three floats in the order $[R_T(\\text{Frozen BN}), R_T(\\text{Updated BN}), R_T(\\text{Whiten}\\to\\text{Frozen})]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, one per test case, enclosed in square brackets. For example, the format must be like $[[r_{A,1},r_{A,2},r_{A,3}],[r_{B,1},r_{B,2},r_{B,3}],\\dots]$ with each $r_{\\cdot,\\cdot}$ a float rounded to $6$ decimal places.", "solution": "The problem requires the calculation of the population risk on a target domain for a simple classifier under three different test-time strategies for Batch Normalization (BN) parameters. The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution.\n\nThe population risk on the target domain $T$ for a predictor $f$ is given by $R_T(f) = \\mathbb{E}_{(x,y) \\sim T}\\left[\\ell(f(x),y)\\right]$, where the loss is the $0$-$1$ loss, $\\ell(\\hat{y},y)=\\mathbf{1}\\{\\hat{y}\\neq y\\}$. The data-generating process specifies that the input coordinate $x_1$ is drawn from a normal distribution $\\mathcal{N}(\\mu_T, \\sigma_T^2)$ on the target domain, and the true label is $y_{\\mathrm{clean}} = \\mathrm{sign}(x_1)$. The observed label $y$ is subject to symmetric label noise, flipped with probability $p$. The predictor, after simplifying the BN transformation for the special case of $\\gamma=1$ and $\\beta=0$, is $f(x) = \\mathrm{sign}(x_1 - \\tau)$, where $\\tau$ is an effective decision threshold.\n\nThe total risk can be decomposed based on the label noise. The probability of misclassification is:\n$$R_T(f) = \\mathbb{P}(f(x) \\neq y) = (1-p) \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}}) + p \\mathbb{P}(f(x) = y_{\\mathrm{clean}})$$\nSince $\\mathbb{P}(f(x) = y_{\\mathrm{clean}}) = 1 - \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}})$, we can rewrite the risk as:\n$$R_T(f) = (1-p) \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}}) + p (1 - \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}}))$$\n$$R_T(f) = p + (1-2p) \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}})$$\nThe disagreement event $f(x) \\neq y_{\\mathrm{clean}}$ corresponds to $\\mathrm{sign}(x_1-\\tau) \\neq \\mathrm{sign}(x_1)$. This occurs precisely when $x_1$ falls in the interval between $0$ and $\\tau$. The probability of this event, for $x_1 \\sim \\mathcal{N}(\\mu_T, \\sigma_T^2)$, is given by the integral of the Gaussian probability density function over this interval. This can be expressed using the cumulative distribution function (CDF) of the standard normal distribution, denoted by $\\Phi(\\cdot)$.\n$$\\mathbb{P}(\\mathrm{sign}(x_1) \\neq \\mathrm{sign}(x_1 - \\tau)) = \\mathbb{P}(\\min(0,\\tau) < x_1 < \\max(0,\\tau))$$\n$$= \\Phi\\left(\\frac{\\max(0,\\tau) - \\mu_T}{\\sigma_T}\\right) - \\Phi\\left(\\frac{\\min(0,\\tau) - \\mu_T}{\\sigma_T}\\right)$$\nSince probability must be non-negative, and the order of $\\max$ and $\\min$ depends on the sign of $\\tau$, the expression is written with an absolute value as provided:\n$$\\mathbb{P}(\\text{disagreement}) = \\left|\\Phi\\left(\\frac{\\max\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right) - \\Phi\\left(\\frac{\\min\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right)\\right|$$\nTherefore, the full expression for the population risk on the target domain with a decision threshold $\\tau$ is:\n$$R_T(\\tau) = p + (1-2p)\\left|\\Phi\\left(\\frac{\\max\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right) - \\Phi\\left(\\frac{\\min\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right)\\right|$$\nWe are given fixed parameters: source statistics $(\\mu_S, \\sigma_S) = (0.5, 1.0)$ and label noise probability $p=0.1$. The factor $(1-2p)$ is $1 - 2(0.1) = 0.8$.\n\nThe three inference pipelines correspond to different choices for the threshold $\\tau$:\n1.  **Frozen BN**: The BN statistics from the source domain are used, so $(\\mu_{\\mathrm{used}}, \\sigma_{\\mathrm{used}}) = (\\mu_S, \\sigma_S)$. The classifier becomes $f(x) = \\mathrm{sign}(x_1 - \\mu_S)$. The effective threshold is $\\tau_{\\mathrm{frozen}} = \\mu_S = 0.5$.\n2.  **Updated BN on T**: The BN statistics are re-computed on the target domain, so $(\\mu_{\\mathrm{used}}, \\sigma_{\\mathrm{used}}) = (\\mu_T, \\sigma_T)$. The classifier is $f(x) = \\mathrm{sign}(x_1 - \\mu_T)$. The effective threshold is $\\tau_{\\mathrm{updated}} = \\mu_T$.\n3.  **Whitening then Frozen BN**: An initial affine transformation $x_1' = \\mu_S + \\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T)$ is applied, followed by the frozen BN model. The decision function is $f(x) = \\mathrm{sign}\\left(\\frac{x_1' - \\mu_S}{\\sigma_S}\\right)$. Substituting the expression for $x_1'$ yields:\n$$ \\mathrm{sign}\\left(\\frac{\\left(\\mu_S + \\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T)\\right) - \\mu_S}{\\sigma_S}\\right) = \\mathrm{sign}\\left(\\frac{\\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T)}{\\sigma_S}\\right) = \\mathrm{sign}\\left(\\frac{x_1 - \\mu_T}{\\sigma_T}\\right) $$\nSince $\\sigma_T > 0$, this is equivalent to $\\mathrm{sign}(x_1 - \\mu_T)$. Thus, the effective threshold is $\\tau_{\\mathrm{whiten}} = \\mu_T$, which is identical to the Updated BN case. Consequently, their population risks will always be equal.\n\nWe now compute the risks for each test case.\n\n**Case A:** Target domain parameters $(\\mu_T, \\sigma_T) = (0.5, 1.0)$.\n- Frozen BN: $\\tau = 0.5$. Risk argument is $\\left|\\Phi\\left(\\frac{0.5 - 0.5}{1.0}\\right) - \\Phi\\left(\\frac{0 - 0.5}{1.0}\\right)\\right| = |\\Phi(0) - \\Phi(-0.5)| \\approx 0.191462$.\n$R_T = 0.1 + 0.8 \\times 0.191462 = 0.253170$.\n- Updated BN: $\\tau = \\mu_T = 0.5$. The calculation is identical to Frozen BN. $R_T = 0.253170$.\n- Whitening then Frozen BN: Risk is identical to Updated BN. $R_T = 0.253170$.\n\n**Case B:** Target domain parameters $(\\mu_T, \\sigma_T) = (0.0, 1.0)$.\n- Frozen BN: $\\tau = 0.5$. Risk argument is $\\left|\\Phi\\left(\\frac{0.5 - 0.0}{1.0}\\right) - \\Phi\\left(\\frac{0 - 0.0}{1.0}\\right)\\right| = |\\Phi(0.5) - \\Phi(0)| \\approx 0.191462$.\n$R_T = 0.1 + 0.8 \\times 0.191462 = 0.253170$.\n- Updated BN: $\\tau = \\mu_T = 0.0$. The disagreement interval is from $0$ to $0$, which has zero measure. The disagreement probability is $0$.\n$R_T = 0.1 + 0.8 \\times 0 = 0.100000$.\n- Whitening then Frozen BN: Risk is identical to Updated BN. $R_T = 0.100000$.\n\n**Case C:** Target domain parameters $(\\mu_T, \\sigma_T) = (0.5, 0.5)$.\n- Frozen BN: $\\tau = 0.5$. Risk argument is $\\left|\\Phi\\left(\\frac{0.5-0.5}{0.5}\\right) - \\Phi\\left(\\frac{0-0.5}{0.5}\\right)\\right| = |\\Phi(0) - \\Phi(-1.0)| \\approx 0.341345$.\n$R_T = 0.1 + 0.8 \\times 0.341345 = 0.373076$.\n- Updated BN: $\\tau = \\mu_T = 0.5$. The calculation is identical to Frozen BN. $R_T = 0.373076$.\n- Whitening then Frozen BN: Risk is identical to Updated BN. $R_T = 0.373076$.\n\n**Case D:** Target domain parameters $(\\mu_T, \\sigma_T) = (-1.0, 2.0)$.\n- Frozen BN: $\\tau = 0.5$. Risk argument is $\\left|\\Phi\\left(\\frac{0.5 - (-1.0)}{2.0}\\right) - \\Phi\\left(\\frac{0 - (-1.0)}{2.0}\\right)\\right| = |\\Phi(0.75) - \\Phi(0.5)| \\approx 0.081911$.\n$R_T = 0.1 + 0.8 \\times 0.081911 = 0.165529$.\n- Updated BN: $\\tau = \\mu_T = -1.0$. Risk argument is $\\left|\\Phi\\left(\\frac{0 - (-1.0)}{2.0}\\right) - \\Phi\\left(\\frac{-1.0 - (-1.0)}{2.0}\\right)\\right| = |\\Phi(0.5) - \\Phi(0)| \\approx 0.191462$.\n$R_T = 0.1 + 0.8 \\times 0.191462 = 0.253170$.\n- Whitening then Frozen BN: Risk is identical to Updated BN. $R_T = 0.253170$.\n\nThese calculations are implemented in the provided program.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes population risks for three Batch Normalization inference strategies\n    across a suite of test cases.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    # Source domain statistics\n    mu_S = 0.5\n    # sigma_S is not directly used in the simplified risk calculation,\n    # as it's only a positive scaling factor for the sign function's argument.\n    # sigma_S = 1.0\n\n    # Label noise probability\n    p = 0.1\n\n    # --- Test Cases ---\n    # Each case is a tuple (mu_T, sigma_T) for the target domain.\n    test_cases = [\n        (0.5, 1.0),   # Case A\n        (0.0, 1.0),   # Case B\n        (0.5, 0.5),   # Case C\n        (-1.0, 2.0)   # Case D\n    ]\n\n    def calculate_risk(tau, mu_T, sigma_T, p):\n        \"\"\"\n        Calculates the population risk R_T(tau).\n\n        Args:\n            tau (float): The decision threshold on the original feature x_1.\n            mu_T (float): The mean of x_1 in the target domain.\n            sigma_T (float): The standard deviation of x_1 in the target domain.\n            p (float): The label noise probability.\n\n        Returns:\n            float: The calculated population risk.\n        \"\"\"\n        # The disagreement event {sign(x_1) != sign(x_1 - tau)} occurs when x_1 is\n        # between 0 and tau.\n        # We calculate the probability of this event using the standard normal CDF.\n        z_arg1 = (max(0, tau) - mu_T) / sigma_T\n        z_arg2 = (min(0, tau) - mu_T) / sigma_T\n        \n        # The absolute value handles cases where tau  0.\n        disagreement_prob = abs(norm.cdf(z_arg1) - norm.cdf(z_arg2))\n        \n        # The risk is calculated based on the derived formula.\n        risk = p + (1 - 2 * p) * disagreement_prob\n        return risk\n\n    all_results = []\n    for mu_T, sigma_T in test_cases:\n        case_results = []\n\n        # 1. Frozen BN: uses source mean as threshold.\n        tau_frozen = mu_S\n        risk_frozen = calculate_risk(tau_frozen, mu_T, sigma_T, p)\n        case_results.append(risk_frozen)\n\n        # 2. Updated BN on T: uses target mean as threshold.\n        tau_updated = mu_T\n        risk_updated = calculate_risk(tau_updated, mu_T, sigma_T, p)\n        case_results.append(risk_updated)\n\n        # 3. Whitening then Frozen BN: effective threshold is also the target mean.\n        # Therefore, its risk is identical to Updated BN.\n        risk_whiten = risk_updated\n        case_results.append(risk_whiten)\n        \n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists, with each risk\n    # value formatted to 6 decimal places.\n    output_parts = []\n    for res_list in all_results:\n        formatted_nums = [f\"{r:.6f}\" for r in res_list]\n        output_parts.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3117605"}, {"introduction": "Having seen how domain shift can degrade performance, a natural next step is to develop methods that actively adapt to the target domain. This practice introduces two powerful and widely-used frameworks for aligning feature distributions: Maximum Mean Discrepancy (MMD) and Optimal Transport (OT) [@problem_id:3117509]. You will implement both techniques to transform source features to better match the target distribution, enabling the training of a classifier that generalizes more effectively. This hands-on comparison will illuminate the different principles behind these fundamental domain adaptation algorithms.", "problem": "You are given a binary classification domain adaptation task framed in terms of feature alignment between a source domain and a target domain using Optimal Transport (OT) and Maximum Mean Discrepancy (MMD). Both domains are represented by finite samples with labels in the source domain and labels in the target domain for evaluation only. The feature extractor is the identity map, so the features are identical to the input vectors. The goal is to implement entropically regularized OT with uniform marginals, construct a barycentric alignment from the resulting transport plan, fine-tune a classifier on aligned source features, and compare its target-domain accuracy to a classifier fine-tuned using an MMD-based alignment under a linear kernel. The final output should aggregate accuracy improvements of OT over MMD across multiple test cases.\n\nFundamental base and definitions to use:\n- The task is set in the context of domain adaptation. A source domain provides labeled samples, while a target domain provides unlabeled samples for alignment and labeled samples for evaluation.\n- The identity feature extractor is denoted by $f_{\\theta}(x) = x$. The parameter vector $\\theta$ is not explicitly optimized in this task.\n- The Optimal Transport alignment solves the following entropically regularized transportation problem: given source features $\\{z_{i}^{S}\\}_{i=1}^{n_{S}}$, target features $\\{z_{j}^{T}\\}_{j=1}^{n_{T}}$, uniform marginals $a \\in \\mathbb{R}^{n_{S}}$ and $b \\in \\mathbb{R}^{n_{T}}$ with $a_{i} = \\frac{1}{n_{S}}$ and $b_{j} = \\frac{1}{n_{T}}$, and the cost matrix $C \\in \\mathbb{R}^{n_{S} \\times n_{T}}$ defined by $C_{ij} = \\lVert z_{i}^{S} - z_{j}^{T} \\rVert_{2}^{2}$, find a transport plan $\\pi \\in \\mathbb{R}^{n_{S} \\times n_{T}}$ that minimizes the regularized objective subject to marginal constraints:\n$$\n\\min_{\\pi} \\sum_{i=1}^{n_{S}} \\sum_{j=1}^{n_{T}} \\pi_{ij} \\lVert z_{i}^{S} - z_{j}^{T} \\rVert_{2}^{2} + \\varepsilon \\sum_{i=1}^{n_{S}} \\sum_{j=1}^{n_{T}} \\pi_{ij} \\log \\pi_{ij}\n$$\nsubject to $\\sum_{j=1}^{n_{T}} \\pi_{ij} = a_{i}$ and $\\sum_{i=1}^{n_{S}} \\pi_{ij} = b_{j}$. The entropic regularization parameter is $\\varepsilon  0$.\n- Use the Sinkhorn-Knopp iterative scaling method (no explicit formulas are provided here; you must derive and implement the procedure) to solve the entropically regularized problem and obtain the transport plan $\\pi$ that satisfies the marginal constraints.\n- Construct a barycentric alignment of the source features with respect to the target features using the transport plan. Specifically, produce aligned source features $\\{z_{i}^{S,\\mathrm{OT}}\\}_{i=1}^{n_{S}}$ by forming the barycentric combinations of target features weighted by the transport plan and the source marginal.\n- Fine-tune a binary classifier on the aligned source features using the source labels. The classifier must be implemented as logistic regression trained via gradient descent on the cross-entropy loss.\n- Maximum Mean Discrepancy (MMD) is a discrepancy measure between distributions based on kernel mean embeddings. Use the linear kernel $k(x,y) = x^{\\top}y$. Under this kernel, derive and implement the alignment transformation on source features that minimizes the empirical MMD between aligned source features and target features. Fine-tune the same logistic regression classifier on the MMD-aligned source features using the source labels.\n- Evaluate both classifiers on the original target features using the true target labels. Compute the accuracy for the OT-aligned classifier and the MMD-aligned classifier. Report, for each test case, the accuracy improvement computed as the scalar $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$, where $A_{\\mathrm{OT}}$ and $A_{\\mathrm{MMD}}$ are the target-domain accuracies achieved by the respective methods.\n\nData generation protocol (all angles are specified in radians):\n- Generate source data for two classes with equal sample sizes (binary labels). Let $d = 2$. The class-conditional source means are $\\mu_{0}^{S} = [-2, 0]$ and $\\mu_{1}^{S} = [2, 0]$. For source samples, draw independent Gaussian noise with zero mean and covariance $\\sigma_{S}^{2} I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix. The source labels are known.\n- Generate target data by applying a rotation and translation to the source class means to produce target class means. Let the rotation angle be $\\alpha$ and the translation vector be $t = [t_{x}, t_{y}]$. The target class means are $\\mu_{c}^{T} = R(\\alpha) \\mu_{c}^{S} + t$, for $c \\in \\{0,1\\}$, where $R(\\alpha)$ is the $2 \\times 2$ rotation matrix for angle $\\alpha$ in radians. Draw independent Gaussian noise with zero mean and covariance $\\sigma_{T}^{2} I_{2}$ around each target class mean. The target labels are used only for evaluation.\n\nImplementation requirements:\n- Use $f_{\\theta}(x) = x$.\n- Use uniform marginals $a_{i} = \\frac{1}{n_{S}}$ and $b_{j} = \\frac{1}{n_{T}}$.\n- Use the Sinkhorn-Knopp method to solve the entropically regularized OT problem for the given $\\varepsilon$.\n- Construct the barycentric alignment of the source features using the transport plan and the target features.\n- Train logistic regression via gradient descent with a fixed number of epochs and learning rate as specified in the test suite. Use cross-entropy loss and predict by thresholding the logistic function at $0.5$.\n- For the MMD-based alignment with a linear kernel, derive the alignment that minimizes empirical MMD between aligned source features and target features. Implement this alignment and then fine-tune logistic regression on the aligned source features.\n- Evaluate accuracies on the target domain and compute the accuracy improvement $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$ for each case.\n\nTest suite:\n- All random sampling must use the specified seed for reproducibility. Use independent seeds per test case: seed $= 10 + \\text{case\\_index}$, where case index is $0,1,2,3$ for the four cases.\n- Provide four test cases with parameters $(n_{S}, n_{T}, \\sigma_{S}, \\sigma_{T}, \\alpha, t_{x}, t_{y}, \\varepsilon, \\text{epochs}, \\text{learning\\_rate})$:\n    1. Case $0$ (happy path): $(n_{S}, n_{T}) = (60, 60)$, $\\sigma_{S} = 0.4$, $\\sigma_{T} = 0.4$, $\\alpha = 0.5$, $t_{x} = 1.0$, $t_{y} = 0.5$, $\\varepsilon = 0.2$, epochs $= 200$, learning rate $= 0.1$.\n    2. Case $1$ (no shift boundary condition): $(n_{S}, n_{T}) = (60, 60)$, $\\sigma_{S} = 0.4$, $\\sigma_{T} = 0.4$, $\\alpha = 0.0$, $t_{x} = 0.0$, $t_{y} = 0.0$, $\\varepsilon = 0.2$, epochs $= 200$, learning rate $= 0.1$.\n    3. Case $2$ (hard shift): $(n_{S}, n_{T}) = (80, 80)$, $\\sigma_{S} = 0.4$, $\\sigma_{T} = 0.6$, $\\alpha = 1.0$, $t_{x} = 2.0$, $t_{y} = -1.0$, $\\varepsilon = 0.3$, epochs $= 250$, learning rate $= 0.08$.\n    4. Case $3$ (small sample size edge case): $(n_{S}, n_{T}) = (12, 12)$, $\\sigma_{S} = 0.5$, $\\sigma_{T} = 0.5$, $\\alpha = 0.8$, $t_{x} = 1.5$, $t_{y} = 1.5$, $\\varepsilon = 0.25$, epochs $= 300$, learning rate $= 0.12$.\n\nAnswer type and final output format:\n- For each test case, compute the float $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$, where $A_{\\mathrm{OT}}$ and $A_{\\mathrm{MMD}}$ are target-domain accuracies expressed as fractions in $[0,1]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float formatted to four decimal places in the order of the test cases (for example, $[0.1234,0.0000,-0.0567,0.0321]$).", "solution": "The problem requires a comparative analysis of two domain adaptation techniques, one based on Optimal Transport (OT) and the other on Maximum Mean Discrepancy (MMD), for a binary classification task. The objective is to align a labeled source domain distribution with an unlabeled target domain distribution in a feature space, which, in this case, is the input space itself as the feature extractor is the identity map, $f_{\\theta}(x) = x$. We will implement both alignment methods, train a logistic regression classifier on the aligned source data, evaluate its performance on the target domain, and compute the accuracy improvement of the OT-based method over the MMD-based method.\n\nFirst, we define the data generation process. The source data consists of $n_S$ samples from two balanced classes in a $d=2$ dimensional space. The class-conditional distributions are Gaussian. The means are $\\mu_{0}^{S} = [-2, 0]^{\\top}$ and $\\mu_{1}^{S} = [2, 0]^{\\top}$. Samples are generated as $z_i^S \\sim \\mathcal{N}(\\mu_{y_i^S}^S, \\sigma_S^2 I_2)$, where $y_i^S \\in \\{0, 1\\}$ are the source labels and $I_2$ is the $2 \\times 2$ identity matrix. The target domain data consists of $n_T$ samples generated by applying a rotation and a translation to the source class means. The target class means are $\\mu_{c}^{T} = R(\\alpha) \\mu_{c}^{S} + t$ for $c \\in \\{0,1\\}$, where $R(\\alpha)$ is the rotation matrix for an angle $\\alpha$, and $t = [t_x, t_y]^\\top$ is a translation vector. Target samples are generated as $z_j^T \\sim \\mathcal{N}(\\mu_{y_j^T}^T, \\sigma_T^2 I_2)$, where $y_j^T$ are the target labels, used only for evaluation.\n\nThe core of the first alignment method is Entropically Regularized Optimal Transport. We aim to find a transport plan $\\pi \\in \\mathbb{R}^{n_S \\times n_T}$ that minimizes the regularized transport cost. The cost between a source feature $z_i^S$ and a target feature $z_j^T$ is the squared Euclidean distance, $C_{ij} = \\lVert z_i^S - z_j^T \\rVert_2^2$. The optimization problem is:\n$$\n\\min_{\\pi} \\sum_{i=1}^{n_{S}} \\sum_{j=1}^{n_{T}} \\pi_{ij} C_{ij} - \\varepsilon H(\\pi)\n$$\nwhere $H(\\pi) = -\\sum_{ij} \\pi_{ij} \\log \\pi_{ij}$ is the entropy of the plan, and $\\varepsilon > 0$ is the regularization strength. The minimization is subject to marginal constraints $\\pi \\mathbf{1}_{n_T} = a$ and $\\pi^\\top \\mathbf{1}_{n_S} = b$, where $\\mathbf{1}_k$ is a vector of ones of size $k$. The marginals are uniform, with $a_i = 1/n_S$ and $b_j = 1/n_T$.\n\nThis problem is solved efficiently using the Sinkhorn-Knopp algorithm. The solution $\\pi$ can be expressed as $\\pi = \\text{diag}(u) K \\text{diag}(v)$, where $K_{ij} = e^{-C_{ij}/\\varepsilon}$ and $u, v$ are scaling vectors. The algorithm iteratively updates $u$ and $v$ to satisfy the marginal constraints:\n$$\nu \\leftarrow a \\oslash (K v) \\quad \\text{and} \\quad v \\leftarrow b \\oslash (K^\\top u)\n$$\nwhere $\\oslash$ denotes element-wise division. Starting with an initial guess for $v$ (e.g., $v = \\mathbf{1}_{n_T}$), these updates are repeated until convergence.\n\nOnce the optimal transport plan $\\pi$ is found, we perform a barycentric alignment of the source features. The aligned source feature $z_i^{S,\\mathrm{OT}}$ is the expectation of the target features to which $z_i^S$ is mapped, weighted by the conditional transport probabilities. The conditional probability that $z_i^S$ maps to $z_j^T$ is $p(j|i) = \\pi_{ij} / a_i = n_S \\pi_{ij}$. Thus, the aligned feature is:\n$$\nz_i^{S,\\mathrm{OT}} = \\sum_{j=1}^{n_T} p(j|i) z_j^T = n_S \\sum_{j=1}^{n_T} \\pi_{ij} z_j^T\n$$\nIn matrix notation, this is $Z_{S,\\mathrm{OT}} = (n_S \\pi) Z_T$, where $Z_T$ is the matrix of target features.\n\nThe second alignment method is based on minimizing the Maximum Mean Discrepancy (MMD) with a linear kernel $k(x,y) = x^\\top y$. The squared empirical MMD between a set of transformed source features $\\{\\mathcal{T}(z_i^S)\\}_{i=1}^{n_S}$ and target features $\\{z_j^T\\}_{j=1}^{n_T}$ is:\n$$\n\\text{MMD}^2 = \\left\\lVert \\frac{1}{n_S} \\sum_{i=1}^{n_S} \\phi(\\mathcal{T}(z_i^S)) - \\frac{1}{n_T} \\sum_{j=1}^{n_T} \\phi(z_j^T) \\right\\rVert_{\\mathcal{H}}^2\n$$\nFor the linear kernel, the feature map $\\phi$ is the identity, $\\phi(x) = x$. We choose a simple alignment transformation class: a global translation, $\\mathcal{T}(z) = z + v$. The objective is to find the vector $v$ that minimizes:\n$$\n\\min_v \\left\\lVert \\frac{1}{n_S} \\sum_{i=1}^{n_S} (z_i^S + v) - \\frac{1}{n_T} \\sum_{j=1}^{n_T} z_j^T \\right\\rVert_2^2 = \\min_v \\left\\lVert (\\bar{z}^S + v) - \\bar{z}^T \\right\\rVert_2^2\n$$\nwhere $\\bar{z}^S$ and $\\bar{z}^T$ are the empirical means of the source and target features, respectively. This expression is minimized when $v = \\bar{z}^T - \\bar{z}^S$. The MMD-based alignment is therefore a mean-matching transformation:\n$$\nz_i^{S,\\mathrm{MMD}} = z_i^S + (\\bar{z}^T - \\bar{z}^S)\n$$\nThis aligns the centroid of the source distribution with the centroid of the target distribution.\n\nAfter aligning the source features using both OT ($Z_{S,\\mathrm{OT}}$) and MMD ($Z_{S,\\mathrm{MMD}}$), we train two separate binary logistic regression classifiers. The model predicts the probability of class $1$ as $\\hat{y} = \\sigma(z^\\top w + b)$, where $\\sigma(s) = 1/(1+e^{-s})$ is the sigmoid function, $w \\in \\mathbb{R}^2$ are the weights, and $b \\in \\mathbb{R}$ is the bias. The classifiers are trained by minimizing the binary cross-entropy loss function using gradient descent for a fixed number of epochs and a given learning rate. The gradient of the loss $L$ for a dataset of size $N$ with features $X$ and labels $Y$ is:\n$$\n\\nabla_w L = \\frac{1}{N} X^\\top (\\hat{Y} - Y), \\quad \\nabla_b L = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n$$\nThe weights and bias are updated as $w \\leftarrow w - \\eta \\nabla_w L$ and $b \\leftarrow b - \\eta \\nabla_b L$, where $\\eta$ is the learning rate.\n\nFinally, we evaluate the performance of both classifiers. The classifier trained on $Z_{S,\\mathrm{OT}}$ and the classifier trained on $Z_{S,\\mathrm{MMD}}$ are both tested on the original, unaligned target features $Z_T$ with their true labels $Y_T$. A prediction is classified as $1$ if the output probability is greater than $0.5$, and $0$ otherwise. The accuracy is the fraction of correctly classified target samples. The final result for each test case is the difference in accuracies: $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_data(n_s, n_t, sigma_s, sigma_t, alpha, t_x, t_y, seed):\n    \"\"\"Generates source and target domain data.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Source domain data\n    mu_s0 = np.array([-2.0, 0.0])\n    mu_s1 = np.array([2.0, 0.0])\n    \n    n_s_half = n_s // 2\n    z_s0 = rng.multivariate_normal(mu_s0, sigma_s**2 * np.eye(2), size=n_s_half)\n    z_s1 = rng.multivariate_normal(mu_s1, sigma_s**2 * np.eye(2), size=n_s - n_s_half)\n    \n    Z_s = np.vstack((z_s0, z_s1))\n    Y_s = np.hstack((np.zeros(n_s_half), np.ones(n_s - n_s_half)))\n\n    # Target domain data\n    R = np.array([[np.cos(alpha), -np.sin(alpha)], [np.sin(alpha), np.cos(alpha)]])\n    t = np.array([t_x, t_y])\n    \n    mu_t0 = R @ mu_s0 + t\n    mu_t1 = R @ mu_s1 + t\n    \n    n_t_half = n_t // 2\n    z_t0 = rng.multivariate_normal(mu_t0, sigma_t**2 * np.eye(2), size=n_t_half)\n    z_t1 = rng.multivariate_normal(mu_t1, sigma_t**2 * np.eye(2), size=n_t - n_t_half)\n    \n    Z_t = np.vstack((z_t0, z_t1))\n    Y_t = np.hstack((np.zeros(n_t_half), np.ones(n_t - n_t_half)))\n    \n    return Z_s, Y_s, Z_t, Y_t\n\ndef sinkhorn_knopp(C, epsilon, n_s, n_t, num_iter=100):\n    \"\"\"Solves entropically regularized OT using Sinkhorn-Knopp.\"\"\"\n    a = np.full(n_s, 1.0 / n_s)\n    b = np.full(n_t, 1.0 / n_t)\n    \n    K = np.exp(-C / epsilon)\n    v = np.ones(n_t)\n    \n    for _ in range(num_iter):\n        u = a / (K @ v)\n        v = b / (K.T @ u)\n        \n    pi = np.diag(u) @ K @ np.diag(v)\n    return pi\n\ndef ot_align(Z_s, Z_t, epsilon):\n    \"\"\"Aligns source features to target features using Optimal Transport.\"\"\"\n    n_s = Z_s.shape[0]\n    n_t = Z_t.shape[0]\n    \n    C = cdist(Z_s, Z_t, 'sqeuclidean')\n    pi = sinkhorn_knopp(C, epsilon, n_s, n_t)\n    \n    Z_s_ot = (n_s * pi) @ Z_t\n    return Z_s_ot\n\ndef mmd_align(Z_s, Z_t):\n    \"\"\"Aligns source features to target features by matching means (linear MMD).\"\"\"\n    mean_s = np.mean(Z_s, axis=0)\n    mean_t = np.mean(Z_t, axis=0)\n    \n    translation = mean_t - mean_s\n    Z_s_mmd = Z_s + translation\n    return Z_s_mmd\n\ndef train_logistic_regression(X, y, epochs, lr):\n    \"\"\"Trains a logistic regression classifier using gradient descent.\"\"\"\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    y = y.reshape(-1, 1)\n\n    for _ in range(epochs):\n        linear_model = (X @ w + b).reshape(-1, 1)\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        \n        dw = (1 / n_samples) * (X.T @ (y_pred - y))\n        db = (1 / n_samples) * np.sum(y_pred - y)\n        \n        w -= lr * dw.flatten()\n        b -= lr * db\n\n    return w, b\n\ndef predict_and_evaluate(X, y, w, b):\n    \"\"\"Makes predictions and computes accuracy.\"\"\"\n    linear_model = (X @ w + b).reshape(-1, 1)\n    y_pred_prob = 1 / (1 + np.exp(-linear_model))\n    y_pred_class = (y_pred_prob > 0.5).astype(int)\n    \n    accuracy = np.mean(y_pred_class.flatten() == y.flatten())\n    return accuracy\n\ndef solve():\n    test_cases = [\n        (60, 60, 0.4, 0.4, 0.5, 1.0, 0.5, 0.2, 200, 0.1),\n        (60, 60, 0.4, 0.4, 0.0, 0.0, 0.0, 0.2, 200, 0.1),\n        (80, 80, 0.4, 0.6, 1.0, 2.0, -1.0, 0.3, 250, 0.08),\n        (12, 12, 0.5, 0.5, 0.8, 1.5, 1.5, 0.25, 300, 0.12),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        n_s, n_t, sigma_s, sigma_t, alpha, t_x, t_y, epsilon, epochs, lr = params\n        seed = 10 + i\n        \n        Z_s, Y_s, Z_t, Y_t = generate_data(n_s, n_t, sigma_s, sigma_t, alpha, t_x, t_y, seed)\n        \n        # OT path\n        Z_s_ot = ot_align(Z_s, Z_t, epsilon)\n        w_ot, b_ot = train_logistic_regression(Z_s_ot, Y_s, epochs, lr)\n        acc_ot = predict_and_evaluate(Z_t, Y_t, w_ot, b_ot)\n        \n        # MMD path\n        Z_s_mmd = mmd_align(Z_s, Z_t)\n        w_mmd, b_mmd = train_logistic_regression(Z_s_mmd, Y_s, epochs, lr)\n        acc_mmd = predict_and_evaluate(Z_t, Y_t, w_mmd, b_mmd)\n        \n        accuracy_improvement = acc_ot - acc_mmd\n        results.append(accuracy_improvement)\n\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "3117509"}, {"introduction": "In many real-world applications, the target domain contains not only shifted versions of the source data but also completely novel or irrelevant inputs, known as out-of-distribution (OOD) samples. Forcing a model to make predictions on such inputs can lead to unpredictable and unreliable behavior. This final practice addresses this challenge by guiding you to build a selective classifier that can say 'I don't know' [@problem_id:3117588]. You will use Kernel Density Estimation (KDE) to identify OOD inputs and abstain from classifying them, a crucial skill for developing safe and trustworthy machine learning systems.", "problem": "Consider a binary classification problem under domain adaptation and generalization, with a source domain $S$ and a target domain $T$. The input space is $\\mathbb{R}^2$, and the label space is $\\{0,1\\}$. The source domain $S$ generates independently and identically distributed (i.i.d.) pairs $(x,y)$ with the following structure. The marginal distribution over $x$ is uniform over the union of two axis-aligned rectangles, and the conditional label distribution assigns one rectangle to label $0$ and the other to label $1$. Specifically, define the rectangles\n$$R_0 = \\{x \\in \\mathbb{R}^2 : x_1 \\in [-3,-1],\\ x_2 \\in [-1,1]\\},$$\n$$R_1 = \\{x \\in \\mathbb{R}^2 : x_1 \\in [1,3],\\ x_2 \\in [-1,1]\\},$$\nand the source marginal support $\\operatorname{supp}(P_S^X) = R_0 \\cup R_1$. A simple base classifier $g$ uses the sign of the first coordinate, defined by $g(x) = \\mathbb{I}\\{x_1  0\\}$, where $\\mathbb{I}\\{\\cdot\\}$ denotes the indicator function.\n\nThe target domain $T$ introduces an out-of-support shift that includes inputs outside $\\operatorname{supp}(P_S^X)$. Concretely, let an out-of-distribution (OOD) rectangle\n$$R_o = \\{x \\in \\mathbb{R}^2 : x_1 \\in [-1,1],\\ x_2 \\in [3,5]\\}$$\ngenerate OOD inputs without labels. The target marginal $P_T^X$ is a mixture that draws $x$ i.i.d. by first choosing with probability $\\pi_o$ to sample uniformly from $R_o$ (OOD) and with probability $1-\\pi_o$ to sample uniformly from $R_0 \\cup R_1$ (in-support). If $x$ is drawn from $R_0$ then $y=0$, and if $x$ is drawn from $R_1$ then $y=1$.\n\nYou must implement an abstaining selective classifier for $T$ that detects OOD points and abstains on them while maintaining high precision on in-support target examples. The abstention mechanism should be based on a nonparametric estimate of the source marginal density $p_S^X$. Use a smooth isotropic Gaussian window function with bandwidth $h0$ to construct a kernel density estimator (KDE) $\\hat{p}_S(x)$ from source samples. Define a detection threshold $\\tau0$ and abstain on any $x$ with $\\hat{p}_S(x)  \\tau$. Predictions are only made on non-abstained points using the base classifier $g(x)$.\n\nStarting from core definitions of probability distributions, support, and i.i.d. sampling, implement the full pipeline:\n- Sample $n_S$ source points from $S$ and $n_T$ target points from $T$ according to the specified rectangles and mixture.\n- Estimate $\\hat{p}_S(x)$ using the isotropic Gaussian window with bandwidth $h$ and the source samples.\n- Apply the threshold-based abstention rule using $\\tau$ and classify with $g(x)$ when not abstaining.\n- Compute the following metrics for the target set:\n    1. In-support precision: among target points $x \\in \\operatorname{supp}(P_S^X)$ on which the method did not abstain, the fraction of correct predictions $\\in [0,1]$ expressed as a float. If the denominator is $0$, return $0$.\n    2. OOD abstention rate: among target points $x \\in R_o$, the fraction that were abstained $\\in [0,1]$ expressed as a float. If there are no OOD points, return $0$.\n    3. Meets criteria: a boolean indicating whether both the in-support precision is at least $0.95$ and the OOD abstention rate is at least $0.9$.\n\nYour program must be a complete, runnable implementation that uses the following test suite of parameter values, designed to test a happy path, a boundary condition with too-loose detection, and a boundary condition with too-strict detection:\n- Test case $1$: seed $0$, $n_S=400$, $n_T=300$, $h=0.3$, $\\tau=0.015$, $\\pi_o=0.3$.\n- Test case $2$: seed $1$, $n_S=400$, $n_T=300$, $h=0.3$, $\\tau=0.0$, $\\pi_o=0.3$.\n- Test case $3$: seed $2$, $n_S=400$, $n_T=300$, $h=0.3$, $\\tau=0.05$, $\\pi_o=0.3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case is represented by a sub-list $[p,a,b]$ with no spaces, $p$ and $a$ rounded to $4$ decimal places, and $b$ being either $True$ or $False$. For example, the final output format should be like $[[0.9732,0.9458,True],[0.8123,0.1021,False],[0.0000,1.0000,False]]$. No physical units, angles, or percentage signs are involved in this problem; all numeric answers are dimensionless floats or booleans as specified.", "solution": "This problem addresses out-of-distribution (OOD) detection using a density-based approach. The core principle is that inputs belonging to the source distribution's support should have a high density under a model of that distribution, while OOD inputs, which lie far from this support, will have a low density. We will implement a selective classifier that models the source data density using Kernel Density Estimation (KDE) and abstains from making predictions on target points that fall below a certain density threshold. This approach allows the classifier to identify and reject novel inputs it was not trained to handle, which is a key component of building robust and safe machine learning systems.\n\nThe solution follows these steps:\n1.  **Data Generation**: First, we simulate data for both the source and target domains. The source data consists of $n_S$ points sampled uniformly from the union of two rectangles, $R_0$ and $R_1$, which define the support of the source distribution. The target data consists of $n_T$ points drawn from a mixture distribution: with probability $\\pi_o$, a point is sampled from an OOD rectangle $R_o$; otherwise, it is sampled from the same in-support distribution as the source data.\n\n2.  **Kernel Density Estimation**: We estimate the source marginal probability density function, $p_S^X(x)$, using the $n_S$ source samples. The problem specifies a non-parametric approach using a Gaussian KDE. The estimated density at a point $x$ is given by:\n    $$ \\hat{p}_S(x) = \\frac{1}{n_S} \\sum_{i=1}^{n_S} K_h(x - x_{S,i}) $$\n    where $\\{x_{S,i}\\}$ are the source samples and $K_h$ is the scaled kernel. For an isotropic Gaussian kernel in $d=2$ dimensions with bandwidth $h$, the kernel function is:\n    $$ K_h(u) = \\frac{1}{2\\pi h^2} \\exp\\left(-\\frac{\\|u\\|^2}{2h^2}\\right) $$\n    We compute this density estimate $\\hat{p}_S(x_T)$ for every point $x_T$ in the target dataset.\n\n3.  **Selective Classification**: The selective classifier uses the density estimates to decide whether to predict or abstain. For each target point $x_T$:\n    - If $\\hat{p}_S(x_T)  \\tau$, where $\\tau$ is the given density threshold, the classifier abstains.\n    - Otherwise, it makes a prediction using the provided base classifier, $g(x) = \\mathbb{I}\\{x_1 > 0\\}$.\n\n4.  **Metric Calculation**: We evaluate the performance on the target set.\n    - **OOD Abstention Rate**: This is the fraction of target points known to be from the OOD region $R_o$ on which the classifier correctly abstained.\n    - **In-support Precision**: This is the precision on the subset of target points known to be from the in-support regions $R_0 \\cup R_1$ that were *not* abstained upon. A crucial observation is that the base classifier $g(x)$ is perfect for any in-support data point (for $x \\in R_0$, $x_1  0$, so $g(x)=0$; for $x \\in R_1$, $x_1 > 0$, so $g(x)=1$). Therefore, any in-support prediction is guaranteed to be correct. The precision will be $1.0$ as long as at least one in-support point is classified, and $0.0$ otherwise, per the problem specification.\n    - **Meets Criteria**: A boolean check to see if the in-support precision is at least $0.95$ and the OOD abstention rate is at least $0.9$.\n\nThis entire process is implemented for each test case to produce the required metrics.", "answer": "```python\nimport numpy as np\n\n# This script implements a selective classifier for out-of-distribution detection.\n# It adheres to the Python 3.12 environment with numpy 1.23.5.\n# No other libraries like scipy are used, ensuring a first-principles implementation.\n\n# Define rectangle boundaries as constants for clarity and correctness.\nR0_X_RANGE = [-3.0, -1.0]\nR0_Y_RANGE = [-1.0, 1.0]\nR1_X_RANGE = [1.0, 3.0]\nR1_Y_RANGE = [-1.0, 1.0]\nRO_X_RANGE = [-1.0, 1.0]\nRO_Y_RANGE = [3.0, 5.0]\n\ndef sample_from_rectangle(n_samples: int, x_range: list, y_range: list) - np.ndarray:\n    \"\"\"Samples n_samples uniformly from a given axis-aligned rectangle.\"\"\"\n    if n_samples == 0:\n        return np.empty((0, 2))\n    x1 = np.random.uniform(x_range[0], x_range[1], n_samples)\n    x2 = np.random.uniform(y_range[0], y_range[1], n_samples)\n    return np.stack((x1, x2), axis=1)\n\ndef generate_source_data(n_S: int) - tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generates n_S source data points from the uniform distribution over R0 U R1.\n    Since Area(R0) = Area(R1), we sample from each with 0.5 probability.\n    \"\"\"\n    from_R0_mask = np.random.rand(n_S)  0.5\n    n_R0 = np.sum(from_R0_mask)\n    n_R1 = n_S - n_R0\n\n    points_R0 = sample_from_rectangle(n_R0, R0_X_RANGE, R0_Y_RANGE)\n    labels_R0 = np.zeros(n_R0, dtype=int)\n\n    points_R1 = sample_from_rectangle(n_R1, R1_X_RANGE, R1_Y_RANGE)\n    labels_R1 = np.ones(n_R1, dtype=int)\n\n    source_points = np.concatenate((points_R0, points_R1), axis=0)\n    source_labels = np.concatenate((labels_R0, labels_R1), axis=0)\n\n    # Shuffle to ensure the returned data is i.i.d.\n    p = np.random.permutation(n_S)\n    return source_points[p], source_labels[p]\n\ndef generate_target_data(n_T: int, pi_o: float) - tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generates n_T target data points from the specified mixture distribution.\"\"\"\n    is_ood_mask = np.random.rand(n_T)  pi_o\n    n_ood = np.sum(is_ood_mask)\n    n_insupport = n_T - n_ood\n\n    points_ood = sample_from_rectangle(n_ood, RO_X_RANGE, RO_Y_RANGE)\n    points_insupport, labels_insupport = generate_source_data(n_insupport)\n    \n    target_points = np.zeros((n_T, 2))\n    target_labels = np.full(n_T, -1, dtype=int)  # -1 for OOD points\n    \n    target_points[is_ood_mask] = points_ood\n    target_points[~is_ood_mask] = points_insupport\n    target_labels[~is_ood_mask] = labels_insupport\n\n    return target_points, target_labels, is_ood_mask\n\ndef kde_gauss(x_eval: np.ndarray, x_source: np.ndarray, h: float) - np.ndarray:\n    \"\"\"\n    Computes Gaussian Kernel Density Estimates for evaluation points.\n    Formula for 2D: p_hat(x) = (1 / (n_S * 2*pi*h^2)) * sum_i exp(-||x-x_i||^2 / (2h^2))\n    \"\"\"\n    n_S = x_source.shape[0]\n    \n    # Efficiently compute squared Euclidean distances using broadcasting\n    # dist_sq[j, i] = ||x_eval_j - x_source_i||^2\n    dist_sq = np.sum(x_eval**2, axis=1)[:, np.newaxis] + np.sum(x_source**2, axis=1) - 2 * np.dot(x_eval, x_source.T)\n    \n    # Apply the exponential part of the Gaussian kernel\n    kernel_vals = np.exp(-dist_sq / (2 * h**2))\n    \n    # Final normalization and averaging over source points\n    norm_const = 1 / (2 * np.pi * h**2)\n    density_estimates = norm_const * np.mean(kernel_vals, axis=1)\n    \n    return density_estimates\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {'seed': 0, 'n_S': 400, 'n_T': 300, 'h': 0.3, 'tau': 0.015, 'pi_o': 0.3},\n        {'seed': 1, 'n_S': 400, 'n_T': 300, 'h': 0.3, 'tau': 0.0, 'pi_o': 0.3},\n        {'seed': 2, 'n_S': 400, 'n_T': 300, 'h': 0.3, 'tau': 0.05, 'pi_o': 0.3},\n    ]\n\n    results = []\n    for params in test_cases:\n        np.random.seed(params['seed'])\n        \n        # 1. Generate data\n        source_points, _ = generate_source_data(params['n_S'])\n        target_points, _, target_is_ood_mask = generate_target_data(\n            params['n_T'], params['pi_o']\n        )\n        \n        # 2. Estimate density for target points\n        density_estimates = kde_gauss(target_points, source_points, params['h'])\n\n        # 3. Apply abstention rule\n        abstained_mask = density_estimates  params['tau']\n\n        # 4. Compute metrics\n        \n        # OOD abstention rate\n        ood_indices = np.where(target_is_ood_mask)[0]\n        if len(ood_indices) == 0:\n            ood_abstention_rate = 0.0\n        else:\n            num_abstained_ood = np.sum(abstained_mask[ood_indices])\n            ood_abstention_rate = num_abstained_ood / len(ood_indices)\n            \n        # In-support precision\n        insupport_indices = np.where(~target_is_ood_mask)[0]\n        non_abstained_insupport_mask = ~abstained_mask[insupport_indices]\n        num_non_abstained_insupport = np.sum(non_abstained_insupport_mask)\n        \n        if num_non_abstained_insupport == 0:\n            insupport_precision = 0.0\n        else:\n            # The base classifier g(x) = I{x_1 > 0} is perfect for in-support data.\n            # Thus, any non-abstained in-support point is classified correctly.\n            insupport_precision = 1.0\n\n        # Meets criteria\n        meets_criteria = (insupport_precision >= 0.95) and (ood_abstention_rate >= 0.9)\n        \n        results.append([insupport_precision, ood_abstention_rate, meets_criteria])\n\n    # Format output string exactly as specified in the problem statement\n    result_strings = []\n    for p, a, b in results:\n        # Python's default str(bool) is 'True' or 'False' which is the required format.\n        result_strings.append(f\"[{p:.4f},{a:.4f},{b}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3117588"}]}