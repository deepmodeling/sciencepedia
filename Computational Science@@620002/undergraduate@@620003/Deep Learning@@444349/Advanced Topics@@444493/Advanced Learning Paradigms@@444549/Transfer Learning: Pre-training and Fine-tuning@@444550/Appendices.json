{"hands_on_practices": [{"introduction": "A successful fine-tuning process requires a delicate balance between adapting to a new task and preserving the powerful knowledge learned during pre-training. One of the key challenges is preventing \"catastrophic forgetting,\" where the model's parameters drift too far from their effective starting point. This exercise explores L2 Starting Point (L2-SP) regularization, a technique that adds a penalty term to the loss function, explicitly encouraging the fine-tuned parameters $\\theta$ to remain close to the pre-trained parameters $\\theta_0$. By working through a mathematical derivation for a simple linear model, you will gain a foundational, first-principles understanding of how this essential regularization technique works to stabilize fine-tuning. [@problem_id:3195259]", "problem": "Consider transfer learning where a model is first trained on a source dataset to obtain pre-trained parameters $\\theta_{0} \\in \\mathbb{R}^{d}$, and then fine-tuned on a target dataset using Starting Point (SP) regularization, also known as L2 Starting Point (L2-SP) regularization. Let the target dataset be given by a design matrix $X \\in \\mathbb{R}^{n \\times d}$ and a target response vector $y \\in \\mathbb{R}^{n}$. Consider a linear model $f(x) = x^{\\top}\\theta$, and define the Mean Squared Error (MSE) empirical risk on the target data as\n$$\n\\mathcal{L}_{\\text{target}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}.\n$$\nFine-tuning with L2-SP regularization uses the objective\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\mathcal{L}_{\\text{target}}(\\theta) + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2},\n$$\nwhere $\\lambda > 0$ is the regularization strength. Starting from first principles of empirical risk minimization and properties of convex quadratic functions, explain why the L2-SP term biases the fine-tuned solution toward the pre-trained parameters $\\theta_{0}$, and derive the closed-form minimizer of $\\mathcal{L}_{\\text{ft}}(\\theta)$ for the linear model. Assume $X$ and $\\lambda$ are such that the minimizer is unique. Your final answer must be the single closed-form analytic expression for the minimizer $\\theta^{\\star}$ of $\\mathcal{L}_{\\text{ft}}(\\theta)$. No numerical approximation is required.", "solution": "The problem asks for two things: first, a conceptual explanation of how L2 Starting Point (L2-SP) regularization biases the fine-tuned parameters towards the pre-trained parameters, and second, the derivation of the closed-form solution for the fine-tuned parameters in a linear regression context.\n\nFirst, let's address the conceptual explanation. The objective function for fine-tuning with L2-SP regularization is given by:\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\mathcal{L}_{\\text{target}}(\\theta) + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}\n$$\nThis objective function is composed of two terms. The first term, $\\mathcal{L}_{\\text{target}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}$, is the empirical risk, specifically the Mean Squared Error (MSE), on the target dataset. This term measures how well the model with parameters $\\theta$ fits the target data. Minimizing this term alone would drive the parameters towards a solution that best explains the target data, without any regard for the pre-trained parameters $\\theta_{0}$.\n\nThe second term, $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$, is the L2-SP regularization penalty. This term measures the squared Euclidean distance between the current parameters $\\theta$ and the pre-trained parameters $\\theta_{0}$, scaled by a regularization hyperparameter $\\lambda > 0$. This penalty term is minimized when $\\theta = \\theta_{0}$. It increases quadratically as $\\theta$ moves away from $\\theta_{0}$.\n\nThe process of minimizing the total objective $\\mathcal{L}_{\\text{ft}}(\\theta)$ involves a trade-off between these two competing objectives, which is controlled by the regularization strength $\\lambda$.\n-   To minimize $\\mathcal{L}_{\\text{target}}(\\theta)$, the optimization must adjust $\\theta$ to reduce the prediction error on the target dataset.\n-   To minimize $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$, the optimization must keep $\\theta$ as close as possible to the starting point $\\theta_{0}$.\n\nThe final solution, $\\theta^{\\star}$, will be a point that balances these two pressures. If the solution $\\theta$ strays too far from $\\theta_{0}$ to achieve a better fit on the target data, the penalty term will grow, increasing the overall loss. Consequently, the optimization process is guided not only by the target data's error landscape but also by a \"gravity\" pulling the solution towards $\\theta_{0}$. This is why the L2-SP term is said to bias the fine-tuned solution toward the pre-trained parameters. The magnitude of this bias is determined by $\\lambda$. As $\\lambda \\to \\infty$, the penalty term dominates, and the solution $\\theta^{\\star}$ will be forced to be very close to $\\theta_{0}$, i.e., $\\theta^{\\star} \\to \\theta_{0}$. Conversely, as $\\lambda \\to 0$, the regularization effect vanishes, and the solution converges to the standard empirical risk minimizer for the target data.\n\nNext, we derive the closed-form minimizer of $\\mathcal{L}_{\\text{ft}}(\\theta)$. The objective function is:\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2} + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}\n$$\nThe function $\\mathcal{L}_{\\text{ft}}(\\theta)$ is a sum of two convex functions. The term $\\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}$ is convex, and since $\\lambda > 0$, the term $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$ is strictly convex. Their sum is therefore strictly convex, which guarantees that a unique minimizer exists. To find this minimizer, we compute the gradient of $\\mathcal{L}_{\\text{ft}}(\\theta)$ with respect to $\\theta$ and set it to the zero vector.\n\nFirst, we expand the squared norm terms using their definition in terms of the inner product, $\\|v\\|_{2}^{2} = v^{\\top}v$:\n$$\n\\|X\\theta - y\\|_{2}^{2} = (X\\theta - y)^{\\top}(X\\theta - y) = (\\theta^{\\top}X^{\\top} - y^{\\top})(X\\theta - y) = \\theta^{\\top}X^{\\top}X\\theta - 2y^{\\top}X\\theta + y^{\\top}y\n$$\n$$\n\\|\\theta - \\theta_{0}\\|_{2}^{2} = (\\theta - \\theta_{0})^{\\top}(\\theta - \\theta_{0}) = \\theta^{\\top}\\theta - 2\\theta_{0}^{\\top}\\theta + \\theta_{0}^{\\top}\\theta_{0}\n$$\nSubstituting these back into the objective function:\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}(\\theta^{\\top}X^{\\top}X\\theta - 2y^{\\top}X\\theta + y^{\\top}y) + \\lambda(\\theta^{\\top}\\theta - 2\\theta_{0}^{\\top}\\theta + \\theta_{0}^{\\top}\\theta_{0})\n$$\nNow, we compute the gradient $\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta)$. We use the following standard matrix calculus identities: $\\nabla_{z}(z^{\\top}Az) = (A+A^{\\top})z$ and $\\nabla_{z}(b^{\\top}z) = b$. Since $X^{\\top}X$ and the identity matrix $I$ are symmetric, we have $\\nabla_{\\theta}(\\theta^{\\top}X^{\\top}X\\theta) = 2X^{\\top}X\\theta$ and $\\nabla_{\\theta}(\\theta^{\\top}\\theta) = 2\\theta$.\n\nDifferentiating $\\mathcal{L}_{\\text{ft}}(\\theta)$ term by term:\n$$\n\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}(2X^{\\top}X\\theta - 2X^{\\top}y) + \\lambda(2\\theta - 2\\theta_{0})\n$$\n$$\n\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{n}(X^{\\top}X\\theta - X^{\\top}y) + 2\\lambda(\\theta - \\theta_{0})\n$$\nTo find the minimizer $\\theta^{\\star}$, we set the gradient to the zero vector:\n$$\n\\frac{1}{n}(X^{\\top}X\\theta^{\\star} - X^{\\top}y) + 2\\lambda(\\theta^{\\star} - \\theta_{0}) = 0\n$$\nNow, we solve for $\\theta^{\\star}$. First, multiply by $n$ to clear the fraction:\n$$\nX^{\\top}X\\theta^{\\star} - X^{\\top}y + 2n\\lambda(\\theta^{\\star} - \\theta_{0}) = 0\n$$\n$$\nX^{\\top}X\\theta^{\\star} - X^{\\top}y + 2n\\lambda\\theta^{\\star} - 2n\\lambda\\theta_{0} = 0\n$$\nGroup the terms containing $\\theta^{\\star}$:\n$$\n(X^{\\top}X + 2n\\lambda I)\\theta^{\\star} = X^{\\top}y + 2n\\lambda\\theta_{0}\n$$\nwhere $I$ is the $d \\times d$ identity matrix. The problem assumes that the minimizer is unique, which implies that the matrix $(X^{\\top}X + 2n\\lambda I)$ is invertible. This is guaranteed because $X^{\\top}X$ is positive semi-definite and, since $n>0$ and $\\lambda>0$, $2n\\lambda I$ is positive definite. The sum of a positive semi-definite and a positive definite matrix is positive definite, and thus invertible.\n\nFinally, we find a closed-form expression for $\\theta^{\\star}$ by left-multiplying by the inverse of $(X^{\\top}X + 2n\\lambda I)$:\n$$\n\\theta^{\\star} = (X^{\\top}X + 2n\\lambda I)^{-1}(X^{\\top}y + 2n\\lambda\\theta_{0})\n$$\nThis expression is the closed-form minimizer for the L2-SP regularized fine-tuning objective for a linear model.", "answer": "$$\n\\boxed{(X^{\\top}X + 2n\\lambda I)^{-1}(X^{\\top}y + 2n\\lambda\\theta_{0})}\n$$", "id": "3195259"}, {"introduction": "Beyond achieving high accuracy, real-world machine learning models often must operate within strict constraints for safety, fairness, or monotonicity. For instance, a medical diagnostic model might need to guarantee that a higher risk score always corresponds to a more severe condition. This exercise moves from unconstrained optimization to the practical challenge of fine-tuning with such \"guardrails.\" You will implement projected gradient descent, an algorithm that enforces linear equality constraints on a model's parameters, ensuring that it respects pre-defined rules throughout the fine-tuning process. This hands-on practice will equip you with a crucial technique for developing more reliable and trustworthy AI systems. [@problem_id:3195172]", "problem": "Consider a linear model with parameter vector $\\theta \\in \\mathbb{R}^d$ used for fine-tuning on a target dataset $(X, y)$, where $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^{n}$. The target objective is the Mean Squared Error (MSE), defined as\n$$\nL(\\theta) = \\frac{1}{2n} \\left\\|X\\theta - y\\right\\|_2^2,\n$$\nwhere $\\left\\|\\cdot\\right\\|_2$ denotes the Euclidean norm. Fine-tuning from a pre-trained source parameter $\\theta_{\\text{src}}$ is performed using gradient descent. To enforce safety or monotonicity constraints during fine-tuning, impose linear equality constraints $C\\theta = d$, where $C \\in \\mathbb{R}^{m \\times d}$ and $d \\in \\mathbb{R}^m$. Use projected gradient updates onto the affine constraint set.\n\nStart from the following fundamental bases:\n- The gradient descent update is defined by $\\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)$, where $\\alpha > 0$ is the step size and $\\nabla L(\\theta)$ is the gradient of the loss with respect to $\\theta$.\n- The Euclidean projection of any $u \\in \\mathbb{R}^d$ onto the affine set $\\{\\theta: C\\theta = d\\}$ is defined as the minimizer of $\\left\\|z - u\\right\\|_2^2$ subject to $Cz = d$.\n\nImplement two fine-tuning procedures:\n- Unconstrained gradient descent starting from $\\theta_{\\text{src}}$.\n- Projected gradient descent starting from $\\theta_{\\text{src}}$, where each gradient step is followed by Euclidean projection onto $\\{\\theta \\in \\mathbb{R}^d : C\\theta = d\\}$.\nAlso compute a scratch baseline by unconstrained gradient descent starting from $\\theta_{\\text{scratch}} = 0$.\n\nDefine the negative transfer amount for a method with initialization $\\theta_{\\text{init}}$ as\n$$\n\\operatorname{NT}(\\theta_{\\text{init}}) = L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{init}})\\bigr) - L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{scratch}})\\bigr),\n$$\nwhere $\\theta_{\\text{final}}(\\cdot)$ denotes the parameter after a fixed number of gradient steps. Test whether the constraints reduce negative transfer by checking\n$$\n\\operatorname{NT}_{\\text{constrained}} < \\operatorname{NT}_{\\text{unconstrained}}.\n$$\n\nUse the following fixed target dataset and hyperparameters:\n- Dimension $d = 3$.\n- Number of samples $n = 4$.\n- Design matrix\n$$\nX = \\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n2 & 0 & 1\n\\end{bmatrix},\n$$\nand target labels\n$$\ny = \\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 2\n\\end{bmatrix}.\n$$\n- Gradient descent step size $\\alpha = 0.1$ and total steps $T = 5$.\n\nTest Suite (each test case specifies $(\\theta_{\\text{src}}, C, d)$):\n1. Happy path (safety constraint on a spurious source feature):\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$,\n   - $C = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}$,\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$.\n2. Boundary case (no constraints):\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$,\n   - $C \\in \\mathbb{R}^{0 \\times 3}$ (the $0 \\times 3$ empty matrix),\n   - $d \\in \\mathbb{R}^{0}$ (the empty vector).\n3. Edge case (irrelevant or harmful calibration constraint):\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$,\n   - $C = \\begin{bmatrix} 1 & 1 & 0 \\end{bmatrix}$,\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$.\n4. Positive transfer scenario (source already aligned with target):\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 0.0 \\end{bmatrix}$,\n   - $C = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}$,\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$.\n\nYour program must:\n- Implement gradient descent for the MSE objective.\n- Implement projected gradient descent onto $C\\theta = d$ using Euclidean projection after each gradient step.\n- For each test case, compute the boolean\n$$\nb = \\left( \\operatorname{NT}_{\\text{constrained}} < \\operatorname{NT}_{\\text{unconstrained}} \\right).\n$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be the boolean for one test case in the order given above, for example, $\\left[\\text{True},\\text{False},\\text{True},\\text{False}\\right]$. No physical units or angle units are involved in this problem. Express all boolean results explicitly as $\\text{True}$ or $\\text{False}$.", "solution": "The problem requires a comparison between unconstrained and constrained fine-tuning of a linear model using gradient-based methods. The goal is to determine if imposing linear equality constraints can mitigate negative transfer. This is assessed by comparing the final loss of a constrained fine-tuned model to that of an unconstrained one, relative to a baseline model trained from scratch.\n\nFirst, we define the mathematical components of the problem. The model is a linear function of its parameters $\\theta \\in \\mathbb{R}^d$, and its performance on a target dataset $(X, y)$ with $n$ samples, where $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$, is measured by the Mean Squared Error (MSE) loss function:\n$$\nL(\\theta) = \\frac{1}{2n} \\|X\\theta - y\\|_2^2 = \\frac{1}{2n} (X\\theta - y)^T(X\\theta - y)\n$$\nThis is a convex and differentiable function of $\\theta$.\n\nTo perform gradient descent, we need the gradient of the loss function with respect to the parameters $\\theta$. Expanding the loss function gives:\n$$\nL(\\theta) = \\frac{1}{2n} (\\theta^T X^T X \\theta - 2y^T X \\theta + y^T y)\n$$\nTaking the derivative with respect to $\\theta$ yields the gradient:\n$$\n\\nabla L(\\theta) = \\frac{1}{2n} (2 X^T X \\theta - 2 X^T y) = \\frac{1}{n} X^T (X\\theta - y)\n$$\n\nThe problem specifies three training procedures, all using a fixed step size $\\alpha > 0$ for a total of $T$ iterations.\n\n1.  **Unconstrained Gradient Descent**: This standard algorithm is used for both the \"scratch\" baseline (starting from $\\theta_{\\text{scratch}} = 0$) and the \"unconstrained\" fine-tuning (starting from a pre-trained $\\theta_{\\text{src}}$). The update rule at each step $k$ is:\n    $$\n    \\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)\n    $$\n\n2.  **Projected Gradient Descent**: This algorithm is used for \"constrained\" fine-tuning. It ensures that the parameter vector $\\theta$ always satisfies the linear equality constraints $C\\theta = d$, where $C \\in \\mathbb{R}^{m \\times d}$ and $d \\in \\mathbb{R}^m$. Each iteration consists of two steps:\n    a. A standard gradient descent step to find an intermediate point $u_{k+1}$:\n    $$\n    u_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)\n    $$\n    b. A projection of $u_{k+1}$ onto the affine constraint set $\\mathcal{A} = \\{\\theta \\in \\mathbb{R}^d : C\\theta = d\\}$ to obtain the next iterate $\\theta_{k+1}$. The projection, denoted $P_{\\mathcal{A}}(u_{k+1})$, is the point in $\\mathcal{A}$ closest to $u_{k+1}$ in the Euclidean sense:\n    $$\n    \\theta_{k+1} = P_{\\mathcal{A}}(u_{k+1}) = \\arg\\min_{z \\in \\mathcal{A}} \\|z - u_{k+1}\\|_2^2\n    $$\n\nTo derive the formula for the projection operator $P_{\\mathcal{A}}(u)$, we solve the constrained optimization problem $\\min_z \\frac{1}{2} \\|z - u\\|_2^2$ subject to $Cz = d$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(z, \\lambda) = \\frac{1}{2} (z - u)^T(z - u) + \\lambda^T(Cz - d)\n$$\nwhere $\\lambda \\in \\mathbb{R}^m$ is the vector of Lagrange multipliers. The first-order optimality conditions are:\n$$\n\\nabla_z \\mathcal{L} = z - u + C^T \\lambda = 0 \\implies z = u - C^T \\lambda\n$$\nSubstituting this into the constraint $Cz = d$:\n$$\nC(u - C^T \\lambda) = d \\implies Cu - CC^T \\lambda = d \\implies CC^T \\lambda = Cu - d\n$$\nAssuming the constraint matrix $C$ has full row rank (i.e., its rows are linearly independent), the matrix $CC^T \\in \\mathbb{R}^{m \\times m}$ is invertible. We can solve for $\\lambda$:\n$$\n\\lambda = (CC^T)^{-1} (Cu - d)\n$$\nSubstituting $\\lambda$ back into the expression for $z$ gives the projection formula:\n$$\nP_{\\mathcal{A}}(u) = z = u - C^T (CC^T)^{-1} (Cu - d)\n$$\nIn the special case where there are no constraints ($m=0$), the matrix $C$ is empty ($C \\in \\mathbb{R}^{0 \\times d}$). The constraint set is the entire space $\\mathbb{R}^d$, and the projection is simply the identity operator, $P_{\\mathcal{A}}(u) = u$.\n\nThe evaluation metric is the negative transfer amount, $\\operatorname{NT}(\\theta_{\\text{init}})$, defined as the difference between the final loss of a model fine-tuned from $\\theta_{\\text{init}}$ and the final loss of a model trained from scratch, $\\theta_{\\text{scratch}}$:\n$$\n\\operatorname{NT}(\\theta_{\\text{init}}) = L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{init}})\\bigr) - L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{scratch}})\\bigr)\n$$\nWe must test whether the constraints reduce negative transfer, which is formulated as the inequality:\n$$\n\\operatorname{NT}_{\\text{constrained}} < \\operatorname{NT}_{\\text{unconstrained}}\n$$\nLet $\\theta_{\\text{final, constrained}}$ be the final parameters from projected gradient descent and $\\theta_{\\text{final, unconstrained}}$ be the final parameters from unconstrained gradient descent, both starting from $\\theta_{\\text{src}}$. The inequality becomes:\n$$\nL(\\theta_{\\text{final, constrained}}) - L(\\theta_{\\text{final, scratch}}) < L(\\theta_{\\text{final, unconstrained}}) - L(\\theta_{\\text{final, scratch}})\n$$\nThis simplifies to a direct comparison of the final losses of the two fine-tuning methods:\n$$\nL(\\theta_{\\text{final, constrained}}) < L(\\theta_{\\text{final, unconstrained}})\n$$\n\nTo solve the problem, we implement the three gradient descent procedures using the provided data ($X, y$), hyperparameters ($\\alpha=0.1, T=5$), and the specifics of each test case $(\\theta_{\\text{src}}, C, d)$. For each case, we compute the final losses $L(\\theta_{\\text{final, constrained}})$ and $L(\\theta_{\\text{final, unconstrained}})$ and evaluate the boolean truth of the inequality above.\n\nThe values used are:\n$d = 3$, $n = 4$.\n$X = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 2 & 0 & 1 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 2 \\end{bmatrix}$.\n$\\alpha = 0.1$, $T = 5$.\n$\\theta_{\\text{scratch}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating constrained fine-tuning against negative transfer.\n    \"\"\"\n    \n    # Fixed target dataset and hyperparameters\n    X = np.array([\n        [1, 0, 1],\n        [0, 1, 1],\n        [1, 1, 1],\n        [2, 0, 1]\n    ])\n    y = np.array([1, 2, 3, 2]).reshape(-1, 1)\n    \n    n, d_dim = X.shape\n    alpha = 0.1\n    T = 5\n    theta_scratch = np.zeros((d_dim, 1))\n\n    # Test Suite (theta_src, C, d)\n    test_cases = [\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.array([[0, 0, 1]]),\n            np.array([[0]])\n        ),\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.empty((0, d_dim)),\n            np.empty((0, 1))\n        ),\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.array([[1, 1, 0]]),\n            np.array([[0]])\n        ),\n        (\n            np.array([1.0, 2.0, 0.0]).reshape(-1, 1),\n            np.array([[0, 0, 1]]),\n            np.array([[0]])\n        )\n    ]\n\n    def compute_loss(theta):\n        \"\"\"Computes the MSE loss.\"\"\"\n        return (1 / (2 * n)) * np.sum((X @ theta - y)**2)\n\n    def compute_gradient(theta):\n        \"\"\"Computes the gradient of the MSE loss.\"\"\"\n        return (1 / n) * X.T @ (X @ theta - y)\n\n    def project(u, C, d_vec):\n        \"\"\"Projects a vector u onto the affine set {z : C z = d}.\"\"\"\n        if C.shape[0] == 0:  # No constraints\n            return u\n        \n        # u - C.T @ inv(C @ C.T) @ (C @ u - d)\n        C_T = C.T\n        CC_T = C @ C_T\n        CC_T_inv = np.linalg.inv(CC_T)\n        \n        projection_offset = C_T @ CC_T_inv @ (C @ u - d_vec)\n        return u - projection_offset\n\n    def unconstrained_gd(theta_init):\n        \"\"\"Performs unconstrained gradient descent.\"\"\"\n        theta = theta_init.copy()\n        for _ in range(T):\n            grad = compute_gradient(theta)\n            theta -= alpha * grad\n        return theta\n\n    def projected_gd(theta_init, C, d_vec):\n        \"\"\"Performs projected gradient descent.\"\"\"\n        theta = theta_init.copy()\n        for _ in range(T):\n            grad = compute_gradient(theta)\n            u = theta - alpha * grad\n            theta = project(u, C, d_vec)\n        return theta\n\n    # Calculate baseline loss (from scratch) once\n    theta_final_scratch = unconstrained_gd(theta_scratch)\n    L_final_scratch = compute_loss(theta_final_scratch)\n\n    results = []\n    for theta_src, C, d_vec in test_cases:\n        # Unconstrained fine-tuning\n        theta_final_unconstrained = unconstrained_gd(theta_src)\n        L_final_unconstrained = compute_loss(theta_final_unconstrained)\n        \n        # Constrained fine-tuning\n        theta_final_constrained = projected_gd(theta_src, C, d_vec)\n        L_final_constrained = compute_loss(theta_final_constrained)\n\n        # Negative transfer definitions\n        # NT_unconstrained = L_final_unconstrained - L_final_scratch\n        # NT_constrained = L_final_constrained - L_final_scratch\n        \n        # Check if NT_constrained < NT_unconstrained\n        # This simplifies to L_final_constrained < L_final_unconstrained\n        test_result = L_final_constrained < L_final_unconstrained\n        results.append(test_result)\n        \n    # Format and print the final output\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3195172"}, {"introduction": "Adapting a large, pre-trained language model to a new, specialized domain—like legal documents or medical literature—often introduces a practical hurdle: tokenization mismatch. The new text may contain domain-specific terms or jargon that were not in the model's original vocabulary, creating Out-Of-Vocabulary (OOV) tokens. This exercise challenges you to investigate this problem by implementing and comparing two distinct and sophisticated fine-tuning strategies. By coding both an embedding-only update mechanism and a more modern adapter-based approach, you will gain practical insight into how to effectively handle vocabulary shifts, a common and important task in applied Natural Language Processing. [@problem_id:3195164]", "problem": "You are given a simplified, purely mathematical model of transfer learning for language models to study tokenization mismatch between a source subword vocabulary and a target token vocabulary. The goal is to test, under a well-defined surrogate loss, whether fine-tuning embeddings only is sufficient for handling Out-Of-Vocabulary (OOV) tokens or whether adapter updates are required. You will implement a program that, for each test case, computes and compares the minimum achievable mean squared error under two constrained fine-tuning strategies, and returns a boolean verdict per case.\n\nFundamental setup and definitions:\n\n- Let the source subword vocabulary be denoted by $\\mathcal{V}_S$ of size $|\\mathcal{V}_S| = m_S$, and the target token vocabulary be denoted by $\\mathcal{V}_T$ of size $|\\mathcal{V}_T| = m_T$. The embedding dimension is $d$.\n\n- A pre-trained model at the output layer is represented by a tied embedding matrix $E_S \\in \\mathbb{R}^{m_S \\times d}$ shared between input and output. The contextual representation (encoder output) for a batch of $N$ target examples is given by a matrix $H \\in \\mathbb{R}^{d \\times N}$, treated as fixed features. The ground-truth labels for these $N$ examples are represented by a one-hot matrix $Y \\in \\{0,1\\}^{m_T \\times N}$, with each column containing exactly one entry equal to $1$ and all others equal to $0$.\n\n- The tokenization mismatch is captured by an incidence matrix $G \\in \\{0,1\\}^{m_T \\times m_S}$ that maps target tokens to source subwords. Specifically, for target token index $t \\in \\{0,\\ldots,m_T-1\\}$ and source subword index $s \\in \\{0,\\ldots,m_S-1\\}$, $G_{t,s} = 1$ indicates that $s$ contributes to the aggregated logit of $t$, and $G_{t,s} = 0$ otherwise. An Out-Of-Vocabulary (OOV) token relative to the source subword set is characterized by a row of zeros in $G$ (that is, $\\sum_{s} G_{t,s} = 0$).\n\n- The pre-trained target-level output weight matrix is $U_0 = G E_S \\in \\mathbb{R}^{m_T \\times d}$. The prediction logits under the pre-trained target space are $U_0 H$.\n\n- Two constrained fine-tuning strategies are considered with a scalar regularization parameter $\\lambda \\geq 0$:\n  1. Embedding-only (OOV rows updatable): You are allowed to modify only the rows of a target-level output weight matrix $U_E \\in \\mathbb{R}^{m_T \\times d}$ corresponding to OOV tokens; for non-OOV tokens, the rows must remain equal to the corresponding rows of $U_0$. The penalized least-squares objective is\n     $$\\min_{U_E} \\; \\| Y - U_E H \\|_F^2 + \\lambda \\| U_E \\|_F^2,$$\n     subject to the constraint that for any non-OOV target index $t$, the $t$-th row of $U_E$ equals the $t$-th row of $U_0$. Here $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n  2. Adapter-only (fixed output, rank-$r$ linear adapter): You fix $U_0$ and insert a linear adapter $A \\in \\mathbb{R}^{d \\times d}$ of rank at most $r$ into the contextual pathway, producing $H' = (I_d + A) H$, where $I_d$ is the $d \\times d$ identity matrix. The penalized least-squares objective is\n     $$\\min_{A} \\; \\| Y - U_0 (I_d + A) H \\|_F^2 + \\lambda \\| A \\|_F^2,$$\n     subject to $\\mathrm{rank}(A) \\leq r$.\n\n- For each test case, the program must compute the minimum achievable mean squared error under each strategy and return a boolean verdict indicating whether the embedding-only strategy achieves a strictly lower minimal loss than the adapter-only strategy when the adapter is constrained to rank $r$. A small numerical tolerance is acceptable when comparing floating-point values.\n\nThe task requires deriving solutions starting from the following foundational base in deep learning and numerical linear algebra:\n\n- Linear models with squared error loss and $\\ell_2$ regularization (ridge regression), and the existence of closed-form solutions under suitable convexity conditions.\n- Vectorization identities for matrix equations, particularly $\\mathrm{vec}(U A Z) = (Z^\\top \\otimes U)\\mathrm{vec}(A)$, where $\\otimes$ is the Kronecker product.\n- Properties of the Frobenius norm and the existence of low-rank approximations via Singular Value Decomposition (SVD).\n\nYour program must implement the comparison for the following test suite. In each case, generate $E_S$ and $H$ with independent and identically distributed standard normal entries using the specified pseudorandom seed and construct $Y$ by drawing each column’s target index uniformly at random from $\\{0,\\ldots,m_T-1\\}$ using the same seed. Construct $G$ exactly as described below.\n\n- Test case $1$ (mixed OOV and shared tokens):\n  - Parameters: $d = 8$, $m_S = 6$, $m_T = 6$, $N = 40$, $r = 2$, $\\lambda = 0.05$, seed $= 123$.\n  - Mapping $G$ rows (target index to source subwords indices):\n    - Row $0$: $\\{0\\}$,\n    - Row $1$: $\\{1\\}$,\n    - Row $2$: $\\{2\\}$,\n    - Row $3$: $\\{0,1\\}$,\n    - Row $4$: OOV (no source subwords, row of zeros),\n    - Row $5$: $\\{3\\}$.\n- Test case $2$ (no OOV):\n  - Parameters: $d = 8$, $m_S = 6$, $m_T = 5$, $N = 40$, $r = 2$, $\\lambda = 0.05$, seed $= 456$.\n  - Mapping $G$ rows:\n    - Row $0$: $\\{0\\}$,\n    - Row $1$: $\\{1\\}$,\n    - Row $2$: $\\{2\\}$,\n    - Row $3$: $\\{3\\}$,\n    - Row $4$: $\\{4\\}$.\n- Test case $3$ (all OOV):\n  - Parameters: $d = 8$, $m_S = 6$, $m_T = 3$, $N = 30$, $r = 2$, $\\lambda = 0.05$, seed $= 789$.\n  - Mapping $G$ rows:\n    - Row $0$: OOV,\n    - Row $1$: OOV,\n    - Row $2$: OOV.\n\nFinal output requirement:\n\n- For the three test cases above, your program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets. Each element should be the boolean value indicating whether the embedding-only strategy achieves a strictly lower minimal loss than the adapter-only strategy for that case. For example, the format should be exactly like\n- \"[True,False,True]\".\n\nYour implementation must be self-contained and must not require any external input. You must base your derivation and algorithm design on the stated fundamental laws and well-tested formulas without using unspecified shortcuts or black-box procedures. No physical units, angles, or percentages are involved in this problem; all outputs must be booleans as specified.", "solution": "The problem presents a simplified mathematical model to compare two fine-tuning strategies in the context of transfer learning for language models, specifically addressing the issue of Out-Of-Vocabulary (OOV) tokens. The task is to determine which strategy yields a lower penalized least-squares loss for three distinct test cases. The problem is well-posed, scientifically grounded in mathematical optimization and linear algebra, and all necessary data and constraints are provided. We proceed to derive the solutions for each strategy.\n\nThe core of the problem lies in solving two distinct constrained optimization problems. Let us analyze each one.\n\n### Strategy 1: Embedding-only Fine-tuning\n\nThe first strategy involves updating only the rows of a target-level output weight matrix, $U_E \\in \\mathbb{R}^{m_T \\times d}$, that correspond to OOV tokens. The objective function to minimize is:\n$$ \\mathcal{L}_E(U_E) = \\| Y - U_E H \\|_F^2 + \\lambda \\| U_E \\|_F^2 $$\nsubject to the constraint that for any non-OOV target index $t$, the $t$-th row of $U_E$ is fixed to the $t$-th row of the pre-trained matrix $U_0 = G E_S$.\n\nThe Frobenius norm squared, $\\|M\\|_F^2$, is the sum of squared Euclidean norms of its rows. This allows the objective function to be decomposed into a sum of independent objectives for each row of $U_E$. Let $u_{E,t}^{\\top}$ be the $t$-th row of $U_E$ and $y_t^{\\top}$ be the $t$-th row of the label matrix $Y$. The objective is:\n$$ \\mathcal{L}_E(U_E) = \\sum_{t=0}^{m_T-1} \\left( \\| y_t^{\\top} - u_{E,t}^{\\top} H \\|_2^2 + \\lambda \\| u_{E,t}^{\\top} \\|_2^2 \\right) $$\n\nFor a non-OOV token index $t$, the row $u_{E,t}$ is fixed to $u_{0,t}$ (the $t$-th row of $U_0$). The contribution to the total loss from these rows is a fixed constant.\n\nFor an OOV token index $t$, the row $u_{E,t}$ is a variable to be optimized. The subproblem for each such row is:\n$$ \\min_{u_{E,t}} \\left( \\| y_t^{\\top} - u_{E,t}^{\\top} H \\|_2^2 + \\lambda \\| u_{E,t}^{\\top} \\|_2^2 \\right) $$\nThis is a standard ridge regression problem. To find the minimum, we take the gradient of the objective with respect to the vector $u_{E,t}$ and set it to zero. The objective for a single OOV row $u$ is $f(u) = (y - H^{\\top}u)^{\\top}(y-H^{\\top}u) + \\lambda u^{\\top}u$. Expanding this gives $f(u) = y^{\\top}y - 2y^{\\top}H^{\\top}u + u^{\\top}HH^{\\top}u + \\lambda u^{\\top}u$. The gradient is:\n$$ \\nabla_u f(u) = -2 H y + 2(HH^{\\top} + \\lambda I_d)u $$\nSetting the gradient to zero yields the optimal solution for $u_{E,t}$:\n$$ (HH^{\\top} + \\lambda I_d) u_{E,t} = H y_t $$\n$$ u_{E,t}^* = (HH^{\\top} + \\lambda I_d)^{-1} H y_t $$\nwhere $y_t$ is the transpose of the $t$-th row of $Y$. This calculation is performed for each OOV row. The final minimoized loss $\\mathcal{L}_E^*$ is computed by substituting these optimal row vectors (and the fixed non-OOV rows) back into the objective function.\n\n### Strategy 2: Adapter-only Fine-tuning\n\nThe second strategy involves inserting a low-rank linear adapter $A \\in \\mathbb{R}^{d \\times d}$ while keeping the output weights $U_0$ fixed. The objective is:\n$$ \\mathcal{L}_A(A) = \\| Y - U_0 (I_d + A) H \\|_F^2 + \\lambda \\| A \\|_F^2 $$\nsubject to the constraint $\\mathrm{rank}(A) \\leq r$.\n\nLet $Y' = Y - U_0 H$ be the error of the pre-trained model. The objective can be rewritten as:\n$$ \\min_{A, \\mathrm{rank}(A) \\leq r} \\| Y' - U_0 A H \\|_F^2 + \\lambda \\| A \\|_F^2 $$\nThis is a non-convex problem due to the rank constraint. A common and practical approach, suggested by the problem's reference to SVD for low-rank approximation, is to first solve the problem without the rank constraint and then project the solution onto the space of rank-$r$ matrices.\n\nThe unconstrained problem is to minimize $\\mathcal{L}_A(A)$ with respect to $A \\in \\mathbb{R}^{d \\times d}$. Taking the gradient of $\\mathcal{L}_A(A)$ with respect to $A$ and setting it to zero gives:\n$$ \\nabla_A \\mathcal{L}_A(A) = -2 U_0^{\\top} (Y' - U_0 A H) H^{\\top} + 2 \\lambda A = 0 $$\n$$ U_0^{\\top} U_0 A H H^{\\top} + \\lambda A = U_0^{\\top} Y' H^{\\top} $$\nThis is a continuous-time Sylvester equation of the form $C_U A C_H + \\lambda A = B$, where $C_U = U_0^{\\top} U_0$, $C_H = H H^{\\top}$, and $B = U_0^{\\top} Y' H^{\\top}$.\n\nTo solve for the unconstrained matrix $A_{unc}$, we can vectorize the equation. Using the identity $\\mathrm{vec}(UXV) = (V^{\\top} \\otimes U) \\mathrm{vec}(X)$, the Sylvester equation becomes:\n$$ (C_H^{\\top} \\otimes C_U) \\mathrm{vec}(A) + \\lambda \\mathrm{vec}(A) = \\mathrm{vec}(B) $$\nSince $C_H$ is symmetric ($C_H=C_H^{\\top}$), this simplifies to:\n$$ (C_H \\otimes C_U + \\lambda I_{d^2}) \\mathrm{vec}(A) = \\mathrm{vec}(B) $$\nThis is a standard linear system of size $d^2 \\times d^2$, which can be solved for $\\mathrm{vec}(A_{unc})$. Reshaping $\\mathrm{vec}(A_{unc})$ gives the matrix $A_{unc}$.\n\nThe next step is to find the best rank-$r$ approximation of $A_{unc}$. The Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of a matrix in the Frobenius norm is obtained by truncating its Singular Value Decomposition (SVD). We compute the SVD of $A_{unc} = U \\Sigma V^{\\top}$ and form the rank-$r$ approximation $A^*_r$ by keeping the top $r$ singular values and corresponding singular vectors:\n$$ A^*_r = U_{:, :r} \\Sigma_{:r, :r} V^{\\top}_{:r, :} $$\nThis matrix $A^*_r$ is the solution used for this strategy. The minimum loss $\\mathcal{L}_A^*$ is then calculated by substituting $A^*_r$ into the objective function.\n\n### Algorithm Summary and Comparison\n\nFor each test case, the program will execute the following steps:\n1.  Generate the matrices $E_S$, $H$, and $Y$ based on the specified parameters and pseudorandom seed. Construct the incidence matrix $G$ and the pre-trained output matrix $U_0 = G E_S$.\n2.  **Calculate embedding-only loss $\\mathcal{L}_E^*$**:\n    a. Identify OOV rows from $G$.\n    b. For each OOV row $t$, compute the optimal row vector $u_{E,t}^* = (HH^{\\top} + \\lambda I_d)^{-1} H y_t$.\n    c. Construct the optimal matrix $U_E^*$ by combining the new OOV rows and the original non-OOV rows from $U_0$.\n    d. Compute the total loss $\\mathcal{L}_E^* = \\| Y - U_E^* H \\|_F^2 + \\lambda \\| U_E^* \\|_F^2$.\n3.  **Calculate adapter-only loss $\\mathcal{L}_A^*$**:\n    a. Solve the linear system $(C_H \\otimes C_U + \\lambda I_{d^2}) \\mathrm{vec}(A_{unc}) = \\mathrm{vec}(B)$ for $A_{unc}$.\n    b. Compute the rank-$r$ SVD approximation $A_r^*$ of $A_{unc}$.\n    c. Compute the total loss $\\mathcal{L}_A^* = \\| Y - U_0 (I_d + A_r^*) H \\|_F^2 + \\lambda \\| A_r^* \\|_F^2$.\n4.  **Compare**: Return `True` if $\\mathcal{L}_E^* < \\mathcal{L}_A^*$, and `False` otherwise.\nThis procedure is implemented for each provided test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transfer learning model comparison problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 8, \"m_S\": 6, \"m_T\": 6, \"N\": 40, \"r\": 2, \"lambda\": 0.05, \"seed\": 123,\n            \"G_map\": {0: {0}, 1: {1}, 2: {2}, 3: {0, 1}, 4: set(), 5: {3}}\n        },\n        {\n            \"d\": 8, \"m_S\": 6, \"m_T\": 5, \"N\": 40, \"r\": 2, \"lambda\": 0.05, \"seed\": 456,\n            \"G_map\": {0: {0}, 1: {1}, 2: {2}, 3: {3}, 4: {4}}\n        },\n        {\n            \"d\": 8, \"m_S\": 6, \"m_T\": 3, \"N\": 30, \"r\": 2, \"lambda\": 0.05, \"seed\": 789,\n            \"G_map\": {0: set(), 1: set(), 2: set()}\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        d = case[\"d\"]\n        m_S = case[\"m_S\"]\n        m_T = case[\"m_T\"]\n        N = case[\"N\"]\n        r = case[\"r\"]\n        lam = case[\"lambda\"]\n        seed = case[\"seed\"]\n        G_map = case[\"G_map\"]\n\n        # --- Data Generation ---\n        rng = np.random.default_rng(seed)\n        E_S = rng.standard_normal((m_S, d))\n        H = rng.standard_normal((d, N))\n        \n        target_indices = rng.choice(m_T, size=N)\n        Y = np.zeros((m_T, N))\n        Y[target_indices, np.arange(N)] = 1.0\n\n        G = np.zeros((m_T, m_S))\n        for t, s_indices in G_map.items():\n            for s in s_indices:\n                G[t, s] = 1.0\n\n        U0 = G @ E_S\n\n        # --- Strategy 1: Embedding-only ---\n        oov_indices = np.where(np.sum(G, axis=1) == 0)[0]\n        \n        C_H = H @ H.T\n        C_H_reg_inv = np.linalg.inv(C_H + lam * np.eye(d))\n        \n        Ue_star = U0.copy()\n        \n        if oov_indices.size > 0:\n            Y_oov = Y[oov_indices, :]\n            # Derivation: u_E,t* = (H H^T + lambda I)^-1 H y_t\n            # Matrix form for rows: U_E,oov = Y_oov H^T (H H^T + lambda I)^-1\n            Ue_star_oov = Y_oov @ H.T @ C_H_reg_inv\n            Ue_star[oov_indices, :] = Ue_star_oov\n\n        loss_E = np.linalg.norm(Y - Ue_star @ H, 'fro')**2 + lam * np.linalg.norm(Ue_star, 'fro')**2\n\n        # --- Strategy 2: Adapter-only ---\n        Y_prime = Y - U0 @ H\n        C_U = U0.T @ U0\n        B = U0.T @ Y_prime @ H.T\n\n        # Solve (C_H kron C_U + lam*I) vec(A) = vec(B)\n        K = np.kron(C_H, C_U) + lam * np.eye(d*d)\n        \n        # `vec(B)` corresponds to flattening B in Fortran order (column-major)\n        vec_A_unc = np.linalg.solve(K, B.flatten(order='F'))\n        # Reshape back to matrix form\n        A_unc = vec_A_unc.reshape((d, d), order='F')\n\n        # Find best rank-r approximation via SVD\n        U_svd, s_svd, Vh_svd = np.linalg.svd(A_unc)\n        \n        Ar_star = np.zeros((d, d))\n        if r > 0 and len(s_svd) > 0:\n            s_r = s_svd[:r]\n            Ar_star = U_svd[:, :r] @ np.diag(s_r) @ Vh_svd[:r, :]\n        \n        loss_A = np.linalg.norm(Y - U0 @ (np.eye(d) + Ar_star) @ H, 'fro')**2 + lam * np.linalg.norm(Ar_star, 'fro')**2\n\n        # --- Comparison ---\n        results.append(loss_E  loss_A)\n\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3195164"}]}