{"hands_on_practices": [{"introduction": "The foundation of Model-Agnostic Meta-Learning (MAML) is its ability to rapidly adapt to a new task from a shared initialization. This first exercise isolates the fundamental mechanism of this adaptation: a single gradient descent step. By analyzing how one update adjusts a simple classifier to correctly handle a new piece of data, you will derive the precise conditions required for this \"fast adaptation\" to succeed. This practice provides a concrete, analytical look at the mechanics of MAML's inner loop and the crucial role of the learning rate, $\\alpha$ [@problem_id:3180380].", "problem": "Consider a binary classifier implemented as a single artificial neuron with affine score $s = \\mathbf{w}^{\\top}\\mathbf{x} + b$, where $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector, $b \\in \\mathbb{R}$ is the bias, and $\\mathbf{x} \\in \\mathbb{R}^{d}$ is the input. The classifier predicts the class by the sign of $s$. In a meta-learning scenario such as Model-Agnostic Meta-Learning (MAML), you adapt $(\\mathbf{w}, b)$ to a new task using one labeled example $(\\mathbf{x}, y)$ with $y \\in \\{-1, +1\\}$ by taking a single gradient descent step on the logistic loss $L(\\mathbf{w}, b; \\mathbf{x}, y) = \\ln\\!\\big(1 + \\exp(-y(\\mathbf{w}^{\\top}\\mathbf{x} + b))\\big)$ with learning rate $\\alpha > 0$.\n\nStarting from the definition of the logistic loss and the gradient descent update rule, derive the one-step adapted parameters $(\\mathbf{w}', b')$ as functions of $(\\mathbf{w}, b, \\mathbf{x}, y, \\alpha)$. Then, analyze the condition under which a single update suffices to correct a misclassification of $\\mathbf{x}$, that is, when $y(\\mathbf{w}^{\\prime\\top}\\mathbf{x} + b') > 0$ given that initially $y(\\mathbf{w}^{\\top}\\mathbf{x} + b)  0$. Your final task is to provide a closed-form expression for the minimal learning rate $\\alpha_{\\min}$ that guarantees the post-update score is correctly signed for this single example.\n\nProvide only the analytic expression for $\\alpha_{\\min}$ in your final answer. No numerical approximation or rounding is required.", "solution": "The problem asks for the minimal learning rate $\\alpha_{\\min}$ required for a single gradient descent step on the logistic loss to correct a misclassified example.\n\nFirst, we establish the necessary components of the problem.\nThe affine score is given by $s = \\mathbf{w}^{\\top}\\mathbf{x} + b$.\nThe logistic loss for a single example $(\\mathbf{x}, y)$ is $L(\\mathbf{w}, b; \\mathbf{x}, y) = \\ln(1 + \\exp(-y(\\mathbf{w}^{\\top}\\mathbf{x} + b))) = \\ln(1 + \\exp(-ys))$.\nThe parameters $(\\mathbf{w}, b)$ are updated using a single step of gradient descent with learning rate $\\alpha > 0$. The updated parameters are denoted $(\\mathbf{w}', b')$.\nThe update rules are:\n$$ \\mathbf{w}' = \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} L $$\n$$ b' = b - \\alpha \\nabla_{b} L $$\n\nTo apply these rules, we must first compute the gradients of the loss function $L$ with respect to $\\mathbf{w}$ and $b$. We use the chain rule.\n\nThe gradient of $L$ with respect to the score $s$ is:\n$$ \\frac{\\partial L}{\\partial s} = \\frac{1}{1 + \\exp(-ys)} \\cdot \\frac{\\partial}{\\partial s}(1 + \\exp(-ys)) = \\frac{1}{1 + \\exp(-ys)} \\cdot (\\exp(-ys) \\cdot (-y)) = \\frac{-y \\exp(-ys)}{1 + \\exp(-ys)} $$\nThis can be expressed using the sigmoid function $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$:\n$$ \\frac{\\partial L}{\\partial s} = -y \\frac{\\exp(-ys)}{1 + \\exp(-ys)} = -y \\frac{1}{\\exp(ys) + 1} = -y \\sigma(ys) $$\nAlternatively, and more conveniently for this problem, we can express it as:\n$$ \\frac{\\partial L}{\\partial s} = -y \\sigma(-ys) $$\nWhere $\\sigma(-ys) = \\frac{1}{1 + \\exp(ys)}$.\n\nNow we compute the gradients with respect to $\\mathbf{w}$ and $b$:\n$$ \\nabla_{\\mathbf{w}} L = \\frac{\\partial L}{\\partial s} \\nabla_{\\mathbf{w}} s = \\frac{\\partial L}{\\partial s} \\cdot \\mathbf{x} = -y \\sigma(-ys) \\mathbf{x} $$\n$$ \\nabla_{b} L = \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial s} \\frac{\\partial s}{\\partial b} = \\frac{\\partial L}{\\partial s} \\cdot 1 = -y \\sigma(-ys) $$\n\nSubstituting these gradients into the update rules, we obtain the one-step adapted parameters $(\\mathbf{w}', b')$:\n$$ \\mathbf{w}' = \\mathbf{w} - \\alpha (-y \\sigma(-ys) \\mathbf{x}) = \\mathbf{w} + \\alpha y \\sigma(-ys) \\mathbf{x} $$\n$$ b' = b - \\alpha (-y \\sigma(-ys)) = b + \\alpha y \\sigma(-ys) $$\n\nNext, we calculate the post-update score, $s' = \\mathbf{w}^{\\prime\\top}\\mathbf{x} + b'$:\n$$ s' = (\\mathbf{w} + \\alpha y \\sigma(-ys) \\mathbf{x})^{\\top}\\mathbf{x} + (b + \\alpha y \\sigma(-ys)) $$\n$$ s' = \\mathbf{w}^{\\top}\\mathbf{x} + \\alpha y \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x}) + b + \\alpha y \\sigma(-ys) $$\n$$ s' = (\\mathbf{w}^{\\top}\\mathbf{x} + b) + \\alpha y \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1) $$\nRecognizing that $s = \\mathbf{w}^{\\top}\\mathbf{x} + b$ and $\\mathbf{x}^{\\top}\\mathbf{x}$ is the squared Euclidean norm of $\\mathbf{x}$, we have:\n$$ s' = s + \\alpha y \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1) $$\n\nThe problem specifies an initial misclassification, which means the sign of the score does not match the label: $y s  0$. We want to find the minimal learning rate $\\alpha_{\\min}$ such that the classification is corrected after one update, which means the new score $s'$ has the correct sign: $y s' > 0$.\n\nLet's substitute the expression for $s'$ into the target condition:\n$$ y \\left( s + \\alpha y \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1) \\right) > 0 $$\nDistributing $y$ gives:\n$$ ys + \\alpha y^2 \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1) > 0 $$\nSince $y \\in \\{-1, +1\\}$, we have $y^2 = 1$. The inequality becomes:\n$$ ys + \\alpha \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1) > 0 $$\n\nNow, we solve for $\\alpha$:\n$$ \\alpha \\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1) > -ys $$\nThe terms multiplying $\\alpha$ are all positive: $\\sigma(z) > 0$ for all $z \\in \\mathbb{R}$, and $\\mathbf{x}^{\\top}\\mathbf{x} + 1 \\ge 1$. Thus, we can divide by the coefficient of $\\alpha$ without changing the direction of the inequality:\n$$ \\alpha > \\frac{-ys}{\\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1)} $$\n\nThe minimal learning rate $\\alpha_{\\min}$ is the value that defines the lower bound of this inequality.\n$$ \\alpha_{\\min} = \\frac{-ys}{\\sigma(-ys) (\\mathbf{x}^{\\top}\\mathbf{x} + 1)} $$\nGiven the initial condition $ys  0$, the numerator $-ys$ is positive, ensuring that $\\alpha_{\\min} > 0$, which is consistent with the definition of a learning rate.\n\nTo obtain the final expression, we substitute the definition of the sigmoid function, $\\sigma(-ys) = \\frac{1}{1 + \\exp(ys)}$, and the score, $s = \\mathbf{w}^{\\top}\\mathbf{x} + b$:\n$$ \\alpha_{\\min} = \\frac{-ys}{\\left(\\frac{1}{1 + \\exp(ys)}\\right) (\\mathbf{x}^{\\top}\\mathbf{x} + 1)} $$\n$$ \\alpha_{\\min} = \\frac{-ys (1 + \\exp(ys))}{\\mathbf{x}^{\\top}\\mathbf{x} + 1} $$\nFinally, replacing $s$ with its definition provides the expression in terms of the given variables:\n$$ \\alpha_{\\min} = \\frac{-y(\\mathbf{w}^{\\top}\\mathbf{x} + b)(1 + \\exp(y(\\mathbf{w}^{\\top}\\mathbf{x} + b)))}{\\mathbf{x}^{\\top}\\mathbf{x} + 1} $$\nThis is the closed-form analytical expression for the minimal learning rate that guarantees a corrected classification for the example $(\\mathbf{x}, y)$ after a single update step.", "answer": "$$\\boxed{\\frac{-y(\\mathbf{w}^{\\top}\\mathbf{x} + b)(1 + \\exp(y(\\mathbf{w}^{\\top}\\mathbf{x} + b)))}{\\mathbf{x}^{\\top}\\mathbf{x} + 1}}$$", "id": "3180380"}, {"introduction": "Once we understand the inner update, the crucial next step in MAML is learning a good initialization through meta-optimization. This requires computing a \"meta-gradient\" that differentiates through the entire inner learning process, a computationally intensive step. This practice allows you to dissect this meta-gradient and precisely calculate the second-order component that is ignored in the popular, more efficient \"First-Order MAML\" (FOMAML) approximation. By quantifying this lost term, you will gain a fundamental insight into the trade-offs between the full second-order algorithm and its first-order counterpart [@problem_id:3100440].", "problem": "Consider one step of inner-loop adaptation in Model-Agnostic Meta-Learning (MAML), where the updated parameter is defined by applying one step of gradient descent on a training loss. Let the parameter vector be two-dimensional, $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$, and define the training loss and validation loss by\n$$\n\\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\,\\boldsymbol{\\theta}^{\\top}\\mathbf{A}\\,\\boldsymbol{\\theta}, \n\\quad\n\\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\,\\boldsymbol{\\theta}^{\\top}\\mathbf{C}\\,\\boldsymbol{\\theta} + \\mathbf{r}^{\\top}\\boldsymbol{\\theta},\n$$\nwith\n$$\n\\mathbf{A}=\\begin{pmatrix}3  1 \\\\ 1  2\\end{pmatrix}, \n\\quad\n\\mathbf{C}=\\begin{pmatrix}2  -1 \\\\ -1  4\\end{pmatrix}, \n\\quad\n\\mathbf{r}=\\begin{pmatrix}1 \\\\ -2\\end{pmatrix}.\n$$\nStarting from the initialization $\\boldsymbol{\\theta}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$, perform one inner update with step size $\\alpha=\\frac{1}{2}$:\n$$\n\\boldsymbol{\\theta}' \\;=\\; \\boldsymbol{\\theta} - \\alpha\\,\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta}).\n$$\nThe outer (meta) objective is $\\mathcal{F}(\\boldsymbol{\\theta}) = \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}')$. The true meta-gradient uses the chain rule,\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\mathcal{F}(\\boldsymbol{\\theta}) \\;=\\; \\left(\\frac{\\partial \\boldsymbol{\\theta}'}{\\partial \\boldsymbol{\\theta}}\\right)^{\\top} \\nabla_{\\boldsymbol{\\theta}'} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}').\n$$\nSuppose instead that the inner-loop output $\\boldsymbol{\\theta}'$ is replaced by $\\mathrm{stop\\_grad}(\\boldsymbol{\\theta}')$ (that is, detached from the computational graph in automatic differentiation), and the first-order surrogate meta-gradient is taken to be\n$$\n\\mathbf{g}_{\\mathrm{FO}} \\;=\\; \\nabla_{\\boldsymbol{\\theta}'} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}').\n$$\nDefine the lost second-order contribution due to detaching as the difference\n$$\n\\Delta \\;=\\; \\mathbf{g}_{\\mathrm{FO}} - \\nabla_{\\boldsymbol{\\theta}} \\mathcal{F}(\\boldsymbol{\\theta}).\n$$\nUsing only the fundamental definitions of gradient, Hessian, and the multivariate chain rule from calculus, compute the squared Euclidean norm of the lost term, $\\,\\|\\Delta\\|_{2}^{2}\\,$, for the specified $\\mathcal{L}_{\\mathrm{tr}}$, $\\mathcal{L}_{\\mathrm{val}}$, $\\boldsymbol{\\theta}$, and $\\alpha$. Express your answer exactly as a rational number. No rounding is required.", "solution": "The problem asks for the squared Euclidean norm of the difference between the first-order surrogate meta-gradient and the true meta-gradient in a simplified Model-Agnostic Meta-Learning (MAML) setup. We must first validate the problem statement and, if valid, proceed with a rigorous derivation.\n\n### Problem Validation\nThe problem statement is self-contained and mathematically well-posed. All required variables, matrices, vectors, and initial conditions are explicitly provided.\n- **Parameters and Functions**: A two-dimensional parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$, quadratic training loss $\\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta})$, and quadratic validation loss $\\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta})$.\n- **Constants**: Matrices $\\mathbf{A}$, $\\mathbf{C}$, vector $\\mathbf{r}$, initial parameter $\\boldsymbol{\\theta}$, and step size $\\alpha$. The matrices $\\mathbf{A}$ and $\\mathbf{C}$ are symmetric and positive definite (their determinants are $5$ and $7$, respectively, and their main diagonal elements are positive), ensuring the loss functions are convex, a standard property in optimization problems.\n- **Definitions**: The inner update rule $\\boldsymbol{\\theta}'$, the meta-objective $\\mathcal{F}(\\boldsymbol{\\theta})$, the true meta-gradient $\\nabla_{\\boldsymbol{\\theta}} \\mathcal{F}(\\boldsymbol{\\theta})$, the first-order surrogate $\\mathbf{g}_{\\mathrm{FO}}$, and the difference vector $\\Delta$ are all defined unambiguously.\n- **Scientific Grounding**: The problem is a standard, albeit simplified, representation of MAML, a well-established algorithm in machine learning. The use of gradients, Hessians, and the chain rule are fundamental calculus concepts correctly applied to this context.\n\nThe problem is valid as it is scientifically grounded, well-posed, objective, and contains no contradictions or ambiguities. We may proceed with the solution.\n\n### Step 1: Compute the Updated Parameter $\\boldsymbol{\\theta}'$\nThe inner-loop update is given by $\\boldsymbol{\\theta}' = \\boldsymbol{\\theta} - \\alpha\\,\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta})$.\nThe training loss is a quadratic form $\\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\,\\boldsymbol{\\theta}^{\\top}\\mathbf{A}\\,\\boldsymbol{\\theta}$. Since $\\mathbf{A}$ is a symmetric matrix, its gradient is $\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta}) = \\mathbf{A}\\,\\boldsymbol{\\theta}$.\n\nWe are given the initial parameter vector $\\boldsymbol{\\theta}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$ and the matrix $\\mathbf{A}=\\begin{pmatrix}3  1 \\\\ 1  2\\end{pmatrix}$. First, we compute the gradient of the training loss at $\\boldsymbol{\\theta}$:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta}) = \\begin{pmatrix}3  1 \\\\ 1  2\\end{pmatrix} \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}3(1) + 1(-1) \\\\ 1(1) + 2(-1)\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}.\n$$\nNow, we can compute the updated parameter $\\boldsymbol{\\theta}'$ using the step size $\\alpha=\\frac{1}{2}$:\n$$\n\\boldsymbol{\\theta}' = \\boldsymbol{\\theta} - \\alpha\\,\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta}) = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix}2 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ -1/2\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1/2\\end{pmatrix}.\n$$\n\n### Step 2: Compute the First-Order Surrogate Meta-Gradient $\\mathbf{g}_{\\mathrm{FO}}$\nThe surrogate gradient is defined as $\\mathbf{g}_{\\mathrm{FO}} = \\nabla_{\\boldsymbol{\\theta}'} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}')$.\nThe validation loss is $\\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\,\\boldsymbol{\\theta}^{\\top}\\mathbf{C}\\,\\boldsymbol{\\theta} + \\mathbf{r}^{\\top}\\boldsymbol{\\theta}$. Since $\\mathbf{C}$ is a symmetric matrix, its gradient is $\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}) = \\mathbf{C}\\,\\boldsymbol{\\theta} + \\mathbf{r}$.\nWe evaluate this gradient at the updated parameter $\\boldsymbol{\\theta}' = \\begin{pmatrix}0 \\\\ -1/2\\end{pmatrix}$, using $\\mathbf{C}=\\begin{pmatrix}2  -1 \\\\ -1  4\\end{pmatrix}$ and $\\mathbf{r}=\\begin{pmatrix}1 \\\\ -2\\end{pmatrix}$:\n$$\n\\mathbf{g}_{\\mathrm{FO}} = \\mathbf{C}\\boldsymbol{\\theta}' + \\mathbf{r} = \\begin{pmatrix}2  -1 \\\\ -1  4\\end{pmatrix} \\begin{pmatrix}0 \\\\ -1/2\\end{pmatrix} + \\begin{pmatrix}1 \\\\ -2\\end{pmatrix}.\n$$\n$$\n\\mathbf{g}_{\\mathrm{FO}} = \\begin{pmatrix}2(0) + (-1)(-1/2) \\\\ -1(0) + 4(-1/2)\\end{pmatrix} + \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix}1/2 \\\\ -2\\end{pmatrix} + \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix}3/2 \\\\ -4\\end{pmatrix}.\n$$\n\n### Step 3: Compute the Lost Second-Order Contribution $\\Delta$\nThe lost contribution is defined as the difference $\\Delta = \\mathbf{g}_{\\mathrm{FO}} - \\nabla_{\\boldsymbol{\\theta}} \\mathcal{F}(\\boldsymbol{\\theta})$, where $\\mathcal{F}(\\boldsymbol{\\theta}) = \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}')$ is the meta-objective.\nThe true meta-gradient is given by the multivariate chain rule:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\mathcal{F}(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}'(\\boldsymbol{\\theta})) = \\left(\\frac{\\partial \\boldsymbol{\\theta}'}{\\partial \\boldsymbol{\\theta}}\\right)^{\\top} \\nabla_{\\boldsymbol{\\theta}'} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}').\n$$\nThe second term, $\\nabla_{\\boldsymbol{\\theta}'} \\mathcal{L}_{\\mathrm{val}}(\\boldsymbol{\\theta}')$, is precisely $\\mathbf{g}_{\\mathrm{FO}}$. The first term is the transpose of the Jacobian of $\\boldsymbol{\\theta}'$ with respect to $\\boldsymbol{\\theta}$.\nFrom the update rule $\\boldsymbol{\\theta}' = \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathrm{tr}}(\\boldsymbol{\\theta})$, we find the Jacobian:\n$$\n\\frac{\\partial \\boldsymbol{\\theta}'}{\\partial \\boldsymbol{\\theta}} = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left(\\boldsymbol{\\theta} - \\alpha \\mathbf{A}\\boldsymbol{\\theta}\\right) = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\left((\\mathbf{I} - \\alpha \\mathbf{A})\\boldsymbol{\\theta}\\right) = \\mathbf{I} - \\alpha \\mathbf{A}.\n$$\nThe matrix $\\mathbf{A}$ is symmetric, so $(\\mathbf{I} - \\alpha \\mathbf{A})^{\\top} = \\mathbf{I} - \\alpha \\mathbf{A}$.\nSubstituting this into the expression for the true meta-gradient:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\mathcal{F}(\\boldsymbol{\\theta}) = (\\mathbf{I} - \\alpha \\mathbf{A}) \\mathbf{g}_{\\mathrm{FO}}.\n$$\nNow we can express $\\Delta$ as:\n$$\n\\Delta = \\mathbf{g}_{\\mathrm{FO}} - (\\mathbf{I} - \\alpha \\mathbf{A})\\mathbf{g}_{\\mathrm{FO}} = \\left(\\mathbf{I} - (\\mathbf{I} - \\alpha \\mathbf{A})\\right)\\mathbf{g}_{\\mathrm{FO}} = \\alpha \\mathbf{A} \\mathbf{g}_{\\mathrm{FO}}.\n$$\nThis simplification shows that the lost term is the result of applying the Hessian of the training loss (represented by $\\mathbf{A}$) to the validation gradient, scaled by the learning rate $\\alpha$.\nLet's compute $\\Delta$ using the values we have:\n$$\n\\Delta = \\frac{1}{2} \\begin{pmatrix}3  1 \\\\ 1  2\\end{pmatrix} \\begin{pmatrix}3/2 \\\\ -4\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}3(3/2) + 1(-4) \\\\ 1(3/2) + 2(-4)\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}9/2 - 8/2 \\\\ 3/2 - 16/2\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}1/2 \\\\ -13/2\\end{pmatrix} = \\begin{pmatrix}1/4 \\\\ -13/4\\end{pmatrix}.\n$$\n\n### Step 4: Compute the Squared Euclidean Norm $\\|\\Delta\\|_{2}^{2}$\nThe final step is to compute the squared Euclidean norm of the vector $\\Delta$:\n$$\n\\|\\Delta\\|_{2}^{2} = \\left(\\frac{1}{4}\\right)^2 + \\left(-\\frac{13}{4}\\right)^2 = \\frac{1^2}{4^2} + \\frac{(-13)^2}{4^2} = \\frac{1}{16} + \\frac{169}{16} = \\frac{170}{16}.\n$$\nAs a final step, we simplify the fraction:\n$$\n\\|\\Delta\\|_{2}^{2} = \\frac{170 \\div 2}{16 \\div 2} = \\frac{85}{8}.\n$$", "answer": "$$\n\\boxed{\\frac{85}{8}}\n$$", "id": "3100440"}, {"introduction": "In practice, the choice of the inner-loop optimizer can dramatically alter MAML's behavior and complexity. This final exercise explores how using advanced optimizers like SGD with momentum or Adam, which maintain internal states, complicates the dependency of the adapted parameters on the initialization. By numerically comparing the true meta-gradient with its first-order approximation for these different optimizers, you will develop an intuition for why these second-order effects are non-trivial and why simpler optimizers are often favored in MAML research [@problem_id:3149873].", "problem": "Consider a one-dimensional meta-learning setup with Model-Agnostic Meta-Learning (MAML). A single scalar parameter $\\theta \\in \\mathbb{R}$ is adapted on a task-specific quadratic loss. For any task, define the inner-task loss as $$L(\\theta; a, b) = \\frac{1}{2} a (\\theta - b)^2,$$ where $a \\in \\mathbb{R}_{>0}$ is the curvature and $b \\in \\mathbb{R}$ is the task-specific optimum. The inner loop applies one of two optimizers starting from an initial parameter $\\theta_0$:\n\n- Stochastic Gradient Descent (SGD): One or more steps of the update $\\theta \\leftarrow \\theta - \\alpha \\, g$, where $g = \\nabla_{\\theta} L(\\theta; a,b)$ and $\\alpha \\in \\mathbb{R}_{>0}$ is the step size.\n- Adaptive Moment Estimation (Adam): Two or more steps with first moment $m$ and second moment $v$ initialized to zero, updated as $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$ and $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$, bias-corrected $\\hat{m}_t = m_t / (1-\\beta_1^t)$ and $\\hat{v}_t = v_t / (1-\\beta_2^t)$, with parameter update $\\theta \\leftarrow \\theta - \\alpha \\, \\hat{m}_t / (\\sqrt{\\hat{v}_t} + \\varepsilon)$, where $\\beta_1, \\beta_2 \\in (0,1)$ and $\\varepsilon \\in \\mathbb{R}_{>0}$.\n\nAfter $K$ inner steps, denote the adapted parameter by $\\theta_K(\\theta_0)$ and the meta-objective for the task by $J(\\theta_0) = L(\\theta_K(\\theta_0); a, b)$. The exact meta-gradient with respect to the initial parameter $\\theta_0$ is $\\nabla_{\\theta_0} J(\\theta_0)$ and, by the chain rule, it depends on the Jacobian of the inner update map $\\theta_0 \\mapsto \\theta_K(\\theta_0)$. A common first-order approximation (First-Order MAML) ignores second-order terms by treating $\\theta_K$ as a constant with respect to $\\theta_0$, yielding the approximation $g_{\\mathrm{FO}} = \\nabla_{\\theta} L(\\theta; a,b)\\big\\vert_{\\theta = \\theta_K}$.\n\nYour task is to implement a program that, for each provided test case, computes:\n- The exact meta-gradient $\\nabla_{\\theta_0} J(\\theta_0)$ numerically using a symmetric finite-difference approximation in $\\theta_0$ with a small step $h$ (use the same $h$ for all cases).\n- The first-order approximation $g_{\\mathrm{FO}}$ evaluated at $\\theta_K$.\n- The absolute difference $\\lvert \\nabla_{\\theta_0} J(\\theta_0) - g_{\\mathrm{FO}} \\rvert$, which quantifies the contribution from second-order terms. This scalar will be the result reported for each test case.\n\nScientific base assumptions to use:\n- The gradient of the quadratic loss is $\\nabla_{\\theta} L(\\theta; a,b) = a(\\theta-b)$.\n- The chain rule for differentiation and the definitions of Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam) as specified above hold.\n- Numerical symmetric finite differences for a sufficiently small $h$ provide a consistent approximation to derivatives in one dimension: $$\\nabla_{\\theta_0} J(\\theta_0) \\approx \\frac{J(\\theta_0 + h) - J(\\theta_0 - h)}{2h}.$$\n\nImplement the following optimizers for the inner loop:\n- SGD without momentum: apply $K$ steps with $\\theta \\leftarrow \\theta - \\alpha a (\\theta - b)$.\n- SGD with momentum: maintain a velocity $v$ initialized to $0$, update $v \\leftarrow \\mu v + a (\\theta-b)$ and $\\theta \\leftarrow \\theta - \\alpha v$, with momentum coefficient $\\mu \\in [0,1)$, for $K \\ge 2$ steps.\n- Adam: maintain moments $m$ and $v$ initialized to $0$, update as above with bias correction and parameter update $\\theta \\leftarrow \\theta - \\alpha \\hat{m}/(\\sqrt{\\hat{v}}+\\varepsilon)$, for $K \\ge 2$ steps.\n\nTest Suite:\nProvide results for the following five test cases (each is independent):\n1. Optimizer: SGD, parameters: $a = 1.0$, $b = 2.0$, $\\theta_0 = 0.5$, $\\alpha = 0.1$, $K = 1$.\n2. Optimizer: SGD with momentum, parameters: $a = 1.0$, $b = -1.0$, $\\theta_0 = 0.5$, $\\alpha = 0.1$, $\\mu = 0.9$, $K = 2$.\n3. Optimizer: Adam, parameters: $a = 1.0$, $b = 2.0$, $\\theta_0 = 0.5$, $\\alpha = 0.1$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon = 10^{-8}$, $K = 2$.\n4. Optimizer: SGD, parameters: $a = 10.0$, $b = 1.0$, $\\theta_0 = 0.0$, $\\alpha = 0.01$, $K = 1$.\n5. Optimizer: Adam, parameters: $a = 1.0$, $b = 0.0$, $\\theta_0 = 0.0$, $\\alpha = 0.1$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon = 10^{-8}$, $K = 2$.\n\nComputation and Output Requirements:\n- Use a fixed symmetric finite-difference step size $h = 10^{-6}$ for all numerical derivative approximations with respect to $\\theta_0$.\n- For each test case, compute the scalar absolute difference $\\lvert \\nabla_{\\theta_0} J(\\theta_0) - g_{\\mathrm{FO}} \\rvert$.\n- No physical units are involved.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places (for example, $\"[0.123456,0.000001,0.500000]\"$).", "solution": "The problem requires the calculation of the discrepancy between the exact meta-gradient and its first-order approximation (FOMAML) in a one-dimensional meta-learning context. For several test cases, each defined by a specific inner-loop optimizer and its parameters, we must calculate the absolute difference $\\lvert \\nabla_{\\theta_0} J(\\theta_0) - g_{\\mathrm{FO}} \\rvert$.\n\n**1. Theoretical Framework**\n\nThe core of Model-Agnostic Meta-Learning (MAML) involves a two-level optimization process. The inner loop adapts a model's parameters to a specific task, while the outer loop updates the initial parameters based on the performance after adaptation.\n\nLet the initial parameter be $\\theta_0 \\in \\mathbb{R}$. The inner loop performs $K$ steps of an optimization algorithm to minimize a task-specific loss function, $L(\\theta; a, b) = \\frac{1}{2} a (\\theta - b)^2$. This process maps the initial parameter $\\theta_0$ to an adapted parameter $\\theta_K$. We can represent this mapping as $\\theta_K(\\theta_0)$, where the function $\\theta_K(\\cdot)$ represents the $K$ steps of the chosen optimizer starting from its argument.\n\nThe meta-objective, or outer-loop loss, is the loss evaluated at the adapted parameter:\n$$J(\\theta_0) = L(\\theta_K(\\theta_0); a, b) = \\frac{1}{2} a (\\theta_K(\\theta_0) - b)^2$$\n\nThe goal of the meta-optimizer is to minimize $J(\\theta_0)$ with respect to $\\theta_0$. This requires computing the meta-gradient, $\\nabla_{\\theta_0} J(\\theta_0)$. Using the chain rule, the exact meta-gradient is:\n$$\\nabla_{\\theta_0} J(\\theta_0) = \\frac{d}{d \\theta_0} L(\\theta_K(\\theta_0); a, b) = \\left( \\frac{d L}{d \\theta} \\bigg|_{\\theta=\\theta_K} \\right) \\cdot \\left( \\frac{d \\theta_K}{d \\theta_0} \\right)$$\nThe term $\\frac{d L}{d \\theta} |_{\\theta=\\theta_K}$ is simply the gradient of the loss at the adapted parameter, which is $a(\\theta_K - b)$. The term $\\frac{d \\theta_K}{d \\theta_0}$ is the Jacobian of the inner-loop update map. This Jacobian contains second-order information, as it describes how the final parameter $\\theta_K$ changes with respect to the initial parameter $\\theta_0$.\n\nThe First-Order MAML (FOMAML) approximation simplifies this by ignoring the second-order term, effectively treating $\\theta_K$ as a constant with respect to $\\theta_0$ during differentiation of the outer objective. This is equivalent to setting the Jacobian term to $1$: $\\frac{d \\theta_K}{d \\theta_0} \\approx 1$. This yields the approximate gradient:\n$$g_{\\mathrm{FO}} = \\nabla_{\\theta} L(\\theta; a,b)\\big\\vert_{\\theta = \\theta_K} = a(\\theta_K - b)$$\n\nThe quantity we must compute, $\\lvert \\nabla_{\\theta_0} J(\\theta_0) - g_{\\mathrm{FO}} \\rvert$, measures the magnitude of the terms ignored by the first-order approximation.\n\n**2. Numerical Computation Strategy**\n\nWhile $g_{\\mathrm{FO}}$ is straightforward to compute, the exact meta-gradient $\\nabla_{\\theta_0} J(\\theta_0)$ is more complex due to the Jacobian term $\\frac{d \\theta_K}{d \\theta_0}$. Instead of deriving this term analytically for each optimizer, the problem specifies a numerical approach using the symmetric finite difference formula:\n$$\\nabla_{\\theta_0} J(\\theta_0) \\approx \\frac{J(\\theta_0 + h) - J(\\theta_0 - h)}{2h}$$\nwhere $h$ is a small step size, given as $h = 10^{-6}$. This approach approximates the derivative by evaluating the meta-objective function $J$ at two points infinitesimally close to $\\theta_0$.\n\n**3. Algorithmic Procedure**\n\nTo implement the solution, we will follow these steps for each test case:\n\n**Step 3.1: Implement the Inner-Loop Optimizers**\nWe must first implement the function that maps $\\theta_0$ to $\\theta_K$ for the three specified optimizers. Let $g_k = a(\\theta_k - b)$ be the gradient at step $k$.\n\n-   **SGD (without momentum)**: For $k = 0, \\dots, K-1$:\n    $$\\theta_{k+1} = \\theta_k - \\alpha g_k$$\n-   **SGD with momentum**: With velocity $v_0 = 0$: For $k = 0, \\dots, K-1$:\n    $$v_{k+1} = \\mu v_k + g_k$$\n    $$\\theta_{k+1} = \\theta_k - \\alpha v_{k+1}$$\n-   **Adam**: With moments $m_0 = 0, v_0 = 0$: For $t = 1, \\dots, K$, let $g_{t-1} = a(\\theta_{t-1}-b)$:\n    $$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_{t-1}$$\n    $$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_{t-1}^2$$\n    $$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n    $$\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$$\n\n**Step 3.2: Compute the FOMAML Gradient ($g_{\\mathrm{FO}}$)**\n1.  Set the initial parameter to the value specified in the test case, $\\theta_0$.\n2.  Run the corresponding inner-loop optimizer for $K$ steps to obtain the adapted parameter $\\theta_K(\\theta_0)$.\n3.  Calculate $g_{\\mathrm{FO}} = a(\\theta_K(\\theta_0) - b)$.\n\n**Step 3.3: Compute the \"Exact\" Meta-Gradient ($\\nabla_{\\theta_0} J(\\theta_0)$)**\n1.  Define a function for the meta-objective, $J(\\theta_{val})$, which takes a starting parameter $\\theta_{val}$, runs the inner-loop optimizer to get $\\theta_K(\\theta_{val})$, and returns the final loss $L(\\theta_K(\\theta_{val}); a, b)$.\n2.  Evaluate this function at two points: $J(\\theta_0 + h)$ and $J(\\theta_0 - h)$.\n3.  Compute the numerical gradient using the finite difference formula: $\\nabla_{\\theta_0} J(\\theta_0) = (J(\\theta_0 + h) - J(\\theta_0 - h)) / (2h)$.\n\n**Step 3.4: Calculate the Final Difference**\nThe final result for the test case is the absolute difference between the two computed gradients: $\\lvert \\nabla_{\\theta_0} J(\\theta_0) - g_{\\mathrm{FO}} \\rvert$. This process is repeated for all five test cases to generate the required output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference between the exact MAML gradient and the FOMAML\n    approximation for a series of test cases.\n    \"\"\"\n    \n    # Global constant for numerical differentiation\n    h = 1e-6\n\n    # Test cases defined in the problem statement.\n    test_cases = [\n        # 1. Optimizer: SGD\n        {'optimizer': 'sgd', 'params': {'a': 1.0, 'b': 2.0, 'theta_0': 0.5, 'alpha': 0.1, 'K': 1}},\n        # 2. Optimizer: SGD with momentum\n        {'optimizer': 'sgd_momentum', 'params': {'a': 1.0, 'b': -1.0, 'theta_0': 0.5, 'alpha': 0.1, 'mu': 0.9, 'K': 2}},\n        # 3. Optimizer: Adam\n        {'optimizer': 'adam', 'params': {'a': 1.0, 'b': 2.0, 'theta_0': 0.5, 'alpha': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'K': 2}},\n        # 4. Optimizer: SGD\n        {'optimizer': 'sgd', 'params': {'a': 10.0, 'b': 1.0, 'theta_0': 0.0, 'alpha': 0.01, 'K': 1}},\n        # 5. Optimizer: Adam\n        {'optimizer': 'adam', 'params': {'a': 1.0, 'b': 0.0, 'theta_0': 0.0, 'alpha': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'K': 2}},\n    ]\n\n    results = []\n    for case in test_cases:\n        optimizer_type = case['optimizer']\n        params = case['params']\n\n        # Define helper functions that capture the parameters of the current test case.\n        def loss_gradient(theta, a, b):\n            return a * (theta - b)\n\n        def loss_function(theta, a, b):\n            return 0.5 * a * (theta - b)**2\n\n        def run_inner_loop(theta_start):\n            \"\"\"\n            Runs the inner optimization loop for K steps starting from theta_start.\n            \"\"\"\n            theta = theta_start\n            a = params['a']\n            b = params['b']\n            k_steps = params['K']\n            alpha = params['alpha']\n\n            if optimizer_type == 'sgd':\n                for _ in range(k_steps):\n                    grad = loss_gradient(theta, a, b)\n                    theta = theta - alpha * grad\n                return theta\n\n            elif optimizer_type == 'sgd_momentum':\n                mu = params['mu']\n                velocity = 0.0\n                for _ in range(k_steps):\n                    grad = loss_gradient(theta, a, b)\n                    velocity = mu * velocity + grad\n                    theta = theta - alpha * velocity\n                return theta\n\n            elif optimizer_type == 'adam':\n                beta1 = params['beta1']\n                beta2 = params['beta2']\n                epsilon = params['epsilon']\n                m = 0.0\n                v = 0.0\n                for t in range(1, k_steps + 1):\n                    grad = loss_gradient(theta, a, b)\n                    m = beta1 * m + (1 - beta1) * grad\n                    v = beta2 * v + (1 - beta2) * grad**2\n                    m_hat = m / (1 - beta1**t)\n                    v_hat = v / (1 - beta2**t)\n                    theta = theta - alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n                return theta\n            \n            raise ValueError(f\"Unknown optimizer: {optimizer_type}\")\n\n        def meta_objective(theta_0_val):\n            \"\"\"\n            Calculates the meta-objective J(theta_0) = L(theta_K(theta_0)).\n            \"\"\"\n            theta_k = run_inner_loop(theta_0_val)\n            return loss_function(theta_k, params['a'], params['b'])\n\n        # --- Main computation for the current test case ---\n        theta_0 = params['theta_0']\n        a = params['a']\n        b = params['b']\n\n        # 1. Compute the First-Order MAML gradient (g_FO)\n        theta_k_at_center = run_inner_loop(theta_0)\n        g_fo = loss_gradient(theta_k_at_center, a, b)\n\n        # 2. Compute the exact meta-gradient numerically\n        J_plus_h = meta_objective(theta_0 + h)\n        J_minus_h = meta_objective(theta_0 - h)\n        grad_exact = (J_plus_h - J_minus_h) / (2 * h)\n\n        # 3. Calculate the absolute difference and store it\n        diff = np.abs(grad_exact - g_fo)\n        results.append(diff)\n\n    # Format results to six decimal places and print in the required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3149873"}]}