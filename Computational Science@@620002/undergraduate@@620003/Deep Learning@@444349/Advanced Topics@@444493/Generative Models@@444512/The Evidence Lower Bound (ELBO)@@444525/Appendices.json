{"hands_on_practices": [{"introduction": "The first step to truly understanding the Evidence Lower Bound is to derive it from first principles. This exercise guides you through the complete derivation for a Variational Autoencoder (VAE) with a Gaussian likelihood, breaking the ELBO down into its two fundamental components: the expected reconstruction term and the Kullback-Leibler (KL) divergence. By analyzing the role of the observation noise variance, $\\sigma^2$, you will gain a deeper intuition for the critical trade-off between data fidelity and latent space regularization. [@problem_id:3184516]", "problem": "Consider a Variational Autoencoder (VAE) with a deterministic decoder under Gaussian observation noise for a single data point $x \\in \\mathbb{R}^{d}$ and a latent variable $z \\in \\mathbb{R}^{k}$. The generative model is specified by a standard normal prior $p(z)$ and a Gaussian likelihood $p(x \\mid z)$ with fixed, isotropic noise variance. Concretely, assume\n- $p(z) = \\mathcal{N}(0, I_{k})$,\n- $p(x \\mid z) = \\mathcal{N}(W z + b, \\sigma^{2} I_{d})$, where $W \\in \\mathbb{R}^{d \\times k}$ and $b \\in \\mathbb{R}^{d}$ are decoder parameters and $\\sigma^{2} > 0$ is the observation noise variance,\n- a mean-field Gaussian variational posterior $q(z \\mid x) = \\mathcal{N}(m, \\operatorname{diag}(s^{2}))$, where $m \\in \\mathbb{R}^{k}$ and $s^{2} \\in \\mathbb{R}_{>0}^{k}$.\n\nStarting from first principles, namely Bayes’ rule, the definition of Kullback–Leibler (KL) divergence, and Jensen’s inequality, derive an explicit Evidence Lower Bound (ELBO) on $\\log p(x)$ for this model that is fully expressed in terms of $x$, $W$, $b$, $m$, $s^{2}$, $\\sigma^{2}$, $d$, and $k$. Your derivation should explicitly evaluate the expectation over $q(z \\mid x)$ for the Gaussian likelihood and should reduce the KL divergence between the two Gaussians to a closed form that depends only on $m$ and $s^{2}$. Then, analyze how the observation noise variance $\\sigma^{2}$ balances the contribution of the reconstruction quality against the KL divergence by identifying the coefficient that scales the expected squared reconstruction error and by explaining qualitatively how changing $\\sigma^{2}$ affects this balance.\n\nYour final answer must be a single, closed-form analytic expression for the ELBO for the single data point $x$ in terms of $x$, $W$, $b$, $m$, $s^{2}$, $\\sigma^{2}$, $d$, and $k$. Do not include any intermediate steps or explanations in the final answer. No rounding is required.", "solution": "The objective is to derive the Evidence Lower Bound (ELBO) on the log-marginal likelihood $\\log p(x)$ for a specified Variational Autoencoder (VAE) model. The derivation will proceed from first principles.\n\nLet $x \\in \\mathbb{R}^{d}$ be a single data point and $z \\in \\mathbb{R}^{k}$ be a latent variable. The log-marginal likelihood $\\log p(x)$ can be rewritten by introducing an arbitrary variational distribution $q(z \\mid x)$:\n$$\n\\log p(x) = \\log \\int p(x, z) dz = \\log \\int p(x, z) \\frac{q(z \\mid x)}{q(z \\mid x)} dz = \\log \\left( \\mathbb{E}_{q(z \\mid x)} \\left[ \\frac{p(x, z)}{q(z \\mid x)} \\right] \\right)\n$$\nThe logarithm is a concave function, so by applying Jensen's inequality, $\\log(\\mathbb{E}[Y]) \\ge \\mathbb{E}[\\log Y]$, we obtain a lower bound on $\\log p(x)$:\n$$\n\\log p(x) \\ge \\mathbb{E}_{q(z \\mid x)} \\left[ \\log \\left( \\frac{p(x, z)}{q(z \\mid x)} \\right) \\right] \\equiv \\mathcal{L}(x)\n$$\nThis lower bound $\\mathcal{L}(x)$ is the ELBO. Using the product rule of probability, $p(x, z) = p(x \\mid z) p(z)$, we can expand the ELBO:\n$$\n\\mathcal{L}(x) = \\mathbb{E}_{q(z \\mid x)} [\\log p(x \\mid z) + \\log p(z) - \\log q(z \\mid x)]\n$$\nRearranging the terms allows us to express the ELBO as the difference between an expected log-likelihood (reconstruction fidelity) term and a Kullback-Leibler (KL) divergence (regularization) term:\n$$\n\\mathcal{L}(x) = \\mathbb{E}_{q(z \\mid x)} [\\log p(x \\mid z)] - (\\mathbb{E}_{q(z \\mid x)} [\\log q(z \\mid x)] - \\mathbb{E}_{q(z \\mid x)} [\\log p(z)]) = \\mathbb{E}_{q(z \\mid x)} [\\log p(x \\mid z)] - D_{KL}(q(z \\mid x) \\| p(z))\n$$\nWe now proceed to derive the closed-form expressions for each of these two terms based on the given model specifications.\n\n**1. Evaluation of the Expected Log-Likelihood Term**\n\nThe likelihood distribution is given as $p(x \\mid z) = \\mathcal{N}(x; W z + b, \\sigma^{2} I_{d})$. The log-likelihood is:\n$$\n\\log p(x \\mid z) = \\log \\left( (2\\pi\\sigma^2)^{-d/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|x - (Wz + b)\\|_2^2\\right) \\right) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - Wz - b\\|_2^2\n$$\nWe take the expectation of this expression with respect to the variational posterior $q(z \\mid x) = \\mathcal{N}(z; m, S)$, where $S = \\operatorname{diag}(s^2)$.\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = \\mathbb{E}_{q} \\left[ -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - Wz - b\\|_2^2 \\right] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathbb{E}_{q} [\\|x - Wz - b\\|_2^2]\n$$\nThe expected squared norm can be expanded. Let $\\mathbb{E}_q[z] = m$ and $\\operatorname{Cov}_q[z] = \\mathbb{E}_q[(z-m)(z-m)^T] = S$.\n$$\n\\mathbb{E}_{q} [\\|x - Wz - b\\|_2^2] = \\mathbb{E}_{q} [\\|(x - Wm - b) - W(z - m)\\|_2^2]\n$$\nExpanding the square and noting that the cross-term expectation $\\mathbb{E}_{q}[-2(x-Wm-b)^T W (z-m)]$ is zero because $\\mathbb{E}_{q}[z-m] = 0$:\n$$\n\\mathbb{E}_{q}[ \\cdots ] = \\|x - Wm - b\\|_2^2 + \\mathbb{E}_{q} [(z-m)^T W^T W (z-m)]\n$$\nThe expectation of a quadratic form is $\\mathbb{E}[v^T A v] = \\operatorname{Tr}(A \\operatorname{Cov}[v]) + \\mathbb{E}[v]^T A \\mathbb{E}[v]$. Here, $v=z-m$, so $\\mathbb{E}[v]=0$. Thus:\n$$\n\\mathbb{E}_{q} [(z-m)^T W^T W (z-m)] = \\operatorname{Tr}(W^T W S) = \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2))\n$$\nSo, the full expected squared norm is:\n$$\n\\mathbb{E}_{q} [\\|x - Wz - b\\|_2^2] = \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2))\n$$\nSubstituting this back, the expected log-likelihood becomes:\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2)) \\right)\n$$\n\n**2. Evaluation of the KL Divergence Term**\n\nThe KL divergence is between the variational posterior $q(z \\mid x) = \\mathcal{N}(z; m, \\operatorname{diag}(s^2))$ and the prior $p(z) = \\mathcal{N}(z; 0, I_k)$. The general formula for the KL divergence between two $k$-dimensional Gaussians $q = \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $p = \\mathcal{N}(\\mu_2, \\Sigma_2)$ is:\n$$\nD_{KL}(q \\| p) = \\frac{1}{2} \\left( \\operatorname{Tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1} (\\mu_2 - \\mu_1) - k + \\log\\frac{\\det \\Sigma_2}{\\det \\Sigma_1} \\right)\n$$\nWith our parameters $\\mu_1 = m$, $\\Sigma_1 = \\operatorname{diag}(s^2)$, $\\mu_2 = 0$, and $\\Sigma_2 = I_k$, we get:\n-   Trace term: $\\operatorname{Tr}(I_k^{-1} \\operatorname{diag}(s^2)) = \\operatorname{Tr}(\\operatorname{diag}(s^2)) = \\sum_{j=1}^k s_j^2$.\n-   Quadratic term: $(0 - m)^T I_k^{-1} (0 - m) = m^T m = \\|m\\|_2^2$.\n-   Log-determinant term: $\\log\\frac{\\det I_k}{\\det(\\operatorname{diag}(s^2))} = \\log(1) - \\log(\\prod_{j=1}^k s_j^2) = -\\sum_{j=1}^k \\log(s_j^2)$.\n\nSubstituting these into the formula provides the closed-form expression for the KL divergence:\n$$\nD_{KL}(q(z \\mid x) \\| p(z)) = \\frac{1}{2} \\left( \\sum_{j=1}^k s_j^2 + \\|m\\|_2^2 - k - \\sum_{j=1}^k \\log(s_j^2) \\right)\n$$\n\n**3. Complete ELBO Expression**\n\nBy combining the two derived components, $\\mathcal{L}(x) = \\mathbb{E}_{q}[\\log p(x \\mid z)] - D_{KL}(q \\| p)$, we obtain the final explicit ELBO for the given VAE model:\n$$\n\\mathcal{L}(x) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2)) \\right) - \\frac{1}{2} \\left( \\|m\\|_2^2 + \\sum_{j=1}^k s_j^2 - k - \\sum_{j=1}^k \\log(s_j^2) \\right)\n$$\n\n**4. Analysis of the Role of Observation Noise Variance $\\sigma^2$**\n\nIn the VAE framework, the parameters of the model (including $W, b$ and the parameters of the encoder that produces $m, s^2$) are optimized to maximize the ELBO, which is equivalent to minimizing the negative ELBO, $L(x) = -\\mathcal{L}(x)$. The term in the ELBO related to reconstruction is the expected log-likelihood, $\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)]$. The part of this term that corresponds to the reconstruction error is $-\\frac{1}{2\\sigma^2} \\mathbb{E}_{q(z \\mid x)}[\\|x - (Wz+b)\\|_2^2]$.\n\nThe coefficient that scales the expected squared reconstruction error, $\\mathbb{E}_{q(z \\mid x)}[\\|x - (Wz+b)\\|_2^2]$, is $\\frac{1}{2\\sigma^2}$. This coefficient governs the trade-off between the reconstruction quality and the KL divergence term, which acts as a regularizer.\n\n-   When $\\sigma^2$ is small, the coefficient $\\frac{1}{2\\sigma^2}$ is large. This places a high penalty on reconstruction errors in the loss function $L(x)$. The optimization procedure is driven to make the model's reconstructions very accurate, i.e., to minimize $\\|x - (Wz+b)\\|_2^2$. This prioritizes data fidelity, potentially at the cost of forcing the variational posterior $q(z \\mid x)$ to deviate far from the prior $p(z)$, thus increasing the KL divergence.\n\n-   When $\\sigma^2$ is large, the coefficient $\\frac{1}{2\\sigma^2}$ is small. This lessens the penalty on reconstruction errors, allowing for less precise, or \"blurrier,\" reconstructions. The optimization then places greater relative importance on minimizing the KL divergence term, which encourages the latent representations encoded by $q(z \\mid x)$ to conform more closely to the structure of the standard normal prior $p(z)$.\n\nTherefore, the observation noise variance $\\sigma^2$ acts as a crucial hyperparameter that balances the two primary objectives of the VAE: achieving high-quality reconstructions and maintaining a regularized, structured latent space.", "answer": "$$\n\\boxed{-\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2)) \\right) - \\frac{1}{2} \\left( \\|m\\|_2^2 + \\sum_{j=1}^k s_j^2 - k - \\sum_{j=1}^k \\log(s_j^2) \\right)}\n$$", "id": "3184516"}, {"introduction": "After dissecting the ELBO's mathematical form, it is crucial to solidify your conceptual understanding of why each component is necessary. This thought experiment challenges you to consider the consequences of removing stochasticity from the latent space by setting its variance to zero. By analyzing this hypothetical scenario, you will uncover the fundamental distinction between a VAE and a deterministic autoencoder and appreciate the indispensable role the KL divergence plays in structuring the latent space for generative tasks. [@problem_id:2439791]", "problem": "A Variational Autoencoder (VAE) is trained on single-cell RNA sequencing (scRNA-seq) gene expression data, where each cell is represented by a vector $x \\in \\mathbb{N}^G$. The encoder defines an approximate posterior $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2(x)))$ over a latent variable $z \\in \\mathbb{R}^d$, with prior $p(z) = \\mathcal{N}(0, I)$. The decoder specifies a likelihood $p_{\\theta}(x \\mid z)$ appropriate for counts. Training maximizes the evidence lower bound (ELBO),\n$$\n\\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] - D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\nusing the reparameterization $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, I)$. Consider modifying training so that, during the latent sampling step, the variance is set to zero, i.e., $z$ is set deterministically to $z = \\mu_{\\phi}(x)$ for every $x$.\n\nWhich statement best describes the consequence of this modification for learning on biological data and the underlying objective?\n\nA. The model effectively becomes a deterministic autoencoder with $z=\\mu_{\\phi}(x)$, destroying stochasticity and uncertainty calibration; as $\\sigma_{\\phi}(x) \\to 0$ the Kullback–Leibler divergence $D_{\\mathrm{KL}}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$ diverges, and generative diversity for synthetic cells collapses.\n\nB. The removal of noise improves sample diversity because the decoder receives cleaner latent codes, and training is more stable without affecting the variational objective.\n\nC. It induces posterior collapse by enforcing $q_{\\phi}(z \\mid x)$ to match the prior with $\\mu_{\\phi}(x)\\approx 0$ and $\\sigma_{\\phi}(x)\\approx 1$, which enhances disentanglement and generation.\n\nD. Eliminating randomness in $z$ makes the evidence lower bound equal to the true log-evidence, because the Monte Carlo estimation error vanishes.\n\nE. It only speeds up training; uncertainty estimates and generative properties are unaffected because the decoder can reintroduce variability.", "solution": "The problem statement will be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Model**: A Variational Autoencoder (VAE).\n-   **Data**: Single-cell RNA sequencing (scRNA-seq) gene expression data, represented by a vector $x \\in \\mathbb{N}^G$. The variable $G$ represents the number of genes.\n-   **Encoder (Approximate Posterior)**: $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2(x)))$.\n-   **Latent Variable**: $z \\in \\mathbb{R}^d$, where $d$ is the dimensionality of the latent space.\n-   **Prior Distribution**: $p(z) = \\mathcal{N}(0, I)$, a standard multivariate normal distribution.\n-   **Decoder (Likelihood)**: $p_{\\theta}(x \\mid z)$, specified as a distribution appropriate for count data.\n-   **Objective Function**: Maximization of the Evidence Lower Bound (ELBO):\n    $$\n    \\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] - D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n    $$\n-   **Training Method**: The reparameterization trick is used for sampling: $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, I)$.\n-   **Proposed Modification**: During the latent sampling step, the variance is set to zero, which means the latent variable is set deterministically as $z = \\mu_{\\phi}(x)$ for every input $x$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n\n-   **Scientifically Grounded**: The problem describes a standard VAE architecture and training objective. Its application to scRNA-seq data is a well-established and important area in computational biology. The mathematical formulations of the encoder, prior, ELBO, and reparameterization trick are all correct and standard. The proposed modification is a hypothetical \"what-if\" scenario designed to test the understanding of the VAE's fundamental principles, which is a valid scientific and pedagogical approach. The problem is firmly based on established principles of machine learning and statistics.\n-   **Well-Posed**: The question asks for the consequences of a specific modification. This is a clear and unambiguous question that has a definite answer derivable from the mathematical definition of the VAE.\n-   **Objective**: The language is formal, precise, and free of subjective claims.\n\nThe problem is scientifically sound, self-contained, and well-posed. No inconsistencies or logical flaws are present.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Solution\nThe problem asks for the consequences of modifying the VAE training process by setting the latent variable $z$ deterministically to $z = \\mu_{\\phi}(x)$. This is equivalent to taking the limit of the variance of the approximate posterior, $\\sigma_{\\phi}^2(x)$, to zero. We must analyze the effect of this modification on the two terms of the ELBO objective function.\n\nThe ELBO is given by:\n$$\n\\mathcal{L}(\\theta,\\phi;x) = \\underbrace{\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right]}_{\\text{Reconstruction Term}} - \\underbrace{D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)}_{\\text{Regularization Term}}\n$$\n\n$1$. **Analysis of the Reconstruction Term**:\nThe reconstruction term is the expected log-likelihood of the data given the latent variable. The expectation is taken over the approximate posterior $q_{\\phi}(z \\mid x)$. When we enforce $z = \\mu_{\\phi}(x)$, we are effectively replacing the distribution $q_{\\phi}(z \\mid x)$ with a Dirac delta function centered at $\\mu_{\\phi}(x)$. The expectation over a Dirac delta function simplifies to an evaluation at its center. Therefore, the reconstruction term becomes:\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\longrightarrow \\log p_{\\theta}(x \\mid z=\\mu_{\\phi}(x))\n$$\nThis is the objective of a standard, deterministic autoencoder. The encoder maps an input $x$ to a single point $\\mu_{\\phi}(x)$ in the latent space, and the decoder attempts to reconstruct $x$ from that point. This change completely removes the stochasticity from the latent encoding process, and with it, the model's ability to represent uncertainty in the latent space (i.e., uncertainty about the \"true\" cellular state for an observed expression profile $x$).\n\n$2$. **Analysis of the Regularization Term**:\nThe regularization term is the Kullback–Leibler (KL) divergence between the approximate posterior $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2(x)))$ and the prior $p(z) = \\mathcal{N}(0, I)$. The analytical expression for this KL divergence is:\n$$\nD_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right) = \\frac{1}{2} \\sum_{j=1}^{d} \\left( \\mu_{\\phi,j}^2(x) + \\sigma_{\\phi,j}^2(x) - 1 - \\log(\\sigma_{\\phi,j}^2(x)) \\right)\n$$\nwhere the sum is over the $d$ dimensions of the latent space.\nThe modification implies that $\\sigma_{\\phi,j}^2(x) \\to 0$ for all dimensions $j=1, \\dots, d$. Let us examine the component terms in the limit:\n-   $\\mu_{\\phi,j}^2(x)$ remains finite.\n-   $\\sigma_{\\phi,j}^2(x) \\to 0$.\n-   $\\log(\\sigma_{\\phi,j}^2(x)) \\to -\\infty$.\n\nTherefore, the term $-\\log(\\sigma_{\\phi,j}^2(x))$ approaches $+\\infty$. As a result, the entire KL divergence diverges to positive infinity:\n$$\n\\lim_{\\sigma_{\\phi}^2(x) \\to 0} D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right) = \\infty\n$$\nSince the KL divergence term is subtracted in the ELBO, the objective function $\\mathcal{L}(\\theta,\\phi;x)$ would approach $-\\infty$. For a maximization problem, this is a catastrophic failure. The optimization process would be dominated by an infinite penalty against learning zero variance.\n\n$3$. **Consequences for Model Properties**:\n-   **Generative Capability**: A key purpose of the KL regularization is to force the aggregate posterior distribution of all data points to approximate the prior $p(z)$. This structures the latent space, ensuring it is dense and continuous, which allows for meaningful generation of new data by sampling a latent vector $z'$ from the prior $p(z)$ and decoding it with $p_{\\theta}(x \\mid z')$. By effectively removing this regularization (or making it an infinite penalty), the encoder is free to place the latent means $\\mu_{\\phi}(x)$ anywhere in the latent space to optimize reconstruction. This will lead to a \"fractured\" or \"gappy\" latent space where most points drawn from the prior $p(z)$ do not correspond to any learned data manifold, causing the decoder to produce nonsensical outputs. The generative diversity collapses.\n-   **Uncertainty Calibration**: VAEs model uncertainty by learning a distribution $q_{\\phi}(z \\mid x)$ for each data point. The variance $\\sigma_{\\phi}^2(x)$ quantifies the uncertainty of the latent representation. Setting this variance to zero by definition destroys this capability.\n\n### Option-by-Option Analysis\n\n**A. The model effectively becomes a deterministic autoencoder with $z=\\mu_{\\phi}(x)$, destroying stochasticity and uncertainty calibration; as $\\sigma_{\\phi}(x) \\to 0$ the Kullback–Leibler divergence $D_{\\mathrm{KL}}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$ diverges, and generative diversity for synthetic cells collapses.**\n-   `becomes a deterministic autoencoder with z=mu_phi(x)`: Correct, as shown in the analysis of the reconstruction term.\n-   `destroying stochasticity and uncertainty calibration`: Correct. The posterior becomes a point mass.\n-   `as sigma_phi(x) -> 0 the ... KL divergence ... diverges`: Correct, as derived from the analytical formula for KL divergence.\n-   `generative diversity ... collapses`: Correct, due to the loss of the regularizing effect of the KL term.\nThis statement accurately summarizes all the key consequences. **Correct**.\n\n**B. The removal of noise improves sample diversity because the decoder receives cleaner latent codes, and training is more stable without affecting the variational objective.**\n-   `improves sample diversity`: Incorrect. It destroys generative capability and collapses diversity.\n-   `decoder receives cleaner latent codes`: While technically true that the latent code is no longer noisy, this is detrimental to the VAE framework.\n-   `training is more stable`: Incorrect. The diverging KL term would make training numerically unstable.\n-   `without affecting the variational objective`: Incorrect. It fundamentally alters the objective, making it ill-defined.\n**Incorrect**.\n\n**C. It induces posterior collapse by enforcing $q_{\\phi}(z \\mid x)$ to match the prior with $\\mu_{\\phi}(x)\\approx 0$ and $\\sigma_{\\phi}(x)\\approx 1$, which enhances disentanglement and generation.**\n-   `induces posterior collapse`: Incorrect. Posterior collapse occurs when the posterior $q_{\\phi}(z \\mid x)$ becomes equal to the prior $p(z)$ for all $x$, meaning $\\mu_{\\phi}(x) \\approx 0$ and $\\sigma_{\\phi}(x) \\approx 1$. The proposed modification forces $\\sigma_{\\phi}(x) \\to 0$, which is the opposite of posterior collapse.\n-   `enhances disentanglement and generation`: Incorrect. Both properties are severely degraded or destroyed.\n**Incorrect**.\n\n**D. Eliminating randomness in $z$ makes the evidence lower bound equal to the true log-evidence, because the Monte Carlo estimation error vanishes.**\n-   This statement confuses two different concepts. The gap between the ELBO and the true log-evidence $\\log p(x)$ is the KL divergence between the approximate posterior and the true posterior, $D_{\\mathrm{KL}}(q_{\\phi}(z \\mid x) \\| p_{\\theta}(z \\mid x))$. This gap does not vanish; a delta function in $q$ is generally a worse approximation of the true posterior than a learned Gaussian, so the gap would likely increase. Monte Carlo estimation error refers to the error in estimating the gradient of the reconstruction term, not the value of the ELBO itself relative to the log-evidence.\n**Incorrect**.\n\n**E. It only speeds up training; uncertainty estimates and generative properties are unaffected because the decoder can reintroduce variability.**\n-   `only speeds up training`: Incorrect. The primary effects are on the model's fundamental properties. Furthermore, training would likely become unstable, not faster.\n-   `uncertainty estimates ... are unaffected`: Incorrect. They are completely destroyed.\n-   `generative properties are unaffected`: Incorrect. They collapse.\n-   `decoder can reintroduce variability`: The decoder models observation noise, which is fundamentally different from latent variable uncertainty and the mechanism for generating novel data types.\n**Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2439791"}, {"introduction": "This final practice showcases the power and flexibility of the ELBO as a framework for principled inference in complex scenarios. You will tackle the common real-world challenge of missing data by deriving and implementing an ELBO that correctly marginalizes over the unobserved variables. This exercise not only demonstrates how generative models can \"reason\" about missing information but also provides a concrete verification of the core theoretical guarantee that the ELBO equals the true log-evidence when the optimal posterior is used. [@problem_id:3184447]", "problem": "Consider a latent-variable generative model used in deep learning for images, and suppose a subset of pixels is missing due to occlusion or corruption. The goal is to derive a principled treatment of missing data in the Evidence Lower Bound (ELBO) and to verify the derivation numerically on small Modified National Institute of Standards and Technology (MNIST) dataset-inspired toy images with masks that remove some pixels.\n\nStart from the following fundamental definitions and facts:\n- Bayes' rule: for random variables $x$ and $z$, $p(z \\mid x) = \\dfrac{p(x \\mid z) p(z)}{p(x)}$.\n- The marginal likelihood integrates out the latent variables: $p(x) = \\int p(x \\mid z) p(z) \\, dz$.\n- Jensen's inequality: for any integrable function $f$ and random variable $Z$, $\\log \\mathbb{E}[f(Z)] \\geq \\mathbb{E}[\\log f(Z)]$.\n\nAssume a linear-Gaussian latent-variable model with the following specifications:\n- Latent dimension $k = 2$ and data dimension $d = 16$.\n- Prior $p(z) = \\mathcal{N}(0, I_k)$.\n- Decoder (likelihood) $p(x \\mid z) = \\mathcal{N}(W z + b, \\sigma_x^2 I_d)$, where $W \\in \\mathbb{R}^{d \\times k}$ and $b \\in \\mathbb{R}^{d}$ are fixed parameters, and $\\sigma_x^2 > 0$ is a known noise variance.\n\nLet $m \\in \\{0,1\\}^d$ be a binary mask that indicates which components of $x$ are observed: for index $i \\in \\{1,\\dots,d\\}$, $m_i = 1$ means pixel $x_i$ is observed, and $m_i = 0$ means pixel $x_i$ is missing. Denote the observed subset as $x_{\\mathrm{obs}} = \\{x_i : m_i = 1\\}$ and the corresponding submatrices $W_{\\mathrm{obs}}$ (rows of $W$ where $m_i = 1$) and $b_{\\mathrm{obs}}$ (entries of $b$ where $m_i = 1$). Missing components $x_{\\mathrm{miss}}$ are to be integrated out in any likelihood term.\n\nTasks:\n1. Derive, starting only from the fundamental definitions and facts listed above, an ELBO for the partially observed data case, that is, a lower bound on $\\log p(x_{\\mathrm{obs}})$ that explicitly integrates over missing components and does not condition on them. Your derivation should make precise how the mask $m$ changes the likelihood term and how the missing pixels $x_{\\mathrm{miss}}$ are integrated out.\n2. Specialize your derivation to the linear-Gaussian model given above and derive the closed-form posterior distribution $q(z \\mid x_{\\mathrm{obs}})$ that optimizes the ELBO within the family of Gaussian distributions. Your derivation should yield a Gaussian with mean and covariance expressed in terms of $W_{\\mathrm{obs}}$, $b_{\\mathrm{obs}}$, $x_{\\mathrm{obs}}$, and $\\sigma_x^2$.\n3. Implement a program that, for each test case below, computes:\n   - the exact log marginal density $\\log p(x_{\\mathrm{obs}})$ under the model by analytically integrating out $z$ and $x_{\\mathrm{miss}}$, and\n   - the masked ELBO evaluated at the exact posterior $q(z \\mid x_{\\mathrm{obs}})$ you derived.\n   Return the absolute difference between these two quantities for each test case. Each result must be a real-valued float.\n\nModel parameters to use in your implementation:\n- $d = 16$, arranged as a $4 \\times 4$ image and flattened in row-major order.\n- $k = 2$.\n- The decoder matrix $W \\in \\mathbb{R}^{16 \\times 2}$ is defined by two stroke-like basis columns on the $4 \\times 4$ grid:\n  - Column $1$ (vertical stroke along the second column of the image): entries $W_{i,1} = 0.8$ at flattened indices $\\{1, 5, 9, 13\\}$ and $W_{i,1} = 0$ elsewhere.\n  - Column $2$ (horizontal stroke along the third row of the image): entries $W_{i,2} = 0.7$ at flattened indices $\\{8, 9, 10, 11\\}$ and $W_{i,2} = 0$ elsewhere.\n- The bias vector $b \\in \\mathbb{R}^{16}$ has entries $b_i = 0.1$ for all $i \\in \\{1,\\dots,16\\}$.\n\nTest suite of images and masks (all images are $4 \\times 4$ grids flattened in row-major order; intensities are in $[0,1]$):\n- Test case $1$ (\"zero-like\" ring image, moderate noise, scattered missing pixels):\n  - Image $x^{(1)}$ as a ring: border pixels have intensity $0.9$ and interior pixels have intensity $0.1$. Concretely, the $4 \\times 4$ matrix is\n    $$\n    \\begin{pmatrix}\n    0.9 & 0.9 & 0.9 & 0.9 \\\\\n    0.9 & 0.1 & 0.1 & 0.9 \\\\\n    0.9 & 0.1 & 0.1 & 0.9 \\\\\n    0.9 & 0.9 & 0.9 & 0.9\n    \\end{pmatrix}\n    $$.\n  - Mask $m^{(1)}$ with missing pixels at flattened indices $\\{2, 3, 10, 11\\}$ and observed elsewhere. As a $4 \\times 4$ binary matrix:\n    $$\n    \\begin{pmatrix}\n    1 & 1 & 0 & 0 \\\\\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 0 & 0 \\\\\n    1 & 1 & 1 & 1\n    \\end{pmatrix}\n    $$.\n  - Noise variance $\\sigma_{x,1}^2 = 0.05$.\n- Test case $2$ (same \"zero-like\" image, higher noise, contiguous missing block):\n  - Image $x^{(2)}$ equal to $x^{(1)}$.\n  - Mask $m^{(2)}$ with missing top-left block at flattened indices $\\{0, 1, 4, 5\\}$ and observed elsewhere. As a $4 \\times 4$ binary matrix:\n    $$\n    \\begin{pmatrix}\n    0 & 0 & 1 & 1 \\\\\n    0 & 0 & 1 & 1 \\\\\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 1\n    \\end{pmatrix}\n    $$.\n  - Noise variance $\\sigma_{x,2}^2 = 0.2$.\n- Test case $3$ (\"one-like\" vertical line image, very high noise, entire last row missing):\n  - Image $x^{(3)}$ with a vertical line on the second column: pixels on the second column have intensity $0.9$ and all other pixels have intensity $0.1$. Concretely, the $4 \\times 4$ matrix is\n    $$\n    \\begin{pmatrix}\n    0.1 & 0.9 & 0.1 & 0.1 \\\\\n    0.1 & 0.9 & 0.1 & 0.1 \\\\\n    0.1 & 0.9 & 0.1 & 0.1 \\\\\n    0.1 & 0.9 & 0.1 & 0.1\n    \\end{pmatrix}\n    $$.\n  - Mask $m^{(3)}$ with missing entire last row at flattened indices $\\{12, 13, 14, 15\\}$ and observed elsewhere. As a $4 \\times 4$ binary matrix:\n    $$\n    \\begin{pmatrix}\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 1 \\\\\n    0 & 0 & 0 & 0\n    \\end{pmatrix}\n    $$.\n  - Noise variance $\\sigma_{x,3}^2 = 1.0$.\n\nImplementation requirements:\n- For each test case, flatten the $4 \\times 4$ image into a length-$16$ vector in row-major order, and construct the observed index set from the mask.\n- Compute the exact log marginal $\\log p(x_{\\mathrm{obs}})$ by analytically integrating out $z$ under the linear-Gaussian model and restricting to the observed entries via $W_{\\mathrm{obs}}$ and $b_{\\mathrm{obs}}$.\n- Derive and implement the masked ELBO evaluated at the exact posterior $q(z \\mid x_{\\mathrm{obs}})$ for the linear-Gaussian model.\n- Return the absolute differences for all three test cases as a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to $10$ decimal places, for example, `[r_1,r_2,r_3]`.\n\nNotes:\n- Angles do not appear in this problem, so no angle unit specification is needed.\n- There are no physical units involved; all quantities are dimensionless.\n- The answer for each test case must be a float.", "solution": "The problem requires a three-part analysis: (1) a derivation of the Evidence Lower Bound (ELBO) for a generative model with missing data, (2) the specialization of this result to a linear-Gaussian model to find the optimal variational posterior, and (3) a numerical implementation to verify the theoretical results. The core of the problem lies in demonstrating the relationship between the log-marginal likelihood and the ELBO when the variational distribution matches the true posterior.\n\n### Part 1: Derivation of the ELBO for Partially Observed Data\n\nWe are given a latent variable model $p(x, z) = p(x \\mid z)p(z)$. The data vector $x \\in \\mathbb{R}^d$ is partially observed. Let $m \\in \\{0, 1\\}^d$ be a binary mask where $m_i = 1$ indicates that $x_i$ is observed and $m_i = 0$ indicates it is missing. We denote the set of observed data as $x_{\\mathrm{obs}}$ and missing data as $x_{\\mathrm{miss}}$. Our goal is to find a lower bound on the log-marginal likelihood of the observed data, $\\log p(x_{\\mathrm{obs}})$.\n\nThe marginal likelihood $p(x_{\\mathrm{obs}})$ is obtained by integrating over both the latent variables $z$ and the missing data $x_{\\mathrm{miss}}$:\n$$\np(x_{\\mathrm{obs}}) = \\int \\int p(x_{\\mathrm{obs}}, x_{\\mathrm{miss}}, z) \\, dx_{\\mathrm{miss}} \\, dz\n$$\nUsing the chain rule, $p(x, z) = p(x \\mid z) p(z)$, we can write:\n$$\np(x_{\\mathrm{obs}}) = \\int \\left( \\int p(x_{\\mathrm{obs}}, x_{\\mathrm{miss}} \\mid z) \\, dx_{\\mathrm{miss}} \\right) p(z) \\, dz\n$$\nLet's define the likelihood of the observed data given $z$ as $p(x_{\\mathrm{obs}} \\mid z) = \\int p(x \\mid z) \\, dx_{\\mathrm{miss}}$. Then the marginal likelihood becomes:\n$$\np(x_{\\mathrm{obs}}) = \\int p(x_{\\mathrm{obs}} \\mid z) p(z) \\, dz\n$$\nTo derive the ELBO, we introduce an arbitrary variational distribution $q(z)$ over the latent variables. We can write the log-marginal likelihood as:\n$$\n\\log p(x_{\\mathrm{obs}}) = \\log \\int p(x_{\\mathrm{obs}} \\mid z) p(z) \\frac{q(z)}{q(z)} \\, dz = \\log \\mathbb{E}_{q(z)} \\left[ \\frac{p(x_{\\mathrm{obs}} \\mid z) p(z)}{q(z)} \\right]\n$$\nUsing Jensen's inequality, which states that for a concave function like the logarithm, $\\log \\mathbb{E}[Y] \\ge \\mathbb{E}[\\log Y]$, we obtain the lower bound:\n$$\n\\log p(x_{\\mathrm{obs}}) \\ge \\mathbb{E}_{q(z)} \\left[ \\log \\left( \\frac{p(x_{\\mathrm{obs}} \\mid z) p(z)}{q(z)} \\right) \\right]\n$$\nThis inequality defines the Evidence Lower Bound, $\\mathcal{L}(q)$:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(z)} [ \\log p(x_{\\mathrm{obs}} \\mid z) + \\log p(z) - \\log q(z) ]\n$$\nThis can be rewritten in a more familiar form:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(z)}[\\log p(x_{\\mathrm{obs}} \\mid z)] - D_{KL}(q(z) || p(z))\n$$\nwhere the first term is the expected log-likelihood of the observed data under the variational posterior, and the second term is the Kullback-Leibler (KL) divergence between the variational posterior $q(z)$ and the prior $p(z)$. This expression correctly accounts for missing data by marginalizing them out from the likelihood term $p(x \\mid z)$.\n\n### Part 2: Optimal Gaussian Posterior for the Linear-Gaussian Model\n\nWe are given a linear-Gaussian model:\n- Prior: $p(z) = \\mathcal{N}(z; 0, I_k)$\n- Likelihood: $p(x \\mid z) = \\mathcal{N}(x; Wz + b, \\sigma_x^2 I_d)$\n\nThe full likelihood is a product of independent Gaussians for each dimension. Therefore, integrating out the missing dimensions $x_{\\mathrm{miss}}$ simply removes the corresponding terms from the product:\n$$\np(x_{\\mathrm{obs}} \\mid z) = \\int \\prod_{i=1}^d p(x_i \\mid z) \\, dx_{\\mathrm{miss}} = \\prod_{i: m_i=1} p(x_i \\mid z)\n$$\nThis implies that the distribution of $x_{\\mathrm{obs}}$ conditioned on $z$ is also Gaussian:\n$$\np(x_{\\mathrm{obs}} \\mid z) = \\mathcal{N}(x_{\\mathrm{obs}}; W_{\\mathrm{obs}}z + b_{\\mathrm{obs}}, \\sigma_x^2 I_{d_{\\mathrm{obs}}})\n$$\nwhere $W_{\\mathrm{obs}}$, $b_{\\mathrm{obs}}$ are sub-blocks corresponding to the observed dimensions, and $d_{\\mathrm{obs}}$ is the number of observed dimensions.\n\nThe ELBO is maximized when the variational distribution $q(z)$ is equal to the true posterior $p(z \\mid x_{\\mathrm{obs}})$. Since the prior and likelihood are Gaussian, the posterior is also Gaussian. We derive its parameters using Bayes' rule:\n$$\np(z \\mid x_{\\mathrm{obs}}) \\propto p(x_{\\mathrm{obs}} \\mid z) p(z)\n$$\nTaking the logarithm and ignoring constants:\n$$\n\\log p(z \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2\\sigma_x^2} (x_{\\mathrm{obs}} - W_{\\mathrm{obs}}z - b_{\\mathrm{obs}})^T(x_{\\mathrm{obs}} - W_{\\mathrm{obs}}z - b_{\\mathrm{obs}}) - \\frac{1}{2} z^T z\n$$\nExpanding and collecting terms in $z$:\n$$\n\\log p(z \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2} \\left[ z^T \\left(\\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T W_{\\mathrm{obs}} + I_k\\right) z - 2 z^T \\left(\\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T (x_{\\mathrm{obs}} - b_{\\mathrm{obs}})\\right) \\right]\n$$\nBy completing the square, we can identify this as the log-density of a Gaussian distribution $\\mathcal{N}(z; \\mu_{z|x}, \\Sigma_{z|x})$. By matching the quadratic and linear terms of $z$ with the general form $\\log \\mathcal{N}(z; \\mu, \\Sigma) \\propto -\\frac{1}{2}(z^T\\Sigma^{-1}z - 2z^T\\Sigma^{-1}\\mu)$, we find the posterior parameters:\n- Posterior Precision (Inverse Covariance): $\\Sigma_{z|x}^{-1} = \\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T W_{\\mathrm{obs}} + I_k$\n- Posterior Covariance: $\\Sigma_{z|x} = \\left(\\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T W_{\\mathrm{obs}} + I_k\\right)^{-1}$\n- Posterior Mean: $\\mu_{z|x} = \\Sigma_{z|x} \\left( \\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T (x_{\\mathrm{obs}} - b_{\\mathrm{obs}}) \\right)$\n\nThe optimal Gaussian variational distribution $q(z \\mid x_{\\mathrm{obs}})$ is this exact posterior, $p(z \\mid x_{\\mathrm{obs}})$.\n\n### Part 3: Equivalence of ELBO and Log-Marginal Likelihood\n\nThe relationship between the log-marginal likelihood and the ELBO is given by:\n$$\n\\log p(x_{\\mathrm{obs}}) = \\mathcal{L}(q) + D_{KL}(q(z) || p(z \\mid x_{\\mathrm{obs}}))\n$$\nWhen the variational distribution $q(z)$ is set to be the true posterior $p(z \\mid x_{\\mathrm{obs}})$, the KL divergence term becomes $D_{KL}(p(z \\mid x_{\\mathrm{obs}}) || p(z \\mid x_{\\mathrm{obs}})) = 0$. Consequently, the ELBO becomes equal to the log-marginal likelihood:\n$$\n\\log p(x_{\\mathrm{obs}}) = \\mathcal{L}(q(z) = p(z \\mid x_{\\mathrm{obs}}))\n$$\nThe problem asks to compute the absolute difference between these two quantities. Theoretically, this difference is zero. The numerical implementation will verify this up to floating-point precision.\n\n### Part 4: Computational Strategy\n\nWe will compute both quantities separately and then find their absolute difference.\n\n1.  **Exact Log-Marginal Likelihood $\\log p(x_{\\mathrm{obs}})$**: Since the model is linear-Gaussian, we can analytically integrate out $z$.\n    - $z \\sim \\mathcal{N}(0, I_k)$\n    - $x_{\\mathrm{obs}} \\mid z \\sim \\mathcal{N}(W_{\\mathrm{obs}}z + b_{\\mathrm{obs}}, \\sigma_x^2 I_{d_{\\mathrm{obs}}})$\n    The marginal distribution of $x_{\\mathrm{obs}}$ is obtained by marginalizing over $z$, which results in:\n    $$\n    p(x_{\\mathrm{obs}}) = \\mathcal{N}(x_{\\mathrm{obs}}; b_{\\mathrm{obs}}, W_{\\mathrm{obs}}W_{\\mathrm{obs}}^T + \\sigma_x^2 I_{d_{\\mathrm{obs}}})\n    $$\n    We can compute $\\log p(x_{\\mathrm{obs}})$ by evaluating the log-probability density function of this multivariate Gaussian at the given vector $x_{\\mathrm{obs}}$.\n\n2.  **ELBO at the Optimal Posterior**: We compute $\\mathcal{L}(q)$ with $q(z) = p(z \\mid x_{\\mathrm{obs}}) = \\mathcal{N}(z; \\mu_{z|x}, \\Sigma_{z|x})$.\n    $$\n    \\mathcal{L}(q) = \\mathbb{E}_{q(z)}[\\log p(x_{\\mathrm{obs}} \\mid z)] - D_{KL}(q(z) || p(z))\n    $$\n    The terms are calculated as follows:\n    - **Expected Log-Likelihood**:\n    $$\n    \\mathbb{E}_{q(z)}[\\log p(x_{\\mathrm{obs}} \\mid z)] = -\\frac{d_{\\mathrm{obs}}}{2} \\log(2\\pi\\sigma_x^2) -\\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q(z)}[\\|x_{\\mathrm{obs}} - (W_{\\mathrm{obs}}z + b_{\\mathrm{obs}})\\|^2]\n    $$\n    The expectation of the squared norm is $\\mathbb{E}_{q(z)}[\\| (x_{\\mathrm{obs}} - b_{\\mathrm{obs}} - W_{\\mathrm{obs}}\\mu_{z|x}) - W_{\\mathrm{obs}}(z-\\mu_{z|x}) \\|^2] = \\|x_{\\mathrm{obs}} - b_{\\mathrm{obs}} - W_{\\mathrm{obs}}\\mu_{z|x}\\|^2 + \\mathrm{Tr}(W_{\\mathrm{obs}}^T W_{\\mathrm{obs}}\\Sigma_{z|x})$.\n    - **KL Divergence to Prior**:\n    For $q(z) = \\mathcal{N}(\\mu_{z|x}, \\Sigma_{z|x})$ and $p(z) = \\mathcal{N}(0, I_k)$, the KL divergence is:\n    $$\n    D_{KL}(q(z) || p(z)) = \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma_{z|x}) + \\mu_{z|x}^T \\mu_{z|x} - k - \\log\\det(\\Sigma_{z|x}) \\right)\n    $$\n    These expressions allow for a direct numerical computation of the ELBO. The absolute difference between $\\log p(x_{\\mathrm{obs}})$ and $\\mathcal{L}(q)$ will then be calculated for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    # Define model parameters\n    d = 16\n    k = 2\n    \n    # Define decoder weight matrix W\n    W = np.zeros((d, k), dtype=np.float64)\n    # Column 1: vertical stroke\n    W[[1, 5, 9, 13], 0] = 0.8\n    # Column 2: horizontal stroke\n    W[[8, 9, 10, 11], 1] = 0.7\n    \n    # Define decoder bias vector b\n    b = np.full(d, 0.1, dtype=np.float64)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: \"zero-like\" ring, scattered missing pixels\n        {\n            \"x_matrix\": np.array([\n                [0.9, 0.9, 0.9, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.9, 0.9, 0.9]\n            ], dtype=np.float64),\n            \"m_matrix\": np.array([\n                [1, 1, 0, 0],\n                [1, 1, 1, 1],\n                [1, 1, 0, 0],\n                [1, 1, 1, 1]\n            ], dtype=np.float64),\n            \"sigma_sq\": 0.05\n        },\n        # Case 2: \"zero-like\" ring, top-left block missing\n        {\n            \"x_matrix\": np.array([\n                [0.9, 0.9, 0.9, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.9, 0.9, 0.9]\n            ], dtype=np.float64),\n            \"m_matrix\": np.array([\n                [0, 0, 1, 1],\n                [0, 0, 1, 1],\n                [1, 1, 1, 1],\n                [1, 1, 1, 1]\n            ], dtype=np.float64),\n            \"sigma_sq\": 0.2\n        },\n        # Case 3: \"one-like\" line, last row missing\n        {\n            \"x_matrix\": np.array([\n                [0.1, 0.9, 0.1, 0.1],\n                [0.1, 0.9, 0.1, 0.1],\n                [0.1, 0.9, 0.1, 0.1],\n                [0.1, 0.9, 0.1, 0.1]\n            ], dtype=np.float64),\n            \"m_matrix\": np.array([\n                [1, 1, 1, 1],\n                [1, 1, 1, 1],\n                [1, 1, 1, 1],\n                [0, 0, 0, 0]\n            ], dtype=np.float64),\n            \"sigma_sq\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        diff = compute_difference(\n            case[\"x_matrix\"], \n            case[\"m_matrix\"], \n            case[\"sigma_sq\"], \n            W, b, k, d\n        )\n        results.append(diff)\n        \n    # Format the final output string as specified.\n    # The rounding to 10 decimal places is handled by the f-string.\n    formatted_results = [f'{r:.10f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_difference(x_matrix, m_matrix, sigma_sq, W, b, k, d):\n    \"\"\"\n    Computes the absolute difference between the exact log marginal likelihood\n    and the ELBO evaluated at the true posterior.\n    \"\"\"\n    # Flatten images and masks to vectors (row-major order)\n    x = x_matrix.flatten()\n    m = m_matrix.flatten()\n    \n    # Create boolean mask for observed pixels\n    mask = m.astype(bool)\n    \n    # Extract observed data and corresponding model parameters\n    x_obs = x[mask]\n    W_obs = W[mask, :]\n    b_obs = b[mask]\n    d_obs = len(x_obs)\n\n    # --- 1. Compute exact log marginal likelihood log p(x_obs) ---\n    # The marginal p(x_obs) is Gaussian with:\n    # mean = b_obs\n    # cov = W_obs @ W_obs.T + sigma_sq * I\n    mu_x_obs = b_obs\n    cov_x_obs = W_obs @ W_obs.T + sigma_sq * np.identity(d_obs, dtype=np.float64)\n    \n    # Use scipy's multivariate_normal for stable calculation\n    log_p_x_obs = multivariate_normal.logpdf(x_obs, mean=mu_x_obs, cov=cov_x_obs)\n\n    # --- 2. Compute the ELBO at the exact posterior q(z) = p(z|x_obs) ---\n    \n    # First, calculate parameters of the posterior q(z) = N(mu_z_x, Sigma_z_x)\n    Sigma_z_x_inv = (1 / sigma_sq) * (W_obs.T @ W_obs) + np.identity(k, dtype=np.float64)\n    Sigma_z_x = np.linalg.inv(Sigma_z_x_inv)\n    \n    mu_z_x = Sigma_z_x @ ((1 / sigma_sq) * W_obs.T @ (x_obs - b_obs))\n\n    # Then, calculate the two terms of the ELBO: E[log p(x_obs|z)] - KL(q||p)\n\n    # Term 1: Expected log-likelihood E_{q(z)}[log p(x_obs|z)]\n    # E[log p(x_obs|z)] = -d_obs/2*log(2*pi*sigma_sq) - 1/(2*sigma_sq) * E[||x_obs - (W_obs*z + b_obs)||^2]\n    # The expectation of the squared norm expands to:\n    # ||x_obs - b_obs - W_obs*mu_z_x||^2 + Tr(W_obs.T @ W_obs @ Sigma_z_x)\n    recon_error_mean = np.sum((x_obs - b_obs - W_obs @ mu_z_x)**2)\n    recon_error_var = np.trace(W_obs.T @ W_obs @ Sigma_z_x)\n    \n    expected_log_likelihood = \\\n        -0.5 * d_obs * np.log(2 * np.pi * sigma_sq) \\\n        - (1 / (2 * sigma_sq)) * (recon_error_mean + recon_error_var)\n\n    # Term 2: KL-divergence D_KL(q(z)||p(z)) where p(z) = N(0, I)\n    # D_KL = 0.5 * (Tr(Sigma_z_x) + mu_z_x.T @ mu_z_x - k - log_det(Sigma_z_x))\n    sign, log_det_Sigma_z_x = np.linalg.slogdet(Sigma_z_x)\n    if sign = 0: # Covariance must be positive definite\n        log_det_Sigma_z_x = -np.inf\n        \n    kl_divergence = 0.5 * (\n        np.trace(Sigma_z_x) + \n        mu_z_x.T @ mu_z_x - \n        k - \n        log_det_Sigma_z_x\n    )\n\n    # ELBO = E[log p(x_obs|z)] - D_KL\n    elbo = expected_log_likelihood - kl_divergence\n    \n    # --- 3. Compute the absolute difference ---\n    # Theoretically, this difference is 0. Numerically, it will be close to 0.\n    abs_difference = np.abs(log_p_x_obs - elbo)\n    \n    return abs_difference\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3184447"}]}