{"hands_on_practices": [{"introduction": "The core mechanism of a diffusion model is the reverse process, which learns to denoise a corrupted input step-by-step. This exercise peels back the neural network abstraction to reveal the underlying mathematical structure of this process. By analytically deriving the true reverse posterior for a simple, non-Gaussian data distribution, you will gain a first-principles understanding of why the optimal denoiser is inherently non-linear and why powerful function approximators are necessary for practical success [@problem_id:3116015].", "problem": "Consider a one-dimensional Diffusion Probabilistic Model (DPM) with a single forward step defined by the stochastic process $x_{1} = \\sqrt{\\alpha}\\, x_{0} + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\beta)$ is independent of $x_{0}$ and $\\alpha = 1 - \\beta$. The data distribution $p_{\\text{data}}$ is a symmetric two-component Gaussian mixture with equal weights: $x_{0} \\sim \\frac{1}{2}\\,\\mathcal{N}(-m, \\sigma_{0}^{2}) + \\frac{1}{2}\\,\\mathcal{N}(m, \\sigma_{0}^{2})$. In this minimal example, take $m = 1$, $\\sigma_{0}^{2} = 1$, and $\\beta = \\frac{1}{2}$ (hence $\\alpha = \\frac{1}{2}$). \nUsing only Bayesâ€™ rule, linear-Gaussian conjugacy, and properties of mixtures:\n1) Derive an analytic expression for the true reverse-step posterior mean $\\mathbb{E}[x_{0} \\mid x_{1}]$ under $p_{\\text{data}}$.\n2) Suppose a linear predictor $\\hat{x}_{0} = a\\, x_{1}$ is trained to minimize mean squared error $\\mathbb{E}\\big[(x_{0} - a x_{1})^{2}\\big]$ under the joint distribution induced by the forward process and $p_{\\text{data}}$. Derive a closed-form expression for the optimal $a$ from first principles of least-squares regression.\n3) Evaluate the difference between the analytic posterior mean and the learned linear estimate at $x_{1} = 1$, and provide the exact simplified expression. Express your final answer as a single closed-form analytic expression. Do not round.", "solution": "The problem is validated as well-posed, scientifically grounded, and self-contained. The solution is derived in three parts as requested.\n\n### Part 1: Derivation of the True Posterior Mean $\\mathbb{E}[x_{0} \\mid x_{1}]$\n\nThe data distribution for $x_0$ is a symmetric Gaussian mixture model (GMM):\n$$ p(x_0) = \\frac{1}{2}\\,\\mathcal{N}(x_0; -m, \\sigma_0^2) + \\frac{1}{2}\\,\\mathcal{N}(x_0; m, \\sigma_0^2) $$\nThis can be modeled by introducing a latent Rademacher random variable $z \\in \\{-1, 1\\}$ with $p(z=1) = p(z=-1) = \\frac{1}{2}$. Conditional on $z$, the distribution of $x_0$ is a single Gaussian:\n$$ p(x_0 \\mid z) = \\mathcal{N}(x_0; z \\cdot m, \\sigma_0^2) $$\nThe forward process is defined as $x_1 = \\sqrt{\\alpha}\\, x_0 + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\beta)$ is independent of $x_0$. This defines the conditional likelihood:\n$$ p(x_1 \\mid x_0) = \\mathcal{N}(x_1; \\sqrt{\\alpha}\\, x_0, \\beta) $$\nWe seek the posterior mean $\\mathbb{E}[x_0 \\mid x_1]$. By the law of total expectation:\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\mathbb{E}_{z \\mid x_1} \\left[ \\mathbb{E}[x_0 \\mid x_1, z] \\right] = \\sum_{z \\in \\{-1, 1\\}} \\mathbb{E}[x_0 \\mid x_1, z] \\, p(z \\mid x_1) $$\nFirst, we find the conditional mean $\\mathbb{E}[x_0 \\mid x_1, z]$. For a fixed $z$, we have a linear-Gaussian system:\n- Prior: $x_0 \\mid z \\sim \\mathcal{N}(z \\cdot m, \\sigma_0^2)$\n- Likelihood: $x_1 \\mid x_0 \\sim \\mathcal{N}(\\sqrt{\\alpha}\\, x_0, \\beta)$\n\nThe joint distribution $p(x_0, x_1 \\mid z)$ is a bivariate Gaussian. The posterior $p(x_0 \\mid x_1, z)$ is also Gaussian. Its mean, using standard Bayesian conjugacy formulas for linear-Gaussian models, is:\n$$ \\mathbb{E}[x_0 \\mid x_1, z] = \\mu_{x_0|z} + \\frac{\\text{Cov}(x_0, x_1 \\mid z)}{\\text{Var}(x_1 \\mid z)}(x_1 - \\mu_{x_1|z}) $$\nThe required moments, conditional on $z$, are:\n- $\\mu_{x_0|z} = \\mathbb{E}[x_0 \\mid z] = z \\cdot m$\n- $\\mu_{x_1|z} = \\mathbb{E}[x_1 \\mid z] = \\mathbb{E}[\\sqrt{\\alpha}\\, x_0 + \\epsilon \\mid z] = \\sqrt{\\alpha}\\, \\mathbb{E}[x_0 \\mid z] = \\sqrt{\\alpha} \\, z \\cdot m$\n- $\\text{Cov}(x_0, x_1 \\mid z) = \\text{Cov}(x_0, \\sqrt{\\alpha}\\, x_0 + \\epsilon \\mid z) = \\sqrt{\\alpha}\\, \\text{Var}(x_0 \\mid z) = \\sqrt{\\alpha}\\, \\sigma_0^2$\n- $\\text{Var}(x_1 \\mid z) = \\text{Var}(\\sqrt{\\alpha}\\, x_0 + \\epsilon \\mid z) = \\alpha\\, \\text{Var}(x_0 \\mid z) + \\text{Var}(\\epsilon) = \\alpha\\, \\sigma_0^2 + \\beta$\n\nLet $\\sigma_1^2 = \\alpha\\, \\sigma_0^2 + \\beta$. The mean of the component-wise posterior is:\n$$ \\mu_z(x_1) \\equiv \\mathbb{E}[x_0 \\mid x_1, z] = z \\cdot m + \\frac{\\sqrt{\\alpha}\\, \\sigma_0^2}{\\alpha\\, \\sigma_0^2 + \\beta} (x_1 - \\sqrt{\\alpha}\\, z \\cdot m) $$\nNext, we determine the posterior probability $p(z \\mid x_1)$ using Bayes' rule:\n$$ p(z \\mid x_1) = \\frac{p(x_1 \\mid z) p(z)}{p(x_1)} \\propto p(x_1 \\mid z) $$\nThe distribution $p(x_1 \\mid z)$ is the marginal of the joint $p(x_0, x_1 \\mid z)$ over $x_0$, which is Gaussian with mean $\\mu_{x_1|z}$ and variance $\\sigma_1^2$:\n$$ p(x_1 \\mid z) = \\mathcal{N}(x_1; \\sqrt{\\alpha}\\, z \\cdot m, \\sigma_1^2) $$\nThe posterior probabilities for $z=1$ and $z=-1$ are:\n$$ p(z=1 \\mid x_1) \\propto \\exp\\left(-\\frac{(x_1 - \\sqrt{\\alpha}\\, m)^2}{2\\sigma_1^2}\\right) $$\n$$ p(z=-1 \\mid x_1) \\propto \\exp\\left(-\\frac{(x_1 + \\sqrt{\\alpha}\\, m)^2}{2\\sigma_1^2}\\right) $$\nNormalizing these gives:\n$$ p(z=1 \\mid x_1) = \\frac{\\exp(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2})}{\\exp(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2}) + \\exp(-\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2})} = \\frac{1}{2} \\left[ 1 + \\tanh\\left(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2}\\right) \\right] $$\n$$ p(z=-1 \\mid x_1) = 1 - p(z=1 \\mid x_1) = \\frac{1}{2} \\left[ 1 - \\tanh\\left(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2}\\right) \\right] $$\nLet $\\theta = \\frac{\\sqrt{\\alpha}\\,m}{\\sigma_1^2}$. The full posterior mean is:\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\mu_1(x_1) p(z=1|x_1) + \\mu_{-1}(x_1) p(z=-1|x_1) $$\n$$ = \\frac{1}{2}(\\mu_1(x_1) + \\mu_{-1}(x_1)) + \\frac{1}{2}(\\mu_1(x_1) - \\mu_{-1}(x_1)) \\tanh(\\theta x_1) $$\nWe calculate the terms:\n$$ \\mu_1(x_1) + \\mu_{-1}(x_1) = 2 \\frac{\\sqrt{\\alpha}\\, \\sigma_0^2}{\\sigma_1^2} x_1 $$\n$$ \\mu_1(x_1) - \\mu_{-1}(x_1) = 2m \\left( 1 - \\frac{\\alpha\\, \\sigma_0^2}{\\sigma_1^2} \\right) = 2m \\frac{\\sigma_1^2 - \\alpha\\,\\sigma_0^2}{\\sigma_1^2} = 2m \\frac{\\beta}{\\sigma_1^2} $$\nSubstituting these into the expression for $\\mathbb{E}[x_0 \\mid x_1]$:\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\frac{\\sqrt{\\alpha}\\, \\sigma_0^2}{\\sigma_1^2} x_1 + \\frac{m \\beta}{\\sigma_1^2} \\tanh\\left(\\frac{\\sqrt{\\alpha}\\,m}{\\sigma_1^2} x_1\\right) $$\nUsing the given values: $m=1$, $\\sigma_0^2=1$, $\\beta=\\frac{1}{2}$, and $\\alpha=1-\\beta=\\frac{1}{2}$.\n- $\\sqrt{\\alpha} = \\frac{1}{\\sqrt{2}}$\n- $\\sigma_1^2 = \\alpha\\, \\sigma_0^2 + \\beta = \\frac{1}{2}(1) + \\frac{1}{2} = 1$\n- The coefficient of $x_1$ is $\\frac{(1/\\sqrt{2})(1)}{1} = \\frac{1}{\\sqrt{2}}$.\n- The coefficient of the tanh term is $\\frac{(1)(1/2)}{1} = \\frac{1}{2}$.\n- The argument of tanh is $\\frac{(1/\\sqrt{2})(1)}{1} x_1 = \\frac{x_1}{\\sqrt{2}}$.\nThus, the posterior mean is:\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\frac{1}{\\sqrt{2}} x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) $$\n\n### Part 2: Derivation of the Optimal Linear Predictor Coefficient $a$\n\nWe want to find the coefficient $a$ that minimizes the mean squared error $L(a) = \\mathbb{E}[(x_0 - a x_1)^2]$. The expectation is over the joint distribution $p(x_0, x_1)$. The optimal $a$ is found by setting the derivative of $L(a)$ to zero:\n$$ \\frac{d L}{d a} = \\mathbb{E}[-2 x_1 (x_0 - a x_1)] = -2 (\\mathbb{E}[x_0 x_1] - a \\mathbb{E}[x_1^2]) = 0 $$\nSolving for $a$ gives the classic least-squares solution:\n$$ a = \\frac{\\mathbb{E}[x_0 x_1]}{\\mathbb{E}[x_1^2]} $$\nWe need to calculate the moments $\\mathbb{E}[x_0 x_1]$ and $\\mathbb{E}[x_1^2]$.\nThe moments of $x_0$ are:\n- $\\mathbb{E}[x_0] = \\frac{1}{2}(-m) + \\frac{1}{2}(m) = 0$\n- $\\mathbb{E}[x_0^2] = \\text{Var}(x_0) + (\\mathbb{E}[x_0])^2 = \\text{Var}(x_0)$. By the law of total variance, $\\text{Var}(x_0) = \\mathbb{E}[\\text{Var}(x_0|z)] + \\text{Var}(\\mathbb{E}[x_0|z])$.\n- $\\text{Var}(x_0|z) = \\sigma_0^2$, so $\\mathbb{E}[\\text{Var}(x_0|z)] = \\sigma_0^2$.\n- $\\mathbb{E}[x_0|z] = z \\cdot m$, so $\\text{Var}(\\mathbb{E}[x_0|z]) = \\text{Var}(z \\cdot m) = m^2 \\text{Var}(z) = m^2(1) = m^2$.\n- $\\mathbb{E}[x_0^2] = \\sigma_0^2 + m^2$.\n\nNow we find the required cross-moment and second moment:\n- $\\mathbb{E}[x_0 x_1] = \\mathbb{E}[x_0(\\sqrt{\\alpha}\\, x_0 + \\epsilon)] = \\mathbb{E}[\\sqrt{\\alpha}\\, x_0^2 + x_0 \\epsilon] = \\sqrt{\\alpha}\\, \\mathbb{E}[x_0^2] + \\mathbb{E}[x_0]\\mathbb{E}[\\epsilon] = \\sqrt{\\alpha}(\\sigma_0^2 + m^2)$.\n- $\\mathbb{E}[x_1^2] = \\text{Var}(x_1) + (\\mathbb{E}[x_1])^2$. Since $\\mathbb{E}[x_1] = \\mathbb{E}[\\sqrt{\\alpha}\\, x_0 + \\epsilon] = 0$, we have $\\mathbb{E}[x_1^2]=\\text{Var}(x_1)$.\n- $\\text{Var}(x_1) = \\text{Var}(\\sqrt{\\alpha}\\, x_0 + \\epsilon) = \\alpha \\text{Var}(x_0) + \\text{Var}(\\epsilon) = \\alpha(\\sigma_0^2 + m^2) + \\beta$.\n\nSubstituting these into the expression for $a$:\n$$ a = \\frac{\\sqrt{\\alpha}(\\sigma_0^2 + m^2)}{\\alpha(\\sigma_0^2 + m^2) + \\beta} $$\nUsing the given values: $m=1$, $\\sigma_0^2=1$, $\\beta=\\frac{1}{2}$, $\\alpha=\\frac{1}{2}$:\n- $\\sigma_0^2 + m^2 = 1^2 + 1 = 2$.\n- Numerator: $\\sqrt{\\frac{1}{2}}(2) = \\frac{1}{\\sqrt{2}}(2) = \\sqrt{2}$.\n- Denominator: $\\frac{1}{2}(2) + \\frac{1}{2} = 1 + \\frac{1}{2} = \\frac{3}{2}$.\n$$ a = \\frac{\\sqrt{2}}{3/2} = \\frac{2\\sqrt{2}}{3} $$\n\n### Part 3: Evaluation of the Difference\n\nWe need to evaluate the difference between the true posterior mean (the optimal estimator) and the learned linear estimate (the optimal linear estimator) at the point $x_1=1$.\nThe difference is given by:\n$$ D(x_1) = \\mathbb{E}[x_0 \\mid x_1] - a x_1 $$\nSubstituting the expressions from Part 1 and Part 2:\n$$ D(x_1) = \\left[ \\frac{1}{\\sqrt{2}} x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) \\right] - \\frac{2\\sqrt{2}}{3} x_1 $$\n$$ D(x_1) = \\left(\\frac{1}{\\sqrt{2}} - \\frac{2\\sqrt{2}}{3}\\right)x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) $$\nThe coefficient of $x_1$ simplifies to:\n$$ \\frac{1}{\\sqrt{2}} - \\frac{2\\sqrt{2}}{3} = \\frac{\\sqrt{2}}{2} - \\frac{2\\sqrt{2}}{3} = \\left(\\frac{3-4}{6}\\right)\\sqrt{2} = -\\frac{\\sqrt{2}}{6} $$\nSo the difference as a function of $x_1$ is:\n$$ D(x_1) = -\\frac{\\sqrt{2}}{6} x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) $$\nWe are asked to evaluate this at $x_1=1$:\n$$ D(1) = -\\frac{\\sqrt{2}}{6}(1) + \\frac{1}{2} \\tanh\\left(\\frac{1}{\\sqrt{2}}\\right) $$\nThe final simplified expression is:\n$$ D(1) = \\frac{1}{2} \\tanh\\left(\\frac{1}{\\sqrt{2}}\\right) - \\frac{\\sqrt{2}}{6} $$\nThis expression is the exact simplified difference and is the final answer.", "answer": "$$\n\\boxed{\\frac{1}{2} \\tanh\\left(\\frac{1}{\\sqrt{2}}\\right) - \\frac{\\sqrt{2}}{6}}\n$$", "id": "3116015"}, {"introduction": "While low training loss is a desirable outcome, it doesn't always guarantee a good generative model. A common failure mode, especially with limited data, is overfitting that leads to \"diversity collapse,\" where the model memorizes training examples instead of learning the underlying distribution. This hands-on coding practice guides you through setting up a minimal experiment to demonstrate and diagnose this phenomenon, teaching you to track not just loss but also the entropy of generated samples as a crucial measure of model quality [@problem_id:3115973].", "problem": "You are tasked with constructing a minimal but scientifically faithful experiment that demonstrates an overfitting failure mode in diffusion probabilistic models. You will implement and analyze a one-dimensional denoising diffusion model with a simple linear denoiser at each diffusion time step, trained on a mixture-of-Gaussians dataset. Your goals are to show that during training the average denoising loss on the training set decreases while the diversity of generated samples collapses, measured via a discrete entropy computed from histograms of generated samples over training.\n\nFundamental base and assumptions:\n- Consider a forward noising process with a finite number of diffusion steps, indexed by integers $t \\in \\{1,\\dots,T\\}$. Let $\\beta_t \\in (0,1)$ be a predefined variance schedule, $\\alpha_t = 1 - \\beta_t$, and $\\overline{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. The forward process is given by $x_t = \\sqrt{\\overline{\\alpha}_t}\\,x_0 + \\sqrt{1 - \\overline{\\alpha}_t}\\,\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,1)$ in one dimension.\n- The training objective is denoising score matching in the form of a mean squared error between the true noise $\\epsilon$ and a parametric prediction $\\epsilon_\\theta(x_t, t)$, that is, an empirical estimate of $\\mathbb{E}\\left[ \\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2 \\right]$ using the training set.\n- You will use a linear denoiser at each timestep: $\\epsilon_\\theta(x_t, t) = a_t x_t + b_t$, where $a_t$ and $b_t$ are scalar parameters specific to time $t$.\n- The ancestral reverse-time sampling step uses the standard conditional mean parameterization consistent with the original denoising diffusion probabilistic model. For $t \\ge 1$, letting $x_t$ denote the current state, the model computes a predicted noise $\\widehat{\\epsilon}_t = a_t x_t + b_t$ and updates\n$$\n\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha}_t}} \\widehat{\\epsilon}_t \\right),\n$$\nand draws $x_{t-1} \\sim \\mathcal{N}\\!\\left(\\mu_\\theta(x_t,t),\\, \\gamma^2 \\beta_t \\right)$ for $t > 1$, and sets $x_0 = \\mu_\\theta(x_1,1)$ for $t = 1$. Here $\\gamma \\in [0,1]$ is a sampling stochasticity parameter: $\\gamma = 1$ is fully stochastic ancestral sampling, while $\\gamma = 0$ is deterministic sampling.\n\nData distribution:\n- The data distribution is a balanced one-dimensional Gaussian mixture with two symmetric modes: $x_0 \\sim \\tfrac{1}{2}\\,\\mathcal{N}(-\\mu, \\sigma_{\\text{data}}^2) + \\tfrac{1}{2}\\,\\mathcal{N}(+\\mu, \\sigma_{\\text{data}}^2)$ with given parameters $\\mu > 0$ and $\\sigma_{\\text{data}} > 0$.\n\nTraining protocol:\n- Initialize $a_t$ and $b_t$ to zero for all $t \\in \\{1,\\dots,T\\}$.\n- At each epoch, draw a timestep index $t$ uniformly for each training example, generate $x_t$ via the forward process using a fresh $\\epsilon \\sim \\mathcal{N}(0,1)$, compute the per-sample loss $\\ell = \\left(\\epsilon - (a_t x_t + b_t)\\right)^2$, and take a gradient step with respect to $a_t$ and $b_t$ using full-batch gradient descent with learning rate $\\eta$ on the average loss across the batch. You may use gradient clipping if needed to maintain numerical stability, provided it is applied consistently across all test cases.\n\nGenerated diversity tracking:\n- At specified epochs, generate $K$ samples by running the reverse process from $x_T \\sim \\mathcal{N}(0,1)$ to $x_0$ using the current parameters $\\{a_t, b_t\\}$. Compute the discrete entropy of the generated marginal distribution at $x_0$ using a fixed histogram with $B$ bins over a fixed range $[-R, R]$. Let the histogram bin probabilities be $\\{p_i\\}_{i=1}^B$; compute $H = -\\sum_{i=1}^B p_i \\log(p_i)$ in nats, with the convention that $0 \\log 0 = 0$. Track the entropy at the start of training and at the end of training.\n\nYour tasks for each test case:\n- Implement the training and sampling procedure described above.\n- Measure the initial average training loss (before any parameter updates) and the final average training loss (after training), both computed as the empirical mean squared error between $\\epsilon$ and $\\epsilon_\\theta(x_t,t)$ on the training set with freshly sampled timesteps and noises.\n- Measure the generated-sample entropy at the start and at the end of training, using the histogram-based estimator defined above.\n- Return a boolean indicating whether both of the following hold:\n  1. The final training loss is strictly smaller than the initial training loss by at least a specified tolerance $\\delta_{\\text{loss}}$.\n  2. The final entropy is strictly smaller than the initial entropy by at least a specified tolerance $\\delta_{\\text{ent}}$.\n\nNumerical and implementation requirements:\n- Use one-dimensional arrays for all data and parameters.\n- Use a linear $\\beta_t$ schedule between a small positive minimum and a modest maximum, inclusive.\n- Use deterministic pseudorandom number generation with a given seed per test case to ensure reproducibility.\n- All computed entropies must be expressed in nats. No other physical units are involved.\n\nTest suite to implement and evaluate:\n- Fixed global hyperparameters:\n  - Diffusion steps $T = 20$.\n  - Linear schedule $\\beta_t$ from $\\beta_{\\min} = 10^{-4}$ to $\\beta_{\\max} = 2\\times 10^{-2}$ inclusive over $T$ steps.\n  - Mixture parameters $\\mu = 2.0$ and $\\sigma_{\\text{data}} = 0.3$.\n  - Learning rate $\\eta = 0.1$.\n  - Histogram bin count $B = 50$ and range $[-R,R]$ with $R = 6.0$.\n  - Number of generated samples per evaluation $K = 1000$.\n  - Tolerances $\\delta_{\\text{loss}} = 10^{-2}$ and $\\delta_{\\text{ent}} = 5\\times 10^{-2}$.\n- Three test cases, each specified as a tuple $(\\text{seed}, N, \\text{epochs}, \\gamma)$:\n  1. $(42, 8, 600, 0.0)$: tiny dataset, long training, deterministic sampling; intended to exhibit overfitting and diversity collapse.\n  2. $(123, 512, 200, 1.0)$: larger dataset, moderate training, stochastic sampling; intended to avoid diversity collapse while still reducing loss.\n  3. $(7, 8, 5, 1.0)$: tiny dataset, very short training, stochastic sampling; intended as a boundary case without meaningful collapse.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three booleans corresponding to the three test cases in the order listed above, formatted as a Python-style list with no spaces, for example, \"[True,False,False]\".", "solution": "We outline a principled derivation and algorithmic plan connecting the fundamentals of diffusion probabilistic models to a concrete and testable implementation that reveals overfitting-driven diversity collapse.\n\nStart from the forward diffusion process. For a fixed schedule $\\{\\beta_t\\}_{t=1}^T$ with $\\beta_t \\in (0,1)$, define $\\alpha_t = 1 - \\beta_t$ and the cumulative product $\\overline{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. The standard forward corruption process is \n$$\nx_t = \\sqrt{\\overline{\\alpha}_t}\\,x_0 + \\sqrt{1 - \\overline{\\alpha}_t}\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1),\n$$\nwhich follows from recursively composing Gaussian perturbations with variances $\\beta_t$ and noting that the marginal at time $t$ is Gaussian conditioned on $x_0$. This is a well-tested fact from the original denoising diffusion formulation.\n\nTraining objective via denoising score matching. At each time $t$, one seeks to approximate the conditional score using a parametric model $\\epsilon_\\theta(x_t,t)$. A widely used and well-justified surrogate loss is the expected mean squared error\n$$\n\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\text{Unif}(\\{1,\\dots,T\\}),\\,x_0 \\sim p_{\\text{data}},\\,\\epsilon \\sim \\mathcal{N}(0,1)}\n\\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t,t) \\right\\|^2 \\right],\n$$\nwith $x_t$ as above. In our one-dimensional setting, we take a linear denoiser for each timestep $t$: $\\epsilon_\\theta(x_t,t) = a_t x_t + b_t$, where $a_t$ and $b_t$ are scalars. The linear model provides an analytically tractable and computationally efficient baseline, allowing a clear demonstration of the phenomena.\n\nEmpirical risk and gradient. Consider the empirical counterpart of $\\mathcal{L}(\\theta)$ on a dataset $\\{x_0^{(i)}\\}_{i=1}^N$:\n$$\n\\widehat{\\mathcal{L}}(\\{a_t,b_t\\}) = \\frac{1}{N} \\sum_{i=1}^N \\left( \\epsilon^{(i)} - \\left[a_{t^{(i)}} x_{t^{(i)}}^{(i)} + b_{t^{(i)}}\\right] \\right)^2,\n$$\nwhere for each $i$ we draw $t^{(i)}$ uniformly from $\\{1,\\dots,T\\}$ and $\\epsilon^{(i)} \\sim \\mathcal{N}(0,1)$, and form $x_{t^{(i)}}^{(i)} = \\sqrt{\\overline{\\alpha}_{t^{(i)}}}\\,x_0^{(i)} + \\sqrt{1 - \\overline{\\alpha}_{t^{(i)}}}\\,\\epsilon^{(i)}$. Differentiating the squared loss with respect to the linear parameters yields, for each timestep $t$,\n$$\n\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial a_t} = -\\frac{2}{N} \\sum_{i: t^{(i)}=t} \\left(\\epsilon^{(i)} - (a_t x_{t}^{(i)} + b_t)\\right) x_t^{(i)},\n\\quad\n\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial b_t} = -\\frac{2}{N} \\sum_{i: t^{(i)}=t} \\left(\\epsilon^{(i)} - (a_t x_{t}^{(i)} + b_t)\\right).\n$$\nA full-batch gradient descent update with learning rate $\\eta$ applies\n$$\na_t \\leftarrow a_t - \\eta \\,\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial a_t}, \\quad\nb_t \\leftarrow b_t - \\eta \\,\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial b_t}.\n$$\nInitializing with $a_t = 0$ and $b_t = 0$ for all $t$ corresponds to a near-zero denoiser and provides a neutral baseline for observing training dynamics.\n\nReverse-time sampling. Given the predicted noise $\\widehat{\\epsilon}_t = a_t x_t + b_t$, the nonparametric reverse-time conditional mean in the original denoising diffusion probabilistic model is\n$$\n\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha}_t}} \\widehat{\\epsilon}_t \\right).\n$$\nSampling $x_{t-1}$ from a Gaussian with mean $\\mu_\\theta(x_t,t)$ and variance $\\gamma^2 \\beta_t$ for $t>1$ recovers the stochastic ancestral sampler when $\\gamma=1$. Setting $\\gamma=0$ removes the sampling noise and yields a deterministic sampler consistent with the Denoising Diffusion Implicit Models limit; this can exacerbate diversity collapse when the model is overfitted.\n\nMeasuring diversity via discrete entropy. To quantify sample diversity, draw $K$ samples $\\{x_0^{(k)}\\}_{k=1}^K$ from the current model and discretize them into $B$ bins over a fixed range $[-R,R]$. The resulting normalized counts define probabilities $\\{p_i\\}_{i=1}^B$ satisfying $\\sum_i p_i = 1$. The discrete entropy in nats is\n$$\nH = -\\sum_{i=1}^B p_i \\log(p_i),\n$$\nwith the convention that summands with $p_i = 0$ contribute $0$. Lower entropy indicates less diversity.\n\nExperimental design logic. Overfitting can be induced by using a very small training set and training for many epochs. Especially when combined with deterministic sampling $\\gamma=0$, the learned denoiser may implement a contractive reverse dynamics that maps many initial conditions $x_T$ into a narrower set of outputs, reducing the histogram entropy. In contrast, a larger dataset and moderate training with stochastic sampling $\\gamma=1$ should both decrease training loss and avoid a large drop in entropy. A boundary case with tiny data and very few epochs should not show meaningful loss decrease nor entropy collapse.\n\nAlgorithm steps to implement:\n1. Fix $T$, the linear schedule endpoints $\\beta_{\\min}$ and $\\beta_{\\max}$, and compute $\\{\\beta_t\\}$, $\\{\\alpha_t\\}$, and $\\{\\overline{\\alpha}_t\\}$.\n2. For each test case:\n   - Set the pseudorandom seed, sample $N$ data points from the mixture distribution with parameters $(\\mu, \\sigma_{\\text{data}})$, and initialize $\\{a_t,b_t\\}$ to zeros.\n   - Compute the initial empirical loss by sampling one batch of timesteps and noises and averaging squared errors between $\\epsilon$ and $\\epsilon_\\theta(x_t,t)$.\n   - Estimate the initial entropy $H_{\\text{init}}$ by generating $K$ samples from the reverse process with the given $\\gamma$ and computing the histogram-based entropy.\n   - Train for the specified number of epochs using full-batch gradient descent on the empirical loss, with optional gradient clipping to maintain stability.\n   - Compute the final empirical loss and final entropy $H_{\\text{final}}$ using the same procedures as at initialization (but with fresh timesteps and noises for the loss).\n   - Decide a boolean outcome by checking whether the decrease in loss is at least $\\delta_{\\text{loss}}$ and the decrease in entropy is at least $\\delta_{\\text{ent}}$.\n3. Output a single list of three booleans, one for each test case in the prescribed order, with no spaces.\n\nConcrete test suite parameters:\n- $T = 20$, $\\beta_{\\min} = 10^{-4}$, $\\beta_{\\max} = 2 \\times 10^{-2}$, $\\eta = 0.1$, $\\mu = 2.0$, $\\sigma_{\\text{data}} = 0.3$, $B = 50$, $R = 6.0$, $K = 1000$, $\\delta_{\\text{loss}} = 10^{-2}$, and $\\delta_{\\text{ent}} = 5 \\times 10^{-2}$.\n- Test case tuples $(\\text{seed}, N, \\text{epochs}, \\gamma)$:\n  1. $(42, 8, 600, 0.0)$.\n  2. $(123, 512, 200, 1.0)$.\n  3. $(7, 8, 5, 1.0)$.\n\nThese steps and parameters draw directly from the standard diffusion modeling framework and constitute a scientifically realistic and computationally tractable experiment. The final output must be a single line formatted as a Python-style list of booleans, such as \"[True,False,False]\".", "answer": "```python\nimport numpy as np\n\ndef make_beta_schedule(T, beta_min, beta_max):\n    # Linear schedule inclusive\n    return np.linspace(beta_min, beta_max, T, dtype=np.float64)\n\ndef precompute_alphas(betas):\n    alphas = 1.0 - betas\n    alpha_bars = np.cumprod(alphas)\n    return alphas, alpha_bars\n\ndef sample_mog_1d(rng, N, mu, sigma):\n    signs = rng.choice([-1.0, 1.0], size=N)\n    return signs * mu + sigma * rng.standard_normal(N)\n\ndef forward_sample_xt(rng, x0, t_idx, sqrt_alpha_bar, sqrt_one_minus_alpha_bar):\n    # t_idx is integer in [0, T-1]\n    eps = rng.standard_normal(x0.shape[0])\n    xt = sqrt_alpha_bar[t_idx] * x0 + sqrt_one_minus_alpha_bar[t_idx] * eps\n    return xt, eps\n\ndef compute_loss_and_grads(rng, x0, a, b, alpha_bar, batch_timesteps=None):\n    # Full-batch loss and grads for linear epsilon model per time step\n    N = x0.shape[0]\n    T = a.shape[0]\n    if batch_timesteps is None:\n        t_idx = rng.integers(low=0, high=T, size=N)\n    else:\n        t_idx = batch_timesteps\n    sqrt_ab = np.sqrt(alpha_bar)\n    sqrt_1mab = np.sqrt(1.0 - alpha_bar)\n    eps = rng.standard_normal(N)\n    xt = sqrt_ab[t_idx] * x0 + sqrt_1mab[t_idx] * eps\n    pred = a[t_idx] * xt + b[t_idx]\n    diff = eps - pred\n    loss = np.mean(diff**2)\n\n    # grads\n    # dL/da_t = -2/N sum_{i: t_i=t} diff_i * x_t_i\n    # dL/db_t = -2/N sum_{i: t_i=t} diff_i\n    grad_a = np.zeros(T, dtype=np.float64)\n    grad_b = np.zeros(T, dtype=np.float64)\n    scale = -2.0 / N\n    # Use numpy.add.at to accumulate\n    np.add.at(grad_a, t_idx, scale * diff * xt)\n    np.add.at(grad_b, t_idx, scale * diff)\n    return loss, grad_a, grad_b\n\ndef reverse_sample(rng, a, b, betas, alphas, alpha_bar, K, gamma):\n    # Start from x_T ~ N(0,1), go down to x_0\n    x = rng.standard_normal(K).astype(np.float64)\n    T = a.shape[0]\n    for i in range(T-1, -1, -1):\n        # predict epsilon\n        eps_hat = a[i] * x + b[i]\n        # mu = 1/sqrt(alpha) * (x - beta / sqrt(1-alpha_bar) * eps_hat)\n        denom = np.sqrt(1.0 - alpha_bar[i])\n        mu = (x - betas[i] * (eps_hat / denom)) / np.sqrt(alphas[i])\n        if i > 0:\n            noise = rng.standard_normal(K) * (np.sqrt(betas[i]) * gamma)\n            x = mu + noise\n        else:\n            x = mu\n    return x\n\ndef discrete_entropy(samples, bins, rng, R=6.0):\n    # Histogram over fixed range [-R, R], convert to probabilities\n    hist, edges = np.histogram(samples, bins=bins, range=(-R, R), density=False)\n    total = np.sum(hist)\n    if total == 0:\n        return 0.0\n    p = hist.astype(np.float64) / total\n    # compute -sum p log p; ignore zeros\n    mask = p > 0\n    H = -np.sum(p[mask] * np.log(p[mask]))\n    return float(H)\n\ndef train_and_evaluate_case(seed, N, epochs, gamma,\n                            T=20, beta_min=1e-4, beta_max=2e-2,\n                            lr=0.1, mu=2.0, sigma_data=0.3,\n                            bins=50, R=6.0, K=1000,\n                            loss_tol=1e-2, ent_tol=5e-2,\n                            clip_value=1.0):\n    rng = np.random.default_rng(seed)\n    betas = make_beta_schedule(T, beta_min, beta_max)\n    alphas, alpha_bar = precompute_alphas(betas)\n    sqrt_alpha_bar = np.sqrt(alpha_bar)\n    sqrt_one_minus_alpha_bar = np.sqrt(1.0 - alpha_bar)\n\n    # Data\n    x0 = sample_mog_1d(rng, N, mu, sigma_data)\n\n    # Initialize parameters\n    a = np.zeros(T, dtype=np.float64)\n    b = np.zeros(T, dtype=np.float64)\n\n    # Initial loss\n    init_loss, _, _ = compute_loss_and_grads(rng, x0, a, b, alpha_bar)\n\n    # Initial entropy\n    init_samples = reverse_sample(rng, a, b, betas, alphas, alpha_bar, K, gamma)\n    init_entropy = discrete_entropy(init_samples, bins=bins, rng=rng, R=R)\n\n    # Training loop\n    for ep in range(epochs):\n        loss, grad_a, grad_b = compute_loss_and_grads(rng, x0, a, b, alpha_bar)\n        # Optional gradient clipping for stability\n        ga_norm = np.linalg.norm(grad_a)\n        gb_norm = np.linalg.norm(grad_b)\n        if ga_norm > clip_value and ga_norm > 0:\n            grad_a = grad_a * (clip_value / ga_norm)\n        if gb_norm > clip_value and gb_norm > 0:\n            grad_b = grad_b * (clip_value / gb_norm)\n        a -= lr * grad_a\n        b -= lr * grad_b\n\n    # Final loss (fresh batch of t and eps)\n    final_loss, _, _ = compute_loss_and_grads(rng, x0, a, b, alpha_bar)\n\n    # Final entropy\n    final_samples = reverse_sample(rng, a, b, betas, alphas, alpha_bar, K, gamma)\n    final_entropy = discrete_entropy(final_samples, bins=bins, rng=rng, R=R)\n\n    loss_decrease = init_loss - final_loss\n    ent_decrease = init_entropy - final_entropy\n\n    outcome = (loss_decrease >= loss_tol) and (ent_decrease >= ent_tol)\n    return outcome, (init_loss, final_loss, init_entropy, final_entropy)\n\ndef solve():\n    # Define the test cases as specified in the problem statement:\n    # (seed, N, epochs, gamma)\n    test_cases = [\n        (42, 8, 600, 0.0),\n        (123, 512, 200, 1.0),\n        (7, 8, 5, 1.0),\n    ]\n    # Fixed hyperparameters per problem statement\n    T = 20\n    beta_min = 1e-4\n    beta_max = 2e-2\n    lr = 0.1\n    mu = 2.0\n    sigma_data = 0.3\n    bins = 50\n    R = 6.0\n    K = 1000\n    loss_tol = 1e-2\n    ent_tol = 5e-2\n    clip_value = 1.0\n\n    results = []\n    # Optionally, collect diagnostics if needed for debugging (not printed)\n    for seed, N, epochs, gamma in test_cases:\n        outcome, _ = train_and_evaluate_case(\n            seed=seed, N=N, epochs=epochs, gamma=gamma,\n            T=T, beta_min=beta_min, beta_max=beta_max,\n            lr=lr, mu=mu, sigma_data=sigma_data,\n            bins=bins, R=R, K=K,\n            loss_tol=loss_tol, ent_tol=ent_tol,\n            clip_value=clip_value\n        )\n        results.append(outcome)\n\n    # Final print statement in the exact required format (no spaces).\n    print(f\"[{','.join('True' if r else 'False' for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3115973"}, {"introduction": "Denoising Diffusion Implicit Models (DDIMs) provide a faster sampling alternative to traditional DDPMs by enabling deterministic, larger steps. However, this speed comes with new considerations for sampler stability, as reducing the stochasticity can lead to a collapse in the variance of generated samples. This problem challenges you to analyze the dynamics of a simplified DDIM sampler, derive the variance propagation law, and determine the critical amount of noise scaling $\\eta$ required to maintain sample diversity, providing insight into the delicate balance between sampling speed and fidelity [@problem_id:3116034].", "problem": "Consider a one-dimensional Denoising Diffusion Implicit Models (DDIM) sampler applied to a dataset with known variance. The base forward process from Denoising Diffusion Probabilistic Models (DDPM) is defined as follows: for discrete time indices $t \\in \\{1,2,\\dots,T\\}$, the forward diffusion is given by the conditional distribution $q(x_t \\mid x_{t-1})$ as a Gaussian with mean $\\sqrt{1 - \\beta_t}\\,x_{t-1}$ and variance $\\beta_t$, that is,\n$$\nq(x_t \\mid x_{t-1}) = \\mathcal{N}\\!\\left(\\sqrt{1 - \\beta_t}\\,x_{t-1},\\,\\beta_t\\right),\n$$\nwhere $\\beta_t \\in (0,1)$ is a small positive number.\n\nAssume a simplified linearized DDIM-style sampler in which the learned denoiser and deterministic mapping together induce an effective contraction coefficient $c_t \\in (0,1)$ per step, and the stochasticity term uses a variance $\\sigma_t$ that is replaced by $\\eta\\,\\sigma_t$ for a scalar noise-scaling parameter $\\eta \\ge 0$. The resulting one-dimensional sampling recursion is modeled as\n$$\nx_{t-1} = c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t,\n$$\nwith $z_t \\sim \\mathcal{N}(0,1)$ independent across $t$, and the initial condition $x_T \\sim \\mathcal{N}(0,1)$ is standard normal. This captures the aggregate impact of the DDIM update when the learned denoiser is linearized around the trajectory and the variance term is scaled.\n\nDefine the collapse criterion as follows: the sample variance at the final time $t=0$, denoted $\\mathrm{Var}[x_0]$, is said to \"collapse\" if\n$$\n\\mathrm{Var}[x_0] < r_c \\cdot v_{\\text{data}},\n$$\nwhere $v_{\\text{data}}$ is the known variance of the fixed dataset and $r_c \\in (0,1)$ is a specified variance ratio target expressed as a decimal.\n\nYour task is to determine the critical noise-scaling value $\\eta_c \\ge 0$ for which the sampler transitions from collapse to non-collapse, defined as the smallest $\\eta$ such that the collapse criterion is not triggered:\n$$\n\\eta_c = \\inf\\left\\{ \\eta \\ge 0 \\,\\big|\\, \\mathrm{Var}[x_0] \\ge r_c \\cdot v_{\\text{data}} \\right\\}.\n$$\n\nWork from fundamental definitions and well-tested formulas: start from the DDPM forward process and the stated linear sampling recursion, and derive the variance propagation across time steps $t$ under independence assumptions. Use the following scientifically consistent modeling choices for this problem:\n- Use the forward-process-aligned coefficients $c_t = \\sqrt{1 - \\beta_t}$ and $\\sigma_t = \\sqrt{\\beta_t}$.\n- Use a linearly spaced schedule for $\\beta_t$: for $t \\in \\{1,2,\\dots,T\\}$,\n$$\n\\beta_t = \\beta_{\\min} + \\frac{t-1}{T-1}\\left(\\beta_{\\max} - \\beta_{\\min}\\right),\n$$\nwith $\\beta_{\\min} > 0$ and $\\beta_{\\max} > 0$ sufficiently small to ensure numerical stability, and $T \\ge 1$ an integer.\n- The initialization variance is $\\mathrm{Var}[x_T] = 1$.\n\nImplement a program to compute $\\eta_c$ for a small test suite. For each test case, compute the exact $\\eta_c$ implied by the variance recursion and the collapse criterion. If the sampler is already non-collapsing at $\\eta = 0$ (that is, the inequality holds with $\\eta = 0$), then report $\\eta_c = 0$ for that case. If the required $\\eta_c$ would be undefined due to zero variance injection (which does not occur under the given schedule), handle it gracefully as $\\eta_c = 0$.\n\nTest suite parameters:\n- Case $1$: $T = 100$, $\\beta_{\\min} = 0.0001$, $\\beta_{\\max} = 0.02$, $v_{\\text{data}} = 1.5$, $r_c = 0.3$.\n- Case $2$: $T = 50$, $\\beta_{\\min} = 0.0005$, $\\beta_{\\max} = 0.01$, $v_{\\text{data}} = 4.0$, $r_c = 0.25$.\n- Case $3$: $T = 100$, $\\beta_{\\min} = 0.001$, $\\beta_{\\max} = 0.005$, $v_{\\text{data}} = 0.25$, $r_c = 0.5$.\n\nAnswer format:\n- Compute the critical values $\\eta_c$ for all cases as real numbers and report them rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[0.123456,0.000000,1.234567]$).", "solution": "The problem is valid. It presents a self-contained, scientifically grounded, and well-posed task within the domain of deep learning theory, specifically concerning the stability of diffusion model samplers. All parameters and conditions are clearly defined, and there are no contradictions or ambiguities.\n\nThe solution proceeds by first deriving the variance propagation dynamics of the simplified Denoising Diffusion Implicit Models (DDIM) sampler, then using this result to determine the final sample variance, and finally solving for the critical noise-scaling parameter $\\eta_c$ that marks the boundary of the \"collapse\" regime.\n\n### Step 1: Derivation of the Variance Propagation Law\n\nThe problem defines a one-dimensional linearized DDIM-style sampling recursion as:\n$$x_{t-1} = c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t$$\nwhere $z_t \\sim \\mathcal{N}(0,1)$ are independent standard normal random variables, and $\\eta \\ge 0$ is a scalar noise-scaling parameter. The initial condition is $x_T \\sim \\mathcal{N}(0,1)$.\n\nWe aim to find a recursive relationship for the variance, $\\mathrm{Var}[x_t]$. Let $V_t = \\mathrm{Var}[x_t]$. First, we establish the mean, $\\mathbb{E}[x_t]$.\nThe initial mean is $\\mathbb{E}[x_T] = 0$. For any subsequent step $t \\in \\{1, 2, \\dots, T\\}$, the mean is:\n$$\\mathbb{E}[x_{t-1}] = \\mathbb{E}[c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t]$$\nBy linearity of expectation,\n$$\\mathbb{E}[x_{t-1}] = c_t\\,\\mathbb{E}[x_t] + \\eta\\,\\sigma_t\\,\\mathbb{E}[z_t]$$\nSince $\\mathbb{E}[z_t] = 0$, if we assume $\\mathbb{E}[x_t] = 0$, it follows that $\\mathbb{E}[x_{t-1}] = 0$. By backward induction from $t=T$, we conclude that $\\mathbb{E}[x_t] = 0$ for all $t \\in \\{0, 1, \\dots, T\\}$.\n\nThe variance $V_{t-1} = \\mathrm{Var}[x_{t-1}]$ is thus equal to the second moment $\\mathbb{E}[x_{t-1}^2]$.\n$$V_{t-1} = \\mathbb{E}[(c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t)^2] = \\mathbb{E}[c_t^2\\,x_t^2 + 2\\,c_t\\,\\eta\\,\\sigma_t\\,x_t\\,z_t + \\eta^2\\,\\sigma_t^2\\,z_t^2]$$\nBy linearity of expectation:\n$$V_{t-1} = c_t^2\\,\\mathbb{E}[x_t^2] + 2\\,c_t\\,\\eta\\,\\sigma_t\\,\\mathbb{E}[x_t\\,z_t] + \\eta^2\\,\\sigma_t^2\\,\\mathbb{E}[z_t^2]$$\nThe state $x_t$ is a function of the initial state $x_T$ and the noise terms $\\{z_k\\}_{k=t+1}^T$. The noise $z_t$ is independent of $x_T$ and all $\\{z_k\\}_{k=t+1}^T$, and therefore $z_t$ is independent of $x_t$. This implies $\\mathbb{E}[x_t\\,z_t] = \\mathbb{E}[x_t]\\,\\mathbb{E}[z_t] = 0 \\cdot 0 = 0$.\nAlso, we have $\\mathbb{E}[x_t^2] = V_t$ and $\\mathbb{E}[z_t^2] = \\mathrm{Var}[z_t] = 1$. Substituting these into the equation gives the variance propagation law:\n$$V_{t-1} = c_t^2\\,V_t + \\eta^2\\,\\sigma_t^2$$\n\n### Step 2: Derivation of the Final Variance $\\mathrm{Var}[x_0]$\n\nWe can unroll the recursive formula starting from $V_T = \\mathrm{Var}[x_T] = 1$:\n$$V_{T-1} = c_T^2\\,V_T + \\eta^2\\,\\sigma_T^2 = c_T^2 + \\eta^2\\,\\sigma_T^2$$\n$$V_{T-2} = c_{T-1}^2\\,V_{T-1} + \\eta^2\\,\\sigma_{T-1}^2 = c_{T-1}^2\\,(c_T^2 + \\eta^2\\,\\sigma_T^2) + \\eta^2\\,\\sigma_{T-1}^2 = c_{T-1}^2\\,c_T^2 + \\eta^2\\,(c_{T-1}^2\\,\\sigma_T^2 + \\sigma_{T-1}^2)$$\nContinuing this process down to $t=0$, we obtain the variance at the final step, $V_0 = \\mathrm{Var}[x_0]$:\n$$V_0 = \\left(\\prod_{k=1}^T c_k^2\\right) V_T + \\eta^2 \\sum_{j=1}^T \\left( \\sigma_j^2 \\prod_{k=1}^{j-1} c_k^2 \\right)$$\nwhere the empty product for $j=1$ is defined as $1$. Given $V_T=1$, the expression simplifies to:\n$$V_0 = \\prod_{k=1}^T c_k^2 + \\eta^2 \\sum_{j=1}^T \\left( \\sigma_j^2 \\prod_{k=1}^{j-1} c_k^2 \\right)$$\nFor clarity, let's define two coefficients:\n$$A = \\prod_{k=1}^T c_k^2$$\n$$B = \\sum_{j=1}^T \\left( \\sigma_j^2 \\prod_{k=1}^{j-1} c_k^2 \\right)$$\nThe final variance can then be written as a function of $\\eta$:\n$$\\mathrm{Var}[x_0] = A + B\\,\\eta^2$$\nThe problem specifies forward-process-aligned coefficients, $c_t = \\sqrt{1 - \\beta_t}$ and $\\sigma_t = \\sqrt{\\beta_t}$, so $c_t^2 = 1 - \\beta_t$ and $\\sigma_t^2 = \\beta_t$. The $\\beta_t$ schedule uses $\\beta_{\\min}>0$, ensuring $\\beta_t > 0$ for all $t$ and thus $c_t \\in (0,1)$ and $\\sigma_t > 0$. Consequently, $A > 0$ and $B > 0$.\n\n### Step 3: Formulation of the Critical Noise Value $\\eta_c$\n\nThe \"collapse\" criterion is $\\mathrm{Var}[x_0] < r_c \\cdot v_{\\text{data}}$. The critical noise-scaling value $\\eta_c$ is defined as the smallest $\\eta \\ge 0$ for which the sampler is not in a state of collapse:\n$$\\eta_c = \\inf\\left\\{ \\eta \\ge 0 \\,\\big|\\, \\mathrm{Var}[x_0] \\ge r_c \\cdot v_{\\text{data}} \\right\\}$$\nSubstituting the derived expression for $\\mathrm{Var}[x_0]$:\n$$A + B\\,\\eta^2 \\ge r_c \\cdot v_{\\text{data}}$$\nWe solve for $\\eta$:\n$$B\\,\\eta^2 \\ge r_c \\cdot v_{\\text{data}} - A$$\nSince $B>0$:\n$$\\eta^2 \\ge \\frac{r_c \\cdot v_{\\text{data}} - A}{B}$$\nLet $V_{\\text{target}} = r_c \\cdot v_{\\text{data}}$.\nTwo cases arise:\n1.  If $A \\ge V_{\\text{target}}$, the deterministic part of the variance (corresponding to $\\eta=0$) is already sufficient to avoid collapse. The inequality $\\eta^2 \\ge (V_{\\text{target}} - A)/B$ holds for all $\\eta \\ge 0$ because the right-hand side is non-positive. The set of valid $\\eta$ is $[0, \\infty)$, and its infimum is $\\eta_c = 0$.\n2.  If $A < V_{\\text{target}}$, the system collapses at $\\eta=0$. To avoid collapse, $\\eta$ must be sufficiently large. Since $\\eta \\ge 0$, the inequality becomes $\\eta \\ge \\sqrt{\\frac{V_{\\text{target}} - A}{B}}$. The set of valid $\\eta$ is $[\\sqrt{(V_{\\text{target}} - A)/B}, \\infty)$, and its infimum is $\\eta_c = \\sqrt{\\frac{V_{\\text{target}} - A}{B}}$.\n\nCombining both cases, the formula for $\\eta_c$ is:\n$$\\eta_c = \\sqrt{\\max\\left(0, \\frac{r_c \\cdot v_{\\text{data}} - A}{B}\\right)}$$\n\n### Step 4: Computational Algorithm\n\nThe computation of $\\eta_c$ for each test case proceeds as follows:\n1.  Generate the schedule of $\\beta_t$ values using a linear spacing from $\\beta_{\\min}$ to $\\beta_{\\max}$ over $T$ steps. For $t \\in \\{1, \\dots, T\\}$, $\\beta_t = \\beta_{\\min} + \\frac{t-1}{T-1}(\\beta_{\\max} - \\beta_{\\min})$. This can be implemented efficiently using `numpy.linspace(beta_min, beta_max, T)`.\n2.  Iteratively compute the coefficients $A$ and $B$. Initialize a cumulative product `prod_c_sq = 1.0` and a sum `B_sum = 0.0`. Loop from $t=1$ to $T$:\n    a. Get the current $\\beta_t$. Calculate $\\sigma_t^2 = \\beta_t$ and $c_t^2 = 1 - \\beta_t$.\n    b. Add the term for the sum $B$: `B_sum += sigma_t_sq * prod_c_sq`.\n    c. Update the cumulative product for the next iteration: `prod_c_sq *= c_t_sq`.\n3.  After the loop, the final value of `prod_c_sq` is the coefficient $A$, and `B_sum` is the coefficient $B$.\n4.  Calculate the target variance $V_{\\text{target}} = r_c \\cdot v_{\\text{data}}$.\n5.  Calculate the numerator for the fraction, `num = V_target - A`.\n6.  If `num <= 0`, set $\\eta_c = 0$.\n7.  Otherwise, calculate $\\eta_c = \\sqrt{\\text{num} / B}$.\n8.  The result is then formatted to $6$ decimal places.\n\nThis algorithm directly implements the derived formula and is applied to each test case specified in the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the critical noise-scaling value eta_c for a simplified DDIM sampler.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, beta_min, beta_max, v_data, r_c)\n        (100, 0.0001, 0.02, 1.5, 0.3),\n        (50, 0.0005, 0.01, 4.0, 0.25),\n        (100, 0.001, 0.005, 0.25, 0.5),\n    ]\n\n    results = []\n    for T, beta_min, beta_max, v_data, r_c in test_cases:\n        # Step 1: Generate the beta schedule.\n        # Per the problem description, for t in {1, ..., T},\n        # beta_t = beta_min + ((t-1)/(T-1))*(beta_max - beta_min).\n        # np.linspace is a robust way to implement this, handling T=1 correctly.\n        if T == 1:\n            # For T=1, t-1=0 and T-1=0. A reasonable interpretation,\n            # consistent with np.linspace(..., num=1), is beta_1 = beta_min.\n            betas = np.array([beta_min], dtype=np.float64)\n        else:\n            betas = np.linspace(beta_min, beta_max, T, dtype=np.float64)\n\n        # Step 2: Iteratively compute coefficients A and B.\n        # A = product_{k=1 to T} c_k^2\n        # B = sum_{j=1 to T} (sigma_j^2 * product_{k=1 to j-1} c_k^2)\n        # We can compute both in a single loop.\n        \n        # 'A_running_prod' will hold the cumulative product of c_k^2.\n        # At the start of iteration j, it holds product_{k=1 to j-1} c_k^2.\n        A_running_prod = 1.0\n        B_sum = 0.0\n        \n        for beta_val in betas:\n            # Per problem, c_t^2 = 1 - beta_t and sigma_t^2 = beta_t\n            sigma_sq = beta_val\n            c_sq = 1.0 - beta_val\n            \n            # Add the term for the sum B\n            B_sum += sigma_sq * A_running_prod\n            \n            # Update the cumulative product for the next iteration\n            A_running_prod *= c_sq\n        \n        A = A_running_prod\n        B = B_sum\n        \n        # Step 3: Calculate the target variance.\n        target_var = r_c * v_data\n        \n        # Step 4: Calculate the numerator for the eta_c formula.\n        numerator = target_var - A\n        \n        # Step 5: Determine eta_c based on the sign of the numerator.\n        if numerator <= 0:\n            # If A >= target_var, the sampler is non-collapsing even at eta=0.\n            # The infimum of the set of valid eta values is 0.\n            eta_c = 0.0\n        else:\n            # If B=0, eta_c would be undefined, but problem constraints (beta_min > 0)\n            # ensure B > 0.\n            eta_c = np.sqrt(numerator / B)\n            \n        results.append(f\"{eta_c:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3116034"}]}