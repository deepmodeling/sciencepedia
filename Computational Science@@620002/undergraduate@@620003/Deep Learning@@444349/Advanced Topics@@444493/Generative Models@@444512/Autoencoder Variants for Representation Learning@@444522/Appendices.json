{"hands_on_practices": [{"introduction": "A primary goal of representation learning is to organize complex data in a simpler, more meaningful way. This practice explores a fundamental method for evaluating whether a latent space has learned to separate data according to known class labels. By computing class \"prototypes\" as the average of latent codes and using a nearest-prototype classifier, you can quantitatively assess the geometric structure and class separability of a representation [@problem_id:3099275].", "problem": "You are given a representation learning evaluation task grounded in the concept of Autoencoder-based embeddings. Consider an autoencoder whose encoder is modeled as an affine map from an input space to a latent space. Formally, for an input vector $x \\in \\mathbb{R}^d$, the latent representation is $z = W x + b$, where $W \\in \\mathbb{R}^{k \\times d}$ and $b \\in \\mathbb{R}^k$ are fixed parameters. To assess whether the latent space $z$ aligns with class structure, you will compute class prototypes and evaluate nearest-prototype classification on held-out test data. Your program must implement the following from first principles.\n\nFundamental base to use:\n- Definition of an autoencoder encoder as a deterministic function $f_{\\text{enc}}: \\mathbb{R}^d \\to \\mathbb{R}^k$ given by $z = W x + b$.\n- Empirical class prototype defined as the sample mean of latent codes for a class.\n- Euclidean distance in $\\mathbb{R}^k$ defined by $\\|u - v\\|_2$.\n- Nearest-prototype classification rule: assign a test point to the class whose prototype is closest in Euclidean distance. In case of exact ties (identical minimal distances), break ties by choosing the smallest class label value.\n\nTask:\n- For each test case below, compute the latent codes $z$ for all training inputs, compute the prototype (empirical mean) for each class in latent space, then classify each test input by the nearest prototype (Euclidean distance) with the specified tie-breaking rule, and output the classification accuracy as the fraction of correctly classified test examples. Report each accuracy rounded to $3$ decimal places.\n\nImportant implementation details:\n- Classes are indexed by integers such as $0$, $1$, $2$, and so on.\n- Accuracy must be computed as $(\\text{number of correct predictions}) / (\\text{number of test samples})$ and expressed as a decimal (not a percentage).\n\nTest suite (four cases) covering separation, overlap, collapse, and multi-class with ties:\n- Case $1$ (well-separated classes, perfect alignment):\n  - Dimensions: $d = 2$, $k = 2$.\n  - Encoder: $W = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n  - Training data:\n    - Class $0$: $[-1, 0]$, $[0, 0]$, $[1, 0]$.\n    - Class $1$: $[4, 0]$, $[5, 0]$, $[6, 0]$.\n  - Test data and labels:\n    - Inputs: $[-0.5, 0]$, $[0.5, 0]$, $[4.5, 0]$, $[5.5, 0]$.\n    - Labels: $[0, 0, 1, 1]$.\n- Case $2$ (overlapping classes causing some errors):\n  - Dimensions: $d = 2$, $k = 2$.\n  - Encoder: $W = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n  - Training data:\n    - Class $0$: $[0, 0]$, $[0, 0]$.\n    - Class $1$: $[1, 0]$, $[1, 0]$.\n  - Test data and labels:\n    - Inputs: $[0.1, 0]$, $[0.9, 0]$, $[0.49, 0]$, $[0.51, 0]$.\n    - Labels: $[0, 1, 1, 1]$.\n- Case $3$ (collapsed encoder; identical prototypes; tests tie-breaking and chance-level accuracy):\n  - Dimensions: $d = 2$, $k = 2$.\n  - Encoder: $W = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n  - Training data:\n    - Class $0$: $[1, 2]$, $[3, 4]$.\n    - Class $1$: $[-1, -2]$, $[-3, -4]$.\n  - Test data and labels:\n    - Inputs: $[10, 10]$, $[-10, -10]$, $[7, -7]$, $[-7, 7]$.\n    - Labels: $[0, 1, 1, 0]$.\n- Case $4$ (multi-class with anisotropic encoder and an explicit tie):\n  - Dimensions: $d = 2$, $k = 2$.\n  - Encoder: $W = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n  - Training data:\n    - Class $0$: $[-0.5, 2]$, $[-0.5, 2]$.\n    - Class $1$: $[1.5, 2]$, $[1.5, 2]$.\n    - Class $2$: $[-0.5, 8]$, $[-0.5, 8]$.\n  - Test data and labels:\n    - Inputs: $[-0.5, 2.2]$, $[1.5, 1.8]$, $[-0.5, 5]$, $[-0.5, 8.2]$.\n    - Labels: $[0, 1, 2, 2]$.\n\nProgram requirements:\n- Implement the exact pipeline described using only the specified encoder model, Euclidean distance, empirical class means in latent space, and the specified tie-breaking rule.\n- Your program should produce a single line of output containing the four accuracies, each rounded to $3$ decimal places, as a comma-separated list enclosed in square brackets (e.g., $[0.875,0.500,1.000,0.750]$).", "solution": "The problem statement is a valid computational task grounded in the principles of linear algebra and elementary statistics, commonly used in the evaluation of representation learning methods. The problem is well-posed, with all necessary parameters, data, and procedural rules explicitly defined, ensuring a unique and verifiable solution for each test case. We will now proceed with a step-by-step solution.\n\nThe core of the task is to implement and evaluate a nearest-prototype classifier in a latent space defined by an affine transformation. The process for each test case involves three main stages:\n1.  **Latent Space Transformation**: Map all training and test data points from the input space $\\mathbb{R}^d$ to the latent space $\\mathbb{R}^k$ using the given encoder function.\n2.  **Prototype Computation**: For each class, compute a single representative vector, or \"prototype,\" by calculating the empirical mean of the latent codes of its training members.\n3.  **Classification and Evaluation**: For each test point, predict its class by finding the prototype that is closest in Euclidean distance. The final accuracy is the fraction of test points classified correctly.\n\nLet's formalize these steps. The encoder is an affine map $f_{\\text{enc}}: \\mathbb{R}^d \\to \\mathbb{R}^k$ defined as:\n$$z = f_{\\text{enc}}(x) = W x + b$$\nwhere $x \\in \\mathbb{R}^d$ is an input vector, $W \\in \\mathbb{R}^{k \\times d}$ is the weight matrix, and $b \\in \\mathbb{R}^k$ is the bias vector.\n\nFor a given class $C_j$ with a set of training samples $S_j = \\{x_1, x_2, \\ldots, x_{N_j}\\}$, the corresponding latent codes are $Z_j = \\{z_1, z_2, \\ldots, z_{N_j}\\}$, where $z_i = f_{\\text{enc}}(x_i)$. The class prototype $P_j \\in \\mathbb{R}^k$ is the sample mean of these latent codes:\n$$P_j = \\frac{1}{N_j} \\sum_{i=1}^{N_j} z_i$$\nFor a new test input $x_{\\text{test}}$, its latent code is $z_{\\text{test}} = f_{\\text{enc}}(x_{\\text{test}})$. The predicted class label $\\hat{y}$ is determined by the nearest-prototype rule:\n$$\\hat{y} = \\underset{j}{\\arg\\min} \\, \\| z_{\\text{test}} - P_j \\|_2$$\nwhere $\\| \\cdot \\|_2$ denotes the Euclidean distance. If the minimum distance is achieved for multiple classes, the tie is broken by selecting the class with the smallest integer label.\n\nFinally, the accuracy is computed as the ratio of correctly classified test samples to the total number of test samples. We now apply this procedure to each of the four test cases.\n\n**Case 1: Well-separated classes**\n- Dimensions: $d=2$, $k=2$.\n- Encoder: $W = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. The transformation is the identity, $z = x$.\n- Training data for class $0$: $\\left\\{ \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\}$.\n- Prototype for class $0$: $P_0 = \\frac{1}{3} \\left( \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Training data for class $1$: $\\left\\{ \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} \\right\\}$.\n- Prototype for class $1$: $P_1 = \\frac{1}{3} \\left( \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}$.\n- Test data: $z_{\\text{test},1}=\\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},2}=\\begin{bmatrix} 0.5 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},3}=\\begin{bmatrix} 4.5 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},4}=\\begin{bmatrix} 5.5 \\\\ 0 \\end{bmatrix}$. True labels: $[0, 0, 1, 1]$.\n- Predictions:\n  - For $z_{\\text{test},1}$: $\\|z_{\\text{test},1} - P_0\\|_2=0.5$, $\\|z_{\\text{test},1} - P_1\\|_2=5.5$. Predict $0$. Correct.\n  - For $z_{\\text{test},2}$: $\\|z_{\\text{test},2} - P_0\\|_2=0.5$, $\\|z_{\\text{test},2} - P_1\\|_2=4.5$. Predict $0$. Correct.\n  - For $z_{\\text{test},3}$: $\\|z_{\\text{test},3} - P_0\\|_2=4.5$, $\\|z_{\\text{test},3} - P_1\\|_2=0.5$. Predict $1$. Correct.\n  - For $z_{\\text{test},4}$: $\\|z_{\\text{test},4} - P_0\\|_2=5.5$, $\\|z_{\\text{test},4} - P_1\\|_2=0.5$. Predict $1$. Correct.\n- Accuracy: $4/4 = 1.0$.\n\n**Case 2: Overlapping classes**\n- Dimensions: $d=2$, $k=2$.\n- Encoder: $W = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. The transformation is the identity, $z = x$.\n- Training data for class $0$: $\\left\\{ \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\right\\}$. Prototype $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Training data for class $1$: $\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\}$. Prototype $P_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n- The decision boundary is the perpendicular bisector of the segment connecting $P_0$ and $P_1$, which is the line $x=0.5$.\n- Test data: $z_{\\text{test},1}=\\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},2}=\\begin{bmatrix} 0.9 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},3}=\\begin{bmatrix} 0.49 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},4}=\\begin{bmatrix} 0.51 \\\\ 0 \\end{bmatrix}$. True labels: $[0, 1, 1, 1]$.\n- Predictions:\n  - For $z_{\\text{test},1}$: $\\|z_{\\text{test},1} - P_0\\|_2=0.1$, $\\|z_{\\text{test},1} - P_1\\|_2=0.9$. Predict $0$. Correct.\n  - For $z_{\\text{test},2}$: $\\|z_{\\text{test},2} - P_0\\|_2=0.9$, $\\|z_{\\text{test},2} - P_1\\|_2=0.1$. Predict $1$. Correct.\n  - For $z_{\\text{test},3}$: $\\|z_{\\text{test},3} - P_0\\|_2=0.49$, $\\|z_{\\text{test},3} - P_1\\|_2=0.51$. Predict $0$. Incorrect (true label $1$).\n  - For $z_{\\text{test},4}$: $\\|z_{\\text{test},4} - P_0\\|_2=0.51$, $\\|z_{\\text{test},4} - P_1\\|_2=0.49$. Predict $1$. Correct.\n- Accuracy: $3/4 = 0.75$.\n\n**Case 3: Collapsed encoder**\n- Dimensions: $d=2$, $k=2$.\n- Encoder: $W = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. All inputs map to the origin: $z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Training data for class $0$: $\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\right\\}$. Both map to $z=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. Prototype $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Training data for class $1$: $\\left\\{ \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} \\right\\}$. Both map to $z=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. Prototype $P_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- The prototypes for both classes are identical.\n- Test data: Any test input $x_{\\text{test}}$ is mapped to $z_{\\text{test}} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Predictions: For any $z_{\\text{test}}$, the distance to both prototypes is $0$: $\\|z_{\\text{test}} - P_0\\|_2 = 0$ and $\\|z_{\\text{test}} - P_1\\|_2 = 0$. This is an exact tie. According to the tie-breaking rule, we choose the smallest class label, which is $0$. Therefore, every test sample is predicted as class $0$.\n- Predicted labels: $[0, 0, 0, 0]$. True labels: $[0, 1, 1, 0]$.\n- Correct predictions are for the first and fourth samples.\n- Accuracy: $2/4 = 0.5$.\n\n**Case 4: Multi-class with anisotropic encoder and tie**\n- Dimensions: $d=2$, $k=2$.\n- Encoder: $W = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. The mapping is $z = \\begin{bmatrix} 2x_1+1 \\\\ 0.5x_2-1 \\end{bmatrix}$.\n- Prototypes:\n  - Class $0$ (train: $\\begin{bmatrix} -0.5 \\\\ 2 \\end{bmatrix}$): $z = \\begin{bmatrix} 2(-0.5)+1 \\\\ 0.5(2)-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. So, $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n  - Class $1$ (train: $\\begin{bmatrix} 1.5 \\\\ 2 \\end{bmatrix}$): $z = \\begin{bmatrix} 2(1.5)+1 \\\\ 0.5(2)-1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}$. So, $P_1 = \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}$.\n  - Class $2$ (train: $\\begin{bmatrix} -0.5 \\\\ 8 \\end{bmatrix}$): $z = \\begin{bmatrix} 2(-0.5)+1 \\\\ 0.5(8)-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$. So, $P_2 = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$.\n- Test data transformation and classification. True labels: $[0, 1, 2, 2]$.\n  - $x_{\\text{test},1} = \\begin{bmatrix} -0.5 \\\\ 2.2 \\end{bmatrix} \\implies z_1 = \\begin{bmatrix} 0 \\\\ 0.1 \\end{bmatrix}$.\n    - $\\|z_1 - P_0\\|_2 = 0.1$, $\\|z_1 - P_1\\|_2 = \\sqrt{4^2+0.1^2} \\approx 4.001$, $\\|z_1 - P_2\\|_2 = 2.9$. Predict $0$. Correct.\n  - $x_{\\text{test},2} = \\begin{bmatrix} 1.5 \\\\ 1.8 \\end{bmatrix} \\implies z_2 = \\begin{bmatrix} 4 \\\\ -0.1 \\end{bmatrix}$.\n    - $\\|z_2 - P_0\\|_2 \\approx 4.001$, $\\|z_2 - P_1\\|_2 = 0.1$, $\\|z_2 - P_2\\|_2 = \\sqrt{4^2+(-3.1)^2} \\approx 5.06$. Predict $1$. Correct.\n  - $x_{\\text{test},3} = \\begin{bmatrix} -0.5 \\\\ 5 \\end{bmatrix} \\implies z_3 = \\begin{bmatrix} 0 \\\\ 1.5 \\end{bmatrix}$.\n    - $\\|z_3 - P_0\\|_2 = 1.5$, $\\|z_3 - P_1\\|_2 = \\sqrt{4^2+1.5^2} \\approx 4.27$, $\\|z_3 - P_2\\|_2 = 1.5$.\n    - Tie between class $0$ and class $2$. Smallest label is $0$. Predict $0$. Incorrect (true label $2$).\n  - $x_{\\text{test},4} = \\begin{bmatrix} -0.5 \\\\ 8.2 \\end{bmatrix} \\implies z_4 = \\begin{bmatrix} 0 \\\\ 3.1 \\end{bmatrix}$.\n    - $\\|z_4 - P_0\\|_2 = 3.1$, $\\|z_4 - P_1\\|_2 \\approx 5.06$, $\\|z_4 - P_2\\|_2 = 0.1$. Predict $2$. Correct.\n- Accuracy: $3/4 = 0.75$.\n\nThe calculated accuracies are $1.0$, $0.75$, $0.5$, and $0.75$. These will be rounded to $3$ decimal places for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes classification accuracy for four test cases using a nearest-prototype\n    classifier in an autoencoder's latent space.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-separated classes, perfect alignment)\n        {\n            'W': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([-1.0, 0.0]), np.array([0.0, 0.0]), np.array([1.0, 0.0])],\n                1: [np.array([4.0, 0.0]), np.array([5.0, 0.0]), np.array([6.0, 0.0])]\n            },\n            'test_inputs': [\n                np.array([-0.5, 0.0]), np.array([0.5, 0.0]),\n                np.array([4.5, 0.0]), np.array([5.5, 0.0])\n            ],\n            'test_labels': [0, 0, 1, 1]\n        },\n        # Case 2 (overlapping classes causing some errors)\n        {\n            'W': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([0.0, 0.0]), np.array([0.0, 0.0])],\n                1: [np.array([1.0, 0.0]), np.array([1.0, 0.0])]\n            },\n            'test_inputs': [\n                np.array([0.1, 0.0]), np.array([0.9, 0.0]),\n                np.array([0.49, 0.0]), np.array([0.51, 0.0])\n            ],\n            'test_labels': [0, 1, 1, 1]\n        },\n        # Case 3 (collapsed encoder; tests tie-breaking)\n        {\n            'W': np.array([[0.0, 0.0], [0.0, 0.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([1.0, 2.0]), np.array([3.0, 4.0])],\n                1: [np.array([-1.0, -2.0]), np.array([-3.0, -4.0])]\n            },\n            'test_inputs': [\n                np.array([10.0, 10.0]), np.array([-10.0, -10.0]),\n                np.array([7.0, -7.0]), np.array([-7.0, 7.0])\n            ],\n            'test_labels': [0, 1, 1, 0]\n        },\n        # Case 4 (multi-class with anisotropic encoder and an explicit tie)\n        {\n            'W': np.array([[2.0, 0.0], [0.0, 0.5]]),\n            'b': np.array([1.0, -1.0]),\n            'train_data': {\n                0: [np.array([-0.5, 2.0]), np.array([-0.5, 2.0])],\n                1: [np.array([1.5, 2.0]), np.array([1.5, 2.0])],\n                2: [np.array([-0.5, 8.0]), np.array([-0.5, 8.0])]\n            },\n            'test_inputs': [\n                np.array([-0.5, 2.2]), np.array([1.5, 1.8]),\n                np.array([-0.5, 5.0]), np.array([-0.5, 8.2])\n            ],\n            'test_labels': [0, 1, 2, 2]\n        }\n    ]\n\n    accuracies = []\n    \n    for case in test_cases:\n        W = case['W']\n        b = case['b']\n        train_data = case['train_data']\n        test_inputs = case['test_inputs']\n        test_labels = case['test_labels']\n        \n        prototypes = {}\n        # Ensure class labels are sorted for consistent processing\n        class_labels = sorted(train_data.keys())\n        \n        # 1. Compute class prototypes in latent space\n        for label in class_labels:\n            points = train_data[label]\n            # Encode each training point x to latent code z = Wx + b\n            latent_codes = [W @ x + b for x in points]\n            # Prototype is the mean of the latent codes\n            prototypes[label] = np.mean(latent_codes, axis=0)\n            \n        predictions = []\n        # 2. Classify each test input\n        for x_test in test_inputs:\n            # Encode the test point\n            z_test = W @ x_test + b\n            \n            # Calculate Euclidean distance to each prototype\n            distances = []\n            for label in class_labels:\n                p = prototypes[label]\n                dist = np.linalg.norm(z_test - p)\n                distances.append((dist, label))\n            \n            # Find the minimum distance\n            min_dist = min(d for d, _ in distances)\n            \n            # Find all class labels corresponding to the minimum distance (handles ties)\n            # Use np.isclose for robust floating-point comparison\n            tied_labels = [label for dist, label in distances if np.isclose(dist, min_dist)]\n            \n            # Apply the tie-breaking rule: choose the smallest class label\n            predicted_label = min(tied_labels)\n            predictions.append(predicted_label)\n            \n        # 3. Calculate accuracy for the current case\n        correct_predictions = sum(1 for pred, true in zip(predictions, test_labels) if pred == true)\n        accuracy = correct_predictions / len(test_labels)\n        accuracies.append(accuracy)\n\n    # Format the results as specified: a list of floats rounded to 3 decimal places\n    formatted_accuracies = [f\"{acc:.3f}\" for acc in accuracies]\n    print(f\"[{','.join(formatted_accuracies)}]\")\n\nsolve()\n```", "id": "3099275"}, {"introduction": "While geometric separability is important, a powerful representation should also be useful for downstream tasks. This practice advances our evaluation toolkit by introducing linear probing, a standard method for assessing a representation's utility by training a simple linear classifier on the learned latent codes. You will implement and train autoencoders with two different reconstruction losses—pixel-wise Mean Squared Error (MSE) and a feature-space perceptual loss—to see firsthand how the training objective shapes the quality and usefulness of the final representation [@problem_id:3099257].", "problem": "You will implement and compare two linear autoencoder variants for representation learning on a synthetic image dataset, differing only in the reconstruction loss: pixel-wise Mean Squared Error (MSE) versus a perceptual loss computed in a fixed feature space. The goal is to evaluate how the choice of reconstruction loss affects the learned latent representation by training a linear probe classifier on the latent codes.\n\nFundamental base and definitions:\n- An autoencoder is a pair of parametric functions $(f_{\\text{enc}}, f_{\\text{dec}})$ trained to minimize a reconstruction objective on data $\\{x_i\\}_{i=1}^{N}$, where $x_i \\in \\mathbb{R}^{D}$. The encoder maps $x$ to a latent code $z = f_{\\text{enc}}(x)$, and the decoder reconstructs $\\hat{x} = f_{\\text{dec}}(z)$, with the aim of learning a representation that captures regularities in the data. The training follows empirical risk minimization under a specified reconstruction loss.\n- In this problem, you will restrict to a linear autoencoder: $z = W_e x$ and $\\hat{x} = W_d z$, where $W_e \\in \\mathbb{R}^{d \\times D}$ and $W_d \\in \\mathbb{R}^{D \\times d}$, with $d$ being the latent dimension.\n- Two reconstruction losses will be compared:\n  1. Pixel-wise Mean Squared Error (MSE): this measures reconstruction error in input space as the sum of squared differences between $x$ and $\\hat{x}$.\n  2. Perceptual loss in a fixed feature space $\\phi$: this measures reconstruction error after a fixed linear feature transform $\\phi(x) = A x$, as the sum of squared differences between $\\phi(x)$ and $\\phi(\\hat{x})$.\n\nDataset specification:\n- You will construct a synthetic dataset of binary images on an $8 \\times 8$ grid, flattened to vectors in $\\mathbb{R}^{64}$. There are $3$ classes defined by simple structural patterns:\n  - Class $0$: vertical stripes on even columns.\n  - Class $1$: horizontal stripes on even rows.\n  - Class $2$: a diagonal line; to add variability, allow random single-pixel shifts along the diagonal direction.\n- Generate $N_{\\text{train}} = 120$ training samples and $N_{\\text{test}} = 60$ test samples, balanced as $40$ and $20$ per class, respectively. Add independent Gaussian noise with standard deviation $\\sigma = 0.1$ to each image and clip values to $[0,1]$ to maintain valid pixel intensities. Use a fixed random seed for reproducibility.\n\nFeature transform $\\phi$:\n- For the perceptual loss, use a fixed linear transform $\\phi(x) = A x$ where $A \\in \\mathbb{R}^{m \\times D}$. Construct $A$ as follows:\n  - Build a discrete gradient operator $G \\in \\mathbb{R}^{D_g \\times D}$ that computes horizontal and vertical finite differences on the $8 \\times 8$ image grid, where $D_g = 8 \\cdot 7 + 7 \\cdot 8 = 112$. Specifically, horizontal differences are $x_{i,j+1} - x_{i,j}$ and vertical differences are $x_{i+1,j} - x_{i,j}$, stacked into a single vector.\n  - For cases labeled “gradient”, choose $m$ and create a random Gaussian projection $P \\in \\mathbb{R}^{m \\times D_g}$ with appropriately scaled entries so that $A = P G \\in \\mathbb{R}^{m \\times D}$. For the “identity” case, set $A = I_D \\in \\mathbb{R}^{D \\times D}$.\n\nTraining and evaluation protocol:\n- Train two separate linear autoencoders with identical architecture per test case, differing only in the reconstruction loss:\n  - Pixel-wise MSE loss in input space.\n  - Perceptual loss in feature space defined by $A$.\n- Use full-batch gradient descent with learning rate $\\eta$, number of epochs $E$, and weight decay regularization parameter $\\gamma$ on both $W_e$ and $W_d$.\n- After training each autoencoder, compute latent codes $z$ for both train and test sets. Train a linear probe classifier on the train codes using ridge regression (i.e., linear least squares with $\\ell_2$ regularization) to predict the class labels (encoded as one-hot vectors). Evaluate classification accuracy on the test codes by taking the class with the largest predicted score.\n\nTest suite:\nImplement the above for the following three test cases, each specified by latent dimension $d$, feature dimension $m$, and the type of $A$ (“gradient” or “identity”):\n- Case $\\mathrm{A}$: $d = 4$, $m = 32$, $A$ type = “gradient”.\n- Case $\\mathrm{B}$: $d = 64$, $m = 64$, $A$ type = “identity”.\n- Case $\\mathrm{C}$: $d = 2$, $m = 16$, $A$ type = “gradient”.\n\nHyperparameters and constants:\n- Image grid size $8 \\times 8$, flattened dimension $D = 64$.\n- Training samples $N_{\\text{train}} = 120$, test samples $N_{\\text{test}} = 60$.\n- Gaussian noise standard deviation $\\sigma = 0.1$.\n- Learning rate $\\eta = 0.1$, epochs $E = 200$, weight decay $\\gamma = 10^{-4}$.\n- Ridge regression regularization $\\lambda = 10^{-3}$.\n- Use a fixed random seed $s = 123$ for all randomized components.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain, in order, for each test case, the linear probe accuracy (as a decimal) for the MSE-trained autoencoder followed by the accuracy for the perceptual-loss-trained autoencoder. Concretely, the output must be of the form\n$[a_{\\mathrm{A},\\mathrm{MSE}}, a_{\\mathrm{A},\\mathrm{perc}}, a_{\\mathrm{B},\\mathrm{MSE}}, a_{\\mathrm{B},\\mathrm{perc}}, a_{\\mathrm{C},\\mathrm{MSE}}, a_{\\mathrm{C},\\mathrm{perc}}]$,\nwhere each $a_{\\cdot,\\cdot}$ is a float in $[0,1]$.", "solution": "The problem statement is deemed valid. It is scientifically grounded in the field of representation learning, well-posed with a clear objective and constraints, and formulated using objective, formalizable language. All parameters and procedures are specified, allowing for a unique and verifiable solution.\n\nThe problem requires the implementation and comparison of two linear autoencoder variants, differing in their reconstruction loss function, to assess their impact on the quality of learned representations. The evaluation is performed by training a linear classifier (a \"probe\") on the learned latent codes.\n\n### 1. Synthetic Dataset Generation\n\nThe dataset consists of $8 \\times 8$ binary images, flattened into vectors $x \\in \\mathbb{R}^{D}$ where $D=64$. There are three classes of patterns.\n- **Class 0 (Vertical Stripes)**: The image has a value of $1$ in even-numbered columns ($0, 2, 4, 6$) and $0$ otherwise.\n- **Class 1 (Horizontal Stripes)**: The image has a value of $1$ in even-numbered rows ($0, 2, 4, 6$) and $0$ otherwise.\n- **Class 2 (Diagonal Line)**: The image has a value of $1$ on the main diagonal ($i=j$) and $0$ otherwise. To introduce variability, each sample of this class is generated by taking the base diagonal image and applying a random shift of $(s, s)$ where $s$ is chosen uniformly from $\\{-1, 0, 1\\}$.\n\nA training set of $N_{\\text{train}}=120$ samples ($40$ per class) and a test set of $N_{\\text{test}}=60$ samples ($20$ per class) are generated. Independent Gaussian noise with mean $0$ and standard deviation $\\sigma=0.1$ is added to each pixel of every image. The pixel values are then clipped to the range $[0, 1]$. All random processes use a fixed seed $s=123$ for reproducibility.\n\nThe data matrices are denoted $X_{\\text{train}} \\in \\mathbb{R}^{D \\times N_{\\text{train}}}$ and $X_{\\text{test}} \\in \\mathbb{R}^{D \\times N_{\\text{test}}}$, where each column is a flattened image vector.\n\n### 2. Linear Autoencoder Model\n\nThe autoencoder consists of a linear encoder $f_{\\text{enc}}$ and a linear decoder $f_{\\text{dec}}$.\n- **Encoder**: $z = f_{\\text{enc}}(x) = W_e x$, where $x \\in \\mathbb{R}^D$ is the input, $z \\in \\mathbb{R}^d$ is the latent code, and $W_e \\in \\mathbb{R}^{d \\times D}$ is the encoder weight matrix.\n- **Decoder**: $\\hat{x} = f_{\\text{dec}}(z) = W_d z$, where $\\hat{x} \\in \\mathbb{R}^D$ is the reconstruction, and $W_d \\in \\mathbb{R}^{D \\times d}$ is the decoder weight matrix.\nThe full mapping is $\\hat{x} = W_d W_e x$. For a batch of data $X \\in \\mathbb{R}^{D \\times N}$, the reconstruction is $\\hat{X} = W_d W_e X$.\n\n### 3. Reconstruction Losses and Training\n\nTwo loss functions are compared. For a batch of $N$ samples, the total loss $\\mathcal{L}_{\\text{total}}$ includes a reconstruction term and an $\\ell_2$ regularization (weight decay) term on the weights. The reconstruction loss is averaged over the batch.\n$$\n\\mathcal{L}_{\\text{total}}(W_e, W_d) = \\mathcal{L}_{\\text{rec}} + \\frac{\\gamma}{2} \\left( \\|W_e\\|_F^2 + \\|W_d\\|_F^2 \\right)\n$$\nwhere $\\gamma=10^{-4}$ is the weight decay parameter and $\\|\\cdot\\|_F$ is the Frobenius norm.\n\n**a) Pixel-wise Mean Squared Error (MSE) Loss**\nThe loss is the mean squared error between the input and the reconstruction.\n$$\n\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^N \\|x_i - \\hat{x}_i\\|_2^2 = \\frac{1}{N} \\|X - \\hat{X}\\|_F^2\n$$\n\n**b) Perceptual Loss**\nThe loss is the mean squared error in a fixed feature space defined by a linear transform $\\phi(x) = Ax$.\n$$\n\\mathcal{L}_{\\text{perc}} = \\frac{1}{N} \\sum_{i=1}^N \\|\\phi(x_i) - \\phi(\\hat{x}_i)\\|_2^2 = \\frac{1}{N} \\|A(X - \\hat{X})\\|_F^2\n$$\nThe matrix $A \\in \\mathbb{R}^{m \\times D}$ is constructed as follows:\n- For the \"identity\" case, $A = I_D$, the identity matrix of size $D=64$. In this case, $\\mathcal{L}_{\\text{perc}}$ becomes identical to $\\mathcal{L}_{\\text{MSE}}$.\n- For the \"gradient\" case, $A = PG$, where $G \\in \\mathbb{R}^{112 \\times 64}$ is a discrete gradient operator computing horizontal and vertical finite differences on the $8 \\times 8$ grid. $P \\in \\mathbb{R}^{m \\times 112}$ is a random projection matrix with entries drawn from a Gaussian distribution $N(0, 1/m)$, which is an appropriate scaling for preserving distances.\n\n**Training via Gradient Descent**\nThe weights $W_e$ and $W_d$ are trained using full-batch gradient descent for $E=200$ epochs with learning rate $\\eta=0.1$. The gradients of the total loss with respect to the weights are:\n$$\n\\nabla_{W_d} \\mathcal{L}_{\\text{total}} = \\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_d} + \\gamma W_d \\quad ; \\quad \\nabla_{W_e} \\mathcal{L}_{\\text{total}} = \\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_e} + \\gamma W_e\n$$\nLet $\\hat{X} = W_d W_e X$. The gradients of the reconstruction loss terms are derived using the chain rule. For both loss types, we can write:\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_d} = \\left(\\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial \\hat{X}}\\right) (W_e X)^T \\quad ; \\quad \\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_e} = W_d^T \\left(\\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial \\hat{X}}\\right) X^T\n$$\nThe gradient with respect to $\\hat{X}$ depends on the loss function:\n- For MSE: $\\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial \\hat{X}} = \\frac{2}{N} (\\hat{X} - X)$\n- For Perceptual: $\\frac{\\partial \\mathcal{L}_{\\text{perc}}}{\\partial \\hat{X}} = \\frac{2}{N} A^T A (\\hat{X} - X)$\n\nThe weights are initialized using Glorot uniform initialization and updated in each epoch:\n$W_e \\leftarrow W_e - \\eta \\nabla_{W_e} \\mathcal{L}_{\\text{total}}$ and $W_d \\leftarrow W_d - \\eta \\nabla_{W_d} \\mathcal{L}_{\\text{total}}$.\n\n### 4. Linear Probe Evaluation\n\nAfter an autoencoder is trained, its encoder $W_e$ is used to generate latent codes for the training and test sets: $Z_{\\text{train}} = W_e X_{\\text{train}}$ and $Z_{\\text{test}} = W_e X_{\\text{test}}$. To evaluate the quality of these representations, a linear classifier (probe) is trained to predict the class labels from the latent codes.\n\nThe probe is a linear model trained with ridge regression. The labels $y_{\\text{train}}$ are one-hot encoded into a matrix $Y_{\\text{oh}} \\in \\mathbb{R}^{N_{\\text{train}} \\times 3}$. The probe's weights $W_{\\text{probe}} \\in \\mathbb{R}^{d \\times 3}$ are found by solving the regularized least-squares problem:\n$$\n\\min_{W_{\\text{probe}}} \\|Z_{\\text{train}} W_{\\text{probe}} - Y_{\\text{oh}}\\|_F^2 + \\lambda \\|W_{\\text{probe}}\\|_F^2\n$$\nwhere $Z_{\\text{train}} \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ is the matrix of training codes and $\\lambda = 10^{-3}$ is the regularization parameter. The closed-form solution is:\n$$\nW_{\\text{probe}} = (Z_{\\text{train}}^T Z_{\\text{train}} + \\lambda I_d)^{-1} Z_{\\text{train}}^T Y_{\\text{oh}}\n$$\nwhere $I_d$ is the $d \\times d$ identity matrix.\n\nThe trained probe is then used to predict labels for the test set. The predicted scores are $\\hat{Y}_{\\text{test}} = Z_{\\text{test}}^T W_{\\text{probe}}$. The predicted class for each sample is the one with the highest score, i.e., $\\hat{y}_i = \\arg\\max_j (\\hat{Y}_{\\text{test}})_{ij}$. The final classification accuracy is the fraction of correctly predicted labels on the test set.\n\nThis entire procedure is repeated for each of the three test cases, for both the MSE-trained and the perceptual-loss-trained autoencoders. The resulting six accuracy values are reported. For Case B, where $A=I_D$, the MSE and perceptual losses are identical. Therefore, the trained models and resulting accuracies are expected to be the same, providing a sanity check for the implementation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares linear autoencoders with MSE and perceptual loss.\n    \"\"\"\n\n    # --- Problem Constants and Hyperparameters ---\n    H, W = 8, 8\n    D = H * W\n    N_TRAIN, N_TEST = 120, 60\n    N_CLASSES = 3\n    N_PER_CLASS_TRAIN = N_TRAIN // N_CLASSES\n    N_PER_CLASS_TEST = N_TEST // N_CLASSES\n    NOISE_STD = 0.1\n    ETA = 0.1\n    EPOCHS = 200\n    GAMMA = 1e-4\n    LAMBDA_RIDGE = 1e-3\n    SEED = 123\n\n    TEST_CASES = [\n        {'d': 4, 'm': 32, 'A_type': 'gradient'}, # Case A\n        {'d': 64, 'm': 64, 'A_type': 'identity'}, # Case B\n        {'d': 2, 'm': 16, 'A_type': 'gradient'},  # Case C\n    ]\n\n    rng = np.random.default_rng(SEED)\n\n    def generate_dataset():\n        \"\"\"Generates the synthetic image dataset.\"\"\"\n        # Base patterns\n        patterns = []\n        # Class 0: Vertical stripes\n        p0 = np.zeros((H, W))\n        p0[:, ::2] = 1\n        patterns.append(p0.flatten())\n        # Class 1: Horizontal stripes\n        p1 = np.zeros((H, W))\n        p1[::2, :] = 1\n        patterns.append(p1.flatten())\n        # Class 2: Diagonal line (base)\n        patterns.append(np.eye(H).flatten()) # This is just a placeholder\n\n        def create_data_for_class(n_samples, class_idx):\n            if class_idx  2:\n                base_pattern = patterns[class_idx]\n                X = np.tile(base_pattern, (n_samples, 1))\n            else: # Class 2: Diagonal with shifts\n                base_diag = np.eye(H)\n                X_list = []\n                shifts = rng.choice([-1, 0, 1], size=n_samples)\n                for s in shifts:\n                    shifted_img = np.roll(base_diag, shift=(s, s), axis=(0, 1))\n                    X_list.append(shifted_img.flatten())\n                X = np.array(X_list)\n            \n            # Add noise and clip\n            X += rng.normal(loc=0.0, scale=NOISE_STD, size=X.shape)\n            np.clip(X, 0, 1, out=X)\n            return X\n\n        X_train_list, y_train_list = [], []\n        X_test_list, y_test_list = [], []\n        for i in range(N_CLASSES):\n            X_train_list.append(create_data_for_class(N_PER_CLASS_TRAIN, i))\n            y_train_list.append(np.full(N_PER_CLASS_TRAIN, i))\n            X_test_list.append(create_data_for_class(N_PER_CLASS_TEST, i))\n            y_test_list.append(np.full(N_PER_CLASS_TEST, i))\n\n        X_train = np.vstack(X_train_list).T  # Shape (D, N_train)\n        y_train = np.concatenate(y_train_list)\n        X_test = np.vstack(X_test_list).T   # Shape (D, N_test)\n        y_test = np.concatenate(y_test_list)\n        \n        return X_train, y_train, X_test, y_test\n\n    def build_A(m, A_type):\n        \"\"\"Builds the feature transform matrix A.\"\"\"\n        if A_type == 'identity':\n            assert m == D\n            return np.eye(D)\n        \n        if A_type == 'gradient':\n            Dg = H * (W - 1) + (H - 1) * W  # 112\n            G = np.zeros((Dg, D))\n            row_idx = 0\n            # Horizontal differences\n            for r in range(H):\n                for c in range(W - 1):\n                    idx1 = r * W + c\n                    idx2 = r * W + c + 1\n                    G[row_idx, idx1] = -1\n                    G[row_idx, idx2] = 1\n                    row_idx += 1\n            # Vertical differences\n            for r in range(H - 1):\n                for c in range(W):\n                    idx1 = r * W + c\n                    idx2 = (r + 1) * W + c\n                    G[row_idx, idx1] = -1\n                    G[row_idx, idx2] = 1\n                    row_idx += 1\n            \n            # Random projection matrix P\n            P = rng.normal(loc=0.0, scale=1.0 / np.sqrt(m), size=(m, Dg))\n            return P @ G\n        return None\n\n    def train_autoencoder(X, loss_type, A, d):\n        \"\"\"Trains a linear autoencoder.\"\"\"\n        N = X.shape[1]\n\n        # Glorot uniform initialization\n        lim_e = np.sqrt(6.0 / (D + d))\n        We = rng.uniform(-lim_e, lim_e, (d, D))\n        lim_d = np.sqrt(6.0 / (d + D))\n        Wd = rng.uniform(-lim_d, lim_d, (D, d))\n        \n        for _ in range(EPOCHS):\n            # Forward pass\n            Z = We @ X\n            X_hat = Wd @ Z\n            \n            # Gradient computation\n            E = X_hat - X\n            \n            if loss_type == 'mse':\n                grad_X_hat = (2.0 / N) * E\n            elif loss_type == 'perceptual':\n                E_perc = A @ E\n                grad_X_hat = (2.0 / N) * A.T @ E_perc\n            else:\n                raise ValueError(\"Unknown loss type\")\n            \n            grad_Wd = grad_X_hat @ Z.T + GAMMA * Wd\n            grad_We = Wd.T @ grad_X_hat @ X.T + GAMMA * We\n            \n            # Update weights\n            Wd -= ETA * grad_Wd\n            We -= ETA * grad_We\n            \n        return We\n\n    def evaluate_probe(We, X_train, y_train, X_test, y_test):\n        \"\"\"Trains and evaluates a linear probe on latent codes.\"\"\"\n        d = We.shape[0]\n        N_train, N_test = X_train.shape[1], X_test.shape[1]\n        \n        # Get latent codes, transpose to (N, d)\n        Z_train = (We @ X_train).T\n        Z_test = (We @ X_test).T\n        \n        # One-hot encode training labels\n        Y_train_oh = np.zeros((N_train, N_CLASSES))\n        Y_train_oh[np.arange(N_train), y_train] = 1\n        \n        # Train ridge regression probe\n        # W_probe = (Z^T Z + lambda I)^-1 Z^T Y\n        ZT_Z = Z_train.T @ Z_train\n        inv_term = np.linalg.inv(ZT_Z + LAMBDA_RIDGE * np.eye(d))\n        W_probe = inv_term @ Z_train.T @ Y_train_oh\n        \n        # Evaluate on test set\n        Y_pred_test_scores = Z_test @ W_probe\n        y_pred_test = np.argmax(Y_pred_test_scores, axis=1)\n        \n        accuracy = np.mean(y_pred_test == y_test)\n        return accuracy\n\n    # --- Main Execution ---\n    X_train, y_train, X_test, y_test = generate_dataset()\n    results = []\n\n    for case in TEST_CASES:\n        d, m, A_type = case['d'], case['m'], case['A_type']\n        \n        A = build_A(m, A_type)\n        \n        # Train and evaluate MSE model\n        We_mse = train_autoencoder(X_train, 'mse', A, d)\n        acc_mse = evaluate_probe(We_mse, X_train, y_train, X_test, y_test)\n        results.append(acc_mse)\n        \n        # Train and evaluate Perceptual model\n        We_perc = train_autoencoder(X_train, 'perceptual', A, d)\n        acc_perc = evaluate_probe(We_perc, X_train, y_train, X_test, y_test)\n        results.append(acc_perc)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\n\nsolve()\n```", "id": "3099257"}, {"introduction": "An ideal representation is not just useful, but also interpretable. This leads to the concept of \"disentanglement,\" where individual axes of the latent space correspond to distinct, meaningful factors of variation in the data. In this advanced practice, you will explore how to quantitatively measure disentanglement in a controlled synthetic environment where the true generative factors are known, using a whitening transform and correlation analysis to test the alignment between latent codes and ground-truth parameters [@problem_id:3099320].", "problem": "You are given a synthetic setting to test disentanglement in a linear autoencoder variant for representation learning. The observed data are generated from independent physical parameters: mass, friction, and force. Let the parameter vector be $p \\in \\mathbb{R}^3$ with coordinates $p = (m, b, F)$, where $m$ denotes mass, $b$ denotes friction coefficient, and $F$ denotes external force. Observations are generated by a linear mixing model with additive Gaussian noise:\n$$\nx = A p + \\varepsilon,\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$ is an invertible mixing matrix, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_3)$ with $I_3$ the $3 \\times 3$ identity matrix and $\\sigma \\ge 0$. Assume $p$ is sampled i.i.d. across $N$ examples with independent coordinates, each uniformly distributed within a fixed interval.\n\nDefine a linear autoencoder (AE) with encoder $f(x) = W x$ and decoder $g(z) = V z$, where $W, V \\in \\mathbb{R}^{3 \\times 3}$. Consider the following training objective with a disentanglement regularizer:\n$$\n\\mathcal{L}(W, V) = \\mathbb{E}\\left[\\|x - V W x\\|_2^2\\right] + \\lambda \\left\\|\\mathrm{OffDiag}\\left(\\mathrm{Cov}(z)\\right)\\right\\|_F^2,\n$$\nwhere $z = W x$, $\\mathrm{Cov}(z)$ is the covariance matrix of $z$, $\\mathrm{OffDiag}(\\cdot)$ zeros out the diagonal entries of its matrix argument, $\\|\\cdot\\|_F$ is the Frobenius norm, and $\\lambda \\ge 0$ controls the strength of the factorization penalty. In the limit $\\lambda \\to \\infty$, the encoder must enforce a factorized latent representation such that $\\mathrm{Cov}(z)$ is diagonal (that is, $z$ has uncorrelated coordinates with some variances). This family of solutions admits the particular constraint $\\mathrm{Cov}(z) = I_3$, also known as whitening. Under whitening, multiple encoders are possible that differ by an orthogonal rotation.\n\nYour task is to implement an evaluator that, for a fixed dataset and for a chosen whitening encoder $W$ derived from the sample covariance of $x$, probes whether each latent coordinate $z_i$ corresponds to exactly one ground-truth physical parameter component in $p$. Correspondence is defined by a one-to-one assignment between coordinates of $z$ and coordinates of $p$ that maximizes the total absolute Pearson correlation, with success declared if all matched absolute correlations exceed a given threshold $\\tau$.\n\nUse the following fundamental bases and definitions without revealing the derivation targets directly:\n- The covariance of a zero-mean random vector $x$ with distribution having finite second moments is $\\Sigma_x = \\mathbb{E}[x x^\\top]$.\n- A whitening transform $W$ satisfies $W \\Sigma_x W^\\top = I_3$.\n- Principal Component Analysis (PCA) whitening is achieved by the spectral decomposition $\\Sigma_x = U \\Lambda U^\\top$ with $U$ orthogonal and $\\Lambda$ diagonal with positive entries, and setting $W = \\Lambda^{-1/2} U^\\top$.\n- The Pearson correlation between zero-mean scalars $a$ and $b$ is $\\rho(a, b) = \\frac{\\mathbb{E}[a b]}{\\sqrt{\\mathbb{E}[a^2]} \\sqrt{\\mathbb{E}[b^2]}}$.\n\nImplement the following procedure in a single program that produces the final evaluation booleans for the test suite:\n1. Fix a random seed and sample $N$ parameter vectors $p = (m, b, F)$ with independent coordinates $m \\sim \\mathrm{Uniform}([1, 3])$, $b \\sim \\mathrm{Uniform}([0.2, 0.8])$, and $F \\sim \\mathrm{Uniform}([2, 10])$, where $N = 5000$. Treat the dataset as population (that is, use population moments).\n2. For each test case $(A, \\sigma)$, generate observations $x = A p + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_3)$ independently per sample.\n3. Center $x$ by subtracting the sample mean, compute the sample covariance $\\hat{\\Sigma}_x$, and construct the PCA whitening transform $W = \\Lambda^{-1/2} U^\\top$ using the eigen-decomposition $\\hat{\\Sigma}_x = U \\Lambda U^\\top$. For numerical stability, add a small regularizer $\\epsilon I_3$ to $\\hat{\\Sigma}_x$ with $\\epsilon = 10^{-6}$ before inversion.\n4. Compute $z = W x$ for the centered $x$ and probe for disentanglement by computing the $3 \\times 3$ absolute Pearson correlation matrix $C$ with $C_{ij} = |\\rho(z_i, p_j)|$, where $z_i$ is the $i$-th coordinate across samples, and $p_j$ is the $j$-th ground-truth parameter across samples. Find the one-to-one matching between $z$ coordinates and $p$ coordinates that maximizes $\\sum_{i=1}^3 C_{i, \\pi(i)}$ over permutations $\\pi$. Declare success if $\\min_{i} C_{i, \\pi^\\star(i)} \\ge \\tau$, where $\\pi^\\star$ is the maximizing permutation.\n5. Output a single list of booleans, one for each test case, indicating success or failure of disentanglement by the above criterion.\n\nUse the following test suite to cover different regimes:\n- Case $1$ (already disentangled): $A = I_3$, $\\sigma = 0.01$.\n- Case $2$ (axis scaling): $A = \\mathrm{diag}(2, 0.5, 1.5)$, $\\sigma = 0.02$.\n- Case $3$ (orthogonal rotation, ambiguous up to rotation): $A = R$, where\n$$\nR = \\begin{bmatrix}\n\\cos(\\theta)  -\\sin(\\theta)  0 \\\\\n\\sin(\\theta)  \\cos(\\theta)  0 \\\\\n0  0  1\n\\end{bmatrix}, \\quad \\theta = \\frac{\\pi}{4},\n$$\nand $\\sigma = 0.01$.\n- Case $4$ (entangled and noisy): \n$$\nA = \\begin{bmatrix}\n1  1  0.2 \\\\\n0.5  1  1 \\\\\n1  0.2  1\n\\end{bmatrix}, \\quad \\sigma = 0.1.\n$$\n\nUse threshold $\\tau = 0.9$, the number of samples $N = 5000$, and the covariance regularizer $\\epsilon = 10^{-6}$ as specified above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be a boolean. No additional text should be printed.", "solution": "The user requires the implementation of an evaluation procedure to assess the disentanglement capabilities of a specific linear autoencoder. This autoencoder's encoder matrix $W$ is constructed as a Principal Component Analysis (PCA) whitening transform. The evaluation is performed on synthetically generated data where the ground-truth generative factors are known, allowing for a quantitative measure of disentanglement success.\n\nThe procedure begins by generating a dataset of $N=5000$ ground-truth parameter vectors $p \\in \\mathbb{R}^3$. Each vector $p = (m, b, F)$ has components drawn independently from uniform distributions: $m \\sim \\mathrm{Uniform}([1, 3])$, $b \\sim \\mathrm{Uniform}([0.2, 0.8])$, and $F \\sim \\mathrm{Uniform}([2, 10])$. Because the components of $p$ are independent by construction, their population covariance matrix $\\Sigma_p$ is diagonal.\n\nFor each test case, defined by a mixing matrix $A \\in \\mathbb{R}^{3 \\times 3}$ and a noise level $\\sigma \\ge 0$, a corresponding set of observed data vectors $x \\in \\mathbb{R}^3$ is generated according to the linear model $x = A p + \\varepsilon$, where $\\varepsilon$ is an isotropic Gaussian noise vector with distribution $\\mathcal{N}(0, \\sigma^2 I_3)$. The resulting observed data $x$ represents a linear mixture of the original independent factors, corrupted by noise.\n\nThe core of the method is to a-priori define an encoder $W$ that enforces a key property of disentangled representations: uncorrelated latent components. The objective function $\\mathcal{L}(W, V)$ suggests that in the limit $\\lambda \\to \\infty$, the covariance of the latent representation, $\\mathrm{Cov}(z)$, must be a diagonal matrix. A specific version of this constraint is whitening, which enforces $\\mathrm{Cov}(z) = I_3$. The chosen encoder $W$ aims to achieve this property.\n\nFirst, the sample covariance matrix of the observed data, $\\hat{\\Sigma}_x$, is computed from the mean-centered data $x_c = x - \\mathbb{E}[x]$. For a dataset of $N$ samples represented by a data matrix $X_c$ of shape $(N, 3)$, this is $\\hat{\\Sigma}_x = \\frac{1}{N} X_c^\\top X_c$. Theoretically, the population covariance is $\\Sigma_x = \\mathbb{E}[(Ap_c + \\varepsilon)(Ap_c + \\varepsilon)^\\top] = A \\Sigma_p A^\\top + \\sigma^2 I_3$, where $p_c = p - \\mathbb{E}[p]$.\n\nThe PCA whitening transform $W$ is derived from the spectral decomposition of the regularized sample covariance matrix, $\\hat{\\Sigma}_{x, \\text{reg}} = \\hat{\\Sigma}_x + \\epsilon I_3$, where $\\epsilon = 10^{-6}$ is a small constant for numerical stability. Let the eigendecomposition be $\\hat{\\Sigma}_{x, \\text{reg}} = U \\Lambda U^\\top$, where $U$ is an orthogonal matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of corresponding positive eigenvalues. The whitening matrix is then constructed as $W = \\Lambda^{-1/2} U^\\top$. This choice of $W$ ensures that the resulting latent vectors $z = W x_c$ have a sample covariance that is approximately the identity matrix: $\\mathrm{Cov}(z) = \\frac{1}{N} Z^\\top Z = W (\\frac{1}{N} X_c^\\top X_c) W^\\top = W \\hat{\\Sigma}_x W^\\top \\approx I_3$. The matrix $U^\\top$ rotates the data to align with its principal components, and $\\Lambda^{-1/2}$ scales each component to have unit variance.\n\nThe central hypothesis is that if the principal axes of variation in the observed data $x$ correspond to the axes of the original factors $p$ (up to rotation), then this whitening process will recover $p$ up to permutation, sign, and scaling. The evaluation step tests this hypothesis directly. The latent representation $Z$ is computed for the entire dataset. Then, a $3 \\times 3$ matrix $C$ of absolute Pearson correlations is computed, where each entry $C_{ij} = |\\rho(z_i, p_j)|$ measures the strength of the linear relationship between the $i$-th latent component and the $j$-th ground-truth factor. Both $z$ and $p$ must be mean-centered for this calculation; $z$ is zero-mean by construction, and $p$ is explicitly centered.\n\nIf the latent representation $z$ is successfully disentangled, each latent coordinate $z_i$ should correspond to exactly one ground-truth factor $p_j$. This implies that the correlation matrix $C$ should be close to a permutation matrix (a matrix with one entry of $1$ in each row and column, and zeros elsewhere). To find the best possible one-to-one matching, we solve the assignment problem (or maximum weight bipartite matching) on $C$. This finds the permutation $\\pi^\\star$ of the indices $\\{1, 2, 3\\}$ that maximizes the sum of matched correlations, $\\sum_{i=1}^3 C_{i, \\pi^\\star(i)}$.\n\nFinally, disentanglement is declared a success if and only if every correlation in this optimal assignment meets or exceeds a high threshold, $\\tau = 0.9$. That is, the condition for success is $\\min_{i} C_{i, \\pi^\\star(i)} \\ge \\tau$. This strict criterion ensures that every latent component has a strong, unique correspondence to a ground-truth factor. The procedure is repeated for each test case, and a boolean result is recorded for each.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Implements an evaluator for disentanglement in a linear autoencoder\n    using PCA whitening and correlation-based matching.\n    \"\"\"\n    # Define problem constants\n    N = 5000\n    TAU = 0.9\n    EPSILON = 1e-6\n    RANDOM_SEED = 42\n\n    # Set the random seed for reproducibility\n    np.random.seed(RANDOM_SEED)\n\n    # Define the test suite\n    theta = np.pi / 4\n    # Case 3: Orthogonal rotation matrix\n    R_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta),  np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    # Case 4: General invertible mixing matrix\n    A_entangled = np.array([\n        [1.0, 1.0, 0.2],\n        [0.5, 1.0, 1.0],\n        [1.0, 0.2, 1.0]\n    ])\n    \n    test_cases = [\n        # Case 1: Identity mixing matrix (already disentangled)\n        (np.eye(3), 0.01),\n        # Case 2: Axis-aligned scaling\n        (np.diag([2.0, 0.5, 1.5]), 0.02),\n        # Case 3: Orthogonal rotation\n        (R_matrix, 0.01),\n        # Case 4: General entanglement and higher noise\n        (A_entangled, 0.1)\n    ]\n\n    # 1. Generate ground-truth parameters p\n    p_m = np.random.uniform(low=1.0, high=3.0, size=(N, 1))\n    p_b = np.random.uniform(low=0.2, high=0.8, size=(N, 1))\n    p_F = np.random.uniform(low=2.0, high=10.0, size=(N, 1))\n    P_data = np.hstack((p_m, p_b, p_F))\n\n    # Center p for later correlation calculation. This is P_centered.\n    p_mean = np.mean(P_data, axis=0)\n    P_centered = P_data - p_mean\n\n    results = []\n\n    for A, sigma in test_cases:\n        # 2. Generate observed data x = A*p + noise\n        noise = np.random.normal(loc=0.0, scale=sigma, size=(N, 3))\n        # For data matrices where each row is a sample, x_row^T = A @ p_row^T\n        # This translates to X = P @ A^T\n        X_data = P_data @ A.T + noise\n\n        # 3. Center x and compute regularized sample covariance\n        x_mean = np.mean(X_data, axis=0)\n        X_centered = X_data - x_mean\n        \n        # Sigma_x = (X_centered^T @ X_centered) / N\n        Sigma_x = (X_centered.T @ X_centered) / N\n        Sigma_x_reg = Sigma_x + EPSILON * np.eye(3)\n\n        # 4. Compute PCA whitening transform W\n        # Eigendecomposition of the symmetric covariance matrix\n        eigenvalues, U = np.linalg.eigh(Sigma_x_reg)\n        # Create diagonal matrix of 1/sqrt(eigenvalues)\n        Lambda_inv_sqrt = np.diag(1.0 / np.sqrt(eigenvalues))\n        # Whitening matrix W = (Lambda^-1/2) @ U^T\n        W = Lambda_inv_sqrt @ U.T\n\n        # 5. Compute latent representation z = W*x\n        # Z = X_centered @ W^T\n        Z_data = X_centered @ W.T\n\n        # 6. Compute absolute Pearson correlation matrix C\n        # cov(z, p) = (Z_data^T @ P_centered) / N\n        cov_zp = (Z_data.T @ P_centered) / N\n        \n        # Calculate standard deviations for z and p\n        # std_z should be ~1 due to whitening\n        std_z = np.std(Z_data, axis=0)\n        std_p = np.std(P_centered, axis=0)\n        \n        # Correlation matrix C_ij = cov(z_i,p_j) / (std(z_i)*std(p_j))\n        corr_matrix = cov_zp / np.outer(std_z, std_p)\n        C = np.abs(corr_matrix)\n        \n        # 7. Find optimal one-to-one matching and evaluate success\n        # Use linear_sum_assignment to find the permutation that maximizes the sum of correlations\n        row_ind, col_ind = linear_sum_assignment(C, maximize=True)\n        \n        # Get the correlations from the optimal assignment\n        matched_correlations = C[row_ind, col_ind]\n        \n        # Success is declared if the minimum correlation in the matching is >= TAU\n        min_matched_corr = np.min(matched_correlations)\n        success = min_matched_corr >= TAU\n        results.append(success)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3099320"}]}