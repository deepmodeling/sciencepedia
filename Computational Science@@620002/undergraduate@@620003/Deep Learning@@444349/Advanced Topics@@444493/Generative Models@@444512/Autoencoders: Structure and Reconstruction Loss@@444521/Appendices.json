{"hands_on_practices": [{"introduction": "Choosing the right loss function is a critical design decision that must align with the decoder's output activation and the data's underlying distribution. For binary or probabilistic outputs produced by a sigmoid activation, Binary Cross-Entropy (BCE) is the standard choice. This practice delves into the mathematical reasons for this preference over the more intuitive Mean Squared Error (MSE) [@problem_id:3099815]. By deriving and comparing the gradients of both loss functions, you will uncover how BCE elegantly sidesteps the \"vanishing gradient\" problem that plagues MSE in this context, enabling faster and more reliable training.", "problem": "Consider a single-output autoencoder decoder whose output activation is the sigmoid function $\\sigma(z)$, where $z$ is the scalar pre-activation (logit). The reconstruction target is binary, $x \\in \\{0,1\\}$, and the predicted reconstruction is $y = \\sigma(z)$. Two common reconstruction losses are defined below:\n\n- Binary Cross-Entropy (BCE): $L_{\\mathrm{BCE}}(y;x) = -\\left[x \\ln(y) + (1 - x)\\ln(1 - y)\\right]$.\n- Mean Squared Error (MSE): $L_{\\mathrm{MSE}}(y;x) = \\frac{1}{2}(y - x)^{2}$.\n\nStarting only from the definitions above, the definition of the sigmoid $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$, and basic calculus (the chain rule), do the following:\n\n1. Derive the gradient with respect to the logit $z$ for each loss, $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}$ and $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}$, expressing each as a function of $y$ and $x$.\n2. Specialize to the binary targets $x \\in \\{0,1\\}$ and simplify the gradient magnitudes $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|$ and $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|$ as functions of $y$ alone.\n3. Derive a closed-form expression for the ratio $r(y)$ of the gradient magnitudes,\n   $$r(y) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|},$$\n   for $x \\in \\{0,1\\}$, and simplify it fully as a function of $y$.\n4. Briefly explain, based on the expression for $r(y)$, why Binary Cross-Entropy can induce faster learning than Mean Squared Error when the sigmoid output saturates near $y \\approx 0$ or $y \\approx 1$.\n5. Empirically validate your symbolic result by computing $r(y)$ numerically for the two cases: $x=1$ with $y = 10^{-4}$ and $x=0$ with $y = 1 - 10^{-4}$. Round your numerical values to four significant figures.\n\nYour final answer should be only the simplified closed-form expression for $r(y)$ from part 3.", "solution": "The objective is to analyze the gradients of the Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions with respect to the logit $z$ for a single-output unit with a sigmoid activation function. This analysis will elucidate why BCE is often preferred over MSE for binary reconstruction tasks.\n\nThe core of the derivation relies on the chain rule for differentiation. Given a loss function $L$ that depends on the model output $y$, which in turn is a function of the logit $z$ (i.e., $y = \\sigma(z)$), the gradient of the loss with respect to the logit is:\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}\n$$\nFirst, we derive the common term $\\frac{\\partial y}{\\partial z}$, which is the derivative of the sigmoid function $\\sigma(z)$.\nGiven $y = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$, its derivative with respect to $z$ is:\n$$\n\\frac{\\partial y}{\\partial z} = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -1 \\cdot (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}\n$$\nThis expression can be simplified by expressing it in terms of $y = \\sigma(z)$:\n$$\n\\frac{\\partial y}{\\partial z} = \\left(\\frac{1}{1 + \\exp(-z)}\\right) \\left(\\frac{\\exp(-z)}{1 + \\exp(-z)}\\right) = \\left(\\frac{1}{1 + \\exp(-z)}\\right) \\left(\\frac{1 + \\exp(-z) - 1}{1 + \\exp(-z)}\\right) = y (1 - y)\n$$\nThis is a standard and useful identity for the sigmoid function.\n\n**1. Gradient Derivations**\n\nWe now compute the gradient for each loss function.\n\n**Binary Cross-Entropy (BCE) Gradient:**\nThe BCE loss is defined as $L_{\\mathrm{BCE}}(y;x) = -\\left[x \\ln(y) + (1 - x)\\ln(1 - y)\\right]$.\nFirst, we find the partial derivative with respect to $y$:\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial y} = -\\left[ \\frac{x}{y} - \\frac{1 - x}{1 - y} \\right] = -\\frac{x(1 - y) - (1 - x)y}{y(1 - y)} = -\\frac{x - xy - y + xy}{y(1 - y)} = -\\frac{x - y}{y(1 - y)} = \\frac{y - x}{y(1 - y)}\n$$\nUsing the chain rule, we find the gradient with respect to $z$:\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{BCE}}}{\\partial y} \\frac{\\partial y}{\\partial z} = \\left( \\frac{y - x}{y(1 - y)} \\right) \\cdot (y(1 - y)) = y - x\n$$\n\n**Mean Squared Error (MSE) Gradient:**\nThe MSE loss is defined as $L_{\\mathrm{MSE}}(y;x) = \\frac{1}{2}(y - x)^{2}$.\nFirst, we find the partial derivative with respect to $y$:\n$$\n\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial y} = \\frac{1}{2} \\cdot 2(y - x) \\cdot 1 = y - x\n$$\nUsing the chain rule, we find the gradient with respect to $z$:\n$$\n\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{MSE}}}{\\partial y} \\frac{\\partial y}{\\partial z} = (y - x) \\cdot (y(1 - y))\n$$\n\n**2. Gradient Magnitudes for Binary Targets**\n\nWe now specialize these results for the binary target values $x \\in \\{0, 1\\}$. The output of the sigmoid function is strictly $y \\in (0, 1)$.\n\n**BCE Gradient Magnitude:**\nThe gradient is $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y - x$.\n- If $x = 0$: $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y$. Since $y > 0$, the magnitude is $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right| = y$.\n- If $x = 1$: $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y - 1$. Since $y < 1$, the term $y-1$ is negative. The magnitude is $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right| = -(y - 1) = 1 - y$.\n\n**MSE Gradient Magnitude:**\nThe gradient is $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y - x)y(1 - y)$. The term $y(1-y)$ is always positive for $y \\in (0, 1)$.\n- If $x = 0$: $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = y \\cdot y(1 - y) = y^2(1 - y)$. This term is positive. The magnitude is $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right| = y^2(1 - y)$.\n- If $x = 1$: $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y - 1)y(1 - y) = -y(1 - y)^2$. This term is negative. The magnitude is $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right| = -(-y(1 - y)^2) = y(1 - y)^2$.\n\n**3. Ratio of Gradient Magnitudes**\n\nWe compute the ratio $r(y) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|}$ for both cases of the binary target $x$.\n\n- If $x = 0$:\n$$\nr(y) = \\frac{y}{y^2(1 - y)} = \\frac{1}{y(1 - y)}\n$$\n- If $x = 1$:\n$$\nr(y) = \\frac{1 - y}{y(1 - y)^2} = \\frac{1}{y(1 - y)}\n$$\nIn both cases, the expression for the ratio is identical. The simplified closed-form expression is:\n$$\nr(y) = \\frac{1}{y(1 - y)}\n$$\n\n**4. Explanation of Learning Speed**\n\nThe gradient $\\frac{\\partial L}{\\partial z}$ is the error signal that is backpropagated to update the weights of the layer preceding the output unit. A larger gradient magnitude generally leads to a larger weight update and thus faster learning, especially when the prediction is incorrect.\n\nThe MSE gradient, $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y-x)y(1-y)$, contains the factor $y(1-y) = \\sigma'(z)$. This term $\\sigma'(z)$ approaches $0$ as the sigmoid output $y$ saturates, i.e., as $y \\to 0$ or $y \\to 1$. If the model makes a confident but incorrect prediction (e.g., $y \\approx 0$ when the target $x=1$), the $y(1-y)$ term becomes very small, causing the entire gradient to \"vanish.\" This leads to extremely slow learning, as the model receives almost no signal to correct its large error.\n\nIn contrast, the BCE gradient, $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y-x$, does not have this problematic $y(1-y)$ term. The use of the logarithmic loss function precisely cancels out the $\\sigma'(z)$ factor from the chain rule. Consequently, if the model makes a confident but incorrect prediction, the gradient's magnitude remains large. For instance, if $x=1$ and $y \\approx 0$, the gradient $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} \\approx 0-1 = -1$, a strong corrective signal.\n\nThe ratio $r(y) = \\frac{1}{y(1-y)}$ formally captures this behavior. As the output $y$ saturates near $0$ or $1$, the denominator $y(1-y)$ approaches $0$, causing the ratio $r(y)$ to approach infinity. This shows that the BCE gradient magnitude becomes unboundedly larger than the MSE gradient magnitude in these saturation regimes, thereby inducing much faster learning and helping the model escape from poor local minima.\n\n**5. Numerical Validation**\n\nWe compute $r(y) = \\frac{1}{y(1 - y)}$ for the given cases.\n\n- Case 1: $x=1$ with $y = 10^{-4}$.\n  Here, $y = 0.0001$. The prediction is highly confident and incorrect.\n  $$\n  r(10^{-4}) = \\frac{1}{0.0001 \\times (1 - 0.0001)} = \\frac{1}{0.0001 \\times 0.9999} = \\frac{1}{0.00009999} \\approx 10001.0001\n  $$\n  Rounded to four significant figures, this is $1.000 \\times 10^4$.\n\n- Case 2: $x=0$ with $y = 1 - 10^{-4}$.\n  Here, $y = 0.9999$. The prediction is again highly confident and incorrect.\n  $$\n  r(1 - 10^{-4}) = \\frac{1}{0.9999 \\times (1 - 0.9999)} = \\frac{1}{0.9999 \\times 0.0001} = \\frac{1}{0.00009999} \\approx 10001.0001\n  $$\n  Rounded to four significant figures, this is $1.000 \\times 10^4$.\n\nThe numerical results confirm that for confident, incorrect predictions where the sigmoid output is saturated, the BCE gradient is approximately $10000$ times larger than the MSE gradient, validating the theoretical analysis.", "answer": "$$\n\\boxed{\\frac{1}{y(1 - y)}}\n$$", "id": "3099815"}, {"introduction": "An autoencoder's reconstruction fidelity depends on a delicate interplay between its architecture and the chosen loss function. The structure of the encoder and decoder itself imposes a strong prior on what kind of information can be preserved. This exercise focuses on a fundamental structural property of convolutional autoencoders: the receptive field [@problem_id:3099855]. By implementing a simple 1D convolutional autoencoder and varying its kernel size, you will directly observe how the receptive field size impacts the reconstruction of signals with different frequency characteristics, providing a tangible understanding of the trade-off between smoothing and preserving fine details.", "problem": "You are asked to implement and analyze a simplified one-dimensional convolutional autoencoder to study how receptive field size affects fine-detail reconstruction quality under two different reconstruction losses: mean squared error and mean absolute error. Your program must be a complete, runnable script that computes the specified quantities and prints the final results in the required format.\n\nStart from the following fundamental base:\n\n- A one-dimensional autoencoder maps an input signal $x \\in \\mathbb{R}^N$ to a latent representation via an encoder and reconstructs it via a decoder. In this problem, both encoder and decoder are linear, shift-invariant operators implemented as discrete cross-correlations with finite impulse response kernels.\n- The encoder computes a cross-correlation $y = x \\star w$, where, for an odd-length kernel $w \\in \\mathbb{R}^K$ with $K \\ge 1$, the discrete cross-correlation with zero-padding is defined as\n$$\ny[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor,\n$$\nwith $x[m] = 0$ for indices $m$ outside $[0, N-1]$. This produces an output $y$ of the same length $N$ as $x$.\n- The decoder computes another cross-correlation with the reversed kernel, $\\tilde{w}[j] = w[K-1-j]$, yielding the reconstruction $\\hat{x} = y \\star \\tilde{w}$. This corresponds to a tied-weights linear convolutional autoencoder with stride $1$ and zero-padding, so that the overall mapping is a linear, time-invariant smoothing operator.\n- Reconstruction losses are defined per-sample and averaged along the signal:\n  - Mean squared error (MSE): $L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left( \\hat{x}[n] - x[n] \\right)^2$.\n  - Mean absolute error (L1): $L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left| \\hat{x}[n] - x[n] \\right|$.\n- The receptive field is the kernel length $K$, and the kernel entries are taken to be a normalized box filter: $w[j] = \\frac{1}{K}$ for $j \\in \\{0,1,\\dots,K-1\\}$.\n\nYour task is to quantify how $K$ affects fine-detail reconstruction under both $L_2$ and $L_1$ losses using the specified architecture (no training is performed; the weights are fixed by definition). To isolate the effect of receptive field on fine-detail content, you will evaluate three synthetic inputs of length $N$:\n\n- $x^{(1)}$: a unit impulse at the center, $x^{(1)}[n] = 1$ if $n = \\frac{N}{2}$ and $x^{(1)}[n] = 0$ otherwise.\n- $x^{(2)}$: a high-frequency alternating cosine, $x^{(2)}[n] = \\cos(\\pi n)$.\n- $x^{(3)}$: a low-frequency cosine, $x^{(3)}[n] = \\cos\\!\\left( \\frac{2\\pi f_0 n}{N} \\right)$.\n\nUse $N = 128$ and $f_0 = 4$.\n\nFor each kernel size $K$ in the test suite below, compute the following:\n\n1. For each signal $x^{(i)}$ with $i \\in \\{1,2,3\\}$, form $\\hat{x}^{(i)}$ using the encoder-decoder defined above.\n2. Compute $L_2\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$ and $L_1\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$.\n3. Average the losses over the three signals to obtain\n$$\n\\overline{L}_2(K) = \\frac{1}{3} \\sum_{i=1}^3 L_2\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right), \\quad\n\\overline{L}_1(K) = \\frac{1}{3} \\sum_{i=1}^3 L_1\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right).\n$$\n4. Quantify fine-detail retention using the impulse signal: compute the ratio\n$$\nr(K) = \\frac{\\hat{x}^{(1)}\\!\\left[\\frac{N}{2}\\right]}{x^{(1)}\\!\\left[\\frac{N}{2}\\right]}.\n$$\nSince $x^{(1)}\\!\\left[\\frac{N}{2}\\right] = 1$, this simplifies to $r(K) = \\hat{x}^{(1)}\\!\\left[\\frac{N}{2}\\right]$.\n\nTest suite:\n\n- Use kernel sizes $K \\in \\{1, 3, 7, 15\\}$.\n\nAnswer specification:\n\n- For each $K$ in ascending order, produce a result list $[K, \\overline{L}_2(K), \\overline{L}_1(K), r(K)]$.\n- Your program should produce a single line of output containing the results as a comma-separated list of these lists, enclosed in square brackets. The values $\\overline{L}_2(K)$, $\\overline{L}_1(K)$, and $r(K)$ must be rounded to six decimal places. For example, a valid output format is\n`[[1,0.000000,0.000000,1.000000],[3, ...],[7, ...],[15, ...]]`\n(with no spaces in the printed line).", "solution": "The problem requires an analysis of a simplified one-dimensional linear convolutional autoencoder. The core of the task is to implement the specified signal processing chain and quantify how the receptive field size, determined by the kernel length $K$, affects the reconstruction of signals with different frequency characteristics. The autoencoder's weights are fixed as a normalized box filter, meaning no training is involved. The analysis is purely based on the signal processing properties of the defined architecture.\n\nThe input is a one-dimensional signal $x \\in \\mathbb{R}^N$. The encoder maps the input $x$ to a latent representation $y$ via a discrete cross-correlation with a kernel $w \\in \\mathbb{R}^K$: $y = x \\star w$. The decoder reconstructs the signal, $\\hat{x}$, by applying another cross-correlation to $y$ with the reversed kernel $\\tilde{w}$: $\\hat{x} = y \\star \\tilde{w}$. The kernel size $K$ must be an odd integer. The cross-correlation is defined with zero-padding to maintain the signal length $N$:\n$$y[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor$$\nThe kernel weights are given by a normalized box filter, $w[j] = 1/K$ for all $j \\in \\{0, \\dots, K-1\\}$. A key property of this kernel is its symmetry, meaning $w[j] = w[K-1-j]$. Consequently, the reversed kernel $\\tilde{w}$ is identical to the original kernel $w$. The overall transformation from input $x$ to reconstruction $\\hat{x}$ is therefore a cascade of two identical cross-correlation operations:\n$$\\hat{x} = (x \\star w) \\star w$$\nThis operation is equivalent to a single convolution of the input signal $x$ with an effective kernel that is the self-convolution of $w$. This transformation constitutes a linear, time-invariant (LTI) system.\n\nThe kernel $w$ acts as a moving average filter, which is a fundamental form of low-pass filter. Applying this filter twice in succession results in a more potent low-pass filtering effect. The effective impulse response of the complete autoencoder system is a triangular filter of length $2K-1$. The parameter $K$, representing the receptive field size, controls the width of this filter. A larger $K$ leads to a wider effective filter, which causes more aggressive smoothing of the signal. This increased smoothing attenuates high-frequency components and fine details more severely. The problem uses three specific signals to probe this behavior:\n1.  $x^{(1)}$: A unit impulse. The reconstruction $\\hat{x}^{(1)}$ is the impulse response of the system, directly revealing the shape of the effective filter.\n2.  $x^{(2)}$: A high-frequency cosine, $\\cos(\\pi n)$. This signal will be strongly attenuated by the low-pass filter, especially for large $K$.\n3.  $x^{(3)}$: A low-frequency cosine. This signal is expected to be better preserved than $x^{(2)}$, serving as a reference.\n\nThe computational procedure is as follows. The analysis is conducted for kernel sizes $K \\in \\{1, 3, 7, 15\\}$. For each value of $K$, the following steps are executed:\n1.  The three input signals, $x^{(1)}$, $x^{(2)}$, and $x^{(3)}$, are generated for a length of $N = 128$ and frequency parameter $f_0 = 4$.\n    -   $x^{(1)}[n] = \\delta[n - N/2]$, with $N/2 = 64$.\n    -   $x^{(2)}[n] = \\cos(\\pi n)$.\n    -   $x^{(3)}[n] = \\cos(2\\pi f_0 n / N)$.\n2.  The kernel $w$ of size $K$ is created with entries $w[j] = 1/K$.\n3.  For each signal $x^{(i)}$, the reconstruction $\\hat{x}^{(i)}$ is computed by applying the specified cross-correlation operation twice.\n4.  The reconstruction quality is measured using two loss functions, the Mean Squared Error ($L_2$) and the Mean Absolute Error ($L_1$), defined as:\n    $$L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{x}[n] - x[n])^2$$\n    $$L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} |\\hat{x}[n] - x[n]|$$\n5.  These individual losses are averaged over the three signals to produce the final metrics $\\overline{L}_2(K)$ and $\\overline{L}_1(K)$.\n6.  The retention of fine detail is quantified by the ratio $r(K) = \\hat{x}^{(1)}[N/2] / x^{(1)}[N/2]$. Since $x^{(1)}[N/2] = 1$, this simplifies to $r(K) = \\hat{x}^{(1)}[N/2]$. An analytical derivation based on the system's impulse response confirms that this value is exactly $1/K$, which serves as a valuable check for the implementation.\nThe final output aggregates these computed values for each $K$ from the test suite.", "answer": "```python\nimport numpy as np\n\ndef cross_correlate(x, w):\n    \"\"\"\n    Computes the 1D cross-correlation with zero-padding as defined in the problem.\n    y[n] = sum_{j=0}^{K-1} w[j] * x[n + j - c], where c = floor(K/2).\n    \"\"\"\n    N = len(x)\n    K = len(w)\n    # For odd K, floor(K/2) is equivalent to (K-1)//2.\n    c = (K - 1) // 2\n    y = np.zeros(N, dtype=np.float64)\n    \n    for n in range(N):\n        sum_val = 0.0\n        for j in range(K):\n            m = n + j - c\n            if 0 <= m < N:\n                sum_val += w[j] * x[m]\n        y[n] = sum_val\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as specified.\n    \"\"\"\n    # Define problem parameters\n    N = 128\n    f0 = 4\n    kernel_sizes = [1, 3, 7, 15]\n\n    # Generate input signals\n    n_indices = np.arange(N)\n    \n    # x^(1): Unit impulse at the center\n    x1 = np.zeros(N, dtype=np.float64)\n    center_idx = N // 2\n    x1[center_idx] = 1.0\n\n    # x^(2): High-frequency alternating cosine\n    x2 = np.cos(np.pi * n_indices)\n\n    # x^(3): Low-frequency cosine\n    x3 = np.cos(2 * np.pi * f0 * n_indices / N)\n    \n    signals = [x1, x2, x3]\n    \n    all_results = []\n\n    for K in kernel_sizes:\n        # Create the normalized box filter kernel\n        w = np.full(K, 1.0 / K, dtype=np.float64)\n        \n        # The reversed kernel w_tilde is the same as w because w is symmetric\n        w_tilde = w\n        \n        total_l2_loss = 0.0\n        total_l1_loss = 0.0\n        r_K = 0.0\n        \n        # Process each signal\n        for i, x in enumerate(signals):\n            # Form reconstruction x_hat by applying the operation twice\n            # Encoder pass\n            y = cross_correlate(x, w)\n            # Decoder pass\n            x_hat = cross_correlate(y, w_tilde)\n            \n            # Compute losses\n            l2_loss = np.mean((x_hat - x)**2)\n            l1_loss = np.mean(np.abs(x_hat - x))\n            \n            # Accumulate losses for averaging\n            total_l2_loss += l2_loss\n            total_l1_loss += l1_loss\n            \n            # For signal x1, compute the retention ratio r(K)\n            if i == 0:  # Signal x1 is at index 0\n                # r(K) = x_hat[N/2] / x[N/2]. Since x[N/2]=1, it's just x_hat[N/2]\n                r_K = x_hat[center_idx]\n\n        # Average the losses over the three signals\n        avg_l2 = total_l2_loss / 3.0\n        avg_l1 = total_l1_loss / 3.0\n        \n        # Store raw float values for this K\n        result_tuple = [K, avg_l2, avg_l1, r_K]\n        all_results.append(result_tuple)\n        \n    # Format the final output string as required\n    # The f-string formatting with ':.6f' handles rounding to 6 decimal places\n    formatted_results = [f\"[{k},{l2:.6f},{l1:.6f},{r:.6f}]\" for k, l2, l1, r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n```", "id": "3099855"}, {"introduction": "While generic loss functions like Mean Squared Error (MSE) are easy to implement, they often fail to align with human perception of quality, particularly for complex data like images. A small MSE might not correspond to a visually pleasing result. This advanced practice guides you to engineer a loss function tailored to the domain of human vision [@problem_id:3099813]. You will design and implement a perceptually-aware loss that transforms color information into an opponent-color space, allowing you to more heavily penalize unrealistic color shifts over simple changes in luminance. This exercise demonstrates how incorporating domain knowledge into the loss function can lead to models that produce far more meaningful and high-quality outputs.", "problem": "Consider an autoencoder in deep learning that maps an input image tensor to a reconstruction through an encoder and decoder composition. Let the input image be denoted by $X \\in \\mathbb{R}^{H \\times W \\times 3}$ and the reconstructed image be denoted by $\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}$, where the last dimension corresponds to the red, green, and blue channels. The standard empirical risk minimization objective uses a reconstruction loss between $X$ and $\\hat{X}$ that aggregates pixelwise errors. The goal in this problem is to design a reconstruction loss that penalizes perceptual color shifts more than luminance errors, by measuring errors in an opponent-color space and then applying channel-wise weighting before averaging.\n\nUse a linear opponent-color transform defined per pixel by a matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that maps $\\begin{bmatrix}R & G & B\\end{bmatrix}^\\top$ to $\\begin{bmatrix}L & RG & BY\\end{bmatrix}^\\top$ through\n$$\nM \\;=\\;\n\\begin{bmatrix}\n0.2126 & 0.7152 & 0.0722 \\\\\n1 & -1 & 0 \\\\\n0.5 & 0.5 & -1\n\\end{bmatrix},\n$$\nwhere $L$ denotes a luminance channel, $RG$ denotes a red-green opponent channel, and $BY$ denotes a blue-yellow opponent channel. You must construct the reconstruction loss by: (i) computing the per-pixel opponent-color difference induced by $M$ between $\\hat{X}$ and $X$, (ii) applying channel-wise nonnegative weights $\\alpha_L$, $\\alpha_{RG}$, and $\\alpha_{BY}$ to these differences, and (iii) aggregating the result by averaging over all pixels using the squared Euclidean norm. The entire construction must be expressed and implemented only from the foundational definitions of linear transforms, Euclidean norms, and mean aggregation; do not introduce any additional heuristic terms.\n\nYour program must implement this loss and evaluate it on the following test suite. All image values are intended to be dimensionless intensities in the closed interval $[0,1]$:\n\n- Test case $1$ (general case with mixed luminance and color errors): Let $H = 2$ and $W = 2$. Define\n$$\nX_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} &\n\\begin{bmatrix} 0.20 & 0.60 & 0.20 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.90 & 0.10 & 0.10 \\end{bmatrix} &\n\\begin{bmatrix} 0.10 & 0.90 & 0.90 \\end{bmatrix}\n\\end{bmatrix},\n\\quad\n\\hat{X}_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.51 & 0.50 & 0.49 \\end{bmatrix} &\n\\begin{bmatrix} 0.18 & 0.62 & 0.20 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.90 & 0.12 & 0.08 \\end{bmatrix} &\n\\begin{bmatrix} 0.13 & 0.89 & 0.88 \\end{bmatrix}\n\\end{bmatrix}.\n$$\nUse weights $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\n- Test case $2$ (boundary case of perfect reconstruction): Let $H = 2$ and $W = 2$. Define\n$$\nX_2 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.15 & 0.25 & 0.35 \\end{bmatrix} &\n\\begin{bmatrix} 0.45 & 0.55 & 0.65 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.40 & 0.30 & 0.20 \\end{bmatrix} &\n\\begin{bmatrix} 0.60 & 0.70 & 0.80 \\end{bmatrix}\n\\end{bmatrix},\n\\quad\n\\hat{X}_2 = X_2.\n$$\nUse weights $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\n- Test case $3$ (edge case of pure color shift with no luminance change per pixel): Let $H = 2$ and $W = 2$. Define\n$$\nX_3 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} &\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} &\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix}\n\\end{bmatrix},\n$$\nand construct $\\hat{X}_3$ by setting, for each pixel, a channelwise increment $\\Delta R = 0.05$, $\\Delta G = -\\left(\\frac{0.2126}{0.7152}\\right)\\Delta R$, and $\\Delta B = 0.0$, then defining $\\hat{X}_3 = X_3 + \\begin{bmatrix} \\Delta R & \\Delta G & \\Delta B \\end{bmatrix}$ per pixel. Use weights $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\nYour program should produce a single line of output containing the three loss values for test cases $1$, $2$, and $3$, respectively, as a comma-separated list enclosed in square brackets. Each number must be a floating-point value rounded to six decimal places, for example, `[0.123456,0.000000,0.987654]`.", "solution": "The problem statement is valid as it is scientifically grounded in color theory and deep learning practices, well-posed with all necessary information provided, and objective in its formulation. We will proceed to derive the specified reconstruction loss and apply it to the given test cases.\n\nThe objective is to construct a reconstruction loss function $\\mathcal{L}$ that operates on an input image tensor $X \\in \\mathbb{R}^{H \\times W \\times 3}$ and its reconstruction $\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}$. This loss function should be more sensitive to perceptual color shifts than to changes in luminance. This is achieved by transforming pixel-wise errors into an opponent-color space, applying channel-specific weights, and then aggregating the results. The derivation follows a principled, step-by-step construction based on foundational mathematical operations.\n\nLet $X_{ij} \\in \\mathbb{R}^3$ and $\\hat{X}_{ij} \\in \\mathbb{R}^3$ represent the Red-Green-Blue (RGB) color vectors for the pixel at spatial location $(i,j)$, where $i \\in \\{1, \\dots, H\\}$ and $j \\in \\{1, \\dots, W\\}$.\n\n**1. Per-Pixel Error in RGB Space**\nThe initial step is to compute the difference between the reconstructed and original pixel values in the standard RGB color space. This per-pixel difference vector is denoted by $\\Delta_{ij}$:\n$$\n\\Delta_{ij} = \\hat{X}_{ij} - X_{ij} \\in \\mathbb{R}^3\n$$\n\n**2. Transformation to Opponent-Color Space**\nThe core of the perceptual weighting strategy is to analyze this error in an opponent-color space. The problem provides a linear transformation matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that maps an RGB vector to a vector containing luminance ($L$), red-green ($RG$), and blue-yellow ($BY$) components.\n$$\nM =\n\\begin{bmatrix}\n0.2126 & 0.7152 & 0.0722 \\\\\n1 & -1 & 0 \\\\\n0.5 & 0.5 & -1\n\\end{bmatrix}\n$$\nApplying this linear transformation to the RGB error vector $\\Delta_{ij}$ yields the error vector in the opponent-color space, $o_{ij} \\in \\mathbb{R}^3$:\n$$\no_{ij} = M \\Delta_{ij} = \\begin{bmatrix} o_{L,ij} \\\\ o_{RG,ij} \\\\ o_{BY,ij} \\end{bmatrix}\n$$\nHere, $o_{L,ij}$, $o_{RG,ij}$, and $o_{BY,ij}$ represent the error components along the luminance, red-green, and blue-yellow axes, respectively.\n\n**3. Weighted Per-Pixel Error**\nTo penalize color errors more heavily than luminance errors, non-negative weights $\\alpha_L$, $\\alpha_{RG}$, and $\\alpha_{BY}$ are applied to the squared components of the opponent-color error vector $o_{ij}$. The per-pixel loss, $L_{ij}$, is the weighted sum of these squared errors, which is equivalent to a weighted squared Euclidean norm of $o_{ij}$.\n$$\nL_{ij} = \\alpha_L (o_{L,ij})^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2\n$$\n\n**4. Aggregation into Final Loss**\nThe total reconstruction loss for the entire image, $\\mathcal{L}(X, \\hat{X})$, is the mean of the per-pixel losses $L_{ij}$ over all $N = H \\times W$ pixels.\n$$\n\\mathcal{L}(X, \\hat{X}) = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} L_{ij} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\alpha_L (o_{L,ij})^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2 \\right)\n$$\nThis completes the formal definition of the reconstruction loss. We now evaluate this loss for the three specified test cases. In all cases, the weights are $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\n**Test Case 1: General Case**\nWe are given $X_1$ and $\\hat{X}_1$. The per-pixel RGB difference tensor is:\n$$\n\\Delta_1 = \\hat{X}_1 - X_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.01 & 0.00 & -0.01 \\end{bmatrix} &\n\\begin{bmatrix} -0.02 & 0.02 & 0.00 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.00 & 0.02 & -0.02 \\end{bmatrix} &\n\\begin{bmatrix} 0.03 & -0.01 & -0.01 \\end{bmatrix}\n\\end{bmatrix}\n$$\nWe apply the transformation $o_{ij} = M \\Delta_{ij}$ and compute the weighted squared error for each of the $4$ pixels. The sum of these errors is calculated and then divided by $4$. The computation yields a total loss $\\mathcal{L}_1 \\approx 0.003986$.\n\n**Test Case 2: Perfect Reconstruction**\nHere, $\\hat{X}_2 = X_2$. This implies that the difference tensor $\\Delta_2 = \\hat{X}_2 - X_2$ is a zero tensor. Consequently, the opponent-color error vectors $o_{ij}$ are all zero vectors, and the per-pixel losses $L_{ij}$ are all $0$. The total loss is therefore:\n$$\n\\mathcal{L}_2 = 0.0\n$$\n\n**Test Case 3: Pure Color Shift**\nThe image $X_3$ is a uniform gray. $\\hat{X}_3$ is constructed by adding a constant vector $[\\Delta R, \\Delta G, \\Delta B]^\\top$ to each pixel of $X_3$, where $\\Delta R = 0.05$, $\\Delta G = -\\left(\\frac{0.2126}{0.7152}\\right)\\Delta R$, and $\\Delta B = 0.0$. The per-pixel RGB difference is constant across the image:\n$$\n\\Delta_{ij} = \\left[ 0.05, -0.05 \\left(\\frac{0.2126}{0.7152}\\right), 0 \\right]^\\top \\approx \\left[ 0.05, -0.014863, 0 \\right]^\\top\n$$\nThe transformation into the opponent space gives $o_{ij} = M \\Delta_{ij}$. By construction, the luminance component of this error is zero:\n$$\no_{L,ij} = 0.2126(\\Delta R) + 0.7152(\\Delta G) = 0.2126(\\Delta R) + 0.7152\\left(-\\frac{0.2126}{0.7152}\\Delta R\\right) = 0\n$$\nThe color components are non-zero. Since the per-pixel error is the same for all pixels, the total loss $\\mathcal{L}_3$ is equal to the error for a single pixel:\n$$\n\\mathcal{L}_3 = L_{ij} = \\alpha_L (0)^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2\n$$\nThe calculation gives $o_{RG,ij} \\approx 0.064863$ and $o_{BY,ij} \\approx 0.017568$. The resulting loss is $\\mathcal{L}_3 \\approx 0.013548$.\nThe final results are $[0.003986, 0.000000, 0.013548]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_loss(X, X_hat, M, alpha):\n    \"\"\"\n    Calculates the perceptually-weighted reconstruction loss.\n\n    Args:\n        X (np.ndarray): The original image tensor of shape (H, W, 3).\n        X_hat (np.ndarray): The reconstructed image tensor of shape (H, W, 3).\n        M (np.ndarray): The opponent-color transformation matrix of shape (3, 3).\n        alpha (np.ndarray): The channel-wise weights of shape (3,).\n\n    Returns:\n        float: The final aggregated loss value.\n    \"\"\"\n    # Step 1: Compute the difference tensor in RGB space.\n    # The shape of delta is (H, W, 3).\n    delta = X_hat - X\n\n    # Step 2: Transform the RGB differences to the opponent-color space.\n    # We perform a matrix multiplication for each pixel's 3-element vector.\n    # In numpy, (H, W, 3) @ (3, 3) correctly performs this operation,\n    # resulting in a tensor of shape (H, W, 3).\n    # M must be transposed to align dimensions for the matmul.\n    opp_diff = delta @ M.T\n\n    # Step 3: Apply channel-wise weights and compute the squared error.\n    # Square the opponent differences element-wise.\n    opp_diff_sq = opp_diff**2\n    # Apply weights. The alpha array of shape (3,) is broadcast\n    # across the (H, W) dimensions.\n    weighted_opp_diff_sq = alpha * opp_diff_sq\n    # Sum the weighted squared errors across the color channels (axis=2)\n    # to get the per-pixel loss.\n    pixel_errors = np.sum(weighted_opp_diff_sq, axis=2)\n\n    # Step 4: Aggregate by averaging over all pixels.\n    # np.mean computes the average of all elements in the pixel_errors tensor.\n    loss = np.mean(pixel_errors)\n\n    return loss\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute their losses.\n    \"\"\"\n    # Define the opponent-color transform matrix and weights\n    M = np.array([\n        [0.2126, 0.7152, 0.0722],\n        [1.0, -1.0, 0.0],\n        [0.5, 0.5, -1.0]\n    ])\n    alpha = np.array([1.0, 3.0, 3.0])\n\n    # Test Case 1\n    X1 = np.array([\n        [[0.50, 0.50, 0.50], [0.20, 0.60, 0.20]],\n        [[0.90, 0.10, 0.10], [0.10, 0.90, 0.90]]\n    ])\n    X1_hat = np.array([\n        [[0.51, 0.50, 0.49], [0.18, 0.62, 0.20]],\n        [[0.90, 0.12, 0.08], [0.13, 0.89, 0.88]]\n    ])\n    loss1 = calculate_loss(X1, X1_hat, M, alpha)\n\n    # Test Case 2\n    X2 = np.array([\n        [[0.15, 0.25, 0.35], [0.45, 0.55, 0.65]],\n        [[0.40, 0.30, 0.20], [0.60, 0.70, 0.80]]\n    ])\n    X2_hat = X2.copy()  # Perfect reconstruction\n    loss2 = calculate_loss(X2, X2_hat, M, alpha)\n\n    # Test Case 3\n    X3 = np.full((2, 2, 3), 0.50)\n    delta_R = 0.05\n    delta_G = -(0.2126 / 0.7152) * delta_R\n    delta_B = 0.0\n    pixel_delta = np.array([delta_R, delta_G, delta_B])\n    X3_hat = X3 + pixel_delta\n    loss3 = calculate_loss(X3, X3_hat, M, alpha)\n\n    results = [loss1, loss2, loss3]\n    \n    # Format the output string as required\n    results_str = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3099813"}]}