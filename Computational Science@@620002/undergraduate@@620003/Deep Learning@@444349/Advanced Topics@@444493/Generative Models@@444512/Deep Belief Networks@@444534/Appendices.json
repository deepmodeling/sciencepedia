{"hands_on_practices": [{"introduction": "Before building any complex system, it's essential to understand its resource requirements. This exercise challenges you to dissect a Deep Belief Network's architecture to calculate its total number of parameters and the computational cost of pretraining it [@problem_id:3112338]. By analyzing how these factors scale with network size and depth, you'll gain crucial insights into the practical constraints of designing deep networks and the benefits of techniques like sparse connectivity.", "problem": "Consider a Deep Belief Network (DBN) formed by stacking $L$ Restricted Boltzmann Machines (RBMs), where the visible layer has size $n_{v}$ and each hidden layer has size $n_{h}$. Each RBM is a bipartite graphical model with energy function $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$, where $v \\in \\{0,1\\}^{n_{v}}$ or $\\{0,1\\}^{n_{h}}$ depending on layer, $h \\in \\{0,1\\}^{n_{h}}$, $W$ is the weight matrix connecting the two layers, and $b$ and $c$ are the visible and hidden biases, respectively. Pretraining is performed layer-wise using Contrastive Divergence (CD), specifically $k$ steps of Contrastive Divergence (CD-$k$) per data case per layer. Assume a dataset of size $N$ examples and one parameter update per layer per epoch.\n\nStarting from these definitions and the bipartite structure of RBMs, and using only well-tested facts about matrix-vector multiplication costs, perform the following:\n\n1. Derive the total number of trainable parameters for the dense DBN, denoted $P_{\\text{dense}}$, counting all weights and biases across all $L$ RBMs.\n2. Derive a leading-order expression for the floating-point operation count per epoch for CD-$k$ pretraining of the dense DBN, denoted $C_{\\text{dense}}$, under the following scientifically reasonable assumptions:\n   - The dominant computation per CD step per data case per RBM consists of two matrix-vector multiplications (the up-pass $W^{\\top}v$ and the down-pass $W h$), and the cost scales linearly with the number of nonzero entries in $W$.\n   - Non-matrix operations (additions, activation evaluations) contribute a constant multiplicative factor that can be absorbed into an abstract constant.\n   Introduce a positive constant $\\alpha$ that captures the per-edge computational cost per CD step per data case, and express $C_{\\text{dense}}$ in terms of $n_{v}$, $n_{h}$, $L$, $N$, $k$, and $\\alpha$.\n3. Propose a sparse connectivity scheme in which, in every RBM, each potential visible-hidden edge is independently present with probability $p \\in (0,1)$. Under this model, derive the expected total number of trainable parameters $P_{\\text{sparse}}$ and the expected per-epoch floating-point operation count $C_{\\text{sparse}}$.\n4. Finally, compute the closed-form expression for the ratio $R=C_{\\text{sparse}}/C_{\\text{dense}}$.\n\nYour final answer must be the single symbolic expression for $R$. No numerical approximation or rounding is required.", "solution": "We begin with structural definitions for Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs). An RBM is bipartite: all visible units connect to all hidden units, and there are no lateral connections within a layer. The energy function $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$ implies a parameter set consisting of the weight matrix $W$ and the bias vectors $b$ and $c$. The size of $W$ is determined by the sizes of the connected layers.\n\nA DBN pretraining procedure stacks $L$ RBMs: the first RBM connects the visible layer of size $n_{v}$ to the first hidden layer of size $n_{h}$, and each subsequent RBM connects a hidden layer of size $n_{h}$ to the next hidden layer of size $n_{h}$. Thus, there are $L$ RBMs total: one with dimensions $n_{v} \\times n_{h}$, and $L-1$ with dimensions $n_{h} \\times n_{h}$.\n\nStep 1: Parameter count $P_{\\text{dense}}$.\n\nFor the first RBM:\n- Weights: $n_{v} \\times n_{h}$ entries.\n- Biases: $n_{v}$ visible biases and $n_{h}$ hidden biases.\n\nFor each of the remaining $L-1$ RBMs:\n- Weights: $n_{h} \\times n_{h}$ entries.\n- Biases: $n_{h}$ visible (now the lower hidden layer acts as \"visible\") and $n_{h}$ hidden biases for each RBM. However, in a stacked RBM pretraining setup, each layer has its own bias vector. Across the stack, the total biases are the sum of all visible-layer biases and all hidden-layer biases for each RBM. Aggregating uniquely across layers, each layer contributes one bias vector. There are $1$ visible layer (size $n_{v}$) and $L$ hidden layers (each size $n_{h}$), giving a total bias count of $n_{v} + L n_{h}$.\n\nTherefore, the total number of parameters under dense connectivity is\n$$\nP_{\\text{dense}} = \\underbrace{n_{v} n_{h} + (L-1) n_{h}^{2}}_{\\text{weights}} + \\underbrace{n_{v} + L n_{h}}_{\\text{biases}}.\n$$\n\nStep 2: Per-epoch complexity $C_{\\text{dense}}$.\n\nUnder Contrastive Divergence (CD-$k$) with dataset size $N$ per epoch, the dominant computation is two matrix-vector products per CD step per data case per RBM: the up-pass $W^{\\top} v$ and the down-pass $W h$. The cost of each matrix-vector product scales with the number of nonzero entries in $W$. Let $\\alpha>0$ denote the constant of proportionality capturing the cumulative cost per edge per CD step per data case, including both the up-pass and down-pass.\n\nFor the first RBM, the number of edges equals the number of weights $n_{v} n_{h}$. For each of the remaining $L-1$ RBMs, the number of edges is $n_{h}^{2}$. Summing edges across the stack gives $n_{v} n_{h} + (L-1) n_{h}^{2}$.\n\nThus, the leading-order per-epoch complexity for dense connectivity is\n$$\nC_{\\text{dense}} = \\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\nStep 3: Sparse connectivity with edge probability $p$.\n\nIn the proposed sparse connectivity scheme, each potential visible-hidden edge in an RBM is independently present with probability $p \\in (0,1)$. The expected number of edges (and hence weights) becomes $p$ times the dense count, because the expectation of a sum of independent Bernoulli indicators equals the sum of their expectations.\n\nTherefore, the expected weight count across the stack is\n$$\n\\mathbb{E}[\\text{weights}] = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\nBiases remain unchanged, because they are not pruned in this scheme. Hence,\n$$\nP_{\\text{sparse}} = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right) + n_{v} + L n_{h}.\n$$\n\nFor the complexity, because the dominant per-epoch cost scales linearly with the number of edges, the expected per-epoch cost is\n$$\nC_{\\text{sparse}} = \\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\nStep 4: Ratio $R=C_{\\text{sparse}}/C_{\\text{dense}}$.\n\nUsing the expressions above,\n$$\nR = \\frac{C_{\\text{sparse}}}{C_{\\text{dense}}} = \\frac{\\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)}{\\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)} = p.\n$$\n\nThus, the closed-form expression for the compute reduction factor under the proposed sparse connectivity is $p$.", "answer": "$$\\boxed{p}$$", "id": "3112338"}, {"introduction": "The success of training a Deep Belief Network hinges on the stability of the optimization algorithm used for its constituent Restricted Boltzmann Machines. This practice moves beyond simply using an optimizer to analyzing its behavior near a solution, using the principles of calculus and linear algebra [@problem_id:3112322]. By deriving the stability conditions for Stochastic Gradient Descent (SGD) with momentum, you will develop a deep, theoretical understanding of how hyperparameters like learning rate $\\eta$ and momentum $\\mu$ interact with the loss landscape to ensure convergence.", "problem": "A Deep Belief Network (DBN) is often pretrained layer-wise using a Restricted Boltzmann Machine (RBM). Consider training an RBMâ€™s weight vector $\\mathbf{w}$ by Stochastic Gradient Descent (SGD) with classical momentum. Let the expected negative log-likelihood objective be denoted by $L(\\mathbf{w})$, which is twice continuously differentiable. Assume there is a local minimizer $\\mathbf{w}^{\\star}$ with a positive semidefinite Hessian $\\nabla^{2} L(\\mathbf{w}^{\\star})$, and denote by $\\lambda$ the largest eigenvalue of this Hessian. The momentum update is\n$$\n\\mathbf{v}_{t+1} = \\mu \\mathbf{v}_{t} - \\eta \\nabla L(\\mathbf{w}_{t}), \\quad \\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\mathbf{v}_{t+1},\n$$\nwhere $\\eta > 0$ is the learning rate and $\\mu \\in [0,1)$ is the momentum coefficient. Suppose mini-batches are sufficiently large that the gradient noise is negligible, and approximate the dynamics near $\\mathbf{w}^{\\star}$ by linearizing $L(\\mathbf{w})$ around $\\mathbf{w}^{\\star}$ using a quadratic expansion.\n\nFocus on the one-dimensional mode corresponding to the largest-curvature direction of $\\nabla^{2} L(\\mathbf{w}^{\\star})$ (i.e., the eigenvector associated with the eigenvalue $\\lambda$), and analyze the resulting linear time-invariant recurrence. Using only fundamental properties of Taylor expansions for smooth functions and eigen-decomposition of symmetric matrices, derive a sufficient condition on $(\\eta,\\mu)$ that ensures the iterates converge to $\\mathbf{w}^{\\star}$ without oscillations, understood here as having a non-oscillatory, monotone approach in the dominant mode (i.e., both characteristic roots of the linearized recurrence are real, nonnegative, and strictly less than $1$).\n\nProvide your final result as the tightest upper bound on the learning rate in that dominant mode, written as a single closed-form expression $\\eta_{\\max}(\\mu,\\lambda)$. No inequality symbols are permitted in the final answer. If any numerical evaluation is required, round to four significant figures; however, in this problem, an exact symbolic expression is expected.", "solution": "The problem requires the derivation of a sufficient condition for non-oscillatory convergence of Stochastic Gradient Descent (SGD) with classical momentum near a local minimizer $\\mathbf{w}^{\\star}$ of an objective function $L(\\mathbf{w})$. The convergence is analyzed in the one-dimensional mode corresponding to the largest eigenvalue, $\\lambda$, of the Hessian matrix $\\nabla^{2} L(\\mathbf{w}^{\\star})$.\n\nFirst, we linearize the optimization dynamics around the local minimizer $\\mathbf{w}^{\\star}$. The Taylor series expansion of the objective function $L(\\mathbf{w})$ around $\\mathbf{w}^{\\star}$ up to the second order is:\n$$L(\\mathbf{w}) \\approx L(\\mathbf{w}^{\\star}) + \\nabla L(\\mathbf{w}^{\\star})^{T}(\\mathbf{w} - \\mathbf{w}^{\\star}) + \\frac{1}{2}(\\mathbf{w} - \\mathbf{w}^{\\star})^{T} \\nabla^{2}L(\\mathbf{w}^{\\star}) (\\mathbf{w} - \\mathbf{w}^{\\star})$$\nSince $\\mathbf{w}^{\\star}$ is a local minimizer, the gradient at this point is zero: $\\nabla L(\\mathbf{w}^{\\star}) = \\mathbf{0}$. Let $\\mathbf{H} = \\nabla^{2}L(\\mathbf{w}^{\\star})$ be the Hessian matrix. The approximation for $L(\\mathbf{w})$ simplifies to:\n$$L(\\mathbf{w}) \\approx L(\\mathbf{w}^{\\star}) + \\frac{1}{2}(\\mathbf{w} - \\mathbf{w}^{\\star})^{T} \\mathbf{H} (\\mathbf{w} - \\mathbf{w}^{\\star})$$\nThe gradient of this quadratic approximation is:\n$$\\nabla L(\\mathbf{w}) \\approx \\mathbf{H} (\\mathbf{w} - \\mathbf{w}^{\\star})$$\nLet's define the error vector at iteration $t$ as $\\mathbf{\\delta}_{t} = \\mathbf{w}_{t} - \\mathbf{w}^{\\star}$. The linearized gradient at $\\mathbf{w}_{t}$ is $\\nabla L(\\mathbf{w}_{t}) \\approx \\mathbf{H} \\mathbf{\\delta}_{t}$. The momentum update equations are given as:\n$$\n\\mathbf{v}_{t+1} = \\mu \\mathbf{v}_{t} - \\eta \\nabla L(\\mathbf{w}_{t}) \\\\\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\mathbf{v}_{t+1}\n$$\nSubstituting the linearized gradient, the first equation becomes $\\mathbf{v}_{t+1} = \\mu \\mathbf{v}_{t} - \\eta \\mathbf{H} \\mathbf{\\delta}_{t}$.\nFrom the second equation, we can express the error vector relation: $\\mathbf{w}_{t+1} - \\mathbf{w}^{\\star} = (\\mathbf{w}_{t} - \\mathbf{w}^{\\star}) + \\mathbf{v}_{t+1}$, which yields $\\mathbf{\\delta}_{t+1} = \\mathbf{\\delta}_{t} + \\mathbf{v}_{t+1}$. This implies $\\mathbf{v}_{t+1} = \\mathbf{\\delta}_{t+1} - \\mathbf{\\delta}_{t}$. Similarly, $\\mathbf{v}_{t} = \\mathbf{\\delta}_{t} - \\mathbf{\\delta}_{t-1}$.\n\nSubstituting these expressions for $\\mathbf{v}_{t}$ and $\\mathbf{v}_{t+1}$ into the momentum update equation gives:\n$$\\mathbf{\\delta}_{t+1} - \\mathbf{\\delta}_{t} = \\mu(\\mathbf{\\delta}_{t} - \\mathbf{\\delta}_{t-1}) - \\eta \\mathbf{H} \\mathbf{\\delta}_{t}$$\nRearranging the terms, we obtain a second-order vector recurrence relation for the error vector:\n$$\\mathbf{\\delta}_{t+1} = (\\mathbf{I}(1+\\mu) - \\eta \\mathbf{H})\\mathbf{\\delta}_{t} - \\mu \\mathbf{I} \\mathbf{\\delta}_{t-1}$$\nThe problem asks to analyze the dynamics in the direction of the eigenvector $\\mathbf{u}$ of $\\mathbf{H}$ corresponding to its largest eigenvalue $\\lambda$. We project the error vector onto this direction, defining a scalar error component $\\delta_{t} = \\mathbf{\\delta}_{t}^{T} \\mathbf{u}$. The dynamics of this component are governed by the scalar recurrence relation:\n$$\\delta_{t+1} = (1+\\mu - \\eta \\lambda) \\delta_{t} - \\mu \\delta_{t-1}$$\nThis is a linear homogeneous recurrence relation with constant coefficients. We seek a solution of the form $\\delta_t = r^t$, which leads to the characteristic equation:\n$$r^2 - (1+\\mu - \\eta \\lambda)r + \\mu = 0$$\nThe problem defines non-oscillatory convergence as the condition that both characteristic roots, $r_1$ and $r_2$, are real, non-negative, and strictly less than $1$. Let's analyze these conditions for the quadratic polynomial $P(r) = r^2 - (1+\\mu - \\eta \\lambda)r + \\mu$.\n\n$1$. **Real roots**: The discriminant $D$ must be non-negative.\n$$D = (1+\\mu - \\eta \\lambda)^{2} - 4\\mu \\ge 0$$\nThis is equivalent to $|1+\\mu - \\eta \\lambda| \\ge 2\\sqrt{\\mu}$, which leads to two possibilities:\n(a) $1+\\mu - \\eta \\lambda \\ge 2\\sqrt{\\mu} \\implies \\eta \\lambda \\le 1+\\mu - 2\\sqrt{\\mu} = (1-\\sqrt{\\mu})^2$.\n(b) $1+\\mu - \\eta \\lambda \\le -2\\sqrt{\\mu} \\implies \\eta \\lambda \\ge 1+\\mu + 2\\sqrt{\\mu} = (1+\\sqrt{\\mu})^2$.\n\n$2$. **Non-negative roots**: For a quadratic equation with real roots, the roots are non-negative if their product is non-negative and their sum is non-negative.\n- Product of roots: $r_1 r_2 = \\mu$. Since $\\mu \\in [0, 1)$, this condition is satisfied, $r_1 r_2 \\ge 0$.\n- Sum of roots: $r_1 + r_2 = 1+\\mu - \\eta \\lambda$. This must be non-negative.\n$$1+\\mu - \\eta \\lambda \\ge 0 \\implies \\eta \\lambda \\le 1+\\mu$$\n\n$3$. **Roots strictly less than $1$**: For an upward-opening parabola with real roots, this is ensured if the larger root is less than $1$. This is equivalent to requiring that the vertex of the parabola is less than $1$ and $P(1)>0$.\n- Vertex location: $r_v = \\frac{1+\\mu-\\eta\\lambda}{2}$. The condition $r_v < 1$ implies $1+\\mu-\\eta\\lambda < 2$, or $\\eta\\lambda > \\mu-1$. Since $\\eta>0$, $\\lambda>0$ (for a non-degenerate curvature), and $\\mu<1$, this condition is always satisfied.\n- $P(1) = 1 - (1+\\mu-\\eta\\lambda) + \\mu = \\eta\\lambda$. For the roots to be strictly less than $1$, we need $P(1)>0$, so $\\eta\\lambda>0$, which is given.\n\nWe must now find the region of $(\\eta, \\mu, \\lambda)$ that satisfies all necessary conditions simultaneously. The governing conditions are:\n(i) $\\eta \\lambda \\le (1-\\sqrt{\\mu})^2$ OR $\\eta \\lambda \\ge (1+\\sqrt{\\mu})^2$\n(ii) $\\eta \\lambda \\le 1+\\mu$\n\nLet's reconcile these.\n- If we consider case (i-b), $\\eta \\lambda \\ge (1+\\sqrt{\\mu})^2$, then to also satisfy (ii), we must have $(1+\\sqrt{\\mu})^2 \\le \\eta \\lambda \\le 1+\\mu$. This requires $(1+\\sqrt{\\mu})^2 \\le 1+\\mu$, which simplifies to $1+2\\sqrt{\\mu}+\\mu \\le 1+\\mu$, or $2\\sqrt{\\mu} \\le 0$. This is only possible if $\\mu=0$.\n- If we consider case (i-a), $\\eta \\lambda \\le (1-\\sqrt{\\mu})^2$, we must also satisfy (ii), $\\eta \\lambda \\le 1+\\mu$. Since $\\mu \\in [0,1)$, we have $2\\sqrt{\\mu} \\ge 0$, so $(1-\\sqrt{\\mu})^2 = 1 - 2\\sqrt{\\mu} + \\mu \\le 1+\\mu$. Thus, the condition $\\eta \\lambda \\le (1-\\sqrt{\\mu})^2$ is stricter than $\\eta \\lambda \\le 1+\\mu$.\n\nTherefore, the encompassing condition for non-oscillatory convergence is given by case (i-a):\n$$\\eta \\lambda \\le (1-\\sqrt{\\mu})^2$$\nThis inequality must hold for the largest eigenvalue $\\lambda$. To guarantee non-oscillatory convergence, the learning rate $\\eta$ must be bounded. The tightest upper bound on $\\eta$ is derived from this condition:\n$$\\eta \\le \\frac{(1-\\sqrt{\\mu})^2}{\\lambda}$$\nThe maximum learning rate that ensures non-oscillatory convergence in this dominant mode is therefore:\n$$\\eta_{\\max}(\\mu, \\lambda) = \\frac{(1-\\sqrt{\\mu})^2}{\\lambda}$$\nThis expresssion provides the required closed-form upper bound.", "answer": "$$\\boxed{\\frac{(1-\\sqrt{\\mu})^2}{\\lambda}}$$", "id": "3112322"}, {"introduction": "A defining feature of Deep Belief Networks is their dual role as both a feature extractor (via the upward recognition pass) and a generative model (via the downward generative pass). This hands-on coding exercise explores the critical concept of \"tied weights,\" where the generative pathway reuses the transposed weights of the recognition pathway [@problem_id:3112369]. By implementing a full upward-downward pass and measuring the reconstruction quality, you will empirically verify why this architectural symmetry is fundamental to a DBN's ability to generate data similar to what it has learned.", "problem": "Consider a two-layer Deep Belief Network (DBN) with binary units at all layers, constructed by stacking two Restricted Boltzmann Machines (RBMs). Each RBM uses Bernoulli variables for visible and hidden units, and a logistic activation function. The fundamental base is the RBM energy-based model: for visible vector $\\mathbf{v} \\in \\{0,1\\}^d$ and hidden vector $\\mathbf{h} \\in \\{0,1\\}^m$, the energy is given by\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h},\n$$\nand the conditional distributions factorize due to bipartite structure:\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right),\\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\nwhere $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the logistic function.\n\nIn a two-layer DBN with visible layer $\\mathbf{v} \\in \\{0,1\\}^{d}$, first hidden layer $\\mathbf{h}^{(1)} \\in \\{0,1\\}^{m}$, and second hidden layer $\\mathbf{h}^{(2)} \\in \\{0,1\\}^{n}$, consider a deterministic mean-field upward pass (recognition model) followed by a deterministic downward pass (generative model). The upward pass uses recognition weights $W_{1r} \\in \\mathbb{R}^{d \\times m}$ and $W_{2r} \\in \\mathbb{R}^{m \\times n}$ with biases $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^{n}$:\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right), \\quad\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\nThe downward pass uses generative weights $W_{2g} \\in \\mathbb{R}^{n \\times m}$ and $W_{1g} \\in \\mathbb{R}^{m \\times d}$ with biases $\\mathbf{c}_1 \\in \\mathbb{R}^{m}$ and $\\mathbf{c}_0 \\in \\mathbb{R}^{d}$:\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right), \\quad\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\nWe define the generative consistency score for a dataset $\\mathcal{D} = \\{\\mathbf{v}^{(k)}\\}_{k=1}^{K}$ as the mean component-wise binary cross-entropy between the original visible vectors and their deterministic reconstructions $\\tilde{\\mathbf{v}}^{(k)}$ after the up-down pass:\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\nTo ensure numerical stability, use $\\epsilon = 10^{-12}$ and replace each $\\tilde{v}^{(k)}_i$ by $\\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$ before evaluating logarithms.\n\nYour task is to write a program that computes $J$ for four test cases that explore tied versus untied weights between the upward and downward passes, measuring the impact on generative consistency. Use the following fixed dataset with $K = 3$ and $d = 4$:\n$$\n\\mathbf{v}^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n\\mathbf{v}^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n\\mathbf{v}^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nLet $m = 3$ and $n = 2$. For all cases, the upward (recognition) parameters are fixed as:\n$$\nW_{1r} = \\begin{bmatrix}\n0.8 & -0.4 & 0.3 \\\\\n0.1 & 0.5 & -0.6 \\\\\n0.7 & -0.2 & 0.2 \\\\\n-0.5 & 0.3 & 0.4\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.6 & -0.3 \\\\\n0.4 & 0.2 \\\\\n-0.7 & 0.5\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}.\n$$\nEvaluate the following four cases for the downward (generative) parameters:\n\nCase $1$ (tied weights, moderate magnitudes):\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.6 & 0.4 & -0.7 \\\\ -0.3 & 0.2 & 0.5 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} -0.05 \\\\ 0.1 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.8 & 0.1 & 0.7 & -0.5 \\\\\n-0.4 & 0.5 & -0.2 & 0.3 \\\\\n0.3 & -0.6 & 0.2 & 0.4\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nCase $2$ (untied weights, small perturbations):\n$$\nW_{2g} = W_{2r}^\\top + \\Delta_2,\\quad\n\\Delta_2 = \\begin{bmatrix} 0.01 & -0.02 & 0.02 \\\\ -0.02 & 0.03 & -0.01 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} -0.02 \\\\ 0.05 \\\\ -0.01 \\end{bmatrix},\\quad\nW_{1g} = W_{1r}^\\top + \\Delta_1,\n$$\n$$\n\\Delta_1 = \\begin{bmatrix}\n0.02 & -0.01 & 0.03 & -0.02 \\\\\n-0.03 & 0.02 & 0.01 & -0.01 \\\\\n0.01 & 0.00 & -0.02 & 0.03\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ -0.02 \\end{bmatrix}.\n$$\n\nCase $3$ (untied weights, severe mismatch):\n$$\nW_{2g} = - W_{2r}^\\top + M_2,\\quad M_2 = \\begin{bmatrix} 0.0 & 0.0 & 0.0 \\\\ 0.2 & -0.2 & 0.2 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix},\\quad\nW_{1g} = - W_{1r}^\\top + M_1,\n$$\n$$\nM_1 = \\begin{bmatrix}\n0.0 & 0.0 & 0.0 & 0.0 \\\\\n0.1 & -0.1 & 0.1 & -0.1 \\\\\n-0.2 & 0.2 & -0.2 & 0.2\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} -0.3 \\\\ 0.3 \\\\ -0.3 \\\\ 0.3 \\end{bmatrix}.\n$$\n\nCase $4$ (tied weights at boundary, all zeros):\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.0 & 0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\nwith the upward (recognition) parameters set to\n$$\nW_{1r} = \\begin{bmatrix}\n0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.0 & 0.0 \\\\\n0.0 & 0.0 \\\\\n0.0 & 0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nImplement the deterministic mean-field upward and downward mappings as specified above, apply the numerical stabilizer $\\epsilon$ in the cross-entropy, and compute the generative consistency score $J$ for each case. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$). Each result must be a floating-point number. Round each result to $6$ decimal places in the final output.", "solution": "We start from the foundational definitions of the Restricted Boltzmann Machine (RBM). The RBM energy function for Bernoulli visible units $\\mathbf{v} \\in \\{0,1\\}^d$ and Bernoulli hidden units $\\mathbf{h} \\in \\{0,1\\}^m$ is\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h}.\n$$\nBecause the RBM graph is bipartite, the conditional distributions factorize over units. By marginalizing and using the Boltzmann distribution $p(\\mathbf{x}) \\propto \\exp(-E(\\mathbf{x}))$, one obtains the sigmoidal conditionals:\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right), \\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\nwith $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. These follow from the fact that, for a Bernoulli unit, the log-odds is the affine input given by the corresponding bias plus the weighted sum from connected units.\n\nA Deep Belief Network (DBN) stacks RBMs. In a two-layer DBN, the upward pass (recognition model) maps the visible layer to the top hidden layer deterministically using mean-field expectations (sigmoid probabilities). With recognition weights $W_{1r}$ and $W_{2r}$ and upward biases $\\mathbf{b}_1$ and $\\mathbf{b}_2$, the upward transformations are:\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right),\n$$\n$$\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\nThe downward pass (generative model) maps the top hidden layer back to the visible layer via generative weights $W_{2g}$ and $W_{1g}$ and downward biases $\\mathbf{c}_1$ and $\\mathbf{c}_0$:\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right),\n$$\n$$\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\nTied weights correspond to $W_{1g} = W_{1r}^\\top$ and $W_{2g} = W_{2r}^\\top$, enforcing a symmetry between recognition and generative mappings. Untied weights break this symmetry by allowing $W_{1g}$ and $W_{2g}$ to deviate from the transposes.\n\nTo quantify generative consistency, we use the mean component-wise binary cross-entropy between the original visible vectors and their reconstructions after an upward-downward pass:\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\nThis expression arises from the negative log-likelihood of independent Bernoulli outputs $\\tilde{v}^{(k)}_i$ for targets $v^{(k)}_i$, averaged across samples and dimensions. For numerical stability, before evaluating $\\log(\\cdot)$, we clamp probabilities via $\\tilde{v}^{(k)}_i \\leftarrow \\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$ with $\\epsilon = 10^{-12}$, ensuring arguments lie in $(0,1)$.\n\nAlgorithmically, for each case:\n$1.$ For each data vector $\\mathbf{v}^{(k)}$, compute $\\mathbf{h}^{(1)}$ using $W_{1r}$ and $\\mathbf{b}_1$.\n$2.$ Compute $\\mathbf{h}^{(2)}$ using $W_{2r}$ and $\\mathbf{b}_2$.\n$3.$ Compute $\\tilde{\\mathbf{h}}^{(1)}$ using $W_{2g}$ and $\\mathbf{c}_1$.\n$4.$ Compute $\\tilde{\\mathbf{v}}$ using $W_{1g}$ and $\\mathbf{c}_0$.\n$5.$ Clamp $\\tilde{\\mathbf{v}}$ component-wise to $[\\epsilon, 1 - \\epsilon]$.\n$6.$ Accumulate the cross-entropy for that $\\mathbf{v}^{(k)}$ against $\\tilde{\\mathbf{v}}$, then average over $K$ and divide by $d$ to obtain $J$.\n\nFrom principle-based reasoning, tied weights improve consistency because the upward mapping approximates an inference of sufficient statistics under the RBM conditionals, and the downward mapping with transposed weights enforces a matched linear transformation back toward the visible space. In the mean-field setting with moderate weights and balanced biases, this tends to reduce distortion between $\\mathbf{v}$ and $\\tilde{\\mathbf{v}}$. Small perturbations in untied weights introduce asymmetry, modestly degrading $J$. Severe mismatches reverse or distort the mapping, causing $\\tilde{\\mathbf{v}}$ to reflect a different generative manifold, thereby increasing the cross-entropy substantially. The boundary case with all-zero weights (and zero biases) yields $\\sigma(0) = 0.5$ at all units, making reconstructions uninformative and leading to a fixed cross-entropy determined solely by the entropy of Bernoulli targets relative to $0.5$ predictions.\n\nThe program implements these steps directly, uses the provided matrices and biases for each case, computes $J$ for the fixed dataset, and prints the four results as a single bracketed, comma-separated list with each value rounded to $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid using float64\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef clamp_probs(p, eps=1e-12):\n    # Clamp probabilities to (eps, 1-eps) for numerical stability in logs\n    return np.clip(p, eps, 1.0 - eps)\n\ndef generative_consistency_score(V, W1r, b1, W2r, b2, W2g, c1, W1g, c0, eps=1e-12):\n    \"\"\"\n    Compute mean component-wise binary cross-entropy between original visible vectors V\n    and reconstructions after upward-downward deterministic mean-field pass.\n    \"\"\"\n    K, d = V.shape\n    total_ce = 0.0\n    for k in range(K):\n        v = V[k]  # shape (d,)\n        # Upward pass: v -> h1 -> h2\n        h1_input = v @ W1r + b1  # shape (m,)\n        h1 = sigmoid(h1_input)\n        h2_input = h1 @ W2r + b2  # shape (n,)\n        h2 = sigmoid(h2_input)\n        # Downward pass: h2 -> h1_down -> v_hat\n        h1_down_input = h2 @ W2g + c1  # shape (m,)\n        h1_down = sigmoid(h1_down_input)\n        v_hat_input = h1_down @ W1g + c0  # shape (d,)\n        v_hat = sigmoid(v_hat_input)\n        v_hat = clamp_probs(v_hat, eps=eps)\n        # Binary cross-entropy per component\n        ce_vec = -(v * np.log(v_hat) + (1.0 - v) * np.log(1.0 - v_hat))\n        total_ce += np.sum(ce_vec)\n    # Mean across samples and dimensions\n    J = total_ce / (K * d)\n    return J\n\ndef solve():\n    # Fixed dataset V (K=3, d=4)\n    V = np.array([\n        [1.0, 0.0, 1.0, 0.0],\n        [0.0, 1.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0, 0.0]\n    ], dtype=np.float64)\n\n    # Recognition (upward) parameters for cases 1-3\n    W1r_base = np.array([\n        [0.8, -0.4, 0.3],\n        [0.1,  0.5, -0.6],\n        [0.7, -0.2, 0.2],\n        [-0.5, 0.3, 0.4]\n    ], dtype=np.float64)\n    b1_base = np.array([0.1, -0.2, 0.05], dtype=np.float64)\n\n    W2r_base = np.array([\n        [0.6, -0.3],\n        [0.4,  0.2],\n        [-0.7, 0.5]\n    ], dtype=np.float64)\n    b2_base = np.array([0.0, 0.1], dtype=np.float64)\n\n    # Case 1: tied weights\n    W2g_1 = W2r_base.T.copy()\n    c1_1 = np.array([-0.05, 0.1, 0.0], dtype=np.float64)\n    W1g_1 = W1r_base.T.copy()\n    c0_1 = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float64)\n\n    # Case 2: untied (small perturbations)\n    Delta2 = np.array([\n        [0.01, -0.02, 0.02],\n        [-0.02, 0.03, -0.01]\n    ], dtype=np.float64)\n    W2g_2 = W2r_base.T + Delta2\n    c1_2 = np.array([-0.02, 0.05, -0.01], dtype=np.float64)\n    Delta1 = np.array([\n        [0.02, -0.01, 0.03, -0.02],\n        [-0.03,  0.02, 0.01, -0.01],\n        [0.01,  0.00, -0.02, 0.03]\n    ], dtype=np.float64)\n    W1g_2 = W1r_base.T + Delta1\n    c0_2 = np.array([0.02, -0.01, 0.0, -0.02], dtype=np.float64)\n\n    # Case 3: untied (severe mismatch)\n    M2 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W2g_3 = -W2r_base.T + M2\n    c1_3 = np.array([0.5, -0.5, 0.5], dtype=np.float64)\n    M1 = np.array([\n        [0.0,  0.0,  0.0,  0.0],\n        [0.1, -0.1,  0.1, -0.1],\n        [-0.2, 0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W1g_3 = -W1r_base.T + M1\n    c0_3 = np.array([-0.3, 0.3, -0.3, 0.3], dtype=np.float64)\n\n    # Case 4: boundary all zeros (tied zeros)\n    W1r_4 = np.zeros((4, 3), dtype=np.float64)\n    b1_4 = np.zeros(3, dtype=np.float64)\n    W2r_4 = np.zeros((3, 2), dtype=np.float64)\n    b2_4 = np.zeros(2, dtype=np.float64)\n    W2g_4 = W2r_4.T.copy()  # zeros\n    c1_4 = np.zeros(3, dtype=np.float64)\n    W1g_4 = W1r_4.T.copy()  # zeros\n    c0_4 = np.zeros(4, dtype=np.float64)\n\n    test_cases = [\n        # Case 1: tied weights moderate\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_1, \"c1\": c1_1, \"W1g\": W1g_1, \"c0\": c0_1\n        },\n        # Case 2: untied small perturbations\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_2, \"c1\": c1_2, \"W1g\": W1g_2, \"c0\": c0_2\n        },\n        # Case 3: untied severe mismatch\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_3, \"c1\": c1_3, \"W1g\": W1g_3, \"c0\": c0_3\n        },\n        # Case 4: boundary all zeros (tied zeros)\n        {\n            \"W1r\": W1r_4, \"b1\": b1_4, \"W2r\": W2r_4, \"b2\": b2_4,\n            \"W2g\": W2g_4, \"c1\": c1_4, \"W1g\": W1g_4, \"c0\": c0_4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        J = generative_consistency_score(\n            V,\n            case[\"W1r\"], case[\"b1\"],\n            case[\"W2r\"], case[\"b2\"],\n            case[\"W2g\"], case[\"c1\"],\n            case[\"W1g\"], case[\"c0\"],\n            eps=1e-12\n        )\n        results.append(f\"{J:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3112369"}]}