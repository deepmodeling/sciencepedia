{"hands_on_practices": [{"introduction": "A common architectural choice in GAN generators is the transposed convolution layer for upsampling feature maps, but this can introduce conspicuous \"checkerboard\" artifacts in the generated images. This exercise [@problem_id:3127615] provides a hands-on opportunity to quantify these artifacts by implementing a custom metric, the Periodic Subgrid Variance, and to compare this upsampling strategy against a more robust alternative. By doing so, you will develop practical skills in diagnosing artifacts and connecting architectural choices to tangible improvements in output quality.", "problem": "You are given a task to study checkerboard edge artifacts that can arise from transposed convolution (also called deconvolution) in Generative Adversarial Networks (GANs) for image-to-image translation, and to compare them against a resize-then-convolution alternative. The goal is to define mathematically grounded artifact metrics and a proxy for the sensitivity of a PatchGAN (patch-based discriminator in a Generative Adversarial Network) to such artifacts, and to implement them in a program that runs deterministically on a small synthetic test suite.\n\nBegin from core definitions of discrete convolution and transposed convolution. Let an image be a two-dimensional array of real numbers. Let the convolution of an image with a kernel be defined by the discrete sum\n$$\n(y \\star k)[i,j] \\equiv \\sum_{u}\\sum_{v} y[i-u,j-v]\\,k[u,v],\n$$\nwith zero padding implicitly used where needed. Let the nearest-neighbor upsampling by factor $s$ of an image $x$ be denoted by $U_{s}^{\\mathrm{NN}}(x)$, defined such that each pixel is replicated into an $s \\times s$ block. Let the transposed convolution with stride $s$ be implemented by inserting $(s-1)$ zeros between adjacent pixels along both spatial axes to form a sparse image $Z_{s}(x)$ of shape enlarged by factor $s$, followed by a standard convolution with kernel $k$. The resize-then-convolution alternative is the composition $(U_{s}^{\\mathrm{NN}}(x)) \\star k$ with zero padding to maintain size.\n\nDefine the following artifact metric called the Periodic Subgrid Variance (PSV). Given an image $I$ and stride $s$, for each residue class $g=(a,b)$ with $a \\in \\{0,\\dots,s-1\\}$ and $b \\in \\{0,\\dots,s-1\\}$, let\n$$\n\\mu_g \\equiv \\frac{1}{|\\{(i,j): i \\equiv a \\bmod s,\\; j \\equiv b \\bmod s\\}|}\\sum_{\\substack{i\\equiv a \\bmod s\\\\ j\\equiv b \\bmod s}} I[i,j],\n$$\nand let the global mean be $\\mu \\equiv \\frac{1}{HW}\\sum_{i,j} I[i,j]$, where $H$ and $W$ are the height and width. The Periodic Subgrid Variance is then\n$$\n\\mathrm{PSV}_s(I) \\equiv \\frac{1}{s^2}\\sum_{a=0}^{s-1}\\sum_{b=0}^{s-1}\\left(\\mu_{(a,b)}-\\mu\\right)^2.\n$$\nIntuitively, if there is a checkerboard pattern aligned with the stride-$s$ grid, then $\\mathrm{PSV}_s(I)$ increases, while for a constant image it is $0$.\n\nTo relate to PatchGAN sensitivity, define a proxy sensitivity functional that computes PSV within non-overlapping patches of size $p \\times p$. For a fixed $p$ divisible by $s$, partition the image into $N$ disjoint patches $\\{I^{(n)}\\}_{n=1}^N$. Define the per-patch PSV as $\\mathrm{PSV}_s(I^{(n)})$ and the global variance as $\\sigma^2 \\equiv \\frac{1}{HW}\\sum_{i,j}\\left(I[i,j]-\\mu\\right)^2$. For a fixed threshold $\\tau \\equiv \\beta \\,\\sigma^2$ with a small constant $\\beta \\in (0,1)$, define the sensitivity proxy\n$$\nS_{\\text{patch}}(I; s,p,\\beta)\\equiv \\frac{1}{N}\\sum_{n=1}^N \\mathbf{1}\\left\\{\\mathrm{PSV}_s\\left(I^{(n)}\\right)>\\tau\\right\\}.\n$$\nThis quantity approximates the fraction of patches that would be flagged as artifact-dominated by a patch-based discriminator.\n\nYour program must implement the following pipeline for each test case:\n- Input: a low-resolution image $x$ of shape $L \\times L$ with values in $[0,1]$, an upsampling stride $s$, and a convolution kernel size $k$ with a predefined separable kernel $K$ constructed from a one-dimensional vector $v$ by $K = v v^\\top$, normalized so that $\\sum_{u,v} K[u,v] = 1$.\n- Transposed convolution output: $I_{\\text{deconv}} = Z_s(x) \\star K$ with zero padding and output size exactly $(sL) \\times (sL)$.\n- Resize-then-convolution output: $I_{\\text{resize}} = (U_{s}^{\\mathrm{NN}}(x)) \\star K$ with zero padding and output size exactly $(sL) \\times (sL)$.\n- Compute $G_{\\text{deconv}} \\equiv \\mathrm{PSV}_s(I_{\\text{deconv}})$ and $G_{\\text{resize}} \\equiv \\mathrm{PSV}_s(I_{\\text{resize}})$.\n- Compute the differences $R \\equiv G_{\\text{deconv}} - G_{\\text{resize}}$.\n- Compute $S_{\\text{deconv}} \\equiv S_{\\text{patch}}(I_{\\text{deconv}}; s,p,\\beta)$ and $S_{\\text{resize}} \\equiv S_{\\text{patch}}(I_{\\text{resize}}; s,p,\\beta)$, and the difference $P \\equiv S_{\\text{deconv}} - S_{\\text{resize}}$.\n\nUse the following fixed parameters for all test cases:\n- Stride $s = 2$.\n- Patch size $p = 8$.\n- Sensitivity threshold scale $\\beta = 0.1$.\n- Convolution kernels:\n  - If $k=3$, use $v = [1,2,1]$.\n  - If $k=4$, use $v = [1,3,3,1]$.\nIn both cases, use $K = \\frac{1}{\\sum_{u} \\sum_{v} v[u]v[v]} \\, v v^\\top$ so that $\\sum K = 1$.\n- Convolution is two-dimensional with zero padding and “same” output size.\n\nTest suite:\n- Case $1$ (happy path with expected checkerboard): $L=16$, $k=3$, and $x$ is a binary image with a centered square of ones. Specifically, $x[i,j] = 1$ if $i \\in [L/4, 3L/4)$ and $j \\in [L/4, 3L/4)$, and $0$ otherwise.\n- Case $2$ (kernel divisible by stride, reduced artifacts): $L=16$, $k=4$, and $x$ is the same centered square as in Case $1$.\n- Case $3$ (edge case: constant image): $L=16$, $k=3$, and $x[i,j] = 1$ for all $i,j$.\n- Case $4$ (random texture): $L=16$, $k=3$, and $x$ is a Bernoulli random image with $\\mathbb{P}(x[i,j]=1)=0.5$, generated with fixed random seed $42$.\n\nRequired final output:\n- For each case, compute the pair $(R,P)$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated flat list of $8$ real numbers in the order $[R_1,P_1,R_2,P_2,R_3,P_3,R_4,P_4]$, rounded to exactly $6$ decimal places, enclosed in square brackets, for example $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$.\n\nAll array indices are zero-based. There are no physical units. Angles do not appear. Your implementation must be fully deterministic given the test suite and constants above. Use only the specified environment and libraries. The correctness of your solution is judged by the numerical values produced for the test cases.", "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and objective. It is based on established concepts in deep learning and signal processing, specifically addressing checkerboard artifacts from transposed convolutions in Generative Adversarial Networks (GANs). All mathematical definitions, parameters, and test cases are specified with sufficient precision to permit a unique, verifiable solution.\n\nThe core of the problem is to implement two upsampling-convolution procedures, compute specified artifact metrics for their outputs, and compare the results. The two procedures are:\n1.  **Transposed Convolution**: Represented as zero-insertion followed by a standard convolution. An input image $x$ of size $L \\times L$ is first transformed into a sparse image $Z_s(x)$ of size $(sL-s+1) \\times (sL-s+1)$ by inserting $s-1$ zeros between adjacent pixels. This sparse image is then convolved with a kernel $K$. The convolution operation must be padded to produce a final output $I_{\\text{deconv}}$ of size $(sL) \\times (sL)$.\n2.  **Resize-then-Convolution**: A conceptually simpler alternative where the input image $x$ is first upscaled by a factor of $s$ using nearest-neighbor replication, resulting in an image $U_{s}^{\\mathrm{NN}}(x)$ of size $(sL) \\times (sL)$. This upscaled image is then convolved with the kernel $K$ using padding that preserves the image size (`'same'` convolution), yielding the output $I_{\\text{resize}}$.\n\nThe problem introduces two metrics to quantify artifacts:\n-   **Periodic Subgrid Variance ($\\mathrm{PSV}_s(I)$)**: This metric measures the variance of mean pixel values across $s \\times s$ periodic subgrids of an image $I$. A high $\\mathrm{PSV}_s$ value indicates a strong periodic pattern with a period matching the upsampling stride $s$, which is characteristic of checkerboard artifacts. It is defined as $\\mathrm{PSV}_s(I) \\equiv \\frac{1}{s^2}\\sum_{g}\\left(\\mu_{g}-\\mu\\right)^2$, where $\\mu$ is the global mean and $\\mu_g$ are the means of the $s^2$ subgrids.\n-   **Patch-based Sensitivity ($S_{\\text{patch}}(I; s, p, \\beta)$)**: This metric serves as a proxy for how a PatchGAN discriminator might react to artifacts. It computes the fraction of non-overlapping $p \\times p$ patches in the image for which the local $\\mathrm{PSV}_s$ exceeds a dynamic threshold $\\tau = \\beta \\sigma^2$, where $\\sigma^2$ is the global variance of the image and $\\beta$ is a small constant.\n\nThe solution proceeds as follows for each of the four test cases:\n1.  **Input Generation**: The low-resolution input image $x$ of size $L \\times L$ and the convolution kernel $K$ of size $k \\times k$ are constructed based on the test case parameters. The separable kernel $K$ is derived from a vector $v$ as $K = vv^\\top$ and normalized to sum to $1$.\n2.  **Image Generation**: The high-resolution images $I_{\\text{deconv}}$ and $I_{\\text{resize}}$ are generated according to their definitions. The convolution operation for $I_{\\text{deconv}}$ is implemented using a `'full'` convolution followed by a symmetric crop to achieve the specified output dimension of $(sL) \\times (sL)$. The convolution for $I_{\\text{resize}}$ is implemented using a `'same'` convolution, which naturally produces an output of the same size as its input, $(sL) \\times (sL)$.\n3.  **Metric Computation**:\n    -   The PSV values, $G_{\\text{deconv}} = \\mathrm{PSV}_s(I_{\\text{deconv}})$ and $G_{\\text{resize}} = \\mathrm{PSV}_s(I_{\\text{resize}})$, are calculated. Their difference is $R = G_{\\text{deconv}} - G_{\\text{resize}}$. A positive $R$ indicates that the transposed convolution method produces more checkerboard artifacts than the resize-convolution method.\n    -   The patch-based sensitivity values, $S_{\\text{deconv}} = S_{\\text{patch}}(I_{\\text{deconv}})$ and $S_{\\text{resize}} = S_{\\text{patch}}(I_{\\text{resize}})$, are calculated using the specified parameters $s=2$, $p=8$, and $\\beta=0.1$. Their difference is $P = S_{\\text{deconv}} - S_{\\text{resize}}$. A positive $P$ suggests that a PatchGAN is more likely to identify artifacts in the transposed convolution output.\n4.  **Result Aggregation**: The computed pairs $(R, P)$ for all four test cases are collected and formatted into a single list for the final output. The use of a fixed random seed ensures the result for the random image case is deterministic.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef get_kernel(k_size):\n    \"\"\"Constructs a normalized separable 2D kernel.\"\"\"\n    if k_size == 3:\n        v = np.array([1, 2, 1], dtype=float)\n    elif k_size == 4:\n        v = np.array([1, 3, 3, 1], dtype=float)\n    else:\n        raise ValueError(\"Unsupported kernel size\")\n    \n    kernel = np.outer(v, v)\n    kernel /= kernel.sum()\n    return kernel\n\ndef get_input_image(L, image_type, seed=42):\n    \"\"\"Generates the low-resolution input image for a test case.\"\"\"\n    if image_type == 'centered_square':\n        x = np.zeros((L, L), dtype=float)\n        start = L // 4\n        end = 3 * L // 4\n        x[start:end, start:end] = 1.0\n        return x\n    elif image_type == 'constant':\n        return np.ones((L, L), dtype=float)\n    elif image_type == 'random':\n        rng = np.random.default_rng(seed)\n        return rng.choice([0.0, 1.0], size=(L, L), p=[0.5, 0.5])\n    else:\n        raise ValueError(\"Unsupported image type\")\n\ndef upsample_zeros(x, s):\n    \"\"\"Upsamples by inserting s-1 zeros between pixels.\"\"\"\n    L = x.shape[0]\n    up_L = s * L - s + 1\n    z = np.zeros((up_L, up_L), dtype=float)\n    z[::s, ::s] = x\n    return z\n\ndef upsample_nn(x, s):\n    \"\"\"Upsamples using nearest-neighbor replication.\"\"\"\n    return np.kron(x, np.ones((s, s), dtype=float))\n\ndef custom_convolve(img, kernel, output_shape):\n    \"\"\"Performs 2D convolution with padding to achieve a target output size.\"\"\"\n    full_conv = convolve2d(img, kernel, mode='full')\n    \n    crop_total_h = full_conv.shape[0] - output_shape[0]\n    crop_total_w = full_conv.shape[1] - output_shape[1]\n\n    crop_start_h = crop_total_h // 2\n    crop_start_w = crop_total_w // 2\n    \n    crop_end_h = crop_start_h + output_shape[0]\n    crop_end_w = crop_start_w + output_shape[1]\n    \n    return full_conv[crop_start_h:crop_end_h, crop_start_w:crop_end_w]\n\ndef psv(I, s):\n    \"\"\"Computes the Periodic Subgrid Variance (PSV).\"\"\"\n    if I.size == 0:\n        return 0.0\n    mu = I.mean()\n    subgrid_means = []\n    for a in range(s):\n        for b in range(s):\n            subgrid = I[a::s, b::s]\n            if subgrid.size > 0:\n                subgrid_means.append(subgrid.mean())\n            else:\n                subgrid_means.append(mu) \n    \n    psv_val = np.mean((np.array(subgrid_means) - mu)**2)\n    return psv_val\n\ndef s_patch(I, s, p, beta):\n    \"\"\"Computes the patch-based sensitivity proxy.\"\"\"\n    H, W = I.shape\n    num_patches_h = H // p\n    num_patches_w = W // p\n    N = num_patches_h * num_patches_w\n\n    if N == 0:\n        return 0.0\n\n    sigma2 = np.var(I)\n    tau = beta * sigma2\n\n    flagged_patches = 0\n    for i in range(num_patches_h):\n        for j in range(num_patches_w):\n            patch = I[i*p : (i+1)*p, j*p : (j+1)*p]\n            psv_patch = psv(patch, s)\n            if psv_patch > tau:\n                flagged_patches += 1\n    \n    return float(flagged_patches) / N\n\ndef process_case(L, k, image_type, s, p, beta):\n    \"\"\"Processes a single test case and computes (R, P).\"\"\"\n    output_L = s * L\n    \n    x = get_input_image(L, image_type)\n    kernel = get_kernel(k)\n\n    # Transposed convolution path\n    z_x = upsample_zeros(x, s)\n    I_deconv = custom_convolve(z_x, kernel, output_shape=(output_L, output_L))\n    \n    # Resize-then-convolution path\n    u_x = upsample_nn(x, s)\n    I_resize = convolve2d(u_x, kernel, mode='same')\n    \n    # Compute R\n    G_deconv = psv(I_deconv, s)\n    G_resize = psv(I_resize, s)\n    R = G_deconv - G_resize\n    \n    # Compute P\n    S_deconv = s_patch(I_deconv, s, p, beta)\n    S_resize = s_patch(I_resize, s, p, beta)\n    P = S_deconv - S_resize\n    \n    return R, P\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    # Fixed parameters\n    s = 2\n    p = 8\n    beta = 0.1\n    \n    test_cases = [\n        {'L': 16, 'k': 3, 'image_type': 'centered_square'},\n        {'L': 16, 'k': 4, 'image_type': 'centered_square'},\n        {'L': 16, 'k': 3, 'image_type': 'constant'},\n        {'L': 16, 'k': 3, 'image_type': 'random'},\n    ]\n\n    results = []\n    for case in test_cases:\n        R, P = process_case(case['L'], case['k'], case['image_type'], s, p, beta)\n        results.append(R)\n        results.append(P)\n\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3127615"}, {"introduction": "The adversarial nature of GANs makes their training notoriously unstable, often leading to diverging model parameters. To build intuition for how to combat this, this practice [@problem_id:3127717] has you analyze a simplified linear model of the generator-discriminator game, where you can directly observe the stabilizing effects of key techniques like gradient clipping and spectral normalization. Simulating these dynamics will give you a deeper, more tangible understanding of why these methods are crucial for successfully training modern GANs.", "problem": "You will implement and analyze gradient clipping for the discriminator in a simplified linearized min-max game that approximates the local training dynamics of Generative Adversarial Networks (GANs) used for image-to-image translation tasks such as pix2pix and Cycle-Consistent Generative Adversarial Network (CycleGAN). You will compare its impact on stability against two alternative constraints motivated by the Wasserstein Generative Adversarial Network (WGAN): spectral normalization and weight clipping. Your program must simulate the discrete-time gradient descent-ascent dynamics in a purely mathematical setting and report quantitative stability metrics across a prescribed test suite.\n\nStarting point and definitions:\n- Consider a bilinear objective modeling the local behavior of the generator-discriminator game, given by $L(\\mathbf{d}, \\mathbf{g}) = \\mathbf{d}^{\\top} \\mathbf{A} \\mathbf{g}$, where $\\mathbf{d} \\in \\mathbb{R}^{n}$ are discriminator parameters, $\\mathbf{g} \\in \\mathbb{R}^{n}$ are generator parameters, and $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is a fixed matrix representing the linearized interaction. The discriminator maximizes and the generator minimizes the objective.\n- The unconstrained simultaneous gradient descent-ascent update from state $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ with step size $\\eta > 0$ uses the gradients $\\nabla_{\\mathbf{d}} L(\\mathbf{d}_{t}, \\mathbf{g}_{t}) = \\mathbf{A} \\mathbf{g}_{t}$ and $\\nabla_{\\mathbf{g}} L(\\mathbf{d}_{t}, \\mathbf{g}_{t}) = \\mathbf{A}^{\\top} \\mathbf{d}_{t}$, producing the next state $(\\mathbf{d}_{t+1}, \\mathbf{g}_{t+1})$ via\n  simultaneous updates that depend only on $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$.\n- You will implement three discriminator constraints:\n  1) Gradient clipping (apply only to the discriminator gradient): replace $\\nabla_{\\mathbf{d}} L$ by $\\operatorname{clip}_{c}(\\nabla_{\\mathbf{d}} L)$, where $\\operatorname{clip}_{c}(\\mathbf{v}) = \\mathbf{v} \\cdot \\min\\{1, c / \\lVert \\mathbf{v} \\rVert_{2}\\}$ and $c \\ge 0$ is a threshold.\n  2) Spectral normalization: rescale $\\mathbf{A}$ to $\\mathbf{A}_{\\text{SN}} = \\alpha \\cdot \\mathbf{A} / \\sigma_{\\max}(\\mathbf{A})$, where $\\sigma_{\\max}(\\mathbf{A})$ is the spectral norm (largest singular value) and $\\alpha \\ge 0$ is a target value.\n  3) Weight clipping: form $\\mathbf{A}_{\\text{WC}}$ by applying elementwise clipping with bound $w \\ge 0$, that is, $(\\mathbf{A}_{\\text{WC}})_{ij} = \\max\\{-w, \\min\\{A_{ij}, w\\}\\}$.\n- Stability will be quantified by the empirical growth ratio $r = \\lVert [\\mathbf{d}_{T}; \\mathbf{g}_{T}] \\rVert_{2} / \\lVert [\\mathbf{d}_{0}; \\mathbf{g}_{0}] \\rVert_{2}$ after $T$ steps, computed for a common random initialization. Additionally, report the spectral radius $\\rho$ of the linearized unconstrained update operator associated with the effective interaction matrix used by the method (for gradient clipping, report the spectral radius induced by the unmodified $\\mathbf{A}$; for spectral normalization and weight clipping, report that of the modified matrix). Finally, report the fraction of steps in which discriminator gradient clipping was active, defined as the fraction of iterations for which $\\lVert \\nabla_{\\mathbf{d}} L(\\mathbf{d}_{t}, \\mathbf{g}_{t}) \\rVert_{2} > c$. For methods not using gradient clipping, this fraction is defined to be $0$.\n\nProgram requirements:\n- Dimension is $n = 5$. Initialize $\\mathbf{A}$ by drawing independent standard normal entries with a fixed seed $s_{A} = 7$. Initialize $(\\mathbf{d}_{0}, \\mathbf{g}_{0})$ by drawing independent standard normal entries with a fixed seed $s_{0} = 11$. Use the same $\\mathbf{A}$ and $(\\mathbf{d}_{0}, \\mathbf{g}_{0})$ across all test cases.\n- Use simultaneous updates: at each step $t$, compute gradients from $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ and produce $(\\mathbf{d}_{t+1}, \\mathbf{g}_{t+1})$ using only those gradients, without using intermediate updated values within the same iteration.\n- For the gradient clipping method, only the discriminator gradient is clipped; the generator gradient is never clipped.\n- Simulation horizon is $T = 200$ steps for all test cases. Use Euclidean norm $\\lVert \\cdot \\rVert_{2}$.\n- Spectral norm $\\sigma_{\\max}(\\mathbf{A})$ is defined as the largest singular value of $\\mathbf{A}$. Spectral radius $\\rho(\\mathbf{M})$ is defined as the largest absolute value among eigenvalues of $\\mathbf{M}$.\n\nDiscriminator constraints to compare:\n- Method $0$ (none): no clipping and use the original $\\mathbf{A}$.\n- Method $1$ (gradient clipping): apply clipping with threshold $c \\ge 0$ to the discriminator gradient only.\n- Method $2$ (spectral normalization): use $\\mathbf{A}_{\\text{SN}}$ with target $\\alpha \\ge 0$.\n- Method $3$ (weight clipping): use $\\mathbf{A}_{\\text{WC}}$ with bound $w \\ge 0$.\n\nSimulation details to be implemented for one step given $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$:\n- For methods $0, 2, 3$, form an effective matrix $\\mathbf{A}_{\\text{eff}}$ once per test case: for method $0$, $\\mathbf{A}_{\\text{eff}} = \\mathbf{A}$; for method $2$, $\\mathbf{A}_{\\text{eff}} = \\alpha \\cdot \\mathbf{A} / \\sigma_{\\max}(\\mathbf{A})$; for method $3$, $\\mathbf{A}_{\\text{eff}}$ is the elementwise clipped $\\mathbf{A}$ with bound $w$. Then update\n  $\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\, \\mathbf{A}_{\\text{eff}} \\mathbf{g}_{t}$ and $\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta \\, \\mathbf{A}_{\\text{eff}}^{\\top} \\mathbf{d}_{t}$.\n- For method $1$, use $\\mathbf{A}$ and compute the discriminator gradient $\\mathbf{g}^{(d)}_{t} = \\mathbf{A} \\mathbf{g}_{t}$, clip it to threshold $c$, and update $\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\, \\operatorname{clip}_{c}(\\mathbf{g}^{(d)}_{t})$. The generator update uses the unclipped generator gradient: $\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta \\, \\mathbf{A}^{\\top} \\mathbf{d}_{t}$.\n\nLinearized operator and spectral radius:\n- For methods $0, 2, 3$, the simultaneous update defines a linear map on the stacked state $\\mathbf{z}_{t} = [\\mathbf{d}_{t}; \\mathbf{g}_{t}]$ of the form $\\mathbf{z}_{t+1} = \\mathbf{M} \\mathbf{z}_{t}$, where $\\mathbf{M}$ depends on $\\eta$ and $\\mathbf{A}_{\\text{eff}}$. Compute and report the spectral radius $\\rho(\\mathbf{M})$ for these methods.\n- For method $1$, report the spectral radius computed as if no clipping were applied (use $\\mathbf{A}_{\\text{eff}} = \\mathbf{A}$) to provide a common linear reference.\n\nOutput metrics per test case:\n- Growth ratio $r = \\lVert [\\mathbf{d}_{T}; \\mathbf{g}_{T}] \\rVert_{2} / \\lVert [\\mathbf{d}_{0}; \\mathbf{g}_{0}] \\rVert_{2}$ as a floating-point number.\n- Spectral radius $\\rho$ of the corresponding linear update matrix $\\mathbf{M}$ as a floating-point number.\n- Clipped fraction $q \\in [0, 1]$ defined as the fraction of steps for which discriminator gradient clipping was active; for methods that do not use gradient clipping, $q = 0$.\n\nTest suite:\nUse $n = 5$, $T = 200$, $s_{A} = 7$, $s_{0} = 11$, and the following five test cases, each specified as a tuple $(\\text{method}, \\eta, c, \\alpha, w)$:\n1) $(0, 0.05, 0.0, 0.0, 0.0)$: no constraint, moderate step size.\n2) $(1, 0.20, 0.50, 0.0, 0.0)$: discriminator gradient clipping with threshold $c = 0.50$ at larger step size.\n3) $(2, 0.20, 0.0, 0.50, 0.0)$: spectral normalization to $\\alpha = 0.50$ at larger step size.\n4) $(3, 0.20, 0.0, 0.0, 0.10)$: weight clipping with bound $w = 0.10$ at larger step size.\n5) $(1, 0.20, 0.00, 0.0, 0.0)$: boundary case for gradient clipping with $c = 0.00$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists of the form $[\\,[r_{1}, \\rho_{1}, q_{1}],\\,[r_{2}, \\rho_{2}, q_{2}],\\,[r_{3}, \\rho_{3}, q_{3}],\\,[r_{4}, \\rho_{4}, q_{4}],\\,[r_{5}, \\rho_{5}, q_{5}]\\,]$ with no extra text.\n\nAll numeric results should be standard real numbers; no physical units are involved. Angles are not used. Percentages are not used; the clipped fraction is a real number in $[0, 1]$.", "solution": "The problem requires the simulation and analysis of a linearized min-max game, which serves as a simplified model for the training dynamics of Generative Adversarial Networks (GANs). We are to compare the stability of the unconstrained system with three common regularization techniques applied to the discriminator: gradient clipping, spectral normalization, and weight clipping.\n\nThe core of the model is the bilinear objective function $L(\\mathbf{d}, \\mathbf{g}) = \\mathbf{d}^{\\top} \\mathbf{A} \\mathbf{g}$, where $\\mathbf{d}, \\mathbf{g} \\in \\mathbb{R}^{n}$ are the parameter vectors for the discriminator and generator, respectively, and $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is a fixed interaction matrix. The discriminator aims to maximize $L$, while the generator aims to minimize it. This is a zero-sum game.\n\nThe dynamics are modeled by simultaneous gradient descent-ascent. The gradients are $\\nabla_{\\mathbf{d}} L = \\mathbf{A} \\mathbf{g}$ and $\\nabla_{\\mathbf{g}} L = \\mathbf{A}^{\\top} \\mathbf{d}$. With a learning rate $\\eta > 0$, the update rules from state $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ to $(\\mathbf{d}_{t+1}, \\mathbf{g}_{t+1})$ are:\n$$\n\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta (\\nabla_{\\mathbf{d}} L)|_{(\\mathbf{d}_{t}, \\mathbf{g}_{t})}\n$$\n$$\n\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta (\\nabla_{\\mathbf{g}} L)|_{(\\mathbf{d}_{t}, \\mathbf{g}_{t})}\n$$\n\nFor three of the four methods (none, spectral normalization, weight clipping), the dynamics are linear. These methods can be described by using an effective interaction matrix $\\mathbf{A}_{\\text{eff}}$. The updates become:\n$$\n\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\mathbf{A}_{\\text{eff}} \\mathbf{g}_{t}\n$$\n$$\n\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta \\mathbf{A}_{\\text{eff}}^{\\top} \\mathbf{d}_{t}\n$$\nWe can represent the full state as a stacked vector $\\mathbf{z}_{t} = [\\mathbf{d}_{t}^{\\top}, \\mathbf{g}_{t}^{\\top}]^{\\top} \\in \\mathbb{R}^{2n}$. The update can be written as a linear transformation $\\mathbf{z}_{t+1} = \\mathbf{M} \\mathbf{z}_{t}$, where the update matrix $\\mathbf{M}$ is:\n$$\n\\mathbf{M} = \\begin{pmatrix} \\mathbf{I}_{n} & \\eta \\mathbf{A}_{\\text{eff}} \\\\ -\\eta \\mathbf{A}_{\\text{eff}}^{\\top} & \\mathbf{I}_{n} \\end{pmatrix}\n$$\nHere, $\\mathbf{I}_{n}$ is the $n \\times n$ identity matrix. The stability of this linear dynamical system is determined by the spectral radius $\\rho(\\mathbf{M})$ of the matrix $\\mathbf{M}$, which is the largest absolute value of its eigenvalues. If $\\rho(\\mathbf{M}) > 1$, the norm of the state vector $\\lVert \\mathbf{z}_{t} \\rVert$ will generally grow exponentially, indicating instability. If $\\rho(\\mathbf{M}) \\le 1$, the dynamics are stable.\n\nThe eigenvalues of $\\mathbf{M}$ can be found by relating them to the singular values of $\\mathbf{A}_{\\text{eff}}$. The matrix $\\mathbf{M}$ can be written as $\\mathbf{M} = \\mathbf{I}_{2n} + \\mathbf{K}$, where $\\mathbf{K} = \\begin{pmatrix} \\mathbf{0} & \\eta \\mathbf{A}_{\\text{eff}} \\\\ -\\eta \\mathbf{A}_{\\text{eff}}^{\\top} & \\mathbf{0} \\end{pmatrix}$. The eigenvalues of $\\mathbf{K}$ are purely imaginary, given by $\\lambda_{\\mathbf{K}} = \\pm i \\eta s_{k}$, where $s_{k}$ are the singular values of $\\mathbf{A}_{\\text{eff}}$. The eigenvalues of $\\mathbf{M}$ are then $\\lambda_{\\mathbf{M}} = 1 + \\lambda_{\\mathbf{K}} = 1 \\pm i \\eta s_{k}$. The magnitude of these eigenvalues is $|\\lambda_{\\mathbf{M}}| = \\sqrt{1^2 + (\\eta s_{k})^2} = \\sqrt{1 + \\eta^2 s_{k}^2}$. The spectral radius $\\rho(\\mathbf{M})$ is the maximum of these magnitudes, which occurs for the largest singular value, $\\sigma_{\\max}(\\mathbf{A}_{\\text{eff}})$:\n$$\n\\rho(\\mathbf{M}) = \\sqrt{1 + \\eta^2 \\sigma_{\\max}(\\mathbf{A}_{\\text{eff}})^2}\n$$\nThis analytical result provides a clear connection between the step size $\\eta$, the spectral properties of the interaction matrix $\\mathbf{A}_{\\text{eff}}$, and the stability of the linear system.\n\nWe now analyze each method:\n1.  **Method 0 (None)**: $\\mathbf{A}_{\\text{eff}} = \\mathbf{A}$. The stability is determined by $\\rho(\\mathbf{M}) = \\sqrt{1 + \\eta^2 \\sigma_{\\max}(\\mathbf{A})^2}$. For any $\\eta > 0$, $\\rho(\\mathbf{M}) > 1$, so the unconstrained system is expected to be unstable.\n2.  **Method 2 (Spectral Normalization)**: $\\mathbf{A}_{\\text{eff}} = \\alpha \\cdot \\mathbf{A} / \\sigma_{\\max}(\\mathbf{A})$. This procedure directly sets the spectral norm of the effective matrix, $\\sigma_{\\max}(\\mathbf{A}_{\\text{eff}}) = \\alpha$. The spectral radius is then given by $\\rho(\\mathbf{M}) = \\sqrt{1 + (\\eta\\alpha)^2}$. This method provides explicit control over the linear stability of the game.\n3.  **Method 3 (Weight Clipping)**: $\\mathbf{A}_{\\text{eff}}$ is formed by element-wise clipping: $(\\mathbf{A}_{\\text{eff}})_{ij} = \\max\\{-w, \\min\\{A_{ij}, w\\}\\}$. This operation generally reduces the magnitudes of the matrix entries, which typically leads to a smaller spectral norm, i.e., $\\sigma_{\\max}(\\mathbf{A}_{\\text{eff}}) \\le \\sigma_{\\max}(\\mathbf{A})$. This can stabilize the system by reducing $\\rho(\\mathbf{M})$.\n4.  **Method 1 (Gradient Clipping)**: This method introduces a non-linearity. The discriminator update is $\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\cdot \\operatorname{clip}_{c}(\\mathbf{A} \\mathbf{g}_{t})$, where $\\operatorname{clip}_{c}(\\mathbf{v}) = \\mathbf{v} \\cdot \\min\\{1, c / \\lVert \\mathbf{v} \\rVert_{2}\\}$. This prevents the norm of the update vector $\\eta \\nabla_{\\mathbf{d}} L$ from exceeding $\\eta c$. The system is no longer linear, so the spectral radius analysis of $\\mathbf{M}$ does not describe the true dynamics. We compute $\\rho$ using $\\mathbf{A}_{\\text{eff}}=\\mathbf{A}$ to provide a reference for the underlying linear instability that this method must counteract. The fraction of steps where clipping is active, $q$, measures the extent of this non-linear intervention. The special case $c=0$ results in $\\operatorname{clip}_{0}(\\mathbf{v})=\\mathbf{0}$ for any $\\mathbf{v}$, which freezes the discriminator parameters $\\mathbf{d}$ at their initial values.\n\nThe implementation follows these principles. First, the matrix $\\mathbf{A}$ and initial state $(\\mathbf{d}_0, \\mathbf{g}_0)$ are generated using fixed seeds. For each test case, the appropriate $\\mathbf{A}_{\\text{eff}}$ is determined for the linear methods. The spectral radius $\\rho$ is computed. Then, a simulation loop runs for $T=200$ steps, applying the specific update rule for the chosen method. For gradient clipping, a counter tracks when the discriminator gradient norm $\\lVert \\mathbf{A} \\mathbf{g}_t \\rVert_2$ exceeds the threshold $c$. Finally, the growth ratio $r = \\lVert [\\mathbf{d}_T; \\mathbf{g}_T] \\rVert_2 / \\lVert [\\mathbf{d}_0; \\mathbf{g}_0] \\rVert_2$ and the clipped fraction $q$ are calculated.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN dynamics simulation and analysis.\n    \"\"\"\n    # Global parameters\n    n = 5\n    T = 200\n    s_A = 7\n    s_0 = 11\n\n    # Initialize matrix A and initial state vectors d0, g0 using specified seeds\n    rng_A = np.random.default_rng(seed=s_A)\n    A = rng_A.standard_normal((n, n))\n\n    rng_0 = np.random.default_rng(seed=s_0)\n    d0 = rng_0.standard_normal(n)\n    g0 = rng_0.standard_normal(n)\n\n    # Test suite: (method, eta, c, alpha, w)\n    test_cases = [\n        (0, 0.05, 0.0, 0.0, 0.0),  # Method 0: No constraint\n        (1, 0.20, 0.50, 0.0, 0.0),  # Method 1: Gradient clipping\n        (2, 0.20, 0.0, 0.50, 0.0),  # Method 2: Spectral normalization\n        (3, 0.20, 0.0, 0.0, 0.10),  # Method 3: Weight clipping\n        (1, 0.20, 0.00, 0.0, 0.0),  # Method 1: Gradient clipping with c=0\n    ]\n\n    results = []\n    for case in test_cases:\n        method, eta, c, alpha, w = case\n        \n        # --- 1. Calculate Spectral Radius (rho) ---\n        # For methods 0, 2, 3, A_eff is determined by the method's parameters.\n        # For method 1, rho is calculated for the underlying unconstrained linear system.\n        A_eff_for_rho = A\n        if method == 2:\n            s = np.linalg.svd(A, compute_uv=False)\n            sigma_max_A = s[0] if s.size > 0 else 0.0\n            A_eff_for_rho = alpha * A / sigma_max_A if sigma_max_A > 0 else np.zeros_like(A)\n        elif method == 3:\n            A_eff_for_rho = np.clip(A, -w, w)\n        \n        I_n = np.identity(n)\n        M = np.block([\n            [I_n, eta * A_eff_for_rho],\n            [-eta * A_eff_for_rho.T, I_n]\n        ])\n        eigvals = np.linalg.eigvals(M)\n        rho = np.max(np.abs(eigvals))\n\n        # --- 2. Run Simulation ---\n        d, g = d0.copy(), g0.copy()\n        clipped_steps_count = 0\n\n        # Determine the effective matrix for simulation (for linear methods)\n        A_eff_sim = None\n        if method == 0:\n            A_eff_sim = A\n        elif method == 2:\n            A_eff_sim = A_eff_for_rho\n        elif method == 3:\n            A_eff_sim = A_eff_for_rho\n        \n        for _ in range(T):\n            if method == 1:  # Non-linear update for gradient clipping\n                grad_d = A @ g\n                norm_grad_d = np.linalg.norm(grad_d)\n                \n                d_update = grad_d\n                if norm_grad_d > c:\n                    clipped_steps_count += 1\n                    if norm_grad_d > 1e-9: # Avoid division by zero\n                        d_update = grad_d * (c / norm_grad_d)\n                    else: # if norm is zero, gradient is zero vector\n                        d_update = np.zeros_like(grad_d)\n\n                d_next = d + eta * d_update\n                g_next = g - eta * (A.T @ d)\n                d, g = d_next, g_next\n            else:  # Linear updates for methods 0, 2, 3\n                d_next = d + eta * (A_eff_sim @ g)\n                g_next = g - eta * (A_eff_sim.T @ d)\n                d, g = d_next, g_next\n\n        # --- 3. Calculate Final Metrics (r, q) ---\n        z0 = np.concatenate((d0, g0))\n        norm_z0 = np.linalg.norm(z0)\n        \n        zT = np.concatenate((d, g))\n        norm_zT = np.linalg.norm(zT)\n\n        r = norm_zT / norm_z0 if norm_z0 > 1e-9 else 0.0\n        \n        q = 0.0\n        if method == 1:\n            q = clipped_steps_count / T if T > 0 else 0.0\n\n        results.append([r, rho, q])\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3127717"}, {"introduction": "An effective loss function should guide a model towards a desired solution, but sometimes models find clever, unintended shortcuts. This exercise [@problem_id:3127696] explores a famous failure mode of CycleGAN, where the model \"cheats\" the cycle-consistency objective by hiding information steganographically rather than performing a meaningful translation. You will model this phenomenon as a noisy communication channel and design an information-theoretic penalty, learning how to create more robust training objectives that discourage such behavior.", "problem": "You are tasked with constructing and analyzing a synthetic scenario that demonstrates a failure mode of Cycle-Consistent Generative Adversarial Networks (CycleGAN) for image-to-image translation, and designing a principled penalty to mitigate it. The context is the following. A Generative Adversarial Network (GAN) consists of a generator and a discriminator trained in opposition to model a target distribution. A Cycle-Consistent Generative Adversarial Network (CycleGAN) augments this with two generators, denoted $F$ and $G$, that map between two domains $X$ and $Y$, together with a cycle-consistency constraint encouraging $G(F(x)) \\approx x$ for $x \\sim p_X$ and $F(G(y)) \\approx y$ for $y \\sim p_Y$. Consider an adversarial dataset where the distribution $p_X$ contains images $x$ that are pairs of content and machine-readable code: each $x$ has a content region and an embedded \"Quick Response (QR)\"-like code region that deterministically encodes the content of $x$ itself. This creates a potential failure mode: the generator $F$ can hide the content of $x$ in a high-frequency microtexture matching the marginal statistics of $p_Y$, and the generator $G$ can decode the hidden code to reconstruct $x$, achieving low cycle-consistency error without performing meaningful translation.\n\nFormulate a mathematically precise toy model of this hiding and decoding channel and compute two quantities for various parameter settings. Your model must satisfy the following conditions.\n\n1. Let $x \\in \\{0,1\\}^{m \\times m}$ denote the content region bits, treated as independent and identically distributed Bernoulli random variables with parameter $1/2$. Let $k = m^2$ denote the number of content bits. Define a hiding mechanism where $F$ encodes each content bit $b \\in \\{0,1\\}$ into a real-valued microtexture symbol $s \\in \\mathbb{R}$ using amplitude coding $s = a \\cdot (2b - 1)$, where $a > 0$ is a design amplitude. Model the corruption between $F$ and $G$ as additive Gaussian noise $n \\sim \\mathcal{N}(0, \\sigma^2)$ applied independently per symbol, yielding $z = s + n$. The decoder in $G$ uses a symmetric erasure threshold policy with threshold $\\tau \\ge 0$: if $|z| < \\tau$ the bit is treated as erased; otherwise the bit is decoded by the sign of $z$, that is, $\\hat{b} = \\mathbb{I}[z \\ge 0]$ for $|z| \\ge \\tau$. In case of erasure, let $G$ default to decoding $\\hat{b} = 0$.\n2. Under this channel, derive expressions for the following probabilities per bit in terms of $a$, $\\sigma$, and $\\tau$ using the standard normal cumulative distribution function, denoted $\\Phi$:\n   a. The erasure probability $p_{\\mathrm{erase}}$, that is, the probability that $|z| < \\tau$.\n   b. The misclassification probability when the true bit is $1$, denoted $p_{\\mathrm{mis1}}$, that is, the probability that $z \\le -\\tau$ under $b=1$.\n   c. The misclassification probability when the true bit is $0$, denoted $p_{\\mathrm{mis0}}$, that is, the probability that $z \\ge \\tau$ under $b=0$.\n3. Using the probabilities in item $2$, compute the expected cycle-consistency error per bit, defined as the expected absolute difference $\\mathbb{E}[|b - \\hat{b}|]$ under the described decoding policy and assuming $b$ is uniformly distributed on $\\{0,1\\}$. Express this expected cycle-consistency error in terms of $p_{\\mathrm{erase}}$, $p_{\\mathrm{mis1}}$, and $p_{\\mathrm{mis0}}$, and multiply by $k$ to obtain the expected cycle-consistency error over all $k$ bits of the content region.\n4. Design an information bottleneck penalty using the Mutual Information (MI) between the input content $x$ and the decoded output $G(F(x))$, denoted $I(x; G(F(x)))$. For analytical tractability, approximate the microtexture channel by a Binary Erasure Channel (BEC) whose erasure probability equals the per-bit erasure probability $p_{\\mathrm{erase}}$ computed in item $2$, and whose output symbol retains one bit of information when not erased. Under this BEC approximation and assuming independent bits, compute an estimate of the MI in bits as a function of $k$ and $p_{\\mathrm{erase}}$.\n\nImplement a program that, for each specified parameter setting $(a, \\sigma, \\tau, k)$, computes:\n- The expected cycle-consistency error over $k$ bits as a float.\n- The information bottleneck penalty $I(x;G(F(x)))$ in bits as a float under the BEC approximation.\n\nYour program must use the following test suite and produce the final output exactly in the required format. For each test case, compute the two floats in the order described above, and aggregate them into a single flat list. The test suite parameters are:\n- Case $1$ (happy path): $a = 1.0$, $\\sigma = 0.1$, $\\tau = 0.2$, $k = 256$.\n- Case $2$ (moderate noise): $a = 0.3$, $\\sigma = 0.7$, $\\tau = 0.3$, $k = 256$.\n- Case $3$ (boundary threshold): $\\tau = 0.0$ with $a = 0.5$, $\\sigma = 0.5$, $k = 256$.\n- Case $4$ (edge case, high noise and small amplitude): $a = 0.1$, $\\sigma = 1.2$, $\\tau = 0.4$, $k = 256$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[\\text{error\\_case1}, I\\_\\text{case1}, \\text{error\\_case2}, I\\_\\text{case2}, \\text{error\\_case3}, I\\_\\text{case3}, \\text{error\\_case4}, I\\_\\text{case4}]$.", "solution": "The user has provided a problem concerning a failure mode in CycleGANs, which is to be analyzed using a simplified, mathematically tractable model. The task is to derive and compute two key metrics: the expected cycle-consistency error and an information-theoretic penalty, for several parameter sets. The problem is well-posed, scientifically grounded in probability theory and information theory, and all necessary components for a unique solution are provided.\n\nThe solution proceeds in three stages. First, the probabilities of key events in the defined bit-level communication channel (erasure and misclassification) are derived. Second, these probabilities are used to formulate an expression for the expected cycle-consistency error. Third, an approximation based on a Binary Erasure Channel is used to formulate the information bottleneck penalty.\n\n**1. Derivation of Per-Bit Channel Probabilities**\n\nThe problem defines a communication channel model for a single bit $b \\in \\{0,1\\}$. The bit is encoded via amplitude modulation into a signal $s = a(2b-1)$, where $a>0$. This means if $b=1$, the signal is $s=+a$, and if $b=0$, the signal is $s=-a$. This signal is corrupted by additive white Gaussian noise, $n \\sim \\mathcal{N}(0, \\sigma^2)$, resulting in a received signal $z=s+n$.\n\nLet $\\Phi(\\cdot)$ denote the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$. The CDF of a general normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ at point $x$ is given by $\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$.\n\nWhen $b=1$, the received signal is $z_1 = a+n$, which follows the distribution $\\mathcal{N}(a, \\sigma^2)$.\nWhen $b=0$, the received signal is $z_0 = -a+n$, which follows the distribution $\\mathcal{N}(-a, \\sigma^2)$.\n\na. **Erasure Probability ($p_{\\mathrm{erase}}$)**: An erasure event is defined as $|z| < \\tau$ for a given threshold $\\tau \\ge 0$. The input bit $b$ is a Bernoulli random variable with parameter $1/2$. The total erasure probability is found by the law of total probability:\n$$p_{\\mathrm{erase}} = P(|z| < \\tau) = P(|z| < \\tau | b=0)P(b=0) + P(|z| < \\tau | b=1)P(b=1)$$\nDue to the symmetry of the problem (the distributions for $b=0$ and $b=1$ are reflections of each other about $0$, and the erasure interval $(-\\tau, \\tau)$ is symmetric), the conditional probabilities are equal: $P(|z_0| < \\tau) = P(|z_1| < \\tau)$.\nTherefore, we can compute $p_{\\mathrm{erase}}$ using just one of the conditional probabilities:\n$$p_{\\mathrm{erase}} = P(|z_1| < \\tau) = P(-\\tau < z_1 < \\tau)$$\n$$p_{\\mathrm{erase}} = P(z_1 < \\tau) - P(z_1 \\le -\\tau)$$\nUsing the CDF for $\\mathcal{N}(a, \\sigma^2)$, this becomes:\n$$p_{\\mathrm{erase}} = \\Phi\\left(\\frac{\\tau - a}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right)$$\n\nb. **Misclassification Probabilities ($p_{\\mathrm{mis1}}, p_{\\mathrm{mis0}}$)**: Misclassification occurs when the received signal falls into the wrong decision region, excluding the erasure region.\nFor $b=1$, the true bit is $1$. The decoder outputs $\\hat{b}=0$ if $z_1 \\le -\\tau$. The misclassification probability $p_{\\mathrm{mis1}}$ is:\n$$p_{\\mathrm{mis1}} = P(z_1 \\le -\\tau) = \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right)$$\nFor $b=0$, the true bit is $0$. The decoder outputs $\\hat{b}=1$ if $z_0 \\ge \\tau$. The misclassification probability $p_{\\mathrm{mis0}}$ is:\n$$p_{\\mathrm{mis0}} = P(z_0 \\ge \\tau) = 1 - P(z_0 < \\tau) = 1 - \\Phi\\left(\\frac{\\tau - (-a)}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau+a}{\\sigma}\\right)$$\nUsing the property $\\Phi(-x) = 1 - \\Phi(x)$, this simplifies to:\n$$p_{\\mathrm{mis0}} = \\Phi\\left(-\\frac{a+\\tau}{\\sigma}\\right) = \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right)$$\nAs expected from the problem's symmetry, $p_{\\mathrm{mis0}} = p_{\\mathrm{mis1}}$. We denote this common probability by $p_{\\mathrm{mis}}$.\n\n**2. Derivation of Expected Cycle-Consistency Error**\n\nThe cycle-consistency error for a single bit is defined as $\\mathbb{E}[|b - \\hat{b}|]$. As the error magnitude $|b-\\hat{b}|$ is either $0$ or $1$, this expectation is equal to the total probability of error, $P(b \\neq \\hat{b})$.\n$$P(b \\neq \\hat{b}) = P(\\hat{b} \\neq b | b=0)P(b=0) + P(\\hat{b} \\neq b | b=1)P(b=1)$$\nWith $P(b=0) = P(b=1) = 1/2$:\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2} P(\\hat{b}=1|b=0) + \\frac{1}{2} P(\\hat{b}=0|b=1)$$\n- If $b=0$, an error occurs if $\\hat{b}=1$. This happens if $z_0 \\ge \\tau$, with probability $p_{\\mathrm{mis0}} = p_{\\mathrm{mis}}$.\n- If $b=1$, an error occurs if $\\hat{b}=0$. This can happen in two disjoint ways: either the bit is misclassified ($z_1 \\le -\\tau$), or it is erased ($|z_1|<\\tau$) and defaults to $\\hat{b}=0$.\n$$P(\\hat{b}=0|b=1) = P(z_1 \\le -\\tau) + P(|z_1| < \\tau) = p_{\\mathrm{mis1}} + p_{\\mathrm{erase}} = p_{\\mathrm{mis}} + p_{\\mathrm{erase}}$$\nCombining these, the per-bit expected error is:\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2} p_{\\mathrm{mis}} + \\frac{1}{2} (p_{\\mathrm{mis}} + p_{\\mathrm{erase}}) = p_{\\mathrm{mis}} + \\frac{1}{2} p_{\\mathrm{erase}}$$\nSubstituting the derived expressions for $p_{\\mathrm{mis}}$ and $p_{\\mathrm{erase}}$:\n$$\\mathbb{E}[|b - \\hat{b}|] = \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) + \\frac{1}{2}\\left[ \\Phi\\left(\\frac{\\tau-a}{\\sigma}\\right) - \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) \\right]$$\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2}\\left[ \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) + \\Phi\\left(\\frac{\\tau-a}{\\sigma}\\right) \\right]$$\nThe total expected cycle-consistency error over $k$ independent bits is simply $k$ times the per-bit error.\n\n**3. Derivation of the Information Bottleneck Penalty**\n\nThe penalty is defined as the mutual information (MI) $I(x; G(F(x)))$, where $x$ is the input content and $G(F(x))$ is the reconstructed content. As the input bits are i.i.d. and the channel acts independently on each bit, the total MI is $k$ times the per-bit MI: $I(x; G(F(x))) = k \\cdot I(b; \\hat{b})$.\n\nThe problem specifies approximating the channel as a Binary Erasure Channel (BEC) with an erasure probability $p_e$ equal to our derived $p_{\\mathrm{erase}}$. In a BEC, a bit is either transmitted correctly with probability $1-p_e$ or erased with probability $p_e$. The MI for a BEC with a uniform binary input $b$ is given by $I(b; \\hat{b}) = 1 - p_e$ bits.\n\nThus, the information bottleneck penalty is approximated as:\n$$I(x; G(F(x))) \\approx k \\cdot (1 - p_{\\mathrm{erase}})$$\nSubstituting the expression for $p_{\\mathrm{erase}}$:\n$$I(x; G(F(x))) \\approx k \\cdot \\left(1 - \\left[ \\Phi\\left(\\frac{\\tau - a}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right) \\right]\\right)$$\n\n**Summary of Formulas for Implementation**\nFor each parameter set $(a, \\sigma, \\tau, k)$:\n1.  Calculate argument values: $v_1 = \\frac{-a-\\tau}{\\sigma}$ and $v_2 = \\frac{\\tau-a}{\\sigma}$.\n2.  Compute CDF values: $\\phi_1 = \\Phi(v_1)$ and $\\phi_2 = \\Phi(v_2)$.\n3.  Compute total expected error: $E = k \\cdot \\frac{1}{2}(\\phi_1 + \\phi_2)$.\n4.  Compute erasure probability: $p_{\\mathrm{erase}} = \\phi_2 - \\phi_1$.\n5.  Compute information penalty: $I = k \\cdot (1 - p_{\\mathrm{erase}})$.\nThese formulas will be implemented in the final program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the expected cycle-consistency error and mutual information penalty\n    for a toy model of a CycleGAN failure mode.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, sigma, tau, k)\n    test_cases = [\n        (1.0, 0.1, 0.2, 256),  # Case 1 (happy path)\n        (0.3, 0.7, 0.3, 256),  # Case 2 (moderate noise)\n        (0.5, 0.5, 0.0, 256),  # Case 3 (boundary threshold)\n        (0.1, 1.2, 0.4, 256),  # Case 4 (edge case, high noise and small amplitude)\n    ]\n\n    results = []\n    for case in test_cases:\n        a, sigma, tau, k = case\n\n        # 1. Calculate argument values for the standard normal CDF (Phi).\n        # These correspond to the normalized thresholds.\n        # arg_mis: argument for p_mis calculation\n        # arg_erase_upper: upper bound for p_erase calculation\n        arg_mis = (-a - tau) / sigma\n        arg_erase_upper = (tau - a) / sigma\n\n        # 2. Compute the required CDF values using scipy.stats.norm.cdf.\n        phi_mis = norm.cdf(arg_mis)\n        phi_erase_upper = norm.cdf(arg_erase_upper)\n\n        # 3. Compute the per-bit probabilities.\n        # p_mis is the probability of misclassifying a bit (0->1 or 1->0) without erasure.\n        # The problem shows p_mis0 = p_mis1, so we use a single p_mis.\n        p_mis = phi_mis\n        \n        # p_erase is the probability of a bit being erased (decoded value defaults to 0).\n        # p_erase = Phi((tau-a)/sigma) - Phi((-tau-a)/sigma)\n        p_erase = phi_erase_upper - phi_mis\n\n        # 4. Compute the expected cycle-consistency error over k bits.\n        # Per-bit error is p_mis + 0.5 * p_erase, which simplifies to\n        # 0.5 * [Phi((-a-tau)/sigma) + Phi((tau-a)/sigma)].\n        total_expected_error = k * 0.5 * (phi_mis + phi_erase_upper)\n\n        # 5. Compute the information bottleneck penalty (Mutual Information) in bits.\n        # This is approximated by modeling the channel as a Binary Erasure Channel (BEC)\n        # with erasure probability p_erase. The MI for k bits is k * (1 - p_erase).\n        mi_penalty = k * (1.0 - p_erase)\n\n        results.append(total_expected_error)\n        results.append(mi_penalty)\n\n    # Final print statement in the exact required format.\n    # Produces a single line of output: [error1,I1,error2,I2,...]\n    formatted_results = [f\"{r:.16f}\".rstrip('0').rstrip('.') if 'e' not in f\"{r:.16f}\" else f\"{r:.16e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3127696"}]}