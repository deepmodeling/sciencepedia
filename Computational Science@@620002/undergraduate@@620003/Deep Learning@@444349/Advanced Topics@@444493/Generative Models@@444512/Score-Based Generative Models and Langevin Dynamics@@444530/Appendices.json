{"hands_on_practices": [{"introduction": "Before we can build complex generative models, it's crucial to understand the score function in a simple, analytically tractable case. This first exercise [@problem_id:3172987] grounds the abstract definition of the score, $s(x) = \\nabla_{x} \\ln p(x)$, by applying it to the multivariate Gaussian distribution. By deriving the score for this fundamental distribution, you will see that it takes a surprisingly simple linear form, providing a powerful intuition for how score functions guide samples toward regions of high probability.", "problem": "Consider a $d$-dimensional random variable $x \\in \\mathbb{R}^{d}$ distributed according to a multivariate Gaussian with mean $\\mu \\in \\mathbb{R}^{d}$ and strictly positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. The probability density function is\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right).\n$$\nIn score-based generative models, the score function is defined as $s(x) = \\nabla_{x} \\ln p(x)$. Starting from the provided density and the definition of the score function, derive an explicit analytic expression for $s(x)$ in terms of $\\mu$ and $\\Sigma$. Then briefly explain, using the overdamped Langevin dynamics stochastic differential equation (SDE) $dX_{t} = s(X_{t})\\, dt + \\sqrt{2}\\, dW_{t}$ (where $W_{t}$ is a standard $d$-dimensional Wiener process), why the deterministic drift is an affine function of $x$ and identify the point where this drift vanishes.\n\nProvide your final result by giving the closed-form expression for $s(x)$ only. No numerical rounding is required, and no physical units are involved.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides all necessary definitions and constraints to derive a unique and meaningful solution within the standard mathematical framework of probability theory and stochastic processes.\n\nThe starting point is the probability density function (PDF) of a $d$-dimensional multivariate Gaussian distribution for a random variable $x \\in \\mathbb{R}^{d}$ with mean $\\mu \\in \\mathbb{R}^{d}$ and a strictly positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$:\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right)\n$$\nThe score function, denoted $s(x)$, is defined as the gradient of the logarithm of the probability density function with respect to $x$:\n$$\ns(x) = \\nabla_{x} \\ln p(x)\n$$\nTo derive the expression for $s(x)$, we first compute the natural logarithm of $p(x)$:\n$$\n\\ln p(x) = \\ln \\left( \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right) \\right)\n$$\nUsing the properties of the logarithm, specifically $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ and $\\ln(\\exp(c)) = c$, we can separate the terms:\n$$\n\\ln p(x) = \\ln \\left( \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\right) + \\ln \\left( \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right) \\right)\n$$\n$$\n\\ln p(x) = -\\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(|\\Sigma|) - \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\n$$\nThe first two terms, $-\\frac{d}{2} \\ln(2\\pi)$ and $-\\frac{1}{2} \\ln(|\\Sigma|)$, are constant with respect to the variable $x$. Therefore, their gradient is zero. We can now compute the score function $s(x)$ by taking the gradient of $\\ln p(x)$:\n$$\ns(x) = \\nabla_{x} \\left( -\\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(|\\Sigma|) - \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) \\right)\n$$\n$$\ns(x) = 0 - 0 - \\frac{1}{2} \\nabla_{x} \\left( (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) \\right)\n$$\nTo evaluate the gradient of the quadratic form, we use a standard result from matrix calculus. For a vector variable $z$ and a symmetric matrix $A$, the gradient is given by $\\nabla_{z} (z^{\\top} A z) = 2 A z$. In our case, the variable is $x$, the vector corresponding to $z$ is $(x - \\mu)$, and the matrix corresponding to $A$ is $\\Sigma^{-1}$. The covariance matrix $\\Sigma$ is symmetric by definition, and the inverse of a symmetric matrix is also symmetric, so $\\Sigma^{-1}$ is symmetric. Applying the chain rule, we have:\n$$\n\\nabla_{x} \\left( (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) \\right) = 2 \\Sigma^{-1} (x - \\mu)\n$$\nSubstituting this result back into the expression for $s(x)$:\n$$\ns(x) = -\\frac{1}{2} \\left( 2 \\Sigma^{-1} (x - \\mu) \\right) = - \\Sigma^{-1} (x - \\mu)\n$$\nThis is the explicit analytic expression for the score function of a multivariate Gaussian distribution.\n\nThe problem further asks to explain why the deterministic drift in the overdamped Langevin dynamics SDE is an affine function of $x$ and to identify where it vanishes. The SDE is given by:\n$$\ndX_{t} = s(X_{t})\\, dt + \\sqrt{2}\\, dW_{t}\n$$\nThe deterministic drift term is $s(X_{t})$. Based on our derived expression for the score, we can write this as:\n$$\ns(X_t) = -\\Sigma^{-1}(X_t - \\mu) = -\\Sigma^{-1}X_t + \\Sigma^{-1}\\mu\n$$\nThis expression is in the form $A x + b$, where $A = -\\Sigma^{-1}$ is a $d \\times d$ matrix and $b = \\Sigma^{-1}\\mu$ is a $d \\times 1$ vector. A function of this form is, by definition, an affine transformation. Therefore, the deterministic drift is an affine function of the state $X_t$.\n\nTo find the point where the drift vanishes, we set $s(x) = 0$:\n$$\n- \\Sigma^{-1} (x - \\mu) = 0\n$$\nSince $\\Sigma$ is a strictly positive-definite matrix, it is invertible, and its inverse $\\Sigma^{-1}$ is also invertible. We can multiply both sides of the equation by $-\\Sigma$ from the left:\n$$\n(-\\Sigma)(-\\Sigma^{-1}) (x - \\mu) = (-\\Sigma) 0\n$$\n$$\nI (x - \\mu) = 0\n$$\nwhere $I$ is the $d \\times d$ identity matrix. This simplifies to:\n$$\nx - \\mu = 0 \\implies x = \\mu\n$$\nThus, the drift vanishes at $x = \\mu$, which is the mean of the Gaussian distribution. This is expected, as the score function points in the direction of the steepest ascent of the log-probability, and the gradient must be zero at the mode of the distribution, which for a Gaussian is its mean.", "answer": "$$\\boxed{-\\Sigma^{-1}(x - \\mu)}$$", "id": "3172987"}, {"introduction": "Langevin dynamics provides a continuous-time recipe for sampling, but in practice, we must use discrete time steps to simulate the process on a computer. This exercise [@problem_id:3172952] delves into this crucial discretization step, comparing the simplest method, Euler–Maruyama, with a more sophisticated Predictor-Corrector scheme. By analyzing how each method approximates the true stationary distribution, you will gain insight into the inherent trade-offs between computational simplicity, bias, and variance in Langevin samplers.", "problem": "You must write a complete, runnable program that compares a simple Euler–Maruyama sampler to a predictor–corrector sampler for a one-dimensional score-based process. The comparison must be based on the bias–variance tradeoff for estimating the second moment of the stationary distribution generated by each sampler. Begin from the following fundamental bases and definitions and do not assume any shortcut formulas.\n\nConsider a one-dimensional stochastic differential equation (SDE) driven by a model score function in score-based generative modeling. Let the score function be $s_{\\theta}(x) = -\\left(1+\\delta\\right)x$, where $\\delta$ is a scalar parameter encoding model mismatch relative to the true score of a standard normal target. The SDE is\n$$\n\\mathrm{d}X_t = s_{\\theta}(X_t) \\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Wiener process.\n\nDiscretize the SDE with time step $h$ using the Euler–Maruyama method (EM), which produces updates of the form\n$$\nX_{k+1} = X_k + h\\, s_{\\theta}(X_k) + \\sqrt{2h}\\, Z_k,\n$$\nwhere $Z_k \\sim \\mathcal{N}(0,1)$ are independent standard normal random variables.\n\nDefine a predictor–corrector (PC) scheme that first applies the Euler–Maruyama predictor as above to obtain an intermediate $X_{k+1}^{(\\text{pred})}$, and then applies a deterministic corrector step using the same score function with step $c$:\n$$\nX_{k+1} = X_{k+1}^{(\\text{pred})} + c\\, s_{\\theta}\\!\\left(X_{k+1}^{(\\text{pred})}\\right).\n$$\nThis PC scheme combines a stochastic predictor with a deterministic drift corrector based on $s_{\\theta}$.\n\nThe target second moment of the true distribution is that of a standard normal variable, namely $E[X^2] = 1$. For each sampler (EM and PC), assume the number of steps $T$ is sufficiently large for the chain to reach stationarity. Focusing on the second moment $E[X^2]$ under the stationary distribution induced by each scheme, compute:\n- The absolute bias, defined as $\\left| E[X^2] - 1 \\right|$.\n- The variance of the Monte Carlo estimator of $E[X^2]$ formed by the average of $M$ independent samples from the stationary distribution. Under the stationary distribution, for $X \\sim \\mathcal{N}(0, V)$, use the fact that $E[X^2] = V$ and $E[X^4] = 3V^2$ to determine the estimator variance of the sample mean of $X^2$ across $M$ independent samples.\n\nYour program must compute these quantities for both EM and PC for each test case specified below. Use the independence idealization at stationarity to express the estimator variance as a closed-form function of the stationary variance $V$ and the sample size $M$.\n\nTest suite (each test case is a tuple $(\\delta, h, T, c)$):\n1. $(0,\\, 0.05,\\, 400,\\, 0)$: baseline with no corrector (predictor–corrector reduces to Euler–Maruyama).\n2. $(0,\\, 0.05,\\, 400,\\, 0.005)$: corrector with small deterministic drift, matched score.\n3. $(0.1,\\, 0.05,\\, 400,\\, 0.005)$: positive model mismatch, same steps.\n4. $(-0.2,\\, 0.1,\\, 400,\\, 0.1)$: negative model mismatch, larger steps.\n\nFor the estimator variance calculation, use $M = 10000$ independent samples per method per test case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, append four floating-point numbers in this order: $\\left[\\text{bias}_{\\text{EM}}, \\text{var}_{\\text{EM}}, \\text{bias}_{\\text{PC}}, \\text{var}_{\\text{PC}}\\right]$.\n- Concatenate the results for all test cases into one list, preserving the order of the test suite as listed above. For example, a two-case output would look like $\\left[\\text{b}_{1,\\text{EM}}, \\text{v}_{1,\\text{EM}}, \\text{b}_{1,\\text{PC}}, \\text{v}_{1,\\text{PC}}, \\text{b}_{2,\\text{EM}}, \\text{v}_{2,\\text{EM}}, \\text{b}_{2,\\text{PC}}, \\text{v}_{2,\\text{PC}}\\right]$.\n\nYour program must be self-contained, require no user input, and adhere to the execution environment constraints.", "solution": "The problem requires a comparison of the Euler–Maruyama (EM) and a predictor–corrector (PC) sampler for a one-dimensional Ornstein–Uhlenbeck-type stochastic differential equation (SDE) arising in score-based generative modeling. The comparison is based on the bias and variance for estimating the second moment of the stationary distribution generated by each sampler. We will first derive analytical expressions for the stationary variance of each numerical scheme.\n\nThe SDE is given by:\n$$\n\\mathrm{d}X_t = s_{\\theta}(X_t) \\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}W_t\n$$\nwith the score function $s_{\\theta}(x) = -(1+\\delta)x$. Substituting the score function, we get a linear SDE:\n$$\n\\mathrm{d}X_t = -(1+\\delta)X_t \\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}W_t\n$$\nThe stationary distribution of this continuous-time process is a zero-mean normal distribution, $\\mathcal{N}(0, V_{true})$, where the variance $V_{true}$ satisfies the fluctuation-dissipation relation $-(1+\\delta)V_{true} + 1 = 0$, which gives $V_{true} = 1/(1+\\delta)$. The problem, however, specifies the target second moment as $E[X^2] = 1$, corresponding to a standard normal distribution (i.e., the case where $\\delta=0$ in the underlying SDE). The bias will be measured against this target value of $1$.\n\nBoth discretization schemes are linear, of the general form $X_{k+1} = A X_k + B Z_k$, where $Z_k \\sim \\mathcal{N}(0,1)$. This is an autoregressive process of order 1 (AR(1)). If $|A|<1$, the process has a unique stationary distribution. Since the innovation $Z_k$ is Gaussian, the stationary distribution is also Gaussian with mean zero, $X \\sim \\mathcal{N}(0, V)$. The variance $V$ can be found by equating the variance of both sides at stationarity:\n$$\n\\mathrm{Var}(X_{k+1}) = \\mathrm{Var}(A X_k + B Z_k)\n$$\nSince $X_k$ and $Z_k$ are independent,\n$$\nV = A^2 \\mathrm{Var}(X_k) + B^2 \\mathrm{Var}(Z_k) = A^2 V + B^2\n$$\nSolving for $V$, we get the stationary variance:\n$$\nV = \\frac{B^2}{1 - A^2}\n$$\nThis formula is the cornerstone of our analysis.\n\nFor each sampler, we first find its stationary variance $V$. The quantities of interest are then derived from $V$:\n1.  The absolute bias of the second moment estimator: $b = |E[X^2] - 1| = |V-1|$.\n2.  The variance of the Monte Carlo estimator for $E[X^2]$. The estimator is the sample mean $\\hat{E}[X^2] = \\frac{1}{M}\\sum_{i=1}^M X_i^2$, where $X_i \\sim \\mathcal{N}(0, V)$ are $M$ independent samples from the stationary distribution. The variance of this estimator is:\n    $$\n    \\mathrm{Var}(\\hat{E}[X^2]) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{i=1}^M X_i^2\\right) = \\frac{1}{M}\\mathrm{Var}(X^2)\n    $$\n    For $X \\sim \\mathcal{N}(0, V)$, we are given $E[X^4] = 3V^2$. Thus:\n    $$\n    \\mathrm{Var}(X^2) = E[(X^2)^2] - (E[X^2])^2 = E[X^4] - V^2 = 3V^2 - V^2 = 2V^2\n    $$\n    The estimator variance is therefore $v = \\frac{2V^2}{M}$.\n\nNow, we apply this framework to each sampler.\n\n**1. Euler–Maruyama (EM) Sampler**\n\nThe update rule is:\n$$\nX_{k+1} = X_k + h s_{\\theta}(X_k) + \\sqrt{2h} Z_k\n$$\nSubstituting $s_{\\theta}(X_k) = -(1+\\delta)X_k$:\n$$\nX_{k+1} = X_k - h(1+\\delta)X_k + \\sqrt{2h} Z_k = \\left(1 - h(1+\\delta)\\right)X_k + \\sqrt{2h} Z_k\n$$\nThis is an AR(1) process with coefficients:\n$$\nA_{\\text{EM}} = 1 - h(1+\\delta) \\quad \\text{and} \\quad B_{\\text{EM}} = \\sqrt{2h}\n$$\nThe stationary variance $V_{\\text{EM}}$ is:\n$$\nV_{\\text{EM}} = \\frac{B_{\\text{EM}}^2}{1-A_{\\text{EM}}^2} = \\frac{(\\sqrt{2h})^2}{1 - (1 - h(1+\\delta))^2} = \\frac{2h}{1 - (1 - 2h(1+\\delta) + h^2(1+\\delta)^2)} = \\frac{2h}{2h(1+\\delta) - h^2(1+\\delta)^2}\n$$\n$$\nV_{\\text{EM}} = \\frac{2}{(1+\\delta)(2 - h(1+\\delta))}\n$$\nThe bias and estimator variance for the EM sampler are:\n$$\n\\text{bias}_{\\text{EM}} = |V_{\\text{EM}} - 1| \\quad \\text{and} \\quad \\text{var}_{\\text{EM}} = \\frac{2V_{\\text{EM}}^2}{M}\n$$\n\n**2. Predictor-Corrector (PC) Sampler**\n\nThe PC scheme consists of a predictor step followed by a corrector step.\nPredictor:\n$$\nX_{k+1}^{(\\text{pred})} = X_k + h s_{\\theta}(X_k) + \\sqrt{2h} Z_k = (1 - h(1+\\delta))X_k + \\sqrt{2h} Z_k\n$$\nCorrector:\n$$\nX_{k+1} = X_{k+1}^{(\\text{pred})} + c s_{\\theta}(X_{k+1}^{(\\text{pred})}) = X_{k+1}^{(\\text{pred})} - c(1+\\delta)X_{k+1}^{(\\text{pred})} = (1 - c(1+\\delta))X_{k+1}^{(\\text{pred})}\n$$\nCombining these two steps, we express $X_{k+1}$ directly in terms of $X_k$:\n$$\nX_{k+1} = (1 - c(1+\\delta)) \\left[ (1 - h(1+\\delta))X_k + \\sqrt{2h}Z_k \\right]\n$$\n$$\nX_{k+1} = (1 - c(1+\\delta))(1 - h(1+\\delta)) X_k + (1 - c(1+\\delta))\\sqrt{2h} Z_k\n$$\nThis is again an AR(1) process with coefficients:\n$$\nA_{\\text{PC}} = (1 - c(1+\\delta))(1 - h(1+\\delta)) \\quad \\text{and} \\quad B_{\\text{PC}} = (1 - c(1+\\delta))\\sqrt{2h}\n$$\nThe stationary variance $V_{\\text{PC}}$ is:\n$$\nV_{\\text{PC}} = \\frac{B_{\\text{PC}}^2}{1-A_{\\text{PC}}^2} = \\frac{\\left((1 - c(1+\\delta))\\sqrt{2h}\\right)^2}{1 - \\left((1 - c(1+\\delta))(1 - h(1+\\delta))\\right)^2}\n$$\n$$\nV_{\\text{PC}} = \\frac{(1 - c(1+\\delta))^2 (2h)}{1 - (1 - c(1+\\delta))^2 (1 - h(1+\\delta))^2}\n$$\nThe bias and estimator variance for the PC sampler are:\n$$\n\\text{bias}_{\\text{PC}} = |V_{\\text{PC}} - 1| \\quad \\text{and} \\quad \\text{var}_{\\text{PC}} = \\frac{2V_{\\text{PC}}^2}{M}\n$$\nNote that if $c=0$, $A_{\\text{PC}}=A_{\\text{EM}}$ and $B_{\\text{PC}}=B_{\\text{EM}}$, so $V_{\\text{PC}} = V_{\\text{EM}}$. This is consistent, as a zero-step corrector makes the PC scheme identical to the EM scheme. The program will implement these derived formulas to compute the required quantities for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias-variance tradeoff for EM and PC samplers for a 1D SDE.\n\n    The solution is based on the analytical derivation of the stationary variance\n    for the discretized processes, which are AR(1) models.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is (delta, h, T, c). The parameter T is not used in the\n    # analytical stationary-state calculation.\n    test_cases = [\n        (0.0, 0.05, 400, 0.0),\n        (0.0, 0.05, 400, 0.005),\n        (0.1, 0.05, 400, 0.005),\n        (-0.2, 0.1, 400, 0.1),\n    ]\n\n    # Sample size for Monte Carlo estimator variance calculation.\n    M = 10000.0\n\n    results = []\n    for delta, h, T, c in test_cases:\n        # Common term in the score function s_theta(x) = -(1 + delta) * x\n        one_plus_delta = 1.0 + delta\n\n        # --- Euler-Maruyama (EM) Analysis ---\n        \n        # The EM update is X_{k+1} = (1 - h*(1+delta)) * X_k + sqrt(2*h) * Z_k\n        # This is an AR(1) process X_{k+1} = A*X_k + B*Z_k\n        # The stationary variance is V_EM = B^2 / (1 - A^2)\n        \n        # A_em = 1.0 - h * one_plus_delta\n        # B_em_sq = 2.0 * h\n        # v_em = B_em_sq / (1.0 - A_em_sq**2)\n        \n        # Using an expanded form for better numerical stability with small h\n        v_em_numerator = 2.0 * h\n        v_em_denominator = (2.0 * h * one_plus_delta) - (h**2 * one_plus_delta**2)\n\n        # Ensure stability by checking denominator > 0, which corresponds to\n        # 0 < h*(1+delta) < 2\n        if v_em_denominator <= 0:\n            # This case won't be hit with the given test values.\n            v_em = float('inf')\n        else:\n            v_em = v_em_numerator / v_em_denominator\n        \n        # Absolute bias: |E[X^2] - 1| = |V_em - 1|\n        bias_em = abs(v_em - 1.0)\n        \n        # Estimator variance: Var([sum X_i^2]/M) = Var(X^2) / M = 2*V_em^2/M\n        var_em_estimator = (2.0 * v_em**2) / M\n\n        # --- Predictor-Corrector (PC) Analysis ---\n\n        # The PC update is X_{k+1} = (1-c*(1+d)) * X_{k+1_pred}\n        # which results in X_{k+1} = A_pc * X_k + B_pc * Z_k\n        \n        # Coefficients for the AR(1) form of the PC scheme\n        alpha_h = 1.0 - h * one_plus_delta\n        alpha_c = 1.0 - c * one_plus_delta\n        \n        A_pc = alpha_c * alpha_h\n        B_pc_sq = (alpha_c**2) * (2.0 * h)\n        \n        v_pc_numerator = B_pc_sq\n        v_pc_denominator = 1.0 - A_pc**2\n        \n        # Ensure stability by checking |A_pc| < 1\n        if v_pc_denominator <= 0:\n            # This case won't be hit with the given test values.\n            v_pc = float('inf')\n        else:\n            v_pc = v_pc_numerator / v_pc_denominator\n\n        # Absolute bias: |E[X^2] - 1| = |V_pc - 1|\n        bias_pc = abs(v_pc - 1.0)\n        \n        # Estimator variance: 2*V_pc^2/M\n        var_pc_estimator = (2.0 * v_pc**2) / M\n\n        results.extend([bias_em, var_em_estimator, bias_pc, var_pc_estimator])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3172952"}, {"introduction": "A true score function is not just any vector field; it is the gradient of a scalar potential, $\\ln p(x)$. A key consequence of this property, drawn from vector calculus, is that the field must be \"conservative,\" meaning the line integral of the score between two points is independent of the path taken. This final practice [@problem_id:3172994] provides a hands-on method to test this property, showing how a non-conservative component added to the score field breaks path independence, which has profound implications for the validity and performance of a trained score model.", "problem": "Consider a two-dimensional density $p(\\mathbf{x})$ on $\\mathbb{R}^2$ and a score-based generative model that produces a parametric score field $s_\\theta(\\mathbf{x})$. The score field is intended to approximate the true score $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$. In overdamped Langevin dynamics, which is a Stochastic Differential Equation (SDE) used for sampling, the continuous-time update is $\\mathrm{d}\\mathbf{x}_t = \\tfrac{1}{2}s_\\theta(\\mathbf{x}_t)\\,\\mathrm{d}t + \\mathrm{d}\\mathbf{W}_t$, where $\\mathbf{W}_t$ is a standard Wiener process. If $s_\\theta(\\mathbf{x})$ equals the true gradient field, then by the fundamental theorem for line integrals, the difference $\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)$ can be computed as a path integral of $s_\\theta$ between $\\mathbf{x}_0$ and $\\mathbf{x}$. Deviations from a conservative (gradient) field can be detected by the path dependence of such line integrals.\n\nYour task is to implement a program that:\n- Defines a scientifically plausible target distribution $p(\\mathbf{x})$ as a mixture of two Gaussians in $d = 2$ dimensions with equal weights. Use\n$$\np(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathcal{N}\\!\\left(\\mathbf{x}\\mid\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1\\right) + \\tfrac{1}{2}\\,\\mathcal{N}\\!\\left(\\mathbf{x}\\mid\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2\\right),\n$$\nwhere $\\boldsymbol{\\mu}_1 = [0,0]^\\top$, $\\boldsymbol{\\mu}_2 = [2,-1]^\\top$, $\\boldsymbol{\\Sigma}_1 = \\begin{bmatrix}1.0 & 0.3 \\\\ 0.3 & 0.5\\end{bmatrix}$, and $\\boldsymbol{\\Sigma}_2 = \\begin{bmatrix}0.9 & -0.2 \\\\ -0.2 & 0.8\\end{bmatrix}$.\n- Computes the true score $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$ from first principles using $s(\\mathbf{x}) = \\left(\\nabla_{\\mathbf{x}} p(\\mathbf{x})\\right)/p(\\mathbf{x})$ and the identity $\\nabla_{\\mathbf{x}} \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = -\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\,\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$.\n- Constructs a parametric score field $s_\\theta(\\mathbf{x})$ that includes a controlled non-conservative component:\n$$\ns_\\theta(\\mathbf{x}) = s(\\mathbf{x}) + \\varepsilon\\,\\mathbf{R}(\\mathbf{x}),\n$$\nwhere $\\varepsilon \\ge 0$ is a scalar parameter and $\\mathbf{R}(\\mathbf{x})$ is a rotational field in $d=2$ defined by\n$$\n\\mathbf{R}(\\mathbf{x}) = \\frac{\\mathbf{Q}\\,(\\mathbf{x} - \\mathbf{c})}{1 + \\lVert \\mathbf{x} - \\mathbf{c} \\rVert^2}, \\quad \\mathbf{Q} = \\begin{bmatrix}0 & -1 \\\\ 1 & 0\\end{bmatrix}, \\quad \\mathbf{c} = [0.5,-0.5]^\\top.\n$$\nNote that $\\mathbf{Q}$ rotates vectors by $90^\\circ$; $\\mathbf{R}(\\mathbf{x})$ has nonzero curl and thus is not the gradient of any scalar potential.\n- Estimates $\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)$ by numerically integrating $s_\\theta(\\mathbf{x})$ along two paths that share endpoints:\n    1. A straight path\n    $$\n    \\boldsymbol{\\gamma}_{\\text{straight}}(t) = \\mathbf{x}_0 + t\\,(\\mathbf{x} - \\mathbf{x}_0), \\quad t \\in [0,1],\n    $$\n    with $\\boldsymbol{\\gamma}'_{\\text{straight}}(t) = \\mathbf{x} - \\mathbf{x}_0$.\n    2. A curved path with a sinusoidal detour orthogonal to the displacement:\n    $$\n    \\boldsymbol{\\gamma}_{\\text{curved}}(t) = \\mathbf{x}_0 + t\\,\\mathbf{d} + a\\,\\sin(\\pi t)\\,\\hat{\\mathbf{n}}, \\quad t \\in [0,1],\n    $$\n    where $\\mathbf{d} = \\mathbf{x} - \\mathbf{x}_0$, $\\hat{\\mathbf{n}} = \\frac{\\mathbf{Q}\\,\\mathbf{d}}{\\lVert \\mathbf{d} \\rVert}$ is the unit normal obtained by rotating $\\mathbf{d}$ by $90^\\circ$, and $a \\ge 0$ controls detour amplitude. The path derivative is\n    $$\n    \\boldsymbol{\\gamma}'_{\\text{curved}}(t) = \\mathbf{d} + a\\,\\pi\\,\\cos(\\pi t)\\,\\hat{\\mathbf{n}}.\n    $$\n- Uses a uniform discretization of $t \\in [0,1]$ with $N$ steps and the composite trapezoidal rule to approximate the line integrals\n$$\nI_{\\text{path}} \\approx \\int_{0}^{1} s_\\theta(\\boldsymbol{\\gamma}(t)) \\cdot \\boldsymbol{\\gamma}'(t)\\,\\mathrm{d}t.\n$$\nIf $\\lVert \\mathbf{d} \\rVert = 0$, both integrals must be reported as $0$ to satisfy the fundamental theorem under zero displacement.\n\nThen, for each test case in the suite below, compute two quantities:\n- $D = \\left\\lvert I_{\\text{straight}} - I_{\\text{curved}} \\right\\rvert$, which quantifies path dependence and thus non-conservativity of $s_\\theta$.\n- $E = \\left\\lvert I_{\\text{straight}} - \\left(\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)\\right) \\right\\rvert$, which measures the absolute error of the straight-path estimate relative to the true log-density difference.\n\nUse $N = 4096$ discretization steps. For numerical stability in computing $\\log p(\\mathbf{x})$, use a mathematically sound log-sum-exp consolidation of the mixture components. Round each reported float to $6$ decimal places.\n\nTest suite:\n- Case $1$: $\\mathbf{x}_0 = [0,0]^\\top$, $\\mathbf{x} = [2,1]^\\top$, $\\varepsilon = 0.0$, $a = 0.5$.\n- Case $2$: $\\mathbf{x}_0 = [0,0]^\\top$, $\\mathbf{x} = [2,1]^\\top$, $\\varepsilon = 0.05$, $a = 0.5$.\n- Case $3$: $\\mathbf{x}_0 = [0,0]^\\top$, $\\mathbf{x} = [2,1]^\\top$, $\\varepsilon = 0.2$, $a = 0.5$.\n- Case $4$ (boundary): $\\mathbf{x}_0 = [1,-1]^\\top$, $\\mathbf{x} = [1,-1]^\\top$, $\\varepsilon = 0.25$, $a = 1.0$.\n- Case $5$ (high curvature): $\\mathbf{x}_0 = [0,-2]^\\top$, $\\mathbf{x} = [3,1]^\\top$, $\\varepsilon = 0.1$, $a = 2.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, one inner list per test case in order. Each inner list must be of the form $[D,E]$ with both entries rounded to $6$ decimal places. For example: $[[d_1,e_1],[d_2,e_2],\\dots]$.", "solution": "The problem requires an analysis of a parametric score field $s_\\theta(\\mathbf{x})$ used in score-based generative models. A key property of the true score field, $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$, is that it is a conservative vector field, meaning its line integral between two points is path-independent. An approximate score field $s_\\theta(\\mathbf{x})$ may fail to be conservative, introducing errors in sampling algorithms like Langevin dynamics. This exercise quantifies the effects of a non-conservative perturbation on line integrals of the score.\n\nThe solution proceeds in several steps:\n1.  Define the target probability density $p(\\mathbf{x})$ and derive expressions for its logarithm, $\\log p(\\mathbf{x})$, and its true score, $s(\\mathbf{x})$.\n2.  Define the non-conservative parametric score field $s_\\theta(\\mathbf{x})$.\n3.  Define the two integration paths, $\\boldsymbol{\\gamma}_{\\text{straight}}(t)$ and $\\boldsymbol{\\gamma}_{\\text{curved}}(t)$, and their derivatives.\n4.  Formulate the numerical line integral using the composite trapezoidal rule.\n5.  Define the metrics $D$ (path dependence) and $E$ (estimation error).\n\n**1. Target Density and True Score**\n\nThe target density is a Gaussian Mixture Model (GMM) in $d=2$ dimensions:\n$$\np(\\mathbf{x}) = \\frac{1}{2}\\,\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1) + \\frac{1}{2}\\,\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)\n$$\nwith parameters $\\boldsymbol{\\mu}_1 = [0,0]^\\top$, $\\boldsymbol{\\mu}_2 = [2,-1]^\\top$, $\\boldsymbol{\\Sigma}_1 = \\begin{bmatrix}1.0 & 0.3 \\\\ 0.3 & 0.5\\end{bmatrix}$, and $\\boldsymbol{\\Sigma}_2 = \\begin{bmatrix}0.9 & -0.2 \\\\ -0.2 & 0.8\\end{bmatrix}$.\n\nTo compute the logarithm $\\log p(\\mathbf{x})$ stably, we use the log-sum-exp (LSE) identity. Letting $p_i(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)$, we have:\n$$\n\\log p(\\mathbf{x}) = \\log\\left(\\frac{1}{2}\\left(e^{\\log p_1(\\mathbf{x})} + e^{\\log p_2(\\mathbf{x})}\\right)\\right) = -\\log(2) + \\text{LSE}(\\log p_1(\\mathbf{x}), \\log p_2(\\mathbf{x}))\n$$\nThe log-density of a multivariate normal distribution is given by:\n$$\n\\log \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\det(\\boldsymbol{\\Sigma})| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\n$$\n\nThe true score is the gradient of the log-density, $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$. Using the chain rule and the identity $\\nabla_{\\mathbf{x}} p(\\mathbf{x}) = p(\\mathbf{x}) \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$:\n$$\ns(\\mathbf{x}) = \\frac{\\nabla_{\\mathbf{x}} p(\\mathbf{x})}{p(\\mathbf{x})} = \\frac{\\frac{1}{2}\\nabla_{\\mathbf{x}} p_1(\\mathbf{x}) + \\frac{1}{2}\\nabla_{\\mathbf{x}} p_2(\\mathbf{x})}{\\frac{1}{2} p_1(\\mathbf{x}) + \\frac{1}{2} p_2(\\mathbf{x})}\n$$\nUsing the provided identity $\\nabla_{\\mathbf{x}} p_i(\\mathbf{x}) = p_i(\\mathbf{x}) \\nabla_{\\mathbf{x}} \\log p_i(\\mathbf{x})$ where $\\nabla_{\\mathbf{x}} \\log p_i(\\mathbf{x}) = -\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_i) = s_i(\\mathbf{x})$, the score can be expressed as a weighted average of the individual component scores:\n$$\ns(\\mathbf{x}) = \\frac{p_1(\\mathbf{x})s_1(\\mathbf{x}) + p_2(\\mathbf{x})s_2(\\mathbf{x})}{p_1(\\mathbf{x}) + p_2(\\mathbf{x})}\n$$\nThis form can be computed stably using the LSE trick by factoring out the maximum of $\\log p_1(\\mathbf{x})$ and $\\log p_2(\\mathbf{x})$.\n\n**2. Parametric Score Field**\n\nThe parametric score field $s_\\theta(\\mathbf{x})$ is constructed by adding a non-conservative (rotational) component to the true score:\n$$\ns_\\theta(\\mathbf{x}) = s(\\mathbf{x}) + \\varepsilon\\,\\mathbf{R}(\\mathbf{x})\n$$\nwhere $\\varepsilon \\ge 0$ is a scalar controlling the magnitude of the non-conservative part. The rotational field $\\mathbf{R}(\\mathbf{x})$ is defined as:\n$$\n\\mathbf{R}(\\mathbf{x}) = \\frac{\\mathbf{Q}\\,(\\mathbf{x} - \\mathbf{c})}{1 + \\lVert \\mathbf{x} - \\mathbf{c} \\rVert^2}, \\quad \\text{with } \\mathbf{Q} = \\begin{bmatrix}0 & -1 \\\\ 1 & 0\\end{bmatrix}, \\quad \\mathbf{c} = [0.5,-0.5]^\\top.\n$$\nThe matrix $\\mathbf{Q}$ rotates a $2$D vector by $+90^\\circ$. The curl of $\\mathbf{R}(\\mathbf{x})$ is non-zero, confirming it is not a gradient field.\n\n**3. Integration Paths**\n\nWe compute the line integral of $s_\\theta(\\mathbf{x})$ from a starting point $\\mathbf{x}_0$ to an endpoint $\\mathbf{x}$ along two distinct paths, both parameterized by $t \\in [0,1]$.\n1.  **Straight path**: A direct line segment.\n    $$\n    \\boldsymbol{\\gamma}_{\\text{straight}}(t) = \\mathbf{x}_0 + t\\,(\\mathbf{x} - \\mathbf{x}_0) \\quad \\implies \\quad \\boldsymbol{\\gamma}'_{\\text{straight}}(t) = \\mathbf{x} - \\mathbf{x}_0\n    $$\n2.  **Curved path**: A sinusoidal detour. Let $\\mathbf{d} = \\mathbf{x} - \\mathbf{x}_0$ be the displacement vector and $\\hat{\\mathbf{n}} = \\mathbf{Q}\\,\\mathbf{d} / \\lVert \\mathbf{d} \\rVert$ be the orthogonal unit vector.\n    $$\n    \\boldsymbol{\\gamma}_{\\text{curved}}(t) = \\mathbf{x}_0 + t\\,\\mathbf{d} + a\\,\\sin(\\pi t)\\,\\hat{\\mathbf{n}} \\quad \\implies \\quad \\boldsymbol{\\gamma}'_{\\text{curved}}(t) = \\mathbf{d} + a\\,\\pi\\,\\cos(\\pi t)\\,\\hat{\\mathbf{n}}\n    $$\nThe parameter $a \\ge 0$ controls the amplitude of the detour.\n\n**4. Numerical Line Integral**\n\nThe line integral along a path $\\boldsymbol{\\gamma}$ is given by $I_{\\text{path}} = \\int_{\\boldsymbol{\\gamma}} s_\\theta(\\mathbf{x}) \\cdot \\mathrm{d}\\mathbf{x}$. Using the parameterization $\\boldsymbol{\\gamma}(t)$, this becomes:\n$$\nI_{\\text{path}} = \\int_{0}^{1} s_\\theta(\\boldsymbol{\\gamma}(t)) \\cdot \\boldsymbol{\\gamma}'(t)\\,\\mathrm{d}t\n$$\nThis integral is approximated numerically using the composite trapezoidal rule with $N=4096$ steps. Let $t_k = k/N$ for $k=0, 1, \\dots, N$, and let $g(t) = s_\\theta(\\boldsymbol{\\gamma}(t)) \\cdot \\boldsymbol{\\gamma}'(t)$. The integral is approximated as:\n$$\nI_{\\text{path}} \\approx \\frac{1}{N} \\left( \\frac{g(t_0) + g(t_N)}{2} + \\sum_{k=1}^{N-1} g(t_k) \\right)\n$$\nA special case occurs if $\\mathbf{x}_0 = \\mathbf{x}$, which implies $\\mathbf{d}=\\mathbf{0}$. In this case, both paths are stationary, the path derivatives are zero, and both integrals $I_{\\text{straight}}$ and $I_{\\text{curved}}$ are defined to be $0$.\n\n**5. Error Metrics**\n\nTwo metrics are computed for each test case:\n-   **Path Dependence ($D$)**: The absolute difference between the integrals along the two paths. If $s_\\theta$ were conservative, this would be $0$. A non-zero value indicates a deviation.\n    $$\n    D = \\left\\lvert I_{\\text{straight}} - I_{\\text{curved}} \\right\\rvert\n    $$\n-   **Estimation Error ($E$)**: The absolute error between the straight-path integral and the true log-density difference. For a perfectly conservative field ($\\varepsilon=0$), this reflects the numerical integration error. For $\\varepsilon > 0$, it also includes the error from the non-conservative component.\n    $$\n    E = \\left\\lvert I_{\\text{straight}} - \\left(\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)\\right) \\right\\rvert\n    $$\n\nThe program implements these steps in a vectorized manner using `numpy` for efficiency, processing each test case to compute and report the values of $D$ and $E$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes path-dependence and estimation error for a non-conservative score field.\n    \"\"\"\n    # Define problem constants\n    D_DIM = 2\n    MU1 = np.array([0.0, 0.0])\n    MU2 = np.array([2.0, -1.0])\n    SIGMA1 = np.array([[1.0, 0.3], [0.3, 0.5]])\n    SIGMA2 = np.array([[0.9, -0.2], [-0.2, 0.8]])\n    Q = np.array([[0.0, -1.0], [1.0, 0.0]])\n    C = np.array([0.5, -0.5])\n    N_STEPS = 4096\n\n    # Pre-compute matrix inverses and log-determinants\n    SIGMA1_INV = np.linalg.inv(SIGMA1)\n    SIGMA2_INV = np.linalg.inv(SIGMA2)\n    _, LOG_DET_SIGMA1 = np.linalg.slogdet(SIGMA1)\n    _, LOG_DET_SIGMA2 = np.linalg.slogdet(SIGMA2)\n    \n    LOG_PDF_CONST = -0.5 * D_DIM * np.log(2 * np.pi)\n\n    def log_pdf_gaussian_batch(x, mu, inv_sigma, log_det_sigma):\n        \"\"\"Computes log-PDF for a batch of points for a single Gaussian.\"\"\"\n        # x shape: (D_DIM, M)\n        # mu shape: (D_DIM,)\n        diff = x - mu[:, np.newaxis]  # shape (D_DIM, M)\n        mahalanobis = np.einsum('im,ij,jm->m', diff, inv_sigma, diff)\n        return LOG_PDF_CONST - 0.5 * log_det_sigma - 0.5 * mahalanobis\n\n    def log_pdf_gmm_batch(x):\n        \"\"\"Computes log-PDF for a batch of points for the GMM.\"\"\"\n        log_p1 = log_pdf_gaussian_batch(x, MU1, SIGMA1_INV, LOG_DET_SIGMA1)\n        log_p2 = log_pdf_gaussian_batch(x, MU2, SIGMA2_INV, LOG_DET_SIGMA2)\n        \n        max_log = np.maximum(log_p1, log_p2)\n        lse = max_log + np.log(np.exp(log_p1 - max_log) + np.exp(log_p2 - max_log))\n        \n        return -np.log(2) + lse\n\n    def true_score_batch(x):\n        \"\"\"Computes the true score field for a batch of points.\"\"\"\n        diff1 = x - MU1[:, np.newaxis]\n        diff2 = x - MU2[:, np.newaxis]\n        s1 = -SIGMA1_INV @ diff1\n        s2 = -SIGMA2_INV @ diff2\n        \n        log_p1 = log_pdf_gaussian_batch(x, MU1, SIGMA1_INV, LOG_DET_SIGMA1)\n        log_p2 = log_pdf_gaussian_batch(x, MU2, SIGMA2_INV, LOG_DET_SIGMA2)\n        \n        max_log = np.maximum(log_p1, log_p2)\n        exp1 = np.exp(log_p1 - max_log)\n        exp2 = np.exp(log_p2 - max_log)\n        \n        # Broadcasting for weighted average\n        score = (exp1[np.newaxis, :] * s1 + exp2[np.newaxis, :] * s2) / (exp1 + exp2)[np.newaxis, :]\n        return score\n\n    def rotational_field_batch(x):\n        \"\"\"Computes the rotational field component for a batch of points.\"\"\"\n        diff = x - C[:, np.newaxis]\n        norm_sq = np.sum(diff * diff, axis=0)\n        return (Q @ diff) / (1.0 + norm_sq)\n\n    def parametric_score_batch(x, epsilon):\n        \"\"\"Computes the perturbed parametric score field.\"\"\"\n        return true_score_batch(x) + epsilon * rotational_field_batch(x)\n\n    def compute_metrics(case):\n        \"\"\"Computes D and E for a single test case.\"\"\"\n        x0, x, epsilon, a = case\n        x0, x = np.array(x0), np.array(x)\n\n        d_vec = x - x0\n        d_norm = np.linalg.norm(d_vec)\n\n        if np.isclose(d_norm, 0):\n            return 0.0, 0.0\n\n        t = np.linspace(0, 1, N_STEPS + 1)\n\n        # Path 1: Straight\n        gamma_straight = x0[:, np.newaxis] + d_vec[:, np.newaxis] * t\n        gamma_prime_straight = np.tile(d_vec[:, np.newaxis], (1, len(t)))\n        \n        s_theta_straight = parametric_score_batch(gamma_straight, epsilon)\n        integrand_straight = np.einsum('ij,ij->j', s_theta_straight, gamma_prime_straight)\n        I_straight = np.trapz(integrand_straight, t)\n\n        # Path 2: Curved\n        n_hat = (Q @ d_vec) / d_norm\n        gamma_curved = (x0[:, np.newaxis] + d_vec[:, np.newaxis] * t +\n                        a * np.sin(np.pi * t) * n_hat[:, np.newaxis])\n        gamma_prime_curved = (d_vec[:, np.newaxis] +\n                              a * np.pi * np.cos(np.pi * t) * n_hat[:, np.newaxis])\n\n        s_theta_curved = parametric_score_batch(gamma_curved, epsilon)\n        integrand_curved = np.einsum('ij,ij->j', s_theta_curved, gamma_prime_curved)\n        I_curved = np.trapz(integrand_curved, t)\n\n        # True log-density difference\n        log_p_x = log_pdf_gmm_batch(x[:, np.newaxis])[0]\n        log_p_x0 = log_pdf_gmm_batch(x0[:, np.newaxis])[0]\n        true_diff = log_p_x - log_p_x0\n        \n        # Metrics D and E\n        D = np.abs(I_straight - I_curved)\n        E = np.abs(I_straight - true_diff)\n\n        return D, E\n\n    test_cases = [\n        ([0.0, 0.0], [2.0, 1.0], 0.0, 0.5),      # Case 1\n        ([0.0, 0.0], [2.0, 1.0], 0.05, 0.5),     # Case 2\n        ([0.0, 0.0], [2.0, 1.0], 0.2, 0.5),      # Case 3\n        ([1.0, -1.0], [1.0, -1.0], 0.25, 1.0),   # Case 4\n        ([0.0, -2.0], [3.0, 1.0], 0.1, 2.0),     # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        D_val, E_val = compute_metrics(case)\n        results.append(f\"[{D_val:.6f},{E_val:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3172994"}]}