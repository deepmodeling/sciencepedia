{"hands_on_practices": [{"introduction": "An autoregressive (AR) model describes a time-dependent process by regressing a variable on its own past values. The core of this model lies in its parameters, which quantify the influence of previous time steps. This first exercise provides a foundational, hands-on calculation using the Yule-Walker equations, which form a direct bridge between a time series's observable statistical properties—its autocovariances—and the hidden parameters of the AR model that generated it [@problem_id:2373810]. Mastering this method builds a concrete understanding of how we can infer a model's structure directly from data.", "problem": "A demeaned, weakly stationary monthly excess return series $\\{r_{t}\\}$ is modeled as an autoregressive (AR) model of order $2$, that is,\n$r_{t} = \\phi_{1} r_{t-1} + \\phi_{2} r_{t-2} + \\varepsilon_{t}$,\nwhere $\\{\\varepsilon_{t}\\}$ is a zero-mean white noise innovation process with variance $\\sigma_{\\varepsilon}^{2}$ and $\\varepsilon_{t}$ is uncorrelated with $\\{r_{s}\\}$ for all $s  t$. From a long sample, the following sample autocovariances at lags $0$, $1$, and $2$ are computed and will be treated as population values for estimation:\n$\\hat{\\gamma}(0) = 0.010$, $\\hat{\\gamma}(1) = 0.006$, $\\hat{\\gamma}(2) = 0.002$.\nUsing the Yule–Walker equations associated with the autoregressive structure and the given autocovariances, determine the Yule–Walker estimates of the parameters $\\phi_{1}$, $\\phi_{2}$, and the innovation variance $\\sigma_{\\varepsilon}^{2}$. Express your final answer as a single row matrix $(\\phi_{1} \\ \\phi_{2} \\ \\sigma_{\\varepsilon}^{2})$. No rounding is required; provide exact values.", "solution": "The problem requires the determination of the parameters of a weakly stationary autoregressive model of order $2$, or AR($2$), using the Yule-Walker equations. This constitutes a standard application of the method of moments in time series analysis.\n\nFirst, we must validate the problem statement.\nThe given information is as follows:\n- The process $\\{r_{t}\\}$ is a demeaned, weakly stationary AR($2$) process: $r_{t} = \\phi_{1} r_{t-1} + \\phi_{2} r_{t-2} + \\varepsilon_{t}$.\n- The innovation process $\\{\\varepsilon_{t}\\}$ is zero-mean white noise with variance $\\sigma_{\\varepsilon}^{2}$.\n- The population autocovariances are taken to be the given sample values: $\\gamma(0) = 0.010$, $\\gamma(1) = 0.006$, and $\\gamma(2) = 0.002$.\n\nThe problem is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data sufficient for a unique solution. The autocovariance values are physically plausible for a stationary process; the variance $\\gamma(0)$ is positive, and the corresponding autocorrelations $\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = 0.6$ and $\\rho(2) = \\frac{\\gamma(2)}{\\gamma(0)} = 0.2$ are within the valid range of $[-1, 1]$. The problem is therefore deemed valid.\n\nWe proceed with the solution. The Yule-Walker equations for a general AR($p$) process are derived by multiplying the defining equation by $r_{t-k}$ for $k > 0$ and taking the expectation. This yields:\n$$E[r_{t}r_{t-k}] = \\sum_{i=1}^{p} \\phi_{i} E[r_{t-i}r_{t-k}] + E[\\varepsilon_{t}r_{t-k}]$$\nGiven weak stationarity and the fact that $\\varepsilon_{t}$ is uncorrelated with past values of $r_s$ (i.e., $E[\\varepsilon_{t}r_{t-k}]=0$ for $k>0$), this simplifies to:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_{i} \\gamma(k-i)$$\nFor the given AR($2$) process, we set $p=2$ and use $k=1$ and $k=2$:\nFor $k=1$: $\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(-1)$. Since $\\gamma(k) = \\gamma(-k)$, this is $\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(1)$.\nFor $k=2$: $\\gamma(2) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(0)$.\n\nThis gives a system of two linear equations in the two unknown parameters $\\phi_{1}$ and $\\phi_{2}$:\n$$\n\\begin{cases}\n\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(1) \\\\\n\\gamma(2) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(0)\n\\end{cases}\n$$\nIn matrix form, this is:\n$$\n\\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\end{pmatrix} =\n\\begin{pmatrix} \\gamma(0)  \\gamma(1) \\\\ \\gamma(1)  \\gamma(0) \\end{pmatrix}\n\\begin{pmatrix} \\phi_{1} \\\\ \\phi_{2} \\end{pmatrix}\n$$\nThe problem states to use the provided sample autocovariances as population values. To maintain precision, we convert the decimal values to fractions:\n$\\gamma(0) = 0.010 = \\frac{10}{1000} = \\frac{1}{100}$\n$\\gamma(1) = 0.006 = \\frac{6}{1000} = \\frac{3}{500}$\n$\\gamma(2) = 0.002 = \\frac{2}{1000} = \\frac{1}{500}$\n\nSubstituting these values into the system of equations:\n$$\n\\begin{cases}\n\\frac{3}{500} = \\phi_{1} \\frac{1}{100} + \\phi_{2} \\frac{3}{500} \\\\\n\\frac{1}{500} = \\phi_{1} \\frac{3}{500} + \\phi_{2} \\frac{1}{100}\n\\end{cases}\n$$\nTo simplify, we multiply both equations by $500$:\n$$\n\\begin{cases}\n3 = 5\\phi_{1} + 3\\phi_{2} \\\\\n1 = 3\\phi_{1} + 5\\phi_{2}\n\\end{cases}\n$$\nWe can solve this system. From the second equation, $3\\phi_{1} = 1 - 5\\phi_{2}$, so $\\phi_{1} = \\frac{1 - 5\\phi_{2}}{3}$. Substituting this into the first equation:\n$$3 = 5\\left(\\frac{1 - 5\\phi_{2}}{3}\\right) + 3\\phi_{2}$$\nMultiplying by $3$ gives:\n$$9 = 5(1 - 5\\phi_{2}) + 9\\phi_{2}$$\n$$9 = 5 - 25\\phi_{2} + 9\\phi_{2}$$\n$$4 = -16\\phi_{2}$$\n$$\\phi_{2} = -\\frac{4}{16} = -\\frac{1}{4}$$\nNow, we find $\\phi_{1}$:\n$$\\phi_{1} = \\frac{1 - 5(-\\frac{1}{4})}{3} = \\frac{1 + \\frac{5}{4}}{3} = \\frac{\\frac{9}{4}}{3} = \\frac{9}{12} = \\frac{3}{4}$$\nThe Yule-Walker estimates for the autoregressive coefficients are $\\phi_{1} = \\frac{3}{4}$ and $\\phi_{2} = -\\frac{1}{4}$.\n\nNext, we must find the innovation variance, $\\sigma_{\\varepsilon}^{2}$. This is obtained from the Yule-Walker equation for $k=0$, which involves the variance of the process, $\\gamma(0)$. We multiply the AR($2$) equation by $r_t$ and take expectations:\n$$E[r_{t}^2] = \\phi_{1} E[r_{t}r_{t-1}] + \\phi_{2} E[r_{t}r_{t-2}] + E[r_{t}\\varepsilon_{t}]$$\n$$\\gamma(0) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(2) + E[(\\phi_{1}r_{t-1} + \\phi_{2}r_{t-2} + \\varepsilon_{t})\\varepsilon_{t}]$$\nSince $\\varepsilon_{t}$ is uncorrelated with past values of $r$, $E[r_{t-1}\\varepsilon_{t}]=0$ and $E[r_{t-2}\\varepsilon_{t}]=0$. This leads to:\n$$\\gamma(0) = \\phi_{1}\\gamma(1) + \\phi_{2}\\gamma(2) + E[\\varepsilon_{t}^2]$$\n$$\\gamma(0) = \\phi_{1}\\gamma(1) + \\phi_{2}\\gamma(2) + \\sigma_{\\varepsilon}^{2}$$\nSolving for $\\sigma_{\\varepsilon}^{2}$:\n$$\\sigma_{\\varepsilon}^{2} = \\gamma(0) - \\phi_{1}\\gamma(1) - \\phi_{2}\\gamma(2)$$\nSubstituting the known values:\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\left(\\frac{3}{4}\\right)\\left(\\frac{3}{500}\\right) - \\left(-\\frac{1}{4}\\right)\\left(\\frac{1}{500}\\right)$$\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\frac{9}{2000} + \\frac{1}{2000}$$\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\frac{8}{2000} = \\frac{1}{100} - \\frac{1}{250}$$\nUsing a common denominator of $500$:\n$$\\sigma_{\\varepsilon}^{2} = \\frac{5}{500} - \\frac{2}{500} = \\frac{3}{500}$$\nThus, the innovation variance is $\\sigma_{\\varepsilon}^{2} = \\frac{3}{500}$.\n\nThe requested parameters are $\\phi_{1} = \\frac{3}{4}$, $\\phi_{2} = -\\frac{1}{4}$, and $\\sigma_{\\varepsilon}^{2} = \\frac{3}{500}$. We present these in the required row matrix format.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{4}  -\\frac{1}{4}  \\frac{3}{500}\n\\end{pmatrix}\n}\n$$", "id": "2373810"}, {"introduction": "After learning how to estimate model parameters from data, a natural next question is: what do these parameters imply about the model's behavior? This practice flips the previous exercise on its head. Here, you will start with a given set of AR(2) parameters and derive the model's theoretical Autocorrelation Function (ACF) [@problem_id:2373783]. The ACF is a crucial diagnostic tool that reveals the \"memory\" of a process—how quickly the influence of past values decays—and whether the dynamics are smooth or oscillatory, providing deep insight into the nature of the time series.", "problem": "You are asked to write a complete, runnable program that computes the theoretical Autocorrelation Function (ACF) for any weakly stationary Autoregressive order two (AR(2)) process in computational economics and finance. The starting point must be the fundamental definitions of weak stationarity and autocovariance, the defining autoregressive equation, and the fact that the shock process is white noise.\n\nConsider a zero-mean AR(2) process $\\{X_t\\}$ defined by the recursion\n$$\nX_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\varepsilon_t,\n$$\nwhere $\\{\\varepsilon_t\\}$ is white noise with $E[\\varepsilon_t]=0$, $E[\\varepsilon_t^2]=\\sigma_\\varepsilon^2$, and $E[\\varepsilon_t \\varepsilon_s]=0$ for $t \\neq s$. Define the autocovariance function $\\gamma_k = \\operatorname{Cov}(X_t, X_{t-k})$ and the autocorrelation function $\\rho_k = \\gamma_k / \\gamma_0$. Assume weak stationarity holds, so that $\\gamma_k$ depends only on the lag $k$ and $\\gamma_{-k}=\\gamma_k$.\n\nYour tasks are:\n- Starting from the autoregressive definition and the properties of white noise, derive the set of linear relations that link $\\gamma_k$ to $\\gamma_{k-1}$ and $\\gamma_{k-2}$ for appropriate lags. Then obtain a recursion for the normalized ACF $\\rho_k$ that permits computing $\\rho_k$ for all integer lags $k \\ge 0$ once initial values are determined, without simulating any data.\n- Explain how to determine the initial values needed for the recursion from the same principles.\n- Implement a function that, given $(\\phi_1,\\phi_2)$ and a nonnegative integer maximum lag $K$, returns the list $[\\rho_0,\\rho_1,\\dots,\\rho_K]$ for the theoretical AR(2) ACF.\n- Ensure weak stationarity by verifying that all roots of the characteristic polynomial $1 - \\phi_1 z - \\phi_2 z^2$ lie strictly outside the unit circle (i.e., have modulus greater than $1$). If this condition fails, the theoretical ACF is not defined; in such a case, for that parameter set, return a list of length $K+1$ filled with $\\text{NaN}$ values.\n\nImplementation requirements:\n- The program must define a function that computes the theoretical ACF deterministically from $(\\phi_1,\\phi_2)$ and $K$, using only the derived relations. Do not simulate.\n- The program must then evaluate this function on the following test suite. Each test case is a tuple $(\\phi_1,\\phi_2,K)$:\n    1. $(0.8, 0.0, 5)$: a pure AR(1) embedded in AR(2), testing reduction to lower order.\n    2. $(0.5, 0.2, 5)$: a stable AR(2) with real roots and persistent dynamics.\n    3. $(0.0, -0.5, 6)$: a stable AR(2) with complex conjugate roots, producing oscillatory autocorrelation.\n    4. $(0.49, 0.5, 5)$: a near-boundary but stationary AR(2) with slow decay.\n    5. $(0.0, 0.0, 5)$: white noise, testing the edge case of zero autoregression.\n- Numerical formatting and output:\n    - For each test case, compute the list $[\\rho_0,\\rho_1,\\dots,\\rho_K]$.\n    - Round each number to $6$ decimal places.\n    - Aggregate the results from all test cases into a single line of output containing a comma-separated list of lists enclosed in square brackets, for example: \n      \"[$[\\rho_0^{(1)},\\dots,\\rho_{K_1}^{(1)}], [\\rho_0^{(2)},\\dots], \\dots$]\".\n      The superscripts denote the test case index; do not print superscripts in your output. Ensure that the printed numbers are in plain decimal notation with exactly $6$ digits after the decimal point.\n\nNotes:\n- There are no physical units involved.\n- Angles are not used.\n- Express any fraction as a decimal where needed.", "solution": "The problem requires the derivation and implementation of a method to compute the theoretical Autocorrelation Function (ACF) for a weakly stationary, zero-mean Autoregressive process of order two (AR(2)). We begin by validating the problem statement.\n\nThe problem is found to be **valid**. It is scientifically grounded in the established theory of stochastic processes, specifically time series analysis. The definitions provided, including the AR(2) process, white noise properties, autocovariance, and autocorrelation, are standard and correct. The task is well-posed, requiring the derivation of the Yule-Walker equations and their application, which for a stationary process yields a unique and meaningful ACF. The problem is objective, stated with mathematical precision, and contains all necessary information to proceed to a solution. We shall now derive the solution from first principles as requested.\n\nAn AR(2) process $\\{X_t\\}$ with zero mean is defined by the equation:\n$$\nX_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\varepsilon_t\n$$\nwhere $\\{\\varepsilon_t\\}$ is a white noise process with mean $E[\\varepsilon_t] = 0$, variance $E[\\varepsilon_t^2] = \\sigma_\\varepsilon^2$, and $E[\\varepsilon_t \\varepsilon_s] = 0$ for $t \\neq s$. We assume weak stationarity, which implies that the mean of the process is constant (given as $0$), and the autocovariance $\\gamma_k = \\mathrm{Cov}(X_t, X_{t-k}) = E[X_t X_{t-k}]$ depends only on the lag $k$, not on time $t$. A property of autocovariance is symmetry: $\\gamma_k = \\gamma_{-k}$.\n\n**1. Derivation of the Yule-Walker Equations**\n\nOur goal is to find a set of linear relations for the autocovariance function $\\gamma_k$. We start by multiplying the AR(2) process equation by $X_{t-k}$ for a lag $k \\ge 0$ and taking the expectation:\n$$\nE[X_t X_{t-k}] = E[(\\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\varepsilon_t) X_{t-k}]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\gamma_k = \\phi_1 E[X_{t-1} X_{t-k}] + \\phi_2 E[X_{t-2} X_{t-k}] + E[\\varepsilon_t X_{t-k}]\n$$\nBy the definition of autocovariance under stationarity, $E[X_{t-1} X_{t-k}] = \\gamma_{k-1}$ and $E[X_{t-2} X_{t-k}] = \\gamma_{k-2}$. The equation becomes:\n$$\n\\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} + E[\\varepsilon_t X_{t-k}]\n$$\nWe now analyze the term $E[\\varepsilon_t X_{t-k}]$.\nFor $k  0$, the variable $X_{t-k}$ is a function of past and present shocks up to time $t-k$ (i.e., $\\{\\varepsilon_s | s \\le t-k\\}$). Since the current shock $\\varepsilon_t$ is uncorrelated with all past shocks, it is also uncorrelated with any function of them. Therefore, for $k0$, $X_{t-k}$ and $\\varepsilon_t$ are uncorrelated:\n$$\nE[\\varepsilon_t X_{t-k}] = E[\\varepsilon_t] E[X_{t-k}] = 0 \\cdot 0 = 0\n$$\nThis yields the recursive relationship for the autocovariance for lags $k \\ge 1$:\n$$\n\\gamma_k = \\phi_1 \\gamma_{k-1} + \\phi_2 \\gamma_{k-2} \\quad \\text{for } k \\ge 1\n$$\nThis set of equations, for different values of $k$, are known as the Yule-Walker equations.\n\n**2. Derivation of the ACF Recursion**\n\nThe autocorrelation function (ACF) is defined as $\\rho_k = \\gamma_k / \\gamma_0$. To find a recursion for $\\rho_k$, we divide the Yule-Walker equation by the variance $\\gamma_0$ (which is non-zero for a non-trivial process):\n$$\n\\frac{\\gamma_k}{\\gamma_0} = \\phi_1 \\frac{\\gamma_{k-1}}{\\gamma_0} + \\phi_2 \\frac{\\gamma_{k-2}}{\\gamma_0}\n$$\nThis directly gives the recursion for the ACF for lags $k \\ge 1$:\n$$\n\\rho_k = \\phi_1 \\rho_{k-1} + \\phi_2 \\rho_{k-2} \\quad \\text{for } k \\ge 1\n$$\nNote that for $k=1$, this gives $\\rho_1 = \\phi_1 \\rho_0 + \\phi_2 \\rho_{-1}$. For $k \\ge 2$, it can be used to compute $\\rho_k$ from the two preceding values. To operationalize this recursion, we need the initial values $\\rho_0$ and $\\rho_1$.\n\n**3. Determination of Initial Values**\n\nBy definition, the autocorrelation at lag $k=0$ is:\n$$\n\\rho_0 = \\frac{\\gamma_0}{\\gamma_0} = 1\n$$\nTo find $\\rho_1$, we use the ACF recursion for $k=1$:\n$$\n\\rho_1 = \\phi_1 \\rho_0 + \\phi_2 \\rho_{-1}\n$$\nUsing the known properties $\\rho_0 = 1$ and the symmetry of the ACF, $\\rho_{-1} = \\rho_1$, we substitute them into the equation:\n$$\n\\rho_1 = \\phi_1(1) + \\phi_2 \\rho_1\n$$\nRearranging the terms to solve for $\\rho_1$:\n$$\n\\rho_1 - \\phi_2 \\rho_1 = \\phi_1\n$$\n$$\n\\rho_1(1 - \\phi_2) = \\phi_1\n$$\nProvided that $\\phi_2 \\neq 1$ (which is guaranteed by the stationarity condition discussed next), we can solve for $\\rho_1$:\n$$\n\\rho_1 = \\frac{\\phi_1}{1 - \\phi_2}\n$$\nWith the initial values $\\rho_0 = 1$ and $\\rho_1 = \\frac{\\phi_1}{1 - \\phi_2}$, we can compute the entire ACF sequence using the recursion $\\rho_k = \\phi_1 \\rho_{k-1} + \\phi_2 \\rho_{k-2}$ for all $k \\ge 2$.\n\n**4. Weak Stationarity Conditions**\n\nThe derivations above are valid only if the AR(2) process is weakly stationary. The stationarity of an AR(p) process is determined by the roots of its characteristic polynomial, $P(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\dots - \\phi_p z^p$. For an AR(2) process, the polynomial is $P(z) = 1 - \\phi_1 z - \\phi_2 z^2$. For the process to be stationary, all roots of the equation $P(z) = 0$ must lie strictly outside the unit circle in the complex plane (i.e., have a modulus greater than $1$). These conditions on the roots translate to the following three conditions on the coefficients $\\phi_1$ and $\\phi_2$:\n1.  $\\phi_1 + \\phi_2  1$\n2.  $\\phi_2 - \\phi_1  1$\n3.  $|\\phi_2|  1$\nThese three inequalities define a triangular region in the $(\\phi_1, \\phi_2)$ parameter space. Any parameter pair outside this region corresponds to a non-stationary process, for which the theoretical ACF is not well-defined.\n\n**5. Computational Algorithm**\n\nThe complete algorithm to compute the theoretical ACF $[\\rho_0, \\rho_1, \\dots, \\rho_K]$ for a given set of parameters $(\\phi_1, \\phi_2)$ and a maximum lag $K$ is as follows:\n1.  Verify the stationarity conditions:\n    - If $\\phi_1 + \\phi_2 \\ge 1$, or $\\phi_2 - \\phi_1 \\ge 1$, or $|\\phi_2| \\ge 1$, the process is non-stationary. The result is a list of $K+1$ Not-a-Number (NaN) values.\n2.  If the process is stationary, initialize an array `rho` of size $K+1$.\n3.  Set the initial value $\\rho_0 = 1.0$.\n4.  If $K \\ge 1$, calculate $\\rho_1 = \\frac{\\phi_1}{1 - \\phi_2}$.\n5.  Iterate for $k$ from $2$ to $K$: compute $\\rho_k = \\phi_1 \\rho_{k-1} + \\phi_2 \\rho_{k-2}$.\n6.  Return the computed array `rho`.\nThis deterministic algorithm avoids any stochastic simulation and is based purely on the derived analytical relations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_ar2_acf(phi1: float, phi2: float, K: int) - list[float]:\n    \"\"\"\n    Computes the theoretical Autocorrelation Function (ACF) for an AR(2) process.\n\n    Args:\n        phi1: The coefficient of the first lag, X_{t-1}.\n        phi2: The coefficient of the second lag, X_{t-2}.\n        K: The maximum lag for which to compute the ACF.\n\n    Returns:\n        A list of floats [rho_0, rho_1, ..., rho_K]. If the process is\n        non-stationary, returns a list of NaNs of the same length.\n    \"\"\"\n    # Step 1: Verify weak stationarity conditions.\n    is_stationary = (\n        phi1 + phi2  1.0 and\n        phi2 - phi1  1.0 and\n        abs(phi2)  1.0\n    )\n\n    if not is_stationary:\n        return [np.nan] * (K + 1)\n\n    # Step 2: Initialize the ACF array.\n    # Using a numpy array for easier vectorized operations and NaN handling if needed.\n    rho = np.zeros(K + 1)\n\n    # Step 3: Set initial value rho_0.\n    rho[0] = 1.0\n\n    # Step 4: Calculate rho_1 if K = 1.\n    if K = 1:\n        # This formula is derived from the Yule-Walker equations.\n        # The stationarity check ensures 1.0 - phi2 is not zero.\n        rho[1] = phi1 / (1.0 - phi2)\n\n    # Step 5: Use the recursion for k from 2 to K.\n    for k in range(2, K + 1):\n        rho[k] = phi1 * rho[k - 1] + phi2 * rho[k - 2]\n    \n    return rho.tolist()\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the ACF calculation on a suite of test cases\n    and printing the results in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (phi1, phi2, K).\n    test_cases = [\n        (0.8, 0.0, 5),   # Pure AR(1) case\n        (0.5, 0.2, 5),   # Stable AR(2) with real roots\n        (0.0, -0.5, 6),  # Stable AR(2) with complex roots (oscillatory)\n        (0.49, 0.5, 5),  # Near-boundary stationary AR(2)\n        (0.0, 0.0, 5),   # White noise case\n    ]\n\n    all_results_str = []\n    for phi1, phi2, K in test_cases:\n        # Calculate the ACF for the current case.\n        rho_values = calculate_ar2_acf(phi1, phi2, K)\n\n        # Format each value to 6 decimal places, handling NaNs.\n        # The f-string format specifier '{val:.6f}' correctly formats np.nan as 'nan'.\n        # The problem asks for \"NaN values\", and 'nan' is the standard string representation.\n        # \"plain decimal notation\" is interpreted as applying to actual numbers.\n        formatted_values = []\n        for val in rho_values:\n            if np.isnan(val):\n                formatted_values.append('nan')\n            else:\n                formatted_values.append(f\"{val:.6f}\")\n        \n        # Create the string representation of the list for this case.\n        case_str = f\"[{','.join(formatted_values)}]\"\n        all_results_str.append(case_str)\n\n    # Aggregate all case results into a single output string.\n    final_output = f\"[{','.join(all_results_str)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2373783"}, {"introduction": "We now transition from the classical analysis of autoregressive models to their modern application in sequence generation, a cornerstone of deep learning. Given a probabilistic AR model, the ultimate goal is often to generate the single most probable sequence, a task known as Maximum a Posteriori (MAP) inference. This exercise highlights the immense computational challenge of this task by asking you to implement both an exact, brute-force search and the widely-used heuristic approximation: beam search [@problem_id:3100866]. By comparing the two, you will gain a practical appreciation for why approximations are essential and discover the specific conditions under which a simple greedy approach (beam search with width $B=1$) can fail to find the optimal sequence.", "problem": "You are given an autoregressive sequence model and asked to formalize how beam search approximates Maximum a Posteriori (MAP) inference by exploring the $B$-best partial prefixes. You must start from the chain rule of probability and core definitions. Then, you must implement exact enumeration and beam search, quantify the probability mass captured by the beam at the final step, and compare the beam’s top sequence with the exact MAP on short sequences. Your final answer must be a complete, runnable program that produces the specified output format.\n\nFundamental base:\n- Chain rule for an autoregressive model: for a sequence $x_{1:T} = (x_1, x_2, \\dots, x_T)$ over a finite vocabulary $\\mathcal{V}$, the joint probability factorizes as\n$$\nP(x_{1:T}) = \\prod_{t=1}^T P(x_t \\mid x_{1:t-1}),\n$$\nwith the convention that $P(x_1 \\mid x_{1:0}) = P(x_1)$.\n- Maximum a posteriori (MAP): the MAP sequence $x^{\\star}_{1:T}$ satisfies\n$$\nx^{\\star}_{1:T} = \\arg\\max_{x_{1:T} \\in \\mathcal{V}^T} P(x_{1:T}).\n$$\n\nBeam search definition for this problem:\n- At step $t$, maintain a beam $\\mathcal{B}_t$ of the $B$ most-probable partial prefixes $x_{1:t}$. Expand each $x_{1:t}$ by one token $a \\in \\mathcal{V}$ to form $x_{1:t+1}$ with probability $P(x_{1:t+1}) = P(x_{1:t}) P(x_{t+1} \\mid x_{1:t})$. Prune to the top $B$ by $P(x_{1:t+1})$.\n- Define the captured mass at step $t$ as\n$$\nM_t = \\sum_{x_{1:t} \\in \\mathcal{B}_t} P(x_{1:t}).\n$$\nNote that at the final step $t = T$, $M_T$ equals the total probability mass of the $B$ completed sequences retained by the beam.\n\nModel class and exact search:\n- To ensure mathematical tractability, you will work with a first-order Markov (bigram) autoregressive model where $P(x_t \\mid x_{1:t-1}) = P(x_t \\mid x_{t-1})$ for $t \\ge 2$.\n- Let $\\mathcal{V} = \\{0, 1, 2\\}$, let the initial distribution be $P(x_1) \\in \\mathbb{R}^3$, and define a transition matrix $\\mathbf{T} \\in \\mathbb{R}^{3 \\times 3}$ with entries $\\mathbf{T}_{ij} = P(x_t = j \\mid x_{t-1} = i)$. All distributions are valid and normalized.\n- Exact MAP for a given horizon $T$ is computed by enumerating all sequences in $\\mathcal{V}^T$, evaluating their joint probabilities using the chain rule and the bigram factorization, and selecting the sequence with maximum probability.\n\nTasks to implement:\n- Implement exact enumeration to compute the exact MAP probability $P_{\\text{MAP}}^{\\text{exact}}$ for the given $T$.\n- Implement beam search with width $B$ and compute:\n  - The captured mass at the final step $M_T$,\n  - The beam’s top sequence probability $P_{\\text{MAP}}^{\\text{beam}}$,\n  - Whether the beam’s top sequence matches the exact MAP in probability (within a small numerical tolerance), returned as a boolean.\n- Use probability arithmetic (multiplication) directly; logarithms are not required for these short horizons.\n\nTest suite:\nFor all cases below, use the same initial distribution and transition matrix:\n- Vocabulary size: $|\\mathcal{V}| = 3$ with tokens $\\{0, 1, 2\\}$.\n- Initial distribution $P(x_1) = [\\,0.6,\\; 0.3,\\; 0.1\\,]$.\n- Transition matrix $\\mathbf{T}$ with rows indexed by the previous token and columns indexed by the next token:\n  - From token $0$: $[\\,0.2,\\; 0.2,\\; 0.6\\,]$,\n  - From token $1$: $[\\,0.05,\\; 0.9,\\; 0.05\\,]$,\n  - From token $2$: $[\\,0.5,\\; 0.1,\\; 0.4\\,]$.\n\nDefine three cases to cover complementary behaviors:\n- Case A (greedy failure edge case): horizon $T = 3$, beam width $B = 1$.\n- Case B (happy path): horizon $T = 3$, beam width $B = 2$.\n- Case C (boundary where beam equals exact due to exhaustiveness): horizon $T = 4$, beam width $B = 81$.\n\nRequired outputs per case:\n- For each case, return a list $[\\,M_T,\\; \\text{map\\_match},\\; P_{\\text{MAP}}^{\\text{exact}},\\; P_{\\text{MAP}}^{\\text{beam}}\\,]$, where $M_T$ and the probabilities are real numbers and $\\text{map\\_match}$ is a boolean indicating whether the beam’s top sequence probability equals the exact MAP probability within a small tolerance.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per case and each entry itself being a comma-separated list in square brackets. For example, the structure must be\n$$\n[\\,[M_T^{(A)}, \\text{map\\_match}^{(A)}, P_{\\text{MAP}}^{\\text{exact},(A)}, P_{\\text{MAP}}^{\\text{beam},(A)}],\\; [M_T^{(B)}, \\dots],\\; [M_T^{(C)}, \\dots]\\,].\n$$\nNo extra text or lines should be printed.", "solution": "The problem asks for a formalization and implementation of beam search as an approximation for Maximum a Posteriori (MAP) inference in a first-order autoregressive model. We must validate the problem, provide a reasoned solution based on fundamental principles, and implement the specified algorithms.\n\n### Problem Validation\n\nThe problem statement has been validated and is deemed valid.\n\n1.  **Givens Extracted**:\n    -   **Model**: First-order Markov autoregressive model, $P(x_t \\mid x_{1:t-1}) = P(x_t \\mid x_{t-1})$.\n    -   **Vocabulary**: $\\mathcal{V} = \\{0, 1, 2\\}$, with size $|\\mathcal{V}| = 3$.\n    -   **Sequence Horizon**: $T$.\n    -   **Beam Width**: $B$.\n    -   **Initial Distribution**: $P(x_1) = [0.6, 0.3, 0.1]$.\n    -   **Transition Matrix**: $T_{ij} = P(x_t=j \\mid x_{t-1}=i)$, given as\n        $$\n        \\mathbf{T} = \\begin{pmatrix} 0.2  0.2  0.6 \\\\ 0.05  0.9  0.05 \\\\ 0.5  0.1  0.4 \\end{pmatrix}.\n        $$\n    -   **Test Cases**:\n        -   Case A: $T=3, B=1$.\n        -   Case B: $T=3, B=2$.\n        -   Case C: $T=4, B=81$.\n    -   **Outputs Required**: For each case, a list containing final captured mass $M_T$, a boolean `map_match` for optimality, the exact MAP probability $P_{\\text{MAP}}^{\\text{exact}}$, and the beam search MAP probability $P_{\\text{MAP}}^{\\text{beam}}$.\n\n2.  **Validation Verdict**:\n    -   **Scientifically Grounded**: The problem is well-founded in probability theory and machine learning. The concepts of autoregressive models, MAP inference, and beam search are standard. The provided probability distributions are valid as all entries are non-negative and the initial probabilities and transition matrix rows each sum to $1$.\n    -   **Well-Posed**: The problem is fully specified with all necessary parameters, definitions, and constraints. The tasks are clearly defined, leading to a unique, computable solution.\n    -   **Objective**: The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n\nThe problem is valid and can be solved as stated.\n\n### Principle-Based Solution Design\n\nThe core task is to find the most probable sequence $x^{\\star}_{1:T}$ in a space of $|\\mathcal{V}|^T$ possible sequences. This is a MAP inference problem.\n\n#### 1. Formalizing Probability and MAP Inference\n\nFor a sequence $x_{1:T} = (x_1, x_2, \\dots, x_T)$, its joint probability under an autoregressive model is given by the chain rule:\n$$\nP(x_{1:T}) = P(x_1) \\prod_{t=2}^T P(x_t \\mid x_{1:t-1})\n$$\nThe problem specifies a first-order Markov (bigram) model, which simplifies the conditional probability to depend only on the previous token: $P(x_t \\mid x_{1:t-1}) = P(x_t \\mid x_{t-1})$. The joint probability thus becomes:\n$$\nP(x_{1:T}) = P(x_1) \\prod_{t=2}^T P(x_t \\mid x_{t-1})\n$$\nThe MAP inference task is to find the sequence $x^{\\star}_{1:T}$ that maximizes this probability:\n$$\nx^{\\star}_{1:T} = \\arg\\max_{x_{1:T} \\in \\mathcal{V}^T} P(x_{1:T})\n$$\nThe probability associated with this MAP sequence is $P_{\\text{MAP}}^{\\text{exact}} = P(x^{\\star}_{1:T})$.\n\n#### 2. Exact MAP through Enumeration\n\nFor small values of $T$ and $|\\mathcal{V}|$, we can compute $P_{\\text{MAP}}^{\\text{exact}}$ by exhaustively enumerating all $|\\mathcal{V}|^T$ possible sequences. For each sequence, we calculate its joint probability using the bigram factorization and identify the maximum probability among them. This approach is computationally expensive, scaling exponentially as $O(|\\mathcal{V}|^T)$, but guarantees finding the true MAP sequence.\n\nFor a sequence $x_{1:T} = (s_1, s_2, \\dots, s_T)$, where each $s_i \\in \\mathcal{V}$, its probability is calculated as:\n$$\nP(x_{1:T}) = P_{\\text{initial}}[s_1] \\times \\mathbf{T}[s_1, s_2] \\times \\mathbf{T}[s_2, s_3] \\times \\dots \\times \\mathbf{T}[s_{T-1}, s_T]\n$$\nwhere $P_{\\text{initial}}$ is the initial distribution vector and $\\mathbf{T}$ is the transition matrix.\n\n#### 3. Beam Search as Approximate MAP Inference\n\nBeam search is a heuristic algorithm that provides a tractable approximation to MAP inference by trading completeness for efficiency. It prunes the search space by exploring only a fixed number, $B$ (the beam width), of the most promising partial sequences (prefixes) at each step.\n\nThe algorithm proceeds as follows:\n-   **Step $t=1$**: Calculate the probabilities $P(x_1)$ for all $x_1 \\in \\mathcal{V}$. The initial beam, $\\mathcal{B}_1$, consists of the $B$ single-token prefixes with the highest probabilities.\n-   **Step $t \\to t+1$**: For each prefix $x_{1:t} \\in \\mathcal{B}_t$ with probability $P(x_{1:t})$:\n    1.  **Expand**: Generate $|\\mathcal{V}|$ new candidate prefixes $x_{1:t+1} = (x_{1:t}, a)$ for every token $a \\in \\mathcal{V}$.\n    2.  **Score**: The probability of each new candidate is $P(x_{1:t+1}) = P(x_{1:t}) \\times P(x_{t+1}=a \\mid x_t)$.\n    3.  **Prune**: Collect all $B \\times |\\mathcal{V}|$ candidates generated from the beam. Select the $B$ candidates with the highest probabilities to form the new beam, $\\mathcal{B}_{t+1}$.\n-   **Termination**: After $T$ steps, the final beam $\\mathcal{B}_T$ contains $B$ complete sequences. The top sequence in this beam is the algorithm's estimate for the MAP sequence, and its probability is denoted $P_{\\text{MAP}}^{\\text{beam}}$.\n\nBeam search is not guaranteed to find the true MAP sequence because a prefix that has a lower probability at an intermediate step might lead to a sequence with the highest overall probability. By pruning such prefixes, the algorithm may miss the optimal solution. This trade-off is illustrated by Case A ($B=1$, greedy search) versus Case B ($B=2$).\n\nWhen the beam width $B$ is greater than or equal to the total number of possible sequences at any step (i.e., $B \\ge |\\mathcal{V}|^t$), no pruning occurs, and beam search becomes equivalent to exact enumeration. This is demonstrated in Case C, where $B=81 = |\\mathcal{V}|^T = 3^4$.\n\n#### 4. Captured Probability Mass\n\nThe captured mass $M_t$ at step $t$ is the sum of probabilities of all prefixes in the beam $\\mathcal{B}_t$:\n$$\nM_t = \\sum_{x_{1:t} \\in \\mathcal{B}_t} P(x_{1:t})\n$$\nAt the final step $T$, $M_T$ represents the total probability mass of the $B$ sequences that the algorithm ultimately considers. It serves as a measure of how much of the total probability space (which sums to $1$) is covered by the final beam. For Case C, where beam search is exact, $M_T$ must equal $1$.\n\nThe implementation will consist of two primary functions: `exact_search`, which performs full enumeration, and `beam_search`, which implements the iterative expansion and pruning logic. The results from both will be compared for the specified test cases.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef exact_search(P_initial: np.ndarray, T_matrix: np.ndarray, V_size: int, T_horizon: int) - float:\n    \"\"\"\n    Computes the exact Maximum a Posteriori (MAP) probability by enumerating all possible sequences.\n\n    Args:\n        P_initial: The initial probability distribution for the first token.\n        T_matrix: The transition probability matrix.\n        V_size: The size of the vocabulary.\n        T_horizon: The length of the sequences.\n\n    Returns:\n        The probability of the most likely sequence.\n    \"\"\"\n    max_prob = 0.0\n    \n    # Generate all possible sequences of length T_horizon\n    all_sequences = itertools.product(range(V_size), repeat=T_horizon)\n    \n    for seq in all_sequences:\n        # Calculate the probability of the current sequence\n        prob = P_initial[seq[0]]\n        for t in range(1, T_horizon):\n            prev_token = seq[t-1]\n            current_token = seq[t]\n            prob *= T_matrix[prev_token, current_token]\n        \n        if prob  max_prob:\n            max_prob = prob\n            \n    return max_prob\n\ndef beam_search(P_initial: np.ndarray, T_matrix: np.ndarray, V_size: int, T_horizon: int, B: int) - tuple[float, float]:\n    \"\"\"\n    Performs beam search to find an approximation of the MAP sequence.\n\n    Args:\n        P_initial: The initial probability distribution for the first token.\n        T_matrix: The transition probability matrix.\n        V_size: The size of the vocabulary.\n        T_horizon: The length of the sequences.\n        B: The beam width.\n\n    Returns:\n        A tuple containing:\n        - M_T: The captured probability mass at the final step.\n        - P_map_beam: The probability of the top sequence found by the beam search.\n    \"\"\"\n    # Initialize beam at t=1\n    # Each element in the beam is a tuple: (probability, sequence_tuple)\n    beam = []\n    for v in range(V_size):\n        prob = P_initial[v]\n        beam.append((prob, (v,)))\n        \n    beam.sort(key=lambda x: x[0], reverse=True)\n    beam = beam[:B]\n    \n    # Iterate for remaining steps\n    for t in range(1, T_horizon):\n        candidates = []\n        for prefix_prob, prefix in beam:\n            last_token = prefix[-1]\n            for v in range(V_size):\n                transition_prob = T_matrix[last_token, v]\n                new_prob = prefix_prob * transition_prob\n                new_prefix = prefix + (v,)\n                candidates.append((new_prob, new_prefix))\n        \n        # Sort all candidates and prune to form the new beam\n        candidates.sort(key=lambda x: x[0], reverse=True)\n        beam = candidates[:B]\n\n    # After the loop, the beam contains the final sequences\n    if not beam:\n        return 0.0, 0.0\n\n    p_map_beam = beam[0][0]\n    m_t = sum(p for p, _ in beam)\n    \n    return m_t, p_map_beam\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define model parameters\n    p_initial = np.array([0.6, 0.3, 0.1])\n    t_matrix = np.array([\n        [0.2, 0.2, 0.6],  # Transitions from token 0\n        [0.05, 0.9, 0.05], # Transitions from token 1\n        [0.5, 0.1, 0.4]   # Transitions from token 2\n    ])\n    v_size = 3\n    \n    # Define test cases\n    test_cases = [\n        # (T_horizon, Beam_width B)\n        (3, 1),  # Case A: Greedy failure\n        (3, 2),  # Case B: Happy path\n        (4, 81), # Case C: Beam search equals exact\n    ]\n    \n    all_results = []\n    \n    for T_horizon, B in test_cases:\n        # 1. Compute exact MAP probability\n        p_map_exact = exact_search(p_initial, t_matrix, v_size, T_horizon)\n        \n        # 2. Run beam search\n        m_t, p_map_beam = beam_search(p_initial, t_matrix, v_size, T_horizon, B)\n        \n        # 3. Compare probabilities for map_match\n        # Use a small tolerance for floating point comparison\n        map_match = np.isclose(p_map_exact, p_map_beam, atol=1e-9)\n        \n        # 4. Collect results for the case\n        case_results = [m_t, bool(map_match), p_map_exact, p_map_beam]\n        all_results.append(case_results)\n\n    # Format the final output string\n    # Convert Python list of lists to the required string format\n    # e.g., [[val1, True, ...], [val2, False, ...]]\n    # str(list) will produce the correct format with 'True'/'False' for booleans.\n    # The only subtlety is removing spaces after commas.\n    result_str = str(all_results).replace(\" \", \"\")\n    \n    print(result_str)\n\nsolve()\n```", "id": "3100866"}]}