{"hands_on_practices": [{"introduction": "A common failure mode in GAN training occurs when the discriminator becomes too confident, quickly learning to perfectly distinguish real from fake samples. This leads to vanishing gradients for the generator, stalling its learning process. This exercise explores label smoothing, a simple yet powerful regularization technique designed to curb the discriminator's overconfidence. By deriving the form of the optimal discriminator with label smoothing, you will gain a first-principles understanding of how it alters the GAN objective and helps maintain healthier, more informative gradients for the generator [@problem_id:3127248].", "problem": "Consider the standard Generative Adversarial Network (GAN) framework, where the generator produces samples distributed according to a density $p_{g}(x)$ and the real data follow a density $p_{r}(x)$. The discriminator outputs a scalar $D(x) \\in (0,1)$ interpreted as the predicted probability that $x$ is real. The discriminator is trained by maximizing the Binary Cross-Entropy (BCE) objective over real and fake samples.\n\nSuppose we apply label smoothing to the real class during discriminator training: the target for real samples is replaced from $1$ to $1-\\alpha$ with $0 < \\alpha < 1$, while the target for fake samples remains $0$. The discriminatorâ€™s expected objective becomes\n$$\nJ(D) \\;=\\; \\mathbb{E}_{x \\sim p_{r}}\\!\\left[(1-\\alpha)\\,\\ln D(x) \\;+\\; \\alpha\\,\\ln\\!\\big(1-D(x)\\big)\\right] \\;+\\; \\mathbb{E}_{x \\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right].\n$$\nAssume $D$ is optimized pointwise in $x$ and treat $p_{r}(x)$ and $p_{g}(x)$ as fixed nonnegative densities.\n\nYour tasks are:\n1. Starting from the BCE form and pointwise maximization of $J(D)$, derive the closed-form optimal discriminator $D^{\\star}(x)$ as a function of $p_{r}(x)$, $p_{g}(x)$, and $\\alpha$.\n2. Consider the non-saturating generator objective $L_{G}^{\\mathrm{ns}}(G) = -\\,\\mathbb{E}_{x \\sim p_{g}}[\\ln D(x)]$. With $D$ treated as fixed at $D^{\\star}$, use the chain rule to express the gradient flowing from the discriminator to the generator at a generated sample $x$ and isolate the scalar multiplier that scales the backpropagated discriminator gradient. Compute the ratio of this scalar multiplier under label smoothing ($\\alpha > 0$) to its value with no smoothing ($\\alpha = 0$), and simplify this ratio to a closed-form expression depending only on $\\alpha$.\n\nProvide your final answer as a single row matrix whose first entry is $D^{\\star}(x)$ and whose second entry is the simplified ratio from part 2. No numerical approximation is required.", "solution": "The problem asks for two derivations related to a Generative Adversarial Network (GAN) with one-sided label smoothing. First, we must derive the optimal discriminator $D^{\\star}(x)$. Second, we must analyze the effect of this on the generator's gradient.\n\n**Part 1: Derivation of the Optimal Discriminator $D^{\\star}(x)$**\n\nThe discriminator's objective function $J(D)$ is given by:\n$$J(D) \\;=\\; \\mathbb{E}_{x \\sim p_{r}}\\!\\left[(1-\\alpha)\\,\\ln D(x) \\;+\\; \\alpha\\,\\ln\\!\\big(1-D(x)\\big)\\right] \\;+\\; \\mathbb{E}_{x \\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right]$$\nwhere $p_r(x)$ is the real data density, $p_g(x)$ is the generator's density, $D(x)$ is the discriminator's output, and $\\alpha$ is the label smoothing parameter with $0 < \\alpha < 1$.\n\nWe can express the expectations as integrals over the data space:\n$$J(D) = \\int p_{r}(x)\\left[(1-\\alpha)\\ln D(x) + \\alpha\\ln(1-D(x))\\right] dx + \\int p_{g}(x)\\ln(1-D(x)) dx$$\nTo find the optimal discriminator $D^{\\star}(x)$, we can maximize this functional with respect to the function $D(x)$. Since the integral is a sum over all values of $x$, we can maximize the integrand pointwise for each $x$. Let's define the integrand as a function $f(y)$ where $y = D(x)$, for a fixed $x$:\n$$f(y) = p_{r}(x)\\left[(1-\\alpha)\\ln(y) + \\alpha\\ln(1-y)\\right] + p_{g}(x)\\ln(1-y)$$\nWe can collect the terms containing $\\ln(y)$ and $\\ln(1-y)$:\n$$f(y) = p_{r}(x)(1-\\alpha)\\ln(y) + \\left[p_{r}(x)\\alpha + p_{g}(x)\\right]\\ln(1-y)$$\nTo find the value of $y$ that maximizes $f(y)$, we compute its derivative with respect to $y$ and set it to zero.\n$$\\frac{df}{dy} = \\frac{p_{r}(x)(1-\\alpha)}{y} - \\frac{p_{r}(x)\\alpha + p_{g}(x)}{1-y}$$\nSetting the derivative to zero gives the condition for the optimal $D^{\\star}(x)$:\n$$\\frac{p_{r}(x)(1-\\alpha)}{D^{\\star}(x)} = \\frac{p_{r}(x)\\alpha + p_{g}(x)}{1-D^{\\star}(x)}$$\nNow, we solve for $D^{\\star}(x)$:\n$$p_{r}(x)(1-\\alpha)(1-D^{\\star}(x)) = D^{\\star}(x)\\left[p_{r}(x)\\alpha + p_{g}(x)\\right]$$\n$$p_{r}(x)(1-\\alpha) - p_{r}(x)(1-\\alpha)D^{\\star}(x) = p_{r}(x)\\alpha D^{\\star}(x) + p_{g}(x)D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x)(1-\\alpha) + p_{r}(x)\\alpha + p_{g}(x)\\right]D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x) - p_{r}(x)\\alpha + p_{r}(x)\\alpha + p_{g}(x)\\right]D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x) + p_{g}(x)\\right]D^{\\star}(x)$$\nIsolating $D^{\\star}(x)$, we obtain the closed-form expression for the optimal discriminator:\n$$D^{\\star}(x) = \\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}$$\nThis is the solution to the first part of the problem.\n\n**Part 2: Ratio of Generator Gradient Scalar Multipliers**\n\nThe generator is trained to minimize the non-saturating objective:\n$$L_{G}^{\\mathrm{ns}}(G) = -\\,\\mathbb{E}_{x \\sim p_{g}}[\\ln D(x)]$$\nWe consider a single generated sample $x$, for which the loss contribution is $L_{G}(x) = -\\ln D(x)$. We need to find the gradient of this loss with respect to the generated sample $x$, which is then backpropagated to the generator's parameters. Using the chain rule, the gradient is:\n$$\\nabla_x L_{G}(x) = \\nabla_x (-\\ln D(x)) = -\\frac{1}{D(x)} \\nabla_x D(x)$$\nThe term $\\nabla_x D(x)$ is the gradient of the discriminator's output with respect to its input, which is computed and backpropagated by the deep learning framework. The problem asks for the scalar multiplier that scales this gradient. From the expression above, this scalar multiplier is:\n$$M(x) = -\\frac{1}{D(x)}$$\nWe are asked to evaluate this multiplier when the discriminator is optimal, i.e., $D(x) = D^{\\star}(x)$. So the multiplier becomes:\n$$M(x, \\alpha) = -\\frac{1}{D^{\\star}(x)} = -\\frac{1}{\\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}} = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-\\alpha)}$$\nThis is the multiplier with label smoothing ($\\alpha > 0$). To find the multiplier without label smoothing, we set $\\alpha=0$:\n$$M(x, 0) = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-0)} = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)}$$\nThe problem asks for the ratio of the scalar multiplier under label smoothing to its value with no smoothing.\n$$\\text{Ratio} = \\frac{M(x, \\alpha)}{M(x, 0)} = \\frac{-\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-\\alpha)}}{-\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)}}$$\nThe common terms $-\\left(p_{r}(x) + p_{g}(x)\\right)$ in the numerators and $p_r(x)$ in the denominators cancel out:\n$$\\text{Ratio} = \\frac{\\frac{1}{1-\\alpha}}{1} = \\frac{1}{1-\\alpha}$$\nThis simplified expression depends only on $\\alpha$ and represents the scaling factor of the generator's gradient magnitude due to one-sided label smoothing.\n\nThe two results for the final answer are:\n1.  $D^{\\star}(x) = \\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}$\n2.  The ratio is $\\frac{1}{1-\\alpha}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)} & \\frac{1}{1-\\alpha}\n\\end{pmatrix}\n}\n$$", "id": "3127248"}, {"introduction": "One of the most notorious challenges in GAN training is mode collapse, where the generator learns to produce only a few, or even just one, of the many modes present in the true data distribution. Standard discriminators evaluate each sample in isolation, making them vulnerable to this type of failure. This practice introduces minibatch discrimination, a technique that explicitly encourages intra-batch diversity by allowing the discriminator to assess relationships between samples. By deriving and analyzing the generator's gradient, you will see exactly how this mechanism creates a repulsive force between generated samples in a minibatch, directly combating mode collapse [@problem_id:3127206].", "problem": "Consider a Generative Adversarial Network (GAN), defined as a two-player game between a generator and a discriminator, where the generator $G$ maps a latent variable $z$ to a data point $x$, and the discriminator $D$ estimates the probability that an input comes from the real data distribution rather than from $G$. Let the generator be the scalar linear map $G_{w}(z) = w z$ with parameter $w \\in \\mathbb{R}$ and $z \\in \\mathbb{R}$. Let the discriminator output be $D(x, s) = \\sigma(h(x, s))$, where $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$ is the logistic sigmoid, and the discriminator logit is $h(x, s) = a x + b - \\lambda s$, with $a, b, \\lambda \\in \\mathbb{R}$. To encourage diversity in the generator outputs within a minibatch, augment the discriminator with a minibatch discrimination similarity term\n$$\nc(x, x') = \\exp\\!\\big(-\\beta (x - x')^{2}\\big),\n$$\nwhere $\\beta > 0$, and define for a minibatch $\\{z_{i}\\}_{i=1}^{n}$ the per-sample similarity summary\n$$\ns_{i} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j}), \\quad x_{i} = G_{w}(z_{i}).\n$$\nAssume the generator uses the non-saturating objective\n$$\nL_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big), \\quad \\ell(p) = -\\ln(p).\n$$\nTasks:\n1. Starting only from these definitions, derive a closed-form expression for $\\frac{d L_{G}}{d w}$ in terms of $a$, $b$, $\\lambda$, $\\beta$, $w$, and $\\{z_{i}\\}_{i=1}^{n}$.\n2. Numerically evaluate the derivative for the minibatch size $n = 3$ with $z_{1} = -1$, $z_{2} = 0$, $z_{3} = 1$, parameters $a = 1.5$, $b = 0$, $\\lambda = 2$, $\\beta = 0.5$, at $w = 1$. Round your final numerical answer to four significant figures. Express the final result as a unitless scalar.\n3. Explain qualitatively, based on your derived gradient, how the minibatch discrimination contribution encourages output diversity across the minibatch, and discuss pitfalls if the minibatch size $n$ is small relative to the count of distinct modes in the data distribution. No additional calculation is required for this part.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of machine learning and calculus, is well-posed with a clear objective, and provides a complete and consistent set of definitions. We can therefore proceed with the solution.\n\nThe problem is divided into three tasks: 1) deriving the gradient of the generator's loss function, 2) numerically evaluating this gradient for a specific case, and 3) qualitatively explaining the role of the minibatch discrimination term.\n\n### Task 1: Derivation of the Gradient $\\frac{d L_{G}}{d w}$\n\nThe generator's loss function is given by:\n$$L_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big)$$\nwhere $\\ell(p) = -\\ln(p)$, $x_i = G_w(z_i) = w z_i$, and $D(x, s) = \\sigma(h(x,s))$ is the discriminator output.\nThe derivative of the loss with respect to the generator parameter $w$ is:\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{dw} \\left[ -\\ln\\left(D(x_{i}, s_{i})\\right) \\right]$$\nLet's analyze a single term in the summation. Using the chain rule, we have:\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\frac{dD_i}{dw}$$\nwhere we use the shorthand $D_i = D(x_i, s_i)$.\n\nThe discriminator output is $D_i = \\sigma(h_i)$, where $h_i = h(x_i, s_i) = a x_i + b - \\lambda s_i$. The derivative of the sigmoid function $\\sigma(u)$ is $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$. Applying the chain rule again:\n$$\\frac{dD_i}{dw} = \\frac{d}{dw} \\sigma(h_i) = \\sigma'(h_i) \\frac{dh_i}{dw} = \\sigma(h_i)(1-\\sigma(h_i)) \\frac{dh_i}{dw} = D_i(1 - D_i) \\frac{dh_i}{dw}$$\nSubstituting this back into the derivative of the single loss term:\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\left[ D_i(1 - D_i) \\frac{dh_i}{dw} \\right] = -(1 - D_i) \\frac{dh_i}{dw}$$\nThis is a standard result for the non-saturating generator loss.\n\nNext, we need to find the derivative of the discriminator logit $h_i$ with respect to $w$:\n$$h_i = a x_i + b - \\lambda s_i$$\n$$\\frac{dh_i}{dw} = \\frac{d}{dw} (a x_i + b - \\lambda s_i) = a \\frac{dx_i}{dw} - \\lambda \\frac{ds_i}{dw}$$\nGiven $x_i = w z_i$, its derivative is $\\frac{dx_i}{dw} = z_i$.\n\nNow, we compute the derivative of the similarity summary term $s_i$:\n$$s_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j})$$\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\frac{d}{dw} c(x_i, x_j)$$\nThe similarity function is $c(x, x') = \\exp(-\\beta (x - x')^2)$. Its derivative with respect to $w$ is found using the chain rule:\n$$\\frac{d}{dw} c(x_i, x_j) = \\exp(-\\beta (x_i - x_j)^2) \\cdot \\frac{d}{dw} (-\\beta (x_i - x_j)^2)$$\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta (x_i - x_j)) \\cdot \\frac{d}{dw}(x_i - x_j)$$\nWith $x_i = w z_i$ and $x_j = w z_j$, we have $x_i - x_j = w(z_i - z_j)$, so $\\frac{d}{dw}(x_i - x_j) = z_i - z_j$.\nSubstituting this in, we get:\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta w(z_i - z_j)) \\cdot (z_i - z_j) = -2\\beta w (z_i - z_j)^2 c(x_i, x_j)$$\nTherefore, the derivative of $s_i$ is:\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\left[ -2\\beta w (z_i - z_j)^2 c(x_i, x_j) \\right] = -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\nNow we substitute this into the expression for $\\frac{dh_i}{dw}$:\n$$\\frac{dh_i}{dw} = a z_i - \\lambda \\left[ -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right] = a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\nFinally, we assemble the complete expression for $\\frac{d L_{G}}{d w}$:\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} -(1 - D(x_i, s_i)) \\left[ a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right]$$\nwhere $x_i = w z_i$, $s_i = \\sum_{j \\neq i} c(x_i, x_j)$, $c(x_i, x_j) = \\exp(-\\beta(x_i-x_j)^2)$, and $D(x_i, s_i) = \\sigma(a x_i + b - \\lambda s_i)$. This is the desired closed-form expression.\n\n### Task 2: Numerical Evaluation of the Gradient\n\nWe are given the following values:\n$n = 3$, $z_1 = -1$, $z_2 = 0$, $z_3 = 1$.\n$a = 1.5$, $b = 0$, $\\lambda = 2$, $\\beta = 0.5$, and we evaluate at $w = 1$.\n\nAt $w = 1$, the generated samples are $x_i = 1 \\cdot z_i = z_i$, so $x_1 = -1$, $x_2 = 0$, $x_3 = 1$.\n\nFirst, compute the similarity terms $c(x_i, x_j) = \\exp(-0.5(x_i - x_j)^2)$:\n$c(x_1, x_2) = c(x_2, x_1) = \\exp(-0.5(-1 - 0)^2) = \\exp(-0.5)$.\n$c(x_1, x_3) = c(x_3, x_1) = \\exp(-0.5(-1 - 1)^2) = \\exp(-0.5(-2)^2) = \\exp(-2)$.\n$c(x_2, x_3) = c(x_3, x_2) = \\exp(-0.5(0 - 1)^2) = \\exp(-0.5)$.\n\nNext, compute the per-sample similarity summaries $s_i = \\sum_{j \\neq i} c(x_i, x_j)$:\n$s_1 = c(x_1, x_2) + c(x_1, x_3) = \\exp(-0.5) + \\exp(-2)$.\n$s_2 = c(x_2, x_1) + c(x_2, x_3) = \\exp(-0.5) + \\exp(-0.5) = 2\\exp(-0.5)$.\n$s_3 = c(x_3, x_1) + c(x_3, x_2) = \\exp(-2) + \\exp(-0.5) = s_1$.\n\nNow compute the discriminator logits $h_i = a x_i + b - \\lambda s_i = 1.5 x_i - 2 s_i$:\n$h_1 = 1.5(-1) - 2(\\exp(-0.5) + \\exp(-2))$.\n$h_2 = 1.5(0) - 2(2\\exp(-0.5)) = -4\\exp(-0.5)$.\n$h_3 = 1.5(1) - 2(\\exp(-0.5) + \\exp(-2))$.\n\nWe also need the terms in the square brackets from the Task 1 result, let's call it $K_i$:\n$K_i = a z_i + 2\\lambda\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2(2)(0.5)(1) \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2 \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$.\n$K_1 = 1.5(-1) + 2\\left[(z_1-z_2)^2 c(x_1,x_2) + (z_1-z_3)^2 c(x_1,x_3)\\right] = -1.5 + 2\\left[(-1)^2 \\exp(-0.5) + (-2)^2 \\exp(-2)\\right] = -1.5 + 2\\exp(-0.5) + 8\\exp(-2)$.\n$K_2 = 1.5(0) + 2\\left[(z_2-z_1)^2 c(x_2,x_1) + (z_2-z_3)^2 c(x_2,x_3)\\right] = 2\\left[(1)^2 \\exp(-0.5) + (-1)^2 \\exp(-0.5)\\right] = 4\\exp(-0.5)$.\n$K_3 = 1.5(1) + 2\\left[(z_3-z_1)^2 c(x_3,x_1) + (z_3-z_2)^2 c(x_3,x_2)\\right] = 1.5 + 2\\left[(2)^2 \\exp(-2) + (1)^2 \\exp(-0.5)\\right] = 1.5 + 8\\exp(-2) + 2\\exp(-0.5)$.\n\nNow, we perform numerical evaluation:\n$\\exp(-0.5) \\approx 0.606531$\n$\\exp(-2) \\approx 0.135335$\n\n$s_1 = s_3 \\approx 0.606531 + 0.135335 = 0.741866$.\n$s_2 \\approx 2 \\times 0.606531 = 1.213062$.\n\n$h_1 \\approx -1.5 - 2(0.741866) = -2.983732$.\n$h_2 \\approx -4(0.606531) = -2.426124$.\n$h_3 \\approx 1.5 - 2(0.741866) = 0.016268$.\n\n$K_1 \\approx -1.5 + 2(0.606531) + 8(0.135335) = -1.5 + 1.213062 + 1.08268 = 0.795742$.\n$K_2 \\approx 4(0.606531) = 2.426124$.\n$K_3 \\approx 1.5 + 8(0.135335) + 2(0.606531) = 1.5 + 1.08268 + 1.213062 = 3.795742$.\n\nThe gradient is $\\frac{d L_G}{d w} = \\frac{1}{3} \\sum_{i=1}^3 -(1 - \\sigma(h_i)) K_i = \\frac{1}{3} \\sum_{i=1}^3 -\\sigma(-h_i) K_i$.\n$\\sigma(-h_1) = \\sigma(2.983732) = \\frac{1}{1 + \\exp(-2.983732)} \\approx 0.951825$.\n$\\sigma(-h_2) = \\sigma(2.426124) = \\frac{1}{1 + \\exp(-2.426124)} \\approx 0.918790$.\n$\\sigma(-h_3) = \\sigma(-0.016268) = \\frac{1}{1 + \\exp(0.016268)} \\approx 0.495935$.\n\nThe terms in the sum are:\nTerm 1: $-\\sigma(-h_1) K_1 \\approx -0.951825 \\times 0.795742 \\approx -0.75747$.\nTerm 2: $-\\sigma(-h_2) K_2 \\approx -0.918790 \\times 2.426124 \\approx -2.22940$.\nTerm 3: $-\\sigma(-h_3) K_3 \\approx -0.495935 \\times 3.795742 \\approx -1.88251$.\n\nSum $= -0.75747 - 2.22940 - 1.88251 = -4.86938$.\n$\\frac{d L_G}{d w} = \\frac{-4.86938}{3} \\approx -1.623127$.\n\nRounding to four significant figures, the result is $-1.623$.\n\n### Task 3: Qualitative Explanation\n\nThe goal of the generator is to produce samples that the discriminator classifies as real. For the non-saturating loss, this means the generator aims to maximize the discriminator's output probability $D(x_i, s_i) = \\sigma(h_i)$, which is equivalent to maximizing the logit $h_i = a x_i + b - \\lambda s_i$.\n\nThe minibatch discrimination term enters the logit as $-\\lambda s_i$, where $s_i = \\sum_{j \\neq i} \\exp(-\\beta (x_i-x_j)^2)$. The term $s_i$ is a measure of similarity of sample $x_i$ to other samples in the minibatch. If the generator produces samples that are very close to each other (low diversity, potential mode collapse), the distances $|x_i-x_j|$ will be small, making the exponential term $\\exp(-\\beta(x_i-x_j)^2)$ close to $1$. This results in a large $s_i$, which, due to the $-\\lambda$ coefficient (with $\\lambda > 0$), significantly penalizes the logit $h_i$. A lower logit means the discriminator is more likely to classify the sample as fake. To counteract this and fool the discriminator, the generator is forced to produce samples that are far apart from each other, i.e., to increase the distances $|x_i-x_j|$. This directly encourages diversity within the minibatch.\n\nThe gradient derived in Task 1 reflects this. The gradient update for $w$ is driven by terms like $(1 - D_i) \\lambda (-\\frac{ds_i}{dw})$. Since $\\frac{ds_i}{dw} = -2\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$ is generally negative (for $w>0$), this part of the total gradient is negative. In gradient descent ($w \\leftarrow w - \\eta \\frac{dL_G}{dw}$), a negative gradient causes an increase in $w$. Increasing $w$ scales all distances $|x_i-x_j|=|w(z_i-z_j)|$ up, effectively pushing samples apart and promoting diversity.\n\nA significant pitfall arises when the minibatch size $n$ is small compared to the number of distinct modes in the true data distribution. Minibatch discrimination enforces diversity *locally*, within each batch. If $n$ is too small, a single batch may only contain samples from one or a few of the true modes. The mechanism will still try to force these samples apart, which can be detrimental. It might prevent the generator from learning the true (often small) variance within a single mode, effectively distorting the learned distribution. Furthermore, it does not prevent inter-batch mode collapse. The generator could learn to produce diverse samples for one mode in one batch, and diverse samples for the *same* mode in the next batch, never discovering the other modes of the data distribution. Thus, while it is effective at preventing complete collapse to a single point, its ability to ensure coverage of all data modes is limited by the minibatch size.", "answer": "$$\\boxed{-1.623}$$", "id": "3127206"}, {"introduction": "Beyond the architecture and loss function, the stability of GAN training is fundamentally tied to the dynamics of the optimization algorithm itself. The interplay between the generator and discriminator updates can be viewed as a dynamical system, which can either converge to a stable equilibrium or spiral out of control. This advanced exercise models the GAN training process as a simple bilinear game to analyze and contrast two update schemes: simultaneous gradient descent-ascent and the Two Time-Scale Update Rule (TTUR). Through spectral analysis, you will uncover why one scheme is inherently unstable while the other can be stabilized, offering deep insight into the critical role of the update schedule in successful GAN training [@problem_id:3127265].", "problem": "Consider a zero-sum Generative Adversarial Network (GAN) game with a bilinear value function given by $$V(\\theta,\\phi)=\\theta^{\\top}A\\phi,$$ where $\\theta\\in\\mathbb{R}^{n}$, $\\phi\\in\\mathbb{R}^{m}$, and $A\\in\\mathbb{R}^{n\\times m}$. The generator minimizes $V$ and the discriminator maximizes $V$. Let the generator learning rate be $\\eta_{G}>0$ and the discriminator learning rate be $\\eta_{D}>0$, and define the ratio $$r=\\frac{\\eta_{D}}{\\eta_{G}}>0.$$\n\nAnalyze two discrete-time training schemes linearized around the saddle point $(\\theta,\\phi)=(0,0)$:\n1. One-time-scale simultaneous gradient descent-ascent:\n$$\\theta_{k+1}=\\theta_{k}-\\eta_{G}\\nabla_{\\theta}V(\\theta_{k},\\phi_{k})=\\theta_{k}-\\eta_{G}A\\phi_{k},\\quad \\phi_{k+1}=\\phi_{k}+\\eta_{D}\\nabla_{\\phi}V(\\theta_{k},\\phi_{k})=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k}.$$\n2. Two Time-Scale Update Rule (TTUR; alternating updates): first update the discriminator, then the generator,\n$$\\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k},\\quad \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1}.$$\n\nStarting from the definitions of gradient descent-ascent and the spectral radius of a linear operator, use the singular value decomposition of $A$ to analyze the linearized dynamics along the right-left singular vector pairs of $A$. Derive the iteration matrices for both schemes and obtain the spectral radius in terms of the singular values of $A$ and the step sizes. From first principles, determine whether there exists any positive ratio $r$ that yields local stabilization in the sense of non-expansive linear dynamics (spectral radius less than or equal to $1$) for the alternating TTUR scheme, and compare this with the simultaneous scheme. Report the minimal achievable spectral radius (over all $r>0$) for the alternating TTUR scheme as a single real number. No rounding is required.", "solution": "The problem requires an analysis of the local stability of two discrete-time training schemes for a Generative Adversarial Network (GAN) with a bilinear value function $V(\\theta,\\phi)=\\theta^{\\top}A\\phi$. The stability is determined by the spectral radius of the iteration matrix for the dynamics linearized around the saddle point $(\\theta,\\phi)=(0,0)$.\n\nLet the state of the system at step $k$ be the concatenated vector $z_k = \\begin{pmatrix} \\theta_k \\\\ \\phi_k \\end{pmatrix}$. The updates are linear, so they can be expressed in the form $z_{k+1} = M z_k$ for an appropriate iteration matrix $M$. A linear system is non-expansive (a form of stability) if the spectral radius of its iteration matrix, $\\rho(M)$, is less than or equal to $1$.\n\nTo analyze the matrices, we use the Singular Value Decomposition (SVD) of the matrix $A$, which is $A = U\\Sigma V^{\\top}$. Here, $U \\in \\mathbb{R}^{n\\times n}$ and $V \\in \\mathbb{R}^{m\\times m}$ are orthogonal matrices whose columns are the left-singular vectors ($u_i$) and right-singular vectors ($v_i$) of $A$, respectively. $\\Sigma \\in \\mathbb{R}^{n\\times m}$ is a rectangular diagonal matrix containing the non-negative singular values $\\sigma_i$. The singular vectors satisfy $Av_i = \\sigma_i u_i$ and $A^{\\top}u_i = \\sigma_i v_i$. By projecting the dynamics onto the basis of singular vectors, the system decouples into a set of independent $2 \\times 2$ systems, one for each singular value $\\sigma_i$. We analyze the dynamics of the coefficients of $\\theta_k$ and $\\phi_k$ in these bases.\n\n**1. Simultaneous Gradient Descent-Ascent (SGDA)**\n\nThe update rules are:\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k} $$\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\nIn matrix form, this is $z_{k+1} = M_{\\text{sim}} z_k$, with the iteration matrix:\n$$ M_{\\text{sim}} = \\begin{pmatrix} I & -\\eta_{G}A \\\\ \\eta_{D}A^{\\top} & I \\end{pmatrix} $$\nLet's analyze the dynamics for a single mode corresponding to a singular value $\\sigma_i$. We express $\\theta_k = c_k u_i$ and $\\phi_k = d_k v_i$. The updates for the coefficients $(c_k, d_k)$ become:\n$$ c_{k+1}u_i = c_k u_i - \\eta_{G} A (d_k v_i) = c_k u_i - \\eta_{G} d_k (\\sigma_i u_i) \\implies c_{k+1} = c_k - \\eta_{G}\\sigma_i d_k $$\n$$ d_{k+1}v_i = d_k v_i + \\eta_{D} A^{\\top} (c_k u_i) = d_k v_i + \\eta_{D} c_k (\\sigma_i v_i) \\implies d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\nThis yields a $2 \\times 2$ system for each mode:\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i & 1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\nThe eigenvalues $\\lambda$ of the $2 \\times 2$ matrix are given by the characteristic equation $(\\lambda-1)^2 - (-\\eta_{G}\\sigma_i)(\\eta_{D}\\sigma_i) = 0$, which simplifies to $(\\lambda-1)^2 = -\\eta_{G}\\eta_{D}\\sigma_i^2$. The eigenvalues are $\\lambda = 1 \\pm i\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}}$.\nThe magnitude of these eigenvalues is $|\\lambda| = \\sqrt{1^2 + (\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}})^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2}$.\nFor any non-zero singular value $\\sigma_i > 0$ and positive learning rates $\\eta_G, \\eta_D > 0$, we have $|\\lambda| > 1$. The spectral radius of the full system matrix $M_{\\text{sim}}$ is the maximum of these magnitudes over all singular values:\n$$ \\rho(M_{\\text{sim}}) = \\max_i \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_{\\max}^2} $$\nwhere $\\sigma_{\\max}$ is the largest singular value of $A$. Since $\\rho(M_{\\text{sim}}) > 1$ for any choice of positive learning rates, the simultaneous SGDA scheme is always locally expansive and thus unstable.\n\n**2. Two Time-Scale Update Rule (TTUR, alternating)**\n\nThe update rules are:\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1} $$\nSubstituting the first equation into the second gives the full update for $\\theta_k$:\n$$ \\theta_{k+1} = \\theta_k - \\eta_G A (\\phi_k + \\eta_D A^{\\top}\\theta_k) = (I - \\eta_G \\eta_D A A^{\\top})\\theta_k - \\eta_G A \\phi_k $$\nThe iteration matrix $M_{\\text{alt}}$ is:\n$$ M_{\\text{alt}} = \\begin{pmatrix} I - \\eta_{G}\\eta_{D}AA^{\\top} & -\\eta_{G}A \\\\ \\eta_{D}A^{\\top} & I \\end{pmatrix} $$\nAgain, we analyze the dynamics in the basis of singular vectors. Let $\\theta_k = c_k u_i$ and $\\phi_k = d_k v_i$. The coefficient updates are:\n$$ d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\n$$ c_{k+1} = c_k - \\eta_G \\sigma_i d_{k+1} = c_k - \\eta_G \\sigma_i (d_k + \\eta_D \\sigma_i c_k) = (1 - \\eta_G \\eta_D \\sigma_i^2)c_k - \\eta_G \\sigma_i d_k $$\nThe $2 \\times 2$ system for the coefficients is:\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1-\\eta_{G}\\eta_{D}\\sigma_i^2 & -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i & 1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\nLet $x_i = \\eta_G \\eta_D \\sigma_i^2$. The characteristic equation for the eigenvalues $\\lambda$ is $(1-x_i-\\lambda)(1-\\lambda) - (-\\eta_G \\sigma_i)(\\eta_D \\sigma_i) = 0$, which simplifies to $\\lambda^2 - (2-x_i)\\lambda + 1 = 0$.\nThe eigenvalues are $\\lambda = \\frac{(2-x_i) \\pm \\sqrt{(2-x_i)^2 - 4}}{2} = 1 - \\frac{x_i}{2} \\pm \\frac{\\sqrt{x_i^2 - 4x_i}}{2}$.\n\nWe analyze the magnitude $|\\lambda|$ based on the value of $x_i$:\n- **Case $0 \\le x_i \\le 4$**: The term under the square root is non-positive, so the eigenvalues are a complex conjugate pair: $\\lambda = 1 - \\frac{x_i}{2} \\pm i\\frac{\\sqrt{4x_i - x_i^2}}{2}$.\nThe squared magnitude is $|\\lambda|^2 = (1-\\frac{x_i}{2})^2 + (\\frac{\\sqrt{4x_i - x_i^2}}{2})^2 = 1 - x_i + \\frac{x_i^2}{4} + \\frac{4x_i - x_i^2}{4} = 1$. Thus, $|\\lambda|=1$.\n- **Case $x_i > 4$**: The eigenvalues are real. From Vieta's formulas, their product is $1$. This means if one has magnitude greater than $1$, the other must have magnitude less than $1$. The larger magnitude is $|\\lambda| = |\\frac{x_i}{2}-1 + \\frac{\\sqrt{x_i^2-4x_i}}{2}|$. Since for $x_i>4$, $\\frac{x_i}{2}-1>1$, this magnitude is strictly greater than $1$.\n\nFor the alternating TTUR scheme to be non-expansive, its spectral radius $\\rho(M_{\\text{alt}})$ must be less than or equal to $1$. This requires that $|\\lambda| \\le 1$ for all modes, which in turn requires $x_i \\le 4$ for all $i$. This condition must hold for the largest singular value, $\\sigma_{\\max}$.\nThe condition for non-expansive dynamics is therefore $\\eta_{G}\\eta_{D}\\sigma_{\\max}^2 \\le 4$.\nWith $r = \\eta_D/\\eta_G$, this is $r\\eta_G^2\\sigma_{\\max}^2 \\le 4$. For any positive ratio $r>0$, one can always choose the learning rate $\\eta_G$ to be sufficiently small, e.g., $\\eta_G \\le \\frac{2}{\\sigma_{\\max}\\sqrt{r}}$, to satisfy this condition. Thus, unlike the simultaneous scheme, the alternating TTUR scheme can be stabilized for any positive ratio $r$.\n\nThe problem asks for the minimal achievable spectral radius over all $r>0$. This is equivalent to finding $\\inf_{\\eta_G>0, \\eta_D>0} \\rho(M_{\\text{alt}})$.\nThe magnitude of the eigenvalues for any mode is $|\\lambda_i| \\ge 1$. The minimum value of $|\\lambda_i|=1$ is achieved when $0 \\le x_i \\le 4$. To achieve a system-wide spectral radius of $1$, we require $x_i \\le 4$ for all $i$, which is guaranteed if $x_{\\max} = \\eta_G \\eta_D \\sigma_{\\max}^2 \\le 4$. As shown, this condition is achievable for any $r>0$ by selecting small enough learning rates. When this condition is met, every mode has eigenvalues of magnitude $1$, so the spectral radius of the entire system is $\\rho(M_{\\text{alt}}) = \\max_i|\\lambda_i|=1$. Since the magnitude can never be less than $1$, the minimal achievable spectral radius is $1$.", "answer": "$$\\boxed{1}$$", "id": "3127265"}]}