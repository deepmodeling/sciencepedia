{"hands_on_practices": [{"introduction": "Training a Variational Autoencoder involves a delicate balance between reconstruction accuracy and regularization. A key component of this is the Kullback-Leibler (KL) divergence term in the objective function. This exercise [@problem_id:3197900] provides a foundational look into the practicalities of VAE optimization by contrasting the exact, analytical form of the KL divergence with its common Monte Carlo estimate. By quantifying the estimation error and the resulting gradient's signal-to-noise ratio, you will gain a concrete understanding of the sources of variance in VAE training and the importance of sample size for stable optimization.", "problem": "You are given a Variational Autoencoder (VAE) context in which the encoder defines a diagonal multivariate normal distribution $q_{\\phi}(z \\mid x)$ with parameters $\\mu \\in \\mathbb{R}^k$ and $\\sigma^2 \\in \\mathbb{R}^k_{>0}$, and the prior is the standard normal $p(z) = \\mathcal{N}(0, I_k)$. The Kullback–Leibler (KL) divergence is defined as $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{q}[\\log q(z \\mid x) - \\log p(z)]$. Your task is to compare the Kullback–Leibler (KL) divergence computed analytically to a Monte Carlo (MC) estimate, and analyze how estimator noise affects a simple proxy for training stability through gradient signal-to-noise.\n\nFundamental base to use:\n- Definition of Kullback–Leibler (KL) divergence: $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{q}[\\log q(z) - \\log p(z)]$.\n- Reparameterization: $z = \\mu + \\sigma \\odot \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0, I_k)$, where $\\odot$ denotes elementwise multiplication.\n- Log-density of a diagonal multivariate normal: for $z \\in \\mathbb{R}^k$, $\\log \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2)) = -\\tfrac{1}{2}\\left(\\sum_{i=1}^k \\tfrac{(z_i - \\mu_i)^2}{\\sigma_i^2} + \\sum_{i=1}^k \\log(2\\pi \\sigma_i^2)\\right)$.\n- Log-density of the standard normal: $\\log \\mathcal{N}(z \\mid 0, I_k) = -\\tfrac{1}{2}\\left(\\|z\\|_2^2 + k \\log(2\\pi)\\right)$.\n\nRequirements:\n1. Derive from the above base an analytical expression for $\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z))$ for a diagonal multivariate normal $q_{\\phi}$ and standard normal $p$. Use this analytical expression to compute an exact $\\mathrm{KL}$ for given parameters.\n2. Construct a Monte Carlo estimator of $\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z))$ by drawing $S$ independent samples $z^{(s)} \\sim q_{\\phi}(z \\mid x)$ using the reparameterization $z^{(s)} = \\mu + \\sigma \\odot \\epsilon^{(s)}$ with $\\epsilon^{(s)} \\sim \\mathcal{N}(0, I_k)$, and averaging $\\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)})$ over $s = 1, \\dots, S$.\n3. Define a gradient estimator for the $\\mathrm{KL}$ term with respect to $\\mu$ using the pathwise derivative. Let $g^{(s)} = \\nabla_{z}\\left[\\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)})\\right]$ and note that $\\nabla_{\\mu} z^{(s)} = I_k$. Use $g^{(s)}$ as the per-sample gradient estimate with respect to $\\mu$. For a given sample size $S$, compute the empirical mean vector $\\bar{g}$ over $S$ samples and the empirical per-coordinate standard deviation vector $\\operatorname{std}(g)$. Define the signal-to-noise ratio as $\\mathrm{SNR} = \\frac{\\|\\bar{g}\\|_2}{\\|\\operatorname{std}(g)\\|_2}$. If the denominator is $0$, define the ratio to be $0$.\n4. For each test case below, compute the following four quantities: the analytical $\\mathrm{KL}$ value, the Monte Carlo estimate of $\\mathrm{KL}$ using $S$ samples, the absolute estimation error $|\\text{MC} - \\text{Analytic}|$, and the signal-to-noise ratio $\\mathrm{SNR}$ of the gradient estimator with respect to $\\mu$ using the same $S$ samples.\n\nYou must implement a complete, runnable program that does the following:\n- Uses a fixed random seed $42$ for reproducibility.\n- For each test case, computes the four quantities as specified above.\n- Produces a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases. For each test case $i$, append the four values $K_i$, $M_i$, $E_i$, $R_i$ in that order. The final output therefore has $4 \\times N$ entries for $N$ test cases.\n\nTest suite:\n- Case $1$ (happy path, small sample): $k = 2$, $\\mu = [0.5, -0.5]$, $\\log \\sigma^2 = [-0.2, 0.3]$, $S = 1$.\n- Case $2$ (happy path, moderate sample): $k = 2$, $\\mu = [0.5, -0.5]$, $\\log \\sigma^2 = [-0.2, 0.3]$, $S = 10$.\n- Case $3$ (happy path, large sample): $k = 2$, $\\mu = [0.5, -0.5]$, $\\log \\sigma^2 = [-0.2, 0.3]$, $S = 100$.\n- Case $4$ (boundary, zero mean and unit variance): $k = 2$, $\\mu = [0.0, 0.0]$, $\\log \\sigma^2 = [0.0, 0.0]$, $S = 100$.\n- Case $5$ (edge, near-deterministic encoder dimension): $k = 1$, $\\mu = [0.1]$, $\\log \\sigma^2 = [\\log(10^{-4})]$, $S = 50$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3}]$).\n- The entries must be numeric values in the exact order: for Case $1$, $K_1, M_1, E_1, R_1$; for Case $2$, $K_2, M_2, E_2, R_2$; and so on through Case $5$.", "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It is scientifically grounded in the established theory of variational autoencoders, is well-posed with all necessary information provided, and is expressed in objective, formal language. We may therefore proceed with a complete solution.\n\nThe problem requires us to perform a comparative analysis between the analytical and Monte Carlo (MC) estimated Kullback-Leibler (KL) divergence for a Variational Autoencoder (VAE). The encoder's approximate posterior is a multivariate normal distribution with a diagonal covariance matrix, $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2))$, and the prior is a standard multivariate normal distribution, $p(z) = \\mathcal{N}(z \\mid 0, I_k)$. We are also asked to analyze the signal-to-noise ratio (SNR) of the gradient of the KL divergence with respect to the mean parameter $\\mu$.\n\n**1. Analytical Expression for the KL Divergence**\n\nThe Kullback-Leibler divergence between two distributions $q(z)$ and $p(z)$ is defined as $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{z \\sim q(z)}[\\log q(z) - \\log p(z)]$. Since both $q_{\\phi}(z \\mid x)$ and $p(z)$ are multivariate normal distributions that factorize across dimensions, the total KL divergence is the sum of the KL divergences for each dimension:\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)) = \\sum_{i=1}^k \\mathrm{KL}(\\mathcal{N}(z_i \\mid \\mu_i, \\sigma_i^2) \\,\\|\\, \\mathcal{N}(z_i \\mid 0, 1))\n$$\nFor a single dimension $i$, the KL divergence between two univariate normal distributions $\\mathcal{N}(\\mu_a, \\sigma_a^2)$ and $\\mathcal{N}(\\mu_b, \\sigma_b^2)$ is given by the formula:\n$$\n\\mathrm{KL}(\\mathcal{N}_a \\,\\|\\, \\mathcal{N}_b) = \\log\\frac{\\sigma_b}{\\sigma_a} + \\frac{\\sigma_a^2 + (\\mu_a-\\mu_b)^2}{2\\sigma_b^2} - \\frac{1}{2}\n$$\nIn our case, for each dimension $i$, we have $\\mu_a = \\mu_i$, $\\sigma_a^2 = \\sigma_i^2$, $\\mu_b = 0$, and $\\sigma_b^2 = 1$. Substituting these into the formula yields:\n$$\n\\mathrm{KL}(\\mathcal{N}(z_i \\mid \\mu_i, \\sigma_i^2) \\,\\|\\, \\mathcal{N}(z_i \\mid 0, 1)) = \\log\\frac{1}{\\sigma_i} + \\frac{\\sigma_i^2 + (\\mu_i-0)^2}{2 \\cdot 1} - \\frac{1}{2}\n$$\n$$\n= -\\log\\sigma_i + \\frac{\\mu_i^2 + \\sigma_i^2}{2} - \\frac{1}{2}\n$$\nSince $\\log\\sigma_i = \\frac{1}{2}\\log\\sigma_i^2$, we can rewrite this as:\n$$\n= \\frac{1}{2}(\\mu_i^2 + \\sigma_i^2 - \\log\\sigma_i^2 - 1)\n$$\nSumming over all $k$ dimensions, we obtain the final analytical expression for the KL divergence:\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)) = \\frac{1}{2} \\sum_{i=1}^k \\left( \\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1 \\right)\n$$\nThis expression allows for the exact computation of the KL divergence given the parameters $\\mu$ and $\\sigma^2$.\n\n**2. Monte Carlo Estimator for the KL Divergence**\n\nThe KL divergence can be estimated using Monte Carlo integration. Following the law of large numbers, the expectation $\\mathbb{E}_{z \\sim q}[f(z)]$ can be approximated by averaging $f(z)$ over a set of $S$ samples drawn from $q(z)$. Here, the function is $f(z) = \\log q_{\\phi}(z \\mid x) - \\log p(z)$.\n\nTo draw samples $z^{(s)}$ from $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2))$, we use the reparameterization trick. We first draw $S$ samples $\\epsilon^{(s)}$ from a standard normal distribution, $\\epsilon^{(s)} \\sim \\mathcal{N}(0, I_k)$, and then transform them as follows:\n$$\nz^{(s)} = \\mu + \\sigma \\odot \\epsilon^{(s)}\n$$\nwhere $\\sigma = (\\sigma_1, \\dots, \\sigma_k)$ is the vector of standard deviations and $\\odot$ denotes element-wise multiplication.\n\nThe Monte Carlo estimator for the KL divergence, $\\hat{K}_S$, is then:\n$$\n\\hat{K}_S = \\frac{1}{S} \\sum_{s=1}^S \\left[ \\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)}) \\right]\n$$\nThe required log-densities are:\n$$\n\\log q_{\\phi}(z^{(s)} \\mid x) = -\\frac{1}{2} \\left( \\sum_{i=1}^k \\frac{(z_i^{(s)} - \\mu_i)^2}{\\sigma_i^2} + \\sum_{i=1}^k \\log(2\\pi \\sigma_i^2) \\right)\n$$\n$$\n\\log p(z^{(s)}) = -\\frac{1}{2} \\left( \\sum_{i=1}^k (z_i^{(s)})^2 + k \\log(2\\pi) \\right)\n$$\nThe absolute estimation error is computed as $|\\hat{K}_S - \\mathrm{KL}_{\\text{analytic}}|$.\n\n**3. Gradient Estimator and Signal-to-Noise Ratio**\n\nWe are tasked with constructing an estimator for the gradient of the KL term with respect to $\\mu$, i.e., $\\nabla_{\\mu} \\mathrm{KL}(q_{\\phi} \\,\\|\\, p)$. The reparameterization trick is crucial here, as it allows the gradient to be passed inside the expectation.\n$$\n\\nabla_{\\mu} \\mathrm{KL} = \\nabla_{\\mu} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[\\log q(\\mu+\\sigma\\epsilon) - \\log p(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[\\nabla_{\\mu}(\\log q(z) - \\log p(z))]\n$$\nwhere $z = \\mu + \\sigma\\epsilon$. By the chain rule, $\\nabla_{\\mu}f(z) = (\\nabla_z f(z))^T \\nabla_{\\mu}z$. Since $z_i = \\mu_i + \\sigma_i \\epsilon_i$, the Jacobian $\\nabla_{\\mu}z$ is the identity matrix $I_k$. Thus, $\\nabla_{\\mu}f(z) = \\nabla_z f(z)$. The gradient is:\n$$\n\\nabla_{\\mu} \\mathrm{KL} = \\mathbb{E}_{z \\sim q}[\\nabla_z(\\log q(z) - \\log p(z))]\n$$\nThe problem defines the per-sample gradient estimate as $g^{(s)} = \\nabla_{z}(\\log q(z^{(s)}) - \\log p(z^{(s)}))$. Let's derive its components, $g_i(z) = \\frac{\\partial}{\\partial z_i}(\\log q(z) - \\log p(z))$.\n$$\n\\frac{\\partial}{\\partial z_i} \\log q(z) = \\frac{\\partial}{\\partial z_i} \\left[ -\\frac{1}{2} \\sum_{j=1}^k \\left( \\frac{(z_j - \\mu_j)^2}{\\sigma_j^2} + \\log(2\\pi\\sigma_j^2) \\right) \\right] = -\\frac{z_i - \\mu_i}{\\sigma_i^2}\n$$\n$$\n\\frac{\\partial}{\\partial z_i} \\log p(z) = \\frac{\\partial}{\\partial z_i} \\left[ -\\frac{1}{2} \\sum_{j=1}^k (z_j^2 + \\log(2\\pi)) \\right] = -z_i\n$$\nCombining these terms gives the expression for the per-sample gradient component:\n$$\ng_i(z) = -\\frac{z_i - \\mu_i}{\\sigma_i^2} - (-z_i) = z_i - \\frac{z_i - \\mu_i}{\\sigma_i^2}\n$$\nUsing the reparameterization $z_i-\\mu_i = \\sigma_i \\epsilon_i$, we can simplify this to $g_i(z) = z_i - \\epsilon_i / \\sigma_i$.\n\nThe Monte Carlo estimate of the gradient is the empirical mean of these per-sample gradients, $\\bar{g} = \\frac{1}{S} \\sum_{s=1}^S g(z^{(s)})$. The empirical per-coordinate standard deviation vector, $\\operatorname{std}(g)$, is calculated over the $S$ samples for each dimension of the gradient. The signal-to-noise ratio is then defined as:\n$$\n\\mathrm{SNR} = \\frac{\\|\\bar{g}\\|_2}{\\|\\operatorname{std}(g)\\|_2}\n$$\nIf the denominator $\\|\\operatorname{std}(g)\\|_2$ is zero (which occurs if $S=1$ or if the gradient samples have zero variance), the SNR is defined to be $0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analytical KL divergence, a Monte Carlo estimate, the absolute error,\n    and a gradient signal-to-noise ratio for a VAE setting for several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, mu, log_sigma_sq, S)\n        (2, [0.5, -0.5], [-0.2, 0.3], 1),\n        (2, [0.5, -0.5], [-0.2, 0.3], 10),\n        (2, [0.5, -0.5], [-0.2, 0.3], 100),\n        (2, [0.0, 0.0], [0.0, 0.0], 100),\n        (1, [0.1], [np.log(1e-4)], 50),\n    ]\n\n    # Use a fixed random seed for reproducibility.\n    rng = np.random.default_rng(42)\n    \n    results = []\n\n    for k, mu_list, log_sigma_sq_list, S in test_cases:\n        mu = np.array(mu_list, dtype=np.float64)\n        log_sigma_sq = np.array(log_sigma_sq_list, dtype=np.float64)\n        \n        sigma_sq = np.exp(log_sigma_sq)\n        sigma = np.sqrt(sigma_sq)\n        \n        # 1. Analytical KL divergence (K_i)\n        kl_analytic = 0.5 * np.sum(mu**2 + sigma_sq - log_sigma_sq - 1)\n        \n        # Draw S samples for epsilon from a standard normal distribution\n        epsilons = rng.normal(size=(S, k))\n        \n        # Apply reparameterization trick to get samples for z\n        z_samples = mu + sigma * epsilons\n        \n        # 2. Monte Carlo estimate of KL divergence (M_i)\n        # log q(z|x) term for each sample\n        log_q_z = -0.5 * np.sum(epsilons**2 + log_sigma_sq + np.log(2 * np.pi), axis=1)\n        \n        # log p(z) term for each sample\n        log_p_z = -0.5 * np.sum(z_samples**2 + np.log(2 * np.pi), axis=1)\n        \n        # KL term for each sample and the final MC estimate\n        kl_terms = log_q_z - log_p_z\n        kl_mc = np.mean(kl_terms)\n        \n        # 3. Absolute estimation error |MC - Analytic| (E_i)\n        abs_error = np.abs(kl_mc - kl_analytic)\n        \n        # 4. Signal-to-noise ratio of the gradient estimator (R_i)\n        # Per-sample gradient estimates g(z^(s))\n        g_samples = z_samples - epsilons / sigma\n        \n        # Empirical mean of the gradient\n        g_bar = np.mean(g_samples, axis=0)\n        \n        # Empirical per-coordinate standard deviation of the gradient (using ddof=0 for population std of the sample)\n        # For S=1, this will correctly result in a vector of zeros.\n        g_std = np.std(g_samples, axis=0, ddof=0)\n        \n        # L2-norms of the mean and std vectors\n        norm_g_bar = np.linalg.norm(g_bar)\n        norm_g_std = np.linalg.norm(g_std)\n        \n        # SNR calculation, handling the case of zero standard deviation\n        snr = norm_g_bar / norm_g_std if norm_g_std > 0 else 0.0\n        \n        results.extend([kl_analytic, kl_mc, abs_error, snr])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197900"}, {"introduction": "The power of a VAE lies in its ability to learn a rich, probabilistic latent space, a feature enforced by the Kullback-Leibler (KL) divergence term in the Evidence Lower Bound (ELBO). But what happens if this regularization is weakened or removed? This practice [@problem_id:3197922] delves into this question by analyzing the behavior of the ELBO as the encoder's variance approaches zero, effectively becoming deterministic. Through an analytical investigation, you will discover precisely how the KL term acts as a regularizer, preventing the model from ignoring the latent space and collapsing into a simple autoencoder.", "problem": "Consider a one-dimensional Variational Autoencoder (VAE) with the following components. The latent variable is $z \\in \\mathbb{R}$, the observation is $x \\in \\mathbb{R}$, and all distributions are univariate Gaussians. The prior is $p(z) = \\mathcal{N}(0,1)$. The encoder (approximate posterior) is $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\sigma_{\\phi}(x)^2)$ with encoder mean $\\mu_{\\phi}(x) \\in \\mathbb{R}$ and encoder standard deviation $\\sigma_{\\phi}(x) \\in \\mathbb{R}_{>0}$. The decoder (likelihood) is $p_{\\theta}(x \\mid z) = \\mathcal{N}(w z, \\sigma_x^2)$ with a fixed scalar weight $w \\in \\mathbb{R}$ and fixed observation noise standard deviation $\\sigma_x \\in \\mathbb{R}_{>0}$. Define the Evidence Lower Bound (ELBO) with a Kullback–Leibler (KL) weight $\\beta \\in \\mathbb{R}_{\\ge 0}$ as\n$$\n\\mathcal{L}_{\\beta}(x) \\;=\\; \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right] \\;-\\; \\beta \\, \\mathrm{KL}\\!\\left( q_{\\phi}(z \\mid x) \\parallel p(z) \\right).\n$$\nYou will analyze the behavior of $\\mathcal{L}_{\\beta}(x)$ when the encoder noise tends to zero, that is, when $\\sigma_{\\phi}(x) \\to 0$. Use only fundamental definitions of Gaussian log-likelihood, expectations, and Kullback–Leibler divergence as your starting point, and derive any closed forms you need from these definitions. You must not rely on any unstated shortcuts.\n\nTask. For each given test case below, assume a fixed encoder standard deviation $\\sigma_{\\phi}(x) = s$ and optimize the ELBO with respect to the encoder mean $\\mu_{\\phi}(x)$ while holding $s$, $w$, and $\\sigma_x$ fixed. That is, for each set of parameters $(x, w, \\sigma_x, s, \\beta)$:\n- Compute the encoder mean $\\mu^{\\star}$ that maximizes $\\mathcal{L}_{\\beta}(x)$ with respect to $\\mu_{\\phi}(x)$, given $s$ is fixed.\n- Using this $\\mu^{\\star}$ and the given parameters, compute the corresponding value of $\\mathcal{L}_{\\beta}(x)$ exactly, using the definition of the Gaussian log-likelihood and the definition of the Kullback–Leibler divergence between univariate Gaussians. The computation must be analytic (that is, do not use Monte Carlo sampling).\n- Your program should output, for each test case, the pair $[\\mathcal{L}_{\\beta}(x), \\mu^{\\star}]$ as floating-point numbers.\n\nInterpretation goal. By comparing the results across test cases where $s$ is very small and where $\\beta$ varies, your outputs should reveal how $\\mathcal{L}_{\\beta}(x)$ behaves as $s \\to 0$, and how small encoder noise can lead to overfitting and lack of regularization in $z$ when $\\beta$ is small or zero.\n\nTest suite. Use exactly the following five test cases, each specified as $(x, w, \\sigma_x, s, \\beta)$:\n- Case $1$: $(x, w, \\sigma_x, s, \\beta) = (\\,1.0,\\, 1.0,\\, 0.5,\\, 10^{-6},\\, 1.0\\,)$.\n- Case $2$: $(x, w, \\sigma_x, s, \\beta) = (\\,1.0,\\, 1.0,\\, 0.5,\\, 10^{-12},\\, 1.0\\,)$.\n- Case $3$: $(x, w, \\sigma_x, s, \\beta) = (\\,1.0,\\, 1.0,\\, 0.5,\\, 10^{-6},\\, 0.1\\,)$.\n- Case $4$: $(x, w, \\sigma_x, s, \\beta) = (\\,3.0,\\, 0.1,\\, 0.5,\\, 10^{-6},\\, 0.0\\,)$.\n- Case $5$: $(x, w, \\sigma_x, s, \\beta) = (\\,2.0,\\, 0.0,\\, 0.5,\\, 10^{-6},\\, 1.0\\,)$.\n\nCoverage rationale:\n- Case $1$ is a general baseline with moderate parameters.\n- Case $2$ decreases $s$ further toward zero to expose the behavior of $\\mathcal{L}_{\\beta}(x)$ as $\\sigma_{\\phi}(x) \\to 0$ under regularization.\n- Case $3$ reduces $\\beta$ to show weaker regularization.\n- Case $4$ uses $\\beta = 0$ to remove regularization and a small $w$ to highlight that $\\mu^{\\star}$ can become large in magnitude while the objective does not penalize $z$.\n- Case $5$ sets $w = 0$ so the decoder ignores $z$, testing the boundary where the latent is unused and the KL term dominates the $\\sigma_{\\phi}(x) \\to 0$ behavior.\n\nFinal output format. Your program should produce a single line containing the results for the five test cases, in order, as a comma-separated list of pairs with no additional text, for example:\n$[[r_{1,1},r_{1,2}],[r_{2,1},r_{2,2}],[r_{3,1},r_{3,2}],[r_{4,1},r_{4,2}],[r_{5,1},r_{5,2}]]$\nwhere $r_{i,1}$ is $\\mathcal{L}_{\\beta}(x)$ for case $i$ and $r_{i,2}$ is $\\mu^{\\star}$ for case $i$. All $r_{i,j}$ must be floating-point numbers.", "solution": "The problem asks for the analysis of a one-dimensional Variational Autoencoder (VAE). We are given the model's components and the objective function, the Evidence Lower Bound (ELBO), which we must maximize with respect to the encoder's mean parameter.\n\nLet us first state the given quantities.\nThe observation is $x \\in \\mathbb{R}$ and the latent variable is $z \\in \\mathbb{R}$.\nThe prior distribution over the latent variable is a standard normal distribution:\n$$p(z) = \\mathcal{N}(z \\mid 0, 1)$$\nThe encoder, or approximate posterior, is a Gaussian distribution whose parameters are functions of the input $x$:\n$$q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{\\phi}(x), \\sigma_{\\phi}(x)^2)$$\nFor this problem, the encoder's standard deviation is fixed to a constant $s \\in \\mathbb{R}_{>0}$, so $\\sigma_{\\phi}(x) = s$. We will denote the mean as $\\mu \\equiv \\mu_{\\phi}(x)$ for simplicity, as it is the variable we will optimize.\n$$q(z \\mid x) = \\mathcal{N}(z \\mid \\mu, s^2)$$\nThe decoder, or likelihood, is also a Gaussian distribution:\n$$p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid wz, \\sigma_x^2)$$\nHere, the weight $w \\in \\mathbb{R}$ and observation noise standard deviation $\\sigma_x \\in \\mathbb{R}_{>0}$ are fixed parameters.\n\nThe $\\beta$-VAE objective function, or ELBO, is defined as:\n$$\n\\mathcal{L}_{\\beta}(x) \\;=\\; \\mathbb{E}_{q(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right] \\;-\\; \\beta \\, \\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right)\n$$\nwhere $\\beta \\in \\mathbb{R}_{\\ge 0}$ is a weight for the Kullback–Leibler (KL) divergence term. Our task is to find the optimal mean $\\mu^{\\star}$ that maximizes this objective for a given set of parameters $(x, w, \\sigma_x, s, \\beta)$ and then compute the value of $\\mathcal{L}_{\\beta}(x)$ at this optimum.\n\nWe will derive the closed-form expression for each of the two terms in the ELBO.\n\n**1. The Reconstruction Term: $\\mathbb{E}_{q(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right]$**\n\nThe log-likelihood of the decoder is:\n$$\n\\log p_{\\theta}(x \\mid z) = \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left(-\\frac{(x - wz)^2}{2\\sigma_x^2}\\right) \\right) = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{(x - wz)^2}{2\\sigma_x^2}\n$$\nWe need to compute the expectation of this quantity with respect to the encoder distribution $q(z \\mid x) = \\mathcal{N}(z \\mid \\mu, s^2)$. Let $\\mathbb{E}_{q}[\\cdot]$ denote this expectation.\n$$\n\\mathbb{E}_{q}\\left[ \\log p_{\\theta}(x \\mid z) \\right] = \\mathbb{E}_{q}\\left[ -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{(x - wz)^2}{2\\sigma_x^2} \\right]\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}_{q}\\left[ \\log p_{\\theta}(x \\mid z) \\right] = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2}\\mathbb{E}_{q}\\left[ (x - wz)^2 \\right]\n$$\nLet's expand the squared term inside the expectation:\n$$\n\\mathbb{E}_{q}\\left[ (x - wz)^2 \\right] = \\mathbb{E}_{q}\\left[ x^2 - 2xwz + w^2z^2 \\right] = x^2 - 2xw\\mathbb{E}_{q}[z] + w^2\\mathbb{E}_{q}[z^2]\n$$\nFor a random variable $z \\sim \\mathcal{N}(\\mu, s^2)$, we have $\\mathbb{E}_{q}[z] = \\mu$ and $\\mathrm{Var}_{q}(z) = s^2$. The second moment is $\\mathbb{E}_{q}[z^2] = \\mathrm{Var}_{q}(z) + (\\mathbb{E}_{q}[z])^2 = s^2 + \\mu^2$.\nSubstituting these back:\n$$\n\\mathbb{E}_{q}\\left[ (x - wz)^2 \\right] = x^2 - 2xw\\mu + w^2(s^2 + \\mu^2)\n$$\nSo, the full reconstruction term is:\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right] = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\left[ x^2 - 2xw\\mu + w^2(s^2 + \\mu^2) \\right]\n$$\n\n**2. The KL Divergence Term: $\\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right)$**\n\nWe need the KL divergence between two univariate Gaussian distributions, $q(z \\mid x) = \\mathcal{N}(\\mu, s^2)$ and $p(z) = \\mathcal{N}(0, 1)$. The general definition is $\\mathrm{KL}(q \\parallel p) = \\int q(z) \\log \\frac{q(z)}{p(z)} dz$. The closed-form solution is:\n$$\n\\mathrm{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\parallel \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nSubstituting our parameters ($\\mu_1 = \\mu, \\sigma_1 = s, \\mu_2 = 0, \\sigma_2 = 1$):\n$$\n\\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right) = \\log\\frac{1}{s} + \\frac{s^2 + (\\mu-0)^2}{2 \\cdot 1^2} - \\frac{1}{2} = -\\log s + \\frac{s^2 + \\mu^2}{2} - \\frac{1}{2}\n$$\nThis can be rewritten using $\\log s = \\frac{1}{2}\\log s^2$:\n$$\n\\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right) = \\frac{1}{2} \\left( \\mu^2 + s^2 - \\log s^2 - 1 \\right)\n$$\n\n**3. Maximizing the ELBO**\n\nNow we assemble the full ELBO expression as a function of $\\mu$:\n$$\n\\mathcal{L}_{\\beta}(\\mu) = \\left( -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2} [x^2 - 2xw\\mu + w^2(s^2 + \\mu^2)] \\right) - \\frac{\\beta}{2} \\left( \\mu^2 + s^2 - \\log s^2 - 1 \\right)\n$$\nTo find the optimal mean $\\mu^{\\star}$ that maximizes $\\mathcal{L}_{\\beta}(\\mu)$, we take the derivative with respect to $\\mu$ and set it to zero.\n$$\n\\frac{\\partial\\mathcal{L}_{\\beta}}{\\partial\\mu} = \\frac{\\partial}{\\partial\\mu} \\left( -\\frac{1}{2\\sigma_x^2} [-2xw\\mu + w^2\\mu^2] - \\frac{\\beta}{2}\\mu^2 \\right)\n$$\n$$\n\\frac{\\partial\\mathcal{L}_{\\beta}}{\\partial\\mu} = -\\frac{1}{2\\sigma_x^2}(-2xw + 2w^2\\mu) - \\beta\\mu = \\frac{xw}{\\sigma_x^2} - \\frac{w^2}{\\sigma_x^2}\\mu - \\beta\\mu\n$$\nSetting the derivative to zero to find the optimal $\\mu^{\\star}$:\n$$\n\\frac{xw}{\\sigma_x^2} - \\left(\\frac{w^2}{\\sigma_x^2} + \\beta\\right)\\mu^{\\star} = 0\n$$\nSolving for $\\mu^{\\star}$:\n$$\n\\mu^{\\star} = \\frac{xw/\\sigma_x^2}{w^2/\\sigma_x^2 + \\beta} = \\frac{xw}{w^2 + \\beta\\sigma_x^2}\n$$\nThe second derivative $\\frac{\\partial^2\\mathcal{L}_{\\beta}}{\\partial\\mu^2} = -\\frac{w^2}{\\sigma_x^2} - \\beta$ is non-positive, and strictly negative for the given test cases (since $w$ and $\\beta$ are not simultaneously zero), confirming that $\\mu^{\\star}$ corresponds to a maximum.\n\n**4. Final Calculation**\n\nFor each test case $(x, w, \\sigma_x, s, \\beta)$, we perform the following two steps:\n1.  Compute the optimal encoder mean: $\\mu^{\\star} = \\frac{xw}{w^2 + \\beta\\sigma_x^2}$.\n2.  Substitute $\\mu = \\mu^{\\star}$ into the full ELBO expression to get the maximum value:\n    $$\n    \\mathcal{L}_{\\beta}(x) = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\left[ x^2 - 2xw\\mu^{\\star} + w^2(s^2 + (\\mu^{\\star})^2) \\right] - \\frac{\\beta}{2} \\left( (\\mu^{\\star})^2 + s^2 - 2\\log s - 1 \\right)\n    $$\nThese computations will be implemented in the provided Python script.\nThe behavior as $s \\to 0$ merits comment. For $\\beta > 0$, the term $-\\frac{\\beta}{2}(-2\\log s)$ dominates, causing $\\mathcal{L}_{\\beta}(x) \\to -\\infty$. This penalizes the encoder for becoming overly confident (i.e., having a variance near zero). For $\\beta=0$, this penalty vanishes, and $\\mathcal{L}_{\\beta}(x)$ converges to a finite value. In this case, $\\mu^{\\star}=x/w$ (if $w \\ne 0$), which corresponds to deterministically inverting the decoder's mean function, a form of overfitting in the latent space.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal encoder mean and the corresponding ELBO for a 1D VAE\n    for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x, w, sigma_x, s, beta)\n    test_cases = [\n        (1.0, 1.0, 0.5, 1e-6, 1.0),\n        (1.0, 1.0, 0.5, 1e-12, 1.0),\n        (1.0, 1.0, 0.5, 1e-6, 0.1),\n        (3.0, 0.1, 0.5, 1e-6, 0.0),\n        (2.0, 0.0, 0.5, 1e-6, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        x, w, sigma_x, s, beta = case\n        \n        # Calculate the optimal encoder mean mu_star\n        # mu_star = (x * w) / (w^2 + beta * sigma_x^2)\n        sigma_x_sq = sigma_x**2\n        w_sq = w**2\n        mu_star_numerator = x * w\n        mu_star_denominator = w_sq + beta * sigma_x_sq\n        \n        # Handle the case where the denominator could be zero, although not in the test cases\n        if mu_star_denominator == 0:\n            # If w=0 and beta=0, the ELBO is independent of mu. Any mu is optimal.\n            # We can choose mu_star = 0 for convention.\n            mu_star = 0.0\n        else:\n            mu_star = mu_star_numerator / mu_star_denominator\n            \n        # Calculate the two components of the ELBO\n        \n        # 1. Reconstruction Term: E_q[log p(x|z)]\n        # = -0.5*log(2*pi*sigma_x^2) - (1/(2*sigma_x^2)) * [x^2 - 2*x*w*mu + w^2*(s^2 + mu^2)]\n        s_sq = s**2\n        mu_star_sq = mu_star**2\n        \n        # The term inside the expectation's main part\n        recon_exp_term = x**2 - 2 * x * w * mu_star + w_sq * (s_sq + mu_star_sq)\n        \n        # The full reconstruction term\n        recon_term = -0.5 * np.log(2 * np.pi * sigma_x_sq) - (1 / (2 * sigma_x_sq)) * recon_exp_term\n        \n        # 2. KL Divergence Term: KL(q(z|x) || p(z))\n        # = 0.5 * (mu^2 + s^2 - log(s^2) - 1)\n        # log(s^2) = 2 * log(s)\n        kl_divergence = 0.5 * (mu_star_sq + s_sq - 2 * np.log(s) - 1)\n        \n        # Combine to get the beta-ELBO\n        elbo = recon_term - beta * kl_divergence\n        \n        results.append([elbo, mu_star])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197922"}, {"introduction": "While powerful, expressive decoders can improve reconstruction quality in VAEs, they also introduce a significant risk: posterior collapse. This is a phenomenon where the latent variable $z$ is ignored by the model, undermining the entire purpose of learning a meaningful latent representation. This exercise [@problem_id:3197907] uses a simplified linear model to provide an analytical window into why posterior collapse occurs, specifically when skip connections allow the decoder to bypass the latent bottleneck. You will not only diagnose the problem by measuring the information flow but also implement and evaluate standard mitigation strategies, providing you with essential tools for building robust VAEs.", "problem": "Construct a small, fully specified linear-Gaussian study to examine how skip connections from the input $x$ to the decoder can bypass the latent variable $z$ in a Variational Autoencoder (VAE), potentially causing posterior collapse, and to evaluate mitigation strategies. Work in a scalar setting with the following assumptions that are standard in the literature and serve as the foundational base:\n\n- Evidence Lower Bound (ELBO) for a Variational Autoencoder (VAE) is defined as $\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\mathrm{KL}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$.\n- The Kullback–Leibler divergence for univariate Gaussians $q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^{2})$ and $p(z) = \\mathcal{N}(0, 1)$ is $\\frac{1}{2}\\left(\\sigma^{2} + \\mu^{2} - 1 - \\log \\sigma^{2}\\right)$.\n- For a Gaussian likelihood with variance $\\sigma_{\\epsilon}^{2}$, the negative log-likelihood corresponds to a scaled squared error, and expectations distribute over linear-quadratic forms.\n\nModel and objective:\n\n- Prior: $p(z) = \\mathcal{N}(0, 1)$.\n- Data: $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ with $\\sigma_{x}^{2} = 1$.\n- Encoder: $q_{\\phi}(z \\mid x) = \\mathcal{N}(a x, v)$ with scalar parameters $a \\in \\mathbb{R}$ and $v \\in \\mathbb{R}_{>0}$ to be optimized.\n- Decoder with skip: $p_{\\theta}(x \\mid z, x) = \\mathcal{N}(W z + s x, \\sigma_{\\epsilon}^{2})$, where the skip coefficient $s \\in \\mathbb{R}$ and decoder gain $W \\in \\mathbb{R}$ are treated as fixed hyperparameters of this study.\n- Training objective (negative of the beta-ELBO): minimize over $a$ and $v$ the expected loss\n$$\n\\mathcal{L}_{\\beta}(a, v) \\;=\\; \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right] \\;+\\; \\beta \\, \\mathbb{E}_{x}\\!\\left[ \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\\right],\n$$\nwhere $\\beta \\in \\mathbb{R}_{>0}$ is a weight on the Kullback–Leibler divergence (beta-VAE).\n\nTasks:\n\n- Starting only from the definitions above, derive closed-form optimal encoder parameters $a^{\\star}$ and $v^{\\star}$ that minimize $\\mathcal{L}_{\\beta}(a, v)$ for fixed $W$, $s$, $\\sigma_{\\epsilon}^{2}$, and $\\beta$.\n- Using the optimal $a^{\\star}$ and $v^{\\star}$, compute the mutual information between $x$ and $z$ under the encoder,\n$$\nI(x; z) \\;=\\; \\frac{1}{2}\\log\\!\\left(1 + \\frac{a^{2}\\,\\sigma_{x}^{2}}{v}\\right),\n$$\nwith natural logarithm, to quantify latent usage. Use $\\sigma_{x}^{2} = 1$.\n- Show analytically how the skip parameter $s$ affects $a^{\\star}$, and explain why $s$ near $1$ can lead to $a^{\\star}$ near $0$, indicating potential posterior collapse due to bypassing $z$.\n- Implement and evaluate mitigation strategies programmatically:\n  - Reduce the Kullback–Leibler weight $\\beta$.\n  - Reduce the skip strength $s$.\n  - Apply the free-bits heuristic: replace the Kullback–Leibler term by $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$ with a threshold $\\tau \\in \\mathbb{R}_{\\ge 0}$. Interpret the solution as follows: if the unconstrained beta-VAE optimum has $\\mathrm{KL} \\ge \\tau$, keep it; otherwise, minimize the reconstruction term subject to $\\mathrm{KL} = \\tau$ and compute the resulting $a$ and $v$. Use natural logarithms throughout.\n\nYour program must:\n\n- Compute $I(x; z)$ for each of the following test cases, using the beta-VAE optimum or the free-bits rule exactly as specified. In all cases, use $\\sigma_{x}^{2} = 1$ and natural logarithms. The tuple order is $(s, W, \\sigma_{\\epsilon}^{2}, \\beta, \\tau, \\text{regime})$, where $\\text{regime}$ is either the literal string \"beta\" or \"freebits\".\n  - Case A: $(s = 0,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$.\n  - Case B: $(s = 1,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$.\n  - Case C: $(s = 0.9,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 0.1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$.\n  - Case D: $(s = 0.5,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$.\n  - Case E: $(s = 0.5,\\, W = 0,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$.\n  - Case F: $(s = 0.9,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 1,\\, \\text{regime}=\\text{\"freebits\"})$.\n- For the \"beta\" regime, use the closed-form optimum you derive for $\\mathcal{L}_{\\beta}(a, v)$ to compute $a^{\\star}$, $v^{\\star}$, then $I(x; z)$.\n- For the \"freebits\" regime, if the beta-VAE closed-form optimum yields $\\mathrm{KL} \\ge \\tau$, use it unchanged; otherwise, minimize the reconstruction term subject to $\\mathrm{KL} = \\tau$ and compute $I(x; z)$. If $W = 0$, handle the degeneracy carefully and return $I(x; z) = 0$.\n\nFinal output requirement:\n\n- Your program should produce a single line of output containing the six mutual information values for Cases A through F, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places and expressed in nats. For example, an output line has the form $[\\text{r}_{A},\\text{r}_{B},\\text{r}_{C},\\text{r}_{D},\\text{r}_{E},\\text{r}_{F}]$.", "solution": "The user has provided a well-defined theoretical problem in the context of Variational Autoencoders (VAEs). I will first validate the problem statement and then proceed with a full derivation and solution.\n\n### Step 1: Extract Givens\n- **Prior Distribution**: $p(z) = \\mathcal{N}(0, 1)$.\n- **Data Distribution**: $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ with $\\sigma_{x}^{2} = 1$.\n- **Encoder (Approximate Posterior)**: $q_{\\phi}(z \\mid x) = \\mathcal{N}(a x, v)$, with parameters to optimize $a \\in \\mathbb{R}$, $v \\in \\mathbb{R}_{>0}$.\n- **Decoder (Likelihood)**: $p_{\\theta}(x \\mid z, x) = \\mathcal{N}(W z + s x, \\sigma_{\\epsilon}^{2})$, with fixed hyperparameters $W \\in \\mathbb{R}$, $s \\in \\mathbb{R}$.\n- **Objective Function (to minimize)**: \n$$\n\\mathcal{L}_{\\beta}(a, v) \\;=\\; \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right] \\;+\\; \\beta \\, \\mathbb{E}_{x}\\!\\left[ \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\\right]\n$$\n- **KL Divergence Formula**: For $q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^{2})$ and $p(z) = \\mathcal{N}(0, 1)$, $\\mathrm{KL}(q\\,\\|\\,p) = \\frac{1}{2}\\left(\\sigma^{2} + \\mu^{2} - 1 - \\log \\sigma^{2}\\right)$.\n- **Mutual Information Formula**: $I(x; z) \\;=\\; \\frac{1}{2}\\log\\!\\left(1 + \\frac{a^{2}\\,\\sigma_{x}^{2}}{v}\\right)$, using natural logarithm.\n- **Free-Bits Heuristic**: The KL term is replaced by $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard theoretical analysis of VAEs, using established concepts like the ELBO, KL divergence, and linear-Gaussian models. This is a common simplification in machine learning research for analytical tractability.\n- **Well-Posed**: The problem asks for the minimization of a convex function (in terms of the variables we optimize), which guarantees a unique minimum. The tasks are clearly defined and lead to a unique set of results.\n- **Objective**: The language is precise, and all models and objectives are formally specified. There is no subjectivity.\n- **Conclusion**: The problem is valid. It is scientifically sound, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the full solution.\n\n### Part 1: Derivation of Optimal Encoder Parameters $a^{\\star}$ and $v^{\\star}$\n\nThe objective function $\\mathcal{L}_{\\beta}(a, v)$ consists of two terms: the expected reconstruction error and the expected KL divergence.\n\n**Term 1: Reconstruction Error**\nLet $R$ be the reconstruction term.\n$$\nR = \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right]\n$$\nThe expression inside the expectation can be rewritten as $\\left((1-s)x - Wz\\right)^{2}$. We first take the expectation with respect to $z \\sim q_{\\phi}(z \\mid x) = \\mathcal{N}(ax, v)$.\n$$\n\\mathbb{E}_{q_z}[((1-s)x - Wz)^2] = \\mathbb{E}_{q_z}[(1-s)^2x^2 - 2W(1-s)xz + W^2z^2]\n$$\nUsing $\\mathbb{E}_{q_z}[z] = ax$ and $\\mathbb{E}_{q_z}[z^2] = \\text{Var}_{q_z}[z] + (\\mathbb{E}_{q_z}[z])^2 = v + (ax)^2$:\n$$\n= (1-s)^2x^2 - 2W(1-s)x(ax) + W^2(v + a^2x^2)\n$$\n$$\n= x^2[(1-s)^2 - 2aW(1-s) + a^2W^2] + W^2v\n$$\nThis simplifies to the squared term $x^2(1-s-aW)^2 + W^2v$.\nNow, we take the expectation with respect to $x \\sim \\mathcal{N}(0, \\sigma_x^2)$. We use $\\mathbb{E}_x[x^2] = \\sigma_x^2$.\n$$\n\\mathbb{E}_{x}[x^2(1-s-aW)^2 + W^2v] = \\sigma_x^2(1-s-aW)^2 + W^2v\n$$\nSo, the reconstruction term is:\n$$\nR = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( \\sigma_x^2(1-s-aW)^2 + W^2v \\right)\n$$\n\n**Term 2: KL Divergence**\nLet $K$ be the KL divergence term. The KL divergence between $q_{\\phi}(z \\mid x) = \\mathcal{N}(ax, v)$ and $p(z)=\\mathcal{N}(0, 1)$ is:\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)) = \\frac{1}{2}\\left(v + (ax)^2 - 1 - \\log v\\right)\n$$\nTaking the expectation with respect to $x \\sim \\mathcal{N}(0, \\sigma_x^2)$:\n$$\n\\mathbb{E}_x[\\mathrm{KL}] = \\mathbb{E}_x\\left[\\frac{1}{2}(v + a^2x^2 - 1 - \\log v)\\right] = \\frac{1}{2}(v + a^2\\mathbb{E}_x[x^2] - 1 - \\log v) = \\frac{1}{2}(v + a^2\\sigma_x^2 - 1 - \\log v)\n$$\nSo, the full KL term is:\n$$\nK = \\frac{\\beta}{2} \\left(v + a^2\\sigma_x^2 - 1 - \\log v \\right)\n$$\n\n**Full Objective and Optimization**\nThe total loss is $\\mathcal{L}_{\\beta}(a, v) = R + K$. To find the optimal parameters $a^{\\star}$ and $v^{\\star}$, we set the partial derivatives to zero. For the scope of this problem, $\\sigma_x^2=1$.\n$$\n\\mathcal{L}_{\\beta}(a, v) = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( (1-s-aW)^2 + W^2v \\right) + \\frac{\\beta}{2} \\left( v + a^2 - 1 - \\log v \\right)\n$$\nPartial derivative with respect to $a$:\n$$\n\\frac{\\partial \\mathcal{L}_{\\beta}}{\\partial a} = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( 2(1-s-aW)(-W) \\right) + \\frac{\\beta}{2} (2a) = -\\frac{W(1-s-aW)}{\\sigma_{\\epsilon}^{2}} + \\beta a = 0\n$$\n$$\n-W(1-s) + aW^2 + \\beta a \\sigma_{\\epsilon}^{2} = 0 \\implies a(W^2 + \\beta \\sigma_{\\epsilon}^{2}) = W(1-s)\n$$\n$$\na^{\\star} = \\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}\n$$\nPartial derivative with respect to $v$:\n$$\n\\frac{\\partial \\mathcal{L}_{\\beta}}{\\partial v} = \\frac{W^2}{2\\sigma_{\\epsilon}^{2}} + \\frac{\\beta}{2} \\left(1 - \\frac{1}{v}\\right) = 0\n$$\n$$\n\\frac{W^2}{\\sigma_{\\epsilon}^{2}} + \\beta\\left(1 - \\frac{1}{v}\\right) = 0 \\implies \\beta\\left(\\frac{1}{v}-1\\right) = \\frac{W^2}{\\sigma_{\\epsilon}^{2}} \\implies \\frac{1}{v} = 1 + \\frac{W^2}{\\beta \\sigma_{\\epsilon}^{2}}\n$$\n$$\n\\frac{1}{v} = \\frac{\\beta \\sigma_{\\epsilon}^{2} + W^2}{\\beta \\sigma_{\\epsilon}^{2}} \\implies v^{\\star} = \\frac{\\beta \\sigma_{\\epsilon}^{2}}{\\beta \\sigma_{\\epsilon}^{2} + W^2}\n$$\n\n### Part 2: Analysis of the Skip Connection Parameter $s$\nThe optimal encoder parameter $a^{\\star}$ is given by $a^{\\star} = \\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}$. The denominator is strictly positive. Therefore, $a^{\\star}$ is a linear function of $s$.\nAs the skip connection strength $s$ approaches $1$, the term $(1-s)$ approaches $0$. Consequently, $a^{\\star}$ approaches $0$.\nWhen $a^{\\star}=0$, the encoder becomes $q_{\\phi}(z \\mid x) = \\mathcal{N}(0, v^{\\star})$. This distribution is independent of the input $x$. This means that no information about $x$ is encoded in the latent variable $z$. The decoder's skip connection term $sx$ allows for accurate reconstruction of $x$ even without information from $z$, effectively bypassing the latent bottleneck. This phenomenon where the posterior $q_{\\phi}(z \\mid x)$ collapses to the prior $p(z)$ (or becomes independent of $x$) is known as posterior collapse. The mutual information $I(x;z)$, which quantifies the information about $x$ carried by $z$, becomes zero, as $I(x; z) = \\frac{1}{2}\\log(1 + (a^{\\star})^2/v^{\\star}) = \\frac{1}{2}\\log(1) = 0$.\n\n### Part 3: Mitigation Strategies and Free-Bits Heuristic\nThe test cases are designed to evaluate three mitigation strategies:\n1.  **Reducing KL weight $\\beta$**: A smaller $\\beta$ reduces the penalty for deviating from the prior, encouraging the model to use the latent variable. Case C ($s=0.9, \\beta=0.1$) demonstrates this.\n2.  **Reducing skip strength $s$**: A smaller $s$ (further from $1$) forces the model to rely more on the $Wz$ term for reconstruction, increasing $|a^{\\star}|$. Case D ($s=0.5$) demonstrates this.\n3.  **Free-bits heuristic**: This method prevents the KL divergence from becoming too small by enforcing a minimum capacity $\\tau$. The objective for the KL term becomes $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$.\n    - If the unconstrained optimum $(a^{\\star}, v^{\\star})$ yields a KL divergence $\\mathrm{KL}^{\\star} \\ge \\tau$, the objective is unchanged (up to a constant), and this solution is used.\n    - If $\\mathrm{KL}^{\\star} < \\tau$, we must find a new optimum on the boundary $\\mathrm{KL} = \\tau$. This is achieved by finding an effective KL weight, $\\beta_{\\text{eff}} < \\beta$, such that the resulting unconstrained optimum satisfies $\\mathrm{KL}(a^{\\star}(\\beta_{\\text{eff}}), v^{\\star}(\\beta_{\\text{eff}})) = \\tau$. Since the KL divergence is a monotonically decreasing function of $\\beta$, this $\\beta_{\\text{eff}}$ is unique and can be found with a numerical root-finding algorithm. The calculations in the provided code use this approach for Case F.\n\n### Part 4: Computing Mutual Information $I(x;z)$\nUsing the derived $a^{\\star}$, $v^{\\star}$, and the given formula with $\\sigma_x^2=1$:\n$$ I(x; z) = \\frac{1}{2}\\log\\left(1 + \\frac{(a^{\\star})^2}{v^{\\star}}\\right)$$\nWe substitute the expressions for $a^{\\star}$ and $v^{\\star}$:\n$$\n\\frac{(a^{\\star})^2}{v^{\\star}} = \\frac{\\left(\\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}\\right)^2}{\\frac{\\beta \\sigma_{\\epsilon}^{2}}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}} = \\frac{W^2(1-s)^2}{(W^2 + \\beta \\sigma_{\\epsilon}^{2})^2} \\frac{W^2 + \\beta \\sigma_{\\epsilon}^{2}}{\\beta \\sigma_{\\epsilon}^{2}} = \\frac{W^2(1-s)^2}{\\beta \\sigma_{\\epsilon}^{2}(W^2 + \\beta \\sigma_{\\epsilon}^{2})}\n$$\nTherefore, the mutual information is:\n$$\nI(x; z) = \\frac{1}{2}\\log\\left(1 + \\frac{W^2(1-s)^2}{\\beta \\sigma_{\\epsilon}^{2}(W^2 + \\beta \\sigma_{\\epsilon}^{2})}\\right)\n$$\nThis formula is used for all \"beta\" regime calculations. For the \"freebits\" regime, the same logic applies, but potentially with a numerically found effective $\\beta$.\nThe special case $W=0$ results in $a^{\\star}=0$ and thus $I(x;z)=0$. The formula is robust, but the problem instruction to handle this degeneracy is noted.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mutual information I(x; z) for a linear-Gaussian VAE\n    under different hyperparameter settings to study posterior collapse.\n    \"\"\"\n\n    test_cases = [\n        # (s, W, sigma_eps_sq, beta, tau, regime)\n        (0.0, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # A: Baseline, no skip\n        (1.0, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # B: Full skip, expect collapse\n        (0.9, 1.0, 0.1, 0.1, 0.0, \"beta\"),      # C: Mitigation by low beta\n        (0.5, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # D: Mitigation by moderate s\n        (0.5, 0.0, 0.1, 1.0, 0.0, \"beta\"),      # E: Degenerate case W=0\n        (0.9, 1.0, 0.1, 1.0, 1.0, \"freebits\"), # F: Mitigation by free-bits\n    ]\n\n    results = []\n    \n    # Set sigma_x^2 = 1 as per problem\n    sigma_x_sq = 1.0\n\n    for case in test_cases:\n        s, W, sigma_eps_sq, beta, tau, regime = case\n\n        # Handle degeneracy for W=0 as specified.\n        # This applies to both beta and freebits regimes.\n        if W == 0:\n            results.append(0.0)\n            continue\n        \n        # --- Beta-VAE optimum calculation ---\n        def get_beta_optimum(b_eff):\n            # Calculate optimal a* and v* for a given beta_eff\n            denominator = W**2 + b_eff * sigma_eps_sq\n            a_star = (W * (1 - s)) / denominator\n            v_star = (b_eff * sigma_eps_sq) / denominator\n            return a_star, v_star\n\n        # --- KL divergence calculation ---\n        def get_kl(a, v):\n            # Calculate expected KL divergence for given a, v\n            if v <= 0:\n                return float('inf')\n            # Using sigma_x^2 = 1\n            return 0.5 * (v + a**2 * sigma_x_sq - 1 - np.log(v))\n\n        # --- Mutual information calculation ---\n        def get_mi(a, v):\n            # Calculate mutual information I(x;z)\n            if v <= 0:\n                return 0.0 # Or handle error appropriately\n            # Using sigma_x^2 = 1\n            return 0.5 * np.log(1 + (a**2 * sigma_x_sq) / v)\n            \n        a_final, v_final = 0, 1 # Default values\n\n        if regime == \"beta\":\n            a_final, v_final = get_beta_optimum(beta)\n\n        elif regime == \"freebits\":\n            # 1. Calculate the unconstrained beta-VAE optimum\n            a_beta, v_beta = get_beta_optimum(beta)\n            kl_beta = get_kl(a_beta, v_beta)\n\n            # 2. Check if the KL is above the threshold tau\n            if kl_beta >= tau:\n                # If so, the optimum is the same as the unconstrained one\n                a_final, v_final = a_beta, v_beta\n            else:\n                # 3. If not, find effective beta that makes KL = tau\n                # We need to find `b_eff` where `get_kl(get_beta_optimum(b_eff)) - tau = 0`.\n                # Let gamma = b_eff * sigma_eps_sq.\n                # function to find root of: kl(gamma) - tau = 0\n                def kl_of_gamma_minus_tau(gamma):\n                    if gamma <= 0: return float('inf')\n                    denom_g = W**2 + gamma\n                    a_g = (W * (1 - s)) / denom_g\n                    v_g = gamma / denom_g\n                    return get_kl(a_g, v_g) - tau\n\n                # Bisection method to find gamma_eff\n                gamma_orig = beta * sigma_eps_sq\n                low_gamma = 1e-12  # Avoid log(0)\n                high_gamma = gamma_orig\n\n                # We know kl(gamma_orig) < tau, and kl(gamma->0) -> inf.\n                # So a root exists in (0, gamma_orig).\n                for _ in range(100): # 100 iterations for high precision\n                    mid_gamma = (low_gamma + high_gamma) / 2\n                    if kl_of_gamma_minus_tau(mid_gamma) > 0:\n                        low_gamma = mid_gamma\n                    else:\n                        high_gamma = mid_gamma\n                \n                gamma_eff = high_gamma\n                beta_eff = gamma_eff / sigma_eps_sq\n                \n                a_final, v_final = get_beta_optimum(beta_eff)\n\n        # Compute the final mutual information for the case\n        mi = get_mi(a_final, v_final)\n        results.append(mi)\n\n    # Format output to 6 decimal places per value\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3197907"}]}