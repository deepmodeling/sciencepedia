{"hands_on_practices": [{"introduction": "The core of GAN training is a delicate dance between the generator and the discriminator. A common heuristic to manage this two-player game is to update the discriminator $k$ times for every single generator update. This hands-on coding exercise [@problem_id:3128933] invites you to build a simplified GAN from the ground up and empirically investigate how this crucial hyperparameter $k$ affects training stability. By simulating the dynamics on a simple Gaussian dataset, you will gain an intuitive understanding of how the balance of power between the two networks can lead to successful convergence, persistent oscillations, or catastrophic divergence.", "problem": "You are given a simplified Generative Adversarial Network (GAN) experiment designed to analyze how alternating $k$ discriminator steps per generator step impacts training stability on a one-dimensional Gaussian dataset. The experiment is grounded in first principles: a Generator $G$ transforms standard normal noise $z \\sim \\mathcal{N}(0,1)$ into $x_g = a z + b$, and a Discriminator $D$ estimates the probability that an input is real via $D(x) = \\sigma(w x + c)$ where $\\sigma$ is the logistic sigmoid function. The real data distribution is $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$. The Discriminator maximizes the expected log-likelihood of correctly classifying real and fake samples, and the Generator minimizes the non-saturating loss based on the Discriminator's output.\n\nStart from the following fundamental bases and core definitions in deep learning and probability:\n- The objective of Generative Adversarial Networks (GANs) is a two-player minimax game between a Generator and a Discriminator, driven by expected values over data and noise distributions.\n- Gradient-based learning dynamics approximate these expectations using Monte Carlo samples and apply gradient ascent for maximization and gradient descent for minimization.\n- For a linear Generator $G(z) = a z + b$ where $z \\sim \\mathcal{N}(0,1)$, the Generator’s induced distribution is Gaussian with mean $b$ and standard deviation $|a|$.\n- The logistic sigmoid function is $\\sigma(s) = \\frac{1}{1 + e^{-s}}$, and the derivative identities needed for gradients are well tested in statistics and machine learning.\n\nYour tasks:\n1. Formulate the alternating gradient dynamics where the Discriminator parameters $(w,c)$ are updated via gradient ascent on its objective $k$ times for every single gradient descent update of the Generator parameters $(a,b)$ on its non-saturating objective. Use Monte Carlo estimation with a fixed batch size and fixed learning rates for both players. Ensure numerical stability of $\\sigma$ by controlling the input domain.\n2. Define an empirical notion of training stability. Use the following composite criterion: let $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$ where $\\mu_g(t) = b(t)$ and $\\sigma_g(t) = |a(t)|$ are the Generator’s mean and standard deviation at training step $t$. Define the improvement fraction $I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)}$ for a small $\\epsilon$, and define an oscillation index $O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon}$ where $\\theta_g(t) = [a(t), b(t)]^\\top$. Classify a run as stable if the parameters remain finite and bounded, $I$ exceeds a minimum threshold, and $O$ is below a maximum threshold.\n3. Empirically derive a phase diagram of stability versus $k$ by running the training at several values of $k$ on a simple dataset and reporting whether each run is stable or unstable, encoded as $1$ for stable and $0$ for unstable.\n\nUse the following dataset, training, and evaluation setup:\n- Real data parameters $(\\mu_r, \\sigma_r)$ and schedule parameter $k$ vary per test case.\n- Noise distribution is $z \\sim \\mathcal{N}(0,1)$.\n- Generator is $G(z) = a z + b$ initialized at $a = 0.2$ and $b = 0.0$.\n- Discriminator is $D(x) = \\sigma(w x + c)$ with $(w,c)$ initialized at $(0.0, 0.0)$.\n- Discriminator objective is the expected sum of $\\log D(x)$ over real samples and $\\log(1 - D(G(z)))$ over fake samples; Discriminator uses gradient ascent.\n- Generator objective is the expected value of $- \\log D(G(z))$ (non-saturating); Generator uses gradient descent.\n- Use learning rates $\\alpha_D = 0.05$ and $\\alpha_G = 0.02$, batch size $B = 1024$, and $T = 200$ Generator steps.\n- For numerical stability, clip the sigmoid input $s$ to the interval $[-50, 50]$ before applying $\\sigma(s)$.\n\nDefine the stability classification thresholds and safeguards:\n- Use $\\epsilon = 10^{-8}$ for denominator stabilization.\n- Declare divergence if any parameter magnitude exceeds $100$ or any parameter becomes not-a-number at any point.\n- Use thresholds $I_{\\min} = 0.25$ and $O_{\\max} = 2.5$.\n- A run is stable if it does not diverge, $I \\ge I_{\\min}$, and $O \\le O_{\\max}$.\n\nTest suite:\n- Case $1$: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 0, 42)$.\n- Case $2$: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 1, 42)$.\n- Case $3$: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 5, 42)$.\n- Case $4$: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (2.0, 0.5, 2, 123)$.\n- Case $5$: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (-1.0, 1.5, 10, 7)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is an integer where $1$ denotes a stable run and $0$ denotes an unstable run.", "solution": "The problem requires an empirical investigation into the training stability of a simplified Generative Adversarial Network (GAN) as a function of the number of discriminator updates, $k$, per generator update. This involves deriving the gradient-based learning dynamics, implementing the simulation, and evaluating the outcome against a specified set of stability criteria.\n\nFirst, we formalize the components of the model. The real data distribution is a one-dimensional Gaussian, $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$. The Generator, $G$, maps a standard normal noise variable $z \\sim \\mathcal{N}(0,1)$ to a sample $x_g$ via a linear transformation:\n$$G(z) = a z + b$$\nThe parameters of the generator are $\\theta_G = (a, b)$. The distribution induced by the generator is therefore also Gaussian, with mean $\\mu_g = b$ and standard deviation $\\sigma_g = |a|$. The Discriminator, $D$, is a logistic regressor that models the probability of an input $x$ being from the real data distribution:\n$$D(x) = \\sigma(w x + c)$$\nwhere $\\sigma(s) = (1 + e^{-s})^{-1}$ is the logistic sigmoid function. The parameters of the discriminator are $\\theta_D = (w,c)$.\n\nThe training process is a minimax game. The discriminator is trained to maximize the objective function $V_D$, which is the sum of the log-likelihood of correctly classifying real and fake samples:\n$$V_D(\\theta_D, \\theta_G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\nThe generator is trained to minimize the non-saturating objective function $V_G$, which aims to produce samples that the discriminator classifies as real:\n$$V_G(\\theta_G) = -\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$$\n\nTo implement the learning dynamics, we derive the gradients of these objectives with respect to the model parameters. The expectations are approximated using Monte Carlo estimation over a batch of $B$ samples.\n\nThe gradients of the discriminator objective $V_D$ with respect to its parameters $w$ and $c$ are found using the chain rule and the derivative properties of the sigmoid function, specifically $\\frac{d}{ds}\\log \\sigma(s) = 1 - \\sigma(s)$ and $\\frac{d}{ds}\\log(1-\\sigma(s)) = -\\sigma(s)$. The gradients for a batch of real data $\\{x_i\\}_{i=1}^B$ and generated data $\\{x_{g,i}\\}_{i=1}^B$ are:\n$$ \\nabla_w \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i))x_i - D(x_{g,i})x_{g,i} \\right) $$\n$$ \\nabla_c \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i)) - D(x_{g,i}) \\right) $$\nThe discriminator parameters are updated via gradient ascent:\n$$ w \\leftarrow w + \\alpha_D \\nabla_w \\hat{V}_D $$\n$$ c \\leftarrow c + \\alpha_D \\nabla_c \\hat{V}_D $$\n\nThe gradients of the generator's non-saturating objective $V_G$ with respect to its parameters $a$ and $b$ are:\n$$ \\nabla_a V_G = \\frac{\\partial V_G}{\\partial a} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial a} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial a} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w z \\right] $$\n$$ \\nabla_b V_G = \\frac{\\partial V_G}{\\partial b} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial b} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial b} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w \\right] $$\nApproximating with a batch of noise $\\{z_i\\}_{i=1}^B$, the generator parameters are updated via gradient descent:\n$$ a \\leftarrow a - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i \\right) = a + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i $$\n$$ b \\leftarrow b - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) \\right) = b + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) $$\n\nThe complete training algorithm proceeds for $T$ generator steps. In each step $t \\in \\{1, \\dots, T\\}$:\n1.  The discriminator is updated $k$ times. For each discriminator update, a new batch of real data $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$ and noise $z \\sim \\mathcal{N}(0,1)$ is drawn. The parameters $(w, c)$ are updated using gradient ascent with learning rate $\\alpha_D$.\n2.  The generator is updated once. A new batch of noise $z \\sim \\mathcal{N}(0,1)$ is drawn. The parameters $(a, b)$ are updated using gradient descent on the non-saturating loss with learning rate $\\alpha_G$.\nThroughout the simulation, parameter values are monitored. If any parameter's magnitude exceeds $100$ or becomes non-finite (NaN), the run is terminated and classified as divergent. The input to the sigmoid function is clipped to $[-50, 50]$ to prevent numerical overflow.\n\nUpon completion of $T$ steps, the training run is evaluated for stability. The distance between the generator's distribution parameters $(\\mu_g(t)=b(t), \\sigma_g(t)=|a(t)|)$ and the target real data parameters $(\\mu_r, \\sigma_r)$ is defined as $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$. Two metrics are calculated:\n1.  The improvement fraction, $I$, measures the relative reduction in distance from the initial state to the final state:\n    $$ I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)} $$\n    where $d_0$ and $d_T$ are the distances at step $0$ and $T$, and $\\epsilon = 10^{-8}$ is a small constant for numerical stability. A run must achieve $I \\ge I_{\\min} = 0.25$.\n2.  The oscillation index, $O$, measures the ratio of the total path length of the generator parameters $\\theta_g(t)=[a(t), b(t)]^\\top$ to the net displacement:\n    $$ O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon} $$\n    A run must have $O \\le O_{\\max} = 2.5$.\n\nA run is classified as stable (output $1$) if and only if it does not diverge, achieves $I \\ge I_{\\min}$, and has $O \\le O_{\\max}$. Otherwise, it is unstable (output $0$). The provided implementation executes this entire procedure for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN stability analysis for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # (mu_r, sigma_r, k, seed)\n        (0.0, 1.0, 0, 42),\n        (0.0, 1.0, 1, 42),\n        (0.0, 1.0, 5, 42),\n        (2.0, 0.5, 2, 123),\n        (-1.0, 1.5, 10, 7)\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_r, sigma_r, k, seed = case\n        result = run_training_and_evaluate(mu_r, sigma_r, k, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef sigmoid(s):\n    \"\"\"\n    Computes the logistic sigmoid function with input clipping for numerical stability.\n    \"\"\"\n    s_clipped = np.clip(s, -50.0, 50.0)\n    return 1.0 / (1.0 + np.exp(-s_clipped))\n\ndef run_training_and_evaluate(mu_r, sigma_r, k, seed,\n                              a_init=0.2, b_init=0.0,\n                              w_init=0.0, c_init=0.0,\n                              alpha_D=0.05, alpha_G=0.02,\n                              B=1024, T=200,\n                              epsilon=1e-8, divergence_threshold=100.0,\n                              I_min=0.25, O_max=2.5):\n    \"\"\"\n    Simulates the GAN training for one parameter set and evaluates its stability.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Initialize parameters\n    a, b = a_init, b_init\n    w, c = w_init, c_init\n\n    # History for stability metrics\n    theta_g_history = [np.array([a, b])]\n    diverged = False\n\n    # Calculate initial distance to target distribution parameters\n    d_0 = np.sqrt((b - mu_r)**2 + (np.abs(a) - sigma_r)**2)\n\n    # Main training loop (T generator steps)\n    for _ in range(T):\n        # --- Discriminator updates (k steps) ---\n        if k > 0:\n            for _ in range(k):\n                # Sample data\n                x_r = np.random.normal(loc=mu_r, scale=sigma_r, size=B)\n                z = np.random.normal(loc=0.0, scale=1.0, size=B)\n                x_g = a * z + b\n\n                # Discriminator predictions\n                d_real = sigmoid(w * x_r + c)\n                d_fake = sigmoid(w * x_g + c)\n                \n                # Discriminator gradients (for maximizing log-likelihood)\n                grad_w = np.mean((1.0 - d_real) * x_r - d_fake * x_g)\n                grad_c = np.mean((1.0 - d_real) - d_fake)\n\n                # Update D parameters via gradient ascent\n                w += alpha_D * grad_w\n                c += alpha_D * grad_c\n\n                # Check for D divergence\n                if not (np.isfinite(w) and np.isfinite(c) and\n                        abs(w)  divergence_threshold and abs(c)  divergence_threshold):\n                    diverged = True\n                    break\n            if diverged:\n                break\n\n        # --- Generator update (1 step) ---\n        z_g = np.random.normal(loc=0.0, scale=1.0, size=B)\n        x_g_g = a * z_g + b\n        d_fake_for_g = sigmoid(w * x_g_g + c)\n\n        # Generator gradients (for minimizing non-saturating loss -log(D(G(z))))\n        grad_V_G_a = -np.mean((1.0 - d_fake_for_g) * w * z_g)\n        grad_V_G_b = -np.mean((1.0 - d_fake_for_g) * w)\n        \n        # Update G parameters via gradient descent\n        a -= alpha_G * grad_V_G_a\n        b -= alpha_G * grad_V_G_b\n\n        # Check for G divergence\n        if not (np.isfinite(a) and np.isfinite(b) and\n                abs(a)  divergence_threshold and abs(b)  divergence_threshold):\n            diverged = True\n            break\n            \n        theta_g_history.append(np.array([a, b]))\n\n    # --- Stability Evaluation ---\n    if diverged:\n        return 0\n\n    a_T, b_T = theta_g_history[-1]\n    \n    # Final distance\n    d_T = np.sqrt((b_T - mu_r)**2 + (np.abs(a_T) - sigma_r)**2)\n    \n    # Improvement Fraction (I)\n    I = (d_0 - d_T) / max(d_0, epsilon)\n\n    # Oscillation Index (O)\n    path_length = np.sum([np.linalg.norm(theta_g_history[t] - theta_g_history[t-1]) for t in range(1, T + 1)])\n    net_displacement = np.linalg.norm(theta_g_history[-1] - theta_g_history[0])\n    O = path_length / (net_displacement + epsilon)\n\n    # Classify as stable (1) or unstable (0)\n    if I >= I_min and O = O_max:\n        return 1\n    else:\n        return 0\n\nsolve()\n```", "id": "3128933"}, {"introduction": "In the previous exercise, you saw firsthand how GAN training can become unstable. A powerful theoretical and practical solution to this problem is the Wasserstein GAN (WGAN), which reframes the objective and requires the discriminator (often called a 'critic' in this context) to be $1$-Lipschitz. This exercise [@problem_id:3124549] moves from observing instability to actively preventing it by analyzing three popular methods for enforcing this Lipschitz constraint: weight clipping, spectral normalization, and the gradient penalty. By calculating the effective Lipschitz constant for each method in a simple network, you will develop a quantitative understanding of their relative strengths and weaknesses.", "problem": "Consider a discriminator (also called a critic) $D$ in a Generative Adversarial Network (GAN) trained in the Wasserstein Generative Adversarial Network (WGAN) framework, where the $1$-Lipschitz constraint on $D$ is required. The network is a three-layer feedforward map with Rectified Linear Unit (ReLU) activations,\n$$\nD(x) \\;=\\; W_{3}\\,\\sigma\\!\\left(W_{2}\\,\\sigma\\!\\left(W_{1}\\,x\\right)\\right),\n$$\nwhere $\\sigma$ denotes the ReLU activation and the weight matrices have shapes $W_{1} \\in \\mathbb{R}^{128 \\times 64}$, $W_{2} \\in \\mathbb{R}^{64 \\times 128}$, and $W_{3} \\in \\mathbb{R}^{1 \\times 64}$. Use only the following foundational facts from analysis and linear algebra:\n- For a function $f$ between normed spaces, $f$ is $L$-Lipschitz if for all $x,y$, $|f(x)-f(y)| \\leq L\\,\\|x-y\\|$, and the minimal such $L$ is the Lipschitz constant of $f$.\n- If $h = g \\circ f$ is the composition of two maps that are $L_{f}$- and $L_{g}$-Lipschitz respectively, then $h$ is $L_{g}\\,L_{f}$-Lipschitz.\n- A linear map represented by a matrix $W$ with respect to the Euclidean norm has Lipschitz constant equal to the operator norm (largest singular value) $\\|W\\|_{2}$.\n- The Rectified Linear Unit (ReLU) activation $\\sigma(z) = \\max\\{0,z\\}$ is $1$-Lipschitz.\n- For any matrix $W \\in \\mathbb{R}^{m \\times n}$ whose entries satisfy $|W_{ij}| \\leq c$, the operator norm obeys $\\|W\\|_{2} \\leq \\|W\\|_{F} \\leq c\\,\\sqrt{mn}$, where $\\|W\\|_{F}$ is the Frobenius norm.\n\nYou will analyze three enforcement mechanisms for the $1$-Lipschitz constraint:\n\n(1) Weight clipping: After each update, all entries of each $W_{i}$ are clipped to lie in $[-c,c]$ with $c = 0.01$. Using only the given facts, form an upper bound on the Lipschitz constant of $D$ under clipping.\n\n(2) Spectral normalization: Suppose spectral normalization is used, and the resulting operator norms (largest singular values) of the trained weight matrices are measured as $(\\|W_{1}\\|_{2}, \\|W_{2}\\|_{2}, \\|W_{3}\\|_{2}) = (0.98,\\,1.03,\\,0.99)$. Using only the given facts, form an upper bound on the Lipschitz constant of $D$ under spectral normalization.\n\n(3) Gradient penalty: In WGAN with Gradient Penalty (GP), the penalty encourages $\\|\\nabla_{x} D(x)\\|_{2} \\approx 1$ on lines between real and generated samples. On a batch of interpolated points $\\{x_{t}\\}$, the following gradient norms are observed: $\\|\\nabla_{x} D(x_{t})\\|_{2} \\in \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\}$. Using only the given facts, approximate the Lipschitz constant by the maximum observed gradient norm over the batch.\n\nCompute the three resulting approximate Lipschitz constants in the order: weight clipping, spectral normalization, gradient penalty. Round all numerical results to four significant figures. Express your final answer as a single row matrix containing the three values.", "solution": "The problem requires computing estimates for the Lipschitz constant of a discriminator network $D(x)$ under three different constraint enforcement mechanisms. The discriminator is a three-layer feedforward network given by\n$$D(x) \\;=\\; W_{3}\\,\\sigma\\!\\left(W_{2}\\,\\sigma\\!\\left(W_{1}\\,x\\right)\\right)$$\nwhere $W_{1} \\in \\mathbb{R}^{128 \\times 64}$, $W_{2} \\in \\mathbb{R}^{64 \\times 128}$, and $W_{3} \\in \\mathbb{R}^{1 \\times 64}$ are weight matrices, and $\\sigma$ is the Rectified Linear Unit (ReLU) activation function.\n\nTo determine the Lipschitz constant of $D(x)$, we model it as a composition of functions. Let $f_1(x) = W_1 x$, $g_1(y_1) = \\sigma(y_1)$, $f_2(z_1) = W_2 z_1$, $g_2(y_2) = \\sigma(y_2)$, and $f_3(z_2) = W_3 z_2$. The discriminator can then be written as the composition $D = f_3 \\circ g_2 \\circ f_2 \\circ g_1 \\circ f_1$.\n\nAccording to the provided facts:\n- The composition of an $L_f$-Lipschitz function $f$ and an $L_g$-Lipschitz function $g$ is $(L_g L_f)$-Lipschitz. By extension, the Lipschitz constant of $D$, denoted $L_D$, is bounded by the product of the Lipschitz constants of its constituent functions:\n$$L_D \\leq L_{f_3} \\cdot L_{g_2} \\cdot L_{f_2} \\cdot L_{g_1} \\cdot L_{f_1}$$\n- The functions $f_i$ are linear maps represented by matrices $W_i$. Their Lipschitz constants with respect to the Euclidean norm are their operator norms, $L_{f_i} = \\|W_i\\|_2$.\n- The functions $g_i$ are the ReLU activation function $\\sigma$. The problem states that ReLU is $1$-Lipschitz, so $L_{g_1} = L_{g_2} = 1$.\n\nSubstituting these into the inequality, we obtain an upper bound for the Lipschitz constant of the discriminator:\n$$L_D \\leq \\|W_3\\|_2 \\cdot 1 \\cdot \\|W_2\\|_2 \\cdot 1 \\cdot \\|W_1\\|_2 = \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2$$\nThis inequality forms the basis for the analysis of the first two cases.\n\n(1) **Weight clipping:**\nIn this scenario, all entries of each weight matrix $W_i$ are clipped such that $|(W_i)_{jk}| \\leq c$ with $c = 0.01$. We are given the fact that for a matrix $W \\in \\mathbb{R}^{m \\times n}$, its operator norm is bounded by $\\|W\\|_2 \\leq c \\sqrt{mn}$. We apply this bound to each weight matrix.\n\n- For $W_1 \\in \\mathbb{R}^{128 \\times 64}$:\nThe dimensions are $m=128$ and $n=64$. The upper bound on its operator norm is:\n$$\\|W_1\\|_2 \\leq c \\sqrt{128 \\times 64} = 0.01 \\sqrt{8192} = 0.01 \\sqrt{64^2 \\times 2} = 0.01 \\times 64\\sqrt{2} = 0.64\\sqrt{2}$$\n\n- For $W_2 \\in \\mathbb{R}^{64 \\times 128}$:\nThe dimensions are $m=64$ and $n=128$. The upper bound on its operator norm is:\n$$\\|W_2\\|_2 \\leq c \\sqrt{64 \\times 128} = 0.01 \\sqrt{8192} = 0.64\\sqrt{2}$$\n\n- For $W_3 \\in \\mathbb{R}^{1 \\times 64}$:\nThe dimensions are $m=1$ and $n=64$. The upper bound on its operator norm is:\n$$\\|W_3\\|_2 \\leq c \\sqrt{1 \\times 64} = 0.01 \\sqrt{64} = 0.01 \\times 8 = 0.08$$\n\nThe upper bound on the Lipschitz constant $L_D$ is the product of these individual bounds:\n$$L_D \\leq (0.64\\sqrt{2}) \\cdot (0.64\\sqrt{2}) \\cdot (0.08) = (0.64)^2 \\cdot (\\sqrt{2})^2 \\cdot 0.08 = 0.4096 \\cdot 2 \\cdot 0.08 = 0.8192 \\cdot 0.08 = 0.065536$$\nRounding to four significant figures, the approximate Lipschitz constant is $0.06554$.\n\n(2) **Spectral normalization:**\nHere, the operator norms of the trained weight matrices are provided directly: $\\|W_{1}\\|_{2} = 0.98$, $\\|W_{2}\\|_{2} = 1.03$, and $\\|W_{3}\\|_{2} = 0.99$. We use these values in our derived inequality for $L_D$:\n$$L_D \\leq \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2 = 0.98 \\times 1.03 \\times 0.99$$\nCalculating the product:\n$$L_D \\leq 1.0094 \\times 0.99 = 0.999306$$\nRounding to four significant figures, the approximate Lipschitz constant is $0.9993$.\n\n(3) **Gradient penalty:**\nFor a differentiable function such as the discriminator $D(x)$, its Lipschitz constant $L_D$ is equal to the supremum of the norm of its gradient over the entire input domain: $L_D = \\sup_x \\|\\nabla_x D(x)\\|_2$. The gradient penalty method encourages this gradient norm to be close to $1$. The problem provides a set of observed gradient norms on a batch of points: $\\|\\nabla_{x} D(x_{t})\\|_{2} \\in \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\}$.\nWe are instructed to approximate the Lipschitz constant by the maximum observed gradient norm from this set.\n$$L_D \\approx \\max \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\} = 1.10$$\nTo adhere to the four significant figures requirement, this is expressed as $1.100$.\n\nThe three resulting approximate Lipschitz constants are $0.06554$ (weight clipping), $0.9993$ (spectral normalization), and $1.100$ (gradient penalty).", "answer": "$$\\boxed{\\begin{pmatrix} 0.06554  0.9993  1.100 \\end{pmatrix}}$$", "id": "3124549"}, {"introduction": "After successfully training a stable generator, the final step is to evaluate its performance: how good are the generated samples? While visual inspection is useful, quantitative metrics are essential for rigorous comparison. The Fréchet Inception Distance (FID) is one of the most widely used metrics for this purpose. In this two-part practice [@problem_id:3128911], you will first derive the closed-form expression for FID, solidifying your understanding of its mathematical foundations. You will then explore a constructed scenario where FID provides a misleadingly perfect score, teaching a critical lesson about the limitations of any automated evaluation metric and the importance of understanding what they truly measure.", "problem": "In training Generative Adversarial Networks (GANs), a widely used evaluation metric is the Fréchet Inception Distance (FID), which compares real and generated data in a learned feature space. Let a feature extractor be a map $\\phi:\\mathcal{X}\\to\\mathbb{R}^d$, where $d\\in\\mathbb{N}$, and suppose that the feature distributions of real and generated images are each approximated by multivariate normal distributions with parameters $(\\mu_r,\\Sigma_r)$ and $(\\mu_g,\\Sigma_g)$, respectively, where $\\mu_r,\\mu_g\\in\\mathbb{R}^d$ and $\\Sigma_r,\\Sigma_g\\in\\mathbb{R}^{d\\times d}$ are symmetric positive semidefinite matrices. The Fréchet Inception Distance is defined as the squared $2$-Wasserstein distance between these Gaussian approximations, with the squared $2$-Wasserstein distance defined by\n$$\nW_2^2(P,Q)\\;=\\;\\inf_{\\gamma\\in\\Pi(P,Q)}\\;\\mathbb{E}_{(X,Y)\\sim\\gamma}\\big[\\|X-Y\\|_2^2\\big],\n$$\nwhere $\\Pi(P,Q)$ is the set of all couplings with marginals $P$ and $Q$ on $\\mathbb{R}^d$.\n\nTask (a): Using only the definition of $W_2^2$ above, properties of multivariate normal distributions, and basic linear algebra about symmetric positive semidefinite matrices (including the principal square root), derive a closed-form expression for $W_2^2\\big(\\mathcal{N}(\\mu_r,\\Sigma_r),\\mathcal{N}(\\mu_g,\\Sigma_g)\\big)$ in terms of $\\mu_r,\\mu_g,\\Sigma_r,\\Sigma_g$.\n\nTask (b): Construct a simple feature space where the Fréchet Inception Distance can be misleading. Consider the one-dimensional case $d=1$ with image space $\\mathcal{X}=\\{-1,1\\}$ and feature extractor $\\phi:\\mathcal{X}\\to\\mathbb{R}$ defined by $\\phi(x)=|x|$. Let the real data distribution place probability $1/2$ on $-1$ and $1/2$ on $1$, and let the generator deterministically output $1$ with probability $1$. Compute the resulting Gaussian parameters $(\\mu_r,\\Sigma_r)$ and $(\\mu_g,\\Sigma_g)$ in feature space and then evaluate the squared Fréchet Inception Distance you derived in part (a) for these parameters. Report the final value as a single exact real number. No rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in machine learning theory and optimal transport. All necessary information and definitions are provided.\n\nThe problem consists of two parts. Part (a) requires the derivation of the closed-form expression for the squared $2$-Wasserstein distance between two multivariate normal distributions. Part (b) requires applying this formula to a specific one-dimensional example to demonstrate a potential pitfall of the Fréchet Inception Distance (FID).\n\n**Part (a): Derivation of the squared $2$-Wasserstein distance between two Gaussian distributions**\n\nLet the two multivariate normal distributions be $P = \\mathcal{N}(\\mu_r, \\Sigma_r)$ and $Q = \\mathcal{N}(\\mu_g, \\Sigma_g)$, where $\\mu_r, \\mu_g \\in \\mathbb{R}^d$ are the means and $\\Sigma_r, \\Sigma_g \\in \\mathbb{R}^{d \\times d}$ are the symmetric positive semidefinite covariance matrices. Let $X \\sim P$ and $Y \\sim Q$.\n\nThe squared $2$-Wasserstein distance is defined as:\n$$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\mathbb{E}_{(X,Y) \\sim \\gamma} \\left[ \\|X-Y\\|_2^2 \\right] $$\nwhere $\\Pi(P, Q)$ is the set of all joint distributions (couplings) $\\gamma$ on $\\mathbb{R}^d \\times \\mathbb{R}^d$ with marginals $P$ and $Q$.\n\nLet's decompose the random vectors into their mean and zero-mean components: $X = \\mu_r + X_0$ and $Y = \\mu_g + Y_0$, where $X_0 \\sim \\mathcal{N}(0, \\Sigma_r)$ and $Y_0 \\sim \\mathcal{N}(0, \\Sigma_g)$. The expectation term can be expanded:\n$$ \\|X-Y\\|_2^2 = \\|(\\mu_r - \\mu_g) + (X_0 - Y_0)\\|_2^2 = \\|\\mu_r - \\mu_g\\|_2^2 + \\|X_0 - Y_0\\|_2^2 + 2(\\mu_r - \\mu_g)^T(X_0 - Y_0) $$\nTaking the expectation with respect to any coupling $\\gamma$:\n$$ \\mathbb{E}_{(X,Y)\\sim\\gamma}\\left[\\|X-Y\\|_2^2\\right] = \\|\\mu_r - \\mu_g\\|_2^2 + \\mathbb{E}_{(X_0,Y_0)\\sim\\gamma'}\\left[\\|X_0 - Y_0\\|_2^2\\right] + 2(\\mu_r - \\mu_g)^T(\\mathbb{E}[X_0] - \\mathbb{E}[Y_0]) $$\nSince $X_0$ and $Y_0$ are zero-mean, the third term is $2(\\mu_r - \\mu_g)^T(0 - 0) = 0$. The coupling for $(X,Y)$ induces a coupling $\\gamma'$ for the zero-mean components $(X_0, Y_0)$ with marginals $\\mathcal{N}(0, \\Sigma_r)$ and $\\mathcal{N}(0, \\Sigma_g)$. The term $\\|\\mu_r - \\mu_g\\|_2^2$ is constant with respect to the choice of coupling. Therefore, the infimum acts only on the second term:\n$$ W_2^2(P,Q) = \\|\\mu_r - \\mu_g\\|_2^2 + \\inf_{\\gamma'} \\mathbb{E}_{(X_0,Y_0)\\sim\\gamma'}\\left[\\|X_0 - Y_0\\|_2^2\\right] $$\nLet's analyze the term inside the infimum:\n$$ \\mathbb{E}\\left[\\|X_0 - Y_0\\|_2^2\\right] = \\mathbb{E}[X_0^T X_0] - 2\\mathbb{E}[X_0^T Y_0] + \\mathbb{E}[Y_0^T Y_0] $$\nFor a zero-mean random vector $Z$ with covariance $\\Sigma$, we have $\\mathbb{E}[Z^T Z] = \\mathbb{E}[\\text{Tr}(ZZ^T)] = \\text{Tr}(\\mathbb{E}[ZZ^T]) = \\text{Tr}(\\Sigma)$.\nSo, $\\mathbb{E}[X_0^T X_0] = \\text{Tr}(\\Sigma_r)$ and $\\mathbb{E}[Y_0^T Y_0] = \\text{Tr}(\\Sigma_g)$. These terms are independent of the coupling. The problem reduces to:\n$$ \\inf_{\\gamma'} \\mathbb{E}\\left[\\|X_0 - Y_0\\|_2^2\\right] = \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\sup_{\\gamma'} \\mathbb{E}[X_0^T Y_0] $$\nFor Gaussian measures, the optimal transport plan (the coupling that achieves the supremum) is induced by a linear transformation. We seek a matrix $A$ such that if $X_0 \\sim \\mathcal{N}(0, \\Sigma_r)$, then setting $Y_0 = AX_0$ results in $Y_0$ having the correct marginal distribution $\\mathcal{N}(0, \\Sigma_g)$. The covariance of $Y_0$ is $\\text{Cov}(AX_0) = A \\text{Cov}(X_0) A^T = A\\Sigma_rA^T$. We thus require $A\\Sigma_rA^T = \\Sigma_g$.\nThe term to maximize is $\\mathbb{E}[X_0^T Y_0] = \\mathbb{E}[X_0^T A X_0]$. Using the trace property $\\mathbb{E}[Z^T M Z] = \\text{Tr}(M \\text{Cov}(Z))$ for a zero-mean vector $Z$, we get:\n$$ \\mathbb{E}[X_0^T A X_0] = \\text{Tr}(A \\Sigma_r) $$\nSo, we must solve $\\sup_{A: A\\Sigma_rA^T=\\Sigma_g} \\text{Tr}(A\\Sigma_r)$.\nLet's assume $\\Sigma_r$ is invertible for a moment. Then $\\Sigma_r^{1/2}$ is its unique symmetric positive definite square root. Let $B = A \\Sigma_r^{1/2}$, so $A = B \\Sigma_r^{-1/2}$. The constraint becomes $B \\Sigma_r^{-1/2} \\Sigma_r (\\Sigma_r^{-1/2})^T B^T = BB^T = \\Sigma_g$. We need to maximize $\\text{Tr}(B\\Sigma_r^{-1/2}\\Sigma_r) = \\text{Tr}(B\\Sigma_r^{1/2})$.\nAny matrix $B$ satisfying $BB^T = \\Sigma_g$ can be written as $B = \\Sigma_g^{1/2}O$ for an orthogonal matrix $O$. We want to maximize $\\text{Tr}(\\Sigma_g^{1/2} O \\Sigma_r^{1/2}) = \\text{Tr}(O \\Sigma_r^{1/2} \\Sigma_g^{1/2})$.\nLet $M = \\Sigma_r^{1/2} \\Sigma_g^{1/2}$. Let its singular value decomposition be $M = USV^T$. We want to maximize $\\text{Tr}(O U S V^T) = \\text{Tr}(V^T O U S)$. The matrix $O' = V^T O U$ is orthogonal. The expression is maximized when $O'=I$, giving a maximum value of $\\text{Tr}(S)$. The singular values in $S$ are the square roots of the eigenvalues of $M^T M = (\\Sigma_g^{1/2}\\Sigma_r^{1/2})(\\Sigma_r^{1/2}\\Sigma_g^{1/2}) = \\Sigma_g^{1/2} \\Sigma_r \\Sigma_g^{1/2}$. Alternatively, they are the square roots of the eigenvalues of $MM^T = \\Sigma_r^{1/2}\\Sigma_g\\Sigma_r^{1/2}$. The trace of the diagonal matrix of these singular values is $\\text{Tr}(S) = \\text{Tr}\\left(\\left( \\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} \\right)^{1/2}\\right)$. This result holds even for singular $\\Sigma_r$ by a continuity argument.\n\nCombining all parts:\n$$ \\sup_{\\gamma'} \\mathbb{E}[X_0^T Y_0] = \\text{Tr}\\left(\\left( \\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} \\right)^{1/2}\\right) $$\nSo, the full expression for the squared $2$-Wasserstein distance is:\n$$ W_2^2(\\mathcal{N}(\\mu_r,\\Sigma_r), \\mathcal{N}(\\mu_g,\\Sigma_g)) = \\|\\mu_r - \\mu_g\\|_2^2 + \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\text{Tr}\\left( \\left(\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2}\\right)^{1/2} \\right) $$\n\n**Part (b): Calculation for a specific example**\n\nWe are given a one-dimensional case ($d=1$) with:\n- Image space $\\mathcal{X} = \\{-1, 1\\}$.\n- Feature extractor $\\phi: \\mathcal{X} \\to \\mathbb{R}$ defined by $\\phi(x) = |x|$.\n- Real data distribution $P_r$: $P_r(X=-1) = \\frac{1}{2}$, $P_r(X=1) = \\frac{1}{2}$.\n- Generated data distribution $P_g$: a deterministic output of $1$.\n\nFirst, we compute the feature distributions and their parameters $(\\mu, \\Sigma)$. Since $d=1$, $\\Sigma$ is a $1 \\times 1$ matrix representing the variance, i.e., $\\Sigma = [\\sigma^2]$.\n\n**Real data features:**\nThe real data points $x_r$ can be $-1$ or $1$. The corresponding feature value is $z_r = \\phi(x_r) = |x_r|$.\n- If $x_r = -1$, $z_r = |-1| = 1$.\n- If $x_r = 1$, $z_r = |1| = 1$.\nThe distribution of real features is deterministic: it is always $1$.\nThe mean of the real features is $\\mu_r = \\mathbb{E}[Z_r] = \\frac{1}{2} \\cdot \\phi(-1) + \\frac{1}{2} \\cdot \\phi(1) = \\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot 1 = 1$.\nThe variance of the real features is $\\sigma_r^2 = \\mathbb{E}[(Z_r - \\mu_r)^2] = \\mathbb{E}[(1-1)^2] = 0$.\nSo, $\\mu_r = 1$ and $\\Sigma_r = [0]$.\n\n**Generated data features:**\nThe generator deterministically outputs $x_g=1$. The corresponding feature is $z_g = \\phi(1) = |1| = 1$.\nThe distribution of generated features is also deterministic: it is always $1$.\nThe mean of the generated features is $\\mu_g = \\mathbb{E}[Z_g] = 1$.\nThe variance of the generated features is $\\sigma_g^2 = \\mathbb{E}[(Z_g - \\mu_g)^2] = \\mathbb{E}[(1-1)^2] = 0$.\nSo, $\\mu_g = 1$ and $\\Sigma_g = [0]$.\n\nNow, we compute the squared FID using the formula from part (a).\n$$ \\text{FID}^2 = W_2^2 = \\|\\mu_r - \\mu_g\\|_2^2 + \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\text{Tr}\\left( \\left(\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2}\\right)^{1/2} \\right) $$\nSubstituting the parameters:\n- $\\mu_r = 1$, $\\mu_g = 1$.\n- $\\Sigma_r = [0]$, $\\Sigma_g = [0]$.\n\nThe terms are:\n1. Difference of means: $\\|\\mu_r - \\mu_g\\|_2^2 = (1-1)^2 = 0$.\n2. Trace of $\\Sigma_r$: $\\text{Tr}(\\Sigma_r) = \\text{Tr}([0]) = 0$.\n3. Trace of $\\Sigma_g$: $\\text{Tr}(\\Sigma_g) = \\text{Tr}([0]) = 0$.\n4. The covariance cross-term:\n   - $\\Sigma_r^{1/2} = [0]^{1/2} = [0]$.\n   - The product inside the parentheses is $\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} = [0][0][0] = [0]$.\n   - The principal square root of this is $([0])^{1/2} = [0]$.\n   - The trace is $\\text{Tr}([0]) = 0$.\n   - The entire term is $-2 \\times 0 = 0$.\n\nSumming the terms:\n$$ \\text{FID}^2 = 0 + 0 + 0 - 0 = 0 $$\nThe resulting squared Fréchet Inception Distance is $0$. This result is \"misleading\" because the real data distribution covers two modes ($-1$ and $1$), while the generator has collapsed to a single mode ($1$). The non-injective feature map $\\phi(x)=|x|$ maps both modes of the real data to the same feature representation, making the real and generated feature distributions identical. Thus, FID reports a perfect score of $0$ despite the generator's severe mode collapse.", "answer": "$$\\boxed{0}$$", "id": "3128911"}]}