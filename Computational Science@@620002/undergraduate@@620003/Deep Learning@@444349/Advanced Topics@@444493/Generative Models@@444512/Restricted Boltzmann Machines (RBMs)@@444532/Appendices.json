{"hands_on_practices": [{"introduction": "Training a Restricted Boltzmann Machine effectively hinges on approximating a complex probability distribution, a task managed by the Contrastive Divergence (CD) algorithm. This practice delves into CD's most critical hyperparameter: $k$, the number of Gibbs sampling steps. By comparing models trained with a fast approximation ($k=1$) versus a more accurate one ($k=10$), you will gain direct, empirical insight into the trade-off between computational cost and the model's ability to learn the distinct modes of the underlying data distribution. [@problem_id:3170448]", "problem": "You are tasked with designing and implementing a complete, runnable program that empirically compares Contrastive Divergence with $k=1$ steps (CD-1) and with $k=10$ steps (CD-10) for training a Restricted Boltzmann Machine (RBM) on a synthetic dataset whose underlying modes are known. The comparison criterion is mode coverage measured via cluster assignment to the known modes.\n\nBegin from the following fundamental base:\n\n- A Restricted Boltzmann Machine (RBM) defines a joint distribution over binary visible variables $v \\in \\{0,1\\}^D$ and binary hidden variables $h \\in \\{0,1\\}^H$ by an energy function $E(v,h)$ and a partition function $Z$. The joint probability is $p(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$.\n- The energy function for a binary-binary RBM is parameterized by a weight matrix, visible biases, and hidden biases. Visible units are conditionally independent given hidden units, and hidden units are conditionally independent given visible units.\n- Contrastive Divergence with $k$ Gibbs steps (CD-$k$) approximates the gradient of the log-likelihood by starting a short Gibbs chain at the data and running $k$ alternating conditional sampling steps.\n\nProblem requirements:\n\n1. Construct a synthetic dataset with a known set of binary base modes. Let $D=8$ and $M=4$ base modes $m_1, m_2, m_3, m_4 \\in \\{0,1\\}^8$ be defined explicitly as:\n   - $m_1 = [1,1,0,0,1,0,1,0]$\n   - $m_2 = [0,1,1,0,0,1,0,1]$\n   - $m_3 = [1,0,1,1,0,0,1,0]$\n   - $m_4 = [0,0,0,1,1,1,0,0]$\n   For each mode $m_i$, generate $n_i$ samples by independently flipping each bit with probability $p_{\\text{flip}} \\in [0,1]$ and aggregating all modes' samples. This yields a dataset with known mode counts $(n_1,n_2,n_3,n_4)$.\n\n2. Train two RBMs with binary visible and binary hidden units on the same dataset and identical initial parameters, differing only in the Contrastive Divergence steps $k$:\n   - RBM with CD-1,\n   - RBM with CD-10.\n   Use a fixed number of hidden units $H=6$, a learning rate $\\alpha$, mini-batch training, and a fixed number of epochs. Ensure identical random initialization for both RBMs to isolate the effect of $k$.\n\n3. After training, draw samples from each trained RBM by running a Gibbs chain starting from a random binary visible vector. For each generated sample, assign it to the nearest base mode by minimum Hamming distance. Using these assignments, define the mode coverage metric as follows:\n   - Let $\\mathcal{M} = \\{m_1, m_2, m_3, m_4\\}$ be the set of base modes.\n   - For a set of generated samples $\\{v^{(s)}\\}_{s=1}^S$, define the assigned mode index for each sample by $i^\\star(s) = \\arg\\min_{i \\in \\{1,\\dots,M\\}} d_H(v^{(s)}, m_i)$, where $d_H$ is the Hamming distance.\n   - Define coverage as the fraction of modes with at least one assigned sample: $\\text{coverage} = \\frac{|\\{i \\in \\{1,\\dots,M\\} : \\exists s \\text{ with } i^\\star(s) = i\\}|}{M}$.\n   Express coverage as a decimal in $[0,1]$.\n\n4. Implement the full training and evaluation pipeline starting from the RBM energy-based definition and the conditional independence structure. Derive and implement the CD-$k$ training updates based on the difference between data-driven and model-driven correlations and biases.\n\n5. Use the following test suite, which specifies $(n_1,n_2,n_3,n_4)$ and $p_{\\text{flip}}$:\n   - Case A (balanced, low noise): $(250,250,250,250)$, $p_{\\text{flip}} = 0.05$.\n   - Case B (balanced, higher noise): $(250,250,250,250)$, $p_{\\text{flip}} = 0.25$.\n   - Case C (imbalanced, low noise): $(400,100,75,25)$, $p_{\\text{flip}} = 0.05$.\n\n6. For each case, train both RBMs with the same configuration: $H=6$, learning rate $\\alpha = 0.1$, mini-batch size $50$, epochs $100$, and sample $S=200$ points from each trained model with $100$ Gibbs steps per sample for assignment.\n\n7. Final output specification: Your program should produce a single line of output containing the results for the three cases as a comma-separated list of pairs, where each pair lists $[\\text{coverage}_{\\text{CD-1}}, \\text{coverage}_{\\text{CD-10}}]$. Round each coverage value to three decimal places and format the output exactly as:\n   - $[[c_{A,1},c_{A,10}],[c_{B,1},c_{B,10}],[c_{C,1},c_{C,10}]]$\n   where $c_{\\cdot,\\cdot} \\in [0,1]$ are decimal numbers.\n\nNo physical units are involved in this problem, and angles are not present. All answers are decimals in $[0,1]$ rounded to three decimal places. Ensure scientific realism by using the described RBM architecture, the specified dataset generation process, and the defined evaluation metric.", "solution": "The present task requires a rigorous empirical comparison of Contrastive Divergence training for Restricted Boltzmann Machines (RBMs) with $k=1$ and $k=10$ Gibbs sampling steps. The solution is structured around fundamental principles of RBMs, from the energy-based model definition to the gradient approximation via CD-$k$.\n\n**1. Restricted Boltzmann Machine: Theoretical Framework**\n\nAn RBM is an energy-based model for a joint probability distribution over binary visible units $v \\in \\{0, 1\\}^D$ and binary hidden units $h \\in \\{0, 1\\}^H$. The model is parameterized by $\\theta = \\{W, b, c\\}$, where $W \\in \\mathbb{R}^{H \\times D}$ is the weight matrix connecting hidden and visible units, $b \\in \\mathbb{R}^D$ is the visible unit bias vector, and $c \\in \\mathbb{R}^H$ is the hidden unit bias vector.\n\nThe energy function $E(v, h; \\theta)$ defines the system's state:\n$$\nE(v, h) = - \\sum_{i=1}^D b_i v_i - \\sum_{j=1}^H c_j h_j - \\sum_{i=1}^D \\sum_{j=1}^H h_j W_{ji} v_i\n$$\nIn matrix notation, this is $E(v, h) = -b^T v - c^T h - h^T W v$.\n\nThe joint probability distribution is given by the Boltzmann distribution:\n$$\np(v, h) = \\frac{1}{Z} e^{-E(v, h)}\n$$\nwhere $Z = \\sum_{v', h'} e^{-E(v', h')}$ is the partition function, an intractable normalization constant. The marginal probability of a visible vector is $p(v) = \\frac{1}{Z} \\sum_h e^{-E(v, h)}$.\n\nA key property of RBMs is the conditional independence of units within a layer given the state of the other layer. This allows for efficient block Gibbs sampling. The conditional probabilities are:\n$$\np(h_j=1 | v) = \\sigma\\left(c_j + \\sum_{i=1}^D W_{ji} v_i\\right) = \\sigma(c_j + (Wv)_j)\n$$\n$$\np(v_i=1 | h) = \\sigma\\left(b_i + \\sum_{j=1}^H h_j W_{ji}\\right) = \\sigma(b_i + (W^T h)_i)\n$$\nwhere $\\sigma(x) = (1 + e^{-x})^{-1}$ is the logistic sigmoid function.\n\n**2. Training via Contrastive Divergence (CD-$k$)**\n\nTraining an RBM involves adjusting $\\theta$ to maximize the log-likelihood of the training data $\\mathcal{D} = \\{v^{(n)}\\}$. The gradient of the log-likelihood for a single data point $v$ is:\n$$\n\\frac{\\partial \\log p(v)}{\\partial \\theta} = \\mathbb{E}_{h \\sim p(h|v)}\\left[-\\frac{\\partial E(v,h)}{\\partial \\theta}\\right] - \\mathbb{E}_{v',h' \\sim p(v',h')}\\left[-\\frac{\\partial E(v',h')}{\\partial \\theta}\\right]\n$$\nThe first term, the \"positive phase,\" is the expectation over hidden states given the data. The second term, the \"negative phase,\" is the expectation over the full model distribution. The intractability of $Z$ makes the negative phase computationally infeasible.\n\nContrastive Divergence with $k$ steps (CD-$k$) approximates this gradient. It replaces the model expectation with an expectation over samples obtained from a short Gibbs chain of $k$ steps, initialized with a data vector. For a given data vector $v^{(0)}$:\n1.  **Positive Phase Statistics**: Calculate the expectation of correlations based on the data. For the weights, this is $v^{(0)} p(h|v^{(0)})^T$.\n2.  **Negative Phase Generation**: Run a $k$-step Gibbs chain, starting with $h_s^{(0)} \\sim p(h|v^{(0)})$:\n    $$\n    v^{(1)} \\sim p(v|h_s^{(0)}), h_s^{(1)} \\sim p(h|v^{(1)}), \\dots, v^{(k)} \\sim p(v|h_s^{(k-1)})\n    $$\n3.  **Negative Phase Statistics**: Calculate correlations based on the final \"reconstruction\" $v^{(k)}$, giving $v^{(k)} p(h|v^{(k)})^T$.\n\nThe parameter updates for a mini-batch are derived from the average difference between positive and negative phase statistics, scaled by a learning rate $\\alpha$:\n$$\n\\Delta W \\propto \\alpha \\left( \\langle v^{(0)} p(h|v^{(0)})^T \\rangle_{\\text{batch}} - \\langle v^{(k)} p(h|v^{(k)})^T \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta b \\propto \\alpha \\left( \\langle v^{(0)} \\rangle_{\\text{batch}} - \\langle v^{(k)} \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta c \\propto \\alpha \\left( \\langle p(h|v^{(0)}) \\rangle_{\\text{batch}} - \\langle p(h|v^{(k)}) \\rangle_{\\text{batch}} \\right)\n$$\nThis experiment compares $k=1$ (a coarse but fast approximation) with $k=10$ (a more accurate but slower approximation). It is expected that a larger $k$ allows the Gibbs chain to move further from the initial data point and closer to the model's stationary distribution, yielding a better gradient estimate. This often leads to the model learning a better representation of the data distribution, including capturing multiple modes more effectively.\n\n**3. Experimental Design and Implementation**\n\nThe implementation follows the problem specification precisely.\n-   **Dataset Generation**: A function generates synthetic data by taking $M=4$ base modes and creating $n_i$ noisy versions of each mode $m_i$ by flipping bits with probability $p_{\\text{flip}}$.\n-   **RBM Class**: An RBM class encapsulates the parameters ($W, b, c$) and methods for sampling ($p(h|v)$, $p(v|h)$) and training (the CD-$k$ update). To ensure a fair comparison, both the CD-1 and CD-10 RBMs are initialized with identical parameters by using the same random seed.\n-   **Training Pipeline**: A training function iterates for a fixed number of epochs, processes the data in mini-batches, and applies the CD-$k$ update rule for the specified $k$. The shuffling of data is also seeded to be identical for both training runs within a given test case.\n-   **Evaluation**: After training, samples are generated from each RBM by running a long Gibbs chain ($100$ steps) starting from random initial vectors. Each generated sample is assigned to the nearest base mode using Hamming distance. The mode coverage is a practical metric for this problem, measuring the model's ability to reproduce the various underlying patterns in the data. An RBM that learns all modes will have its generated samples fall near all four base modes, resulting in a high coverage score.\n\nThe experiment is conducted for three distinct cases: a balanced dataset with low noise, a balanced dataset with high noise, and an imbalanced dataset. This tests the algorithms' robustness to noise and data imbalance. The expectation is that CD-10 will exhibit superior mode coverage, especially in the more challenging imbalanced case, as CD-1's biased gradient might cause the model to focus only on the most frequent modes.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\n# Main class for the Restricted Boltzmann Machine\nclass RBM:\n    \"\"\"\n    A class for a binary-binary Restricted Boltzmann Machine (RBM).\n    \"\"\"\n\n    def __init__(self, n_visible, n_hidden, seed=None):\n        \"\"\"\n        Initializes the RBM with random weights and zero biases.\n        \n        Args:\n            n_visible (int): Number of visible units (D).\n            n_hidden (int): Number of hidden units (H).\n            seed (int, optional): Seed for the random number generator for reproducibility.\n        \"\"\"\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        self.rng = np.random.default_rng(seed)\n\n        # Initialize parameters\n        # Weights are initialized from a normal distribution with small variance\n        self.W = self.rng.normal(0, 0.01, (self.n_hidden, self.n_visible))\n        # Biases are initialized to zero\n        self.b = np.zeros(self.n_visible)\n        self.c = np.zeros(self.n_hidden)\n\n    def _propup(self, v):\n        \"\"\"Calculates hidden layer activation probabilities given visible layer state.\"\"\"\n        return sigmoid(self.c + v @ self.W.T)\n\n    def _propdown(self, h):\n        \"\"\"Calculates visible layer activation probabilities given hidden layer state.\"\"\"\n        return sigmoid(self.b + h @ self.W)\n\n    def sample_h_given_v(self, v):\n        \"\"\"Samples hidden layer states given visible layer states.\"\"\"\n        h_probs = self._propup(v)\n        return self.rng.binomial(1, h_probs)\n\n    def sample_v_given_h(self, h):\n        \"\"\"Samples visible layer states given hidden layer states.\"\"\"\n        v_probs = self._propdown(h)\n        return self.rng.binomial(1, v_probs)\n\n    def update_params(self, v_batch, k, learning_rate):\n        \"\"\"\n        Performs a single parameter update using k-step Contrastive Divergence (CD-k).\n        \n        Args:\n            v_batch (np.ndarray): A mini-batch of data samples.\n            k (int): The number of Gibbs sampling steps.\n            learning_rate (float): The learning rate for the update.\n        \"\"\"\n        batch_size = v_batch.shape[0]\n        \n        # --- Positive Phase ---\n        v0 = v_batch\n        ph0_probs = self._propup(v0)\n        \n        # --- Negative Phase ---\n        vk = v0\n        for _ in range(k):\n            hk_samples = self.sample_h_given_v(vk)\n            vk = self.sample_v_given_h(hk_samples)\n        \n        phk_probs = self._propup(vk)\n\n        # --- Calculate Gradients ---\n        # Note: We use probabilities for the hidden units for a lower variance gradient\n        grad_W = (ph0_probs.T @ v0 - phk_probs.T @ vk) / batch_size\n        grad_b = np.mean(v0 - vk, axis=0)\n        grad_c = np.mean(ph0_probs - phk_probs, axis=0)\n\n        # --- Update Parameters ---\n        self.W += learning_rate * grad_W\n        self.b += learning_rate * grad_b\n        self.c += learning_rate * grad_c\n\n    def sample(self, n_samples, n_gibbs_steps, seed=None):\n        \"\"\"\n        Generates samples from the RBM by running a Gibbs chain.\n        \"\"\"\n        eval_rng = np.random.default_rng(seed)\n        v = eval_rng.integers(0, 2, size=(n_samples, self.n_visible))\n        for _ in range(n_gibbs_steps):\n            h = self.sample_h_given_v(v)\n            v = self.sample_v_given_h(h)\n        return v\n\ndef generate_dataset(modes, mode_counts, p_flip, seed=None):\n    \"\"\"\n    Generates a synthetic dataset from base modes by flipping bits.\n    \"\"\"\n    data_rng = np.random.default_rng(seed)\n    dataset = []\n    for mode, count in zip(modes, mode_counts):\n        for _ in range(count):\n            noise = data_rng.choice([0, 1], size=mode.shape, p=[1 - p_flip, p_flip])\n            sample = np.bitwise_xor(mode, noise)\n            dataset.append(sample)\n    \n    dataset = np.array(dataset)\n    data_rng.shuffle(dataset)\n    return dataset\n\ndef train_rbm(rbm, data, epochs, batch_size, k, learning_rate, seed=None):\n    \"\"\"\n    Trains an RBM on the provided data.\n    \"\"\"\n    train_rng = np.random.default_rng(seed)\n    n_samples = data.shape[0]\n    \n    for epoch in range(epochs):\n        indices = np.arange(n_samples)\n        train_rng.shuffle(indices)\n        \n        for i in range(0, n_samples, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            v_batch = data[batch_indices]\n            rbm.update_params(v_batch, k, learning_rate)\n\ndef calculate_coverage(samples, modes):\n    \"\"\"\n    Calculates mode coverage based on Hamming distance.\n    \"\"\"\n    # Calculate Hamming distance from each sample to each mode\n    # Broadcasting: (n_samples, 1, D) vs (1, n_modes, D)\n    distances = np.sum(samples[:, np.newaxis, :] != modes[np.newaxis, :, :], axis=2)\n    \n    # Assign each sample to the nearest mode\n    assignments = np.argmin(distances, axis=1)\n    \n    # Find the unique modes that were covered\n    covered_mode_indices = np.unique(assignments)\n    \n    # Calculate coverage fraction\n    coverage = len(covered_mode_indices) / len(modes)\n    return coverage\n\ndef solve():\n    \"\"\"\n    Main function to run the RBM comparison experiment.\n    \"\"\"\n    # --- Problem Constants and Parameters ---\n    D = 8  # Number of visible units\n    H = 6  # Number of hidden units\n    M = 4  # Number of modes\n    \n    base_modes = np.array([\n        [1, 1, 0, 0, 1, 0, 1, 0],\n        [0, 1, 1, 0, 0, 1, 0, 1],\n        [1, 0, 1, 1, 0, 0, 1, 0],\n        [0, 0, 0, 1, 1, 1, 0, 0]\n    ])\n\n    # Test cases: (mode_counts, p_flip)\n    test_cases = [\n        ((250, 250, 250, 250), 0.05),  # Case A\n        ((250, 250, 250, 250), 0.25),  # Case B\n        ((400, 100, 75, 25), 0.05),    # Case C\n    ]\n\n    # Training and evaluation parameters\n    learning_rate = 0.1\n    epochs = 100\n    batch_size = 50\n    n_eval_samples = 200\n    n_gibbs_steps_eval = 100\n\n    results = []\n    \n    # Master RNG for reproducibility of the entire experiment\n    master_rng = np.random.default_rng(42)\n\n    for mode_counts, p_flip in test_cases:\n        # Generate seeds for this case to ensure fair comparison\n        data_seed = master_rng.integers(2**32 - 1)\n        init_seed = master_rng.integers(2**32 - 1)\n        train_seed = master_rng.integers(2**32 - 1)\n        eval_seed = master_rng.integers(2**32 - 1)\n\n        # 1. Generate dataset\n        dataset = generate_dataset(base_modes, mode_counts, p_flip, seed=data_seed)\n\n        # 2. Train and evaluate for k=1 and k=10\n        case_results = []\n        for k_val in [1, 10]:\n            # Initialize RBM with identical starting weights\n            rbm = RBM(n_visible=D, n_hidden=H, seed=init_seed)\n            \n            # Train RBM with identical batch ordering\n            train_rbm(rbm, dataset, epochs=epochs, batch_size=batch_size, \n                      k=k_val, learning_rate=learning_rate, seed=train_seed)\n            \n            # Generate samples for evaluation\n            samples = rbm.sample(n_samples=n_eval_samples, \n                                 n_gibbs_steps=n_gibbs_steps_eval, \n                                 seed=eval_seed)\n            \n            # Calculate mode coverage\n            coverage = calculate_coverage(samples, base_modes)\n            case_results.append(round(coverage, 3))\n        \n        results.append(case_results)\n    \n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3170448"}, {"introduction": "Beyond just training a model, a crucial skill is to analyze what it has learned. A well-trained RBM should discover meaningful, high-level features in its hidden layer that are ideally independent of one another. This exercise equips you with a quantitative method to assess this \"disentanglement\" by connecting the statistical independence of hidden unit activations to the geometric orthogonality of the model's weight vectors, providing a powerful diagnostic for the quality of the learned representation. [@problem_id:3170455]", "problem": "You are given the framework of a binary-binary Restricted Boltzmann Machine (RBM). The RBM has a visible binary vector $v \\in \\{0,1\\}^{m}$, a hidden binary vector $h \\in \\{0,1\\}^{n}$, a weight matrix $W \\in \\mathbb{R}^{m \\times n}$, a visible bias vector $a \\in \\mathbb{R}^{m}$, and a hidden bias vector $b \\in \\mathbb{R}^{n}$. The joint probability distribution over $(v,h)$ is defined by the energy-based model\n$$\nE(v,h) = - a^{\\top} v - b^{\\top} h - v^{\\top} W h,\n$$\nwith\n$$\np(v,h) = \\frac{1}{Z} \\exp\\left(-E(v,h)\\right),\n$$\nwhere $Z$ is the partition function satisfying\n$$\nZ = \\sum_{v \\in \\{0,1\\}^{m}} \\sum_{h \\in \\{0,1\\}^{n}} \\exp\\left(-E(v,h)\\right).\n$$\nFrom these foundations, derive the conditional distribution $p(h \\mid v)$ and use it to obtain the componentwise expression for $\\mathbb{E}[h \\mid v]$. Then, for a dataset of visible vectors $\\{v^{(i)}\\}_{i=1}^{N}$, define the matrix $H \\in \\mathbb{R}^{N \\times n}$ whose $i$-th row is $\\mathbb{E}[h \\mid v^{(i)}]^{\\top}$, and compute the sample covariance matrix $C \\in \\mathbb{R}^{n \\times n}$ of the expected hidden activations across the dataset. The sample covariance matrix must be computed as\n$$\n\\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[h \\mid v^{(i)}], \\quad C = \\frac{1}{N-1} \\sum_{i=1}^{N} \\left(\\mathbb{E}[h \\mid v^{(i)}] - \\mu\\right)\\left(\\mathbb{E}[h \\mid v^{(i)}] - \\mu\\right)^{\\top}.\n$$\nDesign a quantitative test for approximate independence of hidden features by measuring the extent to which $C$ is near-diagonal. Define the independence deviation metric\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon},\n$$\nwhere $\\epsilon = 10^{-12}$ is a small constant for numerical stability. A small value of $\\mathrm{ID}(C)$ indicates that the covariance matrix is close to diagonal and thus the hidden features are approximately independent across data in expectation.\n\nAdditionally, define an orthogonality penalty on the hidden feature weight vectors to encourage near-diagonal covariances. Let $W_{:,j}$ denote the $j$-th column of $W$ (the weight vector for hidden unit $j$). Form the Gram matrix $G = W^{\\top} W \\in \\mathbb{R}^{n \\times n}$ and define\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\nA small value of $\\mathrm{OP}(W)$ indicates that the hidden-unit weight vectors are approximately orthogonal, which should encourage $\\mathrm{ID}(C)$ to be small.\n\nImplement a program that, for a given set of RBM parameters and datasets, computes $\\mathrm{ID}(C)$ and $\\mathrm{OP}(W)$ for each test case.\n\nUse the following test suite, which includes a general case, a correlated-weights case, and a boundary case:\n\n- Test case $1$ (general \"happy path\"): $m = 4$, $n = 3$, $W = \\begin{bmatrix} 5 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 5 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $a$ is unused in this task and may be set to $\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$. Dataset of $N = 8$ visible vectors:\n$$\n\\{(0,0,0,0),(1,0,0,0),(0,1,0,0),(0,0,1,0),(1,1,0,0),(1,0,1,0),(0,1,1,0),(1,1,1,0)\\}.\n$$\n- Test case $2$ (correlated hidden features): $m = 4$, $n = 3$, $W = \\begin{bmatrix} 5 & 5 & 0 \\\\ 5 & 5 & 0 \\\\ 0 & 0 & 5 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, same dataset as in test case $1$.\n- Test case $3$ (boundary case): $m = 4$, $n = 3$, $W = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, same dataset as in test case $1$.\n\nYour program must:\n- Derive and use $\\mathbb{E}[h \\mid v]$ from the RBM fundamentals above.\n- Compute the sample covariance matrix $C$ of the rows of $H$ for each test case.\n- Compute $\\mathrm{ID}(C)$ and $\\mathrm{OP}(W)$ for each test case, using $\\epsilon = 10^{-12}$.\n- Aggregate the results into a single line of output containing the numbers for all test cases in order, with each test case contributing two numbers in sequence, first $\\mathrm{ID}(C)$ then $\\mathrm{OP}(W)$. Thus, the final output must be a single line in the format\n$$\n[\\mathrm{ID}_1,\\mathrm{OP}_1,\\mathrm{ID}_2,\\mathrm{OP}_2,\\mathrm{ID}_3,\\mathrm{OP}_3],\n$$\nwhere each item is a floating-point number. No other text may be printed.", "solution": "The problem is valid as it is scientifically grounded in the theory of Restricted Boltzmann Machines (RBMs), mathematically well-posed, objective, and provides all necessary information for a unique solution.\n\nOur objective is to compute the independence deviation metric $\\mathrm{ID}(C)$ and the orthogonality penalty $\\mathrm{OP}(W)$ for three given test cases. This requires a multi-step derivation and calculation.\n\nFirst, we must derive the expression for the conditional expectation of the hidden units given the visible units, $\\mathbb{E}[h \\mid v]$. The conditional probability distribution $p(h \\mid v)$ is given by the ratio of the joint probability $p(v,h)$ to the marginal probability $p(v)$:\n$$\np(h \\mid v) = \\frac{p(v,h)}{p(v)} = \\frac{\\frac{1}{Z} \\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\frac{1}{Z} \\exp(-E(v,h'))} = \\frac{\\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(-E(v,h'))}.\n$$\nSubstituting the energy function $E(v,h) = -a^{\\top} v - b^{\\top} h - v^{\\top} W h$:\n$$\np(h \\mid v) = \\frac{\\exp(a^{\\top} v + b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(a^{\\top} v + b^{\\top} h' + v^{\\top} W h')}.\n$$\nThe term $\\exp(a^{\\top} v)$ is constant with respect to the summation variable $h'$ in the denominator, so it can be factored out and canceled with the term in the numerator. This demonstrates that the conditional distribution $p(h \\mid v)$ is independent of the visible bias vector $a$, as noted in the problem statement. The expression simplifies to:\n$$\np(h \\mid v) = \\frac{\\exp(b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(b^{\\top} h' + v^{\\top} W h')}.\n$$\nThe exponent can be rewritten as a sum over the hidden units: $b^{\\top} h + v^{\\top} W h = \\sum_{j=1}^{n} (b_j h_j + (v^{\\top} W)_{j} h_j) = \\sum_{j=1}^{n} (b_j + v^{\\top} W_{:,j}) h_j$. Due to the lack of connections between hidden units in an RBM, the joint conditional probability factorizes into a product of individual conditional probabilities:\n$$\np(h \\mid v) = \\prod_{j=1}^{n} p(h_j \\mid v).\n$$\nThis conditional independence is a key property of RBMs. For a single binary hidden unit $h_j \\in \\{0,1\\}$, its conditional probability is:\n$$\np(h_j=1 \\mid v) = \\frac{\\exp(b_j + v^{\\top} W_{:,j})}{1 + \\exp(b_j + v^{\\top} W_{:,j})} = \\frac{1}{1 + \\exp(-(b_j + v^{\\top} W_{:,j}))} = \\sigma(b_j + v^{\\top} W_{:,j}),\n$$\nwhere $\\sigma(x) = 1/(1+e^{-x})$ is the logistic sigmoid function.\nThe expectation of a binary variable is the probability of it being $1$. Thus, the expectation of the $j$-th hidden unit is:\n$$\n\\mathbb{E}[h_j \\mid v] = 1 \\cdot p(h_j=1 \\mid v) + 0 \\cdot p(h_j=0 \\mid v) = p(h_j=1 \\mid v).\n$$\nIn vector form, the conditional expectation of the hidden vector $h$ given a visible vector $v$ is:\n$$\n\\mathbb{E}[h \\mid v] = \\sigma(b + W^{\\top}v),\n$$\nwhere the sigmoid function $\\sigma$ is applied element-wise to the vector argument $b + W^{\\top}v$.\n\nWith this result, we can outline the computational procedure:\n\n1.  **Compute Expected Activations**: For each visible vector $v^{(i)}$ in the dataset $\\{v^{(i)}\\}_{i=1}^{N}$, calculate the vector of expected hidden activations $h^{(i)*} = \\mathbb{E}[h \\mid v^{(i)}]$. These $N$ vectors, each of size $n \\times 1$, are then used to form the rows of a matrix $H \\in \\mathbb{R}^{N \\times n}$, where the $i$-th row is $(h^{(i)*})^{\\top}$.\n\n2.  **Compute Sample Covariance Matrix $C$**: First, compute the sample mean of the expected hidden activations, which is an $n$-dimensional column vector $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} h^{(i)*}$. Then, compute the $n \\times n$ sample covariance matrix $C$ using the provided formula with a denominator of $N-1$ for an unbiased estimate:\n$$\nC = \\frac{1}{N-1} \\sum_{i=1}^{N} (h^{(i)*} - \\mu)(h^{(i)*} - \\mu)^{\\top}.\n$$\n\n3.  **Compute Independence Deviation $\\mathrm{ID}(C)$**: Using the computed matrix $C$, we calculate the independence deviation metric. This involves partitioning the sum of squared elements of $C$ into diagonal and off-diagonal components:\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon}.\n$$\nThe numerator is a measure of the total off-diagonal covariance, while the denominator is a measure of the total variance. A small value signifies that the expected hidden features are approximately uncorrelated across the dataset. The constant $\\epsilon = 10^{-12}$ ensures numerical stability if the variances are all zero.\n\n4.  **Compute Orthogonality Penalty $\\mathrm{OP}(W)$**: This metric quantifies the non-orthogonality of the weight vectors associated with the hidden units. First, we compute the Gram matrix $G = W^{\\top} W$. Then, similar to $\\mathrm{ID}(C)$, we calculate $\\mathrm{OP}(W)$:\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\nThe term $G_{ij} = (W_{:,i})^{\\top}W_{:,j}$ is the dot product of the weight vectors for hidden units $i$ and $j$. A small value of $\\mathrm{OP}(W)$ indicates that these weight vectors are nearly orthogonal.\n\nThese steps are implemented for each of the three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the independence deviation and orthogonality penalty for given RBM parameters.\n    \"\"\"\n\n    def sigmoid(x):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def compute_metrics(W, b, V, epsilon):\n        \"\"\"\n        Computes ID(C) and OP(W) for a single RBM configuration.\n\n        Args:\n            W (np.ndarray): Weight matrix of size (m, n).\n            b (np.ndarray): Hidden bias vector of size (n, 1).\n            V (np.ndarray): Dataset of visible vectors of size (N, m).\n            epsilon (float): Small constant for numerical stability.\n\n        Returns:\n            tuple: A tuple containing (id_c, op_w).\n        \"\"\"\n        N, m = V.shape\n        n = W.shape[1]\n\n        # 1. Compute H, the matrix of expected hidden activations\n        H = np.zeros((N, n))\n        for i in range(N):\n            v_i = V[i, :].reshape(-1, 1)  # Shape (m, 1)\n            # Argument to sigmoid: b (n,1) + W.T (n,m) @ v_i (m,1) -> (n,1)\n            h_exp = sigmoid(b + W.T @ v_i)\n            H[i, :] = h_exp.T  # Store as a row vector (1, n)\n\n        # 2. Compute the sample covariance matrix C\n        if N <= 1:\n            C = np.zeros((n, n))\n        else:\n            # np.cov with rowvar=False computes covariance of columns.\n            # ddof=1 uses N-1 in the denominator for an unbiased estimate.\n            C = np.cov(H, rowvar=False, ddof=1)\n            # Manually:\n            # mu = np.mean(H, axis=0)\n            # H_centered = H - mu.reshape(1, -1)\n            # C = (H_centered.T @ H_centered) / (N - 1)\n\n        # 3. Compute ID(C)\n        diag_C_sq_sum = np.sum(np.diag(C)**2)\n        total_C_sq_sum = np.sum(C**2)\n        off_diag_C_sq_sum = total_C_sq_sum - diag_C_sq_sum\n        id_c = off_diag_C_sq_sum / (diag_C_sq_sum + epsilon)\n\n        # 4. Compute G and OP(W)\n        G = W.T @ W\n        diag_G_sq_sum = np.sum(np.diag(G)**2)\n        total_G_sq_sum = np.sum(G**2)\n        off_diag_G_sq_sum = total_G_sq_sum - diag_G_sq_sum\n        op_w = off_diag_G_sq_sum / (diag_G_sq_sum + epsilon)\n\n        return id_c, op_w\n\n    epsilon = 1e-12\n    m, n = 4, 3\n    V_data = np.array([\n        [0., 0., 0., 0.], [1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.],\n        [1., 1., 0., 0.], [1., 0., 1., 0.], [0., 1., 1., 0.], [1., 1., 1., 0.]\n    ])\n    b_common = np.zeros((n, 1))\n\n    test_cases = [\n        # Test case 1 (general \"happy path\"): Orthogonal weights\n        (\n            np.array([[5., 0., 0.], [0., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 2 (correlated hidden features): Correlated weights\n        (\n            np.array([[5., 5., 0.], [5., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 3 (boundary case): Zero weights\n        (\n            np.zeros((m, n)),\n            b_common,\n            V_data\n        ),\n    ]\n\n    results = []\n    for W, b, V in test_cases:\n        id_c, op_w = compute_metrics(W, b, V, epsilon)\n        results.append(id_c)\n        results.append(op_w)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3170455"}, {"introduction": "The energy-based formulation of RBMs offers a unique perspective on advanced topics like adversarial robustness. This practice challenges you to frame an \"adversarial attack\" not as fooling a classifier, but as finding a minimal input perturbation that maximally increases the model's free energy, or \"surprise\". You will implement a gradient-based method to craft these perturbations and investigate whether modified training techniques, like tempered sampling, can build more robust generative models. [@problem_id:3170453]", "problem": "Consider a Restricted Boltzmann Machine (RBM) with Bernoulli visible units and Bernoulli hidden units. The RBM defines a joint distribution over a visible vector $v \\in [0,1]^n$ and a hidden vector $h \\in \\{0,1\\}^m$ through an energy function. Use the following universally accepted foundations as the starting point. An energy-based model defines the joint probability as $P(v,h) \\propto \\exp(-E(v,h))$. For a Bernoulli-Bernoulli RBM, the energy function is given by $E(v,h) = -b^{\\top} v - c^{\\top} h - v^{\\top} W h$, where $W \\in \\mathbb{R}^{n \\times m}$ is the weight matrix, $b \\in \\mathbb{R}^{n}$ is the visible bias vector, and $c \\in \\mathbb{R}^{m}$ is the hidden bias vector. The free energy of the visible vector is defined by marginalizing the hidden units: $F(v) = -\\log \\sum_{h} \\exp(-E(v,h))$. Adversarial robustness in this context refers to the difficulty of increasing the free energy for perturbed inputs: given a small perturbation budget, the model is more robust if $F(v + \\delta)$ is less increased by optimal perturbations $\\delta$.\n\nYour task is to implement and evaluate adversarial robustness against perturbations $\\delta$ that increase the free energy, and to test whether training with tempered sampling in the negative phase increases robustness, relative to standard training. Solve the following steps from first principles.\n\n1. Starting from the given energy function $E(v,h)$ and the definition of free energy $F(v)$, derive the explicit form of $F(v)$ for a Bernoulli-Bernoulli RBM in terms of $W$, $b$, and $c$, and derive the gradient $\\nabla_v F(v)$ with respect to $v$. Then, from the constraint $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ and $0 \\le v + \\delta \\le 1$, derive a first-order adversarial perturbation rule that approximately maximizes $F(v + \\delta)$.\n\n2. Implement two training procedures for the RBM on a synthetic dataset of $N$ samples in $[0,1]^n$:\n   - Standard Contrastive Divergence with one step in the negative phase, using Gibbs sampling at temperature $\\tau = 1$.\n   - Tempered negative-phase sampling at temperature $\\tau > 1$. In temperature-based sampling, the Boltzmann distribution is modified as $P(v,h) \\propto \\exp(-E(v,h)/\\tau)$. For Gibbs conditionals, use $p(h_j = 1 \\mid v) = \\sigma((c_j + W_{\\cdot j}^{\\top} v)/\\tau)$ and $p(v_i = 1 \\mid h) = \\sigma((b_i + W_{i \\cdot}^{\\top} h)/\\tau)$ during the negative phase, while keeping the positive phase at $\\tau = 1$. Here $\\sigma(x)$ denotes the logistic sigmoid function.\n\n3. For each trained RBM, implement a fast adversarial crafting method that, for each data point $v$, computes a perturbation $\\delta$ that aims to maximally increase $F(v + \\delta)$ under the $\\ell_{\\infty}$ constraint $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ and the box constraint $0 \\le v + \\delta \\le 1$. Use the gradient of the free energy with respect to $v$ to produce an explicit perturbation, and clip the adversarial example back into the valid range.\n\n4. Define a robustness metric as the mean free-energy increase over the dataset under this adversarial perturbation: compute $\\Delta F_{\\text{mean}} = \\frac{1}{N} \\sum_{k=1}^{N} \\left[ F(v_k^{\\text{adv}}) - F(v_k) \\right]$, where $v_k^{\\text{adv}}$ is the adversarially perturbed version of $v_k$. A model is considered more robust if this mean increase is smaller.\n\n5. Evaluate whether the tempered-training RBM is more robust than the standard-training RBM for the test suite specified below. For each test case, produce a boolean indicating whether tempered training yields $\\Delta F_{\\text{mean}}$ less than or equal to the standard training’s $\\Delta F_{\\text{mean}}$.\n\nUse the following test suite of parameter values, which should probe a normal case, a boundary condition, and a larger-perturbation edge case:\n- Case $1$: $\\epsilon = 0.0$, $\\tau = 1.5$.\n- Case $2$: $\\epsilon = 0.05$, $\\tau = 1.5$.\n- Case $3$: $\\epsilon = 0.2$, $\\tau = 2.0$.\n\nYour program must:\n- Construct a small synthetic dataset in $[0,1]^n$ with $n = 6$ and $N = 64$, consisting of two clusters around two distinct prototype patterns, with small noise added and clipped to $[0,1]$.\n- Train two RBMs with $m = 4$ hidden units: one with standard training ($\\tau = 1$ in the negative phase), and one with tempered negative-phase sampling using the test case’s $\\tau$.\n- For each test case, compute the boolean result described in step $5$.\n\nYour program should produce a single line of output containing the three boolean results for the three test cases, as a comma-separated list enclosed in square brackets, for example $[{\\text{True}},{\\text{False}},{\\text{True}}]$. No physical units are involved in this problem. Angles are not used. Percentages are not used; all quantities are dimensionless real numbers, and all required outputs are booleans. The program must be self-contained and require no user input.", "solution": "The problem requires a derivation of the free energy and its gradient for a Bernoulli-Bernoulli Restricted Boltzmann Machine (RBM), followed by an implementation to compare the adversarial robustness of two training methods: standard Contrastive Divergence (CD) and CD with tempered negative-phase sampling.\n\n### Part 1: Analytical Derivations\n\nWe begin from the foundational definitions provided for the RBM. The model consists of visible units $v \\in [0,1]^n$ and hidden units $h \\in \\{0,1\\}^m$. The joint probability distribution is given by an energy function: $P(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$, where $Z$ is the partition function. The energy function is defined as:\n$$E(v,h) = -b^{\\top} v - c^{\\top} h - v^{\\top} W h = -\\sum_{i=1}^n b_i v_i - \\sum_{j=1}^m c_j h_j - \\sum_{i=1}^n \\sum_{j=1}^m v_i W_{ij} h_j$$\nwhere $W \\in \\mathbb{R}^{n \\times m}$ is the weight matrix, $b \\in \\mathbb{R}^{n}$ is the visible bias vector, and $c \\in \\mathbb{R}^{m}$ is the hidden bias vector.\n\n#### 1.1. Free Energy $F(v)$\n\nThe free energy $F(v)$ is defined by marginalizing the joint distribution over the hidden units:\n$$F(v) = -\\log \\sum_{h} \\exp(-E(v,h))$$\nSubstituting the expression for $E(v,h)$:\n$$F(v) = -\\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(b^{\\top} v + c^{\\top} h + v^{\\top} W h\\right)$$\nWe can separate the term that does not depend on $h$:\n$$F(v) = -\\log \\left( \\exp(b^{\\top} v) \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(c^{\\top} h + v^{\\top} W h\\right) \\right)$$\n$$F(v) = -b^{\\top} v - \\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(\\sum_{j=1}^m c_j h_j + \\sum_{j=1}^m \\sum_{i=1}^n v_i W_{ij} h_j\\right)$$\nLet $W_{\\cdot j}$ be the $j$-th column of $W$. The term $v^{\\top} W h$ can be rewritten as $\\sum_{j=1}^m h_j (v^{\\top} W_{\\cdot j})$.\n$$F(v) = -b^{\\top} v - \\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(\\sum_{j=1}^m h_j (c_j + v^{\\top} W_{\\cdot j})\\right)$$\nSince the hidden units $h_j$ are binary and independent in the sum, we can factor the sum over all configurations of $h$ into a product over individual hidden units:\n$$\\sum_{h \\in \\{0,1\\}^m} \\prod_{j=1}^m \\exp\\left(h_j (c_j + v^{\\top} W_{\\cdot j})\\right) = \\prod_{j=1}^m \\sum_{h_j \\in \\{0,1\\}} \\exp\\left(h_j (c_j + v^{\\top} W_{\\cdot j})\\right)$$\nFor each $j$, the sum is over $h_j=0$ and $h_j=1$:\n$$\\sum_{h_j \\in \\{0,1\\}} \\exp\\left(h_j (c_j + v^{\\top} W_{\\cdot j})\\right) = \\exp(0) + \\exp(c_j + v^{\\top} W_{\\cdot j}) = 1 + \\exp(c_j + v^{\\top} W_{\\cdot j})$$\nSubstituting this back into the expression for $F(v)$:\n$$F(v) = -b^{\\top} v - \\log \\left(\\prod_{j=1}^m (1 + \\exp(c_j + v^{\\top} W_{\\cdot j}))\\right)$$\n$$F(v) = -b^{\\top} v - \\sum_{j=1}^m \\log(1 + \\exp(c_j + v^{\\top} W_{\\cdot j}))$$\nThis is the explicit form of the free energy. The term $\\log(1 + \\exp(x))$ is known as the softplus function.\n\n#### 1.2. Gradient of Free Energy $\\nabla_v F(v)$\n\nTo find the adversarial perturbation, we first need the gradient of the free energy with respect to the visible vector $v$. We differentiate $F(v)$ with respect to a component $v_k$ of $v$:\n$$\\frac{\\partial F(v)}{\\partial v_k} = \\frac{\\partial}{\\partial v_k} \\left( -\\sum_{i=1}^n b_i v_i - \\sum_{j=1}^m \\log(1 + \\exp(c_j + \\sum_{i=1}^n v_i W_{ij})) \\right)$$\n$$\\frac{\\partial F(v)}{\\partial v_k} = -b_k - \\sum_{j=1}^m \\frac{\\partial}{\\partial v_k} \\log(1 + \\exp(c_j + v^{\\top} W_{\\cdot j}))$$\nUsing the chain rule, and recalling that $\\frac{d}{dx} \\log(1+e^x) = \\frac{e^x}{1+e^x} = \\sigma(x)$, where $\\sigma(x)$ is the logistic sigmoid function:\n$$\\frac{\\partial}{\\partial v_k} \\log(1 + \\exp(c_j + v^{\\top} W_{\\cdot j})) = \\sigma(c_j + v^{\\top} W_{\\cdot j}) \\cdot \\frac{\\partial}{\\partial v_k}(c_j + \\sum_{i=1}^n v_i W_{ij}) = \\sigma(c_j + v^{\\top} W_{\\cdot j}) \\cdot W_{kj}$$\nSubstituting this into the gradient expression:\n$$\\frac{\\partial F(v)}{\\partial v_k} = -b_k - \\sum_{j=1}^m W_{kj} \\sigma(c_j + v^{\\top} W_{\\cdot j})$$\nNote that $\\sigma(c_j + v^{\\top} W_{\\cdot j})$ is the conditional probability $p(h_j=1|v)$. In vector notation, the gradient is:\n$$\\nabla_v F(v) = -b - W \\cdot \\sigma(c + W^{\\top} v)$$\n\n#### 1.3. First-Order Adversarial Perturbation\n\nThe goal is to find a perturbation $\\delta$ that maximizes $F(v+\\delta)$ subject to the constraints $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ and $0 \\le v + \\delta \\le 1$. A first-order Taylor expansion of $F(v+\\delta)$ around $v$ is:\n$$F(v+\\delta) \\approx F(v) + \\delta^{\\top} \\nabla_v F(v)$$\nTo maximize this linear approximation, $\\delta$ should be aligned with the gradient $\\nabla_v F(v)$. Given the $\\ell_{\\infty}$-norm constraint, the optimal perturbation is:\n$$\\delta = \\epsilon \\cdot \\text{sign}(\\nabla_v F(v))$$\nThis is the principle of the Fast Gradient Sign Method (FGSM). The resulting adversarial candidate is $v' = v + \\delta$. To satisfy the box constraint $0 \\le v + \\delta \\le 1$, we must clip the result:\n$$v^{\\text{adv}} = \\text{clip}(v + \\epsilon \\cdot \\text{sign}(\\nabla_v F(v)), 0, 1)$$\n\n### Part 2: Algorithmic Design\n\n#### 2.1. RBM Training (Contrastive Divergence)\n\nThe RBM parameters ($W, b, c$) are trained using Contrastive Divergence (CD-1), an approximation to the gradient of the log-likelihood. The update rules are derived from this gradient:\n$$\\Delta W \\propto v_0 p(h|v_0)^{\\top} - v_1 p(h|v_1)^{\\top}$$\n$$\\Delta b \\propto v_0 - v_1$$\n$$\\Delta c \\propto p(h|v_0) - p(h|v_1)$$\nwhere $v_0$ is a data sample, and $v_1$ is a one-step reconstruction. The process is:\n1.  **Positive Phase**: For a data vector $v_0$, compute the hidden unit activation probabilities $p(h|v_0) = \\sigma(v_0 W + c)$.\n2.  **Negative Phase**: Generate a reconstruction $v_1$ from the hidden activations. For continuous inputs, we use the probabilities: $p(v|h) = \\sigma(h W^{\\top} + b)$. We then compute the new hidden probabilities $p(h|v_1) = \\sigma(v_1 W + c)$.\n\nFor this problem, two training variants are implemented:\n-   **Standard Training**: All phases use a temperature $\\tau=1$. The conditional probabilities are exactly as described above.\n-   **Tempered Training**: The positive phase remains at $\\tau=1$. The negative phase uses a temperature $\\tau > 1$. The conditional probabilities are modified by dividing their arguments by $\\tau$:\n    $$p_{\\tau}(v=1|h) = \\sigma((h W^{\\top} + b)/\\tau)$$\n    $$p_{\\tau}(h=1|v) = \\sigma((v W + c)/\\tau)$$\nThis tempering smooths the distribution in the negative phase, which can influence the learning dynamics and model robustness. To reduce variance, we use the activation probabilities directly rather than sampling binary states from them.\n\n#### 2.2. Robustness Evaluation\n\nFor each trained RBM, we quantify its adversarial robustness using the specified metric.\n1.  For each data point $v_k$ in the dataset, we generate an adversarial counterpart $v_k^{\\text{adv}}$ using the FGSM-based rule derived in Part 1.3 with a given perturbation budget $\\epsilon$.\n2.  We compute the free energy for the original point, $F(v_k)$, and for the adversarial point, $F(v_k^{\\text{adv}})$, using the formula derived in Part 1.1.\n3.  The robustness metric is the mean increase in free energy across the dataset:\n    $$\\Delta F_{\\text{mean}} = \\frac{1}{N} \\sum_{k=1}^{N} \\left[ F(v_k^{\\text{adv}}) - F(v_k) \\right]$$\nA smaller $\\Delta F_{\\text{mean}}$ indicates that the model is more robust, as the adversarial attack is less effective at increasing the free energy.\n\n### Part 3: Experimental Protocol\n\nThe comparison is performed on a synthetic dataset ($N=64, n=6$) composed of two noisy clusters. For each test case defined by $(\\epsilon, \\tau)$:\n1.  Two RBMs ($m=4$) are initialized with the same random weights and biases to ensure a fair comparison.\n2.  One RBM is trained using standard CD-1 ($\\tau_{\\text{neg}}=1$), and the other is trained with tempered negative-phase CD-1 ($\\tau_{\\text{neg}}=\\tau$).\n3.  For both trained models, $\\Delta F_{\\text{mean}}$ is calculated using the test case's $\\epsilon$.\n4.  The final result is a boolean value indicating whether the tempered model is more robust or equally robust, i.e., whether $\\Delta F_{\\text{mean, tempered}} \\le \\Delta F_{\\text{mean, standard}}$.\n\nThe test case with $\\epsilon=0.0$ serves as a sanity check. Since the perturbation is zero, $\\Delta F_{\\text{mean}}$ will be $0$ for both models, and the comparison $0 \\le 0$ will correctly yield `True`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _sigmoid(x):\n    \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-x))\n\nclass RBM:\n    \"\"\"A Bernoulli-Bernoulli Restricted Boltzmann Machine.\"\"\"\n    \n    def __init__(self, n_vis, n_hid, seed=None):\n        \"\"\"\n        Initializes the RBM.\n        \n        Args:\n            n_vis (int): Number of visible units.\n            n_hid (int): Number of hidden units.\n            seed (int, optional): Seed for the random number generator.\n        \"\"\"\n        self.rng = np.random.default_rng(seed)\n        self.n_vis = n_vis\n        self.n_hid = n_hid\n        \n        # Initialize weights with small random values and biases to zero.\n        scale = 0.01\n        self.W = self.rng.normal(loc=0.0, scale=scale, size=(n_vis, n_hid))\n        self.b = np.zeros(n_vis)\n        self.c = np.zeros(n_hid)\n        \n    def copy_params_from(self, other_rbm):\n        \"\"\"Copies parameters from another RBM instance for fair comparison.\"\"\"\n        self.W = np.copy(other_rbm.W)\n        self.b = np.copy(other_rbm.b)\n        self.c = np.copy(other_rbm.c)\n        self.rng = other_rbm.rng # ensure samplers are also identical if used\n\n    def free_energy(self, v):\n        \"\"\"\n        Computes the free energy for a batch of visible vectors.\n        \n        Args:\n            v (np.ndarray): A batch of visible vectors, shape (N, n_vis).\n        \n        Returns:\n            np.ndarray: An array of free energy values, shape (N,).\n        \"\"\"\n        if v.ndim == 1:\n            v = v.reshape(1, -1)\n        \n        vb_term = v @ self.b\n        vWc = (v @ self.W) + self.c\n        # np.logaddexp(0, x) is a numerically stable way to compute log(1 + exp(x))\n        hidden_term = np.sum(np.logaddexp(0, vWc), axis=1)\n        \n        return -vb_term - hidden_term\n\n    def grad_free_energy(self, v):\n        \"\"\"\n        Computes the gradient of the free energy with respect to v.\n        \n        Args:\n            v (np.ndarray): A batch of visible vectors, shape (N, n_vis).\n        \n        Returns:\n            np.ndarray: The gradient of F(v) w.r.t v, shape (N, n_vis).\n        \"\"\"\n        if v.ndim == 1:\n            v = v.reshape(1, -1)\n        \n        hidden_probs = _sigmoid((v @ self.W) + self.c)\n        # Gradient is -b - W * sigma(c + W^T * v)\n        # The -self.b term is broadcasted to match the batch size.\n        grad = -self.b - (hidden_probs @ self.W.T)\n        \n        return grad\n\n    def train(self, data, epochs, learning_rate, temp_neg_phase=1.0):\n        \"\"\"\n        Trains the RBM using Contrastive Divergence (CD-1).\n        \n        Args:\n            data (np.ndarray): The training data, shape (N, n_vis).\n            epochs (int): Number of training epochs.\n            learning_rate (float): The learning rate for parameter updates.\n            temp_neg_phase (float): Temperature for negative phase sampling.\n        \"\"\"\n        n_samples = data.shape[0]\n        v0 = data\n        \n        for _ in range(epochs):\n            # Positive phase (tau=1)\n            pos_hidden_probs = _sigmoid((v0 @ self.W) + self.c)\n            \n            # Negative phase (reconstruction)\n            # Use probabilities for reconstruction (mean-field)\n            neg_vis_probs = _sigmoid(((pos_hidden_probs @ self.W.T) + self.b) / temp_neg_phase)\n            neg_hidden_probs = _sigmoid(((neg_vis_probs @ self.W) + self.c) / temp_neg_phase)\n\n            # Update parameters\n            dW = (v0.T @ pos_hidden_probs) - (neg_vis_probs.T @ neg_hidden_probs)\n            db = np.sum(v0 - neg_vis_probs, axis=0)\n            dc = np.sum(pos_hidden_probs - neg_hidden_probs, axis=0)\n            \n            self.W += learning_rate * dW / n_samples\n            self.b += learning_rate * db / n_samples\n            self.c += learning_rate * dc / n_samples\n\ndef generate_dataset(N, n, seed=None):\n    \"\"\"Generates a synthetic dataset with two clusters.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    p1 = np.array([1, 1, 1, 0, 0, 0])\n    p2 = np.array([0, 0, 0, 1, 1, 1])\n    \n    data = np.zeros((N, n))\n    \n    n_half = N // 2\n    data[:n_half, :] = p1 + rng.normal(loc=0.0, scale=0.1, size=(n_half, n))\n    data[n_half:, :] = p2 + rng.normal(loc=0.0, scale=0.1, size=(N - n_half, n))\n    \n    return np.clip(data, 0, 1)\n\ndef generate_adversarial(rbm, v_data, epsilon):\n    \"\"\"Generates adversarial examples using FGSM on the free energy.\"\"\"\n    if epsilon == 0.0:\n        return np.copy(v_data)\n        \n    grad = rbm.grad_free_energy(v_data)\n    delta = epsilon * np.sign(grad)\n    v_adv = np.clip(v_data + delta, 0, 1)\n    \n    return v_adv\n\ndef evaluate_robustness(rbm, data, epsilon):\n    \"\"\"Computes the mean increase in free energy under adversarial attack.\"\"\"\n    if epsilon == 0.0:\n        return 0.0\n        \n    data_adv = generate_adversarial(rbm, data, epsilon)\n    \n    F_orig = rbm.free_energy(data)\n    F_adv = rbm.free_energy(data_adv)\n    \n    delta_F = F_adv - F_orig\n    \n    return np.mean(delta_F)\n\ndef solve():\n    # Fixed parameters for the experiment\n    N = 64\n    n_vis = 6\n    n_hid = 4\n    epochs = 1000\n    learning_rate = 0.1\n    master_seed = 42\n\n    test_cases = [\n        (0.0, 1.5),\n        (0.05, 1.5),\n        (0.2, 2.0),\n    ]\n\n    results = []\n    \n    # Generate a single dataset for all test cases\n    dataset = generate_dataset(N, n_vis, seed=master_seed)\n    \n    for i, (epsilon, tau) in enumerate(test_cases):\n        # We must re-initialize and retrain models for each case to ensure\n        # that the comparison is based on the specific tau.\n        # Use a consistent seed for RBM initialization across runs for reproducibility.\n        rbm_init_seed = master_seed + 1 + i\n\n        # Standard RBM (train with tau=1)\n        rbm_std = RBM(n_vis, n_hid, seed=rbm_init_seed)\n        rbm_std.train(dataset, epochs, learning_rate, temp_neg_phase=1.0)\n        \n        # Tempered RBM (train with given tau)\n        # Initialize with the same weights for a fair comparison.\n        rbm_temp = RBM(n_vis, n_hid, seed=rbm_init_seed) # seed ensures identical init\n        rbm_temp.train(dataset, epochs, learning_rate, temp_neg_phase=tau)\n        \n        # Evaluate robustness\n        delta_F_std = evaluate_robustness(rbm_std, dataset, epsilon)\n        delta_F_temp = evaluate_robustness(rbm_temp, dataset, epsilon)\n        \n        # Compare and store result\n        is_more_robust = delta_F_temp = delta_F_std\n        results.append(is_more_robust)\n\n    # Format the final output string\n    # str(True) -> 'True', so no special capitalization logic is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3170453"}]}