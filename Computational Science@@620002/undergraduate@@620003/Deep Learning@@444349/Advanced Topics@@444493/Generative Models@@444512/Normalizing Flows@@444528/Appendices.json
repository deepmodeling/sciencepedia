{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we connect the core principle of normalizing flows—the change of variables formula—to a familiar operation from convolutional neural networks: the $1 \\times 1$ convolution. This operation acts as an invertible channel-mixing transformation, and understanding its Jacobian is a crucial first step. This practice will guide you through deriving the log-determinant for such a layer, revealing how the overall Jacobian determinant for a high-dimensional tensor is elegantly composed from the determinant of the small channel-mixing matrix, a key insight for building scalable flow architectures for image data. [@problem_id:3160147]", "problem": "Consider a single layer of a Normalizing Flow (NF) that applies an invertible $1 \\times 1$ convolution across channels at each spatial location. Let the input be a single sample tensor $X \\in \\mathbb{R}^{C \\times H \\times W}$ and the output be $Y \\in \\mathbb{R}^{C \\times H \\times W}$. At each spatial coordinate $(h,w)$, the layer acts as a channel-wise linear map $Y_{:,h,w} = A X_{:,h,w}$, where $A \\in \\mathbb{R}^{C \\times C}$ is an invertible matrix shared across all spatial locations. To avoid ambiguity with the image width, the channel mixing matrix is denoted by $A$ (this matrix is often denoted by $W$ in the literature). Assume a standard change-of-variables setting for probability densities under a differentiable bijection and use only fundamental linear algebra facts about Jacobians and determinants.\n\nTasks:\n1. Starting from the change-of-variables principle for densities under a differentiable bijection and basic properties of Jacobians and determinants, derive the exact expression for the log-determinant term $\\ln |\\det J_f(X)|$ of this layer, where $f$ maps $X$ to $Y$ and $J_f(X)$ is the Jacobian of $f$ with respect to the vectorized input $X$. Your derivation should make precise how this quantity depends on the spatial dimensions $H$ and $W$ and on the channel mixing matrix $A$.\n2. Using your derived expression, evaluate the log-determinant for the concrete case $C = 3$, $H = 16$, $W = 16$, and \n$$\nA = \\mathrm{diag}\\!\\left(2, \\frac{1}{3}, \\frac{9}{2}\\right).\n$$\n\nProvide your final answer as a single closed-form analytic expression (do not numerically approximate; no rounding is required). If you wish to connect to the literature’s notation, you may interpret $\\ln|\\det A|$ as $\\ln|\\det W|$ for the channel mixing matrix, but be explicit in your derivation. The final answer must be a single expression.", "solution": "The core of a normalizing flow is the change of variables formula for probability densities. If $X$ is a random variable with density $p_X(X)$ and $Y=f(X)$ is an invertible, differentiable transformation (a bijection), then the density of the random variable $Y$ is given by:\n$$\np_Y(Y) = p_X(f^{-1}(Y)) \\left| \\det J_{f^{-1}}(Y) \\right|\n$$\nwhere $J_{f^{-1}}(Y)$ is the Jacobian of the inverse transformation. Using the inverse function theorem, this can be expressed in terms of the forward transformation $f$:\n$$\np_Y(Y) = p_X(X) \\left| \\det J_f(X) \\right|^{-1}\n$$\nThe log-likelihood of observing a sample $X$ is typically computed, which involves the term $\\ln \\left| \\det J_f(X) \\right|$. We are tasked with deriving this term for the specified layer.\n\n**Part 1: Derivation of the log-determinant of the Jacobian**\n\nThe transformation $f$ maps an input tensor $X \\in \\mathbb{R}^{C \\times H \\times W}$ to an output tensor $Y \\in \\mathbb{R}^{C \\times H \\times W}$. To compute the Jacobian, we must first flatten the tensors $X$ and $Y$ into vectors. Let the total number of elements be $N = C \\times H \\times W$. We define a vectorization scheme. A natural choice, given the structure of the operation, is to group elements by their spatial location. Let $x_{h,w} \\in \\mathbb{R}^C$ denote the vector of channels at spatial position $(h,w)$, i.e., $x_{h,w} = X_{:,h,w}$. Similarly, let $y_{h,w} = Y_{:,h,w}$. The transformation is given as:\n$$\ny_{h,w} = A x_{h,w}\n$$\nfor each spatial location $(h,w)$, where $h \\in \\{1, ..., H\\}$ and $w \\in \\{1, ..., W\\}$.\n\nLet's form high-dimensional vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{N}$ by concatenating the vectors from all $H \\times W$ spatial locations. We can order these locations, for instance, lexicographically by $(h,w)$.\n$$\n\\mathbf{x} = \\begin{pmatrix} x_{1,1} \\\\ x_{1,2} \\\\ \\vdots \\\\ x_{H,W} \\end{pmatrix} \\quad \\text{and} \\quad \\mathbf{y} = \\begin{pmatrix} y_{1,1} \\\\ y_{1,2} \\\\ \\vdots \\\\ y_{H,W} \\end{pmatrix}\n$$\nThe transformation $f$ maps $\\mathbf{x}$ to $\\mathbf{y}$. The Jacobian of $f$, denoted $J_f(\\mathbf{x})$, is an $N \\times N$ matrix whose entries are the partial derivatives $\\frac{\\partial y_i}{\\partial x_j}$. Due to the nature of the transformation, the output at one spatial location $(h,w)$ depends only on the input at the same spatial location.\n$$\n\\frac{\\partial y_{h,w}}{\\partial x_{h',w'}} = \\begin{cases} \\frac{\\partial (A x_{h,w})}{\\partial x_{h,w}} & \\text{if } (h,w) = (h',w') \\\\ 0 & \\text{if } (h,w) \\neq (h',w') \\end{cases}\n$$\nThe derivative of the linear map $z \\mapsto Az$ with respect to $z$ is the matrix $A$ itself. Therefore, $\\frac{\\partial y_{h,w}}{\\partial x_{h,w}} = A$.\n\nThis means the full Jacobian matrix $J_f(\\mathbf{x})$ is block-diagonal. It consists of $H \\times W$ blocks along its diagonal, and each block is the $C \\times C$ matrix $A$. All off-diagonal blocks are $C \\times C$ zero matrices.\n$$\nJ_f(\\mathbf{x}) = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{pmatrix}\nA & 0 & \\cdots & 0 \\\\\n0 & A & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & A\n\\end{pmatrix}\n$$\nThis Jacobian is a constant matrix; it does not depend on the input $X$. This is a property of linear transformations.\n\nThe determinant of a block-diagonal matrix is the product of the determinants of the blocks on the diagonal. In this case, there are $H \\times W$ identical blocks, each being the matrix $A$.\n$$\n\\det J_f(X) = \\prod_{i=1}^{H \\times W} \\det(A) = (\\det A)^{H \\times W}\n$$\nThe final step is to compute the log of the absolute value of this determinant.\n$$\n\\ln |\\det J_f(X)| = \\ln |(\\det A)^{H \\times W}|\n$$\nUsing the logarithm property $\\ln(a^b) = b \\ln(a)$ and the absolute value property $|a^b| = |a|^b$, we get:\n$$\n\\ln |\\det J_f(X)| = (H \\times W) \\ln |\\det A|\n$$\nThis is the derived expression for the log-determinant of the Jacobian, which explicitly shows its dependence on the spatial dimensions $H$ and $W$ and the channel mixing matrix $A$.\n\n**Part 2: Evaluation for the specific case**\n\nWe are given the following values:\n- $C = 3$ (number of channels)\n- $H = 16$ (height)\n- $W = 16$ (width)\n- $A = \\mathrm{diag}\\!\\left(2, \\frac{1}{3}, \\frac{9}{2}\\right)$ (channel mixing matrix)\n\nFirst, we compute the determinant of the matrix $A$. Since $A$ is a diagonal matrix, its determinant is the product of its diagonal elements.\n$$\n\\det A = 2 \\times \\frac{1}{3} \\times \\frac{9}{2} = \\frac{18}{6} = 3\n$$\nThe value of the determinant is $3$, which is positive, so $|\\det A| = 3$.\n\nNext, we calculate the number of spatial locations, which is the product of the height and width.\n$$\nH \\times W = 16 \\times 16 = 256\n$$\nFinally, we substitute these values into the derived expression for the log-determinant:\n$$\n\\ln |\\det J_f(X)| = (H \\times W) \\ln |\\det A| = 256 \\ln(3)\n$$\nThis is the final answer in the required closed-form analytic expression.", "answer": "$$\\boxed{256 \\ln(3)}$$", "id": "3160147"}, {"introduction": "A theoretical understanding of the Jacobian is essential, but a practical implementation must also be numerically robust. The log-determinant term, $\\log|\\det J|$, which is central to the training objective of any normalizing flow, can be a source of instability in high-dimensional settings, where the determinant may become astronomically large or infinitesimally small. This exercise [@problem_id:3160140] addresses this critical challenge by demonstrating how to compute the log-determinant in a stable manner, avoiding the catastrophic effects of numerical overflow and underflow by working directly in log-space.", "problem": "You are given the task of analyzing and stabilizing the computation of $\\log|\\det J|$ for Jacobian matrices $J$ arising in normalizing flows, where the change-of-variables rule in probability density transformations requires evaluating $\\log|\\det J|$ accurately in potentially high-dimensional settings. Begin from fundamental and widely accepted bases: (i) the definition of the determinant as a volume scaling factor, (ii) the property that the determinant of a triangular matrix equals the product of its diagonal entries, and (iii) the identity that $\\log\\left(\\prod_{i=1}^n a_i\\right)=\\sum_{i=1}^n\\log a_i$ for positive $a_i$. Use these bases to reason about numerical overflow and underflow in floating-point arithmetic when computing products of many large or small numbers. Your goal is to articulate, implement, and test numerically stable parametrizations that avoid overflow and underflow when computing $\\log|\\det J|$ in high dimensions.\n\nSpecifically, derive principled computational strategies for the following settings without giving or using any shortcut formulas in the problem statement itself:\n- Diagonal Jacobian $J=\\mathrm{diag}(s_1,\\dots,s_d)$ with unconstrained real scales $s_i$, including negative entries, ensuring that the result is well-defined via $|\\det J|$ and that the computation avoids overflow/underflow.\n- Lower-triangular factor $L$ with strictly positive diagonal entries used to parametrize a symmetric positive definite matrix $A=LL^\\top$, relevant to certain normalizing flow components. Establish how to stably evaluate $\\log\\det A$ from $L$.\n\nPropose stable parametrizations suitable for deep learning implementations. At minimum, include:\n- An exponential parametrization $s_i=\\exp(\\alpha_i)$ for diagonal $J$ that enforces invertibility and stability, with $\\alpha_i\\in\\mathbb{R}$ unconstrained.\n- A Cholesky parametrization $A=LL^\\top$ with $L$ lower-triangular and diagonal entries strictly positive, making $A$ symmetric positive definite and providing a stable path to compute $\\log\\det A$.\n\nInvestigate floating-point behavior in the following test suite. Each test case specifies the dimension $d$ and a construction of $J$ or $L$. Your program must compute two quantities per case: a naive result that relies on direct products to form determinants and then applies the logarithm, and a stable result using the derivations you present. The final reported answer for each case must be a boolean reflecting whether numerical stability succeeded according to the specified rule for that case.\n\nTest suite:\n1. Happy path diagonal case: $d=16$ with $J=\\mathrm{diag}(\\mathbf{s})$ and $s_i$ linearly spaced between $0.8$ and $1.2$ (inclusive). A correct result requires both the naive and the stable computations to be finite and to agree within an absolute tolerance of $10^{-10}$.\n2. Overflow diagonal case: $d=1024$ with $J=\\mathrm{diag}(\\mathbf{s})$ and $s_i=10^{10}$ for all $i$. A correct result requires the naive computation to be non-finite (infinite) while the stable computation is finite.\n3. Underflow diagonal case: $d=1024$ with $J=\\mathrm{diag}(\\mathbf{s})$ and $s_i=10^{-10}$ for all $i$. A correct result requires the naive computation to be non-finite (negative infinite after taking the logarithm of an underflowed product) while the stable computation is finite.\n4. Cholesky overflow case: $d=256$ with $L$ lower-triangular and diagonal entries $10^{10}$, and strictly zero off-diagonal entries. Consider $A=LL^\\top$. A correct result requires the naive computation (based on forming $A$ and multiplying $A$'s diagonal entries in a product before taking the logarithm) to be non-finite, while the stable computation (based directly on $L$) is finite.\n5. Signed diagonal overflow case: $d=1024$ with $J=\\mathrm{diag}(\\mathbf{s})$ and $s_i=(-1)^i \\cdot 10^{10}$. A correct result requires the naive computation (based on the product of magnitudes) to be non-finite, while the stable computation (which must depend only on magnitudes inside the logarithm via $|\\det J|$) is finite.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the five test cases above. Each list element must be a boolean that matches the correctness rule for the corresponding case. No angles or physical units are involved. The program must be self-contained and must not read any input. Ensure the numerical implementation is robust in the presence of overflow and underflow events, and base your logic strictly on the principles outlined at the beginning without inserting any shortcut formulas into this problem statement itself.", "solution": "The problem requires the derivation and implementation of numerically stable methods for computing the logarithm of the absolute value of the determinant of a Jacobian matrix, $\\log|\\det J|$, a quantity central to the change of variables formula used in normalizing flows. The primary challenge arises from the finite precision and limited range of floating-point arithmetic, which can lead to numerical overflow or underflow when computing determinants in high-dimensional spaces. The analysis will be grounded in three fundamental principles provided: (i) the geometric interpretation of the determinant, (ii) the determinant of a triangular matrix, and (iii) the identity relating the logarithm of a product to the sum of logarithms.\n\nLet a random variable $\\mathbf{z} \\in \\mathbb{R}^d$ with probability density function $p_{\\mathbf{z}}(\\mathbf{z})$ be transformed by an invertible function $f: \\mathbb{R}^d \\to \\mathbb{R}^d$, yielding a new random variable $\\mathbf{x} = f(\\mathbf{z})$. The probability density function of $\\mathbf{x}$, $p_{\\mathbf{x}}(\\mathbf{x})$, is given by the change of variables formula:\n$$p_{\\mathbf{x}}(\\mathbf{x}) = p_{\\mathbf{z}}(f^{-1}(\\mathbf{x})) \\left| \\det \\left( \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|$$\nIn the context of normalizing flows, it is more common to define the forward transformation $\\mathbf{x} = f(\\mathbf{z})$ and compute its log-likelihood. Let $J_f = \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}$ be the Jacobian of the forward transformation. Then the log-density is:\n$$\\log p_{\\mathbf{x}}(\\mathbf{x}) = \\log p_{\\mathbf{z}}(\\mathbf{z}) - \\log |\\det J_f|$$\nEfficient and stable computation of the log-determinant term, $\\log |\\det J_f|$, is therefore critical.\n\nA naive computation of $\\det J_f$ involves a product of many terms. In floating-point arithmetic, multiplying a large number of values greater than $1$ can result in overflow (exceeding the largest representable number, yielding $\\infty$), while multiplying many values between $0$ and $1$ can result in underflow (producing a number smaller than the smallest positive representable number, often flushing to $0$). The subsequent application of the logarithm to $\\infty$ or $0$ yields $\\infty$ or $-\\infty$, respectively, causing a loss of all numerical information.\n\nThe fundamental identity $\\log\\left(\\prod_{i=1}^n a_i\\right)=\\sum_{i=1}^n\\log a_i$ for positive $a_i$ provides the key to a stable computation. By converting the product into a sum *before* the computation, we operate in log-space. The sum of logarithms is far less susceptible to overflow or underflow than the product of the original numbers, as the range of representable exponents is much larger than the range of the mantissa.\n\nWe will now derive stable computational strategies for the specific matrix structures requested.\n\n**1. Diagonal Jacobian Matrix**\n\nConsider a Jacobian matrix $J$ that is diagonal, $J = \\mathrm{diag}(s_1, s_2, \\dots, s_d)$, where $s_i \\in \\mathbb{R}$ are unconstrained real-valued scaling factors.\nThe matrix $J$ is a special case of a triangular matrix. Based on the provided principle, its determinant is the product of its diagonal entries:\n$$\\det J = \\prod_{i=1}^d s_i$$\nThe absolute value of the determinant is:\n$$|\\det J| = \\left| \\prod_{i=1}^d s_i \\right| = \\prod_{i=1}^d |s_i|$$\nThe required log-determinant is:\n$$\\log |\\det J| = \\log \\left( \\prod_{i=1}^d |s_i| \\right)$$\nA **naive computational approach** would first compute the product $P = \\prod_{i=1}^d |s_i|$ and then take its logarithm, $\\log P$. If $d$ is large and the magnitudes $|s_i|$ are consistently greater than $1$ or less than $1$, the intermediate product $P$ is prone to overflow or underflow.\n\nA **stable computational approach**, derived from the log-product identity, computes the sum of the logarithms of the individual absolute scaling factors:\n$$\\log |\\det J| = \\sum_{i=1}^d \\log |s_i|$$\nThis computation avoids the large or small intermediate product, instead summing values of a more manageable magnitude, thus preventing overflow and underflow and preserving numerical precision.\n\nFor deep learning applications, a common and principled parametrization to ensure invertibility ($s_i \\neq 0$) is to define the scaling factors as the output of an exponential function, $s_i = \\exp(\\alpha_i)$, where $\\alpha_i \\in \\mathbb{R}$ are unconstrained parameters (e.g., outputs of a neural network). This enforces $s_i > 0$, so $|s_i| = s_i$. The log-determinant computation then simplifies further to:\n$$\\log |\\det J| = \\sum_{i=1}^d \\log(\\exp(\\alpha_i)) = \\sum_{i=1}^d \\alpha_i$$\nThe log-determinant is simply the sum of the unconstrained parameters, which is maximally stable and computationally efficient.\n\n**2. Symmetric Positive Definite Matrix via Cholesky Factorization**\n\nCertain transformations in normalizing flows involve symmetric positive definite (SPD) matrices. A robust method to parametrize an SPD matrix $A$ is through its Cholesky decomposition, $A = L L^\\top$, where $L$ is a lower-triangular matrix with strictly positive diagonal entries, $L_{ii} > 0$. This parametrization inherently guarantees that $A$ is SPD.\n\nWe seek to stably compute $\\log \\det A$. Using the properties of determinants, $\\det(XY) = (\\det X)(\\det Y)$ and $\\det(X^\\top) = \\det X$, we have:\n$$\\det A = \\det(L L^\\top) = (\\det L)(\\det L^\\top) = (\\det L)^2$$\nSince $L$ is a lower-triangular matrix, its determinant is the product of its diagonal entries:\n$$\\det L = \\prod_{i=1}^d L_{ii}$$\nTherefore, the determinant of $A$ is:\n$$\\det A = \\left( \\prod_{i=1}^d L_{ii} \\right)^2$$\nAs it is given that $L_{ii} > 0$ for all $i$, we are assured that $\\det A > 0$, so the logarithm is well-defined.\n$$\\log \\det A = \\log \\left( \\left( \\prod_{i=1}^d L_{ii} \\right)^2 \\right) = 2 \\log \\left( \\prod_{i=1}^d L_{ii} \\right)$$\nA **naive computational approach** might involve calculating $A = LL^\\top$, and then computing its determinant (e.g., as the product of its eigenvalues or, if $A$ happens to be triangular, the product of its diagonal entries), followed by taking the logarithm. This reintroduces the risk of overflow/underflow in the intermediate determinant calculation.\n\nThe **stable computational approach** applies the log-product identity directly to the expression for $\\log \\det A$:\n$$\\log \\det A = 2 \\sum_{i=1}^d \\log(L_{ii})$$\nThis calculation is robust as it relies on the sum of logarithms of the diagonal entries of $L$, which are directly available from the parametrization and are guaranteed to be positive. This avoids forming $A$ or calculating any large intermediate determinant product. This is the principled strategy for computing the log-determinant of a matrix represented by its Cholesky factor.\n\nThe provided test suite allows for the empirical validation of these derivations, demonstrating the failure of naive methods and the success of stable methods under conditions designed to induce overflow and underflow.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests numerically stable computations for log-determinants\n    as specified in the problem statement.\n    \"\"\"\n\n    results = []\n\n    # Suppress runtime warnings for overflow/underflow, as they are expected\n    # in the 'naive' computations and are part of the test.\n    with np.errstate(over='ignore', under='ignore', divide='ignore'):\n        # Test Case 1: Happy path diagonal case\n        d1 = 16\n        s1 = np.linspace(0.8, 1.2, d1)\n        # Naive computation: log(product)\n        naive1 = np.log(np.abs(np.prod(s1)))\n        # Stable computation: sum(log)\n        stable1 = np.sum(np.log(np.abs(s1)))\n        # Rule: both finite and agree within a tight tolerance.\n        rule1 = np.isfinite(naive1) and np.isfinite(stable1) and np.isclose(naive1, stable1, atol=1e-10)\n        results.append(rule1)\n\n        # Test Case 2: Overflow diagonal case\n        d2 = 1024\n        s2 = np.full(d2, 10.0**10)\n        # Naive computation: Product will overflow to inf. log(inf) is inf.\n        naive2 = np.log(np.prod(s2))\n        # Stable computation: Sum of logs remains finite.\n        stable2 = np.sum(np.log(s2))\n        # Rule: naive is non-finite, stable is finite.\n        rule2 = not np.isfinite(naive2) and np.isfinite(stable2)\n        results.append(rule2)\n\n        # Test Case 3: Underflow diagonal case\n        d3 = 1024\n        s3 = np.full(d3, 10.0**-10)\n        # Naive computation: Product will underflow to 0. log(0) is -inf.\n        naive3 = np.log(np.prod(s3))\n        # Stable computation: Sum of logs remains finite.\n        stable3 = np.sum(np.log(s3))\n        # Rule: naive is non-finite, stable is finite.\n        rule3 = not np.isfinite(naive3) and np.isfinite(stable3)\n        results.append(rule3)\n\n        # Test Case 4: Cholesky overflow case\n        d4 = 256\n        L_diag4 = np.full(d4, 10.0**10)\n        # In this case, L is a diagonal matrix. A = L L^T = L^2.\n        # The diagonal of A consists of the squares of the diagonal of L.\n        A_diag4 = L_diag4**2\n        # Naive computation (as per problem): product of A's diagonal entries.\n        # This product (10^20)^256 = 10^5120 will overflow.\n        naive4 = np.log(np.prod(A_diag4))\n        # Stable computation: 2 * sum(log of L's diagonal entries).\n        stable4 = 2 * np.sum(np.log(L_diag4))\n        # Rule: naive is non-finite, stable is finite.\n        rule4 = not np.isfinite(naive4) and np.isfinite(stable4)\n        results.append(rule4)\n\n        # Test Case 5: Signed diagonal overflow case\n        d5 = 1024\n        s5 = np.full(d5, 10.0**10)\n        # s_i = (-1)^i * 10^10 for i=0, 1, ...\n        s5[1::2] *= -1\n        # Naive computation: product of magnitudes. Same as case 2.\n        naive5 = np.log(np.prod(np.abs(s5)))\n        # Stable computation: sum of log magnitudes. Same as case 2.\n        stable5 = np.sum(np.log(np.abs(s5)))\n        # Rule: naive is non-finite, stable is finite.\n        rule5 = not np.isfinite(naive5) and np.isfinite(stable5)\n        results.append(rule5)\n\n    # Final print statement in the exact required format.\n    # Python's str() on booleans yields \"True\" or \"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160140"}, {"introduction": "Having established the foundational theory and addressed numerical stability, we can now build more powerful and expressive flows. While simple affine layers are easy to implement, their modeling capacity is limited. This practice introduces rational-quadratic splines, a state-of-the-art method for constructing highly flexible, non-linear, and invertible transformations. By deriving and implementing a spline-based coupling layer from first principles, you will gain hands-on experience with the design of modern normalizing flows and appreciate the mathematical engineering required to ensure both expressivity and tractability. [@problem_id:3160093]", "problem": "You are asked to design and implement a one-dimensional rational-quadratic spline coupling layer as a component of a normalizing flow in deep learning. A normalizing flow is an invertible transformation $f : \\mathbb{R}^d \\to \\mathbb{R}^d$ composed to transform a simple base distribution into a complex target distribution. The core starting points you must use are: (i) the change-of-variables theorem for densities stating that if $\\mathbf{y} = f(\\mathbf{x})$, then $\\log p_{\\mathbf{Y}}(\\mathbf{y}) = \\log p_{\\mathbf{X}}(\\mathbf{x}) - \\log |\\det J_f(\\mathbf{x})|$, where $J_f(\\mathbf{x})$ is the Jacobian matrix of $f$ at $\\mathbf{x}$; (ii) the coupling transform architecture where a binary mask partitions $\\mathbf{x}$ into unchanged components and components transformed elementwise by an invertible function, making the Jacobian triangular so that $\\log |\\det J_f(\\mathbf{x})|$ reduces to a sum of per-coordinate $\\log$-derivatives.\n\nIn a spline-based coupling layer, each transformed scalar coordinate $x$ is mapped to $y$ by a piecewise function defined over a bounded interval $[x_{\\min}, x_{\\max}]$ using $K$ bins. Let the bin widths be $\\{w_i\\}_{i=1}^K$ and heights be $\\{h_i\\}_{i=1}^K$ such that $\\sum_{i=1}^K w_i = x_{\\max} - x_{\\min}$ and $\\sum_{i=1}^K h_i = y_{\\max} - y_{\\min}$, with $y_{\\min} = x_{\\min}$ and $y_{\\max} = x_{\\max}$ to ensure overall range preservation. Define knot positions $\\{x_i\\}_{i=0}^K$ and $\\{y_i\\}_{i=0}^K$ by $x_0 = x_{\\min}$, $x_{i+1} = x_i + w_{i+1}$ and $y_0 = y_{\\min}$, $y_{i+1} = y_i + h_{i+1}$. For each bin $i$ (with $i \\in \\{0,1,\\dots,K-1\\}$), specify positive endpoint slopes $\\{s_i^L, s_i^R\\}$ with respect to the original coordinates $(x,y)$ at the left and right bin edges. Outside the interval $[x_{\\min}, x_{\\max}]$, the mapping must be the identity, i.e., $y = x$.\n\nYour tasks are:\n\n1. Derive a closed-form, monotone, rational-quadratic function $f_i$ on each bin that satisfies the interpolation constraints $f_i(0) = 0$, $f_i(1) = 1$, $f_i'(0) = s_i^L \\cdot \\frac{w_i}{h_i}$, and $f_i'(1) = s_i^R \\cdot \\frac{w_i}{h_i}$ after the normalized change of variables $t = \\frac{x - x_i}{w_i}$ and $u = \\frac{y - y_i}{h_i}$. You must start from the constraints and the rational-quadratic ansatz and obtain explicit expressions for the coefficients needed to compute $u$ from $t$, and then invert to recover $t$ from $u$. You must ensure scientific realism by enforcing positivity of bin widths, heights, and endpoint slopes, and by making the tails outside $[x_{\\min}, x_{\\max}]$ identity.\n\n2. From first principles using the change-of-variables theorem and the triangular Jacobian of the coupling transform, derive the expression for $\\log |\\det J_f(\\mathbf{x})|$ as a sum of per-bin, per-coordinate $\\log$-derivatives. Your derivation must explain why the Jacobian is triangular and how the per-bin derivative combines with the width and height normalization to give the derivative $\\frac{dy}{dx}$ inside a bin. Provide the final closed-form derivative used for evaluation.\n\n3. Implement a complete, runnable program that:\n   - Constructs a one-dimensional rational-quadratic spline coupling layer acting on the second coordinate of a two-dimensional vector $\\mathbf{x} = (x_0, x_1)$ with a binary mask that leaves $x_0$ unchanged and transforms $x_1$.\n   - Uses $K$ bins per transformed coordinate with parameters $(x_{\\min}, x_{\\max})$, bin widths $\\{w_i\\}$, bin heights $\\{h_i\\}$, and endpoint slopes $\\{s_i^L, s_i^R\\}$ per bin, all positive and summing appropriately as specified above.\n   - Implements both the forward transformation $\\mathbf{y} = f(\\mathbf{x})$, the inverse $\\mathbf{x} = f^{-1}(\\mathbf{y})$, and the computation of $\\log |\\det J_f(\\mathbf{x})|$ using only the derived closed-form expressions without numerical optimization.\n\n4. Validate your implementation with the following test suite and produce a single line of output containing the results as a comma-separated list enclosed in square brackets:\n   - Case $1$ (happy path inside domain): Use $d = 2$, $K = 3$, $x_{\\min} = -2$, $x_{\\max} = 2$, uniform bin widths and heights, and endpoint slopes at knots $\\{s_0, s_1, s_2, s_3\\} = \\{2, 0.5, 2, 0.5\\}$. Evaluate at $\\mathbf{x} = (0.25, -0.5)$. Output a boolean indicating whether $f^{-1}(f(\\mathbf{x}))$ returns $\\mathbf{x}$ within a tolerance of $10^{-9}$, and a float equal to $\\log |\\det J_f(\\mathbf{x})|$.\n   - Case $2$ (identity mapping): Same $d$, $K$, and bounds, with uniform widths and heights and slopes $\\{1, 1, 1, 1\\}$. Evaluate at $\\mathbf{x} = (0.1, 0.3)$. Output a boolean indicating whether $f(\\mathbf{x}) = \\mathbf{x}$ within a tolerance of $10^{-12}$, and a float equal to $\\log |\\det J_f(\\mathbf{x})|$.\n   - Case $3$ (outside-domain tail): Use the same parameters as Case $1$, but with $\\mathbf{x} = (-1.0, 3.0)$ where the transformed coordinate satisfies $x_1 > x_{\\max}$. Output a boolean indicating whether $f(\\mathbf{x}) = \\mathbf{x}$ within a tolerance of $10^{-12}$, and a float equal to $\\log |\\det J_f(\\mathbf{x})|$.\n   - Case $4$ (knot boundary): Same parameters as Case $1$, but evaluate at $\\mathbf{x} = (0.0, -2.0)$ where $x_1 = x_{\\min}$ is exactly at the leftmost knot. Output a float equal to $\\log |\\det J_f(\\mathbf{x})|$ and a boolean indicating whether $f^{-1}(f(\\mathbf{x}))$ returns $\\mathbf{x}$ within a tolerance of $10^{-12}$.\n   - Case $5$ (finite-difference derivative check): Same parameters as Case $1$, evaluate at $\\mathbf{x} = (0.0, 0.1)$ and compare the analytic $\\frac{dy}{dx}$ for the transformed coordinate to a symmetric finite-difference estimate with step $\\epsilon = 10^{-6}$. Output a float equal to the absolute error between the analytic and numerical derivatives.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$\\text{result1},\\text{result2},\\dots$]\"). All quantities in this problem are dimensionless real numbers, and angles are not involved.", "solution": "### Principle-Based Design and Derivations\n\nThe core of this problem is to construct an invertible, differentiable, piecewise transformation and compute the logarithm of its Jacobian determinant, $\\log |\\det J_f|$. The design relies on two key principles: the change-of-variables theorem and the architecture of a coupling layer.\n\n**1. Coupling Layer and Jacobian Determinant**\n\nA coupling layer transforms a vector $\\mathbf{x} \\in \\mathbb{R}^d$ into $\\mathbf{y} \\in \\mathbb{R}^d$ by partitioning the dimensions of $\\mathbf{x}$ using a binary mask. Some components are passed through unchanged (identity transformation), while the remaining components are transformed by a function whose parameters are determined by the unchanged components. For this problem, with $\\mathbf{x} = (x_0, x_1)$, the transformation is:\n$$\n\\begin{cases}\ny_0 = x_0 \\\\\ny_1 = g(x_1)\n\\end{cases}\n$$\nHere, the parameters of the transformation $g$ are constant, a simplification of the general case where they would depend on $x_0$. The Jacobian matrix $J_f(\\mathbf{x})$ of this transformation $f$ is:\n$$\nJ_f(\\mathbf{x}) = \\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} \\\\\n\\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1}\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & g'(x_1)\n\\end{pmatrix}\n$$\nThe Jacobian is lower triangular, a key feature of coupling layers. Its determinant is the product of its diagonal entries: $\\det J_f(\\mathbf{x}) = 1 \\cdot g'(x_1) = g'(x_1)$. The change-of-variables formula requires $\\log |\\det J_f(\\mathbf{x})|$. Since the transformation must be monotonic, we require $g'(x_1) > 0$, so $\\log |\\det J_f(\\mathbf{x})| = \\log(g'(x_1))$. If the transformation were applied to multiple components, the log-determinant would be the sum of the individual log-derivatives, $\\sum_i \\log(\\frac{\\partial y_i}{\\partial x_i})$, over all transformed components $i$.\n\n**2. Rational-Quadratic Spline Formulation**\n\nThe function $g(x_1)$ is a piecewise rational-quadratic spline. For each bin $i$ from $x_i$ to $x_{i+1}$, there is a distinct rational-quadratic function. Outside the domain $[x_{\\min}, x_{\\max}]$, $g$ is the identity function, $g(x_1)=x_1$, so $g'(x_1)=1$ and $\\log(g'(x_1))=0$.\n\nInside a bin $i$, the transformation is defined in normalized coordinates. Let $t = \\frac{x_1 - x_i}{w_i} \\in [0, 1]$ and $u = \\frac{y_1 - y_i}{h_i} \\in [0, 1]$. We need a function $u = G(t)$ satisfying four boundary conditions: $G(0)=0$, $G(1)=1$, $G'(0)=d_0$, and $G'(1)=d_1$. The normalized derivatives $d_0$ and $d_1$ are related to the original derivatives $s_i^L = \\frac{dy_1}{dx_1}|_{x_1=x_i}$ and $s_i^R = \\frac{dy_1}{dx_1}|_{x_1=x_{i+1}}$ via the chain rule:\n$$ \\frac{du}{dt} = \\frac{dy_1/h_i}{dx_1/w_i} = \\frac{dy_1}{dx_1} \\frac{w_i}{h_i} $$\nThus, $d_0 = s_i^L \\frac{w_i}{h_i}$ and $d_1 = s_i^R \\frac{w_i}{h_i}$.\n\nA key insight for constructing such splines is that it is often simpler to define the inverse function $t = H(u)$ and then solve for $u$ to obtain the forward function $u=G(t)$. The derivative of the inverse is the reciprocal of the forward derivative: $H'(u) = \\frac{dt}{du} = 1/G'(t)$. Therefore, the boundary conditions for the inverse are $H(0)=0$, $H(1)=1$, $H'(0)=1/d_0$, and $H'(1)=1/d_1$.\n\nWe use a rational-quadratic ansatz for the inverse function $H(u)$:\n$$ t = H(u) = \\frac{\\alpha u^2 + \\beta u}{\\gamma u^2 + \\delta u + 1} $$\nThis form already satisfies $H(0)=0$. The remaining three conditions and a simplifying choice determine the coefficients. A common choice that simplifies the forward pass is to set $\\alpha=0$, making the numerator linear.\nWith $\\alpha=0$:\n1.  $H'(0)=\\beta = 1/d_0$.\n2.  $H(1)=1 \\implies \\beta = \\gamma+\\delta+1$.\n3.  $H'(1)=1/d_1$. The derivative is $H'(u) = \\frac{\\beta(\\gamma u^2+\\delta u+1) - \\beta u(2\\gamma u+\\delta)}{(\\gamma u^2+\\delta u+1)^2} = \\frac{\\beta(-\\gamma u^2+1)}{(\\gamma u^2+\\delta u+1)^2}$.\n    At $u=1$, $H'(1) = \\frac{\\beta(-\\gamma+1)}{(\\gamma+\\delta+1)^2} = \\frac{\\beta(-\\gamma+1)}{\\beta^2} = \\frac{1-\\gamma}{\\beta} = 1/d_1$.\n\nFrom condition 3, $1-\\gamma = \\beta/d_1 = (1/d_0)/d_1 = 1/(d_0 d_1)$. This gives $\\gamma = 1 - 1/(d_0 d_1)$.\nFrom condition 2, $\\delta = \\beta - \\gamma - 1 = 1/d_0 - (1 - 1/(d_0 d_1)) - 1 = 1/d_0 + 1/(d_0 d_1) - 2$.\nSo, the inverse mapping within a bin is given by a closed-form expression for $t=H(u)$ with coefficients:\n$$ \\beta = \\frac{1}{d_0}, \\quad \\gamma = 1 - \\frac{1}{d_0 d_1}, \\quad \\delta = \\frac{1}{d_0} + \\frac{1}{d_0 d_1} - 2 $$\nThis is computationally efficient for the inverse pass $f^{-1}$.\n\nFor the forward pass $f$, we must solve $t = H(u)$ for $u$:\n$$ t(\\gamma u^2 + \\delta u + 1) = \\beta u $$\n$$ (t\\gamma) u^2 + (t\\delta - \\beta) u + t = 0 $$\nThis is a standard quadratic equation of the form $Au^2+Bu+C=0$ with $A=t\\gamma$, $B=t\\delta-\\beta$, $C=t$. The solution for $u$ is found via the quadratic formula. To maintain numerical stability, particularly for small $t$, we use the form $u = \\frac{2C}{-B \\pm \\sqrt{B^2-4AC}}$. The correct root is chosen to ensure $u \\in [0,1]$:\n$$ u(t) = \\frac{2t}{-(t\\delta - \\beta) + \\sqrt{(t\\delta - \\beta)^2 - 4t^2\\gamma}} $$\n\n**3. Log-Derivative Calculation**\nThe log-determinant is $\\log(g'(x_1))$. We have $g'(x_1) = \\frac{dy_1}{dx_1} = \\frac{h_i}{w_i} G'(t) = \\frac{h_i}{w_i} \\frac{1}{H'(u)}$.\nThe full expression for the log-derivative is:\n$$ \\log(g'(x_1)) = \\log\\left(\\frac{h_i}{w_i}\\right) - \\log(H'(u)) $$\n$$ = \\log\\left(\\frac{h_i}{w_i}\\right) - \\left[ \\log(\\beta(-\\gamma u^2+1)) - 2\\log(\\gamma u^2+\\delta u+1) \\right] $$\nTo evaluate this, for a given $x_1$, we first perform the forward pass to find the corresponding bin and the value of $u$, then substitute it into this formula.\n\nThis completes the derivations required to implement the forward pass, the inverse pass, and the log-determinant calculation.", "answer": "```python\nimport numpy as np\n\nclass RationalQuadraticSplineCoupling:\n    \"\"\"\n    Implements a 1D rational-quadratic spline coupling layer.\n    \n    This implementation follows the formulation where the inverse transformation\n    is defined as a simple rational-linear function, and the forward \n    transformation is found by solving a quadratic equation. This simplifies\n    the design while satisfying all constraints.\n    \"\"\"\n    \n    def __init__(self, K, x_min, x_max, widths, heights, knot_slopes):\n        \"\"\"\n        Initializes the spline parameters.\n        \n        Args:\n            K (int): Number of bins.\n            x_min (float): Lower bound of the spline domain.\n            x_max (float): Upper bound of the spline domain.\n            widths (list or np.ndarray): Widths of the K bins in the domain.\n            heights (list or np.ndarray): Heights of the K bins in the range.\n            knot_slopes (list or np.ndarray): Derivatives at the K+1 knots.\n        \"\"\"\n        self.K = K\n        self.x_min = float(x_min)\n        self.x_max = float(x_max)\n\n        self.widths = np.array(widths, dtype=float)\n        self.heights = np.array(heights, dtype=float)\n        self.knot_slopes = np.array(knot_slopes, dtype=float)\n\n        # Precompute knot coordinates\n        self.x_knots = np.concatenate(([self.x_min], self.x_min + np.cumsum(self.widths)))\n        self.y_knots = np.concatenate(([self.x_min], self.x_min + np.cumsum(self.heights)))\n\n        # Derivatives at left and right of each bin\n        s_L = self.knot_slopes[:-1]\n        s_R = self.knot_slopes[1:]\n        \n        # Normalized derivatives for the forward map g'(t)\n        # We add a small epsilon to prevent division by zero, though problem constraints imply positivity.\n        eps = 1e-9\n        d_fwd_0 = s_L * self.widths / (self.heights + eps)\n        d_fwd_1 = s_R * self.widths / (self.heights + eps)\n\n        # Normalized derivatives for the inverse map h'(u)\n        d_inv_0 = 1.0 / (d_fwd_0 + eps)\n        d_inv_1 = 1.0 / (d_fwd_1 + eps)\n        \n        # Coefficients for the inverse map h(u) = (beta * u) / (gamma * u^2 + delta * u + 1)\n        self.beta = d_inv_0\n        self.gamma = 1.0 - (d_inv_0 * d_inv_1)\n        self.delta = d_inv_0 + 1.0 / (d_fwd_0 * d_fwd_1 + eps) - 2.0\n\n    def _find_bin(self, z, knots):\n        # np.searchsorted finds insertion indices to maintain order.\n        # Subtracting 1 gives the index of the bin z falls into.\n        bin_idx = np.searchsorted(knots, z, side='right') - 1\n        return np.clip(bin_idx, 0, self.K - 1)\n\n    def forward(self, x):\n        \"\"\"Computes the forward transformation y = f(x).\"\"\"\n        x0, x1 = x\n        y0 = x0\n\n        if not (self.x_min <= x1 <= self.x_max):\n            return np.array([y0, x1])\n\n        bin_idx = self._find_bin(x1, self.x_knots)\n        \n        x_k, y_k = self.x_knots[bin_idx], self.y_knots[bin_idx]\n        w_k, h_k = self.widths[bin_idx], self.heights[bin_idx]\n        \n        beta, gamma, delta = self.beta[bin_idx], self.gamma[bin_idx], self.delta[bin_idx]\n        \n        t = (x1 - x_k) / w_k\n\n        # Handle boundary cases for numerical stability\n        if t == 0.0: u = 0.0\n        elif t == 1.0: u = 1.0\n        else:\n            # Solve (t*gamma)u^2 + (t*delta - beta)u + t = 0 for u\n            A = t * gamma\n            B = t * delta - beta\n            C = t\n            # Use stable quadratic formula: u = 2C / (-B + sqrt(B^2 - 4AC))\n            discriminant = np.sqrt(B**2 - 4 * A * C)\n            u = (2 * C) / (-B + discriminant)\n\n        y1 = y_k + u * h_k\n        return np.array([y0, y1])\n\n    def inverse(self, y):\n        \"\"\"Computes the inverse transformation x = f_inv(y).\"\"\"\n        y0, y1 = y\n        x0 = y0\n\n        if not (self.x_min <= y1 <= self.x_max):\n            return np.array([x0, y1])\n\n        bin_idx = self._find_bin(y1, self.y_knots)\n        \n        x_k, y_k = self.x_knots[bin_idx], self.y_knots[bin_idx]\n        w_k, h_k = self.widths[bin_idx], self.heights[bin_idx]\n        \n        beta, gamma, delta = self.beta[bin_idx], self.gamma[bin_idx], self.delta[bin_idx]\n        \n        u = (y1 - y_k) / h_k\n\n        if u == 0.0: t = 0.0\n        elif u == 1.0: t = 1.0\n        else:\n            # Evaluate t = h(u)\n            t = (beta * u) / (gamma * u**2 + delta * u + 1)\n        \n        x1 = x_k + t * w_k\n        return np.array([x0, x1])\n\n    def log_det_jacobian(self, x):\n        \"\"\"Computes log|det(J_f(x))|.\"\"\"\n        _x0, x1 = x\n        \n        if not (self.x_min <= x1 <= self.x_max):\n            return 0.0\n\n        bin_idx = self._find_bin(x1, self.x_knots)\n        \n        w_k, h_k = self.widths[bin_idx], self.heights[bin_idx]\n        beta, gamma, delta = self.beta[bin_idx], self.gamma[bin_idx], self.delta[bin_idx]\n        \n        # First, find u corresponding to x1 (part of forward pass)\n        x_k = self.x_knots[bin_idx]\n        t = (x1 - x_k) / w_k\n        \n        if t == 0.0: u = 0.0\n        elif t == 1.0: u = 1.0\n        else:\n            A = t*gamma; B = t*delta - beta; C = t\n            discriminant = np.sqrt(B**2 - 4 * A * C)\n            u = (2 * C) / (-B + discriminant)\n\n        # Derivative of inverse h'(u) = beta * (-gamma*u^2 + 1) / (gamma*u^2 + delta*u + 1)^2\n        h_prime_u_num = beta * (-gamma * u**2 + 1)\n        h_prime_u_den = (gamma * u**2 + delta * u + 1)**2\n        h_prime_u = h_prime_u_num / h_prime_u_den\n        \n        # Derivative dy/dx = (h/w) * (1/h'(u))\n        # log(dy/dx) = log(h/w) - log(h'(u))\n        return np.log(h_k / w_k) - np.log(h_prime_u)\n\ndef solve():\n    results = []\n\n    # Case 1: Happy path inside domain\n    K1, x_min1, x_max1 = 3, -2.0, 2.0\n    domain_width1 = x_max1 - x_min1\n    widths1 = [domain_width1 / K1] * K1\n    heights1 = [domain_width1 / K1] * K1\n    slopes1 = [2.0, 0.5, 2.0, 0.5]\n    coupling1 = RationalQuadraticSplineCoupling(K1, x_min1, x_max1, widths1, heights1, slopes1)\n    x1 = np.array([0.25, -0.5])\n    y1 = coupling1.forward(x1)\n    x1_recon = coupling1.inverse(y1)\n    is_invertible1 = np.allclose(x1, x1_recon, atol=1e-9)\n    log_det1 = coupling1.log_det_jacobian(x1)\n    results.extend([str(is_invertible1).lower(), f\"{log_det1:.8f}\"])\n\n    # Case 2: Identity mapping\n    K2, x_min2, x_max2 = 3, -2.0, 2.0\n    domain_width2 = x_max2 - x_min2\n    widths2 = [domain_width2 / K2] * K2\n    heights2 = [domain_width2 / K2] * K2\n    slopes2 = [1.0, 1.0, 1.0, 1.0]\n    coupling2 = RationalQuadraticSplineCoupling(K2, x_min2, x_max2, widths2, heights2, slopes2)\n    x2 = np.array([0.1, 0.3])\n    y2 = coupling2.forward(x2)\n    is_identity2 = np.allclose(x2, y2, atol=1e-12)\n    log_det2 = coupling2.log_det_jacobian(x2)\n    results.extend([str(is_identity2).lower(), f\"{log_det2:.8f}\"])\n\n    # Case 3: Outside-domain tail\n    coupling3 = coupling1 # Use same parameters as Case 1\n    x3 = np.array([-1.0, 3.0])\n    y3 = coupling3.forward(x3)\n    is_identity3 = np.allclose(x3, y3, atol=1e-12)\n    log_det3 = coupling3.log_det_jacobian(x3)\n    results.extend([str(is_identity3).lower(), f\"{log_det3:.8f}\"])\n    \n    # Case 4: Knot boundary\n    coupling4 = coupling1 # Use same parameters as Case 1\n    x4 = np.array([0.0, -2.0])\n    log_det4 = coupling4.log_det_jacobian(x4)\n    y4 = coupling4.forward(x4)\n    x4_recon = coupling4.inverse(y4)\n    is_invertible4 = np.allclose(x4, x4_recon, atol=1e-12)\n    results.extend([f\"{log_det4:.8f}\", str(is_invertible4).lower()])\n\n    # Case 5: Finite-difference derivative check\n    coupling5 = coupling1 # Use same parameters as Case 1\n    x5_val = 0.1\n    x5 = np.array([0.0, x5_val])\n    analytic_deriv = np.exp(coupling5.log_det_jacobian(x5))\n    eps = 1e-6\n    x5_plus = np.array([0.0, x5_val + eps])\n    x5_minus = np.array([0.0, x5_val - eps])\n    y5_plus = coupling5.forward(x5_plus)\n    y5_minus = coupling5.forward(x5_minus)\n    numerical_deriv = (y5_plus[1] - y5_minus[1]) / (2 * eps)\n    error5 = abs(analytic_deriv - numerical_deriv)\n    results.append(f\"{error5:.8e}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3160093"}]}