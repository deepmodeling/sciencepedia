{"hands_on_practices": [{"introduction": "Training an Energy-Based Model involves adjusting its parameters to increase the likelihood of observed data. The key to this process is the log-likelihood gradient, which elegantly decomposes into two competing forces: a \"positive phase\" that pulls the model towards the data, and a \"negative phase\" that corrects the model's overall distribution. This exercise makes this abstract concept concrete by having you derive the exact gradient for a simple quadratic energy function, building a core intuition for the EBM training dynamic [@problem_id:3122263].", "problem": "Consider an Energy-Based Model (EBM) over two-dimensional real-valued inputs $x \\in \\mathbb{R}^{2}$ with density $p_{\\theta}(x)$ defined by the core EBM principle $p_{\\theta}(x) = Z(\\theta)^{-1} \\exp(-E_{\\theta}(x))$, where the partition function $Z(\\theta)$ is given by $Z(\\theta) = \\int_{\\mathbb{R}^{2}} \\exp(-E_{\\theta}(x)) \\, dx$ and the energy is specified as $E_{\\theta}(x) = \\frac{1}{2} \\|x\\|^{2} - \\theta^{\\top} x$, with parameter $\\theta \\in \\mathbb{R}^{2}$. You are given a toy dataset of $N = 3$ two-dimensional samples,\n$$\nx^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\nx^{(2)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad\nx^{(3)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nStarting from first principles, namely the definition of the log-likelihood and the partition function, derive the exact gradient of the average log-likelihood $\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln p_{\\theta}(x^{(i)})$ with respect to $\\theta$, and explicitly separate the contribution coming from the data expectation (often called the positive phase) and the contribution coming from the model expectation (often called the negative phase). Then, evaluate the resulting exact gradient at the parameter\n$$\n\\theta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nExpress the final gradient as a single exact row vector using rational numbers where appropriate, and do not round.", "solution": "The user wants to find the gradient of the average log-likelihood of an Energy-Based Model (EBM) and evaluate it at a specific point. The solution will proceed in three main stages: first, deriving the general form of the gradient; second, simplifying the gradient by identifying the specific probability distribution defined by the given energy function; and third, evaluating this gradient at the specified parameter value using the provided data.\n\nThe average log-likelihood $\\ell(\\theta)$ for a dataset of $N$ samples $\\{x^{(i)}\\}_{i=1}^{N}$ is given by:\n$$\n\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln p_{\\theta}(x^{(i)})\n$$\nThe probability density function $p_{\\theta}(x)$ is defined as $p_{\\theta}(x) = \\frac{1}{Z(\\theta)} \\exp(-E_{\\theta}(x))$, where $E_{\\theta}(x)$ is the energy function and $Z(\\theta)$ is the partition function. Substituting this into the log-likelihood expression gives:\n$$\n\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\ln\\left(\\frac{1}{Z(\\theta)}\\right) + \\ln(\\exp(-E_{\\theta}(x^{(i)}))) \\right)\n$$\n$$\n\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( - \\ln Z(\\theta) - E_{\\theta}(x^{(i)}) \\right)\n$$\nSince $\\ln Z(\\theta)$ does not depend on the summation index $i$, we can simplify this to:\n$$\n\\ell(\\theta) = - \\left( \\frac{1}{N} \\sum_{i=1}^{N} E_{\\theta}(x^{(i)}) \\right) - \\ln Z(\\theta)\n$$\nTo find the gradient with respect to the parameter vector $\\theta$, we differentiate $\\ell(\\theta)$:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} E_{\\theta}(x^{(i)}) - \\nabla_{\\theta} \\ln Z(\\theta)\n$$\nThis expression separates the gradient into two components. Let's analyze each one.\n\nThe first component involves the gradient of the energy function, $E_{\\theta}(x) = \\frac{1}{2} \\|x\\|^{2} - \\theta^{\\top} x$. Differentiating with respect to $\\theta$:\n$$\n\\nabla_{\\theta} E_{\\theta}(x) = \\nabla_{\\theta} \\left( \\frac{1}{2} x^{\\top}x - \\theta^{\\top} x \\right) = -x\n$$\nSubstituting this into the first component of the gradient of the log-likelihood:\n$$\n- \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} E_{\\theta}(x^{(i)}) = - \\frac{1}{N} \\sum_{i=1}^{N} (-x^{(i)}) = \\frac{1}{N} \\sum_{i=1}^{N} x^{(i)}\n$$\nThis term is the empirical mean of the data, often denoted as $\\mathbb{E}_{\\text{data}}[x]$ or $\\bar{x}$. This is the contribution from the data expectation, known as the **positive phase**.\n\nThe second component is the gradient of the log-partition function, $-\\nabla_{\\theta} \\ln Z(\\theta)$. We use the log-derivative trick: $\\nabla_{\\theta} \\ln Z(\\theta) = \\frac{1}{Z(\\theta)} \\nabla_{\\theta} Z(\\theta)$. The partition function is $Z(\\theta) = \\int_{\\mathbb{R}^{2}} \\exp(-E_{\\theta}(x)) \\, dx$. Using the Leibniz integral rule to differentiate under the integral sign:\n$$\n\\nabla_{\\theta} Z(\\theta) = \\int_{\\mathbb{R}^{2}} \\nabla_{\\theta} \\exp(-E_{\\theta}(x)) \\, dx = \\int_{\\mathbb{R}^{2}} \\exp(-E_{\\theta}(x)) (-\\nabla_{\\theta} E_{\\theta}(x)) \\, dx\n$$\nUsing our earlier result that $\\nabla_{\\theta} E_{\\theta}(x) = -x$, we get:\n$$\n\\nabla_{\\theta} Z(\\theta) = \\int_{\\mathbb{R}^{2}} x \\exp(-E_{\\theta}(x)) \\, dx\n$$\nTherefore, the second component of the log-likelihood gradient is:\n$$\n-\\nabla_{\\theta} \\ln Z(\\theta) = - \\frac{1}{Z(\\theta)} \\int_{\\mathbb{R}^{2}} x \\exp(-E_{\\theta}(x)) \\, dx = - \\int_{\\mathbb{R}^{2}} x \\left( \\frac{\\exp(-E_{\\theta}(x))}{Z(\\theta)} \\right) \\, dx\n$$\nRecognizing the term in the parentheses as the model's probability density $p_{\\theta}(x)$, this integral is the definition of the expected value of $x$ under the model distribution:\n$$\n-\\nabla_{\\theta} \\ln Z(\\theta) = - \\mathbb{E}_{x \\sim p_{\\theta}(x)}[x]\n$$\nThis term is the contribution from the model expectation, known as the **negative phase**.\n\nCombining the positive and negative phases, the full gradient of the average log-likelihood is:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = \\underbrace{\\frac{1}{N} \\sum_{i=1}^{N} x^{(i)}}_{\\text{Positive Phase}} - \\underbrace{\\mathbb{E}_{x \\sim p_{\\theta}(x)}[x]}_{\\text{Negative Phase}}\n$$\nTo evaluate this, we must find the model expectation $\\mathbb{E}_{x \\sim p_{\\theta}(x)}[x]$. This requires identifying the distribution $p_{\\theta}(x)$. We analyze the energy function $E_{\\theta}(x) = \\frac{1}{2}\\|x\\|^2 - \\theta^{\\top}x$ by completing the square with respect to $x$:\n$$\nE_{\\theta}(x) = \\frac{1}{2} (x^{\\top}x) - \\theta^{\\top}x = \\frac{1}{2} (x^{\\top}x - 2\\theta^{\\top}x)\n$$\n$$\n= \\frac{1}{2} (x^{\\top}x - 2\\theta^{\\top}x + \\theta^{\\top}\\theta - \\theta^{\\top}\\theta) = \\frac{1}{2} ( (x-\\theta)^{\\top}(x-\\theta) - \\theta^{\\top}\\theta )\n$$\n$$\n= \\frac{1}{2}\\|x-\\theta\\|^2 - \\frac{1}{2}\\|\\theta\\|^2\n$$\nThe probability density is thus:\n$$\np_{\\theta}(x) = \\frac{1}{Z(\\theta)} \\exp\\left( -\\left(\\frac{1}{2}\\|x-\\theta\\|^2 - \\frac{1}{2}\\|\\theta\\|^2\\right) \\right) = \\frac{\\exp(\\frac{1}{2}\\|\\theta\\|^2)}{Z(\\theta)} \\exp\\left(-\\frac{1}{2} (x-\\theta)^{\\top} I^{-1} (x-\\theta)\\right)\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. This is the functional form of a bivariate Gaussian distribution with mean $\\mu = \\theta$ and covariance matrix $\\Sigma = I$. The normalization constant for a general multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ in $\\mathbb{R}^d$ is $(2\\pi)^{d/2} |\\det(\\Sigma)|^{1/2}$. For our case, $d=2$, $\\mu=\\theta$, $\\Sigma=I$, so the normalization constant is $(2\\pi)^{2/2} |\\det(I)|^{1/2} = 2\\pi$.\nThus, $p_{\\theta}(x) = \\mathcal{N}(x | \\theta, I)$.\nThe expectation of a random variable $x$ following a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ is its mean $\\mu$. Therefore:\n$$\n\\mathbb{E}_{x \\sim p_{\\theta}(x)}[x] = \\theta\n$$\nThe gradient expression simplifies significantly:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = \\bar{x} - \\theta\n$$\nNow we can evaluate this gradient using the given data and parameter value. First, we compute the sample mean $\\bar{x}$:\n$$\nx^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad x^{(2)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad x^{(3)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\bar{x} = \\frac{1}{3} \\left( x^{(1)} + x^{(2)} + x^{(3)} \\right) = \\frac{1}{3} \\left( \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} 0+2+1 \\\\ 1+0+1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\nThe gradient is evaluated at $\\theta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}$:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{2} \\\\ \\frac{2}{3} - \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{4}{6} - \\frac{9}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{5}{6} \\end{pmatrix}\n$$\nThe problem asks for the final gradient as a single exact row vector. This is the transpose of the column vector we calculated.\n$$\n\\left( \\nabla_{\\theta} \\ell(\\theta) \\right)^{\\top} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{5}{6} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} & -\\frac{5}{6} \\end{pmatrix}}\n$$", "id": "3122263"}, {"introduction": "In the previous practice, the model's expectation (the negative phase) was analytically solvable, a rare luxury. For most complex EBMs, this term is intractable and must be approximated using Markov Chain Monte Carlo (MCMC) sampling, which forms the computational engine of EBM training. The choice of sampler is not arbitrary; its efficiency is critical, and this exercise explores the trade-offs between different strategies [@problem_id:3122336]. By comparing full, coordinate-wise, and block-based samplers on a model with tunable correlations, you will learn to analyze mixing times and understand why leveraging the energy function's local structure is key to effective training.", "problem": "Consider an Energy-Based Model (EBM), defined by the unnormalized density $p(\\mathbf{x}) \\propto \\exp(-E(\\mathbf{x}))$, where the energy is quadratic, $E(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}$, and $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite. Let $d$ denote the dimension of $\\mathbf{x}$. Assume zero external field so that the unique minimizer is at $\\mathbf{x} = \\mathbf{0}$.\n\nYou will compare three Markov Chain Monte Carlo (MCMC) update schemes and quantify their mixing behavior via the spectral radius of their one-sweep deterministic mean update operator and the resulting worst-mode integrated autocorrelation time:\n\n- Full Unadjusted Langevin Algorithm (ULA): One-step update defined by $\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - \\varepsilon \\nabla E(\\mathbf{x}_{t}) + \\sqrt{2\\varepsilon}\\,\\boldsymbol{\\xi}_{t}$ where $\\boldsymbol{\\xi}_{t} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and $\\varepsilon > 0$ is the step size.\n- Coordinate-wise ULA (cyclic sweep): Sequentially apply $d$ coordinate updates; for coordinate $i$, update $\\mathbf{x}_{t+1}^{(i)} = \\mathbf{x}_{t}^{(i)} - \\varepsilon \\,\\partial_{i}E(\\mathbf{x}_{t}) + \\sqrt{2\\varepsilon}\\,\\xi_{t}^{(i)}$, leaving all other coordinates unchanged in that micro-step. One sweep consists of $d$ such micro-steps applied in cyclic order.\n- Block Gibbs with local energies: Partition coordinates into contiguous blocks of size $m$, and for each block $b$, replace $\\mathbf{x}_{b}$ by an exact draw from the conditional Gaussian $p(\\mathbf{x}_{b} \\mid \\mathbf{x}_{-b})$ induced by the quadratic energy.\n\nStarting from the core definitions and facts:\n- For an EBM with energy $E(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}$, the gradient is $\\nabla E(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$.\n- For Gaussian models with precision $\\mathbf{A}$, the conditional distribution over a block $b$ given the complement $-b$ is Gaussian with precision $\\mathbf{A}_{bb}$ and mean $-\\mathbf{A}_{bb}^{-1}\\mathbf{A}_{b,-b}\\mathbf{x}_{-b}$.\n\nDerive, for each scheme, the deterministic linear operator on the mean after one full sweep, denoted $\\mathbf{M}_{\\text{full}}(\\varepsilon)$ for the full ULA, $\\mathbf{M}_{\\text{coord}}(\\varepsilon)$ for the coordinate-wise cyclic ULA sweep, and $\\mathbf{M}_{\\text{block}}$ for the block Gibbs sweep. Then, for each operator $\\mathbf{M}$, compute its spectral radius $\\lambda_{\\max}(\\mathbf{M}) = \\max\\{|\\lambda| : \\lambda \\text{ eigenvalue of } \\mathbf{M}\\}$, and the worst-mode integrated autocorrelation time approximation $\\tau = \\tfrac{1 + \\lambda_{\\max}(\\mathbf{M})}{1 - \\lambda_{\\max}(\\mathbf{M})}$.\n\nConstruct the precision matrix $\\mathbf{A}(d,\\rho)$ using a local one-dimensional coupling model:\n- Let $\\mathbf{L}_{d}$ be the path-graph Laplacian with Dirichlet boundary conditions: $(\\mathbf{L}_{d})_{ii} = 2$ for $i \\in \\{2,\\dots,d-1\\}$, $(\\mathbf{L}_{d})_{11} = 1$, $(\\mathbf{L}_{d})_{dd} = 1$, and $(\\mathbf{L}_{d})_{i,i+1} = (\\mathbf{L}_{d})_{i+1,i} = -1$ for $i \\in \\{1,\\dots,d-1\\}$, with all other entries zero.\n- Define $\\mathbf{A}(d,\\rho) = (1 - \\rho)\\,\\mathbf{I}_{d} + \\rho\\,\\mathbf{L}_{d}$ for a coupling parameter $\\rho \\in [0,1)$, ensuring $\\mathbf{A}$ is symmetric positive definite.\n\nFor the Unadjusted Langevin Algorithm step size, use $\\varepsilon(d,\\rho) = \\tfrac{0.9}{\\lambda_{\\max}(\\mathbf{A}(d,\\rho))}$, where $\\lambda_{\\max}(\\mathbf{A})$ denotes the largest eigenvalue of $\\mathbf{A}$.\n\nTest Suite:\nProvide results for the following four test cases, each specified by $(d,\\rho,m)$:\n- Case $1$: $(d=10, \\rho=0.3, m=5)$.\n- Case $2$: $(d=100, \\rho=0.9, m=10)$.\n- Case $3$: $(d=50, \\rho=0.0, m=5)$.\n- Case $4$: $(d=100, \\rho=0.99, m=20)$.\n\nFor each case, your program must:\n- Build $\\mathbf{A}(d,\\rho)$.\n- Compute $\\varepsilon(d,\\rho)$.\n- Construct the one-sweep deterministic mean operators $\\mathbf{M}_{\\text{full}}(\\varepsilon)$, $\\mathbf{M}_{\\text{coord}}(\\varepsilon)$, and $\\mathbf{M}_{\\text{block}}$ based solely on the definitions above.\n- Compute $\\lambda_{\\max}$ for each operator and the corresponding $\\tau$ for each scheme.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three floating-point numbers $[\\tau_{\\text{full}}, \\tau_{\\text{coord}}, \\tau_{\\text{block}}]$. For example, the output should look like $[[\\tau_{1, \\text{full}}, \\tau_{1, \\text{coord}}, \\tau_{1, \\text{block}}], [\\tau_{2, \\text{full}}, \\tau_{2, \\text{coord}}, \\tau_{2, \\text{block}}], \\dots]$. No physical units are involved in this problem; all quantities are dimensionless real numbers.", "solution": "We begin from the Energy-Based Model (EBM) definition $p(\\mathbf{x}) \\propto \\exp(-E(\\mathbf{x}))$ with a quadratic energy $E(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}$ and symmetric positive definite $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$. The gradient of the energy is $\\nabla E(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$. Because the model is zero-centered (no linear term), the target mean under $p$ is $\\mathbf{0}$.\n\nWe analyze three schemes through their deterministic linear action on the mean over a sweep. Since the injected noise has zero mean, the evolution of the mean is governed by the deterministic part, and for linear Gaussian targets the mean update is linear in $\\mathbf{x}$. Therefore, the contraction of the mean is fully characterized by the spectral radius of the corresponding linear operator.\n\nFull Unadjusted Langevin Algorithm (ULA):\nThe one-step ULA update is $\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - \\varepsilon \\nabla E(\\mathbf{x}_{t}) + \\sqrt{2\\varepsilon}\\boldsymbol{\\xi}_{t} = \\mathbf{x}_{t} - \\varepsilon \\mathbf{A}\\mathbf{x}_{t} + \\sqrt{2\\varepsilon}\\boldsymbol{\\xi}_{t}$, where $\\boldsymbol{\\xi}_{t} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. Taking expectations and using $\\mathbb{E}[\\boldsymbol{\\xi}_{t}] = \\mathbf{0}$, the mean update is $\\mathbb{E}[\\mathbf{x}_{t+1}] = (\\mathbf{I} - \\varepsilon \\mathbf{A})\\mathbb{E}[\\mathbf{x}_{t}]$. Thus the deterministic mean operator is\n$$\n\\mathbf{M}_{\\text{full}}(\\varepsilon) = \\mathbf{I} - \\varepsilon \\mathbf{A}.\n$$\nThe chain is contractive in mean provided the spectral radius $\\lambda_{\\max}(\\mathbf{M}_{\\text{full}}(\\varepsilon)) < 1$, which is ensured by choosing $\\varepsilon$ so that $|1 - \\varepsilon \\lambda_{j}(\\mathbf{A})| < 1$ for all eigenvalues $\\lambda_{j}(\\mathbf{A})$. A sufficient condition is $\\varepsilon < \\tfrac{2}{\\lambda_{\\max}(\\mathbf{A})}$. We use the conservative choice $\\varepsilon = \\tfrac{0.9}{\\lambda_{\\max}(\\mathbf{A})}$ to keep $\\lambda_{\\max}(\\mathbf{M}_{\\text{full}})$ comfortably below $1$.\n\nCoordinate-wise ULA (cyclic sweep):\nConsider updating coordinate $i$ alone via a one-dimensional ULA step, keeping other coordinates fixed in that micro-step. The deterministic update for that micro-step is\n$$\n\\mathbf{x}' = \\mathbf{x} - \\varepsilon\\,\\mathbf{P}_{i}\\,\\mathbf{A}\\,\\mathbf{x},\n$$\nwhere $\\mathbf{P}_{i} = \\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}$ is a projection that leaves only the $i$-th row active and zeros all other rows, and $\\mathbf{e}_{i}$ is the $i$-th standard basis vector. The noise added to coordinate $i$ has zero mean and does not affect the mean operator. Therefore, the one-step operator for coordinate $i$ is\n$$\n\\mathbf{M}_{i}(\\varepsilon) = \\mathbf{I} - \\varepsilon\\,\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}\\mathbf{A}.\n$$\nA full cyclic sweep that updates coordinates in order $i = 1,2,\\dots,d$ results in the composite mean operator\n$$\n\\mathbf{M}_{\\text{coord}}(\\varepsilon) = \\mathbf{M}_{d}(\\varepsilon)\\,\\mathbf{M}_{d-1}(\\varepsilon)\\,\\cdots\\,\\mathbf{M}_{1}(\\varepsilon).\n$$\nSince each $\\mathbf{M}_{i}(\\varepsilon)$ is a contraction on the $i$-th row and leaves other rows intact for that micro-step, the product captures cumulative contraction across coordinates. The spectral radius $\\lambda_{\\max}(\\mathbf{M}_{\\text{coord}}(\\varepsilon))$ quantifies the worst-case decay of the mean over one sweep.\n\nBlock Gibbs with local energies:\nFor a Gaussian EBM with precision $\\mathbf{A}$, the conditional distribution of a block $b$ given its complement $-b$ is Gaussian with precision $\\mathbf{A}_{bb}$ and mean $-\\mathbf{A}_{bb}^{-1}\\mathbf{A}_{b,-b}\\mathbf{x}_{-b}$. A single block Gibbs update sets $\\mathbf{x}_{b}$ to this conditional mean plus zero-mean noise independent of $\\mathbf{x}_{-b}$. Consequently, the deterministic mean map for that block is\n$$\n\\mathbf{x}'_{b} = -\\mathbf{A}_{bb}^{-1}\\mathbf{A}_{b,-b}\\,\\mathbf{x}_{-b}, \\quad \\mathbf{x}'_{-b} = \\mathbf{x}_{-b}.\n$$\nThis defines a block update operator $\\mathbf{M}_{b}$ as a linear map that replaces the rows corresponding to block $b$ with $-\\mathbf{A}_{bb}^{-1}\\mathbf{A}_{b,-b}$ acting on the complement and zeros on the block itself, while keeping the complement coordinates as identity. A full sweep over blocks $b_{1},b_{2},\\dots$ yields the composite operator\n$$\n\\mathbf{M}_{\\text{block}} = \\mathbf{M}_{b_{K}}\\,\\mathbf{M}_{b_{K-1}}\\,\\cdots\\,\\mathbf{M}_{b_{1}},\n$$\nwhere $K$ is the number of blocks in the partition. Because block updates use exact conditional distributions, they can eliminate intra-block correlations in a single step, with remaining contraction determined by inter-block couplings.\n\nLink to mixing and integrated autocorrelation time:\nFor a linear Gaussian Markov chain with mean update $\\mathbf{m}_{t+1} = \\mathbf{M}\\,\\mathbf{m}_{t}$ and stationary mean zero, the worst-mode autocorrelation at lag $k$ decays as $\\lambda_{\\max}(\\mathbf{M})^{k}$ for the eigenmode associated with the largest-magnitude eigenvalue. For a scalar autoregressive process $x_{t+1} = \\lambda x_{t} + \\text{noise}$, the integrated autocorrelation time is $\\tau = \\tfrac{1+\\lambda}{1-\\lambda}$ when $|\\lambda|<1$. As a conservative worst-mode proxy in the multivariate case, we use\n$$\n\\tau(\\mathbf{M}) = \\frac{1 + \\lambda_{\\max}(\\mathbf{M})}{1 - \\lambda_{\\max}(\\mathbf{M})},\n$$\nwhich increases rapidly as $\\lambda_{\\max}(\\mathbf{M}) \\to 1$.\n\nPrecision matrix construction:\nWe use local one-dimensional coupling via the path-graph Laplacian $\\mathbf{L}_{d}$ with Dirichlet boundaries: $(\\mathbf{L}_{d})_{ii} = 2$ for $i \\in \\{2,\\dots,d-1\\}$, $(\\mathbf{L}_{d})_{11} = 1$, $(\\mathbf{L}_{d})_{dd} = 1$, $(\\mathbf{L}_{d})_{i,i+1} = (\\mathbf{L}_{d})_{i+1,i} = -1$ for $i \\in \\{1,\\dots,d-1\\}$, and zeros otherwise. The precision is\n$$\n\\mathbf{A}(d,\\rho) = (1 - \\rho)\\,\\mathbf{I}_{d} + \\rho\\,\\mathbf{L}_{d},\n$$\nwhich is symmetric positive definite for $\\rho \\in [0,1)$.\n\nStep size choice:\nThe full ULA operator is $\\mathbf{M}_{\\text{full}}(\\varepsilon) = \\mathbf{I} - \\varepsilon \\mathbf{A}$. To ensure contraction and avoid instability, we choose\n$$\n\\varepsilon(d,\\rho) = \\frac{0.9}{\\lambda_{\\max}(\\mathbf{A}(d,\\rho))},\n$$\nwhich is less than $\\tfrac{2}{\\lambda_{\\max}(\\mathbf{A})}$ and keeps $|1 - \\varepsilon \\lambda_{j}(\\mathbf{A})| \\leq 1 - 0.9 \\,\\tfrac{\\lambda_{j}(\\mathbf{A})}{\\lambda_{\\max}(\\mathbf{A})} < 1$.\n\nAlgorithmic construction of operators:\n- Compute $\\mathbf{M}_{\\text{full}}(\\varepsilon)$ directly via $\\mathbf{I} - \\varepsilon \\mathbf{A}$.\n- For coordinate-wise ULA, define $\\mathbf{M}_{i}(\\varepsilon) = \\mathbf{I} - \\varepsilon\\,\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}\\mathbf{A}$ and form $\\mathbf{M}_{\\text{coord}}(\\varepsilon)$ by ordered multiplication $\\mathbf{M}_{\\text{coord}}(\\varepsilon) = \\mathbf{M}_{d}(\\varepsilon)\\cdots\\mathbf{M}_{1}(\\varepsilon)$.\n- For block Gibbs, for each contiguous block $b$ with index set $\\mathcal{I}_{b}$, take submatrices $\\mathbf{A}_{bb}$ and $\\mathbf{A}_{b,-b}$, compute $\\mathbf{K}_{b} = -\\mathbf{A}_{bb}^{-1}\\mathbf{A}_{b,-b}$, and form $\\mathbf{M}_{b}$ by replacing rows $\\mathcal{I}_{b}$ with $\\mathbf{K}_{b}$ on columns of the complement and zeros on the block columns, leaving identity on the complement rows. Multiply sequentially to obtain $\\mathbf{M}_{\\text{block}}$.\n\nMixing comparison and when block Gibbs accelerates:\nFor local couplings, such as with $\\mathbf{L}_{d}$, the spectrum of $\\mathbf{A}(d,\\rho)$ has a smallest eigenvalue that decreases as $d$ grows (for the Dirichlet Laplacian, $\\lambda_{\\min} = \\Theta(d^{-2})$). With $\\varepsilon$ scaled to $\\lambda_{\\max}^{-1}$, the largest-magnitude eigenvalue of $\\mathbf{M}_{\\text{full}}$ behaves like $1 - c\\,\\tfrac{\\lambda_{\\min}}{\\lambda_{\\max}}$, approaching $1$ as $d$ increases, leading to large $\\tau$. Coordinate-wise ULA compounds single-row contractions and retains slow inter-coordinate propagation, resulting in similar or worse spectral radii in strongly coupled regimes. In contrast, block Gibbs uses exact conditionals to eliminate intra-block modes immediately; the remaining contraction is governed by inter-block couplings via Schur complements, yielding significantly smaller $\\lambda_{\\max}(\\mathbf{M}_{\\text{block}})$ when $\\rho$ is large and $m$ captures local neighborhoods. This effect is amplified in high dimensions with strong local correlations, where larger blocks mitigate the small-eigenvalue bottleneck of $\\mathbf{A}$.\n\nWe implement the above for four test cases: $(d=\\;10,\\;\\rho=\\;0.3,\\;m=\\;5)$, $(d=\\;100,\\;\\rho=\\;0.9,\\;m=\\;10)$, $(d=\\;50,\\;\\rho=\\;0.0,\\;m=\\;5)$, $(d=\\;100,\\;\\rho=\\;0.99,\\;m=\\;20)$. For each, we compute $\\tau_{\\text{full}}$, $\\tau_{\\text{coord}}$, and $\\tau_{\\text{block}}$. In weakly coupled or independent cases (e.g., $\\rho = 0$), block Gibbs yields immediate decorrelation ($\\lambda_{\\max} = 0$ and $\\tau = 1$), while in strongly coupled, high-dimensional cases, block Gibbs achieves markedly smaller $\\lambda_{\\max}$ and hence smaller $\\tau$ than both full and coordinate-wise ULA, demonstrating accelerated mixing due to leveraging local energies.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_laplacian_dirichlet(d: int) -> np.ndarray:\n    \"\"\"\n    Build the 1D path-graph Laplacian L_d with Dirichlet boundary entries:\n    diag = [1, 2, 2, ..., 2, 1], off-diagonals = -1 on immediate neighbors.\n    \"\"\"\n    L = np.zeros((d, d), dtype=np.float64)\n    for i in range(d):\n        if i == 0 or i == d - 1:\n            L[i, i] = 1.0\n        else:\n            L[i, i] = 2.0\n        if i + 1 < d:\n            L[i, i + 1] = -1.0\n            L[i + 1, i] = -1.0\n    return L\n\ndef build_precision_matrix(d: int, rho: float) -> np.ndarray:\n    \"\"\"\n    A(d, rho) = (1 - rho) * I + rho * L_d, where L_d is the Dirichlet Laplacian.\n    \"\"\"\n    L = build_laplacian_dirichlet(d)\n    A = (1.0 - rho) * np.eye(d, dtype=np.float64) + rho * L\n    return A\n\ndef step_size(A: np.ndarray) -> float:\n    \"\"\"\n    epsilon = 0.9 / lambda_max(A), where lambda_max is the largest eigenvalue.\n    \"\"\"\n    # Since A is symmetric, use eigvalsh for numerical stability.\n    eigenvals = np.linalg.eigvalsh(A)\n    lam_max = float(np.max(eigenvals))\n    return 0.9 / lam_max\n\ndef M_full(A: np.ndarray, eps: float) -> np.ndarray:\n    \"\"\"\n    Full ULA mean operator: M_full = I - eps * A\n    \"\"\"\n    d = A.shape[0]\n    return np.eye(d, dtype=np.float64) - eps * A\n\ndef M_coord(A: np.ndarray, eps: float) -> np.ndarray:\n    \"\"\"\n    Coordinate-wise ULA cyclic sweep mean operator:\n    M_coord = M_d-1 ... M_1 M_0, where each M_i = I - eps * e_i e_i^T A.\n    \"\"\"\n    d = A.shape[0]\n    M = np.eye(d, dtype=np.float64)\n    # Sequentially apply coordinate updates: left-multiply by each M_i\n    for i in range(d):\n        # e_i e_i^T A is a matrix that keeps only the i-th row of A\n        # Construct P_i = e_i e_i^T\n        P_i = np.zeros((d, d), dtype=np.float64)\n        P_i[i, i] = 1.0\n        M_i = np.eye(d, dtype=np.float64) - eps * (P_i @ A)\n        M = M_i @ M\n    return M\n\ndef block_partitions(d: int, m: int):\n    \"\"\"\n    Generate contiguous blocks of size m across 0..d-1.\n    Last block may be shorter if d is not divisible by m.\n    \"\"\"\n    blocks = []\n    start = 0\n    while start < d:\n        end = min(start + m, d)\n        blocks.append(list(range(start, end)))\n        start = end\n    return blocks\n\ndef M_block(A: np.ndarray, m: int) -> np.ndarray:\n    \"\"\"\n    Block Gibbs sweep mean operator:\n    For each block b, x_b' = -A_bb^{-1} A_b,-b x_-b; x_-b' = x_-b.\n    Construct corresponding operator and multiply over blocks.\n    \"\"\"\n    d = A.shape[0]\n    M = np.eye(d, dtype=np.float64)\n    blocks = block_partitions(d, m)\n    for blk in blocks:\n        blk = np.array(blk, dtype=np.int64)\n        # Complement indices\n        comp = np.array([i for i in range(d) if i not in set(blk.tolist())], dtype=np.int64)\n        A_bb = A[np.ix_(blk, blk)]\n        A_b_comp = A[np.ix_(blk, comp)]\n        # K = -A_bb^{-1} A_b,-b\n        # Solve A_bb * X = A_b_comp for X, then K = -X\n        X = np.linalg.solve(A_bb, A_b_comp)\n        K = -X\n        # Build M_b operator\n        M_b = np.eye(d, dtype=np.float64)\n        # Zero out rows corresponding to block on block columns\n        M_b[np.ix_(blk, blk)] = 0.0\n        # Set rows corresponding to block on complement columns to K\n        M_b[np.ix_(blk, comp)] = K\n        # Compose\n        M = M_b @ M\n    return M\n\ndef spectral_radius(M: np.ndarray) -> float:\n    \"\"\"\n    Compute spectral radius: max absolute eigenvalue. For non-symmetric M, use eigvals.\n    \"\"\"\n    eigvals = np.linalg.eigvals(M)\n    return float(np.max(np.abs(eigvals)))\n\ndef tau_from_lambda(lam: float) -> float:\n    \"\"\"\n    Worst-mode integrated autocorrelation time proxy: tau = (1 + lam) / (1 - lam).\n    Assumes |lam| < 1; for numerical safety, if lam >= 1 - 1e-12, set tau to a large number.\n    \"\"\"\n    if lam >= 1.0 - 1e-12:\n        return float('inf')\n    return (1.0 + lam) / (1.0 - lam)\n\ndef run_case(d: int, rho: float, m: int):\n    \"\"\"\n    Build A, compute epsilon, construct operators, and return taus for full, coord, block.\n    \"\"\"\n    A = build_precision_matrix(d, rho)\n    eps = step_size(A)\n\n    Mfull = M_full(A, eps)\n    Mcoord = M_coord(A, eps)\n    Mblk = M_block(A, m)\n\n    lam_full = spectral_radius(Mfull)\n    lam_coord = spectral_radius(Mcoord)\n    lam_block = spectral_radius(Mblk)\n\n    tau_full = tau_from_lambda(lam_full)\n    tau_coord = tau_from_lambda(lam_coord)\n    tau_block = tau_from_lambda(lam_block)\n\n    return [tau_full, tau_coord, tau_block]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 0.3, 5),   # Case 1: moderate dimension and coupling\n        (100, 0.9, 10), # Case 2: high dimension, strong coupling\n        (50, 0.0, 5),   # Case 3: independent variables\n        (100, 0.99, 20) # Case 4: very strong coupling in high dimension\n    ]\n\n    results = []\n    for d, rho, m in test_cases:\n        taus = run_case(d, rho, m)\n        results.append(taus)\n\n    # Final print statement in the exact required format.\n    # Print nested list with default string conversion.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "3122336"}, {"introduction": "With a grasp on how to define and train EBMs, we now explore one of their most powerful features: compositionality. Because EBMs operate on the level of unnormalized log-probabilities, or energies, combining models can be as simple as performing arithmetic on their energy functions. This practice demonstrates this principle in a generative context by asking you to create a new model by additively interpolating the energies of two different class-conditional models [@problem_id:3122280]. By deriving the properties of the resulting interpolated distribution, you will gain a hands-on appreciation for how the energy-based perspective provides a flexible and intuitive framework for generative modeling.", "problem": "Consider an Energy-Based Model (EBM), that is, a model that assigns an energy function to data and turns it into a probability density via exponentiation and normalization. For a class-conditional setup for handwritten digits, suppose we have an energy function $E_{\\theta}(x \\mid y)$ over feature vectors $x \\in \\mathbb{R}^d$ conditioned on a class label $y$, where $\\theta$ denotes parameters. The probability density of $x$ given class $y$ is defined by\n$$\np_{\\theta}(x \\mid y) = \\frac{\\exp\\left(-E_{\\theta}(x \\mid y)\\right)}{Z_{\\theta}(y)},\n$$\nwhere $Z_{\\theta}(y)$ is the partition function ensuring normalization.\n\nStarting from core definitions of energy-based models, propose a class-conditional energy of the quadratic form suitable for intermediate-dimensional features of handwritten digits (for example, roundness and slant), namely\n$$\nE_{\\theta}(x \\mid y) = \\frac{1}{2} \\left(x - \\mu_y\\right)^{\\top} \\Sigma_y^{-1} \\left(x - \\mu_y\\right),\n$$\nwhere $\\mu_y \\in \\mathbb{R}^d$ is a class-dependent mean, and $\\Sigma_y \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive definite covariance matrix. This choice ensures a finite partition function and a well-defined probability density.\n\nDefine an interpolation of two classes $y_0$ and $y_1$ by the additive energy\n$$\nE_{\\alpha}(x) = \\alpha \\, E_{\\theta}(x \\mid y_0) + (1 - \\alpha) \\, E_{\\theta}(x \\mid y_1),\n$$\nwith interpolation weight $\\alpha \\in [0,1]$. Show that $E_{\\alpha}(x)$ is again quadratic and thus induces a Gaussian density over $x$, and derive its mean and covariance in closed form from first principles.\n\nImplement a program that, given specific class parameters and interpolation weights, computes the interpolated Gaussian’s mean and covariance, and then tests whether “adding energies yields meaningful class interpolation” according to the following quantitative criteria:\n- Boundary consistency: At $\\alpha=0$, the interpolated distribution coincides with the class $y_1$ distribution; at $\\alpha=1$, it coincides with class $y_0$.\n- Linear mean interpolation under equal covariances: If $\\Sigma_{y_0} = \\Sigma_{y_1}$, then the interpolated mean equals the linear interpolation $\\alpha \\mu_{y_0} + (1-\\alpha)\\mu_{y_1}$.\n- Precision-weighted bias under unequal covariances: If $\\Sigma_{y_1}$ has smaller variance than $\\Sigma_{y_0}$ along the line connecting $\\mu_{y_0}$ and $\\mu_{y_1}$, then the interpolated mean is biased closer to $\\mu_{y_1}$ than the naive linear interpolation would suggest.\n- Stationarity: The gradient of $E_{\\alpha}(x)$ evaluated at the interpolated mean is the zero vector.\n- Variance bounds: Each coordinate of the interpolated covariance lies between the corresponding coordinates of $\\Sigma_{y_0}$ and $\\Sigma_{y_1}$ when both are diagonal.\n\nUse the following test suite with $d=2$ and parameters specified below. In all cases, treat $y_0$ and $y_1$ as two digit classes:\n\n- Test $1$ (Boundary at $\\alpha = 0$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$; $\\alpha = 0$. Check both $\\mu_{\\alpha} = \\mu_{y_1}$ and $\\Sigma_{\\alpha} = \\Sigma_{y_1}$.\n- Test $2$ (Equal covariances and $\\alpha = 0.3$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$; $\\alpha = 0.3$. Check $\\mu_{\\alpha} = \\alpha \\mu_{y_0} + (1 - \\alpha)\\mu_{y_1}$.\n- Test $3$ (Bias with unequal covariances and $\\alpha = 0.5$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 0.25 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$; $\\alpha = 0.5$. Let $t$ be the scalar such that the projection of $\\mu_{\\alpha}$ onto the line from $\\mu_{y_0}$ to $\\mu_{y_1}$ satisfies $\\mu_{\\alpha} = \\mu_{y_0} + t(\\mu_{y_1} - \\mu_{y_0})$. Check $t > \\alpha$ and $0 \\le t \\le 1$.\n- Test $4$ (Stationarity with off-diagonal covariances and $\\alpha = 0.4$): $\\mu_{y_0} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.0 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 1.5 & -0.2 \\\\ -0.2 & 1.0 \\end{bmatrix}$; $\\alpha = 0.4$. Check that $\\nabla E_{\\alpha}(\\mu_{\\alpha}) = \\mathbf{0}$.\n- Test $5$ (Variance bounds with diagonal covariances and $\\alpha = 0.75$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 4.0 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 9.0 & 0 \\\\ 0 & 0.25 \\end{bmatrix}$; $\\alpha = 0.75$. Check that each diagonal entry of $\\Sigma_{\\alpha}$ lies between the corresponding entries of $\\Sigma_{y_0}$ and $\\Sigma_{y_1}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$), where each entry is a boolean indicating whether the corresponding test passed.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of energy-based models and multivariate statistics, is well-posed with all necessary information provided, and is framed using objective, formal language. The tasks are mathematically tractable and verifiable.\n\nThe problem requires us to analyze the properties of an interpolated energy function formed by the linear combination of two quadratic energy functions, which correspond to two class-conditional Gaussian distributions. The primary tasks are to derive the parameters of the resulting distribution and then to verify several key properties of this interpolation scheme through a series of numerical tests.\n\n### Principle-Based Derivation of Interpolated Gaussian Parameters\n\nAn energy-based model defines a probability distribution over a variable $x$ via an energy function $E(x)$ as $p(x) \\propto \\exp(-E(x))$. The given class-conditional energy function is:\n$$\nE_{\\theta}(x \\mid y) = \\frac{1}{2} (x - \\mu_y)^{\\top} \\Sigma_y^{-1} (x - \\mu_y)\n$$\nThis is, up to an additive constant, the negative log-likelihood of a multivariate Gaussian distribution $N(x; \\mu_y, \\Sigma_y)$. The matrix $\\Sigma_y^{-1}$ is known as the precision matrix.\n\nThe interpolated energy for two classes, $y_0$ and $y_1$, is defined as:\n$$\nE_{\\alpha}(x) = \\alpha E_{\\theta}(x \\mid y_0) + (1 - \\alpha) E_{\\theta}(x \\mid y_1)\n$$\nwhere $\\alpha \\in [0,1]$ is the interpolation weight.\n\nTo show that $E_{\\alpha}(x)$ is also quadratic and to find the parameters of its corresponding Gaussian distribution, we first substitute the explicit form of the energy functions:\n$$\nE_{\\alpha}(x) = \\frac{\\alpha}{2} (x - \\mu_{y_0})^{\\top} \\Sigma_{y_0}^{-1} (x - \\mu_{y_0}) + \\frac{1-\\alpha}{2} (x - \\mu_{y_1})^{\\top} \\Sigma_{y_1}^{-1} (x - \\mu_{y_1})\n$$\nWe expand the quadratic terms. A general quadratic form $(x - \\mu)^{\\top}P(x - \\mu)$ expands to $x^{\\top}Px - 2\\mu^{\\top}Px + \\mu^{\\top}P\\mu$, assuming $P$ is symmetric (which precision matrices are). Applying this, we get:\n$$\n2E_{\\alpha}(x) = \\alpha (x^{\\top}\\Sigma_{y_0}^{-1}x - 2\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1}x + C_0) + (1-\\alpha) (x^{\\top}\\Sigma_{y_1}^{-1}x - 2\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1}x + C_1)\n$$\nwhere $C_0$ and $C_1$ are constants with respect to $x$. We can group the terms based on their dependence on $x$:\n$$\n2E_{\\alpha}(x) = x^{\\top} \\left( \\alpha\\Sigma_{y_0}^{-1} + (1-\\alpha)\\Sigma_{y_1}^{-1} \\right) x - 2 \\left( \\alpha\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1} + (1-\\alpha)\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1} \\right) x + C_{total}\n$$\nThis expression is clearly a quadratic function of $x$. We can match this form to the general energy function of a Gaussian distribution $N(x; \\mu_{\\alpha}, \\Sigma_{\\alpha})$, which is $E(x) = \\frac{1}{2}(x - \\mu_{\\alpha})^{\\top}\\Sigma_{\\alpha}^{-1}(x - \\mu_{\\alpha}) + \\text{const}$. Expanding this gives:\n$$\n2E(x) = x^{\\top}\\Sigma_{\\alpha}^{-1}x - 2\\mu_{\\alpha}^{\\top}\\Sigma_{\\alpha}^{-1}x + \\text{const}\n$$\nBy comparing the coefficients of the quadratic ($x^{\\top}(\\cdot)x$) and linear ($(\\cdot)x$) terms between the two expressions for the energy, we can identify the parameters $\\mu_{\\alpha}$ and $\\Sigma_{\\alpha}$ of the interpolated distribution.\n\n1.  **Interpolated Covariance**: Comparing the quadratic terms, we find the inverse covariance (precision matrix) of the interpolated distribution, $\\Sigma_{\\alpha}^{-1}$:\n    $$\n    \\Sigma_{\\alpha}^{-1} = \\alpha \\Sigma_{y_0}^{-1} + (1 - \\alpha) \\Sigma_{y_1}^{-1}\n    $$\n    This shows that \"adding energies\" corresponds to a linear interpolation of the precision matrices. The new covariance matrix is the inverse of this result:\n    $$\n    \\Sigma_{\\alpha} = \\left( \\alpha \\Sigma_{y_0}^{-1} + (1 - \\alpha) \\Sigma_{y_1}^{-1} \\right)^{-1}\n    $$\n    Since $\\Sigma_{y_0}$ and $\\Sigma_{y_1}$ are positive definite and $\\alpha \\in [0,1]$, $\\Sigma_{y_0}^{-1}$ and $\\Sigma_{y_1}^{-1}$ are positive definite. Their weighted sum $\\Sigma_{\\alpha}^{-1}$ is also positive definite, ensuring that $\\Sigma_{\\alpha}$ is well-defined and positive definite.\n\n2.  **Interpolated Mean**: Comparing the linear terms, we have:\n    $$\n    \\mu_{\\alpha}^{\\top}\\Sigma_{\\alpha}^{-1} = \\alpha\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1} + (1-\\alpha)\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1}\n    $$\n    To solve for $\\mu_{\\alpha}$, we can transpose both sides and then pre-multiply by $\\Sigma_{\\alpha}$:\n    $$\n    (\\mu_{\\alpha}^{\\top}\\Sigma_{\\alpha}^{-1})^{\\top} = (\\alpha\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1} + (1-\\alpha)\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1})^{\\top}\n    $$\n    $$\n    (\\Sigma_{\\alpha}^{-1})^{\\top} \\mu_{\\alpha} = (\\alpha\\Sigma_{y_0}^{-1})^{\\top}\\mu_{y_0} + ((1-\\alpha)\\Sigma_{y_1}^{-1})^{\\top}\\mu_{y_1}\n    $$\n    Since precision matrices are symmetric, this simplifies to:\n    $$\n    \\Sigma_{\\alpha}^{-1} \\mu_{\\alpha} = \\alpha\\Sigma_{y_0}^{-1}\\mu_{y_0} + (1-\\alpha)\\Sigma_{y_1}^{-1}\\mu_{y_1}\n    $$\n    Finally, pre-multiplying by $\\Sigma_{\\alpha}$:\n    $$\n    \\mu_{\\alpha} = \\Sigma_{\\alpha} \\left( \\alpha\\Sigma_{y_0}^{-1}\\mu_{y_0} + (1-\\alpha)\\Sigma_{y_1}^{-1}\\mu_{y_1} \\right)\n    $$\n    The mean of the interpolated distribution is a precision-weighted average of the component means.\n\n### Algorithmic Design and Test Verification\n\nThe implementation will compute $\\mu_{\\alpha}$ and $\\Sigma_{\\alpha}$ using the derived closed-form expressions. The five tests verify important behaviors of this interpolation scheme.\n\n-   **Test 1 (Boundary consistency)**: Setting $\\alpha=0$ in the formulas yields $\\Sigma_{\\alpha}^{-1} = \\Sigma_{y_1}^{-1} \\implies \\Sigma_{\\alpha} = \\Sigma_{y_1}$ and $\\mu_{\\alpha} = \\Sigma_{y_1}(\\Sigma_{y_1}^{-1}\\mu_{y_1}) = \\mu_{y_1}$. The case for $\\alpha=1$ is symmetric. This test confirms the implementation correctly recovers the original distributions at the boundaries of the interpolation.\n-   **Test 2 (Linear mean interpolation under equal covariances)**: If $\\Sigma_{y_0} = \\Sigma_{y_1} = \\Sigma$, then $\\Sigma_{\\alpha}^{-1} = (\\alpha + 1-\\alpha)\\Sigma^{-1} = \\Sigma^{-1}$, so $\\Sigma_{\\alpha} = \\Sigma$. The mean becomes $\\mu_{\\alpha} = \\Sigma(\\alpha\\Sigma^{-1}\\mu_{y_0} + (1-\\alpha)\\Sigma^{-1}\\mu_{y_1}) = \\alpha\\mu_{y_0} + (1-\\alpha)\\mu_{y_1}$. The test confirms this special case, where precision-weighting reduces to simple linear interpolation of the means.\n-   **Test 3 (Precision-weighted bias)**: When covariances are unequal, the mean is pulled towards the mean of the distribution with higher precision (lower variance). The component with lower variance has a \"stronger\" influence, as its energy function rises more steeply away from its minimum. The test quantifies this bias.\n-   **Test 4 (Stationarity)**: The mean of a distribution is also its mode (point of maximum probability), which corresponds to the an extremum (minimum) of the energy function. A necessary condition for an extremum is that the gradient is zero. The test verifies that $\\nabla E_{\\alpha}(\\mu_{\\alpha}) = \\mathbf{0}$, confirming that our derived $\\mu_{\\alpha}$ is indeed the minimum of the interpolated energy function. The gradient is $\\nabla E_{\\alpha}(x) = \\alpha\\Sigma_{y_0}^{-1}(x-\\mu_{y_0}) + (1-\\alpha)\\Sigma_{y_1}^{-1}(x-\\mu_{y_1})$. Substituting $x=\\mu_\\alpha$ and using the derived formulas shows this is analytically zero.\n-   **Test 5 (Variance bounds)**: The interpolated precision $\\Sigma_{\\alpha,ii}^{-1}$ is a linear interpolation of the component precisions. Since the function $f(v) = 1/v$ is convex for $v > 0$, the interpolated variance $(\\Sigma_{\\alpha,ii}^{-1})^{-1}$ lies between the component variances. This is a consequence of the properties of weighted harmonic means. The test verifies this bounding property for diagonal covariance matrices.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing five tests on the interpolation of two \n    Gaussian distributions defined via an energy-based model formulation.\n    \"\"\"\n    \n    test_cases = [\n        # Test 1: Boundary at alpha = 0\n        {\n            \"id\": 1,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1., 0.], [0., 1.]]),\n            \"mu1\": np.array([2., 1.]),\n            \"S1\": np.array([[0.5, 0.], [0., 0.5]]),\n            \"alpha\": 0.0\n        },\n        # Test 2: Equal covariances and alpha = 0.3\n        {\n            \"id\": 2,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1., 0.], [0., 1.]]),\n            \"mu1\": np.array([2., 0.]),\n            \"S1\": np.array([[1., 0.], [0., 1.]]),\n            \"alpha\": 0.3\n        },\n        # Test 3: Bias with unequal covariances and alpha = 0.5\n        {\n            \"id\": 3,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1.0, 0.], [0., 0.5]]),\n            \"mu1\": np.array([2., 0.]),\n            \"S1\": np.array([[0.25, 0.], [0., 0.5]]),\n            \"alpha\": 0.5\n        },\n        # Test 4: Stationarity with off-diagonal covariances and alpha = 0.4\n        {\n            \"id\": 4,\n            \"mu0\": np.array([-1., 1.]),\n            \"S0\": np.array([[1.0, 0.3], [0.3, 1.0]]),\n            \"mu1\": np.array([1., -1.]),\n            \"S1\": np.array([[1.5, -0.2], [-0.2, 1.0]]),\n            \"alpha\": 0.4\n        },\n        # Test 5: Variance bounds with diagonal covariances and alpha = 0.75\n        {\n            \"id\": 5,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1.0, 0.], [0., 4.0]]),\n            \"mu1\": np.array([0., 0.]),\n            \"S1\": np.array([[9.0, 0.], [0., 0.25]]),\n            \"alpha\": 0.75\n        }\n    ]\n\n    def compute_interpolated_params(mu0, S0, mu1, S1, alpha):\n        \"\"\"\n        Computes the mean and covariance of the interpolated Gaussian distribution.\n        \"\"\"\n        S0_inv = np.linalg.inv(S0)\n        S1_inv = np.linalg.inv(S1)\n        \n        Sa_inv = alpha * S0_inv + (1 - alpha) * S1_inv\n        Sa = np.linalg.inv(Sa_inv)\n        \n        # Note: Sa is pre-multiplied\n        mu_a_term = alpha * (S0_inv @ mu0) + (1 - alpha) * (S1_inv @ mu1)\n        mu_a = Sa @ mu_a_term\n        \n        return mu_a, Sa\n\n    results = []\n    for case in test_cases:\n        mu0, S0, mu1, S1, alpha = case[\"mu0\"], case[\"S0\"], case[\"mu1\"], case[\"S1\"], case[\"alpha\"]\n        mu_a, Sa = compute_interpolated_params(mu0, S0, mu1, S1, alpha)\n        \n        test_id = case[\"id\"]\n        result = False\n        \n        if test_id == 1:\n            check_mu = np.allclose(mu_a, mu1)\n            check_S = np.allclose(Sa, S1)\n            result = check_mu and check_S\n        \n        elif test_id == 2:\n            mu_linear_interp = alpha * mu0 + (1 - alpha) * mu1\n            result = np.allclose(mu_a, mu_linear_interp)\n\n        elif test_id == 3:\n            # We calculated that mu_a lies on the line connecting mu0 and mu1 for this case.\n            # So the projection is mu_a itself.\n            # We solve mu_a = mu0 + t * (mu1-mu0) for t.\n            #  mu_a - mu0 = t * (mu1-mu0)\n            # Take dot product with (mu1-mu0) to solve for t\n            v = mu1 - mu0\n            w = mu_a - mu0\n            t = (w @ v) / (v @ v)\n            result = (t > alpha) and (0 <= t <= 1)\n\n        elif test_id == 4:\n            # Check stationarity: grad E_alpha(mu_a) == 0\n            S0_inv = np.linalg.inv(S0)\n            S1_inv = np.linalg.inv(S1)\n            grad = alpha * S0_inv @ (mu_a - mu0) + (1 - alpha) * S1_inv @ (mu_a - mu1)\n            result = np.allclose(grad, np.zeros_like(grad))\n\n        elif test_id == 5:\n            # Check variance bounds for diagonal matrices\n            Sa_diag = np.diag(Sa)\n            S0_diag = np.diag(S0)\n            S1_diag = np.diag(S1)\n            \n            lower_bounds = np.minimum(S0_diag, S1_diag)\n            upper_bounds = np.maximum(S0_diag, S1_diag)\n            \n            check_lower = np.all(Sa_diag >= lower_bounds)\n            check_upper = np.all(Sa_diag <= upper_bounds)\n            result = check_lower and check_upper\n            \n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3122280"}]}