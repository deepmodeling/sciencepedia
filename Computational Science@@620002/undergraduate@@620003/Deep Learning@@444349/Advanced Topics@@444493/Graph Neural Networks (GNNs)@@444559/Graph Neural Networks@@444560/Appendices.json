{"hands_on_practices": [{"introduction": "We begin our hands-on journey by exploring the fundamental mechanism of Graph Neural Networks: message passing for node classification. In this exercise ([@problem_id:3106157]), you will implement and compare several propagation-based models, from simple label diffusion to more sophisticated architectures like Simplified Graph Convolution (SGC) and Approximate Personalized Propagation of Neural Predictions (APPNP). This practice will provide a concrete understanding of how information propagates across a graph and reveal the critical challenge of over-smoothing, where node representations can become indistinguishable after too many propagation steps.", "problem": "You are given an undirected simple graph with weak node features and sparse supervision. Your goal is to implement three propagation-based methods and compare their test accuracies under a controlled set of parameters and to examine the boundary behavior as the number of propagation steps grows. The methods are: label propagation using powers of the normalized adjacency, Simplified Graph Convolution (SGC), and Approximate Personalized Propagation of Neural Predictions (APPNP). You must derive each method from core definitions and implement it as a complete program that produces the requested output format without user input.\n\nStart from the following fundamental base:\n- A graph is represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with $A_{ij} = 1$ if there is an edge between node $i$ and node $j$, and $A_{ij} = 0$ otherwise. Let $I$ denote the identity matrix. Define $\\tilde{A} = A + I$ to include self-loops. Define the degree matrix $\\tilde{D}$ by $\\tilde{D}_{ii} = \\sum_{j} \\tilde{A}_{ij}$. The symmetric normalized adjacency is $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$.\n- For any nonnegative integer $K$, the $K$-step propagation by $\\hat{A}$ is $\\hat{A}^K$. For a connected graph with $\\hat{A}$ symmetric and stochastic normalization implied by $\\tilde{A}$ with self-loops, the spectral radius satisfies $\\rho(\\hat{A}) = 1$, and for $K \\to \\infty$ one has convergence of $\\hat{A}^K$ to a rank-$1$ projector onto the principal eigenvector direction under standard conditions (e.g., aperiodicity ensured by self-loops).\n\nGraph, labels, features, and split:\n- Number of nodes $n = 8$ with node indices $\\{0,1,2,3,4,5,6,7\\}$.\n- Edges: there are two dense clusters of size $4$ joined by a single bridge. Specifically:\n  - For all distinct $i,j \\in \\{0,1,2,3\\}$, include the edge $\\{i,j\\}$.\n  - For all distinct $i,j \\in \\{4,5,6,7\\}$, include the edge $\\{i,j\\}$.\n  - Include the bridge edge $\\{3,4\\}$.\n- Ground-truth classes: nodes $\\{0,1,2,3\\}$ belong to class $0$, nodes $\\{4,5,6,7\\}$ belong to class $1$. Let $C = 2$ be the number of classes. Let $Y \\in \\mathbb{R}^{n \\times C}$ be the one-hot encoding of ground-truth labels, that is, $Y_{ic} = 1$ if node $i$ has class $c$, otherwise $Y_{ic} = 0$.\n- Labeled training set indices $L = \\{0,7\\}$ and test set indices $T = \\{1,2,3,4,5,6\\}$. Define the semi-supervised label seed matrix $Y_{0} \\in \\mathbb{R}^{n \\times C}$ by $Y_{0i:} = Y_{i:}$ if $i \\in L$ and $Y_{0i:} = 0$ if $i \\notin L$.\n- Weak node features: feature dimension $d = 3$. Let $X \\in \\mathbb{R}^{n \\times d}$ with entries $X_{ij} = 1 + \\epsilon_{ij} + s_{i} \\cdot \\delta \\cdot \\mathbb{1}\\{j=0\\}$ where $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent noise terms with $\\sigma = 0.02$, and $s_{i} = +1$ if node $i$ is in class $0$ and $s_{i} = -1$ if node $i$ is in class $1$, and $\\delta = 0.05$. This yields a weakly informative first feature and otherwise nearly constant features.\n\nMethods to implement:\n- Label propagation baseline: for a given integer $K \\ge 0$, compute $Y_{\\text{LP}} = \\hat{A}^{K} Y_{0}$, and classify each node $i$ by $\\arg\\max_{c} (Y_{\\text{LP}})_{ic}$. Evaluate accuracy only on the test set indices $T$ by comparing to ground-truth $Y$.\n- Simplified Graph Convolution (SGC): for a given integer $K \\ge 0$, compute $\\tilde{X} = \\hat{A}^{K} X$, fit a linear classifier by multi-output ridge regression on the labeled nodes $L$ to obtain weights $W \\in \\mathbb{R}^{d \\times C}$ that minimize $\\|\\tilde{X}_{L} W - Y_{L}\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2}$ with $\\lambda = 10^{-2}$, then predict $Z = \\tilde{X} W$ and classify with $\\arg\\max_{c} Z_{ic}$. Evaluate accuracy on $T$.\n- Approximate Personalized Propagation of Neural Predictions (APPNP): initialize $H^{(0)} = Y_{0}$ and iterate for $t = 0,1,\\dots,T_{\\max}-1$ the update $H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)}$ with teleportation parameter $\\alpha = 0.15$ and $T_{\\max} = 100$, then classify with $\\arg\\max_{c} H^{(T_{\\max})}_{ic}$ and evaluate accuracy on $T$.\n\nBoundary analysis to study:\n- For $K \\to \\infty$ in the label propagation and in SGCâ€™s smoothing $\\hat{A}^{K}$, argue what happens to class scores in terms of the spectrum of $\\hat{A}$, and how this affects accuracy when there are few labels and weak features.\n\nTest suite parameters:\n- Use the shared graph, labels, features, and splits as specified above.\n- Evaluate the three methods on the following propagation exponents $K \\in \\{0,1,5,200\\}$ for the label propagation and SGC methods, while APPNP uses fixed $\\alpha = 0.15$ and $T_{\\max} = 100$ for all cases.\n\nRequired outputs:\n- For each $K$ in the test suite, compute the tuple of test accuracies $(a_{\\text{LP}}, a_{\\text{SGC}}, a_{\\text{APPNP}})$, where each $a$ is the fraction of correctly classified nodes in the test set, expressed as a decimal rounded to three decimals.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order $K \\in \\{0,1,5,200\\}$, and is itself a three-element list of floats $[a_{\\text{LP}},a_{\\text{SGC}},a_{\\text{APPNP}}]$ rounded to three decimals. For example, the format must be like $[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}],\\dots]$.\n\nAll random quantities must be generated with a fixed seed to ensure deterministic results. No physical units are involved in this problem. Angles are not used. The final output must match the specified single-line format exactly.", "solution": "The user provided a valid problem statement.\n\nThe problem requires the implementation and comparison of three semi-supervised learning methods on graphs: Label Propagation (LP), Simplified Graph Convolution (SGC), and Approximate Personalized Propagation of Neural Predictions (APPNP). We will first establish the common mathematical framework, then derive each method, analyze their behavior in the limit of infinite propagation steps, and finally implement them to obtain the specified test accuracies.\n\n### 1. Graph and Data Representation\n\nThe graph has $n=8$ nodes and consists of two 4-node cliques, $\\{0,1,2,3\\}$ and $\\{4,5,6,7\\}$, connected by a single edge $\\{3,4\\}$. The adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ represents this structure. To incorporate node self-connections, which is standard practice in Graph Neural Networks (GNNs) to ensure aperiodicity and include a node's own features in its update, we define $\\tilde{A} = A + I$, where $I$ is the $n \\times n$ identity matrix.\n\nThe propagation dynamics are governed by the symmetrically normalized adjacency matrix. We define the degree matrix $\\tilde{D}$ of $\\tilde{A}$ as a diagonal matrix with entries $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$. The normalized adjacency matrix is then:\n$$ \\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} $$\nThis matrix is symmetric and its eigenvalues $\\lambda$ are bounded, $|\\lambda| \\le 1$. The operation $H' = \\hat{A} H$ can be interpreted as aggregating information from neighboring nodes, where $H$ could be a matrix of node features or labels.\n\nThe data consists of:\n- **Node Features** $X \\in \\mathbb{R}^{n \\times d}$: A matrix where row $i$ is the feature vector of node $i$. The features are designed to be weakly informative.\n- **Ground Truth Labels** $Y \\in \\{0,1\\}^{n \\times C}$: A one-hot encoded matrix of the true class assignments, with $C=2$ classes.\n- **Seed Labels** $Y_0 \\in \\mathbb{R}^{n \\times C}$: A sparse matrix containing the one-hot labels for the training nodes in set $L=\\{0,7\\}$ and zeros for all other nodes. This matrix seeds the propagation process.\n\n### 2. Propagation-Based Methods\n\n**Method 1: Label Propagation (LP)**\nLP is a simple semi-supervised algorithm that diffuses label information from labeled nodes across the graph. The core idea is that connected nodes are likely to share the same label. The propagation for $K$ steps is achieved by multiplying the seed label matrix $Y_0$ by the $K$-th power of the normalized adjacency matrix $\\hat{A}$.\n\nThe predicted label scores after $K$ steps are given by:\n$$ Y_{\\text{LP}} = \\hat{A}^K Y_0 $$\nThe final class prediction for node $i$ is the one that maximizes its score: $\\arg\\max_c (Y_{\\text{LP}})_{ic}$. For $K=0$, $\\hat{A}^0 = I$, so only the initially labeled nodes have non-zero scores, and all unlabeled nodes are predicted as class $0$ (due to `argmax` tie-breaking).\n\n**Method 2: Simplified Graph Convolution (SGC)**\nSGC simplifies GNNs by decoupling the feature propagation from the non-linear transformation. It consists of two stages:\n1.  **Feature Propagation**: The node features $X$ are smoothed over the graph by applying the propagation matrix $\\hat{A}^K$:\n    $$ \\tilde{X} = \\hat{A}^K X $$\n    This step effectively creates new features for each node by aggregating feature information from its $K$-hop neighborhood.\n2.  **Linear Classification**: A linear classifier is trained on the propagated features of the labeled nodes. We seek a weight matrix $W \\in \\mathbb{R}^{d \\times C}$ that minimizes a ridge regression objective:\n    $$ \\underset{W}{\\text{minimize}} \\quad \\|\\tilde{X}_L W - Y_L\\|_F^2 + \\lambda \\|W\\|_F^2 $$\n    where $\\tilde{X}_L$ and $Y_L$ are the rows of $\\tilde{X}$ and $Y$ corresponding to the labeled set $L$, and $\\| \\cdot \\|_F$ is the Frobenius norm. The regularization parameter is given as $\\lambda=10^{-2}$. This optimization problem has a closed-form solution:\n    $$ W = (\\tilde{X}_L^T \\tilde{X}_L + \\lambda I_d)^{-1} \\tilde{X}_L^T Y_L $$\n    where $I_d$ is the $d \\times d$ identity matrix.\nPredictions for all nodes are then computed as $Z = \\tilde{X} W$, and the class for node $i$ is $\\arg\\max_c Z_{ic}$.\n\n**Method 3: Approximate Personalized Propagation of Neural Predictions (APPNP)**\nAPPNP is inspired by Personalized PageRank and is designed to combat the over-smoothing problem inherent in deep GNNs. It maintains a balance between propagating neighborhood information and preserving the initial \"root\" information. While the original APPNP uses the output of a neural network as the root information, here we use the seed labels $Y_0$.\n\nStarting with $H^{(0)} = Y_0$, APPNP performs the following iterative update for $t=0, 1, \\dots, T_{\\max}-1$:\n$$ H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)} $$\nHere, $\\alpha \\in [0,1]$ is the teleport probability, given as $\\alpha=0.15$. At each step, the process involves one step of neighborhood aggregation (the $\\hat{A}H^{(t)}$ term) and a \"teleport\" back to the initial predictions (the $H^{(0)}$ term). This ensures that the influence from the seed nodes is never completely lost. The final predictions are taken from a converged or near-converged state, here $H^{(T_{\\max})}$, with $T_{\\max}=100$. Classification is performed via $\\arg\\max_c (H^{(T_{\\max})})_{ic}$.\n\n### 3. Boundary Analysis for $K \\to \\infty$\n\nThe behavior of LP and SGC for a large number of propagation steps $K$ is dictated by the spectral properties of $\\hat{A}$. Since the graph is connected and aperiodic (due to self-loops), the Perron-Frobenius theorem implies that the largest eigenvalue of $\\hat{A}$ is $\\lambda_1 = 1$ with multiplicity one. All other eigenvalues $|\\lambda_i| < 1$. As $K \\to \\infty$, the powers of $\\hat{A}$ converge to a rank-1 projector onto the principal eigenvector $u_1$:\n$$ \\lim_{K \\to \\infty} \\hat{A}^K = u_1 u_1^T $$\nwhere $u_1$ is the normalized eigenvector corresponding to $\\lambda_1=1$. For the symmetric normalization, this eigenvector is given by $(u_1)_i \\propto \\sqrt{\\tilde{D}_{ii}}$.\n\n**LP Over-smoothing**:\nFor large $K$, the LP predictions become:\n$$ Y_{\\text{LP}} \\approx (u_1 u_1^T) Y_0 = u_1 (u_1^T Y_0) $$\nThe term $v^T = u_1^T Y_0$ is a $1 \\times C$ row vector of constants. The score matrix $Y_{\\text{LP}}$ becomes $u_1 v^T$, meaning the score vector for node $i$ is $(u_1)_i v^T$. Since $(u_1)_i > 0$, the classification decision $\\arg\\max_c ((u_1)_i v_c)$ simplifies to $\\arg\\max_c v_c$, which is independent of the node $i$. Consequently, all nodes receive the same prediction. This phenomenon is known as \"over-smoothing\". The test accuracy will collapse to the proportion of test nodes belonging to this single predicted class. In this problem, the degrees of nodes $0$ and $7$ are equal, so the weights they contribute to the final aggregated score are equal, leading to a tie that `argmax` breaks in favor of class $0$. This yields an accuracy of $3/6 = 0.5$.\n\n**SGC Over-smoothing**:\nA similar fate befalls SGC. For large $K$, the smoothed features become:\n$$ \\tilde{X} \\approx (u_1 u_1^T) X = u_1 (u_1^T X) $$\nThe term $f^T = u_1^T X$ is a $1 \\times d$ row vector. The feature vector for any node $i$ becomes $(\\tilde{X})_{i:} = (u_1)_i f^T$. All node features become collinear, pointing in the direction of $f^T$. A linear classifier trained on such features will make the same prediction for every node, as the separating hyperplane will divide the feature space, and all feature vectors lie on the same line. This again results in an accuracy of $0.5$.\n\n**APPNP's Resistance to Over-smoothing**:\nAPPNP avoids this issue due to the teleportation mechanism. The converged solution for APPNP is $H^{(\\infty)} = \\alpha(I - (1-\\alpha)\\hat{A})^{-1} H^{(0)}$. The presence of the identity matrix $I$ and the teleport probability $\\alpha$ prevents the propagation from completely washing out the initial information in $H^{(0)}$. It ensures that the final predictions remain \"personalized\" to the initial seed nodes, thus maintaining high accuracy even on structures susceptible to over-smoothing.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates Label Propagation, SGC, and APPNP on a\n    specified graph and dataset.\n    \"\"\"\n\n    def build_graph():\n        \"\"\"Constructs the adjacency matrix for the specified graph.\"\"\"\n        n = 8\n        A = np.zeros((n, n), dtype=np.float64)\n        # Clique on {0, 1, 2, 3}\n        for i in range(4):\n            for j in range(i + 1, 4):\n                A[i, j] = A[j, i] = 1.0\n        # Clique on {4, 5, 6, 7}\n        for i in range(4, 8):\n            for j in range(i + 1, 8):\n                A[i, j] = A[j, i] = 1.0\n        # Bridge {3, 4}\n        A[3, 4] = A[4, 3] = 1.0\n        return A\n\n    def normalize_adjacency(A):\n        \"\"\"Computes the symmetrically normalized adjacency matrix.\"\"\"\n        A_tilde = A + np.eye(A.shape[0])\n        D_tilde_diag = A_tilde.sum(axis=1)\n        # Replace zeros in D_tilde_diag to avoid division by zero, though not expected here.\n        D_tilde_diag[D_tilde_diag == 0] = 1e-12\n        D_tilde_inv_sqrt = np.diag(np.power(D_tilde_diag, -0.5))\n        A_hat = D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt\n        return A_hat\n\n    def generate_data(n, seed):\n        \"\"\"Generates labels, features, and data splits.\"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Ground truth labels\n        y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n        C = 2\n        Y = np.eye(C, dtype=np.float64)[y_true]\n        \n        # Labeled and test sets\n        L = [0, 7]\n        T = [1, 2, 3, 4, 5, 6]\n        \n        # Seed labels (for LP and APPNP)\n        Y_0 = np.zeros_like(Y)\n        Y_0[L] = Y[L]\n        \n        # Node features\n        d = 3\n        sigma = 0.02\n        delta = 0.05\n        s = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n        \n        noise = rng.normal(0, sigma, size=(n, d))\n        signal = np.zeros((n, d))\n        signal[:, 0] = s * delta\n        \n        X = 1.0 + noise + signal\n        \n        return X, Y, y_true, Y_0, L, T\n\n    def solve_lp(A_hat, Y_0, K, T, y_true):\n        \"\"\"Calculates test accuracy for Label Propagation.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n        \n        Y_lp = A_hat_K @ Y_0\n        preds = np.argmax(Y_lp, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_sgc(A_hat, X, Y, K, T, y_true, L, reg_lambda):\n        \"\"\"Calculates test accuracy for Simplified Graph Convolution.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n            \n        X_tilde = A_hat_K @ X\n        \n        # Ridge regression\n        X_L = X_tilde[L]\n        Y_L = Y[L]\n        \n        d = X.shape[1]\n        term1 = X_L.T @ X_L + reg_lambda * np.eye(d)\n        term2 = X_L.T @ Y_L\n        W = np.linalg.solve(term1, term2)\n        \n        Z = X_tilde @ W\n        preds = np.argmax(Z, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_appnp(A_hat, Y_0, T, y_true, alpha, T_max):\n        \"\"\"Calculates test accuracy for APPNP.\"\"\"\n        H = Y_0.copy()\n        for _ in range(T_max):\n            H = (1 - alpha) * (A_hat @ H) + alpha * Y_0\n        \n        preds = np.argmax(H, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    # Main execution logic\n    n_nodes = 8\n    random_seed = 42 # For reproducibility\n    \n    A = build_graph()\n    A_hat = normalize_adjacency(A)\n    X, Y, y_true, Y_0, L, T = generate_data(n=n_nodes, seed=random_seed)\n    \n    test_suite_K = [0, 1, 5, 200]\n    sgc_lambda = 1e-2\n    appnp_alpha = 0.15\n    appnp_T_max = 100\n    \n    all_results = []\n    \n    # APPNP result is constant for all K\n    acc_appnp = solve_appnp(A_hat, Y_0, T, y_true, appnp_alpha, appnp_T_max)\n    \n    for K in test_suite_K:\n        acc_lp = solve_lp(A_hat, Y_0, K, T, y_true)\n        acc_sgc = solve_sgc(A_hat, X, Y, K, T, y_true, L, sgc_lambda)\n        \n        all_results.append([acc_lp, acc_sgc, acc_appnp])\n\n    # Format output string without spaces\n    result_strings = []\n    for r in all_results:\n        inner_str = '[' + ','.join([f'{val:.3f}' for val in r]) + ']'\n        result_strings.append(inner_str)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n\n```", "id": "3106157"}, {"introduction": "While powerful, many standard GNNs implicitly assume homophilyâ€”that connected nodes are similar. This practice ([@problem_id:3131968]) directly challenges that assumption by placing you in a scenario with strong heterophily, where neighboring nodes carry opposing information. You will implement both a simple mean aggregator and an attention-based aggregator to see firsthand why the former fails and how the latter can intelligently weigh messages to succeed, offering a vital lesson on the importance of adaptive aggregation mechanisms.", "problem": "You are given a small suite of synthetic graphs designed to exhibit strong heterophily, where neighboring nodes predominantly have opposite attribute values. The task is to perform graph-level classification using two different message-passing schemes within a Graph Neural Network (GNN): a mean aggregator and an attention-based aggregator. The graphs are stars with a single hub (center) node connected to $n$ leaves. Each node has a feature vector in $\\mathbb{R}^2$ with components $(x,z)$, where $x \\in \\{-1,+1\\}$ encodes a semantic type and $z \\in \\{0,1\\}$ encodes a latent marker. Strong heterophily is enforced by setting each leaf nodeâ€™s $x$ to be the opposite sign of the hubâ€™s $x$. The graph label $y \\in \\{0,1\\}$ equals the hub nodeâ€™s marker $z$.\n\nYou must implement two message-passing layers ($L=2$) for both schemes:\n- Mean aggregation (excluding self-loops): For node $u$, the update at layer $\\ell$ is\n$$\nh_u^{(\\ell+1)} = \\frac{1}{|\\mathcal{N}(u)|} \\sum_{v \\in \\mathcal{N}(u)} W h_v^{(\\ell)},\n$$\nwhere $W \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix, $h_u^{(0)} = [x_u, z_u]^\\top$, and $\\mathcal{N}(u)$ denotes the set of neighbors of $u$.\n\n- Attention-based aggregation with self-loops following the Graph Attention Network (GAT): For node $u$, define the unnormalized attention score\n$$\ne_{uv} = a^\\top [W h_u^{(\\ell)} \\,\\|\\, W h_v^{(\\ell)}],\n$$\nwhere $[\\cdot \\,\\|\\, \\cdot]$ denotes concatenation, $W \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix, and $a \\in \\mathbb{R}^{4}$ is a fixed attention vector. Use the softmax to normalize over the neighborhood that includes a self-loop,\n$$\n\\alpha_{uv} = \\frac{\\exp(e_{uv})}{\\sum_{t \\in \\mathcal{N}'(u)} \\exp(e_{ut})}, \\quad \\mathcal{N}'(u) = \\mathcal{N}(u) \\cup \\{u\\},\n$$\nand update\n$$\nh_u^{(\\ell+1)} = \\sum_{v \\in \\mathcal{N}'(u)} \\alpha_{uv} \\, W h_v^{(\\ell)}.\n$$\nUse the fixed attention vector\n$$\na = [2, 1, 2, 1]^\\top.\n$$\n\nAfter $L=2$ layers, compute a graph-level decision as follows. Let $N$ be the number of nodes in the graph. Define the fraction\n$$\nf = \\frac{1}{N} \\sum_{u} \\mathbf{1}\\{h_{u,z}^{(L)} \\geq 0.5\\},\n$$\nwhere $h_{u,z}^{(L)}$ is the $z$-component of $h_u^{(L)}$ and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Predict\n$$\n\\hat{y} = \\begin{cases}\n1 & \\text{if } f \\geq 0.5, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\nYou must implement both schemes on the following test suite of star graphs, each specified by $(n, x_{\\text{center}}, z_{\\text{center}})$, where $n$ is the number of leaves, $x_{\\text{center}} \\in \\{-1,+1\\}$ is the hubâ€™s $x$ value, and $z_{\\text{center}} \\in \\{0,1\\}$ is the hubâ€™s marker. In every case, each leaf has $x_{\\text{leaf}} = - x_{\\text{center}}$ and $z_{\\text{leaf}} = 0$.\n\nTest suite:\n- Case $1$: $(n = 3, x_{\\text{center}} = +1, z_{\\text{center}} = 1)$.\n- Case $2$: $(n = 3, x_{\\text{center}} = +1, z_{\\text{center}} = 0)$.\n- Case $3$: $(n = 5, x_{\\text{center}} = -1, z_{\\text{center}} = 1)$.\n- Case $4$: $(n = 5, x_{\\text{center}} = -1, z_{\\text{center}} = 0)$.\n- Case $5$: $(n = 1, x_{\\text{center}} = +1, z_{\\text{center}} = 1)$.\n\nFor each case, the ground-truth label is $y = z_{\\text{center}}$.\n\nYour program must:\n- Construct the adjacency matrix and initial node features for each case.\n- Apply two layers ($L=2$) of mean aggregation and attention-based aggregation as specified.\n- Produce the predicted labels $\\hat{y}_{\\text{mean}}$ and $\\hat{y}_{\\text{attn}}$ for each case.\n- Compute the overall accuracies across all cases as decimals (not percentages), namely\n$$\n\\text{acc}_{\\text{mean}} = \\frac{\\text{number of correct } \\hat{y}_{\\text{mean}}}{5}, \\qquad \\text{acc}_{\\text{attn}} = \\frac{\\text{number of correct } \\hat{y}_{\\text{attn}}}{5}.\n$$\n- For each case, also compute a boolean indicator $b$ that equals $1$ if attention is correct and mean is incorrect, and equals $0$ otherwise.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\hat{y}_{\\text{mean},1}, \\hat{y}_{\\text{attn},1}, \\hat{y}_{\\text{mean},2}, \\hat{y}_{\\text{attn},2}, \\hat{y}_{\\text{mean},3}, \\hat{y}_{\\text{attn},3}, \\hat{y}_{\\text{mean},4}, \\hat{y}_{\\text{attn},4}, \\hat{y}_{\\text{mean},5}, \\hat{y}_{\\text{attn},5}, \\text{acc}_{\\text{mean}}, \\text{acc}_{\\text{attn}}, B],\n$$\nwhere $B$ is the total number of cases for which attention outperforms mean,\n$$\nB = \\sum_{i=1}^{5} b_i.\n$$\nNo physical units are involved. Angles are not involved. All quantities are dimensionless. Ensure scientific realism by following the exact aggregation definitions above. The program must be self-contained and not require external input.\n\nExplain, in your solution, from first principles, why attention helps under label disparity in heterophilous graphs by weighting messages according to feature compatibility rather than indiscriminate averaging.", "solution": "The user-provided problem is rigorously defined and scientifically sound, situated within the established domain of graph neural networks (GNNs). It presents a didactic scenario to contrast the behavior of mean and attention-based aggregation mechanisms on heterophilous graphs. All parameters, update rules, and evaluation metrics are specified without ambiguity. The problem is therefore deemed valid and a full solution is provided below.\n\nThe core task is to classify star-structured graphs characterized by strong heterophily. In these graphs, a central hub node and its connected leaf nodes have opposing semantic features ($x_{\\text{center}} = -x_{\\text{leaf}}$). The graph's label $y \\in \\{0, 1\\}$ is determined solely by a latent marker $z_{\\text{center}}$ on the hub node. This setup is designed to challenge standard GNN aggregators, which typically assume homophily (i.e., that connected nodes are similar). We will analyze and implement two aggregation schemes: mean aggregation and attention-based aggregation.\n\nLet the graph have $N=n+1$ nodes, with node $u=0$ as the center and nodes $u=1, \\dots, n$ as the leaves. The initial features (at layer $\\ell=0$) are $h_0^{(0)} = [x_c, z_c]^\\top$ for the center and $h_i^{(0)} = [-x_c, 0]^\\top$ for any leaf $i$.\n\n### Analysis of Mean Aggregation\n\nThe mean aggregation update rule is given by:\n$$\nh_u^{(\\ell+1)} = \\frac{1}{|\\mathcal{N}(u)|} \\sum_{v \\in \\mathcal{N}(u)} W h_v^{(\\ell)}\n$$\nWith the weight matrix $W$ being the identity matrix $I$, this simplifies to averaging the feature vectors of a node's neighbors.\n\n**Layer 1 ($L=1$) Update:**\n- **Hub Node ($u=0$):** The hub is connected to all $n$ leaves. Its updated feature vector is the average of the initial leaf features.\n$$\nh_0^{(1)} = \\frac{1}{n} \\sum_{i=1}^n h_i^{(0)} = \\frac{1}{n} \\sum_{i=1}^n \\begin{pmatrix} -x_c \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -x_c \\\\ 0 \\end{pmatrix}\n$$\nCritically, the hub's original marker $z_c$, which contains the ground-truth label information, is immediately erased and replaced with $0$. The hub's features become identical to those of a leaf.\n\n- **Leaf Nodes ($u=i \\in \\{1,\\dots,n\\}$):** Each leaf is connected only to the hub.\n$$\nh_i^{(1)} = \\frac{1}{1} h_0^{(0)} = \\begin{pmatrix} x_c \\\\ z_c \\end{pmatrix}\n$$\nThe leaves inherit the hub's original features, including the marker $z_c$.\n\n**Layer 2 ($L=2$) Update:**\n- **Hub Node ($u=0$):** The hub now averages the features of the leaves from the previous layer.\n$$\nh_0^{(2)} = \\frac{1}{n} \\sum_{i=1}^n h_i^{(1)} = \\frac{1}{n} \\sum_{i=1}^n \\begin{pmatrix} x_c \\\\ z_c \\end{pmatrix} = \\begin{pmatrix} x_c \\\\ z_c \\end{pmatrix}\n$$\nThe hub's features revert to their original state, restoring the marker $z_c$.\n\n- **Leaf Nodes ($u=i \\in \\{1,\\dots,n\\}$):** Each leaf receives the features of the hub from the previous layer.\n$$\nh_i^{(2)} = h_0^{(1)} = \\begin{pmatrix} -x_c \\\\ 0 \\end{pmatrix}\n$$\nThe leaves revert to their original features.\n\n**Prediction with Mean Aggregation:**\nAfter $L=2$ layers, the final $z$-components of the feature vectors are $h_{0,z}^{(2)} = z_c$ and $h_{i,z}^{(2)} = 0$ for all leaves $i$. The prediction $\\hat{y}$ is based on the fraction of nodes $f$ for which $h_{u,z}^{(2)} \\geq 0.5$.\n\n- If the true label is $y=z_c=1$: The condition $h_{u,z}^{(2)} \\geq 0.5$ is met only by the hub node ($u=0$). The number of such nodes is $1$. The fraction is $f = 1/N = 1/(n+1)$. The prediction is $\\hat{y}_{\\text{mean}} = 1$ if and only if $1/(n+1) \\geq 0.5$, which simplifies to $n \\leq 1$. For any star graph with more than one leaf ($n>1$), the prediction will be $\\hat{y}_{\\text{mean}}=0$, which is incorrect.\n\n- If the true label is $y=z_c=0$: No node satisfies $h_{u,z}^{(2)} \\geq 0.5$. The count is $0$, so $f=0$. The prediction is $\\hat{y}_{\\text{mean}}=0$, which is always correct.\n\nIn summary, mean aggregation fails on these heterophilous graphs whenever the label is $1$ and the graph is larger than a simple pair of nodes.\n\n### Analysis of Attention-Based Aggregation\n\nThe GAT-style attention mechanism updates features as:\n$$\nh_u^{(\\ell+1)} = \\sum_{v \\in \\mathcal{N}'(u)} \\alpha_{uv} \\, W h_v^{(\\ell)}\n$$\nwhere $\\mathcal{N}'(u) = \\mathcal{N}(u) \\cup \\{u\\}$ includes a self-loop. The attention coefficients $\\alpha_{uv}$ are computed via a softmax over scores $e_{uv} = a^\\top [W h_u^{(\\ell)} \\,\\|\\, W h_v^{(\\ell)}]$. With $W=I$ and $a = [2, 1, 2, 1]^\\top$, the score is $e_{uv} = 2x_u^{(\\ell)} + z_u^{(\\ell)} + 2x_v^{(\\ell)} + z_v^{(\\ell)}$.\n\nThe key difference from mean aggregation is two-fold:\n1.  **Self-Loops:** A node `u` can attend to itself, allowing it to preserve its own features via the weight $\\alpha_{uu}$.\n2.  **Feature-Dependent Weights:** The weights $\\alpha_{uv}$ are not uniform; they depend on the features of both the source node $v$ and the target node $u$. This allows the model to learn or, in this case, be designed to prioritize messages from more \"compatible\" or important neighbors.\n\nLet's analyze the behavior when $y=z_c=1$.\n**Layer 1 ($L=1$) Update:**\n- **Hub Node ($u=0$):** The hub attends to itself and the $n$ leaves.\n    - **Self-attention score:** $e_{00} = 2x_c^{(0)} + z_c^{(0)} + 2x_c^{(0)} + z_c^{(0)} = 4x_c + 2(1) = 4x_c+2$.\n    - **Neighbor attention score (to leaf $i$):** $e_{0i} = 2x_c^{(0)} + z_c^{(0)} + 2x_i^{(0)} + z_i^{(0)} = 2x_c + 1 + 2(-x_c) + 0 = 1$.\nThe score for the self-loop ($e_{00}$) is significantly different from the score for neighbors ($e_{0i}$). For $x_c=+1$, $e_{00}=6$; for $x_c=-1$, $e_{00}=-2$. In either case, the exponentiated self-attention score $\\exp(e_{00})$ will be substantially different from the neighbor score $\\exp(1)$, causing the self-attention weight $\\alpha_{00}$ to be large. This mechanism allows the hub to largely retain its own features, preventing the critical marker $z_c=1$ from being averaged out by the leaves' $z=0$ features.\n\n- **Leaf Nodes ($u=i$):** Each leaf attends to itself and the hub.\n    - **Self-attention score:** $e_{ii} = 4x_i^{(0)} + 2z_i^{(0)} = 4(-x_c) + 0 = -4x_c$.\n    - **Neighbor attention score (to hub $0$):** $e_{i0} = 2x_i^{(0)} + z_i^{(0)} + 2x_c^{(0)} + z_c^{(0)} = 2(-x_c) + 0 + 2x_c + 1 = 1$.\nThe score for attending to the hub is $1$, while the self-attention score is $\\pm 4$. The weight $\\alpha_{i0}$ will be high, meaning the leaf incorporates a large portion of the hub's features. Specifically, the $z$-component of the leaf's new feature vector $h_{i,z}^{(1)}$ will be a weighted average where the hub's feature $h_{0,z}^{(0)}=1$ receives a high weight.\n\n**Outcome of Attention Mechanism:**\nWhen $z_c=1$, the hub preserves its marker $z_c=1$ due to strong self-attention. Simultaneously, the leaves strongly attend to the hub, and their $z$-components are updated from $0$ to a value close to $1$. After the first layer, all nodes in the graph will have a $z$-component $h_{u,z}^{(1)}$ significantly greater than $0.5$. In the second layer, the nodes are now highly similar (homophilous) in their features, and aggregation will maintain this property. The final features $h_{u,z}^{(2)}$ will remain high for all nodes.\n\n**Prediction with Attention Aggregation:**\n- If $y=z_c=1$: All $N=n+1$ nodes will have $h_{u,z}^{(2)} \\geq 0.5$. The fraction is $f = (n+1)/(n+1) = 1$. The prediction is $\\hat{y}_{\\text{attn}} = 1$, which is correct regardless of $n$.\n- If $y=z_c=0$: The attention scores change. For the hub, $e_{0i}=0$. For the leaf, $e_{i0}=0$. The $z$-component of all messages is $0$, so the final $z$-components of all nodes remain $0$. The fraction is $f=0$, and the prediction $\\hat{y}_{\\text{attn}}=0$ is correct.\n\nTherefore, the attention-based aggregator correctly classifies all cases in the test suite by selectively weighting information based on feature compatibility, using self-loops to preserve essential local information and directed attention to propagate it where needed. The specific attention vector $a$ is well-chosen to reward attending to the informative hub when its marker is active ($z_c=1$).\n\nThe numerical calculations for each test case are performed by the program in the `<final_answer>` section, which implements these described update rules for the two layers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GNN classification problem for a suite of synthetic heterophilous graphs.\n    \"\"\"\n    \n    # Test suite: (n, x_center, z_center)\n    test_cases = [\n        (3, 1, 1),   # Case 1\n        (3, 1, 0),   # Case 2\n        (5, -1, 1),  # Case 3\n        (5, -1, 0),  # Case 4\n        (1, 1, 1),   # Case 5\n    ]\n    \n    results_list = []\n    y_preds_mean = []\n    y_preds_attn = []\n    y_trues = []\n\n    for n, x_c, z_c in test_cases:\n        # --- 1. Graph and Feature Initialization ---\n        num_nodes = n + 1\n        y_true = z_c\n        y_trues.append(y_true)\n\n        # Initial features h^(0)\n        h0 = np.zeros((num_nodes, 2), dtype=np.float64)\n        h0[0] = [x_c, z_c]  # Hub node\n        if n > 0:\n            h0[1:] = [-x_c, 0] # Leaf nodes\n\n        # Adjacency matrix\n        adj = np.zeros((num_nodes, num_nodes), dtype=int)\n        if n > 0:\n            adj[0, 1:] = 1\n            adj[1:, 0] = 1\n\n        # --- 2. Mean Aggregation ---\n        h_mean = h0.copy()\n        \n        # Apply L=2 layers\n        for _ in range(2):\n            h_next = np.zeros_like(h_mean)\n            for u in range(num_nodes):\n                neighbors = np.where(adj[u] == 1)[0]\n                if len(neighbors) == 0:\n                    # Handle disconnected node (n=0 case which is not in test suite)\n                    # A node with no neighbors keeps its features in mean aggregation.\n                    h_next[u] = h_mean[u]\n                    continue\n\n                neighbor_features = h_mean[neighbors]\n                aggregated_message = np.sum(neighbor_features, axis=0) / len(neighbors)\n                h_next[u] = aggregated_message\n            h_mean = h_next\n\n        # Prediction for mean aggregation\n        z_components_mean = h_mean[:, 1]\n        count_mean = np.sum(z_components_mean >= 0.5)\n        f_mean = count_mean / num_nodes if num_nodes > 0 else 0\n        y_pred_mean = 1 if f_mean >= 0.5 else 0\n        \n        y_preds_mean.append(y_pred_mean)\n        results_list.extend([y_pred_mean])\n\n        # --- 3. Attention-based Aggregation ---\n        h_attn = h0.copy()\n        a = np.array([2.0, 1.0, 2.0, 1.0])\n        \n        # Apply L=2 layers\n        for _ in range(2):\n            h_next = np.zeros_like(h_attn)\n            for u in range(num_nodes):\n                # Neighborhood with self-loop\n                neighbors_prime_list = list(np.where(adj[u] == 1)[0])\n                neighbors_prime_list.append(u)\n                neighbors_prime = np.array(neighbors_prime_list, dtype=int)\n\n                # Compute unnormalized attention scores e_uv\n                e = np.zeros(len(neighbors_prime))\n                h_u = h_attn[u]\n                for j, v in enumerate(neighbors_prime):\n                    h_v = h_attn[v]\n                    concat_features = np.concatenate((h_u, h_v))\n                    e[j] = np.dot(a, concat_features)\n                \n                # Normalize using softmax\n                # Subtract max for numerical stability, although not strictly necessary here.\n                e_stable = e - np.max(e)\n                exp_e = np.exp(e_stable)\n                alpha = exp_e / np.sum(exp_e)\n                \n                # Apply attention weights\n                neighbor_features = h_attn[neighbors_prime]\n                h_next[u] = np.sum(neighbor_features * alpha[:, np.newaxis], axis=0)\n\n            h_attn = h_next\n            \n        # Prediction for attention aggregation\n        z_components_attn = h_attn[:, 1]\n        count_attn = np.sum(z_components_attn >= 0.5)\n        f_attn = count_attn / num_nodes if num_nodes > 0 else 0\n        y_pred_attn = 1 if f_attn >= 0.5 else 0\n        \n        y_preds_attn.append(y_pred_attn)\n        results_list.extend([y_pred_attn])\n\n    # --- 4. Final Metrics Calculation ---\n    y_preds_mean = np.array(y_preds_mean)\n    y_preds_attn = np.array(y_preds_attn)\n    y_trues = np.array(y_trues)\n\n    num_cases = len(test_cases)\n    \n    # Accuracies\n    acc_mean = np.sum(y_preds_mean == y_trues) / num_cases\n    acc_attn = np.sum(y_preds_attn == y_trues) / num_cases\n    \n    # B: count of cases where attention is correct AND mean is incorrect\n    attn_correct = (y_preds_attn == y_trues)\n    mean_incorrect = (y_preds_mean != y_trues)\n    B = np.sum(attn_correct & mean_incorrect)\n\n    results_list.extend([acc_mean, acc_attn, B])\n    \n    # Format the final output string\n    output_str = \",\".join([\n        str(int(x)) if i < 10 or i == 12 else f\"{x:.1f}\" for i, x in enumerate(results_list)\n    ])\n    \n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3131968"}, {"introduction": "Having seen how GNNs work and can be improved, we now turn to a more theoretical question: what are their fundamental limits? This practice ([@problem_id:3131875]) investigates the expressive power of GNNs by tasking you with distinguishing non-isomorphic graphs. You will discover that standard message-passing schemes, whose power is equivalent to the 1-Weisfeiler-Lehman (1-WL) test, can fail to differentiate structurally distinct graphs, highlighting why certain properties like triangle counts can be invisible to them and motivating the need for more powerful architectures.", "problem": "You are asked to formalize and test the expressivity of Graph Neural Networks (GNNs) by examining whether a class of Message Passing Neural Networks (MPNNs) can discriminate between graphs that differ only in triangle counts. Start from core definitions of graphs, adjacency matrices, and message passing. Then augment a graph-level representation with a motif-based pooling operator that explicitly counts triangles. Your program must implement the specified computations and produce the required outputs on a fixed test suite.\n\nGiven a simple undirected graph with adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ and initial node features $h^{(0)} \\in \\mathbb{R}^{n}$, consider the following class of Message Passing Neural Network (MPNN) update rules using sum aggregation and linear transformations with identity activation:\n$$\nh^{(t+1)} \\;=\\; \\alpha\\,h^{(t)} \\;+\\; \\beta\\,A\\,h^{(t)}, \\quad t = 0,1,\\dots,T-1,\n$$\nwhere $h^{(0)}$ is the all-ones vector, $T \\in \\mathbb{N}$ is the number of rounds, and $\\alpha,\\beta \\in \\mathbb{R}$ are fixed scalars. The graph-level readout is the sum pooling\n$$\n\\mathrm{pool}_{\\mathrm{sum}}(h^{(T)}) \\;=\\; \\sum_{v=1}^{n} h^{(T)}_v.\n$$\nDefine a motif-based pooling operator that counts the number of triangles in the graph:\n$$\n\\mathrm{pool}_{\\triangle}(A) \\;=\\; \\frac{\\mathrm{trace}\\!\\left(A^3\\right)}{6}.\n$$\nYou must examine expressivity by constructing graph pairs that differ only in triangle counts (or deliberately do not) and determining whether the standard MPNN readout $\\mathrm{pool}_{\\mathrm{sum}}$ can discriminate them, and whether adding $\\mathrm{pool}_{\\triangle}$ improves discrimination.\n\nYour program must:\n- Implement the MPNN update described above with parameters $T = 3$, $\\alpha = 0.7$, $\\beta = 0.3$.\n- Implement triangle counting via $\\mathrm{pool}_{\\triangle}(A) = \\mathrm{trace}(A^3)/6$ for simple undirected graphs without self-loops.\n- For each test case, compute two booleans:\n  1. $b_{\\mathrm{MPNN}}$: whether $\\mathrm{pool}_{\\mathrm{sum}}$ yields identical scalar values on both graphs (use exact equality within a tolerance $10^{-9}$).\n  2. $b_{\\triangle}$: whether $\\mathrm{pool}_{\\triangle}$ yields different integers on both graphs.\n- Aggregate the results for all test cases into a single list where each element is the list $[b_{\\mathrm{MPNN}}, b_{\\triangle}]$.\n\nUse the following test suite of graph pairs; each graph has $n = 6$ nodes:\n- Test $1$ (happy path): Graph $G_1$ is the cycle $C_6$; Graph $G_2$ is the disjoint union $K_3 \\cup K_3$. These are both $2$-regular on $6$ nodes but differ in triangle counts ($0$ versus $2$).\n- Test $2$ (boundary condition with zero triangles): Graph $G_1$ is $C_6$; Graph $G_2$ is the path $P_6$. Both have $0$ triangles.\n- Test $3$ (edge case with high triangle disparity): Graph $G_1$ is the complete graph $K_6$ (triangles $20$); Graph $G_2$ is the complete bipartite graph $K_{3,3}$ (triangles $0$).\n- Test $4$ (identical graphs): Graph $G_1$ is $C_6$; Graph $G_2$ is $C_6$.\n\nThe required final output format is a single line containing the list of results as a comma-separated list enclosed in square brackets, with each element itself being a two-element list of booleans. For example:\nOutput: [[True,False],[False,True],[False,False],[True,True]].\n\nYour program should produce a single line of output containing the results in exactly this bracketed list format (with Python boolean capitalization).", "solution": "The problem requires an analysis of the expressive power of a specific Message Passing Neural Network (MPNN) architecture, contrasted with a motif-based feature. We must formalize the computations and apply them to a suite of test graph pairs.\n\nFirst, we define the computational model for the MPNN. Given a simple, undirected graph on $n$ nodes with adjacency matrix $A \\in \\{0, 1\\}^{n \\times n}$, the node features $h^{(t)} \\in \\mathbb{R}^n$ are updated over $T$ rounds. The initial features are the all-ones vector, $h^{(0)} = \\mathbf{1}$. The update rule is a linear combination of the current features and the aggregated features from neighboring nodes:\n$$\nh^{(t+1)} \\;=\\; \\alpha h^{(t)} \\;+\\; \\beta A h^{(t)} \\;=\\; (\\alpha I + \\beta A) h^{(t)}\n$$\nwhere $I$ is the $n \\times n$ identity matrix, and the parameters are given as $T=3$, $\\alpha=0.7$, and $\\beta=0.3$. This is a linear recurrence relation, which can be unrolled to express the final node features $h^{(T)}$ directly in terms of the initial features $h^{(0)}$:\n$$\nh^{(T)} \\;=\\; (\\alpha I + \\beta A)^T h^{(0)}\n$$\nSince $h^{(0)}$ is the all-ones vector $\\mathbf{1}$, we have $h^{(T)} = (\\alpha I + \\beta A)^T \\mathbf{1}$. The graph-level representation is obtained via sum pooling:\n$$\n\\mathrm{pool}_{\\mathrm{sum}}(h^{(T)}) \\;=\\; \\sum_{v=1}^{n} h^{(T)}_v \\;=\\; \\mathbf{1}^T h^{(T)} \\;=\\; \\mathbf{1}^T (\\alpha I + \\beta A)^T \\mathbf{1}\n$$\nThis expression reveals that the readout is a function of the matrix powers $A^k$ for $k \\in \\{0, 1, \\dots, T\\}$, as $(\\alpha I + \\beta A)^T$ is a polynomial in $A$ of degree $T$. The term $\\mathbf{1}^T A^k \\mathbf{1}$ corresponds to the total number of walks of length $k$ in the graph. Thus, this MPNN's readout is determined by a weighted sum of the total counts of walks up to length $T=3$. A critical limitation arises for regular graphs. If a graph is $d$-regular, its adjacency matrix $A$ satisfies $A\\mathbf{1} = d\\mathbf{1}$, meaning $\\mathbf{1}$ is an eigenvector with eigenvalue $d$. In this case, the computation simplifies significantly:\n$$\nh^{(1)} = (\\alpha I + \\beta A)\\mathbf{1} = \\alpha \\mathbf{1} + \\beta(A\\mathbf{1}) = \\alpha \\mathbf{1} + \\beta d\\mathbf{1} = (\\alpha + \\beta d)\\mathbf{1}\n$$\nBy induction, $h^{(t)} = (\\alpha + \\beta d)^t \\mathbf{1}$. The final graph-level readout for a $d$-regular graph on $n$ nodes is:\n$$\n\\mathrm{pool}_{\\mathrm{sum}}(h^{(T)}) \\;=\\; \\mathbf{1}^T h^{(T)} \\;=\\; \\mathbf{1}^T ((\\alpha + \\beta d)^T \\mathbf{1}) \\;=\\; (\\alpha + \\beta d)^T (\\mathbf{1}^T \\mathbf{1}) \\;=\\; n(\\alpha + \\beta d)^T\n$$\nThis shows that for any two $d$-regular graphs on $n$ nodes, this MPNN will produce the exact same readout, rendering them indistinguishable.\n\nThe second operator, $\\mathrm{pool}_{\\triangle}(A)$, is designed to capture a specific structural motif: triangles. The formula is given by:\n$$\n\\mathrm{pool}_{\\triangle}(A) \\;=\\; \\frac{\\mathrm{trace}(A^3)}{6}\n$$\nThis formula is a well-known result from algebraic graph theory. The matrix entry $(A^3)_{ii}$ counts the number of walks of length $3$ starting and ending at node $i$. In a simple graph, any such walk must be of the form $i \\to j \\to k \\to i$. If the nodes $i, j, k$ are distinct, they form a triangle. Each triangle $\\{i, j, k\\}$ gives rise to $6$ such closed walks of length $3$ (e.g., $i \\to j \\to k \\to i$, $i \\to k \\to j \\to i$, $j \\to i \\to k \\to j$, etc.). The sum of the diagonal elements of $A^3$, $\\mathrm{trace}(A^3)$, therefore counts every triangle $6$ times. Dividing by $6$ yields the total number of unique triangles.\n\nWe now evaluate the four test cases. A tolerance of $10^{-9}$ is used for comparing floating-point MPNN readouts.\n- **Test Case 1**: $G_1$ is the cycle $C_6$; $G_2$ is the disjoint union $K_3 \\cup K_3$.\n  - $G_1$ is a $2$-regular graph with $n=6$ nodes.\n  - $G_2$ is also a $2$-regular graph with $n=6$ nodes.\n  - As both graphs are $2$-regular on $6$ nodes, their MPNN readouts must be identical:\n  $$ \\mathrm{pool}_{\\mathrm{sum}} = 6(0.7 + 0.3 \\times 2)^3 = 6(1.3)^3 = 13.182 $$\n  So, $b_{\\mathrm{MPNN}}$ is True.\n  - $G_1$ ($C_6$) has $0$ triangles. $G_2$ ($K_3 \\cup K_3$) has $2$ triangles. The counts differ. So, $b_{\\triangle}$ is True.\n  - Result: $[\\text{True}, \\text{True}]$.\n\n- **Test Case 2**: $G_1$ is $C_6$; $G_2$ is the path $P_6$.\n  - $G_1$ is $2$-regular, while $G_2$ is not (node degrees are $1, 2, 2, 2, 2, 1$). Their structural properties differ, so we expect different MPNN readouts. The readout for $C_6$ is $13.182$. Direct computation for $P_6$ yields a readout of approximately $10.734$. These are different. So, $b_{\\mathrm{MPNN}}$ is False.\n  - Neither $C_6$ nor $P_6$ contains any triangles. The triangle count is $0$ for both. The counts are identical. So, $b_{\\triangle}$ is False.\n  - Result: $[\\text{False}, \\text{False}]$.\n\n- **Test Case 3**: $G_1$ is the complete graph $K_6$; $G_2$ is the complete bipartite graph $K_{3,3}$.\n  - $G_1$ is a $5$-regular graph on $n=6$ nodes.\n  $$ \\mathrm{pool}_{\\mathrm{sum}} = 6(0.7 + 0.3 \\times 5)^3 = 6(2.2)^3 = 63.888 $$\n  - $G_2$ is a $3$-regular graph on $n=6$ nodes.\n  $$ \\mathrm{pool}_{\\mathrm{sum}} = 6(0.7 + 0.3 \\times 3)^3 = 6(1.6)^3 = 24.576 $$\n  - The readouts are clearly different. So, $b_{\\mathrm{MPNN}}$ is False.\n  - The number of triangles in $K_6$ is $\\binom{6}{3} = 20$. The graph $K_{3,3}$ is bipartite and thus has no odd cycles, meaning it has $0$ triangles. The counts differ. So, $b_{\\triangle}$ is True.\n  - Result: $[\\text{False}, \\text{True}]$.\n\n- **Test Case 4**: $G_1$ is $C_6$; $G_2$ is $C_6$.\n  - The graphs are identical. All computations will yield identical results.\n  - The MPNN readouts are identical. So, $b_{\\mathrm{MPNN}}$ is True.\n  - The triangle counts ($0$ for both) are identical. So, $b_{\\triangle}$ is False.\n  - Result: $[\\text{True}, \\text{False}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and tests the expressivity of a simple Message Passing Neural Network (MPNN)\n    by comparing its ability to discriminate graph pairs against a triangle-counting operator.\n    \"\"\"\n\n    def get_adjacency_matrices():\n        \"\"\"\n        Constructs and returns the adjacency matrices for all graphs in the test suite.\n        All graphs have n=6 nodes.\n        \"\"\"\n        n = 6\n        \n        # C6: Cycle graph on 6 nodes\n        A_c6 = np.zeros((n, n), dtype=int)\n        for i in range(n):\n            A_c6[i, (i + 1) % n] = 1\n            A_c6[(i + 1) % n, i] = 1\n            \n        # K3_U_K3: Disjoint union of two K3 graphs\n        A_k3_k3 = np.zeros((n, n), dtype=int)\n        A_k3_k3[0:3, 0:3] = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n        A_k3_k3[3:6, 3:6] = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n        \n        # P6: Path graph on 6 nodes\n        A_p6 = np.zeros((n, n), dtype=int)\n        for i in range(n - 1):\n            A_p6[i, i + 1] = 1\n            A_p6[i + 1, i] = 1\n            \n        # K6: Complete graph on 6 nodes\n        A_k6 = np.ones((n, n), dtype=int) - np.eye(n, dtype=int)\n        \n        # K3,3: Complete bipartite graph\n        A_k33 = np.zeros((n, n), dtype=int)\n        A_k33[0:3, 3:6] = 1\n        A_k33[3:6, 0:3] = 1\n        \n        return {\n            \"C6\": A_c6,\n            \"K3_U_K3\": A_k3_k3,\n            \"P6\": A_p6,\n            \"K6\": A_k6,\n            \"K3,3\": A_k33,\n        }\n\n    def compute_mpnn_readout(A, T, alpha, beta):\n        \"\"\"\n        Implements the MPNN update and sum pooling readout.\n        \"\"\"\n        n = A.shape[0]\n        h = np.ones(n)\n        for _ in range(T):\n            h = alpha * h + beta * (A @ h)\n        return np.sum(h)\n\n    def count_triangles(A):\n        \"\"\"\n        Counts triangles in a graph using the trace(A^3)/6 formula.\n        \"\"\"\n        # For simple undirected graphs, trace(A^3) is always divisible by 6.\n        # Integer division is safe and avoids floating point issues.\n        return np.trace(np.linalg.matrix_power(A, 3)) // 6\n\n    def analyze_pair(A1, A2, T, alpha, beta, tol):\n        \"\"\"\n        Analyzes a pair of graphs and computes the two required booleans.\n        \"\"\"\n        # 1. MPNN discriminability\n        readout1 = compute_mpnn_readout(A1, T, alpha, beta)\n        readout2 = compute_mpnn_readout(A2, T, alpha, beta)\n        b_mpnn = np.abs(readout1 - readout2) <= tol\n        \n        # 2. Triangle count discriminability\n        triangles1 = count_triangles(A1)\n        triangles2 = count_triangles(A2)\n        b_triangle = triangles1 != triangles2\n        \n        return [b_mpnn, b_triangle]\n\n    # Problem parameters\n    T = 3\n    alpha = 0.7\n    beta = 0.3\n    tolerance = 1e-9\n\n    # Get graph data\n    adj_matrices = get_adjacency_matrices()\n    \n    # Define the test suite\n    test_cases = [\n        (adj_matrices[\"C6\"], adj_matrices[\"K3_U_K3\"]),  # Test 1\n        (adj_matrices[\"C6\"], adj_matrices[\"P6\"]),       # Test 2\n        (adj_matrices[\"K6\"], adj_matrices[\"K3,3\"]),     # Test 3\n        (adj_matrices[\"C6\"], adj_matrices[\"C6\"]),       # Test 4\n    ]\n\n    results = []\n    for A1, A2 in test_cases:\n        result = analyze_pair(A1, A2, T, alpha, beta, tolerance)\n        results.append(result)\n\n    # Format the output string precisely as required\n    inner_parts = [f\"[{b1},{b2}]\" for b1, b2 in results]\n    output_str = f\"[{','.join(inner_parts)}]\"\n\n    print(output_str)\n\nsolve()\n```", "id": "3131875"}]}