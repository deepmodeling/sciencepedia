{"hands_on_practices": [{"introduction": "Quantization is a cornerstone of model compression, but its correct implementation is filled with important details. This exercise focuses on the distinction between symmetric and asymmetric quantization and the crucial role of the \"zero-point\" parameter, $z$. You will derive the mathematical bias introduced by a naive integer-only implementation that ignores the zero-point, and quantify how this error can corrupt the student model's output, thereby undermining the alignment sought through knowledge distillation. This practice is essential for understanding the low-level mechanics of deploying models with integer arithmetic. [@problem_id:3152827]", "problem": "Consider a two-class teacher network and a student network trained using Knowledge Distillation (KD). The teacher produces a logit difference $y_{\\text{T}}$ for an input vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ via a linear map $y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x}$, where $\\mathbf{w} \\in \\mathbb{R}^{n}$. For KD with temperature $\\tau$, the softened class-$1$ probability is defined by the well-tested formula $p(y;\\tau) = \\frac{1}{1+\\exp\\!\\left(-\\frac{y}{\\tau}\\right)}$. The student performs integer-only inference with uniform quantization on both weights and activations, using the standard definitions: for any real value $r$, the quantized integer is $q = \\operatorname{round}\\!\\left(\\frac{r}{s}\\right) + z$, with dequantization $r \\approx s\\,(q - z)$, where $s > 0$ is the scale and $z \\in \\mathbb{Z}$ is the zero-point. The correct integer dot-product for approximating $\\mathbf{w}^{\\top}\\mathbf{x}$ is $y_{\\text{proper}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} \\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right)$, where $q_{w,i}$ and $q_{x,i}$ are the quantized weights and activations. However, a naive implementation that omits zero-point compensation computes $y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i}$.\n\nStarting from these core definitions, derive an analytic expression for the additive bias $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$ in terms of $s_{w}$, $s_{x}$, $z_{w}$, $z_{x}$, $\\sum_{i=1}^{n} q_{w,i}$, $\\sum_{i=1}^{n} q_{x,i}$, and $n$. Then, for the specific instance below, compute the KD alignment error as the absolute difference between the teacher’s softened probability and the student’s softened probability under the naive asymmetric quantization. Concretely:\n\n- Let $n = 4$, $\\mathbf{w} = \\left[0.12,\\,-0.08,\\,0.05,\\,0.20\\right]$, and $\\mathbf{x} = \\left[0.50,\\,-0.40,\\,0.10,\\,0.30\\right]$, so that $y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x}$.\n- Use scales $s_{w} = 0.01$ and $s_{x} = 0.01$.\n- For the symmetric scheme, take $z_{w} = 0$ and $z_{x} = 0$.\n- For the asymmetric scheme, take $z_{w} = 124$ and $z_{x} = 128$.\n- Quantize by $q_{w,i} = \\operatorname{round}\\!\\left(\\frac{w_{i}}{s_{w}}\\right) + z_{w}$ and $q_{x,i} = \\operatorname{round}\\!\\left(\\frac{x_{i}}{s_{x}}\\right) + z_{x}$.\n- Use KD temperature $\\tau = 2$.\n\nAssume the teacher’s softened probability uses $y_{\\text{T}}$ and the student’s softened probability under the naive asymmetric quantization uses $y_{\\text{S}} = y_{\\text{proper}} + \\Delta$, with $y_{\\text{proper}}$ approximating $y_{\\text{T}}$ for the given scales and inputs. Compute the absolute KD alignment error $|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,|$ for the asymmetric scheme and express your final answer as a single real number. Round your final answer to four significant figures.", "solution": "## Problem Validation ##\n\n### Step 1: Extract Givens ###\n- **Teacher Network**: Produces a logit difference $y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x}$ for an input $\\mathbf{x} \\in \\mathbb{R}^{n}$ and weight vector $\\mathbf{w} \\in \\mathbb{R}^{n}$.\n- **Knowledge Distillation (KD)**: Uses temperature $\\tau$. Softened class-$1$ probability is $p(y;\\tau) = \\frac{1}{1+\\exp\\!\\left(-\\frac{y}{\\tau}\\right)}$.\n- **Uniform Quantization**: For a real value $r$, the quantized integer is $q = \\operatorname{round}\\!\\left(\\frac{r}{s}\\right) + z$. The dequantized value is $r \\approx s\\,(q - z)$. $s > 0$ is the scale, $z \\in \\mathbb{Z}$ is the zero-point.\n- **Proper Integer Dot-Product**: $y_{\\text{proper}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} \\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right)$.\n- **Naive Integer Dot-Product**: $y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i}$.\n- **Additive Bias**: $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$.\n- **Student Logit Difference**: $y_{\\text{S}} = y_{\\text{proper}} + \\Delta$.\n- **Problem Instance Parameters**:\n    - $n = 4$\n    - $\\mathbf{w} = \\left[0.12,\\,-0.08,\\,0.05,\\,0.20\\right]$\n    - $\\mathbf{x} = \\left[0.50,\\,-0.40,\\,0.10,\\,0.30\\right]$\n    - $s_{w} = 0.01$\n    - $s_{x} = 0.01$\n- **Quantization Schemes**:\n    - Symmetric: $z_{w} = 0$, $z_{x} = 0$. (This is contextual information not used in the final calculation).\n    - Asymmetric: $z_{w} = 124$, $z_{x} = 128$.\n- **Quantization Formulas**:\n    - $q_{w,i} = \\operatorname{round}\\!\\left(\\frac{w_{i}}{s_{w}}\\right) + z_{w}$\n    - $q_{x,i} = \\operatorname{round}\\!\\left(\\frac{x_{i}}{s_{x}}\\right) + z_{x}$\n- **KD Temperature**: $\\tau = 2$.\n- **Objective**:\n    1. Derive an analytic expression for $\\Delta$ in terms of $s_{w}$, $s_{x}$, $z_{w}$, $z_{x}$, $\\sum_{i=1}^{n} q_{w,i}$, $\\sum_{i=1}^{n} q_{x,i}$, and $n$.\n    2. Compute the KD alignment error $|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,|$ for the asymmetric scheme.\n\n### Step 2: Validate Using Extracted Givens ###\nThe problem is scientifically grounded, situated within the field of deep learning, specifically model compression and quantization. All definitions provided for quantization, knowledge distillation, and dot-product implementations are standard and correct within this context. The problem is well-posed, providing all necessary formulas, parameters, and a clear objective. It is also objective, stated in precise mathematical language without subjective claims. There are no contradictions, missing information, or violations of scientific principles. The problem is a straightforward application of the given definitions and requires rigorous calculation, not speculation.\n\n### Step 3: Verdict and Action ###\nThe problem is valid. A complete solution will be provided.\n\n## Solution Derivation ##\n\nThe first part of the problem requires deriving an analytic expression for the additive bias $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$. We start with the given definitions for $y_{\\text{proper}}$ and $y_{\\text{naive}}$.\n\nThe expression for the proper integer dot-product is:\n$$y_{\\text{proper}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} \\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right)$$\nWe expand the term inside the summation:\n$$\\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right) = q_{w,i}q_{x,i} - q_{w,i}z_{x} - z_{w}q_{x,i} + z_{w}z_{x}$$\nSubstituting this back into the expression for $y_{\\text{proper}}$ and distributing the summation:\n$$y_{\\text{proper}} = s_{w}\\,s_{x} \\left( \\sum_{i=1}^{n} q_{w,i}q_{x,i} - \\sum_{i=1}^{n} q_{w,i}z_{x} - \\sum_{i=1}^{n} z_{w}q_{x,i} + \\sum_{i=1}^{n} z_{w}z_{x} \\right)$$\nSince $z_{w}$ and $z_{x}$ are constants with respect to the index $i$, they can be factored out of the summations:\n$$y_{\\text{proper}} = s_{w}\\,s_{x} \\left( \\sum_{i=1}^{n} q_{w,i}q_{x,i} - z_{x}\\sum_{i=1}^{n} q_{w,i} - z_{w}\\sum_{i=1}^{n} q_{x,i} + n z_{w}z_{x} \\right)$$\nThe definition of the naive dot-product is:\n$$y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i}$$\nThe additive bias $\\Delta$ is defined as $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$. Substituting the expressions:\n$$\\Delta = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i} - s_{w}\\,s_{x} \\left( \\sum_{i=1}^{n} q_{w,i}q_{x,i} - z_{x}\\sum_{i=1}^{n} q_{w,i} - z_{w}\\sum_{i=1}^{n} q_{x,i} + n z_{w}z_{x} \\right)$$\nFactoring out $s_{w}s_{x}$ and simplifying:\n$$\\Delta = s_{w}\\,s_{x} \\left[ \\left(\\sum_{i=1}^{n} q_{w,i}q_{x,i}\\right) - \\left(\\sum_{i=1}^{n} q_{w,i}q_{x,i} - z_{x}\\sum_{i=1}^{n} q_{w,i} - z_{w}\\sum_{i=1}^{n} q_{x,i} + n z_{w}z_{x}\\right) \\right]$$\n$$\\Delta = s_{w}\\,s_{x} \\left( z_{x}\\sum_{i=1}^{n} q_{w,i} + z_{w}\\sum_{i=1}^{n} q_{x,i} - n z_{w}z_{x} \\right)$$\nThis is the required analytic expression for the additive bias $\\Delta$.\n\nThe second part of the problem requires computing the KD alignment error $|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,|$ for the specified asymmetric scheme.\n\nFirst, calculate the teacher's logit difference, $y_{\\text{T}}$:\n$$y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x} = \\sum_{i=1}^{n} w_i x_i$$\nUsing the given vectors $\\mathbf{w} = \\left[0.12,\\,-0.08,\\,0.05,\\,0.20\\right]$ and $\\mathbf{x} = \\left[0.50,\\,-0.40,\\,0.10,\\,0.30\\right]$:\n$$y_{\\text{T}} = (0.12)(0.50) + (-0.08)(-0.40) + (0.05)(0.10) + (0.20)(0.30)$$\n$$y_{\\text{T}} = 0.060 + 0.032 + 0.005 + 0.060 = 0.157$$\n\nNext, we quantize the weight and input vectors using the asymmetric scheme parameters: $s_w = 0.01$, $z_w = 124$ and $s_x = 0.01$, $z_x = 128$.\nFor the weights $\\mathbf{w}$:\n$q_{w,1} = \\operatorname{round}\\!\\left(\\frac{0.12}{0.01}\\right) + 124 = \\operatorname{round}(12) + 124 = 12 + 124 = 136$\n$q_{w,2} = \\operatorname{round}\\!\\left(\\frac{-0.08}{0.01}\\right) + 124 = \\operatorname{round}(-8) + 124 = -8 + 124 = 116$\n$q_{w,3} = \\operatorname{round}\\!\\left(\\frac{0.05}{0.01}\\right) + 124 = \\operatorname{round}(5) + 124 = 5 + 124 = 129$\n$q_{w,4} = \\operatorname{round}\\!\\left(\\frac{0.20}{0.01}\\right) + 124 = \\operatorname{round}(20) + 124 = 20 + 124 = 144$\nThe quantized weight vector is $\\mathbf{q}_w = \\left[136, 116, 129, 144\\right]$.\n\nFor the inputs $\\mathbf{x}$:\n$q_{x,1} = \\operatorname{round}\\!\\left(\\frac{0.50}{0.01}\\right) + 128 = \\operatorname{round}(50) + 128 = 50 + 128 = 178$\n$q_{x,2} = \\operatorname{round}\\!\\left(\\frac{-0.40}{0.01}\\right) + 128 = \\operatorname{round}(-40) + 128 = -40 + 128 = 88$\n$q_{x,3} = \\operatorname{round}\\!\\left(\\frac{0.10}{0.01}\\right) + 128 = \\operatorname{round}(10) + 128 = 10 + 128 = 138$\n$q_{x,4} = \\operatorname{round}\\!\\left(\\frac{0.30}{0.01}\\right) + 128 = \\operatorname{round}(30) + 128 = 30 + 128 = 158$\nThe quantized input vector is $\\mathbf{q}_x = \\left[178, 88, 138, 158\\right]$.\n\nThe student's logit difference is given as $y_{\\text{S}} = y_{\\text{proper}} + \\Delta$. From the definition $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$, this implies $y_{\\text{S}} = y_{\\text{naive}}$. We compute $y_{\\text{naive}}$:\n$$y_{\\text{S}} = y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}q_{x,i}$$\n$$y_{\\text{S}} = (0.01)(0.01) \\left[ (136)(178) + (116)(88) + (129)(138) + (144)(158) \\right]$$\n$$y_{\\text{S}} = 0.0001 \\left[ 24208 + 10208 + 17792 + 22752 \\right]$$\n$$y_{\\text{S}} = 0.0001 \\left[ 74960 \\right] = 7.496$$\n\nNow we compute the softened probabilities using $\\tau = 2$.\nFor the teacher:\n$$p(y_{\\text{T}}; \\tau) = \\frac{1}{1 + \\exp\\left(-\\frac{y_{\\text{T}}}{\\tau}\\right)} = \\frac{1}{1 + \\exp\\left(-\\frac{0.157}{2}\\right)} = \\frac{1}{1 + \\exp\\left(-0.0785\\right)}$$\n$$p(y_{\\text{T}}; 2) \\approx \\frac{1}{1 + 0.924490} \\approx \\frac{1}{1.924490} \\approx 0.519616$$\n\nFor the student (with naive quantization):\n$$p(y_{\\text{S}}; \\tau) = \\frac{1}{1 + \\exp\\left(-\\frac{y_{\\text{S}}}{\\tau}\\right)} = \\frac{1}{1 + \\exp\\left(-\\frac{7.496}{2}\\right)} = \\frac{1}{1 + \\exp\\left(-3.748\\right)}$$\n$$p(y_{\\text{S}}; 2) \\approx \\frac{1}{1 + 0.023565} \\approx \\frac{1}{1.023565} \\approx 0.976974$$\n\nFinally, we compute the absolute KD alignment error:\n$$|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,| \\approx |\\,0.976974 - 0.519616\\,| = 0.457358$$\nRounding the final result to four significant figures gives $0.4574$.", "answer": "$$\\boxed{0.4574}$$", "id": "3152827"}, {"introduction": "While pruning can dramatically reduce a model's theoretical operation count (FLOPs), these savings do not always translate into real-world speedups. This hands-on coding problem explores why the *pattern* of sparsity is often more critical than its overall percentage. You will use a hypothetical hardware model to compare the performance of unstructured pruning against hardware-friendly structured pruning techniques like block and row pruning. This exercise provides a crucial, practical lesson: model efficiency is not just about reducing arithmetic, but also about maintaining regularity that modern processors can exploit. [@problem_id:3152881]", "problem": "You are given a hypothetical execution model for inference in a single fully connected neural network layer with weight matrix $W \\in \\mathbb{R}^{m \\times n}$ and input vector $x \\in \\mathbb{R}^{n}$. The model aims to compare pruning-induced sparsity patterns with hardware-friendly structures and measure actual speed gains versus theoretical sparsity-derived gains. The fundamental base you must use is the operation counting principle for matrix-vector multiplication: a dense computation of $y = W x$ performs approximately $m n$ multiply-accumulate steps, each considered as a constant-cost unit in this model. Sparsity reduces the number of arithmetic operations, but hardware costs also include non-arithmetic overhead that depends on the pattern structure.\n\nYou must implement a program that, for each specified test case, constructs a binary nonzero mask for $W$ based on the pruning pattern, computes theoretical and actual speedups, and aggregates the realized efficiency ratios. The theoretical speedup assumes that arithmetic work scales perfectly with the number of nonzero entries and that overhead is negligible. The actual speed under the given hardware model includes overheads and depends on the structure of sparsity.\n\nDefinitions and execution model:\n\n- Floating Point Operations (FLOPs) definition: For dense matrix-vector multiplication, the arithmetic work is proportional to $m n$. In this model, its time is $T_{\\text{dense}} = \\alpha \\cdot m n$, where $\\alpha$ is a constant that aggregates per-operation cost and memory effects for dense compute.\n\n- Sparse compute model: For a weight matrix with $s$ nonzero entries, actual sparse time is modeled as\n$$\nT_{\\text{sparse}} = \\beta \\cdot s + \\gamma \\cdot B,\n$$\nwhere $\\beta$ is the cost per nonzero arithmetic operation in sparse mode, $\\gamma$ is the overhead per processed structural unit, and $B$ is the number of processed structural units determined by the sparsity pattern:\n    - Unstructured pruning: Each nonzero is a unit; $B = s$.\n    - Block pruning with block size $b \\times b$: Each kept block is a unit; $B$ equals the number of kept blocks (assume blocks are either fully kept or fully pruned).\n    - Row pruning: Each kept row is a unit; $B$ equals the number of kept rows.\n    - Column pruning: Each kept column is a unit; $B$ equals the number of kept columns.\n\n- Ideal theoretical time ignoring overheads is\n$$\nT_{\\text{ideal}} = \\beta \\cdot s,\n$$\nwhich yields a theoretical speedup from sparsity alone.\n\nSparsity patterns and mask construction rules:\n\n- Unstructured pruning with target sparsity $p$: Select exactly $\\lfloor (1 - p) \\cdot m n \\rfloor$ entries uniformly at random to keep as nonzero; all others are zero.\n\n- Block pruning with target sparsity $p$ and block size $b \\times b$: Partition $W$ into $\\lfloor m / b \\rfloor \\times \\lfloor n / b \\rfloor$ non-overlapping blocks of size $b \\times b$ (ignore any remainder if $m$ or $n$ are not multiples of $b$). Keep exactly $\\lfloor (1 - p) \\cdot \\lfloor m / b \\rfloor \\cdot \\lfloor n / b \\rfloor \\rfloor$ blocks chosen uniformly at random; all entries within kept blocks are nonzero; all entries within pruned blocks are zero.\n\n- Row pruning with target sparsity $p$: Keep exactly $\\lfloor (1 - p) \\cdot m \\rfloor$ rows chosen uniformly at random, with all entries in kept rows nonzero and all entries in pruned rows zero.\n\n- Column pruning with target sparsity $p$: Keep exactly $\\lfloor (1 - p) \\cdot n \\rfloor$ columns chosen uniformly at random, with all entries in kept columns nonzero and all entries in pruned columns zero.\n\nSpeedup definitions to compute:\n\n- Theoretical speedup:\n$$\n\\text{speedup}_{\\text{theory}} = \\frac{T_{\\text{dense}}}{T_{\\text{ideal}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s}.\n$$\n\n- Actual speedup under the hardware model:\n$$\n\\text{speedup}_{\\text{actual}} = \\frac{T_{\\text{dense}}}{T_{\\text{sparse}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}.\n$$\n\n- Realized efficiency ratio comparing actual speed gains to theoretical sparsity gains:\n$$\n\\rho = \\frac{\\text{speedup}_{\\text{actual}}}{\\text{speedup}_{\\text{theory}}}.\n$$\n\nConstants for the execution model (use these exact values):\n- $\\alpha = 1.0$,\n- $\\beta = 1.2$,\n- $\\gamma = 5.0$.\n\nRandomness requirement: Use a fixed random seed of $42$ to ensure reproducibility of mask construction.\n\nTest suite (five cases that probe different facets of sparsity and structure):\n\n1. $m = 512$, $n = 512$, $p = 0.9$, unstructured pruning, $b$ not applicable.\n2. $m = 512$, $n = 512$, $p = 0.9$, block pruning with $b = 4$.\n3. $m = 512$, $n = 512$, $p = 0.9$, row pruning, $b$ not applicable.\n4. $m = 1024$, $n = 256$, $p = 0.5$, column pruning, $b$ not applicable.\n5. $m = 256$, $n = 256$, $p = 0.5$, unstructured pruning, $b$ not applicable.\n\nYour program must:\n- Construct the nonzero mask for each case following the rules above.\n- Compute $s$, $B$, $T_{\\text{dense}}$, $T_{\\text{ideal}}$, $T_{\\text{sparse}}$, $\\text{speedup}_{\\text{theory}}$, $\\text{speedup}_{\\text{actual}}$, and $\\rho$ for each case.\n- Produce a single line of output containing the realized efficiency ratios $\\rho$ for all test cases as a comma-separated list enclosed in square brackets, for example, $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$. No additional text should be printed.\n\nAll mathematical entities and numbers in your derivations must obey the provided definitions and the specified execution model. No physical units are involved; all times and speedups are dimensionless quantities. Ensure scientific realism by strictly following the operation counting principle and pattern-dependent overhead model. The final outputs must be real-valued floats.", "solution": "The problem statement has been validated and is determined to be sound. It is scientifically grounded in the principles of computational cost modeling for sparse matrix operations in neural networks, well-posed with all necessary definitions and constants, and objectively formulated. We may proceed with the solution.\n\nThe primary objective is to calculate the realized efficiency ratio, $\\rho$, for five different scenarios of weight matrix pruning. This ratio compares the actual speedup achieved on hypothetical hardware to the theoretical speedup expected from merely reducing the number of non-zero parameters. The formula for $\\rho$ is given by:\n$$\n\\rho = \\frac{\\text{speedup}_{\\text{actual}}}{\\text{speedup}_{\\text{theory}}}\n$$\nSubstituting the definitions for speedup provides a more direct formula for computation.\n$$\n\\text{speedup}_{\\text{actual}} = \\frac{T_{\\text{dense}}}{T_{\\text{sparse}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}\n$$\n$$\n\\text{speedup}_{\\text{theory}} = \\frac{T_{\\text{dense}}}{T_{\\text{ideal}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s}\n$$\nWhere $s$ is the number of non-zero entries, $B$ is the number of processed structural units, $m$ and $n$ are matrix dimensions, and $\\alpha$, $\\beta$, $\\gamma$ are cost constants.\n\nThe ratio $\\rho$ can be simplified as:\n$$\n\\rho = \\frac{\\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}}{\\frac{\\alpha \\cdot m n}{\\beta \\cdot s}} = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1}{1 + \\frac{\\gamma \\cdot B}{\\beta \\cdot s}}\n$$\nThis simplified form reveals that the realized efficiency is determined by the ratio of the structural overhead cost ($\\gamma \\cdot B$) to the arithmetic cost ($\\beta \\cdot s$). We will use the provided constants $\\beta = 1.2$ and $\\gamma = 5.0$. The problem specifies rules for determining $s$ and $B$ for each pruning type, which are deterministic calculations based on the given parameters. The requirement for a random seed is noted, but since the quantities $s$ and $B$ are defined by deterministic formulas based on counts, the specific random choices do not alter the final value of $\\rho$.\n\nWe will now compute $\\rho$ for each test case.\n\n**Case 1: Unstructured Pruning**\n- Parameters: $m = 512$, $n = 512$, target sparsity $p = 0.9$.\n- The total number of entries in the matrix is $m \\cdot n = 512 \\cdot 512 = 262144$.\n- The number of non-zero entries to keep is $s = \\lfloor (1 - p) \\cdot m n \\rfloor = \\lfloor (1 - 0.9) \\cdot 262144 \\rfloor = \\lfloor 26214.4 \\rfloor = 26214$.\n- For unstructured pruning, each non-zero entry is a structural unit, so $B = s = 26214$.\n- The efficiency ratio $\\rho_1$ is:\n$$\n\\rho_1 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot s} = \\frac{\\beta}{\\beta + \\gamma} = \\frac{1.2}{1.2 + 5.0} = \\frac{1.2}{6.2} \\approx 0.193548\n$$\n\n**Case 2: Block Pruning**\n- Parameters: $m = 512$, $n = 512$, $p = 0.9$, block size $b=4$.\n- The matrix is partitioned into non-overlapping blocks of size $4 \\times 4$.\n- Number of blocks: $(\\lfloor 512/4 \\rfloor) \\times (\\lfloor 512/4 \\rfloor) = 128 \\times 128 = 16384$.\n- The number of blocks to keep is the number of structural units: $B = \\lfloor (1 - p) \\cdot 16384 \\rfloor = \\lfloor 0.1 \\cdot 16384 \\rfloor = \\lfloor 1638.4 \\rfloor = 1638$.\n- The total number of non-zero entries is $s = B \\cdot b^2 = 1638 \\cdot 4^2 = 1638 \\cdot 16 = 26208$.\n- The efficiency ratio $\\rho_2$ is:\n$$\n\\rho_2 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 26208}{1.2 \\cdot 26208 + 5.0 \\cdot 1638} = \\frac{31449.6}{31449.6 + 8190} = \\frac{31449.6}{39639.6} \\approx 0.793388\n$$\n\n**Case 3: Row Pruning**\n- Parameters: $m = 512$, $n = 512$, $p = 0.9$.\n- The number of rows to keep is the number of structural units: $B = \\lfloor (1 - p) \\cdot m \\rfloor = \\lfloor (1 - 0.9) \\cdot 512 \\rfloor = \\lfloor 51.2 \\rfloor = 51$.\n- The total number of non-zero entries is $s = B \\cdot n = 51 \\cdot 512 = 26112$.\n- The efficiency ratio $\\rho_3$ is:\n$$\n\\rho_3 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 26112}{1.2 \\cdot 26112 + 5.0 \\cdot 51} = \\frac{31334.4}{31334.4 + 255} = \\frac{31334.4}{31589.4} \\approx 0.991928\n$$\n\n**Case 4: Column Pruning**\n- Parameters: $m = 1024$, $n = 256$, $p = 0.5$.\n- The number of columns to keep is the number of structural units: $B = \\lfloor (1 - p) \\cdot n \\rfloor = \\lfloor (1 - 0.5) \\cdot 256 \\rfloor = \\lfloor 128 \\rfloor = 128$.\n- The total number of non-zero entries is $s = B \\cdot m = 128 \\cdot 1024 = 131072$.\n- The efficiency ratio $\\rho_4$ is:\n$$\n\\rho_4 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 131072}{1.2 \\cdot 131072 + 5.0 \\cdot 128} = \\frac{157286.4}{157286.4 + 640} = \\frac{157286.4}{157926.4} \\approx 0.995947\n$$\n\n**Case 5: Unstructured Pruning**\n- Parameters: $m = 256$, $n = 256$, $p = 0.5$.\n- The total number of entries is $m \\cdot n = 256 \\cdot 256 = 65536$.\n- The number of non-zero entries to keep is $s = \\lfloor (1 - p) \\cdot m n \\rfloor = \\lfloor (1 - 0.5) \\cdot 65536 \\rfloor = \\lfloor 32768 \\rfloor = 32768$.\n- For unstructured pruning, $B = s = 32768$.\n- The efficiency ratio $\\rho_5$ is:\n$$\n\\rho_5 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot s} = \\frac{\\beta}{\\beta + \\gamma} = \\frac{1.2}{1.2 + 5.0} = \\frac{1.2}{6.2} \\approx 0.193548\n$$\nThese calculations demonstrate that structured pruning methods (block, row, column) are significantly more efficient as they greatly reduce the overhead term $\\gamma B$ relative to the arithmetic term $\\beta s$, resulting in $\\rho$ values close to $1$. Unstructured pruning is highly inefficient in this model because the overhead scales with the number of non-zero elements, making the overhead cost dominant.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the realized efficiency ratio for different neural network pruning strategies\n    based on a given hardware execution model.\n    \"\"\"\n    # Constants for the execution model from the problem statement\n    beta = 1.2\n    gamma = 5.0\n\n    # Test suite defined in the problem\n    test_cases = [\n        {'m': 512, 'n': 512, 'p': 0.9, 'type': 'unstructured', 'b': None},\n        {'m': 512, 'n': 512, 'p': 0.9, 'type': 'block', 'b': 4},\n        {'m': 512, 'n': 512, 'p': 0.9, 'type': 'row', 'b': None},\n        {'m': 1024, 'n': 256, 'p': 0.5, 'type': 'column', 'b': None},\n        {'m': 256, 'n': 256, 'p': 0.5, 'type': 'unstructured', 'b': None},\n    ]\n\n    results = []\n\n    # Although the problem mentions a random seed, the quantities s and B are\n    # determined by deterministic formulas. No random sampling is needed to compute them.\n    # rng = np.random.default_rng(42)\n\n    for case in test_cases:\n        m, n, p, pruning_type, b = case['m'], case['n'], case['p'], case['type'], case['b']\n        s = 0  # Number of non-zero entries\n        B = 0  # Number of processed structural units\n\n        if pruning_type == 'unstructured':\n            # For unstructured pruning, each non-zero element is a structural unit.\n            s = int(np.floor((1 - p) * m * n))\n            B = s\n        elif pruning_type == 'block':\n            # For block pruning, each kept block is a structural unit.\n            num_blocks_m = m // b\n            num_blocks_n = n // b\n            total_blocks = num_blocks_m * num_blocks_n\n            B = int(np.floor((1 - p) * total_blocks))\n            s = B * (b**2)\n        elif pruning_type == 'row':\n            # For row pruning, each kept row is a structural unit.\n            B = int(np.floor((1 - p) * m))\n            s = B * n\n        elif pruning_type == 'column':\n            # For column pruning, each kept column is a structural unit.\n            B = int(np.floor((1 - p) * n))\n            s = B * m\n            \n        # The realized efficiency ratio rho is defined as:\n        # rho = speedup_actual / speedup_theory\n        # which simplifies to rho = (beta * s) / (beta * s + gamma * B)\n        \n        # Handle the case where s could be zero to avoid division by zero.\n        # Based on the test cases, s will always be positive.\n        if (beta * s + gamma * B) == 0:\n             # This case happens if s=0 and B=0, meaning the matrix is empty, \n             # pruned completely. The concept of speedup is not well-defined.\n             # We can define rho as 1.0, as there is no overhead.\n            rho = 1.0\n        else:\n            rho = (beta * s) / (beta * s + gamma * B)\n        \n        results.append(rho)\n\n    # The final output must be a single line containing a comma-separated list\n    # of the realized efficiency ratios.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3152881"}, {"introduction": "Effective model compression rarely relies on a single technique; instead, methods like pruning, knowledge distillation, and quantization are combined to maximize efficiency. This final exercise provides a holistic view by asking you to synthesize these concepts and evaluate their total impact on a critical hardware metric: energy consumption. Using a linear energy model based on compute operations and memory access, you will calculate the overall energy savings of moving from a large teacher model to a highly compressed student model. This practice connects the individual techniques you've studied to the ultimate goal of deploying lean, efficient models in resource-constrained environments. [@problem_id:3152867]", "problem": "A deep neural network for inference is executed on hardware where the energy per inference can be approximated by a linear model of compute and memory traffic. Let energy per inference be modeled as $E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem}$, where $\\mathrm{FLOPs}$ is the number of arithmetic operations (floating-point operations (FLOPs) treated here as general arithmetic operations per inference), $\\mathrm{Mem}$ is the total number of accessed bytes per inference, $a$ is the energy per operation, and $b$ is the energy per accessed byte. Consider a baseline \"teacher\" model using $32$-bit floating point representation and a compressed \"student\" model obtained via knowledge distillation and pruning, then quantized to $8$-bit integers for inference (int8). Use the following scientifically plausible and consistent assumptions grounded in well-tested observations:\n- The teacher model performs $3.2 \\times 10^{9}$ operations per inference and accesses $1.25 \\times 10^{8}$ bytes per inference at $32$-bit precision.\n- The energy per operation at $32$-bit is $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ joules per operation, and the energy per accessed byte is $b = 2.5 \\times 10^{-10}$ joules per byte.\n- The student model, as a result of knowledge distillation and pruning, reduces operation count to $40\\%$ of the teacher and reduces memory traffic (in bytes, at the same precision) to $30\\%$ of the teacher.\n- Quantization to $8$-bit (int8) reduces the per operation energy to $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ joules per operation. Memory energy per byte $b$ remains the same, but total accessed bytes scale with representation bit-width, so going from $32$-bit to $8$-bit reduces accessed bytes by a factor of $4$ for both weights and activations.\nUsing these base definitions and assumptions, derive the fractional energy savings of moving from the $32$-bit teacher inference to the int8 student inference, defined as\n$$ S \\equiv \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}}. $$\nCompute $S$ and express your final answer as a unitless decimal. Round your answer to four significant figures.", "solution": "The problem statement is evaluated for validity according to the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Energy model per inference: $E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem}$\n- $a$: energy per operation (Joules/operation)\n- $b$: energy per accessed byte (Joules/byte)\n- Teacher model parameters (32-bit floating point, fp32):\n  - $\\mathrm{FLOPs}_{\\mathrm{teacher}} = 3.2 \\times 10^9$\n  - $\\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 1.25 \\times 10^8$ bytes\n- Energy coefficients for fp32:\n  - $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ J/op\n  - $b = 2.5 \\times 10^{-10}$ J/byte\n- Student model derivation (structural changes):\n  - $\\mathrm{FLOPs}_{\\mathrm{student}} = 0.40 \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}}$\n  - Memory traffic for a hypothetical 32-bit student model is $0.30$ of the teacher's memory traffic in bytes.\n- Student model derivation (quantization to 8-bit integer, int8):\n  - Energy per operation: $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ J/op\n  - Memory energy per byte $b$ is unchanged.\n  - Total accessed bytes are reduced by a factor of $4$ compared to a 32-bit representation.\n- Target quantity: Fractional energy savings, $S \\equiv \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}}$\n- Final answer requirements: Unitless decimal rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed for scientific grounding, well-posedness, and objectivity.\n- **Scientifically Grounded**: The problem is grounded in the established field of deep learning model compression and hardware efficiency. The linear model for energy consumption is a standard and valid first-order approximation. The specified values for operations, memory, and energy coefficients are scientifically plausible for modern computing hardware. The described effects of pruning and quantization are standard techniques and their impact on FLOPs, memory, and energy is modeled correctly.\n- **Well-Posed**: The problem provides all necessary data and definitions to calculate a unique numerical solution. The relationships between the teacher and student models are explicitly defined and unambiguous.\n- **Objective**: The problem is stated in precise, quantitative terms, free from subjective or speculative language.\n\n**Step 3: Verdict and Action**\nThe problem is **valid** as it is scientifically sound, well-posed, objective, and internally consistent. A rigorous solution can be derived.\n\n### Solution Derivation\n\nThe analysis proceeds by first calculating the energy consumption of the teacher model, then determining the parameters and energy consumption of the student model, and finally computing the fractional energy savings.\n\nThe energy per inference is given by the linear model:\n$$ E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem} $$\n\n**1. Energy of the Teacher Model ($E_{\\mathrm{teacher,fp32}}$)**\nFor the teacher model operating at $32$-bit precision (fp32), the givens are:\n- $\\mathrm{FLOPs}_{\\mathrm{teacher}} = 3.2 \\times 10^9$\n- $\\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 1.25 \\times 10^8$ bytes\n- $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ J/op\n- $b = 2.5 \\times 10^{-10}$ J/byte\n\nThe total energy for the teacher model is the sum of the compute energy and memory energy:\n$$ E_{\\mathrm{teacher,fp32}} = a_{\\mathrm{fp32}} \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}} + b \\cdot \\mathrm{Mem}_{\\mathrm{teacher,fp32}} $$\nSubstituting the values:\n$$ E_{\\mathrm{teacher,fp32}} = (2.0 \\times 10^{-12}) \\cdot (3.2 \\times 10^9) + (2.5 \\times 10^{-10}) \\cdot (1.25 \\times 10^8) $$\n$$ E_{\\mathrm{teacher,fp32}} = 6.4 \\times 10^{-3} \\, \\mathrm{J} + 3.125 \\times 10^{-2} \\, \\mathrm{J} $$\n$$ E_{\\mathrm{teacher,fp32}} = 0.0064 \\, \\mathrm{J} + 0.03125 \\, \\mathrm{J} = 0.03765 \\, \\mathrm{J} $$\n\n**2. Parameters and Energy of the Student Model ($E_{\\mathrm{student,int8}}$)**\nThe student model's parameters are derived from the teacher's.\nFirst, the effects of pruning and knowledge distillation reduce the operation count and memory traffic structurally.\n- $\\mathrm{FLOPs}_{\\mathrm{student}} = 0.40 \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}} = 0.40 \\cdot (3.2 \\times 10^9) = 1.28 \\times 10^9$ operations.\n\nSecond, the memory traffic is affected by both structural pruning and quantization. The pruning reduces the memory traffic (number of parameters/activations accessed) to $30\\%$ of the teacher's, measured at the same precision. Let's denote the memory traffic of a hypothetical 32-bit student model as $\\mathrm{Mem}_{\\mathrm{student,fp32}}$.\n$$ \\mathrm{Mem}_{\\mathrm{student,fp32}} = 0.30 \\cdot \\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 0.30 \\cdot (1.25 \\times 10^8) = 3.75 \\times 10^7 \\, \\mathrm{bytes} $$\nNext, quantization from $32$-bit to $8$-bit reduces the data size for each accessed element by a factor of $\\frac{32}{8} = 4$. Therefore, the final memory traffic for the $8$-bit student model, $\\mathrm{Mem}_{\\mathrm{student,int8}}$, is:\n$$ \\mathrm{Mem}_{\\mathrm{student,int8}} = \\frac{\\mathrm{Mem}_{\\mathrm{student,fp32}}}{4} = \\frac{3.75 \\times 10^7}{4} = 9.375 \\times 10^6 \\, \\mathrm{bytes} $$\n\nThe energy coefficients for the int8 student model are:\n- $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ J/op\n- $b = 2.5 \\times 10^{-10}$ J/byte (unchanged)\n\nThe total energy for the student model is:\n$$ E_{\\mathrm{student,int8}} = a_{\\mathrm{int8}} \\cdot \\mathrm{FLOPs}_{\\mathrm{student}} + b \\cdot \\mathrm{Mem}_{\\mathrm{student,int8}} $$\nSubstituting the values:\n$$ E_{\\mathrm{student,int8}} = (5.0 \\times 10^{-13}) \\cdot (1.28 \\times 10^9) + (2.5 \\times 10^{-10}) \\cdot (9.375 \\times 10^6) $$\n$$ E_{\\mathrm{student,int8}} = 6.4 \\times 10^{-4} \\, \\mathrm{J} + 2.34375 \\times 10^{-3} \\, \\mathrm{J} $$\n$$ E_{\\mathrm{student,int8}} = 0.00064 \\, \\mathrm{J} + 0.00234375 \\, \\mathrm{J} = 0.00298375 \\, \\mathrm{J} $$\n\n**3. Fractional Energy Savings (S)**\nThe fractional energy savings $S$ is defined as:\n$$ S = \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}} = 1 - \\frac{E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}} $$\nSubstituting the calculated energy values:\n$$ S = 1 - \\frac{0.00298375}{0.03765} $$\n$$ S = 1 - 0.079249667... $$\n$$ S = 0.920750332... $$\nRounding the result to four significant figures as required gives:\n$$ S \\approx 0.9208 $$\nThis represents a $92.08\\%$ reduction in energy consumption per inference.", "answer": "$$\n\\boxed{0.9208}\n$$", "id": "3152867"}]}