{"hands_on_practices": [{"introduction": "Differential privacy can be applied to protect different aspects of data, not just entire records. This first exercise [@problem_id:3165689] introduces the concept of *label privacy*, where we assume the features of a dataset are public but the labels are sensitive. You will implement a differentially private training step that adds calibrated noise only to the parts of the gradient computation dependent on labels. To make the privacy guarantee tangible, you will then switch roles and perform a model inversion attack to see if the 'private' model still leaks information about a sensitive feature, providing a hands-on understanding of the scope and limits of this privacy definition.", "problem": "You are given a supervised binary classification problem with a linear model trained by Stochastic Gradient Descent (SGD). The domain contains a designated sensitive feature. Your task is to implement training with privacy applied to labels only, and then perform a model inversion attack to assess leakage of the sensitive feature. The analysis must be framed under $(\\epsilon,\\delta)$-Differential Privacy (DP).\n\nConstruct a synthetic dataset with $N$ samples and $d$ features, where the first feature is the sensitive feature. For each sample index $i$ with feature vector $\\mathbf{x}_i \\in \\mathbb{R}^d$, define the sensitive coordinate as $x_{i,0} \\in \\{-1, +1\\}$ obtained by centering a Bernoulli random variable, and the remaining coordinates $x_{i,1},\\dots,x_{i,d-1}$ as independent Gaussian variables scaled so that the per-sample vector norm is bounded above by a constant $C_x$. Generate labels $y_i \\in \\{0,1\\}$ according to a logistic generative process where the true logit depends strongly on the sensitive coordinate and very weakly on the non-sensitive coordinates, with additive small Gaussian noise. You must ensure that each $\\mathbf{x}_i$ satisfies $\\lVert \\mathbf{x}_i \\rVert_2 \\leq C_x$.\n\nTrain a linear model with parameters $\\mathbf{w} \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$ using one gradient step of SGD from the zero initialization, with learning rate $\\eta$. Let the model’s predicted probability for input $\\mathbf{x}$ be $\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma$ is the logistic sigmoid. Compute the per-example residuals as $r_i = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) - y_i$ and clip each $r_i$ into the interval $[-c, c]$ with $c$ chosen so that changing one label can change $r_i$ by at most $1$. Form the average gradient with respect to $\\mathbf{w}$ as the mean of $\\mathbf{x}_i r_i$ over the batch. To achieve label privacy only, add independent Gaussian noise to the aggregated gradient with variance calibrated by the Gaussian mechanism applied to functions of labels, treating features as fixed and bounded. The sensitivity of the averaged gradient with respect to changing one label must be derived from the bound $\\lVert \\mathbf{x}_i \\rVert_2 \\leq C_x$ and the clipping of $r_i$; calibrate the Gaussian mechanism to guarantee $(\\epsilon,\\delta)$-DP for labels. Apply the same label-private Gaussian mechanism to the bias gradient computed from the averaged $r_i$.\n\nAfter this single noisy gradient step, perform a model inversion attack that seeks an input $\\hat{\\mathbf{x}}$ maximizing the model’s logit while penalizing large inputs via a quadratic regularizer. Specifically, solve for the maximizer of the objective $J(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b - \\lambda \\lVert \\mathbf{x} \\rVert_2^2$ for a chosen $\\lambda > 0$. Use the resulting $\\hat{\\mathbf{x}}$ to define a leakage metric comparing the magnitude of the sensitive coordinate to the magnitudes of the non-sensitive coordinates:\n$$\nR = \\frac{\\left| \\hat{x}_0 \\right|}{\\frac{1}{d-1} \\sum_{j=1}^{d-1} \\left| \\hat{x}_j \\right|}.\n$$\nDeclare that leakage is present if $R > \\tau$, for a chosen threshold $\\tau > 0$, and absent otherwise.\n\nYour program must implement the above pipeline and run the following test suite, each test being specified by the tuple $(N, d, \\epsilon, \\delta, \\lambda, \\tau)$:\n- Test $1$ (happy path, no privacy noise): $(N=200,\\; d=4,\\; \\epsilon=\\text{None},\\; \\delta=10^{-5},\\; \\lambda=0.1,\\; \\tau=1.5)$.\n- Test $2$ (significant privacy, moderate batch): $(N=50,\\; d=4,\\; \\epsilon=0.1,\\; \\delta=10^{-5},\\; \\lambda=0.1,\\; \\tau=1.5)$.\n- Test $3$ (boundary condition, very strong privacy): $(N=20,\\; d=4,\\; \\epsilon=0.01,\\; \\delta=10^{-5},\\; \\lambda=0.1,\\; \\tau=1.5)$.\n\nIn all tests, use a learning rate $\\eta=1.0$, per-example residual clipping parameter $c=0.5$ so that flipping one label changes $r_i$ by at most $1$, and feature norm bound $C_x=1.0$ via per-example clipping of $\\mathbf{x}_i$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests above, where each element is a boolean indicating whether leakage is present for that test (e.g., $\\left[\\text{True},\\text{False},\\text{False}\\right]$). No other text should be printed.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of differential privacy and machine learning, is well-posed with a clear objective, and provides a sufficiently complete, consistent, and formalizable set of instructions.\n\nThe solution is implemented by following a sequence of three main steps: synthetic data generation, a single step of label-private Stochastic Gradient Descent (SGD), and a model inversion attack to quantify information leakage.\n\n### Step 1: Synthetic Data Generation\nWe construct a synthetic dataset of $N$ samples, each with $d$ features, represented by a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ and a label vector $\\mathbf{y} \\in \\{0, 1\\}^N$.\n\n1.  **Feature Generation**:\n    The first feature, $\\mathbf{x}_{\\cdot, 0}$, is designated as sensitive. Its values are drawn from a centered Bernoulli distribution, i.e., $x_{i,0} \\in \\{-1, +1\\}$ for each sample $i=1, \\dots, N$. The remaining $d-1$ non-sensitive features, $x_{i,j}$ for $j=1, \\dots, d-1$, are drawn from a standard normal distribution $\\mathcal{N}(0,1)$.\n\n2.  **Feature Norm Clipping**:\n    To bound the sensitivity of the gradient in the subsequent privacy analysis, each feature vector $\\mathbf{x}_i$ is normalized to ensure its $L_2$ norm does not exceed a constant $C_x=1.0$. Specifically, if $\\lVert \\mathbf{x}_i \\rVert_2 > C_x$, the vector is scaled by a factor of $C_x / \\lVert \\mathbf{x}_i \\rVert_2$. This operation is stated as per-example clipping in the problem.\n\n3.  **Label Generation**:\n    Labels $y_i \\in \\{0, 1\\}$ are generated via a logistic model. A true weight vector $\\mathbf{w}_{\\text{true}}$ is defined to create a strong dependency on the sensitive feature and a weak dependency on the non-sensitive features. We choose $\\mathbf{w}_{\\text{true}} = [10, 0.1, \\dots, 0.1]^\\top$. The true logit for sample $i$ is computed as $z_i = \\mathbf{w}_{\\text{true}}^\\top \\mathbf{x}_i + n_i$, where $n_i \\sim \\mathcal{N}(0, 0.1^2)$ is a small amount of Gaussian noise. The probability of label $y_i=1$ is given by the logistic sigmoid function, $p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$. The final labels are drawn from a Bernoulli distribution, $y_i \\sim \\text{Bernoulli}(p_i)$.\n\n### Step 2: Differentially Private SGD\nWe perform a single gradient descent step on a linear model, starting from zero initialization, i.e., $\\mathbf{w}_0 = \\mathbf{0}$ and $b_0 = 0$. The privacy guarantee is $(\\epsilon, \\delta)$-Differential Privacy (DP) with respect to the labels.\n\n1.  **Gradient Calculation**: The model's prediction for an input $\\mathbf{x}$ is $\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$. For the first step, with $\\mathbf{w}=\\mathbf{w}_0$ and $b=b_0$, the prediction for any input is $\\sigma(0) = 0.5$. The per-example residual is $r_i = \\sigma(\\mathbf{w}_0^\\top \\mathbf{x}_i + b_0) - y_i = 0.5 - y_i$. These residuals are clipped to the range $[-c, c]$ with $c=0.5$. Since $y_i \\in \\{0,1\\}$, $r_i$ is either $0.5$ or $-0.5$, so clipping has no effect. The average gradients with respect to $\\mathbf{w}$ and $b$ are:\n    $$\n    \\nabla_{\\mathbf{w}} L = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i r_i \\quad , \\quad \\nabla_{b} L = \\frac{1}{N} \\sum_{i=1}^N r_i\n    $$\n\n2.  **Sensitivity Analysis**: For label privacy, we analyze the sensitivity of the average gradients to a change in a single label $y_k$. If $y_k$ flips, the corresponding residual $r_k$ changes by $\\Delta r_k = y_k - y'_k = \\pm 1$. The maximum change in the average gradient for $\\mathbf{w}$ is $\\Delta(\\nabla_{\\mathbf{w}} L) = \\frac{1}{N} \\mathbf{x}_k (\\Delta r_k)$. The $L_2$ sensitivity is the maximum $L_2$ norm of this change:\n    $$\n    \\mathcal{S}_{\\mathbf{w}} = \\sup \\lVert \\Delta(\\nabla_{\\mathbf{w}} L) \\rVert_2 = \\sup \\frac{1}{N} \\lVert \\mathbf{x}_k \\rVert_2 |\\Delta r_k| = \\frac{C_x}{N}\n    $$\n    Similarly, the sensitivity for the bias gradient is:\n    $$\n    \\mathcal{S}_{b} = \\sup |\\Delta(\\nabla_{b} L)| = \\sup \\frac{1}{N} |\\Delta r_k| = \\frac{1}{N}\n    $$\n\n3.  **Gaussian Mechanism**: To ensure $(\\epsilon, \\delta)$-DP, we add Gaussian noise to the gradients. The standard deviation of the noise is calibrated by the sensitivity. For a function with $L_2$ sensitivity $\\mathcal{S}$, the noise standard deviation is $\\sigma_{\\text{noise}} = \\frac{\\mathcal{S} \\sqrt{2 \\ln(1.25/\\delta)}}{\\epsilon}$.\n    - For $\\nabla_{\\mathbf{w}} L$: $\\sigma_{\\mathbf{w}} = \\frac{C_x \\sqrt{2 \\ln(1.25/\\delta)}}{N \\epsilon}$. We add noise $\\mathbf{n}_{\\mathbf{w}} \\sim \\mathcal{N}(0, \\sigma_{\\mathbf{w}}^2 \\mathbf{I})$.\n    - For $\\nabla_{b} L$: $\\sigma_{b} = \\frac{\\sqrt{2 \\ln(1.25/\\delta)}}{N \\epsilon}$. We add noise $n_b \\sim \\mathcal{N}(0, \\sigma_{b}^2)$.\n    For the test case where $\\epsilon$ is `None`, no noise is added ($\\sigma_{\\mathbf{w}}=0, \\sigma_{b}=0$).\n\n4.  **Model Update**: The weights and bias are updated using the noisy gradients and learning rate $\\eta=1.0$:\n    $$\n    \\mathbf{w}_1 = \\mathbf{w}_0 - \\eta(\\nabla_{\\mathbf{w}} L + \\mathbf{n}_{\\mathbf{w}}) = -\\eta(\\nabla_{\\mathbf{w}} L + \\mathbf{n}_{\\mathbf{w}})\n    $$\n    $$\n    b_1 = b_0 - \\eta(\\nabla_{b} L + n_b) = -\\eta(\\nabla_{b} L + n_b)\n    $$\n\n### Step 3: Model Inversion and Leakage Assessment\nAfter the single training step, we perform a model inversion attack to recover information about the training data from the learned parameters $\\mathbf{w}_1$ and $b_1$.\n\n1.  **Attack Formulation**: The attacker seeks an input $\\hat{\\mathbf{x}}$ that maximizes the model's logit, penalized by an $L_2$ regularizer to prevent trivial solutions. The objective function is:\n    $$\n    J(\\mathbf{x}) = \\mathbf{w}_1^\\top \\mathbf{x} + b_1 - \\lambda \\lVert \\mathbf{x} \\rVert_2^2\n    $$\n    To find the maximizer $\\hat{\\mathbf{x}}$, we set the gradient with respect to $\\mathbf{x}$ to zero:\n    $$\n    \\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{w}_1 - 2\\lambda \\mathbf{x} = 0\n    $$\n    Solving for $\\mathbf{x}$ gives the reconstructed input:\n    $$\n    \\hat{\\mathbf{x}} = \\frac{1}{2\\lambda} \\mathbf{w}_1\n    $$\n\n2.  **Leakage Metric**: The reconstructed vector $\\hat{\\mathbf{x}}$ is expected to reflect properties of the training data. Since the labels are strongly correlated with the sensitive feature $x_0$, the first component of the gradient, and thus of $\\mathbf{w}_1$ and $\\hat{\\mathbf{x}}$, should be large in magnitude when no privacy is used. We measure this disparity using the ratio $R$:\n    $$\n    R = \\frac{\\left| \\hat{x}_0 \\right|}{\\frac{1}{d-1} \\sum_{j=1}^{d-1} \\left| \\hat{x}_j \\right|}\n    $$\n    This ratio compares the magnitude of the reconstructed sensitive coordinate to the average magnitude of the non-sensitive coordinates. A large value of $R$ indicates that the model has disproportionately encoded information about the sensitive feature. Leakage is declared if $R > \\tau$.\n\nThe implementation carries out this entire pipeline for each test case. In the absence of privacy noise, $R$ is expected to be large, indicating leakage. With sufficient privacy noise (i.e., small $\\epsilon$), the signal in the gradient is obscured, causing all components of $\\hat{\\mathbf{x}}$ to have similar magnitudes (driven by isotropic noise), resulting in $R \\approx 1$ and no detected leakage.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the label-private SGD and model inversion attack pipeline.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # --- Fixed Parameters ---\n    # Learning rate for SGD\n    ETA = 1.0\n    # Per-example residual clipping parameter\n    C_RESIDUAL = 0.5\n    # Per-example feature norm bound\n    C_X = 1.0\n    \n    # --- Test Suite ---\n    test_cases = [\n        # (N, d, epsilon, delta, lambda, tau)\n        (200, 4, None, 1e-5, 0.1, 1.5),  # Test 1: No privacy\n        (50, 4, 0.1, 1e-5, 0.1, 1.5),    # Test 2: Significant privacy\n        (20, 4, 0.01, 1e-5, 0.1, 1.5),   # Test 3: Very strong privacy\n    ]\n\n    results = []\n\n    for N, d, epsilon, delta, lambda_reg, tau in test_cases:\n        # --- Step 1: Data Generation ---\n        \n        # Sensitive feature: centered Bernoulli\n        sensitive_feature = 2 * np.random.binomial(1, 0.5, size=(N, 1)) - 1\n        \n        # Non-sensitive features: Gaussian\n        other_features = np.random.randn(N, d - 1)\n        \n        # Combine features\n        X = np.hstack([sensitive_feature, other_features])\n        \n        # Clip feature vectors to have L2 norm at most C_X\n        norms = np.linalg.norm(X, axis=1, keepdims=True)\n        # Add a small constant to avoid division by zero for zero-norm vectors\n        X /= np.maximum(1.0, norms / C_X) \n\n        # Generate labels from a logistic model\n        # True weights with strong dependency on the sensitive feature\n        w_true = np.array([10.0] + [0.1] * (d - 1))\n        # Add small Gaussian noise to true logits\n        logits_true = X @ w_true + np.random.normal(0, 0.1, size=N)\n        probs = 1 / (1 + np.exp(-logits_true))\n        y = np.random.binomial(1, probs).astype(float)\n\n        # --- Step 2: Differentially Private SGD Step ---\n        \n        # Start from zero initialization for weights and bias\n        w0 = np.zeros(d)\n        b0 = 0.0\n        \n        # With zero-init model, prediction is sigma(0) = 0.5 for all inputs\n        pred = 0.5\n        \n        # Calculate per-example residuals\n        residuals = pred - y  # Values are either +0.5 or -0.5\n        \n        # The problem requires clipping, but residuals are already within [-0.5, 0.5]\n        # np.clip(residuals, -C_RESIDUAL, C_RESIDUAL, out=residuals)\n        \n        # Calculate average gradients (non-private)\n        grad_w = (X.T @ residuals) / N\n        grad_b = np.mean(residuals)\n        \n        noisy_grad_w = grad_w\n        noisy_grad_b = grad_b\n\n        # Add Gaussian noise for differential privacy if epsilon is specified\n        if epsilon is not None and epsilon > 0:\n            # L2 sensitivity for label privacy\n            sensitivity_w = C_X / N\n            sensitivity_b = 1.0 / N\n            \n            # Calculate noise standard deviation for the Gaussian mechanism\n            # Common factor for both noise scales\n            privacy_term = np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n            \n            sigma_w = sensitivity_w * privacy_term\n            sigma_b = sensitivity_b * privacy_term\n            \n            # Add noise to gradients\n            noise_w = np.random.normal(0, sigma_w, size=d)\n            noise_b = np.random.normal(0, sigma_b)\n            \n            noisy_grad_w += noise_w\n            noisy_grad_b += noise_b\n            \n        # Perform a single SGD update step\n        w1 = w0 - ETA * noisy_grad_w\n        b1 = b0 - ETA * noisy_grad_b\n        \n        # --- Step 3: Model Inversion Attack and Leakage Assessment ---\n        \n        # Attacker reconstructs input by maximizing the regularized logit\n        # J(x) = w1.T @ x + b1 - lambda * ||x||^2\n        # Setting dJ/dx = 0 -> w1 - 2 * lambda * x = 0 -> x = w1 / (2 * lambda)\n        if lambda_reg = 0:\n            raise ValueError(\"Regularization parameter lambda must be positive.\")\n        x_hat = w1 / (2 * lambda_reg)\n        \n        # Calculate leakage metric R\n        x_hat_0_abs = np.abs(x_hat[0])\n        x_hat_rest_abs_mean = np.mean(np.abs(x_hat[1:]))\n\n        # Handle potential division by zero if d=1, though problem states d=4\n        if x_hat_rest_abs_mean > 1e-12:\n            R = x_hat_0_abs / x_hat_rest_abs_mean\n        else: # If non-sensitive components are all zero, leakage is maximal\n            R = np.inf\n            \n        # Declare leakage if R exceeds the threshold tau\n        leakage_detected = R > tau\n        results.append(leakage_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3165689"}, {"introduction": "As models become more complex, so do the details of implementing differential privacy. This practice [@problem_id:3165699] moves from a simple linear model to a Convolutional Neural Network (CNN) and investigates the subtle but important choice of gradient clipping strategy. You will compare 'per-layer' versus structured 'per-filter' clipping, measuring how this design decision impacts both model utility and the total privacy budget, which you will track using the modern and powerful zero-Concentrated Differential Privacy (zCDP) framework.", "problem": "You are given a scenario that isolates the privacy and utility effects of structured gradient clipping on convolutional filters in a simple deep learning model. Your task is to implement a complete, runnable program that trains a minimal convolutional classifier using different clipping strategies on the convolutional filters only, adds Gaussian noise to ensure Differential Privacy (DP), and computes the privacy-utility trade-offs for a fixed test suite of configurations. Throughout, use the following foundational base.\n\nDefinitions and foundational base:\n- Differential Privacy (DP): A randomized mechanism is said to satisfy $(\\varepsilon,\\delta)$-differential privacy if, for every pair of neighboring datasets (that differ by the addition or removal of one sample) and for every measurable output set, the probability of producing outputs in that set differs by at most a factor of $e^{\\varepsilon}$ plus $\\delta$.\n- Zero-Concentrated Differential Privacy (zCDP): A mechanism satisfies $\\rho$-zCDP if its privacy loss random variable has sub-Gaussian tails parameterized by $\\rho$. The Gaussian mechanism with noise standard deviation $\\sigma_{\\text{noise}}$ and $\\ell_2$ sensitivity $\\Delta$ satisfies $\\rho$-zCDP with $\\rho = \\Delta^2 / (2 \\sigma_{\\text{noise}}^2)$. zCDP composes additively: if $k$ mechanisms with parameters $\\rho_1, \\ldots, \\rho_k$ are composed, the total parameter is $\\rho_{\\text{tot}} = \\sum_{i=1}^k \\rho_i$. Any $\\rho$-zCDP mechanism implies $(\\varepsilon,\\delta)$-DP for any $\\delta \\in (0,1)$ via $\\varepsilon = \\rho + 2 \\sqrt{\\rho \\log(1/\\delta)}$.\n- Gaussian mechanism calibration for averaged gradients: Consider a batch of size $b$. If each per-sample gradient is clipped to have $\\ell_2$ norm at most $C$, then the $\\ell_2$ sensitivity of the averaged gradient is $\\Delta = C/b$ under the addition/removal adjacency relation. If Gaussian noise with standard deviation $\\sigma_{\\text{noise}} = \\sigma \\cdot C / b$ is added (where $\\sigma$ is the noise multiplier), then the per-step zCDP parameter becomes $\\rho_{\\text{step}} = 1/(2\\sigma^2)$, independent of $b$ and $C$. For structured clipping with multiple independent groups (e.g., separate convolutional filters), the per-step zCDP parameter adds over the groups.\n\nModel and training setup:\n- Data: Create a binary classification dataset of $N$ grayscale images of size $8 \\times 8$ with labels $y \\in \\{0,1\\}$. Class $0$ images contain a horizontal bar; class $1$ images contain a vertical bar. Add independent zero-mean Gaussian noise with a small variance to all pixels to make the data non-trivial.\n- Model: A single convolutional layer with $2$ filters of size $3\\times 3$ (stride $1$, valid convolution, no padding), followed by global average pooling per filter, producing a $2$-dimensional feature vector $\\mathbf{h} \\in \\mathbb{R}^2$. Then a binary classifier computes a scalar logit $z = \\mathbf{v}^{\\top}\\mathbf{h} + b$ with weight vector $\\mathbf{v} \\in \\mathbb{R}^2$ and bias $b \\in \\mathbb{R}$. The output probability is $p = \\sigma(z)$ where $\\sigma$ is the logistic sigmoid, and the per-sample loss is $L = -y \\log p - (1-y)\\log(1-p)$.\n- Gradients: Compute per-sample gradients from first principles using the chain rule and the discrete definition of convolution. In particular, use $\\partial L / \\partial z = p - y$, $\\partial L / \\partial \\mathbf{v} = (p - y)\\,\\mathbf{h}$, $\\partial L / \\partial b = p - y$, $\\partial L / \\partial \\mathbf{h} = (p - y)\\,\\mathbf{v}$, and for each filter $f$, express $h_f$ as the spatial average of its valid convolution outputs and use the discrete convolution definition to derive $\\partial L / \\partial W^{(f)}$.\n- Privacy scope: Only the convolutional filters are protected with DP. The classifier parameters $\\mathbf{v}$ and $b$ are updated without noise. The DP analysis is applied to the sequence of noisy averaged gradient releases for the convolutional filters.\n\nClipping strategies on convolutional filters:\n- Per-layer clipping: For each sample, concatenate the gradients of all convolutional filters into a single vector and clip its $\\ell_2$ norm to at most $C$.\n- Per-parameter clipping: For each sample, clip the gradient of each convolutional filter separately to have $\\ell_2$ norm at most $C$, treating each filter as an independent group.\n\nNoise addition:\n- For each training step, after clipping per-sample gradients on the convolutional filters, average them over the batch, and add independent Gaussian noise to the averaged gradient releases. Use noise standard deviation $\\sigma_{\\text{noise}} = \\sigma \\cdot C / b$ for each released group, where $\\sigma$ is the noise multiplier and $b$ is the batch size.\n- In per-layer clipping, there is one released group (the whole layer), so the per-step zCDP parameter is $\\rho_{\\text{step}} = 1 / (2\\sigma^2)$.\n- In per-parameter clipping with $g$ filters, there are $g$ independent released groups, so the per-step zCDP parameter is $\\rho_{\\text{step}} = g / (2\\sigma^2)$.\n\nPrivacy accounting:\n- Over $T$ training steps, the total zCDP parameter is $\\rho_{\\text{tot}} = T \\cdot \\rho_{\\text{step}}$.\n- For a fixed $\\delta \\in (0,1)$, the final $(\\varepsilon,\\delta)$ is obtained by $\\varepsilon = \\rho_{\\text{tot}} + 2 \\sqrt{\\rho_{\\text{tot}} \\log(1/\\delta)}$.\n\nUtility metric:\n- After training, compute the classification accuracy on the entire dataset as the fraction of correctly predicted labels, where a label prediction is $1$ if $p \\geq 0.5$ and $0$ otherwise. Express accuracy as a decimal in $[0,1]$.\n\nTask:\n- Implement the full training loop with the two clipping strategies, the specified Gaussian noise addition, and the zCDP privacy accounting to produce $(\\varepsilon,\\delta)$ for a fixed $\\delta$.\n- Use a fixed random seed to ensure deterministic behavior so the test suite is reproducible.\n- Use scientifically reasonable hyperparameters so that training converges sufficiently to distinguish the utility effects across test cases.\n\nTest suite:\nRun your program on the following four test cases. Each case is specified as a tuple $(\\text{mode}, C, \\sigma, b, T, \\delta)$ with $g=2$ convolutional filters:\n1. Case A (happy path, per-layer): $(\\text{\"layer\"}, 1.0, 1.0, 16, 100, 10^{-5})$.\n2. Case B (structured per-parameter, same noise multiplier): $(\\text{\"param\"}, 1.0, 1.0, 16, 100, 10^{-5})$.\n3. Case C (structured per-parameter, stronger noise): $(\\text{\"param\"}, 1.0, 2.0, 16, 100, 10^{-5})$.\n4. Case D (boundary, small batch and strong noise, per-layer): $(\\text{\"layer\"}, 0.1, 3.0, 1, 50, 10^{-5})$.\n\nOutput specification:\n- For each test case, compute the final $(\\varepsilon,\\delta)$ using the described zCDP accountant and report only $\\varepsilon$, together with the final accuracy after training. Round both $\\varepsilon$ and accuracy to $4$ decimal places.\n- Your program should produce a single line of output containing a list of lists, one per test case, in the order A, B, C, D. The format is: $[\\,[\\varepsilon_1,\\text{acc}_1],\\,[\\varepsilon_2,\\text{acc}_2],\\,[\\varepsilon_3,\\text{acc}_3],\\,[\\varepsilon_4,\\text{acc}_4]\\,]$.", "solution": "The problem requires the implementation of a simulation to study the privacy-utility trade-off in a differentially private convolutional neural network. The simulation must compare two gradient clipping strategies: per-layer clipping and per-filter (structured) clipping. The solution involves several steps: data generation, model implementation, gradient computation, application of differential privacy mechanisms, training, and privacy accounting.\n\n### 1. Problem Validation\nThe problem statement is parsed and validated.\n\n*   **Givens**: All definitions for $(\\varepsilon, \\delta)$-DP, $\\rho$-zCDP, the Gaussian mechanism, model architecture ($2$ convolutional filters of size $3 \\times 3$, global average pooling, and a logistic classifier), loss function (binary cross-entropy), gradient formulas for the classifier part, clipping strategies, noise addition rules, privacy accounting method, and the specific parameters for $4$ test cases are explicitly provided.\n*   **Validation**: The problem is scientifically grounded in the established theories of deep learning and differential privacy. It is well-posed, providing a clear objective and all necessary components to build a deterministic, verifiable simulation. The language is objective and the setup is self-contained. The task is a non-trivial but feasible implementation of DP-SGD for a simplified CNN, making it a valid and meaningful problem.\n\n### 2. Algorithmic and Scientific Design\n\nA complete Python program is constructed to meet the requirements. The design follows these key steps.\n\n**2.1. Data Generation**\nA synthetic binary classification dataset of $N=400$ images of size $8 \\times 8$ is generated.\n*   Class $0$: A $3$-pixel wide horizontal bar in the center of the image.\n*   Class $1$: A $3$-pixel wide vertical bar in the center of the image.\n*   The background is $0$ and the bar is $1$. To make the classification non-trivial, independent Gaussian noise with mean $0$ and standard deviation $0.1$ is added to every pixel. The dataset is shuffled to ensure random batch composition during training. A fixed random seed ensures the dataset is identical for all test runs.\n\n**2.2. Model Architecture and Forward Propagation**\nThe model consists of a single convolutional layer followed by a logistic regression classifier.\n*   **Convolutional Layer**: It has $g=2$ filters, each of size $3 \\times 3$. Given an $8 \\times 8$ input image, a 'valid' convolution (stride $1$, no padding) produces a $6 \\times 6$ activation map for each filter.\n*   **Global Average Pooling**: For each of the $2$ activation maps, the values are averaged to produce a single scalar feature. This results in a $2$-dimensional feature vector $\\mathbf{h} \\in \\mathbb{R}^2$.\n*   **Classifier**: A logit $z$ is computed as $z = \\mathbf{v}^{\\top}\\mathbf{h} + b$, where $\\mathbf{v} \\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$ are the weights and bias. The final probability is obtained via the sigmoid function, $p = \\sigma(z) = 1 / (1 + e^{-z})$.\n\n**2.3. Backward Propagation and Gradient Computation**\nPer-sample gradients are computed using the chain rule, as specified.\n*   The upstream gradients are $\\frac{\\partial L}{\\partial z} = p - y$, $\\frac{\\partial L}{\\partial \\mathbf{v}} = \\frac{\\partial L}{\\partial z} \\mathbf{h}$, $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}$, and $\\frac{\\partial L}{\\partial \\mathbf{h}} = \\frac{\\partial L}{\\partial z} \\mathbf{v}$.\n*   The gradient with respect to the convolutional filters $\\mathbf{W}^{(f)}$ is derived. For a filter $f$, the feature $h_f$ is the mean of its convolution output. The gradient $\\frac{\\partial L}{\\partial h_f}$ is distributed uniformly across the convolution output map. The gradient $\\frac{\\partial L}{\\partial \\mathbf{W}^{(f)}}$ is then computed by correlating the input image with this uniform gradient map. This operation is efficiently implemented using `scipy.signal.correlate2d`.\n\n**2.4. Differentially Private Gradient Descent**\nFor each training step, a batch of data is sampled. Per-sample gradients for the convolutional filters are first clipped, then averaged, and finally noised.\n*   **Clipping Strategies**:\n    1.  **Per-layer (`\"layer\"`)**: The gradients of all $g=2$ filters for a single sample are flattened into one vector, and its $\\ell_2$ norm is clipped to a maximum of $C$.\n    2.  **Per-parameter (`\"param\"`)**: The gradients for each of the $g=2$ filters are treated as separate vectors, and the $\\ell_2$ norm of each is independently clipped to a maximum of $C$.\n*   **Noise Addition**: After averaging the clipped per-sample gradients over a batch of size $b$, Gaussian noise is added. The standard deviation of the noise is $\\sigma_{\\text{noise}} = \\frac{\\sigma \\cdot C}{b}$, where $\\sigma$ is the noise multiplier. This noise is added to the averaged gradients of the convolutional filters $\\mathbf{W}$. The classifier parameters $\\mathbf{v}$ and $b$ are updated with un-noised averaged gradients.\n\n**2.5. Training and Evaluation**\nThe model is trained for a specified number of steps $T$ using stochastic gradient descent with a learning rate of $0.5$. After training, the model's utility is measured by its classification accuracy on the entire dataset of $N=400$ images.\n\n**2.6. Privacy Accounting with zCDP**\nThe privacy cost is tracked using Zero-Concentrated Differential Privacy (zCDP).\n*   The per-step privacy parameter $\\rho_{\\text{step}}$ depends on the clipping strategy. For per-layer clipping (1 group), $\\rho_{\\text{step}} = \\frac{1}{2\\sigma^2}$. For per-parameter clipping ($g$ groups), the costs add up, so $\\rho_{\\text{step}} = \\frac{g}{2\\sigma^2}$.\n*   The total privacy cost over $T$ steps is $\\rho_{\\text{total}} = T \\cdot \\rho_{\\text{step}}$.\n*   This is converted to the final $(\\varepsilon, \\delta)$-DP guarantee using the formula $\\varepsilon = \\rho_{\\text{total}} + 2\\sqrt{\\rho_{\\text{total}} \\log(1/\\delta)}$.\n\n### 3. Implementation and Execution\nThe entire process is encapsulated in a single Python script. A fixed global random seed ensures the entire simulation is deterministic and reproducible. The program iterates through the four test cases provided, re-initializes the model for each, runs the training and evaluation, computes the corresponding $\\varepsilon$ and final accuracy, and formats the output as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import correlate2d\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and runs the DP-CNN simulation for the specified test suite.\n    \"\"\"\n    \n    # 0. Global Parameters and Seed\n    GLOBAL_SEED = 42\n    np.random.seed(GLOBAL_SEED)\n    \n    # Hyperparameters not specified in test cases\n    N = 400\n    IMG_SIZE = 8\n    DATA_NOISE_STD = 0.1\n    LEARNING_RATE = 0.5\n    NUM_FILTERS = 2\n    FILTER_SIZE = 3\n\n    # 1. Data Generation\n    def generate_data(n_samples, img_dim, noise_std):\n        \"\"\"Generates a binary classification dataset of bars.\"\"\"\n        X = np.zeros((n_samples, img_dim, img_dim))\n        y = np.zeros(n_samples, dtype=int)\n        \n        mid_dim = img_dim // 2\n        bar_width = 3  # bar is 3 pixels wide\n        \n        for i in range(n_samples):\n            if i  n_samples // 2:\n                # Class 0: Horizontal bar\n                y[i] = 0\n                X[i, mid_dim - bar_width//2 : mid_dim + bar_width//2 + 1, :] = 1\n            else:\n                # Class 1: Vertical bar\n                y[i] = 1\n                X[i, :, mid_dim - bar_width//2 : mid_dim + bar_width//2 + 1] = 1\n                \n        # Add Gaussian noise\n        X += np.random.normal(0, noise_std, size=X.shape)\n        \n        # Shuffle dataset\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n        \n        return X[indices], y[indices]\n\n    X_train, y_train = generate_data(N, IMG_SIZE, DATA_NOISE_STD)\n\n    # 2. Helper Functions\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-z))\n        \n    # 3. Model Gradient Logic\n    def compute_per_sample_gradients(x_i, y_i, W, v, b_param):\n        \"\"\"Computes gradients for a single sample.\"\"\"\n        g, k, _ = W.shape # g=num_filters, k=filter_size\n        img_h, img_w = x_i.shape\n        \n        # --- Forward pass for one sample ---\n        h = np.zeros(g)\n        conv_outputs = np.zeros((g, img_h - k + 1, img_w - k + 1))\n        for f_idx in range(g):\n            # correlate2d is used as it matches the gradient definition of convolution\n            conv_outputs[f_idx] = correlate2d(x_i, W[f_idx], mode='valid')\n            h[f_idx] = np.mean(conv_outputs[f_idx])\n            \n        z = v.T @ h + b_param\n        p = sigmoid(z)\n        \n        # --- Backward pass for one sample ---\n        dL_dz = p - y_i\n        grad_v = dL_dz * h\n        grad_b = dL_dz\n        dL_dh = dL_dz * v\n        \n        grad_W = np.zeros_like(W)\n        conv_out_h, conv_out_w = conv_outputs[0].shape\n        num_conv_out_pixels = conv_out_h * conv_out_w\n        \n        for f_idx in range(g):\n            dL_dh_f = dL_dh[f_idx]\n            grad_kernel = np.full((conv_out_h, conv_out_w), dL_dh_f / num_conv_out_pixels)\n            grad_W[f_idx] = correlate2d(x_i, grad_kernel, mode='valid')\n            \n        return grad_W, grad_v, grad_b\n\n    # 4. Privacy Accountant\n    def calculate_epsilon(mode, sigma, T, delta, g):\n        \"\"\"Calculates epsilon using the zCDP to (epsilon, delta)-DP conversion.\"\"\"\n        if mode == \"layer\":\n            rho_step = 1.0 / (2.0 * sigma**2)\n        elif mode == \"param\":\n            rho_step = g / (2.0 * sigma**2)\n        else:\n            raise ValueError(f\"Invalid clipping mode: {mode}\")\n            \n        rho_total = T * rho_step\n        if rho_total  0: return 0.0\n        epsilon = rho_total + 2.0 * math.sqrt(rho_total * math.log(1.0 / delta))\n        return epsilon\n\n    # 5. Main Simulation\n    test_cases = [\n        (\"layer\", 1.0, 1.0, 16, 100, 1e-5),\n        (\"param\", 1.0, 1.0, 16, 100, 1e-5),\n        (\"param\", 1.0, 2.0, 16, 100, 1e-5),\n        (\"layer\", 0.1, 3.0, 1, 50, 1e-5)\n    ]\n    \n    results = []\n    \n    for case_idx, (mode, C, sigma, b, T, delta) in enumerate(test_cases):\n        # Re-initialize model parameters for each test case for fair comparison\n        # The sequence of initializations is deterministic due to the global seed.\n        W = np.random.normal(0, 0.1, (NUM_FILTERS, FILTER_SIZE, FILTER_SIZE))\n        v = np.random.normal(0, 0.1, (NUM_FILTERS,))\n        b_param = np.random.normal(0, 0.1)\n        \n        # Training loop\n        for _ in range(T):\n            batch_indices = np.random.choice(N, size=b, replace=True)\n            X_batch, y_batch = X_train[batch_indices], y_train[batch_indices]\n            \n            sum_clipped_grad_W = np.zeros_like(W)\n            sum_grad_v = np.zeros_like(v)\n            sum_grad_b = 0.0\n            \n            for i in range(b):\n                x_i, y_i = X_batch[i], y_batch[i]\n                grad_W_i, grad_v_i, grad_b_i = compute_per_sample_gradients(x_i, y_i, W, v, b_param)\n                \n                # Clipping logic for convolutional filter gradients\n                if mode == \"layer\":\n                    norm = np.linalg.norm(grad_W_i)\n                    scale = min(1.0, C / (norm + 1e-9))\n                    clipped_grad_W_i = grad_W_i * scale\n                elif mode == \"param\":\n                    clipped_grad_W_i = np.zeros_like(grad_W_i)\n                    for f_idx in range(NUM_FILTERS):\n                        grad_W_f = grad_W_i[f_idx]\n                        norm = np.linalg.norm(grad_W_f)\n                        scale = min(1.0, C / (norm + 1e-9))\n                        clipped_grad_W_i[f_idx] = grad_W_f * scale\n                \n                sum_clipped_grad_W += clipped_grad_W_i\n                sum_grad_v += grad_v_i\n                sum_grad_b += grad_b_i\n\n            avg_grad_W = sum_clipped_grad_W / b\n            avg_grad_v = sum_grad_v / b\n            avg_grad_b = sum_grad_b / b\n            \n            noise_std = sigma * C / b\n            noise = np.random.normal(0, noise_std, size=W.shape)\n            noisy_grad_W = avg_grad_W + noise\n            \n            W -= LEARNING_RATE * noisy_grad_W\n            v -= LEARNING_RATE * avg_grad_v\n            b_param -= LEARNING_RATE * avg_grad_b\n\n        # Evaluation\n        correct_predictions = 0\n        for i in range(N):\n            h = np.zeros(NUM_FILTERS)\n            for f_idx in range(NUM_FILTERS):\n                h[f_idx] = np.mean(correlate2d(X_train[i], W[f_idx], mode='valid'))\n            z = v.T @ h + b_param\n            p = sigmoid(z)\n            prediction = 1 if p >= 0.5 else 0\n            if prediction == y_train[i]:\n                correct_predictions += 1\n        \n        accuracy = correct_predictions / N\n        \n        # Privacy calculation\n        epsilon = calculate_epsilon(mode, sigma, T, delta, g=NUM_FILTERS)\n        \n        results.append([round(epsilon, 4), round(accuracy, 4)])\n        \n    # Final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3165699"}, {"introduction": "A theoretical privacy guarantee is only as strong as its implementation. A small bug, such as using the wrong noise scale, can silently violate the privacy promise. This final practice [@problem_id:3165736] equips you with the tools of a privacy auditor by teaching you how to empirically test a mechanism's privacy. You will simulate a membership inference attack to measure the practical privacy loss and use a statistical test to check if this loss exceeds the theoretically promised upper bound given by $\\epsilon$, providing a powerful method for validating the correctness of DP systems.", "problem": "You are tasked with implementing a quantitative audit of a differentially private mechanism in a stylized membership-inference setting. The audit should compare an empirical attack success rate against a theoretically justified upper bound implied solely by the privacy accountant’s parameter $\\epsilon$ and should decide whether there is a detectable discrepancy that suggests a potential implementation error.\n\nStart from the following fundamental base:\n\n- A randomized mechanism $M$ satisfies $(\\epsilon, 0)$-Differential Privacy (DP) if for all neighboring datasets $D$ and $D'$ (differing in one individual) and for all measurable events $S$, $\\mathbb{P}(M(D) \\in S) \\leq e^{\\epsilon} \\, \\mathbb{P}(M(D') \\in S)$. \n- Under equal priors, an optimal membership-inference adversary is the Bayes-optimal test that minimizes error, which is given by the likelihood ratio test, and its success probability is $\\frac{1 + \\mathrm{TV}(P, Q)}{2}$, where $\\mathrm{TV}(P, Q)$ is the total variation distance between the two output distributions $P$ and $Q$.\n- The Laplace mechanism that adds noise $\\eta \\sim \\mathrm{Laplace}(0, b)$ to a query of global sensitivity $1$ achieves $(\\epsilon, 0)$-DP with $\\epsilon = \\frac{1}{b}$.\n\nYour program must implement the following audit procedure:\n\n1. For a fixed integer count $c_{\\mathrm{in}}$ and its neighboring count $c_{\\mathrm{out}} = c_{\\mathrm{in}} - 1$, define a mechanism that releases $Y = c + \\eta$, where $c$ equals $c_{\\mathrm{in}}$ if the individual is in the dataset and $c_{\\mathrm{out}}$ otherwise. The noise $\\eta$ is drawn from a Laplace distribution with scale $b = \\frac{\\mathrm{factor}}{\\epsilon}$, where $\\epsilon$ is the privacy accountant’s parameter and $\\mathrm{factor}$ is a multiplicative implementation factor. The correct implementation should have $\\mathrm{factor} = 1$, while deviations model potential implementation errors.\n\n2. To estimate the empirical attack success rate, simulate $N$ independent trials with equal prior $\\frac{1}{2}$ for membership. In each trial, draw a Bernoulli variable with probability $\\frac{1}{2}$ to decide whether the output comes from the “in” distribution $\\mathrm{Laplace}(c_{\\mathrm{in}}, b)$ or the “out” distribution $\\mathrm{Laplace}(c_{\\mathrm{out}}, b)$. Use the Bayes-optimal decision rule specialized to this pair of distributions: decide “in” if $Y \\geq \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2}$ and “out” otherwise. The empirical success rate is the fraction of correct decisions among $N$ trials.\n\n3. Derive from the $(\\epsilon, 0)$-DP definition a rigorous upper bound on the Bayes-optimal success probability for equal priors as a function of $\\epsilon$ only, without referencing the mechanism internals. Use this bound as the “accountant-predicted” upper limit.\n\n4. To avoid false positives due to sampling variability, compute a non-asymptotic tolerance using Hoeffding’s inequality for Bernoulli trials. For a user-specified confidence parameter $\\alpha \\in (0, 1)$, with probability at least $1 - \\alpha$, the empirical success rate $\\hat{p}$ deviates from the true success probability $p$ by at most\n$$\n\\tau = \\sqrt{\\frac{\\ln\\left(\\frac{2}{\\alpha}\\right)}{2 N}}.\n$$\nFlag a discrepancy if $\\hat{p} - \\tau$ strictly exceeds the accountant-predicted upper bound.\n\nYour program must implement the above audit for the following test suite, each test case specified as a tuple $(\\epsilon, \\mathrm{factor}, N, \\alpha, \\mathrm{seed})$:\n\n- Test $1$: $(\\epsilon = 1.0, \\mathrm{factor} = 1.0, N = 100000, \\alpha = 10^{-6}, \\mathrm{seed} = 42)$.\n- Test $2$: $(\\epsilon = 1.0, \\mathrm{factor} = 0.5, N = 50000, \\alpha = 10^{-4}, \\mathrm{seed} = 7)$.\n- Test $3$: $(\\epsilon = 0.001, \\mathrm{factor} = 1.0, N = 200000, \\alpha = 10^{-6}, \\mathrm{seed} = 2023)$.\n- Test $4$: $(\\epsilon = 5.0, \\mathrm{factor} = 1.0, N = 50000, \\alpha = 10^{-6}, \\mathrm{seed} = 123456)$.\n- Test $5$: $(\\epsilon = 1.0, \\mathrm{factor} = 0.8, N = 500, \\alpha = 0.05, \\mathrm{seed} = 99)$.\n\nImplementation details:\n\n- Use a fixed integer $c_{\\mathrm{in}} = 10$ and set $c_{\\mathrm{out}} = 9$. The midpoint decision threshold is $\\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} = 9.5$.\n- Use the provided random seeds to ensure reproducibility.\n\nFinal output specification:\n\n- For each test case, output a boolean indicating whether a discrepancy was detected.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4,\\mathrm{result}_5]$, where each $\\mathrm{result}_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "The task is to implement a quantitative audit of a differentially private mechanism. The audit compares an empirically measured attack success rate against a theoretical upper bound derived from the privacy parameter $\\epsilon$. A discrepancy is flagged if the empirical rate is statistically significantly higher than the theoretical bound, suggesting a potential implementation error. The process involves three main steps: deriving the theoretical bound, simulating the attack to find an empirical success rate, and applying a statistical test to make a decision.\n\n### 1. Theoretical Upper Bound from Differential Privacy\n\nA mechanism $M$ provides $(\\epsilon, 0)$-Differential Privacy (DP) if for any two neighboring datasets $D$ and $D'$, and any event $S$, the output distributions $P=M(D)$ and $Q=M(D')$ satisfy $\\mathbb{P}(P \\in S) \\leq e^{\\epsilon} \\mathbb{P}(Q \\in S)$.\n\nThe problem states that for equal priors, the success probability of an optimal membership-inference adversary is $p_{succ} = \\frac{1 + \\mathrm{TV}(P, Q)}{2}$, where $\\mathrm{TV}(P, Q)$ is the total variation distance between the output distributions. To find the \"accountant-predicted\" upper bound, we must find the maximum possible value of $\\mathrm{TV}(P, Q)$ under the $(\\epsilon, 0)$-DP constraint, irrespective of the specific mechanism.\n\nIt is a standard result in differential privacy that for any two distributions $P$ and $Q$ satisfying the $(\\epsilon, 0)$-DP condition, their total variation distance is bounded by:\n$$\n\\mathrm{TV}(P, Q) \\leq \\frac{e^\\epsilon - 1}{e^\\epsilon + 1}\n$$\nThis bound is tight; there exists a pair of distributions satisfying $(\\epsilon, 0)$-DP that achieves this maximum TV distance.\n\nSubstituting this maximum possible TV distance into the formula for the adversary's success probability gives the theoretical upper bound, which we denote as $p_{bound}$:\n$$\np_{succ} \\leq \\frac{1}{2} \\left( 1 + \\frac{e^\\epsilon - 1}{e^\\epsilon + 1} \\right)\n$$\nSimplifying this expression yields the accountant-predicted upper bound on success probability:\n$$\np_{bound} = \\frac{1}{2} \\left( \\frac{e^\\epsilon + 1 + e^\\epsilon - 1}{e^\\epsilon + 1} \\right) = \\frac{1}{2} \\left( \\frac{2e^\\epsilon}{e^\\epsilon + 1} \\right) = \\frac{e^\\epsilon}{e^\\epsilon + 1}\n$$\nThis bound depends only on the claimed privacy parameter $\\epsilon$.\n\n### 2. Empirical Attack Success Rate Simulation\n\nThe audit simulates a specific membership inference attack to estimate the mechanism's true attack success probability.\n- The mechanism adds noise from a Laplace distribution, $Y = c + \\eta$, where $\\eta \\sim \\mathrm{Laplace}(0, b)$.\n- The scale parameter is $b = \\frac{\\mathrm{factor}}{\\epsilon}$. For a correct implementation on a query with global sensitivity $1$, we should have $\\mathrm{factor}=1$. The global sensitivity is $\\Delta c = |c_{\\mathrm{in}} - c_{\\mathrm{out}}| = |10 - 9| = 1$.\n- Two distributions are considered: $P_{in}$ corresponding to $c=c_{\\mathrm{in}}=10$, so $Y \\sim \\mathrm{Laplace}(10, b)$, and $P_{out}$ for $c=c_{\\mathrm{out}}=9$, so $Y \\sim \\mathrm{Laplace}(9, b)$.\n- The Bayes-optimal decision rule for distinguishing between these two symmetric distributions with equal priors is to compare the likelihoods, which simplifies to checking which mean the output $Y$ is closer to. The decision boundary is the midpoint $T = \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} = 9.5$. The rule is: decide \"in\" if $Y \\geq 9.5$, and \"out\" otherwise.\n\nThe simulation runs $N$ trials. In each trial:\n1. A true state (\"in\" or \"out\") is chosen with probability $\\frac{1}{2}$.\n2. A noisy output $Y$ is generated from the corresponding distribution, $\\mathrm{Laplace}(c_{\\mathrm{in}}, b)$ or $\\mathrm{Laplace}(c_{\\mathrm{out}}, b)$.\n3. The decision rule is applied to $Y$.\n4. The decision is compared with the true state to check if it was correct.\n\nThe empirical success rate, $\\hat{p}$, is the total number of correct decisions divided by the total number of trials, $N$. This $\\hat{p}$ is an estimate of the true success probability, $p_{true}$, for this specific attack on this specific mechanism.\n\nFor completeness, the analytical value for $p_{true}$ is derived from the success probability of the midpoint classifier on two Laplace distributions. The probability of a correct decision is $p_{true} = \\frac{1}{2} \\mathbb{P}(\\text{correct} | \\text{in}) + \\frac{1}{2} \\mathbb{P}(\\text{correct} | \\text{out})$. By symmetry, the two conditional probabilities are equal: $\\mathbb{P}(Y \\geq T | Y \\sim \\mathrm{Laplace}(c_{\\mathrm{in}}, b)) = \\mathbb{P}(Y  T | Y \\sim \\mathrm{Laplace}(c_{\\mathrm{out}}, b))$. Using the CDF of the Laplace distribution, this probability is $1 - \\frac{1}{2} \\exp\\left(-\\frac{c_{\\mathrm{in}}-T}{b}\\right)$. This results in:\n$$\np_{true} = 1 - \\frac{1}{2} \\exp\\left(-\\frac{\\Delta c}{2b}\\right) = 1 - \\frac{1}{2} \\exp\\left(-\\frac{\\epsilon}{2 \\cdot \\mathrm{factor}}\\right)\n$$\n\n### 3. Statistical Test for Discrepancy Detection\n\nThe empirical rate $\\hat{p}$ is a random variable. To account for sampling variability and avoid false alarms, we use Hoeffding's inequality. For $N$ Bernoulli trials with true probability $p$, the empirical estimate $\\hat{p}$ deviates from $p$ by at most a tolerance $\\tau$ with high probability:\n$$\n\\mathbb{P}(|\\hat{p} - p| \\geq \\tau) \\leq 2e^{-2N\\tau^2}\n$$\nSetting the right-hand side to a small confidence parameter $\\alpha$, we can solve for the one-sided tolerance $\\tau$:\n$$\n\\alpha = 2e^{-2N\\tau^2} \\implies \\frac{\\alpha}{2} = e^{-2N\\tau^2} \\implies \\ln\\left(\\frac{2}{\\alpha}\\right) = 2N\\tau^2 \\implies \\tau = \\sqrt{\\frac{\\ln\\left(\\frac{2}{\\alpha}\\right)}{2 N}}\n$$\nWith probability at least $1-\\alpha$, the true success rate $p_{true}$ is not more than $\\hat{p}+\\tau$. If the implementation is faulty making it less private (e.g., $\\mathrm{factor}  1$), $p_{true}$ might exceed the accountant's bound $p_{bound}$. We flag a discrepancy if we have strong statistical evidence that $p_{true} > p_{bound}$. This is established if the lower bound of our confidence interval for $p_{true}$, which is $\\hat{p} - \\tau$, is strictly greater than the theoretical maximum $p_{bound}$.\n\nThe final audit condition is to flag a discrepancy if and only if:\n$$\n\\hat{p} - \\tau > p_{bound}\n$$\n\n### Summary of the Audit Algorithm\nFor each test case $(\\epsilon, \\mathrm{factor}, N, \\alpha, \\mathrm{seed})$:\n1.  Calculate the Laplace scale parameter $b = \\frac{\\mathrm{factor}}{\\epsilon}$.\n2.  Set up a random number generator with the given `seed`.\n3.  Simulate $N$ trials of the membership inference attack to compute the empirical success rate $\\hat{p}$.\n4.  Calculate the accountant-predicted upper bound: $p_{bound} = \\frac{e^\\epsilon}{e^\\epsilon + 1}$.\n5.  Calculate the statistical tolerance: $\\tau = \\sqrt{\\frac{\\ln(2/\\alpha)}{2N}}$.\n6.  Compare $\\hat{p} - \\tau$ with $p_{bound}$. If $\\hat{p} - \\tau > p_{bound}$, a discrepancy is detected (True). Otherwise, no discrepancy is detected (False).\n7.  Collect the boolean results for all test cases.", "answer": "```python\nimport numpy as np\n\ndef run_audit(epsilon, factor, N, alpha, seed):\n    \"\"\"\n    Performs a quantitative audit of a differentially private mechanism.\n\n    Args:\n        epsilon (float): The claimed privacy parameter epsilon.\n        factor (float): The implementation factor for the Laplace scale.\n        N (int): The number of simulation trials.\n        alpha (float): The confidence parameter for Hoeffding's inequality.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        bool: True if a discrepancy is detected, False otherwise.\n    \"\"\"\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Step 1: Mechanism and Attack Setup ---\n    c_in = 10.0\n    c_out = 9.0\n    decision_threshold = (c_in + c_out) / 2.0\n    \n    # The global sensitivity of the count query is |c_in - c_out| = 1.\n    # The Laplace scale b should be GS/epsilon = 1/epsilon for a correct implementation.\n    # A 'factor' is introduced to model implementation errors.\n    if epsilon == 0:\n        # Avoid division by zero, although not in test cases.\n        # Infinite noise means output is always 0, attack success is 0.5.\n        b = float('inf')\n    else:\n        b = factor / epsilon\n\n    # --- Step 2: Estimate Empirical Attack Success Rate ---\n    correct_decisions = 0\n    \n    # Generate all random choices at once for efficiency\n    true_labels_are_in = rng.random(size=N)  0.5\n    \n    # Generate Laplace noise for both 'in' and 'out' cases\n    # Note: np.random.laplace takes scale parameter b\n    laplace_noise = rng.laplace(loc=0.0, scale=b, size=N)\n\n    # Calculate noisy outputs\n    outputs_in = c_in + laplace_noise\n    outputs_out = c_out + laplace_noise\n    \n    # Combine outputs based on true labels\n    Y = np.where(true_labels_are_in, outputs_in, outputs_out)\n    \n    # Apply the Bayes-optimal decision rule\n    decisions_are_in = Y >= decision_threshold\n    \n    # Count correct decisions\n    correct_decisions = np.sum(decisions_are_in == true_labels_are_in)\n\n    # Empirical success rate\n    p_hat = correct_decisions / N\n\n    # --- Step 3: Theoretical Bound ---\n    # The upper bound on success probability is derived from the TV-distance bound of (eps, 0)-DP\n    # p_bound = (1 + (e^eps - 1)/(e^eps + 1)) / 2 = e^eps / (e^eps + 1)\n    p_bound = np.exp(epsilon) / (np.exp(epsilon) + 1.0)\n\n    # --- Step 4: Statistical Test ---\n    # Calculate the tolerance tau using Hoeffding's inequality\n    tau = np.sqrt(np.log(2.0 / alpha) / (2.0 * N))\n    \n    # A discrepancy is flagged if the empirical rate, adjusted for sampling error,\n    # is strictly greater than the theoretical bound.\n    discrepancy_detected = (p_hat - tau) > p_bound\n\n    return discrepancy_detected\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the audit for each test case.\n    \"\"\"\n    test_cases = [\n        # (epsilon, factor, N, alpha, seed)\n        (1.0, 1.0, 100000, 1e-6, 42),\n        (1.0, 0.5, 50000, 1e-4, 7),\n        (0.001, 1.0, 200000, 1e-6, 2023),\n        (5.0, 1.0, 50000, 1e-6, 123456),\n        (1.0, 0.8, 500, 0.05, 99)\n    ]\n\n    results = []\n    for case in test_cases:\n        epsilon, factor, N, alpha, seed = case\n        result = run_audit(epsilon, factor, N, alpha, seed)\n        results.append(str(result))\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3165736"}]}