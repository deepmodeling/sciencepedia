{"hands_on_practices": [{"introduction": "Our journey into adversarial machine learning begins with a foundational exercise. We will implement two of the most well-known gradient-based attacks, the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), to probe the vulnerabilities of a simple classifier. This practice [@problem_id:3098457] further explores how a common regularization technique, label smoothing, can serve as a defense by investigating its impact on the model's robustness and its loss landscape.", "problem": "You are given a fixed multi-class linear classifier with a softmax output and a synthetic dataset. The goal is to study how label smoothing, controlled by the scalar parameter $\\alpha \\in [0,1]$, affects adversarial robustness by tracking the change in the average gradient magnitude with respect to the input and the misclassification rates under attacks generated by the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). You must implement a program that computes, for each specified $\\alpha$, the average input-gradient magnitude and the misclassification rates after FGSM and PGD attacks.\n\nFoundational base to use:\n- Multi-class softmax regression: for input $x \\in \\mathbb{R}^d$, logits $z \\in \\mathbb{R}^K$ are $z = W x + b$, where $W \\in \\mathbb{R}^{K \\times d}$ and $b \\in \\mathbb{R}^K$. The softmax mapping is $p_i = \\exp(z_i) / \\sum_{j=1}^K \\exp(z_j)$ for $i \\in \\{1,\\dots,K\\}$.\n- Cross-entropy loss: for a target distribution $y \\in \\Delta^{K-1}$ (the probability simplex), the loss is $L(x,y) = -\\sum_{i=1}^K y_i \\log p_i$.\n- Label smoothing: for a true class $t \\in \\{0,\\dots,K-1\\}$ and smoothing parameter $\\alpha \\in [0,1]$, define $y^{(\\alpha)}$ as $y^{(\\alpha)}_i = \\alpha/K$ for all $i$, then add $(1-\\alpha)$ to the coordinate $i=t$ so that $y^{(\\alpha)}_t = (1-\\alpha) + \\alpha/K$ and $y^{(\\alpha)}_i = \\alpha/K$ for $i \\neq t$.\n- Chain rule in calculus applied to the softmax cross-entropy model.\n\nYou must not use any results beyond these definitions, and you must derive any needed expressions from these starting points.\n\nAttacks to implement:\n- Fast Gradient Sign Method (FGSM): for a given $\\varepsilon > 0$, the adversarial example is $x_{\\text{FGSM}} = x + \\varepsilon \\cdot \\operatorname{sign}(\\nabla_x L(x, y^{(\\alpha)}))$, where $\\operatorname{sign}(\\cdot)$ is applied elementwise.\n- Projected Gradient Descent (PGD) with $\\ell_\\infty$ constraint: starting at $x^{(0)} = x$, for a given step size $\\eta > 0$ and number of steps $T \\in \\mathbb{N}$, iterate $x^{(t+1)} = \\Pi_{B_\\infty(x,\\varepsilon)}\\left(x^{(t)} + \\eta \\cdot \\operatorname{sign}(\\nabla_x L(x^{(t)}, y^{(\\alpha)}))\\right)$ for $t=0,\\dots,T-1$, where $\\Pi_{B_\\infty(x,\\varepsilon)}(\\cdot)$ denotes the projection onto the $\\ell_\\infty$ ball of radius $\\varepsilon$ centered at $x$.\n\nDataset and model specification (to ensure scientific realism and determinism):\n- Number of classes: $K = 3$.\n- Input dimension: $d = 4$.\n- Class means: $\\mu_0 = (2,0,0,0)$, $\\mu_1 = (0,2,0,0)$, $\\mu_2 = (0,0,2,0)$.\n- Number of samples per class: $N_c = 20$, total $N = 60$.\n- Data generation: for each class $c \\in \\{0,1,2\\}$, draw $N_c$ points $x = \\mu_c + \\delta$, where $\\delta$ is independently sampled from a zero-mean normal distribution in $\\mathbb{R}^4$ with standard deviation $0.2$ along each coordinate. Use a fixed pseudo-random generator seed so the dataset is deterministic.\n- Model parameters: $W = \\begin{bmatrix} 1.2 & 0 & 0 & 0 \\\\ 0 & 1.2 & 0 & 0 \\\\ 0 & 0 & 1.2 & 0 \\end{bmatrix}$ and $b = (0,0,0)$.\n\nTargets and measurements:\n- Gradient magnitude: for each sample $(x,t)$ and given $\\alpha$, compute the Euclidean norm $\\|\\nabla_x L(x, y^{(\\alpha)})\\|_2$, then report the dataset-wide average over all $N$ samples.\n- Misclassification rate under FGSM for given $\\alpha$: generate $x_{\\text{FGSM}}$ for each sample, classify it as $\\hat{t} = \\arg\\max_i z_i$ using the original model (no retraining), and report the fraction of samples where $\\hat{t} \\neq t$ as a decimal number.\n- Misclassification rate under PGD for given $\\alpha$: similarly generate $x_{\\text{PGD}}$ after $T$ steps and report the fraction of samples where the predicted class is not the true class.\n\nTest suite:\n- Smoothing parameters: $\\alpha \\in \\{0.0, 0.1, 0.5, 0.9, 0.99\\}$.\n- FGSM parameters: $\\varepsilon = 0.25$.\n- PGD parameters: $\\varepsilon = 0.25$, $\\eta = 0.07$, $T = 12$.\n\nAngle units do not apply. No physical units appear. All outputs must be decimals (floats).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one $\\alpha$ in the order given and must itself be a list of four floats in the order $[\\alpha, \\text{avg\\_grad\\_norm}, \\text{fgsm\\_misrate}, \\text{pgd\\_misrate}]$. For example, the overall format must be like $[[\\alpha_1,g_1,m^{\\text{FGSM}}_1,m^{\\text{PGD}}_1],[\\alpha_2,g_2,m^{\\text{FGSM}}_2,m^{\\text{PGD}}_2],\\dots]$.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of deep learning, mathematically well-posed with all necessary constants and parameters defined, and completely objective. It presents a standard, albeit simplified, research task in the domain of adversarial machine learning. All components—the linear softmax model, cross-entropy loss, label smoothing, and adversarial attack methods (FGSM, PGD)—are standard in the field. The provided parameters for the model, dataset, and attacks are self-contained and consistent, permitting a unique and verifiable numerical solution.\n\nWe proceed to derive the solution. The core of the task is to compute the gradient of the loss function with respect to the input, which is then used to generate adversarial examples and analyze the model's properties.\n\n### Mathematical Formulation and Gradient Derivation\n\nThe model is a multi-class linear classifier with a softmax activation. For an input vector $x \\in \\mathbb{R}^d$, the logits $z \\in \\mathbb{R}^K$ are computed as:\n$$ z = Wx + b $$\nwhere $W \\in \\mathbb{R}^{K \\times d}$ are the weights and $b \\in \\mathbb{R}^K$ is the bias. The problem specifies $d=4$ and $K=3$.\n\nThe softmax function converts the logits into a probability distribution $p \\in \\Delta^{K-1}$:\n$$ p_i = \\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} $$\nfor each class $i \\in \\{1, \\dots, K\\}$.\n\nThe loss function is the cross-entropy between the predicted probability distribution $p$ and a target distribution $y \\in \\Delta^{K-1}$:\n$$ L(x, y) = -\\sum_{i=1}^K y_i \\log p_i $$\n\nThe target distribution $y$ is constructed using label smoothing. For a true class $t$, the one-hot target vector is $y^{\\text{onehot}}$, where $(y^{\\text{onehot}})_t = 1$ and $(y^{\\text{onehot}})_i = 0$ for $i \\neq t$. The smoothed label vector $y^{(\\alpha)}$ is a convex combination of the one-hot vector and the uniform distribution $u$ (where $u_i = 1/K$ for all $i$):\n$$ y^{(\\alpha)} = (1-\\alpha)y^{\\text{onehot}} + \\alpha u $$\nThis yields the component-wise definition provided in the problem:\n$$ y^{(\\alpha)}_i = \\begin{cases} (1-\\alpha) + \\alpha/K & \\text{if } i=t \\\\ \\alpha/K & \\text{if } i \\neq t \\end{cases} $$\n\nTo generate adversarial attacks, we need the gradient of the loss $L$ with respect to the input $x$, denoted $\\nabla_x L$. We apply the chain rule:\n$$ \\nabla_x L(x, y^{(\\alpha)}) = \\left(\\frac{\\partial z}{\\partial x}\\right)^T \\nabla_z L(x, y^{(\\alpha)}) $$\nThe Jacobian of the logits $z=Wx+b$ with respect to $x$ is simply the weight matrix $W$:\n$$ \\frac{\\partial z}{\\partial x} = W $$\nNext, we find the gradient of the loss with respect to the logits, $\\nabla_z L$. The partial derivative with respect to a single logit $z_j$ is:\n$$ \\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_j} $$\nFrom the loss definition, $\\frac{\\partial L}{\\partial p_i} = -y_i/p_i$. The derivative of the softmax function is a standard result: $\\frac{\\partial p_i}{\\partial z_j} = p_i(\\delta_{ij} - p_j)$, where $\\delta_{ij}$ is the Kronecker delta.\nSubstituting these into the sum:\n$$ \\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^K \\left(-\\frac{y_i}{p_i}\\right) (p_i(\\delta_{ij} - p_j)) = \\sum_{i=1}^K -y_i(\\delta_{ij} - p_j) $$\n$$ = -y_j \\cdot 1 - \\sum_{i=1}^K (-y_i p_j) = -y_j + p_j \\sum_{i=1}^K y_i $$\nSince $y^{(\\alpha)}$ is a probability distribution, $\\sum_i y^{(\\alpha)}_i = 1$. This simplifies the expression to:\n$$ \\frac{\\partial L}{\\partial z_j} = p_j - y_j^{(\\alpha)} $$\nIn vector form, $\\nabla_z L = p - y^{(\\alpha)}$.\n\nFinally, we combine the parts to get the gradient with respect to the input $x$:\n$$ \\nabla_x L(x, y^{(\\alpha)}) = W^T (p - y^{(\\alpha)}) $$\nThis gradient is the cornerstone for all subsequent calculations.\n\n### Adversarial Attack Algorithms\n\nWith the gradient expression, we can define the attack methods.\n\n1.  **Fast Gradient Sign Method (FGSM):** A single-step attack that adds a perturbation in the direction of the sign of the gradient.\n    $$ x_{\\text{FGSM}} = x + \\varepsilon \\cdot \\text{sign}(\\nabla_x L(x, y^{(\\alpha)})) $$\n    The $\\text{sign}(\\cdot)$ function is applied element-wise, and $\\varepsilon$ is the attack magnitude, constrained by the $\\ell_\\infty$ norm.\n\n2.  **Projected Gradient Descent (PGD):** An iterative version of FGSM that takes multiple smaller steps and projects the result back into an $\\ell_\\infty$ ball of radius $\\varepsilon$ around the original input $x$.\n    Starting with $x^{(0)} = x$, the update rule for $T$ steps is:\n    $$ x^{(t+1)} = \\Pi_{B_\\infty(x,\\varepsilon)}\\left(x^{(t)} + \\eta \\cdot \\text{sign}(\\nabla_x L(x^{(t)}, y^{(\\alpha)}))\\right) $$\n    where $\\eta < \\varepsilon$ is the step size. The projection operator $\\Pi_{B_\\infty(x,\\varepsilon)}(z)$ clips the perturbation to ensure that the resulting point $z$ satisfies $\\|z-x\\|_\\infty \\le \\varepsilon$. This is implemented by element-wise clipping:\n    $$ \\Pi_{B_\\infty(x,\\varepsilon)}(z) = x + \\text{clip}(z-x, -\\varepsilon, \\varepsilon) $$\n\n### Implementation Plan\n\nThe program will execute the following steps:\n1.  **Initialize Parameters:** Set up model parameters ($W, b$), dataset parameters ($K, d, \\mu_c, N_c$), and attack parameters ($\\varepsilon, \\eta, T$).\n2.  **Generate Dataset:** Create a deterministic synthetic dataset using a fixed random seed. For each of the $K=3$ classes, generate $N_c=20$ samples by adding Gaussian noise with standard deviation $0.2$ to the respective class mean $\\mu_c$.\n3.  **Iterate over Smoothing Parameters:** Loop through each specified value of $\\alpha \\in \\{0.0, 0.1, 0.5, 0.9, 0.99\\}$.\n4.  **For each $\\alpha$:**\n    a. Initialize accumulators for the average gradient norm and misclassification counts for FGSM and PGD.\n    b. Iterate through each sample $(x_{\\text{orig}}, t)$ in the dataset.\n    c. Construct the smoothed label vector $y^{(\\alpha)}$ for the true class $t$.\n    d. **Calculate Gradient Norm:** Compute $\\nabla_x L(x_{\\text{orig}}, y^{(\\alpha)})$ and its Euclidean norm $\\|\\cdot\\|_2$. Add this to the accumulator.\n    e. **Perform FGSM Attack:** Generate $x_{\\text{FGSM}}$. Classify it using the model ($z=Wx+b$). If the predicted class is not $t$, increment the FGSM misclassification counter.\n    f. **Perform PGD Attack:** Run the iterative PGD algorithm for $T=12$ steps to generate $x_{\\text{PGD}}$. Classify it. If the prediction is incorrect, increment the PGD misclassification counter.\n    g. **Compute Metrics:** After processing all samples, calculate the average gradient norm and the misclassification rates (count / total samples).\n5.  **Output Results:** Store the computed metrics $[\\alpha, \\text{avg\\_grad\\_norm}, \\text{fgsm\\_misrate}, \\text{pgd\\_misrate}]$ for each $\\alpha$ and format the final output as a list of these lists.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes adversarial robustness metrics for a linear softmax classifier\n    under varying levels of label smoothing.\n    \"\"\"\n    # Dataset and Model Specifications\n    K = 3  # Number of classes\n    D = 4  # Input dimension\n    NC = 20  # Number of samples per class\n    N = K * NC  # Total number of samples\n\n    # Class means\n    MEANS = np.array([\n        [2.0, 0.0, 0.0, 0.0],\n        [0.0, 2.0, 0.0, 0.0],\n        [0.0, 0.0, 2.0, 0.0]\n    ])\n    \n    # Noise standard deviation\n    STD_DEV = 0.2\n\n    # Model parameters\n    W = np.array([\n        [1.2, 0.0, 0.0, 0.0],\n        [0.0, 1.2, 0.0, 0.0],\n        [0.0, 0.0, 1.2, 0.0]\n    ])\n    B = np.array([0.0, 0.0, 0.0])\n\n    # Test Suite Parameters\n    ALPHA_VALUES = [0.0, 0.1, 0.5, 0.9, 0.99]\n    EPSILON_FGSM = 0.25\n    EPSILON_PGD = 0.25\n    ETA_PGD = 0.07\n    T_PGD = 12\n    \n    # Fixed seed for deterministic dataset generation\n    SEED = 42\n    rng = np.random.default_rng(seed=SEED)\n\n    # Generate dataset\n    X_data = np.zeros((N, D))\n    T_labels = np.zeros(N, dtype=int)\n    for c in range(K):\n        start_idx = c * NC\n        end_idx = (c + 1) * NC\n        noise = rng.normal(loc=0.0, scale=STD_DEV, size=(NC, D))\n        X_data[start_idx:end_idx, :] = MEANS[c] + noise\n        T_labels[start_idx:end_idx] = c\n\n    def stable_softmax(z):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        z_shifted = z - np.max(z, axis=-1, keepdims=True)\n        exp_z = np.exp(z_shifted)\n        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n    \n    def get_logits(x_input):\n        \"\"\"Computes logits: z = Wx + b.\"\"\"\n        return x_input @ W.T + B\n\n    def compute_gradient(x, y_target):\n        \"\"\"Computes gradient of cross-entropy loss wrt input x.\"\"\"\n        z = get_logits(x)\n        p = stable_softmax(z)\n        grad = W.T @ (p - y_target)\n        return grad\n\n    results = []\n\n    for alpha in ALPHA_VALUES:\n        \n        total_grad_norm = 0.0\n        fgsm_misclassified_count = 0\n        pgd_misclassified_count = 0\n\n        for i in range(N):\n            x_orig = X_data[i]\n            t_true = T_labels[i]\n\n            # Construct smoothed label vector\n            y_target = np.full(K, alpha / K)\n            y_target[t_true] += (1 - alpha)\n            \n            # 1. Calculate gradient norm on original sample\n            grad_orig = compute_gradient(x_orig, y_target)\n            total_grad_norm += np.linalg.norm(grad_orig)\n\n            # 2. FGSM Attack\n            x_fgsm = x_orig + EPSILON_FGSM * np.sign(grad_orig)\n            z_fgsm = get_logits(x_fgsm)\n            pred_fgsm = np.argmax(z_fgsm)\n            if pred_fgsm != t_true:\n                fgsm_misclassified_count += 1\n            \n            # 3. PGD Attack\n            x_pgd = np.copy(x_orig)\n            for _ in range(T_PGD):\n                grad_pgd = compute_gradient(x_pgd, y_target)\n                x_pgd_step = x_pgd + ETA_PGD * np.sign(grad_pgd)\n                # Projection step\n                perturbation = x_pgd_step - x_orig\n                perturbation = np.clip(perturbation, -EPSILON_PGD, EPSILON_PGD)\n                x_pgd = x_orig + perturbation\n\n            z_pgd = get_logits(x_pgd)\n            pred_pgd = np.argmax(z_pgd)\n            if pred_pgd != t_true:\n                pgd_misclassified_count += 1\n\n        avg_grad_norm = total_grad_norm / N\n        fgsm_misrate = fgsm_misclassified_count / N\n        pgd_misrate = pgd_misclassified_count / N\n\n        results.append([alpha, avg_grad_norm, fgsm_misrate, pgd_misrate])\n\n    # Format the final output string exactly as required\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3098457"}, {"introduction": "Defenses against adversarial attacks are not limited to modifying the model's training process; they can also involve preprocessing the input data. This exercise [@problem_id:3098464] investigates an intuitive defense strategy based on signal processing: using a low-pass filter to remove the high-frequency noise characteristic of many adversarial perturbations. By implementing this defense, you will gain insight into the crucial trade-off between improving a model's robust accuracy on attacked inputs and potentially degrading its clean accuracy on legitimate ones.", "problem": "You are to implement and evaluate a simple preprocessing defense based on ideal low-pass filtering against a first-order adversarial attack for a linear classifier on synthetic one-dimensional signals. The task is to derive the required components from foundational definitions, implement them, and quantify the trade-off between clean accuracy and robust accuracy as the low-pass cutoff frequency varies.\n\nConsider the following setting.\n\n- Data generation: Each input is a one-dimensional discrete signal of length $N$, denoted by $\\mathbf{x} \\in \\mathbb{R}^{N}$. Let labels be $y \\in \\{-1,+1\\}$. For each sample, the signal is generated as\n  $$\\mathbf{x} = y \\left(A_{\\text{low}} \\,\\mathbf{s}_{\\text{low}} + A_{\\text{high}} \\,\\mathbf{s}_{\\text{high}}\\right) + \\boldsymbol{\\epsilon},$$\n  where $\\mathbf{s}_{\\text{low}}$ and $\\mathbf{s}_{\\text{high}}$ are fixed sinusoidal basis vectors of angular frequencies $2\\pi k_{\\text{low}}/N$ and $2\\pi k_{\\text{high}}/N$ respectively, and $\\boldsymbol{\\epsilon}$ is independent and identically distributed Gaussian noise with zero mean and standard deviation $\\sigma$. The parameters $A_{\\text{low}}$, $A_{\\text{high}}$, $k_{\\text{low}}$, $k_{\\text{high}}$, $N$, $\\sigma$, and the number of samples $M$ are specified in the test suite below.\n\n- Classifier: Use a logistic regression model with a fixed weight vector $\\mathbf{w} \\in \\mathbb{R}^{N}$ and bias $b \\in \\mathbb{R}$. The classifier computes the logit $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$ and predicts $\\hat{y} = \\mathrm{sign}(z)$. The loss for an example $(\\mathbf{x}, y)$ is the logistic loss\n  $$\\mathcal{L}(\\mathbf{x}, y) = \\log\\big(1 + \\exp(-y\\,(\\mathbf{w}^{\\top}\\mathbf{x} + b))\\big).$$\n  The weight vector is constructed to emphasize the high-frequency component, as described in the test suite.\n\n- Adversary and attack: Use the Fast Gradient Sign Method (FGSM) under an $\\ell_{\\infty}$ constraint. The adversarial example is defined by\n  $$\\mathbf{x}_{\\text{adv}} = \\mathbf{x} + \\varepsilon \\,\\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)\\right),$$\n  where $\\varepsilon$ is the perturbation budget. You must derive $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)$ from the above loss using first principles (the chain rule and the derivative of the logistic function) and use that to implement FGSM.\n\n- Defense: Define an ideal low-pass filter as follows. Given a real signal $\\mathbf{x}$ of length $N$ and a cutoff angular frequency $\\omega_{c} \\in [0,\\pi]$ measured in radians per sample, compute its Discrete Fourier Transform (DFT) coefficients $\\{X_{k}\\}$, where the angular frequency associated with bin $k$ is $\\omega_{k} = 2\\pi k/N$. Zero out all frequency components with $\\omega_{k} > \\omega_{c}$ and invert the DFT to obtain the filtered signal. In practice for real signals, you may use the one-sided real DFT and map indices to angular frequencies accordingly. The low-pass defense preprocesses inputs by applying this ideal filter.\n\n- Metrics:\n  1. Clean accuracy for a cutoff $\\omega_{c}$: the fraction of correctly classified clean examples after preprocessing the inputs with the low-pass filter at $\\omega_{c}$.\n  2. Robust accuracy for a cutoff $\\omega_{c}$: the fraction of correctly classified adversarial examples, where each adversarial example is generated from the unfiltered clean input using FGSM and then passed through the low-pass filter at $\\omega_{c}$ before classification.\n\n- Quantities to report for each cutoff $\\omega_{c}$:\n  1. Clean accuracy drop: the difference between the clean accuracy with no filtering (baseline at $\\omega_{c}=\\pi$) and the clean accuracy at the given $\\omega_{c}$.\n  2. Robust accuracy gain: the difference between the robust accuracy at the given $\\omega_{c}$ and the robust accuracy with no filtering (baseline at $\\omega_{c}=\\pi$).\n\nYour program must:\n\n- Generate the dataset, construct the classifier, implement FGSM by deriving and using $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)$, implement the ideal low-pass filter in the frequency domain, and compute the requested metrics.\n- Use the following test suite (angles are in radians per sample, dataset sizes are unitless):\n  - Signal length: $N = 64$.\n  - Number of samples: $M = 400$ with a balanced label set $y \\in \\{-1, +1\\}$.\n  - Frequencies: $k_{\\text{low}} = 2$, $k_{\\text{high}} = 16$.\n  - Amplitudes: $A_{\\text{low}} = 0.8$, $A_{\\text{high}} = 0.7$.\n  - Noise standard deviation: $\\sigma = 0.2$.\n  - Classifier construction: $\\mathbf{w} = \\alpha \\,\\mathbf{s}_{\\text{high}} + \\beta \\,\\mathbf{s}_{\\text{low}}$, then normalized to unit $\\ell_2$ norm, with $\\alpha = 1.0$, $\\beta = 0.15$, and $b=0$.\n  - FGSM budget: $\\varepsilon = 0.5$.\n  - Cutoff set (including the baseline): $\\{\\omega_{c}\\} = \\{\\pi, 0.8\\pi, 0.5\\pi, 0.2\\pi\\}$.\n- Round each reported value (clean accuracy drop and robust accuracy gain) to $4$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a list of two-element lists, one per cutoff in the specified order, where each inner list is $[\\text{clean\\_drop}, \\text{robust\\_gain}]$. For example,\n\"[ [cd_1,rg_1],[cd_2,rg_2],[cd_3,rg_3],[cd_4,rg_4] ]\" but without spaces after commas. The actual numeric values must be decimals (not percentages). Only this single line must be printed.\n\nAngle unit requirement: All angular frequencies must be interpreted in radians per sample. There are no physical units in this problem.", "solution": "The problem is valid as it is scientifically grounded in digital signal processing and machine learning, well-posed with all necessary parameters defined, and objective in its formulation. The task is to implement a simulation to evaluate an ideal low-pass filter as a defense against the Fast Gradient Sign Method (FGSM) attack.\n\nThe solution proceeds in several steps: derivation of the necessary gradient for the attack, definition of the data generation and classification models, implementation of the low-pass filter defense, and execution of the simulation to compute the required performance metrics.\n\n**1. Data Generation and Classifier Model**\n\nThe input signals $\\mathbf{x} \\in \\mathbb{R}^{N}$ are generated based on a label $y \\in \\{-1, +1\\}$. Each signal is a sum of two sinusoids, $\\mathbf{s}_{\\text{low}}$ and $\\mathbf{s}_{\\text{high}}$, scaled by the label, with added Gaussian noise $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I})$:\n$$\n\\mathbf{x} = y \\left(A_{\\text{low}} \\,\\mathbf{s}_{\\text{low}} + A_{\\text{high}} \\,\\mathbf{s}_{\\text{high}}\\right) + \\boldsymbol{\\epsilon}\n$$\nThe basis vectors are defined as $\\mathbf{s}_{\\text{mode}}[n] = \\cos(2\\pi k_{\\text{mode}} n / N)$ for time index $n \\in \\{0, 1, \\dots, N-1\\}$.\n\nThe classifier is a linear model followed by a sign function, $\\hat{y} = \\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{x} + b)$. The weight vector $\\mathbf{w}$ is constructed to be a linear combination of the high- and low-frequency basis vectors, $\\mathbf{w} \\propto \\alpha \\mathbf{s}_{\\text{high}} + \\beta \\mathbf{s}_{\\text{low}}$, and is normalized to unit $\\ell_2$ norm. The bias is $b=0$. Given the parameters $\\alpha=1.0$ and $\\beta=0.15$, the classifier is primarily sensitive to the high-frequency component $\\mathbf{s}_{\\text{high}}$.\n\n**2. Adversarial Attack: Fast Gradient Sign Method (FGSM)**\n\nThe FGSM attack crafts an adversarial example $\\mathbf{x}_{\\text{adv}}$ by adding a small perturbation to the original input $\\mathbf{x}$ in the direction that maximizes the loss:\n$$\n\\mathbf{x}_{\\text{adv}} = \\mathbf{x} + \\varepsilon \\,\\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)\\right)\n$$\nwhere $\\mathcal{L}$ is the logistic loss and $\\varepsilon$ is the perturbation budget. To implement this, we must first derive the gradient of the loss with respect to the input, $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)$.\n\nThe logistic loss is $\\mathcal{L}(\\mathbf{x}, y) = \\log\\big(1 + \\exp(-y\\,z)\\big)$, where the logit is $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$.\nUsing the chain rule, $\\nabla_{\\mathbf{x}}\\mathcal{L} = \\frac{\\partial\\mathcal{L}}{\\partial z} \\frac{\\partial z}{\\partial \\mathbf{x}}$.\n\nThe derivative of the logit with respect to the input vector $\\mathbf{x}$ is:\n$$\n\\frac{\\partial z}{\\partial \\mathbf{x}} = \\nabla_{\\mathbf{x}}(\\mathbf{w}^{\\top}\\mathbf{x} + b) = \\mathbf{w}\n$$\nThe derivative of the loss with respect to the logit $z$ is:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial z} = \\frac{1}{1 + \\exp(-yz)} \\cdot \\frac{\\partial}{\\partial z}\\left(1 + \\exp(-yz)\\right) = \\frac{1}{1 + \\exp(-yz)} \\cdot \\left(-\\exp(-yz) \\cdot y\\right) = \\frac{-y \\exp(-yz)}{1 + \\exp(-yz)}\n$$\nMultiplying the numerator and denominator by $\\exp(yz)$ simplifies this to:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial z} = \\frac{-y}{ \\exp(yz) + 1}\n$$\nCombining these results, the gradient is:\n$$\n\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y) = \\left( \\frac{-y}{\\exp(y(\\mathbf{w}^{\\top}\\mathbf{x} + b)) + 1} \\right) \\mathbf{w}\n$$\nFor FGSM, we only need the sign of this gradient. The term in the parenthesis is a scalar. Its denominator, $\\exp(\\cdot) + 1$, is always positive. Therefore, its sign is determined by the numerator, $-y$.\n$$\n\\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)\\right) = \\mathrm{sign}\\left( \\left( \\frac{-y}{\\exp(\\dots)+1} \\right) \\mathbf{w} \\right) = \\mathrm{sign}(-y) \\cdot \\mathrm{sign}(\\mathbf{w})\n$$\nSince $y \\in \\{-1, +1\\}$, we have $\\mathrm{sign}(-y) = -y$. The perturbation is thus simplified to:\n$$\n\\varepsilon \\cdot \\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}\\right) = - \\varepsilon y \\cdot \\mathrm{sign}(\\mathbf{w})\n$$\nAnd the adversarial example is:\n$$\n\\mathbf{x}_{\\text{adv}} = \\mathbf{x} - \\varepsilon y \\,\\mathrm{sign}(\\mathbf{w})\n$$\nThis simplified form is computationally stable and will be used to generate the adversarial examples. The attack perturbs the input in the direction of $-\\mathrm{sign}(\\mathbf{w})$ for $y=+1$ and $+\\mathrm{sign}(\\mathbf{w})$ for $y=-1$, which maximally reduces the classifier's logit $z=\\mathbf{w}^\\top\\mathbf{x}$.\n\n**3. Defense Mechanism: Ideal Low-Pass Filter**\n\nThe defense preprocesses any input by applying an ideal low-pass filter. This is implemented in the frequency domain using the Discrete Fourier Transform (DFT). For a real-valued signal $\\mathbf{x}$ of length $N$, we use the one-sided Real DFT (`numpy.fft.rfft`), which yields $N/2+1$ complex coefficients $X_k$ for $k \\in \\{0, 1, \\dots, N/2\\}$. The angular frequency corresponding to coefficient $X_k$ is $\\omega_k = 2\\pi k / N$.\n\nGiven a cutoff angular frequency $\\omega_c$, the filter zeroes out all coefficients $X_k$ for which $\\omega_k > \\omega_c$. This condition is equivalent to $k > \\omega_c N / (2\\pi)$. After zeroing out the high-frequency components, the filtered signal is recovered by applying the inverse Real DFT (`numpy.fft.irfft`).\n\n**4. Computational Procedure and Evaluation**\n\nThe simulation is executed as follows:\n1.  **Setup**: The parameters ($N, M, k_{\\text{low}}, k_{\\text{high}}$, etc.) are defined. The basis vectors $\\mathbf{s}_{\\text{low}}, \\mathbf{s}_{\\text{high}}$ and the classifier weight vector $\\mathbf{w}$ are constructed.\n2.  **Data Generation**: A dataset of $M=400$ samples $(\\mathbf{X}, \\mathbf{y})$ is created according to the specified model, with balanced labels.\n3.  **Attack Generation**: For each clean sample $(\\mathbf{x}_i, y_i)$, the corresponding adversarial example $\\mathbf{x}_{\\text{adv}, i}$ is generated using the derived FGSM formula.\n4.  **Evaluation Loop**: The program iterates through the specified set of cutoff frequencies $\\{\\omega_c\\} = \\{\\pi, 0.8\\pi, 0.5\\pi, 0.2\\pi\\}$. For each $\\omega_c$:\n    a. Clean inputs $\\mathbf{X}$ are passed through the low-pass filter. The accuracy of the classifier on these filtered inputs is calculated (clean accuracy).\n    b. Adversarial inputs $\\mathbf{X}_{\\text{adv}}$ are passed through the same low-pass filter. The accuracy on these inputs is calculated (robust accuracy).\n5.  **Metric Calculation**: The baseline accuracies are those at $\\omega_c = \\pi$, which corresponds to no filtering as it is the Nyquist frequency. For each $\\omega_c$, the \"clean accuracy drop\" and \"robust accuracy gain\" are computed relative to this baseline.\n\nThis comprehensive procedure allows for a quantitative assessment of the trade-off introduced by the low-pass filtering defense: its potential to degrade performance on clean data versus its ability to improve robustness to adversarial attacks.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a low-pass filtering defense against an FGSM attack\n    on a linear classifier for 1D signals.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    N = 64\n    M = 400\n    k_low = 2\n    k_high = 16\n    A_low = 0.8\n    A_high = 0.7\n    sigma = 0.2\n    alpha = 1.0\n    beta = 0.15\n    b = 0.0\n    epsilon = 0.5\n    # Cutoff frequencies in radians per sample, in the specified order.\n    omega_cs = [math.pi, 0.8 * math.pi, 0.5 * math.pi, 0.2 * math.pi]\n\n    # Use a fixed random seed for reproducibility of the results.\n    np.random.seed(42)\n\n    # --- 1. Model and Data Component Construction ---\n    # Generate sinusoidal basis vectors.\n    n = np.arange(N)\n    s_low = np.cos(2 * np.pi * k_low * n / N)\n    s_high = np.cos(2 * np.pi * k_high * n / N)\n    \n    # Construct the classifier's weight vector.\n    w_unnormalized = alpha * s_high + beta * s_low\n    w = w_unnormalized / np.linalg.norm(w_unnormalized)\n\n    # --- 2. Dataset Generation ---\n    # Create a balanced set of labels.\n    num_pos = M // 2\n    num_neg = M - num_pos\n    y = np.array([1] * num_pos + [-1] * num_neg)\n    \n    # Create the base signal component (without noise).\n    signal_base = A_low * s_low + A_high * s_high\n    # Scale base signal by label y for each sample.\n    X_signal = np.outer(y, signal_base)\n    \n    # Add independent and identically distributed Gaussian noise.\n    noise = np.random.normal(0, sigma, (M, N))\n    X = X_signal + noise\n\n    # --- 3. Adversarial Attack Generation ---\n    # Use the simplified FGSM formula: x_adv = x - epsilon * y * sign(w)\n    y_reshaped = y[:, np.newaxis]  # For broadcasting\n    sign_w = np.sign(w)\n    \n    perturbation = -epsilon * y_reshaped * sign_w\n    X_adv = X + perturbation\n    \n    # --- 4. Defense and Evaluation ---\n    def ideal_low_pass_filter(signals, wc, N_len):\n        \"\"\"Applies an ideal low-pass filter to a batch of signals.\"\"\"\n        # If cutoff is at or above Nyquist, no filtering is needed.\n        if wc >= math.pi:\n            return signals\n\n        # Compute the one-sided real DFT of the signal batch.\n        fft_coeffs = np.fft.rfft(signals, axis=1)\n        \n        # Determine the cutoff index for frequency components.\n        # Angular frequency of bin k is omega_k = 2*pi*k/N.\n        # We zero out where k > wc*N/(2*pi).\n        k_cutoff_float = wc * N_len / (2 * np.pi)\n        \n        num_coeffs = fft_coeffs.shape[1]\n        freq_indices = np.arange(num_coeffs)\n        \n        # Apply the filter by zeroing out coefficients above the cutoff.\n        fft_coeffs[:, freq_indices > k_cutoff_float] = 0\n        \n        # Invert the DFT to get the filtered signals.\n        filtered_signals = np.fft.irfft(fft_coeffs, n=N_len, axis=1)\n        return filtered_signals\n\n    all_accuracies = {}\n    for wc in omega_cs:\n        # Evaluate performance on clean data with filtering.\n        X_filtered = ideal_low_pass_filter(X, wc, N)\n        logits_clean = (X_filtered @ w) + b\n        preds_clean = np.sign(logits_clean)\n        acc_clean = np.mean(preds_clean == y)\n        \n        # Evaluate performance on adversarial data with filtering.\n        X_adv_filtered = ideal_low_pass_filter(X_adv, wc, N)\n        logits_robust = (X_adv_filtered @ w) + b\n        preds_robust = np.sign(logits_robust)\n        acc_robust = np.mean(preds_robust == y)\n        \n        all_accuracies[wc] = {'clean': acc_clean, 'robust': acc_robust}\n\n    # --- 5. Metric Calculation and Final Output ---\n    # Get baseline accuracies (no filtering, wc = pi).\n    baseline_wc = math.pi\n    baseline_acc_clean = all_accuracies[baseline_wc]['clean']\n    baseline_acc_robust = all_accuracies[baseline_wc]['robust']\n    \n    results = []\n    for wc in omega_cs:\n        current_acc_clean = all_accuracies[wc]['clean']\n        current_acc_robust = all_accuracies[wc]['robust']\n        \n        clean_drop = baseline_acc_clean - current_acc_clean\n        robust_gain = current_acc_robust - baseline_acc_robust\n        \n        # Round to 4 decimal places as required.\n        clean_drop = round(clean_drop, 4)\n        robust_gain = round(robust_gain, 4)\n\n        # Ensure -0.0 is formatted as 0.0 for clean output.\n        if clean_drop == -0.0: clean_drop = 0.0\n        if robust_gain == -0.0: robust_gain = 0.0\n            \n        results.append([clean_drop, robust_gain])\n        \n    # Format the final list of lists into the required string format.\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_string)\n\nsolve()\n```", "id": "3098464"}, {"introduction": "A critical skill in adversarial ML is distinguishing true robustness from a mere illusion. Some \"defenses\" achieve apparent invulnerability simply by obscuring or shattering the gradients an attacker relies on—a phenomenon known as gradient masking. This practice [@problem_id:3097091] provides hands-on experience with a powerful method for detecting this failure mode: the transfer attack. By crafting adversarial examples on a standard model $f_\\theta$ and testing their effectiveness against a candidate defended model $g_\\phi$, you will learn to uncover false senses of security and build a more rigorous approach to evaluating adversarial defenses.", "problem": "Consider a binary classification setting with input vectors $\\mathbf{x} \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$. The learning process follows empirical risk minimization, where a model $f_\\theta$ with parameters $\\theta$ minimizes an empirical loss over training data. The standard base loss for binary classification is the logistic loss, defined by probabilities $p_\\theta(\\mathbf{x}) \\in (0,1)$ and the cross-entropy $L(\\mathbf{x},y;\\theta) = -y \\log p_\\theta(\\mathbf{x}) - (1-y) \\log(1 - p_\\theta(\\mathbf{x}))$. An adversarial example for a given input-label pair $(\\mathbf{x},y)$ and a norm budget $\\epsilon > 0$ is any perturbation $\\delta$ satisfying $\\|\\delta\\|_\\infty \\le \\epsilon$ that increases the loss $L(\\mathbf{x} + \\delta, y; \\theta)$. The adversarial crafting problem can be posed as constrained maximization: find $\\delta^\\star$ to maximize $L(\\mathbf{x} + \\delta, y; \\theta)$ over the set $\\{\\delta : \\|\\delta\\|_\\infty \\le \\epsilon\\}$. Transferability is the property that adversarial examples crafted to increase the loss of one model $f_\\theta$ also cause errors in a different model $g_\\phi$ trained on the same data distribution.\n\nGradient masking is a failure mode where gradients used for optimization or attack generation are rendered uninformative (for example by non-differentiable pre-processing or saturating functions), reducing white-box attack effectiveness without providing true robustness. A principle-based detection method is to compare white-box attack success on a candidate masked model against transfer attack success from a surrogate model whose gradients are informative. If adversarial examples crafted on a surrogate $f_\\theta$ substantially increase error in $g_\\phi$ while white-box attacks crafted on $g_\\phi$ itself have low effectiveness, this indicates gradient masking in $g_\\phi$.\n\nYour task is to write a complete program implementing the following experiment from first principles:\n\n- Construct two models on the same synthetic data:\n  1. A standard logistic regression classifier $f_\\theta$ that maps $\\mathbf{x} \\mapsto p_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, with parameters $\\theta = (\\mathbf{w}, b)$ trained by gradient descent to minimize the empirical cross-entropy.\n  2. A second classifier $g_\\phi$ trained on the same data, with three possible instantiations depending on the test case:\n     - A model with non-differentiable input pre-processing $h(\\mathbf{x})$ (e.g., elementwise sign) followed by logistic regression on the transformed inputs, producing $p_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{u}^\\top h(\\mathbf{x}) + c)$, with parameters $\\phi = (\\mathbf{u}, c)$ trained by gradient descent with respect to $(\\mathbf{u}, c)$ only. The gradient of the loss with respect to the input $\\mathbf{x}$ must be treated as zero for attack generation on $g_\\phi$.\n     - A standard logistic regression trained with adversarial training, where in each training epoch, inputs are replaced by adversarially perturbed inputs within an $\\ell_\\infty$ budget to minimize worst-case empirical loss.\n     - A standard logistic regression trained normally (no masking and no adversarial training).\n\n- Implement an $\\ell_\\infty$-constrained projected gradient ascent attack to craft adversarial examples, using the gradient of the loss with respect to $\\mathbf{x}$. The optimization should proceed by iteratively ascending along the sign of the gradient and projecting back to the $\\ell_\\infty$ ball of radius $\\epsilon$ around the original input. The attack must be implemented for arbitrary $\\epsilon > 0$, step size $\\alpha > 0$, and a fixed number of steps $T \\in \\mathbb{N}$.\n\n- For each test case, compute two empirical success rates:\n  1. The white-box attack success rate on $g_\\phi$: the fraction of inputs for which $g_\\phi$'s predicted label changes when attacked using $g_\\phi$'s own (possibly masked) gradients.\n  2. The transfer attack success rate from $f_\\theta$ to $g_\\phi$: the fraction of inputs for which $g_\\phi$'s predicted label changes when adversarial examples are crafted using $f_\\theta$'s gradients and then applied to $g_\\phi$.\n\n- Declare gradient masking detected for a test case if and only if the following two conditions both hold: the white-box success rate on $g_\\phi$ is less than a threshold $\\tau \\in (0,1)$ and the transfer success rate from $f_\\theta$ to $g_\\phi$ exceeds the white-box success rate on $g_\\phi$ by at least a margin $\\Delta \\in (0,1)$.\n\nData generation must be fully synthetic, statistically sound, and reproducible. Use a two-dimensional input space $d = 2$ with class-conditional Gaussian distributions: generate $n$ independent samples for each class, where class $y = 1$ has mean $\\boldsymbol{\\mu}_1 \\in \\mathbb{R}^2$ and diagonal covariance $\\operatorname{diag}(\\sigma^2, \\sigma^2)$, and class $y = 0$ has mean $\\boldsymbol{\\mu}_0 \\in \\mathbb{R}^2$ and the same covariance. Draw inputs, clip them to the range $[-1, 1]^2$, and use labels $y \\in \\{0,1\\}$ accordingly. Fix all random seeds inside your program to ensure determinism.\n\nYour program must implement the above and evaluate the following test suite of parameter values:\n\n- Test case $1$ (masked $g_\\phi$):\n  - Data: $n = 200$ per class, $\\boldsymbol{\\mu}_1 = (0.3, 0.3)$, $\\boldsymbol{\\mu}_0 = (-0.3, -0.3)$, $\\sigma = 0.6$.\n  - Attack: $\\epsilon = 1.0$, $\\alpha = 0.2$, $T = 10$.\n  - Model $f_\\theta$: standard logistic regression trained normally.\n  - Model $g_\\phi$: input pre-processing $h(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{x})$ (elementwise), logistic regression on $h(\\mathbf{x})$; gradient with respect to $\\mathbf{x}$ set to $0$ for attacks on $g_\\phi$.\n  - Detection parameters: $\\tau = 0.1$, $\\Delta = 0.2$.\n\n- Test case $2$ (adversarially trained $g_\\phi$):\n  - Data: $n = 200$ per class, $\\boldsymbol{\\mu}_1 = (0.7, 0.7)$, $\\boldsymbol{\\mu}_0 = (-0.7, -0.7)$, $\\sigma = 0.3$.\n  - Attack: $\\epsilon = 0.3$, $\\alpha = 0.05$, $T = 40$.\n  - Model $f_\\theta$: standard logistic regression trained normally.\n  - Model $g_\\phi$: standard logistic regression trained with adversarial training using the same $\\epsilon$ and $\\alpha$ during training for $50$ epochs.\n  - Detection parameters: $\\tau = 0.2$, $\\Delta = 0.2$.\n\n- Test case $3$ (both models normal):\n  - Data: $n = 200$ per class, $\\boldsymbol{\\mu}_1 = (0.2, 0.2)$, $\\boldsymbol{\\mu}_0 = (-0.2, -0.2)$, $\\sigma = 0.5$.\n  - Attack: $\\epsilon = 0.3$, $\\alpha = 0.05$, $T = 20$.\n  - Model $f_\\theta$: standard logistic regression trained normally.\n  - Model $g_\\phi$: standard logistic regression trained normally.\n  - Detection parameters: $\\tau = 0.2$, $\\Delta = 0.2$.\n\nRequired outputs:\n\n- For each test case $i \\in \\{1,2,3\\}$, compute a boolean indicating whether gradient masking is detected in $g_\\phi$ under the stated detection rule.\n- Your program should produce a single line of output containing the three booleans as a comma-separated list enclosed in square brackets, in order of the test cases, for example $[{\\rm True},{\\rm False},{\\rm True}]$.\n\nAll calculations are unitless. Angles are not used. Percentages must not be printed; only booleans are required. Your program must be self-contained, require no user input, and follow the specified output format exactly.", "solution": "The problem requires an implementation of an experiment to detect gradient masking, a failure mode in evaluating the robustness of machine learning models. The solution involves constructing, training, and attacking two binary classifiers, $f_\\theta$ and $g_\\phi$, and evaluating a specific detection criterion.\n\n### 1. Theoretical Framework\n\n**Binary Classification and Logistic Loss:**\nThe setting is a binary classification task with input vectors $\\mathbf{x} \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$. We use a logistic regression model, which calculates the probability of the positive class ($y=1$) as $p_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the sigmoid function and $\\theta = (\\mathbf{w}, b)$ are the model parameters. The model is trained by minimizing the empirical average of the binary cross-entropy (logistic) loss over a training dataset:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L(\\mathbf{x}_i, y_i; \\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -y_i \\log p_\\theta(\\mathbf{x}_i) - (1-y_i) \\log(1 - p_\\theta(\\mathbf{x}_i)) \\right]\n$$\nThis minimization is performed using gradient descent, where parameters are updated iteratively in the opposite direction of the loss gradient:\n$$\n\\nabla_\\mathbf{w} \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (p_\\theta(\\mathbf{x}_i) - y_i)\\mathbf{x}_i \\quad , \\quad \\nabla_b \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (p_\\theta(\\mathbf{x}_i) - y_i)\n$$\n\n**Adversarial Attacks and Gradient Masking:**\nAn adversarial example is a slightly perturbed input $\\mathbf{x}' = \\mathbf{x} + \\delta$ designed to fool a model. The perturbation $\\delta$ is constrained, typically within an $\\ell_\\infty$-norm ball: $\\|\\delta\\|_\\infty \\le \\epsilon$. The most effective perturbation within this constraint is found by maximizing the model's loss. A common method for this is Projected Gradient Ascent (PGA), which iteratively updates the input in the direction of the loss gradient with respect to the input:\n$$\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\cdot \\operatorname{sign}(\\nabla_{\\mathbf{x}^{(t)}} L(\\mathbf{x}^{(t)}, y; \\theta))\n$$\nwhere $\\alpha$ is the step size. The gradient of the loss with respect to a single input $\\mathbf{x}$ is given by:\n$$\n\\nabla_\\mathbf{x} L = (p_\\theta(\\mathbf{x}) - y)\\mathbf{w}\n$$\nAfter each step, the total perturbation is projected (clipped) to satisfy $\\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^{(0)}\\|_\\infty \\le \\epsilon$, and the resulting adversarial example is clipped to the valid input domain (e.g., $[-1, 1]^d$).\n\nGradient masking occurs when a model appears robust to such white-box attacks (where the attacker has full access to the model) because its loss surface is \"flat\" or its gradients are otherwise uninformative. This provides a false sense of security. A key indicator of gradient masking is high transferability: adversarial examples created for a different, non-masked model $f_\\theta$ are still effective against the candidate model $g_\\phi$, even when $g_\\phi$ seems robust to attacks based on its own gradients.\n\n### 2. Experimental Design\n\n**Data Generation:**\nWe generate a synthetic dataset in $\\mathbb{R}^2$ from two class-conditional Gaussian distributions. For a number of samples $n$ per class, class $y=1$ is drawn from $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2\\mathbf{I})$ and class $y=0$ from $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2\\mathbf{I})$. All generated points are clipped to the hypercube $[-1, 1]^2$. All random processes are seeded for reproducibility.\n\n**Model Configurations:**\n- **Surrogate Model $f_\\theta$**: A standard logistic regression classifier trained via gradient descent on the generated data. This model serves as the source for transfer attacks.\n\n- **Target Model $g_\\phi$**: This model takes one of three forms depending on the test case:\n    1.  **Masked Model**: Employs a non-differentiable pre-processing step $h(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{x})$. The model is $p_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{u}^\\top h(\\mathbf{x}) + c)$. It is trained normally via gradient descent with respect to its parameters $\\phi = (\\mathbf{u}, c)$ on the transformed data. For the purpose of crafting a white-box attack on this model, the problem specifies that the gradient of the loss with respect to the input $\\mathbf{x}$ is treated as zero ($\\nabla_\\mathbf{x} L = \\mathbf{0}$). This directly simulates gradient masking.\n    2.  **Adversarially Trained Model**: A standard logistic regression model trained to achieve true robustness. The training objective is an empirical worst-case loss minimization:\n        $$\n        \\min_{\\phi} \\frac{1}{N} \\sum_{i=1}^N \\max_{\\|\\delta_i\\|_\\infty \\le \\epsilon} L(\\mathbf{x}_i + \\delta_i, y_i; \\phi)\n        $$\n        The inner maximization is approximated using PGA for a fixed number of steps at each epoch of training.\n    3.  **Normal Model**: A standard logistic regression classifier identical in architecture and training to $f_\\theta$.\n\n**Evaluation and Detection Logic:**\nFor each test case, we compute two metrics:\n1.  **White-box Success Rate**: The fraction of samples for which the prediction of $g_\\phi$ changes when attacked using its own gradients.\n    $$\n    R_{\\text{white-box}} = \\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} \\mathbb{I} \\left[ g_\\phi(\\mathbf{x}_i^{\\text{adv-g}}) \\neq g_\\phi(\\mathbf{x}_i) \\right]\n    $$\n    where $\\mathbf{x}_i^{\\text{adv-g}}$ is the adversarial example crafted from $\\mathbf{x}_i$ using $g_\\phi$'s gradients.\n2.  **Transfer Success Rate**: The fraction of samples for which the prediction of $g_\\phi$ changes when subjected to adversarial examples crafted using the gradients of the surrogate model $f_\\theta$.\n    $$\n    R_{\\text{transfer}} = \\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} \\mathbb{I} \\left[ g_\\phi(\\mathbf{x}_i^{\\text{adv-f}}) \\neq g_\\phi(\\mathbf{x}_i) \\right]\n    $$\n    where $\\mathbf{x}_i^{\\text{adv-f}}$ is crafted from $\\mathbf{x}_i$ using $f_\\theta$'s gradients.\n\nGradient masking is detected if and only if both conditions are met:\n- $R_{\\text{white-box}} < \\tau$\n- $R_{\\text{transfer}} > R_{\\text{white-box}} + \\Delta$\n\n### 3. Expected Outcomes\n\n- **Test Case 1 (Masked $g_\\phi$)**: The white-box attack on $g_\\phi$ will fail because its input gradient is defined as zero, yielding $R_{\\text{white-box}} = 0$. The transfer attack from the standard model $f_\\theta$ is expected to be effective because the perturbations can alter the signs of the input features, changing the output of $g_\\phi$'s pre-processor $h(\\mathbf{x})$. Thus, we expect $R_{\\text{transfer}} > \\Delta$. The conditions for detection should be met, resulting in `True`.\n- **Test Case 2 (Adversarially Trained $g_\\phi$)**: Adversarial training should confer true robustness. We expect $R_{\\text{white-box}}$ to be low, possibly below $\\tau$. However, this robustness should also reduce the effectiveness of transfer attacks, so $R_{\\text{transfer}}$ is expected to be low as well, and likely not exceed $R_{\\text{white-box}}$ by the margin $\\Delta$. The detection should fail, resulting in `False`.\n- **Test Case 3 (Normal $g_\\phi$)**: Both $f_\\theta$ and $g_\\phi$ are standard, vulnerable models. The white-box attack on $g_\\phi$ should be highly effective, so $R_{\\text{white-box}}$ will likely be greater than $\\tau$. This will cause the first condition for detection to fail, resulting in `False`.\n\nThe implementation proceeds by creating a `LogisticRegression` class capable of handling all three model types, a `pga_attack` function, and a main loop to execute the logic for each test case as described.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are permitted, not even scipy.\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient masking detection experiment.\n    \"\"\"\n\n    class LogisticRegression:\n        \"\"\"\n        A logistic regression classifier with configurable training and attack modes.\n        \"\"\"\n        def __init__(self, model_type='normal', preprocessor=None):\n            if model_type not in ['normal', 'masked', 'adversarial']:\n                raise ValueError(\"Invalid model_type\")\n            self.model_type = model_type\n            self.preprocessor = preprocessor\n            self.w = None\n            self.b = None\n\n        def _sigmoid(self, z):\n            # Clip z to prevent overflow in np.exp\n            z = np.clip(z, -500, 500)\n            return 1 / (1 + np.exp(-z))\n\n        def predict_proba(self, X):\n            X_proc = self.preprocessor(X) if self.preprocessor else X\n            z = X_proc @ self.w + self.b\n            return self._sigmoid(z)\n\n        def predict(self, X):\n            return (self.predict_proba(X) >= 0.5).astype(int)\n\n        def loss(self, X, y):\n            p = self.predict_proba(X)\n            # Add a small epsilon to avoid log(0)\n            epsilon = 1e-9\n            p = np.clip(p, epsilon, 1 - epsilon)\n            return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n\n        def fit(self, X, y, lr, epochs, adv_params=None):\n            X_proc = self.preprocessor(X) if self.preprocessor else X\n            n_samples, n_features = X_proc.shape\n            \n            # Initialize weights\n            self.w = np.zeros((n_features, 1))\n            self.b = 0.0\n\n            # Reshape y to be a column vector if it's not\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n\n            for epoch in range(epochs):\n                if self.model_type == 'adversarial' and adv_params:\n                    # Adversarial Training: generate adversarial examples for this epoch\n                    X_adv = self._pga_attack(X, y, **adv_params)\n                    X_train = X_adv\n                else:\n                    X_train = X\n\n                # Use preprocessor if the model is 'masked'\n                X_train_proc = self.preprocessor(X_train) if self.preprocessor else X_train\n\n                p = self.predict_proba(X_train) # Probabilities on original X for grad, but update on adversarial\n                \n                # Full-batch gradient descent\n                dw = (1 / n_samples) * X_train_proc.T @ (p - y)\n                db = (1 / n_samples) * np.sum(p - y)\n\n                self.w -= lr * dw\n                self.b -= lr * db\n        \n        def _gradient_wrt_input(self, X, y):\n            \"\"\"Computes the gradient of the loss with respect to the input X.\"\"\"\n            if self.model_type == 'masked':\n                return np.zeros_like(X)\n\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n\n            p = self.predict_proba(X)\n            grad = (p - y) @ self.w.T\n            return grad\n\n        def _pga_attack(self, X, y, epsilon, alpha, T):\n            \"\"\"Projected Gradient Ascent attack.\"\"\"\n            X_adv = X.copy()\n            X_orig = X.copy()\n\n            for _ in range(T):\n                grad = self._gradient_wrt_input(X_adv, y)\n                X_adv = X_adv + alpha * np.sign(grad)\n                \n                # Project perturbation back to epsilon-ball\n                delta = X_adv - X_orig\n                delta = np.clip(delta, -epsilon, epsilon)\n                X_adv = X_orig + delta\n                \n                # Clip to valid input range [-1, 1]\n                X_adv = np.clip(X_adv, -1.0, 1.0)\n            \n            return X_adv\n\n    def generate_data(n_per_class, mu1, mu0, sigma, seed):\n        \"\"\"Generates synthetic 2D data from two Gaussian distributions.\"\"\"\n        np.random.seed(seed)\n        cov = np.diag([sigma**2, sigma**2])\n        \n        X1 = np.random.multivariate_normal(mu1, cov, n_per_class)\n        y1 = np.ones(n_per_class)\n        \n        X0 = np.random.multivariate_normal(mu0, cov, n_per_class)\n        y0 = np.zeros(n_per_class)\n        \n        X = np.vstack((X1, X0))\n        y = np.hstack((y1, y0))\n\n        # Clip data to [-1, 1]^2\n        X = np.clip(X, -1.0, 1.0)\n        \n        # Shuffle data\n        perm = np.random.permutation(2 * n_per_class)\n        return X[perm], y[perm]\n\n    test_cases = [\n        # Case 1: Masked g_phi\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.3, 0.3), 'mu0': (-0.3, -0.3), 'sigma': 0.6},\n            'attack_params': {'epsilon': 1.0, 'alpha': 0.2, 'T': 10},\n            'g_phi_type': 'masked',\n            'g_phi_preprocessor': np.sign,\n            'detection_params': {'tau': 0.1, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000},\n            'g_phi_adv_train_params': None,\n            'seed': 42\n        },\n        # Case 2: Adversarially trained g_phi\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.7, 0.7), 'mu0': (-0.7, -0.7), 'sigma': 0.3},\n            'attack_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 40},\n            'g_phi_type': 'adversarial',\n            'g_phi_preprocessor': None,\n            'detection_params': {'tau': 0.2, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000}, # For f_theta\n            'g_phi_adv_train_params': {'lr':0.1, 'epochs':50, 'adv_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 10}},\n            'seed': 43\n        },\n        # Case 3: Both models normal\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.2, 0.2), 'mu0': (-0.2, -0.2), 'sigma': 0.5},\n            'attack_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 20},\n            'g_phi_type': 'normal',\n            'g_phi_preprocessor': None,\n            'detection_params': {'tau': 0.2, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000},\n            'g_phi_adv_train_params': None,\n            'seed': 44\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Data Generation\n        X, y = generate_data(**case['data_params'], seed=case['seed'])\n        \n        # 2. Train f_theta (always standard)\n        f_theta = LogisticRegression(model_type='normal')\n        f_theta.fit(X, y, **case['train_params'])\n\n        # 3. Train g_phi (type depends on case)\n        g_phi = LogisticRegression(model_type=case['g_phi_type'], preprocessor=case['g_phi_preprocessor'])\n        if g_phi.model_type == 'adversarial':\n            g_phi.fit(X, y, **case['g_phi_adv_train_params'])\n        else:\n            g_phi.fit(X, y, **case['train_params'])\n\n        # 4. Evaluation\n        y_pred_g_orig = g_phi.predict(X)\n\n        # 4a. White-box attack on g_phi\n        X_adv_g = g_phi._pga_attack(X, y, **case['attack_params'])\n        y_pred_g_whitebox = g_phi.predict(X_adv_g)\n        whitebox_rate = np.mean(y_pred_g_orig != y_pred_g_whitebox)\n        \n        # 4b. Transfer attack from f_theta to g_phi\n        X_adv_f = f_theta._pga_attack(X, y, **case['attack_params'])\n        y_pred_g_transfer = g_phi.predict(X_adv_f)\n        transfer_rate = np.mean(y_pred_g_orig != y_pred_g_transfer)\n\n        # 5. Detection Logic\n        tau = case['detection_params']['tau']\n        Delta = case['detection_params']['Delta']\n        is_masked = (whitebox_rate  tau) and (transfer_rate > whitebox_rate + Delta)\n        results.append(is_masked)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda b: str(b).capitalize(), results))}]\")\n\nsolve()\n```", "id": "3097091"}]}