{"hands_on_practices": [{"introduction": "The effectiveness of a search strategy often depends on the geometry of the problem landscape. This first exercise provides a powerful, hands-on demonstration of a key scenario where random search outperforms grid search [@problem_id:3133087]. By creating a synthetic loss function with a narrow, diagonal \"valley\" of optimal values, you will see firsthand how an axis-aligned grid can completely miss the target, while random sampling effectively explores the space and finds better solutions.", "problem": "Consider hyperparameter optimization in deep learning as the task of minimizing an empirical risk function over a hyperparameter space. Let the hyperparameter space be the rectangle $\\mathcal{H} = [\\eta_{\\min}, \\eta_{\\max}] \\times [\\lambda_{\\min}, \\lambda_{\\max}]$. The empirical risk induced by hyperparameters can be abstracted as a synthetic landscape $f(\\eta, \\lambda)$ designed to exhibit sharp ridges. Assume the following class of landscapes with a single elongated valley (ridge of low loss) oriented by an angle $\\theta$ in radians:\n$$\nf(\\eta, \\lambda) = \\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2} + \\beta \\left(\\eta^2 + \\lambda^2\\right),\n$$\nwhere $c$ is a scalar offset locating the ridge, $w > 0$ is the ridge half-width controlling sharpness, and $\\beta > 0$ is a bowl curvature coefficient. The objective is to minimize $f(\\eta, \\lambda)$.\n\nFundamental base:\n- The hyperparameter search problem is to find $\\arg\\min_{(\\eta,\\lambda) \\in \\mathcal{H}} f(\\eta,\\lambda)$ under a fixed evaluation budget. This is grounded in the empirical risk minimization principle and the definition of grid and random search.\n- Axis-aligned grid search evaluates $f$ on an axis-aligned lattice of equally spaced points. Specifically, choose $n_\\eta$ points for $\\eta$ and $n_\\lambda$ points for $\\lambda$, and evaluate all $n_\\eta \\cdot n_\\lambda$ combinations.\n- Random search evaluates $f$ on $N$ points sampled independently and identically distributed (i.i.d.) uniformly from $\\mathcal{H}$. Use a seeded Random Number Generator (RNG) to ensure reproducible sampling.\n\nYou must implement both axis-aligned grid search and random search on the synthetic landscapes and demonstrate cases where the axis-aligned grid can fail while random search succeeds when ridges are misaligned with the axes. The notion of success is defined by finding a function value at or below a specified threshold $\\tau$, which represents “near-optimal” loss for the test case.\n\nAngle unit specification: all angles (the variable $\\theta$) must be expressed in radians.\n\nImplement the following test suite of three cases. For each case, the program must:\n- Construct $f(\\eta,\\lambda)$ with the given parameters.\n- Run axis-aligned grid search with the specified budget $n_\\eta$ and $n_\\lambda$.\n- Run random search with the specified budget $N$ and random seed.\n- Compute a boolean indicating whether random search succeeds while grid search fails for that case.\n\nHyperparameter domain for all tests: $[\\eta_{\\min}, \\eta_{\\max}] = [-1, 1]$ and $[\\lambda_{\\min}, \\lambda_{\\max}] = [-1, 1]$.\n\nTest cases:\n- Case A (axis-aligned ridge; “happy path”):\n  - $\\theta = 0$, $c = 0$, $w = 0.25$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 49$, seed $= 123$, threshold $\\tau = 0.5$.\n  - Interpretation: ridge is aligned with the $\\eta$ axis and sufficiently wide. Both methods should succeed.\n- Case B (diagonal ridge; misaligned; grid fails, random succeeds):\n  - $\\theta = \\pi/4$, $c = 0.1$, $w = 0.03$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 200$, seed $= 456$, threshold $\\tau = 1.0$.\n  - Interpretation: ridge is narrow and misaligned. Axis-aligned grid samples cannot get sufficiently close; random search with a larger budget should succeed.\n- Case C (diagonal ridge; extreme sharpness; both fail):\n  - $\\theta = \\pi/4$, $c = 0.1$, $w = 0.004$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 49$, seed $= 789$, threshold $\\tau = 0.4$.\n  - Interpretation: ridge is extremely sharp, making it unlikely that either method meets the threshold with the given budget.\n\nSuccess criterion for a method in a case: its minimum observed value $\\min f$ over evaluated points is $\\le \\tau$ for that case.\n\nOutput specification:\n- For each case, produce a boolean that is true if and only if random search succeeds and grid search fails.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example: \"[True,False,True]\".", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical optimization and machine learning, specifically hyperparameter tuning. The problem is well-posed, providing a clearly defined mathematical function, unambiguous descriptions of two search algorithms (grid search and random search), and a complete set of parameters for three distinct test cases. The objective is precise and the success criterion is formally defined. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe core of this problem is to compare the efficacy of axis-aligned grid search and uniform random search for minimizing a synthetic loss function, $f(\\eta, \\lambda)$, over a bounded hyperparameter domain $\\mathcal{H} = [-1, 1] \\times [-1, 1]$. The function is given by:\n$$\nf(\\eta, \\lambda) = \\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2} + \\beta \\left(\\eta^2 + \\lambda^2\\right)\n$$\nThis function is designed to model challenging landscapes encountered in hyperparameter optimization. It consists of two main components:\n$1$. The term $\\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2}$ defines a parabolic valley. The minimum of this term, which is $0$, occurs along the line defined by $\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c = 0$. This line represents a \"ridge\" of low loss values. The parameter $\\theta$ controls the orientation of this ridge in the $(\\eta, \\lambda)$-plane, $c$ determines its offset from the origin, and $w > 0$ controls its width. A smaller $w$ corresponds to a sharper, narrower ridge, making it a smaller target for any search algorithm.\n$2$. The term $\\beta \\left(\\eta^2 + \\lambda^2\\right)$ adds a convex, bowl-shaped component to the landscape, where $\\beta > 0$ is a curvature coefficient. This term acts as a form of regularization, ensuring that a unique global minimum exists and is located near the origin $(0,0)$. The final minimum of $f$ is a balance between being on the ridge and being close to the origin.\n\nThe two search algorithms are implemented as follows:\n\n**Axis-Aligned Grid Search:** This method evaluates the function $f(\\eta, \\lambda)$ at points forming a regular, axis-aligned grid. For a budget of $n_\\eta$ points for the $\\eta$ hyperparameter and $n_\\lambda$ for the $\\lambda$ hyperparameter, we generate two sets of equally spaced points:\n$$\n\\eta_i = -1 + i \\frac{2}{n_\\eta - 1}, \\quad i = 0, 1, \\dots, n_\\eta - 1\n$$\n$$\n\\lambda_j = -1 + j \\frac{2}{n_\\lambda - 1}, \\quad j = 0, 1, \\dots, n_\\lambda - 1\n$$\nThe function is then evaluated at all $n_\\eta \\times n_\\lambda$ pairs $(\\eta_i, \\lambda_j)$. The key limitation of this method is its sampling pattern. If a narrow ridge of low loss is not aligned with the grid axes (i.e., $\\theta$ is not a multiple of $\\pi/2$), it is possible for all grid points to fall on the high-loss slopes of the ridge, effectively \"missing\" the optimal region.\n\n**Random Search:** This method evaluates $f(\\eta, \\lambda)$ at $N$ points sampled independently and uniformly from the search space $\\mathcal{H}$. Each point $(\\eta_k, \\lambda_k)$ for $k=1, \\dots, N$ is drawn from a uniform distribution over $[-1, 1] \\times [-1, 1]$. The primary advantage of random search, as highlighted by Bergstra and Bengio (2012), is its robustness to the geometry of the loss landscape. By sampling randomly, it is not constrained by a fixed axis alignment. For any sub-region of the search space with a non-zero area (such as a narrow ridge), the probability of placing a sample within it increases with the number of samples $N$. This makes it statistically more likely to find a good solution when the \"effective dimensionality\" of the problem is low (i.e., when the function's value is primarily determined by a small subset or combination of parameters), as is the case with a landscape dominated by a ridge.\n\nThe evaluation of the test cases proceeds as follows:\n\n**Case A:** $\\theta = 0$, $w = 0.25$. The ridge is aligned with the $\\lambda$-axis (since $\\cos(0)=1, \\sin(0)=0$, the ridge is at $\\eta-c=0$, i.e., $\\eta=0$). The grid is perfectly suited to sample along this axis. The ridge is also relatively wide ($w=0.25$). Both grid search and random search are expected to find points with low $f$ values, thus succeeding. The condition `random_succeeds and not grid_succeeds` will be `False`.\n\n**Case B:** $\\theta = \\pi/4$, $w = 0.03$. The ridge is diagonal and narrow. The grid points for $n_\\eta = 7, n_\\lambda = 7$ are spaced by a distance of $2/(7-1) \\approx 0.333$ along each axis. Given the narrowness of the ridge ($w=0.03$), it is highly probable that all $49$ grid points lie far from the minimal-loss line, resulting in high function values. Grid search is thus expected to fail. Random search, with a larger budget of $N=200$ samples, has a significantly higher probability of placing at least one sample within the narrow diagonal valley, thereby achieving a function value below the threshold $\\tau=1.0$. The condition is expected to be `True`.\n\n**Case C:** $\\theta = \\pi/4$, $w = 0.004$. The ridge is diagonal and extremely sharp. The effective area of the low-loss region is exceptionally small. With a limited budget of $N=49$ samples, the probability of a random point landing within this tiny region is very low. Likewise, the grid search will fail for the same reasons as in Case B. Therefore, it is expected that both methods will fail to find a point with a value below the threshold $\\tau=0.4$. The condition will be `False`.\n\nThe implementation will compute the minimum function value found by each method for each case and compare it to the specified threshold $\\tau$ to determine success or failure, ultimately computing the required boolean output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares grid search and random search on synthetic \n    hyperparameter optimization landscapes.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"theta\": 0.0,\n            \"c\": 0.0,\n            \"w\": 0.25,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 49,\n            \"seed\": 123,\n            \"tau\": 0.5\n        },\n        {\n            \"name\": \"Case B\",\n            \"theta\": np.pi / 4.0,\n            \"c\": 0.1,\n            \"w\": 0.03,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 200,\n            \"seed\": 456,\n            \"tau\": 1.0\n        },\n        {\n            \"name\": \"Case C\",\n            \"theta\": np.pi / 4.0,\n            \"c\": 0.1,\n            \"w\": 0.004,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 49,\n            \"seed\": 789,\n            \"tau\": 0.4\n        }\n    ]\n\n    def landscape_function(eta, lamb, theta, c, w, beta):\n        \"\"\"\n        Calculates the value of the synthetic loss function f(eta, lambda).\n        This function is vectorized to work with numpy arrays.\n        \"\"\"\n        term1_numerator = np.cos(theta) * eta + np.sin(theta) * lamb - c\n        term1 = (term1_numerator**2) / (w**2)\n        term2 = beta * (eta**2 + lamb**2)\n        return term1 + term2\n\n    def run_grid_search(params):\n        \"\"\"\n        Performs an axis-aligned grid search and checks for success.\n        \"\"\"\n        eta_points = np.linspace(-1.0, 1.0, params['n_eta'])\n        lambda_points = np.linspace(-1.0, 1.0, params['n_lambda'])\n        eta_grid, lambda_grid = np.meshgrid(eta_points, lambda_points)\n\n        f_values = landscape_function(\n            eta_grid, lambda_grid,\n            params['theta'], params['c'], params['w'], params['beta']\n        )\n\n        min_f = np.min(f_values)\n        return min_f = params['tau']\n\n    def run_random_search(params):\n        \"\"\"\n        Performs a random search and checks for success.\n        Uses a fixed seed for reproducibility.\n        \"\"\"\n        np.random.seed(params['seed'])\n        eta_samples = np.random.uniform(-1.0, 1.0, size=params['N'])\n        lambda_samples = np.random.uniform(-1.0, 1.0, size=params['N'])\n\n        f_values = landscape_function(\n            eta_samples, lambda_samples,\n            params['theta'], params['c'], params['w'], params['beta']\n        )\n\n        min_f = np.min(f_values)\n        return min_f = params['tau']\n\n    results = []\n    for case in test_cases:\n        # Run both search methods for the current case\n        grid_succeeds = run_grid_search(case)\n        random_succeeds = run_random_search(case)\n        \n        # Determine if random search succeeds AND grid search fails\n        case_result = random_succeeds and not grid_succeeds\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map to str is necessary as the list contains booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3133087"}, {"introduction": "A common temptation in hyperparameter tuning is to simplify the problem by tuning one parameter at a time, a method known as blocked or one-at-a-time search. This exercise simulates this exact scenario to reveal its critical flaw: it can easily get \"stuck\" in a suboptimal region by failing to account for interactions between parameters [@problem_id:3133107]. By comparing this blocked approach to a proper random search that samples hyperparameters jointly, you will quantify the advantage of exploring the entire search space simultaneously.", "problem": "You are asked to write a complete, deterministic program that simulates and quantifies the effect of blocking in grid search when one hyperparameter is fixed at a suboptimal value, and to compare it to random search’s capacity to escape such blocks. The setting is a synthetic validation-loss surface relevant to deep learning hyperparameter tuning, with two scalar hyperparameters: learning rate denoted by $\\,\\eta\\,$ and weight decay denoted by $\\,\\lambda\\,$. The goal is to minimize the loss function $\\,f(\\eta,\\lambda)\\,$.\n\nFundamental base and definitions:\n- A search strategy evaluates a set of hyperparameter pairs and returns the best observed value of $\\,f\\,$. \n- Grid search with blocking: one hyperparameter is fixed at a chosen value $\\,\\lambda_{\\mathrm{fix}}\\,$ and the other hyperparameter $\\,\\eta\\,$ is evaluated at $\\,N\\,$ grid points.\n- Random search: both hyperparameters are sampled jointly at $\\,N\\,$ independent points from a specified distribution.\n\nSynthetic objective:\n- Define $\\,x = \\log_{10}(\\eta)\\,$ and $\\,y = \\log_{10}(\\lambda)\\,$.\n- Let the search domain be $\\,x \\in [-4,0]\\,$ and $\\,y \\in [-5,-1]\\,$, which corresponds to $\\,\\eta \\in [10^{-4},10^{0}]\\,$ and $\\,\\lambda \\in [10^{-5},10^{-1}]\\,$.\n- Define\n$$\nf(\\eta,\\lambda) \\;=\\; (x+2)^2 \\;+\\; 2.5\\,(y+3)^2 \\;+\\; 0.3\\,(x+2)(y+3) \\;+\\; 0.1\\,\\sin(5x)\\,\\sin(5y),\n$$\nwith $\\,x=\\log_{10}(\\eta)\\,$ and $\\,y=\\log_{10}(\\lambda)\\,$. This function encodes a basin near $\\,x=-2\\,$ and $\\,y=-3\\,$ (that is, near $\\,\\eta=10^{-2}\\,$ and $\\,\\lambda=10^{-3}\\,$) with mild interaction and ripples.\n\nAlgorithms to implement:\n- Grid search with blocking:\n  - Given a budget $\\,N\\,$ and a fixed $\\,\\lambda_{\\mathrm{fix}}\\,$, construct $\\,N\\,$ values of $\\,\\eta\\,$ by choosing $\\,N\\,$ equally spaced points in $\\,x=\\log_{10}(\\eta)\\,$ over the closed interval $\\,[-4,0]\\,$ (that is, including both endpoints), then mapping each $\\,x\\,$ to $\\,\\eta=10^x\\,$.\n  - Evaluate $\\,f(\\eta,\\lambda_{\\mathrm{fix}})\\,$ at these $\\,N\\,$ points and report the minimum value. When $\\,N=1\\,$, the single grid point is the left endpoint $\\,x=-4\\,$.\n- Random search:\n  - Given a budget $\\,N\\,$ and a random seed $\\,s\\,$, draw $\\,N\\,$ independent samples with $\\,x \\sim \\mathrm{Uniform}([-4,0])\\,$ and $\\,y \\sim \\mathrm{Uniform}([-5,-1])\\,$. Map to $\\,\\eta=10^x\\,$ and $\\,\\lambda=10^y\\,$.\n  - Evaluate $\\,f(\\eta,\\lambda)\\,$ at these $\\,N\\,$ points and report the minimum value.\n\nQuantity to report for each test case:\n- Define the advantage of random search over blocked grid search as\n$$\n\\Delta \\;=\\; f_{\\min}^{\\mathrm{grid}} \\;-\\; f_{\\min}^{\\mathrm{rand}},\n$$\nwhere $\\,f_{\\min}^{\\mathrm{grid}}\\,$ is the best value found by the blocked grid search and $\\,f_{\\min}^{\\mathrm{rand}}\\,$ is the best value found by random search under the same budget $\\,N\\,$. A positive $\\,\\Delta\\,$ indicates that random search achieved a lower (better) loss.\n\nTest suite:\n- Use the following test cases, each specified as a triple $\\,(\\,N,\\,\\lambda_{\\mathrm{fix}},\\,s\\,)\\,$:\n  - Case $\\,1$: $\\,(\\,30,\\,10^{-1},\\,2027\\,)\\,$.\n  - Case $\\,2$: $\\,(\\,1,\\,10^{-1},\\,123\\,)\\,$.\n  - Case $\\,3$: $\\,(\\,30,\\,10^{-3},\\,2027\\,)\\,$.\n  - Case $\\,4$: $\\,(\\,10,\\,10^{-2},\\,42\\,)\\,$.\nFor each case, compute $\\,\\Delta\\,$ as defined above using exactly the algorithms and the domain stated. Randomness must be made deterministic by seeding the generator with $\\,s\\,$ for that case.\n\nFinal output format:\n- Your program should produce a single line of output containing the four results $\\,\\Delta\\,$ for the test cases, in order, formatted as a comma-separated list enclosed in square brackets. Each number should be formatted as a fixed-point decimal with exactly $\\,6\\,$ digits after the decimal point, for example $\\,\\bigl[0.123456,-0.010000,0.000001,0.250000\\bigr]\\,$.", "solution": "The problem is valid as it is scientifically grounded in the principles of hyperparameter optimization, is mathematically well-posed, and provides a complete and unambiguous specification for all required components. A deterministic solution can be derived by implementing the specified algorithms and objective function.\n\nThe core of the problem is to compare the performance of two hyperparameter search strategies, blocked grid search and random search, on a synthetic validation loss surface. The performance is quantified by the advantage $\\Delta$, which measures how much better (lower) the loss found by random search is compared to blocked grid search.\n\nThe analysis is performed in the logarithmic space of the hyperparameters, a standard practice in machine learning for parameters like learning rate and weight decay that often have a multiplicative effect on performance. The hyperparameters are the learning rate $\\eta$ and weight decay $\\lambda$. Their logarithmic transformations are $x = \\log_{10}(\\eta)$ and $y = \\log_{10}(\\lambda)$.\n\nThe search domain is defined as a rectangle in this log-space: $x \\in [-4, 0]$ and $y \\in [-5, -1]$. This corresponds to $\\eta \\in [10^{-4}, 10^0]$ and $\\lambda \\in [10^{-5}, 10^{-1}]$.\n\nThe synthetic loss function to be minimized is given by:\n$$f(x, y) = (x+2)^2 + 2.5(y+3)^2 + 0.3(x+2)(y+3) + 0.1\\sin(5x)\\sin(5y)$$\nThis function is a quadratic bowl centered near $(x,y)=(-2,-3)$, which represents a region of good hyperparameters, with an interaction term and a small, high-frequency sinusoidal component to mimic the complex, non-convex nature of real loss landscapes.\n\nThe solution proceeds by first implementing this function and then implementing the two search algorithms as specified.\n\n**1. Blocked Grid Search Algorithm**\n\nThis algorithm simulates a common but potentially flawed tuning strategy where one hyperparameter is fixed while the other is varied.\nGiven a budget of $N$ evaluations and a fixed hyperparameter value $\\lambda_{\\mathrm{fix}}$, the corresponding value in log-space is $y_{\\mathrm{fix}} = \\log_{10}(\\lambda_{\\mathrm{fix}})$. The search is then restricted to the line $y = y_{\\mathrm{fix}}$ within the search domain.\n\nThe algorithm generates $N$ evaluation points for the other hyperparameter, $x$, by creating a uniform grid over its entire range, $[-4, 0]$.\n- If $N>1$, the grid points are generated by `linspace`, creating $N$ equally spaced points from $-4$ to $0$ inclusive. The points are $x_i = -4 + i \\cdot \\frac{4}{N-1}$ for $i \\in \\{0, 1, \\dots, N-1\\}$.\n- If $N=1$, the problem specifies that the single point evaluated is the left endpoint of the interval, $x_0 = -4$.\n\nThe loss function $f(x, y_{\\mathrm{fix}})$ is evaluated at each of these $N$ grid points $(x_i, y_{\\mathrm{fix}})$. The minimum value found among these evaluations is the result of the search, $f_{\\min}^{\\mathrm{grid}}$.\n$$f_{\\min}^{\\mathrm{grid}} = \\min_{i \\in \\{0, \\dots, N-1\\}} f(x_i, y_{\\mathrm{fix}})$$\n\n**2. Random Search Algorithm**\n\nThis algorithm explores the hyperparameter space more broadly by sampling both hyperparameters simultaneously.\nGiven a budget of $N$ evaluations and a random seed $s$ for reproducibility, the algorithm proceeds as follows:\nFirst, a pseudo-random number generator is seeded with $s$. Then, $N$ independent points $(x_j, y_j)$ for $j \\in \\{1, \\dots, N\\}$ are sampled from the search domain. The sampling distributions are uniform over their respective intervals:\n$$x_j \\sim \\mathrm{Uniform}([-4, 0])$$\n$$y_j \\sim \\mathrm{Uniform}([-5, -1])$$\nThe loss function $f(x_j, y_j)$ is evaluated at each of these $N$ randomly sampled points. The minimum value found is the result of the search, $f_{\\min}^{\\mathrm{rand}}$.\n$$f_{\\min}^{\\mathrm{rand}} = \\min_{j \\in \\{1, \\dots, N\\}} f(x_j, y_j)$$\n\n**3. Calculation of Advantage and Execution for Test Cases**\n\nFor each test case, given as a tuple $(N, \\lambda_{\\mathrm{fix}}, s)$, both $f_{\\min}^{\\mathrm{grid}}$ and $f_{\\min}^{\\mathrm{rand}}$ are computed according to the algorithms above. The advantage of random search over blocked grid search, $\\Delta$, is then calculated as:\n$$\\Delta = f_{\\min}^{\\mathrm{grid}} - f_{\\min}^{\\mathrm{rand}}$$\nA positive value of $\\Delta$ indicates that random search found a better (lower) loss value than the blocked grid search.\n\nThis procedure is executed for each of the four test cases:\n- Case $1$: $(N, \\lambda_{\\mathrm{fix}}, s) = (30, 10^{-1}, 2027)$\n- Case $2$: $(N, \\lambda_{\\mathrm{fix}}, s) = (1, 10^{-1}, 123)$\n- Case $3$: $(N, \\lambda_{\\mathrm{fix}}, s) = (30, 10^{-3}, 2027)$\n- Case $4$: $(N, \\lambda_{\\mathrm{fix}}, s) = (10, 10^{-2}, 42)$\n\nThe Python program in the final answer implements these steps precisely, computes the four $\\Delta$ values, and formats them according to the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by simulating and comparing\n    blocked grid search and random search for hyperparameter optimization.\n    \"\"\"\n\n    def objective_function(x, y):\n        \"\"\"\n        Computes the synthetic validation loss f(x, y).\n        x = log10(eta), y = log10(lambda).\n        \"\"\"\n        term1 = (x + 2.0)**2\n        term2 = 2.5 * (y + 3.0)**2\n        term3 = 0.3 * (x + 2.0) * (y + 3.0)\n        term4 = 0.1 * np.sin(5.0 * x) * np.sin(5.0 * y)\n        return term1 + term2 + term3 + term4\n\n    def grid_search_blocked(N, lambda_fix):\n        \"\"\"\n        Performs blocked grid search.\n        Fixes lambda (and thus y) and searches over a grid of eta (x) values.\n        \"\"\"\n        y_fix = np.log10(lambda_fix)\n\n        if N == 1:\n            x_points = np.array([-4.0])\n        else:\n            x_points = np.linspace(-4.0, 0.0, N)\n        \n        # Evaluate the objective function for all x points at the fixed y.\n        # This can be vectorized since objective_function is written with numpy ops.\n        y_points = np.full_like(x_points, y_fix)\n        losses = objective_function(x_points, y_points)\n        \n        return np.min(losses)\n\n    def random_search(N, s):\n        \"\"\"\n        Performs random search.\n        Samples N points (x, y) from the uniform distribution over the domain.\n        \"\"\"\n        # Create a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(s)\n        \n        # Sample N points from the search space.\n        x_samples = rng.uniform(-4.0, 0.0, N)\n        y_samples = rng.uniform(-5.0, -1.0, N)\n        \n        # Evaluate the objective function at the sampled points.\n        losses = objective_function(x_samples, y_samples)\n        \n        return np.min(losses)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, lambda_fix, s)\n        (30, 1e-1, 2027),  # Case 1\n        (1, 1e-1, 123),   # Case 2\n        (30, 1e-3, 2027),  # Case 3\n        (10, 1e-2, 42),   # Case 4\n    ]\n\n    results = []\n    for N, lambda_fix, s in test_cases:\n        # Calculate the minimum loss found by each method\n        f_min_grid = grid_search_blocked(N, lambda_fix)\n        f_min_rand = random_search(N, s)\n        \n        # Calculate the advantage of random search\n        delta = f_min_grid - f_min_rand\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    # Each result is formatted to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3133107"}, {"introduction": "Having built intuition through coding, we now turn to a more formal analysis to understand why random search is often more efficient in high-dimensional spaces. This problem challenges you to derive the expected performance of random search and a worst-case performance guarantee for grid search under specific, simplifying assumptions [@problem_id:3133146]. The results will mathematically expose the \"curse of dimensionality\" and provide a solid theoretical foundation for choosing random search, especially when tuning many hyperparameters.", "problem": "You are tuning hyperparameters for a deep neural network by either random search or grid search over the hyperparameter domain $\\Lambda = [0,1]^d$. Let $g:\\Lambda \\to [0,1]$ denote the validation accuracy as a function of hyperparameters, and assume $g$ is $L$-Lipschitz with respect to the Euclidean norm, meaning $|g(\\lambda) - g(\\lambda')| \\le L \\|\\lambda - \\lambda'\\|_{2}$ for all $\\lambda,\\lambda' \\in \\Lambda$. Let $g^{\\star} = \\max_{\\lambda \\in \\Lambda} g(\\lambda)$ denote the optimal validation accuracy in the domain.\n\nRandom search samples hyperparameters $\\lambda_{1},\\dots,\\lambda_{n}$ independently and uniformly from $\\Lambda$. Denote the resulting validation accuracies by $X_{1} = g(\\lambda_{1}),\\dots,X_{n} = g(\\lambda_{n})$. For the purpose of this analysis, model the marginal distribution of $X_{i}$ as independent and identically distributed (i.i.d.) with cumulative distribution function $F$ supported on $[0,1]$. In particular, assume that $F(x) = x$ on $[0,1]$ (that is, $X_{i} \\sim \\mathrm{Uniform}(0,1)$), which is a common non-informative prior for accuracies on $[0,1]$ when no structure is assumed.\n\nGrid search uses a regular grid with $m$ points per coordinate axis (including endpoints), so that the total number of grid points is $n = m^{d}$ with integer $m \\ge 2$. The grid spacing along each coordinate is $h = \\frac{1}{m-1}$.\n\nUsing only the definitions of cumulative distribution function, independence, and order statistics, derive:\n- An analytical expression for the expected best validation accuracy under random search after $n$ trials, $E[\\max\\{X_{1},\\dots,X_{n}\\}]$, specialized to the case $F(x) = x$ on $[0,1]$.\n- A worst-case lower bound on the best validation accuracy attained by the $n$-point grid search in terms of $g^{\\star}$, $L$, $d$, and $n$.\n\nReport your final result as a two-entry row matrix whose first entry is the exact expression for $E[\\max\\{X_{1},\\dots,X_{n}\\}]$ under the uniform model and whose second entry is the lower bound for the grid search expressed purely in terms of $g^{\\star}$, $L$, $d$, and $n$ by eliminating $m$. Do not provide any numerical approximations; the final answer must be an exact analytic expression with no units.", "solution": "The user has provided a problem statement that requires the derivation of two quantities related to hyperparameter optimization strategies: the expected best accuracy for random search and a worst-case lower bound for grid search.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Hyperparameter domain: $\\Lambda = [0,1]^d$.\n- Validation accuracy function: $g:\\Lambda \\to [0,1]$.\n- Lipschitz property of $g$: $|g(\\lambda) - g(\\lambda')| \\le L \\|\\lambda - \\lambda'\\|_{2}$ for all $\\lambda,\\lambda' \\in \\Lambda$.\n- Optimal validation accuracy: $g^{\\star} = \\max_{\\lambda \\in \\Lambda} g(\\lambda)$.\n- **Random Search**:\n    - Samples $\\lambda_{1},\\dots,\\lambda_{n}$ are i.i.d. uniform from $\\Lambda$.\n    - Validation accuracies $X_{i} = g(\\lambda_{i})$ are modeled as i.i.d. random variables.\n    - The CDF of $X_i$ is $F(x) = x$ for $x \\in [0,1]$.\n- **Grid Search**:\n    - A regular grid with $m$ points per coordinate axis.\n    - Total grid points: $n = m^{d}$, where $m \\ge 2$ is an integer.\n    - Grid spacing: $h = \\frac{1}{m-1}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the domain of machine learning and optimization theory. The concepts used (random search, grid search, Lipschitz continuity, order statistics) are standard and well-defined. The problem is well-posed, providing sufficient information to derive the requested quantities. The assumption that validation accuracies $X_i$ are distributed as $\\mathrm{Uniform}(0,1)$ is explicitly stated as a modeling choice for the sake of analysis, which is a common practice in theoretical comparisons of algorithms. The problem is objective, using precise mathematical language and contains no contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n---\n\n### Derivation of a Solution\n\nThe problem consists of two parts. The first part is to calculate the expected best accuracy for random search, and the second is to find a worst-case lower bound for the best accuracy found by grid search.\n\n#### Part 1: Expected Best Accuracy for Random Search\n\nLet $X_1, X_2, \\dots, X_n$ be an i.i.d. sample of validation accuracies. The problem states that these are modeled as random variables with a cumulative distribution function (CDF) $F(x) = x$ for $x \\in [0,1]$, which corresponds to a standard uniform distribution, $X_i \\sim \\mathrm{Uniform}(0,1)$. We are asked to find the expected value of the maximum of these samples, $E[\\max\\{X_1, \\dots, X_n\\}]$.\n\nLet $Y = \\max\\{X_1, \\dots, X_n\\}$. The random variable $Y$ is the $n$-th order statistic of the sample. To find its expectation, we first determine its CDF, $F_Y(y)$.\nBy definition, $F_Y(y) = P(Y \\le y)$.\nThe maximum of a set of random variables is less than or equal to $y$ if and only if all variables in the set are less than or equal to $y$.\n$$F_Y(y) = P(\\max\\{X_1, \\dots, X_n\\} \\le y) = P(X_1 \\le y, X_2 \\le y, \\dots, X_n \\le y)$$\nSince the random variables $X_i$ are independent, the joint probability is the product of the marginal probabilities:\n$$F_Y(y) = \\prod_{i=1}^n P(X_i \\le y)$$\nAs the variables are also identically distributed with CDF $F(x)$, this simplifies to:\n$$F_Y(y) = [F(y)]^n$$\nGiven that $F(x)=x$ for $x \\in [0,1]$, the CDF of $Y$ for $y \\in [0,1]$ is:\n$$F_Y(y) = y^n$$\nThe expected value of a non-negative random variable $Y$ with support on $[0,1]$ can be calculated using its CDF as $E[Y] = \\int_0^1 (1 - F_Y(y)) dy$.\nSubstituting the expression for $F_Y(y)$:\n$$E[Y] = \\int_0^1 (1 - y^n) dy$$\nEvaluating the integral:\n$$E[Y] = \\left[ y - \\frac{y^{n+1}}{n+1} \\right]_0^1 = \\left(1 - \\frac{1^{n+1}}{n+1}\\right) - \\left(0 - \\frac{0^{n+1}}{n+1}\\right) = 1 - \\frac{1}{n+1}$$\n$$E[Y] = \\frac{n+1-1}{n+1} = \\frac{n}{n+1}$$\nThus, the expected best validation accuracy under random search with the given model is $\\frac{n}{n+1}$.\n\n#### Part 2: Worst-Case Lower Bound for Grid Search\n\nFor grid search, we need to find a lower bound on the best accuracy achieved, $\\max_{\\lambda \\in \\Lambda_{grid}} g(\\lambda)$, where $\\Lambda_{grid}$ is the set of $n$ grid points. This bound must hold for any $L$-Lipschitz function $g$. The worst case corresponds to a function $g$ and a location of its maximum $g^\\star$ that makes the grid search perform as poorly as possible.\n\nLet $\\lambda^\\star \\in \\Lambda = [0,1]^d$ be a hyperparameter vector where the true optimal validation accuracy is achieved, i.e., $g(\\lambda^\\star) = g^\\star$. The grid search method does not know $\\lambda^\\star$; it only evaluates $g$ at the points in $\\Lambda_{grid}$. The value it finds is $\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i)$.\n\nWe must find how far $\\lambda^\\star$ can be from the nearest grid point. The grid has $m$ equally spaced points along each of the $d$ axes, including the endpoints $0$ and $1$. The grid spacing along each axis is $h = \\frac{1}{m-1}$. The coordinates of the grid points along any axis are $\\{0, h, 2h, \\dots, (m-1)h=1\\}$.\n\nFor any point $\\lambda = (c_1, \\dots, c_d) \\in \\Lambda$, and for each coordinate $c_j$, there exists a grid coordinate $p_j$ on that axis such that $|c_j - p_j| \\le \\frac{h}{2}$. Let $\\lambda_p = (p_1, \\dots, p_d)$ be the grid point formed by these closest coordinates. The squared Euclidean distance between $\\lambda$ and $\\lambda_p$ is bounded:\n$$\\|\\lambda - \\lambda_p\\|_2^2 = \\sum_{j=1}^d (c_j - p_j)^2 \\le \\sum_{j=1}^d \\left(\\frac{h}{2}\\right)^2 = d \\frac{h^2}{4}$$\nTaking the square root, we find that for any point $\\lambda \\in \\Lambda$, there is a grid point $\\lambda_p \\in \\Lambda_{grid}$ such that:\n$$\\|\\lambda - \\lambda_p\\|_2 \\le \\frac{h\\sqrt{d}}{2}$$\nThis bound is tight and represents the maximum distance from any point in the domain to the nearest grid point (achieved at the center of a grid hypercell).\n\nNow, let's apply this to the optimal point $\\lambda^\\star$. There must exist a grid point, let's call it $\\lambda_{closest} \\in \\Lambda_{grid}$, such that:\n$$\\|\\lambda^\\star - \\lambda_{closest}\\|_2 \\le \\frac{h\\sqrt{d}}{2}$$\nThe function $g$ is $L$-Lipschitz, which means $|g(\\lambda^\\star) - g(\\lambda_{closest})| \\le L \\|\\lambda^\\star - \\lambda_{closest}\\|_2$. Since $g(\\lambda^\\star)=g^\\star$ is the maximum value, we can write:\n$$g^\\star - g(\\lambda_{closest}) \\le L \\|\\lambda^\\star - \\lambda_{closest}\\|_2$$\nUsing the distance bound we found:\n$$g^\\star - g(\\lambda_{closest}) \\le L \\frac{h\\sqrt{d}}{2}$$\nRearranging this inequality gives a lower bound on the accuracy at the grid point $\\lambda_{closest}$:\n$$g(\\lambda_{closest}) \\ge g^\\star - \\frac{Lh\\sqrt{d}}{2}$$\nThe best accuracy found by grid search is $\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i)$. Since $\\lambda_{closest}$ is one of the grid points, the maximum accuracy found must be at least as large as $g(\\lambda_{closest})$:\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g(\\lambda_{closest})$$\nCombining the inequalities provides the worst-case lower bound:\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g^\\star - \\frac{Lh\\sqrt{d}}{2}$$\nThe problem requires this bound to be expressed in terms of $g^\\star, L, d,$ and $n$. We use the given relations $h = \\frac{1}{m-1}$ and $n=m^d$. From $n=m^d$, we solve for $m$ to get $m=n^{1/d}$. We substitute this into the expression for $h$:\n$$h = \\frac{1}{n^{1/d} - 1}$$\nSubstituting this expression for $h$ into the lower bound:\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g^\\star - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}$$\nThis is the desired worst-case lower bound for grid search.\n\n### Final Expressions\n\nThe two requested expressions are:\n1.  Expected best accuracy for random search: $\\frac{n}{n+1}$.\n2.  Worst-case lower bound for grid search: $g^\\star - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}$.\n\nThese will be formatted into a two-entry row matrix as requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{n+1}  g^{\\star} - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}\n\\end{pmatrix}\n}\n$$", "id": "3133146"}]}