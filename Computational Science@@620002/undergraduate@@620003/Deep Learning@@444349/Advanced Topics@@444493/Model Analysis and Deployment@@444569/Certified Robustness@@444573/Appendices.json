{"hands_on_practices": [{"introduction": "At the heart of many certified robustness methods lies a simple yet powerful technique: Interval Bound Propagation (IBP). This exercise will guide you through implementing IBP from scratch, allowing you to see how an entire region of input points, defined as an interval, can be propagated through a neural network to determine the corresponding range of possible outputs [@problem_id:3105264]. By calculating these output bounds, you will develop a fundamental skill for reasoning about network behavior over input perturbations and compute a concrete measure of local stability.", "problem": "You are given a fully specified three-layer Rectified Linear Unit (ReLU) network with affine transformations followed by ReLU activations at each layer. The network has input dimension $2$, with layers defined by weight matrices and bias vectors $(W_1,b_1)$, $(W_2,b_2)$, $(W_3,b_3)$ as follows. The first layer maps $\\mathbb{R}^2$ to $\\mathbb{R}^3$, the second maps $\\mathbb{R}^3$ to $\\mathbb{R}^2$, and the third maps $\\mathbb{R}^2$ to $\\mathbb{R}^2$. All layers are followed by the ReLU nonlinearity, applied elementwise. Explicitly, the layer parameters are:\n- $W_1 = \\begin{bmatrix} 1.0  -0.5 \\\\ -0.3  0.8 \\\\ 0.6  0.6 \\end{bmatrix}$ and $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.0 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} 0.5  -1.0  0.2 \\\\ 1.2  0.3  -0.4 \\end{bmatrix}$ and $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\end{bmatrix}$,\n- $W_3 = \\begin{bmatrix} 0.7  -0.6 \\\\ -0.5  0.9 \\end{bmatrix}$ and $b_3 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n\nThe input vector $x \\in \\mathbb{R}^2$ is constrained by two conditions that together define the admissible perturbation set. First, $x$ must lie in the axis-aligned input box $[l,u]$ with $l = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ and $u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$. Second, $x$ must lie within an $L_\\infty$ ball of radius $\\epsilon$ around a given nominal input $x_0$, i.e., $\\|x - x_0\\|_\\infty \\le \\epsilon$. The admissible set is their intersection $\\mathcal{S} = \\{ x \\in \\mathbb{R}^2 \\mid l \\le x \\le u \\text{ and } \\|x - x_0\\|_\\infty \\le \\epsilon \\}$, where inequalities are coordinate-wise.\n\nYour task is to implement Interval Bound Propagation (IBP), which is the process of computing interval bounds that enclose all possible activations of each layer over the set $\\mathcal{S}$. Starting from the interval representation of $\\mathcal{S}$, propagate bounds through each affine layer and ReLU. At each layer $k \\in \\{1,2,3\\}$, let $z_k(x)$ denote the pre-activation vector before the ReLU. Define the layerâ€™s certified stability margin as the scalar\n$$\nm_k \\triangleq \\min_{i} \\left( \\inf_{x \\in \\mathcal{S}} |(z_k(x))_i| \\right),\n$$\nwhich is the minimum over neurons in that layer of the lower bound on the absolute value of their pre-activations across the admissible set. A strictly positive $m_k$ certifies that all neurons in layer $k$ are sign-stable on $\\mathcal{S}$, while $m_k = 0$ indicates at least one neuron may switch sign.\n\nImplement a program that:\n- Constructs the effective input interval $[x_L, x_U]$ that represents $\\mathcal{S}$ via the coordinate-wise intersection of $[l,u]$ and the $L_\\infty$ ball around $x_0$, i.e., $x_L = \\max(l, x_0 - \\epsilon)$ and $x_U = \\min(u, x_0 + \\epsilon)$.\n- Applies Interval Bound Propagation layer by layer to obtain pre-activation intervals for $z_1$, $z_2$, and $z_3$ and the corresponding ReLU-activated intervals.\n- Computes the certified stability margin $m_k$ at each layer as defined above.\n\nUse the following test suite of nominal inputs and perturbation radii:\n- Case $1$: $x_0 = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}$ and $\\epsilon = 0.1$.\n- Case $2$: $x_0 = \\begin{bmatrix} 0.95 \\\\ 0.95 \\end{bmatrix}$ and $\\epsilon = 0.1$.\n- Case $3$: $x_0 = \\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix}$ and $\\epsilon = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered by concatenating the three layer margins for each case in sequence: $[m_1^{(1)}, m_2^{(1)}, m_3^{(1)}, m_1^{(2)}, m_2^{(2)}, m_3^{(2)}, m_1^{(3)}, m_2^{(3)}, m_3^{(3)}]$, where $m_k^{(j)}$ denotes the certified stability margin for layer $k$ under test case $j$. Each entry must be a real number (a float). No physical units or angle units are involved in this problem, and no percentages are required.", "solution": "The user has provided a well-defined computational problem in the domain of certified robustness for deep neural networks. The problem is to apply Interval Bound Propagation (IBP) to a given three-layer Rectified Linear Unit (ReLU) network to compute certified stability margins for each layer under various input conditions. The problem is scientifically grounded, well-posed, and contains all necessary information for a unique solution.\n\nThe solution proceeds by first establishing the interval representation of the input set, then systematically propagating these interval bounds through each layer of the network, and finally, calculating the required stability margins from the pre-activation bounds.\n\nLet the network function be denoted by $f(x)$, where $x \\in \\mathbb{R}^2$. The network consists of three layers. The output of layer $k \\in \\{1, 2, 3\\}$ is denoted by $h_k(x)$, with the input being $h_0(x) = x$. Each layer performs an affine transformation followed by an element-wise ReLU activation:\n$$z_k(x) = W_k h_{k-1}(x) + b_k$$\n$$h_k(x) = \\text{ReLU}(z_k(x)) = \\max(0, z_k(x))$$\nwhere $z_k(x)$ is the pre-activation vector of layer $k$.\n\nThe core of Interval Bound Propagation is to represent the set of possible values for any vector in the network (activations or pre-activations) as a hyperrectangle, or an interval. For a vector $v$, its corresponding set of possible values is bounded by a lower vector $v_L$ and an upper vector $v_U$, such that for all possible $v$, the inequality $v_L \\le v \\le v_U$ holds element-wise.\n\n### Step 1: Effective Input Interval\nThe input $x$ is constrained to an admissible set $\\mathcal{S}$, which is the intersection of an axis-aligned box $[l, u]$ and an $L_\\infty$ ball of radius $\\epsilon$ around a nominal input $x_0$. This set is given by:\n$$\\mathcal{S} = \\{ x \\in \\mathbb{R}^2 \\mid l \\le x \\le u \\text{ and } \\|x - x_0\\|_\\infty \\le \\epsilon \\}$$\nThe $L_\\infty$ constraint $\\|x - x_0\\|_\\infty \\le \\epsilon$ is equivalent to $x_0 - \\epsilon \\mathbf{1} \\le x \\le x_0 + \\epsilon \\mathbf{1}$, where $\\mathbf{1}$ is a vector of ones. The set $\\mathcal{S}$ is itself a hyperrectangle, and its bounds $[x_L, x_U]$ are found by taking the element-wise intersection of the two constraint intervals:\n$$x_L = \\max(l, x_0 - \\epsilon \\mathbf{1})$$\n$$x_U = \\min(u, x_0 + \\epsilon \\mathbf{1})$$\nHere, $\\max$ and $\\min$ operations are performed element-wise. This pair $[x_L, x_U]$ forms the initial interval for propagation.\n\n### Step 2: Propagation through an Affine Layer\nGiven an interval $[h_{L}, h_{U}]$ for the input $h$ to an affine layer with parameters $(W, b)$, we need to compute the interval $[z_{L}, z_{U}]$ for the pre-activation output $z = Wh + b$. To find the tightest possible interval for $z$, we can compute the lower and upper bounds for each component of $z$ independently. For the $i$-th component $z_i = \\sum_j W_{ij} h_j + b_i$, the bounds are:\n$$\\inf(z_i) = \\sum_j \\inf(W_{ij} h_j) + b_i$$\n$$\\sup(z_i) = \\sum_j \\sup(W_{ij} h_j) + b_i$$\nThe extrema of $W_{ij} h_j$ depend on the sign of $W_{ij}$. If $W_{ij}  0$, then $\\inf(W_{ij}h_j) = W_{ij}h_{j,L}$ and $\\sup(W_{ij}h_j) = W_{ij}h_{j,U}$. If $W_{ij}  0$, then $\\inf(W_{ij}h_j) = W_{ij}h_{j,U}$ and $\\sup(W_{ij}h_j) = W_{ij}h_{j,L}$.\n\nThis can be expressed concisely in matrix form by splitting the weight matrix $W$ into its positive and negative parts: $W^+ = \\max(W, 0)$ and $W^- = \\min(W, 0)$, where the operations are element-wise. The bounds on the pre-activation vector $z$ are then:\n$$z_L = W^+ h_L + W^- h_U + b$$\n$$z_U = W^+ h_U + W^- h_L + b$$\n\n### Step 3: Propagation through a ReLU Activation\nGiven the pre-activation interval $[z_L, z_U]$, we compute the post-activation interval $[h_L, h_U]$ for $h = \\text{ReLU}(z)$. Since the ReLU function, $\\text{ReLU}(v) = \\max(0,v)$, is element-wise and monotonically non-decreasing, the bounds can be computed by applying the function directly to the interval's endpoints:\n$$h_L = \\text{ReLU}(z_L) = \\max(0, z_L)$$\n$$h_U = \\text{ReLU}(z_U) = \\max(0, z_U)$$\nThese operations are performed element-wise.\n\n### Step 4: Computing the Certified Stability Margin\nThe certified stability margin for layer $k$ is defined as:\n$$m_k \\triangleq \\min_{i} \\left( \\inf_{x \\in \\mathcal{S}} |(z_k(x))_i| \\right)$$\nIBP provides the interval $[(z_{k,L})_i, (z_{k,U})_i]$ for each pre-activation neuron $(z_k(x))_i$. We need to find the infimum of the absolute value of a variable $v$ that lies in an interval $[l_i, u_i] = [(z_{k,L})i, (z_{k,U})_i]$. This infimum, let's call it $\\mu_i$, is determined by the signs of the interval endpoints:\n1. If the entire interval is non-negative ($l_i \\ge 0$), the value closest to zero is $l_i$. So, $\\mu_i = l_i$.\n2. If the entire interval is non-positive ($u_i \\le 0$), the absolute values range from $-u_i$ to $-l_i$. The minimum absolute value is $-u_i$. So, $\\mu_i = -u_i$.\n3. If the interval contains zero ($l_i  0  u_i$), it is possible for the pre-activation to be zero. Thus, the infimum of its absolute value is $0$. So, $\\mu_i = 0$.\n\nThe stability margin for the layer, $m_k$, is the minimum of these individual neuron margins:\n$$m_k = \\min_i \\mu_i$$\nA margin $m_k  0$ certifies that all neurons in layer $k$ maintain the same sign (are \"stable\") across the entire input set $\\mathcal{S}$. A margin of $m_k=0$ indicates that at least one neuron is \"unstable\" and may change its sign, meaning its pre-activation interval crosses zero.\n\nThe overall algorithm consists of starting with the input interval $[x_L, x_U]$ and repeatedly applying Step 2 and Step 3 for each layer of the network, calculating the margin $m_k$ at each pre-activation stage.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Interval Bound Propagation problem for the given network and test cases.\n    \"\"\"\n\n    # Define the network parameters\n    W1 = np.array([[1.0, -0.5], [-0.3, 0.8], [0.6, 0.6]])\n    b1 = np.array([[0.1], [-0.2], [0.0]])\n    \n    W2 = np.array([[0.5, -1.0, 0.2], [1.2, 0.3, -0.4]])\n    b2 = np.array([[0.0], [0.05]])\n\n    W3 = np.array([[0.7, -0.6], [-0.5, 0.9]])\n    b3 = np.array([[0.1], [-0.1]])\n\n    network_params = [\n        (W1, b1),\n        (W2, b2),\n        (W3, b3)\n    ]\n\n    # Define the test cases\n    test_cases = [\n        (np.array([[0.6], [0.4]]), 0.1),  # Case 1\n        (np.array([[0.95], [0.95]]), 0.1), # Case 2\n        (np.array([[0.2], [0.8]]), 0.0),  # Case 3\n    ]\n\n    l_box = np.array([[0.0], [0.0]])\n    u_box = np.array([[1.0], [1.0]])\n\n    results = []\n\n    def propagate_affine(W, b, h_L, h_U):\n        \"\"\"Propagates interval bounds through an affine layer z = Wh + b.\"\"\"\n        W_pos = np.maximum(W, 0)\n        W_neg = np.minimum(W, 0)\n        \n        z_L = W_pos @ h_L + W_neg @ h_U + b\n        z_U = W_pos @ h_U + W_neg @ h_L + b\n        \n        return z_L, z_U\n\n    def propagate_relu(z_L, z_U):\n        \"\"\"Propagates interval bounds through a ReLU activation.\"\"\"\n        h_L = np.maximum(z_L, 0)\n        h_U = np.maximum(z_U, 0)\n        return h_L, h_U\n\n    def calculate_margin(z_L, z_U):\n        \"\"\"Calculates the certified stability margin for a layer.\"\"\"\n        neuron_margins = []\n        for l_i, u_i in zip(z_L, z_U):\n            if l_i >= 0:\n                neuron_margins.append(l_i[0])\n            elif u_i = 0:\n                neuron_margins.append(-u_i[0])\n            else: # l_i  0  u_i\n                neuron_margins.append(0.0)\n        return min(neuron_margins)\n\n    for x0, epsilon in test_cases:\n        # Step 1: Compute the effective input interval\n        x_L = np.maximum(l_box, x0 - epsilon)\n        x_U = np.minimum(u_box, x0 + epsilon)\n\n        h_L_current, h_U_current = x_L, x_U\n        \n        # Propagate through all layers\n        for W, b in network_params:\n            # Propagate through affine layer\n            z_L, z_U = propagate_affine(W, b, h_L_current, h_U_current)\n\n            # Calculate and store the margin for the pre-activation\n            margin = calculate_margin(z_L, z_U)\n            results.append(margin)\n\n            # Propagate through ReLU activation for the next layer's input\n            h_L_current, h_U_current = propagate_relu(z_L, z_U)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105264"}, {"introduction": "Beyond the mechanics of propagation, certified robustness is deeply connected to the geometry of the decision boundaries learned by a model. This practice problem explores the elegant relationship between a model's classification marginâ€”a measure of its confidenceâ€”and its certified radius against adversarial attacks [@problem_id:3105220]. By assuming a global smoothness property known as a Lipschitz constant, you will derive and apply a formula that directly links the margin $m(x)$ to a guaranteed robustness radius $r(x)$, providing a clear intuition for how methods like adversarial training can enhance provable security by shaping the model's loss landscape.", "problem": "Consider a multiclass classifier represented by a function $f:\\mathbb{R}^d \\to \\mathbb{R}^C$ that maps an input $x \\in \\mathbb{R}^d$ to a vector of real-valued class logits $f(x) = (f_1(x),\\dots,f_C(x))$. For a given labeled example $(x,y)$ with correct class index $y \\in \\{1,\\dots,C\\}$, define the empirical margin of $x$ by\n$$\nm(x) \\triangleq f_y(x) - \\max_{j \\neq y} f_j(x).\n$$\nAssume a fixed global Lipschitz bound $L  0$ holds uniformly for all logit difference functions $g_j(x) \\triangleq f_y(x) - f_j(x)$ with respect to the input norm $\\|\\cdot\\|_2$, meaning that for all $j \\neq y$ and all $x, x' \\in \\mathbb{R}^d$,\n$$\n\\big|g_j(x) - g_j(x')\\big| \\le L \\,\\|x - x'\\|_2.\n$$\nProjected Gradient Descent (PGD) is an adversarial example generation method used within adversarial training; adversarial training with PGD attempts to minimize a worst-case loss over a norm-bounded perturbation set. Empirically, such training can increase margins $m(x)$ by pushing correct-class logit differences upward against the hardest competing classes near decision boundaries.\n\nYour task is to:\n- Compute empirical margins before and after adversarial training with PGD from provided pre-training and post-training logit vectors.\n- Show numerically whether adversarial training increases the empirical margins for each test case.\n- Using only the definition of Lipschitz continuity and the margin $m(x)$, derive and implement a certified robustness radius $r(x)$ in the $\\ell_2$ input norm that guarantees the predicted class $y$ cannot change for any perturbation $\\delta$ with $\\|\\delta\\|_2 \\le r(x)$. If $m(x) \\le 0$, the certificate should be reported as $0$ because no robustness guarantee can be made when the example is misclassified or tied.\n\nImplement a program that performs these computations for the following test suite with a fixed global Lipschitz bound $L = 3.0$. Each test case contains the correct class index $y$, a pre-training logit vector $f_{\\text{pre}}(x)$, and a post-training logit vector $f_{\\text{post}}(x)$. Class indices are $0$-based.\n\nTest suite:\n1. $y=2$, $f_{\\text{pre}}(x) = [1.2,\\,1.8,\\,3.0,\\,0.5]$, $f_{\\text{post}}(x) = [1.1,\\,1.9,\\,5.0,\\,0.3]$.\n2. $y=0$, $f_{\\text{pre}}(x) = [2.0,\\,2.0,\\,1.0,\\,0.7]$, $f_{\\text{post}}(x) = [2.6,\\,2.0,\\,1.0,\\,0.7]$.\n3. $y=3$, $f_{\\text{pre}}(x) = [2.2,\\,2.5,\\,3.1,\\,2.0]$, $f_{\\text{post}}(x) = [2.3,\\,2.2,\\,2.4,\\,3.0]$.\n4. $y=1$, $f_{\\text{pre}}(x) = [0.2,\\,4.1,\\,1.8,\\,2.0]$, $f_{\\text{post}}(x) = [0.1,\\,4.3,\\,1.7,\\,1.9]$.\n5. $y=2$, $f_{\\text{pre}}(x) = [1.5,\\,1.6,\\,1.61,\\,1.60]$, $f_{\\text{post}}(x) = [1.4,\\,1.5,\\,1.9,\\,1.60]$.\n\nYour program should:\n- For each test case, compute the empirical margin $m_{\\text{pre}}(x)$ and $m_{\\text{post}}(x)$ from the logits, then compute the certified radii $r_{\\text{pre}}(x)$ and $r_{\\text{post}}(x)$ using the derived expression. If $m(x) \\le 0$, return $r(x)=0$.\n- Produce a boolean indicator per test case that is `true` if the margin increased after PGD adversarial training, i.e., $m_{\\text{post}}(x)  m_{\\text{pre}}(x)$, and `false` otherwise.\n- Produce a final boolean indicator that is `true` only if the margin increased for all test cases.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, structured as\n$$\n[\\,[r_{\\text{pre},1},\\dots,r_{\\text{pre},5}],\\,[r_{\\text{post},1},\\dots,r_{\\text{post},5}],\\,[b_1,\\dots,b_5],\\,B_{\\text{all}}\\,],\n$$\nwhere $r_{\\text{pre},i}$ and $r_{\\text{post},i}$ are floats, $b_i$ are booleans indicating margin increase for test case $i$, and $B_{\\text{all}}$ is a boolean indicating that margins increased for all test cases. No physical units or angle units are involved. The booleans must be the programming language booleans `true` or `false` as rendered by the language. Ensure the nesting and ordering match exactly.", "solution": "The problem requires us to compute empirical margins and certified robustness radii for a multiclass classifier both before and after adversarial training. We are given pre- and post-training logit vectors for several test cases, a correct class index $y$, and a global Lipschitz constant $L$. The core of the task is to first derive the formula for the certified radius $r(x)$ and then apply it to the provided data.\n\nFirst, we will derive the certified robustness radius $r(x)$ for an input $x \\in \\mathbb{R}^d$ with correct class label $y \\in \\{0, \\dots, C-1\\}$. The classifier's prediction for a given input $x'$ is the class index with the highest logit value, i.e., $\\arg\\max_k f_k(x')$. The input $x$ is correctly classified if $f_y(x)  f_j(x)$ for all $j \\neq y$. This is equivalent to the condition that the empirical margin, $m(x) \\triangleq f_y(x) - \\max_{j \\neq y} f_j(x)$, is positive ($m(x)  0$).\n\nA certified radius $r(x)$ guarantees that for any perturbation $\\delta$ with an $\\ell_2$-norm less than or equal to this radius, i.e., $\\|\\delta\\|_2 \\le r(x)$, the classifier's prediction for the perturbed input $x' = x + \\delta$ remains the correct class $y$. The condition for the prediction to be $y$ at $x'$ is $f_y(x')  f_j(x')$ for all competitor classes $j \\neq y$.\n\nLet us define the logit difference functions $g_j(x) \\triangleq f_y(x) - f_j(x)$ for each $j \\neq y$. The condition for correct classification at $x'$ becomes $g_j(x')  0$ for all $j \\neq y$. We are given that these difference functions are $L$-Lipschitz with respect to the $\\ell_2$ norm:\n$$\n|g_j(x) - g_j(x')| \\le L \\|x - x'\\|_2\n$$\nfor all $x, x' \\in \\mathbb{R}^d$ and for all $j \\neq y$. Substituting $x' = x + \\delta$, this becomes $|g_j(x) - g_j(x+\\delta)| \\le L \\|\\delta\\|_2$. This inequality implies a lower bound on the value of $g_j$ at the perturbed point $x'$:\n$$\ng_j(x+\\delta) \\ge g_j(x) - L \\|\\delta\\|_2\n$$\nTo guarantee that the classification remains correct at $x'$, we need to ensure that $g_j(x+\\delta)  0$ for all $j \\neq y$. Using the lower bound, this condition is met if we can ensure:\n$$\ng_j(x) - L \\|\\delta\\|_2  0 \\quad \\text{for all } j \\neq y\n$$\nThis must hold for any perturbation $\\delta$ such that $\\|\\delta\\|_2 \\le r(x)$. The most challenging perturbation is one that maximally reduces the difference $g_j(x+\\delta)$, which corresponds to the largest norm $\\|\\delta\\|_2 = r(x)$. Thus, we must satisfy:\n$$\ng_j(x) - L \\cdot r(x)  0 \\quad \\text{for all } j \\neq y\n$$\nSolving for $r(x)$, we obtain $r(x)  \\frac{g_j(x)}{L}$. To satisfy this for all $j \\neq y$, the radius must be smaller than the minimum of these bounds:\n$$\nr(x)  \\min_{j \\neq y} \\frac{g_j(x)}{L} = \\frac{1}{L} \\min_{j \\neq y} g_j(x)\n$$\nThe term $\\min_{j \\neq y} g_j(x)$ can be expanded as:\n$$\n\\min_{j \\neq y} (f_y(x) - f_j(x)) = f_y(x) - \\max_{j \\neq y} f_j(x)\n$$\nThis is precisely the definition of the empirical margin $m(x)$. Therefore, the condition is $r(x)  \\frac{m(x)}{L}$. The largest possible radius $r(x)$ for which the guarantee holds is the upper bound of this interval:\n$$\nr(x) = \\frac{m(x)}{L}\n$$\nThe problem specifies that if the margin is non-positive, $m(x) \\le 0$, the certified radius is $0$. This is because if the input is already misclassified or on a decision boundary, no guarantee can be made even for infinitesimally small perturbations. This leads to the final expression for the certified radius:\n$$\nr(x) = \\max\\left(0, \\frac{m(x)}{L}\\right) = \\max\\left(0, \\frac{f_y(x) - \\max_{j \\neq y} f_j(x)}{L}\\right)\n$$\nWith this derived formula, we will proceed with the computational part of the problem. For each test case, we are given a correct class index $y$, a pre-training logit vector $f_{\\text{pre}}(x)$, a post-training logit vector $f_{\\text{post}}(x)$, and a Lipschitz constant $L=3.0$.\n\nThe procedure for each test case is as follows:\n1.  For both $f_{\\text{pre}}(x)$ and $f_{\\text{post}}(x)$, calculate the empirical margin $m(x)$ using its definition. This requires finding the logit of the correct class $y$ and subtracting the maximum logit of all other classes.\n2.  Using the calculated margin $m(x)$ and the given Lipschitz constant $L=3.0$, compute the certified radius $r(x)$ using the formula $r(x) = \\max(0, m(x)/L)$. This gives $r_{\\text{pre}}(x)$ and $r_{\\text{post}}(x)$.\n3.  Compare the margins to determine if adversarial training led to an increase. A boolean indicator $b_i$ is set to $true$ if $m_{\\text{post}}(x)  m_{\\text{pre}}(x)$, and $false$ otherwise.\n4.  Finally, a global boolean indicator $B_{\\text{all}}$ is determined, which is $true$ if and only if the margin increased for all test cases ($b_i$ is $true$ for all $i$).\n\nThe results will be compiled into the required list-based format for the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the certified robustness problem by calculating margins and radii\n    for pre- and post-training logits, and determines if margins increased.\n    \"\"\"\n    \n    # Global Lipschitz constant\n    L = 3.0\n    \n    # Test suite data: (y, f_pre, f_post)\n    test_cases = [\n        (2, [1.2, 1.8, 3.0, 0.5], [1.1, 1.9, 5.0, 0.3]),\n        (0, [2.0, 2.0, 1.0, 0.7], [2.6, 2.0, 1.0, 0.7]),\n        (3, [2.2, 2.5, 3.1, 2.0], [2.3, 2.2, 2.4, 3.0]),\n        (1, [0.2, 4.1, 1.8, 2.0], [0.1, 4.3, 1.7, 1.9]),\n        (2, [1.5, 1.6, 1.61, 1.60], [1.4, 1.5, 1.9, 1.60]),\n    ]\n\n    def calculate_margin_and_radius(logits, y, L_val):\n        \"\"\"\n        Computes the empirical margin and certified radius for a given logit vector.\n        \n        Args:\n            logits (np.ndarray): The vector of class logits.\n            y (int): The index of the correct class.\n            L_val (float): The Lipschitz constant.\n            \n        Returns:\n            tuple: A tuple containing the margin (float) and radius (float).\n        \"\"\"\n        logits_np = np.array(logits)\n        \n        # Logit of the correct class\n        f_y = logits_np[y]\n        \n        # Maximum logit of other classes\n        other_logits = np.delete(logits_np, y)\n        max_f_j = np.max(other_logits)\n        \n        # Empirical margin\n        margin = f_y - max_f_j\n        \n        # Certified radius\n        radius = max(0.0, margin / L_val)\n        \n        return margin, radius\n\n    r_pre_list = []\n    r_post_list = []\n    b_list = []\n\n    for y_true, f_pre, f_post in test_cases:\n        # Calculate for pre-training logits\n        m_pre, r_pre = calculate_margin_and_radius(f_pre, y_true, L)\n        r_pre_list.append(r_pre)\n        \n        # Calculate for post-training logits\n        m_post, r_post = calculate_margin_and_radius(f_post, y_true, L)\n        r_post_list.append(r_post)\n        \n        # Check if margin increased\n        margin_increased = m_post > m_pre\n        b_list.append(margin_increased)\n\n    # Check if margin increased for all cases\n    B_all = all(b_list)\n\n    # Format the output string exactly as specified\n    r_pre_str = ','.join(map(str, r_pre_list))\n    r_post_str = ','.join(map(str, r_post_list))\n    b_list_str = ','.join(str(b).lower() for b in b_list)\n    b_all_str = str(B_all).lower()\n\n    final_output = f\"[[{r_pre_str}],[{r_post_str}],[{b_list_str}],{b_all_str}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3105220"}, {"introduction": "While Interval Bound Propagation is fast, the bounds it produces can sometimes be too loose to certify robustness. To achieve tighter, more precise guarantees, we can employ a \"divide and conquer\" strategy known as branch-and-bound verification [@problem_id:3105282]. In this advanced exercise, you will build a complete verifier that recursively partitions the input space into smaller subdomains and applies IBP to each, systematically tightening the overall lower bound on the network's output until a proof of robustness is found or a budget is exhausted. You will also investigate how different branching heuristics can dramatically impact the verifier's efficiency, a key consideration in practical applications.", "problem": "You are given a binary classification margin function implemented by a feedforward Rectified Linear Unit (ReLU) network. The margin of an input is defined as the difference between the logit of the predicted class and the logit of the runner-up class. Consider an input vector $x \\in \\mathbb{R}^d$, and a one-hidden-layer ReLU network defined by\n$$\nm(x) = c^\\top \\,\\sigma\\!\\left(W_1 x + b_1\\right) + b_0,\n$$\nwhere $W_1 \\in \\mathbb{R}^{h \\times d}$ is the first-layer weight matrix, $b_1 \\in \\mathbb{R}^h$ is the first-layer bias vector, $\\sigma(\\cdot)$ is the element-wise Rectified Linear Unit activation $\\sigma(z) = \\max(0,z)$, $c \\in \\mathbb{R}^h$ is the difference of the second-layer weight vectors of the two classes (true class minus runner-up class), and $b_0 \\in \\mathbb{R}$ is the difference of the second-layer biases.\n\nCertified robustness asks to prove that, for a given center $x_0$ and radius $r \\ge 0$ under the infinity norm, the margin $m(x)$ stays strictly positive for all $x$ inside the closed $\\ell_\\infty$ ball\n$$\n\\mathcal{B}_\\infty(x_0,r) = \\{ x \\in \\mathbb{R}^d \\mid \\|x - x_0\\|_\\infty \\le r \\}.\n$$\nIf one can compute a sound lower bound $L(r)$ on the minimum of $m(x)$ over $\\mathcal{B}_\\infty(x_0,r)$ and $L(r)  0$, then $r$ is a certified radius for $x_0$.\n\nYour task is to implement a branch-and-bound verifier with a fixed time budget, using interval bound propagation (IBP) to bound $m(x)$ over axis-aligned boxes. The algorithm must:\n- Represent the domain $\\mathcal{B}_\\infty(x_0,r)$ as an initial axis-aligned box $[l,u]$ where $l = x_0 - r$ and $u = x_0 + r$, split boxes along one input dimension at a time, and maintain a global lower bound on $m(x)$ as the minimum of lower bounds over all active boxes.\n- Use interval bound propagation as follows. For a box $[l,u]$, each pre-activation coordinate $z_i = (W_1 x + b_1)_i$ lies in an interval $[z_i^{\\mathrm{L}}, z_i^{\\mathrm{U}}]$ computed by interval arithmetic,\n$$\nz_i^{\\mathrm{L}} = b_{1,i} + \\sum_{j=1}^d \\left( \\min\\{ w_{ij} l_j, w_{ij} u_j \\} \\right), \\quad\nz_i^{\\mathrm{U}} = b_{1,i} + \\sum_{j=1}^d \\left( \\max\\{ w_{ij} l_j, w_{ij} u_j \\} \\right),\n$$\nwith $w_{ij}$ denoting the $(i,j)$ entry of $W_1$. After the Rectified Linear Unit, the activation interval is $[\\alpha_i^{\\mathrm{L}}, \\alpha_i^{\\mathrm{U}}] = [\\max(0, z_i^{\\mathrm{L}}), \\max(0, z_i^{\\mathrm{U}})]$. The margin over the box satisfies\n$$\nm(x) \\in \\left[\\, b_0 + \\sum_{i=1}^h \\left( c_i \\cdot \\alpha_i \\right)_{\\min},\\; b_0 + \\sum_{i=1}^h \\left( c_i \\cdot \\alpha_i \\right)_{\\max} \\,\\right],\n$$\nwhere, for each $i$, the minimal contribution is $\\left( c_i \\cdot \\alpha_i \\right)_{\\min} = \\begin{cases} c_i \\alpha_i^{\\mathrm{L}}  \\text{if } c_i \\ge 0, \\\\ c_i \\alpha_i^{\\mathrm{U}}  \\text{if } c_i  0, \\end{cases}$ and the maximal contribution is $\\left( c_i \\cdot \\alpha_i \\right)_{\\max} = \\begin{cases} c_i \\alpha_i^{\\mathrm{U}}  \\text{if } c_i \\ge 0, \\\\ c_i \\alpha_i^{\\mathrm{L}}  \\text{if } c_i  0. \\end{cases}$ The lower bound on $m(x)$ over the box is the left endpoint, and the upper bound is the right endpoint.\n\nThe branch-and-bound verifier should:\n- Maintain a set of boxes, each with its interval lower bound and upper bound on $m(x)$. If the global lower bound (the minimum lower bound over the set) exceeds $0$, the verifier declares the current radius certified.\n- Use a fixed budget of expansion steps, where each expansion replaces one box by two boxes split at the midpoint along a chosen input coordinate. Selection of the next box to expand must be the one with the smallest lower bound, to prioritize the worst-case subdomain for minimizing $m(x)$.\n- Implement two splitting heuristics for choosing the input coordinate $j$ to split:\n  1. Width-based heuristic $\\mathsf{H}_{\\mathrm{width}}$: choose $j$ that maximizes $u_j - l_j$ (break ties by smallest index).\n  2. Influence-based heuristic $\\mathsf{H}_{\\mathrm{influence}}$: define an influence score for coordinate $j$ as\n  $$\n  I_j([l,u]) = (u_j - l_j) \\sum_{i=1}^h \\left( |c_i| \\cdot |w_{ij}| \\cdot \\mathbf{1}\\{ z_i^{\\mathrm{U}}  0 \\} \\right),\n  $$\n  where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and $[z_i^{\\mathrm{L}}, z_i^{\\mathrm{U}}]$ is computed from $[l,u]$ as above. Choose $j$ maximizing $I_j([l,u])$ (break ties by smallest index).\n  \nAnytime certification is performed by progressively attempting larger radii within the same fixed budget. For a list of candidate radii $r_1  r_2  \\dots  r_K$, the verifier attempts to certify $r_1$ using at most a per-attempt expansion budget, then if successful, attempts $r_2$, and so on, until either the total expansion budget is exhausted or a candidate fails to certify. The certified radius reported for an input $x_0$ is the largest $r_k$ successfully certified within the budget. This procedure is run separately under each heuristic and the resulting certified radii are compared.\n\nImplement the above for the following concrete test suite. Use input dimension $d = 2$ and hidden dimension $h = 3$ with\n$$\nW_1 = \\begin{bmatrix}\n1.0  -0.5 \\\\\n-0.8  1.0 \\\\\n0.6  0.6\n\\end{bmatrix},\\quad\nb_1 = \\begin{bmatrix}\n0.1 \\\\\n-0.2 \\\\\n0.0\n\\end{bmatrix},\\quad\nc = \\begin{bmatrix}\n1.2 \\\\\n-0.7 \\\\\n0.9\n\\end{bmatrix},\\quad\nb_0 = 0.05.\n$$\nUse the candidate radii list\n$$\n\\{\\, 0.00,\\; 0.05,\\; 0.10,\\; 0.15,\\; 0.20,\\; 0.30 \\,\\}.\n$$\nUse the total expansion budget per input of $120$ expansions and the per-attempt expansion budget of $20$ expansions (so up to $6$ attempts can be made for each input). The test inputs are three centers\n$$\nx_0^{(1)} = \\begin{bmatrix} 0.5 \\\\ -0.3 \\end{bmatrix},\\quad\nx_0^{(2)} = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix},\\quad\nx_0^{(3)} = \\begin{bmatrix} 0.8 \\\\ 0.8 \\end{bmatrix}.\n$$\nThese cases are chosen to cover: a typical case (first), a near-boundary case with small margins (second), and an edge case where both coordinates contribute positively (third).\n\nYour program must:\n- Implement the branch-and-bound verifier using interval bound propagation as described.\n- For each $x_0^{(i)}$, compute the certified radius under $\\mathsf{H}_{\\mathrm{width}}$ and under $\\mathsf{H}_{\\mathrm{influence}}$, respecting the fixed budgets.\n- Compute the average certified radius over the three cases for each heuristic.\n\nFinal output specification:\n- Your program should produce a single line of output containing the two averages as a comma-separated list enclosed in square brackets, in the order $[\\text{average under } \\mathsf{H}_{\\mathrm{width}}, \\text{average under } \\mathsf{H}_{\\mathrm{influence}}]$. For clarity, both values must be printed as floating-point numbers.", "solution": "The user has provided a valid problem statement. The task is to implement a branch-and-bound verifier to determine the certified robustness radius for a small neural network against adversarial perturbations under the $\\ell_\\infty$ norm. The solution will be systematically developed based on the principles of certified robustness, interval bound propagation, and budgeted search algorithms.\n\n### 1. The Principle of Certified Robustness\n\nThe core objective of certified robustness is to provide a formal guarantee that a machine learning model's prediction remains constant for any input within a specified neighborhood of a given data point $x_0$. For a binary classification margin function $m(x)$, this means proving that $m(x)$ remains positive for all $x$ in a perturbation set. The problem defines this set as the $\\ell_\\infty$ ball $\\mathcal{B}_\\infty(x_0,r)$ of radius $r$, which is an axis-aligned hyperrectangle (or box) centered at $x_0$.\n\nTo prove that $m(x)  0$ for all $x \\in \\mathcal{B}_\\infty(x_0,r)$, we must find the global minimum of $m(x)$ within this domain. If this minimum value is strictly positive, the network is certified as robust for the radius $r$. However, finding the exact minimum of a neural network function is an NP-hard problem. Therefore, we resort to sound relaxations that compute a guaranteed lower bound, $L(r) \\le \\min_{x \\in \\mathcal{B}_\\infty(x_0,r)} m(x)$. If we can show that $L(r)  0$, then it follows that the true minimum is also positive, and the network is robust.\n\n### 2. The Branch-and-Bound Framework\n\nThe branch-and-bound algorithm is a systematic method for solving global optimization problems. It operates on the principle of recursively partitioning the search space and pruning partitions that cannot contain the optimal solution.\n\n1.  **Domain Representation**: The initial domain $\\mathcal{B}_\\infty(x_0,r)$ is represented as a single box $[l, u]$, where $l = x_0-r$ and $u=x_0+r$.\n2.  **Bounding**: For each box, we compute a lower bound $m^{\\mathrm{L}}$ and an upper bound $m^{\\mathrm{U}}$ on the margin function $m(x)$. The method for this is Interval Bound Propagation (IBP), detailed below.\n3.  **Branching (Splitting)**: If a box's lower bound $m^{\\mathrm{L}}$ is not sufficient to prove robustness (i.e., $m^{\\mathrm{L}} \\le 0$), we split it into smaller sub-boxes. In this problem, a box is split at its midpoint along a single chosen dimension. This creates two new, smaller boxes that cover the parent's domain.\n4.  **Search Strategy**: To find the global minimum efficiently, we must prioritize exploring the most promising regions. A min-priority queue is used to store all active boxes, prioritized by their lower bound $m^{\\mathrm{L}}$. At each step, we select the box with the smallest lower bound from the queue, as this is the region most likely to contain the true global minimum. This is the \"branch\" step.\n5.  **Termination**: The process continues until one of two conditions is met:\n    a. The smallest lower bound among all active boxes is found to be greater than $0$. In this case, the property is certified as true for the entire domain.\n    b. A predefined computational budget (e.g., number of box expansions) is exhausted. If at this point the minimum lower bound is not greater than $0$, the verifier fails to certify the property within the given budget.\n\n### 3. Interval Bound Propagation (IBP)\n\nIBP is a method for computing bounds on a function's output over a given input interval. It works by propagating intervals layer by layer through the network. For a given input box $[l,u]$ where $x \\in [l,u]$ (element-wise):\n\n1.  **Affine Layer Bounds**: The pre-activation values are given by the affine transformation $z = W_1 x + b_1$. The tightest interval $[z^{\\mathrm{L}}, z^{\\mathrm{U}}]$ for $z$ can be computed using interval arithmetic. The formulas can be vectorized for efficiency. Let $W_1^+ = \\max(0, W_1)$ and $W_1^- = \\min(0, W_1)$ be the positive and negative parts of the weight matrix. The bounds are:\n    $$\n    z^{\\mathrm{L}} = W_1^+ l + W_1^- u + b_1 \\\\\n    z^{\\mathrm{U}} = W_1^+ u + W_1^- l + b_1\n    $$\n2.  **ReLU Activation Bounds**: The ReLU function $\\sigma(z) = \\max(0,z)$ is applied element-wise. The interval for the post-activation values $\\alpha = \\sigma(z)$ is:\n    $$\n    [\\alpha^{\\mathrm{L}}, \\alpha^{\\mathrm{U}}] = [\\max(0, z^{\\mathrm{L}}), \\max(0, z^{\\mathrm{U}})]\n    $$\n    Here, $\\max(0, z^{\\mathrm{L}})$ is an element-wise operation on the vector $z^{\\mathrm{L}}$.\n3.  **Output Margin Bounds**: The final margin is $m(x) = c^\\top \\alpha + b_0$. We apply the same interval arithmetic principle. Let $c^+ = \\max(0, c)$ and $c^- = \\min(0, c)$. The lower bound $m^{\\mathrm{L}}$ and upper bound $m^{\\mathrm{U}}$ on the margin are:\n    $$\n    m^{\\mathrm{L}} = c^{+\\top} \\alpha^{\\mathrm{L}} + c^{-\\top} \\alpha^{\\mathrm{U}} + b_0 \\\\\n    m^{\\mathrm{U}} = c^{+\\top} \\alpha^{\\mathrm{U}} + c^{-\\top} \\alpha^{\\mathrm{L}} + b_0\n    $$\n\n### 4. Splitting Heuristics\n\nThe efficiency of branch-and-bound heavily depends on the choice of which dimension to split at each step. A good heuristic will prioritize splits that lead to the fastest increase in the global lower bound.\n\n1.  **Width-based Heuristic ($\\mathsf{H}_{\\mathrm{width}}$)**: This simple heuristic chooses the dimension $j$ that has the greatest width, i.e., $j = \\arg\\max_k (u_k - l_k)$. The intuition is that reducing the largest dimension of the box is a good general-purpose strategy for reducing uncertainty.\n\n2.  **Influence-based Heuristic ($\\mathsf{H}_{\\mathrm{influence}}$)**: This is a more informed heuristic that considers the structure of the network. It aims to identify the input dimension that has the largest \"influence\" on the output margin's uncertainty. The influence score for an input dimension $j$ is:\n    $$\n    I_j([l,u]) = (u_j - l_j) \\sum_{i=1}^h \\left( |c_i| \\cdot |w_{ij}| \\cdot \\mathbf{1}\\{ z_i^{\\mathrm{U}}  0 \\} \\right)\n    $$\n    -   $(u_j - l_j)$ factors in the input uncertainty along dimension $j$.\n    -   $|w_{ij}|$ and $|c_i|$ account for the sensitivity of the output to changes in input $x_j$, propagated through hidden neuron $i$.\n    -   The indicator $\\mathbf{1}\\{ z_i^{\\mathrm{U}}  0 \\}$ focuses the heuristic on neurons that are \"unstable\" or \"stably on\" within the box $[l,u]$. If a neuron's pre-activation upper bound $z_i^{\\mathrm{U}}$ is non-positive, its activation is constant ($0$) over the entire box, so splitting an input dimension will not affect its contribution to the final margin bound. This heuristic prioritizes splitting dimensions that affect neurons whose uncertainty most impacts the final margin.\n\n### 5. Anytime Certification and Budgeting\n\nIn practice, verification is computationally expensive. The problem specifies an \"anytime\" approach combined with budgeting to find the largest possible certified radius within a fixed computational limit.\n\n-   A sorted list of candidate radii $r_1  r_2  \\dots  r_K$ is attempted sequentially.\n-   The algorithm starts by trying to certify the smallest radius, $r_1$. This attempt is constrained by a `per-attempt budget` of expansions.\n-   If certification for $r_k$ is successful, the algorithm proceeds to attempt to certify the next radius, $r_{k+1}$, again with a `per-attempt budget`. The `total budget` accumulates over all attempts.\n-   The process terminates when either a radius fails to be certified (in which case the previously certified radius is the final result) or the `total budget` of expansions is exhausted. For $r=0$, the check is trivial: if $m(x_0)0$, it's certified with 0 expansions.\n\nThis structure allows the verifier to return a useful, non-zero certified radius quickly, and then progressively improve it if time permits.\nThe implementation will encapsulate this logic, calling the core branch-and-bound verifier for each radius attempt and managing the budgets accordingly. The final result is the average of the largest certified radii found for each test input, computed separately for each of the two heuristics.", "answer": "```python\nimport numpy as np\nimport heapq\n\n# Define network parameters and problem constants globally\nW1 = np.array([[1.0, -0.5], [-0.8, 1.0], [0.6, 0.6]])\nB1 = np.array([0.1, -0.2, 0.0])\nC = np.array([1.2, -0.7, 0.9])\nB0 = 0.05\nD_IN = 2\nH_DIM = 3\n\n# Pre-compute positive and negative parts of weight matrices for IBP\nW1_POS = np.maximum(0, W1)\nW1_NEG = np.minimum(0, W1)\nC_POS = np.maximum(0, C)\nC_NEG = np.minimum(0, C)\n\nclass Box:\n    \"\"\"Represents an axis-aligned box [l, u] with its margin bounds.\"\"\"\n    _box_counter = 0\n\n    def __init__(self, l, u):\n        self.l = l\n        self.u = u\n        self.lower_bound, self.upper_bound = self.compute_bounds()\n        self.id = Box._box_counter\n        Box._box_counter += 1\n\n    def compute_bounds(self):\n        \"\"\"Computes margin bounds using Interval Bound Propagation (IBP).\"\"\"\n        # Layer 1 (Affine)\n        z_L = W1_POS @ self.l + W1_NEG @ self.u + B1\n        z_U = W1_POS @ self.u + W1_NEG @ self.l + B1\n        \n        # Layer 1 (ReLU)\n        alpha_L = np.maximum(0, z_L)\n        alpha_U = np.maximum(0, z_U)\n        \n        # Layer 2 (Output Margin)\n        lower = C_POS @ alpha_L + C_NEG @ alpha_U + B0\n        upper = C_POS @ alpha_U + C_NEG @ alpha_L + B0\n        return lower, upper\n\n    def __lt__(self, other):\n        \"\"\"Comparison for priority queue based on lower bound.\"\"\"\n        if self.lower_bound == other.lower_bound:\n            return self.id  other.id\n        return self.lower_bound  other.lower_bound\n\ndef select_split_dim(box, heuristic):\n    \"\"\"Selects the dimension to split based on the given heuristic.\"\"\"\n    if heuristic == 'width':\n        widths = box.u - box.l\n        return np.argmax(widths)\n    \n    elif heuristic == 'influence':\n        # Recompute z_U for the current box\n        z_U = W1_POS @ box.u + W1_NEG @ box.l + B1\n        is_relevant_neuron = (z_U > 0).astype(float)\n        \n        # Calculate per-neuron influence factors\n        neuron_influence = np.abs(C) * is_relevant_neuron\n        \n        # Aggregate influence per input dimension\n        # A_ij = |c_i| * |w_ij| * indicator\n        # Column sum of A gives for each j: sum_i |c_i|*|w_ij|*indicator\n        # Equivalent to W1.T @ neuron_influence\n        input_influence = np.abs(W1).T @ neuron_influence\n        \n        # Final scores\n        scores = (box.u - box.l) * input_influence\n        return np.argmax(scores)\n    else:\n        raise ValueError(\"Unknown heuristic\")\n\ndef branch_and_bound(initial_box, max_expansions, heuristic):\n    \"\"\"\n    Performs branch-and-bound verification for a single radius.\n    \n    Returns a tuple (is_verified, expansions_used).\n    \"\"\"\n    if initial_box.lower_bound > 0:\n        return True, 0\n\n    pq = [initial_box]\n    expansions_used = 0\n\n    while pq and expansions_used  max_expansions:\n        # Get box with the smallest lower bound\n        current_box = heapq.heappop(pq)\n        \n        # If the minimum lower bound in the entire domain is positive, we are done\n        if current_box.lower_bound > 0:\n            return True, expansions_used\n\n        expansions_used += 1\n\n        # Select dimension to split\n        split_dim = select_split_dim(current_box, heuristic)\n        \n        # Split the box at the midpoint\n        midpoint = (current_box.l[split_dim] + current_box.u[split_dim]) / 2.0\n        \n        # Create first child box\n        l1, u1 = np.copy(current_box.l), np.copy(current_box.u)\n        u1[split_dim] = midpoint\n        box1 = Box(l1, u1)\n        if box1.lower_bound = 0:\n            heapq.heappush(pq, box1)\n\n        # Create second child box\n        l2, u2 = np.copy(current_box.l), np.copy(current_box.u)\n        l2[split_dim] = midpoint\n        box2 = Box(l2, u2)\n        if box2.lower_bound = 0:\n            heapq.heappush(pq, box2)\n\n    # After loop, check if the smallest lower bound in the remaining queue is positive\n    if not pq or pq[0].lower_bound > 0:\n        return True, expansions_used\n    \n    return False, expansions_used\n\ndef run_anytime_certification(x0, radii, total_budget, per_attempt_budget, heuristic):\n    \"\"\"\n    Runs the anytime certification procedure for a given input and heuristic.\n    \"\"\"\n    certified_radius = 0.0\n    total_expansions_so_far = 0\n\n    # Ensure radii are sorted\n    sorted_radii = sorted(radii)\n\n    for r in sorted_radii:\n        remaining_total = total_budget - total_expansions_so_far\n        if remaining_total = 0 and r > 0:\n            break\n            \n        budget_for_this_attempt = min(per_attempt_budget, remaining_total)\n        \n        # For r=0, the domain is a single point.\n        if r == 0:\n            m_x0 = (C @ np.maximum(0, W1 @ x0 + B1)) + B0\n            if m_x0 > 0:\n                certified_radius = 0.0\n                continue\n            else:\n                break\n\n        # Create the initial box for the given radius\n        l, u = x0 - r, x0 + r\n        initial_box = Box(l, u)\n        \n        is_verified, expansions_this_attempt = branch_and_bound(initial_box, budget_for_this_attempt, heuristic)\n        \n        total_expansions_so_far += expansions_this_attempt\n\n        if is_verified:\n            certified_radius = r\n        else:\n            # If a radius fails, no larger radius can be certified\n            break\n            \n    return certified_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases and print the result.\n    \"\"\"\n    test_cases = [\n        np.array([0.5, -0.3]),\n        np.array([0.1, 0.1]),\n        np.array([0.8, 0.8]),\n    ]\n    \n    radii_list = [0.00, 0.05, 0.10, 0.15, 0.20, 0.30]\n    total_exp_budget = 120\n    per_attempt_exp_budget = 20\n    \n    heuristics = ['width', 'influence']\n    avg_results = []\n    \n    for heuristic in heuristics:\n        certified_radii_for_heuristic = []\n        for x0 in test_cases:\n            # Reset box counter for deterministic behavior between runs\n            Box._box_counter = 0\n            radius = run_anytime_certification(\n                x0, radii_list, total_exp_budget, per_attempt_exp_budget, heuristic\n            )\n            certified_radii_for_heuristic.append(radius)\n        \n        avg_radius = np.mean(certified_radii_for_heuristic)\n        avg_results.append(avg_radius)\n        \n    print(f\"[{','.join(map(str, avg_results))}]\")\n\nsolve()\n\n```", "id": "3105282"}]}