{"hands_on_practices": [{"introduction": "Before training complex models, it is crucial to understand the properties of the loss function. This first exercise explores how the Negative Log-Likelihood ($NLL$) behaves when we apply simple transformations, like scaling, to the target variable $y$. By working through this simplified scenario, you will gain concrete insight into why preprocessing your data is not just a practical trick, but has direct mathematical consequences for the model's parameters and loss value [@problem_id:3151379].", "problem": "You are given a simplified Mixture Density Network (MDN) setting with two Gaussian components and known hard assignments. For each observation index $n$, there is an input $x_n \\in \\mathbb{R}$, a scalar output $y_n \\in \\mathbb{R}$, and a known component label $z_n \\in \\{0,1\\}$. The conditional density model is\n$$\np(y_n \\mid x_n, z_n = k) = \\mathcal{N}\\big(y_n \\mid a_k x_n + b_k,\\ \\sigma_k^2\\big),\n$$\nwhere $a_k \\in \\mathbb{R}$, $b_k \\in \\mathbb{R}$, and $\\sigma_k > 0$ are component-specific parameters to be estimated by maximum likelihood. The dataset has $N=11$ points split into two components with labels provided. The given data are:\n\n- Inputs $x$ and labels $z$:\n  - $x = [\\,0.0,\\,1.0,\\,2.0,\\,3.0,\\,4.0,\\,5.0,\\,0.5,\\,1.5,\\,2.5,\\,3.5,\\,4.5\\,]$\n  - $z = [\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- Outputs $y$:\n  - $y = [\\,0.7,\\,1.4,\\,2.5,\\,3.6,\\,4.3,\\,5.55,\\,1.65,\\,1.45,\\,0.7,\\,0.25,\\,-0.15\\,]$\n\nFundamental base and definitions:\n- The Gaussian probability density function (PDF) for a scalar variable $y$ with mean $m$ and variance $s^2$ is\n$$\n\\mathcal{N}(y \\mid m, s^2) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\!\\Big( -\\frac{(y - m)^2}{2 s^2} \\Big).\n$$\n- The negative log-likelihood (NLL) for independent observations $\\{(x_n,y_n,z_n)\\}_{n=1}^N$ under the above model is\n$$\n\\mathrm{NLL}(a,b,\\sigma) = -\\sum_{n=1}^N \\log p(y_n \\mid x_n, z_n) = \\frac{1}{2}\\sum_{n=1}^N \\left[ \\log\\big(2\\pi \\sigma_{z_n}^2\\big) + \\frac{\\big(y_n - (a_{z_n} x_n + b_{z_n})\\big)^2}{\\sigma_{z_n}^2} \\right].\n$$\n- With known labels $z_n$, this decomposes into two independent linear-Gaussian regressions, one per component $k \\in \\{0,1\\}$, with mean functions $a_k x + b_k$ and Gaussian noise with variance $\\sigma_k^2$.\n\nTask:\n1. Implement maximum likelihood estimation for each component $k \\in \\{0,1\\}$:\n   - Estimate the linear parameters $(a_k,b_k)$ by maximizing the likelihood within the component. This is equivalent to minimizing the sum of squared residuals within that component using ordinary least squares on the design matrix with columns $[\\,\\mathbf{1},\\,x\\,]$.\n   - Using the maximum likelihood estimator under the Gaussian model, estimate the variance as\n     $$\n     \\sigma_k^2 = \\frac{1}{N_k} \\sum_{n: z_n = k} \\big( y_n - (a_k x_n + b_k) \\big)^2,\n     $$\n     where $N_k$ is the number of observations assigned to component $k$.\n   - Compute the total negative log-likelihood using the formula provided above.\n\n2. Examine how scaling and standardizing $y$ affects the learned $\\sigma_k$ and the NLL. Define three transformed datasets:\n   - Case A (baseline): use the original $y$ as given.\n   - Case B (scaled): use $y^{(s)} = s\\,y$ with $s=2.0$ and also with $s=-0.5$ (two separate tests).\n   - Case C (standardized): compute the population mean $\\mu_y = \\frac{1}{N}\\sum_{n=1}^N y_n$ and the population standard deviation $\\sigma_y = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N (y_n - \\mu_y)^2}$, then use the standardized output $y^{(\\mathrm{std})} = (y - \\mu_y)/\\sigma_y$.\n\n3. For each transformed dataset, re-estimate $(a_k,b_k,\\sigma_k)$ for $k \\in \\{0,1\\}$ and recompute the total NLL. Test the following claims derived from maximum likelihood principles and linearity of least squares with an intercept:\n   - If $y$ is scaled by a nonzero scalar $s$, then the estimated $\\sigma_k$ scale by $|s|$, i.e., $\\sigma_k^{(s)} / \\sigma_k \\approx |s|$ for $k \\in \\{0,1\\}$, and the total NLL shifts by $N \\log|s|$, i.e., $\\mathrm{NLL}^{(s)} - \\mathrm{NLL} \\approx N \\log|s|$.\n   - If $y$ is standardized to $(y - \\mu_y)/\\sigma_y$, then the estimated $\\sigma_k$ scale by $1/\\sigma_y$, i.e., $\\sigma_k^{(\\mathrm{std})} / \\sigma_k \\approx 1/\\sigma_y$ for $k \\in \\{0,1\\}$, and the total NLL shifts by $N \\log(1/\\sigma_y)$, i.e., $\\mathrm{NLL}^{(\\mathrm{std})} - \\mathrm{NLL} \\approx -N \\log \\sigma_y$.\n\n4. Numerical test suite and required outputs:\n   - Use the original dataset (Case A) to compute baseline $\\sigma_k$ and $\\mathrm{NLL}$.\n   - Test $s=2.0$ (Case B.1) and $s=-0.5$ (Case B.2).\n   - Test standardization using the population standard deviation (denominator $N$) as defined above (Case C).\n   - For each of the three tests (Case B.1, Case B.2, Case C), output a boolean that is `True` if and only if both component-wise $\\sigma_k$ scaling checks and the NLL shift check pass within an absolute tolerance of $10^{-10}$; otherwise output `False`.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three tests in the order $[\\,\\text{Case B.1},\\,\\text{Case B.2},\\,\\text{Case C}\\,]$. For example, an output might look like `[True,True,True]`.", "solution": "The problem is valid. It is scientifically grounded in the principles of maximum likelihood estimation for linear-Gaussian models, is well-posed with all necessary data and definitions provided, and is stated objectively. The tasks involve standard statistical computations and the verification of established theoretical properties concerning the behavior of estimators and likelihood under data transformations.\n\n### Theoretical Foundation\n\nThe problem investigates the properties of Maximum Likelihood Estimators (MLEs) for a simple Mixture Density Network (MDN) with known component assignments. With known assignments $z_n$, the problem decouples into two independent linear regression problems, one for each component $k \\in \\{0, 1\\}$.\n\nFor each component $k$, we model the output $y_n$ as a linear function of the input $x_n$ with additive Gaussian noise:\n$y_n = a_k x_n + b_k + \\epsilon_{n,k}$, where $\\epsilon_{n,k} \\sim \\mathcal{N}(0, \\sigma_k^2)$.\nMaximizing the likelihood of this model is equivalent to minimizing the Negative Log-Likelihood (NLL).\n\nThe parameters $(a_k, b_k)$ are estimated using Ordinary Least Squares (OLS), which minimizes the sum of squared residuals, $\\sum_{n: z_n=k} (y_n - (a_k x_n + b_k))^2$.\nThe MLE for the variance $\\sigma_k^2$ is the mean of the squared residuals using the estimated parameters $(\\hat{a}_k, \\hat{b}_k)$:\n$$\n\\hat{\\sigma}_k^2 = \\frac{1}{N_k} \\sum_{n: z_n=k} \\big(y_n - (\\hat{a}_k x_n + \\hat{b}_k)\\big)^2\n$$\nwhere $N_k$ is the number of data points in component $k$.\n\nOnce the optimal parameters are found, the NLL for component $k$ simplifies. The sum of squared residuals term in the NLL becomes $\\sum_{n: z_n=k} (\\dots)^2 / \\hat{\\sigma}_k^2 = (N_k \\hat{\\sigma}_k^2) / \\hat{\\sigma}_k^2 = N_k$. The total NLL is then:\n$$\n\\mathrm{NLL} = \\sum_{k=0}^{1} \\frac{N_k}{2} \\left[ \\log(2\\pi \\hat{\\sigma}_k^2) + 1 \\right]\n$$\nThe problem tests the behavior of these estimators and the NLL under affine transformations of the target variable $y$. If $y$ is transformed to $y' = s y + d$, where $s$ is a scaling factor and $d$ is a shift, the OLS estimators for the linear model with an intercept term $b_k$ transform as $\\hat{a}'_k = s \\hat{a}_k$ and $\\hat{b}'_k = s \\hat{b}_k + d$. The residuals transform as $e'_n = s e_n$, leading to the estimated standard deviation scaling by $|s|$, i.e., $\\hat{\\sigma}'_k = |s|\\hat{\\sigma}_k$. Consequently, the total NLL shifts by an additive constant related to the scaling factor: $\\mathrm{NLL}' - \\mathrm{NLL} = N \\log|s|$, where $N = N_0 + N_1$. The claims in the problem statement are direct consequences of these principles.\n\n### Methodology\n\nThe solution proceeds as follows:\n\n1.  **Data Partitioning**: The dataset $\\{(x_n, y_n, z_n)\\}_{n=1}^N$ is partitioned into two subsets based on the component label $z_n \\in \\{0, 1\\}$. This creates two independent datasets, one for each component.\n\n2.  **Parameter Estimation**: A dedicated function estimates the model parameters for a single component's dataset $(x_k, y_k)$.\n    *   The OLS parameters $(a_k, b_k)$ are found by solving the normal equations. For a design matrix $X_k$ with columns for the intercept (a vector of ones) and the inputs $x_k$, and parameter vector $\\beta_k = [b_k, a_k]^T$, we solve $(X_k^T X_k) \\beta_k = X_k^T y_k$.\n    *   Using the estimated $\\hat{\\beta}_k$, predictions $\\hat{y}_k = X_k \\hat{\\beta}_k$ are made.\n    *   The MLE for the standard deviation $\\hat{\\sigma}_k$ is computed as the square root of the mean squared error: $\\hat{\\sigma}_k = \\sqrt{\\frac{1}{N_k} \\sum_{n=1}^{N_k} (y_{k,n} - \\hat{y}_{k,n})^2}$.\n\n3.  **NLL Calculation**: The total NLL is calculated using the simplified formula derived from the MLE properties, summing the contributions from each component.\n\n4.  **Baseline Calculation (Case A)**: The above procedure is first applied to the original, untransformed dataset to establish baseline values for the standard deviations ($\\hat{\\sigma}_{0,\\text{base}}, \\hat{\\sigma}_{1,\\text{base}}$) and the total NLL ($\\mathrm{NLL}_{\\text{base}}$).\n\n5.  **Transformed Cases (B and C)**: For each specified transformation of $y$:\n    *   **Case B (Scaling)**: The target variable is scaled: $y^{(s)} = s \\cdot y$ for $s=2.0$ and $s=-0.5$.\n    *   **Case C (Standardization)**: The target variable is standardized: $y^{(\\mathrm{std})} = (y - \\mu_y) / \\sigma_y$, where $\\mu_y$ and $\\sigma_y$ are the sample mean and population standard deviation of the original $y$ vector, respectively.\n    *   For each transformed dataset, the parameters $(\\hat{a}'_k, \\hat{b}'_k, \\hat{\\sigma}'_k)$ and the new NLL, $\\mathrm{NLL}'$, are re-estimated.\n\n6.  **Verification**: The numerically computed results are compared against the theoretical predictions. For each test case, the implementation verifies the following two conditions within an absolute tolerance of $10^{-10}$:\n    *   **Sigma Scaling**: $\\hat{\\sigma}'_k / \\hat{\\sigma}_{k,\\text{base}} \\approx |s|$ for both $k=0$ and $k=1$. For standardization, the scaling factor is $1/\\sigma_y$.\n    *   **NLL Shift**: $\\mathrm{NLL}' - \\mathrm{NLL}_{\\text{base}} \\approx N \\log|s|$. For standardization, this is $N \\log(1/\\sigma_y) = -N \\log(\\sigma_y)$.\n    A test case is considered passed (True) if and only if all corresponding checks are satisfied.\n\nThe implementation encapsulates this logic in a self-contained script.", "answer": "```python\nimport numpy as np\n\ndef estimate_params_and_nll(x_all, y_all, z_all):\n    \"\"\"\n    Estimates parameters and NLL for the two-component model.\n    \"\"\"\n    params = {}\n    total_nll = 0.0\n    \n    # Total number of observations\n    N = len(x_all)\n    \n    for k in [0, 1]:\n        mask = (z_all == k)\n        x_k = x_all[mask]\n        y_k = y_all[mask]\n        N_k = len(x_k)\n\n        # Design matrix with intercept: X = [1, x]\n        X_k = np.vstack([np.ones(N_k), x_k]).T\n        \n        # Solve normal equations: (X^T X) beta = X^T y for beta = [b, a]^T\n        try:\n            XTX = X_k.T @ X_k\n            XTy = X_k.T @ y_k\n            beta_k = np.linalg.solve(XTX, XTy)\n            b_k, a_k = beta_k[0], beta_k[1]\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, though not expected here\n            # For this problem, X is full rank for both components.\n            return None\n\n        # Calculate residuals and MLE variance/sigma\n        y_pred_k = X_k @ beta_k\n        residuals_k = y_k - y_pred_k\n        sigma_k_sq = np.mean(residuals_k**2)\n        sigma_k = np.sqrt(sigma_k_sq)\n        \n        params[k] = {'a': a_k, 'b': b_k, 'sigma': sigma_k}\n\n    # Alternative and more direct NLL calculation from definition to avoid potential\n    # issues if the simplified formula derivation has subtle assumptions.\n    # It turns out the results are identical.\n    nll_terms = []\n    for i in range(N):\n        k = z_all[i]\n        a, b, sigma = params[k]['a'], params[k]['b'], params[k]['sigma']\n        mean = a * x_all[i] + b\n        sigma_sq = sigma**2\n        term = 0.5 * ( np.log(2 * np.pi * sigma_sq) + ((y_all[i] - mean)**2) / sigma_sq )\n        nll_terms.append(term)\n    \n    total_nll = np.sum(nll_terms)\n\n    return params[0]['sigma'], params[1]['sigma'], total_nll\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem tasks and generate the final output.\n    \"\"\"\n    # Given data\n    x = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 0.5, 1.5, 2.5, 3.5, 4.5])\n    y = np.array([0.7, 1.4, 2.5, 3.6, 4.3, 5.55, 1.65, 1.45, 0.7, 0.25, -0.15])\n    z = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n    N = len(x)\n    TOL = 1e-10\n\n    # Case A: Baseline\n    sigma0_base, sigma1_base, nll_base = estimate_params_and_nll(x, y, z)\n\n    # Test cases to evaluate\n    test_cases = [\n        {'type': 'scale', 's': 2.0, 'name': 'Case B.1'},\n        {'type': 'scale', 's': -0.5, 'name': 'Case B.2'},\n        {'type': 'standardize', 'name': 'Case C'}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        if case['type'] == 'scale':\n            s = case['s']\n            y_transformed = s * y\n            \n            sigma0_new, sigma1_new, nll_new = estimate_params_and_nll(x, y_transformed, z)\n            \n            # Check sigma scaling\n            check_sigma0 = np.isclose(sigma0_new / sigma0_base, abs(s), atol=TOL)\n            check_sigma1 = np.isclose(sigma1_new / sigma1_base, abs(s), atol=TOL)\n            \n            # Check NLL shift\n            nll_shift_pred = N * np.log(abs(s))\n            nll_shift_obs = nll_new - nll_base\n            check_nll = np.isclose(nll_shift_obs, nll_shift_pred, atol=TOL)\n            \n            results.append(check_sigma0 and check_sigma1 and check_nll)\n\n        elif case['type'] == 'standardize':\n            mu_y = np.mean(y)\n            sigma_y = np.std(y) # Population std dev (ddof=0)\n            \n            y_transformed = (y - mu_y) / sigma_y\n            \n            sigma0_new, sigma1_new, nll_new = estimate_params_and_nll(x, y_transformed, z)\n            \n            # Check sigma scaling\n            check_sigma0 = np.isclose(sigma0_new / sigma0_base, 1.0 / sigma_y, atol=TOL)\n            check_sigma1 = np.isclose(sigma1_new / sigma1_base, 1.0 / sigma_y, atol=TOL)\n            \n            # Check NLL shift\n            nll_shift_pred = N * np.log(1.0 / sigma_y) # or -N * np.log(sigma_y)\n            nll_shift_obs = nll_new - nll_base\n            check_nll = np.isclose(nll_shift_obs, nll_shift_pred, atol=TOL)\n            \n            results.append(check_sigma0 and check_sigma1 and check_nll)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Run the solver\nsolve()\n\n```", "id": "3151379"}, {"introduction": "A common failure mode when training MDNs is \"component collapse,\" where multiple mixture components converge to model the same part of the data, wasting model capacity. To combat this, we can modify the loss function to explicitly encourage diversity. This practice guides you through implementing a custom regularizer that penalizes models where component means $\\mu_k$ are too close, teaching you a powerful technique for shaping a model's behavior during training [@problem_id:3151334].", "problem": "Consider a Mixture Density Network (MDN), that models a conditional probability density as a finite mixture of isotropic Gaussian components. The foundational base consists of the following well-tested facts and definitions. A univariate Gaussian probability density function for a real-valued random variable is given by the formula $$\\mathcal{N}(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right),$$ where $y \\in \\mathbb{R}$ is an observation, $\\mu \\in \\mathbb{R}$ is the mean, and $\\sigma > 0$ is the standard deviation. A finite mixture of $K$ Gaussian components with mixture weights $\\pi_k$ satisfying $\\pi_k \\ge 0$ and $\\sum_{k=1}^K \\pi_k = 1$ defines the mixture density $$p(y) = \\sum_{k=1}^K \\pi_k \\,\\mathcal{N}(y \\mid \\mu_k, \\sigma_k^2).$$ Given a dataset of $N$ observations $\\{y_n\\}_{n=1}^N$ with $y_n \\in \\mathbb{R}$, the negative log-likelihood is defined as $$\\mathrm{NLL} = -\\sum_{n=1}^N \\log\\left( \\sum_{k=1}^K \\pi_k \\,\\mathcal{N}(y_n \\mid \\mu_k, \\sigma_k^2)\\right).$$\n\nYour task is to create a diversity-encouraging regularizer on the component means that, when combined with the fit loss (the negative log-likelihood), yields a composite objective that explicitly balances fitting the data against increasing pairwise separation of component means. Define the diversity regularizer as the negative of the average pairwise Euclidean distance among the $K$ means, $$R_{\\mathrm{div}}(\\{\\mu_k\\}_{k=1}^K) = -\\frac{1}{\\binom{K}{2}} \\sum_{1 \\le i < j \\le K} \\|\\mu_i - \\mu_j\\|_2,$$ and define the composite loss as $$\\mathcal{L} = \\mathrm{NLL} + \\lambda \\, R_{\\mathrm{div}}(\\{\\mu_k\\}),$$ where $\\lambda \\ge 0$ is a scalar regularization weight. This composite objective formalizes the trade-off: decreasing $\\mathrm{NLL}$ fits the data more closely, while making $R_{\\mathrm{div}}$ more negative increases diversity by encouraging $\\|\\mu_i - \\mu_j\\|_2$ to be larger.\n\nImplement a program that, for specified parameter sets, computes the composite loss $\\mathcal{L}$, the regularizer value $R_{\\mathrm{div}}$, and the negative log-likelihood $\\mathrm{NLL}$, using stable arithmetic for the mixture log-density. Use the log-sum-exp identity to evaluate $\\log\\left(\\sum_k \\exp(a_k)\\right)$ as $m + \\log\\left(\\sum_k \\exp(a_k - m)\\right)$ with $m = \\max_k a_k$, to prevent numerical underflow or overflow.\n\nThere are no physical units involved in this problem. All angles, if any, are in radians, though none are used here. All outputs must be expressed as floating-point numbers rounded to $6$ decimal places.\n\nImplement the computation for the following test suite of parameter sets, which is designed to cover a general case, a boundary condition, and edge cases. In all cases, the data are $$y = [-1.8, -2.2, 1.9, 2.1],$$ and the mixture weights are $$\\pi = [0.5, 0.5].$$\n\n- Test case $1$ (general \"happy path\"): $$K = 2,\\quad \\mu = [-2.0, 2.0],\\quad \\sigma = [0.5, 0.5],\\quad \\lambda = 0.1.$$\n- Test case $2$ (boundary: identical means, zero diversity): $$K = 2,\\quad \\mu = [0.0, 0.0],\\quad \\sigma = [0.5, 0.5],\\quad \\lambda = 0.1.$$\n- Test case $3$ (edge: very large mean separation with larger regularization weight): $$K = 2,\\quad \\mu = [-10.0, 10.0],\\quad \\sigma = [0.5, 0.5],\\quad \\lambda = 5.0.$$\n- Test case $4$ (edge: very small variances): $$K = 2,\\quad \\mu = [-2.0, 2.0],\\quad \\sigma = [0.1, 0.1],\\quad \\lambda = 0.5.$$\n\nYour program should:\n- Compute, for each test case, the triple $[\\mathcal{L}, R_{\\mathrm{div}}, \\mathrm{NLL}]$, each rounded to $6$ decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case, itself formatted as a list. For example, the final output should look like $$[[\\mathcal{L}_1,R_{\\mathrm{div},1},\\mathrm{NLL}_1],[\\mathcal{L}_2,R_{\\mathrm{div},2},\\mathrm{NLL}_2],\\ldots].$$", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical machine learning, specifically concerning Mixture Density Networks (MDNs). The definitions of the Gaussian probability density function, mixture model, negative log-likelihood, and the proposed regularizer are mathematically sound and standard or plausible within the discipline. The problem is well-posed, providing all necessary data and parameters for the required computations, and the task is to perform a deterministic calculation. The language is objective and unambiguous. There are no contradictions, missing information, or violations of scientific principles.\n\nThe task is to compute a composite loss function $\\mathcal{L}$, a diversity regularizer $R_{\\mathrm{div}}$, and the negative log-likelihood $\\mathrm{NLL}$ for a given set of parameters. The process for this computation is as follows.\n\nFirst, we calculate the diversity regularizer, $R_{\\mathrm{div}}$. The definition is given as $$R_{\\mathrm{div}}(\\{\\mu_k\\}_{k=1}^K) = -\\frac{1}{\\binom{K}{2}} \\sum_{1 \\le i < j \\le K} \\|\\mu_i - \\mu_j\\|_2.$$ For all test cases, the number of mixture components is $K=2$. The number of pairs of means is $\\binom{2}{2} = 1$. The problem is univariate, so the means $\\mu_k$ are scalars. The $L_2$ norm $\\|\\mu_i - \\mu_j\\|_2$ simplifies to the absolute difference $|\\mu_i - \\mu_j|$. Therefore, for $K=2$, the regularizer is $$R_{\\mathrm{div}} = -|\\mu_1 - \\mu_2|.$$\n\nSecond, we calculate the negative log-likelihood, $\\mathrm{NLL}$. For a dataset $\\{y_n\\}_{n=1}^N$, the NLL is given by $$\\mathrm{NLL} = -\\sum_{n=1}^N \\log\\left(p(y_n)\\right),$$ where $p(y_n)$ is the mixture probability density evaluated at the data point $y_n$: $$p(y_n) = \\sum_{k=1}^K \\pi_k \\,\\mathcal{N}(y_n \\mid \\mu_k, \\sigma_k^2).$$ Direct computation of this sum and then its logarithm is prone to numerical instability. To ensure stability, we compute the logarithm of the mixture density, $\\log p(y_n)$, using the log-sum-exp identity. Let $a_{n,k} = \\log\\left(\\pi_k \\,\\mathcal{N}(y_n \\mid \\mu_k, \\sigma_k^2)\\right)$. We can rewrite $\\log p(y_n)$ as $$\\log p(y_n) = \\log\\left(\\sum_{k=1}^K \\exp(a_{n,k})\\right).$$\nThe term $a_{n,k}$ can be expanded using the definition of the Gaussian log-PDF:\n$$a_{n,k} = \\log(\\pi_k) - \\frac{1}{2}\\log(2\\pi) - \\log(\\sigma_k) - \\frac{(y_n - \\mu_k)^2}{2\\sigma_k^2}.$$\nWe apply the log-sum-exp identity for each data point $y_n$. First, we compute the vector of terms $[a_{n,1}, a_{n,2}, \\dots, a_{n,K}]$. Then we find the maximum value, $m_n = \\max_k a_{n,k}$. The log-density is then calculated as:\n$$\\log p(y_n) = m_n + \\log\\left(\\sum_{k=1}^K \\exp(a_{n,k} - m_n)\\right).$$\nThis formulation prevents overflow, as the arguments to $\\exp$ are at most $0$, and it prevents underflow by factoring out the largest term, ensuring at least one term in the sum is $\\exp(0)=1$. The total NLL is the negative sum of these log-probabilities over all $N$ data points: $$\\mathrm{NLL} = -\\sum_{n=1}^N \\left( m_n + \\log\\left(\\sum_{k=1}^K \\exp(a_{n,k} - m_n)\\right) \\right).$$\n\nThird, we compute the composite loss, $\\mathcal{L}$. This is a weighted sum of the NLL and the diversity regularizer:\n$$\\mathcal{L} = \\mathrm{NLL} + \\lambda \\, R_{\\mathrm{div}},$$\nwhere $\\lambda \\ge 0$ is the regularization weight.\n\nThe overall algorithm for each test case is:\n1.  Calculate $R_{\\mathrm{div}}$ using the given means $\\mu_1$ and $\\mu_2$.\n2.  Initialize a total log-likelihood accumulator to $0$.\n3.  For each data point $y_n$ in the dataset:\n    a. For each mixture component $k \\in \\{1, \\dots, K\\}$, compute the log-term $a_{n,k}$.\n    b. Use the log-sum-exp trick to compute $\\log p(y_n)$.\n    c. Add $\\log p(y_n)$ to the total log-likelihood accumulator.\n4.  Calculate NLL by negating the total log-likelihood.\n5.  Calculate $\\mathcal{L}$ using the computed NLL, $R_{\\mathrm{div}}$, and the given $\\lambda$.\n6.  Round the final values of $\\mathcal{L}$, $R_{\\mathrm{div}}$, and NLL to $6$ decimal places.\n\nThis procedure is deterministic and will be implemented for each of the four specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Mixture Density Network loss calculation problem for a suite of test cases.\n    \"\"\"\n\n    def compute_losses(y_data, pi, mu, sigma, K, lambda_reg):\n        \"\"\"\n        Computes the composite loss, diversity regularizer, and NLL for one set of parameters.\n\n        Args:\n            y_data (list or np.ndarray): The dataset of observations.\n            pi (list or np.ndarray): The mixture weights.\n            mu (list or np.ndarray): The means of the Gaussian components.\n            sigma (list or np.ndarray): The standard deviations of the Gaussian components.\n            K (int): The number of mixture components.\n            lambda_reg (float): The regularization weight.\n\n        Returns:\n            list: A list containing [composite_loss, diversity_regularizer, nll].\n        \"\"\"\n\n        # 1. Calculate the diversity regularizer, R_div\n        if K < 2:\n            r_div = 0.0\n        else:\n            # The problem gives K=2, so the sum has only one term.\n            # The norm ||mu_i - mu_j||_2 for scalars is |mu_i - mu_j|.\n            # The number of pairs is C(2,2) = 1.\n            r_div = -np.abs(mu[0] - mu[1])\n\n        # 2. Calculate the Negative Log-Likelihood (NLL)\n        \n        def gaussian_log_pdf(y, mu_k, sigma_k):\n            \"\"\"Computes the log of the Gaussian PDF.\"\"\"\n            var = sigma_k**2\n            return -0.5 * np.log(2 * np.pi * var) - ((y - mu_k)**2) / (2 * var)\n\n        def log_sum_exp(terms):\n            \"\"\"Computes log(sum(exp(terms))) stably.\"\"\"\n            max_term = np.max(terms)\n            return max_term + np.log(np.sum(np.exp(terms - max_term)))\n\n        total_log_likelihood = 0.0\n        log_pi = np.log(pi)\n\n        for y_n in y_data:\n            # For each data point y_n, compute the arguments for log-sum-exp\n            a_n_k = np.zeros(K)\n            for k in range(K):\n                a_n_k[k] = log_pi[k] + gaussian_log_pdf(y_n, mu[k], sigma[k])\n            \n            # Compute log p(y_n) using the log-sum-exp trick\n            log_p_yn = log_sum_exp(a_n_k)\n            total_log_likelihood += log_p_yn\n        \n        nll = -total_log_likelihood\n\n        # 3. Calculate the composite loss, L\n        composite_loss = nll + lambda_reg * r_div\n        \n        return [composite_loss, r_div, nll]\n\n    # Data and parameters common to all test cases\n    y_data = np.array([-1.8, -2.2, 1.9, 2.1])\n    pi_base = np.array([0.5, 0.5])\n    K_base = 2\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Case 1: General\n        {'K': K_base, 'mu': np.array([-2.0, 2.0]), 'sigma': np.array([0.5, 0.5]), 'lambda': 0.1},\n        # Case 2: Boundary (identical means)\n        {'K': K_base, 'mu': np.array([0.0, 0.0]), 'sigma': np.array([0.5, 0.5]), 'lambda': 0.1},\n        # Case 3: Edge (large mean separation)\n        {'K': K_base, 'mu': np.array([-10.0, 10.0]), 'sigma': np.array([0.5, 0.5]), 'lambda': 5.0},\n        # Case 4: Edge (small variances)\n        {'K': K_base, 'mu': np.array([-2.0, 2.0]), 'sigma': np.array([0.1, 0.1]), 'lambda': 0.5},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        loss_vals = compute_losses(y_data, pi_base, case['mu'], case['sigma'], case['K'], case['lambda'])\n        \n        # Round each value to 6 decimal places\n        rounded_vals = [round(v, 6) for v in loss_vals]\n        all_results.append(rounded_vals)\n\n    # Format the final output string as a list of lists, with no internal spaces.\n    results_as_strings = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in all_results]\n    final_output_string = f\"[{','.join(results_as_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "3151334"}, {"introduction": "Minimizing the $NLL$ during training does not guarantee that our model produces high-quality probabilistic forecasts. How do we measure the quality of a predicted distribution against a single observed outcome? This final exercise introduces proper scoring rules, such as the Logarithmic Score and the Continuous Ranked Probability Score ($CRPS$), which are designed for this exact purpose, moving your evaluation skills beyond simple loss metrics toward a more complete assessment of your MDN's performance [@problem_id:3151363].", "problem": "Consider a Mixture Density Network (MDN), which is a neural network that outputs parameters of a mixture distribution for a conditional target. In this problem, the MDN models the conditional predictive density of temperature given features, denoted by $p(T \\mid x)$, where $T$ is temperature (in degrees Celsius) and $x$ is a feature vector. Specifically, the MDN outputs a Gaussian mixture with $K$ components: mixture weights $w_k(x)$ satisfying $w_k(x) \\ge 0$ and $\\sum_{k=1}^K w_k(x) = 1$, component means $\\mu_k(x)$ (in degrees Celsius), and component standard deviations $\\sigma_k(x)$ (in degrees Celsius), for $k \\in \\{1,\\dots,K\\}$. The predictive density for $T$ is therefore a mixture of Gaussian distributions.\n\nYour task is to evaluate the calibration of these MDN predictive densities using two proper scoring rules: the logarithmic score and the Continuous Ranked Probability Score (CRPS). The logarithmic score is defined as the natural logarithm of the predictive density evaluated at the realized observation, and CRPS is defined as an integral of squared difference between the predictive cumulative distribution function (CDF) and the empirical CDF of the observation. Both scores must be derived and implemented starting solely from core probabilistic definitions without relying on pre-stated shortcut formulas. You must compute the average (mean) logarithmic score and the average (mean) CRPS over each provided test case.\n\nUse the following test suite. Each test case consists of a set of independent samples, and each sample gives the realized observation $T$ together with the MDN predictive mixture parameters $(w_k, \\mu_k, \\sigma_k)_{k=1}^K$ for that sample. All temperatures and standard deviations are in degrees Celsius, and the logarithmic score uses the natural logarithm (base $e$). Report CRPS in degrees Celsius.\n\nTest Case $1$ (happy path: multi-modal, moderate dispersion):\n- Sample $1$: $T = 21$, $K=3$, $w = [0.5, 0.3, 0.2]$, $\\mu = [18, 22, 26]$, $\\sigma = [2.0, 1.5, 3.5]$.\n- Sample $2$: $T = 19$, $K=3$, $w = [0.4, 0.4, 0.2]$, $\\mu = [17, 20, 28]$, $\\sigma = [2.5, 1.2, 4.0]$.\n- Sample $3$: $T = 24$, $K=3$, $w = [0.2, 0.5, 0.3]$, $\\mu = [15, 23, 27]$, $\\sigma = [2.0, 2.0, 3.0]$.\n- Sample $4$: $T = 29$, $K=3$, $w = [0.3, 0.3, 0.4]$, $\\mu = [16, 21, 30]$, $\\sigma = [1.8, 1.8, 5.0]$.\n- Sample $5$: $T = 20$, $K=3$, $w = [0.6, 0.2, 0.2]$, $\\mu = [19, 25, 31]$, $\\sigma = [1.0, 2.5, 6.0]$.\n\nTest Case $2$ (boundary condition: near-deterministic single-component predictions):\n- Sample $1$: $T = 20.1$, $K=1$, $w = [1.0]$, $\\mu = [20.0]$, $\\sigma = [0.05]$.\n- Sample $2$: $T = 14.8$, $K=1$, $w = [1.0]$, $\\mu = [15.0]$, $\\sigma = [0.1]$.\n- Sample $3$: $T = 33.0$, $K=1$, $w = [1.0]$, $\\mu = [30.0]$, $\\sigma = [0.2]$.\n\nTest Case $3$ (edge cases: extreme observations, tiny variances, and skewed weights):\n- Sample $1$: $T = 22.5$, $K=2$, $w = [0.9, 0.1]$, $\\mu = [22.0, 5.0]$, $\\sigma = [2.0, 0.01]$.\n- Sample $2$: $T = 35.0$, $K=2$, $w = [0.01, 0.99]$, $\\mu = [35.0, 25.0]$, $\\sigma = [1.0, 2.0]$.\n- Sample $3$: $T = 20.0$, $K=2$, $w = [0.5, 0.5]$, $\\mu = [10.0, 30.0]$, $\\sigma = [1.5, 1.5]$.\n- Sample $4$: $T = 16.0$, $K=2$, $w = [0.7, 0.3]$, $\\mu = [28.0, 18.0]$, $\\sigma = [3.0, 0.5]$.\n\nRequirements:\n- Derive and implement the logarithmic score and the Continuous Ranked Probability Score (CRPS) from core probabilistic definitions. The derivation must start from the definition of a probability density function (PDF) and cumulative distribution function (CDF) and use the properties of expectations for independent random variables.\n- For each sample, compute the logarithmic score $s_{\\log} = \\log p(T \\mid x)$ and the CRPS $s_{\\mathrm{CRPS}}$ for its mixture predictive distribution. Then compute the mean over samples for each test case.\n- Express CRPS in degrees Celsius and use the natural logarithm for the logarithmic score.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[$mean log score of Test Case $1$, mean CRPS of Test Case $1$, mean log score of Test Case $2$, mean CRPS of Test Case $2$, mean log score of Test Case $3$, mean CRPS of Test Case $3]$. Each number should be rounded to six decimal places.\n\nThe scenario, parameters, and data are scientifically plausible and self-consistent. No external data files are allowed, and the program must be entirely self-contained and deterministic. The final outputs for the test suite must be floats.", "solution": "The problem requires the evaluation of Mixture Density Network (MDN) predictions using the logarithmic score and the Continuous Ranked Probability Score (CRPS). We must first derive the computational formulas for these scores for a Gaussian Mixture Model (GMM) from fundamental principles, and then apply them to the provided test cases.\n\nLet the conditional predictive density for the temperature $T$ given features $x$ be a GMM with $K$ components:\n$$\np(T \\mid x) = \\sum_{k=1}^{K} w_k \\mathcal{N}(T \\mid \\mu_k, \\sigma_k^2)\n$$\nwhere $w_k$, $\\mu_k$, and $\\sigma_k$ are the weight, mean, and standard deviation of the $k$-th component, respectively. The weights satisfy $w_k \\ge 0$ and $\\sum_{k=1}^{K} w_k = 1$. For brevity, we will denote the observation $T_{obs}$ as $y$.\n\nThe probability density function (PDF) for a single Gaussian component $\\mathcal{N}(t \\mid \\mu, \\sigma^2)$ is:\n$$\n\\phi(t; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{t - \\mu}{\\sigma}\\right)^2\\right)\n$$\nThe corresponding cumulative distribution function (CDF) is denoted $\\Phi(t; \\mu, \\sigma)$. For the standard normal distribution ($\\mu=0, \\sigma=1$), we use the notations $\\phi(z)$ and $\\Phi(z)$.\n\n**1. Logarithmic Score**\n\nThe logarithmic score is defined as the natural logarithm of the predictive probability density function evaluated at the actual observation $y$. A higher score indicates a better prediction.\n\nFor a GMM, the predictive density at $y$ is:\n$$\np(y \\mid x) = \\sum_{k=1}^{K} w_k \\phi(y; \\mu_k, \\sigma_k)\n$$\nThe logarithmic score $s_{\\log}$ for a single observation $y$ is therefore:\n$$\ns_{\\log} = \\ln \\left( p(y \\mid x) \\right) = \\ln \\left( \\sum_{k=1}^{K} w_k \\frac{1}{\\sigma_k\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{y - \\mu_k}{\\sigma_k}\\right)^2\\right) \\right)\n$$\nTo evaluate the model over a test case, we compute the mean of the logarithmic scores for all samples in that case.\n\n**2. Continuous Ranked Probability Score (CRPS)**\n\nThe CRPS is a proper scoring rule that measures the difference between a predictive distribution and an observation. It is defined as the integrated squared error between the predictive CDF, $F(t) = P(X \\le t)$, and the empirical CDF of the observation $y$, which is a Heaviside step function $H(t-y)$.\n$$\ns_{CRPS}(F, y) = \\int_{-\\infty}^{\\infty} (F(t) - H(t-y))^2 dt\n$$\nThe CRPS is expressed in the same units as the observation, which is degrees Celsius in this problem. Lower values indicate better calibration.\n\nA fundamental and more computationally convenient identity for CRPS expresses it in terms of expectations:\n$$\ns_{CRPS}(F, y) = \\mathbb{E}_{X \\sim F}[|X-y|] - \\frac{1}{2}\\mathbb{E}_{X, X' \\sim F}[|X-X'|]\n$$\nwhere $X$ and $X'$ are independent random variables drawn from the predictive distribution with CDF $F$. We will derive the CRPS for the GMM using this identity.\n\nLet $X$ be a random variable drawn from the mixture distribution $p(t) = \\sum_{k=1}^{K} w_k \\phi(t; \\mu_k, \\sigma_k)$.\n\n**Term 1: $\\mathbb{E}[|X-y|]$**\nBy the law of total expectation, we can write:\n$$\n\\mathbb{E}[|X-y|] = \\sum_{k=1}^{K} w_k \\mathbb{E}_{Z_k}[|Z_k-y|]\n$$\nwhere $Z_k$ is a random variable drawn from the $k$-th component, $Z_k \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2)$. Let $U_k = Z_k - y$. Then $U_k \\sim \\mathcal{N}(\\mu_k - y, \\sigma_k^2)$. We need to find the expectation $\\mathbb{E}[|U|]$ for a normally distributed variable $U \\sim \\mathcal{N}(m, s^2)$. The result is a standard one:\n$$\n\\mathbb{E}[|U|] = 2s\\phi(m/s) + m(2\\Phi(m/s) - 1)\n$$\nApplying this to $U_k$ with $m = \\mu_k - y$ and $s = \\sigma_k$, we get:\n$$\n\\mathbb{E}_{Z_k}[|Z_k-y|] = 2\\sigma_k\\phi\\left(\\frac{\\mu_k-y}{\\sigma_k}\\right) + (\\mu_k-y)\\left(2\\Phi\\left(\\frac{\\mu_k-y}{\\sigma_k}\\right) - 1\\right)\n$$\nSo, the first term of the CRPS is:\n$$\n\\mathbb{E}[|X-y|] = \\sum_{k=1}^{K} w_k \\left[ 2\\sigma_k\\phi\\left(\\frac{\\mu_k-y}{\\sigma_k}\\right) + (\\mu_k-y)\\left(2\\Phi\\left(\\frac{\\mu_k-y}{\\sigma_k}\\right) - 1\\right) \\right]\n$$\n\n**Term 2: $\\frac{1}{2}\\mathbb{E}[|X-X'|]$**\nHere, $X$ and $X'$ are independent and identically distributed draws from the mixture. A draw from a mixture means we first select a component $k$ with probability $w_k$, then draw from $\\mathcal{N}(\\mu_k, \\sigma_k^2)$. Thus:\n$$\n\\mathbb{E}[|X-X'|] = \\sum_{i=1}^{K} \\sum_{j=1}^{K} w_i w_j \\mathbb{E}[|Z_i - Z_j'|]\n$$\nwhere $Z_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$ and $Z_j' \\sim \\mathcal{N}(\\mu_j, \\sigma_j^2)$ are independent. The difference $D_{ij} = Z_i - Z_j'$ is also a normal random variable. Its mean is $\\mathbb{E}[D_{ij}] = \\mu_i - \\mu_j$ and its variance is $Var(D_{ij}) = \\sigma_i^2 + \\sigma_j^2$.\nSo, $D_{ij} \\sim \\mathcal{N}(\\mu_i - \\mu_j, \\sigma_i^2 + \\sigma_j^2)$.\nWe use the same formula for $\\mathbb{E}[|U|]$ with $m = \\mu_i - \\mu_j$ and $s = \\sqrt{\\sigma_i^2 + \\sigma_j^2}$:\n$$\n\\mathbb{E}[|Z_i - Z_j'|] = 2\\sqrt{\\sigma_i^2 + \\sigma_j^2} \\phi\\left(\\frac{\\mu_i - \\mu_j}{\\sqrt{\\sigma_i^2 + \\sigma_j^2}}\\right) + (\\mu_i - \\mu_j)\\left(2\\Phi\\left(\\frac{\\mu_i - \\mu_j}{\\sqrt{\\sigma_i^2 + \\sigma_j^2}}\\right) - 1\\right)\n$$\nThe second term of the CRPS is $\\frac{1}{2}$ of the expectation over this double summation.\n\n**Final CRPS Formula for GMM**\nCombining the two terms, the CRPS for a GMM prediction is:\n\\begin{align*}\ns_{CRPS} = & \\sum_{k=1}^{K} w_k \\left[ \\sigma_k \\left( 2\\phi\\left(\\frac{y-\\mu_k}{\\sigma_k}\\right) + \\frac{y-\\mu_k}{\\sigma_k} \\left(2\\Phi\\left(\\frac{y-\\mu_k}{\\sigma_k}\\right) - 1\\right) \\right) \\right] \\\\\n& - \\frac{1}{2} \\sum_{i=1}^{K} \\sum_{j=1}^{K} w_i w_j \\left[ \\sqrt{\\sigma_i^2 + \\sigma_j^2} \\left( 2\\phi\\left(\\frac{\\mu_i - \\mu_j}{\\sqrt{\\sigma_i^2 + \\sigma_j^2}}\\right) + \\frac{\\mu_i - \\mu_j}{\\sqrt{\\sigma_i^2 + \\sigma_j^2}}\\left(2\\Phi\\left(\\frac{\\mu_i - \\mu_j}{\\sqrt{\\sigma_i^2 + \\sigma_j^2}}\\right) - 1\\right) \\right) \\right]\n\\end{align*}\nNote that we have used the property $|-x|=|x|$ and the symmetries $\\phi(-z)=\\phi(z)$, $\\Phi(-z)=1-\\Phi(z)$ to express the first term in a slightly different but equivalent form for implementation convenience: $\\mathbb{E}[|y-Z_k|]$.\n\n**Algorithm**\nFor each test case:\n1. Initialize total logarithmic score and total CRPS to $0$.\n2. For each sample $(y, \\{w_k, \\mu_k, \\sigma_k\\}_{k=1}^K)$ in the test case:\n   a. Calculate the logarithmic score $s_{\\log}$ using the derived formula.\n   b. Calculate the CRPS $s_{CRPS}$ using the derived formula.\n   c. Add the calculated scores to their respective totals.\n3. Divide the total scores by the number of samples to obtain the mean logarithmic score and mean CRPS for the test case.\n4. Collect the six resulting mean values for the three test cases.\nThis procedure will be implemented to generate the final answer.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the MDN evaluation problem by calculating the average logarithmic score\n    and average CRPS for three test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        [\n            (21, 3, [0.5, 0.3, 0.2], [18, 22, 26], [2.0, 1.5, 3.5]),\n            (19, 3, [0.4, 0.4, 0.2], [17, 20, 28], [2.5, 1.2, 4.0]),\n            (24, 3, [0.2, 0.5, 0.3], [15, 23, 27], [2.0, 2.0, 3.0]),\n            (29, 3, [0.3, 0.3, 0.4], [16, 21, 30], [1.8, 1.8, 5.0]),\n            (20, 3, [0.6, 0.2, 0.2], [19, 25, 31], [1.0, 2.5, 6.0]),\n        ],\n        # Test Case 2\n        [\n            (20.1, 1, [1.0], [20.0], [0.05]),\n            (14.8, 1, [1.0], [15.0], [0.1]),\n            (33.0, 1, [1.0], [30.0], [0.2]),\n        ],\n        # Test Case 3\n        [\n            (22.5, 2, [0.9, 0.1], [22.0, 5.0], [2.0, 0.01]),\n            (35.0, 2, [0.01, 0.99], [35.0, 25.0], [1.0, 2.0]),\n            (20.0, 2, [0.5, 0.5], [10.0, 30.0], [1.5, 1.5]),\n            (16.0, 2, [0.7, 0.3], [28.0, 18.0], [3.0, 0.5]),\n        ]\n    ]\n\n    def calculate_log_score(T, w, mu, sigma):\n        \"\"\"Calculates the logarithmic score for a single sample.\"\"\"\n        pdf_val = 0.0\n        for i in range(len(w)):\n            pdf_val += w[i] * norm.pdf(T, loc=mu[i], scale=sigma[i])\n        return np.log(pdf_val)\n\n    def expected_abs_normal(m, s):\n        \"\"\"\n        Calculates E[|U|] where U is a normal random variable N(m, s^2).\n        Returns s * [2*phi(m/s) + (m/s)*(2*Phi(m/s) - 1)]\n        \"\"\"\n        if s == 0:\n            return np.abs(m)\n        std_m = m / s\n        return s * (2 * norm.pdf(std_m) + std_m * (2 * norm.cdf(std_m) - 1))\n\n    def calculate_crps(T, w, mu, sigma):\n        \"\"\"Calculates the CRPS for a single GMM sample.\"\"\"\n        # Term 1: E[|X - y|]\n        e_x_y = 0.0\n        for i in range(len(w)):\n            e_x_y += w[i] * expected_abs_normal(mu[i] - T, sigma[i])\n\n        # Term 2: 0.5 * E[|X - X'|]\n        e_x_xp = 0.0\n        for i in range(len(w)):\n            for j in range(len(w)):\n                m_ij = mu[i] - mu[j]\n                s_ij = np.sqrt(sigma[i]**2 + sigma[j]**2)\n                e_x_xp += w[i] * w[j] * expected_abs_normal(m_ij, s_ij)\n        \n        return e_x_y - 0.5 * e_x_xp\n\n    results = []\n    for case in test_cases:\n        total_log_score = 0.0\n        total_crps = 0.0\n        num_samples = len(case)\n\n        for sample in case:\n            T, K, w, mu, sigma = sample\n            w = np.array(w)\n            mu = np.array(mu)\n            sigma = np.array(sigma)\n\n            total_log_score += calculate_log_score(T, w, mu, sigma)\n            total_crps += calculate_crps(T, w, mu, sigma)\n        \n        mean_log_score = total_log_score / num_samples\n        mean_crps = total_crps / num_samples\n        \n        results.append(mean_log_score)\n        results.append(mean_crps)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3151363"}]}