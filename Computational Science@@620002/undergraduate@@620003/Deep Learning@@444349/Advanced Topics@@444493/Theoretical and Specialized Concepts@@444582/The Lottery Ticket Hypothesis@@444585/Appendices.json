{"hands_on_practices": [{"introduction": "This first practice grounds our exploration in the core mechanism of the Lottery Ticket Hypothesis: Iterative Magnitude Pruning (IMP). This exercise challenges you to compare the effectiveness of IMP against a simpler, one-time pruning approach to understand why the iterative cycle is so crucial. By implementing these methods in a controlled linear regression setting, you will test the hypothesis that IMP discovers subnetworks better aligned with the gradient flow early in training, a key insight into how winning tickets emerge [@problem_id:3188076].", "problem": "You are asked to formalize and implement a controlled comparison between iterative magnitude pruning (IMP) with rewinding and single-shot pruning at the same target sparsity to test whether repeated rewinding to a fixed training iteration provides a unique advantage attributable to optimization trajectory alignment. The experiment must be conducted in a fully specified, purely mathematical surrogate of supervised learning where all components are analytically defined.\n\nConsider a linear model with parameters $w \\in \\mathbb{R}^d$, a data matrix $X \\in \\mathbb{R}^{n \\times d}$, and a target vector $y \\in \\mathbb{R}^n$. The objective is the regularized least squares loss\n$$\nL(w) \\;=\\; \\frac{1}{2n}\\,\\lVert X w - y \\rVert_2^2 \\;+\\; \\frac{\\lambda}{2}\\,\\lVert w \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $n$ and $d$ are given. Training proceeds by full-batch gradient descent with a fixed step size $\\eta  0$:\n$$\nw_{t+1} \\;=\\; w_t \\;-\\; \\eta \\,\\nabla L(w_t), \\quad \\text{with} \\quad \\nabla L(w) \\;=\\; \\frac{1}{n}\\,X^\\top\\!\\left(X w - y\\right) \\;+\\; \\lambda w.\n$$\nAssume the step size is chosen to satisfy the standard stability condition for gradient descent on a smooth convex function, that is,\n$$\n0 \\;\\; \\eta \\;\\le\\; \\frac{1}{L_{\\max}}, \\quad \\text{where} \\quad L_{\\max} \\;\\text{is the largest eigenvalue of}\\; \\frac{1}{n}X^\\top X + \\lambda I_d.\n$$\n\nDefine a binary mask $m \\in \\{0,1\\}^d$ indicating which parameters are kept ($1$) or pruned ($0$). When training with a mask $m$, the update is projected onto the masked coordinates so that pruned coordinates remain zero. Concretely, starting from an initialization $w_{\\text{init}}$, one step of masked gradient descent is\n$$\nw^{+} \\;=\\; m \\odot \\bigl(w - \\eta \\,\\nabla L(w)\\bigr),\n$$\nwhere $\\odot$ denotes elementwise multiplication and $m$ is held fixed during any given masked training phase.\n\nMagnitude pruning at target sparsity $s \\in [0,1)$ keeps the $K$ coordinates of largest absolute magnitude and sets the others to zero, where\n$$\nK \\;=\\; \\left\\lceil (1 - s)\\,d \\right\\rceil.\n$$\nSingle-shot pruning computes one mask $m_{\\text{SS}}$ by training the dense model from $w_0$ for $T$ steps to obtain $w_T$, then keeping the $K$ largest coordinates of $|w_T|$. The single-shot solution is then trained for $T$ masked steps from the rewind point $w_k$ (the dense parameters after $k$ steps of training from $w_0$), that is, initialized at $m_{\\text{SS}} \\odot w_k$. Iterative magnitude pruning (IMP) with rewinding performs $R$ rounds indexed by $r \\in \\{1,\\dots,R\\}$: in each round, starting from $m^{(r-1)} \\odot w_k$ (with $m^{(0)} = \\mathbf{1}$), train for $T$ masked steps to $w_T^{(r)}$, prune additional coordinates of smallest magnitude in $|w_T^{(r)}|$ among those still unpruned to update $m^{(r)}$, and repeat until the final mask $m_{\\text{IMP}} = m^{(R)}$ has exactly $K$ kept coordinates. After the final round, train for $T$ masked steps from $m_{\\text{IMP}} \\odot w_k$ to produce the IMP solution.\n\nDefine the trajectory-alignment score at rewind time $k$ for a mask $m$ as the fraction of gradient energy at $w_k$ captured by the kept coordinates:\n$$\n\\mathcal{A}(m; k) \\;=\\; \\frac{\\left\\| m \\odot \\nabla L(w_k) \\right\\|_2^2}{\\left\\| \\nabla L(w_k) \\right\\|_2^2},\n$$\nwith the convention that if $\\left\\| \\nabla L(w_k) \\right\\|_2 = 0$ then $\\mathcal{A}(m;k) = 1$.\n\nYour task is to implement both procedures and report, for each test case, a list of four real numbers\n$$\n\\bigl[L_{\\text{IMP}},\\,L_{\\text{SS}},\\,\\mathcal{A}(m_{\\text{IMP}};k),\\,\\mathcal{A}(m_{\\text{SS}};k)\\bigr],\n$$\nwhere $L_{\\text{IMP}}$ and $L_{\\text{SS}}$ are the final losses of the IMP and single-shot solutions respectively, computed after the final $T$ steps from the rewind point $w_k$ under their respective final masks, and $\\mathcal{A}$ are the alignment scores defined above.\n\nImplementation requirements:\n- Use the exact gradient descent update defined above.\n- Use the stability step size $\\eta = 1 / L_{\\max}$ with $L_{\\max}$ computed from the provided $X$ and $\\lambda$ for each test case.\n- In IMP, set the per-round pruning so that the final number of kept coordinates is exactly $K$. You may prune a fixed fraction per round but must correct for rounding so that exactly $K$ remain after $R$ rounds. During each round, prune only among coordinates that are still unpruned, choosing those with smallest absolute magnitude at the end of that round’s $T$ masked steps.\n- In single-shot pruning, select $m_{\\text{SS}}$ from the magnitudes at the end of $T$ dense steps starting from $w_0$, then rewind to $w_k$, apply the mask, and train for $T$ masked steps.\n\nTest suite:\nFor each test case, you are given $X$, $y$, $w_0$, $\\lambda$, $s$, $k$, $T$, and $R$. All numbers are real and dimensionally consistent. The matrices and vectors are:\n\n- Test case $1$:\n  - $n = 6$, $d = 8$,\n  $$\n  X = \\begin{bmatrix}\n  0.7  -0.2  0.3  0.0  0.5  -0.1  0.2  0.4 \\\\\n  -0.1  0.8  -0.3  0.6  0.0  0.2  -0.5  0.1 \\\\\n  0.4  -0.6  0.9  -0.2  0.1  0.3  0.0  -0.4 \\\\\n  0.3  0.1  0.2  0.7  -0.3  0.0  0.6  -0.2 \\\\\n  -0.5  0.4  -0.1  0.3  0.2  -0.7  0.1  0.0 \\\\\n  0.2  -0.3  0.5  -0.4  0.6  0.1  -0.2  0.3\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 0.9 \\\\ -0.2 \\\\ 0.5 \\\\ 0.7 \\\\ -0.6 \\\\ 0.1 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\\\ 0.01 \\\\ -0.04 \\\\ 0.02 \\\\ 0.0 \\\\ -0.01 \\end{bmatrix},\n  \\quad \\lambda = 0.1, \\quad s = 0.5, \\quad k = 5, \\quad T = 200, \\quad R = 4.\n  $$\n\n- Test case $2$:\n  - $n = 8$, $d = 8$,\n  $$\n  X = I_8,\n  \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.8 \\\\ -1.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.7 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ -0.03 \\\\ 0.01 \\\\ 0.0 \\\\ 0.02 \\\\ -0.02 \\\\ 0.01 \\end{bmatrix},\n  \\quad \\lambda = 0.0, \\quad s = 0.1, \\quad k = 0, \\quad T = 100, \\quad R = 3.\n  $$\n\n- Test case $3$:\n  - $n = 10$, $d = 8$,\n  $$\n  X = \\begin{bmatrix}\n  1.0  0.95  0.1  -0.1  0.0  0.2  -0.2  0.1 \\\\\n  0.9  0.85  -0.1  0.0  0.1  -0.1  0.2  -0.2 \\\\\n  1.1  1.05  0.0  0.1  -0.1  0.0  0.1  0.2 \\\\\n  0.8  0.75  0.2  -0.2  0.2  -0.2  0.0  -0.1 \\\\\n  1.2  1.15  -0.2  0.2  -0.2  0.1  -0.1  0.0 \\\\\n  1.0  0.95  0.1  -0.2  0.2  0.0  0.2  -0.2 \\\\\n  0.95  0.9  0.0  0.2  -0.1  0.1  -0.2  0.2 \\\\\n  1.05  1.0  -0.1  0.0  0.0  -0.1  0.1  -0.1 \\\\\n  0.85  0.8  0.2  -0.1  0.1  0.0  0.0  0.1 \\\\\n  1.15  1.1  -0.2  0.1  -0.1  0.1  -0.1  0.0\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 1.2 \\\\ 0.6 \\\\ 1.4 \\\\ 1.0 \\\\ 0.9 \\\\ 1.1 \\\\ 0.7 \\\\ 1.3 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ 0.01 \\\\ -0.02 \\\\ 0.03 \\\\ -0.01 \\\\ 0.0 \\end{bmatrix},\n  \\quad \\lambda = 0.05, \\quad s = 0.8, \\quad k = 10, \\quad T = 300, \\quad R = 5.\n  $$\n\n- Test case $4$:\n  - $n = 5$, $d = 6$,\n  $$\n  X = \\begin{bmatrix}\n  0.6  -0.1  0.2  0.0  0.3  -0.2 \\\\\n  0.0  0.5  -0.3  0.4  -0.1  0.2 \\\\\n  0.2  -0.4  0.6  -0.2  0.0  0.1 \\\\\n  -0.3  0.2  0.1  0.5  -0.2  0.0 \\\\\n  0.1  0.0  -0.2  0.3  0.4  -0.1\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 0.4 \\\\ -0.1 \\\\ 0.5 \\\\ 0.3 \\\\ -0.2 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ 0.01 \\\\ -0.02 \\\\ 0.0 \\\\ 0.03 \\end{bmatrix},\n  \\quad \\lambda = 0.15, \\quad s = 0.0, \\quad k = 7, \\quad T = 150, \\quad R = 4.\n  $$\n\nYour program must compute, for each test case in the order $1,2,3,4$, the quadruple $\\bigl[L_{\\text{IMP}},\\,L_{\\text{SS}},\\,\\mathcal{A}(m_{\\text{IMP}};k),\\,\\mathcal{A}(m_{\\text{SS}};k)\\bigr]$ and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s result is itself a list of four floating-point numbers. For example, the final output should have the form\n$$\n\\bigl[ [a_1,b_1,c_1,d_1], [a_2,b_2,c_2,d_2], [a_3,b_3,c_3,d_3], [a_4,b_4,c_4,d_4] \\bigr],\n$$\nwith no additional text. You must output numerical values as plain real numbers; no units or percentages are involved in this task. You may round to a fixed number of decimal places if you wish, as long as you output real numbers.", "solution": "The user has provided a well-defined computational problem to formalize and compare two neural network pruning strategies, Iterative Magnitude Pruning (IMP) and Single-Shot (SS) pruning, within the controlled environment of regularized linear regression. The problem is scientifically grounded, mathematically precise, and algorithmically specified, making it valid for a full solution.\n\nThe core of the problem is to simulate two distinct pruning pipelines and evaluate them on both final performance (loss) and a proposed \"trajectory-alignment\" metric. This metric hypothesizes that the success of a pruning mask is related to how well it preserves the gradient direction at an early stage of training (the \"rewind\" point).\n\nThe solution will be structured as a series of functions to implement the required mathematical and algorithmic components, which are then orchestrated to process each test case.\n\n**1. Mathematical Preliminaries**\n\nThe loss function is the regularized least squares objective:\n$$\nL(w) = \\frac{1}{2n}\\,\\lVert X w - y \\rVert_2^2 + \\frac{\\lambda}{2}\\,\\lVert w \\rVert_2^2\n$$\nIts gradient, required for gradient descent, is:\n$$\n\\nabla L(w) = \\frac{1}{n}\\,X^\\top(X w - y) + \\lambda w\n$$\nThe training algorithm is gradient descent with a fixed step size $\\eta$. For stability and convergence in this convex setting, the step size must be less than or equal to the inverse of the Lipschitz constant of the gradient, $L_{\\text{max}}$. The gradient's Lipschitz constant is the maximum eigenvalue of the Hessian matrix, $\\nabla^2 L(w) = \\frac{1}{n}X^\\top X + \\lambda I_d$. We will use the upper bound $\\eta = 1/L_{\\max}$.\n\n**2. Algorithmic Implementation Steps**\n\nThe overall process for each test case involves the following stages:\n\n**Stage I: Dense Pre-training and Setup**\nFirst, we set up the common parameters for both pruning methods.\n- Calculate the number of parameters to keep, $K = \\lceil (1-s)d \\rceil$, where $s$ is the target sparsity and $d$ is the model dimension.\n- Calculate the Hessian matrix $H = \\frac{1}{n}X^\\top X + \\lambda I_d$.\n- Compute the maximum eigenvalue of $H$, $L_{\\max}$, to determine the stable step size $\\eta = 1/L_{\\max}$.\n- Perform a full dense training run for $T$ steps starting from the initial weights $w_0$. We must store the entire trajectory of weight vectors, $\\{w_0, w_1, \\dots, w_T\\}$, to access the rewind weights $w_k$ and the weights for single-shot mask generation, $w_T$.\n- Compute the gradient at the rewind point, $\\nabla L(w_k)$, which is essential for calculating the trajectory-alignment scores later.\n\n**Stage II: Single-Shot (SS) Pruning**\nThe single-shot procedure is as follows:\n1.  **Mask Generation**: The mask $m_{\\text{SS}}$ is determined from the dense weights after $T$ steps, $w_T$. We identify the $K$ positions corresponding to the largest absolute values in $w_T$ and set their corresponding mask entries to $1$, with all others being $0$.\n2.  **Rewind and Train**: The model is rewound to step $k$. The weights are initialized to $w_{\\text{SS, init}} = m_{\\text{SS}} \\odot w_k$. A final training phase of $T$ steps is performed using masked gradient descent: $w^{+} = m_{\\text{SS}} \\odot (w - \\eta \\nabla L(w))$.\n3.  **Evaluation**: The final loss $L_{\\text{SS}}$ is computed using the final weights $w_{\\text{SS, final}}$. The alignment score $\\mathcal{A}(m_{\\text{SS}}; k)$ is calculated as $\\frac{\\|m_{\\text{SS}} \\odot \\nabla L(w_k)\\|_2^2}{\\|\\nabla L(w_k)\\|_2^2}$.\n\n**Stage III: Iterative Magnitude Pruning (IMP)**\nIMP involves a multi-round pruning process:\n1.  **Pruning Schedule**: We define a schedule to prune weights over $R$ rounds to reach the final target count $K$. A linear schedule is robust and easy to implement: in each round $r \\in \\{1, \\dots, R\\}$, we calculate the target total number of pruned weights as $\\text{round}(r \\cdot (d-K)/R)$ (with a final correction for round $R$ to ensure exactly $d-K$ total are pruned). This determines how many additional weights to prune in the current round.\n2.  **Iterative Pruning and Training**: For each round $r=1, \\dots, R$:\n    a. The model is rewound to $w_k$ and masked with the current mask, $m^{(r-1)}$, to get the starting weights $w_{\\text{start}}^{(r)} = m^{(r-1)} \\odot w_k$.\n    b. The model is trained for $T$ steps using masked gradient descent with $m^{(r-1)}$ to obtain weights $w_T^{(r)}$.\n    c. A new mask $m^{(r)}$ is generated by pruning the scheduled number of weights from the currently active set (where $m^{(r-1)}=1$) that have the smallest absolute magnitude in $w_T^{(r)}$.\n3.  **Final Training**: After $R$ rounds, the final mask $m_{\\text{IMP}} = m^{(R)}$ is obtained. The model is again rewound to $w_k$, initialized as $w_{\\text{IMP, init}} = m_{\\text{IMP}} \\odot w_k$, and trained for a final $T$ steps.\n4.  **Evaluation**: The final loss $L_{\\text{IMP}}$ is computed. The alignment score $\\mathcal{A}(m_{\\text{IMP}}; k)$ is calculated similarly to the SS case, using the final mask $m_{\\text{IMP}}$.\n\n**Stage IV: Reporting**\nFor each test case, the four computed values $[L_{\\text{IMP}}, L_{\\text{SS}}, \\mathcal{A}(m_{\\text{IMP}};k), \\mathcal{A}(m_{\\text{SS}};k)]$ are collected. The final output aggregates these quadruples into a list of lists. A special case logic handles $\\|\\nabla L(w_k)\\|_2 = 0$, setting the alignment score to $1$.\n\nThis structured approach ensures that both pruning strategies are implemented exactly as specified and that the comparison is fair and controlled.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the setup, simulation, and result collection.\n    \"\"\"\n    \n    # Define helper functions for loss, gradient, and training\n    def get_loss(w, X, y, lambda_reg):\n        n = X.shape[0]\n        if n == 0:\n            return 0.5 * lambda_reg * np.dot(w, w)\n        residual = X @ w - y\n        loss = (0.5 / n) * np.dot(residual, residual) + 0.5 * lambda_reg * np.dot(w, w)\n        return loss\n\n    def get_grad(w, X, y, lambda_reg):\n        n = X.shape[0]\n        if n == 0:\n            return lambda_reg * w\n        grad = (1.0 / n) * X.T @ (X @ w - y) + lambda_reg * w\n        return grad\n\n    def train_masked_gd(w_init, mask, T, eta, X, y, lambda_reg):\n        w = np.copy(w_init)\n        for _ in range(T):\n            grad = get_grad(w, X, y, lambda_reg)\n            w = mask * (w - eta * grad)\n        return w\n\n    def get_alignment_score(mask, grad_at_wk):\n        norm_grad_sq = np.dot(grad_at_wk, grad_at_wk)\n        if norm_grad_sq  1e-12: # Handle case where gradient is zero\n            return 1.0\n        masked_grad_norm_sq = np.linalg.norm(mask * grad_at_wk)**2\n        return masked_grad_norm_sq / norm_grad_sq\n\n    # Define test cases as specified in the problem\n    test_cases = [\n        # Test case 1\n        {\n            \"X\": np.array([\n                [0.7, -0.2, 0.3, 0.0, 0.5, -0.1, 0.2, 0.4], [-0.1, 0.8, -0.3, 0.6, 0.0, 0.2, -0.5, 0.1],\n                [0.4, -0.6, 0.9, -0.2, 0.1, 0.3, 0.0, -0.4], [0.3, 0.1, 0.2, 0.7, -0.3, 0.0, 0.6, -0.2],\n                [-0.5, 0.4, -0.1, 0.3, 0.2, -0.7, 0.1, 0.0], [0.2, -0.3, 0.5, -0.4, 0.6, 0.1, -0.2, 0.3]\n            ]),\n            \"y\": np.array([0.9, -0.2, 0.5, 0.7, -0.6, 0.1]),\n            \"w0\": np.array([0.05, -0.02, 0.03, 0.01, -0.04, 0.02, 0.0, -0.01]),\n            \"lambda_reg\": 0.1, \"s\": 0.5, \"k\": 5, \"T\": 200, \"R\": 4\n        },\n        # Test case 2\n        {\n            \"X\": np.identity(8),\n            \"y\": np.array([1.0, 0.5, -0.5, 0.8, -1.2, 0.3, 0.0, 0.7]),\n            \"w0\": np.array([-0.01, 0.02, -0.03, 0.01, 0.0, 0.02, -0.02, 0.01]),\n            \"lambda_reg\": 0.0, \"s\": 0.1, \"k\": 0, \"T\": 100, \"R\": 3\n        },\n        # Test case 3\n        {\n            \"X\": np.array([\n                [1.0, 0.95, 0.1, -0.1, 0.0, 0.2, -0.2, 0.1], [0.9, 0.85, -0.1, 0.0, 0.1, -0.1, 0.2, -0.2],\n                [1.1, 1.05, 0.0, 0.1, -0.1, 0.0, 0.1, 0.2], [0.8, 0.75, 0.2, -0.2, 0.2, -0.2, 0.0, -0.1],\n                [1.2, 1.15, -0.2, 0.2, -0.2, 0.1, -0.1, 0.0], [1.0, 0.95, 0.1, -0.2, 0.2, 0.0, 0.2, -0.2],\n                [0.95, 0.9, 0.0, 0.2, -0.1, 0.1, -0.2, 0.2], [1.05, 1.0, -0.1, 0.0, 0.0, -0.1, 0.1, -0.1],\n                [0.85, 0.8, 0.2, -0.1, 0.1, 0.0, 0.0, 0.1], [1.15, 1.1, -0.2, 0.1, -0.1, 0.1, -0.1, 0.0]\n            ]),\n            \"y\": np.array([1.0, 0.8, 1.2, 0.6, 1.4, 1.0, 0.9, 1.1, 0.7, 1.3]),\n            \"w0\": np.array([0.02, -0.01, 0.0, 0.01, -0.02, 0.03, -0.01, 0.0]),\n            \"lambda_reg\": 0.05, \"s\": 0.8, \"k\": 10, \"T\": 300, \"R\": 5\n        },\n        # Test case 4\n        {\n            \"X\": np.array([\n                [0.6, -0.1, 0.2, 0.0, 0.3, -0.2], [0.0, 0.5, -0.3, 0.4, -0.1, 0.2],\n                [0.2, -0.4, 0.6, -0.2, 0.0, 0.1], [-0.3, 0.2, 0.1, 0.5, -0.2, 0.0],\n                [0.1, 0.0, -0.2, 0.3, 0.4, -0.1]\n            ]),\n            \"y\": np.array([0.4, -0.1, 0.5, 0.3, -0.2]),\n            \"w0\": np.array([-0.01, 0.02, 0.01, -0.02, 0.0, 0.03]),\n            \"lambda_reg\": 0.15, \"s\": 0.0, \"k\": 7, \"T\": 150, \"R\": 4\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        X, y, w0, lambda_reg, s, k, T, R = case.values()\n        n, d = X.shape\n        \n        # --- Stage I: Setup and Dense Pre-training ---\n        K = math.ceil((1 - s) * d)\n        \n        H = (1.0 / n) * (X.T @ X) + lambda_reg * np.eye(d)\n        L_max = np.max(np.linalg.eigvalsh(H))\n        eta = 1.0 / L_max\n        \n        w_dense_traj = [np.copy(w0)]\n        w = np.copy(w0)\n        for _ in range(T):\n            grad = get_grad(w, X, y, lambda_reg)\n            w -= eta * grad\n            w_dense_traj.append(np.copy(w))\n            \n        w_k = w_dense_traj[k]\n        w_T_dense = w_dense_traj[T]\n        grad_at_wk = get_grad(w_k, X, y, lambda_reg)\n\n        # --- Stage II: Single-Shot Pruning (SS) ---\n        m_ss = np.zeros(d, dtype=float)\n        if K > 0:\n            indices_to_keep = np.argpartition(np.abs(w_T_dense), -K)[-K:]\n            m_ss[indices_to_keep] = 1.0\n\n        w_ss_init = m_ss * w_k\n        w_ss_final = train_masked_gd(w_ss_init, m_ss, T, eta, X, y, lambda_reg)\n        L_ss = get_loss(w_ss_final, X, y, lambda_reg)\n        A_ss = get_alignment_score(m_ss, grad_at_wk)\n\n        # --- Stage III: Iterative Magnitude Pruning (IMP) ---\n        m_imp = np.ones(d, dtype=float)\n        pruned_count = 0\n        \n        for r in range(1, R + 1):\n            w_init_round = m_imp * w_k\n            w_trained_round = train_masked_gd(w_init_round, m_imp, T, eta, X, y, lambda_reg)\n            \n            if r  R:\n                target_pruned_total = round(r * (d - K) / R)\n            else: # Final round\n                target_pruned_total = d - K\n\n            num_to_prune_now = int(target_pruned_total - pruned_count)\n            \n            if num_to_prune_now > 0:\n                active_indices = np.where(m_imp == 1.0)[0]\n                active_weights_abs = np.abs(w_trained_round[active_indices])\n                \n                if num_to_prune_now >= len(active_indices):\n                    indices_to_prune_global = active_indices\n                else:\n                    prune_indices_local = np.argpartition(active_weights_abs, num_to_prune_now)[:num_to_prune_now]\n                    indices_to_prune_global = active_indices[prune_indices_local]\n\n                m_imp[indices_to_prune_global] = 0.0\n\n            pruned_count = d - np.sum(m_imp)\n\n        w_imp_init = m_imp * w_k\n        w_imp_final = train_masked_gd(w_imp_init, m_imp, T, eta, X, y, lambda_reg)\n        L_imp = get_loss(w_imp_final, X, y, lambda_reg)\n        A_imp = get_alignment_score(m_imp, grad_at_wk)\n        \n        all_results.append([L_imp, L_ss, A_imp, A_ss])\n\n    print(all_results)\n\nsolve()\n```", "id": "3188076"}, {"introduction": "Having established the iterative pruning process, we now consider what to prune: individual weights or entire structural blocks? This practice compares unstructured (weight-level) pruning with structured pruning, where entire channels or heads are removed from simplified CNN- and Transformer-like models. This hands-on comparison at equivalent sparsity levels will reveal the trade-offs between fine-grained and coarse-grained pruning in finding effective lottery tickets [@problem_id:3188072].", "problem": "You are given the task of programmatically testing the Lottery Ticket Hypothesis (LTH) by comparing structured pruning and unstructured pruning at equal sparsity on two simplified network families that capture the grouping logic of a Convolutional Neural Network (CNN) and a Transformer. The goal is to determine, for each scenario, whether there exists a sparse subnetwork (a \"lottery ticket\") obtained by magnitude pruning and rewinding to the original initialization that, when trained under the same optimizer from the same initialization, matches the dense model’s validation accuracy within a specified tolerance.\n\nStart from the following fundamental base in machine learning:\n\n- Empirical risk minimization: given labeled data $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, minimize the empirical risk $\\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\theta(\\mathbf{x}_i), y_i)$ by adjusting parameters $\\theta$.\n- Gradient descent: with learning rate $\\eta$, parameters are updated by $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathbb{E}[\\ell(f_\\theta(\\mathbf{x}), y)]$, approximated by the empirical mean over the dataset.\n- Cross-entropy for $K=2$ classes with softmax outputs: if logits are $\\mathbf{z} \\in \\mathbb{R}^2$ and $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$, then the loss for one-hot label $\\mathbf{y}$ is $\\ell(\\mathbf{z}, \\mathbf{y}) = -\\sum_{k=1}^2 y_k \\log p_k$.\n- ReLU activation: $\\mathrm{ReLU}(a) = \\max(0,a)$ applied elementwise.\n- Sparsity level $s \\in [0,1]$: the fraction of weights set to zero after pruning.\n- Magnitude pruning: remove weights (or whole groups of weights) by ranking their absolute values and zeroing the smallest until the target sparsity $s$ is reached.\n- Rewinding: after pruning based on magnitudes measured from a trained dense model, reset the surviving weights to their original initialization and retrain.\n\nYou will implement, from first principles, two toy architectures and training procedures:\n\n- CNN-like grouped linear network: input dimension $D=16$, number of channels $C=8$, number of classes $K=2$.\n  - Parameters: a masked linear layer with weights $W \\in \\mathbb{R}^{C \\times D}$ constrained by a fixed connectivity mask that enforces disjoint receptive fields of size $D/C=2$ per channel, followed by ReLU, then a linear classifier $V \\in \\mathbb{R}^{C \\times K}$.\n  - Forward pass for a batch $X \\in \\mathbb{R}^{N \\times D}$: $H = \\mathrm{ReLU}(X W^\\top)$, logits $Z = H V$, softmax to probabilities, and cross-entropy loss.\n  - Structured pruning at filter/channel level: remove entire channels, i.e., zero the corresponding row in $W$ (within its receptive field segment) and the corresponding row in $V$. At equal sparsity $s$, choose the number of channels to prune so that the exact fraction of total weights pruned equals $s$.\n  - Unstructured pruning: remove individual weights across $W$ (only within receptive field segments) and $V$ to reach exact sparsity $s$.\n\n- Transformer-like grouped linear network: input dimension $D=16$, number of heads $H=4$, per-head projection dimension $d_h=4$, number of classes $K=2$.\n  - Parameters: per-head projections $P_h \\in \\mathbb{R}^{d_h \\times D}$ for $h \\in \\{1,\\dots,H\\}$, head outputs concatenated into a vector in $\\mathbb{R}^{H \\cdot d_h}$, then a linear classifier $U \\in \\mathbb{R}^{H \\cdot d_h \\times K}$. ReLU is applied per head after the linear projection.\n  - Forward pass for a batch $X \\in \\mathbb{R}^{N \\times D}$: compute $Z_h = \\mathrm{ReLU}(X P_h^\\top)$ for each head, concatenate $Z = [Z_1,\\dots,Z_H]$, then logits $L = Z U$, softmax and cross-entropy loss.\n  - Structured pruning at head level: remove entire heads, i.e., zero all of $P_h$ and the corresponding segment of rows in $U$. At equal sparsity $s$, choose the number of heads to prune so that the exact fraction of total weights pruned equals $s$.\n  - Unstructured pruning: remove individual weights across all $P_h$ and $U$ to reach exact sparsity $s$.\n\nThe training and evaluation protocol must implement the following steps:\n\n1. Data generation: For each test case, create a balanced two-class dataset in $\\mathbb{R}^{D}$ by sampling $N_{\\text{train}}$ training points and $N_{\\text{val}}$ validation points. Let $u \\in \\mathbb{R}^{D}$ be a random unit vector, and define class means $\\mu_+ = +a u$ and $\\mu_- = -a u$, with $a  0$. Sample $\\mathbf{x}$ for class $+1$ from $\\mathcal{N}(\\mu_+, \\sigma^2 I)$ and for class $-1$ from $\\mathcal{N}(\\mu_-, \\sigma^2 I)$. Use one-hot labels and compute accuracy as the fraction of correct predictions on the validation set. Use no physical units; all quantities are dimensionless.\n2. Initialization: Initialize all weights with independent Gaussian entries scaled to keep gradients stable. For the CNN-like $W$, enforce the connectivity mask by zeroing entries outside the receptive fields at initialization.\n3. Dense training: Train each dense model (CNN-like and Transformer-like) for $T$ epochs using full-batch gradient descent with learning rate $\\eta$, cross-entropy loss, and ReLU activations as specified. Record the dense validation accuracy $A_{\\text{dense}}$ for each architecture.\n4. Pruning:\n   - Unstructured pruning: On the trained dense weights, prune the smallest-magnitude weights to achieve exact sparsity $s$ across all trainable weights. Produce a binary mask for surviving weights.\n   - Structured pruning: On the trained dense weights, compute a group importance score for each channel (CNN-like) or head (Transformer-like) as the sum of absolute values of weights in that group. Prune the groups with the smallest scores to achieve exact sparsity $s$. Produce a binary mask for surviving groups.\n5. Rewinding and retraining: For each pruning method and architecture, rewind surviving weights to their original initialization values, set pruned weights to zero, and retrain for $T$ epochs with the same optimizer and learning rate while enforcing the masks so pruned weights remain fixed at zero. Compute the validation accuracy $A_{\\text{ticket}}$ after retraining.\n6. Lottery ticket check: For each case, output a boolean indicating whether $A_{\\text{ticket}} \\geq A_{\\text{dense}} - \\varepsilon$, where $\\varepsilon$ is the tolerance specified in the test case.\n\nTest suite and required outputs:\n\n- Use $D=16$, $C=8$, $H=4$, $d_h=4$, $K=2$, $N_{\\text{train}}=256$, $N_{\\text{val}}=256$, $a=2$, $\\sigma=0.5$, $T=200$, and learning rate $\\eta=0.05$.\n- The test suite must contain three cases with distinct sparsity levels $s$ and random seeds to test different regimes:\n  1. Case $1$: seed $=7$, $s=0.0$, tolerance $\\varepsilon=0.005$.\n  2. Case $2$: seed $=13$, $s=0.5$, tolerance $\\varepsilon=0.03$.\n  3. Case $3$: seed $=42$, $s=0.75$, tolerance $\\varepsilon=0.05$.\n- For each case, compute four booleans in the order:\n  - CNN-like structured pruning lottery ticket exists,\n  - CNN-like unstructured pruning lottery ticket exists,\n  - Transformer-like structured pruning lottery ticket exists,\n  - Transformer-like unstructured pruning lottery ticket exists.\n- Final output format: Your program should produce a single line of output containing a list of lists, one per test case, with the four booleans in the specified order per case, e.g., $[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$ where each $b_{ij}$ is either $\\mathrm{True}$ or $\\mathrm{False}$.\n\nYour program must be a complete, runnable program that performs all computations from scratch using only the Python standard library, and the libraries NumPy and SciPy specified in the execution environment. No user input is required. The program must print only the single-line output in the exact format specified.", "solution": "The problem asks for a programmatic verification of the Lottery Ticket Hypothesis (LTH) on two simplified, group-structured neural network architectures, comparing structured and unstructured pruning. The LTH posits that a dense, randomly-initialized neural network contains a sparse subnetwork (the \"winning ticket\") that, when trained in isolation from the same initial state, can match the performance of the original dense network. We will implement the entire experimental pipeline, from data generation to model training and evaluation, following a \"prune, rewind, retrain\" methodology.\n\nThe solution is conceptualized in several stages: defining the mathematical models, specifying the training procedure based on gradient descent, detailing the pruning algorithms, and finally executing the complete experiment for the given test cases.\n\n**1. Mathematical Formulation of Models and Learning**\n\nThe core of the task is to minimize an empirical risk function using gradient-based optimization. Given a dataset $\\{\\left(\\mathbf{x}_i, y_i\\right)\\}_{i=1}^N$, we seek to find parameters $\\theta$ that minimize the average loss:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f_{\\theta}(\\mathbf{x}_i), y_i)\n$$\nwhere $f_{\\theta}$ is the model, and $\\ell$ is the loss function. We will use full-batch gradient descent, where the parameter update at each epoch is given by:\n$$\n\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)\n$$\nwith $\\eta$ being the learning rate.\n\nThe loss function is the cross-entropy for $K=2$ classes. For a single data point $(\\mathbf{x}, \\mathbf{y})$ with one-hot label $\\mathbf{y} \\in \\{[1, 0]^\\top, [0, 1]^\\top\\}$ and model logits $\\mathbf{z} = f_{\\theta}(\\mathbf{x}) \\in \\mathbb{R}^2$, the probabilities are computed via the softmax function, $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$, where $p_k = e^{z_k} / \\sum_{j=1}^2 e^{z_j}$. The loss is:\n$$\n\\ell(\\mathbf{z}, \\mathbf{y}) = -\\sum_{k=1}^2 y_k \\log p_k\n$$\nNumerically, this is computed as $\\ell(\\mathbf{z}, \\mathbf{y}) = \\log\\left(\\sum_{k=1}^2 e^{z_k}\\right) - \\mathbf{y} \\cdot \\mathbf{z}$ to maintain stability. The gradient of the loss with respect to the logits for a batch of $N$ samples is $\\nabla_{\\mathbf{Z}} \\mathcal{L} = \\frac{1}{N}(\\mathbf{P} - \\mathbf{Y})$, where $\\mathbf{P}$ and $\\mathbf{Y}$ are the matrices of predicted probabilities and one-hot labels, respectively.\n\nWe consider two architectures with $\\mathrm{ReLU}(a) = \\max(0, a)$ activation.\n\n**1.1. CNN-like Grouped Linear Network**\nThis model simplifies the channel-wise, local-receptive-field structure of a convolutional layer.\n- Parameters: $\\theta_{CNN} = \\{W, V\\}$, where $W \\in \\mathbb{R}^{C \\times D}$ and $V \\in \\mathbb{R}^{C \\times K}$.\n- Dimensions: Input $D=16$, channels $C=8$, classes $K=2$.\n- Structural Constraint: The weight matrix $W$ is constrained by a fixed binary mask $M_W$ that enforces $C$ disjoint receptive fields of size $D/C=2$. Specifically, $M_W[c, j] = 1$ only if $2c \\le j  2(c+1)$, and $0$ otherwise. The trainable weights are effectively $W \\odot M_W$, where $\\odot$ is the element-wise product.\n- Forward Pass: For an input batch $X \\in \\mathbb{R}^{N \\times D}$, the hidden representation is $H = \\mathrm{ReLU}(X (W \\odot M_W)^\\top)$, and the logits are $Z = H V$.\n- Gradients: Using the chain rule, $\\nabla_V \\mathcal{L} = H^\\top (\\nabla_Z \\mathcal{L})$ and $\\nabla_W \\mathcal{L} = ((\\nabla_Z \\mathcal{L} V^\\top) \\odot \\mathbb{I}[X (W \\odot M_W)^\\top > 0])^\\top X \\odot M_W$, where $\\mathbb{I}[\\cdot]$ is the indicator function.\n\n**1.2. Transformer-like Grouped Linear Network**\nThis model captures the multi-head structure of attention mechanisms, where input is projected independently by several \"heads\".\n- Parameters: $\\theta_{Trans} = \\{P_1, \\dots, P_H, U\\}$, where each $P_h \\in \\mathbb{R}^{d_h \\times D}$ and $U \\in \\mathbb{R}^{(H \\cdot d_h) \\times K}$.\n- Dimensions: Input $D=16$, heads $H=4$, head dimension $d_h=4$, classes $K=2$.\n- Forward Pass: For $X \\in \\mathbb{R}^{N \\times D}$, each head computes a representation $Z_h = \\mathrm{ReLU}(X P_h^\\top)$. These are concatenated into $Z_{cat} = [Z_1 | Z_2 | \\dots | Z_H] \\in \\mathbb{R}^{N \\times (H \\cdot d_h)}$. The final logits are $L = Z_{cat} U$.\n- Gradients: $\\nabla_U \\mathcal{L} = Z_{cat}^\\top (\\nabla_L \\mathcal{L})$. The gradient w.r.t. the concatenated hidden state, $\\nabla_{Z_{cat}} \\mathcal{L} = \\nabla_L \\mathcal{L} U^\\top$, is partitioned to find the gradient for each head's parameters: $\\nabla_{P_h} \\mathcal{L} = ((\\nabla_{Z_h} \\mathcal{L}) \\odot \\mathbb{I}[X P_h^\\top > 0])^\\top X$.\n\n**2. Data Generation**\n\nA synthetic, linearly separable two-class dataset is generated.\n- A random unit vector $\\mathbf{u} \\in \\mathbb{R}^D$ is chosen.\n- The two class means are defined as $\\boldsymbol{\\mu}_+ = a \\mathbf{u}$ and $\\boldsymbol{\\mu}_- = -a \\mathbf{u}$.\n- Training and validation data are sampled from isotropic Gaussian distributions: $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_y, \\sigma^2 I)$, where $y \\in \\{+1, -1\\}$ is the class label. For the experiment, $N_{\\text{train}}=256$, $N_{\\text{val}}=256$, $a=2$, and $\\sigma=0.5$.\n\n**3. LTH Experimental Protocol**\n\nFor each architecture and test case (defined by a random seed, sparsity $s$, and tolerance $\\varepsilon$):\n1.  **Initialization**: All trainable weights are initialized by sampling from a Gaussian distribution $\\mathcal{N}(0, \\sigma_{init}^2)$, where $\\sigma_{init}^2$ is scaled by the fan-in of the layer to ensure stable gradients. For instance, for a layer with weight matrix $W_{\\text{layer}}$ and fan-in $d_{in}$, weights are drawn from $\\mathcal{N}(0, 1/d_{in})$. These initial parameters, $\\theta_0$, are stored.\n2.  **Dense Training**: The full (dense) model is trained for $T=200$ epochs with learning rate $\\eta=0.05$ using full-batch gradient descent. The resulting trained parameters are $\\theta_T$. The validation accuracy on the held-out set is computed and stored as $A_{\\text{dense}}$.\n3.  **Pruning**: Using the trained weights $\\theta_T$, a binary mask $M_{prune}$ is generated to achieve a target sparsity level $s$.\n    - **Unstructured Pruning**: All individual weights across all parameter tensors are collected. Their absolute magnitudes $|\\theta_{T,i}|$ are ranked. A threshold is found such that setting weights below this threshold to zero results in a fraction $s$ of all weights being zero. The mask $M_{prune}$ is $1$ for surviving weights and $0$ otherwise.\n    - **Structured Pruning**: Weights are grouped by channel (CNN-like) or head (Transformer-like). An importance score is computed for each group, defined as the sum of the absolute magnitudes of all weights within that group. The groups with the lowest scores are pruned entirely until the exact fraction of pruned parameters equals $s$. The mask $M_{prune}$ zeros out all weights belonging to the pruned groups.\n4.  **Rewinding and Retraining**: The surviving weights are reset to their initial values from $\\theta_0$. This creates the \"winning ticket\" initialization $\\theta'_{0} = \\theta_0 \\odot M_{prune}$. The sparse subnetwork is then retrained for $T=200$ epochs, with the pruning mask $M_{prune}$ applied after every gradient update to ensure pruned weights remain zero.\n5.  **Evaluation**: The validation accuracy of the retrained subnetwork, $A_{\\text{ticket}}$, is computed.\n6.  **Lottery Ticket Check**: A winning ticket is deemed to exist if its performance is close to the dense model's, i.e., $A_{\\text{ticket}} \\geq A_{\\text{dense}} - \\varepsilon$. This check is performed for each combination of architecture and pruning method.\n\nThe procedure is executed for three test cases with varying sparsity levels $s \\in \\{0.0, 0.5, 0.75\\}$, and the results are compiled into the specified output format. The case $s=0.0$ serves as a baseline, where no pruning occurs, and thus $A_{\\text{ticket}}$ is expected to be identical to $A_{\\text{dense}}$, guaranteeing the check passes for any $\\varepsilon > 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, sparsity_level, tolerance)\n        (7, 0.0, 0.005),\n        (13, 0.5, 0.03),\n        (42, 0.75, 0.05),\n    ]\n    \n    # Global parameters\n    D = 16  # Input dimension\n    K = 2   # Number of classes\n    N_TRAIN = 256\n    N_VAL = 256\n    A = 2.0\n    SIGMA = 0.5\n    T_EPOCHS = 200\n    ETA = 0.05\n    \n    # CNN-like parameters\n    C = 8\n    \n    # Transformer-like parameters\n    H = 4\n    D_H = 4\n\n    results = []\n    for seed, s, eps in test_cases:\n        case_results = []\n        \n        # Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # 1. Data Generation\n        u = np.random.randn(D)\n        u /= np.linalg.norm(u)\n        mu_plus = A * u\n        mu_minus = -A * u\n        \n        # Training data\n        x_train_plus = np.random.multivariate_normal(mu_plus, SIGMA**2 * np.eye(D), N_TRAIN // 2)\n        x_train_minus = np.random.multivariate_normal(mu_minus, SIGMA**2 * np.eye(D), N_TRAIN // 2)\n        x_train = np.vstack((x_train_plus, x_train_minus))\n        y_train_labels = np.array([1] * (N_TRAIN // 2) + [0] * (N_TRAIN // 2))\n        y_train = np.eye(K)[y_train_labels]\n\n        # Validation data\n        x_val_plus = np.random.multivariate_normal(mu_plus, SIGMA**2 * np.eye(D), N_VAL // 2)\n        x_val_minus = np.random.multivariate_normal(mu_minus, SIGMA**2 * np.eye(D), N_VAL // 2)\n        x_val = np.vstack((x_val_plus, x_val_minus))\n        y_val_labels = np.array([1] * (N_VAL // 2) + [0] * (N_VAL // 2))\n        y_val = np.eye(K)[y_val_labels]\n\n        # --- Helper functions ---\n        def accuracy(y_pred_logits, y_true_one_hot):\n            pred_labels = np.argmax(y_pred_logits, axis=1)\n            true_labels = np.argmax(y_true_one_hot, axis=1)\n            return np.mean(pred_labels == true_labels)\n\n        # --- CNN-like Architecture ---\n        def run_cnn_experiment():\n            # Init\n            w_init = np.random.randn(C, D) * np.sqrt(1.0 / D)\n            v_init = np.random.randn(C, K) * np.sqrt(1.0 / C)\n            \n            w_connectivity_mask = np.zeros_like(w_init)\n            receptive_field_size = D // C\n            for i in range(C):\n                w_connectivity_mask[i, i*receptive_field_size:(i+1)*receptive_field_size] = 1.0\n            \n            w_init *= w_connectivity_mask\n\n            def forward_cnn(x, w, v):\n                h = np.maximum(0, x @ w.T)\n                z = h @ v\n                return z, h\n\n            def gradients_cnn(x, y, w, v, w_mask):\n                n_samples = x.shape[0]\n                z, h = forward_cnn(x, w, v)\n                log_p = z - logsumexp(z, axis=1, keepdims=True)\n                \n                dz = (np.exp(log_p) - y) / n_samples\n                \n                dv = h.T @ dz\n                dh = dz @ v.T\n                dh[h = 0] = 0\n                dw = (dh.T @ x) * w_mask\n\n                return dw, dv\n\n            def train(init_params, masks, grad_fn, T, eta, x, y, w_mask=None):\n                w, v = [p.copy() for p in init_params]\n                w_prune_mask, v_prune_mask = masks\n                \n                for _ in range(T):\n                    dw, dv = grad_fn(x, y, w, v, w_mask if w_mask is not None else np.ones_like(w))\n                    w -= eta * dw\n                    v -= eta * dv\n                    w *= w_prune_mask\n                    v *= v_prune_mask\n                return w, v\n\n            # Dense model training\n            dense_masks = (np.ones_like(w_init), np.ones_like(v_init))\n            w_dense, v_dense = train((w_init, v_init), dense_masks, gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_dense, _ = forward_cnn(x_val, w_dense, v_dense)\n            acc_dense = accuracy(logits_dense, y_val)\n            \n            # Structured Pruning\n            if s > 0:\n                weights_per_channel = (D // C) + K\n                total_weights = C * weights_per_channel\n                num_ch_to_prune = int(round(s * total_weights / weights_per_channel))\n                \n                channel_scores = np.sum(np.abs(w_dense), axis=1) + np.sum(np.abs(v_dense), axis=1)\n                pruned_channels_indices = np.argsort(channel_scores)[:num_ch_to_prune]\n                \n                w_mask_struct = np.ones_like(w_init)\n                v_mask_struct = np.ones_like(v_init)\n                w_mask_struct[pruned_channels_indices, :] = 0\n                v_mask_struct[pruned_channels_indices, :] = 0\n            else:\n                w_mask_struct, v_mask_struct = dense_masks\n\n            w_ticket_s, v_ticket_s = train((w_init, v_init), (w_mask_struct * w_connectivity_mask, v_mask_struct), gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_ticket_s, _ = forward_cnn(x_val, w_ticket_s, v_ticket_s)\n            acc_ticket_s = accuracy(logits_ticket_s, y_val)\n            lth_check_struct = acc_ticket_s >= acc_dense - eps\n\n            # Unstructured Pruning\n            if s > 0:\n                all_weights = np.concatenate([w_dense[w_connectivity_mask == 1], v_dense.flatten()])\n                threshold = np.percentile(np.abs(all_weights), s * 100)\n                \n                w_mask_unstruct = w_connectivity_mask.copy()\n                w_mask_unstruct[w_connectivity_mask == 1] = (np.abs(w_dense[w_connectivity_mask == 1]) >= threshold).astype(float)\n                v_mask_unstruct = (np.abs(v_dense) >= threshold).astype(float)\n            else:\n                w_mask_unstruct, v_mask_unstruct = dense_masks\n\n            w_ticket_u, v_ticket_u = train((w_init, v_init), (w_mask_unstruct, v_mask_unstruct), gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_ticket_u, _ = forward_cnn(x_val, w_ticket_u, v_ticket_u)\n            acc_ticket_u = accuracy(logits_ticket_u, y_val)\n            lth_check_unstruct = acc_ticket_u >= acc_dense - eps\n            \n            return lth_check_struct, lth_check_unstruct\n\n        cnn_struct, cnn_unstruct = run_cnn_experiment()\n        case_results.extend([cnn_struct, cnn_unstruct])\n        \n        # --- Transformer-like Architecture ---\n        def run_transformer_experiment():\n            # Init\n            p_list_init = [np.random.randn(D_H, D) * np.sqrt(1.0 / D) for _ in range(H)]\n            u_init = np.random.randn(H * D_H, K) * np.sqrt(1.0 / (H * D_H))\n\n            def forward_transformer(x, p_list, u):\n                z_h_list = [np.maximum(0, x @ p.T) for p in p_list]\n                z_cat = np.concatenate(z_h_list, axis=1)\n                l = z_cat @ u\n                return l, z_h_list, z_cat\n\n            def gradients_transformer(x, y, p_list, u):\n                n_samples = x.shape[0]\n                l, z_h_list, z_cat = forward_transformer(x, p_list, u)\n                log_p = l - logsumexp(l, axis=1, keepdims=True)\n\n                dl = (np.exp(log_p) - y) / n_samples\n                \n                du = z_cat.T @ dl\n                dz_cat = dl @ u.T\n                \n                dp_list = []\n                for h in range(H):\n                    dz_h = dz_cat[:, h*D_H:(h+1)*D_H]\n                    dz_h[z_h_list[h] = 0] = 0\n                    dp_h = dz_h.T @ x\n                    dp_list.append(dp_h)\n                \n                return dp_list, du\n\n            def train_t(init_params, masks, T, eta, x, y):\n                p_list, u = [p.copy() for p in init_params[0]], init_params[1].copy()\n                p_masks, u_mask = masks\n                \n                for _ in range(T):\n                    dp_list, du = gradients_transformer(x, y, p_list, u)\n                    for h in range(H):\n                        p_list[h] -= eta * dp_list[h]\n                        p_list[h] *= p_masks[h]\n                    u -= eta * du\n                    u *= u_mask\n                return p_list, u\n            \n            # Dense model training\n            dense_p_masks = [np.ones_like(p) for p in p_list_init]\n            dense_u_mask = np.ones_like(u_init)\n            \n            p_list_dense, u_dense = train_t((p_list_init, u_init), (dense_p_masks, dense_u_mask), T_EPOCHS, ETA, x_train, y_train)\n            logits_dense_t, _, _ = forward_transformer(x_val, p_list_dense, u_dense)\n            acc_dense_t = accuracy(logits_dense_t, y_val)\n            \n            # Structured pruning\n            if s > 0:\n                weights_per_head = (D_H * D) + (D_H * K)\n                total_weights = H * weights_per_head\n                num_heads_to_prune = int(round(s * total_weights / weights_per_head))\n                \n                head_scores = [np.sum(np.abs(p_list_dense[h])) + np.sum(np.abs(u_dense[h*D_H:(h+1)*D_H, :])) for h in range(H)]\n                pruned_head_indices = np.argsort(head_scores)[:num_heads_to_prune]\n                \n                p_masks_struct = [np.ones_like(p) for p in p_list_init]\n                u_mask_struct = np.ones_like(u_init)\n                for h_idx in pruned_head_indices:\n                    p_masks_struct[h_idx] = 0.\n                    u_mask_struct[h_idx*D_H:(h_idx+1)*D_H, :] = 0.\n            else:\n                p_masks_struct, u_mask_struct = dense_p_masks, dense_u_mask\n            \n            p_list_ticket_s, u_ticket_s = train_t((p_list_init, u_init), (p_masks_struct, u_mask_struct), T_EPOCHS, ETA, x_train, y_train)\n            logits_ticket_s, _, _ = forward_transformer(x_val, p_list_ticket_s, u_ticket_s)\n            acc_ticket_s_t = accuracy(logits_ticket_s, y_val)\n            lth_check_struct_t = acc_ticket_s_t >= acc_dense_t - eps\n\n            # Unstructured pruning\n            if s > 0:\n                all_weights = np.concatenate([p.flatten() for p in p_list_dense] + [u_dense.flatten()])\n                threshold = np.percentile(np.abs(all_weights), s * 100)\n\n                p_masks_unstruct = [(np.abs(p) >= threshold).astype(float) for p in p_list_dense]\n                u_mask_unstruct = (np.abs(u_dense) >= threshold).astype(float)\n            else:\n                p_masks_unstruct, u_mask_unstruct = dense_p_masks, dense_u_mask\n\n            p_list_ticket_u, u_ticket_u = train_t((p_list_init, u_init), (p_masks_unstruct, u_mask_unstruct), T_EPOCHS, ETA, x_train, y_train)\n            logits_ticket_u, _, _ = forward_transformer(x_val, p_list_ticket_u, u_ticket_u)\n            acc_ticket_u_t = accuracy(logits_ticket_u, y_val)\n            lth_check_unstruct_t = acc_ticket_u_t >= acc_dense_t - eps\n            \n            return lth_check_struct_t, lth_check_unstruct_t\n\n        trans_struct, trans_unstruct = run_transformer_experiment()\n        case_results.extend([trans_struct, trans_unstruct])\n        \n        results.append(case_results)\n\n    # Convert boolean arrays to Python booleans for standard JSON-like output\n    final_results = [[bool(val) for val in row] for row in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"{final_results}\".replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3188072"}, {"introduction": "The standard LTH protocol rewinds all surviving weights, but is this strictly necessary for every part of the network? This final exercise delves into this nuanced question by testing a \"partial rewinding\" strategy. You will empirically determine which layers of a network most benefit from being reset to their early-training state, $\\theta_k$, versus being left at their fully trained state, $\\theta_T$, providing a deeper understanding of where the \"magic\" of a winning ticket initialization resides [@problem_id:3188074].", "problem": "You are asked to write a complete and runnable program that empirically tests partial rewinding in the context of the Lottery Ticket Hypothesis for a small, fully connected feedforward network. The investigation must be framed purely in mathematical and logical terms and must adhere to the definitions and procedures below. The goal is to determine which layers of a network require early-state initialization to sustain winning tickets.\n\nFundamental base and core definitions to be used:\n- A fully connected feedforward network with Rectified Linear Unit (ReLU) activation is a composition of affine transformations and elementwise nonlinearity. Let the input dimension be $d$, the hidden layer widths be $h_1$ and $h_2$, and the number of classes be $C$. Denote parameters by $\\theta = \\{W_1, b_1, W_2, b_2, W_3, b_3\\}$ where $W_1 \\in \\mathbb{R}^{d \\times h_1}$, $b_1 \\in \\mathbb{R}^{h_1}$, $W_2 \\in \\mathbb{R}^{h_1 \\times h_2}$, $b_2 \\in \\mathbb{R}^{h_2}$, $W_3 \\in \\mathbb{R}^{h_2 \\times C}$, $b_3 \\in \\mathbb{R}^{C}$. The forward map is $f(x;\\theta) = \\mathrm{softmax}(a_3)$ with \n$$\nz_1 = x W_1 + b_1,\\quad a_1 = \\max(0,z_1),\\quad\nz_2 = a_1 W_2 + b_2,\\quad a_2 = \\max(0,z_2),\\quad\na_3 = a_2 W_3 + b_3.\n$$\n- Training uses full-batch Gradient Descent (GD), a special case of Stochastic Gradient Descent (SGD), on the average cross-entropy loss. For logits matrix $A_3 \\in \\mathbb{R}^{N \\times C}$ with rows $a_3^{(i)}$ and one-hot labels $Y \\in \\mathbb{R}^{N \\times C}$, the softmax probabilities are $P_{ij} = \\exp(a_{3,ij} - \\max_k a_{3,ik}) / \\sum_{k=1}^{C} \\exp(a_{3,ik} - \\max_\\ell a_{3,i\\ell})$, and the loss is\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C} Y_{ij}\\log P_{ij} + \\frac{\\lambda}{2}\\sum_{\\ell=1}^{3}\\left(\\|W_\\ell\\|_F^2\\right).\n$$\nGradients are computed via the chain rule; for softmax with cross-entropy, the gradient at logits is $P - Y$ divided by $N$.\n- Magnitude pruning produces a binary mask $M$ over all parameters by zeroing out a fixed fraction $s$ of entries with the smallest absolute values and keeping the rest. The masked parameters are $\\theta \\odot M$, where $\\odot$ denotes elementwise multiplication and $M$ has the same shapes as $\\theta$’s tensors. During training with a mask, the projection $\\theta \\leftarrow \\theta \\odot M$ is applied after each gradient step to keep pruned entries at zero.\n- Rewinding to step $k$ sets selected layers’ parameters to their values at training step $k$, denoted $\\theta_k$, while other layers can be kept at their values at step $T$, denoted $\\theta_T$. The Lottery Ticket Hypothesis considers whether masked networks initialized at early states can match the performance of dense networks.\n\nYour program must:\n1. Construct a synthetic binary classification dataset defined as follows: sample $N_{\\text{train}}$ and $N_{\\text{val}}$ points independently and uniformly from $[-1,1]^2$. For a point $x = (x_1,x_2)$, assign label $y=1$ if $x_1 x_2  0$ and $y=0$ otherwise. Use one-hot encoding with $C=2$ classes. Use a fixed random seed to ensure reproducibility.\n2. Use a three-layer fully connected network with ReLU nonlinearity and softmax output as defined above, with $d=2$, $h_1=16$, $h_2=16$, and $C=2$. Initialize parameters with independent Gaussian entries with mean $0$ and standard deviation $\\sigma$ for all weights and biases.\n3. Train the dense network for $T$ full-batch gradient descent steps with a constant learning rate $\\eta$ and $\\ell_2$ regularization coefficient $\\lambda$. Save snapshots $\\theta_k$ at step $k$ and $\\theta_T$ at step $T$.\n4. Create a global magnitude pruning mask $M$ from $\\theta_T$ that zeros exactly a fraction $s$ of the smallest-magnitude entries across all parameters (including biases), keeping the remaining entries.\n5. For each specified subset of layers $S \\subseteq \\{1,2,3\\}$, form a partially rewound initialization $\\tilde{\\theta}^{(S)}$ as follows: for layers $\\ell \\in S$, set that layer’s parameters to the masked early values from $\\theta_k$; for layers $\\ell \\notin S$, set that layer’s parameters to the masked late values from $\\theta_T$. In all cases apply the mask $M$ to produce $\\tilde{\\theta}^{(S)} \\odot M$.\n6. Retrain the masked network starting from $\\tilde{\\theta}^{(S)} \\odot M$ for $R$ full-batch gradient descent steps with the same $\\eta$ and $\\lambda$, projecting onto the mask after each update. Evaluate the average cross-entropy on the validation set to obtain $L^{(S)}_{\\text{val}}$.\n7. Define the baseline masked winning ticket validation loss $L^{\\text{full}}_{\\text{val}}$ as the value of $L^{(S)}_{\\text{val}}$ for $S=\\{1,2,3\\}$ (that is, full rewinding of all layers to $\\theta_k$). A subset $S$ is said to sustain tickets if $L^{(S)}_{\\text{val}} \\le L^{\\text{full}}_{\\text{val}} + \\tau$.\n\nYou must implement all computations in double precision or single precision real numbers consistently. No physical units are involved. Angles are not used.\n\nUse the following fixed parameters:\n- Network sizes: $d=2$, $h_1=16$, $h_2=16$, $C=2$.\n- Dataset sizes: $N_{\\text{train}}=512$, $N_{\\text{val}}=512$.\n- Initialization: Gaussian with standard deviation $\\sigma=0.1$.\n- Training hyperparameters: learning rate $\\eta=0.1$, regularization $\\lambda=10^{-4}$.\n- Training steps: total $T=120$, rewind step $k=10$, masked retraining steps $R=110$.\n- Pruning fraction: $s=0.8$.\n- Random seed: $r_0=1337$.\n- Tolerance for sustaining tickets: $\\tau=0.02$.\n\nTest suite (layer-indexing convention: layer $1$ is $\\{W_1,b_1\\}$, layer $2$ is $\\{W_2,b_2\\}$, layer $3$ is $\\{W_3,b_3\\}$):\n- $S_1 = \\varnothing$ (rewind none; all layers kept at $\\theta_T$),\n- $S_2 = \\{1\\}$,\n- $S_3 = \\{2\\}$,\n- $S_4 = \\{3\\}$,\n- $S_5 = \\{1,2\\}$,\n- $S_6 = \\{2,3\\}$,\n- $S_7 = \\{1,3\\}$,\n- $S_8 = \\{1,2,3\\}$ (full rewind).\n\nFor each $S_i$, compute the boolean $b_i$ that is $\\mathrm{True}$ if $L^{(S_i)}_{\\text{val}} \\le L^{\\text{full}}_{\\text{val}} + \\tau$ and $\\mathrm{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[b_1,b_2,b_3,b_4,b_5,b_6,b_7,b_8]$. For example, an output could look like $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False},\\mathrm{True}]$.", "solution": "The user has provided a well-defined computational problem to investigate the Lottery Ticket Hypothesis, specifically focusing on the impact of partial layer rewinding on the performance of a pruned neural network. The problem is scientifically grounded in established deep learning literature, mathematically precise, and algorithmically specified in sufficient detail to permit a unique solution. All parameters, definitions, and procedures are self-contained and consistent. Therefore, the problem is deemed valid and a full solution can be constructed.\n\nThe solution will be implemented by following the sequence of steps prescribed in the problem statement:\n\n1.  **Dataset Generation**: A synthetic binary classification dataset will be created. We will sample $N_{\\text{train}}$ and $N_{\\text{val}}$ points from a uniform distribution over $[-1, 1]^2$. The classification rule is $y=1$ if $x_1 x_2  0$ and $y=0$ otherwise, which defines a non-linearly separable problem. The labels will be one-hot encoded. A fixed random seed ensures reproducibility.\n\n2.  **Network Architecture and Initialization**: A three-layer fully connected network will be used. The parameters $\\theta = \\{W_1, b_1, W_2, b_2, W_3, b_3\\}$ are initialized from a Gaussian distribution with mean $0$ and standard deviation $\\sigma$. All computations will consistently use double-precision floating-point numbers (`numpy.float64`).\n\n3.  **Initial Dense Training**: The network will be trained for $T=120$ steps using full-batch Gradient Descent. The loss function is the average cross-entropy with $\\ell_2$ regularization. During this phase, we will store a snapshot of the network's parameters, $\\theta_k$, after $k=10$ training steps. The final parameters after $T$ steps are denoted $\\theta_T$.\n\n4.  **Magnitude Pruning**: A global magnitude pruning mask, $M$, is created based on the final parameters $\\theta_T$. We calculate the magnitudes of all parameters (weights and biases) across all layers, identify the threshold corresponding to the $s \\times 100$-th percentile of these magnitudes, and create a binary mask $M$ that preserves parameters with magnitudes greater than or equal to this threshold. This procedure ensures that exactly a fraction $s$ of the parameters with the smallest magnitudes are pruned (set to zero).\n\n5.  **Partial Rewinding and Retraining**: For each specified subset of layers $S_i$, a new initial set of parameters for retraining is constructed.\n    *   First, an intermediate parameter set $\\tilde{\\theta}^{(S_i)}$ is formed by taking the parameters for layers in $S_i$ from the early snapshot $\\theta_k$ and parameters for layers not in $S_i$ from the late-training state $\\theta_T$.\n    *   The actual starting state for retraining is then given by the elementwise product $\\tilde{\\theta}^{(S_i)} \\odot M$.\n    *   Starting from this state, the network is retrained for $R=110$ steps. After each gradient update, the parameters are projected back onto the subnetwork defined by the mask $M$ by applying the mask again: $\\theta \\leftarrow \\theta \\odot M$.\n\n6.  **Evaluation**: After retraining, the final validation loss, $L^{(S_i)}_{\\text{val}}$, is computed for each experiment $S_i$. The baseline loss, $L^{\\text{full}}_{\\text{val}}$, is defined as the loss obtained from the full rewind experiment where $S = \\{1, 2, 3\\}$. A layer subset $S_i$ is considered to \"sustain the ticket\" if its final validation loss is no more than a tolerance $\\tau$ greater than the baseline loss, i.e., $L^{(S_i)}_{\\text{val}} \\le L^{\\text{full}}_{\\text{val}} + \\tau$.\n\nThe final output will be a list of boolean values indicating whether each layer subset $S_i$ sustains the ticket according to this criterion. The implementation will be encapsulated within a single Python script adhering to the specified environment and libraries.\n\nThe mathematical operations, including the forward pass, stable softmax, cross-entropy loss, and backpropagation for gradient computation, will be implemented using `numpy`.\n\n-   **Forward Pass**: $f(x;\\theta) = \\mathrm{softmax}(a_3)$, where $z_1 = x W_1 + b_1$, $a_1 = \\mathrm{ReLU}(z_1)$, $z_2 = a_1 W_2 + b_2$, $a_2 = \\mathrm{ReLU}(z_2)$, and $a_3 = a_2 W_3 + b_3$.\n-   **Loss Function**: $\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i,j} Y_{ij}\\log P_{ij} + \\frac{\\lambda}{2}\\sum_{\\ell=1}^{3}\\|W_\\ell\\|_F^2$.\n-   **Gradient of Loss wrt. Logits**: $\\nabla_{a_3}\\mathcal{L} = (P - Y) / N$.\n-   **Parameter Update**: $\\theta_{t+1} = \\theta_t - \\eta\\nabla\\mathcal{L}(\\theta_t)$.\n-   **Masked Update**: $\\theta_{t+1} = (\\theta_t - \\eta\\nabla\\mathcal{L}(\\theta_t)) \\odot M$.\n\nThis rigorous, step-by-step procedure will yield the desired empirical comparison of partial rewinding strategies.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically tests partial rewinding in the context of the Lottery Ticket Hypothesis.\n    \"\"\"\n    #\n    # ------------------ 0. PROBLEM PARAMETERS AND CONSTANTS ------------------\n    #\n    DTYPE = np.float64\n    # Network architecture\n    d, h1, h2, C = 2, 16, 16, 2\n    # Dataset sizes\n    N_train, N_val = 512, 512\n    # Initialization\n    sigma = 0.1\n    # Training hyperparameters\n    eta = 0.1\n    lambda_ = 1e-4\n    # Training steps\n    T = 120\n    k = 10\n    R = 110\n    # Pruning and evaluation\n    s = 0.8\n    tau = 0.02\n    # Reproducibility\n    r_0 = 1337\n\n    #\n    # ---------------------- 1. HELPER FUNCTIONS ----------------------\n    #\n    def generate_data(n_samples, rng):\n        X = rng.uniform(-1, 1, size=(n_samples, d)).astype(DTYPE)\n        # y=1 if x1*x2 > 0 (quadrants I and III), y=0 otherwise\n        y = (X[:, 0] * X[:, 1] > 0).astype(int)\n        # One-hot encode labels\n        Y = np.zeros((n_samples, C), dtype=DTYPE)\n        Y[np.arange(n_samples), y] = 1\n        return X, Y\n\n    def initialize_params(rng):\n        params = {\n            'W1': rng.normal(0, sigma, (d, h1)).astype(DTYPE),\n            'b1': rng.normal(0, sigma, (h1,)).astype(DTYPE),\n            'W2': rng.normal(0, sigma, (h1, h2)).astype(DTYPE),\n            'b2': rng.normal(0, sigma, (h2,)).astype(DTYPE),\n            'W3': rng.normal(0, sigma, (h2, C)).astype(DTYPE),\n            'b3': rng.normal(0, sigma, (C,)).astype(DTYPE),\n        }\n        return params\n\n    def forward_pass(X, params):\n        z1 = X @ params['W1'] + params['b1']\n        a1 = np.maximum(0, z1)\n        z2 = a1 @ params['W2'] + params['b2']\n        a2 = np.maximum(0, z2)\n        logits = a2 @ params['W3'] + params['b3']\n        activations = {'a1': a1, 'a2': a2}\n        return activations, logits\n\n    def compute_loss(Y, logits, params, lambda_val):\n        N = Y.shape[0]\n        # Numerically stable softmax and cross-entropy loss\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(logits - max_logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        true_class_probs = probs[np.arange(N), Y.argmax(axis=1)]\n        # Add a small epsilon to prevent log(0)\n        data_loss = np.mean(-np.log(true_class_probs + 1e-12))\n        \n        reg_loss = (np.sum(params['W1']**2) + np.sum(params['W2']**2) + np.sum(params['W3']**2))\n        total_loss = data_loss + (lambda_val / 2) * reg_loss\n        return total_loss\n\n    def compute_gradients(X, Y, activations, logits, params, lambda_val):\n        N = Y.shape[0]\n        # Numerically stable softmax probability calculation\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(logits - max_logits)\n        P = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        \n        # Gradient of loss with respect to pre-softmax logits\n        d_a3 = (P - Y) / N\n        \n        a2, a1 = activations['a2'], activations['a1']\n        \n        # Layer 3 gradients\n        d_W3 = a2.T @ d_a3 + lambda_val * params['W3']\n        d_b3 = d_a3.sum(axis=0)\n        \n        # Layer 2 gradients\n        d_a2 = d_a3 @ params['W3'].T\n        d_z2 = d_a2 * (a2 > 0)\n        d_W2 = a1.T @ d_z2 + lambda_val * params['W2']\n        d_b2 = d_z2.sum(axis=0)\n        \n        # Layer 1 gradients\n        d_a1 = d_z2 @ params['W2'].T\n        d_z1 = d_a1 * (a1 > 0)\n        d_W1 = X.T @ d_z1 + lambda_val * params['W1']\n        d_b1 = d_z1.sum(axis=0)\n\n        grads = {'W1': d_W1, 'b1': d_b1, 'W2': d_W2, 'b2': d_b2, 'W3': d_W3, 'b3': d_b3}\n        return grads\n\n    #\n    # ---------------------- 2. MAIN EXPERIMENT LOGIC ----------------------\n    #\n    # Setup\n    rng = np.random.default_rng(r_0)\n    X_train, Y_train = generate_data(N_train, rng)\n    X_val, Y_val = generate_data(N_val, rng)\n\n    # Initial Dense Training\n    params = initialize_params(rng)\n    params_k = None\n    for i in range(T):\n        if i == k:\n            params_k = {key: val.copy() for key, val in params.items()}\n        \n        activations, logits = forward_pass(X_train, params)\n        grads = compute_gradients(X_train, Y_train, activations, logits, params, lambda_)\n\n        for p_key in params:\n            params[p_key] -= eta * grads[p_key]\n    params_T = params\n\n    # Create Pruning Mask\n    param_info = []\n    param_keys_ordered = ['W1', 'b1', 'W2', 'b2', 'W3', 'b3']\n    for key in param_keys_ordered:\n        p = params_T[key]\n        param_info.append({'key': key, 'shape': p.shape, 'size': p.size})\n\n    flat_params = np.concatenate([params_T[info['key']].flatten() for info in param_info])\n    num_to_prune = int(s * flat_params.size)\n    prune_indices = np.argsort(np.abs(flat_params))[:num_to_prune]\n\n    flat_mask = np.ones(flat_params.size, dtype=DTYPE)\n    flat_mask[prune_indices] = 0\n    \n    masks = {}\n    current_pos = 0\n    for info in param_info:\n        masks[info['key']] = flat_mask[current_pos : current_pos + info['size']].reshape(info['shape'])\n        current_pos += info['size']\n\n    # Rewinding and Retraining Experiments\n    test_cases = [(), (1,), (2,), (3,), (1, 2), (2, 3), (1, 3), (1, 2, 3)]\n    layer_keys_map = {1: ['W1', 'b1'], 2: ['W2', 'b2'], 3: ['W3', 'b3']}\n    final_losses = []\n\n    for S in test_cases:\n        # Form the partially rewound initialization `tilde_theta`\n        tilde_theta = {}\n        for layer_idx in range(1, 4):\n            source_params = params_k if layer_idx in S else params_T\n            for p_key in layer_keys_map[layer_idx]:\n                tilde_theta[p_key] = source_params[p_key].copy()\n        \n        # Retraining starts from the masked rewound state\n        current_params = {key: tilde_theta[key] * masks[key] for key in tilde_theta}\n        \n        for _ in range(R):\n            activations, logits = forward_pass(X_train, current_params)\n            grads = compute_gradients(X_train, Y_train, activations, logits, current_params, lambda_)\n            \n            # Gradient update step\n            for p_key in current_params:\n                current_params[p_key] -= eta * grads[p_key]\n            \n            # Projection onto the mask\n            for p_key in current_params:\n                current_params[p_key] *= masks[p_key]\n        \n        # Evaluate final validation loss\n        _, val_logits = forward_pass(X_val, current_params)\n        val_loss = compute_loss(Y_val, val_logits, current_params, lambda_)\n        final_losses.append(val_loss)\n\n    # Final Analysis\n    baseline_loss = final_losses[-1]  # Loss for S = {1, 2, 3}\n    results = [loss = baseline_loss + tau for loss in final_losses]\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3188074"}]}