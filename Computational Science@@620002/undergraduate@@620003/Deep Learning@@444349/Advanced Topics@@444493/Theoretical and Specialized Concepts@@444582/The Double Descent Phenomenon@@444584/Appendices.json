{"hands_on_practices": [{"introduction": "The first step in understanding any scientific phenomenon is to observe it. This practice provides a direct, hands-on opportunity to generate the iconic double descent curve by training a simple neural network. By systematically varying the model's capacity—specifically, the number of parameters relative to the number of data points—you will witness how the test error first decreases, then spikes at the interpolation threshold, and surprisingly decreases again in the over-parameterized regime. This foundational experiment [@problem_id:3151120] is crucial for building intuition about the relationship between model complexity, interpolation, and generalization.", "problem": "You are tasked with empirically demonstrating the double descent phenomenon in a feedforward Multilayer Perceptron (MLP) by sweeping the parameter–sample ratio and linking the interpolation threshold to a spike in test error. The core setting is as follows.\n\nStart from the foundational base of empirical risk minimization under squared loss. Let the input space be $\\mathbb{R}^d$. Consider a two-layer feedforward Multilayer Perceptron (MLP) with a Rectified Linear Unit (ReLU) nonlinearity, where the hidden layer weights are fixed and only the output layer weights are trained. For an input vector $x \\in \\mathbb{R}^d$, the MLP computes\n$$\n\\phi(x) = \\big(\\sigma(w_1^\\top x + b_1), \\ldots, \\sigma(w_m^\\top x + b_m)\\big) \\in \\mathbb{R}^m,\n$$\nwhere $\\sigma(z) = \\max\\{0, z\\}$ is the ReLU activation, $w_j \\in \\mathbb{R}^d$ and $b_j \\in \\mathbb{R}$ are fixed hidden-layer parameters, and $m$ is the number of hidden units. The predicted output is\n$$\n\\hat{y}(x) = a^\\top \\phi(x),\n$$\nwhere $a \\in \\mathbb{R}^m$ are the trained output-layer weights. Given training data $\\{(x_i, y_i)\\}_{i=1}^n$, define the design matrix\n$$\n\\Phi \\in \\mathbb{R}^{n \\times m}, \\quad \\Phi_{ij} = \\sigma(w_j^\\top x_i + b_j).\n$$\nThe empirical risk minimization problem under squared loss seeks $a$ minimizing $\\sum_{i=1}^n (\\hat{y}(x_i) - y_i)^2$, which is a linear least squares problem. The minimum-norm solution is given by the Moore–Penrose pseudoinverse:\n$$\na^\\star = \\Phi^+ y,\n$$\nwhere $y = (y_1, \\ldots, y_n)^\\top$ and $\\Phi^+$ denotes the pseudoinverse of $\\Phi$. The interpolation threshold is reached when the training mean squared error becomes zero, which typically occurs once $\\Phi$ reaches full row rank as $m$ grows, so that the system $\\Phi a = y$ admits a solution. The parameter–sample ratio is defined as $p/n$, where $p$ is the number of trained parameters. In this setting, $p = m$.\n\nThe double descent phenomenon refers to the typical behavior where the test error as a function of model capacity initially decreases, then increases near the interpolation threshold (where training error hits zero), and then decreases again beyond that threshold as capacity further increases. Your program will construct synthetic data from a teacher network and measure test mean squared error across a sweep of $m$ values corresponding to different parameter–sample ratios.\n\nData generation and evaluation protocol:\n- Draw inputs $x \\in \\mathbb{R}^d$ independently from a standard normal distribution.\n- Generate labels using a fixed teacher network with a small number of ReLU units:\n$$\ny = \\sum_{k=1}^{m_{\\text{teacher}}} \\beta_k \\, \\sigma(u_k^\\top x + c_k) + \\varepsilon,\n$$\nwhere $u_k \\in \\mathbb{R}^d$, $c_k \\in \\mathbb{R}$, and $\\beta_k \\in \\mathbb{R}$ are fixed teacher parameters, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is independent Gaussian noise with standard deviation $\\sigma$. Use $m_{\\text{teacher}} = 5$.\n- Train the student MLP by computing $a^\\star = \\Phi^+ y$ for each chosen $m$.\n- Compute the training mean squared error\n$$\n\\mathrm{MSE}_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}(x_i) - y_i \\right)^2,\n$$\nand the test mean squared error on an independent test set of size $n_{\\text{test}}$,\n$$\n\\mathrm{MSE}_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{y}(x_i^{\\text{test}}) - y_i^{\\text{test}} \\right)^2.\n$$\nSet $n_{\\text{test}} = \\max\\{3n, 200\\}$.\n\nInterpolation threshold and spike detection:\n- Define the interpolation tolerance $\\epsilon = 10^{-10}$ and the spike margin $\\delta = 0.1$ (meaning $10\\%$).\n- For a given $n$ and a sweep of $m$ values, define the interpolation threshold $m_{\\text{interp}}$ as the smallest $m$ in the sweep such that $\\mathrm{MSE}_{\\text{train}}(m) \\le \\epsilon$.\n- Define a spike at the interpolation threshold if $\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}})$ exceeds the median of $\\mathrm{MSE}_{\\text{test}}(m)$ over all other $m$ values by at least a factor of $(1+\\delta)$. Formally, if\n$$\n\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}}) > (1 + \\delta) \\cdot \\operatorname{median}\\left(\\{\\mathrm{MSE}_{\\text{test}}(m) : m \\in \\mathcal{M}, m \\ne m_{\\text{interp}}\\}\\right),\n$$\nthen output the boolean value $\\mathrm{True}$; otherwise output $\\mathrm{False}$. If no $m$ in the sweep achieves interpolation, output $\\mathrm{False}$.\n\nSweep design:\n- For each test case, construct the list of hidden unit counts $\\mathcal{M}$ by scaling $n$ with ratios $\\{0.5, 0.8, 1.0, 1.2, 1.5\\}$ and rounding to the nearest integer, ensuring $m \\ge 1$. That is,\n$$\n\\mathcal{M} = \\left\\{ \\max\\left(1, \\left\\lfloor r \\cdot n \\right\\rceil \\right) : r \\in \\{0.5, 0.8, 1.0, 1.2, 1.5\\} \\right\\}.\n$$\nWithin a single test case, ensure all student hidden-layer parameters $\\{(w_j, b_j)\\}_{j=1}^{m_{\\max}}$ are fixed once at the largest $m_{\\max} = \\max \\mathcal{M}$, and the models for smaller $m$ reuse the first $m$ features to make the sweep comparable.\n\nTest suite:\nRun your program on the following four test cases. For each case, report a single boolean indicating whether a spike is detected at the interpolation threshold according to the rule above. Use independent random seeds per case to fix all randomness deterministically.\n\n- Case $1$: $n = 60$, $d = 20$, $\\sigma = 0.5$, $\\text{seed} = 0$.\n- Case $2$: $n = 60$, $d = 20$, $\\sigma = 0.0$, $\\text{seed} = 1$.\n- Case $3$: $n = 24$, $d = 8$, $\\sigma = 0.5$, $\\text{seed} = 2$.\n- Case $4$: $n = 80$, $d = 30$, $\\sigma = 0.8$, $\\text{seed} = 3$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4\\right]$, where each $\\mathrm{result}_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$ corresponding to the spike detection for Case $i$.", "solution": "The problem statement is valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of instructions for an empirical study of the double descent phenomenon in a random feature model. There are no contradictions, ambiguities, or factual inaccuracies. We may therefore proceed with a solution.\n\nThe objective is to empirically investigate the double descent phenomenon in a simplified multilayer perceptron (MLP). This phenomenon describes a U-shaped then downward-sloping curve for test error as a function of model capacity. The initial decrease corresponds to the classical bias-variance trade-off in the underparameterized regime. The test error then peaks near the interpolation threshold, where the model has just enough capacity to fit the training data perfectly. Beyond this point, in the overparameterized regime, the test error surprisingly decreases again.\n\nThe methodology is structured as a controlled numerical experiment based on a teacher-student framework.\n\n**1. Model Specification and Training**\n\nThe model is a two-layer feedforward network with a Rectified Linear Unit (ReLU) activation function, $\\sigma(z) = \\max\\{0, z\\}$. For an input $x \\in \\mathbb{R}^d$, the output is $\\hat{y}(x) = a^\\top \\phi(x)$, where $\\phi(x) = (\\sigma(w_1^\\top x + b_1), \\ldots, \\sigma(w_m^\\top x + b_m))$ is a vector of feature activations. A crucial aspect of this problem is that the hidden layer parameters, $\\{w_j, b_j\\}_{j=1}^m$, are fixed after random initialization. Only the output layer weights, $a \\in \\mathbb{R}^m$, are trained. This transforms the nonlinear neural network problem into a linear regression problem in a high-dimensional feature space, known as a random feature model. The number of hidden units, $m$, directly corresponds to the number of trainable parameters and serves as our measure of model capacity.\n\nGiven $n$ training samples $\\{(x_i, y_i)\\}_{i=1}^n$, we form the design matrix $\\Phi \\in \\mathbb{R}^{n \\times m}$, where each element is $\\Phi_{ij} = \\sigma(w_j^\\top x_i + b_j)$. The goal is to find the weight vector $a$ that minimizes the squared loss, $\\mathcal{L}(a) = \\sum_{i=1}^n (y_i - a^\\top \\phi(x_i))^2 = \\|y - \\Phi a\\|_2^2$.\n\nThe solution to this linear least-squares problem that also has the minimum Euclidean norm $\\|a\\|_2$ is given by $a^\\star = \\Phi^+ y$. Here, $\\Phi^+$ is the Moore-Penrose pseudoinverse of the design matrix $\\Phi$, and $y = (y_1, \\ldots, y_n)^\\top$ is the vector of training labels. The pseudoinverse provides a unique, stable solution for any shape of $\\Phi$, correctly handling both the underparameterized ($m  n$, typically full column rank) and overparameterized ($m > n$, typically full row rank) regimes.\n\n**2. Data Generation and Evaluation**\n\nWe employ a teacher-student setup to create a synthetic dataset with a known ground truth.\n- A fixed \"teacher\" network of the same architecture, with $m_{\\text{teacher}} = 5$ hidden units, generates the labels: $y = \\sum_{k=1}^{m_{\\text{teacher}}} \\beta_k \\, \\sigma(u_k^\\top x + c_k) + \\varepsilon$. The input vectors $x \\in \\mathbb{R}^d$ are drawn from a standard normal distribution. Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is added to the output, modeling inherent measurement error or unmodeled effects.\n- A \"student\" model (our trainable model) is then trained on a set of $n$ such data points.\n- The performance of the trained student model is evaluated using the Mean Squared Error (MSE) on both the training set and an independent, larger test set of size $n_{\\text{test}} = \\max\\{3n, 200\\}$.\n  - $\\mathrm{MSE}_{\\text{train}}(m) = \\frac{1}{n} \\|\\Phi a^\\star - y\\|^2$\n  - $\\mathrm{MSE}_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\| \\Phi_{\\text{test}} a^\\star - y_{\\text{test}}\\|^2$\n\n**3. Experimental Procedure and Spike Detection**\n\nThe core of the experiment is to track $\\mathrm{MSE}_{\\text{test}}(m)$ as we sweep the model capacity $m$. The problem defines a specific sweep protocol: the set of capacities $\\mathcal{M}$ is generated by scaling the number of samples $n$ by ratios $\\{0.5, 0.8, 1.0, 1.2, 1.5\\}$. This set of ratios is designed to probe model behavior in the underparameterized regime ($m/n  1$), at the interpolation threshold ($m/n \\approx 1$), and in the overparameterized regime ($m/n > 1$). To ensure results are comparable across the sweep, the random features for the largest model ($m_{\\max} = \\max \\mathcal{M}$) are generated once, and smaller models simply use a subset of these features.\n\nThe double descent hypothesis predicts a spike in $\\mathrm{MSE}_{\\text{test}}$ around the point where the model first perfectly fits the training data. This point is formalized as the interpolation threshold, $m_{\\text{interp}}$, defined as the smallest $m \\in \\mathcal{M}$ for which $\\mathrm{MSE}_{\\text{train}}(m) \\le \\epsilon$, with a small tolerance $\\epsilon = 10^{-10}$.\n\nA \"spike\" is said to be detected if the test error at this threshold is significantly higher than the typical test error at other capacities. The condition is given by the inequality:\n$$\n\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}}) > (1 + \\delta) \\cdot \\operatorname{median}\\left(\\{\\mathrm{MSE}_{\\text{test}}(m) : m \\in \\mathcal{M}, m \\ne m_{\\text{interp}}\\}\\right)\n$$\nwhere the spike margin is $\\delta = 0.1$. If this condition is met, the result for the test case is $\\mathrm{True}$; otherwise, it is $\\mathrm{False}$. If no $m$ in the sweep achieves interpolation, the result is also $\\mathrm{False}$.\n\nThe implementation will execute this entire procedure for each of the four specified test cases, using the given random seeds for reproducibility, and report the boolean outcome of the spike detection test.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_case(n, d, sigma, seed):\n    \"\"\"\n    Runs a single simulation case for the double descent experiment.\n    \n    Args:\n        n (int): Number of training samples.\n        d (int): Input dimension.\n        sigma (float): Standard deviation of label noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        bool: True if a spike is detected, False otherwise.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Generate Teacher Network\n    m_teacher = 5\n    u_teacher = np.random.randn(m_teacher, d)\n    c_teacher = np.random.randn(m_teacher)\n    beta_teacher = np.random.randn(m_teacher)\n    \n    def teacher_model(X):\n        activations = np.maximum(0, X @ u_teacher.T + c_teacher)\n        return activations @ beta_teacher\n\n    # 2. Generate Training and Test Data\n    X_train = np.random.randn(n, d)\n    noise_train = sigma * np.random.randn(n)\n    y_train = teacher_model(X_train) + noise_train\n\n    n_test = max(3 * n, 200)\n    X_test = np.random.randn(n_test, d)\n    noise_test = sigma * np.random.randn(n_test)\n    y_test = teacher_model(X_test) + noise_test\n\n    # 3. Define Model Sweep\n    ratios = [0.5, 0.8, 1.0, 1.2, 1.5]\n    m_values = sorted([max(1, int(np.round(r * n))) for r in ratios])\n    m_max = m_values[-1]\n\n    # 4. Generate Student Network's Fixed Features\n    W_student = np.random.randn(m_max, d)\n    b_student = np.random.randn(m_max)\n\n    train_mses = []\n    test_mses = []\n\n    # 5. Sweep through model capacities (m)\n    for m in m_values:\n        # Select the first m features\n        W_m = W_student[:m, :]\n        b_m = b_student[:m]\n        \n        # Construct design matrix for training\n        Phi_train = np.maximum(0, X_train @ W_m.T + b_m)\n        \n        # Train model using Moore-Penrose pseudoinverse\n        # a_star = pinv(Phi_train) @ y_train\n        a_star = np.linalg.pinv(Phi_train) @ y_train\n        \n        # Evaluate Training MSE\n        y_train_pred = Phi_train @ a_star\n        train_mse = np.mean((y_train_pred - y_train) ** 2)\n        train_mses.append(train_mse)\n        \n        # Evaluate Test MSE\n        Phi_test = np.maximum(0, X_test @ W_m.T + b_m)\n        y_test_pred = Phi_test @ a_star\n        test_mse = np.mean((y_test_pred - y_test) ** 2)\n        test_mses.append(test_mse)\n\n    # 6. Analyze Results for Spike Detection\n    epsilon = 1e-10\n    delta = 0.1\n    \n    np_train_mses = np.array(train_mses)\n    np_test_mses = np.array(test_mses)\n    \n    # Find interpolation threshold m_interp\n    interp_indices = np.where(np_train_mses = epsilon)[0]\n    \n    if len(interp_indices) == 0:\n        # No model achieved interpolation\n        return False\n        \n    idx_interp = interp_indices[0]\n    # m_interp = m_values[idx_interp] # not needed for calculation\n    mse_test_at_interp = np_test_mses[idx_interp]\n\n    # Get test MSEs for all other m values\n    other_indices = np.arange(len(m_values)) != idx_interp\n    other_test_mses = np_test_mses[other_indices]\n\n    if len(other_test_mses) == 0:\n        return False\n\n    median_other_mses = np.median(other_test_mses)\n    \n    # Check for spike condition\n    is_spike = mse_test_at_interp > (1 + delta) * median_other_mses\n    \n    return is_spike\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, d, sigma, seed)\n        (60, 20, 0.5, 0),\n        (60, 20, 0.0, 1),\n        (24, 8, 0.5, 2),\n        (80, 30, 0.8, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, d, sigma, seed = case\n        result = run_single_case(n, d, sigma, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3151120"}, {"introduction": "Having observed the double descent curve, we now delve deeper into the mechanism behind the characteristic peak in test error. This peak is primarily driven by a massive spike in variance when the model has just enough capacity to perfectly fit the training data, making it highly unstable. In this exercise [@problem_id:3183597], you will compare the effects of two different types of corruption—noise in the data labels versus noise in the input features—to understand their distinct impacts on the error peak. This practice sharpens your analytical skills by requiring you to first predict the outcome based on first principles and then validate your hypothesis through simulation, revealing subtle but important differences in how models handle noise.", "problem": "You are asked to examine, from first principles and by experiment, how training-time noise in inputs versus training-time noise in labels affects the height of the peak in the test risk curve that appears near the interpolation threshold in the double descent phenomenon for linear regression. You must predict which type of noise yields a higher peak and validate your prediction experimentally by simulating a teacher-student linear model with isotropic Gaussian features and computing the out-of-sample mean squared error as model complexity increases.\n\nFundamental base and setting:\n- Consider a fixed but unknown parameter vector $\\beta \\in \\mathbb{R}^m$ and an input vector $x \\in \\mathbb{R}^m$ with independent and identically distributed standard normal coordinates. The noiseless teacher function is $f^\\star(x) = x^\\top \\beta$.\n- You will train a linear predictor $\\hat{f}(x) = x^\\top \\hat{w}$ by minimizing training error subject to minimum Euclidean norm among interpolating solutions (that is, the minimum-norm interpolating linear regressor). You will vary the number of model parameters $m$ across a range that sweeps through the interpolation threshold near $m \\approx n$, where $n$ is the number of training samples.\n- Two training-time noise models must be compared under identical conditions, with noise standard deviation $\\sigma$:\n  1. Label noise: clean inputs and noisy labels, with training labels $y = f^\\star(x) + \\eta$, where $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $x$.\n  2. Input noise: noisy inputs and clean labels, with observed training inputs $x_{\\text{obs}} = x + \\xi$, where $\\xi \\sim \\mathcal{N}(0,\\sigma^2 I_m)$ independent of $x$, and training labels $y = f^\\star(x)$.\n- In both cases, evaluate the out-of-sample mean squared error against the noiseless teacher $f^\\star$ using clean test inputs. That is, for a learned $\\hat{w}$ and a fresh clean test input matrix $X_{\\text{test}}$, compute the test risk as the empirical average of $(x^\\top \\hat{w} - x^\\top \\beta)^2$ over test inputs $x$.\n\nDesign requirements:\n- Begin your derivation from the following foundations: the bias-variance decomposition of mean squared error, the linear model $f^\\star(x)=x^\\top \\beta$, and properties of minimum-norm interpolating solutions for linear regression. Do not invoke any shortcut formulas specific to double descent; instead, argue from these principles to explain why a peak occurs near the interpolation threshold and how the two noise mechanisms affect the peak height differently.\n- Your program must simulate both noise models, sweep $m$ over a specified grid, compute the average test mean squared error across multiple independent trials for each $m$, and report, for each test case, whether the peak test error under label noise is higher than the peak test error under input noise.\n\nTest suite:\nFor each test case below, use the corresponding parameters $(n, \\{m\\text{-grid}\\}, \\sigma, T, s)$, where $n$ is the number of training samples, $\\{m\\text{-grid}\\}$ is the set of model sizes to evaluate, $\\sigma$ is the noise standard deviation, $T$ is the number of independent trials to average over, and $s$ is the random seed to initialize the random number generator. For each $m$ in the grid, draw a fresh $\\beta \\in \\mathbb{R}^m$ with independent coordinates $\\beta_j \\sim \\mathcal{N}(0, 1/m)$ (so that $\\mathbb{E}[f^\\star(x)^2] \\approx 1$), generate a training design matrix with $n$ rows, and a test design matrix with $n_{\\text{test}}$ rows, where $n_{\\text{test}} = 1000$. In the label-noise condition, add label noise of standard deviation $\\sigma$ to the training labels only; in the input-noise condition, add input noise of standard deviation $\\sigma$ to the training inputs only. For evaluation, always use clean test inputs and the noiseless teacher $f^\\star$.\n\nYou must implement the minimum-norm interpolating linear regressor exactly as follows: for a training matrix $X \\in \\mathbb{R}^{n \\times m}$ and training labels $y \\in \\mathbb{R}^n$, if $m \\le n$ compute $\\hat{w}$ via the normal equations $\\hat{w} = (X^\\top X)^{-1} X^\\top y$ (with a small ridge regularization to ensure numerical stability), and if $m > n$ compute the minimum-norm interpolator via $\\hat{w} = X^\\top (X X^\\top)^{-1} y$ (again with a small ridge for stability). The ridge must be a tiny positive scalar (e.g., $10^{-9}$), only to ensure numerical stability, not to regularize meaningfully.\n\nProvide results for the following three test cases:\n- Test case A: $n = 100$, $\\{m\\text{-grid}\\} = \\{10, 20, 30, \\dotsc, 200\\}$, $\\sigma = 0.5$, $T = 6$, $s = 1729$.\n- Test case B: $n = 60$, $\\{m\\text{-grid}\\} = \\{6, 12, 18, \\dotsc, 120\\}$, $\\sigma = 1.0$, $T = 6$, $s = 2027$.\n- Test case C: $n = 120$, $\\{m\\text{-grid}\\} = \\{12, 24, 36, \\dotsc, 240\\}$, $\\sigma = 0.25$, $T = 6$, $s = 9811$.\n\nPrediction and validation:\n- Based on your derivation, predict which noise type should produce the higher peak near the interpolation threshold. Then, for each test case, compute the maximum test mean squared error across the $m$-grid under label noise and under input noise. Output an integer indicator for each test case, equal to $1$ if the label-noise peak is strictly higher than the input-noise peak, and $0$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,0,1]\"), corresponding to the three test cases in the order A, B, C. No other text should be printed.", "solution": "The problem requires an analysis, from first principles, of the comparative effect of label noise versus input noise on the peak of the test risk curve in the context of the double descent phenomenon for linear regression. A prediction must be made and then validated through a numerical experiment.\n\n### Theoretical Derivation\n\nWe begin by establishing the framework for our analysis. The out-of-sample mean squared error, or test risk, for a learned weight vector $\\hat{w} \\in \\mathbb{R}^m$ against a true parameter vector $\\beta \\in \\mathbb{R}^m$ is given by the expectation over clean test data $x \\sim \\mathcal{N}(0, I_m)$:\n$$\nR(\\hat{w}) = \\mathbb{E}_{x} \\left[ (x^\\top \\hat{w} - x^\\top \\beta)^2 \\right]\n$$\nGiven that the input features are isotropic, i.e., $\\mathbb{E}[x x^\\top] = I_m$, the risk simplifies to the expected squared Euclidean distance between the estimated and true weight vectors, where the expectation is over the distribution of the training data $\\mathcal{D}$:\n$$\nR(\\hat{w}) = \\mathbb{E}_{\\mathcal{D}} \\left[ (\\hat{w} - \\beta)^\\top \\mathbb{E}[x x^\\top] (\\hat{w} - \\beta) \\right] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\|\\hat{w} - \\beta\\|^2 \\right]\n$$\nThis risk can be decomposed into squared bias and variance. Let $\\bar{w} = \\mathbb{E}_{\\mathcal{D}}[\\hat{w}]$ be the average estimated weight vector. The decomposition is:\n$$\nR(\\hat{w}) = \\underbrace{\\|\\bar{w} - \\beta\\|^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{\\mathcal{D}}\\left[\\|\\hat{w} - \\bar{w}\\|^2\\right]}_{\\text{Variance}}\n$$\nThe \"double descent\" curve describes the non-monotonic behavior of this risk as the model complexity, parameterized by the number of features $m$, increases relative to the number of training samples $n$. A characteristic peak appears at the interpolation threshold, where $m \\approx n$. This peak is primarily a variance-driven phenomenon, arising from the ill-conditioning of the training data matrix. Our analysis will focus on how each noise model affects this variance term.\n\nThe specified estimator is the minimum-norm linear regressor. For a training data matrix $A \\in \\mathbb{R}^{n \\times m}$ and labels $y \\in \\mathbb{R}^n$, the estimated weights $\\hat{w}$ are:\n$$\n\\hat{w} =\n\\begin{cases}\n    (A^\\top A)^{-1} A^\\top y  \\text{if } m \\le n \\text{ (under-parameterized)} \\\\\n    A^\\top (A A^\\top)^{-1} y  \\text{if } m > n \\text{ (over-parameterized)}\n\\end{cases}\n$$\nNumerically, a small ridge regularization term $\\lambda I$ with $\\lambda \\to 0^+$ is added to the matrix being inverted to ensure stability.\n\n#### Case 1: Label Noise\n\nIn this scenario, the training data consists of clean inputs $X \\in \\mathbb{R}^{n \\times m}$ and noisy labels $y = X\\beta + \\eta$, where $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. The regressor is trained on $(A, y) = (X, X\\beta + \\eta)$.\n\nFor $m \\le n$:\n$$\n\\hat{w}_{\\text{label}} = (X^\\top X)^{-1} X^\\top (X\\beta + \\eta) = \\beta + (X^\\top X)^{-1} X^\\top \\eta\n$$\nThe estimator is unbiased with respect to the noise, i.e., $\\mathbb{E}_{\\eta}[\\hat{w}_{\\text{label}}] = \\beta$. The risk is entirely due to variance:\n$$\nR(\\hat{w}_{\\text{label}}) = \\mathbb{E}_{X, \\eta} \\left[ \\|(X^\\top X)^{-1} X^\\top \\eta\\|^2 \\right] = \\mathbb{E}_{X} \\left[ \\text{Tr}\\left( (X^\\top X)^{-1} X^\\top \\mathbb{E}_{\\eta}[\\eta\\eta^\\top] X (X^\\top X)^{-1} \\right) \\right]\n$$\nSince $\\mathbb{E}_{\\eta}[\\eta\\eta^\\top] = \\sigma^2 I_n$, this simplifies to:\n$$\nR(\\hat{w}_{\\text{label}}) = \\sigma^2 \\mathbb{E}_{X} \\left[ \\text{Tr}\\left( (X^\\top X)^{-1} \\right) \\right]\n$$\nThe matrix $X^\\top X$ is a Wishart matrix. As $m \\to n^-$, its smallest eigenvalue approaches zero, causing the trace of its inverse to explode. This leads to the characteristic peak in test risk. A similar analysis for $m > n$ yields a risk contribution proportional to $\\sigma^2 \\mathbb{E}_{X} \\left[ \\text{Tr}\\left( (X X^\\top)^{-1} \\right) \\right]$, which also diverges as $m \\to n^+$. The peak is thus directly proportional to the noise variance $\\sigma^2$ and the an amplification factor related to the ill-conditioning of the data matrix.\n\n#### Case 2: Input Noise\n\nHere, the training data consists of noisy inputs $X_{\\text{obs}} = X_{\\text{true}} + \\Xi$ and clean labels $y = X_{\\text{true}}\\beta$. The random matrix $\\Xi \\in \\mathbb{R}^{n \\times m}$ contains i.i.d. noise elements $\\Xi_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. The regressor is trained on $(A, y) = (X_{\\text{obs}}, X_{\\text{true}}\\beta)$.\n\nWe can reframe this as an equivalent label noise problem. The target labels can be expressed in terms of the observed inputs:\n$$\ny = X_{\\text{true}}\\beta = (X_{\\text{obs}} - \\Xi)\\beta = X_{\\text{obs}}\\beta - \\Xi\\beta\n$$\nThis shows that training a linear model on $(X_{\\text{obs}}, y)$ is equivalent to training on data $(X_{\\text{obs}}, X_{\\text{obs}}\\beta + \\eta')$, where the \"effective\" label noise is $\\eta' = -\\Xi\\beta$.\n\nLet us analyze this effective noise $\\eta'$. It is a vector in $\\mathbb{R}^n$. The $i$-th component is $\\eta'_i = -\\xi_i^\\top \\beta$, where $\\xi_i^\\top$ is the $i$-th row of $\\Xi$. The rows $\\xi_i$ are independent, so the noise terms $\\eta'_i$ are independent. The variance of each term is:\n$$\n\\text{Var}(\\eta'_i) = \\mathbb{E}_{\\Xi} [(\\xi_i^\\top \\beta)^2] = \\beta^\\top \\mathbb{E}_{\\Xi}[\\xi_i \\xi_i^\\top] \\beta = \\beta^\\top (\\sigma^2 I_m) \\beta = \\sigma^2 \\|\\beta\\|^2\n$$\nThe problem specifies that the true weights $\\beta$ are drawn such that $\\mathbb{E}[\\|\\beta\\|^2] = \\mathbb{E}[\\sum_{j=1}^m \\beta_j^2] = \\sum_{j=1}^m (1/m) = 1$. Thus, on average, the variance of the effective label noise is $\\sigma^2$. This is the same as the variance of the explicit label noise in Case 1.\n\nHowever, the crucial difference lies in the data matrix used by the regressor. The estimator for $m \\le n$ is:\n$$\n\\hat{w}_{\\text{input}} = (X_{\\text{obs}}^\\top X_{\\text{obs}})^{-1} X_{\\text{obs}}^\\top y\n$$\nThe matrix being inverted is $X_{\\text{obs}}^\\top X_{\\text{obs}}$, not $X_{\\text{true}}^\\top X_{\\text{true}}$. The entries of $X_{\\text{true}}$ are drawn from $\\mathcal{N}(0, 1)$, while the entries of $\\Xi$ are drawn from $\\mathcal{N}(0, \\sigma^2)$. Since they are independent, the entries of the observed matrix $X_{\\text{obs}} = X_{\\text{true}} + \\Xi$ are drawn from $\\mathcal{N}(0, 1+\\sigma^2)$.\n\nThis implies that we can write $X_{\\text{obs}} = \\sqrt{1+\\sigma^2} Z$, where $Z$ is a matrix with i.i.d. $\\mathcal{N}(0, 1)$ entries, just like $X_{\\text{true}}$. The matrix to be inverted becomes:\n$$\nX_{\\text{obs}}^\\top X_{\\text{obs}} = (1+\\sigma^2) Z^\\top Z\n$$\nThe eigenvalues of $X_{\\text{obs}}^\\top X_{\\text{obs}}$ are therefore a factor of $(1+\\sigma^2)$ larger than the eigenvalues of a standard Wishart matrix $Z^\\top Z$ (which has the same spectral distribution as $X_{\\text{true}}^\\top X_{\\text{true}}$).\n\nThe variance amplification factor for input noise, analogous to Case 1, will be proportional to $\\text{Tr}((X_{\\text{obs}}^\\top X_{\\text{obs}})^{-1})$.\n$$\n\\text{Tr}\\left( (X_{\\text{obs}}^\\top X_{\\text{obs}})^{-1} \\right) = \\text{Tr}\\left( ((1+\\sigma^2) Z^\\top Z)^{-1} \\right) = \\frac{1}{1+\\sigma^2} \\text{Tr}\\left( (Z^\\top Z)^{-1} \\right)\n$$\nComparing the variance contribution to the risk at the peak:\n-   **Label Noise Peak Height** $\\propto \\sigma^2 \\cdot \\mathbb{E}[\\text{Tr}((X^\\top X)^{-1})]$\n-   **Input Noise Peak Height** $\\propto (\\sigma^2 \\mathbb{E}[\\|\\beta\\|^2]) \\cdot \\mathbb{E}[\\text{Tr}((X_{\\text{obs}}^\\top X_{\\text{obs}})^{-1})] \\approx \\sigma^2 \\cdot \\frac{1}{1+\\sigma^2} \\mathbb{E}[\\text{Tr}((Z^\\top Z)^{-1})]$\n\nSince $\\sigma^2 > 0$, the factor $\\frac{1}{1+\\sigma^2}$ is strictly less than $1$. The input noise has a self-regularizing effect: it increases the variance of the input features, which makes the sample covariance matrix better conditioned and thus reduces the amplification of noise by the pseudo-inverse. While a more detailed analysis would also account for bias terms (input noise induces an errors-in-variables bias), the peak at the interpolation threshold is dominated by the variance explosion, which our analysis shows is significantly dampened in the input noise case.\n\n### Prediction\n\nBased on this derivation, the test risk peak near the interpolation threshold $m \\approx n$ will be **higher for label noise** than for input noise, given the same noise standard deviation $\\sigma$. The numerical experiment specified in the problem will serve to validate this prediction.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_w_hat(X, y, ridge_lambda):\n    \"\"\"\n    Computes the minimum-norm interpolating linear regressor.\n    - If m = n, computes via normal equations.\n    - If m  n, computes via dual form (minimum L2 norm solution).\n    A small ridge is added for numerical stability.\n    \"\"\"\n    n, m = X.shape\n    \n    if m = n:\n        # Under-parameterized or exactly determined case\n        # Solve (X.T @ X + lambda*I) w = X.T @ y\n        A = X.T @ X + ridge_lambda * np.identity(m)\n        b = X.T @ y\n        try:\n            w_hat = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if solve fails despite ridge\n            w_hat = np.linalg.pinv(X) @ y\n    else:\n        # Over-parameterized case (minimum norm solution)\n        # Solve (X @ X.T + lambda*I) z = y, then w = X.T @ z\n        A = X @ X.T + ridge_lambda * np.identity(n)\n        try:\n            z = np.linalg.solve(A, y)\n            w_hat = X.T @ z\n        except np.linalg.LinAlgError:\n            w_hat = np.linalg.pinv(X) @ y\n            \n    return w_hat\n\ndef run_experiment(n, m_grid, sigma, T, s):\n    \"\"\"\n    Runs one full experiment for a given test case configuration.\n    \"\"\"\n    n_test = 1000\n    ridge_lambda = 1e-9\n    rng = np.random.default_rng(s)\n\n    mse_label_noise = np.zeros(len(m_grid))\n    mse_input_noise = np.zeros(len(m_grid))\n\n    for i, m in enumerate(m_grid):\n        trial_mses_label = []\n        trial_mses_input = []\n\n        for _ in range(T):\n            # Generate new data for each trial\n            \n            # 1. Generate true parameters and clean data\n            beta = rng.normal(0, 1 / np.sqrt(m), size=(m, 1))\n            X_true_train = rng.normal(0, 1, size=(n, m))\n            y_true_train = X_true_train @ beta\n            X_test = rng.normal(0, 1, size=(n_test, m))\n            y_test = X_test @ beta\n\n            # 2. Label Noise Simulation\n            eta = rng.normal(0, sigma, size=(n, 1))\n            y_train_label = y_true_train + eta\n            X_train_label = X_true_train\n            \n            w_hat_label = compute_w_hat(X_train_label, y_train_label, ridge_lambda)\n            mse_label = np.mean((X_test @ w_hat_label - y_test)**2)\n            trial_mses_label.append(mse_label)\n\n            # 3. Input Noise Simulation\n            Xi = rng.normal(0, sigma, size=(n, m))\n            X_train_input = X_true_train + Xi\n            y_train_input = y_true_train\n            \n            w_hat_input = compute_w_hat(X_train_input, y_train_input, ridge_lambda)\n            mse_input = np.mean((X_test @ w_hat_input - y_test)**2)\n            trial_mses_input.append(mse_input)\n\n        # Average MSE over trials for the current m\n        mse_label_noise[i] = np.mean(trial_mses_label)\n        mse_input_noise[i] = np.mean(trial_mses_input)\n\n    # Find the peak MSE for each noise type\n    peak_label = np.max(mse_label_noise)\n    peak_input = np.max(mse_input_noise)\n\n    return 1 if peak_label > peak_input else 0\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, then prints the final result.\n    \"\"\"\n    test_cases = [\n        # (n, m-grid, sigma, T, s)\n        (100, list(range(10, 201, 10)), 0.5, 6, 1729),   # Test Case A\n        (60, list(range(6, 121, 6)), 1.0, 6, 2027),      # Test Case B\n        (120, list(range(12, 241, 12)), 0.25, 6, 9811),  # Test Case C\n    ]\n\n    results = []\n    for n, m_grid, sigma, T, s in test_cases:\n        result = run_experiment(n, m_grid, sigma, T, s)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3183597"}, {"introduction": "The double descent phenomenon is not just a function of model size; it also appears dynamically over the course of training a single, large model. This \"epoch-wise\" double descent is highly relevant to modern deep learning practice. This advanced exercise [@problem_id:3183606] simulates this dynamic in a setting analogous to training a transformer on a language task. By using a clever combination of \"generalizing\" and \"memorizing\" features with a phased training schedule, you will see the test error first fall, then rise as the model memorizes noisy data to achieve interpolation, and finally fall again as regularization helps it \"forget\" the noise and find a better-generalized solution. This provides a tangible microcosm of the complex training dynamics observed in state-of-the-art models.", "problem": "You are asked to construct a scientifically grounded simulation that examines epoch-wise double descent in a surrogate for a transformer trained on a small language modeling task. The objective is to connect training dynamics to interpolation and to quantify the behavior of the test cross-entropy over epochs. Your implementation must be a complete, runnable program that produces the specified final output from the given test suite, with no user input.\n\nBegin from the following foundational base of definitions and facts:\n- Empirical risk minimization for supervised learning considers a dataset $\\{(x_i, y_i)\\}_{i=1}^{n}$ and a model with parameters $\\theta$ that outputs class probabilities $p_\\theta(y \\mid x)$. The empirical risk under the multiclass cross-entropy is\n$$\n\\mathcal{L}_{\\text{emp}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} -\\log\\left( p_\\theta(y_i \\mid x_i) \\right).\n$$\n- The softmax function for logits $z \\in \\mathbb{R}^K$ is defined by\n$$\n\\text{softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } k \\in \\{1,\\dots,K\\}.\n$$\n- For a linear softmax classifier with weight matrix $W \\in \\mathbb{R}^{K \\times F}$ and feature vector $x \\in \\mathbb{R}^F$, the logits are $z = W x$, and the predicted class probabilities are $p(y \\mid x) = \\text{softmax}(W x)$.\n- The gradient of the cross-entropy loss with respect to $W$ for a single example $(x,y)$ with one-hot class vector $e_y \\in \\mathbb{R}^K$ is\n$$\n\\nabla_W \\ell(W; x,y) = \\left( p - e_y \\right) x^\\top, \\quad \\text{where } p = \\text{softmax}(W x).\n$$\n- Interpolation occurs when training classification accuracy becomes $1.0$ (or effectively reaches a near-perfect threshold), implying that the model fits all training labels exactly; in the context of the cross-entropy loss, this corresponds to logits that separate the training data so that the predicted class for each example matches $y_i$.\n\nYou will simulate a next-token language modeling task with a vocabulary of size $V$ using a first-order Markov (bigram) process to generate sequences. Let $P \\in \\mathbb{R}^{V \\times V}$ be a bigram matrix where $P_{ij} = \\Pr(\\text{token at time } t = i \\mid \\text{token at time } t-1 = j)$, with each column $j$ summing to $1$.\n\nDefine a surrogate \"transformer-like\" model with two feature groups:\n- Generalizable features $x^{\\text{gen}} \\in \\mathbb{R}^{V}$ given by the one-hot encoding of the previous token.\n- Memorization features $x^{\\text{mem}} \\in \\mathbb{R}^{M}$ that are one-hot features tied uniquely to each training example; these features are present in training examples but are zero on test examples. The full feature vector is $x = \\left[ x^{\\text{gen}}, x^{\\text{mem}} \\right] \\in \\mathbb{R}^{V+M}$.\n\nTrain a linear softmax classifier with weights $W \\in \\mathbb{R}^{V \\times (V+M)}$ using full-batch gradient descent for $T$ epochs on cross-entropy. Use a piecewise-constant per-epoch learning rate schedule that assigns different learning rates to the two parameter blocks $W^{\\text{gen}} \\in \\mathbb{R}^{V \\times V}$ and $W^{\\text{mem}} \\in \\mathbb{R}^{V \\times M}$:\n- Early phase (fraction of epochs $t/T \\in [0,1/3)$): larger learning rate for $W^{\\text{gen}}$, small learning rate for $W^{\\text{mem}}$.\n- Middle phase ($t/T \\in [1/3,2/3)$): small learning rate for $W^{\\text{gen}}$, large learning rate for $W^{\\text{mem}}$ to drive interpolation via memorization features.\n- Late phase ($t/T \\in [2/3,1]$): larger learning rate for $W^{\\text{gen}}$, and set the learning rate for $W^{\\text{mem}}$ to $0$; apply stronger $\\ell_2$ weight decay to $W^{\\text{mem}}$ in the late phase to reduce reliance on memorization features and reintroduce gradient signal to $W^{\\text{gen}}$. For clarity, weight decay corresponds to adding a penalty term $\\frac{\\lambda}{2} \\lVert W \\rVert_2^2$ to the loss, yielding a per-epoch shrinkage factor applied to the weights.\n\nData generation:\n- Sample a bigram matrix $P$ by drawing each column from a Dirichlet distribution to ensure stochastic columns. Generate $n_{\\text{train}}$ training pairs $(x_i, y_i)$ and $n_{\\text{test}}$ test pairs $(\\tilde{x}_j, \\tilde{y}_j)$ using the bigram process, where $x_i^{\\text{gen}}$ and $\\tilde{x}_j^{\\text{gen}}$ are one-hot previous tokens and $y_i$, $\\tilde{y}_j$ are next tokens. Inject a small fraction $\\rho$ of label noise into training labels by randomly replacing $y_i$ for a $\\rho$ proportion of training samples to simulate spurious correlations that memorization features can fit.\n\nTraining, tracking, and detection:\n- At each epoch $t$, compute the average training cross-entropy and training accuracy, and the test cross-entropy and test accuracy.\n- Define the interpolation epoch $t_\\star$ as the earliest epoch where training accuracy $\\geq 0.99$. If no such epoch exists, declare that interpolation has not occurred.\n- Define an early checkpoint $t_a = \\lfloor 0.2 T \\rfloor$ and the final epoch $T$.\n- Detect epoch-wise double descent as the condition that the test cross-entropy at the interpolation epoch $L_{\\text{test}}(t_\\star)$ is strictly larger than both $L_{\\text{test}}(t_a)$ and $L_{\\text{test}}(T)$, and that $L_{\\text{test}}(T)$ is strictly smaller than $L_{\\text{test}}(t_a)$. Formally, report True if\n$$\nL_{\\text{test}}(t_\\star) > \\max\\left( L_{\\text{test}}(t_a), L_{\\text{test}}(T) \\right) \\quad \\text{and} \\quad L_{\\text{test}}(T)  L_{\\text{test}}(t_a),\n$$\nand False otherwise; if interpolation does not occur, report False.\n\nYour program must implement the above and run the following test suite, each specified by $(V, n_{\\text{train}}, n_{\\text{test}}, M, T)$:\n1. $(8, 150, 500, 0, 180)$: Underparameterized baseline with no memorization features; interpolation should not occur.\n2. $(8, 150, 500, 150, 180)$: Balanced overparameterization where memorization features match training sample count.\n3. $(8, 100, 500, 300, 180)$: Heavily overparameterized memorization features; stronger interpolation tendency.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean indicating whether epoch-wise double descent is detected for the corresponding test case. No units are required for this task. The output must be exactly one line with no additional text.", "solution": "The user problem asks for a simulation of epoch-wise double descent in a simplified language model setting. The procedure involves generating synthetic data, training a linear model with a specific feature and training schedule, and then analyzing the test loss curve for a particular signature of double descent.\n\n### Step 1: Problem Validation\n\nFirst, I will validate the problem statement.\n\n#### Extracted Givens\n- **Loss Function**: Multiclass cross-entropy, $\\mathcal{L}_{\\text{emp}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} -\\log\\left( p_\\theta(y_i \\mid x_i) \\right)$.\n- **Model**: Linear softmax classifier, $p(y \\mid x) = \\text{softmax}(W x)$.\n- **Gradient**: $\\nabla_W \\ell(W; x,y) = \\left( p - e_y \\right) x^\\top$.\n- **Data Generation**: A first-order Markov process with a $V \\times V$ bigram matrix $P$. Columns of $P$ are drawn from a Dirichlet distribution. $n_{\\text{train}}$ training samples, $n_{\\text{test}}$ test samples. A fraction $\\rho$ of training labels are noisy.\n- **Feature Space**: $x = [x^{\\text{gen}}, x^{\\text{mem}}]$.\n    - $x^{\\text{gen}} \\in \\mathbb{R}^V$: one-hot encoding of the previous token.\n    - $x^{\\text{mem}} \\in \\mathbb{R}^M$: one-hot features unique to each training example, zero for test examples.\n- **Training**: Full-batch gradient descent for $T$ epochs. The weight matrix $W$ is split into $W^{\\text{gen}}$ and $W^{\\text{mem}}$ with a three-phase piecewise-constant learning rate schedule:\n    1.  **Early ($t/T \\in [0, 1/3)$)**: LR is high for $W^{\\text{gen}}$, low for $W^{\\text{mem}}$.\n    2.  **Middle ($t/T \\in [1/3, 2/3)$)**: LR is low for $W^{\\text{gen}}$, high for $W^{\\text{mem}}$.\n    3.  **Late ($t/T \\in [2/3, 1]$)**: LR is high for $W^{\\text{gen}}$, zero for $W^{\\text{mem}}$.\n- **Regularization**: $\\ell_2$ weight decay, with a stronger decay on $W^{\\text{mem}}$ in the late phase.\n- **Detection Criteria**:\n    - **Interpolation epoch $t_\\star$**: First epoch $t$ where training accuracy $\\ge 0.99$. If it does not occur, no double descent is detected.\n    - **Checkpoints**: $t_a = \\lfloor 0.2 T \\rfloor$ and final epoch $T$.\n    - **Double Descent Condition**: $L_{\\text{test}}(t_\\star) > \\max\\left( L_{\\text{test}}(t_a), L_{\\text{test}}(T) \\right)$ AND $L_{\\text{test}}(T)  L_{\\text{test}}(t_a)$.\n- **Test Suite**:\n    1. $(V, n_{\\text{train}}, n_{\\text{test}}, M, T) = (8, 150, 500, 0, 180)$\n    2. $(V, n_{\\text{train}}, n_{\\text{test}}, M, T) = (8, 150, 500, 150, 180)$\n    3. $(V, n_{\\text{train}}, n_{\\text{test}}, M, T) = (8, 100, 500, 300, 180)$\n\n#### Validation Verdict\nThe problem is scientifically grounded, well-posed, and objective. It provides a formal, albeit simplified, framework to investigate the double descent phenomenon, a topic of significant interest in deep learning research. The use of distinct feature types (\"generalizable\" vs. \"memorization\") and a phased training schedule is a sound methodological choice to induce and isolate the dynamics of interest.\n\nThe problem does not specify concrete values for all hyperparameters (e.g., learning rates, weight decay coefficients, noise level). This is interpreted not as an invalidating omission, but as a delegation of responsibility to the solver to choose scientifically reasonable values that allow the simulation to demonstrate the intended phenomena. I will select appropriate values to fulfill the simulation's objectives. The problem is therefore deemed **valid**.\n\n### Step 2: Solution Design\n\nThe solution will be implemented as a Python program that iterates through the provided test cases. For each case, it will perform the following steps.\n\n#### Data Generation\nA bigram probability matrix $P \\in \\mathbb{R}^{V \\times V}$ is constructed by drawing each of its $V$ columns from a Dirichlet distribution with a uniform prior (all concentration parameters equal to $1$). This ensures each column represents a valid probability distribution. Using this matrix, we generate $n_{\\text{train}}$ training and $n_{\\text{test}}$ test token pairs $(x, y)$ by simulating a Markov chain. For a fraction $\\rho$ of the training data, the label $y$ is replaced with a random token to simulate label noise, which is crucial for observing the effects of overfitting and subsequent recovery.\n\n#### Feature Engineering\nThe feature vector $x$ for each sample is a concatenation of two parts:\n1.  $x^{\\text{gen}} \\in \\mathbb{R}^V$: A one-hot vector representing the input token. Its corresponding weights $W^{\\text{gen}}$ are intended to learn the general transition probabilities from $P$.\n2.  $x^{\\text{mem}} \\in \\mathbb{R}^M$: A one-hot vector unique to each training sample. For the $i$-th training sample, $x^{\\text{mem}}_i$ is the $i$-th standard basis vector $e_i \\in \\mathbb{R}^M$. For all test samples, $x^{\\text{mem}}$ is the zero vector. These features allow the model to memorize specific training examples, including noisy ones.\n\n#### Model and Training Dynamics\nThe model is a linear softmax classifier. The weight matrix $W \\in \\mathbb{R}^{V \\times (V+M)}$ is trained using full-batch gradient descent. The training process is structured into three phases to orchestrate the learning dynamics:\n1.  **Early Phase ($t/T \\in [0, 1/3)$)**: A high learning rate for $W^{\\text{gen}}$ and a low one for $W^{\\text{mem}}$ encourages the model to first learn the generalizable bigram structure present in the data. Test error is expected to decrease.\n2.  **Middle Phase ($t/T \\in [1/3, 2/3)$)**: The learning rates are flipped. A high learning rate for $W^{\\text{mem}}$ drives the model to fit the training data perfectly by using the memorization features. This leads to interpolation (training accuracy $\\to 1.0$), often by fitting noisy labels. This overfitting causes the test error to increase, creating the \"ascent\" part of the U-shaped curve.\n3.  **Late Phase ($t/T \\in [2/3, 1]$)**: The learning rate for $W^{\\text{mem}}$ is set to $0$, halting gradient-based updates. Simultaneously, a strong $\\ell_2$ weight decay is applied to $W^{\\text{mem}}$, causing its weights to shrink towards zero. This forces the model to \"forget\" the noisy patterns it memorized. With $W^{\\text{gen}}$ again trained with a high learning rate, the model relearns the generalizable structure, causing the test error to decrease again—the \"second descent.\"\n\nThe weight update rule is implemented as a decoupled weight decay. For each block of weights $W_p \\in \\{W^{\\text{gen}}, W^{\\text{mem}}\\}$, the update in one epoch is:\n1.  Gradient step: $W_p \\leftarrow W_p - \\eta_p \\nabla_{W_p} \\mathcal{L}_{\\text{emp}}$\n2.  Decay step: $W_p \\leftarrow W_p (1 - \\alpha_p)$, where $\\alpha_p$ is the per-epoch weight decay rate.\nThis formulation ensures that even when the learning rate $\\eta_{\\text{mem}}$ is zero, the decay step for $W^{\\text{mem}}$ remains active.\n\n#### Detection Logic\nThroughout training, the test cross-entropy loss is recorded at each epoch. After training is complete, the recorded data is analyzed:\n1.  The interpolation epoch $t_\\star$ is identified as the first epoch where training accuracy surpasses $0.99$. If this never happens, the result is `False`.\n2.  The test losses at three key epochs are compared: an early checkpoint $t_a = \\lfloor 0.2 T \\rfloor$, the interpolation epoch $t_\\star$, and the final epoch $T-1$ (in $0$-based indexing).\n3.  The double descent condition—$L_{\\text{test}}(t_\\star)$ being a peak higher than both $L_{\\text{test}}(t_a)$ and $L_{\\text{test}}(T-1)$, and $L_{\\text{test}}(T-1)$ showing improvement over $L_{\\text{test}}(t_a)$—is checked. If it holds, the result is `True`, otherwise it is `False`.\n\nThis structured simulation provides a clear, verifiable demonstration of epoch-wise double descent by carefully manipulating model capacity and training dynamics.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef _run_simulation(params, seed):\n    \"\"\"\n    Runs a single simulation for a given set of parameters.\n    \"\"\"\n    V, n_train, n_test, M, T = params\n    rng = np.random.default_rng(seed)\n\n    # --- Hyperparameters ---\n    # These values are chosen to be reasonable for demonstrating the phenomenon.\n    dirichlet_alpha = 1.0\n    label_noise_rho = 0.1\n    # Learning rates for the three phases\n    lr_gen_large, lr_gen_small = 1.0, 0.01\n    lr_mem_large, lr_mem_small = 1.0, 0.01\n    # Decoupled weight decay rates\n    wd_rate_gen = 1e-4\n    wd_rate_mem_base = 1e-4\n    wd_rate_mem_late = 1e-1  # Stronger late-phase decay for memorization weights\n\n    # --- Data Generation ---\n    P_bigram = np.zeros((V, V))\n    for j in range(V):\n        P_bigram[:, j] = rng.dirichlet(np.ones(V) * dirichlet_alpha)\n\n    def generate_samples(num_samples, P_matrix):\n        xs, ys = [], []\n        current_token = rng.choice(V)\n        for _ in range(num_samples):\n            prev_token = current_token\n            probs = P_matrix[:, prev_token]\n            current_token = rng.choice(V, p=probs)\n            xs.append(prev_token)\n            ys.append(current_token)\n        return np.array(xs), np.array(ys)\n\n    x_train_tokens, y_train_tokens = generate_samples(n_train, P_bigram)\n    x_test_tokens, y_test_tokens = generate_samples(n_test, P_bigram)\n    \n    noise_indices = rng.choice(n_train, size=int(n_train * label_noise_rho), replace=False)\n    y_train_tokens[noise_indices] = rng.choice(V, size=len(noise_indices))\n\n    y_train_onehot = np.eye(V)[y_train_tokens].T\n    y_test_onehot = np.eye(V)[y_test_tokens].T\n\n    # --- Feature Construction ---\n    X_gen_train = np.eye(V)[x_train_tokens].T\n    X_gen_test = np.eye(V)[x_test_tokens].T\n\n    X_mem_train = np.eye(M, n_train) if M > 0 else np.empty((0, n_train))\n    X_mem_test = np.zeros((M, n_test))\n\n    X_train = np.vstack([X_gen_train, X_mem_train])\n    X_test = np.vstack([X_gen_test, X_mem_test])\n    \n    # --- Model and Training ---\n    F = V + M  # Total feature dimension\n    limit = np.sqrt(6 / (V + F)) # Glorot/Xavier initialization\n    W_gen = rng.uniform(-limit, limit, (V, V))\n    W_mem = rng.uniform(-limit, limit, (V, M)) if M > 0 else np.empty((V, 0))\n\n    test_losses = []\n    t_star = -1\n\n    def cross_entropy_loss(logits, y_onehot_targets):\n        log_probs = logits - logsumexp(logits, axis=0, keepdims=True)\n        # Use a small epsilon to avoid log(0)\n        return -np.sum(y_onehot_targets * log_probs) / y_onehot_targets.shape[1]\n\n    for t in range(T):\n        phase_frac = t / T\n        if phase_frac  1/3:  # Early phase\n            lr_gen, lr_mem = lr_gen_large, lr_mem_small\n            wd_rate_mem = wd_rate_mem_base\n        elif phase_frac  2/3:  # Middle phase\n            lr_gen, lr_mem = lr_gen_small, lr_mem_large\n            wd_rate_mem = wd_rate_mem_base\n        else:  # Late phase\n            lr_gen, lr_mem = lr_gen_large, 0.0\n            wd_rate_mem = wd_rate_mem_late\n        \n        W = np.hstack([W_gen, W_mem])\n        logits_train = W @ X_train\n        probs_train = np.exp(logits_train - logsumexp(logits_train, axis=0, keepdims=True))\n\n        grad_W = (probs_train - y_train_onehot) @ X_train.T / n_train\n        \n        # Gradient update step\n        W_gen -= lr_gen * grad_W[:, :V]\n        if M > 0:\n            W_mem -= lr_mem * grad_W[:, V:]\n        \n        # Decoupled weight decay step\n        W_gen *= (1 - wd_rate_gen)\n        if M > 0:\n            W_mem *= (1 - wd_rate_mem)\n\n        W_eval = np.hstack([W_gen, W_mem])\n        \n        # Find interpolation epoch t_star\n        if t_star == -1:\n            preds_train = np.argmax(W_eval @ X_train, axis=0)\n            train_acc = np.mean(preds_train == y_train_tokens)\n            if train_acc >= 0.99:\n                t_star = t\n\n        # Record test loss\n        logits_test = W_eval @ X_test\n        test_loss = cross_entropy_loss(logits_test, y_test_onehot)\n        test_losses.append(test_loss)\n    \n    # --- Analysis for Double Descent ---\n    if t_star == -1:  # Interpolation did not occur\n        return False\n        \n    t_a = int(np.floor(0.2 * T))\n    L_test_ta = test_losses[t_a]\n    L_test_tstar = test_losses[t_star]\n    L_test_T = test_losses[T-1]\n\n    is_peak = L_test_tstar > max(L_test_ta, L_test_T)\n    is_recovery = L_test_T  L_test_ta\n    \n    return is_peak and is_recovery\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (8, 150, 500, 0, 180),   # Case 1: Underparameterized\n        (8, 150, 500, 150, 180), # Case 2: Balanced overparameterization\n        (8, 100, 500, 300, 180)  # Case 3: Heavily overparameterized\n    ]\n\n    results = []\n    # Use a fixed seed for reproducibility of each simulation run\n    seed = 42 \n    for case in test_cases:\n        result = _run_simulation(case, seed)\n        results.append(result)\n\n    # Format output exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3183606"}]}