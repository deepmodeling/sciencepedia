{"hands_on_practices": [{"introduction": "A network's width is fundamentally linked to its ability to represent complex functions, but how can we make this abstract idea concrete? This practice provides a tangible link by connecting network width $w$ to the expressive power of the final feature representation, measured by its matrix rank $\\mathrm{rank}(H)$. By performing a simulated pruning experiment [@problem_id:3157479], you will discover the minimal width required for the network to have the theoretical capacity to perfectly fit a given dataset, thereby turning the abstract concept of 'function space' into a computable quantity.", "problem": "You are to design and implement a controlled width-pruning experiment for a fully specified deep network, to determine the minimal width threshold at which the function space realized by a fixed-depth network loses the ability to interpolate arbitrary labels on a fixed finite dataset when only the final linear readout is trainable. The experiment must adhere to first principles from linear algebra and a precise architecture definition. All mathematical symbols, functions, operators, and numbers must be written in LaTeX.\n\nConsider the following setting. Let there be a dataset with $m$ input samples collected as a matrix $X \\in \\mathbb{R}^{m \\times d}$. Consider a depth-$L$ feedforward network with $L$ hidden layers and a single linear output neuron, where only the final linear readout weights are trainable and all hidden-layer parameters are fixed. The hidden representation after $L$ layers is denoted by $H \\in \\mathbb{R}^{m \\times w}$, where $w$ is the width (number of channels) shared by every hidden layer. The final network output for a label vector $y \\in \\mathbb{R}^m$ when training only the last layer to fit $y$ corresponds to solving a linear system in the features $H$, so the ability to interpolate arbitrary labels on the $m$ samples requires that the column space of $H$ has dimension $m$; equivalently, $\\mathrm{rank}(H) = m$.\n\nArchitecture. The hidden layers are defined as follows:\n- First hidden layer: $h^{(1)} = \\mathrm{ReLU}(X A + \\mathbf{1} b^{(1)\\top})$, where $A \\in \\mathbb{R}^{d \\times W_0}$ is the initial first-layer weight matrix, $b^{(1)} \\in \\mathbb{R}^{W_0}$ is the bias vector, and $\\mathbf{1} \\in \\mathbb{R}^{m}$ is the vector of all ones.\n- Subsequent hidden layers for $\\ell \\in \\{2,\\dots,L\\}$: $h^{(\\ell)} = \\mathrm{ReLU}\\!\\left(h^{(\\ell-1)} \\odot s^{(\\ell)} + \\mathbf{1} b^{(\\ell)\\top}\\right)$, where $s^{(\\ell)} \\in \\mathbb{R}^{W_0}$ is a per-channel scaling vector, $b^{(\\ell)} \\in \\mathbb{R}^{W_0}$ is a per-channel bias vector, and $\\odot$ denotes elementwise multiplication broadcast over the $m$ samples.\n- The final feature matrix is $H = h^{(L)} \\in \\mathbb{R}^{m \\times w}$ after structured width pruning to width $w$ as defined below.\n\nWidth pruning protocol. You must implement a width pruning procedure that selects a subset of channels of size $w$ from an initial width $W_0$ using an importance measure computed from the fixed hidden parameters. Define the importance of channel $j \\in \\{1,\\dots,W_0\\}$ by\n$$\nI_j \\;=\\; \\left\\|A_{:,j}\\right\\|_2 \\,\\times\\, \\prod_{\\ell=2}^{L}\\left|s^{(\\ell)}_j\\right| ,\n$$\nwith the convention that the empty product equals $1$ when $L = 1$. To prune to width $w$, select the $w$ channels with the largest $I_j$ values and keep those indices, preserving their original order. Let the selected index set be $S_w \\subset \\{1,\\dots,W_0\\}$ with $|S_w| = w$. The pruned hidden parameters are then $A_{:,S_w}$, $b^{(1)}_{S_w}$, and for $\\ell \\in \\{2,\\dots,L\\}$, the vectors $s^{(\\ell)}_{S_w}$ and $b^{(\\ell)}_{S_w}$. The forward propagation definitions above are applied with these pruned parameters to produce $H_w \\in \\mathbb{R}^{m \\times w}$. Only the last linear readout (not part of this computation) would be trained; thus, the function space over the $m$ data points realizable by varying only the last layer is the column space of $H_w$, whose dimension is $\\mathrm{rank}(H_w)$.\n\nCollapse threshold. Define the collapse threshold width $w^\\star$ to be the minimal width $w \\in \\{1,\\dots,W_0\\}$ such that $\\mathrm{rank}(H_w) = m$. If no such $w$ exists, define $w^\\star = -1$. Intuitively, for any $w  w^\\star$, the function space collapses in the sense that it cannot interpolate arbitrary labels on the $m$ samples when only the last layer is trained.\n\nData and parameter generation. For each test case, use a pseudorandom number generator initialized with the specified integer seed, and draw all entries independently from the standard normal distribution with zero mean and unit variance:\n- $X \\in \\mathbb{R}^{m \\times d}$,\n- $A \\in \\mathbb{R}^{d \\times W_0}$,\n- $b^{(1)} \\in \\mathbb{R}^{W_0}$,\n- For each $\\ell \\in \\{2,\\dots,L\\}$: $s^{(\\ell)} \\in \\mathbb{R}^{W_0}$ and $b^{(\\ell)} \\in \\mathbb{R}^{W_0}$.\n\nComputation of rank. Given $H_w \\in \\mathbb{R}^{m \\times w}$, compute $\\mathrm{rank}(H_w)$ using the standard singular value decomposition definition with a numerically stable threshold. You may use a well-tested numerical linear algebra routine for matrix rank based on singular value decomposition.\n\nTask. Implement a program that, for each test case below, constructs the data and parameters, performs the pruning for all candidate widths $w \\in \\{1,2,\\dots,W_0\\}$, computes $\\mathrm{rank}(H_w)$ for each $w$, and returns the threshold $w^\\star$ as defined above.\n\nTest suite. Use the following four test cases, each given as a tuple $(L, d, m, W_0, \\text{seed})$:\n- Case $1$: $(L, d, m, W_0, \\text{seed}) = (2, 8, 6, 16, 1)$.\n- Case $2$: $(L, d, m, W_0, \\text{seed}) = (4, 8, 6, 12, 2)$.\n- Case $3$ (edge case where $W_0  m$): $(L, d, m, W_0, \\text{seed}) = (3, 8, 10, 7, 3)$.\n- Case $4$ (boundary case with $L=1$): $(L, d, m, W_0, \\text{seed}) = (1, 6, 5, 5, 4)$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite above. For example, if the thresholds are $a_1,a_2,a_3,a_4$, print exactly the single line\n$[a_1,a_2,a_3,a_4]$.\n\nThere are no physical units, no angle units, and no percentages in this problem. All outputs are integers, with the value $-1$ reserved to indicate that no width achieves $\\mathrm{rank}(H_w)=m$ for the given test case.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the theory of neural network capacity, mathematically well-posed, and provides a complete, objective, and formalizable description of the computational experiment to be performed. All terms are clearly defined, and the required parameters are specified for each test case.\n\nThe solution proceeds by implementing the specified computational experiment for each test case. The objective is to find the collapse threshold width $w^\\star$, defined as the minimal width $w \\in \\{1, \\dots, W_0\\}$ for which the rank of the final hidden representation matrix $H_w$ equals the number of data samples $m$. If no such width exists, $w^\\star = -1$.\n\nThe overall procedure for a single test case $(L, d, m, W_0, \\text{seed})$ is as follows:\n\n1.  **Parameter Generation**: A pseudorandom number generator is initialized with the provided `seed`. The data matrix $X \\in \\mathbb{R}^{m \\times d}$ and all fixed network parameters—$A \\in \\mathbb{R}^{d \\times W_0}$, $b^{(1)} \\in \\mathbb{R}^{W_0}$, and for each layer $\\ell \\in \\{2, \\dots, L\\}$, the scaling and bias vectors $s^{(\\ell)}, b^{(\\ell)} \\in \\mathbb{R}^{W_0}$—are drawn from the standard normal distribution $\\mathcal{N}(0, 1)$.\n\n2.  **Importance Score Calculation**: The importance $I_j$ of each of the $W_0$ initial channels (indexed by $j \\in \\{1, \\dots, W_0\\}$) is calculated according to the formula:\n    $$\n    I_j = \\left\\|A_{:,j}\\right\\|_2 \\times \\prod_{\\ell=2}^{L}\\left|s^{(\\ell)}_j\\right|\n    $$\n    Here, $A_{:,j}$ is the $j$-th column of the first layer weight matrix $A$. The L2-norm $\\| \\cdot \\|_2$ measures the magnitude of the weights connecting the input features to the $j$-th channel. The product term aggregates the magnitudes of the per-channel scaling factors across all subsequent layers. For the case where $L=1$, the product is empty and conventionally defined to be $1$. The importance scores for all $W_0$ channels are computed and stored.\n\n3.  **Channel Pruning Order**: The channels are ranked in descending order of their importance scores $I_j$. This determines the sequence in which channels are considered for inclusion in the pruned network. Specifically, we obtain an ordered list of the channel indices $\\{1, \\dots, W_0\\}$ from most important to least important.\n\n4.  **Search for Collapse Threshold $w^\\star$**: The core of the task is to find the minimal width $w$ that satisfies the rank condition.\n    -   A fundamental property of matrix rank is that $\\mathrm{rank}(H_w) \\le \\min(m, w)$. For the condition $\\mathrm{rank}(H_w) = m$ to be met, it is necessary that $w \\ge m$. Therefore, if the maximum possible width $W_0$ is less than the number of samples $m$, no solution can exist. In this case, $w^\\star = -1$ by definition.\n    -   If $W_0 \\ge m$, we search for the minimal $w$ that satisfies the condition. The search proceeds by testing widths $w$ in increasing order. Since any width $w  m$ cannot satisfy the condition, the search can efficiently start from $w=m$ and proceed up to $W_0$.\n    -   For each candidate width $w \\in \\{m, m+1, \\dots, W_0\\}$:\n        a.  **Select Channels**: The $w$ channels with the highest importance scores are selected. Let their original indices be the set $S_w$. To preserve the original channel numbering, these indices are sorted in ascending order before being used to slice the parameter tensors.\n        b.  **Construct Pruned Network**: The parameters for the pruned network of width $w$ are formed by selecting the columns and elements corresponding to the indices in $S_w$ from the original parameter tensors: $A_{:,S_w}$, $b^{(1)}_{S_w}$, $s^{(\\ell)}_{S_w}$, and $b^{(\\ell)}_{S_w}$.\n        c.  **Forward Pass**: The input data $X$ is propagated through the pruned network to compute the final hidden representation matrix $H_w \\in \\mathbb{R}^{m \\times w}$.\n            -   The first hidden layer activation is $h^{(1)} = \\mathrm{ReLU}(X A_{:,S_w} + \\mathbf{1} (b^{(1)}_{S_w})^\\top)$.\n            -   For layers $\\ell \\in \\{2, \\dots, L\\}$, the activations are updated iteratively: $h^{(\\ell)} = \\mathrm{ReLU}(h^{(\\ell-1)} \\odot s^{(\\ell)}_{S_w} + \\mathbf{1} (b^{(\\ell)}_{S_w})^\\top)$, where $\\odot$ is element-wise multiplication with broadcasting.\n            -   The final matrix is $H_w = h^{(L)}$.\n        d.  **Rank Computation**: The rank of the resulting matrix $H_w$ is computed using a numerically stable method based on Singular Value Decomposition (SVD), as provided by standard numerical libraries.\n        e.  **Condition Check**: If $\\mathrm{rank}(H_w) = m$, then $w$ is the smallest width (starting from $m$) that satisfies the interpolation condition. This value is recorded as $w^\\star$, and the search for the current test case terminates.\n\n5.  **Final Result**: If the search loop completes without finding a suitable $w$, it means no width up to $W_0$ can satisfy the rank condition, and thus $w^\\star$ is set to $-1$. The calculated value of $w^\\star$ for each test case is collected. The final output is a list of these values.", "answer": "```python\nimport numpy as np\n\ndef compute_w_star(L: int, d: int, m: int, W0: int, seed: int) - int:\n    \"\"\"\n    Computes the collapse threshold width w_star for a single test case.\n\n    Args:\n        L: Number of hidden layers.\n        d: Input dimension.\n        m: Number of samples.\n        W0: Initial width of hidden layers.\n        seed: Seed for the pseudorandom number generator.\n\n    Returns:\n        The collapse threshold width w_star, or -1 if no such width exists.\n    \"\"\"\n    # According to the problem, rank(H_w) must be m. The rank of a matrix\n    # cannot exceed its number of columns, w. Thus, if W0  m, it's impossible\n    # to achieve rank m.\n    if W0  m:\n        return -1\n\n    # 1. Parameter Generation\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal(size=(m, d))\n    A = rng.standard_normal(size=(d, W0))\n    b1 = rng.standard_normal(size=W0)\n    \n    s_params = []\n    b_params = []\n    if L  1:\n        for _ in range(2, L + 1):\n            s_params.append(rng.standard_normal(size=W0))\n            b_params.append(rng.standard_normal(size=W0))\n\n    # 2. Importance Score Calculation\n    # I_j = ||A_(:,j)||_2 * product(|s^(l)_j|) for l=2..L\n    A_col_norms = np.linalg.norm(A, axis=0)\n    \n    # The product is 1 if L=1 (empty product)\n    s_prods = np.ones(W0)\n    if L  1:\n        for s_l in s_params:\n            s_prods *= np.abs(s_l)\n            \n    importances = A_col_norms * s_prods\n\n    # 3. Channel Pruning Order\n    # Get indices of channels sorted by importance in descending order\n    sorted_channel_indices = np.argsort(importances)[::-1]\n\n    # 4. Search for Collapse Threshold w_star\n    # Search starts from w=m, as rank(H_w) = w  m is not possible.\n    # The first w = m that satisfies the condition is the minimal one.\n    for w in range(m, W0 + 1):\n        # a. Select Channels\n        top_w_indices = sorted_channel_indices[:w]\n        # Sort indices to preserve original order as per problem spec\n        current_indices = np.sort(top_w_indices)\n\n        # b. Construct Pruned Network\n        A_w = A[:, current_indices]\n        b1_w = b1[current_indices]\n\n        # c. Forward Pass\n        h = np.maximum(0, X @ A_w + b1_w)\n        \n        if L  1:\n            for l_idx in range(L - 1):\n                s_l_w = s_params[l_idx][current_indices]\n                b_l_w = b_params[l_idx][current_indices]\n                h = np.maximum(0, h * s_l_w + b_l_w)\n        \n        H_w = h\n        \n        # d. Rank Computation and e. Condition Check\n        if np.linalg.matrix_rank(H_w) == m:\n            return w\n\n    # If the loop finishes, no width w in [m, W0] satisfied the condition\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (2, 8, 6, 16, 1),  # Case 1\n        (4, 8, 6, 12, 2),  # Case 2\n        (3, 8, 10, 7, 3),  # Case 3\n        (1, 6, 5, 5, 4),   # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        L, d, m, W0, seed = case\n        w_star = compute_w_star(L, d, m, W0, seed)\n        results.append(w_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3157479"}, {"introduction": "While deeper networks are theoretically more expressive, making them work in practice requires architectural innovations. Skip connections are a key innovation, but their effect can seem magical. This exercise demystifies them by modeling the network as a directed graph [@problem_id:3157518], allowing you to count the number of signal paths and their lengths. By computing metrics like the 'effective depth utilization' $U$, you will gain a quantitative understanding of how adding shortcuts fundamentally alters the pathways available for information and gradient flow.", "problem": "Consider a Directed Acyclic Graph (DAG) representation of a feedforward deep neural network with residual skip connections. The network has layers indexed by $0,1,2,\\dots,L$, where layer $0$ is the input and layer $L$ is the output. Each layer has a fixed width of $w$ neurons. There is a complete bipartite set of edges from every neuron in layer $i$ to every neuron in layer $i+1$ for all $i \\in \\{0,1,\\dots,L-1\\}$. Additionally, skip connections are added with a specified frequency parameter $s \\in \\mathbb{N}$: if $i \\equiv 0 \\pmod{s}$ and $i+2 \\leq L$, then there is a complete bipartite set of edges from every neuron in layer $i$ to every neuron in layer $i+2$. No other skip connections exist.\n\nA directed path from the output layer $L$ to the input layer $0$ is defined by traversing edges backward (equivalently, forward from $0$ to $L$). For any path, define the gradient path length $N_{\\text{edges}}$ as the number of edges traversed by that path. Assume a linearized regime where each edge has unit local Jacobian gain. Under this assumption, every neuron-to-neuron path contributes equally to the gradient magnitude, so the distribution of gradient path lengths coincides with the distribution induced by counting neuron-to-neuron paths.\n\nDefine the following quantities for given $L$, $w$, and $s$:\n- The effective depth utilization $U$, defined as the expected number of edges $\\mathbb{E}[N_{\\text{edges}}]$ across all neuron-to-neuron paths from layer $0$ to layer $L$, divided by $L$, that is, $U = \\mathbb{E}[N_{\\text{edges}}] / L$. This measures the fraction of layer-to-layer transitions typically utilized by gradients in the presence of skip connections.\n- The minimal gradient path length $N_{\\min}$, defined as the smallest possible number of edges among all valid paths from layer $0$ to layer $L$.\n- The fraction $F_{\\min}$ of neuron-to-neuron paths that achieve the minimal gradient path length $N_{\\min}$, computed as the ratio of the number of minimal-length neuron-to-neuron paths to the total number of neuron-to-neuron paths.\n\nYour task is to write a complete program that, for a fixed width $w$, computes $U$ and $F_{\\min}$ for several $(L,s)$ configurations by exact combinatorial counting. You must derive these quantities from first principles, using the following foundational bases:\n- The chain rule of backpropagation, which implies that gradient propagation follows directed computational paths and accumulates multiplicatively along edges.\n- The definition of a feedforward layered architecture and residual skip connections as edges in a DAG.\n- Counting of paths in a DAG via dynamic programming and the fact that complete bipartite connectivity across layers multiplies the number of neuron-to-neuron paths by $w$ per traversed edge.\n\nYou must not assume or use any shortcut formulas beyond these bases. In particular, you must not postulate closed-form path counts for this specific skip schedule; instead, the counting must be performed constructively.\n\nTest Suite:\nUse the fixed width $w = 8$ and evaluate the following four cases:\n1. $(L,s) = (10,1)$\n2. $(L,s) = (10,2)$\n3. $(L,s) = (11,3)$\n4. $(L,s) = (10,64)$\n\nFor each case, compute $U$ and $F_{\\min}$ as defined above. There are no physical units involved. Express $U$ and $F_{\\min}$ as decimal floats rounded to six decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list $[U,F_{\\min}]$ with both entries rounded to six decimal places. For example, the output should look like\n$[[U_1,F_1],[U_2,F_2],[U_3,F_3],[U_4,F_4]]$,\nwith no additional text printed.", "solution": "The user's request is to solve a computational problem related to path counting in a specific type of neural network graph. I will first validate the problem statement according to the specified criteria.\n\n### Step 1: Extract Givens\n- **Network Structure**: Directed Acyclic Graph (DAG) representing a feedforward neural network.\n- **Layers**: Indexed from $0$ (input) to $L$ (output).\n- **Layer Width**: Each layer $i$ has a fixed width of $w$ neurons.\n- **Sequential Connections**: A complete bipartite set of edges connects every neuron in layer $i$ to every neuron in layer $i+1$ for all $i \\in \\{0, 1, \\dots, L-1\\}$.\n- **Skip Connections**: A skip connection, in the form of a complete bipartite set of edges, exists from layer $i$ to layer $i+2$ if and only if $i \\equiv 0 \\pmod{s}$ and $i+2 \\leq L$, where $s \\in \\mathbb{N}$ is a frequency parameter.\n- **Path Definition**: A directed path from layer $0$ to layer $L$.\n- **Gradient Path Length ($N_{\\text{edges}}$)**: The number of edges traversed by a path.\n- **Linearized Regime Assumption**: Each edge has a unit local Jacobian gain, making the distribution of gradient path lengths equivalent to the distribution of neuron-to-neuron path counts.\n- **Quantities to Compute**:\n    1.  **Effective Depth Utilization ($U$)**: $U = \\mathbb{E}[N_{\\text{edges}}] / L$, where $\\mathbb{E}[N_{\\text{edges}}]$ is the expected number of edges across all neuron-to-neuron paths from layer $0$ to layer $L$.\n    2.  **Fraction of Minimal Paths ($F_{\\min}$)**: The ratio of the number of paths with the minimal length ($N_{\\min}$) to the total number of paths from layer $0$ to layer $L$.\n- **Derivation Constraint**: The quantities must be derived from first principles (backpropagation, DAG structure, combinatorial counting) and not by using pre-existing closed-form solutions for this specific problem.\n- **Test Suite**:\n    - Fixed width: $w = 8$.\n    - Case 1: $(L, s) = (10, 1)$\n    - Case 2: $(L, s) = (10, 2)$\n    - Case 3: $(L, s) = (11, 3)$\n    - Case 4: $(L, s) = (10, 64)$\n- **Output Format**: A single line string `[[U_1,F_1],[U_2,F_2],[U_3,F_3],[U_4,F_4]]` with values rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n\n- **Scientifically Grounded**: The problem is an application of graph theory to an idealized model of a deep neural network. The concepts of layers, skip connections (as in ResNets), path counting, and their relation to gradient flow are standard and well-established in the field of deep learning. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The graph structure is unambiguously defined by the parameters $L$, $w$, and $s$. The quantities to be computed, $U$ and $F_{\\min}$, are precisely defined. The problem is self-contained and has a unique, computable solution for each test case.\n- **Objective**: The problem is stated in precise mathematical language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the enumerated invalidity flaws. It is a formalizable, complete, and verifiable mathematical problem that requires substantive reasoning.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived and provided.\n\n### Principle-Based Design\n\nThe problem requires us to analyze the distribution of path lengths in a DAG. A path is a sequence of neurons, starting in layer $0$ and ending in layer $L$. The structure of the network is highly regular, which allows for a systematic counting method. We will use dynamic programming to constructively count the paths, as required.\n\n**Symmetry and Simplification**\nThe network exhibits a high degree of symmetry. All neurons within a given layer are topologically equivalent. Paths originating from any of the $w$ neurons in layer $0$ have the same length distribution. Consequently, we can simplify the problem by counting paths originating from a *single, arbitrary source neuron* in layer $0$. The quantities of interest, $U$ and $F_{\\min}$, are ratios where the total number of starting neurons ($w$) would appear as a common factor in both the numerator and denominator, thus canceling out.\n\n**Dynamic Programming Formulation**\nLet us define the state for our dynamic programming approach. We need to track the number of paths of varying lengths to each layer.\n\nLet $C_j$ be a data structure (specifically, a map or dictionary) that stores the distribution of path lengths for all paths starting from a single source neuron in layer $0$ and ending at *any* neuron in layer $j$. The keys of the map will be the path length $k$ (number of edges), and the values will be the total count of such paths.\n\n**Base Case**\nThe process starts at layer $0$. We consider a path from our source neuron to itself to have length $0$. This serves as the foundation for our recurrence. Thus, the distribution for layer $0$ is:\n$$C_0 = \\{0: 1\\}$$\nThis signifies one path of length $0$ ending at layer $0$.\n\n**Recurrence Relation**\nWe can compute the path distribution $C_j$ for layer $j  0$ by considering all possible preceding layers. According to the problem, a neuron in layer $j$ can be reached from:\n1.  **Layer $j-1$ (Sequential Connection)**: There is always a connection from layer $j-1$ to layer $j$.\n2.  **Layer $j-2$ (Skip Connection)**: A connection exists from layer $j-2$ to layer $j$ if and only if $j-2 \\geq 0$ and $(j-2) \\pmod{s} = 0$.\n\nConsider a path of length $k'$ that ends at some neuron in a preceding layer (e.g., layer $j-1$). To extend this path to layer $j$, we traverse one additional edge. Since the connection from any neuron in layer $j-1$ to layer $j$ is complete bipartite, this single path can be extended to $w$ distinct paths, one for each of the $w$ neurons in layer $j$.\n\nTherefore, for each path of length $k'$ ending in layer $j-1$ (with a total count of $C_{j-1}[k']$), we generate $w \\times C_{j-1}[k']$ new paths of length $k'+1$ that end in layer $j$.\n\nApplying this logic, we derive the recurrence for building $C_j$:\n1.  Initialize $C_j$ as an empty map.\n2.  **Contribution from Layer $j-1$**: For each length $k'$ and its count $N_{k'}$ in $C_{j-1}$, add $w \\cdot N_{k'}$ to the count for length $k'+1$ in $C_j$.\n3.  **Contribution from Layer $j-2$**: If a skip connection exists from layer $j-2$ to $j$, for each length $k'$ and its count $N_{k'}$ in $C_{j-2}$, add $w \\cdot N_{k'}$ to the count for length $k'+1$ in $C_j$.\n\nThis process is iterated for $j=1, 2, \\dots, L$. The final map $C_L$ will contain the complete path length distribution from the source neuron in layer $0$ to all neurons in layer $L$.\n\n**Calculation of Final Quantities**\nOnce $C_L$ is computed, we can determine the required quantities. Let $C_L = \\{k_1: N_1, k_2: N_2, \\dots\\}$.\n\n1.  **Total number of paths ($N_{\\text{total}}$)**: This is the sum of counts for all path lengths.\n    $$N_{\\text{total}} = \\sum_{i} N_i$$\n2.  **Expected Path Length ($\\mathbb{E}[N_{\\text{edges}}]$)**: This is the weighted average of path lengths, where the weights are the path counts.\n    $$\\mathbb{E}[N_{\\text{edges}}] = \\frac{\\sum_{i} k_i \\cdot N_i}{N_{\\text{total}}}$$\n3.  **Effective Depth Utilization ($U$)**:\n    $$U = \\frac{\\mathbb{E}[N_{\\text{edges}}]}{L}$$\n4.  **Minimal Path Length ($N_{\\min}$)**: This is simply the smallest key in the map $C_L$.\n    $$N_{\\min} = \\min(\\{k_1, k_2, \\dots\\})$$\n5.  **Fraction of Minimal Paths ($F_{\\min}$)**: This is the count of paths with length $N_{\\min}$ divided by the total number of paths.\n    $$F_{\\min} = \\frac{C_L[N_{\\min}]}{N_{\\text{total}}}$$\n\nThe counts can become very large, but Python's arbitrary-precision integers handle this without overflow. The final division to compute $U$ and $F_{\\min}$ will be done using floating-point arithmetic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the path counting problem for the specified test cases.\n    \"\"\"\n\n    # Fixed parameters as per the problem statement.\n    w = 8\n    \n    # Test cases: tuples of (L, s).\n    test_cases = [\n        (10, 1),\n        (10, 2),\n        (11, 3),\n        (10, 64)\n    ]\n\n    results = []\n\n    for L, s in test_cases:\n        # C is a list of dictionaries. C[j] will store the path length distribution\n        # for paths from a single source neuron in layer 0 to all neurons in layer j.\n        # C[j][k] = count of paths with length k.\n        C = [{} for _ in range(L + 1)]\n\n        # Base case: A path of length 0 from the source neuron to itself.\n        C[0] = {0: 1}\n\n        # Dynamic programming to compute path distributions for each layer.\n        for j in range(1, L + 1):\n            \n            current_dist = {}\n\n            # 1. Contribution from the sequential connection (layer j-1)\n            # Paths to layer j-1 are extended by one edge.\n            prev_dist_seq = C[j-1]\n            for k, count in prev_dist_seq.items():\n                new_length = k + 1\n                # Each of the 'count' paths can be extended to 'w' new paths,\n                # one for each neuron in layer j.\n                num_new_paths = w * count\n                current_dist[new_length] = current_dist.get(new_length, 0) + num_new_paths\n\n            # 2. Contribution from the skip connection (layer j-2), if it exists.\n            if j - 2 = 0 and (j - 2) % s == 0:\n                prev_dist_skip = C[j-2]\n                for k, count in prev_dist_skip.items():\n                    new_length = k + 1\n                    num_new_paths = w * count\n                    current_dist[new_length] = current_dist.get(new_length, 0) + num_new_paths\n            \n            C[j] = current_dist\n\n        # The final distribution of path lengths to layer L.\n        final_dist = C[L]\n        \n        # Calculate the required quantities from the final distribution.\n        # Python's integers handle arbitrarily large numbers, preventing overflow.\n        total_paths = 0\n        weighted_sum_of_lengths = 0\n        \n        for k, count in final_dist.items():\n            total_paths += count\n            weighted_sum_of_lengths += k * count\n\n        # Calculate U (Effective Depth Utilization)\n        if total_paths  0:\n            expected_n_edges = float(weighted_sum_of_lengths) / float(total_paths)\n            U = expected_n_edges / L\n        else:\n            U = 0.0\n\n        # Calculate F_min (Fraction of Minimal Paths)\n        if total_paths  0:\n            n_min = min(final_dist.keys())\n            count_min_paths = final_dist[n_min]\n            F_min = float(count_min_paths) / float(total_paths)\n        else:\n            F_min = 0.0\n\n        # Store results rounded to six decimal places.\n        results.append([round(U, 6), round(F_min, 6)])\n\n    # Format the final output string as required.\n    # e.g., [[U1,F1],[U2,F2],...]\n    output_str = \"[\" + \",\".join([f\"[{u},{f}]\" for u, f in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3157518"}, {"introduction": "Ultimately, architectural choices are judged by their impact on performance. In modern deep learning, a key practice is to run large-scale experiments to find empirical 'scaling laws' that relate model size to performance metrics like test loss. In this final exercise [@problem_id:3157535], you will step into the role of an ML researcher to analyze hypothetical experimental data, using statistical modeling to uncover the precise power-law relationship between test loss $t$ and the network's depth $L$ and width $w$. This practice bridges the gap from architectural theory to data-driven engineering.", "problem": "You are tasked with estimating how test loss scales with network depth and width using a principled statistical procedure. Consider a family of neural networks characterized by depth $L$ (a positive integer) and width $w$ (a positive integer). For a fixed dataset size regime (treated as a categorical group), empirical test loss is observed to follow a multiplicative power law with group-specific proportionality:\n$$\nt \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon,\n$$\nwhere $t$ is the observed test loss (a positive real number), $A_g$ is an unknown positive constant specific to group $g$ (dataset size regime), $\\alpha$ and $\\beta$ are real-valued exponents that are invariant across groups, and $\\varepsilon$ is a positive noise factor. Assume that the noise is multiplicative and log-normal with zero mean in the logarithmic domain, which implies that the natural logarithm of the observed test loss has additive, independent, identically distributed Gaussian perturbations. Your task is to infer the exponents $\\alpha$ and $\\beta$ using a statistically grounded estimator consistent with these assumptions, simultaneously pooling data across all groups while allowing each group to have its own intercept.\n\nFundamental base you must rely on:\n- Definitions of the natural logarithm transformation converting multiplicative relationships into additive ones.\n- Properties of the Gaussian distribution and the equivalence between Maximum Likelihood Estimation under additive Gaussian noise and the least squares estimator.\n- Linear least squares and the normal equations.\n\nYour program must implement an estimator grounded in these principles to infer $\\alpha$ and $\\beta$ from the provided observations. Treat each dataset size as a separate group with its own constant $A_g$ and estimate $\\alpha$ and $\\beta$ jointly across all groups by minimizing the sum of squared residuals in the log-domain with group-specific intercepts.\n\nYou are given the following test suite with three cases. In each case, you are provided observations grouped by dataset size (group label shown as $N$). Each observation is a tuple $(L,w,t)$ where $L$ is the depth, $w$ is the width, and $t$ is the observed test loss. All numbers are dimensionless.\n\nTest Case $1$ (happy path, three groups, ample variation):\n- Group $N = 10^3$:\n  - $(L,w,t) = (\\,4,\\,128,\\,0.0883883476483184\\,)$\n  - $(L,w,t) = (\\,16,\\,128,\\,0.0441941738241592\\,)$\n- Group $N = 10^4$:\n  - $(L,w,t) = (\\,8,\\,64,\\,0.0441941738241592\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.0220970869120796\\,)$\n- Group $N = 10^5$:\n  - $(L,w,t) = (\\,16,\\,256,\\,0.0125\\,)$\n  - $(L,w,t) = (\\,4,\\,64,\\,0.05\\,)$\n\nTest Case $2$ (boundary of identifiability: exactly determined, two groups):\n- Group $N = 2 \\times 10^3$:\n  - $(L,w,t) = (\\,4,\\,64,\\,0.75\\,)$\n  - $(L,w,t) = (\\,8,\\,64,\\,0.375\\,)$\n- Group $N = 2 \\times 10^4$:\n  - $(L,w,t) = (\\,8,\\,128,\\,0.15\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.15\\,)$\n\nTest Case $3$ (edge case with near-collinearity in $(L,w)$ patterns, two groups):\n- Group $N = 5 \\times 10^2$:\n  - $(L,w,t) = (\\,4,\\,64,\\,0.078125\\,)$\n  - $(L,w,t) = (\\,16,\\,64,\\,0.078125\\,)$\n- Group $N = 5 \\times 10^3$:\n  - $(L,w,t) = (\\,8,\\,128,\\,0.01953125\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.009765625\\,)$\n\nRequirements:\n- Model the logarithm of the observed test loss as an additive linear model with group-specific intercepts and common slopes for $\\ln L$ and $\\ln w$. Estimate $\\alpha$ and $\\beta$ using a least squares estimator that is consistent with Maximum Likelihood Estimation under the Gaussian noise assumption in the log-domain.\n- Implement a single program that processes all three test cases (fixed in your code) and outputs the estimated $\\alpha$ and $\\beta$ for each case, in that order, rounded to three decimal places. If a value is within $5 \\times 10^{-4}$ of zero in magnitude, print it as $0.0$ after rounding.\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a two-element list $[\\alpha,\\beta]$. For example, a valid final output has the form $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$ where $a_i$ and $b_i$ are floating-point numbers printed without additional spaces.\n\nNo physical units are involved. Angles are not involved. Percentages are not used. The answer for each test case must be a list of floats. The final printed line must be exactly in the specified format.", "solution": "The user wants to find the scaling exponents $\\alpha$ and $\\beta$ for a model of neural network test loss.\n\n### Step 1: Extract Givens\n- **Model**: The test loss $t$ is modeled as a multiplicative power law: $t \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon$.\n- **Variables**:\n    - $t$: Observed test loss (positive real number).\n    - $L$: Network depth (positive integer).\n    - $w$: Network width (positive integer).\n    - $g$: Group identifier, representing a dataset size regime (categorical).\n- **Parameters to Estimate**:\n    - $\\alpha, \\beta$: Real-valued exponents, invariant across groups.\n    - $A_g$: Unknown positive proportionality constant specific to group $g$.\n- **Noise Model**: $\\varepsilon$ is a positive, multiplicative, log-normal noise factor. This implies $\\ln(\\varepsilon)$ follows a Gaussian distribution with a mean of zero.\n- **Task**: Estimate $\\alpha$ and $\\beta$ jointly across all groups using a least squares estimator on a linearized version of the model. This approach is consistent with Maximum Likelihood Estimation (MLE) under the specified noise model.\n- **Data**: Three test cases are provided, each with a set of observations $(L, w, t)$ categorized into groups.\n- **Output Format**: For each test case, produce a list $[\\alpha, \\beta]$ with values rounded to three decimal places. Values with a magnitude less than $5 \\times 10^{-4}$ are to be reported as $0.0$. The final output should be a list of these lists.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in the empirical study of scaling laws in deep learning. The power-law model is a standard ansatz, and the assumption of multiplicative log-normal noise, leading to an additive Gaussian model in the log domain, is a conventional and statistically sound approach for parameter estimation in such models. The problem relies on the fundamental principles of logarithm transforms, linear regression, and the normal equations.\n- **Well-Posedness**: The problem describes the estimation of parameters in a linear model (specifically, an Analysis of Covariance or ANCOVA model). A unique solution exists provided the design matrix is of full rank. The problem statements for the test cases, mentioning \"boundary of identifiability\" and \"near-collinearity,\" correctly allude to standard conditions in regression analysis that affect the stability and uniqueness of the solution, indicating that these are features of the data to be handled by the estimator, not flaws in the problem formulation.\n- **Objectivity**: The problem is stated using precise, objective mathematical language, with no subjective or ambiguous terms.\n- **Verdict**: The problem is scientifically sound, well-posed, objective, and contains all necessary information. It is deemed **valid**.\n\n### Step 3: Action\nProceed with providing a complete, reasoned solution.\n\n### Principle-Based Solution Design\nThe core principle is to transform the multiplicative, non-linear model into an additive, linear model, which can then be solved using standard linear least squares methods. This is justified by the log-normal noise assumption, which becomes additive Gaussian noise after the transformation.\n\n**1. Model Linearization**\nThe given model is:\n$$\nt \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon\n$$\nTaking the natural logarithm of both sides linearizes the relationship between the variables and parameters:\n$$\n\\ln(t) \\approx \\ln(A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon)\n$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(a^b) = b\\ln(a)$, we get:\n$$\n\\ln(t) \\approx \\ln(A_g) + \\alpha \\ln(L) + \\beta \\ln(w) + \\ln(\\varepsilon)\n$$\nThis equation has the form of a linear model. Let us define the variables for our regression:\n- Response variable: $y = \\ln(t)$\n- Predictor variables: $x_1 = \\ln(L)$ and $x_2 = \\ln(w)$\n- Parameters: The exponents $\\alpha$ and $\\beta$ are the slopes. The term $\\ln(A_g)$ is an intercept that is specific to each group $g$. Let's denote this group-specific intercept as $\\gamma_g = \\ln(A_g)$.\n- Error term: $\\epsilon' = \\ln(\\varepsilon)$. Based on the problem statement, $\\epsilon'$ is an additive noise term drawn from a Gaussian distribution with mean zero.\n\nThe model for the $i$-th observation, belonging to group $g(i)$, is:\n$$\ny_i = \\gamma_{g(i)} + \\alpha x_{1,i} + \\beta x_{2,i} + \\epsilon'_i\n$$\n\n**2. Least Squares Estimation**\nWe aim to find the parameter values $(\\hat{\\gamma}_g, \\hat{\\alpha}, \\hat{\\beta})$ that minimize the sum of squared residuals (SSR), which is the sum of the squared differences between the observed and predicted $y_i$ values.\n$$\n\\text{SSR} = \\sum_{i=1}^{n} (y_i - (\\hat{\\gamma}_{g(i)} + \\hat{\\alpha} x_{1,i} + \\hat{\\beta} x_{2,i}))^2\n$$\nMinimizing this quantity is equivalent to performing a Maximum Likelihood Estimation of the parameters under the assumption of independent and identically distributed additive Gaussian noise.\n\n**3. Matrix Formulation**\nThis problem can be expressed in matrix form as a general linear model, $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\theta}$.\n- Let there be $n$ total observations and $k$ distinct groups.\n- The parameter vector $\\boldsymbol{\\theta}$ contains the $k$ group intercepts and the two common slopes:\n$$\n\\boldsymbol{\\theta} = [\\gamma_1, \\gamma_2, \\dots, \\gamma_k, \\alpha, \\beta]^T\n$$\nThis is a vector of size $(k+2) \\times 1$.\n- The response vector $\\mathbf{y}$ contains the logarithm of the observed test losses for all $n$ observations:\n$$\n\\mathbf{y} = [\\ln(t_1), \\ln(t_2), \\dots, \\ln(t_n)]^T\n$$\n- The design matrix $\\mathbf{X}$ is an $n \\times (k+2)$ matrix. For each observation $i$ corresponding to group $g(i)$, the $i$-th row of $\\mathbf{X}$ is constructed as follows:\n    - The first $k$ columns are indicator variables (one-hot encoding) for the group membership. The column corresponding to group $g(i)$ will have a $1$, while the other $k-1$ group columns will have a $0$.\n    - The $(k+1)$-th column contains the value $\\ln(L_i)$.\n    - The $(k+2)$-th column contains the value $\\ln(w_i)$.\nThe structure for the $i$-th row associated with group $j$ is:\n$$\n\\mathbf{x}_i^T = [0, \\dots, 1, \\dots, 0, \\ln(L_i), \\ln(w_i)]\n$$\nwhere the $1$ is in the $j$-th position.\n\n**4. Solving the System**\nThe least squares estimate $\\hat{\\boldsymbol{\\theta}}$ that minimizes the SSR is given by the solution to the normal equations:\n$$\n\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nIn practice, computing the matrix inverse directly can be numerically unstable. A more robust method is to use a standard linear least-squares solver, such as `numpy.linalg.lstsq`, which typically employs techniques like QR decomposition or SVD.\n\nOnce the vector $\\hat{\\boldsymbol{\\theta}}$ is computed, the estimates for $\\alpha$ and $\\beta$ are simply the last two elements of this vector:\n$$\n\\hat{\\alpha} = \\hat{\\boldsymbol{\\theta}}_{k} \\quad (\\text{the } (k+1)\\text{-th element})\n$$\n$$\n\\hat{\\beta} = \\hat{\\boldsymbol{\\theta}}_{k+1} \\quad (\\text{the } (k+2)\\text{-th element})\n$$\nThis procedure will be applied to each of the three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes all test cases to estimate scaling exponents alpha and beta.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1 (happy path, three groups, ample variation)\n        {\n            \"10^3\": [\n                (4, 128, 0.0883883476483184),\n                (16, 128, 0.0441941738241592)\n            ],\n            \"10^4\": [\n                (8, 64, 0.0441941738241592),\n                (8, 256, 0.0220970869120796)\n            ],\n            \"10^5\": [\n                (16, 256, 0.0125),\n                (4, 64, 0.05)\n            ]\n        },\n        # Test Case 2 (boundary of identifiability: exactly determined, two groups)\n        {\n            \"2x10^3\": [\n                (4, 64, 0.75),\n                (8, 64, 0.375)\n            ],\n            \"2x10^4\": [\n                (8, 128, 0.15),\n                (8, 256, 0.15)\n            ]\n        },\n        # Test Case 3 (edge case with near-collinearity in (L,w) patterns, two groups)\n        {\n            \"5x10^2\": [\n                (4, 64, 0.078125),\n                (16, 64, 0.078125)\n            ],\n            \"5x10^3\": [\n                (8, 128, 0.01953125),\n                (8, 256, 0.009765625)\n            ]\n        }\n    ]\n\n    def custom_round(value):\n        \"\"\"\n        Rounds a value to 3 decimal places. If the magnitude is less than\n        5e-4, it returns 0.0.\n        \"\"\"\n        if abs(value)  5e-4:\n            return 0.0\n        return round(value, 3)\n\n    results_str_list = []\n    \n    for case in test_cases:\n        observations = []\n        group_indices = []\n        \n        # Assign an integer index to each unique group label\n        group_labels = sorted(case.keys())\n        group_map = {label: i for i, label in enumerate(group_labels)}\n        num_groups = len(group_map)\n\n        # Collect all observations and their corresponding group indices\n        for group_label, data_points in case.items():\n            group_idx = group_map[group_label]\n            for point in data_points:\n                observations.append(point)\n                group_indices.append(group_idx)\n\n        num_obs = len(observations)\n        \n        # Extract L, w, t and transform them to the log domain\n        L_vals = np.array([obs[0] for obs in observations])\n        w_vals = np.array([obs[1] for obs in observations])\n        t_vals = np.array([obs[2] for obs in observations])\n\n        ln_L = np.log(L_vals)\n        ln_w = np.log(w_vals)\n        y = np.log(t_vals) # This is the response vector\n        \n        # Construct the design matrix X\n        # Columns: k group intercepts, 1 for alpha (ln L), 1 for beta (ln w)\n        num_params = num_groups + 2\n        X = np.zeros((num_obs, num_params))\n\n        # Populate the one-hot encoded group intercept columns\n        X[np.arange(num_obs), group_indices] = 1\n        \n        # Populate the slope columns for alpha and beta\n        X[:, num_groups] = ln_L\n        X[:, num_groups + 1] = ln_w\n        \n        # Perform linear least squares regression\n        # theta = (X^T X)^-1 X^T y\n        theta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        # Extract estimated alpha and beta from the parameter vector theta\n        alpha_hat = theta[num_groups]\n        beta_hat = theta[num_groups + 1]\n        \n        # Apply custom rounding as per problem specification\n        rounded_alpha = custom_round(alpha_hat)\n        rounded_beta = custom_round(beta_hat)\n\n        # Format the result for this case\n        results_str_list.append(f\"[{rounded_alpha},{rounded_beta}]\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3157535"}]}