{"hands_on_practices": [{"introduction": "The Neural Tangent Kernel (NTK) provides powerful insights into how neural networks learn. One of its most fundamental predictions is that the speed at which a function is learned is governed by the spectral properties of the kernel, with functions aligned with large-eigenvalue eigenvectors of the NTK Gram matrix $K$ being learned fastest. This exercise provides a concrete, hands-on demonstration of this core principle, allowing you to directly observe how the kernel's spectrum dictates convergence speed by simulating kernel gradient descent on targets that are themselves eigenvectors.", "problem": "You are given a small Neural Tangent Kernel (NTK) Gram matrix and asked to analyze gradient descent dynamics in the corresponding kernel regression on the training set. The Neural Tangent Kernel (NTK) Gram matrix $K \\in \\mathbb{R}^{n \\times n}$ is symmetric positive semidefinite and acts as a linear operator on the vector of predictions over the $n$ training examples. The task is to demonstrate, by explicit computation, that when the target vector is aligned with the top eigenvector of $K$, the error under kernel gradient descent decays rapidly in a few steps.\n\nFundamental base and setup:\n- The kernel regression training loss on the training set is the squared error $L(f) = \\tfrac{1}{2} \\lVert f - y \\rVert_2^2$, where $f \\in \\mathbb{R}^n$ is the vector of model predictions on the $n$ training points, and $y \\in \\mathbb{R}^n$ is the target vector.\n- Kernel gradient descent (in function space) with learning rate $\\eta$ and NTK Gram matrix $K$ performs the update\n$$\nf_{t+1} = f_t - \\eta \\, K \\,(f_t - y),\n$$\nstarting from $f_0 = 0$. This follows from applying gradient descent to $L(f)$, since $\\nabla_f L = f - y$, and the NTK preconditions updates by $K$ in function space.\n- Let the eigendecomposition be $K = U \\Lambda U^\\top$, where $U = [u_1, \\dots, u_n]$ is orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ with $\\lambda_1 \\ge \\cdots \\ge \\lambda_n \\ge 0$. Any target $y$ decomposes as $y = \\sum_{i=1}^n c_i u_i$, where $c_i = u_i^\\top y$.\n\nYour program must:\n1. Use the fixed NTK Gram matrix\n$$\nK = \\begin{bmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{bmatrix}.\n$$\n2. Compute its eigendecomposition to obtain eigenvalues and corresponding unit eigenvectors $u_1, u_2, u_3$, with $u_1$ associated with the largest eigenvalue.\n3. For each test case, construct the specified target $y \\in \\mathbb{R}^3$, run kernel gradient descent for the given number of steps $T$ with learning rate $\\eta$, starting from $f_0 = 0$, and compute the training loss\n$$\nL_T = \\tfrac{1}{2} \\lVert f_T - y \\rVert_2^2.\n$$\n4. Output the results for all test cases as a single line containing a Python-style list with each float rounded to six decimal places.\n\nTest suite:\n- Case $1$: $y = u_1$ (unit vector), $\\eta = 0.25$, $T = 3$.\n- Case $2$: $y = u_3$ (unit vector), $\\eta = 0.25$, $T = 3$.\n- Case $3$: $y = \\mathrm{normalize}(0.8\\, u_1 + 0.2\\, u_3)$ to unit norm, $\\eta = 0.25$, $T = 3$. Here $0.8$ and $0.2$ are real coefficients and $\\mathrm{normalize}(\\cdot)$ scales the vector to have unit Euclidean norm.\n- Case $4$: $y = u_1$, $\\eta = 0.1$, $T = 3$.\n- Case $5$: $y = u_1$, $\\eta = 0.25$, $T = 1$.\n\nAngle units are not applicable. There are no physical units. All outputs must be floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the value of $L_T$ for the corresponding test case, rounded to six decimal places.", "solution": "The problem is valid as it is scientifically grounded in the theory of kernel methods, well-posed with a complete and consistent set of definitions and data, and objectively formulated. We shall proceed with a full solution.\n\nThe problem asks us to analyze the dynamics of kernel gradient descent for a specific Neural Tangent Kernel (NTK) Gram matrix $K$. We will first derive a general analytical expression for the training loss dynamics and then apply it to the given matrix and test cases. This will demonstrate how the convergence rate depends on the alignment of the target vector with the eigenvectors of the kernel matrix.\n\nLet $f_t \\in \\mathbb{R}^n$ be the vector of model predictions on the $n$ training points at step $t$, and let $y \\in \\mathbb{R}^n$ be the target vector. The training loss is the mean squared error $L(f) = \\frac{1}{2} \\lVert f - y \\rVert_2^2$.\nThe kernel gradient descent update rule, with learning rate $\\eta$ and NTK Gram matrix $K$, is given as:\n$$\nf_{t+1} = f_t - \\eta K (f_t - y)\n$$\nWe start from the initial condition $f_0 = 0$. Let the error vector at step $t$ be $e_t = f_t - y$. The initial error is $e_0 = f_0 - y = -y$.\nWe can express the update rule in terms of the error vector by subtracting $y$ from both sides:\n$$\nf_{t+1} - y = (f_t - y) - \\eta K (f_t - y)\n$$\n$$\ne_{t+1} = e_t - \\eta K e_t = (I - \\eta K) e_t\n$$\nwhere $I$ is the $n \\times n$ identity matrix. This is a linear dynamical system for the error vector. By unrolling the recursion, we find the error at step $t$:\n$$\ne_t = (I - \\eta K)^t e_0 = - (I - \\eta K)^t y\n$$\nThe training loss at step $T$ is $L_T = \\frac{1}{2} \\lVert e_T \\rVert_2^2$. Substituting the expression for $e_T$:\n$$\nL_T = \\frac{1}{2} \\left\\lVert - (I - \\eta K)^T y \\right\\rVert_2^2 = \\frac{1}{2} \\left\\lVert (I - \\eta K)^T y \\right\\rVert_2^2\n$$\nTo analyze this expression, we use the eigendecomposition of the symmetric matrix $K$, given by $K = U \\Lambda U^\\top$. Here, $U$ is an orthonormal matrix whose columns are the eigenvectors $u_1, u_2, \\dots, u_n$, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$.\nThe operator $(I - \\eta K)$ can be diagonalized in the same basis:\n$$\nI - \\eta K = U I U^\\top - \\eta U \\Lambda U^\\top = U (I - \\eta \\Lambda) U^\\top\n$$\nRaising this to the power of $T$:\n$$\n(I - \\eta K)^T = U (I - \\eta \\Lambda)^T U^\\top\n$$\nSubstituting this into the expression for $L_T$:\n$$\nL_T = \\frac{1}{2} \\left\\lVert U (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\nSince $U$ is an orthonormal matrix, it preserves the Euclidean norm, i.e., $\\lVert Uv \\rVert_2 = \\lVert v \\rVert_2$ for any vector $v$. Therefore:\n$$\nL_T = \\frac{1}{2} \\left\\lVert (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\nThe vector $U^\\top y$ represents the coordinates of $y$ in the basis of eigenvectors. Its $i$-th component is $c_i = u_i^\\top y$. The matrix $(I - \\eta \\Lambda)^T$ is diagonal with entries $(1 - \\eta \\lambda_i)^T$. Applying this diagonal matrix to the vector $U^\\top y$ scales its $i$-th component by $(1 - \\eta \\lambda_i)^T$. The squared norm of the resulting vector is the sum of the squares of its components:\n$$\nL_T = \\frac{1}{2} \\sum_{i=1}^n \\left( c_i (1 - \\eta \\lambda_i)^T \\right)^2 = \\frac{1}{2} \\sum_{i=1}^n c_i^2 (1 - \\eta \\lambda_i)^{2T}\n$$\nThis final expression reveals the core principle: the contribution of each eigen-component of the target $y$ to the loss decays exponentially. The rate of decay for the $i$-th component is determined by the factor $(1 - \\eta \\lambda_i)^2$. For a stable learning rate where $0 < \\eta \\lambda_i < 2$, this factor is less than $1$. A larger eigenvalue $\\lambda_i$ results in a smaller factor $(1 - \\eta \\lambda_i)^2$, leading to faster convergence for that component.\n\nNow, we perform the explicit computation for the given problem.\nThe NTK Gram matrix is:\n$$\nK = \\begin{bmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}\n$$\nThe eigenvalues are the roots of the characteristic equation $\\det(K - \\lambda I) = 0$, which are $\\lambda_1 = 2 + \\sqrt{2}$, $\\lambda_2 = 2$, and $\\lambda_3 = 2 - \\sqrt{2}$. The corresponding orthonormal eigenvectors are:\n$$\nu_1 = \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}, \\quad u_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\end{pmatrix}, \\quad u_3 = \\begin{pmatrix} 1/2 \\\\ -\\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}\n$$\nWe now evaluate the loss $L_T$ for each test case.\n\nCase $1$: $y = u_1$, $\\eta = 0.25$, $T = 3$.\nThe target $y$ is aligned with the first eigenvector $u_1$. The coefficients are $c_1 = u_1^\\top y = u_1^\\top u_1 = 1$, $c_2 = 0$, $c_3 = 0$.\nThe loss is determined solely by the first eigen-component:\n$L_3 = \\frac{1}{2} c_1^2 (1 - \\eta \\lambda_1)^{2T} = \\frac{1}{2} (1 - 0.25 (2+\\sqrt{2}))^6 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^6 \\approx 0.000004$.\n\nCase $2$: $y = u_3$, $\\eta = 0.25$, $T = 3$.\nThe target $y$ is aligned with the third eigenvector $u_3$. The coefficients are $c_1 = 0$, $c_2 = 0$, $c_3 = 1$.\nThe loss is determined solely by the third eigen-component:\n$L_3 = \\frac{1}{2} c_3^2 (1 - \\eta \\lambda_3)^{2T} = \\frac{1}{2} (1 - 0.25 (2-\\sqrt{2}))^6 = \\frac{1}{2} (0.5 + \\frac{\\sqrt{2}}{4})^6 \\approx 0.192317$.\nThe convergence is much slower compared to Case $1$ because $\\lambda_3$ is small.\n\nCase $3$: $y = \\mathrm{normalize}(0.8 u_1 + 0.2 u_3)$, $\\eta = 0.25$, $T = 3$.\nLet $v = 0.8 u_1 + 0.2 u_3$. Then $y = v / \\lVert v \\rVert_2$. Since $u_1$ and $u_3$ are orthonormal, $\\lVert v \\rVert_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68}$.\nThe coefficients are $c_1 = u_1^\\top y = 0.8/\\sqrt{0.68}$ and $c_3 = u_3^\\top y = 0.2/\\sqrt{0.68}$, with $c_2 = 0$.\nSo, $c_1^2 = 0.64/0.68 = 16/17$ and $c_3^2 = 0.04/0.68 = 1/17$.\nThe loss is a weighted sum of the contributions from the first and third components:\n$L_3 = \\frac{1}{2} [c_1^2 (1 - \\eta \\lambda_1)^6 + c_3^2 (1 - \\eta \\lambda_3)^6] = \\frac{1}{2} [ \\frac{16}{17} (0.5 - \\frac{\\sqrt{2}}{4})^6 + \\frac{1}{17} (0.5 + \\frac{\\sqrt{2}}{4})^6 ] \\approx 0.011317$.\nThe final error is dominated by the slowly decaying component associated with $u_3$.\n\nCase $4$: $y = u_1$, $\\eta = 0.1$, $T = 3$.\nThis is similar to Case $1$ but with a smaller learning rate. $c_1 = 1$, $c_2 = 0$, $c_3 = 0$.\n$L_3 = \\frac{1}{2} (1 - 0.1 \\lambda_1)^6 = \\frac{1}{2} (1 - 0.1(2+\\sqrt{2}))^6 = \\frac{1}{2} (0.8 - \\frac{\\sqrt{2}}{10})^6 \\approx 0.040727$.\nThe smaller learning rate results in a decay factor $(1 - \\eta \\lambda_1)$ closer to $1$, hence slower convergence compared to Case $1$.\n\nCase $5$: $y = u_1$, $\\eta = 0.25$, $T = 1$.\nThis is similar to Case $1$ but with fewer steps. $c_1 = 1$, $c_2 = 0$, $c_3 = 0$.\n$L_1 = \\frac{1}{2} (1 - \\eta \\lambda_1)^{2 \\times 1} = \\frac{1}{2} (1 - 0.25(2+\\sqrt{2}))^2 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^2 \\approx 0.010723$.\nWith only one step, the error has less time to decay compared to Case $1$.\n\nThe results confirm the theoretical analysis: components of the target aligned with eigenvectors corresponding to larger eigenvalues of the NTK Gram matrix are learned much more rapidly under kernel gradient descent.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training loss for kernel gradient descent under several scenarios.\n    \"\"\"\n    # 1. Define the fixed NTK Gram matrix K.\n    K = np.array([\n        [2.0, 1.0, 0.0],\n        [1.0, 2.0, 1.0],\n        [0.0, 1.0, 2.0]\n    ])\n\n    # 2. Compute the eigendecomposition of K.\n    # np.linalg.eigh returns eigenvalues in ascending order. We sort them descending.\n    eigenvalues, eigenvectors = np.linalg.eigh(K)\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    # lambdas = eigenvalues[sort_indices] # Not used directly in simulation\n    U = eigenvectors[:, sort_indices]\n\n    # Extract the unit eigenvectors u1, u2, u3.\n    u1 = U[:, 0]\n    # u2 = U[:, 1] # Not used in test cases\n    u3 = U[:, 2]\n\n    # 3. Define the test suite.\n    # Each case is a tuple: (y_description, eta, T)\n    # y_description is a tuple: (type, spec)\n    test_cases = [\n        # Case 1: y = u1, eta = 0.25, T = 3\n        (('eigenvector', u1), 0.25, 3),\n        # Case 2: y = u3, eta = 0.25, T = 3\n        (('eigenvector', u3), 0.25, 3),\n        # Case 3: y = normalize(0.8*u1 + 0.2*u3), eta = 0.25, T = 3\n        (('combo', (0.8, u1, 0.2, u3)), 0.25, 3),\n        # Case 4: y = u1, eta = 0.1, T = 3\n        (('eigenvector', u1), 0.1, 3),\n        # Case 5: y = u1, eta = 0.25, T = 1\n        (('eigenvector', u1), 0.25, 1),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        y_desc, eta, T = case\n        y_type, y_spec = y_desc\n\n        # Construct the target vector y for the current case.\n        if y_type == 'eigenvector':\n            y = y_spec\n        elif y_type == 'combo':\n            c1, v1, c2, v2 = y_spec\n            v = c1 * v1 + c2 * v2\n            y = v / np.linalg.norm(v)\n        else:\n            raise ValueError(\"Unknown y_type\")\n        \n        # Initialize predictions f_0 = 0.\n        f = np.zeros_like(y)\n\n        # Run kernel gradient descent for T steps.\n        for _ in range(T):\n            gradient_term = f - y\n            update = K @ gradient_term\n            f = f - eta * update\n        \n        # This is f_T. Now compute the final loss L_T.\n        loss = 0.5 * np.linalg.norm(f - y)**2\n        results.append(loss)\n\n    # 4. Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3159036"}, {"introduction": "Having seen how the NTK's eigenvectors govern learning, we now investigate their structure in a more specific, yet fundamental, scenario. For models with certain symmetries, the eigenfunctions of the NTK are not arbitrary but correspond to well-known mathematical functions. This practice explores the deep connection between the NTK and Fourier analysis, showing that for a model on a periodic domain, the kernel's eigenfunctions are precisely the Fourier modes. By implementing this, you will gain a powerful intuition for how a network's architecture can create an inductive bias towards learning functions of specific frequencies.", "problem": "You are given a one-dimensional real input domain with points sampled on a periodic interval. Consider the Neural Tangent Kernel (NTK) associated with a linear-in-parameters model built from fixed Fourier features. Specifically, let the model be defined by\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0 \\cdot \\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big),\n$$\nwith parameters $\\theta = \\big(a_0, \\{a_k^{(c)}, a_k^{(s)}\\}_{k=1}^M\\big)$ and fixed features\n$$\n\\phi_0(x)=1,\\quad \\phi_k^{(c)}(x)=\\cos(2\\pi k x),\\quad \\phi_k^{(s)}(x)=\\sin(2\\pi k x),\n$$\nfor integer $k \\ge 1$. Here $\\alpha_k \\ge 0$ are nonnegative scalars that set the relative contribution of each frequency. Using the definition of the Neural Tangent Kernel (NTK),\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top} \\nabla_{\\theta} f(x';\\theta_0),\n$$\nshow that for this model the kernel is\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big),\n$$\nwhere all angles are in radians. Consider the discrete set of evenly spaced points on the unit circle,\n$$\nx_n \\;=\\; \\frac{n}{N}, \\quad n \\in \\{0,1,\\dots,N-1\\},\n$$\nand let $K \\in \\mathbb{R}^{N\\times N}$ be the kernel matrix with entries $K_{n,m} = k(x_n,x_m)$. Note that $K$ is real symmetric and depends only on the difference $n-m$ modulo $N$, hence it is circulant.\n\nTasks:\n1) Compute the eigendecomposition $K = U \\Lambda U^{\\top}$ numerically, where $U \\in \\mathbb{R}^{N\\times N}$ is orthonormal and $\\Lambda \\in \\mathbb{R}^{N\\times N}$ is diagonal with nonnegative entries. \n2) Relate the eigenfunctions to Fourier modes: For a circulant matrix, the complex Discrete Fourier Transform (DFT) matrix diagonalizes $K$, and the eigenvalues are given by the DFT of the first row of $K$. Use this fact to produce an analytical reference for the eigenvalues by computing the DFT of the first row. \n3) Validate the numerical eigendecomposition by comparing its eigenvalues to the analytical eigenvalues from the DFT of the first row and report the maximum absolute difference.\n\nYour program must implement the above for the following test suite. For each test case, you are given $N$ and a dictionary of nonzero $\\alpha_k$ values. Any $\\alpha_k$ not listed should be treated as zero. All angles are in radians.\n\nTest suite:\n- Test 1 (single nonzero frequency): $N=8$, $\\{\\alpha_0=0.0,\\;\\alpha_1=2.0\\}$.\n- Test 2 (mixture including the Nyquist term): $N=16$, $\\{\\alpha_0=1.5,\\;\\alpha_1=1.0,\\;\\alpha_2=0.5,\\;\\alpha_8=0.25\\}$.\n- Test 3 (pure constant kernel): $N=10$, $\\{\\alpha_0=3.0\\}$.\n\nYour program should:\n- Construct $K$ using the exact kernel formula above for each test case.\n- Compute the numerical eigenvalues of $K$ via a symmetric eigensolver.\n- Compute the analytical eigenvalues as the complex DFT of the first row of $K$.\n- Compare the sorted lists of numerical and analytical eigenvalues by computing the maximum absolute difference for each test case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the maximum absolute difference (a float) for the corresponding test case, rounded to ten decimal places, in the order of the tests above. For example, an output of the form \"[0.0,0.0,0.0]\" is acceptable if the differences are numerically zero to within rounding. No extra text should be printed.", "solution": "We begin from the definition of the Neural Tangent Kernel (NTK). For a model $f(x;\\theta)$, the NTK at an initialization $\\theta_0$ is\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top}\\nabla_{\\theta} f(x';\\theta_0).\n$$\nWhen the model is linear in its parameters, with a fixed feature map, the gradient with respect to parameters is independent of $\\theta_0$ and equals the feature vector. In the present case, the model is\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0\\,\\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big).\n$$\nThe gradient with respect to parameters is\n$$\n\\nabla_{\\theta} f(x;\\theta) \\;=\\; \\big[\\sqrt{\\alpha_0}\\,\\phi_0(x),\\;\\{\\sqrt{\\alpha_k}\\,\\phi_k^{(c)}(x),\\sqrt{\\alpha_k}\\,\\phi_k^{(s)}(x)\\}_{k=1}^M\\big]^{\\top}.\n$$\nThe NTK is therefore the inner product of these gradients:\n$$\n\\begin{aligned}\nk(x,x') \n&= \\alpha_0 \\,\\phi_0(x)\\phi_0(x') \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\phi_k^{(c)}(x)\\phi_k^{(c)}(x') \\;+\\; \\phi_k^{(s)}(x)\\phi_k^{(s)}(x')\\Big) \\\\\n&= \\alpha_0 \\cdot 1 \\cdot 1 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\cos(2\\pi k x)\\cos(2\\pi k x') \\;+\\; \\sin(2\\pi k x)\\sin(2\\pi k x')\\Big).\n\\end{aligned}\n$$\nUsing the trigonometric identity $\\cos(A)\\cos(B)+\\sin(A)\\sin(B)=\\cos(A-B)$, we obtain\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big).\n$$\nAll angles are in radians. This kernel is translation-invariant on the circle, depending only on the difference $x-x'$ modulo $1$.\n\nNow consider the discrete set of points $x_n = n/N$ for $n\\in\\{0,\\dots,N-1\\}$. Define the kernel matrix $K \\in \\mathbb{R}^{N\\times N}$ by $K_{n,m} = k(x_n,x_m)$. Because $k$ depends only on $x_n - x_m$ modulo $1$, and $x_n - x_m = (n-m)/N$ modulo $1$, $K$ depends only on $(n-m)\\bmod N$. Therefore, $K$ is a real symmetric circulant matrix. A core property of circulant matrices is that they are diagonalized by the complex Discrete Fourier Transform (DFT) matrix. Let $F$ be the $N\\times N$ DFT matrix with entries $F_{r,m} = \\exp\\!\\big(-2\\pi i r m / N\\big)$ for $r,m\\in\\{0,\\dots,N-1\\}$. Then,\n$$\nK \\;=\\; \\frac{1}{N} F^{\\ast} \\,\\mathrm{diag}(\\lambda_0,\\dots,\\lambda_{N-1})\\, F,\n$$\nwhere $(\\lambda_r)_{r=0}^{N-1}$ are the eigenvalues of $K$, and are given by the DFT of the first row of $K$. If the first row is $(c_0,\\dots,c_{N-1})$, then\n$$\n\\lambda_r \\;=\\; \\sum_{m=0}^{N-1} c_m \\, e^{-2\\pi i r m / N}, \\quad r=0,\\dots,N-1.\n$$\nBecause $K$ is real symmetric and $c_m$ is real with $c_m=c_{N-m}$, the eigenvalues $\\lambda_r$ are real and nonnegative. The corresponding eigenvectors are the complex Fourier modes, and in the real eigendecomposition these appear as cosine and sine pairs spanning the same eigenspaces.\n\nAlgorithmic plan:\n1) Construct the kernel matrix $K$ from the given $N$ and $\\{\\alpha_k\\}$ using\n$$\nK_{n,m} \\;=\\; \\alpha_0 \\;+\\; \\sum_{k\\ge 1} \\alpha_k \\,\\cos\\!\\Big(2\\pi k \\,\\frac{n-m}{N}\\Big).\n$$\n2) Compute the numerical eigendecomposition using a symmetric eigensolver to obtain the eigenvalues. Sort them in ascending order.\n3) Compute the analytical eigenvalues by taking the complex DFT of the first row of $K$. Extract the real part (numerical imaginary parts are due to floating-point rounding) and sort in ascending order.\n4) For each test case, report the maximum absolute difference between the sorted numerical eigenvalues and the sorted analytical eigenvalues. A small difference validates both the eigendecomposition and the relation between eigenfunctions and Fourier modes.\n\nTest suite specifics:\n- Test 1: $N=8$, $\\alpha_0=0.0$, $\\alpha_1=2.0$. The kernel is rank-$2$ with nonzero eigenvalues at frequencies $1$ and $7$; each expected to be $N\\alpha_1/2 = 8$.\n- Test 2: $N=16$, $\\alpha_0=1.5$, $\\alpha_1=1.0$, $\\alpha_2=0.5$, $\\alpha_8=0.25$. The expected eigenvalues are at $r=0$ with $N\\alpha_0=24$, at $r=1,15$ with $N\\alpha_1/2=8$, at $r=2,14$ with $N\\alpha_2/2=4$, and at the Nyquist frequency $r=8$ with $N\\alpha_8=4$. All others are zero.\n- Test 3: $N=10$, $\\alpha_0=3.0$. This is a pure constant kernel with a single nonzero eigenvalue $N\\alpha_0=30$ at $r=0$ and zeros elsewhere.\n\nThe program implements these steps and prints a single line containing a list of three floating-point numbers: the maximum absolute eigenvalue differences for Tests 1–3, rounded to ten decimal places. This directly connects the eigendecomposition of the NTK on evenly spaced points to Fourier modes via the circulant structure and the DFT.", "answer": "```python\nimport numpy as np\n\ndef build_kernel_and_first_row(N: int, alpha: dict) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Build the NTK kernel matrix K and its first row c for evenly spaced points x_n = n/N on [0,1).\n    Kernel: k(x, x') = alpha_0 + sum_{k>=1} alpha_k cos(2*pi*k*(x - x')).\n    Angles are in radians.\n\n    Parameters:\n        N: number of evenly spaced points.\n        alpha: dict mapping frequency k to alpha_k (nonnegative). Unspecified k treated as 0.\n\n    Returns:\n        K: NxN kernel matrix.\n        c: length-N first row of K.\n    \"\"\"\n    n = np.arange(N)\n    # Differences tau = n - m mod N; shape (N, N)\n    tau = (n[:, None] - n[None, :]) % N\n    tau = tau.astype(float)\n\n    # Start with alpha_0 term\n    a0 = alpha.get(0, 0.0)\n    K = np.full((N, N), fill_value=a0, dtype=float)\n\n    # Add cosine terms for k >= 1\n    for k, ak in alpha.items():\n        if k == 0 or ak == 0.0:\n            continue\n        # cos(2*pi*k*(n-m)/N)\n        K += ak * np.cos(2.0 * np.pi * k * tau / N)\n\n    # First row c_m = K_{0, m}\n    c = K[0, :].copy()\n    return K, c\n\ndef numerical_sorted_eigenvalues(K: np.ndarray) -> np.ndarray:\n    \"\"\"Compute and return sorted eigenvalues of a real symmetric matrix K.\"\"\"\n    w = np.linalg.eigh(K)[0]\n    return np.sort(w)\n\ndef analytical_sorted_eigenvalues_from_first_row(c: np.ndarray) -> np.ndarray:\n    \"\"\"\n    For a circulant matrix with first row c, eigenvalues are the DFT of c.\n    Return sorted real parts of these eigenvalues.\n    \"\"\"\n    lam = np.fft.fft(c)\n    lam_real = np.real_if_close(lam, tol=1000)  # strip negligible imaginary parts\n    lam_real = lam_real.astype(float)\n    return np.sort(lam_real)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays of same shape.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef run_test_cases():\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        # Test 1: N=8, alpha_0=0.0, alpha_1=2.0\n        {\"N\": 8, \"alpha\": {0: 0.0, 1: 2.0}},\n        # Test 2: N=16, alpha_0=1.5, alpha_1=1.0, alpha_2=0.5, alpha_8=0.25\n        {\"N\": 16, \"alpha\": {0: 1.5, 1: 1.0, 2: 0.5, 8: 0.25}},\n        # Test 3: N=10, alpha_0=3.0\n        {\"N\": 10, \"alpha\": {0: 3.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n        K, c = build_kernel_and_first_row(N, alpha)\n\n        # Numerical eigenvalues (sorted)\n        w_num = numerical_sorted_eigenvalues(K)\n\n        # Analytical eigenvalues via DFT of first row (sorted)\n        w_ana = analytical_sorted_eigenvalues_from_first_row(c)\n\n        # Compute maximum absolute difference\n        diff = max_abs_diff(w_num, w_ana)\n        results.append(diff)\n\n    # Print results as a single line with specified format: list of floats rounded to 10 decimal places.\n    formatted = \"[\" + \",\".join(f\"{val:.10f}\" for val in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    run_test_cases()\n```", "id": "3159099"}, {"introduction": "The NTK theory provides a precise description of neural network training in the infinite-width limit, a regime often called \"lazy training\" where the kernel remains fixed. However, practical networks are finite, and their parameters can undergo significant updates, leading to a phenomenon called \"feature learning\" where the kernel itself evolves. This final practice challenges you to explore the boundary of the NTK approximation by directly comparing the learning trajectory of a finite-width network with that of its idealized, linearized counterpart. This will provide a tangible understanding of when and why a real network's behavior may diverge from the NTK's predictions.", "problem": "You are asked to implement and compare two training dynamics for a two-layer Rectified Linear Unit (ReLU) neural network on a fixed regression task, and to quantify deviations between them. The two dynamics are: (a) full batch gradient descent on the actual finite-width network (allowing features to change), and (b) training the linearized model obtained by a first-order Taylor expansion of the network around its random initialization (which keeps features fixed and corresponds to training under the Neural Tangent Kernel (NTK) approximation). Your goal is to construct a case where the finite-width training deviates from the NTK prediction due to feature learning and to quantify that deviation.\n\nDefinitions and setup:\n- Consider a two-layer fully connected ReLU network with width $w$ and scalar output, defined for input $\\mathbf{x} \\in \\mathbb{R}^d$ by\n$$\nf(\\mathbf{x}; W, \\mathbf{a}) \\;=\\; \\frac{1}{\\sqrt{w}} \\sum_{j=1}^{w} a_j \\, \\max\\{0, \\langle \\mathbf{w}_j, \\mathbf{x} \\rangle\\},\n$$\nwhere $W \\in \\mathbb{R}^{w \\times d}$ has rows $\\mathbf{w}_j \\in \\mathbb{R}^d$, and $\\mathbf{a} \\in \\mathbb{R}^{w}$.\n- Initialize parameters using independent standard normal draws with a fixed random seed $2024$: each $W_{jk} \\sim \\mathcal{N}(0,1)$ and $a_j \\sim \\mathcal{N}(0,1)$.\n- Use the average squared loss\n$$\n\\mathcal{L}(W,\\mathbf{a}) \\;=\\; \\frac{1}{2n} \\sum_{i=1}^{n} \\big(f(\\mathbf{x}_i; W,\\mathbf{a}) - y_i \\big)^2,\n$$\nwhere $(\\mathbf{x}_i,y_i)$ are training samples.\n- Use batch gradient descent with learning rate $\\eta$ and $T$ steps on the full network:\n$$\n(W^{t+1}, \\mathbf{a}^{t+1}) \\;=\\; (W^{t}, \\mathbf{a}^{t}) - \\eta \\, \\nabla \\mathcal{L}(W^{t}, \\mathbf{a}^{t}),\n$$\nstarting from $(W^0,\\mathbf{a}^0)$ given by the initialization.\n\nLinearized (NTK) model:\n- Let $\\boldsymbol{\\theta}$ denote all parameters stacked into a single vector, and let $\\mathbf{f}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^n$ denote the vector of network outputs on the $n$ training inputs. Consider the first-order Taylor expansion around initialization $\\boldsymbol{\\theta}^0$:\n$$\n\\mathbf{f}(\\boldsymbol{\\theta}) \\;\\approx\\; \\mathbf{f}(\\boldsymbol{\\theta}^0) \\;+\\; J_0 \\, (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0),\n$$\nwhere $J_0 \\in \\mathbb{R}^{n \\times p}$ is the Jacobian of training outputs with respect to parameters at initialization and $p$ is the number of parameters. Train this linearized model by batch gradient descent with the same learning rate $\\eta$ and number of steps $T$, using the same average squared loss on the training set. For test inputs, use the corresponding Jacobian at initialization to obtain predictions\n$$\n\\mathbf{f}_{\\text{test}}(\\boldsymbol{\\theta}) \\;\\approx\\; \\mathbf{f}_{\\text{test}}(\\boldsymbol{\\theta}^0) \\;+\\; J^{\\text{test}}_0 \\, (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0).\n$$\n\nData generation:\n- Input dimension is $d = 2$. Generate $n = 12$ training inputs $\\{\\mathbf{x}_i\\}_{i=1}^{n}$ by drawing each component independently from the standard normal distribution using random seed $123$. Generate $m = 12$ test inputs using the same rule with random seed $456$. All random angles for trigonometric functions are understood to be in radians.\n- Define fixed vectors $\\mathbf{u} = (1.0, -0.5)$ and $\\mathbf{v} = (0.3, 1.1)$. Define the target function by\n$$\ny(\\mathbf{x}) \\;=\\; \\sin(\\langle \\mathbf{u}, \\mathbf{x} \\rangle) \\;+\\; 0.5 \\, \\cos(\\langle \\mathbf{v}, \\mathbf{x} \\rangle).\n$$\nCompute $y_i = y(\\mathbf{x}_i)$ for the training set and $y^{\\text{test}}_j = y(\\mathbf{z}_j)$ for the test set, where $\\{\\mathbf{z}_j\\}_{j=1}^{m}$ are test inputs.\n\nDeviation metric:\n- After training both the full network and the linearized model for the same $(w,\\eta,T)$, compute their predictions on the union of the $n$ training and $m$ test inputs. Let $\\widehat{f}_{\\text{full}}$ and $\\widehat{f}_{\\text{lin}}$ denote these predictions. Quantify deviation by the mean squared deviation\n$$\nD \\;=\\; \\frac{1}{n+m} \\sum_{i=1}^{n+m} \\Big(\\widehat{f}_{\\text{full},i} - \\widehat{f}_{\\text{lin},i}\\Big)^2.\n$$\n\nTest suite:\nRun the experiment for the following parameter sets $(w,\\eta,T)$:\n- Case $1$: $(w,\\eta,T) = (10, 0.1, 200)$.\n- Case $2$: $(w,\\eta,T) = (200, 0.1, 200)$.\n- Case $3$: $(w,\\eta,T) = (10, 0.5, 200)$.\n- Case $4$ (boundary condition): $(w,\\eta,T) = (10, 0.1, 0)$.\n\nRequired program behavior:\n- Your program must implement both training procedures exactly as specified, using the same initialization $(W^0,\\mathbf{a}^0)$ for the full and linearized models, and the same learning rate $\\eta$ and steps $T$ per case.\n- Compute the deviation $D$ for each case.\n- Final output format: Print a single line containing a Python-style list of the four deviations, rounded to $6$ decimal places, in the order of the cases above, for example\n\"[0.123456,0.000789,0.234567,0.000000]\".", "solution": "The user's request is a valid and well-defined problem in the field of deep learning theory. It asks for a numerical comparison between the training dynamics of a two-layer ReLU network and its first-order Taylor approximation (the linearized or Neural Tangent Kernel model). The problem is scientifically grounded, formally specified, and computationally feasible. We will proceed with a complete solution.\n\nThe core of the problem is to implement two distinct training procedures for a neural network on a regression task and to quantify the difference in their final predictions.\n\n**1. Models and Setup**\n\nThe network is a two-layer fully-connected ReLU network with width $w$ and scalar output, defined for an input $\\mathbf{x} \\in \\mathbb{R}^d$ as:\n$$\nf(\\mathbf{x}; W, \\mathbf{a}) = \\frac{1}{\\sqrt{w}} \\sum_{j=1}^{w} a_j \\, \\max\\{0, \\langle \\mathbf{w}_j, \\mathbf{x} \\rangle\\}\n$$\nwhere $W \\in \\mathbb{R}^{w \\times d}$ and $\\mathbf{a} \\in \\mathbb{R}^w$ are the parameters. The training objective is to minimize the average squared loss on a training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$:\n$$\n\\mathcal{L}(W, \\mathbf{a}) = \\frac{1}{2n} \\sum_{i=1}^{n} \\big(f(\\mathbf{x}_i; W, \\mathbf{a}) - y_i \\big)^2\n$$\n\n**2. Full Network Gradient Descent Training**\n\nThe first training dynamic is standard batch gradient descent on the parameters $(W, \\mathbf{a})$. The parameters are updated iteratively for $T$ steps with a learning rate $\\eta$:\n$$\n(W^{t+1}, \\mathbf{a}^{t+1}) = (W^{t}, \\mathbf{a}^{t}) - \\eta \\, \\nabla \\mathcal{L}(W^{t}, \\mathbf{a}^{t})\n$$\nThe gradients are computed using the chain rule. Let $r_i = f(\\mathbf{x}_i; W, \\mathbf{a}) - y_i$ be the residual for sample $i$, and let $\\sigma(z)=\\max\\{0,z\\}$ be the ReLU activation function, with its derivative (subgradient) taken as $\\sigma'(z) = \\mathbb{1}_{z > 0}$. The gradients of the loss with respect to the parameters are:\n\n- **Gradient with respect to the output layer weights $\\mathbf{a}$**:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_j} = \\frac{1}{n} \\sum_{i=1}^{n} r_i \\frac{\\partial f(\\mathbf{x}_i)}{\\partial a_j} = \\frac{1}{n \\sqrt{w}} \\sum_{i=1}^{n} r_i \\, \\sigma(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)\n$$\nIn matrix form, if $\\mathbf{f} \\in \\mathbb{R}^n$ is the vector of predictions and $\\mathbf{r} = \\mathbf{f} - \\mathbf{y}$ is the residual vector, and $A \\in \\mathbb{R}^{n \\times w}$ is the matrix of post-activation values with $A_{ij} = \\sigma(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)$, then $\\nabla_{\\mathbf{a}}\\mathcal{L} = \\frac{1}{n \\sqrt{w}} A^T \\mathbf{r}$.\n\n- **Gradient with respect to the input layer weights $W$**:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W_{jk}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i \\frac{\\partial f(\\mathbf{x}_i)}{\\partial W_{jk}} = \\frac{1}{n \\sqrt{w}} \\sum_{i=1}^{n} r_i \\, a_j \\, \\sigma'(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle) x_{ik}\n$$\nIn matrix form, letting $S \\in \\mathbb{R}^{n \\times w}$ be the matrix where $S_{ij} = \\sigma'(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)$, the gradient matrix $\\nabla_W \\mathcal{L} \\in \\mathbb{R}^{w \\times d}$ can be computed by first finding the gradient of the loss with respect to the pre-activations $H_{ij} = \\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle$. This intermediate gradient matrix $G_H \\in \\mathbb{R}^{n \\times w}$ has entries $(G_H)_{ij} = \\frac{\\partial \\mathcal{L}}{\\partial H_{ij}} = \\frac{1}{n} r_i \\frac{a_j}{\\sqrt{w}} S_{ij}$. The final gradient is then $\\nabla_W \\mathcal{L} = G_H^T X$, where $X \\in \\mathbb{R}^{n \\times d}$ is the training data matrix.\n\nThese updates are performed for $T$ steps to obtain the final parameters $(W^T, \\mathbf{a}^T)$, which are used to make predictions $\\widehat{f}_{\\text{full}}$ on the union of training and test data.\n\n**3. Linearized (NTK) Model Training**\n\nThe second dynamic trains a linearized version of the model. The network output function $\\mathbf{f}(\\boldsymbol{\\theta})$ (where $\\boldsymbol{\\theta}$ represents all parameters) is approximated by its first-order Taylor series around the initial parameters $\\boldsymbol{\\theta}^0 = (W^0, \\mathbf{a}^0)$:\n$$\n\\mathbf{f}_{\\text{lin}}(\\boldsymbol{\\theta}) = \\mathbf{f}(\\boldsymbol{\\theta}^0) + J_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0)\n$$\nwhere $J_0 = \\nabla_{\\boldsymbol{\\theta}} \\mathbf{f}(\\boldsymbol{\\theta}^0)$ is the Jacobian of network outputs with respect to parameters, evaluated at initialization. Training this model with gradient descent leads to a simplified dynamic for the predictions themselves. The update for the training set predictions $\\mathbf{f}_{\\text{lin,train}}^t$ is:\n$$\n\\mathbf{f}_{\\text{lin,train}}^{t+1} = \\mathbf{f}_{\\text{lin,train}}^{t} - \\frac{\\eta}{n} K_{\\text{train,train}} (\\mathbf{f}_{\\text{lin,train}}^t - \\mathbf{y}_{\\text{train}})\n$$\nwhere $K_{\\text{train,train}} = J_0^{\\text{train}} (J_0^{\\text{train}})^T$ is the Neural Tangent Kernel (NTK) matrix. The predictions on any other dataset (e.g., the test set) co-evolve as:\n$$\n\\mathbf{f}_{\\text{lin,test}}^{t+1} = \\mathbf{f}_{\\text{lin,test}}^{t} - \\frac{\\eta}{n} K_{\\text{test,train}} (\\mathbf{f}_{\\text{lin,train}}^t - \\mathbf{y}_{\\text{train}})\n$$\nwhere $(K_{\\text{test,train}})_{ij} = \\langle \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{z}_i; \\boldsymbol{\\theta}^0), \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{x}_j; \\boldsymbol{\\theta}^0) \\rangle$.\n\nInstead of forming the potentially large Jacobian matrix, we can directly compute the kernel function $K(\\mathbf{x}, \\mathbf{x}')$ for any two inputs $\\mathbf{x}, \\mathbf{x}'$:\n$$\nK(\\mathbf{x}, \\mathbf{x}') = \\underbrace{\\frac{1}{w} \\sum_{k=1}^{w} \\sigma(\\langle\\mathbf{w}_k^0,\\mathbf{x}\\rangle)\\sigma(\\langle\\mathbf{w}_k^0,\\mathbf{x}'\\rangle)}_{\\text{from } \\nabla_{\\mathbf{a}}} + \\underbrace{\\frac{\\langle\\mathbf{x},\\mathbf{x}'\\rangle}{w} \\sum_{k=1}^{w} (a_k^0)^2 \\mathbb{1}_{\\langle\\mathbf{w}_k^0,\\mathbf{x}\\rangle > 0} \\mathbb{1}_{\\langle\\mathbf{w}_k^0,\\mathbf{x}'\\rangle > 0}}_{\\text{from } \\nabla_W}\n$$\nThis allows for an efficient simulation of the linearized model dynamics by tracking only the prediction vectors.\n\n**4. Deviation and Implementation**\n\nAfter $T$ steps, we obtain the predictions from both models on the combined set of $n+m$ training and test inputs, denoted $\\widehat{\\mathbf{f}}_{\\text{full}}$ and $\\widehat{\\mathbf{f}}_{\\text{lin}}$. The deviation is quantified by the Mean Squared Deviation:\n$$\nD = \\frac{1}{n+m} \\sum_{i=1}^{n+m} \\left( \\widehat{f}_{\\text{full},i} - \\widehat{f}_{\\text{lin},i} \\right)^2\n$$\nThe implementation will follow these steps for each test case $(w, \\eta, T)$:\n1. Generate training and test data according to the specified distributions and seeds.\n2. Initialize network parameters $(W^0, \\mathbf{a}^0)$ with the specified seed.\n3. For the full network, run the gradient descent loop for $T$ steps and compute final predictions.\n4. For the linearized model, compute the necessary kernel matrices using $(W^0, \\mathbf{a}^0)$ and simulate the evolution of predictions for $T$ steps.\n5. Compute and store the deviation $D$.\n\nThe case $T=0$ serves as a sanity check, where both models remain at their initial state, and thus the deviation must be $0$. The other cases explore how network width $w$ and learning rate $\\eta$ affect the validity of the NTK approximation. We expect the deviation to be smaller for larger widths (Case 2 vs. Case 1) and larger for higher learning rates (Case 3 vs. Case 1).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares full network training with linearized (NTK) dynamics \n    for a two-layer ReLU network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (w, eta, T)\n        (10, 0.1, 200),\n        (200, 0.1, 200),\n        (10, 0.5, 200),\n        (10, 0.1, 0),\n    ]\n\n    results = []\n\n    # --- Data Generation ---\n    d = 2\n    n = 12\n    m = 12\n\n    # Generate training data\n    rng_train = np.random.default_rng(123)\n    X_train = rng_train.standard_normal((n, d), dtype=np.float64)\n\n    # Generate test data\n    rng_test = np.random.default_rng(456)\n    X_test = rng_test.standard_normal((m, d), dtype=np.float64)\n\n    # Define target function and compute labels\n    u = np.array([1.0, -0.5], dtype=np.float64)\n    v = np.array([0.3, 1.1], dtype=np.float64)\n\n    def target_function(x_data):\n        return np.sin(x_data @ u) + 0.5 * np.cos(x_data @ v)\n\n    y_train = target_function(X_train)\n    \n    # Combine training and test inputs for final predictions\n    X_all = np.vstack([X_train, X_test])\n\n    for w, eta, T in test_cases:\n        \n        # --- Initialization ---\n        rng_init = np.random.default_rng(2024)\n        W0 = rng_init.standard_normal((w, d), dtype=np.float64)\n        a0 = rng_init.standard_normal(w, dtype=np.float64)\n\n        # --- Full Network Training ---\n        W, a = W0.copy(), a0.copy()\n        for _ in range(T):\n            # Forward pass on training data\n            H_train = X_train @ W.T  # Shape: (n, w)\n            A_train = np.maximum(0, H_train)\n            f_pred = (1 / np.sqrt(w)) * (A_train @ a)\n            residuals = f_pred - y_train\n\n            # Gradient w.r.t. a\n            grad_a = (1 / (n * np.sqrt(w))) * (A_train.T @ residuals)\n\n            # Gradient w.r.t. W\n            S_train = (H_train > 0).astype(np.float64)\n            d_loss_d_h = (1 / (n * np.sqrt(w))) * (residuals.reshape(-1, 1) * S_train) * a.reshape(1, -1)\n            grad_W = d_loss_d_h.T @ X_train\n\n            # Update parameters\n            W -= eta * grad_W\n            a -= eta * grad_a\n\n        # Final predictions from the fully trained network\n        def predict_network(x_data, W_final, a_final):\n            H_final = x_data @ W_final.T\n            A_final = np.maximum(0, H_final)\n            return (1 / np.sqrt(w)) * (A_final @ a_final)\n\n        f_full_final = predict_network(X_all, W, a)\n\n        # --- Linearized Model (NTK) Training ---\n\n        # Kernel computation function\n        def compute_kernel(x1, x2, width, W_init, a_init):\n            # Part 1: Contribution from derivatives w.r.t. 'a'\n            act1 = np.maximum(0, x1 @ W_init.T)\n            act2 = np.maximum(0, x2 @ W_init.T)\n            K1 = (1 / width) * (act1 @ act2.T)\n\n            # Part 2: Contribution from derivatives w.r.t. 'W'\n            dot_prod_xx = x1 @ x2.T\n            S1 = (x1 @ W_init.T > 0).astype(np.float64)\n            S2 = (x2 @ W_init.T > 0).astype(np.float64)\n            a_init_sq = a_init**2\n            K2_scalar_part = (S1 * a_init_sq) @ S2.T\n            K2 = (1 / width) * dot_prod_xx * K2_scalar_part\n\n            return K1 + K2\n\n        K_train_train = compute_kernel(X_train, X_train, w, W0, a0)\n        K_all_train = compute_kernel(X_all, X_train, w, W0, a0)\n\n        # Initial predictions for the linearized model\n        f0_all = predict_network(X_all, W0, a0)\n        \n        # Evolve all predictions using the linearized dynamics\n        f_lin_all = f0_all.copy()\n        \n        # Track training predictions to compute residuals\n        f_lin_train = f0_all[:n].copy()\n        \n        for _ in range(T):\n            r_lin_train = f_lin_train - y_train\n            # Update predictions on all data points\n            f_lin_all -= (eta / n) * (K_all_train @ r_lin_train)\n            # Update local copy of training predictions for the next iteration\n            f_lin_train = f_lin_all[:n]\n\n        f_lin_final = f_lin_all\n\n        # --- Deviation Calculation ---\n        deviation = np.mean((f_full_final - f_lin_final)**2)\n        results.append(deviation)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3159054"}]}