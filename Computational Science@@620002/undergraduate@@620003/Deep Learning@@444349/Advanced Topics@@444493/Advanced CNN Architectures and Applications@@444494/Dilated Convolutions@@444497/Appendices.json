{"hands_on_practices": [{"introduction": "The primary motivation for using dilated convolutions is to efficiently increase a model's receptive field without proportionally increasing computational cost. This exercise provides a foundational understanding of this mechanism by asking you to derive the precise mathematical relationship between the number of layers, kernel size, and a sequence of dilation rates. By completing this practice, you will master the core calculation needed to design architectures capable of capturing long-range dependencies, as demonstrated in a hypothetical genomics scenario where interactions across vast DNA sequences are the target [@problem_id:3116399].", "problem": "A genomics laboratory is designing a one-dimensional dilated convolutional architecture within a Convolutional Neural Network (CNN) to detect promoter-enhancer interactions across kilobase spans in Deoxyribonucleic Acid (DNA) sequences. The input is a single-channel one-dimensional signal of length $N$ (base pairs). The network consists of a stack of $L$ dilated convolutional layers, each with kernel size $k = 5$, stride $s = 1$, zero-padding chosen to preserve the spatial length, and no pooling. The dilation in layer $\\ell$ is $d_{\\ell} = 2^{\\ell}$ for $\\ell = 0, 1, \\dots, L-1$.\n\nStarting from the definition of discrete convolution and the definition of dilation (atrous spacing of kernel taps), derive the receptive field $R_{L}$ at a single output position after $L$ layers in terms of $k$ and $\\{d_{\\ell}\\}$. Then, impose the requirement that the receptive field must be at least a prescribed context length $L_{c} = 10^{4}$ (base pairs) to capture promoter-enhancer interactions at kilobase scales. Determine the minimal integer $L$ that satisfies this requirement. The final answer must be a single integer. No rounding specification is needed beyond exact evaluation.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. The parameters provided are consistent and realistic within the context of applying deep learning models to genomics. The problem asks for a standard derivation of the receptive field for a stack of dilated convolutions, followed by a direct calculation. Therefore, the problem is valid and a solution can be derived.\n\nThe primary task is to determine the receptive field of a one-dimensional convolutional neural network. Let $r_\\ell$ be the receptive field size of an output unit after layer $\\ell$, where the layers are indexed from $\\ell=0, 1, \\dots, L-1$. The input signal itself can be considered to have a receptive field of $r_{-1} = 1$. The problem specifies a constant kernel size $k$, a constant stride $s=1$, and layer-dependent dilation $d_\\ell$.\n\nThe receptive field of a single convolutional layer with kernel size $k$, stride $s=1$, and dilation $d$ expands the receptive field of the previous layer. The recurrence relation for the receptive field size $r_\\ell$ after layer $\\ell$ in terms of the receptive field size $r_{\\ell-1}$ after layer $\\ell-1$ is given by:\n$$r_\\ell = r_{\\ell-1} + (k-1) d_\\ell$$\nThis relation arises because each of the $k$ taps in the kernel of layer $\\ell$ \"sees\" a receptive field of size $r_{\\ell-1}$ from layer $\\ell-1$. The stride of $s=1$ at all previous layers ensures that adjacent units in the output of layer $\\ell-1$ have receptive fields that are shifted by one position in the input. The $k$ taps in layer $\\ell$ are separated by a distance $d_\\ell$. The total span added by the kernel, beyond the receptive field of the first tap, is thus $(k-1)d_\\ell$.\n\nTo find the total receptive field after $L$ layers, which we denote $R_L$ (corresponding to $r_{L-1}$ in our notation), we can unroll the recurrence relation starting from $r_{-1}=1$:\n$$r_{L-1} = r_{L-2} + (k-1)d_{L-1}$$\n$$r_{L-1} = \\left( r_{L-3} + (k-1)d_{L-2} \\right) + (k-1)d_{L-1}$$\n...\n$$r_{L-1} = r_{-1} + \\sum_{i=0}^{L-1} (k-1)d_i$$\nSubstituting $r_{-1}=1$, we obtain the general formula for the receptive field $R_L$ after $L$ layers:\n$$R_L = 1 + (k-1) \\sum_{\\ell=0}^{L-1} d_\\ell$$\nThis completes the first part of the problem: the derivation of the receptive field in terms of $k$ and $\\{d_\\ell\\}$.\n\nNext, we substitute the specific values provided in the problem. The dilation for layer $\\ell$ is given as $d_\\ell = 2^\\ell$. The sum becomes a geometric series:\n$$\\sum_{\\ell=0}^{L-1} d_\\ell = \\sum_{\\ell=0}^{L-1} 2^\\ell = \\frac{2^L - 1}{2-1} = 2^L - 1$$\nSubstituting this result back into the expression for $R_L$, we get the receptive field in terms of $k$ and $L$:\n$$R_L = 1 + (k-1)(2^L - 1)$$\nThe problem provides the kernel size $k=5$ and requires that the receptive field $R_L$ must be at least the context length $L_c = 10^4$. We must find the minimal integer $L$ that satisfies this condition.\nThe inequality is:\n$$R_L \\ge L_c$$\nSubstituting the expressions and values:\n$$1 + (5-1)(2^L - 1) \\ge 10^4$$\n$$1 + 4(2^L - 1) \\ge 10000$$\nWe now solve for $L$:\n$$4(2^L - 1) \\ge 9999$$\n$$2^L - 1 \\ge \\frac{9999}{4}$$\n$$2^L - 1 \\ge 2499.75$$\n$$2^L \\ge 2500.75$$\nTo find the smallest integer $L$ that satisfies this inequality, we can take the base-2 logarithm of both sides:\n$$L \\ge \\log_2(2500.75)$$\nWe can evaluate this logarithm using the change of base formula, $\\log_b(x) = \\frac{\\ln(x)}{\\ln(b)}$:\n$$L \\ge \\frac{\\ln(2500.75)}{\\ln(2)}$$\nNumerically evaluating the right-hand side:\n$$L \\ge \\frac{7.824310...}{0.693147...} \\approx 11.28801...$$\nSince the number of layers $L$ must be an integer, the minimal integer value for $L$ is the smallest integer greater than or equal to $11.28801...$, which is $12$.\n\nWe can verify this result by checking $L=11$ and $L=12$:\nFor $L=11$:\n$$R_{11} = 1 + (4)(2^{11} - 1) = 1 + 4(2048-1) = 1 + 4(2047) = 1 + 8188 = 8189$$\nSince $8189  10000$, $L=11$ is insufficient.\nFor $L=12$:\n$$R_{12} = 1 + (4)(2^{12} - 1) = 1 + 4(4096-1) = 1 + 4(4095) = 1 + 16380 = 16381$$\nSince $16381 \\ge 10000$, $L=12$ is sufficient.\nTherefore, the minimal integer number of layers is $12$.", "answer": "$$ \\boxed{12} $$", "id": "3116399"}, {"introduction": "While convolution is an elegant mathematical operation, its practical implementation requires careful handling of signal boundaries through padding. This problem explores how different padding choices, specifically zero-padding versus reflective padding, can introduce artifacts at the signal edges, an effect often amplified by large dilation rates. By implementing and quantifying the \"border bias\" in a controlled experiment, you will gain practical insight into how these implementation details impact model behavior and why more sophisticated padding schemes can be crucial for robust performance [@problem_id:3116389].", "problem": "You are given a one-dimensional discrete signal model of a Convolutional Neural Network (CNN) layer. Let a signal be a sequence $\\{x[n]\\}_{n=0}^{N-1}$ and a finite impulse response kernel be $\\{w[m]\\}_{m=0}^{K-1}$, with kernel length $K$ and dilation factor $d \\in \\mathbb{Z}_{0}$. The dilated convolution at index $i \\in \\{0,1,\\ldots,N-1\\}$ with \"same\" alignment is defined by linearity and shift-invariance of convolution, combined with sample selection spaced by the dilation, as\n$$\ny[i] \\triangleq \\sum_{m=0}^{K-1} w[m] \\, x\\big(i + m \\cdot d - o\\big),\n$$\nwhere $o \\triangleq \\left\\lfloor \\frac{(K-1)d}{2} \\right\\rfloor$ is the alignment offset. The index $i + m \\cdot d - o$ may fall outside the valid input range $\\{0,\\ldots,N-1\\}$, in which case a boundary extension rule (padding) is required to define $x[\\cdot]$ everywhere.\n\nConsider two padding rules:\n- Zero padding: for any integer $j$, define $x_{\\text{zero}}[j] \\triangleq x[j]$ if $0 \\le j \\le N-1$, and $x_{\\text{zero}}[j] \\triangleq 0$ otherwise.\n- Reflective padding: for any integer $j$, define the reflection mapping $r(j)$ by repeatedly reflecting across the boundaries excluding edge duplication, i.e., \n$$\nr(j) = \n\\begin{cases}\nj,  0 \\le j \\le N-1, \\\\\n-r(j)-1,  j  0, \\\\\n2N - r(j) - 1,  j \\ge N,\n\\end{cases}\n$$\napplied until $0 \\le r(j) \\le N-1$, and set $x_{\\text{refl}}[j] \\triangleq x\\big(r(j)\\big)$.\n\nDefine the \"border index set\" for a given dilation $d$ as the set of indices where the receptive field extends beyond the signal support:\n$$\n\\mathcal{B}(d) \\triangleq \\left\\{ i \\in \\{0,\\ldots,N-1\\} \\;\\middle|\\; \\exists\\, m \\in \\{0,\\ldots,K-1\\} \\text{ such that } i + m \\cdot d - o \\notin \\{0,\\ldots,N-1\\} \\right\\}.\n$$\nFor a constant input $x[n] \\equiv c$ with $c \\in \\mathbb{R}$, define the interior reference output (i.e., the output if the receptive field is fully inside the signal support) as\n$$\ny_{\\text{center}} \\triangleq c \\sum_{m=0}^{K-1} w[m].\n$$\nDefine the average border bias under a padding rule $p \\in \\{\\text{zero}, \\text{refl}\\}$ as\n$$\nb_p(d) \\triangleq \\frac{1}{|\\mathcal{B}(d)|} \\sum_{i \\in \\mathcal{B}(d)} \\left( y_p[i] - y_{\\text{center}} \\right),\n$$\nwhere $y_p[i]$ is computed using $x_p[\\cdot]$ for the selected padding rule.\n\nTask: Implement a complete, runnable program that:\n1. Constructs each test signal as $x[n] \\equiv 1$ (unitless constant one).\n2. Computes $y_{\\text{zero}}[i]$ and $y_{\\text{refl}}[i]$ for all $i \\in \\{0,\\ldots,N-1\\}$ according to the definitions above.\n3. Identifies $\\mathcal{B}(d)$ exactly as specified.\n4. Computes $b_{\\text{zero}}(d)$ and $b_{\\text{refl}}(d)$, and also the difference $b_{\\text{zero}}(d) - b_{\\text{refl}}(d)$, for each test case.\n5. Produces a single line of output containing all results as a comma-separated list enclosed in square brackets, in the order specified below. No units, angles, or percentages are involved in this problem, and all numeric outputs must be real-valued numbers.\n\nUse the following test suite of $(N, K, w, d)$ parameters:\n- Case $1$: $N = 21$, $K = 5$, $w = [0.25, 0.5, 1.0, 0.5, 0.25]$, $d = 1$.\n- Case $2$: $N = 21$, $K = 5$, $w = [0.25, 0.5, 1.0, 0.5, 0.25]$, $d = 2$.\n- Case $3$: $N = 21$, $K = 5$, $w = [0.25, 0.5, 1.0, 0.5, 0.25]$, $d = 3$.\n- Case $4$ (edge case where receptive field significantly exceeds signal length): $N = 5$, $K = 7$, $w = [1, 1, 1, 1, 1, 1, 1]$, $d = 2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case $j \\in \\{1,2,3,4\\}$, include three numbers in sequence: $b_{\\text{zero}}(d_j)$, $b_{\\text{refl}}(d_j)$, and $b_{\\text{zero}}(d_j) - b_{\\text{refl}}(d_j)$.\n- The overall output list must therefore contain $12$ numbers in the order of the cases, for example, $\\big[ \\text{case }1\\text{ triple}, \\text{case }2\\text{ triple}, \\text{case }3\\text{ triple}, \\text{case }4\\text{ triple} \\big]$.", "solution": "The problem has been validated and is determined to be well-posed, scientifically grounded, and internally consistent. We may proceed with a formal solution.\n\nThe core task is to compute the average border bias for two padding schemes, zero padding and reflective padding, under a dilated convolution operation. A critical simplification arises from the specific choice of input signal, $x[n] \\equiv 1$.\n\nLet's begin by analyzing the definitions provided. The dilated convolution at index $i \\in \\{0, 1, \\ldots, N-1\\}$ is given by:\n$$\ny[i] \\triangleq \\sum_{m=0}^{K-1} w[m] \\, x\\big(i + m \\cdot d - o\\big)\n$$\nwith the alignment offset $o \\triangleq \\left\\lfloor \\frac{(K-1)d}{2} \\right\\rfloor$.\nThe interior reference output for a constant signal $x[n] \\equiv c$ is:\n$$\ny_{\\text{center}} \\triangleq c \\sum_{m=0}^{K-1} w[m]\n$$\nThe problem specifies a unit constant signal, so $c=1$. Thus, $y_{\\text{center}} = \\sum_{m=0}^{K-1} w[m]$. The average border bias for a padding rule $p$ is:\n$$\nb_p(d) \\triangleq \\frac{1}{|\\mathcal{B}(d)|} \\sum_{i \\in \\mathcal{B}(d)} \\left( y_p[i] - y_{\\text{center}} \\right)\n$$\n\n**Analysis of Reflective Padding ($b_{\\text{refl}}(d)$)**\n\nThe reflective padding rule maps any integer index $j$ to an index $r(j) \\in \\{0, \\ldots, N-1\\}$. The padded signal value is $x_{\\text{refl}}[j] \\triangleq x[r(j)]$.\nGiven that the input signal is constant, $x[n] = 1$ for all $n \\in \\{0, \\ldots, N-1\\}$, the value at any valid reflected index $r(j)$ will also be $1$. Therefore, for any integer $j$, $x_{\\text{refl}}[j] = x[r(j)] = 1$.\nThe output using reflective padding, $y_{\\text{refl}}[i]$, can be computed for any index $i$:\n$$\ny_{\\text{refl}}[i] = \\sum_{m=0}^{K-1} w[m] \\, x_{\\text{refl}}\\big(i + m \\cdot d - o\\big) = \\sum_{m=0}^{K-1} w[m] \\cdot 1 = \\sum_{m=0}^{K-1} w[m]\n$$\nComparing this to the reference output, we find that $y_{\\text{refl}}[i] = y_{\\text{center}}$ for all $i \\in \\{0, \\ldots, N-1\\}$.\nConsequently, the difference term in the bias calculation is always zero:\n$$y_{\\text{refl}}[i] - y_{\\text{center}} = 0 \\quad \\forall i$$\nThis directly implies that the average border bias for reflective padding is zero for all test cases, provided $|\\mathcal{B}(d)| \\neq 0$:\n$$\nb_{\\text{refl}}(d) = \\frac{1}{|\\mathcal{B}(d)|} \\sum_{i \\in \\mathcal{B}(d)} 0 = 0\n$$\n\n**Analysis of Zero Padding ($b_{\\text{zero}}(d)$)**\n\nFor zero padding, $x_{\\text{zero}}[j] = 1$ if $0 \\le j \\le N-1$ and $x_{\\text{zero}}[j] = 0$ otherwise.\nThe difference term for an index $i$, $(y_{\\text{zero}}[i] - y_{\\text{center}})$, can be expressed as:\n$$\ny_{\\text{zero}}[i] - y_{\\text{center}} = \\sum_{m=0}^{K-1} w[m] \\, x_{\\text{zero}}\\big(i + m \\cdot d - o\\big) - \\sum_{m=0}^{K-1} w[m] \\cdot 1\n$$\n$$\n= \\sum_{m=0}^{K-1} w[m] \\left( x_{\\text{zero}}\\big(i + m \\cdot d - o\\big) - 1 \\right)\n$$\nLet $j(m) = i + m \\cdot d - o$. The term $(x_{\\text{zero}}[j(m)] - 1)$ is $0$ if $j(m)$ is within the signal bounds $[0, N-1]$ (where $x_{\\text{zero}}[j(m)] = 1$), and it is $-1$ if $j(m)$ is out of bounds (where $x_{\\text{zero}}[j(m)] = 0$).\nThus, the sum only includes contributions from kernel taps whose corresponding input samples are out of bounds:\n$$\ny_{\\text{zero}}[i] - y_{\\text{center}} = \\sum_{m \\text{ s.t. } j(m) \\notin [0, N-1]} w[m] \\cdot (-1) = - \\sum_{m \\text{ s.t. } i + m \\cdot d - o \\notin [0, N-1]} w[m]\n$$\nThis provides a direct way to compute the difference for each border index $i \\in \\mathcal{B}(d)$: it is the negative sum of the kernel weights that \"fall off\" the signal edges.\n\n**Algorithm**\n\nThe solution will be implemented by following these steps for each test case $(N, K, w, d)$:\n\n1.  Read the parameters $N$, $K$, $w$, and $d$.\n2.  Calculate the alignment offset $o = \\lfloor \\frac{(K-1)d}{2} \\rfloor$.\n3.  Calculate the reference output $y_{\\text{center}} = \\sum_{m=0}^{K-1} w[m]$.\n4.  Set $b_{\\text{refl}}(d) = 0.0$.\n5.  Identify the border index set $\\mathcal{B}(d)$. An index $i \\in \\{0, \\ldots, N-1\\}$ is in $\\mathcal{B}(d)$ if any part of its receptive field falls outside $[0, N-1]$. This is true if the minimum index accessed, $i-o$, is less than $0$, or the maximum index, $i+(K-1)d-o$, is greater than or equal to $N$.\n6.  Initialize a variable `sum_of_differences_zero` to $0$.\n7.  For each index $i \\in \\mathcal{B}(d)$:\n    a. Calculate the difference $y_{\\text{zero}}[i] - y_{\\text{center}}$ by summing the weights $w[m]$ for which the access index $i + m \\cdot d - o$ is outside $[0, N-1]$, and negating the sum.\n    b. Add this difference to `sum_of_differences_zero`.\n8.  Calculate the bias $b_{\\text{zero}}(d) = \\frac{\\text{sum\\_of\\_differences\\_zero}}{|\\mathcal{B}(d)|}$.\n9.  The final results for the test case are $b_{\\text{zero}}(d)$, $b_{\\text{refl}}(d)$, and the difference $b_{\\text{zero}}(d) - b_{\\text{refl}}(d) = b_{\\text{zero}}(d)$.\n\nThis algorithm will be applied to all four test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the dilated convolution border bias problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        {'N': 21, 'K': 5, 'w': [0.25, 0.5, 1.0, 0.5, 0.25], 'd': 1},\n        {'N': 21, 'K': 5, 'w': [0.25, 0.5, 1.0, 0.5, 0.25], 'd': 2},\n        {'N': 21, 'K': 5, 'w': [0.25, 0.5, 1.0, 0.5, 0.25], 'd': 3},\n        {'N': 5, 'K': 7, 'w': [1.0] * 7, 'd': 2}\n    ]\n\n    def calculate_biases(N, K, w_list, d):\n        \"\"\"\n        Calculates the average border biases for a single test case.\n        \"\"\"\n        w = np.array(w_list, dtype=np.float64)\n        \n        # Calculate alignment offset.\n        o = (K - 1) * d // 2\n\n        # For a constant input signal x[n] = 1, reflective padding always samples the value 1.\n        # This makes the output y_refl[i] equal to the sum of kernel weights for all i.\n        # The reference output y_center is also the sum of kernel weights.\n        # Thus, the difference (y_refl[i] - y_center) is always 0.\n        # Consequently, the average border bias for reflective padding is 0.\n        b_refl = 0.0\n\n        # Identify the border index set B(d).\n        border_indices = set()\n        for i in range(N):\n            # The receptive field for output 'i' spans input indices from i - o to i + (K-1)*d - o.\n            # An index is on the border if any part of its receptive field is out of bounds.\n            rf_min_idx = i - o\n            rf_max_idx = i + (K - 1) * d - o\n            if rf_min_idx  0 or rf_max_idx >= N:\n                border_indices.add(i)\n\n        if not border_indices:\n            # If there are no border indices, the bias is 0.\n            b_zero = 0.0\n        else:\n            sum_of_differences_zero = 0.0\n            for i in border_indices:\n                # The difference (y_zero[i] - y_center) is the negative sum of weights that\n                # correspond to out-of-bounds input samples (padded with 0).\n                lost_weight_sum = 0.0\n                for m in range(K):\n                    j = i + m * d - o\n                    if not (0 = j  N):\n                        lost_weight_sum += w[m]\n                \n                diff = -lost_weight_sum\n                sum_of_differences_zero += diff\n            \n            b_zero = sum_of_differences_zero / len(border_indices)\n\n        b_diff = b_zero - b_refl\n        return b_zero, b_refl, b_diff\n\n    results = []\n    for case in test_cases:\n        b_zero, b_refl, b_diff = calculate_biases(case['N'], case['K'], case['w'], case['d'])\n        results.extend([b_zero, b_refl, b_diff])\n        \n    # Format the final output string exactly as required.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3116389"}, {"introduction": "The flexibility of dilated convolutions can be further enhanced by applying different dilation rates along different axes, creating an anisotropic receptive field. This advanced practice challenges you to implement an anisotropic dilated convolution and apply it to a synthetic edge detection task. Through this exercise, you will investigate how aligning the shape of the receptive field with the underlying structure of the data—in this case, oriented stripes—can dramatically improve performance, providing a powerful intuition for designing specialized and efficient convolutional operators [@problem_id:3116423].", "problem": "You are asked to implement and analyze two-dimensional anisotropic dilated (atrous) convolutions to study how receptive field elongation affects edge detection on oriented textures. The setting is purely mathematical and algorithmic. All images are discrete arrays, and all operations are on finite grids. Angles must be specified in radians. No physical units are involved.\n\nFundamental base to use:\n- The discrete two-dimensional convolution is defined for a discrete image $I:\\mathbb{Z}^2\\to\\mathbb{R}$ and a finite discrete kernel $K:\\{-\\lfloor k_y/2\\rfloor,\\ldots,\\lfloor k_y/2\\rfloor\\}\\times\\{-\\lfloor k_x/2\\rfloor,\\ldots,\\lfloor k_x/2\\rfloor\\}\\to\\mathbb{R}$ by the sum over shifted products of $I$ and $K$.\n- A dilated (atrous) convolution places the kernel taps on a grid with spacing given by dilation factors in each axis, effectively skipping pixels between taps while keeping the kernel weights fixed.\n- The receptive field (RF) of a convolutional operator is the set of input positions that influence a single output position. In anisotropic dilation, the spacing of kernel taps differs along axes, which elongates the receptive field more in one axis than the other.\n\nTask overview:\n1. Construct synthetic oriented stripe textures on a square grid of size $N\\times N$ with $N=64$. Let $(x,y)$ index pixel coordinates with $x\\in\\{0,\\ldots,N-1\\}$ for columns and $y\\in\\{0,\\ldots,N-1\\}$ for rows. Center the coordinates by $x_c=x-\\frac{N}{2}$ and $y_c=y-\\frac{N}{2}$. For an angle $\\theta$ (in radians) and stripe period $p=8$, define the projection $u(x,y)=x_c\\cos\\theta+y_c\\sin\\theta$. Define a binary stripe texture by\n$$\nI(x,y)=\\begin{cases}\n1  \\text{if } \\left\\lfloor \\frac{u(x,y)}{p}\\right\\rfloor \\bmod 2 = 1,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n2. Define the ground-truth edge mask $E_{\\text{gt}}(x,y)$ using $4$-connected adjacency. A pixel $(x,y)$ is a ground-truth edge if at least one of its $4$-neighbors has a different intensity, that is, if $I(x,y)\\neq I(x\\pm1,y)$ or $I(x,y)\\neq I(x,y\\pm1)$ for any neighbor that lies within bounds.\n3. Implement anisotropic dilated convolution with \"same\" output shape. For dilation factors $(d_y,d_x)$ and a kernel $K$ of size $k_y\\times k_x$, the output at location $(y,x)$ is the sum of the kernel weights multiplied by input samples whose positions are spaced by $d_y$ vertically and $d_x$ horizontally. Use zero padding sufficient to maintain \"same\" output shape.\n4. Use the Sobel pair of kernels for horizontal and vertical gradients:\n$$\nK_x=\\begin{bmatrix}-101\\\\-202\\\\-101\\end{bmatrix},\\quad\nK_y=\\begin{bmatrix}-1-2-1\\\\000\\\\121\\end{bmatrix}.\n$$\nApply the anisotropic dilated convolution with $(d_y,d_x)$ to both $K_x$ and $K_y$ and take the gradient magnitude\n$$\nM(x,y)=\\sqrt{\\big( (I*K_x)_{d_y,d_x}(x,y)\\big)^2+\\big( (I*K_y)_{d_y,d_x}(x,y)\\big)^2}.\n$$\n5. Threshold the magnitude map to predict edges. Use a fixed relative threshold $\\tau=0.3$ of the maximum response:\n$$\nE_{\\text{pred}}(x,y)=\\begin{cases}\n1  \\text{if } M(x,y)\\ge \\tau\\cdot \\max_{x',y'} M(x',y'),\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n6. Compute the F1 score (F1) between $E_{\\text{pred}}$ and $E_{\\text{gt}}$, using the standard definitions. Let $T_P$ be the number of true positives, $F_P$ the number of false positives, and $F_N$ the number of false negatives. Define precision $P=T_P/(T_P+F_P)$ (with the convention $P=0$ if the denominator is $0$), recall $R=T_P/(T_P+F_N)$ (with the convention $R=0$ if the denominator is $0$), and\n$$\n\\text{F1}=\\begin{cases}\n\\frac{2PR}{P+R}  \\text{if } P+R0,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n\nTest suite:\nRun the program on the following parameter sets $(\\theta,d_x,d_y)$, where angles are in radians:\n- Case $1$: $\\theta=0.0$, $d_x=3$, $d_y=1$ (receptive field elongated horizontally on vertical stripes).\n- Case $2$: $\\theta=0.0$, $d_x=1$, $d_y=3$ (receptive field elongated vertically on vertical stripes).\n- Case $3$: $\\theta=\\frac{\\pi}{4}$, $d_x=3$, $d_y=1$ (misaligned elongation on diagonal stripes).\n- Case $4$: $\\theta=\\frac{\\pi}{2}$, $d_x=1$, $d_y=3$ (aligned elongation on horizontal stripes).\n- Case $5$: $\\theta=0.0$, $d_x=1$, $d_y=1$ (isotropic baseline).\n- Case $6$: $\\theta=0.0$, $d_x=7$, $d_y=1$ (extreme elongation boundary on vertical stripes).\n\nRequired final output format:\nYour program should produce a single line of output containing the F1 scores for the cases in the order above, each rounded to $4$ decimal places, as a comma-separated list enclosed in square brackets, for example, $\\big[$$0.8123,0.6456,0.5021,0.9000,0.7801,0.4200$$\\big]$.", "solution": "The problem is valid. It is a well-posed, scientifically grounded, and objective algorithmic task within the domain of digital image processing and deep learning. All necessary parameters, definitions, and procedures for a unique solution are provided.\n\nThe objective is to investigate how the elongation and orientation of a convolutional filter's receptive field affect its ability to detect edges in oriented textures. This is achieved by implementing an anisotropic dilated convolution pipeline and evaluating its performance using the F1 score on a set of synthetic textures.\n\nThe solution proceeds through the following steps:\n\n1.  **Synthetic Texture Generation**: For each test case, we first generate a synthetic square image of size $N \\times N$, where $N=64$. The pixel coordinates $(x,y)$ are defined on the grid $\\{0, 1, \\ldots, N-1\\} \\times \\{0, 1, \\ldots, N-1\\}$. These are centered to obtain $(x_c, y_c)$ where $x_c = x - \\frac{N}{2}$ and $y_c = y - \\frac{N}{2}$. A projection coordinate $u(x,y)$ is calculated for a given orientation angle $\\theta$ (in radians) as $u(x,y) = x_c\\cos\\theta + y_c\\sin\\theta$. This projection effectively rotates the coordinate system. A binary stripe texture $I(x,y)$ is then generated based on this projection and a stripe period $p=8$, using the rule:\n    $$\n    I(x,y)=\\begin{cases}\n    1  \\text{if } \\left\\lfloor \\frac{u(x,y)}{p}\\right\\rfloor \\bmod 2 = 1,\\\\\n    0  \\text{otherwise.}\n    \\end{cases}\n    $$\n    This formula creates parallel stripes of constant width $p$, with their orientation determined by the angle $\\theta$.\n\n2.  **Ground-Truth Edge Mask**: The ground-truth edge mask, $E_{\\text{gt}}$, represents the ideal output of an edge detector. A pixel $(x,y)$ is defined as an edge point if its value $I(x,y)$ differs from that of any of its $4$-connected neighbors (up, down, left, right) that are within the image boundaries. This is a standard method for defining edges on a discrete grid. The resulting binary mask $E_{\\text{gt}}$ will contain $1$s at edge locations and $0$s elsewhere.\n\n3.  **Anisotropic Dilated Convolution**: This is the core operation under study. A standard $2D$ convolution is an operation where a kernel (a small matrix of weights) is slid across an input image. A dilated (or atrous) convolution introduces gaps between the kernel's weights. The size of these gaps is controlled by a dilation factor. In an anisotropic dilated convolution, the dilation factors $(d_y, d_x)$ can be different for the vertical and horizontal axes. This modifies the receptive field (RF) of the operation—the region of the input image that affects a single output value. For a $k_y \\times k_x$ kernel, the effective RF size becomes approximately $(k_y-1)d_y+1 \\times (k_x-1)d_x+1$. By choosing $d_y \\ne d_x$, we create an elongated RF.\n    To implement this, we perform a $2D$ cross-correlation (the standard in deep learning libraries) for an input image $I$ and kernel $K$. The output at $(y,x)$ is computed as:\n    $$\n    (I*K)_{d_y,d_x}(y,x) = \\sum_{j=-\\lfloor k_y/2 \\rfloor}^{\\lfloor k_y/2 \\rfloor} \\sum_{i=-\\lfloor k_x/2 \\rfloor}^{\\lfloor k_x/2 \\rfloor} K_{j,i} \\cdot I(y+j\\cdot d_y, x+i\\cdot d_x)\n    $$\n    where $K_{j,i}$ is the kernel weight at relative position $(j,i)$. To ensure the output has the \"same\" shape as the input, the input image $I$ is first padded with zeros. For a $3 \\times 3$ kernel, the required padding is $d_y$ on the top and bottom, and $d_x$ on the left and right.\n\n4.  **Gradient Magnitude Calculation**: To detect edges, we estimate the image gradient. The Sobel operator provides a pair of $3 \\times 3$ kernels, $K_x$ and $K_y$, designed to approximate the partial derivatives with respect to $x$ and $y$.\n    $$\n    K_x=\\begin{bmatrix}-101\\\\-202\\\\-101\\end{bmatrix},\\quad\n    K_y=\\begin{bmatrix}-1-2-1\\\\000\\\\121\\end{bmatrix}\n    $$\n    We apply the anisotropic dilated convolution of the image $I$ with each of these kernels, using the specified dilation factors $(d_y, d_x)$, to obtain the gradient component maps, $G_x = (I*K_x)_{d_y,d_x}$ and $G_y = (I*K_y)_{d_y,d_x}$. The total gradient magnitude $M(x,y)$ at each pixel is then computed as the Euclidean norm of the gradient vector:\n    $$\n    M(x,y)=\\sqrt{G_x(x,y)^2 + G_y(x,y)^2}\n    $$\n\n5.  **Edge Prediction by Thresholding**: The continuous-valued gradient magnitude map $M(x,y)$ is converted into a binary predicted edge mask, $E_{\\text{pred}}$, by applying a threshold. The problem specifies a relative threshold $\\tau=0.3$. The threshold value is $T = \\tau \\cdot \\max_{x',y'} M(x',y')$. The predicted edge mask is then:\n    $$\n    E_{\\text{pred}}(x,y)=\\begin{cases}\n    1  \\text{if } M(x,y)\\ge T,\\\\\n    0  \\text{otherwise.}\n    \\end{cases}\n    $$\n\n6.  **Performance Evaluation**: The quality of the prediction $E_{\\text{pred}}$ is measured by comparing it against the ground truth $E_{\\text{gt}}$ using the F1 score. This requires counting the number of true positives ($T_P$), false positives ($F_P$), and false negatives ($F_N$).\n    -   $T_P = \\sum_{y,x} [E_{\\text{pred}}(y,x) = 1 \\text{ and } E_{\\text{gt}}(y,x) = 1]$\n    -   $F_P = \\sum_{y,x} [E_{\\text{pred}}(y,x) = 1 \\text{ and } E_{\\text{gt}}(y,x) = 0]$\n    -   $F_N = \\sum_{y,x} [E_{\\text{pred}}(y,x) = 0 \\text{ and } E_{\\text{gt}}(y,x) = 1]$\n    From these counts, we calculate precision $P = T_P / (T_P + F_P)$ and recall $R = T_P / (T_P + F_N)$. The F1 score is the harmonic mean of precision and recall:\n    $$\n    \\text{F1}=\\frac{2PR}{P+R}\n    $$\n    with special handling for cases where denominators are zero, as specified. The F1 score provides a balanced measure of performance, penalizing both missed edges (low recall) and spurious detections (low precision).\n\nThis complete pipeline is executed for each parameter set $(\\theta, d_x, d_y)$ in the test suite, yielding an F1 score for each configuration.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes anisotropic dilated convolutions for edge detection.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (theta, d_x, d_y)\n        (0.0, 3, 1),\n        (0.0, 1, 3),\n        (np.pi / 4, 3, 1),\n        (np.pi / 2, 1, 3),\n        (0.0, 1, 1),\n        (0.0, 7, 1),\n    ]\n\n    results = []\n    \n    N = 64\n    p = 8.0\n    tau = 0.3\n    \n    K_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float64)\n    K_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=np.float64)\n\n    def dilated_convolution(image, kernel, d_y, d_x):\n        \"\"\"\n        Computes 2D dilated cross-correlation with zero padding for 'same' output.\n        \"\"\"\n        img_h, img_w = image.shape\n        k_h, k_w = kernel.shape\n        \n        # Center of the 3x3 kernel is at (1,1)\n        k_h_center, k_w_center = k_h // 2, k_w // 2\n        \n        # Required padding on each side to maintain 'same' shape\n        pad_y = k_h_center * d_y\n        pad_x = k_w_center * d_x\n\n        padded_image = np.pad(image, ((pad_y, pad_y), (pad_x, pad_x)), mode='constant', constant_values=0)\n        output = np.zeros_like(image, dtype=np.float64)\n\n        for y in range(img_h):\n            for x in range(img_w):\n                val = 0.0\n                for ky in range(k_h):\n                    for kx in range(k_w):\n                        # Kernel indices relative to center, e.g., -1, 0, 1\n                        j = ky - k_h_center\n                        i = kx - k_w_center\n                        \n                        # Corresponding image coordinates in the padded image\n                        img_y = y + pad_y + j * d_y\n                        img_x = x + pad_x + i * d_x\n                        \n                        val += padded_image[img_y, img_x] * kernel[ky, kx]\n                output[y, x] = val\n        return output\n\n    for case in test_cases:\n        theta, d_x, d_y = case\n\n        # 1. Generate synthetic oriented stripe texture I\n        coords = np.arange(N)\n        x_c = coords - N / 2.0\n        y_c = coords - N / 2.0\n        xx_c, yy_c = np.meshgrid(x_c, y_c)\n        \n        u = xx_c * np.cos(theta) + yy_c * np.sin(theta)\n        I = (np.floor(u / p) % 2 == 1).astype(np.float64)\n\n        # 2. Generate ground-truth edge mask E_gt\n        E_gt = np.zeros_like(I, dtype=int)\n        for y in range(N):\n            for x in range(N):\n                current_val = I[y, x]\n                is_edge = False\n                # Check 4-connected neighbors\n                if y > 0 and I[y - 1, x] != current_val: is_edge = True\n                if y  N - 1 and I[y + 1, x] != current_val: is_edge = True\n                if x > 0 and I[y, x - 1] != current_val: is_edge = True\n                if x  N - 1 and I[y, x + 1] != current_val: is_edge = True\n                if is_edge:\n                    E_gt[y, x] = 1\n        \n        # 4. Apply kernels and compute gradient magnitude M\n        G_x = dilated_convolution(I, K_x, d_y, d_x)\n        G_y = dilated_convolution(I, K_y, d_y, d_x)\n        M = np.sqrt(G_x**2 + G_y**2)\n\n        # 5. Threshold magnitude map to get predicted edges E_pred\n        M_max = np.max(M)\n        if M_max > 0:\n            threshold = tau * M_max\n            E_pred = (M >= threshold).astype(int)\n        else:\n            E_pred = np.zeros_like(M, dtype=int)\n\n        # 6. Compute F1 score\n        TP = np.sum((E_pred == 1)  (E_gt == 1))\n        FP = np.sum((E_pred == 1)  (E_gt == 0))\n        FN = np.sum((E_pred == 0)  (E_gt == 1))\n        \n        # Precision\n        if (TP + FP) == 0:\n            P = 0.0\n        else:\n            P = TP / (TP + FP)\n        \n        # Recall\n        if (TP + FN) == 0:\n            R = 0.0\n        else:\n            R = TP / (TP + FN)\n        \n        # F1 Score\n        if (P + R) == 0:\n            F1 = 0.0\n        else:\n            F1 = (2 * P * R) / (P + R)\n            \n        results.append(round(F1, 4))\n\n    # Format and print the final output as specified\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "3116423"}]}