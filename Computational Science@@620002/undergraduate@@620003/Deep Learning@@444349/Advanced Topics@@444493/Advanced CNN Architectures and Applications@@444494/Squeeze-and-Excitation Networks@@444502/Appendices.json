{"hands_on_practices": [{"introduction": "To truly understand a neural network module, there is no substitute for building it from the ground up. This first exercise guides you through implementing a Squeeze-and-Excitation (SE) block from its core mathematical definitions [@problem_id:3139403]. By coding the squeeze, excitation, and scaling operations yourself, you will gain a concrete understanding of how SE blocks dynamically recalibrate channel-wise feature responses and learn to quantify their computational cost.", "problem": "You are asked to implement a channel-wise Squeeze-and-Excitation (SE) block and to quantify its parameter overhead from first principles. The SE block operates on a three-dimensional input tensor with channel, height, and width axes. The operations you must implement are defined below using only core definitions of linear maps and pointwise nonlinearities.\n\nGiven an input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, define the squeeze operation by the channel-wise global average\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\nStacking $s_c$ yields the descriptor vector $s \\in \\mathbb{R}^{C}$. Define the excitation as a two-layer Multilayer Perceptron (MLP) with reduction ratio $r$, where $r$ is a positive integer that divides $C$. Let $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ denote the Rectified Linear Unit and $\\sigma(u)=\\frac{1}{1+e^{-u}}$ denote the logistic sigmoid. With learned parameters $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$, $b_1 \\in \\mathbb{R}^{C/r}$, $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$, and $b_2 \\in \\mathbb{R}^{C}$, the excitation output $z \\in \\mathbb{R}^{C}$ is\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1), \\qquad z \\;=\\; \\sigma(W_2 t + b_2).\n$$\nFinally, scale the input tensor channel-wise by $z$,\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\nto obtain the SE-augmented tensor $y \\in \\mathbb{R}^{C \\times H \\times W}$.\n\nParameter overhead is defined as the number of additional parameters introduced by the excitation MLP relative to a baseline without the SE block. For two untied linear layers, the weight count is\n$$\n\\#\\text{weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\nand the bias count is\n$$\n\\#\\text{biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\nIn a tied-weights variant where $W_2 = W_1^{\\top}$, the independent weights count reduces to\n$$\n\\#\\text{weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\nImplement the SE block exactly as specified and compute the following outputs for each test case:\n- The sum of the squeezed descriptor, $\\sum_{c=0}^{C-1} s_c$.\n- The sum of all elements of the scaled output, $\\sum_{c,i,j} y_{cij}$, rounded to six decimal places.\n- The untied-weights overhead $\\#\\text{weights}_{\\text{untied}}$.\n- The tied-weights overhead $\\#\\text{weights}_{\\text{tied}}$.\n- The untied-weights-plus-biases overhead $\\#\\text{weights}_{\\text{untied}} + \\#\\text{biases}$.\n\nYour program must use the following test suite. In all cases, $C$ is divisible by $r$ and all definitions below must be implemented exactly.\n\nTest case A (happy path):\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$.\n- Input defined by $x_{cij} = c - i + j$ for $c \\in \\{0,1,2,3\\}$, $i \\in \\{0,1\\}$, $j \\in \\{0,1\\}$.\n- Parameters:\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1  -1  0.5  0 \\\\\n0.25  0.5  -0.5  1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1  0.5 \\\\\n-0.5  0.25 \\\\\n0  1 \\\\\n0.75  -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\nTest case B (boundary on spatial extent, $H W = 1$):\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$.\n- Input defined by $x_{c00} = c + 1$ for $c \\in \\{0,1,\\dots,7\\}$.\n- Parameters defined elementwise by index-based formulas. Let $a$ index rows and $b$ index columns, both zero-based:\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\nTest case C (edge case $r = 1$ with sign-alternating channels):\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$.\n- Input defined by $x_{ci0} = (-1)^c \\cdot (i+1)$ for $c \\in \\{0,1,\\dots,5\\}$ and $i \\in \\{0,1,2\\}$.\n- Parameters:\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\nwhere $I_6$ is the $6 \\times 6$ identity matrix and $\\mathbf{1}_6$ is the $6$-vector of ones.\n\nAngle units and physical units do not apply here. All numeric outputs should be real numbers in standard decimal form.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all three test cases as a list of lists. Each inner list must be ordered as\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\#\\text{weights}_{\\text{untied}},\\; \\#\\text{weights}_{\\text{tied}},\\; \\#\\text{weights}_{\\text{untied}} + \\#\\text{biases} \\right].\n$$\nFor example, the printed structure must look like\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$", "solution": "The problem requires the implementation and analysis of a Squeeze-and-Excitation (SE) block, a common architectural component in deep learning, based on its fundamental mathematical definition. The solution involves a step-by-step execution of the defined operations for three distinct test cases. The core principles are channel-wise feature recalibration through a data-driven mechanism.\n\nThe SE block operates on an input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, where $C$ is the number of channels, and $H$ and $W$ are the spatial height and width, respectively. The process consists of three main stages: Squeeze, Excitation, and Scale.\n\n**1. Squeeze: Global Information Embedding**\nThe first step, \"Squeeze,\" aggregates feature maps across their spatial dimensions ($H \\times W$) to produce a channel descriptor. This is achieved using global average pooling, which calculates the mean value for each channel. The resulting vector $s \\in \\mathbb{R}^{C}$ embeds a global receptive field for each channel. The formula for the $c$-th element of $s$ is:\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\nIn implementation, this corresponds to taking the mean of the input tensor $x$ along its spatial axes (axes $1$ and $2$). The first required output for each test case is the sum of the elements of this squeezed vector, $\\sum_{c=0}^{C-1} s_c$.\n\n**2. Excitation: Adaptive Recalibration**\nThe \"Excitation\" stage generates a set of per-channel modulation weights, or \"activations,\" from the squeezed descriptor $s$. This is accomplished by a small neural network, specifically a two-layer Multilayer Perceptron (MLP). The MLP is structured to first reduce the channel dimensionality and then restore it, creating a computational bottleneck that helps capture non-linear channel interdependencies.\n\nThe MLP consists of:\n- A dimensionality-reduction linear layer with weights $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$ and biases $b_1 \\in \\mathbb{R}^{C/r}$, where $r$ is the reduction ratio.\n- A Rectified Linear Unit ($\\mathrm{ReLU}$) activation function, defined as $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$.\n- A dimensionality-increasing linear layer with weights $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ and biases $b_2 \\in \\mathbb{R}^{C}$.\n- A logistic sigmoid activation function, $\\sigma(u)=\\frac{1}{1+e^{-u}}$, which normalizes the output to the range $(0, 1)$.\n\nThe computation proceeds as follows:\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\nThe resulting vector $z \\in \\mathbb{R}^{C}$ contains the channel-wise scaling factors.\n\n**3. Scale: Feature Recalibration**\nThe final stage applies the learned scaling factors from the excitation step to the original input tensor $x$. Each channel of the input tensor is multiplied by its corresponding scaling factor from the vector $z$. This recalibrates the feature maps, amplifying informative channels and suppressing less useful ones. The output tensor $y \\in \\mathbb{R}^{C \\times H \\times W}$ is given by:\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\nIn implementation, this is achieved by broadcasting the scaling vector $z$ (reshaped to have dimensions $C \\times 1 \\times 1$) over the input tensor $x$. The second required output is the sum of all elements in the final scaled tensor, $\\sum_{c,i,j} y_{cij}$, which is calculated as $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$.\n\n**4. Parameter Overhead Calculation**\nThe problem also requires quantifying the number of additional learnable parameters introduced by the SE block's MLP. The parameters consist of weights and biases from the two linear layers.\n- **Untied Weights:** The total number of weights when $W_1$ and $W_2$ are independent is the sum of the elements in both matrices: $(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$.\n- **Tied Weights:** In a variant where $W_2 = W_1^{\\top}$, the number of independent weights reduces to the size of $W_1$, which is $\\frac{C^2}{r}$.\n- **Biases:** The number of bias parameters is the sum of the elements in $b_1$ and $b_2$, which is $\\frac{C}{r} + C$.\n- **Total Untied Overhead:** The total number of parameters for the untied-weights case is the sum of untied weights and biases: $\\frac{2C^2}{r} + \\frac{C}{r} + C$.\n\nThe implementation will compute these five quantities for each of the three test cases provided, using the specified input tensors and MLP parameters.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            sum_s,\n            round(sum_y, 6),\n            weights_untied,\n            weights_tied,\n            total_untied_with_biases\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A))\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string and join them with commas.\n    # The outer brackets are added by the f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139403"}, {"introduction": "With a working SE block implemented, we can now investigate its design more critically. The original architecture uses a sigmoid function to generate channel weights independently, but what if we enforced competition using a softmax function? This practice [@problem_id:3175710] challenges you to compare these two gating mechanisms, using analytical metrics to reveal fundamental differences in how they allocate attention and rescale feature energy.", "problem": "You must implement two channel-attention mechanisms for squeeze-and-excitation within a convolutional feature map and compute quantitative metrics that reveal their behavioral differences. The mechanisms are: a constrained squeeze-and-excitation gate that lies in the probability simplex via the softmax mapping, and an independent element-wise gate via the logistic sigmoid mapping. The task must be completed by writing a single, self-contained program that produces the specified outputs for the given test suite.\n\nStart from the following base definitions:\n- A convolutional tensor is represented as $X \\in \\mathbb{R}^{C \\times H \\times W}$, where $C$ is the number of channels, $H$ is the height, and $W$ is the width.\n- The global average pooling descriptor is $s \\in \\mathbb{R}^{C}$ with components $s_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c, i, j}$.\n- The squeeze network is a two-layer transformation with a reduction ratio $r = \\max(1, \\lfloor C/2 \\rfloor)$, a rectified linear unit mapping $\\mathrm{ReLU}(x) = \\max(0, x)$, and deterministic weights defined below. Let $z \\in \\mathbb{R}^{C}$ be the pre-gate activation:\n$$\nz = W_2 \\, \\mathrm{ReLU}(W_1 s + b_1) + b_2,\n$$\nwhere $W_1 \\in \\mathbb{R}^{r \\times C}$, $b_1 \\in \\mathbb{R}^{r}$, $W_2 \\in \\mathbb{R}^{C \\times r}$, and $b_2 \\in \\mathbb{R}^{C}$.\n\nDefine the deterministic weights using elementary trigonometric functions so that the program is reproducible without training:\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$,\n$$\n(W_1)_{i, j} = 0.1 \\, \\sin\\big((i)(j)\\big), \\quad (W_2)_{j, i} = 0.1 \\, \\cos\\big((j)(i)\\big).\n$$\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$,\n$$\n(b_1)_i = 0.05 \\, \\cos(i), \\quad (b_2)_j = -0.02 \\, \\sin(j).\n$$\n\nConstruct two channel gates:\n- The constrained gate $g^{\\mathrm{soft}} \\in \\mathbb{R}^{C}$ is computed by the softmax mapping applied to $z$:\n$$\ng^{\\mathrm{soft}}_c = \\frac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)}, \\quad \\text{so } \\sum_{c=1}^{C} g^{\\mathrm{soft}}_c = 1 \\text{ and } g^{\\mathrm{soft}}_c  0.\n$$\n- The independent gate $g^{\\mathrm{sig}} \\in \\mathbb{R}^{C}$ is computed by the logistic sigmoid mapping applied element-wise to $z$:\n$$\ng^{\\mathrm{sig}}_c = \\frac{1}{1 + \\exp(-z_c)}, \\quad \\text{so } 0  g^{\\mathrm{sig}}_c  1 \\text{ independently for each } c.\n$$\n\nApply the gates channel-wise to the original feature map:\n- The reweighted tensors are $Y^{\\mathrm{soft}} \\in \\mathbb{R}^{C \\times H \\times W}$ and $Y^{\\mathrm{sig}} \\in \\mathbb{R}^{C \\times H \\times W}$ with\n$$\nY^{\\mathrm{soft}}_{c, i, j} = g^{\\mathrm{soft}}_c \\, X_{c, i, j}, \\quad Y^{\\mathrm{sig}}_{c, i, j} = g^{\\mathrm{sig}}_c \\, X_{c, i, j}.\n$$\n\nFor each test case, compute the following metrics:\n- $M_1$: A boolean that is $\\mathrm{True}$ if $\\left|\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c - 1\\right| \\leq \\varepsilon$ with tolerance $\\varepsilon = 10^{-12}$, and $\\mathrm{False}$ otherwise.\n- $M_2$: The $\\ell_1$-norm of $g^{\\mathrm{sig}}$, which equals $\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c$.\n- $M_3$: The emphasis ratio for the constrained gate, defined as $\\frac{\\max_c g^{\\mathrm{soft}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c}$.\n- $M_4$: The emphasis ratio for the independent gate, defined as $\\frac{\\max_c g^{\\mathrm{sig}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c}$.\n- $M_5$: The variance of the constrained gate, $\\mathrm{Var}(g^{\\mathrm{soft}})$.\n- $M_6$: The variance of the independent gate, $\\mathrm{Var}(g^{\\mathrm{sig}})$.\n- $M_7$: The energy scaling ratio for the constrained gate, defined as $\\frac{\\|Y^{\\mathrm{soft}}\\|_1}{\\|X\\|_1}$, where $\\|T\\|_1 = \\sum_{c=1}^{C}\\sum_{i=1}^{H}\\sum_{j=1}^{W} |T_{c, i, j}|$. If $\\|X\\|_1 = 0$, define the ratio to be $0$.\n- $M_8$: The energy scaling ratio for the independent gate, defined as $\\frac{\\|Y^{\\mathrm{sig}}\\|_1}{\\|X\\|_1}$, with the same $0$ convention when $\\|X\\|_1 = 0$.\n\nImplement the above for the following test suite. Each test case specifies $(C, H, W)$ and the tensor $X$ explicitly.\n\nTest case $1$ (general mixed values):\n- $C = 4$, $H = 2$, $W = 3$.\n- $X$ channels:\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.5  0.2  -0.1 \\\\ 0.3  -0.2  0.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} -0.4  0.1  0.2 \\\\ -0.1  -0.05  0.3 \\end{bmatrix},\n$$\n$$\nX_{2,:,:} = \\begin{bmatrix} 1.2  0.8  0.5 \\\\ 0.4  0.0  -0.3 \\end{bmatrix}, \\quad\nX_{3,:,:} = \\begin{bmatrix} 0.0  -0.1  -0.2 \\\\ 0.3  0.1  0.0 \\end{bmatrix}.\n$$\n\nTest case $2$ (single-channel boundary):\n- $C = 1$, $H = 3$, $W = 3$.\n- $X$ channel:\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.1  0.2  0.3 \\\\ 0.4  0.5  0.6 \\\\ 0.7  0.8  0.9 \\end{bmatrix}.\n$$\n\nTest case $3$ (zero tensor edge case):\n- $C = 3$, $H = 2$, $W = 2$.\n- $X$ channels:\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}, \\quad\nX_{2,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}.\n$$\n\nTest case $4$ (dominant channel and mixed signs):\n- $C = 5$, $H = 2$, $W = 2$.\n- $X$ channels:\n$$\nX_{0,:,:} = \\begin{bmatrix} 10.0  9.0 \\\\ 8.0  7.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} -5.0  -4.0 \\\\ -3.0  -2.0 \\end{bmatrix},\n$$\n$$\nX_{2,:,:} = \\begin{bmatrix} 0.1  -0.1 \\\\ 0.05  -0.05 \\end{bmatrix}, \\quad\nX_{3,:,:} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  -1.0 \\end{bmatrix}, \\quad\nX_{4,:,:} = \\begin{bmatrix} 0.3  0.2 \\\\ 0.1  0.0 \\end{bmatrix}.\n$$\n\nYour program must compute $(M_1, M_2, M_3, M_4, M_5, M_6, M_7, M_8)$ for each test case and produce a single line of output containing the results flattened across all test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_{32}]$. No physical units or angles are involved, so none are required. The outputs must be of type boolean or floating-point numbers and must be computed exactly as defined.", "solution": "The problem requires the implementation and comparative analysis of two distinct channel-attention mechanisms derived from the Squeeze-and-Excitation (SE) architecture. The first mechanism, a constrained gate $g^{\\mathrm{soft}}$, uses the softmax function to distribute a total attention budget of $1$ across channels, enforcing competition. The second, an independent gate $g^{\\mathrm{sig}}$, uses the element-wise logistic sigmoid function, allowing each channel's attention weight to be determined independently of the others. The process is deterministic, with all network weights and biases defined by trigonometric functions. For each input tensor $X$, we compute a set of eight metrics ($M_1$ through $M_8$) to quantitatively assess the behavioral differences between these two gating strategies.\n\nThe solution is implemented by following a sequence of well-defined computational steps for each test case.\n\n**Step 1: Global Average Pooling**\nThe process begins with the \"squeeze\" operation, which aggregates spatial information across the height $H$ and width $W$ of each channel into a single descriptor value. For an input tensor $X \\in \\mathbb{R}^{C \\times H \\times W}$, we compute a global average pooling descriptor vector $s \\in \\mathbb{R}^{C}$. Each component $s_c$ of this vector represents the average activation for the $c$-th channel:\n$$\ns_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c, i, j} \\quad \\text{for } c \\in \\{1, \\dots, C\\}.\n$$\n\n**Step 2: Squeeze Network Transformation**\nThe descriptor vector $s$ is then processed by a two-layer fully connected network, the \"excitation\" block, to generate channel-specific pre-gate activations. This network consists of a dimensionality-reduction layer followed by a dimensionality-expansion layer.\n\nFirst, the reduction ratio $r$ is determined by the number of channels $C$:\n$$\nr = \\max(1, \\lfloor C/2 \\rfloor).\n$$\nThe network weights $W_1 \\in \\mathbb{R}^{r \\times C}$, $b_1 \\in \\mathbb{R}^{r}$, $W_2 \\in \\mathbb{R}^{C \\times r}$, and $b_2 \\in \\mathbb{R}^{C}$ are defined deterministically. Using $1$-based indexing for matrices and vectors as specified:\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$:\n  $$\n  (W_1)_{i, j} = 0.1 \\, \\sin(ij), \\quad (W_2)_{j, i} = 0.1 \\, \\cos(ji).\n  $$\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$:\n  $$\n  (b_1)_i = 0.05 \\, \\cos(i), \\quad (b_2)_j = -0.02 \\, \\sin(j).\n  $$\nThe pre-gate activation vector $z \\in \\mathbb{R}^{C}$ is then calculated using the Rectified Linear Unit ($\\mathrm{ReLU}$) activation function, defined as $\\mathrm{ReLU}(x) = \\max(0, x)$:\n$$\nz = W_2 \\, \\mathrm{ReLU}(W_1 s + b_1) + b_2.\n$$\n\n**Step 3: Gate Generation**\nFrom the pre-gate activation vector $z$, we compute the two types of channel gates:\n\n- **Constrained Gate ($g^{\\mathrm{soft}}$)**: The softmax function is applied to $z$ to produce a gate vector where the components sum to $1$ and are all positive. This creates a competitive dynamic where increasing the weight of one channel necessarily decreases the weights of others.\n$$\ng^{\\mathrm{soft}}_c = \\frac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)}.\n$$\n- **Independent Gate ($g^{\\mathrm{sig}}$)**: The logistic sigmoid function is applied element-wise to $z$. Each gate weight $g^{\\mathrm{sig}}_c$ is mapped to the range $(0, 1)$ independently.\n$$\ng^{\\mathrm{sig}}_c = \\frac{1}{1 + \\exp(-z_c)}.\n$$\n\n**Step 4: Channel Reweighting and Metric Computation**\nThe computed gates are used to rescale the channels of the original input tensor $X$. The reweighted tensors are $Y^{\\mathrm{soft}}$ and $Y^{\\mathrm{sig}}$:\n$$\nY^{\\mathrm{soft}}_{c, i, j} = g^{\\mathrm{soft}}_c \\, X_{c, i, j}, \\quad Y^{\\mathrm{sig}}_{c, i, j} = g^{\\mathrm{sig}}_c \\, X_{c, i, j}.\n$$\nFinally, the eight analytical metrics are computed:\n\n- $M_1$: A boolean check on the fundamental property of the softmax function, ensuring $\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c$ is numerically close to $1$. The condition is $|\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c - 1| \\leq \\varepsilon$, with tolerance $\\varepsilon = 10^{-12}$.\n\n- $M_2$: The $\\ell_1$-norm of the independent gate, $\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c$. This metric measures the total \"attentional energy\" allocated by the sigmoid gate, which is not constrained to a fixed budget.\n\n- $M_3$: The emphasis ratio for the constrained gate, $\\frac{\\max_c g^{\\mathrm{soft}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c}$. Because the sum of $g^{\\mathrm{soft}}_c$ is $1$, the average is $1/C$, and the ratio simplifies to $C \\cdot \\max_c g^{\\mathrm{soft}}_c$. It measures how much more attention the most important channel receives compared to the average. A higher value indicates a more \"peaked\" or selective attention distribution.\n\n- $M_4$: The emphasis ratio for the independent gate, $\\frac{\\max_c g^{\\mathrm{sig}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c}$. This measures the same concept of selectivity for the sigmoid gate.\n\n- $M_5$: The variance of the constrained gate, $\\mathrm{Var}(g^{\\mathrm{soft}})$, measures the spread of the attention weights. A higher variance indicates that attention is more unevenly distributed across channels.\n\n- $M_6$: The variance of the independent gate, $\\mathrm{Var}(g^{\\mathrm{sig}})$.\n\n- $M_7$: The energy scaling ratio for the constrained gate, $\\frac{\\|Y^{\\mathrm{soft}}\\|_1}{\\|X\\|_1}$, where $\\|T\\|_1 = \\sum_{c,i,j} |T_{c,i,j}|$. Since $g^{\\mathrm{soft}}_c  0$, this simplifies to $\\frac{\\sum_c g^{\\mathrm{soft}}_c \\|X_c\\|_1}{\\sum_c \\|X_c\\|_1}$, which is a weighted average of the channel-wise $\\ell_1$-norms, weighted by the attention scores. It quantifies the overall scaling effect of the gate on the tensor's magnitude. If $\\|X\\|_1=0$, this is defined as $0$.\n\n- $M_8$: The energy scaling ratio for the independent gate, $\\frac{\\|Y^{\\mathrm{sig}}\\|_1}{\\|X\\|_1}$, calculated analogously to $M_7$.\n\nThis complete pipeline is applied to each test case to generate the required numerical results.", "answer": "```python\nimport numpy as np\n\ndef compute_metrics(C, H, W, X):\n    \"\"\"\n    Computes squeeze-and-excitation gates and derived metrics for a given tensor.\n    \"\"\"\n    # Step 1: Global Average Pooling\n    s = np.mean(X, axis=(1, 2)) if H * W > 0 else np.zeros(C)\n\n    # Step 2: Squeeze Network\n    r = max(1, C // 2)\n\n    # Generate deterministic weights (using 1-based indexing from problem)\n    i_vec_r = np.arange(1, r + 1, dtype=float).reshape(-1, 1)\n    j_vec_C = np.arange(1, C + 1, dtype=float).reshape(1, -1)\n\n    W1 = 0.1 * np.sin(i_vec_r * j_vec_C)\n    b1 = 0.05 * np.cos(np.arange(1, r + 1, dtype=float))\n\n    j_vec_C_T = j_vec_C.T\n    i_vec_r_T = i_vec_r.T\n    W2 = 0.1 * np.cos(j_vec_C_T * i_vec_r_T)\n    b2 = -0.02 * np.sin(np.arange(1, C + 1, dtype=float))\n    \n    # Compute pre-gate activation z\n    z = W2 @ np.maximum(0, W1 @ s + b1) + b2\n\n    # Step 3: Gate Generation\n    # Constrained gate (softmax) with numerical stability\n    if z.size > 0:\n        stable_z = z - np.max(z)\n        exp_z = np.exp(stable_z)\n        g_soft = exp_z / np.sum(exp_z)\n    else: # Edge case for C=0, though not in tests\n        g_soft = np.array([])\n\n    # Independent gate (sigmoid)\n    g_sig = 1 / (1 + np.exp(-z))\n\n    # Step 4: Metric Computations\n    # M1: Softmax sum-to-one check\n    M1 = np.abs(np.sum(g_soft) - 1.0) = 1e-12 if g_soft.size > 0 else True\n    \n    # M2: Sigmoid l1-norm\n    M2 = np.sum(g_sig)\n    \n    # M3, M4: Emphasis Ratios\n    # Handle C=1 case where mean=max\n    mean_g_soft = np.mean(g_soft) if g_soft.size > 0 else 0.0\n    M3 = np.max(g_soft) / mean_g_soft if mean_g_soft > 0 else 1.0\n    \n    mean_g_sig = np.mean(g_sig) if g_sig.size > 0 else 0.0\n    M4 = np.max(g_sig) / mean_g_sig if mean_g_sig > 0 else 1.0\n\n    # M5, M6: Variances\n    M5 = np.var(g_soft)\n    M6 = np.var(g_sig)\n\n    # M7, M8: Energy Scaling Ratios\n    l1_norm_X = np.sum(np.abs(X))\n    if l1_norm_X == 0:\n        M7 = 0.0\n        M8 = 0.0\n    else:\n        channel_l1_norms = np.sum(np.abs(X), axis=(1, 2))\n        M7 = np.sum(g_soft * channel_l1_norms) / l1_norm_X\n        M8 = np.sum(g_sig * channel_l1_norms) / l1_norm_X\n\n    return (M1, M2, M3, M4, M5, M6, M7, M8)\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run computations, and print results.\n    \"\"\"\n    test_cases = [\n        (4, 2, 3, np.array([\n            [[0.5, 0.2, -0.1], [0.3, -0.2, 0.0]],\n            [[-0.4, 0.1, 0.2], [-0.1, -0.05, 0.3]],\n            [[1.2, 0.8, 0.5], [0.4, 0.0, -0.3]],\n            [[0.0, -0.1, -0.2], [0.3, 0.1, 0.0]]\n        ])),\n        (1, 3, 3, np.array([\n            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]\n        ])),\n        (3, 2, 2, np.zeros((3, 2, 2))),\n        (5, 2, 2, np.array([\n            [[10.0, 9.0], [8.0, 7.0]],\n            [[-5.0, -4.0], [-3.0, -2.0]],\n            [[0.1, -0.1], [0.05, -0.05]],\n            [[1.0, 0.0], [0.0, -1.0]],\n            [[0.3, 0.2], [0.1, 0.0]]\n        ]))\n    ]\n\n    all_results = []\n    for C, H, W, X in test_cases:\n        metrics = compute_metrics(C, H, W, X)\n        all_results.extend(metrics)\n    \n    # Format according to spec: [val1,val2,...]\n    # Python's str(True) is 'True', which is the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3175710"}, {"introduction": "The ultimate test of a feature enhancement module is its ability to perform in the face of noise and distraction. In this final hands-on practice, you will stage an experiment to see the SE block in action [@problem_id:3175797]. You will train a model on a synthetic dataset containing adversarially designed \"clutter\" channels and test whether the SE mechanism, guided by spectral norm regularization, can learn to suppress this clutter and amplify the true signal.", "problem": "You are asked to implement and study a simplified Squeeze-and-Excitation (SE) mechanism under spectral norm regularization on adversarially crafted clutter channels. Work entirely in vector form with channel-wise features, and assume binary classification.\n\nFundamental base to use:\n- The binary logistic model with cross-entropy loss: for a logit $a$, prediction $\\hat{y} = \\sigma(a)$ where $\\sigma(\\cdot)$ is the logistic sigmoid, and loss $L = -y \\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})$. The derivative with respect to the logit is $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$.\n- Channel-wise SE gating computed via a two-layer bottleneck with Rectified Linear Unit (ReLU) and sigmoid activation.\n- Spectral norm of a matrix $W$ defined as $\\|W\\|_{2} = \\max_{\\|v\\|=1} \\|W v\\|$, which equals the largest singular value. The gradient of the largest singular value with respect to $W$ is given by the outer product of the top left and right singular vectors.\n\nSetting:\n- There are $C$ channels with $C = 8$. An input sample is a vector $x \\in \\mathbb{R}^{C}$ representing channel activations (spatial dimensions are suppressed; squeeze is therefore the identity).\n- The SE block computes a gating vector $g \\in \\mathbb{R}^{C}$ from $x$ using two learned matrices $W_1 \\in \\mathbb{R}^{C/r \\times C}$ and $W_2 \\in \\mathbb{R}^{C \\times C/r}$ with reduction ratio $r \\in \\{2,4\\}$. Compute $z_1 = W_1 x$, $a_1 = \\max(0, z_1)$ elementwise, $z_2 = W_2 a_1$, and $g = \\sigma(z_2)$ elementwise.\n- The modulated feature is $x^{\\mathrm{tilde}} = g \\odot x$, where $\\odot$ denotes elementwise product. The classifier is fixed as a uniform aggregator $a = \\sum_{i=1}^{C} x^{\\mathrm{tilde}}_i$ and $\\hat{y} = \\sigma(a)$.\n\nTraining objective:\n- Minimize the average cross-entropy over the dataset plus a spectral norm regularization penalty on both $W_1$ and $W_2$. The total loss is\n$$\n\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\left( -y_n \\log(\\hat{y}_n) - (1 - y_n)\\log(1 - \\hat{y}_n) \\right) + \\lambda\\left(\\|W_1\\|_2 + \\|W_2\\|_2\\right),\n$$\nwhere $N$ is the number of samples, $y_n \\in \\{0,1\\}$, and $\\lambda  0$ is the spectral regularization strength.\n\nDataset design with adversarial clutter:\n- Let the signal channels be indices $\\{0,1\\}$ and the clutter channels be indices $\\{2,3,4,5,6,7\\}$. For each sample $n$:\n  1. Draw the label $y_n \\in \\{0,1\\}$ uniformly.\n  2. Set the signal channels to $x_{n,i} = s \\cdot \\mu + \\varepsilon_{n,i}$ for $i \\in \\{0,1\\}$, where $s = +1$ if $y_n = 1$ and $s = -1$ if $y_n = 0$, $\\mu \\ge 0$ is the signal magnitude, and $\\varepsilon_{n,i} \\sim \\mathcal{N}(0, \\sigma^2)$ is Gaussian noise with standard deviation $\\sigma$.\n  3. Compute $S_n = \\sum_{i \\in \\{0,1\\}} x_{n,i}$ and define an adversarial sign $c_n = -\\operatorname{sign}(S_n)$ if $S_n \\neq 0$; if $\\mu = 0$ or $S_n = 0$, set $c_n$ to a random $\\pm 1$ independent of $y_n$.\n  4. For each clutter channel $j \\in \\{2,3,4,5,6,7\\}$ set $x_{n,j} = \\gamma \\cdot c_n \\cdot (\\mu + |\\eta_{n,j}|)$ with $\\eta_{n,j} \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\gamma  0$ governing clutter magnitude. This crafts clutter to oppose the signal sum in sign and to have large magnitude.\n\nTraining method:\n- Use batch gradient descent on $W_1$ and $W_2$ only; the classifier is fixed as described. Derive gradients via the chain rule: from the cross-entropy derivative $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$, propagate through $x^{\\mathrm{tilde}} = g \\odot x$, $g = \\sigma(z_2)$, $z_2 = W_2 a_1$, $a_1 = \\max(0, z_1)$, and $z_1 = W_1 x$. For the spectral penalty, approximate the top singular vectors via power iteration and add $\\lambda \\, u_1 v_1^{\\top}$ to $\\frac{\\partial \\mathcal{L}}{\\partial W_1}$ and $\\lambda \\, u_2 v_2^{\\top}$ to $\\frac{\\partial \\mathcal{L}}{\\partial W_2}$, where $u_k, v_k$ are the top left and right singular vectors of $W_k$.\n- Initialize $W_1$ and $W_2$ with small random values. Use a constant learning rate and a fixed number of epochs.\n\nEvaluation and decision rule:\n- After training, compute the average gate per channel $\\bar{g}_i = \\frac{1}{N} \\sum_{n=1}^{N} g_{n,i}$. Let $\\bar{g}_{\\mathrm{sig}}$ be the mean of $\\bar{g}_i$ over the signal channels and $\\bar{g}_{\\mathrm{clutter}}$ be the mean of $\\bar{g}_i$ over the clutter channels.\n- Declare that clutter has been suppressed if $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}}  \\delta$, for a tolerance $\\delta = 0.1$ (dimensionless gates).\n\nAngle and physical units are not involved; all quantities are dimensionless.\n\nTest suite:\nRun your program on the following parameter sets, each producing one boolean according to the decision rule above:\n- Case $1$: $C = 8$, $r = 2$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.3$, $\\gamma = 3.0$, $\\lambda = 0.1$, random seed $0$.\n- Case $2$: $C = 8$, $r = 2$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.3$, $\\gamma = 10.0$, $\\lambda = 0.2$, random seed $1$.\n- Case $3$: $C = 8$, $r = 4$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.5$, $\\gamma = 15.0$, $\\lambda = 0.5$, random seed $2$.\n- Case $4$ (boundary): $C = 8$, $r = 2$, $N = 240$, $\\mu = 0.0$, $\\sigma = 0.5$, $\\gamma = 10.0$, $\\lambda = 0.2$, random seed $3$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is a boolean for the corresponding test case indicating whether the SE block suppressed clutter under spectral norm regularization according to the rule $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}}  \\delta$.", "solution": "This problem sets up an experiment to test if a Squeeze-and-Excitation (SE) block can learn to suppress adversarially designed clutter channels. The solution involves implementing the specified model, training procedure, and evaluation metric.\n\n### 1. Model and Forward Propagation\nThe model consists of a simplified SE block followed by a fixed classifier, operating on vector inputs $x \\in \\mathbb{R}^{C}$.\n\n1.  **SE Block**: The input $x$ is passed through a two-layer fully connected network to compute a gating vector $g$.\n    - The first layer, with weight matrix $W_1 \\in \\mathbb{R}^{C/r \\times C}$, reduces the channel dimension by a ratio $r$. The output is $z_1 = W_1 x$.\n    - A ReLU activation is applied element-wise: $a_1 = \\text{ReLU}(z_1)$.\n    - The second layer, with weight matrix $W_2 \\in \\mathbb{R}^{C \\times C/r}$, projects the vector back to the original dimension: $z_2 = W_2 a_1$.\n    - A sigmoid activation produces the final gating vector $g = \\sigma(z_2)$, with elements $g_i \\in (0, 1)$.\n2.  **Modulation**: The original input $x$ is element-wise re-scaled by the gating vector: $x^{\\mathrm{tilde}} = g \\odot x$.\n3.  **Classifier**: A fixed aggregation layer sums the components of the modulated vector to get the logit $a = \\sum_{i=1}^{C} x^{\\mathrm{tilde}}_i$. The final prediction is $\\hat{y} = \\sigma(a)$.\n\n### 2. Loss Function and Gradient Derivation\nThe total loss $\\mathcal{L}$ is the sum of the average binary cross-entropy and a spectral norm regularization penalty on the weights $W_1$ and $W_2$.\n$$ \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} L_n + \\lambda \\left( \\|W_1\\|_2 + \\|W_2\\|_2 \\right) $$\nThe gradients are computed for a single sample via the chain rule (backpropagation):\n-   The gradient of the cross-entropy loss with respect to the logit is $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$.\n-   This gradient is propagated backward through the network:\n    - $\\nabla_{x^{\\mathrm{tilde}}} L = (\\hat{y} - y) \\mathbf{1}$\n    - $\\nabla_g L = (\\hat{y} - y) x$\n    - $\\nabla_{z_2} L = (\\nabla_g L) \\odot g \\odot (1-g)$\n    - $\\frac{\\partial L}{\\partial W_2} = (\\nabla_{z_2} L) a_1^\\top$\n    - $\\nabla_{a_1} L = W_2^\\top (\\nabla_{z_2} L)$\n    - $\\nabla_{z_1} L = (\\nabla_{a_1} L) \\odot \\mathbb{I}(z_1 > 0)$\n    - $\\frac{\\partial L}{\\partial W_1} = (\\nabla_{z_1} L) x^\\top$\n\nThe total gradient for the cross-entropy term is the average of these sample-wise gradients over the batch.\n\n### 3. Spectral Norm Regularization Gradient\nThe gradient of the spectral norm, $\\|W\\|_2$, is given by $\\frac{\\partial \\|W\\|_2}{\\partial W} = u v^\\top$, where $u$ and $v$ are the left and right singular vectors corresponding to the largest singular value. As specified, these vectors are approximated using the power iteration method. The regularization components are then added to the gradients:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_k} \\mathrel{+}= \\lambda u_k v_k^\\top \\quad \\text{for } k \\in \\{1, 2\\} $$\n\n### 4. Training and Evaluation\nBatch gradient descent is used to update the weights $W_1$ and $W_2$ over a fixed number of epochs. Standard hyperparameters are chosen (e.g., learning rate $\\eta=0.01$, 100 epochs). After training, the average gating value for each channel is computed over the entire dataset:\n$$ \\bar{g}_i = \\frac{1}{N} \\sum_{n=1}^{N} g_{n,i} $$\nThe mean gate value for signal channels ($\\bar{g}_{\\mathrm{sig}}$ for $i \\in \\{0, 1\\}$) and clutter channels ($\\bar{g}_{\\mathrm{clutter}}$ for $i \\in \\{2, \\ldots, 7\\}$) are compared. The test passes if the SE block has learned to prioritize signal over clutter, as determined by the rule $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}} > \\delta$, with tolerance $\\delta = 0.1$.", "answer": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Computes the sigmoid function element-wise.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef relu(z):\n    \"\"\"Computes the Rectified Linear Unit function element-wise.\"\"\"\n    return np.maximum(0, z)\n\ndef power_iteration(W, rng, num_iterations=20):\n    \"\"\"\n    Approximates top left and right singular vectors of a matrix W via power iteration.\n    \n    Args:\n        W (np.ndarray): The matrix.\n        rng (np.random.Generator): Random number generator for initialization.\n        num_iterations (int): Number of iterations to run.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The top left singular vector u and top right singular vector v.\n    \"\"\"\n    m, n = W.shape\n    if n == 0 or m == 0:\n        return np.zeros(m), np.zeros(n)\n        \n    v = rng.normal(size=n)\n    v_norm = np.linalg.norm(v)\n    if v_norm == 0: return np.zeros(m), np.zeros(n)\n    v = v / v_norm\n\n    for _ in range(num_iterations):\n        u_un = W @ v\n        u_norm = np.linalg.norm(u_un)\n        if u_norm == 0: return np.zeros(m), v\n        u = u_un / u_norm\n        \n        v_un = W.T @ u\n        v_norm = np.linalg.norm(v_un)\n        if v_norm == 0: return u, np.zeros(n)\n        v = v_un / v_norm\n    \n    u = W @ v\n    u_norm = np.linalg.norm(u)\n    if u_norm == 0: return np.zeros(m), v\n    u = u / u_norm\n\n    return u, v\n\ndef generate_data(N, C, mu, sigma, gamma, rng):\n    \"\"\"\n    Generates the dataset with adversarial clutter.\n    \"\"\"\n    X = np.zeros((N, C))\n    y = rng.integers(0, 2, size=N)\n\n    for n in range(N):\n        s = 1.0 if y[n] == 1 else -1.0\n        \n        # Signal channels\n        eps = rng.normal(0, sigma, size=2)\n        X[n, 0:2] = s * mu + eps\n        \n        S_n = np.sum(X[n, 0:2])\n        \n        # Adversarial sign\n        if mu == 0 or S_n == 0:\n            c_n = rng.choice([-1.0, 1.0])\n        else:\n            c_n = -np.sign(S_n)\n            \n        # Clutter channels\n        eta = rng.normal(0, sigma, size=C - 2)\n        X[n, 2:] = gamma * c_n * (mu + np.abs(eta))\n        \n    return X, y\n\ndef run_case(C, r, N, mu, sigma, gamma, lambda_reg, seed):\n    \"\"\"\n    Runs a single test case from the problem statement.\n    \"\"\"\n    # Hyperparameters\n    learning_rate = 0.01\n    num_epochs = 100\n    delta = 0.1\n    init_weight_scale = 0.1\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate Data\n    X, y = generate_data(N, C, mu, sigma, gamma, rng)\n\n    # 2. Initialize Weights\n    hidden_dim = C // r\n    W1 = rng.uniform(-init_weight_scale, init_weight_scale, size=(hidden_dim, C))\n    W2 = rng.uniform(-init_weight_scale, init_weight_scale, size=(C, hidden_dim))\n\n    # 3. Training Loop (Batch Gradient Descent)\n    y_reshaped = y.reshape(-1, 1)\n\n    for epoch in range(num_epochs):\n        # -- Forward Pass --\n        z1 = X @ W1.T\n        a1 = relu(z1)\n        z2 = a1 @ W2.T\n        g = sigmoid(z2)\n        x_tilde = g * X\n        a = np.sum(x_tilde, axis=1, keepdims=True)\n        y_hat = sigmoid(a)\n        \n        # -- Backward Pass --\n        # Gradient of cross-entropy loss\n        dL_da = (y_hat - y_reshaped) / N\n        dL_dx_tilde = dL_da\n        dL_dg = dL_dx_tilde * X\n        dL_dz2 = dL_dg * (g * (1 - g))\n        \n        grad_L_W2 = dL_dz2.T @ a1\n        \n        dL_da1 = dL_dz2 @ W2\n        d_relu_dz1 = (z1 > 0).astype(float)\n        dL_dz1 = dL_da1 * d_relu_dz1\n        \n        grad_L_W1 = dL_dz1.T @ X\n        \n        # Gradient of spectral norm regularization\n        u1, v1 = power_iteration(W1, rng)\n        grad_reg_W1 = lambda_reg * np.outer(u1, v1)\n        \n        u2, v2 = power_iteration(W2, rng)\n        grad_reg_W2 = lambda_reg * np.outer(u2, v2)\n\n        # Total gradient\n        grad_W1 = grad_L_W1 + grad_reg_W1\n        grad_W2 = grad_L_W2 + grad_reg_W2\n        \n        # -- Weight Update --\n        W1 -= learning_rate * grad_W1\n        W2 -= learning_rate * grad_W2\n\n    # 4. Evaluation\n    z1_final = X @ W1.T\n    a1_final = relu(z1_final)\n    z2_final = a1_final @ W2.T\n    g_final = sigmoid(z2_final)\n\n    g_bar = np.mean(g_final, axis=0)\n    g_bar_sig = np.mean(g_bar[0:2])\n    g_bar_clutter = np.mean(g_bar[2:])\n    \n    return g_bar_sig - g_bar_clutter > delta\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # C, r, N, mu, sigma, gamma, lambda, seed\n        (8, 2, 240, 1.0, 0.3, 3.0, 0.1, 0),\n        (8, 2, 240, 1.0, 0.3, 10.0, 0.2, 1),\n        (8, 4, 240, 1.0, 0.5, 15.0, 0.5, 2),\n        (8, 2, 240, 0.0, 0.5, 10.0, 0.2, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        C, r, N, mu, sigma, gamma, lambda_reg, seed = case\n        result = run_case(C, r, N, mu, sigma, gamma, lambda_reg, seed)\n        results.append(str(result))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3175797"}]}