{"hands_on_practices": [{"introduction": "The core innovation of Neural Style Transfer is its ability to quantify the \"style\" of an image by examining correlations between feature responses in a deep neural network. This practice [@problem_id:3158617] invites you to build and test this \"style engine\"—the Gram matrix—from scratch. By observing how the Gram matrix representation, $G_l = \\frac{1}{N} F F^\\top$, changes under transformations like rotation and cropping, you will gain a concrete understanding of its invariance properties, which are key to its success in separating style from spatial structure.", "problem": "Consider a synthetic experiment to assess the invariance properties of the Gram-based style representation used in Neural Style Transfer (NST). Let a style image be a real-valued function $s: \\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\} \\rightarrow \\mathbb{R}$ of spatial coordinates $(x, y)$, with $H$ and $W$ specified. A single-layer feature extractor is defined using spatial convolution from Convolutional Neural Networks (CNNs): for a bank of $C$ filters $\\{k_c\\}_{c=1}^C$, each feature map $f_c$ is computed by the two-dimensional convolution $f_c = s \\star k_c$, followed by a Rectified Linear Unit (ReLU) nonlinearity, yielding $a_c(x, y) = \\max(0, f_c(x, y))$. Stack the activations $a_c$ into a tensor $A \\in \\mathbb{R}^{C \\times H \\times W}$.\n\nDefine the layer-$l$ Gram matrix $G_l \\in \\mathbb{R}^{C \\times C}$ as the normalized channel-wise inner products of flattened activations,\n$$\nG_l = \\frac{1}{N} F F^\\top,\n$$\nwhere $F \\in \\mathbb{R}^{C \\times N}$ is obtained by reshaping $A$ so that each row corresponds to one channel flattened over spatial positions, and $N = H \\cdot W$. The Gram matrix $G_l$ is a core component used in Neural Style Transfer to capture second-order statistics of feature activations across spatial positions.\n\nFor a given augmentation $\\mathcal{T}$ applied to the style image $s$, define the augmented image $s_{\\mathcal{T}}$, compute its corresponding Gram matrix $G_l^{(\\mathcal{T})}$ using the same feature extractor, and quantify the change in the Gram-based style capture by the normalized Frobenius difference\n$$\n\\Delta G_l(\\mathcal{T}) = \\frac{\\left\\|G_l^{(\\mathcal{T})} - G_l\\right\\|_F}{\\left\\|G_l\\right\\|_F},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm. The angle unit for rotations must be in degrees.\n\nYour task is to write a complete, runnable program that:\n- Constructs a deterministic synthetic style image $s$ of size $H = 128$ and $W = 128$ defined by a mixture of sinusoidal patterns:\n$$\ns(x,y) = \\sin\\!\\left(2\\pi f_1 \\frac{x}{H}\\right) + \\sin\\!\\left(2\\pi f_2 \\frac{y}{W}\\right) + \\sin\\!\\left(2\\pi f_3 \\left(\\frac{x}{H}\\cos\\alpha + \\frac{y}{W}\\sin\\alpha\\right)\\right),\n$$\nwith $f_1 = 4$, $f_2 = 6$, $f_3 = 5$, and $\\alpha = \\frac{\\pi}{4}$.\n- Builds a filter bank of $C = 6$ filters consisting of $4$ oriented Gabor filters with orientations $\\theta \\in \\{0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ\\}$ and parameters $\\sigma = 2.5$, $\\gamma = 0.5$, and wavelength $\\lambda = 4.0$, one Laplacian-of-Gaussian filter with standard deviation $\\sigma_{\\text{LoG}} = 2.0$, and one Gaussian blur filter with standard deviation $\\sigma_{\\text{G}} = 1.5$. Each filter should be implemented as a real-valued $K \\times K$ kernel with $K = 15$ and used with two-dimensional convolution followed by ReLU as described above.\n- Computes $G_l$ for the original $s$ and, for each augmentation $\\mathcal{T}$ in the test suite below, computes $G_l^{(\\mathcal{T})}$ and $\\Delta G_l(\\mathcal{T})$.\n\nAugmentations $\\mathcal{T}$ to evaluate (test suite; angles in degrees, crop sizes in pixels):\n1. Rotation by $\\theta = 0^\\circ$; no crop.\n2. Rotation by $\\theta = 90^\\circ$; no crop.\n3. Rotation by $\\theta = 180^\\circ$; no crop.\n4. Rotation by $\\theta = 0^\\circ$; centered crop of size $64 \\times 64$.\n5. Rotation by $\\theta = 0^\\circ$; top-left crop of size $64 \\times 64$.\n6. Rotation by $\\theta = 90^\\circ$; centered crop of size $64 \\times 64$.\n\nCropping is defined as selecting a contiguous axis-aligned subarray of the rotated image: for the centered crop, the crop window starts at $(\\lfloor (H - h)/2 \\rfloor, \\lfloor (W - w)/2 \\rfloor)$, and for the top-left crop, at $(0, 0)$, where $(h, w)$ is the crop size.\n\nScientific rationale to ground the problem in first principles:\n- Spatial convolution $f_c = s \\star k_c$ is defined as the sum over spatial products of $s$ and $k_c$ with spatial shifts, which is a fundamental operation in CNNs.\n- The Gram matrix $G_l$ aggregates second-order channel statistics across spatial positions and, with normalization by $N$, is invariant to spatial permutations of positions. Cropping changes the set of positions and thus can alter $G_l$ depending on stationarity of the feature statistics.\n- Rotations by multiples of $90^\\circ$ permute spatial positions but do not permute channels; the impact on $G_l$ depends on how oriented filters respond to rotated content.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_6]$), where each $r_i$ is the floating-point value of $\\Delta G_l(\\mathcal{T}_i)$ for the $i$-th test case in the order listed above. No units are required for $\\Delta G_l(\\mathcal{T})$ values. The program must not read any input and must not access external files or networks. The angle unit to be used is degrees, and all crop dimensions are in pixels.", "solution": "The problem requires an investigation into the invariance properties of the Gram-based style representation used in Neural Style Transfer (NST). This is accomplished through a synthetic experiment involving a procedurally generated image, a bank of fixed filters, and a set of geometric augmentations. The change in the style representation, captured by the Gram matrix, is quantified using a normalized Frobenius distance.\n\nThe solution proceeds in a structured manner: first, we define and implement all necessary components, including the synthetic image, the filter bank, and the procedure for computing the Gram matrix. Second, we apply the specified augmentations to the image and compute the resulting deviation in the Gram matrix.\n\n### 1. Synthetic Style Image Generation\nThe style image $s(x,y)$ is defined on a discrete grid of size $H \\times W$, where $H=128$ and $W=128$. Its values are determined by a superposition of three sinusoidal waves:\n$$\ns(x,y) = \\sin\\!\\left(2\\pi f_1 \\frac{x}{H}\\right) + \\sin\\!\\left(2\\pi f_2 \\frac{y}{W}\\right) + \\sin\\!\\left(2\\pi f_3 \\left(\\frac{x}{H}\\cos\\alpha + \\frac{y}{W}\\sin\\alpha\\right)\\right)\n$$\nThe parameters are given as $f_1 = 4$, $f_2 = 6$, $f_3 = 5$, and $\\alpha = \\frac{\\pi}{4}$. The first two terms represent vertical and horizontal sinusoidal patterns, while the third term represents a pattern oriented at an angle $\\alpha$. This construction provides a deterministic and structured input with known frequency and orientation content. We generate this image by creating two-dimensional coordinate grids for $x \\in \\{0, \\dots, W-1\\}$ and $y \\in \\{0, \\dots, H-1\\}$ and evaluating the function at each point.\n\n### 2. Feature Extractor and Filter Bank\nThe feature extractor is modeled as a single convolutional layer followed by a Rectified Linear Unit (ReLU) nonlinearity. The layer consists of a bank of $C=6$ filters, each of size $K \\times K$ with $K=15$. The filters are designed to be sensitive to different types of local image structures.\n\n- **Gabor Filters ($4$ filters)**: These are biologically-inspired filters that model the receptive fields of simple cells in the visual cortex. They are effective at detecting edges and textures at specific orientations. A real-valued 2D Gabor filter kernel is defined as:\n$$\ng(x,y) = \\exp\\left(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\right) \\cos\\left(2\\pi \\frac{x'}{\\lambda}\\right)\n$$\nwhere $x' = x \\cos\\theta + y \\sin\\theta$ and $y' = -x \\sin\\theta + y \\cos\\theta$. The parameters are provided: orientations $\\theta \\in \\{0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ\\}$, wavelength $\\lambda=4.0$, standard deviation $\\sigma=2.5$, and aspect ratio $\\gamma=0.5$. To ensure these filters act as band-pass filters without affecting the mean brightness, their kernels are normalized to have a zero mean.\n\n- **Laplacian-of-Gaussian (LoG) Filter ($1$ filter)**: This filter is an effective blob detector, finding regions of uniform intensity surrounded by different intensities. It is computed by applying the Laplacian operator to a Gaussian function:\n$$\n\\text{LoG}(x,y) = -\\frac{1}{\\pi\\sigma^4}\\left(1 - \\frac{x^2+y^2}{2\\sigma^2}\\right)e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n$$\nwith $\\sigma_{\\text{LoG}} = 2.0$. This kernel is inherently zero-sum.\n\n- **Gaussian Blur Filter ($1$ filter)**: This is a low-pass filter that smooths the image. The kernel is:\n$$\n\\text{G}(x,y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n$$\nwith $\\sigma_{\\text{G}} = 1.5$. To preserve image brightness, this kernel is normalized to sum to $1$.\n\n### 3. Feature Map and Gram Matrix Computation\nFor each filter $k_c$ in the bank, a feature map $a_c$ is generated by:\n1.  **Convolution**: $f_c = s \\star k_c$. This is a 2D convolution performed with `same` padding to ensure the output feature map has the same spatial dimensions as the input image. A symmetric boundary condition is used to handle edge effects.\n2.  **ReLU Activation**: $a_c(x,y) = \\max(0, f_c(x,y))$. This introduces nonlinearity, a key component of deep neural networks.\n\nThe resulting activation maps $\\{a_c\\}_{c=1}^C$ form a tensor $A \\in \\mathbb{R}^{C \\times H' \\times W'}$, where $(H', W')$ are the spatial dimensions of the maps. These maps are flattened into a matrix $F \\in \\mathbb{R}^{C \\times N}$, where $N = H' \\cdot W'$. Each row of $F$ corresponds to the flattened activations of a single channel.\n\nThe Gram matrix $G_l \\in \\mathbb{R}^{C \\times C}$ is then computed as the normalized outer product of this matrix with itself:\n$$\nG_l = \\frac{1}{N} F F^\\top\n$$\nEach element $(G_l)_{ij}$ of the Gram matrix represents the correlation between the feature responses of channels $i$ and $j$, averaged over all spatial positions. It serves as a statistical summary of the image's style.\n\n### 4. Augmentations and Difference Calculation\nThe core of the experiment is to evaluate how the Gram matrix changes when the input image is subjected to geometric augmentations $\\mathcal{T}$. For each augmentation, an augmented image $s_{\\mathcal{T}}$ is created, and its corresponding Gram matrix $G_l^{(\\mathcal{T})}$ is computed. The augmentations are:\n- **Rotation**: The image is rotated by a specified angle $\\theta \\in \\{0^\\circ, 90^\\circ, 180^\\circ\\}$. The rotation is performed such that the image dimensions are preserved, with areas outside the original image boundaries filled with zero.\n- **Cropping**: A sub-array of the image is selected. A centered crop takes a patch from the middle, while a top-left crop takes a patch from the corner. Cropping changes the spatial domain and thus the normalization factor $N = H' \\cdot W'$.\n\nThe change in style representation is quantified by the normalized Frobenius difference:\n$$\n\\Delta G_l(\\mathcal{T}) = \\frac{\\left\\|G_l^{(\\mathcal{T})} - G_l\\right\\|_F}{\\left\\|G_l\\right\\|_F}\n$$\nwhere $\\|M\\|_F = \\sqrt{\\sum_{i,j} M_{ij}^2}$ is the Frobenius norm. This metric gives a scalar measure of the relative difference between the original and augmented Gram matrices.\n\n### 5. Execution of the Test Suite\nThe program systematically evaluates the six test cases specified. For each case, it applies the corresponding rotation and/or crop to obtain $s_{\\mathcal{T}}$, computes $G_l^{(\\mathcal{T})}$, and then calculates $\\Delta G_l(\\mathcal{T})$ relative to the Gram matrix $G_l$ of the original, un-augmented image.\n\n- Test case 1 (rotation $0^\\circ$, no crop) serves as a control, as $s_{\\mathcal{T}} = s$ and thus $\\Delta G_l$ must be $0$.\n- Rotations by $90^\\circ$ and $180^\\circ$ are expected to yield non-zero differences because the oriented Gabor filters will respond differently to the rotated patterns.\n- Cropping is expected to yield non-zero differences because the statistical sample of features changes, and the synthetic image content is not spatially stationary.\n\nThe final output is a list of the computed $\\Delta G_l(\\mathcal{T})$ values for each test case, formatted as required.", "answer": "```python\nimport numpy as np\nfrom scipy import signal, ndimage\n\ndef create_style_image(H, W, f1, f2, f3, alpha):\n    \"\"\"\n    Constructs the synthetic style image from a mixture of sinusoidal patterns.\n    \"\"\"\n    y_coords, x_coords = np.mgrid[0:H, 0:W]\n    term1 = np.sin(2 * np.pi * f1 * x_coords / W)\n    term2 = np.sin(2 * np.pi * f2 * y_coords / H)\n    term3 = np.sin(2 * np.pi * f3 * (x_coords / W * np.cos(alpha) + y_coords / H * np.sin(alpha)))\n    return term1 + term2 + term3\n\ndef create_gabor_filter(K, sigma, theta_deg, lam, gamma):\n    \"\"\"\n    Creates a real-valued, zero-mean Gabor filter kernel.\n    \"\"\"\n    theta_rad = np.deg2rad(theta_deg)\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    x_theta = x * np.cos(theta_rad) + y * np.sin(theta_rad)\n    y_theta = -x * np.sin(theta_rad) + y * np.cos(theta_rad)\n    \n    gb = np.exp(-(x_theta**2 + gamma**2 * y_theta**2) / (2 * sigma**2)) * np.cos(2 * np.pi * x_theta / lam)\n    \n    # Make the filter zero-mean\n    return gb - np.mean(gb)\n\ndef create_log_filter(K, sigma):\n    \"\"\"\n    Creates a Laplacian-of-Gaussian (LoG) filter kernel.\n    \"\"\"\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    # Unnormalized LoG\n    norm_sq = x**2 + y**2\n    factor1 = (norm_sq - 2 * sigma**2) / (sigma**4)\n    factor2 = np.exp(-norm_sq / (2 * sigma**2))\n    log_kernel = factor1 * factor2\n    \n    # Constant scaling factor is not critical as Gram matrix is correlation-based\n    # and we normalize the final difference. \n    # Let's make it zero-mean for numerical stability.\n    return log_kernel - log_kernel.mean()\n\ndef create_gaussian_filter(K, sigma):\n    \"\"\"\n    Creates a Gaussian blur filter kernel, normalized to sum to 1.\n    \"\"\"\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    g = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n    return g / np.sum(g)\n\ndef get_gram_matrix(image, filters):\n    \"\"\"\n    Computes the Gram matrix for an image given a bank of filters.\n    \"\"\"\n    activations = []\n    for k in filters:\n        # Convolve using 'same' padding to maintain dimensions\n        conv_result = signal.convolve2d(image, k, mode='same', boundary='symm')\n        # Apply ReLU nonlinearity\n        relu_result = np.maximum(0, conv_result)\n        activations.append(relu_result)\n    \n    A = np.array(activations)\n    C, H_prime, W_prime = A.shape\n    N = H_prime * W_prime\n    \n    F = A.reshape(C, N)\n    \n    if N == 0:\n        return np.zeros((C,C))\n\n    G = (1 / N) * (F @ F.T)\n    return G\n\ndef solve():\n    # Problem parameters\n    H, W = 128, 128\n    F1, F2, F3 = 4, 6, 5\n    ALPHA = np.pi / 4\n    K = 15\n    \n    GABOR_SIGMA, GABOR_GAMMA, GABOR_LAM = 2.5, 0.5, 4.0\n    GABOR_THETAS = [0, 45, 90, 135]\n    LOG_SIGMA = 2.0\n    GAUSS_SIGMA = 1.5\n    \n    # Test suite definition\n    test_cases = [\n        {'rot': 0, 'crop': None, 'crop_size': None},\n        {'rot': 90, 'crop': None, 'crop_size': None},\n        {'rot': 180, 'crop': None, 'crop_size': None},\n        {'rot': 0, 'crop': 'center', 'crop_size': (64, 64)},\n        {'rot': 0, 'crop': 'top_left', 'crop_size': (64, 64)},\n        {'rot': 90, 'crop': 'center', 'crop_size': (64, 64)},\n    ]\n\n    # 1. Generate original style image\n    s_orig = create_style_image(H, W, F1, F2, F3, ALPHA)\n    \n    # 2. Build filter bank\n    filters = []\n    for theta in GABOR_THETAS:\n        filters.append(create_gabor_filter(K, GABOR_SIGMA, theta, GABOR_LAM, GABOR_GAMMA))\n    filters.append(create_log_filter(K, LOG_SIGMA))\n    filters.append(create_gaussian_filter(K, GAUSS_SIGMA))\n    \n    # 3. Compute Gram matrix for the original image\n    G_orig = get_gram_matrix(s_orig, filters)\n    norm_G_orig = np.linalg.norm(G_orig, 'fro')\n    \n    results = []\n    for case in test_cases:\n        s_aug = s_orig.copy()\n        \n        # Apply rotation\n        if case['rot'] != 0:\n            s_aug = ndimage.rotate(s_aug, case['rot'], reshape=False, mode='constant', cval=0.0)\n            \n        # Apply cropping\n        if case['crop'] is not None:\n            h_crop, w_crop = case['crop_size']\n            h_img, w_img = s_aug.shape\n            \n            if case['crop'] == 'center':\n                y_start = (h_img - h_crop) // 2\n                x_start = (w_img - w_crop) // 2\n                s_aug = s_aug[y_start : y_start + h_crop, x_start : x_start + w_crop]\n            elif case['crop'] == 'top_left':\n                s_aug = s_aug[0:h_crop, 0:w_crop]\n\n        # Compute Gram matrix for the augmented image\n        G_aug = get_gram_matrix(s_aug, filters)\n        \n        # Compute normalized Frobenius difference\n        diff_norm = np.linalg.norm(G_aug - G_orig, 'fro')\n        \n        if norm_G_orig == 0:\n            delta_G = 0.0 if diff_norm == 0.0 else np.inf\n        else:\n            delta_G = diff_norm / norm_G_orig\n            \n        results.append(delta_G)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158617"}, {"introduction": "Once we can represent style and content, the style transfer process becomes an optimization problem: create a new image that minimizes a combined loss for content and style. This exercise [@problem_id:3158635] provides a hands-on coding challenge to implement this core optimization process using gradient descent. By working in a simplified feature space, you will focus on the interplay between the content loss, $\\mathcal{L}_c$, and style loss, $\\mathcal{L}_s$, and investigate a critical scenario where the style's \"vocabulary\" is insufficient for the content, leading to feature collapse.", "problem": "You will study a simplified, purely mathematical model of Neural Style Transfer in feature space to probe what happens when the style signal lacks a key texture present in the content. Consider a single layer with channel and spatial dimensions represented by matrices. Let the content feature map be denoted by $\\phi(c) \\in \\mathbb{R}^{C \\times N}$ and the style feature map by $\\phi(s) \\in \\mathbb{R}^{C \\times N}$, where $C$ is the number of channels and $N$ is the number of spatial positions (for example, $N$ can be the number of pixels across all spatial locations in a feature map). The stylized feature to be optimized is $\\Phi \\in \\mathbb{R}^{C \\times N}$. The style Gram matrix is defined as $G_{s} = \\frac{1}{N} \\phi(s)\\phi(s)^{\\top} \\in \\mathbb{R}^{C \\times C}$. The objective is to minimize the following energy:\n$$\n\\mathcal{L}(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2} + \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2},\n$$\nwhere $\\alpha \\ge 0$ and $\\beta \\ge 0$ are weights, and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. You will work directly in the feature space, optimizing $\\Phi$ by gradient descent starting from $\\Phi^{(0)} = \\phi(c)$ with a fixed step size $\\eta > 0$ and a fixed number of steps $T$.\n\nFundamental base for derivation and implementation:\n- Use the definition of the Frobenius norm and its standard variational properties.\n- Use the Gram matrix definition $G = \\frac{1}{N}XX^{\\top}$ for a feature matrix $X$.\n- Use basic rules of matrix calculus with the trace and inner product identities.\n\nYour tasks:\n- Starting from the objective definition and standard rules of matrix calculus, derive the gradient of $\\mathcal{L}(\\Phi)$ with respect to $\\Phi$ and design a gradient descent algorithm to compute an approximate minimizer $\\Phi^{*}$.\n- Construct a style that lacks a key texture present in the content by zeroing out the corresponding channel in $\\phi(s)$, and test whether the optimized $\\Phi^{*}$ collapses that channel’s energy relative to $\\phi(c)$. Define the energy ratio for any channel index $i$ as $r_{i} = \\frac{\\|\\Phi^{*}_{i,:}\\|_{2}}{\\|\\phi(c)_{i,:}\\|_{2}}$, where $\\|\\cdot\\|_{2}$ is the Euclidean norm applied to the row vector. For a set of channel indices $S$, define the average ratio $R(S)$ as the arithmetic mean of $r_{i}$ over $i \\in S$.\n- Let the “missing” set be $M = \\{ i \\mid \\|\\phi(s)_{i,:}\\|_{2} = 0 \\}$ and the “present” set be $P = \\{0,\\ldots,C-1\\}\\setminus M$. By convention, if $M$ is empty, set $R(M) = 1.0$, and if $P$ is empty, set $R(P) = 1.0$.\n- Use a collapse detection threshold $\\tau = 0.6$: declare collapse if $R(M) \\le \\tau$. In addition to the ratios, report the final objective value $\\mathcal{L}(\\Phi^{*})$.\n\nImplementation details you must follow:\n- Use gradient descent with a fixed step size $\\eta$ and a fixed number of steps $T$, starting from $\\Phi^{(0)} = \\phi(c)$.\n- Build $\\phi(c)$ and $\\phi(s)$ as rank-$1$ matrices with constant columns: let $v_{c} \\in \\mathbb{R}^{C}$ and $v_{s} \\in \\mathbb{R}^{C}$ be channel energy vectors, and set $\\phi(c) = v_{c}\\mathbf{1}^{\\top}$ and $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the all-ones vector.\n\nTest suite:\n- Use $C = 3$ and $N = 4$ for all test cases, so $\\phi(c), \\phi(s) \\in \\mathbb{R}^{3 \\times 4}$. Let $\\mathbf{1} \\in \\mathbb{R}^{4}$.\n- In all cases below, construct $\\phi(c) = v_{c}\\mathbf{1}^{\\top}$ and $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$ from the specified $v_{c}$ and $v_{s}$.\n- Use the following four cases spanning a happy path, a missing-channel style, and boundary conditions on $\\alpha$ and $\\beta$:\n  - Case A (happy path, matching style): $v_{c} = [2.0, 1.0, 0.5]$, $v_{s} = [2.0, 1.0, 0.5]$, $\\alpha = 1.0$, $\\beta = 3.0$, $\\eta = 0.05$, $T = 400$.\n  - Case B (style lacks key texture in channel $0$): $v_{c} = [2.0, 1.0, 0.5]$, $v_{s} = [0.0, 1.0, 0.5]$, $\\alpha = 1.0$, $\\beta = 3.0$, $\\eta = 0.05$, $T = 400$.\n  - Case C (no style term): $v_{c} = [2.0, 1.0, 0.5]$, $v_{s} = [0.0, 1.0, 0.5]$, $\\alpha = 1.0$, $\\beta = 0.0$, $\\eta = 0.05$, $T = 400$.\n  - Case D (style only, content weight zero, style lacks channel $0$): $v_{c} = [2.0, 1.0, 0.5]$, $v_{s} = [0.0, 1.0, 0.5]$, $\\alpha = 0.0$, $\\beta = 3.0$, $\\eta = 0.05$, $T = 400$.\n\nFor each case, compute:\n- $R(M)$ as the mean ratio over missing channels (or $1.0$ if $M$ is empty).\n- $R(P)$ as the mean ratio over present channels (or $1.0$ if $P$ is empty).\n- The final value $\\mathcal{L}(\\Phi^{*})$.\n- A boolean “collapse detected” given by $R(M) \\le \\tau$ with $\\tau = 0.6$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Each test case result must be a list of the form $[R(M), R(P), \\mathcal{L}(\\Phi^{*}), \\text{collapse}]$ where $R(M)$ and $R(P)$ are rounded to $6$ decimal places, $\\mathcal{L}(\\Phi^{*})$ is rounded to $6$ decimal places, and $\\text{collapse}$ is a boolean.\n- The final printed line therefore has the structure $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$ with no extra whitespace or text.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a self-contained, mathematically precise framework for studying a simplified model of neural style transfer. All parameters and conditions are specified, allowing for a unique and verifiable computational solution.\n\nThe solution process involves two main stages: first, deriving the gradient of the objective function $\\mathcal{L}(\\Phi)$, and second, implementing a gradient descent algorithm to find the optimized feature map $\\Phi^{*}$ for each specified test case.\n\nThe objective function is given by:\n$$\n\\mathcal{L}(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2} + \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2}\n$$\nThis can be written as the sum of a content loss $\\mathcal{L}_c(\\Phi)$ and a style loss $\\mathcal{L}_s(\\Phi)$:\n$$\n\\mathcal{L}_c(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2}\n$$\n$$\n\\mathcal{L}_s(\\Phi) = \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2}\n$$\nThe gradient of $\\mathcal{L}(\\Phi)$ with respect to $\\Phi$ is the sum of the gradients of its constituent parts:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Phi} = \\frac{\\partial \\mathcal{L}_c}{\\partial \\Phi} + \\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi}\n$$\n\nWe will now derive each gradient term using the definition of the Frobenius norm, $\\|\\mathbf{A}\\|_{F}^2 = \\operatorname{tr}(\\mathbf{A}^{\\top}\\mathbf{A})$, and standard rules of matrix calculus.\n\n**1. Gradient of the Content Loss ($\\mathcal{L}_c$)**\n\nThe content loss is $\\mathcal{L}_c(\\Phi) = \\alpha \\operatorname{tr}\\left((\\Phi - \\phi(c))^{\\top}(\\Phi - \\phi(c))\\right)$.\nUsing the identity $\\frac{\\partial}{\\partial \\mathbf{X}} \\operatorname{tr}((\\mathbf{X}-\\mathbf{A})^{\\top}(\\mathbf{X}-\\mathbf{A})) = 2(\\mathbf{X}-\\mathbf{A})$, we can directly find the gradient.\nLet $\\mathbf{X} = \\Phi$ and $\\mathbf{A} = \\phi(c)$. Then, the gradient of the content loss with respect to $\\Phi$ is:\n$$\n\\frac{\\partial \\mathcal{L}_c}{\\partial \\Phi} = 2\\alpha(\\Phi - \\phi(c))\n$$\n\n**2. Gradient of the Style Loss ($\\mathcal{L}_s$)**\n\nThe style loss is $\\mathcal{L}_s(\\Phi) = \\beta \\left\\| \\mathbf{G}_{\\Phi} - G_{s} \\right\\|_{F}^{2}$, where $\\mathbf{G}_{\\Phi} = \\frac{1}{N}\\Phi\\Phi^{\\top}$ is the Gram matrix of the feature map $\\Phi$.\nWe can write this as $\\mathcal{L}_s(\\Phi) = \\beta \\operatorname{tr}\\left((\\mathbf{G}_{\\Phi} - G_s)^{\\top}(\\mathbf{G}_{\\Phi} - G_s)\\right)$.\nSince $\\mathbf{G}_{\\Phi}$ and $G_s$ are symmetric, $(\\mathbf{G}_{\\Phi} - G_s)^{\\top} = \\mathbf{G}_{\\Phi} - G_s$. Thus, $\\mathcal{L}_s(\\Phi) = \\beta \\operatorname{tr}\\left((\\mathbf{G}_{\\Phi} - G_s)^2\\right)$.\n\nTo find the gradient, we use differentials. Let $\\mathbf{E} = \\mathbf{G}_{\\Phi} - G_s$.\nThe differential of $\\mathcal{L}_s$ is $d\\mathcal{L}_s = \\beta \\cdot d(\\operatorname{tr}(\\mathbf{E}^{\\top}\\mathbf{E})) = \\beta \\cdot \\operatorname{tr}(d(\\mathbf{E}^{\\top})\\mathbf{E} + \\mathbf{E}^{\\top}d\\mathbf{E})$. Since $\\mathbf{E}$ is symmetric, this simplifies to $d\\mathcal{L}_s = 2\\beta \\operatorname{tr}(\\mathbf{E}^{\\top}d\\mathbf{E})$.\nThe differential of $\\mathbf{E}$ is $d\\mathbf{E} = d\\mathbf{G}_{\\Phi} = d\\left(\\frac{1}{N}\\Phi\\Phi^{\\top}\\right) = \\frac{1}{N}(d\\Phi \\cdot \\Phi^{\\top} + \\Phi \\cdot d\\Phi^{\\top})$.\nSubstituting $d\\mathbf{E}$ into the expression for $d\\mathcal{L}_s$:\n$$\nd\\mathcal{L}_s = 2\\beta \\operatorname{tr}\\left( \\mathbf{E}^{\\top} \\frac{1}{N}(d\\Phi \\cdot \\Phi^{\\top} + \\Phi \\cdot d\\Phi^{\\top}) \\right) = \\frac{2\\beta}{N} \\left( \\operatorname{tr}(\\mathbf{E}^{\\top} d\\Phi \\Phi^{\\top}) + \\operatorname{tr}(\\mathbf{E}^{\\top} \\Phi d\\Phi^{\\top}) \\right)\n$$\nUsing the cyclic property of the trace, $\\operatorname{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = \\operatorname{tr}(\\mathbf{C}\\mathbf{A}\\mathbf{B})$, and the property $\\operatorname{tr}(\\mathbf{A}) = \\operatorname{tr}(\\mathbf{A}^{\\top})$:\nThe first term is $\\operatorname{tr}(\\mathbf{E}^{\\top} d\\Phi \\Phi^{\\top}) = \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}^{\\top}d\\Phi)$.\nThe second term is $\\operatorname{tr}(\\mathbf{E}^{\\top} \\Phi d\\Phi^{\\top}) = \\operatorname{tr}(d\\Phi^{\\top}\\mathbf{E}^{\\top}\\Phi) = \\operatorname{tr}((\\mathbf{E}^{\\top}\\Phi)^{\\top}d\\Phi) = \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi)$.\nSince $\\mathbf{E}$ is symmetric, $\\mathbf{E}^{\\top}=\\mathbf{E}$. Both terms are identical.\n$$\nd\\mathcal{L}_s = \\frac{2\\beta}{N} \\left( \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi) + \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi) \\right) = \\frac{4\\beta}{N} \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi)\n$$\nBy the identity $df = \\operatorname{tr}(\\mathbf{A}^{\\top}d\\mathbf{X}) \\implies \\frac{\\partial f}{\\partial \\mathbf{X}} = \\mathbf{A}$, we identify the gradient:\n$$\n\\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi} = \\left(\\frac{4\\beta}{N} \\Phi^{\\top}\\mathbf{E}\\right)^{\\top} = \\frac{4\\beta}{N} \\mathbf{E}^{\\top}\\Phi = \\frac{4\\beta}{N} \\mathbf{E}\\Phi\n$$\nSubstituting $\\mathbf{E} = \\frac{1}{N}\\Phi\\Phi^{\\top} - G_s$, we get:\n$$\n\\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi} = \\frac{4\\beta}{N} \\left(\\frac{1}{N}\\Phi\\Phi^{\\top} - G_s\\right) \\Phi\n$$\n\n**3. Total Gradient and Gradient Descent Algorithm**\n\nCombining the gradients of the content and style losses, the total gradient of $\\mathcal{L}(\\Phi)$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Phi} = 2\\alpha (\\Phi - \\phi(c)) + \\frac{4\\beta}{N} \\left( \\frac{1}{N}\\Phi\\Phi^{\\top} - G_s \\right) \\Phi\n$$\nThe gradient descent algorithm iteratively updates $\\Phi$ to minimize $\\mathcal{L}(\\Phi)$. Starting from $\\Phi^{(0)} = \\phi(c)$, the update rule at each step $t$ is:\n$$\n\\Phi^{(t+1)} = \\Phi^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\Phi}\\bigg|_{\\Phi=\\Phi^{(t)}}\n$$\nThis process is repeated for a fixed number of steps, $T$. The final optimized feature map is $\\Phi^{*} = \\Phi^{(T)}$.\n\nFor the specific implementation where $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$, the style Gram matrix simplifies to a rank-1 matrix:\n$$\nG_s = \\frac{1}{N}\\phi(s)\\phi(s)^{\\top} = \\frac{1}{N}(v_{s}\\mathbf{1}^{\\top})(\\mathbf{1}v_{s}^{\\top}) = \\frac{1}{N}v_{s}(\\mathbf{1}^{\\top}\\mathbf{1})v_{s}^{\\top} = \\frac{1}{N}v_{s}(N)v_{s}^{\\top} = v_{s}v_{s}^{\\top}\n$$\nThis simplification is used in the implementation. After $T$ iterations, we compute the required metrics: $R(M)$, $R(P)$, $\\mathcal{L}(\\Phi^{*})$, and the collapse detection boolean.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Happy path, matching style\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([2.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n        # Case B: Style lacks key texture in channel 0\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n        # Case C: No style term\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 0.0, 'eta': 0.05, 'T': 400},\n        # Case D: Style only, content weight zero, style lacks channel 0\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 0.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Format the final output string as per problem specification.\n    # [[R(M),R(P),L,collapse],[...],...]\n    formatted_results = []\n    for r in results:\n        rm, rp, l_val, c_val = r\n        s = f\"[{rm:.6f},{rp:.6f},{l_val:.6f},{str(c_val).lower()}]\"\n        formatted_results.append(s)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_simulation(vc, vs, alpha, beta, eta, T):\n    \"\"\"\n    Performs the gradient descent optimization and calculates final metrics for one test case.\n    \"\"\"\n    C, N = 3, 4\n    tau = 0.6\n    \n    # Construct feature maps and style Gram matrix\n    ones_N = np.ones((1, N))\n    phi_c = vc.reshape(C, 1) @ ones_N\n    \n    # G_s simplifies to the outer product v_s * v_s^T\n    G_s = vs.reshape(C, 1) @ vs.reshape(1, C)\n\n    # Initialize Phi at the content feature map\n    Phi = phi_c.copy()\n    \n    # Gradient Descent Loop\n    for _ in range(T):\n        # Calculate Gram matrix of the current Phi\n        G_Phi = (1/N) * (Phi @ Phi.T)\n        \n        # Calculate content loss gradient\n        grad_content = 2 * alpha * (Phi - phi_c)\n        \n        # Calculate style loss gradient\n        grad_style = (4 * beta / N) * ((G_Phi - G_s) @ Phi)\n        \n        # Total gradient\n        grad_L = grad_content + grad_style\n        \n        # Update Phi\n        Phi -= eta * grad_L\n\n    Phi_star = Phi\n\n    # Calculate final objective value L(Phi*)\n    G_Phi_star = (1/N) * (Phi_star @ Phi_star.T)\n    loss_content_final = alpha * np.linalg.norm(Phi_star - phi_c, 'fro')**2\n    loss_style_final = beta * np.linalg.norm(G_Phi_star - G_s, 'fro')**2\n    L_final = loss_content_final + loss_style_final\n    \n    # Identify missing (M) and present (P) channel sets\n    M_indices = np.where(vs == 0)[0]\n    P_indices = np.where(vs != 0)[0]\n    \n    # Calculate energy ratios r_i\n    phi_c_row_norms = np.linalg.norm(phi_c, axis=1)\n    phi_star_row_norms = np.linalg.norm(Phi_star, axis=1)\n    \n    # Avoid division by zero, although not an issue with the given vc.\n    ratios = np.divide(phi_star_row_norms, phi_c_row_norms, \n                       out=np.zeros_like(phi_c_row_norms, dtype=float), \n                       where=phi_c_row_norms!=0)\n\n    # Calculate average ratio R(M)\n    if len(M_indices) == 0:\n        R_M = 1.0\n    else:\n        R_M = np.mean(ratios[M_indices])\n        \n    # Calculate average ratio R(P)\n    if len(P_indices) == 0:\n        R_P = 1.0\n    else:\n        R_P = np.mean(ratios[P_indices])\n        \n    # Detect collapse\n    collapse_detected = R_M = tau\n\n    return [R_M, R_P, L_final, collapse_detected]\n\nsolve()\n```", "id": "3158635"}, {"introduction": "While classic Neural Style Transfer is powerful, it often distorts the precise structure of the content, a problem for applications like stylized portraits. This advanced practice [@problem_id:3158668] tackles this limitation by introducing a powerful concept from metric learning: a contrastive loss. You will enhance the standard NST objective to not only match content and style but also to ensure the output image remains closer in \"identity space\" to the original content than to a negative example, a foundational technique for photorealistic transfer.", "problem": "You are asked to design and implement an identity-preserving Neural Style Transfer (NST) objective and a gradient-based solver from first principles, starting from fundamental definitions. Consider fixed feature extractors defined by linear maps. Let the image be represented as a vector $x \\in \\mathbb{R}^{n}$, and define three feature mappings: an identity-preserving map $\\phi_{\\mathrm{id}}(x) = A x$ with $A \\in \\mathbb{R}^{p \\times n}$, a content map $\\phi_{\\mathrm{c}}(x) = B x$ with $B \\in \\mathbb{R}^{q \\times n}$, and a style map $\\phi_{\\mathrm{s}}(x) = W_{\\mathrm{s}} x$ with $W_{\\mathrm{s}} \\in \\mathbb{R}^{m t \\times n}$. The style features are reshaped as $F(x) = \\mathrm{reshape}(W_{\\mathrm{s}} x, (m, t))$, whose Gram matrix is $G(x) = F(x)^{\\top} F(x) \\in \\mathbb{R}^{t \\times t}$. Use the following definitions, which are well tested in the literature and grounded in linear algebra and optimization: Euclidean norm and Mean Squared Error (MSE).\n\n- Define the content loss as $L_{\\mathrm{content}}(x; c) = \\left\\|\\phi_{\\mathrm{c}}(x) - \\phi_{\\mathrm{c}}(c)\\right\\|_{2}^{2}$ for a given content target $c \\in \\mathbb{R}^{n}$.\n- Define the style loss as $L_{\\mathrm{style}}(x; s) = \\left\\|G(x) - G(s)\\right\\|_{F}^{2}$ for a given style target $s \\in \\mathbb{R}^{n}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm.\n- Define the identity contrastive loss to preserve identity against negatives as $L_{\\mathrm{con}}(x; c, \\tilde c) = \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(c)\\right\\|_{2} - \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(\\tilde c)\\right\\|_{2}$, where $\\tilde c \\in \\mathbb{R}^{n}$ is a negative example that should be repelled in identity feature space.\n\nThe total objective is\n$$\nL(x) = \\alpha \\, L_{\\mathrm{content}}(x; c)\n+ \\beta \\, L_{\\mathrm{style}}(x; s)\n+ \\gamma \\, L_{\\mathrm{con}}(x; c, \\tilde c),\n$$\nwith weights $\\alpha, \\beta, \\gamma \\in \\mathbb{R}_{\\ge 0}$. Your tasks:\n\n- Using only the chain rule and linear algebra, derive the gradient $\\nabla_{x} L_{\\mathrm{content}}(x; c)$, the gradient $\\nabla_{x} L_{\\mathrm{style}}(x; s)$ for the Gram-matrix-based style loss, and the gradient $\\nabla_{x} L_{\\mathrm{con}}(x; c, \\tilde c)$. You must start from the fundamental definitions of the Euclidean norm $\\|z\\|_{2} = \\sqrt{z^{\\top} z}$ and the Frobenius norm $\\|M\\|_{F}^{2} = \\mathrm{trace}(M^{\\top} M)$, and apply the chain rule through the given linear maps. No pre-derived formulas may be assumed.\n- Implement a gradient descent solver that, starting from an initialization $x_{0}$, iteratively updates $x \\leftarrow x - \\eta \\, \\nabla_{x} L(x)$ for a fixed number of steps $T$, with learning rate $\\eta > 0$.\n- For each test case, after $T$ steps, compute the identity distances $d_{\\mathrm{pos}} = \\left\\|\\phi_{\\mathrm{id}}(x^{(T)}) - \\phi_{\\mathrm{id}}(c)\\right\\|_{2}$ and $d_{\\mathrm{neg}} = \\left\\|\\phi_{\\mathrm{id}}(x^{(T)}) - \\phi_{\\mathrm{id}}(\\tilde c)\\right\\|_{2}$, and produce a boolean indicating whether identity is preserved, defined as the condition $d_{\\mathrm{pos}}  d_{\\mathrm{neg}}$.\n\nTo ensure universal applicability and determinism, generate synthetic parameters and data using a pseudorandom generator with fixed seeds, and use fixed dimensions. Specifically, for each test case:\n\n- Use dimensions $n = 16$, $m = 8$, $t = 2$, which imply $W_{\\mathrm{s}} \\in \\mathbb{R}^{16 \\times 16}$ and $F(x) \\in \\mathbb{R}^{8 \\times 2}$, and Gram matrix $G(x) \\in \\mathbb{R}^{2 \\times 2}$.\n- Generate $A \\in \\mathbb{R}^{16 \\times 16}$, $B \\in \\mathbb{R}^{16 \\times 16}$, $W_{\\mathrm{s}} \\in \\mathbb{R}^{16 \\times 16}$, and vectors $c, s, \\tilde c \\in \\mathbb{R}^{16}$ using a zero-mean unit-variance Gaussian with the given seed for each test.\n- Use $x_{0} = 0 \\in \\mathbb{R}^{16}$.\n- Use an identity-preserving small constant $\\varepsilon = 10^{-8}$ to avoid division by zero in any denominator arising from derivatives of the Euclidean norm.\n- Use the specified weights $(\\alpha, \\beta, \\gamma)$, learning rate $\\eta$, and steps $T$ given per test case.\n\nTest suite:\n\n- Case $1$ (happy path): seed $s_{1} = 42$, weights $(\\alpha, \\beta, \\gamma) = (1.0, 0.5, 1.0)$, learning rate $\\eta = 0.05$, steps $T = 200$, negative example generated independently from the same Gaussian as $c$ and $s$.\n- Case $2$ (boundary condition with no contrastive term): seed $s_{2} = 7$, weights $(\\alpha, \\beta, \\gamma) = (1.0, 0.5, 0.0)$, learning rate $\\eta = 0.05$, steps $T = 200$, negative example generated independently.\n- Case $3$ (edge case where negative equals positive): seed $s_{3} = 123$, weights $(\\alpha, \\beta, \\gamma) = (1.0, 0.5, 1.0)$, learning rate $\\eta = 0.05$, steps $T = 200$, negative example set to $\\tilde c = c$.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the boolean for the corresponding test case, in order. For example, output in the form $[r_{1}, r_{2}, r_{3}]$, where each $r_{i} \\in \\{\\mathrm{True}, \\mathrm{False}\\}$.", "solution": "The problem requires the design and implementation of an identity-preserving Neural Style Transfer (NST) objective function and its corresponding gradient-based solver. The solution comprises two main stages: first, the analytical derivation of the gradients for each component of the objective function from first principles; second, the implementation of a gradient descent algorithm to optimize the objective and evaluate the specified identity-preservation criterion on a set of test cases.\n\nThe total loss function is a weighted sum of three components:\n$$L(x) = \\alpha L_{\\mathrm{content}}(x; c) + \\beta L_{\\mathrm{style}}(x; s) + \\gamma L_{\\mathrm{con}}(x; c, \\tilde c)$$\nwhere $x \\in \\mathbb{R}^{n}$ is the image to be optimized, $c \\in \\mathbb{R}^{n}$ is the content target, $s \\in \\mathbb{R}^{n}$ is the style target, and $\\tilde c \\in \\mathbb{R}^{n}$ is a negative example for identity preservation. The weights are given by non-negative scalars $\\alpha, \\beta, \\gamma$. We will now derive the gradient $\\nabla_x L(x)$ by computing the gradient of each component separately.\n\n### 1. Gradient of the Content Loss\n\nThe content loss is defined as the squared Euclidean distance between the content features of the generated image $x$ and the content target $c$:\n$$L_{\\mathrm{content}}(x; c) = \\left\\|\\phi_{\\mathrm{c}}(x) - \\phi_{\\mathrm{c}}(c)\\right\\|_{2}^{2}$$\nGiven the linear content map $\\phi_{\\mathrm{c}}(z) = Bz$, this becomes:\n$$L_{\\mathrm{content}}(x; c) = \\|Bx - Bc\\|_{2}^{2} = \\|B(x-c)\\|_{2}^{2}$$\nTo find the gradient with respect to $x$, we first expand the squared Euclidean norm $\\|v\\|_{2}^{2} = v^{\\top}v$. Let $v = B(x-c)$.\n$$L_{\\mathrm{content}}(x; c) = (B(x-c))^{\\top}(B(x-c)) = (x-c)^{\\top}B^{\\top}B(x-c)$$\nThis is a quadratic form in terms of $(x-c)$. Let $u = x-c$. The expression is $u^{\\top}(B^{\\top}B)u$. For a general quadratic form $u^{\\top}Mu$, the gradient with respect to $u$ is given by $\\nabla_u(u^{\\top}Mu) = (M + M^{\\top})u$. Since the matrix $B^{\\top}B$ is symmetric, $(B^{\\top}B)^{\\top} = B^{\\top}B$.\nThus, the gradient with respect to $u$ is:\n$$\\nabla_u L_{\\mathrm{content}} = 2(B^{\\top}B)u = 2B^{\\top}B(x-c)$$\nTo find the gradient with respect to $x$, we apply the chain rule: $\\nabla_x L_{\\mathrm{content}} = (\\frac{\\partial u}{\\partial x})^{\\top} \\nabla_u L_{\\mathrm{content}}$. Since $u=x-c$, the Jacobian matrix $\\frac{\\partial u}{\\partial x}$ is the identity matrix $I$.\nTherefore, the gradient of the content loss with respect to $x$ is:\n$$\\nabla_x L_{\\mathrm{content}}(x; c) = 2B^{\\top}B(x-c)$$\n\n### 2. Gradient of the Style Loss\n\nThe style loss is based on the Frobenius norm of the difference between the Gram matrices of the generated image $x$ and the style target $s$:\n$$L_{\\mathrm{style}}(x; s) = \\|G(x) - G(s)\\|_{F}^{2}$$\nwhere $G(z) = F(z)^{\\top}F(z)$ and $F(z) = \\mathrm{reshape}(W_s z, (m, t))$. The squared Frobenius norm is defined as $\\|M\\|_{F}^{2} = \\mathrm{trace}(M^{\\top}M)$.\nLet $E = G(x) - G(s)$. The loss is $L_{\\mathrm{style}} = \\mathrm{trace}(E^{\\top}E)$.\nWe use differentials to find the gradient. The differential of $L_{\\mathrm{style}}$ is:\n$$dL_{\\mathrm{style}} = d(\\mathrm{trace}(E^{\\top}E)) = \\mathrm{trace}(d(E^{\\top}E)) = \\mathrm{trace}((dE)^{\\top}E + E^{\\top}dE)$$\nUsing the property $\\mathrm{trace}(A) = \\mathrm{trace}(A^{\\top})$, we have $\\mathrm{trace}((dE)^{\\top}E) = \\mathrm{trace}(E^{\\top}dE)$. Thus:\n$$dL_{\\mathrm{style}} = 2 \\mathrm{trace}(E^{\\top}dE)$$\nSince $E = G(x) - G(s)$, its differential is $dE = dG(x)$ (as $G(s)$ is a constant with respect to $x$). So, $dL_{\\mathrm{style}} = 2 \\mathrm{trace}((G(x)-G(s))^{\\top} dG(x))$.\nNext, we find the differential of the Gram matrix $G(x) = F(x)^{\\top}F(x)$:\n$$dG(x) = d(F(x)^{\\top})F(x) + F(x)^{\\top}dF(x) = (dF(x))^{\\top}F(x) + F(x)^{\\top}dF(x)$$\nSubstituting this into the expression for $dL_{\\mathrm{style}}$:\n$$dL_{\\mathrm{style}} = 2 \\mathrm{trace}\\left((G(x)-G(s))^{\\top} \\left((dF(x))^{\\top}F(x) + F(x)^{\\top}dF(x)\\right)\\right)$$\nLet $K = G(x)-G(s)$. Note that $G(x)$ and $G(s)$ are symmetric, hence $K$ is also symmetric ($K^{\\top}=K$).\n$$dL_{\\mathrm{style}} = 2 \\mathrm{trace}\\left(K(dF(x))^{\\top}F(x)\\right) + 2 \\mathrm{trace}\\left(K F(x)^{\\top}dF(x)\\right)$$\nUsing the cyclic property of a trace, $\\mathrm{trace}(ABC) = \\mathrm{trace}(CAB)$, on the first term: $\\mathrm{trace}(K(dF(x))^{\\top}F(x)) = \\mathrm{trace}(F(x)K(dF(x))^{\\top})$. Also, $\\mathrm{trace}(A) = \\mathrm{trace}(A^\\top)$, so $\\mathrm{trace}(F(x)K(dF(x))^{\\top}) = \\mathrm{trace}((F(x)K(dF(x))^{\\top})^\\top) = \\mathrm{trace}(dF(x)K^\\top F(x)^\\top) = \\mathrm{trace}(dF(x)K F(x)^\\top)$. The first term becomes $2\\mathrm{trace}(dF(x)K F(x)^\\top)$.\nThe second term is $2\\mathrm{trace}(K F(x)^{\\top}dF(x))$.\n$dL_{\\mathrm{style}} = 2\\mathrm{trace}(dF(x)K F(x)^\\top) + 2\\mathrm{trace}(K F(x)^{\\top}dF(x)) = 2\\mathrm{trace}((F(x)K^\\top)^\\top dF(x)) + 2\\mathrm{trace}(K F(x)^{\\top}dF(x)) = 2\\mathrm{trace}(K F(x)^{\\top}dF(x)) + 2\\mathrm{trace}(K F(x)^{\\top}dF(x))$.\n$$dL_{\\mathrm{style}} = 4 \\mathrm{trace}\\left((G(x)-G(s))F(x)^{\\top}dF(x)\\right)$$\nThe general relation between a scalar function $f(Z)$ and its differential is $df = \\mathrm{trace}((\\nabla_Z f)^{\\top}dZ)$. From this, we can identify the gradient of $L_{\\mathrm{style}}$ with respect to the matrix $F(x)$:\n$$\\nabla_{F(x)} L_{\\mathrm{style}} = 4 F(x)(G(x)-G(s))$$\nThe final step is to relate this gradient back to $x$. Let $y = W_s x$. Then $F(x) = \\mathrm{reshape}(y, (m,t))$. The vectorization operator $\\mathrm{vec}(\\cdot)$ flattens a matrix into a vector. The elements of $y$ are the same as the elements of $F(x)$, just arranged differently. Thus, the gradient with respect to $y$, $\\nabla_y L_{\\mathrm{style}}$, is the vectorized version of $\\nabla_{F(x)} L_{\\mathrm{style}}$.\n$$\\nabla_y L_{\\mathrm{style}} = \\mathrm{vec}(\\nabla_{F(x)} L_{\\mathrm{style}})$$\nFinally, applying the chain rule for the linear map $y=W_s x$:\n$$\\nabla_x L_{\\mathrm{style}} = \\left(\\frac{\\partial y}{\\partial x}\\right)^{\\top}\\nabla_y L_{\\mathrm{style}} = W_s^{\\top} \\nabla_y L_{\\mathrm{style}}$$\nSubstituting the expressions, we get the gradient of the style loss with respect to $x$:\n$$\\nabla_x L_{\\mathrm{style}}(x; s) = W_s^{\\top} \\mathrm{vec}\\left(4 F(x)(F(x)^{\\top}F(x) - G(s))\\right)$$\n\n### 3. Gradient of the Identity Contrastive Loss\n\nThe identity contrastive loss is defined as:\n$$L_{\\mathrm{con}}(x; c, \\tilde c) = \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(c)\\right\\|_{2} - \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(\\tilde c)\\right\\|_{2}$$\nWith the linear identity map $\\phi_{\\mathrm{id}}(z)=Az$, this becomes:\n$$L_{\\mathrm{con}}(x; c, \\tilde c) = \\|A(x-c)\\|_{2} - \\|A(x-\\tilde c)\\|_{2}$$\nThis loss is a difference of two terms of the form $f(x) = \\|A(x-z)\\|_{2}$. Let's find the gradient of such a term.\nUsing the definition $\\|v\\|_{2} = \\sqrt{v^{\\top}v}$, we have $f(x) = \\sqrt{(A(x-z))^{\\top}(A(x-z))}$.\nWe apply the chain rule. Let $v(x) = A(x-z)$. Then $f(x) = \\sqrt{v^{\\top}v}$.\n$$\\nabla_x f(x) = \\frac{1}{2\\sqrt{v^{\\top}v}} \\nabla_x (v^{\\top}v)$$\nThe term $\\nabla_x (v^{\\top}v)$ is the gradient of $(x-z)^{\\top}A^{\\top}A(x-z)$. As shown for the content loss, this gradient is $2A^{\\top}A(x-z)$.\nSubstituting this back, we get:\n$$\\nabla_x f(x) = \\frac{1}{2\\|A(x-z)\\|_{2}} \\left(2A^{\\top}A(x-z)\\right) = \\frac{A^{\\top}A(x-z)}{\\|A(x-z)\\|_{2}}$$\nTo prevent division by zero when the norm is close to zero, we add a small constant $\\varepsilon > 0$ to the denominator, as specified.\nApplying this result to both terms in $L_{\\mathrm{con}}$:\n$$\\nabla_x L_{\\mathrm{con}}(x; c, \\tilde c) = \\frac{A^{\\top}A(x-c)}{\\|A(x-c)\\|_{2} + \\varepsilon} - \\frac{A^{\\top}A(x-\\tilde c)}{\\|A(x-\\tilde c)\\|_{2} + \\varepsilon}$$\n\n### 4. Total Gradient and Solver\n\nThe total gradient is the weighted sum of the individual gradients:\n$$\\nabla_x L(x) = \\alpha \\nabla_x L_{\\mathrm{content}}(x; c) + \\beta \\nabla_x L_{\\mathrm{style}}(x; s) + \\gamma \\nabla_x L_{\\mathrm{con}}(x; c, \\tilde c)$$\nThe solver iteratively updates the image vector $x$ using gradient descent, starting from an initial $x_0 = 0$. For a given learning rate $\\eta > 0$ and for $k = 0, 1, \\dots, T-1$:\n$$x_{k+1} = x_k - \\eta \\nabla_x L(x_k)$$\nAfter $T$ iterations, the final image $x^{(T)}$ is used to evaluate the identity preservation condition, $d_{\\mathrm{pos}}  d_{\\mathrm{neg}}$, where $d_{\\mathrm{pos}} = \\|A(x^{(T)}-c)\\|_2$ and $d_{\\mathrm{neg}} = \\|A(x^{(T)}-\\tilde{c})\\|_2$.\n\nThe implementation will follow these derived formulas to perform the optimization and evaluation for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the identity-preserving NST problem for the given test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        {'seed': 42, 'alpha': 1.0, 'beta': 0.5, 'gamma': 1.0, 'eta': 0.05, 'T': 200, 'neg_is_pos': False},\n        {'seed': 7, 'alpha': 1.0, 'beta': 0.5, 'gamma': 0.0, 'eta': 0.05, 'T': 200, 'neg_is_pos': False},\n        {'seed': 123, 'alpha': 1.0, 'beta': 0.5, 'gamma': 1.0, 'eta': 0.05, 'T': 200, 'neg_is_pos': True},\n    ]\n\n    results = []\n\n    # Fixed parameters from the problem description\n    n_dim = 16\n    m_dim = 8\n    t_dim = 2\n    epsilon = 1e-8\n\n    for case in test_cases:\n        seed = case['seed']\n        alpha = case['alpha']\n        beta = case['beta']\n        gamma = case['gamma']\n        eta = case['eta']\n        T = case['T']\n\n        # Generate synthetic data with the specified seed\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(n_dim, n_dim))\n        B = rng.normal(size=(n_dim, n_dim))\n        Ws = rng.normal(size=(m_dim * t_dim, n_dim))\n        \n        c = rng.normal(size=n_dim)\n        s = rng.normal(size=n_dim)\n        \n        if case['neg_is_pos']:\n            tilde_c = c.copy()\n        else:\n            tilde_c = rng.normal(size=n_dim)\n\n        # Pre-compute constant matrices for efficiency\n        AtA = A.T @ A\n        BtB = B.T @ B\n        \n        # Pre-compute style target's Gram matrix\n        s_features = Ws @ s\n        Fs = s_features.reshape((m_dim, t_dim))\n        Gs = Fs.T @ Fs\n\n        # Initialize x\n        x = np.zeros(n_dim)\n\n        # Gradient Descent loop\n        for _ in range(T):\n            # 1. Content Loss Gradient\n            # grad_content = 2 * B^T * B * (x - c)\n            grad_content = 2 * (BtB @ (x - c))\n\n            # 2. Style Loss Gradient\n            # grad_style = Ws^T * vec(4 * F(x) * (G(x) - G(s)))\n            if beta > 0:\n                x_features = Ws @ x\n                Fx = x_features.reshape((m_dim, t_dim))\n                Gx = Fx.T @ Fx\n                grad_F = 4 * (Fx @ (Gx - Gs))\n                grad_y = grad_F.flatten()\n                grad_style = Ws.T @ grad_y\n            else:\n                grad_style = np.zeros(n_dim)\n            \n            # 3. Contrastive Loss Gradient\n            # grad_con = (A^T*A*(x-c))/(||A(x-c)||_2 + eps) - (A^T*A*(x-tilde_c))/(||A(x-tilde_c)||_2 + eps)\n            if gamma > 0:\n                # To handle the case where tilde_c = c, which would lead to 0/0\n                if np.array_equal(c, tilde_c):\n                    grad_con = np.zeros(n_dim)\n                else:\n                    v_pos = A @ (x - c)\n                    norm_pos = np.linalg.norm(v_pos)\n                    term_pos = (AtA @ (x - c)) / (norm_pos + epsilon)\n\n                    v_neg = A @ (x - tilde_c)\n                    norm_neg = np.linalg.norm(v_neg)\n                    term_neg = (AtA @ (x - tilde_c)) / (norm_neg + epsilon)\n                    \n                    grad_con = term_pos - term_neg\n            else:\n                grad_con = np.zeros(n_dim)\n            \n            # Total gradient\n            total_grad = alpha * grad_content + beta * grad_style + gamma * grad_con\n\n            # Update x\n            x = x - eta * total_grad\n        \n        # After T steps, compute identity distances\n        x_final = x\n        d_pos = np.linalg.norm(A @ (x_final - c))\n        d_neg = np.linalg.norm(A @ (x_final - tilde_c))\n\n        # Check preservation condition\n        is_preserved = d_pos  d_neg\n        results.append(is_preserved)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```", "id": "3158668"}]}