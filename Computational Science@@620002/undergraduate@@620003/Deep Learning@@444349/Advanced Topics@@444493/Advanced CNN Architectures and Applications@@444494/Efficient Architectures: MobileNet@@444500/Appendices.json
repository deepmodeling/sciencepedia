{"hands_on_practices": [{"introduction": "The core innovation behind MobileNet is the replacement of standard convolutional layers with depthwise separable convolutions. To truly appreciate this design choice, it is essential to move beyond a qualitative understanding and quantify the computational savings. This first practice challenges you to derive the computational cost, measured in Multiply-Accumulate (MAC) operations, for both standard and depthwise separable layers from first principles, giving you a concrete grasp of their remarkable efficiency.", "problem": "A MobileNet Version $1$ (MobileNetV$1$) stage at spatial resolution $H=W=112$ transforms an input tensor with $C_{in}=32$ channels into an output tensor with $C_{out}=64$ channels using a convolution with kernel size $K=3$ and stride $1$. Assume zero padding that preserves spatial dimensions. Consider two implementations of this stage: (i) a standard convolution with $K \\times K$ filters across all input channels and (ii) a depthwise separable convolution composed of a depthwise $K \\times K$ convolution followed by a pointwise $1 \\times 1$ convolution. Using first principles of discrete convolution and operation counting, compute the exact total number of Multiply-Accumulate operations (MACs), where a Multiply-Accumulate (MAC) is defined as one multiplication paired with one addition, for both implementations across the full feature maps. Then, as the comparison metric, compute the speedup factor defined as the ratio of the standard convolution MAC count to the depthwise separable convolution MAC count. Report the speedup factor as a single simplified exact expression.", "solution": "The fundamental base is the definition of discrete convolution and operation counting. For each output element of a convolutional layer, the computation involves summing products between a local receptive field of the input and the corresponding filter weights. Each product and its accumulation is counted as one Multiply-Accumulate (MAC).\n\nFor a standard convolution:\n- There are $H \\times W$ spatial positions and $C_{out}$ output channels, yielding $H W C_{out}$ output elements.\n- Each output element is computed from a $K \\times K$ spatial neighborhood across all $C_{in}$ input channels, resulting in $K^{2} C_{in}$ MACs per output element.\nTherefore, the total MAC count for the standard convolution is\n$$\n\\text{MAC}_{\\text{std}} = H W C_{out} K^{2} C_{in}.\n$$\n\nFor a depthwise separable convolution, the computation is split into two parts:\n\n1. Depthwise convolution:\n- There are $H \\times W$ spatial positions and $C_{in}$ separate depthwise filters, one per input channel, yielding $H W C_{in}$ output elements.\n- Each depthwise output element uses a $K \\times K$ kernel limited to its own channel, contributing $K^{2}$ MACs per output element.\nThus, the depthwise MAC count is\n$$\n\\text{MAC}_{\\text{dw}} = H W C_{in} K^{2}.\n$$\n\n2. Pointwise $1 \\times 1$ convolution:\n- There are $H \\times W$ spatial positions and $C_{out}$ output channels, yielding $H W C_{out}$ output elements.\n- Each pointwise output element is a weighted sum over $C_{in}$ input channels with a $1 \\times 1$ kernel, contributing $C_{in}$ MACs per output element.\nThus, the pointwise MAC count is\n$$\n\\text{MAC}_{\\text{pw}} = H W C_{out} C_{in}.\n$$\n\nCombining the two, the total MAC count for the depthwise separable convolution is\n$$\n\\text{MAC}_{\\text{dws}} = \\text{MAC}_{\\text{dw}} + \\text{MAC}_{\\text{pw}} = H W C_{in} K^{2} + H W C_{out} C_{in} = H W C_{in} \\left(K^{2} + C_{out}\\right).\n$$\n\nThe speedup factor $S$ is defined as the ratio of the standard convolution MAC count to the depthwise separable convolution MAC count:\n$$\nS = \\frac{\\text{MAC}_{\\text{std}}}{\\text{MAC}_{\\text{dws}}} = \\frac{H W C_{out} K^{2} C_{in}}{H W C_{in} \\left(K^{2} + C_{out}\\right)}.\n$$\nCanceling the common factors $H$, $W$, and $C_{in}$ gives\n$$\nS = \\frac{C_{out} K^{2}}{K^{2} + C_{out}}.\n$$\n\nSubstitute the given values $C_{in}=32$, $C_{out}=64$, $K=3$, $H=W=112$ into the simplified symbolic expression for $S$:\n$$\nS = \\frac{64 \\cdot 3^{2}}{3^{2} + 64} = \\frac{64 \\cdot 9}{9 + 64} = \\frac{576}{73}.\n$$\n\nFor completeness, we can verify the exact MAC counts numerically:\n- Standard convolution:\n$$\n\\text{MAC}_{\\text{std}} = 112 \\cdot 112 \\cdot 64 \\cdot 9 \\cdot 32 = 12544 \\cdot 64 \\cdot 288 = 231{,}211{,}008.\n$$\n- Depthwise separable convolution:\n$$\n\\text{MAC}_{\\text{dws}} = 112 \\cdot 112 \\cdot 32 \\cdot \\left(9 + 64\\right) = 12544 \\cdot 32 \\cdot 73 = 29{,}302{,}784.\n$$\nTheir ratio indeed equals\n$$\n\\frac{231{,}211{,}008}{29{,}302{,}784} = \\frac{576}{73}.\n$$\n\nThe requested final output is the speedup factor as a single simplified exact expression.", "answer": "$$\\boxed{\\frac{576}{73}}$$", "id": "3120106"}, {"introduction": "Once an efficient architecture is established, the next step is tailoring it to specific hardware constraints. MobileNets offer knobs like the width multiplier ($\\alpha$) and resolution multiplier ($\\rho$) to navigate the accuracy-versus-latency trade-off. This practice frames this as a formal constrained optimization problem, where you will find the optimal values of $\\alpha$ and $\\rho$ that maximize a given accuracy model under a strict computational budget, a crucial skill for deploying models in resource-constrained environments.", "problem": "Consider a MobileNet-style model that uses a width multiplier $\\alpha \\in (0,1]$ and a resolution multiplier $\\rho \\in (0,1]$. It is an empirically well-supported fact for depthwise separable convolution networks that, holding all other design choices fixed, the computational cost (in Million Floating-Point Operations (MFLOPs)) scales approximately as $\\alpha^{2}\\rho^{2}$ times a baseline cost. Suppose the baseline cost at $\\alpha=1$ and $\\rho=1$ is $C_{\\text{base}}=500$ MFLOPs. A hypothetical accuracy model (expressed as a decimal fraction, not a percentage) is given by\n$$\nA(\\alpha,\\rho)=A_{0}-k_{1}\\left(1-\\alpha\\right)^{p}-k_{2}\\left(1-\\rho\\right)^{q},\n$$\nwith constants $A_{0}=0.80$, $k_{1}=0.10$, $k_{2}=0.10$, $p=2$, and $q=2$. You must choose $(\\alpha,\\rho)$ to maximize $A(\\alpha,\\rho)$ subject to a hard budget of $B=125$ MFLOPs, using only the fundamental scaling fact above and principled constrained optimization.\n\nFormulate and solve the constrained optimization problem from first principles, and determine the maximum predicted accuracy $A^{\\star}$ achievable under the budget. Express $A^{\\star}$ as a decimal fraction and round your final answer to four significant figures.", "solution": "The problem is to maximize the accuracy function\n$$\nA(\\alpha, \\rho) = 0.80 - 0.10(1-\\alpha)^{2} - 0.10(1-\\rho)^{2}\n$$\nsubject to the constraints\n$$\nC(\\alpha, \\rho) = 500 \\alpha^{2} \\rho^{2} \\le 125\n$$\n$$\n\\alpha \\in (0, 1]\n$$\n$$\n\\rho \\in (0, 1]\n$$\nFirst, we simplify the computational cost constraint:\n$$\n500 \\alpha^{2} \\rho^{2} \\le 125 \\implies \\alpha^{2} \\rho^{2} \\le \\frac{125}{500} \\implies \\alpha^{2} \\rho^{2} \\le \\frac{1}{4}\n$$\nSince $\\alpha > 0$ and $\\rho > 0$, we can take the square root of both sides, yielding\n$$\n\\alpha \\rho \\le \\frac{1}{2}\n$$\nThe objective function $A(\\alpha, \\rho)$ is a strictly increasing function of both $\\alpha$ and $\\rho$ within their specified domains. To show this, we examine the partial derivatives:\n$$\n\\frac{\\partial A}{\\partial \\alpha} = -0.10 \\cdot 2 (1-\\alpha)(-1) = 0.20(1-\\alpha)\n$$\n$$\n\\frac{\\partial A}{\\partial \\rho} = -0.10 \\cdot 2 (1-\\rho)(-1) = 0.20(1-\\rho)\n$$\nFor $\\alpha \\in (0, 1)$ and $\\rho \\in (0, 1)$, both partial derivatives are positive. This indicates that to maximize $A(\\alpha, \\rho)$, we must choose the largest possible values for $\\alpha$ and $\\rho$ that satisfy the constraints. Therefore, the maximum must lie on the boundary of the feasible region, where the budget is fully utilized. The inequality constraint becomes an equality constraint:\n$$\n\\alpha \\rho = \\frac{1}{2}\n$$\nWe now solve the constrained optimization problem using the method of Lagrange multipliers. Maximizing $A(\\alpha, \\rho)$ is equivalent to minimizing $-A(\\alpha, \\rho)$, which in turn is equivalent to minimizing the function $F(\\alpha, \\rho) = 0.10(1-\\alpha)^{2} + 0.10(1-\\rho)^{2}$, as the $0.80$ term is a constant offset.\n\nThe Lagrangian function $\\mathcal{L}$ is constructed as:\n$$\n\\mathcal{L}(\\alpha, \\rho, \\lambda) = 0.10(1-\\alpha)^{2} + 0.10(1-\\rho)^{2} - \\lambda \\left(\\alpha \\rho - \\frac{1}{2}\\right)\n$$\nTo find the critical points, we set the gradient of $\\mathcal{L}$ with respect to $\\alpha$, $\\rho$, and $\\lambda$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = -0.20(1-\\alpha) - \\lambda \\rho = 0 \\quad (1)\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\rho} = -0.20(1-\\rho) - \\lambda \\alpha = 0 \\quad (2)\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -\\left(\\alpha \\rho - \\frac{1}{2}\\right) = 0 \\quad (3)\n$$\nFrom equation (3), we recover our constraint $\\alpha \\rho = \\frac{1}{2}$.\nFrom (1) and (2), we can express $\\lambda$:\n$$\n\\lambda = -\\frac{0.20(1-\\alpha)}{\\rho}\n$$\n$$\n\\lambda = -\\frac{0.20(1-\\rho)}{\\alpha}\n$$\nEquating these expressions for $\\lambda$ (we can confirm $\\lambda \\neq 0$, otherwise $\\alpha=1$ and $\\rho=1$, which violates the constraint):\n$$\n\\frac{1-\\alpha}{\\rho} = \\frac{1-\\rho}{\\alpha}\n$$\n$$\n\\alpha(1-\\alpha) = \\rho(1-\\rho)\n$$\n$$\n\\alpha - \\alpha^{2} = \\rho - \\rho^{2}\n$$\n$$\n\\alpha - \\rho = \\alpha^{2} - \\rho^{2}\n$$\n$$\n\\alpha - \\rho = (\\alpha - \\rho)(\\alpha + \\rho)\n$$\nThis equation yields two possibilities: $\\alpha - \\rho = 0$ or, if $\\alpha \\neq \\rho$, we can divide by $(\\alpha - \\rho)$ to get $\\alpha + \\rho = 1$.\n\nCase 1: $\\alpha = \\rho$.\nSubstituting this into the constraint $\\alpha \\rho = \\frac{1}{2}$:\n$$\n\\alpha^{2} = \\frac{1}{2} \\implies \\alpha = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}\n$$\nSo, $(\\alpha, \\rho) = \\left(\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right)$. Since $\\frac{\\sqrt{2}}{2} \\approx 0.707$, these values lie within the domain $(0,1]$. This is a valid candidate for the optimum.\n\nCase 2: $\\alpha + \\rho = 1$.\nFrom the constraint, we have $\\rho = \\frac{1}{2\\alpha}$. Substituting this into the case equation:\n$$\n\\alpha + \\frac{1}{2\\alpha} = 1\n$$\nMultiplying by $2\\alpha$ (since $\\alpha > 0$):\n$$\n2\\alpha^{2} + 1 = 2\\alpha \\implies 2\\alpha^{2} - 2\\alpha + 1 = 0\n$$\nThe discriminant of this quadratic equation is $\\Delta = b^{2} - 4ac = (-2)^{2} - 4(2)(1) = 4 - 8 = -4$. Since $\\Delta < 0$, there are no real solutions for $\\alpha$ in this case.\n\nThus, the only critical point is $\\alpha^{\\star} = \\rho^{\\star} = \\frac{\\sqrt{2}}{2}$. This symmetric result is expected given the symmetry in the objective function and constraint with respect to $\\alpha$ and $\\rho$.\n\nNow, we calculate the maximum accuracy $A^{\\star}$ by substituting these optimal values back into the accuracy function:\n$$\nA^{\\star} = A\\left(\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right) = 0.80 - 0.10\\left(1 - \\frac{\\sqrt{2}}{2}\\right)^{2} - 0.10\\left(1 - \\frac{\\sqrt{2}}{2}\\right)^{2}\n$$\n$$\nA^{\\star} = 0.80 - 0.20\\left(1 - \\frac{\\sqrt{2}}{2}\\right)^{2}\n$$\nWe expand the squared term:\n$$\n\\left(1 - \\frac{\\sqrt{2}}{2}\\right)^{2} = 1 - 2\\left(\\frac{\\sqrt{2}}{2}\\right) + \\left(\\frac{\\sqrt{2}}{2}\\right)^{2} = 1 - \\sqrt{2} + \\frac{2}{4} = 1 - \\sqrt{2} + \\frac{1}{2} = \\frac{3}{2} - \\sqrt{2}\n$$\nSubstituting this back into the expression for $A^{\\star}$:\n$$\nA^{\\star} = 0.80 - 0.20\\left(\\frac{3}{2} - \\sqrt{2}\\right)\n$$\n$$\nA^{\\star} = 0.80 - \\left(0.20 \\cdot \\frac{3}{2} - 0.20 \\cdot \\sqrt{2}\\right) = 0.80 - (0.30 - 0.20\\sqrt{2})\n$$\n$$\nA^{\\star} = 0.80 - 0.30 + 0.20\\sqrt{2} = 0.50 + 0.2\\sqrt{2}\n$$\nFinally, we compute the numerical value and round to four significant figures as requested.\n$$\n\\sqrt{2} \\approx 1.41421356...\n$$\n$$\nA^{\\star} \\approx 0.50 + 0.2 \\times 1.41421356 = 0.50 + 0.282842712 = 0.782842712\n$$\nRounding to four significant figures, we get $0.7828$.", "answer": "$$\\boxed{0.7828}$$", "id": "3120133"}, {"introduction": "State-of-the-art efficient architectures are often built by combining several powerful ideas. This final practice explores such a scenario by asking you to analyze the impact of adding a Squeeze-and-Excitation (SE) module to a MobileNetV2 inverted residual block. You will calculate the baseline computational cost, the overhead introduced by the SE module, and key performance metrics that quantify the accuracy gain per unit of added computation, enabling a data-driven evaluation of architectural enhancements.", "problem": "You are asked to reason about and implement the compute cost impact of adding Squeeze-and-Excitation (SE) to MobileNetV2 inverted residual blocks, and then quantify an accuracy-versus-compute trade-off for several configurations. Work from the following fundamental base and core definitions and do not assume any specialized formulas beyond these principles.\n\nFundamental base and definitions:\n- A Multiply–Accumulate Operation (MAC) is the basic unit of arithmetic combining one multiplication and one addition. For the purposes of this task, count one MAC per weight-times-input contribution to an output.\n- A dense (fully connected) layer mapping an input vector of length $u$ to an output vector of length $v$ uses a weight matrix of size $u \\times v$, thus $u \\cdot v$ MACs per inference for one input vector.\n- A standard two-dimensional convolution with spatial kernel of size $k \\times k$, with $C_{\\text{in}}$ input channels and $C_{\\text{out}}$ output channels, applied at spatial resolution $H \\times W$ (stride $1$, same padding) uses $H \\cdot W \\cdot k \\cdot k \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$ MACs.\n- A depthwise convolution with spatial kernel size $k \\times k$ and $C$ channels applied at resolution $H \\times W$ uses $H \\cdot W \\cdot k \\cdot k \\cdot C$ MACs, because each channel is convolved independently.\n- A $1 \\times 1$ convolution with $C_{\\text{in}}$ inputs and $C_{\\text{out}}$ outputs applied at resolution $H \\times W$ uses $H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$ MACs.\n- A Squeeze-and-Excitation (SE) block for a tensor with $C$ channels consists of: global average pooling to produce a vector of length $C$ (ignore its cost for this problem), followed by two dense layers: a reduction from $C$ to $C/r$ channels, then an expansion from $C/r$ back to $C$ channels, where $r$ is a positive integer reduction ratio. Ignore costs from bias additions, nonlinearities such as the Rectified Linear Unit (ReLU), and the final sigmoid, and count only MACs from the two dense layers. Assume $C/r$ is an integer in all test cases below.\n\nMobileNetV2 inverted residual block model to analyze\n- Consider a MobileNetV2 block with stride $1$ whose input and output channel counts are both $C$, using an expansion factor $t$. The block consists of: a $1 \\times 1$ pointwise convolution expanding from $C$ to $tC$, a $3 \\times 3$ depthwise convolution on $tC$ channels, and a $1 \\times 1$ pointwise projection convolution from $tC$ back to $C$. The spatial resolution is $H \\times W$ throughout the block. Ignore any skip connection cost and any non-convolutional operations.\n\nTasks:\n1. From the fundamental base above, derive an expression for the total baseline MACs of the MobileNetV2 block (without SE) as a function of $H$, $W$, $C$, $t$, and kernel size $k = 3$. Then, using the same principles, derive the additional MACs introduced by inserting an SE module operating on the block’s $C$-channel output with reduction ratio $r$.\n2. For each test case, compute:\n   - the baseline MACs per block without SE,\n   - the additional MACs due to SE,\n   - the overhead fraction defined as the ratio of the additional MACs to the baseline MACs,\n   - the accuracy gain per added mega-MAC, defined as $(A_{\\text{SE}} - A_{\\text{base}})$ divided by the added MACs measured in millions of MACs (i.e., divide added MACs by $10^6$ before computing the ratio).\n3. Express all accuracy values as decimals in $[0,1]$. No physical units or angles are involved. Your final outputs for each test case should be two floats: [accuracy_gain_per_megaMAC, overhead_fraction]. Round both floats to nine decimal places.\n\nTest suite:\nFor each test case, you are given $(H, W, C, t, r, A_{\\text{base}}, A_{\\text{SE}})$, with all quantities integers except accuracies:\n- Case $1$: $H = 56$, $W = 56$, $C = 24$, $t = 6$, $r = 1$, $A_{\\text{base}} = 0.720$, $A_{\\text{SE}} = 0.732$.\n- Case $2$: $H = 56$, $W = 56$, $C = 24$, $t = 6$, $r = 8$, $A_{\\text{base}} = 0.720$, $A_{\\text{SE}} = 0.724$.\n- Case $3$: $H = 28$, $W = 28$, $C = 32$, $t = 6$, $r = 8$, $A_{\\text{base}} = 0.720$, $A_{\\text{SE}} = 0.723$.\n- Case $4$: $H = 14$, $W = 14$, $C = 96$, $t = 6$, $r = 16$, $A_{\\text{base}} = 0.720$, $A_{\\text{SE}} = 0.728$.\n- Case $5$: $H = 7$, $W = 7$, $C = 320$, $t = 6$, $r = 32$, $A_{\\text{base}} = 0.720$, $A_{\\text{SE}} = 0.726$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for the five cases, where each result is a two-element list in the order [accuracy_gain_per_megaMAC, overhead_fraction]. The overall format must be a single Python-style list of lists with no extra text, for example: [[x1,y1],[x2,y2],[x3,y3],[x4,y4],[x5,y5]]. Round all floats to nine decimal places before printing.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded, and objective task. We will now proceed with a principled derivation of the necessary formulas and their application.\n\nThe primary goal is to quantify the computational cost and accuracy trade-off of integrating a Squeeze-and-Excitation (SE) module into a MobileNetV2 inverted residual block. The unit of computational cost is the Multiply-Accumulate operation (MAC).\n\nFirst, we derive an expression for the baseline computational cost of a MobileNetV2 inverted residual block without an SE module. The block, as defined, operates at a constant spatial resolution of $H \\times W$ and consists of three convolutional layers.\n\n1.  **Pointwise Expansion Convolution**: This is a $1 \\times 1$ convolution that expands the number of channels from $C$ to $tC$. According to the provided definition for a $1 \\times 1$ convolution, the cost is the product of the output spatial dimensions, the input channels, and the output channels.\n    $$ \\text{MACs}_1 = H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} = H \\cdot W \\cdot C \\cdot (tC) = H \\cdot W \\cdot t \\cdot C^2 $$\n\n2.  **Depthwise Convolution**: This is a $3 \\times 3$ depthwise convolution operating on $tC$ channels. With a kernel size of $k=3$, the cost is given by the product of the output spatial dimensions, the kernel area, and the number of channels.\n    $$ \\text{MACs}_2 = H \\cdot W \\cdot k^2 \\cdot (\\text{channels}) = H \\cdot W \\cdot 3^2 \\cdot (tC) = 9 \\cdot H \\cdot W \\cdot t \\cdot C $$\n\n3.  **Pointwise Projection Convolution**: This is a $1 \\times 1$ convolution that projects the $tC$ channels back to $C$ channels. Similar to the expansion layer, its cost is:\n    $$ \\text{MACs}_3 = H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} = H \\cdot W \\cdot (tC) \\cdot C = H \\cdot W \\cdot t \\cdot C^2 $$\n\nThe total baseline MACs for the MobileNetV2 block, $\\text{MACs}_{\\text{base}}$, is the sum of the costs of these three layers.\n$$ \\text{MACs}_{\\text{base}} = \\text{MACs}_1 + \\text{MACs}_2 + \\text{MACs}_3 $$\n$$ \\text{MACs}_{\\text{base}} = (H \\cdot W \\cdot t \\cdot C^2) + (9 \\cdot H \\cdot W \\cdot t \\cdot C) + (H \\cdot W \\cdot t \\cdot C^2) $$\nFactoring out common terms, we obtain the final expression for the baseline cost:\n$$ \\text{MACs}_{\\text{base}} = H \\cdot W \\cdot t \\cdot C \\cdot (C + 9 + C) = H \\cdot W \\cdot t \\cdot C \\cdot (2C + 9) $$\n\nNext, we derive the additional computational cost, $\\text{MACs}_{\\text{add}}$, introduced by the Squeeze-and-Excitation module. The SE module is applied to the output of the block, which has $C$ channels. After global average pooling (whose cost is ignored), the SE module consists of two dense (fully connected) layers.\n\n1.  **Reduction Dense Layer**: This layer reduces the channel vector length from $C$ to $C/r$, where $r$ is the reduction ratio. The cost is the product of the input and output vector lengths.\n    $$ \\text{MACs}_{\\text{SE},1} = C \\cdot (C/r) = \\frac{C^2}{r} $$\n\n2.  **Expansion Dense Layer**: This layer expands the vector length from $C/r$ back to $C$.\n    $$ \\text{MACs}_{\\text{SE},2} = (C/r) \\cdot C = \\frac{C^2}{r} $$\n\nThe total additional MACs from the SE module is the sum of the costs of these two dense layers.\n$$ \\text{MACs}_{\\text{add}} = \\text{MACs}_{\\text{SE},1} + \\text{MACs}_{\\text{SE},2} = \\frac{C^2}{r} + \\frac{C^2}{r} = \\frac{2C^2}{r} $$\n\nWith these derived expressions, we can compute the required metrics for each test case.\n\nThe first metric is the **overhead fraction**, which is the ratio of the additional MACs to the baseline MACs.\n$$ \\text{Overhead Fraction} = \\frac{\\text{MACs}_{\\text{add}}}{\\text{MACs}_{\\text{base}}} = \\frac{2C^2/r}{H \\cdot W \\cdot t \\cdot C \\cdot (2C + 9)} = \\frac{2C}{r \\cdot H \\cdot W \\cdot t \\cdot (2C + 9)} $$\n\nThe second metric is the **accuracy gain per added mega-MAC**. This quantifies the efficiency of the added computation in terms of accuracy improvement. It is defined as the difference in accuracy ($A_{\\text{SE}} - A_{\\text{base}}$) divided by the additional MACs converted to millions of MACs (mega-MACs).\n$$ \\text{Accuracy Gain per Mega-MAC} = \\frac{A_{\\text{SE}} - A_{\\text{base}}}{\\text{MACs}_{\\text{add}} / 10^6} $$\n\nThese formulas will now be implemented to process the provided test suite. For each case, we will calculate $\\text{MACs}_{\\text{base}}$ and $\\text{MACs}_{\\text{add}}$, and subsequently the two required metrics, rounding the final float values to nine decimal places as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the accuracy-vs-compute trade-off for adding\n    Squeeze-and-Excitation (SE) blocks to MobileNetV2 inverted residual blocks.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (H, W, C, t, r, A_base, A_SE)\n    test_cases = [\n        (56, 56, 24, 6, 1, 0.720, 0.732),\n        (56, 56, 24, 6, 8, 0.720, 0.724),\n        (28, 28, 32, 6, 8, 0.720, 0.723),\n        (14, 14, 96, 6, 16, 0.720, 0.728),\n        (7, 7, 320, 6, 32, 0.720, 0.726),\n    ]\n\n    results = []\n    for case in test_cases:\n        H, W, C, t, r, A_base, A_SE = case\n\n        # Task 1: Calculate baseline and additional MACs based on derived formulas.\n        \n        # Baseline MACs for the MobileNetV2 block:\n        # MACs_base = H * W * t * C * (2*C + 9)\n        macs_base = H * W * t * C * (2 * C + 9)\n        \n        # Additional MACs for the Squeeze-and-Excitation module:\n        # MACs_add = 2 * C^2 / r\n        # Ensure integer division is not used accidentally, though all inputs are integers\n        macs_add = (2 * C**2) / r\n\n        # Task 2: Compute the required metrics.\n\n        # Overhead fraction: ratio of additional MACs to baseline MACs\n        overhead_fraction = macs_add / macs_base\n\n        # Accuracy gain per added mega-MAC\n        # First, calculate the absolute accuracy gain\n        accuracy_gain = A_SE - A_base\n        # Convert additional MACs to millions (Mega-MACs)\n        added_mega_macs = macs_add / 1_000_000\n        # Calculate the ratio. Handle case where added_mega_macs is zero to avoid division by zero.\n        if added_mega_macs > 0:\n            accuracy_gain_per_megaMAC = accuracy_gain / added_mega_macs\n        else:\n            accuracy_gain_per_megaMAC = 0.0\n\n        # Task 3: Round the results to nine decimal places.\n        rounded_gain = round(accuracy_gain_per_megaMAC, 9)\n        rounded_overhead = round(overhead_fraction, 9)\n\n        results.append([rounded_gain, rounded_overhead])\n\n    # Final print statement in the exact required format: [[x1,y1],[x2,y2],...]\n    # Using join and map(str) to avoid spaces that `str(list_of_lists)` would add.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120155"}]}