{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with the fundamental mechanics of Depthwise Separable Convolutions (DSC). Understanding how to calculate the output dimensions and computational cost is crucial for appreciating the efficiency that makes DSC a cornerstone of modern, lightweight neural networks. This exercise challenges you to derive the core arithmetic for convolutional layers and then apply it to quantify the number of operations in a DSC block, providing a concrete measure of its computational footprint [@problem_id:3115192].", "problem": "A Depthwise Separable Convolution (DSC) consists of a depthwise convolution followed by a pointwise convolution. The depthwise convolution applies one spatial filter per input channel, and the pointwise convolution uses filters of spatial size $1 \\times 1$ to mix channels. Consider a two-dimensional discrete convolution and use the following fundamental bases:\n\n- The two-dimensional discrete convolution of a single-channel input $x$ with a kernel $w$ at spatial location $(u,v)$ computes a weighted sum over a local receptive field of $x$.\n- Zero-padding of size $p$ extends the input by $p$ zeros on each spatial boundary, and stride $s$ advances the convolution window by $s$ pixels along each spatial axis.\n\nFrom these bases, derive the counting argument that determines how many valid spatial positions the depthwise convolution can produce along height and width when applied to an input of spatial size $(H,W)$ with a square kernel of size $k \\times k$, stride $s$, and symmetric zero-padding $p$.\n\nThen, apply your derivation to the following DSC block:\n\n- Input tensor has height $H = 128$, width $W = 96$, and number of channels $C_{\\text{in}} = 32$.\n- Depthwise convolution uses a square kernel of size $k = 5$, stride $s = 2$, and symmetric zero-padding $p = 1$ on both height and width.\n- Pointwise convolution uses $C_{\\text{out}} = 64$ output channels, stride $1$, and zero-padding $0$.\n\nAssume no dilation, and that the pointwise convolution preserves the spatial dimensions produced by the depthwise convolution. Define one multiply-add operation as one multiplication followed immediately by one addition, counted once per weight-activation pair contributing to a single output value. Bias terms are not counted.\n\nCompute the total number of multiply-add operations required to process one input image through the entire DSC block described above (depthwise stage plus pointwise stage). Express your final answer as a single integer with no units and no rounding. Only provide this total count as your final answer.", "solution": "The problem asks for two tasks: first, to derive the formula for the spatial output dimensions of a 2D convolution, and second, to apply this to calculate the total number of multiply-add operations for a specific Depthwise Separable Convolution (DSC) block.\n\nFirst, we derive the formula for the output dimensions. Consider a single spatial dimension of an input tensor, with size $D_{in}$. When symmetric zero-padding of size $p$ is applied, $p$ zeros are added to each of the two ends of this dimension. The effective size of the padded dimension becomes $D_{padded} = D_{in} + 2p$. A kernel of size $k$ is convolved over this padded dimension with a stride of $s$.\n\nLet the positions of the kernel be indexed by $i$, starting from $i=0$. The receptive field covered by the kernel at position $i$ starts at index $i \\times s$ and ends at index $i \\times s + k - 1$ (using $0$-based indexing). For the kernel to be placed at a valid position, its entire receptive field must lie within the padded dimension. Therefore, the last index covered must be less than the padded dimension size:\n$$i \\times s + k - 1  D_{padded}$$\n$$i \\times s \\le D_{padded} - k$$\n$$i \\le \\frac{D_{padded} - k}{s}$$\nSince $i$ must be an integer, the maximum value for the index $i$ is $\\lfloor \\frac{D_{padded} - k}{s} \\rfloor$. The number of valid positions is the count of possible integer values for $i$, which range from $0$ to $\\lfloor \\frac{D_{padded} - k}{s} \\rfloor$. The total number of such positions, which corresponds to the output dimension $D_{out}$, is:\n$$D_{out} = \\left\\lfloor \\frac{D_{padded} - k}{s} \\right\\rfloor + 1$$\nSubstituting $D_{padded} = D_{in} + 2p$, we obtain the general formula for an output spatial dimension:\n$$D_{out} = \\left\\lfloor \\frac{D_{in} + 2p - k}{s} \\right\\rfloor + 1$$\nApplying this to the height ($H$) and width ($W$) dimensions, we get:\n$$H_{out} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\n$$W_{out} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\nThis completes the first part of the problem.\n\nNext, we compute the total number of multiply-add operations for the given DSC block. The total cost, $\\text{Ops}_{\\text{total}}$, is the sum of the costs of the depthwise convolution stage, $\\text{Ops}_{\\text{dw}}$, and the pointwise convolution stage, $\\text{Ops}_{\\text{pw}}$.\n\n**1. Depthwise Convolution Stage**\nThe input tensor has dimensions $H = 128$, $W = 96$, and $C_{\\text{in}} = 32$.\nThe depthwise convolution parameters are: kernel size $k = 5$, stride $s = 2$, and padding $p = 1$.\nFirst, we compute the spatial dimensions of the output feature map from this stage, which we denote as $(H_{\\text{dw}}, W_{\\text{dw}})$:\n$$H_{\\text{dw}} = \\left\\lfloor \\frac{128 + 2(1) - 5}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{125}{2} \\right\\rfloor + 1 = 62 + 1 = 63$$\n$$W_{\\text{dw}} = \\left\\lfloor \\frac{96 + 2(1) - 5}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{93}{2} \\right\\rfloor + 1 = 46 + 1 = 47$$\nThe depthwise convolution applies a distinct $k \\times k$ filter to each of the $C_{\\text{in}}$ input channels. To compute a single value in one output channel's feature map requires $k \\times k$ multiply-add operations. This is repeated for all $H_{\\text{dw}} \\times W_{\\text{dw}}$ spatial locations and for all $C_{\\text{in}}$ channels.\nThe total number of operations for the depthwise stage is:\n$$\\text{Ops}_{\\text{dw}} = C_{\\text{in}} \\times k \\times k \\times H_{\\text{dw}} \\times W_{\\text{dw}}$$\nSubstituting the given values:\n$$\\text{Ops}_{\\text{dw}} = 32 \\times 5 \\times 5 \\times 63 \\times 47$$\n$$\\text{Ops}_{\\text{dw}} = 32 \\times 25 \\times 2961$$\n$$\\text{Ops}_{\\text{dw}} = 800 \\times 2961 = 2,368,800$$\n\n**2. Pointwise Convolution Stage**\nThe input to this stage is the output of the depthwise stage, with dimensions $(H_{\\text{in}}, W_{\\text{in}}, C_{\\text{in}}) = (63, 47, 32)$.\nThe pointwise convolution is a standard convolution with $1 \\times 1$ filters. Its parameters are: kernel size $k_{pw} = 1$, stride $s_{pw} = 1$, padding $p_{pw} = 0$, and number of output channels $C_{\\text{out}} = 64$. The spatial dimensions are preserved, which is consistent with these parameters.\nTo calculate one value at a single spatial position in one of the $C_{\\text{out}}$ output channels, a dot product is performed across the $C_{\\text{in}}$ input channels. This requires $1 \\times 1 \\times C_{\\text{in}} = C_{\\text{in}}$ multiply-add operations. This computation is repeated for all spatial locations in the output map ($H_{\\text{dw}} \\times W_{\\text{dw}}$) and for every output channel ($C_{\\text{out}}$).\nThe total number of operations for the pointwise stage is:\n$$\\text{Ops}_{\\text{pw}} = C_{\\text{in}} \\times C_{\\text{out}} \\times H_{\\text{dw}} \\times W_{\\text{dw}}$$\nSubstituting the values:\n$$\\text{Ops}_{\\text{pw}} = 32 \\times 64 \\times 63 \\times 47$$\n$$\\text{Ops}_{\\text{pw}} = 2048 \\times 2961 = 6,064,128$$\n\n**Total Operations**\nThe total number of multiply-add operations for the entire DSC block is the sum of the operations from both stages:\n$$\\text{Ops}_{\\text{total}} = \\text{Ops}_{\\text{dw}} + \\text{Ops}_{\\text{pw}} = 2,368,800 + 6,064,128 = 8,432,928$$", "answer": "$$\\boxed{8432928}$$", "id": "3115192"}, {"introduction": "Having mastered the operational cost, we now explore the underlying mathematical structure of DSC and its relationship to standard convolution. This practice is a thought experiment designed to reveal that DSC is not an entirely new operation, but rather a factorized or constrained form of a standard convolution. By constructing a specific scenario where the outputs of both operations are identical, you will gain a deeper intuition for what a separable convolution truly represents in terms of linear transformations [@problem_id:3185403].", "problem": "A Convolutional Neural Network (CNN) layer performs forward propagation by applying spatial aggregations defined by convolutional kernels. Consider a synthetic input tensor with $C = 2$ channels and spatial dimensions $H = W = 3$, where each channel carries an orthogonal pattern under the Frobenius inner product. The input channels are\n$$\nX^{(1)} = \\begin{pmatrix}\n1  0  -1 \\\\\n1  0  -1 \\\\\n1  0  -1\n\\end{pmatrix},\n\\qquad\nX^{(2)} = \\begin{pmatrix}\n1  1  1 \\\\\n0  0  0 \\\\\n-1  -1  -1\n\\end{pmatrix}.\n$$\nYou will compute the forward pass of a depthwise separable convolution and compare it to a full convolution, both using the same stride and padding conventions.\n\nDepthwise separable convolution is defined here as follows:\n- Depthwise step: For each channel $c \\in \\{1,2\\}$, convolve $X^{(c)}$ with a channel-specific spatial kernel $K^{(c)}$ using stride $1$ and valid padding, producing a scalar $d^{(c)} \\in \\mathbb{R}$ because the kernel and the input have the same spatial size.\n- Pointwise step: Linearly mix the depthwise outputs with a $1 \\times 1$ pointwise weight vector $p = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$ to obtain a single-output scalar $y_{\\mathrm{dw}} = \\alpha \\, d^{(1)} + \\beta \\, d^{(2)}$.\n\nFull convolution in this setup is defined as a single-output convolution with a kernel $W$ that has one spatial slice per input channel, applied with stride $1$ and valid padding to yield a scalar $y_{\\mathrm{full}}$.\n\nUse the following specific, scientifically consistent choices:\n- Depthwise kernels are equal to the channel patterns: $K^{(1)} = X^{(1)}$ and $K^{(2)} = X^{(2)}$.\n- Pointwise mixing weights are $\\alpha = 2$ and $\\beta = -3$.\n- Full convolution kernel $W$ is chosen to have channel slices $W^{(1)} = \\alpha \\, K^{(1)}$ and $W^{(2)} = \\beta \\, K^{(2)}$.\n\nStarting from the core definition of forward propagation as linear aggregation via convolution, compute both outputs $y_{\\mathrm{dw}}$ and $y_{\\mathrm{full}}$ and then determine the squared absolute difference\n$$\n\\Delta = \\left| y_{\\mathrm{dw}} - y_{\\mathrm{full}} \\right|^{2}.\n$$\nExpress your final answer as a single real number. No rounding is required.", "solution": "The problem requires the computation of two quantities, $y_{\\mathrm{dw}}$ and $y_{\\mathrm{full}}$, resulting from depthwise separable convolution and full convolution, respectively, and then finding the squared difference between them.\n\nFirst, we must formalize the convolution operation as described. The input channels $X^{(c)}$ and the spatial kernels $K^{(c)}$ have dimensions $3 \\times 3$. The convolution is performed with a stride of $1$ and \"valid\" padding. For an input of size $N \\times N$ and a kernel of size $F \\times F$, with stride $S$ and padding $P$, the output spatial dimension is given by $\\lfloor \\frac{N - F + 2P}{S} \\rfloor + 1$. With $N=3$, $F=3$, $S=1$, and $P=0$ (for valid padding), the output dimension is $\\lfloor \\frac{3 - 3 + 0}{1} \\rfloor + 1 = 1$. This means the output of the spatial convolution for each channel is a single scalar. The value of this scalar is the sum of the element-wise products of the kernel and the input matrix, which is precisely the definition of the Frobenius inner product, $\\langle A, B \\rangle_F = \\sum_{i,j} A_{ij} B_{ij}$.\n\nLet's begin by computing the output of the depthwise separable convolution, $y_{\\mathrm{dw}}$. This process consists of two steps: a depthwise step and a pointwise step.\n\n**1. Depthwise Step:**\nFor each input channel $c$, we convolve the input $X^{(c)}$ with its corresponding depthwise kernel $K^{(c)}$. The output is a scalar $d^{(c)}$.\n$$\nd^{(c)} = \\text{conv}(X^{(c)}, K^{(c)}) = \\langle X^{(c)}, K^{(c)} \\rangle_F\n$$\nThe problem specifies that the depthwise kernels are equal to the input channel patterns themselves: $K^{(1)} = X^{(1)}$ and $K^{(2)} = X^{(2)}$.\n\nFor the first channel ($c=1$):\n$$\nd^{(1)} = \\langle X^{(1)}, K^{(1)} \\rangle_F = \\langle X^{(1)}, X^{(1)} \\rangle_F = \\|X^{(1)}\\|_F^2\n$$\nGiven the matrix for $X^{(1)}$:\n$$\nX^{(1)} = \\begin{pmatrix}\n1  0  -1 \\\\\n1  0  -1 \\\\\n1  0  -1\n\\end{pmatrix}\n$$\nThe squared Frobenius norm is the sum of the squares of its elements:\n$$\nd^{(1)} = 1^2 + 0^2 + (-1)^2 + 1^2 + 0^2 + (-1)^2 + 1^2 + 0^2 + (-1)^2 = 1 + 0 + 1 + 1 + 0 + 1 + 1 + 0 + 1 = 6\n$$\n\nFor the second channel ($c=2$):\n$$\nd^{(2)} = \\langle X^{(2)}, K^{(2)} \\rangle_F = \\langle X^{(2)}, X^{(2)} \\rangle_F = \\|X^{(2)}\\|_F^2\n$$\nGiven the matrix for $X^{(2)}$:\n$$\nX^{(2)} = \\begin{pmatrix}\n1  1  1 \\\\\n0  0  0 \\\\\n-1  -1  -1\n\\end{pmatrix}\n$$\nThe squared Frobenius norm is:\n$$\nd^{(2)} = 1^2 + 1^2 + 1^2 + 0^2 + 0^2 + 0^2 + (-1)^2 + (-1)^2 + (-1)^2 = 1 + 1 + 1 + 0 + 0 + 0 + 1 + 1 + 1 = 6\n$$\n\n**2. Pointwise Step:**\nThe scalars from the depthwise step, $d^{(1)}=6$ and $d^{(2)}=6$, are linearly combined using the pointwise weight vector $p = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$. The mixing weights are given as $\\alpha = 2$ and $\\beta = -3$.\n$$\ny_{\\mathrm{dw}} = \\alpha \\, d^{(1)} + \\beta \\, d^{(2)}\n$$\nSubstituting the values:\n$$\ny_{\\mathrm{dw}} = (2)(6) + (-3)(6) = 12 - 18 = -6\n$$\n\nNext, we compute the output of the full convolution, $y_{\\mathrm{full}}$.\nThe full convolution operates on the entire input volume (all channels) with a 3D kernel $W$ that has spatial slices $W^{(c)}$ for each input channel $c$. Since the output is a single scalar, the operation is the sum of the Frobenius inner products of corresponding input and kernel slices over all channels.\n$$\ny_{\\mathrm{full}} = \\sum_{c=1}^{2} \\langle X^{(c)}, W^{(c)} \\rangle_F = \\langle X^{(1)}, W^{(1)} \\rangle_F + \\langle X^{(2)}, W^{(2)} \\rangle_F\n$$\nThe problem specifies the kernel slices for the full convolution as $W^{(1)} = \\alpha \\, K^{(1)}$ and $W^{(2)} = \\beta \\, K^{(2)}$. We also know that $K^{(1)} = X^{(1)}$ and $K^{(2)} = X^{(2)}$. Therefore, we can write the kernel slices as:\n$$\nW^{(1)} = \\alpha \\, X^{(1)}\n\\quad \\text{and} \\quad\nW^{(2)} = \\beta \\, X^{(2)}\n$$\nSubstituting these into the expression for $y_{\\mathrm{full}}$:\n$$\ny_{\\mathrm{full}} = \\langle X^{(1)}, \\alpha X^{(1)} \\rangle_F + \\langle X^{(2)}, \\beta X^{(2)} \\rangle_F\n$$\nUsing the linearity property of the inner product, which states that $\\langle A, k B \\rangle = k \\langle A, B \\rangle$ for a scalar $k$:\n$$\ny_{\\mathrm{full}} = \\alpha \\langle X^{(1)}, X^{(1)} \\rangle_F + \\beta \\langle X^{(2)}, X^{(2)} \\rangle_F\n$$\nThis expression is equivalent to:\n$$\ny_{\\mathrm{full}} = \\alpha \\|X^{(1)}\\|_F^2 + \\beta \\|X^{(2)}\\|_F^2\n$$\nWe recognize that $\\|X^{(1)}\\|_F^2 = d^{(1)}$ and $\\|X^{(2)}\\|_F^2 = d^{(2)}$. Thus, the expression for $y_{\\mathrm{full}}$ is identical to the one for $y_{\\mathrm{dw}}$:\n$$\ny_{\\mathrm{full}} = \\alpha \\, d^{(1)} + \\beta \\, d^{(2)} = y_{\\mathrm{dw}}\n$$\nThis demonstrates that for the specific construction of the kernels in this problem, the full convolution is mathematically equivalent to the depthwise separable convolution.\nNumerically, we have:\n$$\ny_{\\mathrm{full}} = -6\n$$\n\nFinally, we compute the desired quantity, $\\Delta$, which is the squared absolute difference between $y_{\\mathrm{dw}}$ and $y_{\\mathrm{full}}$.\n$$\n\\Delta = | y_{\\mathrm{dw}} - y_{\\mathrm{full}} |^2\n$$\nSince $y_{\\mathrm{dw}} = y_{\\mathrm{full}} = -6$:\n$$\n\\Delta = | -6 - (-6) |^2 = |0|^2 = 0\n$$\nThe squared absolute difference is $0$. The orthogonality of the input channels $X^{(1)}$ and $X^{(2)}$ is a property of the provided data but was not directly required to establish the equality of $y_{\\mathrm{dw}}$ and $y_{\\mathrm{full}}$ under the problem's specific kernel definitions. This equality arose because the full convolution kernel $W$ was constructed to be \"separable\" in exactly the same way as the depthwise separable convolution was defined.", "answer": "$$\\boxed{0}$$", "id": "3185403"}, {"introduction": "Finally, we move from theory to a nuanced aspect of practical network design: the placement of nonlinear activation functions. The interaction between the separable convolution and nonlinearities like the Rectified Linear Unit (ReLU) can significantly alter a model's representational power. This hands-on coding problem allows you to investigate how different ordering schemes—applying ReLU before or after the pointwise mixing—enable a layer to learn fundamentally different types of functions, from simple linear classifiers to more complex, piecewise-linear decision boundaries [@problem_id:3115159].", "problem": "Consider one-dimensional depthwise separable convolution under the following simplifying but scientifically sound regime: the spatial kernel size is $1$, the depthwise filter is the identity per channel, and the operation reduces to per-channel bias followed by cross-channel mixing. Begin from the core definitions: convolution as linear shift-invariant mapping, the depthwise separable structure as per-channel convolution followed by a $1 \\times 1$ pointwise mixing across channels, and the Rectified Linear Unit (ReLU) nonlinearity defined as $\\,\\mathrm{ReLU}(t)=\\max\\{0,t\\}\\,$. Under these constraints, there are two orderings of nonlinearity placement to compare: (A) ReLU after depthwise and before pointwise, and (B) ReLU after pointwise. Your tasks are to derive the functional class each ordering represents and to empirically test their representational consequences on a synthetic separability benchmark. Work in purely mathematical terms with an input at a single spatial location of $C=2$ channels, denoted $x=(x_1,x_2) \\in \\mathbb{R}^2$.\n\nDerivation task:\n- From the core definitions above, derive the functional form of ordering (A) and ordering (B) when the depthwise filter is the identity and the kernel size is $1$. Carefully identify the linear maps and the placement of the ReLU. Do not assume any shortcut formula; start from the linear per-channel transformation plus bias, then the cross-channel linear mixing, and place the ReLU according to each ordering.\n\nBenchmark task:\n- Define classification by threshold at $0$: a model predicts class $1$ for an input $x$ if its scalar output is greater than or equal to $0$, and class $0$ otherwise. Use three synthetic test cases that probe the role of nonlinearity placement:\n    1. Case $1$ (mixing-needed halfspace): Inputs are all pairs $(x_1,x_2)$ from the grid $S=\\{-1.0,-0.5,0.0,0.5,1.0\\}$, yielding $25$ points. The target class is $1$ if $x_1 - x_2 \\ge 0$ and $0$ otherwise. This target requires mixing before thresholding.\n    2. Case $2$ (sum of positive parts): Inputs are the same $25$ points from the grid $S$. The target class is $1$ if $\\mathrm{ReLU}(x_1) + 2\\,\\mathrm{ReLU}(x_2) - 1.0 \\ge 0$ and $0$ otherwise. This target requires per-channel rectification before mixing.\n    3. Case $3$ (nonnegative boundary equivalence): Inputs are all pairs $(x_1,x_2)$ from the grid $S_+=\\{0.0,0.5,1.0\\}$, yielding $9$ points. The target class is $1$ if $x_1 + 2\\,x_2 - 1.0 \\ge 0$ and $0$ otherwise. With nonnegative inputs, per-channel rectification has no effect, so the two orderings should be equivalent in representational capacity for this halfspace.\n\nEvaluation protocol:\n- For each case and each ordering, search over an integer parameter grid to find the best possible classification accuracy on the given inputs under the classification rule described above. The parameterization is:\n    - Depthwise biases $b_{d,1}, b_{d,2} \\in \\{-1,0,1\\}$.\n    - Pointwise mixing weights $w_1, w_2 \\in \\{-2,-1,0,1,2\\}$.\n    - Output bias $b_p \\in \\{-2,-1,0,1,2\\}$.\n- For ordering (A), the scalar output is the cross-channel linear mixing applied to the per-channel rectified and biased inputs, plus the output bias, with no further nonlinearity. Predict class $1$ if the output is $\\ge 0$ and class $0$ otherwise.\n- For ordering (B), the scalar output is the rectified (via ReLU) result of the cross-channel linear mixing of biased inputs plus the output bias. Predict class $1$ if the output is $\\ge 0$ and class $0$ otherwise, which is equivalent to checking whether the pre-ReLU sum is $\\ge 0$.\n\nTest suite and final output specification:\n- Implement both orderings and evaluate the best achievable classification accuracy over the specified parameter grid for each of the three cases.\n- Your program should produce a single line of output containing the six accuracies in the following order: $[\\text{A1},\\text{B1},\\text{A2},\\text{B2},\\text{A3},\\text{B3}]$, where $\\text{A}k$ is the best accuracy for ordering (A) on case $k$, and $\\text{B}k$ is the best accuracy for ordering (B) on case $k$.\n- The final outputs are floats in $[0,1]$, each equal to the fraction of inputs correctly classified under the best parameters found for that ordering and case. There are no physical units or angle units involved.\n\nYour solution must start from the foundational definitions of convolution, linear mixing, and the Rectified Linear Unit (ReLU), derive the functional classes for the two orderings, and implement the described benchmark and search protocol to produce the specified output.", "solution": "The problem requires a derivation of the functional forms for two variants of a simplified depthwise separable convolution layer, followed by an empirical evaluation on synthetic classification tasks. The validation confirms the problem is scientifically sound, well-posed, and all necessary information for a complete solution is provided.\n\n### Derivation of Functional Forms\n\nThe context is a depthwise separable convolution with a spatial kernel size of $1$, performed on an input at a single spatial location with $C=2$ channels, denoted by the vector $x = (x_1, x_2)^T \\in \\mathbb{R}^2$. The operation consists of a depthwise stage followed by a pointwise stage.\n\n**1. Stage Definitions**\n\n*   **Depthwise Stage**: The problem specifies that the depthwise filter is the identity and the kernel size is $1$. This simplifies the per-channel convolution to a function that applies a bias to each channel independently. Let the depthwise biases be $b_d = (b_{d,1}, b_{d,2})^T$. The output of this stage, $x'$, is given by:\n    $$\n    x' = \\begin{pmatrix} x'_1 \\\\ x'_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot x_1 + b_{d,1} \\\\ 1 \\cdot x_2 + b_{d,2} \\end{pmatrix} = \\begin{pmatrix} x_1 + b_{d,1} \\\\ x_2 + b_{d,2} \\end{pmatrix}\n    $$\n\n*   **Pointwise Stage**: This stage performs a $1 \\times 1$ convolution, which reduces to a linear combination of the input channels to produce a single scalar output. Let the input to this stage be a vector $z = (z_1, z_2)^T$. The pointwise weights are $w = (w_1, w_2)$ and there is a single output bias $b_p$. The output is:\n    $$\n    y_{\\text{out}} = w_1 z_1 + w_2 z_2 + b_p\n    $$\n\n*   **Nonlinearity**: The Rectified Linear Unit (ReLU) is defined as $\\mathrm{ReLU}(t) = \\max\\{0, t\\}$.\n\nWe now derive the functional forms for the two specified orderings of nonlinearity placement.\n\n**2. Ordering (A): ReLU after Depthwise, before Pointwise**\n\nIn this configuration, the ReLU activation is applied to the output of the depthwise stage, and the result is then fed into the pointwise stage.\n\n1.  The input $x=(x_1, x_2)^T$ passes through the depthwise stage, resulting in $x'=(x_1+b_{d,1}, x_2+b_{d,2})^T$.\n2.  The ReLU function is applied element-wise to $x'$:\n    $$\n    x'' = \\begin{pmatrix} \\mathrm{ReLU}(x_1 + b_{d,1}) \\\\ \\mathrm{ReLU}(x_2 + b_{d,2}) \\end{pmatrix}\n    $$\n3.  The vector $x''$ is the input to the pointwise stage ($z=x''$). The final scalar output, which we denote $f_A(x)$, is:\n    $$\n    f_A(x; b_d, w, b_p) = w_1 \\mathrm{ReLU}(x_1 + b_{d,1}) + w_2 \\mathrm{ReLU}(x_2 + b_{d,2}) + b_p\n    $$\nThe functional class represented by ordering (A) is a linear combination of shifted and rectified inputs. This is the form of a simple two-layer neural network with a hidden layer of two ReLU neurons and a linear output unit. The decision boundary defined by $f_A(x) = 0$ is piecewise linear, allowing it to approximate non-linear and non-convex decision regions.\n\n**3. Ordering (B): ReLU after Pointwise**\n\nIn this configuration, the depthwise and pointwise stages are composed linearly first, and the ReLU activation is applied only to the final scalar output.\n\n1.  The input $x=(x_1, x_2)^T$ passes through the depthwise stage, resulting in $x'=(x_1+b_{d,1}, x_2+b_{d,2})^T$.\n2.  The vector $x'$ is the input to the pointwise stage ($z=x'$). The pre-activation output is:\n    $$\n    y_{\\text{pre}} = w_1(x_1 + b_{d,1}) + w_2(x_2 + b_{d,2}) + b_p\n    $$\n    We can rearrange this into the standard form of a linear function of $x_1$ and $x_2$:\n    $$\n    y_{\\text{pre}} = w_1 x_1 + w_2 x_2 + (w_1 b_{d,1} + w_2 b_{d,2} + b_p)\n    $$\n3.  The ReLU function is applied to this scalar value. The final output, $f_B(x)$, is:\n    $$\n    f_B(x; b_d, w, b_p) = \\mathrm{ReLU}(y_{\\text{pre}}) = \\mathrm{ReLU}(w_1 x_1 + w_2 x_2 + (w_1 b_{d,1} + w_2 b_{d,2} + b_p))\n    $$\nThe classification rule is to predict class $1$ if $f_B(x) \\ge 0$. As $\\mathrm{ReLU}(t) \\ge 0$ if and only if $t \\ge 0$, this is equivalent to checking if $y_{\\text{pre}} \\ge 0$. The decision boundary is therefore $y_{\\text{pre}}=0$, which is the equation of a line in the $(x_1, x_2)$ plane. Thus, for any choice of parameters, ordering (B) acts as a linear classifier, capable of learning only hyperplane decision boundaries.\n\n### Benchmark Task Analysis and Implementation Strategy\n\nThe benchmark tasks are designed to probe the differing representational capacities of these two functional forms.\n\n*   **Case 1 (mixing-needed halfspace):** The target is $x_1 - x_2 \\ge 0$. This is a linear decision boundary. As derived, ordering (B) is inherently a linear classifier and should be able to solve this perfectly. Ordering (A) can also represent this linear function, for example, by choosing depthwise biases $b_{d,1}, b_{d,2}$ large enough such that for all inputs on the grid, the arguments to the ReLUs are always positive. In this case, $\\mathrm{ReLU}(x_i+b_{d,i}) = x_i+b_{d,i}$, and the model becomes linear. Both models are expected to achieve perfect accuracy.\n\n*   **Case 2 (sum of positive parts):** The target is $\\mathrm{ReLU}(x_1) + 2\\mathrm{ReLU}(x_2) - 1.0 \\ge 0$. The form of this target function matches the structure of ordering (A) precisely. By selecting parameters $b_{d,1}=0, b_{d,2}=0, w_1=1, w_2=2, b_p=-1$, all of which are available in the specified search grid, ordering (A) can perfectly represent the target function. In contrast, this decision boundary is non-linear, so the linear classifier represented by ordering (B) will be unable to achieve perfect accuracy.\n\n*   **Case 3 (nonnegative boundary equivalence):** The target is $x_1 + 2x_2 - 1.0 \\ge 0$, and all inputs $(x_1, x_2)$ have $x_i \\ge 0$. For ordering (A), if we select $b_{d,1}=0$ and $b_{d,2}=0$, the function becomes $f_A(x) = w_1\\mathrm{ReLU}(x_1) + w_2\\mathrm{ReLU}(x_2) + b_p$. Since $x_i \\ge 0$, $\\mathrm{ReLU}(x_i) = x_i$, so the function simplifies to $f_A(x) = w_1 x_1 + w_2 x_2 + b_p$. This is identical to the pre-activation function of ordering (B) (with $b_{d,1}=b_{d,2}=0$). Since ordering (A)'s parameter search space contains the parameters that make it equivalent to ordering (B), and the target is a linear separator, both models have the capacity to solve this task perfectly.\n\nThe implementation will proceed by exhaustively searching the discrete parameter grid for each of the six scenarios (3 cases $\\times$ 2 orderings). For each combination of parameters, the model's accuracy is calculated on the corresponding dataset. The maximum accuracy found across all parameter combinations is reported for each scenario.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Derives and empirically tests two orderings of nonlinearity placement\n    in a simplified depthwise separable convolution.\n    \"\"\"\n\n    # Define parameter grids\n    b_d_vals = [-1, 0, 1]\n    w_vals = [-2, -1, 0, 1, 2]\n    b_p_vals = [-2, -1, 0, 1, 2]\n    \n    b_d_pairs = list(itertools.product(b_d_vals, repeat=2))\n    w_pairs = list(itertools.product(w_vals, repeat=2))\n\n    # Define input grids for test cases\n    S = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    S_plus = np.array([0.0, 0.5, 1.0])\n\n    inputs_1_2 = np.array(list(itertools.product(S, repeat=2)))\n    inputs_3 = np.array(list(itertools.product(S_plus, repeat=2)))\n    \n    # Define ground truth labels for test cases\n    # Case 1: mixing-needed halfspace\n    x1, x2 = inputs_1_2[:, 0], inputs_1_2[:, 1]\n    y_true_1 = (x1 - x2 >= 0).astype(int)\n\n    # Case 2: sum of positive parts\n    y_true_2 = (np.maximum(0, x1) + 2 * np.maximum(0, x2) - 1.0 >= 0).astype(int)\n\n    # Case 3: nonnegative boundary equivalence\n    x1_3, x2_3 = inputs_3[:, 0], inputs_3[:, 1]\n    y_true_3 = (x1_3 + 2 * x2_3 - 1.0 >= 0).astype(int)\n\n    test_cases = [\n        (inputs_1_2, y_true_1),\n        (inputs_1_2, y_true_2),\n        (inputs_3, y_true_3)\n    ]\n\n    final_results = []\n    \n    # ReLU function\n    def relu(t):\n        return np.maximum(0, t)\n\n    for case_inputs, y_true in test_cases:\n        max_acc_A = 0.0\n        max_acc_B = 0.0\n        \n        num_points = len(case_inputs)\n        x1_data, x2_data = case_inputs[:, 0], case_inputs[:, 1]\n\n        # Exhaustive search over the parameter grid\n        for bd1, bd2 in b_d_pairs:\n            for w1, w2 in w_pairs:\n                for a_bp in b_p_vals: # 'a_bp' to distinguish from the var name in the loop\n                    bp = a_bp\n                    \n                    # Ordering (A): ReLU after depthwise, before pointwise\n                    output_A = w1 * relu(x1_data + bd1) + w2 * relu(x2_data + bd2) + bp\n                    preds_A = (output_A >= 0).astype(int)\n                    acc_A = np.sum(preds_A == y_true) / num_points\n                    if acc_A > max_acc_A:\n                        max_acc_A = acc_A\n\n                    # Ordering (B): ReLU after pointwise\n                    # Classification is based on the pre-ReLU value being >= 0\n                    output_B_pre = w1 * (x1_data + bd1) + w2 * (x2_data + bd2) + bp\n                    preds_B = (output_B_pre >= 0).astype(int)\n                    acc_B = np.sum(preds_B == y_true) / num_points\n                    if acc_B > max_acc_B:\n                        max_acc_B = acc_B\n        \n        final_results.append(max_acc_A)\n        final_results.append(max_acc_B)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3115159"}]}