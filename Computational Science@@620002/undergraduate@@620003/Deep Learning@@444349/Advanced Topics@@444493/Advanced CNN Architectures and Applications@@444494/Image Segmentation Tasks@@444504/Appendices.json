{"hands_on_practices": [{"introduction": "The choice of a loss function is fundamental to training any deep learning model, and this is especially true for image segmentation. This first practice will guide you through deriving and implementing three cornerstone loss functions: pixel-wise cross-entropy ($L_{CE}$), focal loss ($L_{focal}$), and Dice loss ($L_{Dice}$). By analyzing their mathematical forms and gradient behaviors, you will gain a deep understanding of how they address the critical challenge of class imbalance, a common scenario in medical and autonomous driving applications [@problem_id:3136332].", "problem": "You are tasked with deriving, analyzing, and implementing three loss functions for binary semantic segmentation at the pixel level, suitable for highly imbalanced data where the foreground class is rare. Consider a segmentation setting in which each pixel is modeled as an independent Bernoulli random variable with target $y \\in \\{0,1\\}$ and predicted probability $p \\in (0,1)$ produced by a logistic function $p = \\sigma(z)$ with $z \\in \\mathbb{R}$ and $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. Start from the Maximum Likelihood Estimation principle for independent Bernoulli variables and the definition of negative log-likelihood as a loss. Derive the following three loss functions and their gradients with respect to the logit $z$:\n\n1. Pixel-wise cross-entropy loss $L_{CE}$ based on the Bernoulli negative log-likelihood.\n2. Focal loss $L_{focal}$ for binary classification with a focusing parameter $\\gamma \\ge 0$ and class-balancing factor $\\alpha \\in (0,1)$.\n3. Dice loss $L_{Dice}$ based on the soft Sørensen–Dice coefficient, with a strictly positive smoothing constant $s$ to ensure numerical stability.\n\nFor each loss, you must:\n- Derive the analytical expression of the loss from first principles using the Bernoulli likelihood and appropriate definitions, without shortcutting to known final formulas.\n- Use the chain rule to derive the gradient with respect to the logit $z$, assuming $p = \\sigma(z)$ and $\\frac{dp}{dz} = p(1-p)$.\n- Analyze and explain the behavior of the gradient magnitude near severe class imbalance when the prediction probability for the positive class satisfies $p \\ll 1$, contrasting the behavior for $y=1$ (positive pixels) and $y=0$ (negative pixels).\n\nImplementation requirements:\n- Implement numerically stable versions of the three losses and their gradients with respect to $z$ for arrays of pixels. Use clamping of probabilities at a small $\\epsilon$ to avoid undefined logarithms.\n- Compute, for each test case, the mean loss per pixel and the mean absolute gradient magnitude with respect to $z$ separated for positive pixels ($y=1$) and negative pixels ($y=0$).\n\nTest suite:\n- Construct synthetic segmentation datasets with specified total pixels $N$, positive class ratio $r$, and constant predicted probabilities $p_{pos}$ for positive pixels and $p_{neg}$ for negative pixels. Use the following four test cases:\n    1. Case $1$: $N = 100$, $r = 0.2$, $p_{pos} = 0.6$, $p_{neg} = 0.4$, focal parameters $\\gamma = 2$, $\\alpha = 0.25$, dice smoothing $s = 1.0$.\n    2. Case $2$ (severe class imbalance and $p \\ll 1$): $N = 1000$, $r = 0.01$, $p_{pos} = 0.01$, $p_{neg} = 0.01$, focal parameters $\\gamma = 2$, $\\alpha = 0.25$, dice smoothing $s = 1.0$.\n    3. Case $3$ (near-perfect predictions): $N = 500$, $r = 0.1$, $p_{pos} = 0.999$, $p_{neg} = 0.001$, focal parameters $\\gamma = 2$, $\\alpha = 0.25$, dice smoothing $s = 1.0$.\n    4. Case $4$ (focal with $\\gamma = 0$ to compare with cross-entropy weighting): $N = 200$, $r = 0.3$, $p_{pos} = 0.7$, $p_{neg} = 0.3$, focal parameters $\\gamma = 0$, $\\alpha = 0.25$, dice smoothing $s = 1.0$.\n\nOutput specification:\n- For each test case, compute a nested list containing three sublists, one per loss in the order [$L_{CE}$, $L_{focal}$, $L_{Dice}$]. Each sublist must have the form $[\\text{mean loss}, \\text{mean absolute gradient for } y=1, \\text{mean absolute gradient for } y=0]$, all as floating-point numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. The top-level list must contain one element per test case, in the same order as above. For example, the format must be $[[l_{11},g_{11}^+,g_{11}^-],[l_{12},g_{12}^+,g_{12}^-],[l_{13},g_{13}^+,g_{13}^-]]$ for three test cases, extended appropriately for four cases.\n\nNo physical units are involved; all results are dimensionless floats. Express all quantities as decimals.", "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, well-posed with all necessary data and definitions, and expressed in objective, formal language. We proceed with the derivations, analysis, and implementation.\n\nThe foundation for this analysis is the modeling of each pixel in a binary semantic segmentation task as an independent Bernoulli trial. The true label for a pixel is $y \\in \\{0, 1\\}$, and the model's predicted probability for the positive class ($y=1$) is $p \\in (0,1)$. This probability is the output of a logistic sigmoid function applied to a logit $z \\in \\mathbb{R}$, such that $p = \\sigma(z) = (1 + e^{-z})^{-1}$. A crucial property of the sigmoid function is its simple derivative with respect to its input: $\\frac{dp}{dz} = \\frac{d\\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$.\n\nThe likelihood for a single pixel observation is given by the Bernoulli probability mass function: $P(y|p) = p^y(1-p)^{1-y}$. For an image with $N$ pixels, assuming independence, the total likelihood is the product of individual likelihoods: $\\mathcal{L} = \\prod_{i=1}^{N} p_i^{y_i}(1-p_i)^{1-y_i}$.\n\nIn machine learning, it is standard practice to maximize the log-likelihood or, equivalently, minimize the negative log-likelihood (NLL). The NLL for the entire image is:\n$$ \\text{NLL} = -\\log(\\mathcal{L}) = -\\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] $$\nThe per-pixel contribution to the NLL forms the basis of the cross-entropy loss.\n\n### 1. Pixel-wise Cross-Entropy Loss ($L_{CE}$)\n\n#### Derivation\nThe cross-entropy loss for a single pixel is derived directly from the negative log-likelihood of a single Bernoulli trial.\n$$ L_{CE}(y, p) = -[y \\log(p) + (1-y) \\log(1-p)] $$\nThis loss penalizes the model for being confident in the wrong prediction. For instance, if $y=1$, the loss is $-\\log(p)$, which approaches infinity as the predicted probability $p$ approaches $0$.\n\n#### Gradient Derivation\nWe seek the gradient of the loss with respect to the logit $z$, $\\frac{dL_{CE}}{dz}$. Using the chain rule, $\\frac{dL_{CE}}{dz} = \\frac{dL_{CE}}{dp} \\frac{dp}{dz}$.\nFirst, we find the derivative with respect to $p$:\n$$ \\frac{dL_{CE}}{dp} = -\\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = -\\frac{y(1-p) - p(1-y)}{p(1-p)} = -\\frac{y - yp - p + py}{p(1-p)} = -\\frac{y-p}{p(1-p)} $$\nNow, we multiply by $\\frac{dp}{dz} = p(1-p)$:\n$$ \\frac{dL_{CE}}{dz} = \\left( -\\frac{y-p}{p(1-p)} \\right) \\cdot p(1-p) = -(y-p) = p-y $$\nThis remarkably simple result shows that the gradient of the cross-entropy loss with respect to the logit is simply the difference between the prediction and the target.\n\n#### Gradient Analysis ($p \\ll 1$)\nIn cases of severe class imbalance, the positive class is rare. We analyze the gradient when the model, as is common, predicts a low probability for the positive class ($p \\ll 1$).\n- **For a positive pixel ($y=1$):** The gradient is $\\frac{dL_{CE}}{dz} = p-1$. As $p \\to 0$, the gradient approaches $-1$. The gradient magnitude is $|\\frac{dL_{CE}}{dz}| \\approx 1$. These rare positive pixels, even when misclassified with low probability, generate a strong, constant error signal to update the model.\n- **For a negative pixel ($y=0$):** The gradient is $\\frac{dL_{CE}}{dz} = p-0 = p$. As $p \\to 0$, the gradient approaches $0$. The gradient magnitude is $|\\frac{dL_{CE}}{dz}| \\approx 0$. These common negative pixels, when correctly classified with low probability, generate a very small error signal.\nThe problem with $L_{CE}$ in imbalanced settings is that the sum of the many small gradients from \"easy\" negative examples can overwhelm the sum of the few large gradients from \"hard\" positive examples.\n\n### 2. Focal Loss ($L_{focal}$)\n\n#### Derivation\nFocal loss is designed to address class imbalance by modifying the cross-entropy loss with a modulating factor that reduces the loss contribution from well-classified examples. The focusing parameter $\\gamma \\ge 0$ controls the rate of down-weighting. The loss also incorporates a weighting factor $\\alpha \\in (0,1)$ to balance the importance of positive/negative classes.\n\nThe loss for a single pixel is defined as:\n$$ L_{focal}(y, p) = -y \\alpha (1-p)^\\gamma \\log(p) - (1-y)(1-\\alpha) p^\\gamma \\log(1-p) $$\nWhen $\\gamma=0$, this reduces to a weighted cross-entropy. As $\\gamma$ increases, the modulating factor $((1-p)^\\gamma$ for $y=1$, $p^\\gamma$ for $y=0$) more aggressively down-weights easy examples (e.g., $y=0, p \\to 0$ or $y=1, p \\to 1$).\n\n#### Gradient Derivation\nWe differentiate $L_{focal}$ with respect to $z$ piece-wise for the cases $y=1$ and $y=0$.\nCase $y=1$: $L = -\\alpha (1-p)^\\gamma \\log(p)$.\n$$ \\frac{dL}{dz} = \\frac{dL}{dp} \\frac{dp}{dz} = \\left(-\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\log(p) + \\frac{(1-p)^\\gamma}{p} \\right] \\right) \\cdot (p(1-p)) $$\n$$ = -\\alpha [-\\gamma p (1-p)^\\gamma \\log(p) + (1-p)^{\\gamma+1}] = \\alpha (1-p)^\\gamma [\\gamma p \\log(p) - (1-p)] $$\n$$ = \\alpha (1-p)^\\gamma (\\gamma p \\log(p) + p - 1) $$\nCase $y=0$: $L = -(1-\\alpha) p^\\gamma \\log(1-p)$.\n$$ \\frac{dL}{dz} = \\frac{dL}{dp} \\frac{dp}{dz} = \\left(-(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\log(1-p) + p^\\gamma \\frac{-1}{1-p} \\right] \\right) \\cdot (p(1-p)) $$\n$$ = -(1-\\alpha) [\\gamma p^\\gamma(1-p) \\log(1-p) - p^{\\gamma+1}] = (1-\\alpha) p^\\gamma [p - \\gamma(1-p)\\log(1-p)] $$\nCombining these gives the full gradient expression:\n$$ \\frac{dL_{focal}}{dz} = y \\cdot \\left[ \\alpha (1-p)^\\gamma (\\gamma p \\log(p) + p - 1) \\right] + (1-y) \\cdot \\left[ (1-\\alpha) p^\\gamma (p - \\gamma(1-p)\\log(1-p)) \\right] $$\n\n#### Gradient Analysis ($p \\ll 1$)\n- **For a positive pixel ($y=1$):** The gradient is $\\frac{dL_{focal}}{dz} = \\alpha (1-p)^\\gamma (\\gamma p \\log(p) + p - 1)$. As $p \\to 0$, we use the fact that $\\lim_{p\\to 0} p\\log(p) = 0$. The gradient approaches $\\alpha(1-0)^\\gamma(0 + 0 - 1) = -\\alpha$. The gradient magnitude is $|\\frac{dL_{focal}}{dz}| \\approx \\alpha$. Similar to cross-entropy, this provides a constant learning signal, but scaled by $\\alpha$.\n- **For a negative pixel ($y=0$):** The gradient is $\\frac{dL_{focal}}{dz} = (1-\\alpha) p^\\gamma (p - \\gamma(1-p)\\log(1-p))$. As $p \\to 0$, we use the Taylor expansion $\\log(1-p) \\approx -p$. The term in parentheses becomes $p - \\gamma(1-p)(-p) = p(1 + \\gamma(1-p)) \\approx p(1+\\gamma)$. The gradient is approximately $(1-\\alpha) p^\\gamma \\cdot p(1+\\gamma) = (1-\\alpha)(1+\\gamma)p^{\\gamma+1}$. For $\\gamma > 0$, this gradient diminishes to zero much faster than the cross-entropy gradient ($p$). For $\\gamma=2$, the gradient is $O(p^3)$, effectively silencing the contribution from the vast number of easy negative examples.\n\n### 3. Dice Loss ($L_{Dice}$)\n\n#### Derivation\nThe Dice loss is based on the Sørensen–Dice coefficient, a metric for measuring the overlap between two sets. It is not derived from the Bernoulli NLL but is included here as a common and effective alternative for segmentation. The soft Dice coefficient for predicted probabilities $p_i$ and true labels $y_i$ across all $N$ pixels is:\n$$ D(y, p) = \\frac{2 \\sum_{i=1}^N y_i p_i + s}{\\sum_{i=1}^N y_i + \\sum_{i=1}^N p_i + s} $$\nwhere $s>0$ is a smoothing constant to prevent division by zero and improve stability. The Dice loss is defined as $L_{Dice} = 1 - D$.\n$$ L_{Dice} = 1 - \\frac{2 \\sum_{i=1}^N y_i p_i + s}{\\sum_{i=1}^N y_i + \\sum_{i=1}^N p_i + s} $$\nUnlike CE and Focal loss, Dice loss is a global metric; the loss value is computed over the entire image, not per-pixel.\n\n#### Gradient Derivation\nThe gradient of $L_{Dice}$ with respect to a single logit $z_j$ is $\\frac{\\partial L_{Dice}}{\\partial z_j} = \\frac{\\partial L_{Dice}}{\\partial p_j} \\frac{dp_j}{dz_j}$. Let $U = 2 \\sum_i y_i p_i + s$ and $V = \\sum_i y_i + \\sum_i p_i + s$.\n$$ \\frac{\\partial L_{Dice}}{\\partial p_j} = -\\frac{\\partial}{\\partial p_j}\\left(\\frac{U}{V}\\right) = - \\frac{\\frac{\\partial U}{\\partial p_j}V - U\\frac{\\partial V}{\\partial p_j}}{V^2} $$\nThe partial derivatives of the sums are $\\frac{\\partial U}{\\partial p_j} = 2y_j$ and $\\frac{\\partial V}{\\partial p_j} = 1$.\n$$ \\frac{\\partial L_{Dice}}{\\partial p_j} = - \\frac{2y_j V - U}{V^2} = - \\frac{2y_j (\\sum_i y_i + \\sum_i p_i + s) - (2 \\sum_i y_i p_i + s)}{(\\sum_i y_i + \\sum_i p_i + s)^2} $$\nMultiplying by $\\frac{dp_j}{dz_j} = p_j(1-p_j)$ gives the final gradient:\n$$ \\frac{\\partial L_{Dice}}{\\partial z_j} = -p_j(1-p_j) \\frac{2y_j (\\sum_i y_i + \\sum_i p_i + s) - (2 \\sum_i y_i p_i + s)}{(\\sum_i y_i + \\sum_i p_i + s)^2} $$\n\n#### Gradient Analysis ($p_i \\ll 1$ for all i)\nAssume all predictions $p_i$ are small.\n- **For a positive pixel ($y_j=1$):** The term $2y_j V$ in the numerator is non-zero. The gradient depends on the global sums $\\sum y_i$, $\\sum p_i$, etc. Even if $p_j \\to 0$, the gradient does not necessarily vanish because the large fraction term, which depends on global statistics like the total number of positive pixels $\\sum y_i$, provides a substantial signal. The gradient signal for positive pixels is maintained.\n- **For a negative pixel ($y_j=0$):** The term $2y_j V$ in the numerator is zero. The gradient expression becomes $\\frac{\\partial L_{Dice}}{\\partial z_j} = p_j(1-p_j) \\frac{U}{V^2}$. As $p_j \\to 0$, the factor $p_j(1-p_j)$ drives the gradient to zero. The gradients for easy negative examples are suppressed.\n\nDice loss naturally balances classes because its gradient structure intrinsically considers the global count of positive pixels, making it robust to imbalance without explicit re-weighting parameters like $\\alpha$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constant for numerical stability in log operations\nEPSILON = 1e-7\n\ndef compute_ce(y_true, p_pred):\n    \"\"\"\n    Computes pixel-wise Cross-Entropy loss and its gradient statistics.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels {0, 1}.\n        p_pred (np.ndarray): Array of predicted probabilities (0, 1).\n\n    Returns:\n        list: [mean_loss, mean_abs_grad_pos, mean_abs_grad_neg]\n    \"\"\"\n    # Clamp probabilities to avoid log(0)\n    p_clamped = np.clip(p_pred, EPSILON, 1 - EPSILON)\n    \n    # 1. Compute mean loss per pixel\n    loss_ce = -(y_true * np.log(p_clamped) + (1 - y_true) * np.log(1 - p_clamped))\n    mean_loss = np.mean(loss_ce)\n    \n    # 2. Compute gradient with respect to the logit z\n    grad_z = p_pred - y_true\n    \n    # 3. Separate gradients for positive (y=1) and negative (y=0) pixels\n    pos_mask = y_true == 1\n    neg_mask = y_true == 0\n    \n    grad_abs_pos = np.abs(grad_z[pos_mask])\n    mean_grad_abs_pos = np.mean(grad_abs_pos) if grad_abs_pos.size > 0 else 0.0\n    \n    grad_abs_neg = np.abs(grad_z[neg_mask])\n    mean_grad_abs_neg = np.mean(grad_abs_neg) if grad_abs_neg.size > 0 else 0.0\n    \n    return [mean_loss, mean_grad_abs_pos, mean_grad_abs_neg]\n\ndef compute_focal(y_true, p_pred, gamma, alpha):\n    \"\"\"\n    Computes pixel-wise Focal loss and its gradient statistics.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels {0, 1}.\n        p_pred (np.ndarray): Array of predicted probabilities (0, 1).\n        gamma (float): The focusing parameter.\n        alpha (float): The class-balancing factor.\n\n    Returns:\n        list: [mean_loss, mean_abs_grad_pos, mean_abs_grad_neg]\n    \"\"\"\n    p_clamped = np.clip(p_pred, EPSILON, 1 - EPSILON)\n    \n    # 1. Compute mean loss per pixel\n    loss_pos = -alpha * ((1 - p_clamped)**gamma) * np.log(p_clamped)\n    loss_neg = -(1 - alpha) * (p_clamped**gamma) * np.log(1 - p_clamped)\n    loss_focal = y_true * loss_pos + (1 - y_true) * loss_neg\n    mean_loss = np.mean(loss_focal)\n    \n    # 2. Compute gradient with respect to the logit z\n    # Note: Use p_clamped for log terms in gradient to maintain numerical stability.\n    grad_pos = alpha * ((1 - p_pred)**gamma) * (gamma * p_pred * np.log(p_clamped) + p_pred - 1)\n    grad_neg = (1 - alpha) * (p_pred**gamma) * (p_pred - gamma * (1 - p_pred) * np.log(1 - p_clamped))\n    grad_z = y_true * grad_pos + (1 - y_true) * grad_neg\n    \n    # 3. Separate gradients\n    pos_mask = y_true == 1\n    neg_mask = y_true == 0\n    \n    grad_abs_pos = np.abs(grad_z[pos_mask])\n    mean_grad_abs_pos = np.mean(grad_abs_pos) if grad_abs_pos.size > 0 else 0.0\n    \n    grad_abs_neg = np.abs(grad_z[neg_mask])\n    mean_grad_abs_neg = np.mean(grad_abs_neg) if grad_abs_neg.size > 0 else 0.0\n    \n    return [mean_loss, mean_grad_abs_pos, mean_grad_abs_neg]\n    \ndef compute_dice(y_true, p_pred, s):\n    \"\"\"\n    Computes Dice loss and its gradient statistics.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels {0, 1}.\n        p_pred (np.ndarray): Array of predicted probabilities (0, 1).\n        s (float): The smoothing constant.\n\n    Returns:\n        list: [loss_value, mean_abs_grad_pos, mean_abs_grad_neg]\n    \"\"\"\n    # 1. Compute global loss value\n    intersection = np.sum(y_true * p_pred)\n    total_sum = np.sum(y_true) + np.sum(p_pred)\n    dice_coeff = (2. * intersection + s) / (total_sum + s)\n    loss_dice = 1. - dice_coeff  # This is the single loss value for the whole image.\n    \n    # 2. Compute gradient with respect to the logit z (per-pixel)\n    U = 2. * intersection + s\n    V = total_sum + s\n    \n    grad_p = - (2 * y_true * V - U) / (V**2)\n    grad_z = grad_p * p_pred * (1 - p_pred)\n    \n    # 3. Separate gradients\n    pos_mask = y_true == 1\n    neg_mask = y_true == 0\n    \n    grad_abs_pos = np.abs(grad_z[pos_mask])\n    mean_grad_abs_pos = np.mean(grad_abs_pos) if grad_abs_pos.size > 0 else 0.0\n    \n    grad_abs_neg = np.abs(grad_z[neg_mask])\n    mean_grad_abs_neg = np.mean(grad_abs_neg) if grad_abs_neg.size > 0 else 0.0\n    \n    return [loss_dice, mean_grad_abs_pos, mean_grad_abs_neg]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 100, 'r': 0.2, 'p_pos': 0.6, 'p_neg': 0.4, 'gamma': 2, 'alpha': 0.25, 's': 1.0},\n        {'N': 1000, 'r': 0.01, 'p_pos': 0.01, 'p_neg': 0.01, 'gamma': 2, 'alpha': 0.25, 's': 1.0},\n        {'N': 500, 'r': 0.1, 'p_pos': 0.999, 'p_neg': 0.001, 'gamma': 2, 'alpha': 0.25, 's': 1.0},\n        {'N': 200, 'r': 0.3, 'p_pos': 0.7, 'p_neg': 0.3, 'gamma': 0, 'alpha': 0.25, 's': 1.0},\n    ]\n\n    all_results = []\n    for params in test_cases:\n        N = params['N']\n        r = params['r']\n        p_pos = params['p_pos']\n        p_neg = params['p_neg']\n        gamma = params['gamma']\n        alpha = params['alpha']\n        s = params['s']\n\n        n_pos = int(round(N * r))\n        n_neg = N - n_pos\n\n        y_true = np.array([1] * n_pos + [0] * n_neg, dtype=np.float64)\n        p_pred = np.array([p_pos] * n_pos + [p_neg] * n_neg, dtype=np.float64)\n\n        case_results = []\n        # L_CE\n        case_results.append(compute_ce(y_true, p_pred))\n        # L_focal\n        case_results.append(compute_focal(y_true, p_pred, gamma, alpha))\n        # L_Dice\n        case_results.append(compute_dice(y_true, p_pred, s))\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified in the problem statement\n    all_case_strings = []\n    for case_result in all_results:\n        loss_strings = []\n        for loss_result in case_result:\n            # Format each sublist of floats into \"[v1,v2,v3]\"\n            loss_strings.append(f\"[{','.join(f'{v:.10f}'.rstrip('0').rstrip('.') if v != 0 else '0.0' for v in loss_result)}]\")\n        # Join the sublists for a single test case\n        case_string = f\"[{','.join(loss_strings)}]\"\n        all_case_strings.append(case_string)\n    \n    # Join all test case results into the final string\n    final_output = f\"[{','.join(all_case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3136332"}, {"introduction": "Evaluating a segmentation model requires metrics that are sensitive to the nuances of the task. While simple pixel-wise accuracy can be misleading, Panoptic Quality ($PQ$) offers a comprehensive assessment by evaluating both class labeling and instance delineation. In this exercise, you will deconstruct $PQ$ into its core components—Segmentation Quality ($SQ$) and Recognition Quality ($RQ$)—and use controlled experiments to see how it effectively penalizes common errors like object merging and splitting [@problem_id:3136328].", "problem": "You are tasked with formalizing and testing how the standard panoptic segmentation metric decomposes into components that isolate recognition versus segmentation quality, and then designing controlled failure cases in which semantic accuracy remains identical while instance-level partitioning differs. Work entirely in a single semantic class, and evaluate on synthetic binary masks whose union equals the ground-truth semantic region, so that semantic accuracy is identical across the designed cases. All computations must be implemented by your program without reading input.\n\nFundamental base and definitions to use:\n- Define a per-class matching between ground-truth instances and predicted instances using Intersection-over-Union (IoU). For each ground-truth instance mask $g$ and predicted instance mask $p$, define the IoU as $|g \\cap p| / |g \\cup p|$, where $|\\cdot|$ denotes set cardinality measured in pixels.\n- A one-to-one matching is constructed between instances by maximizing total IoU subject to the constraint that only pairs with $\\text{IoU} > 0.5$ are eligible. Unmatched ground-truth instances count as false negatives, unmatched predicted instances count as false positives, and matched pairs count as true positives.\n- Define Segmentation Quality (SQ) as the average IoU across all matched pairs, with the convention that if the number of true positives $|TP|$ is $0$, then $SQ=0$.\n- Define Recognition Quality (RQ) as $RQ = \\frac{2|TP|}{2|TP| + |FP| + |FN|}$, which is the F1-style term on instance recognition under the thresholded matching protocol described above.\n- Define Panoptic Quality (PQ) from first principles as the product of a per-match quality component and a recognition component, by starting with the standard definition as the sum of IoU over matched pairs divided by a penalized count of detections, namely the denominator $|TP| + 0.5|FP| + 0.5|FN|$. Show how this yields a multiplicative decomposition without stating it a priori.\n\nYour program must:\n1. Implement per-class instance matching using the above IoU definition and the threshold $\\text{IoU} > 0.5$. The matching must be one-to-one and must maximize the sum of IoU over accepted pairs.\n2. Compute $SQ$, $RQ$, and their product $PQ$, and also compute semantic accuracy as the fraction of pixels whose semantic class label matches between prediction and ground truth. Use the convention that the background label is $0$ and a single foreground semantic class is encoded by any positive integer instance identifier. For semantic accuracy, collapse instances and consider only foreground versus background, not instance identity. There are no physical units in this problem.\n3. Use the following test suite, each on an image of size $10 \\times 10$ pixels. All numbers and indices in this specification are inclusive of the start index and exclusive of the end index in interval notation, and row and column indices start at $0$.\n   - Case A (perfect prediction):\n     - Ground truth: one instance covering all $10 \\times 10$ pixels. Use label value $1$ for that instance and $0$ for background outside the instance (there is no background here).\n     - Prediction: identical to ground truth.\n   - Case B (over-merge, identical semantics):\n     - Ground truth: two disjoint instances of sizes $60$ and $40$ pixels created by filling rows $[0,6)$ with instance label $1$ and rows $[6,10)$ with instance label $2$ across all columns $[0,10)$.\n     - Prediction: one merged instance covering all $10 \\times 10$ pixels with label $1$.\n   - Case C (over-split, identical semantics):\n     - Ground truth: one instance covering all $10 \\times 10$ pixels with label $1$.\n     - Prediction: two disjoint instances of sizes $60$ and $40$ pixels created by filling rows $[0,6)$ with instance label $1$ and rows $[6,10)$ with instance label $2$ across all columns $[0,10)$.\n   - Case D (extreme over-split, identical semantics):\n     - Ground truth: one instance covering all $10 \\times 10$ pixels with label $1$.\n     - Prediction: four disjoint instances of size $25$ each placed as four quadrants of size $5 \\times 5$: top-left rows $[0,5)$, columns $[0,5)$ label $1$; top-right rows $[0,5)$, columns $[5,10)$ label $2$; bottom-left rows $[5,10)$, columns $[0,5)$ label $3$; bottom-right rows $[5,10)$, columns $[5,10)$ label $4$.\n\nRequirements:\n- Matching must use the IoU threshold strictly greater than $0.5$.\n- For each case, compute $PQ$, $SQ$, $RQ$, and semantic accuracy. All values must be floating point numbers rounded to $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of four-element lists in the order of cases A, B, C, D, where each inner list is `[PQ, SQ, RQ, SA]` with SA being semantic accuracy. For example: \"[[vA1,vA2,vA3,vA4],[vB1,vB2,vB3,vB4],[vC1,vC2,vC3,vC4],[vD1,vD2,vD3,vD4]]\".\n- The output must be consistent with the described test suite and the definitions above, and must illustrate how over-merge and over-split produce identical semantic accuracy while isolating the sensitivity of the recognition versus segmentation components under the same semantics.", "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the principles of computer vision evaluation metrics, specifically panoptic segmentation. The problem is well-posed, providing a self-contained set of definitions, constraints, and test data that admit a unique, verifiable solution. All terms are defined objectively and without ambiguity. The task requires a formal derivation and a concrete implementation, representing a substantive and well-structured exercise in scientific computing.\n\n### Theoretical Foundation and Metric Decomposition\n\nThe problem centers on Panoptic Quality ($PQ$), a metric designed to evaluate panoptic segmentation tasks. Panoptic segmentation unifies semantic segmentation (assigning a class label to every pixel) and instance segmentation (detecting and segmenting individual object instances). The $PQ$ metric elegantly decomposes into two components: Segmentation Quality ($SQ$) and Recognition Quality ($RQ$).\n\nLet a set of ground-truth instance masks be denoted by $\\{g_i\\}$ and a set of predicted instance masks by $\\{p_j\\}$. The foundational measures are defined as follows:\n\n1.  **Intersection-over-Union (IoU)**: For a ground-truth instance $g$ and a predicted instance $p$, the IoU is given by:\n    $$ \\text{IoU}(g, p) = \\frac{|g \\cap p|}{|g \\cup p|} $$\n    where $|\\cdot|$ represents the cardinality (pixel count) of a set.\n\n2.  **Instance Matching**: A one-to-one matching is established between ground-truth and predicted instances. A pair $(g, p)$ is considered a potential match only if their $\\text{IoU}(g, p) > 0.5$. Among all possible one-to-one matchings of potential pairs, the one that maximizes the sum of IoUs is chosen. This constitutes a maximum weight bipartite matching problem.\n    -   **True Positives ($TP$)**: The set of matched pairs $(g, p)$ resulting from this procedure.\n    -   **False Negatives ($FN$)**: The set of ground-truth instances that remain unmatched.\n    -   **False Positives ($FP$)**: The set of predicted instances that remain unmatched.\n\nFrom these counts, the quality metrics are defined:\n\n-   **Segmentation Quality ($SQ$)**: This measures the average IoU over all correctly matched instances (true positives). It reflects how well the pixels of detected objects are segmented.\n    $$ SQ = \\frac{\\sum_{(g,p) \\in TP} \\text{IoU}(g,p)}{|TP|} $$\n    By convention, if $|TP|=0$, then $SQ=0$.\n\n-   **Recognition Quality ($RQ$)**: This is an F1-score computed on the instance detection level. It measures how well the model detects objects, regardless of segmentation accuracy.\n    $$ RQ = \\frac{|TP|}{|TP| + \\frac{1}{2}|FP| + \\frac{1}{2}|FN|} = \\frac{2|TP|}{2|TP|+|FP|+|FN|} $$\n\n-   **Panoptic Quality ($PQ$)**: The problem provides the fundamental definition of $PQ$ as the total IoU of matched pairs, penalized by the number of false detections.\n    $$ PQ = \\frac{\\sum_{(g,p) \\in TP} \\text{IoU}(g,p)}{|TP| + \\frac{1}{2}|FP| + \\frac{1}{2}|FN|} $$\n\n**Decomposition of $PQ$**: We can demonstrate that $PQ$ is the product of $SQ$ and $RQ$. Starting from the definition of $PQ$, we can rewrite it as:\n$$ PQ = \\left( \\frac{\\sum_{(g,p) \\in TP} \\text{IoU}(g,p)}{|TP|} \\right) \\times \\left( \\frac{|TP|}{|TP| + \\frac{1}{2}|FP| + \\frac{1}{2}|FN|} \\right) $$\nThis manipulation is valid for $|TP| > 0$. The first term is precisely the definition of $SQ$, and the second term is the definition of $RQ$. Therefore:\n$$ PQ = SQ \\times RQ $$\nIf $|TP|=0$, then by definition $SQ=0$ and the numerator of $PQ$ is $0$, making $PQ=0$. The numerator of $RQ$ is also $0$, making $RQ=0$. The identity $PQ = SQ \\times RQ$ holds as $0 = 0 \\times 0$. This multiplicative decomposition shows that $PQ$ jointly measures both segmentation and recognition quality. A perfect score of $PQ=1$ requires both perfect recognition ($RQ=1$, meaning no unmatched instances) and perfect segmentation ($SQ=1$, meaning all matched instances have an IoU of $1$).\n\n-   **Semantic Accuracy ($SA$)**: This is a simpler, pixel-level metric that ignores instance information. All positive instance labels are collapsed into a single \"foreground\" class. $SA$ is the fraction of pixels where the semantic label (foreground vs. background) is the same in the prediction and the ground truth.\n\n### Algorithmic Implementation and Case Analysis\n\nThe core of the implementation involves a function that takes ground-truth and prediction masks, identifies unique instances, computes a pairwise IoU matrix, solves the assignment problem to find the optimal matching, and then calculates the metrics.\n\n-   **Matching**: The matching is found by solving the maximum weight bipartite matching problem. We construct a cost matrix where the cost of matching ground-truth instance $i$ with predicted instance $j$ is $-\\text{IoU}(g_i, p_j)$ if $\\text{IoU}(g_i, p_j) > 0.5$, and a large positive number otherwise (to prevent matching). `scipy.optimize.linear_sum_assignment` is used to find the minimum cost assignment, which corresponds to the maximum IoU sum.\n\nThe analysis is performed on four test cases on a $10 \\times 10$ image.\n\n**Case A: Perfect Prediction**\n-   Ground Truth ($GT$): 1 instance, size $100$.\n-   Prediction ($Pred$): 1 instance, size $100$.\n-   IoU Matrix: A $1 \\times 1$ matrix with value $1.0$.\n-   Matching: The single $GT$ instance matches the single $Pred$ instance with $\\text{IoU}=1.0$.\n-   Counts: $|TP|=1$, $|FP|=0$, $|FN|=0$.\n-   Metrics:\n    -   $SQ = 1.0 / 1 = 1.0$\n    -   $RQ = (2 \\times 1) / (2 \\times 1 + 0 + 0) = 1.0$\n    -   $PQ = SQ \\times RQ = 1.0 \\times 1.0 = 1.0$\n    -   $SA = (100 \\text{ matching pixels}) / 100 = 1.0$\n-   Result: $[1.0, 1.0, 1.0, 1.0]$\n\n**Case B: Over-merge**\n-   $GT$: 2 instances, $g_1$ (size $60$) and $g_2$ (size $40$).\n-   $Pred$: 1 instance, $p_1$ (size $100$).\n-   IoUs: $\\text{IoU}(g_1, p_1) = 60/100 = 0.6$. $\\text{IoU}(g_2, p_1) = 40/100 = 0.4$.\n-   Matching: Since $\\text{IoU}(g_2, p_1) \\le 0.5$, it is not a valid match candidate. $p_1$ can only match to $g_1$.\n-   Counts: $|TP|=1$ (for the pair $(g_1, p_1)$), $|FP|=0$ (the single predicted instance is matched), $|FN|=1$ ($g_2$ is unmatched).\n-   Metrics:\n    -   $SQ = 0.6 / 1 = 0.6$\n    -   $RQ = (2 \\times 1) / (2 \\times 1 + 0 + 1) = 2/3 \\approx 0.666667$\n    -   $PQ = SQ \\times RQ = 0.6 \\times (2/3) = 0.4$\n    -   $SA$: Both $GT$ and $Pred$ cover all $100$ pixels, so they are semantically identical. $SA=1.0$.\n-   Result: $[0.4, 0.6, 0.666667, 1.0]$\n\n**Case C: Over-split**\n-   $GT$: 1 instance, $g_1$ (size $100$).\n-   $Pred$: 2 instances, $p_1$ (size $60$) and $p_2$ (size $40$).\n-   IoUs: $\\text{IoU}(g_1, p_1) = 60/100 = 0.6$. $\\text{IoU}(g_1, p_2) = 40/100 = 0.4$.\n-   Matching: $g_1$ can only match one prediction. It matches with $p_1$ as $\\text{IoU}(g_1, p_1) > 0.5$.\n-   Counts: $|TP|=1$ (for the pair $(g_1, p_1)$), $|FP|=1$ ($p_2$ is unmatched), $|FN|=0$ (the single $GT$ instance is matched).\n-   Metrics:\n    -   $SQ = 0.6 / 1 = 0.6$\n    -   $RQ = (2 \\times 1) / (2 \\times 1 + 1 + 0) = 2/3 \\approx 0.666667$\n    -   $PQ = SQ \\times RQ = 0.6 \\times (2/3) = 0.4$\n    -   $SA$: Semantically identical to $GT$, so $SA=1.0$.\n-   Result: $[0.4, 0.6, 0.666667, 1.0]$. Notably, this is identical to Case B, demonstrating that $PQ$ penalizes a false merge (1 $FN$) and a false split (1 $FP$) symmetrically.\n\n**Case D: Extreme Over-split**\n-   $GT$: 1 instance, $g_1$ (size $100$).\n-   $Pred$: 4 instances, $p_1, p_2, p_3, p_4$ (each size $25$).\n-   IoUs: For any $p_i$, $\\text{IoU}(g_1, p_i) = 25/100 = 0.25$.\n-   Matching: Since all IoU values are $0.25$, which is not strictly greater than $0.5$, no matches are possible.\n-   Counts: $|TP|=0$, $|FP|=4$ (all four predicted instances are unmatched), $|FN|=1$ ($g_1$ is unmatched).\n-   Metrics:\n    -   $SQ$: Since $|TP|=0$, $SQ=0.0$ by definition.\n    -   $RQ$: Since $|TP|=0$, $RQ = 0 / (0+4+1) = 0.0$.\n    -   $PQ = SQ \\times RQ = 0.0 \\times 0.0 = 0.0$.\n    -   $SA$: Semantically identical to $GT$, so $SA=1.0$.\n-   Result: $[0.0, 0.0, 0.0, 1.0]$. This highlights a catastrophic failure in recognition ($RQ=0$) which drives the overall $PQ$ to zero, even though the segmentation quality of the individual (but unrecognized) parts might be high and the semantic accuracy is perfect.\n\nThese cases correctly demonstrate how $PQ$ and its components provide a nuanced evaluation of instance-level errors that is completely missed by semantic accuracy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef get_test_cases():\n    \"\"\"Generates the four test cases as pairs of (gt_mask, pred_mask).\"\"\"\n    cases = []\n    \n    # Case A: Perfect prediction\n    gt_a = np.ones((10, 10), dtype=int)\n    pred_a = np.ones((10, 10), dtype=int)\n    cases.append(('A', gt_a, pred_a))\n\n    # Case B: Over-merge\n    gt_b = np.zeros((10, 10), dtype=int)\n    gt_b[0:6, :] = 1\n    gt_b[6:10, :] = 2\n    pred_b = np.ones((10, 10), dtype=int)\n    cases.append(('B', gt_b, pred_b))\n\n    # Case C: Over-split\n    gt_c = np.ones((10, 10), dtype=int)\n    pred_c = np.zeros((10, 10), dtype=int)\n    pred_c[0:6, :] = 1\n    pred_c[6:10, :] = 2\n    cases.append(('C', gt_c, pred_c))\n\n    # Case D: Extreme over-split\n    gt_d = np.ones((10, 10), dtype=int)\n    pred_d = np.zeros((10, 10), dtype=int)\n    pred_d[0:5, 0:5] = 1\n    pred_d[0:5, 5:10] = 2\n    pred_d[5:10, 0:5] = 3\n    pred_d[5:10, 5:10] = 4\n    cases.append(('D', gt_d, pred_d))\n    \n    return cases\n\ndef calculate_metrics(gt_mask, pred_mask, iou_threshold=0.5):\n    \"\"\"\n    Computes PQ, SQ, RQ, and SA for a given pair of masks.\n    \"\"\"\n    # 1. Semantic Accuracy (SA)\n    gt_semantic = gt_mask > 0\n    pred_semantic = pred_mask > 0\n    sa = np.mean(gt_semantic == pred_semantic)\n\n    # 2. Instance Extraction\n    gt_ids = np.unique(gt_mask[gt_mask > 0])\n    pred_ids = np.unique(pred_mask[pred_mask > 0])\n    \n    num_gt = len(gt_ids)\n    num_pred = len(pred_ids)\n    \n    if num_gt == 0 and num_pred == 0:\n        return 1.0, 1.0, 1.0, sa # PQ, SQ, RQ, SA\n\n    # 3. IoU Matrix Calculation\n    iou_matrix = np.zeros((num_gt, num_pred))\n    for i, gt_id in enumerate(gt_ids):\n        g_mask = (gt_mask == gt_id)\n        for j, pred_id in enumerate(pred_ids):\n            p_mask = (pred_mask == pred_id)\n            intersection = np.sum(np.logical_and(g_mask, p_mask))\n            union = np.sum(np.logical_or(g_mask, p_mask))\n            iou = intersection / union if union > 0 else 0\n            iou_matrix[i, j] = iou\n\n    # 4. Instance Matching (Maximum Weight Bipartite Matching)\n    # We want to maximize sum of IoUs, which is equivalent to minimizing sum of -IoUs.\n    # Set costs for invalid matches (IoU = threshold) to a high value.\n    cost_matrix = -iou_matrix\n    cost_matrix[iou_matrix = iou_threshold] = 1.0 # high cost for non-matchable pairs\n    \n    gt_ind, pred_ind = linear_sum_assignment(cost_matrix)\n    \n    # Filter matches to only include those above the threshold\n    matched_pairs = []\n    sum_iou = 0.0\n    for r, c in zip(gt_ind, pred_ind):\n        if iou_matrix[r, c] > iou_threshold:\n            matched_pairs.append((r, c))\n            sum_iou += iou_matrix[r, c]\n            \n    # 5. Calculate TP, FP, FN\n    tp = len(matched_pairs)\n    fp = num_pred - tp\n    fn = num_gt - tp\n    \n    # 6. Calculate SQ, RQ, PQ\n    # Segmentation Quality\n    sq = sum_iou / tp if tp > 0 else 0.0\n\n    # Recognition Quality\n    denominator_rq = 2 * tp + fp + fn\n    rq = (2 * tp) / denominator_rq if denominator_rq > 0 else 0.0\n    \n    # Panoptic Quality\n    # Using the product form, which is equivalent to the first-principles definition\n    pq = sq * rq\n    \n    # Round all values to 6 decimal places\n    pq = round(pq, 6)\n    sq = round(sq, 6)\n    rq = round(rq, 6)\n    sa = round(sa, 6)\n    \n    return [pq, sq, rq, sa]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = get_test_cases()\n    \n    all_results = []\n    for name, gt, pred in test_cases:\n        result = calculate_metrics(gt, pred)\n        all_results.append(result)\n\n    # Format the final output string\n    result_strings = []\n    for res in all_results:\n        result_strings.append(f\"[{','.join(map(str, res))}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3136328"}, {"introduction": "State-of-the-art panoptic segmentation models often produce separate predictions for \"stuff\" (amorphous regions like sky or road) and \"thing\" (countable objects like cars or people). This practice tackles the real-world algorithmic challenge of fusing these separate outputs into a single, coherent panoptic map, a crucial final step in the pipeline. You will implement and compare different fusion heuristics, using the Panoptic Quality ($PQ$) metric you explored previously to measure the impact of your algorithmic choices on the final segmentation quality [@problem_id:3136241].", "problem": "You are given the task of designing and evaluating a fusion procedure to produce a panoptic segmentation from separate predictions of semantic \"stuff\" classes and instance \"thing\" masks. The evaluation shall be based on Panoptic Quality (PQ), which is computed from first principles using the following foundational definitions. A panoptic segmentation is a partition of the image into non-overlapping segments, each segment being associated with exactly one category; categories are divided into \"stuff\" categories and \"thing\" categories. Intersection-over-Union (IoU) between two binary segment masks $A$ and $B$ is defined as $IoU(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$. A predicted segment and a ground-truth segment are considered a match if they have the same category and their IoU exceeds a fixed threshold. A true positive is a matched predicted-ground-truth pair. A false positive is a predicted segment that fails to match any ground-truth segment of the same category. A false negative is a ground-truth segment that fails to match any predicted segment of the same category. Panoptic Quality (PQ) is defined as \n$$\nPQ = \\frac{\\sum_{(p,g) \\in \\mathcal{M}} IoU(p,g)}{|\\mathcal{TP}| + \\frac{1}{2}|\\mathcal{FP}| + \\frac{1}{2}|\\mathcal{FN}|},\n$$\nwhere $\\mathcal{M}$ is the set of matched predicted-ground-truth segment pairs of the same category (each segment participates in at most one match), $\\mathcal{TP}$ is the set of true positives, $\\mathcal{FP}$ is the set of false positives, and $\\mathcal{FN}$ is the set of false negatives.\n\nYou must implement two panoptic fusion heuristics (policies), starting from the following core definitions and widely accepted facts:\n- For fusion, the semantic prediction supplies one \"stuff\" label per pixel. The instance predictions for \"thing\" classes supply per-instance binary masks with category labels and confidence scores.\n- Non-Maximum Suppression (NMS) among instances of the same category is applied to reduce duplicate instance predictions. NMS keeps an instance if its mask IoU with any already kept instance of the same category does not exceed a threshold; instances are considered in descending confidence order.\n- Conflict resolution between \"thing\" instance masks and \"stuff\" pixel labels must enforce non-overlapping final segments: each pixel is assigned either to exactly one \"thing\" instance or to a \"stuff\" class, but never both.\n- Matching between predicted and ground-truth segments is one-to-one by category subject to the IoU threshold, using a greedy procedure that sorts candidate matches by IoU descending and accepts a match when both segments are unmatched and the IoU exceeds the threshold.\n\nFusion policies to implement:\n- Policy $\\mathcal{P}_1$ (instance-priority): Keep instances with confidence $\\alpha \\ge \\tau_1$. Apply NMS with IoU threshold $\\beta_1$ among instances of the same category. Assign pixels to instances in descending confidence order; instance pixels always overwrite \"stuff\" labels. Any overlapping instance pixels are assigned to the highest-confidence instance through the ordering.\n- Policy $\\mathcal{P}_2$ (stuff-protected): Keep instances with confidence $\\alpha \\ge \\tau_2$. Apply NMS with IoU threshold $\\beta_2$ among instances of the same category. Maintain a protected set of \"stuff\" categories that cannot be overwritten by instances; if a pixel's semantic label belongs to the protected set, that pixel cannot be assigned to a \"thing\" instance. For other pixels, assign to instances in descending confidence order similarly to $\\mathcal{P}_1$.\n\nUse threshold values $\\tau_1 = 0.6$, $\\beta_1 = 0.5$, $\\tau_2 = 0.7$, $\\beta_2 = 0.3$, and the protected \"stuff\" set $\\{\\text{sky}\\}$.\n\nCategories and identifiers:\n- \"stuff\": sky $\\rightarrow$ category id $1$, road $\\rightarrow$ category id $2$.\n- \"thing\": person $\\rightarrow$ category id $10$, car $\\rightarrow$ category id $11$.\n\nUse four-connectivity for connected components in \"stuff\" classes. Matching threshold for IoU is strict: a match requires $\\text{IoU} > 0.5$.\n\nYou will implement the fusion and PQ evaluation on the following test suite of synthetic images, each fully specified by its ground truth and predictions. All images use zero-based row and column indices.\n\nTest case $1$:\n- Image size $H=5$, $W=5$.\n- Ground truth segments:\n  - Sky: rows $0$ to $1$, columns $0$ to $4$.\n  - Road: rows $2$ to $4$, columns $0$ to $4$, except pixels covered by the person instance.\n  - Person instance: rows $2$ to $3$, columns $2$ to $3$ (a $2 \\times 2$ block).\n- Predicted semantic \"stuff\" labels:\n  - Sky: rows $0$ to $1$ all columns; additionally row $2$, columns $0$ to $1$.\n  - Road: all remaining pixels.\n- Predicted instances:\n  - Person: mask rows $2$ to $3$, columns $1$ to $2$, confidence $\\alpha = 0.8$.\n  - Car: mask rows $0$ to $1$, columns $3$ to $4$, confidence $\\alpha = 0.6$.\n\nTest case $2$:\n- Image size $H=6$, $W=6$.\n- Ground truth segments:\n  - Sky: rows $0$ to $2$, columns $0$ to $5$.\n  - Road: rows $4$ to $5$, columns $0$ to $5$, except pixels covered by \"thing\" instances.\n  - Person instance: rows $2$ to $4$, columns $1$ to $2$ (a $3 \\times 2$ rectangle).\n  - Car instance: row $3$, columns $3$ to $5$ (three pixels).\n- Predicted semantic \"stuff\" labels:\n  - Sky: rows $0$ to $2$ all columns; additionally row $4$, columns $0$ to $2$.\n  - Road: all remaining pixels.\n- Predicted instances:\n  - Person instance $1$: rows $2$ to $4$, columns $1$ to $2$, confidence $\\alpha = 0.7$.\n  - Person instance $2$: rows $1$ to $4$, columns $1$ to $3$, confidence $\\alpha = 0.65$.\n  - Car: pixels at row $3$, columns $4$ to $5$ and row $2$, column $5$, confidence $\\alpha = 0.9$. Note the ground-truth car is at row $3$, columns $3$ to $5$, so the IoU between predicted and ground-truth car masks equals $0.5$, which is the boundary case for matching and must be treated as non-matching.\n  \nTest case $3$:\n- Image size $H=4$, $W=4$.\n- Ground truth segments:\n  - Sky: rows $0$ to $1$, columns $0$ to $3$.\n  - Road: rows $2$ to $3$, columns $0$ to $3$.\n- Predicted semantic \"stuff\" labels:\n  - Same as ground truth except pixel at row $3$, column $0$ predicted as sky.\n- Predicted instances:\n  - Person: single pixel at row $0$, column $0$, confidence $\\alpha = 0.5$.\n\nAlgorithmic requirements:\n- Build ground-truth panoptic maps with \"thing\" instance ids for \"thing\" pixels and \"stuff\" labels elsewhere.\n- Apply each fusion policy to the predictions to produce a panoptic output. Use four-connectivity to split \"stuff\" labels into connected segments for evaluation. \"Thing\" segments are defined by distinct instance ids.\n- Compute $PQ$ as defined, using greedy one-to-one matching per category, IoU threshold $0.5$ strict. Sum over all categories.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a bracketed pair of the two $PQ$ values [PQ under $\\mathcal{P}_1$, PQ under $\\mathcal{P}_2$]. For example, the output must have the form $[[p_{1,1},p_{1,2}],[p_{2,1},p_{2,2}],[p_{3,1},p_{3,2}]]$ with each $p_{i,j}$ rounded to three decimal places. No physical units or angle units are involved; the outputs are pure decimal numbers.", "solution": "The user has provided a problem that requires the implementation and evaluation of two panoptic segmentation fusion heuristics. The problem is grounded in the field of computer vision and deep learning and uses standard concepts and metrics such as Intersection-over-Union (IoU), Non-Maximum Suppression (NMS), and Panoptic Quality (PQ).\n\n### Step 1: Problem Validation\n\nThe problem statement is critically analyzed against the validation criteria.\n\n**1. Extracted Givens:**\n- **Task**: Implement two panoptic fusion policies ($\\mathcal{P}_1, \\mathcal{P}_2$) and evaluate them using Panoptic Quality (PQ).\n- **Core Definitions**:\n    - Panoptic Quality: $PQ = \\frac{\\sum_{(p,g) \\in \\mathcal{M}} IoU(p,g)}{|\\mathcal{TP}| + \\frac{1}{2}|\\mathcal{FP}| + \\frac{1}{2}|\\mathcal{FN}|}$.\n    - Intersection-over-Union: $IoU(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$.\n    - Match Condition: Same category and $IoU > 0.5$ (strict).\n    - Matching Algorithm: Greedy, one-to-one, per category, sorted by descending IoU.\n- **Fusion Inputs**: A semantic map of \"stuff\" labels per pixel and a set of \"thing\" instance predictions (mask, category, confidence).\n- **Policy $\\mathcal{P}_1$ (instance-priority)**:\n    - Instance confidence threshold $\\tau_1 = 0.6$.\n    - NMS IoU threshold $\\beta_1 = 0.5$.\n    - Conflict Resolution: Instances overwrite stuff. Overlapping instances are resolved by highest confidence (descending order processing).\n- **Policy $\\mathcal{P}_2$ (stuff-protected)**:\n    - Instance confidence threshold $\\tau_2 = 0.7$.\n    - NMS IoU threshold $\\beta_2 = 0.3$.\n    - Conflict Resolution: Pixels belonging to the protected \"stuff\" set $\\{\\text{sky}\\}$ cannot be assigned to an instance. Other conflicts are resolved as in $\\mathcal{P}_1$.\n- **Categories  IDs**:\n    - Stuff: sky $\\rightarrow 1$, road $\\rightarrow 2$.\n    - Thing: person $\\rightarrow 10$, car $\\rightarrow 11$.\n- **Algorithmic Details**:\n    - \"Stuff\" segments are identified using four-connectivity for connected components.\n    - Three fully specified test cases are provided.\n- **Output Format**: A string `[[p11,p12],[p21,p22],[p31,p32]]` with PQ values rounded to three decimal places.\n\n**2. Validation Verdict:**\n-   **Scientifically Grounded**: The problem is based on established concepts from the \"Panoptic Segmentation\" paper by Kirillov et al. (2019). The definitions of PQ, IoU, and NMS are standard or clearly specified variations thereof. The task is a realistic, albeit simplified, representation of a problem in computational image analysis. It is scientifically and factually sound.\n-   **Well-Posed**: The problem is well-posed. All parameters, thresholds, and data are explicitly provided. The fusion and evaluation algorithms are described with sufficient detail to ensure a unique, deterministic solution for each test case.\n-   **Objective**: The problem is stated in precise, objective language. There are no subjective or ambiguous terms.\n-   **Completeness and Consistency**: The problem is self-contained. The provided test data and algorithmic rules are consistent and sufficient for developing a solution. An explicit clarification on the handling of the boundary case $IoU=0.5$ for matching removes potential ambiguity.\n-   **Feasibility**: The problem is computationally feasible. The synthetic test cases are small and manageable.\n\n**Conclusion**: The problem is **valid**. It is a well-defined and rigorous algorithmic challenge within its specified domain.\n\n### Step 2: Solution Design and Implementation\n\nThe solution involves a step-by-step implementation of the required components: data representation, fusion policies, and the PQ evaluation metric.\n\n**1. Data Structures and Initial Setup:**\n-   Images and masks are represented using `numpy` arrays.\n-   Instance predictions are stored as lists of dictionaries, each containing a boolean mask, category ID, and confidence score.\n-   Panoptic maps use a specific encoding: \"stuff\" classes use their category ID ($1, 2, \\dots$), while \"thing\" instances are assigned a unique ID, e.g., `category_id * 1000 + instance_index`. This ensures all segments have a unique identifier in the map.\n-   The ground truth and prediction data for each test case are parsed into these structures. Ground truth panoptic maps are constructed by first laying down \"stuff\" regions and then overwriting them with \"thing\" instance masks, as is standard practice.\n\n**2. Fusion Policies Implementation:**\nA single function `run_policy` is designed to handle both policies by accepting parameters for confidence threshold ($\\tau$), NMS threshold ($\\beta$), and the set of protected \"stuff\" categories.\n\n-   **Instance Filtering**: Instances are first filtered based on the confidence threshold $\\tau$.\n-   **Non-Maximum Suppression (NMS)**: The retained instances are grouped by category and NMS is applied. The standard greedy NMS algorithm is used: instances are sorted by confidence, and an instance is suppressed if its mask IoU with any previously kept instance of the same category exceeds $\\beta$.\n-   **Conflict Resolution and Fusion**:\n    1. The final panoptic map is initialized with the predicted semantic \"stuff\" map.\n    2. All instances that survived NMS are sorted globally by confidence in descending order.\n    3. A loop iterates through the sorted instances. For each instance, its pixels are assigned to the panoptic map, subject to two rules:\n        a. **Stuff Protection (for $\\mathcal{P}_2$)**: Pixels in the instance's mask that correspond to a protected \"stuff\" category in the initial semantic map are not modified.\n        b. **Instance Overlap**: To ensure each pixel is assigned to at most one instance, an instance can only claim pixels that have not already been assigned to a higher-confidence instance. This is achieved by tracking the set of pixels already assigned to any instance.\n\n**3. Segment Extraction:**\nA utility function `get_segments_from_panoptic_map` converts a final panoptic map into a list of segments for evaluation.\n-   **Thing Segments**: For each unique \"thing\" ID in the map, a corresponding segment (category ID and mask) is created.\n-   **Stuff Segments**: For each \"stuff\" category, all pixels belonging to it are isolated. Then, `scipy.ndimage.label` with a four-connectivity structure is used to identify disjoint connected components. Each component is treated as a separate segment.\n\n**4. Panoptic Quality (PQ) Calculation:**\nThe `calculate_pq` function implements the PQ metric as defined.\n-   Ground truth and predicted segments are grouped by category.\n-   For each category, a list of potential matches between predicted and ground truth segments is created, containing pairs with $IoU > 0.5$.\n-   **Greedy Matching**: The list of potential matches is sorted by IoU in descending order. The algorithm iterates through this list, forming one-to-one matches. A match is accepted if neither the predicted nor the ground truth segment has been matched yet.\n-   **Metric Aggregation**: For each category, the number of true positives ($\\mathcal{TP}$, matched pairs), false positives ($\\mathcal{FP}$, unmatched predictions), and false negatives ($\\mathcal{FN}$, unmatched ground truths) are counted. The sum of IoUs for all matched pairs is accumulated.\n-   **Final PQ**: The total sums of `IoU`, $\\mathcal{TP}$, $\\mathcal{FP}$, and $\\mathcal{FN}$ across all categories are used to compute the final PQ value. The denominator is checked for zero to prevent division errors.\n\nThis structured implementation ensures all specifications from the problem statement are met precisely.", "answer": "```python\nimport numpy as np\nfrom scipy.ndimage import label as ndimage_label\n\n# Define given constants\n# Categories\nCAT_SKY = 1\nCAT_ROAD = 2\nCAT_PERSON = 10\nCAT_CAR = 11\n\nSTUFF_CATS = {CAT_SKY, CAT_ROAD}\nTHING_CATS = {CAT_PERSON, CAT_CAR}\nTHING_ID_FACTOR = 1000\n\n# Policy parameters\nTAU_1 = 0.6\nBETA_1 = 0.5\nTAU_2 = 0.7\nBETA_2 = 0.3\nPROTECTED_STUFF_P2 = {CAT_SKY}\nMATCHING_IOU_THRESHOLD = 0.5\n\n# Connectivity for stuff\nCONNECTIVITY_STRUCTURE = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]], dtype=int)\n\ndef iou(mask1, mask2):\n    \"\"\"Computes Intersection-over-Union for two boolean masks.\"\"\"\n    intersection = np.sum(mask1  mask2)\n    union = np.sum(mask1 | mask2)\n    return intersection / union if union > 0 else 0.0\n\ndef apply_nms(instances, beta):\n    \"\"\"Applies Non-Maximum Suppression to a list of instances of the same category.\"\"\"\n    if not instances:\n        return []\n    \n    instances.sort(key=lambda x: x['confidence'], reverse=True)\n    \n    kept_instances = []\n    for inst in instances:\n        is_suppressed = False\n        for kept_inst in kept_instances:\n            if iou(inst['mask'], kept_inst['mask']) > beta:\n                is_suppressed = True\n                break\n        if not is_suppressed:\n            kept_instances.append(inst)\n    return kept_instances\n\ndef get_segments_from_panoptic_map(pano_map):\n    \"\"\"Extracts a list of segments from a panoptic map.\"\"\"\n    segments = []\n    \n    # Extract 'thing' segments\n    thing_ids = np.unique(pano_map[pano_map >= THING_ID_FACTOR])\n    for thing_id in thing_ids:\n        category_id = thing_id // THING_ID_FACTOR\n        mask = (pano_map == thing_id)\n        if np.any(mask):\n            segments.append({\n                'category_id': category_id,\n                'mask': mask,\n            })\n\n    # Extract 'stuff' segments\n    stuff_cat_ids = np.unique(pano_map[pano_map > 0][pano_map[pano_map > 0]  THING_ID_FACTOR])\n    for cat_id in stuff_cat_ids:\n        if cat_id in STUFF_CATS:\n            cat_mask = (pano_map == cat_id)\n            labeled_mask, num_components = ndimage_label(cat_mask, structure=CONNECTIVITY_STRUCTURE)\n            for i in range(1, num_components + 1):\n                component_mask = (labeled_mask == i)\n                segments.append({\n                    'category_id': cat_id,\n                    'mask': component_mask,\n                })\n    return segments\n\ndef calculate_pq(gt_segments, pred_segments):\n    \"\"\"Calculates Panoptic Quality (PQ).\"\"\"\n    gt_by_cat = {cat: [] for cat in STUFF_CATS | THING_CATS}\n    pred_by_cat = {cat: [] for cat in STUFF_CATS | THING_CATS}\n\n    for seg in gt_segments:\n        gt_by_cat[seg['category_id']].append(seg)\n    for seg in pred_segments:\n        pred_by_cat[seg['category_id']].append(seg)\n    \n    total_iou_sum = 0\n    total_tp = 0\n    total_fp = 0\n    total_fn = 0\n\n    all_cats = sorted(list(STUFF_CATS | THING_CATS))\n\n    for cat_id in all_cats:\n        gt_segs = gt_by_cat[cat_id]\n        pred_segs = pred_by_cat[cat_id]\n\n        if not gt_segs and not pred_segs:\n            continue\n\n        matches = []\n        for i, pred_seg in enumerate(pred_segs):\n            for j, gt_seg in enumerate(gt_segs):\n                iou_val = iou(pred_seg['mask'], gt_seg['mask'])\n                if iou_val > MATCHING_IOU_THRESHOLD:\n                    matches.append({'iou': iou_val, 'pred_idx': i, 'gt_idx': j})\n        \n        matches.sort(key=lambda x: x['iou'], reverse=True)\n        \n        gt_matched = np.zeros(len(gt_segs), dtype=bool)\n        pred_matched = np.zeros(len(pred_segs), dtype=bool)\n        \n        cat_tp = 0\n        cat_iou_sum = 0\n        \n        for match in matches:\n            if not gt_matched[match['gt_idx']] and not pred_matched[match['pred_idx']]:\n                gt_matched[match['gt_idx']] = True\n                pred_matched[match['pred_idx']] = True\n                cat_tp += 1\n                cat_iou_sum += match['iou']\n        \n        cat_fp = len(pred_segs) - cat_tp\n        cat_fn = len(gt_segs) - cat_tp\n        \n        total_iou_sum += cat_iou_sum\n        total_tp += cat_tp\n        total_fp += cat_fp\n        total_fn += cat_fn\n\n    denominator = total_tp + 0.5 * total_fp + 0.5 * total_fn\n    return total_iou_sum / denominator if denominator > 0 else 0.0\n\ndef run_policy(H, W, semantic_pred, instances, gt_segments, policy_params):\n    \"\"\"Runs a fusion policy and calculates PQ.\"\"\"\n    tau = policy_params['tau']\n    beta = policy_params['beta']\n    protected_stuff = policy_params.get('protected_stuff', set())\n\n    filtered_instances = [inst for inst in instances if inst['confidence'] >= tau]\n\n    post_nms_instances = []\n    instances_by_cat = {}\n    for inst in filtered_instances:\n        cat = inst['category_id']\n        if cat not in instances_by_cat: instances_by_cat[cat] = []\n        instances_by_cat[cat].append(inst)\n    \n    for cat in instances_by_cat:\n        post_nms_instances.extend(apply_nms(instances_by_cat[cat], beta))\n\n    post_nms_instances.sort(key=lambda x: x['confidence'], reverse=True)\n    \n    for i, inst in enumerate(post_nms_instances):\n        inst['id'] = inst['category_id'] * THING_ID_FACTOR + i + 1\n\n    panoptic_pred_map = semantic_pred.copy()\n    \n    protected_mask = np.zeros((H, W), dtype=bool)\n    if protected_stuff:\n        for cat_id in protected_stuff:\n            protected_mask |= (semantic_pred == cat_id)\n            \n    assigned_instance_pixels = np.zeros((H, W), dtype=bool)\n    for inst in post_nms_instances:\n        inst_mask = inst['mask']\n        if protected_stuff:\n            inst_mask = inst_mask  ~protected_mask\n        \n        pixels_to_assign = inst_mask  ~assigned_instance_pixels\n        panoptic_pred_map[pixels_to_assign] = inst['id']\n        assigned_instance_pixels |= pixels_to_assign\n\n    pred_segments = get_segments_from_panoptic_map(panoptic_pred_map)\n    pq = calculate_pq(gt_segments, pred_segments)\n    \n    return pq\n\ndef solve_case(case_data):\n    \"\"\"Solves a single test case for both policies.\"\"\"\n    H, W = case_data['size']\n    \n    gt_panoptic_map = np.zeros((H, W), dtype=int)\n    for stuff in case_data['gt_stuff']:\n        for r_start, r_end, c_start, c_end in stuff['rects']:\n            gt_panoptic_map[r_start:r_end, c_start:c_end] = stuff['category_id']\n\n    for i, thing in enumerate(case_data['gt_things']):\n        mask = np.zeros((H, W), dtype=bool)\n        if 'rects' in thing:\n            for r_start, r_end, c_start, c_end in thing['rects']:\n                mask[r_start:r_end, c_start:c_end] = True\n        if 'pixels' in thing:\n            for r, c in thing['pixels']:\n                mask[r, c] = True\n        gt_panoptic_map[mask] = thing['category_id'] * THING_ID_FACTOR + i + 1\n    \n    gt_segments = get_segments_from_panoptic_map(gt_panoptic_map)\n    \n    semantic_pred_map = np.full((H, W), case_data['pred_semantic_fill'], dtype=int)\n    for stuff in case_data['pred_semantic']:\n        mask = np.zeros((H, W), dtype=bool)\n        if 'rects' in stuff:\n            for r_start, r_end, c_start, c_end in stuff['rects']:\n                mask[r_start:r_end, c_start:c_end] = True\n        if 'pixels' in stuff:\n            for r, c in stuff['pixels']:\n                mask[r, c] = True\n        semantic_pred_map[mask] = stuff['category_id']\n\n    pred_instances = []\n    for inst in case_data['pred_instances']:\n        mask = np.zeros((H, W), dtype=bool)\n        if 'rects' in inst:\n             for r_start, r_end, c_start, c_end in inst['rects']:\n                mask[r_start:r_end, c_start:c_end] = True\n        if 'pixels' in inst:\n            for r, c in inst['pixels']:\n                mask[r, c] = True\n        pred_instances.append({\n            'category_id': inst['category_id'], 'confidence': inst['confidence'], 'mask': mask\n        })\n\n    pq1 = run_policy(H, W, semantic_pred_map, pred_instances, gt_segments, {'tau': TAU_1, 'beta': BETA_1})\n    pq2 = run_policy(H, W, semantic_pred_map, pred_instances, gt_segments, {'tau': TAU_2, 'beta': BETA_2, 'protected_stuff': PROTECTED_STUFF_P2})\n    \n    return [f\"{pq1:.3f}\", f\"{pq2:.3f}\"]\n\ndef solve():\n    test_cases = [\n        # Test case 1\n        {\n            'size': (5, 5),\n            'gt_stuff': [{'category_id': CAT_SKY, 'rects': [(0, 2, 0, 5)]}, {'category_id': CAT_ROAD, 'rects': [(2, 5, 0, 5)]}],\n            'gt_things': [{'category_id': CAT_PERSON, 'rects': [(2, 4, 2, 4)]}],\n            'pred_semantic': [{'category_id': CAT_SKY, 'rects': [(0, 2, 0, 5), (2, 3, 0, 2)]}],\n            'pred_semantic_fill': CAT_ROAD,\n            'pred_instances': [{'category_id': CAT_PERSON, 'confidence': 0.8, 'rects': [(2, 4, 1, 3)]}, {'category_id': CAT_CAR, 'confidence': 0.6, 'rects': [(0, 2, 3, 5)]}]\n        },\n        # Test case 2\n        {\n            'size': (6, 6),\n            'gt_stuff': [{'category_id': CAT_SKY, 'rects': [(0, 3, 0, 6)]}, {'category_id': CAT_ROAD, 'rects': [(4, 6, 0, 6)]}],\n            'gt_things': [{'category_id': CAT_PERSON, 'rects': [(2, 5, 1, 3)]}, {'category_id': CAT_CAR, 'pixels': [(3, 3), (3, 4), (3, 5)]}],\n            'pred_semantic': [{'category_id': CAT_SKY, 'rects': [(0, 3, 0, 6), (4, 5, 0, 3)]}],\n            'pred_semantic_fill': CAT_ROAD,\n            'pred_instances': [{'category_id': CAT_PERSON, 'confidence': 0.7, 'rects': [(2, 5, 1, 3)]}, {'category_id': CAT_PERSON, 'confidence': 0.65, 'rects': [(1, 5, 1, 4)]}, {'category_id': CAT_CAR, 'confidence': 0.9, 'pixels': [(3, 4), (3, 5), (2, 5)]}]\n        },\n        # Test case 3\n        {\n            'size': (4, 4),\n            'gt_stuff': [{'category_id': CAT_SKY, 'rects': [(0, 2, 0, 4)]}, {'category_id': CAT_ROAD, 'rects': [(2, 4, 0, 4)]}],\n            'gt_things': [],\n            'pred_semantic': [{'category_id': CAT_SKY, 'rects': [(0, 2, 0, 4)], 'pixels': [(3, 0)]}],\n            'pred_semantic_fill': CAT_ROAD,\n            'pred_instances': [{'category_id': CAT_PERSON, 'confidence': 0.5, 'pixels': [(0, 0)]}]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        pq_pair = solve_case(case)\n        results.append(f\"[{','.join(pq_pair)}]\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3136241"}]}