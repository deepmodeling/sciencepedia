{"hands_on_practices": [{"introduction": "The effectiveness of $Q$-learning hinges critically on the agent's ability to distinguish between different situations that require different actions. This practice explores perceptual aliasing, where distinct underlying environment states appear identical to the agent, posing a fundamental challenge to standard Markovian assumptions. By implementing a tabular learner in a simple cyclic environment [@problem_id:3163045], you will discover how a minimal form of memory—remembering the last action—can disambiguate states and enable the learning of an optimal policy, providing a foundational insight into the need for recurrent architectures in partially observable settings.", "problem": "Construct a finite, fully specified Markov Decision Process (MDP) and analyze how action-value iteration through tabular methods retrieves cyclic action choices. Then, evaluate whether adding a minimal recurrent memory to the agent’s observable state helps capture such cycles under observation aliasing. Use the following canonical setup, and implement the algorithmic solution as a complete program.\n\nThe underlying environment is an MDP with state space $\\mathcal{S} = \\{0,1\\}$ and action space $\\mathcal{A} = \\{0,1\\}$. For any time step $t$, if the current state is $s_t \\in \\{0,1\\}$ and the agent takes action $a_t \\in \\{0,1\\}$, then the transition is deterministic: the next state is $s_{t+1} = 1 - s_t$. The reward function is $r_t = 1$ when $a_t = s_t$ and $r_t = 0$ otherwise.\n\nDefine three agent-side “state representations” (what the agent uses as the argument to its action-value function), each inducing different observability levels while the underlying MDP remains the same.\n\n- Full-state representation: the agent observes $x_t = s_t$ at every step.\n\n- Aliased representation: the agent observes the same symbol for both underlying states, i.e., $x_t = 0$ for all $t$ regardless of $s_t$.\n\n- Recurrent augmentation (finite-memory) representation: the agent does not observe the underlying state $s_t$. Instead, it observes a scalar memory $m_t \\in \\{0,1\\}$ representing the previous action taken (with $m_{t+1} = a_t$). At the beginning of each episode, set $m_0 = 0$ and set the initial underlying state to $s_0 = 1$. This alignment ensures that the alternation pattern $a_t = 1 - m_t$ can, in principle, achieve maximal long-run reward under this MDP.\n\nYour program must implement a tabular action-value learner that follows the canonical action-value iteration logic grounded in dynamic programming principles for discounted control. Use an $\\epsilon$-greedy policy with a fixed exploration rate for action selection. Use a fixed constant step-size for learning. Do not use any function approximators beyond simple tables over the agent’s state representation; do not use replay buffers; do not batch updates.\n\nTraining and evaluation protocol:\n- Use a fixed pseudo-random seed so that results are reproducible.\n- For each test case, train for $N$ episodes, each of length $L$ steps. At the start of every episode, set the initial underlying state to $s_0 = 1$. For the recurrent augmentation, also set $m_0 = 0$ at the start of every episode.\n- During training, follow an $\\epsilon$-greedy policy derived from the current action-value estimates.\n- After training, evaluate the greedy (no-exploration) policy induced by the learned action-value function for $T$ steps, starting from the same initial conditions as above for the corresponding representation. Report the empirical average reward per step over these $T$ steps as a floating-point value.\n\nHyperparameters to use uniformly for all test cases:\n- Learning rate (step size) $\\alpha = 0.1$.\n- Exploration rate $\\epsilon = 0.05$.\n- Number of episodes $N = 200$.\n- Episode length $L = 200$.\n- Evaluation horizon $T = 1000$.\n- Pseudo-random seed set to a fixed integer of your choice and kept constant across all test cases.\n\nTest suite:\nFor each pair $(\\text{representation}, \\gamma)$ below, train and then evaluate as specified, and record the resulting empirical average reward per step as a float.\n\n1. $(\\text{full}, \\gamma = 0.0)$\n2. $(\\text{full}, \\gamma = 0.9)$\n3. $(\\text{full}, \\gamma = 0.99)$\n4. $(\\text{aliased}, \\gamma = 0.0)$\n5. $(\\text{aliased}, \\gamma = 0.9)$\n6. $(\\text{aliased}, \\gamma = 0.99)$\n7. $(\\text{recurrent}, \\gamma = 0.0)$\n8. $(\\text{recurrent}, \\gamma = 0.9)$\n9. $(\\text{recurrent}, \\gamma = 0.99)$\n\nNotes and clarifications:\n- In the full-state representation, the optimal stationary policy maps $s \\mapsto a = s$, which induces a two-state cycle over time with a periodic action pattern and maximal average reward.\n- In the aliased representation, the agent observes the same symbol at every step. The best stationary mapping from the aliased observation to an action yields long-run average reward $0.5$ under this MDP, since the hidden state flips deterministically.\n- In the recurrent augmentation representation, the observable memory $m_t$ is updated as $m_{t+1} = a_t$. Starting from $m_0 = 0$ and $s_0 = 1$, the optimal action rule $a_t = 1 - m_t$ induces correct alternation and achieves high average reward.\n\nFinal output format:\n- Your program should produce a single line of output containing $9$ floating-point numbers corresponding to the test suite above, in order, each rounded to exactly three digits after the decimal point, as a comma-separated list enclosed in square brackets. For example, the output must look like: \n\"[0.997,0.998,0.999,0.500,0.501,0.500,0.998,0.999,0.999]\"\n\nAll mathematical symbols and numbers in this problem are specified in LaTeX, and every numerical answer requested is unitless. The output of your program must strictly follow the specified format and rounding.", "solution": "The problem requires the implementation and evaluation of a tabular action-value learning agent under three different state representation schemes for a given deterministic Markov Decision Process (MDP). The underlying MDP is defined by a state space $\\mathcal{S} = \\{0,1\\}$, an action space $\\mathcal{A} = \\{0,1\\}$, a deterministic transition function $s_{t+1} = 1 - s_t$, and a reward function $r_t = 1$ if the action matches the state, $a_t = s_t$, and $r_t = 0$ otherwise. We analyze the agent's performance based on its ability to perceive the underlying state $s_t$.\n\nThe learning algorithm is action-value iteration, commonly known as Q-learning. The update rule for the action-value function $Q(x, a)$, where $x$ is the agent's observable state and $a$ is the chosen action, is given by:\n$$\nQ(x_t, a_t) \\leftarrow Q(x_t, a_t) + \\alpha [r_t + \\gamma \\max_{a'} Q(x_{t+1}, a') - Q(x_t, a_t)]\n$$\nHere, $\\alpha$ is the learning rate, $\\gamma$ is the discount factor, $r_t$ is the reward received at step $t$, and $x_{t+1}$ is the subsequent observable state. Actions during training are selected via an $\\epsilon$-greedy policy, which chooses a random action with probability $\\epsilon$ and the greedy action $\\arg\\max_a Q(x_t, a)$ with probability $1-\\epsilon$.\n\nThe solution involves implementing a unified framework that can accommodate three distinct agent-side representations, each inducing a different level of observability:\n\n1.  **Full-state representation**: The agent's observable state is the true environment state, so $x_t = s_t$. The agent's state space is $\\{0,1\\}$. The Q-table is a $2 \\times 2$ matrix for states $\\{0,1\\}$ and actions $\\{0,1\\}$. The optimal policy is $\\pi^*(s) = s$, which is stationary and directly learnable within this representation. The agent is expected to learn to choose action $a=0$ in state $s=0$ and action $a=1$ in state $s=1$. This policy induces a sequence of states and actions that are always matched, yielding a consistent reward of $1$.\n\n2.  **Aliased representation**: The agent always observes the same symbol, $x_t = 0$, regardless of the underlying state $s_t$. The agent's state space consists of a single state, $\\{0\\}$. The Q-table is a $1 \\times 2$ matrix, representing the values $Q(0,0)$ and $Q(0,1)$. Since the underlying state $s_t$ flips at each step (the sequence starting from $s_0=1$ is $1, 0, 1, 0, \\dots$), any stationary policy $a_t = \\text{const}$ will be correct exactly $50\\%$ of the time. For instance, if the agent always chooses $a_t=1$, the reward sequence will be $1, 0, 1, 0, \\dots$. The best an agent with this limited observation can do is to commit to a fixed action, resulting in an average reward of $0.5$.\n\n3.  **Recurrent augmentation**: The agent's observable state $x_t$ is its own previous action, $m_t = a_{t-1}$. The agent's observable state space is therefore $\\{0,1\\}$, representing the memory of the last action. The initial conditions are set to $s_0=1$ and $m_0=0$. This finite memory allows the agent to break the perceptual aliasing of the previous case. The optimal policy is non-stationary with respect to the underlying state $s_t$ but is stationary with respect to the agent's memory state $m_t$. The optimal action rule is $a_t = 1 - m_t$. This policy creates an alternating sequence of actions ($1, 0, 1, 0, \\dots$ starting with $a_0 = 1-m_0=1$) that perfectly synchronizes with the flipping environment state ($1, 0, 1, 0, \\dots$), yielding a reward of $1$ at every step. The agent's $2 \\times 2$ Q-table should learn to reflect this policy: $Q(0,1)$ and $Q(1,0)$ will converge to high values, while $Q(0,0)$ and $Q(1,1)$ will converge to low values.\n\nThe implementation follows the specified protocol: for each of the nine test cases, combining one of the three representations with a discount factor $\\gamma \\in \\{0.0, 0.9, 0.99\\}$, we train an agent for $N=200$ episodes of $L=200$ steps each. The learning rate is $\\alpha=0.1$ and the exploration rate is $\\epsilon=0.05$. After training, the learned greedy policy ($\\epsilon=0$) is evaluated for $T=1000$ steps, and the empirical average reward per step is recorded. A fixed pseudo-random seed is used across all runs to ensure reproducibility. The program systematically executes these nine test cases and formats the results as a single comma-separated list.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a Q-learning agent for a specified MDP under three\n    different state representations and three discount factors.\n    \"\"\"\n\n    # Hyperparameters as defined in the problem statement\n    ALPHA = 0.1\n    EPSILON = 0.05\n    N_EPISODES = 200\n    L_STEPS = 200\n    T_EVAL_STEPS = 1000\n    SEED = 42  # A fixed integer for reproducibility\n\n    # MDP initial conditions\n    S_INITIAL = 1\n    M_INITIAL = 0\n\n    test_cases = [\n        ('full', 0.0),\n        ('full', 0.9),\n        ('full', 0.99),\n        ('aliased', 0.0),\n        ('aliased', 0.9),\n        ('aliased', 0.99),\n        ('recurrent', 0.0),\n        ('recurrent', 0.9),\n        ('recurrent', 0.99),\n    ]\n\n    results = []\n\n    for representation, gamma in test_cases:\n        # --- Agent and Environment Setup ---\n        if representation == 'full':\n            num_obs_states = 2  # Observable states {0, 1}\n        elif representation == 'aliased':\n            num_obs_states = 1  # Single observable state {0}\n        else:  # 'recurrent'\n            num_obs_states = 2  # Memory states {0, 1}\n        \n        num_actions = 2  # Actions {0, 1}\n\n        # Initialize Q-table with zeros\n        q_table = np.zeros((num_obs_states, num_actions))\n        rng = np.random.default_rng(SEED)\n\n        # --- Training Loop ---\n        for _ in range(N_EPISODES):\n            s_t = S_INITIAL\n            if representation == 'recurrent':\n                m_t = M_INITIAL\n\n            for _ in range(L_STEPS):\n                # 1. Determine agent's observable state\n                if representation == 'full':\n                    obs_state = s_t\n                elif representation == 'aliased':\n                    obs_state = 0\n                else:  # 'recurrent'\n                    obs_state = m_t\n                \n                # 2. Choose action using epsilon-greedy policy\n                if rng.random() < EPSILON:\n                    action = rng.integers(0, num_actions)\n                else:\n                    action = np.argmax(q_table[obs_state, :])\n\n                # 3. Get reward and next underlying state from the environment\n                reward = 1 if action == s_t else 0\n                s_t_plus_1 = 1 - s_t\n\n                # 4. Determine agent's next observable state\n                if representation == 'full':\n                    next_obs_state = s_t_plus_1\n                elif representation == 'aliased':\n                    next_obs_state = 0\n                else:  # 'recurrent'\n                    m_t_plus_1 = action\n                    next_obs_state = m_t_plus_1\n                \n                # 5. Update Q-table (action-value iteration)\n                old_value = q_table[obs_state, action]\n                next_max = np.max(q_table[next_obs_state, :])\n                target = reward + gamma * next_max\n                new_value = old_value + ALPHA * (target - old_value)\n                q_table[obs_state, action] = new_value\n\n                # 6. Update states for the next time step\n                s_t = s_t_plus_1\n                if representation == 'recurrent':\n                    m_t = m_t_plus_1\n        \n        # --- Evaluation Loop ---\n        total_eval_reward = 0\n        s_t = S_INITIAL\n        if representation == 'recurrent':\n            m_t = M_INITIAL\n\n        for _ in range(T_EVAL_STEPS):\n            # 1. Determine observable state\n            if representation == 'full':\n                obs_state = s_t\n            elif representation == 'aliased':\n                obs_state = 0\n            else:  # 'recurrent'\n                obs_state = m_t\n\n            # 2. Choose greedy action (no exploration)\n            action = np.argmax(q_table[obs_state, :])\n\n            # 3. Get reward and update total reward\n            reward = 1 if action == s_t else 0\n            total_eval_reward += reward\n            s_t_plus_1 = 1 - s_t\n\n            # 4. Update states for the next time step\n            s_t = s_t_plus_1\n            if representation == 'recurrent':\n                m_t = action\n\n        avg_reward = total_eval_reward / T_EVAL_STEPS\n        results.append(avg_reward)\n    \n    # --- Final Print Statement ---\n    # Format the results as a comma-separated list of floats rounded to 3 decimal places.\n    formatted_results = [f'{r:.3f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3163045"}, {"introduction": "While combining $Q$-learning with function approximation unlocks immense power, it also introduces significant stability challenges. This exercise confronts the \"deadly triad\" of off-policy learning, function approximation, and bootstrapping, a combination known to cause learning to diverge catastrophically. You will construct a Baird-like counterexample to witness this instability firsthand and then implement two cornerstone solutions that form the bedrock of modern deep reinforcement learning: target networks and Double $Q$-learning. This practice [@problem_id:3163145] is essential for understanding why algorithms like DQN are designed with these specific stabilization mechanisms.", "problem": "You are asked to design and run a reproducible computational experiment that probes the stability of semi-gradient control with bootstrapping in a finite Markov Decision Process (MDP) under function approximation, using a deep linear network as the action-value function approximator. Your task is to implement a Baird-like off-policy setting that can exhibit divergence, and then to test whether two common stabilizing ideas—target networks and Double action-value learning—ameliorate the instability. The final answer must be a complete, runnable program.\n\nThe fundamental base you must start from is:\n- The definition of a Markov Decision Process (MDP) with state set $\\mathcal{S}$, action set $\\mathcal{A}$, transition dynamics $P(s' \\mid s,a)$, reward function $R(s,a)$, and discount factor $\\gamma \\in [0,1)$.\n- The definition of an action-value function $Q^{\\pi}(s,a)$ for a policy $\\pi$, and the Bellman optimality operator for action values as the basis for control updates.\n- The well-tested idea of semi-gradient temporal-difference control (for example as in Deep Q-Networks (DQN), where a bootstrapped target is formed and only the prediction $Q_{\\theta}(s,a)$ is differentiated), and the well-tested ideas of target networks and Double Q-learning (Double DQN uses online selection and target evaluation) as stabilizing variants.\n\nProblem setup to implement:\n1. Environment. Use a finite MDP with $|\\mathcal{S}|=7$ states, indexed as the six upper states $\\{s_0,\\dots,s_5\\}$ and one lower state $s_6$, and two actions $\\mathcal{A}=\\{a_{\\mathrm{solid}}, a_{\\mathrm{dashed}}\\}$. The transition dynamics are deterministic or pseudo-deterministic as follows:\n   - For any upper state $s_i$ with $i \\in \\{0,\\dots,5\\}$:\n     - If $a_{\\mathrm{solid}}$ is taken, the next state is $s_6$.\n     - If $a_{\\mathrm{dashed}}$ is taken, the next state is a uniformly random upper state drawn from $\\{s_0,\\dots,s_5\\}$.\n   - For the lower state $s_6$:\n     - For either action, the next state is a uniformly random upper state drawn from $\\{s_0,\\dots,s_5\\}$.\n   - Rewards are identically zero, $R(s,a)=0$ for all $(s,a)$.\n   - The behavior policy used to sample experience is fixed and off-policy relative to greedy control: at every step, choose $a_{\\mathrm{dashed}}$ with probability $\\beta=0.95$ and $a_{\\mathrm{solid}}$ with probability $1-\\beta=0.05$.\n   - The discount factor is $\\gamma \\in [0,1)$ as specified per test case.\n   All stochastic choices must be generated by a reproducible pseudo-random number generator with a fixed seed.\n\n2. Function approximation. Parameterize the action-value function $Q_{\\theta}(s,a)$ by a deep linear network with no nonlinearity and with architecture input dimension $d_{\\mathrm{in}}=14$ (one-hot for $(s,a)$), one hidden layer of width $h$ (choose $h=4$), and scalar output. Let the parameters be $\\theta = \\{W_1 \\in \\mathbb{R}^{14 \\times 4}, W_2 \\in \\mathbb{R}^{4 \\times 1}\\}$, and define\n   $$Q_{\\theta}(s,a) \\equiv x(s,a)^{\\top} W_1 W_2,$$\n   where $x(s,a) \\in \\mathbb{R}^{14}$ is the one-hot encoding of the state-action pair. Initialize $W_1$ and $W_2$ to small positive values with small random perturbations.\n\n3. Learning rules to compare. For each sampled transition $(s,a,r,s')$ from behavior, perform a semi-gradient step to minimize the squared temporal-difference error with a bootstrapped target constructed differently in each variant:\n   - Plain semi-gradient control (analogous to basic DQN without a target network): form the target using the same parameters that are being updated (the online network), i.e., use the greedy action according to the current $Q_{\\theta}$ and evaluate it with the same $Q_{\\theta}$.\n   - Target network variant: maintain a separate target parameter set $\\bar{\\theta}$ that is periodically hard-updated (copied) from $\\theta$ every $C$ steps. Form the target using $\\bar{\\theta}$ exclusively. The gradient is still taken only through $Q_{\\theta}(s,a)$.\n   - Double action-value variant (Double DQN): form the target by selecting the greedy action at $s'$ under the online parameters $\\theta$ but evaluating that action-value under the target parameters $\\bar{\\theta}$.\n\n4. Divergence detection and trajectory summary. Over a training run of $T$ steps, record the parameter norm\n   $$\\|\\theta_t\\|_2 \\equiv \\sqrt{\\|W_{1,t}\\|_F^2 + \\|W_{2,t}\\|_F^2}$$\n   at regular intervals, compute the sequence of logarithms of norms, and fit a univariate linear model of $\\log \\|\\theta_t\\|_2$ versus the step index $t$ to estimate a slope. Declare a run as “diverged” if either the final-to-initial norm ratio exceeds a specified growth factor and the fitted slope is positive beyond a small threshold, or any numerical overflow or not-a-number occurs. The precise thresholds are left for you to choose, but they must be fixed constants applied uniformly across test cases.\n\nTest suite and required output:\nImplement the following parameter cases. Each case is a tuple $(\\mathrm{algo}, \\alpha, \\gamma, C, T)$ where $\\mathrm{algo} \\in \\{\\text{plain}, \\text{target}, \\text{double}\\}$ is the learning rule, $\\alpha$ is the learning rate, $\\gamma$ is the discount factor, $C$ is the hard-update period for the target parameters in steps (ignored for the plain variant), and $T$ is the number of training steps:\n- Case $1$: $(\\text{plain}, 0.20, 0.99, 1, 20000)$\n- Case $2$: $(\\text{target}, 0.20, 0.99, 50, 20000)$\n- Case $3$: $(\\text{target}, 0.01, 0.99, 50, 20000)$\n- Case $4$: $(\\text{double}, 0.05, 0.99, 50, 20000)$\n- Case $5$ (boundary condition with no bootstrapping carryover): $(\\text{plain}, 0.20, 0.00, 1, 10000)$\n\nYour program must:\n- Use a fixed pseudo-random seed for all randomness.\n- Implement the environment and learning rules precisely as described.\n- For each case, return a boolean indicating whether the run diverged according to your uniform criteria.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the five cases in the given order. For example, the format must be exactly like\n\"[result1,result2,result3,result4,result5]\"\nwhere each result is a boolean literal, either \"True\" or \"False\". No additional text is permitted in the output line.", "solution": "The problem statement has been validated and is deemed valid. It presents a well-posed, scientifically grounded, and objective computational experiment in the field of deep reinforcement learning. The task is to implement a specific Markov Decision Process (MDP) and several semi-gradient Q-learning algorithms to investigate the stability of off-policy learning with function approximation. The setup is a variant of the classic Baird's counterexample, designed to elicit divergence. The use of a deep linear network as the function approximator is a modern and relevant formulation. The instructions are complete, consistent, and formalizable into a runnable program.\n\nThe solution proceeds as follows: First, we formalize the components of the experiment—the MDP, the function approximator, and the learning rules. Second, we detail the implementation of the simulation loop. Finally, we specify the criteria for detecting divergence.\n\nThe environment is a finite MDP defined by a tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$.\n- The state set is $\\mathcal{S} = \\{s_0, s_1, s_2, s_3, s_4, s_5, s_6\\}$, where $|\\mathcal{S}|=7$. States $\\{s_0, \\dots, s_5\\}$ are designated as \"upper states\" and $s_6$ as the \"lower state\".\n- The action set is $\\mathcal{A} = \\{a_{\\text{solid}}, a_{\\text{dashed}}\\}$.\n- The transition dynamics $P(s'|s,a)$ are deterministic for some transitions and stochastic for others:\n  - For an upper state $s \\in \\{s_0, \\dots, s_5\\}$:\n    - taking $a_{\\text{solid}}$ leads to $s' = s_6$.\n    - taking $a_{\\text{dashed}}$ leads to a next state $s'$ drawn uniformly at random from $\\{s_0, \\dots, s_5\\}$.\n  - For the lower state $s_6$, any action leads to a next state $s'$ drawn uniformly at random from $\\{s_0, \\dots, s_5\\}$.\n- The reward function is identically zero for all transitions: $R(s,a,s')=0$.\n- The discount factor is a parameter $\\gamma \\in [0, 1)$, specified per test case.\n- The behavior policy $\\pi_b$, used to generate experience, is fixed to choose $a_{\\text{dashed}}$ with probability $\\beta=0.95$ and $a_{\\text{solid}}$ with probability $1-\\beta=0.05$.\n\nThe action-value function $Q(s,a)$ is approximated by a deep linear network with parameters $\\theta = \\{W_1, W_2\\}$. The network has one hidden layer of width $h=4$.\n- The input is a one-hot vector $x(s,a) \\in \\mathbb{R}^{14}$ representing one of the $7 \\times 2 = 14$ possible state-action pairs.\n- The network parameters are two matrices: $W_1 \\in \\mathbb{R}^{14 \\times 4}$ and $W_2 \\in \\mathbb{R}^{4 \\times 1}$.\n- The action-value is computed as $Q_{\\theta}(s,a) = x(s,a)^{\\top} W_1 W_2$.\n\nThe learning process involves updating the parameters $\\theta$ using a semi-gradient method based on a single sampled transition $(s_t, a_t, r_t, s_{t+1})$, where $r_t=0$. The update rule is a stochastic gradient descent step on the squared TD error:\n$$ \\theta_{t+1} \\leftarrow \\theta_t - \\alpha \\cdot (Q_{\\theta_t}(s_t, a_t) - Y_t) \\cdot \\nabla_{\\theta_t} Q_{\\theta_t}(s_t, a_t) $$\nHere, $\\alpha$ is the learning rate, and $Y_t$ is the bootstrapped target. The key difference between the algorithms lies in the construction of $Y_t$. Let $\\theta_t$ be the online parameters being updated and $\\bar{\\theta}_t$ be a separate set of target parameters.\n- **Plain semi-gradient control:** The target is formed using only the online parameters.\n  $$ Y_t^{\\text{plain}} = r_t + \\gamma \\max_{a'} Q_{\\theta_t}(s_{t+1}, a') $$\n- **Target network variant:** The target is formed using the fixed target-network parameters $\\bar{\\theta}_t$, which are updated to match the online parameters $\\theta_t$ every $C$ steps (i.e., $\\bar{\\theta}_{t} \\leftarrow \\theta_{t}$ if $t \\pmod C = 0$).\n  $$ Y_t^{\\text{target}} = r_t + \\gamma \\max_{a'} Q_{\\bar{\\theta}_t}(s_{t+1}, a') $$\n- **Double action-value variant:** The greedy action is selected using the online network, but its value is estimated using the target network. This also uses a target network updated every $C$ steps.\n  $$ Y_t^{\\text{double}} = r_t + \\gamma Q_{\\bar{\\theta}_t}(s_{t+1}, \\arg\\max_{a'} Q_{\\theta_t}(s_{t+1}, a')) $$\n\nSince $Q_{\\theta}(s,a)$ is a linear function of its parameters (in a factored form), the gradient $\\nabla_{\\theta} Q_{\\theta}(s,a)$ is computed with respect to $W_1$ and $W_2$. Using the chain rule, we have:\n$$ \\nabla_{W_1} Q_{\\theta}(s,a) = x(s,a) W_2^{\\top} $$\n$$ \\nabla_{W_2} Q_{\\theta}(s,a) = W_1^{\\top} x(s,a) $$\nGiven that $x(s,a)$ is a one-hot vector with a $1$ at index $k$, the gradient updates simplify. The update for $W_1$ only affects its $k$-th row, and the update for $W_2$ depends on the $k$-th row of $W_1$.\n\nTo assess stability, we monitor the trajectory of the parameter norm $\\|\\theta_t\\|_2 = \\sqrt{\\|W_{1,t}\\|_F^2 + \\|W_{2,t}\\|_F^2}$, where $\\|\\cdot\\|_F$ is the Frobenius norm. A run is declared to have diverged if one of two conditions is met:\n1. A numerical overflow (`NaN` or `inf`) is detected in the parameters at any step.\n2. The parameters exhibit sustained exponential growth. This is assessed at the end of a run of $T$ steps by checking if both of the following are true:\n   a. The ratio of the final to initial parameter norm exceeds a growth factor threshold: $\\|\\theta_T\\|_2 / \\|\\theta_0\\|_2 > G_{\\text{thresh}}$. We set $G_{\\text{thresh}} = 1000$.\n   b. A linear regression of the log-norm, $\\log \\|\\theta_t\\|_2$, against the time step $t$ yields a positive slope greater than a threshold: $m > m_{\\text{thresh}}$. We set $m_{\\text{thresh}} = 10^{-5}$. The norm is recorded at regular intervals to perform this fit.\n\nThe computational experiment is conducted for each of the five specified test cases. For reproducibility, all stochastic elements (initial weights, environment transitions, behavior policy choices) are derived from a pseudo-random number generator initialized with a fixed seed. The seed is reset for each test case to ensure comparable conditions. The boolean result (diverged or not) for each case is collected and reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are imported, satisfying the constraints.\n\n# --- Problem Constants and Configuration ---\n# MDP parameters\nNUM_STATES = 7\nNUM_ACTIONS = 2\nUPPER_STATES = list(range(6))\nLOWER_STATE = 6\nACTION_SOLID = 0\nACTION_DASHED = 1\nBEHAVIOR_PROB_DASHED = 0.95\n\n# Function approximator architecture\nINPUT_DIM = 14\nHIDDEN_DIM = 4\nOUTPUT_DIM = 1\n\n# Divergence detection parameters\nGROWTH_FACTOR_THRESH = 1000.0\nSLOPE_THRESH = 1e-5\nNORM_LOG_INTERVAL = 100\n\n# Reproducibility\nSEED = 42\n\ndef step_env(state, action, rng):\n    \"\"\"Simulates one step in the MDP.\"\"\"\n    if state in UPPER_STATES:\n        if action == ACTION_SOLID:\n            next_state = LOWER_STATE\n        else:  # ACTION_DASHED\n            next_state = rng.choice(UPPER_STATES)\n    else:  # state == LOWER_STATE\n        next_state = rng.choice(UPPER_STATES)\n    return next_state\n\ndef select_behavior_action(rng):\n    \"\"\"Samples an action from the behavior policy.\"\"\"\n    if rng.random() < BEHAVIOR_PROB_DASHED:\n        return ACTION_DASHED\n    else:\n        return ACTION_SOLID\n\ndef get_q_values(state, W_eff):\n    \"\"\"Computes Q(state, a) for all actions 'a' given an effective weight matrix.\"\"\"\n    return W_eff[2 * state: 2 * state + 2, 0]\n\ndef run_experiment(algo, alpha, gamma, C, T, rng):\n    \"\"\"\n    Runs a single experiment for a given configuration.\n\n    Returns:\n        bool: True if divergence is detected, False otherwise.\n    \"\"\"\n    # 1. Initialize parameters\n    W1 = 0.1 + 0.01 * rng.standard_normal(size=(INPUT_DIM, HIDDEN_DIM))\n    W2 = 0.1 + 0.01 * rng.standard_normal(size=(HIDDEN_DIM, OUTPUT_DIM))\n    \n    W1_target = np.copy(W1)\n    W2_target = np.copy(W2)\n    \n    # 2. Initialize tracking variables\n    initial_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n    norm_history = []\n    steps_history = []\n    has_overflown = False\n    \n    # 3. Training Loop\n    state = rng.choice(UPPER_STATES)\n    for t in range(T):\n        # Sample transition from behavior policy\n        action = select_behavior_action(rng)\n        reward = 0.0\n        next_state = step_env(state, action, rng)\n\n        # Pre-compute effective weight matrices\n        W_online = W1 @ W2\n        W_target = W1_target @ W2_target\n\n        # Get Q-value for the current state-action pair\n        sa_index = 2 * state + action\n        q_sa = W_online[sa_index, 0]\n        \n        # Calculate target value Yt based on algorithm\n        q_next_online = get_q_values(next_state, W_online)\n        target_q_val = 0.0\n        \n        if algo == 'plain':\n            target_q_val = np.max(q_next_online)\n        elif algo == 'target':\n            q_next_target = get_q_values(next_state, W_target)\n            target_q_val = np.max(q_next_target)\n        elif algo == 'double':\n            q_next_target = get_q_values(next_state, W_target)\n            best_action_online = np.argmax(q_next_online)\n            target_q_val = q_next_target[best_action_online]\n\n        td_target = reward + gamma * target_q_val\n        td_error = q_sa - td_target\n\n        # Calculate gradients and perform semi-gradient update\n        grad_wrt_w1_row = W2.T\n        grad_wrt_w2_col = W1[sa_index, :].reshape(HIDDEN_DIM, 1)\n\n        W1[sa_index, :] -= alpha * td_error * grad_wrt_w1_row.flatten()\n        W2 -= alpha * td_error * grad_wrt_w2_col\n\n        # Update target network if applicable\n        if algo in ['target', 'double'] and (t + 1) % C == 0:\n            W1_target = np.copy(W1)\n            W2_target = np.copy(W2)\n        \n        # Check for numerical overflow\n        if not np.all(np.isfinite(W1)) or not np.all(np.isfinite(W2)):\n            has_overflown = True\n            break\n            \n        # Log parameter norm\n        if t % NORM_LOG_INTERVAL == 0:\n            current_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n            norm_history.append(current_norm)\n            steps_history.append(t)\n            \n        # Update state for next iteration\n        state = next_state\n\n    # 4. Divergence Analysis\n    if has_overflown:\n        return True\n\n    if not norm_history: # In case T is very small\n        return False\n\n    final_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n\n    # Avoid division by zero if initial norm is zero\n    if initial_norm > 1e-9:\n        norm_ratio = final_norm / initial_norm\n    else:\n        norm_ratio = np.inf if final_norm > 1e-9 else 1.0\n\n    # Fit linear model to log(norm) vs. steps\n    log_norms = np.log(np.maximum(norm_history, 1e-9)) # avoid log(0)\n    \n    # np.polyfit returns [slope, intercept] for deg=1\n    if len(steps_history) > 1:\n        slope = np.polyfit(steps_history, log_norms, 1)[0]\n    else:\n        slope = 0.0\n\n    # Apply divergence criteria\n    if norm_ratio > GROWTH_FACTOR_THRESH and slope > SLOPE_THRESH:\n        return True\n        \n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of experiments and print results.\n    \"\"\"\n    # Parameter cases from the problem statement\n    test_cases = [\n        # (algo, alpha, gamma, C, T)\n        ('plain', 0.20, 0.99, 1, 20000),\n        ('target', 0.20, 0.99, 50, 20000),\n        ('target', 0.01, 0.99, 50, 20000),\n        ('double', 0.05, 0.99, 50, 20000),\n        ('plain', 0.20, 0.00, 1, 10000)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Reset the seed for each run to ensure comparable conditions\n        rng = np.random.default_rng(SEED)\n        diverged = run_experiment(*case, rng=rng)\n        results.append(str(diverged))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3163145"}, {"introduction": "Beyond the update rule itself, the way an agent utilizes its past experiences can dramatically affect learning stability. This practice moves beyond simple uniform sampling from a replay buffer to investigate more sophisticated data curation strategies, particularly in settings prone to high-variance Temporal Difference (TD) errors. You will implement and compare standard Prioritized Experience Replay (PER) with a balanced sampling method that aims to create more statistically diverse mini-batches. Through this hands-on comparison [@problem_id:3163134], you will gain a deeper appreciation for how controlling the data distribution for gradient updates serves as a powerful tool for stabilizing training and improving agent performance.", "problem": "You are asked to design and evaluate a replay buffering and sampling strategy for value-based reinforcement learning that explicitly controls the distribution of Temporal Difference (TD) error magnitudes. Your program must implement and compare three samplers for a replay buffer used with a linear state-action value function (also called a linear $Q$-function), and quantitatively assess whether a balanced TD error distribution mitigates gradient domination by outliers and stabilizes $Q$ updates. The comparison must be performed on a fixed synthetic dataset generated from a Markov Decision Process (MDP) with two actions and continuous features.\n\nYou must start from the following foundational base:\n\n- A Markov Decision Process (MDP) has a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a transition kernel $p(\\mathbf{s}^{\\prime}\\mid \\mathbf{s}, a)$, a reward function $r(\\mathbf{s}, a)$, and a discount factor $\\gamma \\in (0,1)$.\n- The action-value function (the $Q$-function) for a policy is $Q(\\mathbf{s}, a) = \\mathbb{E}\\big[\\sum_{t=0}^{\\infty} \\gamma^{t} r(\\mathbf{s}_{t}, a_{t}) \\,\\big|\\, \\mathbf{s}_{0}=\\mathbf{s}, a_{0}=a\\big]$.\n- The Temporal Difference (TD) error for one-step $Q$-learning with a target network is defined as $\\delta = r + \\gamma \\max_{a^{\\prime}} Q_{\\text{tgt}}(\\mathbf{s}^{\\prime}, a^{\\prime}) - Q(\\mathbf{s}, a)$, where $Q_{\\text{tgt}}$ is a target $Q$ function held fixed for several parameter updates.\n- In function approximation with a squared TD error loss $\\ell = \\tfrac{1}{2}\\delta^{2}$ and a target treated as a constant, the gradient step is proportional to $\\delta \\,\\nabla_{\\boldsymbol{\\theta}} Q(\\mathbf{s}, a; \\boldsymbol{\\theta})$.\n\nYour program must:\n\n- Construct a fixed dataset of transitions $(\\mathbf{s}, a, r, \\mathbf{s}^{\\prime})$ of size $N$ with $\\mathbf{s} \\in \\mathbb{R}^{d}$, two actions $a \\in \\{0,1\\}$, and a reward $r$ that is a linear function of action-augmented features plus additive noise. The noise must be a mixture with an outlier mode to produce a heavy-tailed error scenario. Actions must be uniformly distributed. The dataset must be generated once per test case and reused across all samplers for that test case to ensure a fair comparison. Use a fixed random seed for reproducibility.\n- Use a linear $Q$-function $Q(\\mathbf{s}, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a)$ with features $\\boldsymbol{\\phi}(\\mathbf{s}, a) \\in \\mathbb{R}^{2d}$ defined by concatenating $\\mathbf{s}$ into either the first or second $d$ coordinates depending on $a$. Initialize $\\boldsymbol{\\theta}$ to zero.\n- Use a discount factor $\\gamma$, a learning rate $\\eta$, a mini-batch size $B$, a total of $T$ parameter update steps per sampler, and a target network update period $K$ steps. The target network parameters $\\boldsymbol{\\theta}_{\\text{tgt}}$ must be synchronized with $\\boldsymbol{\\theta}$ every $K$ steps.\n- Maintain in the replay buffer a per-transition estimate of the absolute TD error magnitude $|\\delta|$, recomputed periodically during training for all samples to keep sampling statistics up to date.\n\nImplement three sampling strategies over the replay buffer:\n\n- Uniform: sample each transition with equal probability.\n- Proportional Prioritized Experience Replay (PER): sample transition $i$ with probability proportional to $(|\\delta_{i}| + \\varepsilon)^{\\alpha}$, where $\\alpha \\in [0,1]$ controls prioritization strength, and $\\varepsilon$ is a small positive number to avoid zero probabilities.\n- Balanced-Quantile TD Sampler (the method to be evaluated): compute empirical quantile bin edges of the distribution of $|\\delta|$ into $Q$ bins of equal mass, and draw approximately $B/Q$ samples uniformly from each bin (with replacement if needed) so that the mini-batch contains a balanced mixture across TD error magnitudes.\n\nDefine and compute the following quantitative diagnostics during training for each sampler:\n\n- Gradient domination by outliers metric: at each update step $t$, compute per-sample contribution norms $\\|\\delta_{i} \\,\\boldsymbol{\\phi}(\\mathbf{s}_{i}, a_{i})\\|_{2}$ for the $B$ samples in the mini-batch. Let $k=\\lceil 0.1 B \\rceil$ be the top-decile count. Let $G_{t}$ be the sum of these norms over the top $k$ samples divided by the sum over all $B$ samples (with a tiny positive stabilizer to avoid division by zero). The domination metric for a run is the mean of $G_{t}$ over the second half of training steps (to exclude early transients).\n- $Q$-update stability metric: at each step $t$, compute the mean mini-batch loss $\\bar{\\ell}_{t} = \\tfrac{1}{2}\\tfrac{1}{B}\\sum_{i=1}^{B}\\delta_{i}^{2}$. The stability metric for a run is the sample variance of $\\bar{\\ell}_{t}$ over all steps after an initial burn-in.\n\nFor each test case, your program must run training independently for the three samplers and report a trio of decision booleans:\n\n- $b_{1}$: True if the balanced-quantile sampler reduces gradient domination compared to PER by more than a small tolerance, i.e., if $\\overline{G}_{\\text{balanced}} < \\overline{G}_{\\text{PER}} - \\tau_{g}$, otherwise False.\n- $b_{2}$: True if the balanced-quantile sampler reduces loss variance compared to PER by more than a small tolerance, i.e., if $\\mathrm{Var}(\\bar{\\ell})_{\\text{balanced}} < \\mathrm{Var}(\\bar{\\ell})_{\\text{PER}} - \\tau_{v}$, otherwise False.\n- $b_{3}$: True if the balanced-quantile sampler with $Q=1$ degenerates to uniform sampling in terms of the domination metric up to a tolerance, i.e., if $|\\overline{G}_{\\text{balanced}} - \\overline{G}_{\\text{uniform}}| \\le \\tau_{m}$, otherwise False. For cases where $Q \\ne 1$, set $b_{3}$ to False.\n\nParameter settings common to all runs:\n\n- Feature dimension $d = 4$.\n- Dataset size $N = 3000$.\n- Discount factor $\\gamma = 0.97$.\n- Learning rate $\\eta = 0.01$.\n- Total update steps $T = 400$.\n- Target update period $K = 20$.\n- TD error table recomputation period for all transitions is every $10$ steps.\n- PER parameters: exponent $\\alpha = 0.95$, offset $\\varepsilon = 10^{-3}$.\n- Stabilizers for metric computations may use $\\epsilon = 10^{-12}$ in denominators.\n\nTest suite. Your program must execute the following three test cases:\n\n- Case A (heavy-tailed noise): outlier probability $p_{\\mathrm{out}} = 0.2$, outlier scale factor $s_{\\mathrm{out}} = 8.0$, balanced sampler bins $Q = 5$, batch size $B = 64$.\n- Case B (no outliers): outlier probability $p_{\\mathrm{out}} = 0.0$, outlier scale factor $s_{\\mathrm{out}} = 8.0$, balanced sampler bins $Q = 5$, batch size $B = 64$.\n- Case C (degenerate balanced sampling): outlier probability $p_{\\mathrm{out}} = 0.2$, outlier scale factor $s_{\\mathrm{out}} = 8.0$, balanced sampler bins $Q = 1$, batch size $B = 64$.\n\nReward noise model. Let base noise be Gaussian with standard deviation $\\sigma_{\\mathrm{base}} = 0.1$. With probability $p_{\\mathrm{out}}$, replace the base noise with Gaussian of standard deviation $\\sigma_{\\mathrm{out}} = s_{\\mathrm{out}} \\cdot \\sigma_{\\mathrm{base}}$.\n\nOutput tolerances to decide booleans:\n\n- Gradient domination tolerance $\\tau_{g} = 0.02$.\n- Loss variance tolerance $\\tau_{v} = 10^{-5}$.\n- Degeneracy match tolerance $\\tau_{m} = 0.03$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of three booleans in the order $[b_{1}, b_{2}, b_{3}]$. For example: \"[[True,False,False],[False,False,False],[True,True,True]]\". There must be no extra spaces or characters beyond this single line.\n\nAngle units are not applicable. There are no physical units in this problem; all quantities are unitless real numbers. All numerical values in this problem statement must be respected exactly as specified.", "solution": "The problem statement is valid. It specifies a well-defined computational experiment in reinforcement learning that is scientifically grounded, objective, and contains sufficient information to produce a unique, verifiable result. The minor ambiguities regarding the precise Markov Decision Process (MDP) dynamics are resolved by the problem's emphasis on comparing samplers on a fixed, generated dataset, allowing for reasonable, fixed choices for these underlying functions.\n\nThe objective is to implement and compare three experience replay sampling strategies—Uniform, Prioritized Experience Replay (PER), and a proposed Balanced-Quantile TD Sampler—to assess their impact on the stability of Q-learning. The assessment will be based on two metrics: gradient domination by outliers and Q-update loss variance.\n\nFirst, we establish the synthetic data-generating process. The environment is a Markov Decision Process with a continuous state space $\\mathcal{S} \\subseteq \\mathbb{R}^{d}$ with $d=4$, and a discrete action space $\\mathcal{A} = \\{0, 1\\}$. A fixed dataset of $N=3000$ transitions $(\\mathbf{s}, a, r, \\mathbf{s}^{\\prime})$ is generated for each test case. State vectors $\\mathbf{s}$ are drawn independently from a standard multivariate normal distribution, $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$. Action $a$ is chosen uniformly from $\\{0, 1\\}$. The transition dynamics are defined as a simple linear transformation $\\mathbf{s}^{\\prime} = \\mathbf{M}_a \\mathbf{s}$, where $\\mathbf{M}_0$ and $\\mathbf{M}_1$ are fixed random matrices generated once per test case and scaled by $1/\\sqrt{d}$ to prevent the norm of states from exploding. The reward $r$ is a linear function of action-augmented features plus noise: $r(\\mathbf{s}, a) = \\boldsymbol{w}_r^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a) + \\epsilon_r$, where $\\boldsymbol{w}_r$ is a fixed random weight vector. The noise $\\epsilon_r$ is drawn from a mixture model: with probability $1-p_{\\text{out}}$, it comes from a base Gaussian distribution $\\mathcal{N}(0, \\sigma_{\\text{base}}^2)$ where $\\sigma_{\\text{base}}=0.1$; with probability $p_{\\text{out}}$, it comes from an outlier distribution $\\mathcal{N}(0, \\sigma_{\\text{out}}^2)$ where $\\sigma_{\\text{out}} = s_{\\text{out}} \\cdot \\sigma_{\\text{base}}$. This setup, particularly when $p_{\\text{out}} > 0$, is designed to create a heavy-tailed distribution of Temporal Difference (TD) errors. This same dataset is used for all three samplers within a test case to ensure a fair comparison.\n\nThe learning agent uses a linear function approximator for the action-value function: $Q(\\mathbf{s}, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a)$. The feature vector $\\boldsymbol{\\phi}(\\mathbf{s}, a) \\in \\mathbb{R}^{2d}$ is defined by concatenating the state vector $\\mathbf{s}$ into one of two positions based on the action: $\\boldsymbol{\\phi}(\\mathbf{s}, a=0) = [\\mathbf{s}^{\\top}, \\mathbf{0}_d^{\\top}]^{\\top}$ and $\\boldsymbol{\\phi}(\\mathbf{s}, a=1) = [\\mathbf{0}_d^{\\top}, \\mathbf{s}^{\\top}]^{\\top}$. The parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2d}$ are initialized to zero. Learning proceeds via mini-batch stochastic gradient descent on the mean squared TD error, $\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\mathbb{E}[ (r + \\gamma \\max_{a'} Q(\\mathbf{s}', a'; \\boldsymbol{\\theta}_{\\text{tgt}}) - Q(\\mathbf{s}, a; \\boldsymbol{\\theta}))^2 ]$. The gradient update for a mini-batch of size $B$ is $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\frac{\\eta}{B} \\sum_{i=1}^{B} \\delta_i \\boldsymbol{\\phi}(\\mathbf{s}_i, a_i)$, where $\\eta=0.01$ is the learning rate. A target network with parameters $\\boldsymbol{\\theta}_{\\text{tgt}}$, synchronized with $\\boldsymbol{\\theta}$ every $K=20$ steps, is used to compute the TD target $y = r + \\gamma \\max_{a'} Q_{\\text{tgt}}(\\mathbf{s}', a')$, stabilizing the learning process. The TD error for sample $i$ is $\\delta_i = y_i - Q(\\mathbf{s}_i, a_i; \\boldsymbol{\\theta})$.\n\nThe core of the investigation lies in the three sampling strategies:\n1.  **Uniform Sampling**: This is the baseline strategy where each of the $N$ transitions in the replay buffer is sampled with equal probability $1/N$. It makes no assumptions about the utility of any given transition.\n2.  **Proportional Prioritized Experience Replay (PER)**: This method prioritizes transitions from which the agent can learn the most, under the assumption that the magnitude of the TD error $|\\delta|$ is a good proxy for \"surprise\" or learning potential. A transition $i$ is sampled with probability $P_i \\propto (|\\delta_i| + \\varepsilon)^{\\alpha}$, with prioritization exponent $\\alpha=0.95$ and a small constant $\\varepsilon=10^{-3}$ to ensure non-zero probabilities. This strategy oversamples high-error transitions. To implement this, the absolute TD errors $|\\delta_i|$ for all $N$ transitions are re-evaluated every $10$ steps.\n3.  **Balanced-Quantile TD Sampler**: This proposed strategy is designed to mitigate the risk of training instability caused by PER's focus on high-error outliers. It aims to construct mini-batches that are diverse with respect to TD error magnitudes. It operates by partitioning the entire replay buffer into $Q$ quantile bins based on the distribution of $|\\delta_i|$. First, all $N$ transitions are sorted by their current $|\\delta_i|$ values. The sorted list of indices is then split into $Q$ contiguous blocks, each containing $N/Q$ indices, thus forming bins of equal mass. To form a mini-batch of size $B$, approximately $B/Q$ samples are drawn uniformly (with replacement) from each of these $Q$ bins. This ensures that the mini-batch gradient is an average over contributions from low, medium, and high-error samples, thereby promoting more stable updates. For the specific case where $Q=1$, the single bin contains all samples, and this method becomes equivalent to uniform sampling.\n\nTo quantify the performance of each sampler, two metrics are computed over the second half of the $T=400$ training steps (i.e., after a burn-in period of $200$ steps):\n-   **Gradient Domination Metric ($\\overline{G}$)**: For each mini-batch update, we compute the norms of each sample's contribution to the gradient update, $\\|\\delta_i \\boldsymbol{\\phi}(\\mathbf{s}_i, a_i)\\|_2$. The per-step metric $G_t$ is the ratio of the sum of norms from the top decile (top $\\lceil 0.1 B \\rceil$ samples) to the total sum of norms in the batch. A high value indicates that a few outlier samples are dominating the gradient. The final metric $\\overline{G}$ is the mean of $G_t$ over the evaluation period.\n-   **Q-Update Stability Metric ($\\mathrm{Var}(\\bar{\\ell})$)**: For each mini-batch, the mean squared TD error (loss) is calculated: $\\bar{\\ell}_t = \\frac{1}{2B}\\sum_{i=1}^B \\delta_i^2$. The sample variance of this loss sequence, $\\mathrm{Var}(\\bar{\\ell}_t)$, is computed over the evaluation period. A lower variance suggests more stable and predictable learning dynamics.\n\nFor each test case, the three samplers are run independently, and the resulting metrics are used to evaluate three boolean conditions based on specified tolerances ($\\tau_g=0.02$, $\\tau_v=10^{-5}$, $\\tau_m=0.03$), which compare the balanced sampler against PER and uniform sampling.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the reinforcement learning sampler comparison experiment.\n    \"\"\"\n\n    # Common parameters\n    D = 4\n    N = 3000\n    GAMMA = 0.97\n    ETA = 0.01\n    T = 400\n    K = 20\n    TD_RECOMPUTE_PERIOD = 10\n    ALPHA = 0.95\n    EPS_PER = 1e-3\n    EPS_METRIC = 1e-12\n    SIGMA_BASE = 0.1\n    TAU_G = 0.02\n    TAU_V = 1e-5\n    TAU_M = 0.03\n\n    def phi(s, a, d_val):\n        \"\"\"Constructs the action-augmented feature vector.\"\"\"\n        features = np.zeros(2 * d_val)\n        if a == 0:\n            features[:d_val] = s\n        else:\n            features[d_val:] = s\n        return features\n\n    def generate_dataset(n_val, d_val, p_out, s_out, rng):\n        \"\"\"Generates a fixed synthetic dataset of transitions.\"\"\"\n        w_r = rng.standard_normal(size=2 * d_val)\n        m_0 = rng.standard_normal(size=(d_val, d_val)) / np.sqrt(d_val)\n        m_1 = rng.standard_normal(size=(d_val, d_val)) / np.sqrt(d_val)\n        \n        states = rng.standard_normal(size=(n_val, d_val))\n        actions = rng.integers(0, 2, size=n_val)\n        \n        next_states = np.zeros_like(states)\n        rewards = np.zeros(n_val)\n        \n        sigma_out = s_out * SIGMA_BASE\n        \n        for i in range(n_val):\n            s, a = states[i], actions[i]\n            \n            if a == 0:\n                next_states[i] = m_0 @ s\n            else:\n                next_states[i] = m_1 @ s\n            \n            features = phi(s, a, d_val)\n            r_det = w_r @ features\n            \n            noise_std = sigma_out if rng.random() < p_out else SIGMA_BASE\n            noise = rng.normal(0, noise_std)\n            rewards[i] = r_det + noise\n            \n        return states, actions, rewards, next_states\n\n    def run_sampler_experiment(sampler_type, dataset, params, run_rng):\n        \"\"\"Runs the training loop for a given sampler and returns metrics.\"\"\"\n        b_val, q_val = params['B'], params['Q']\n        \n        theta = np.zeros(2 * D)\n        theta_tgt = np.zeros(2 * D)\n        td_errors_abs = np.ones(N)\n\n        g_values = []\n        loss_values = []\n        \n        states, actions, rewards, next_states = dataset\n\n        # Pre-compute features for efficiency\n        phis = np.array([phi(s, a, D) for s, a in zip(states, actions)])\n        phis_next_0 = np.array([phi(s_prime, 0, D) for s_prime in next_states])\n        phis_next_1 = np.array([phi(s_prime, 1, D) for s_prime in next_states])\n\n        for t in range(1, T + 1):\n            if (t - 1) % TD_RECOMPUTE_PERIOD == 0:\n                q_values = phis @ theta\n                q_tgt_next_0 = phis_next_0 @ theta_tgt\n                q_tgt_next_1 = phis_next_1 @ theta_tgt\n                max_q_tgt_next = np.maximum(q_tgt_next_0, q_tgt_next_1)\n                deltas = rewards + GAMMA * max_q_tgt_next - q_values\n                td_errors_abs = np.abs(deltas)\n\n            if sampler_type == 'uniform':\n                batch_indices = run_rng.choice(N, size=b_val, replace=True)\n            elif sampler_type == 'per':\n                probs = (td_errors_abs + EPS_PER) ** ALPHA\n                probs_sum = probs.sum()\n                if probs_sum > 0:\n                    probs /= probs_sum\n                else: # Fallback to uniform if all probabilities are zero\n                    probs = np.full(N, 1/N)\n                batch_indices = run_rng.choice(N, size=b_val, p=probs, replace=True)\n            else: # balanced\n                if q_val == 1:\n                    batch_indices = run_rng.choice(N, size=b_val, replace=True)\n                else:\n                    samples_per_bin_base = b_val // q_val\n                    rem = b_val % q_val\n                    bin_sample_counts = [samples_per_bin_base] * q_val\n                    for i in range(rem): bin_sample_counts[i] += 1\n                    \n                    sorted_indices = np.argsort(td_errors_abs)\n                    partitioned_indices = np.array_split(sorted_indices, q_val)\n                    \n                    batch_indices_list = []\n                    for i in range(q_val):\n                        bin_indices = partitioned_indices[i]\n                        if len(bin_indices) > 0:\n                            chosen = run_rng.choice(bin_indices, size=bin_sample_counts[i], replace=True)\n                            batch_indices_list.append(chosen)\n                    batch_indices = np.concatenate(batch_indices_list)\n\n            batch_phis = phis[batch_indices]\n            batch_rewards = rewards[batch_indices]\n            batch_q_tgt_next_0 = phis_next_0[batch_indices] @ theta_tgt\n            batch_q_tgt_next_1 = phis_next_1[batch_indices] @ theta_tgt\n            batch_max_q_tgt_next = np.maximum(batch_q_tgt_next_0, batch_q_tgt_next_1)\n            batch_q_values = batch_phis @ theta\n            batch_deltas = batch_rewards + GAMMA * batch_max_q_tgt_next - batch_q_values\n\n            grad_sum = (batch_deltas.reshape(-1, 1) * batch_phis).sum(axis=0)\n            theta += (ETA / b_val) * grad_sum\n\n            # Diagnostics\n            grad_contrib_norms = np.linalg.norm(batch_deltas.reshape(-1, 1) * batch_phis, axis=1)\n            k = int(np.ceil(0.1 * b_val))\n            sorted_norms = np.sort(grad_contrib_norms)[::-1]\n            sum_top_k = np.sum(sorted_norms[:k])\n            sum_all = np.sum(sorted_norms)\n            g_t = sum_top_k / (sum_all + EPS_METRIC)\n            g_values.append(g_t)\n            \n            loss_t = 0.5 * np.mean(batch_deltas**2)\n            loss_values.append(loss_t)\n\n            if t % K == 0:\n                theta_tgt = theta.copy()\n\n        burn_in_steps = T // 2\n        mean_g = np.mean(g_values[burn_in_steps:])\n        var_loss = np.var(loss_values[burn_in_steps:], ddof=1) if len(loss_values[burn_in_steps:]) > 1 else 0\n\n        return mean_g, var_loss\n\n    test_cases = [\n        {'p_out': 0.2, 's_out': 8.0, 'Q': 5, 'B': 64, 'id': 'A', 'seed': 123},\n        {'p_out': 0.0, 's_out': 8.0, 'Q': 5, 'B': 64, 'id': 'B', 'seed': 456},\n        {'p_out': 0.2, 's_out': 8.0, 'Q': 1, 'B': 64, 'id': 'C', 'seed': 789},\n    ]\n    \n    all_results = []\n    \n    for case_params in test_cases:\n        rng = np.random.default_rng(case_params['seed'])\n        dataset = generate_dataset(N, D, case_params['p_out'], case_params['s_out'], rng)\n\n        metrics = {}\n        for sampler in ['uniform', 'per', 'balanced']:\n            run_rng = np.random.default_rng(rng.integers(2**32 - 1))\n            params = {'B': case_params['B'], 'Q': case_params['Q']}\n            mean_g, var_loss = run_sampler_experiment(sampler, dataset, params, run_rng)\n            metrics[sampler] = {'g_dom': mean_g, 'loss_var': var_loss}\n\n        g_bal = metrics['balanced']['g_dom']\n        g_per = metrics['per']['g_dom']\n        g_uni = metrics['uniform']['g_dom']\n        v_bal = metrics['balanced']['loss_var']\n        v_per = metrics['per']['loss_var']\n        \n        b1 = bool(g_bal < g_per - TAU_G)\n        b2 = bool(v_bal < v_per - TAU_V)\n        \n        if case_params['Q'] == 1:\n            b3 = bool(abs(g_bal - g_uni) <= TAU_M)\n        else:\n            b3 = False\n            \n        all_results.append([b1, b2, b3])\n\n    result_str = ','.join(f'[{b1},{b2},{b3}]' for b1, b2, b3 in all_results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3163134"}]}