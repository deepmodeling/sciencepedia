{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it is crucial to understand why they work at all. This first practice explores the theoretical bedrock of value-based reinforcement learning: the Contraction Mapping Theorem. By analyzing a minimalist Markov Decision Process, you will empirically and analytically verify how the discount factor, $\\gamma$, ensures that the Bellman optimality operator is a contraction mapping, guaranteeing convergence to a unique optimal solution [@problem_id:3113609].", "problem": "Consider a finite Markov Decision Process (MDP) defined as follows: there is a single state $s_0$ and a single action $a_0$. The transition is deterministic with $P(s_0 \\mid s_0, a_0) = 1$, and the immediate reward is $R(s_0, a_0, s_0) = 1$. Let the discount factor be $\\gamma \\in [0,1]$. The action-value function $Q$ maps pairs $(s,a)$ to real numbers, and the Bellman optimality operator $\\mathcal{T}^*$ acts on action-value functions $Q$ by assigning to each $(s,a)$ the optimal one-step backup under $\\gamma$.\n\nStarting from the fundamental definition of the Bellman optimality operator on action-values for a deterministic MDP, the operator applied to $Q$ at $(s_0,a_0)$ obeys the defining rule that captures immediate reward plus discounted next-step optimal value. Under the supremum norm $\\|\\cdot\\|_{\\infty}$ on action-value functions, a mapping is a contraction if there exists $c \\in [0,1)$ such that for all $Q$ and $Q'$ the inequality $\\|\\mathcal{T}^* Q - \\mathcal{T}^* Q'\\|_{\\infty} \\le c \\|Q - Q'\\|_{\\infty}$ holds. The Contraction Mapping Theorem guarantees a unique fixed point and convergence of iterated application of a contraction mapping to that fixed point. In this problem, you will construct the explicit operator for the given MDP, reason about its Lipschitz constant with respect to $\\|\\cdot\\|_{\\infty}$, and empirically demonstrate the difference between fixed-point convergence for $0 < \\gamma < 1$ and divergence for $\\gamma = 1$ via iterated optimality backups on $Q$.\n\nYour task is to implement a program that performs the following steps for the specified MDP:\n\n1. Implement the Bellman optimality backup on $Q$ for $(s_0,a_0)$, and perform iterated backups $Q_{k+1} = \\mathcal{T}^* Q_k$ starting from $Q_0 = 0$ for exactly $K$ iterations, where $K = 500$.\n2. For each discount factor $\\gamma$ in the test suite $\\{0.5, 0.9, 0.999, 1.0\\}$, compute:\n   - The exact non-expansiveness constant $c$ of $\\mathcal{T}^*$ on this MDP under $\\|\\cdot\\|_{\\infty}$, defined by $c = \\sup_{Q \\neq Q'} \\frac{\\|\\mathcal{T}^* Q - \\mathcal{T}^* Q'\\|_{\\infty}}{\\|Q - Q'\\|_{\\infty}}$.\n   - The iterated value $Q_K(s_0,a_0)$ after $K$ backups from $Q_0 = 0$.\n   - For $\\gamma < 1$, the analytic fixed point $Q^*(s_0,a_0)$ that solves the fixed-point equation for the given MDP, and a boolean $\\mathrm{converged}$ indicating whether $|\\lvert Q_K(s_0,a_0) - Q^*(s_0,a_0) \\rvert| \\le \\varepsilon$ with $\\varepsilon = 10^{-6}$.\n   - A boolean $\\mathrm{divergent}$ indicating backup sequence divergence, defined as $\\mathrm{divergent} = (\\gamma = 1)$ or $\\frac{\\Delta_K}{\\Delta_{K-1}} \\ge \\rho_{\\mathrm{th}}$, where $\\Delta_k = Q_k(s_0,a_0) - Q_{k-1}(s_0,a_0)$ and $\\rho_{\\mathrm{th}} = 0.9999$.\n3. The final program output must be a single line consisting of a comma-separated list enclosed in square brackets, where each element corresponds to one $\\gamma$ in the test suite, in order. Each element itself must be a list of the form $[c, Q_K, \\mathrm{converged}, \\mathrm{divergent}]$ with the given definitions. For example, the output format must be like $[[c_1,Q_{K,1},\\mathrm{converged}_1,\\mathrm{divergent}_1],[c_2,Q_{K,2},\\mathrm{converged}_2,\\mathrm{divergent}_2],\\ldots]$ and must contain no spaces.\n\nThis test suite covers a fast-converging case ($\\gamma = 0.5$), a typical converging case ($\\gamma = 0.9$), a near-boundary case that converges slowly ($\\gamma = 0.999$), and a boundary failure case ($\\gamma = 1.0$) where contraction fails and iterated backups do not converge to a finite fixed point. No physical units or angle units are involved. All numerical answers must be represented as floats or booleans as specified. The program must be entirely self-contained and require no input.", "solution": "The user wants to analyze a simple Markov Decision Process (MDP) to understand the contraction property of the Bellman optimality operator and its implications for the convergence of value iteration.\n\n### Step 1: Formalizing the Bellman Optimality Operator\n\nThe problem defines an MDP with a single state $s_0$ and a single action $a_0$. The transition is deterministic, $P(s_0 \\mid s_0, a_0) = 1$, and the reward is constant, $R(s_0, a_0, s_0) = 1$. The discount factor is $\\gamma \\in [0,1]$.\n\nThe Bellman optimality operator, $\\mathcal{T}^*$, applied to an action-value function $Q$ is defined as:\n$$\n(\\mathcal{T}^*Q)(s,a) = \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma \\max_{a'} Q(s', a') \\right]\n$$\n\nFor our specific MDP, we evaluate this at the only state-action pair $(s_0, a_0)$:\n- The sum over next states $s'$ has only one term, $s' = s_0$, since the transition is deterministic.\n- The reward $R(s_0, a_0, s_0)$ is given as $1$.\n- The maximization over next actions $a'$ in state $s_0$ is trivial, as there is only one action, $a_0$. Thus, $\\max_{a'} Q(s_0, a') = Q(s_0, a_0)$.\n\nSubstituting these into the general equation gives:\n$$\n(\\mathcal{T}^*Q)(s_0, a_0) = 1 \\cdot \\left[ 1 + \\gamma Q(s_0, a_0) \\right]\n$$\n\nSince the action-value function $Q$ is only defined over a single point, we can represent its value by a scalar $q = Q(s_0, a_0)$. The operator $\\mathcal{T}^*$ acts on this scalar value as a simple affine transformation:\n$$\n\\mathcal{T}^*(q) = 1 + \\gamma q\n$$\n\n### Step 2: The Non-Expansiveness Constant $c$\n\nThe non-expansiveness (or Lipschitz) constant $c$ of an operator $\\mathcal{T}^*$ under the supremum norm $\\|\\cdot\\|_{\\infty}$ is defined as:\n$$\nc = \\sup_{Q \\neq Q'} \\frac{\\|\\mathcal{T}^* Q - \\mathcal{T}^* Q'\\|_{\\infty}}{\\|Q - Q'\\|_{\\infty}}\n$$\nA mapping is a contraction if $c < 1$.\n\nFor our simplified MDP, the supremum norm of the difference between two functions $Q$ and $Q'$ becomes the absolute difference of their scalar values, $q = Q(s_0, a_0)$ and $q' = Q'(s_0, a_0)$:\n$$\n\\|Q - Q'\\|_{\\infty} = |q - q'|\n$$\nThe norm of the difference after applying the operator is:\n$$\n\\|\\mathcal{T}^* Q - \\mathcal{T}^* Q'\\|_{\\infty} = |\\mathcal{T}^*(q) - \\mathcal{T}^*(q')| = |(1 + \\gamma q) - (1 + \\gamma q')| = |\\gamma(q - q')| = \\gamma |q - q'|\n$$\nThe ratio is therefore:\n$$\n\\frac{\\|\\mathcal{T}^* Q - \\mathcal{T}^* Q'\\|_{\\infty}}{\\|Q - Q'\\|_{\\infty}} = \\frac{\\gamma |q - q'|}{|q - q'|} = \\gamma\n$$\nThis ratio is constant for all $q \\neq q'$, so the supremum is simply $\\gamma$.\n$$\nc = \\gamma\n$$\nThe Bellman optimality operator for this MDP is a contraction if and only if $\\gamma < 1$. This is a key result from reinforcement learning theory, which states that the Bellman optimality operator is a $\\gamma$-contraction in general.\n\n### Step 3: Value Iteration and the Analytic Fixed Point\n\nValue iteration is the process of repeatedly applying the Bellman operator starting from an initial guess. The update rule is $Q_{k+1} = \\mathcal{T}^* Q_k$. For our scalar representation $q_k = Q_k(s_0, a_0)$, this is:\n$$\nq_{k+1} = 1 + \\gamma q_k\n$$\nStarting with $q_0 = 0$, the sequence unfolds as follows:\n- $q_1 = 1 + \\gamma q_0 = 1$\n- $q_2 = 1 + \\gamma q_1 = 1 + \\gamma$\n- $q_3 = 1 + \\gamma q_2 = 1 + \\gamma (1 + \\gamma) = 1 + \\gamma + \\gamma^2$\nIn general, $q_k$ is the sum of a finite geometric series:\n$$\nq_k = \\sum_{i=0}^{k-1} \\gamma^i\n$$\nThe Contraction Mapping Theorem guarantees that if $\\gamma < 1$, this sequence converges to a unique fixed point $Q^*$. The fixed point $q^*$ is found by solving $q^* = \\mathcal{T}^*(q^*)$:\n$$\nq^* = 1 + \\gamma q^* \\implies q^*(1 - \\gamma) = 1 \\implies q^* = \\frac{1}{1 - \\gamma}\n$$\nThis is valid for $\\gamma < 1$. For $\\gamma = 1$, the equation becomes $q^* = 1 + q^*$, which has no finite solution, and the sequence $q_k = \\sum_{i=0}^{k-1} 1^i = k$ diverges.\n\nThe problem requires checking for convergence after $K=500$ iterations using the condition $|\\,q_K - q^*\\,| \\le \\varepsilon = 10^{-6}$. The error term can be expressed analytically for $\\gamma < 1$:\n$$\n|\\,q_K - q^*\\,| = \\left| \\left(\\sum_{i=0}^{K-1} \\gamma^i\\right) - \\left(\\sum_{i=0}^{\\infty} \\gamma^i\\right) \\right| = \\left| -\\sum_{i=K}^{\\infty} \\gamma^i \\right| = \\frac{\\gamma^K}{1-\\gamma}\n$$\nThe program will evaluate this condition for each $\\gamma < 1$.\n\n### Step 4: The Divergence Condition\n\nThe `divergent` flag is defined as $(\\gamma = 1) \\lor (\\frac{\\Delta_K}{\\Delta_{K-1}} \\ge \\rho_{\\mathrm{th}})$, where $\\Delta_k = q_k - q_{k-1}$ and $\\rho_{\\mathrm{th}} = 0.9999$.\n\nLet's find the ratio $\\frac{\\Delta_k}{\\Delta_{k-1}}$.\n$$\n\\Delta_k = q_k - q_{k-1} = \\left(\\sum_{i=0}^{k-1} \\gamma^i\\right) - \\left(\\sum_{i=0}^{k-2} \\gamma^i\\right) = \\gamma^{k-1} \\quad (\\text{for } k \\ge 1)\n$$\nTherefore, the ratio is:\n$$\n\\frac{\\Delta_K}{\\Delta_{K-1}} = \\frac{\\gamma^{K-1}}{\\gamma^{K-2}} = \\gamma \\quad (\\text{for } \\gamma \\neq 0)\n$$\nFor the case $\\gamma=1$, the sequence is $q_k=k$, so $\\Delta_k = k-(k-1)=1$ for $k \\ge 1$. The ratio is $\\frac{1}{1}=1$.\nThe divergence condition thus simplifies to $(\\gamma = 1) \\lor (\\gamma \\ge 0.9999)$. For the test suite $\\{\\,0.5, 0.9, 0.999, 1.0\\,\\}$, this condition is only met for $\\gamma = 1.0$.\n\n### Step 5: Implementation Strategy\n\nThe program will iterate through the provided list of $\\gamma$ values. For each $\\gamma$:\n1.  The constant $c$ is set to $\\gamma$.\n2.  Value iteration is performed for $K=500$ steps starting from $q_0 = 0$ using the recurrence $q_{k+1} = 1 + \\gamma q_k$. All intermediate values are stored to calculate the deltas. The final value is $q_K$.\n3.  For $\\gamma < 1.0$, the analytic fixed point $q^* = 1/(1-\\gamma)$ is computed, and the convergence flag is determined by checking if $|q_K - q^*| \\le 10^{-6}$. For $\\gamma=1.0$, `converged` is set to `False`.\n4.  The divergence flag is computed using the rule $(\\gamma = 1.0) \\lor (\\Delta_K / \\Delta_{K-1} \\ge 0.9999)$. The deltas $\\Delta_K$ and $\\Delta_{K-1}$ are calculated from the stored history of $q_k$ values.\n5.  These four results ($c$, $q_K$, `converged`, `divergent`) are collected and formatted into the final output string as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Bellman operator properties for a simple MDP across different discount factors.\n    \"\"\"\n    # Define the test cases and constants from the problem statement.\n    test_cases = [0.5, 0.9, 0.999, 1.0]\n    K = 500\n    EPSILON = 1e-6\n    RHO_TH = 0.9999\n\n    results = []\n    \n    for gamma in test_cases:\n        # 1. Compute the non-expansiveness constant c\n        c = float(gamma)\n\n        # 2. Perform iterated backups for Q_k\n        q_k = 0.0\n        # Store history to calculate deltas later\n        # q_history[i] will store q_i\n        q_history = [0.0] * (K + 1)\n\n        for i in range(1, K + 1):\n            # Bellman backup: Q_{k+1} = R + gamma * Q_k\n            q_k = 1.0 + gamma * q_k\n            q_history[i] = q_k\n        \n        q_K = q_k\n\n        # 3. Calculate analytic fixed point and convergence (for gamma < 1)\n        # For gamma=1, there is no finite fixed point, so it does not converge.\n        if gamma < 1.0:\n            q_star = 1.0 / (1.0 - gamma)\n            converged = abs(q_K - q_star) <= EPSILON\n        else:\n            # The sequence diverges for gamma=1, so converged is False.\n            converged = False\n\n        # 4. Calculate divergence flag\n        delta_K = q_history[K] - q_history[K-1]\n        delta_K_minus_1 = q_history[K-1] - q_history[K-2]\n\n        # Avoid division by zero, although not possible for the given gammas > 0\n        if delta_K_minus_1 == 0:\n            # If gammas could be 0, this might happen. The ratio would be undefined.\n            # For this problem's test cases, this branch is not taken.\n            ratio = 0.0\n        else:\n            ratio = delta_K / delta_K_minus_1\n\n        is_gamma_one = (gamma == 1.0)\n        # Per the problem definition, the ratio condition is part of the check\n        # Note: As derived, ratio is simply gamma for gamma < 1.\n        # For gamma = 1, ratio is 1. The code calculates it numerically.\n        ratio_cond = (ratio >= RHO_TH)\n        divergent = is_gamma_one or ratio_cond\n\n        # Collect the results for this gamma\n        results.append([c, q_K, converged, divergent])\n\n    # Format the final output string to be exactly as specified, with no spaces.\n    results_str = []\n    for res in results:\n        c_val, qk_val, conv_val, div_val = res\n        # Convert Python booleans (True/False) to string representations ('True'/'False')\n        # and format floats. The f-string handles this automatically.\n        formatted_res = f\"[{c_val},{qk_val},{conv_val},{div_val}]\"\n        results_str.append(formatted_res)\n    \n    final_output = f\"[{','.join(results_str)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3113609"}, {"introduction": "With the principle of convergence established, we can now explore how different algorithms learn. This exercise presents a classic dilemma to highlight the crucial difference between on-policy learning (SARSA) and off-policy learning (Q-learning). By implementing both algorithms in an environment with a safe but suboptimal path and a risky but optimal one, you will directly observe how their distinct update rules lead to fundamentally different behaviors and learned values [@problem_id:3113683]. This is key to understanding the trade-offs an agent makes while exploring its world.", "problem": "Consider a finite Markov Decision Process (MDP) with a single nonterminal start state and two disjoint corridors to a goal, one corridor being risky due to an action that can terminate in a cliff with a large negative return, and one corridor being safe because all actions avoid catastrophic termination. The purpose is to compare the convergence dynamics of State-Action-Reward-State-Action (SARSA) and Q-learning in a stochastic setting with an exploratory behavior policy, highlighting on-policy versus off-policy differences via the action-value functions $Q^{\\pi}$ and $Q^{*}$.\n\nThe finite MDP is specified as follows. The set of states is $\\{s_{0}, r_{1}, r_{2}, u_{1}, u_{2}, u_{3}, g, c\\}$, where $s_{0}$ is the start state, $g$ is the goal terminal state, and $c$ is the cliff terminal state. The set of actions is the same for all states and equals $\\{0,1,2\\}$; for interpretability:\n- At $s_{0}$, action $0$ is \"go risky,\" action $1$ is \"go safe,\" and action $2$ is \"stall.\"\n- At corridor states $r_{1}, r_{2}, u_{1}, u_{2}, u_{3}$, action $0$ is \"forward,\" action $1$ is \"stall,\" and action $2$ is \"slip.\"\n- At terminal states $g$ and $c$, episodes terminate immediately.\n\nThe deterministic transitions and rewards are:\n- From $s_{0}$:\n  - Action $0$ transitions to $r_{1}$ with reward $-1$.\n  - Action $1$ transitions to $u_{1}$ with reward $-1$.\n  - Action $2$ transitions to $s_{0}$ with reward $-1$.\n- Risky corridor:\n  - From $r_{1}$: action $0$ to $r_{2}$ with reward $-1$, action $1$ to $r_{1}$ with reward $-1$, action $2$ to $c$ with reward $-100$.\n  - From $r_{2}$: action $0$ to $g$ with reward $0$, action $1$ to $r_{2}$ with reward $-1$, action $2$ to $c$ with reward $-100$.\n- Safe corridor:\n  - From $u_{1}$: action $0$ to $u_{2}$ with reward $-1$, action $1$ to $u_{1}$ with reward $-1$, action $2$ to $u_{1}$ with reward $-1$.\n  - From $u_{2}$: action $0$ to $u_{3}$ with reward $-1$, action $1$ to $u_{2}$ with reward $-1$, action $2$ to $u_{2}$ with reward $-1$.\n  - From $u_{3}$: action $0$ to $g$ with reward $0$, action $1$ to $u_{3}$ with reward $-1$, action $2$ to $u_{3}$ with reward $-1$.\n\nThe behavior policy used during learning is $\\epsilon$-greedy over the current action-value estimates $Q(s,a)$: with probability $1-\\epsilon$ select an action that maximizes $Q(s,a)$ and with probability $\\epsilon$ select an action uniformly at random from the action set. If multiple actions tie for the maximum $Q(s,a)$, choose the action with the smallest index.\n\nTraining requirements:\n- Implement tabular Q-learning and tabular State-Action-Reward-State-Action (SARSA) with constant learning rate $\\alpha$ and discount factor $\\gamma$; initialize all action-value estimates $Q(s,a)$ to $0$.\n- Episodes start at $s_{0}$ and terminate upon reaching $g$ or $c$, or after a fixed cap of $M$ steps to prevent infinite loops. Use $M = 100$.\n- Use a fixed pseudorandom seed for reproducible action selection during $\\epsilon$-greedy exploration.\n\nEvaluation:\n- After training, compute the differences\n  $$d_{\\mathrm{QL}} = Q_{\\mathrm{QL}}(s_{0}, 0) - Q_{\\mathrm{QL}}(s_{0}, 1), \\quad d_{\\mathrm{SARSA}} = Q_{\\mathrm{SARSA}}(s_{0}, 0) - Q_{\\mathrm{SARSA}}(s_{0}, 1),$$\n  where $Q_{\\mathrm{QL}}$ and $Q_{\\mathrm{SARSA}}$ are the learned action-value functions under Q-learning and SARSA respectively. A positive difference indicates a learned preference for the risky action at the start, while a negative difference indicates a learned preference for the safe action.\n\nFundamental base to be used in the derivation and implementation:\n- The definition of an action-value function for a policy $Q^{\\pi}(s,a)$ in a finite Markov Decision Process (MDP).\n- The Bellman expectation equation for $Q^{\\pi}(s,a)$ and the Bellman optimality equation for $Q^{*}(s,a)$.\n- The $\\epsilon$-greedy policy construction for exploration in reinforcement learning.\n\nTest suite:\n- Case $1$: $\\epsilon = 0.10$, $\\alpha = 0.50$, $\\gamma = 1.00$, episodes $= 20000$, seed $= 7$.\n- Case $2$ (boundary): $\\epsilon = 0.00$, $\\alpha = 0.50$, $\\gamma = 1.00$, episodes $= 10000$, seed $= 7$.\n- Case $3$ (heavy exploration): $\\epsilon = 0.40$, $\\alpha = 0.50$, $\\gamma = 1.00$, episodes $= 30000$, seed $= 7$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the pair of differences for each test case flattened into a single list:\n  $$[d_{\\mathrm{QL}}^{(1)}, d_{\\mathrm{SARSA}}^{(1)}, d_{\\mathrm{QL}}^{(2)}, d_{\\mathrm{SARSA}}^{(2)}, d_{\\mathrm{QL}}^{(3)}, d_{\\mathrm{SARSA}}^{(3)}],$$\n  where the superscript $(i)$ indexes test cases $1$ through $3$. Each entry must be a real number (float).", "solution": "The problem presented requires a comparative analysis of two fundamental temporal-difference (TD) control algorithms in reinforcement learning: Q-learning and State-Action-Reward-State-Action (SARSA). The analysis is performed within a specifically constructed finite Markov Decision Process (MDP) designed to highlight the crucial distinction between off-policy and on-policy learning.\n\nFirst, we formalize the foundational concepts. A finite MDP is a tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$, where $\\mathcal{S}$ is the finite set of states, $\\mathcal{A}$ is the finite set of actions, $P$ is the state transition probability function $P(s'|s,a) = \\Pr(S_{t+1}=s'|S_t=s, A_t=a)$, $R$ is the reward function $R(s,a,s') = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a, S_{t+1}=s']$, and $\\gamma \\in [0, 1]$ is the discount factor. In this problem, the transitions are deterministic, so $P(s'|s,a) = 1$ for a specific $s'$ and $0$ otherwise.\n\nThe objective of an agent is to learn a policy, $\\pi(a|s) = \\Pr(A_t=a|S_t=s)$, that maximizes the expected discounted cumulative reward. The value of a state-action pair $(s,a)$ under a policy $\\pi$ is given by the action-value function $Q^{\\pi}(s,a)$:\n$$Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t=s, A_t=a \\right]$$\nThis function satisfies the Bellman expectation equation:\n$$Q^{\\pi}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^{\\pi}(s',a') \\right]$$\nThe optimal action-value function, $Q^{*}(s,a) = \\max_{\\pi} Q^{\\pi}(s,a)$, provides the maximum expected return achievable from any state-action pair. It satisfies the Bellman optimality equation:\n$$Q^{*}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\max_{a'} Q^{*}(s',a') \\right]$$\n\nQ-learning and SARSA are TD methods that learn these action-value functions iteratively without a model of the environment. Their update rules reveal their fundamental difference.\n\nQ-learning is an off-policy algorithm. Its update rule for an action value $Q(S_t, A_t)$ following a transition from state $S_t$ to $S_{t+1}$ with reward $R_{t+1}$ is:\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right]$$\nThe term $\\max_{a'} Q(S_{t+1}, a')$ represents the estimated value of the optimal action from the next state, $S_{t+1}$. Q-learning uses this greedy choice to update its value function, irrespective of which action is actually taken next by the behavior policy. Consequently, it directly learns an estimate of the optimal action-value function, $Q^*$.\n\nSARSA is an on-policy algorithm. Its update rule uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$:\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\nHere, the update target includes $Q(S_{t+1}, A_{t+1})$, the value of the action $A_{t+1}$ that was actually selected in state $S_{t+1}$ according to the behavior policy. Thus, SARSA learns the action-value function for the policy it is currently following, $Q^{\\pi}$. In this problem, the policy $\\pi$ is $\\epsilon$-greedy with respect to the current $Q$-values.\n\nNow, we analyze the specific MDP. An agent at the start state $s_0$ can choose a \"risky\" corridor (action $0$) or a \"safe\" corridor (action $1$). The discount factor is $\\gamma=1$, meaning we seek to maximize the undiscounted sum of rewards.\nThe optimal path to the goal is via the risky corridor: $s_0 \\to r_1 \\to r_2 \\to g$. This path takes $3$ steps, with a total reward of $(-1) + (-1) + 0 = -2$.\nThe safe path is longer: $s_0 \\to u_1 \\to u_2 \\to u_3 \\to g$. This path takes $4$ steps, with a total reward of $(-1) + (-1) + (-1) + 0 = -3$.\nTherefore, the optimal action-value for starting at $s_0$ is $Q^*(s_0, 0) = -2$ and $Q^*(s_0, 1) = -3$.\n\nWhen $\\epsilon > 0$, the behavior policy is exploratory.\nQ-learning, being off-policy, aims to find $Q^*$. It learns that the risky path is superior ($Q(s_0, 0) \\approx -2$ and $Q(s_0, 1) \\approx -3$), despite the fact that its own exploratory actions might occasionally lead it off the cliff. Its value estimates are based on the assumption that it will act greedily in the future. We therefore predict that $Q_{\\mathrm{QL}}(s_0, 0) > Q_{\\mathrm{QL}}(s_0, 1)$, resulting in a positive difference $d_{\\mathrm{QL}} > 0$.\n\nSARSA, being on-policy, learns the value of the $\\epsilon$-greedy policy itself. This policy has a non-zero probability ($\\epsilon/3$) of selecting the \"slip\" action (action $2$) in states $r_1$ and $r_2$, which leads to the cliff and a large negative reward of $-100$. This possibility is factored into SARSA's value estimates. The expected return for taking the risky path is thus a weighted average of the optimal outcome and the catastrophic \"slip\" outcome. For a sufficiently large penalty, this expected return will be lower than the expected return of the safe path, where exploration (\"slip\" or \"stall\") is not catastrophic. SARSA will learn that, under its own exploratory policy, the safe corridor is preferable. We predict $Q_{\\mathrm{SARSA}}(s_0, 0) < Q_{\\mathrm{SARSA}}(s_0, 1)$, resulting in a negative difference $d_{\\mathrm{SARSA}} < 0$. The magnitude of this effect should increase with $\\epsilon$.\n\nFor the case where $\\epsilon=0.00$, the policy is purely greedy. The action $A_{t+1}$ chosen in SARSA will be $\\arg\\max_{a'} Q(S_{t+1}, a')$. In this specific scenario, the SARSA update rule becomes functionally identical to the Q-learning update rule, assuming a unique maximum. Both algorithms will follow the same greedy trajectory and update their values identically. They will both converge towards $Q^*$ and learn that the risky path is optimal. We thus expect $d_{\\mathrm{QL}}^{(2)} > 0$ and $d_{\\mathrm{SARSA}}^{(2)} > 0$, with their values being very close. This demonstrates SARSA's \"conservative\" nature is a direct result of on-policy evaluation of an exploratory policy.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Q-learning and SARSA on a specified MDP\n    to demonstrate the difference between off-policy and on-policy learning.\n    \"\"\"\n    \n    #\n    # Environment Setup\n    #\n    states = ['s0', 'r1', 'r2', 'u1', 'u2', 'u3', 'g', 'c']\n    state_map = {name: i for i, name in enumerate(states)}\n    num_states = len(states)\n    num_actions = 3\n    \n    s0, r1, r2, u1, u2, u3, g, c = (state_map[s] for s in states)\n    terminal_states = {g, c}\n    \n    # Transitions: T[state_idx][action_idx] = (next_state_idx, reward)\n    T = {\n        s0: {0: (r1, -1.0), 1: (u1, -1.0), 2: (s0, -1.0)},\n        r1: {0: (r2, -1.0), 1: (r1, -1.0), 2: (c, -100.0)},\n        r2: {0: (g, 0.0),  1: (r2, -1.0), 2: (c, -100.0)},\n        u1: {0: (u2, -1.0), 1: (u1, -1.0), 2: (u1, -1.0)},\n        u2: {0: (u3, -1.0), 1: (u2, -1.0), 2: (u2, -1.0)},\n        u3: {0: (g, 0.0),  1: (u3, -1.0), 2: (u3, -1.0)},\n    }\n    \n    #\n    # Agent and Policy Logic\n    #\n    def select_action(q_s, epsilon, rng):\n        \"\"\"Selects an action using an epsilon-greedy policy.\"\"\"\n        if rng.random() < epsilon:\n            return rng.integers(num_actions)\n        else:\n            # np.argmax handles tie-breaking by returning the first index\n            # with the maximum value, which matches the problem spec.\n            return np.argmax(q_s)\n\n    #\n    # Training Function\n    #\n    def run_experiment(algorithm, params, rng):\n        \"\"\"Runs a full training experiment for a given algorithm.\"\"\"\n        epsilon, alpha, gamma, total_episodes, max_steps = params\n        \n        Q = np.zeros((num_states, num_actions))\n\n        for _ in range(total_episodes):\n            state = s0\n            \n            # For SARSA, the first action must be selected before the loop begins.\n            if algorithm == 'sarsa':\n                action = select_action(Q[state], epsilon, rng)\n\n            for _ in range(max_steps):\n                if state in terminal_states:\n                    break\n                \n                # For Q-learning, action is selected inside the main loop.\n                if algorithm == 'q_learning':\n                    action = select_action(Q[state], epsilon, rng)\n\n                # Get next state and reward from the environment model.\n                next_state, reward = T[state][action]\n                \n                if algorithm == 'q_learning':\n                    # Off-policy update: uses max Q-value at next state\n                    if next_state in terminal_states:\n                        target = reward\n                    else:\n                        target = reward + gamma * np.max(Q[next_state])\n                    Q[state, action] += alpha * (target - Q[state, action])\n                \n                elif algorithm == 'sarsa':\n                    # On-policy update: uses Q-value of the actual next action\n                    if next_state in terminal_states:\n                        # No next action, target is just the reward\n                        target = reward\n                    else:\n                        next_action = select_action(Q[next_state], epsilon, rng)\n                        target = reward + gamma * Q[next_state, next_action]\n                    Q[state, action] += alpha * (target - Q[state, action])\n                    # Update for next iteration\n                    action = next_action if next_state not in terminal_states else 0\n\n                state = next_state\n        return Q\n\n    #\n    # Main Execution Logic\n    #\n    test_cases = [\n        # (epsilon, alpha, gamma, episodes, seed)\n        (0.10, 0.50, 1.00, 20000, 7),\n        (0.00, 0.50, 1.00, 10000, 7),\n        (0.40, 0.50, 1.00, 30000, 7),\n    ]\n    max_steps_per_episode = 100\n    all_results = []\n    \n    for epsilon, alpha, gamma, episodes, seed in test_cases:\n        params = (epsilon, alpha, gamma, episodes, max_steps_per_episode)\n        \n        # Run Q-learning for the current case\n        rng_ql = np.random.default_rng(seed)\n        Q_ql = run_experiment('q_learning', params, rng_ql)\n        d_ql = Q_ql[s0, 0] - Q_ql[s0, 1]\n        \n        # Run SARSA for the current case, resetting the seed for fair comparison\n        rng_sarsa = np.random.default_rng(seed)\n        Q_sarsa = run_experiment('sarsa', params, rng_sarsa)\n        d_sarsa = Q_sarsa[s0, 0] - Q_sarsa[s0, 1]\n        \n        all_results.extend([d_ql, d_sarsa])\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3113683"}, {"introduction": "As we transition from tabular methods to Deep Reinforcement Learning, the combination of neural network function approximators and bootstrapping can introduce complex instabilities. This advanced practice provides a window into these dynamics by modeling a Deep Q-Network's (DQN) behavior as a linear system. You will derive the conditions under which learning becomes oscillatory as a function of the target network update frequency and verify your theoretical predictions through simulation, gaining a deeper appreciation for the techniques used to stabilize modern DRL agents [@problem_id:3113592].", "problem": "You are given a simplified linear model of the learning dynamics of a Deep Q-Network (DQN), which is a Deep Reinforcement Learning (DRL) algorithm. The goal is to analytically approximate when oscillatory behavior emerges as a function of the target network update period and to verify this approximation by measuring the oscillation spectra of the learned action-value function using the Fast Fourier Transform (FFT). The approximation must start from first principles of Reinforcement Learning and linear system theory, without relying on shortcut formulas.\n\nConsider a deterministic two-state Markov Decision Process (MDP) with two scalar action-value function components $q_1$ and $q_2$ that represent the online network estimates for the two states. Let $h_1$ and $h_2$ denote the corresponding target network estimates. The environment is deterministic and transitions from state $1$ to state $2$ and from state $2$ to state $1$. Rewards are constant and given by $r_1$ and $r_2$. The discount factor is $\\gamma \\in [0,1)$ and the learning rate is $\\alpha \\in (0,1)$.\n\nThe canonical Bellman optimality relations for this deterministic setting are\n$$\nQ^\\star_1 = r_1 + \\gamma Q^\\star_2, \\quad Q^\\star_2 = r_2 + \\gamma Q^\\star_1,\n$$\nwhere $Q^\\star_i$ denotes the optimal action-value for state $i$. A Deep Q-Network (DQN) approximates these relations via temporal-difference updates. With a fixed target network during a target-hold interval, the per-step online updates are\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2(t) - q_1(t) \\right),\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1(t) - q_2(t) \\right),\n$$\nand the target network is updated by a hard copy every $T$ steps:\n$$\nh_i(t+1) = \n\\begin{cases}\nq_i(t+1), & \\text{if } (t+1) \\bmod T = 0, \\\\\nh_i(t), & \\text{otherwise},\n\\end{cases}\n\\quad i \\in \\{1,2\\}.\n$$\n\nWithin any target-hold interval of length $T$, treat $h_1$ and $h_2$ as constants equal to the last copied values $s_1$ and $s_2$. Derive the across-cycle linear affine map that advances the pair $(s_1, s_2)$ to the next cycle's pair $(s'_1, s'_2)$ after $T$ online updates and one target copy. Then, obtain the eigenvalues of the resulting linear part to determine the presence of an oscillatory mode. Use the presence of a negative eigenvalue to define a predicted per-step oscillation frequency, and validate this prediction by computing the FFT of the time series of the difference mode $d(t) = q_1(t) - q_2(t)$.\n\nYour program must:\n- Start from the above per-step DQN dynamics and derive the cycle-to-cycle linear map for $(s_1, s_2)$, express its matrix in terms of $\\alpha$, $\\gamma$, and $T$, and compute its eigenvalues.\n- Use the eigenstructure to predict the emergence of an oscillatory mode. Specifically, if the smaller eigenvalue is negative, define the predicted angular frequency per step as $\\omega_{\\text{pred}} = \\pi / T$; otherwise, set $\\omega_{\\text{pred}} = 0$.\n- Simulate the per-step dynamics for $N$ steps to generate $d(t)$ and compute its FFT magnitude spectrum, excluding the zero-frequency bin, to obtain the measured peak angular frequency $\\omega_{\\text{meas}}$ in radians per step.\n- Report, for each test case, the tuple $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$, where $\\text{resonant}$ is a boolean that is $\\text{True}$ if the smaller eigenvalue is negative and $\\text{False}$ otherwise.\n\nUse the following fixed parameter values for all test cases:\n- Discount factor $\\gamma = 0.9$,\n- Learning rate $\\alpha = 0.2$,\n- Rewards $r_1 = 0$ and $r_2 = 0$,\n- Initial online values $q_1(0) = 1$ and $q_2(0) = -1$,\n- Initial target values $h_1(0) = q_1(0)$ and $h_2(0) = q_2(0)$,\n- Simulation length $N = 4096$ steps.\n\nAngle unit specification: All angular frequencies must be expressed in radians per step.\n\nTest suite:\n- Sweep the target update period $T$ across the values $T \\in \\{1, 3, 4, 20, 50\\}$ to cover frequent updates, near-threshold behavior, and long target-hold intervals.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output a nested list $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$, with both $\\omega_{\\text{pred}}$ and $\\omega_{\\text{meas}}$ rounded to six decimal places, and $\\text{resonant}$ as a boolean.\n- Example format: $[[0.785398,0.780000,True],[0.000000,0.020000,False],\\dots]$.", "solution": "The problem requires an analysis of the oscillatory dynamics in a simplified Deep Q-Network (DQN) model. This will be accomplished in two parts: first, a theoretical derivation of the condition for oscillation based on a linear systems analysis of the learning updates, and second, a numerical simulation to measure the oscillation frequency and validate the theoretical prediction.\n\n### Part 1: Theoretical Analysis\n\nThe learning dynamics are described by a set of coupled first-order difference equations for the online action-value estimates $q_1(t)$ and $q_2(t)$. The target network values, $h_1(t)$ and $h_2(t)$, are held constant for a period of $T$ steps. Let us analyze the system's evolution over one such cycle.\n\nAt the beginning of a cycle, the target network is updated with the current values of the online network. Let these values be $s_1$ and $s_2$. Thus, for the duration of the cycle (from step $t=0$ to $t=T-1$ within the cycle), we have $h_1 = s_1$ and $h_2 = s_2$. The per-step updates for the online network are given by:\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2 - q_1(t) \\right) = (1-\\alpha) q_1(t) + \\alpha (r_1 + \\gamma s_2)\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1 - q_2(t) \\right) = (1-\\alpha) q_2(t) + \\alpha (r_2 + \\gamma s_1)\n$$\nThe problem specifies that the initial values for the online network at the start of a cycle are the same as the target values for that cycle, i.e., $q_1(0) = s_1$ and $q_2(0) = s_2$.\n\nThese are linear first-order non-homogeneous difference equations. The general solution for an equation of the form $x(t+1) = a x(t) + b$ is $x(t) = a^t x(0) + b \\sum_{k=0}^{t-1} a^k = a^t x(0) + b \\frac{1-a^t}{1-a}$.\nApplying this to our system, with $a = (1-\\alpha)$, we get:\n$$\nq_1(t) = (1-\\alpha)^t q_1(0) + \\alpha(r_1 + \\gamma s_2) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_1 + (1-(1-\\alpha)^t)(r_1 + \\gamma s_2)\n$$\n$$\nq_2(t) = (1-\\alpha)^t q_2(0) + \\alpha(r_2 + \\gamma s_1) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_2 + (1-(1-\\alpha)^t)(r_2 + \\gamma s_1)\n$$\nAfter $T$ steps, the online network values are used to update the target network for the next cycle. Let the new target values be $s'_1$ and $s'_2$.\n$$\ns'_1 = q_1(T) = (1-\\alpha)^T s_1 + (1-(1-\\alpha)^T)(r_1 + \\gamma s_2)\n$$\n$$\ns'_2 = q_2(T) = (1-\\alpha)^T s_2 + (1-(1-\\alpha)^T)(r_2 + \\gamma s_1)\n$$\nThe problem specifies constant rewards $r_1=0$ and $r_2=0$. The map simplifies to a linear transformation. Let $\\beta = (1-\\alpha)^T$. The equations become:\n$$\ns'_1 = \\beta s_1 + (1-\\beta)\\gamma s_2\n$$\n$$\ns'_2 = \\beta s_2 + (1-\\beta)\\gamma s_1\n$$\nIn matrix form, this is $\\mathbf{s'} = A \\mathbf{s}$, where $\\mathbf{s} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$ and the cycle-to-cycle transition matrix $A$ is:\n$$\nA = \\begin{pmatrix} \\beta & (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma & \\beta \\end{pmatrix}\n$$\n\nTo determine the stability and oscillatory nature of this map, we compute the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} \\beta-\\lambda & (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma & \\beta-\\lambda \\end{pmatrix} = (\\beta-\\lambda)^2 - ((1-\\beta)\\gamma)^2 = 0\n$$\nThis yields two solutions for the eigenvalues $\\lambda$:\n$$\n\\beta - \\lambda = \\pm (1-\\beta)\\gamma \\implies \\lambda = \\beta \\mp (1-\\beta)\\gamma\n$$\nThe two eigenvalues are:\n$$\n\\lambda_1 = \\beta + (1-\\beta)\\gamma = (1-\\alpha)^T + \\gamma(1-(1-\\alpha)^T)\n$$\n$$\n\\lambda_2 = \\beta - (1-\\beta)\\gamma = (1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T)\n$$\nAn oscillatory mode emerges when the system's trajectory flips its orientation in some direction in the state space from one cycle to the next. In a linear system, this corresponds to an eigenvalue being negative. Since $0 < \\alpha < 1$ and $0 \\le \\gamma < 1$, we have $0 < \\beta < 1$, which implies $(1-\\beta)\\gamma \\ge 0$. Therefore, $\\lambda_2$ is always the smaller eigenvalue. The condition for the emergence of an oscillatory mode (which we term `resonant`) is $\\lambda_2 < 0$.\n$$\n(1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T) < 0\n$$\n$$\n(1-\\alpha)^T < \\gamma - \\gamma(1-\\alpha)^T\n$$\n$$\n(1+\\gamma)(1-\\alpha)^T < \\gamma\n$$\n$$\n(1-\\alpha)^T < \\frac{\\gamma}{1+\\gamma}\n$$\nIf this condition holds, the system is in a `resonant` regime. The corresponding eigenvector to $\\lambda_2$ is proportional to $(1, -1)^T$, which represents the difference mode $d(t) = q_1(t) - q_2(t)$. A negative eigenvalue means this mode flips sign every cycle (every $T$ steps). This corresponds to an oscillation with a period of $2T$ steps. The fundamental angular frequency of such an oscillation is $\\omega = 2\\pi / (2T) = \\pi/T$ radians per step. Thus, the predicted frequency is:\n$$\n\\omega_{\\text{pred}} = \\begin{cases} \\pi/T, & \\text{if } (1-\\alpha)^T < \\frac{\\gamma}{1+\\gamma} \\\\ 0, & \\text{otherwise} \\end{cases}\n$$\n\n### Part 2: Numerical Simulation and Measurement\n\nWe will now simulate the per-step dynamics for $N=4096$ steps to generate the time series for the difference mode $d(t) = q_1(t) - q_2(t)$. The simulation directly implements the given update rules for $q_i(t)$ and $h_i(t+1) = q_i(t+1)$ if $(t+1) \\bmod T = 0$, or $h_i(t+1) = h_i(t)$ otherwise.\n\nAfter simulating and storing the history of $q_1(t)$ and $q_2(t)$, we construct the time series $d(t)$ for $t = 0, \\dots, N-1$. We then compute the Fast Fourier Transform (FFT) of this series to find its frequency spectrum.\nThe measured angular frequency, $\\omega_{\\text{meas}}$, is determined by finding the frequency component with the largest magnitude in the spectrum, excluding the zero-frequency (DC) component. The frequencies provided by the FFT algorithm, $f$, are in cycles per step. They are converted to angular frequency in radians per step using the relation $\\omega = 2\\pi f$.\n\nFor each given value of the target update period $T$, we will compute the triplet $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$ and report the results. The fixed parameters are $\\gamma = 0.9$, $\\alpha = 0.2$, $r_1=0$, $r_2=0$, and initial conditions $q_1(0)=1, q_2(0)=-1, h_1(0)=1, h_2(0)=-1$.\nThe critical threshold for oscillation is $(1-0.2)^T < \\frac{0.9}{1+0.9}$, which simplifies to $0.8^T < 0.9/1.9 \\approx 0.47368$.\n\nFor $T=1, 3$: $0.8^1 = 0.8 \\not< 0.47368$, $0.8^3 = 0.512 \\not< 0.47368$. The system is not `resonant`.\nFor $T=4, 20, 50$: $0.8^4 = 0.4096 < 0.47368$, $0.8^{20} \\approx 0.0115 < 0.47368$, $0.8^{50} \\approx 1.4 \\times 10^{-5} < 0.47368$. The system is `resonant`.\nThese theoretical predictions will be verified by the simulation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and simulates the dynamics of a simplified DQN to find\n    and verify oscillatory behavior based on the target network update period.\n    \"\"\"\n    \n    # Define the fixed parameters from the problem statement.\n    GAMMA = 0.9\n    ALPHA = 0.2\n    R1 = 0.0\n    R2 = 0.0\n    Q1_0 = 1.0\n    Q2_0 = -1.0\n    N_STEPS = 4096\n    \n    # Define the test cases (sweep over target update period T).\n    test_cases = [1, 3, 4, 20, 50]\n\n    results = []\n\n    for T in test_cases:\n        # Part 1: Theoretical Prediction\n        \n        # The condition for oscillatory resonance is (1-alpha)^T < gamma / (1+gamma)\n        resonant_threshold = GAMMA / (1 + GAMMA)\n        is_resonant = (1 - ALPHA)**T < resonant_threshold\n        \n        if is_resonant:\n            omega_pred = np.pi / T\n        else:\n            omega_pred = 0.0\n\n        # Part 2: Simulation\n        \n        # Initialize arrays to store the time series of q and h values.\n        # q_hist stores q(0), q(1), ..., q(N)\n        q_hist = np.zeros((N_STEPS + 1, 2))\n        h_hist = np.zeros((N_STEPS + 1, 2))\n        \n        # Set initial conditions at t=0\n        q_hist[0] = [Q1_0, Q2_0]\n        h_hist[0] = [Q1_0, Q2_0]\n        \n        for t in range(N_STEPS):\n            # Current online and target values\n            q1_t, q2_t = q_hist[t]\n            h1_t, h2_t = h_hist[t]\n            \n            # Calculate next online values q(t+1)\n            q1_tp1 = q1_t + ALPHA * (R1 + GAMMA * h2_t - q1_t)\n            q2_tp1 = q2_t + ALPHA * (R2 + GAMMA * h1_t - q2_t)\n            q_hist[t+1] = [q1_tp1, q2_tp1]\n            \n            # Update target network for next step h(t+1)\n            if (t + 1) % T == 0:\n                h_hist[t+1] = [q1_tp1, q2_tp1]\n            else:\n                h_hist[t+1] = h_hist[t]\n\n        # Part 3: Measurement via FFT\n        \n        # Create the difference mode time series d(t) = q1(t) - q2(t) for t=0...N-1\n        d_series = q_hist[:N_STEPS, 0] - q_hist[:N_STEPS, 1]\n        \n        # Compute the FFT magnitude spectrum\n        fft_magnitudes = np.abs(np.fft.fft(d_series))\n        \n        # Get the corresponding frequencies in cycles per step\n        fft_frequencies = np.fft.fftfreq(N_STEPS)\n        \n        # Find the peak frequency, excluding the DC component (at index 0)\n        # We only need to check the positive frequencies (first half of the array)\n        positive_freq_range = slice(1, N_STEPS // 2)\n        \n        if np.all(fft_magnitudes[positive_freq_range] == 0):\n             peak_idx = 0 # No signal\n        else:\n            # Add 1 to peak_idx because we searched in a slice starting at index 1\n            peak_idx = np.argmax(fft_magnitudes[positive_freq_range]) + 1\n        \n        # Peak frequency in cycles per step\n        peak_freq_cycles = fft_frequencies[peak_idx]\n        \n        # Convert to angular frequency in radians per step\n        omega_meas = 2 * np.pi * peak_freq_cycles\n\n        # Store the results for this test case\n        results.append([\n            round(omega_pred, 6), \n            round(omega_meas, 6), \n            is_resonant\n        ])\n\n    # Final print statement in the exact required format.\n    # The format requires a boolean literal 'True' or 'False'\n    result_str = ','.join([\n        f\"[{res[0]},{res[1]},{str(res[2])}]\" for res in results\n    ])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3113592"}]}