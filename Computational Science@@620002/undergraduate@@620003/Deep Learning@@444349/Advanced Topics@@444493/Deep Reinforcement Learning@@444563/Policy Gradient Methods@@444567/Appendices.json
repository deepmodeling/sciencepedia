{"hands_on_practices": [{"introduction": "The basic REINFORCE algorithm often suffers from high variance, making learning slow and unstable. This exercise explores a powerful variance reduction technique called antithetic sampling, which leverages symmetry in the action sampling process to create more reliable gradient estimates. By working through this problem [@problem_id:3158001], you will quantify the performance gain and build an intuition for why such methods are crucial in practice.", "problem": "Consider a one-dimensional stochastic bandit with action $a \\in \\mathbb{R}$ drawn from a symmetric Gaussian policy $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$, where the variance $\\sigma^{2} > 0$ is fixed and only the mean $\\theta \\in \\mathbb{R}$ is the trainable parameter. The reward is linear in the action, $r(a) = k a$, where $k \\in \\mathbb{R}$ is a nonzero constant. The objective is $J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$. Use the likelihood-ratio (score-function) identity $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$ as the fundamental starting point.\n\nDefine the following unbiased gradient estimators for $\\nabla_{\\theta} J(\\theta)$:\n- A two-sample independent-and-identically-distributed averaged estimator: draw $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$ and set $\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$.\n- An antithetic-pair averaged estimator: draw $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, set $a_{+} = \\theta + \\varepsilon$ and $a_{-} = \\theta - \\varepsilon$, and define $\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$.\n\nStarting from the definition of $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$ for a univariate Gaussian with fixed variance and basic moment identities for a Gaussian random variable, derive closed-form expressions for $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$ and $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$. Then, quantify the variance reduction achieved by antithetic sampling under an equal reward-evaluation budget by providing the closed-form analytic expression for the variance-reduction factor\n$$V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}.$$\n\nExpress your final result as a single simplified expression in terms of $\\theta$ and $\\sigma$. No numerical rounding is required.", "solution": "First, we establish some preliminary quantities. The probability density function for the policy $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^2)$ is\n$$p(a; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right).$$\nThe log-policy is $\\ln \\pi_{\\theta}(a) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(a-\\theta)^2}{2\\sigma^2}$. The score function, which is the gradient of the log-policy with respect to the parameter $\\theta$, is\n$$ \\nabla_{\\theta} \\ln \\pi_{\\theta}(a) = \\frac{\\partial}{\\partial\\theta}\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{2(a-\\theta)(-1)}{2\\sigma^2} = \\frac{a-\\theta}{\\sigma^2}. $$\nLet's define a single-sample gradient estimate as $\\hat{g}(a) = r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$. Using the given reward and the derived score function, we have\n$$ \\hat{g}(a) = (ka) \\left(\\frac{a-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2} a(a-\\theta). $$\nThe true gradient $\\nabla_{\\theta} J(\\theta)$ is the expectation of this estimator. Let $a \\sim \\mathcal{N}(\\theta, \\sigma^2)$. We can write $a = \\theta + z$, where $z \\sim \\mathcal{N}(0, \\sigma^2)$.\n$$ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}[\\hat{g}(a)] = \\mathbb{E}\\left[\\frac{k}{\\sigma^2} a(a-\\theta)\\right] = \\frac{k}{\\sigma^2}\\mathbb{E}[(\\theta+z)(z)] = \\frac{k}{\\sigma^2}\\mathbb{E}[\\theta z + z^2]. $$\nUsing the linearity of expectation and noting that for $z \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\mathbb{E}[z]=0$ and $\\mathbb{E}[z^2]=\\sigma^2$, we get\n$$ \\nabla_{\\theta} J(\\theta) = \\frac{k}{\\sigma^2}(\\theta \\mathbb{E}[z] + \\mathbb{E}[z^2]) = \\frac{k}{\\sigma^2}(0 + \\sigma^2) = k. $$\nThis confirms that $\\hat{g}(a)$ is an unbiased estimator of the true gradient $k$.\n\nNow, we derive the variance of the two estimators.\n\n**1. Variance of the I.I.D. Estimator, $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$**\n\nThe estimator $\\hat{g}_{\\text{iid}}$ is the average of two i.i.d. random variables, $\\hat{g}(a_1)$ and $\\hat{g}(a_2)$. Due to independence, the variance is\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\operatorname{Var}\\left[\\frac{1}{2}(\\hat{g}(a_1) + \\hat{g}(a_2))\\right] = \\frac{1}{4}(\\operatorname{Var}[\\hat{g}(a_1)] + \\operatorname{Var}[\\hat{g}(a_2)]) = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)]. $$\nWe need to compute $\\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2$. We already know $\\mathbb{E}[\\hat{g}(a)] = k$.\nLet's compute the second moment $\\mathbb{E}[\\hat{g}(a)^2]$:\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\mathbb{E}\\left[\\left(\\frac{k}{\\sigma^2} a(a-\\theta)\\right)^2\\right] = \\frac{k^2}{\\sigma^4} \\mathbb{E}[a^2(a-\\theta)^2]. $$\nSubstitute $a = \\theta + z$ where $z \\sim \\mathcal{N}(0, \\sigma^2)$:\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\mathbb{E}[(\\theta+z)^2 z^2] = \\mathbb{E}[(\\theta^2+2\\theta z+z^2)z^2] = \\mathbb{E}[\\theta^2 z^2 + 2\\theta z^3 + z^4]. $$\nUsing the moments of a centered Gaussian, $\\mathbb{E}[z^2]=\\sigma^2$, $\\mathbb{E}[z^3]=0$ (by symmetry), and $\\mathbb{E}[z^4]=3\\sigma^4$:\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\theta^2\\mathbb{E}[z^2] + 2\\theta\\mathbb{E}[z^3] + \\mathbb{E}[z^4] = \\theta^2\\sigma^2 + 0 + 3\\sigma^4 = \\sigma^2(\\theta^2 + 3\\sigma^2). $$\nSo, the second moment of $\\hat{g}(a)$ is\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\frac{k^2}{\\sigma^4}\\sigma^2(\\theta^2 + 3\\sigma^2) = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right). $$\nThen, the variance of a single-sample estimator is\n$$ \\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right) - k^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\nFinally, the variance of the two-sample i.i.d. estimator is\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)] = \\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n\n**2. Variance of the Antithetic Estimator, $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$**\n\nThe antithetic estimator uses a correlated pair of samples. Let's first express the estimator in terms of the random variable $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\nThe two components of the estimator are $\\hat{g}(a_{+})$ and $\\hat{g}(a_{-})$.\nFor $a_{+} = \\theta + \\varepsilon$:\n$$ \\hat{g}(a_{+}) = r(a_{+}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{+}) = k(\\theta+\\varepsilon) \\left(\\frac{(\\theta+\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta+\\varepsilon)\\varepsilon = \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2). $$\nFor $a_{-} = \\theta - \\varepsilon$:\n$$ \\hat{g}(a_{-}) = r(a_{-}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{-}) = k(\\theta-\\varepsilon) \\left(\\frac{(\\theta-\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta-\\varepsilon)(-\\varepsilon) = \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2). $$\nThe antithetic estimator is the average of these two components:\n$$ \\hat{g}_{\\text{anti}} = \\frac{1}{2}(\\hat{g}(a_{+}) + \\hat{g}(a_{-})) = \\frac{1}{2} \\left[ \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2) + \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2) \\right] = \\frac{1}{2} \\frac{k}{\\sigma^2} (2\\varepsilon^2) = \\frac{k}{\\sigma^2}\\varepsilon^2. $$\nThe term dependent on $\\theta$ has been cancelled out. Now we can compute the variance of this simplified expression.\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\operatorname{Var}\\left[\\frac{k}{\\sigma^2}\\varepsilon^2\\right] = \\left(\\frac{k}{\\sigma^2}\\right)^2 \\operatorname{Var}[\\varepsilon^2] = \\frac{k^2}{\\sigma^4}\\operatorname{Var}[\\varepsilon^2]. $$\nThe variance of $\\varepsilon^2$ is $\\operatorname{Var}[\\varepsilon^2] = \\mathbb{E}[(\\varepsilon^2)^2] - (\\mathbb{E}[\\varepsilon^2])^2 = \\mathbb{E}[\\varepsilon^4] - (\\mathbb{E}[\\varepsilon^2])^2$.\nUsing the moments for $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ and $\\mathbb{E}[\\varepsilon^4] = 3\\sigma^4$, we have\n$$ \\operatorname{Var}[\\varepsilon^2] = 3\\sigma^4 - (\\sigma^2)^2 = 2\\sigma^4. $$\nSubstituting this back, we find the variance of the antithetic estimator:\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\frac{k^2}{\\sigma^4}(2\\sigma^4) = 2k^2. $$\n\n**3. Variance Reduction Factor, $V(\\theta, \\sigma)$**\n\nFinally, we compute the ratio of the two variances.\n$$ V(\\theta, \\sigma) = \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]} = \\frac{\\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2k^2}. $$\nSince $k \\neq 0$, we can cancel the $k^2$ terms:\n$$ V(\\theta, \\sigma) = \\frac{\\frac{1}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2} = \\frac{1}{4}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right) = \\frac{\\theta^2}{4\\sigma^2} + \\frac{2}{4} = \\frac{\\theta^2}{4\\sigma^2} + \\frac{1}{2}. $$\nTo write this as a single simplified expression, we can use a common denominator:\n$$ V(\\theta, \\sigma) = \\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}. $$\nThis is the closed-form analytic expression for the variance-reduction factor.", "answer": "$$\\boxed{\\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}}$$", "id": "3158001"}, {"introduction": "In many real-world applications, not all actions are available in every state. This practice [@problem_id:3158020] addresses the common challenge of action masking, where the policy must adapt to changing sets of legal actions. You will derive the score function for a masked softmax policy, a fundamental skill for implementing policy gradient agents in environments with dynamic constraints, such as games or resource allocation problems.", "problem": "Consider a discrete-action policy in Reinforcement Learning (RL) that uses action masking to forbid illegal actions in a given state. Let the action set be $\\{1,2,3,4\\}$ and let the mask be $m \\in \\{0,1\\}^{4}$, where $m_{i} = 1$ indicates that action $i$ is legal and $m_{i} = 0$ indicates that action $i$ is illegal. The policy is parameterized by a vector of logits $\\theta \\in \\mathbb{R}^{4}$, and the probability assigned to action $a$ in state $s$ under mask $m$ is defined by\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} ,\n$$\nwhich enforces zero probability for illegal actions and normalizes only over legal actions. Assume the chosen action $a$ is legal, so that $\\pi(a \\mid s, m, \\theta) > 0$.\n\nStarting from the definition of the score function and basic rules of differentiation, derive the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$ for a legal chosen action $a$ under arbitrary mask $m$, taking care to justify why any components corresponding to illegal actions vanish. Then, evaluate this gradient at\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3 .\n$$\nYour final answer must be the resulting gradient vector expressed exactly in analytic form as a single row matrix. Do not approximate; no rounding is required. No physical units are involved.", "solution": "The primary objective is to derive the gradient of the log-policy, also known as the score function. The policy is given by:\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nTo find the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$, we first compute the logarithm of the policy probability.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln \\left( \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\right)\n$$\nUsing the property of logarithms $\\ln(x/y) = \\ln(x) - \\ln(y)$, we get:\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln(\\exp(\\theta_{a}) \\, m_{a}) - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe problem states that the chosen action $a$ is legal, which means $m_a = 1$. This simplifies the first term: $\\ln(\\exp(\\theta_{a}) \\cdot 1) = \\theta_a$.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nNow, we compute the gradient of this expression with respect to the parameter vector $\\theta$. The gradient is a vector whose $j$-th component is the partial derivative with respect to $\\theta_j$.\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\frac{\\partial}{\\partial \\theta_j} \\left( \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) \\right)\n$$\nWe differentiate term by term:\n$$\n\\frac{\\partial}{\\partial \\theta_j} (\\theta_a) = \\delta_{aj}\n$$\nwhere $\\delta_{aj}$ is the Kronecker delta, which is $1$ if $j=a$ and $0$ otherwise.\n\nFor the second term, we use the chain rule for differentiation ($\\frac{d}{dx}\\ln(f(x)) = \\frac{f'(x)}{f(x)}$):\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\frac{1}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\cdot \\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe derivative of the sum with respect to $\\theta_j$ is:\n$$\n\\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\exp(\\theta_j) \\, m_j\n$$\nCombining these, the derivative of the second term is:\n$$\n\\frac{\\exp(\\theta_j) \\, m_j}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nThis expression is exactly the definition of the policy probability for action $j$, $\\pi(j \\mid s, m, \\theta)$.\n\nSo, the $j$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\delta_{aj} - \\pi(j \\mid s, m, \\theta)\n$$\nThis is the general form of the score function for a masked softmax policy.\n\nThe problem asks to justify why components corresponding to illegal actions vanish. Let $k$ be an index for an illegal action, meaning $m_k=0$. The $k$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = \\delta_{ak} - \\pi(k \\mid s, m, \\theta)\n$$\nSince $a$ is a legal action ($m_a=1$) and $k$ is an illegal action ($m_k=0$), we must have $a \\neq k$. Therefore, $\\delta_{ak} = 0$.\nThe probability of the illegal action $k$ is:\n$$\n\\pi(k \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{k}) \\, m_{k}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = \\frac{\\exp(\\theta_{k}) \\cdot 0}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = 0\n$$\nThus, the $k$-th component of the gradient becomes:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = 0 - 0 = 0\n$$\nThis confirms that the gradient components for all illegal actions are zero.\n\nNow we evaluate the gradient for the given values:\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3\n$$\nFirst, we compute the normalization term, $Z = \\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}$:\n$$\nZ = \\exp(\\theta_1)m_1 + \\exp(\\theta_2)m_2 + \\exp(\\theta_3)m_3 + \\exp(\\theta_4)m_4\n$$\n$$\nZ = \\exp(0) \\cdot 1 + \\exp(1) \\cdot 0 + \\exp(2) \\cdot 1 + \\exp(-1) \\cdot 1\n$$\n$$\nZ = 1 \\cdot 1 + 0 + e^2 \\cdot 1 + e^{-1} \\cdot 1 = 1 + e^2 + e^{-1}\n$$\nNext, we compute the probability vector $\\vec{\\pi}$ whose components are $\\pi(j \\mid s, m, \\theta)$:\n$$\n\\pi(1) = \\frac{\\exp(0) \\cdot 1}{Z} = \\frac{1}{Z}\n$$\n$$\n\\pi(2) = \\frac{\\exp(1) \\cdot 0}{Z} = 0\n$$\n$$\n\\pi(3) = \\frac{\\exp(2) \\cdot 1}{Z} = \\frac{e^2}{Z}\n$$\n$$\n\\pi(4) = \\frac{\\exp(-1) \\cdot 1}{Z} = \\frac{e^{-1}}{Z}\n$$\nThe gradient vector is given by $\\nabla_{\\theta} \\ln \\pi(a=3 \\mid \\dots) = \\mathbf{e}_3 - \\vec{\\pi}$, where $\\mathbf{e}_3$ is the one-hot vector $(0, 0, 1, 0)^T$.\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\pi(1) \\\\ \\pi(2) \\\\ \\pi(3) \\\\ \\pi(4) \\end{pmatrix} = \\begin{pmatrix} -\\pi(1) \\\\ -\\pi(2) \\\\ 1 - \\pi(3) \\\\ -\\pi(4) \\end{pmatrix}\n$$\nSubstituting the probabilities:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ 1 - e^2/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (Z - e^2)/Z \\\\ -e^{-1}/Z \\end{pmatrix}\n$$\nSubstitute $Z = 1 + e^2 + e^{-1}$:\n$$\nZ - e^2 = (1 + e^2 + e^{-1}) - e^2 = 1 + e^{-1}\n$$\nSo the gradient vector is:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{1}{1 + e^2 + e^{-1}} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix}\n$$\nTo simplify, we multiply the numerator and denominator by $e$:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{e}{e(1 + e^2 + e^{-1})} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{e}{e + e^3 + 1} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e(1+e^{-1}) \\\\ e(-e^{-1}) \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e+1 \\\\ -1 \\end{pmatrix}\n$$\nExpressed as a row matrix, this is:\n$$\n\\begin{pmatrix} -\\frac{e}{e^3+e+1} & 0 & \\frac{e+1}{e^3+e+1} & -\\frac{1}{e^3+e+1} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{e}{e^3+e+1} & 0 & \\frac{e+1}{e^3+e+1} & -\\frac{1}{e^3+e+1} \\end{pmatrix}}\n$$", "id": "3158020"}, {"introduction": "Beyond score-function methods, zeroth-order or 'black-box' optimization offers an alternative for policy gradients, especially when the policy is non-differentiable. This problem [@problem_id:3157968] delves into Evolution Strategies (ES), a type of zeroth-order method, and asks you to analyze the bias introduced by using correlated perturbations. This advanced exercise will sharpen your analytical skills by connecting the properties of a gradient estimator back to the underlying dynamics of the environment.", "problem": "Consider a finite-horizon linear Markov Decision Process (MDP) with horizon $T=2$ and scalar state-action dynamics. The state space and action space are both the real line. The dynamics are given by $s_{t+1} = \\alpha s_{t} + \\beta a_{t}$ for $t \\in \\{0,1\\}$, with a known initial state $s_{0} \\in \\mathbb{R}$. The step reward is $r_{t} = c_{s} s_{t} + c_{a} a_{t}$, and the total return is $R = r_{0} + r_{1}$. The policy is time-indexed and Gaussian: at time $t \\in \\{0,1\\}$, the action $a_{t}$ is drawn from a normal distribution with mean $\\theta_{t} s_{t}$ and variance $\\sigma^{2}$, that is $a_{t} \\sim \\mathcal{N}(\\theta_{t} s_{t}, \\sigma^{2})$. Let the policy parameters be $\\theta = (\\theta_{0}, \\theta_{1})$. Define the expected return under the policy and dynamics as $J(\\theta) = \\mathbb{E}[R]$, where the expectation is taken over the action distributions induced by the policy.\n\nTasks:\n- Derive $J(\\theta)$ from first principles, using only the definitions of the dynamics, reward, and linearity of expectation.\n- Compute the exact gradient $\\nabla_{\\theta} J(\\theta)$.\n- Define a central finite-difference evolution strategies estimator with antithetic pairs using correlated perturbations. Let $u = (u_{0}, u_{1})^{\\top}$ be a jointly Gaussian random vector with zero mean and covariance matrix $\\Sigma = \\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}$, where $|\\rho| < 1$. For a small step size $\\epsilon > 0$, define the estimator\n$$\\hat{g}(u) = \\frac{R(\\theta + \\epsilon u) - R(\\theta - \\epsilon u)}{2 \\epsilon}\\, u,$$\nwhere $R(\\cdot)$ denotes the realized return under the perturbed parameters and the same environment model as above. Assume $u$ is independent of the policy action noises and that expectations are taken both over the trajectory randomness and the perturbations $u$.\n- Compare $\\mathbb{E}[\\hat{g}(u)]$ to the exact gradient $\\nabla_{\\theta} J(\\theta)$, and provide the bias vector defined by $\\operatorname{Bias}(\\theta, \\rho) = \\mathbb{E}[\\hat{g}(u)] - \\nabla_{\\theta} J(\\theta)$.\n\nYour final answer must be the closed-form analytic expression for the bias vector $\\operatorname{Bias}(\\theta, \\rho)$ as a row vector in terms of $\\rho$, $\\alpha$, $\\beta$, $c_{s}$, $c_{a}$, $s_{0}$, $\\theta_{0}$, and $\\theta_{1}$. No numerical approximation is required.", "solution": "The derivation proceeds by completing the tasks outlined in the problem statement.\n\nFirst, we derive the expected return $J(\\theta)$. The objective is the expected total return, $J(\\theta) = \\mathbb{E}[R] = \\mathbb{E}[r_{0}] + \\mathbb{E}[r_{1}]$.\nThe expected reward at $t=0$ is:\n$$\\mathbb{E}[r_{0}] = \\mathbb{E}[c_{s} s_{0} + c_{a} a_{0}] = c_{s} s_{0} + c_{a} \\mathbb{E}[a_{0}]$$\nSince $a_{0} \\sim \\mathcal{N}(\\theta_{0} s_{0}, \\sigma^{2})$, its expectation is $\\mathbb{E}[a_{0}] = \\theta_{0} s_{0}$.\n$$\\mathbb{E}[r_{0}] = c_{s} s_{0} + c_{a} (\\theta_{0} s_{0}) = s_{0} (c_{s} + c_{a} \\theta_{0})$$\nThe expected reward at $t=1$ is $\\mathbb{E}[r_{1}] = c_{s} \\mathbb{E}[s_{1}] + c_{a} \\mathbb{E}[a_{1}]$. We find the required expectations:\n$$\\mathbb{E}[s_{1}] = \\mathbb{E}[\\alpha s_{0} + \\beta a_{0}] = \\alpha s_{0} + \\beta \\mathbb{E}[a_{0}] = \\alpha s_{0} + \\beta (\\theta_{0} s_{0}) = s_{0} (\\alpha + \\beta \\theta_{0})$$\n$$\\mathbb{E}[a_{1}] = \\mathbb{E}[\\mathbb{E}[a_{1} | s_{1}]] = \\mathbb{E}[\\theta_{1} s_{1}] = \\theta_{1} \\mathbb{E}[s_{1}] = \\theta_{1} s_{0} (\\alpha + \\beta \\theta_{0})$$\nSubstituting these back into $\\mathbb{E}[r_{1}]$:\n$$\\mathbb{E}[r_{1}] = c_{s} [s_{0} (\\alpha + \\beta \\theta_{0})] + c_{a} [\\theta_{1} s_{0} (\\alpha + \\beta \\theta_{0})] = s_{0} (c_{s} + c_{a} \\theta_{1}) (\\alpha + \\beta \\theta_{0})$$\nSumming the expected rewards gives the total expected return:\n$$J(\\theta) = s_{0} (c_{s} + c_{a} \\theta_{0}) + s_{0} (c_{s} + c_{a} \\theta_{1}) (\\alpha + \\beta \\theta_{0})$$\n\nSecond, we compute the exact gradient $\\nabla_{\\theta} J(\\theta)$. Expanding $J(\\theta)$ simplifies differentiation:\n$$J(\\theta) = s_{0} [c_{s} + c_{a} \\theta_{0} + c_{s}\\alpha + c_{s}\\beta\\theta_{0} + c_{a}\\alpha\\theta_{1} + c_{a}\\beta\\theta_{0}\\theta_{1}]$$\nThe partial derivative with respect to $\\theta_{0}$ is:\n$$\\frac{\\partial J}{\\partial \\theta_{0}} = \\frac{\\partial}{\\partial \\theta_{0}} \\left( s_{0} [c_{a} \\theta_{0} + c_{s}\\beta\\theta_{0} + c_{a}\\beta\\theta_{0}\\theta_{1}] \\right) = s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1})$$\nThe partial derivative with respect to $\\theta_{1}$ is:\n$$\\frac{\\partial J}{\\partial \\theta_{1}} = \\frac{\\partial}{\\partial \\theta_{1}} \\left( s_{0} [c_{a}\\alpha\\theta_{1} + c_{a}\\beta\\theta_{0}\\theta_{1}] \\right) = s_{0} (c_{a}\\alpha + c_{a}\\beta\\theta_{0}) = s_{0} c_{a} (\\alpha + \\beta\\theta_{0})$$\nSo, the gradient vector is:\n$$\\nabla_{\\theta} J(\\theta) = \\begin{pmatrix} s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1}) \\\\ s_{0} c_{a} (\\alpha + \\beta\\theta_{0}) \\end{pmatrix}$$\n\nThird, we compute the expectation of the estimator $\\mathbb{E}[\\hat{g}(u)]$. We take the expectation over both the trajectory randomness (from actions $a_t$) and the perturbation randomness (from $u$).\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u, \\tau} \\left[ \\frac{R(\\theta + \\epsilon u) - R(\\theta - \\epsilon u)}{2 \\epsilon} u \\right]$$\nTaking the expectation over the trajectory first yields the objective function $J(\\cdot)$.\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u} \\left[ \\frac{J(\\theta + \\epsilon u) - J(\\theta - \\epsilon u)}{2 \\epsilon} u \\right]$$\nSince $J(\\theta)$ is a quadratic polynomial in $\\theta$, the central finite difference is exact and gives the directional derivative:\n$$\\frac{J(\\theta + \\epsilon u) - J(\\theta - \\epsilon u)}{2\\epsilon} = u^{\\top} \\nabla_{\\theta} J(\\theta)$$\nSubstituting this back into the expectation:\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u} [ (u^{\\top} \\nabla_{\\theta} J(\\theta)) u ]$$\nLet $g = \\nabla_{\\theta} J(\\theta)$ be the true gradient, which is constant with respect to $u$.\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u} [ u (u^{\\top} g) ] = \\mathbb{E}_{u} [ u u^{\\top} ] g$$\nThe term $\\mathbb{E}_{u} [ u u^{\\top} ]$ is the covariance matrix of the random vector $u$, which is given as $\\Sigma$.\nTherefore, the expected value of the estimator is:\n$$\\mathbb{E}[\\hat{g}(u)] = \\Sigma \\nabla_{\\theta} J(\\theta)$$\n\nFourth, we compute the bias vector $\\operatorname{Bias}(\\theta, \\rho)$.\n$$\\operatorname{Bias}(\\theta, \\rho) = \\mathbb{E}[\\hat{g}(u)] - \\nabla_{\\theta} J(\\theta) = \\Sigma \\nabla_{\\theta} J(\\theta) - I \\nabla_{\\theta} J(\\theta) = (\\Sigma - I) \\nabla_{\\theta} J(\\theta)$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\n$$\\Sigma - I = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & \\rho \\\\ \\rho & 0 \\end{pmatrix}$$\nLet $g = \\nabla_{\\theta} J(\\theta) = (g_{0}, g_{1})^{\\top}$. The bias is:\n$$\\operatorname{Bias}(\\theta, \\rho) = \\begin{pmatrix} 0 & \\rho \\\\ \\rho & 0 \\end{pmatrix} \\begin{pmatrix} g_{0} \\\\ g_{1} \\end{pmatrix} = \\begin{pmatrix} \\rho g_{1} \\\\ \\rho g_{0} \\end{pmatrix}$$\nSubstituting the expressions for $g_{0}$ and $g_{1}$:\n$$g_{0} = s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1})$$\n$$g_{1} = s_{0} c_{a} (\\alpha + \\beta\\theta_{0})$$\nThe bias vector components are:\n-   First component: $\\rho g_{1} = \\rho s_{0} c_{a} (\\alpha + \\beta\\theta_{0})$\n-   Second component: $\\rho g_{0} = \\rho s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1})$\n\nAs a row vector, the bias is:\n$$\\operatorname{Bias}(\\theta, \\rho) = \\begin{pmatrix} \\rho s_{0} c_{a} (\\alpha + \\beta\\theta_{0}) & \\rho s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1}) \\end{pmatrix}$$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\rho s_{0} c_{a} (\\alpha + \\beta\\theta_{0}) & \\rho s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1}) \\end{pmatrix} } $$", "id": "3157968"}]}