{"hands_on_practices": [{"introduction": "Prioritized Experience Replay (PER) is a powerful technique that improves learning efficiency by focusing on \"surprising\" transitions. However, this non-uniform sampling can introduce its own set of problems. This first exercise [@problem_id:3113071] asks you to build a simulation to explore a critical side effect of PER: sampling bias and potential \"mode collapse,\" where the agent fixates on high-error transitions and ignores other valuable experiences. By observing this phenomenon in a controlled setting, you'll gain a deeper intuition for the delicate balance required in designing smart replay systems.", "problem": "Construct a minimal Markov Decision Process that reveals how stochastic rewards can induce a bimodal Temporal-Difference (TD) error and how Prioritized Experience Replay can bias sampling toward one mode, potentially collapsing the diversity of sampled transitions. Use the following fundamental base: the Bellman optimality equation and the definition of the Temporal-Difference error, together with the standard definition of prioritized sampling probability.\n\nYou are given a single-state, single-action environment. A Deep Q-Network (DQN) is assumed to be present but held fixed (no learning), so that the action-value estimate remains constant. Let the reward at each time step be drawn independently from a two-point mixture distribution:\n- With probability $p_{\\mathrm{small}}$, the reward is $r_{\\mathrm{small}}$.\n- With probability $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$, the reward is $r_{\\mathrm{big}}$.\n\nLet the discount factor be $\\gamma$, and let the fixed Q-value be $q_0$ for the unique state-action pair. The Temporal-Difference (TD) error is defined by the core Temporal-Difference (TD) learning definition\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a).\n$$\nBecause $Q$ is held fixed at $q_0$ and there is only one action, you must use\n$$\n\\delta = r + \\gamma q_0 - q_0.\n$$\nWhen $q_0 = 0$, this reduces to $\\delta = r$, which directly inherits the bimodality from the two-point reward distribution whenever $\\lvert r_{\\mathrm{small}} \\rvert \\neq \\lvert r_{\\mathrm{big}} \\rvert$.\n\nFor Prioritized Experience Replay, transitions stored in a buffer of size $N$ are sampled with probability proportional to a power of their absolute TD error, using the standard, well-tested formulation:\n- For each transition $i$ with TD error $\\delta_i$, assign an unnormalized priority $w_i = \\left( \\lvert \\delta_i \\rvert + \\varepsilon \\right)^{\\alpha}$ for given $\\alpha \\ge 0$ and small $\\varepsilon \\ge 0$.\n- The sampling probability is $p_i = w_i / \\sum_j w_j$.\n- Samples are drawn with replacement.\n\nYour task is to write a complete, runnable program that:\n1. Constructs the described environment and fills a replay buffer with $N$ independent transitions by drawing rewards from the specified mixture. Use a fixed random seed $s$ for reproducibility.\n2. Labels each stored transition by which mixture mode generated its reward. Define the “minor mode” to be the mode with smaller absolute TD error magnitude $m_{\\mathrm{small}} = \\min\\{\\lvert r_{\\mathrm{small}} + (\\gamma - 1) q_0 \\rvert, \\lvert r_{\\mathrm{big}} + (\\gamma - 1) q_0 \\rvert\\}$, and the “big mode” to be the other one with magnitude $m_{\\mathrm{big}}$.\n3. Performs $M$ prioritized samples with replacement from the buffer using $p_i \\propto (\\lvert \\delta_i \\rvert + \\varepsilon)^{\\alpha}$.\n4. Computes the observed fraction $\\hat{f}_{\\mathrm{minor}}$ of sampled transitions that came from the minor mode.\n5. Reports $\\hat{f}_{\\mathrm{minor}}$ for each test case as a float rounded to exactly $6$ decimal places.\n\nPrinciple-based reasoning target: starting from the TD error definition and the prioritized weighting rule above, reason about how the expected minor-mode sampling fraction depends on the mixture probabilities and the magnitudes of the two modes. Programmatically estimate this quantity by Monte Carlo sampling and report the results.\n\nTest suite and parameterization. Run the program on the following four parameter sets, each specified as a tuple $(N, M, p_{\\mathrm{small}}, r_{\\mathrm{small}}, r_{\\mathrm{big}}, \\alpha, \\varepsilon, \\gamma, q_0, s)$:\n- Case A (boundary, uniform replay): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.2,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 3,\\; \\alpha = \\; 0,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n- Case B (happy path, moderate prioritization): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.5,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 5,\\; \\alpha = \\; 1,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n- Case C (edge, strong prioritization yielding mode collapse): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n- Case D (edge, adding $\\varepsilon$ to mitigate collapse): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 20,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n\nNotes:\n- Because $q_0 = 0$ and $\\gamma = 0$ in all cases, the TD error simplifies to $\\delta = r$.\n- Case A tests the boundary $\\alpha = 0$, where sampling must be uniform over the buffer regardless of magnitudes.\n- Case B demonstrates bias toward the larger-magnitude mode under moderate prioritization.\n- Case C demonstrates “mode collapse,” operationalized here as a very small $\\hat{f}_{\\mathrm{minor}}$ when $\\alpha$ is large and the magnitude ratio is large.\n- Case D demonstrates that making $\\varepsilon$ comparable to the larger magnitude can mitigate collapse by flattening priorities.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of the four rounded minor-mode fractions $[\\hat{f}_{\\mathrm{minor}}^{(A)}, \\hat{f}_{\\mathrm{minor}}^{(B)}, \\hat{f}_{\\mathrm{minor}}^{(C)}, \\hat{f}_{\\mathrm{minor}}^{(D)}]$, for Cases A, B, C, and D respectively. For example, an output of the form $[0.200000,0.166700,0.010000,0.524000]$ would be acceptable if those were the computed values.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded simulation task within the domain of deep reinforcement learning. All parameters and definitions are provided, and the objective is clear and unambiguous.\n\nThe objective is to demonstrate the sampling bias induced by Prioritized Experience Replay (PER) in a minimal Markov Decision Process (MDP). This MDP is constructed with a single state and a single action, such that the action-value function $Q(s, a)$ is a fixed constant, $q_0$. The reward $r$ at each step is stochastic, drawn from a two-point mixture distribution: $r = r_{\\mathrm{small}}$ with probability $p_{\\mathrm{small}}$ and $r = r_{\\mathrm{big}}$ with probability $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$.\n\nThe core of the analysis rests on the Temporal-Difference (TD) error, $\\delta$, which is the basis for calculating priorities in PER. For a given transition, the TD error is defined by the Bellman equation:\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\n$$\nIn our simplified one-state MDP, the next state $s'$ is identical to the current state $s$, and there is only one action. The Q-value is fixed at $q_0$. The equation thus simplifies to:\n$$\n\\delta = r + (\\gamma - 1)q_0\n$$\nSince the reward $r$ is bimodal, the TD error $\\delta$ is also bimodal, with two possible values:\n$$\n\\delta_{\\mathrm{small}} = r_{\\mathrm{small}} + (\\gamma - 1)q_0\n$$\n$$\n\\delta_{\\mathrm{big}} = r_{\\mathrm{big}} + (\\gamma - 1)q_0\n$$\nThe problem defines two modes based on the absolute magnitude of these TD errors. The \"minor mode\" corresponds to the smaller magnitude, $m_{\\mathrm{minor}} = \\min(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$, and the \"big mode\" to the larger magnitude, $m_{\\mathrm{major}} = \\max(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$.\n\nIn PER, transitions are sampled from a replay buffer of size $N$ with a probability proportional to their priority. The priority $w_i$ of a transition $i$ with TD error $\\delta_i$ is given by:\n$$\nw_i = (|\\delta_i| + \\varepsilon)^{\\alpha}\n$$\nwhere $\\alpha \\ge 0$ is the prioritization exponent and $\\varepsilon \\ge 0$ is a small constant to prevent zero priorities. The probability of sampling transition $i$ is:\n$$\np_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j}\n$$\nLet the replay buffer contain $N_{\\mathrm{minor}}$ transitions from the minor mode and $N_{\\mathrm{major}}$ from the major mode, where $N_{\\mathrm{minor}} + N_{\\mathrm{major}} = N$. The priorities for these modes are $w_{\\mathrm{minor}} = (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$ and $w_{\\mathrm{major}} = (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}$. The total sum of priorities in the buffer is $W = N_{\\mathrm{minor}} w_{\\mathrm{minor}} + N_{\\mathrm{major}} w_{\\mathrm{major}}$.\n\nThe theoretical probability of picking any transition from the minor mode in a single draw is the sum of their individual probabilities, which simplifies to:\n$$\nP(\\text{sample is minor}) = \\frac{N_{\\mathrm{minor}} \\cdot w_{\\mathrm{minor}}}{W} = \\frac{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}}{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha} + N_{\\mathrm{major}} \\cdot (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}}\n$$\nThis equation reveals the source of the sampling bias.\n- When $\\alpha = 0$, $w_{\\mathrm{minor}} = w_{\\mathrm{major}} = 1$. The sampling probability becomes $P(\\text{sample is minor}) = N_{\\mathrm{minor}} / (N_{\\mathrm{minor}} + N_{\\mathrm{major}}) = N_{\\mathrm{minor}} / N$. This corresponds to uniform sampling, where the sampling fraction mirrors the composition of the buffer.\n- When $\\alpha > 0$, the ratio of priorities $w_{\\mathrm{major}}/w_{\\mathrm{minor}}$ can become very large if $m_{\\mathrm{major}} \\gg m_{\\mathrm{minor}}$. This heavily biases sampling towards the major mode, even if $N_{\\mathrm{minor}} \\gg N_{\\mathrm{major}}$. This can lead to \"mode collapse,\" where transitions from the minor mode are rarely sampled.\n- A non-zero $\\varepsilon$, especially when comparable to $m_{\\mathrm{major}}$, reduces the ratio of priorities $(m_{\\mathrm{major}} + \\varepsilon)^{\\alpha} / (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$, thus mitigating the sampling bias.\n\nTo solve the problem, we perform a Monte Carlo simulation for each test case:\n1.  A replay buffer of $N$ transitions is instantiated. For each transition, a reward is drawn from the specified two-point distribution. We use a fixed random seed $s$ for reproducibility.\n2.  Based on the parameters for each case, we determine which reward ($r_{\\mathrm{small}}$ or $r_{\\mathrm{big}}$) gives rise to the minor TD error mode. Each transition in the buffer is labeled accordingly. For all provided test cases, $\\gamma = 0$ and $q_0 = 0$, so the TD error is identical to the reward, $\\delta = r$. The minor mode is thus simply the one with the smaller absolute reward.\n3.  The priorities for all $N$ transitions are calculated using the formula $w_i = (|\\delta_i| + \\varepsilon)^{\\alpha}$.\n4.  These priorities are normalized to form a sampling probability distribution over the $N$ transitions.\n5.  $M$ samples are drawn with replacement from the buffer according to this distribution.\n6.  The fraction of these $M$ samples that belong to the minor mode, $\\hat{f}_{\\mathrm{minor}}$, is calculated. This fraction serves as the numerical estimate for $P(\\text{sample is minor})$.\n\nThis procedure is systematically applied to all four test cases to quantify the impact of the parameters $\\alpha$ and $\\varepsilon$ on sampling diversity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to demonstrate the effect of Prioritized\n    Experience Replay (PER) on sampling bias in a simplified MDP.\n    \"\"\"\n    test_cases = [\n        # Case A: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.2, -1, 3, 0, 0, 0, 0, 42),\n        # Case B: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.5, -1, 5, 1, 0, 0, 0, 42),\n        # Case C: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 0, 0, 0, 42),\n        # Case D: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 20, 0, 0, 42),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s = case\n\n        # 1. Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # 2. Fill the replay buffer with N transitions.\n        # Draw N random numbers to determine the reward for each transition.\n        reward_choices = rng.random(size=N)\n        # Assign rewards based on the mixture probability p_small.\n        rewards = np.where(reward_choices < p_small, r_small, r_big)\n\n        # 3. Calculate TD errors and identify which transitions belong to the minor mode.\n        td_error_for_r_small = r_small + (gamma - 1) * q0\n        td_error_for_r_big = r_big + (gamma - 1) * q0\n\n        # An array indicating which transitions were generated by the r_small component.\n        is_from_small_reward_source = (rewards == r_small)\n\n        # Determine which reward source corresponds to the minor TD error mode.\n        if np.abs(td_error_for_r_small) < np.abs(td_error_for_r_big):\n            # The minor mode corresponds to r_small.\n            is_minor_mode = is_from_small_reward_source\n        else:\n            # The minor mode corresponds to r_big.\n            is_minor_mode = ~is_from_small_reward_source\n\n        # Calculate the TD error for each transition in the buffer.\n        td_errors = rewards + (gamma - 1) * q0\n\n        # 4. Compute sampling priorities and probabilities.\n        priorities = (np.abs(td_errors) + epsilon)**alpha\n        \n        total_priority = np.sum(priorities)\n        \n        if total_priority > 0:\n            sampling_probs = priorities / total_priority\n        else:\n            # Fallback to uniform sampling if all priorities are zero.\n            # This can happen if alpha > 0 and |delta_i| + epsilon is 0 for all i.\n            # Not expected for the given test cases.\n            sampling_probs = np.full(N, 1.0 / N)\n\n        # 5. Perform M prioritized samples with replacement.\n        sampled_indices = rng.choice(N, size=M, replace=True, p=sampling_probs)\n\n        # 6. Compute the observed fraction of sampled transitions from the minor mode.\n        num_minor_sampled = np.sum(is_minor_mode[sampled_indices])\n        f_minor_hat = num_minor_sampled / M\n        \n        results.append(f_minor_hat)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113071"}, {"introduction": "Having seen how standard prioritization can be problematic, the next step is to design improved replay strategies. This practice [@problem_id:3113068] challenges you to implement and analyze a novel \"age-aware\" mechanism that balances a transition's TD error with its age. By computing metrics for learning speed and stability, you will gain hands-on experience in evaluating the trade-offs of new algorithm designs.", "problem": "Consider the setting of Deep Q-Network (DQN) training with Experience Replay (ER). Let a replay buffer contain $N$ transitions indexed by $i \\in \\{1, \\dots, N\\}$, each with a temporal-difference error $\\delta_i$ and an associated age $\\text{age}_i$ measured as the number of environment steps since storage. We propose an age-aware prioritized replay where the unnormalized priority of transition $i$ is given by\n$$\ns_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i},\n$$\nwith $\\alpha \\ge 0$ controlling the degree of prioritization and $\\lambda \\ge 0$ penalizing older transitions. The sampling probability is\n$$\np_i = \\frac{s_i}{\\sum_{j=1}^{N} s_j},\n$$\nwith the convention that if $\\sum_{j=1}^{N} s_j = 0$ then $p_i = \\frac{1}{N}$ for all $i$.\n\nTo mitigate bias from non-uniform sampling, importance sampling weights are used. Let $\\beta \\in [0,1]$ control the strength of correction. Define the unnormalized weight\n$$\nw_i = (N \\, p_i)^{-\\beta}\n$$\nfor $p_i > 0$, and $w_i = 0$ when $p_i = 0$. For numerical stability and to avoid uncontrolled step size scaling, define a normalized weight\n$$\n\\hat{w}_i = \n\\begin{cases}\n\\frac{w_i}{\\max_{k: p_k > 0} w_k} & \\text{if } \\max_{k: p_k > 0} w_k > 0, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\n\nWe analyze two core aspects:\n- Learning speed proxy $S$, defined as the expected absolute update magnitude under the sampling distribution,\n$$\nS = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, |\\delta_i|.\n$$\n- Stability proxy $V$, defined as the variance of signed updates,\n$$\n\\mu = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, \\delta_i, \\quad\nV = \\sum_{i=1}^{N} p_i \\left(\\hat{w}_i \\, \\delta_i - \\mu \\right)^2.\n$$\n\nAdditionally, report the effective sample size (ESS) and an outlier-sensitivity factor:\n$$\n\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} p_i^2}, \\quad\nO = \\frac{\\max_{i} \\hat{w}_i}{\\frac{1}{N} \\sum_{i=1}^{N} \\hat{w}_i}.\n$$\nThe ESS quantifies diversity of the sampling distribution, and $O$ quantifies the potential dominance of the largest normalized weight relative to the mean normalized weight.\n\nStarting from the fundamental base that DQN minimizes the mean-squared Bellman error using stochastic gradient descent and that importance sampling corrects the expectation of gradient estimates under non-uniform sampling, derive how $S$ and $V$ relate to $p_i$ and $\\hat{w}_i$. Implement a program that, for each test case below, computes and returns $(S, V, \\text{ESS}, O)$.\n\nTest suite:\n- Case $1$ (general happy path): $N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 0.5$, $\\beta = 0.4$.\n- Case $2$ (uniform baseline): $N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0$, $\\lambda = 0$, $\\beta = 0$.\n- Case $3$ (strong age penalty): $N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 2.0$, $\\beta = 0.4$.\n- Case $4$ (outlier temporal-difference error): $N=6$, $\\delta = [\\,0.01,\\,0.02,\\,0.01,\\,5.0,\\,0.0,\\,-0.01\\,]$, $\\text{age} = [\\,0,\\,1,\\,0,\\,0,\\,50,\\,2\\,]$, $\\alpha = 1.0$, $\\lambda = 0.1$, $\\beta = 0.6$.\n- Case $5$ (degenerate zero priorities): $N=4$, $\\delta = [\\,0,\\,0,\\,0,\\,0\\,]$, $\\text{age} = [\\,0,\\,10,\\,20,\\,30\\,]$, $\\alpha = 0.5$, $\\lambda = 1.0$, $\\beta = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The order must be $[S_1,V_1,\\text{ESS}_1,O_1,S_2,V_2,\\text{ESS}_2,O_2,S_3,V_3,\\text{ESS}_3,O_3,S_4,V_4,\\text{ESS}_4,O_4,S_5,V_5,\\text{ESS}_5,O_5]$, where the subscript denotes the case index. All values must be floats.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in the analysis of a prioritized experience replay mechanism for deep Q-networks (DQNs). It provides a complete and consistent set of definitions and data, allowing for a unique and verifiable solution. We may therefore proceed with a full solution.\n\nThe core of the problem requires us to understand the roles of the learning speed proxy, $S$, and the stability proxy, $V$, in the context of importance sampling corrections for prioritized replay. To do so, we must first revisit the fundamentals of learning in DQNs.\n\nA DQN is trained to minimize the mean-squared Bellman error (MSBE) over a distribution of transitions $\\mathcal{D}$. The loss function for a single transition $i$, consisting of state $s_i$, action $a_i$, reward $r_i$, and next state $s'_i$, is given by $L_i(\\theta) = \\delta_i^2$, where the temporal-difference (TD) error $\\delta_i = (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-)) - Q(s_i, a_i; \\theta)$, with $\\theta$ being the weights of the online network and $\\theta^-$ being the weights of the target network.\n\nTraining proceeds via stochastic gradient descent (SGD). The gradient of the loss with respect to the online network weights $\\theta$ for transition $i$ is:\n$$\n\\nabla_\\theta L_i(\\theta) = \\nabla_\\theta \\left( (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-) - Q(s_i, a_i; \\theta))^2 \\right) \\approx -2\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)\n$$\nHere, we treat the target term as a constant, a standard practice in Q-learning. The SGD update rule is thus $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)$, where $\\eta$ is the learning rate. For a single sample $i$, the update is proportional to $\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$.\n\nIf we sample transitions uniformly from the replay buffer of size $N$, the probability of sampling any transition $i$ is $1/N$. The expected gradient over a minibatch is an unbiased estimate of the true gradient over the entire buffer. However, uniform sampling is inefficient as it gives equal importance to all transitions, including those with small errors that contribute little to learning.\n\nPrioritized Experience Replay (PER) addresses this by sampling transitions non-uniformly with probabilities $p_i$ that are monotonically related to the magnitude of their TD errors, $|\\delta_i|$. In this problem, the sampling probability $p_i$ is derived from the score $s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$. This introduces a bias: the expectation of the gradient under this sampling distribution $p_i$ no longer matches the true gradient expectation.\n$$\n\\mathbb{E}_{i \\sim p}[\\nabla_\\theta L_i] = \\sum_{i=1}^{N} p_i \\nabla_\\theta L_i \\neq \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_\\theta L_i = \\mathbb{E}_{i \\sim U(1/N)}[\\nabla_\\theta L_i]\n$$\nTo correct for this bias, we employ importance sampling (IS). The update for each sample $i$ is weighted by the ratio of the target distribution (uniform) to the sampling distribution, $\\frac{1/N}{p_i}$. The loss for sample $i$ becomes $L_i^{IS}(\\theta) = \\frac{1}{N p_i} \\delta_i^2$. However, using these raw weights can lead to instability. For this reason, two modifications are standard:\n1. The weight is raised to a power $\\beta \\in [0, 1]$, giving $w_i = (N p_i)^{-\\beta}$. This parameter allows for a smooth interpolation between uniform sampling ($\\beta=0$, where $w_i=1$) and full IS correction ($\\beta=1$). This introduces a controlled amount of bias for a significant reduction in variance.\n2. The weights are normalized by their maximum value, $\\hat{w}_i = w_i / \\max_k w_k$. This ensures that the largest weight is capped at $1$, preventing any single transition from dominating the gradient update and destabilizing the learning process.\n\nThe resulting gradient update for a sample $i$ is now proportional to $\\hat{w}_i \\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$. The term $\\hat{w}_i \\delta_i$ can be seen as the effective, corrected error signal that drives learning for sample $i$.\n\nWith this foundation, we can now justify the definitions of the proxies $S$ and $V$.\n- **Learning Speed Proxy, $S$**: The magnitude of the parameter update for sample $i$ is proportional to $|\\hat{w}_i \\delta_i| = \\hat{w}_i |\\delta_i|$ (since $\\hat{w}_i \\ge 0$). The expected magnitude of an update step, when sampling with probabilities $p_i$, is the expectation of this quantity over the sampling distribution:\n$$\n\\mathbb{E}_{i \\sim p}[\\text{update magnitude}] \\propto \\sum_{i=1}^N p_i \\hat{w}_i |\\delta_i| = S\n$$\nTherefore, $S$ represents the expected size of a gradient step. A larger value of $S$ suggests that, on average, the network is making larger adjustments to its weights, which is a plausible proxy for accelerated learning.\n\n- **Stability Proxy, $V$**: The stability of SGD is critically dependent on the variance of the gradient estimates. High variance can cause the learning process to oscillate or diverge. The term $\\hat{w}_i \\delta_i$ is the random variable representing the signed corrective signal for a sampled transition $i$. The quantity $V$ is defined as the variance of this random variable under the sampling distribution $p_i$:\n$$\n\\mu = \\mathbb{E}[\\hat{w} \\delta] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i)\n$$\n$$\nV = \\text{Var}(\\hat{w} \\delta) = \\mathbb{E}[(\\hat{w} \\delta - \\mu)^2] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i - \\mu)^2\n$$\nA smaller value of $V$ indicates that the corrective signals are more consistent across different samples, which is a proxy for a more stable and reliable learning process.\n\nThe other two metrics provide additional diagnostics:\n- **Effective Sample Size, ESS**: ESS measures the diversity of the samples. For a uniform distribution ($p_i = 1/N$), $\\text{ESS} = N$. For a highly skewed distribution where one $p_k \\approx 1$, ESS approaches $1$. A low ESS indicates that the replay is repeatedly sampling a small subset of transitions, potentially leading to overfitting and poor generalization.\n- **Outlier-sensitivity Factor, $O$**: This metric quantifies the dominance of the largest normalized weight $\\hat{w}_i$ compared to the average. A high value of $O$ indicates that one or a few samples have a disproportionately large influence on the updates, which can be a source of instability.\n\nThe algorithm to compute these four metrics for a given test case is as follows:\n1.  Given $N, \\{\\delta_i\\}, \\{\\text{age}_i\\}, \\alpha, \\lambda, \\beta$.\n2.  Calculate the unnormalized priority for each transition $i$: $s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$.\n3.  Calculate the sum of priorities, $S_{tot} = \\sum_{j=1}^N s_j$.\n4.  If $S_{tot} = 0$, set sampling probabilities to uniform: $p_i = 1/N$ for all $i$. Otherwise, normalize: $p_i = s_i / S_{tot}$.\n5.  Calculate unnormalized importance-sampling weights $w_i$. For each $i$ where $p_i > 0$, set $w_i = (N p_i)^{-\\beta}$. For $i$ where $p_i = 0$, set $w_i = 0$.\n6.  Calculate the maximum weight, $w_{max} = \\max_k w_k$.\n7.  If $w_{max} > 0$, calculate normalized weights $\\hat{w}_i = w_i / w_{max}$. Otherwise, set $\\hat{w}_i = 0$ for all $i$.\n8.  Compute the four metrics using their definitions:\n    - $S = \\sum_{i=1}^{N} p_i \\hat{w}_i |\\delta_i|$.\n    - $\\mu = \\sum_{i=1}^{N} p_i \\hat{w}_i \\delta_i$, then $V = \\sum_{i=1}^{N} p_i (\\hat{w}_i \\delta_i - \\mu)^2$.\n    - $\\text{ESS} = 1 / (\\sum_{i=1}^{N} p_i^2)$.\n    - $O = (\\max_i \\hat{w}_i) / (\\frac{1}{N}\\sum_{i=1}^N \\hat{w}_i)$, assuming the denominator is non-zero.\nThis procedure is deterministic and computationally straightforward.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes learning and stability metrics for an age-aware prioritized experience replay scheme.\n    \"\"\"\n    test_cases = [\n        # Case 1: general happy path\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 0.5, 'beta': 0.4},\n        # Case 2: uniform baseline\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.0, 'lambda': 0.0, 'beta': 0.0},\n        # Case 3: strong age penalty\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 2.0, 'beta': 0.4},\n        # Case 4: outlier temporal-difference error\n        {'N': 6, 'delta': [0.01, 0.02, 0.01, 5.0, 0.0, -0.01], 'age': [0, 1, 0, 0, 50, 2], 'alpha': 1.0, 'lambda': 0.1, 'beta': 0.6},\n        # Case 5: degenerate zero priorities\n        {'N': 4, 'delta': [0.0, 0.0, 0.0, 0.0], 'age': [0, 10, 20, 30], 'alpha': 0.5, 'lambda': 1.0, 'beta': 0.5},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        delta = np.array(case['delta'], dtype=float)\n        age = np.array(case['age'], dtype=float)\n        alpha = case['alpha']\n        lambda_ = case['lambda']\n        beta = case['beta']\n\n        # 1. Calculate unnormalized priorities s_i\n        # np.power(0.0, 0.0) correctly returns 1.0, handling the alpha=0 case.\n        s = np.power(np.abs(delta), alpha) / (1.0 + lambda_ * age)\n\n        # 2. Calculate sampling probabilities p_i\n        sum_s = np.sum(s)\n        if sum_s == 0.0:\n            p = np.full(N, 1.0 / N)\n        else:\n            p = s / sum_s\n\n        # 3. Calculate unnormalized importance sampling weights w_i\n        w = np.zeros(N, dtype=float)\n        positive_p_mask = p > 0.0\n        if np.any(positive_p_mask):\n            w[positive_p_mask] = np.power(N * p[positive_p_mask], -beta)\n            \n        # 4. Calculate normalized importance sampling weights w_hat_i\n        w_hat = np.zeros(N, dtype=float)\n        max_w = np.max(w)\n        if max_w > 0.0:\n            w_hat = w / max_w\n            \n        # 5. Calculate metrics S, V, ESS, O\n        # S: Learning speed proxy\n        S = np.sum(p * w_hat * np.abs(delta))\n\n        # V: Stability proxy (Variance)\n        mu = np.sum(p * w_hat * delta)\n        V = np.sum(p * np.power(w_hat * delta - mu, 2))\n\n        # ESS: Effective Sample Size\n        ESS = 1.0 / np.sum(np.power(p, 2))\n\n        # O: Outlier-sensitivity factor\n        max_w_hat = np.max(w_hat)\n        mean_w_hat = np.mean(w_hat)\n        \n        if mean_w_hat > 0:\n            O = max_w_hat / mean_w_hat\n        else:\n            # This case implies all w_hat are 0, so max_w_hat is also 0.\n            # No single dominant weight exists. A ratio of 1.0 is a reasonable convention.\n            O = 1.0\n\n        results.extend([S, V, ESS, O])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3113068"}, {"introduction": "While simulations and empirical metrics are invaluable, a formal mathematical analysis provides the deepest understanding of an algorithm's properties. This final practice [@problem_id:3113098] guides you through a rigorous derivation of the bias and variance introduced by a replay modification that prunes \"easy\" transitions based on their Temporal Difference (TD) error, $\\delta$. Starting from first principles, you will use probability theory to quantify the statistical consequences of this heuristic, honing a core skill for developing provably effective reinforcement learning methods.", "problem": "A reinforcement learning agent uses Deep Q-Networks (DQN) with Experience Replay (ER). Let the Temporal Difference (TD) error be defined by the Bellman residual $\\delta = r + \\gamma \\max_{a'} Q_{\\theta^{-}}(s', a') - Q_{\\theta}(s, a)$, and let the squared TD loss be $L(\\theta) = \\mathbb{E}[\\delta^{2}]$ under uniform sampling from the replay buffer.\n\nTo reduce computation on \"easy\" transitions that have small absolute TD error while avoiding excessive bias, consider the following pruning-and-sampling algorithm for forming a mini-batch:\n- Compute $|\\delta|$ for each stored transition.\n- Fix a pruning threshold $\\tau > 0$ and a small random keep-rate $r \\in (0, 1)$.\n- Keep every transition with $|\\delta| \\ge \\tau$. For transitions with $|\\delta| < \\tau$, keep each independently with probability $r$.\n- Form a mini-batch of size $m$ by sampling uniformly without importance weights from the kept transitions, and estimate the loss by the empirical mean $\\hat{\\mu} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta_{i}^{2}$.\n\nAssume the following modeling assumptions:\n- The TD error $\\delta$ across the buffer is independent and identically distributed as a Gaussian $\\mathcal{N}(0, \\sigma^{2})$.\n- The buffer is sufficiently large that sampling from the kept set is equivalent to sampling independent draws from the distribution of $\\delta$ conditioned on being kept.\n\nStarting from these definitions and the properties of the normal distribution, derive closed-form expressions for:\n- The expected bias $b(\\tau, r, \\sigma) = \\mathbb{E}[\\hat{\\mu}] - \\mathbb{E}[\\delta^{2}]$.\n- The variance $\\operatorname{Var}(\\hat{\\mu})$ of the mini-batch estimator under the pruning-and-sampling algorithm.\n\nExpress your final answer in terms of the standard normal probability density function $\\phi(x)$ and cumulative distribution function $\\Phi(x)$, introducing the standardized threshold $\\alpha = \\tau / \\sigma$. Provide your final expressions in a single line as a two-entry row matrix $\\begin{pmatrix}\\text{bias} & \\text{variance}\\end{pmatrix}$. No numerical rounding is required.", "solution": "The problem asks for the bias and variance of a mini-batch loss estimator for a Deep Q-Network, where the mini-batch is formed using a specific pruning-and-sampling algorithm based on the Temporal Difference (TD) error, $\\delta$.\n\n### Step 1: Problem Formulation and Definitions\n\nThe TD error, $\\delta$, is assumed to be an independent and identically distributed (i.i.d.) random variable following a Gaussian distribution with mean $0$ and variance $\\sigma^2$, denoted as $\\delta \\sim \\mathcal{N}(0, \\sigma^2)$. The true expected squared TD loss is the second moment of this distribution:\n$$ \\mathbb{E}[\\delta^2] = \\operatorname{Var}(\\delta) + (\\mathbb{E}[\\delta])^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\nThe sampling algorithm for creating a mini-batch is as follows:\n1.  A transition is kept with probability $1$ if its TD error magnitude $|\\delta|$ is greater than or equal to a threshold $\\tau > 0$.\n2.  A transition is kept with probability $r \\in (0, 1)$ if $|\\delta| < \\tau$.\n\nLet $K$ be the event that a transition is kept. The probability of this event, conditioned on $\\delta$, is:\n$$ P(K|\\delta) = \\begin{cases} 1 & \\text{if } |\\delta| \\ge \\tau \\\\ r & \\text{if } |\\delta| < \\tau \\end{cases} $$\nThe mini-batch loss estimator is the empirical mean of the squared TD error for $m$ samples drawn uniformly from the set of kept transitions:\n$$ \\hat{\\mu} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta_i^2 $$\nwhere each $\\delta_i$ is a sample from the distribution of $\\delta$ conditioned on the event $K$. Let us denote a random variable from this conditioned distribution as $\\delta_{kept}$. Due to the i.i.d. assumption for the samples in the mini-batch, the expectation and variance of the estimator $\\hat{\\mu}$ are:\n$$ \\mathbb{E}[\\hat{\\mu}] = \\mathbb{E}[\\delta_{kept}^2] $$\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\operatorname{Var}\\left(\\frac{1}{m} \\sum_{i=1}^{m} \\delta_i^2\\right) = \\frac{1}{m^2} \\sum_{i=1}^{m} \\operatorname{Var}(\\delta_i^2) = \\frac{1}{m} \\operatorname{Var}(\\delta_{kept}^2) = \\frac{1}{m} \\left( \\mathbb{E}[\\delta_{kept}^4] - (\\mathbb{E}[\\delta_{kept}^2])^2 \\right) $$\nOur primary task is to compute the second and fourth moments of $\\delta_{kept}$.\n\n### Step 2: Moments of the \"Kept\" Distribution\n\nThe moments of $\\delta_{kept}$ are found using the law of total expectation. For any function $g(\\delta)$, its expectation under the \"kept\" distribution is:\n$$ \\mathbb{E}[g(\\delta_{kept})] = \\mathbb{E}[g(\\delta) | K] = \\frac{\\mathbb{E}[g(\\delta) \\cdot \\mathbb{I}(K)]}{P(K)} $$\nwhere $\\mathbb{I}(K)$ is the indicator function for event $K$.\n\nFirst, we find the total probability of keeping a sample, $P(K)$. Let $f(\\delta)$ be the probability density function (PDF) of $\\delta \\sim \\mathcal{N}(0, \\sigma^2)$.\n$$ P(K) = \\int_{-\\infty}^{\\infty} P(K|\\delta) f(\\delta) d\\delta = \\int_{|\\delta| \\ge \\tau} 1 \\cdot f(\\delta) d\\delta + \\int_{|\\delta| < \\tau} r \\cdot f(\\delta) d\\delta $$\n$$ P(K) = P(|\\delta| \\ge \\tau) + r \\cdot P(|\\delta| < \\tau) = P(|\\delta| \\ge \\tau) + r(1 - P(|\\delta| \\ge \\tau)) $$\nLet $Z = \\delta/\\sigma$ be a standard normal variable, $Z \\sim \\mathcal{N}(0, 1)$, and let $\\alpha = \\tau/\\sigma$ be the standardized threshold. Let $\\phi(z)$ and $\\Phi(z)$ be the PDF and cumulative distribution function (CDF) of the standard normal distribution, respectively.\n$$ P(|\\delta| \\ge \\tau) = P(|Z| \\ge \\alpha) = P(Z \\le -\\alpha) + P(Z \\ge \\alpha) = \\Phi(-\\alpha) + (1-\\Phi(\\alpha)) $$\nUsing the symmetry property $\\Phi(-\\alpha) = 1 - \\Phi(\\alpha)$, we get:\n$$ P(|\\delta| \\ge \\tau) = 2(1 - \\Phi(\\alpha)) $$\nAnd thus, $P(|\\delta| < \\tau) = 1 - 2(1 - \\Phi(\\alpha)) = 2\\Phi(\\alpha) - 1$.\nSubstituting these into the expression for $P(K)$:\n$$ P(K) = 2(1 - \\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1) $$\nNext, we find the numerator term $\\mathbb{E}[\\delta^k \\cdot \\mathbb{I}(K)]$ for $k=2$ and $k=4$:\n$$ \\mathbb{E}[\\delta^k \\cdot \\mathbb{I}(K)] = \\int_{|\\delta| \\ge \\tau} \\delta^k f(\\delta) d\\delta + r \\int_{|\\delta| < \\tau} \\delta^k f(\\delta) d\\delta $$\nThis can be rewritten as:\n$$ \\mathbb{E}[\\delta^k \\cdot \\mathbb{I}(K)] = (1-r)\\int_{|\\delta| \\ge \\tau} \\delta^k f(\\delta) d\\delta + r \\int_{-\\infty}^{\\infty} \\delta^k f(\\delta) d\\delta = (1-r)\\mathbb{E}[\\delta^k \\mathbb{I}(|\\delta|\\ge\\tau)] + r\\mathbb{E}[\\delta^k] $$\nSince $\\delta^k$ is an even function for $k=2, 4$, the integral over $|\\delta| \\ge \\tau$ is $2 \\int_{\\tau}^{\\infty} \\delta^k f(\\delta) d\\delta$.\n$$ \\mathbb{E}[\\delta^k \\cdot \\mathbb{I}(K)] = (1-r) \\cdot 2 \\int_{\\tau}^{\\infty} \\delta^k f(\\delta) d\\delta + r\\mathbb{E}[\\delta^k] $$\nWe convert the integral to one over the standard normal variable $Z$:\n$$ \\int_{\\tau}^{\\infty} \\delta^k f(\\delta) d\\delta = \\int_{\\alpha}^{\\infty} (\\sigma z)^k \\phi(z/\\sigma) \\frac{1}{\\sigma} d(\\sigma z) = \\sigma^k \\int_{\\alpha}^{\\infty} z^k \\phi(z) dz $$\nThe required integrals $\\int_{\\alpha}^{\\infty} z^k \\phi(z) dz$ for $k=2,4$ can be solved using integration by parts, noting that $\\phi'(z) = -z\\phi(z)$.\nFor $k=2$:\n$$ \\int_{\\alpha}^{\\infty} z^2\\phi(z)dz = \\int_{\\alpha}^{\\infty} z(-z\\phi(z))dz = \\int_{\\alpha}^{\\infty} -z\\phi'(z)dz = [-z\\phi(z)]_{\\alpha}^{\\infty} + \\int_{\\alpha}^{\\infty} \\phi(z)dz = \\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha) $$\nFor $k=4$:\n$$ \\int_{\\alpha}^{\\infty} z^4\\phi(z)dz = \\int_{\\alpha}^{\\infty} -z^3\\phi'(z)dz = [-z^3\\phi(z)]_{\\alpha}^{\\infty} + 3\\int_{\\alpha}^{\\infty} z^2\\phi(z)dz = \\alpha^3\\phi(\\alpha) + 3(\\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha)) $$\nThe full moments of $\\delta$ are $\\mathbb{E}[\\delta^2] = \\sigma^2$ and $\\mathbb{E}[\\delta^4] = 3\\sigma^4$.\nNow we can write the expressions for $\\mathbb{E}[\\delta_{kept}^2]$ and $\\mathbb{E}[\\delta_{kept}^4]$.\n$$ \\mathbb{E}[\\delta_{kept}^2] = \\frac{(1-r) 2\\sigma^2 \\left( \\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha) \\right) + r\\sigma^2}{P(K)} $$\n$$ \\mathbb{E}[\\delta_{kept}^4] = \\frac{(1-r) 2\\sigma^4 \\left( \\alpha^3\\phi(\\alpha) + 3\\alpha\\phi(\\alpha) + 3(1 - \\Phi(\\alpha)) \\right) + 3r\\sigma^4}{P(K)} $$\n\n### Step 3: Derivation of Bias\n\nThe bias is $b = \\mathbb{E}[\\hat{\\mu}] - \\mathbb{E}[\\delta^2] = \\mathbb{E}[\\delta_{kept}^2] - \\sigma^2$.\n$$ b = \\sigma^2 \\left( \\frac{2(1-r)(\\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha)) + r}{2(1 - \\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1)} - 1 \\right) $$\nTo simplify, we find a common denominator:\n$$ b = \\sigma^2 \\frac{2(1-r)(\\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha)) + r - [2(1 - \\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1)]}{2(1 - \\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1)} $$\nThe numerator simplifies as:\n$$ N_b = 2(1-r)\\alpha\\phi(\\alpha) + 2(1-r)(1 - \\Phi(\\alpha)) + r - 2(1 - \\Phi(\\alpha)) - 2r\\Phi(\\alpha) + r $$\n$$ N_b = 2(1-r)\\alpha\\phi(\\alpha) + (1-\\Phi(\\alpha))(2-2r-2) + 2r - 2r\\Phi(\\alpha) $$\n$$ N_b = 2(1-r)\\alpha\\phi(\\alpha) -2r(1-\\Phi(\\alpha)) + 2r(1-\\Phi(\\alpha)) = 2(1-r)\\alpha\\phi(\\alpha) $$\nSo, the bias is:\n$$ b(\\tau, r, \\sigma) = \\frac{2\\sigma^2(1-r)\\alpha\\phi(\\alpha)}{2(1-\\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1)} $$\n\n### Step 4: Derivation of Variance\n\nThe variance is $\\operatorname{Var}(\\hat{\\mu}) = \\frac{1}{m}(\\mathbb{E}[\\delta_{kept}^4] - (\\mathbb{E}[\\delta_{kept}^2])^2)$. We substitute the derived expressions for the moments.\nLet us define the numerators of the moment expressions for clarity:\n$$ N_2 = 2\\sigma^2(1-r)(\\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha)) + r\\sigma^2 $$\n$$ N_4 = 2\\sigma^4(1-r)(\\alpha^3\\phi(\\alpha) + 3\\alpha\\phi(\\alpha) + 3(1 - \\Phi(\\alpha))) + 3r\\sigma^4 $$\nThen $\\mathbb{E}[\\delta_{kept}^2] = N_2/P(K)$ and $\\mathbb{E}[\\delta_{kept}^4] = N_4/P(K)$.\nThe variance is:\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\frac{1}{m} \\left( \\frac{N_4}{P(K)} - \\left( \\frac{N_2}{P(K)} \\right)^2 \\right) = \\frac{N_4 P(K) - N_2^2}{m P(K)^2} $$\nSubstituting the full expressions for $N_2$, $N_4$, and $P(K)$ gives the final form for the variance.\nLet's assemble the result. Let $D = 2(1 - \\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1)$.\nThe bias is $b = \\frac{2\\sigma^2(1-r)\\alpha\\phi(\\alpha)}{D}$.\nThe variance is:\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^4}{m D^2} \\Biggl( \\left[ 2(1-r)(\\alpha^3\\phi(\\alpha) + 3\\alpha\\phi(\\alpha) + 3(1 - \\Phi(\\alpha))) + 3r \\right] D - \\left[ 2(1-r)(\\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha)) + r \\right]^2 \\Biggr) $$\nThis expression is the complete, closed-form solution for the variance of the estimator.", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{2\\sigma^{2}(1-r)\\alpha\\phi(\\alpha)}{2(1-\\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1)} & \\frac{\\sigma^{4}}{m(2(1-\\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1))^{2}} \\Biggl( \\left[ 2(1-r)(\\alpha^{3}\\phi(\\alpha) + 3\\alpha\\phi(\\alpha) + 3(1 - \\Phi(\\alpha))) + 3r \\right] \\left[ 2(1-\\Phi(\\alpha)) + r(2\\Phi(\\alpha) - 1) \\right] - \\left[ 2(1-r)(\\alpha\\phi(\\alpha) + 1 - \\Phi(\\alpha)) + r \\right]^{2} \\Biggr) \\end{pmatrix}} $$", "id": "3113098"}]}