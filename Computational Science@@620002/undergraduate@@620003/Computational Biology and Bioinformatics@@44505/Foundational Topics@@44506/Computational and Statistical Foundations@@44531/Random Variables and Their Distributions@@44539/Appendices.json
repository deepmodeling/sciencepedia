{"hands_on_practices": [{"introduction": "Understanding the distribution of a biological quantity is the first step in building a quantitative model. In genomics, lengths of features like introns are often positive and highly skewed, meaning simple distributions like the normal or exponential are poor fits. This practice [@problem_id:2424252] guides you through characterizing a more appropriate model, the log-normal distribution, and computing key statistical diagnostics to reveal its properties. You will see how metrics like the coefficient of variation and the hazard rate can quantitatively justify why a more sophisticated model is necessary over simpler, memoryless alternatives.", "problem": "You are given a mathematical model for the distribution of intron lengths in a eukaryotic genome. Let the random variable $L$ denote the intron length measured in base pairs. Assume that, for each test case, $L$ follows a log-normal distribution, meaning that $\\ln L$ is normally distributed with parameters $\\mu$ and $\\sigma^{2}$; that is, $\\ln L \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. All quantities below refer to this $L$.\n\nFor each test case with parameters $(\\mu,\\sigma)$, compute the following diagnostics that characterize the distribution of $L$ and assess whether it can be well described by a simple geometric or exponential distribution:\n\n1. The coefficient of variation $c_v$, defined by $c_v = \\sqrt{\\mathrm{Var}(L)}/\\mathbb{E}[L]$.\n2. The upper-tail quantile spread ratio $r$, defined by\n$$\nr = \\frac{Q_{0.99} - Q_{0.5}}{Q_{0.9} - Q_{0.5}},\n$$\nwhere $Q_{q}$ is the $q$-quantile of $L$.\n3. The hazard variability ratio $\\rho$ computed from the continuous hazard function $h(x) = f(x)/(1-F(x))$, where $f$ and $F$ are the probability density function and the cumulative distribution function of $L$, respectively. Evaluate $h(x)$ at the three points $x_{1} = Q_{0.5}$, $x_{2} = Q_{0.75}$, and $x_{3} = Q_{0.9}$, and define\n$$\n\\rho = \\frac{\\max\\{h(x_{1}),h(x_{2}),h(x_{3})\\}}{\\min\\{h(x_{1}),h(x_{2}),h(x_{3})\\}}.\n$$\n\nUse these diagnostics to produce two boolean conclusions for each test case:\n- A boolean flag $\\text{reject\\_exp}$ indicating whether the data are inconsistent with a simple exponential model. Set $\\text{reject\\_exp} = \\text{True}$ if either $|c_v - 1| \\ge 0.05$ or $\\rho \\ge 1.05$; otherwise set it to $\\text{False}$.\n- A boolean flag $\\text{reject\\_geom}$ indicating whether the data are inconsistent with a simple geometric model of intron lengths viewed as a discrete memoryless process. Set $\\text{reject\\_geom} = \\text{True}$ if $\\rho \\ge 1.05$; otherwise set it to $\\text{False}$.\n\nThe test suite of parameter values $(\\mu,\\sigma)$ to be used is:\n- Test case A: $(\\mu,\\sigma) = (7.3, 1.1)$.\n- Test case B: $(\\mu,\\sigma) = (7.3, 0.3)$.\n- Test case C: $(\\mu,\\sigma) = (6.5, 1.8)$.\n\nYour program must compute, for each test case, the list\n$$\n[c_v,\\; r,\\; \\rho,\\; \\text{reject\\_exp},\\; \\text{reject\\_geom}],\n$$\nwhere the three real-valued entries $c_v$, $r$, and $\\rho$ must be rounded to exactly $6$ decimal places. The two boolean entries must be given as the words True or False.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of three inner lists in the same order as the test suite, with no spaces anywhere. For example, the output must look like\n$$\n[[c\\_v^{(A)},r^{(A)},\\rho^{(A)},\\text{reject\\_exp}^{(A)},\\text{reject\\_geom}^{(A)}],[c\\_v^{(B)},r^{(B)},\\rho^{(B)},\\text{reject\\_exp}^{(B)},\\text{reject\\_geom}^{(B)}],[c\\_v^{(C)},r^{(C)},\\rho^{(C)},\\text{reject\\_exp}^{(C)},\\text{reject\\_geom}^{(C)}]]\n$$\nwith all real values rounded to $6$ decimal places and with the booleans unquoted.\n\nNotes:\n- Angles are not involved; no angle unit is required.\n- No physical unit needs to be printed; all computations are dimensionless functions of the specified parameters.\n- Answers must be numeric (floats) and booleans as described; do not print any other text.", "solution": "We model intron length as a positive random variable $L$ with $\\ln L \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. From the definition of the log-normal distribution, the following are standard consequences of the properties of the exponential and normal functions.\n\n1. Expectation and variance. Since $\\ln L \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, by the moment-generating properties of the normal distribution and the monotonicity of the exponential, we obtain\n$$\n\\mathbb{E}[L] = \\exp\\!\\left(\\mu + \\frac{\\sigma^{2}}{2}\\right),\n\\quad\n\\mathrm{Var}(L) = \\left(\\exp(\\sigma^{2}) - 1\\right)\\exp\\!\\left(2\\mu + \\sigma^{2}\\right).\n$$\nTherefore, the coefficient of variation simplifies to\n$$\nc_v = \\frac{\\sqrt{\\mathrm{Var}(L)}}{\\mathbb{E}[L]} = \\sqrt{\\exp(\\sigma^{2}) - 1},\n$$\nwhich depends only on $\\sigma$.\n\n2. Quantiles. If $Z \\sim \\mathcal{N}(0,1)$ has cumulative distribution function $\\Phi$, then $L = \\exp(\\mu + \\sigma Z)$, so the $q$-quantile of $L$ is\n$$\nQ_q = \\exp\\!\\left(\\mu + \\sigma \\Phi^{-1}(q)\\right).\n$$\nIn particular, for $q \\in \\{0.5,0.75,0.9,0.99\\}$, we have $Q_q = \\exp(\\mu + \\sigma z_q)$ where $z_q = \\Phi^{-1}(q)$. Using this, the ratio\n$$\nr = \\frac{Q_{0.99} - Q_{0.5}}{Q_{0.9} - Q_{0.5}} = \\frac{\\exp(\\sigma z_{0.99}) - 1}{\\exp(\\sigma z_{0.9}) - 1}\n$$\nis independent of $\\mu$.\n\n3. Hazard function and its variability. The probability density function and cumulative distribution function of $L$ are\n$$\nf(x) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(\\ln x - \\mu)^{2}}{2\\sigma^{2}}\\right), \\quad F(x) = \\Phi\\!\\left(\\frac{\\ln x - \\mu}{\\sigma}\\right), \\quad x > 0.\n$$\nThe hazard function is defined by $h(x) = f(x)/(1-F(x))$. Evaluated at quantiles $x = Q_q = \\exp(\\mu + \\sigma z_q)$, we have $\\ln x = \\mu + \\sigma z_q$, so\n$$\nh(Q_q) = \\frac{1}{Q_q \\sigma \\sqrt{2\\pi}} \\cdot \\frac{\\exp(-z_q^{2}/2)}{1 - \\Phi(z_q)}.\n$$\nConsequently, the ratio $\\rho = \\max\\{h(Q_{0.5}),h(Q_{0.75}),h(Q_{0.9})\\}/\\min\\{\\cdot\\}$ does not depend on $\\mu$ because each $h(Q_q)$ carries a common factor of $\\exp(-\\mu)$ via $Q_q$ in the denominator that cancels when taking ratios. For any $\\sigma > 0$, the log-normal hazard is not constant in $x$, hence $\\rho > 1$.\n\nDecision logic. An exponential distribution has constant hazard and coefficient of variation equal to $1$. A geometric distribution is the discrete memoryless analog and also has constant hazard on its support. Therefore, the criteria $|c_v - 1| \\ge 0.05$ or $\\rho \\ge 1.05$ provide quantitative tests against these simple memoryless models. Specifically, we set\n- $\\text{reject\\_exp} = \\text{True}$ if $|c_v - 1| \\ge 0.05$ or $\\rho \\ge 1.05$, otherwise $\\text{False}$.\n- $\\text{reject\\_geom} = \\text{True}$ if $\\rho \\ge 1.05$, otherwise $\\text{False}$.\n\nFor the provided test suite:\n- Test case A: $(\\mu,\\sigma) = (7.3, 1.1)$.\n- Test case B: $(\\mu,\\sigma) = (7.3, 0.3)$.\n- Test case C: $(\\mu,\\sigma) = (6.5, 1.8)$.\n\nWe compute\n$$\nc_v = \\sqrt{\\exp(\\sigma^{2}) - 1},\n$$\n$$\nr = \\frac{\\exp(\\sigma z_{0.99}) - 1}{\\exp(\\sigma z_{0.9}) - 1}, \\quad z_{0.9} = \\Phi^{-1}(0.9), \\; z_{0.99} = \\Phi^{-1}(0.99),\n$$\nand\n$$\n\\rho = \\frac{\\max\\{h(Q_{0.5}), h(Q_{0.75}), h(Q_{0.9})\\}}{\\min\\{h(Q_{0.5}), h(Q_{0.75}), h(Q_{0.9})\\}},\n$$\nwhere $h(Q_q)$ is as above with $q \\in \\{0.5, 0.75, 0.9\\}$.\n\nQualitatively, for $\\sigma = 1.1$ and $\\sigma = 1.8$, we have $c_v > 1$ and non-constant hazard, strongly rejecting the exponential and geometric models. Even for the milder case $\\sigma = 0.3$, we have $c_v \\ne 1$ and a non-constant hazard across quantiles, again rejecting exponential and geometric models. The final program evaluates these quantities exactly (within floating-point precision), rounds real-valued outputs to $6$ decimal places, and applies the stated decision rules to produce the requested single-line output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import exp, sqrt\nfrom scipy.stats import norm\n\ndef lognormal_quantile(mu, sigma, q):\n    z = norm.ppf(q)\n    return np.exp(mu + sigma * z)\n\ndef lognormal_pdf(x, mu, sigma):\n    # f(x) for log-normal with parameters (mu, sigma)\n    # x > 0\n    return (1.0 / (x * sigma * np.sqrt(2.0 * np.pi))) * np.exp(-((np.log(x) - mu) ** 2) / (2.0 * sigma * sigma))\n\ndef lognormal_cdf(x, mu, sigma):\n    # F(x) for log-normal with parameters (mu, sigma)\n    z = (np.log(x) - mu) / sigma\n    return norm.cdf(z)\n\ndef lognormal_hazard(x, mu, sigma):\n    F = lognormal_cdf(x, mu, sigma)\n    S = 1.0 - F\n    f = lognormal_pdf(x, mu, sigma)\n    return f / S\n\ndef compute_case(mu, sigma, tol_cv=0.05, tol_hazard=1.05):\n    # Coefficient of variation for log-normal\n    cv = sqrt(exp(sigma * sigma) - 1.0)\n\n    # Quantiles\n    q50 = lognormal_quantile(mu, sigma, 0.5)\n    q75 = lognormal_quantile(mu, sigma, 0.75)\n    q90 = lognormal_quantile(mu, sigma, 0.9)\n    q99 = lognormal_quantile(mu, sigma, 0.99)\n\n    # Tail spread ratio r\n    r = (q99 - q50) / (q90 - q50)\n\n    # Hazard variability ratio rho using h at Q0.5, Q0.75, Q0.9\n    h50 = lognormal_hazard(q50, mu, sigma)\n    h75 = lognormal_hazard(q75, mu, sigma)\n    h90 = lognormal_hazard(q90, mu, sigma)\n    h_vals = [h50, h75, h90]\n    rho = max(h_vals) / min(h_vals)\n\n    # Decisions\n    reject_exp = (abs(cv - 1.0) >= tol_cv) or (rho >= tol_hazard)\n    reject_geom = (rho >= tol_hazard)\n\n    # Round floats to 6 decimals as required\n    cv_r = round(cv, 6)\n    r_r = round(float(r), 6)\n    rho_r = round(float(rho), 6)\n\n    return [cv_r, r_r, rho_r, reject_exp, reject_geom]\n\ndef serialize_no_spaces(obj):\n    # Serialize lists of floats/bools with no spaces and floats to 6 decimals\n    if isinstance(obj, list):\n        return '[' + ','.join(serialize_no_spaces(x) for x in obj) + ']'\n    elif isinstance(obj, bool):\n        return 'True' if obj else 'False'\n    elif isinstance(obj, float):\n        # Ensure exactly 6 decimal places\n        return f\"{obj:.6f}\"\n    elif isinstance(obj, (int, np.integer)):\n        return str(int(obj))\n    else:\n        # Fallback to str (should not happen for specified types)\n        return str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each is a tuple (mu, sigma)\n    test_cases = [\n        (7.3, 1.1),   # Case A\n        (7.3, 0.3),   # Case B\n        (6.5, 1.8),   # Case C\n    ]\n\n    results = []\n    for mu, sigma in test_cases:\n        result = compute_case(mu, sigma, tol_cv=0.05, tol_hazard=1.05)\n        results.append(result)\n\n    # Final print statement in the exact required format: no spaces anywhere.\n    print(serialize_no_spaces(results))\n\nsolve()\n```", "id": "2424252"}, {"introduction": "Often, more than one mathematical model can plausibly describe a biological process, and we need a principled way to choose the best one. This exercise [@problem_id:2424264] places you in the role of a computational neuroscientist modeling a neuron's firing pattern, where you will compare the memoryless Exponential distribution against the more flexible Gamma distribution. By applying the Akaike Information Criterion (AIC), you will learn a fundamental technique for balancing model complexity and goodness-of-fit—a crucial skill in all data-driven scientific fields.", "problem": "You are given a modeling task inspired by spike train analysis in computational neurobiology. The inter-spike interval (ISI) of a spontaneously firing neuron is the elapsed time between consecutive action potentials and can be treated as a positive-valued random variable. You will implement a program that, for several synthetic ISI datasets generated under known mechanisms, selects the best-fitting distribution among a candidate set by principled statistical model selection.\n\nUse the following foundational base:\n- The independent and identically distributed (i.i.d.) assumption: observed ISIs $\\{x_i\\}_{i=1}^{n}$ are modeled as i.i.d. samples from a parametric family.\n- The Exponential distribution with rate parameter $\\lambda$: probability density function $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$ for $x > 0$.\n- The Gamma distribution with shape parameter $k$ and scale parameter $\\theta$: probability density function $f(x \\mid k,\\theta) = \\dfrac{x^{k-1} e^{-x/\\theta}}{\\Gamma(k)\\,\\theta^{k}}$ for $x > 0$.\n- Maximum likelihood estimation (MLE): for an i.i.d. sample, the likelihood is $L(\\boldsymbol{\\theta}\\,;\\,\\mathbf{x}) = \\prod_{i=1}^{n} f(x_i \\mid \\boldsymbol{\\theta})$, and the log-likelihood is $\\ell(\\boldsymbol{\\theta}\\,;\\,\\mathbf{x}) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\boldsymbol{\\theta})$. The MLE $\\widehat{\\boldsymbol{\\theta}}$ maximizes $\\ell(\\boldsymbol{\\theta}\\,;\\,\\mathbf{x})$ over admissible parameters.\n- Akaike Information Criterion (AIC): for a model with $p$ free parameters and maximized log-likelihood $\\ell(\\widehat{\\boldsymbol{\\theta}}\\,;\\,\\mathbf{x})$, the model score is $\\mathrm{AIC} = 2p - 2\\,\\ell(\\widehat{\\boldsymbol{\\theta}}\\,;\\,\\mathbf{x})$. Prefer the model with the smaller AIC.\n\nYour program must:\n1. For each provided test case, simulate an ISI dataset under the given generating mechanism using the specified random seed, assuming times are in seconds.\n2. Fit the candidate models Exponential and Gamma by maximum likelihood. For Exponential, constrain $\\lambda > 0$. For Gamma, constrain $k > 0$ and $\\theta > 0$, and use the unshifted form with support on $(0,\\infty)$.\n3. Compute each model’s AIC and select the model with the smaller AIC.\n4. Encode the selected model as an integer: Exponential $\\rightarrow 0$, Gamma $\\rightarrow 1$.\n\nPhysical units: ISIs are measured in seconds. No unit conversion is required because the outputs are model identifiers.\n\nAngle units: Not applicable.\n\nPercentages: Not applicable.\n\nTest suite (use a single fixed seed for reproducibility):\n- Use random seed $20231119$ for all simulations.\n- Case $1$ (happy path, memoryless firing): Exponential with rate $\\lambda = 8\\,\\mathrm{s}^{-1}$, sample size $n = 2000$.\n- Case $2$ (regularized firing, sub-Poisson variability): Gamma with shape $k = 3$ and scale $\\theta = 0.04\\,\\mathrm{s}$, sample size $n = 2000$.\n- Case $3$ (clustered/bursty firing, super-Poisson variability): Gamma with shape $k = 0.5$ and scale $\\theta = 0.2\\,\\mathrm{s}$, sample size $n = 2000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases $1,2,3$. Each entry must be the integer code of the selected model for that case. For example, a valid output looks like: \"[0,1,1]\".", "solution": "The task is to perform model selection between an Exponential and a Gamma distribution for three synthetic datasets. This requires simulating the data, performing Maximum Likelihood Estimation (MLE) for the parameters of each candidate model, and then using the Akaike Information Criterion (AIC) to select the better model.\n\n#### Data Simulation\nData must be generated according to the specifications for each test case. A single random number generator, initialized with the seed $20231119$, will be used for reproducibility.\n- **Case 1 (Exponential):** The Exponential distribution with rate $\\lambda$ has a mean of $1/\\lambda$. Many standard library functions, such as `numpy.random.Generator.exponential`, are parameterized by the scale parameter $\\beta$, which is equal to the mean. Thus, for $\\lambda = 8\\,\\mathrm{s}^{-1}$, the scale is $\\beta = 1/8\\,\\mathrm{s} = 0.125\\,\\mathrm{s}$. We will generate $n = 2000$ samples using this scale.\n- **Cases 2 & 3 (Gamma):** The Gamma distribution is specified with shape $k$ and scale $\\theta$. The `numpy.random.Generator.gamma` function is directly parameterized by `shape` and `scale`, so we can use the provided parameters $k$ and $\\theta$ without transformation to generate $n=2000$ samples for each case.\n\n#### Maximum Likelihood Estimation (MLE) and Log-Likelihood Calculation\n\n**1. Exponential Model**\nThe PDF is $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$. For a dataset $\\mathbf{x} = \\{x_1, \\dots, x_n\\}$, the log-likelihood function is:\n$$ \\ell(\\lambda; \\mathbf{x}) = \\sum_{i=1}^n \\log(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n (\\log \\lambda - \\lambda x_i) = n \\log \\lambda - \\lambda \\sum_{i=1}^n x_i $$\nTo find the MLE $\\widehat{\\lambda}$, we differentiate with respect to $\\lambda$ and set the result to zero:\n$$ \\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0 $$\nThis yields the well-known MLE for the rate parameter:\n$$ \\widehat{\\lambda}_{\\text{MLE}} = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}} $$\nwhere $\\bar{x}$ is the sample mean. After computing $\\widehat{\\lambda}_{\\text{MLE}}$, the maximized log-likelihood is $\\ell_{\\text{max, exp}} = \\ell(\\widehat{\\lambda}_{\\text{MLE}}; \\mathbf{x})$.\n\n**2. Gamma Model**\nThe PDF is $f(x \\mid k, \\theta) = \\frac{x^{k-1} e^{-x/\\theta}}{\\Gamma(k) \\theta^k}$. The log-likelihood function for a dataset $\\mathbf{x}$ is:\n$$ \\ell(k, \\theta; \\mathbf{x}) = \\sum_{i=1}^n \\left[ (k-1)\\log x_i - \\frac{x_i}{\\theta} - \\log \\Gamma(k) - k \\log \\theta \\right] $$\nUnlike the Exponential case, there is no closed-form solution for the MLEs $\\widehat{k}_{\\text{MLE}}$ and $\\widehat{\\theta}_{\\text{MLE}}$. These must be found numerically by maximizing $\\ell(k, \\theta; \\mathbf{x})$. This is equivalent to minimizing the negative log-likelihood $-\\ell(k, \\theta; \\mathbf{x})$. We will use the `scipy.stats.gamma.fit` function, which is a specialized tool for this purpose. The problem specifies an unshifted Gamma distribution on $(0, \\infty)$, which corresponds to a location parameter of $0$. We will enforce this by using the `floc=0` argument in the fitting function. This function will return the estimated shape $\\widehat{k}_{\\text{MLE}}$ and scale $\\widehat{\\theta}_{\\text{MLE}}$. The maximized log-likelihood $\\ell_{\\text{max, gamma}}$ is then computed by summing the log-PDF values using these estimated parameters.\n\n#### Akaike Information Criterion (AIC)\nThe AIC for a model is given by $\\mathrm{AIC} = 2p - 2\\ell_{\\text{max}}$, where $p$ is the number of estimated parameters.\n- **For the Exponential model:** We estimate one parameter, $\\lambda$, so $p_{\\text{exp}} = 1$.\n$$ \\mathrm{AIC}_{\\text{exp}} = 2(1) - 2\\ell_{\\text{max, exp}} $$\n- **For the Gamma model:** We estimate two parameters, $k$ and $\\theta$, so $p_{\\text{gamma}} = 2$.\n$$ \\mathrm{AIC}_{\\text{gamma}} = 2(2) - 2\\ell_{\\text{max, gamma}} $$\n\n#### Model Selection\nFor each dataset, we will compute $\\mathrm{AIC}_{\\text{exp}}$ and $\\mathrm{AIC}_{\\text{gamma}}$. The model with the smaller AIC value is chosen as the better fit for the data. The chosen model is then encoded as an integer ($0$ for Exponential, $1$ for Gamma). This procedure is repeated for all three test cases.\n- **Case 1:** Data is generated from an Exponential distribution. We expect the Exponential model to have a lower AIC.\n- **Cases 2 & 3:** Data is generated from Gamma distributions. The Gamma model is more general than the Exponential (the Exponential is a special case of the Gamma with $k=1$). Thus, we expect the Gamma model to provide a significantly better fit and have a lower AIC.\n\nThe implementation will follow these steps precisely.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import expon, gamma\n\ndef solve():\n    \"\"\"\n    Simulates inter-spike interval data, fits Exponential and Gamma models,\n    and selects the best model using AIC.\n    \"\"\"\n    # Define the random seed and test cases from the problem statement.\n    seed = 20231119\n    test_cases = [\n        {'type': 'exponential', 'params': {'rate': 8.0}, 'n': 2000},\n        {'type': 'gamma', 'params': {'shape': 3.0, 'scale': 0.04}, 'n': 2000},\n        {'type': 'gamma', 'params': {'shape': 0.5, 'scale': 0.2}, 'n': 2000},\n    ]\n\n    # Initialize the random number generator for reproducibility.\n    # The numpy Generator provides modern, preferred random number generation.\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for case in test_cases:\n        # Step 1: Simulate the ISI dataset\n        n = case['n']\n        if case['type'] == 'exponential':\n            # numpy.random.exponential is parameterized by scale = 1/rate\n            rate = case['params']['rate']\n            scale = 1.0 / rate\n            data = rng.exponential(scale=scale, size=n)\n        elif case['type'] == 'gamma':\n            # numpy.random.gamma is parameterized by shape (k) and scale (theta)\n            shape = case['params']['shape']\n            scale = case['params']['scale']\n            data = rng.gamma(shape=shape, scale=scale, size=n)\n        \n        # Ensure all data points are positive, as required by the distributions.\n        # This is a safeguard; generated values should already be positive.\n        data = data[data > 0]\n\n        # Step 2: Fit candidate models by Maximum Likelihood Estimation (MLE)\n\n        # --- Exponential Model ---\n        # The MLE for the rate (lambda) is the reciprocal of the sample mean.\n        # The scale parameter (beta) used by scipy is 1/lambda.\n        lambda_mle = 1.0 / np.mean(data)\n        \n        # Calculate the maximized log-likelihood.\n        # scipy.stats.expon uses scale = 1/lambda.\n        loglik_exp = np.sum(expon.logpdf(data, scale=1.0/lambda_mle))\n        \n        # Number of parameters for the Exponential model is 1 (lambda).\n        p_exp = 1\n        \n        # Compute AIC.\n        aic_exp = 2 * p_exp - 2 * loglik_exp\n\n        # --- Gamma Model ---\n        # The MLE for Gamma parameters (k, theta) has no closed-form solution.\n        # We use scipy.stats.gamma.fit for numerical MLE.\n        # The problem states an unshifted Gamma, so we fix location (loc) to 0.\n        # The fit method returns (shape, location, scale).\n        k_mle, _, theta_mle = gamma.fit(data, floc=0)\n        \n        # Calculate the maximized log-likelihood using the estimated parameters.\n        # scipy.stats.gamma uses shape 'a' and 'scale'.\n        loglik_gamma = np.sum(gamma.logpdf(data, a=k_mle, scale=theta_mle, loc=0))\n        \n        # Number of parameters for the Gamma model is 2 (k, theta).\n        p_gamma = 2\n        \n        # Compute AIC.\n        aic_gamma = 2 * p_gamma - 2 * loglik_gamma\n\n        # Step 3: Select model with the smaller AIC and encode the result\n        # Exponential -> 0, Gamma -> 1\n        if aic_exp  aic_gamma:\n            selected_model = 0\n        else:\n            selected_model = 1\n        \n        results.append(selected_model)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2424264"}, {"introduction": "Biological samples are frequently heterogeneous, containing a mixture of different populations, such as distinct cell types that respond differently to a treatment. A single probability distribution cannot capture this complexity, as the data will be multi-modal. This practice [@problem_id:2424270] introduces a powerful solution: the Gaussian Mixture Model (GMM), which describes data as a weighted sum of multiple Gaussian \"sub-distributions\". By working with a classic bioinformatics scenario of flow cytometry data, you will learn how to calculate the likelihood of data under a given GMM and assign individual data points to their most likely source population, a core task in unsupervised machine learning.", "problem": "A population of cells measured by flow cytometry exhibits heterogeneous fluorescence signals due to two distinct cell types. Let the base-$10$ logarithm of single-cell fluorescence intensity be a real-valued random variable $I \\in \\mathbb{R}$. Assume that $I$ arises from a two-component Gaussian Mixture Model (GMM), where component $j \\in \\{1,2\\}$ corresponds to a biological cell type with Gaussian distribution of $I$ characterized by mean $\\mu_j \\in \\mathbb{R}$ and standard deviation $\\sigma_j \\in \\mathbb{R}_{0}$, and where the mixing proportions are $\\pi_1 \\in (0,1)$ and $\\pi_2 = 1 - \\pi_1$. Observations are independent and identically distributed draws from this mixture.\n\nFor each test case defined below, you are given the mixture parameters $(\\pi_1,\\mu_1,\\sigma_1,\\mu_2,\\sigma_2)$ and a finite list of observed log-intensities $I_1,\\dots,I_n$. Using only first principles and natural logarithms, compute the following for each test case:\n- The total sample log-likelihood $L = \\sum_{i=1}^{n} \\log\\left(\\pi_1 \\, \\phi(I_i \\mid \\mu_1,\\sigma_1^2) + (1-\\pi_1)\\, \\phi(I_i \\mid \\mu_2,\\sigma_2^2)\\right)$, where $\\phi(\\cdot \\mid \\mu,\\sigma^2)$ denotes the Gaussian probability density function with mean $\\mu$ and variance $\\sigma^2$.\n- The maximum a posteriori classification count $C$, defined as the number of observations $I_i$ whose posterior probability of belonging to component $1$ is at least $0.5$ under the specified model.\n\nReport $L$ rounded to $6$ decimal places and $C$ as an integer.\n\nTest suite (parameters followed by the observation list for each case):\n- Case $1$: $(\\pi_1,\\mu_1,\\sigma_1,\\mu_2,\\sigma_2) = (0.55,\\, 1.10,\\, 0.20,\\, 3.00,\\, 0.35)$; observations $[0.90,\\, 1.05,\\, 1.25,\\, 2.85,\\, 2.95,\\, 3.15]$.\n- Case $2$: $(\\pi_1,\\mu_1,\\sigma_1,\\mu_2,\\sigma_2) = (0.50,\\, 2.00,\\, 0.40,\\, 2.40,\\, 0.50)$; observations $[1.50,\\, 2.10,\\, 2.30,\\, 2.70,\\, 3.00]$.\n- Case $3$: $(\\pi_1,\\mu_1,\\sigma_1,\\mu_2,\\sigma_2) = (0.20,\\, 2.50,\\, 0.15,\\, 2.50,\\, 0.60)$; observations $[2.50,\\, 2.55,\\, 1.50,\\, 3.70,\\, 2.30,\\, 2.70]$.\n\nYour program must compute $(L,C)$ for each case and produce a single line of output containing all results in a flat list, in the order case $1$ then case $2$ then case $3$, formatted as $[L_1, C_1, L_2, C_2, L_3, C_3]$, where each $L_k$ is rounded to $6$ decimal places and each $C_k$ is an integer. For example, a valid schematic format is $[r_1, r_2, r_3, r_4, r_5, r_6]$ with $r_1 = L_1$, $r_2 = C_1$, etc. No units are required, and all logarithms must be natural logarithms.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard task in statistical modeling and computational biology: the evaluation of a Gaussian Mixture Model (GMM) given a set of parameters and observations. The quantities to be computed—the log-likelihood and a classification count based on posterior probabilities—are defined with mathematical precision. All necessary data are provided. Therefore, the problem is valid, and we proceed with the solution.\n\nThe problem requires the calculation of two quantities for a given two-component GMM with parameters $\\Theta = (\\pi_1, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2)$ and a set of $n$ observations $\\{I_i\\}_{i=1}^n$.\n\nFirst, we define the Gaussian probability density function (PDF). For a random variable $x \\in \\mathbb{R}$, the PDF of a normal distribution with mean $\\mu$ and variance $\\sigma^2$ is given by:\n$$\n\\phi(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n$$\nThe problem provides the standard deviations $\\sigma_1$ and $\\sigma_2$.\n\nThe GMM describes the probability density of an observation $I_i$ as a weighted sum of the two component densities:\n$$\np(I_i \\mid \\Theta) = \\pi_1 \\, \\phi(I_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\, \\phi(I_i \\mid \\mu_2, \\sigma_2^2)\n$$\nwhere the mixing proportions are $\\pi_1 \\in (0,1)$ and $\\pi_2 = 1 - \\pi_1$.\n\nThe first required quantity is the total sample log-likelihood, $L$. Since the observations $I_1, \\dots, I_n$ are independent and identically distributed, the total likelihood of the sample is the product of the individual likelihoods, $P(\\{I_i\\} \\mid \\Theta) = \\prod_{i=1}^n p(I_i \\mid \\Theta)$. The total sample log-likelihood is the natural logarithm of this product, which simplifies to the sum of the individual log-likelihoods:\n$$\nL = \\log\\left(\\prod_{i=1}^n p(I_i \\mid \\Theta)\\right) = \\sum_{i=1}^n \\log(p(I_i \\mid \\Theta))\n$$\nSubstituting the expression for $p(I_i \\mid \\Theta)$, we get the formula for computation:\n$$\nL = \\sum_{i=1}^{n} \\log\\left(\\pi_1 \\, \\phi(I_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi_1)\\, \\phi(I_i \\mid \\mu_2, \\sigma_2^2)\\right)\n$$\nThis value must be computed for each test case and rounded to $6$ decimal places.\n\nThe second required quantity is the maximum a posteriori (MAP) classification count, $C$. This requires us to assign each observation $I_i$ to one of the two components based on its posterior probability. Let $Z_i \\in \\{1, 2\\}$ be a latent variable representing the component from which observation $I_i$ originates. The posterior probability of $I_i$ belonging to component $1$, given the observation and model parameters, is found using Bayes' theorem:\n$$\nP(Z_i=1 \\mid I_i, \\Theta) = \\frac{p(I_i \\mid Z_i=1, \\Theta) \\, P(Z_i=1 \\mid \\Theta)}{p(I_i \\mid \\Theta)}\n$$\nHere, the terms are:\n-   The prior probability, $P(Z_i=1 \\mid \\Theta) = \\pi_1$.\n-   The component-conditional likelihood, $p(I_i \\mid Z_i=1, \\Theta) = \\phi(I_i \\mid \\mu_1, \\sigma_1^2)$.\n-   The evidence or marginal likelihood, $p(I_i \\mid \\Theta)$, which is the GMM PDF defined previously.\n\nThus, the posterior probability for component $1$ is:\n$$\n\\gamma(Z_i=1) \\equiv P(Z_i=1 \\mid I_i, \\Theta) = \\frac{\\pi_1 \\, \\phi(I_i \\mid \\mu_1, \\sigma_1^2)}{\\pi_1 \\, \\phi(I_i \\mid \\mu_1, \\sigma_1^2) + (1 - \\pi_1) \\, \\phi(I_i \\mid \\mu_2, \\sigma_2^2)}\n$$\nThe problem defines the MAP classification rule: an observation $I_i$ is assigned to component $1$ if this posterior probability is at least $0.5$.\nThe count $C$ is the total number of observations that satisfy this condition:\n$$\nC = \\sum_{i=1}^{n} \\mathbb{I}(\\gamma(Z_i=1) \\ge 0.5)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\nThe computational procedure for each test case is as follows:\n$1$. Initialize the total log-likelihood $L = 0$ and the count $C = 0$.\n$2$. For each observation $I_i$ in the provided list:\n    a. Calculate the density of $I_i$ under component $1$, $d_1 = \\phi(I_i \\mid \\mu_1, \\sigma_1^2)$.\n    b. Calculate the density of $I_i$ under component $2$, $d_2 = \\phi(I_i \\mid \\mu_2, \\sigma_2^2)$.\n    c. Compute the weighted density for component $1$, $w_1 = \\pi_1 d_1$.\n    d. Compute the weighted density for component $2$, $w_2 = (1-\\pi_1) d_2$.\n    e. The marginal likelihood for $I_i$ is $p_i = w_1 + w_2$.\n    f. Add $\\log(p_i)$ to the total log-likelihood $L$.\n    g. Calculate the posterior probability for component $1$: $\\gamma(Z_i=1) = w_1 / p_i$.\n    h. If $\\gamma(Z_i=1) \\ge 0.5$, increment the count $C$.\n$3$. After iterating through all observations, round the final value of $L$ to $6$ decimal places. The value of $C$ is an integer.\n$4$. Store the pair $(L, C)$.\n\nThis procedure is to be repeated for all three test cases, and the results are to be compiled into a single flat list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian Mixture Model problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": (0.55, 1.10, 0.20, 3.00, 0.35),\n            \"observations\": [0.90, 1.05, 1.25, 2.85, 2.95, 3.15]\n        },\n        {\n            \"params\": (0.50, 2.00, 0.40, 2.40, 0.50),\n            \"observations\": [1.50, 2.10, 2.30, 2.70, 3.00]\n        },\n        {\n            \"params\": (0.20, 2.50, 0.15, 2.50, 0.60),\n            \"observations\": [2.50, 2.55, 1.50, 3.70, 2.30, 2.70]\n        }\n    ]\n\n    results = []\n\n    def gaussian_pdf(x, mu, sigma):\n        \"\"\"\n        Calculates the probability density function of a Gaussian distribution.\n        Uses first principles as required.\n        \"\"\"\n        if sigma = 0:\n            raise ValueError(\"Standard deviation must be positive.\")\n        variance = sigma**2\n        coeff = 1.0 / np.sqrt(2.0 * np.pi * variance)\n        exponent = -((x - mu)**2) / (2.0 * variance)\n        return coeff * np.exp(exponent)\n\n    for case in test_cases:\n        pi1, mu1, sigma1, mu2, sigma2 = case[\"params\"]\n        observations = case[\"observations\"]\n        \n        pi2 = 1.0 - pi1\n        total_log_likelihood = 0.0\n        map_count_c1 = 0\n\n        for I_i in observations:\n            # Calculate the PDF values for each component\n            pdf1 = gaussian_pdf(I_i, mu1, sigma1)\n            pdf2 = gaussian_pdf(I_i, mu2, sigma2)\n\n            # Calculate weighted densities\n            weighted_pdf1 = pi1 * pdf1\n            weighted_pdf2 = pi2 * pdf2\n\n            # Calculate marginal likelihood for the observation I_i\n            p_i = weighted_pdf1 + weighted_pdf2\n\n            # Update total log-likelihood, using natural logarithm\n            if p_i > 0:\n                total_log_likelihood += np.log(p_i)\n\n            # Calculate posterior probability for component 1 (responsibility)\n            # and perform MAP classification\n            if p_i > 0:\n                posterior1 = weighted_pdf1 / p_i\n                if posterior1 >= 0.5:\n                    map_count_c1 += 1\n        \n        # Round the total log-likelihood to 6 decimal places\n        L = round(total_log_likelihood, 6)\n        # C is the integer count\n        C = map_count_c1\n        \n        results.append(L)\n        results.append(C)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2424270"}]}