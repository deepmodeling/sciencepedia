{"hands_on_practices": [{"introduction": "In computational biology, the Poisson distribution is a fundamental tool for modeling count-based data, such as read counts in an RNA-seq experiment. This exercise explores the direct relationship between the parameters of the Poisson distribution—its expected value and variance—and a key experimental variable: sequencing depth. By working through this problem [@problem_id:2389172], you will gain an intuitive understanding of how experimental design choices directly impact the statistical properties of your data.", "problem": "In a differential gene expression study using RNA sequencing (RNA-seq), assume that for a particular gene under fixed experimental conditions and a fixed sequencing depth, the observed read count $X$ follows a Poisson distribution with parameter $\\lambda$, that is $X \\sim \\operatorname{Pois}(\\lambda)$. The laboratory then repeats the assay at the same conditions but sequences to $3$ times the original depth and pools all reads into a single library. Under the standard assumption that read-generation events are independent and that the expected count for a gene scales linearly with sequencing depth, determine the new expected value and variance of the pooled read count for this gene.\n\nReport your answer as a row vector $\\left(\\mathbb{E}[X'], \\operatorname{Var}(X')\\right)$ expressed in terms of $\\lambda$. No numerical approximation is required.", "solution": "The problem statement must first be validated for scientific and logical consistency. The premises are:\n1. The initial read count, a random variable $X$, follows a Poisson distribution with parameter $\\lambda$. This is written as $X \\sim \\operatorname{Pois}(\\lambda)$.\n2. The expected read count for a gene scales linearly with sequencing depth.\n3. A new experiment is conducted where the sequencing depth is $3$ times the original depth.\n\nThese premises are standard and well-established in the field of computational biology for analyzing RNA-seq data. The model is scientifically grounded and the problem is well-posed. We may proceed with the solution.\n\nLet the original sequencing depth be denoted by $D$. The initial read count is the random variable $X$, and it is given that $X \\sim \\operatorname{Pois}(\\lambda)$. The fundamental properties of the Poisson distribution state that its expected value and variance are both equal to its parameter. Therefore, for the initial experiment, we have:\n$$\n\\mathbb{E}[X] = \\lambda\n$$\n$$\n\\operatorname{Var}(X) = \\lambda\n$$\n\nThe problem states that the expected count is directly proportional to the sequencing depth. We can express this relationship mathematically with a constant of proportionality, $k$, which depends on the specific gene and fixed experimental conditions.\n$$\n\\mathbb{E}[X] = kD\n$$\nFrom this, we can equate the two expressions for the expected value to relate the Poisson parameter $\\lambda$ to the sequencing depth $D$:\n$$\n\\lambda = kD\n$$\n\nNext, a new experiment is described. Let $X'$ be the random variable representing the read count from this new experiment. The new sequencing depth is $3$ times the original depth, which we denote as $D' = 3D$. The problem implies that the new count $X'$ also follows a Poisson distribution, consistent with the standard model. Let the parameter of this new distribution be $\\lambda'$. So, $X' \\sim \\operatorname{Pois}(\\lambda')$.\n\nWe apply the linear scaling assumption to the new experiment. The constant of proportionality $k$ remains the same because the gene and other conditions are unchanged. The new expected value is:\n$$\n\\mathbb{E}[X'] = kD' = k(3D)\n$$\nWe can rewrite this expression by grouping terms:\n$$\n\\mathbb{E}[X'] = 3(kD)\n$$\nSubstituting our earlier finding that $\\lambda = kD$, we determine the new expected value in terms of $\\lambda$:\n$$\n\\mathbb{E}[X'] = 3\\lambda\n$$\n\nFor the new Poisson-distributed variable $X'$, its parameter $\\lambda'$ must be equal to its expected value. Thus:\n$$\n\\lambda' = \\mathbb{E}[X'] = 3\\lambda\n$$\nThe variance of the Poisson distribution is also equal to its parameter. Therefore, the variance of the new read count $X'$ is:\n$$\n\\operatorname{Var}(X') = \\lambda' = 3\\lambda\n$$\n\nThe problem asks for the new expected value and variance, reported as a row vector $(\\mathbb{E}[X'], \\operatorname{Var}(X'))$. We have found both quantities. The new expected value is $3\\lambda$ and the new variance is $3\\lambda$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3\\lambda & 3\\lambda\n\\end{pmatrix}\n}\n$$", "id": "2389172"}, {"introduction": "Many complex biological processes, like the accumulation of somatic mutations over a lifetime, can be effectively modeled by combining simpler stochastic events. This practice challenges you to calculate the expected number of mutations in a stem cell population by modeling cell division and mutation as layered Poisson processes. This exercise [@problem_id:2389107] demonstrates the power of the law of total expectation to dissect multi-stage random phenomena, a crucial skill for building sophisticated quantitative models of biological systems.", "problem": "A healthy adult human maintains a roughly constant pool of hematopoietic stem cells (HSCs). Consider the following stochastic model for the accumulation of single-nucleotide variant (SNV) point mutations in the HSC compartment from birth to age $50$ years.\n\n- The HSC population size is constant at $N = 5 \\times 10^{4}$ cells.\n- Each HSC undergoes mitosis according to a homogeneous Poisson process with rate $d = 0.7$ divisions per year, independently across cells and time, over the interval of length $T = 50$ years.\n- Conditional on an HSC division, the number of new SNV point mutations introduced into the genome of the dividing HSC is a Poisson random variable with mean $\\mu = 2.8$ mutations per cell division. Mutation counts across divisions are independent.\n- Ignore selection, apoptosis, and any mutations arising outside of DNA replication. Count every mutation event that occurs within the HSC compartment during the $T$-year interval, regardless of whether the mutated daughter remains an HSC or differentiates.\n\nUsing only these assumptions, compute the expected total number of SNV mutation events accrued in the HSC compartment between birth and age $50$. Round your answer to two significant figures. Express the final answer as a pure number (no units).", "solution": "The problem statement must first be rigorously validated for scientific and mathematical soundness.\n\nFirst, I will extract all given information verbatim from the statement.\n- The HSC population size is constant at $N = 5 \\times 10^{4}$ cells.\n- Each HSC undergoes mitosis according to a homogeneous Poisson process with rate $d = 0.7$ divisions per year, independently across cells and time.\n- The time interval is of length $T = 50$ years.\n- Conditional on an HSC division, the number of new SNV point mutations introduced is a Poisson random variable with mean $\\mu = 2.8$ mutations per cell division.\n- Mutation counts across divisions are independent.\n- The model requires ignoring selection, apoptosis, and any mutations arising outside of DNA replication.\n- The task is to count every mutation event that occurs within the HSC compartment during the $T$-year interval.\n- The final goal is to compute the expected total number of SNV mutation events and round the answer to two significant figures.\n\nNext, I will validate the problem against the required criteria.\n1.  **Scientific Groundedness**: The problem uses a stochastic model (Poisson process) which is a standard and valid tool in computational biology for modeling events like cell division and mutation. The parameter values for population size ($N$), division rate ($d$), and mutation burden per division ($\\mu$) are within biologically plausible ranges for human hematopoietic stem cells. The assumptions, while simplifying, are explicitly stated. The problem is scientifically grounded as a modeling exercise.\n2.  **Well-Posedness**: All necessary parameters ($N$, $d$, $T$, $\\mu$) are provided. The objective—to compute an expected value—is clearly defined. The stochastic processes are well-specified, allowing for a unique solution.\n3.  **Objectivity**: The problem is stated in precise, quantitative terms, free of subjective or ambiguous language.\n\nThe problem is a valid, self-contained, and well-posed question in applied probability and computational biology. It does not violate any of the invalidity criteria. Therefore, I will proceed with the solution.\n\nLet $M_{total}$ be the random variable representing the total number of SNV mutation events across the entire HSC compartment over the time interval of length $T$. The goal is to compute the expectation, $E[M_{total}]$.\n\nThe total number of mutations is the sum of mutations occurring at each division. This can be modeled as a sum of a random number of random variables. Let $D_{total}$ be the total number of HSC divisions in the compartment over the time interval $[0, T]$. Let $M_j$ be the number of new mutations occurring during the $j$-th division event. We are given that $M_j$ is a Poisson random variable with mean $\\mu$. Thus, $E[M_j] = \\mu$.\n\nThe total number of mutations is $M_{total} = \\sum_{j=1}^{D_{total}} M_j$.\nTo find the expectation of $M_{total}$, we can use the law of total expectation, also known as Wald's identity in this context:\n$$E[M_{total}] = E[E[M_{total} | D_{total}]]$$\nFirst, we compute the inner expectation, conditional on a fixed total number of divisions, $D_{total}$.\n$$E[M_{total} | D_{total}] = E\\left[\\sum_{j=1}^{D_{total}} M_j \\bigg| D_{total}\\right]$$\nSince the number of mutations in each division ($M_j$) is independent of the number of mutations in other divisions and also independent of the total number of divisions itself (other than setting the number of terms in the sum), we can use the linearity of expectation:\n$$E\\left[\\sum_{j=1}^{D_{total}} M_j \\bigg| D_{total}\\right] = \\sum_{j=1}^{D_{total}} E[M_j] = \\sum_{j=1}^{D_{total}} \\mu = D_{total} \\mu$$\nNow, we substitute this result back into the law of total expectation:\n$$E[M_{total}] = E[D_{total} \\mu] = \\mu E[D_{total}]$$\nThe problem is now reduced to finding the expected total number of divisions, $E[D_{total}]$.\n\nThe HSC compartment consists of $N$ cells. The total number of divisions, $D_{total}$, is the sum of divisions from each of the $N$ cells over the time interval $T$. Let $D_i(T)$ be the number of divisions of cell $i$ in $[0, T]$. Then:\n$$D_{total} = \\sum_{i=1}^{N} D_i(T)$$\nBy the linearity of expectation:\n$$E[D_{total}] = E\\left[\\sum_{i=1}^{N} D_i(T)\\right] = \\sum_{i=1}^{N} E[D_i(T)]$$\nFor each cell $i$, the number of divisions $D_i(T)$ follows a homogeneous Poisson process with rate $d$. The number of events for such a process over an interval of length $T$ is a Poisson-distributed random variable with mean $d T$.\nTherefore, $E[D_i(T)] = d T$ for each cell $i=1, \\dots, N$.\n\nSubstituting this back into the expression for $E[D_{total}]$:\n$$E[D_{total}] = \\sum_{i=1}^{N} (d T) = N d T$$\nFinally, we can find the expected total number of mutations:\n$$E[M_{total}] = \\mu E[D_{total}] = \\mu N d T$$\nThis is the final analytical expression for the expected total number of mutation events.\n\nNow, I will substitute the given numerical values:\n- $N = 5 \\times 10^{4}$\n- $d = 0.7$ year$^{-1}$\n- $T = 50$ years\n- $\\mu = 2.8$\n\n$$E[M_{total}] = (2.8) \\times (5 \\times 10^{4}) \\times (0.7) \\times (50)$$\n$$E[M_{total}] = 2.8 \\times 5 \\times 0.7 \\times 50 \\times 10^{4}$$\n$$E[M_{total}] = (2.8 \\times 0.7) \\times (5 \\times 50) \\times 10^{4}$$\n$$E[M_{total}] = 1.96 \\times 250 \\times 10^{4}$$\n$$E[M_{total}] = 490 \\times 10^{4}$$\n$$E[M_{total}] = 4.9 \\times 10^{6}$$\nThe problem requires the answer to be rounded to two significant figures. The calculated result, $4.9 \\times 10^{6}$, is already expressed in two significant figures.", "answer": "$$\\boxed{4.9 \\times 10^{6}}$$", "id": "2389107"}, {"introduction": "When we compute a statistic like fold change from experimental data, the result is an estimate, not a perfect value; it has its own variance. This advanced problem tackles the critical task of quantifying the uncertainty in a log-fold change estimator used in RNA-seq analysis. To solve this [@problem_id:2389135], you will apply the Delta Method, a cornerstone of biostatistics, to approximate how variance from the original measurements propagates through a non-linear function, a vital concept for assessing the reliability of bioinformatics findings.", "problem": "In an RNA sequencing (RNA-seq) differential expression study of a single gene, suppose there are $n_{1}$ independent biological replicates under condition $A$ and $n_{2}$ independent biological replicates under condition $B$. Let the read counts for condition $A$ be $X_{A,1}, \\dots, X_{A,n_{1}}$ and for condition $B$ be $X_{B,1}, \\dots, X_{B,n_{2}}$. Assume that within each condition the replicate counts are independent and identically distributed according to a Poisson distribution with means $\\mu_{A}$ and $\\mu_{B}$, respectively, and that all counts across the two conditions are independent. Define the sample means $\\overline{X}_{A} = \\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} X_{A,i}$ and $\\overline{X}_{B} = \\frac{1}{n_{2}} \\sum_{j=1}^{n_{2}} X_{B,j}$. For numerical stability, a small positive pseudocount $c>0$ is added to stabilize ratios, and the base-$2$ logarithm fold change estimator is\n$$\nL \\;=\\; \\log_{2}\\!\\left(\\frac{\\overline{X}_{A} + c}{\\overline{X}_{B} + c}\\right).\n$$\nUnder the model described, obtain a closed-form leading-order approximation (in powers of $1/n_{1}$ and $1/n_{2}$) to the variance of $L$, expressed solely in terms of $\\mu_{A}$, $\\mu_{B}$, $n_{1}$, $n_{2}$, and $c$. Provide your final answer as a single analytic expression. Do not round your answer.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens.\n-   $n_{1}$ independent biological replicates for condition $A$.\n-   $n_{2}$ independent biological replicates for condition $B$.\n-   Read counts for condition $A$: $X_{A,1}, \\dots, X_{A,n_{1}}$.\n-   Read counts for condition $B$: $X_{B,1}, \\dots, X_{B,n_{2}}$.\n-   $X_{A,i}$ are independent and identically distributed (i.i.d.) following a Poisson distribution with mean $\\mu_{A}$, denoted as $X_{A,i} \\sim \\text{Poisson}(\\mu_{A})$.\n-   $X_{B,j}$ are i.i.d. following a Poisson distribution with mean $\\mu_{B}$, denoted as $X_{B,j} \\sim \\text{Poisson}(\\mu_{B})$.\n-   All random variables $X_{A,i}$ and $X_{B,j}$ are mutually independent.\n-   Sample mean for condition $A$: $\\overline{X}_{A} = \\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} X_{A,i}$.\n-   Sample mean for condition $B$: $\\overline{X}_{B} = \\frac{1}{n_{2}} \\sum_{j=1}^{n_{2}} X_{B,j}$.\n-   A positive pseudocount $c > 0$.\n-   The base-$2$ logarithm fold change estimator is $L = \\log_{2}\\left(\\frac{\\overline{X}_{A} + c}{\\overline{X}_{B} + c}\\right)$.\n-   The objective is to find a closed-form leading-order approximation to the variance of $L$, $\\text{Var}(L)$, as a function of $\\mu_{A}$, $\\mu_{B}$, $n_{1}$, $n_{2}$, and $c$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard statistical approximation problem in the context of bioinformatics, specifically RNA-seq data analysis. The use of a Poisson model for read counts, while a simplification of more complex models like the Negative Binomial, is a fundamental and valid approach. The request for a leading-order approximation points to a standard, well-defined mathematical procedure (the Delta Method). The problem is self-contained, with all necessary statistical properties of the random variables defined. There are no contradictions, ambiguities, or scientifically unsound premises.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be derived.\n\nThe problem requires the approximation of the variance of a function of two random variables, $L = g(\\overline{X}_{A}, \\overline{X}_{B})$. The function is given by\n$$L = g(\\overline{X}_{A}, \\overline{X}_{B}) = \\log_{2}\\left(\\frac{\\overline{X}_{A} + c}{\\overline{X}_{B} + c}\\right)$$\nFor easier differentiation, we rewrite the base-$2$ logarithm in terms of the natural logarithm:\n$$L = \\frac{\\ln\\left(\\frac{\\overline{X}_{A} + c}{\\overline{X}_{B} + c}\\right)}{\\ln 2} = \\frac{1}{\\ln 2} \\left[ \\ln(\\overline{X}_{A} + c) - \\ln(\\overline{X}_{B} + c) \\right]$$\nTo find a leading-order approximation for the variance of $L$, we employ the multivariate Delta Method. For a function $g(X, Y)$ of two random variables $X$ and $Y$, the variance of $g(X, Y)$ can be approximated by a first-order Taylor series expansion around the means $(E[X], E[Y])$. The formula is:\n$$ \\text{Var}(g(X, Y)) \\approx \\left( \\frac{\\partial g}{\\partial X} \\right)^2 \\text{Var}(X) + \\left( \\frac{\\partial g}{\\partial Y} \\right)^2 \\text{Var}(Y) + 2 \\left( \\frac{\\partial g}{\\partial X} \\right) \\left( \\frac{\\partial g}{\\partial Y} \\right) \\text{Cov}(X, Y) $$\nwhere the partial derivatives are evaluated at the means of $X$ and $Y$. In our case, the random variables are the sample means $\\overline{X}_{A}$ and $\\overline{X}_{B}$.\n\nFirst, we must determine the expectation, variance, and covariance of these sample means.\nThe expectation of a random variable $X \\sim \\text{Poisson}(\\lambda)$ is $E[X] = \\lambda$, and its variance is $\\text{Var}(X) = \\lambda$.\nFor condition $A$, since $X_{A,i} \\sim \\text{Poisson}(\\mu_{A})$, we have $E[X_{A,i}] = \\mu_{A}$ and $\\text{Var}(X_{A,i}) = \\mu_{A}$.\nThe expectation of the sample mean $\\overline{X}_{A}$ is:\n$$ E[\\overline{X}_{A}] = E\\left[ \\frac{1}{n_1} \\sum_{i=1}^{n_1} X_{A,i} \\right] = \\frac{1}{n_1} \\sum_{i=1}^{n_1} E[X_{A,i}] = \\frac{1}{n_1} (n_1 \\mu_A) = \\mu_A $$\nThe variance of the sample mean $\\overline{X}_{A}$, using the independence of the $X_{A,i}$ replicates, is:\n$$ \\text{Var}(\\overline{X}_{A}) = \\text{Var}\\left( \\frac{1}{n_1} \\sum_{i=1}^{n_1} X_{A,i} \\right) = \\frac{1}{n_1^2} \\sum_{i=1}^{n_1} \\text{Var}(X_{A,i}) = \\frac{1}{n_1^2} (n_1 \\mu_A) = \\frac{\\mu_A}{n_1} $$\nBy identical reasoning for condition $B$:\n$$ E[\\overline{X}_{B}] = \\mu_{B} \\quad \\text{and} \\quad \\text{Var}(\\overline{X}_{B}) = \\frac{\\mu_{B}}{n_2} $$\nThe covariance term $\\text{Cov}(\\overline{X}_{A}, \\overline{X}_{B})$ is zero because the problem statement explicitly says that all counts across the two conditions are independent. Independence of random variables implies their covariance is zero.\n$$ \\text{Cov}(\\overline{X}_{A}, \\overline{X}_{B}) = 0 $$\nNext, we compute the partial derivatives of the function $g(x, y) = \\frac{1}{\\ln 2} [\\ln(x+c) - \\ln(y+c)]$ with respect to its arguments $x$ and $y$.\n$$ \\frac{\\partial g}{\\partial x} = \\frac{1}{\\ln 2} \\frac{\\partial}{\\partial x} \\left[ \\ln(x+c) - \\ln(y+c) \\right] = \\frac{1}{(\\ln 2)(x+c)} $$\n$$ \\frac{\\partial g}{\\partial y} = \\frac{1}{\\ln 2} \\frac{\\partial}{\\partial y} \\left[ \\ln(x+c) - \\ln(y+c) \\right] = -\\frac{1}{(\\ln 2)(y+c)} $$\nWe evaluate these partial derivatives at the point of the expectations, $(E[\\overline{X}_{A}], E[\\overline{X}_{B}]) = (\\mu_A, \\mu_B)$.\n$$ \\left. \\frac{\\partial g}{\\partial \\overline{X}_{A}} \\right|_{(\\mu_A, \\mu_B)} = \\frac{1}{(\\ln 2)(\\mu_A + c)} $$\n$$ \\left. \\frac{\\partial g}{\\partial \\overline{X}_{B}} \\right|_{(\\mu_A, \\mu_B)} = -\\frac{1}{(\\ln 2)(\\mu_B + c)} $$\nNow we substitute these components into the Delta Method formula for $\\text{Var}(L) = \\text{Var}(g(\\overline{X}_{A}, \\overline{X}_{B}))$:\n$$ \\text{Var}(L) \\approx \\left( \\left. \\frac{\\partial g}{\\partial \\overline{X}_{A}} \\right|_{(\\mu_A, \\mu_B)} \\right)^2 \\text{Var}(\\overline{X}_{A}) + \\left( \\left. \\frac{\\partial g}{\\partial \\overline{X}_{B}} \\right|_{(\\mu_A, \\mu_B)} \\right)^2 \\text{Var}(\\overline{X}_{B}) $$\n$$ \\text{Var}(L) \\approx \\left( \\frac{1}{(\\ln 2)(\\mu_A + c)} \\right)^2 \\left( \\frac{\\mu_A}{n_1} \\right) + \\left( -\\frac{1}{(\\ln 2)(\\mu_B + c)} \\right)^2 \\left( \\frac{\\mu_B}{n_2} \\right) $$\nSimplifying the expression:\n$$ \\text{Var}(L) \\approx \\frac{1}{(\\ln 2)^2 (\\mu_A + c)^2} \\frac{\\mu_A}{n_1} + \\frac{1}{(\\ln 2)^2 (\\mu_B + c)^2} \\frac{\\mu_B}{n_2} $$\n$$ \\text{Var}(L) \\approx \\frac{1}{(\\ln 2)^2} \\left[ \\frac{\\mu_A}{n_1 (\\mu_A + c)^2} + \\frac{\\mu_B}{n_2 (\\mu_B + c)^2} \\right] $$\nThis expression represents the leading-order approximation to the variance of the log-fold change estimator $L$. The approximation error involves higher-order terms in the Taylor expansion, which correspond to higher powers of $1/n_1$ and $1/n_2$. This is the requested result.", "answer": "$$\n\\boxed{\\frac{1}{(\\ln 2)^{2}} \\left( \\frac{\\mu_{A}}{n_{1} (\\mu_{A} + c)^{2}} + \\frac{\\mu_{B}}{n_{2} (\\mu_{B} + c)^{2}} \\right)}\n$$", "id": "2389135"}]}