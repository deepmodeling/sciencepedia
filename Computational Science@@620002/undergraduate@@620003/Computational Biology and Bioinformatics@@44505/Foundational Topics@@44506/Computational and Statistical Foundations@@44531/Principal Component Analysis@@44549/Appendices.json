{"hands_on_practices": [{"introduction": "Before relying on automated software packages, it is essential to understand the core mechanics of Principal Component Analysis. This exercise ([@problem_id:2416060]) provides a hands-on opportunity to compute the first principal component from scratch for a small gene expression dataset. By manually calculating the covariance matrix and its leading eigenvector, you will demystify the process and build a solid foundation in the linear algebra that powers PCA.", "problem": "A gene expression experiment measured log-transformed expression values for $G_1$, $G_2$, and $G_3$ across $S_1$, $S_2$, $S_3$, and $S_4$. The data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has samples as rows and genes as columns:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\nTreat samples as independent observations and genes as variables. Using principal component analysis (PCA), compute the first principal component loading vector in gene space by:\n- column-centering $X$,\n- forming the sample covariance matrix across genes with denominator $n-1$ for $n=4$ samples, and\n- taking the unit-norm eigenvector of this covariance matrix corresponding to the largest eigenvalue.\n\nReport the loading vector as a $1 \\times 3$ row matrix ordered as $(G_1, G_2, G_3)$, with the sign chosen so that its first nonzero entry is positive. No rounding is required.", "solution": "We are asked to compute the first principal component loading vector in gene space using the eigen-decomposition of the sample covariance matrix across genes. Let $n=4$ be the number of samples and $p=3$ be the number of genes. The data matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows as samples and columns as genes.\n\nStep $1$: Column-centering. Compute the column means of $X$:\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\nSubtract these means from each column to obtain the centered matrix $Z$:\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\nStep $2$: Sample covariance matrix across genes. Using the denominator $n-1=3$, the sample covariance of genes is\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\nObserve that each row of $Z$ is a scalar multiple of $(1,\\,1,\\,1)$, so all three centered gene columns are identical. Compute $Z^{\\top}Z$ by noting that for any two columns $j$ and $k$, the $(j,k)$ entry equals $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$. Since all three columns are identical, every entry of $Z^{\\top}Z$ equals the sum of squares of a single centered column:\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\nTherefore,\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\nStep $3$: Eigen-decomposition. Let $J \\in \\mathbb{R}^{3 \\times 3}$ denote the all-ones matrix, i.e., $J_{jk}=1$ for all $j,k$. It is known from first principles that $J$ has rank $1$ with eigenvalues $3$ and $0$ (with multiplicity $2$). A corresponding unit-norm eigenvector for the eigenvalue $3$ is proportional to $(1,\\,1,\\,1)^{\\top}$, specifically\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nSince $S = \\frac{5}{3} J$, the eigenvalues of $S$ are $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ and $\\lambda_{2} = 0$, $\\lambda_{3} = 0$, with the same eigenvectors as $J$. Thus, the first principal component loading vector in gene space is the unit-norm eigenvector associated with $\\lambda_{1}=5$, namely $v$ as above. Choosing the sign so that the first nonzero entry is positive yields\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\nTherefore, ordered as $(G_1, G_2, G_3)$ and written as a $1 \\times 3$ row matrix, the first principal component loading vector is\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}, {"introduction": "Principal components, especially the leading ones that capture the most variance, can sometimes be surprisingly sensitive to individual data points. This practice ([@problem_id:2416110]) explores the important concept of model stability by investigating how the removal of a single sample can alter PCA results. You will implement a quantitative test to detect when an influential point is powerful enough to swap the order of the top two principal components, a key skill for validating the robustness of findings from exploratory data analysis.", "problem": "You are given a definition of Principal Component Analysis (PCA) applied to a gene expression matrix across patients. Let $X \\in \\mathbb{R}^{n \\times m}$ denote a data matrix with $n$ patient samples (rows) and $m$ genes (columns). Define the column-wise centered matrix $\\tilde{X}$ by subtracting the sample mean of each column of $X$. The sample covariance matrix is\n$$\nC \\;=\\; \\frac{1}{n-1}\\,\\tilde{X}^\\top \\tilde{X} \\;\\in\\; \\mathbb{R}^{m \\times m}.\n$$\nLet the top two eigenpairs of $C$ be $(\\lambda_1, v_1)$ and $(\\lambda_2, v_2)$, with $\\lambda_1 \\ge \\lambda_2 \\ge 0$ and $v_1, v_2 \\in \\mathbb{R}^m$ of unit norm. For a given index $r \\in \\{0,1,\\dots,n-1\\}$, let $X^{(-r)}$ denote the matrix obtained by removing the $r$-th row of $X$, and define its covariance $C^{(-r)}$ similarly with top two eigenpairs $(\\lambda_1', v_1')$ and $(\\lambda_2', v_2')$.\n\nFix two thresholds: an eigenvalue tie-tolerance $\\epsilon > 0$ and a direction-alignment threshold $\\tau \\in (0,1)$. Define that removing a single patient sample “completely swaps the order of $PC_1$ and $PC_2$” if and only if all of the following hold:\n- The leading eigenvalues are not tied in either analysis: $|\\lambda_1 - \\lambda_2| > \\epsilon$ and $|\\lambda_1' - \\lambda_2'| > \\epsilon$.\n- The leading directions exchange up to sign: $|v_1^\\top v_2'| \\ge \\tau$ and $|v_2^\\top v_1'| \\ge \\tau$.\n\nUse $\\epsilon = 1\\times 10^{-9}$ and $\\tau = 0.99$.\n\nConstruct a program that, for each test case below, computes whether the removal of the specified sample produces a complete swap according to the definition above, and outputs a list of booleans corresponding to the test cases.\n\nTest suite. Each case consists of a matrix $X^{(i)}$ with $n$ samples by $m$ genes, and a removal index $r^{(i)}$ using $0$-based indexing. All values are real numbers representing gene expression in arbitrary consistent units.\n\n- Case $1$ (designed swap with $m=2$ genes, $n=5$ patients): choose parameters $a=1.0$, $k=1.6$, $p=3.0$. Let\n$$\nX^{(1)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n3.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(1)} = 4.\n$$\n\n- Case $2$ (no swap; smaller leverage, same structure): $a=1.0$, $k=1.6$, $p=1.0$.\n$$\nX^{(2)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n1.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(2)} = 4.\n$$\n\n- Case $3$ (boundary tie; top eigenvalues equal before removal): $a=1.0$, $k=1.6$, and $p = \\sqrt{\\frac{5}{2}\\,\\big(k^2 - a^2\\big)}$.\n$$\nX^{(3)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n\\sqrt{\\tfrac{5}{2}\\,(1.6^2 - 1.0^2)} & 0.0\n\\end{bmatrix},\n\\quad r^{(3)} = 4.\n$$\n\n- Case $4$ (remove a non-leverage sample; no swap expected): reuse the matrix from Case $1$ and remove the first row.\n$$\nX^{(4)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n3.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(4)} = 0.\n$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[b_1,b_2,b_3,b_4]$, where each $b_i$ is the boolean result for Case $i$ in order.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Data Matrix: $X \\in \\mathbb{R}^{n \\times m}$, with $n$ patient samples and $m$ genes.\n- Column-wise Centered Matrix: $\\tilde{X}$, obtained by subtracting the sample mean from each column of $X$.\n- Sample Covariance Matrix: $C = \\frac{1}{n-1}\\,\\tilde{X}^\\top \\tilde{X} \\in \\mathbb{R}^{m \\times m}$.\n- Eigenpairs of $C$: $(\\lambda_1, v_1)$ and $(\\lambda_2, v_2)$, where $\\lambda_1 \\ge \\lambda_2 \\ge 0$ and $v_1, v_2$ are unit vectors in $\\mathbb{R}^m$. These correspond to the first two principal components ($PC_1$, $PC_2$).\n- Leave-one-out Matrix: $X^{(-r)}$ is the matrix $X$ with its $r$-th row removed.\n- Leave-one-out Covariance and Eigenpairs: $C^{(-r)}$ is the covariance matrix of $X^{(-r)}$, with top two eigenpairs $(\\lambda_1', v_1')$ and $(\\lambda_2', v_2')$.\n- Eigenvalue Tie-Tolerance: $\\epsilon = 1 \\times 10^{-9}$.\n- Direction-Alignment Threshold: $\\tau = 0.99$.\n- Definition of \"complete swap\": The removal of sample $r$ causes a complete swap if and only if all four of the following conditions hold:\n    1. $|\\lambda_1 - \\lambda_2| > \\epsilon$ (eigenvalues are not tied in the original analysis).\n    2. $|\\lambda_1' - \\lambda_2'| > \\epsilon$ (eigenvalues are not tied in the leave-one-out analysis).\n    3. $|v_1^\\top v_2'| \\ge \\tau$ (original $PC_1$ direction aligns with new $PC_2$ direction).\n    4. $|v_2^\\top v_1'| \\ge \\tau$ (original $PC_2$ direction aligns with new $PC_1$ direction).\n- Test Cases:\n    - Case 1: $X^{(1)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ 3.0 & 0.0 \\end{bmatrix}$, $r^{(1)} = 4$.\n    - Case 2: $X^{(2)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ 1.0 & 0.0 \\end{bmatrix}$, $r^{(2)} = 4$.\n    - Case 3: $X^{(3)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ \\sqrt{\\tfrac{5}{2}\\,(1.6^2 - 1.0^2)} & 0.0 \\end{bmatrix}$, $r^{(3)} = 4$.\n    - Case 4: $X^{(4)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ 3.0 & 0.0 \\end{bmatrix}$, $r^{(4)} = 0$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in statistical linear algebra and its application to bioinformatics, specifically Principal Component Analysis (PCA). The analysis of component stability via leave-one-out resampling is a standard and valid technique to assess the influence of individual data points. The definitions are mathematically rigorous.\n- **Well-Posedness**: The problem is well-posed. All necessary inputs (data matrices, removal indices, tolerance parameters) are provided. The criteria for a \"complete swap\" are defined unambiguously, ensuring a unique boolean outcome for each test case.\n- **Objectivity**: The problem is stated objectively using formal mathematical language, free from subjective or speculative content.\n\nThe problem does not exhibit any of the enumerated flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a direct and formalizable computational task.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be furnished.\n\n**Solution Derivation**\nThe objective is to implement an algorithm that verifies, for each provided test case, whether the removal of a specific sample (row) from a data matrix causes a \"complete swap\" of the first two principal components ($PC_1$ and $PC_2$). The procedure for a single test case $(X, r)$ is as follows.\n\n1.  **Analyze the Full Data Matrix $X$**:\n    Let the given matrix be $X$ with $n$ rows and $m$ columns.\n    a. Compute the column means: $\\mu = \\frac{1}{n} \\sum_{i=1}^n x_{i,:}$, where $x_{i,:}$ is the $i$-th row of $X$.\n    b. Center the data: $\\tilde{X} = X - \\mathbf{1}_{n \\times 1} \\mu^\\top$, where $\\mathbf{1}_{n \\times 1}$ is a column vector of ones.\n    c. Compute the sample covariance matrix: $C = \\frac{1}{n-1} \\tilde{X}^\\top \\tilde{X}$.\n    d. Perform eigendecomposition of the symmetric matrix $C$. This yields a set of real eigenvalues and a corresponding orthogonal basis of eigenvectors.\n    e. Identify the two largest eigenvalues, $\\lambda_1 \\ge \\lambda_2$, and their corresponding unit-norm eigenvectors, $v_1$ and $v_2$.\n\n2.  **Analyze the Leave-One-Out Data Matrix $X^{(-r)}$**:\n    Let $X^{(-r)}$ be the matrix obtained by removing row $r$ from $X$. This matrix has $n' = n-1$ rows and $m$ columns.\n    a. Compute the new column means: $\\mu' = \\frac{1}{n'} \\sum_{i \\ne r} x_{i,:}$.\n    b. Center the reduced data: $\\tilde{X}^{(-r)} = X^{(-r)} - \\mathbf{1}_{(n-1) \\times 1} (\\mu')^\\top$.\n    c. Compute the new sample covariance matrix: $C^{(-r)} = \\frac{1}{n'-1} (\\tilde{X}^{(-r)})^\\top \\tilde{X}^{(-r)}$.\n    d. Perform eigendecomposition of $C^{(-r)}$ to find its two largest eigenvalues, $\\lambda_1' \\ge \\lambda_2'$, and corresponding unit-norm eigenvectors, $v_1'$ and $v_2'$.\n\n3.  **Evaluate the \"Complete Swap\" Conditions**:\n    Using the specified thresholds $\\epsilon = 1 \\times 10^{-9}$ and $\\tau = 0.99$, evaluate the four conditions:\n    a. Condition 1 (No tie in original data): Test if $|\\lambda_1 - \\lambda_2| > \\epsilon$.\n    b. Condition 2 (No tie in reduced data): Test if $|\\lambda_1' - \\lambda_2'| > \\epsilon$.\n    c. Condition 3 (Alignment of $PC_1$ and $PC_2'$): Test if $|v_1^\\top v_2'| \\ge \\tau$. The dot product $v_1^\\top v_2'$ measures the cosine of the angle between the two vector directions. The absolute value accounts for the arbitrary sign of eigenvectors.\n    d. Condition 4 (Alignment of $PC_2$ and $PC_1'$): Test if $|v_2^\\top v_1'| \\ge \\tau$.\n\n    A complete swap occurs if and only if all four conditions are true. The procedure will be applied to each of the four test cases provided. Numerical computation will rely on standard libraries for matrix operations and eigendecomposition, ensuring accuracy and adherence to established algorithms. Specifically, for a real symmetric matrix such as the covariance matrix, specialized algorithms exist (e.g., the Jacobi method or QR-based algorithms) which are numerically stable and efficient. The use of `numpy.linalg.eigh` is appropriate as it is designed for Hermitian (or real symmetric) matrices and guarantees real eigenvalues and a complete orthonormal set of eigenvectors, sorted by eigenvalue magnitude.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing PCA stability for each test case.\n    \"\"\"\n\n    def check_swap(X, r_idx, epsilon, tau):\n        \"\"\"\n        Checks if removing a sample causes a complete swap of PC1 and PC2.\n\n        Args:\n            X (np.ndarray): The data matrix (n_samples, n_features).\n            r_idx (int): The 0-based index of the row to remove.\n            epsilon (float): The eigenvalue tie-tolerance.\n            tau (float): The direction-alignment threshold.\n\n        Returns:\n            bool: True if a complete swap occurs, False otherwise.\n        \"\"\"\n\n        # --- PCA on the full dataset ---\n        n_samples, n_features = X.shape\n        if n_samples <= 1:\n            return False # Covariance is not well-defined.\n\n        # Center the data\n        X_centered = X - X.mean(axis=0)\n        # Compute the sample covariance matrix\n        cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # np.linalg.eigh returns eigenvalues in ascending order\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        # Extract top two eigenpairs\n        lambda1, lambda2 = eigenvalues[0], eigenvalues[1]\n        v1, v2 = eigenvectors[:, 0], eigenvectors[:, 1]\n        \n        # --- PCA on the leave-one-out dataset ---\n        X_loo = np.delete(X, r_idx, axis=0)\n        n_samples_loo, _ = X_loo.shape\n        \n        if n_samples_loo <= 1:\n            return False\n\n        # Center the leave-one-out data\n        X_loo_centered = X_loo - X_loo.mean(axis=0)\n        # Compute the new sample covariance matrix\n        cov_matrix_loo = (X_loo_centered.T @ X_loo_centered) / (n_samples_loo - 1)\n        \n        # Eigendecomposition\n        eigenvalues_loo, eigenvectors_loo = np.linalg.eigh(cov_matrix_loo)\n        \n        # Sort in descending order\n        sorted_indices_loo = np.argsort(eigenvalues_loo)[::-1]\n        eigenvalues_loo = eigenvalues_loo[sorted_indices_loo]\n        eigenvectors_loo = eigenvectors_loo[:, sorted_indices_loo]\n        \n        # Extract top two eigenpairs\n        lambda1_p, lambda2_p = eigenvalues_loo[0], eigenvalues_loo[1]\n        v1_p, v2_p = eigenvectors_loo[:, 0], eigenvectors_loo[:, 1]\n\n        # --- Evaluate the four conditions for a complete swap ---\n        \n        # Condition 1: Eigenvalues are not tied in the original analysis\n        cond1 = abs(lambda1 - lambda2) > epsilon\n        \n        # Condition 2: Eigenvalues are not tied in the leave-one-out analysis\n        cond2 = abs(lambda1_p - lambda2_p) > epsilon\n        \n        # Condition 3: Original PC1 aligns with new PC2\n        cond3 = abs(np.dot(v1, v2_p)) >= tau\n        \n        # Condition 4: Original PC2 aligns with new PC1\n        cond4 = abs(np.dot(v2, v1_p)) >= tau\n        \n        return cond1 and cond2 and cond3 and cond4\n\n    # Define constants from the problem statement\n    epsilon = 1e-9\n    tau = 0.99\n\n    # Define the test cases from the problem statement\n    X1 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [3.0, 0.0]\n    ])\n    r1 = 4\n\n    X2 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [1.0, 0.0]\n    ])\n    r2 = 4\n\n    # Calculate p for Case 3\n    a = 1.0\n    k = 1.6\n    p_val = np.sqrt((5.0 / 2.0) * (k**2 - a**2))\n    X3 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [p_val, 0.0]\n    ])\n    r3 = 4\n\n    X4 = X1.copy() # Reuse matrix from Case 1\n    r4 = 0\n    \n    test_cases = [\n        (X1, r1),\n        (X2, r2),\n        (X3, r3),\n        (X4, r4),\n    ]\n\n    results = []\n    for X, r_idx in test_cases:\n        is_swap = check_swap(X, r_idx, epsilon, tau)\n        # Python's bool `True` stringifies to 'True', which is standard.\n        results.append(str(is_swap))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2416110"}, {"introduction": "Standard PCA excels at finding linear relationships, but many biological datasets contain complex, non-linear structures. This exercise ([@problem_id:2416090]) introduces Kernel PCA, a powerful extension for uncovering such patterns. Using a classic example of two cell populations that are not linearly separable, you will implement Kernel PCA with a polynomial kernel to project the data into a new feature space where the populations can be clearly distinguished.", "problem": "You are given a scenario with $2$ cell populations measured by $2$ quantitative features per cell (for example, two gene expression measurements). Each population consists of points arranged deterministically on a circle in $\\mathbb{R}^2$, forming a configuration that is not linearly separable in the original space. Let the first population (population $A$) lie on a circle of radius $r_A$ and the second population (population $B$) lie on a circle of radius $r_B$, with $r_A \\ne r_B$. The $i$-th point of population $A$ is defined as\n$$\n\\mathbf{x}_i^{(A)} = \\begin{bmatrix} r_A \\cos\\left(\\frac{2\\pi i}{n_A}\\right) \\\\ r_A \\sin\\left(\\frac{2\\pi i}{n_A}\\right) \\end{bmatrix}, \\quad i \\in \\{0,1,\\dots,n_A-1\\},\n$$\nand the $j$-th point of population $B$ is defined as\n$$\n\\mathbf{x}_j^{(B)} = \\begin{bmatrix} r_B \\cos\\left(\\frac{2\\pi j}{n_B}\\right) \\\\ r_B \\sin\\left(\\frac{2\\pi j}{n_B}\\right) \\end{bmatrix}, \\quad j \\in \\{0,1,\\dots,n_B-1\\},\n$$\nwhere all angles are in radians.\n\nDefine the overall dataset $\\{\\mathbf{x}_k\\}_{k=1}^N$ by concatenating the points from populations $A$ and $B$, where $N = n_A + n_B$. Consider the Kernel Principal Component Analysis (Kernel PCA) of this dataset in the Reproducing Kernel Hilbert Space induced by the polynomial kernel\n$$\n\\kappa(\\mathbf{x},\\mathbf{y}) = \\left(\\alpha \\, \\mathbf{x}^\\top \\mathbf{y} + \\beta\\right)^\\delta,\n$$\nwith $\\alpha \\ge 0$, $\\beta \\ge 0$, and integer degree $\\delta \\in \\mathbb{N}$. Let $K \\in \\mathbb{R}^{N \\times N}$ be the kernel matrix with entries $K_{ij} = \\kappa(\\mathbf{x}_i,\\mathbf{x}_j)$. Let $H = I_N - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ denote the centering matrix, and let $K_c = H K H$ be the centered kernel matrix. Let the eigenvalue decomposition of $K_c$ be $K_c = V \\Lambda V^\\top$, where $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_N)$ with $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_N$, and columns of $V$ are orthonormal eigenvectors. For a chosen target dimension $m \\in \\{1,2\\}$, form the embedding $Z \\in \\mathbb{R}^{N \\times m}$ with entries\n$$\nZ_{i\\ell} = \\frac{1}{\\sqrt{\\lambda_\\ell}} \\, V_{i\\ell}, \\quad \\text{for } \\ell \\in \\{1,\\dots,m\\} \\text{ with } \\lambda_\\ell > 0.\n$$\n\nDefine linear separability in $\\mathbb{R}^m$ as follows: the embedded sets $Z_A$ and $Z_B$ (rows of $Z$ corresponding to populations $A$ and $B$, respectively) are linearly separable if and only if there exist $\\mathbf{w} \\in \\mathbb{R}^m$ and $b \\in \\mathbb{R}$ such that $\\mathbf{w}^\\top \\mathbf{z} + b > 0$ for all $\\mathbf{z} \\in Z_A$ and $\\mathbf{w}^\\top \\mathbf{z} + b < 0$ for all $\\mathbf{z} \\in Z_B$. Answer each test case with a boolean indicating whether the two populations are linearly separable in the embedded space of dimension $m$ under the specified kernel.\n\nTest Suite:\nFor each tuple $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m)$ below, construct the dataset and compute the Kernel Principal Component Analysis embedding of dimension $m$ using the definitions above, then determine whether $Z_A$ and $Z_B$ are linearly separable.\n\n- Test $1$: $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,2.0,\\,1.0,\\,1.0,\\,2,\\,2\\,)$.\n- Test $2$: $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,2.0,\\,1.0,\\,0.0,\\,1,\\,2\\,)$.\n- Test $3$: $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,1.4,\\,0.2,\\,1.0,\\,2,\\,2\\,)$.\n- Test $4$: $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,2.0,\\,1.0,\\,1.0,\\,2,\\,1\\,)$.\n- Test $5$: $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,8,\\,8,\\,1.0,\\,2.0,\\,1.0,\\,1.0,\\,3,\\,2\\,)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for Tests $1$ through $5$ in order, as a comma-separated list of booleans enclosed in square brackets (for example, $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{False},\\mathrm{True}]$), with no spaces.", "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and data to specify a solvable mathematical and computational task. The problem asks to determine the linear separability of two populations of data points after they have been embedded into a lower-dimensional space using Kernel Principal Component Analysis (Kernel PCA). The data represents a classic case in bioinformatics where cell populations, defined by a set of quantitative features (e.g., gene expression levels), are not linearly separable in their original feature space.\n\nThe core of the problem lies in the application of a non-linear transformation via a kernel function to project the data into a higher-dimensional feature space where the populations may become linearly separable. Principal Component Analysis is then performed in this new space to find the directions of maximum variance, which are the most informative for distinguishing the populations.\n\nThe supplied dataset consists of two populations of points, $A$ and $B$, distributed on two concentric circles in $\\mathbb{R}^2$ with radii $r_A$ and $r_B$ respectively, where $r_A \\neq r_B$. Such a configuration is not linearly separable in its original $2$-dimensional space.\n\nThe specified kernel is the polynomial kernel, defined as:\n$$\n\\kappa(\\mathbf{x},\\mathbf{y}) = \\left(\\alpha \\, \\mathbf{x}^\\top \\mathbf{y} + \\beta\\right)^\\delta\n$$\nThis kernel function computes the dot product of two vectors in a high-dimensional feature space without explicitly forming the feature vectors $\\Phi(\\mathbf{x})$. The mapping $\\Phi$ is induced by the kernel. For the polynomial kernel with parameters $\\alpha, \\beta > 0$ and degree $\\delta \\geq 2$, the feature space includes features that are functions of the norm of the input vectors. For a data point $\\mathbf{x}$ on a circle of radius $r$, its squared norm is $\\|\\mathbf{x}\\|^2 = \\mathbf{x}^\\top \\mathbf{x} = r^2$. The kernel evaluated for a single point is $\\kappa(\\mathbf{x},\\mathbf{x}) = (\\alpha \\|\\mathbf{x}\\|^2 + \\beta)^\\delta = (\\alpha r^2 + \\beta)^\\delta$. Since the radii $r_A$ and $r_B$ are different, this value is constant within each population but differs between the two populations. This implies that in the feature space, the two populations are mapped to distinct manifolds. This difference in norm-related features is a source of variance that PCA is designed to capture. Consequently, it is expected that at least one of the principal components will be strongly aligned with this separating feature, thereby making the populations linearly separable in the PCA-derived embedding.\n\nThe algorithm to be executed for each test case is as follows:\n\n1.  **Data Generation**: Construct the total dataset $X \\in \\mathbb{R}^{N \\times 2}$ where $N = n_A + n_B$. The first $n_A$ rows correspond to population $A$ and the subsequent $n_B$ rows correspond to population $B$, with point coordinates calculated according to the given formulas:\n    $$\n    \\mathbf{x}_i^{(A)} = \\begin{bmatrix} r_A \\cos\\left(\\frac{2\\pi i}{n_A}\\right) \\\\ r_A \\sin\\left(\\frac{2\\pi i}{n_A}\\right) \\end{bmatrix}, \\quad \\mathbf{x}_j^{(B)} = \\begin{bmatrix} r_B \\cos\\left(\\frac{2\\pi j}{n_B}\\right) \\\\ r_B \\sin\\left(\\frac{2\\pi j}{n_B}\\right) \\end{bmatrix}\n    $$\n\n2.  **Kernel Matrix Construction**: Compute the $N \\times N$ Gram matrix $K$ where each element $K_{ij}$ is the kernel evaluation of the $i$-th and $j$-th points, $K_{ij} = \\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$. This can be efficiently computed using matrix multiplication: $K = (\\alpha (X X^\\top) + \\beta)^\\delta$.\n\n3.  **Kernel Matrix Centering**: The kernel matrix must be centered to correspond to PCA on centered data in the feature space. The centered kernel matrix $K_c$ is computed as $K_c = H K H$, where $H = I_N - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ is the centering matrix, $I_N$ is the identity matrix of size $N$, and $\\mathbf{1}$ is a column vector of ones.\n\n4.  **Eigendecomposition**: Solve the eigenvalue problem for the symmetric matrix $K_c$: $K_c V = V \\Lambda$. This yields $N$ real eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_N$ stored in a diagonal matrix $\\Lambda$, and a matrix $V$ whose columns are the corresponding orthonormal eigenvectors.\n\n5.  **Data Embedding**: Construct the embedding $Z \\in \\mathbb{R}^{N \\times m}$ using the top $m$ principal components. According to the problem's definition, the coordinates of the $i$-th data point in the embedded space are given by:\n    $$\n    Z_{i\\ell} = \\frac{1}{\\sqrt{\\lambda_\\ell}} V_{i\\ell}, \\quad \\text{for } \\ell \\in \\{1, \\dots, m\\}\n    $$\n    This is performed for the components corresponding to the $m$ largest positive eigenvalues.\n\n6.  **Linear Separability Test**: The final step is to determine if the two sets of embedded points, $Z_A$ (the first $n_A$ rows of $Z$) and $Z_B$ (the last $n_B$ rows of $Z$), are linearly separable in $\\mathbb{R}^m$. Two finite sets of points are linearly separable if and only if there exists a hyperplane that separates them. This can be formulated as a linear programming feasibility problem. A separating hyperplane defined by a normal vector $\\mathbf{w} \\in \\mathbb{R}^m$ and an offset $b \\in \\mathbb{R}$ exists if there is a solution to the following system of linear inequalities:\n    $$\n    \\begin{cases}\n    \\mathbf{w}^\\top \\mathbf{z} + b \\ge 1 & \\forall \\mathbf{z} \\in Z_A \\\\\n    \\mathbf{w}^\\top \\mathbf{z} + b \\le -1 & \\forall \\mathbf{z} \\in Z_B\n    \\end{cases}\n    $$\n    We seek to find if there exist feasible $[\\mathbf{w}^\\top, b]^\\top$. This system can be written as $A_{ub} \\mathbf{v} \\le \\mathbf{b}_{ub}$, where $\\mathbf{v} = [\\mathbf{w}^\\top, b]^\\top$. A linear programming solver can determine if a feasible solution exists. If it does, the populations are linearly separable; otherwise, they are not.\n\nThis complete procedure will be applied to each test case to yield a definitive boolean result. An exception is the case of a linear kernel ($\\delta=1, \\beta=0$), where Kernel PCA is equivalent to standard PCA on the original data. As the original data forms two concentric circles, which are already centered, standard PCA will not find a separating direction, as the variance is equal in all directions. The principal components are degenerate, and the transformed data remains non-linearly separable.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves the Kernel PCA linear separability problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (n_A, n_B, r_A, r_B, alpha, beta, delta, m)\n        (24, 24, 1.0, 2.0, 1.0, 1.0, 2, 2),\n        (24, 24, 1.0, 2.0, 1.0, 0.0, 1, 2),\n        (24, 24, 1.0, 1.4, 0.2, 1.0, 2, 2),\n        (24, 24, 1.0, 2.0, 1.0, 1.0, 2, 1),\n        (8, 8, 1.0, 2.0, 1.0, 1.0, 3, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_a, n_b, r_a, r_b, alpha, beta, delta, m = case\n        N = n_a + n_b\n\n        # Step 1: Data Generation\n        t_a = np.linspace(0, 2 * np.pi, n_a, endpoint=False)\n        pop_a = r_a * np.c_[np.cos(t_a), np.sin(t_a)]\n\n        t_b = np.linspace(0, 2 * np.pi, n_b, endpoint=False)\n        pop_b = r_b * np.c_[np.cos(t_b), np.sin(t_b)]\n\n        X = np.vstack([pop_a, pop_b])\n\n        # Step 2: Kernel Matrix Construction\n        # K_ij = (alpha * x_i^T x_j + beta)^delta\n        K = (alpha * (X @ X.T) + beta) ** delta\n\n        # Step 3: Kernel Matrix Centering\n        # K_c = H K H, where H = I - 1/N * 1_N\n        one_n = np.ones((N, N)) / N\n        K_c = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n        # Step 4: Eigendecomposition\n        # Use eigh for symmetric matrices. It returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(K_c)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        # Step 5: Data Embedding\n        # Select top m components with positive eigenvalues\n        valid_indices = np.where(eigenvalues > 1e-9)[0]\n        if len(valid_indices) < m:\n            # If not enough positive eigenvalues, separability might be ambiguous\n            # or impossible. It is safe to assume not separable.\n            results.append(False)\n            continue\n            \n        top_m_indices = valid_indices[:m]\n        lambdas_m = eigenvalues[top_m_indices]\n        V_m = eigenvectors[:, top_m_indices]\n\n        # Z_il = V_il / sqrt(lambda_l)\n        Z = V_m / np.sqrt(lambdas_m)\n        \n        # Get the embedding for each population\n        Z_a = Z[:n_a, :]\n        Z_b = Z[n_a:, :]\n\n        # Step 6: Linear Separability Test using Linear Programming\n        # We want to find w, b such that:\n        # w^T z + b >= 1 for z in Z_a\n        # w^T z + b <= -1 for z in Z_b\n        #\n        # In linprog format (A_ub @ x <= b_ub):\n        # -w^T z - b <= -1 for z in Z_a\n        #  w^T z + b <= -1 for z in Z_b\n        # where x = [w_1, ..., w_m, b]\n        \n        # The number of variables for linprog is m (for w) + 1 (for b)\n        num_vars = m + 1\n        \n        # The objective function is irrelevant; we only care about feasibility.\n        c = np.zeros(num_vars)\n        \n        # Construct the inequality constraints matrix A_ub and vector b_ub\n        A_ub = np.zeros((N, num_vars))\n        \n        # Constraints for population A\n        A_ub[:n_a, :m] = -Z_a\n        A_ub[:n_a, m] = -1\n        \n        # Constraints for population B\n        A_ub[n_a:, :m] = Z_b\n        A_ub[n_a:, m] = 1\n        \n        b_ub = np.full(N, -1.0)\n\n        # The variables w and b are unbounded.\n        bounds = (None, None)\n        \n        # Solve the linear program\n        # The 'highs' method is robust and recommended for new code.\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        \n        # res.success is True if a feasible solution was found.\n        results.append(res.success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2416090"}]}