{"hands_on_practices": [{"introduction": "Observational data can often be misleading, and naively aggregating data from different sources can lead to paradoxical conclusions. This exercise provides a hands-on calculation to demonstrate Simpson's paradox, a statistical phenomenon where a trend appearing in several groups of data disappears or reverses when the groups are combined. By working through this classic example of confounding, you will gain a deeper appreciation for the critical importance of identifying and accounting for underlying variables before drawing causal inferences from correlational data [@problem_id:2383010].", "problem": "A computational biologist is auditing outcomes from a curated clinical dataset evaluating a new antimicrobial drug. Patients were stratified by disease severity into two strata: mild and severe. For each stratum and treatment arm (drug versus control), the number of patients who recovered and the total number of patients are reported as follows: mild stratum, drug arm: $18$ recovered out of $20$; mild stratum, control arm: $160$ recovered out of $200$; severe stratum, drug arm: $240$ recovered out of $800$; severe stratum, control arm: $4$ recovered out of $20$. Define the recovery rate in any group as the ratio of the number of recoveries to the total number of patients in that group. Define the pooled recovery rate for an arm as the ratio of the total number of recoveries across both strata to the total number of patients across both strata. Define the pooled risk difference as the pooled recovery rate of the drug arm minus the pooled recovery rate of the control arm. Compute the pooled risk difference when the stratum labels are ignored and all patients within each arm are pooled. Express your answer as a decimal fraction and round your answer to four significant figures.", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, and contains all necessary information for a unique solution. We shall proceed with the analysis.\n\nThe objective is to compute the pooled risk difference between the drug and control arms, which requires aggregating the data across the specified strata. We are given data for two strata, mild and severe, for both a drug arm and a control arm.\n\nLet us define the variables systematically. Let $R$ denote the number of recovered patients and $N$ denote the total number of patients. We use subscripts to denote the arm (D for drug, C for control) and the stratum (m for mild, s for severe).\n\nThe given data are:\n-   Mild stratum, drug arm: $R_{D,m} = 18$, $N_{D,m} = 20$.\n-   Mild stratum, control arm: $R_{C,m} = 160$, $N_{C,m} = 200$.\n-   Severe stratum, drug arm: $R_{D,s} = 240$, $N_{D,s} = 800$.\n-   Severe stratum, control arm: $R_{C,s} = 4$, $N_{C,s} = 20$.\n\nThe recovery rate, $r$, within any specific group is defined as the ratio of recoveries to the total patients, $r = \\frac{R}{N}$. Let us first compute the recovery rates within each stratum as a preliminary analysis, although the problem asks for the pooled result.\n\nFor the mild stratum:\n-   Drug arm recovery rate: $r_{D,m} = \\frac{R_{D,m}}{N_{D,m}} = \\frac{18}{20} = 0.9$.\n-   Control arm recovery rate: $r_{C,m} = \\frac{R_{C,m}}{N_{C,m}} = \\frac{160}{200} = 0.8$.\nThe risk difference in the mild stratum is $r_{D,m} - r_{C,m} = 0.9 - 0.8 = 0.1$.\n\nFor the severe stratum:\n-   Drug arm recovery rate: $r_{D,s} = \\frac{R_{D,s}}{N_{D,s}} = \\frac{240}{800} = 0.3$.\n-   Control arm recovery rate: $r_{C,s} = \\frac{R_{C,s}}{N_{C,s}} = \\frac{4}{20} = 0.2$.\nThe risk difference in the severe stratum is $r_{D,s} - r_{C,s} = 0.3 - 0.2 = 0.1$.\n\nObserve that in both strata, the drug arm shows a higher recovery rate by $0.1$ or $10$ percentage points. A naive interpretation would be that the drug is beneficial.\n\nNow, we proceed to the main task: computing the pooled risk difference. This involves ignoring the stratum labels and aggregating the data for each arm.\n\nFirst, calculate the total number of recoveries and total patients for the pooled drug arm.\n-   Total recoveries in the drug arm: $R_D = R_{D,m} + R_{D,s} = 18 + 240 = 258$.\n-   Total patients in the drug arm: $N_D = N_{D,m} + N_{D,s} = 20 + 800 = 820$.\n\nThe pooled recovery rate for the drug arm, $RR_D$, is therefore:\n$$RR_D = \\frac{R_D}{N_D} = \\frac{258}{820}$$\n\nNext, calculate the total number of recoveries and total patients for the pooled control arm.\n-   Total recoveries in the control arm: $R_C = R_{C,m} + R_{C,s} = 160 + 4 = 164$.\n-   Total patients in the control arm: $N_C = N_{C,m} + N_{C,s} = 200 + 20 = 220$.\n\nThe pooled recovery rate for the control arm, $RR_C$, is:\n$$RR_C = \\frac{R_C}{N_C} = \\frac{164}{220}$$\n\nThe pooled risk difference, $\\Delta_{RD}$, is defined as the difference between the pooled recovery rate of the drug arm and the pooled recovery rate of the control arm.\n$$\\Delta_{RD} = RR_D - RR_C = \\frac{258}{820} - \\frac{164}{220}$$\n\nNow we evaluate this expression numerically.\n$$RR_D = \\frac{258}{820} \\approx 0.3146341...$$\n$$RR_C = \\frac{164}{220} = \\frac{41}{55} \\approx 0.7454545...$$\n\nSubstituting these values into the expression for the pooled risk difference:\n$$\\Delta_{RD} \\approx 0.3146341 - 0.7454545 = -0.4308204...$$\n\nThe problem requires the answer to be rounded to four significant figures. The first significant figure is $4$. We must keep the next three digits: $3$, $0$, and $8$. The digit following the fourth significant figure ($8$) is $2$. Since $2 < 5$, we do not round up the last digit.\nTherefore, the pooled risk difference rounded to four significant figures is $-0.4308$.\n\nThis result demonstrates a classic case of Simpson's paradox. The confounding variable is the disease severity. The drug appears effective when analyzed within each stratum, but when the data are pooled, the conclusion is reversed, suggesting the drug is harmful. This is because the treatment assignment is heavily skewed: most patients on the drug were in the severe category (who have a low recovery rate regardless of treatment), while most patients in the control group were in the mild category (who have a high recovery rate regardless of treatment). Uncritically pooling the data leads to a fallacious conclusion. Any serious analysis must account for such confounding factors.", "answer": "$$\n\\boxed{-0.4308}\n$$", "id": "2383010"}, {"introduction": "To move from identifying confounding to formally reasoning about it, we need a mathematical language for causality. This practice introduces the use of Bayesian networks and Pearl's do-calculus to distinguish between observational probability, $P(C=1|S=1)$, and interventional probability, $P(C=1|do(S=1))$. By modeling the causal relationships between a gene, a behavior, and a disease outcome, you will calculate the confounding bias and see precisely how a common cause can create a spurious association between two variables [@problem_id:2382934].", "problem": "You are given a discrete Bayesian network with three binary random variables that model a genetic and behavioral influence on disease risk in a population: genotype at a single-nucleotide polymorphism in the cholinergic receptor nicotinic alpha 5 subunit gene (CHRNA5), smoking status, and lung cancer outcome. Let $G \\in \\{0,1\\}$ denote the genotype indicator for the risk allele at CHRNA5, with $G=1$ indicating the presence of at least one risk allele. Let $S \\in \\{0,1\\}$ denote smoking status, with $S=1$ indicating a current smoker. Let $C \\in \\{0,1\\}$ denote lung cancer, with $C=1$ indicating presence of disease. The directed acyclic graph (DAG) structure is $G \\to S$, $S \\to C$, and optionally $G \\to C$. The joint distribution factorizes as $P(G,S,C) = P(G) P(S \\mid G) P(C \\mid S,G)$.\n\nParameterization is given by the following probabilities for each test case:\n- $P(G=1) = p_g$,\n- $P(S=1 \\mid G=0) = s_0$, $P(S=1 \\mid G=1) = s_1$,\n- $P(C=1 \\mid S=s, G=g) = c_{sg}$ for $(s,g) \\in \\{0,1\\} \\times \\{0,1\\}$.\n\nInterventions are defined using the do-operator. Under an intervention $do(S=1)$, the variable $S$ is set to $1$ exogenously, which removes any incoming arrows into $S$ and replaces the conditional distribution of $S$ with a point mass at $1$, while leaving all other conditional distributions unchanged.\n\nFor each test case below, compute the following scalar quantity:\n- The difference $\\Delta = P(C=1 \\mid S=1) - P(C=1 \\mid do(S=1))$.\n\nYour program must compute $\\Delta$ for each test case from first principles using the definitions above and the laws of probability, without any external data. Express each $\\Delta$ as a decimal number rounded to six decimal places (do not use a percentage sign).\n\nTest suite (each case is specified by $(p_g, s_0, s_1, c_{00}, c_{10}, c_{01}, c_{11})$):\n1. Case A (genetic effect on smoking and direct genetic effect on lung cancer present, smoking causes lung cancer): $(p_g, s_0, s_1, c_{00}, c_{10}, c_{01}, c_{11}) = (0.3, 0.2, 0.6, 0.01, 0.05, 0.02, 0.12)$.\n2. Case B (genetic effect on smoking present, no direct genetic effect on lung cancer, smoking causes lung cancer): $(p_g, s_0, s_1, c_{00}, c_{10}, c_{01}, c_{11}) = (0.3, 0.2, 0.6, 0.01, 0.05, 0.01, 0.05)$.\n3. Case C (genetic effect on smoking and direct genetic effect on lung cancer present, no causal effect of smoking on lung cancer): $(p_g, s_0, s_1, c_{00}, c_{10}, c_{01}, c_{11}) = (0.3, 0.2, 0.6, 0.01, 0.01, 0.05, 0.05)$.\n4. Case D (no genetic effect on smoking, genetic and smoking effects on lung cancer present): $(p_g, s_0, s_1, c_{00}, c_{10}, c_{01}, c_{11}) = (0.3, 0.4, 0.4, 0.01, 0.05, 0.02, 0.12)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above, for example $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$.", "solution": "The problem requires the computation of the quantity $\\Delta = P(C=1 \\mid S=1) - P(C=1 \\mid do(S=1))$ for a given Bayesian network. This quantity represents the difference between the observational conditional probability of lung cancer given smoking and the causal probability of lung cancer under an intervention that forces smoking. This difference is precisely the confounding bias introduced by the gene $G$, which is a potential common cause of smoking $S$ and lung cancer $C$.\n\nThe analysis proceeds in three steps: first, deriving the expression for the observational term $P(C=1 \\mid S=1)$; second, deriving the expression for the interventional term $P(C=1 \\mid do(S=1))$; and third, combining these to find $\\Delta$ and applying it to the given test cases.\n\nThe joint probability distribution is given by the factorization $P(G,S,C) = P(G) P(S \\mid G) P(C \\mid S,G)$. The parameters are defined as $P(G=1) = p_g$, $P(S=1 \\mid G=0) = s_0$, $P(S=1 \\mid G=1) = s_1$, and $P(C=1 \\mid S=s, G=g) = c_{sg}$.\n\n1.  Derivation of the observational term $P(C=1 \\mid S=1)$:\n    By definition of conditional probability,\n    $$ P(C=1 \\mid S=1) = \\frac{P(C=1, S=1)}{P(S=1)} $$\n    The denominator, $P(S=1)$, is found by marginalizing over the variable $G$ using the law of total probability:\n    $$ P(S=1) = \\sum_{g \\in \\{0,1\\}} P(S=1, G=g) = \\sum_{g \\in \\{0,1\\}} P(S=1 \\mid G=g) P(G=g) $$\n    $$ P(S=1) = P(S=1 \\mid G=0)P(G=0) + P(S=1 \\mid G=1)P(G=1) $$\n    Substituting the given parameters, with $P(G=0) = 1 - p_g$:\n    $$ P(S=1) = s_0 (1 - p_g) + s_1 p_g $$\n    The numerator, $P(C=1, S=1)$, is also found by marginalizing over $G$:\n    $$ P(C=1, S=1) = \\sum_{g \\in \\{0,1\\}} P(C=1, S=1, G=g) $$\n    Using the chain rule according to the network structure:\n    $$ P(C=1, S=1) = P(C=1 \\mid S=1, G=0)P(S=1 \\mid G=0)P(G=0) + P(C=1 \\mid S=1, G=1)P(S=1 \\mid G=1)P(G=1) $$\n    Substituting the given parameters:\n    $$ P(C=1, S=1) = c_{10} s_0 (1 - p_g) + c_{11} s_1 p_g $$\n    Combining the numerator and denominator gives the observational probability:\n    $$ P(C=1 \\mid S=1) = \\frac{c_{10} s_0 (1 - p_g) + c_{11} s_1 p_g}{s_0 (1 - p_g) + s_1 p_g} $$\n\n2.  Derivation of the interventional term $P(C=1 \\mid do(S=1))$:\n    The intervention $do(S=1)$ modifies the system by setting the value of $S$ to $1$ for the entire population. This corresponds to removing all incoming edges to node $S$ in the graph (in this case, the edge $G \\to S$) and setting $S=1$. The joint distribution of the remaining variables $G$ and $C$ in this modified graph, denoted $P_{do(S=1)}$, becomes $P_{do(S=1)}(G, C) = P(G) P(C \\mid S=1, G)$.\n    The desired probability is the marginal probability of $C=1$ in this new distribution:\n    $$ P(C=1 \\mid do(S=1)) = \\sum_{g \\in \\{0,1\\}} P_{do(S=1)}(C=1, G=g) $$\n    $$ P(C=1 \\mid do(S=1)) = \\sum_{g \\in \\{0,1\\}} P(C=1 \\mid S=1, G=g)P(G=g) $$\n    This is the back-door adjustment formula for the causal effect of $S$ on $C$, with $G$ as the confounding variable.\n    Substituting the parameters:\n    $$ P(C=1 \\mid do(S=1)) = P(C=1 \\mid S=1, G=0)P(G=0) + P(C=1 \\mid S=1, G=1)P(G=1) $$\n    $$ P(C=1 \\mid do(S=1)) = c_{10}(1 - p_g) + c_{11} p_g $$\n\n3.  Calculation of $\\Delta$:\n    The final expression for $\\Delta$ is the difference between the two derived terms:\n    $$ \\Delta = \\frac{c_{10} s_0 (1 - p_g) + c_{11} s_1 p_g}{s_0 (1 - p_g) + s_1 p_g} - \\left( c_{10}(1 - p_g) + c_{11} p_g \\right) $$\n    This expression can be simplified by algebraic manipulation. Let the denominator be $D = s_0 (1 - p_g) + s_1 p_g$. After placing both terms over the common denominator $D$ and simplifying the numerator, we arrive at a more insightful form:\n    $$ \\Delta = \\frac{p_g(1-p_g)(s_1 - s_0)(c_{11} - c_{10})}{s_0 (1 - p_g) + s_1 p_g} $$\n    This simplified formula shows that the confounding bias $\\Delta$ is non-zero if and only if:\n    a) The gene is polymorphic ($0 < p_g < 1$).\n    b) The gene influences smoking behavior ($s_1 \\neq s_0$), creating the path $G \\to S$.\n    c) The gene influences lung cancer risk differently for different alleles among smokers ($c_{11} \\neq c_{10}$), indicating a $G \\to C$ path or an interaction. This means $G$ is a common cause of $S$ and $C$.\n\nApplying this formula to each test case:\n\nCase A: $(p_g, s_0, s_1, c_{10}, c_{11}) = (0.3, 0.2, 0.6, 0.05, 0.12)$\n$$ \\Delta_A = \\frac{0.3 \\times (1-0.3) \\times (0.6 - 0.2) \\times (0.12 - 0.05)}{(1-0.3) \\times 0.2 + 0.3 \\times 0.6} = \\frac{0.3 \\times 0.7 \\times 0.4 \\times 0.07}{0.7 \\times 0.2 + 0.3 \\times 0.6} = \\frac{0.00588}{0.32} = 0.018375 $$\n\nCase B: $(p_g, s_0, s_1, c_{10}, c_{11}) = (0.3, 0.2, 0.6, 0.05, 0.05)$\nHere, $c_{11} - c_{10} = 0.05 - 0.05 = 0$. The numerator becomes $0$.\n$$ \\Delta_B = 0 $$\nThis is expected. The condition $c_{s0}=c_{s1}$ for both $s=0$ and $s=1$ signifies no direct causal path $G \\to C$. Thus, $G$ is not a confounder for the $S \\to C$ relationship.\n\nCase C: $(p_g, s_0, s_1, c_{10}, c_{11}) = (0.3, 0.2, 0.6, 0.01, 0.05)$\n$$ \\Delta_C = \\frac{0.3 \\times (1-0.3) \\times (0.6 - 0.2) \\times (0.05 - 0.01)}{(1-0.3) \\times 0.2 + 0.3 \\times 0.6} = \\frac{0.3 \\times 0.7 \\times 0.4 \\times 0.04}{0.32} = \\frac{0.00336}{0.32} = 0.0105 $$\nIn this case, an association between smoking and cancer exists due to confounding by $G$, even though smoking itself has no causal effect on cancer (as $c_{0g}=c_{1g}$).\n\nCase D: $(p_g, s_0, s_1, c_{10}, c_{11}) = (0.3, 0.4, 0.4, 0.05, 0.12)$\nHere, $s_1 - s_0 = 0.4 - 0.4 = 0$. The numerator becomes $0$.\n$$ \\Delta_D = 0 $$\nThis is also expected. The condition $s_0 = s_1$ signifies that the gene $G$ has no effect on smoking behavior, breaking the $G \\to S$ path. Without this path, $G$ cannot be a common cause and thus cannot be a confounder.\n\nThe results, rounded to six decimal places, are: $\\Delta_A=0.018375$, $\\Delta_B=0.000000$, $\\Delta_C=0.010500$, and $\\Delta_D=0.000000$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the difference between associational and\n    causal probabilities in a given Bayesian network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (p_g, s_0, s_1, c_00, c_10, c_01, c_11)\n    test_cases = [\n        (0.3, 0.2, 0.6, 0.01, 0.05, 0.02, 0.12),  # Case A\n        (0.3, 0.2, 0.6, 0.01, 0.05, 0.01, 0.05),  # Case B\n        (0.3, 0.2, 0.6, 0.01, 0.01, 0.05, 0.05),  # Case C\n        (0.3, 0.4, 0.4, 0.01, 0.05, 0.02, 0.12),  # Case D\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters for the current test case.\n        # Notation from problem statement:\n        # pg: P(G=1)\n        # s0: P(S=1 | G=0)\n        # s1: P(S=1 | G=1)\n        # c10: P(C=1 | S=1, G=0)\n        # c11: P(C=1 | S=1, G=1)\n        # c00 and c01 are not needed for this specific calculation.\n        pg, s0, s1, c00, c10, c01, c11 = case\n\n        # The quantity to compute is Delta = P(C=1|S=1) - P(C=1|do(S=1)).\n        # This difference represents the confounding bias.\n        # A simplified formula derived from first principles is used:\n        # Delta = (pg * (1-pg) * (s1-s0) * (c11-c10)) / P(S=1)\n        # where P(S=1) = (1-pg)*s0 + pg*s1.\n        \n        # Calculate the denominator, P(S=1).\n        prob_s1 = (1.0 - pg) * s0 + pg * s1\n\n        # The problem statement implies P(S=1) will not be zero for any test case,\n        # which would make the observational probability P(C=1|S=1) undefined.\n        # For the given cases, P(S=1) is always positive.\n        if prob_s1 == 0.0:\n            # If P(S=1) is 0, the subpopulation of smokers is empty.\n            # The confounding bias is taken to be 0 in this edge case.\n            delta = 0.0\n        else:\n            # Calculate the numerator of the simplified formula.\n            # This represents the covariance between the genetic effect on smoking\n            # and the genetic effect on cancer risk among a population of smokers.\n            numerator = pg * (1.0 - pg) * (s1 - s0) * (c11 - c10)\n            \n            # Calculate Delta, the confounding bias.\n            delta = numerator / prob_s1\n        \n        # Format the result to six decimal places as required.\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2382934"}, {"introduction": "In many biological systems, it is impossible to measure all potential confounders, posing a significant challenge to causal inference. This exercise introduces Mendelian Randomization (MR), a powerful method that uses genetic variants as instrumental variables to estimate causal effects in the presence of unobserved confounding. By simulating a gene regulatory network and applying a two-stage least squares (2SLS) regression, you will see how a valid instrument can recover the true causal effect, contrasting it with the biased estimate from a standard regression model [@problem_id:2382981].", "problem": "You are given a mathematically specified, linear structural causal model for a simplified transcriptional regulation scenario, motivated by Mendelian Randomization (MR). The variables are: a genetic instrument $Z$ (allele dosage for a single nucleotide polymorphism in Hardy–Weinberg equilibrium), a transcription factor expression $X$, a target gene expression $Y$, and an unobserved confounder $U$ that affects both $X$ and $Y$. For individual $i \\in \\{1,\\dots,n\\}$, the data-generating process is\n$$\nZ_i \\sim \\text{Binomial}(2,p), \\quad U_i \\sim \\mathcal{N}(0,1), \\quad \\varepsilon^X_i \\sim \\mathcal{N}(0,\\sigma_X^2), \\quad \\varepsilon^Y_i \\sim \\mathcal{N}(0,\\sigma_Y^2),\n$$\nindependently across $i$, and\n$$\nX_i \\;=\\; \\alpha Z_i \\;+\\; \\delta U_i \\;+\\; \\varepsilon^X_i, \\qquad\nY_i \\;=\\; \\beta X_i \\;+\\; \\kappa U_i \\;+\\; \\gamma Z_i \\;+\\; \\varepsilon^Y_i.\n$$\nAll intercepts are zero. The causal hypothesis of interest is that $X$ has a causal effect on $Y$ encoded by the structural parameter $\\beta$. The parameter $\\gamma$ encodes a possible direct effect of $Z$ on $Y$ not mediated by $X$ (violation of the instrumental variable exclusion restriction). The parameters $\\delta$ and $\\kappa$ encode unobserved confounding.\n\nYour task is to, for each parameter set in the test suite below, generate a sample of size $n$ from the model, and from that sample compute the following quantities:\n- The identified coefficient $\\widehat{\\beta}_{\\mathrm{IV}}$ for $X$ in the linear structural equation for $Y$ when $X$ is instrumented by $Z$ under the model above, obtained by projecting the regressor for $Y$ onto the column space of the instrument and solving the corresponding normal equations.\n- A two-sided $p$-value for testing the null hypothesis $H_0:\\beta=0$ for the identified coefficient above, using the homoskedastic linear model with Student’s $t$ distribution and degrees of freedom $n-k$, where $k=2$ is the number of coefficients in the structural equation (intercept and slope).\n- The first-stage $F$-statistic for testing the null hypothesis that the coefficient of $Z$ in the linear projection of $X$ on the instrument space is zero. Use the standard $F$ statistic for a single instrument with an intercept, with degrees of freedom $\\left(1, n-2\\right)$.\n- The ordinary least squares coefficient $\\widehat{\\beta}_{\\mathrm{OLS}}$ for regressing $Y$ on $X$ with an intercept.\n\nAll quantities must be computed from the sample and expressed as real numbers. No physical units are involved. Whenever a significance level is required, use $\\alpha=0.05$ as a decimal. Random sampling must be reproducible: for each test case use a pseudo-random number generator initialized with the provided integer seed $s$.\n\nTest suite (four cases):\n- Case A (valid instrument, nonzero causal effect): $n=5000$, $p=0.3$, $\\alpha=0.8$, $\\beta=1.2$, $\\delta=1.0$, $\\kappa=1.0$, $\\gamma=0.0$, $\\sigma_X=1.0$, $\\sigma_Y=1.0$, $s=2021$.\n- Case B (valid instrument, null causal effect): $n=5000$, $p=0.3$, $\\alpha=0.8$, $\\beta=0.0$, $\\delta=1.0$, $\\kappa=1.0$, $\\gamma=0.0$, $\\sigma_X=1.0$, $\\sigma_Y=1.0$, $s=2022$.\n- Case C (invalid instrument via direct effect): $n=5000$, $p=0.3$, $\\alpha=0.8$, $\\beta=1.0$, $\\delta=1.0$, $\\kappa=1.0$, $\\gamma=0.6$, $\\sigma_X=1.0$, $\\sigma_Y=1.0$, $s=2023$.\n- Case D (weak instrument): $n=5000$, $p=0.3$, $\\alpha=0.05$, $\\beta=1.0$, $\\delta=1.0$, $\\kappa=1.0$, $\\gamma=0.0$, $\\sigma_X=1.0$, $\\sigma_Y=1.0$, $s=2024$.\n\nFinal output format:\n- For each test case, return the list $\\left[\\widehat{\\beta}_{\\mathrm{IV}},\\, p\\text{-value}_{\\mathrm{IV}},\\, F_{\\text{first-stage}},\\, \\widehat{\\beta}_{\\mathrm{OLS}}\\right]$, with each entry rounded to exactly $4$ decimal places.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list of these lists, enclosed in square brackets (for example, $\\left[\\left[a,b,c,d\\right],\\left[\\dots\\right],\\left[\\dots\\right],\\left[\\dots\\right]\\right]$), with all numbers rounded as specified.", "solution": "The problem is scientifically grounded, well-posed, and all variables and procedures are defined with sufficient mathematical precision. It describes a simulation study based on a linear Structural Causal Model, a standard tool in causal inference, to evaluate the performance of Ordinary Least Squares (OLS) and Instrumental Variable (IV) estimators under different scenarios relevant to Mendelian Randomization. The problem is valid.\n\nThe task is to generate data from the specified model for four different parameter sets and compute four statistical quantities for each set: the IV estimate $\\widehat{\\beta}_{\\mathrm{IV}}$, its corresponding $p$-value, the first-stage $F$-statistic, and the OLS estimate $\\widehat{\\beta}_{\\mathrm{OLS}}$.\n\nThe solution proceeds in five stages:\n1. Data Generation: For each test case, we generate a sample of size $n$ according to the specified distributions and structural equations.\n2. OLS Estimation: We compute the OLS estimate $\\widehat{\\beta}_{\\mathrm{OLS}}$ by regressing $Y$ on $X$ and an intercept.\n3. IV Estimation: We compute the IV estimate $\\widehat{\\beta}_{\\mathrm{IV}}$ using the two-stage least squares (2SLS) procedure.\n4. IV Inference: We compute the standard error of $\\widehat{\\beta}_{\\mathrm{IV}}$ to find the $t$-statistic and the corresponding $p$-value for the null hypothesis $H_0: \\beta=0$.\n5. First-Stage Diagnostic: We compute the $F$-statistic for the first-stage regression to assess instrument strength.\n\nLet vectors $\\mathbf{y}$, $\\mathbf{x}$, $\\mathbf{z}$ of length $n$ represent the sampled data $\\{Y_i\\}_{i=1}^n$, $\\{X_i\\}_{i=1}^n$, and $\\{Z_i\\}_{i=1}^n$, respectively. Let $\\mathbf{1}$ be a vector of ones of length $n$.\n\n**1. Data Generation**\nFor each individual $i \\in \\{1, \\dots, n\\}$, we generate the variables as specified:\n- Genetic instrument: $Z_i \\sim \\text{Binomial}(2,p)$\n- Unobserved confounder: $U_i \\sim \\mathcal{N}(0,1)$\n- Error terms: $\\varepsilon^X_i \\sim \\mathcal{N}(0,\\sigma_X^2)$ and $\\varepsilon^Y_i \\sim \\mathcal{N}(0,\\sigma_Y^2)$\nUsing these, we construct the observable variables $X_i$ and $Y_i$:\n$$\nX_i \\;=\\; \\alpha Z_i \\;+\\; \\delta U_i \\;+\\; \\varepsilon^X_i\n$$\n$$\nY_i \\;=\\; \\beta X_i \\;+\\; \\kappa U_i \\;+\\; \\gamma Z_i \\;+\\; \\varepsilon^Y_i\n$$\nA pseudo-random number generator initialized with the seed $s$ ensures reproducibility for each case.\n\n**2. Ordinary Least Squares (OLS) Estimation**\nThe OLS model includes an intercept: $Y_i = b_0 + b_1 X_i + e_i$. In matrix form, this is $\\mathbf{y} = \\mathbf{X}_{\\text{OLS}} \\mathbf{b} + \\mathbf{e}$, where $\\mathbf{X}_{\\text{OLS}} = [\\mathbf{1}, \\mathbf{x}]$ is the $n \\times 2$ design matrix.\nThe OLS estimator for $\\mathbf{b} = [b_0, b_1]'$ is given by the solution to the normal equations:\n$$\n\\widehat{\\mathbf{b}}_{\\mathrm{OLS}} = (\\mathbf{X}_{\\text{OLS}}' \\mathbf{X}_{\\text{OLS}})^{-1} \\mathbf{X}_{\\text{OLS}}' \\mathbf{y}\n$$\nThe required quantity is the slope coefficient, $\\widehat{\\beta}_{\\mathrm{OLS}} = \\widehat{b}_1$.\n\n**3. Instrumental Variable (IV) Estimation**\nThe IV estimation is performed using two-stage least squares (2SLS).\nThe structural equation for $Y$ is $Y_i = \\beta_0 + \\beta_1 X_i + \\eta_i$. The instrument for the endogenous regressor $X$ is $Z$. We include an intercept in all regressions.\nThe regressor matrix is $\\mathbf{X}_{\\text{reg}} = [\\mathbf{1}, \\mathbf{x}]$ and the instrument matrix is $\\mathbf{Z}_{\\text{inst}} = [\\mathbf{1}, \\mathbf{z}]$.\n\nFirst Stage: Project the regressors onto the space spanned by the instruments. The projection matrix is $\\mathbf{P_Z} = \\mathbf{Z}_{\\text{inst}} (\\mathbf{Z}_{\\text{inst}}' \\mathbf{Z}_{\\text{inst}})^{-1} \\mathbf{Z}_{\\text{inst}}'$. The projected regressors are $\\widehat{\\mathbf{X}}_{\\text{reg}} = \\mathbf{P_Z} \\mathbf{X}_{\\text{reg}}$.\n\nSecond Stage: The 2SLS estimator $\\widehat{\\mathbf{\\beta}}_{\\mathrm{IV}} = [\\widehat{\\beta}_0, \\widehat{\\beta}_1]'$ is obtained by regressing $\\mathbf{y}$ on $\\widehat{\\mathbf{X}}_{\\text{reg}}$. However, a more direct and standard formula is:\n$$\n\\widehat{\\mathbf{\\beta}}_{\\mathrm{IV}} = (\\mathbf{X}_{\\text{reg}}' \\mathbf{P_Z} \\mathbf{X}_{\\text{reg}})^{-1} \\mathbf{X}_{\\text{reg}}' \\mathbf{P_Z} \\mathbf{y}\n$$\nThe estimate of the causal effect is the slope coefficient, which we denote $\\widehat{\\beta}_{\\mathrm{IV}}$ (abusing notation slightly, this is $\\widehat{\\beta}_1$ from the vector).\n\n**4. P-value for $\\widehat{\\beta}_{\\mathrm{IV}}$**\nTo test the null hypothesis $H_0: \\beta=0$, we compute a $t$-statistic.\nFirst, we calculate the residuals using the estimated coefficients and the original regressors:\n$$\n\\mathbf{e}_{\\mathrm{IV}} = \\mathbf{y} - \\mathbf{X}_{\\text{reg}} \\widehat{\\mathbf{\\beta}}_{\\mathrm{IV}}\n$$\nUnder the assumption of homoskedasticity, the estimated variance of the regression error is:\n$$\n\\widehat{\\sigma}^2_{\\mathrm{IV}} = \\frac{\\mathbf{e}_{\\mathrm{IV}}' \\mathbf{e}_{\\mathrm{IV}}}{n-k}\n$$\nwhere $k=2$ is the number of parameters (intercept and slope). The variance-covariance matrix of the estimator is:\n$$\n\\widehat{\\text{Var}}(\\widehat{\\mathbf{\\beta}}_{\\mathrm{IV}}) = \\widehat{\\sigma}^2_{\\mathrm{IV}} (\\mathbf{X}_{\\text{reg}}' \\mathbf{P_Z} \\mathbf{X}_{\\text{reg}})^{-1}\n$$\nThe standard error of the slope coefficient, $SE(\\widehat{\\beta}_{\\mathrm{IV}})$, is the square root of the second diagonal element of this matrix. The $t$-statistic is:\n$$\nt = \\frac{\\widehat{\\beta}_{\\mathrm{IV}}}{SE(\\widehat{\\beta}_{\\mathrm{IV}})}\n$$\nThe two-sided $p$-value is calculated using the Student's $t$-distribution with $n-k = n-2$ degrees of freedom:\n$$\np\\text{-value}_{\\mathrm{IV}} = 2 \\cdot P(T_{n-2} > |t|)\n$$\n\n**5. First-Stage F-statistic**\nThe first-stage regression is $X_i = \\pi_0 + \\pi_1 Z_i + \\nu_i$. We need to test the null hypothesis $H_0: \\pi_1 = 0$.\nThe $F$-statistic compares the full model (with $Z_i$) to a restricted model (with only an intercept).\nLet $RSS_1$ be the residual sum of squares from the full first-stage regression ($X$ on $\\mathbf{1}$ and $\\mathbf{z}$). Let $TSS$ be the total sum of squares for $X$, which is the residual sum of squares from regressing $X$ on only an intercept. The formula for the $F$-statistic with $1$ instrument and an intercept is:\n$$\nF_{\\text{first-stage}} = \\frac{(TSS - RSS_1) / (2-1)}{RSS_1 / (n-2)} = \\frac{TSS - RSS_1}{RSS_1 / (n-2)}\n$$\nThis statistic follows an $F$-distribution with $(1, n-2)$ degrees of freedom under the null hypothesis.\n\nThese five steps will be implemented for each of the four test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, simulating data,\n    and calculating the required statistical quantities.\n    \"\"\"\n    test_cases = [\n        # Case A (valid instrument, nonzero causal effect)\n        {'n': 5000, 'p': 0.3, 'alpha': 0.8, 'beta': 1.2, 'delta': 1.0, 'kappa': 1.0, 'gamma': 0.0, 'sigma_X': 1.0, 'sigma_Y': 1.0, 's': 2021},\n        # Case B (valid instrument, null causal effect)\n        {'n': 5000, 'p': 0.3, 'alpha': 0.8, 'beta': 0.0, 'delta': 1.0, 'kappa': 1.0, 'gamma': 0.0, 'sigma_X': 1.0, 'sigma_Y': 1.0, 's': 2022},\n        # Case C (invalid instrument via direct effect)\n        {'n': 5000, 'p': 0.3, 'alpha': 0.8, 'beta': 1.0, 'delta': 1.0, 'kappa': 1.0, 'gamma': 0.6, 'sigma_X': 1.0, 'sigma_Y': 1.0, 's': 2023},\n        # Case D (weak instrument)\n        {'n': 5000, 'p': 0.3, 'alpha': 0.05, 'beta': 1.0, 'delta': 1.0, 'kappa': 1.0, 'gamma': 0.0, 'sigma_X': 1.0, 'sigma_Y': 1.0, 's': 2024},\n    ]\n\n    results = []\n    for params in test_cases:\n        n = params['n']\n        p = params['p']\n        alpha = params['alpha']\n        beta = params['beta']\n        delta = params['delta']\n        kappa = params['kappa']\n        gamma = params['gamma']\n        sigma_X = params['sigma_X']\n        sigma_Y = params['sigma_Y']\n        s = params['s']\n\n        # 1. Data Generation\n        rng = np.random.default_rng(s)\n        Z = rng.binomial(2, p, size=n)\n        U = rng.normal(0, 1, size=n)\n        eps_X = rng.normal(0, sigma_X, size=n)\n        eps_Y = rng.normal(0, sigma_Y, size=n)\n        \n        X = alpha * Z + delta * U + eps_X\n        Y = beta * X + kappa * U + gamma * Z + eps_Y\n\n        # Prepare matrices for regressions\n        ones = np.ones(n)\n        \n        # OLS matrices\n        X_ols_mat = np.vstack([ones, X]).T\n        \n        # IV/2SLS matrices\n        X_reg_mat = np.vstack([ones, X]).T  # Regressors for 2nd stage\n        Z_inst_mat = np.vstack([ones, Z]).T # Instruments for 1st stage\n\n        # 2. OLS Estimation\n        try:\n            b_ols_hat_vec = np.linalg.solve(X_ols_mat.T @ X_ols_mat, X_ols_mat.T @ Y)\n            beta_ols_hat = b_ols_hat_vec[1]\n        except np.linalg.LinAlgError:\n            beta_ols_hat = np.nan\n\n        # 3. IV Estimation (2SLS)\n        try:\n            # Projection matrix P_Z\n            ZtZ_inv = np.linalg.inv(Z_inst_mat.T @ Z_inst_mat)\n            P_Z = Z_inst_mat @ ZtZ_inv @ Z_inst_mat.T\n            \n            # 2SLS estimator\n            X_reg_T_P_Z = X_reg_mat.T @ P_Z\n            beta_iv_vec = np.linalg.solve(X_reg_T_P_Z @ X_reg_mat, X_reg_T_P_Z @ Y)\n            beta_iv_hat = beta_iv_vec[1]\n\n            # 4. IV p-value calculation\n            k = 2  # Number of parameters in 2nd stage (intercept, slope)\n            df = n - k\n            residuals_iv = Y - X_reg_mat @ beta_iv_vec\n            sigma2_hat_iv = np.sum(residuals_iv**2) / df\n            \n            var_cov_beta_iv = sigma2_hat_iv * np.linalg.inv(X_reg_T_P_Z @ X_reg_mat)\n            se_beta_iv_hat = np.sqrt(var_cov_beta_iv[1, 1])\n            \n            t_stat_iv = beta_iv_hat / se_beta_iv_hat\n            p_value_iv = 2 * t.sf(np.abs(t_stat_iv), df=df)\n\n        except np.linalg.LinAlgError:\n            beta_iv_hat = np.nan\n            p_value_iv = np.nan\n\n        # 5. First-stage F-statistic\n        try:\n            # Full model: X ~ 1 + Z\n            _, rss1, _, _ = np.linalg.lstsq(Z_inst_mat, X, rcond=None)\n            \n            # Restricted model: X ~ 1\n            X_mean = np.mean(X)\n            tss = np.sum((X - X_mean)**2)\n            \n            df_num = 1\n            df_den = n - 2\n            \n            f_stat = ((tss - rss1[0]) / df_num) / (rss1[0] / df_den)\n        except (np.linalg.LinAlgError, IndexError):\n            f_stat = np.nan\n\n        # Store results for the current case\n        results.append([beta_iv_hat, p_value_iv, f_stat, beta_ols_hat])\n\n    # Final print statement in the exact required format.\n    formatted_results = [\n        f\"[{r[0]:.4f},{r[1]:.4f},{r[2]:.4f},{r[3]:.4f}]\" for r in results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2382981"}]}