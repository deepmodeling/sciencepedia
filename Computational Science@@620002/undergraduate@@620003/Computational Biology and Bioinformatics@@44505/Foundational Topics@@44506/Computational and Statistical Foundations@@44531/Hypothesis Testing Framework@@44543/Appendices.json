{"hands_on_practices": [{"introduction": "The theoretical elegance of many statistical tests, like the Student's $t$-test, rests on assumptions that are not always met by real-world biological data. This practice demonstrates the critical concept of statistical robustness by exploring how a single outlier—a common feature in RNA-seq experiments—can dramatically impact the outcome of a parametric test versus a non-parametric alternative. By completing this exercise [@problem_id:2398972], you will gain practical insight into how to choose the appropriate statistical tool for your data, ensuring your conclusions are sound and reliable.", "problem": "Construct a program that, for a fixed gene measured across two biological conditions in Ribonucleic Acid sequencing (RNA-seq), computes the two-sided p-values of both the two-sample Welch $t$-test and the Wilcoxon rank-sum test (also known as the Mann–Whitney $U$ test) and aggregates the results across a specified test suite. Assume each condition produces independent replicate counts for the gene, and treat the provided counts as the observed values. The null hypothesis for both tests is that the two conditions have equal central tendency, stated as $H_0$: the two conditions are sampled from populations with equal means (Welch $t$-test) or equal continuous distributions (Wilcoxon rank-sum test). No transformations are to be applied to the counts. All tests must be two-sided.\n\nYour program must process the following test suite, where each item specifies the counts for condition $A$ and condition $B$:\n\n- Test case $1$ (baseline, no outlier, comparable distributions):\n  - Condition $A$: $\\left(43, 50, 39, 61, 55, 47\\right)$\n  - Condition $B$: $\\left(45, 52, 41, 58, 53, 49\\right)$\n\n- Test case $2$ (single extreme high outlier in condition $B$):\n  - Condition $A$: $\\left(43, 50, 39, 61, 55, 47\\right)$\n  - Condition $B$: $\\left(45, 52, 41, 58, 53, 1000\\right)$\n\n- Test case $3$ (small sample size with a single extreme high outlier in condition $B$):\n  - Condition $A$: $\\left(20, 22, 25\\right)$\n  - Condition $B$: $\\left(19, 21, 400\\right)$\n\n- Test case $4$ (location shift with a single extreme low outlier in condition $B$):\n  - Condition $A$: $\\left(15, 16, 18, 20, 19, 17\\right)$\n  - Condition $B$: $\\left(28, 30, 27, 29, 31, 0\\right)$\n\nFor each test case $i \\in \\{1,2,3,4\\}$, compute:\n- $p_{t,i}$: the two-sided p-value from the two-sample Welch $t$-test comparing condition $A$ and condition $B$.\n- $p_{w,i}$: the two-sided p-value from the Wilcoxon rank-sum test comparing condition $A$ and condition $B$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $8$ floating-point numbers ordered as $\\left[p_{t,1}, p_{w,1}, p_{t,2}, p_{w,2}, p_{t,3}, p_{w,3}, p_{t,4}, p_{w,4}\\right]$, where each number is rounded to $6$ decimal places. No additional text should be printed. All numbers are dimensionless and must be expressed as decimal fractions (for example, $0.012345$), not as percentages.", "solution": "The problem requires the computation of p-values from two distinct statistical hypothesis tests—the Welch two-sample $t$-test and the Wilcoxon rank-sum test—applied to RNA-sequencing count data from two conditions. The objective is to compare the performance of a parametric test against a non-parametric test, particularly in the presence of outliers, which are common in biological datasets. Before proceeding to the computational solution, we must establish the theoretical foundation for each test.\n\n**1. Welch's Two-Sample $t$-test**\n\nThe Welch's $t$-test is a statistical tool for comparing the means of two independent samples, denoted here as condition $A$ and condition $B$. It is an adaptation of the Student's $t$-test and is considered more reliable when the two samples have unequal variances, a scenario known as the Behrens-Fisher problem. The null hypothesis, $H_0$, posits that the population means are equal, i.e., $H_0: \\mu_A = \\mu_B$.\n\nThe test statistic $t$ is calculated as the difference between the sample means, scaled by the standard error of the difference:\n$$\nt = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}\n$$\nwhere:\n- $\\bar{x}_A$ and $\\bar{x}_B$ are the sample means for conditions $A$ and $B$, respectively.\n- $s_A^2$ and $s_B^2$ are the unbiased sample variances.\n- $n_A$ and $n_B$ are the sample sizes.\n\nUnlike the Student's $t$-test, the degrees of freedom, $\\nu$, for the Welch's test are not simply $n_A + n_B - 2$. Instead, they are approximated using the Welch-Satterthwaite equation:\n$$\n\\nu \\approx \\frac{\\left( \\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B} \\right)^2}{\\frac{(s_A^2/n_A)^2}{n_A - 1} + \\frac{(s_B^2/n_B)^2}{n_B - 1}}\n$$\nThe p-value is then determined from the Student's $t$-distribution with $\\nu$ degrees of freedom. For a two-sided test, the p-value is $2 \\cdot P(T_\\nu > |t|)$, where $T_\\nu$ is a random variable following the $t$-distribution.\n\nA critical assumption of the $t$-test is that the data in both samples are approximately normally distributed. The test's performance degrades in the presence of strong deviations from normality, such as skewness or heavy tails. Crucially, the sample mean $(\\bar{x})$ and variance $(s^2)$ are highly sensitive to extreme values (outliers), which can disproportionately influence the $t$-statistic and lead to erroneous conclusions.\n\n**2. Wilcoxon Rank-Sum (Mann-Whitney U) Test**\n\nThe Wilcoxon rank-sum test is a non-parametric alternative to the $t$-test. It does not assume a specific distribution for the data, making it more robust. The null hypothesis for this test is that for randomly selected values $X$ and $Y$ from the two populations, the probability of $X$ being greater than $Y$ is equal to the probability of $Y$ being greater than $X$. More generally, it tests if the two samples are drawn from populations with the same distribution.\n\nThe procedure is as follows:\n1. Combine all $n_A + n_B$ observations into a single ranked list. If ties exist, assign the average of the ranks that would have been assigned.\n2. Calculate the sum of the ranks for one of the samples, for instance, sample $A$, denoted as $R_A$.\n3. The test statistic $U$ is calculated based on this rank sum. For sample $A$, the statistic $U_A$ is:\n   $$\n   U_A = R_A - \\frac{n_A(n_A + 1)}{2}\n   $$\n   The test statistic $U$ is typically taken as $U = \\min(U_A, U_B)$, where $U_B$ is calculated similarly for sample $B$.\n4. The p-value is determined from the known distribution of the $U$ statistic under the null hypothesis. For small sample sizes, an exact distribution is used. For larger samples, a normal approximation with a continuity correction is employed.\n\nBecause the Wilcoxon test operates on ranks rather than the original data values, its statistic is not affected by the magnitude of outliers, only by their ordinal position. An extreme value is simply treated as the highest (or lowest) rank, and its specific numerical value does not further influence the test statistic. This property confers substantial robustness against outliers.\n\n**3. Application and Interpretation of Test Cases**\n\nThe provided test cases are designed to highlight the differing sensitivities of these two tests.\n- **Test case 1**: The distributions are similar and lack outliers. Both tests are expected to yield high p-values, indicating no significant difference.\n- **Test cases 2 and 3**: An extreme high outlier is introduced into condition $B$. The $t$-test's p-value is expected to be substantially affected because the outlier will inflate both the mean and variance of condition $B$. In contrast, the Wilcoxon test should be relatively unaffected, providing a more stable assessment of the central tendency shift.\n- **Test case 4**: An extreme low outlier is introduced into condition $B$, whose other members have shifted to higher values compared to condition $A$. This outlier pulls the mean of condition $B$ down, potentially masking the true location shift from the $t$-test. The rank-sum test, however, is better equipped to detect the consistent rank difference between the bulk of the two samples.\n\nThe computational implementation will use the `scipy.stats` library, specifically `ttest_ind` with the parameter `equal_var=False` to perform the Welch's $t$-test, and `mannwhitneyu` with `alternative='two-sided'` for the Wilcoxon rank-sum test. The results will be systematically computed for each test case and formatted as required.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import ttest_ind, mannwhitneyu\n\ndef solve():\n    \"\"\"\n    Computes and prints p-values for Welch's t-test and Wilcoxon rank-sum test\n    for a suite of RNA-seq count data test cases.\n    \"\"\"\n\n    # Define the test suite as specified in the problem statement.\n    test_cases = [\n        # Test case 1: baseline, no outlier, comparable distributions\n        (np.array([43, 50, 39, 61, 55, 47]), np.array([45, 52, 41, 58, 53, 49])),\n        # Test case 2: single extreme high outlier in condition B\n        (np.array([43, 50, 39, 61, 55, 47]), np.array([45, 52, 41, 58, 53, 1000])),\n        # Test case 3: small sample size with a single extreme high outlier in condition B\n        (np.array([20, 22, 25]), np.array([19, 21, 400])),\n        # Test case 4: location shift with a single extreme low outlier in condition B\n        (np.array([15, 16, 18, 20, 19, 17]), np.array([28, 30, 27, 29, 31, 0])),\n    ]\n\n    results = []\n    for cond_a, cond_b in test_cases:\n        # Perform Welch's two-sample t-test.\n        # The `equal_var=False` argument specifies that we should perform Welch's t-test,\n        # which does not assume equal population variance.\n        # The test is two-sided by default.\n        t_stat, p_t = ttest_ind(cond_a, cond_b, equal_var=False)\n        results.append(p_t)\n\n        # Perform the Wilcoxon rank-sum test (Mann-Whitney U test).\n        # We explicitly specify a two-sided test.\n        # The 'auto' method for p-value calculation is used by default,\n        # which chooses between an exact test and a normal approximation\n        # based on sample size and presence of ties.\n        u_stat, p_w = mannwhitneyu(cond_a, cond_b, alternative='two-sided')\n        results.append(p_w)\n\n    # Format the results as a comma-separated list of floating-point numbers\n    # rounded to 6 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2398972"}, {"introduction": "When analyzing data, the whole is not always the sum of its parts; in fact, it can sometimes be the complete opposite. This exercise introduces Simpson's Paradox, a counter-intuitive statistical phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This hands-on problem [@problem_id:2398958] will guide you in programmatically identifying this paradox in a hypothetical clinical trial, highlighting the crucial importance of accounting for confounding variables and analyzing stratified data.", "problem": "You are given a binary-outcome clinical comparison framed for computational biology and bioinformatics: for each test case, two groups exist (a new treatment and a control), and the cohort is stratified by biological sex (male and female). In each stratum and arm you are provided the total number of individuals and the number who experienced an adverse event. Let $N_{s,g}$ denote the total number in sex $s \\in \\{\\text{male}, \\text{female}\\}$ and group $g \\in \\{\\text{treatment}, \\text{control}\\}$, and let $X_{s,g}$ denote the corresponding number of adverse events. Define the event probabilities $p_{s,g}$ by $p_{s,g} = X_{s,g} / N_{s,g}$, and the overall arm totals $N_{\\text{treatment}} = \\sum_{s} N_{s,\\text{treatment}}$, $N_{\\text{control}} = \\sum_{s} N_{s,\\text{control}}$, with overall events $X_{\\text{treatment}} = \\sum_{s} X_{s,\\text{treatment}}$ and $X_{\\text{control}} = \\sum_{s} X_{s,\\text{control}}$, and overall probabilities $p_{\\text{treatment}} = X_{\\text{treatment}} / N_{\\text{treatment}}$ and $p_{\\text{control}} = X_{\\text{control}} / N_{\\text{control}}$.\n\nFor each test case, you must determine whether the dataset exhibits Simpson’s paradox with statistical significance at a two-sided significance level $\\alpha = 0.05$, defined here as:\n- Within the male stratum, the treatment is significantly harmful: $p_{\\text{male},\\text{treatment}} > p_{\\text{male},\\text{control}}$ and the difference is statistically significant at level $\\alpha$ under a valid hypothesis test of equality of adverse-event probabilities between treatment and control.\n- Within the female stratum, the treatment is significantly harmful: $p_{\\text{female},\\text{treatment}} > p_{\\text{female},\\text{control}}$ and the difference is statistically significant at level $\\alpha$ under a valid hypothesis test of equality of adverse-event probabilities between treatment and control.\n- Overall (ignoring sex), the treatment is significantly beneficial: $p_{\\text{treatment}} < p_{\\text{control}}$ and the difference is statistically significant at level $\\alpha$ under a valid hypothesis test of equality of adverse-event probabilities between treatment and control.\n\nIn each hypothesis test, the null hypothesis is that the adverse-event probability is equal in the treatment and control arms, and the decision rule is to reject the null hypothesis if and only if the two-sided $p$-value is less than or equal to $\\alpha$. You must return a boolean value for each test case indicating whether all three bullets above hold simultaneously.\n\nTest suite. For each test case, the input parameters are eight integers $(N_{\\text{male},\\text{treatment}}, X_{\\text{male},\\text{treatment}}, N_{\\text{male},\\text{control}}, X_{\\text{male},\\text{control}}, N_{\\text{female},\\text{treatment}}, X_{\\text{female},\\text{treatment}}, N_{\\text{female},\\text{control}}, X_{\\text{female},\\text{control}})$:\n- Case A (intended to satisfy Simpson’s paradox with strong statistical significance): $(1000, 180, 20000, 1800, 20000, 400, 5000, 50)$.\n- Case B (directional reversal present but not significant overall): $(200, 40, 1000, 100, 2000, 80, 1000, 20)$.\n- Case C (no paradox; treatment harmful within strata and overall): $(1000, 150, 1000, 100, 1000, 40, 1000, 20)$.\n\nProgram requirements. Your program must, for each test case, construct the $2\\times 2$ tables for male, female, and overall; perform valid hypothesis tests at two-sided level $\\alpha = 0.05$ to assess equality of adverse-event probabilities between treatment and control; verify the required direction of effect in each comparison; and output a boolean indicating whether Simpson’s paradox with statistical significance is present. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA, resultB, resultC]\"), where each entry is either the literal token True or False. No other output is permitted.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens.\n- Cohort stratification: Two strata by biological sex, $s \\in \\{\\text{male}, \\text{female}\\}$.\n- Experimental arms: Two groups, $g \\in \\{\\text{treatment}, \\text{control}\\}$.\n- Raw data: For each stratum $s$ and group $g$, we are given the total number of individuals, $N_{s,g}$, and the number of individuals experiencing an adverse event, $X_{s,g}$.\n- Derived quantities:\n  - Stratum-specific event probabilities: $p_{s,g} = X_{s,g} / N_{s,g}$.\n  - Overall total individuals: $N_{\\text{treatment}} = \\sum_{s} N_{s,\\text{treatment}}$, $N_{\\text{control}} = \\sum_{s} N_{s,\\text{control}}$.\n  - Overall total events: $X_{\\text{treatment}} = \\sum_{s} X_{s,\\text{treatment}}$, $X_{\\text{control}} = \\sum_{s} X_{s,\\text{control}}$.\n  - Overall event probabilities: $p_{\\text{treatment}} = X_{\\text{treatment}} / N_{\\text{treatment}}$, $p_{\\text{control}} = X_{\\text{control}} / N_{\\text{control}}$.\n- Significance level: $\\alpha = 0.05$ for two-sided tests.\n- Hypothesis Test: The null hypothesis is the equality of adverse-event probabilities between treatment and control arms. The null is rejected if the two-sided $p$-value is less than or equal to $\\alpha$.\n- Definition of Simpson's paradox with statistical significance: The simultaneous satisfaction of three conditions:\n  1. Male stratum: Treatment is significantly harmful, i.e., $p_{\\text{male},\\text{treatment}} > p_{\\text{male},\\text{control}}$ and the difference is statistically significant at level $\\alpha$.\n  2. Female stratum: Treatment is significantly harmful, i.e., $p_{\\text{female},\\text{treatment}} > p_{\\text{female},\\text{control}}$ and the difference is statistically significant at level $\\alpha$.\n  3. Overall population: Treatment is significantly beneficial, i.e., $p_{\\text{treatment}} < p_{\\text{control}}$ and the difference is statistically significant at level $\\alpha$.\n- Test Cases (Input: $(N_{m,t}, X_{m,t}, N_{m,c}, X_{m,c}, N_{f,t}, X_{f,t}, N_{f,c}, X_{f,c})$):\n  - Case A: $(1000, 180, 20000, 1800, 20000, 400, 5000, 50)$.\n  - Case B: $(200, 40, 1000, 100, 2000, 80, 1000, 20)$.\n  - Case C: $(1000, 150, 1000, 100, 1000, 40, 1000, 20)$.\n- Output format: A single line with a list of boolean results, e.g., `[True,False,False]`.\n\nStep 2: Validate Using Extracted Givens.\n- The problem is **scientifically grounded**. It addresses Simpson's paradox, a well-documented statistical phenomenon, in the context of a clinical trial analysis, which is a standard application in biostatistics and computational biology.\n- The problem is **well-posed**. The conditions for the paradox are stated with mathematical precision. All necessary data and parameters, including the significance level $\\alpha$, are provided. The task of determining a boolean outcome for each case is unambiguous.\n- The problem is **objective**. It is formulated using precise, quantitative language, free of subjective or opinion-based assertions. The term \"valid hypothesis test\" grants the choice of a standard statistical procedure, which is an acceptable specification in a scientific context. The counts provided are integers where $X \\le N$, which is physically consistent.\n\nStep 3: Verdict and Action.\nThe problem is deemed **valid**. It is self-contained, scientifically sound, and well-posed. A rigorous solution can be constructed.\n\nThe task is to verify, for each test case, whether a specific set of three conditions defining a statistically significant Simpson's paradox is met. This requires performing three separate hypothesis tests: one for the male stratum, one for the female stratum, and one for the aggregated (overall) data. Each test assesses the null hypothesis of equal adverse-event probabilities between the treatment and control groups ($H_0: p_{\\text{treatment}} = p_{\\text{control}}$) against a two-sided alternative ($H_1: p_{\\text{treatment}} \\neq p_{\\text{control}}$).\n\nA scientifically appropriate and \"valid hypothesis test\" for comparing two proportions derived from independent samples is Pearson's chi-squared test of independence on a $2 \\times 2$ contingency table. This test is equivalent to the two-sided two-proportion $z$-test for large samples, which are present here. For any comparison between a treatment group with $X_t$ events out of $N_t$ individuals and a control group with $X_c$ events out of $N_c$ individuals, the data can be arranged in the following contingency table:\n$$\n\\begin{array}{c|cc|c}\n & \\text{Adverse Event} & \\text{No Event} & \\text{Total} \\\\ \\hline\n\\text{Treatment} & X_t & N_t - X_t & N_t \\\\\n\\text{Control} & X_c & N_c - X_c & N_c\n\\end{array}\n$$\nThe chi-squared test statistic is calculated from this table, and its corresponding $p$-value is compared against the significance level $\\alpha = 0.05$. The null hypothesis is rejected if $p \\le \\alpha$.\n\nThe procedure for each test case is as follows:\n1.  Verify the condition for the male stratum.\n    - First, check the directionality: Is the sample proportion of adverse events in the treatment group greater than in the control group? That is, is $p_{\\text{male},\\text{treatment}} > p_{\\text{male},\\text{control}}$?\n    - If the direction is correct, construct the $2 \\times 2$ table for males and perform the chi-squared test. Check if the resulting $p$-value is $\\le 0.05$.\n    - The condition for males is met only if both the direction and statistical significance criteria are fulfilled.\n\n2.  Verify the condition for the female stratum.\n    - Check the directionality: Is $p_{\\text{female},\\text{treatment}} > p_{\\text{female},\\text{control}}$?\n    - If so, construct the $2 \\times 2$ table for females, perform the chi-squared test, and check if the $p$-value is $\\le 0.05$.\n    - The condition for females is met only if both criteria are fulfilled.\n\n3.  Verify the condition for the overall population.\n    - First, aggregate the data: $N_t = N_{m,t} + N_{f,t}$, $X_t = X_{m,t} + X_{f,t}$, $N_c = N_{m,c} + N_{f,c}$, and $X_c = X_{m,c} + X_{f,c}$.\n    - Check the directionality: Is the overall proportion of adverse events in the treatment group less than in the control group? That is, is $p_{\\text{treatment}} < p_{\\text{control}}$?\n    - If the direction is correct, construct the $2 \\times 2$ table for the aggregated data, perform the chi-squared test, and check if the $p$-value is $\\le 0.05$.\n    - The overall condition is met only if both criteria are fulfilled.\n\nFinally, Simpson's paradox, as defined, is present if and only if all three of the above conditions—for males, females, and the overall population—are simultaneously true. A boolean result is returned for each test case based on this final determination.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef solve():\n    \"\"\"\n    Solves the Simpson's paradox problem for the given test cases.\n    \"\"\"\n\n    # Significance level\n    alpha = 0.05\n\n    # Test cases: (N_mt, X_mt, N_mc, X_mc, N_ft, X_ft, N_fc, X_fc)\n    # m = male, f = female, t = treatment, c = control\n    test_cases = [\n        # Case A: Intended to satisfy Simpson’s paradox with strong statistical significance\n        (1000, 180, 20000, 1800, 20000, 400, 5000, 50),\n        # Case B: Directional reversal present but not significant overall\n        (200, 40, 1000, 100, 2000, 80, 1000, 20),\n        # Case C: No paradox; treatment harmful within strata and overall\n        (1000, 150, 1000, 100, 1000, 40, 1000, 20),\n    ]\n\n    def check_hypothesis(X_t, N_t, X_c, N_c, direction, alpha_level):\n        \"\"\"\n        Performs a hypothesis test for two proportions and checks directionality.\n\n        Args:\n            X_t (int): Number of events in treatment group.\n            N_t (int): Total individuals in treatment group.\n            X_c (int): Number of events in control group.\n            N_c (int): Total individuals in control group.\n            direction (str): 'greater' or 'less', specifying the required\n                             relationship p_t vs p_c.\n            alpha_level (float): The significance level.\n\n        Returns:\n            bool: True if direction and significance conditions are met.\n        \"\"\"\n        # Avoid division by zero if a group has no subjects.\n        if N_t == 0 or N_c == 0:\n            return False\n\n        p_t = X_t / N_t\n        p_c = X_c / N_c\n\n        # 1. Check directionality\n        if direction == 'greater':\n            if not p_t > p_c:\n                return False\n        elif direction == 'less':\n            if not p_t  p_c:\n                return False\n        else:\n            # Invalid direction specification\n            return False\n            \n        # 2. Check statistical significance\n        # Construct the 2x2 contingency table\n        # Table: [[events_t, no_events_t], [events_c, no_events_c]]\n        contingency_table = np.array([\n            [X_t, N_t - X_t],\n            [X_c, N_c - X_c]\n        ])\n\n        # Perform Pearson's chi-squared test.\n        # The p-value is for a two-sided test, as required.\n        # correction=True is the default (Yates' correction for continuity).\n        try:\n            _, p_value, _, _ = chi2_contingency(contingency_table)\n        except ValueError:\n            # This can happen if a row/column sum is zero, which is handled\n            # by the N_t/N_c check, but as a safeguard.\n            return False\n\n        return p_value = alpha_level\n\n    results = []\n    for case in test_cases:\n        N_mt, X_mt, N_mc, X_mc, N_ft, X_ft, N_fc, X_fc = case\n\n        # Condition 1: Male stratum - Treatment significantly harmful\n        male_cond_met = check_hypothesis(X_mt, N_mt, X_mc, N_mc, 'greater', alpha)\n\n        # Condition 2: Female stratum - Treatment significantly harmful\n        female_cond_met = check_hypothesis(X_ft, N_ft, X_fc, N_fc, 'greater', alpha)\n\n        # Condition 3: Overall - Treatment significantly beneficial\n        # Aggregate data\n        N_t_overall = N_mt + N_ft\n        X_t_overall = X_mt + X_ft\n        N_c_overall = N_mc + N_fc\n        X_c_overall = X_mc + X_fc\n        \n        overall_cond_met = check_hypothesis(X_t_overall, N_t_overall, X_c_overall, N_c_overall, 'less', alpha)\n        \n        # Simpson's Paradox holds if all three conditions are met\n        is_paradox = male_cond_met and female_cond_met and overall_cond_met\n        results.append(is_paradox)\n\n    # Format the output as a string list of booleans literal\n    # e.g., \"[True,False,False]\"\n    output_str = f\"[{','.join(str(r) for r in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2398958"}, {"introduction": "Modern bioinformatics often requires us to ask questions that can't be answered with off-the-shelf statistical tests, as the test statistic may be too complex for a standard formula. This practice introduces a powerful and flexible solution: Monte Carlo simulation. In this exercise [@problem_id:2398987], you will write code to generate a null distribution empirically, allowing you to calculate a p-value for a custom statistic like the length of the longest shared genomic segment, a foundational skill for hypothesis testing in cutting-edge research.", "problem": "You are investigating whether two individuals share unusually long identical-by-state segments along a chromosome. The test statistic is the length, in base pairs, of the longest contiguous block of markers where their alleles match, measured as the difference in physical positions between the block’s end marker and start marker. Under the null hypothesis that the two individuals are unrelated, each marker match is modeled as an independent Bernoulli trial with a given per-marker match probability. Because the null distribution of the maximum contiguous segment length is analytically intractable for realistic marker layouts and heterogeneous match probabilities, you will approximate it by Monte Carlo simulation.\n\nYour task is to implement a program that, for multiple test cases, estimates the Monte Carlo p-value for the observed longest matching segment length using simulated null data. Follow these scientific principles and definitions as the foundation for your design:\n\n- Let $M_i \\in \\{0,1\\}$ denote the match indicator at marker $i$, where $M_i = 1$ indicates a match and $M_i = 0$ indicates a mismatch. Under the null hypothesis $\\mathcal{H}_0$ of no relatedness, assume that $\\{M_i\\}_{i=0}^{n-1}$ are independent Bernoulli random variables with either a common match probability $p$ or per-marker probabilities $p_i$.\n- Let $x_i$ denote the physical position (in base pairs) of marker $i$, with strictly increasing $x_0  x_1  \\dots  x_{n-1}$.\n- Define the test statistic $T(M, x)$ as the maximum, over all contiguous runs where $M_i = 1$, of the segment length computed as $x_{j} - x_{i}$ for a run beginning at index $i$ and ending at index $j \\ge i$. By definition, a run of length one marker has segment length $0$ base pairs.\n- Given an observed statistic $T_{\\text{obs}}$, approximate the null p-value by Monte Carlo using $B$ independent simulations from the null model. In each simulation $b \\in \\{1,\\dots,B\\}$, generate an independent sequence $M^{(b)}$ from the Bernoulli model, compute $T^{(b)} = T(M^{(b)}, x)$, and estimate\n$$\n\\widehat{p} \\;=\\; \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}}{B+1},\n$$\nwhich is the standard Monte Carlo p-value with a $+1$ continuity correction to avoid zero estimates. The Strong Law of Large Numbers guarantees that $\\widehat{p}$ converges almost surely to the true p-value as $B \\to \\infty$ under $\\mathcal{H}_0$.\n\nImplementation requirements:\n\n- Use a fixed pseudo-random number generator seed of $20231011$ to ensure reproducibility.\n- For each test case, simulate $B$ null replicates, compute $\\widehat{p}$ as above, and report it.\n- All simulated marker matches must be independent given their probabilities. If a scalar $p$ is provided, use $M_i \\sim \\text{Bernoulli}(p)$ for all $i$. If a vector $(p_0,\\dots,p_{n-1})$ is provided, use $M_i \\sim \\text{Bernoulli}(p_i)$ independently.\n- The statistic must be computed in base pairs using the provided $x_i$ positions. There is no unit conversion in the output; the outputs are p-values and must be dimensionless decimals (no percentage signs).\n\nTest suite:\n\nImplement your program to compute $\\widehat{p}$ for the following four cases. In each case, indices are $i \\in \\{0,1,\\dots,n-1\\}$.\n\n- Case $1$ (happy path, long array, moderate match probability):\n  - $n = 1000$.\n  - Positions: $x_i = 10000 \\cdot i$ (in base pairs).\n  - Match probabilities: scalar $p = 0.25$.\n  - Observed statistic: $T_{\\text{obs}} = 180000$ (in base pairs).\n  - Simulations: $B = 5000$.\n\n- Case $2$ (irregular spacing, higher match probability):\n  - $n = 200$.\n  - Positions: $x_i = 1000 \\cdot i + 5000 \\cdot \\left\\lfloor \\dfrac{i}{50} \\right\\rfloor$ (in base pairs), introducing extra gaps at indices $50$, $100$, and $150$.\n  - Match probabilities: scalar $p = 0.5$.\n  - Observed statistic: $T_{\\text{obs}} = 100000$ (in base pairs).\n  - Simulations: $B = 5000$.\n\n- Case $3$ (small array, sparse matches, boundary observed statistic):\n  - $n = 50$.\n  - Positions: $x_i = 20000 \\cdot i$ (in base pairs).\n  - Match probabilities: scalar $p = 0.1$.\n  - Observed statistic: $T_{\\text{obs}} = 0$ (in base pairs).\n  - Simulations: $B = 5000$.\n\n- Case $4$ (position-dependent large gaps, heterogeneous per-marker probabilities):\n  - $n = 300$.\n  - Positions: $x_i = 500 \\cdot i + 20000 \\cdot \\left\\lfloor \\dfrac{i}{75} \\right\\rfloor$ (in base pairs), inducing larger gaps at indices $75$, $150$, and $225$.\n  - Match probabilities: per-marker $p_i = 0.05 + 0.45 \\cdot \\dfrac{i}{n-1}$ for $i=0,\\dots,n-1$.\n  - Observed statistic: $T_{\\text{obs}} = 50000$ (in base pairs).\n  - Simulations: $B = 5000$.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the four Monte Carlo p-values, in the order of Cases $1$ through $4$, formatted as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places (for example, $[0.123456,0.000200,1.000000,0.042314]$).", "solution": "We formulate hypothesis testing for shared genomic segments as follows. The null hypothesis $\\mathcal{H}_0$ posits that two individuals are unrelated, and hence the match indicators along a marker panel behave as independent Bernoulli random variables with given per-marker match probabilities. Let $M_i \\in \\{0,1\\}$ denote the match indicator at marker $i$, and let the ordered physical positions be $x_0  x_1  \\dots  x_{n-1}$, in base pairs. A contiguous block (or run) of matches is a maximal interval of indices $[i,j]$ with $M_k = 1$ for all $k \\in \\{i,i+1,\\dots,j\\}$, and with either $i=0$ or $M_{i-1}=0$, and either $j=n-1$ or $M_{j+1}=0$.\n\nDefine the test statistic $T(M,x)$ to be the maximum segment length (in base pairs) over all such runs, computed as $x_j - x_i$. A run of length one marker yields a segment length of $0$ because there is no span between adjacent distinct markers. The observed statistic $T_{\\text{obs}}$ is compared to the null distribution of $T(M,x)$.\n\nThe challenge is that the exact null distribution of the maximum run length is not available in closed form for realistic $x$ and heterogeneous probabilities. Instead, we approximate it by Monte Carlo simulation, grounded in the following principles:\n\n- Under $\\mathcal{H}_0$, the match indicators are independent Bernoulli trials: $M_i \\sim \\text{Bernoulli}(p)$ if a common probability $p$ is specified, or $M_i \\sim \\text{Bernoulli}(p_i)$ with possibly varying $p_i$. These are well-tested probabilistic models for independent binary outcomes.\n- The Strong Law of Large Numbers states that empirical averages of independent and identically distributed integrable random variables converge almost surely to their expectations as the number of samples grows. While the indicators $\\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}$ across simulations are independent and identically distributed Bernoulli random variables with mean equal to the true p-value, their average converges almost surely to that true mean as $B \\to \\infty$.\n- The Monte Carlo p-value with a $+1$ correction is\n$$\n\\widehat{p} \\;=\\; \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}}{B+1},\n$$\nwhich is a standard, conservative estimator that avoids zero estimates for finite $B$ and is consistent by the Strong Law of Large Numbers.\n\nAlgorithmic design:\n\n1. Input the marker positions $x \\in \\mathbb{R}^n$ (in base pairs), the match probabilities (either scalar $p$ or vector $p_i$), the observed statistic $T_{\\text{obs}}$, and the number of simulations $B$.\n2. Define a function to compute $T(M,x)$ given a Boolean vector $M$ and positions $x$:\n   - Initialize $t_{\\max} \\leftarrow 0$.\n   - Scan $i$ from $0$ to $n-1$. When a run of ones begins, record its start index $s$. Advance $i$ until the run ends at index $e$. Compute the run length $x_e - x_s$ and update $t_{\\max}$ if larger.\n   - The resulting $t_{\\max}$ is the longest contiguous matching segment length in base pairs.\n   The time complexity is $\\mathcal{O}(n)$ per evaluation because each index is visited at most twice (once to detect a run start and once as part of the run).\n3. For $b = 1$ to $B$, simulate one null replicate:\n   - If a scalar $p$ is provided, generate $M^{(b)}_i \\sim \\text{Bernoulli}(p)$ independently for $i=0,\\dots,n-1$ by drawing $U_i \\sim \\text{Uniform}(0,1)$ and setting $M^{(b)}_i = \\mathbf{1}\\{U_i  p\\}$.\n   - If a vector $(p_0,\\dots,p_{n-1})$ is provided, generate $M^{(b)}_i \\sim \\text{Bernoulli}(p_i)$ independently via $U_i \\sim \\text{Uniform}(0,1)$ and $M^{(b)}_i = \\mathbf{1}\\{U_i  p_i\\}$.\n   - Compute $T^{(b)} = T(M^{(b)}, x)$.\n4. Count $C = \\sum_{b=1}^{B} \\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}$ and report\n$$\n\\widehat{p} \\;=\\; \\frac{1+C}{B+1}.\n$$\n5. Repeat for each test case using the same pseudo-random number generator seeded at $20231011$ to ensure reproducibility. Because each case is independent, a single seeded generator used sequentially suffices.\n\nCorrectness rationale:\n\n- Under $\\mathcal{H}_0$, the simulated $M^{(b)}$ have the same distribution as the null data-generating process. Therefore, $T^{(b)}$ are independent and identically distributed samples from the null distribution of $T$.\n- The indicator variables $\\mathbf{1}\\{T^{(b)} \\ge T_{\\text{obs}}\\}$ are independent Bernoulli with mean equal to the true null tail probability at $T_{\\text{obs}}$. Their sum $C$ divided by $B$ estimates this mean. The $+1$ correction yields a conservative finite-sample estimator that converges to the true p-value as $B \\to \\infty$.\n- The implementation respects units by computing segment lengths as differences in $x$ measured in base pairs; the reported p-values are dimensionless and expressed as decimals.\n\nEdge cases in the provided test suite:\n\n- Case $3$ uses $T_{\\text{obs}} = 0$, which ensures $C = B$ and hence $\\widehat{p} = \\dfrac{1+B}{B+1} = 1$ exactly.\n- Cases $1$ and $2$ probe extreme tails with long observed segments relative to typical behavior under the null, while Case $4$ introduces heterogeneity in $p_i$ and position-dependent large gaps, exercising the general simulation logic.\n\nThe final program constructs the specified position arrays and probabilities exactly as defined, sets the seed to $20231011$, runs the simulation with $B=5000$ for each case, computes the four $\\widehat{p}$ values, rounds each to $6$ decimal places, and prints a single line in the required list format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef longest_segment_bp(matches: np.ndarray, positions: np.ndarray) - int:\n    \"\"\"\n    Compute the longest contiguous run of True values in 'matches',\n    measured in base pairs as positions[end] - positions[start].\n    A run of length 1 has length 0.\n    \"\"\"\n    n = matches.size\n    max_len = 0\n    i = 0\n    # Ensure boolean array\n    m = matches.astype(bool, copy=False)\n    while i  n:\n        if m[i]:\n            start = i\n            # advance until the run ends\n            while i + 1  n and m[i + 1]:\n                i += 1\n            end = i\n            seg_len = int(positions[end] - positions[start])\n            if seg_len  max_len:\n                max_len = seg_len\n        i += 1\n    return max_len\n\ndef monte_carlo_pvalue(positions, probs, T_obs, B, rng) - float:\n    \"\"\"\n    Estimate Monte Carlo p-value: (1 + count[T_sim = T_obs]) / (B + 1)\n    positions: 1D array of increasing integers (base pairs)\n    probs: scalar in [0,1] or 1D array of per-marker probabilities in [0,1]\n    T_obs: observed longest segment length (base pairs)\n    B: number of simulations\n    rng: numpy Generator for reproducibility\n    \"\"\"\n    positions = np.asarray(positions, dtype=np.int64)\n    n = positions.size\n\n    count_ge = 0\n    # Prepare probability vector\n    if np.isscalar(probs):\n        p_vec = float(probs)\n        for _ in range(B):\n            matches = rng.random(n)  p_vec\n            T_sim = longest_segment_bp(matches, positions)\n            if T_sim = T_obs:\n                count_ge += 1\n    else:\n        p_arr = np.asarray(probs, dtype=float)\n        assert p_arr.shape == (n,), \"Per-marker probability vector must match number of positions.\"\n        for _ in range(B):\n            matches = rng.random(n)  p_arr\n            T_sim = longest_segment_bp(matches, positions)\n            if T_sim = T_obs:\n                count_ge += 1\n\n    pval = (1.0 + count_ge) / (B + 1.0)\n    return pval\n\ndef solve():\n    # Fixed random seed as specified\n    rng = np.random.default_rng(20231011)\n\n    # Test Case 1\n    n1 = 1000\n    positions1 = 10000 * np.arange(n1, dtype=np.int64)\n    p1 = 0.25\n    T_obs1 = 180000  # base pairs\n    B1 = 5000\n\n    # Test Case 2: irregular spacing with extra gaps at indices 50, 100, 150\n    n2 = 200\n    idx2 = np.arange(n2, dtype=np.int64)\n    positions2 = 1000 * idx2 + 5000 * (idx2 // 50)\n    p2 = 0.5\n    T_obs2 = 100000  # base pairs\n    B2 = 5000\n\n    # Test Case 3: small array, sparse matches, boundary observed value\n    n3 = 50\n    positions3 = 20000 * np.arange(n3, dtype=np.int64)\n    p3 = 0.1\n    T_obs3 = 0  # base pairs\n    B3 = 5000\n\n    # Test Case 4: large gaps at 75, 150, 225; heterogeneous probabilities\n    n4 = 300\n    idx4 = np.arange(n4, dtype=np.int64)\n    positions4 = 500 * idx4 + 20000 * (idx4 // 75)\n    p4 = 0.05 + 0.45 * (idx4.astype(float) / (n4 - 1))\n    T_obs4 = 50000  # base pairs\n    B4 = 5000\n\n    test_cases = [\n        (positions1, p1, T_obs1, B1),\n        (positions2, p2, T_obs2, B2),\n        (positions3, p3, T_obs3, B3),\n        (positions4, p4, T_obs4, B4),\n    ]\n\n    results = []\n    for positions, probs, T_obs, B in test_cases:\n        pval = monte_carlo_pvalue(positions, probs, T_obs, B, rng)\n        results.append(pval)\n\n    # Final print statement in the exact required format, rounded to 6 decimals.\n    formatted = \"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2398987"}]}