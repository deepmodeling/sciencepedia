{"hands_on_practices": [{"introduction": "Planning a next-generation sequencing experiment requires careful consideration of parameters like sequencing depth and read length. These choices directly determine the scale of the resulting dataset, impacting storage requirements and the computational resources needed for analysis. This exercise [@problem_id:2417496] lets you practice a fundamental skill: performing a 'back-of-the-envelope' calculation to estimate the total data size, turning abstract experimental goals into concrete, practical figures.", "problem": "A whole-genome sequencing experiment targets a haploid human genome of size $3.2\\times 10^{9}$ base pairs at $30\\times$ depth using $150$ base pair paired-end reads on a platform that outputs raw reads in the FASTQ (Fast All-in-one Sequence with Quality) text format. Assume the following:\n\n- Coverage depth is defined as total sequenced bases divided by the haploid genome size.\n- Each read has length $L=150$ bases and is represented in a FASTQ record of four lines: a header line beginning with the character $@$; a sequence line of $L$ nucleotide characters; a plus line beginning with the character $+$ and containing no additional annotation; and a quality line of $L$ American Standard Code for Information Interchange (ASCII) characters encoding per-base quality scores.\n- All characters are single-byte ASCII, and each line is terminated by a single newline character of $1$ byte.\n- The header line for each read contains exactly $40$ ASCII characters (including the initial $@$) before the newline.\n- The FASTQ files are uncompressed.\n- Use the decimal definition of a gigabyte, where $1$ gigabyte $=$ $10^{9}$ bytes.\n\nEstimate the total amount of data generated, in gigabytes, for the entire experiment. Round your answer to three significant figures and express it in gigabytes.", "solution": "The problem is subjected to validation.\n\nGivens extracted verbatim from the problem statement:\n- Haploid human genome size: $G = 3.2 \\times 10^9$ base pairs\n- Target coverage depth: $C = 30\\times$\n- Read length: $L = 150$ base pairs\n- Read type: paired-end\n- Output format: FASTQ\n- FASTQ record structure:\n    - Header line: $40$ characters (including $@$) + $1$ newline\n    - Sequence line: $L$ characters + $1$ newline\n    - Plus line: $1$ character ($+$) + $1$ newline\n    - Quality line: $L$ characters + $1$ newline\n- Character encoding: single-byte ASCII\n- Line terminator: $1$ byte newline\n- File format: uncompressed\n- Unit conversion: $1$ gigabyte = $10^9$ bytes\n- Required precision: round the final answer to three significant figures\n\nValidation verdict:\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard bioinformatics scenario for whole-genome sequencing. The parameters provided, such as genome size ($G$), coverage depth ($C$), and read length ($L$), are realistic values for a human sequencing project. The description of the FASTQ format is accurate and sufficient for the calculation. The problem is self-contained, consistent, and requires a straightforward, formalizable calculation. Thus, the problem is deemed valid and a solution will be provided.\n\nThe objective is to calculate the total size of the data generated by the sequencing experiment in gigabytes.\n\nFirst, we calculate the total number of bases sequenced, denoted by $B_{total}$. This is given by the product of the genome size, $G = 3.2 \\times 10^9$ base pairs, and the coverage depth, $C = 30$.\n$$B_{total} = G \\times C$$\nSubstituting the given values:\n$$B_{total} = (3.2 \\times 10^9) \\times 30 = 9.6 \\times 10^{10} \\text{ bases}$$\n\nNext, we determine the total number of individual reads, $N_{reads}$. Since each read has a length of $L = 150$ bases, the total number of reads is the total number of sequenced bases divided by the length of a single read.\n$$N_{reads} = \\frac{B_{total}}{L}$$\nSubstituting the values for $B_{total}$ and $L$:\n$$N_{reads} = \\frac{9.6 \\times 10^{10}}{150} = \\frac{96 \\times 10^9}{150} = 0.64 \\times 10^9 = 6.4 \\times 10^8 \\text{ reads}$$\nEach read corresponds to one FASTQ record. Therefore, there are $6.4 \\times 10^8$ FASTQ records in total.\n\nNow, we calculate the size of a single FASTQ record, $S_{read}$, in bytes. The problem specifies that all characters are single-byte ASCII ($1$ byte per character) and each line is terminated by a single newline character ($1$ byte). The FASTQ record consists of $4$ lines:\n1.  Header line: This line contains $40$ characters before the newline. The total size is $40 \\text{ bytes} + 1 \\text{ byte} = 41 \\text{ bytes}$.\n2.  Sequence line: This line contains $L = 150$ nucleotide characters. The total size is $150 \\text{ bytes} + 1 \\text{ byte} = 151 \\text{ bytes}$.\n3.  Plus line: This line contains the single character `$+$`. The total size is $1 \\text{ byte} + 1 \\text{ byte} = 2 \\text{ bytes}$.\n4.  Quality line: This line contains $L = 150$ quality score characters. The total size is $150 \\text{ bytes} + 1 \\text{ byte} = 151 \\text{ bytes}$.\n\nThe total size of one FASTQ record is the sum of the sizes of these four lines:\n$$S_{read} = 41 + 151 + 2 + 151 = 345 \\text{ bytes}$$\n\nThe total amount of data generated, $S_{total}$, is the product of the total number of reads, $N_{reads}$, and the size of a single read's record, $S_{read}$.\n$$S_{total} = N_{reads} \\times S_{read}$$\nSubstituting the calculated values:\n$$S_{total} = (6.4 \\times 10^8) \\times 345 = 2208 \\times 10^8 = 2.208 \\times 10^{11} \\text{ bytes}$$\n\nFinally, we convert the total size from bytes to gigabytes (GB), using the given conversion factor $1 \\text{ GB} = 10^9 \\text{ bytes}$.\n$$S_{total, \\text{GB}} = \\frac{S_{total}}{10^9} = \\frac{2.208 \\times 10^{11}}{10^9} = 2.208 \\times 10^2 = 220.8 \\text{ GB}$$\n\nThe problem requires the answer to be rounded to three significant figures. The value $220.8$ rounded to three significant figures is $221$.", "answer": "$$\\boxed{221}$$", "id": "2417496"}, {"introduction": "Raw sequencing data is rarely perfect; it often contains artifacts stemming from the molecular biology steps of library preparation. A crucial skill for any bioinformatician is to diagnose these issues by interpreting quality control reports and understanding their root causes. This problem challenges you to act as a data detective, linking specific signatures in the reads to a common experimental artifact and selecting the right computational tools to clean the data before analysis [@problem_id:2417424].", "problem": "A paired-end Next-Generation Sequencing (NGS) run yields a high proportion of reads dominated by known adapter motifs. Quality control reports indicate strong adapter content from the start of many reads, and after adapter removal many reads have negligible insert length. Which step in the library construction most plausibly produced these sequences, and what computational strategy is most appropriate to remove them before downstream analysis?\n\nA. Excess adapter molecules during the adapter ligation step led to adapter–adapter ligation products, and insufficient post-ligation size selection allowed these short constructs to persist; computationally, perform adapter-aware trimming using the known adapter sequences on each read (or read pair), then discard any read or read pair whose post-trim insert length is below a minimum threshold $\\ell_{\\min}$, indicating that the sequence is primarily adapter.\n\nB. Over-fragmentation during mechanical shearing generated ultra-short inserts; computationally, increase the base-quality trimming threshold so that reads with average Phred quality below a cutoff $Q_{\\min}$ are discarded.\n\nC. Index hopping during cluster amplification created spurious chimeric reads; computationally, demultiplex with stricter barcode mismatch tolerance by reducing the allowed number of index mismatches to $m_{\\max}$ close to zero.\n\nD. Incomplete end-repair produced hairpin-like molecules that sequenced aberrantly; computationally, align all reads to a reference genome and discard any read that remains unmapped after up to $k$ allowed mismatches.\n\nE. Primer-dimer formation during Polymerase Chain Reaction (PCR) created short amplicons; computationally, remove highly abundant $k$-mers by collapsing reads that share identical $k$-mer profiles to a representative sequence.", "solution": "The validity of the problem statement must first be established.\n\n### Step 1: Extract Givens\nThe problem provides the following observations from a paired-end Next-Generation Sequencing (NGS) run:\n1.  A high proportion of reads are dominated by known adapter motifs.\n2.  Quality control reports indicate strong adapter content from the start of many reads.\n3.  After adapter removal, many reads have negligible insert length.\n\nThe problem asks for the most plausible cause of these observations during the library construction phase and the most appropriate computational strategy to remove the resulting artifactual sequences before downstream analysis.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement will now be assessed for scientific validity and clarity.\n- **Scientific Groundedness**: The scenario described is a classic and frequent artifact in NGS library preparation. The concepts presented—paired-end sequencing, adapter motifs, quality control, insert length, adapter ligation, size selection, and various computational filtering strategies—are fundamental and well-established in the field of genomics and bioinformatics. The observations are consistent with the formation of so-called \"adapter-dimers\". This is not speculative.\n- **Well-Posedness**: The problem is well-posed. The set of observations points strongly toward a specific molecular artifact, and the question requests the identification of the generating mechanism and the corresponding corrective computational procedure. A standard, unique, and well-accepted answer exists within the discipline.\n- **Objectivity**: The language is technical and objective. Phrases like \"high proportion,\" \"strong adapter content,\" and \"negligible insert length\" are standard descriptors derived from common QC software outputs (e.g., FastQC). There is no ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, and objective. It is based on a realistic and common scenario in experimental genomics. Therefore, the problem is **valid**. A full solution will be derived.\n\n### Derivation and Option Analysis\n\nThe provided observations must be logically connected to a mechanism in library preparation.\n1.  **\"Adapter content from the start of many reads\"**: In a standard sequencing library, a read begins at the 5' end of the DNA insert. Adapter sequences are ligated to the ends of this insert. Therefore, adapter sequence should only appear at the 3' end of a read, and only if the DNA insert is shorter than the sequencing read length (a phenomenon known as \"read-through\"). The presence of adapter sequence at the very beginning (5' end) of the read indicates that the sequencing process began directly on an adapter molecule, not a DNA insert.\n2.  **\"High proportion of reads dominated by known adapter motifs\" and \"negligible insert length after adapter removal\"**: These two points reinforce the first. If a read consists almost entirely of adapter sequence, then after computationally trimming this adapter sequence away, the remaining \"insert\" will have a length close to zero. The high abundance suggests a systematic issue in the library preparation.\n\nThe only plausible mechanism that produces a sequencing template consisting of two adapters ligated together is the formation of **adapter-dimers**. This occurs during the adapter ligation step. If the molar concentration of adapter molecules is excessively high relative to the concentration of DNA fragments, the ligase is more likely to join two adapter molecules together than to join an adapter to a DNA fragment. These adapter-dimer constructs are short, typically between $120$ and $150$ base pairs.\n\nA subsequent step in library preparation is size selection, which is designed to enrich for the desired fragment size distribution (e.g., $300$-$500$ base pairs) and eliminate very short fragments. If this size selection step is performed suboptimally or is not sufficiently stringent, these short adapter-dimer constructs will be retained in the final library, amplified by PCR, and sequenced.\n\nThe computational strategy to correct this issue must specifically target these artifactual reads. The strategy is twofold:\n1.  **Identification**: Use an adapter trimming tool (e.g., `cutadapt`, `Trimmomatic`) with the known sequences of the adapters to identify and remove adapter content from all reads.\n2.  **Filtering**: After trimming, reads that originated from adapter-dimers will be very short. Therefore, a length filter must be applied to discard any read (or, in this paired-end case, the entire read pair) if the remaining sequence length falls below a specified minimum threshold, $\\ell_{\\min}$.\n\nNow, each option will be evaluated against this derived understanding.\n\n**A. Excess adapter molecules during the adapter ligation step led to adapter–adapter ligation products, and insufficient post-ligation size selection allowed these short constructs to persist; computationally, perform adapter-aware trimming using the known adapter sequences on each read (or read pair), then discard any read or read pair whose post-trim insert length is below a minimum threshold $\\ell_{\\min}$, indicating that the sequence is primarily adapter.**\nThis option correctly identifies the cause: adapter-dimer formation from excess adapter concentration, which is then inadequately removed by size selection. The proposed computational strategy—adapter trimming followed by filtering based on a minimum length threshold $\\ell_{\\min}$—is precisely the standard and most effective method for removing these artifacts.\n**Verdict: Correct.**\n\n**B. Over-fragmentation during mechanical shearing generated ultra-short inserts; computationally, increase the base-quality trimming threshold so that reads with average Phred quality below a cutoff $Q_{\\min}$ are discarded.**\nThe cause is inconsistent with the evidence. Ultra-short inserts would lead to adapter read-through at the 3' end of reads, not adapter content at the 5' start. The computational strategy is also incorrect. The problem is one of sequence content (adapter vs. insert), not base quality. While quality can be a proxy for certain issues, it is not the direct solution here. Discarding reads based on an average quality cutoff $Q_{\\min}$ would not specifically target adapter-dimers.\n**Verdict: Incorrect.**\n\n**C. Index hopping during cluster amplification created spurious chimeric reads; computationally, demultiplex with stricter barcode mismatch tolerance by reducing the allowed number of index mismatches to $m_{\\max}$ close to zero.**\nThis describes a completely different artifact. Index hopping (or index misassignment) leads to reads from one sample being incorrectly assigned to another sample during demultiplexing. It does not create reads that are composed entirely of adapter sequence. The cause and the proposed solution are irrelevant to the problem described.\n**Verdict: Incorrect.**\n\n**D. Incomplete end-repair produced hairpin-like molecules that sequenced aberrantly; computationally, align all reads to a reference genome and discard any read that remains unmapped after up to $k$ allowed mismatches.**\nIncomplete end-repair or A-tailing results in reduced ligation efficiency, leading to lower library yield, not a specific artifact of high abundance. The proposed computational strategy is also suboptimal. Using alignment as the primary filter for adapter-dimers is computationally expensive and presupposes the existence of a reference genome. The standard workflow is to perform adapter/quality trimming and filtering *before* alignment.\n**Verdict: Incorrect.**\n\n**E. Primer-dimer formation during Polymerase Chain Reaction (PCR) created short amplicons; computationally, remove highly abundant $k$-mers by collapsing reads that share identical $k$-mer profiles to a representative sequence.**\nWhile primer-dimers can form during PCR, the problem specifies reads dominated by \"known adapter motifs.\" In this context, \"adapter\" typically refers to the ligation adapters, not the PCR primers that anneal to a site on the adapters. Adapter-dimers form during ligation, which is pre-PCR. The computational strategy described is a form of digital normalization or error correction based on $k$-mer counts, which is not the standard procedure for removing adapter contamination. The goal is complete removal of these artifactual reads, not collapsing them to a single representative sequence.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2417424"}, {"introduction": "After assembling a genome from millions of short reads, how do we measure the quality of the result? The contiguity of an assembly—the extent to which it consists of long, unbroken sequences—is a critical metric. The NG50 statistic provides a standardized method for quantifying this quality by relating contig lengths to the estimated size of the genome. Working through this calculation [@problem_id:2417502] will give you a practical grasp on how assembly success is measured and reported.", "problem": "A draft genome assembly derived from Next-Generation Sequencing (NGS) consists of contigs with lengths $\\{100, 50, 20, 5, 5\\}$ kilobase pairs (kbp). The estimated haploid genome size is $150$ kilobase pairs (kbp). The NG50 statistic is defined as follows: given an estimated genome size $G$, sort all contigs in nonincreasing order by length and compute the running cumulative sum from the longest to the shortest; the NG50 is the contig length $L$ at which the cumulative sum first equals or exceeds $\\frac{1}{2}G$. Compute the NG50 for this assembly. Express your answer in kilobase pairs (kbp). Do not use a percentage sign anywhere in your work.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n1.  A set of contig lengths: $\\{100, 50, 20, 5, 5\\}$ kilobase pairs (kbp).\n2.  Estimated haploid genome size: $150$ kilobase pairs (kbp).\n3.  Definition of NG50: Given an estimated genome size $G$, sort all contigs in nonincreasing order by length and compute the running cumulative sum from the longest to the shortest; the NG50 is the contig length $L$ at which the cumulative sum first equals or exceeds $\\frac{1}{2}G$.\n\nValidation verdict:\nThe problem is valid. It is scientifically grounded, as it concerns the calculation of the NG50 statistic, a standard and well-defined metric in bioinformatics for evaluating genome assembly continuity. The problem is well-posed, providing all necessary data and a clear, unambiguous definition to arrive at a unique solution. The data provided are self-consistent and realistic. The problem is objective and free of any logical or factual flaws.\n\nWe proceed with the solution.\n\nThe task is to compute the NG50 statistic for a given draft genome assembly.\nThe set of contig lengths is given as $C = \\{100, 50, 20, 5, 5\\}$ in units of kilobase pairs (kbp).\nThe estimated haploid genome size is $G = 150$ kbp.\n\nThe definition of NG50 requires us to find the contig length $L$ that pushes the cumulative sum of contig lengths to meet or exceed $50\\%$ of the estimated genome size. The contigs must be sorted by length in descending order for this calculation.\n\nFirst, we calculate the target cumulative length, which is half of the estimated genome size:\n$$\n\\text{Target Sum} = \\frac{1}{2} G = \\frac{1}{2} \\times 150 \\text{ kbp} = 75 \\text{ kbp}\n$$\n\nNext, we sort the given contig lengths in nonincreasing (descending) order. The provided list is already sorted:\n$L = [100, 50, 20, 5, 5]$.\n\nNow, we compute the cumulative sum of these lengths, starting from the longest contig, and we check at each step if the sum has reached or exceeded the target of $75$ kbp.\n\nLet $l_i$ be the length of the $i$-th contig in the sorted list.\nLet $S_k$ be the cumulative sum of the lengths of the first $k$ contigs, $S_k = \\sum_{i=1}^{k} l_i$. We are looking for the smallest integer $k$ such that $S_k \\ge 75$. The NG50 is then the length of the $k$-th contig, $l_k$.\n\nStep 1: Consider the first (longest) contig.\nThe length is $l_1 = 100$ kbp.\nThe cumulative sum is $S_1 = l_1 = 100$ kbp.\n\nWe check if the cumulative sum meets the condition:\nIs $S_1 \\ge 75$ kbp?\n$100 \\ge 75$. Yes, the condition is satisfied.\n\nSince the condition is met with the very first contig, the process stops here. The index $k$ is $1$. The NG50 statistic is defined as the length of this specific contig, $l_k$, which is $l_1$.\n\nThus, the NG50 for this assembly is the length of the first contig.\n$$\n\\text{NG50} = l_1 = 100 \\text{ kbp}\n$$\nThe question asks for the answer to be expressed in kilobase pairs. The computed value is $100$.", "answer": "$$\\boxed{100}$$", "id": "2417502"}]}