{"hands_on_practices": [{"introduction": "A perfect alignment with zero mismatches does not always guarantee a confident mapping. This exercise challenges you to explore the crucial distinction between alignment quality and mapping uniqueness, which is quantified by the Mapping Quality (MAPQ) score. By dissecting a scenario with a repetitive sequence, you will learn to apply probabilistic reasoning to understand why a read mapping to multiple genomic locations receives a low confidence score, a fundamental concept in interpreting alignment results [@problem_id:2425337].", "problem": "A single-end DNA sequencing read of length $L$ is the periodic low-complexity string $r = \\text{ATAT...}$ (alternating `A` and `T`) and aligns with zero mismatches at multiple loci in a reference genome. Assume the read aligner reports one of the best-scoring alignments when there are ties, without using external priors about locus uniqueness. Neglect sequencing errors. The Mapping Quality (MAPQ) is defined as $\\mathrm{MAPQ}=-10\\log_{10} P_{\\text{err}}$, where $P_{\\text{err}}$ is the probability that the reported alignment location is incorrect.\n\nDoes this read deserve a high MAPQ score? Choose the option that correctly justifies the answer using a probabilistic argument.\n\nA. Yes. A perfect match implies $P_{\\text{err}}=0$, so $\\mathrm{MAPQ}$ should be maximal.\n\nB. No. If there are $k$ equally good genomic placements for $r$, and the aligner reports one among them, then $P_{\\text{err}}=1-\\frac{1}{k}$, so $\\mathrm{MAPQ}=-10\\log_{10}\\!\\left(1-\\frac{1}{k}\\right)$, which is small when $k\\gg 1$.\n\nC. Yes. With negligible sequencing error, $P_{\\text{err}}\\approx 4^{-L}$, so $\\mathrm{MAPQ}$ is high for moderate $L$.\n\nD. It depends only on $L$. Any perfect match with $L\\ge 30$ merits $\\mathrm{MAPQ}\\ge 30$, regardless of genomic repetitiveness.\n\nE. No. Low-complexity reads have intrinsically higher per-base error rate, which increases $P_{\\text{err}}$ even if the sequence matches perfectly.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\n**Step 1: Extract Givens**\n- The sequencing read is single-end of length $L$.\n- The read sequence is $r = \\text{ATAT...}$, a periodic low-complexity string.\n- The read aligns with zero mismatches at multiple loci in a reference genome.\n- The aligner reports one of the best-scoring alignments when there are ties.\n- The aligner does not use external priors.\n- Sequencing errors are to be neglected.\n- The Mapping Quality is defined as $\\mathrm{MAPQ}=-10\\log_{10} P_{\\text{err}}$.\n- $P_{\\text{err}}$ is the probability that the reported alignment location is incorrect.\n- The question is whether this read deserves a high $\\mathrm{MAPQ}$ score, justified by a probabilistic argument.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a common and fundamental scenario in bioinformatics: the mapping of repetitive sequences. The definition of Mapping Quality ($\\mathrm{MAPQ}$) is standard, as used in formats like SAM/BAM. The assumptions, such as neglecting sequencing errors and the behavior of the aligner in case of ties, are clearly stated to simplify the problem to its core probabilistic nature. The problem is self-contained and free of contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A rigorous solution can be derived from the provided information.\n\n**Derivation of Solution**\nThe Mapping Quality, $\\mathrm{MAPQ}$, is a measure of confidence that the reported alignment position is the true position from which the read originated. Its definition is given as $\\mathrm{MAPQ}=-10\\log_{10} P_{\\text{err}}$, where $P_{\\text{err}}$ is the probability that the mapping is incorrect.\n\nThe problem states that the read $r$ aligns with zero mismatches at \"multiple loci\". Let us denote the number of such equally perfect alignment locations as $k$. Since there are \"multiple\" loci, we know that $k > 1$.\n\nGiven that all $k$ alignments are perfect (zero mismatches), and the problem instructs us to neglect sequencing errors, each of these $k$ locations is an equally plausible origin for the read. The aligner, lacking any other information or priors, has no basis to prefer one of these locations over another. Therefore, the probability that any specific locus $i$ (where $i \\in \\{1, 2, \\dots, k\\}$) is the true origin of the read is $\\frac{1}{k}$.\n\nThe aligner reports one of these $k$ locations. Let's call the reported location $L_{rep}$. The probability that this reported location is the correct one is:\n$$ P(\\text{correct}) = P(L_{rep} \\text{ is true origin}) = \\frac{1}{k} $$\nThe probability of error, $P_{\\text{err}}$, is the probability that the reported location is *not* the true origin. This is the complement of $P(\\text{correct})$:\n$$ P_{\\text{err}} = 1 - P(\\text{correct}) = 1 - \\frac{1}{k} = \\frac{k-1}{k} $$\nNow, we can compute the $\\mathrm{MAPQ}$ score:\n$$ \\mathrm{MAPQ} = -10\\log_{10} P_{\\text{err}} = -10\\log_{10}\\left(1 - \\frac{1}{k}\\right) $$\nLet us analyze this expression. Since the read aligns at multiple loci, $k \\ge 2$.\n- If $k=2$, $P_{\\text{err}} = 1 - \\frac{1}{2} = 0.5$. $\\mathrm{MAPQ} = -10\\log_{10}(0.5) \\approx -10(-0.301) \\approx 3$.\n- If $k=10$, $P_{\\text{err}} = 1 - \\frac{1}{10} = 0.9$. $\\mathrm{MAPQ} = -10\\log_{10}(0.9) \\approx -10(-0.046) \\approx 0.46$.\n- As $k \\to \\infty$, $P_{\\text{err}} \\to 1$, and $\\mathrm{MAPQ} \\to 0$.\n\nIn all cases where $k>1$, the $\\mathrm{MAPQ}$ score is a small positive number. A $\\mathrm{MAPQ}$ of $3$ corresponds to a $50\\%$ chance of error, which is extremely low confidence. A $\\mathrm{MAPQ}$ score approaching $0$ represents near certainty that the mapping is incorrect (or, more precisely, that the reported alignment is arbitrarily chosen from a large pool of equally good candidates). Therefore, a low-complexity read that maps to multiple locations does not deserve a high $\\mathrm{MAPQ}$ score.\n\n**Option-by-Option Analysis**\n\nA. Yes. A perfect match implies $P_{\\text{err}}=0$, so $\\mathrm{MAPQ}$ should be maximal.\nThis statement is incorrect. A perfect match does not imply a unique match. The core of the problem is that there are multiple perfect matches. The existence of multiple equally good alignments ($k>1$) means that the probability of the single reported alignment being correct is $\\frac{1}{k} < 1$, and thus $P_{\\text{err}} = 1 - \\frac{1}{k} > 0$. The premise that a perfect match implies $P_{\\text{err}}=0$ is false. **Incorrect**.\n\nB. No. If there are $k$ equally good genomic placements for $r$, and the aligner reports one among them, then $P_{\\text{err}}=1-\\frac{1}{k}$, so $\\mathrm{MAPQ}=-10\\log_{10}\\!\\left(1-\\frac{1}{k}\\right)$, which is small when $k\\gg 1$.\nThis statement accurately reflects the probabilistic reasoning derived above. It correctly identifies that with $k$ equally good placements, the probability of error for a single reported alignment is $1 - \\frac{1}{k}$. It correctly calculates the resulting $\\mathrm{MAPQ}$ and correctly concludes that this value is small, especially for a large number of repeats ($k\\gg 1$). The final answer \"No\" is also consistent with the derivation. **Correct**.\n\nC. Yes. With negligible sequencing error, $P_{\\text{err}}\\approx 4^{-L}$, so $\\mathrm{MAPQ}$ is high for moderate $L$.\nThis statement is incorrect. The value $4^{-L}$ represents the probability of a specific sequence of length $L$ appearing by chance at a random position, assuming an i.i.d. nucleotide model with equal frequencies. This is a model for the probability of a random hit, not the empirical mapping error probability $P_{\\text{err}}$. $P_{\\text{err}}$ must be calculated based on the number of actual high-quality alignment sites found in the given reference genome. The problem explicitly states there are multiple such sites, which this option ignores. **Incorrect**.\n\nD. It depends only on $L$. Any perfect match with $L\\ge 30$ merits $\\mathrm{MAPQ}\\ge 30$, regardless of genomic repetitiveness.\nThis statement is incorrect. $\\mathrm{MAPQ}$ is fundamentally dependent on mapping uniqueness, not just read length $L$. A very long read (e.g., $L=500$) consisting of the repeating dinucleotide `AT` could still map to thousands of locations in a large eukaryotic genome. In this case, $k$ would be very large, $P_{\\text{err}}$ would be close to $1$, and $\\mathrm{MAPQ}$ would be close to $0$. The claim that $\\mathrm{MAPQ}$ is high regardless of repetitiveness is a direct contradiction of its definition. **Incorrect**.\n\nE. No. Low-complexity reads have intrinsically higher per-base error rate, which increases $P_{\\text{err}}$ even if the sequence matches perfectly.\nThis statement is incorrect within the context of the problem. The problem explicitly directs us to \"Neglect sequencing errors\". Therefore, introducing an argument based on intrinsic error rates violates the premises of the problem. Furthermore, the logic is confused: if the sequence matches perfectly, it means no errors were observed in the alignment, so speculating about underlying error rates is irrelevant to the calculation of $P_{\\text{err}}$ from the given alignment results. **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2425337"}, {"introduction": "Alignment scores are not arbitrary; they are mathematically derived to reflect the likelihood of one nucleotide substituting for another, given a model of evolution or error. This practice guides you through the construction of a custom log-odds scoring matrix, akin to the famous BLOSUM matrices, but specifically tailored to a hypothetical sequencing technology's error profile. Completing this exercise will clarify how the scoring systems at the heart of alignment tools are built upon rigorous probabilistic foundations [@problem_id:2425346].", "problem": "A read mapper aligns short nucleotide reads to a reference genome. For base-to-base scoring, you decide to use a log-odds substitution scheme inspired by BLOcks SUbstitution Matrix (BLOSUM), but specialized for nucleotides and the known sequencing error model. Let the nucleotide alphabet be $\\{A,C,G,T\\}$. Assume the reference genome base frequencies are uniform, so the background probability $q(x)$ for any base $x$ is $q(A)=q(C)=q(G)=q(T)=0.25$. The sequencing technology has total per-base error rate $\\epsilon=0.10$ and a characteristic elevated transversion error $A\\leftrightarrow T$ described as follows:\n\n- If the true base is $A$, the observed base is $A$ with probability $1-\\epsilon$, $T$ with probability $\\epsilon\\cdot 0.60$, $C$ with probability $\\epsilon\\cdot 0.20$, and $G$ with probability $\\epsilon\\cdot 0.20$.\n- If the true base is $T$, the observed base is $T$ with probability $1-\\epsilon$, $A$ with probability $\\epsilon\\cdot 0.60$, $C$ with probability $\\epsilon\\cdot 0.20$, and $G$ with probability $\\epsilon\\cdot 0.20$.\n- If the true base is $C$ (respectively $G$), the observed base is the true base with probability $1-\\epsilon$, and each of the other three nucleotides with equal probability $\\epsilon/3$.\n\nYou will score aligning observed read base $x$ to reference base $y$ using the log-odds\n$$\ns(x,y)\\;=\\;\\log_{2}\\!\\left(\\frac{P(x\\mid y)}{q(x)}\\right),\n$$\nwhere $P(x\\mid y)$ is the sequencing channel probability described above and $q(x)$ is the background probability.\n\nWhich candidate modification to the substitution matrix entries is consistent with this error model and the log-odds definition?\n\nA. Keep the matrix symmetric and favor classical transitions: set $s(A,G)=s(G,A)=s(C,T)=s(T,C)=-2.00$; set all transversions, including $A\\leftrightarrow T$, to $-3.00$; set all matches to $+2.00$.\n\nB. Use a direction-aware matrix reflecting the channel asymmetry: set $s(A,A)=s(C,C)=s(G,G)=s(T,T)\\approx +1.85$; set $s(T\\mid A)=s(A\\mid T)\\approx -2.06$; set $s(C\\mid A)=s(G\\mid A)=s(C\\mid T)=s(G\\mid T)\\approx -3.64$; set $s(A\\mid C)=s(G\\mid C)=s(T\\mid C)\\approx -2.91$ and the analogous entries for $y=G$ also $\\approx -2.91$.\n\nC. Penalize the frequent $A\\leftrightarrow T$ transversion more harshly to discourage it: set $s(A,T)=s(T,A)=-4.50$; set all other mismatches to $-3.00$; set matches to $+2.00$.\n\nD. Increase match rewards to overwhelm errors but keep all mismatches equal: set all matches to $+5.00$ and all mismatches, including $A\\leftrightarrow T$, to $-3.00$.\n\nAnswer by selecting the option whose pattern and approximate magnitudes best follow from the stated model and the log-odds definition.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n\n-   **Nucleotide Alphabet**: $\\{A,C,G,T\\}$.\n-   **Background Probability**: The background probability for any base $x$ is uniform, $q(x) = 0.25$. So, $q(A) = q(C) = q(G) = q(T) = 0.25$.\n-   **Total Per-Base Error Rate**: $\\epsilon = 0.10$.\n-   **Sequencing Channel Probabilities**, $P(x \\mid y)$, where $y$ is the true reference base and $x$ is the observed read base:\n    -   If $y=A$: $P(A \\mid A) = 1-\\epsilon$, $P(T \\mid A) = \\epsilon \\cdot 0.60$, $P(C \\mid A) = \\epsilon \\cdot 0.20$, $P(G \\mid A) = \\epsilon \\cdot 0.20$.\n    -   If $y=T$: $P(T \\mid T) = 1-\\epsilon$, $P(A \\mid T) = \\epsilon \\cdot 0.60$, $P(C \\mid T) = \\epsilon \\cdot 0.20$, $P(G \\mid T) = \\epsilon \\cdot 0.20$.\n    -   If $y=C$: $P(C \\mid C) = 1-\\epsilon$, and for $x \\in \\{A, G, T\\}$, $P(x \\mid C) = \\epsilon/3$.\n    -   If $y=G$: $P(G \\mid G) = 1-\\epsilon$, and for $x \\in \\{A, C, T\\}$, $P(x \\mid G) = \\epsilon/3$.\n-   **Scoring Function**: The log-odds score for aligning observed base $x$ to reference base $y$ is given by\n    $$\n    s(x,y) = \\log_{2}\\!\\left(\\frac{P(x\\mid y)}{q(x)}\\right)\n    $$\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientific Grounding**: The problem is well-grounded in computational biology and bioinformatics. The construction of a log-odds scoring matrix based on a probabilistic model of error or evolution is a standard and fundamental technique (e.g., PAM, BLOSUM). The specified error model, while simplified, reflects realistic characteristics of some sequencing technologies, such as biased error patterns.\n-   **Well-Posedness**: The problem is well-posed. All required parameters ($\\epsilon$, $q(x)$), a complete probabilistic model ($P(x \\mid y)$), and a precise scoring formula ($s(x,y)$) are provided. The task is to calculate the matrix entries and compare them to the given options.\n-   **Consistency Check**: Let's verify that the probabilities for each true base sum to $1$.\n    -   For $y=A$: $(1-\\epsilon) + \\epsilon \\cdot 0.60 + \\epsilon \\cdot 0.20 + \\epsilon \\cdot 0.20 = 1 - \\epsilon + \\epsilon(0.60+0.20+0.20) = 1 - \\epsilon + \\epsilon = 1$.\n    -   For $y=T$: The probabilities are identical to the case $y=A$, so they also sum to $1$.\n    -   For $y=C$: $(1-\\epsilon) + 3 \\cdot (\\epsilon/3) = 1 - \\epsilon + \\epsilon = 1$.\n    -   For $y=G$: The probabilities are identical to the case $y=C$, so they also sum to $1$.\n-   **Objectivity**: The language is precise, mathematical, and free of subjectivity.\n\n### Step 3: Verdict and Action\n\nThe problem statement is scientifically sound, self-contained, consistent, and well-posed. It has no discernible flaws. Therefore, the problem is **valid**. I will proceed to derive the solution.\n\n### Derivation of the Substitution Matrix\n\nThe scoring function is $s(x,y) = \\log_{2}(P(x \\mid y) / q(x))$. Given $q(x) = 0.25$ for all $x$, this simplifies to:\n$$\ns(x,y) = \\log_{2}\\left(\\frac{P(x \\mid y)}{0.25}\\right) = \\log_{2}(4 \\cdot P(x \\mid y))\n$$\nWe use the given value $\\epsilon = 0.10$.\n\n**1. Match Scores: $s(y,y)$**\nFor any base $y$, the probability of a correct observation is $P(y \\mid y) = 1 - \\epsilon = 1 - 0.10 = 0.90$.\nThe match score is therefore the same for all bases:\n$$\ns(y,y) = \\log_{2}(4 \\cdot 0.90) = \\log_{2}(3.6) \\approx +1.848\n$$\nThus, $s(A,A) = s(C,C) = s(G,G) = s(T,T) \\approx +1.85$.\n\n**2. Mismatch Scores: $s(x,y)$ for $x \\neq y$**\n\n**Case $y=A$ (true base is $A$):**\n-   Observed $x=T$: $P(T \\mid A) = \\epsilon \\cdot 0.60 = 0.10 \\cdot 0.60 = 0.06$.\n    $$s(T,A) = \\log_{2}(4 \\cdot 0.06) = \\log_{2}(0.24) \\approx -2.059$$\n-   Observed $x=C$: $P(C \\mid A) = \\epsilon \\cdot 0.20 = 0.10 \\cdot 0.20 = 0.02$.\n    $$s(C,A) = \\log_{2}(4 \\cdot 0.02) = \\log_{2}(0.08) \\approx -3.644$$\n-   Observed $x=G$: $P(G \\mid A) = \\epsilon \\cdot 0.20 = 0.10 \\cdot 0.20 = 0.02$.\n    $$s(G,A) = \\log_{2}(4 \\cdot 0.02) = \\log_{2}(0.08) \\approx -3.644$$\n\n**Case $y=T$ (true base is $T$):**\n-   Observed $x=A$: $P(A \\mid T) = \\epsilon \\cdot 0.60 = 0.10 \\cdot 0.60 = 0.06$.\n    $$s(A,T) = \\log_{2}(4 \\cdot 0.06) = \\log_{2}(0.24) \\approx -2.059$$\n-   Observed $x=C$: $P(C \\mid T) = \\epsilon \\cdot 0.20 = 0.10 \\cdot 0.20 = 0.02$.\n    $$s(C,T) = \\log_{2}(4 \\cdot 0.02) = \\log_{2}(0.08) \\approx -3.644$$\n-   Observed $x=G$: $P(G \\mid T) = \\epsilon \\cdot 0.20 = 0.10 \\cdot 0.20 = 0.02$.\n    $$s(G,T) = \\log_{2}(4 \\cdot 0.02) = \\log_{2}(0.08) \\approx -3.644$$\n\n**Case $y=C$ (true base is $C$):**\n-   For any observed mismatch $x \\in \\{A, G, T\\}$, the probability is $P(x \\mid C) = \\epsilon/3 = 0.10/3$.\n    $$s(A,C) = s(G,C) = s(T,C) = \\log_{2}(4 \\cdot (0.10/3)) = \\log_{2}(0.4/3) \\approx \\log_{2}(0.1333) \\approx -2.907$$\n\n**Case $y=G$ (true base is $G$):**\n-   For any observed mismatch $x \\in \\{A, C, T\\}$, the probability is $P(x \\mid G) = \\epsilon/3 = 0.10/3$.\n    $$s(A,G) = s(C,G) = s(T,G) = \\log_{2}(4 \\cdot (0.10/3)) \\approx -2.907$$\n\n**Summary of the substitution matrix $s(x,y)$ (observed $x$, reference $y$):**\n\n| $x$ \\textbackslash $y$ | A | C | G | T |\n| :---: | :---: | :---: | :---: | :---: |\n| **A** | $+1.85$ | $-2.91$ | $-2.91$ | $-2.06$ |\n| **C** | $-3.64$ | $+1.85$ | $-2.91$ | $-3.64$ |\n| **G** | $-3.64$ | $-2.91$ | $+1.85$ | $-3.64$ |\n| **T** | $-2.06$ | $-2.91$ | $-2.91$ | $+1.85$ |\n\nThe scoring matrix is \"direction-aware\" or asymmetric, because $P(x \\mid y)$ is not in general equal to $P(y \\mid x)$. For example, $s(C,A) \\approx -3.64$ while $s(A,C) \\approx -2.91$.\n\n### Option-by-Option Analysis\n\n**A. Keep the matrix symmetric and favor classical transitions: set $s(A,G)=s(G,A)=s(C,T)=s(T,C)=-2.00$; set all transversions, including $A\\leftrightarrow T$, to $-3.00$; set all matches to $+2.00$.**\n-   The matrix should not be symmetric, as calculated ($s(C,A) \\neq s(A,C)$). The statement to \"keep the matrix symmetric\" is a direct contradiction of the model's implications.\n-   The transition score $s(A,G)$ is calculated as $\\approx -2.91$, not $-2.00$.\n-   The $A\\leftrightarrow T$ transversion score is calculated as $\\approx -2.06$, which is the least penalized mismatch, because it is the most probable error. This option gives it a severe penalty of $-3.00$.\n-   Match scores are $\\approx +1.85$, not $+2.00$.\n-   This option is based on a generic, biologically-motivated scoring scheme for evolutionary divergence (transitions are more common than transversions), which is irrelevant to the specific sequencing error model provided.\nVerdict: **Incorrect**.\n\n**B. Use a direction-aware matrix reflecting the channel asymmetry: set $s(A,A)=s(C,C)=s(G,G)=s(T,T)\\approx +1.85$; set $s(T\\mid A)=s(A\\mid T)\\approx -2.06$; set $s(C\\mid A)=s(G\\mid A)=s(C\\mid T)=s(G\\mid T)\\approx -3.64$; set $s(A\\mid C)=s(G\\mid C)=s(T\\mid C)\\approx -2.91$ and the analogous entries for $y=G$ also $\\approx -2.91$.**\n-   This option correctly identifies that the matrix is \"direction-aware\" (asymmetric).\n-   Match scores are stated as $\\approx +1.85$, which matches our calculation of $\\approx +1.848$.\n-   The score for the $A\\leftrightarrow T$ error (i.e., $s(T,A)$ and $s(A,T)$) is stated as $\\approx -2.06$, which matches our calculation of $\\approx -2.059$.\n-   The scores for errors from $A$ or $T$ to $C$ or $G$ (e.g., $s(C,A)$) are stated as $\\approx -3.64$. This matches our calculation of $\\approx -3.644$.\n-   The scores for errors from $C$ or $G$ to any other base (e.g., $s(A,C)$) are stated as $\\approx -2.91$. This matches our calculation of $\\approx -2.907$.\n-   Every value and the overall structure proposed in this option are consistent with our derivation.\nVerdict: **Correct**.\n\n**C. Penalize the frequent $A\\leftrightarrow T$ transversion more harshly to discourage it: set $s(A,T)=s(T,A)=-4.50$; set all other mismatches to $-3.00$; set matches to $+2.00$.**\n-   The central premise of this option is fundamentally flawed. A log-odds score reflects the logarithm of a probability ratio. A frequent error (high $P(x \\mid y)$) results in a higher (less negative) score than a rare error. This option proposes the opposite: to penalize the most frequent error ($A \\leftrightarrow T$) most harshly. This contradicts the definition of a log-odds score. Our calculation gives $s(A,T) \\approx -2.06$, the *least* negative mismatch score.\nVerdict: **Incorrect**.\n\n**D. Increase match rewards to overwhelm errors but keep all mismatches equal: set all matches to $+5.00$ and all mismatches, including $A\\leftrightarrow T$, to $-3.00$.**\n-   The proposal to keep all mismatches equal is inconsistent with the provided error model, which explicitly defines different probabilities for different types of errors. Our calculations show three distinct classes of mismatch scores: $\\approx -2.06$, $\\approx -2.91$, and $\\approx -3.64$.\n-   The numerical values ($+5.00$ for matches, $-3.00$ for mismatches) are arbitrary and do not follow from the log-odds formula. Our calculated match score is $\\approx +1.85$.\n-   This option describes an ad-hoc, simplistic scoring scheme, not one derived from the given probabilistic model.\nVerdict: **Incorrect**.\n\nFinal conclusion based on the analysis is that Option B is the only one that correctly reflects the consequences of the given error model and log-odds scoring formula.", "answer": "$$\\boxed{B}$$", "id": "2425346"}, {"introduction": "Modern read aligners achieve their remarkable speed through sophisticated indexing structures like the one based on the Burrows-Wheeler Transform (BWT), but their performance depends on careful tuning. This problem puts you in the role of an algorithm designer, tasking you with optimizing a key parameter of a BWT index to balance query speed against memory consumption. By setting up and solving a cost-minimization problem, you will gain insight into the practical engineering trade-offs that underpin high-performance bioinformatics tools [@problem_id:2425278].", "problem": "A reference genome of length $N$ characters over an alphabet of size $\\sigma$ is indexed using the Burrows–Wheeler Transform (BWT). The index stores occurrence-count checkpoints every $k$ BWT positions. Each checkpoint stores cumulative counts for all $\\sigma$ symbols, using $b$ bytes per counter. Thus, the checkpointing overhead in memory is proportional to the number of checkpoints. For read mapping via last-to-first (LF) mapping, consider backward search over a read of length $L$. In each step of backward search, two rank queries are performed. Each rank query is computed by starting from the most recent checkpoint preceding the queried BWT position and scanning forward symbol-by-symbol up to the queried position to accumulate counts. Assume:\n- The queried BWT positions are uniformly distributed with respect to the checkpoint lattice, so the distance (in symbols) from a queried position to its preceding checkpoint is uniformly distributed on $\\{0,1,\\ldots,k-1\\}$.\n- The time cost per rank query has a fixed overhead of $\\tau_{0}$ seconds plus an additional $\\tau_{s}$ seconds per scanned BWT symbol.\n- The total time to process a read is the sum of the times of all rank queries in its $L$ LF-mapping steps.\n- The memory overhead due to checkpoints is $(N/k)\\,\\sigma\\,b$ bytes (ignore ceiling effects).\n- A tradeoff parameter $\\eta$ converts bytes of memory into an equivalent time cost in seconds per byte, so that the combined objective to minimize is $J(k)=T(k)+\\eta\\,M(k)$, where $T(k)$ is the expected time (in seconds) to process one read and $M(k)$ is the checkpoint memory overhead (in bytes).\n\nGiven:\n- $N=5.0\\times 10^{6}$,\n- $\\sigma=5$,\n- $b=4$,\n- $L=150$,\n- $\\tau_{0}=30\\times 10^{-9}\\ \\text{s}$,\n- $\\tau_{s}=3.0\\times 10^{-9}\\ \\text{s}$,\n- $\\eta=1.7\\times 10^{-10}\\ \\text{s/byte}$,\n\ntreat $k$ as a positive real variable. Find the real-valued $k^{\\*}$ that minimizes $J(k)$. Report $k^{\\*}$ as a pure number (dimensionless), rounded to four significant figures.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard optimization problem in computational biology concerning the trade-off between time and memory in genomic indexing. All parameters are provided, and the objective is clearly defined. Therefore, the problem is valid, and a solution can be derived.\n\nThe objective is to find the checkpoint interval $k$ that minimizes the combined cost function $J(k) = T(k) + \\eta M(k)$, where $T(k)$ is the expected time to process one read and $M(k)$ is the memory overhead from checkpoints. We will treat $k$ as a positive real variable as specified.\n\nFirst, we formulate the expected time cost, $T(k)$. A read has length $L$, and processing it involves $L$ steps of backward search. Each step requires $2$ rank queries, for a total of $2L$ rank queries per read.\n\nThe time cost for a single rank query is composed of a fixed overhead $\\tau_0$ and a variable part that depends on the number of symbols scanned from the last checkpoint. Let this distance be $d$. The time for one query is $\\tau_0 + d \\cdot \\tau_s$. The problem states that the distance $d$ from a queried position to its preceding checkpoint is uniformly distributed on the set of integers $\\{0, 1, \\ldots, k-1\\}$. The expected value of this distance, $E[d]$, is:\n$$E[d] = \\frac{1}{k} \\sum_{i=0}^{k-1} i = \\frac{1}{k} \\frac{(k-1)k}{2} = \\frac{k-1}{2}$$\nThe expected time for a single rank query, $E[T_{\\text{query}}]$, is therefore:\n$$E[T_{\\text{query}}] = \\tau_0 + E[d] \\cdot \\tau_s = \\tau_0 + \\left(\\frac{k-1}{2}\\right)\\tau_s$$\nThe total expected time to process one read of length $L$, denoted $T(k)$, is the product of the number of queries ($2L$) and the expected time per query:\n$$T(k) = 2L \\cdot E[T_{\\text{query}}] = 2L \\left( \\tau_0 + \\frac{k-1}{2}\\tau_s \\right) = 2L\\tau_0 + L(k-1)\\tau_s$$\n\nNext, we formulate the memory overhead cost, $M(k)$. The problem statement provides this directly:\n$$M(k) = \\frac{N}{k} \\sigma b$$\nwhere $N$ is the genome length, $\\sigma$ is the alphabet size, and $b$ is the number of bytes per counter.\n\nThe combined objective function $J(k)$ is the sum of the time cost and the memory cost converted to an equivalent time cost via the parameter $\\eta$:\n$$J(k) = T(k) + \\eta M(k) = \\left( 2L\\tau_0 + L(k-1)\\tau_s \\right) + \\eta \\left( \\frac{N \\sigma b}{k} \\right)$$\nTo simplify for differentiation, we can expand the expression for $T(k)$:\n$$J(k) = 2L\\tau_0 + Lk\\tau_s - L\\tau_s + \\frac{\\eta N \\sigma b}{k}$$\nTo find the value of $k$ that minimizes $J(k)$, we compute the first derivative of $J(k)$ with respect to $k$ and set it to zero. The terms $2L\\tau_0$ and $-L\\tau_s$ are constant with respect to $k$.\n$$\\frac{dJ}{dk} = \\frac{d}{dk} \\left( (2L\\tau_0 - L\\tau_s) + L\\tau_s k + \\eta N \\sigma b k^{-1} \\right)$$\n$$\\frac{dJ}{dk} = L\\tau_s - \\eta N \\sigma b k^{-2} = L\\tau_s - \\frac{\\eta N \\sigma b}{k^2}$$\nSetting the derivative to zero to find the optimal value $k^*$:\n$$L\\tau_s - \\frac{\\eta N \\sigma b}{(k^*)^2} = 0$$\n$$L\\tau_s = \\frac{\\eta N \\sigma b}{(k^*)^2}$$\nSolving for $(k^*)^2$:\n$$(k^*)^2 = \\frac{\\eta N \\sigma b}{L \\tau_s}$$\nSince $k$ must be a positive value, we take the positive square root:\n$$k^* = \\sqrt{\\frac{\\eta N \\sigma b}{L \\tau_s}}$$\nTo confirm this is a minimum, we check the second derivative:\n$$\\frac{d^2J}{dk^2} = \\frac{d}{dk} \\left( L\\tau_s - \\eta N \\sigma b k^{-2} \\right) = 2 \\eta N \\sigma b k^{-3} = \\frac{2 \\eta N \\sigma b}{k^3}$$\nFor $k > 0$, all parameters ($\\eta, N, \\sigma, b$) are positive, so $\\frac{d^2J}{dk^2} > 0$. This confirms that $k^*$ corresponds to a minimum of the function $J(k)$.\n\nNow, we substitute the given numerical values:\n$N=5.0 \\times 10^{6}$\n$\\sigma=5$\n$b=4$\n$L=150$\n$\\tau_s=3.0 \\times 10^{-9}\\ \\text{s}$\n$\\eta=1.7 \\times 10^{-10}\\ \\text{s/byte}$\n\n$$k^* = \\sqrt{\\frac{(1.7 \\times 10^{-10}) \\cdot (5.0 \\times 10^{6}) \\cdot (5) \\cdot (4)}{(150) \\cdot (3.0 \\times 10^{-9})}}$$\nLet's compute the numerator and denominator of the fraction under the square root.\nNumerator:\n$$\\eta N \\sigma b = (1.7 \\times 10^{-10}) \\cdot (5.0 \\times 10^{6}) \\cdot (20) = 1.7 \\times 10^{-10} \\cdot 100 \\times 10^{6} = 1.7 \\times 10^{-2}$$\nDenominator:\n$$L \\tau_s = 150 \\cdot (3.0 \\times 10^{-9}) = 450 \\times 10^{-9} = 4.5 \\times 10^{-7}$$\nNow, we compute the value of $(k^*)^2$:\n$$(k^*)^2 = \\frac{1.7 \\times 10^{-2}}{4.5 \\times 10^{-7}} = \\frac{1.7}{4.5} \\times 10^5 \\approx 0.3777\\ldots \\times 10^5 = 37777.7\\ldots$$\nFinally, we compute $k^*$:\n$$k^* = \\sqrt{37777.7\\ldots} \\approx 194.36506 \\ldots$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $1, 9, 4, 3$. The fifth digit is $6$, so we round up the third decimal place.\n$$k^* \\approx 194.4$$", "answer": "$$\\boxed{194.4}$$", "id": "2425278"}]}