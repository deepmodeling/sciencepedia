{"hands_on_practices": [{"introduction": "Detecting copy number variations from sequencing data begins with understanding the specific signatures, or 'footprints,' they leave behind. This practice explores one such fundamental signal related to tandem duplications. By deriving the expected distribution of insert sizes for read pairs that span a duplication junction, you will gain a clear, mathematical understanding of how this structural event distorts paired-end read alignments, a critical concept for developing and interpreting CNV detection algorithms. [@problem_id:2382724]", "problem": "A paired-end sequencing experiment is performed on a diploid human genome. The library is prepared such that the true physical template length between the outer ends of each DNA fragment, denoted by $T$, is a continuous random variable with a normal distribution having mean $I_s$ and variance $\\sigma^2$, that is $T \\sim \\mathcal{N}(I_s,\\sigma^2)$. The read length is negligible compared to $I_s$ so that the reported insert size can be identified with the genomic span between the outer mapped positions of the read pair. A Copy Number Variation (CNV) event corresponding to a tandem duplication of length $L_d$ is present in the sample but absent in the reference genome. Reads are aligned to the reference genome using a standard mapper that reports the insert size $D$ as the genomic distance on the reference between the outermost mapped bases of the pair.\n\nFocus exclusively on read pairs whose physical template spans the tandem duplication junction in the sample (that is, the fragment crosses the junction joining the first and second copy of the duplicated segment). Assume that for such pairs, alignment to the single-copy reference collapses the duplicated segment, so the reported insert size equals the sample’s physical template length minus the duplicated length, that is $D = T - L_d$. Neglect soft clipping, mapping errors, and orientation anomalies for the purpose of this question. Assume $I_s \\gg 0$ and $\\sigma > 0$.\n\nDerive the probability density function of the reported insert size $D$ for these junction-spanning pairs, expressed in closed form in terms of $I_s$, $\\sigma$, and $L_d$. Provide your final answer as a single analytic expression for $f_D(d)$. No rounding is required.", "solution": "The problem will first be subjected to a rigorous validation to ascertain its scientific and logical integrity.\n\n### Step 1: Extract Givens\n\nThe following information is provided in the problem statement:\n- The true physical template length, $T$, is a continuous random variable.\n- The distribution of $T$ is normal with mean $I_s$ and variance $\\sigma^2$, denoted as $T \\sim \\mathcal{N}(I_s, \\sigma^2)$.\n- A tandem duplication of length $L_d$ exists in the sample genome.\n- For read pairs spanning the duplication junction, the reported insert size $D$ is related to the true template length $T$ by the equation $D = T - L_d$.\n- The parameters $I_s$ and $\\sigma$ satisfy the conditions $I_s \\gg 0$ and $\\sigma > 0$.\n- The objective is to derive the probability density function (PDF) of $D$, denoted $f_D(d)$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded**: The problem describes a simplified but standard model used in bioinformatics for the detection of structural variations, specifically tandem duplications, from paired-end sequencing data. The assumption that the reported insert size for junction-spanning reads is reduced by the length of the duplicated segment ($D = T - L_d$) is a foundational concept in this type of analysis. The modeling of library fragment sizes with a normal distribution is also a common and reasonable approximation. The problem is therefore firmly grounded in the principles of computational biology.\n- **Well-Posed**: The problem requires the derivation of the probability distribution of a random variable, $D$, which is a simple linear transformation of another random variable, $T$, whose distribution is known. This is a standard, well-defined problem in probability theory that admits a unique and stable solution.\n- **Objective**: The problem is stated using precise, unambiguous technical language, free from any subjective or non-scientific claims.\n\nBased on this analysis, the problem statement is free of scientific unsoundness, ambiguity, and contradiction. It is a formalizable and relevant problem within the specified field.\n\n### Step 3: Verdict and Action\n\nThe problem statement is deemed **valid**. A solution will now be derived.\n\nThe problem asks for the probability density function (PDF) of the reported insert size, $D$. We are given that the true physical template length, $T$, is a random variable following a normal distribution with mean $I_s$ and variance $\\sigma^2$. The PDF of $T$, denoted $f_T(t)$, is therefore:\n$$\nf_T(t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t - I_s)^2}{2\\sigma^2}\\right)\n$$\nThe problem states that for the specific subset of read pairs under consideration, the reported insert size $D$ is a linear transformation of the true length $T$:\n$$\nD = T - L_d\n$$\nHere, $L_d$ is a constant representing the length of the duplication.\n\nA fundamental property of the normal distribution is that it is closed under linear transformations. If a random variable $X$ is normally distributed, $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$, then any new random variable $Y$ defined as $Y = aX + b$ (where $a$ and $b$ are constants) is also normally distributed. Its mean and variance are given by:\n$$\nE[Y] = E[aX + b] = aE[X] + b = a\\mu_X + b\n$$\n$$\n\\text{Var}(Y) = \\text{Var}(aX + b) = a^2\\text{Var}(X) = a^2\\sigma_X^2\n$$\nWe apply this principle to our random variable $D = T - L_d$. In this case, $T$ corresponds to $X$, $D$ corresponds to $Y$, the coefficient $a=1$, and the constant offset $b = -L_d$.\n\nThe mean of $D$, which we denote $\\mu_D$, is:\n$$\n\\mu_D = E[D] = E[T - L_d] = E[T] - L_d\n$$\nSince $E[T] = I_s$, we have:\n$$\n\\mu_D = I_s - L_d\n$$\nThe variance of $D$, which we denote $\\sigma_D^2$, is:\n$$\n\\sigma_D^2 = \\text{Var}(D) = \\text{Var}(T - L_d) = 1^2 \\cdot \\text{Var}(T)\n$$\nSince $\\text{Var}(T) = \\sigma^2$, we have:\n$$\n\\sigma_D^2 = \\sigma^2\n$$\nThus, the reported insert size $D$ for junction-spanning reads also follows a normal distribution, specifically $D \\sim \\mathcal{N}(I_s - L_d, \\sigma^2)$.\n\nThe probability density function for a normally distributed variable $d$ with mean $\\mu_D$ and variance $\\sigma_D^2$ is given by the general form:\n$$\nf_D(d) = \\frac{1}{\\sqrt{2\\pi\\sigma_D^2}} \\exp\\left(-\\frac{(d - \\mu_D)^2}{2\\sigma_D^2}\\right)\n$$\nSubstituting the derived mean $\\mu_D = I_s - L_d$ and variance $\\sigma_D^2 = \\sigma^2$ into this equation, we obtain the required PDF for $D$:\n$$\nf_D(d) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(d - (I_s - L_d))^2}{2\\sigma^2}\\right)\n$$\nThis expression can be simplified to:\n$$\nf_D(d) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(d - I_s + L_d)^2}{2\\sigma^2}\\right)\n$$\nThis is the closed-form analytical expression for the probability density function of the reported insert size $D$ for the specified population of read pairs.", "answer": "$$\n\\boxed{f_D(d) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(d - I_s + L_d)^2}{2\\sigma^2}\\right)}\n$$", "id": "2382724"}, {"introduction": "Identifying the precise boundaries of a CNV, or its breakpoints, is a central challenge in genomics. This exercise reframes this challenge as a statistical change-point detection problem, a powerful paradigm for analyzing sequential data. You will implement the cumulative sum (CUSUM) algorithm, a classic and highly effective method for sequentially monitoring a data stream—in this case, genomic read depth—to rapidly detect shifts in the underlying copy number state. [@problem_id:2382736]", "problem": "You are given non-negative integer counts representing bin-level read-depth measurements along a single chromosome under a piecewise-constant copy-number model. Assume the following generative assumptions hold: for each genomic bin indexed by $t \\in \\{1,\\dots,n\\}$, an observed count $x_t \\in \\mathbb{Z}_{\\ge 0}$ is an independent realization from a Poisson distribution with mean $\\lambda_t$, where the mean $\\lambda_t$ is piecewise-constant in $t$ and takes values in a finite set $\\{\\lambda^{(0)},\\lambda^{(u)},\\lambda^{(d)}\\}$ with strict ordering $\\lambda^{(u)} > \\lambda^{(0)} > \\lambda^{(d)} > 0$. Interpret $\\lambda^{(0)}$ as a reference diploid state, $\\lambda^{(u)}$ as a duplication state, and $\\lambda^{(d)}$ as a deletion state. The sequence begins in the reference state $\\lambda^{(0)}$.\n\nDefine, for any ordered pair of states $(\\alpha \\to \\beta)$ with means $(\\mu_\\alpha,\\mu_\\beta)$, the single-observation log-likelihood ratio increment\n$$\nw_{\\alpha \\to \\beta}(x) \\;=\\; \\log\\!\\left(\\frac{ \\Pr(X=x \\mid X \\sim \\text{Poisson}(\\mu_\\beta)) }{ \\Pr(X=x \\mid X \\sim \\text{Poisson}(\\mu_\\alpha)) }\\right) \\;=\\; x \\log\\!\\left(\\frac{\\mu_\\beta}{\\mu_\\alpha}\\right) - (\\mu_\\beta - \\mu_\\alpha).\n$$\nConsider the following finite-state change model with allowed transitions:\n- From the reference state $\\lambda^{(0)}$, a transition may occur to either $\\lambda^{(u)}$ or $\\lambda^{(d)}$.\n- From the duplication state $\\lambda^{(u)}$, a transition may occur only to $\\lambda^{(0)}$.\n- From the deletion state $\\lambda^{(d)}$, a transition may occur only to $\\lambda^{(0)}$.\n\nFor each currently allowed transition $(\\alpha \\to \\beta)$ at time $t$, define the cumulative sum statistic recursively by\n$$\nS^{(\\alpha \\to \\beta)}_t \\;=\\; \\max\\!\\left(0,\\; S^{(\\alpha \\to \\beta)}_{t-1} \\;+\\; w_{\\alpha \\to \\beta}(x_t)\\right), \\quad \\text{with } S^{(\\alpha \\to \\beta)}_0 = 0.\n$$\nLet $h > 0$ be a fixed threshold. A transition $(\\alpha \\to \\beta)$ is declared to have occurred at time $t$ if $S^{(\\alpha \\to \\beta)}_t > h$. The estimated breakpoint index reported for this transition is the index of the first bin in the current strictly positive run of $S^{(\\alpha \\to \\beta)}$, that is,\n$$\n\\widehat{\\tau} \\;=\\; \\min\\{k \\in \\{1,\\dots,t\\} : S^{(\\alpha \\to \\beta)}_k > 0 \\text{ and } S^{(\\alpha \\to \\beta)}_{k-1} = 0\\}.\n$$\nUpon declaring a transition $(\\alpha \\to \\beta)$ at time $t$ with estimated breakpoint $\\widehat{\\tau}$, the system’s current state changes from $\\alpha$ to $\\beta$, all cumulative sum statistics are reset to zero, and processing continues at the next time index $t+1$ under the newly current state. If, at some time $t$, more than one allowed transition exceeds the threshold, select the one with the largest $S^{(\\alpha \\to \\beta)}_t$; if there is still a tie, select the one whose target mean $\\mu_\\beta$ has the largest absolute difference $|\\mu_\\beta - \\mu_\\alpha|$ from the current state mean.\n\nYou are required to implement the above mathematical specification exactly and return, for each input sequence, the list of estimated breakpoint indices $\\widehat{\\tau}$ in increasing order, using indices starting at $1$. If no transitions are declared for a sequence, return the empty list.\n\nUse natural logarithms for all $\\log(\\cdot)$ computations. There are no physical units to report.\n\nTest suite:\nFor all test cases, use the same parameters $\\lambda^{(0)} = 30$, $\\lambda^{(u)} = 45$, $\\lambda^{(d)} = 15$, and threshold $h = 8$. Each test case provides an observed count sequence $\\{x_t\\}_{t=1}^n$.\n\n- Test case A (single duplication flanked by reference):\n  Sequence of length $40$:\n  $[\\, 28,\\, 31,\\, 33,\\, 29,\\, 30,\\, 27,\\, 34,\\, 32,\\, 31,\\, 29,\\, 44,\\, 47,\\, 42,\\, 46,\\, 45,\\, 43,\\, 48,\\, 44,\\, 46,\\, 43,\\, 45,\\, 47,\\, 44,\\, 46,\\, 43,\\, 31,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29,\\, 32,\\, 28,\\, 30,\\, 31,\\, 29 \\,]$.\n  Expected qualitative structure: reference ($\\lambda^{(0)}$) on bins $1$–$10$, duplication ($\\lambda^{(u)}$) on bins $11$–$25$, reference on bins $26$–$40$.\n\n- Test case B (single deletion flanked by reference):\n  Sequence of length $30$:\n  $[\\, 30,\\, 28,\\, 31,\\, 33,\\, 29,\\, 32,\\, 30,\\, 14,\\, 17,\\, 16,\\, 13,\\, 15,\\, 14,\\, 16,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29,\\, 32,\\, 28,\\, 30,\\, 31 \\,]$.\n  Expected qualitative structure: reference on bins $1$–$7$, deletion ($\\lambda^{(d)}$) on bins $8$–$14$, reference on bins $15$–$30$.\n\n- Test case C (no change; all reference-like):\n  Sequence of length $20$:\n  $[\\, 30,\\, 29,\\, 31,\\, 28,\\, 32,\\, 30,\\, 27,\\, 33,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29 \\,]$.\n\n- Test case D (two separated events: duplication then deletion):\n  Sequence of length $35$:\n  $[\\, 30,\\, 29,\\, 32,\\, 28,\\, 46,\\, 44,\\, 47,\\, 45,\\, 43,\\, 31,\\, 28,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29,\\, 16,\\, 14,\\, 15,\\, 17,\\, 13,\\, 30,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29 \\,]$.\n  Expected qualitative structure: reference on bins $1$–$4$, duplication on bins $5$–$9$, reference on bins $10$–$17$, deletion on bins $18$–$22$, reference on bins $23$–$35$.\n\n- Test case E (event starting at the first bin):\n  Sequence of length $18$:\n  $[\\, 16,\\, 13,\\, 15,\\, 14,\\, 17,\\, 15,\\, 30,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30 \\,]$.\n  Expected qualitative structure: deletion on bins $1$–$6$, reference on bins $7$–$18$.\n\n- Test case F (event extending to the last bin):\n  Sequence of length $18$:\n  $[\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 44,\\, 46,\\, 45,\\, 47,\\, 43,\\, 46,\\, 44,\\, 45,\\, 47 \\,]$.\n  Expected qualitative structure: reference on bins $1$–$9$, duplication on bins $10$–$18$.\n\nYour program must read no input and must use the above six sequences and parameters exactly as provided. It must output a single line containing a list of six items, where each item is the list of detected breakpoint indices for the corresponding test case in order A, B, C, D, E, F. The output format must be a single line with no extra characters, as a comma-separated list enclosed in square brackets; for example, an output conforming to the required format looks like\n[ [a_1,a_2], [b_1], [], [d_1,d_2,d_3], [e_1,e_2], [f_1] ]\nwhere each $a_i,b_i,d_i,e_i,f_i$ is an integer index. Use indices starting at $1$.", "solution": "The problem requires the implementation of a sequential change-point detection algorithm based on the Cumulative Sum (CUSUM) method to identify copy number variations (CNVs) in genomic data. The data are provided as a sequence of read-depth counts, which are modeled as observations from a Poisson distribution. The mean of this distribution is assumed to be piecewise-constant, taking one of three values corresponding to different copy number states: reference ($\\lambda^{(0)}$), duplication ($\\lambda^{(u)}$), and deletion ($\\lambda^{(d)}$).\n\nThe algorithm proceeds sequentially through the data, at each step updating a set of CUSUM statistics. Each statistic tracks the evidence for a specific type of state change (e.g., from reference to duplication). A change is declared when a CUSUM statistic exceeds a predefined threshold $h$. Upon detection, the system state is updated, the breakpoint location is recorded, and all CUSUM statistics are reset to zero.\n\nHere is a step-by-step breakdown of the methodology.\n\nFirst, we define the states and their associated mean parameters:\n- Reference state, $\\mu_0 = \\lambda^{(0)} = 30$.\n- Duplication state, $\\mu_u = \\lambda^{(u)} = 45$.\n- Deletion state, $\\mu_d = \\lambda^{(d)} = 15$.\nThe detection threshold is given as $h=8$. The system starts in the reference state, $\\lambda^{(0)}$.\n\nThe core of the detection method is the log-likelihood ratio (LLR) increment for an observation $x_t$ under a hypothesized transition from a state with mean $\\mu_\\alpha$ to one with mean $\\mu_\\beta$:\n$$\nw_{\\alpha \\to \\beta}(x_t) = x_t \\log\\left(\\frac{\\mu_\\beta}{\\mu_\\alpha}\\right) - (\\mu_\\beta - \\mu_\\alpha)\n$$\nThis value represents the weight of evidence provided by observation $x_t$ in favor of the new state $\\beta$ over the current state $\\alpha$. We must calculate the constant terms for this equation for all possible state transitions defined by the model:\n- Reference to Duplication ($0 \\to u$): $w_{0 \\to u}(x_t) = x_t \\log(45/30) - (45-30) = x_t \\log(1.5) - 15$.\n- Reference to Deletion ($0 \\to d$): $w_{0 \\to d}(x_t) = x_t \\log(15/30) - (15-30) = x_t \\log(0.5) + 15$.\n- Duplication to Reference ($u \\to 0$): $w_{u \\to 0}(x_t) = x_t \\log(30/45) - (30-45) = x_t \\log(2/3) + 15$.\n- Deletion to Reference ($d \\to 0$): $w_{d \\to 0}(x_t) = x_t \\log(30/15) - (30-15) = x_t \\log(2) - 15$.\n\nThe algorithm maintains a CUSUM statistic $S_t$ for each transition allowed from the current state. The statistic is updated at each time step $t$ according to the rule:\n$$\nS^{(\\alpha \\to \\beta)}_t = \\max(0, S^{(\\alpha \\to \\beta)}_{t-1} + w_{\\alpha \\to \\beta}(x_t))\n$$\nThis recursive definition means the CUSUM score accumulates positive LLR increments and resets to zero whenever the cumulative evidence turns negative, preventing the propagation of evidence against a change.\n\nThe algorithm proceeds as follows for a given sequence of counts $\\{x_t\\}_{t=1}^n$:\n1.  Initialize the current state to reference ($\\lambda^{(0)}$), an empty list of breakpoints, and all CUSUM statistics to $0$. We also track, for each CUSUM statistic, the time index at which its current positive run began.\n2.  Iterate through the sequence from $t=1$ to $n$.\n3.  At each time $t$, identify the set of allowed transitions based on the current state.\n    - If in state $\\lambda^{(0)}$, allowed transitions are to $\\lambda^{(u)}$ and $\\lambda^{(d)}$.\n    - If in state $\\lambda^{(u)}$, the only allowed transition is to $\\lambda^{(0)}$.\n    - If in state $\\lambda^{(d)}$, the only allowed transition is to $\\lambda^{(0)}$.\n4.  For each allowed transition $(\\alpha \\to \\beta)$, calculate $w_{\\alpha \\to \\beta}(x_t)$ and update the corresponding CUSUM statistic $S_t$. If the score was $0$ at $t-1$ and becomes positive at $t$, record $t$ as the start of the run.\n5.  Check if any of the updated CUSUM statistics for the allowed transitions exceed the threshold $h$.\n6.  If one or more thresholds are crossed, a change-point is declared. If multiple transitions trigger, the winner is decided by:\n    a. The transition with the largest CUSUM score $S_t$.\n    b. If scores are tied, the transition with the largest absolute difference in means, $|\\mu_\\beta - \\mu_\\alpha|$.\n7.  Upon declaring a winning transition $(\\alpha \\to \\beta)$, the estimated breakpoint index $\\widehat{\\tau}$ is the saved start time of the positive run for that CUSUM statistic. This index $\\widehat{\\tau}$ is appended to the results list.\n8.  The system state is updated to $\\beta$, all CUSUM statistics are reset to $0$, and their associated start-time trackers are cleared.\n9.  The process continues with the next observation at $t+1$.\n10. If the loop completes, the final list of recorded breakpoints is returned.\n\nA critical note on the problem specification must be made. An ambiguity exists in the tie-breaking procedure. If the system is in the reference state $\\lambda^{(0)}$ and both $S^{(0 \\to u)}_t$ and $S^{(0 \\to d)}_t$ exceed $h$ and are equal, the first tie-breaking rule fails. The second rule, which compares $|\\mu_\\beta - \\mu_\\alpha|$, also fails to resolve this specific tie, since $|\\lambda^{(u)} - \\lambda^{(0)}| = |45 - 30| = 15$ and $|\\lambda^{(d)} - \\lambda^{(0)}| = |15 - 30| = 15$. This renders the problem ill-posed. However, this specific tie is unlikely to occur with floating-point arithmetic and the given test data. We proceed under the assumption this scenario is not encountered.\n\nThe implementation will process each test case sequence according to this exact logic, using 1-based indexing for reporting breakpoints as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis of all test cases and print the final result.\n    \"\"\"\n    # Define model parameters\n    params = {\n        \"l0\": 30.0,\n        \"lu\": 45.0,\n        \"ld\": 15.0,\n        \"h\": 8.0,\n    }\n\n    # Define the six test cases from the problem statement\n    test_cases = [\n        # Test case A\n        [28, 31, 33, 29, 30, 27, 34, 32, 31, 29, 44, 47, 42, 46, 45, 43, 48, 44, 46, 43, 45, 47, 44, 46, 43, 31, 28, 32, 29, 30, 33, 27, 31, 30, 29, 32, 28, 30, 31, 29],\n        # Test case B\n        [30, 28, 31, 33, 29, 32, 30, 14, 17, 16, 13, 15, 14, 16, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30, 29, 32, 28, 30, 31],\n        # Test case C\n        [30, 29, 31, 28, 32, 30, 27, 33, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30, 29],\n        # Test case D\n        [30, 29, 32, 28, 46, 44, 47, 45, 43, 31, 28, 30, 33, 27, 31, 30, 29, 16, 14, 15, 17, 13, 30, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30, 29],\n        # Test case E\n        [16, 13, 15, 14, 17, 15, 30, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30],\n        # Test case F\n        [29, 31, 30, 28, 32, 29, 30, 33, 27, 44, 46, 45, 47, 43, 46, 44, 45, 47],\n    ]\n\n    # Process all test cases\n    all_results = [detect_breakpoints(seq, params) for seq in test_cases]\n\n    # Format the combined results into a single string as specified\n    # The result for each case is formatted as '[i,j,k]' (no spaces)\n    # These are then joined by commas into a larger list string: '[[i,j],[k],[]]'\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef detect_breakpoints(x_sequence, params):\n    \"\"\"\n    Implements the CUSUM-based change-point detection algorithm for a single sequence.\n    \"\"\"\n    l0, lu, ld, h = params[\"l0\"], params[\"lu\"], params[\"ld\"], params[\"h\"]\n    n = len(x_sequence)\n    \n    # Pre-calculate log-likelihood ratio terms\n    # w(x) = x * C1 - C2\n    C1 = {\n        '0_to_u': np.log(lu / l0), '0_to_d': np.log(ld / l0),\n        'u_to_0': np.log(l0 / lu), 'd_to_0': np.log(l0 / ld)\n    }\n    C2 = {\n        '0_to_u': lu - l0, '0_to_d': ld - l0,\n        'u_to_0': l0 - lu, 'd_to_0': l0 - ld\n    }\n\n    # System state\n    breakpoints = []\n    current_state_mean = l0\n\n    # CUSUM statistics and start times of positive runs (1-based index)\n    cusums = {key: 0.0 for key in C1}\n    start_times = {key: 0 for key in C1}\n\n    for i in range(n):\n        t = i + 1  # Use 1-based indexing for time and breakpoints\n        x = x_sequence[i]\n\n        allowed_transitions = []\n        if current_state_mean == l0:\n            allowed_transitions = ['0_to_u', '0_to_d']\n        elif current_state_mean == lu:\n            allowed_transitions = ['u_to_0']\n        elif current_state_mean == ld:\n            allowed_transitions = ['d_to_0']\n\n        # Update CUSUMs for allowed transitions\n        for trans_key in allowed_transitions:\n            w = x * C1[trans_key] - C2[trans_key]\n            prev_s = cusums[trans_key]\n            new_s = max(0.0, prev_s + w)\n            cusums[trans_key] = new_s\n\n            if prev_s == 0.0 and new_s > 0.0:\n                start_times[trans_key] = t  # Record start of positive run\n\n        # Check for threshold crossings\n        triggered = [key for key in allowed_transitions if cusums[key] > h]\n\n        if triggered:\n            winner = None\n            if len(triggered) == 1:\n                winner = triggered[0]\n            else: # Must be '0_to_u' and '0_to_d'\n                s_0u = cusums['0_to_u']\n                s_0d = cusums['0_to_d']\n                if s_0u > s_0d:\n                    winner = '0_to_u'\n                elif s_0d > s_0u:\n                    winner = '0_to_d'\n                else: \n                    # Tie on scores. Second tie-breaker on |mu_beta - mu_alpha|.\n                    # |45-30|=15, |15-30|=15. This rule doesn't resolve the tie.\n                    # The problem is technically ill-posed. Assume this case is not hit by test data.\n                    # If it were, we'd have to make an arbitrary choice.\n                    # As a fallback, we could prefer duplication over deletion, for example.\n                    winner = '0_to_u' # Arbitrary choice, not expected to be used.\n\n            # Record breakpoint and update system state\n            breakpoints.append(start_times[winner])\n\n            # Update state\n            if winner == '0_to_u': current_state_mean = lu\n            elif winner == '0_to_d': current_state_mean = ld\n            elif winner in ['u_to_0', 'd_to_0']: current_state_mean = l0\n\n            # Reset all CUSUMs and start times\n            for key in cusums:\n                cusums[key] = 0.0\n                start_times[key] = 0\n\n    return breakpoints\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2382736"}, {"introduction": "While individual signals like read depth are informative, the most robust CNV callers achieve high accuracy by synthesizing multiple, independent lines of evidence. This hands-on practice guides you through building a Bayesian framework to formally combine information from read depth, discordant paired-end reads, and split reads. By calculating the posterior probability for each copy number state, you will learn how to weigh and integrate different data types to make a final, statistically-grounded inference about the presence and type of a CNV. [@problem_id:2382738]", "problem": "You are given a mathematical model to combine evidence for copy number variation (CNV) analysis from three independent sequencing-derived signals: read depth, discordant paired-end counts, and split-read counts. The objective is to compute the posterior distribution over discrete copy number states and to report the maximum a posteriori (MAP) state and its posterior probability for several test cases.\n\nMathematical setup:\n- Let the copy number (CN) state be a discrete random variable $CN \\in \\{0,1,2,3,4\\}$.\n- Let the observed data for a genomic region be the triple $(D,PE,SR)$, where $D$ is a normalized read depth ratio (unitless), $PE$ is a count of discordant paired-ends, and $SR$ is a count of split-reads.\n- Assume conditional independence of $D$, $PE$, and $SR$ given $CN$.\n- The prior distribution over $CN$ is specified by\n$$\nP(CN=0)=0.03,\\quad P(CN=1)=0.07,\\quad P(CN=2)=0.80,\\quad P(CN=3)=0.07,\\quad P(CN=4)=0.03.\n$$\n- The likelihood models are:\n  1. Read depth: $D \\mid CN=k \\sim \\mathcal{N}\\!\\left(\\mu_k,\\sigma^2\\right)$ with $\\mu_k = \\frac{k}{2}$ and $\\sigma^2 = 0.04$.\n  2. Discordant paired-ends: $PE \\mid CN=k \\sim \\mathrm{Pois}\\!\\left(\\lambda_{PE}(k)\\right)$ with $\\lambda_{PE}(k) = \\beta_{PE} + \\alpha_{PE}\\,\\lvert k-2 \\rvert$, where $\\alpha_{PE}=3.0$ and $\\beta_{PE}=0.1$.\n  3. Split-reads: $SR \\mid CN=k \\sim \\mathrm{Pois}\\!\\left(\\lambda_{SR}(k)\\right)$ with $\\lambda_{SR}(k) = \\beta_{SR} + \\alpha_{SR}\\,\\lvert k-2 \\rvert$, where $\\alpha_{SR}=2.0$ and $\\beta_{SR}=0.05$.\n\nBayesian objective:\n- For each observed triple $(D,PE,SR)$, compute the posterior\n$$\nP(CN=k \\mid D,PE,SR) \\propto P(D \\mid CN=k)\\;P(PE \\mid CN=k)\\;P(SR \\mid CN=k)\\;P(CN=k),\n$$\nnormalized over $k \\in \\{0,1,2,3,4\\}$ to obtain a valid probability distribution that sums to $1$.\n- For each case, report the MAP estimate\n$$\n\\hat{k} = \\arg\\max_{k \\in \\{0,1,2,3,4\\}} P(CN=k \\mid D,PE,SR),\n$$\nand the corresponding posterior probability $P(CN=\\hat{k} \\mid D,PE,SR)$.\n\nTest suite:\n- Use the following $5$ observed triples $(D,PE,SR)$:\n  1. $(0.52, 4, 3)$\n  2. $(1.47, 3, 2)$\n  3. $(1.02, 0, 0)$\n  4. $(0.08, 7, 5)$\n  5. $(0.78, 1, 1)$\n\nAnswer specification:\n- For each test case, output the integer $\\hat{k}$ and the float $P(CN=\\hat{k} \\mid D,PE,SR)$ rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, concatenated in the order of the test cases. Specifically, the output format must be\n$$\n[\\hat{k}_1, p_1, \\hat{k}_2, p_2, \\hat{k}_3, p_3, \\hat{k}_4, p_4, \\hat{k}_5, p_5],\n$$\nwhere $p_i$ denotes $P(CN=\\hat{k}_i \\mid D_i,PE_i,SR_i)$ rounded to $6$ decimals. No additional text or lines should be printed.", "solution": "The problem requires the computation of the posterior probability distribution for a discrete copy number state, $CN$, given three independent sources of genomic data: normalized read depth $D$, discordant paired-end counts $PE$, and split-read counts $SR$. The objective is to identify the maximum a posteriori (MAP) state and its associated posterior probability for a given set of observations.\n\nThe analytical framework is based on Bayes' theorem. The copy number state $CN$ is a discrete random variable that can take values in the set $k \\in \\{0, 1, 2, 3, 4\\}$. For each potential state $k$, we seek to calculate the posterior probability $P(CN=k \\mid D, PE, SR)$. According to Bayes' theorem, this is given by:\n$$ P(CN=k \\mid D, PE, SR) = \\frac{P(D, PE, SR \\mid CN=k) P(CN=k)}{P(D, PE, SR)} $$\nThe term $P(CN=k)$ is the prior probability of observing copy number state $k$. The problem provides these priors: $P(CN=0)=0.03$, $P(CN=1)=0.07$, $P(CN=2)=0.80$, $P(CN=3)=0.07$, and $P(CN=4)=0.03$.\n\nThe term $P(D, PE, SR \\mid CN=k)$ is the likelihood of the observed data given the state $k$. The problem states that the data sources $D$, $PE$, and $SR$ are conditionally independent given $CN$. This allows us to factorize the likelihood:\n$$ P(D, PE, SR \\mid CN=k) = P(D \\mid CN=k) \\times P(PE \\mid CN=k) \\times P(SR \\mid CN=k) $$\nEach of these individual likelihood terms is defined by a specific statistical model:\n\n1.  Read Depth Likelihood, $P(D \\mid CN=k)$: The normalized read depth $D$ is modeled by a Gaussian (Normal) distribution, $D \\mid CN=k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$. The probability density function is:\n    $$ P(D \\mid CN=k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(D - \\mu_k)^2}{2\\sigma^2}\\right) $$\n    The parameters are given as $\\mu_k = \\frac{k}{2}$ and the variance $\\sigma^2 = 0.04$.\n\n2.  Discordant Paired-End Likelihood, $P(PE \\mid CN=k)$: The count of discordant paired-ends $PE$ is modeled by a Poisson distribution, $PE \\mid CN=k \\sim \\mathrm{Pois}(\\lambda_{PE}(k))$. The probability mass function is:\n    $$ P(PE \\mid CN=k) = \\frac{\\lambda_{PE}(k)^{PE} e^{-\\lambda_{PE}(k)}}{PE!} $$\n    The rate parameter $\\lambda_{PE}(k)$ is a function of the copy number state $k$, defined as $\\lambda_{PE}(k) = \\beta_{PE} + \\alpha_{PE}\\,\\lvert k-2 \\rvert$, with constants $\\alpha_{PE} = 3.0$ and $\\beta_{PE} = 0.1$.\n\n3.  Split-Read Likelihood, $P(SR \\mid CN=k)$: Similarly, the split-read count $SR$ is modeled by a Poisson distribution, $SR \\mid CN=k \\sim \\mathrm{Pois}(\\lambda_{SR}(k))$. The probability mass function is:\n    $$ P(SR \\mid CN=k) = \\frac{\\lambda_{SR}(k)^{SR} e^{-\\lambda_{SR}(k)}}{SR!} $$\n    The rate parameter is $\\lambda_{SR}(k) = \\beta_{SR} + \\alpha_{SR}\\,\\lvert k-2 \\rvert$, with constants $\\alpha_{SR} = 2.0$ and $\\beta_{SR} = 0.05$.\n\nThe denominator in Bayes' theorem, $P(D, PE, SR)$, is the marginal likelihood or \"evidence\". It serves as a normalization constant that ensures the posterior probabilities sum to $1$ over all possible states of $k$. It is calculated by summing the unnormalized posterior probabilities over all states:\n$$ P(D, PE, SR) = \\sum_{j=0}^{4} P(D, PE, SR \\mid CN=j) P(CN=j) $$\nThus, for each state $k$, the full posterior probability is:\n$$ P(CN=k \\mid D, PE, SR) = \\frac{P(D \\mid CN=k) P(PE \\mid CN=k) P(SR \\mid CN=k) P(CN=k)}{\\sum_{j=0}^{4} P(D \\mid CN=j) P(PE \\mid CN=j) P(SR \\mid CN=j) P(CN=j)} $$\nThe algorithmic procedure for each of the $5$ test cases is as follows:\n1.  For each copy number state $k \\in \\{0, 1, 2, 3, 4\\}$, calculate the unnormalized posterior probability, which is the product of the prior and the three likelihoods evaluated at the given data $(D, PE, SR)$.\n2.  Sum these unnormalized posterior values over all $k$ to compute the normalization constant (evidence).\n3.  Divide each unnormalized posterior by the evidence to obtain the normalized posterior probability for each state $k$.\n4.  Identify the state $\\hat{k}$ that has the maximum posterior probability. This is the MAP estimate: $\\hat{k} = \\arg\\max_{k} P(CN=k \\mid D, PE, SR)$.\n5.  Report the integer MAP state $\\hat{k}$ and its corresponding posterior probability, rounded to $6$ decimal places. This process is repeated for all $5$ provided test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, poisson\n\ndef solve():\n    \"\"\"\n    Computes the MAP estimate and posterior probability for CNV states.\n    \"\"\"\n    # Define the parameter space and models as specified in the problem.\n    cn_states = np.array([0, 1, 2, 3, 4])\n    priors = np.array([0.03, 0.07, 0.80, 0.07, 0.03])\n\n    # 1. Read depth model: D | CN=k ~ N(mu_k, sigma^2)\n    rd_sigma_sq = 0.04\n    rd_sigma = np.sqrt(rd_sigma_sq)\n    rd_mu_k = cn_states / 2.0\n\n    # 2. Discordant paired-end model: PE | CN=k ~ Pois(lambda_pe(k))\n    pe_alpha = 3.0\n    pe_beta = 0.1\n    pe_lambda_k = pe_beta + pe_alpha * np.abs(cn_states - 2)\n\n    # 3. Split-read model: SR | CN=k ~ Pois(lambda_sr(k))\n    sr_alpha = 2.0\n    sr_beta = 0.05\n    sr_lambda_k = sr_beta + sr_alpha * np.abs(cn_states - 2)\n\n    # Test suite of observed data triples (D, PE, SR)\n    test_cases = [\n        (0.52, 4, 3),\n        (1.47, 3, 2),\n        (1.02, 0, 0),\n        (0.08, 7, 5),\n        (0.78, 1, 1),\n    ]\n\n    results = []\n    for D, PE, SR in test_cases:\n        # Array to hold the unnormalized posterior for each CN state k\n        unnormalized_posteriors = np.zeros_like(cn_states, dtype=float)\n\n        for i, k in enumerate(cn_states):\n            # Calculate the likelihood for each data type\n            likelihood_D = norm.pdf(D, loc=rd_mu_k[i], scale=rd_sigma)\n            likelihood_PE = poisson.pmf(PE, mu=pe_lambda_k[i])\n            likelihood_SR = poisson.pmf(SR, mu=sr_lambda_k[i])\n\n            # Combine likelihoods and prior based on conditional independence\n            # This is the unnormalized posterior (proportional to joint probability)\n            unnormalized_posteriors[i] = likelihood_D * likelihood_PE * likelihood_SR * priors[i]\n\n        # Normalize to get the posterior distribution\n        evidence = np.sum(unnormalized_posteriors)\n        \n        # Handle the case where all posteriors are zero (e.g., underflow),\n        # though unlikely with these parameters.\n        if evidence > 0:\n            posterior_probs = unnormalized_posteriors / evidence\n        else:\n            # If evidence is zero, probabilities cannot be determined,\n            # though this indicates an issue. A uniform distribution is a fallback.\n            posterior_probs = np.full_like(cn_states, 1.0 / len(cn_states), dtype=float)\n\n        # Find the MAP estimate (state with the highest posterior probability)\n        map_k_index = np.argmax(posterior_probs)\n        map_k = cn_states[map_k_index]\n        map_prob = posterior_probs[map_k_index]\n\n        # Append results to the list\n        results.append(int(map_k))\n        results.append(f\"{map_prob:.6f}\")\n\n    # Format the final output string as per the specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382738"}]}