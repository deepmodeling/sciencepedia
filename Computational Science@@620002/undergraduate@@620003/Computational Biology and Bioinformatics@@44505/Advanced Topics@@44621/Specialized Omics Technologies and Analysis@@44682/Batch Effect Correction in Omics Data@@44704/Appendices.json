{"hands_on_practices": [{"introduction": "In omics experiments, it is common for technical artifacts, or \"batch effects,\" to be much stronger than the subtle biological signals we aim to detect. This exercise puts you in the driver's seat of this classic challenge by simulating a dataset where a strong, known batch effect masks a weaker biological signal. You will then apply multiple linear regression to precisely estimate and remove the batch effect, thereby recovering the true biological coefficient [@problem_id:2374348]. This practice is fundamental to understanding how including known confounding variables in a linear model can isolate the variable of interest.", "problem": "You are given a mathematical model for a single-feature omics measurement subject to batch effects and a biological signal. For each sample index $i \\in \\{1,\\dots,n\\}$, let $y_i$ denote the measured value, $x_i \\in \\{0,1\\}$ denote a binary biological condition indicator, and let $\\mathrm{batch}(i) \\in \\{1,\\dots,B\\}$ denote the batch membership. The generative model is\n$$\ny_i = \\mu + \\beta x_i + \\gamma_{\\mathrm{batch}(i)} + \\varepsilon_i,\n$$\nwhere $\\mu$ is an intercept, $\\beta$ is the biological effect size of interest, $\\gamma_b$ is the batch-specific offset for batch $b$ with the reference constraint $\\gamma_1 = 0$, and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent Gaussian noise terms. The biological signal magnitude is set to be an order of magnitude weaker than the batch effect, concretely enforced by choosing $|\\gamma_b - \\gamma_{b'}| \\approx 10|\\beta|$ for representative batch differences. All physical quantities in this problem statement are unitless.\n\nFor each test case below, you must construct the dataset exactly as specified and compute the unique coefficient $\\hat{\\beta}$ that minimizes the sum of squared residuals under the linear model\n$$\ny_i = \\theta_0 + \\theta_1 x_i + \\sum_{b=2}^{B} \\delta_b \\mathbb{1}\\{\\mathrm{batch}(i)=b\\} + \\eta_i,\n$$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function, and $(\\theta_0,\\theta_1,\\delta_2,\\dots,\\delta_B)$ are free parameters. The reported estimate must be the minimizer $\\hat{\\theta}_1$, which serves as the batch-adjusted estimate of the biological effect $\\beta$.\n\nData construction details to ensure determinism:\n- For each batch $b \\in \\{1,\\dots,B\\}$, you are given the number of samples $n_b$ and the number $k_b$ of samples with $x_i=1$ in that batch. Construct the vector $x$ within each batch by placing $k_b$ ones followed by $(n_b - k_b)$ zeros; concatenate batches in increasing $b$.\n- Construct the vector of batch offsets by assigning $\\gamma_{\\mathrm{batch}(i)}$ to sample $i$ with $\\gamma_1 = 0$ and the provided $\\gamma_b$ for $b \\ge 2$.\n- The intercept $\\mu$, the biological coefficient $\\beta$, and noise standard deviation $\\sigma$ are given per test.\n- Draw $\\varepsilon_i$ independently from the Gaussian distribution $\\mathcal{N}(0,\\sigma^2)$ using a pseudo-random number generator initialized with the given integer seed for that test case, and set $y_i = \\mu + \\beta x_i + \\gamma_{\\mathrm{batch}(i)} + \\varepsilon_i$.\n\nTest suite of parameter sets:\n- Test $1$:\n  - Batches: $B=2$ with $(n_1,n_2)=(50,50)$.\n  - Biological condition counts: $(k_1,k_2)=(25,25)$.\n  - Parameters: $\\mu=7.0$, $\\beta=0.2$, $(\\gamma_1,\\gamma_2)=(0.0,2.0)$, $\\sigma=0.4$.\n  - Seed: $123$.\n- Test $2$:\n  - Batches: $B=2$ with $(n_1,n_2)=(40,60)$.\n  - Biological condition counts: $(k_1,k_2)=(8,42)$.\n  - Parameters: $\\mu=5.0$, $\\beta=0.2$, $(\\gamma_1,\\gamma_2)=(0.0,2.0)$, $\\sigma=0.5$.\n  - Seed: $456$.\n- Test $3$:\n  - Batches: $B=3$ with $(n_1,n_2,n_3)=(30,50,40)$.\n  - Biological condition counts: $(k_1,k_2,k_3)=(10,25,15)$.\n  - Parameters: $\\mu=10.0$, $\\beta=0.15$, $(\\gamma_1,\\gamma_2,\\gamma_3)=(0.0,1.5,-0.5)$, $\\sigma=0.6$.\n  - Seed: $789$.\n- Test $4$:\n  - Batches: $B=4$ with $(n_1,n_2,n_3,n_4)=(5,5,5,5)$.\n  - Biological condition counts: $(k_1,k_2,k_3,k_4)=(2,3,2,3)$.\n  - Parameters: $\\mu=3.0$, $\\beta=0.1$, $(\\gamma_1,\\gamma_2,\\gamma_3,\\gamma_4)=(0.0,1.0,-1.0,0.5)$, $\\sigma=0.2$.\n  - Seed: $2468$.\n\nRequirements:\n- For each test, build the design matrix with a column of ones (intercept), one column for $x$, and $(B-1)$ indicator columns for batches $b \\in \\{2,\\dots,B\\}$.\n- Compute the unique minimizer $\\hat{\\theta}$ of the sum of squared residuals and report $\\hat{\\theta}_1$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the tests, with each value rounded to exactly six decimal places (for example, $[0.123456,0.000000,1.500000,2.718282]$).", "solution": "The problem statement presented is subjected to rigorous validation.\n\nStep 1: Extract Givens\nThe problem provides a generative model for omics data $y_i$ for a sample $i \\in \\{1,\\dots,n\\}$:\n$$\ny_i = \\mu + \\beta x_i + \\gamma_{\\mathrm{batch}(i)} + \\varepsilon_i\n$$\nwhere:\n- $y_i$: the measured value.\n- $x_i \\in \\{0,1\\}$: a binary biological condition indicator.\n- $\\mathrm{batch}(i) \\in \\{1,\\dots,B\\}$: the batch membership of sample $i$.\n- $\\mu$: a global intercept.\n- $\\beta$: the biological effect size.\n- $\\gamma_b$: the batch-specific offset for batch $b$, with $\\gamma_1 = 0$.\n- $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$: independent and identically distributed Gaussian noise terms.\n\nThe task is to estimate $\\beta$ by fitting the linear model:\n$$\ny_i = \\theta_0 + \\theta_1 x_i + \\sum_{b=2}^{B} \\delta_b \\mathbb{1}\\{\\mathrm{batch}(i)=b\\} + \\eta_i\n$$\nand finding the coefficient $\\hat{\\theta}_1$ that minimizes the sum of squared residuals. Here, $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. The parameters to be estimated are $(\\theta_0, \\theta_1, \\delta_2, \\dots, \\delta_B)$.\n\nData construction rules:\n- For each batch $b \\in \\{1,\\dots,B\\}$, the number of samples is $n_b$ and the number of samples with $x_i=1$ is $k_b$. The vector $x$ within each batch is constructed by $k_b$ ones followed by $(n_b - k_b)$ zeros. The full data vectors are formed by concatenating batch data in increasing order of $b$.\n- The batch offset vector is constructed using the given $\\gamma_b$ values.\n- $\\mu$, $\\beta$, and $\\sigma$ are provided for each test case.\n- The noise vector $\\varepsilon$ is generated from $\\mathcal{N}(0,\\sigma^2)$ using a pseudo-random number generator initialized with a specified integer seed for each test case.\n\nFour test cases are provided with specific parameters for $(B, \\{n_b\\}, \\{k_b\\}, \\mu, \\beta, \\{\\gamma_b\\}, \\sigma, \\mathrm{seed})$.\n\nStep 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is set in the context of batch effect correction in omics data, a standard and critical topic in computational biology and bioinformatics. The use of a linear model to represent additive batch effects and a biological signal, and then using another linear model (multiple regression) to correct for these effects, is a fundamental and widely accepted method (e.g., as implemented in the ComBat algorithm or as a direct regression). The premise is scientifically sound.\n2.  **Well-Posed**: The problem asks for the Ordinary Least Squares (OLS) solution to a multiple linear regression problem. A unique solution exists if and only if the design matrix has full column rank. The design matrix consists of an intercept, the biological predictor vector $x$, and $B-1$ dummy variables for the batches. For the matrix to be rank-deficient, one column must be a linear combination of the others. This would happen if the biological variable $x$ is perfectly confounded with batch membership (e.g., all samples in one batch have $x=1$ and all samples in another have $x=0$). A review of the provided test cases confirms that for each case, every batch contains samples with both $x_i=1$ and $x_i=0$. This ensures that there is no perfect multicollinearity between the biological predictor and the batch indicators. Therefore, the design matrix will have full column rank for all test cases, guaranteeing a unique solution. The problem is well-posed.\n3.  **Objective**: The problem is specified using precise mathematical notation and unambiguous algorithmic instructions for data generation and analysis. It is free from subjective claims.\n\nStep 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, objective, and provides a complete, consistent set of specifications. Proceeding with the solution.\n\nThe problem requires the estimation of a parameter in a multiple linear regression model. The model can be expressed in matrix form as:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\eta}\n$$\nwhere $\\mathbf{y}$ is the $n \\times 1$ vector of observed values, $\\mathbf{X}$ is the $n \\times (2+B-1)$ design matrix, $\\boldsymbol{\\theta}$ is the $(2+B-1) \\times 1$ vector of parameters to be estimated, and $\\boldsymbol{\\eta}$ is the $n \\times 1$ vector of residual errors. The parameter vector is $\\boldsymbol{\\theta} = [\\theta_0, \\theta_1, \\delta_2, \\dots, \\delta_B]^T$.\n\nThe objective is to find the parameter vector $\\hat{\\boldsymbol{\\theta}}$ that minimizes the sum of squared residuals (SSR), given by:\n$$\n\\mathrm{SSR}(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} \\eta_i^2 = \\boldsymbol{\\eta}^T\\boldsymbol{\\eta} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\n$$\nThe solution to this minimization problem is the Ordinary Least Squares (OLS) estimator, which is found by solving the normal equations:\n$$\n(\\mathbf{X}^T \\mathbf{X}) \\boldsymbol{\\theta} = \\mathbf{X}^T \\mathbf{y}\n$$\nAssuming the matrix $\\mathbf{X}^T \\mathbf{X}$ is invertible (which we have established it is for the given test cases), the unique solution is:\n$$\n\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nFor each test case, the procedure is as follows:\n1.  Construct the total sample size $n = \\sum_{b=1}^{B} n_b$.\n2.  Construct the $n \\times 1$ biological condition vector $\\mathbf{x}$ by concatenating blocks of $k_b$ ones and $(n_b - k_b)$ zeros for $b=1, \\dots, B$.\n3.  Construct the $n \\times 1$ batch assignment vector, where each entry identifies the batch for the corresponding sample.\n4.  Construct the $n \\times 1$ true batch effect vector $\\boldsymbol{\\gamma}$ using the provided $\\gamma_b$ values and the batch assignment vector.\n5.  Initialize a pseudo-random number generator with the given seed. Generate the $n \\times 1$ noise vector $\\boldsymbol{\\varepsilon}$ by drawing from $\\mathcal{N}(0, \\sigma^2)$.\n6.  Generate the $n \\times 1$ measurement vector $\\mathbf{y}$ according to the generative model: $\\mathbf{y} = \\mu\\mathbf{1} + \\beta\\mathbf{x} + \\boldsymbol{\\gamma} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{1}$ is a vector of ones.\n7.  Construct the $n \\times (B+1)$ design matrix $\\mathbf{X}$.\n    - The first column is an intercept, an $n \\times 1$ vector of ones.\n    - The second column is the biological condition vector $\\mathbf{x}$.\n    - The subsequent $B-1$ columns are indicator (dummy) variables for batches $2, \\dots, B$. The $j$-th dummy variable column (for $j \\in \\{2, \\dots, B\\}$) has a $1$ for samples in batch $j$ and $0$ otherwise.\n8.  Solve the OLS problem for $\\hat{\\boldsymbol{\\theta}}$. Computationally, this is best done using a stable numerical method like QR decomposition, which is implemented in libraries such as NumPy via the `linalg.lstsq` function.\n9.  The desired result is the second element of the resulting vector $\\hat{\\boldsymbol{\\theta}}$, which corresponds to the estimate $\\hat{\\theta}_1$ of the biological coefficient $\\beta$.\n\nThis procedure is deterministic and will be applied to each test case to compute the required values.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the OLS problem for batch effect correction across multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test 1\n        {\n            \"B\": 2, \"n\": (50, 50), \"k\": (25, 25),\n            \"mu\": 7.0, \"beta\": 0.2, \"gamma\": (0.0, 2.0), \"sigma\": 0.4,\n            \"seed\": 123\n        },\n        # Test 2\n        {\n            \"B\": 2, \"n\": (40, 60), \"k\": (8, 42),\n            \"mu\": 5.0, \"beta\": 0.2, \"gamma\": (0.0, 2.0), \"sigma\": 0.5,\n            \"seed\": 456\n        },\n        # Test 3\n        {\n            \"B\": 3, \"n\": (30, 50, 40), \"k\": (10, 25, 15),\n            \"mu\": 10.0, \"beta\": 0.15, \"gamma\": (0.0, 1.5, -0.5), \"sigma\": 0.6,\n            \"seed\": 789\n        },\n        # Test 4\n        {\n            \"B\": 4, \"n\": (5, 5, 5, 5), \"k\": (2, 3, 2, 3),\n            \"mu\": 3.0, \"beta\": 0.1, \"gamma\": (0.0, 1.0, -1.0, 0.5), \"sigma\": 0.2,\n            \"seed\": 2468\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Extract parameters for the current test case\n        B = case[\"B\"]\n        n_b = case[\"n\"]\n        k_b = case[\"k\"]\n        mu = case[\"mu\"]\n        beta = case[\"beta\"]\n        gamma_params = case[\"gamma\"]\n        sigma = case[\"sigma\"]\n        seed = case[\"seed\"]\n\n        n_total = sum(n_b)\n\n        # 1. Construct data vectors: x, batch_ids, gamma_vec\n        x_parts = []\n        batch_id_parts = []\n        gamma_vec_parts = []\n\n        for b in range(B):\n            # Construct x vector part for batch b\n            x_b = np.concatenate([np.ones(k_b[b]), np.zeros(n_b[b] - k_b[b])])\n            x_parts.append(x_b)\n\n            # Construct batch_id part for batch b (1-indexed)\n            batch_id_parts.append(np.full(n_b[b], b + 1))\n\n            # Construct gamma vector part for batch b\n            gamma_b = np.full(n_b[b], gamma_params[b])\n            gamma_vec_parts.append(gamma_b)\n\n        x = np.concatenate(x_parts)\n        batch_ids = np.concatenate(batch_id_parts)\n        gamma_vec = np.concatenate(gamma_vec_parts)\n\n        # 2. Generate noise and the response variable y\n        rng = np.random.default_rng(seed)\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n_total)\n        y = mu + beta * x + gamma_vec + epsilon\n\n        # 3. Construct the design matrix X\n        # Column 1: Intercept\n        intercept_col = np.ones(n_total)\n        \n        # Column 2: Biological condition x\n        x_col = x\n        \n        # Columns 3 to B+1: Batch dummy variables for batches 2 to B\n        dummy_cols = []\n        if B > 1:\n            for b_idx in range(2, B + 1):\n                dummy_col = (batch_ids == b_idx).astype(float)\n                dummy_cols.append(dummy_col)\n        \n        design_matrix_cols = [intercept_col, x_col] + dummy_cols\n        X = np.column_stack(design_matrix_cols)\n\n        # 4. Solve the Ordinary Least Squares problem\n        # theta_hat = (X^T X)^-1 X^T y\n        # We use np.linalg.lstsq for numerical stability\n        theta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        # 5. Extract the estimated biological effect (theta_1_hat)\n        beta_hat = theta_hat[1]\n        \n        results.append(beta_hat)\n\n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "2374348"}, {"introduction": "Batch effects do not always manifest as simple additive shifts in measurements; they can also be multiplicative, scaling the data in a batch-specific manner, as in the model $Y_{ij} = \\alpha_i \\cdot \\beta_j \\cdot \\epsilon_{ij}$. This scenario requires a different strategy, which you will implement in this practice by applying a natural logarithm transformation to convert the multiplicative model into a linear, additive one. This approach, which makes the problem solvable with least-squares methods, demonstrates the power of data transformation and highlights the principles of decomposing variation [@problem_id:2374360].", "problem": "You are given a strictly positive data-generating model for aggregated omics intensities. For batch index $i$ in $\\{1,\\dots,B\\}$ and feature index $j$ in $\\{1,\\dots,F\\}$, the observed quantity $Y_{ij}$ is modeled as\n$$\nY_{ij} = \\alpha_i \\cdot \\beta_j \\cdot \\epsilon_{ij},\n$$\nwhere $\\alpha_i \\gt 0$ is an unknown batch-specific multiplicative effect, $\\beta_j \\gt 0$ is an unknown feature-specific multiplicative effect, and $\\epsilon_{ij} \\gt 0$ is a multiplicative noise factor. Assume that $\\log \\epsilon_{ij}$ has zero mean and finite variance, and that all $Y_{ij}$ are strictly positive so that the natural logarithm is well-defined.\n\nDefine a batch effect correction strategy as follows. Let $L_{ij} = \\log Y_{ij}$. Let $(a_1,\\dots,a_B)$ and $(b_1,\\dots,b_F)$ be real-valued parameters that solve the constrained least-squares problem\n$$\n\\min_{\\{a_i\\},\\{b_j\\}} \\; \\sum_{i=1}^{B} \\sum_{j=1}^{F} \\left(L_{ij} - a_i - b_j\\right)^2\n\\quad \\text{subject to} \\quad \\sum_{i=1}^{B} a_i = 0,\\;\\; \\sum_{j=1}^{F} b_j = 0.\n$$\nInterpret $\\exp(a_i)$ as an estimate of the batch effect up to a global multiplicative constant and $\\exp(b_j)$ as an estimate of the feature effect up to a global multiplicative constant. The correction of batch effects for observation $Y_{ij}$ is given by $Z_{ij} = Y_{ij} \\cdot \\exp(-a_i)$, which is invariant to the common multiplicative scaling ambiguity.\n\nFor evaluation, compare the estimated effects in log-scale to the centered ground truth. Define the centered true batch effects $s_i = \\log \\alpha_i - \\frac{1}{B}\\sum_{k=1}^{B} \\log \\alpha_k$ and the centered true feature effects $t_j = \\log \\beta_j - \\frac{1}{F}\\sum_{\\ell=1}^{F} \\log \\beta_\\ell$. Let $\\widehat{a}_i$ and $\\widehat{b}_j$ denote the solutions to the optimization above. For each dataset, compute the single summary error\n$$\nE = \\max\\!\\left\\{ \\max_{1 \\le i \\le B} \\left| \\widehat{a}_i - s_i \\right|,\\; \\max_{1 \\le j \\le F} \\left| \\widehat{b}_j - t_j \\right| \\right\\}.\n$$\nAll logarithms are natural logarithms. There are no physical units involved; all quantities are dimensionless. Report each $E$ as a real number rounded to six digits after the decimal point.\n\nTest suite. For each test case below, you are given $B$, $F$, and explicit arrays for $(\\alpha_i)$, $(\\beta_j)$, and $(\\epsilon_{ij})$. Construct $Y_{ij}$ using the model above for all indices and then compute $E$ for that dataset.\n\n- Test case $1$ (general, noiseless):\n  - $B = 3$, $F = 4$.\n  - $(\\alpha_i) = \\left[\\, 2.0,\\; 0.5,\\; 1.5 \\,\\right]$.\n  - $(\\beta_j) = \\left[\\, 10.0,\\; 5.0,\\; 20.0,\\; 8.0 \\,\\right]$.\n  - $(\\epsilon_{ij})$ is the $3 \\times 4$ matrix with all entries equal to $1.0$.\n\n- Test case $2$ (general, with mild multiplicative noise):\n  - $B = 2$, $F = 5$.\n  - $(\\alpha_i) = \\left[\\, 3.0,\\; 0.25 \\,\\right]$.\n  - $(\\beta_j) = \\left[\\, 7.0,\\; 1.4,\\; 0.9,\\; 11.0,\\; 4.0 \\,\\right]$.\n  - $(\\epsilon_{ij})$ is the $2 \\times 5$ matrix\n    $$\n    \\begin{bmatrix}\n    1.02 & 0.97 & 1.05 & 0.94 & 1.01 \\\\\n    0.98 & 1.03 & 0.95 & 1.06 & 0.99\n    \\end{bmatrix}.\n    $$\n\n- Test case $3$ (no batch effect, noiseless):\n  - $B = 3$, $F = 3$.\n  - $(\\alpha_i) = \\left[\\, 1.0,\\; 1.0,\\; 1.0 \\,\\right]$.\n  - $(\\beta_j) = \\left[\\, 3.0,\\; 9.0,\\; 0.6 \\,\\right]$.\n  - $(\\epsilon_{ij})$ is the $3 \\times 3$ matrix with all entries equal to $1.0$.\n\n- Test case $4$ (single batch, noiseless):\n  - $B = 1$, $F = 4$.\n  - $(\\alpha_i) = \\left[\\, 2.5 \\,\\right]$.\n  - $(\\beta_j) = \\left[\\, 0.5,\\; 5.0,\\; 1.25,\\; 10.0 \\,\\right]$.\n  - $(\\epsilon_{ij})$ is the $1 \\times 4$ matrix with all entries equal to $1.0$.\n\n- Test case $5$ (single feature, mild noise across batches):\n  - $B = 4$, $F = 1$.\n  - $(\\alpha_i) = \\left[\\, 1.2,\\; 0.8,\\; 2.0,\\; 0.5 \\,\\right]$.\n  - $(\\beta_j) = \\left[\\, 3.3 \\,\\right]$.\n  - $(\\epsilon_{ij})$ is the $4 \\times 1$ matrix\n    $$\n    \\begin{bmatrix}\n    1.00 \\\\\n    0.95 \\\\\n    1.05 \\\\\n    1.00\n    \\end{bmatrix}.\n    $$\n\nProgram requirements. Your program must, for each test case, construct $Y_{ij}$ from the provided $(\\alpha_i)$, $(\\beta_j)$, and $(\\epsilon_{ij})$, solve the constrained least-squares problem above to obtain $(\\widehat{a}_i)$ and $(\\widehat{b}_j)$, compute $E$ as defined, round $E$ to six digits after the decimal point, and finally print a single line containing the results for the five test cases as a comma-separated list enclosed in square brackets, for example $[e_1,e_2,e_3,e_4,e_5]$, where each $e_k$ is the rounded value for test case $k$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in standard statistical methods for bioinformatics, is mathematically well-posed with all necessary information provided, and is expressed in objective, unambiguous terms. There are no scientific flaws, contradictions, or ill-posed structures. We may therefore proceed with the derivation of a solution.\n\nThe problem requires the computation of an error metric based on the solution to a constrained least-squares problem arising from a multiplicative model for omics data. The model is given by:\n$$\nY_{ij} = \\alpha_i \\cdot \\beta_j \\cdot \\epsilon_{ij}\n$$\nwhere $Y_{ij} > 0$, $\\alpha_i > 0$, $\\beta_j > 0$, and $\\epsilon_{ij} > 0$. The first step is to linearize the model by taking the natural logarithm:\n$$\nL_{ij} = \\log Y_{ij} = \\log \\alpha_i + \\log \\beta_j + \\log \\epsilon_{ij}\n$$\nThis transforms the multiplicative effects into additive components. Let us define $A_i = \\log \\alpha_i$, $B_j = \\log \\beta_j$, and $E_{ij} = \\log \\epsilon_{ij}$. The model becomes:\n$$\nL_{ij} = A_i + B_j + E_{ij}\n$$\nThe problem is to find parameters $\\{a_i\\}_{i=1}^B$ and $\\{b_j\\}_{j=1}^F$ that solve the constrained minimization problem:\n$$\n\\min_{\\{a_i\\},\\{b_j\\}} \\; S = \\sum_{i=1}^{B} \\sum_{j=1}^{F} \\left(L_{ij} - a_i - b_j\\right)^2\n\\quad \\text{subject to} \\quad \\sum_{i=1}^{B} a_i = 0 \\text{ and } \\sum_{j=1}^{F} b_j = 0.\n$$\nThis is a standard problem in statistics, equivalent to a two-way analysis of variance (ANOVA) decomposition without an explicit intercept term, using sum-to-zero constraints for identifiability. We can find the optimal parameters $\\widehat{a}_i$ and $\\widehat{b}_j$ using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(\\{a_i\\}, \\{b_j\\}, \\lambda_1, \\lambda_2) = \\sum_{i=1}^{B} \\sum_{j=1}^{F} (L_{ij} - a_i - b_j)^2 + \\lambda_1 \\sum_{i=1}^{B} a_i + \\lambda_2 \\sum_{j=1}^{F} b_j\n$$\nSetting the partial derivatives with respect to $a_k$ and $b_k$ to zero yields the Karush-Kuhn-Tucker (KKT) conditions:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial a_k} = -2\\sum_{j=1}^{F} (L_{kj} - a_k - b_j) + \\lambda_1 = 0 \\quad \\implies \\quad \\sum_{j=1}^{F} L_{kj} - F a_k - \\sum_{j=1}^{F} b_j = \\frac{\\lambda_1}{2}\n$$\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial b_k} = -2\\sum_{i=1}^{B} (L_{ik} - a_i - b_k) + \\lambda_2 = 0 \\quad \\implies \\quad \\sum_{i=1}^{B} L_{ik} - \\sum_{i=1}^{B} a_i - B b_k = \\frac{\\lambda_2}{2}\n$$\nApplying the constraints $\\sum a_i = 0$ and $\\sum b_j = 0$ to these equations simplifies them:\n$$\n\\sum_{j=1}^{F} L_{kj} - F a_k = \\frac{\\lambda_1}{2} \\quad \\implies \\quad a_k = \\frac{1}{F}\\sum_{j=1}^{F} L_{kj} - \\frac{\\lambda_1}{2F}\n$$\n$$\n\\sum_{i=1}^{B} L_{ik} - B b_k = \\frac{\\lambda_2}{2} \\quad \\implies \\quad b_k = \\frac{1}{B}\\sum_{i=1}^{B} L_{ik} - \\frac{\\lambda_2}{2B}\n$$\nWe determine the Lagrange multipliers $\\lambda_1$ and $\\lambda_2$ by substituting these expressions for $a_k$ and $b_k$ back into the constraint equations.\n$$\n\\sum_{k=1}^{B} a_k = 0 \\implies \\sum_{k=1}^{B} \\left(\\frac{1}{F}\\sum_{j=1}^{F} L_{kj}\\right) - B\\frac{\\lambda_1}{2F} = 0 \\implies \\frac{1}{F}L_{..} = B\\frac{\\lambda_1}{2F} \\implies \\frac{\\lambda_1}{2} = \\frac{L_{..}}{B}\n$$\n$$\n\\sum_{k=1}^{F} b_k = 0 \\implies \\sum_{k=1}^{F} \\left(\\frac{1}{B}\\sum_{i=1}^{B} L_{ik}\\right) - F\\frac{\\lambda_2}{2B} = 0 \\implies \\frac{1}{B}L_{..} = F\\frac{\\lambda_2}{2B} \\implies \\frac{\\lambda_2}{2} = \\frac{L_{..}}{F}\n$$\nwhere $L_{..} = \\sum_{i=1}^{B}\\sum_{j=1}^{F} L_{ij}$ is the grand sum. Let us introduce sample means:\n$\\bar{L}_{i.} = \\frac{1}{F}\\sum_{j=1}^{F} L_{ij}$ (row means), $\\bar{L}_{.j} = \\frac{1}{B}\\sum_{i=1}^{B} L_{ij}$ (column means), and $\\bar{L}_{..} = \\frac{1}{BF}\\sum_{i=1}^{B}\\sum_{j=1}^{F} L_{ij}$ (grand mean).\nThe estimators are then:\n$$\n\\widehat{a}_i = \\bar{L}_{i.} - \\frac{1}{F}\\frac{L_{..}}{B} = \\bar{L}_{i.} - \\frac{BF\\bar{L}_{..}}{BF} = \\bar{L}_{i.} - \\bar{L}_{..}\n$$\n$$\n\\widehat{b}_j = \\bar{L}_{.j} - \\frac{1}{B}\\frac{L_{..}}{F} = \\bar{L}_{.j} - \\frac{BF\\bar{L}_{..}}{BF} = \\bar{L}_{.j} - \\bar{L}_{..}\n$$\nThe problem asks to compare these estimators to the centered true effects, defined as $s_i = \\log \\alpha_i - \\frac{1}{B}\\sum_k \\log \\alpha_k = A_i - \\bar{A}$ and $t_j = \\log \\beta_j - \\frac{1}{F}\\sum_\\ell \\log \\beta_\\ell = B_j - \\bar{B}$.\n\nTo find the estimation error, we substitute the additive model $L_{ij} = A_i + B_j + E_{ij}$ into the expressions for the estimators.\n$$\n\\bar{L}_{i.} = \\frac{1}{F}\\sum_{j=1}^{F}(A_i + B_j + E_{ij}) = A_i + \\bar{B} + \\bar{E}_{i.}\n$$\n$$\n\\bar{L}_{..} = \\frac{1}{BF}\\sum_{i=1}^{B}\\sum_{j=1}^{F}(A_i + B_j + E_{ij}) = \\bar{A} + \\bar{B} + \\bar{E}_{..}\n$$\nThe estimator for the batch effect is:\n$$\n\\widehat{a}_i = \\bar{L}_{i.} - \\bar{L}_{..} = (A_i + \\bar{B} + \\bar{E}_{i.}) - (\\bar{A} + \\bar{B} + \\bar{E}_{..}) = (A_i - \\bar{A}) + (\\bar{E}_{i.} - \\bar{E}_{..})\n$$\nThe error in the batch effect estimate is therefore:\n$$\n\\widehat{a}_i - s_i = (\\bar{E}_{i.} - \\bar{E}_{..})\n$$\nBy symmetry, the error in the feature effect estimate is:\n$$\n\\widehat{b}_j - t_j = (\\bar{E}_{.j} - \\bar{E}_{..})\n$$\nThe summary error $E$ is defined as the maximum of the absolute values of these estimation errors:\n$$\nE = \\max\\!\\left\\{ \\max_{1 \\le i \\le B} \\left| \\bar{E}_{i.} - \\bar{E}_{..} \\right|,\\; \\max_{1 \\le j \\le F} \\left| \\bar{E}_{.j} - \\bar{E}_{..} \\right| \\right\\}\n$$\nThis derivation shows that the estimation error $E$ depends only on the matrix of log-transformed noise values, $E_{ij} = \\log \\epsilon_{ij}$. The true effects $\\alpha_i$ and $\\beta_j$ do not influence the error metric $E$ at all.\n\nFor test cases $1$, $3$, and $4$, the noise matrix $\\epsilon_{ij}$ contains only entries equal to $1.0$. Thus, $E_{ij} = \\log(1.0) = 0$ for all $i,j$. In this noiseless scenario, all error terms are zero: $\\bar{E}_{i.}=0$, $\\bar{E}_{.j}=0$, $\\bar{E}_{..}=0$. Consequently, the summary error $E$ is exactly $0$ for these cases.\n\nFor test cases $2$ and $5$, the noise is non-trivial. The error $E$ must be calculated by first computing the matrix of $E_{ij} = \\log\\epsilon_{ij}$, then its row means, column means, and grand mean, and finally the maximum absolute deviation as per the formula for $E$. The entire computation for all test cases can be implemented based on this simplified formula.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the batch effect correction problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general, noiseless)\n        {\n            \"B\": 3, \"F\": 4,\n            \"alpha\": [2.0, 0.5, 1.5],\n            \"beta\": [10.0, 5.0, 20.0, 8.0],\n            \"epsilon\": [[1.0] * 4] * 3,\n        },\n        # Test case 2 (general, with mild multiplicative noise)\n        {\n            \"B\": 2, \"F\": 5,\n            \"alpha\": [3.0, 0.25],\n            \"beta\": [7.0, 1.4, 0.9, 11.0, 4.0],\n            \"epsilon\": [\n                [1.02, 0.97, 1.05, 0.94, 1.01],\n                [0.98, 1.03, 0.95, 1.06, 0.99]\n            ],\n        },\n        # Test case 3 (no batch effect, noiseless)\n        {\n            \"B\": 3, \"F\": 3,\n            \"alpha\": [1.0, 1.0, 1.0],\n            \"beta\": [3.0, 9.0, 0.6],\n            \"epsilon\": [[1.0] * 3] * 3,\n        },\n        # Test case 4 (single batch, noiseless)\n        {\n            \"B\": 1, \"F\": 4,\n            \"alpha\": [2.5],\n            \"beta\": [0.5, 5.0, 1.25, 10.0],\n            \"epsilon\": [[1.0] * 4],\n        },\n        # Test case 5 (single feature, mild noise across batches)\n        {\n            \"B\": 4, \"F\": 1,\n            \"alpha\": [1.2, 0.8, 2.0, 0.5],\n            \"beta\": [3.3],\n            \"epsilon\": [\n                [1.00],\n                [0.95],\n                [1.05],\n                [1.00]\n            ],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # As derived in the solution, the error E depends only on the noise term epsilon.\n        # We do not need alpha, beta, B, or F, except to structure the epsilon matrix.\n        epsilon_matrix = np.array(case[\"epsilon\"])\n        \n        # log_epsilon_matrix is the matrix E_ij from the derivation.\n        log_epsilon_matrix = np.log(epsilon_matrix)\n        \n        # Calculate row means (E_i.), column means (E_.j), and grand mean (E_..)\n        grand_mean = np.mean(log_epsilon_matrix)\n        row_means = np.mean(log_epsilon_matrix, axis=1)\n        col_means = np.mean(log_epsilon_matrix, axis=0)\n        \n        # Calculate estimation errors for batch and feature effects.\n        # err_a corresponds to (a_hat_i - s_i)\n        err_a = row_means - grand_mean\n        # err_b corresponds to (b_hat_j - t_j)\n        err_b = col_means - grand_mean\n        \n        # Find the maximum absolute error for each set of effects.\n        # The size check handles potential empty arrays, although not expected here.\n        max_err_a = np.max(np.abs(err_a)) if err_a.size > 0 else 0.0\n        max_err_b = np.max(np.abs(err_b)) if err_b.size > 0 else 0.0\n        \n        # The total error E is the maximum of these two.\n        E = max(max_err_a, max_err_b)\n        \n        # Format the result to six decimal places as required.\n        results.append(f\"{E:.6f}\")\n\n    # Print the final result in the specified format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2374360"}, {"introduction": "Perhaps the most challenging scenario in data analysis is when you observe systematic variation but do not know its source because batch labels are missing. This advanced practice introduces a powerful technique inspired by Surrogate Variable Analysis (SVA) to address this issue. You will use linear algebra to identify a latent, or \"surrogate,\" batch subspace from the data residuals and project its influence out of the data, preserving the known biological signal [@problem_id:2374389]. This exercise provides a deep, hands-on understanding of how matrix decomposition can be used to disentangle known and unknown sources of variation.", "problem": "You are given a mathematical model for omics expression data with unknown batch effects. Let there be a data matrix $X \\in \\mathbb{R}^{p \\times n}$ of $p$ features (e.g., genes) measured across $n$ samples, a known design matrix of covariates $C \\in \\mathbb{R}^{n \\times q}$ (e.g., phenotype), an unknown batch-factor design $H \\in \\mathbb{R}^{n \\times k}$, and corresponding feature loadings $B \\in \\mathbb{R}^{p \\times q}$ and $A \\in \\mathbb{R}^{p \\times k}$. The data are generated by the linear model\n$$\nX \\;=\\; B C^\\top \\;+\\; A H^\\top \\;+\\; E,\n$$\nwhere $E \\in \\mathbb{R}^{p \\times n}$ is mean-zero noise.\n\nThe objective is to construct, from $X$ and $C$ only, an estimator $\\widehat{H} \\in \\mathbb{R}^{n \\times k_{\\text{est}}}$ of a $k_{\\text{est}}$-dimensional latent batch subspace and to produce a batch-corrected matrix $X_{\\text{corr}} \\in \\mathbb{R}^{p \\times n}$ that removes the linear effects of the unknown batch subspace while preserving the linear effects attributable to the known covariates $C$. Define orthogonal projection matrices onto sample-space column spans by\n$$\nP_Z \\;=\\; Z Z^+ \\in \\mathbb{R}^{n \\times n}, \\qquad M_Z \\;=\\; I_n - P_Z,\n$$\nfor any $Z \\in \\mathbb{R}^{n \\times r}$, where $Z^+$ is the Moore–Penrose pseudoinverse and $I_n$ is the $n \\times n$ identity matrix. For the joint column space of two matrices $Z_1 \\in \\mathbb{R}^{n \\times r_1}$ and $Z_2 \\in \\mathbb{R}^{n \\times r_2}$, concatenate them as $Q = [Z_1 \\; Z_2] \\in \\mathbb{R}^{n \\times (r_1 + r_2)}$ and use $P_Q = Q Q^+$, $M_Q = I_n - P_Q$.\n\nYour program must, for each test case below, perform the following steps purely in linear algebraic terms:\n1. Given $X$ and $C$, choose an estimator $\\widehat{H} \\in \\mathbb{R}^{n \\times k_{\\text{est}}}$ of a $k_{\\text{est}}$-dimensional latent sample subspace that maximizes the captured variance in the residual matrix after removing the projection onto $C$. Formally, let $R = X M_C \\in \\mathbb{R}^{p \\times n}$. Among all $n \\times k_{\\text{est}}$ matrices with orthonormal columns,\n$$\n\\widehat{H} \\;=\\; \\arg\\max_{H' \\in \\mathbb{R}^{n \\times k_{\\text{est}}},\\, (H')^\\top H' = I_{k_{\\text{est}}}} \\; \\lVert R P_{H'} \\rVert_F^2,\n$$\nwhere $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n2. Form the corrected data matrix\n$$\nX_{\\text{corr}} \\;=\\; X P_C \\;+\\; X M_Q,\n$$\nwhere $Q = [\\, C \\;\\; \\widehat{H} \\,] \\in \\mathbb{R}^{n \\times (q + k_{\\text{est}})}$. This removes linear effects attributable to the estimated latent batch subspace while retaining the effects attributable to $C$.\n3. For evaluation, let $H_{\\text{true}} \\in \\mathbb{R}^{n \\times k_{\\text{true}}}$ denote the true batch design used to generate $X$ in each test case (it may have zero columns when $k_{\\text{true}} = 0$). Compute the fraction of batch-associated variance remaining after correction as\n$$\nf \\;=\\; \\frac{\\lVert X_{\\text{corr}} \\, P_{H_{\\text{true}}} \\rVert_F^2}{\\lVert X_{\\text{corr}} \\rVert_F^2}.\n$$\nWhen $k_{\\text{true}} = 0$, interpret $P_{H_{\\text{true}}}$ as the $n \\times n$ zero matrix, so $f = 0$.\n\nConstruction of the synthetic test data is deterministic and uses the same procedure in every test case. All matrices are generated by a fixed pseudorandom pipeline that maps an integer seed to independent standard normal variables via a linear congruential generator (LCG) and the Box–Muller transform:\n- Uniform generator: starting from an integer state $x_0 = s$ (the given seed), define\n$$\nx_{t+1} \\;=\\; (a x_t + c) \\bmod m, \\quad u_{t+1} \\;=\\; \\frac{x_{t+1}}{m},\n$$\nwith constants $m = 2^{32}$, $a = 1664525$, $c = 1013904223$, producing $u_{t} \\in [0,1)$.\n- Standard normal generator: transform successive pairs $(u_1, u_2)$, with $u_1 \\in (0,1)$ and $u_2 \\in [0,1)$, into\n$$\nz_1 \\;=\\; \\sqrt{-2 \\ln u_1}\\, \\cos(2\\pi u_2), \\qquad z_2 \\;=\\; \\sqrt{-2 \\ln u_1}\\, \\sin(2\\pi u_2).\n$$\nIf $u_1 = 0$, replace it by $u_1 = \\frac{1}{m}$ before applying the transform. To generate a matrix of shape $r \\times c$ with entries distributed as $\\mathcal{N}(0,\\sigma^2)$, draw $r \\cdot c$ such $z$ values, arrange them in row-major order, and multiply by $\\sigma$.\n\nData generation for each test case uses the following steps and parameters:\n- Given integers $p, n, q, k_{\\text{true}}$, and nonnegative scalars $\\sigma_B, \\sigma_A, \\sigma_E$:\n  - Generate $B \\in \\mathbb{R}^{p \\times q}$ from seed $s_B$ with standard deviation $\\sigma_B$ (if $q = 0$, $B$ is the empty matrix).\n  - Generate $A \\in \\mathbb{R}^{p \\times k_{\\text{true}}}$ from seed $s_A$ with standard deviation $\\sigma_A$ (if $k_{\\text{true}} = 0$, $A$ is the empty matrix).\n  - Generate $H_{\\text{true}} \\in \\mathbb{R}^{n \\times k_{\\text{true}}}$ from seed $s_H$ with standard deviation $1$ (ignored if $k_{\\text{true}} = 0$).\n  - Generate $E \\in \\mathbb{R}^{p \\times n}$ from seed $s_E$ with standard deviation $\\sigma_E$.\n  - Define $C \\in \\mathbb{R}^{n \\times q}$ deterministically per test case.\n  - Form $X = B C^\\top + A H_{\\text{true}}^\\top + E$.\n- Use the prescribed $k_{\\text{est}}$ in the estimator step above. If $q = 0$ or $k_{\\text{est}} = 0$, interpret the corresponding concatenations and projections with empty matrices in the natural way, with $P_Z$ equal to the zero matrix whenever $Z$ has zero columns.\n\nTest suite. For each case, the program must generate the data exactly as specified and compute the scalar $f$ defined above:\n- Case 1 (general case):\n  - Dimensions: $p = 60$, $n = 40$, $q = 1$, $k_{\\text{true}} = 2$, $k_{\\text{est}} = 2$.\n  - Seeds: $s_B = 101$, $s_A = 202$, $s_H = 303$, $s_E = 404$.\n  - Scales: $\\sigma_B = 1.0$, $\\sigma_A = 1.5$, $\\sigma_E = 0.3$.\n  - Covariate: $C \\in \\mathbb{R}^{40 \\times 1}$ with entries $C_{j,1} = 0$ for $1 \\le j \\le 20$ and $C_{j,1} = 1$ for $21 \\le j \\le 40$.\n- Case 2 (no batch; overestimated latent dimension):\n  - Dimensions: $p = 50$, $n = 30$, $q = 1$, $k_{\\text{true}} = 0$, $k_{\\text{est}} = 2$.\n  - Seeds: $s_B = 505$, $s_E = 606$.\n  - Scales: $\\sigma_B = 0.8$, $\\sigma_E = 0.4$.\n  - Covariate: $C \\in \\mathbb{R}^{30 \\times 1}$ with entries $C_{j,1} = 0$ for $1 \\le j \\le 15$ and $C_{j,1} = 1$ for $16 \\le j \\le 30$.\n- Case 3 (rank-deficient covariate; strong batch):\n  - Dimensions: $p = 80$, $n = 50$, $q = 2$, $k_{\\text{true}} = 1$, $k_{\\text{est}} = 1$.\n  - Seeds: $s_B = 707$, $s_A = 808$, $s_H = 909$, $s_E = 100$.\n  - Scales: $\\sigma_B = 0.5$, $\\sigma_A = 2.0$, $\\sigma_E = 0.2$.\n  - Covariates: $C \\in \\mathbb{R}^{50 \\times 2}$ with $C_{j,1} = \\frac{j}{50}$ and $C_{j,2} = 2 \\, C_{j,1}$ for $1 \\le j \\le 50$.\n- Case 4 (no known covariates; only batch and noise):\n  - Dimensions: $p = 40$, $n = 25$, $q = 0$, $k_{\\text{true}} = 2$, $k_{\\text{est}} = 2$.\n  - Seeds: $s_A = 111$, $s_H = 222$, $s_E = 333$.\n  - Scales: $\\sigma_A = 1.2$, $\\sigma_E = 0.5$.\n  - Covariate: $C$ is the empty $25 \\times 0$ matrix.\n\nYour program should produce a single line of output containing the four scalar results $f$ for Cases $1$ through $4$ as a comma-separated list enclosed in square brackets, in the order of the cases. For example, the output format must be exactly like $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the computed float for Case $i$ with no additional whitespace or text. No physical units are involved in this problem, and any angles introduced by trigonometric functions must be interpreted in radians.", "solution": "The problem statement has been validated and is deemed to be scientifically sound, well-posed, and computationally tractable. It describes a standard procedure for batch effect correction in high-dimensional data, grounded in the principles of linear models and matrix decomposition. We will proceed with a rigorous, step-by-step derivation and implementation of the required algorithm.\n\nThe model for the observed data matrix $X \\in \\mathbb{R}^{p \\times n}$ is given as a linear combination of known covariate effects, unknown batch effects, and noise:\n$$ X \\;=\\; B C^\\top \\;+\\; A H^\\top \\;+\\; E $$\nHere, $C \\in \\mathbb{R}^{n \\times q}$ is the known design matrix of covariates, and $H \\in \\mathbb{R}^{n \\times k_{\\text{true}}}$ is the unknown batch design matrix. Our objective is to estimate the column space of $H$, remove its influence from $X$, while preserving the influence of the column space of $C$.\n\n**Step 1: Estimation of the Latent Batch Subspace**\n\nThe first step is to construct an estimator $\\widehat{H} \\in \\mathbb{R}^{n \\times k_{\\text{est}}}$ for the latent subspace. The strategy is to first remove the linear effects of the known covariates $C$ and then identify the principal directions of variation in the remaining data.\n\nThe residual matrix after removing the projection onto the column space of $C$ is $R = X M_C$, where $M_C = I_n - P_C$ and $P_C = C C^+$ is the orthogonal projection matrix onto the column space of $C$. Here, $C^+$ denotes the Moore-Penrose pseudoinverse of $C$, which correctly handles cases where $C$ is not of full column rank or has zero columns (i.e., $q=0$).\n\nThe estimator $\\widehat{H}$ is defined as the matrix with $k_{\\text{est}}$ orthonormal columns that maximizes the captured variance in $R$:\n$$ \\widehat{H} \\;=\\; \\arg\\max_{H' \\in \\mathbb{R}^{n \\times k_{\\text{est}}},\\, (H')^\\top H' = I_{k_{\\text{est}}}} \\; \\lVert R P_{H'} \\rVert_F^2 $$\nSince the columns of $H'$ are orthonormal, its pseudoinverse is its transpose, so $P_{H'} = H' (H')^+ = H' (H')^\\top$. The objective function can be rewritten using the property $\\lVert A \\rVert_F^2 = \\text{tr}(A^\\top A)$:\n$$ \\lVert R H' (H')^\\top \\rVert_F^2 \\;=\\; \\text{tr}\\left(\\left(R H' (H')^\\top\\right)^\\top \\left(R H' (H')^\\top\\right)\\right) \\;=\\; \\text{tr}\\left(H' (H')^\\top R^\\top R H' (H')^\\top\\right) $$\nUsing the cyclic property of the trace and the constraint $(H')^\\top H' = I_{k_{\\text{est}}}$, this simplifies to:\n$$ \\text{tr}\\left((H')^\\top R^\\top R H'\\right) $$\nThis is a classical trace maximization problem from linear algebra. The solution is that the columns of the optimal matrix, $\\widehat{H}$, must be the orthonormal eigenvectors corresponding to the $k_{\\text{est}}$ largest eigenvalues of the symmetric, positive semi-definite matrix $S_R = R^\\top R \\in \\mathbb{R}^{n \\times n}$. This procedure is equivalent to performing Principal Component Analysis (PCA) on the sample dimension of the residual data.\n\nThe computational procedure is thus:\n$1$. Compute $P_C = C C^+$ via an SVD-based pseudoinverse. If $q=0$, $C$ is an $n \\times 0$ matrix, and $P_C$ correctly evaluates to the $n \\times n$ zero matrix.\n$2$. Compute the residual matrix $R = X (I_n - P_C)$.\n$3$. Form the sample Gram matrix $S_R = R^\\top R$.\n$4$. Perform an eigendecomposition of the symmetric matrix $S_R$ to find its eigenvalues and eigenvectors.\n$5$. Construct $\\widehat{H}$ from the $k_{\\text{est}}$ eigenvectors corresponding to the largest $k_{\\text{est}}$ eigenvalues. If $k_{\\text{est}}=0$, $\\widehat{H}$ is an empty $n \\times 0$ matrix.\n\n**Step 2: Construction of the Corrected Data Matrix**\n\nThe corrected data matrix, $X_{\\text{corr}}$, is formed to retain the signal associated with $C$ while removing the estimated batch effects. The formula is:\n$$ X_{\\text{corr}} \\;=\\; X P_C \\;+\\; X M_Q $$\nwhere $Q = [\\, C \\;\\; \\widehat{H} \\,]$ is the concatenation of the covariate and estimated batch factor matrices, and $M_Q = I_n - P_Q$ is the residual-forming matrix for the combined subspace.\n\nThis formula has a clear interpretation. The term $X P_C$ is the projection of the original data onto the covariate subspace, representing the part of the data we wish to preserve. The term $X M_Q$ is the projection of the original data onto the subspace orthogonal to the combined space of covariates and estimated batch effects.\n\nTo verify that the signal attributable to $C$ is preserved, we project $X_{\\text{corr}}$ back onto the covariate subspace:\n$$ X_{\\text{corr}} P_C = (X P_C + X M_Q) P_C = X P_C P_C + X M_Q P_C $$\nSince $P_C$ is an idempotent projection matrix, $P_C P_C = P_C$. The column space of $C$ is a subspace of the column space of $Q$, so any vector in the column space of $C$ is an eigenvector of $P_Q$ with eigenvalue $1$. This implies that the projection $P_Q$ acts as the identity on the column space of $C$, so $P_Q P_C = P_C$. Therefore, the product $M_Q P_C = (I_n - P_Q) P_C = P_C - P_Q P_C = P_C - P_C$ is the zero matrix.\nIt follows that $X_{\\text{corr}} P_C = X P_C$. The projection of the corrected data onto the covariate subspace is identical to that of the original data, confirming that the signal of interest is preserved by the transformation.\n\n**Step 3: Evaluation of Batch Effect Removal**\n\nThe effectiveness of the correction is quantified by the fraction of batch-associated variance remaining in the corrected data:\n$$ f \\;=\\; \\frac{\\lVert X_{\\text{corr}} \\, P_{H_{\\text{true}}} \\rVert_F^2}{\\lVert X_{\\text{corr}} \\rVert_F^2} $$\nHere, $H_{\\text{true}} \\in \\mathbb{R}^{n \\times k_{\\text{true}}}$ is the ground-truth batch design matrix. The numerator measures the energy (squared Frobenius norm) of the corrected data projected onto the true batch subspace. The denominator is the total energy of the corrected data. A value of $f$ close to $0$ indicates successful removal of batch effects. If $k_{\\text{true}}=0$, there are no batch effects to begin with; in this case $P_{H_{\\text{true}}}$ is the zero matrix, which correctly yields $f=0$. The computation requires calculating the projection matrix $P_{H_{\\text{true}}}$ and then the squared Frobenius norms. If the denominator is zero, which implies $X_{\\text{corr}}$ is a zero matrix, the fraction is taken to be $0$, as the numerator must also be zero.\n\nThe implementation will follow these steps, using a deterministic pseudorandom number generator as specified to synthesize the test data for each case. Robust numerical linear algebra routines from the `numpy.linalg` library will be used for pseudoinverses and eigendecompositions.", "answer": "```python\nimport numpy as np\n\nclass LCG:\n    \"\"\"A Linear Congruential Generator as specified in the problem.\"\"\"\n    def __init__(self, seed):\n        self.m = 2**32\n        self.a = 1664525\n        self.c = 1013904223\n        self.state = seed\n\n    def next_int(self):\n        \"\"\"Generate the next integer state.\"\"\"\n        self.state = (self.a * self.state + self.c) % self.m\n        return self.state\n\n    def next_uniform(self):\n        \"\"\"Generate a uniform random number in [0, 1).\"\"\"\n        return self.next_int() / self.m\n\ndef generate_normals_from_seed(seed, size, prng_class):\n    \"\"\"\n    Generates an array of standard normal random numbers from a seed\n    using the specified LCG and Box-Muller transform.\n    \"\"\"\n    rng = prng_class(seed)\n    normals = []\n    \n    # Pre-calculate constants to avoid recomputation in loop\n    pi_2 = 2.0 * np.pi\n    u1_min_val = 1.0 / rng.m\n\n    while len(normals)  size:\n        u1 = rng.next_uniform()\n        u2 = rng.next_uniform()\n\n        if u1 == 0.0:\n            u1 = u1_min_val  # Handle u1=0 as per problem description\n        \n        magnitude = np.sqrt(-2.0 * np.log(u1))\n        z1 = magnitude * np.cos(pi_2 * u2)\n        z2 = magnitude * np.sin(pi_2 * u2)\n        \n        normals.append(z1)\n        if len(normals)  size:\n            normals.append(z2)\n            \n    return np.array(normals[:size])\n\ndef generate_matrix(seed, p, n, sigma, prng_class):\n    \"\"\"Generates a matrix with entries from N(0, sigma^2).\"\"\"\n    if p == 0 or n == 0:\n        return np.empty((p, n))\n    num_elements = p * n\n    normals = generate_normals_from_seed(seed, num_elements, prng_class)\n    return normals.reshape((p, n)) * sigma\n\ndef projection_matrix(Z):\n    \"\"\"Computes the orthogonal projection matrix onto the column space of Z.\"\"\"\n    n, k = Z.shape\n    if k == 0:\n        return np.zeros((n, n))\n    # Use SVD-based Moore-Penrose pseudoinverse for numerical stability\n    Z_pinv = np.linalg.pinv(Z)\n    return Z @ Z_pinv\n\ndef solve():\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"p\": 60, \"n\": 40, \"q\": 1, \"k_true\": 2, \"k_est\": 2,\n            \"s_B\": 101, \"s_A\": 202, \"s_H\": 303, \"s_E\": 404,\n            \"sigma_B\": 1.0, \"sigma_A\": 1.5, \"sigma_E\": 0.3,\n            \"C_fn\": lambda n, q: np.vstack([np.zeros((n // 2, q)), np.ones((n - n // 2, q))])\n        },\n        # Case 2 (no batch; overestimated latent dimension)\n        {\n            \"p\": 50, \"n\": 30, \"q\": 1, \"k_true\": 0, \"k_est\": 2,\n            \"s_B\": 505, \"s_A\": None, \"s_H\": None, \"s_E\": 606,\n            \"sigma_B\": 0.8, \"sigma_A\": 0.0, \"sigma_E\": 0.4,\n            \"C_fn\": lambda n, q: np.vstack([np.zeros((n // 2, q)), np.ones((n - n // 2, q))])\n        },\n        # Case 3 (rank-deficient covariate; strong batch)\n        {\n            \"p\": 80, \"n\": 50, \"q\": 2, \"k_true\": 1, \"k_est\": 1,\n            \"s_B\": 707, \"s_A\": 808, \"s_H\": 909, \"s_E\": 100,\n            \"sigma_B\": 0.5, \"sigma_A\": 2.0, \"sigma_E\": 0.2,\n            \"C_fn\": lambda n, q: np.column_stack([ (np.arange(1, n + 1) / n), 2 * (np.arange(1, n + 1) / n) ])\n        },\n        # Case 4 (no known covariates; only batch and noise)\n        {\n            \"p\": 40, \"n\": 25, \"q\": 0, \"k_true\": 2, \"k_est\": 2,\n            \"s_B\": None, \"s_A\": 111, \"s_H\": 222, \"s_E\": 333,\n            \"sigma_B\": 0.0, \"sigma_A\": 1.2, \"sigma_E\": 0.5,\n            \"C_fn\": lambda n, q: np.empty((n, q))\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p, n, q, k_true, k_est = case[\"p\"], case[\"n\"], case[\"q\"], case[\"k_true\"], case[\"k_est\"]\n        \n        # 1. Generate data\n        C = case[\"C_fn\"](n, q)\n        B = generate_matrix(case[\"s_B\"], p, q, case[\"sigma_B\"], LCG)\n        A = generate_matrix(case[\"s_A\"], p, k_true, case[\"sigma_A\"], LCG)\n        H_true = generate_matrix(case[\"s_H\"], n, k_true, 1.0, LCG)\n        E = generate_matrix(case[\"s_E\"], p, n, case[\"sigma_E\"], LCG)\n        \n        X = B @ C.T + A @ H_true.T + E\n        \n        # 2. Estimate latent batch subspace H_hat\n        I_n = np.identity(n)\n        P_C = projection_matrix(C)\n        M_C = I_n - P_C\n        R = X @ M_C\n        \n        if k_est > 0:\n            S_R = R.T @ R\n            eigvals, eigvecs = np.linalg.eigh(S_R)\n            top_k_indices = np.argsort(eigvals)[::-1][:k_est]\n            H_hat = eigvecs[:, top_k_indices]\n        else:\n            H_hat = np.empty((n, 0))\n\n        # 3. Form corrected data matrix X_corr\n        Q = np.hstack([C, H_hat])\n        P_Q = projection_matrix(Q)\n        M_Q = I_n - P_Q\n        \n        X_corr = X @ P_C + X @ M_Q\n\n        # 4. Evaluate batch variance fraction f\n        if k_true > 0:\n            P_H_true = projection_matrix(H_true)\n            numerator = np.linalg.norm(X_corr @ P_H_true)**2\n        else: # If k_true is 0, P_H_true is zero matrix, numerator is 0.\n            numerator = 0.0\n\n        denominator = np.linalg.norm(X_corr)**2\n\n        if denominator == 0.0:\n            f = 0.0 # If X_corr is zero, numerator must also be zero.\n        else:\n            f = numerator / denominator\n        \n        results.append(f)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2374389"}]}