{"hands_on_practices": [{"introduction": "Before we delve into sophisticated computational pipelines for large-scale data, it's essential to grasp the fundamental goal of matrix balancing from first principles. This exercise challenges you to derive an exact, analytical solution for a small $3 \\times 3$ matrix, bypassing iterative methods entirely [@problem_id:2397239]. By solving this system of non-linear equations directly, you will build a concrete intuition for what the balancing process achieves and how scaling factors are determined to equalize row sums.", "problem": "A High-throughput Chromosome Conformation Capture (Hi-C) contact map is a symmetric matrix whose entries represent interaction counts between genomic loci. A common normalization goal is to balance a contact matrix by diagonal rescaling so that all rows (and thus columns, by symmetry) of the rescaled matrix have the same sum. Consider the following symmetric Hi-C contact submatrix of size $3 \\times 3$:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nLet $D = \\mathrm{diag}(x_1, x_2, x_3)$ with $x_1, x_2, x_3 \\in \\mathbb{R}_{>0}$, and define the rescaled matrix $B = D A D$. Determine the unique positive diagonal scaling factors $x_1, x_2, x_3$ such that every row sum of $B$ equals $1$, that is,\n$$\n\\sum_{j=1}^{3} B_{ij} \\;=\\; 1 \\quad \\text{for each } i \\in \\{1,2,3\\}.\n$$\nProvide your final answer as a single row matrix $\\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}$ in exact form (no approximations, no rounding).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It describes a standard matrix balancing procedure used in computational biology for the analysis of Hi-C data. The provided matrix is a valid, symmetric, non-negative, and irreducible matrix, for which a unique positive diagonal scaling solution is known to exist. Therefore, the problem is valid and we may proceed with the solution.\n\nThe problem requires finding a diagonal matrix $D = \\mathrm{diag}(x_1, x_2, x_3)$ with $x_1, x_2, x_3 > 0$ such that the rescaled matrix $B = DAD$ has all its row sums equal to $1$. The given matrix is\n$$\nA =\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\nThe rescaled matrix $B$ is calculated as $B = DAD$. Its elements $B_{ij}$ are given by the formula $B_{ij} = x_i A_{ij} x_j$.\n$$\nB =\n\\begin{pmatrix}\nx_1 & 0 & 0 \\\\\n0 & x_2 & 0 \\\\\n0 & 0 & x_3\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 & 0 & 0 \\\\\n0 & x_2 & 0 \\\\\n0 & 0 & x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\cdot x_1 x_1 & 1 \\cdot x_1 x_2 & 0 \\cdot x_1 x_3 \\\\\n1 \\cdot x_2 x_1 & 2 \\cdot x_2 x_2 & 1 \\cdot x_2 x_3 \\\\\n0 \\cdot x_3 x_1 & 1 \\cdot x_3 x_2 & 1 \\cdot x_3 x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_1^2 & x_1 x_2 & 0 \\\\\nx_1 x_2 & 2x_2^2 & x_2 x_3 \\\\\n0 & x_2 x_3 & x_3^2\n\\end{pmatrix}\n$$\nThe condition that all row sums of $B$ equal $1$, i.e., $\\sum_{j=1}^{3} B_{ij} = 1$ for $i \\in \\{1, 2, 3\\}$, produces the following system of non-linear equations:\n$$\n\\begin{cases}\n    x_1^2 + x_1 x_2 = 1 & (1) \\\\\n    x_1 x_2 + 2x_2^2 + x_2 x_3 = 1 & (2) \\\\\n    x_2 x_3 + x_3^2 = 1 & (3)\n\\end{cases}\n$$\nWe must solve this system for positive real numbers $x_1, x_2, x_3$.\nFrom equation $(1)$, we can factor $x_1$:\n$$\nx_1(x_1 + x_2) = 1\n$$\nSince $x_1 > 0$, we can express $x_1 + x_2 = \\frac{1}{x_1}$, which leads to $x_2 = \\frac{1}{x_1} - x_1 = \\frac{1-x_1^2}{x_1}$.\nFor $x_2$ to be positive, we must have $1-x_1^2 > 0$, which implies $0 < x_1 < 1$.\n\nSimilarly, from equation $(3)$, we factor $x_3$:\n$$\nx_3(x_2 + x_3) = 1\n$$\nSince $x_3 > 0$, we have $x_2 + x_3 = \\frac{1}{x_3}$, which leads to $x_2 = \\frac{1}{x_3} - x_3 = \\frac{1-x_3^2}{x_3}$.\nFor $x_2$ to be positive, we must have $1-x_3^2 > 0$, which implies $0 < x_3 < 1$.\n\nBy equating the two expressions for $x_2$, we obtain a key relationship between $x_1$ and $x_3$:\n$$\n\\frac{1}{x_1} - x_1 = \\frac{1}{x_3} - x_3\n$$\n$$\n\\frac{1}{x_1} - \\frac{1}{x_3} = x_1 - x_3\n$$\n$$\n\\frac{x_3 - x_1}{x_1 x_3} = -(x_3 - x_1)\n$$\nThis equation has two possible solutions.\nCase 1: $x_3 - x_1 = 0$, which means $x_1 = x_3$.\nCase 2: If $x_3 - x_1 \\neq 0$, we can divide both sides by $(x_3 - x_1)$, yielding $\\frac{1}{x_1 x_3} = -1$, or $x_1 x_3 = -1$. This is impossible, as the problem specifies that $x_1$ and $x_3$ must be positive real numbers.\nThus, the only valid possibility is $x_1 = x_3$.\n\nWe now substitute $x_3 = x_1$ into the original system. Equation $(3)$ becomes identical to equation $(1)$. Equation $(2)$ simplifies to:\n$$\nx_1 x_2 + 2x_2^2 + x_2 x_1 = 1\n$$\n$$\n2x_1 x_2 + 2x_2^2 = 1\n$$\n$$\n2x_2(x_1 + x_2) = 1\n$$\nWe now have a reduced system of two equations:\n$$\n\\begin{cases}\n    x_1(x_1 + x_2) = 1 & (A) \\\\\n    2x_2(x_1 + x_2) = 1 & (B)\n\\end{cases}\n$$\nSince the right-hand sides are equal to $1$, we can equate the left-hand sides:\n$$\nx_1(x_1 + x_2) = 2x_2(x_1 + x_2)\n$$\nSince $x_1 > 0$ and $x_2 > 0$, the term $(x_1 + x_2)$ is strictly positive. We can divide by it without loss of generality:\n$$\nx_1 = 2x_2\n$$\nNow substitute this relation into equation $(A)$:\n$$\n(2x_2)(2x_2 + x_2) = 1\n$$\n$$\n(2x_2)(3x_2) = 1\n$$\n$$\n6x_2^2 = 1\n$$\n$$\nx_2^2 = \\frac{1}{6}\n$$\nSince $x_2 > 0$, we take the positive square root:\n$$\nx_2 = \\sqrt{\\frac{1}{6}} = \\frac{1}{\\sqrt{6}} = \\frac{\\sqrt{6}}{6}\n$$\nUsing the relationships we derived, we find $x_1$ and $x_3$:\n$$\nx_1 = 2x_2 = 2 \\left( \\frac{\\sqrt{6}}{6} \\right) = \\frac{2\\sqrt{6}}{6} = \\frac{\\sqrt{6}}{3}\n$$\nAnd since $x_3 = x_1$:\n$$\nx_3 = \\frac{\\sqrt{6}}{3}\n$$\nThe unique positive diagonal scaling factors are therefore $x_1 = \\frac{\\sqrt{6}}{3}$, $x_2 = \\frac{\\sqrt{6}}{6}$, and $x_3 = \\frac{\\sqrt{6}}{3}$. These values satisfy the constraints and the original system of equations. The final answer must be presented as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{6}}{3} & \\frac{\\sqrt{6}}{6} & \\frac{\\sqrt{6}}{3}\n\\end{pmatrix}\n}\n$$", "id": "2397239"}, {"introduction": "In practice, Hi-C contact maps are far too large for analytical solutions, making iterative algorithms essential. This exercise guides you through implementing a standard iterative balancing procedure to normalize a contact matrix [@problem_id:2397206]. More importantly, it prompts an investigation into a critical aspect of real-world data analysis: the sensitivity of normalization results to the initial filtering of low-quality data, teaching you to think critically about the robustness of your computational methods.", "problem": "You are given the task of assessing the sensitivity of matrix balancing to the initial choice of excluded rows and columns in the context of High-throughput Chromosome conformation capture (Hi-C) contact maps. Let $A \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ be a symmetric matrix of nonnegative entries representing contact counts between $n$ genomic bins. For an initial exclusion set $B \\subseteq \\{1,2,\\ldots,n\\}$, define a scaling vector $x \\in \\mathbb{R}_{\\ge 0}^{n}$ such that $x_i = 0$ for all $i \\in B$ and $x_i > 0$ for all $i \\notin B$, and let the balanced matrix be $N = \\mathrm{diag}(x) \\, A \\, \\mathrm{diag}(x)$. The balancing condition requires that for every $i \\notin B$,\n$$\n\\sum_{j \\notin B} N_{ij} = \\sum_{j \\notin B} x_i \\, A_{ij} \\, x_j = 1.\n$$\nAssume that for the matrices provided below and for the specified exclusion sets, a unique such vector $x$ exists satisfying the above constraints.\n\nDefine the sensitivity between two initial exclusion sets $B^{(1)}$ and $B^{(2)}$ for the same matrix $A$ as the mean absolute difference between the corresponding scaling vectors on the intersection of retained indices. More precisely, let $x^{(1)}$ and $x^{(2)}$ be the scaling vectors obtained from $B^{(1)}$ and $B^{(2)}$, respectively, and let $S = \\{ i \\mid i \\notin B^{(1)}, \\, i \\notin B^{(2)} \\}$. The sensitivity is\n$$\n\\Delta(A,B^{(1)},B^{(2)}) \\;=\\; \\frac{1}{|S|} \\sum_{i \\in S} \\left| x^{(1)}_i - x^{(2)}_i \\right|.\n$$\nYour program must compute $\\Delta(A,B^{(1)},B^{(2)})$ for each test case in the suite below.\n\nThe test suite consists of two Hi-C-like symmetric contact matrices with near-diagonal enrichment and five pairs of exclusion sets. Indices are $1$-based. The matrices are:\n$$\nA^{(1)} \\;=\\; \\begin{bmatrix}\n10 & 6 & 3 & 1 & 0.5 \\\\\n6 & 9 & 5 & 2 & 1 \\\\\n3 & 5 & 8 & 4 & 2 \\\\\n1 & 2 & 4 & 7 & 3 \\\\\n0.5 & 1 & 2 & 3 & 6\n\\end{bmatrix},\n\\qquad\nA^{(2)} \\;=\\; \\begin{bmatrix}\n20 & 10 & 5 & 2 & 1 \\\\\n10 & 18 & 7 & 3 & 1.5 \\\\\n5 & 7 & 15 & 6 & 2 \\\\\n2 & 3 & 6 & 12 & 4 \\\\\n1 & 1.5 & 2 & 4 & 10\n\\end{bmatrix}.\n$$\nCompute $\\Delta(A,B^{(1)},B^{(2)})$ for the following five cases:\n- Case $1$: $A = A^{(1)}$, $B^{(1)} = \\varnothing$, $B^{(2)} = \\varnothing$.\n- Case $2$: $A = A^{(1)}$, $B^{(1)} = \\varnothing$, $B^{(2)} = \\{5\\}$.\n- Case $3$: $A = A^{(1)}$, $B^{(1)} = \\{1\\}$, $B^{(2)} = \\{5\\}$.\n- Case $4$: $A = A^{(2)}$, $B^{(1)} = \\varnothing$, $B^{(2)} = \\{3\\}$.\n- Case $5$: $A = A^{(2)}$, $B^{(1)} = \\{2,4\\}$, $B^{(2)} = \\{2,5\\}$.\n\nYour program must produce a single line of output containing the five results as a comma-separated list enclosed in square brackets, in the order of the cases above, with each value rounded to $6$ decimal places, i.e., $[r_1,r_2,r_3,r_4,r_5]$ where each $r_k$ is $\\Delta(A,B^{(1)},B^{(2)})$ for case $k$. No physical units are involved.", "solution": "The problem presented is a well-defined task in computational biology, specifically concerning the normalization of Hi-C contact maps. Before proceeding to a solution, a validation of the problem statement is required.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n- An input symmetric matrix of non-negative entries $A \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$.\n- An initial exclusion set of $1$-based indices $B \\subseteq \\{1, 2, \\ldots, n\\}$.\n- A scaling vector $x \\in \\mathbb{R}_{\\ge 0}^{n}$, where $x_i = 0$ for $i \\in B$ and $x_i > 0$ for $i \\notin B$.\n- A balanced matrix $N = \\mathrm{diag}(x) \\, A \\, \\mathrm{diag}(x)$.\n- A balancing condition for every $i \\notin B$: $\\sum_{j \\notin B} N_{ij} = \\sum_{j \\notin B} x_i \\, A_{ij} \\, x_j = 1$.\n- An assumption that a unique scaling vector $x$ exists for the given matrices and exclusion sets.\n- A sensitivity measure between two exclusion sets $B^{(1)}$ and $B^{(2)}$ for a matrix $A$: $\\Delta(A,B^{(1)},B^{(2)}) = \\frac{1}{|S|} \\sum_{i \\in S} | x^{(1)}_i - x^{(2)}_i |$, where $x^{(1)}$ and $x^{(2)}$ are the scaling vectors for $B^{(1)}$ and $B^{(2)}$, and $S = \\{ i \\mid i \\notin B^{(1)}, \\, i \\notin B^{(2)} \\}$.\n- Two specific matrices, $A^{(1)}$ and $A^{(2)}$, are provided.\n- Five test cases specifying pairs of exclusion sets for these matrices are given.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientifically Grounded**: The problem describes matrix balancing, a standard and critical step in Hi-C data analysis for removing experimental biases. The specified balancing condition is a form of Knight-Ruiz (KR) balancing, which is a well-established algorithm in the field. The formulation is scientifically sound.\n- **Well-Posed**: The problem statement asserts the existence and uniqueness of the solution vector $x$. For the class of matrices relevant to Hi-C data (non-negative, symmetric, and typically irreducible), this property holds. The problem is therefore mathematically well-posed.\n- **Objective and Complete**: All terms are defined with mathematical precision. The input data and test cases are specified completely. The problem is objective and self-contained.\n\n**Step 3: Verdict and Action**\nThe problem statement is deemed valid. It is scientifically grounded, mathematically well-posed, and complete. A solution will now be derived and implemented.\n\n**Solution Derivation**\n\nThe core task is to determine the scaling vector $x$ for a given symmetric matrix $A$ and an exclusion set $B$. Let $I = \\{1, 2, \\ldots, n\\} \\setminus B$ denote the set of retained indices. The balancing condition, restricted to these indices, is\n$$\nx_i \\sum_{j \\in I} A_{ij} x_j = 1 \\quad \\forall i \\in I.\n$$\nThis constitutes a system of non-linear equations for the positive components of $x$. Let $y$ be the vector consisting of the elements $x_i$ for $i \\in I$, and let $A_I$ be the submatrix of $A$ formed by selecting the rows and columns with indices in $I$. The system can be written as:\n$$\ny_k \\left( \\sum_{l} (A_I)_{kl} y_l \\right) = 1\n$$\nfor each component $y_k$ of $y$. This structure suggests a fixed-point iteration approach. We can rearrange the equation to define a mapping $F(y)$:\n$$\ny_k = \\frac{1}{\\sum_{l} (A_I)_{kl} y_l} \\equiv F(y)_k.\n$$\nA solution $y$ is a fixed point of this mapping, i.e., $y = F(y)$. This leads to the following iterative algorithm:\n1. Initialize the scaling vector for retained indices, $y^{(0)}$, with all elements equal to $1$.\n2. For each iteration $k = 0, 1, 2, \\ldots$, compute the next approximation $y^{(k+1)}$ using the update rule:\n   $$\n   y^{(k+1)} = F(y^{(k)}).\n   $$\n   Component-wise, this is $y_i^{(k+1)} = 1 / \\sum_{j} (A_I)_{ij} y_j^{(k)}$.\n3. The iteration continues until the change between successive vectors is negligible, e.g., when the infinity norm of the difference, $\\|y^{(k+1)} - y^{(k)}\\|_\\infty$, falls below a prescribed tolerance $\\epsilon$. The problem's guarantee of a unique solution ensures that this process converges.\n\nThe overall procedure to compute the sensitivity $\\Delta(A, B^{(1)}, B^{(2)})$ is as follows:\n1. For the exclusion set $B^{(1)}$, apply the iterative algorithm to the matrix $A$ to find the corresponding scaling vector $x^{(1)}$. The components of $x^{(1)}$ are zero for indices in $B^{(1)}$.\n2. Repeat the process for the exclusion set $B^{(2)}$ to find the vector $x^{(2)}$.\n3. Identify the set of common retained indices $S = \\{ i \\mid i \\notin B^{(1)}, i \\notin B^{(2)} \\}$.\n4. Compute the sensitivity by calculating the mean of the absolute differences $|x^{(1)}_i - x^{(2)}_i|$ for all indices $i \\in S$.\n\nThis methodology is systematic and computationally efficient. It will be implemented to solve the provided test cases. The use of 1-based indexing in the problem statement requires careful conversion to 0-based indexing for implementation in programming environments like Python with NumPy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_scaling_vector(A, B_one_based, tol=1e-12, max_iter=2000):\n    \"\"\"\n    Computes the scaling vector x for a given matrix A and exclusion set B.\n    \n    The balancing condition is that for the submatrix of retained indices A_I,\n    diag(x_I) @ A_I @ x_I = 1 (vector of ones). This is solved iteratively.\n    \"\"\"\n    n = A.shape[0]\n    # Convert 1-based exclusion set to 0-based list of retained indices\n    I_zero_based = [i for i in range(n) if (i + 1) not in B_one_based]\n    \n    # If all indices are excluded, return a zero vector.\n    if not I_zero_based:\n        return np.zeros(n)\n        \n    # Extract the submatrix corresponding to retained indices.\n    A_sub = A[np.ix_(I_zero_based, I_zero_based)]\n    m = len(I_zero_based)\n    \n    # Initialize the scaling vector for the subproblem.\n    y = np.ones(m, dtype=float)\n    \n    # Iteratively solve for y using the fixed-point method.\n    for _ in range(max_iter):\n        y_prev = y\n        c = A_sub @ y\n        \n        # Guard against division by zero, though not expected for the given problem.\n        # The input matrices are positive, so for positive y, c will be positive.\n        if np.any(c == 0):\n           c[c == 0] = 1e-20\n\n        y = 1.0 / c\n        \n        # Check for convergence using the infinity norm.\n        if np.linalg.norm(y - y_prev, ord=np.inf) < tol:\n            break\n            \n    # Construct the full scaling vector x.\n    x = np.zeros(n, dtype=float)\n    x[I_zero_based] = y\n    \n    return x\n\ndef compute_delta(A, B1_one_based, B2_one_based):\n    \"\"\"\n    Computes the sensitivity between two exclusion sets B1 and B2 for matrix A.\n    \"\"\"\n    # Compute the scaling vectors for each exclusion set.\n    x1 = get_scaling_vector(A, B1_one_based)\n    x2 = get_scaling_vector(A, B2_one_based)\n    \n    n = A.shape[0]\n    U_one_based = set(range(1, n + 1))\n    \n    # Determine the set of common retained indices.\n    I1 = U_one_based - B1_one_based\n    I2 = U_one_based - B2_one_based\n    S_one_based = I1.intersection(I2)\n    \n    # If the intersection is empty, the mean difference is 0.\n    if not S_one_based:\n        return 0.0\n        \n    # Convert to 0-based indices for numpy array access.\n    S_zero_based = [s - 1 for s in S_one_based]\n    \n    # Compute the mean absolute difference over the common indices.\n    abs_diff = np.abs(x1[S_zero_based] - x2[S_zero_based])\n    delta = np.mean(abs_diff)\n    \n    return delta\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [10, 6, 3, 1, 0.5],\n        [6, 9, 5, 2, 1],\n        [3, 5, 8, 4, 2],\n        [1, 2, 4, 7, 3],\n        [0.5, 1, 2, 3, 6]\n    ], dtype=float)\n    \n    A2 = np.array([\n        [20, 10, 5, 2, 1],\n        [10, 18, 7, 3, 1.5],\n        [5, 7, 15, 6, 2],\n        [2, 3, 6, 12, 4],\n        [1, 1.5, 2, 4, 10]\n    ], dtype=float)\n\n    test_cases = [\n        (A1, set(), set()),             # Case 1\n        (A1, set(), {5}),               # Case 2\n        (A1, {1}, {5}),                 # Case 3\n        (A2, set(), {3}),               # Case 4\n        (A2, {2, 4}, {2, 5})            # Case 5\n    ]\n\n    results = []\n    for A, B1, B2 in test_cases:\n        result = compute_delta(A, B1, B2)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2397206"}, {"introduction": "The principles of identifying and correcting for systemic biases are not unique to genomics; they form a powerful part of the modern data scientist's toolkit. This advanced practice demonstrates the remarkable generality of Hi-C normalization by applying an analogous pipeline to a completely different domain: analyzing file co-access patterns on a computer system [@problem_id:2397168]. By modeling covariate biases with regression and then applying matrix balancing, you will see how these concepts can be adapted to uncover true underlying relationships in diverse types of network data.", "problem": "You are given a symmetric, nonnegative integer matrix that encodes the number of co-accesses between files on a computer system over a fixed observation window. Each diagonal entry is the number of self-access events for a file. You are also given, for each file, two covariates: file size and file age. The task is to construct a program that normalizes the co-access matrix to identify functional associations by removing covariate-driven biases, using principles analogous to High-throughput Chromosome Conformation Capture (Hi-C) contact map normalization in computational biology.\n\nAssume the following fundamental base:\n- Co-access events between files are independent and can be modeled by a Poisson process, so that observed counts arise from a Poisson distribution with an intensity that depends on latent association strength and per-node biases.\n- The per-node bias is multiplicative across file pairs and depends on file covariates through a log-linear model.\n- A balanced symmetric normalization, analogous to doubly stochastic balancing in matrix theory, removes remaining node-specific bias by enforcing equal row and column sums for nonzero rows and columns.\n\nLet the observed count matrix be denoted by $C \\in \\mathbb{N}_0^{n \\times n}$ with $C_{ij} = C_{ji} \\ge 0$, file sizes by $\\{s_i\\}_{i=1}^n$ with $s_i > 0$, and file ages by $\\{a_i\\}_{i=1}^n$ with $a_i > 0$. The latent model is that for each pair $(i,j)$, the expected intensity $E[C_{ij}]$ factors as\n$$\nE[C_{ij}] = \\theta_{ij} \\cdot b_i \\cdot b_j,\n$$\nwhere $\\theta_{ij}$ encodes the covariate-independent association propensity and $b_i$ is a positive bias factor for file $i$. The bias factor obeys a log-linear relationship\n$$\n\\log b_i = \\beta_0 + \\beta_1 \\log s_i + \\beta_2 a_i,\n$$\nwith unknown coefficients $\\beta_0, \\beta_1, \\beta_2 \\in \\mathbb{R}$.\n\nYour program must implement the following normalization pipeline derived from the assumptions above:\n1. Estimate the dependence of total co-access per file on the covariates by fitting a linear model to the logarithm of the row sums of $C$. For each file $i$, define the marginal $m_i = \\sum_{j=1}^n C_{ij}$. Use only indices with $m_i > 0$ when fitting the linear model of the form\n$$\n\\log m_i \\approx \\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i,\n$$\nby least squares. Use the fitted coefficients to compute a bias estimate $\\hat{b}_i = \\exp(\\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i)$ for all $i \\in \\{1,\\dots,n\\}$.\n2. Form a covariate-weighted matrix $W$ by multiplicative reweighting:\n$$\nW_{ij} = \\frac{C_{ij}}{\\hat{b}_i \\hat{b}_j}.\n$$\n3. Apply a symmetric iterative proportional fitting (balancing) procedure to find a vector $x \\in \\mathbb{R}_{\\ge 0}^n$ such that the balanced matrix\n$$\nB = \\operatorname{diag}(x) \\, W \\, \\operatorname{diag}(x)\n$$\nhas row sums equal to $1$ for all rows $i$ with $\\sum_{j=1}^n W_{ij} > 0$, and has row sum equal to $0$ for any row that is all zeros. Use the fixed-point iteration\n$$\nx_i^{(t+1)} = \\begin{cases}\n\\displaystyle \\frac{1}{\\sum_{j=1}^n W_{ij} x_j^{(t)}} & \\text{if } \\sum_{j=1}^n W_{ij} > 0,\\\\[1.25em]\n0 & \\text{otherwise},\n\\end{cases}\n$$\ninitialized with $x_i^{(0)} = 1$ if $\\sum_{j=1}^n W_{ij} > 0$ and $x_i^{(0)} = 0$ otherwise. Iterate until convergence where the maximum absolute change $\\max_i \\left| x_i^{(t+1)} - x_i^{(t)} \\right|$ is below a tolerance, or until a pre-specified maximum number of iterations is reached.\n\nImplementation requirements:\n- Implement the steps above precisely. You may choose a reasonable small tolerance (for example, $10^{-9}$) and a maximum number of iterations (for example, $10^4$) for the balancing step.\n- For numerical stability in the regression step, exclude files with $m_i = 0$ from the fit. Do not add arbitrary pseudocounts to $m_i$ for the regression. After fitting, compute $\\hat{b}_i$ for all $i$ using the fitted coefficients.\n- After computing $B$, compute the row sums $r_i = \\sum_{j=1}^n B_{ij}$.\n\nYour program must process the following test suite and output the list of row sums for each case:\n\nTest case A (happy path, heterogeneous counts and covariates):\n- Matrix\n$$\nC^{(A)} =\n\\begin{bmatrix}\n100 & 10 & 5 & 0\\\\\n10 & 80 & 20 & 5\\\\\n5 & 20 & 60 & 15\\\\\n0 & 5 & 15 & 40\n\\end{bmatrix}\n$$\n- Sizes\n$$\ns^{(A)} = \\begin{bmatrix} 1000 & 500 & 200 & 100 \\end{bmatrix}\n$$\n- Ages\n$$\na^{(A)} = \\begin{bmatrix} 5 & 2 & 1 & 0.5 \\end{bmatrix}\n$$\n\nTest case B (edge case with an isolated file having zero co-access):\n- Matrix\n$$\nC^{(B)} =\n\\begin{bmatrix}\n30 & 5 & 0 & 0 & 0\\\\\n5 & 25 & 5 & 0 & 0\\\\\n0 & 5 & 20 & 10 & 0\\\\\n0 & 0 & 10 & 15 & 0\\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Sizes\n$$\ns^{(B)} = \\begin{bmatrix} 300 & 200 & 150 & 100 & 50 \\end{bmatrix}\n$$\n- Ages\n$$\na^{(B)} = \\begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\end{bmatrix}\n$$\n\nTest case C (boundary case with uniform counts but heterogeneous covariates):\n- Matrix\n$$\nC^{(C)} =\n\\begin{bmatrix}\n10 & 10 & 10\\\\\n10 & 10 & 10\\\\\n10 & 10 & 10\n\\end{bmatrix}\n$$\n- Sizes\n$$\ns^{(C)} = \\begin{bmatrix} 10^6 & 10^3 & 10 \\end{bmatrix}\n$$\n- Ages\n$$\na^{(C)} = \\begin{bmatrix} 10 & 5 & 1 \\end{bmatrix}\n$$\n\nFinal output specification:\n- For each test case, compute the balanced matrix $B$ and then the vector of row sums $r = \\left(r_1, \\dots, r_n\\right)$.\n- Round each entry of $r$ to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of the three row-sum vectors, each enclosed in square brackets. For example, the output format must be\n$$\n\\big[ [r_1^{(A)}, r_2^{(A)}, r_3^{(A)}, r_4^{(A)}], [r_1^{(B)}, \\dots, r_5^{(B)}], [r_1^{(C)}, r_2^{(C)}, r_3^{(C)}] \\big],\n$$\nwith each $r_i^{(\\cdot)}$ displayed as a decimal number with exactly $6$ places after the decimal point. Do not print any other text.", "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a computational task analogous to Hi-C contact map normalization in bioinformatics, applied to a novel \"file co-access\" scenario. The underlying model, which assumes Poisson-distributed counts with multiplicative biases governed by a log-linear dependence on covariates, is a standard and sound approach in this domain. The prescribed normalization pipeline, involving linear regression to estimate biases followed by iterative matrix balancing, is a well-defined and algorithmically-specified procedure. The provision of specific test cases, parameters, and a required output format makes the problem statement complete and unambiguous.\n\nThe solution proceeds by implementing the three specified steps for each test case:\n\n**Step 1: Covariate Bias Estimation via Linear Regression**\n\nFor each file $i \\in \\{1, \\dots, n\\}$, we first compute its total co-access count, or marginal, $m_i = \\sum_{j=1}^n C_{ij}$. The model assumes that the bias $b_i$ for file $i$ is related to its size $s_i$ and age $a_i$ through the log-linear model $\\log b_i = \\beta_0 + \\beta_1 \\log s_i + \\beta_2 a_i$. The marginals are expected to reflect these biases. We estimate a related set of coefficients $(\\gamma_0, \\gamma_1, \\gamma_2)$ by fitting the linear model\n$$\n\\log m_i \\approx \\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i\n$$\nThis model is fitted using ordinary least squares, but only on the subset of files for which the marginal count is non-zero, i.e., $\\{i \\mid m_i > 0\\}$. This avoids taking the logarithm of zero and ensures the regression is performed on observable data. If we let $I = \\{i \\mid m_i > 0\\}$, we form a design matrix $X$ where each row is $(1, \\log s_i, a_i)$ for $i \\in I$, and a response vector $y$ with elements $\\log m_i$ for $i \\in I$. The coefficient vector $\\gamma = [\\gamma_0, \\gamma_1, \\gamma_2]^T$ is found by solving the linear system $X\\gamma=y$ in the least-squares sense using `numpy.linalg.lstsq`.\n\nOnce the coefficients $\\gamma$ are determined, the bias estimate $\\hat{b}_i$ for *every* file $i \\in \\{1, \\dots, n\\}$ is computed as:\n$$\n\\hat{b}_i = \\exp(\\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i)\n$$\n\n**Step 2: Covariate-Corrected Matrix Construction**\n\nThe original count matrix $C$ is then corrected for the estimated covariate-driven biases. A new matrix $W$ is formed by dividing each element $C_{ij}$ by the product of the corresponding bias estimates $\\hat{b}_i$ and $\\hat{b}_j$:\n$$\nW_{ij} = \\frac{C_{ij}}{\\hat{b}_i \\hat{b}_j}\n$$\nThis operation can be expressed in matrix form as $W = \\operatorname{diag}(\\hat{b}^{-1}) \\, C \\, \\operatorname{diag}(\\hat{b}^{-1})$, where $\\hat{b}^{-1}$ is the vector of element-wise reciprocals of $\\hat{b}$. This matrix $W$ represents the interaction propensities with covariate effects removed.\n\n**Step 3: Symmetric Iterative Balancing**\n\nThe final step addresses any remaining latent node-specific biases by applying an iterative balancing algorithm. The goal is to find a diagonal scaling matrix $\\operatorname{diag}(x)$ such that the resulting balanced matrix $B = \\operatorname{diag}(x) \\, W \\, \\operatorname{diag}(x)$ has row (and column) sums equal to $1$ for all rows/columns that are not identically zero. The problem specifies the following fixed-point iteration to find the scaling vector $x \\in \\mathbb{R}_{\\ge 0}^n$:\n$$\nx_i^{(t+1)} = \\begin{cases}\n\\displaystyle \\frac{1}{\\sum_{j=1}^n W_{ij} x_j^{(t)}} & \\text{if } \\sum_{j=1}^n W_{ij} > 0,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nThe iteration is initialized with $x_i^{(0)} = 1$ for all non-zero rows $i$ (where $\\sum_j W_{ij} > 0$) and $x_i^{(0)} = 0$ for zero rows. The process continues until the maximum absolute change in the vector $x$, $\\max_i |x_i^{(t+1)} - x_i^{(t)}|$, falls below a specified tolerance of $10^{-9}$, or a maximum of $10^4$ iterations is completed.\n\nFor Test Cases A and B, the matrix $W$ for the connected components is irreducible, and the iterative procedure is expected to converge, yielding a balanced matrix with row sums equal to $1$ for non-zero rows. For Test Case C, due to the uniform counts in matrix $C$, the regression correctly identifies that biases are independent of covariates, resulting in a rank-$1$ matrix $W$. The prescribed iterative scheme oscillates and does not converge to a solution where row sums are $1$. It instead terminates upon reaching the maximum iteration count ($10^4$). Following the instructions precisely, we use the value of $x$ at this final iteration.\n\nFinally, for each test case, the balanced matrix $B$ is computed, and its row sums $r_i = \\sum_{j=1}^n B_{ij}$ are calculated and reported to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the file co-access matrix normalization problem for the given test cases.\n    \"\"\"\n\n    def normalize_matrix(C, s, a, tol=1e-9, max_iter=10000):\n        \"\"\"\n        Implements the complete 3-step normalization pipeline.\n        \n        Args:\n            C (np.ndarray): The symmetric co-access count matrix.\n            s (np.ndarray): The vector of file sizes.\n            a (np.ndarray): The vector of file ages.\n            tol (float): Convergence tolerance for the balancing step.\n            max_iter (int): Maximum number of iterations for the balancing step.\n\n        Returns:\n            np.ndarray: The vector of row sums of the final balanced matrix B.\n        \"\"\"\n        n = C.shape[0]\n\n        # Step 1: Estimate covariate dependence via linear regression\n        m = C.sum(axis=1)\n        valid_indices = np.where(m > 0)[0]\n        \n        # We need at least as many data points as parameters for a determined or over-determined system.\n        # The number of parameters is 3 (gamma_0, gamma_1, gamma_2).\n        if len(valid_indices) < 3:\n            # This case is not in the test suite but is a potential issue.\n            # A simple assumption might be no bias, but we follow the spec.\n            # `lstsq` will handle this by finding a least-norm solution.\n            pass\n\n        log_s_valid = np.log(s[valid_indices])\n        a_valid = a[valid_indices]\n        log_m_valid = np.log(m[valid_indices])\n\n        # Design matrix X for regression: [1, log(s_i), a_i]\n        X = np.vstack([np.ones(len(valid_indices)), log_s_valid, a_valid]).T\n        \n        # Solve for gamma = [gamma_0, gamma_1, gamma_2]\n        gamma, _, _, _ = np.linalg.lstsq(X, log_m_valid, rcond=None)\n        \n        # Compute bias estimates for all files\n        log_b_hat = gamma[0] + gamma[1] * np.log(s) + gamma[2] * a\n        b_hat = np.exp(log_b_hat)\n\n        # Step 2: Form the covariate-weighted matrix W\n        # Using np.outer for efficient calculation of the denominator\n        W = C / np.outer(b_hat, b_hat)\n\n        # Step 3: Apply symmetric iterative proportional fitting (balancing)\n        is_nonzero_row = W.sum(axis=1) > 0\n        x = is_nonzero_row.astype(float)\n        \n        for _ in range(max_iter):\n            x_old = x.copy()\n            \n            # Compute v = W @ x\n            v = W @ x_old\n            \n            # Update x for non-zero rows. \n            # For irreducible non-negative matrices W and positive x, v will be positive.\n            # No special handling for v[i]=0 needed for the given test cases.\n            x[is_nonzero_row] = 1.0 / v[is_nonzero_row]\n\n            # Check for convergence\n            if np.max(np.abs(x - x_old)) < tol:\n                break\n        \n        # Compute the final balanced matrix B and its row sums\n        B = np.diag(x) @ W @ np.diag(x)\n        row_sums = B.sum(axis=1)\n        \n        return row_sums\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"C\": np.array([\n                [100, 10, 5, 0],\n                [10, 80, 20, 5],\n                [5, 20, 60, 15],\n                [0, 5, 15, 40]\n            ], dtype=float),\n            \"s\": np.array([1000, 500, 200, 100], dtype=float),\n            \"a\": np.array([5, 2, 1, 0.5], dtype=float)\n        },\n        {\n            \"C\": np.array([\n                [30, 5, 0, 0, 0],\n                [5, 25, 5, 0, 0],\n                [0, 5, 20, 10, 0],\n                [0, 0, 10, 15, 0],\n                [0, 0, 0, 0, 0]\n            ], dtype=float),\n            \"s\": np.array([300, 200, 150, 100, 50], dtype=float),\n            \"a\": np.array([1, 2, 3, 4, 5], dtype=float)\n        },\n        {\n            \"C\": np.array([\n                [10, 10, 10],\n                [10, 10, 10],\n                [10, 10, 10]\n            ], dtype=float),\n            \"s\": np.array([1e6, 1e3, 10], dtype=float),\n            \"a\": np.array([10, 5, 1], dtype=float)\n        }\n    ]\n\n    results_formatted = []\n    for case in test_cases:\n        row_sums = normalize_matrix(case[\"C\"], case[\"s\"], case[\"a\"])\n        # Format each row sum to 6 decimal places and join into a string representation of a list\n        formatted_sums = f\"[{','.join([f'{rs:.6f}' for rs in row_sums])}]\"\n        results_formatted.append(formatted_sums)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n```", "id": "2397168"}]}