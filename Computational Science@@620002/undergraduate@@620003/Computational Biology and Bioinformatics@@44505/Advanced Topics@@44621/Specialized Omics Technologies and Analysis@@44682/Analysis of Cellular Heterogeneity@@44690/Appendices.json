{"hands_on_practices": [{"introduction": "Before drawing biological conclusions, we must first ensure the quality of our data by identifying and addressing technical artifacts. This practice provides a unique opportunity to understand these artifacts from the ground up [@problem_id:2371686]. You will simulate single-cell RNA sequencing data, intentionally introducing a common technical effect, and then develop a detection algorithm based on quality control metrics, bridging the gap between the theoretical cause of an artifact and its practical detection.", "problem": "You must write a complete, runnable program that simulates single-cell RNA sequencing (scRNA-seq) data under a generative model where a technical artifact (for example, poor sequencing on one lane) introduces a subset of cells with globally reduced capture efficiency, leading to a spurious cluster in quality-control (QC) feature space. The program must then detect whether such an artifact-driven cluster is present, using only logically derived criteria based on QC metrics.\n\nBegin from the following foundational definitions and well-tested facts. In single-cell RNA sequencing (scRNA-seq), messenger ribonucleic acid (mRNA) molecules are reverse-transcribed and counted via Unique Molecular Identifier (UMI) tags; UMI counts per gene per cell are commonly modeled as draws from a Poisson distribution, reflecting a counting process with mean proportional to the underlying expression level and the total sampling depth. Specifically, let $X_{i,g}$ denote the UMI count for cell $i$ and gene $g$. Given a biological cluster for cell $i$ with per-gene expression means $\\mu_{b(i),g} > 0$ and a cell-specific size factor $s_i > 0$ that captures sampling depth and capture efficiency, it is standard to model\n$$\nX_{i,g} \\sim \\mathrm{Poisson}\\!\\left(s_i \\,\\mu_{b(i),g}\\right),\n$$\nwith independence across $g$ conditional on $(s_i,\\mu_{b(i),\\cdot})$. A technical lane artifact that reduces sequencing depth can be represented as a multiplicative scaling of $s_i$ by a factor $a$ with $0 < a \\le 1$ for all cells in the affected lane. For quality control (QC), define the library size for cell $i$ as $L_i = \\sum_{g=1}^{G} X_{i,g}$ and the zero fraction as $Z_i = \\frac{1}{G} \\sum_{g=1}^{G} \\mathbf{1}\\{X_{i,g} = 0\\}$. Under the Poisson model, the sum of independent Poisson random variables is Poisson with mean equal to the sum of means, so $\\mathbb{E}[L_i] = s_i \\sum_{g=1}^{G} \\mu_{b(i),g}$, and the probability of zero for one gene is $\\mathbb{P}(X_{i,g}=0) = \\exp(-s_i \\mu_{b(i),g})$. Therefore, multiplicative reduction $s_i \\mapsto a s_i$ decreases $\\mathbb{E}[L_i]$ by factor $a$ and increases the expected zero fraction, tending to create a distinct cluster in QC feature space spanned by $\\log_{10}(L_i+1)$ and $Z_i$.\n\nYou must implement the following tasks:\n\n1. Simulation of biological heterogeneity and lane artifact:\n   - Simulate $N$ cells and $G$ genes. There are $B$ biological clusters. For each biological cluster $b \\in \\{1,\\dots,B\\}$, generate cluster-specific mean expression $\\mu_{b,g}$ for each gene $g$ by first drawing a baseline profile $\\mu_{1,g}$ from a Gamma distribution with shape and scale chosen to yield realistic means, then perturbing per cluster by independent log-normal multipliers to reflect biological variability. Assign cells uniformly at random to biological clusters.\n   - Draw cell size factors $s_i$ from a log-normal distribution to reflect variability in sequencing depth. Select a subset of cells (an “artifact lane”) of proportion $p$ and multiply their $s_i$ by a scalar $a$ with $0 < a \\le 1$, where $a < 1$ indicates an artifact.\n   - Generate counts $X_{i,g} \\sim \\mathrm{Poisson}(s_i \\mu_{b(i),g})$ independently across genes.\n   - Compute the QC features per cell: $F_i = \\left[\\log_{10}(L_i+1), Z_i\\right]$.\n\n2. Unsupervised artifact detection in QC space:\n   - Cluster the cells in the two-dimensional QC feature space using $k$-means with $k=2$. Initialize the two centroids deterministically as the points with the minimum and maximum $\\log_{10}(L_i+1)$ values, and iterate until convergence or a fixed iteration cap.\n   - Let the two cluster centroids be $C_{\\text{small}}$ and $C_{\\text{large}}$, where $C_{\\text{small}}$ corresponds to the cluster with fewer cells and $C_{\\text{large}}$ to the other cluster. Denote their coordinates by $C_{\\text{small}} = [\\ell_s, z_s]$ and $C_{\\text{large}} = [\\ell_\\ell, z_\\ell]$.\n   - Declare an artifact-driven cluster to be “detected” if and only if all of the following hold:\n     - The centroid differences exceed minimum separations: $\\ell_\\ell - \\ell_s \\ge \\tau_\\ell$ and $z_s - z_\\ell \\ge \\tau_z$.\n     - The size fraction of the smaller cluster lies within bounds: $f_{\\min} \\le \\frac{n_{\\text{small}}}{N} \\le f_{\\max}$.\n   - Use fixed thresholds $\\tau_\\ell = 0.2$, $\\tau_z = 0.05$, $f_{\\min} = 0.05$, and $f_{\\max} = 0.6$.\n\n3. Determinism:\n   - Use a pseudorandom number generator with a fixed seed per test case to ensure determinism.\n\nYour program must evaluate the following test suite of parameter sets (each written as an ordered tuple $(N,G,B,p,a,\\text{seed})$), simulate, and then for each case output a boolean indicating whether an artifact-driven cluster is detected by the above rule:\n- Case A (clear artifact, substantial depth loss, moderate prevalence): $(N,G,B,p,a,\\text{seed}) = (\\,600,\\,1500,\\,2,\\,0.3,\\,0.25,\\,1\\,)$.\n- Case B (moderate artifact, lower prevalence, added biological complexity): $(N,G,B,p,a,\\text{seed}) = (\\,500,\\,1200,\\,3,\\,0.2,\\,0.5,\\,2\\,)$.\n- Case C (no artifact control): $(N,G,B,p,a,\\text{seed}) = (\\,500,\\,1200,\\,2,\\,0.0,\\,1.0,\\,3\\,)$.\n- Case D (artifact too rare to be a coherent cluster): $(N,G,B,p,a,\\text{seed}) = (\\,400,\\,1000,\\,2,\\,0.02,\\,0.2,\\,4\\,)$.\n\nRequirements and output:\n- Your implementation must follow the above modeling and detection design precisely.\n- The final program output must be a single line containing a comma-separated list of the $4$ boolean detection results enclosed in square brackets and without spaces, in the order A, B, C, D. For example, an output like $[{\\tt True},{\\tt False},{\\tt False},{\\tt False}]$ is acceptable if and only if it matches the results of your implementation for the specified test cases.", "solution": "The problem statement is examined and found to be valid. It constitutes a well-posed computational task in bioinformatics, specifically the simulation and analysis of single-cell RNA sequencing (scRNA-seq) data. The underlying models—the Poisson model for UMI counts, the representation of technical artifacts via size factor scaling, and the use of quality control (QC) metrics for cluster detection—are all grounded in established principles of the field. The objectives are stated with sufficient clarity and formality to permit a unique and verifiable solution. The problem provides a set of parameters and a deterministic procedure, ensuring that the outcome is reproducible.\n\nThe solution is implemented by following the prescribed algorithm, which is divided into two primary stages: data simulation and artifact detection.\n\n**1. Data Simulation**\n\nThe simulation generates a synthetic scRNA-seq count matrix, $X \\in \\mathbb{N}_0^{N \\times G}$, for $N$ cells and $G$ genes, incorporating both biological and technical sources of variation.\n\nFirst, biological heterogeneity is established by defining $B$ distinct cell clusters. A baseline mean expression profile, $\\{\\mu_{1,g}\\}_{g=1}^G$, is generated by drawing values from a Gamma distribution, chosen for its suitability in modeling non-negative, skewed data like gene expression levels. Specifically, $\\mu_{1,g} \\sim \\text{Gamma}(k=2.0, \\theta=0.5)$. For the remaining $B-1$ biological clusters, their mean expression profiles $\\{\\mu_{b,g}\\}_{g=1}^G$ are created by applying gene-specific multiplicative perturbations to the baseline profile. These multipliers are drawn from a log-normal distribution, $\\text{LogNormal}(\\mu=0, \\sigma^2=0.25)$, to model up- or down-regulation of genes across cell types. Each of the $N$ cells is then assigned to one of the $B$ biological clusters uniformly at random.\n\nSecond, cell-specific technical variability is introduced through size factors, $\\{s_i\\}_{i=1}^N$, which account for differences in mRNA capture and sequencing efficiency. These are drawn from a log-normal distribution, $s_i \\sim \\text{LogNormal}(\\mu=0, \\sigma^2=0.25)$. To model the specified technical artifact, a subset of cells, comprising a proportion $p$ of the total population, is selected. The size factors of these designated \"artifact\" cells are scaled by a multiplicative factor $a \\in (0, 1]$. For a non-artifact case, $a=1$ and $p=0$.\n\nFinally, the UMI count for cell $i$ and gene $g$, denoted $X_{i,g}$, is generated as an independent draw from a Poisson distribution with a rate parameter $\\lambda_{i,g}$ given by the product of the cell's effective size factor and the gene's mean expression in that cell's biological cluster.\n$$\nX_{i,g} \\sim \\mathrm{Poisson}(s_i \\mu_{b(i),g})\n$$\nwhere $b(i)$ is the biological cluster identity of cell $i$. This process is executed for all $N \\times G$ cell-gene pairs to form the complete count matrix $X$.\n\n**2. Artifact Detection**\n\nThe detection phase operates on a two-dimensional QC feature space derived from the count matrix. For each cell $i$, two QC metrics are computed: the library size, $L_i = \\sum_{g=1}^{G} X_{i,g}$, and the zero fraction, $Z_i = G^{-1} \\sum_{g=1}^{G} \\mathbf{1}\\{X_{i,g}=0\\}$. The QC feature vector for cell $i$ is defined as $F_i = [\\log_{10}(L_i + 1), Z_i]$.\n\nThe core of the detection algorithm is $k$-means clustering with $k=2$ applied to the set of all feature vectors $\\{F_i\\}_{i=1}^N$. As mandated, the procedure is initialized deterministically. The two initial centroids are set to the feature vectors of the cells possessing the minimum and maximum values of $\\log_{10}(L_i+1)$, respectively. The standard iterative $k$-means algorithm then proceeds until the cluster centroids converge.\n\nAfter convergence, the two resulting clusters are labeled based on their size. The cluster containing fewer cells is designated \"small,\" and the other is designated \"large.\" Let their respective cell counts be $n_{\\text{small}}$ and $n_{\\text{large}}$, and their centroids be $C_{\\text{small}} = [\\ell_s, z_s]$ and $C_{\\text{large}} = [\\ell_\\ell, z_\\ell]$.\n\nAn artifact-driven cluster is declared \"detected\" if and only if a set of three logical conditions is satisfied simultaneously. These conditions formalize the expected signature of a low-quality cell cluster.\n1.  **Sufficient separation on the library size axis:** The centroid of the large cluster must have a sufficiently larger log-library size than the small cluster.\n    $$ \\ell_\\ell - \\ell_s \\ge \\tau_\\ell $$\n    with threshold $\\tau_\\ell = 0.2$.\n2.  **Sufficient separation on the zero fraction axis:** The centroid of the small cluster must have a sufficiently larger zero fraction than the large cluster. This is consistent with low-quality cells having fewer detected genes.\n    $$ z_s - z_\\ell \\ge \\tau_z $$\n    with threshold $\\tau_z = 0.05$.\n3.  **Plausible cluster size:** The fractional size of the small cluster, $n_{\\text{small}} / N$, must fall within a predefined range to be considered a coherent group rather than a negligible set of outliers or a majority of the data.\n    $$ f_{\\min} \\le \\frac{n_{\\text{small}}}{N} \\le f_{\\max} $$\n    with thresholds $f_{\\min} = 0.05$ and $f_{\\max} = 0.6$.\n\nA boolean result indicating detection status is computed for each test case by evaluating the logical AND of these three conditions.", "answer": "```python\nimport numpy as np\n\ndef run_simulation(N, G, B, p, a, seed):\n    \"\"\"\n    Simulates scRNA-seq data and detects an artifact-driven cluster.\n\n    Args:\n        N (int): Number of cells.\n        G (int): Number of genes.\n        B (int): Number of biological clusters.\n        p (float): Proportion of cells in the artifact lane.\n        a (float): Multiplicative scaling factor for the artifact.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        bool: True if an artifact-driven cluster is detected, False otherwise.\n    \"\"\"\n    # 3. Determinism: Use a seeded RNG\n    rng = np.random.default_rng(seed)\n\n    # 1. Simulation of biological heterogeneity and lane artifact\n    \n    # Generate biological means mu_b,g\n    # These parameters are chosen to be reasonable but are not specified in the problem.\n    gamma_shape = 2.0\n    gamma_scale = 0.5\n    lognorm_sigma = 0.5\n    \n    # Baseline profile for the first biological cluster\n    mu_baseline = rng.gamma(gamma_shape, gamma_scale, size=G)\n    \n    # Generate profiles for all B clusters\n    mu_profiles = np.zeros((B, G))\n    mu_profiles[0, :] = mu_baseline\n    for b in range(1, B):\n        perturbations = rng.lognormal(mean=0.0, sigma=lognorm_sigma, size=G)\n        mu_profiles[b, :] = mu_baseline * perturbations\n\n    # Assign cells uniformly at random to biological clusters\n    cell_bio_clusters = rng.integers(0, B, size=N)\n\n    # Draw cell size factors s_i from a log-normal distribution\n    # These parameters are chosen to be reasonable. Mean=0 gives a median of 1.\n    size_factors = rng.lognormal(mean=0.0, sigma=lognorm_sigma, size=N)\n    \n    # Select a subset of cells for the artifact lane and apply the artifact\n    num_artifact_cells = int(np.floor(N * p))\n    if num_artifact_cells > 0:\n        artifact_indices = rng.choice(N, size=num_artifact_cells, replace=False)\n        size_factors[artifact_indices] *= a\n\n    # Generate count matrix X_i,g ~ Poisson(s_i * mu_b(i),g)\n    # The rates matrix lambda_ig has dimensions N x G\n    rates = size_factors[:, np.newaxis] * mu_profiles[cell_bio_clusters, :]\n    X = rng.poisson(rates)\n\n    # Compute QC features per cell\n    L = X.sum(axis=1)\n    Z = (X == 0).mean(axis=1)\n    F = np.vstack([np.log10(L + 1), Z]).T\n\n    # 2. Unsupervised artifact detection in QC space\n    \n    # k-means clustering with k=2\n    # Deterministic initialization\n    idx_min_l = np.argmin(F[:, 0])\n    idx_max_l = np.argmax(F[:, 0])\n    centroids = np.array([F[idx_min_l, :], F[idx_max_l, :]])\n\n    # Iterate until convergence or max iterations\n    max_iter = 100\n    for _ in range(max_iter):\n        old_centroids = centroids.copy()\n        \n        # Assign points to the nearest centroid (Euclidean distance)\n        dist_sq_0 = np.sum((F - centroids[0])**2, axis=1)\n        dist_sq_1 = np.sum((F - centroids[1])**2, axis=1)\n        labels = (dist_sq_1  dist_sq_0).astype(int)\n        \n        # Handle case where a cluster becomes empty\n        if np.sum(labels == 0) == 0 or np.sum(labels == 1) == 0:\n            # If a cluster is empty, re-initialization is needed.\n            # However, for this problem's setup, it is highly unlikely.\n            # We can stop here, as centroids cannot be computed.\n            return False # No valid two-cluster structure found\n\n        # Update centroids\n        centroids[0] = F[labels == 0].mean(axis=0)\n        centroids[1] = F[labels == 1].mean(axis=0)\n        \n        if np.allclose(old_centroids, centroids):\n            break\n\n    # Label clusters as 'small' and 'large' based on cell count\n    n_0 = np.sum(labels == 0)\n    n_1 = np.sum(labels == 1)\n\n    if n_0  n_1:\n        n_small, C_small = n_0, centroids[0]\n        n_large, C_large = n_1, centroids[1]\n    else:\n        n_small, C_small = n_1, centroids[1]\n        n_large, C_large = n_0, centroids[0]\n    \n    l_s, z_s = C_small\n    l_l, z_l = C_large\n\n    # Define detection thresholds\n    tau_l = 0.2\n    tau_z = 0.05\n    f_min = 0.05\n    f_max = 0.6\n\n    # Declare an artifact-driven cluster \"detected\" if all criteria are met\n    cond1 = (l_l - l_s) >= tau_l\n    cond2 = (z_s - z_l) >= tau_z\n    frac_small = n_small / N\n    cond3 = (f_min = frac_small) and (frac_small = f_max)\n\n    return cond1 and cond2 and cond3\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Clear artifact, substantial depth loss, moderate prevalence\n        (600, 1500, 2, 0.3, 0.25, 1),\n        # Case B: Moderate artifact, lower prevalence, added biological complexity\n        (500, 1200, 3, 0.2, 0.5, 2),\n        # Case C: No artifact control\n        (500, 1200, 2, 0.0, 1.0, 3),\n        # Case D: Artifact too rare to be a coherent cluster\n        (400, 1000, 2, 0.02, 0.2, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        N, G, B, p, a, seed = params\n        detection_result = run_simulation(N, G, B, p, a, seed)\n        results.append(detection_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2371686"}, {"introduction": "With clean data in hand, the next step is to identify groups of biologically similar cells through clustering. A common challenge in this process is selecting the appropriate level of granularity, or 'resolution,' for the clusters. This exercise guides you through implementing a sophisticated, stability-based approach to automatically determine an optimal resolution parameter $\\gamma$ for the Louvain community detection algorithm [@problem_id:2371617], a cornerstone technique for revealing the structure within heterogeneous cell populations.", "problem": "You are given the task of designing and implementing an algorithm to automatically suggest the optimal resolution parameter for Louvain clustering based on the stability of the resulting partitions when analyzing cellular heterogeneity from single-cell similarity graphs. Your implementation must be a complete and runnable program that performs the following tasks for a set of predefined test cases.\n\nFundamental base and definitions:\n- A cell-by-cell similarity graph is represented as an undirected weighted graph with adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\ge 0$, $A_{ij} = A_{ji}$, and $A_{ii} = 0$.\n- The degree of node $i$ is $k_i = \\sum_{j=1}^n A_{ij}$, and $m = \\frac{1}{2} \\sum_{i,j=1}^n A_{ij}$ is the total edge weight.\n- The Louvain objective optimized is the generalized modularity with resolution parameter $\\gamma  0$:\n$$\nQ(\\gamma) \\;=\\; \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\Big( A_{ij} \\;-\\; \\gamma \\frac{k_i k_j}{2m} \\Big) \\, \\mathbb{1}\\{c_i = c_j\\},\n$$\nwhere $c_i$ is the community label of node $i$, and $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n- Cluster stability is quantified using the Adjusted Rand Index (ARI). For two partitions of $n$ items with contingency table entries $n_{ij}$, row sums $a_i = \\sum_j n_{ij}$, and column sums $b_j = \\sum_i n_{ij}$, the Adjusted Rand Index is\n$$\n\\mathrm{ARI} \\;=\\; \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} \\;-\\; \\frac{\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}}{\\frac{1}{2}\\Big( \\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2} \\Big) \\;-\\; \\frac{\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}}.\n$$\n- The stability score for a given $\\gamma$ is defined as the mean pairwise Adjusted Rand Index across $R$ independent runs (distinct random seeds) of a Louvain-like local moving procedure.\n\nAlgorithmic requirements:\n- Implement a Louvain-like local moving algorithm that, for a fixed $\\gamma$, starts with each node in its own community and repeatedly attempts to move individual nodes to neighboring communities if doing so increases $Q(\\gamma)$. At each step:\n  - Visit nodes in a random order.\n  - For a node $i$, consider moving it to the community of any neighboring node (and the option to stay). For each candidate, compute the resulting modularity $Q(\\gamma)$ and choose the move that yields the largest increase. Break ties randomly. Apply the best move if and only if it strictly increases $Q(\\gamma)$.\n  - Repeat passes until a full pass yields no moves that increase $Q(\\gamma)$.\n- For each $\\gamma$ in a candidate set, run the local moving algorithm $R$ times with different random seeds. Compute:\n  - The mean pairwise Adjusted Rand Index across the $R$ partitions as the stability score $S(\\gamma)$.\n  - The mean modularity $\\overline{Q}(\\gamma)$ over the $R$ runs.\n  - The mean number of clusters $\\overline{K}(\\gamma)$ over the $R$ runs.\n- Selection rule for the suggested resolution:\n  - Choose $\\gamma^\\star$ that maximizes $S(\\gamma)$.\n  - If multiple $\\gamma$ values share the maximum $S(\\gamma)$ within a tolerance of $10^{-8}$, choose the one with the largest $\\overline{Q}(\\gamma)$.\n  - If still tied, choose the smallest $\\gamma$.\n\nTest suite:\nImplement and evaluate your method on the following three synthetic graphs. Each graph is defined by block structure parameters from which you must construct $A$.\n- Case $1$ (two well-separated groups):\n  - Group sizes: $[5, 5]$.\n  - Within-group edge weight: $1.0$.\n  - Between-group edge weight: $0.05$ (uniformly between any two nodes from different groups).\n  - Candidate $\\gamma$ values: $[0.5, 1.0, 1.5, 2.0]$.\n  - Number of runs per $\\gamma$: $R = 6$.\n- Case $2$ (homogeneous graph):\n  - A complete graph on $n = 9$ nodes with all off-diagonal weights $1.0$ and diagonal entries $0$.\n  - Candidate $\\gamma$ values: $[0.1, 0.5, 1.0, 2.0]$.\n  - Number of runs per $\\gamma$: $R = 6$.\n- Case $3$ (three moderately separated groups):\n  - Group sizes: $[4, 4, 4]$.\n  - Within-group edge weight: $1.0$.\n  - Between-group edge weight: $0.1$ (uniformly between any two nodes from different groups).\n  - Candidate $\\gamma$ values: $[0.5, 1.0, 1.5, 2.5]$.\n  - Number of runs per $\\gamma$: $R = 6$.\n\nConstruction of $A$ for block graphs:\n- For a block model with group sizes $[s_1, s_2, \\dots, s_g]$, set $A_{ij} = 1.0$ if nodes $i$ and $j$ are in the same group and $i \\ne j$, and $A_{ij} = w_{\\text{between}}$ if they are in different groups. Set all diagonals to $0$.\n- For the homogeneous graph, set $A_{ij} = 1.0$ for all $i \\ne j$, and $A_{ii} = 0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the suggested $\\gamma^\\star$ for each of the three cases, in order, as a comma-separated list enclosed in square brackets. For example, the output should look like $[1.0,0.5,1.5]$.\n- Each entry must be a single floating-point number equal to one of the candidate $\\gamma$ values for that case.\n\nAll computations are dimensionless; no physical units are involved. Angles are not present. Percentages, if any derived, must be represented as decimals, but the final outputs here are floating-point numbers as specified.", "solution": "The problem statement has been rigorously evaluated and is determined to be **valid**. It is scientifically grounded in established principles of network science and computational biology, is mathematically and algorithmically well-posed, and provides a complete and consistent set of requirements for a solvable computational task. There are no violations of fundamental principles, ambiguous definitions, or missing information that would preclude a unique and verifiable solution.\n\nThe task is to determine the optimal resolution parameter $\\gamma$ for Louvain community detection by assessing the stability of partitions. The approach is to execute a local moving algorithm multiple times for each candidate $\\gamma$, and then select the $\\gamma$ that produces the most consistent (stable) clustering results across runs.\n\nThe methodology is implemented through the following sequence of steps for each test case.\n\n**1. Graph Representation**\n\nFor each test case, an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ is constructed based on the provided block model parameters. Given group sizes, a within-group edge weight $w_{\\text{in}}$, and a between-group edge weight $w_{\\text{out}}$, the matrix elements $A_{ij}$ are defined as:\n$$\nA_{ij} = \\begin{cases}\n    w_{\\text{in}}  \\text{if nodes } i \\text{ and } j \\text{ are in the same group and } i \\neq j \\\\\n    w_{\\text{out}}  \\text{if nodes } i \\text{ and } j \\text{ are in different groups} \\\\\n    0  \\text{if } i = j\n\\end{cases}\n$$\nThe degree of each node, $k_i = \\sum_{j=1}^n A_{ij}$, and the total edge weight of the graph, $m = \\frac{1}{2}\\sum_{i,j} A_{ij}$, are pre-calculated as they are fundamental to modularity computations.\n\n**2. Louvain-like Local Moving Algorithm**\n\nThe core of the clustering process is a local moving heuristic designed to optimize the generalized modularity function $Q(\\gamma)$:\n$$\nQ(\\gamma) = \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\gamma \\frac{k_i k_j}{2m} \\right) \\mathbb{1}\\{c_i = c_j\\}\n$$\nDirectly re-calculating $Q(\\gamma)$ for every potential node move is computationally prohibitive. Instead, we compute the change in modularity, $\\Delta Q$, that results from moving a node $i$ from its current community $C_{\\text{old}}$ to a candidate community $C_{\\text{new}}$. This change is given by the efficient formula:\n$$\n\\Delta Q = \\frac{k_{i, \\text{new}} - k_{i, \\text{old}}}{m} - \\frac{\\gamma k_i}{2m^2} \\left( \\Sigma_{\\text{tot}, \\text{new}} - \\Sigma_{\\text{tot}, \\text{old}} + k_i \\right)\n$$\nwhere $k_{i,C}$ is the sum of weights of edges connecting node $i$ to nodes in community $C$, and $\\Sigma_{\\text{tot}, C}$ is the sum of degrees of all nodes in community $C$.\n\nThe algorithm proceeds as follows:\n-   **Initialization**: Each node $i$ is assigned to its own unique community, $c_i = i$.\n-   **Iteration**: The algorithm repeatedly passes through all nodes. In each pass:\n    1.  The nodes are visited in a random order to avoid any bias from a fixed ordering.\n    2.  For each node $i$, we consider moving it to the community of one of its neighbors.\n    3.  The $\\Delta Q$ is calculated for each potential move.\n    4.  If the maximum $\\Delta Q$ is strictly positive, the move is executed. Ties for the maximal gain are broken randomly.\n    5.  Community-level statistics, such as $\\Sigma_{\\text{tot}, C}$, are updated after each move.\n-   **Termination**: The process halts when a full pass over all nodes results in no moves that yield a strict increase in $Q(\\gamma)$, indicating a local optimum has been reached.\n\n**3. Stability Quantification using Adjusted Rand Index (ARI)**\n\nFor each candidate $\\gamma$, the local moving algorithm is executed $R$ times, each with a different random seed, yielding a set of $R$ partitions. The stability of clustering at this resolution is quantified by the mean pairwise Adjusted Rand Index (ARI) over all pairs of these partitions. The ARI is a measure of similarity between two data clusterings, corrected for chance. Given two partitions, a contingency table $n_{ij}$ is formed. The ARI is computed using the formula:\n$$\n\\mathrm{ARI} = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - \\frac{[\\sum_i \\binom{a_i}{2}][\\sum_j \\binom{b_j}{2}]}{\\binom{n}{2}}}{\\frac{1}{2}[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - \\frac{[\\sum_i \\binom{a_i}{2}][\\sum_j \\binom{b_j}{2}]}{\\binom{n}{2}}}\n$$\nwhere $a_i = \\sum_j n_{ij}$ and $b_j = \\sum_i n_{ij}$ are the marginal sums of the contingency table, and $n$ is the total number of nodes. The binomial coefficient $\\binom{k}{2}$ is calculated as $k(k-1)/2$. The stability score for a given $\\gamma$ is then $S(\\gamma) = \\text{mean}(\\text{ARI}_{jk})$ for $1 \\le j  k \\le R$.\n\n**4. Optimal Resolution Selection**\n\nThe optimal resolution parameter, $\\gamma^\\star$, is selected from the set of candidate values based on a deterministic, three-tiered rule:\n1.  Primary Criterion: Select the value(s) of $\\gamma$ that maximize the stability score $S(\\gamma)$. A tolerance of $10^{-8}$ is used to identify ties for the maximum.\n2.  First Tie-Breaker: If multiple $\\gamma$ values are tied, select from this subset the one that maximizes the mean modularity, $\\overline{Q}(\\gamma)$, averaged over the $R$ runs.\n3.  Second Tie-Breaker: If a tie persists, select the smallest $\\gamma$ value among the remaining candidates.\n\nThis procedure is systematically applied to each of the three test cases specified in the problem to derive the final suggested resolutions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_graph(group_sizes, w_in, w_out):\n    \"\"\"Constructs the adjacency matrix for a block model graph.\"\"\"\n    n_nodes = sum(group_sizes)\n    A = np.zeros((n_nodes, n_nodes))\n    node_to_group = np.zeros(n_nodes, dtype=int)\n    \n    start_idx = 0\n    for i, size in enumerate(group_sizes):\n        end_idx = start_idx + size\n        node_to_group[start_idx:end_idx] = i\n        start_idx = end_idx\n\n    for i in range(n_nodes):\n        for j in range(i + 1, n_nodes):\n            if node_to_group[i] == node_to_group[j]:\n                weight = w_in\n            else:\n                weight = w_out\n            A[i, j] = A[j, i] = weight\n            \n    return A\n\ndef n_choose_2(n):\n    \"\"\"Computes the binomial coefficient C(n, 2).\"\"\"\n    if n  2:\n        return 0\n    return n * (n - 1) // 2\n\ndef calculate_ari(labels_1, labels_2):\n    \"\"\"Calculates the Adjusted Rand Index between two clusterings.\"\"\"\n    n = len(labels_1)\n    if n = 1:\n        return 1.0\n\n    # Relabel to contiguous integers starting from 0\n    _, u_labels_1 = np.unique(labels_1, return_inverse=True)\n    _, u_labels_2 = np.unique(labels_2, return_inverse=True)\n    \n    n_labels_1 = np.max(u_labels_1) + 1 if len(u_labels_1) > 0 else 0\n    n_labels_2 = np.max(u_labels_2) + 1 if len(u_labels_2) > 0 else 0\n\n    contingency = np.zeros((n_labels_1, n_labels_2), dtype=np.int64)\n    for i in range(n):\n        contingency[u_labels_1[i], u_labels_2[i]] += 1\n\n    sum_comb_nij = np.sum([n_choose_2(nij) for nij in contingency.flat])\n    \n    sum_comb_a = np.sum([n_choose_2(k) for k in np.sum(contingency, axis=1)])\n    sum_comb_b = np.sum([n_choose_2(k) for k in np.sum(contingency, axis=0)])\n    \n    total_comb = n_choose_2(n)\n    if total_comb == 0:\n        return 1.0\n\n\n    expected_index = (sum_comb_a * sum_comb_b) / total_comb\n    max_index = (sum_comb_a + sum_comb_b) / 2\n    \n    denominator = max_index - expected_index\n    if denominator == 0:\n        # This occurs on trivial clusterings. Conventionally ARI is 0,\n        # unless it is a perfect match (numerator is also 0).\n        return 0.0\n\n    numerator = sum_comb_nij - expected_index\n    return numerator / denominator\n\n\ndef calculate_modularity(A, gamma, communities, two_m):\n    \"\"\"Calculates the modularity of a given partition.\"\"\"\n    if two_m == 0:\n        return 0.0\n    \n    Q = 0.0\n    unique_comms = np.unique(communities)\n    k = A.sum(axis=1)\n\n    for comm_id in unique_comms:\n        nodes_in_comm = np.where(communities == comm_id)[0]\n        subgraph = A[np.ix_(nodes_in_comm, nodes_in_comm)]\n        sigma_in = np.sum(subgraph) / 2.0  # Sum of internal edge weights\n        sigma_tot = np.sum(k[nodes_in_comm])\n        \n        term1 = sigma_in / two_m\n        term2 = (sigma_tot / two_m)**2\n        Q += term1 - gamma * term2\n\n    return Q * 2 # The formula Q = sum(...) represents sum over pairs, we summed over edges, so double it. Wait no.\n    # The formula is 1/2m * sum(...) -> sum (sigma_in/2m - (gamma*sigma_tot^2)/(2m)^2)\n    # The sum of weights is Sum(A_ij) over i,j in C. Sum(subgraph) is this. But A is symmetric.\n    # sum_ij in C A_ij = 2 * sum_edges_in_C. Let's use Blondel's definition.\n    # Q = sum_c [ (sum_in_c / 2m) - (sum_tot_c / 2m)^2 ]\n    # My sum_in sums weights of edges (not pairs). Total sum of weights is m. sum(subgraph) is 2 * sum_in.\n    # So sum_in is sum(subgraph)/2. And total edge weight m is two_m/2.\n    # Q_c = (sum(subgraph)/2 / m) - gamma * (sigma_tot / 2m)^2\n    # Q_c = sum(subgraph)/two_m - gamma * (sigma_tot / two_m)^2\n    # So sum over communities. Let's re-verify.\n    mod = 0.0\n    for comm_id in unique_comms:\n        nodes_in_comm = np.where(partition == comm_id)[0]\n        sigma_tot = np.sum(k[nodes_in_comm])\n        \n        # Sum of weights within community\n        sum_in_comm_weights = 0\n        for i in range(len(nodes_in_comm)):\n            for j in range(i + 1, len(nodes_in_comm)):\n                u, v = nodes_in_comm[i], nodes_in_comm[j]\n                sum_in_comm_weights += A[u,v]\n\n        mod += (sum_in_comm_weights / (two_m / 2.0)) - gamma * (sigma_tot / two_m)**2\n    \n    return mod\n\n\ndef louvain_local_moving(A, gamma, rng):\n    \"\"\"Performs Louvain-like local moving to optimize modularity.\"\"\"\n    n = A.shape[0]\n    k = A.sum(axis=1)\n    two_m = k.sum()\n\n    if two_m == 0:\n        return np.arange(n)\n\n    communities = np.arange(n)\n    sigma_tot = np.copy(k) # Sigma_tot for each community\n    \n    while True:\n        moved = False\n        node_order = rng.permutation(n)\n        \n        for i in node_order:\n            old_comm_id = communities[i]\n            ki = k[i]\n            \n            # Efficiently compute sum of weights to each community\n            k_i_comms = np.bincount(communities, weights=A[i], minlength=n)\n            k_i_in = k_i_comms[old_comm_id]\n\n            neighbors = np.where(A[i] > 0)[0]\n            cand_comm_ids = np.unique(communities[neighbors])\n            \n            best_gain = 0.0\n            best_comm_id = old_comm_id\n            \n            potential_gains = []\n            potential_comms = []\n\n            for cand_comm_id in cand_comm_ids:\n                if cand_comm_id == old_comm_id:\n                    continue\n\n                k_i_to = k_i_comms[cand_comm_id]\n                sigma_tot_old = sigma_tot[old_comm_id]\n                sigma_tot_new = sigma_tot[cand_comm_id]\n\n                gain = (k_i_to - k_i_in) - gamma * ki * (sigma_tot_new - sigma_tot_old + ki) / two_m\n                potential_gains.append(gain)\n                potential_comms.append(cand_comm_id)\n            \n            if potential_gains:\n                max_gain = np.max(potential_gains)\n                if max_gain > 1e-12: # Strict increase, with a small tolerance for floating point\n                    max_indices = np.where(np.abs(potential_gains - max_gain)  1e-12)[0]\n                    chosen_idx = rng.choice(max_indices)\n                    best_gain = potential_gains[chosen_idx]\n                    best_comm_id = potential_comms[chosen_idx]\n\n            if best_gain > 0:\n                moved = True\n                sigma_tot[old_comm_id] -= ki\n                sigma_tot[best_comm_id] += ki\n                communities[i] = best_comm_id\n        \n        if not moved:\n            break\n            \n    return communities\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    \n    test_cases = [\n        { # Case 1\n            \"group_sizes\": [5, 5], \"w_in\": 1.0, \"w_out\": 0.05,\n            \"gammas\": [0.5, 1.0, 1.5, 2.0], \"R\": 6\n        },\n        { # Case 2\n            \"group_sizes\": [9], \"w_in\": 1.0, \"w_out\": 1.0, # Homogeneous\n            \"gammas\": [0.1, 0.5, 1.0, 2.0], \"R\": 6\n        },\n        { # Case 3\n            \"group_sizes\": [4, 4, 4], \"w_in\": 1.0, \"w_out\": 0.1,\n            \"gammas\": [0.5, 1.0, 1.5, 2.5], \"R\": 6\n        }\n    ]\n    \n    base_seed = 42 # For deterministic \"random\" runs\n    final_gammas = []\n    \n    for case in test_cases:\n        if case[\"group_sizes\"] == [9]: # Homogeneous case\n            A = np.ones((9,9)) - np.eye(9)\n        else:\n            A = construct_graph(case[\"group_sizes\"], case[\"w_in\"], case[\"w_out\"])\n        \n        n = A.shape[0]\n        two_m = A.sum()\n        k = A.sum(axis=1)\n\n        results_per_gamma = []\n        \n        for gamma in case[\"gammas\"]:\n            partitions = []\n            modularities = []\n            \n            for r in range(case[\"R\"]):\n                rng = np.random.default_rng(base_seed + r)\n                partition = louvain_local_moving(A, gamma, rng)\n                partitions.append(partition)\n\n                mod = 0.0\n                if two_m > 0:\n                    unique_comms = np.unique(partition)\n                    for comm_id in unique_comms:\n                         nodes_in_comm = np.where(partition == comm_id)[0]\n                         subgraph_A = A[np.ix_(nodes_in_comm, nodes_in_comm)]\n                         sum_in = np.sum(subgraph_A)\n                         sum_tot = np.sum(k[nodes_in_comm])\n                         mod += sum_in - gamma * sum_tot**2 / two_m\n                    mod /= two_m\n                \n                modularities.append(mod)\n\n            aris = []\n            if case[\"R\"] > 1:\n                for i in range(case[\"R\"]):\n                    for j in range(i + 1, case[\"R\"]):\n                        ari = calculate_ari(partitions[i], partitions[j])\n                        aris.append(ari)\n            \n            S_gamma = np.mean(aris) if aris else 1.0\n            Q_bar_gamma = np.mean(modularities)\n            \n            results_per_gamma.append((S_gamma, Q_bar_gamma, gamma))\n            \n        # Selection rule\n        # 1. Maximize S(gamma)\n        max_S = -np.inf\n        for S, Q, g in results_per_gamma:\n            if S > max_S:\n                max_S = S\n                \n        tied_on_S = []\n        for S, Q, g in results_per_gamma:\n            if S >= max_S - 1e-8:\n                tied_on_S.append((S, Q, g))\n\n        # 2. Tie-break with Q_bar\n        max_Q = -np.inf\n        for S, Q, g in tied_on_S:\n            if Q > max_Q:\n                max_Q = Q\n        \n        tied_on_Q = []\n        for S, Q, g in tied_on_S:\n            if np.isclose(Q, max_Q):\n                tied_on_Q.append((S, Q, g))\n\n        # 3. Tie-break with smallest gamma\n        tied_on_Q.sort(key=lambda x: x[2])\n        best_gamma = tied_on_Q[0][2]\n        \n        final_gammas.append(best_gamma)\n\n    print(f\"[{','.join(map(str, final_gammas))}]\")\n\nsolve()\n\n```", "id": "2371617"}, {"introduction": "After defining robust cell clusters, the final analytical step is to assign them biological meaning, a process known as annotation. This practice demonstrates how to move beyond noisy single \"marker genes\" to the more powerful concept of a \"marker gene module\" [@problem_id:2371687]. By building a score from a set of co-expressed genes, you will learn to generate a more stable and reliable signal for identifying cell types, thereby increasing the statistical power to differentiate between distinct cell populations.", "problem": "You are given the task of formalizing and implementing a robust generalization of a single \"marker gene\" to a \"marker gene module\" for identifying a target cell type in single-cell gene expression data. The core idea is to exploit co-expression: a small set of genes that are statistically co-expressed with a seed marker gene yields a more stable signal across heterogeneous cells than any single gene alone.\n\nStart from the following fundamental base:\n- The Central Dogma of Molecular Biology: genes are transcribed into messenger ribonucleic acid (mRNA), which can be quantified to estimate gene expression.\n- A gene expression experiment yields a matrix where rows are genes and columns are cells. Entry $X_{g,c} \\ge 0$ is the measured abundance (e.g., counts) for gene $g$ in cell $c$.\n- Co-expression can be quantified with the Pearson correlation coefficient. Standardization across cells converts raw expression into a dimensionless score comparable across genes.\n\nYour program must implement the following pipeline for each dataset in the test suite.\n\nDefinitions and procedure:\n1. Data model. Let $G = \\{g_1,\\dots,g_p\\}$ be the set of $p$ genes and $C = \\{c_1,\\dots,c_n\\}$ be the set of $n$ cells. You are given a nonnegative matrix $X \\in \\mathbb{R}_{\\ge 0}^{p \\times n}$ and a binary label vector $y \\in \\{0,1\\}^n$ indicating whether each cell $c$ is of the target type ($y_c = 1$) or not ($y_c = 0$). A seed marker gene $g^\\ast \\in G$ and a correlation threshold $\\tau \\in [0,1]$ are also given.\n2. Co-expression and module construction.\n   - For each gene $g \\in G$, compute the Pearson correlation $r(g^\\ast,g)$ between the vectors $X_{g^\\ast,\\cdot}$ and $X_{g,\\cdot}$ across the $n$ cells, using the population formulation: for centered variables with population standard deviation (denominator $n$).\n   - Exclude any gene with zero variance across cells (population standard deviation equal to $0$) from consideration (both in correlation and subsequent scoring).\n   - Define the marker gene module at threshold $\\tau$ as\n     $$ M(\\tau) \\equiv \\{ g \\in G \\,:\\, r(g^\\ast,g) \\ge \\tau \\ \\text{and}\\ r(g^\\ast,g)  0 \\} \\cup \\{ g^\\ast \\}. $$\n     If any gene in $M(\\tau)$ has zero variance, remove it from $M(\\tau)$. The seed $g^\\ast$ is always included unless its variance is zero; if that happens, include it only for the single-gene baseline and exclude it from the module if needed to avoid division by zero.\n3. Standardization and scoring.\n   - For each gene $g$ with nonzero variance, compute the standardized $z$-score across cells using the population mean and population standard deviation:\n     $$ z_{g,c} \\equiv \\frac{X_{g,c} - \\mu_g}{\\sigma_g}, \\quad \\mu_g \\equiv \\frac{1}{n}\\sum_{c=1}^n X_{g,c}, \\quad \\sigma_g \\equiv \\sqrt{\\frac{1}{n}\\sum_{c=1}^n (X_{g,c} - \\mu_g)^2}. $$\n   - Define the module score for each cell $c$ as the average of standardized expressions over genes in the module:\n     $$ s_c \\equiv \\frac{1}{|M(\\tau)|}\\sum_{g \\in M(\\tau)} z_{g,c}. $$\n   - Define the single-gene score for each cell $c$ using only the seed gene:\n     $$ s_c^{(1)} \\equiv z_{g^\\ast,c}. $$\n4. Classification rule.\n   - Predict the target cell type from the module and from the single gene using the sign of the score:\n     $$ \\widehat{y}^{(M)}_c \\equiv \\mathbb{I}[\\, s_c  0 \\,], \\qquad \\widehat{y}^{(1)}_c \\equiv \\mathbb{I}[\\, s_c^{(1)}  0 \\,], $$\n     where $\\mathbb{I}[\\cdot]$ is the indicator function that equals $1$ when its argument is true and $0$ otherwise.\n   - Count the number of correct predictions in each case:\n     $$ N^{(M)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(M)}_c = y_c \\,], \\qquad N^{(1)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(1)}_c = y_c \\,]. $$\n5. Effect size comparison via pooled standardized difference (Cohen’s $d$). Compute the effect size for both the module scores and the single-gene scores. Let $S = \\{ c : y_c = 1 \\}$ and $T = \\{ c : y_c = 0 \\}$ with sizes $n_S$ and $n_T$. For a score vector $u \\in \\mathbb{R}^n$, let\n   $$ \\overline{u}_S \\equiv \\frac{1}{n_S}\\sum_{c \\in S} u_c, \\quad \\overline{u}_T \\equiv \\frac{1}{n_T}\\sum_{c \\in T} u_c, $$\n   $$ s_S^2 \\equiv \\frac{1}{n_S - 1} \\sum_{c \\in S} (u_c - \\overline{u}_S)^2, \\quad s_T^2 \\equiv \\frac{1}{n_T - 1} \\sum_{c \\in T} (u_c - \\overline{u}_T)^2, $$\n   $$ s_p \\equiv \\sqrt{ \\frac{(n_S - 1)s_S^2 + (n_T - 1)s_T^2}{n_S + n_T - 2} }, \\quad d(u) \\equiv \\frac{\\overline{u}_S - \\overline{u}_T}{s_p}. $$\n   Apply this with $u = s$ (module) and $u = s^{(1)}$ (single gene).\n6. Output specification for each dataset. Report the list\n   $$ \\big[\\, |M(\\tau)|,\\ \\mathrm{round}(d(s^{(1)}), 3),\\ \\mathrm{round}(d(s), 3),\\ N^{(M)} - N^{(1)} \\,\\big], $$\n   where $\\mathrm{round}(\\cdot,3)$ rounds to three decimal places.\n\nTest suite. Implement your solution on the following three datasets. The rows are ordered $(g_1,g_2,g_3,g_4,g_5,g_6)$ and the columns are ordered $(c_1,\\dots,c_n)$. The seed is always $g^\\ast = g_1$. The label vector $y$ is provided for each dataset.\n\n- Dataset A (happy path; strong co-expression module):\n  $$ X^{(A)} = \\begin{bmatrix}\n  10  11  9  10  1  2  1  1 \\\\\n  9  10  8  9  1  1  2  1 \\\\\n  1  1  1  2  9  8  10  9 \\\\\n  1  2  1  1  8  9  9  8 \\\\\n  2  1  2  2  2  2  1  2 \\\\\n  6  7  5  6  1  1  1  2\n  \\end{bmatrix}, \\quad y^{(A)} = [\\,1,1,1,1,0,0,0,0\\,], \\quad \\tau^{(A)} = 0.6. $$\n- Dataset B (boundary condition; threshold forces single-gene module):\n  $$ X^{(B)} = X^{(A)}, \\quad y^{(B)} = y^{(A)}, \\quad \\tau^{(B)} = 1.0. $$\n- Dataset C (dropout edge case; module aids recovery):\n  $$ X^{(C)} = \\begin{bmatrix}\n  0  0  8  1  1  1 \\\\\n  7  6  7  1  1  1 \\\\\n  1  1  1  6  7  6 \\\\\n  1  1  1  5  6  5 \\\\\n  2  2  2  2  2  2 \\\\\n  5  5  6  1  1  1\n  \\end{bmatrix}, \\quad y^{(C)} = [\\,1,1,1,0,0,0\\,], \\quad \\tau^{(C)} = 0.3. $$\n\nFinal output format. Your program should produce a single line of output containing the results for Datasets A, B, and C, in this exact format (with no spaces): a comma-separated list of the three per-dataset result lists, enclosed in square brackets, for example\n$$ \\text{\"[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]\"} $$\nwhere each $a_i$ is an integer and each $b_i,c_i$ is a float rounded to three decimal places and $d_i$ is an integer. There must be no other printed text.\n\nNotes:\n- All mathematical entities (symbols, variables, functions, operators, and numbers) above are written in LaTeX, but your implementation must follow their computational definitions.\n- Angles and physical units are not applicable.", "solution": "We formalize a robust marker gene module by aggregating standardized, co-expressed genes. The reasoning proceeds from basic definitions in molecular biology and statistics to a concrete algorithm.\n\nFundamental basis and motivation. The Central Dogma of Molecular Biology states that genes are transcribed into messenger ribonucleic acid (mRNA), which can be measured to obtain quantitative gene expression. Single-cell measurements yield an expression matrix $X_{g,c}$ where the variability across cells reflects both biological heterogeneity and technical noise. A single marker gene $g^\\ast$ may fail under dropout or subtle shifts, whereas a small set of co-expressed genes can average out noise and amplify the true biological signal. Statistically, if co-expressed genes share signal but have partially independent noise, then averaging their standardized expressions reduces variance approximately in proportion to the module size by an argument akin to the additivity of variance under independence.\n\nCo-expression via correlation. A standard measure of co-expression is the Pearson correlation, which for two gene vectors across cells quantifies linear concordance. Using population standardization (denominator $n$) for both mean and variance yields the population correlation coefficient. Denote centered vectors by $X^\\circ_{g,c} \\equiv X_{g,c} - \\mu_g$, population standard deviation $\\sigma_g \\equiv \\sqrt{\\frac{1}{n} \\sum_c (X_{g,c} - \\mu_g)^2}$, and correlation\n$$ r(g^\\ast,g) \\equiv \\frac{1}{n}\\sum_{c=1}^n \\frac{(X_{g^\\ast,c} - \\mu_{g^\\ast})(X_{g,c} - \\mu_g)}{\\sigma_{g^\\ast}\\sigma_g}. $$\nIf $\\sigma_g = 0$, correlation is undefined and the gene must be excluded from both correlation and subsequent scoring to avoid division by zero.\n\nModule definition. For a threshold $\\tau \\in [0,1]$, define\n$$ M(\\tau) \\equiv \\{ g \\in G \\,:\\, r(g^\\ast,g) \\ge \\tau \\ \\text{and}\\ r(g^\\ast,g)  0 \\} \\cup \\{ g^\\ast \\}, $$\nand remove any gene with zero variance if present. The positivity constraint ensures inclusion of co-upregulated genes and excludes anti-markers. The seed $g^\\ast$ is always included unless it has zero variance; this guarantees at least the single-gene baseline is available.\n\nStandardization and scoring. For each retained gene, compute the population $z$-score\n$$ z_{g,c} \\equiv \\frac{X_{g,c} - \\mu_g}{\\sigma_g}. $$\nThe module score is the average standardized expression\n$$ s_c \\equiv \\frac{1}{|M(\\tau)|}\\sum_{g \\in M(\\tau)} z_{g,c}, $$\nand the single-gene baseline is $s_c^{(1)} \\equiv z_{g^\\ast,c}$. Because each $z_{g,\\cdot}$ has population mean $0$ across cells, the average $s_c$ also has population mean $0$, so a natural, nonparametric threshold for classification is the sign test at $0$:\n$$ \\widehat{y}^{(M)}_c \\equiv \\mathbb{I}[\\, s_c  0 \\,], \\qquad \\widehat{y}^{(1)}_c \\equiv \\mathbb{I}[\\, s_c^{(1)}  0 \\,]. $$\n\nPerformance quantification. We compute two metrics:\n1. The number of correct predictions\n   $$ N^{(M)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(M)}_c = y_c \\,], \\qquad N^{(1)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(1)}_c = y_c \\,]. $$\n   The improvement is $N^{(M)} - N^{(1)}$.\n2. The standardized mean difference (Cohen’s $d$), reflecting separation between target and non-target cells. For any score vector $u \\in \\mathbb{R}^n$, let $S = \\{ c : y_c = 1 \\}$, $T = \\{ c : y_c = 0 \\}$, means $\\overline{u}_S$, $\\overline{u}_T$, sample variances $s_S^2$, $s_T^2$ (denominator $n_S - 1$ and $n_T - 1$ respectively), and pooled standard deviation\n   $$ s_p \\equiv \\sqrt{ \\frac{(n_S - 1)s_S^2 + (n_T - 1)s_T^2}{n_S + n_T - 2} }. $$\n   Then\n   $$ d(u) \\equiv \\frac{\\overline{u}_S - \\overline{u}_T}{s_p}. $$\n   We report $d(s^{(1)})$ for the single-gene baseline and $d(s)$ for the module, each rounded to three decimals.\n\nAlgorithmic steps for each dataset:\n- Compute population means $\\mu_g$ and standard deviations $\\sigma_g$ for each gene; discard zero-variance genes for correlation and scoring.\n- Compute population Pearson correlations $r(g^\\ast,g)$ for all genes $g$ with nonzero variance.\n- Construct $M(\\tau)$ by including $g^\\ast$ and all genes with $r(g^\\ast,g) \\ge \\tau$ and strictly positive.\n- Compute $z_{g,c}$ for $g \\in M(\\tau) \\cup \\{g^\\ast\\}$, then compute $s_c$ and $s_c^{(1)}$, followed by $\\widehat{y}^{(M)}_c$, $\\widehat{y}^{(1)}_c$, $N^{(M)}$, $N^{(1)}$, and $d(s)$, $d(s^{(1)})$.\n\nTest-suite expectations:\n- Dataset A with $\\tau = 0.6$ yields a meaningful module containing multiple positively co-expressed genes with $g^\\ast = g_1$, so $|M(\\tau)|$ is greater than $1$, and both $d(s)$ and classification accuracy are strong.\n- Dataset B uses the same data as A but $\\tau = 1.0$, so only $g^\\ast$ meets the threshold (exact correlation of $1$ is not attained by other genes), hence $|M(\\tau)| = 1$, $d(s) = d(s^{(1)})$, and $N^{(M)} - N^{(1)} = 0$.\n- Dataset C includes dropout for $g^\\ast$ in some target cells, while co-expressed partners remain high. With $\\tau = 0.3$, at least two co-expressed genes join the module; averaging $z$-scores recovers target cells, improving both $d(s)$ and $N^{(M)} - N^{(1)}$ relative to the single-gene baseline.\n\nThe program implements these steps exactly and prints a single line with three lists\n$$ \\big[\\, |M(\\tau)|,\\ \\mathrm{round}(d(s^{(1)}), 3),\\ \\mathrm{round}(d(s), 3),\\ N^{(M)} - N^{(1)} \\,\\big] $$\nfor Datasets A, B, and C, respectively, concatenated as a comma-separated list with no spaces as required.", "answer": "```python\nimport numpy as np\n\ndef population_mean_std(X, axis):\n    \"\"\"\n    Compute population mean and population standard deviation (ddof=0).\n    Returns (mean, std).\n    \"\"\"\n    mu = np.mean(X, axis=axis)\n    sigma = np.std(X, axis=axis, ddof=0)\n    return mu, sigma\n\ndef population_pearson_corr(x, Y):\n    \"\"\"\n    Compute population Pearson correlation between a 1D vector x (length n)\n    and each row of 2D array Y (m x n). Uses population std (ddof=0).\n    Returns a 1D array of length m with correlations; returns 0.0 for zero-variance rows.\n    \"\"\"\n    # Center x and Y\n    x = np.asarray(x, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    n = x.shape[0]\n    x_mu = np.mean(x)\n    x_std = np.std(x, ddof=0)\n    # Handle zero variance in x (rare by construction)\n    if x_std == 0:\n        return np.zeros(Y.shape[0], dtype=float)\n    Xc = x - x_mu\n    Y_mu = np.mean(Y, axis=1)\n    Y_std = np.std(Y, axis=1, ddof=0)\n    Yc = Y - Y_mu[:, None]\n    # Compute covariance: mean of product of centered variables\n    cov = (Yc @ Xc) / n  # shape (m,)\n    # Avoid division by zero: where Y_std==0 set corr to 0\n    denom = x_std * Y_std\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r = np.where(denom > 0, cov / denom, 0.0)\n    return r\n\ndef build_module(X, seed_idx, tau):\n    \"\"\"\n    Build marker gene module M(tau) around seed gene index seed_idx with threshold tau.\n    Exclude genes with zero population variance from module and correlation.\n    Only include positively correlated genes with r >= tau.\n    Always include the seed gene unless its variance is zero.\n    Returns a sorted list of unique gene indices in the module.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    p, n = X.shape\n    # Compute per-gene population std to filter zero-variance genes\n    mu_g = np.mean(X, axis=1)\n    std_g = np.std(X, axis=1, ddof=0)\n    nonzero_var = std_g > 0\n    # Compute correlations for eligible genes\n    r = population_pearson_corr(X[seed_idx, :], X)\n    module = set()\n    # Include genes with positive correlation >= tau and nonzero variance\n    for g in range(p):\n        if nonzero_var[g] and (g == seed_idx or (r[g] > 0 and r[g] >= tau)):\n            module.add(g)\n    # Ensure seed inclusion if it has nonzero variance; if zero, exclude to avoid div-by-zero\n    if nonzero_var[seed_idx]:\n        module.add(seed_idx)\n    else:\n        module.discard(seed_idx)\n    # Remove any zero-variance genes (safety)\n    module = [g for g in module if nonzero_var[g]]\n    module.sort()\n    return module\n\ndef z_scores(X):\n    \"\"\"\n    Compute population z-scores per gene across cells.\n    X is (p x n). Returns Z of same shape.\n    For zero-variance genes, set z-scores to 0 (they will be excluded upstream anyway).\n    \"\"\"\n    mu, sigma = population_mean_std(X, axis=1)\n    # Avoid division by zero: where sigma==0, set to 1 to produce zeros\n    sigma_safe = np.where(sigma == 0, 1.0, sigma)\n    Z = (X - mu[:, None]) / sigma_safe[:, None]\n    return Z, mu, sigma\n\ndef cohen_d(u, y):\n    \"\"\"\n    Compute Cohen's d between groups y==1 and y==0 using pooled sample standard deviation.\n    u: 1D array of scores length n\n    y: 1D array of 0/1 labels length n\n    Returns float d. Assumes both groups have at least 2 samples and nonzero pooled variance.\n    \"\"\"\n    u = np.asarray(u, dtype=float)\n    y = np.asarray(y, dtype=int)\n    S = (y == 1)\n    T = (y == 0)\n    uS = u[S]\n    uT = u[T]\n    nS = uS.size\n    nT = uT.size\n    mS = np.mean(uS) if nS > 0 else 0.0\n    mT = np.mean(uT) if nT > 0 else 0.0\n    # Sample variances\n    sS2 = np.var(uS, ddof=1) if nS > 1 else 0.0\n    sT2 = np.var(uT, ddof=1) if nT > 1 else 0.0\n    # Pooled standard deviation\n    denom_df = (nS + nT - 2)\n    if denom_df = 0:\n        sp = 0.0\n    else:\n        sp = np.sqrt(((nS - 1) * sS2 + (nT - 1) * sT2) / denom_df)\n    if sp == 0.0:\n        return 0.0\n    return (mS - mT) / sp\n\ndef evaluate_dataset(X, y, seed_idx, tau):\n    \"\"\"\n    For a dataset (X, y), seed gene index, and threshold tau:\n    - Build module\n    - Compute z-scores\n    - Compute module score and single-gene score\n    - Classify with threshold > 0\n    - Count correct predictions\n    - Compute Cohen's d for both scores\n    Returns [module_size, d_single_rounded, d_module_rounded, improvement_int]\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=int)\n    p, n = X.shape\n    module = build_module(X, seed_idx, tau)\n    Z, _, _ = z_scores(X)\n    # Single-gene score\n    s_single = Z[seed_idx, :]\n    # Module score: average across genes in module\n    if len(module) == 0:\n        s_module = s_single.copy()\n    else:\n        s_module = np.mean(Z[module, :], axis=0)\n    # Predictions with threshold > 0\n    yhat_single = (s_single > 0).astype(int)\n    yhat_module = (s_module > 0).astype(int)\n    correct_single = int(np.sum(yhat_single == y))\n    correct_module = int(np.sum(yhat_module == y))\n    # Effect sizes\n    d_single = cohen_d(s_single, y)\n    d_module = cohen_d(s_module, y)\n    # Round to 3 decimals for reporting\n    d_single_r = round(float(d_single), 3)\n    d_module_r = round(float(d_module), 3)\n    improvement = int(correct_module - correct_single)\n    return [len(module), d_single_r, d_module_r, improvement]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Dataset A\n    XA = np.array([\n        [10, 11,  9, 10, 1, 2, 1, 1],\n        [ 9, 10,  8,  9, 1, 1, 2, 1],\n        [ 1,  1,  1,  2, 9, 8,10, 9],\n        [ 1,  2,  1,  1, 8, 9, 9, 8],\n        [ 2,  1,  2,  2, 2, 2, 1, 2],\n        [ 6,  7,  5,  6, 1, 1, 1, 2]\n    ], dtype=float)\n    yA = np.array([1,1,1,1,0,0,0,0], dtype=int)\n    seed_idx = 0  # g1\n    tauA = 0.6\n\n    # Dataset B (same X and y, higher tau)\n    XB = XA.copy()\n    yB = yA.copy()\n    tauB = 1.0\n\n    # Dataset C\n    XC = np.array([\n        [0, 0, 8, 1, 1, 1],\n        [7, 6, 7, 1, 1, 1],\n        [1, 1, 1, 6, 7, 6],\n        [1, 1, 1, 5, 6, 5],\n        [2, 2, 2, 2, 2, 2],\n        [5, 5, 6, 1, 1, 1]\n    ], dtype=float)\n    yC = np.array([1,1,1,0,0,0], dtype=int)\n    tauC = 0.3\n\n    test_cases = [\n        (XA, yA, seed_idx, tauA),\n        (XB, yB, seed_idx, tauB),\n        (XC, yC, seed_idx, tauC),\n    ]\n\n    results = []\n    for X, y, seed, tau in test_cases:\n        res = evaluate_dataset(X, y, seed, tau)\n        results.append(res)\n\n    # Final print statement in the exact required format (no spaces).\n    # Example: [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]]\n    # Convert to string without spaces.\n    def to_str_list(lst):\n        return \"[\" + \",\".join(str(x) for x in lst) + \"]\"\n    output = \"[\" + \",\".join(to_str_list(r) for r in results) + \"]\"\n    print(output)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2371687"}]}