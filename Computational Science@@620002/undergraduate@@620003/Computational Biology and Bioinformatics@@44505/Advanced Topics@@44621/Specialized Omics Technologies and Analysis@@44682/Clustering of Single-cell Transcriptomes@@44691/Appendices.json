{"hands_on_practices": [{"introduction": "The journey of clustering single-cell data begins with a crucial pre-processing step: data transformation. This practice explores how a common transformation, the logarithm with a pseudo-count, influences the final clustering outcome. By investigating how different pseudo-count values alter the geometric distances between cells, you will gain a deeper intuition for why understanding the interplay between your data's properties and your analytical choices is fundamental to sound scientific conclusions. [@problem_id:2379581]", "problem": "You are given a family of element-wise transformations for nonnegative integer count matrices that model single-cell messenger Ribonucleic Acid (mRNA) transcript counts, defined for any pseudo-count parameter $\\alpha \\in \\mathbb{R}_{>0}$ by $T_{\\alpha}(X) = \\log(X + \\alpha)$, where $\\log$ denotes the natural logarithm and $X$ is a matrix of counts. For any two transformed cell-by-gene matrices $T_{\\alpha}(C) \\in \\mathbb{R}^{n \\times d}$, define the pairwise dissimilarity between cells as the Euclidean distance between their corresponding rows. For a fixed positive integer $K$, define the clustering $\\mathcal{K}_{\\alpha}(C,K)$ to be the partition of the $n$ cells into exactly $K$ nonempty clusters obtained by applying agglomerative hierarchical clustering with average linkage to the full pairwise Euclidean distance matrix of $T_{\\alpha}(C)$ and then cutting the resulting dendrogram to yield exactly $K$ clusters. For any two partitions $U$ and $V$ of the same $n$ items, define the Adjusted Rand Index (ARI) between $U$ and $V$ by\n$$\n\\mathrm{ARI}(U,V) \\;=\\; \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} \\;-\\; \\frac{\\left(\\sum_{i} \\binom{a_i}{2}\\right)\\left(\\sum_{j} \\binom{b_j}{2}\\right)}{\\binom{n}{2}}}{\\tfrac{1}{2}\\left(\\sum_{i} \\binom{a_i}{2} + \\sum_{j} \\binom{b_j}{2}\\right) \\;-\\; \\frac{\\left(\\sum_{i} \\binom{a_i}{2}\\right)\\left(\\sum_{j} \\binom{b_j}{2}\\right)}{\\binom{n}{2}}},\n$$\nwhere $n_{ij}$ is the number of items placed in cluster $i$ of partition $U$ and cluster $j$ of partition $V$, $a_i = \\sum_j n_{ij}$, $b_j = \\sum_i n_{ij}$, and $\\binom{m}{2} = \\frac{m(m-1)}{2}$ for any integer $m \\ge 0$. In any degenerate case where the denominator is $0$ and the numerator is also $0$, define $\\mathrm{ARI}(U,V)=1$.\n\nInvestigate the effect of different pseudo-count values on the final clustering by computing, for each test case below, the Adjusted Rand Index between $\\mathcal{K}_{\\alpha_1}(C,K)$ and $\\mathcal{K}_{\\alpha_2}(C,K)$, with $\\alpha_1 = 1$ and $\\alpha_2 = 0.01$.\n\nTest suite specification. For each case, $C$ is an $n \\times d$ matrix whose rows are listed explicitly in order; all entries are nonnegative integers. Each line shows one row (cell) as a list of $d$ integers.\n\n- Test case $1$: $C^{(1)} \\in \\mathbb{N}_0^{8 \\times 6}$, $K=2$, rows in order:\n  $[\\,1,1,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0\\,]$,\n  $[\\,1,1,1,0,0,0\\,]$,\n  $[\\,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,1,1,1\\,]$,\n  $[\\,0,0,0,0,1,1\\,]$.\n- Test case $2$: $C^{(2)} \\in \\mathbb{N}_0^{6 \\times 5}$, $K=2$, rows in order:\n  $[\\,0,0,0,0,0\\,]$,\n  $[\\,1,0,0,0,0\\,]$,\n  $[\\,1,0,0,0,0\\,]$,\n  $[\\,0,1,0,0,0\\,]$,\n  $[\\,0,1,0,0,0\\,]$,\n  $[\\,0,0,1,0,0\\,]$.\n- Test case $3$: $C^{(3)} \\in \\mathbb{N}_0^{9 \\times 7}$, $K=3$, rows in order:\n  $[\\,1,1,0,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0,0\\,]$,\n  $[\\,0,0,1,1,0,0,0\\,]$,\n  $[\\,0,0,1,1,0,0,0\\,]$,\n  $[\\,0,0,1,1,0,0,0\\,]$,\n  $[\\,0,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,0,0,1,1\\,]$.\n- Test case $4$: $C^{(4)} \\in \\mathbb{N}_0^{5 \\times 4}$, $K=2$, rows in order:\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$.\n\nYour program must compute, for each test case $t \\in \\{1,2,3,4\\}$, the single real number $\\mathrm{ARI}\\!\\left(\\mathcal{K}_{\\alpha_1}\\!\\left(C^{(t)},K\\right), \\mathcal{K}_{\\alpha_2}\\!\\left(C^{(t)},K\\right)\\right)$ and aggregate the four results into one output line. The final output format must be a single line containing a Python-style list of the four results as decimal numbers rounded to exactly $6$ digits after the decimal point, in the order of the test cases, for example $[\\,a_1,a_2,a_3,a_4\\,]$ where each $a_t$ is the rounded value for test case $t$. No other text should be printed.", "solution": "The problem requires an investigation into the effect of the pseudo-count parameter, $\\alpha$, on the outcome of a specific clustering pipeline applied to single-cell count data. The pipeline consists of a logarithmic transformation, calculation of pairwise Euclidean distances, and agglomerative hierarchical clustering with average linkage. The comparison between clusterings obtained with different $\\alpha$ values is to be quantified using the Adjusted Rand Index (ARI).\n\nBefore proceeding to computation, a theoretical analysis of the problem is mandatory. Let us formalize the components.\n\nThe data consists of a count matrix $C \\in \\mathbb{N}_0^{n \\times d}$, where $n$ is the number of cells and $d$ is the number of genes. The transformation is given by $T_{\\alpha}(C)$, where each element $x_{ij}$ of $C$ is replaced by $\\log(x_{ij} + \\alpha)$ for a given $\\alpha > 0$. Let $u, v \\in \\mathbb{N}_0^d$ be the count vectors for two distinct cells (rows of $C$). After transformation, their new representations are $u' = (\\log(u_1+\\alpha), \\dots, \\log(u_d+\\alpha))$ and $v' = (\\log(v_1+\\alpha), \\dots, \\log(v_d+\\alpha))$.\n\nThe dissimilarity is the Euclidean distance between these transformed vectors:\n$$\nd_{\\alpha}(u,v) = \\sqrt{\\sum_{j=1}^{d} (\\log(u_j+\\alpha) - \\log(v_j+\\alpha))^2} = \\sqrt{\\sum_{j=1}^{d} \\left(\\log\\left(\\frac{u_j+\\alpha}{v_j+\\alpha}\\right)\\right)^2}\n$$\n\nA critical observation must be made regarding the data provided in all four test cases. The count matrices $C^{(1)}, C^{(2)}, C^{(3)}, C^{(4)}$ contain exclusively binary values, i.e., for all $i,j$, the entry $c_{ij} \\in \\{0, 1\\}$. This profoundly simplifies the analysis.\n\nFor any two binary vectors $u, v \\in \\{0, 1\\}^d$, let us analyze the terms in the summation for the squared distance $d_{\\alpha}(u,v)^2$. For each gene $j \\in \\{1, \\dots, d\\}$, there are four possibilities for the pair $(u_j, v_j)$:\n1. If $u_j = 0$ and $v_j = 0$, the contribution to the sum is $\\left(\\log\\left(\\frac{0+\\alpha}{0+\\alpha}\\right)\\right)^2 = (\\log(1))^2 = 0$.\n2. If $u_j = 1$ and $v_j = 1$, the contribution is $\\left(\\log\\left(\\frac{1+\\alpha}{1+\\alpha}\\right)\\right)^2 = (\\log(1))^2 = 0$.\n3. If $u_j = 1$ and $v_j = 0$, the contribution is $\\left(\\log\\left(\\frac{1+\\alpha}{0+\\alpha}\\right)\\right)^2 = \\left(\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2$.\n4. If $u_j = 0$ and $v_j = 1$, the contribution is $\\left(\\log\\left(\\frac{0+\\alpha}{1+\\alpha}\\right)\\right)^2 = \\left(-\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2 = \\left(\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2$.\n\nThe total squared distance is the sum of these contributions. The only non-zero contributions come from genes where the counts differ. Let $H(u,v)$ be the Hamming distance between the vectors $u$ and $v$, which is the number of positions $j$ at which $u_j \\neq v_j$. The squared Euclidean distance is thus:\n$$\nd_{\\alpha}(u,v)^2 = H(u,v) \\cdot \\left(\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2\n$$\nTaking the square root, the distance is:\n$$\nd_{\\alpha}(u,v) = \\sqrt{H(u,v)} \\cdot \\left|\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right|\n$$\nSince $\\alpha > 0$, it follows that $1+\\alpha > \\alpha$, so $\\frac{1+\\alpha}{\\alpha} > 1$, and $\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right) > 0$. Let us define a scaling factor $S_{\\alpha} = \\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)$. The distance is then $d_{\\alpha}(u,v) = S_{\\alpha} \\sqrt{H(u,v)}$.\n\nThe entire matrix of pairwise distances for a given $\\alpha$ is simply the matrix of square roots of Hamming distances, scaled by a constant factor $S_{\\alpha}$.\n\nThe clustering algorithm is agglomerative hierarchical clustering with average linkage. At each step, this algorithm merges the two closest clusters. The distance between two clusters $C_A$ and $C_B$ is defined as the average of all pairwise distances between their constituent points:\n$$\nD_{\\alpha}(C_A, C_B) = \\frac{1}{|C_A||C_B|} \\sum_{u \\in C_A, v \\in C_B} d_{\\alpha}(u,v)\n$$\nSubstituting our expression for $d_{\\alpha}(u,v)$:\n$$\nD_{\\alpha}(C_A, C_B) = \\frac{1}{|C_A||C_B|} \\sum_{u \\in C_A, v \\in C_B} S_{\\alpha} \\sqrt{H(u,v)} = S_{\\alpha} \\left( \\frac{1}{|C_A||C_B|} \\sum_{u \\in C_A, v \\in C_B} \\sqrt{H(u,v)} \\right)\n$$\nThe term in parentheses is independent of $\\alpha$. Thus, the distance between any two clusters is also scaled by the same positive constant $S_{\\alpha}$.\n\nThe hierarchical clustering algorithm proceeds by comparing inter-cluster distances to find the minimum. If we compare the distances between two pairs of clusters, say $(C_A, C_B)$ and $(C_X, C_Y)$, we have:\n$$\nD_{\\alpha}(C_A, C_B) < D_{\\alpha}(C_X, C_Y) \\iff S_{\\alpha} \\cdot (\\text{term}_1) < S_{\\alpha} \\cdot (\\text{term}_2)\n$$\nSince $S_{\\alpha} > 0$, this is equivalent to $(\\text{term}_1) < (\\text{term}_2)$. The decision of which clusters to merge at any given step is therefore independent of the value of $\\alpha$.\n\nThis implies that the sequence of merges, and thus the entire structure of the resulting dendrogram, is identical for any choice of $\\alpha > 0$, provided the input data is binary. Cutting the dendrogram to yield exactly $K$ clusters will produce the same partition of cells regardless of $\\alpha$.\n\nTherefore, for each test case $t \\in \\{1,2,3,4\\}$, since the input matrix $C^{(t)}$ is binary, the partitions obtained are identical:\n$$\n\\mathcal{K}_{\\alpha_1}(C^{(t)}, K) = \\mathcal{K}_{\\alpha_2}(C^{(t)}, K)\n$$\nwhere $\\alpha_1 = 1$ and $\\alpha_2 = 0.01$.\n\nThe final step is to compute the Adjusted Rand Index between two identical partitions. Let the two partitions be $U$ and $V$, with $U=V$. In this case, the contingency table $n_{ij}$ becomes a diagonal matrix (after appropriate ordering of clusters). The ARI formula simplifies to yield a value of $1$, which signifies perfect agreement. Specifically, $\\sum_{i,j} \\binom{n_{ij}}{2} = \\sum_i \\binom{a_i}{2}$ and $\\sum_i \\binom{a_i}{2} = \\sum_j \\binom{b_j}{2}$. This leads to the numerator and denominator of the ARI formula being equal (and non-zero, unless the partition is trivial), resulting in $\\mathrm{ARI}(U,U) = 1$. In the trivial case where all clusters are singletons, the numerator and denominator both become $0$, and the problem statement specifies to define $\\mathrm{ARI}=1$ in this scenario.\n\nConclusion: For all four test cases provided, the theoretical result for the Adjusted Rand Index is exactly $1$. The following computational solution will verify this deduction by explicitly performing all required steps.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases as specified.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"C\": np.array([\n                [1, 1, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0],\n                [1, 1, 1, 0, 0, 0],\n                [0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 1, 1]\n            ]),\n            \"K\": 2\n        },\n        {\n            \"C\": np.array([\n                [0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 1, 0, 0, 0],\n                [0, 1, 0, 0, 0],\n                [0, 0, 1, 0, 0]\n            ]),\n            \"K\": 2\n        },\n        {\n            \"C\": np.array([\n                [1, 1, 0, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1]\n            ]),\n            \"K\": 3\n        },\n        {\n            \"C\": np.array([\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0]\n            ]),\n            \"K\": 2\n        }\n    ]\n\n    alpha1 = 1.0\n    alpha2 = 0.01\n\n    results = []\n    for case in test_cases:\n        C = case[\"C\"]\n        K = case[\"K\"]\n\n        # Get clustering for alpha1\n        labels1 = get_clustering(C, alpha1, K)\n        \n        # Get clustering for alpha2\n        labels2 = get_clustering(C, alpha2, K)\n        \n        # Compute ARI\n        ari = adjusted_rand_index(labels1, labels2)\n        results.append(ari)\n\n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_clustering(C, alpha, K):\n    \"\"\"\n    Performs the specified clustering pipeline for a given count matrix,\n    pseudo-count, and number of clusters.\n    \"\"\"\n    # 1. Transform data: T_alpha(C) = log(C + alpha)\n    transformed_C = np.log(C + alpha)\n    \n    # 2. Compute pairwise Euclidean distance matrix\n    # pdist computes a condensed distance matrix (upper triangle)\n    dist_matrix = pdist(transformed_C, 'euclidean')\n    \n    # 3. Perform agglomerative hierarchical clustering with average linkage\n    # 'average' linkage corresponds to UPGMA\n    linkage_matrix = linkage(dist_matrix, method='average')\n    \n    # 4. Cut dendrogram to get K clusters\n    # 'maxclust' criterion forms flat clusters from the linkage matrix\n    cluster_labels = fcluster(linkage_matrix, K, criterion='maxclust')\n    \n    return cluster_labels\n\ndef comb2(n):\n    \"\"\"\n    Computes nC2 = n * (n - 1) / 2.\n    Handles arrays element-wise.\n    \"\"\"\n    # Ensure input is an array of integers and handle n < 2 case\n    n = np.asarray(n, dtype=np.int64)\n    return n * (n - 1) / 2\n\ndef adjusted_rand_index(labels_true, labels_pred):\n    \"\"\"\n    Computes the Adjusted Rand Index from scratch based on the provided formula.\n    \"\"\"\n    n = len(labels_true)\n    if n < 2:\n        return 1.0\n\n    # Create contingency table\n    unique_labels_true = np.unique(labels_true)\n    unique_labels_pred = np.unique(labels_pred)\n    \n    contingency_table = np.zeros((len(unique_labels_true), len(unique_labels_pred)), dtype=int)\n    \n    true_map = {label: i for i, label in enumerate(unique_labels_true)}\n    pred_map = {label: i for i, label in enumerate(unique_labels_pred)}\n\n    for i in range(n):\n        true_idx = true_map[labels_true[i]]\n        pred_idx = pred_map[labels_pred[i]]\n        contingency_table[true_idx, pred_idx] += 1\n    \n    # Sum of n_ij choose 2\n    sum_nij_c2 = np.sum(comb2(contingency_table))\n\n    # Sums of row totals and column totals choose 2\n    a = np.sum(contingency_table, axis=1)\n    b = np.sum(contingency_table, axis=0)\n    \n    sum_a_c2 = np.sum(comb2(a))\n    sum_b_c2 = np.sum(comb2(b))\n\n    # Total number of pairs\n    n_c2 = comb2(n)\n    \n    # Calculate expected index\n    # Handle case where n < 2, so n_c2 is 0\n    if n_c2 == 0:\n        expected_index = 0\n    else:\n        expected_index = (sum_a_c2 * sum_b_c2) / n_c2\n\n    # Calculate max index\n    max_index = 0.5 * (sum_a_c2 + sum_b_c2)\n    \n    # Calculate ARI\n    numerator = sum_nij_c2 - expected_index\n    denominator = max_index - expected_index\n    \n    if numerator == 0 and denominator == 0:\n        # Per problem specification for this degenerate case\n        return 1.0\n    elif denominator == 0:\n        # This implies no agreement above chance\n        return 0.0\n    else:\n        return numerator / denominator\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2379581"}, {"introduction": "Once a clustering algorithm has assigned each cell to a cluster, a critical next step is to evaluate the confidence of these assignments. This exercise challenges you to develop and apply a 'confidence score' that quantifies how securely a cell belongs to its assigned cluster versus the next-closest alternative. By formalizing this geometric concept from first principles, you will learn to move beyond binary assignments and appreciate the continuous nature of cell-state landscapes. [@problem_id:2379661]", "problem": "You are analyzing a single-cell ribonucleic acid sequencing dataset of single-cell transcriptomes that has been embedded into a two-dimensional space using Principal Component Analysis (PCA). Clustering has been performed in this embedding, and each cluster is represented by a centroid. For a given cell at coordinate $\\left(2,-1\\right)$ and three cluster centroids at $\\mu_{A}=\\left(1,0\\right)$, $\\mu_{B}=\\left(4,-2\\right)$, and $\\mu_{C}=\\left(-1,3\\right)$, distances are measured in this space by the Euclidean norm, that is, for a cell at $x \\in \\mathbb{R}^{2}$ and a centroid at $\\mu \\in \\mathbb{R}^{2}$, the distance is $d\\left(x,\\mu\\right)=\\left\\|x-\\mu\\right\\|_{2}$.\n\nDefine the assigned cluster for the cell to be the one whose centroid is at the smallest Euclidean distance from the cell. Let $d_{1}$ be the distance from the cell to the assigned centroid and $d_{2}$ be the distance from the cell to the next-closest centroid. You are tasked with developing a dimensionless confidence score $c\\left(d_{1},d_{2}\\right)$ for the cell’s cluster assignment that satisfies all of the following principles:\n\n- The score is invariant under any uniform scaling of the embedding coordinates, so it depends on $\\left(d_{1},d_{2}\\right)$ only through the ratio $r=d_{1}/d_{2}$.\n- On the domain $r \\in \\left[0,1\\right]$ (noting that $d_{1} \\leq d_{2}$ by definition), the score is a continuous, strictly decreasing function of $r$.\n- The score attains $c=1$ when $r=0$ and $c=0$ when $r=1$.\n- The dependence on $r$ is affine.\n\nUsing only these principles, first derive a closed-form expression for $c\\left(d_{1},d_{2}\\right)$ in terms of $d_{1}$ and $d_{2}$. Then, using the provided coordinates, compute the numerical value of the confidence score for the given cell. Round your answer to four significant figures. Express the final answer as a decimal with no units.", "solution": "The problem statement is first subjected to validation.\n\nGivens are extracted verbatim:\n- A cell is at coordinate $\\left(2,-1\\right)$.\n- Three cluster centroids are at $\\mu_{A}=\\left(1,0\\right)$, $\\mu_{B}=\\left(4,-2\\right)$, and $\\mu_{C}=\\left(-1,3\\right)$.\n- The space is a two-dimensional embedding.\n- Distance is the Euclidean norm: for a cell at $x \\in \\mathbb{R}^{2}$ and a centroid at $\\mu \\in \\mathbb{R}^{2}$, the distance is $d\\left(x,\\mu\\right)=\\left\\|x-\\mu\\right\\|_{2}$.\n- The assigned cluster is the one with the centroid at the smallest Euclidean distance.\n- $d_{1}$ is the distance from the cell to the assigned centroid.\n- $d_{2}$ is the distance from the cell to the next-closest centroid.\n- A dimensionless confidence score $c\\left(d_{1},d_{2}\\right)$ must be derived.\n- Principles for the score:\n  1. Invariant under uniform scaling, depending only on the ratio $r=d_{1}/d_{2}$.\n  2. On the domain $r \\in \\left[0,1\\right]$, the score is a continuous, strictly decreasing function of $r$.\n  3. The score attains $c=1$ when $r=0$.\n  4. The score attains $c=0$ when $r=1$.\n  5. The dependence on $r$ is affine.\n\nValidation is performed.\n- Scientific Groundedness: The problem is a standard exercise in computational biology, specifically relating to the analysis of single-cell transcriptome clustering. The concepts of PCA embedding, Euclidean distance for clustering, and defining a confidence score for cluster assignment are all well-established and scientifically sound.\n- Well-Posedness: The problem is well-posed. It provides all necessary data and defines a set of consistent constraints that uniquely determine the function for the confidence score.\n- Objectivity: The problem is stated in precise mathematical terms, free of subjective or ambiguous language.\n\nThe verdict is that the problem is valid. We may now proceed to the solution.\n\nThe solution requires two steps: first, to derive the general form of the confidence score $c\\left(d_{1},d_{2}\\right)$ based on the given principles, and second, to compute its numerical value for the specified cell and centroids.\n\nPart 1: Derivation of the confidence score formula.\nThe problem states that the dependence of the score $c$ on the ratio $r = d_{1}/d_{2}$ is affine. An affine function is a linear function of the form:\n$$c(r) = m \\cdot r + b$$\nwhere $m$ and $b$ are constants to be determined.\n\nThe problem provides two boundary conditions for this function:\n1. $c=1$ when $r=0$.\n2. $c=0$ when $r=1$.\n\nWe substitute these conditions into the affine equation.\nFor the first condition ($r=0$):\n$$c(0) = m \\cdot 0 + b = 1$$\nThis immediately yields $b=1$.\n\nFor the second condition ($r=1$):\n$$c(1) = m \\cdot 1 + b = 0$$\nSubstituting the value $b=1$ into this equation gives:\n$$m + 1 = 0$$\nwhich implies $m=-1$.\n\nWith $m=-1$ and $b=1$, the affine function for the confidence score is:\n$$c(r) = 1 - r$$\n\nWe must verify that this function satisfies the remaining principles.\n- The score depends only on $r$, so it is invariant under uniform scaling of coordinates. If coordinates are scaled by a factor $\\alpha > 0$, distances are also scaled by $\\alpha$, so $d'_{1} = \\alpha d_{1}$ and $d'_{2} = \\alpha d_{2}$. The new ratio is $r' = d'_{1}/d'_{2} = (\\alpha d_{1})/(\\alpha d_{2}) = d_{1}/d_{2} = r$, leaving the score unchanged. This is satisfied.\n- The function $c(r)=1-r$ is continuous for all $r$. Its derivative with respect to $r$ is $c'(r)=-1$, which is strictly negative. Therefore, $c(r)$ is a strictly decreasing function of $r$ on its domain $r \\in \\left[0,1\\right]$. This is satisfied.\n\nAll principles are satisfied by the derived function. Substituting $r = d_{1}/d_{2}$ gives the final expression for the confidence score in terms of $d_1$ and $d_2$:\n$$c\\left(d_{1},d_{2}\\right) = 1 - \\frac{d_{1}}{d_{2}}$$\n\nPart 2: Calculation of the confidence score for the given cell.\nThe cell is located at the point $x = \\left(2,-1\\right)$. The centroids are $\\mu_{A}=\\left(1,0\\right)$, $\\mu_{B}=\\left(4,-2\\right)$, and $\\mu_{C}=\\left(-1,3\\right)$. We must compute the Euclidean distance from the cell to each centroid. For computational convenience, we first calculate the squared distances.\n\nThe squared distance to centroid A is:\n$$d(x, \\mu_{A})^{2} = (2-1)^{2} + (-1-0)^{2} = 1^{2} + (-1)^{2} = 1 + 1 = 2$$\nThe squared distance to centroid B is:\n$$d(x, \\mu_{B})^{2} = (2-4)^{2} + (-1 - (-2))^{2} = (-2)^{2} + (1)^{2} = 4 + 1 = 5$$\nThe squared distance to centroid C is:\n$$d(x, \\mu_{C})^{2} = (2 - (-1))^{2} + (-1-3)^{2} = 3^{2} + (-4)^{2} = 9 + 16 = 25$$\n\nThe distances are the square roots of these values:\n$$d_{A} = d(x, \\mu_{A}) = \\sqrt{2}$$\n$$d_{B} = d(x, \\mu_{B}) = \\sqrt{5}$$\n$$d_{C} = d(x, \\mu_{C}) = \\sqrt{25} = 5$$\n\nBy definition, the assigned cluster is the one corresponding to the minimum distance. Comparing the distances: $\\sqrt{2} \\approx 1.414$, $\\sqrt{5} \\approx 2.236$, and $5$.\nThe smallest distance is $d_{A} = \\sqrt{2}$. Therefore, the cell is assigned to cluster A.\nThis means $d_{1} = d_{A} = \\sqrt{2}$.\n\nThe next-closest centroid is the one corresponding to the second-smallest distance. The second-smallest distance is $d_{B} = \\sqrt{5}$.\nThis means $d_{2} = d_{B} = \\sqrt{5}$.\n\nNow we can compute the confidence score using the derived formula:\n$$c = 1 - \\frac{d_{1}}{d_{2}} = 1 - \\frac{\\sqrt{2}}{\\sqrt{5}} = 1 - \\sqrt{\\frac{2}{5}}$$\n\nTo provide the numerical answer, we calculate this value:\n$$c = 1 - \\sqrt{0.4} \\approx 1 - 0.632455532... \\approx 0.367544467...$$\n\nThe problem requires rounding the answer to four significant figures. The first significant figure is $3$, so we need to keep three digits after it. The fifth significant figure is $4$, which is less than $5$, so we round down.\n$$c \\approx 0.3675$$\nThis is the final numerical value of the confidence score.", "answer": "$$\\boxed{0.3675}$$", "id": "2379661"}, {"introduction": "Clustering algorithms, particularly at high resolution, can often lead to 'over-clustering,' where biologically homogeneous cell populations are artificially split. This advanced practice guides you through designing an automated procedure to correct this by merging clusters that lack statistically significant evidence of being distinct. You will implement a powerful workflow that combines differential gene expression analysis with graph-based merging, providing a data-driven method for refining cluster annotations and achieving more biologically meaningful results. [@problem_id:2379646]", "problem": "You are given a gene-by-cell count matrix and an initial set of cluster labels for single-cell ribonucleic acid sequencing (scRNA-seq) transcriptomes. The goal is to design and implement a principled procedure that automatically merges over-clustered populations by testing whether pairs of clusters lack statistically significant separating marker genes. The method must be based on core definitions and well-tested statistical procedures commonly used in single-cell transcriptomics.\n\nStart from the following foundations:\n- A cell-by-gene count matrix is a nonnegative integer array of shape $n \\times g$, where $n$ is the number of cells and $g$ is the number of genes. Denote the matrix by $X \\in \\mathbb{N}_{0}^{n \\times g}$, with entries $X_{i j}$ representing the count for cell $i$ and gene $j$.\n- Library-size normalization rescales each cell so that total counts per cell are comparable. A standard approach is to compute for each cell $i$ the total count $s_{i} = \\sum_{j=1}^{g} X_{i j}$, and then transform counts to counts-per-$10^{4}$ followed by natural logarithm stabilization, i.e., define\n$$\n\\tilde{X}_{i j} = \\log\\left(1 + \\frac{X_{i j}}{s_{i}} \\cdot 10^{4}\\right),\n$$\nfor all cells $i \\in \\{1,\\dots,n\\}$ and genes $j \\in \\{1,\\dots,g\\}$, where $\\log$ denotes the natural logarithm.\n- For two clusters $a$ and $b$, let $A$ be the index set of cells with label $a$ and $B$ the index set of cells with label $b$. For each gene $j \\in \\{1,\\dots,g\\}$, consider the two-sided Mann–Whitney $U$ test (also known as the Wilcoxon rank-sum test) applied to the samples $\\{\\tilde{X}_{i j} : i \\in A\\}$ and $\\{\\tilde{X}_{i j} : i \\in B\\}$, yielding a $p$-value $p_{j} \\in [0,1]$. Let the absolute log-fold-change per gene be\n$$\nL_{j} = \\left|\\frac{1}{|A|} \\sum_{i \\in A} \\tilde{X}_{i j} - \\frac{1}{|B|} \\sum_{i \\in B} \\tilde{X}_{i j}\\right|.\n$$\n- Control for multiple testing across the $g$ genes using the Benjamini–Hochberg False Discovery Rate (FDR) step-up procedure. Given the vector $(p_{1},\\dots,p_{g})$, sort it in nondecreasing order to obtain $p_{(1)} \\le \\cdots \\le p_{(g)}$, and compute\n$$\nq_{(k)} = \\min_{t \\in \\{k,\\dots,g\\}} \\left\\{ \\frac{g}{t} \\, p_{(t)} \\right\\},\n$$\nthen unsort to obtain adjusted $q$-values $(q_{1},\\dots,q_{g})$.\n- Define a decision rule at fixed thresholds $\\alpha \\in (0,1)$, $\\tau > 0$, and $m_{\\min} \\in \\mathbb{N}$: the pair of clusters $(a,b)$ is declared indistinguishable if the number of genes satisfying both $q_{j} \\le \\alpha$ and $L_{j} \\ge \\tau$ is strictly less than $m_{\\min}$. Equivalently, if\n$$\nS(a,b) = \\left| \\left\\{ j \\in \\{1,\\dots,g\\} : q_{j} \\le \\alpha \\;\\wedge\\; L_{j} \\ge \\tau \\right\\} \\right| < m_{\\min},\n$$\nthen clusters $a$ and $b$ are to be merged.\n\nAlgorithmic requirement:\n- Construct an undirected graph whose nodes are the current cluster identities. Add an edge between distinct clusters $a$ and $b$ if and only if $S(a,b) < m_{\\min}$. Replace the set of clusters by the connected components of this graph (merging each connected component into a single cluster). Iterate this process until no edges are added in an iteration (i.e., a fixed point is reached). At the end, compress cluster labels to consecutive integers starting at $0$ using order of first appearance when scanning cells $i=1$ to $n$.\n\nUse the following fixed hyperparameters for all test cases:\n- Significance level $\\alpha = 0.01$,\n- Log-fold-change threshold $\\tau = 0.25$,\n- Minimum marker count $m_{\\min} = 2$.\n\nInput specification for the program:\n- Hard-code the test suite below directly in the program. No input is to be read from standard input or files.\n\nTest suite:\n- Test case $1$ (happy path with over-clustering): $g = 5$ genes, $n = 12$ cells. The matrix $X \\in \\mathbb{N}_{0}^{12 \\times 5}$ is defined row-wise as\n  - Rows $1$ to $4$: $[50, 100, 80, 30, 20]$,\n  - Rows $5$ to $7$: $[50, 100, 80, 30, 20]$,\n  - Rows $8$ to $12$: $[50, 100, 80, 300, 200]$.\n  The initial label vector is $L = [0,0,0,0,1,1,1,2,2,2,2,2]$. Clusters with labels $0$ and $1$ are compositionally identical and should merge; cluster $2$ is distinct.\n- Test case $2$ (well-separated clusters): $g = 5$, $n = 10$. The matrix $X \\in \\mathbb{N}_{0}^{10 \\times 5}$ is\n  - Rows $1$ to $5$: $[5, 5, 5, 5, 30]$,\n  - Rows $6$ to $10$: $[30, 5, 5, 5, 5]$.\n  Initial labels $L = [0,0,0,0,0,1,1,1,1,1]$. These two clusters differ on at least $2$ genes and should not merge.\n- Test case $3$ (edge case with a singleton cluster and limited markers): $g = 5$, $n = 5$. The matrix $X \\in \\mathbb{N}_{0}^{5 \\times 5}$ is\n  - Rows $1$ to $4$: $[20, 20, 20, 20, 20]$,\n  - Row $5$: $[20, 20, 50, 20, 20]$.\n  Initial labels $L = [0,0,0,0,1]$. This simulates a small over-split singleton differing on only $1$ gene; with $m_{\\min} = 2$, it should merge back.\n\nOutput specification:\n- For each test case, the program must output a list with two elements: the final number of clusters after convergence, and the final compressed label vector of length $n$.\n- Aggregate the results for all three test cases into a single line as a comma-separated list enclosed in square brackets. Concretely, the output must be a single line representing\n$$\n\\left[ [K_{1}, \\text{labels}_{1}], [K_{2}, \\text{labels}_{2}], [K_{3}, \\text{labels}_{3}] \\right],\n$$\nwhere $K_{t} \\in \\mathbb{N}$ and $\\text{labels}_{t}$ is a list of $n_{t}$ integers for test case $t \\in \\{1,2,3\\}$. The integers inside the lists must be printed without spaces. For example, a syntactically correct output could look like $[[2,[0,0,0]],[2,[0,1]],[1,[0]]]$ (this is only an example format, not the expected answer).", "solution": "We design a statistically principled merge procedure for over-clustered single-cell ribonucleic acid sequencing (scRNA-seq) datasets by formalizing the lack of separating marker genes between clusters as a null hypothesis that cannot be rejected at a controlled False Discovery Rate (FDR). The derivation begins with core definitions and widely used statistical tools in transcriptomics.\n\nPreprocessing and normalization. Each observed count $X_{i j}$ is influenced by both the cellular gene expression program and technical factors such as library size. A widely accepted first-order correction is library-size normalization combined with a variance-stabilizing transform. For each cell $i$, compute the library size $s_{i} = \\sum_{j=1}^{g} X_{i j}$. The normalized expression on a continuous scale is then\n$$\n\\tilde{X}_{i j} = \\log\\left(1 + \\frac{X_{i j}}{s_{i}} \\cdot 10^{4}\\right).\n$$\nThis procedure rescales all cells to a common effective depth of $10^{4}$ total counts and applies a natural logarithm to dampen the mean-variance relationship, which is a well-tested practice in single-cell analysis.\n\nTesting for separating marker genes. Consider two clusters $a$ and $b$ with index sets $A$ and $B$, respectively. For each gene $j \\in \\{1,\\dots,g\\}$ we require a test to detect a difference in the central tendency of $\\tilde{X}_{i j}$ between $A$ and $B$. Since the scRNA-seq normalized data are typically non-Gaussian and may have ties, we adopt the two-sided Mann–Whitney $U$ test (Wilcoxon rank-sum), which tests\n$$\nH_{0}: \\text{the distributions of } \\{\\tilde{X}_{i j}\\}_{i \\in A} \\text{ and } \\{\\tilde{X}_{i j}\\}_{i \\in B} \\text{ are equal}\n$$\nversus\n$$\nH_{1}: \\text{the distributions differ in location}.\n$$\nThis yields a $p$-value $p_{j} \\in [0,1]$ for each gene $j$.\n\nMultiple-testing correction. Since we test across $g$ genes, we control the expected proportion of false discoveries using the Benjamini–Hochberg (BH) step-up FDR procedure. Given $p$-values $(p_{1},\\dots,p_{g})$, sort them to obtain $p_{(1)} \\le \\cdots \\le p_{(g)}$. Define the BH-adjusted values via\n$$\nq_{(k)} = \\min_{t \\in \\{k,\\dots,g\\}} \\left\\{ \\frac{g}{t} \\, p_{(t)} \\right\\},\n$$\nand then map back to the original gene order to obtain $(q_{1},\\dots,q_{g})$. This produces $q$-values that control the expected proportion of false positives among discoveries under standard independence or positive-dependence assumptions.\n\nEffect size threshold. To avoid merging clusters that have statistically detectable yet practically negligible differences, we include an effect size constraint via the absolute log-fold-change\n$$\nL_{j} = \\left|\\mu_{A j} - \\mu_{B j}\\right|, \\quad \\text{where } \\mu_{A j} = \\frac{1}{|A|} \\sum_{i \\in A} \\tilde{X}_{i j}, \\;\\; \\mu_{B j} = \\frac{1}{|B|} \\sum_{i \\in B} \\tilde{X}_{i j}.\n$$\nA gene is deemed a separating marker only if both $q_{j} \\le \\alpha$ and $L_{j} \\ge \\tau$, for fixed thresholds $\\alpha$ and $\\tau$. The number of separating markers is\n$$\nS(a,b) = \\left| \\left\\{ j \\in \\{1,\\dots,g\\} : q_{j} \\le \\alpha \\;\\wedge\\; L_{j} \\ge \\tau \\right\\} \\right|.\n$$\n\nMerge rule and convergence. We define an undirected graph with nodes equal to current cluster identities. For each pair of distinct clusters $(a,b)$, if $S(a,b) < m_{\\min}$, then we add an edge between $a$ and $b$, interpreting this as evidence that the clusters are indistinguishable (insufficient separating markers at the FDR and effect-size thresholds). All clusters within a connected component are merged into a single cluster. We iterate: at each step, recompute the graph on the merged clusters, and merge again, until no new edges appear. This process must terminate in finitely many steps because each merge strictly reduces the number of clusters, and the number of clusters is a nonnegative integer bounded below by $1$.\n\nCorrectness reasoning under the given test suite. We use fixed hyperparameters $\\alpha = 0.01$, $\\tau = 0.25$, and $m_{\\min} = 2$.\n\n- Test case $1$. Clusters with initial labels $0$ and $1$ have identical compositions across all $g = 5$ genes by construction. After normalization, for any gene $j$, the distributions of $\\tilde{X}_{i j}$ for $i$ in cluster $0$ and cluster $1$ are equal, yielding $p_{j} = 1$, hence $q_{j} = 1$. For all $j$, neither $q_{j} \\le \\alpha$ nor $L_{j} \\ge \\tau$ holds; thus $S(0,1) = 0 < m_{\\min}$, so they merge. For the merged cluster versus cluster $2$, the compositions differ strongly for genes $j = 4$ and $j = 5$ (indexing genes from $1$ to $5$), giving large $L_{j}$ and small $q_{j}$ that pass both thresholds for at least $2$ genes. Hence $S(\\text{merged},2) \\ge 2 \\ge m_{\\min}$, so they do not merge, and we end with $K_{1} = 2$ clusters.\n\n- Test case $2$. The two clusters differ compositionally on at least $2$ genes (gene $1$ and gene $5$), which yields $S(0,1) \\ge 2$ with $L_{j}$ large and $q_{j}$ small for those genes. Therefore $S(0,1) \\ge m_{\\min}$, no edge is added, and we end with $K_{2} = 2$ clusters.\n\n- Test case $3$. The singleton cluster has a difference relative to the larger cluster on only one gene (gene $3$), so $S(0,1) = 1 < m_{\\min}$, and the pair is deemed indistinguishable; they merge to a single cluster with $K_{3} = 1$.\n\nLabel compression. After convergence, we relabel clusters to integers starting at $0$ in order of first appearance across cells $i = 1$ to $n$. This ensures a canonical, reproducible output.\n\nAlgorithmic considerations. The Mann–Whitney $U$ test is computed per gene with a two-sided alternative and an asymptotic $p$-value, which is appropriate for small to moderate sample sizes and can handle ties. The Benjamini–Hochberg procedure is implemented exactly as specified. The graph-based merging uses union-find or connected components via standard disjoint-set logic. Termination is guaranteed as noted above.\n\nThe program encodes the three matrices $X$ and label vectors $L$ exactly as specified, applies the normalization, computes per-pair statistics, performs iterative merging, and outputs the required aggregate list on a single line in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\ndef normalize_log1p_cpm(X, scale=1e4):\n    \"\"\"\n    Library-size normalize counts per cell to counts-per-scale and apply log1p (natural log).\n    X: (n_cells, n_genes) nonnegative counts\n    Returns: (n_cells, n_genes) normalized matrix\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    # Compute library sizes per cell; add small epsilon to avoid divide-by-zero if necessary\n    lib = X.sum(axis=1, keepdims=True)\n    # Assumes lib > 0 in our test suite; if zeros occur, leave as zeros.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        norm = np.where(lib > 0, (X / lib) * scale, 0.0)\n    return np.log1p(norm)\n\ndef bh_fdr(pvals):\n    \"\"\"\n    Benjamini-Hochberg FDR adjustment.\n    pvals: array-like of length m\n    Returns: adjusted q-values array of length m\n    \"\"\"\n    p = np.asarray(pvals, dtype=float)\n    m = p.size\n    order = np.argsort(p)\n    p_sorted = p[order]\n    # Compute adjusted values\n    # q_(k) = min_{t >= k} (m/t * p_(t))\n    denom = np.arange(1, m + 1)\n    q_sorted = (m / denom) * p_sorted\n    # Enforce monotonicity\n    q_sorted = np.minimum.accumulate(q_sorted[::-1])[::-1]\n    # Cap at 1\n    q_sorted = np.minimum(q_sorted, 1.0)\n    # Unsort\n    q = np.empty_like(q_sorted)\n    q[order] = q_sorted\n    return q\n\ndef count_markers_between_clusters(Xnorm, labels, a, b, alpha, tau):\n    \"\"\"\n    For clusters a and b, compute the number of genes that satisfy q <= alpha and |logFC| >= tau.\n    Uses two-sided Mann-Whitney U test with asymptotic p-values.\n    \"\"\"\n    labels = np.asarray(labels)\n    idx_a = np.where(labels == a)[0]\n    idx_b = np.where(labels == b)[0]\n    Xa = Xnorm[idx_a, :]\n    Xb = Xnorm[idx_b, :]\n    # Compute per-gene means for effect size\n    mean_a = Xa.mean(axis=0)\n    mean_b = Xb.mean(axis=0)\n    logfc = np.abs(mean_a - mean_b)\n    # Compute per-gene p-values using Mann-Whitney U test\n    g = Xnorm.shape[1]\n    pvals = np.empty(g, dtype=float)\n    # Use asymptotic method to handle ties deterministically\n    for j in range(g):\n        # Handle degenerate case where both groups are constant and equal => p-value = 1.0\n        col_a = Xa[:, j]\n        col_b = Xb[:, j]\n        if np.all(col_a == col_a[0]) and np.all(col_b == col_b[0]) and (col_a[0] == col_b[0]):\n            pvals[j] = 1.0\n        else:\n            try:\n                res = mannwhitneyu(col_a, col_b, alternative='two-sided', method='asymptotic')\n                pvals[j] = res.pvalue\n            except Exception:\n                # Fallback: if test fails for any numerical reason, treat as non-significant\n                pvals[j] = 1.0\n    qvals = bh_fdr(pvals)\n    # Count markers satisfying both thresholds\n    markers = np.logical_and(qvals <= alpha, logfc >= tau)\n    return int(np.count_nonzero(markers))\n\ndef connected_components_merge(labels, indist_pairs):\n    \"\"\"\n    Merge clusters based on indistinguishable pairs using union-find.\n    labels: array of cluster ids\n    indist_pairs: list of tuples (a,b) that should be merged\n    Returns: new_labels with merged cluster ids (not yet compressed)\n    \"\"\"\n    labels = np.asarray(labels)\n    unique_clusters = np.unique(labels)\n    parent = {int(c): int(c) for c in unique_clusters}\n\n    def find(x):\n        # Path compression\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n\n    def union(x, y):\n        rx, ry = find(x), find(y)\n        if rx != ry:\n            parent[ry] = rx\n\n    # Union all indistinguishable pairs\n    for a, b in indist_pairs:\n        union(int(a), int(b))\n\n    # Map each original cluster to its root\n    root_map = {c: find(int(c)) for c in unique_clusters}\n    # Build new labels by replacing each label with its root\n    new_labels = np.array([root_map[int(c)] for c in labels], dtype=int)\n    return new_labels\n\ndef compress_labels_stable(labels):\n    \"\"\"\n    Compress labels to 0..K-1 in order of first appearance across the array.\n    \"\"\"\n    labels = np.asarray(labels, dtype=int)\n    mapping = {}\n    next_id = 0\n    compressed = np.empty_like(labels)\n    for i, c in enumerate(labels):\n        if int(c) not in mapping:\n            mapping[int(c)] = next_id\n            next_id += 1\n        compressed[i] = mapping[int(c)]\n    return compressed\n\ndef merge_overclustered(X, init_labels, alpha=0.01, tau=0.25, m_min=2):\n    \"\"\"\n    Main procedure:\n    - Normalize X with log1p CPM.\n    - Iteratively merge clusters with insufficient markers until convergence.\n    - Return final compressed labels and number of clusters.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    labels = np.asarray(init_labels, dtype=int)\n    Xnorm = normalize_log1p_cpm(X)\n\n    changed = True\n    while changed:\n        changed = False\n        # Current distinct clusters\n        clusters = np.unique(labels)\n        # Compute indistinguishable pairs\n        indist_pairs = []\n        for i in range(len(clusters)):\n            for j in range(i + 1, len(clusters)):\n                a = clusters[i]\n                b = clusters[j]\n                S = count_markers_between_clusters(Xnorm, labels, a, b, alpha, tau)\n                if S < m_min:\n                    indist_pairs.append((int(a), int(b)))\n        if indist_pairs:\n            new_labels = connected_components_merge(labels, indist_pairs)\n            if not np.array_equal(new_labels, labels):\n                labels = new_labels\n                changed = True\n    # Compress to consecutive integers in order of first appearance\n    final_labels = compress_labels_stable(labels)\n    K = int(np.unique(final_labels).size)\n    return K, final_labels.tolist()\n\ndef fmt(obj):\n    \"\"\"\n    Format lists (possibly nested) and integers into a compact string without spaces.\n    \"\"\"\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(fmt(x) for x in obj) + \"]\"\n    elif isinstance(obj, (tuple, np.ndarray)):\n        return \"[\" + \",\".join(fmt(x) for x in list(obj)) + \"]\"\n    else:\n        return str(int(obj)) if isinstance(obj, (np.integer,)) else str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    X1_top = [50, 100, 80, 30, 20]\n    X1_mid = [50, 100, 80, 30, 20]\n    X1_bot = [50, 100, 80, 300, 200]\n    X1 = np.array(\n        [X1_top] * 4 + [X1_mid] * 3 + [X1_bot] * 5,\n        dtype=float\n    )\n    L1 = [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2]\n\n    # Test case 2\n    X2_a = [5, 5, 5, 5, 30]\n    X2_b = [30, 5, 5, 5, 5]\n    X2 = np.array([X2_a] * 5 + [X2_b] * 5, dtype=float)\n    L2 = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n    # Test case 3\n    X3_a = [20, 20, 20, 20, 20]\n    X3_b = [20, 20, 50, 20, 20]\n    X3 = np.array([X3_a] * 4 + [X3_b] * 1, dtype=float)\n    L3 = [0, 0, 0, 0, 1]\n\n    test_cases = [\n        (X1, L1),\n        (X2, L2),\n        (X3, L3),\n    ]\n\n    alpha = 0.01\n    tau = 0.25\n    m_min = 2\n\n    results = []\n    for X, L in test_cases:\n        K, labels = merge_overclustered(X, L, alpha=alpha, tau=tau, m_min=m_min)\n        results.append([K, labels])\n\n    # Final print statement in the exact required format: no spaces inside lists.\n    print(fmt(results))\n\nsolve()\n```", "id": "2379646"}]}