{"hands_on_practices": [{"introduction": "We begin with a fundamental and highly informative property of ATAC-seq data: the distribution of sequenced fragment lengths. This practice guides you to build a Bayesian classifier to distinguish between hypothetical healthy and tumor tissues, a distinction reflected in these distributions due to differences in nucleosome packing and chromatin organization. This exercise highlights how a global data feature can serve as a powerful diagnostic and provides a practical application of statistical mixture models in a biological context. [@problem_id:2378288]", "problem": "You are given discrete fragment-length histograms from Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) for multiple samples. Each histogram consists of nonnegative integer counts over contiguous fragment-length bins with specified edges measured in base pairs. Assume the following generative model and classification objective.\n\n1. Fragment lengths are modeled as a continuous variable $\\ell$ in base pairs with a class-conditional probability density constructed as a mixture of Gaussian components corresponding to subnucleosomal and nucleosomal fragments. For each class $C \\in \\{\\mathrm{H}, \\mathrm{T}\\}$, the unnormalized density is\n$$\nf_C(\\ell) \\;=\\; \\sum_{k=1}^{4} w_{C,k} \\cdot \\phi(\\ell \\mid \\mu_k, \\sigma_{C,k}),\n$$\nwhere $\\phi(\\ell \\mid \\mu, \\sigma)$ is the Gaussian density with mean $\\mu$ and standard deviation $\\sigma$. The component means are fixed at\n$$\n\\mu_1 = 75,\\quad \\mu_2 = 180,\\quad \\mu_3 = 360,\\quad \\mu_4 = 540,\n$$\nall in base pairs. The class-specific mixture weights and standard deviations are\n$$\n(w_{\\mathrm{H},1}, w_{\\mathrm{H},2}, w_{\\mathrm{H},3}, w_{\\mathrm{H},4}) = (0.25, 0.45, 0.22, 0.08),\n$$\n$$\n(\\sigma_{\\mathrm{H},1}, \\sigma_{\\mathrm{H},2}, \\sigma_{\\mathrm{H},3}, \\sigma_{\\mathrm{H},4}) = (20, 15, 20, 25),\n$$\n$$\n(w_{\\mathrm{T},1}, w_{\\mathrm{T},2}, w_{\\mathrm{T},3}, w_{\\mathrm{T},4}) = (0.50, 0.35, 0.12, 0.03),\n$$\n$$\n(\\sigma_{\\mathrm{T},1}, \\sigma_{\\mathrm{T},2}, \\sigma_{\\mathrm{T},3}, \\sigma_{\\mathrm{T},4}) = (25, 25, 35, 45),\n$$\nwith all $\\sigma$ values in base pairs. The class label $\\mathrm{H}$ denotes healthy tissue and $\\mathrm{T}$ denotes rapidly dividing tumor tissue.\n\n2. Let the fragment-length bin edges (in base pairs) be the ordered list\n$$\n[\\,0,\\; 100,\\; 150,\\; 200,\\; 250,\\; 300,\\; 400,\\; 500,\\; 700,\\; 1000\\,].\n$$\nFor a class $C$, define the discrete bin probabilities $p_{C,i}$ for bin $i$ by integrating the density $f_C$ over the $i$-th bin and normalizing across all bins so that the probabilities sum to $1$:\n$$\np_{C,i} \\;=\\; \\frac{\\displaystyle \\int_{b_i}^{b_{i+1}} f_C(\\ell)\\, d\\ell}{\\displaystyle \\sum_{j=1}^{9} \\int_{b_j}^{b_{j+1}} f_C(\\ell)\\, d\\ell},\n$$\nwhere $[b_i, b_{i+1})$ is the $i$-th bin for $i = 1,\\dots,9$.\n\n3. For a given sample with total count $N$ and count vector $\\mathbf{c} = (c_1,\\dots,c_9)$ over the $9$ bins, assume that conditioned on class $C$ the counts follow a multinomial distribution with parameters $N$ and $(p_{C,1},\\dots,p_{C,9})$.\n\n4. Assume equal class priors $P(\\mathrm{H}) = P(\\mathrm{T}) = 1/2$. For each sample, decide the class label that maximizes the posterior probability under this model.\n\nTest Suite. Use the following four test cases, each specified as a count vector over the $9$ bins defined above. All fragment-lengths are in base pairs and all counts are integers.\n\n- Case $1$ (healthy-like, moderate depth, total $20000$): $[\\,1600,\\;2000,\\;6800,\\;1200,\\;1000,\\;4400,\\;800,\\;1800,\\;400\\,]$.\n- Case $2$ (tumor-like, moderate depth, total $20000$): $[\\,4000,\\;4400,\\;5000,\\;1400,\\;1400,\\;2400,\\;600,\\;600,\\;200\\,]$.\n- Case $3$ (ambiguous, moderate depth, total $20000$): $[\\,3000,\\;3200,\\;5400,\\;1400,\\;1400,\\;3200,\\;1000,\\;1200,\\;200\\,]$.\n- Case $4$ (healthy-like, low depth, total $100$): $[\\,0,\\;5,\\;40,\\;0,\\;0,\\;30,\\;0,\\;20,\\;5\\,]$.\n\nRequired output. Your program must, for each case, output a boolean indicating whether the sample is classified as healthy tissue ($\\mathrm{H}$) or rapidly dividing tumor tissue ($\\mathrm{T}$). Represent healthy as the boolean value True and tumor as the boolean value False. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\,\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}\\,]$). No additional text must be printed. All lengths must be treated in base pairs and no other physical units are required in the output.", "solution": "The problem requires the classification of ATAC-seq samples into one of two classes, Healthy ($\\mathrm{H}$) or Tumor ($\\mathrm{T}$), based on fragment-length histograms. This is a Bayesian classification problem. The objective is to determine, for a given vector of fragment counts $\\mathbf{c} = (c_1, \\dots, c_9)$, which class $C \\in \\{\\mathrm{H}, \\mathrm{T}\\}$ maximizes the posterior probability $P(C \\mid \\mathbf{c})$.\n\nAccording to Bayes' theorem, the posterior probability is given by:\n$$\nP(C \\mid \\mathbf{c}) = \\frac{P(\\mathbf{c} \\mid C) P(C)}{P(\\mathbf{c})}\n$$\nTo maximize $P(C \\mid \\mathbf{c})$, we need to compare $P(\\mathrm{H} \\mid \\mathbf{c})$ and $P(\\mathrm{T} \\mid \\mathbf{c})$. The term $P(\\mathbf{c})$ is a normalizing constant, identical for both classes, and can be disregarded for the purpose of comparison. The problem states that the class priors are equal, $P(\\mathrm{H}) = P(\\mathrm{T}) = 1/2$. Therefore, maximizing the posterior probability is equivalent to maximizing the class-conditional likelihood, $P(\\mathbf{c} \\mid C)$. The decision rule is to choose class $\\mathrm{H}$ if $P(\\mathbf{c} \\mid \\mathrm{H}) > P(\\mathbf{c} \\mid \\mathrm{T})$, and class $\\mathrm{T}$ otherwise.\n\nThe problem states that the count vector $\\mathbf{c}$, conditioned on the class $C$ and total count $N = \\sum_{i=1}^{9} c_i$, follows a multinomial distribution with parameters $N$ and a probability vector $\\mathbf{p}_C = (p_{C,1}, \\dots, p_{C,9})$. The likelihood is given by the multinomial probability mass function:\n$$\nP(\\mathbf{c} \\mid C) = \\frac{N!}{c_1! c_2! \\dots c_9!} \\prod_{i=1}^{9} p_{C,i}^{c_i}\n$$\nThe multinomial coefficient $\\frac{N!}{\\prod_{i=1}^{9} c_i!}$ is also identical for both classes and can be ignored in the comparison. The decision is thus based on comparing the product term $\\prod_{i=1}^{9} p_{C,i}^{c_i}$. To avoid numerical underflow from multiplying many small probabilities, it is computationally more stable to compare the log-likelihoods. The decision rule becomes: choose class $\\mathrm{H}$ if $\\mathcal{L}(\\mathrm{H}) > \\mathcal{L}(\\mathrm{T})$, where the relevant part of the log-likelihood for a class $C$ is:\n$$\n\\mathcal{L}(C) = \\log\\left(\\prod_{i=1}^{9} p_{C,i}^{c_i}\\right) = \\sum_{i=1}^{9} c_i \\log p_{C,i}\n$$\nThe core of the problem is therefore to compute the bin probability vectors $\\mathbf{p}_\\mathrm{H}$ and $\\mathbf{p}_\\mathrm{T}$.\n\nThe problem defines these probabilities based on a continuous generative model for fragment length $\\ell$. For each class $C$, the unnormalized density $f_C(\\ell)$ is a mixture of $4$ Gaussian components:\n$$\nf_C(\\ell) = \\sum_{k=1}^{4} w_{C,k} \\cdot \\phi(\\ell \\mid \\mu_k, \\sigma_{C,k})\n$$\nwhere $\\phi(\\ell \\mid \\mu, \\sigma)$ is the probability density function (PDF) of a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$. The bin probability $p_{C,i}$ for the $i$-th bin $[b_i, b_{i+1})$ is obtained by integrating $f_C(\\ell)$ over this interval and normalizing by the sum of integrals over all $9$ specified bins.\n\nFirst, we calculate the unnormalized probability mass for each bin $i$ and class $C$, which we denote by $I_{C,i}$:\n$$\nI_{C,i} = \\int_{b_i}^{b_{i+1}} f_C(\\ell)\\, d\\ell = \\int_{b_i}^{b_{i+1}} \\sum_{k=1}^{4} w_{C,k} \\cdot \\phi(\\ell \\mid \\mu_k, \\sigma_{C,k})\\, d\\ell\n$$\nBy the linearity of integration, this is:\n$$\nI_{C,i} = \\sum_{k=1}^{4} w_{C,k} \\int_{b_i}^{b_{i+1}} \\phi(\\ell \\mid \\mu_k, \\sigma_{C,k})\\, d\\ell\n$$\nThe integral of a Gaussian PDF over an interval can be expressed using its cumulative distribution function (CDF), $\\Phi(\\ell \\mid \\mu, \\sigma)$. Specifically, $\\int_{a}^{b} \\phi(\\ell \\mid \\mu, \\sigma)\\,d\\ell = \\Phi(b \\mid \\mu, \\sigma) - \\Phi(a \\mid \\mu, \\sigma)$. The Gaussian CDF can be computed from the standard normal CDF, $\\Phi_{std}(z) = \\Phi(z|0,1)$, as $\\Phi(\\ell \\mid \\mu, \\sigma) = \\Phi_{std}((\\ell-\\mu)/\\sigma)$. The standard normal CDF is related to the error function, $\\operatorname{erf}(x)$, by $\\Phi_{std}(z) = \\frac{1}{2}(1 + \\operatorname{erf}(z/\\sqrt{2}))$.\nThus, for each Gaussian component $k$ and bin $i$:\n$$\n\\int_{b_i}^{b_{i+1}} \\phi(\\ell \\mid \\mu_k, \\sigma_{C,k})\\,d\\ell = \\Phi\\left(\\frac{b_{i+1}-\\mu_k}{\\sigma_{C,k}}\\right) - \\Phi\\left(\\frac{b_i-\\mu_k}{\\sigma_{C,k}}\\right)\n$$\nAfter computing $I_{C,i}$ for all $i=1,\\dots,9$, we find the total unnormalized mass over the considered range $[b_1, b_{10}) = [0, 1000)$:\n$$\nS_C = \\sum_{j=1}^{9} I_{C,j} = \\int_{0}^{1000} f_C(\\ell)\\, d\\ell\n$$\nThe final bin probabilities are then obtained by normalization:\n$$\np_{C,i} = \\frac{I_{C,i}}{S_C}\n$$\nThis process is performed for both class $\\mathrm{H}$ and class $\\mathrm{T}$ to obtain the probability vectors $\\mathbf{p}_\\mathrm{H}$ and $\\mathbf{p}_\\mathrm{T}$.\n\nThe complete algorithm is as follows:\n1.  Define the model parameters: means $\\mu_k$, weights $w_{C,k}$, standard deviations $\\sigma_{C,k}$ for $C \\in \\{\\mathrm{H}, \\mathrm{T}\\}$, and bin edges $b_i$.\n2.  Compute the probability vector $\\mathbf{p}_\\mathrm{H}$ by integrating the mixture density $f_\\mathrm{H}(\\ell)$ over each of the $9$ bins and normalizing the results.\n3.  Compute the probability vector $\\mathbf{p}_\\mathrm{T}$ similarly, using $f_\\mathrm{T}(\\ell)$.\n4.  For each test case given by a count vector $\\mathbf{c} = (c_1, \\dots, c_9)$:\n    a. Calculate the log-likelihood score for class $\\mathrm{H}$: $\\mathcal{L}(\\mathrm{H}) = \\sum_{i=1}^{9} c_i \\log p_{\\mathrm{H},i}$.\n    b. Calculate the log-likelihood score for class $\\mathrm{T}$: $\\mathcal{L}(\\mathrm{T}) = \\sum_{i=1}^{9} c_i \\log p_{\\mathrm{T},i}$.\n    c. Compare the scores. If $\\mathcal{L}(\\mathrm{H}) > \\mathcal{L}(\\mathrm{T})$, the sample is classified as Healthy ($\\mathrm{H}$), represented by the boolean `True`. Otherwise, it is classified as Tumor ($\\mathrm{T}$), represented by `False`.\n5.  Collect the boolean results for all test cases and format them as required.\nThis procedure will be implemented to solve the problem for the given test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves the ATAC-seq sample classification problem.\n    \"\"\"\n\n    # 1. Define model parameters as given in the problem statement.\n    \n    # Fragment-length bin edges in base pairs\n    bin_edges = np.array([0, 100, 150, 200, 250, 300, 400, 500, 700, 1000], dtype=np.float64)\n\n    # Gaussian component means (fixed for both classes)\n    means = np.array([75, 180, 360, 540], dtype=np.float64)\n\n    # Class H (Healthy) parameters\n    weights_H = np.array([0.25, 0.45, 0.22, 0.08], dtype=np.float64)\n    std_devs_H = np.array([20, 15, 20, 25], dtype=np.float64)\n\n    # Class T (Tumor) parameters\n    weights_T = np.array([0.50, 0.35, 0.12, 0.03], dtype=np.float64)\n    std_devs_T = np.array([25, 25, 35, 45], dtype=np.float64)\n\n    # Test cases: count vectors\n    test_cases = [\n        np.array([1600, 2000, 6800, 1200, 1000, 4400, 800, 1800, 400]), # Case 1\n        np.array([4000, 4400, 5000, 1400, 1400, 2400, 600, 600, 200]), # Case 2\n        np.array([3000, 3200, 5400, 1400, 1400, 3200, 1000, 1200, 200]), # Case 3\n        np.array([0, 5, 40, 0, 0, 30, 0, 20, 5]),                     # Case 4\n    ]\n\n    def standard_normal_cdf(x):\n        \"\"\"\n        Computes the standard normal CDF using the error function erf.\n        Phi(z) = 0.5 * (1 + erf(z / sqrt(2)))\n        \"\"\"\n        return 0.5 * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def calculate_bin_probabilities(weights, std_devs, means, bin_edges):\n        \"\"\"\n        Calculates the multinomial probability vector for a given class.\n        \n        Args:\n            weights (np.ndarray): Mixture weights for the class.\n            std_devs (np.ndarray): Standard deviations for the class.\n            means (np.ndarray): Component means.\n            bin_edges (np.ndarray): Edges of the fragment length bins.\n\n        Returns:\n            np.ndarray: A vector of bin probabilities.\n        \"\"\"\n        num_bins = len(bin_edges) - 1\n        unnormalized_probs = np.zeros(num_bins, dtype=np.float64)\n\n        for i in range(num_bins):\n            b_low = bin_edges[i]\n            b_high = bin_edges[i+1]\n            bin_integral = 0.0\n            \n            for k in range(len(means)):\n                mu_k = means[k]\n                sigma_k = std_devs[k]\n                w_k = weights[k]\n\n                # Integral of Gaussian PDF over [b_low, b_high]\n                # is CDF(b_high) - CDF(b_low)\n                z_high = (b_high - mu_k) / sigma_k\n                z_low = (b_low - mu_k) / sigma_k\n                \n                component_integral = standard_normal_cdf(z_high) - standard_normal_cdf(z_low)\n                \n                bin_integral += w_k * component_integral\n            \n            unnormalized_probs[i] = bin_integral\n\n        # Normalize probabilities to sum to 1 over the specified bins\n        total_prob_mass = np.sum(unnormalized_probs)\n        if total_prob_mass > 0:\n            return unnormalized_probs / total_prob_mass\n        else:\n            # This case should not happen with the given positive parameters\n            return unnormalized_probs\n\n    # 2. Calculate the probability vectors p_H and p_T\n    p_H = calculate_bin_probabilities(weights_H, std_devs_H, means, bin_edges)\n    p_T = calculate_bin_probabilities(weights_T, std_devs_T, means, bin_edges)\n\n    # 3. Classify each test case\n    results = []\n    for counts in test_cases:\n        # Using np.log1p on probabilities could be safer for very small probs near 0,\n        # but here they are integrals of positive functions over non-zero intervals,\n        # so they should be robustly > 0.\n        # Handle cases where counts[i] == 0, where `counts * log(p)` would be `0 * -inf = nan`.\n        # We can use np.where to ensure the term is 0.\n        log_likelihood_H = np.sum(counts * np.where(p_H > 0, np.log(p_H), -np.inf))\n        log_likelihood_T = np.sum(counts * np.where(p_T > 0, np.log(p_T), -np.inf))\n        \n        # Classify as Healthy (H) if its log-likelihood is greater.\n        is_healthy = log_likelihood_H > log_likelihood_T\n        results.append(is_healthy)\n\n    # 4. Print the final output in the required format\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "2378288"}, {"introduction": "After examining global patterns, we now zoom into the high-resolution data that makes ATAC-seq so powerful: transcription factor (TF) footprinting. This exercise challenges you to implement a probabilistic model to evaluate competing hypotheses—cooperative binding ($H_C$) versus independent binding ($H_I$)—based on idealized cut-site data. By comparing the likelihood of the observed patterns under each model, you will gain practical experience with statistical model selection and its application to understanding the quantitative nature of gene regulation. [@problem_id:2378307]", "problem": "Consider a one-dimensional genomic window modeled as integer positions $i \\in \\{0,1,\\dots,L-1\\}$, where $L$ is a positive integer. At each position $i$, an integer cut count $c_i \\ge 0$ is observed from Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq). Two putative motif centers at integer positions $m_1$ and $m_2$ (in base pairs) are given. You must decide, for each provided test case, whether the observed counts are more consistent with a single cooperative Transcription Factor (TF) binding footprint spanning the region between the two motifs, or with two independent TF footprints near the two motifs. The decision must be based solely on a probabilistic generative model defined below, expressed in purely mathematical terms.\n\nModel assumptions and definitions. Under either hypothesis, cuts at distinct positions are independent and follow a Poisson distribution with position-specific rate. Let the background cut rate be $b > 0$, the footprint attenuation factor be $f \\in (0,1)$, the flanking enhancement amplitude be $a \\ge 0$, the Gaussian spread be $s > 0$ (in base pairs), and the footprint half-width be a positive integer $w$. Define the discrete Gaussian shape $G(x) = \\exp\\!\\left(-\\dfrac{x^2}{2 s^2}\\right)$ for any real $x$.\n\nDefine the interior sets. Let $I_1 = \\{i \\in \\mathbb{Z} : m_1 - w \\le i \\le m_1 + w\\}$ and $I_2 = \\{i \\in \\mathbb{Z} : m_2 - w \\le i \\le m_2 + w\\}$. Under the cooperative hypothesis $H_C$, the footprint interior is the single interval $I_C = \\{i \\in \\mathbb{Z} : \\min(m_1,m_2) - w \\le i \\le \\max(m_1,m_2) + w\\}$ with two outer edges at $e_1 = \\min(m_1,m_2) - w$ and $e_2 = \\max(m_1,m_2) + w$. Under the independent hypothesis $H_I$, the footprint interior is the union $I_I = I_1 \\cup I_2$ with four edges at $e_{1a} = m_1 - w$, $e_{1b} = m_1 + w$, $e_{2a} = m_2 - w$, and $e_{2b} = m_2 + w$.\n\nDefine the position-specific Poisson rates. For each position $i \\in \\{0,1,\\dots,L-1\\}$,\n- under $H_C$,\n$$\n\\lambda_i^{(C)} \\;=\\; b \\cdot \\left( f \\cdot \\mathbf{1}_{\\{i \\in I_C\\}} \\;+\\; 1 \\cdot \\mathbf{1}_{\\{i \\notin I_C\\}} \\right) \\;+\\; b\\,a \\left[ G(i - e_1) + G(i - e_2) \\right],\n$$\n- under $H_I$,\n$$\n\\lambda_i^{(I)} \\;=\\; b \\cdot \\left( f \\cdot \\mathbf{1}_{\\{i \\in I_I\\}} \\;+\\; 1 \\cdot \\mathbf{1}_{\\{i \\notin I_I\\}} \\right) \\;+\\; b\\,a \\left[ G(i - e_{1a}) + G(i - e_{1b}) + G(i - e_{2a}) + G(i - e_{2b}) \\right].\n$$\n\nFor observed counts $\\{c_i\\}_{i=0}^{L-1}$, the probability of the data under each hypothesis is\n$$\n\\mathbb{P}(\\{c_i\\}\\mid H) \\;=\\; \\prod_{i=0}^{L-1} \\frac{\\left(\\lambda_i^{(H)}\\right)^{c_i} e^{-\\lambda_i^{(H)}}}{c_i!},\n$$\nwhere $H \\in \\{H_C, H_I\\}$. For each test case, you must decide whether $H_C$ yields a greater probability than $H_I$ for the provided counts and parameters.\n\nRequired output format. For the complete set of test cases, your program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets (for example, \"[True,False,True]\"). For each test case, output the boolean value $\\mathrm{True}$ if the cooperative hypothesis $H_C$ is more probable than the independent hypothesis $H_I$, and $\\mathrm{False}$ otherwise.\n\nAll positions are integers measured in base pairs, and counts are dimensionless integers. No other physical units are involved.\n\nParameters and test suite. Use the same global parameters for all test cases: $L = 41$, $b = 5.0$, $f = 0.2$, $a = 1.2$, $s = 1.0$, and $w = 3$. The test cases are:\n\nTest case $1$:\n- Motif centers: $m_1 = 15$, $m_2 = 25$.\n- Observed counts $\\{c_i\\}_{i=0}^{40}$:\n[$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$12$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$1$,$12$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$].\n\nTest case $2$:\n- Motif centers: $m_1 = 15$, $m_2 = 25$.\n- Observed counts $\\{c_i\\}_{i=0}^{40}$:\n[$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$11$,$1$,$1$,$1$,$1$,$1$,$11$,$5$,$5$,$5$,$11$,$1$,$1$,$1$,$1$,$1$,$11$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$].\n\nTest case $3$:\n- Motif centers: $m_1 = 20$, $m_2 = 23$.\n- Observed counts $\\{c_i\\}_{i=0}^{40}$:\n[$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$9$,$3$,$3$,$3$,$3$,$3$,$3$,$3$,$3$,$9$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$,$5$].\n\nYour program must implement the decision using the model above and produce the single-line boolean list for these three test cases in the exact output format described.", "solution": "The problem presented is a well-posed and scientifically valid task in the domain of computational biology. It requires a decision between two competing hypotheses, $H_C$ (cooperative binding) and $H_I$ (independent binding), to explain observed ATAC-seq cut count data, $\\{c_i\\}$, within a specific genomic window. The decision is to be made by comparing the posterior probabilities of the data under each hypothesis. Assuming equal prior probabilities for $H_C$ and $H_I$, this is equivalent to comparing the likelihoods $\\mathbb{P}(\\{c_i\\}\\mid H_C)$ and $\\mathbb{P}(\\{c_i\\}\\mid H_I)$.\n\nThe core of the model is the assumption that the cut count $c_i$ at each position $i$ is an independent random variable drawn from a Poisson distribution with a position-dependent rate parameter $\\lambda_i$. The specific functional form of $\\lambda_i$ differs between the two hypotheses, $H_C$ and $H_I$, capturing the distinct biophysical scenarios.\n\nThe likelihood of observing the entire dataset $\\{c_i\\}_{i=0}^{L-1}$ under a hypothesis $H \\in \\{H_C, H_I\\}$ is given by the product of individual Poisson probabilities:\n$$\n\\mathbb{P}(\\{c_i\\}\\mid H) = \\prod_{i=0}^{L-1} \\frac{\\left(\\lambda_i^{(H)}\\right)^{c_i} e^{-\\lambda_i^{(H)}}}{c_i!}\n$$\nDirect computation and comparison of these likelihoods can be numerically unstable due to the product of many small probabilities, which can lead to floating-point underflow. A standard and mathematically equivalent approach is to compare the log-likelihoods. Since the natural logarithm is a strictly monotonic function, the inequality $\\mathbb{P}(\\{c_i\\}\\mid H_C) > \\mathbb{P}(\\{c_i\\}\\mid H_I)$ is equivalent to $\\log\\mathbb{P}(\\{c_i\\}\\mid H_C) > \\log\\mathbb{P}(\\{c_i\\}\\mid H_I)$.\n\nThe log-likelihood, $\\mathcal{L}(H)$, is:\n$$\n\\mathcal{L}(H) = \\log \\mathbb{P}(\\{c_i\\}\\mid H) = \\sum_{i=0}^{L-1} \\left( c_i \\log(\\lambda_i^{(H)}) - \\lambda_i^{(H)} - \\log(c_i!) \\right)\n$$\nWhen comparing $\\mathcal{L}(H_C)$ and $\\mathcal{L}(H_I)$, the term $\\sum_{i=0}^{L-1} \\log(c_i!)$ is common to both and can be eliminated. The decision rule simplifies to comparing the reduced log-likelihood scores, which we may denote as $S_H$:\n$$\nS_H = \\sum_{i=0}^{L-1} \\left( c_i \\log(\\lambda_i^{(H)}) - \\lambda_i^{(H)} \\right)\n$$\nThe cooperative hypothesis $H_C$ is deemed more probable if $S_C > S_I$.\n\nThe solution, therefore, requires a systematic computation of the rate vectors $\\{\\lambda_i^{(C)}\\}$ and $\\{\\lambda_i^{(I)}\\}$ for each test case, followed by the calculation and comparison of their respective scores $S_C$ and $S_I$.\n\nStep-by-step procedure for each test case:\n\n1.  **Define Constants and Position Vector**: The global parameters $L=41, b=5.0, f=0.2, a=1.2, s=1.0, w=3$ are fixed. The integer positions are represented by a vector $i \\in \\{0, 1, ..., 40\\}$. The Gaussian shape function is $G(x) = \\exp(-x^2 / (2s^2))$.\n\n2.  **Calculate Rates under Hypothesis $H_C$**:\n    - For given motif centers $m_1$ and $m_2$, determine the boundaries of the cooperative footprint: $e_1 = \\min(m_1, m_2) - w$ and $e_2 = \\max(m_1, m_2) + w$.\n    - The interior of the cooperative footprint is the set of integers $I_C = [e_1, e_2]$.\n    - For each position $i$, the rate $\\lambda_i^{(C)}$ is calculated. The rate consists of a base component and an enhancement component.\n        - The base component models the footprint itself: it is $b \\cdot f$ for $i \\in I_C$ (attenuation) and $b$ for $i \\notin I_C$ (background).\n        - The enhancement component models the increased cut probability at the footprint edges: $b \\cdot a \\cdot [G(i - e_1) + G(i - e_2)]$.\n    - The total rate is $\\lambda_i^{(C)} = b \\cdot (f \\cdot \\mathbf{1}_{\\{i \\in I_C\\}} + \\mathbf{1}_{\\{i \\notin I_C\\}}) + b \\cdot a \\cdot [G(i - e_1) + G(i - e_2)]$.\n\n3.  **Calculate Rates under Hypothesis $H_I$**:\n    - The independent binding hypothesis models two separate footprints. The four edges are $e_{1a} = m_1 - w$, $e_{1b} = m_1 + w$, $e_{2a} = m_2 - w$, and $e_{2b} = m_2 + w$.\n    - The interior of the independent footprints is the union of two sets, $I_I = [m_1-w, m_1+w] \\cup [m_2-w, m_2+w]$.\n    - For each position $i$, the rate $\\lambda_i^{(I)}$ is calculated.\n        - The base component is $b \\cdot f$ for $i \\in I_I$ and $b$ for $i \\notin I_I$.\n        - The enhancement component is derived from all four edges: $b \\cdot a \\cdot [G(i - e_{1a}) + G(i - e_{1b}) + G(i - e_{2a}) + G(i - e_{2b})]$.\n    - The total rate is $\\lambda_i^{(I)} = b \\cdot (f \\cdot \\mathbf{1}_{\\{i \\in I_I\\}} + \\mathbf{1}_{\\{i \\notin I_I\\}}) + b \\cdot a \\cdot [G(i - e_{1a}) + G(i - e_{1b}) + G(i - e_{2a}) + G(i - e_{2b})]$.\n\n4.  **Compute Scores and Conclude**:\n    - Using the observed counts $\\{c_i\\}$ and the calculated rate vectors $\\{\\lambda_i^{(C)}\\}$ and $\\{\\lambda_i^{(I)}\\}$, compute the two scores:\n      $S_C = \\sum_{i=0}^{L-1} (c_i \\log(\\lambda_i^{(C)}) - \\lambda_i^{(C)})$\n      $S_I = \\sum_{i=0}^{L-1} (c_i \\log(\\lambda_i^{(I)}) - \\lambda_i^{(I)})$\n    - The result for the test case is the boolean value of the expression $S_C > S_I$.\n\nThis procedure is implemented for each of the three test cases provided, and the resulting boolean values are collected into a list for final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the chromatin accessibility model comparison problem.\n    \"\"\"\n    # Global parameters\n    L = 41\n    b = 5.0\n    f = 0.2\n    a = 1.2\n    s = 1.0\n    w = 3\n\n    # Test cases\n    test_cases = [\n        {\n            \"m1\": 15, \"m2\": 25,\n            \"counts\": np.array([5,5,5,5,5,5,5,5,5,5,5,5,12,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,12,5,5,5,5,5,5,5,5,5,5,5,5])\n        },\n        {\n            \"m1\": 15, \"m2\": 25,\n            \"counts\": np.array([5,5,5,5,5,5,5,5,5,5,5,5,11,1,1,1,1,1,11,5,5,5,11,1,1,1,1,1,11,5,5,5,5,5,5,5,5,5,5,5,5])\n        },\n        {\n            \"m1\": 20, \"m2\": 23,\n            \"counts\": np.array([5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,9,3,3,3,3,3,3,3,3,9,5,5,5,5,5,5,5,5,5,5,5,5,5,5])\n        }\n    ]\n\n    results = []\n    \n    # Position vector\n    i_vals = np.arange(L)\n    \n    # Gaussian shape function\n    def G(x):\n        return np.exp(-x**2 / (2 * s**2))\n\n    for case in test_cases:\n        m1 = case[\"m1\"]\n        m2 = case[\"m2\"]\n        counts = case[\"counts\"]\n\n        # -----------------------------------------------\n        # Hypothesis H_C: Cooperative Binding\n        # -----------------------------------------------\n        m_min, m_max = min(m1, m2), max(m1, m2)\n        e1 = m_min - w\n        e2 = m_max + w\n        \n        # Indicator for points inside the cooperative footprint\n        is_in_Ic = (i_vals >= e1)  (i_vals = e2)\n        \n        # Base rate for H_C\n        base_rate_C = np.where(is_in_Ic, b * f, b)\n        \n        # Enhancement for H_C\n        enhancement_C = b * a * (G(i_vals - e1) + G(i_vals - e2))\n        \n        # Total rate vector for H_C\n        lambda_C = base_rate_C + enhancement_C\n        \n        #\n        # Log-likelihood score for H_C (ignoring constant log(c_i!) term)\n        # Note: We must handle cases where lambda is zero, though the model ensures lambda > 0.\n        # Likewise for counts being zero, 0 * log(lambda) = 0.\n        log_likelihood_C = np.sum(counts * np.log(lambda_C) - lambda_C)\n\n        # -----------------------------------------------\n        # Hypothesis H_I: Independent Binding\n        # -----------------------------------------------\n        e1a = m1 - w\n        e1b = m1 + w\n        e2a = m2 - w\n        e2b = m2 + w\n        \n        # Indicator for points inside either independent footprint\n        is_in_I1 = (i_vals >= e1a)  (i_vals = e1b)\n        is_in_I2 = (i_vals >= e2a)  (i_vals = e2b)\n        is_in_Ii = is_in_I1 | is_in_I2\n        \n        # Base rate for H_I\n        base_rate_I = np.where(is_in_Ii, b * f, b)\n\n        # Enhancement for H_I\n        enhancement_I = b * a * (G(i_vals - e1a) + G(i_vals - e1b) + G(i_vals - e2a) + G(i_vals - e2b))\n        \n        # Total rate vector for H_I\n        lambda_I = base_rate_I + enhancement_I\n\n        # Log-likelihood score for H_I\n        log_likelihood_I = np.sum(counts * np.log(lambda_I) - lambda_I)\n\n        # -----------------------------------------------\n        # Decision\n        # -----------------------------------------------\n        results.append(log_likelihood_C > log_likelihood_I)\n\n    # Final output as per specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2378307"}, {"introduction": "Our final practice synthesizes multiple data features to create a comprehensive map of chromatin states across a genomic region. You will implement the Viterbi algorithm for a Hidden Markov Model (HMM) to segment a hypothetical genomic locus into accessible, nucleosome-array, and inaccessible states based on local fragment density and size. This exercise demonstrates how sequence models can infer underlying biological structure from noisy observational data, a cornerstone of modern bioinformatics. [@problem_id:2378334]", "problem": "You are given a simplified generative model for Assay for Transposase-Accessible Chromatin using sequencing (ATAC-seq) fragment features along a one-dimensional genome coordinate system. The goal is to segment the genome into discrete chromatin accessibility states using a Hidden Markov Model (HMM), based on two features observed at each genomic position: fragment length in base pairs and local fragment density (counts in a fixed-width window). The three hidden states are: accessible chromatin, nucleosome-array chromatin, and inaccessible chromatin. Each state emits an observation consisting of a pair of values: fragment length (in base pairs) and count (dimensionless). Your task is to implement the Viterbi decoding algorithm to infer the most likely sequence of hidden states for a sequence of observations, given fixed HMM parameters.\n\nStart from the core definition of a Hidden Markov Model and the assumption that observations are generated conditionally independently given the hidden state at each position. Assume that fragment length is modeled by a Gaussian (Normal) distribution and count by a Poisson distribution, and that, conditioned on the hidden state, these two features are independent so that the joint emission probability is the product of the two distributions. Use only the definitions of the Gaussian density and the Poisson probability mass function, along with the Markov property for transitions, to derive the required dynamic programming recurrences in your design. Do not use any external training or parameter estimation; all model parameters are provided below.\n\nModel specification:\n- Hidden states: accessible chromatin, nucleosome-array chromatin, inaccessible chromatin, indexed as $0$, $1$, and $2$, respectively.\n- Initial state probabilities: $\\pi = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right]$.\n- State transition matrix $A$ (rows are current state, columns are next state):\n  - From state $0$: to $0$ with probability $0.96$, to $1$ with probability $0.02$, to $2$ with probability $0.02$.\n  - From state $1$: to $0$ with probability $0.02$, to $1$ with probability $0.96$, to $2$ with probability $0.02$.\n  - From state $2$: to $0$ with probability $0.02$, to $1$ with probability $0.02$, to $2$ with probability $0.96$.\n- Emission parameters per state $s \\in \\{0,1,2\\}$:\n  - Fragment length $x$ (in base pairs) follows a Normal distribution with mean $\\mu_s$ and standard deviation $\\sigma_s$:\n    - State $0$: $\\mu_0 = 50$, $\\sigma_0 = 15$.\n    - State $1$: $\\mu_1 = 200$, $\\sigma_1 = 25$.\n    - State $2$: $\\mu_2 = 300$, $\\sigma_2 = 40$.\n  - Count $k$ follows a Poisson distribution with rate $\\lambda_s$:\n    - State $0$: $\\lambda_0 = 8$.\n    - State $1$: $\\lambda_1 = 3$.\n    - State $2$: $\\lambda_2 = 0.3$.\n- Conditional independence of features given the hidden state: for a given state $s$, the joint emission density for an observation $(x,k)$ is $p(x,k \\mid s) = p_{\\text{Normal}}(x \\mid \\mu_s,\\sigma_s) \\cdot p_{\\text{Poisson}}(k \\mid \\lambda_s)$.\n\nTest suite:\n- Test case $1$ (happy path, length $8$; fragment length in base pairs, counts unitless):\n  - Observations: $[(55, 9), (48, 7), (60, 8), (195, 3), (210, 4), (185, 3), (330, 0), (345, 1)]$.\n- Test case $2$ (boundary condition, length $1$):\n  - Observations: $[(52, 8)]$.\n- Test case $3$ (transition behavior, length $5$):\n  - Observations: $[(50, 8), (62, 7), (70, 6), (205, 4), (198, 3)]$.\n\nYour program must:\n- Implement numerically stable Viterbi decoding in log-space for the provided HMM with the above parameters, using only the definitions of the Normal density and Poisson probability mass function, and the Markov property.\n- Decode each observation sequence into the most likely state sequence, using the state indices $0$ for accessible, $1$ for nucleosome-array, and $2$ for inaccessible.\n- Produce a single line of output containing the decoded state sequences for all test cases as a comma-separated list of lists enclosed in square brackets. For example, the format must be like $[\\,[s_{1,1}, s_{1,2}, \\dots],\\,[s_{2,1}, \\dots],\\,[s_{3,1}, \\dots]\\,]$ where $s_{i,j}$ are integers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case represented as a list of integers, and without any additional text. For example: $[[0,0,1],[1],[2,2,2]]$.\n\nAll fragment lengths must be interpreted in base pairs, counts are dimensionless, and probabilities are unitless. The final outputs are dimensionless integers representing state indices.", "solution": "The problem presented is a canonical application of a Hidden Markov Model (HMM) to segment a sequence of observations into a sequence of generating hidden states. Specifically, we must find the most probable sequence of chromatin accessibility states given a series of observations from a simplified ATAC-seq experiment. The appropriate algorithm for this task is the Viterbi algorithm, which finds this most likely state sequence through dynamic programming. The requirement for numerical stability dictates that all calculations must be performed in logarithmic space to prevent arithmetic underflow from the multiplication of many small probability values.\n\nFirst, we formalize the components of the specified Hidden Markov Model.\nLet the set of hidden states be $S = \\{s_0, s_1, s_2\\}$, corresponding to 'accessible', 'nucleosome-array', and 'inaccessible' chromatin, indexed by $i, j \\in \\{0, 1, 2\\}$.\nThe observation sequence is $O = (o_1, o_2, \\ldots, o_T)$, where $T$ is the length of the sequence. Each observation $o_t$ is a pair $(x_t, k_t)$, where $x_t$ is the fragment length and $k_t$ is the local fragment count.\n\nThe HMM is defined by the following parameters:\n1.  **Initial State Probabilities ($\\pi$)**: The probability of starting in state $s_i$ is $\\pi_i = P(q_1 = s_i)$. The problem specifies a uniform initial distribution: $\\pi = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$. In log-space, $\\log(\\pi_i) = -\\log(3)$ for all $i$.\n\n2.  **State Transition Probabilities ($A$)**: The probability of transitioning from state $s_i$ to state $s_j$ is $A_{ij} = P(q_{t+1} = s_j | q_t = s_i)$. The given transition matrix is:\n    $$\n    A = \\begin{pmatrix} 0.96  0.02  0.02 \\\\ 0.02  0.96  0.02 \\\\ 0.02  0.02  0.96 \\end{pmatrix}\n    $$\n    We will work with the matrix of log-probabilities, $\\log(A_{ij})$.\n\n3.  **Emission Probabilities ($b$)**: The probability of observing $o_t = (x_t, k_t)$ given the system is in state $s_j$ is $b_j(o_t) = P(o_t | q_t = s_j)$. The problem states that fragment length $x_t$ and count $k_t$ are conditionally independent given the state. Thus, the joint emission probability is the product of the individual probabilities:\n    $$\n    b_j(o_t) = P(x_t | q_t=s_j) \\cdot P(k_t | q_t=s_j)\n    $$\n    The fragment length $x_t$ is modeled by a Normal (Gaussian) distribution, $p_{\\text{Normal}}(x_t | \\mu_j, \\sigma_j)$, and the count $k_t$ by a Poisson distribution, $p_{\\text{Poisson}}(k_t | \\lambda_j)$. The parameters are state-dependent:\n    -   State $s_0$: $\\mu_0 = 50$, $\\sigma_0 = 15$, $\\lambda_0 = 8$.\n    -   State $s_1$: $\\mu_1 = 200$, $\\sigma_1 = 25$, $\\lambda_1 = 3$.\n    -   State $s_2$: $\\mu_2 = 300$, $\\sigma_2 = 40$, $\\lambda_2 = 0.3$.\n\nThe log of the emission probability is the sum of the log-probabilities of the two distributions:\n$$\n\\log(b_j(x_t, k_t)) = \\log(p_{\\text{Normal}}(x_t | \\mu_j, \\sigma_j)) + \\log(p_{\\text{Poisson}}(k_t | \\lambda_j))\n$$\nThe log-probability density function of the Normal distribution is:\n$$\n\\log(p_{\\text{Normal}}(x | \\mu, \\sigma)) = -\\log(\\sigma\\sqrt{2\\pi}) - \\frac{(x - \\mu)^2}{2\\sigma^2} = -\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi) - \\frac{(x - \\mu)^2}{2\\sigma^2}\n$$\nThe log-probability mass function of the Poisson distribution is:\n$$\n\\log(p_{\\text{Poisson}}(k | \\lambda)) = \\log\\left(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\right) = k\\log(\\lambda) - \\lambda - \\log(k!)\n$$\nThe term $\\log(k!)$ is computed numerically as $\\log(\\Gamma(k+1))$, where $\\Gamma$ is the Gamma function.\n\nThe Viterbi algorithm finds the most probable sequence of states $Q^* = (q_1^*, q_2^*, \\ldots, q_T^*)$ by finding a path through the state-time trellis that maximizes the joint probability $P(O, Q)$. In log-space, this is equivalent to maximizing the sum of log-probabilities. We define two dynamic programming tables:\n-   $V_t(j)$: The log-probability of the most probable state sequence of length $t$ ending in state $s_j$.\n-   $\\psi_t(j)$: The backpointer, which stores the index of the most likely predecessor state for a path ending in state $s_j$ at time $t$.\n\nThe algorithm proceeds in three steps:\n1.  **Initialization ($t=1$)**: For each state $j \\in \\{0, 1, 2\\}$, we compute the log-probability for the first observation:\n    $$\n    V_1(j) = \\log(\\pi_j) + \\log(b_j(o_1))\n    $$\n    The backpointers $\\psi_1(j)$ are not needed for the first step and can be initialized to $0$.\n\n2.  **Recursion ($t=2, \\ldots, T$)**: We iterate through each time step and each state, extending the most probable paths from time $t-1$. For each state $j \\in \\{0, 1, 2\\}$:\n    $$\n    V_t(j) = \\log(b_j(o_t)) + \\max_{i \\in \\{0,1,2\\}} \\left( V_{t-1}(i) + \\log(A_{ij}) \\right)\n    $$\n    $$\n    \\psi_t(j) = \\arg\\max_{i \\in \\{0,1,2\\}} \\left( V_{t-1}(i) + \\log(A_{ij}) \\right)\n    $$\n    Here, the sum inside the $\\max$ function corresponds to the multiplication of probabilities: $P(\\text{path to } i) \\cdot P(\\text{transition } i \\to j)$. The maximization finds the single most probable path leading to state $j$ at time $t$.\n\n3.  **Termination and Path Backtracking**: After completing the tables up to time $T$, the log-probability of the optimal path is $P^* = \\max_{j \\in \\{0,1,2\\}} (V_T(j))$. The final state of the path is:\n    $$\n    q_T^* = \\arg\\max_{j \\in \\{0,1,2\\}} (V_T(j))\n    $$\n    We then backtrack from this final state to reconstruct the entire path. For $t = T-1$ down to $1$:\n    $$\n    q_t^* = \\psi_{t+1}(q_{t+1}^*)\n    $$\n    This yields the optimal state sequence $Q^* = (q_1^*, \\ldots, q_T^*)$. This procedure is implemented for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Implements the Viterbi algorithm in log-space to decode HMM state sequences\n    for chromatin accessibility from ATAC-seq features.\n    \"\"\"\n    # HMM parameters defined in the problem statement.\n    # States: 0: accessible, 1: nucleosome-array, 2: inaccessible\n    \n    # Initial probabilities (log-space)\n    pi = np.array([1/3, 1/3, 1/3])\n    log_pi = np.log(pi)\n\n    # Transition matrix (log-space)\n    A = np.array([\n        [0.96, 0.02, 0.02],\n        [0.02, 0.96, 0.02],\n        [0.02, 0.02, 0.96]\n    ])\n    log_A = np.log(A)\n\n    # Emission parameters\n    mus = np.array([50.0, 200.0, 300.0])      # Means for Gaussian (fragment length)\n    sigmas = np.array([15.0, 25.0, 40.0])    # Stds for Gaussian\n    lambdas = np.array([8.0, 3.0, 0.3])      # Rates for Poisson (counts)\n\n    # Test cases from the problem statement\n    test_cases = [\n        # Test case 1 (happy path, length 8)\n        [(55, 9), (48, 7), (60, 8), (195, 3), (210, 4), (185, 3), (330, 0), (345, 1)],\n        # Test case 2 (boundary condition, length 1)\n        [(52, 8)],\n        # Test case 3 (transition behavior, length 5)\n        [(50, 8), (62, 7), (70, 6), (205, 4), (198, 3)]\n    ]\n\n    def log_emission_prob(obs, state_idx):\n        \"\"\"\n        Calculates the log-probability of an observation given a state.\n        The observation is a tuple (fragment_length, count).\n        \"\"\"\n        x, k = obs\n        \n        # Log-PDF of Normal distribution for fragment length x\n        mu = mus[state_idx]\n        sigma = sigmas[state_idx]\n        log_p_normal = -np.log(sigma) - 0.5 * np.log(2 * np.pi) - ((x - mu)**2) / (2 * sigma**2)\n        \n        # Log-PMF of Poisson distribution for count k\n        lambda_ = lambdas[state_idx]\n        # Use gammaln(k + 1) for log(k!) which is numerically stable\n        log_p_poisson = k * np.log(lambda_) - lambda_ - gammaln(k + 1)\n        \n        return log_p_normal + log_p_poisson\n\n    results = []\n    for observations in test_cases:\n        T = len(observations)\n        N = len(pi)  # Number of states\n        \n        # Dynamic programming tables\n        # Viterbi table V stores log-probabilities\n        V = np.zeros((T, N))\n        # Backpointer table psi stores the previous state index\n        psi = np.zeros((T, N), dtype=int)\n        \n        # Initialization step (t=0)\n        for j in range(N):\n            V[0, j] = log_pi[j] + log_emission_prob(observations[0], j)\n        \n        # Recursion step (t=1 to T-1)\n        for t in range(1, T):\n            for j in range(N):\n                # Calculate the log-probabilities of reaching state j from any previous state i\n                prev_log_probs = V[t-1, :] + log_A[:, j]\n                \n                # Find the maximum log-probability and the originating state\n                max_log_prob = np.max(prev_log_probs)\n                argmax_state = np.argmax(prev_log_probs)\n                \n                # Update tables with the maximum probability and backpointer\n                V[t, j] = max_log_prob + log_emission_prob(observations[t], j)\n                psi[t, j] = argmax_state\n        \n        # Termination and Backtracking\n        path = np.zeros(T, dtype=int)\n        \n        # Find the last state of the most likely path\n        path[T-1] = np.argmax(V[T-1, :])\n        \n        # Backtrack to reconstruct the full path\n        for t in range(T-2, -1, -1):\n            path[t] = psi[t+1, path[t+1]]\n            \n        results.append(path.tolist())\n\n    # Format the final output to match the specified format \"[[i,j,...],[k,...]]\"\n    output_str = '[' + ','.join(str(r) for r in results) + ']'\n    final_output_str = output_str.replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n\n```", "id": "2378334"}]}