{"hands_on_practices": [{"introduction": "Before diving into sophisticated solutions, it is crucial to understand the problem they are trying to solve. This first exercise demonstrates the fundamental reason why robust statistical controls are necessary in high-throughput biology, such as proteomics. By calculating the expected number of false positives that arise from using a naive $p$-value threshold, you will gain a direct, quantitative appreciation for the scale of the multiple hypothesis testing problem [@problem_id:2389430]. This foundational calculation powerfully illustrates why a conventional cutoff like $p \\lt 0.05$ is untenable in large-scale studies and directly motivates the development of methods that control the False Discovery Rate.", "problem": "A shotgun, bottom-up proteomics database search is performed on a human cell lysate. For each protein entry in the reference database, a hypothesis test is conducted that outputs a calibrated per-protein $p$-value for the null hypothesis that the protein is absent. The reference database contains $20{,}100$ distinct protein entries. In this experiment’s ground truth, exactly $3{,}100$ proteins are truly present in the sample; the remaining entries correspond to proteins that are absent. Assume that the per-protein tests are valid and independent, and that under the null hypothesis the $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$. An analyst naively declares a protein to be identified if its reported $p$-value satisfies $p<0.05$.\n\nUsing only the information above and first principles, compute the expected number of false positive protein identifications under this naive thresholding. Report your answer as a single real number without units. Do not round.", "solution": "The problem statement has been critically validated and is deemed to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The premises are standard in the field of statistical bioinformatics for analyzing proteomics data. We may now proceed with the solution.\n\nLet $M$ denote the total number of distinct protein entries in the reference database. From the problem statement, we are given $M = 20,100$.\nLet $S$ be the number of proteins that are truly present in the sample. We are given $S = 3,100$.\nThe remaining proteins in the database are truly absent. The null hypothesis, stating that a protein is absent, is true for these proteins. Let $M_0$ be the number of such proteins.\n$$M_0 = M - S = 20,100 - 3,100 = 17,000$$\nA protein identification is declared if its associated $p$-value is less than a given threshold, $\\alpha$. The problem specifies this threshold as $\\alpha = 0.05$.\nA \"false positive\" identification occurs when a protein that is truly absent (i.e., one of the $M_0$ proteins) is incorrectly declared as present. This happens if the $p$-value for a truly absent protein is less than $\\alpha$.\n\nWe are asked to compute the expected number of false positive protein identifications. Let us denote the total number of false positives by the random variable $V$.\nFor each of the $M_0$ truly absent proteins, let us define an indicator random variable, $X_i$, for $i = 1, 2, \\dots, M_0$.\n$$\nX_i = \n\\begin{cases} \n1 & \\text{if protein } i \\text{ is a false positive} \\\\\n0 & \\text{otherwise} \n\\end{cases}\n$$\nThe total number of false positives, $V$, is the sum of these indicator variables over all proteins for which the null hypothesis is true:\n$$V = \\sum_{i=1}^{M_0} X_i$$\nWe seek the expectation of $V$, denoted $E[V]$. By the linearity of expectation, we have:\n$$E[V] = E\\left[\\sum_{i=1}^{M_0} X_i\\right] = \\sum_{i=1}^{M_0} E[X_i]$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, for each $i$:\n$$E[X_i] = P(X_i = 1)$$\nThe event $X_i = 1$ occurs if the $p$-value for the $i$-th null protein, let's call it $p_i$, satisfies $p_i < \\alpha$.\n$$E[X_i] = P(p_i < \\alpha)$$\nThe problem states that for proteins under the null hypothesis, the $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$.\nFor a random variable $P \\sim \\mathrm{Uniform}(0,1)$, its probability density function (PDF) is $f(p) = 1$ for $p \\in [0, 1]$ and $f(p) = 0$ otherwise. The probability $P(P < \\alpha)$ is given by the integral of the PDF from $0$ to $\\alpha$:\n$$P(P < \\alpha) = \\int_{0}^{\\alpha} f(p) \\,dp = \\int_{0}^{\\alpha} 1 \\,dp = [p]_{0}^{\\alpha} = \\alpha$$\nTherefore, for each of the $M_0$ null proteins, the probability of it being a false positive is $\\alpha$.\n$$E[X_i] = \\alpha = 0.05 \\quad \\text{for all } i=1, \\dots, M_0$$\nSince the expectation $E[X_i]$ is the same for all $M_0$ proteins, we can write:\n$$E[V] = \\sum_{i=1}^{M_0} \\alpha = M_0 \\cdot \\alpha$$\nSubstituting the known values for $M_0$ and $\\alpha$:\n$$E[V] = 17,000 \\times 0.05$$\n$$E[V] = 17,000 \\times \\frac{5}{100} = 170 \\times 5 = 850$$\nThus, the expected number of false positive protein identifications is $850$. This result follows directly from the definition of a $p$-value and the properties of its theoretical distribution under the null hypothesis. The naive thresholding at $\\alpha$ results in an expected $M_0 \\cdot \\alpha$ false discoveries, a fundamental concept in multiple hypothesis testing.", "answer": "$$\\boxed{850}$$", "id": "2389430"}, {"introduction": "The target-decoy approach is the cornerstone of FDR control in modern proteomics, providing a clever and empirical way to estimate the rate of false positives. This practice delves into the internal mechanics of a properly constructed decoy search, specifically addressing how the results should be interpreted when the size of the decoy database changes. Working through this hypothetical scenario [@problem_id:2389468] reveals the theoretical importance of normalizing the FDR estimator by the decoy multiplicity, a key detail for achieving stable and reliable protein identifications regardless of minor variations in an experimental setup.", "problem": "In a shotgun proteomics experiment, peptide-spectrum matches (PSMs) are identified by searching tandem mass spectra against a concatenated target-decoy database to control the false discovery rate (FDR). Let the decoy multiplicity be $m$, defined as the number of decoy entries per target entry. At a score threshold $ \\tau $, let $T_{m}(\\tau)$ denote the number of accepted target PSMs and $D_{m}(\\tau)$ denote the number of accepted decoy PSMs. The estimated FDR at threshold $ \\tau $ is defined as\n$$\n\\widehat{\\mathrm{FDR}}_{m}(\\tau) \\equiv \\frac{D_{m}(\\tau)}{m\\,T_{m}(\\tau)}.\n$$\nAssume the standard target-decoy modeling assumptions: for each $ \\tau $, the null (incorrect) matches are exchangeable between the target and each decoy entry and independent of $m$, while the distribution of scores for true target PSMs is unaffected by $m$. Under these assumptions, there exist functions $S(\\tau)$ (the number of true target PSMs with scores at least $ \\tau $) and $F(\\tau)$ (the number of false target PSMs with scores at least $ \\tau $) such that, in expectation,\n$$\n\\mathbb{E}[T_{m}(\\tau)] = S(\\tau) + F(\\tau), \\quad \\mathbb{E}[D_{m}(\\tau)] = m\\,F(\\tau).\n$$\nFor a fixed target FDR level $q = 0.01$ (expressed as a decimal), define the selection threshold $ \\tau_{m} $ as the largest threshold satisfying $ \\widehat{\\mathrm{FDR}}_{m}(\\tau_{m}) \\le q $.\n\nYou first perform a search with $m=1$ decoy per target and select $ \\tau_{1} $ accordingly. You then repeat the search with $m=2$ decoys per target and select $ \\tau_{2} $ accordingly. Under the assumptions above, compute the multiplicative factor\n$$\nR \\equiv \\frac{\\mathbb{E}[T_{2}(\\tau_{2})]}{\\mathbb{E}[T_{1}(\\tau_{1})]}.\n$$\nExpress your final answer as a single real number without units. No rounding is required.", "solution": "The goal is to determine how doubling the number of decoy entries affects the expected number of accepted target peptide-spectrum matches (PSMs) when the selection threshold is set to control the false discovery rate (FDR) at a fixed level $q = 0.01$.\n\nBy definition of the estimated false discovery rate at threshold $ \\tau $ with decoy multiplicity $m$,\n$$\n\\widehat{\\mathrm{FDR}}_{m}(\\tau) = \\frac{D_{m}(\\tau)}{m\\,T_{m}(\\tau)}.\n$$\nUnder the stated assumptions, there are functions $S(\\tau)$ and $F(\\tau)$, independent of $m$, such that in expectation\n$$\n\\mathbb{E}[T_{m}(\\tau)] = S(\\tau) + F(\\tau), \\quad \\mathbb{E}[D_{m}(\\tau)] = m\\,F(\\tau).\n$$\nTherefore, the expected estimated FDR at threshold $ \\tau $ is\n$$\n\\mathbb{E}\\big[\\widehat{\\mathrm{FDR}}_{m}(\\tau)\\big] = \\mathbb{E}\\!\\left[\\frac{D_{m}(\\tau)}{m\\,T_{m}(\\tau)}\\right].\n$$\nUsing the proportionality of expectations (and the fact that $S(\\tau)$ and $F(\\tau)$ do not depend on $m$), we analyze the deterministic counterpart obtained by substituting the expectations:\n$$\n\\frac{\\mathbb{E}[D_{m}(\\tau)]}{m\\,\\mathbb{E}[T_{m}(\\tau)]} = \\frac{m\\,F(\\tau)}{m\\,(S(\\tau) + F(\\tau))} = \\frac{F(\\tau)}{S(\\tau) + F(\\tau)}.\n$$\nThis expression is independent of $m$. The selection threshold $ \\tau_{m} $ is defined as the largest $ \\tau $ such that $ \\widehat{\\mathrm{FDR}}_{m}(\\tau) \\le q $. In expectation, the criterion becomes\n$$\n\\frac{F(\\tau)}{S(\\tau) + F(\\tau)} \\le q,\n$$\nwhich does not involve $m$. Consequently, the threshold that satisfies the expected FDR constraint is the same for all $m$, i.e.,\n$$\n\\tau_{2} = \\tau_{1} \\quad \\text{(in expectation)}.\n$$\nAt this common threshold, the expected number of accepted target PSMs is\n$$\n\\mathbb{E}[T_{m}(\\tau_{m})] = S(\\tau_{m}) + F(\\tau_{m}),\n$$\nwhich is also independent of $m$. Therefore,\n$$\nR \\equiv \\frac{\\mathbb{E}[T_{2}(\\tau_{2})]}{\\mathbb{E}[T_{1}(\\tau_{1})]} = \\frac{S(\\tau_{1}) + F(\\tau_{1})}{S(\\tau_{1}) + F(\\tau_{1})} = 1.\n$$\nThus, theoretically and in expectation, doubling the number of decoy entries does not change the number of accepted target PSMs at a fixed FDR when the decoy-based FDR estimator is properly scaled by the decoy multiplicity $m$.", "answer": "$$\\boxed{1}$$", "id": "2389468"}, {"introduction": "The power of the target-decoy method rests on a critical assumption: for any spectrum that does not correspond to a real peptide in the sample, the chances of it matching a target sequence versus a decoy sequence should be equal. This final practice moves from pure calculation to conceptual design, challenging you to critically evaluate a proposed \"pseudo-decoy\" strategy that violates this core principle [@problem_id:2389441]. By analyzing the severe pitfalls of this flawed approach, you will solidify your understanding of what constitutes a valid decoy database and learn about the proper use of an \"entrapment\" database as an essential quality control measure.", "problem": "You are planning a shotgun liquid chromatography–tandem mass spectrometry experiment on a human cell lysate and want to control the False Discovery Rate (FDR) at the peptide-spectrum match (PSM) level. Instead of using conventional reversed or shuffled decoys, a colleague suggests appending the complete Saccharomyces cerevisiae proteome to the human reference proteome and treating yeast as a “pseudo-decoy” to estimate and control FDR via target–decoy competition. Assume tryptic digestion with standard specificity, typical mass tolerances, and that the yeast proteome is roughly $1/3$ the size of the human proteome in terms of the number of proteins. Consider the general behavior of database searches, that incorrect PSMs tend to be distributed across the available peptide search space, and that some tryptic peptides are conserved across distant species. No explicit formulas are provided, and you must reason from first principles (definitions of FDR and properties of decoy-based estimation).\n\nWhich of the following statements best capture a sound experimental design and the severe pitfalls of this “pseudo-decoy” approach? Choose all that apply.\n\nA. Using a distant-species “pseudo-decoy” as the only decoy breaks the equal-chance premise of target–decoy competition because the pseudo-decoy peptide space and composition differ from the human target. If the pseudo-decoy database is substantially smaller than the target, the observed number of pseudo-decoy PSMs among incorrect matches will be reduced roughly in proportion to search-space size, tending to underestimate FDR at the PSM level unless an explicit and accurate search-space scaling is applied.\n\nB. Because some tryptic peptides are conserved or highly similar between human and yeast, a fraction of true human spectra can achieve top scores against yeast entries and be labeled as decoy under competition. This inflates the pseudo-decoy hit count with genuine signals, leading to overly conservative FDR estimates and a loss of sensitivity.\n\nC. A more defensible design is to keep a standard reversed (or shuffled) decoy for the combined target (human) and entrapment (yeast) databases, control PSM-level FDR using conventional target–decoy competition on these synthetic decoys, and then use the fraction of accepted PSMs assigned to yeast targets as an empirical “entrapment” calibration check for the FDR. This provides a way to detect miscalibration caused by database or scoring mismatches.\n\nD. Using yeast as a pseudo-decoy eliminates protein-inference ambiguity because human and yeast share no tryptic peptides of identical mass within typical mass tolerances, so shared-peptide problems no longer arise.\n\nE. If the yeast database is approximately $3$ times smaller than the human database, then estimating FDR as $3$ times the ratio of yeast hits to human hits guarantees an unbiased FDR estimate, because database size alone determines decoy behavior regardless of sequence composition and homologous peptides.", "solution": "The problem statement describes a proposed method for false discovery rate (FDR) control in proteomics that substitutes a standard synthetic decoy database (e.g., reversed sequences) with a \"pseudo-decoy\" database derived from the proteome of a distant species, *Saccharomyces cerevisiae* (yeast), for the analysis of a human sample. This proposal must be evaluated based on the fundamental principles of target-decoy competition (TDC).\n\nThe core principle of TDC for FDR estimation is that for any given experimental spectrum that does not originate from a true peptide in the sample (the null case), the probability of it matching a sequence in the target database is equal to the probability of it matching a sequence in the decoy database. To achieve this \"equal-chance\" premise, a standard decoy database is constructed to have the same size, amino acid composition, and precursor mass distribution as the target database. This is typically achieved by reversing or shuffling the sequences of the target proteins. The proposed method violates this fundamental premise in several critical ways.\n\nLet $T$ denote the human target database and $D_{pseudo}$ denote the yeast \"pseudo-decoy\" database. The sample contains only human peptides.\n\n$1$. **Database Size and Composition Mismatch**: The problem states that the yeast proteome is approximately $\\frac{1}{3}$ the size of the human proteome in terms of protein count, which implies a significantly smaller peptide search space, i.e., $|D_{pseudo}| \\approx \\frac{1}{3} |T|$. Because incorrect peptide-spectrum matches (PSMs) are assumed to be distributed somewhat randomly across the available peptide search space, an incorrect spectrum is approximately $3$ times more likely to find a spurious match in the larger human database than in the smaller yeast database. Furthermore, the amino acid composition and resulting tryptic peptide properties (e.g., length, charge, hydrophobicity) of yeast differ from humans. This means the score distribution for incorrect matches against yeast peptides will not be the same as for incorrect matches against human peptides. Both the size and composition differences break the equal-chance premise of TDC.\n\n$2$. **Presence of Conserved Peptides**: Evolutionarily conserved proteins (e.g., histones, actin, metabolic enzymes) share identical or nearly identical tryptic peptide sequences between human and yeast. A true spectrum from a human peptide that is conserved in yeast can match the yeast peptide sequence with a high score. In a competitive search, if the match to the yeast peptide is the top-scoring one, this genuine signal from the human sample will be incorrectly classified as a \"decoy\" hit.\n\nWith these principles in mind, we evaluate each option.\n\n**A. Using a distant-species “pseudo-decoy” as the only decoy breaks the equal-chance premise of target–decoy competition because the pseudo-decoy peptide space and composition differ from the human target. If the pseudo-decoy database is substantially smaller than the target, the observed number of pseudo-decoy PSMs among incorrect matches will be reduced roughly in proportion to search-space size, tending to underestimate FDR at the PSM level unless an explicit and accurate search-space scaling is applied.**\n\nThis statement is a correct and precise description of one of the primary flaws. It correctly identifies that the difference in database size and composition breaks the foundational \"equal-chance\" premise of TDC. With $|D_{pseudo}| < |T|$, the number of null matches found in the pseudo-decoy database, $N_{D_{pseudo}}$, will be proportionally smaller than the number of null matches found in the target database, $N_{T, \\text{FP}}$. Using $N_{D_{pseudo}}$ as an estimator for the number of false positives in the target list, $N_{T, \\text{FP}}$, without correction, will lead to a significant underestimation of the true FDR. For example, a naive estimate might be $\\text{FDR}_{\\text{est}} = N_{D_{pseudo}} / N_T$, while the true FDR is closer to $(|T|/|D_{pseudo}|) \\times (N_{D_{pseudo}} / N_T)$. This underestimation allows an unacceptably high number of false positives to pass the filter.\n**Verdict: Correct.**\n\n**B. Because some tryptic peptides are conserved or highly similar between human and yeast, a fraction of true human spectra can achieve top scores against yeast entries and be labeled as decoy under competition. This inflates the pseudo-decoy hit count with genuine signals, leading to overly conservative FDR estimates and a loss of sensitivity.**\n\nThis statement accurately describes the second major pitfall. True signals from human peptides that are conserved in yeast can be misclassified as decoy hits. This inflates the number of observed pseudo-decoy hits, $N_{D_{pseudo}}$. When this inflated number is used in the numerator of the FDR calculation (e.g., $\\text{FDR} \\propto N_{D_{pseudo}}$), the resulting FDR for any given score threshold will be overestimated. To achieve a target FDR, such as $1\\%$, one would need to apply a much more stringent score threshold than is actually necessary. This discards many true positive PSMs that fall below this artificially high threshold, resulting in a loss of sensitivity (i.e., fewer correct identifications). The estimate is thus overly conservative.\n**Verdict: Correct.**\n\n**C. A more defensible design is to keep a standard reversed (or shuffled) decoy for the combined target (human) and entrapment (yeast) databases, control PSM-level FDR using conventional target–decoy competition on these synthetic decoys, and then use the fraction of accepted PSMs assigned to yeast targets as an empirical “entrapment” calibration check for the FDR. This provides a way to detect miscalibration caused by database or scoring mismatches.**\n\nThis statement describes a valid and powerful quality control methodology known as the \"entrapment database\" method. In this design, one is not using the yeast database as the *decoy*, but as a known-negative part of the *target* database. A proper synthetic decoy database (e.g., reversed) is created for the combined human-plus-yeast target database. This allows for a valid FDR calculation using standard TDC. After filtering the PSMs at a target FDR (e.g., $1\\%$), the accepted hits assigned to yeast peptides can be counted. Since the sample contains no yeast protein, all such hits are by definition false positives. The rate of these \"entrapment hits\" among all accepted hits serves as an empirical measurement of the actual FDR. If the empirical rate is close to the target FDR, it provides confidence in the results. If it is significantly different, it signals a problem with the analysis that requires investigation. This is an excellent and sound experimental design.\n**Verdict: Correct.**\n\n**D. Using yeast as a pseudo-decoy eliminates protein-inference ambiguity because human and yeast share no tryptic peptides of identical mass within typical mass tolerances, so shared-peptide problems no longer arise.**\n\nThis statement is factually incorrect. The central premise, that \"human and yeast share no tryptic peptides of identical mass,\" is false. Many fundamental proteins are highly conserved across eukaryotes, leading to a significant number of identical or isobaric tryptic peptides. Thus, the problem of shared peptides, which complicates the inference of proteins from identified peptides, is not eliminated and may even be exacerbated by adding a large set of homologous proteins to the search space.\n**Verdict: Incorrect.**\n\n**E. If the yeast database is approximately $3$ times smaller than the human database, then estimating FDR as $3$ times the ratio of yeast hits to human hits guarantees an unbiased FDR estimate, because database size alone determines decoy behavior regardless of sequence composition and homologous peptides.**\n\nThis statement proposes a naive correction for the size difference, $\\text{FDR}_{\\text{est}} = 3 \\times (N_{D_{pseudo}} / N_T)$. While correcting for size is a step in the right direction, the statement makes two fatally strong claims. First, it asserts that this correction \"guarantees an unbiased FDR estimate.\" This is false because it ignores the two other critical, confounding factors: ($1$) differences in sequence composition between human and yeast, which affects score distributions of null matches, and ($2$) the inflation of yeast hits by conserved true human peptides (as described in option B), which biases the estimate in the opposite direction. The net result of these competing biases is unknown and certainly not guaranteed to be unbiased. Second, it wrongly claims that \"database size alone determines decoy behavior.\" This is a gross oversimplification that contradicts the principles of TDC.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABC}$$", "id": "2389441"}]}