{"hands_on_practices": [{"introduction": "The quality of any computational analysis is fundamentally limited by the quality of the experimental data. This practice challenges you to think like an experimental designer by exploring how a key choice in the Hi-C protocol—the type of restriction enzyme used—directly influences the ultimate resolution of your analysis [@problem_id:2437228]. By reasoning from first principles about DNA sequence statistics, you will gain a deeper appreciation for how wet-lab decisions set the stage for identifying genomic features like Topologically Associating Domain (TAD) boundaries.", "problem": "A laboratory plans two in situ High-throughput chromosome conformation capture (Hi-C) experiments on the same mammalian cell line to identify topologically associating domain (TAD) boundaries. In experiment X, they use a restriction enzyme that recognizes a $k = 4$ nucleotide site (a \"$4$-cutter\"); in experiment Y, they use a restriction enzyme that recognizes a $k = 6$ nucleotide site (a \"$6$-cutter\"). Both experiments are performed with the same library preparation protocol, the same number of uniquely mapped read pairs ($N = 5\\times 10^8$ contacts), and comparable duplication rates. The genome size is approximately $G = 3\\times 10^9$ base pairs (bp). The group will call TAD boundaries using an insulation-score method that aggregates contacts in a symmetric window and seeks minima in local insulation.\n\nFrom first principles in nucleic acid sequence statistics and Hi-C contact sampling, reason about how the choice of restriction enzyme affects the achievable resolution and accuracy of TAD boundary localization. Assume, as a first approximation, that bases are independently and identically distributed with equal probability for each nucleotide, that the expected spacing between recognition sites scales inversely with their probability, and that boundary detection requires sufficient contact density per genomic bin. Which of the following statements is/are most accurate?\n\nA. Using a $4$-cutter increases the potential precision of boundary localization because the average restriction fragment length is shorter, yielding denser sampling of contacts near boundaries; however, to realize finer-bin calling without excessive noise, greater sequencing depth per base pair is required compared to a $6$-cutter.\n\nB. Using a $6$-cutter increases boundary resolution because longer fragments span across boundaries more efficiently, producing sharper insulation-score minima even at the same read depth.\n\nC. The choice of restriction enzyme has no practical impact on boundary resolution, because TADs are defined at the megabase scale and are insensitive to restriction fragment size.\n\nD. With the same read depth, a $6$-cutter will typically yield more uniform coverage across the genome and therefore more accurate boundary positions than a $4$-cutter.\n\nE. A $6$-cutter can miss narrow boundary features if recognition sites are sparse near a boundary, biasing called boundary positions toward the nearest available restriction sites and effectively broadening boundary estimates compared to a $4$-cutter.\n\nSelect all that apply.", "solution": "The problem statement is scientifically grounded, well-posed, and contains sufficient information for a rigorous analysis. It describes a standard experimental design choice in the field of chromosome conformation capture and poses a valid question about its consequences. The provided parameters and assumptions are reasonable for a first-principles evaluation. Therefore, the problem is valid, and a solution will be derived.\n\nThe core of the problem is to compare a High-throughput chromosome conformation capture (Hi-C) experiment using a restriction enzyme that recognizes a $k=4$ nucleotide site (a \"$4$-cutter\") with one using an enzyme for a $k=6$ site (a \"$6$-cutter\").\n\nFrom first principles of sequence statistics, assuming an independent and identically distributed (i.i.d.) sequence with equal nucleotide probabilities ($P(A) = P(C) = P(G) = P(T) = 1/4$), the probability $P_k$ of finding a specific $k$-mer recognition site is:\n$$P_k = \\left(\\frac{1}{4}\\right)^k$$\nThe expected distance between sites, which corresponds to the average restriction fragment length $L_k$, is the inverse of this probability.\n\nFor Experiment X (the \"$4$-cutter\"):\n$$k = 4$$\n$$P_4 = \\left(\\frac{1}{4}\\right)^4 = \\frac{1}{256}$$\n$$L_4 = \\frac{1}{P_4} = 256 \\text{ bp}$$\n\nFor Experiment Y (the \"$6$-cutter\"):\n$$k = 6$$\n$$P_6 = \\left(\\frac{1}{4}\\right)^6 = \\frac{1}{4096}$$\n$$L_6 = \\frac{1}{P_6} = 4096 \\text{ bp}$$\n\nThe number of restriction sites in the genome of size $G \\approx 3 \\times 10^9$ bp is approximately $G/L_k$.\n-   For the \"$4$-cutter\": $N_{sites,4} \\approx (3 \\times 10^9) / 256 \\approx 1.17 \\times 10^7$ sites.\n-   For the \"$6$-cutter\": $N_{sites,6} \\approx (3 \\times 10^9) / 4096 \\approx 7.3 \\times 10^5$ sites.\n\nThe fundamental principles for evaluating the experiments are:\n1.  **Resolution Limit**: The ultimate spatial resolution of a Hi-C experiment is limited by the density of restriction sites. A contact is detected between two fragments. The location of the contact is thus resolved only to the level of the fragments involved. Shorter fragments allow for more precise localization of interaction endpoints.\n2.  **Sampling Density**: Both experiments have the same total number of read pairs, $N = 5 \\times 10^8$. These reads are distributed among all possible pairs of restriction fragments. The \"$4$-cutter\" experiment generates a much larger number of fragments, meaning the $N$ reads are sampled from a much larger pool of potential interactions.\n3.  **Coverage Uniformity**: A denser set of restriction sites (from the \"$4$-cutter\") provides more even coverage across the genome. A sparse set of sites (from the \"$6$-cutter\") can lead to large genomic regions with no cut sites, creating \"blind spots\" and non-uniform coverage.\n4.  **Signal-to-Noise Ratio**: For a fixed genomic bin size, the number of reads per bin is a critical factor. To achieve high resolution, one must use small bins. With a fixed total number of reads $N$, smaller bins will contain fewer reads, increasing the statistical noise.\n\nWith these principles, we evaluate each statement.\n\n**A. Using a $4$-cutter increases the potential precision of boundary localization because the average restriction fragment length is shorter, yielding denser sampling of contacts near boundaries; however, to realize finer-bin calling without excessive noise, greater sequencing depth per base pair is required compared to a $6$-cutter.**\n-   The first part of the statement, asserting that a \"$4$-cutter\" increases potential precision due to shorter fragments ($L_4 \\approx 256$ bp), is correct. Shorter fragments act as finer \"rulers\" for mapping interaction locations. This yields denser sampling in a literal sense: the potential anchor points for interactions are closer together.\n-   The second part addresses the trade-off. To leverage this higher potential precision, one must analyze the data using smaller genomic bins. With a fixed total read count $N$, smaller bins mean fewer counts per bin, which reduces the signal-to-noise ratio. To populate these finer bins with a statistically significant number of contacts, a proportionally larger total number of reads (greater sequencing depth) would be necessary. This is a classic resolution-versus-depth trade-off in sequencing-based genomics assays.\n-   Verdict: **Correct**.\n\n**B. Using a $6$-cutter increases boundary resolution because longer fragments span across boundaries more efficiently, producing sharper insulation-score minima even at the same read depth.**\n-   This statement claims a \"$6$-cutter\" *increases* resolution. This is fundamentally incorrect. Resolution is inversely related to the size of the minimal feature one can resolve. The larger fragments from a \"$6$-cutter\" ($L_6 \\approx 4096$ bp) provide a coarser-grained view of the genome, thus *decreasing* the theoretical resolution compared to a \"$4$-cutter\".\n-   The reasoning that \"longer fragments span across boundaries more efficiently\" is misguided. The goal of an insulation score is to detect a deficit of interactions *across* a boundary. The fact that a single fragment might span a boundary is a source of ambiguity, not a feature that terminates on this fragment, thus blurring the apparent location of the boundary.\n-   Consequently, a \"$6$-cutter\" would produce *broader*, not sharper, insulation-score minima because the signal is averaged over larger fragment lengths.\n-   Verdict: **Incorrect**.\n\n**C. The choice of restriction enzyme has no practical impact on boundary resolution, because TADs are defined at the megabase scale and are insensitive to restriction fragment size.**\n-   While topologically associating domains (TADs) are large structures, often in the range of hundreds of kilobases to megabases, their boundaries are often sharp, well-defined regions. The precise localization of these boundaries is a key objective and is known to be associated with specific genomic elements like CTCF binding sites.\n-   The difference in average fragment length between a \"$4$-cutter\" ($256$ bp) and a \"$6$-cutter\" ($4096$ bp) is an order of magnitude. This difference is highly relevant when trying to pinpoint a boundary region that may be only a few kilobases wide. Therefore, the choice of enzyme has a significant practical impact on the precision of boundary localization.\n-   Verdict: **Incorrect**.\n\n**D. With the same read depth, a $6$-cutter will typically yield more uniform coverage across the genome and therefore more accurate boundary positions than a $4$-cutter.**\n-   This statement claims a \"$6$-cutter\" provides more uniform coverage. This is the opposite of what occurs. A \"$4$-cutter\" has sites approximately every $256$ bp, while a \"$6$-cutter\" has sites every $4096$ bp. The sparser distribution of \"$6$-cutter\" sites means there is a much higher probability of large genomic regions existing that contain no restriction sites. This leads to gaps and highly non-uniform coverage. A \"$4$-cutter\" provides a much more even sampling of the genome.\n-   Because the premise of more uniform coverage is false, the conclusion of more accurate boundary positions is unsupported and incorrect. Biased, non-uniform coverage leads to less accurate, not more accurate, results.\n-   Verdict: **Incorrect**.\n\n**E. A $6$-cutter can miss narrow boundary features if recognition sites are sparse near a boundary, biasing called boundary positions toward the nearest available restriction sites and effectively broadening boundary estimates compared to a $4$-cutter.**\n-   This statement correctly identifies a key problem with using enzymes that cut infrequently. If a true biological boundary lies in a region far from any \"$6$-cutter\" recognition sites, the Hi-C experiment simply cannot detect interactions originating precisely at that boundary.\n-   The interactions that are detected will be anchored at the nearest available restriction sites flanking the boundary. If these sites are, for example, $5$ kb apart, the insulation score minimum will reflect a drop in interactions between these distant fragments, not at the true boundary. This misplaces the called boundary and smears the signal over the entire region between the available sites.\n-   A \"$4$-cutter\", with its dense sites, is much more likely to have sites located very close to the true boundary, allowing for a more accurate and precise (i.e., narrower) boundary call.\n-   Verdict: **Correct**.", "answer": "$$\\boxed{AE}$$", "id": "2437228"}, {"introduction": "After collecting data, a central challenge in bioinformatics is distinguishing meaningful biological signals from random noise. This exercise places you in the role of a data scientist tasked with understanding the baseline behavior of a TAD-calling algorithm [@problem_id:2437207]. By calculating the expected number of TADs that would be identified in a completely random genome lacking any true domain structure, you will develop a crucial intuition for statistical significance and the ever-present problem of multiple hypothesis testing in genomics.", "problem": "Consider a chromosome segment binned into a contact matrix from High-throughput Chromosome Conformation Capture (Hi-C), where distance-dependent decay has been removed by standard observed-over-expected normalization so that, under the null hypothesis of no domain structure, every subset of contacts consistent with a given genomic distance has expectation one. A topologically associating domain (TAD) caller evaluates every contiguous interval of bins of lengths between $L_{\\min}=5$ and $L_{\\max}=20$ (inclusive) within a total of $B=200$ bins. For each candidate interval, the caller performs a valid statistical test for intra-interval contact enrichment whose $p$-value is uniformly distributed on $[0,1]$ when the null hypothesis is true. The caller declares an interval to be a TAD if its $p$-value is less than a fixed per-interval significance threshold $\\alpha=0.01$.\n\nAssume the Hi-C matrix was generated from a completely random polymer model with no true domain structure. Under these conditions, determine the expected number of intervals that would be declared as TADs by this caller. Round your answer to four significant figures. Report your answer as a pure number (count), without any units.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n-   The system is a chromosome segment represented by a Hi-C contact matrix of $B = 200$ bins.\n-   The matrix has been normalized (observed-over-expected), so under the null hypothesis of no domain structure, the expectation of contacts is one.\n-   A TAD caller evaluates every contiguous interval of bins with lengths $L$ such that $L_{\\min} \\le L \\le L_{\\max}$, where $L_{\\min} = 5$ and $L_{\\max} = 20$.\n-   For each candidate interval, a statistical test is performed, and its $p$-value is uniformly distributed on the interval $[0,1]$ under the null hypothesis.\n-   An interval is declared a TAD if its $p$-value is less than the significance threshold $\\alpha = 0.01$.\n-   The system is assumed to follow a random polymer model with no true domain structure, meaning the null hypothesis is true for all tests.\n-   The objective is to find the expected number of intervals declared as TADs.\n-   The final answer should be rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is grounded in the standard practices of computational biology and bioinformatics for analyzing Hi-C data. The concepts of Hi-C contact matrices, observed-over-expected normalization, topologically associating domains (TADs), p-values, and the null hypothesis are fundamental and correctly described. The assumption of a uniform $p$-value distribution under the null hypothesis is a defining property of a valid statistical test.\n-   **Well-Posed:** The problem is well-posed. It provides all necessary parameters ($B$, $L_{\\min}$, $L_{\\max}$, $\\alpha$) and a clear objective. The question is unambiguous and has a unique, computable solution.\n-   **Objective:** The problem is stated in precise, objective language, free of any subjective claims or ambiguity.\n\nStep 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, and objective. A solution will be provided.\n\nThe task is to compute the expected number of false positive TAD discoveries under a global null hypothesis. This expectation is the product of the total number of statistical tests performed and the probability of a single test being significant.\n\nFirst, we must determine the total number of candidate intervals, $N$, that are evaluated by the TAD caller. The caller examines all contiguous intervals of length $L$ for every $L$ in the range $[L_{\\min}, L_{\\max}]$. For a chromosome with $B$ bins, the number of possible contiguous intervals of a fixed length $L$ is given by $B - L + 1$.\nThe total number of intervals tested, $N$, is the sum of the number of intervals for each length from $L_{\\min} = 5$ to $L_{\\max} = 20$.\n$$N = \\sum_{L=L_{\\min}}^{L_{\\max}} (B - L + 1)$$\nSubstituting the given values $B = 200$, $L_{\\min} = 5$, and $L_{\\max} = 20$:\n$$N = \\sum_{L=5}^{20} (200 - L + 1) = \\sum_{L=5}^{20} (201 - L)$$\nThis is a sum of an arithmetic progression. The number of terms in the summation is $k = L_{\\max} - L_{\\min} + 1 = 20 - 5 + 1 = 16$.\nThe first term of the series (for $L=5$) is $a_1 = 201 - 5 = 196$.\nThe last term of the series (for $L=20$) is $a_k = 201 - 20 = 181$.\nThe sum of an arithmetic series is given by $S_k = \\frac{k}{2}(a_1 + a_k)$.\n$$N = \\frac{16}{2} (196 + 181) = 8 \\cdot (377) = 3016$$\nSo, a total of $N=3016$ statistical tests are performed.\n\nNext, we consider the probability of an individual test resulting in a TAD call. Let a random variable $P_i$ represent the $p$-value for the $i$-th test, where $i \\in \\{1, 2, \\dots, N\\}$. The problem states that under the null hypothesis (which is assumed to hold true for the entire matrix), the $p$-value is uniformly distributed on the interval $[0,1]$.\n$$P_i \\sim U(0,1)$$\nAn interval is declared a TAD if its $p$-value is less than the significance threshold $\\alpha = 0.01$. The probability of this event for a single test is:\n$$\\mathbb{P}(P_i < \\alpha) = \\int_0^\\alpha 1 \\,dx = \\alpha = 0.01$$\nThis is the probability of a Type I error for a single test.\n\nLet $X_i$ be an indicator random variable for the $i$-th interval being declared a TAD.\n$X_i = 1$ if $P_i < \\alpha$, and $X_i = 0$ otherwise.\nThe expected value of this indicator variable is:\n$$E[X_i] = 1 \\cdot \\mathbb{P}(X_i = 1) + 0 \\cdot \\mathbb{P}(X_i = 0) = \\mathbb{P}(P_i < \\alpha) = \\alpha$$\nThe total number of intervals declared as TADs is the sum of these indicator variables over all tests: $Y = \\sum_{i=1}^{N} X_i$.\nWe are asked to find the expected number of declared TADs, which is $E[Y]$.\n\nBy the linearity of expectation, the expectation of a sum of random variables is the sum of their individual expectations. This property holds regardless of whether the variables are independent. In this problem, overlapping intervals will lead to dependent test statistics, but this does not affect the calculation of the expected value.\n$$E[Y] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i]$$\nSince $E[X_i] = \\alpha$ for all $i=1, \\dots, N$:\n$$E[Y] = \\sum_{i=1}^{N} \\alpha = N \\cdot \\alpha$$\nSubstituting the calculated value of $N$ and the given value of $\\alpha$:\n$$E[Y] = 3016 \\cdot 0.01 = 30.16$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value, $30.16$, already consists of four significant figures. Therefore, no further rounding is necessary. This value represents the expected number of false positives when applying this TAD calling procedure to a null dataset.", "answer": "$$\\boxed{30.16}$$", "id": "2437207"}, {"introduction": "Moving beyond isolated statistical tests, we can build more powerful, model-based methods to interpret genomic data. This hands-on coding challenge invites you to implement a Hidden Markov Model (HMM), a classic probabilistic tool for sequence segmentation, to identify TAD-related features along a chromosome [@problem_id:2437210]. By applying the Viterbi algorithm to decode the most likely sequence of 'domain,' 'boundary,' and 'inter-domain' states, you will translate theoretical concepts into a practical and powerful analytical pipeline.", "problem": "You are given a formal probabilistic model for a one-dimensional segmentation of a chromosome into regions related to topologically associating domains (TADs). The model is a Hidden Markov Model (HMM) with three hidden states representing segment types: domain, boundary, and inter-domain. Let the hidden state at position $t$ be $z_t \\in \\{0,1,2\\}$, where the mapping is domain $\\rightarrow$ $0$, boundary $\\rightarrow$ $1$, inter-domain $\\rightarrow$ $2$. Let the observed data at position $t$ be a single real number $x_t \\in \\mathbb{R}$ representing a normalized summary statistic derived from a chromosome contact matrix; this statistic is dimensionless and has no physical unit. The HMM is defined by the following components:\n\n- Initial state distribution $\\boldsymbol{\\pi}$ over states $\\{0,1,2\\}$:\n  - $\\pi_0 = 0.6$\n  - $\\pi_1 = 0.1$\n  - $\\pi_2 = 0.3$\n\n- State transition probability matrix $\\mathbf{A}$ where $A_{ij} = \\Pr(z_t=j \\mid z_{t-1}=i)$ for $i,j \\in \\{0,1,2\\}$:\n  - From state $0$: $A_{00} = 0.80$, $A_{01} = 0.10$, $A_{02} = 0.10$\n  - From state $1$: $A_{10} = 0.45$, $A_{11} = 0.10$, $A_{12} = 0.45$\n  - From state $2$: $A_{20} = 0.10$, $A_{21} = 0.10$, $A_{22} = 0.80$\n\n- Emission distributions: for each state $s \\in \\{0,1,2\\}$, the observation $x_t$ is generated according to a univariate normal (Gaussian) distribution with mean $\\mu_s$ and variance $\\sigma_s^2$, independent across $t$ conditioned on $z_t$:\n  - For state $0$ (domain): $\\mu_0 = 0.8$, $\\sigma_0^2 = 0.25$\n  - For state $1$ (boundary): $\\mu_1 = 2.0$, $\\sigma_1^2 = 0.25$\n  - For state $2$ (inter-domain): $\\mu_2 = 0.0$, $\\sigma_2^2 = 0.25$\n\nGiven an observed sequence $\\mathbf{x} = (x_1, x_2, \\dots, x_T)$, define the objective as finding the state sequence $\\mathbf{z}^\\star = (z_1, z_2, \\dots, z_T)$ that maximizes the joint probability $\\Pr(\\mathbf{z}, \\mathbf{x})$ under the specified HMM. If multiple state sequences achieve the same maximum value of $\\Pr(\\mathbf{z}, \\mathbf{x})$, you must break ties deterministically by selecting, at each position $t$ during maximization, the smallest state index in $\\{0,1,2\\}$ that attains the maximum.\n\nYour task is to write a program that, for each test case below, returns the maximizing state sequence $\\mathbf{z}^\\star$ as a list of integers, using the state index mapping domain $\\rightarrow$ $0$, boundary $\\rightarrow$ $1$, inter-domain $\\rightarrow$ $2$.\n\nTest suite (each test case is a list of real-valued observations, dimensionless):\n\n- Test case $1$: $\\left[0.9,\\, 0.7,\\, 0.8,\\, 1.9,\\, 2.2,\\, 0.2,\\, 0.1,\\, 0.0\\right]$\n- Test case $2$: $\\left[2.1\\right]$\n- Test case $3$: $\\left[0.1,\\, 0.0,\\, 0.9,\\, 0.8,\\, 0.7,\\, 0.2\\right]$\n- Test case $4$: $\\left[0.75,\\, 0.85,\\, 0.95,\\, 0.05,\\, -0.1,\\, 0.0,\\, 1.8,\\, 2.1,\\, 1.9,\\, 0.6\\right]$\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the inferred state index list for a test case, in the same order as listed above. For example, the printed line should look like \"[[a1,a2,...],[b1,...],[c1,...],[d1,...]]\" where each symbol is an integer in $\\{0,1,2\\}$.\n\nThe answer for each test case must be a list of integers. No physical units are involved, and no rounding is required beyond standard integer representation.", "solution": "The problem will be validated against the required criteria before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides a complete specification for a Hidden Markov Model (HMM) for chromosome segmentation. The explicit givens are:\n-   **Hidden States**: A set of three hidden states, $z_t \\in \\{0, 1, 2\\}$, corresponding to segment types: domain ($0$), boundary ($1$), and inter-domain ($2$).\n-   **Observations**: A sequence of real-valued, dimensionless scalars $x_t \\in \\mathbb{R}$.\n-   **Initial State Distribution $\\boldsymbol{\\pi}$**: A vector defining the probability of the first state, $\\Pr(z_1=i) = \\pi_i$.\n    -   $\\pi_0 = 0.6$\n    -   $\\pi_1 = 0.1$\n    -   $\\pi_2 = 0.3$\n-   **State Transition Probability Matrix $\\mathbf{A}$**: A matrix where $A_{ij} = \\Pr(z_t=j \\mid z_{t-1}=i)$.\n    -   $A_{0,*} = [0.80, 0.10, 0.10]$\n    -   $A_{1,*} = [0.45, 0.10, 0.45]$\n    -   $A_{2,*} = [0.10, 0.10, 0.80]$\n-   **Emission Distributions**: For each state $s \\in \\{0, 1, 2\\}$, the observation $x_t$ is drawn from a normal distribution $\\mathcal{N}(\\mu_s, \\sigma_s^2)$.\n    -   State $0$ (domain): $\\mu_0 = 0.8$, $\\sigma_0^2 = 0.25$\n    -   State $1$ (boundary): $\\mu_1 = 2.0$, $\\sigma_1^2 = 0.25$\n    -   State $2$ (inter-domain): $\\mu_2 = 0.0$, $\\sigma_2^2 = 0.25$\n-   **Objective**: To find the state sequence $\\mathbf{z}^\\star = (z_1, z_2, \\dots, z_T)$ that maximizes the joint probability $\\Pr(\\mathbf{z}, \\mathbf{x})$.\n-   **Tie-Breaking Rule**: When a maximum is not unique, the state with the smallest index ($0 < 1 < 2$) must be chosen.\n-   **Test Suite**: Four specific observation sequences are provided for evaluation.\n-   **Output Format**: A single-line string representing a list of lists of integers, with no whitespace.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria.\n-   **Scientifically Grounded**: The problem is well-grounded. Hidden Markov Models are a standard, powerful tool in bioinformatics for sequence analysis, including epigenetic and chromatin structure studies like TAD identification. The model presented is a simplified but conceptually sound representation of such a system. All parameters are physically plausible dimensionless quantities or probabilities.\n-   **Well-Posed**: The problem is well-posed. It asks for the most likely hidden state sequence given an observation sequence under a fully specified HMM. This is a classic problem in computational statistics, for which a unique and stable solution is guaranteed to exist. The explicit tie-breaking rule removes any ambiguity.\n-   **Objective**: The problem is objective. It is defined using precise mathematical language and formalisms, leaving no room for subjective interpretation.\n-   **Flaw Checklist**: The problem does not exhibit any of the listed flaws. It is scientifically sound, formalizable, complete, consistent, realistic, and algorithmically verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A rigorous solution will be provided.\n\n### Solution\n\nThe problem requires finding the most probable sequence of hidden states $\\mathbf{z}^\\star = (z_1, \\dots, z_T)$ given an observation sequence $\\mathbf{x} = (x_1, \\dots, x_T)$ in a specified Hidden Markov Model. This is equivalent to finding the sequence $\\mathbf{z}$ that maximizes the joint probability $\\Pr(\\mathbf{z}, \\mathbf{x})$. To avoid numerical underflow, it is standard practice to maximize the logarithm of this probability:\n$$ \\mathbf{z}^\\star = \\arg\\max_{\\mathbf{z}} \\Pr(\\mathbf{z}, \\mathbf{x}) = \\arg\\max_{\\mathbf{z}} \\log \\Pr(\\mathbf{z}, \\mathbf{x}) $$\nThe joint log-probability for a specific path $\\mathbf{z}=(z_1, \\dots, z_T)$ is given by:\n$$ \\log \\Pr(\\mathbf{z}, \\mathbf{x}) = \\log \\pi_{z_1} + \\sum_{t=2}^{T} \\log A_{z_{t-1}, z_t} + \\sum_{t=1}^{T} \\log p(x_t | z_t) $$\nwhere $\\pi_{z_1}$ is the initial probability of state $z_1$, $A_{z_{t-1}, z_t}$ is the transition probability from state $z_{t-1}$ to $z_t$, and $p(x_t | z_t)$ is the emission probability of observation $x_t$ given state $z_t$.\n\nThe definitive algorithm for this task is the **Viterbi algorithm**, a dynamic programming approach. It efficiently computes the most likely state sequence by iteratively building a path. Let us define $v_t(j)$ as the maximum log-probability of any state sequence of length $t$ ending in state $j$, having observed $(x_1, \\dots, x_t)$.\n\nThe algorithm proceeds in four steps:\n\n**1. Initialization ($t=1$)**\nFor each state $j \\in \\{0, 1, 2\\}$, we compute the initial log-probability. This is the sum of the log initial probability and the log emission probability for the first observation $x_1$.\n$$ v_1(j) = \\log \\pi_j + \\log p(x_1 | z_1=j) $$\nThe emission probability $p(x|z=j)$ follows a normal distribution $\\mathcal{N}(\\mu_j, \\sigma_j^2)$. The log-probability density function is:\n$$ \\log p(x | z=j) = -\\frac{(x - \\mu_j)^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2) $$\n\n**2. Recursion ($t=2, \\dots, T$)**\nFor each subsequent time step $t$ and for each state $j \\in \\{0, 1, 2\\}$, we find the path from the previous step that is most likely to transition to state $j$. The log-probability $v_t(j)$ is the log-probability of the best such path up to time $t-1$, plus the log transition probability, plus the log emission probability for the current observation $x_t$.\n$$ v_t(j) = \\log p(x_t | z_t=j) + \\max_{i \\in \\{0,1,2\\}} \\left( v_{t-1}(i) + \\log A_{ij} \\right) $$\nTo reconstruct the path later, we must also store the state $i$ that achieved this maximum. Let this be $\\text{ptr}_t(j)$:\n$$ \\text{ptr}_t(j) = \\arg\\max_{i \\in \\{0,1,2\\}} \\left( v_{t-1}(i) + \\log A_{ij} \\right) $$\nThe problem's tie-breaking rule (choose the smallest state index) is handled by the standard `argmax` function, which returns the first index of a maximum value.\n\n**3. Termination**\nAfter computing the trellis of probabilities up to the final time step $T$, the optimal state for the final position, $z_T^\\star$, is the one with the highest overall log-probability:\n$$ z_T^\\star = \\arg\\max_{j \\in \\{0,1,2\\}} v_T(j) $$\nAgain, the tie-breaking rule applies.\n\n**4. Path Backtracking**\nThe most probable state sequence $\\mathbf{z}^\\star$ is reconstructed by starting from $z_T^\\star$ and tracing backwards using the stored pointers:\n$$ z_{t-1}^\\star = \\text{ptr}_t(z_t^\\star) \\quad \\text{for } t = T, T-1, \\dots, 2 $$\nThis procedure yields the unique, optimal state sequence demanded by the problem statement. The implementation will follow this logic precisely, using numerical logarithms for all probability calculations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the HMM decoding problem for the given test cases using the Viterbi algorithm.\n    \"\"\"\n    \n    # HMM parameters as specified in the problem\n    pi = np.array([0.6, 0.1, 0.3])\n    A = np.array([\n        [0.80, 0.10, 0.10],\n        [0.45, 0.10, 0.45],\n        [0.10, 0.10, 0.80]\n    ])\n    mus = np.array([0.8, 2.0, 0.0])\n    # The problem provides variance (sigma^2), scipy.stats.norm requires standard deviation (sigma)\n    sigmas_sq = np.array([0.25, 0.25, 0.25])\n    sigmas = np.sqrt(sigmas_sq)\n    \n    # Convert probabilities to log-space to prevent underflow and for numerical stability\n    log_pi = np.log(pi)\n    log_A = np.log(A)\n    \n    n_states = len(pi)\n\n    def viterbi(obs_seq):\n        \"\"\"\n        Implements the Viterbi algorithm to find the most likely state sequence.\n\n        Args:\n            obs_seq (list or np.ndarray): The sequence of observations.\n\n        Returns:\n            list: The most likely sequence of state indices.\n        \"\"\"\n        T = len(obs_seq)\n        if T == 0:\n            return []\n\n        # viterbi_trellis[t, j] stores the max log-prob of a path ending in state j at time t\n        viterbi_trellis = np.zeros((T, n_states))\n        # backpointers[t, j] stores the best previous state that leads to state j at time t\n        backpointers = np.zeros((T, n_states), dtype=int)\n\n        # 1. Initialization Step (t=0)\n        log_emissions_t0 = np.array([norm.logpdf(obs_seq[0], loc=mu, scale=sigma) \n                                     for mu, sigma in zip(mus, sigmas)])\n        viterbi_trellis[0, :] = log_pi + log_emissions_t0\n\n        # 2. Recursion Step (t=1 to T-1)\n        for t in range(1, T):\n            log_emissions_t = np.array([norm.logpdf(obs_seq[t], loc=mu, scale=sigma) \n                                        for mu, sigma in zip(mus, sigmas)])\n            \n            # Vectorized computation for all current states j\n            # scores[i, j] = viterbi_trellis[t-1, i] + log_A[i, j]\n            scores = viterbi_trellis[t-1, :].reshape(-1, 1) + log_A\n            \n            # For each current state j, find the best previous state i.\n            # np.argmax correctly implements the tie-breaking rule (smallest index first).\n            viterbi_trellis[t, :] = np.max(scores, axis=0) + log_emissions_t\n            backpointers[t, :] = np.argmax(scores, axis=0)\n        \n        # 3. Termination Step\n        # Find the best final state. np.argmax handles tie-breaking.\n        best_last_state = np.argmax(viterbi_trellis[T-1, :])\n\n        # 4. Backtracking Step\n        path = [best_last_state]\n        current_state = best_last_state\n        for t in range(T - 1, 0, -1):\n            current_state = backpointers[t, current_state]\n            path.insert(0, current_state)\n            \n        return path\n\n    test_cases = [\n        [0.9, 0.7, 0.8, 1.9, 2.2, 0.2, 0.1, 0.0],\n        [2.1],\n        [0.1, 0.0, 0.9, 0.8, 0.7, 0.2],\n        [0.75, 0.85, 0.95, 0.05, -0.1, 0.0, 1.8, 2.1, 1.9, 0.6]\n    ]\n\n    results = []\n    for case in test_cases:\n        result_path = viterbi(case)\n        results.append(result_path)\n\n    # Format the output string as specified: [[...],[...],...] with no spaces\n    outer_parts = []\n    for res_list in results:\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        outer_parts.append(inner_str)\n    final_output = '[' + ','.join(outer_parts) + ']'\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2437210"}]}