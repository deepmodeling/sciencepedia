{"hands_on_practices": [{"introduction": "To truly understand a Convolutional Neural Network, we must first trace the journey of an input sequence as it is transformed into a prediction. This exercise provides a hands-on implementation of the entire forward pass, from one-hot encoding to the final output. By simulating the effect of a Single Nucleotide Polymorphism (SNP), you will not only master the mechanics of the CNN but also apply it to perform a foundational task in computational genetics: predicting the functional impact of a mutation [@problem_id:2382374].", "problem": "You are given a small, deterministic convolutional neural network (CNN) for deoxyribonucleic acid (DNA) sequence analysis, along with a protocol for simulating a single nucleotide polymorphism (SNP) by altering one position in the one-hot encoded input. Your task is to implement the exact mathematical pipeline and compute the signed change in the model prediction caused by each SNP for a provided test suite of sequences and mutations. Your program must produce the final results as a single line in a strictly specified format.\n\nThe input domain is the set of DNA sequences over the alphabet $\\{A,C,G,T\\}$. Each sequence of length $L$ is represented by one-hot encoding into a matrix $X \\in \\mathbb{R}^{L \\times 4}$ according to the mapping $A \\mapsto (1,0,0,0)$, $C \\mapsto (0,1,0,0)$, $G \\mapsto (0,0,1,0)$, $T \\mapsto (0,0,0,1)$. A single nucleotide polymorphism is simulated by changing exactly one position $p$ ($0$-based index) in the sequence from its original base to a different target base, and re-encoding to obtain the mutated matrix $X' \\in \\mathbb{R}^{L \\times 4}$.\n\nThe convolutional layer performs a one-dimensional cross-correlation (the conventional operation used in convolutional neural networks) with $F$ filters, stride $1$, and “valid” borders (no padding). Let $K$ be the kernel width and $C=4$ the number of input channels. For filter $f \\in \\{0,1,\\dots,F-1\\}$ with weights $W^{(f)} \\in \\mathbb{R}^{K \\times C}$ and bias $b^{(f)} \\in \\mathbb{R}$, and for output position $i \\in \\{0,1,\\dots,L-K\\}$, the pre-activation is\n$$\nz^{(f)}[i] \\;=\\; \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\, X[i+k,c] \\;+\\; b^{(f)}.\n$$\nThe nonlinearity is the Rectified Linear Unit (ReLU), defined by\n$$\n\\mathrm{ReLU}(u) \\;=\\; \\max(0,u).\n$$\nThis yields activations\n$$\na^{(f)}[i] \\;=\\; \\mathrm{ReLU}\\!\\left(z^{(f)}[i]\\right).\n$$\nA global max pooling is then applied independently to each filter,\n$$\nm^{(f)} \\;=\\; \\max_{i} \\, a^{(f)}[i] \\, ,\n$$\nproducing a vector $m \\in \\mathbb{R}^{F}$. Finally, a fully connected linear readout produces the scalar prediction\n$$\ny \\;=\\; \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\, m^{(f)} \\;+\\; b^{\\mathrm{fc}} \\, .\n$$\n\nYou are provided with a specific network of $F=2$ filters and kernel width $K=3$, with the following weights and biases. All omitted entries are zero.\n- Filter $f=0$ (detects the pattern $A\\!C\\!G$):\n  $$\n  W^{(0)} \\;=\\; \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 0\n  \\end{bmatrix}, \\quad b^{(0)} \\;=\\; -2.5 \\, .\n  $$\n- Filter $f=1$ (detects the pattern $T\\!T\\!T$):\n  $$\n  W^{(1)} \\;=\\; \\begin{bmatrix}\n  0 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 1\n  \\end{bmatrix}, \\quad b^{(1)} \\;=\\; -2.5 \\, .\n  $$\n- Fully connected readout:\n  $$\n  w^{\\mathrm{fc}} \\;=\\; \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}, \\quad b^{\\mathrm{fc}} \\;=\\; 0.1 \\, .\n  $$\n\nFor each test case, compute the signed change in prediction\n$$\n\\Delta \\;=\\; y_{\\mathrm{mut}} \\;-\\; y_{\\mathrm{ref}} \\, ,\n$$\nwhere $y_{\\mathrm{ref}}$ is the prediction for the original sequence and $y_{\\mathrm{mut}}$ is the prediction after applying the SNP. Use $0$-based indexing for positions. Sequences contain only the characters $A$, $C$, $G$, $T$. Every SNP changes a base to a different base.\n\nTest suite to implement and evaluate:\n- Case $1$: sequence $S = \\text{\"AACGTTGA\"}$, position $p = 2$, new base $B' = \\text{\"T\"}$.\n- Case $2$: sequence $S = \\text{\"TTTACGTT\"}$, position $p = 0$, new base $B' = \\text{\"G\"}$.\n- Case $3$: sequence $S = \\text{\"ACG\"}$, position $p = 1$, new base $B' = \\text{\"G\"}$.\n- Case $4$: sequence $S = \\text{\"ACGACG\"}$, position $p = 1$, new base $B' = \\text{\"T\"}$.\n\nYour program must:\n- Implement the one-hot encoding, the convolutional cross-correlation, the Rectified Linear Unit, the global max pooling, and the fully connected readout exactly as specified above.\n- For each test case, compute $\\Delta$ as defined.\n- Round each $\\Delta$ to $6$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$. Each value must be rounded to $6$ decimal places with standard rounding (for example, $0.5$ becomes $\\text{\"0.500000\"}$). No other text should be printed.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of computational biology and deep learning, presents a well-posed mathematical task with all necessary parameters, and is formulated in objective, unambiguous language. All components of the described convolutional neural network—one-hot encoding, convolution, ReLU activation, max pooling, and a fully connected layer—are standard elements in this domain. The task of computing the effect of a single nucleotide polymorphism (SNP) on the model's output is a standard *in silico* sensitivity analysis technique. Therefore, we proceed to a complete solution.\n\nThe problem requires the computation of the change in a deterministic neural network's output, $\\Delta = y_{\\mathrm{mut}} - y_{\\mathrm{ref}}$, resulting from a single point mutation in an input DNA sequence. This is achieved by implementing the forward pass of the network for both the reference sequence ($S_{\\mathrm{ref}}$) and the mutated sequence ($S_{\\mathrm{mut}}$) to obtain their respective scalar predictions, $y_{\\mathrm{ref}}$ and $y_{\\mathrm{mut}}$.\n\nThe computational pipeline is structured as follows:\n\n**1. Input Encoding**\nA DNA sequence of length $L$ is transformed into a numerical matrix $X \\in \\mathbb{R}^{L \\times 4}$ via one-hot encoding. Each nucleotide is mapped to a unique 4-dimensional binary vector, establishing a canonical basis: $A \\mapsto [1, 0, 0, 0]$, $C \\mapsto [0, 1, 0, 0]$, $G \\mapsto [0, 0, 1, 0]$, and $T \\mapsto [0, 0, 0, 1]$.\n\n**2. Convolutional Layer**\nThis layer detects local patterns in the sequence. It consists of $F=2$ filters, each with a kernel of width $K=3$ and weights $W^{(f)} \\in \\mathbb{R}^{3 \\times 4}$. The operation is a 1D cross-correlation with a stride of $1$ and no padding (\"valid\" convolution). For each filter $f$ and each possible starting position $i$ in the input sequence (where $i \\in \\{0, 1, \\dots, L-K\\}$), a pre-activation value $z^{(f)}[i]$ is computed. This value is the sum of the element-wise product of the filter's weights $W^{(f)}$ and the corresponding $K \\times 4$ slice of the input matrix $X[i:i+K, :]$, plus a filter-specific bias $b^{(f)}$. The formula is:\n$$\nz^{(f)}[i] = \\left( \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\cdot X[i+k,c] \\right) + b^{(f)}\n$$\nThe provided filter weights are designed to detect the motifs \"ACG\" ($f=0$) and \"TTT\" ($f=1$). A perfect match between a sequence window and the motif encoded in the filter weights results in a dot product sum of $3$, to which the bias $b^{(f)} = -2.5$ is added, yielding a pre-activation of $0.5$.\n\n**3. Activation Function**\nThe pre-activations are passed through a non-linear activation function, the Rectified Linear Unit (ReLU), defined as $\\mathrm{ReLU}(u) = \\max(0, u)$. This function introduces non-linearity, allowing the model to learn more complex patterns. It effectively sets any negative pre-activation values to zero, meaning a filter only \"fires\" if the input motif provides a sufficiently strong match to overcome the negative bias. The activations are given by:\n$$\na^{(f)}[i] = \\mathrm{ReLU}(z^{(f)}[i])\n$$\n\n**4. Global Max Pooling Layer**\nFor each filter's activation map $a^{(f)}$, a single feature value $m^{(f)}$ is extracted by taking the maximum value across all positions $i$. This is known as global max pooling.\n$$\nm^{(f)} = \\max_{i} a^{(f)}[i]\n$$\nThis operation makes the model's prediction invariant to the position of the detected motif and reduces the dimensionality of the representation to a fixed-size vector $m \\in \\mathbb{R}^{F}$, regardless of the input sequence length $L$. If no position yields a positive activation, the maximum is $0$.\n\n**5. Fully Connected Readout Layer**\nThe final prediction, a scalar value $y$, is computed as a linear combination of the pooled feature values $m^{(f)}$, weighted by the fully connected layer's weights $w^{\\mathrm{fc}}$, plus a final bias term $b^{\\mathrm{fc}}$.\n$$\ny = \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\cdot m^{(f)} + b^{\\mathrm{fc}}\n$$\n\n**Example Calculation: Case 1**\nLet us walk through the calculation for Case 1: $S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$, position $p=2$, new base $B'=\\text{\"T\"}$. The mutated sequence is $S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$.\n\n**Reference Prediction ($y_{\\mathrm{ref}}$):**\n- Sequence: $S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$ ($L=8$).\n- Convolutional windows ($K=3$): \"AAC\", \"ACG\", \"CGT\", \"GTT\", \"TTG\", \"TGA\".\n- **Filter 0 (\"ACG\"):** The window \"ACG\" at position $i=1$ gives a perfect match. The dot product is $3$. $z^{(0)}[1] = 3 - 2.5 = 0.5$. All other windows result in a dot product less than $2.5$, so their pre-activations are negative. The activation map is $a^{(0)}_{\\mathrm{ref}} = [0, 0.5, 0, 0, 0, 0]$.\n- **Filter 1 (\"TTT\"):** No window perfectly matches \"TTT\". The highest dot products come from \"GTT\" and \"TTG\" (score of $2$), yielding pre-activations of $2 - 2.5 = -0.5$. Thus, all activations for this filter are $0$. The activation map is $a^{(1)}_{\\mathrm{ref}} = [0, 0, 0, 0, 0, 0]$.\n- **Max Pooling:** $m^{(0)}_{\\mathrm{ref}} = \\max(a^{(0)}_{\\mathrm{ref}}) = 0.5$. $m^{(1)}_{\\mathrm{ref}} = \\max(a^{(1)}_{\\mathrm{ref}}) = 0$. The pooled feature vector is $m_{\\mathrm{ref}} = [0.5, 0]$.\n- **Readout:** $y_{\\mathrm{ref}} = (1.0 \\cdot 0.5) + (-0.5 \\cdot 0) + 0.1 = 0.5 + 0 + 0.1 = 0.6$.\n\n**Mutated Prediction ($y_{\\mathrm{mut}}$):**\n- Sequence: $S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$. The mutation at $p=2$ changes \"C\" to \"T\".\n- The original \"ACG\" motif is destroyed. The new windows affected are \"AAT\" ($i=0$), \"ATG\" ($i=1$), and \"TGT\" ($i=2$).\n- **Filter 0 (\"ACG\"):** The highest dot product from any window is now $2$ (from \"ATG\"), yielding a pre-activation of $2 - 2.5 = -0.5$. All activations are $0$. $a^{(0)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$.\n- **Filter 1 (\"TTT\"):** The mutation does not create a \"TTT\" motif. The highest dot product is $2$ (from \"TGT\", \"GTT\", \"TTG\"), yielding pre-activations of $-0.5$. All activations are $0$. $a^{(1)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$.\n- **Max Pooling:** $m^{(0)}_{\\mathrm{mut}} = 0$. $m^{(1)}_{\\mathrm{mut}} = 0$. The pooled feature vector is $m_{\\mathrm{mut}} = [0, 0]$.\n- **Readout:** $y_{\\mathrm{mut}} = (1.0 \\cdot 0) + (-0.5 \\cdot 0) + 0.1 = 0.1$.\n\n**Final Calculation for Case 1:**\nThe change in prediction is $\\Delta_1 = y_{\\mathrm{mut}} - y_{\\mathrm{ref}} = 0.1 - 0.6 = -0.5$.\n\nThis exact procedure is applied to all test cases, with the results rounded to $6$ decimal places. The provided program implements this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the signed change in a CNN's prediction due to single nucleotide polymorphisms.\n    \"\"\"\n    # Define the fixed network parameters\n    params = {\n        'W0': np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=float),\n        'b0': -2.5,\n        'W1': np.array([[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]], dtype=float),\n        'b1': -2.5,\n        'w_fc': np.array([1.0, -0.5], dtype=float),\n        'b_fc': 0.1\n    }\n    \n    # One-hot encoding mapping\n    one_hot_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_channels = 4\n    kernel_width = 3\n\n    def get_prediction(sequence: str) -> float:\n        \"\"\"\n        Performs a full forward pass of the CNN for a given DNA sequence.\n        \"\"\"\n        seq_len = len(sequence)\n        \n        # Handle sequences shorter than the kernel\n        if seq_len < kernel_width:\n             # No convolution is possible, so all activations are zero by default\n             m = np.array([0.0, 0.0])\n             y = np.sum(m * params['w_fc']) + params['b_fc']\n             return y\n        \n        # 1. One-hot encode the sequence\n        X = np.zeros((seq_len, num_channels), dtype=float)\n        for i, base in enumerate(sequence):\n            if base in one_hot_map:\n                X[i, one_hot_map[base]] = 1.0\n\n        # Define filters and biases\n        filters = [(params['W0'], params['b0']), (params['W1'], params['b1'])]\n        pooled_features = []\n\n        for W_f, b_f in filters:\n            # 2. Convolutional Layer (pre-activations)\n            conv_len = seq_len - kernel_width + 1\n            pre_activations = np.zeros(conv_len, dtype=float)\n            for i in range(conv_len):\n                window = X[i : i + kernel_width, :]\n                pre_activations[i] = np.sum(window * W_f) + b_f\n            \n            # 3. ReLU Activation\n            activations = np.maximum(0, pre_activations)\n            \n            # 4. Global Max Pooling\n            # np.max on an empty array raises error. If activations is empty, this\n            # means conv_len was 0. In this case max is 0.\n            if activations.size > 0:\n                max_activation = np.max(activations)\n            else:\n                max_activation = 0.0\n            \n            pooled_features.append(max_activation)\n        \n        m = np.array(pooled_features)\n        \n        # 5. Fully Connected Readout\n        y = np.sum(m * params['w_fc']) + params['b_fc']\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"AACGTTGA\", 2, \"T\"),\n        (\"TTTACGTT\", 0, \"G\"),\n        (\"ACG\", 1, \"G\"),\n        (\"ACGACG\", 1, \"T\"),\n    ]\n\n    results = []\n    for seq_ref, p, new_base in test_cases:\n        # Get prediction for reference sequence\n        y_ref = get_prediction(seq_ref)\n        \n        # Create mutated sequence\n        seq_list = list(seq_ref)\n        seq_list[p] = new_base\n        seq_mut = \"\".join(seq_list)\n        \n        # Get prediction for mutated sequence\n        y_mut = get_prediction(seq_mut)\n        \n        # Compute the change and round\n        delta = y_mut - y_ref\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2382374"}, {"introduction": "Now that you understand how a CNN processes data, the next critical question is how to best represent that data in the first place. This practice explores the crucial choice of input representation, contrasting a sparse, discrete one-hot encoding with a dense, feature-based encoding using physicochemical properties. This thought experiment delves into core machine learning concepts such as parameter efficiency, inductive bias, and generalization, challenging you to think like a model designer about how encodings can help a model learn biologically meaningful patterns [@problem_id:2382354].", "problem": "You are designing a protein sequence classifier using a convolutional neural network (CNN). Each protein is a sequence over the standard set of amino acids. You consider two input encodings per residue: (i) a one-hot encoding over the $20$ canonical amino acids, and (ii) a vector of three physicochemical properties, hydrophobicity $h$, net charge $q$, and polarity $p$, so each position is represented by a vector in $\\mathbb{R}^{3}$. The CNN begins with a single $1$-dimensional convolutional layer with kernel width $k=5$, stride $1$, no padding, and $F=64$ filters, followed by a pointwise nonlinearity. Assume the input sequence length is $L$ (a positive integer), and ignore any downstream layers. Unless otherwise stated, treat convolutional kernels as including only their weights and consider bias terms separately.\n\nSelect all options that are correct.\n\nA. Excluding biases, the total number of trainable weights in the first convolutional layer is $k \\times 20 \\times F$ for the one-hot input and $k \\times 3 \\times F$ for the property input; therefore, with $k=5$ and $F=64$, the property input uses exactly $960$ kernel weights while the one-hot input uses $6400$.\n\nB. If the three property channels $(h,q,p)$ are fed as raw numerical values on potentially different scales, the channel with the largest variance can dominate learning dynamics in the first convolutional layer; standardizing each channel to zero mean and unit variance helps mitigate this effect.\n\nC. Because the property encoding aggregates amino acids by physicochemical similarity, a width-$5$ convolution cannot detect position-specific motifs under this encoding, whereas it can under a one-hot encoding.\n\nD. For a fixed set of learned convolution weights, permuting the order of the three property channels (for example, swapping $h$ and $p$) leaves the convolution outputs unchanged.\n\nE. Compared to a one-hot encoding, the property encoding can allow a single learned filter to respond similarly to unseen amino acid substitutions that preserve physicochemical similarity, potentially improving generalization to variants not seen during training.", "solution": "The problem statement is a valid description of a standard setup in computational biology for applying convolutional neural networks (CNNs) to protein sequence analysis. The problem is scientifically grounded, well-posed, objective, and self-contained, providing sufficient information to evaluate the provided options. The two encoding methods, one-hot and physicochemical properties, are standard representations, and the CNN architecture described is typical for sequence processing tasks.\n\nLet us analyze the structure of the input and the convolutional layer. The input is a sequence of length $L$. Each element of the sequence is a vector representing an amino acid.\nFor the one-hot encoding, each amino acid is represented by a vector of dimension $C_1 = 20$. The input tensor thus has a shape of $(L, 20)$, if we consider the format (length, channels).\nFor the physicochemical property encoding, each amino acid is represented by a vector of dimension $C_2 = 3$. The input tensor thus has a shape of $(L, 3)$.\n\nThe first layer is a $1$-dimensional convolution with the following parameters:\n- Kernel width, $k=5$\n- Stride, $S=1$\n- Number of filters (output channels), $F=64$\n- Padding, $P=0$\n\nThe weights of a single filter in a $1$D convolution have dimensions (kernel width, input channels). Since there are $F$ such filters, the total weight tensor for the layer has dimensions $(F, k, C)$, where $C$ is the number of input channels. The total number of trainable weights in the kernels (excluding biases, as specified) is the product of these dimensions: $F \\times k \\times C$.\n\nNow, let us evaluate each option.\n\n**A. Excluding biases, the total number of trainable weights in the first convolutional layer is $k \\times 20 \\times F$ for the one-hot input and $k \\times 3 \\times F$ for the property input; therefore, with $k=5$ and $F=64$, the property input uses exactly $960$ kernel weights while the one-hot input uses $6400$.**\n\nThe number of trainable weights is given by the formula $F \\times k \\times C$.\n- For the one-hot encoding, the number of input channels is $C_1 = 20$. The total number of weights is $F \\times k \\times C_1 = 64 \\times 5 \\times 20 = 320 \\times 20 = 6400$. The formula presented, $k \\times 20 \\times F$, is equivalent due to the commutativity of multiplication.\n- For the physicochemical property encoding, the number of input channels is $C_2 = 3$. The total number of weights is $F \\times k \\times C_2 = 64 \\times 5 \\times 3 = 320 \\times 3 = 960$. The formula presented, $k \\times 3 \\times F$, is also equivalent.\n\nThe statement provides the correct formulae and the correct numerical results based on the given parameters $k=5$ and $F=64$.\n**Verdict: Correct.**\n\n**B. If the three property channels $(h,q,p)$ are fed as raw numerical values on potentially different scales, the channel with the largest variance can dominate learning dynamics in the first convolutional layer; standardizing each channel to zero mean and unit variance helps mitigate this effect.**\n\nThis statement addresses a fundamental principle of training machine learning models, including neural networks. The convolution operation is a linear combination of inputs with learned weights. If the input features (here, the channels $h$, $q$, and $p$) have vastly different numerical ranges or variances, the features with larger values will have a disproportionately large effect on the output of the convolutional layer and, consequently, on the gradients calculated during backpropagation. This can cause training instability and may lead the model to focus primarily on learning from the high-variance channel, while ignoring potentially useful information in other channels. Standardizing the input features (in this case, per channel across the entire dataset) to have a mean of $0$ and a standard deviation of $1$ is a standard and highly recommended preprocessing step. This ensures all channels are on a comparable scale, which typically leads to more stable and efficient training.\n**Verdict: Correct.**\n\n**C. Because the property encoding aggregates amino acids by physicochemical similarity, a width-$5$ convolution cannot detect position-specific motifs under this encoding, whereas it can under a one-hot encoding.**\n\nA convolution operation is inherently position-specific within its receptive field. A filter of width $k=5$ applies a set of $5$ weight vectors to the $5$ consecutive input positions it covers. For the property encoding, each of the $5$ weight vectors has dimension $3$ (one weight for each of $h, q, p$). The filter learns a specific pattern of physicochemical properties across these $5$ positions. For example, it might learn to detect a region of high hydrophobicity at position $1$ of the window, followed by a positive charge at position $3$. This is a \"position-specific motif\" of physicochemical properties.\n\nThe claim that it \"cannot detect\" such motifs is false. What is true is that this encoding makes it impossible to distinguish between two different amino acids if they happen to have identical or very similar physicochemical vectors. A one-hot encoding, being a unique identifier for each amino acid, does not have this ambiguity and allows a filter to learn motifs based on specific amino acid identities (e.g., 'Alanine' at position $1$, not just 'a small hydrophobic amino acid'). However, the inability to distinguish certain amino acids is not equivalent to an inability to detect position-specific patterns. The convolution operator, by its very nature, detects local, position-specific patterns in its input.\n**Verdict: Incorrect.**\n\n**D. For a fixed set of learned convolution weights, permuting the order of the three property channels (for example, swapping $h$ and $p$) leaves the convolution outputs unchanged.**\n\nLet the input vector at sequence position $j$ be $\\mathbf{x}_j = [x_{j,h}, x_{j,q}, x_{j,p}]^T$, representing the $(h,q,p)$ values. Let a single filter's weights for a position $m$ within its kernel ($m \\in \\{0, 1, ..., 4\\}$) be $\\mathbf{w}_m = [w_{m,h}, w_{m,q}, w_{m,p}]^T$. The contribution to the convolution output from position $j=i+m$ in the sequence is the dot product $\\mathbf{w}_m \\cdot \\mathbf{x}_j = w_{m,h}x_{j,h} + w_{m,q}x_{j,q} + w_{m,p}x_{j,p}$. The total output for one filter at position $i$ is the sum of these dot products over the kernel window: $y_i = \\sum_{m=0}^{4} \\mathbf{w}_m \\cdot \\mathbf{x}_{i+m}$.\n\nIf we permute the input channels, for example swapping $h$ and $p$, the new input vector is $\\mathbf{x}'_j = [x_{j,p}, x_{j,q}, x_{j,h}]^T$. The new output, using the same fixed weights $\\mathbf{w}_m$, will have contributions of the form $\\mathbf{w}_m \\cdot \\mathbf{x}'_j = w_{m,h}x_{j,p} + w_{m,q}x_{j,q} + w_{m,p}x_{j,h}$.\nIn general, $w_{m,h}x_{j,h} + w_{m,p}x_{j,p} \\neq w_{m,h}x_{j,p} + w_{m,p}x_{j,h}$.\nThe output will remain unchanged only under the trivial conditions that $x_{j,h} = x_{j,p}$ for all relevant $j$, or if the learned weights have a specific symmetry $w_{m,h} = w_{m,p}$ for all $m$. Neither of these can be assumed. The convolution operation is not invariant to the permutation of its input channels because each channel is processed by a distinct set of weights.\n**Verdict: Incorrect.**\n\n**E. Compared to a one-hot encoding, the property encoding can allow a single learned filter to respond similarly to unseen amino acid substitutions that preserve physicochemical similarity, potentially improving generalization to variants not seen during training.**\n\nThis statement describes the concept of inductive bias introduced by the choice of encoding. A one-hot encoding represents each amino acid as an orthogonal vector. The model has no a priori information about which amino acids are similar. For example, the biochemically similar amino acids Leucine (L) and Isoleucine (I) are as distinct as Leucine (L) and Aspartic Acid (D) in this representation. A filter trained to recognize a motif containing L will not automatically respond to the same motif with I substituted for L.\n\nIn contrast, the physicochemical property encoding is a dense representation where similar amino acids are mapped to nearby points in the $\\mathbb{R}^3$ space. Leucine and Isoleucine would have very similar $(h,q,p)$ vectors. A filter that learns to recognize a pattern of properties (e.g., \"highly hydrophobic residue\") will respond similarly to both L and I in that position. This enables the model to generalize. If the training data contains a functional site with Leucine, the model can learn its properties and correctly predict that a variant with Isoleucine, which was not seen in training, is also a functional site. This is a key motivation for using feature-based encodings and can improve a model's performance on new, unseen data.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABE}$$", "id": "2382354"}, {"introduction": "We have seen the architecture of a CNN and considered its inputs, but how do a model's weights acquire the values that allow them to recognize motifs? This advanced practice dives into the heart of machine learning: model training. You will implement gradient-based optimization from first principles to train a convolutional kernel, with a key focus on the role of regularization, specifically comparing the effects of the $L_1$ and $L_2$ penalties on the learned weights [@problem_id:2382359]. By observing how $L_1$ regularization promotes sparsity, you will gain practical insight into a powerful technique for learning clean, interpretable biological motifs from sequence data.", "problem": "You are to write a complete, runnable program that studies how the choice of regularization penalty affects the sparsity of motifs learned by a single $1$-dimensional convolutional kernel for Deoxyribonucleic Acid (DNA) sequence analysis using a Convolutional Neural Network (CNN), formalized as a linear model with logistic link and average pooling. You must derive the algorithm from first principles and implement it using only linear algebra and vector calculus. All symbols, variables, functions, operators, and numbers must be in LaTeX math mode when they appear in this problem statement.\n\nConsider sequences over the alphabet $\\{A,C,G,T\\}$, with one-hot encoding in $\\mathbb{R}^{4}$. Let the sequence length be $S$, and the kernel (motif) length be $L$. For a one-hot encoded sequence $X \\in \\{0,1\\}^{S \\times 4}$, define the one-dimensional valid convolutional response at position $t$ by\n$$\ns_t = \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b} \\, X_{t+i-1,\\,b},\n$$\nwhere $W \\in \\mathbb{R}^{L \\times 4}$ are the kernel weights. Let there be average pooling across positions to form a scalar pre-activation\n$$\nz = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} s_t + b,\n$$\nwhere $b \\in \\mathbb{R}$ is an intercept. Let the predicted probability for a binary label $y \\in \\{0,1\\}$ be given by the logistic function\n$$\np = \\sigma(z) = \\frac{1}{1+\\exp(-z)}.\n$$\nGiven a dataset $\\{(X^{(n)},y^{(n)})\\}_{n=1}^{N}$, define the average logistic loss\n$$\n\\mathcal{L}(W,b) = \\frac{1}{N} \\sum_{n=1}^{N} \\left[ -y^{(n)} \\log p^{(n)} - (1-y^{(n)}) \\log(1-p^{(n)}) \\right],\n$$\nwith $p^{(n)} = \\sigma(z^{(n)})$ computed as above. Consider two regularizers on $W$ only: the $L_1$ penalty\n$$\nR_{1}(W) = \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\left| W_{i,b} \\right|\n$$\nand the $L_2$ penalty\n$$\nR_{2}(W) = \\frac{1}{2} \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b}^{2}.\n$$\nFor a regularization strength $\\lambda \\ge 0$, the empirical risk to minimize is\n$$\nF(W,b) = \\mathcal{L}(W,b) + \\lambda \\, R(W),\n$$\nwhere $R(W)$ is either $R_{1}(W)$ or $R_{2}(W)$. The average pooling ensures the model is linear in $W$ and $b$ when expressed in terms of the average of all sliding windows of $X$, so the unregularized part is convex.\n\nYour task is to:\n- Derive from first principles the gradient of $\\mathcal{L}(W,b)$ with respect to $W$ and $b$ using the chain rule, and specify a step-size choice justified by a Lipschitz constant bound for the gradient based on the data matrix of pooled features. You must then implement two training procedures:\n  - For $L_2$ regularization, use gradient descent on $W$ and $b$ for $T$ steps with step size $\\alpha$.\n  - For $L_1$ regularization, use proximal gradient descent: take a gradient step on the unregularized part for $W$, then apply the soft-thresholding proximal operator with threshold $\\alpha \\lambda$; update $b$ by gradient descent without penalty.\n- Generate synthetic data as follows. Fix integers $S$ and $L$, and construct a ground-truth motif by choosing, for each position $i \\in \\{1,\\dots,L\\}$, a single preferred base $b_i \\in \\{1,2,3,4\\}$ (corresponding to $\\{A,C,G,T\\}$). For a positive sequence, draw a background sequence of length $S$ with independent and identically distributed uniform bases, then choose a start index $t^{\\star} \\in \\{1, \\dots, S-L+1\\}$ and overwrite the length-$L$ segment with the preferred bases at each offset. For a negative sequence, draw a background sequence of length $S$ without overwriting any segment. Encode sequences as one-hot matrices in $\\{0,1\\}^{S \\times 4}$. Labels are $y=1$ for positive and $y=0$ for negative. For each sequence, compute the average of all length-$L$ one-hot patches:\n$$\n\\Phi(X) = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} X_{t:t+L,:} \\in \\mathbb{R}^{L \\times 4},\n$$\nand flatten to a vector in $\\mathbb{R}^{4L}$ to serve as the feature vector for the linear model. This feature construction is exactly equivalent to the average-pooled convolutional model above.\n- Train the model for each specified test case until $T$ steps. Let $\\tau>0$ be a small threshold. Define the sparsity fraction of the learned kernel as\n$$\n\\mathrm{spar}(W) = \\frac{1}{4L} \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\mathbf{1}\\left\\{ |W_{i,b}| < \\tau \\right\\}.\n$$\nReport this value as a real number in $[0,1]$ for each test case.\n\nImplementation requirements:\n- You must implement the training using only basic linear algebra with the logistic link and proximal operator as required; do not use any machine learning libraries.\n- Choose the step size $\\alpha$ using a Lipschitz constant upper bound derived from the spectral norm of the feature matrix for the unregularized logistic loss. For $L_2$ regularization, account for the additional smoothness in your step size bound appropriately.\n- Use $T=1000$ gradient steps, $\\tau=10^{-3}$, and do not penalize the intercept $b$.\n- All random choices must be reproducible from a specified integer seed.\n\nTest suite:\n- Use $S=40$, $L=8$, and balanced classes with $N/2$ positives and $N/2$ negatives in all cases.\n- Case $\\mathbf{1}$ (happy path, $L_1$): regularization $L_1$, $\\lambda = 0.05$, $N=600$, seed$=7$, with signal as described (positives contain the motif, negatives do not).\n- Case $\\mathbf{2}$ (happy path control, $L_2$): regularization $L_2$, $\\lambda = 0.05$, $N=600$, seed$=7$, with signal.\n- Case $\\mathbf{3}$ (edge case, no signal): regularization $L_1$, $\\lambda = 0.3$, $N=600$, seed$=21$, with no signal (both classes are generated as background; labels are assigned $y=1$ for the first $N/2$ sequences and $y=0$ for the remaining $N/2$ sequences).\n- Case $\\mathbf{4}$ (boundary condition, vanishing regularization): regularization $L_1$, $\\lambda = 10^{-6}$, $N=600$, seed$=11$, with signal.\n\nFinal output specification:\n- Your program must produce a single line of output containing the sparsity fractions for Cases $1$ through $4$, in that order, as a comma-separated list enclosed in square brackets. Each value must be rounded to three digits after the decimal point. For example, the output format must be exactly like\n$$\n[\\text{v}_1,\\text{v}_2,\\text{v}_3,\\text{v}_4]\n$$\nwith each $\\text{v}_k$ a decimal with three digits after the decimal point and no additional text printed.", "solution": "The problem requires the implementation of a numerical experiment to study the sparsity-inducing effects of $L_1$ versus $L_2$ regularization on a simplified one-dimensional convolutional neural network model for DNA sequence analysis. The model is equivalent to a logistic regression on features derived from average pooling over all subsequence patches. The solution will be derived from first principles.\n\nFirst, we formalize the model as a linear logistic regression. A sequence $X \\in \\{0,1\\}^{S \\times 4}$ is one-hot encoded. The kernel is $W \\in \\mathbb{R}^{L \\times 4}$. The convolutional response followed by average pooling yields a pre-activation $z$:\n$$\nz = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} \\left( \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b} X_{t+i-1,b} \\right) + b\n$$\nBy linearity of summation, this can be expressed as an inner product. Let $w = \\text{vec}(W) \\in \\mathbb{R}^{4L}$ be the flattened vector of kernel weights. Let $\\phi(X) \\in \\mathbb{R}^{4L}$ be the flattened vector corresponding to the average of all length-$L$ one-hot patches of the sequence matrix $X$:\n$$\n\\Phi(X) = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} X_{[t:t+L-1],:} \\in \\mathbb{R}^{L \\times 4}\n$$\nand $\\phi(X) = \\text{vec}(\\Phi(X))$. Here, $X_{[t:t+L-1],:}$ denotes the submatrix of $X$ from row $t$ to row $t+L-1$. The pre-activation can then be written as a standard linear model:\n$$\nz = w^T \\phi(X) + b\n$$\nGiven a dataset $\\{(\\phi^{(n)}, y^{(n)})\\}_{n=1}^N$, where $\\phi^{(n)} = \\phi(X^{(n)})$, the predictive model is $p^{(n)} = \\sigma(z^{(n)}) = \\sigma(w^T\\phi^{(n)} + b)$. The objective is to minimize the regularized logistic loss:\n$$\nF(w,b) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}^{(n)}(w,b) + \\lambda R(w)\n$$\nwhere $\\mathcal{L}^{(n)} = -y^{(n)} \\log p^{(n)} - (1-y^{(n)}) \\log(1-p^{(n)})$ and $R(w)$ is either the $L_1$ or $L_2$ norm of $w$.\n\nTo implement the optimization algorithms, we require the gradient of the unregularized loss $\\mathcal{L}(w,b) = \\frac{1}{N} \\sum_n \\mathcal{L}^{(n)}$. Using the chain rule, the gradient of the loss for a single sample $n$ with respect to the pre-activation $z^{(n)}$ is:\n$$\n\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial z^{(n)}} = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial p^{(n)}} \\frac{\\partial p^{(n)}}{\\partial z^{(n)}} = \\left( -\\frac{y^{(n)}}{p^{(n)}} + \\frac{1-y^{(n)}}{1-p^{(n)}} \\right) \\cdot (p^{(n)}(1-p^{(n)})) = p^{(n)} - y^{(n)}\n$$\nThe gradients of $z^{(n)}$ with respect to the parameters are $\\nabla_w z^{(n)} = \\phi^{(n)}$ and $\\partial z^{(n)}/\\partial b = 1$. The full gradients of the average loss $\\mathcal{L}$ are:\n$$\n\\nabla_w \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} (p^{(n)} - y^{(n)}) \\phi^{(n)}\n$$\n$$\n\\nabla_b \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} (p^{(n)} - y^{(n)})\n$$\nIn matrix notation, let $\\Phi$ be the $N \\times (4L)$ design matrix with rows $(\\phi^{(n)})^T$, $y$ be the vector of labels, and $p$ be the vector of predictions. The gradients are $\\nabla_w \\mathcal{L} = \\frac{1}{N} \\Phi^T (p-y)$ and $\\nabla_b \\mathcal{L} = \\frac{1}{N} \\mathbf{1}^T (p-y)$.\n\nFor gradient-based optimization, the step size $\\alpha$ must be chosen carefully. A stable choice for $\\alpha$ is inversely related to the Lipschitz constant of the gradient of the function being optimized. For the logistic loss $\\mathcal{L}(w,b)$, we find an upper bound on its Hessian. Let $\\theta = [w^T, b]^T$ and $\\tilde{\\phi}^{(n)}=[(\\phi^{(n)})^T, 1]^T$. The Hessian is:\n$$\n\\nabla^2_{\\theta} \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial (p^{(n)}-y^{(n)})}{\\partial \\theta^T} \\tilde{\\phi}^{(n)} = \\frac{1}{N} \\sum_{n=1}^{N} p^{(n)}(1-p^{(n)}) \\tilde{\\phi}^{(n)} (\\tilde{\\phi}^{(n)})^T\n$$\nSince $p(1-p) \\le 1/4$, we have $\\nabla^2_{\\theta} \\mathcal{L} \\preceq \\frac{1}{4N} \\sum_{n=1}^{N} \\tilde{\\phi}^{(n)} (\\tilde{\\phi}^{(n)})^T = \\frac{1}{4N} \\tilde{\\Phi}^T \\tilde{\\Phi}$, where $\\tilde{\\Phi}$ is the $N \\times (4L+1)$ augmented design matrix. The Lipschitz constant $L_{\\nabla \\mathcal{L}}$ of $\\nabla \\mathcal{L}$ is bounded by the largest eigenvalue of this matrix: $L_{\\nabla \\mathcal{L}} \\le \\frac{1}{4N} \\lambda_{\\text{max}}(\\tilde{\\Phi}^T \\tilde{\\Phi}) = \\frac{\\sigma_{\\text{max}}(\\tilde{\\Phi})^2}{4N}$. We will set the step size based on this upper bound, $L_{bound} = \\frac{\\sigma_{\\text{max}}(\\tilde{\\Phi})^2}{4N}$.\n\nFor $L_2$ regularization, the objective $F(w,b) = \\mathcal{L}(w,b) + \\frac{\\lambda}{2} \\|w\\|_2^2$ is smooth. The gradient is $\\nabla_w F = \\nabla_w \\mathcal{L} + \\lambda w$ and $\\nabla_b F = \\nabla_b \\mathcal{L}$. The Hessian of this objective is $\\nabla^2_\\theta F = \\nabla^2_\\theta \\mathcal{L} + \\text{diag}(\\lambda, \\dots, \\lambda, 0)$. The gradient $\\nabla F$ is Lipschitz with constant $L_{\\nabla F} \\le L_{bound} + \\lambda$. The gradient descent updates are:\n$$\nw_{k+1} = w_k - \\alpha (\\nabla_w \\mathcal{L}(w_k, b_k) + \\lambda w_k)\n$$\n$$\nb_{k+1} = b_k - \\alpha \\nabla_b \\mathcal{L}(w_k, b_k)\n$$\nA safe step size is $\\alpha = 1/(L_{bound} + \\lambda)$.\n\nFor $L_1$ regularization, the objective $F(w,b) = \\mathcal{L}(w,b) + \\lambda \\|w\\|_1$ is non-smooth. We use proximal gradient descent, which combines a gradient step on the smooth part ($\\mathcal{L}$) with a proximal step for the non-smooth part ($\\lambda \\|w\\|_1$). The updates are:\n$$\nw_{k+1} = \\text{prox}_{\\alpha\\lambda, \\|\\cdot\\|_1} ( w_k - \\alpha \\nabla_w \\mathcal{L}(w_k, b_k) )\n$$\n$$\nb_{k+1} = b_k - \\alpha \\nabla_b \\mathcal{L}(w_k, b_k)\n$$\nThe step size $\\alpha$ is chosen based on the smooth part only, so we set $\\alpha = 1/L_{bound}$. The proximal operator for the $L_1$ norm is the soft-thresholding function, applied element-wise:\n$$\n\\text{prox}_{\\gamma, \\|\\cdot\\|_1}(v)_i = \\text{sign}(v_i) \\max(|v_i| - \\gamma, 0)\n$$\nIn our case, the threshold is $\\gamma = \\alpha\\lambda$.\n\nThe sparsity of the final weight kernel $W$ (reshaped from $w$) is calculated as the fraction of weights whose absolute value is below a small threshold $\\tau > 0$:\n$$\n\\mathrm{spar}(W) = \\frac{1}{4L} \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\mathbf{1}\\left\\{ |W_{i,b}| < \\tau \\right\\}.\n$$\nThe implementation will follow these derivations to perform the required numerical experiments.", "answer": "```python\nimport numpy as np\n\ndef one_hot_encode(seq, alphabet_size=4):\n    \"\"\"Converts a sequence of integers into a one-hot encoded matrix.\"\"\"\n    return np.eye(alphabet_size)[seq]\n\ndef generate_data(N, S, L, seed, signal=True):\n    \"\"\"\n    Generates synthetic DNA sequence data.\n    \n    Args:\n        N (int): Total number of sequences.\n        S (int): Sequence length.\n        L (int): Motif length.\n        seed (int): Random seed for reproducibility.\n        signal (bool): If True, positive samples contain a motif. If False,\n                       all samples are random background.\n\n    Returns:\n        tuple: (sequences, labels)\n            sequences (np.ndarray): N x S x 4 one-hot encoded sequences.\n            labels (np.ndarray): N x 1 binary labels.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_pos = N // 2\n    n_neg = N - n_pos\n    \n    sequences_int = []\n    labels = np.array([1] * n_pos + [0] * n_neg)\n\n    # Ground-truth motif\n    motif_bases = rng.choice(4, size=L)\n\n    # Positive sequences\n    for _ in range(n_pos):\n        seq = rng.choice(4, size=S)\n        if signal:\n            start_pos = rng.integers(0, S - L + 1)\n            seq[start_pos : start_pos + L] = motif_bases\n        sequences_int.append(seq)\n        \n    # Negative sequences\n    for _ in range(n_neg):\n        seq = rng.choice(4, size=S)\n        sequences_int.append(seq)\n\n    sequences_one_hot = np.array([one_hot_encode(s) for s in sequences_int])\n    return sequences_one_hot, labels\n\ndef create_feature_matrix(sequences, S, L):\n    \"\"\"\n    Computes the average patch features for a set of sequences.\n    \n    Args:\n        sequences (np.ndarray): N x S x 4 one-hot encoded sequences.\n        S (int): Sequence length.\n        L (int): Motif length.\n\n    Returns:\n        np.ndarray: N x (4*L) feature matrix.\n    \"\"\"\n    N = sequences.shape[0]\n    num_windows = S - L + 1\n    phi_matrix = np.zeros((N, L, 4))\n\n    for n in range(N):\n        avg_patch = np.zeros((L, 4))\n        for t in range(num_windows):\n            avg_patch += sequences[n, t:t+L, :]\n        phi_matrix[n, :, :] = avg_patch / num_windows\n    \n    return phi_matrix.reshape(N, 4 * L)\n\ndef train_model(phi_matrix, y, N, L, reg_type, lambda_val, T, tau):\n    \"\"\"\n    Trains the logistic regression model and computes sparsity.\n    \n    Args:\n        phi_matrix (np.ndarray): N x (4*L) feature matrix.\n        y (np.ndarray): N-element label vector.\n        N (int): Number of samples.\n        L (int): Kernel length.\n        reg_type (str): 'L1' or 'L2'.\n        lambda_val (float): Regularization strength.\n        T (int): Number of training steps.\n        tau (float): Threshold for sparsity calculation.\n\n    Returns:\n        float: Sparsity fraction of the learned kernel weights.\n    \"\"\"\n    n_features = 4 * L\n    \n    # Augment features for intercept and Lipschitz constant calculation\n    tilde_phi = np.c_[phi_matrix, np.ones(N)]\n    \n    # Calculate Lipschitz constant bound for the gradient of the logistic loss\n    # L <= norm(tilde_Phi^T * tilde_Phi) / (4*N) = sigma_max(tilde_Phi)^2 / (4*N)\n    svd_vals = np.linalg.svd(tilde_phi, compute_uv=False)\n    sigma_max = svd_vals[0]\n    lipschitz_L = (sigma_max**2) / (4 * N)\n    \n    # Initialize weights and bias\n    w = np.zeros(n_features)\n    b = 0.0\n\n    if reg_type == 'L1':\n        alpha = 1.0 / lipschitz_L\n    elif reg_type == 'L2':\n        alpha = 1.0 / (lipschitz_L + lambda_val)\n    else:\n        raise ValueError(\"Invalid regularization type\")\n\n    for _ in range(T):\n        z = phi_matrix @ w + b\n        p = 1.0 / (1.0 + np.exp(-z))\n        \n        error = p - y\n        \n        grad_w = (1.0 / N) * phi_matrix.T @ error\n        grad_b = (1.0 / N) * np.sum(error)\n        \n        if reg_type == 'L1':\n            w_temp = w - alpha * grad_w\n            w = np.sign(w_temp) * np.maximum(np.abs(w_temp) - alpha * lambda_val, 0)\n            b = b - alpha * grad_b\n        elif reg_type == 'L2':\n            w = w - alpha * (grad_w + lambda_val * w)\n            b = b - alpha * grad_b\n            \n    sparsity = np.sum(np.abs(w) < tau) / n_features\n    return sparsity\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Global parameters\n    S = 40\n    L = 8\n    T = 1000\n    tau = 1e-3\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'case': 1, 'reg_type': 'L1', 'lambda_val': 0.05, 'N': 600, 'seed': 7,  'signal': True},\n        {'case': 2, 'reg_type': 'L2', 'lambda_val': 0.05, 'N': 600, 'seed': 7, 'signal': True},\n        {'case': 3, 'reg_type': 'L1', 'lambda_val': 0.3,  'N': 600, 'seed': 21, 'signal': False},\n        {'case': 4, 'reg_type': 'L1', 'lambda_val': 1e-6, 'N': 600, 'seed': 11, 'signal': True},\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate data\n        sequences, labels = generate_data(\n            N=case['N'], S=S, L=L, seed=case['seed'], signal=case['signal']\n        )\n        \n        # 2. Create feature matrix\n        phi_matrix = create_feature_matrix(sequences, S, L)\n        \n        # 3. Train model and get sparsity\n        sparsity = train_model(\n            phi_matrix, labels, case['N'], L,\n            case['reg_type'], case['lambda_val'], T, tau\n        )\n        \n        results.append(sparsity)\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(f'{v:.3f}' for v in results)}]\")\n\nsolve()\n```", "id": "2382359"}]}