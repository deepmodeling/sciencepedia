{"hands_on_practices": [{"introduction": "In biological systems, a key question is not just which individual gene or protein is important, but which *class* of molecular features drives an outcome like gene expression. This exercise provides a hands-on introduction to group feature importance, a method for quantifying the collective contribution of related variables. By working through a simplified linear model, you will learn how to use variance decomposition to compare the predictive power of two entire feature sets—in this hypothetical case, DNA methylation versus histone modifications [@problem_id:2400021].", "problem": "You are given a formalized setting to compare the predictive contribution of two molecular feature groups, Deoxyribonucleic acid (DNA) methylation and histone modifications, to gene expression in a cell line. Consider a generative linear model for an expression variable $y$ defined by\n$$\ny \\;=\\; X^{(M)} w^{(M)} \\;+\\; X^{(H)} w^{(H)} \\;+\\; \\varepsilon,\n$$\nwhere $X^{(M)} \\in \\mathbb{R}^{n \\times m}$ denotes the DNA methylation features, $X^{(H)} \\in \\mathbb{R}^{n \\times h}$ denotes the histone modification features, $w^{(M)} \\in \\mathbb{R}^{m}$ and $w^{(H)} \\in \\mathbb{R}^{h}$ are fixed coefficient vectors, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a zero-mean noise vector with variance $\\sigma^2$ per sample. Assume that each column of $X^{(M)}$ and $X^{(H)}$ is an independent standardized random variable with variance $1$, that $X^{(M)}$, $X^{(H)}$, and $\\varepsilon$ are mutually independent, and that $n$ is sufficiently large for population variance expressions to apply.\n\nDefine the group feature importance for DNA methylation as the proportion of variance in $y$ attributable to DNA methylation under the model assumptions:\n$$\nI_M \\;=\\; \\frac{\\mathrm{Var}\\!\\left(X^{(M)} w^{(M)}\\right)}{\\mathrm{Var}(y)}.\n$$\nSimilarly, define the group feature importance for histone modifications as\n$$\nI_H \\;=\\; \\frac{\\mathrm{Var}\\!\\left(X^{(H)} w^{(H)}\\right)}{\\mathrm{Var}(y)}.\n$$\nUnder the stated assumptions, these quantities reduce to the following expressions in terms of the coefficient vectors and the noise variance:\n$$\n\\mathrm{Var}\\!\\left(X^{(M)} w^{(M)}\\right) \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2,\\quad\n\\mathrm{Var}\\!\\left(X^{(H)} w^{(H)}\\right) \\;=\\; \\left\\| w^{(H)} \\right\\|_2^2,\\quad\n\\mathrm{Var}(y) \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2.\n$$\nYour task is to write a program that, for each parameter set in the test suite below, computes $I_M$ and $I_H$ using the above definitions and outputs, for each case, a single integer decision according to the following rule with tolerance $\\delta = 10^{-9}$:\n- Output $1$ if $I_M - I_H \\ge \\delta$.\n- Output $-1$ if $I_H - I_M \\ge \\delta$.\n- Output $0$ otherwise.\n\nTest Suite (each case specifies $w^{(M)}$, $w^{(H)}$, and $\\sigma$):\n- Case $1$: $w^{(M)} = (1.0,\\, 0.8,\\, 0.0,\\, 0.0,\\, 0.0)$, $w^{(H)} = (0.3,\\, 0.0,\\, 0.0,\\, 0.0)$, $\\sigma = 0.5$.\n- Case $2$: $w^{(M)} = (0.2,\\, 0.0,\\, 0.0)$, $w^{(H)} = (0.9,\\, 0.7,\\, 0.0,\\, 0.0)$, $\\sigma = 0.4$.\n- Case $3$: $w^{(M)} = (0.5,\\, 0.5)$, $w^{(H)} = (0.7071067811865476,\\, 0.0)$, $\\sigma = 0.0$.\n- Case $4$: $w^{(M)} = (0.0,\\, 0.0,\\, 0.0)$, $w^{(H)} = (0.2,\\, 0.2,\\, 0.2)$, $\\sigma = 1.0$.\n- Case $5$: $w^{(M)} = (0.0,\\, 0.0)$, $w^{(H)} = (0.0,\\, 0.0)$, $\\sigma = 0.3$.\n\nYour program should produce a single line of output containing the results for the five cases as a comma-separated list of integers enclosed in square brackets (for example, $[1,-1,0,-1,0]$). No additional output or whitespace is permitted. Angles are not involved. No physical units are involved. All numerical computations must be performed using standard real arithmetic.", "solution": "The posed problem is valid as it is scientifically grounded, well-posed, and objective. It presents a clear computational task based on a simplified but plausible statistical model of gene regulation, which is a standard approach in computational biology. The definitions, assumptions, and test cases are complete and internally consistent. We will proceed with a solution.\n\nThe objective is to compare the predictive contribution of two feature groups, DNA methylation ($M$) and histone modifications ($H$), to a biological response variable, gene expression ($y$). This is accomplished by quantifying the proportion of variance in $y$ explained by each feature group. This is a fundamental technique in model interpretation.\n\nThe problem defines a linear model for gene expression $y$ as:\n$$\ny \\;=\\; X^{(M)} w^{(M)} \\;+\\; X^{(H)} w^{(H)} \\;+\\; \\varepsilon\n$$\nHere, $y$ is the centered response vector. The terms $X^{(M)} w^{(M)}$ and $X^{(H)} w^{(H)}$ represent the contributions from DNA methylation and histone modification features, respectively, weighted by fixed coefficient vectors $w^{(M)}$ and $w^{(H)}$. The term $\\varepsilon$ represents stochastic noise, assumed to be independent and identically distributed with mean $0$ and variance $\\sigma^2$ for each sample.\n\nA critical assumption is that the feature matrices $X^{(M)}$ and $X^{(H)}$ consist of columns that are independent, standardized random variables (mean $0$, variance $1$). Furthermore, the feature groups and the noise are mutually independent. Under these conditions, the total variance of the response $y$ can be additively decomposed, a property stemming from the Bienaymé formula for the variance of sums of uncorrelated random variables.\n\nFor a single sample $y_i$, the variance is:\n$$\n\\mathrm{Var}(y_i) \\;=\\; \\mathrm{Var}\\left(\\sum_{j} X_{ij}^{(M)} w_{j}^{(M)}\\right) \\;+\\; \\mathrm{Var}\\left(\\sum_{k} X_{ik}^{(H)} w_{k}^{(H)}\\right) \\;+\\; \\mathrm{Var}(\\varepsilon_i)\n$$\nDue to the independence and unit variance of the features $X_{ij}$:\n$$\n\\mathrm{Var}\\left(\\sum_{j} X_{ij}^{(M)} w_{j}^{(M)}\\right) \\;=\\; \\sum_{j} (w_{j}^{(M)})^2 \\mathrm{Var}(X_{ij}^{(M)}) \\;=\\; \\sum_{j} (w_{j}^{(M)})^2 \\cdot 1 \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2\n$$\nSimilarly, $\\mathrm{Var}\\left(\\sum_k X_{ik}^{(H)} w_k^{(H)}\\right) = \\left\\| w^{(H)} \\right\\|_2^2$. The noise variance is given as $\\mathrm{Var}(\\varepsilon_i) = \\sigma^2$.\nThus, the total variance of $y$ is correctly given as:\n$$\n\\mathrm{Var}(y) \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2\n$$\nThe squared L$2$-norm, $\\left\\| w \\right\\|_2^2$, is computed as the sum of the squares of the elements of the vector $w$.\n\nThe group feature importance, defined as the proportion of total variance, is then calculated. For DNA methylation:\n$$\nI_M \\;=\\; \\frac{\\mathrm{Var}(X^{(M)} w^{(M)})}{\\mathrm{Var}(y)} \\;=\\; \\frac{\\left\\| w^{(M)} \\right\\|_2^2}{\\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2}\n$$\nAnd for histone modifications:\n$$\nI_H \\;=\\; \\frac{\\mathrm{Var}(X^{(H)} w^{(H)})}{\\mathrm{Var}(y)} \\;=\\; \\frac{\\left\\| w^{(H)} \\right\\|_2^2}{\\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2}\n$$\nThe computational task is to evaluate these quantities for each provided parameter set and apply a decision rule. For each case, we will compute the values of $\\left\\| w^{(M)} \\right\\|_2^2$, $\\left\\| w^{(H)} \\right\\|_2^2$, and $\\sigma^2$. Let these be $V_M$, $V_H$, and $V_{\\text{noise}}$, respectively. The total variance is $V_{\\text{total}} = V_M + V_H + V_{\\text{noise}}$. Then, $I_M = V_M / V_{\\text{total}}$ and $I_H = V_H / V_{\\text{total}}$. Note that if $V_{\\text{total}} = 0$, which occurs only if all weights and $\\sigma$ are zero, these importances are undefined. This situation does not arise in the given test suite.\n\nThe final decision is based on the difference $D = I_M - I_H$, with a tolerance $\\delta = 10^{-9}$:\n1.  If $D \\ge \\delta$, the contribution of methylation is determined to be larger. The output is $1$.\n2.  If $-D \\ge \\delta$ (which is equivalent to $D \\le -\\delta$), the contribution of histones is determined to be larger. The output is $-1$.\n3.  Otherwise, if $|D| < \\delta$, their contributions are considered indistinguishable within the given tolerance. The output is $0$.\n\nThis procedure will be implemented for each test case to generate the final list of decisions. For example, for Case $1$: $w^{(M)} = (1.0, 0.8, \\dots)$, $w^{(H)} = (0.3, \\dots)$, $\\sigma = 0.5$.\n$V_M = 1.0^2 + 0.8^2 = 1.64$.\n$V_H = 0.3^2 = 0.09$.\n$V_{\\text{noise}} = 0.5^2 = 0.25$.\n$V_{\\text{total}} = 1.64 + 0.09 + 0.25 = 1.98$.\n$I_M = 1.64 / 1.98$, $I_H = 0.09 / 1.98$.\n$I_M - I_H = (1.64 - 0.09) / 1.98 = 1.55 / 1.98 \\approx 0.7828$. Since $0.7828 \\ge 10^{-9}$, the output is $1$. The algorithm proceeds identically for all cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares group feature importances for DNA methylation and histone modifications\n    based on a generative linear model.\n    \"\"\"\n    # Test suite with parameter sets (w_M, w_H, sigma).\n    test_cases = [\n        # Case 1\n        (np.array([1.0, 0.8, 0.0, 0.0, 0.0]), np.array([0.3, 0.0, 0.0, 0.0]), 0.5),\n        # Case 2\n        (np.array([0.2, 0.0, 0.0]), np.array([0.9, 0.7, 0.0, 0.0]), 0.4),\n        # Case 3\n        (np.array([0.5, 0.5]), np.array([0.7071067811865476, 0.0]), 0.0),\n        # Case 4\n        (np.array([0.0, 0.0, 0.0]), np.array([0.2, 0.2, 0.2]), 1.0),\n        # Case 5\n        (np.array([0.0, 0.0]), np.array([0.0, 0.0]), 0.3),\n    ]\n\n    # Tolerance for comparison.\n    delta = 1e-9\n\n    results = []\n    \n    for w_M, w_H, sigma in test_cases:\n        # Calculate the variance component for DNA methylation: ||w^(M)||_2^2\n        v_M = np.dot(w_M, w_M)\n        \n        # Calculate the variance component for histone modifications: ||w^(H)||_2^2\n        v_H = np.dot(w_H, w_H)\n        \n        # Calculate the variance component for noise: sigma^2\n        v_noise = sigma**2\n        \n        # Calculate the total variance of y.\n        v_total = v_M + v_H + v_noise\n        \n        # Calculate group feature importances I_M and I_H.\n        # Handle the edge case where total variance is zero, though not in the test data.\n        if v_total == 0.0:\n            i_M = 0.0\n            i_H = 0.0\n        else:\n            i_M = v_M / v_total\n            i_H = v_H / v_total\n            \n        # Calculate the difference in importances.\n        diff = i_M - i_H\n        \n        # Apply the decision rule.\n        if diff >= delta:\n            # Importance of methylation is significantly greater.\n            decision = 1\n        elif diff <= -delta:\n            # Importance of histone modifications is significantly greater.\n            decision = -1\n        else:\n            # Importances are not significantly different.\n            decision = 0\n            \n        results.append(decision)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2400021"}, {"introduction": "A powerful form of model interpretation is generating counterfactual explanations, which answer the question: \"What is the minimal change needed to alter a prediction?\" This practice moves beyond passive observation to active intervention, a critical step for generating new biological hypotheses. You will implement an algorithm to find the smallest set of mutations that flips a protein's predicted function from \"active\" to \"inactive\" in a linear sequence model, a task that has direct applications in protein engineering and design [@problem_id:2399979].", "problem": "You are given a discrete, additive, sequence-based classifier used for protein function prediction, intended to support counterfactual explanations in an interpretable Machine Learning (ML) setting for computational biology and bioinformatics. A protein sequence of length $L$ over an alphabet of $A$ amino acid categories is represented by indices $x_i \\in \\{0,1,\\dots,A-1\\}$ for positions $i \\in \\{0,1,\\dots,L-1\\}$. The model score for a sequence $x$ is\n$$\ns(x) \\;=\\; \\sum_{i=0}^{L-1} W_{i,\\,x_i} \\;+\\; b,\n$$\nwith a weight matrix $W \\in \\mathbb{R}^{L \\times A}$ and a scalar bias $b \\in \\mathbb{R}$. The predicted label is \"active\" if $s(x) \\ge 0$ and \"inactive\" otherwise. A single-residue mutation is defined as replacing $x_p$ at position $p$ with a new amino acid index $a' \\in \\{0,1,\\dots,A-1\\}$ where $a' \\ne x_p$. A set of mutations must act on distinct positions. Applying a set of mutations to $x$ yields a mutated sequence $x'$ and a new score $s(x')$.\n\nYour task is to compute a minimal counterfactual explanation that flips the prediction from \"active\" to \"inactive\" by finding a set of mutations of minimal cardinality. If the original sequence is already \"inactive\", the minimal set is the empty set. If no set of mutations can make the sequence \"inactive\", report that it is impossible.\n\nTie-breaking rules to make the result unique:\n- Among all mutation sets that achieve \"inactive,\" choose one with the smallest number of mutations.\n- Among those, choose the one that yields the smallest final score $s(x')$ (that is, the most negative value).\n- If there is still a tie, represent the mutation set as a list of pairs $(p,a')$ sorted in ascending order by $p$; among solutions tied on $s(x')$, choose the lexicographically smallest such list by comparing $(p,a')$ pairs with $p$ ascending and $a'$ ascending.\n\nAll indices (positions and amino acid categories) must be represented as integers using zero-based indexing.\n\nTest suite. For each test case below, $W$ and $b$ specify the model, and the sequence $x$ is given. For Test Cases $1$–$6$, use the same model $(W^{(1)}, b^{(1)})$ of length $L=5$ and alphabet size $A=4$:\n$$\nW^{(1)} \\;=\\;\n\\begin{bmatrix}\n2 & -1 & 0 & 1\\\\\n1 & 2 & -2 & 0\\\\\n0 & -1 & 3 & -2\\\\\n2 & 0 & -1 & -1\\\\\n-1 & 1 & 0 & 2\n\\end{bmatrix},\n\\qquad\nb^{(1)} \\;=\\; -1.\n$$\nUse the following sequences $x$ for Test Cases $1$–$6$:\n- Test Case $1$: $x = [1,\\,1,\\,0,\\,0,\\,1]$.\n- Test Case $2$: $x = [1,\\,1,\\,0,\\,1,\\,2]$.\n- Test Case $3$: $x = [0,\\,0,\\,2,\\,0,\\,3]$.\n- Test Case $4$: $x = [0,\\,3,\\,0,\\,0,\\,0]$.\n- Test Case $5$: $x = [1,\\,0,\\,0,\\,1,\\,0]$.\n- Test Case $6$: $x = [2,\\,3,\\,0,\\,0,\\,1]$.\n\nFor Test Case $7$, use a different model $(W^{(2)}, b^{(2)})$ with $L=3$ and $A=2$:\n$$\nW^{(2)} \\;=\\;\n\\begin{bmatrix}\n2 & 3\\\\\n1 & 4\\\\\n0 & 5\n\\end{bmatrix},\n\\qquad\nb^{(2)} \\;=\\; 0,\n$$\nand sequence:\n- Test Case $7$: $x = [0,\\,0,\\,0]$.\n\nRequired final output format. Your program should produce a single line of output containing the results for Test Cases $1$ through $7$ as a comma-separated list enclosed in square brackets, where each test case result is itself a list encoded as follows:\n- If it is impossible to flip to \"inactive,\" output the single-element list $[-1]$.\n- If the original sequence is already \"inactive,\" output the single-element list $[0]$.\n- Otherwise, output a list beginning with the minimal number of mutations $m$, followed by $2m$ integers giving the pairs $(p_1,a'_1),(p_2,a'_2),\\dots,(p_m,a'_m)$ in ascending order by $p$: that is, $[m,\\,p_1,\\,a'_1,\\,p_2,\\,a'_2,\\,\\dots,\\,p_m,\\,a'_m]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the seven per-test-case lists, enclosed in square brackets (for example, $[[\\dots],[\\dots],\\dots]$). No external input is needed, and there are no physical units or angles involved in this problem. Answers for each test case must be fully determined by the definitions above.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model:** A discrete, additive, sequence-based classifier.\n- **Sequence representation:** A sequence $x$ of length $L$ over an alphabet of size $A$ is given by indices $x_i \\in \\{0, 1, \\dots, A-1\\}$ for positions $i \\in \\{0, 1, \\dots, L-1\\}$.\n- **Score function:** $s(x) = \\sum_{i=0}^{L-1} W_{i, x_i} + b$, where $W \\in \\mathbb{R}^{L \\times A}$ is a weight matrix and $b \\in \\mathbb{R}$ is a scalar bias.\n- **Classification rule:** The sequence is \"active\" if $s(x) \\ge 0$, and \"inactive\" otherwise.\n- **Mutation:** A single-residue mutation at position $p$ replaces $x_p$ with a new index $a' \\in \\{0, 1, \\dots, A-1\\}$ where $a' \\ne x_p$. Sets of mutations must act on distinct positions.\n- **Objective:** Find a minimal counterfactual explanation, which is a set of mutations of minimal cardinality that transforms an \"active\" sequence into an \"inactive\" one.\n- **Special Cases:**\n    - If $s(x) < 0$ (already \"inactive\"), the minimal set is empty.\n    - If no set of mutations can achieve $s(x') < 0$, the task is impossible.\n- **Tie-Breaking Rules:**\n    1.  Choose a mutation set with the smallest cardinality (number of mutations).\n    2.  Among those, choose the one yielding the smallest final score $s(x')$.\n    3.  If a tie persists, represent mutations as pairs $(p, a')$, sort them by position $p$, and choose the lexicographically smallest list of such pairs.\n- **Provided Data:**\n    - Test Cases $1-6$: $L=5, A=4$.\n      $$W^{(1)} = \\begin{bmatrix} 2 & -1 & 0 & 1 \\\\ 1 & 2 & -2 & 0 \\\\ 0 & -1 & 3 & -2 \\\\ 2 & 0 & -1 & -1 \\\\ -1 & 1 & 0 & 2 \\end{bmatrix}, \\quad b^{(1)} = -1$$\n      Sequences: $x_1=[1,1,0,0,1]$, $x_2=[1,1,0,1,2]$, $x_3=[0,0,2,0,3]$, $x_4=[0,3,0,0,0]$, $x_5=[1,0,0,1,0]$, $x_6=[2,3,0,0,1]$.\n    - Test Case $7$: $L=3, A=2$.\n      $$W^{(2)} = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\\\ 0 & 5 \\end{bmatrix}, \\quad b^{(2)} = 0$$\n      Sequence: $x_7=[0,0,0]$.\n- **Output Format:**\n    - A single-line list of lists, e.g., `[[...],[...]]`.\n    - `[-1]` for impossible cases.\n    - `[0]` for already inactive sequences.\n    - `[m, p_1, a'_1, ..., p_m, a'_m]` for successful counterfactuals, sorted by $p_i$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded:** The model described is a Position-Specific Scoring Matrix (PSSM), a standard and fundamental tool in bioinformatics for modeling protein domains and DNA binding sites. The concept of a counterfactual explanation is a cornerstone of interpretable machine learning. The problem is thus scientifically sound and relevant to the specified fields.\n- **Well-Posed:** The problem seeks an optimal solution under a clear, hierarchical set of objective criteria (cardinality, final score, lexicographical order). This ensures that a unique solution exists and is well-defined. The search space is finite, guaranteeing that a solution can be found algorithmically.\n- **Objective:** The problem is formulated with mathematical precision, using standard notation. All terms are defined unambiguously, and all necessary data for the test cases are provided. The language is objective and devoid of any speculative or subjective claims.\n\nThe problem does not exhibit any of the invalidity flags. It is self-contained, logically consistent, and computationally tractable.\n\n### Step 3: Verdict and Action\n**Verdict:** The problem is valid.\n**Action:** Proceed with providing a solution.\n\n### Solution\n\nThe problem asks for a minimal set of mutations to change a protein's classification from \"active\" ($s(x) \\ge 0$) to \"inactive\" ($s(x') < 0$). This is an optimization problem governed by a strict set of tie-breaking rules.\n\nFirst, let us analyze the effect of a mutation. The score of a sequence $x$ is $s(x) = b + \\sum_{i=0}^{L-1} W_{i,x_i}$. A single mutation at position $p$, changing the amino acid from $x_p$ to $a'$, results in a new sequence $x'$. The score of $x'$ is $s(x') = s(x) - W_{p,x_p} + W_{p,a'}$. The change in score is thus $\\Delta s = W_{p,a'} - W_{p,x_p}$. To make the sequence inactive, we must achieve a final score $s(x') < 0$. This requires selecting a set of mutations whose cumulative score change reduces the initial score $s(x)$ to below zero.\n\nThe optimization is hierarchical:\n1.  Minimize the number of mutations, $|M|$.\n2.  Minimize the final score, $s(x')$.\n3.  Minimize the lexicographical representation of the mutation set.\n\nThis structure allows for a greedy approach. To minimize the number of mutations, we should apply the most effective mutations first—those that provide the largest possible decrease in score.\n\nThe overall algorithm is as follows:\n\n1.  **Calculate Initial Score:** Compute the initial score $s(x)$ for the given sequence $x$. If $s(x) < 0$, the sequence is already \"inactive\". The task is complete, and the result is an empty set of mutations.\n\n2.  **Identify Optimal Single Mutations:** For each position $p \\in \\{0, 1, \\dots, L-1\\}$, we determine the single best mutation. A mutation is defined by the new amino acid index $a' \\in \\{0, 1, \\dots, A-1\\}$, where $a' \\ne x_p$. The \"best\" mutation is the one that minimizes the score. This corresponds to the $a'$ that minimizes the weight $W_{p,a'}$.\n    - If multiple choices of $a'$ yield the same minimum weight, the tie-breaking rule mandates choosing the lexicographically smallest list of mutations. This implies selecting the smallest index $a'$.\n    - Let this optimal new amino acid for position $p$ be $a'_p$. The corresponding score change is $\\Delta_p = W_{p,a'_p} - W_{p,x_p}$.\n    - We are only interested in mutations that decrease the score, so we only consider positions where $\\Delta_p < 0$. These are \"beneficial\" mutations.\n\n3.  **Create and Sort a List of Potential Mutations:** We compile a list of all such beneficial \"best-per-position\" mutations. Each element in the list is a tuple containing the score change, the position, and the new amino acid: $(\\Delta_p, p, a'_p)$. If no such beneficial mutations exist, it is impossible to decrease the score, and the problem has no solution.\n\n4.  **Greedy Selection:** To satisfy the optimization criteria, we must sort this list. To minimize the final score for a fixed number of mutations, we must choose the mutations with the most negative $\\Delta_p$. To break ties in score and satisfy the lexicographical requirement, we should prefer mutations at smaller position indices $p$. Therefore, we sort the list of potential mutations in ascending order, primarily by $\\Delta_p$ and secondarily by $p$.\n\n5.  **Apply Mutations Iteratively:** We iterate through the sorted list, applying one mutation at a time. We add the mutation's score change $\\Delta_p$ to a running total of the score and add the mutation $(p, a'_p)$ to our solution set. After each addition, we check if the current score has dropped below $0$. The first time this condition is met, we have found the solution. This greedy strategy is guaranteed to find a solution with minimal cardinality because we are always using the most effective available mutations. It also produces the minimal possible score for that cardinality. The sorting order ensures the lexicographical requirement is met in case of ties in score.\n\n6.  **Format the Output:** Once the minimal set of mutations is found, we sort it by position $p$ and format it according to the problem specification: $[m, p_1, a'_1, \\dots, p_m, a'_m]$, where $m$ is the number of mutations. If the initial sequence was inactive, the output is `[0]`. If no solution was found, the output is `[-1]`.\n\nThis systematic procedure correctly navigates the problem's constraints and yields the unique, optimal counterfactual explanation.", "answer": "```python\nimport numpy as np\n\ndef list_to_str(lst):\n    \"\"\"Converts a list of integers to a string format '[i1,i2,...]'.\"\"\"\n    return f\"[{','.join(map(str, lst))}]\"\n\ndef find_counterfactual(W, b, x):\n    \"\"\"\n    Finds the minimal counterfactual explanation for a given sequence.\n    \"\"\"\n    L, A = W.shape\n    \n    # Step 1: Calculate initial score\n    current_score = b + sum(W[i, x[i]] for i in range(L))\n    \n    if current_score < 0:\n        return [0]\n    \n    # Step 2 & 3: Identify and collect best beneficial mutations for each position\n    potential_mutations = []\n    for p in range(L):\n        original_aa = x[p]\n        original_weight = W[p, original_aa]\n        \n        best_mut_aa = -1\n        min_weight = float('inf')\n        \n        for a_prime in range(A):\n            if a_prime == original_aa:\n                continue\n            \n            if W[p, a_prime] < min_weight:\n                min_weight = W[p, a_prime]\n                best_mut_aa = a_prime\n            # Tie-breaking rule: choose smallest a' for same weight\n            elif W[p, a_prime] == min_weight:\n                if a_prime < best_mut_aa:\n                    best_mut_aa = a_prime\n                    \n        delta = min_weight - original_weight\n        \n        if delta < 0:\n            potential_mutations.append((delta, p, best_mut_aa))\n            \n    if not potential_mutations:\n        return [-1]\n\n    # Step 4: Sort potential mutations\n    # Primary key: delta (ascending), Secondary key: position p (ascending)\n    potential_mutations.sort(key=lambda item: (item[0], item[1]))\n    \n    # Step 5: Greedily apply mutations\n    mutations_applied = []\n    final_score = current_score\n    \n    for delta, p, a_prime in potential_mutations:\n        final_score += delta\n        mutations_applied.append((p, a_prime))\n        \n        if final_score < 0:\n            break\n    \n    # If loop finishes and score is still not negative, it's impossible\n    if final_score >= 0:\n        return [-1]\n        \n    # Step 6: Format the output\n    mutations_applied.sort(key=lambda item: item[0]) # Sort by position\n    \n    m = len(mutations_applied)\n    result = [m]\n    for p, a_prime in mutations_applied:\n        result.extend([p, a_prime])\n        \n    return result\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Test cases definition\n    W1 = np.array([\n        [2, -1, 0, 1],\n        [1, 2, -2, 0],\n        [0, -1, 3, -2],\n        [2, 0, -1, -1],\n        [-1, 1, 0, 2]\n    ])\n    b1 = -1.0\n    \n    x_sequences_1 = [\n        np.array([1, 1, 0, 0, 1]), # TC1\n        np.array([1, 1, 0, 1, 2]), # TC2\n        np.array([0, 0, 2, 0, 3]), # TC3\n        np.array([0, 3, 0, 0, 0]), # TC4\n        np.array([1, 0, 0, 1, 0]), # TC5\n        np.array([2, 3, 0, 0, 1]), # TC6\n    ]\n\n    W2 = np.array([\n        [2, 3],\n        [1, 4],\n        [0, 5]\n    ])\n    b2 = 0.0\n    x_sequence_2 = np.array([0, 0, 0]) # TC7\n    \n    test_cases = []\n    for x in x_sequences_1:\n        test_cases.append((W1, b1, x))\n    test_cases.append((W2, b2, x_sequence_2))\n    \n    results = []\n    for W, b, x in test_cases:\n        result = find_counterfactual(W, b, x)\n        results.append(result)\n    \n    # Format the final output string\n    all_results_str = [list_to_str(res) for res in results]\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "2399979"}, {"introduction": "One of the most significant challenges in applying machine learning to biology is ensuring that models learn true biological signals rather than spurious artifacts from the experimental process, a phenomenon sometimes called a \"Clever Hans\" effect. This advanced exercise places you in the role of a critical data scientist, tasked with diagnosing a model that may be relying on batch effects instead of biological features to make its predictions. By combining performance evaluation on out-of-distribution data with permutation feature importance, you will develop a robust workflow for detecting and verifying these critical model failures [@problem_id:2400032].", "problem": "You are given the task of designing a program that uncovers a \"Clever Hans\" effect, in which a predictive model uses experimental batch artifacts instead of biological signal to make predictions. Your implementation must use interpretability tools grounded in first principles to detect this behavior. The goal is to design a synthetic scenario that reflects plausible biological data generation and then detect, from first principles, whether a linear classifier trained under empirical risk minimization is relying primarily on batch artifacts rather than biological signal.\n\nStart from the following foundational base:\n- Empirical risk minimization optimizes a model parameter vector to minimize an empirical loss. For binary classification, a common choice is the logistic loss. For samples $\\{(x_i,y_i)\\}_{i=1}^n$ with $y_i \\in \\{0,1\\}$ and features $x_i \\in \\mathbb{R}^d$, a logistic regression with weights $w \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$ minimizes\n$$\n\\mathcal{L}(w,b) \\;=\\; \\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log\\left(1-\\sigma(w^\\top x_i + b)\\right)\\right] \\;+\\; \\frac{\\lambda}{2}\\|w\\|_2^2,\n$$\nwhere $\\sigma(z) = \\frac{1}{1+e^{-z}}$ and $\\lambda \\ge 0$ is the regularization strength.\n- A batch artifact is a non-biological source of variation, here modeled as a binary batch indicator that can be spuriously correlated with the label. Let $y \\in \\{0,1\\}$ be the biological phenotype. Let $b \\in \\{0,1\\}$ be a batch assignment. Define a confounding parameter $c \\in [0,1]$ such that\n$$\n\\mathbb{P}(b = y) = c, \\quad \\mathbb{P}(b \\ne y) = 1-c.\n$$\nThus, when $c = 1$, the batch is perfectly confounded with the label, and when $c = \\tfrac{1}{2}$ the batch is independent of the label.\n- Biological features and batch features are generated as follows. Let there be $p_b$ biological features and $p_a$ batch features. For each sample $i$:\n    - Draw $y_i \\sim \\mathrm{Bernoulli}(0.5)$.\n    - Draw $b_i$ such that $b_i = y_i$ with probability $c$ and $b_i = 1-y_i$ with probability $1-c$.\n    - Biological feature vector $x^{(bio)}_i \\in \\mathbb{R}^{p_b}$ is drawn with class-dependent mean, $x^{(bio)}_i \\sim \\mathcal{N}(\\mu^{(bio)}(y_i), \\sigma^2 I)$ where $\\mu^{(bio)}(1)=+\\mu_b \\cdot \\mathbf{1}$ and $\\mu^{(bio)}(0)=-\\mu_b \\cdot \\mathbf{1}$.\n    - Batch feature vector $x^{(art)}_i \\in \\mathbb{R}^{p_a}$ is drawn with batch-dependent mean, $x^{(art)}_i \\sim \\mathcal{N}(\\mu^{(art)}(b_i), \\sigma^2 I)$ where $\\mu^{(art)}(1)=+\\mu_a \\cdot \\mathbf{1}$ and $\\mu^{(art)}(0)=-\\mu_a \\cdot \\mathbf{1}$.\n    - Concatenate to form $x_i = [x^{(bio)}_i, x^{(art)}_i] \\in \\mathbb{R}^{p_b + p_a}$.\n- Permutation feature importance for a group of features $G$ on a validation set $\\mathcal{D}_{val}$ is defined as the drop in a chosen performance metric when the features in $G$ are permuted across samples, thereby breaking their dependence structure with the target. Using accuracy as the metric, the group importance is\n$$\nI(G) = \\mathrm{Acc}(\\mathcal{D}_{val}) - \\mathrm{Acc}(\\pi_G(\\mathcal{D}_{val})),\n$$\nwhere $\\pi_G(\\cdot)$ denotes independent random permutation of the columns indexed by $G$ across samples in the validation set.\n\nYour program must:\n1. Generate data according to the process above for a training and validation split from a confounded distribution with parameter $c_{train} \\in [0,1]$ and for a separate deconfounded test set with $c_{test} = \\tfrac{1}{2}$.\n2. Train a regularized logistic regression classifier by minimizing the regularized logistic loss $\\mathcal{L}(w,b)$ on the training set with an $\\ell_2$ penalty strength $\\lambda > 0$.\n3. Compute validation accuracy on the validation split from the confounded distribution and test accuracy on the deconfounded test set.\n4. Compute group permutation feature importances $I(G_{bio})$ and $I(G_{art})$ on the validation set for the biological group $G_{bio}$ and batch-artifact group $G_{art}$.\n5. Detect a \"Clever Hans\" effect if both conditions hold:\n   - The batch-artifact group has strictly larger importance than the biological group by at least a small margin $\\epsilon > 0$, i.e., $I(G_{art}) \\ge I(G_{bio}) + \\epsilon$.\n   - The generalization gap from confounded validation to deconfounded test exceeds a threshold $\\Delta > 0$, i.e., $\\mathrm{Acc}_{val} - \\mathrm{Acc}_{test} \\ge \\Delta$.\n\nUse fixed values $\\lambda = 1$, $\\epsilon = 0.01$, $\\Delta = 0.2$, and Gaussian noise standard deviation $\\sigma = 1$. Use a deterministic split with a training fraction of $0.6$ and a validation fraction of $0.4$. For each dataset, use the specified integer random seed to construct a NumPy random number generator.\n\nImplement your solution in Python and evaluate it on the following test suite, where each tuple encodes $(N, p_b, p_a, \\mu_b, \\mu_a, c_{train}, seed)$:\n- Case A (strong confounding, strong artifact, weak biology): $(4000, 50, 2, 0.3, 3.0, 0.95, 42)$.\n- Case B (no confounding, strong biology): $(4000, 50, 2, 1.5, 3.0, 0.5, 43)$.\n- Case C (partial confounding, biology dominates): $(4000, 50, 2, 1.5, 1.0, 0.7, 44)$.\n- Case D (perfect confounding, no biology): $(3000, 10, 10, 0.0, 2.0, 1.0, 45)$.\n- Case E (small sample, strong confounding, strong artifact): $(800, 20, 1, 0.3, 2.5, 0.95, 46)$.\n\nYour program must produce a single line of output containing a comma-separated list of boolean values enclosed in square brackets, indicating for each case whether a \"Clever Hans\" effect was detected. For example, the output format must be exactly like:\n\"[True,False,True,False,True]\"\n\nNo physical units are involved. Angles are not involved. Where fractions are described, they must be handled as decimals in code.\n\nConstraints:\n- You must implement training via numerical optimization of the regularized logistic loss defined above.\n- Permutation importance must be computed by jointly permuting all features in the group across validation samples.\n- The deconfounded test set must use $c_{test} = 0.5$ and have the same size as the validation split.\n- Use only the Python standard library, NumPy, and SciPy as needed for optimization. The final code must run without user input and must print only the required single output line.", "solution": "The problem requires the design and implementation of a program to detect a \"Clever Hans\" effect in a simulated biological classification scenario. This effect occurs when a predictive model exploits spurious correlations, such as experimental batch artifacts, instead of the true biological signal to make its predictions. The solution involves a three-stage process: first, generating synthetic data that models this phenomenon; second, training a classifier on this data; and third, applying interpretability methods based on first principles to diagnose the model's behavior.\n\nFirst, we formalize the data generation process. We consider a binary classification task where each sample $i$ has a true biological label $y_i \\in \\{0, 1\\}$, drawn from a Bernoulli distribution with parameter $0.5$. A batch assignment $b_i \\in \\{0, 1\\}$ is also associated with each sample. The correlation between the batch assignment and the biological label is controlled by a confounding parameter $c \\in [0, 1]$, such that the probability of the batch matching the label is $\\mathbb{P}(b_i = y_i) = c$. A value of $c=0.5$ indicates no confounding (independence), while $c=1.0$ indicates perfect confounding. Each sample's feature vector $x_i \\in \\mathbb{R}^{p_b + p_a}$ is composed of two parts: a biological feature vector $x^{(bio)}_i \\in \\mathbb{R}^{p_b}$ and a batch artifact feature vector $x^{(art)}_i \\in \\mathbb{R}^{p_a}$. The biological features are drawn from a class-conditional Gaussian distribution: $x^{(bio)}_i \\sim \\mathcal{N}(\\mu^{(bio)}(y_i), \\sigma^2 I)$, where the mean vector $\\mu^{(bio)}(y_i)$ depends on the true label. Specifically, $\\mu^{(bio)}(1) = +\\mu_b \\cdot \\mathbf{1}$ and $\\mu^{(bio)}(0) = -\\mu_b \\cdot \\mathbf{1}$, with $\\mu_b$ controlling the strength of the biological signal. Similarly, the artifact features are drawn from a batch-conditional Gaussian distribution: $x^{(art)}_i \\sim \\mathcal{N}(\\mu^{(art)}(b_i), \\sigma^2 I)$, where the mean $\\mu^{(art)}(b_i)$ depends on the batch assignment. Here, $\\mu^{(art)}(1) = +\\mu_a \\cdot \\mathbf{1}$ and $\\mu^{(art)}(0) = -\\mu_a \\cdot \\mathbf{1}$, with $\\mu_a$ controlling the artifact signal strength. The final feature vector is the concatenation $x_i = [x^{(bio)}_i, x^{(art)}_i]$.\n\nSecond, we train a logistic regression classifier on the generated training data. The model learns a weight vector $w \\in \\mathbb{R}^{p_b+p_a}$ and a bias term $b \\in \\mathbbR$ by minimizing the regularized negative log-likelihood, a form of empirical risk minimization. The objective function $\\mathcal{L}(w, b)$, also known as the logistic loss or cross-entropy loss with an $\\ell_2$ penalty, is given by:\n$$\n\\mathcal{L}(w,b) \\;=\\; \\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log\\left(1-\\sigma(w^\\top x_i + b)\\right)\\right] \\;+\\; \\frac{\\lambda}{2}\\|w\\|_2^2\n$$\nHere, $\\sigma(z) = (1+e^{-z})^{-1}$ is the sigmoid function, and $\\lambda > 0$ is the regularization parameter that penalizes large weights to prevent overfitting. This optimization problem is convex, and its minimum can be found efficiently using quasi-Newton methods such as L-BFGS. To do so, we must provide the gradient of the objective function with respect to the parameters. The gradient of the loss term for a single sample $(x_i, y_i)$ is $(\\sigma(w^\\top x_i + b) - y_i)x_i$ for the weights $w$ and $(\\sigma(w^\\top x_i + b) - y_i)$ for the bias $b$. The gradient of the regularization term is $\\lambda w$. Summing over all samples gives the full gradient used in optimization.\n\nThird, we devise a two-part criterion to detect the \"Clever Hans\" behavior.\nThe first part measures the model's failure to generalize from a confounded to a deconfounded setting. We evaluate the trained model's accuracy on two sets: a validation set, drawn from the same confounded distribution as the training data (with confounding $c_{train}$), and a test set, drawn from a deconfounded distribution where the batch artifact is independent of the label ($c_{test} = 0.5$). A significant drop in accuracy, $\\mathrm{Acc}_{val} - \\mathrm{Acc}_{test} \\ge \\Delta$, where $\\Delta$ is a predefined threshold, indicates that the model has learned a shortcut that is only useful in the confounded setting.\n\nThe second part uses an interpretability technique, permutation feature importance, to verify that the artifact features are indeed the source of this dependency. The importance of a group of features $G$, denoted $I(G)$, is the drop in model accuracy on the validation set when the association between those features and the target label is broken. This is achieved by permuting the values of the features in group $G$ across all samples in the validation set. The importance is then $I(G) = \\mathrm{Acc}_{val} - \\mathrm{Acc}_{\\pi_G(val)}$, where $\\mathrm{Acc}_{\\pi_G(val)}$ is the accuracy on the data with permuted features. We compute the importance for the biological feature group, $I(G_{bio})$, and the artifact feature group, $I(G_{art})$. If the model is a \"Clever Hans,\" it will rely more heavily on the artifact features. This reliance is captured by the condition $I(G_{art}) \\ge I(G_{bio}) + \\epsilon$, where $\\epsilon > 0$ is a small margin to ensure the difference is non-trivial.\n\nA \"Clever Hans\" effect is formally detected if and only if both conditions are met: the model shows a significant generalization gap, and its performance is demonstrably more dependent on the artifact features than the biological ones. This principled approach combines performance evaluation with a direct interrogation of the model's internal logic, providing a robust method for diagnosing spurious learning. The implementation will systematically apply this procedure to each specified test case, using the provided parameters and random seeds to ensure reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the \"Clever Hans\" detection analysis on a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (N, p_b, p_a, mu_b, mu_a, c_train, seed)\n        (4000, 50, 2, 0.3, 3.0, 0.95, 42),  # Case A\n        (4000, 50, 2, 1.5, 3.0, 0.5, 43),   # Case B\n        (4000, 50, 2, 1.5, 1.0, 0.7, 44),   # Case C\n        (3000, 10, 10, 0.0, 2.0, 1.0, 45),  # Case D\n        (800, 20, 1, 0.3, 2.5, 0.95, 46),    # Case E\n    ]\n\n    # Fixed parameters from the problem statement\n    LAMBDA = 1.0\n    EPSILON = 0.01\n    DELTA = 0.2\n    SIGMA = 1.0\n    TRAIN_FRAC = 0.6\n    C_TEST = 0.5\n\n    results = []\n    for params in test_cases:\n        result = _run_case(params, TRAIN_FRAC, C_TEST, SIGMA, LAMBDA, EPSILON, DELTA)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(N, p_b, p_a, mu_b, mu_a, c, sigma, rng):\n    \"\"\"\n    Generates synthetic data according to the specified model.\n    \"\"\"\n    # 1. Draw biological labels\n    y = rng.binomial(1, 0.5, size=N)\n\n    # 2. Draw batch assignments based on confounding parameter c\n    u = rng.random(size=N)\n    b = np.where(u < c, y, 1 - y)\n\n    # 3. Generate biological features\n    mean_bio = np.where(y[:, np.newaxis] == 1, mu_b, -mu_b)\n    X_bio = mean_bio + rng.normal(0, sigma, size=(N, p_b))\n\n    # 4. Generate artifact features\n    mean_art = np.where(b[:, np.newaxis] == 1, mu_a, -mu_a)\n    X_art = mean_art + rng.normal(0, sigma, size=(N, p_a))\n    \n    # 5. Concatenate features\n    X = np.hstack([X_bio, X_art])\n    \n    return X, y\n\ndef _cost_function(theta, X, y, lambda_reg):\n    \"\"\"\n    Computes the regularized logistic loss.\n    \"\"\"\n    m = X.shape[0]\n    X_aug = np.hstack([X, np.ones((m, 1))])\n    z = X_aug @ theta\n    \n    # Sigmoid function, clipped for numerical stability\n    h = 1 / (1 + np.exp(-z))\n    h = np.clip(h, 1e-10, 1 - 1e-10)\n\n    log_loss = -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n    reg_term = (lambda_reg / 2) * np.sum(theta[:-1]**2)\n\n    return log_loss + reg_term\n\ndef _gradient_function(theta, X, y, lambda_reg):\n    \"\"\"\n    Computes the gradient of the regularized logistic loss.\n    \"\"\"\n    m = X.shape[0]\n    X_aug = np.hstack([X, np.ones((m, 1))])\n    z = X_aug @ theta\n    h = 1 / (1 + np.exp(-z))\n    \n    error = h - y\n    grad = X_aug.T @ error\n    \n    # Add gradient of regularization term (bias is not regularized)\n    grad[:-1] += lambda_reg * theta[:-1]\n    \n    return grad\n\ndef _train_logistic_regression(X_train, y_train, lambda_reg):\n    \"\"\"\n    Trains a logistic regression model using L-BFGS optimization.\n    \"\"\"\n    d = X_train.shape[1]\n    theta_initial = np.zeros(d + 1)\n    \n    res = minimize(\n        _cost_function,\n        theta_initial,\n        args=(X_train, y_train, lambda_reg),\n        jac=_gradient_function,\n        method='L-BFGS-B'\n    )\n    return res.x\n\ndef _predict(X, theta):\n    \"\"\"\n    Makes predictions using the trained logistic regression model.\n    \"\"\"\n    m = X.shape[0]\n    X_aug = np.hstack([X, np.ones((m, 1))])\n    scores = X_aug @ theta\n    return (scores > 0).astype(int)\n\ndef _calculate_accuracy(y_true, y_pred):\n    \"\"\"\n    Calculates prediction accuracy.\n    \"\"\"\n    return np.mean(y_true == y_pred)\n\ndef _calculate_permutation_importance(X_val, y_val, theta, group_indices, rng):\n    \"\"\"\n    Calculates permutation importance for a given feature group.\n    \"\"\"\n    # Baseline accuracy\n    y_pred_val = _predict(X_val, theta)\n    acc_val = _calculate_accuracy(y_val, y_pred_val)\n\n    # Permuted accuracy\n    X_perm = X_val.copy()\n    n_val = X_val.shape[0]\n    \n    # Jointly permute the feature group columns across samples\n    perm_indices = rng.permutation(n_val)\n    X_perm[:, group_indices] = X_perm[perm_indices, :][:, group_indices]\n    \n    y_pred_perm = _predict(X_perm, theta)\n    acc_perm = _calculate_accuracy(y_val, y_pred_perm)\n    \n    return acc_val - acc_perm\n\ndef _run_case(params, train_frac, c_test, sigma, lambda_reg, epsilon, delta):\n    \"\"\"\n    Executes one full test case for \"Clever Hans\" detection.\n    \"\"\"\n    N, p_b, p_a, mu_b, mu_a, c_train, seed = params\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate confounded data and split\n    X, y = _generate_data(N, p_b, p_a, mu_b, mu_a, c_train, sigma, rng)\n    n_train = int(N * train_frac)\n    X_train, y_train = X[:n_train], y[:n_train]\n    X_val, y_val = X[n_train:], y[n_train:]\n    n_val = X_val.shape[0]\n\n    # 2. Train the model\n    theta = _train_logistic_regression(X_train, y_train, lambda_reg)\n\n    # 3. Compute validation and test accuracies\n    y_pred_val = _predict(X_val, theta)\n    acc_val = _calculate_accuracy(y_val, y_pred_val)\n    \n    X_test, y_test = _generate_data(n_val, p_b, p_a, mu_b, mu_a, c_test, sigma, rng)\n    y_pred_test = _predict(X_test, theta)\n    acc_test = _calculate_accuracy(y_test, y_pred_test)\n\n    # 4. Compute permutation feature importances\n    bio_indices = list(range(p_b))\n    art_indices = list(range(p_b, p_b + p_a))\n    \n    imp_bio = _calculate_permutation_importance(X_val, y_val, theta, bio_indices, rng)\n    imp_art = _calculate_permutation_importance(X_val, y_val, theta, art_indices, rng)\n\n    # 5. Check for \"Clever Hans\" effect\n    cond1 = imp_art >= imp_bio + epsilon\n    cond2 = acc_val - acc_test >= delta\n    \n    is_clever_hans = cond1 and cond2\n    return is_clever_hans\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2400032"}]}