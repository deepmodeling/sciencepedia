{"hands_on_practices": [{"introduction": "Before we use Graph Neural Networks ($GNNs$) as black-box learners, it's insightful to build one from the ground up for an interpretable task. This exercise demystifies message passing by demonstrating how its core operations can be used to systematically count chemical substructures, connecting modern GNNs to traditional chemoinformatics methods. By implementing a deterministic network to generate a molecular fingerprint, you will gain a concrete understanding of how local information is aggregated into a global, meaningful representation [@problem_id:2395403].", "problem": "You are given a formal specification of a Graph Neural Network (GNN) that produces an interpretable binary molecular fingerprint, where each bit corresponds to a precisely defined substructural feature. A molecule is represented as an undirected labeled graph with atoms as nodes and bonds as edges. Each node has a one-hot atom-type feature and each edge has a one-hot bond-type feature. The GNN is defined from first principles as follows, with no learned training, using only fixed, deterministic operations.\n\nAtomic feature encoding and bond feature encoding:\n- The atom type alphabet is ordered as Carbon (C), Oxygen (O), Nitrogen (N), Hydrogen (H). For a node $i$, its atom-type one-hot vector is $x_i \\in \\mathbb{R}^4$ with this order. That is, $x_i = [1,0,0,0]^\\top$ if $i$ is Carbon, $x_i = [0,1,0,0]^\\top$ if $i$ is Oxygen, $x_i = [0,0,1,0]^\\top$ if $i$ is Nitrogen, and $x_i = [0,0,0,1]^\\top$ if $i$ is Hydrogen.\n- The bond type alphabet is ordered as Single, Double. For an undirected edge $\\{i,j\\}$, define two directed edges $(i,j)$ and $(j,i)$ for computation. Each directed edge $(i,j)$ has a bond-type one-hot vector $e_{ij} \\in \\mathbb{R}^2$ with $e_{ij} = [1,0]^\\top$ for a single bond and $e_{ij} = [0,1]^\\top$ for a double bond.\n\nMessage, node representation, and graph representation:\n- For each node $i$, define its neighbor set $N(i)$ as all nodes $j$ such that $\\{i,j\\}$ is an edge. The message aggregated at node $i$ is\n$$\nm_i \\;=\\; \\sum_{j \\in N(i)} \\left( x_j \\otimes e_{ij} \\right) \\in \\mathbb{R}^8,\n$$\nwhere $\\otimes$ denotes the Kronecker product. Under the specified atom-order and bond-order, $x_j \\otimes e_{ij}$ enumerates ordered neighbor atom types crossed with bond types in blocks: for neighbor type index $b \\in \\{0,1,2,3\\}$ and bond type index $t \\in \\{0,1\\}$, the coordinate index is $b \\cdot 2 + t$.\n- For each node $i$, define its typed neighborhood tensor flattened into a vector\n$$\nH_i \\;=\\; x_i \\otimes m_i \\in \\mathbb{R}^{32}.\n$$\nUnder the specified center atom-order and neighbor-bond order, the coordinate indexing is: for center atom index $a \\in \\{0,1,2,3\\}$, neighbor atom index $b \\in \\{0,1,2,3\\}$, bond type index $t \\in \\{0,1\\}$,\n$$\n\\text{index}(a,b,t) \\;=\\; a \\cdot 8 \\;+\\; b \\cdot 2 \\;+\\; t.\n$$\n- The graph-level representation is the sum over nodes:\n$$\ns \\;=\\; \\sum_{i} H_i \\in \\mathbb{R}^{32}.\n$$\n\nInterpretable fingerprint readout:\nYou will compute a $4$-bit fingerprint $f \\in \\{0,1\\}^4$ with bits defined by simple linear thresholds on the corresponding counts in $s$:\n- Bit $0$ (presence of a C–O single bond): let $k_0 = \\text{index}(a{=}0, b{=}1, t{=}0) = 0 \\cdot 8 + 1 \\cdot 2 + 0 = 2$. Define the logit $\\ell_0 = s_{k_0} - 0.5$. Set $f_0 = 1$ if $\\ell_0 \\ge 0$, else $f_0 = 0$.\n- Bit $1$ (presence of a C=O double bond): let $k_1 = \\text{index}(a{=}0, b{=}1, t{=}1) = 0 \\cdot 8 + 1 \\cdot 2 + 1 = 3$. Define the logit $\\ell_1 = s_{k_1} - 0.5$. Set $f_1 = 1$ if $\\ell_1 \\ge 0$, else $f_1 = 0$.\n- Bit $2$ (at least two N–H single bonds anywhere in the molecule): let $k_2 = \\text{index}(a{=}2, b{=}3, t{=}0) = 2 \\cdot 8 + 3 \\cdot 2 + 0 = 22$. Define the logit $\\ell_2 = s_{k_2} - 1.5$. Set $f_2 = 1$ if $\\ell_2 \\ge 0$, else $f_2 = 0$.\n- Bit $3$ (presence of a C–C single bond): let $k_3 = \\text{index}(a{=}0, b{=}0, t{=}0) = 0 \\cdot 8 + 0 \\cdot 2 + 0 = 0$. Define the logit $\\ell_3 = s_{k_3} - 0.5$. Set $f_3 = 1$ if $\\ell_3 \\ge 0$, else $f_3 = 0$.\n\nNote that each bit corresponds to a clearly defined directed edge-type count aggregated over the graph, thus providing interpretability: bit $0$ counts C-centered neighbors that are O via a single bond, bit $1$ counts C-centered neighbors that are O via a double bond, bit $2$ counts N-centered neighbors that are H via single bonds, and bit $3$ counts C-centered neighbors that are C via a single bond.\n\nInput data for the test suite:\nFor each test case, you are given:\n- A list of atom types in order (each from the set {C, O, N, H}), indexed from $0$ to $n-1$ where $n$ is the number of atoms.\n- A list of undirected bonds as triples $(i,j,\\text{type})$ with $i$ and $j$ integer node indices and $\\text{type} \\in \\{\\text{single}, \\text{double}\\}$. Each undirected bond $\\{i,j\\}$ should be treated as two directed bonds $(i,j)$ and $(j,i)$ with the same type in the GNN computation above.\n\nTest suite molecules:\n- Test case $1$ (Ethanol; $\\mathrm{C_2H_6O}$):\n  - Atoms (indices $0$ to $8$): [C, C, O, H, H, H, H, H, H]\n  - Bonds: $(0,1,\\text{single})$, $(1,2,\\text{single})$, $(0,3,\\text{single})$, $(0,4,\\text{single})$, $(0,5,\\text{single})$, $(1,6,\\text{single})$, $(1,7,\\text{single})$, $(2,8,\\text{single})$.\n- Test case $2$ (Formaldehyde; $\\mathrm{CH_2O}$):\n  - Atoms (indices $0$ to $3$): [C, O, H, H]\n  - Bonds: $(0,1,\\text{double})$, $(0,2,\\text{single})$, $(0,3,\\text{single})$.\n- Test case $3$ (Ammonia; $\\mathrm{NH_3}$):\n  - Atoms (indices $0$ to $3$): [N, H, H, H]\n  - Bonds: $(0,1,\\text{single})$, $(0,2,\\text{single})$, $(0,3,\\text{single})$.\n- Test case $4$ (Ethane; $\\mathrm{C_2H_6}$):\n  - Atoms (indices $0$ to $7$): [C, C, H, H, H, H, H, H]\n  - Bonds: $(0,1,\\text{single})$, $(0,2,\\text{single})$, $(0,3,\\text{single})$, $(0,4,\\text{single})$, $(1,5,\\text{single})$, $(1,6,\\text{single})$, $(1,7,\\text{single})$.\n- Test case $5$ (Water; $\\mathrm{H_2O}$):\n  - Atoms (indices $0$ to $2$): [O, H, H]\n  - Bonds: $(0,1,\\text{single})$, $(0,2,\\text{single})$.\n- Test case $6$ (Acetone; $\\mathrm{C_3H_6O}$ heavy-atom skeleton $\\mathrm{O{=}C{-}C}$ with methyl groups):\n  - Atoms (indices $0$ to $9$): [O, C, C, C, H, H, H, H, H, H]\n  - Bonds: $(1,0,\\text{double})$, $(1,2,\\text{single})$, $(1,3,\\text{single})$, $(2,4,\\text{single})$, $(2,5,\\text{single})$, $(2,6,\\text{single})$, $(3,7,\\text{single})$, $(3,8,\\text{single})$, $(3,9,\\text{single})$.\n\nRequired program behavior and output format:\n- Implement the exact GNN computations above using only the given atom and bond encodings, the Kronecker product, and summations.\n- For each test case in the order listed, compute the fingerprint $f \\in \\{0,1\\}^4$.\n- Your program should produce a single line of output containing the results as a comma-separated list of the six fingerprints, each fingerprint itself being a comma-separated list of four integers, all enclosed in square brackets. For example: \"[[a,b,c,d],[e,f,g,h],...]\" where each letter is either $0$ or $1$.", "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, and objective, providing a complete and consistent specification for a deterministic graph neural network (GNN) algorithm. It is free of factual errors, contradictions, or ambiguities. We shall therefore proceed to formulate the solution.\n\nThe task is to implement a GNN that computes a 4-bit molecular fingerprint. This GNN is not a typical machine learning model that requires training; rather, it is a fixed, deterministic feature extractor defined from first principles. The entire procedure, from atomic feature encoding to the final fingerprint, is based on a sequence of precisely defined mathematical operations.\n\nThe foundation of this GNN lies in its hierarchical feature representation, constructed using the Kronecker product ($\\otimes$). The design of the feature vectors and operations ensures that the final graph-level representation, $s \\in \\mathbb{R}^{32}$, has a clear, physical interpretation. Each component of $s$ corresponds to a count of a specific type of directed, typed edge within the molecular graph.\n\nLet us dissect the construction step by step.\n\n1.  **Atomic and Bond-level Encoding**:\n    The initial features are one-hot vectors representing discrete categories. For an atom $i$, its feature $x_i \\in \\mathbb{R}^4$ identifies its type from the set $\\{\\text{C, O, N, H}\\}$. For a directed edge $(i, j)$, its feature $e_{ij} \\in \\mathbb{R}^2$ identifies the bond type from $\\{\\text{Single, Double}\\}$. This discrete encoding is the basis for all subsequent operations.\n\n2.  **Message Construction**:\n    For each node $i$, a message vector $m_i \\in \\mathbb{R}^8$ is aggregated from its neighbors $j \\in N(i)$. The message is defined as:\n    $$\n    m_i \\;=\\; \\sum_{j \\in N(i)} \\left( x_j \\otimes e_{ij} \\right)\n    $$\n    The Kronecker product $x_j \\otimes e_{ij}$ produces an $8$-dimensional vector. Because $x_j$ and $e_{ij}$ are one-hot vectors, $x_j \\otimes e_{ij}$ is also a one-hot vector. Its single non-zero entry uniquely identifies the combination of the neighbor's atom type (from $4$ choices) and the connecting bond's type (from $2$ choices), for a total of $4 \\times 2 = 8$ possibilities. The summation over all neighbors $j \\in N(i)$ thus makes $m_i$ a histogram vector. Each component $(m_i)_k$ counts the number of neighbors of node $i$ that correspond to the $k$-th combination of atom type and bond type. For example, if the neighbor atom types are indexed by $b \\in \\{0,1,2,3\\}$ and bond types by $t \\in \\{0,1\\}$, the component of $m_i$ at index $k = b \\cdot 2 + t$ is the number of neighbors of type $b$ connected to $i$ via a bond of type $t$.\n\n3.  **Node Representation**:\n    The representation for each node $i$, $H_i \\in \\mathbb{R}^{32}$, is computed by combining its own feature vector $x_i$ with its aggregated message $m_i$:\n    $$\n    H_i \\;=\\; x_i \\otimes m_i\n    $$\n    This operation embeds the local neighborhood information ($m_i$) into a larger feature space, partitioned by the type of the central atom $i$. The resulting $32$-dimensional space arises from combining the $4$ possible types for the central atom $i$ with the $8$-dimensional neighborhood histogram. A component of $H_i$ at index $k = a \\cdot 8 + b \\cdot 2 + t$ (where $a$ is the type index of atom $i$) will be non-zero only if atom $i$ is of type $a$. If so, its value will be the count of neighbors of type $b$ connected via a bond of type $t$.\n\n4.  **Graph-Level Representation**:\n    The final graph-level representation $s \\in \\mathbb{R}^{32}$ is the sum of all node representations:\n    $$\n    s \\;=\\; \\sum_{i} H_i\n    $$\n    Due to the structure of $H_i$, this summation has a very specific meaning. Let us consider the component $s_k$ where $k = a \\cdot 8 + b \\cdot 2 + t$.\n    $$\n    s_k = \\left( \\sum_{i} H_i \\right)_k = \\sum_{i} (H_i)_k = \\sum_{\\substack{i \\text{ s.t.} \\\\ \\text{type}(i)=a}} (m_i)_{b \\cdot 2 + t}\n    $$\n    As established, $(m_i)_{b \\cdot 2 + t}$ is the number of neighbors of $i$ with type $b$ connected by a bond of type $t$. Summing this value over all nodes $i$ of type $a$ gives the total count of directed edges in the graph that start at an atom of type $a$, end at an atom of type $b$, and have a bond of type $t$. This direct correspondence between the components of $s$ and substructural counts is the key to the model's interpretability.\n\n5.  **Interpretable Fingerprint Readout**:\n    The $4$-bit fingerprint $f$ is derived by applying simple thresholding rules to specific components of $s$. Each bit tests for the presence of a specific molecular feature.\n    - **Bit 0 (C–O single bond)**: The relevant substructure is a directed edge from Carbon ($a=0$) to Oxygen ($b=1$) via a single bond ($t=0$). The corresponding index is $k_0 = 0 \\cdot 8 + 1 \\cdot 2 + 0 = 2$. The value $s_2$ is the total count of such `C->O` single bonds. The condition is $s_2 - 0.5 \\ge 0$, which for an integer count $s_2$ is equivalent to $s_2 \\ge 1$. This correctly checks for the presence of at least one C–O single bond.\n    - **Bit 1 (C=O double bond)**: Corresponds to index $k_1 = \\text{index}(a=0, b=1, t=1) = 3$. The condition $s_3 - 0.5 \\ge 0$ is equivalent to $s_3 \\ge 1$, checking for the presence of at least one `C->O` double bond.\n    - **Bit 2 (at least two N–H single bonds)**: Corresponds to index $k_2 = \\text{index}(a=2, b=3, t=0) = 22$. The value $s_{22}$ counts all `N->H` single bonds. The condition is $s_{22} - 1.5 \\ge 0$, equivalent to $s_{22} \\ge 2$, which correctly checks for the presence of at least two such bonds.\n    - **Bit 3 (C–C single bond)**: Corresponds to index $k_3 = \\text{index}(a=0, b=0, t=0) = 0$. The condition $s_0 - 0.5 \\ge 0$ is equivalent to $s_0 \\ge 1$, checking for the presence of at least one `C->C` single bond.\n\nThe algorithm is thus a precise, interpretable counting mechanism implemented within a GNN framework. The implementation will follow these steps directly to compute the fingerprints for the given test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified GNN to compute molecular fingerprints.\n    \"\"\"\n    atom_map = {'C': 0, 'O': 1, 'N': 2, 'H': 3}\n    bond_map = {'single': 0, 'double': 1}\n    num_atom_types = 4\n    num_bond_types = 2\n\n    # Define the atom and bond one-hot basis vectors.\n    atom_basis = np.identity(num_atom_types)\n    bond_basis = np.identity(num_bond_types)\n    \n    test_cases = [\n        # Test case 1 (Ethanol)\n        {\n            \"atoms\": ['C', 'C', 'O', 'H', 'H', 'H', 'H', 'H', 'H'],\n            \"bonds\": [(0, 1, 'single'), (1, 2, 'single'), (0, 3, 'single'), (0, 4, 'single'), (0, 5, 'single'), (1, 6, 'single'), (1, 7, 'single'), (2, 8, 'single')]\n        },\n        # Test case 2 (Formaldehyde)\n        {\n            \"atoms\": ['C', 'O', 'H', 'H'],\n            \"bonds\": [(0, 1, 'double'), (0, 2, 'single'), (0, 3, 'single')]\n        },\n        # Test case 3 (Ammonia)\n        {\n            \"atoms\": ['N', 'H', 'H', 'H'],\n            \"bonds\": [(0, 1, 'single'), (0, 2, 'single'), (0, 3, 'single')]\n        },\n        # Test case 4 (Ethane)\n        {\n            \"atoms\": ['C', 'C', 'H', 'H', 'H', 'H', 'H', 'H'],\n            \"bonds\": [(0, 1, 'single'), (0, 2, 'single'), (0, 3, 'single'), (0, 4, 'single'), (1, 5, 'single'), (1, 6, 'single'), (1, 7, 'single')]\n        },\n        # Test case 5 (Water)\n        {\n            \"atoms\": ['O', 'H', 'H'],\n            \"bonds\": [(0, 1, 'single'), (0, 2, 'single')]\n        },\n        # Test case 6 (Acetone)\n        {\n            \"atoms\": ['O', 'C', 'C', 'C', 'H', 'H', 'H', 'H', 'H', 'H'],\n            \"bonds\": [(1, 0, 'double'), (1, 2, 'single'), (1, 3, 'single'), (2, 4, 'single'), (2, 5, 'single'), (2, 6, 'single'), (3, 7, 'single'), (3, 8, 'single'), (3, 9, 'single')]\n        }\n    ]\n\n    all_fingerprints = []\n\n    for case in test_cases:\n        atoms = case[\"atoms\"]\n        bonds = case[\"bonds\"]\n        num_atoms = len(atoms)\n\n        # 1. Build atom feature vectors and adjacency list\n        atom_features = [atom_basis[atom_map[atom_name]] for atom_name in atoms]\n        \n        adj = [[] for _ in range(num_atoms)]\n        for i, j, bond_type_name in bonds:\n            bond_type_idx = bond_map[bond_type_name]\n            adj[i].append((j, bond_type_idx))\n            adj[j].append((i, bond_type_idx))\n            \n        # 2. Compute GNN representations\n        graph_s = np.zeros(32)\n\n        for i in range(num_atoms):\n            # Message aggregation\n            m_i = np.zeros(8)\n            for j, bond_type_idx in adj[i]:\n                x_j = atom_features[j]\n                e_ij = bond_basis[bond_type_idx]\n                m_i += np.kron(x_j, e_ij)\n            \n            # Node representation\n            x_i = atom_features[i]\n            H_i = np.kron(x_i, m_i)\n            \n            # Graph-level aggregation\n            graph_s += H_i\n\n        # 3. Compute fingerprint\n        fingerprint = [0, 0, 0, 0]\n        \n        # Bit 0 (C-O single): k0 = index(a=0, b=1, t=0) = 2\n        k0 = 2\n        l0 = graph_s[k0] - 0.5\n        if l0 >= 0:\n            fingerprint[0] = 1\n\n        # Bit 1 (C=O double): k1 = index(a=0, b=1, t=1) = 3\n        k1 = 3\n        l1 = graph_s[k1] - 0.5\n        if l1 >= 0:\n            fingerprint[1] = 1\n\n        # Bit 2 (>=2 N-H single): k2 = index(a=2, b=3, t=0) = 22\n        k2 = 22\n        l2 = graph_s[k2] - 1.5\n        if l2 >= 0:\n            fingerprint[2] = 1\n\n        # Bit 3 (C-C single): k3 = index(a=0, b=0, t=0) = 0\n        k3 = 0\n        l3 = graph_s[k3] - 0.5\n        if l3 >= 0:\n            fingerprint[3] = 1\n            \n        all_fingerprints.append(fingerprint)\n\n    # Format output as required\n    result_str = \",\".join([f\"[{','.join(map(str, fp))}]\" for fp in all_fingerprints])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "2395403"}, {"introduction": "A powerful model is of little use without informative data. This conceptual problem challenges you to think critically about the GNN's input: the molecular graph representation itself. We will explore what happens to a model's predictive power when critical chemical details, like bond types, are omitted from the edge features [@problem_id:2395408]. This practice highlights a fundamental principle: a GNN's ability to learn is fundamentally bounded by the richness of the features it is given, reinforcing the importance of thoughtful feature engineering.", "problem": "You are training a Message Passing Neural Network (MPNN) for molecular property prediction in computational biology and bioinformatics. Each molecule is represented as an undirected molecular graph $G=(V,E)$ of heavy atoms (hydrogen atoms are not explicitly included), where each node $v\\in V$ carries an initial feature vector $x_v$ containing atomic number and formal charge, and each edge $\\{u,v\\}\\in E$ carries an edge feature $e_{uv}$ encoding the chemical bond type (single, double, triple, aromatic). The MPNN performs $L$ rounds of message passing with permutation-invariant aggregation and then applies a permutation-invariant readout to obtain a prediction $\\hat{y}$ for a target molecular property. All training conditions (architecture depth $L$, optimization, dataset size, and train/validation/test splits) are held fixed. The target property depends on electronic structure features that are known to be influenced by bond order and aromaticity (for example, ultraviolet-visible (UV-Vis) absorbance peak position or conjugation-dependent reactivity).\n\nYou consider modifying the molecular representation by replacing the multi-class edge feature $e_{uv}$ with a single binary indicator that encodes only whether two atoms are connected by a bond ($1$) or not ($0$), leaving all other aspects of the pipeline unchanged.\n\nWhich statement best characterizes the expected impact of this change on the predictive power of the MPNN?\n\nA. With sufficiently large depth $L$, predictive power will be unchanged because deep message passing can reconstruct all necessary chemistry from connectivity alone, making explicit bond types redundant.\n\nB. Predictive power will generally decrease for properties that depend on bond order, resonance, or aromaticity, because the model loses informative edge-label signals that cannot be recovered from binary connectivity.\n\nC. Predictive power will generally increase on typical small datasets because the reduced input dimensionality acts as an implicit regularizer that outweighs any loss of chemical detail.\n\nD. Predictive power will be unchanged if one increases $L$ until every node has a receptive field covering the entire graph, since a global view compensates for missing edge labels.\n\nE. Predictive power will be unchanged when node features include atomic number and formal charge, because bond order can be inferred exactly from valence rules using only binary connectivity and node features.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: Message Passing Neural Network (MPNN).\n- **Task**: Molecular property prediction.\n- **Input**: Undirected molecular graph $G=(V,E)$ representing heavy atoms.\n- **Node Features**: Initial feature vector $x_v$ for each node $v \\in V$, containing atomic number and formal charge.\n- **Edge Features (Original)**: Feature vector $e_{uv}$ for each edge $\\{u,v\\} \\in E$, encoding chemical bond type (single, double, triple, aromatic).\n- **MPNN Process**: $L$ rounds of message passing with permutation-invariant aggregation, followed by a permutation-invariant readout to produce a prediction $\\hat{y}$.\n- **Target Property**: A molecular property that depends on electronic structure, influenced by bond order and aromaticity (e.g., Ultraviolet-Visible (UV-Vis) absorbance).\n- **Fixed Conditions**: Architecture depth $L$, optimization method, dataset size, and data splits are all held fixed.\n- **Modification**: The multi-class edge feature $e_{uv}$ is replaced with a single binary indicator: $1$ if a bond exists, $0$ otherwise.\n- **Question**: Characterize the expected impact of this modification on the MPNN's predictive power.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It describes a standard methodology in chemoinformatics using Graph Neural Networks. The concepts of MPNNs, molecular graphs, atomic features, bond features, and their relation to electronic properties like UV-Vis spectra are well-established in computational chemistry and biology.\n2.  **Non-Formalizable or Irrelevant**: The problem is formal and directly relevant to the application of graph neural networks for molecular data analysis.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent. It clearly defines the initial state, the modification, the constraints (fixed conditions), and the property of interest.\n4.  **Unrealistic or Infeasible**: The proposed modification is a common type of ablation study performed in machine learning research to understand feature importance. It is entirely realistic.\n5.  **Ill-Posed or Poorly Structured**: The question is well-posed, asking for the qualitative impact on model performance, which can be deduced from first principles of GNNs and chemistry.\n6.  **Outside Scientific Verifiability**: The claims can be evaluated based on the established theory of GNN expressivity and fundamental chemical principles.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A solution will be derived.\n\n### Derivation\nThe core of the problem lies in understanding the information content of the molecular graph representation and how its reduction affects the learning capacity of the Message Passing Neural Network (MPNN). The original representation includes rich edge features $e_{uv}$ that specify bond types: single, double, triple, or aromatic. The modified representation discards this information, retaining only the graph's adjacency matrix (connectivity).\n\nThe target property is explicitly stated to depend on electronic structure, bond order, and aromaticity. These chemical characteristics are directly related to the distribution of electrons in a molecule, particularly $\\pi$-electrons in conjugated systems.\n- **Bond order** (e.g., single vs. double) determines bond length, bond strength, and the localization of electrons.\n- **Aromaticity** is a special property of cyclic, planar, conjugated systems that leads to exceptional thermodynamic stability and unique electronic behavior (e.g., specific reactivity, distinct spectroscopic signals).\n\nAn MPNN operates by iteratively updating node representations $h_v^{(k)}$ at layer $k$ based on the representations of their neighbors at the previous layer $k-1$:\n$$h_v^{(k)} = U^{(k)} \\left( h_v^{(k-1)}, \\bigoplus_{u \\in N(v)} M^{(k)}(h_v^{(k-1)}, h_u^{(k-1)}, e_{vu}) \\right)$$\nwhere $M^{(k)}$ is a message function, $\\bigoplus$ is a permutation-invariant aggregation function (e.g., sum, mean), and $U^{(k)}$ is an update function. The initial representations are $h_v^{(0)} = x_v$.\n\nThe crucial point is that all information available to the network is contained in the initial node features $x_v$ and edge features $e_{uv}$. The message passing mechanism can only propagate and transform this initial information across the graph structure. It cannot create new information that is not implicitly or explicitly present in the input.\n\nBy replacing the detailed bond type information in $e_{uv}$ with a binary indicator, we are performing a significant information bottleneck. The question becomes whether this lost information can be recovered from the remaining data: the graph topology, atomic numbers, and formal charges.\n\nConsider two distinct molecules: benzene ($C_6H_6$) and cyclohexane ($C_6H_{12}$). In a heavy-atom graph representation, both are represented as a cycle of $6$ carbon atoms.\n- **Node features**: For both, all $6$ nodes are carbon atoms (same atomic number). Assuming neutral molecules, the formal charges are all $0$. So, the initial node feature vectors $x_v$ are identical for all nodes in both graphs.\n- **Edge features (original)**: For benzene, the edges would be flagged as 'aromatic'. For cyclohexane, they would be 'single'.\n- **Edge features (modified)**: For both molecules, all edges in the $6$-membered ring are now represented by the value $1$.\n\nWith the modified representation, the input graphs for benzene and cyclohexane become completely indistinguishable to the MPNN. They have identical node features, identical edge features (all $1$), and an identical isomorphic graph structure (a 6-cycle). Since the MPNN architecture is a deterministic function of its input graph, it will produce the exact same final graph embedding and, consequently, the exact same property prediction $\\hat{y}$ for both molecules.\n\nHowever, their actual properties are vastly different. Benzene is aromatic, planar, and strongly absorbs UV light due to its delocalized $\\pi$-electron system. Cyclohexane is non-aromatic, non-planar, and is transparent in the same UV region. An MPNN trained on the modified data cannot possibly learn to distinguish between them and will suffer a large error for at least one of them. This is not an isolated example; many isomers and molecules with resonance structures (e.g., carboxylate ions, amides) rely on bond order and delocalization for their properties, and this information is lost without explicit bond-type features.\n\nTherefore, removing the edge features that encode bond type fundamentally limits the expressive power of the model for tasks where such information is critical. The model's performance on properties sensitive to electronic conjugation and aromaticity is expected to decrease substantially.\n\n### Option-by-Option Analysis\n**A. With sufficiently large depth $L$, predictive power will be unchanged because deep message passing can reconstruct all necessary chemistry from connectivity alone, making explicit bond types redundant.**\nThis statement is incorrect. As demonstrated with the benzene/cyclohexane example, if two chemically distinct molecules are represented by isomorphic graphs with identical initial node and edge features, no amount of message passing ($L \\to \\infty$) can make them distinguishable. Message passing propagates information over the existing graph structure; it does not \"create\" information or solve the graph isomorphism problem for cases where initial feature vectors are identical. The necessary chemistry is not contained in connectivity alone.\n\n**B. Predictive power will generally decrease for properties that depend on bond order, resonance, or aromaticity, because the model loses informative edge-label signals that cannot be recovered from binary connectivity.**\nThis statement is correct. It accurately identifies that bond type is a highly informative feature for properties related to electronic structure. It correctly states that this information cannot be fully recovered from the graph's topology and node features alone. The loss of this critical signal cripples the model's ability to distinguish between molecules with fundamentally different electronic properties (e.g., aromatic vs. aliphatic), leading to a decrease in predictive power.\n\n**C. Predictive power will generally increase on typical small datasets because the reduced input dimensionality acts as an implicit regularizer that outweighs any loss of chemical detail.**\nThis statement is incorrect. While reducing feature dimensionality can have a regularizing effect, this is beneficial only when the removed features are noisy or have low predictive value. Here, the bond-type features are of paramount importance for the specified target property. Removing such a critical signal introduces a strong systematic bias, making the model fundamentally incapable of capturing the underlying chemical principles. The resulting underfitting (high bias) will dominate any potential benefit from reduced overfitting (lower variance). The loss of chemical detail is catastrophic.\n\n**D. Predictive power will be unchanged if one increases $L$ until every node has a receptive field covering the entire graph, since a global view compensates for missing edge labels.**\nThis statement is incorrect. This is a variant of argument A. A global receptive field means that the final representation of each node depends on all other nodes and edges in the graph. However, if the initial input graphs for two different molecules are identical (as in the benzene/cyclohexane case with modified features), the global aggregations will also be identical. A global view cannot compensate for a lack of information at the fundamental feature level.\n\n**E. Predictive power will be unchanged when node features include atomic number and formal charge, because bond order can be inferred exactly from valence rules using only binary connectivity and node features.**\nThis statement is incorrect. The claim that bond order can be \"inferred exactly\" from valence rules, connectivity, and atomic features is false. Chemistry is replete with examples of isomerism and resonance where a single connectivity graph corresponds to multiple valid and distinct electronic structures. While valence rules provide constraints, they do not lead to a unique solution for bond orders in many complex organic molecules, especially those involving conjugated systems, aromatic rings, or heteroatoms. For example, for a pyridine ring ($C_5H_5N$), the connectivity and atom types do not uniquely determine the electronic structure without ambiguity. The model cannot \"infer\" a unique truth that is not uniquely determined by the input.", "answer": "$$\\boxed{B}$$", "id": "2395408"}, {"introduction": "Now let's bridge theory and application by tackling a practical design challenge from drug discovery. Your task is to develop a GNN-based procedure to score a molecule's \"drug-likeness\" according to the well-known Lipinski's rule of five. This problem requires you to design a network that can perfectly recover properties that are simple sums of atomic contributions, such as molecular weight [@problem_id:2395422]. In doing so, you will discover how a problem's specific constraints can lead to elegant simplifications in the GNN architecture, revealing a deeper insight into the relationship between model design and the nature of the target property.", "problem": "You are asked to formalize a principled, permutation-invariant Graph Neural Network (GNN) procedure to score the \"drug-likeness\" of a molecule based on Lipinski's rule of five, using only first-principles definitions and widely accepted facts. You must produce a complete program that implements your design and evaluates it on the provided test suite.\n\nStart from the following fundamental base:\n- A molecule can be modeled as an undirected graph with adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ and node features $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of atoms (nodes) and $d$ is the feature dimension.\n- A Graph Neural Network (GNN) with message passing uses permutation-invariant aggregation to obtain a graph-level representation. Sum aggregation is invariant to node ordering.\n- Lipinski's rule of five states that oral drug-like molecules typically satisfy four constraints: molecular weight $\\leq 500$ (in $\\mathrm{g/mol}$), octanol-water partition coefficient $\\log P \\leq 5$ (unitless), hydrogen bond donors $\\leq 5$ (dimensionless count), and hydrogen bond acceptors $\\leq 10$ (dimensionless count). These are empirical heuristics and provide a plausible target for a soft satisfaction score.\n\nYour task:\n1. Model each molecule as a graph with adjacency matrix $A$ and node features $X$ where each node feature vector has four components in order: \n   - per-atom contribution to molecular weight (in $\\mathrm{g/mol}$),\n   - per-atom contribution to $\\log P$ (unitless, following a simplified fragment-additive approximation),\n   - per-atom hydrogen bond donor indicator ($0$ or $1$),\n   - per-atom hydrogen bond acceptor indicator ($0$ or $1$).\n   The four global molecular properties are the sum over nodes of the corresponding components.\n2. Devise a one-layer message passing Graph Neural Network (GNN) that is permutation-invariant and produces a graph-level embedding via sum pooling. Use linear message passing updates and sum pooling so that additive, graph-level properties can be exactly recovered from node-level contributions by a suitable linear readout.\n3. From the predicted properties, define a differentiable soft satisfaction score $s \\in [0,1]$ using a hinge-style penalty normalized by each rule's threshold. Let $\\phi(z) = \\max(0,z)$ denote the rectified linear unit. Define the four normalized violations\n   $$v_{\\mathrm{MW}} = \\phi\\left(\\frac{\\mathrm{MW} - 500}{500}\\right), \\quad v_{\\log P} = \\phi\\left(\\frac{\\log P - 5}{5}\\right), \\quad v_{\\mathrm{HBD}} = \\phi\\left(\\frac{\\mathrm{HBD} - 5}{5}\\right), \\quad v_{\\mathrm{HBA}} = \\phi\\left(\\frac{\\mathrm{HBA} - 10}{10}\\right).$$\n   Then define\n   $$s = \\mathrm{clip}\\left(1 - \\frac{v_{\\mathrm{MW}} + v_{\\log P} + v_{\\mathrm{HBD}} + v_{\\mathrm{HBA}}}{4}, \\, 0, \\, 1\\right),$$\n   where $\\mathrm{MW}$ is in $\\mathrm{g/mol}$, $\\log P$ is unitless, and $\\mathrm{HBD}$ and $\\mathrm{HBA}$ are dimensionless counts. The clip is elementwise clipping into $[0,1]$.\n4. Implement this procedure as a complete, runnable program and evaluate it on the test suite below. The final program must print the scores for the test cases rounded to three decimal places.\n\nTest suite:\nRepresent each molecule by $(A, X)$ where $A$ is the adjacency matrix and $X$ is the node feature matrix with columns ordered as described above.\n\n- Case $1$ (typical drug-like, all constraints well within limits):\n  $$A_1 = \\begin{bmatrix}\n  0 & 1 & 0 & 0 & 0 \\\\\n  1 & 0 & 1 & 0 & 0 \\\\\n  0 & 1 & 0 & 1 & 0 \\\\\n  0 & 0 & 1 & 0 & 1 \\\\\n  0 & 0 & 0 & 1 & 0\n  \\end{bmatrix}, \\quad\n  X_1 = \\begin{bmatrix}\n  60 & 0.4 & 0 & 1 \\\\\n  80 & 0.6 & 0 & 2 \\\\\n  100 & 0.8 & 1 & 1 \\\\\n  55 & 0.5 & 0 & 1 \\\\\n  55 & 0.5 & 0 & 1\n  \\end{bmatrix}.$$\n\n- Case $2$ (boundary case: exactly at all thresholds):\n  $$A_2 = \\begin{bmatrix}\n  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n  1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\\\\n  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\n  \\end{bmatrix}, \\quad\n  X_2 = \\begin{bmatrix}\n  50 & 0.5 & 1 & 1 \\\\\n  50 & 0.5 & 1 & 1 \\\\\n  50 & 0.5 & 1 & 1 \\\\\n  50 & 0.5 & 1 & 1 \\\\\n  50 & 0.5 & 1 & 1 \\\\\n  50 & 0.5 & 0 & 1 \\\\\n  50 & 0.5 & 0 & 1 \\\\\n  50 & 0.5 & 0 & 1 \\\\\n  50 & 0.5 & 0 & 1 \\\\\n  50 & 0.5 & 0 & 1\n  \\end{bmatrix}.$$\n\n- Case $3$ (violates two rules moderately: molecular weight and $\\log P$):\n  $$A_3 = \\begin{bmatrix}\n  0 & 1 & 0 & 0 & 0 & 0 \\\\\n  1 & 0 & 1 & 0 & 1 & 0 \\\\\n  0 & 1 & 0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 & 1 & 0 \\\\\n  0 & 1 & 0 & 1 & 0 & 1 \\\\\n  0 & 0 & 0 & 0 & 1 & 0\n  \\end{bmatrix}, \\quad\n  X_3 = \\begin{bmatrix}\n  100 & 0.9 & 0 & 1 \\\\\n  120 & 1.2 & 0 & 2 \\\\\n  130 & 1.4 & 1 & 2 \\\\\n  90 & 1.0 & 0 & 2 \\\\\n  80 & 0.8 & 1 & 1 \\\\\n  100 & 1.1 & 0 & 1\n  \\end{bmatrix}.$$\n\n- Case $4$ (very small, water-like molecule, trivially satisfying the rules):\n  $$A_4 = \\begin{bmatrix} 0 \\end{bmatrix}, \\quad\n  X_4 = \\begin{bmatrix} 18 & -0.4 & 2 & 1 \\end{bmatrix}.$$\n\nYour program should implement the GNN-based computation as specified, evaluate the soft drug-likeness score $s$ for each $(A, X)$, and produce a single line of output containing the scores as a comma-separated list enclosed in square brackets, in the order $[ \\text{Case }1, \\text{Case }2, \\text{Case }3, \\text{Case }4 ]$, with each score rounded to three decimal places (unitless), for example, $[0.123,0.456,0.789,1.000]$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Molecular Model**: An undirected graph with adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ and node features $X \\in \\mathbb{R}^{n \\times d}$.\n- **Node Features**: For each node (atom), the feature vector has $d=4$ components: (1) per-atom molecular weight contribution ($\\mathrm{g/mol}$), (2) per-atom $\\log P$ contribution (unitless), (3) per-atom hydrogen bond donor (HBD) indicator ($0$ or $1$), and (4) per-atom hydrogen bond acceptor (HBA) indicator ($0$ or $1$).\n- **Global Properties**: The four global molecular properties ($\\mathrm{MW}$, $\\log P$, $\\mathrm{HBD}$, $\\mathrm{HBA}$) are the sum over all nodes of the corresponding feature components.\n- **Lipinski's Rule of Five Thresholds**: $\\mathrm{MW} \\leq 500$, $\\log P \\leq 5$, $\\mathrm{HBD} \\leq 5$, $\\mathrm{HBA} \\leq 10$.\n- **GNN Architecture**: A one-layer message passing GNN that is permutation-invariant, uses sum pooling, and allows for exact recovery of the additive global properties via a linear readout.\n- **Soft Score Definition**:\n    - Rectified linear unit: $\\phi(z) = \\max(0,z)$.\n    - Normalized violations:\n        - $v_{\\mathrm{MW}} = \\phi\\left(\\frac{\\mathrm{MW} - 500}{500}\\right)$\n        - $v_{\\log P} = \\phi\\left(\\frac{\\log P - 5}{5}\\right)$\n        - $v_{\\mathrm{HBD}} = \\phi\\left(\\frac{\\mathrm{HBD} - 5}{5}\\right)$\n        - $v_{\\mathrm{HBA}} = \\phi\\left(\\frac{\\mathrm{HBA} - 10}{10}\\right)$\n    - Final score: $s = \\mathrm{clip}\\left(1 - \\frac{v_{\\mathrm{MW}} + v_{\\log P} + v_{\\mathrm{HBD}} + v_{\\mathrm{HBA}}}{4}, \\, 0, \\, 1\\right)$.\n- **Test Suite**: Four test cases specified by their adjacency matrices $A_k$ and node feature matrices $X_k$ for $k \\in \\{1, 2, 3, 4\\}$.\n- **Output Format**: A single line with a comma-separated list of scores for the four cases, enclosed in square brackets, with each score rounded to three decimal places.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is based on standard principles of graph neural networks and established heuristics in cheminformatics (Lipinski's rule). All definitions, data, and constraints are provided, making the problem self-contained and free of contradictions. The setup is formal and unambiguous.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be provided.\n\n**Principled Solution**\n\nThe task is to devise a GNN-based procedure to compute a drug-likeness score. Let us proceed step-by-step as outlined in the problem.\n\n**1. Molecular Property Calculation**\nA molecule is represented by a graph with $n$ nodes and node features $X \\in \\mathbb{R}^{n \\times 4}$. The $i$-th row of $X$, denoted $X_i$, is the feature vector for the $i$-th atom: $X_i = [x_{i, \\mathrm{MW}}, x_{i, \\log P}, x_{i, \\mathrm{HBD}}, x_{i, \\mathrm{HBA}}]$. The problem states that the global molecular properties are additive. This means they are computed by summing the respective features over all atoms:\n$$ \\mathrm{MW} = \\sum_{i=1}^{n} x_{i, \\mathrm{MW}}, \\quad \\log P = \\sum_{i=1}^{n} x_{i, \\log P}, \\quad \\mathrm{HBD} = \\sum_{i=1}^{n} x_{i, \\mathrm{HBD}}, \\quad \\mathrm{HBA} = \\sum_{i=1}^{n} x_{i, \\mathrm{HBA}} $$\nLet $\\mathbf{p} \\in \\mathbb{R}^4$ be the vector of these four properties. This vector can be computed directly from the input feature matrix $X$ by summing its columns:\n$$ \\mathbf{p} = \\left[ \\sum_{i=1}^{n} X_{i,1}, \\sum_{i=1}^{n} X_{i,2}, \\sum_{i=1}^{n} X_{i,3}, \\sum_{i=1}^{n} X_{i,4} \\right]^T = \\sum_{i=1}^n X_i^T = X^T \\mathbf{1} $$\nwhere $\\mathbf{1}$ is a column vector of $n$ ones.\n\n**2. GNN Design**\nWe must design a one-layer message passing GNN that produces a graph-level embedding $h_G$ from which the property vector $\\mathbf{p}$ can be exactly recovered by a linear readout. Let $H^{(0)} = X$ be the initial node features. A single, linear message passing layer updates the feature of node $i$ as follows:\n$$ H_i^{(1)} = W_{self} H_i^{(0)} + W_{neigh} \\sum_{j \\in \\mathcal{N}(i)} H_j^{(0)} $$\nwhere $H_i^{(l)}$ is the feature vector of node $i$ at layer $l$, and $W_{self}, W_{neigh} \\in \\mathbb{R}^{d \\times d}$ are learnable weight matrices. The summation is over the neighbors $\\mathcal{N}(i)$ of node $i$.\n\nThe graph-level embedding $h_G$ is obtained by sum pooling the final node embeddings:\n$$ h_G = \\sum_{i=1}^n H_i^{(1)} $$\nSubstituting the update rule:\n$$ h_G = \\sum_{i=1}^n \\left( W_{self} H_i^{(0)} + W_{neigh} \\sum_{j \\in \\mathcal{N}(i)} H_j^{(0)} \\right) = W_{self} \\left(\\sum_{i=1}^n H_i^{(0)}\\right) + W_{neigh} \\left(\\sum_{i=1}^n \\sum_{j \\in \\mathcal{N}(i)} H_j^{(0)}\\right) $$\nThe first term is $W_{self} \\sum_i X_i$. The second term depends on the graph's connectivity structure (adjacency matrix $A$). Let $d_j = |\\mathcal{N}(j)|$ be the degree of node $j$. The second term can be rewritten as $\\sum_{j=1}^n d_j H_j^{(0)}$. Thus:\n$$ h_G = W_{self} \\sum_{i=1}^n X_i + W_{neigh} \\sum_{j=1}^n d_j X_j $$\nThe problem requires that the property vector $\\mathbf{p} = \\sum_i X_i$ can be recovered from $h_G$ by a linear readout $W_{readout} \\in \\mathbb{R}^{4 \\times d}$ for any graph. That is, $W_{readout} h_G = \\sum_i X_i$.\n$$ W_{readout} \\left( W_{self} \\sum_{i=1}^n X_i + W_{neigh} \\sum_{j=1}^n d_j X_j \\right) = \\sum_{i=1}^n X_i $$\nFor this equality to hold for arbitrary graphs (with varying degrees $d_j$) and arbitrary feature matrices $X$, the term involving degrees must vanish. This is only possible if $W_{neigh} = 0$. This implies that information from neighboring nodes (i.e., message passing) must be nullified to guarantee the exact recovery of purely additive properties. The adjacency matrix $A$ becomes irrelevant for this specific task.\n\nWith $W_{neigh} = 0$, the equation simplifies to $W_{readout} W_{self} \\sum_i X_i = \\sum_i X_i$. This requires $W_{readout} W_{self} = I$, where $I$ is the identity matrix. The simplest choice that satisfies this condition is to set both $W_{self}$ and $W_{readout}$ to the $4 \\times 4$ identity matrix, $I_4$.\n\nTherefore, the \"GNN\" procedure simplifies to:\n1.  Node update: $H^{(1)}_i = I_4 H^{(0)}_i = X_i$. (The features are passed through unchanged).\n2.  Sum pooling: $h_G = \\sum_i H^{(1)}_i = \\sum_i X_i$.\n3.  Linear readout: $\\mathbf{p} = I_4 h_G = h_G$.\n\nThe resulting graph-level embedding $h_G$ is precisely the vector of global properties $\\mathbf{p}$. This formulation satisfies all constraints: it is a one-layer GNN (albeit a trivial one), it is permutation-invariant due to sum pooling, and it allows exact recovery of additive properties.\n\n**3. Soft Satisfaction Score Calculation**\nGiven the property vector $\\mathbf{p} = [\\mathrm{MW}, \\log P, \\mathrm{HBD}, \\mathrm{HBA}]^T$, we calculate the drug-likeness score $s$. The thresholds are $\\mathbf{t} = [500, 5, 5, 10]^T$.\nThe normalized violations are:\n$v_{\\mathrm{MW}} = \\max\\left(0, \\frac{\\mathrm{MW} - 500}{500}\\right)$\n$v_{\\log P} = \\max\\left(0, \\frac{\\log P - 5}{5}\\right)$\n$v_{\\mathrm{HBD}} = \\max\\left(0, \\frac{\\mathrm{HBD} - 5}{5}\\right)$\n$v_{\\mathrm{HBA}} = \\max\\left(0, \\frac{\\mathrm{HBA} - 10}{10}\\right)$\nThe final score is given by:\n$s = \\max\\left(0, \\min\\left(1, 1 - \\frac{v_{\\mathrm{MW}} + v_{\\log P} + v_{\\mathrm{HBD}} + v_{\\mathrm{HBA}}}{4}\\right)\\right)$\n\n**4. Evaluation on Test Suite**\n\nWe now apply this procedure to the provided test cases.\n\n- **Case 1**:\n  $X_1$ has $5$ rows. $\\mathbf{p}_1 = \\sum_i X_{1,i}^T = [350, 2.8, 1, 6]^T$.\n  All properties are within their thresholds.\n  $v_{\\mathrm{MW}}=v_{\\log P}=v_{\\mathrm{HBD}}=v_{\\mathrm{HBA}} = 0$.\n  $s_1 = \\mathrm{clip}(1 - 0/4, 0, 1) = 1.0$.\n\n- **Case 2**:\n  $X_2$ has $10$ rows. $\\mathbf{p}_2 = \\sum_i X_{2,i}^T = [500, 5.0, 5, 10]^T$.\n  All properties are exactly at their thresholds.\n  $v_{\\mathrm{MW}} = \\max(0, \\frac{500-500}{500})=0$.\n  $v_{\\log P} = \\max(0, \\frac{5-5}{5})=0$.\n  $v_{\\mathrm{HBD}} = \\max(0, \\frac{5-5}{5})=0$.\n  $v_{\\mathrm{HBA}} = \\max(0, \\frac{10-10}{10})=0$.\n  $s_2 = \\mathrm{clip}(1 - 0/4, 0, 1) = 1.0$.\n\n- **Case 3**:\n  $X_3$ has $6$ rows. $\\mathbf{p}_3 = \\sum_i X_{3,i}^T = [620, 6.4, 2, 9]^T$.\n  $\\mathrm{MW}$ and $\\log P$ violate their rules.\n  $v_{\\mathrm{MW}} = \\max(0, \\frac{620-500}{500}) = 0.24$.\n  $v_{\\log P} = \\max(0, \\frac{6.4-5}{5}) = 0.28$.\n  $v_{\\mathrm{HBD}} = \\max(0, \\frac{2-5}{5}) = 0$.\n  $v_{\\mathrm{HBA}} = \\max(0, \\frac{9-10}{10}) = 0$.\n  Total violation: $0.24 + 0.28 = 0.52$.\n  $s_3 = \\mathrm{clip}(1 - 0.52/4, 0, 1) = \\mathrm{clip}(1 - 0.13, 0, 1) = 0.87$.\n\n- **Case 4**:\n  $X_4$ has $1$ row. $\\mathbf{p}_4 = [18, -0.4, 2, 1]^T$.\n  All properties are well within their thresholds.\n  $v_{\\mathrm{MW}}=v_{\\log P}=v_{\\mathrm{HBD}}=v_{\\mathrm{HBA}} = 0$.\n  $s_4 = \\mathrm{clip}(1 - 0/4, 0, 1) = 1.0$.\n\nThe final computed scores are $[1.0, 1.0, 0.87, 1.0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the GNN-based drug-likeness scoring procedure and evaluates it on the test suite.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # The adjacency matrix A is not used in the final calculation, as derived in the solution,\n    # because recovering purely additive properties requires nullifying the message passing term.\n    # It is included here for completeness.\n    test_cases = [\n        # Case 1\n        (np.array([[0, 1, 0, 0, 0], [1, 0, 1, 0, 0], [0, 1, 0, 1, 0], [0, 0, 1, 0, 1], [0, 0, 0, 1, 0]]),\n         np.array([[60, 0.4, 0, 1], [80, 0.6, 0, 2], [100, 0.8, 1, 1], [55, 0.5, 0, 1], [55, 0.5, 0, 1]])),\n        # Case 2\n        (np.array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0], \n                   [0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0], \n                   [0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1], \n                   [1, 0, 0, 0, 0, 0, 0, 0, 1, 0]]),\n         np.array([[50, 0.5, 1, 1], [50, 0.5, 1, 1], [50, 0.5, 1, 1], [50, 0.5, 1, 1], [50, 0.5, 1, 1], \n                   [50, 0.5, 0, 1], [50, 0.5, 0, 1], [50, 0.5, 0, 1], [50, 0.5, 0, 1], [50, 0.5, 0, 1]])),\n        # Case 3\n        (np.array([[0, 1, 0, 0, 0, 0], [1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 1, 0], \n                   [0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0]]),\n         np.array([[100, 0.9, 0, 1], [120, 1.2, 0, 2], [130, 1.4, 1, 2], [90, 1.0, 0, 2], \n                   [80, 0.8, 1, 1], [100, 1.1, 0, 1]])),\n        # Case 4\n        (np.array([[0]]),\n         np.array([[18, -0.4, 2, 1]]))\n    ]\n\n    def calculate_drug_likeness_score(X: np.ndarray) -> float:\n        \"\"\"\n        Calculates the drug-likeness score based on Lipinski's rule of five.\n\n        Args:\n            X: Node feature matrix of shape (n_atoms, 4), where columns are\n               per-atom contributions to MW, logP, HBD, and HBA.\n\n        Returns:\n            The soft satisfaction score s, a float between 0 and 1.\n        \"\"\"\n        # The GNN-based procedure simplifies to summing the node features to get the graph-level properties.\n        # This corresponds to sum pooling of the initial node features.\n        properties = np.sum(X, axis=0)\n        mw, logp, hbd, hba = properties[0], properties[1], properties[2], properties[3]\n\n        # Lipinski's rule thresholds\n        thresholds = {'mw': 500, 'logp': 5, 'hbd': 5, 'hba': 10}\n\n        # Calculate normalized violations using a rectified linear unit (phi(z) = max(0, z))\n        v_mw = max(0, (mw - thresholds['mw']) / thresholds['mw'])\n        v_logp = max(0, (logp - thresholds['logp']) / thresholds['logp'])\n        v_hbd = max(0, (hbd - thresholds['hbd']) / thresholds['hbd'])\n        v_hba = max(0, (hba - thresholds['hba']) / thresholds['hba'])\n\n        # Calculate the total penalty\n        total_violation = v_mw + v_logp + v_hbd + v_hba\n        \n        # Calculate the final score, clipped to the [0, 1] range\n        score = 1.0 - (total_violation / 4.0)\n        final_score = np.clip(score, 0, 1)\n        \n        return final_score\n\n    results = []\n    for _, X_case in test_cases:\n        score = calculate_drug_likeness_score(X_case)\n        results.append(score)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "2395422"}]}