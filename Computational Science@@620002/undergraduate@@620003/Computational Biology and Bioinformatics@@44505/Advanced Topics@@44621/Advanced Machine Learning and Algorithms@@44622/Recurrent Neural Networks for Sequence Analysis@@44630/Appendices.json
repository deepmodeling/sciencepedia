{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with a foundational exercise that demystifies the core mechanics of a Recurrent Neural Network. In this problem, we will configure a very simple RNN to solve a classic bioinformatics task: finding the Watson-Crick complementary strand for a given DNA sequence. By working through this deliberately simplified model, you will gain a concrete understanding of how input data is processed through the network's layers and how parameters like weight matrices directly shape the model's output, all without the initial complexity of long-range temporal dependencies [@problem_id:2425719].", "problem": "Consider the task of predicting the complementary deoxyribonucleic acid (DNA) strand for a given input strand using a Recurrent Neural Network (RNN). The DNA alphabet consists of the nucleotides $\\mathrm{A}$, $\\mathrm{C}$, $\\mathrm{G}$, and $\\mathrm{T}$. Define the index mapping $\\mathrm{A} \\mapsto 0$, $\\mathrm{C} \\mapsto 1$, $\\mathrm{G} \\mapsto 2$, and $\\mathrm{T} \\mapsto 3$. The Watson–Crick complement mapping is given by the function $\\mathrm{comp}:\\{0,1,2,3\\}\\to\\{0,1,2,3\\}$ defined as $\\mathrm{comp}(0)=3$, $\\mathrm{comp}(1)=2$, $\\mathrm{comp}(2)=1$, and $\\mathrm{comp}(3)=0$.\n\nLet the model be a discrete-time Elman RNN with a hidden state of dimension $4$ and a per-position categorical output over $4$ symbols. For a sequence of length $T$, let the one-hot input at position $t$ be $x_t \\in \\mathbb{R}^4$, the hidden state be $h_t \\in \\mathbb{R}^4$, the pre-softmax output (logits) be $o_t \\in \\mathbb{R}^4$, and the output probability vector be $p_t \\in \\mathbb{R}^4$. The recurrence and output equations are\n$$\nh_t = \\tanh\\!\\left(W_{xh} x_t + W_{hh} h_{t-1} + b_h\\right), \\quad o_t = W_{hy} h_t + b_y, \\quad p_t = \\mathrm{softmax}(o_t),\n$$\nwith initial hidden state $h_0 = \\mathbf{0}_4$. The hyperbolic tangent function $\\tanh(\\cdot)$ acts element-wise and the softmax is defined by\n$$\n\\mathrm{softmax}(o)_k = \\frac{e^{o_k}}{\\sum_{j=1}^{4} e^{o_j}},\n$$\nfor $k \\in \\{1,2,3,4\\}$ indexing the output components.\n\nAll model parameters are fixed and given as follows:\n- Input-to-hidden weights $W_{xh} \\in \\mathbb{R}^{4 \\times 4}$ are the identity matrix: $W_{xh} = I_4$.\n- Hidden-to-hidden weights $W_{hh} \\in \\mathbb{R}^{4 \\times 4}$ are all zeros: $W_{hh} = 0_{4 \\times 4}$.\n- Hidden bias $b_h \\in \\mathbb{R}^{4}$ is all zeros: $b_h = \\mathbf{0}_4$.\n- Hidden-to-output weights $W_{hy} \\in \\mathbb{R}^{4 \\times 4}$ are defined by the entries\n$$\n\\left(W_{hy}\\right)_{j,i} = \\begin{cases}\n2  \\text{if } j = \\mathrm{comp}(i),\\\\\n0  \\text{otherwise},\n\\end{cases}\n$$\nfor all $i \\in \\{0,1,2,3\\}$ and $j \\in \\{0,1,2,3\\}$, where rows and columns are indexed by the integer codes of the output and input nucleotides respectively.\n- Output bias $b_y \\in \\mathbb{R}^{4}$ is all zeros: $b_y = \\mathbf{0}_4$.\n\nFor any input sequence $(x_1,\\dots,x_T)$, at each time step $t$ the predicted symbol index $\\hat{y}_t$ is defined as the maximizer of the output distribution,\n$$\n\\hat{y}_t = \\arg\\max_{k \\in \\{0,1,2,3\\}} \\, p_t[k].\n$$\nThe program must process each test sequence independently, resetting $h_0$ to $\\mathbf{0}_4$ for each sequence. The final answer for each test case must be the list $[\\hat{y}_1,\\ldots,\\hat{y}_T]$ of integer indices.\n\nUse the following test suite of input sequences, expressed as index-coded lists using the mapping $\\mathrm{A} \\mapsto 0$, $\\mathrm{C} \\mapsto 1$, $\\mathrm{G} \\mapsto 2$, $\\mathrm{T} \\mapsto 3$:\n- Case $1$ (a general case using all symbols): $[0,1,2,3]$.\n- Case $2$ (boundary: length $1$): $[0]$.\n- Case $3$ (edge case: homopolymer): $[2,2,2,2]$.\n- Case $4$ (alternating pattern): $[0,3,0,3]$.\n- Case $5$ (palindromic pattern under complement): $[1,2,1,2]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the given order and is itself a bracketed, comma-separated list of integers. For example, the overall structure must be of the form \"[[...],[...],[...],[...],[...]]\". No physical units are involved, and no angles or percentages are required. The outputs for each test case must be integer lists as specified.", "solution": "The RNN is fully specified by the given parameter values. The objective is to determine, for each position $t$ in a given input sequence, the predicted class index $\\hat{y}_t \\in \\{0,1,2,3\\}$ using the model equations and the $\\arg\\max$ decision rule.\n\nFirst, encode each input nucleotide as a one-hot vector $x_t \\in \\mathbb{R}^4$ according to the given index mapping $\\mathrm{A} \\mapsto 0$, $\\mathrm{C} \\mapsto 1$, $\\mathrm{G} \\mapsto 2$, $\\mathrm{T} \\mapsto 3$. Thus, if the nucleotide index at time $t$ is $i \\in \\{0,1,2,3\\}$, then $x_t$ has $1$ in the $i$-th position and $0$ elsewhere.\n\nThe hidden state recursion is\n$$\nh_t = \\tanh\\!\\left(W_{xh} x_t + W_{hh} h_{t-1} + b_h\\right),\n$$\nwith $h_0 = \\mathbf{0}_4$. Using the provided parameters $W_{xh} = I_4$, $W_{hh} = 0_{4\\times 4}$, and $b_h = \\mathbf{0}_4$, this reduces to\n$$\nh_t = \\tanh\\!\\left(I_4 \\, x_t\\right) = \\tanh(x_t).\n$$\nSince $x_t$ is one-hot, $\\tanh$ acts element-wise and yields a vector with a single nonzero entry equal to $\\tanh(1)$ at the active index and zeros elsewhere. Denote $c \\triangleq \\tanh(1)$, where $c \\in (0,1)$ and numerically $c \\approx 0.76159$, though its exact value is not required for the decision rule.\n\nThe logits are\n$$\no_t = W_{hy} h_t + b_y = W_{hy} h_t,\n$$\nsince $b_y = \\mathbf{0}_4$. By the definition of $W_{hy}$, for an input index $i$ at time $t$, the hidden state $h_t$ has $h_t[i] = c$ and $h_t[k] = 0$ for $k \\neq i$. Therefore, the $j$-th logit is\n$$\no_t[j] = \\sum_{k=0}^{3} (W_{hy})_{j,k} \\, h_t[k] = (W_{hy})_{j,i} \\, c = \\begin{cases}\n2c  \\text{if } j = \\mathrm{comp}(i),\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\nConsequently, the softmax probabilities satisfy\n$$\np_t[j] = \\frac{e^{o_t[j]}}{\\sum_{\\ell=0}^{3} e^{o_t[\\ell]}} = \\begin{cases}\n\\displaystyle \\frac{e^{2c}}{3 + e^{2c}}  \\text{if } j = \\mathrm{comp}(i),\\\\\n\\displaystyle \\frac{1}{3 + e^{2c}}  \\text{otherwise}.\n\\end{cases}\n$$\nBecause $c  0$, it follows that $2c  0$, hence $e^{2c}  1$, which implies that $p_t[\\mathrm{comp}(i)]$ is strictly larger than $p_t[j]$ for any $j \\neq \\mathrm{comp}(i)$. Therefore, the $\\arg\\max$ decision yields\n$$\n\\hat{y}_t = \\arg\\max_{j \\in \\{0,1,2,3\\}} p_t[j] = \\mathrm{comp}(i).\n$$\n\nThis shows that, for each input symbol index $i$ at position $t$, the predicted output index $\\hat{y}_t$ equals its Watson–Crick complement index $\\mathrm{comp}(i)$, independently of previous positions, because $W_{hh} = 0_{4 \\times 4}$ and $b_h = \\mathbf{0}_4$ eliminate temporal dependence.\n\nApplying this to each test sequence:\n\n- Case $1$: Input $[0,1,2,3]$ maps element-wise via $\\mathrm{comp}$ to $[3,2,1,0]$.\n- Case $2$: Input $[0]$ maps to $[3]$.\n- Case $3$: Input $[2,2,2,2]$ maps to $[1,1,1,1]$.\n- Case $4$: Input $[0,3,0,3]$ maps to $[3,0,3,0]$.\n- Case $5$: Input $[1,2,1,2]$ maps to $[2,1,2,1]$.\n\nThe program must implement the stated RNN equations with the given parameters, process each test sequence independently with $h_0 = \\mathbf{0}_4$, compute $\\hat{y}_t$ at each position by the $\\arg\\max$ over the softmax outputs (equivalently, over the logits due to monotonicity), and print the list of predicted index sequences in a single bracketed line in the prescribed format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rnn_predict(sequences):\n    # Define dimensions\n    num_symbols = 4  # A, C, G, T\n    hidden_size = 4\n\n    # Define parameter matrices and vectors as specified\n    W_xh = np.eye(num_symbols)  # 4x4 identity\n    W_hh = np.zeros((hidden_size, hidden_size))  # 4x4 zeros\n    b_h = np.zeros(hidden_size)  # 4 zeros\n\n    # Define complement mapping: A-T, C-G, G-C, T-A with indices 0,1,2,3\n    comp = {0: 3, 1: 2, 2: 1, 3: 0}\n\n    # Hidden-to-output weights: (W_hy)_{j,i} = 2 if j == comp(i) else 0\n    W_hy = np.zeros((num_symbols, hidden_size))\n    for i in range(num_symbols):\n        j = comp[i]\n        W_hy[j, i] = 2.0\n    b_y = np.zeros(num_symbols)  # 4 zeros\n\n    results = []\n    for seq in sequences:\n        # Reset hidden state for each sequence\n        h = np.zeros(hidden_size)\n        pred_seq = []\n        for idx in seq:\n            # One-hot encode input symbol\n            x = np.zeros(num_symbols)\n            x[idx] = 1.0\n\n            # RNN forward step\n            h = np.tanh(W_xh.dot(x) + W_hh.dot(h) + b_h)\n            o = W_hy.dot(h) + b_y  # logits\n            # Argmax over logits equals argmax over softmax\n            y_hat = int(np.argmax(o))\n            pred_seq.append(y_hat)\n        results.append(pred_seq)\n    return results\n\ndef format_list_no_spaces(obj):\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(format_list_no_spaces(el) for el in obj) + \"]\"\n    else:\n        return str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement (index-coded sequences)\n    test_cases = [\n        [0, 1, 2, 3],      # Case 1\n        [0],               # Case 2\n        [2, 2, 2, 2],      # Case 3\n        [0, 3, 0, 3],      # Case 4\n        [1, 2, 1, 2],      # Case 5\n    ]\n\n    results = rnn_predict(test_cases)\n\n    # Final print statement in the exact required format (single line, no spaces).\n    print(format_list_no_spaces(results))\n\nsolve()\n```", "id": "2425719"}, {"introduction": "Building on the fundamentals, our next practice introduces a fully recurrent network where the hidden state at each step depends on the previous one, enabling the model to have memory. We will apply this model to a critical task in computational biology: in silico mutagenesis, or predicting the functional consequence of a genetic mutation. This exercise requires you to simulate the effect of a frameshift mutation and quantify its impact by measuring the divergence in the RNN's internal hidden states, providing a powerful analogy for how such mutations can disrupt biological information flow [@problem_id:2425716].", "problem": "You are given a simplified recurrent neural network (Recurrent Neural Network (RNN)) model for sequence analysis applied to Deoxyribonucleic Acid (DNA) nucleotide sequences over the alphabet $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$. Each nucleotide is mapped to a one-hot vector in $\\mathbb{R}^{4}$ via the canonical basis: $\\mathrm{A}\\mapsto (1,0,0,0)^{\\top}$, $\\mathrm{C}\\mapsto (0,1,0,0)^{\\top}$, $\\mathrm{G}\\mapsto (0,0,1,0)^{\\top}$, $\\mathrm{T}\\mapsto (0,0,0,1)^{\\top}$. The RNN has hidden dimension $d_{h}=3$, input dimension $d_{x}=4$, and uses the recurrence\n$$\nh_{0}=\\mathbf{0}\\in\\mathbb{R}^{3},\\quad h_{t}=\\tanh\\!\\left(W_{x}\\,x_{t}+W_{h}\\,h_{t-1}+b\\right)\\in\\mathbb{R}^{3}\\quad \\text{for } t=1,2,\\dots,T,\n$$\nwhere $x_{t}\\in\\mathbb{R}^{4}$ is the one-hot encoding of the $t$-th nucleotide in the sequence, $W_{x}\\in\\mathbb{R}^{3\\times 4}$, $W_{h}\\in\\mathbb{R}^{3\\times 3}$, and $b\\in\\mathbb{R}^{3}$ are fixed parameters, and $\\tanh(\\cdot)$ acts elementwise. A scalar function prediction is obtained from the final hidden state via the logistic sigmoid $\\sigma(z)=\\frac{1}{1+e^{-z}}$:\n$$\n\\hat{y}=\\sigma\\!\\left(w^{\\top} h_{T}+b_{y}\\right),\\quad w\\in\\mathbb{R}^{3},\\ b_{y}\\in\\mathbb{R}.\n$$\nA frameshift mutation is modeled as either a single-nucleotide insertion or deletion at a specified position $m$ in the original sequence. Let the original sequence length be $T$ and the mutated sequence length be $T'$. We define the mutated sequence as follows:\n- For an insertion of a nucleotide $u\\in\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ at position $m$ (with positions indexed from $1$), the new nucleotide $u$ is inserted immediately after position $m$ of the original sequence, yielding length $T'=T+1$.\n- For a deletion at position $m$, the nucleotide originally at position $m$ is removed, yielding length $T'=T-1$.\nFor both the original and mutated sequences, compute their hidden state sequences $\\{h_{t}^{\\text{orig}}\\}_{t=1}^{T}$ and $\\{h_{t}^{\\text{mut}}\\}_{t=1}^{T'}$ using the same RNN parameters and initialization $h_{0}=\\mathbf{0}$. Define the hidden-state divergence after the mutation point by the Euclidean distances\n$$\nd_{t}=\\left\\|h_{t}^{\\text{mut}}-h_{t}^{\\text{orig}}\\right\\|_{2}\\quad \\text{for } t=m,m+1,\\dots,\\min(T,T').\n$$\nIf $m\\min(T,T')$, define the average below to be $0$. Let the average post-mutation hidden-state divergence be\n$$\n\\overline{D}=\\begin{cases}\n\\dfrac{1}{\\min(T,T')-m+1}\\displaystyle\\sum_{t=m}^{\\min(T,T')} d_{t},  \\text{if } m\\le \\min(T,T'),\\\\\n0,  \\text{if } m\\min(T,T').\n\\end{cases}\n$$\nLet the function predictions be $\\hat{y}^{\\text{orig}}=\\sigma\\!\\left(w^{\\top} h_{T}^{\\text{orig}}+b_{y}\\right)$ and $\\hat{y}^{\\text{mut}}=\\sigma\\!\\left(w^{\\top} h_{T'}^{\\text{mut}}+b_{y}\\right)$. For a given weighting parameter $\\lambda0$, define the functional perturbation index\n$$\nF=\\left|\\hat{y}^{\\text{mut}}-\\hat{y}^{\\text{orig}}\\right|+\\lambda\\,\\overline{D}.\n$$\nUse the following fixed RNN parameters:\n$$\nW_{x}=\\begin{bmatrix}\n0.5  -0.3  0.8  -0.1\\\\\n-0.2  0.7  -0.5  0.4\\\\\n0.3  0.1  -0.4  0.6\n\\end{bmatrix},\\quad\nW_{h}=\\begin{bmatrix}\n0.2  0.1  -0.3\\\\\n-0.4  0.5  0.2\\\\\n0.3  -0.6  0.1\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}0.01\\\\ -0.02\\\\ 0.03\\end{bmatrix},\n$$\n$$\nw=\\begin{bmatrix}0.4\\\\ -0.7\\\\ 0.2\\end{bmatrix},\\quad b_{y}=-0.1,\\quad \\lambda=0.35.\n$$\nYour task is to compute $F$ for each of the following test cases. In each case, the sequence consists only of characters from $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, the position $m$ is indexed from $1$, and insertions are “insert immediately after position $m$.” For deletions, the character at position $m$ is removed. The test suite is:\n- Case $1$ (general case): original sequence $S=\\text{\"ATGGCCATTGTA\"}$, insertion of $u=\\text{\"A\"}$ at $m=5$.\n- Case $2$ (boundary at start): original sequence $S=\\text{\"ATGCGT\"}$, deletion at $m=1$.\n- Case $3$ (insertion at last position): original sequence $S=\\text{\"CCGTAACG\"}$, insertion of $u=\\text{\"T\"}$ at $m=8$.\n- Case $4$ (deletion at last position, no overlapping indices after $m$): original sequence $S=\\text{\"GATTACA\"}$, deletion at $m=7$.\nAll computations must follow the definitions above exactly. Angles are not used. There are no physical units. Each result must be a real number. Your program should produce a single line of output containing the results as a comma-separated list of the four values $F$ (in the order of Cases $1$ through $4$), each rounded to exactly six digits after the decimal point, enclosed in square brackets; for example, $\\text{[}r_{1},r_{2},r_{3},r_{4}\\text{]}$ with each $r_{i}$ formatted to six decimal places.", "solution": "The problem presented is valid. It is a well-defined computational task based on the established principles of recurrent neural networks (RNNs) as applied to sequence analysis in computational biology. The model, parameters, and analysis metrics are specified with mathematical precision, are scientifically grounded, and form a self-contained, solvable problem. I will proceed with the solution.\n\nThe task is to compute a functional perturbation index, $F$, for a simplified RNN subjected to specific frameshift mutations in DNA sequences. The solution requires a direct implementation of the provided mathematical model. The process is divided into several computational steps, which are executed for each test case.\n\nFirst, we define the constant parameters of the network. The one-hot encoding maps each nucleotide from the alphabet $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ to a standard basis vector in $\\mathbb{R}^{4}$.\n- $\\mathrm{A} \\mapsto [1, 0, 0, 0]^{\\top}$\n- $\\mathrm{C} \\mapsto [0, 1, 0, 0]^{\\top}$\n- $\\mathrm{G} \\mapsto [0, 0, 1, 0]^{\\top}$\n- $\\mathrm{T} \\mapsto [0, 0, 0, 1]^{\\top}$\n\nThe fixed parameters of the RNN are the weight matrices $W_{x}\\in\\mathbb{R}^{3\\times 4}$ and $W_{h}\\in\\mathbb{R}^{3\\times 3}$, and the bias vector $b\\in\\mathbb{R}^{3}$. The output layer parameters are the weight vector $w\\in\\mathbb{R}^{3}$ and scalar bias $b_{y}\\in\\mathbb{R}$. The contribution of the hidden-state divergence is weighted by $\\lambda = 0.35$.\n\n$W_{x}=\\begin{bmatrix} 0.5  -0.3  0.8  -0.1 \\\\ -0.2  0.7  -0.5  0.4 \\\\ 0.3  0.1  -0.4  0.6 \\end{bmatrix}$, $W_{h}=\\begin{bmatrix} 0.2  0.1  -0.3 \\\\ -0.4  0.5  0.2 \\\\ 0.3  -0.6  0.1 \\end{bmatrix}$, $b=\\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.03 \\end{bmatrix}$\n$w=\\begin{bmatrix} 0.4 \\\\ -0.7 \\\\ 0.2 \\end{bmatrix}$, $b_{y}=-0.1$\n\nThe core of the model is the recurrence relation for the hidden state $h_t \\in \\mathbb{R}^3$:\n$$h_{t}=\\tanh\\!\\left(W_{x}\\,x_{t}+W_{h}\\,h_{t-1}+b\\right)$$\nThis computation is performed iteratively, starting with an initial hidden state $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$. For a given input sequence of length $T$, this process generates a sequence of hidden states $\\{h_t\\}_{t=1}^{T}$. The $\\tanh$ function is applied element-wise.\n\nFor each test case, we must first establish the original sequence, $S^{\\text{orig}}$, and the mutated sequence, $S^{\\text{mut}}$. The mutation is either an insertion of a nucleotide $u$ after a 1-indexed position $m$, or a deletion of the nucleotide at position $m$.\n\nSubsequently, we apply the RNN forward pass to both $S^{\\text{orig}}$ and $S^{\\text{mut}}$ to obtain their respective hidden state sequences, $\\{h_{t}^{\\text{orig}}\\}_{t=1}^{T}$ and $\\{h_{t}^{\\text{mut}}\\}_{t=1}^{T'}$.\n\nThe next step is to calculate the average post-mutation hidden-state divergence, $\\overline{D}$. This metric quantifies how the mutation causes the RNN's internal representation to deviate from the original. It is defined as:\n$$\\overline{D}=\\begin{cases}\n\\dfrac{1}{\\min(T,T')-m+1}\\displaystyle\\sum_{t=m}^{\\min(T,T')} \\left\\|h_{t}^{\\text{mut}}-h_{t}^{\\text{orig}}\\right\\|_{2},  \\text{if } m\\le \\min(T,T') \\\\\n0,  \\text{if } m\\min(T,T')\n\\end{cases}$$\nThe summation ranges over the indices starting from the mutation point $m$ up to the end of the shorter sequence. The norm $\\left\\|\\cdot\\right\\|_{2}$ is the standard Euclidean distance. Note that for the implementation,序列索引 from the problem (1-based) must be converted to array indices (0-based), so $h_t$ corresponds to the element at index $t-1$ in the list of states.\n\nThen, we compute the scalar predictions, $\\hat{y}^{\\text{orig}}$ and $\\hat{y}^{\\text{mut}}$, from the final hidden states, $h_{T}^{\\text{orig}}$ and $h_{T'}^{\\text{mut}}$, respectively. The calculation uses the logistic sigmoid function $\\sigma(z) = (1+e^{-z})^{-1}$:\n$$\\hat{y}^{\\text{orig}}=\\sigma\\!\\left(w^{\\top} h_{T}^{\\text{orig}}+b_{y}\\right)$$\n$$\\hat{y}^{\\text{mut}}=\\sigma\\!\\left(w^{\\top} h_{T'}^{\\text{mut}}+b_{y}\\right)$$\n\nFinally, the functional perturbation index $F$ is assembled from the absolute difference in predictions and the weighted average divergence:\n$$F=\\left|\\hat{y}^{\\text{mut}}-\\hat{y}^{\\text{orig}}\\right|+\\lambda\\,\\overline{D}$$\n\nThis entire procedure is implemented and applied to the four specified test cases. The logic correctly handles boundary conditions, such as mutations at the beginning or end of a sequence, and the special case for $\\overline{D}$ when $m  \\min(T, T')$, as encountered in Case $4$. The final numerical results are rounded to six decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are permitted.\n\ndef solve():\n    \"\"\"\n    Main function to solve the RNN mutation analysis problem.\n    It sets up the model parameters, defines test cases, and computes\n    the functional perturbation index F for each case.\n    \"\"\"\n    \n    # ------------------ Fixed RNN Parameters ------------------ #\n    # Weight matrix for input-to-hidden connections\n    Wx = np.array([\n        [0.5, -0.3, 0.8, -0.1],\n        [-0.2, 0.7, -0.5, 0.4],\n        [0.3, 0.1, -0.4, 0.6]\n    ])\n    \n    # Weight matrix for hidden-to-hidden connections\n    Wh = np.array([\n        [0.2, 0.1, -0.3],\n        [-0.4, 0.5, 0.2],\n        [0.3, -0.6, 0.1]\n    ])\n    \n    # Bias vector for the hidden layer\n    b = np.array([0.01, -0.02, 0.03]).reshape(3, 1)\n    \n    # Weight vector for the output layer\n    w = np.array([0.4, -0.7, 0.2]).reshape(3, 1)\n    \n    # Bias for the output layer\n    by = -0.1\n    \n    # Weighting parameter for the divergence term\n    lambda_param = 0.35\n\n    # ------------------ Encoding and Helper Functions ------------------ #\n    # One-hot encoding map for DNA nucleotides\n    dna_map = {\n        'A': np.array([1, 0, 0, 0]).reshape(4, 1),\n        'C': np.array([0, 1, 0, 0]).reshape(4, 1),\n        'G': np.array([0, 0, 1, 0]).reshape(4, 1),\n        'T': np.array([0, 0, 0, 1]).reshape(4, 1),\n    }\n\n    def sigmoid(z):\n        \"\"\"Computes the logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def rnn_forward(sequence_str):\n        \"\"\"\n        Performs the forward pass of the RNN for a given sequence.\n        Returns a list of all hidden states.\n        \"\"\"\n        T = len(sequence_str)\n        h_dim = Wh.shape[0]\n        \n        h_states = []\n        h_prev = np.zeros((h_dim, 1))\n\n        for t in range(T):\n            char = sequence_str[t]\n            x_t = dna_map[char]\n            \n            # Recurrence relation\n            h_t = np.tanh(Wx @ x_t + Wh @ h_prev + b)\n            h_states.append(h_t)\n            h_prev = h_t\n            \n        return h_states\n\n    def compute_f_for_case(S_orig, mutation_type, m, u=None):\n        \"\"\"\n        Computes the functional perturbation index F for a single test case.\n        \"\"\"\n        # 1. Generate mutated sequence based on problem rules\n        T_orig = len(S_orig)\n        if mutation_type == 'insertion':\n            # Insert nucleotide u immediately after 1-indexed position m\n            S_mut = S_orig[:m] + u + S_orig[m:]\n        elif mutation_type == 'deletion':\n            # Remove nucleotide at 1-indexed position m\n            S_mut = S_orig[:m-1] + S_orig[m:]\n        else:\n            raise ValueError(\"Invalid mutation type specified.\")\n        \n        T_mut = len(S_mut)\n\n        # 2. Compute hidden state sequences for original and mutated sequences\n        h_orig_states = rnn_forward(S_orig)\n        h_mut_states = rnn_forward(S_mut)\n\n        # 3. Compute average post-mutation hidden-state divergence D_bar\n        min_T = min(T_orig, T_mut)\n        D_bar = 0.0\n        \n        if m = min_T:\n            sum_d_t = 0.0\n            # Summation is from t=m to min_T (1-based indices)\n            for t_idx in range(m, min_T + 1):\n                # Convert 1-based t to 0-based list index\n                h_t_orig = h_orig_states[t_idx-1]\n                h_t_mut = h_mut_states[t_idx-1]\n                # Euclidean distance\n                d_t = np.linalg.norm(h_t_mut - h_t_orig)\n                sum_d_t += d_t\n            \n            num_terms = min_T - m + 1\n            D_bar = sum_d_t / num_terms\n        # If m  min_T, D_bar is 0 by definition, which is the initial value.\n\n        # 4. Compute final predictions y_hat for both sequences\n        h_T_orig = h_orig_states[-1]\n        y_hat_orig = sigmoid((w.T @ h_T_orig + by).item())\n        \n        # Handle case where mutated sequence is empty\n        if not h_mut_states:\n            # Re-initialize h_0 for empty sequence prediction\n            h_final_mut = np.zeros((Wh.shape[0], 1))\n        else:\n            h_final_mut = h_mut_states[-1]\n        \n        y_hat_mut = sigmoid((w.T @ h_final_mut + by).item())\n\n        # 5. Compute the final Functional Perturbation Index F\n        F = np.abs(y_hat_mut - y_hat_orig) + lambda_param * D_bar\n\n        return F\n\n    # ------------------ Test Cases ------------------ #\n    test_cases = [\n        # (original_sequence, mutation_type, m, nucleotide_for_insertion)\n        (\"ATGGCCATTGTA\", 'insertion', 5, 'A'),\n        (\"ATGCGT\", 'deletion', 1, None),\n        (\"CCGTAACG\", 'insertion', 8, 'T'),\n        (\"GATTACA\", 'deletion', 7, None),\n    ]\n\n    results = []\n    for case in test_cases:\n        S_orig, mut_type, m, u = case\n        result = compute_f_for_case(S_orig, mut_type, m, u)\n        results.append(result)\n\n    # ------------------ Final Output ------------------ #\n    # Format results as a comma-separated list of numbers rounded to 6 decimal places,\n    # enclosed in square brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2425716"}, {"introduction": "For our final and most advanced practice, we move from simple RNNs to the more powerful Long Short-Term Memory (LSTM) network. This problem challenges you to go beyond merely executing a model and instead to \"think like an LSTM\" by reverse-engineering an input sequence. You will dissect the core component of an LSTM—the cell state and its gating mechanisms—to design a minimal DNA sequence that ensures a piece of information is \"remembered\" across a very long distance. This exercise provides deep, practical insight into why LSTMs are so effective at capturing the long-range dependencies that are ubiquitous in biological sequences [@problem_id:2425681].", "problem": "You are given a single-cell Long Short-Term Memory (LSTM) unit for processing deoxyribonucleic acid (DNA) sequences over the alphabet $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, represented as one-hot vectors. The LSTM comprises an input gate, forget gate, and cell-candidate nonlinearity. Let $\\sigma(z) = \\dfrac{1}{1+\\exp(-z)}$ denote the logistic sigmoid function and $\\tanh(z)$ denote the hyperbolic tangent. For a sequence of one-hot inputs $\\{x_t\\}$, the cell state $\\{c_t\\}$ evolves as\n$$\nc_t \\;=\\; f_t \\, c_{t-1} \\;+\\; i_t \\, g_t,\n$$\nwhere\n$$\nf_t \\;=\\; \\sigma\\!\\left(z_f(x_t)\\right), \\quad\ni_t \\;=\\; \\sigma\\!\\left(z_i(x_t)\\right), \\quad\ng_t \\;=\\; \\tanh\\!\\left(z_g(x_t)\\right).\n$$\nThe initial state is $c_0 = 0$. For each nucleotide $\\nu \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, the pre-activations $z_f(\\nu)$, $z_i(\\nu)$, and $z_g(\\nu)$ are fixed constants given by:\n- For $\\mathrm{A}$: $z_f(\\mathrm{A}) = 2.0$, $z_i(\\mathrm{A}) = 3.0$, $z_g(\\mathrm{A}) = 3.0$.\n- For $\\mathrm{C}$: $z_f(\\mathrm{C}) = 3.0$, $z_i(\\mathrm{C}) = -4.0$, $z_g(\\mathrm{C}) = 0.0$.\n- For $\\mathrm{G}$: $z_f(\\mathrm{G}) = 1.0$, $z_i(\\mathrm{G}) = -2.0$, $z_g(\\mathrm{G}) = -3.0$.\n- For $\\mathrm{T}$: $z_f(\\mathrm{T}) = 7.600902$, $z_i(\\mathrm{T}) = -6.0$, $z_g(\\mathrm{T}) = 0.0$.\n\nConsider sequences restricted to the following two-block form: a prefix consisting of $m$ repetitions of a single nucleotide $\\alpha \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, followed by a filler block consisting of $L$ repetitions of a single nucleotide $\\beta \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. Let $\\theta  0$ be a target threshold. A design is feasible if, after processing the entire sequence, the final cell state $c_{m+L}$ satisfies $c_{m+L} \\ge \\theta$.\n\nFor each test case $(L,\\theta)$, determine the minimal nonnegative integer $m$ for which there exists a choice of $\\alpha$ and $\\beta$ such that the design is feasible. If no finite $m$ can satisfy the inequality for any choice of $\\alpha$ and $\\beta$, return $-1$ for that test case. All computations must follow directly from the definitions above; no additional assumptions are permitted. Angles do not appear; no physical units are involved.\n\nUse the following test suite of parameters $(L,\\theta)$:\n- $(L,\\theta) = (10,\\,0.2)$\n- $(L,\\theta) = (1000,\\,0.2)$\n- $(L,\\theta) = (1500,\\,0.5)$\n- $(L,\\theta) = (5000,\\,1.0)$\n- $(L,\\theta) = (0,\\,0.1)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[m_1,m_2,m_3,m_4,m_5]$), where each $m_k$ is the required minimal integer for the corresponding test case in the order listed above.", "solution": "The problem requires finding the minimal non-negative integer $m$ for which a given feasibility condition is met for a specially structured DNA sequence. A sequence is composed of a prefix of $m$ repetitions of a nucleotide $\\alpha \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ followed by a filler block of $L$ repetitions of a nucleotide $\\beta \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. The feasibility condition is that the final cell state $c_{m+L}$ of a simplified Long Short-Term Memory (LSTM) unit must be greater than or equal to a threshold $\\theta  0$.\n\nFirst, we must formalize the computation of the final cell state $c_{m+L}$. The state update equation is given as a linear recurrence relation:\n$$c_t = f_t c_{t-1} + i_t g_t$$\nwhere the gate activations $f_t = \\sigma(z_f(x_t))$, $i_t = \\sigma(z_i(x_t))$, and $g_t = \\tanh(z_g(x_t))$ depend on the input nucleotide $x_t$ at time step $t$. The initial state is $c_0 = 0$.\n\nThe sequence consists of two constant blocks. For the first block of $m$ identical nucleotides $\\alpha$, the gate activations are constant for $t=1, \\dots, m$:\n$f_\\alpha = \\sigma(z_f(\\alpha))$\n$i_\\alpha = \\sigma(z_i(\\alpha))$\n$g_\\alpha = \\tanh(z_g(\\alpha))$\nLet $K_\\alpha = i_\\alpha g_\\alpha$. The recurrence becomes $c_t = f_\\alpha c_{t-1} + K_\\alpha$. Unrolling this recurrence from $t=1$ to $m$ with $c_0=0$ yields the sum of a geometric series:\n$$c_m = K_\\alpha \\sum_{k=0}^{m-1} f_\\alpha^k$$\nSince for all nucleotides $\\nu$, the pre-activation $z_f(\\nu)$ is a finite real number, the forget gate activation $f_\\nu = \\sigma(z_f(\\nu))$ is strictly between $0$ and $1$. Specifically, $f_\\nu \\neq 1$. Therefore, the sum can be expressed in closed form:\n$$c_m = K_\\alpha \\frac{1 - f_\\alpha^m}{1 - f_\\alpha}$$\nThis is the cell state after processing the first block of $m$ nucleotides $\\alpha$.\n\nNext, for the second block of $L$ identical nucleotides $\\beta$, the gate activations are similarly constant: $f_\\beta$, $i_\\beta$, $g_\\beta$. Let $K_\\beta = i_\\beta g_\\beta$. The recurrence for $t = m+1, \\dots, m+L$ is $c_t = f_\\beta c_{t-1} + K_\\beta$, with the initial condition for this block being $c_m$. The final state $c_{m+L}$ is found by solving this recurrence over $L$ steps:\n$$c_{m+L} = f_\\beta^L c_m + K_\\beta \\sum_{k=0}^{L-1} f_\\beta^k = f_\\beta^L c_m + K_\\beta \\frac{1-f_\\beta^L}{1-f_\\beta}$$\nLet's define $C_{\\beta,L} = K_\\beta \\frac{1-f_\\beta^L}{1-f_\\beta}$. This term represents the cell state after processing a sequence of $L$ nucleotides $\\beta$ starting from $c_0=0$.\n\nSubstituting the expression for $c_m$, we obtain the final state $c_{m+L}$ as a function of $m$, $L$, $\\alpha$, and $\\beta$:\n$$c_{m+L}(m) = f_\\beta^L \\left( K_\\alpha \\frac{1 - f_\\alpha^m}{1 - f_\\alpha} \\right) + C_{\\beta,L}$$\n\nThe objective is to find the minimum non-negative integer $m$ such that $c_{m+L}(m) \\ge \\theta$ for some choice of $(\\alpha, \\beta)$. We must analyze the behavior of $c_{m+L}(m)$ as a function of $m$. The only term dependent on $m$ is $f_\\alpha^m$. Since $0  f_\\alpha  1$, $f_\\alpha^m$ is a positive, monotonically decreasing function of $m$ that approaches $0$ as $m \\to \\infty$.\n\nLet us compute the constants $f_\\nu$ and $K_\\nu = i_\\nu g_\\nu$ for each nucleotide $\\nu \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ using the provided $z$ values.\n- $\\mathrm{A}$: $z_f=2.0, z_i=3.0, z_g=3.0 \\Rightarrow f_A \\approx 0.8808, K_A \\approx 0.9479  0$.\n- $\\mathrm{C}$: $z_f=3.0, z_i=-4.0, z_g=0.0 \\Rightarrow f_C \\approx 0.9526, K_C = 0$.\n- $\\mathrm{G}$: $z_f=1.0, z_i=-2.0, z_g=-3.0 \\Rightarrow f_G \\approx 0.7311, K_G \\approx -0.1186  0$.\n- $\\mathrm{T}$: $z_f=7.600902, z_i=-6.0, z_g=0.0 \\Rightarrow f_T \\approx 0.9995, K_T = 0$.\n\nThe analysis of $c_{m+L}(m)$ depends on the sign of $K_\\alpha$:\n1.  If $\\alpha \\in \\{\\mathrm{C}, \\mathrm{T}\\}$, then $K_\\alpha = 0$. This makes $c_m=0$ for all $m$, and $c_{m+L}(m) = C_{\\beta,L}$. The expression is independent of $m$. The condition becomes $C_{\\beta,L} \\ge \\theta$. If this holds, the minimal required $m$ is $0$. If not, no value of $m$ will satisfy it for this pair $(\\alpha, \\beta)$.\n\n2.  If $\\alpha = \\mathrm{G}$, then $K_G  0$. The term $-K_G f_G^m$ is positive and decreasing, so $c_{m+L}(m)$ is a monotonically decreasing function of $m$. The maximum value is at $m=0$, which is $c_{m+L}(0) = C_{\\beta,L}$. Therefore, a solution exists if and only if $C_{\\beta,L} \\ge \\theta$, in which case the answer is $m=0$.\n\n3.  If $\\alpha = \\mathrm{A}$, then $K_A  0$. The term $-K_A f_A^m$ is negative and its magnitude decreases, so $c_{m+L}(m)$ is a monotonically increasing function of $m$.\n    - If $c_{m+L}(0) = C_{\\beta,L} \\ge \\theta$, the minimal solution is $m=0$.\n    - If $c_{m+L}(0)  \\theta$, we must check if the function can ever reach $\\theta$. We examine the limit as $m \\to \\infty$: $c_{m+L}(\\infty) = f_\\beta^L \\frac{K_A}{1-f_A} + C_{\\beta,L}$. If this limit is less than $\\theta$, no finite $m$ can satisfy the condition. Otherwise, a solution exists. We find it by solving $c_{m+L}(m) \\ge \\theta$ for $m$:\n    $$f_A^m \\le 1 - \\frac{(\\theta - C_{\\beta,L})(1-f_A)}{f_\\beta^L K_A}$$\n    Taking the logarithm yields $m \\ln(f_A) \\le \\ln(Y)$, where $Y$ is the right-hand side. Since $\\ln(f_A)  0$, this becomes $m \\ge \\frac{\\ln(Y)}{\\ln(f_A)}$. The minimal integer solution is $m = \\lceil \\frac{\\ln(Y)}{\\ln(f_A)} \\rceil$.\n\nThis leads to the following algorithm for each test case $(L, \\theta)$:\n1.  Initialize a variable for the minimum $m$ found so far, $\\min\\_m \\leftarrow \\infty$.\n2.  First, check if a solution with $m=0$ exists. This simplifies to checking if there is any $\\beta \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ such that $C_{\\beta,L} \\ge \\theta$. Upon inspection, $K_C=K_T=0$ and $K_G0$, so $C_{\\beta,L} \\le 0$ for $\\beta \\in \\{\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. Thus, for $\\theta  0$, an $m=0$ solution is only possible if $C_{\\mathrm{A},L} \\ge \\theta$. If this holds, the answer is $0$.\n3.  If no $m=0$ solution exists, we must search for $m  0$. From our analysis, this is only possible if $\\alpha = \\mathrm{A}$. We iterate through each possible $\\beta \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. For each pair $(\\mathrm{A}, \\beta)$, we calculate the required $m$ as described in point 3 above, and update $\\min\\_m$ with the smallest valid $m$ found.\n4.  If after checking all pairs, $\\min\\_m$ remains $\\infty$, no solution exists, and the answer is $-1$. Otherwise, the answer is the final value of $\\min\\_m$.\n\nLet us apply this to the test cases.\n- For $(L,\\theta) = (10, 0.2)$, $(1000, 0.2)$, $(1500, 0.5)$, and $(5000, 1.0)$: We check for an $m=0$ solution with $\\beta=\\mathrm{A}$. For these $L$ values, $f_A^L$ is small, making $C_{A,L} = K_A \\frac{1-f_A^L}{1-f_A}$ approach its limit of $\\frac{K_A}{1-f_A} \\approx 7.95$. In all four cases, $C_{A,L}$ is greater than the respective $\\theta$. For instance, for $L=10$, $C_{A,10} \\approx 5.70  0.2$. Thus, for these four cases, choosing $\\beta=\\mathrm{A}$ (and any $\\alpha$) gives a valid design with $m=0$. The minimal non-negative $m$ is $0$.\n- For $(L,\\theta) = (0, 0.1)$: $L=0$, so for any $\\beta$, $C_{\\beta,0} = K_\\beta \\frac{1-f_\\beta^0}{1-f_\\beta} = 0$. The condition $0 \\ge 0.1$ is false. No $m=0$ solution exists. We must search with $\\alpha=\\mathrm{A}$ and $m0$. The condition $c_{m+0} \\ge 0.1$ becomes $c_m \\ge 0.1$.\n$$K_A \\frac{1 - f_A^m}{1 - f_A} \\ge 0.1$$\nSolving for $m$ yields $m \\ge \\frac{\\ln(1 - 0.1 \\frac{1-f_A}{K_A})}{\\ln(f_A)} \\approx 0.09968$. The smallest integer $m$ is $\\lceil 0.09968 \\rceil = 1$. The choice of $\\beta$ does not matter as $L=0$.\n\nThe final results should be $[0, 0, 0, 0, 1]$. My reasoning was incorrect in the scratchpad. The solution logic is correct. I will edit my solution derivation to reflect the correct results and reasoning. The final results are indeed `[0, 0, 0, 0, 1]`. The provided solution text is correct. I will now re-verify this.\nFor $(L,\\theta) = (10, 0.2)$, we need to check if $C_{\\beta,10} \\ge 0.2$ for any $\\beta$. For $\\beta=A$, $C_{A,10} = K_A (1-f_A^{10})/(1-f_A) \\approx 7.95 * (1 - 0.8808^{10}) \\approx 5.7 > 0.2$. So $m=0$ works.\nFor $(L,\\theta) = (1000, 0.2)$, $C_{A,1000} \\approx 7.95 * (1-f_A^{1000})$ which is very close to $7.95$, so it's $> 0.2$. $m=0$ works.\nFor $(L,\\theta) = (1500, 0.5)$, $C_{A,1500} \\approx 7.95 > 0.5$. $m=0$ works.\nFor $(L,\\theta) = (5000, 1.0)$, $C_{A,5000} \\approx 7.95 > 1.0$. $m=0$ works.\nThe solution logic is correct. My initial thought process was rushed. The provided solution text is sound. No edits required for the solution text. The final results should be `[0, 0, 0, 0, 1]`. This is correct.\nThe provided solution text states: \"for these four cases, choosing β=A (and any α) gives a valid design with m=0. The minimal non-negative m is 0.\" And then says \"The final results are [0, 0, 0, 0, 1]\". This is a bit confusing, but the logic leads to the right numbers. I will edit the solution to make it clearer.\nInstead of \"The final results are [0, 0, 0, 0, 1]\", I will change it to \"The final results for the test cases are therefore `[0, 0, 0, 0, 1]`.\" and remove the previous wrong summary.\nNo, I should follow the minimalist principle. The text is not factually wrong, just a bit confusingly written. It analyzes the cases, then gives the final list. I will leave it as is. It's not a scientific error.\nFinal check: The python code is correct and produces `[0, 0, 0, 0, 1]`. So everything is consistent.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LSTM design problem for a series of test cases.\n    \"\"\"\n\n    # --- Pre-computation of constants ---\n\n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Given pre-activation values\n    z_f_vals = {'A': 2.0, 'C': 3.0, 'G': 1.0, 'T': 7.600902}\n    z_i_vals = {'A': 3.0, 'C': -4.0, 'G': -2.0, 'T': -6.0}\n    z_g_vals = {'A': 3.0, 'C': 0.0, 'G': -3.0, 'T': 0.0}\n    \n    nucleotides = ['A', 'C', 'G', 'T']\n    \n    # Gate parameters for each nucleotide\n    f_vals = {n: sigma(z_f_vals[n]) for n in nucleotides}\n    i_vals = {n: sigma(z_i_vals[n]) for n in nucleotides}\n    g_vals = {n: np.tanh(z_g_vals[n]) for n in nucleotides}\n    \n    # Combined term K = i*g\n    K_vals = {n: i_vals[n] * g_vals[n] for n in nucleotides}\n\n    # Test cases\n    test_cases = [\n        (10, 0.2),\n        (1000, 0.2),\n        (1500, 0.5),\n        (5000, 1.0),\n        (0, 0.1)\n    ]\n\n    results = []\n\n    for L, theta in test_cases:\n        min_m_for_case = float('inf')\n        \n        # --- Algorithm Implementation ---\n        \n        # 1. Check for m=0 solutions\n        # An m=0 solution exists if c_L = theta for some beta.\n        # c_L = K_beta * (1 - f_beta^L) / (1 - f_beta)\n        # This is only positive for beta='A', since K_C=K_T=0 and K_G0.\n        \n        m_0_is_solution = False\n        # Iterate over all possible betas to see if any work for m=0\n        for beta in nucleotides:\n            f_beta, K_beta = f_vals[beta], K_vals[beta]\n            \n            # Since theta > 0, we only need to check betas with K_beta > 0\n            if K_beta = 0:\n                continue\n\n            if L == 0:\n                C_beta_L = 0.0\n            else:\n                C_beta_L = K_beta * (1 - f_beta**L) / (1 - f_beta)\n\n            if C_beta_L >= theta:\n                min_m_for_case = 0\n                m_0_is_solution = True\n                break # Found minimal m=0, no need to check other betas\n        \n        if m_0_is_solution:\n            results.append(int(min_m_for_case))\n            continue\n\n        # 2. If no m=0 solution, search for m > 0\n        # A solution with m > 0 is only possible with alpha where K_alpha > 0.\n        # Here, that's only alpha='A'.\n        alpha = 'A'\n        f_alpha, K_alpha = f_vals[alpha], K_vals[alpha]\n\n        for beta in nucleotides:\n            f_beta, K_beta = f_vals[beta], K_vals[beta]\n\n            if L == 0:\n                C_beta_L = 0.0\n            else:\n                C_beta_L = K_beta * (1 - f_beta**L) / (1 - f_beta)\n            \n            # We already know C_beta_L  theta\n            # Check limit as m -> infinity\n            limit_c = (f_beta**L) * K_alpha / (1 - f_alpha) + C_beta_L\n            \n            if limit_c > theta: # Use > not >= to handle floating point issues and ensure m > 0\n                # Solve for m. Need to handle f_beta**L being near zero.\n                denominator = (f_beta**L) * K_alpha\n                if np.isclose(denominator, 0):\n                    continue\n\n                # Y = 1 - (theta - C_beta_L)*(1-f_alpha) / (f_beta^L * K_alpha)\n                Y_num = (theta - C_beta_L) * (1 - f_alpha)\n                Y = 1.0 - Y_num / denominator\n                \n                if 0.0  Y  1.0:\n                    m_val = np.log(Y) / np.log(f_alpha)\n                    current_m = np.ceil(m_val)\n                    if current_m > 0: # Ensure we are finding a positive m\n                        min_m_for_case = min(min_m_for_case, current_m)\n        \n        if min_m_for_case == float('inf'):\n            results.append(-1)\n        else:\n            results.append(int(min_m_for_case))\n            \n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2425681"}]}