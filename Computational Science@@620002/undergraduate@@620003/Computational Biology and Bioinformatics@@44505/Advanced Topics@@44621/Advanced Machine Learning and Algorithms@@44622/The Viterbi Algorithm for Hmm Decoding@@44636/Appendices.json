{"hands_on_practices": [{"introduction": "To truly master an algorithm, it's essential to first work through it by hand. This exercise guides you through a manual Viterbi decoding for a profile Hidden Markov Model, a powerful tool for representing protein families [@problem_id:2436882]. By filling the dynamic programming table and tracing backpointers, you will gain a concrete understanding of how the most probable path, and its corresponding sequence alignment, is discovered.", "problem": "A profile Hidden Markov Model (HMM) for a protein family has length $2$ with the standard profile topology consisting of Begin ($B$), Match ($M_{1}, M_{2}$), Insert ($I_{0}, I_{1}, I_{2}$), Delete ($D_{1}, D_{2}$), and End ($E$) states. The amino-acid alphabet is $\\{A, C, D\\}$. Delete states emit nothing. Insert and match states emit according to the probabilities specified below. All transition probabilities not explicitly listed are $0$.\n\nNonzero transition probabilities:\n- $B \\to M_{1}: \\; \\frac{3}{4}$, $B \\to D_{1}: \\; \\frac{1}{4}$, $B \\to I_{0}: \\; 0$.\n- $I_{0} \\to I_{0}: \\; \\frac{1}{2}$, $I_{0} \\to M_{1}: \\; \\frac{1}{2}$, $I_{0} \\to D_{1}: \\; 0$.\n- $M_{1} \\to M_{2}: \\; \\frac{4}{5}$, $M_{1} \\to I_{1}: \\; \\frac{1}{10}$, $M_{1} \\to D_{2}: \\; \\frac{1}{10}$.\n- $D_{1} \\to M_{2}: \\; \\frac{1}{2}$, $D_{1} \\to D_{2}: \\; \\frac{1}{2}$, $D_{1} \\to I_{1}: \\; 0$.\n- $I_{1} \\to I_{1}: \\; \\frac{1}{2}$, $I_{1} \\to M_{2}: \\; \\frac{1}{2}$, $I_{1} \\to D_{2}: \\; 0$.\n- $M_{2} \\to E: \\; \\frac{9}{10}$, $M_{2} \\to I_{2}: \\; \\frac{1}{10}$.\n- $D_{2} \\to E: \\; 1$.\n- $I_{2} \\to I_{2}: \\; \\frac{1}{2}$, $I_{2} \\to E: \\; \\frac{1}{2}$.\n\nEmission probabilities:\n- In $M_{1}$: $P(A)=\\frac{1}{2}$, $P(C)=\\frac{1}{3}$, $P(D)=\\frac{1}{6}$.\n- In $M_{2}$: $P(A)=\\frac{1}{5}$, $P(C)=\\frac{1}{5}$, $P(D)=\\frac{3}{5}$.\n- In $I_{0}, I_{1}, I_{2}$: uniform, $P(A)=P(C)=P(D)=\\frac{1}{3}$.\n\nConsider the protein sequence $X = A\\,D\\,A$. Apply Viterbi decoding to determine the single most probable state path through the model that emits $X$ and the corresponding gapped alignment of $X$ to the modelâ€™s two match columns. Then compute the natural logarithm $\\ln$ of the probability of that Viterbi path.\n\nGive only the numerical value of $\\ln(\\text{Viterbi path probability})$ as your final answer. Round your answer to four significant figures.", "solution": "The problem requires finding the most probable state path (the Viterbi path) for a given sequence $X = A\\,D\\,A$ through a specified profile Hidden Markov Model (HMM), and then computing the natural logarithm of this path's probability. The solution will be derived using the Viterbi algorithm, a dynamic programming approach for this task.\n\nLet the given sequence be $X = x_1x_2x_3$, where $x_1=A$, $x_2=D$, and $x_3=A$. The length of the sequence is $N=3$. The HMM has a length of $L=2$. The states of the model are the Begin state $B$, Match states $M_1, M_2$, Insertion states $I_0, I_1, I_2$, Deletion states $D_1, D_2$, and the End state $E$.\n\nThe core of the Viterbi algorithm is the computation of $v_k(i)$, the probability of the most probable path that generates the prefix of the sequence of length $i$, $x_1 \\dots x_i$, and ends in state $k$. We will also maintain backpointers to reconstruct the path.\n\nThe recurrence relations for a profile HMM are as follows, where $a_{k,l}$ is the transition probability from state $k$ to state $l$, and $e_k(c)$ is the emission probability of character $c$ from state $k$:\nFor Match states $M_j$, $j \\in \\{1, 2\\}$:\n$$v_{M_j}(i) = e_{M_j}(x_i) \\cdot \\max \\begin{cases} v_{M_{j-1}}(i-1) \\cdot a_{M_{j-1}, M_j} \\\\ v_{I_{j-1}}(i-1) \\cdot a_{I_{j-1}, M_j} \\\\ v_{D_{j-1}}(i-1) \\cdot a_{D_{j-1}, M_j} \\end{cases}$$\nFor Insertion states $I_j$, $j \\in \\{0, 1, 2\\}$:\n$$v_{I_j}(i) = e_{I_j}(x_i) \\cdot \\max \\begin{cases} v_{M_j}(i-1) \\cdot a_{M_j, I_j} \\\\ v_{I_j}(i-1) \\cdot a_{I_j, I_j} \\end{cases}$$\nFor Deletion states $D_j$, $j \\in \\{1, 2\\}$:\n$$v_{D_j}(i) = \\max \\begin{cases} v_{M_{j-1}}(i) \\cdot a_{M_{j-1}, D_j} \\\\ v_{D_{j-1}}(i) \\cdot a_{D_{j-1}, D_j} \\end{cases}$$\nThe indices $j-1$ for $M$ and $D$ states are handled by considering the Begin state $B$ for $j=1$. For $I_j$, the previous states are $M_j$ or $I_j$ itself. The state $I_0$ is a special case.\n\nA critical observation is that the state $I_0$ is unreachable. The only specified transition into $I_0$ is from itself ($I_0 \\to I_0$). The transition from the Begin state, $B \\to I_0$, has a probability of $0$. No other state transitions into $I_0$. Consequently, any path involving state $I_0$ has a probability of $0$, and we may disregard this state.\n\nWe now construct the dynamic programming table for $v_k(i)$.\n\n**Initialization ($i=0$):**\nThis step corresponds to paths that do not emit any characters. We start at the Begin state $B$ with probability $1$. Paths can then move to non-emitting Delete states.\n$v_{D_1}(0) = a_{B,D_1} = \\frac{1}{4}$. The backpointer is to $B$.\n$v_{D_2}(0) = v_{D_1}(0) \\cdot a_{D_1,D_2} = \\frac{1}{4} \\cdot \\frac{1}{2} = \\frac{1}{8}$. The backpointer is to $D_1(0)$.\nAll other states $v_k(0)$ are $0$, as they are emitting states.\n\n**Iteration 1 ($i=1$, $x_1=A$):**\nWe compute the probabilities for paths emitting the first character, $A$.\n$v_{M_1}(1) = e_{M_1}(A) \\cdot a_{B,M_1} = \\frac{1}{2} \\cdot \\frac{3}{4} = \\frac{3}{8}$. Backpointer: $B$.\n$v_{M_2}(1) = e_{M_2}(A) \\cdot (v_{D_1}(0) \\cdot a_{D_1,M_2}) = \\frac{1}{5} \\cdot (\\frac{1}{4} \\cdot \\frac{1}{2}) = \\frac{1}{40}$. Backpointer: $D_1(0)$.\nThe probabilities for insertion states are $0$, as they must be entered from a state at a previous step ($i=0$), and no emitting state has a non-zero probability at $i=0$.\n$v_{I_1}(1) = 0$, $v_{I_2}(1) = 0$.\nThe probability for ending in a Delete state after emitting one character:\n$v_{D_2}(1) = v_{M_1}(1) \\cdot a_{M_1,D_2} = \\frac{3}{8} \\cdot \\frac{1}{10} = \\frac{3}{80}$. Backpointer: $M_1(1)$.\n$v_{D_1}$ is only reachable from B, which involves no emissions, so $v_{D_1}(i)=0$ for $i > 0$.\n\n**Iteration 2 ($i=2$, $x_2=D$):**\nWe compute probabilities for paths emitting the first two characters, $A,D$.\n$v_{M_1}(2) = 0$, since $M_1$ can only be reached from $I_0$ after the first character, and $I_0$ is unreachable.\n$v_{I_1}(2) = e_{I_1}(D) \\cdot \\max \\{ v_{M_1}(1) \\cdot a_{M_1,I_1}, v_{I_1}(1) \\cdot a_{I_1,I_1} \\} = \\frac{1}{3} \\cdot \\max\\{\\frac{3}{8} \\cdot \\frac{1}{10}, 0 \\} = \\frac{1}{80}$. Backpointer: $M_1(1)$.\n$v_{M_2}(2) = e_{M_2}(D) \\cdot \\max \\{ v_{M_1}(1) \\cdot a_{M_1,M_2}, v_{I_1}(1) \\cdot a_{I_1,M_2} \\} = \\frac{3}{5} \\cdot \\max \\{ \\frac{3}{8} \\cdot \\frac{4}{5}, 0 \\} = \\frac{3}{5} \\cdot \\frac{3}{10} = \\frac{9}{50}$. Backpointer: $M_1(1)$.\n$v_{I_2}(2) = e_{I_2}(D) \\cdot \\max \\{ v_{M_2}(1) \\cdot a_{M_2,I_2}, v_{I_2}(1) \\cdot a_{I_2,I_2} \\} = \\frac{1}{3} \\cdot \\max \\{ \\frac{1}{40} \\cdot \\frac{1}{10}, 0 \\} = \\frac{1}{1200}$. Backpointer: $M_2(1)$.\nFor Delete states at $i=2$: $v_{D_1}(2)=0$, $v_{D_2}(2)=0$.\n\n**Iteration 3 ($i=3$, $x_3=A$):**\nWe compute probabilities for paths emitting the full sequence $A,D,A$.\n$v_{M_1}(3) = 0$.\n$v_{I_1}(3) = e_{I_1}(A) \\cdot (v_{I_1}(2) \\cdot a_{I_1,I_1}) = \\frac{1}{3} \\cdot (\\frac{1}{80} \\cdot \\frac{1}{2}) = \\frac{1}{480}$. Backpointer: $I_1(2)$.\n$v_{M_2}(3) = e_{M_2}(A) \\cdot (v_{I_1}(2) \\cdot a_{I_1,M_2}) = \\frac{1}{5} \\cdot (\\frac{1}{80} \\cdot \\frac{1}{2}) = \\frac{1}{800}$. Backpointer: $I_1(2)$.\n$v_{I_2}(3) = e_{I_2}(A) \\cdot \\max \\{ v_{M_2}(2) \\cdot a_{M_2,I_2}, v_{I_2}(2) \\cdot a_{I_2,I_2} \\} = \\frac{1}{3} \\cdot \\max\\{\\frac{9}{50} \\cdot \\frac{1}{10}, \\frac{1}{1200} \\cdot \\frac{1}{2}\\} = \\frac{1}{3} \\cdot \\max\\{\\frac{9}{500}, \\frac{1}{2400}\\} = \\frac{1}{3} \\cdot \\frac{9}{500} = \\frac{3}{500}$. Backpointer: $M_2(2)$.\nFor Delete states at $i=3$: $v_{D_1}(3)=0$, $v_{D_2}(3)=0$.\n\n**Termination:**\nThe probability of the most probable path, $P^*$, is found by considering the transition from the final states at $i=3$ to the End state $E$.\n$$P^* = \\max \\begin{cases} v_{M_2}(3) \\cdot a_{M_2,E} \\\\ v_{D_2}(3) \\cdot a_{D_2,E} \\\\ v_{I_2}(3) \\cdot a_{I_2,E} \\end{cases}$$\n$v_{D_2}(3) = 0$, so we compare the other two terms.\nPath ending in $M_2$: $v_{M_2}(3) \\cdot a_{M_2,E} = \\frac{1}{800} \\cdot \\frac{9}{10} = \\frac{9}{8000}$.\nPath ending in $I_2$: $v_{I_2}(3) \\cdot a_{I_2,E} = \\frac{3}{500} \\cdot \\frac{1}{2} = \\frac{3}{1000} = \\frac{24}{8000}$.\nThe maximum probability is $\\frac{3}{1000}$, achieved by the path ending in state $I_2$.\nThe Viterbi path probability is $P^* = \\frac{3}{1000}$.\n\nBy tracing the backpointers from state $I_2$ at $i=3$:\n- The state at $i=3$ is $I_2$, which came from $M_2$ at $i=2$.\n- The state at $i=2$ is $M_2$, which came from $M_1$ at $i=1$.\n- The state at $i=1$ is $M_1$, which came from the Begin state $B$.\nThe single most probable state path is $\\pi^* = (B, M_1, M_2, I_2, E)$.\nThis path emits $x_1=A$ from $M_1$, $x_2=D$ from $M_2$, and $x_3=A$ from $I_2$.\nThe gapped alignment of the sequence $X$ to the model's match columns is:\nModel columns: $1, 2$\nSequence:      $A, D, A$\nThe sequence character $A$ aligns to match column $1$, $D$ aligns to match column $2$, and the final $A$ corresponds to an insertion after column $2$.\n\nThe problem asks for the natural logarithm of the Viterbi path probability, rounded to four significant figures.\n$P^* = \\frac{3}{1000} = 0.003$.\n$$\\ln(P^*) = \\ln(0.003) = \\ln(3) - \\ln(1000) \\approx 1.098612 - 6.907755 \\approx -5.809143$$\nRounding this value to four significant figures yields $-5.809$.", "answer": "$$\\boxed{-5.809}$$", "id": "2436882"}, {"introduction": "Moving from theory to practice often means confronting imperfect data. This hands-on coding problem challenges you to adapt the Viterbi algorithm to handle sequences with missing observations, a common scenario in genomics [@problem_id:2436943]. You will implement the statistically sound approach of marginalizing out the missing information, learning how the algorithm's emission-handling step can be modified to robustly decode incomplete data.", "problem": "You are given a discrete Hidden Markov Model (HMM) with a finite state-space and a finite emission alphabet, as commonly used in computational biology for sequence annotation tasks such as gene finding. The HMM is defined by the following components: an initial state distribution $\\boldsymbol{\\pi}$ over states, a state transition matrix $\\mathbf{A}$, and an emission probability matrix $\\mathbf{B}$. The state at time $t$ is $S_t \\in \\{0,1,\\dots,N-1\\}$ and the observed symbol at time $t$ is $Y_t \\in \\{0,1,\\dots,M-1\\}$. The joint distribution factorizes by the Markov and conditional independence assumptions as\n$$\nP(S_{1:T}, Y_{1:T}) \\;=\\; \\pi_{S_1}\\,\\prod_{t=2}^{T} A_{S_{t-1},S_t} \\;\\prod_{t=1}^{T} B_{S_t, Y_t},\n$$\nwhere $T$ is the sequence length. In many biological settings, some observations are missing, for example due to low coverage or ambiguous base calls. We denote missing observations by the sentinel value $-1$ and, for such time points, define the emission contribution by marginalizing over all possible symbols:\n$$\n\\text{if } Y_t = -1,\\quad \\text{use the factor } \\sum_{x=0}^{M-1} B_{S_t, x}.\n$$\n\nYour task is to implement a program that performs decoding by choosing a most probable state path $\\hat{s}_{1:T}$ under the Maximum A Posteriori (MAP) criterion,\n$$\n\\hat{s}_{1:T} \\in \\arg\\max_{s_{1:T}} P(S_{1:T} = s_{1:T} \\mid Y_{1:T} = y_{1:T}),\n$$\nwhich is equivalent to maximizing the joint probability $P(S_{1:T} = s_{1:T}, Y_{1:T} = y_{1:T})$ because the marginal likelihood of the observations is constant with respect to the path. Your implementation must use dynamic programming in log-space to ensure numerical stability, and it must handle missing observations at specified time points by correctly marginalizing over the emission alphabet for those times. All logarithms must be natural logarithms.\n\nUse the following concrete HMM and test suite. The alphabet is deoxyribonucleic acid (DNA) nucleotides mapped to integers by $\\{0 \\mapsto \\text{A},\\, 1 \\mapsto \\text{C},\\, 2 \\mapsto \\text{G},\\, 3 \\mapsto \\text{T}\\}$. The model has $N=2$ states labeled $0$ and $1$, with initial distribution, transitions, and emissions given by:\n- Initial distribution $\\boldsymbol{\\pi} = [\\,0.6,\\, 0.4\\,]$.\n- Transition matrix\n$$\n\\mathbf{A} \\;=\\;\n\\begin{bmatrix}\n0.9 & 0.1 \\\\\n0.2 & 0.8\n\\end{bmatrix}.\n$$\n- Emission matrix\n$$\n\\mathbf{B} \\;=\\;\n\\begin{bmatrix}\n0.30 & 0.20 & 0.20 & 0.30 \\\\\n0.15 & 0.35 & 0.35 & 0.15\n\\end{bmatrix},\n$$\nwhere row $i$ gives $P(Y_t = x \\mid S_t = i)$ for $x \\in \\{0,1,2,3\\}$.\n\nImplement the decoding algorithm that:\n- Maximizes the posterior over state paths using dynamic programming based solely on the HMM definitions and independence assumptions stated above.\n- Works in log-space.\n- For an observed symbol $y_t \\in \\{0,1,2,3\\}$, uses the emission log-factor $\\log B_{i,y_t}$.\n- For a missing symbol $y_t = -1$, uses the emission log-factor $\\log\\left(\\sum_{x=0}^{3} B_{i,x}\\right)$ for each state $i$, which corresponds to marginalizing over the unobserved symbol.\n\nYour program must apply this decoder to the following test suite of observation sequences (each sequence is a list of integers, with $-1$ denoting a missing observation):\n- Test $1$ (happy path, no missing): $[\\,0,\\,1,\\,2,\\,3,\\,1\\,]$.\n- Test $2$ (intermittent missing): $[\\,0,\\, -1,\\, 2,\\, -1,\\, 3\\,]$.\n- Test $3$ (all missing): $[\\, -1,\\, -1,\\, -1,\\, -1\\,]$.\n- Test $4$ (boundary case, single missing): $[\\, -1\\,]$.\n- Test $5$ (boundary case, single observed): $[\\, 3\\,]$.\n\nThe required output for each test case is the decoded most likely state sequence as a list of integers of length $T$. Your program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets, where each element is itself a decoded path list. For example, the output format must be exactly like:\n[ [s11,s12,...,s1T1], [s21,s22,...], [s31,...], [s41], [s51] ]\nwhere each $s_{ij}$ is an integer state index. There must be no additional text printed. No physical units, angles, or percentages are involved in this problem, so none should be included in the output. The program must be entirely self-contained and must not read any input.", "solution": "The problem is first subjected to validation.\n\nStep 1: Extract Givens\n-   A Hidden Markov Model (HMM) with $N$ states $S_t \\in \\{0, 1, \\dots, N-1\\}$ and $M$ emission symbols $Y_t \\in \\{0, 1, \\dots, M-1\\}$.\n-   Initial state distribution: $\\boldsymbol{\\pi}$, an $N$-element vector.\n-   State transition matrix: $\\mathbf{A}$, an $N \\times N$ matrix where $A_{ij} = P(S_t = j \\mid S_{t-1} = i)$.\n-   Emission probability matrix: $\\mathbf{B}$, an $N \\times M$ matrix where $B_{ix} = P(Y_t = x \\mid S_t = i)$.\n-   Joint probability distribution: $P(S_{1:T}, Y_{1:T}) = \\pi_{S_1} \\prod_{t=2}^{T} A_{S_{t-1},S_t} \\prod_{t=1}^{T} B_{S_t, Y_t}$.\n-   Missing observations: Represented by the value $-1$.\n-   Handling of missing observations: For $Y_t = -1$, the emission factor is $\\sum_{x=0}^{M-1} B_{S_t, x}$.\n-   Task: Find the most probable state path $\\hat{s}_{1:T}$ by maximizing the posterior probability $P(S_{1:T} \\mid Y_{1:T})$, which is equivalent to maximizing the joint probability $P(S_{1:T}, Y_{1:T})$.\n-   Constraints: The implementation must use dynamic programming in log-space (natural logarithm).\n-   Log-factors for emissions: For an observed symbol $y_t$, use $\\log B_{i,y_t}$. For a missing symbol $y_t = -1$, use $\\log\\left(\\sum_{x=0}^{M-1} B_{i,x}\\right)$.\n-   HMM parameters:\n    -   $N=2$ states, $M=4$ symbols (A, C, G, T mapped to $0, 1, 2, 3$).\n    -   $\\boldsymbol{\\pi} = [\\,0.6,\\, 0.4\\,]$.\n    -   $\\mathbf{A} = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.2 & 0.8 \\end{bmatrix}$.\n    -   $\\mathbf{B} = \\begin{bmatrix} 0.30 & 0.20 & 0.20 & 0.30 \\\\ 0.15 & 0.35 & 0.35 & 0.15 \\end{bmatrix}$.\n-   Test cases:\n    1.  $[\\,0,\\,1,\\,2,\\,3,\\,1\\,]$\n    2.  $[\\,0,\\, -1,\\, 2,\\, -1,\\, 3\\,]$\n    3.  $[\\, -1,\\, -1,\\, -1,\\, -1\\,]$\n    4.  $[\\, -1\\,]$\n    5.  $[\\, 3\\,]$\n-   Output format: A single line string representing a list of lists of integers, e.g., `[ [s11,...], [s21,...] ]`.\n\nStep 2: Validate Using Extracted Givens\nThe problem statement is analyzed for validity.\n-   **Scientifically Grounded**: The problem is based on the standard Hidden Markov Model framework and the Viterbi algorithm for decoding. This is a fundamental and widely used methodology in computational biology, bioinformatics, and signal processing. The specified method for handling missing data by marginalizing over possible emissions is a statistically sound and standard approach. The problem is scientifically valid.\n-   **Well-Posed**: The problem asks for the single most probable state path. The Viterbi algorithm is a deterministic procedure guaranteed to find such a path. All necessary parameters ($\\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B}$) and conditions are provided, making the problem self-contained and solvable.\n-   **Objective**: The problem is stated in precise mathematical and algorithmic terms, free of any subjective or ambiguous language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or infeasibility. The parameters are valid probabilities, and the dimensions are consistent.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe task is to find the most likely sequence of hidden states, $\\hat{s}_{1:T} = (\\hat{s}_1, \\dots, \\hat{s}_T)$, given a sequence of observations $y_{1:T} = (y_1, \\dots, y_T)$. This is accomplished by maximizing the joint probability $P(s_{1:T}, y_{1:T})$. Due to the product form of this probability, direct computation is prone to numerical underflow for long sequences. Therefore, we work with log-probabilities. The objective becomes:\n$$\n\\hat{s}_{1:T} = \\arg\\max_{s_{1:T}} \\left( \\log \\pi_{s_1} + \\sum_{t=2}^{T} \\log A_{s_{t-1},s_t} + \\sum_{t=1}^{T} \\log P(y_t \\mid s_t) \\right)\n$$\nThis optimization problem has an optimal substructure and is solved efficiently using dynamic programming, specifically the Viterbi algorithm. We define a recursive quantity, $\\delta_t(i)$, as the maximum log-probability of any state path of length $t$ that ends in state $i$, given the first $t$ observations:\n$$\n\\delta_t(i) = \\max_{s_{1:t-1}} \\log P(s_1, \\dots, s_{t-1}, s_t=i, y_1, \\dots, y_t)\n$$\nTo enable reconstruction of the path, we also store backpointers in a table $\\psi_t(i)$, which records the most likely predecessor state for a path ending in state $i$ at time $t$.\n\nThe algorithm proceeds in three stages: initialization, recursion, and traceback.\n\n**1. Initialization ($t=1$)**\nFor each state $i \\in \\{0, \\dots, N-1\\}$, the initial log-probability is the sum of the log of the initial probability $\\pi_i$ and the log-probability of the first observation $y_1$ given state $i$.\n$$\n\\delta_1(i) = \\log \\pi_i + \\log P(y_1 \\mid s_1=i)\n$$\nThe observation log-probability term $\\log P(y_t \\mid s_t=i)$ is defined as:\n$$\n\\log P(y_t \\mid s_t=i) =\n\\begin{cases}\n    \\log B_{i, y_t} & \\text{if } y_t \\neq -1 \\\\\n    \\log \\left( \\sum_{x=0}^{M-1} B_{i,x} \\right) & \\text{if } y_t = -1\n\\end{cases}\n$$\nFor the given emission matrix $\\mathbf{B}$, each row sums to $1.0$, as it is a valid probability distribution. Thus, for a missing observation ($y_t = -1$), the term $\\sum_{x=0}^{M-1} B_{i,x} = 1$ for any state $i$, and its logarithm is $\\log(1) = 0$. This means a missing observation provides no evidence to favor one state over another, and the decision relies solely on the transition and initial probabilities.\n\nThe backpointer table $\\psi_1(i)$ does not need to be set, as it is the beginning of the path.\n\n**2. Recursion ($t=2, \\dots, T$)**\nFor each subsequent time step $t$ and for each state $j \\in \\{0, \\dots, N-1\\}$, we compute $\\delta_t(j)$ by extending the most likely paths from time $t-1$. The path to state $j$ at time $t$ must have come from some state $i$ at time $t-1$. We maximize over all possible predecessor states $i$:\n$$\n\\delta_t(j) = \\left( \\max_{i=0,\\dots,N-1} \\left( \\delta_{t-1}(i) + \\log A_{ij} \\right) \\right) + \\log P(y_t \\mid s_t=j)\n$$\nThe backpointer for state $j$ at time $t$ stores the predecessor state $i$ that achieved this maximum:\n$$\n\\psi_t(j) = \\arg\\max_{i=0,\\dots,N-1} \\left( \\delta_{t-1}(i) + \\log A_{ij} \\right)\n$$\nThis process is repeated for all time steps up to $T$, filling the $\\delta$ and $\\psi$ tables, which are of size $T \\times N$.\n\n**3. Termination and Path Traceback**\nAfter the forward pass is complete, the log-probability of the single most likely path is the maximum value in the final column of the $\\delta$ table:\n$$\n\\log P(\\hat{s}_{1:T}, y_{1:T}) = \\max_{i=0,\\dots,N-1} \\delta_T(i)\n$$\nThe final state of the optimal path is the state that achieves this maximum:\n$$\n\\hat{s}_T = \\arg\\max_{i=0,\\dots,N-1} \\delta_T(i)\n$$\nThe remainder of the path is recovered by backtracking from this final state using the stored pointers in the $\\psi$ table:\n$$\n\\hat{s}_{t-1} = \\psi_t(\\hat{s}_t) \\quad \\text{for } t = T, T-1, \\dots, 2\n$$\nThis procedure yields the complete most likely state sequence $\\hat{s}_{1:T}$. The implementation will apply this algorithm to each provided test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the HMM decoding problem for the specified test cases.\n    \"\"\"\n\n    # Define HMM parameters as per the problem statement.\n    pi = np.array([0.6, 0.4])\n    A = np.array([\n        [0.9, 0.1],\n        [0.2, 0.8]\n    ])\n    B = np.array([\n        [0.30, 0.20, 0.20, 0.30],\n        [0.15, 0.35, 0.35, 0.15]\n    ])\n\n    # Define the test suite of observation sequences.\n    test_cases = [\n        [0, 1, 2, 3, 1],\n        [0, -1, 2, -1, 3],\n        [-1, -1, -1, -1],\n        [-1],\n        [3],\n    ]\n\n    def viterbi_log_decode(y_seq, pi, A, B):\n        \"\"\"\n        Performs Viterbi decoding in log-space for an HMM, handling missing observations.\n\n        Args:\n            y_seq (list[int]): Sequence of observations. -1 denotes a missing observation.\n            pi (np.ndarray): Initial state distribution.\n            A (np.ndarray): State transition matrix.\n            B (np.ndarray): Emission matrix.\n\n        Returns:\n            list[int]: The most likely sequence of hidden states.\n        \"\"\"\n        T = len(y_seq)\n        if T == 0:\n            return []\n        \n        N, M = B.shape\n\n        # Precompute log probabilities to work in log-space for numerical stability.\n        # Add a small epsilon to avoid log(0) if any probabilities were zero.\n        # Although not needed for this problem's parameters.\n        epsilon = 1e-10\n        log_pi = np.log(pi + epsilon)\n        log_A = np.log(A + epsilon)\n        log_B = np.log(B + epsilon)\n\n        # For missing observations, the factor is sum(P(y|s)) over all y.\n        # Since B rows sum to 1, this is log(1) = 0.\n        log_B_missing = np.log(np.sum(B, axis=1))\n\n        # Initialize DP tables\n        # delta[t, i]: max log-probability of a path of length t ending in state i\n        delta = np.zeros((T, N))\n        # psi[t, i]: most likely predecessor state for a path of length t to state i\n        psi = np.zeros((T, N), dtype=int)\n\n        # Initialization step (t=0)\n        y_0 = y_seq[0]\n        log_emissions_0 = log_B[:, y_0] if y_0 != -1 else log_B_missing\n        delta[0, :] = log_pi + log_emissions_0\n\n        # Recursion step (t=1..T-1)\n        for t in range(1, T):\n            y_t = y_seq[t]\n            log_emissions_t = log_B[:, y_t] if y_t != -1 else log_B_missing\n            \n            # Vectorized computation of scores for all transitions\n            # scores[i, j] = delta[t-1, i] + log_A[i, j]\n            scores = delta[t-1, :, np.newaxis] + log_A\n\n            delta[t, :] = np.max(scores, axis=0) + log_emissions_t\n            psi[t, :] = np.argmax(scores, axis=0)\n\n        # Termination and backtracking\n        path = np.zeros(T, dtype=int)\n        path[T-1] = np.argmax(delta[T-1, :])\n\n        for t in range(T-2, -1, -1):\n            path[t] = psi[t+1, path[t+1]]\n            \n        return path.tolist()\n\n    results = []\n    for y_seq in test_cases:\n        decoded_path = viterbi_log_decode(y_seq, pi, A, B)\n        results.append(decoded_path)\n    \n    # Format the output as specified: list of lists, with spaces after commas.\n    print(f\"[{', '.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2436943"}, {"introduction": "When decoding long sequences, the joint probabilities can become vanishingly small, leading to numerical underflow even in log-space. This advanced implementation practice delves into the critical issue of numerical stability in Viterbi decoding [@problem_id:2436975]. By comparing a standard max-sum decoder with numerically unstable and stable variants, you will uncover the hidden pitfalls of floating-point arithmetic and the importance of designing robust algorithms for real-world bioinformatics.", "problem": "You will design and analyze the Viterbi algorithm for Hidden Markov Model (HMM) decoding through a set of controlled numerical experiments. A Hidden Markov Model (HMM) consists of a finite set of hidden states, a transition matrix $\\mathbf{A}$ whose entry $A_{ij}$ is the probability of transitioning from hidden state $i$ to hidden state $j$, an initial state distribution $\\boldsymbol{\\pi}$ with entry $\\pi_i$ giving the probability of starting in hidden state $i$, and an emission distribution $\\mathbf{B}$ where $B_i(x)$ is the probability of emitting observation symbol $x$ when in hidden state $i$. The Viterbi algorithm computes the maximum a posteriori (MAP) hidden state path $\\hat{\\mathbf{z}} = \\arg\\max_{\\mathbf{z}} \\Pr(\\mathbf{z} \\mid \\mathbf{x})$ for a given observation sequence $\\mathbf{x}$, using dynamic programming over the factorization of the joint distribution $\\Pr(\\mathbf{z}, \\mathbf{x}) = \\pi_{z_0} \\prod_{t=1}^{T-1} A_{z_{t-1}, z_t} \\prod_{t=0}^{T-1} B_{z_t}(x_t)$.\n\nYour task is to implement three decoders and demonstrate how numerical underflow or precision loss can arise when one replaces the max-operator in the Viterbi recursion by a log-sum-exp (LSE) combination that exponentiates large negative log-probabilities. You must implement:\n\n- A correct max-sum Viterbi decoder in log-domain arithmetic that uses $\\max$ over predecessor states in its dynamic program.\n- An unstable log-sum-exp variant that, at each time step for each destination state, computes the predecessor combination using the direct expression $\\operatorname{LSE}_{\\text{unstable}}(\\mathbf{x}) = \\log\\left(\\sum_i e^{x_i}\\right)$ on a vector of candidate log-scores $\\mathbf{x}$, without any stabilization trick, and then adds the log-emission. This variant is known to underflow when any $x_i$ are much smaller than $0$ because $e^{x_i}$ can become numerically zero in double precision.\n- A stabilized log-sum-exp variant that uses the identity $\\operatorname{LSE}_{\\text{stable}}(\\mathbf{x}) = m + \\log\\left(\\sum_i e^{x_i - m}\\right)$, where $m = \\max_i x_i$, to avoid exponentiation of very large negative numbers. In all variants, when a tie occurs in an $\\arg\\max$, break ties by choosing the smallest state index.\n\nAll decoders operate in log space. For the log-sum-exp variants, use the $\\operatorname{LSE}$ combination only to produce the scalar predecessor-combination score that is then added to the log-emission; for the backpointer, always select the predecessor with the largest candidate score (choose the smallest index in case of ties). This makes the stabilized log-sum-exp variant agree with the max-sum decoder on backpointers whenever the same $\\arg\\max$ is selected.\n\nFundamental base to use: start from the standard HMM factorization above and the definition of the MAP path. Derive a dynamic program in log space by applying $\\log$ to products, and reason about floating-point arithmetic in base-$2$ IEEE $754$ double precision, where $\\exp(y)$ underflows to zero for sufficiently negative $y$ (roughly $y \\lesssim -745$).\n\nDefine the alphabet $\\Sigma = \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$. The following test suite specifies three HMMs and observation sequences. Your program must hard-code these and produce a single aggregated result as specified below.\n\nTest Suite (each test includes the number of states $N$, initial distribution $\\boldsymbol{\\pi}$, transition matrix $\\mathbf{A}$, emission probabilities $\\mathbf{B}$ over $\\Sigma$, and an observation sequence $\\mathbf{x}$):\n\n- Test $1$ (happy path where all methods should agree on the path and no underflow occurs):\n  - $N = 2$,\n  - $\\boldsymbol{\\pi} = [\\,0.5,\\,0.5\\,]$,\n  - $\\mathbf{A} = \\begin{bmatrix} 0.995 & 0.005 \\\\ 0.005 & 0.995 \\end{bmatrix}$,\n  - $\\mathbf{B}$:\n    - state $0$: $B_0(\\text{A}) = 0.97$, $B_0(\\text{C}) = 0.01$, $B_0(\\text{G}) = 0.01$, $B_0(\\text{T}) = 0.01$,\n    - state $1$: $B_1(\\text{A}) = 0.01$, $B_1(\\text{C}) = 0.01$, $B_1(\\text{G}) = 0.01$, $B_1(\\text{T}) = 0.97$,\n  - $\\mathbf{x} = \\text{``AAAAT''}$ (length $5$).\n- Test $2$ (adversarial underflow for the unstable log-sum-exp due to extremely long accumulation of small log-probabilities):\n  - $N = 2$,\n  - $\\boldsymbol{\\pi} = [\\,0.5,\\,0.5\\,]$,\n  - $\\mathbf{A} = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}$,\n  - $\\mathbf{B}$ (both states identical):\n    - for each state $i \\in \\{0,1\\}$: $B_i(\\text{A}) = 10^{-2}$, and $B_i(\\text{C}) = B_i(\\text{G}) = B_i(\\text{T}) = \\dfrac{1 - 10^{-2}}{3}$,\n  - $\\mathbf{x}$ is the string of $250$ copies of $\\text{``A''}$ (length $250$).\n- Test $3$ (boundary tie case, stabilized log-sum-exp agrees with max-sum on path):\n  - $N = 2$,\n  - $\\boldsymbol{\\pi} = [\\,0.5,\\,0.5\\,]$,\n  - $\\mathbf{A} = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{bmatrix}$,\n  - $\\mathbf{B}$ (both states identical and uniform): for each state $i \\in \\{0,1\\}$ and each symbol $x \\in \\Sigma$, $B_i(x) = 0.25$,\n  - $\\mathbf{x} = \\text{``ACGT''}$ (length $4$).\n\nRequired outputs for each test:\n\n- For Test $1$, output the integer $1$ if the path from the unstable log-sum-exp variant equals the path from the correct max-sum Viterbi decoder, and $0$ otherwise.\n- For Test $2$, output the integer $1$ if at least one intermediate dynamic-programming cell in the unstable log-sum-exp variant attains the value $-\\infty$ (indicating numerical underflow), and $0$ otherwise.\n- For Test $3$, output the integer $1$ if the path from the stabilized log-sum-exp variant equals the path from the correct max-sum Viterbi decoder, and $0$ otherwise.\n\nFinal output format: Your program should produce a single line of output containing the three integers from the three tests as a comma-separated list enclosed in square brackets (e.g., \"[1,0,1]\"). No other output is permitted.\n\nNo physical units are involved. Angles and percentages do not appear. All probabilities must be treated as real numbers in the open interval $(0,1)$ and all computations must be done in base-$10$ literal constants as provided above.", "solution": "The problem statement has been rigorously validated. It is scientifically grounded, well-posed, and contains all necessary information to proceed. The problem addresses a legitimate and important issue in computational science: numerical stability in dynamic programming algorithms. The parameters for the Hidden Markov Models (HMMs) are consistent and the test cases are well-designed to probe the specified numerical behaviors. The problem is therefore deemed valid.\n\nThe central task is to find the most probable sequence of hidden states $\\hat{\\mathbf{z}} = (\\hat{z}_0, \\hat{z}_1, \\dots, \\hat{z}_{T-1})$ that could have generated a given sequence of observations $\\mathbf{x} = (x_0, x_1, \\dots, x_{T-1})$. This is known as decoding. The Viterbi algorithm provides an exact and efficient solution to this problem.\n\nThe foundation of the HMM is the joint probability of a state path $\\mathbf{z}$ and an observation sequence $\\mathbf{x}$, given by:\n$$\n\\Pr(\\mathbf{z}, \\mathbf{x} \\mid \\boldsymbol{\\lambda}) = \\pi_{z_0} B_{z_0}(x_0) \\prod_{t=1}^{T-1} A_{z_{t-1}, z_t} B_{z_t}(x_t)\n$$\nwhere $\\boldsymbol{\\lambda} = (\\mathbf{A}, \\mathbf{B}, \\boldsymbol{\\pi})$ represents the model parameters. Maximizing the posterior probability $\\Pr(\\mathbf{z} \\mid \\mathbf{x})$ is equivalent to maximizing the joint probability $\\Pr(\\mathbf{z}, \\mathbf{x})$, since $\\Pr(\\mathbf{x})$ is a constant factor for a given $\\mathbf{x}$.\n\nThe Viterbi algorithm finds this optimal path using dynamic programming. Let us define the Viterbi variable $\\delta_t(j)$ as the probability of the most likely state sequence of length $t+1$ ending in state $j$ at time $t$, having generated the observation prefix $(x_0, \\dots, x_t)$.\nThe recurrence relation for $\\delta_t(j)$ is derived as follows:\nThe initialization at time $t=0$ is:\n$$\n\\delta_0(j) = \\pi_j B_j(x_0)\n$$\nFor subsequent time steps $t > 0$, the value $\\delta_t(j)$ is computed by extending the most probable paths from time $t-1$:\n$$\n\\delta_t(j) = \\left[ \\max_{i \\in \\{0, \\dots, N-1\\}} \\left( \\delta_{t-1}(i) \\cdot A_{ij} \\right) \\right] \\cdot B_j(x_t)\n$$\nTo reconstruct the path, a backpointer variable $\\psi_t(j)$ is stored at each step, recording the predecessor state that maximized the probability:\n$$\n\\psi_t(j) = \\arg\\max_{i \\in \\{0, \\dots, N-1\\}} \\left( \\delta_{t-1}(i) \\cdot A_{ij} \\right)\n$$\n\nThe product of multiple probabilities, which are numbers in the range $[0, 1]$, can lead to numerical underflow, especially for long sequences ($T \\gg 1$). To mitigate this, computations are performed in the logarithmic domain. Let $v_t(j) = \\log \\delta_t(j)$. The operations transform as follows: multiplication becomes addition, and the $\\max$ operation on probabilities becomes a $\\max$ on log-probabilities.\nThe log-domain Viterbi recurrence, known as the max-sum algorithm, is:\nInitialization ($t=0$):\n$$\nv_0(j) = \\log \\pi_j + \\log B_j(x_0)\n$$\nRecursion ($t > 0$):\n$$\nv_t(j) = \\left[ \\max_{i \\in \\{0, \\dots, N-1\\}} \\left( v_{t-1}(i) + \\log A_{ij} \\right) \\right] + \\log B_j(x_t)\n$$\nThe backpointer becomes:\n$$\n\\psi_t(j) = \\arg\\max_{i \\in \\{0, \\dots, N-1\\}} \\left( v_{t-1}(i) + \\log A_{ij} \\right)\n$$\nThis is the correct and numerically robust Viterbi decoder.\n\nThis problem requires the implementation of two additional, non-standard variants that replace the $\\max$ operator in the score calculation with a Log-Sum-Exp (LSE) function. It is crucial to note that the backpointer, $\\psi_t(j)$, is still determined by the $\\arg\\max$ of the predecessor scores, even in these LSE variants. The LSE function only affects the propagation of the score $v_t(j)$.\n\nLet the vector of candidate scores from predecessor states be $\\mathbf{s}_j = [ v_{t-1}(0) + \\log A_{0j}, \\dots, v_{t-1}(N-1) + \\log A_{N-1,j} ]$. The recurrence for $v_t(j)$ can be written as $v_t(j) = \\text{Combine}(\\mathbf{s}_j) + \\log B_j(x_t)$.\n\n1.  **Unstable Log-Sum-Exp Variant**: This variant uses the direct LSE formulation for the combination step:\n    $$\n    \\text{Combine}(\\mathbf{s}_j) = \\operatorname{LSE}_{\\text{unstable}}(\\mathbf{s}_j) = \\log\\left(\\sum_i e^{s_{ji}}\\right)\n    $$\n    This formulation is numerically unstable. The log-probabilities $v_{t-1}(i)$, and thus the candidate scores $s_{ji}$, become large negative numbers for long sequences. In standard IEEE $754$ double-precision floating-point arithmetic, the exponential function $\\exp(y)$ underflows to $0$ for $y \\lesssim -745$. If all elements of $\\mathbf{s}_j$ are sufficiently negative, every term $e^{s_{ji}}$ becomes $0$. The sum is $0$, and its logarithm, $\\log(0)$, evaluates to $-\\infty$. This catastrophic loss of precision renders the algorithm useless, as demonstrated by Test $2$.\n\n2.  **Stabilized Log-Sum-Exp Variant**: This variant uses the \"log-sum-exp trick\" to prevent underflow:\n    $$\n    \\operatorname{LSE}_{\\text{stable}}(\\mathbf{s}_j) = m + \\log\\left(\\sum_i e^{s_{ji} - m}\\right), \\quad \\text{where } m = \\max_i s_{ji}\n    $$\n    By factoring out the maximum value $m$, the arguments to the exponential function, $s_{ji} - m$, are transformed such that they are all less than or equal to $0$. The largest argument is exactly $0$, so its exponential is $e^0 = 1$. The sum inside the logarithm is therefore guaranteed to be at least $1$, robustly avoiding underflow. This provides a stable computation of the LSE function.\n\nThe problem requires a specific analysis for three test cases:\n\n- **Test 1**: A short sequence with high-probability transitions and emissions. The log-scores will not become very negative. Consequently, the unstable LSE will not underflow, and its value will be a very close approximation to the $\\max$ function because one term in the sum will dominate. Since the backpointer logic is identical ($\\arg\\max$), the path produced by the unstable LSE variant is expected to be identical to that from the correct max-sum decoder.\n\n- **Test 2**: A long sequence ($T=250$) with small emission probabilities. The log-probability $\\log(10^{-2}) \\approx -4.6$ is added at each of the $250$ steps. The Viterbi scores $v_t(j)$ will accumulate to values on the order of $-1150$, which is far below the underflow threshold of approximately $-745$. The unstable LSE variant will compute $\\exp(s_i)$ on these scores, resulting in numerical zero. The sum will be zero, and $\\log(0)$ will cause the corresponding dynamic programming cells to become $-\\infty$.\n\n- **Test 3**: A scenario with uniform probabilities, leading to exact ties in predecessor scores. For any state $j$, the candidate scores $v_{t-1}(i) + \\log A_{ij}$ will be identical for all predecessors $i$. The backpointer is determined by $\\arg\\max_i$ over these identical scores. The tie-breaking rule specifies choosing the smallest state index. Since both the max-sum and stabilized LSE variants use this exact same $\\arg\\max$ rule for their backpointers, they will construct identical backpointer tables ($\\psi$) and therefore produce the same final path, even though their propagated scores $v_t(j)$ will differ (LSE of identical values is greater than the max).\n\nThe implementation will follow these principles to produce the required outputs for each test.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the HMM decoding experiments and print the final results.\n    \"\"\"\n    alphabet_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    # Test Suite Definition\n    test_cases = [\n        {\n            \"name\": \"Test 1: Happy Path\",\n            \"N\": 2,\n            \"pi\": np.array([0.5, 0.5]),\n            \"A\": np.array([[0.995, 0.005], [0.005, 0.995]]),\n            \"B\": np.array([\n                [0.97, 0.01, 0.01, 0.01],  # State 0\n                [0.01, 0.01, 0.01, 0.97]   # State 1\n            ]),\n            \"x\": \"AAAAT\",\n            \"eval_func\": lambda paths, tables: int(np.array_equal(paths['unstable_lse'], paths['max_sum']))\n        },\n        {\n            \"name\": \"Test 2: Adversarial Underflow\",\n            \"N\": 2,\n            \"pi\": np.array([0.5, 0.5]),\n            \"A\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"B\": np.array([\n                [1e-2, (1 - 1e-2) / 3, (1 - 1e-2) / 3, (1 - 1e-2) / 3],\n                [1e-2, (1 - 1e-2) / 3, (1 - 1e-2) / 3, (1 - 1e-2) / 3]\n            ]),\n            \"x\": \"A\" * 250,\n            \"eval_func\": lambda paths, tables: int(np.any(np.isneginf(tables['unstable_lse'])))\n        },\n        {\n            \"name\": \"Test 3: Boundary Tie Case\",\n            \"N\": 2,\n            \"pi\": np.array([0.5, 0.5]),\n            \"A\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"B\": np.array([\n                [0.25, 0.25, 0.25, 0.25],\n                [0.25, 0.25, 0.25, 0.25]\n            ]),\n            \"x\": \"ACGT\",\n            \"eval_func\": lambda paths, tables: int(np.array_equal(paths['stable_lse'], paths['max_sum']))\n        }\n    ]\n\n    results = []\n    modes = ['max_sum', 'unstable_lse', 'stable_lse']\n\n    # Silence numpy warnings for log(0) which is expected for the unstable LSE test\n    with np.errstate(divide='ignore'):\n        for case in test_cases:\n            N = case[\"N\"]\n            pi = case[\"pi\"]\n            A = case[\"A\"]\n            B = case[\"B\"]\n            obs_seq = case[\"x\"]\n            obs_indices = [alphabet_map[obs] for obs in obs_seq]\n\n            log_pi = np.log(pi)\n            log_A = np.log(A)\n            log_B = np.log(B)\n\n            decoded_paths = {}\n            dp_tables = {}\n\n            for mode in modes:\n                path, v_table = viterbi_decoder(N, log_pi, log_A, log_B, obs_indices, mode)\n                decoded_paths[mode] = path\n                dp_tables[mode] = v_table\n            \n            result = case[\"eval_func\"](decoded_paths, dp_tables)\n            results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef viterbi_decoder(N, log_pi, log_A, log_B, obs_indices, mode):\n    \"\"\"\n    Implements the Viterbi algorithm with different score combination methods.\n\n    Args:\n        N (int): Number of hidden states.\n        log_pi (np.ndarray): Log of initial state probabilities.\n        log_A (np.ndarray): Log of transition probability matrix.\n        log_B (np.ndarray): Log of emission probability matrix.\n        obs_indices (list): List of integer indices for the observation sequence.\n        mode (str): The method for combining predecessor scores. \n                    One of 'max_sum', 'unstable_lse', 'stable_lse'.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The decoded state path.\n            - np.ndarray: The full dynamic programming score table (v).\n    \"\"\"\n    T = len(obs_indices)\n    v = np.zeros((T, N))\n    psi = np.zeros((T, N), dtype=int)\n\n    # Initialization (t=0)\n    v[0, :] = log_pi + log_B[:, obs_indices[0]]\n\n    # Recursion (t=1 to T-1)\n    for t in range(1, T):\n        for j in range(N):\n            # Candidate scores from predecessors. Shape: (N,)\n            candidate_scores = v[t-1, :] + log_A[:, j]\n\n            # Backpointer is always based on max, breaking ties by smallest index (default for argmax)\n            psi[t, j] = np.argmax(candidate_scores)\n\n            # Combined score calculation depends on the mode\n            if mode == 'max_sum':\n                combined_score = np.max(candidate_scores)\n            \n            elif mode == 'unstable_lse':\n                # This naive implementation is prone to underflow\n                combined_score = np.log(np.sum(np.exp(candidate_scores)))\n            \n            elif mode == 'stable_lse':\n                m = np.max(candidate_scores)\n                # If all predecessors have -inf probability, the result is -inf\n                if np.isneginf(m):\n                    combined_score = -np.inf\n                else:\n                    # The stable log-sum-exp trick\n                    combined_score = m + np.log(np.sum(np.exp(candidate_scores - m)))\n            \n            else:\n                raise ValueError(\"Invalid mode specified.\")\n\n            # Update DP table score for current state j at time t\n            v[t, j] = combined_score + log_B[j, obs_indices[t]]\n\n    # Termination and Path Backtracking\n    path = np.zeros(T, dtype=int)\n    # Find the most probable final state\n    path[T-1] = np.argmax(v[T-1, :])\n    # Backtrack to find the full path\n    for t in range(T-2, -1, -1):\n        path[t] = psi[t+1, path[t+1]]\n\n    return path, v\n\nsolve()\n```", "id": "2436975"}]}