{"hands_on_practices": [{"introduction": "A fundamental task in bioinformatics is to translate biological hypotheses into quantitative features. Since biological function is often linked to specific genomic locations, we can engineer features that capture this spatial information. This practice [@problem_id:2389842] guides you through creating a feature from DNA methylation data based on proximity to CpG islands, a classic example in epigenomics that demonstrates how to integrate different data types to measure a biologically relevant signal.", "problem": "You are given synthetic deoxyribonucleic acid (DNA) methylation microarray data with probe genomic coordinates and per-sample methylation values known as beta values. The task is to engineer a single feature per sample: the average beta value across all probes that lie in the Cytosine-phosphate-Guanosine (CpG) island shores, defined as regions flanking each CpG island by a specified window width on both sides, excluding any base pairs that are inside any CpG island. Your solution must be a complete runnable program that computes this feature for multiple test cases and additionally evaluates a simple feature-selection criterion based on variance across samples.\n\nStart from the following fundamental base:\n- A CpG island is represented as a closed genomic interval with integer base-pair coordinates on a chromosome. For an island with start coordinate $s$ and end coordinate $e$ (inclusive), the left shore is the closed interval $[s - w, s - 1]$ and the right shore is the closed interval $[e + 1, e + w]$, where $w$ is a nonnegative integer window width in base pairs. All intervals must be clipped to the chromosome domain $[1, L]$, where $L$ is the chromosome length in base pairs.\n- The union of shore regions over all islands is the set of base-pair positions that lie in at least one such left or right shore, after excluding any positions that lie inside any CpG island.\n- A probe at genomic position $x$ lies in the shore union if and only if $x$ is inside at least one left or right shore interval (after clipping to $[1, L]$) and $x$ is not contained in any CpG island interval.\n- For a given sample $i$, the engineered feature value is the arithmetic mean of beta values over all probes whose positions fall in the shore union, ignoring missing values. If no probe contributes for a sample, the feature value is undefined and should be represented as a not-a-number quantity.\n- The across-sample population variance is defined as $\\sigma^{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(m_{i} - \\bar{m}\\right)^{2}$ where $m_{i}$ is the engineered feature value for sample $i$, $\\bar{m}$ is the mean of the $m_{i}$ over samples with defined values, and $n$ is the count of samples with defined values used in $\\bar{m}$. Missing feature values should be excluded from the variance computation. If no samples have defined feature values, the variance is undefined and should be represented as a not-a-number quantity. The feature-selection decision for a test case is to return a boolean indicating whether $\\sigma^{2} \\ge \\tau$, where $\\tau$ is a provided nonnegative threshold.\n\nAll coordinates are integers and measured in base pairs (bp). Beta values are real numbers on $[0, 1]$ or not-a-number. No angles are involved. All outputs must be decimals, not percentages.\n\nYour program must:\n- For each test case, construct the shore union from the given CpG islands, chromosome length $L$, and window width $w$.\n- Select the probes whose genomic positions lie in the shore union and compute, for each sample, the engineered feature $m_{i}$ by averaging over the selected probes, ignoring missing values.\n- Compute the across-sample population variance $\\sigma^{2}$ over defined $m_{i}$ values and compare to the provided threshold $\\tau$ to produce a boolean decision.\n- Round each $m_{i}$ and $\\sigma^{2}$ to $6$ decimal places for output. If any $m_{i}$ or $\\sigma^{2}$ is undefined, output it as a not-a-number.\n\nInput is hardcoded via the following test suite that your program must embed. For each test case, the data consist of:\n- Chromosome length $L$ in base pairs.\n- Window width $w$ in base pairs.\n- A list of CpG island intervals $[(s_{1}, e_{1}), (s_{2}, e_{2}), \\dots]$ with $s_{k} \\le e_{k}$, endpoints inclusive.\n- A list of probes as pairs $(x, [\\beta_{1}, \\beta_{2}, \\dots, \\beta_{S}])$ where $x$ is the probe position and $\\beta_{i}$ is the beta value for sample $i$ (possibly not-a-number).\n- A nonnegative variance threshold $\\tau$.\n\nTest Suite:\n- Test case $1$ (happy path):\n  - $L = 10000$, $w = 2000$.\n  - Islands: $[(4000, 4500)]$.\n  - Probes:\n    - $(1500, [0.1, 0.2, 0.15])$\n    - $(2500, [0.8, 0.7, 0.9])$\n    - $(4000, [0.9, 0.95, 0.85])$\n    - $(4501, [0.3, 0.4, 0.35])$\n    - $(4600, [0.6, 0.5, 0.55])$\n    - $(7000, [0.2, 0.2, 0.2])$\n  - $\\tau = 0.001$.\n- Test case $2$ (boundary clipping and missing values):\n  - $L = 5000$, $w = 2000$.\n  - Islands: $[(100, 300)]$.\n  - Probes:\n    - $(1, [0.5, 0.6, 0.7])$\n    - $(99, [0.4, \\text{NaN}, 0.6])$\n    - $(100, [0.9, 0.8, 0.7])$\n    - $(200, [0.2, 0.2, 0.2])$\n    - $(301, [0.3, 0.3, 0.3])$\n    - $(2299, [0.2, 0.2, \\text{NaN}])$\n    - $(2300, [0.1, \\text{NaN}, 0.1])$\n    - $(2301, [0.0, 0.0, 0.0])$\n  - $\\tau = 0.002$.\n- Test case $3$ (overlapping shores from adjacent islands, excluding island bases):\n  - $L = 20000$, $w = 3000$.\n  - Islands: $[(5000, 5200), (8000, 8200)]$.\n  - Probes:\n    - $(2100, [0.2, 0.4, 0.6])$\n    - $(5100, [0.9, 0.9, 0.9])$\n    - $(6000, [0.5, 0.7, 0.9])$\n    - $(8100, [0.1, 0.2, 0.3])$\n    - $(9000, [0.4, 0.4, 0.4])$\n    - $(15000, [0.0, 0.0, 0.0])$\n    - $(11200, [0.8, 0.6, 0.4])$\n    - $(8201, [0.3, \\text{NaN}, 0.3])$\n  - $\\tau = 0.001$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-free, bracketed list of the three test-case results. Each test-case result is itself a list structured as $[m_{1}, m_{2}, \\dots, m_{S}, \\sigma^{2}, \\text{pass}]$ where $m_{i}$ are the per-sample engineered feature values rounded to $6$ decimal places, $\\sigma^{2}$ is the across-sample population variance rounded to $6$ decimal places, and $\\text{pass}$ is a boolean indicating whether $\\sigma^{2} \\ge \\tau$. The output must have no spaces. For example, a list of three test-case results should look like $[[\\dots],[\\dots],[\\dots]]$ with no spaces anywhere.", "solution": "The problem presented is a well-defined exercise in computational biology, specifically in the domain of feature engineering from genomic data. The task requires the computation of a single feature per sample from synthetic DNA methylation microarray data, followed by a simple variance-based feature selection step. The problem is scientifically grounded, logically consistent, and provides all necessary information for a unique solution. Therefore, it is deemed valid.\n\nThe solution proceeds by implementing the specified logic in a sequential, step-by-step manner for each test case.\n\nFirst, we must precisely identify the genomic regions of interest, which are the CpG island shores. A CpG island is given as a closed interval $[s, e]$. The shores are defined as regions of width $w$ flanking the island, specifically the intervals $[s - w, s - 1]$ and $[e + 1, e + w]$. These intervals must be clipped to the valid chromosome domain, which is $[1, L]$, where $L$ is the chromosome length. Thus, for an island $[s, e]$, the left shore is $[\\max(1, s - w), s - 1]$ and the right shore is $[e + 1, \\min(L, e + w)]$.\n\nThe core of the problem lies in defining the set of probe positions that are considered for feature computation. A probe at a genomic coordinate $x$ is selected if and only if two conditions are met:\n1. The position $x$ must fall within the union of all shore regions derived from all CpG islands. This means $x$ must be in at least one left or right shore interval.\n2. The position $x$ must NOT fall within any CpG island interval $[s_k, e_k]$.\n\nTo implement this logic efficiently, we can use boolean masks over the entire chromosome length $L$. Let us define two boolean arrays of length $L+1$ (using 1-based indexing for coordinates): `is_island` and `is_shore`.\n- The `is_island` array is initialized to false. For each island $[s_k, e_k]$, we set the elements from index $s_k$ to $e_k$ (inclusive) to true.\n- The `is_shore` array is also initialized to false. For each island $[s_k, e_k]$, we calculate its clipped left and right shore intervals and set the corresponding elements in `is_shore` to true.\n\nA probe at position $x$ is then selected if `is_shore`$[x]$ is true and `is_island`$[x]$ is false. This approach correctly handles complex cases, such as shores from adjacent islands overlapping each other or even overlapping other islands, by correctly applying the exclusion principle.\n\nOnce the set of relevant probes is identified, we engineer the feature for each sample. Let $S$ be the number of samples. The beta values for the selected probes form a matrix of values, where rows correspond to probes and columns to samples. For each sample $i \\in \\{1, \\dots, S\\}$, the feature value $m_i$ is calculated as the arithmetic mean of the beta values $(\\beta)$ from all selected probes, denoted by the set $P_{\\text{sel}}$. Any missing values, represented as not-a-number (NaN), must be ignored in this calculation.\n$$\nm_i = \\frac{1}{|P'_{\\text{sel}, i}|} \\sum_{p \\in P'_{\\text{sel}, i}} \\beta_{p,i}\n$$\nwhere $P'_{\\text{sel}, i}$ is the subset of selected probes for which the beta value for sample $i$ is not NaN. If for a given sample $i$, all selected probes have NaN beta values or if no probes are selected at all ($P_{\\text{sel}}$ is empty), the set $P'_{\\text{sel}, i}$ is empty and the feature value $m_i$ is defined as NaN.\n\nThe final step is to evaluate the utility of this engineered feature using a variance-based selection criterion. We compute the population variance, $\\sigma^2$, of the feature values $m_i$ across all samples for which $m_i$ is not NaN. Let the set of non-NaN feature values be $M' = \\{m_i | m_i \\text{ is not NaN}\\}$ and let $n = |M'|$ be the count of such values. The mean of these values is $\\bar{m} = \\frac{1}{n} \\sum_{m_i \\in M'} m_i$. The population variance is then:\n$$\n\\sigma^2 = \\frac{1}{n} \\sum_{m_i \\in M'} (m_i - \\bar{m})^2\n$$\nIf $n=0$ (all $m_i$ are NaN), $\\sigma^2$ is also NaN. The feature is selected if this variance meets or exceeds a given threshold $\\tau$:\n$$\n\\text{pass} = (\\sigma^2 \\ge \\tau)\n$$\nIf $\\sigma^2$ is NaN, this comparison evaluates to false.\n\nAll computed values for $m_i$ and $\\sigma^2$ are rounded to $6$ decimal places as required. The procedure is applied independently to each test case provided in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the feature engineering and selection problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"L\": 10000, \"w\": 2000,\n            \"islands\": [(4000, 4500)],\n            \"probes\": [\n                (1500, [0.1, 0.2, 0.15]),\n                (2500, [0.8, 0.7, 0.9]),\n                (4000, [0.9, 0.95, 0.85]),\n                (4501, [0.3, 0.4, 0.35]),\n                (4600, [0.6, 0.5, 0.55]),\n                (7000, [0.2, 0.2, 0.2]),\n            ],\n            \"tau\": 0.001\n        },\n        # Test case 2 (boundary clipping and missing values)\n        {\n            \"L\": 5000, \"w\": 2000,\n            \"islands\": [(100, 300)],\n            \"probes\": [\n                (1, [0.5, 0.6, 0.7]),\n                (99, [0.4, np.nan, 0.6]),\n                (100, [0.9, 0.8, 0.7]),\n                (200, [0.2, 0.2, 0.2]),\n                (301, [0.3, 0.3, 0.3]),\n                (2299, [0.2, 0.2, np.nan]),\n                (2300, [0.1, np.nan, 0.1]),\n                (2301, [0.0, 0.0, 0.0]),\n            ],\n            \"tau\": 0.002\n        },\n        # Test case 3 (overlapping shores from adjacent islands)\n        {\n            \"L\": 20000, \"w\": 3000,\n            \"islands\": [(5000, 5200), (8000, 8200)],\n            \"probes\": [\n                (2100, [0.2, 0.4, 0.6]),\n                (5100, [0.9, 0.9, 0.9]),\n                (6000, [0.5, 0.7, 0.9]),\n                (8100, [0.1, 0.2, 0.3]),\n                (9000, [0.4, 0.4, 0.4]),\n                (15000, [0.0, 0.0, 0.0]),\n                (11200, [0.8, 0.6, 0.4]),\n                (8201, [0.3, np.nan, 0.3]),\n            ],\n            \"tau\": 0.001\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        L = case[\"L\"]\n        w = case[\"w\"]\n        islands = case[\"islands\"]\n        probes = case[\"probes\"]\n        tau = case[\"tau\"]\n\n        # Step 1: Identify shore and island regions using boolean masks\n        is_island = np.zeros(L + 1, dtype=bool)\n        is_shore = np.zeros(L + 1, dtype=bool)\n\n        for s, e in islands:\n            is_island[s : e + 1] = True\n            \n            # Left shore\n            left_shore_start = max(1, s - w)\n            left_shore_end = s - 1\n            if left_shore_start <= left_shore_end:\n                is_shore[left_shore_start : left_shore_end + 1] = True\n            \n            # Right shore\n            right_shore_start = e + 1\n            right_shore_end = min(L, e + w)\n            if right_shore_start <= right_shore_end:\n                is_shore[right_shore_start : right_shore_end + 1] = True\n\n        # Step 2: Select probes that are in shores but not in islands\n        selected_betas = []\n        for pos, beta_values in probes:\n            if 1 <= pos <= L and is_shore[pos] and not is_island[pos]:\n                selected_betas.append(beta_values)\n        \n        num_samples = len(probes[0][1]) if probes else 0\n        \n        # Step 3: Compute engineered feature m_i for each sample\n        m_features = []\n        if not selected_betas:\n            m_features = [np.nan] * num_samples\n        else:\n            selected_betas_matrix = np.array(selected_betas)\n            for i in range(num_samples):\n                sample_betas = selected_betas_matrix[:, i]\n                # np.nanmean handles empty arrays (after NaN removal) by returning nan\n                mean_val = np.nanmean(sample_betas)\n                m_features.append(mean_val)\n\n        # Step 4: Compute variance and perform feature selection\n        m_features_valid = [m for m in m_features if not np.isnan(m)]\n        \n        if len(m_features_valid) > 0:\n            # np.var computes population variance by default (ddof=0)\n            variance = np.var(m_features_valid)\n        else:\n            variance = np.nan\n\n        pass_criterion = False\n        if not np.isnan(variance):\n            pass_criterion = variance >= tau\n            \n        # Step 5: Format results\n        rounded_m = [round(m, 6) if not np.isnan(m) else m for m in m_features]\n        rounded_var = round(variance, 6) if not np.isnan(variance) else variance\n\n        case_result = rounded_m + [rounded_var, pass_criterion]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format\n    # Custom formatter to handle nan, bool, and float\n    def format_item(item):\n        if isinstance(item, bool):\n            return str(item)\n        if isinstance(item, float) and np.isnan(item):\n            return 'nan'\n        if isinstance(item, float):\n            return f\"{item:.6f}\"\n        return str(item)\n\n    result_str = \"[\"\n    for i, case_res in enumerate(all_results):\n        result_str += \"[\"\n        result_str += \",\".join([format_item(item) for item in case_res])\n        result_str += \"]\"\n        if i < len(all_results) - 1:\n            result_str += \",\"\n    result_str += \"]\"\n    \n    # Another way to achieve the same result without spaces\n    # import json\n    # print(json.dumps(all_results, separators=(',', ':')).replace('NaN', 'nan'))\n\n    print(result_str)\n\nsolve()\n```", "id": "2389842"}, {"introduction": "Simply having a DNA sequence is not enough; we must convert it into a numerical format that a machine learning model can understand. This conceptual practice [@problem_id:2389823] delves into this crucial step by comparing two powerful strategies: the traditional, position-specific one-hot encoding and the modern, context-rich learned $k$-mer embeddings. By analyzing their strengths and weaknesses, you will develop a deeper understanding of how feature representation choices impact model performance, complexity, and the very nature of what can be learned from sequence data.", "problem": "You are building a binary classifier to predict whether a genomic segment is an enhancer from its Deoxyribonucleic Acid (DNA) sequence. Each example is a DNA sequence of fixed length $L$ over alphabet $\\Sigma=\\{A,C,G,T\\}$. Consider two feature pipelines feeding the same linear classifier.\n\nPipeline $\\mathrm{O}$ (one-hot): Map each position $i\\in\\{1,\\dots,L\\}$ to a one-hot vector in $\\{0,1\\}^{4}$ indicating the nucleotide at that position, and concatenate across positions to obtain a feature vector $\\phi_{\\mathrm{OH}}(s)\\in\\{0,1\\}^{4L}$. The linear model computes a score $f_{\\mathrm{O}}(s)=w^{\\top}\\phi_{\\mathrm{OH}}(s)+b$ for parameters $w\\in\\mathbb{R}^{4L}$ and $b\\in\\mathbb{R}$.\n\nPipeline $\\mathrm{E}$ (learned $k$-mer embedding): Tokenize $s$ into all overlapping $k$-mers with stride $1$, that is $m_t=s_{t:t+k-1}$ for $t\\in\\{1,\\dots,T\\}$ with $T=L-k+1$. Use a pretrained embedding $e:\\Sigma^{k}\\to\\mathbb{R}^{d}$ learned from a large unlabeled genomic corpus (for example, dna2vec-like training by $k$-mer co-occurrence). Aggregate to a fixed-length vector by the mean $x(s)=\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)\\in\\mathbb{R}^{d}$. The linear model computes a score $f_{\\mathrm{E}}(s)=u^{\\top}x(s)+c$ for parameters $u\\in\\mathbb{R}^{d}$ and $c\\in\\mathbb{R}$.\n\nAssume $k\\ge 2$, $d\\ll 4L$, and the same training set of $N$ labeled sequences is used for both pipelines with identical linear learners. Answer the following multiple-choice question about the representational and statistical properties of these pipelines. Select all options that are correct.\n\nA. Because $f_{\\mathrm{E}}(s)=\\frac{1}{T}\\sum_{t=1}^{T}u^{\\top}e(m_t)+c$ depends only on the multiset of $k$-mers in $s$ and not on their positions, Pipeline $\\mathrm{E}$ with a linear classifier can implement a position-independent detector of informative $k$-mer content (a bag-of-$k$-mers model). In contrast, Pipeline $\\mathrm{O}$ with a linear classifier cannot detect $k$-mer patterns without explicit interaction features, because $f_{\\mathrm{O}}(s)$ is a sum of independent positionwise nucleotide contributions.\n\nB. If $d\\ll 4L$ and $N$ is limited, Pipeline $\\mathrm{E}$ typically offers lower-variance learning than Pipeline $\\mathrm{O}$ by reducing the number of free linear parameters from $4L$ to $d$, thereby improving sample efficiency under otherwise comparable conditions.\n\nC. Mapping any unseen $k$-mer at test time to the zero vector in $\\mathbb{R}^{d}$ ensures that pairwise distances between sequences in the averaged-embedding space are preserved up to a constant scaling, so this out-of-vocabulary handling cannot distort relative similarities among sequences.\n\nD. Because the mean of embedded $k$-mers is a linear function of per-position one-hot nucleotides via a fixed transformation, a linear classifier on $x(s)$ can reproduce any position-specific linear scoring function $f_{\\mathrm{O}}(s)$ on $\\phi_{\\mathrm{OH}}(s)$. Therefore, Pipeline $\\mathrm{E}$ strictly dominates Pipeline $\\mathrm{O}$ in representational power for linear models.\n\nE. The number of distinct $k$-mers grows as $4^{k}$, but the embedding maps them into $\\mathbb{R}^{d}$ with $d\\ll 4^{k}$. This compression can act as a form of regularization by tying together rare $k$-mers with similar contexts into nearby vectors, which may improve generalization when many $k$-mers are infrequent in the training set.", "solution": "The problem statement presents a valid and well-posed question comparing two standard feature engineering pipelines in computational biology for a linear classifier. The definitions of Pipeline O (one-hot encoding) and Pipeline E (pretrained $k$-mer embeddings) are precise, scientifically grounded, and allow for a rigorous analysis of their respective properties. We shall proceed to evaluate each option.\n\nThe score function for Pipeline O is given by $f_{\\mathrm{O}}(s)=w^{\\top}\\phi_{\\mathrm{OH}}(s)+b$. The feature vector $\\phi_{\\mathrm{OH}}(s) \\in \\{0,1\\}^{4L}$ is a concatenation of $L$ one-hot vectors, one for each position in the sequence $s$. Let $\\phi_i(s_i) \\in \\{0,1\\}^4$ be the one-hot vector for the nucleotide $s_i$ at position $i$. The weight vector $w \\in \\mathbb{R}^{4L}$ can be partitioned into $L$ vectors $w_i \\in \\mathbb{R}^4$, each corresponding to a position. The score can then be written as:\n$$f_{\\mathrm{O}}(s) = \\sum_{i=1}^{L} w_i^{\\top}\\phi_i(s_i) + b$$\nThis model is linear and additive over the positions. It learns a position-specific preference for each nucleotide but cannot capture any dependencies or interactions between nucleotides at different positions. This is equivalent to a Position Weight Matrix (PWM) model.\n\nThe score function for Pipeline E is $f_{\\mathrm{E}}(s)=u^{\\top}x(s)+c$, where $x(s)=\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)$ is the mean of pretrained embedding vectors for all overlapping $k$-mers $m_t$ in the sequence. $T = L-k+1$ is the number of such $k$-mers. By the linearity of the dot product, the score is:\n$$f_{\\mathrm{E}}(s) = u^{\\top}\\left(\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)\\right)+c = \\frac{1}{T}\\sum_{t=1}^{T} u^{\\top}e(m_t) + c$$\nThe term $u^{\\top}e(m_t)$ can be viewed as a score for the $k$-mer $m_t$. The function $f_{\\mathrm{E}}(s)$ computes the average score over all $k$-mers in the sequence. The averaging operation discards all positional information about the $k$-mers. The representation $x(s)$ depends only on the multiset of $k$-mers present in $s$, not their order or location.\n\nWith these formulations, we analyze each option.\n\n**A. Because $f_{\\mathrm{E}}(s)=\\frac{1}{T}\\sum_{t=1}^{T}u^{\\top}e(m_t)+c$ depends only on the multiset of $k$-mers in $s$ and not on their positions, Pipeline $\\mathrm{E}$ with a linear classifier can implement a position-independent detector of informative $k$-mer content (a bag-of-$k$-mers model). In contrast, Pipeline $\\mathrm{O}$ with a linear classifier cannot detect $k$-mer patterns without explicit interaction features, because $f_{\\mathrm{O}}(s)$ is a sum of independent positionwise nucleotide contributions.**\nThis statement is correct. The first part, regarding Pipeline E, follows directly from the formulation of $x(s)$ as an average of $k$-mer embeddings. This aggregation method is by definition insensitive to the position of the $k$-mers, making it a \"bag-of-$k$-mers\" model. The second part, regarding Pipeline O, is also correct. The score $f_{\\mathrm{O}}(s)$ is a linear combination of features that indicate the presence of a specific nucleotide at a specific position. Such a linear model cannot represent logical AND conditions (e.g., nucleotide $N_1$ at position $i$ AND nucleotide $N_2$ at position $j$), which are required to define a pattern of length $k \\ge 2$. To detect a specific $k$-mer, one would need a feature that is a product of the corresponding one-hot features, which is a non-linear interaction term not available to the linear model in Pipeline O.\n**Verdict: Correct.**\n\n**B. If $d\\ll 4L$ and $N$ is limited, Pipeline $\\mathrm{E}$ typically offers lower-variance learning than Pipeline $\\mathrm{O}$ by reducing the number of free linear parameters from $4L$ to $d$, thereby improving sample efficiency under otherwise comparable conditions.**\nThis statement is correct. The number of learnable parameters in the final classification layer for Pipeline O is the dimension of $w$ plus the bias, i.e., $4L+1$. For Pipeline E, the number of learnable parameters is the dimension of $u$ plus the bias, i.e., $d+1$, since the embedding $e$ is pretrained and fixed. The problem assumes $d \\ll 4L$. In statistical learning, a model with a significantly smaller number of parameters (lower complexity) trained on a dataset of fixed size $N$ is less prone to overfitting the training data. This results in lower variance of the learned hypothesis. Lower variance generally improves generalization to unseen data and means that the model can learn more effectively from a smaller number of examples (improved sample efficiency).\n**Verdict: Correct.**\n\n**C. Mapping any unseen $k$-mer at test time to the zero vector in $\\mathbb{R}^{d}$ ensures that pairwise distances between sequences in the averaged-embedding space are preserved up to a constant scaling, so this out-of-vocabulary handling cannot distort relative similarities among sequences.**\nThis statement is incorrect. The proposed out-of-vocabulary (OOV) strategy consists of replacing the embedding of an unseen $k$-mer with the zero vector, $\\vec{0} \\in \\mathbb{R}^d$. This significantly distorts the geometry of the embedding space. Consider three sequences $s_A, s_B, s_C$ that consist of single $k$-mers $m_A, m_B, m_C$ respectively. Let $m_A, m_B$ be in the vocabulary, and $m_C$ be an OOV $k$-mer. The representations are $x(s_A)=e(m_A)$, $x(s_B)=e(m_B)$, and $x(s_C)=\\vec{0}$. The original relative similarity might have implied that $m_C$ is very similar to $m_A$ but dissimilar to $m_B$, meaning the distance $\\|e(m_A) - e(m_C)\\|$ should be small and $\\|e(m_B) - e(m_C)\\|$ should be large. With the OOV handling, the new distances are $\\|e(m_A) - \\vec{0}\\| = \\|e(m_A)\\|$ and $\\|e(m_B) - \\vec{0}\\| = \\|e(m_B)\\|$. These values have no guaranteed relationship to the true (but unknown) similarities. The mapping makes the OOV $k$-mer seem equally related or unrelated to all other $k$-mers based solely on their vector norms, which is an arbitrary distortion. Thus, relative similarities are not preserved.\n**Verdict: Incorrect.**\n\n**D. Because the mean of embedded $k$-mers is a linear function of per-position one-hot nucleotides via a fixed transformation, a linear classifier on $x(s)$ can reproduce any position-specific linear scoring function $f_{\\mathrm{O}}(s)$ on $\\phi_{\\mathrm{OH}}(s)$. Therefore, Pipeline $\\mathrm{E}$ strictly dominates Pipeline $\\mathrm{O}$ in representational power for linear models.**\nThis statement is incorrect. It rests on a false premise and draws a false conclusion.\nFirst, the premise that \"the mean of embedded $k$-mers is a linear function of per-position one-hot nucleotides\" is false. The function $e:\\Sigma^k \\to \\mathbb{R}^d$ is a lookup table, which is a highly non-linear mapping from the space of nucleotide sequences. A specific $k$-mer is identified by a conjunction of $k$ specific nucleotides at $k$ relative positions. A feature representing this conjunction cannot be expressed as a linear combination of the features for the individual nucleotides.\nSecond, the conclusion is false. Pipeline O implements position-sensitive functions, whereas Pipeline E, as constructed, implements position-insensitive functions. A position-insensitive model cannot, in general, represent a position-sensitive one. For example, let $s_1$ be a sequence with a specific motif at the beginning and $s_2$ be the same sequence with the motif at the end. An enhancer model in Pipeline O could easily distinguish them, assigning $f_{\\mathrm{O}}(s_1) \\neq f_{\\mathrm{O}}(s_2)$. In Pipeline E, if $s_1$ and $s_2$ comprise the exact same multiset of $k$-mers, then $x(s_1) = x(s_2)$, and consequently $f_{\\mathrm{E}}(s_1) = f_{\\mathrm{E}}(s_2)$. Therefore, Pipeline E cannot reproduce arbitrary functions from Pipeline O's hypothesis space. Neither pipeline strictly dominates the other; they capture different types of information.\n**Verdict: Incorrect.**\n\n**E. The number of distinct $k$-mers grows as $4^{k}$, but the embedding maps them into $\\mathbb{R}^{d}$ with $d\\ll 4^{k}$. This compression can act as a form of regularization by tying together rare $k$-mers with similar contexts into nearby vectors, which may improve generalization when many $k$-mers are infrequent in the training set.**\nThis statement is correct. The number of possible $k$-mers, $4^k$, is vast. The embedding projects this high-dimensional discrete space into a low-dimensional continuous space $\\mathbb{R}^d$, where $d \\ll 4^k$. This compression is not random; it is structured. Methods like dna2vec learn embeddings such that $k$-mers appearing in similar genomic contexts are mapped to nearby vectors in $\\mathbb{R}^d$. This \"tying\" of parameters - where the representation of one $k$-mer is constrained by the representations of others - is a form of regularization. It allows the model to generalize. For instance, if a rare $k$-mer did not appear often in the labeled training set but appeared in similar contexts to a common, functional $k$-mer in the large unlabeled pretraining corpus, their embeddings would be close. The model can then transfer the learned knowledge about the functional role of the common $k$-mer to the rare one, improving its predictive performance on sequences containing the rare $k$-mer. This is a primary benefit of using pretrained embeddings.\n**Verdict: Correct.**", "answer": "$$\\boxed{A, B, E}$$", "id": "2389823"}, {"introduction": "The most powerful features are not always simple transformations of raw data; sometimes, they are the parameters of a statistical model themselves. This advanced practice [@problem_id:2389799] introduces this sophisticated concept by tasking you with quantifying an epistatic interaction between two genetic variants. You will fit a logistic regression model and use its estimated interaction coefficient, $\\hat{\\beta}_3$, as the engineered feature, demonstrating a powerful method for capturing non-additive biological effects that are invisible to simpler approaches.", "problem": "You are given binary phenotype data and two genotype features representing two single nucleotide polymorphisms (SNPs), denoted by $S_1$ and $S_2$, for multiple individuals. For an individual index $i \\in \\{1,2,\\ldots,n\\}$, the phenotype is $y_i \\in \\{0,1\\}$ and the genotypes are $S_{1,i} \\in \\{0,1,2\\}$ and $S_{2,i} \\in \\{0,1,2\\}$. Consider the logistic regression model in which the log-odds of $y_i=1$ is a linear function of an intercept, the main effects of $S_{1,i}$ and $S_{2,i}$, and their interaction:\n$$\n\\log\\left(\\frac{\\mathbb{P}(y_i=1 \\mid S_{1,i},S_{2,i})}{\\mathbb{P}(y_i=0 \\mid S_{1,i},S_{2,i})}\\right) \\;=\\; \\beta_0 \\;+\\; \\beta_1 S_{1,i} \\;+\\; \\beta_2 S_{2,i} \\;+\\; \\beta_3 \\left(S_{1,i} \\cdot S_{2,i}\\right).\n$$\nThe engineered interaction feature is defined to be the maximum-likelihood estimate of the coefficient $\\beta_3$ obtained by fitting the above model. If the interaction column $\\left(S_{1,i}\\cdot S_{2,i}\\right)_{i=1}^n$ is identically zero (so that it carries no information), define the engineered interaction feature to be $0$ by convention.\n\nYour task is to compute the engineered interaction feature for each of the following test cases. For each case, you must treat the given data as the complete dataset to which the model above is fitted, with no additional covariates. All computations are unitless. Your program should output a single line that aggregates the results from all test cases into a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ digits after the decimal point.\n\nTest Suite\n\n- Case A (balanced presence of all genotype combinations):\n  The dataset consists of $n=20$ individuals specified as ordered triples $(S_1,S_2,y)$. The samples are\n  $$(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,1),$$\n  $$(1,0,0),(1,0,0),(1,0,1),(1,0,0),(1,0,1),$$\n  $$(0,1,0),(0,1,1),(0,1,0),(0,1,0),(0,1,1),$$\n  $$(1,1,1),(1,1,1),(1,1,0),(1,1,1),(1,1,1).$$\n  For this case, compute the fitted value of $\\hat{\\beta}_3$.\n\n- Case B (approximately additive main effects with minimal interaction):\n  The dataset consists of $n=40$ individuals, partitioned into four groups by $(S_1,S_2)$ with equal group sizes. For $(S_1,S_2)=(0,0)$ there are $10$ individuals with $3$ having $y=1$ and $7$ having $y=0$; for $(S_1,S_2)=(1,0)$ there are $10$ individuals with $5$ having $y=1$ and $5$ having $y=0$; for $(S_1,S_2)=(0,1)$ there are $10$ individuals with $5$ having $y=1$ and $5$ having $y=0$; for $(S_1,S_2)=(1,1)$ there are $10$ individuals with $7$ having $y=1$ and $3$ having $y=0$. Explicitly, the samples are\n  $$(0,0,1),(0,0,1),(0,0,1),(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,0),$$\n  $$(1,0,1),(1,0,1),(1,0,1),(1,0,1),(1,0,1),(1,0,0),(1,0,0),(1,0,0),(1,0,0),(1,0,0),$$\n  $$(0,1,1),(0,1,1),(0,1,1),(0,1,1),(0,1,1),(0,1,0),(0,1,0),(0,1,0),(0,1,0),(0,1,0),$$\n  $$(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,0),(1,1,0),(1,1,0).$$\n  For this case, compute the fitted value of $\\hat{\\beta}_3$.\n\n- Case C (degenerate interaction column):\n  The dataset consists of $n=8$ individuals with $S_2$ identically zero. The samples are\n  $$(0,0,0),(1,0,1),(0,0,0),(2,0,1),(1,0,0),(2,0,1),(0,0,1),(1,0,0).$$\n  For this case, return $0$ as the engineered interaction feature by the stated convention.\n\nFinal Output Format\n\nYour program should produce a single line of output containing the three engineered interaction features as a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ digits after the decimal point, for example: $[\\hat{\\beta}_3^{(A)},\\hat{\\beta}_3^{(B)},\\hat{\\beta}_3^{(C)}]$.", "solution": "The problem statement is subjected to rigorous validation.\n\nStep 1: Extract Givens\n- **Model**: Logistic regression with logit link function for a binary phenotype $y_i \\in \\{0, 1\\}$ and two single nucleotide polymorphism (SNP) features $S_{1,i}, S_{2,i} \\in \\{0, 1, 2\\}$. The model is specified as:\n$$ \\log\\left(\\frac{\\mathbb{P}(y_i=1 \\mid S_{1,i},S_{2,i})}{\\mathbb{P}(y_i=0 \\mid S_{1,i},S_{2,i})}\\right) = \\beta_0 + \\beta_1 S_{1,i} + \\beta_2 S_{2,i} + \\beta_3 (S_{1,i} \\cdot S_{2,i}) $$\n- **Engineered Feature Definition**: The feature is the maximum-likelihood estimate (MLE) of the interaction coefficient, $\\hat{\\beta}_3$.\n- **Special Condition**: If the interaction column $(S_{1,i} \\cdot S_{2,i})_{i=1}^n$ is identically zero, the engineered feature is defined to be $0$.\n- **Data Sets**:\n    - **Case A**: $n=20$ samples provided as $(S_1, S_2, y)$ triples.\n    - **Case B**: $n=40$ samples provided as counts for each of the four $(S_1,S_2)$ combinations, $(0,0), (1,0), (0,1), (1,1)$, and also as explicit samples.\n    - **Case C**: $n=8$ samples for which $S_2$ is identically zero.\n- **Task**: Compute $\\hat{\\beta}_3$ for each case.\n\nStep 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded**: The problem is based on logistic regression, a fundamental statistical method, applied to SNP data analysis, which is a standard practice in computational biology and bioinformatics. The model is scientifically sound.\n- **Well-Posed**: The problem asks for the maximum-likelihood estimate of a parameter in a generalized linear model. For the provided datasets in Case A and Case B, the data are not completely separable (i.e., for predictor combinations with multiple samples, both $y=0$ and $y=1$ outcomes are present), which ensures the existence of a unique and finite MLE. For Case C, the problem statement provides a clear rule to handle the non-identifiability that arises from a degenerate design matrix, making the problem well-posed in all cases.\n- **Objective**: The problem is articulated using precise mathematical language, with clearly defined terms and objective data.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, ambiguity, or contradiction. All data and conditions required for a solution are provided.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be derived and implemented.\n\nThe core of the problem is to find the maximum-likelihood estimate for the parameters $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$ of the specified logistic regression model.\n\nThe probability that $y_i=1$ is given by the sigmoid function, $\\sigma(\\eta_i)$:\n$$ p_i = \\mathbb{P}(y_i=1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}} $$\nwhere $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$ is the linear predictor. The design vector for the $i$-th individual is $\\mathbf{x}_i = [1, S_{1,i}, S_{2,i}, S_{1,i} \\cdot S_{2,i}]^T$.\n\nThe likelihood of the observed data, assuming independence, is the product of the Bernoulli probabilities for each observation:\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i} $$\nMaximizing the likelihood is equivalent to maximizing the log-likelihood, $\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta})$:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] $$\nBy substituting $p_i = e^{\\eta_i} / (1+e^{\\eta_i})$ and $1-p_i = 1 / (1+e^{\\eta_i})$, the log-likelihood function simplifies to:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right] = \\sum_{i=1}^n \\left[ y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) - \\log(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) \\right] $$\nThere is no closed-form solution for $\\boldsymbol{\\beta}$ that maximizes $\\ell(\\boldsymbol{\\beta})$. The solution must be found numerically. The log-likelihood function for a logistic regression model is globally concave, which guarantees that a unique maximum exists if the data are not perfectly separated. We can find this maximum by minimizing the negative log-likelihood, which is a convex function. A standard and efficient method for this is a quasi-Newton algorithm such as BFGS (Broyden–Fletcher–Goldfarb–Shanno), which we will employ using `scipy.optimize.minimize`. This requires the gradient of the objective function.\n\nThe objective function to minimize is $f(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta})$. Its gradient, which is the negative of the log-likelihood gradient, is:\n$$ \\nabla f(\\boldsymbol{\\beta}) = -\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -\\sum_{i=1}^n (y_i - p_i) \\mathbf{x}_i = \\sum_{i=1}^n (p_i - y_i) \\mathbf{x}_i = \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y}) $$\nwhere $\\mathbf{X}$ is the $n \\times 4$ design matrix, $\\mathbf{y}$ is the vector of outcomes, and $\\mathbf{p}$ is the vector of probabilities.\n\nFor each case, we proceed as follows:\n\n**Case A**:\nThe dataset consists of $n=20$ individual samples. We construct the $20 \\times 4$ design matrix $\\mathbf{X}$ and the $20 \\times 1$ response vector $\\mathbf{y}$ directly from the given data. The four columns of $\\mathbf{X}$ correspond to the intercept ($1$), $S_1$, $S_2$, and the interaction product $S_1 \\cdot S_2$. We then numerically minimize the negative log-likelihood using an initial guess of $\\boldsymbol{\\beta} = \\mathbf{0}$ to find the MLE vector $\\hat{\\boldsymbol{\\beta}}$. The desired feature is the fourth component of this vector, $\\hat{\\beta}_3$.\n\n**Case B**:\nThe dataset consists of $n=40$ samples, aggregated into four groups based on $(S_1, S_2)$ values. This structure allows for a more computationally efficient formulation of the log-likelihood. Let $j$ index the four unique groups, $N_j$ be the number of individuals in group $j$, and $k_j$ be the number of cases ($y=1$) in that group. The unique design vectors are $\\mathbf{x}_j$. The log-likelihood is:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{j=1}^4 \\left[ k_j (\\mathbf{x}_j^T \\boldsymbol{\\beta}) - N_j \\log(1 + e^{\\mathbf{x}_j^T \\boldsymbol{\\beta}}) \\right] $$\nThe gradient is similarly aggregated:\n$$ \\nabla f(\\boldsymbol{\\beta}) = \\sum_{j=1}^4 (N_j p_j - k_j) \\mathbf{x}_j $$\nWe form the $4 \\times 4$ matrix of unique design vectors and the corresponding vectors for $N_j$ and $k_j$. The optimization then proceeds as in Case A to find $\\hat{\\boldsymbol{\\beta}}$, and we extract $\\hat{\\beta}_3$.\n\n**Case C**:\nThe dataset has $S_{2,i} = 0$ for all $i=1, \\dots, 8$. Consequently, the interaction term $S_{1,i} \\cdot S_{2,i}$ is also $0$ for all individuals. The column in the design matrix corresponding to the interaction coefficient $\\beta_3$ is a vector of zeros. This creates a non-identifiability issue, as any value of $\\beta_3$ would yield the same likelihood. The problem provides an explicit convention for this scenario: the engineered interaction feature is defined to be $0$. Therefore, for this case, $\\hat{\\beta}_3 = 0$ by definition, and no computation is necessary.\n\nThe implementation will use `scipy.optimize.minimize` with the 'BFGS' method, providing both the objective function (negative log-likelihood) and its Jacobian (gradient) for efficiency and accuracy. To ensure numerical stability when computing $\\log(1+e^{\\eta})$ and the sigmoid function, functions from the `scipy.special` module (`logsumexp` and `expit`) will be utilized.", "answer": "```python\nimport numpy as np\nimport scipy.optimize\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression problem for three test cases and computes the\n    engineered interaction feature (MLE of beta_3) for each.\n    \"\"\"\n\n    # --- Case A ---\n    data_A = [\n        (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 1),\n        (1, 0, 0), (1, 0, 0), (1, 0, 1), (1, 0, 0), (1, 0, 1),\n        (0, 1, 0), (0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 1, 1),\n        (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 1),\n    ]\n    S1_A = np.array([d[0] for d in data_A])\n    S2_A = np.array([d[1] for d in data_A])\n    y_A = np.array([d[2] for d in data_A])\n    X_A = np.c_[np.ones(len(S1_A)), S1_A, S2_A, S1_A * S2_A]\n    \n    beta_A = _fit_logistic_regression(X_A, y_A)\n    beta3_A = beta_A[3]\n\n    # --- Case B ---\n    # The data can be aggregated for efficiency\n    # (S1, S2): {N: total count, k: count with y=1}\n    data_B_grouped = {\n        (0, 0): {'N': 10, 'k': 3},\n        (1, 0): {'N': 10, 'k': 5},\n        (0, 1): {'N': 10, 'k': 5},\n        (1, 1): {'N': 10, 'k': 7},\n    }\n    X_unique_B = []\n    N_B, k_B = [], []\n    for (s1, s2), counts in data_B_grouped.items():\n        X_unique_B.append([1, s1, s2, s1 * s2])\n        N_B.append(counts['N'])\n        k_B.append(counts['k'])\n    \n    X_B_agg = np.array(X_unique_B)\n    N_B_agg = np.array(N_B)\n    k_B_agg = np.array(k_B)\n\n    beta_B = _fit_logistic_regression(X_B_agg, k_B_agg, N=N_B_agg, grouped=True)\n    beta3_B = beta_B[3]\n\n    # --- Case C ---\n    # For this case, S2 is always 0. The interaction term S1*S2 is always 0.\n    # The problem statement defines the engineered feature to be 0 in this scenario.\n    beta3_C = 0.0\n\n    results = [beta3_A, beta3_B, beta3_C]\n    \n    # Format the output as specified\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\n\ndef _fit_logistic_regression(X, y, N=None, grouped=False):\n    \"\"\"\n    Fits a logistic regression model and returns the estimated coefficients.\n    Can handle both individual and grouped data.\n    \"\"\"\n    from scipy.special import expit\n\n    num_features = X.shape[1]\n    beta_initial = np.zeros(num_features)\n\n    if grouped:\n        # For grouped data\n        def objective(beta, X_grp, k_grp, N_grp):\n            eta = X_grp @ beta\n            # Numerically stable calculation of log(1 + exp(eta))\n            log_1_plus_exp_eta = np.logaddexp(0, eta)\n            neg_log_likelihood = -np.sum(k_grp * eta - N_grp * log_1_plus_exp_eta)\n            return neg_log_likelihood\n\n        def jacobian(beta, X_grp, k_grp, N_grp):\n            eta = X_grp @ beta\n            p = expit(eta)\n            grad = X_grp.T @ (N_grp * p - k_grp)\n            return grad\n        \n        args = (X, y, N)\n\n    else:\n        # For individual data\n        def objective(beta, X_ind, y_ind):\n            eta = X_ind @ beta\n            log_1_plus_exp_eta = np.logaddexp(0, eta)\n            neg_log_likelihood = -np.sum(y_ind * eta - log_1_plus_exp_eta)\n            return neg_log_likelihood\n\n        def jacobian(beta, X_ind, y_ind):\n            eta = X_ind @ beta\n            p = expit(eta)\n            grad = X_ind.T @ (p - y_ind)\n            return grad\n        \n        args = (X, y)\n\n    result = scipy.optimize.minimize(\n        fun=objective,\n        x0=beta_initial,\n        args=args,\n        method='BFGS',\n        jac=jacobian\n    )\n\n    if not result.success:\n        raise RuntimeError(f\"Optimization failed: {result.message}\")\n\n    return result.x\n\nsolve()\n\n```", "id": "2389799"}]}