{"hands_on_practices": [{"introduction": "To truly understand the Expectation-Maximization algorithm, it's essential to walk through its iterative steps manually. This exercise provides a simplified yet representative bioinformatics scenario: finding a sequence motif. By calculating the posterior probabilities (E-step) and updating the model parameters (M-step) by hand, you will build a concrete intuition for how the EM algorithm uses latent variables to solve complex estimation problems [@problem_id:2388823].", "problem": "A single double-stranded DNA sequence of length $4$ is observed: positions $1$ through $4$ contain the bases A, C, G, T in that order. A probabilistic motif-finding model is specified as follows.\n\n- A motif of length $L=2$ occurs exactly once in the sequence, and there are exactly two candidate motif placements: positions $1$–$2$ or positions $3$–$4$. Let the latent variable $Z \\in \\{1,2\\}$ indicate the motif start position, with $Z=1$ meaning positions $1$–$2$ are the motif and $Z=2$ meaning positions $3$–$4$ are the motif.\n- The background at non-motif positions is independent and identically distributed with a fixed distribution over the deoxyribonucleic acid (DNA) alphabet $\\{ \\text{A}, \\text{C}, \\text{G}, \\text{T} \\}$ given by $b_{\\text{A}}=b_{\\text{C}}=b_{\\text{G}}=b_{\\text{T}}=\\frac{1}{4}$.\n- The motif is modeled by a position weight matrix (PWM) $\\theta$ of size $2 \\times 4$, where each PWM row is a categorical distribution over $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$. Initially ($t=0$), the PWM is\n  - Motif position $1$: $[\\theta_{1,\\text{A}},\\theta_{1,\\text{C}},\\theta_{1,\\text{G}},\\theta_{1,\\text{T}}] = \\left[\\frac{3}{5},\\frac{1}{5},\\frac{1}{10},\\frac{1}{10}\\right]$,\n  - Motif position $2$: $[\\theta_{2,\\text{A}},\\theta_{2,\\text{C}},\\theta_{2,\\text{G}},\\theta_{2,\\text{T}}] = \\left[\\frac{1}{10},\\frac{1}{10},\\frac{3}{5},\\frac{1}{5}\\right]$.\n- The prior mixture weights over the two motif placements are initially $\\pi^{(0)}_1=\\pi^{(0)}_2=\\frac{1}{2}$.\n- The background distribution $b$ is fixed (not updated). No pseudocounts or regularization are used.\n\nTreat the model as a finite mixture with the complete-data likelihood defined in the usual way for independent categorical emissions from the PWM at motif positions and the background at non-motif positions. Manually execute two complete iterations of the Expectation–Maximization (EM) algorithm, that is, perform an E-step followed by an M-step twice, starting from the initial parameters given above.\n\nAfter completing the second E-step, what is the posterior probability that the motif starts at positions $1$–$2$, i.e., $\\Pr(Z=1 \\mid \\text{A C G T}; \\text{parameters after the first M-step})$? Express your answer as a decimal rounded to four significant figures.", "solution": "The problem is well-defined, scientifically grounded, and contains all necessary components to perform the required calculations. It is a standard application of the Expectation–Maximization (EM) algorithm to the problem of motif finding in bioinformatics. The problem is therefore deemed valid and will be solved as stated.\n\nThe Expectation–Maximization (EM) algorithm is an iterative method to find maximum likelihood estimates of parameters in statistical models where the model depends on unobserved latent variables. Each iteration consists of two steps: an Expectation (E) step and a Maximization (M) step.\n\nLet the observed data be the DNA sequence $X = (X_1, X_2, X_3, X_4) = (\\text{A}, \\text{C}, \\text{G}, \\text{T})$. The latent variable $Z \\in \\{1, 2\\}$ indicates the starting position of the motif. The parameters of the model are $\\Theta = (\\pi, \\theta)$, where $\\pi = (\\pi_1, \\pi_2)$ are the mixture weights and $\\theta$ is the position weight matrix (PWM) for the motif of length $L=2$.\n\nThe complete-data likelihood for a single observation $X$ and a specific value $z$ for the latent variable $Z$ is $\\Pr(X, Z=z; \\Theta) = \\Pr(X \\mid Z=z; \\theta) \\Pr(Z=z; \\pi) = \\pi_z \\Pr(X \\mid Z=z; \\theta)$.\n\nThe EM algorithm proceeds as follows:\n1.  **E-step**: Calculate the posterior probability, or responsibility, of each mixture component $z$ given the data $X$ and the current parameters $\\Theta^{(t)}$.\n    $$ \\gamma_z^{(t)} = \\Pr(Z=z \\mid X; \\Theta^{(t)}) = \\frac{\\pi_z^{(t)} \\Pr(X \\mid Z=z; \\theta^{(t)})}{\\sum_{z'} \\pi_{z'}^{(t)} \\Pr(X \\mid Z=z'; \\theta^{(t)})} $$\n2.  **M-step**: Update the parameters to $\\Theta^{(t+1)}$ by maximizing the expected value of the complete-data log-likelihood, using the responsibilities calculated in the E-step. For this specific problem:\n    $$ \\pi_z^{(t+1)} = \\gamma_z^{(t)} $$\n    The PWM parameters $\\theta_{j,k}$ are updated by calculating the expected count of each base $k$ at each motif position $j$, $C_{j,k} = \\sum_z \\gamma_z^{(t)} I(X_{s_z + j-1} = k)$, and then normalizing:\n    $$ \\theta_{j,k}^{(t+1)} = \\frac{C_{j,k}}{\\sum_{k'} C_{j,k'}} $$\n    where $I(\\cdot)$ is the indicator function, $s_z$ is the start position for component $z$ ($s_1=1, s_2=3$), $j \\in \\{1, 2\\}$ is the position within the motif, and $k \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$.\n\nWe start with the initial parameters at $t=0$:\n- Sequence: $X = (\\text{A}, \\text{C}, \\text{G}, \\text{T})$\n- Mixture weights: $\\pi^{(0)}_1 = \\frac{1}{2}, \\pi^{(0)}_2 = \\frac{1}{2}$\n- Background distribution: $b_k = \\frac{1}{4}$ for $k \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$\n- PWM $\\theta^{(0)}$:\n  - $\\theta^{(0)}_{1,\\cdot} = [\\theta_{1,\\text{A}},\\theta_{1,\\text{C}},\\theta_{1,\\text{G}},\\theta_{1,\\text{T}}] = \\left[\\frac{3}{5},\\frac{1}{5},\\frac{1}{10},\\frac{1}{10}\\right]$\n  - $\\theta^{(0)}_{2,\\cdot} = [\\theta_{2,\\text{A}},\\theta_{2,\\text{C}},\\theta_{2,\\text{G}},\\theta_{2,\\text{T}}] = \\left[\\frac{1}{10},\\frac{1}{10},\\frac{3}{5},\\frac{1}{5}\\right]$\n\n**Iteration 1: E-Step**\n\nFirst, we compute the likelihood of the data for each possible motif placement.\n- If $Z=1$, the motif is at positions $1-2$ (`AC`) and the background is at positions $3-4$ (`GT`). The likelihood is:\n  $$ \\Pr(X \\mid Z=1; \\theta^{(0)}) = (\\theta^{(0)}_{1,\\text{A}} \\cdot \\theta^{(0)}_{2,\\text{C}}) \\cdot (b_{\\text{G}} \\cdot b_{\\text{T}}) = \\left(\\frac{3}{5} \\cdot \\frac{1}{10}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{3}{50} \\cdot \\frac{1}{16} = \\frac{3}{800} $$\n- If $Z=2$, the motif is at positions $3-4$ (`GT`) and the background is at positions $1-2$ (`AC`). The likelihood is:\n  $$ \\Pr(X \\mid Z=2; \\theta^{(0)}) = (\\theta^{(0)}_{1,\\text{G}} \\cdot \\theta^{(0)}_{2,\\text{T}}) \\cdot (b_{\\text{A}} \\cdot b_{\\text{C}}) = \\left(\\frac{1}{10} \\cdot \\frac{1}{5}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{1}{50} \\cdot \\frac{1}{16} = \\frac{1}{800} $$\n\nNext, we compute the joint probabilities and the marginal likelihood:\n- $\\Pr(X, Z=1; \\Theta^{(0)}) = \\pi^{(0)}_1 \\Pr(X \\mid Z=1; \\theta^{(0)}) = \\frac{1}{2} \\cdot \\frac{3}{800} = \\frac{3}{1600}$\n- $\\Pr(X, Z=2; \\Theta^{(0)}) = \\pi^{(0)}_2 \\Pr(X \\mid Z=2; \\theta^{(0)}) = \\frac{1}{2} \\cdot \\frac{1}{800} = \\frac{1}{1600}$\n- $\\Pr(X; \\Theta^{(0)}) = \\frac{3}{1600} + \\frac{1}{1600} = \\frac{4}{1600} = \\frac{1}{400}$\n\nNow, we calculate the responsibilities $\\gamma_z^{(0)}$:\n- $\\gamma_1^{(0)} = \\Pr(Z=1 \\mid X; \\Theta^{(0)}) = \\frac{3/1600}{4/1600} = \\frac{3}{4}$\n- $\\gamma_2^{(0)} = \\Pr(Z=2 \\mid X; \\Theta^{(0)}) = \\frac{1/1600}{4/1600} = \\frac{1}{4}$\n\n**Iteration 1: M-Step**\n\nWe update the parameters using the responsibilities $\\gamma_1^{(0)}$ and $\\gamma_2^{(0)}$.\n- Update mixture weights:\n  $$ \\pi_1^{(1)} = \\gamma_1^{(0)} = \\frac{3}{4} $$\n  $$ \\pi_2^{(1)} = \\gamma_2^{(0)} = \\frac{1}{4} $$\n- Update PWM $\\theta^{(1)}$:\n  For motif position $j=1$:\n  - $\\theta_{1,\\text{A}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_1=\\text{A}) + \\gamma_2^{(0)} \\cdot I(X_3=\\text{A}) = \\frac{3}{4} \\cdot 1 + \\frac{1}{4} \\cdot 0 = \\frac{3}{4}$\n  - $\\theta_{1,\\text{C}}^{(1)} = 0$\n  - $\\theta_{1,\\text{G}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_1=\\text{G}) + \\gamma_2^{(0)} \\cdot I(X_3=\\text{G}) = \\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 1 = \\frac{1}{4}$\n  - $\\theta_{1,\\text{T}}^{(1)} = 0$\n  Since the sum of expected counts is $3/4+1/4=1$, these are the final probabilities.\n\n  For motif position $j=2$:\n  - $\\theta_{2,\\text{A}}^{(1)} = 0$\n  - $\\theta_{2,\\text{C}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_2=\\text{C}) + \\gamma_2^{(0)} \\cdot I(X_4=\\text{C}) = \\frac{3}{4} \\cdot 1 + \\frac{1}{4} \\cdot 0 = \\frac{3}{4}$\n  - $\\theta_{2,\\text{G}}^{(1)} = 0$\n  - $\\theta_{2,\\text{T}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_2=\\text{T}) + \\gamma_2^{(0)} \\cdot I(X_4=\\text{T}) = \\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 1 = \\frac{1}{4}$\n  Similarly, the sum of expected counts is $3/4+1/4=1$, so these are the final probabilities.\n\nThe parameters after one full iteration are $\\Theta^{(1)} = (\\pi^{(1)}, \\theta^{(1)})$.\n\n**Iteration 2: E-Step**\n\nThe problem asks for the posterior probability $\\Pr(Z=1 \\mid X; \\Theta^{(1)})$, which is the responsibility $\\gamma_1^{(1)}$ calculated in the second E-step. We use the updated parameters $\\pi^{(1)}$ and $\\theta^{(1)}$.\n\nFirst, we re-compute the likelihoods with $\\theta^{(1)}$:\n- If $Z=1$:\n  $$ \\Pr(X \\mid Z=1; \\theta^{(1)}) = (\\theta^{(1)}_{1,\\text{A}} \\cdot \\theta^{(1)}_{2,\\text{C}}) \\cdot (b_{\\text{G}} \\cdot b_{\\text{T}}) = \\left(\\frac{3}{4} \\cdot \\frac{3}{4}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{9}{16} \\cdot \\frac{1}{16} = \\frac{9}{256} $$\n- If $Z=2$:\n  $$ \\Pr(X \\mid Z=2; \\theta^{(1)}) = (\\theta^{(1)}_{1,\\text{G}} \\cdot \\theta^{(1)}_{2,\\text{T}}) \\cdot (b_{\\text{A}} \\cdot b_{\\text{C}}) = \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{1}{16} \\cdot \\frac{1}{16} = \\frac{1}{256} $$\n\nNext, we compute the joint probabilities using $\\pi^{(1)}$:\n- $\\Pr(X, Z=1; \\Theta^{(1)}) = \\pi^{(1)}_1 \\Pr(X \\mid Z=1; \\theta^{(1)}) = \\frac{3}{4} \\cdot \\frac{9}{256} = \\frac{27}{1024}$\n- $\\Pr(X, Z=2; \\Theta^{(1)}) = \\pi^{(1)}_2 \\Pr(X \\mid Z=2; \\theta^{(1)}) = \\frac{1}{4} \\cdot \\frac{1}{256} = \\frac{1}{1024}$\n\nThe new marginal likelihood is:\n- $\\Pr(X; \\Theta^{(1)}) = \\frac{27}{1024} + \\frac{1}{1024} = \\frac{28}{1024}$\n\nFinally, the new responsibilities $\\gamma_z^{(1)}$ are:\n- $\\gamma_1^{(1)} = \\Pr(Z=1 \\mid X; \\Theta^{(1)}) = \\frac{27/1024}{28/1024} = \\frac{27}{28}$\n- $\\gamma_2^{(1)} = \\Pr(Z=2 \\mid X; \\Theta^{(1)}) = \\frac{1/1024}{28/1024} = \\frac{1}{28}$\n\nThe posterior probability that the motif starts at positions $1–2$ is $\\gamma_1^{(1)} = \\frac{27}{28}$. Converting this to a decimal and rounding to four significant figures:\n$$ \\frac{27}{28} \\approx 0.9642857... \\approx 0.9643 $$", "answer": "$$\\boxed{0.9643}$$", "id": "2388823"}, {"introduction": "The EM algorithm is a powerful framework, not just a single fixed recipe. This problem challenges you to think about how to adapt the standard algorithm for a Gaussian mixture model to handle a common real-world issue: outliers. By considering how to incorporate a \"junk\" component with a fixed density, you will deepen your understanding of how the E-step and M-step are fundamentally derived from the underlying probabilistic model [@problem_id:2388734].", "problem": "You are analyzing single-cell gene expression profiles from Single-Cell Ribonucleic Acid sequencing (scRNA-seq), represented as vectors $\\mathbf{x}_1,\\dots,\\mathbf{x}_N \\in \\mathbb{R}^d$. You posit that most cells arise from $K$ biological states modeled by a Gaussian mixture, but a small fraction are damaged or contaminated cells that behave as outliers. To capture these, you propose a mixture model with $K$ Gaussian components and a single “junk” outlier component with a fixed, known density $u(\\mathbf{x})$ (for example, a uniform density over a pre-specified hyper-rectangle $\\mathcal{S} \\subset \\mathbb{R}^d$ covering the observed range). The resulting mixture density is\n$$\np(\\mathbf{x}\\mid \\Theta) \\;=\\; \\sum_{k=1}^K \\pi_k \\,\\mathcal{N}\\!\\big(\\mathbf{x}; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)\\;+\\;\\pi_{K+1}\\,u(\\mathbf{x}),\n$$\nwith mixing proportions $\\pi_k \\ge 0$ and $\\sum_{k=1}^{K+1} \\pi_k = 1$, where $\\Theta = \\{\\pi_1,\\dots,\\pi_{K+1}, \\boldsymbol{\\mu}_1,\\dots,\\boldsymbol{\\mu}_K, \\boldsymbol{\\Sigma}_1,\\dots,\\boldsymbol{\\Sigma}_K\\}$ and the outlier density $u(\\mathbf{x})$ has no free parameters to estimate. You wish to maximize the likelihood using the Expectation–Maximization (EM) algorithm.\n\nWhich option correctly specifies the modifications to the EM algorithm (both the E-step responsibilities and the M-step parameter updates) that are necessary to include this outlier component?\n\nA. In the E-step, compute responsibilities for all $K\\!+\\!1$ components by Bayes’ rule:\n$$\nr_{ik} \\;=\\; \\frac{\\pi_k\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)}{\\sum_{j=1}^K \\pi_j\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j\\big) \\;+\\; \\pi_{K+1}\\,u(\\mathbf{x}_i)} \\quad \\text{for } k\\in\\{1,\\dots,K\\},\n$$\nand\n$$\nr_{i,K+1} \\;=\\; \\frac{\\pi_{K+1}\\,u(\\mathbf{x}_i)}{\\sum_{j=1}^K \\pi_j\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j\\big) \\;+\\; \\pi_{K+1}\\,u(\\mathbf{x}_i)}.\n$$\nIn the M-step, update all mixing proportions by $\\pi_k \\leftarrow \\frac{1}{N}\\sum_{i=1}^N r_{ik}$ for $k\\in\\{1,\\dots,K+1\\}$; update $\\boldsymbol{\\mu}_k$ and $\\boldsymbol{\\Sigma}_k$ for $k\\in\\{1,\\dots,K\\}$ by the usual responsibility-weighted formulas; do not update any parameters for $u(\\mathbf{x})$ since it is fixed.\n\nB. In the E-step, compute responsibilities only over the $K$ Gaussian components as\n$$\nr_{ik} \\;=\\; \\frac{\\pi_k\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)}{\\sum_{j=1}^K \\pi_j\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j\\big)}, \\quad k\\in\\{1,\\dots,K\\},\n$$\nand set $r_{i,K+1} \\leftarrow 1 - \\max_{k\\in\\{1,\\dots,K\\}} r_{ik}$. In the M-step, update $\\pi_k$ only for $k\\in\\{1,\\dots,K\\}$ and keep $\\pi_{K+1}$ fixed to a small constant; update $\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k$ as in a standard Gaussian mixture.\n\nC. In the E-step, compute responsibilities as in a standard $K$-component Gaussian mixture without the junk component. Then, hard-threshold low responsibilities by setting $r_{ik}\\leftarrow 0$ if $r_{ik}<\\varepsilon$ for a fixed $\\varepsilon>0$, reassign the mass $\\varepsilon$ to the junk component, and renormalize over $k\\in\\{1,\\dots,K\\!+\\!1\\}$. In the M-step, update only the $K$ Gaussian components using the thresholded responsibilities.\n\nD. In the E-step, include the junk component with the same responsibility formula as the Gaussian components by treating $u(\\mathbf{x})$ as a Gaussian density with parameters to be estimated. In the M-step, update $\\pi_k$, $\\boldsymbol{\\mu}_k$, and $\\boldsymbol{\\Sigma}_k$ for all $k\\in\\{1,\\dots,K\\!+\\!1\\}$ using the standard Gaussian mixture updates, allowing the junk component to adapt its mean and covariance.\n\nE. In the E-step, include the junk component as in option A. In the M-step, update the support $\\mathcal{S}$ of the uniform density $u(\\mathbf{x})$ by maximum likelihood jointly with $\\pi_{K+1}$, shrinking $\\mathcal{S}$ around high-residual points, while updating the Gaussian parameters as usual.", "solution": "We start from the definition of a finite mixture model with latent indicator variables. Introduce latent one-hot variables $Z_{ik}\\in\\{0,1\\}$ with $\\sum_{k=1}^{K+1} Z_{ik}=1$, where $Z_{ik}=1$ indicates that $\\mathbf{x}_i$ arises from component $k$. The complete-data likelihood factorizes as\n$$\np(\\{\\mathbf{x}_i,Z_i\\}_{i=1}^N \\mid \\Theta) \\;=\\; \\prod_{i=1}^N \\prod_{k=1}^{K} \\left[\\pi_k \\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)\\right]^{Z_{ik}} \\;\\times\\; \\prod_{i=1}^N \\left[\\pi_{K+1}\\,u(\\mathbf{x}_i)\\right]^{Z_{i,K+1}}.\n$$\nThe Expectation–Maximization (EM) algorithm alternates between:\n\nE-step: Compute the posterior responsibilities\n$$\nr_{ik} \\;=\\; \\mathbb{E}[Z_{ik}\\mid \\mathbf{x}_i,\\Theta] \\;=\\; \\frac{\\pi_k\\,f_k(\\mathbf{x}_i)}{\\sum_{j=1}^{K+1} \\pi_j\\,f_j(\\mathbf{x}_i)},\n$$\nwhere $f_k(\\mathbf{x})=\\mathcal{N}\\!\\big(\\mathbf{x};\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)$ for $k\\in\\{1,\\dots,K\\}$ and $f_{K+1}(\\mathbf{x})=u(\\mathbf{x})$ for the fixed junk component. This yields, explicitly,\n$$\nr_{ik} \\;=\\; \\frac{\\pi_k\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)}{\\sum_{j=1}^K \\pi_j\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j\\big) + \\pi_{K+1}\\,u(\\mathbf{x}_i)} \\quad \\text{for } k\\in\\{1,\\dots,K\\},\n$$\nand\n$$\nr_{i,K+1} \\;=\\; \\frac{\\pi_{K+1}\\,u(\\mathbf{x}_i)}{\\sum_{j=1}^K \\pi_j\\,\\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_j,\\boldsymbol{\\Sigma}_j\\big) + \\pi_{K+1}\\,u(\\mathbf{x}_i)}.\n$$\nBy construction, $\\sum_{k=1}^{K+1} r_{ik}=1$ for each $i$.\n\nM-step: Maximize the expected complete-data log-likelihood\n$$\nQ(\\Theta) \\;=\\; \\sum_{i=1}^N \\sum_{k=1}^{K} r_{ik}\\,\\Big(\\log \\pi_k + \\log \\mathcal{N}\\!\\big(\\mathbf{x}_i;\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k\\big)\\Big) \\;+\\; \\sum_{i=1}^N r_{i,K+1}\\,\\Big(\\log \\pi_{K+1} + \\log u(\\mathbf{x}_i)\\Big),\n$$\nsubject to $\\sum_{k=1}^{K+1} \\pi_k = 1$ and $\\pi_k\\ge 0$. The terms involving $u(\\mathbf{x}_i)$ have no free parameters other than $\\pi_{K+1}$ because $u(\\mathbf{x})$ is fixed and known. Taking derivatives with respect to $\\pi_k$ with a Lagrange multiplier for the simplex constraint yields the standard update\n$$\n\\pi_k \\;\\leftarrow\\; \\frac{1}{N}\\sum_{i=1}^N r_{ik} \\quad \\text{for all } k\\in\\{1,\\dots,K+1\\}.\n$$\nFor the Gaussian components $k\\in\\{1,\\dots,K\\}$, maximizing $Q(\\Theta)$ with respect to $\\boldsymbol{\\mu}_k$ and $\\boldsymbol{\\Sigma}_k$ gives the usual responsibility-weighted updates,\n$$\n\\boldsymbol{\\mu}_k \\;\\leftarrow\\; \\frac{\\sum_{i=1}^N r_{ik}\\,\\mathbf{x}_i}{\\sum_{i=1}^N r_{ik}}, \n\\qquad\n\\boldsymbol{\\Sigma}_k \\;\\leftarrow\\; \\frac{\\sum_{i=1}^N r_{ik}\\,(\\mathbf{x}_i-\\boldsymbol{\\mu}_k)(\\mathbf{x}_i-\\boldsymbol{\\mu}_k)^{\\top}}{\\sum_{i=1}^N r_{ik}}.\n$$\nThere is no update for any parameter of $u(\\mathbf{x})$ beyond its mixing weight because by assumption $u(\\mathbf{x})$ is known and fixed.\n\nWe now assess each option:\n\nOption A: This matches the derivation above. The E-step computes posterior responsibilities for all $K\\!+\\!1$ components including the fixed-density junk component, with normalization over all components. The M-step updates mixing proportions for all components using the average responsibilities, updates Gaussian parameters for $k\\in\\{1,\\dots,K\\}$ via weighted formulas, and leaves the fixed $u(\\mathbf{x})$ unchanged. Verdict — Correct.\n\nOption B: This excludes the junk component from the normalization in the E-step and then defines $r_{i,K+1}$ via $1-\\max_k r_{ik}$, which is not Bayes-consistent and does not yield $\\sum_{k} r_{ik}=1$ in general. Keeping $\\pi_{K+1}$ fixed breaks the maximum-likelihood update for mixing proportions. Verdict — Incorrect.\n\nOption C: This proposes ad hoc thresholding after a standard Gaussian mixture E-step, then assigns mass to the junk component. This does not correspond to the expectation step derived from the specified likelihood and breaks the ascent property of the Expectation–Maximization algorithm. Verdict — Incorrect.\n\nOption D: This treats the junk component as another Gaussian with free parameters. That changes the model class (it is no longer a fixed-density outlier component) and will typically fit an additional cluster rather than a broad low-informative outlier distribution, failing the intended outlier detection behavior. It also contradicts the assumption that $u(\\mathbf{x})$ is fixed and known. Verdict — Incorrect.\n\nOption E: This attempts to update the support of the uniform density by maximum likelihood. For a uniform density, unconstrained maximum likelihood collapses the support tightly around assigned points, leading to degeneracy and not a fixed broad “junk” component. It violates the stated assumption that $u(\\mathbf{x})$ has no free parameters to estimate. Verdict — Incorrect.\n\nTherefore, the only option that correctly modifies the Expectation–Maximization algorithm to include a fixed “junk” outlier component is option A.", "answer": "$$\\boxed{A}$$", "id": "2388734"}, {"introduction": "While the EM algorithm is guaranteed to improve the likelihood at each step, it is not guaranteed to find the single best solution. This practical coding exercise demonstrates one of EM's most important characteristics: its convergence to a local maximum, which is highly dependent on the initial parameters. By implementing EM for a gene expression clustering task and observing the results from different starting points, you will gain critical insight into the practical challenges and behaviors of the algorithm [@problem_id:2388760].", "problem": "You are given a one-dimensional gene expression setting in which each observation is a real value representing a log-transformed expression level. Consider the finite mixture model with exactly $2$ Gaussian components and a known common variance $\\sigma^2$. Let the parameter vector be $\\theta = (\\pi_1,\\pi_2,\\mu_1,\\mu_2)$ with $\\pi_k \\in (0,1)$, $\\pi_1+\\pi_2=1$, and $\\mu_k \\in \\mathbb{R}$ for $k \\in \\{1,2\\}$. For any $\\theta$, the observed-data likelihood for $n$ independent observations $x_1,\\dots,x_n$ is\n$$\n\\mathcal{L}(\\theta; x_{1:n}) \\;=\\; \\prod_{i=1}^n \\left[ \\pi_1 \\,\\phi(x_i;\\mu_1,\\sigma^2) + \\pi_2 \\,\\phi(x_i;\\mu_2,\\sigma^2) \\right],\n$$\nwhere $\\phi(x;\\mu,\\sigma^2)$ denotes the Gaussian density with mean $\\mu$ and variance $\\sigma^2$. The log-likelihood is $\\ell(\\theta; x_{1:n}) = \\log \\mathcal{L}(\\theta; x_{1:n})$.\n\nA dataset is termed “biologically correct” for this model if there exists a parameter vector $\\theta^\\star$ with mixing weights strictly between $0$ and $1$ and distinct means such that the resulting maximum of the observed-data log-likelihood corresponds to separating two known biological groups of samples. Concretely, let $y_i \\in \\{0,1\\}$ denote the biological group label for $x_i$, with $y_i=0$ for group $A$ and $y_i=1$ for group $B$. For any parameter $\\theta$, define the posterior component membership probabilities\n$$\n\\gamma_{ik}(\\theta) \\;=\\; \\frac{\\pi_k\\,\\phi(x_i;\\mu_k,\\sigma^2)}{\\pi_1\\,\\phi(x_i;\\mu_1,\\sigma^2)+\\pi_2\\,\\phi(x_i;\\mu_2,\\sigma^2)}, \\quad k\\in\\{1,2\\},\n$$\nand the hard assignment $\\hat{z}_i(\\theta) = \\arg\\max_{k\\in\\{1,2\\}} \\gamma_{ik}(\\theta)$. Since mixture labels are non-identifiable, define the label-aligned predicted class $\\hat{y}_i(\\theta)$ by the permutation of $\\{0,1\\}$ that minimizes the misclassification count when comparing $\\hat{z}_i(\\theta)$ to $y_i$. The misclassification fraction is\n$$\n\\mathrm{Err}(\\theta) \\;=\\; \\frac{1}{n}\\,\\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i(\\theta) \\neq y_i \\right\\}.\n$$\nDefine the minority-class recall as\n$$\n\\mathrm{Rec}_{\\text{minor}}(\\theta) \\;=\\; \\frac{\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\}\\,\\mathbf{1}\\{\\hat{y}_i(\\theta)=1\\}}{\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\}},\n$$\nwith the convention that if the denominator is $0$, the recall is defined as $1$.\n\nYour task is to compute, for each test case below, a parameter vector $\\hat{\\theta}$ that is a stationary point of the observed-data log-likelihood for the specified dataset, starting from the provided initial parameter vector, and using the known variance $\\sigma^2$. Approximate stationarity must be certified by the condition that the absolute change in the observed-data log-likelihood between successive iterates is less than $\\varepsilon = 10^{-8}$. Then, report the pair $[\\mathrm{Err}(\\hat{\\theta}), \\mathrm{Rec}_{\\text{minor}}(\\hat{\\theta})]$ for each test case.\n\nAll datasets are generated by the following reproducible rule. For given integers $n_A \\ge 1$ and $n_B \\ge 1$, real means $m_A,m_B$, variance $\\sigma^2>0$, and seed $s$, first set the random number generator to the seed $s$. Then draw $n_A$ independent samples $x_i \\sim \\mathcal{N}(m_A,\\sigma^2)$ with labels $y_i=0$ (group $A$), and $n_B$ independent samples $x_i \\sim \\mathcal{N}(m_B,\\sigma^2)$ with labels $y_i=1$ (group $B$). Concatenate the $n=n_A+n_B$ samples into a single dataset $\\{(x_i,y_i)\\}_{i=1}^n$ in the order generated.\n\nUse the common known variance $\\sigma^2 = 1$. The test suite consists of three cases:\n\n- Test case $1$ (well-separated, balanced initialization):\n  - Data: $n_A=200$, $n_B=5$, $m_A=0$, $m_B=6$, seed $s=31415$.\n  - Initial parameter: $\\theta^{(0)} = (\\pi_1^{(0)},\\pi_2^{(0)},\\mu_1^{(0)},\\mu_2^{(0)}) = (0.5,\\,0.5,\\,-0.5,\\,6.5)$.\n\n- Test case $2$ (same data, imbalanced initialization expected to reach a biologically incorrect local maximum):\n  - Data: identical to Test case $1$.\n  - Initial parameter: $\\theta^{(0)} = (0.99,\\,0.01,\\,0.2,\\,-0.2)$.\n\n- Test case $3$ (boundary minority, extreme imbalance):\n  - Data: $n_A=100$, $n_B=1$, $m_A=0$, $m_B=6$, seed $s=27182$.\n  - Initial parameter: $\\theta^{(0)} = (0.999,\\,0.001,\\,0.1,\\,-0.1)$.\n\nYour program must, for each test case, compute a parameter vector $\\hat{\\theta}$ that satisfies the above approximate stationarity criterion with $\\varepsilon = 10^{-8}$, and then compute and return the two scalar metrics $[\\mathrm{Err}(\\hat{\\theta}), \\mathrm{Rec}_{\\text{minor}}(\\hat{\\theta})]$ as real numbers.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of three items, where each item is itself a bracketed, comma-separated pair of real numbers corresponding to a test case. For example, a syntactically valid output looks like\n\"[a1,b1],[a2,b2],[a3,b3]\"\nbut you must output the actual computed values for the three test cases in the exact same single-line format, with no surrounding outer brackets and no extra text.", "solution": "The problem statement is subjected to rigorous validation before a solution is attempted.\n\n**Step 1: Extract Givens**\n- **Model**: A one-dimensional finite mixture model with $K=2$ Gaussian components. The common variance $\\sigma^2$ is known.\n- **Parameter Vector**: $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$, where $\\pi_k \\in (0,1)$ are mixing weights with $\\pi_1 + \\pi_2 = 1$, and $\\mu_k \\in \\mathbb{R}$ are the component means.\n- **Known Variance**: $\\sigma^2 = 1$.\n- **Observed-Data Likelihood**: For observations $x_{1:n} = \\{x_1, \\dots, x_n\\}$, $\\mathcal{L}(\\theta; x_{1:n}) = \\prod_{i=1}^n [ \\pi_1 \\phi(x_i;\\mu_1,\\sigma^2) + \\pi_2 \\phi(x_i;\\mu_2,\\sigma^2) ]$, where $\\phi(x;\\mu,\\sigma^2)$ is the Gaussian probability density function.\n- **Log-Likelihood**: $\\ell(\\theta; x_{1:n}) = \\log \\mathcal{L}(\\theta; x_{1:n})$.\n- **Posterior Membership Probability (Responsibility)**: $\\gamma_{ik}(\\theta) = \\frac{\\pi_k\\,\\phi(x_i;\\mu_k,\\sigma^2)}{\\sum_{j=1}^2 \\pi_j\\,\\phi(x_i;\\mu_j,\\sigma^2)}$.\n- **Hard Assignment**: $\\hat{z}_i(\\theta) = \\arg\\max_{k\\in\\{1,2\\}} \\gamma_{ik}(\\theta)$.\n- **Metrics**: Misclassification fraction $\\mathrm{Err}(\\theta)$ and minority-class recall $\\mathrm{Rec}_{\\text{minor}}(\\theta)$, computed after aligning predicted labels $\\hat{z}_i$ with true labels $y_i \\in \\{0,1\\}$ to minimize misclassification.\n- **Task**: For each test case, find a stationary point $\\hat{\\theta}$ of the log-likelihood using the Expectation-Maximization (EM) algorithm, starting from a given $\\theta^{(0)}$.\n- **Convergence Criterion**: The algorithm terminates when the absolute change in log-likelihood between successive iterations is less than $\\varepsilon = 10^{-8}$.\n- **Data Generation**: Datasets are generated from two normal distributions, $\\mathcal{N}(m_A, \\sigma^2)$ for group $A$ ($y_i=0$) and $\\mathcal{N}(m_B, \\sigma^2)$ for group $B$ ($y_i=1$), with specified sample sizes $n_A, n_B$, means $m_A, m_B$, variance $\\sigma^2=1$, and random seed $s$.\n- **Test Cases**: Three specific cases are defined with their data generation parameters and initial parameter vectors $\\theta^{(0)}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on the Gaussian Mixture Model (GMM) and the Expectation-Maximization (EM) algorithm, which are fundamental and widely applied concepts in statistics and machine learning, particularly in computational biology for clustering tasks. The scientific foundation is impeccable.\n- **Well-Posedness**: The objective is to find a stationary point of the likelihood function. The EM algorithm is a standard, provably convergent (to a local maximum or saddle point) method for this task. The problem is well-defined and has a clear, achievable objective.\n- **Objectivity**: The problem is stated in precise mathematical terms. All quantities are defined unambiguously. Data generation is specified through a reproducible procedure. There is no subjective or opinion-based content.\n- **Completeness**: All necessary information is provided: the model, the algorithm to use, initial conditions, convergence criteria, data generation protocols, and evaluation metrics. The problem is self-contained.\n- **Consistency and Feasibility**: There are no contradictions in the provided information. The parameters for data generation and initializations are numerically reasonable and do not pose any immediate issues of physical or computational infeasibility.\n\n**Step 3: Verdict and Action**\nThe problem is determined to be **valid**. It is a well-posed, scientifically sound problem in computational statistics that is directly relevant to the specified topic. A formal solution will be developed.\n\nThe problem requires finding a stationary point of the observed-data log-likelihood for a Gaussian mixture model. The standard and appropriate methodology for this task is the Expectation-Maximization (EM) algorithm. The EM algorithm is an iterative procedure that finds maximum likelihood estimates of parameters in statistical models with latent variables.\n\nIn our context, the latent variables are the component assignments for each observation. Let $z_i \\in \\{1, 2\\}$ be a latent variable indicating which of the two Gaussian components generated the observation $x_i$. The complete-data likelihood, assuming the $(x_i, z_i)$ pairs are known, would be:\n$$\n\\mathcal{L}_c(\\theta; x_{1:n}, z_{1:n}) = \\prod_{i=1}^n \\left[ \\pi_{z_i} \\phi(x_i; \\mu_{z_i}, \\sigma^2) \\right]\n$$\nThe corresponding complete-data log-likelihood is:\n$$\n\\ell_c(\\theta; x_{1:n}, z_{1:n}) = \\sum_{i=1}^n \\left[ \\log \\pi_{z_i} + \\log \\phi(x_i; \\mu_{z_i}, \\sigma^2) \\right]\n$$\nThe EM algorithm alternates between two steps: the Expectation (E) step and the Maximization (M) step. Let $\\theta^{(t)} = (\\pi_1^{(t)}, \\pi_2^{(t)}, \\mu_1^{(t)}, \\mu_2^{(t)})$ be the parameter estimates at iteration $t$.\n\n**E-step (Expectation):**\nIn this step, we compute the expectation of the complete-data log-likelihood with respect to the conditional distribution of the latent variables $z_i$ given the observed data $x_{1:n}$ and the current parameter estimates $\\theta^{(t)}$. This is equivalent to computing the posterior probabilities of component membership, which are termed responsibilities. For each observation $i$ and component $k \\in \\{1, 2\\}$, the responsibility $\\gamma_{ik}^{(t)}$ is:\n$$\n\\gamma_{ik}^{(t)} \\equiv \\mathrm{P}(z_i = k | x_i, \\theta^{(t)}) = \\frac{\\pi_k^{(t)} \\phi(x_i; \\mu_k^{(t)}, \\sigma^2)}{\\sum_{j=1}^2 \\pi_j^{(t)} \\phi(x_i; \\mu_j^{(t)}, \\sigma^2)}\n$$\nThis quantity represents the \"soft assignment\" of observation $i$ to component $k$.\n\n**M-step (Maximization):**\nIn this step, we find the parameters $\\theta^{(t+1)}$ that maximize the expected complete-data log-likelihood (the Q-function) computed in the E-step. This maximization yields the following update rules.\nThe update for the mixing weights $\\pi_k$ is the average responsibility for each component over all data points:\n$$\n\\pi_k^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n \\gamma_{ik}^{(t)}\n$$\nThe update for the means $\\mu_k$ is a weighted average of the data points, where the weights are the responsibilities:\n$$\n\\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^n \\gamma_{ik}^{(t)} x_i}{\\sum_{i=1}^n \\gamma_{ik}^{(t)}}\n$$\nThe variance $\\sigma^2$ is known and fixed at $\\sigma^2=1$, so it is not updated.\n\n**Iterative Procedure and Convergence:**\nThe algorithm starts with an initial parameter guess $\\theta^{(0)}$. The E and M steps are repeated, generating a sequence of parameter estimates $\\theta^{(1)}, \\theta^{(2)}, \\dots$. The EM algorithm guarantees that the observed-data log-likelihood is non-decreasing at each iteration, i.e., $\\ell(\\theta^{(t+1)}; x_{1:n}) \\ge \\ell(\\theta^{(t)}; x_{1:n})$. We monitor the change in the observed-data log-likelihood, $\\ell(\\theta; x_{1:n}) = \\sum_{i=1}^n \\log(\\sum_{k=1}^2 \\pi_k \\phi(x_i; \\mu_k, \\sigma^2))$, and terminate when the absolute change falls below a specified tolerance $\\varepsilon = 10^{-8}$. The final parameters are denoted $\\hat{\\theta}$.\n\n**Evaluation Metrics:**\nOnce the algorithm converges to $\\hat{\\theta}$, we evaluate its performance.\nFirst, we compute the final responsibilities $\\gamma_{ik}(\\hat{\\theta})$ and perform a hard assignment for each data point $i$ to the component with the highest responsibility: $\\hat{z}_i = \\arg\\max_{k \\in \\{1,2\\}} \\gamma_{ik}(\\hat{\\theta})$. For implementation, we map $\\{1,2\\}$ to $\\{0,1\\}$.\nSecond, due to the non-identifiability of mixture components (label switching), we must align the predicted cluster labels $\\hat{z}_i$ with the true biological labels $y_i$. We consider two possible mappings: the identity $(\\hat{z}_i \\to \\hat{z}_i)$ and the switched mapping $(\\hat{z}_i \\to 1-\\hat{z}_i)$. We choose the mapping that minimizes the total number of misclassifications, yielding the aligned predictions $\\hat{y}_i$.\nThe misclassification fraction is then $\\mathrm{Err}(\\hat{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i \\neq y_i\\}$.\nThe minority-class recall, for the class where $y_i=1$, is computed as $\\mathrm{Rec}_{\\text{minor}}(\\hat{\\theta}) = (\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\} \\mathbf{1}\\{\\hat{y}_i=1\\}) / (\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\})$.\n\nThe following implementation will execute this complete procedure for each of the three test cases.", "answer": "```python\nimport numpy as np\n\ndef generate_data(n_A, n_B, m_A, m_B, sigma2, seed):\n    \"\"\"\n    Generates a dataset based on the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    sigma = np.sqrt(sigma2)\n    \n    group_A_samples = rng.normal(loc=m_A, scale=sigma, size=n_A)\n    group_B_samples = rng.normal(loc=m_B, scale=sigma, size=n_B)\n    \n    x = np.concatenate([group_A_samples, group_B_samples])\n    \n    labels_A = np.zeros(n_A, dtype=int)\n    labels_B = np.ones(n_B, dtype=int)\n    y = np.concatenate([labels_A, labels_B])\n    \n    return x, y\n\ndef log_norm_pdf(x, mu, sigma2):\n    \"\"\"\n    Computes the log of the Gaussian PDF, vectorized for x.\n    \"\"\"\n    const = -0.5 * np.log(2 * np.pi * sigma2)\n    return const - (x - mu)**2 / (2 * sigma2)\n\ndef run_em(x, theta0, sigma2, epsilon):\n    \"\"\"\n    Runs the EM algorithm for a 2-component GMM with fixed variance.\n    \"\"\"\n    pi1, pi2, mu1, mu2 = theta0\n    n = len(x)\n    \n    # Initial log-likelihood\n    log_pi1 = np.log(pi1) if pi1 > 0 else -np.inf\n    log_pi2 = np.log(pi2) if pi2 > 0 else -np.inf\n    log_pdf1 = log_norm_pdf(x, mu1, sigma2)\n    log_pdf2 = log_norm_pdf(x, mu2, sigma2)\n    \n    log_term1 = log_pi1 + log_pdf1\n    log_term2 = log_pi2 + log_pdf2\n    \n    log_likelihood_per_point = np.logaddexp(log_term1, log_term2)\n    current_log_likelihood = np.sum(log_likelihood_per_point)\n\n    while True:\n        old_log_likelihood = current_log_likelihood\n        \n        # E-step: Compute responsibilities\n        log_gamma1 = log_term1 - log_likelihood_per_point\n        gamma1 = np.exp(log_gamma1)\n        gamma2 = 1.0 - gamma1\n        \n        # M-step: Update parameters\n        N1 = np.sum(gamma1)\n        N2 = np.sum(gamma2)\n        \n        # Guard against empty clusters, though not expected in these cases\n        if N1  1e-9 or N2  1e-9:\n            break\n\n        pi1 = N1 / n\n        pi2 = 1.0 - pi1\n        \n        mu1 = np.sum(gamma1 * x) / N1\n        mu2 = np.sum(gamma2 * x) / N2\n        \n        # Compute new log-likelihood for convergence check\n        log_pi1 = np.log(pi1) if pi1 > 0 else -np.inf\n        log_pi2 = np.log(pi2) if pi2 > 0 else -np.inf\n        log_pdf1 = log_norm_pdf(x, mu1, sigma2)\n        log_pdf2 = log_norm_pdf(x, mu2, sigma2)\n\n        log_term1 = log_pi1 + log_pdf1\n        log_term2 = log_pi2 + log_pdf2\n\n        log_likelihood_per_point = np.logaddexp(log_term1, log_term2)\n        current_log_likelihood = np.sum(log_likelihood_per_point)\n        \n        if abs(current_log_likelihood - old_log_likelihood)  epsilon:\n            break\n            \n    return pi1, pi2, mu1, mu2\n\ndef calculate_metrics(x, y, theta_hat, sigma2):\n    \"\"\"\n    Calculates misclassification error and minority-class recall.\n    \"\"\"\n    pi1, pi2, mu1, mu2 = theta_hat\n    n = len(x)\n    \n    # Calculate final responsibilities\n    pdf1 = np.exp(log_norm_pdf(x, mu1, sigma2))\n    pdf2 = np.exp(log_norm_pdf(x, mu2, sigma2))\n\n    term1 = pi1 * pdf1\n    term2 = pi2 * pdf2\n    denominator = term1 + term2\n    \n    # Handle case where a point is far from both means, denominator is ~0\n    gamma1 = np.divide(term1, denominator, out=np.zeros_like(term1), where=denominator!=0)\n    \n    z_hat = (gamma1  0.5).astype(int) # component 1 -> 0, component 2 -> 1\n    \n    # Label alignment to minimize misclassification\n    misclassifications1 = np.sum(z_hat != y)\n    misclassifications2 = np.sum((1 - z_hat) != y)\n    \n    err = min(misclassifications1, misclassifications2) / n\n    \n    if misclassifications1 = misclassifications2:\n        y_hat = z_hat\n    else:\n        y_hat = 1 - z_hat\n        \n    # Minority-class recall\n    is_minority = (y == 1)\n    n_minority = np.sum(is_minority)\n    \n    if n_minority == 0:\n        recall = 1.0\n    else:\n        true_positives = np.sum(y_hat[is_minority] == 1)\n        recall = true_positives / n_minority\n        \n    return [err, recall]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"data_params\": {\"n_A\": 200, \"n_B\": 5, \"m_A\": 0, \"m_B\": 6, \"seed\": 31415},\n            \"init_params\": (0.5, 0.5, -0.5, 6.5)\n        },\n        {\n            \"data_params\": {\"n_A\": 200, \"n_B\": 5, \"m_A\": 0, \"m_B\": 6, \"seed\": 31415},\n            \"init_params\": (0.99, 0.01, 0.2, -0.2)\n        },\n        {\n            \"data_params\": {\"n_A\": 100, \"n_B\": 1, \"m_A\": 0, \"m_B\": 6, \"seed\": 27182},\n            \"init_params\": (0.999, 0.001, 0.1, -0.1)\n        }\n    ]\n    \n    common_sigma2 = 1.0\n    epsilon = 1e-8\n    \n    results = []\n    \n    for case in test_cases:\n        x, y = generate_data(**case[\"data_params\"], sigma2=common_sigma2)\n        \n        theta_hat = run_em(x, case[\"init_params\"], common_sigma2, epsilon)\n        \n        metrics = calculate_metrics(x, y, theta_hat, common_sigma2)\n        \n        results.append(f\"[{metrics[0]},{metrics[1]}]\")\n        \n    print(','.join(results))\n\nsolve()\n```", "id": "2388760"}]}