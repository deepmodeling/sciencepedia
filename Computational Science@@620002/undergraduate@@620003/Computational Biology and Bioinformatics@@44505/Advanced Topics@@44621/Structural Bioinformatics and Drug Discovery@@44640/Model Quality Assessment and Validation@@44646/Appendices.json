{"hands_on_practices": [{"introduction": "A cornerstone of evaluating binary classifiers is the Receiver Operating Characteristic (ROC) curve, which visualizes the trade-off between the True Positive Rate and the False Positive Rate. While the Area Under the Curve (AUC) is often used as a single-number summary of a model's performance, it can sometimes be misleading. This exercise demonstrates a scenario where two models have identical AUCs but are not equally useful, highlighting the critical importance of considering the specific needs of an application, such as operating under a strict false positive constraint [@problem_id:2406412].", "problem": "A clinical laboratory is validating two binary classifiers, $C_1$ and $C_2$, to screen for a rare pathogenic variant from high-throughput sequencing data. Performance is summarized by Receiver Operating Characteristic (ROC) curves, which plot True Positive Rate ($\\mathrm{TPR}$) against False Positive Rate ($\\mathrm{FPR}$). For each classifier, the ROC curve is defined by linear interpolation between the following points:\n\n- For $C_1$: $(0, 0)$, $(0.05, 0.55)$, $(1, 1)$.\n- For $C_2$: $(0, 0)$, $(0.20, 0.70)$, $(1, 1)$.\n\nAssume the evaluation cohort is sufficiently large that these piecewise-linear curves can be treated as exact. A regulator mandates that any deployed screening threshold must satisfy $\\mathrm{FPR} \\le 0.05$.\n\nWhich option is correct?\n\nA. The areas under the ROC curves are equal, and $C_1$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n\nB. The areas under the ROC curves are equal, and $C_2$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n\nC. The area under the ROC curve of $C_1$ exceeds that of $C_2$, so $C_1$ is preferable for any constraint.\n\nD. The areas under the ROC curves are equal, so $C_1$ and $C_2$ are clinically indistinguishable under any constraint.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Two binary classifiers, denoted $C_1$ and $C_2$.\n- Performance is measured by Receiver Operating Characteristic (ROC) curves, plotting True Positive Rate ($\\mathrm{TPR}$) against False Positive Rate ($\\mathrm{FPR}$).\n- The ROC curves are piecewise-linear.\n- For classifier $C_1$, the ROC curve passes through the points $(0, 0)$, $(0.05, 0.55)$, and $(1, 1)$.\n- For classifier $C_2$, the ROC curve passes through the points $(0, 0)$, $(0.20, 0.70)$, and $(1, 1)$.\n- The curves are considered exact representations of performance.\n- A regulatory constraint is imposed: any deployed screening threshold must satisfy $\\mathrm{FPR} \\le 0.05$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard, cornerstone concepts in statistical classification and medical diagnostics, namely ROC analysis, $\\mathrm{TPR}$, and $\\mathrm{FPR}$. The context of screening for pathogenic variants is a principal application in bioinformatics. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-defined. The ROC curves for both classifiers are explicitly given by a set of points and the rule of linear interpolation. The objective is to compare the classifiers based on these curves and a given, precise constraint. All information required for a unique solution is present.\n- **Objective:** The problem is stated using precise, unambiguous terminology standard to the field. It is free from subjective or opinion-based statements.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically grounded, well-posed, objective, and internally consistent. It is therefore **valid**. I will proceed with the derivation of a solution.\n\nThe solution requires two main steps: first, calculating the overall performance metric, the Area Under the ROC Curve (AUC), for each classifier, and second, evaluating the performance specifically under the given constraint on the False Positive Rate.\n\n**Part 1: Calculation of Area Under the ROC Curve (AUC)**\nThe area under a piecewise-linear ROC curve is calculated by summing the areas of the trapezoids formed by the line segments. The area of a trapezoid defined by points $(x_{i-1}, y_{i-1})$ and $(x_i, y_i)$ is given by $(x_i - x_{i-1}) \\times \\frac{y_{i-1} + y_i}{2}$.\n\nFor classifier $C_1$, the points are $(0, 0)$, $(0.05, 0.55)$, and $(1, 1)$.\nThe total area, $\\mathrm{AUC}_1$, is the sum of the areas of two trapezoids.\n- Area of the first trapezoid (from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=0.05$):\n$$ A_1 = (0.05 - 0) \\times \\frac{0 + 0.55}{2} = 0.05 \\times 0.275 = 0.01375 $$\n- Area of the second trapezoid (from $\\mathrm{FPR}=0.05$ to $\\mathrm{FPR}=1$):\n$$ A_2 = (1 - 0.05) \\times \\frac{0.55 + 1}{2} = 0.95 \\times \\frac{1.55}{2} = 0.95 \\times 0.775 = 0.73625 $$\n- Total AUC for $C_1$:\n$$ \\mathrm{AUC}_1 = A_1 + A_2 = 0.01375 + 0.73625 = 0.75 $$\n\nFor classifier $C_2$, the points are $(0, 0)$, $(0.20, 0.70)$, and $(1, 1)$.\nThe total area, $\\mathrm{AUC}_2$, is also the sum of the areas of two trapezoids.\n- Area of the first trapezoid (from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=0.20$):\n$$ A_1' = (0.20 - 0) \\times \\frac{0 + 0.70}{2} = 0.20 \\times 0.35 = 0.07 $$\n- Area of the second trapezoid (from $\\mathrm{FPR}=0.20$ to $\\mathrm{FPR}=1$):\n$$ A_2' = (1 - 0.20) \\times \\frac{0.70 + 1}{2} = 0.80 \\times \\frac{1.70}{2} = 0.80 \\times 0.85 = 0.68 $$\n- Total AUC for $C_2$:\n$$ \\mathrm{AUC}_2 = A_1' + A_2' = 0.07 + 0.68 = 0.75 $$\n\nThe calculation confirms that $\\mathrm{AUC}_1 = \\mathrm{AUC}_2 = 0.75$. The areas under the ROC curves are equal.\n\n**Part 2: Evaluation under the Constraint $\\mathrm{FPR} \\le 0.05$**\nThe regulator mandates that only operating points with $\\mathrm{FPR} \\le 0.05$ are permissible. To compare the classifiers under this constraint, we must determine which classifier offers a higher $\\mathrm{TPR}$ for any given $\\mathrm{FPR}$ in the interval $[0, 0.05]$.\n\nFor classifier $C_1$, the ROC curve in this region is the line segment connecting $(0, 0)$ and $(0.05, 0.55)$. The equation for the $\\mathrm{TPR}$ of $C_1$ as a function of $\\mathrm{FPR}$ is:\n$$ \\mathrm{TPR}_1(\\mathrm{FPR}) = \\frac{0.55 - 0}{0.05 - 0} \\times \\mathrm{FPR} = 11 \\times \\mathrm{FPR} \\quad \\text{for } 0 \\le \\mathrm{FPR} \\le 0.05 $$\n\nFor classifier $C_2$, the relevant part of its ROC curve is on the segment connecting $(0, 0)$ and $(0.20, 0.70)$. The equation for the $\\mathrm{TPR}$ of $C_2$ in this region is:\n$$ \\mathrm{TPR}_2(\\mathrm{FPR}) = \\frac{0.70 - 0}{0.20 - 0} \\times \\mathrm{FPR} = 3.5 \\times \\mathrm{FPR} \\quad \\text{for } 0 \\le \\mathrm{FPR} \\le 0.20 $$\n\nNow, we compare the two functions for any $\\mathrm{FPR}$ value in the constrained interval $(0, 0.05]$.\n$$ \\mathrm{TPR}_1(\\mathrm{FPR}) = 11 \\times \\mathrm{FPR} $$\n$$ \\mathrm{TPR}_2(\\mathrm{FPR}) = 3.5 \\times \\mathrm{FPR} $$\nSince $11 > 3.5$, it follows that for any positive $\\mathrm{FPR}$ within the allowed range, $\\mathrm{TPR}_1(\\mathrm{FPR}) > \\mathrm{TPR}_2(\\mathrm{FPR})$. This means that classifier $C_1$ strictly dominates classifier $C_2$ in the operating region of interest. For example, at the maximum allowed $\\mathrm{FPR}$ of $0.05$, $C_1$ achieves a $\\mathrm{TPR}$ of $0.55$, whereas $C_2$ achieves a $\\mathrm{TPR}$ of only $3.5 \\times 0.05 = 0.175$.\n\nTherefore, under the regulatory constraint, classifier $C_1$ is unequivocally preferable.\n\n**Option-by-Option Analysis**\n\nA. The areas under the ROC curves are equal, and $C_1$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n- As calculated, $\\mathrm{AUC}_1 = \\mathrm{AUC}_2 = 0.75$. The first part of the statement is correct.\n- As demonstrated, for any given $\\mathrm{FPR} \\in (0, 0.05]$, $C_1$ provides a higher $\\mathrm{TPR}$ than $C_2$. Thus, $C_1$ is preferable under the constraint. The second part of the statement is also correct.\n- Verdict: **Correct**.\n\nB. The areas under the ROC curves are equal, and $C_2$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n- The first part is correct.\n- The second part is incorrect. Classifier $C_1$ is preferable, not $C_2$.\n- Verdict: **Incorrect**.\n\nC. The area under the ROC curve of $C_1$ exceeds that of $C_2$, so $C_1$ is preferable for any constraint.\n- The premise, \"The area under the ROC curve of $C_1$ exceeds that of $C_2$\", is false. The areas are equal.\n- The conclusion, \"so $C_1$ is preferable for any constraint\", is also a logical non sequitur. Even if one AUC were higher, it does not guarantee superiority under every possible constraint.\n- Verdict: **Incorrect**.\n\nD. The areas under the ROC curves are equal, so $C_1$ and $C_2$ are clinically indistinguishable under any constraint.\n- The premise, \"The areas under the ROC curves are equal\", is correct.\n- The conclusion, \"so $C_1$ and $C_2$ are clinically indistinguishable under any constraint\", is a common but profound error in reasoning. Equal overall AUC does not imply equal performance in specific sub-regions of the ROC space. Our analysis shows a clear clinical distinction and preference for $C_1$ under the given constraint. For a different constraint, for example $\\mathrm{FPR} \\ge 0.1$, $C_2$ might be preferable. Thus, they are not indistinguishable.\n- Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2406412"}, {"introduction": "Beyond evaluating a single model's classification accuracy, we often face the challenge of selecting the \"best\" model from a set of candidates with different internal architectures. This practice introduces a powerful tool for this task: the Akaike Information Criterion (AIC). The AIC provides a principled framework for model selection by balancing goodness of fit with model complexity, penalizing models that use more parameters to achieve a similar level of fit. You will apply the AIC to compare two Hidden Markov Models (HMMs) for gene finding, a classic problem in bioinformatics, and quantify the evidence in favor of the better model [@problem_id:2406458].", "problem": "You are comparing two Hidden Markov Models (HMMs) for gene finding that were each trained by maximum likelihood on the same annotated genomic training set. Let the two models be denoted $\\mathrm{M}_{1}$ and $\\mathrm{M}_{2}$. The numbers of independently estimated parameters (free parameters) are $k_{1} = 62$ for $\\mathrm{M}_{1}$ and $k_{2} = 95$ for $\\mathrm{M}_{2}$. The maximized log-likelihoods (natural logarithm) of the training data under the fitted models are $\\ell_{1} = -40215.37$ for $\\mathrm{M}_{1}$ and $\\ell_{2} = -40180.00$ for $\\mathrm{M}_{2}$.\n\nUse the Akaike Information Criterion (AIC) to assess relative model quality. The AIC for a fitted model is defined as $\\mathrm{AIC} = 2k - 2\\ell$, where $k$ is the number of free parameters and $\\ell$ is the maximized log-likelihood. Define the Akaike differences $\\Delta_{i} = \\mathrm{AIC}_{i} - \\min_{j} \\mathrm{AIC}_{j}$, and the two-model Akaike weight for model $\\mathrm{M}_{2}$ as\n$$\nw_{2} = \\frac{\\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{2}\\right)}{\\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{1}\\right) + \\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{2}\\right)}.\n$$\n\nCompute $w_{2}$ for the given $\\mathrm{M}_{1}$ and $\\mathrm{M}_{2}$. Express your final answer as a decimal, rounded to $4$ significant figures.", "solution": "The problem presents a task of model selection between two Hidden Markov Models, $\\mathrm{M}_{1}$ and $\\mathrm{M}_{2}$, using the Akaike Information Criterion (AIC). The problem is scientifically grounded, well-posed, and all necessary information is provided. It constitutes a straightforward application of a standard statistical method in computational biology. Thus, we proceed with the calculation.\n\nThe provided parameters are:\n- For model $\\mathrm{M}_{1}$: number of free parameters $k_{1} = 62$ and maximized log-likelihood $\\ell_{1} = -40215.37$.\n- For model $\\mathrm{M}_{2}$: number of free parameters $k_{2} = 95$ and maximized log-likelihood $\\ell_{2} = -40180.00$.\n\nThe Akaike Information Criterion is defined as $\\mathrm{AIC} = 2k - 2\\ell$. First, we compute the AIC for each model.\n\nFor model $\\mathrm{M}_{1}$:\n$$\n\\mathrm{AIC}_{1} = 2k_{1} - 2\\ell_{1} = 2(62) - 2(-40215.37) = 124 + 80430.74 = 80554.74\n$$\n\nFor model $\\mathrm{M}_{2}$:\n$$\n\\mathrm{AIC}_{2} = 2k_{2} - 2\\ell_{2} = 2(95) - 2(-40180.00) = 190 + 80360.00 = 80550.00\n$$\n\nThe AIC measures the trade-off between model fit (likelihood) and model complexity (number of parameters). A lower AIC value indicates a better model. Comparing the two values, we find:\n$$\n\\mathrm{AIC}_{2} = 80550.00 < \\mathrm{AIC}_{1} = 80554.74\n$$\nTherefore, model $\\mathrm{M}_{2}$ is preferred over model $\\mathrm{M}_{1}$ according to the AIC. The minimum AIC value is $\\min_{j} \\mathrm{AIC}_{j} = \\mathrm{AIC}_{2} = 80550.00$.\n\nNext, we compute the Akaike differences, $\\Delta_{i} = \\mathrm{AIC}_{i} - \\min_{j} \\mathrm{AIC}_{j}$.\n\nFor model $\\mathrm{M}_{1}$:\n$$\n\\Delta_{1} = \\mathrm{AIC}_{1} - \\min_{j} \\mathrm{AIC}_{j} = 80554.74 - 80550.00 = 4.74\n$$\n\nFor model $\\mathrm{M}_{2}$:\n$$\n\\Delta_{2} = \\mathrm{AIC}_{2} - \\min_{j} \\mathrm{AIC}_{j} = 80550.00 - 80550.00 = 0\n$$\n\nFinally, we compute the Akaike weight for model $\\mathrm{M}_{2}$, $w_{2}$, using the provided formula:\n$$\nw_{2} = \\frac{\\exp(-\\frac{1}{2}\\Delta_{2})}{\\exp(-\\frac{1}{2}\\Delta_{1}) + \\exp(-\\frac{1}{2}\\Delta_{2})}\n$$\nSubstituting the calculated values of $\\Delta_{1}$ and $\\Delta_{2}$:\n$$\nw_{2} = \\frac{\\exp(-\\frac{1}{2} \\cdot 0)}{\\exp(-\\frac{1}{2} \\cdot 4.74) + \\exp(-\\frac{1}{2} \\cdot 0)} = \\frac{\\exp(0)}{\\exp(-2.37) + \\exp(0)}\n$$\nSince $\\exp(0) = 1$, the expression simplifies to:\n$$\nw_{2} = \\frac{1}{\\exp(-2.37) + 1}\n$$\nNow, we compute the numerical value:\n$$\nw_{2} \\approx \\frac{1}{0.0934789... + 1} = \\frac{1}{1.0934789...} \\approx 0.914515...\n$$\nThe problem requires the answer to be rounded to $4$ significant figures. The fifth significant figure is $1$, so we do not round up the fourth.\n$$\nw_{2} \\approx 0.9145\n$$\nThis weight can be interpreted as the probability that $\\mathrm{M}_{2}$ is the better model, given the data and the set of two models.", "answer": "$$\n\\boxed{0.9145}\n$$", "id": "2406458"}, {"introduction": "The validity of any model evaluation depends critically on the cross-validation strategy employed. While standard $k$-fold cross-validation is common, it relies on the assumption that data points are independent and identically distributed, which is often violated in biological data, especially data with a temporal component. This hands-on coding challenge addresses such a scenario, guiding you to implement a \"forward-chaining\" cross-validation scheme to assess a model that predicts future events. This practice underscores how choosing the correct validation process is essential for obtaining a realistic estimate of a model's performance on new, unseen data [@problem_id:2406492].", "problem": "You are given a chronological sequence of influenza seasons indexed by integers $t \\in \\{1,2,\\dots,T\\}$ with feature vectors $\\mathbf{x}_t \\in \\mathbb{R}^d$ and corresponding dominant strain labels $y_t \\in \\{0,1,2\\}$. There are $K=3$ possible strains. Consider a linear probabilistic model defined as follows. For a parameter matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times (d+1)}$ and an augmented feature vector $\\tilde{\\mathbf{x}} \\in \\mathbb{R}^{d+1}$ constructed by concatenating a constant term $1$ with $\\mathbf{x}$, the unnormalized scores are $\\mathbf{z} = \\mathbf{W}\\tilde{\\mathbf{x}} \\in \\mathbb{R}^K$. The predicted class-probability vector $\\mathbf{p} \\in \\mathbb{R}^K$ is given by the softmax, with components\n$$\np_k = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)} \\quad \\text{for } k \\in \\{1,\\dots,K\\}.\n$$\nTraining uses a squared-error surrogate to one-hot targets with squared Euclidean norm regularization (also known as $L_2$ regularization) on the non-intercept parameters. Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times (d+1)}$ be the design matrix formed by stacking augmented features for $n$ training seasons, and let $\\mathbf{Y} \\in \\mathbb{R}^{n \\times K}$ be the corresponding one-hot encoding of labels, where the $i$-th row is the standard basis vector $\\mathbf{e}_{y_i+1}^\\top$. For a given nonnegative regularization strength $\\lambda \\in \\mathbb{R}_{\\ge 0}$, the training objective for $\\mathbf{W}$ over $n$ examples is\n$$\n\\mathcal{L}(\\mathbf{W}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{W}\\tilde{\\mathbf{x}}_i - \\mathbf{y}_i \\right\\|_2^2 \\;+\\; \\lambda \\sum_{k=1}^{K} \\sum_{j=2}^{d+1} W_{k,j}^2,\n$$\nwhere the intercept column $j=1$ is not regularized. Assume the solution $\\mathbf{W}^\\star$ is taken to be any minimizer of $\\mathcal{L}(\\mathbf{W})$.\n\nDefine the forward-chaining cross-validation scheme as follows. Given a minimum training size $m \\in \\mathbb{Z}$ with $1 \\le m < T$, for each split index $k \\in \\{m, m+1, \\dots, T-1\\}$, train the model on seasons $\\{1,2,\\dots,k\\}$ to obtain $\\mathbf{W}^\\star_{(k)}$, then evaluate on season $k+1$ to obtain the predicted probability vector $\\mathbf{p}^{(k+1)}$ and the predicted class $\\hat{y}_{k+1} = \\operatorname*{arg\\,max}_{c \\in \\{0,1,2\\}} p^{(k+1)}_{c+1}$. In the event of ties in the maximum, use the smallest index $c$.\n\nFor each split, define the indicator of top-$1$ correctness as $I_{k+1} = 1$ if $\\hat{y}_{k+1} = y_{k+1}$ and $I_{k+1} = 0$ otherwise. Define the per-split negative log-likelihood (multiclass cross-entropy) as\n$$\n\\ell_{k+1} = -\\log \\left( p^{(k+1)}_{y_{k+1}+1} \\right),\n$$\nwhere the natural logarithm is used. Aggregate these over all splits to obtain the mean top-$1$ accuracy\n$$\nA = \\frac{1}{T- m} \\sum_{k=m}^{T-1} I_{k+1}\n$$\nand the mean negative log-likelihood\n$$\nL = \\frac{1}{T- m} \\sum_{k=m}^{T-1} \\ell_{k+1}.\n$$\nBoth $A$ and $L$ are unitless real numbers. Accuracy $A$ must be reported as a decimal fraction, not as a percentage.\n\nUse the following data, in chronological order, with $T = 10$, $d = 4$, and $K = 3$. The augmented feature for season $t$ is $\\tilde{\\mathbf{x}}_t = [1, x_{t,1}, x_{t,2}, x_{t,3}, x_{t,4}]^\\top$.\n\nFeatures $\\mathbf{x}_t$ for $t = 1, \\dots, 10$:\n- $t=1$: $[0.6, 0.2, 0.1, 0.3]$\n- $t=2$: $[0.7, 0.1, 0.15, 0.25]$\n- $t=3$: $[0.2, 0.8, 0.05, 0.4]$\n- $t=4$: $[0.1, 0.85, 0.1, 0.35]$\n- $t=5$: $[0.3, 0.4, 0.2, 0.5]$\n- $t=6$: $[0.25, 0.3, 0.6, 0.6]$\n- $t=7$: $[0.2, 0.25, 0.7, 0.55]$\n- $t=8$: $[0.5, 0.3, 0.2, 0.45]$\n- $t=9$: $[0.4, 0.35, 0.25, 0.5]$\n- $t=10$: $[0.15, 0.6, 0.35, 0.65]$\n\nLabels $y_t$ for $t = 1, \\dots, 10$:\n- $[0, 0, 1, 1, 1, 2, 2, 0, 1, 2]$\n\nYour program must implement the above forward-chaining evaluation and compute $(A,L)$ for each parameter setting in the test suite given below.\n\nTest suite of parameter settings $(\\lambda, m)$:\n- Case $1$: $(\\lambda = 0.5, \\; m = 5)$\n- Case $2$: $(\\lambda = 0.0, \\; m = 5)$\n- Case $3$: $(\\lambda = 10.0, \\; m = 5)$\n- Case $4$: $(\\lambda = 0.1, \\; m = 8)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For case $i$ in the above order, output the mean top-$1$ accuracy $A_i$ followed by the mean negative log-likelihood $L_i$, both rounded to six decimal places, resulting in a flat list of length $8$ in the order $[A_1, L_1, A_2, L_2, A_3, L_3, A_4, L_4]$.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Chronological Sequence**: Influenza seasons indexed by $t \\in \\{1, 2, \\dots, T\\}$ with $T=10$.\n- **Features and Labels**: Feature vectors $\\mathbf{x}_t \\in \\mathbb{R}^d$ with $d=4$. Dominant strain labels $y_t \\in \\{0, 1, 2\\}$, so there are $K=3$ strains.\n- **Model Definition**:\n    - Augmented feature vector: $\\tilde{\\mathbf{x}} \\in \\mathbb{R}^{d+1}$ by prepending a constant term $1$.\n    - Parameter matrix: $\\mathbf{W} \\in \\mathbb{R}^{K \\times (d+1)}$.\n    - Unnormalized scores: $\\mathbf{z} = \\mathbf{W}\\tilde{\\mathbf{x}} \\in \\mathbb{R}^K$.\n    - Predicted probabilities (softmax): $p_k = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$ for $k \\in \\{1, \\dots, K\\}$.\n- **Training Objective**: For $n$ training examples with design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times (d+1)}$ and one-hot label matrix $\\mathbf{Y} \\in \\mathbb{R}^{n \\times K}$, the objective function is:\n  $$\n  \\mathcal{L}(\\mathbf{W}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{W}\\tilde{\\mathbf{x}}_i - \\mathbf{y}_i \\right\\|_2^2 \\;+\\; \\lambda \\sum_{k=1}^{K} \\sum_{j=2}^{d+1} W_{k,j}^2\n  $$\n  where $\\lambda \\ge 0$ is the regularization strength. The intercept column $j=1$ is not regularized. The solution $\\mathbf{W}^\\star$ is any minimizer of $\\mathcal{L}(\\mathbf{W})$.\n- **Evaluation Scheme**: Forward-chaining cross-validation with minimum training size $m$.\n    - For each split index $k \\in \\{m, m+1, \\dots, T-1\\}$:\n        - Train on seasons $\\{1, 2, \\dots, k\\}$ to get $\\mathbf{W}^\\star_{(k)}$.\n        - Evaluate on season $k+1$.\n    - Prediction: $\\hat{y}_{k+1} = \\operatorname*{arg\\,max}_{c \\in \\{0,1,2\\}} p^{(k+1)}_{c+1}$, with ties broken by selecting the smallest index $c$.\n- **Performance Metrics**:\n    - Top-$1$ correctness indicator: $I_{k+1} = 1$ if $\\hat{y}_{k+1} = y_{k+1}$, otherwise $I_{k+1} = 0$.\n    - Per-split negative log-likelihood (NLL): $\\ell_{k+1} = -\\log(p^{(k+1)}_{y_{k+1}+1})$.\n    - Mean top-$1$ accuracy: $A = \\frac{1}{T-m} \\sum_{k=m}^{T-1} I_{k+1}$.\n    - Mean NLL: $L = \\frac{1}{T-m} \\sum_{k=m}^{T-1} \\ell_{k+1}$.\n- **Data**: $T=10, d=4, K=3$.\n    - Features $\\mathbf{x}_t$ for $t \\in \\{1, \\dots, 10\\}$:\n      $[0.6, 0.2, 0.1, 0.3], [0.7, 0.1, 0.15, 0.25], [0.2, 0.8, 0.05, 0.4], [0.1, 0.85, 0.1, 0.35], [0.3, 0.4, 0.2, 0.5], [0.25, 0.3, 0.6, 0.6], [0.2, 0.25, 0.7, 0.55], [0.5, 0.3, 0.2, 0.45], [0.4, 0.35, 0.25, 0.5], [0.15, 0.6, 0.35, 0.65]$.\n    - Labels $y_t$ for $t \\in \\{1, \\dots, 10\\}$: $[0, 0, 1, 1, 1, 2, 2, 0, 1, 2]$.\n- **Test Suite**: $(\\lambda, m)$ pairs: $(0.5, 5)$, $(0.0, 5)$, $(10.0, 5)$, $(0.1, 8)$.\n- **Output Format**: A flat list $[A_1, L_1, A_2, L_2, A_3, L_3, A_4, L_4]$ with values rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard machine learning task: multi-output linear regression with $L_2$ regularization (ridge regression), trained with a squared error loss, and evaluated using forward-chaining cross-validation. The objective function is convex and has a well-defined closed-form minimum. All data, parameters, and procedures are specified without ambiguity. There are no contradictions, scientific inaccuracies, or non-formalizable elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation\n\nThe core of the problem is to find the parameter matrix $\\mathbf{W}^\\star$ that minimizes the objective function $\\mathcal{L}(\\mathbf{W})$ for a given training set and regularization strength $\\lambda$.\n\nThe objective function is:\n$$\n\\mathcal{L}(\\mathbf{W}) = \\sum_{i=1}^{n} \\left\\| \\mathbf{W}\\tilde{\\mathbf{x}}_i - \\mathbf{y}_i \\right\\|_2^2 \\;+\\; \\lambda \\sum_{k=1}^{K} \\sum_{j=2}^{d+1} W_{k,j}^2\n$$\nLet $\\mathbf{w}_k^\\top$ be the $k$-th row of the matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times (d+1)}$. The vector $\\mathbf{w}_k \\in \\mathbb{R}^{d+1}$ contains the parameters for the $k$-th class. Let $y_{i,k}$ be the $k$-th component of the one-hot encoded vector $\\mathbf{y}_i$. The squared norm can be expanded as a sum over the classes:\n$$\n\\left\\| \\mathbf{W}\\tilde{\\mathbf{x}}_i - \\mathbf{y}_i \\right\\|_2^2 = \\sum_{k=1}^{K} (\\mathbf{w}_k^\\top \\tilde{\\mathbf{x}}_i - y_{i,k})^2\n$$\nSubstituting this into the objective function and reordering the summations reveals that the optimization problem decouples for each class:\n$$\n\\mathcal{L}(\\mathbf{W}) = \\sum_{k=1}^{K} \\left( \\sum_{i=1}^{n} (\\mathbf{w}_k^\\top \\tilde{\\mathbf{x}}_i - y_{i,k})^2 + \\lambda \\sum_{j=2}^{d+1} W_{k,j}^2 \\right)\n$$\nThis means we can solve for each row vector $\\mathbf{w}_k$ independently. For each class $k \\in \\{1, \\dots, K\\}$, we must solve a separate ridge regression problem. Let $\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{n \\times (d+1)}$ be the design matrix whose rows are $\\tilde{\\mathbf{x}}_i^\\top$ and let $\\mathbf{y}^{(k)} \\in \\mathbb{R}^n$ be the vector of $k$-th components of the one-hot labels, i.e., the $k$-th column of the matrix $\\mathbf{Y}$. The objective for $\\mathbf{w}_k$ is:\n$$\n\\mathcal{L}_k(\\mathbf{w}_k) = \\| \\tilde{\\mathbf{X}}\\mathbf{w}_k - \\mathbf{y}^{(k)} \\|_2^2 + \\lambda \\mathbf{w}_k^\\top \\mathbf{I}' \\mathbf{w}_k\n$$\nwhere $\\mathbf{I}'$ is a $(d+1) \\times (d+1)$ diagonal matrix with $I'_{1,1}=0$ and $I'_{j,j}=1$ for $j \\in \\{2, \\dots, d+1\\}$, enforcing that the intercept term is not regularized.\n\nThis is a standard quadratic form, and its minimum can be found by setting its gradient with respect to $\\mathbf{w}_k$ to zero:\n$$\n\\nabla_{\\mathbf{w}_k} \\mathcal{L}_k = 2\\tilde{\\mathbf{X}}^\\top (\\tilde{\\mathbf{X}}\\mathbf{w}_k - \\mathbf{y}^{(k)}) + 2\\lambda\\mathbf{I}'\\mathbf{w}_k = 0\n$$\n$$\n(\\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}} + \\lambda\\mathbf{I}') \\mathbf{w}_k = \\tilde{\\mathbf{X}}^\\top \\mathbf{y}^{(k)}\n$$\nThe optimal parameter vector for class $k$ is therefore:\n$$\n\\mathbf{w}_k^\\star = (\\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}} + \\lambda\\mathbf{I}')^{-1} \\tilde{\\mathbf{X}}^\\top \\mathbf{y}^{(k)}\n$$\nThis linear system can be solved for each class $k=1, 2, 3$. A more compact computation combines these into a single matrix equation. Let $\\mathbf{W}^\\top = [\\mathbf{w}_1^\\star, \\dots, \\mathbf{w}_K^\\star] \\in \\mathbb{R}^{(d+1) \\times K}$ and $\\mathbf{Y} = [\\mathbf{y}^{(1)}, \\dots, \\mathbf{y}^{(K)}] \\in \\mathbb{R}^{n \\times K}$. Then we can solve for all weight vectors at once:\n$$\n\\mathbf{W}^{\\star\\top} = (\\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}} + \\lambda\\mathbf{I}')^{-1} \\tilde{\\mathbf{X}}^\\top \\mathbf{Y}\n$$\nThe final weight matrix is $\\mathbf{W}^\\star = (\\mathbf{W}^{\\star\\top})^\\top$.\n\nThe overall procedure is as follows:\n1.  For each test case $(\\lambda, m)$:\n2.  Initialize aggregate metrics $A_{\\text{sum}} = 0$ and $L_{\\text{sum}} = 0$.\n3.  Iterate through the forward-chaining splits, indexed by $k$ from $m$ to $T-1$.\n    a.  Construct the training set using data from seasons $t=1, \\dots, k$. This gives the design matrix $\\tilde{\\mathbf{X}}$ and one-hot target matrix $\\mathbf{Y}$. The number of training samples is $n=k$.\n    b.  Construct the test set using data from season $t=k+1$. This gives the vector $\\tilde{\\mathbf{x}}_{k+1}$ and label $y_{k+1}$.\n    c.  Calculate the optimal weight matrix $\\mathbf{W}^\\star_{(k)}$ using the closed-form solution derived above.\n    d.  For the test sample $\\tilde{\\mathbf{x}}_{k+1}$, compute the scores $\\mathbf{z} = \\mathbf{W}^\\star_{(k)} \\tilde{\\mathbf{x}}_{k+1}$.\n    e.  Compute the probability vector $\\mathbf{p}$ using the softmax function on $\\mathbf{z}$. To maintain numerical stability, we use the transformation $p_c = \\exp(z_c - \\max(\\mathbf{z})) / \\sum_j \\exp(z_j - \\max(\\mathbf{z}))$.\n    f.  Determine the predicted class $\\hat{y}_{k+1} = \\operatorname*{arg\\,max}_{c \\in \\{0,1,2\\}} p_{c+1}$.\n    g.  Calculate the correctness indicator $I_{k+1}$ and the negative log-likelihood $\\ell_{k+1} = -\\log(p_{y_{k+1}+1})$.\n    h.  Add these values to the running sums $A_{\\text{sum}}$ and $L_{\\text{sum}}$.\n4.  After the loop finishes, compute the mean accuracy $A = A_{\\text{sum}} / (T-m)$ and mean NLL $L = L_{\\text{sum}} / (T-m)$.\n5.  Store the pair $(A, L)$ for the current test case.\n6.  Finally, format all resulting values as specified and print them.\n\nThis algorithm directly implements the specified procedure and utilizes the derived analytical solution for model training.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the forward-chaining cross-validation for a linear probabilistic model\n    and computes mean accuracy and mean negative log-likelihood.\n    \"\"\"\n    \n    # Data from the problem statement\n    # T = 10, d = 4, K = 3\n    T = 10\n    d = 4\n    K = 3\n\n    # Feature vectors x_t for t = 1, ..., 10\n    features = np.array([\n        [0.6, 0.2, 0.1, 0.3],     # t=1\n        [0.7, 0.1, 0.15, 0.25],  # t=2\n        [0.2, 0.8, 0.05, 0.4],   # t=3\n        [0.1, 0.85, 0.1, 0.35],  # t=4\n        [0.3, 0.4, 0.2, 0.5],    # t=5\n        [0.25, 0.3, 0.6, 0.6],   # t=6\n        [0.2, 0.25, 0.7, 0.55],  # t=7\n        [0.5, 0.3, 0.2, 0.45],   # t=8\n        [0.4, 0.35, 0.25, 0.5],  # t=9\n        [0.15, 0.6, 0.35, 0.65]   # t=10\n    ])\n\n    # Labels y_t for t = 1, ..., 10\n    labels = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 2])\n\n    # Augment feature vectors by prepending a constant term of 1\n    aug_features = np.hstack([np.ones((T, 1)), features])\n\n    # Test suite of parameter settings (lambda, m)\n    test_cases = [\n        (0.5, 5),   # Case 1\n        (0.0, 5),   # Case 2\n        (10.0, 5),  # Case 3\n        (0.1, 8)    # Case 4\n    ]\n\n    # Regularization matrix I' (does not regularize the intercept)\n    I_prime = np.diag([0.0] + [1.0] * d)\n    \n    results = []\n\n    for lam, m in test_cases:\n        total_I = 0.0\n        total_L = 0.0\n        \n        # Number of splits for forward-chaining CV\n        num_splits = T - m\n        \n        # Iterate through splits k = m, m+1, ..., T-1\n        # In 0-based indexing, this corresponds to training on [0..k-1] and testing on [k].\n        # The problem uses 1-based indexing for seasons, so season k is at index k-1.\n        # Training on {1..k} -> indices {0..k-1}. Test on {k+1} -> index {k}.\n        # Split index k in problem goes from m to T-1.\n        for k_split_idx in range(m, T):\n            # n is the number of training samples for the current split\n            n_train = k_split_idx\n            train_indices = np.arange(n_train)\n            test_index = k_split_idx\n\n            # Construct training data\n            X_train_aug = aug_features[train_indices, :]\n            y_train_raw = labels[train_indices]\n\n            # One-hot encode training labels\n            Y_train_onehot = np.zeros((n_train, K))\n            Y_train_onehot[np.arange(n_train), y_train_raw] = 1.0\n\n            # Construct test data\n            x_test_aug = aug_features[test_index, :]\n            y_test = labels[test_index]\n\n            # Solve for W using the closed-form solution\n            # (X_T * X + lambda * I') * w = X_T * y\n            XTX = X_train_aug.T @ X_train_aug\n            M = XTX + lam * I_prime\n            \n            # Use numpy's solver. For small matrices, inv is fine.\n            M_inv = np.linalg.inv(M)\n            \n            # W_T = (d+1) x K matrix where columns are the weight vectors\n            W_T = M_inv @ X_train_aug.T @ Y_train_onehot\n            \n            # W is K x (d+1), rows correspond to classes\n            W = W_T.T\n\n            # Predict on the test sample\n            # z = W * x_test_aug\n            z = W @ x_test_aug\n\n            # Compute probabilities using numerically stable softmax\n            z_shifted = z - np.max(z)\n            exps = np.exp(z_shifted)\n            p = exps / np.sum(exps)\n            \n            # Predicted class (arg max)\n            y_pred = np.argmax(p)\n            \n            # Calculate metrics for this split\n            I_k = 1.0 if y_pred == y_test else 0.0\n            # NLL: -log(probability of true class)\n            # Add a small epsilon to log to prevent log(0) if p[y_test] is numerically zero\n            epsilon = 1e-15\n            l_k = -np.log(p[y_test] + epsilon)\n            \n            total_I += I_k\n            total_L += l_k\n\n        # Compute mean metrics over all splits\n        mean_A = total_I / num_splits\n        mean_L = total_L / num_splits\n        \n        results.extend([mean_A, mean_L])\n\n    # Format the final output string\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406492"}]}