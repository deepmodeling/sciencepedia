{"hands_on_practices": [{"introduction": "The volcano plot, a staple of differential expression analysis, visualizes two key metrics for each gene: the magnitude of change, often expressed as $\\log_2(\\text{Fold Change})$, and its statistical significance, represented by a $p$-value. A common challenge for new practitioners is navigating results that seem contradictory, such as a large fold change paired with a high, non-significant $p$-value. This exercise [@problem_id:2281817] provides a hands-on opportunity to dissect such a scenario, reinforcing the crucial distinction between statistical evidence and the size of an observed effect.", "problem": "A team of computational biologists is conducting a differential gene expression analysis using RNA-sequencing (RNA-seq) data. Their goal is to identify genes whose expression levels are altered by a new experimental drug in a cancer cell line. They compare gene expression in drug-treated cells to that in untreated control cells. For each gene, their analysis pipeline calculates two key metrics:\n\n1.  **$\\log_2(\\text{Fold Change})$**: This value quantifies the magnitude and direction of the expression change. A positive value indicates upregulation (increased expression), while a negative value indicates downregulation. For example, a $\\log_2(\\text{Fold Change})$ of 2.0 corresponds to a $2^{2} = 4$-fold increase in expression.\n2.  **p-value**: This value assesses the statistical significance of the observed fold change. It represents the probability of observing a fold change at least as extreme as the one measured, assuming that the drug has no actual effect (the null hypothesis). A smaller p-value indicates stronger evidence against the null hypothesis. Conventionally, a p-value less than 0.05 is considered statistically significant.\n\nThe team's analysis of a particular gene, named `REG-17`, yields a $\\log_2(\\text{Fold Change})$ of 4.5 and a p-value of 0.38.\n\nWhich of the following statements provides the most accurate and sound interpretation of this result for the `REG-17` gene?\n\nA. The drug causes a very large and statistically significant upregulation of `REG-17`, making it a high-priority candidate for further investigation.\n\nB. A large upregulation of `REG-17` was observed in the experiment, but due to high variability in the data or an insufficient sample size, we cannot be confident that this change is a real effect of the drug rather than a result of random chance.\n\nC. The drug has no meaningful effect on `REG-17` because the observed change is not statistically significant.\n\nD. The calculation must be incorrect, as a $\\log_2(\\text{Fold Change})$ as large as 4.5 cannot be associated with a p-value as high as 0.38.\n\nE. `REG-17` is a biologically unimportant gene, which is why its expression change did not reach statistical significance.", "solution": "To determine the correct interpretation, we must carefully consider the meaning of both the $\\log_2(\\text{Fold Change})$ and the p-value.\n\n1.  **Interpreting the $\\log_2(\\text{Fold Change})$:** The $\\log_2(\\text{Fold Change})$ is a measure of the effect size. A value of 4.5 for `REG-17` means that the average expression of this gene in the drug-treated group was $2^{4.5} \\approx 22.6$ times higher than in the control group. This is a very large magnitude of change from a biological perspective. It indicates that a strong upregulation was indeed *observed* in the collected data.\n\n2.  **Interpreting the p-value:** The p-value assesses the statistical reliability of the observed effect. A p-value of 0.38 means there is a 38% probability of observing a fold change of this magnitude (or greater) purely by random chance, even if the drug had no true effect on the gene's expression (i.e., if the null hypothesis were true). In biomedical research, a conventional threshold for statistical significance is a p-value less than 0.05. Since 0.38 is much larger than 0.05, the result is considered **not statistically significant**. This high p-value suggests that we cannot confidently rule out random variation as the cause of the observed change. The lack of statistical significance often arises from high variability between the replicate samples within a group (e.g., some treated cells responded strongly, others not at all) or an insufficient number of replicates, which reduces the statistical power of the experiment to detect a true effect.\n\n3.  **Synthesizing the two metrics:** We have a seemingly contradictory situation: a very large effect size ($\\log_2(\\text{Fold Change}) = 4.5$) paired with a lack of statistical significance (p-value = 0.38). The correct interpretation synthesizes these two facts. It acknowledges the large change seen in the data but also acknowledges the high uncertainty surrounding this observation. We have observed something dramatic, but we cannot be sure it's a real, repeatable effect of the drug.\n\nNow, let's evaluate the given options based on this understanding:\n\n*   **A. The drug causes a very large and statistically significant upregulation of `REG-17`, making it a high-priority candidate for further investigation.** This is incorrect. While the upregulation is large, it is explicitly **not** statistically significant (p-value = 0.38 > 0.05).\n\n*   **B. A large upregulation of `REG-17` was observed in the experiment, but due to high variability in the data or an insufficient sample size, we cannot be confident that this change is a real effect of the drug rather than a result of random chance.** This is the correct interpretation. It accurately reports the large observed effect (large upregulation) while correctly interpreting the high p-value as a lack of confidence in the result, rightfully pointing to common causes like high variability or low sample size.\n\n*   **C. The drug has no meaningful effect on `REG-17` because the observed change is not statistically significant.** This is an incorrect conclusion. \"Absence of evidence is not evidence of absence.\" The experiment failed to provide *statistically significant evidence* of an effect, but this does not prove there is *no effect*. The large fold change suggests an effect might be present, but the experiment lacked the power to confirm it.\n\n*   **D. The calculation must be incorrect, as a $\\log_2(\\text{Fold Change})$ as large as 4.5 cannot be associated with a p-value as high as 0.38.** This is incorrect. This is a very common scenario in biological experiments, particularly those with a small number of replicates or inherently \"noisy\" biological systems where variance within groups is high.\n\n*   **E. `REG-17` is a biologically unimportant gene, which is why its expression change did not reach statistical significance.** This is an unsupported leap in logic. The statistical significance in a single experiment says nothing about the overall biological importance of a gene. A gene could be critically important, but the drug might not affect it, or the experiment might not have been robust enough to detect the change.", "answer": "$$\\boxed{B}$$", "id": "2281817"}, {"introduction": "When analyzing thousands of genes simultaneously, we face the challenge of multiple hypothesis testing, where the chance of making a false discovery increases dramatically. To address this, methods like the Benjamini-Hochberg (BH) procedure are used to control the False Discovery Rate (FDR). This hands-on coding exercise [@problem_id:2385494] moves beyond simply using a software function by guiding you to implement the BH procedure from its mathematical definition, providing a fundamental understanding of how raw $p$-values are transformed into adjusted $p$-values, or $q$-values.", "problem": "You are given collections of raw $p$-values arising from $m$ independent gene-level null hypotheses in a differential gene expression experiment comparing two conditions. Let the set of null hypotheses be $\\{H_1,\\dots,H_m\\}$ with corresponding raw $p$-values $p_1,\\dots,p_m \\in [0,1]$. For a target level $q \\in (0,1)$, the Benjamini–Hochberg (BH) procedure for controlling the False Discovery Rate (FDR) at level $q$ is defined as follows. Write the $p$-values in nondecreasing order as $p_{(1)} \\le \\dots \\le p_{(m)}$, where the subscript denotes order statistics. Define the index\n$$\nk \\;=\\; \\max\\left\\{ i \\in \\{1,\\dots,m\\} \\;:\\; p_{(i)} \\le \\frac{i}{m}\\,q \\right\\},\n$$\nwith the convention that if the set is empty then no hypotheses are rejected. The BH rejection set is then all hypotheses with raw $p$-values not exceeding the threshold $p_{(k)}$, that is,\n$$\n\\mathcal{R} \\;=\\; \\{ j \\in \\{1,\\dots,m\\} \\;:\\; p_j \\le p_{(k)} \\},\n$$\nwhen $k$ exists, and $\\mathcal{R}=\\varnothing$ otherwise. The BH adjusted $p$-values (also called BH $q$-values in some contexts) are defined for each hypothesis by first assigning to the $i$-th order statistic the value\n$$\n\\tilde{p}_{(i)} \\;=\\; \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}\\,p_{(j)} \\right),\n$$\nthen truncating at $1$ by $\\min\\{\\tilde{p}_{(i)},1\\}$, and finally mapping back to the original hypothesis order. To make the rank assignment deterministic in the presence of ties among equal $p$-values, use a stable ordering that breaks ties by increasing original index, that is, if $p_a=p_b$ and $a<b$ then $p_a$ precedes $p_b$ in the order. Index hypotheses by their original positions using $0$-based indices.\n\nTask. For each test case below, given a list of raw $p$-values and a target FDR level $q$, compute:\n- the list of BH adjusted $p$-values in the original hypothesis order, each rounded to $6$ decimal places (expressed as decimals, not as percentages), and\n- the list of rejected hypothesis indices (using $0$-based indexing) in increasing order under the BH procedure at level $q$.\n\nTest suite. Evaluate your implementation on the following four cases, which together cover a typical case, boundary values, ties, and a case with no rejections:\n- Case $1$: $p=(0.001,\\,0.04,\\,0.03,\\,0.2,\\,0.5,\\,0.0005,\\,0.07,\\,0.9)$, $q=0.1$.\n- Case $2$: $p=(0,\\,1,\\,0.5,\\,0.05,\\,0.2)$, $q=0.05$.\n- Case $3$: $p=(0.02,\\,0.02,\\,0.02,\\,0.5,\\,0.8,\\,0.9)$, $q=0.1$.\n- Case $4$: $p=(0.2,\\,0.3,\\,0.4,\\,0.6,\\,0.8)$, $q=0.01$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one element per test case. Each element must itself be a two-element list whose first element is the list of rounded BH adjusted $p$-values (in original order) and whose second element is the list of rejected $0$-based indices in increasing order. No other text should be printed. For example, the overall structure must be of the form\n[[adj_case1,rej_case1],[adj_case2,rej_case2],[adj_case3,rej_case3],[adj_case4,rej_case4]]\nwhere adj_case$k$ and rej_case$k$ are lists for case $k$ as specified above.", "solution": "The problem statement is assessed to be valid. It presents a clear, well-defined computational task based on the standard and scientifically sound Benjamini-Hochberg procedure for controlling the False Discovery Rate, a fundamental method in computational biology and statistics. The definitions, conditions, and test cases are complete, consistent, and objective, permitting a unique and verifiable solution.\n\nThe task is to implement the Benjamini-Hochberg (BH) procedure. Given a set of $m$ raw $p$-values $\\{p_1, \\dots, p_m\\}$ from multiple hypothesis tests and a target False Discovery Rate (FDR) level $q$, we must compute the BH adjusted $p$-values and identify the set of rejected hypotheses. The procedure is deterministic and will be implemented through a sequence of principled steps derived directly from the provided mathematical definitions.\n\nLet $m$ be the total number of hypotheses. The algorithm proceeds as follows:\n\n1.  **Data Structuring and Sorting**: Each raw $p$-value $p_j$ is associated with its original $0$-based index $j$. The resulting pairs $(p_j, j)$ are then sorted in non-decreasing order of $p_j$. The problem specifies a stable sorting mechanism to handle ties: if $p_a=p_b$ for original indices $a<b$, the pair corresponding to $a$ must precede the pair for $b$. This step yields the ordered $p$-values $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$, along with their original indices. The rank of $p_{(i)}$ is $i$.\n\n2.  **Identification of the Rejection Threshold**: The core of the BH procedure is to find the largest rank $k \\in \\{1,\\dots,m\\}$ for which the ordered $p$-value $p_{(k)}$ satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m}q\n    $$\n    This condition compares the $k$-th smallest $p$-value to a threshold that becomes linearly more lenient with increasing rank $k$. If the set of ranks satisfying this inequality is empty, it is concluded that no hypotheses can be rejected at the specified FDR level $q$, and the rejection set $\\mathcal{R}$ is empty. Otherwise, the value $p_{(k)}$ corresponding to the largest such rank $k$ becomes the significance threshold.\n\n3.  **Determination of the Rejection Set**: The set of rejected hypotheses, $\\mathcal{R}$, consists of all hypotheses whose original, unadjusted $p$-value $p_j$ is less than or equal to the threshold $p_{(k)}$ found in the previous step. That is, $\\mathcal{R} = \\{ j \\in \\{1,\\dots,m\\} : p_j \\le p_{(k)} \\}$. The final output requires the $0$-based indices of these hypotheses, sorted in increasing order.\n\n4.  **Computation of Adjusted $p$-values**: The BH adjusted $p$-value for the hypothesis corresponding to the $i$-th ordered raw $p$-value, $p_{(i)}$, is defined as:\n    $$\n    \\tilde{p}_{(i)} = \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}p_{(j)} \\right)\n    $$\n    This is then truncated at $1$ if necessary. This definition ensures that the adjusted $p$-values are monotonically non-decreasing with respect to their rank, i.e., $\\tilde{p}_{(1)} \\le \\tilde{p}_{(2)} \\le \\dots \\le \\tilde{p}_{(m)}$. An efficient computational strategy for this step involves first computing the scaled values $\\frac{m}{j}p_{(j)}$ for all ranks $j=1, \\dots, m$. Then, one iterates backwards from rank $m$ down to $1$, calculating the cumulative minimum at each step. Specifically, the adjusted $p$-value for rank $i$ is the minimum of its own scaled value and the adjusted $p$-value of rank $i+1$. The final adjusted $p$-values are then mapped back to their original hypothesis order and rounded to $6$ decimal places as required.\n\nThe implementation will apply this four-step logic to each of the provided test cases to generate the required outputs.", "answer": "```python\nimport numpy as np\n\ndef _format_output(data):\n    \"\"\"\n    Custom recursive function to format the final list into a string\n    without spaces and with floats formatted to 6 decimal places.\n    \"\"\"\n    if isinstance(data, list):\n        return f\"[{','.join(_format_output(item) for item in data)}]\"\n    if isinstance(data, float):\n        return f\"{data:.6f}\"\n    return str(data)\n\ndef benjamini_hochberg(p_values: np.ndarray, q: float):\n    \"\"\"\n    Performs the Benjamini-Hochberg procedure for FDR control.\n\n    Args:\n        p_values: A numpy array of raw p-values.\n        q: The target False Discovery Rate level.\n\n    Returns:\n        A tuple containing:\n        - A list of BH adjusted p-values, rounded to 6 decimal places, in original order.\n        - A sorted list of 0-based indices of rejected hypotheses.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return [], []\n        \n    original_indices = np.arange(m)\n    \n    # Sort p-values while keeping track of original indices.\n    # The 'stable' kind ensures that for equal p-values, the original order is preserved,\n    # satisfying the problem's tie-breaking rule.\n    sorted_order_indices = np.argsort(p_values, kind='stable')\n    sorted_p_values = p_values[sorted_order_indices]\n    \n    # --- Step 1: Find rejection threshold ---\n    ranks = np.arange(1, m + 1)\n    bh_thresholds = (ranks / m) * q\n    \n    significant_mask = sorted_p_values <= bh_thresholds\n    \n    rejected_indices = []\n    if np.any(significant_mask):\n        # Find the largest rank k satisfying the BH condition\n        k = np.max(ranks[significant_mask])\n        \n        # The rejection threshold is the p-value at this rank k\n        rejection_threshold = sorted_p_values[k - 1]\n        \n        # Identify all hypotheses with original p-values <= threshold\n        rejected_mask = p_values <= rejection_threshold\n        rejected_indices = original_indices[rejected_mask].tolist()\n\n    # --- Step 2: Calculate adjusted p-values ---\n    # Calculate raw scaled p-values: (m/i) * p_(i)\n    scaled_p_values = (m / ranks) * sorted_p_values\n    \n    # Enforce monotonicity by taking the cumulative minimum from the end (right-to-left)\n    adj_p_sorted = np.minimum.accumulate(scaled_p_values[::-1])[::-1]\n    \n    # Truncate values at 1.0\n    adj_p_sorted = np.minimum(adj_p_sorted, 1.0)\n    \n    # Unsort the adjusted p-values to match the original p-value order\n    adj_p_original = np.empty(m)\n    adj_p_original[sorted_order_indices] = adj_p_sorted\n    \n    # Round to 6 decimal places for final output\n    adj_p_rounded = [round(p, 6) for p in adj_p_original]\n\n    return adj_p_rounded, rejected_indices\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final result.\n    \"\"\"\n    test_cases = [\n        ((0.001, 0.04, 0.03, 0.2, 0.5, 0.0005, 0.07, 0.9), 0.1),\n        ((0.0, 1.0, 0.5, 0.05, 0.2), 0.05),\n        ((0.02, 0.02, 0.02, 0.5, 0.8, 0.9), 0.1),\n        ((0.2, 0.3, 0.4, 0.6, 0.8), 0.01),\n    ]\n\n    results = []\n    for p_tuple, q_val in test_cases:\n        p_values_np = np.array(p_tuple)\n        adj_p, rej_idx = benjamini_hochberg(p_values_np, q_val)\n        results.append([adj_p, rej_idx])\n    \n    # Print the final result in the specified single-line format\n    print(_format_output(results))\n\nsolve()\n```", "id": "2385494"}, {"introduction": "The statistical models used in differential expression analysis are powerful, but their conclusions are only as reliable as the assumptions they are built upon. A critical assumption is that all major sources of variation are properly accounted for in the model, yet technical artifacts like batch effects often confound biological comparisons. This advanced practice [@problem_id:2385475] challenges you to build a simulation from the ground up, demonstrating firsthand how an unmodeled, confounded batch effect can create the illusion of widespread differential expression where none truly exists. This exercise crystallizes the concept of omitted-variable bias and highlights the crucial role of including known covariates to prevent a flood of false discoveries.", "problem": "Write a complete, runnable program that simulates gene expression data under a linear model with a strong batch effect and evaluates the impact of omitting the batch covariate during differential gene expression analysis. Your program must implement the following scientifically grounded and widely accepted base principles without relying on any shortcut formulas provided in the problem statement.\n\nBase principles to use:\n- Gene expression on an appropriate transformed scale (for example, a logarithmic scale) can be modeled by a linear model with Gaussian noise. For gene index $g$ and sample index $i$, let $y_{g i}$ denote the expression. The model includes a gene-specific baseline term, a condition term, a batch term, and an independent noise term.\n- Ordinary Least Squares (OLS) under the Gaussian noise assumption yields unbiased estimators in correctly specified linear models and provides test statistics for hypotheses on model coefficients that follow the Student's $t$-distribution under the null.\n- Multiple hypothesis testing across many genes should be controlled using the False Discovery Rate (FDR). Use the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$.\n\nSimulation design and hypothesis testing:\n- All genes share the same generative model structure and are simulated under the global null of no true differential expression between the two biological conditions. Specifically, for every gene $g$, the condition effect is $0$; the batch effect is shared across genes as a fixed shift between two batches; the baseline level may vary by gene; and the noise is independent and identically distributed Gaussian across samples within a gene, with constant variance.\n- The program must estimate, for each gene, the two-sided $p$-value for the null hypothesis that the condition coefficient is zero using:\n  1. A misspecified model that omits the batch covariate (condition-only analysis).\n  2. A correctly specified model that includes both condition and batch covariates (condition-plus-batch analysis).\n- Apply the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$ across all genes in each analysis to obtain the number of discoveries (these are all false discoveries because the data are simulated under the global null).\n\nScientific rationale to be demonstrated:\n- When the condition and batch covariates are correlated (confounding) and the batch effect is strong, omitting the batch covariate induces bias in the estimated condition effect and inflates false discoveries, whereas including the batch covariate removes the bias and controls false discoveries near the target FDR level.\n- When the condition and batch covariates are orthogonal, omitting the batch covariate does not induce bias in the estimated condition effect; false discoveries should be close to the target FDR level even without adjustment.\n- As a boundary case, when the batch effect is zero, including or omitting the batch covariate should yield similar behavior.\n\nTest suite:\nSimulate $m = 2000$ genes in each case. Use independent baseline levels $\\mu_g \\sim \\mathcal{N}(0, 1)$ for $g = 1, \\dots, m$. For each case, generate independent Gaussian noise $\\varepsilon_{g i} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.5$.\n\nEncode the binary condition covariate as $x_i \\in \\{0, 1\\}$ for the two conditions, and the binary batch covariate as $z_i \\in \\{0, 1\\}$ for the two batches. The batch shift is a constant $\\gamma$ added for $z_i = 1$ and $0$ for $z_i = 0$. The true condition effect is $0$ for all genes. Use the following three cases, each with a specified sample composition across condition and batch:\n- Case $1$ (high confounding, strong batch): batch $1$ has $19$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $19$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $2$ (orthogonal design, strong batch): batch $1$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; batch $2$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $3$ (high confounding, no batch): batch $1$ has $9$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $9$ samples from condition $1$; $\\gamma = 0.0$; total samples $n = 20$.\n\nRandomness and reproducibility:\n- Use a fixed random seed of $20240513$ to initialize the generator for Case $1$. For Cases $2$ and $3$, you must add the case index to the base seed (that is, $20240513 + 2$ and $20240513 + 3$) to ensure independence across cases while retaining reproducibility.\n\nStatistical analysis requirements:\n- For each gene and each case, compute the two-sided $p$-value testing the null hypothesis that the condition coefficient equals $0$ using:\n  1. A condition-only linear model with an intercept.\n  2. A condition-plus-batch linear model with an intercept.\n- Use the Benjamini–Hochberg (BH) procedure at level $q = 0.05$ on the set of $m$ $p$-values within each analysis to determine the number of discoveries (rejections). Since all nulls are true by construction, every discovery is a false positive. Report the numbers of discoveries for both analyses.\n\nRequired final output format:\n- Your program should produce a single line of output containing $6$ comma-separated integers enclosed in square brackets, in the following order: $[\\text{case1\\_naive}, \\text{case1\\_adjusted}, \\text{case2\\_naive}, \\text{case2\\_adjusted}, \\text{case3\\_naive}, \\text{case3\\_adjusted}]$, where “naive” denotes the condition-only analysis and “adjusted” denotes the condition-plus-batch analysis.\n\nNo external inputs:\n- The program must be entirely self-contained and must not read any input or files or access any network resources. All numerical values must be hard coded as specified above.\n\nAngle units and physical units:\n- No angles or physical units are involved.\n\nAnswer type:\n- Each of the $6$ reported values must be an integer.\n\nYour goal is to implement the simulation and analysis so that the output demonstrates, through these test cases, how omitting a strong, confounded batch effect inflates false discoveries in differential gene expression analysis, while including the batch indicator restores valid inference.", "solution": "The problem presented is a valid, well-posed exercise in computational statistics, designed to demonstrate a critical principle in the analysis of high-throughput biological data: the danger of omitted-variable bias. The specific context is differential gene expression analysis, where unmeasured or unmodeled technical factors, such as experimental batches, can confound the biological signals of interest. The problem is scientifically grounded, using standard linear models, Ordinary Least Squares (OLS), Student's $t$-tests, and the Benjamini-Hochberg (BH) procedure for False Discovery Rate (FDR) control, which are foundational methods in the field. The simulation parameters are clearly specified, ensuring the problem is self-contained and reproducible. We shall proceed with the solution.\n\nThe core of the problem lies in the general linear model, which we write in matrix form for a single gene as $Y = X\\beta + \\epsilon$. Here, $Y$ is an $n \\times 1$ vector of expression measurements for $n$ samples, $X$ is an $n \\times p$ design matrix encoding the experimental covariates for the $p$ parameters, $\\beta$ is a $p \\times 1$ vector of coefficients to be estimated, and $\\epsilon$ is an $n \\times 1$ vector of independent and identically distributed error terms, assumed to follow a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$.\n\nThe data are simulated under a \"global null\" scenario, meaning there is no true differential expression. For each of $m=2000$ genes, indexed by $g$, and $n$ samples, indexed by $i$, the expression $y_{gi}$ is generated according to the true model:\n$y_{gi} = \\mu_g + \\gamma z_i + \\varepsilon_{gi}$\nwhere $\\mu_g \\sim \\mathcal{N}(0, 1)$ is the gene-specific baseline expression, $\\gamma$ is the magnitude of the batch effect, $z_i \\in \\{0, 1\\}$ is the batch indicator covariate, and $\\varepsilon_{gi} \\sim \\mathcal{N}(0, \\sigma^2)$ is the noise term with $\\sigma=0.5$. The true coefficient for the biological condition is zero.\n\nWe will analyze these simulated data using two different models for each gene:\n\n1.  A **misspecified or \"naive\" model**, which omits the batch covariate:\n    $y_{gi} = \\beta_{g0} + \\beta_{g1} x_i + e_{gi}$. The design matrix $X_{\\text{naive}}$ has two columns: an intercept (a vector of ones) and the condition covariate vector $x$. We test the null hypothesis $H_0: \\beta_{g1} = 0$.\n\n2.  A **correctly specified or \"adjusted\" model**, which includes the batch covariate:\n    $y_{gi} = \\beta'_{g0} + \\beta'_{g1} x_i + \\beta'_{g2} z_i + e'_{gi}$. The design matrix $X_{\\text{adj}}$ has three columns: an intercept, the condition vector $x$, and the batch vector $z$. We test the null hypothesis $H_0: \\beta'_{g1} = 0$.\n\nFor both models, the coefficients $\\beta$ are estimated using Ordinary Least Squares (OLS), for which the solution is $\\hat{\\beta} = (X^T X)^{-1}X^T Y$. This can be solved efficiently for all $m$ genes simultaneously.\n\nTo test the significance of the condition coefficient $\\hat{\\beta}_1$ (where the subscript $1$ denotes the coefficient for the condition covariate $x$), we compute the Student's $t$-statistic:\n$t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}$\nThe standard error, $\\text{SE}(\\hat{\\beta}_1)$, is the square root of the estimated variance of the coefficient estimator. The variance-covariance matrix of the estimators is given by $\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}$. We estimate the unknown error variance $\\sigma^2$ with its unbiased estimator, the mean squared error:\n$$\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{\\text{RSS}}{n-p}$$\nwhere $\\hat{y}_i$ are the fitted values from the model, $\\text{RSS}$ is the residual sum of squares, $n$ is the number of samples, and $p$ is the number of parameters in the model (the number of columns in $X$). Let $C = (X^T X)^{-1}$. The specific variance for $\\hat{\\beta}_1$ is $\\hat{\\sigma}^2 C_{11}$ (assuming the condition covariate is the second column of $X$, indexed by $1$). The standard error is thus $\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\hat{\\sigma}^2 C_{11}}$. Under the null hypothesis, this $t$-statistic follows a Student's $t$-distribution with $n-p$ degrees of freedom. From this distribution, we compute a two-sided $p$-value for each gene.\n\nSince we are performing $m=2000$ hypothesis tests, one for each gene, we must correct for multiple testing to control the number of false discoveries. We will use the Benjamini-Hochberg (BH) procedure at a target False Discovery Rate (FDR) of $q = 0.05$. The procedure is as follows:\n1.  Sort the $m$ $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} q$.\n3.  If such a $k$ exists, reject the null hypotheses for all tests corresponding to $p_{(1)}, \\dots, p_{(k)}$. The number of discoveries is $k$. If no such $k$ exists, no hypotheses are rejected, and the number of discoveries is $0$.\n\nWe will implement this entire process for the three specified cases, which differ in their experimental design (confounding vs. orthogonal) and the strength of the batch effect.\n\n-   **Case 1 (Confounding):** The condition and batch covariates are strongly correlated. When the batch effect $\\gamma$ is large, omitting the batch covariate $z$ will lead to a biased estimate of the condition coefficient $\\beta_1$. This bias, known as omitted-variable bias, is proportional to the true batch effect and the correlation between the batch and condition covariates. This bias will systematically shift the estimated condition effects away from zero, leading to a large number of small $p$-values and a massive inflation of false discoveries in the naive analysis. The adjusted model, by accounting for $z$, will remove this bias and provide valid inference, with the number of false discoveries properly controlled near the expected level.\n\n-   **Case 2 (Orthogonal):** The condition and batch covariates are uncorrelated (orthogonal design). In this scenario, the omitted-variable bias term is zero. Therefore, even when a strong batch effect is present, the estimate of the condition coefficient in the naive model remains unbiased. Both a naive and an adjusted analysis are expected to control the false discovery rate correctly.\n\n-   **Case 3 (Confounding, No Batch Effect):** The design is confounded as in Case 1, but the batch effect size $\\gamma$ is zero. The omitted-variable bias is proportional to $\\gamma$, so if $\\gamma=0$, there is no bias. Both the naive and adjusted models should perform correctly, similar to Case 2.\n\nThe program will systematically execute these simulations, perform both naive and adjusted analyses for each case, apply the BH procedure, and report the resulting number of false discoveries, thereby quantitatively demonstrating these fundamental statistical principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation suite and print the final results.\n    \"\"\"\n\n    # Global parameters\n    m = 2000  # Number of genes\n    sigma = 0.5  # Noise standard deviation\n    q_level = 0.05  # Target FDR level for BH procedure\n\n    # Case 1: High confounding, strong batch\n    case1_params = {\n        'n_cond0_batch1': 19, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 19\n    }\n    case1_gamma = 1.5\n    case1_seed = 20240513\n    \n    # Case 2: Orthogonal design, strong batch\n    case2_params = {\n        'n_cond0_batch1': 10, 'n_cond1_batch1': 10,\n        'n_cond0_batch2': 10, 'n_cond1_batch2': 10\n    }\n    case2_gamma = 1.5\n    case2_seed = 20240513 + 2\n\n    # Case 3: High confounding, no batch effect\n    case3_params = {\n        'n_cond0_batch1': 9, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 9\n    }\n    case3_gamma = 0.0\n    case3_seed = 20240513 + 3\n    \n    test_cases = [\n        (case1_params, case1_gamma, m, sigma, q_level, case1_seed),\n        (case2_params, case2_gamma, m, sigma, q_level, case2_seed),\n        (case3_params, case3_gamma, m, sigma, q_level, case3_seed),\n    ]\n\n    results = []\n    for params in test_cases:\n        naive_discoveries, adjusted_discoveries = run_simulation(*params)\n        results.extend([naive_discoveries, adjusted_discoveries])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(case_params, gamma, m, sigma, q, seed):\n    \"\"\"\n    Runs a single simulation case.\n    \n    Generates data, performs naive and adjusted analyses, and returns the number of discoveries for each.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Construct design vectors (x for condition, z for batch)\n    n01 = case_params['n_cond0_batch1']\n    n11 = case_params['n_cond1_batch1']\n    n02 = case_params['n_cond0_batch2']\n    n12 = case_params['n_cond1_batch2']\n    \n    n_batch1 = n01 + n11\n    n_batch2 = n02 + n12\n    n = n_batch1 + n_batch2\n\n    x = np.array([0]*n01 + [1]*n11 + [0]*n02 + [1]*n12)\n    z = np.array([0]*n_batch1 + [1]*n_batch2)\n\n    # 2. Generate gene expression data\n    # True model: y = mu + gamma*z + noise\n    mu_g = np.random.normal(0, 1, size=(m, 1))\n    noise = np.random.normal(0, sigma, size=(m, n))\n    Y = mu_g + gamma * z[np.newaxis, :] + noise\n\n    # 3. Define design matrices\n    X_naive = np.vstack([np.ones(n), x]).T\n    X_adjusted = np.vstack([np.ones(n), x, z]).T\n\n    # 4. Perform analyses and get discovery counts\n    naive_discoveries = perform_analysis(Y, X_naive, q)\n    adjusted_discoveries = perform_analysis(Y, X_adjusted, q)\n\n    return naive_discoveries, adjusted_discoveries\n\ndef perform_analysis(Y, X, q):\n    \"\"\"\n    Performs OLS regression and multiple testing correction for a set of genes.\n    \"\"\"\n    n, p = X.shape # n = samples, p = parameters\n    m = Y.shape[0] # m = genes\n    \n    # 1. Fit linear model for all genes at once using np.linalg.lstsq\n    # Y is (m, n), X is (n, p). We need to solve X @ B.T = Y.T for B.\n    # B will be (m, p). lstsq returns coefficients as (p, m).\n    beta_hat, rss_per_gene, _, _ = np.linalg.lstsq(X, Y.T, rcond=None)\n\n    # 2. Calculate t-statistics for the condition coefficient (at index 1)\n    df = n - p\n    sigma_sq_hat = rss_per_gene / df\n    \n    # The variance of beta_hat is diag(inv(X'X)) * sigma_hat^2\n    # We are interested in the coefficient for the condition 'x', which is at index 1\n    C = np.linalg.inv(X.T @ X)\n    se_beta1 = np.sqrt(sigma_sq_hat * C[1, 1])\n\n    # Avoid division by zero if standard error is somehow zero\n    # This should not happen in this problem's setup\n    t_stats = np.zeros(m)\n    valid_se = se_beta1 > 0\n    t_stats[valid_se] = beta_hat[1, valid_se] / se_beta1[valid_se]\n    \n    # 3. Calculate two-sided p-values\n    p_values = 2 * t.sf(np.abs(t_stats), df=df)\n\n    # 4. Apply Benjamini-Hochberg procedure\n    num_discoveries = bh_procedure(p_values, q)\n    \n    return num_discoveries\n\ndef bh_procedure(p_values, q):\n    \"\"\"\n    Applies the Benjamini-Hochberg procedure to control FDR.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return 0\n        \n    p_values_sorted = np.sort(p_values)\n    ranks = np.arange(1, m + 1)\n    thresholds = (ranks / m) * q\n    \n    # Find all p-values that are below the BH threshold\n    significant_mask = p_values_sorted <= thresholds\n    \n    if np.any(significant_mask):\n        # The number of discoveries is the rank of the last p-value\n        # that is below its threshold\n        k = np.max(np.where(significant_mask))\n        num_discoveries = k + 1\n    else:\n        num_discoveries = 0\n        \n    return num_discoveries\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2385475"}]}