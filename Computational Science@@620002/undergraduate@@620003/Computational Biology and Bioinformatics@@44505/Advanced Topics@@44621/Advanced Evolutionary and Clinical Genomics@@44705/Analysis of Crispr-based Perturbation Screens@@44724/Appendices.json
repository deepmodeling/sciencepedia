{"hands_on_practices": [{"introduction": "The core goal of a CRISPR screen is to identify which genetic perturbations cause a significant phenotypic change. This exercise [@problem_id:2371981] guides you through building a differential analysis pipeline from the ground up, mirroring the statistical engine of widely used bioinformatics tools. By implementing normalization, dispersion estimation, and a Generalized Linear Model (GLM) based on the Negative Binomial distribution, you will gain a profound, 'under-the-hood' understanding of how 'hits' are statistically validated in count-based screen data.", "problem": "You are asked to implement, from first principles, a complete differential analysis procedure for comparing two pooled Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) perturbation screens, in the spirit of Differential Expression analysis for Sequence count data (DESeq2). The goal is to identify condition-specific essential genes, defined here as those with significantly lower guide RNA abundance in one condition relative to the other after proper normalization and statistical modeling.\n\nStart from the following foundational bases and well-tested facts:\n- Count data from pooled screens are nonnegative integers and exhibit overdispersion relative to the Poisson model. A widely used model is the Negative Binomial (NB) distribution with variance function $V(\\mu) = \\mu + \\alpha \\mu^{2}$, where $\\mu$ is the mean and $\\alpha \\ge 0$ is the dispersion.\n- Generalized Linear Models (GLMs) with a log link are standard for modeling counts across conditions. For sample $j$ with size factor $s_{j}$ and a design matrix $X$ having an intercept and a binary condition indicator $x_{j} \\in \\{0,1\\}$, the model is $\\log \\mu_{gj} = \\log s_{j} + \\beta_{g0} + \\beta_{g1} x_{j}$ for gene $g$.\n- The median-of-ratios method provides robust library size normalization. For each sample $j$, the size factor $s_{j}$ is the median across genes of $k_{gj}/G_{g}$, where $k_{gj}$ is the raw count and $G_{g}$ is the geometric mean of $k_{g\\cdot}$ across samples, computed only over genes with strictly positive counts in all samples.\n- The Wald test using the asymptotic normal approximation with variance from the Fisher Information provides a way to test whether a coefficient equals zero.\n- The Benjamini–Hochberg (BH) procedure controls the False Discovery Rate (FDR).\n\nYou must implement the following steps:\n1) Size-factor normalization via the median-of-ratios method:\n   - For each gene $g$, compute the geometric mean $G_{g} = \\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log k_{gj} \\right)$ only if $k_{gj} > 0$ for all $j \\in \\{1,\\dots,m\\}$; otherwise exclude gene $g$ from this step.\n   - For each sample $j$, compute the ratios $r_{gj} = k_{gj} / G_{g}$ over all genes $g$ with valid $G_{g}$ and set $s_{j}$ to be the median of $\\{ r_{gj} \\}$.\n   - Center the size factors so that $\\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log s_{j} \\right) = 1$.\n2) Common dispersion estimation by method-of-moments:\n   - For each gene $g$, compute normalized counts $y_{gj} = k_{gj} / s_{j}$ and their mean $\\bar{y}_{g}$ and sample variance $S^{2}_{g}$ across all $m$ samples.\n   - For genes with $\\bar{y}_{g} > 0$, define a per-gene dispersion estimate $\\hat{\\alpha}_{g} = \\max\\left(0,\\; \\frac{S^{2}_{g} - \\bar{y}_{g}}{\\bar{y}_{g}^{2}} \\right)$.\n   - Define a single common dispersion $\\hat{\\alpha}$ as the median of $\\{ \\hat{\\alpha}_{g} \\}$ over genes with $\\bar{y}_{g} > 0$.\n3) For each gene $g$, fit a Negative Binomial GLM with a log link using Fisher scoring (Iteratively Reweighted Least Squares) with offset $\\log s_{j}$, common dispersion $\\hat{\\alpha}$, and design matrix with intercept and condition indicator $x_{j} \\in \\{0,1\\}$:\n   - Initialize $\\beta_{g0}$ and $\\beta_{g1}$ (for example, $\\beta_{g1}=0$ and $\\beta_{g0}$ as the log of the mean normalized count).\n   - At each iteration, compute the linear predictor $\\eta_{gj} = \\log s_{j} + \\beta_{g0} + \\beta_{g1} x_{j}$, mean $\\mu_{gj} = \\exp(\\eta_{gj})$, weights $w_{gj} = \\frac{\\mu_{gj}}{1 + \\hat{\\alpha}\\mu_{gj}}$, and working response $z_{gj} = \\eta_{gj} + \\frac{k_{gj} - \\mu_{gj}}{\\mu_{gj}}$.\n   - Update $\\beta_{g} = (\\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{z}$ until convergence or a fixed iteration cap, where $\\mathbf{W}$ is diagonal with entries $w_{gj}$.\n4) After convergence for gene $g$, extract the estimated coefficient $\\hat{\\beta}_{g1}$ and its standard error $\\operatorname{se}(\\hat{\\beta}_{g1})$ from the inverse of $(\\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{X})$. Form the Wald statistic $Z_{g} = \\hat{\\beta}_{g1} / \\operatorname{se}(\\hat{\\beta}_{g1})$ and the two-sided $p$-value using the standard normal distribution.\n5) Apply the Benjamini–Hochberg procedure across genes to obtain adjusted $q$-values.\n6) Define a gene to be “condition-specific essential in condition $1$ relative to condition $0$” if two criteria are both satisfied:\n   - The adjusted $q$-value is less than $\\alpha_{\\mathrm{FDR}}$.\n   - The estimated log$_{2}$ fold-change $\\widehat{\\mathrm{LFC}}_{g} = \\hat{\\beta}_{g1} / \\log 2$ is less than or equal to $- \\tau$, where $\\tau$ is a user-specified nonnegative threshold.\n\nGenes with zero counts in all samples, or with $\\bar{y}_{g} = 0$, should be excluded from testing.\n\nYour program must implement the above procedure and run on the following test suite. For each test case, the input consists of a raw count matrix with genes as rows and samples as columns, and a binary condition assignment vector of length equal to the number of samples. Use $\\alpha_{\\mathrm{FDR}} = 0.1$ and $\\tau = 1.0$, and report zero-based indices of genes called condition-specific essential for condition $1$ relative to condition $0$.\n\nTest suite:\n- Case $1$ (balanced replicates, clear differential depletion):\n  - Counts matrix $K$ with $6$ genes and $6$ samples (first $3$ samples condition $0$, last $3$ samples condition $1$):\n    - Gene $0$: $[100, 90, 110, 120, 80, 100]$\n    - Gene $1$: $[50, 45, 55, 60, 40, 50]$\n    - Gene $2$: $[30, 27, 33, 9, 6, 8]$\n    - Gene $3$: $[20, 18, 22, 24, 16, 20]$\n    - Gene $4$: $[10, 9, 11, 3, 2, 3]$\n    - Gene $5$: $[60, 54, 66, 72, 48, 60]$\n  - Condition vector $x = [0, 0, 0, 1, 1, 1]$.\n- Case $2$ (strong library size imbalance, single differential gene):\n  - Counts matrix $K$ with $5$ genes and $4$ samples (first $2$ samples condition $0$, last $2$ samples condition $1$):\n    - Gene $0$: $[160, 40, 120, 56]$\n    - Gene $1$: $[80, 20, 12, 6]$\n    - Gene $2$: $[40, 10, 30, 14]$\n    - Gene $3$: $[30, 7, 23, 10]$\n    - Gene $4$: $[10, 3, 8, 4]$\n  - Condition vector $x = [0, 0, 1, 1]$.\n- Case $3$ (sparse counts with zeros, one strong differential gene):\n  - Counts matrix $K$ with $6$ genes and $4$ samples (first $2$ samples condition $0$, last $2$ samples condition $1$):\n    - Gene $0$: $[2, 1, 2, 1]$\n    - Gene $1$: $[0, 0, 0, 0]$\n    - Gene $2$: $[3, 2, 3, 2]$\n    - Gene $3$: $[1, 0, 1, 0]$\n    - Gene $4$: $[4, 4, 4, 4]$\n    - Gene $5$: $[6, 5, 1, 1]$\n  - Condition vector $x = [0, 0, 1, 1]$.\n\nRequirements and outputs:\n- Implement the complete pipeline exactly as described.\n- For each case, return the sorted list of zero-based gene indices that satisfy the significance and effect-size criteria.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_{1},result_{2},result_{3}]$), where each $result_{i}$ is a list of integers denoting the indices of the called genes for case $i$. For the provided test suite, the program should output a single line like $[[i_{1},i_{2}], [j_{1}], [k_{1}]]$ with the discovered indices.", "solution": "The problem presented is a well-posed and scientifically sound task in computational biology. It requires the implementation of a standard statistical pipeline for identifying differentially abundant features from count data, specifically in the context of CRISPR screens. The methodology is a simplified variant of established algorithms like DESeq2, resting on a firm foundation of generalized linear models and established statistical practices. The problem is valid, and a rigorous solution can be constructed by following the outlined steps.\n\nThe analysis proceeds from first principles, as follows.\n\n**1. Statistical Foundation: The Negative Binomial Model**\nThe raw data from pooled CRISPR screens are counts, $k_{gj}$, representing the number of sequencing reads for guide RNA $g$ in sample $j$. Such data are non-negative integers and invariably exhibit overdispersion, meaning the variance is greater than the mean. The Poisson distribution, for which variance equals the mean, is therefore inadequate. We employ the Negative Binomial (NB) distribution, which adds a dispersion parameter $\\alpha$ to model this extra variance. The variance-mean relationship is given by:\n$$V(\\mu_{g}) = \\mu_{g} + \\alpha \\mu_{g}^{2}$$\nwhere $\\mu_{g}$ is the mean abundance for guide $g$ and $\\alpha \\ge 0$ is the dispersion parameter. For $\\alpha=0$, the NB distribution reduces to the Poisson.\n\n**2. Normalization for Library Size**\nDifferent samples are sequenced to different depths, leading to systematic variations in total read counts (library sizes). To make counts comparable across samples, we must normalize. The problem specifies the median-of-ratios method, which is robust to a large proportion of differentially abundant genes.\n\nFirst, for a subset of genes that have non-zero counts $k_{gj} > 0$ across all $m$ samples, we compute a pseudo-reference sample by taking the geometric mean of counts for each gene $g$:\n$$G_{g} = \\left( \\prod_{j=1}^{m} k_{gj} \\right)^{1/m} = \\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log k_{gj} \\right)$$\nSecond, for each sample $j$, we calculate the ratio of its count $k_{gj}$ to the pseudo-reference $G_{g}$ for all genes in the reference subset. The size factor $s_{j}$ for sample $j$ is the median of these ratios:\n$$s_{j} = \\underset{g}{\\text{median}} \\left\\{ \\frac{k_{gj}}{G_{g}} \\right\\}$$\nFinally, these size factors are centered to have a geometric mean of $1$, ensuring that the normalization does not change the total scale of the count data. This is achieved by dividing each $s_j$ by the geometric mean of all size factors.\n\n**3. Common Dispersion Estimation**\nThe dispersion parameter $\\alpha$ must be estimated from the data. While a per-gene dispersion estimate is possible, it is unstable for low replicate numbers. A common practice is to estimate a single, global dispersion value $\\hat{\\alpha}$ shared across all genes. This is a robust approach that pools information. The method-of-moments is used.\n\nFor each gene $g$, normalized counts are computed as $y_{gj} = k_{gj} / s_{j}$. We calculate the mean $\\bar{y}_{g}$ and sample variance $S^{2}_{g}$ of these normalized counts across all samples. From the NB variance function, we have $S_g^2 \\approx \\bar{y}_g + \\alpha_g \\bar{y}_g^2$. Rearranging gives a per-gene dispersion estimate:\n$$\\hat{\\alpha}_{g} = \\frac{S^{2}_{g} - \\bar{y}_{g}}{\\bar{y}_{g}^{2}}$$\nTo ensure non-negativity, we take $\\max(0, \\hat{\\alpha}_{g})$. The final common dispersion $\\hat{\\alpha}$ is the median of these per-gene estimates, calculated only over genes where $\\bar{y}_{g} > 0$. The median provides robustness against outlier genes with extreme variance.\n\n**4. Generalized Linear Model (GLM) Fitting**\nTo model the effect of the experimental condition on gene abundance, we fit a Negative Binomial GLM to the counts for each gene. The model relates the expected count $\\mu_{gj}$ to the experimental variables via a log link function:\n$$\\log(\\mu_{gj}) = \\log(s_{j}) + \\beta_{g0} + \\beta_{g1} x_{j}$$\nHere, $\\log(s_j)$ is an offset term to account for the library size normalization. $\\mathbf{X}$ is the design matrix with a column of ones for the intercept and a column for the binary condition indicator $x_j \\in \\{0, 1\\}$. The coefficients $\\beta_{g0}$ and $\\beta_{g1}$ represent the log-baseline abundance and the log-fold change between conditions, respectively.\n\nWe estimate the coefficients $\\beta_g = [\\beta_{g0}, \\beta_{g1}]^T$ using Iteratively Reweighted Least Squares (IRLS), which is equivalent to Fisher scoring for this GLM. The iterative update is:\n$$\\beta_g^{(t+1)} = (\\mathbf{X}^{\\top} \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}$$\nwhere, at iteration $t$:\n- $\\eta_{gj} = \\log(s_j) + \\beta_{g0}^{(t)} + \\beta_{g1}^{(t)} x_{j}$ is the linear predictor.\n- $\\mu_{gj} = \\exp(\\eta_{gj})$ is the estimated mean.\n- $w_{gj} = \\frac{\\mu_{gj}}{1 + \\hat{\\alpha}\\mu_{gj}}$ are the diagonal elements of the weight matrix $\\mathbf{W}$.\n- $z_{gj} = \\eta_{gj} + \\frac{k_{gj} - \\mu_{gj}}{\\mu_{gj}}$ is the working response.\nThe iteration proceeds until the change in $\\beta_g$ is below a tolerance threshold or a maximum number of iterations is reached.\n\n**5. Hypothesis Testing and Multiple Test Correction**\nOur primary interest is whether the condition has a significant effect, i.e., if $\\beta_{g1} \\neq 0$. We use a Wald test for this purpose. The test statistic is:\n$$Z_{g} = \\frac{\\hat{\\beta}_{g1}}{\\operatorname{se}(\\hat{\\beta}_{g1})}$$\nwhere $\\hat{\\beta}_{g1}$ is the coefficient estimate from the converged IRLS, and its standard error $\\operatorname{se}(\\hat{\\beta}_{g1})$ is derived from the asymptotic covariance matrix of the estimator, given by $\\widehat{\\text{Cov}}(\\hat{\\beta}_g) = (\\mathbf{X}^{\\top} \\mathbf{W}_{\\text{final}} \\mathbf{X})^{-1}$. Specifically, $\\operatorname{se}(\\hat{\\beta}_{g1})$ is the square root of the diagonal element of this matrix corresponding to $\\beta_{g1}$. Under the null hypothesis $H_0: \\beta_{g1} = 0$, the statistic $Z_g$ follows a standard normal distribution. This allows for the calculation of a two-sided $p$-value.\n\nSince we perform this test for thousands of genes simultaneously, we face a multiple testing problem. To control the proportion of false discoveries, we adjust the $p$-values using the Benjamini-Hochberg (BH) procedure, which yields $q$-values (FDR-adjusted $p$-values).\n\n**6. Identification of Condition-Specific Essential Genes**\nA gene is identified as \"condition-specific essential in condition $1$ relative to condition $0$\" if it meets two criteria:\n1.  **Statistical Significance**: The adjusted $q$-value must be below a specified threshold, $q_g < \\alpha_{\\mathrm{FDR}}$ (e.g., $0.1$).\n2.  **Effect Size**: The gene must be depleted in condition $1$. We quantify this using the log-base-$2$ fold change, $\\widehat{\\mathrm{LFC}}_{g} = \\hat{\\beta}_{g1} / \\log 2$. The criterion is $\\widehat{\\mathrm{LFC}}_{g} \\le -\\tau$, where $\\tau$ is a non-negative magnitude threshold (e.g., $1.0$, corresponding to at least a halving of abundance).\n\nGenes with zero counts across all samples or for which a model cannot be reliably fit are excluded from the analysis. The final output is the sorted list of indices for genes that satisfy both criteria.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the differential analysis pipeline on all test cases.\n    \"\"\"\n\n    class DifferentialAnalysis:\n        \"\"\"\n        Implements the complete differential analysis pipeline for CRISPR screen count data.\n        \"\"\"\n        \n        def __init__(self, counts, conditions, alpha_fdr, tau):\n            self.raw_counts = np.asarray(counts, dtype=float)\n            self.conditions = np.asarray(conditions, dtype=float)\n            self.alpha_fdr = alpha_fdr\n            self.tau_lfc = tau\n            self.num_genes, self.num_samples = self.raw_counts.shape\n            self.log2 = np.log(2)\n            self.irls_iterations = 25\n            self.irls_tol = 1e-6\n\n        def run(self):\n            \"\"\"\n            Executes the full analysis pipeline.\n            \"\"\"\n            # Step 0: Filter genes that are untestable from the start\n            genes_with_counts = self.raw_counts.sum(axis=1) > 0\n            if not np.any(genes_with_counts):\n                return []\n\n            # Step 1: Size-factor normalization\n            size_factors = self._calculate_size_factors()\n            if size_factors is None: # Happens if no genes for geo mean calc\n                return []\n            \n            normalized_counts = self.raw_counts / size_factors\n            mean_normalized_counts = np.mean(normalized_counts, axis=1)\n            \n            # Further filter genes where mean normalized count is zero\n            testable_genes_mask = genes_with_counts & (mean_normalized_counts > 0)\n            testable_gene_indices = np.where(testable_genes_mask)[0]\n            if len(testable_gene_indices) == 0:\n                return []\n\n            # Step 2: Common dispersion estimation\n            common_dispersion = self._estimate_common_dispersion(size_factors, testable_genes_mask)\n\n            # Step 3 & 4: Per-gene GLM fit and Wald test\n            design_matrix = np.vstack([np.ones(self.num_samples), self.conditions]).T\n            \n            p_values = []\n            betas = []\n            final_testable_indices = []\n\n            for i in testable_gene_indices:\n                counts_g = self.raw_counts[i, :]\n                try:\n                    beta, cov_matrix = self._fit_nb_glm(counts_g, design_matrix, size_factors, common_dispersion)\n                    \n                    beta1 = beta[1]\n                    se_beta1 = np.sqrt(cov_matrix[1, 1])\n                    \n                    if np.isinf(se_beta1) or se_beta1 < 1e-8:\n                        p_val = 1.0\n                    else:\n                        wald_stat = beta1 / se_beta1\n                        p_val = 2 * (1 - norm.cdf(np.abs(wald_stat)))\n                    \n                    p_values.append(p_val)\n                    betas.append(beta1)\n                    final_testable_indices.append(i)\n                except (np.linalg.LinAlgError, ValueError):\n                    continue\n\n            # Step 5: Benjamini-Hochberg FDR correction\n            if not p_values:\n                return []\n            q_values = self._benjamini_hochberg(p_values)\n\n            # Step 6: Identify condition-specific essential genes\n            significant_genes = []\n            for gene_idx, beta1, q_val in zip(final_testable_indices, betas, q_values):\n                lfc = beta1 / self.log2\n                if q_val < self.alpha_fdr and lfc <= -self.tau_lfc:\n                    significant_genes.append(gene_idx)\n            \n            return sorted(significant_genes)\n\n        def _calculate_size_factors(self):\n            genes_for_geo_mean_mask = np.all(self.raw_counts > 0, axis=1)\n            \n            if not np.any(genes_for_geo_mean_mask):\n                return np.ones(self.num_samples) # Fallback if no gene is viable\n\n            counts_subset = self.raw_counts[genes_for_geo_mean_mask, :]\n            geo_means = np.exp(np.mean(np.log(counts_subset), axis=1))\n            \n            ratios = counts_subset / geo_means[:, np.newaxis]\n            size_factors_uncentered = np.median(ratios, axis=0)\n            \n            if np.any(size_factors_uncentered <= 0): # Avoid log(0) or log(-)\n                return size_factors_uncentered / np.mean(size_factors_uncentered)\n\n            geo_mean_sf = np.exp(np.mean(np.log(size_factors_uncentered)))\n            size_factors = size_factors_uncentered / geo_mean_sf\n            return size_factors\n\n        def _estimate_common_dispersion(self, size_factors, valid_genes_mask):\n            counts_subset = self.raw_counts[valid_genes_mask, :]\n            if counts_subset.shape[0] == 0:\n                return 0.0\n\n            normalized_counts = counts_subset / size_factors\n            mean_y = np.mean(normalized_counts, axis=1)\n            var_y = np.var(normalized_counts, axis=1, ddof=1)\n            \n            mean_y_sq = mean_y**2\n            # Add a small epsilon to avoid division by zero\n            mean_y_sq[mean_y_sq == 0] = 1e-8\n\n            dispersions_g = (var_y - mean_y) / mean_y_sq\n            dispersions_g[dispersions_g < 0] = 0\n            \n            return np.median(dispersions_g)\n\n        def _fit_nb_glm(self, counts_g, X, s, alpha):\n            mean_norm_count = np.mean(counts_g / s)\n            if mean_norm_count <= 0:\n                raise ValueError(\"Mean normalized count is zero or negative.\")\n            \n            beta = np.array([np.log(mean_norm_count), 0.0])\n\n            for _ in range(self.irls_iterations):\n                beta_old = beta.copy()\n                \n                eta = X @ beta + np.log(s)\n                mu = np.exp(eta)\n                \n                weights = mu / (1.0 + alpha * mu)\n                if np.sum(weights) < 1e-8:\n                    raise np.linalg.LinAlgError(\"All weights are near zero.\")\n                \n                W = np.diag(weights)\n                z = eta + (counts_g - mu) / mu\n                \n                XT_W_X = X.T @ W @ X\n                XT_W_X_inv = np.linalg.inv(XT_W_X)\n                beta = XT_W_X_inv @ X.T @ W @ z\n                \n                if np.sum(np.abs(beta - beta_old)) / (np.sum(np.abs(beta_old)) + 1e-8) < self.irls_tol:\n                    break\n            \n            # Recalculate final covariance matrix with converged beta\n            eta = X @ beta + np.log(s)\n            mu = np.exp(eta)\n            weights = mu / (1.0 + alpha * mu)\n            W = np.diag(weights)\n            XT_W_X = X.T @ W @ X\n            cov_matrix = np.linalg.inv(XT_W_X)\n            \n            return beta, cov_matrix\n\n        def _benjamini_hochberg(self, p_values):\n            p_values = np.asarray(p_values)\n            num_tests = len(p_values)\n            \n            sorted_indices = np.argsort(p_values)\n            sorted_p_values = p_values[sorted_indices]\n            \n            ranks = np.arange(1, num_tests + 1)\n            q_values_sorted = sorted_p_values * num_tests / ranks\n            \n            q_values_sorted = np.minimum.accumulate(q_values_sorted[::-1])[::-1]\n            \n            q_values = np.empty_like(p_values)\n            q_values[sorted_indices] = q_values_sorted\n            return q_values\n\n    # Test suite from the problem statement\n    test_cases = [\n        # Case 1\n        (\n            [[100, 90, 110, 120, 80, 100], [50, 45, 55, 60, 40, 50], [30, 27, 33, 9, 6, 8],\n             [20, 18, 22, 24, 16, 20], [10, 9, 11, 3, 2, 3], [60, 54, 66, 72, 48, 60]],\n            [0, 0, 0, 1, 1, 1]\n        ),\n        # Case 2\n        (\n            [[160, 40, 120, 56], [80, 20, 12, 6], [40, 10, 30, 14],\n             [30, 7, 23, 10], [10, 3, 8, 4]],\n            [0, 0, 1, 1]\n        ),\n        # Case 3\n        (\n            [[2, 1, 2, 1], [0, 0, 0, 0], [3, 2, 3, 2], [1, 0, 1, 0],\n             [4, 4, 4, 4], [6, 5, 1, 1]],\n            [0, 0, 1, 1]\n        )\n    ]\n    \n    alpha_fdr = 0.1\n    tau = 1.0\n    all_results = []\n\n    for counts, conditions in test_cases:\n        analyzer = DifferentialAnalysis(counts, conditions, alpha_fdr, tau)\n        result = analyzer.run()\n        all_results.append(result)\n\n    # Format the final output exactly as required, handling spaces in list string representation.\n    # The default str() includes spaces, which is consistent with the problem's example format: [[i_1, i_2], [j_1], [k_1]]\n    output_str = ','.join(map(str, all_results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2371981"}, {"introduction": "The advent of single-cell CRISPR screens brings new challenges, notably the issue of 'doublets'—droplets containing two cells with different perturbations, confounding the analysis. This practice [@problem_id:2371971] puts you in the role of a modeler, tasking you with implementing a deconvolution algorithm based on a clear probabilistic framework. You will use Maximum A Posteriori (MAP) estimation to determine the most likely perturbation identity for each droplet, a crucial step for ensuring data quality in single-cell screens.", "problem": "You are given a formalized deconvolution task for a single-cell Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) perturbation screen with a high rate of droplets containing two cells. Each droplet yields a non-negative integer vector of feature counts and may contain zero, one, or two distinct perturbations. The goal is to decide, for each droplet, which perturbation subset most plausibly generated the observed counts under a specified generative model.\n\nMathematical setup:\n\n- There are $G=3$ distinct perturbations indexed by $0,1,2$ and $F=3$ features indexed by $1,2,3$.\n- Let $\\mathbf{b}\\in \\mathbb{R}_{>0}^{F}$ denote the baseline mean vector of feature abundances for an unperturbed cell. You are given\n$$\n\\mathbf{b} = \\begin{bmatrix} 2.0 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}.\n$$\n- Let $\\mathbf{S}\\in \\mathbb{R}_{\\ge 0}^{F\\times G}$ denote the perturbation signature matrix whose column $\\mathbf{S}_{:,g}$ is the expected non-negative mean shift attributable to perturbation $g$ in a singlet. You are given\n$$\n\\mathbf{S} = \\begin{bmatrix}\n3.0 & 0.0 & 1.0 \\\\\n1.0 & 2.0 & 0.0 \\\\\n0.0 & 2.0 & 3.0\n\\end{bmatrix}.\n$$\n- For a droplet with size factor $s>0$ and an assignment $A\\subseteq \\{0,1,2\\}$ of size $|A|\\in\\{0,1,2\\}$, the mean vector $\\boldsymbol{\\mu}(A,s)\\in \\mathbb{R}_{>0}^{F}$ is defined as\n$$\n\\boldsymbol{\\mu}(A,s) = s\\left(\\mathbf{b} + \\sum_{g\\in A}\\mathbf{S}_{:,g}\\right).\n$$\n- Conditional on $(A,s)$, the observed feature count vector $\\mathbf{x}\\in \\mathbb{N}_0^{F}$ has independent Poisson components,\n$$\nx_f \\mid (A,s) \\sim \\mathrm{Poisson}\\!\\left(\\mu_f(A,s)\\right)\\quad \\text{independently for } f=1,2,3.\n$$\n- The prior probabilities over assignment cardinalities are given by $P(|A|=0)=p_0$, $P(|A|=1)=p_1$, and $P(|A|=2)=p_2$, with\n$$\np_0 = 0.05,\\quad p_1 = 0.70,\\quad p_2 = 0.25,\\quad p_0+p_1+p_2=1.\n$$\nConditional on the cardinality, all assignments of that cardinality are equiprobable, i.e., for $G=3$,\n$$\nP(A=\\emptyset) = p_0,\\quad P(A=\\{g\\})=\\frac{p_1}{G}\\ \\text{ for each } g\\in\\{0,1,2\\},\\quad P(A=\\{g,h\\})=\\frac{p_2}{\\binom{G}{2}}\\ \\text{ for each unordered } \\{g,h\\},\\ g\\neq h.\n$$\n\nDecision rule for a single droplet:\n\n- For a droplet with observed $\\mathbf{x}\\in \\mathbb{N}_0^{F}$ and size factor $s>0$, consider the candidate assignment set\n$$\n\\mathcal{A}=\\left\\{\\emptyset\\right\\}\\cup \\left\\{\\{g\\}\\,:\\,g\\in\\{0,1,2\\}\\right\\}\\cup \\left\\{\\{g,h\\}\\,:\\,0\\le g<h\\le 2\\right\\}.\n$$\n- For each $A\\in\\mathcal{A}$, define the joint log-score\n$$\n\\ell(A;\\mathbf{x},s) \\equiv \\log P(A) + \\sum_{f=1}^{F}\\left[x_f\\log \\mu_f(A,s) - \\mu_f(A,s) - \\log\\left(x_f!\\right)\\right].\n$$\n- Select the assignment $\\hat{A}$ that maximizes $\\ell(A;\\mathbf{x},s)$ over $A\\in\\mathcal{A}$. If there are ties within an absolute tolerance of $\\varepsilon=10^{-9}$ in $\\ell$, break ties by first preferring smaller $|A|$, and if still tied, by lexicographically smaller ordered tuples of indices.\n\nTest suite:\n\nUse the following five droplets, each specified by its count vector $\\mathbf{x}$ and size factor $s$.\n\n- Case $1$: $\\mathbf{x}=\\begin{bmatrix}5\\\\2\\\\2\\end{bmatrix}$, $s=1.0$.\n- Case $2$: $\\mathbf{x}=\\begin{bmatrix}5\\\\3\\\\4\\end{bmatrix}$, $s=1.0$.\n- Case $3$: $\\mathbf{x}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$, $s=0.5$.\n- Case $4$: $\\mathbf{x}=\\begin{bmatrix}7\\\\1\\\\10\\end{bmatrix}$, $s=2.0$.\n- Case $5$: $\\mathbf{x}=\\begin{bmatrix}3\\\\1\\\\5\\end{bmatrix}$, $s=1.2$.\n\nRequired program behavior:\n\n- Implement the decision rule above exactly as stated, using the given $\\mathbf{b}$, $\\mathbf{S}$, $p_0$, $p_1$, $p_2$, and $G=3$, $F=3$.\n- For each case, output the selected assignment as a list of integers: the empty set as $[\\ ]$, a singlet $\\{g\\}$ as $[g]$, and a doublet $\\{g,h\\}$ (with $g<h$) as $[g,h]$.\n- Final output format: Your program should produce a single line of output containing the results for all five cases as a comma-separated list enclosed in square brackets, where each element is itself a bracketed list of guide indices. For example, an output with five case results might look like\n$$\n[[0],[0,1],[],[2],[2]].\n$$\nYour program must print exactly one line in this bracketed format, with no additional text.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It presents a well-posed statistical inference problem rooted in the principles of computational biology for analyzing CRISPR-based perturbation screens. All parameters, variables, and conditions are specified with sufficient clarity and mathematical precision to permit a unique and meaningful solution. The underlying generative model, which employs a Poisson distribution for feature counts, and the linear model for perturbation effects, are standard and appropriate for this domain. We will now proceed to construct the solution.\n\nThe task is to implement a decision rule to identify the most plausible perturbation assignment $A$ for a given single-cell observation, which consists of a feature count vector $\\mathbf{x} \\in \\mathbb{N}_0^{F}$ and a cell-specific size factor $s > 0$. The decision is based on maximizing a joint log-score, which is equivalent to finding the maximum a posteriori (MAP) estimate for the assignment $A$.\n\nThe set of possible perturbation assignments for $G=3$ perturbations is\n$$\n\\mathcal{A}=\\left\\{\\emptyset\\right\\}\\cup \\left\\{\\{0\\}, \\{1\\}, \\{2\\}\\right\\}\\cup \\left\\{\\{0,1\\}, \\{0,2\\}, \\{1,2\\}\\right\\}.\n$$\nThere are $1 + \\binom{3}{1} + \\binom{3}{2} = 1 + 3 + 3 = 7$ possible assignments.\n\nThe prior probabilities $P(A)$ for these assignments are given by the cardinality-based priors $p_0=0.05$, $p_1=0.70$, and $p_2=0.25$.\n- For $|A|=0$: $P(A=\\emptyset) = p_0 = 0.05$.\n- For $|A|=1$: $P(A=\\{g\\}) = p_1/G = 0.70/3$ for each of the $3$ singlets.\n- For $|A|=2$: $P(A=\\{g,h\\}) = p_2/\\binom{G}{2} = 0.25/3$ for each of the $3$ doublets.\n\nThe generative model for the observed counts $\\mathbf{x}$ is a set of independent Poisson distributions:\n$$\nx_f \\sim \\mathrm{Poisson}(\\mu_f(A,s)) \\quad \\text{for } f \\in \\{1, 2, ..., F\\},\n$$\nwhere the mean vector $\\boldsymbol{\\mu}(A,s)$ is defined as a function of the assignment $A$ and size factor $s$:\n$$\n\\boldsymbol{\\mu}(A,s) = s\\left(\\mathbf{b} + \\sum_{g\\in A}\\mathbf{S}_{:,g}\\right).\n$$\nThe parameters are the baseline mean vector $\\mathbf{b} = \\begin{bmatrix} 2.0 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}$ and the signature matrix $\\mathbf{S} = \\begin{bmatrix} 3.0 & 0.0 & 1.0 \\\\ 1.0 & 2.0 & 0.0 \\\\ 0.0 & 2.0 & 3.0 \\end{bmatrix}$.\n\nThe objective is to find the assignment $\\hat{A}$ that maximizes the joint log-score $\\ell(A;\\mathbf{x},s)$:\n$$\n\\hat{A} = \\arg\\max_{A \\in \\mathcal{A}} \\ell(A;\\mathbf{x},s) = \\arg\\max_{A \\in \\mathcal{A}} \\left( \\log P(A) + \\log P(\\mathbf{x} \\mid A, s) \\right).\n$$\nThe log-likelihood term $\\log P(\\mathbf{x} \\mid A, s)$ is derived from the Poisson probability mass function:\n$$\n\\log P(\\mathbf{x} \\mid A, s) = \\sum_{f=1}^{F} \\log\\left(\\frac{e^{-\\mu_f(A,s)} \\mu_f(A,s)^{x_f}}{x_f!}\\right) = \\sum_{f=1}^{F} \\left(x_f\\log \\mu_f(A,s) - \\mu_f(A,s) - \\log(x_f!)\\right).\n$$\nThe term $\\sum_{f=1}^F \\log(x_f!)$ is constant for a given $\\mathbf{x}$ across all candidate assignments $A$. Therefore, it can be dropped from the maximization objective. We define a simplified score $\\ell'(A; \\mathbf{x}, s)$:\n$$\n\\ell'(A; \\mathbf{x}, s) = \\log P(A) + \\sum_{f=1}^{F}\\left(x_f\\log \\mu_f(A,s) - \\mu_f(A,s)\\right).\n$$\nMaximizing $\\ell'$ is equivalent to maximizing $\\ell$.\n\nThe algorithm for deciding the assignment for a single droplet $(\\mathbf{x}, s)$ is as follows:\n1. For each candidate assignment $A \\in \\mathcal{A}$:\n   a. Compute the prior term $\\log P(A)$. These values can be pre-calculated.\n   b. Compute the mean vector $\\boldsymbol{\\mu}(A,s)$ using the given $\\mathbf{b}$, $\\mathbf{S}$, and $s$.\n   c. Compute the log-likelihood component $\\sum_{f=1}^{F}\\left(x_f\\log \\mu_f(A,s) - \\mu_f(A,s)\\right)$.\n   d. Sum the terms from (a) and (c) to obtain the score $\\ell'(A)$.\n   e. Store the triplet $(A, |A|, \\ell'(A))$.\n\n2. After computing the scores for all $7$ assignments, identify the optimal assignment $\\hat{A}$ using the specified tie-breaking rule:\n   a. Find the maximum score, $L_{max} = \\max_{A \\in \\mathcal{A}} \\ell'(A)$.\n   b. Identify the set of candidates $\\mathcal{A}_{tied} = \\{ A \\in \\mathcal{A} \\mid L_{max} - \\ell'(A) < \\varepsilon \\}$ where $\\varepsilon=10^{-9}$.\n   c. From $\\mathcal{A}_{tied}$, select the assignment with the minimum cardinality $|A|$.\n   d. If a tie still exists, select the assignment whose indices, when sorted, form a lexicographically smallest tuple. For example, $\\{0,2\\}$ is preferred over $\\{1,2\\}$.\n\nThis procedure is applied to each of the five test cases provided. The final implementation will encapsulate this logic in a program that processes all cases and formats the output as required.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the CRISPR screen deconvolution problem for a given set of test cases.\n    \"\"\"\n    # Define problem constants and givens\n    G = 3\n    F = 3\n    b = np.array([2.0, 1.0, 1.5])\n    S = np.array([\n        [3.0, 0.0, 1.0],\n        [1.0, 2.0, 0.0],\n        [0.0, 2.0, 3.0]\n    ])\n    p0, p1, p2 = 0.05, 0.70, 0.25\n    epsilon = 1e-9\n\n    # Define the five test cases\n    test_cases = [\n        {'x': np.array([5, 2, 2]), 's': 1.0},\n        {'x': np.array([5, 3, 4]), 's': 1.0},\n        {'x': np.array([0, 0, 0]), 's': 0.5},\n        {'x': np.array([7, 1, 10]), 's': 2.0},\n        {'x': np.array([3, 1, 5]), 's': 1.2},\n    ]\n\n    # Pre-compute candidate assignments and their log-priors\n    assignments = [\n        (),          # Empty set\n        (0,), (1,), (2,), # Singlets\n        (0, 1), (0, 2), (1, 2) # Doublets\n    ]\n    \n    # The number of doublets is comb(G, 2) = comb(3, 2) = 3\n    num_doublets = 3\n    log_priors = {\n        (): math.log(p0),\n        (0,): math.log(p1 / G),\n        (1,): math.log(p1 / G),\n        (2,): math.log(p1 / G),\n        (0, 1): math.log(p2 / num_doublets),\n        (0, 2): math.log(p2 / num_doublets),\n        (1, 2): math.log(p2 / num_doublets),\n    }\n\n    final_results = []\n    for case in test_cases:\n        x, s = case['x'], case['s']\n        \n        candidates = []\n        for A in assignments:\n            # Step 1: Calculate the mean vector mu(A, s)\n            if not A:\n                S_sum = np.zeros(F)\n            else:\n                S_sum = np.sum(S[:, list(A)], axis=1)\n            \n            mu = s * (b + S_sum)\n            \n            # Step 2: Calculate the simplified log-score l'(A)\n            # The problem statement's constraints ensure mu_f > 0.\n            log_likelihood_part = np.dot(x, np.log(mu)) - np.sum(mu)\n            score = log_priors[A] + log_likelihood_part\n            \n            candidates.append({\n                'score': score, \n                'cardinality': len(A), \n                'assignment': A\n            })\n\n        # Step 3: Find the best assignment with tie-breaking\n        # 3a: Find the maximum score\n        max_score = -np.inf\n        for c in candidates:\n            if c['score'] > max_score:\n                max_score = c['score']\n        \n        # 3b: Filter for candidates within epsilon of the max score\n        tied_candidates = []\n        for c in candidates:\n            if max_score - c['score'] < epsilon:\n                tied_candidates.append(c)\n        \n        # 3c: Sort tied candidates by cardinality, then lexicographically by assignment tuple.\n        # Python's sort is stable, so we can sort by secondary/tertiary keys first.\n        tied_candidates.sort(key=lambda c: c['assignment']) # Tertiary key: lexicographical\n        tied_candidates.sort(key=lambda c: c['cardinality']) # Secondary key: cardinality\n        \n        # The best assignment is the first element after sorting\n        best_assignment = tied_candidates[0]['assignment']\n        final_results.append(list(best_assignment))\n\n    # Step 4: Format the output string as specified\n    result_strings = []\n    for res in final_results:\n        # Format [0,1] as \"[0,1]\", [] as \"[]\", [2] as \"[2]\"\n        if not res:\n            result_strings.append(\"[]\")\n        else:\n            result_strings.append(f\"[{','.join(map(str, res))}]\")\n            \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2371971"}, {"introduction": "How can we trust our analytical algorithms, especially when faced with complex biology? This final practice [@problem_id:2372047] introduces the essential skill of simulation-based benchmarking, where you create a synthetic dataset with a known 'ground truth' to test algorithmic performance. You will design a screen with a challenging scenario—a gene whose isoforms have opposing effects—to compare how a p-value-based method and a Bayesian approach fare, revealing their intrinsic strengths and weaknesses.", "problem": "You are given the task of constructing a fully specified simulation for the analysis of Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-based pooled perturbation screens to evaluate how two different hit-calling algorithms behave when a single gene has two isoforms with opposing functional effects and is targeted by different guide ribonucleic acids (gRNAs). You must implement a program that follows the mathematical specification below, runs a specified set of test cases, and outputs a single-line list of integers encoding the comparative correctness of the two algorithms on each test case.\n\nConsider a pooled screen with $G$ genes and $K$ guides per gene. There is one special gene, denoted $g^\\star$, that has two isoforms with opposing functions. Each guide is sequenced before selection (time $0$) and after selection (time $1$). For each guide $i$ in gene $j$, the pre-selection count $X_{0,ji}$ and post-selection count $X_{1,ji}$ are independent draws from a Negative Binomial distribution chosen to have specified means and a common dispersion parameter. All random sampling must be performed deterministically by seeding the random number generator with the provided seed of each test case.\n\nDefine the following parameters, which are fixed across all test cases unless stated otherwise:\n\n- Total number of genes $G = 200$.\n- Guides per gene $K = 6$.\n- Special gene index $g^\\star = 0$ (zero-based indexing).\n- Baseline pre-selection mean count per guide $\\mu_0 = 500$.\n- Common Negative Binomial dispersion (size parameter) $r = 50$.\n- One-sided significance level for Algorithm M $\\alpha = 0.05$.\n- Bayes factor threshold for Algorithm B $T = 10$.\n- Alternative-effect mean magnitude parameter for Algorithm B $\\beta = 0.6$.\n- Natural logarithms must be used for all logarithmic operations.\n\nNegative Binomial parameterization and sampling: For a desired mean $\\mu$ and dispersion (size) $r$, let $p = \\frac{r}{r+\\mu}$. Then a draw $Y \\sim \\mathrm{NB}(r,p)$ has mean $\\mu$ and variance $\\mu + \\frac{\\mu^2}{r}$. Use this parameterization for all count draws.\n\nTrue guide-level effects: Let $\\ell_{ji}$ denote the true log fold change for guide $i$ in gene $j$.\n\n- For all genes $j \\ne g^\\star$ and all their guides, set $\\ell_{ji} = 0$.\n- For the special gene $g^\\star$, there are three categories of guides:\n  1. Isoform-A-targeting guides with true effect $+\\mu$ (enrichment).\n  2. Isoform-B-targeting guides with true effect $-\\mu$ (depletion).\n  3. Shared-exon guides with true effect $0$.\n  The fractions of the $K$ guides assigned to these three categories are $p_A$, $p_B$, and $p_S$, respectively, with $p_A + p_B + p_S = 1$. To convert fractions to integers $(K_A, K_B, K_S)$ summing to $K$, first compute $(\\lfloor p_A K \\rfloor, \\lfloor p_B K \\rfloor, \\lfloor p_S K \\rfloor)$ and the remainders $(\\{p_A K\\}, \\{p_B K\\}, \\{p_S K\\})$. Let $R = K - (\\lfloor p_A K \\rfloor + \\lfloor p_B K \\rfloor + \\lfloor p_S K \\rfloor)$. Distribute the remaining $R$ guides by giving $+1$ to the categories in decreasing order of their remainders; break any ties by the fixed order A, then B, then S. Assign the first $K_A$ guides to $+\\mu$, the next $K_B$ guides to $-\\mu$, and the remaining $K_S$ guides to $0$.\n\nCount generation and observed effects: For each guide $i$ in gene $j$, draw\n- $X_{0,ji} \\sim \\mathrm{NB}(r, \\frac{r}{r+\\mu_0})$,\n- $X_{1,ji} \\sim \\mathrm{NB}(r, \\frac{r}{r+\\mu_0 \\exp(\\ell_{ji})})$.\nDefine the observed guide-level log fold change $L_{ji} = \\log\\left(\\frac{X_{1,ji} + 1}{X_{0,ji} + 1}\\right)$.\n\nDefine a robust scale estimate $\\hat{\\sigma}$ from all guide-level $L_{ji}$ values by the median absolute deviation: Let $m = \\mathrm{median}(\\{L_{ji}\\})$ and $\\mathrm{MAD} = \\mathrm{median}(|L_{ji} - m|)$. Set $\\hat{\\sigma} = 1.4826 \\cdot \\mathrm{MAD}$. If $\\hat{\\sigma} = 0$, replace it by the sample standard deviation of $\\{L_{ji}\\}$; if that is also $0$, replace by a small constant $10^{-8}$.\n\nAlgorithm M (a direction-specific rank-minimum $p$-value with Bonferroni adjustment):\n1. For each guide $L_{ji}$, define the standardized value $Z_{ji} = \\frac{L_{ji}}{\\hat{\\sigma}}$.\n2. Let $\\Phi(\\cdot)$ denote the standard normal cumulative distribution function. Define one-sided $p$-values for depletion and enrichment as $p^-_{ji} = \\Phi(Z_{ji})$ and $p^+_{ji} = 1 - \\Phi(Z_{ji})$, respectively.\n3. For each gene $j$, define Bonferroni-adjusted gene-level values $P^-_j = \\min\\{1, K \\cdot \\min_i p^-_{ji}\\}$ and $P^+_j = \\min\\{1, K \\cdot \\min_i p^+_{ji}\\}$.\n4. Define $P_j = \\min(P^-_j, P^+_j)$ and the predicted direction $\\mathrm{dir}_M(j) = +1$ if $P^+_j  P^-_j$, otherwise $\\mathrm{dir}_M(j) = -1$. In case $P^+_j = P^-_j$, break the tie by setting $\\mathrm{dir}_M(j) = +1$.\n5. Algorithm M calls gene $j$ a hit if $P_j \\le \\alpha$; otherwise, it does not call a hit.\n\nAlgorithm B (a two-sided Bayes factor using a Gaussian likelihood ratio with fixed alternatives):\n1. Let the null distribution for $L_{ji}$ be $\\mathcal{N}(0, \\tau^2)$ with $\\tau = \\hat{\\sigma}$.\n2. Define two alternative distributions: depletion $\\mathcal{N}(-\\beta, \\tau^2)$ and enrichment $\\mathcal{N}(+\\beta, \\tau^2)$.\n3. For gene $j$, compute the log Bayes factors\n$$\n\\log \\mathrm{BF}^-(j) = \\sum_{i=1}^K \\left[ \\log \\phi(L_{ji}; -\\beta, \\tau) - \\log \\phi(L_{ji}; 0, \\tau) \\right],\n$$\n$$\n\\log \\mathrm{BF}^+(j) = \\sum_{i=1}^K \\left[ \\log \\phi(L_{ji}; +\\beta, \\tau) - \\log \\phi(L_{ji}; 0, \\tau) \\right],\n$$\nwhere $\\phi(x; \\mu, \\tau)$ is the Gaussian probability density function with mean $\\mu$ and standard deviation $\\tau$. Let $\\log \\mathrm{BF}_{\\max}(j) = \\max(\\log \\mathrm{BF}^-(j), \\log \\mathrm{BF}^+(j))$. The predicted direction is $\\mathrm{dir}_B(j) = +1$ if $\\log \\mathrm{BF}^+(j) > \\log \\mathrm{BF}^-(j)$, otherwise $\\mathrm{dir}_B(j) = -1$. In case of equality, break the tie by setting $\\mathrm{dir}_B(j) = +1$.\n4. Algorithm B calls gene $j$ a hit if $\\log \\mathrm{BF}_{\\max}(j) \\ge \\log T$; otherwise, it does not call a hit.\n\nGround-truth direction for the special gene $g^\\star$ is determined solely by the design fractions: define $L^\\star = +1$ if $p_A > p_B$, $L^\\star = -1$ if $p_B > p_A$, and $L^\\star = 0$ if $p_A = p_B$. An algorithm’s decision on $g^\\star$ is deemed correct as follows: if $L^\\star \\in \\{+1, -1\\}$, the algorithm is correct if and only if it calls a hit and its predicted direction equals $L^\\star$; if $L^\\star = 0$, the algorithm is correct if and only if it does not call a hit.\n\nFor each test case, compute two correctness indicators $C_M$ and $C_B$ (each either $0$ or $1$) for Algorithms M and B on the special gene $g^\\star$. Map them to a single integer result $R = 2 \\cdot C_M + C_B$, which takes values in $\\{0,1,2,3\\}$, corresponding to neither correct, only Algorithm B correct, only Algorithm M correct, or both correct, respectively.\n\nTest suite: For each case, you are given a tuple $(\\mathrm{seed}, \\mu, p_A, p_B, p_S)$ with $p_A + p_B + p_S = 1$. Use the following four test cases:\n1. $(42, 0.8, 0.8, 0.2, 0.0)$,\n2. $(43, 0.8, 0.25, 0.75, 0.0)$,\n3. $(44, 0.8, 0.5, 0.5, 0.0)$,\n4. $(45, 1.0, 0.1, 0.1, 0.8)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the integer $R$ for the corresponding test case, in the same order as listed above. No other text should be printed. No physical units or angles are involved, and any fractions must be represented as decimals in the input parameters. All computations must be performed as specified and be reproducible given the seeds.", "solution": "The problem requires the construction and execution of a detailed simulation for a Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-based pooled perturbation screen. The objective is to assess the performance of two distinct hit-calling algorithms, designated Algorithm M and Algorithm B, in a specific scenario. This scenario involves a single gene, $g^\\star$, which possesses two isoforms with antagonistic biological effects (enrichment and depletion). The problem is determined to be valid as it is scientifically grounded in the principles of computational biology and bioinformatics, is mathematically and algorithmically well-posed, and provides a complete and unambiguous specification for the simulation.\n\nThe simulation process is structured as follows. We begin by defining the parameters of the screen and the statistical models for data generation. We then describe the implementation of the two analytical algorithms. Finally, we establish the criteria for evaluating the correctness of each algorithm's classification of the special gene $g^\\star$.\n\n**1. Simulation Framework**\n\nThe simulated screen comprises $G = 200$ genes, each targeted by $K = 6$ guide RNAs (gRNAs). A specific gene, with index $g^\\star = 0$, is modeled to have two isoforms with opposing effects. For each guide $i$ in gene $j$, we simulate read counts from both a pre-selection library ($T_0$) and a post-selection library ($T_1$). These counts, denoted $X_{0,ji}$ and $X_{1,ji}$, are generated from a Negative Binomial distribution, $\\mathrm{NB}(r, p)$, which is a standard choice for modeling overdispersed count data from high-throughput sequencing. The distribution is parameterized by its dispersion (size) parameter $r = 50$ and a probability parameter $p$. For a desired mean count $\\mu$, the probability is set to $p = \\frac{r}{r+\\mu}$. The baseline mean count for a guide in the pre-selection library is $\\mu_0 = 500$.\n\nThe true biological effect of a guide is represented by its log fold change, $\\ell_{ji}$. For all guides targeting non-special genes ($j \\ne g^\\star$), the effect is null, i.e., $\\ell_{ji} = 0$. For the special gene $g^\\star$, the guides are partitioned into three categories:\n- $K_A$ guides targeting Isoform-A, with a true effect of $+\\mu$ (enrichment).\n- $K_B$ guides targeting Isoform-B, with a true effect of $-\\mu$ (depletion).\n- $K_S$ guides targeting a shared region, with a true effect of $0$.\n\nThe counts $K_A$, $K_B$, and $K_S$ are determined from specified fractions $(p_A, p_B, p_S)$ such that $p_A + p_B + p_S = 1$. The integer counts are calculated deterministically by first taking the floor of $p \\cdot K$ for each category, and then distributing the remaining $R = K - (\\lfloor p_A K \\rfloor + \\lfloor p_B K \\rfloor + \\lfloor p_S K \\rfloor)$ guides one by one to the categories with the largest fractional parts, breaking ties in the order A, B, S.\n\nWith the true effects $\\ell_{ji}$ defined, the mean of the post-selection counts is $\\mu_0 \\exp(\\ell_{ji})$. Thus, the count generation is as follows:\n$$\nX_{0,ji} \\sim \\mathrm{NB}\\left(r, \\frac{r}{r+\\mu_0}\\right)\n$$\n$$\nX_{1,ji} \\sim \\mathrm{NB}\\left(r, \\frac{r}{r+\\mu_0 \\exp(\\ell_{ji})}\\right)\n$$\nAll random draws are performed deterministically using a per-case seed for the random number generator. From these counts, the observed guide-level log fold change $L_{ji}$ is calculated, incorporating a pseudocount of $1$ to handle zero counts:\n$$\nL_{ji} = \\log\\left(\\frac{X_{1,ji} + 1}{X_{0,ji} + 1}\\right)\n$$\nwhere $\\log$ denotes the natural logarithm.\n\nA robust estimate of the standard deviation of the null log fold changes, $\\hat{\\sigma}$, is computed from the full set of observed $L_{ji}$ values using the median absolute deviation (MAD): $\\hat{\\sigma} = 1.4826 \\cdot \\mathrm{median}(|L_{ji} - \\mathrm{median}(\\{L\\cdot\\})|)$. Specified fallbacks are used if $\\hat{\\sigma}$ is zero.\n\n**2. Hit-Calling Algorithms**\n\nTwo algorithms are applied to the data to call hits.\n\n**Algorithm M (Modified Rank-Minimum P-value):**\nThis algorithm is based on identifying the single most extreme guide effect for each gene and applying a Bonferroni correction.\n1. For each guide, a standardized Z-score is computed: $Z_{ji} = L_{ji} / \\hat{\\sigma}$.\n2. One-sided p-values for depletion and enrichment are calculated using the standard normal cumulative distribution function $\\Phi(\\cdot)$:\n   $$ p^-_{ji} = \\Phi(Z_{ji}) \\quad \\text{and} \\quad p^+_{ji} = 1 - \\Phi(Z_{ji}) $$\n3. Gene-level p-values are obtained by taking the minimum guide-level p-value for each direction and applying a Bonferroni correction for the $K$ guides:\n   $$ P^-_j = \\min\\left(1, K \\cdot \\min_i p^-_{ji}\\right) \\quad \\text{and} \\quad P^+_j = \\min\\left(1, K \\cdot \\min_i p^+_{ji}\\right) $$\n4. The final gene p-value is $P_j = \\min(P^-_j, P^+_j)$. The predicted direction, $\\mathrm{dir}_M(j)$, is $+1$ if $P^+_j  P^-_j$ and $-1$ otherwise (with a tie-break rule of $+1$).\n5. A gene is called a hit if $P_j \\le \\alpha$, where the significance level is $\\alpha = 0.05$.\n\n**Algorithm B (Bayesian Factor-Based):**\nThis algorithm uses a Bayesian framework to compare a null hypothesis against two alternative hypotheses of fixed effect sizes.\n1. The observed log fold changes $L_{ji}$ for a non-hit gene are assumed to follow a null distribution $\\mathcal{N}(0, \\hat{\\sigma}^2)$.\n2. Two alternative distributions are defined: $\\mathcal{N}(-\\beta, \\hat{\\sigma}^2)$ for depletion and $\\mathcal{N}(+\\beta, \\hat{\\sigma}^2)$ for enrichment, with a fixed effect magnitude $\\beta = 0.6$.\n3. For each gene $j$, log Bayes factors are computed to compare the alternatives against the null. The log-likelihood ratio for a single guide $L_{ji}$ under the positive alternative versus the null simplifies to $\\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2}$. Summing over all guides for gene $j$:\n   $$ \\log \\mathrm{BF}^+(j) = \\sum_{i=1}^K \\left( \\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2} \\right) = \\frac{\\beta}{\\hat{\\sigma}^2} \\sum_{i=1}^K L_{ji} - \\frac{K\\beta^2}{2\\hat{\\sigma}^2} $$\n   Similarly, for the negative alternative:\n   $$ \\log \\mathrm{BF}^-(j) = \\sum_{i=1}^K \\left( -\\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2} \\right) = -\\frac{\\beta}{\\hat{\\sigma}^2} \\sum_{i=1}^K L_{ji} - \\frac{K\\beta^2}{2\\hat{\\sigma}^2} $$\n4. The maximal log Bayes factor is $\\log \\mathrm{BF}_{\\max}(j) = \\max(\\log \\mathrm{BF}^-(j), \\log \\mathrm{BF}^+(j))$. The predicted direction, $\\mathrm{dir}_B(j)$, is $+1$ if $\\log \\mathrm{BF}^+(j)  \\log \\mathrm{BF}^-(j)$ and $-1$ otherwise (with a tie-break rule of $+1$).\n5. A gene is called a hit if the evidence is sufficiently strong, i.e., $\\log \\mathrm{BF}_{\\max}(j) \\ge \\log T$, where the Bayes factor threshold is $T = 10$.\n\n**3. Evaluation and Output**\n\nThe performance of each algorithm is assessed only on the special gene $g^\\star$. The ground-truth direction $L^\\star$ for this gene is defined by the input design: $L^\\star = +1$ if $p_A  p_B$, $L^\\star = -1$ if $p_B  p_A$, and $L^\\star = 0$ if $p_A = p_B$.\n- If $L^\\star \\in \\{+1, -1\\}$, an algorithm is correct if and only if it calls $g^\\star$ as a hit and its predicted direction matches $L^\\star$.\n- If $L^\\star = 0$, an algorithm is correct if and only if it does not call $g^\\star$ as a hit.\n\nFor each test case, we compute binary correctness indicators $C_M$ and $C_B$ for Algorithm M and B, respectively. These are combined into a single integer result $R = 2 \\cdot C_M + C_B$, which uniquely encodes the outcome for both algorithms. The program will execute the simulation for each provided test case and output the list of these integer results.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import nbinom, norm\n\ndef solve():\n    \"\"\"\n    Runs the full CRISPR screen simulation and analysis for a suite of test cases.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    G = 200  # Total number of genes\n    K = 6  # Guides per gene\n    g_star_idx = 0  # Special gene index\n    mu_0 = 500.0  # Baseline pre-selection mean count\n    r = 50.0  # Common Negative Binomial dispersion (size)\n    alpha = 0.05  # Significance level for Algorithm M\n    T = 10.0  # Bayes factor threshold for Algorithm B\n    beta = 0.6  # Alternative effect size for Algorithm B\n\n    # --- Test Cases (seed, mu, p_A, p_B, p_S) ---\n    test_cases = [\n        (42, 0.8, 0.8, 0.2, 0.0),\n        (43, 0.8, 0.25, 0.75, 0.0),\n        (44, 0.8, 0.5, 0.5, 0.0),\n        (45, 1.0, 0.1, 0.1, 0.8),\n    ]\n\n    results = []\n\n    for seed, mu_effect, p_A, p_B, p_S in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # --- 1. Determine Guide Counts for Special Gene ---\n        fractions = {'A': p_A, 'B': p_B, 'S': p_S}\n        base_counts = {cat: int(frac * K) for cat, frac in fractions.items()}\n        remainders = {cat: frac * K - base_counts[cat] for cat, frac in fractions.items()}\n        \n        # Sort categories by remainder descending, with tie-break A, B, S\n        sorted_cats = sorted(fractions.keys(), key=lambda c: (-remainders[c], ['A', 'B', 'S'].index(c)))\n        \n        R = K - sum(base_counts.values())\n        final_counts = base_counts.copy()\n        for i in range(R):\n            final_counts[sorted_cats[i]] += 1\n        \n        K_A, K_B, K_S = final_counts['A'], final_counts['B'], final_counts['S']\n\n        # --- 2. Generate True Log Fold Changes (l_ji) ---\n        l_ji = np.zeros((G, K))\n        effects = np.concatenate([\n            np.full(K_A, mu_effect),\n            np.full(K_B, -mu_effect),\n            np.full(K_S, 0.0)\n        ])\n        l_ji[g_star_idx, :] = effects\n\n        # --- 3. Generate Counts and Observed LFCs ---\n        p0 = r / (r + mu_0)\n        X0 = nbinom.rvs(n=r, p=p0, size=(G, K), random_state=rng)\n        \n        mu1 = mu_0 * np.exp(l_ji)\n        p1 = r / (r + mu1)\n        X1 = nbinom.rvs(n=r, p=p1, size=(G, K), random_state=rng)\n\n        L_ji = np.log((X1 + 1) / (X0 + 1))\n\n        # --- 4. Estimate Robust Scale (sigma_hat) ---\n        median_lfc = np.median(L_ji)\n        mad = np.median(np.abs(L_ji - median_lfc))\n        sigma_hat = 1.4826 * mad\n        if sigma_hat == 0.0:\n            sigma_hat = np.std(L_ji)\n            if sigma_hat == 0.0:\n                sigma_hat = 1e-8\n\n        # --- 5. Run Algorithms on Special Gene g* ---\n        L_star = L_ji[g_star_idx, :]\n        \n        # --- Algorithm M ---\n        Z_star = L_star / sigma_hat\n        p_minus_guides = norm.cdf(Z_star)\n        p_plus_guides = 1.0 - p_minus_guides\n        \n        P_minus_star = min(1.0, K * np.min(p_minus_guides))\n        P_plus_star = min(1.0, K * np.min(p_plus_guides))\n        \n        P_star_M = min(P_minus_star, P_plus_star)\n        \n        # Tie-break: dir=+1 if P+ = P-\n        dir_M = 1 if P_plus_star = P_minus_star else -1\n        hit_M = P_star_M = alpha\n\n        # --- Algorithm B ---\n        sum_L_star = np.sum(L_star)\n        log_BF_plus = (beta / sigma_hat**2) * sum_L_star - (K * beta**2) / (2 * sigma_hat**2)\n        log_BF_minus = (-beta / sigma_hat**2) * sum_L_star - (K * beta**2) / (2 * sigma_hat**2)\n        \n        log_BF_max = max(log_BF_plus, log_BF_minus)\n\n        # Tie-break: dir=+1 if BF+  BF- (or equal)\n        dir_B = 1 if log_BF_plus = log_BF_minus else -1\n        hit_B = log_BF_max = np.log(T)\n\n        # --- 6. Evaluate Correctness ---\n        if p_A  p_B:\n            L_star_truth = 1\n        elif p_B  p_A:\n            L_star_truth = -1\n        else:\n            L_star_truth = 0\n\n        C_M = 0\n        if L_star_truth in [1, -1]:\n            if hit_M and dir_M == L_star_truth:\n                C_M = 1\n        elif L_star_truth == 0:\n            if not hit_M:\n                C_M = 1\n\n        C_B = 0\n        if L_star_truth in [1, -1]:\n            if hit_B and dir_B == L_star_truth:\n                C_B = 1\n        elif L_star_truth == 0:\n            if not hit_B:\n                C_B = 1\n        \n        # --- 7. Compute Final Result ---\n        R = 2 * C_M + C_B\n        results.append(R)\n\n    # --- Print Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2372047"}]}