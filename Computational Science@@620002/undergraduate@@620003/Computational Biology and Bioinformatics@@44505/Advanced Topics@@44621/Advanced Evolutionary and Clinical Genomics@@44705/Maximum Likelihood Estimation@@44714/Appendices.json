{"hands_on_practices": [{"introduction": "This first exercise serves as a fundamental building block for understanding Maximum Likelihood Estimation (MLE). By working through the classic example of a Binomial distribution, you will practice the core mechanics: writing the likelihood function, taking its logarithm, and finding the maximum by differentiation. Mastering this process [@problem_id:1933626] is essential, as it forms the basis for nearly all MLE applications in biology and beyond.", "problem": "A quality control engineer is monitoring the production of semiconductor wafers. Each wafer contains a large, fixed number of individual electronic components, denoted by $k$. The manufacturing process is such that each component has a constant, but unknown, probability $p$ of being defective, independently of all other components. The number of defective components on a single wafer can thus be modeled by a binomial distribution with parameters $k$ (number of trials) and $p$ (probability of success, i.e., being defective).\n\nTo estimate $p$, the engineer inspects a random sample of $n$ wafers and records the number of defective components on each one. Let the observed counts of defective components for this sample be $x_1, x_2, \\ldots, x_n$.\n\nDerive the maximum likelihood estimator, $\\hat{p}$, for the defect probability $p$. Express your answer as a single symbolic expression in terms of $k$, $n$, and the sample observations $x_1, x_2, \\ldots, x_n$.", "solution": "Let $X_{i}$ denote the number of defective components on wafer $i$, modeled as $X_{i} \\sim \\text{Binomial}(k, p)$ independently for $i=1,2,\\ldots,n$. The joint likelihood for observations $x_{1},\\ldots,x_{n}$ is\n$$\nL(p)=\\prod_{i=1}^{n}\\binom{k}{x_{i}} p^{x_{i}} (1-p)^{k-x_{i}}, \\quad 0 \\leq p \\leq 1.\n$$\nThe log-likelihood is\n$$\n\\ell(p)=\\sum_{i=1}^{n}\\ln\\binom{k}{x_{i}}+\\left(\\sum_{i=1}^{n}x_{i}\\right)\\ln p+\\left(nk-\\sum_{i=1}^{n}x_{i}\\right)\\ln(1-p).\n$$\nDifferentiate with respect to $p$ and set to zero:\n$$\n\\ell'(p)=\\frac{\\sum_{i=1}^{n}x_{i}}{p}-\\frac{nk-\\sum_{i=1}^{n}x_{i}}{1-p}=0.\n$$\nSolving for $p$ gives\n$$\n\\left(\\sum_{i=1}^{n}x_{i}\\right)(1-p)=p\\left(nk-\\sum_{i=1}^{n}x_{i}\\right)\\;\\;\\Longrightarrow\\;\\;\\sum_{i=1}^{n}x_{i}=pnk,\n$$\nhence\n$$\n\\hat{p}=\\frac{\\sum_{i=1}^{n}x_{i}}{nk}.\n$$\nThe second derivative,\n$$\n\\ell''(p)=-\\frac{\\sum_{i=1}^{n}x_{i}}{p^{2}}-\\frac{nk-\\sum_{i=1}^{n}x_{i}}{(1-p)^{2}},\n$$\nis strictly negative for $p \\in (0,1)$, so this critical point is a maximum. Since $0 \\leq \\sum_{i=1}^{n}x_{i} \\leq nk$, the estimator lies in $[0,1]$, with boundary cases $\\hat{p}=0$ when $\\sum x_{i}=0$ and $\\hat{p}=1$ when $\\sum x_{i}=nk$.", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} x_{i}}{n k}}$$", "id": "1933626"}, {"introduction": "Biological and experimental data are rarely perfect, and this problem introduces a crucial real-world challenge: data truncation. Here, observations are only recorded if they meet a certain criterion, a common scenario when instruments have detection limits. By modeling flaw counts with a zero-truncated Poisson distribution, you will learn how to correctly formulate the likelihood function when the data generation process itself filters the outcomes [@problem_id:1933615], a vital skill for robust data analysis.", "problem": "A materials science company is developing high-purity optical fibers. The number of microscopic flaws in a one-meter segment of fiber is modeled by a Poisson distribution with an unknown rate parameter $\\lambda$. For quality control, an automated laser scanner inspects each segment. However, the scanner's software is configured such that it only records data for a fiber segment if it detects at least one flaw. Segments with zero flaws are passed without being entered into the quality control database.\n\nA data scientist is tasked with estimating the parameter $\\lambda$ using the available data. A random sample of recorded fiber segments is analyzed, and the sample mean of the flaw counts is found to be exactly 2.5.\n\nBased on this information, determine the Maximum Likelihood Estimate (MLE) for the parameter $\\lambda$. Choose the best answer from the options below.\n\nA. 2.05\n\nB. 2.22\n\nC. 2.50\n\nD. 2.81\n\nE. 3.00", "solution": "Let $Y \\sim \\text{Poisson}(\\lambda)$ denote the flaw count in a segment, but only observations with $Y \\geq 1$ are recorded. The observed data therefore follow the zero-truncated Poisson distribution:\n$$\n\\Pr(Y=k \\mid Y \\geq 1) = \\frac{\\exp(-\\lambda)\\lambda^{k}}{k!\\left(1 - \\exp(-\\lambda)\\right)}, \\quad k=1,2,\\dots\n$$\nGiven a sample $y_{1},\\dots,y_{n}$ from this distribution with sample mean $\\bar{y}=2.5$, the log-likelihood is\n$$\n\\ell(\\lambda) = \\sum_{i=1}^{n}\\left[-\\lambda + y_{i}\\ln \\lambda - \\ln(y_{i}!) - \\ln\\left(1 - \\exp(-\\lambda)\\right)\\right].\n$$\nDifferentiate and set to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda} = -n + \\frac{\\sum_{i=1}^{n} y_{i}}{\\lambda} - n\\,\\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} = 0.\n$$\nLet $m = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$. Dividing by $n$ gives\n$$\n-1 + \\frac{m}{\\lambda} - \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} = 0,\n$$\nhence\n$$\n\\frac{m}{\\lambda} = 1 + \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} = \\frac{1}{1 - \\exp(-\\lambda)}.\n$$\nTherefore the MLE $\\hat{\\lambda}$ satisfies\n$$\nm = \\frac{\\lambda}{1 - \\exp(-\\lambda)}.\n$$\nEquivalently,\n$$\nm\\left(1 - \\exp(-\\lambda)\\right) = \\lambda \\quad \\Longrightarrow \\quad (m - \\lambda)\\exp(\\lambda) = m.\n$$\nLet $x = m - \\lambda$. Then $x\\exp(m - x) = m$, so $x\\exp(-x) = m\\exp(-m)$, which yields\n$$\nx = - W\\!\\left(-m\\exp(-m)\\right), \\quad \\text{so} \\quad \\hat{\\lambda} = m + W\\!\\left(-m\\exp(-m)\\right),\n$$\nwhere $W$ is the Lambert $W$ function. With $m=2.5$,\n$$\n- m\\exp(-m) = -2.5\\exp(-2.5) \\approx -0.2052,\n$$\nso $W(-0.2052) \\approx -0.27$ (since $y\\exp(y)$ at $y=-0.27$ is approximately $-0.206$), giving\n$$\n\\hat{\\lambda} \\approx 2.5 - 0.27 = 2.23.\n$$\nVerification in the estimating equation $m = \\lambda/(1 - \\exp(-\\lambda))$:\n- For $\\lambda = 2.22$, $\\exp(-\\lambda) \\approx 0.1086$, so $m \\approx 2.22/0.8914 \\approx 2.492$.\n- For $\\lambda = 2.23$, $\\exp(-\\lambda) \\approx 0.1075$, so $m \\approx 2.23/0.8925 \\approx 2.500$.\n\nThus the MLE is approximately $2.23$, and the closest option is $2.22$, which is choice B.", "answer": "$$\\boxed{B}$$", "id": "1933615"}, {"introduction": "We now apply our skills to a core challenge in modern computational biology: quantifying errors in DNA synthesis and sequencing. This practice extends the MLE framework to a multi-parameter scenario, where you must simultaneously estimate the probabilities of substitution and deletion errors, $\\alpha$ and $\\beta$. This exercise [@problem_id:2402385] directly demonstrates how MLE provides the statistical foundation for assessing the fidelity of fundamental biotechnologies.", "problem": "A biotechnology laboratory is validating a new Deoxyribonucleic Acid (DNA) synthesis machine by synthesizing a pool of molecules corresponding to a known template sequence. The template has length $L$ nucleotides. The machine synthesizes $N$ molecules independently. At each template position, the machine independently emits one of three outcomes for each molecule: a correct base (match), a wrong base (substitution), or no base (deletion). Insertions do not occur in this experiment. Let the substitution probability be $\\alpha$, the deletion probability be $\\beta$, and the match probability be $1-\\alpha-\\beta$, with constraints $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta \\le 1$. After sequencing and aligning the synthesized molecules to the template (alignment is unambiguous because insertions are absent), the laboratory aggregates counts across all $N$ molecules and all $L$ positions: the total number of substitutions $S$, the total number of deletions $D$, and the total number of matches $M$, where $M = N L - S - D$. Under the independence assumptions stated, treat each position in each molecule as an independent categorical trial with three outcomes and associated probabilities $(1-\\alpha-\\beta, \\alpha, \\beta)$.\n\nUsing Maximum Likelihood Estimation (MLE), write a program that, for each specified test case, computes the MLEs $\\hat{\\alpha}$ and $\\hat{\\beta}$ from the provided $(N,L,S,D)$ values. Your program must produce results rounded to $6$ decimal places as decimals (not percentages).\n\nTest suite:\n- Case $1$: $N=1000$, $L=50$, $S=120$, $D=30$.\n- Case $2$: $N=200$, $L=100$, $S=0$, $D=0$.\n- Case $3$: $N=5$, $L=2$, $S=0$, $D=10$.\n- Case $4$: $N=3$, $L=4$, $S=2$, $D=1$.\n- Case $5$: $N=50$, $L=120$, $S=600$, $D=0$.\n\nFinal output format:\n- For the above cases in order, output a single line containing a comma-separated list enclosed in square brackets with the sequence $[\\hat{\\alpha}_1,\\hat{\\beta}_1,\\hat{\\alpha}_2,\\hat{\\beta}_2,\\hat{\\alpha}_3,\\hat{\\beta}_3,\\hat{\\alpha}_4,\\hat{\\beta}_4,\\hat{\\alpha}_5,\\hat{\\beta}_5]$, where subscripts index the case number. For example, the required format is $[r_1,r_2,\\dots,r_{10}]$ with each $r_i$ rounded to $6$ decimal places.", "solution": "The experiment consists of a total of $T = N \\times L$ independent trials. Each trial results in one of three outcomes: a substitution (with probability $\\alpha$), a deletion (with probability $\\beta$), or a match (with probability $1-\\alpha-\\beta$). The observed data are the total counts for these outcomes: $S$ substitutions, $D$ deletions, and $M = T - S - D$ matches.\n\nThe joint probability of observing these counts, given the parameters $\\alpha$ and $\\beta$, is described by the multinomial probability mass function. The likelihood function $L(\\alpha, \\beta)$ is proportional to this mass function:\n$$L(\\alpha, \\beta) \\propto \\alpha^S \\cdot \\beta^D \\cdot (1 - \\alpha - \\beta)^M$$\nTo simplify the maximization, we work with the log-likelihood function, $\\ell(\\alpha, \\beta) = \\log L(\\alpha, \\beta)$:\n$$\\ell(\\alpha, \\beta) = \\text{const} + S \\log(\\alpha) + D \\log(\\beta) + M \\log(1 - \\alpha - \\beta)$$\nTo find the values of $\\alpha$ and $\\beta$ that maximize $\\ell$, we compute the partial derivatives with respect to $\\alpha$ and $\\beta$ and set them to zero.\n$$\\frac{\\partial \\ell}{\\partial \\alpha} = \\frac{S}{\\alpha} - \\frac{M}{1 - \\alpha - \\beta} = 0$$\n$$\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{D}{\\beta} - \\frac{M}{1 - \\alpha - \\beta} = 0$$\nFrom these equations, we can see that $\\frac{S}{\\alpha} = \\frac{D}{\\beta}$. Let's solve the first equation for $\\alpha$.\n$$S(1 - \\alpha - \\beta) = M\\alpha$$\n$$S = S\\alpha + S\\beta + M\\alpha$$\nFrom $\\frac{S}{\\alpha} = \\frac{D}{\\beta}$, we have $\\beta = \\frac{D\\alpha}{S}$. Substituting this into the equation:\n$$S = S\\alpha + S\\left(\\frac{D\\alpha}{S}\\right) + M\\alpha = S\\alpha + D\\alpha + M\\alpha = (S+D+M)\\alpha$$\nSince the total number of trials is $T = N \\times L = S + D + M$, we have:\n$$S = T\\alpha$$\nThis gives the MLE for $\\alpha$:\n$$\\hat{\\alpha} = \\frac{S}{T} = \\frac{S}{NL}$$\nBy symmetry, solving for $\\beta$ yields:\n$$\\hat{\\beta} = \\frac{D}{T} = \\frac{D}{NL}$$\nThese results are the observed frequencies of each outcome, which is the well-known MLE for the parameters of a categorical/multinomial distribution. These estimators hold even when some counts are zero.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Maximum Likelihood Estimates (MLEs) for substitution and\n    deletion probabilities based on aggregated counts from a DNA synthesis experiment.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, L, S, D)\n    test_cases = [\n        (1000, 50, 120, 30),  # Case 1\n        (200, 100, 0, 0),    # Case 2\n        (5, 2, 0, 10),       # Case 3\n        (3, 4, 2, 1),        # Case 4\n        (50, 120, 600, 0)    # Case 5\n    ]\n\n    results = []\n    for N, L, S, D in test_cases:\n        # The total number of trials is the number of molecules (N)\n        # multiplied by the template length (L).\n        total_trials = N * L\n\n        # The Maximum Likelihood Estimate for the probability of an event in a\n        # categorical distribution is its observed frequency.\n        # This holds true even for edge cases where counts are zero or sum\n        # to the total number of trials.\n        if total_trials > 0:\n            alpha_hat = S / total_trials\n            beta_hat = D / total_trials\n        else:\n            # This case is not in the test suite but is handled for robustness.\n            # If there are no trials, the probabilities are undefined.\n            # We define them as 0.0 by convention.\n            alpha_hat = 0.0\n            beta_hat = 0.0\n        \n        # Append results formatted to 6 decimal places as required.\n        # Using f-string formatting ensures the correct number of decimal places,\n        # e.g., 0.1 becomes 0.100000.\n        results.append(f'{alpha_hat:.6f}')\n        results.append(f'{beta_hat:.6f}')\n\n    # Final print statement in the exact required format.\n    # The results list already contains formatted strings.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2402385"}]}