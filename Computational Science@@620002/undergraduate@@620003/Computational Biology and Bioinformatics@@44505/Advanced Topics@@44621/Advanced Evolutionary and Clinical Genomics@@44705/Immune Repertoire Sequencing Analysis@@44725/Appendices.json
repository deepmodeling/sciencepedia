{"hands_on_practices": [{"introduction": "Before embarking on a costly sequencing experiment, a crucial first step is to estimate the required sampling depth. This practice introduces a foundational concept in experimental design: determining the minimum number of samples needed to detect a specific clonotype of interest with a desired level of confidence. By applying basic principles of probability, you will develop the skill to make informed decisions about experimental scale, ensuring your study is both cost-effective and statistically robust [@problem_id:2399355].", "problem": "In high-throughput T cell receptor (TCR) repertoire sequencing, each clonotype (defined here by a unique complementarity-determining region 3 (CDR3) nucleotide sequence) has an underlying population frequency. Consider an individual whose most frequent clonotype has true relative frequency $0.01$ in the circulating T cell population at the time of sampling, as established by an earlier deep sequencing study. You plan a new experiment that samples T cells at random from this individual and sequences their TCRs, retaining one sequence per successfully captured distinct T cell so that each observation corresponds to one cell. Assume the repertoire is sufficiently large that sampling can be modeled as independent draws with replacement, that per-cell capture and sequencing is unbiased with respect to clonotype, and that technical artifacts have been removed so that observed sequences reflect biological cells.\n\nDetermine the minimum number $n$ of unique TCR sequences (i.e., distinct cells) you must obtain in this experiment so that the probability is at least $0.95$ that you have sampled the most frequent clonotype at least once. Report the smallest integer $n$ that satisfies this requirement. No units are required, and no rounding beyond reporting this minimum integer is needed.", "solution": "The problem statement is first subjected to validation.\n\nStep 1: Extract Givens\n- The true relative frequency of the most frequent clonotype is $0.01$.\n- The experiment involves sampling $n$ unique TCR sequences (distinct cells).\n- Sampling is modeled as independent draws with replacement.\n- The probability of sampling the most frequent clonotype at least once must be at least $0.95$.\n- The objective is to find the minimum integer $n$ that satisfies this condition.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on valid principles of statistics and their application to computational biology, specifically immune repertoire sequencing. The concept of clonotype frequency and stochastic sampling is fundamental to the field. The given frequency of $0.01$ is biologically plausible for a dominant clonotype. The model of sampling with replacement is a standard and appropriate approximation for a large population like the T-cell repertoire.\n- **Well-Posed:** The problem is well-posed. It provides all necessary parameters to construct a mathematical inequality and solve for a unique minimum integer value. The question is unambiguous.\n- **Objective:** The problem is stated using precise, objective, and scientific terminology.\n\nThe problem is deemed valid as it is scientifically sound, well-posed, and objective. It contains no logical contradictions, missing information, or pseudoscientific claims. We may proceed with the solution.\n\nThe problem describes a series of Bernoulli trials. Let $n$ be the total number of unique TCR sequences sampled, which corresponds to the number of independent trials. Let $p$ be the true frequency of the specific clonotype of interest. From the problem statement, we are given:\n$$p = 0.01$$\nEach trial consists of sampling one T cell. A \"success\" is defined as observing the clonotype of interest. The probability of success in any single trial is $p$. A \"failure\" is observing any other clonotype, and the probability of failure in any single trial is $1 - p$.\n\nThe problem requires that the probability of observing the clonotype *at least once* in $n$ trials is at least $0.95$. Let $A$ be the event that the clonotype is observed at least once in $n$ samples. It is more straightforward to first calculate the probability of the complementary event, $A^c$, which is the event that the clonotype is *not* observed in any of the $n$ samples.\n\nSince the samples are independent draws, the probability of not observing the clonotype in one trial is $(1 - p)$. The probability of not observing it in $n$ consecutive independent trials is:\n$$P(A^c) = (1 - p)^n$$\nThe probability of event $A$ is therefore:\n$$P(A) = 1 - P(A^c) = 1 - (1 - p)^n$$\nThe condition specified in the problem is $P(A) \\ge 0.95$. We express this as an inequality:\n$$1 - (1 - p)^n \\ge 0.95$$\nWe must now solve for the smallest integer $n$ that satisfies this inequality. First, we rearrange the terms to isolate the expression containing $n$:\n$$(1 - p)^n \\le 1 - 0.95$$\n$$(1 - p)^n \\le 0.05$$\nNow, we substitute the given value of $p = 0.01$:\n$$(1 - 0.01)^n \\le 0.05$$\n$$(0.99)^n \\le 0.05$$\nTo solve for the exponent $n$, we take the natural logarithm of both sides of the inequality. Since the natural logarithm function, $\\ln(x)$, is a strictly increasing function for $x > 0$, the direction of the inequality is preserved.\n$$\\ln((0.99)^n) \\le \\ln(0.05)$$\nUsing the logarithmic property $\\ln(a^b) = b \\ln(a)$, we have:\n$$n \\ln(0.99) \\le \\ln(0.05)$$\nTo isolate $n$, we must divide by $\\ln(0.99)$. It is critical to recognize that for any value $x$ in the interval $(0, 1)$, its natural logarithm $\\ln(x)$ is negative. Therefore, $\\ln(0.99) < 0$. When dividing both sides of an inequality by a negative number, the inequality sign must be reversed.\n$$n \\ge \\frac{\\ln(0.05)}{\\ln(0.99)}$$\nWe can now evaluate the right-hand side of the inequality.\n$$n \\ge \\frac{-2.995732...}{-0.010050...}$$\n$$n \\ge 298.0728...$$\nThe problem requires the minimum integer value of $n$. Since $n$ must be an integer representing a count of cells, and it must be greater than or equal to $298.0728...$, the smallest integer that satisfies this condition is obtained by taking the ceiling of this value.\n$$n_{min} = \\lceil 298.0728... \\rceil = 299$$\nTherefore, a minimum of $299$ unique TCR sequences must be sampled to ensure the probability of observing the specified clonotype at least once is at least $0.95$.", "answer": "$$\\boxed{299}$$", "id": "2399355"}, {"introduction": "Once you have sequencing data, how do you determine if you have sampled the immune repertoire exhaustively enough to capture its true diversity? This exercise walks you through the implementation of rarefaction analysis, a cornerstone technique in ecology and bioinformatics used to address this very question. You will learn to calculate the expected number of unique clonotypes $E[S(n)]$ as a function of sequencing depth $n$, allowing you to assess whether a sample has been sequenced to saturation or if further investment is warranted [@problem_id:2399358].", "problem": "A laboratory has sequenced the immune receptor repertoire of a lymphocyte population and obtained a list of observed clonotypes with their read counts. Model the sequenced reads as draws without replacement from a finite multiset of size $N$, where $N$ is the total number of reads observed and each clonotype $i$ appears $c_i$ times, with $c_i \\ge 1$ and $\\sum_i c_i = N$. Define the rarefaction function as the expected number of distinct clonotypes observed when subsampling $n$ reads without replacement, denoted by $E[S(n)]$.\n\nYour task is to write a program that, for each specified test case, decides whether additional sequencing is advisable based on the slope of the rarefaction curve near the current depth. Use the following scientifically grounded base:\n\n- Sampling without replacement from a finite population follows the hypergeometric law.\n- For each clonotype $i$ with count $c_i$, the probability that none of its $c_i$ molecules are observed in a subsample of size $n$ is the ratio of binomial coefficients $\\dfrac{\\binom{N - c_i}{n}}{\\binom{N}{n}}$ when $0 \\le n \\le N$.\n- By linearity of expectation, $E[S(n)]$ equals the sum, over clonotypes, of the probability that each clonotype is observed at least once in the subsample.\n\nDecision rule: Given a positive integer window size $m$ with $1 \\le m \\le N$ and a nonnegative threshold $\\alpha$ (measured in expected new clonotypes per additional read), compute the average marginal gain in expected clonal richness over the last $m$ reads,\n$$\ng = \\frac{E[S(N)] - E[S(N - m)]}{m}.\n$$\nIf $g > \\alpha$, return the boolean value True (advise sequencing to greater depth); otherwise return False.\n\nCompute $E[S(n)]$ using only the assumptions above and standard mathematical functions. To ensure numerical stability for large $N$, compute ratios of binomial coefficients via logarithms of factorials (for example, using the logarithm of the gamma function).\n\nInput for each test case is:\n- A list of positive integers $[c_1,c_2,\\dots,c_R]$ with $\\sum_i c_i = N$,\n- An integer window size $m$ with $1 \\le m \\le N$,\n- A nonnegative real threshold $\\alpha$.\n\nYour program must process the following test suite:\n\n- Test case $1$: counts $[25,20,15,10,8,7,5,5,3,2]$, $m = 10$, $\\alpha = 0.05$.\n- Test case $2$: counts $[50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]$ (one clonotype with $50$ reads and $50$ singletons), $m = 10$, $\\alpha = 0.15$.\n- Test case $3$: counts consisting of $20$ clonotypes each with $5$ reads, i.e., $[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]$, $m = 10$, $\\alpha = 0.05$.\n- Test case $4$: counts $[100]$, $m = 10$, $\\alpha = 0.001$.\n- Test case $5$: counts consisting of $100$ singletons, i.e., $[1,1,1,\\dots,1]$ (with $100$ entries), $m = 10$, $\\alpha = 0.5$.\n\nYour program should produce a single line of output containing the boolean decisions for the test cases as a comma-separated list enclosed in square brackets (for example, [True,False,...]). No additional text should be printed.", "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is scientifically grounded in established principles of probability theory and their standard application in computational biology, specifically in the analysis of immune repertoire sequencing data. The problem is well-posed, with all terms, conditions, and data provided in a clear, consistent, and objective manner. There are no logical contradictions, scientific inaccuracies, or ambiguities. We may therefore proceed with a formal solution.\n\nThe core task is to evaluate a decision rule for further sequencing based on the local slope of the clonotype rarefaction curve. The decision is `True` if the average marginal gain $g$ exceeds a threshold $\\alpha$, and `False` otherwise. The marginal gain is defined as:\n$$\ng = \\frac{E[S(N)] - E[S(N - m)]}{m}\n$$\nwhere $N$ is the total number of reads, $m$ is a specified window size, and $E[S(n)]$ is the expected number of distinct clonotypes observed in a random subsample of $n$ reads drawn without replacement. The inputs are a list of clonotype counts $[c_1, c_2, \\dots, c_R]$, the window size $m$, and the threshold $\\alpha$.\n\nOur derivation proceeds in two parts: first, the evaluation of $E[S(N)]$, and second, the evaluation of $E[S(N-m)]$.\n\n**1. Evaluation of $E[S(N)]$**\n\nThe term $E[S(N)]$ represents the expected number of distinct clonotypes observed when sampling all $N$ reads from the population. By definition, if one samples all reads, one is guaranteed to observe every clonotype that was present in the original library. Therefore, the number of observed clonotypes is deterministically equal to $R$, the total number of distinct clonotypes.\n$$\nE[S(N)] = R\n$$\nwhere $R$ is simply the length of the input list of counts. This simplifies the expression for $g$:\n$$\ng = \\frac{R - E[S(N - m)]}{m}\n$$\n\n**2. Evaluation of $E[S(n)]$ for a general subsample size $n$**\n\nThe expected number of distinct clonotypes, $E[S(n)]$, is derived from the principle of linearity of expectation. Let $I_i(n)$ be an indicator random variable such that $I_i(n) = 1$ if clonotype $i$ is observed in a subsample of size $n$, and $I_i(n) = 0$ otherwise. The total number of observed clonotypes is $S(n) = \\sum_{i=1}^{R} I_i(n)$. The expectation is:\n$$\nE[S(n)] = E\\left[\\sum_{i=1}^{R} I_i(n)\\right] = \\sum_{i=1}^{R} E[I_i(n)]\n$$\nThe expectation of an indicator variable is the probability of the event it indicates.\n$$\nE[I_i(n)] = P(\\text{clonotype } i \\text{ is observed}) = 1 - P(\\text{clonotype } i \\text{ is not observed})\n$$\nThe problem correctly states that sampling is without replacement, which is governed by the hypergeometric distribution. The probability of not observing clonotype $i$ (which has $c_i$ reads) in a subsample of size $n$ is the probability of choosing all $n$ reads from the $N-c_i$ reads that do not belong to clonotype $i$. The total number of ways to choose $n$ reads from $N$ is $\\binom{N}{n}$. The number of ways to choose $n$ reads from the non-$i$ reads is $\\binom{N-c_i}{n}$. Thus,\n$$\nP(\\text{clonotype } i \\text{ is not observed}) = \\frac{\\binom{N - c_i}{n}}{\\binom{N}{n}}\n$$\nThis is valid for $0 \\le n \\le N-c_i$. If $n > N-c_i$, it is impossible to avoid selecting at least one read from clonotype $i$, so the probability of not observing it is $0$.\n\nCombining these results, we have the formula for the rarefaction curve:\n$$\nE[S(n)] = \\sum_{i=1}^{R} \\left(1 - \\frac{\\binom{N-c_i}{n}}{\\binom{N}{n}}\\right) = R - \\sum_{i=1}^{R} \\frac{\\binom{N-c_i}{n}}{\\binom{N}{n}}\n$$\n\n**3. Numerical Computation and Final Algorithm**\n\nTo compute $g$, we must calculate $E[S(N-m)]$. Let $n' = N-m$.\n$$\nE[S(n')] = R - \\sum_{i=1}^{R} \\frac{\\binom{N-c_i}{n'}}{\\binom{N}{n'}}\n$$\nDirect computation of binomial coefficients with large arguments leads to numerical overflow. As stipulated, we must compute the ratio using logarithms of the Gamma function, $\\log \\Gamma(z+1) = \\log(z!)$. The logarithm of the probability of missing clonotype $i$ is:\n$$\n\\log P_i(\\text{miss}) = \\log\\left(\\frac{\\binom{N-c_i}{n'}}{\\binom{N}{n'}}\\right) = \\log\\left(\\frac{(N-c_i)!(N-n')!}{N!(N-c_i-n')!}\\right)\n$$\nUsing the log-gamma function, this becomes:\n$$\n\\log P_i(\\text{miss}) = \\log\\Gamma(N-c_i+1) + \\log\\Gamma(N-n'+1) - \\log\\Gamma(N+1) - \\log\\Gamma(N-c_i-n'+1)\n$$\nThe probability is then $P_i(\\text{miss}) = \\exp(\\log P_i(\\text{miss}))$. The `scipy.special.gammaln` function correctly handles cases where arguments are non-positive integers by returning infinity, which, after exponentiation, correctly yields a probability of $0$ when $n' > N-c_i$.\n\nThe complete algorithm is as follows:\n1.  Given the list of counts $[c_1, \\dots, c_R]$, $m$, and $\\alpha$.\n2.  Compute the total reads $N = \\sum_{i=1}^{R} c_i$ and the number of distinct clonotypes $R$.\n3.  Set the subsample size for the calculation to $n' = N-m$.\n4.  Initialize a sum for miss probabilities, $\\Sigma_P = 0$.\n5.  For each count $c_i$ in the list:\n    a. If $n' \\le N-c_i$, compute $\\log P_i(\\text{miss})$ using the log-gamma formula above.\n    b. Add $\\exp(\\log P_i(\\text{miss}))$ to $\\Sigma_P$.\n6.  Calculate the expected number of clonotypes at depth $n'$: $E[S(n')] = R - \\Sigma_P$.\n7.  Calculate the marginal gain: $g = (R - E[S(n')]) / m$.\n8.  Return the boolean result of the comparison $g > \\alpha$.\nThis procedure is robust and directly implements the validated scientific model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Solves the immune repertoire sequencing analysis problem for a suite of test cases.\n    \"\"\"\n\n    def compute_decision(counts, m, alpha):\n        \"\"\"\n        Computes the decision to continue sequencing based on the slope of the rarefaction curve.\n\n        Args:\n            counts (list): A list of positive integers representing clonotype read counts.\n            m (int): A positive integer window size.\n            alpha (float): A non-negative real threshold for the marginal gain.\n\n        Returns:\n            bool: True if further sequencing is advisable, False otherwise.\n        \"\"\"\n        # Total number of reads (N) and distinct clonotypes (R)\n        N = sum(counts)\n        R = len(counts)\n\n        # The expected number of clonotypes at full depth E[S(N)] is exactly R.\n        E_S_N = float(R)\n\n        # We need to compute E[S(N-m)], the expected number of clonotypes when\n        # subsampling n_prime = N - m reads.\n        n_prime = N - m\n\n        # E[S(n')] = sum_{i=1 to R} (1 - P(clonotype i is missed))\n        # This is equivalent to R - sum_{i=1 to R} P(clonotype i is missed).\n        # We calculate the sum of miss probabilities.\n        sum_prob_miss = 0.0\n\n        for c_i in counts:\n            # The probability of missing a clonotype is non-zero only if\n            # it's possible to draw n_prime reads without picking any of its c_i members.\n            # This requires the number of other reads (N - c_i) to be at least n_prime.\n            if N - c_i >= n_prime:\n                # To avoid numerical overflow with large factorials, we compute the\n                # log of the ratio of binomial coefficients C(N-c_i, n') / C(N, n').\n                # log(P_miss) = log( (N-c_i)! * (N-n')! / (N! * (N-c_i-n')!) )\n                # This is computed using the log-gamma function, where lgamma(x+1) = log(x!).\n                log_prob_miss = (gammaln(N - c_i + 1) +\n                                 gammaln(N - n_prime + 1) -\n                                 gammaln(N + 1) -\n                                 gammaln(N - c_i - n_prime + 1))\n                \n                sum_prob_miss += np.exp(log_prob_miss)\n\n        # The expected number of clonotypes at depth n_prime\n        E_S_N_minus_m = R - sum_prob_miss\n\n        # The average marginal gain 'g' is the slope of the rarefaction curve\n        # approximated over the last m reads.\n        # The problem statement guarantees 1 = m = N, so m is never zero.\n        g = (E_S_N - E_S_N_minus_m) / m\n\n        return g > alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # counts, m, alpha\n        ([25, 20, 15, 10, 8, 7, 5, 5, 3, 2], 10, 0.05),\n        (([50] + [1] * 50), 10, 0.15),\n        ([5] * 20, 10, 0.05),\n        ([100], 10, 0.001),\n        ([1] * 100, 10, 0.5),\n    ]\n\n    results = []\n    for counts, m, alpha in test_cases:\n        decision = compute_decision(counts, m, alpha)\n        # Format boolean as specified (True, False)\n        results.append(str(decision))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2399358"}, {"introduction": "The ultimate goal of many immune repertoire studies is to identify groups of related T-cell clonotypes that may recognize the same antigen. This advanced practice integrates sequence analysis with network theory to uncover these hidden structures. You will construct a similarity graph based on CDR3 sequences and apply the principle of modularity maximization to partition the graph into distinct \"communities,\" providing a powerful, data-driven method for hypothesizing about functional clonal families [@problem_id:2399318].", "problem": "You are given a finite set of amino-acid sequences representing complementarity-determining region 3 (CDR3) amino-acid sequences from T-cell receptors. Let the set of sequences be indexed as $\\{s_1,\\dots,s_n\\}$. Define the edit (Levenshtein) distance $d(s_i,s_j)$ between any two sequences $s_i$ and $s_j$ as the minimum number of single-character insertions, deletions, or substitutions required to transform $s_i$ into $s_j$. For a fixed nonnegative integer threshold $\\tau$, construct an undirected simple graph $G=(V,E)$ where $V=\\{1,\\dots,n\\}$ and an edge $\\{i,j\\}\\in E$ is present if and only if $d(s_i,s_j)\\le \\tau$. Let $A\\in\\{0,1\\}^{n\\times n}$ be the adjacency matrix of $G$ with $A_{ii}=0$ for all $i$, and let $k_i=\\sum_{j=1}^n A_{ij}$ denote the degree of node $i$. Let $m=\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n A_{ij}$ denote the total number of edges.\n\nA partition (community assignment) $c$ assigns to each node $i$ a community label $c_i\\in\\{1,\\dots,C\\}$ for some positive integer $C$. The Newman–Girvan modularity $Q(c)$ of a partition $c$ on $G$ is defined as\n$$\nQ(c)=\\frac{1}{2m}\\sum_{i=1}^n\\sum_{j=1}^n\\left(A_{ij}-\\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{c_i=c_j\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. When $m=0$ (no edges), define $Q(c)=0$ for all partitions $c$ by convention.\n\nYour task is to, for each specified test case, compute a partition $c^\\star$ that maximizes $Q(c)$ over all possible partitions $c$. If there are multiple maximizing partitions, apply the following deterministic tie-breaking rules in order:\n- If $m=0$, choose the partition in which every node forms its own community (that is, $C=n$ and each community has size $1$).\n- Otherwise, among partitions with maximal modularity, choose one with the smallest number of communities $C$.\n- If still tied, among the remaining partitions, choose the one whose multiset of community sizes, sorted in nonincreasing order, is lexicographically smallest.\n- If still tied, choose the partition whose vector of community labels $(c_1,\\dots,c_n)$, relabeled to be consecutive in order of first appearance, is lexicographically smallest.\n\nFor each test case, report the list of community sizes of $c^\\star$, sorted in nonincreasing order. The final program output must aggregate the results for all test cases into a single line containing a list whose elements are the per-test-case lists of community sizes, with no spaces.\n\nTest suite:\n- Test case $1$: $\\tau=1$, sequences\n  - $s_1=$ \"CASSLGQETQYF\"\n  - $s_2=$ \"CASSLGQETQFF\"\n  - $s_3=$ \"CASSLGQDTQYF\"\n  - $s_4=$ \"CASSIRSSYEQYF\"\n  - $s_5=$ \"CASSIGSSYEQYF\"\n  - $s_6=$ \"CASSIRSSYEQFF\"\n- Test case $2$: $\\tau=0$, sequences\n  - $s_1=$ \"CASSLAPGNTIYF\"\n  - $s_2=$ \"CATSQRGQLNTQF\"\n  - $s_3=$ \"CATSASGQGNNEQF\"\n  - $s_4=$ \"CASSYNEGYTF\"\n- Test case $3$: $\\tau=2$, sequences\n  - $s_1=$ \"CASSLGQETQYF\"\n  - $s_2=$ \"CASSLGQETQFF\"\n  - $s_3=$ \"CASSLGQDTQYF\"\n  - $s_4=$ \"CASSLGRETQYF\"\n- Test case $4$: $\\tau=1$, sequences\n  - $s_1=$ \"CASSVGQETQYF\"\n  - $s_2=$ \"CASSVGQETQFF\"\n  - $s_3=$ \"CASSVGRETQYF\"\n  - $s_4=$ \"CASSIGQETQYF\"\n  - $s_5=$ \"CASSIRSSYEQYF\"\n  - $s_6=$ \"CASSIRSSYEQFF\"\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results for the test cases in the order given. Each result must be the list of community sizes for that test case, sorted in nonincreasing order. The outer list and all inner lists must be rendered without spaces, for example as \"[[a,b],[c],[d,e,f]]\" where $a$, $b$, $c$, $d$, $e$, and $f$ are integers.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be scientifically grounded, well-posed, and objective. It presents a formal computational task rooted in the established principles of network science and bioinformatics. The problem is therefore valid, and a solution is provided below.\n\nThe core of the problem is to find a partition $c^\\star$ of a set of nodes $V=\\{1, \\dots, n\\}$ that maximizes the Newman-Girvan modularity function $Q(c)$. The nodes represent T-cell receptor CDR3 sequences, and the graph structure is determined by their similarity, quantified using the Levenshtein distance.\n\nThe modularity of a partition $c$ is given by\n$$\nQ(c)=\\frac{1}{2m}\\sum_{i=1}^n\\sum_{j=1}^n\\left(A_{ij}-\\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{c_i=c_j\\},\n$$\nwhere $A$ is the adjacency matrix, $k_i$ is the degree of node $i$, $m$ is the total number of edges, and $\\mathbf{1}\\{c_i=c_j\\}$ is an indicator function. For the degenerate case where $m=0$, $Q(c)$ is defined as $0$. The modularity $Q$ quantifies the quality of a partition by comparing the density of edges within communities to the density expected in a random graph with the same degree distribution. A higher $Q$ indicates a more significant community structure.\n\nThe task of maximizing $Q(c)$ is known to be NP-hard for general graphs. However, the number of nodes $n$ in the provided test cases is small (at most $n=6$). The total number of distinct partitions of an $n$-element set is given by the Bell number, $B_n$. For $n=6$, $B_6=203$, which is a computationally tractable number. Therefore, the problem can be solved by an exhaustive search over all possible partitions of the set of nodes.\n\nThe overall algorithm is as follows:\n\n1.  **Graph Construction**: For each test case, comprised of a set of sequences $\\{s_1, \\dots, s_n\\}$ and a threshold $\\tau$, an undirected graph $G=(V, E)$ is constructed. An edge $\\{i,j\\}$ exists if and only if the Levenshtein distance $d(s_i, s_j) \\le \\tau$. From this, the adjacency matrix $A$, the node degrees $k_i$, and the total number of edges $m$ are computed. The Levenshtein distance is calculated using the standard dynamic programming algorithm.\n\n2.  **Partition Generation**: All possible partitions of the vertex set $V=\\{1, \\dots, n\\}$ are generated. This can be accomplished with a recursive algorithm. Each partition is a set of non-empty, disjoint subsets of $V$ whose union is $V$.\n\n3.  **Modularity Calculation and Optimization**: For each generated partition $c$, the modularity $Q(c)$ is calculated. The formula can be expressed more conveniently for computation as a sum over communities:\n    $$\n    Q(c) = \\sum_{l=1}^C \\left( \\frac{e_l}{m} - \\left(\\frac{d_l}{2m}\\right)^2 \\right),\n    $$\n    where the sum is over the $C$ communities in the partition, $e_l$ is the number of edges within community $l$, and $d_l$ is the sum of the degrees of the nodes in community $l$.\n\n4.  **Tie-Breaking**: The problem specifies a strict, deterministic set of tie-breaking rules. To find the unique optimal partition $c^\\star$, we seek to find the lexicographically smallest tuple from the set of all possible partitions, where the tuple for a partition $c$ is constructed as:\n    $$\n    \\left( -Q(c), C(c), S(c), V(c) \\right)\n    $$\n    - $-Q(c)$: The negative of the modularity. Minimizing this maximizes $Q(c)$.\n    - $C(c)$: The number of communities in partition $c$.\n    - $S(c)$: The tuple of community sizes, sorted in nonincreasing order.\n    - $V(c)$: The canonical vector of community labels $(c_1, \\dots, c_n)$, relabeled to be consecutive integers in order of first appearance (e.g., the community containing node $1$ is labeled $1$, the first new community encountered when scanning nodes $2, 3, \\dots$ is labeled $2$, and so on). This provides a unique representation for each partition.\n\n    The search algorithm iterates through all partitions, computes this tuple for each, and maintains the partition corresponding to the lexicographically smallest tuple found. The special case for $m=0$ is handled separately as defined in the problem statement.\n\n5.  **Decomposition for Disconnected Graphs**: A critical observation is that an optimal partition will never group nodes from different connected components of the graph. Merging communities from different components can be shown to strictly decrease modularity. This implies that the problem can be decomposed: one can find the optimal partition for each connected component independently, and the union of these partitions forms the optimal partition for the entire graph. The modularity of the global partition is not a simple sum of component modularities, but the principle of optimality decomposition holds. This simplification is not strictly necessary for the small values of $n$ in the test suite but is a fundamental property of modularity. For the given test cases, this insight simplifies manual analysis. For instance, in Test Case $1$, the graph decomposes into two disjoint star graphs, allowing for separate analysis.\n\nBy systematically applying this procedure for each test case, a unique optimal partition $c^\\star$ is determined. The final output for each case is the list of sizes of the communities in $c^\\star$, sorted in nonincreasing order.", "answer": "```python\nimport numpy as np\n\ndef levenshtein_distance(s1: str, s2: str) - int:\n    \"\"\"Calculates the Levenshtein distance between two strings.\"\"\"\n    m, n = len(s1), len(s2)\n    if m  n:\n        s1, s2 = s2, s1\n        m, n = n, m\n    \n    if n == 0:\n        return m\n\n    dp_row = np.arange(n + 1, dtype=int)\n\n    for i in range(1, m + 1):\n        prev_row_val = dp_row[0]\n        dp_row[0] = i\n        for j in range(1, n + 1):\n            temp = dp_row[j]\n            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n            dp_row[j] = min(dp_row[j] + 1,          # Deletion\n                            dp_row[j - 1] + 1,      # Insertion\n                            prev_row_val + cost) # Substitution\n            prev_row_val = temp\n            \n    return dp_row[n]\n\ndef generate_partitions(n: int):\n    \"\"\"Generates all partitions of the set {0, 1, ..., n-1}.\"\"\"\n    elements = list(range(n))\n    def _generate(index, partition):\n        if index == n:\n            yield [list(p) for p in partition]\n            return\n        \n        # Add to an existing community\n        for p_set in partition:\n            p_set.add(elements[index])\n            yield from _generate(index + 1, partition)\n            p_set.remove(elements[index])\n            \n        # Add to a new community\n        partition.append({elements[index]})\n        yield from _generate(index + 1, partition)\n        partition.pop()\n\n    if n == 0:\n        yield []\n        return\n        \n    yield from _generate(1, [{elements[0]}])\n\ndef get_canonical_vector(partition, n: int) - tuple[int, ...]:\n    \"\"\"Computes the canonical label vector for a partition.\"\"\"\n    # Create an arbitrary labeling first\n    temp_c = [0] * n\n    label = 1\n    for community in partition:\n        for node_idx in community:\n            temp_c[node_idx] = label\n        label += 1\n    \n    # Relabel to be consecutive in order of first appearance\n    c_canon = [0] * n\n    mapping = {}\n    next_canon_label = 1\n    for i in range(n):\n        original_label = temp_c[i]\n        if original_label not in mapping:\n            mapping[original_label] = next_canon_label\n            next_canon_label += 1\n        c_canon[i] = mapping[original_label]\n    \n    return tuple(c_canon)\n\n\ndef solve():\n    \"\"\"\n    Solves the modularity maximization problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"tau\": 1,\n            \"sequences\": [\n                \"CASSLGQETQYF\", \"CASSLGQETQFF\", \"CASSLGQDTQYF\",\n                \"CASSIRSSYEQYF\", \"CASSIGSSYEQYF\", \"CASSIRSSYEQFF\"\n            ]\n        },\n        {\n            \"tau\": 0,\n            \"sequences\": [\n                \"CASSLAPGNTIYF\", \"CATSQRGQLNTQF\",\n                \"CATSASGQGNNEQF\", \"CASSYNEGYTF\"\n            ]\n        },\n        {\n            \"tau\": 2,\n            \"sequences\": [\n                \"CASSLGQETQYF\", \"CASSLGQETQFF\",\n                \"CASSLGQDTQYF\", \"CASSLGRETQYF\"\n            ]\n        },\n        {\n            \"tau\": 1,\n            \"sequences\": [\n                \"CASSVGQETQYF\", \"CASSVGQETQFF\", \"CASSVGRETQYF\",\n                \"CASSIGQETQYF\", \"CASSIRSSYEQYF\", \"CASSIRSSYEQFF\"\n            ]\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        tau, sequences = case[\"tau\"], case[\"sequences\"]\n        n = len(sequences)\n\n        # Step 1: Construct graph\n        adj = np.zeros((n, n), dtype=int)\n        for i in range(n):\n            for j in range(i + 1, n):\n                if levenshtein_distance(sequences[i], sequences[j]) = tau:\n                    adj[i, j] = adj[j, i] = 1\n\n        degrees = np.sum(adj, axis=1)\n        m_total_edges = int(np.sum(degrees) / 2)\n\n        # Handle m=0 case as per tie-breaking rule 1\n        if m_total_edges == 0:\n            results.append([1] * n)\n            continue\n\n        best_key = (float('inf'),)\n        best_partition_sizes = []\n\n        two_m = 2 * m_total_edges\n\n        # Iterate through all partitions of the nodes\n        for partition in generate_partitions(n):\n            \n            # Calculate modularity\n            q_val = 0.0\n            for community in partition:\n                d_l = np.sum(degrees[community])\n                e_l = 0\n                for i in range(len(community)):\n                    for j in range(i + 1, len(community)):\n                        u, v = community[i], community[j]\n                        e_l += adj[u, v]\n                \n                term_1 = e_l / m_total_edges if m_total_edges > 0 else 0\n                term_2 = (d_l / two_m)**2 if two_m > 0 else 0\n                q_val += (term_1 - term_2)\n            \n            # Get tie-breaker values\n            num_communities = len(partition)\n            sorted_sizes = tuple(sorted([len(c) for c in partition], reverse=True))\n            canonical_vec = get_canonical_vector(partition, n)\n            \n            current_key = (-q_val, num_communities, sorted_sizes, canonical_vec)\n\n            if best_key[0] == float('inf') or current_key  best_key:\n                best_key = current_key\n                best_partition_sizes = list(sorted_sizes)\n        \n        results.append(best_partition_sizes)\n\n    # Final output formatting\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "2399318"}]}