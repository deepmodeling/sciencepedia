{"hands_on_practices": [{"introduction": "To confidently estimate causal effects from observational data, we must first correctly identify and adjust for confounding variables. Directed Acyclic Graphs (DAGs) provide the formal language for reasoning about causal structures and confounding. This exercise will build your skills in applying the backdoor criterion to a hypothetical genomic system, a fundamental step in designing a valid causal analysis. By identifying the correct set of covariates for adjustment, you will learn to block non-causal pathways between an exposure and an outcome, ensuring your effect estimate is not biased by confounding [@problem_id:2377421].", "problem": "In a genomics study of regulatory mechanisms, consider the following causal system represented by a Directed Acyclic Graph (DAG). Genetic variants $Z_1$ and $Z_2$ are single-nucleotide polymorphisms that act upstream of gene regulation. The variable $X$ denotes the messenger RNA expression of a transcription factor, $M$ denotes the expression level of a downstream target module regulated by $X$, and $Y$ denotes a cellular phenotype measured at steady state. The DAG structure is:\n$$Z_1 \\rightarrow X \\rightarrow M \\rightarrow Y \\leftarrow Z_2 \\rightarrow X.$$\nAssume all variables $\\{Z_1, Z_2, X, M, Y\\}$ are observed, and that you aim to identify the total causal effect of $X$ on $Y$ from purely observational data using covariate adjustment based solely on d-separation in the given DAG. Adjustment is restricted to pre-exposure variables that are not descendants of $X$.\n\nUsing the DAG above and d-separation, determine the number of distinct covariate adjustment sets drawn from the variables $\\{Z_1, Z_2, M\\}$ that suffice to identify the total causal effect of $X$ on $Y$ via the backdoor criterion, under the restriction that no descendant of $X$ may be included in the adjustment set.\n\nProvide your answer as a single integer. No rounding is required.", "solution": "The problem requires the identification of the number of distinct covariate adjustment sets sufficient to estimate the total causal effect of a variable $X$ on a variable $Y$, based on a provided Directed Acyclic Graph (DAG) and a set of constraints. I shall first validate the problem statement and then proceed with a rigorous derivation if the problem is deemed valid.\n\nThe givens are as follows:\n- A set of five observed variables: genetic variants $Z_1$ and $Z_2$, a transcription factor expression $X$, a target module expression $M$, and a phenotype $Y$.\n- A causal structure represented by the DAG: $Z_1 \\rightarrow X \\rightarrow M \\rightarrow Y \\leftarrow Z_2 \\rightarrow X$. This notation specifies the following directed edges: $Z_1 \\rightarrow X$, $X \\rightarrow M$, $M \\rightarrow Y$, $Z_2 \\rightarrow Y$, and $Z_2 \\rightarrow X$.\n- The goal is to identify the total causal effect of $X$ on $Y$.\n- The method is covariate adjustment based on the backdoor criterion derived from d-separation.\n- The set of potential covariates for adjustment is $\\{Z_1, Z_2, M\\}$.\n- A crucial restriction is that the adjustment set must not contain any descendants of $X$.\n\nThe problem is scientifically grounded in the field of causal inference in genomics, it is well-posed, objective, and internally consistent. There are no violations of scientific principles or logical fallacies. The problem is valid.\n\nTo identify the total causal effect of $X$ on $Y$ using covariate adjustment, we must find all sets of covariates $\\mathcal{S}$ that satisfy the backdoor criterion. A set of variables $\\mathcal{S}$ satisfies the backdoor criterion relative to an ordered pair of variables $(X, Y)$ if two conditions are met:\n1. No variable in $\\mathcal{S}$ is a descendant of $X$.\n2. $\\mathcal{S}$ blocks every backdoor path between $X$ and $Y$. A backdoor path is a non-causal path between $X$ and $Y$ that begins with an arrow pointing into $X$.\n\nLet us apply these conditions systematically.\n\nFirst, we must identify the descendants of $X$ in the given DAG. The directed paths originating from $X$ are $X \\rightarrow M$ and $X \\rightarrow M \\rightarrow Y$. Therefore, the descendants of $X$ are the variables $\\{M, Y\\}$. The problem explicitly forbids including descendants of $X$ in the adjustment set. The set of available covariates is $\\{Z_1, Z_2, M\\}$. This constraint immediately disqualifies $M$ from being part of any valid adjustment set $\\mathcal{S}$. Consequently, any valid adjustment set $\\mathcal{S}$ must be a subset of $\\{Z_1, Z_2\\}$.\n\nSecond, we must identify all backdoor paths between $X$ and $Y$. A backdoor path is a path that starts with an edge pointing into $X$. In the given DAG, the edges pointing into $X$ are $Z_1 \\rightarrow X$ and $Z_2 \\rightarrow X$.\n- Consider a path starting with $X \\leftarrow Z_1$. For this to be a backdoor path, there must be a path from $Z_1$ to $Y$ that does not pass through $X$. Examining the DAG, there is no such path. The only path from $Z_1$ to any other variable is through $X$. Thus, there are no backdoor paths involving $Z_1$. The variable $Z_1$ acts as an instrumental variable for the effect of $X$ on $Y$, not as a confounder.\n- Consider a path starting with $X \\leftarrow Z_2$. For this to be a backdoor path, there must be a path from $Z_2$ to $Y$ that does not pass through $X$. The DAG includes the edge $Z_2 \\rightarrow Y$. Therefore, the path $X \\leftarrow Z_2 \\rightarrow Y$ exists. This is a backdoor path created by the common cause $Z_2$, which confounds the relationship between $X$ and $Y$. This is the sole backdoor path in the graph.\n\nThird, the adjustment set $\\mathcal{S}$ must block this backdoor path, $X \\leftarrow Z_2 \\rightarrow Y$. This path is a \"fork\" structure, which is d-separated (blocked) if and only if the middle variable, $Z_2$, is included in the conditioning set $\\mathcal{S}$. Therefore, any valid adjustment set must contain $Z_2$.\n\nFourth, we must ensure that conditioning on the set $\\mathcal{S}$ does not open any previously blocked paths. New paths can be opened by conditioning on a collider node or its descendants. The collider nodes in the DAG are $X$ (as seen in the structure $Z_1 \\rightarrow X \\leftarrow Z_2$) and $Y$ (as seen in $M \\rightarrow Y \\leftarrow Z_2$). Our potential conditioning variables are $Z_1$ and $Z_2$. Neither of these is a collider, nor a descendant of one. Thus, conditioning on any subset of $\\{Z_1, Z_2\\}$ will not open any spurious paths.\n\nCombining our findings:\n1. The adjustment set $\\mathcal{S}$ must be a subset of $\\{Z_1, Z_2\\}$.\n2. The adjustment set $\\mathcal{S}$ must contain $Z_2$.\n\nThe possible subsets of $\\{Z_1, Z_2\\}$ are $\\emptyset$, $\\{Z_1\\}$, $\\{Z_2\\}$, and $\\{Z_1, Z_2\\}$. We evaluate each against our criteria:\n- $\\mathcal{S} = \\emptyset$: Invalid. It does not contain $Z_2$.\n- $\\mathcal{S} = \\{Z_1\\}$: Invalid. It does not contain $Z_2$.\n- $\\mathcal{S} = \\{Z_2\\}$: Valid. It contains $Z_2$ and is a subset of $\\{Z_1, Z_2\\}$. It blocks the only backdoor path and opens no new paths.\n- $\\mathcal{S} = \\{Z_1, Z_2\\}$: Valid. It contains $Z_2$, thus blocking the backdoor path. The inclusion of $Z_1$ is redundant for satisfying the backdoor criterion but not incorrect. The set still satisfies the two conditions of the backdoor criterion.\n\nTherefore, there are exactly two distinct covariate adjustment sets that suffice to identify the total causal effect of $X$ on $Y$ under the given constraints. These sets are $\\{Z_2\\}$ and $\\{Z_1, Z_2\\}$. The number of such sets is $2$.", "answer": "$$ \\boxed{2} $$", "id": "2377421"}, {"introduction": "Mendelian Randomization (MR) is a powerful method for causal inference in genomics, but its validity hinges on strong assumptions. This practice explores what happens when a key assumption—that the genetic instrument affects the outcome only through the exposure of interest—is violated due to horizontal pleiotropy. You will model a scenario where the instrument is in linkage disequilibrium with a variant that has a direct effect on the outcome, and quantify the resulting bias [@problem_id:2377409]. This provides a crucial, quantitative understanding of how hidden genetic correlations can distort causal conclusions.", "problem": "Consider two biallelic single-nucleotide polymorphisms (SNPs), denoted $G$ and $Z$, each coded additively and standardized to have mean $0$ and variance $1$. Let their correlation be the signed linkage disequilibrium (LD) correlation $r \\in [-1,1]$, and let the LD measure be $r^2 \\in [0,1]$. Suppose there is an exposure $E$ and an outcome $Y$ generated according to linear structural equations:\n$E = \\gamma G + \\xi$ and $Y = \\beta E + \\delta Z + \\varepsilon$,\nwhere $\\gamma, \\beta, \\delta \\in \\mathbb{R}$ are fixed constants, and the noise terms $\\xi$ and $\\varepsilon$ are mean-zero and jointly independent of $(G,Z)$. Assume $Z$ has no causal effect on $E$, and $G$ has no direct effect on $Y$ other than through $E$. Consider a two-sample Mendelian randomization (MR) study, where the large-sample marginal association of $E$ on $G$ is estimated in one sample, and the large-sample marginal association of $Y$ on $G$ is estimated in an independent sample. The MR estimator is defined as the ratio of these two large-sample marginal associations.\n\nYou are given the signed LD correlation indirectly by $r^2$ and a sign parameter $s \\in \\{-1,+1\\}$, where $r = s \\sqrt{r^2}$. For each specified parameter tuple $(\\gamma, \\beta, \\delta, r^2, s)$, compute:\n1) the large-sample MR estimate of the causal effect of $E$ on $Y$ using $G$ as the instrument,\n2) the absolute bias of this estimate relative to the true causal effect $\\beta$,\n3) a boolean indicating whether the MR estimate has a sign opposite to the sign of $\\beta$.\n\nAll computations are dimensionless; no physical units apply. Your program must use the following test suite of parameter tuples $(\\gamma, \\beta, \\delta, r^2, s)$:\n- Case $1$: $(0.2, 0.5, 0.3, 0.25, +1)$.\n- Case $2$: $(0.2, 0.5, 0.3, 0.0, +1)$.\n- Case $3$: $(0.4, 0.5, 0.3, 0.36, -1)$.\n- Case $4$: $(0.05, 0.1, 0.2, 0.81, -1)$.\n- Case $5$: $(-0.3, 0.4, 0.2, 1.0, +1)$.\n\nFor each case, output a three-element list $[\\widehat{\\beta}_{\\mathrm{MR}}, \\mathrm{bias}, \\mathrm{flip}]$, where $\\widehat{\\beta}_{\\mathrm{MR}}$ is the large-sample MR estimate, $\\mathrm{bias}$ is the absolute bias $|\\widehat{\\beta}_{\\mathrm{MR}} - \\beta|$, and $\\mathrm{flip}$ is a boolean equal to $\\mathrm{True}$ if $\\widehat{\\beta}_{\\mathrm{MR}}$ and $\\beta$ have opposite signs and $\\mathrm{False}$ otherwise. Express all floating-point numbers rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a three-element list corresponding to a test case, for example $[[a,b, \\mathrm{False}],[c,d,\\mathrm{True}],\\dots]$ in the given order of cases.", "solution": "The problem statement submitted for analysis is deemed valid. It is scientifically grounded within the field of statistical genetics and causal inference, specifically Mendelian randomization (MR). The problem is well-posed, with all necessary parameters and assumptions for a unique solution provided. It is objective and free from ambiguity. We may therefore proceed with a formal solution.\n\nThe problem asks for the computation of a large-sample two-sample MR estimate, its bias, and a sign-change indicator, based on a defined linear structural model. We are given two standardized single-nucleotide polymorphisms (SNPs), $G$ and $Z$, with mean $0$ and variance $1$. Their covariance is thus equal to their correlation, $\\mathrm{Cov}(G, Z) = r = s \\sqrt{r^2}$. The system is described by the equations:\n$$ E = \\gamma G + \\xi $$\n$$ Y = \\beta E + \\delta Z + \\varepsilon $$\nwhere $G$ is the instrumental variable for the effect of the exposure $E$ on the outcome $Y$. The true causal effect of $E$ on $Y$ is $\\beta$. The SNP $Z$ is a source of horizontal pleiotropy, as it affects $Y$ directly ($\\delta \\neq 0$) and is in linkage disequilibrium (LD) with the instrument $G$ ($r \\neq 0$).\n\nThe MR estimator, $\\widehat{\\beta}_{\\mathrm{MR}}$, is defined as the ratio of the large-sample association of the outcome $Y$ on the instrument $G$ to the association of the exposure $E$ on $G$. In large samples, these estimated associations converge to their population analogs. For a standardized instrument $G$ with $\\mathrm{Var}(G) = 1$, the association coefficients are equal to the covariances.\n\nFirst, we derive the SNP-exposure association, which we denote $\\gamma_{G,E}$.\n$$ \\gamma_{G,E} = \\frac{\\mathrm{Cov}(E, G)}{\\mathrm{Var}(G)} = \\mathrm{Cov}(E, G) $$\nSubstituting the model for $E$:\n$$ \\gamma_{G,E} = \\mathrm{Cov}(\\gamma G + \\xi, G) = \\gamma \\mathrm{Cov}(G, G) + \\mathrm{Cov}(\\xi, G) $$\nGiven $\\mathrm{Var}(G) = 1$ and that the noise term $\\xi$ is independent of $G$ ($\\mathrm{Cov}(\\xi, G) = 0$), this simplifies to:\n$$ \\gamma_{G,E} = \\gamma \\cdot 1 + 0 = \\gamma $$\n\nSecond, we derive the SNP-outcome association, $\\Gamma_{G,Y}$.\n$$ \\Gamma_{G,Y} = \\frac{\\mathrm{Cov}(Y, G)}{\\mathrm{Var}(G)} = \\mathrm{Cov}(Y, G) $$\nWe substitute the model for $Y$, and then the model for $E$:\n$$ Y = \\beta E + \\delta Z + \\varepsilon = \\beta (\\gamma G + \\xi) + \\delta Z + \\varepsilon = \\beta\\gamma G + \\beta\\xi + \\delta Z + \\varepsilon $$\nNow, we compute the covariance with $G$:\n$$ \\Gamma_{G,Y} = \\mathrm{Cov}(\\beta\\gamma G + \\beta\\xi + \\delta Z + \\varepsilon, G) $$\nUsing the linearity of covariance and the independence of noise terms ($\\mathrm{Cov}(\\xi, G)=0, \\mathrm{Cov}(\\varepsilon, G)=0$):\n$$ \\Gamma_{G,Y} = \\beta\\gamma \\mathrm{Cov}(G, G) + \\beta \\mathrm{Cov}(\\xi, G) + \\delta \\mathrm{Cov}(Z, G) + \\mathrm{Cov}(\\varepsilon, G) = \\beta\\gamma(1) + \\beta(0) + \\delta r + 0 = \\beta\\gamma + \\delta r $$\n\nThe large-sample MR estimate $\\widehat{\\beta}_{\\mathrm{MR}}$ is the ratio of these two quantities:\n$$ \\widehat{\\beta}_{\\mathrm{MR}} = \\frac{\\Gamma_{G,Y}}{\\gamma_{G,E}} = \\frac{\\beta\\gamma + \\delta r}{\\gamma} = \\beta + \\delta \\frac{r}{\\gamma} $$\nUsing the given relation $r = s\\sqrt{r^2}$, the final expression for the estimate is:\n$$ \\widehat{\\beta}_{\\mathrm{MR}} = \\beta + \\delta \\frac{s\\sqrt{r^2}}{\\gamma} $$\nThis expression is valid provided $\\gamma \\neq 0$, which holds for all test cases.\n\nThe absolute bias is the absolute difference between the estimate and the true effect $\\beta$:\n$$ \\mathrm{bias} = |\\widehat{\\beta}_{\\mathrm{MR}} - \\beta| = \\left| \\left(\\beta + \\delta \\frac{r}{\\gamma}\\right) - \\beta \\right| = \\left| \\delta \\frac{r}{\\gamma} \\right| = \\frac{|\\delta| \\sqrt{r^2}}{|\\gamma|} $$\n\nThe sign flip indicator, $\\mathrm{flip}$, is a boolean that is true if $\\widehat{\\beta}_{\\mathrm{MR}}$ and $\\beta$ have opposite signs. This is equivalent to testing the condition $\\widehat{\\beta}_{\\mathrm{MR}} \\cdot \\beta < 0$, as none of the provided $\\beta$ values are zero.\n\nWe now apply these formulas to each test case.\n\n**Case 1:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.2, 0.5, 0.3, 0.25, +1)$\n$r = +1 \\cdot \\sqrt{0.25} = 0.5$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.5 + 0.3 \\cdot \\frac{0.5}{0.2} = 0.5 + 0.75 = 1.25$.\n$\\mathrm{bias} = |1.25 - 0.5| = 0.75$.\n$\\mathrm{flip}$: $1.25 \\cdot 0.5 > 0$, so $\\mathrm{False}$.\nResult: $[1.250000, 0.750000, \\mathrm{False}]$.\n\n**Case 2:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.2, 0.5, 0.3, 0.0, +1)$\n$r = +1 \\cdot \\sqrt{0.0} = 0.0$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.5 + 0.3 \\cdot \\frac{0.0}{0.2} = 0.5$.\n$\\mathrm{bias} = |0.5 - 0.5| = 0.0$.\n$\\mathrm{flip}$: $0.5 \\cdot 0.5 > 0$, so $\\mathrm{False}$.\nResult: $[0.500000, 0.000000, \\mathrm{False}]$. This is the unbiased case where LD is zero.\n\n**Case 3:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.4, 0.5, 0.3, 0.36, -1)$\n$r = -1 \\cdot \\sqrt{0.36} = -0.6$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.5 + 0.3 \\cdot \\frac{-0.6}{0.4} = 0.5 - 0.45 = 0.05$.\n$\\mathrm{bias} = |0.05 - 0.5| = |-0.45| = 0.45$.\n$\\mathrm{flip}$: $0.05 \\cdot 0.5 > 0$, so $\\mathrm{False}$.\nResult: $[0.050000, 0.450000, \\mathrm{False}]$.\n\n**Case 4:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.05, 0.1, 0.2, 0.81, -1)$\n$r = -1 \\cdot \\sqrt{0.81} = -0.9$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.1 + 0.2 \\cdot \\frac{-0.9}{0.05} = 0.1 - 3.6 = -3.5$.\n$\\mathrm{bias} = |-3.5 - 0.1| = |-3.6| = 3.6$.\n$\\mathrm{flip}$: $-3.5 \\cdot 0.1 < 0$, so $\\mathrm{True}$.\nResult: $[-3.500000, 3.600000, \\mathrm{True}]$. Strong bias due to a weak instrument ($\\gamma=0.05$) leads to a sign flip.\n\n**Case 5:** $(\\gamma, \\beta, \\delta, r^2, s) = (-0.3, 0.4, 0.2, 1.0, +1)$\n$r = +1 \\cdot \\sqrt{1.0} = 1.0$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.4 + 0.2 \\cdot \\frac{1.0}{-0.3} = 0.4 - \\frac{2}{3} \\approx -0.266667$.\n$\\mathrm{bias} = |(-0.266667) - 0.4| = |-0.666667| \\approx 0.666667$.\n$\\mathrm{flip}$: $(-0.266667) \\cdot 0.4 < 0$, so $\\mathrm{True}$.\nResult: $[-0.266667, 0.666667, \\mathrm{True}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Mendelian randomization estimate, bias, and sign flip\n    for a series of parameter sets.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (gamma, beta, delta, r_squared, s)\n    test_cases = [\n        (0.2, 0.5, 0.3, 0.25, 1),\n        (0.2, 0.5, 0.3, 0.0, 1),\n        (0.4, 0.5, 0.3, 0.36, -1),\n        (0.05, 0.1, 0.2, 0.81, -1),\n        (-0.3, 0.4, 0.2, 1.0, 1),\n    ]\n\n    results = []\n    for case in test_cases:\n        gamma, beta, delta, r_squared, s = case\n\n        # Calculate the signed LD correlation r\n        r = s * np.sqrt(r_squared)\n\n        # Calculate the large-sample MR estimate beta_mr\n        # The formula is derived as beta_mr = beta + delta * r / gamma\n        # This is valid as gamma is non-zero in all test cases.\n        beta_mr = beta + (delta * r) / gamma\n\n        # Calculate the absolute bias of the estimate\n        bias = abs(beta_mr - beta)\n\n        # Determine if the estimate's sign is opposite to the true effect's sign.\n        # This condition is equivalent to beta_mr * beta < 0, assuming beta is not 0.\n        # In all test cases, beta is positive, so this check simplifies to beta_mr < 0.\n        flip = (beta_mr * beta) < 0\n\n        results.append([beta_mr, bias, flip])\n\n    # Format the results into the required string format.\n    # Each result is a list [beta_mr, bias, flip].\n    # Floating point numbers are formatted to 6 decimal places.\n    # Booleans are converted to their string representation ('True' or 'False').\n    formatted_results = []\n    for res in results:\n        # Format: [float_val, float_val, Boolean]\n        # Example: [-3.500000, 3.600000, True]\n        formatted_res_str = f\"[{res[0]:.6f}, {res[1]:.6f}, {str(res[2])}]\"\n        formatted_results.append(formatted_res_str)\n\n    # Final print statement in the exact required format.\n    # Example: [[result1], [result2], ...]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2377409"}, {"introduction": "Biological systems are complex, and an outcome is rarely influenced by a single exposure. When multiple exposures are correlated, standard MR can be misleading. This is where Multivariable Mendelian Randomization (MVMR) becomes essential, as it can simultaneously estimate the direct causal effect of several exposures on an outcome. In this problem, you will implement the MVMR estimator, framing it as a generalized least squares problem [@problem_id:2377474]. This exercise will equip you with a sophisticated technique to disentangle the effects of interrelated factors, a common challenge in modern genomics.", "problem": "You are given a summary-level formulation of a multivariable instrumental variables problem arising in Mendelian Randomization (MR). The goal is to estimate the vector of direct causal effects of two exposures, body mass index (BMI) and years of schooling, on an income outcome using Multivariable Mendelian Randomization (MVMR). Assume you have $L$ genetic variants used as instruments, indexed by $l \\in \\{1,\\dots,L\\}$. For each variant $l$, you are given:\n- The association estimates with the two exposures, collected row-wise into a matrix $A_{x} \\in \\mathbb{R}^{L \\times 2}$.\n- The association estimate with the outcome, collected into a vector $b_{y} \\in \\mathbb{R}^{L}$.\n- The standard errors of the outcome associations, collected into a diagonal matrix $S_{y} = \\mathrm{diag}(s_{1,y},\\dots,s_{L,y}) \\in \\mathbb{R}^{L \\times L}$.\n- A symmetric positive definite instrument correlation matrix $R \\in \\mathbb{R}^{L \\times L}$.\n\nAssume the asymptotic covariance of $b_{y}$ is $\\Sigma_{y} = S_{y} R S_{y}$. Define the MVMR estimator $\\hat{\\beta} \\in \\mathbb{R}^{2}$ as the minimizer of the weighted quadratic form\n$$\nQ(\\beta) = (b_{y} - A_{x} \\beta)^{\\top} \\Sigma_{y}^{-1} (b_{y} - A_{x} \\beta),\n$$\nwhere $\\beta = [\\beta_{\\mathrm{BMI}}, \\beta_{\\mathrm{Schooling}}]^{\\top}$ is the vector of direct causal effects. Compute $\\hat{\\beta}$ for each of the following test cases.\n\nAll arrays below are provided in row-major order, with each number given as a real value. For each case, use exactly the matrices and vectors as given, with $S_{y} = \\mathrm{diag}(s_{1,y},\\dots,s_{L,y})$ and $\\Sigma_{y} = S_{y} R S_{y}$.\n\nTest case 1 (independent instruments, well-conditioned):\n- $L = 5$.\n- $A_{x} = \\begin{bmatrix}\n0.08 & 0.02\\\\\n0.06 & 0.01\\\\\n0.07 & 0.03\\\\\n0.05 & 0.02\\\\\n0.09 & 0.01\n\\end{bmatrix}$.\n- $b_{y} = \\begin{bmatrix} 0.044\\\\ 0.028\\\\ 0.0445\\\\ 0.0295\\\\ 0.041 \\end{bmatrix}$.\n- $s_{y} = \\begin{bmatrix} 0.02\\\\ 0.02\\\\ 0.02\\\\ 0.02\\\\ 0.02 \\end{bmatrix}$, so $S_{y} = \\mathrm{diag}(0.02, 0.02, 0.02, 0.02, 0.02)$.\n- $R = I_{5}$, the $5 \\times 5$ identity matrix.\n\nTest case 2 (correlated instruments, heteroskedastic standard errors):\n- $L = 4$.\n- $A_{x} = \\begin{bmatrix}\n0.05 & 0.04\\\\\n0.045 & 0.035\\\\\n0.06 & 0.05\\\\\n0.055 & 0.045\n\\end{bmatrix}$.\n- $b_{y} = \\begin{bmatrix} 0.04\\\\ 0.0333\\\\ 0.0488\\\\ 0.0429 \\end{bmatrix}$.\n- $s_{y} = \\begin{bmatrix} 0.015\\\\ 0.02\\\\ 0.018\\\\ 0.022 \\end{bmatrix}$, so $S_{y} = \\mathrm{diag}(0.015, 0.02, 0.018, 0.022)$.\n- $R = \\begin{bmatrix}\n1 & 0.25 & 0.0625 & 0.015625\\\\\n0.25 & 1 & 0.25 & 0.0625\\\\\n0.0625 & 0.25 & 1 & 0.25\\\\\n0.015625 & 0.0625 & 0.25 & 1\n\\end{bmatrix}$.\n\nTest case 3 (nearly collinear exposures, weak instruments):\n- $L = 3$.\n- $A_{x} = \\begin{bmatrix}\n0.02 & 0.0204\\\\\n0.018 & 0.01836\\\\\n0.017 & 0.01751\n\\end{bmatrix}$.\n- $b_{y} = \\begin{bmatrix} -0.0002\\\\ 0.00032\\\\ -0.000555 \\end{bmatrix}$.\n- $s_{y} = \\begin{bmatrix} 0.02\\\\ 0.02\\\\ 0.02 \\end{bmatrix}$, so $S_{y} = \\mathrm{diag}(0.02, 0.02, 0.02)$.\n- $R = I_{3}$, the $3 \\times 3$ identity matrix.\n\nYour task: For each test case, compute the two-dimensional estimator $\\hat{\\beta} = [\\hat{\\beta}_{\\mathrm{BMI}}, \\hat{\\beta}_{\\mathrm{Schooling}}]^{\\top}$ that minimizes $Q(\\beta)$ as defined above. Report each component as a real number.\n\nFinal output format: Your program should produce a single line of output containing the results for all test cases as a single flat list in the order $[\\hat{\\beta}_{\\mathrm{BMI}}^{(1)}, \\hat{\\beta}_{\\mathrm{Schooling}}^{(1)}, \\hat{\\beta}_{\\mathrm{BMI}}^{(2)}, \\hat{\\beta}_{\\mathrm{Schooling}}^{(2)}, \\hat{\\beta}_{\\mathrm{BMI}}^{(3)}, \\hat{\\beta}_{\\mathrm{Schooling}}^{(3)}]$, where the superscript denotes the test case index. Each number must be rounded to six decimal places. The list must be enclosed in square brackets with comma separators, for example, $[0.123456,0.654321, \\dots]$.", "solution": "The problem statement is scrutinized and found to be valid. It is a well-posed problem in statistical estimation, grounded in the established principles of Multivariable Mendelian Randomization (MVMR). All data and definitions required for a unique solution are provided, and there are no scientific or logical contradictions. I will now proceed with the solution.\n\nThe objective is to find the vector of causal effects $\\hat{\\beta} \\in \\mathbb{R}^{2}$ that minimizes the quadratic form:\n$$\nQ(\\beta) = (b_{y} - A_{x} \\beta)^{\\top} \\Sigma_{y}^{-1} (b_{y} - A_{x} \\beta)\n$$\nwhere $b_{y} \\in \\mathbb{R}^{L}$ is the vector of instrument-outcome associations, $A_{x} \\in \\mathbb{R}^{L \\times 2}$ is the matrix of instrument-exposure associations, and $\\Sigma_{y}^{-1}$ is the inverse of the covariance matrix of $b_{y}$. This is a standard generalized least squares (GLS) problem.\n\nTo find the minimizer $\\hat{\\beta}$, we must compute the gradient of $Q(\\beta)$ with respect to $\\beta$ and set it to the zero vector. First, we expand the quadratic form:\n$$\nQ(\\beta) = b_{y}^{\\top}\\Sigma_{y}^{-1}b_{y} - b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta - \\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y} + \\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta\n$$\nThe terms $b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta$ and $\\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}$ are scalars. Since the transpose of a scalar is itself, and noting that $\\Sigma_{y}^{-1}$ is symmetric, we have $(\\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y})^{\\top} = b_{y}^{\\top}(\\Sigma_{y}^{-1})^{\\top}(A_{x}^{\\top})^{\\top}\\beta = b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta$. Therefore, the two terms are equal. The expression simplifies to:\n$$\nQ(\\beta) = b_{y}^{\\top}\\Sigma_{y}^{-1}b_{y} - 2 b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta + \\beta^{\\top}(A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})\\beta\n$$\nThis is a quadratic function of $\\beta$. The gradient with respect to $\\beta$ is obtained by applying standard rules of vector calculus:\n$$\n\\frac{\\partial Q(\\beta)}{\\partial \\beta} = -2 A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y} + 2 (A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})\\beta\n$$\nSetting the gradient to the zero vector, $\\frac{\\partial Q(\\beta)}{\\partial \\beta} = 0$, to find the minimum yields the so-called normal equations for the GLS estimator:\n$$\n(A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})\\hat{\\beta} = A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}\n$$\nProvided that the matrix $A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x}$ is invertible, which requires that the matrix of instrument-exposure associations $A_x$ has full column rank, we can solve for $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = (A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})^{-1} A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}\n$$\nThe covariance matrix $\\Sigma_{y}$ is given by $\\Sigma_{y} = S_{y} R S_{y}$, where $S_{y}$ is the diagonal matrix of standard errors and $R$ is the instrument correlation matrix. Both $S_y$ and $R$ are specified as invertible, so $\\Sigma_y$ is also invertible.\n\nThis formula provides the analytical solution. The implementation will compute this for each test case. For numerical stability, instead of explicitly computing the inverse of the $2 \\times 2$ matrix $C = A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x}$, we solve the linear system $C\\hat{\\beta} = d$, where $d = A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}$.\n\nFor Test Case 1 and Test Case 3, the instrument correlation matrix $R$ is the identity matrix $I$, and the standard errors are uniform, i.e., $s_{l,y} = s$ for all $l$. In this special case, $\\Sigma_{y} = \\mathrm{diag}(s, \\dots, s) \\cdot I \\cdot \\mathrm{diag}(s, \\dots, s) = s^2 I$. The inverse is $\\Sigma_{y}^{-1} = (1/s^2)I$. The estimator simplifies:\n$$\n\\hat{\\beta} = (A_{x}^{\\top}(\\frac{1}{s^2}I)A_{x})^{-1} A_{x}^{\\top}(\\frac{1}{s^2}I)b_{y} = (\\frac{1}{s^2}A_{x}^{\\top}A_{x})^{-1} (\\frac{1}{s^2}A_{x}^{\\top}b_{y}) = (s^2(A_{x}^{\\top}A_{x})^{-1}) (\\frac{1}{s^2}A_{x}^{\\top}b_{y}) = (A_{x}^{\\top}A_{x})^{-1}A_{x}^{\\top}b_{y}\n$$\nThis is the ordinary least squares (OLS) estimator. For Test Case 2, the full GLS formula must be used due to the non-diagonal $R$ and heteroskedastic standard errors.\n\nThe computation proceeds as follows for each test case:\n1. Define the matrices $A_x$, $R$ and the vectors $b_y$, $s_y$.\n2. Construct the diagonal matrix $S_y$ from the vector $s_y$.\n3. Compute the covariance matrix $\\Sigma_y = S_y R S_y$.\n4. Compute the weight matrix $W = \\Sigma_y^{-1}$.\n5. Compute the matrix $C = A_x^{\\top} W A_x$.\n6. Compute the vector $d = A_x^{\\top} W b_y$.\n7. Solve the $2 \\times 2$ linear system $C\\hat{\\beta} = d$ to obtain $\\hat{\\beta} = [\\hat{\\beta}_{\\mathrm{BMI}}, \\hat{\\beta}_{\\mathrm{Schooling}}]^{\\top}$.\n8. The results from all test cases are collected and formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Multivariable Mendelian Randomization (MVMR) estimator\n    for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"L\": 5,\n            \"Ax\": np.array([\n                [0.08, 0.02],\n                [0.06, 0.01],\n                [0.07, 0.03],\n                [0.05, 0.02],\n                [0.09, 0.01]\n            ]),\n            \"by\": np.array([0.044, 0.028, 0.0445, 0.0295, 0.041]),\n            \"sy\": np.array([0.02, 0.02, 0.02, 0.02, 0.02]),\n            \"R\": np.identity(5)\n        },\n        {\n            \"L\": 4,\n            \"Ax\": np.array([\n                [0.05, 0.04],\n                [0.045, 0.035],\n                [0.06, 0.05],\n                [0.055, 0.045]\n            ]),\n            \"by\": np.array([0.04, 0.0333, 0.0488, 0.0429]),\n            \"sy\": np.array([0.015, 0.02, 0.018, 0.022]),\n            \"R\": np.array([\n                [1.0, 0.25, 0.0625, 0.015625],\n                [0.25, 1.0, 0.25, 0.0625],\n                [0.0625, 0.25, 1.0, 0.25],\n                [0.015625, 0.0625, 0.25, 1.0]\n            ])\n        },\n        {\n            \"L\": 3,\n            \"Ax\": np.array([\n                [0.02, 0.0204],\n                [0.018, 0.01836],\n                [0.017, 0.01751]\n            ]),\n            \"by\": np.array([-0.0002, 0.00032, -0.000555]),\n            \"sy\": np.array([0.02, 0.02, 0.02]),\n            \"R\": np.identity(3)\n        }\n    ]\n\n    all_betas = []\n\n    for case in test_cases:\n        Ax = case[\"Ax\"]\n        by = case[\"by\"]\n        sy = case[\"sy\"]\n        R = case[\"R\"]\n        L = case[\"L\"]\n\n        # 1. Construct the diagonal matrix S_y from the standard error vector s_y.\n        Sy = np.diag(sy)\n\n        # 2. Compute the covariance matrix of the outcome associations, Sigma_y.\n        # Sigma_y = S_y * R * S_y\n        Sigma_y = Sy @ R @ Sy\n        \n        # 3. Compute the weight matrix W, which is the inverse of Sigma_y.\n        # This is the precision matrix.\n        W = np.linalg.inv(Sigma_y)\n\n        # The MVMR estimator is the solution to the generalized least squares problem.\n        # beta_hat = (Ax^T * W * Ax)^-1 * Ax^T * W * by\n        \n        # 4. Compute the matrix C = Ax^T * W * Ax.\n        C = Ax.T @ W @ Ax\n        \n        # 5. Compute the vector d = Ax^T * W * by.\n        d = Ax.T @ W @ by\n        \n        # 6. Solve the linear system C * beta_hat = d for beta_hat.\n        # This is more numerically stable than computing the inverse of C directly.\n        beta_hat = np.linalg.solve(C, d)\n        \n        all_betas.extend(beta_hat)\n\n    # Format the results as specified: a flat list with numbers rounded to 6 decimal places.\n    formatted_results = [f\"{x:.6f}\" for x in all_betas]\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2377474"}]}