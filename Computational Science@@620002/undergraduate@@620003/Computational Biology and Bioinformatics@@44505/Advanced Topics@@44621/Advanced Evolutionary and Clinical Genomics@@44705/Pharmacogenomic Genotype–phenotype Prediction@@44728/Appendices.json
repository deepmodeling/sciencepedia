{"hands_on_practices": [{"introduction": "Our journey into building predictive models begins with a foundational task: classifying patient outcomes based on probabilistic reasoning. This exercise challenges you to construct a Naive Bayes classifier from first principles to distinguish between a \"null response\" and an \"adverse drug reaction\" [@problem_id:2413852]. By applying Bayes' theorem, you will learn to systematically combine probabilities from different sources of evidence—including discrete genetic markers and continuous clinical measurements—to arrive at the most likely outcome. This practice is essential for developing a deep intuition for probabilistic modeling, a core skill in bioinformatics.", "problem": "You are given a formalized pharmacogenomic genotype–phenotype classification task to distinguish between a null response to a drug and an adverse drug reaction (ADR), when both clinical outcomes result in treatment cessation. Consider a binary class variable $Y \\in \\{0,1\\}$, where $Y=0$ denotes a null response and $Y=1$ denotes an adverse drug reaction (ADR). For each individual, you observe a feature vector $X=(g_{\\mathrm{HLA}}, g_T, a)$ composed of the following components: a Human Leukocyte Antigen (HLA) risk allele indicator $g_{\\mathrm{HLA}} \\in \\{0,1\\}$, a transporter loss-of-function indicator $g_T \\in \\{0,1\\}$, and a normalized enzyme activity score $a \\in \\mathbb{R}_{\\ge 0}$. Assume the following generative model and probabilistic assumptions.\n\n1. Class prior probabilities: $P(Y=0)=\\tfrac{1}{2}$ and $P(Y=1)=\\tfrac{1}{2}$.\n\n2. Conditional independence of features given the class: for any $x=(g_{\\mathrm{HLA}}, g_T, a)$ and $y \\in \\{0,1\\}$,\n$$\np(x \\mid y) \\;=\\; p(g_{\\mathrm{HLA}} \\mid y)\\; p(g_T \\mid y)\\; f(a \\mid y).\n$$\n\n3. For the Human Leukocyte Antigen (HLA) risk allele indicator,\n$$\nP(g_{\\mathrm{HLA}}=1 \\mid Y=1) = 0.7,\\quad P(g_{\\mathrm{HLA}}=1 \\mid Y=0) = 0.05,\n$$\nand $P(g_{\\mathrm{HLA}}=0 \\mid y) = 1 - P(g_{\\mathrm{HLA}}=1 \\mid y)$ for $y \\in \\{0,1\\}$.\n\n4. For the transporter loss-of-function indicator,\n$$\nP(g_T=1 \\mid Y=0) = 0.6,\\quad P(g_T=1 \\mid Y=1) = 0.2,\n$$\nand $P(g_T=0 \\mid y) = 1 - P(g_T=1 \\mid y)$ for $y \\in \\{0,1\\}$.\n\n5. For the enzyme activity score, assume class-conditional Gaussian densities with parameters\n$$\na \\mid Y=0 \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2), \\quad \\mu_0 = 0.2,\\ \\sigma_0 = 0.1,\n$$\n$$\na \\mid Y=1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2), \\quad \\mu_1 = 1.0,\\ \\sigma_1 = 0.2,\n$$\nwith density\n$$\nf(a \\mid y) \\;=\\; \\frac{1}{\\sigma_y \\sqrt{2\\pi}} \\exp\\!\\left( -\\frac{(a - \\mu_y)^2}{2 \\sigma_y^2} \\right), \\quad y \\in \\{0,1\\}.\n$$\n\nYour task is to build a classifier that, for each observation $x=(g_{\\mathrm{HLA}}, g_T, a)$, selects the class $\\hat{y} \\in \\{0,1\\}$ that maximizes the posterior probability $P(Y=y \\mid X=x)$ under the model defined above. In the event of an exact tie, you must predict the adverse drug reaction (ADR) class, i.e., choose $\\hat{y}=1$.\n\nTest Suite. Use the following five observations, each written as an ordered triple $(g_{\\mathrm{HLA}}, g_T, a)$:\n1. $(1, 0, 0.9)$\n2. $(0, 1, 0.1)$\n3. $(0, 0, 0.5)$\n4. $(0, 1, 0.0)$\n5. $(1, 1, 0.05)$\n\nFinal Output Format. Your program should produce a single line of output containing the predicted classes for the five observations as a comma-separated list of integers enclosed in square brackets, in the same order as listed above. For example, the required format is $[c_1,c_2,c_3,c_4,c_5]$, where each $c_i \\in \\{0,1\\}$ is the predicted class for test case $i$. No units are involved, and no percentages are required. The output must be exactly one line, with no additional characters or text.", "solution": "The objective is to design a classifier that predicts the class $\\hat{y}$ maximizing the posterior probability $P(Y=y \\mid X=x)$. This is known as the Maximum A Posteriori (MAP) decision rule.\n$$\n\\hat{y} = \\arg\\max_{y \\in \\{0, 1\\}} P(Y=y \\mid X=x)\n$$\nAccording to Bayes' theorem, the posterior probability is given by:\n$$\nP(Y=y \\mid X=x) = \\frac{p(X=x \\mid Y=y) P(Y=y)}{p(X=x)}\n$$\nThe denominator, $p(X=x)$, is a marginal probability (also called evidence) that acts as a normalizing constant. Since it is independent of the class $y$, it does not affect the maximization. Therefore, the decision rule is equivalent to maximizing the joint probability:\n$$\n\\hat{y} = \\arg\\max_{y \\in \\{0, 1\\}} p(X=x \\mid Y=y) P(Y=y)\n$$\nThe problem specifies equal class priors, $P(Y=0) = P(Y=1) = \\tfrac{1}{2}$. Thus, the prior probability term also does not influence the maximization and can be omitted. The problem simplifies to maximizing the class-conditional likelihood $p(X=x \\mid Y=y)$:\n$$\n\\hat{y} = \\arg\\max_{y \\in \\{0, 1\\}} p(X=x \\mid Y=y)\n$$\nUsing the conditional independence assumption for the feature vector $x=(g_{\\mathrm{HLA}}, g_T, a)$, the likelihood is:\n$$\np(x \\mid y) = P(g_{\\mathrm{HLA}} \\mid Y=y) \\cdot P(g_T \\mid Y=y) \\cdot f(a \\mid Y=y)\n$$\nTo avoid numerical underflow resulting from the multiplication of small probabilities, and for convenience, it is standard practice to work with the logarithm of the likelihood. Since the logarithm is a strictly increasing function, maximizing a value is equivalent to maximizing its logarithm. Let $S_y$ be a score proportional to the log-posterior for class $y$.\n$$\n\\hat{y} = \\arg\\max_{y \\in \\{0, 1\\}} \\left( \\log P(g_{\\mathrm{HLA}} \\mid Y=y) + \\log P(g_T \\mid Y=y) + \\log f(a \\mid Y=y) \\right)\n$$\nThe log-density of the Gaussian distribution for the feature $a$ is:\n$$\n\\log f(a \\mid y) = \\log\\left(\\frac{1}{\\sigma_y \\sqrt{2\\pi}} \\exp\\left( -\\frac{(a - \\mu_y)^2}{2 \\sigma_y^2} \\right)\\right) = -\\log(\\sigma_y) - \\frac{1}{2}\\log(2\\pi) - \\frac{(a - \\mu_y)^2}{2 \\sigma_y^2}\n$$\nThe term $-\\tfrac{1}{2}\\log(2\\pi)$ is common to both classes and can be dropped from the comparison. The decision score for each class $y$ is thus:\n$$\nS_y = \\log P(g_{\\mathrm{HLA}} \\mid Y=y) + \\log P(g_T \\mid Y=y) - \\log(\\sigma_y) - \\frac{(a - \\mu_y)^2}{2 \\sigma_y^2}\n$$\nThe prediction $\\hat{y}$ is $1$ if $S_1 \\ge S_0$ (incorporating the tie-breaking rule), and $0$ otherwise.\n\nFirst, we must tabulate all necessary conditional probabilities from the given information.\nFor $g_{\\mathrm{HLA}}$:\n- $P(g_{\\mathrm{HLA}}=1 \\mid Y=0) = 0.05$, so $P(g_{\\mathrm{HLA}}=0 \\mid Y=0) = 1 - 0.05 = 0.95$.\n- $P(g_{\\mathrm{HLA}}=1 \\mid Y=1) = 0.7$, so $P(g_{\\mathrm{HLA}}=0 \\mid Y=1) = 1 - 0.7 = 0.3$.\nFor $g_T$:\n- $P(g_T=1 \\mid Y=0) = 0.6$, so $P(g_T=0 \\mid Y=0) = 1 - 0.6 = 0.4$.\n- $P(g_T=1 \\mid Y=1) = 0.2$, so $P(g_T=0 \\mid Y=1) = 1 - 0.2 = 0.8$.\nFor $a$:\n- Class $Y=0$: $\\mu_0 = 0.2$, $\\sigma_0 = 0.1$.\n- Class $Y=1$: $\\mu_1 = 1.0$, $\\sigma_1 = 0.2$.\n\nWe now apply this decision rule to each test case.\n\n1. Observation $(g_{\\mathrm{HLA}}, g_T, a) = (1, 0, 0.9)$:\n$S_0 = \\log(0.05) + \\log(0.4) - \\log(0.1) - \\frac{(0.9 - 0.2)^2}{2(0.1)^2} = -2.996 - 0.916 - (-2.303) - \\frac{0.49}{0.02} \\approx -1.609 - 24.5 = -26.109$\n$S_1 = \\log(0.7) + \\log(0.8) - \\log(0.2) - \\frac{(0.9 - 1.0)^2}{2(0.2)^2} = -0.357 - 0.223 - (-1.609) - \\frac{0.01}{0.08} \\approx 1.029 - 0.125 = 0.904$\nSince $S_1 > S_0$, the prediction is $\\hat{y}=1$.\n\n2. Observation $(g_{\\mathrm{HLA}}, g_T, a) = (0, 1, 0.1)$:\n$S_0 = \\log(0.95) + \\log(0.6) - \\log(0.1) - \\frac{(0.1 - 0.2)^2}{2(0.1)^2} = -0.051 - 0.511 - (-2.303) - \\frac{0.01}{0.02} \\approx 1.741 - 0.5 = 1.241$\n$S_1 = \\log(0.3) + \\log(0.2) - \\log(0.2) - \\frac{(0.1 - 1.0)^2}{2(0.2)^2} = -1.204 - \\frac{0.81}{0.08} \\approx -1.204 - 10.125 = -11.329$\nSince $S_0 > S_1$, the prediction is $\\hat{y}=0$.\n\n3. Observation $(g_{\\mathrm{HLA}}, g_T, a) = (0, 0, 0.5)$:\n$S_0 = \\log(0.95) + \\log(0.4) - \\log(0.1) - \\frac{(0.5 - 0.2)^2}{2(0.1)^2} = -0.051 - 0.916 - (-2.303) - \\frac{0.09}{0.02} \\approx 1.336 - 4.5 = -3.164$\n$S_1 = \\log(0.3) + \\log(0.8) - \\log(0.2) - \\frac{(0.5 - 1.0)^2}{2(0.2)^2} = -1.204 - 0.223 - (-1.609) - \\frac{0.25}{0.08} \\approx 0.182 - 3.125 = -2.943$\nSince $S_1 > S_0$, the prediction is $\\hat{y}=1$.\n\n4. Observation $(g_{\\mathrm{HLA}}, g_T, a) = (0, 1, 0.0)$:\n$S_0 = \\log(0.95) + \\log(0.6) - \\log(0.1) - \\frac{(0.0 - 0.2)^2}{2(0.1)^2} = 1.741 - \\frac{0.04}{0.02} = 1.741 - 2.0 = -0.259$\n$S_1 = \\log(0.3) + \\log(0.2) - \\log(0.2) - \\frac{(0.0 - 1.0)^2}{2(0.2)^2} = -1.204 - \\frac{1.0}{0.08} = -1.204 - 12.5 = -13.704$\nSince $S_0 > S_1$, the prediction is $\\hat{y}=0$.\n\n5. Observation $(g_{\\mathrm{HLA}}, g_T, a) = (1, 1, 0.05)$:\n$S_0 = \\log(0.05) + \\log(0.6) - \\log(0.1) - \\frac{(0.05 - 0.2)^2}{2(0.1)^2} = -2.996 - 0.511 - (-2.303) - \\frac{0.0225}{0.02} \\approx -1.204 - 1.125 = -2.329$\n$S_1 = \\log(0.7) + \\log(0.2) - \\log(0.2) - \\frac{(0.05 - 1.0)^2}{2(0.2)^2} = -0.357 - \\frac{0.9025}{0.08} \\approx -0.357 - 11.281 = -11.638$\nSince $S_0 > S_1$, the prediction is $\\hat{y}=0$.\n\nThe final sequence of predictions is $[1, 0, 1, 0, 0]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the pharmacogenomic genotype-phenotype classification task.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 0, 0.9),\n        (0, 1, 0.1),\n        (0, 0, 0.5),\n        (0, 1, 0.0),\n        (1, 1, 0.05),\n    ]\n\n    # Model parameters extracted from the problem statement.\n    \n    # P(g_HLA = g | Y = y)\n    # Stored as p_ghla[y][g]\n    p_ghla = {\n        0: {1: 0.05, 0: 0.95},\n        1: {1: 0.7, 0: 0.3}\n    }\n\n    # P(g_T = g | Y = y)\n    # Stored as p_gt[y][g]\n    p_gt = {\n        0: {1: 0.6, 0: 0.4},\n        1: {1: 0.2, 0: 0.8}\n    }\n\n    # Parameters for Gaussian density of a | Y = y\n    # Stored as params_a[y] = (mu, sigma)\n    params_a = {\n        0: (0.2, 0.1),  # mu_0, sigma_0\n        1: (1.0, 0.2)   # mu_1, sigma_1\n    }\n\n    def classify(g_hla: int, g_t: int, a: float) -> int:\n        \"\"\"\n        Classifies a single observation using the MAP decision rule.\n        The implementation uses log-probabilities to ensure numerical stability.\n        \n        The decision rule is to maximize the posterior P(Y=y|X=x).\n        This is equivalent to maximizing the log of the class-conditional likelihood,\n        since priors P(Y=y) are uniform.\n        \n        Score_y = log(P(g_HLA|Y=y)) + log(P(g_T|Y=y)) + log(f(a|Y=y))\n        \n        The log of the Gaussian PDF, ignoring constant terms common to both classes, is:\n        log(f(a|Y=y)) ~ -log(sigma_y) - (a - mu_y)^2 / (2 * sigma_y^2)\n        \"\"\"\n        \n        # Calculate score for class Y=0\n        mu0, sigma0 = params_a[0]\n        log_likelihood_g_0 = np.log(p_ghla[0][g_hla]) + np.log(p_gt[0][g_t])\n        log_pdf_a_0 = -np.log(sigma0) - ((a - mu0)**2) / (2 * sigma0**2)\n        score_0 = log_likelihood_g_0 + log_pdf_a_0\n        \n        # Calculate score for class Y=1\n        mu1, sigma1 = params_a[1]\n        log_likelihood_g_1 = np.log(p_ghla[1][g_hla]) + np.log(p_gt[1][g_t])\n        log_pdf_a_1 = -np.log(sigma1) - ((a - mu1)**2) / (2 * sigma1**2)\n        score_1 = log_likelihood_g_1 + log_pdf_a_1\n        \n        # Decision rule: predict Y=1 if score_1 is greater or equal (tie-breaking).\n        if score_1 >= score_0:\n            return 1\n        else:\n            return 0\n\n    results = []\n    for case in test_cases:\n        g_hla_val, g_t_val, a_val = case\n        prediction = classify(g_hla_val, g_t_val, a_val)\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2413852"}, {"introduction": "While many predictive models learn patterns from data, others are built by encoding expert knowledge into deterministic rule systems, which is common in clinical practice. This practice immerses you in one of the most classic and complex areas of pharmacogenomics: modeling the function of the CYP2D6 enzyme based on its intricate genetic architecture [@problem_id:2413841]. You will translate a set of clearly defined rules for star-alleles, copy number variations, and tandem hybrids into a computational model that predicts metabolic activity, ultimately connecting genotype to pharmacokinetic parameters. This hands-on task demonstrates how to systematize complex biological knowledge, a vital skill for building tools with direct clinical relevance.", "problem": "Implement a deterministic, unit-aware model that maps a Cytochrome P450 Family 2 Subfamily D Member 6 (CYP2D6) genotype with structural variation to the predicted codeine O-demethylation rate under fixed plasma concentration. The genotype is specified by two haplotypes. Each haplotype is a string composed from a star-allele token and optional structural modifiers under the following grammar: a simple star allele has the form “*k”, where “k” is a standard star-allele identifier among the set {“*1”, “*2”, “*4”, “*5”, “*10”, “*17”, “*36”, “*41”}; an integer copy number is denoted by suffix “xN” applying to the entire haplotype and indicates $N$ tandem copies on the same chromosome; a tandem hybrid haplotype is denoted “A+B”, which combines two star-allele tokens $A$ and $B$ on a single chromosome segment; copy number may also apply to a tandem hybrid as “(A+B)xN” or “A+BxN”. The per-copy intrinsic activity of a simple star allele $a$ is given by the mapping $\\alpha(a)$ with values $\\alpha(\\text{“*1”})=1.0$, $\\alpha(\\text{“*2”})=1.0$, $\\alpha(\\text{“*4”})=0.0$, $\\alpha(\\text{“*5”})=0.0$, $\\alpha(\\text{“*10”})=0.25$, $\\alpha(\\text{“*17”})=0.5$, $\\alpha(\\text{“*36”})=0.0$, and $\\alpha(\\text{“*41”})=0.5$. For a tandem hybrid “A+B”, the effective per-copy activity is defined as\n$$\n\\alpha_{\\mathrm{tandem}}(A,B) \\equiv p_t \\cdot \\frac{\\alpha(A) + \\alpha(B)}{2},\n$$\nwith a fixed tandem penalty factor $p_t=0.8$. For a haplotype with copy count $c \\in \\mathbb{Z}_{\\ge 1}$, the haplotype activity contribution is $c \\cdot \\alpha(a)$ for a simple allele $a$, or $c \\cdot \\alpha_{\\mathrm{tandem}}(A,B)$ for a tandem hybrid $A+B$. The total activity score is the sum of the two haplotype contributions:\n$$\nS \\equiv S(H_1,H_2) = \\mathrm{contrib}(H_1) + \\mathrm{contrib}(H_2).\n$$\nThe pharmacokinetic mapping from activity score to codeine O-demethylation rate follows Michaelis–Menten (MM) kinetics. Let Wild Type (WT) denote the reference diplotype with activity score $S_{\\mathrm{WT}}=2.0$. Define the maximum velocity $V_{\\max}(S)$ as\n$$\nV_{\\max}(S) \\equiv V_{\\max}^{\\mathrm{WT}} \\cdot \\frac{S}{S_{\\mathrm{WT}}},\n$$\nwith $V_{\\max}^{\\mathrm{WT}}=1.0$ in $\\mathrm{mg}\\,\\mathrm{L}^{-1}\\,\\mathrm{h}^{-1}$. Let the Michaelis constant be $K_m=0.2$ in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$, and let the codeine plasma concentration be $C$ in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$. The morphine formation rate is\n$$\nv(S,C) \\equiv V_{\\max}(S) \\cdot \\frac{C}{K_m + C},\n$$\nreported in $\\mathrm{mg}\\,\\mathrm{L}^{-1}\\,\\mathrm{h}^{-1}$. If $S=0$ or $C=0$, then by the above definition $v=0$.\n\nYour task is to write a program that, for each test case below, parses the two haplotype strings, computes $S$ from the specified rules, and then computes $v(S,C)$. Express the final results for all test cases as a single line containing a comma-separated list enclosed in square brackets, with each entry rounded to exactly $6$ decimal places and in the unit $\\mathrm{mg}\\,\\mathrm{L}^{-1}\\,\\mathrm{h}^{-1}$ (do not print units).\n\nTest suite (each test case is a triple of the form (haplotype$_1$, haplotype$_2$, $C$)):\n$1.$ (“*1”, “*1”, $0.5$)\n$2.$ (“*4”, “*5”, $0.5$)\n$3.$ (“*1x3”, “*1”, $2.0$)\n$4.$ (“*36+*10”, “*1”, $0.5$)\n$5.$ (“*1x2”, “*10”, $0.5$)\n$6.$ (“*17”, “*4”, $10.0$)\n$7.$ (“*1”, “*1x2”, $0.0$)\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[r1,r2,r3,r4,r5,r6,r7]”), where each $r_i$ is a float rounded to $6$ decimal places as specified above.", "solution": "The problem requires the implementation of a deterministic model to predict the rate of codeine O-demethylation to morphine, a metabolic process catalyzed by the CYP2D6 enzyme. The model is based on a specific pharmacogenomic genotype, defined by two haplotypes, and a fixed plasma concentration of codeine. The solution follows a two-part, rule-based approach: first, mapping the genotype to a numerical activity score, and second, mapping this score and the substrate concentration to a metabolic rate using classical enzyme kinetics.\n\nThe first step is to compute a total activity score, $S$, from the two given haplotype strings, $H_1$ and $H_2$. The total score is the sum of the individual contributions from each haplotype:\n$$\nS = \\mathrm{contrib}(H_1) + \\mathrm{contrib}(H_2)\n$$\nThe calculation of the contribution for a single haplotype, $\\mathrm{contrib}(H)$, depends on its structure, which is parsed from its string representation. A haplotype string specifies a base allele configuration and an optional copy number.\n\nFirst, the copy number, denoted by an integer $c$, is determined. If the string contains the suffix \"xN\", the copy number $c$ is $N$. If this suffix is absent, the copy number is taken to be $c=1$. The remainder of the string specifies the base allele configuration.\n\nThere are two types of base allele configurations:\n1.  **Simple Allele:** The base configuration is a single star-allele, such as “*1” or “*10”. Its contribution is calculated as $c \\cdot \\alpha(a)$, where $a$ is the star-allele identifier and $\\alpha(a)$ is its per-copy intrinsic activity. The problem provides the following activity mapping:\n    -   $\\alpha(\\text{“*1”}) = 1.0$\n    -   $\\alpha(\\text{“*2”}) = 1.0$\n    -   $\\alpha(\\text{“*4”}) = 0.0$\n    -   $\\alpha(\\text{“*5”}) = 0.0$\n    -   $\\alpha(\\text{“*10”}) = 0.25$\n    -   $\\alpha(\\text{“*17”}) = 0.5$\n    -   $\\alpha(\\text{“*36”}) = 0.0$\n    -   $\\alpha(\\text{“*41”}) = 0.5$\n\n2.  **Tandem Hybrid Allele:** The base configuration is a hybrid of two star-alleles, denoted as “A+B”. Its per-copy activity is defined by a specific formula that includes a penalty:\n    $$\n    \\alpha_{\\mathrm{tandem}}(A,B) = p_t \\cdot \\frac{\\alpha(A) + \\alpha(B)}{2}\n    $$\n    The tandem penalty factor is given as $p_t=0.8$. The total contribution for this haplotype is then $c \\cdot \\alpha_{\\mathrm{tandem}}(A,B)$.\n\nThe second step is to use the calculated total activity score $S$ to determine the morphine formation rate, $v$, based on the Michaelis-Menten model of enzyme kinetics. The maximum reaction velocity, $V_{\\max}$, is assumed to be directly proportional to the total enzyme activity, which is represented by the score $S$. This relationship is expressed as:\n$$\nV_{\\max}(S) = V_{\\max}^{\\mathrm{WT}} \\cdot \\frac{S}{S_{\\mathrm{WT}}}\n$$\nwhere $S_{\\mathrm{WT}}=2.0$ is the activity score of a reference wild-type (WT) diplotype (e.g., *1/*1), and $V_{\\max}^{\\mathrm{WT}}=1.0 \\, \\mathrm{mg}\\,\\mathrm{L}^{-1}\\,\\mathrm{h}^{-1}$ is the maximum velocity for this reference.\n\nThe rate of reaction $v$ is then given by the Michaelis-Menten equation, which relates the reaction velocity to the substrate concentration $C$:\n$$\nv(S,C) = V_{\\max}(S) \\cdot \\frac{C}{K_m + C}\n$$\nThe Michaelis constant is given as $K_m=0.2 \\, \\mathrm{mg}\\,\\mathrm{L}^{-1}$. A special condition is defined: if either the activity score $S=0$ or the codeine concentration $C=0$, the reaction rate $v$ is $0$.\n\nThe implementation consists of a function to parse an individual haplotype string to calculate its contribution, and a main procedure to process each test case. The parsing function first isolates the copy number and the base allele string. It then checks for the presence of a hybrid structure (“+”) to decide which activity formula to apply. The main procedure calls this parsing function for both haplotypes of a given diplotype, sums their contributions to find $S$, and then computes $v$ using the provided concentration $C$ and the pharmacokinetic equations. The final result for each test case is rounded to $6$ decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the codeine O-demethylation rate based on CYP2D6 genotype and plasma concentration.\n    \"\"\"\n\n    # --- Model Constants ---\n    ALPHA_MAP = {\n        \"*1\": 1.0, \"*2\": 1.0, \"*4\": 0.0, \"*5\": 0.0,\n        \"*10\": 0.25, \"*17\": 0.5, \"*36\": 0.0, \"*41\": 0.5\n    }\n    TANDEM_PENALTY = 0.8  # p_t\n    S_WT = 2.0\n    VMAX_WT = 1.0  # mg L^-1 h^-1\n    KM = 0.2  # mg L^-1\n\n    def _calculate_haplotype_contribution(hap_str: str) -> float:\n        \"\"\"\n        Parses a haplotype string and computes its activity score contribution.\n        \"\"\"\n        base_hap = hap_str\n        copy_count = 1\n\n        # Check for and extract copy number\n        if 'x' in hap_str:\n            parts = hap_str.split('x')\n            base_hap = parts[0]\n            copy_count = int(parts[1])\n\n        # Remove parentheses if present (e.g., from (A+B)xN)\n        base_hap = base_hap.strip('()')\n\n        # Check for tandem hybrid structure\n        if '+' in base_hap:\n            alleles = base_hap.split('+')\n            allele_a = alleles[0]\n            allele_b = alleles[1]\n            \n            activity_a = ALPHA_MAP.get(allele_a, 0.0)\n            activity_b = ALPHA_MAP.get(allele_b, 0.0)\n            \n            alpha_tandem = TANDEM_PENALTY * (activity_a + activity_b) / 2.0\n            per_copy_activity = alpha_tandem\n        else: # Simple allele\n            per_copy_activity = ALPHA_MAP.get(base_hap, 0.0)\n            \n        return copy_count * per_copy_activity\n\n    test_cases = [\n        (\"*1\", \"*1\", 0.5),\n        (\"*4\", \"*5\", 0.5),\n        (\"*1x3\", \"*1\", 2.0),\n        (\"*36+*10\", \"*1\", 0.5),\n        (\"*1x2\", \"*10\", 0.5),\n        (\"*17\", \"*4\", 10.0),\n        (\"*1\", \"*1x2\", 0.0)\n    ]\n\n    results = []\n    for hap1, hap2, C in test_cases:\n        contrib1 = _calculate_haplotype_contribution(hap1)\n        contrib2 = _calculate_haplotype_contribution(hap2)\n        \n        S = contrib1 + contrib2\n        \n        # Calculate metabolic rate v\n        if S == 0 or C == 0:\n            v = 0.0\n        else:\n            Vmax_S = VMAX_WT * (S / S_WT)\n            v = Vmax_S * (C / (KM + C))\n\n        results.append(f\"{v:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2413841"}, {"introduction": "A correct prediction is useful, but an explainable one is powerful, especially in medicine where the 'why' can be as important as the 'what'. This final practice focuses on the crucial area of model interpretability, tasking you with not only predicting an optimal drug dose but also explaining the reasoning behind it [@problem_id:2413806]. After training a linear model, you will use principles from cooperative game theory, such as SHapley Additive exPlanations (SHAP), to quantify the exact contribution of each genetic and clinical feature to a patient's predicted dose. This exercise will equip you with the tools to build transparent and trustworthy models, a critical step in translating machine learning from the lab to the clinic.", "problem": "You will design and implement an interpretable pharmacogenomic genotype–phenotype prediction model that explains dose recommendations for pairs of patients who have similar genotypes but differ in clinical covariates. The scenario is grounded in the widely studied influence of cytochrome P450 and vitamin K epoxide reductase complex genes on warfarin dosing, using a simple linear model trained by Ordinary Least Squares (OLS) and interpreted with SHapley Additive exPlanations (SHAP) or an equivalent additive, cooperative game-theoretic decomposition that satisfies the standard axioms. Your program must be a complete, runnable program that produces the final outputs exactly as specified.\n\nFoundational base and modeling assumptions:\n- Pharmacogenomic genotype affects drug metabolism and drug target sensitivity (Central Dogma: DNA to RNA to protein), leading to dose adjustments. We use the well-tested practice of modeling dose as a linear function of genotype features and clinical covariates when effects are approximately additive for small perturbations.\n- We will train a linear regression model by minimizing the sum of squared residuals (Ordinary Least Squares), a standard and well-tested method for parameter estimation in linear models.\n- We require an interpretable, local explanation of each prediction using SHapley Additive exPlanations (SHAP) or an equivalent cooperative game-theoretic additive attribution method that satisfies the standard axioms (efficiency, symmetry, dummy, and additivity) with a background dataset defined by the empirical feature distribution of the training data.\n\nData specification:\n- Features are ordered and indexed as follows:\n  - Index $0$: CYP2C9 activity score (values in $\\{0,1,2\\}$).\n  - Index $1$: VKORC1 $-1639$ A-count (values in $\\{0,1,2\\}$).\n  - Index $2$: Age in decades (for example, age $50$ years is encoded as $5.0$).\n  - Index $3$: Weight in tens of kilograms (for example, weight $70$ kilograms is encoded as $7.0$).\n\n- Training design matrix $X_{\\text{train}} \\in \\mathbb{R}^{6 \\times 4}$ and target vector $y_{\\text{train}} \\in \\mathbb{R}^{6}$ (stable warfarin dose in mg/day) are given by:\n$$\nX_{\\text{train}}=\n\\begin{bmatrix}\n2 & 0 & 5.0 & 7.0\\\\\n1 & 1 & 6.0 & 6.5\\\\\n2 & 2 & 4.0 & 8.0\\\\\n0 & 1 & 7.0 & 5.0\\\\\n1 & 0 & 3.0 & 9.0\\\\\n2 & 1 & 5.5 & 7.5\n\\end{bmatrix},\n\\quad\ny_{\\text{train}}=\n\\begin{bmatrix}\n6.3\\\\\n5.55\\\\\n5.2\\\\\n5.2\\\\\n8.3\\\\\n5.55\n\\end{bmatrix}.\n$$\n\nTask requirements:\n1) Fit a linear regression model by minimizing the sum of squared errors on $(X_{\\text{train}}, y_{\\text{train}})$, including an intercept term. Denote the fitted model as $f(\\mathbf{x})=\\hat{b}+\\sum_{i=0}^{3}\\hat{w}_i x_i$.\n\n2) Define the background dataset for explanations as the empirical distribution of $X_{\\text{train}}$. Using SHapley Additive exPlanations (SHAP) or an equivalent cooperative game-theoretic additive decomposition that satisfies the standard axioms and uses the specified background dataset, decompose each prediction into a baseline term and additive feature attributions. Explicitly, for any input $\\mathbf{x}$, compute an additive decomposition $f(\\mathbf{x})=\\phi_0+\\sum_{i=0}^{3}\\phi_i(\\mathbf{x})$, where $\\phi_0$ is the baseline prediction associated with the background and $\\phi_i(\\mathbf{x})$ is the contribution of feature $i$. You must implement an attribution method consistent with the SHAP axioms for a linear model and the stated background.\n\n3) For each test case below, compute:\n   - The predicted dose for patient A, $f(\\mathbf{x}^{(A)})$, in mg/day.\n   - The predicted dose for patient B, $f(\\mathbf{x}^{(B)})$, in mg/day.\n   - The difference $f(\\mathbf{x}^{(B)})-f(\\mathbf{x}^{(A)})$, in mg/day.\n   - The feature index $j \\in \\{0,1,2,3\\}$ that maximizes the absolute difference in attributions between B and A, i.e., $j=\\arg\\max_{i} \\left|\\phi_i(\\mathbf{x}^{(B)})-\\phi_i(\\mathbf{x}^{(A)})\\right|$. In the event of a tie, return the smallest such index.\n   - The signed contribution difference for that feature, $\\phi_j(\\mathbf{x}^{(B)})-\\phi_j(\\mathbf{x}^{(A)})$, in mg/day.\n\nTest suite:\n- Case $1$ (happy path: identical genotypes, different weight):\n  - Patient A: $\\mathbf{x}^{(A)}=[2,\\,1,\\,6.0,\\,6.0]$\n  - Patient B: $\\mathbf{x}^{(B)}=[2,\\,1,\\,6.0,\\,8.0]$\n- Case $2$ (genotype difference at CYP2C9, same covariates):\n  - Patient A: $\\mathbf{x}^{(A)}=[1,\\,1,\\,5.0,\\,7.0]$\n  - Patient B: $\\mathbf{x}^{(B)}=[2,\\,1,\\,5.0,\\,7.0]$\n- Case $3$ (boundary: identical patients):\n  - Patient A: $\\mathbf{x}^{(A)}=[2,\\,1,\\,5.5,\\,7.5]$\n  - Patient B: $\\mathbf{x}^{(B)}=[2,\\,1,\\,5.5,\\,7.5]$\n\nUnits and rounding:\n- Express all predicted doses and contribution values in mg/day, rounded to three decimal places. The feature index is an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and itself is a list: $\\left[f(\\mathbf{x}^{(A)}), f(\\mathbf{x}^{(B)}), f(\\mathbf{x}^{(B)})-f(\\mathbf{x}^{(A)}), j, \\phi_j(\\mathbf{x}^{(B)})-\\phi_j(\\mathbf{x}^{(A)})\\right]$.\n- The final output must therefore be a single line of the form:\n  - For example, using placeholders: $[[a_1,b_1,d_1,j_1,c_1],[a_2,b_2,d_2,j_2,c_2],[a_3,b_3,d_3,j_3,c_3]]$\n- No spaces are permitted in the output line. Only the line with the bracketed list must be printed.", "solution": "The objective is to construct a linear regression model for warfarin dose prediction and to explain the differences in predictions for pairs of patients using a game-theoretic attribution method equivalent to SHapley Additive exPlanations (SHAP).\n\n**Step 1: Linear Regression Model Fitting**\n\nThe model to be fitted is a linear function of $p=4$ features, including an intercept term:\n$$f(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_{i-1} = \\beta_0 + \\beta_1 x_0 + \\beta_2 x_1 + \\beta_3 x_2 + \\beta_4 x_3$$\nwhere $\\mathbf{x} = [x_0, x_1, x_2, x_3]^T$ represents the feature vector. The coefficients $\\mathbf{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4]^T$ are estimated by Ordinary Least Squares (OLS), which minimizes the sum of squared residuals.\n\nWe are given the training data $X_{\\text{train}} \\in \\mathbb{R}^{6 \\times 4}$ and $y_{\\text{train}} \\in \\mathbb{R}^{6}$. To accommodate the intercept $\\beta_0$, we augment the feature matrix $X_{\\text{train}}$ with a leading column of ones, creating the design matrix $X' \\in \\mathbb{R}^{6 \\times 5}$.\n$$\nX' =\n\\begin{bmatrix}\n1 & 2 & 0 & 5.0 & 7.0\\\\\n1 & 1 & 1 & 6.0 & 6.5\\\\\n1 & 2 & 2 & 4.0 & 8.0\\\\\n1 & 0 & 1 & 7.0 & 5.0\\\\\n1 & 1 & 0 & 3.0 & 9.0\\\\\n1 & 2 & 1 & 5.5 & 7.5\n\\end{bmatrix},\n\\quad\ny_{\\text{train}}=\n\\begin{bmatrix}\n6.3\\\\\n5.55\\\\\n5.2\\\\\n5.2\\\\\n8.3\\\\\n5.55\n\\end{bmatrix}\n$$\nThe OLS estimate $\\hat{\\mathbf{\\beta}}$ is the solution to the normal equations:\n$$(X'^T X') \\hat{\\mathbf{\\beta}} = X'^T y_{\\text{train}}$$\nAssuming $X'^T X'$ is invertible, the solution is given by:\n$$\\hat{\\mathbf{\\beta}} = (X'^T X')^{-1} X'^T y_{\\text{train}}$$\nSolving this linear system yields the following coefficient vector:\n$$\\hat{\\mathbf{\\beta}} \\approx [9.92349, -1.25301, -0.65000, -0.44157, 0.11747]^T$$\nSo, the intercept is $\\hat{b} = \\hat{\\beta}_0 \\approx 9.923$ and the feature weights are $\\hat{\\mathbf{w}} = [\\hat{w}_0, \\hat{w}_1, \\hat{w}_2, \\hat{w}_3]^T \\approx [-1.253, -0.650, -0.442, 0.117]^T$.\nThe fitted model is:\n$$f(\\mathbf{x}) \\approx 9.923 - 1.253 x_0 - 0.650 x_1 - 0.442 x_2 + 0.117 x_3$$\n\n**Step 2: Attribution Method for Prediction Explanation**\n\nThe problem requires an additive feature attribution method consistent with the SHAP axioms. For a linear model $f(\\mathbf{x}) = \\hat{b} + \\sum_{i=0}^{3} \\hat{w}_i x_i$, the SHAP value for feature $i$ at input $\\mathbf{x}$ is given by:\n$$\\phi_i(\\mathbf{x}) = \\hat{w}_i (x_i - E[X_i])$$\nwhere $E[X_i]$ is the expected value of feature $i$ over the background data distribution, which is defined as the empirical distribution of $X_{\\text{train}}$. Thus, $E[X_i]$ is the mean of the $i$-th column of $X_{\\text{train}}$. The base value $\\phi_0$ is the model's expected output over the background:\n$$\\phi_0 = E[f(\\mathbf{X})] = \\hat{b} + \\sum_{i=0}^{3} \\hat{w}_i E[X_i]$$\nThe decomposition $f(\\mathbf{x}) = \\phi_0 + \\sum_{i=0}^{3} \\phi_i(\\mathbf{x})$ satisfies the efficiency axiom.\n\nThe task is to find the feature that best explains the *difference* in predicted dose between two patients, A and B. The difference in attribution for feature $i$ is:\n$$\\Delta\\phi_i = \\phi_i(\\mathbf{x}^{(B)}) - \\phi_i(\\mathbf{x}^{(A)})$$\nSubstituting the formula for $\\phi_i$:\n$$\\Delta\\phi_i = \\left[\\hat{w}_i (x_i^{(B)} - E[X_i])\\right] - \\left[\\hat{w}_i (x_i^{(A)} - E[X_i])\\right]$$\nThis simplifies to:\n$$\\Delta\\phi_i = \\hat{w}_i (x_i^{(B)} - x_i^{(A)})$$\nThis elegant result shows that the change in a feature's contribution is simply its weight multiplied by the change in its value. This allows us to compute the required quantities without explicitly calculating the mean feature values from the training data. The feature $j$ that explains the largest portion of the prediction difference is found by:\n$$j = \\arg\\max_{i \\in \\{0,1,2,3\\}} |\\Delta\\phi_i| = \\arg\\max_{i \\in \\{0,1,2,3\\}} |\\hat{w}_i (x_i^{(B)} - x_i^{(A)})|$$\nTies are broken by selecting the smallest index $j$.\n\n**Step 3: Test Case Evaluation**\n\nWe now apply these formulas to the three specified test cases. All results are rounded to three decimal places.\n\n**Case 1:**\n- Patient A: $\\mathbf{x}^{(A)} = [2, 1, 6.0, 6.0]$\n- Patient B: $\\mathbf{x}^{(B)} = [2, 1, 6.0, 8.0]$\n- Predicted dose for A: $f(\\mathbf{x}^{(A)}) \\approx 9.923 - 1.253(2) - 0.650(1) - 0.442(6.0) + 0.117(6.0) \\approx 4.817$\n- Predicted dose for B: $f(\\mathbf{x}^{(B)}) \\approx 9.923 - 1.253(2) - 0.650(1) - 0.442(6.0) + 0.117(8.0) \\approx 5.052$\n- Dose difference: $f(\\mathbf{x}^{(B)}) - f(\\mathbf{x}^{(A)}) \\approx 5.052 - 4.817 = 0.235$\n- Attribution differences:\n  - $\\Delta\\phi_0 = \\hat{w}_0(2 - 2) = 0$\n  - $\\Delta\\phi_1 = \\hat{w}_1(1 - 1) = 0$\n  - $\\Delta\\phi_2 = \\hat{w}_2(6.0 - 6.0) = 0$\n  - $\\Delta\\phi_3 = \\hat{w}_3(8.0 - 6.0) = 2 \\hat{w}_3 \\approx 0.235$\n- Feature with max attribution difference: The only non-zero difference is for feature $3$. Thus, $j=3$.\n- Signed contribution difference for $j=3$: $\\Delta\\phi_3 \\approx 0.235$\n- Result: $[4.817, 5.052, 0.235, 3, 0.235]$\n\n**Case 2:**\n- Patient A: $\\mathbf{x}^{(A)} = [1, 1, 5.0, 7.0]$\n- Patient B: $\\mathbf{x}^{(B)} = [2, 1, 5.0, 7.0]$\n- Predicted dose for A: $f(\\mathbf{x}^{(A)}) \\approx 9.923 - 1.253(1) - 0.650(1) - 0.442(5.0) + 0.117(7.0) \\approx 6.633$\n- Predicted dose for B: $f(\\mathbf{x}^{(B)}) \\approx 9.923 - 1.253(2) - 0.650(1) - 0.442(5.0) + 0.117(7.0) \\approx 5.380$\n- Dose difference: $f(\\mathbf{x}^{(B)}) - f(\\mathbf{x}^{(A)}) \\approx 5.380 - 6.633 = -1.253$\n- Attribution differences:\n  - $\\Delta\\phi_0 = \\hat{w}_0(2 - 1) = \\hat{w}_0 \\approx -1.253$\n  - $\\Delta\\phi_1 = \\hat{w}_1(1 - 1) = 0$\n  - $\\Delta\\phi_2 = \\hat{w}_2(5.0 - 5.0) = 0$\n  - $\\Delta\\phi_3 = \\hat{w}_3(7.0 - 7.0) = 0$\n- Feature with max attribution difference: The only non-zero difference is for feature $0$. Thus, $j=0$.\n- Signed contribution difference for $j=0$: $\\Delta\\phi_0 \\approx -1.253$\n- Result: $[6.633, 5.380, -1.253, 0, -1.253]$\n\n**Case 3:**\n- Patient A: $\\mathbf{x}^{(A)} = [2, 1, 5.5, 7.5]$\n- Patient B: $\\mathbf{x}^{(B)} = [2, 1, 5.5, 7.5]$\n- Patients are identical, so $f(\\mathbf{x}^{(A)}) = f(\\mathbf{x}^{(B)})$.\n- Predicted dose: $f(\\mathbf{x}^{(A)}) \\approx 9.923 - 1.253(2) - 0.650(1) - 0.442(5.5) + 0.117(7.5) \\approx 5.217$\n- Dose difference: $f(\\mathbf{x}^{(B)}) - f(\\mathbf{x}^{(A)}) = 0.000$\n- Attribution differences: Since $\\mathbf{x}^{(A)} = \\mathbf{x}^{(B)}$, all $x_i^{(B)} - x_i^{(A)} = 0$. Therefore, $\\Delta\\phi_i = 0$ for all $i \\in \\{0, 1, 2, 3\\}$.\n- Feature with max attribution difference: All absolute differences are $0$. The tie-breaking rule states to choose the smallest index. Thus, $j=0$.\n- Signed contribution difference for $j=0$: $\\Delta\\phi_0 = 0.000$\n- Result: $[5.217, 5.217, 0.000, 0, 0.000]$\n\nThese calculations provide the basis for the final program implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the pharmacogenomic prediction and explanation problem.\n\n    The solution involves three main steps:\n    1. Fit a linear regression model using Ordinary Least Squares (OLS) on the\n       provided training data.\n    2. For each test case, calculate the predicted warfarin dose for two patients,\n       A and B.\n    3. Calculate the difference in predictions and explain it by finding the\n       feature that contributes most to this difference, based on a SHAP-like\n       additive attribution method for linear models.\n    \"\"\"\n    \n    # 1. Define the training data from the problem statement.\n    X_train = np.array([\n        [2.0, 0.0, 5.0, 7.0],\n        [1.0, 1.0, 6.0, 6.5],\n        [2.0, 2.0, 4.0, 8.0],\n        [0.0, 1.0, 7.0, 5.0],\n        [1.0, 0.0, 3.0, 9.0],\n        [2.0, 1.0, 5.5, 7.5]\n    ])\n    y_train = np.array([6.3, 5.55, 5.2, 5.2, 8.3, 5.55])\n    \n    # 2. Fit a linear regression model.\n    # Augment the training data matrix with a column of ones for the intercept.\n    X_prime = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n    \n    # Solve the normal equations: (X'T * X') * beta = X'T * y\n    # This is numerically more stable than calculating the inverse directly.\n    try:\n        beta_hat = np.linalg.solve(X_prime.T @ X_prime, X_prime.T @ y_train)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse if matrix is singular, though not expected here.\n        beta_hat = np.linalg.pinv(X_prime) @ y_train\n\n    intercept = beta_hat[0]\n    weights = beta_hat[1:]\n\n    # 3. Define the test cases.\n    test_cases = [\n        # Case 1 (happy path: identical genotypes, different weight)\n        (np.array([2.0, 1.0, 6.0, 6.0]), np.array([2.0, 1.0, 6.0, 8.0])),\n        # Case 2 (genotype difference at CYP2C9, same covariates)\n        (np.array([1.0, 1.0, 5.0, 7.0]), np.array([2.0, 1.0, 5.0, 7.0])),\n        # Case 3 (boundary: identical patients)\n        (np.array([2.0, 1.0, 5.5, 7.5]), np.array([2.0, 1.0, 5.5, 7.5]))\n    ]\n    \n    all_results = []\n\n    # 4. Process each test case.\n    for x_a, x_b in test_cases:\n        # a. Compute predicted doses for patient A and B.\n        # f(x) = intercept + sum(weights * features)\n        pred_a = intercept + weights.T @ x_a\n        pred_b = intercept + weights.T @ x_b\n\n        # b. Compute the difference in predictions.\n        pred_diff = pred_b - pred_a\n\n        # c. Compute the feature attribution differences.\n        # For a linear model, the SHAP-based attribution difference for feature i\n        # is simply weight_i * (x_b_i - x_a_i).\n        delta_phis = weights * (x_b - x_a)\n\n        # d. Find the feature index 'j' that maximizes the absolute attribution difference.\n        # np.argmax returns the first index in case of a tie, which satisfies the\n        # requirement to return the smallest such index.\n        j = np.argmax(np.abs(delta_phis))\n        \n        # e. Get the signed contribution difference for that feature.\n        contrib_diff = delta_phis[j]\n\n        # Store the 5 required values for the current test case.\n        case_result = [pred_a, pred_b, pred_diff, j, contrib_diff]\n        all_results.append(case_result)\n\n    # 5. Format the output string exactly as specified.\n    # Example format: [[a1,b1,d1,j1,c1],[a2,b2,d2,j2,c2],...]\n    # Floats must be rounded to three decimal places.\n    case_strings = []\n    for result in all_results:\n        pred_a_str = f\"{result[0]:.3f}\"\n        pred_b_str = f\"{result[1]:.3f}\"\n        pred_diff_str = f\"{result[2]:.3f}\"\n        j_str = str(result[3]) # Index is an integer\n        contrib_diff_str = f\"{result[4]:.3f}\"\n        \n        formatted_case = (\n            f\"[{pred_a_str},\"\n            f\"{pred_b_str},\"\n            f\"{pred_diff_str},\"\n            f\"{j_str},\"\n            f\"{contrib_diff_str}]\"\n        )\n        case_strings.append(formatted_case)\n\n    # Join all case strings with a comma and enclose in brackets.\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement must produce only the specified single-line format.\n    print(final_output)\n\nsolve()\n```", "id": "2413806"}]}