{"hands_on_practices": [{"introduction": "The concept of orthogonality extends the familiar geometric idea of \"perpendicularity\" to abstract vector spaces, including those with complex numbers. The primary tool for this is the inner product; two vectors are defined as orthogonal if their inner product is zero. This exercise [@problem_id:1374314] provides a direct, hands-on application of this fundamental definition by tasking you with finding the specific condition that makes two state vectors mutually orthogonal. Mastering this skill is the first step toward constructing and utilizing orthogonal basis sets, which simplify complex calculations in countless engineering and physics applications.", "problem": "Consider a three-level quantum system whose state space is spanned by a set of three orthonormal basis vectors, denoted as $\\{|e_1\\rangle, |e_2\\rangle, |e_3\\rangle\\}$. The orthonormality condition is given by the inner product $\\langle e_i | e_j \\rangle = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nTwo states of this system, $|\\psi\\rangle$ and $|\\phi(\\alpha)\\rangle$, are defined as follows:\n$$ |\\psi\\rangle = i|e_1\\rangle + |e_2\\rangle + |e_3\\rangle $$\n$$ |\\phi(\\alpha)\\rangle = 3|e_1\\rangle + i\\alpha|e_2\\rangle + 5i|e_3\\rangle $$\nHere, $i$ is the imaginary unit ($i^2 = -1$) and $\\alpha$ is an unknown real-valued parameter.\n\nDetermine the specific numerical value of $\\alpha$ that makes the state $|\\psi\\rangle$ orthogonal to the state $|\\phi(\\alpha)\\rangle$.", "solution": "We seek the value of the real parameter $\\alpha$ such that the states $|\\psi\\rangle$ and $|\\phi(\\alpha)\\rangle$ are orthogonal. Orthogonality requires the inner product to vanish:\n$$\n\\langle \\psi | \\phi(\\alpha) \\rangle = 0.\n$$\nGiven the orthonormal basis $\\{|e_{1}\\rangle, |e_{2}\\rangle, |e_{3}\\rangle\\}$ with $\\langle e_{i} | e_{j} \\rangle = \\delta_{ij}$, and the states\n$$\n|\\psi\\rangle = i|e_{1}\\rangle + |e_{2}\\rangle + |e_{3}\\rangle, \\quad |\\phi(\\alpha)\\rangle = 3|e_{1}\\rangle + i\\alpha|e_{2}\\rangle + 5i|e_{3}\\rangle,\n$$\nthe bra corresponding to $|\\psi\\rangle$ is obtained by conjugating the coefficients:\n$$\n\\langle \\psi | = (-i)\\langle e_{1}| + \\langle e_{2}| + \\langle e_{3}|.\n$$\nCompute the inner product using linearity and orthonormality:\n$$\n\\langle \\psi | \\phi(\\alpha) \\rangle = (-i)\\cdot 3 \\langle e_{1}|e_{1}\\rangle + 1 \\cdot i\\alpha \\langle e_{2}|e_{2}\\rangle + 1 \\cdot 5i \\langle e_{3}|e_{3}\\rangle.\n$$\nSince $\\langle e_{j}|e_{j}\\rangle = 1$ and cross terms vanish, this simplifies to\n$$\n\\langle \\psi | \\phi(\\alpha) \\rangle = -3i + i\\alpha + 5i = i(\\alpha + 2).\n$$\nOrthogonality requires\n$$\ni(\\alpha + 2) = 0 \\quad \\Rightarrow \\quad \\alpha + 2 = 0 \\quad \\Rightarrow \\quad \\alpha = -2.\n$$", "answer": "$$\\boxed{-2}$$", "id": "1374314"}, {"introduction": "In many engineering scenarios, we need to find the best approximation of a vector within a given subspace. The answer lies in orthogonal projection, which finds the \"shadow\" of a vector onto a subspace, effectively decomposing it into a component within that subspace and a component orthogonal to it. This problem [@problem_id:2403749] translates this abstract concept into a practical structural analysis task, where you will project a force vector onto a subspace defined by characteristic mode shapes. This procedure is the mathematical engine behind least-squares approximation, a method central to data fitting, signal processing, and numerical solutions of differential equations.", "problem": "In a computational structural model with $4$ degrees of freedom (degrees of freedom (DOF)), consider the standard Euclidean space $\\mathbb{R}^{4}$ equipped with the standard Euclidean inner product $\\langle x,y\\rangle = x^{\\mathsf{T}} y$. Let the force vector be $v = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{pmatrix}$ and consider the two mode shapes $u_{1} = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix}$ and $u_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix}$. Determine the orthogonal projection of $v$ onto the subspace $S \\subset \\mathbb{R}^{4}$ spanned by $u_{1}$ and $u_{2}$ with respect to the standard Euclidean inner product.\n\nProvide your answer as the projected vector written as a column vector with exact rational entries. No rounding is required.", "solution": "The objective is to find the orthogonal projection of the vector $v$ onto the subspace $S = \\text{span}\\{u_1, u_2\\}$. Let this projection be denoted by $p$. By definition, the projection $p$ is the unique vector in $S$ such that the error vector, $e = v - p$, is orthogonal to the subspace $S$. This orthogonality condition is satisfied if $e$ is orthogonal to every vector in a basis for $S$. Therefore, we must have:\n$$\n\\langle v - p, u_1 \\rangle = 0\n$$\n$$\n\\langle v - p, u_2 \\rangle = 0\n$$\nSince $p$ is in $S$, it can be expressed as a linear combination of the basis vectors $u_1$ and $u_2$:\n$$\np = c_1 u_1 + c_2 u_2\n$$\nfor some scalar coefficients $c_1$ and $c_2$. Substituting this expression for $p$ into the orthogonality conditions gives:\n$$\n\\langle v - (c_1 u_1 + c_2 u_2), u_1 \\rangle = \\langle v, u_1 \\rangle - c_1 \\langle u_1, u_1 \\rangle - c_2 \\langle u_2, u_1 \\rangle = 0\n$$\n$$\n\\langle v - (c_1 u_1 + c_2 u_2), u_2 \\rangle = \\langle v, u_2 \\rangle - c_1 \\langle u_1, u_2 \\rangle - c_2 \\langle u_2, u_2 \\rangle = 0\n$$\nThis forms a system of linear equations for the coefficients $c_1$ and $c_2$, known as the normal equations:\n$$\n\\begin{pmatrix} \\langle u_1, u_1 \\rangle & \\langle u_1, u_2 \\rangle \\\\ \\langle u_2, u_1 \\rangle & \\langle u_2, u_2 \\rangle \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} \\langle v, u_1 \\rangle \\\\ \\langle v, u_2 \\rangle \\end{pmatrix}\n$$\nWe now compute the necessary inner products using the given vectors:\n$v = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{pmatrix}$, $u_{1} = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix}$, $u_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix}$.\n\nThe inner products for the Gram matrix on the left side are:\n$$\n\\langle u_1, u_1 \\rangle = 1^2 + 1^2 + 0^2 + 0^2 = 2\n$$\n$$\n\\langle u_2, u_2 \\rangle = 0^2 + 1^2 + 1^2 + 0^2 = 2\n$$\n$$\n\\langle u_1, u_2 \\rangle = \\langle u_2, u_1 \\rangle = (1)(0) + (1)(1) + (0)(1) + (0)(0) = 1\n$$\nThe inner products for the vector on the right side are:\n$$\n\\langle v, u_1 \\rangle = (1)(1) + (2)(1) + (3)(0) + (4)(0) = 3\n$$\n$$\n\\langle v, u_2 \\rangle = (1)(0) + (2)(1) + (3)(1) + (4)(0) = 5\n$$\nSubstituting these values, the normal equations become:\n$$\n\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}\n$$\nWe solve this system for $c_1$ and $c_2$. The determinant of the $2 \\times 2$ matrix is $(2)(2) - (1)(1) = 3$. The inverse of the matrix is:\n$$\n\\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\nSolving for the coefficients:\n$$\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} (2)(3) + (-1)(5) \\\\ (-1)(3) + (2)(5) \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 6 - 5 \\\\ -3 + 10 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}\n$$\nThus, the coefficients are $c_1 = \\frac{1}{3}$ and $c_2 = \\frac{7}{3}$.\n\nFinally, the orthogonal projection $p$ is computed by substituting these coefficients back into the expression for $p$:\n$$\np = c_1 u_1 + c_2 u_2 = \\frac{1}{3} u_1 + \\frac{7}{3} u_2\n$$\n$$\np = \\frac{1}{3} \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix} + \\frac{7}{3} \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{7}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} + \\frac{7}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{8}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix}\n$$\nTo verify the correctness of the result, one can check that the error vector $e = v - p$ is orthogonal to both $u_1$ and $u_2$.\n$$\ne = v - p = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{pmatrix} - \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{8}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{2}{3} \\\\ \\frac{2}{3} \\\\ 4 \\end{pmatrix}\n$$\n$$\n\\langle e, u_1 \\rangle = (\\frac{2}{3})(1) + (-\\frac{2}{3})(1) + (\\frac{2}{3})(0) + (4)(0) = \\frac{2}{3} - \\frac{2}{3} = 0\n$$\n$$\n\\langle e, u_2 \\rangle = (\\frac{2}{3})(0) + (-\\frac{2}{3})(1) + (\\frac{2}{3})(1) + (4)(0) = -\\frac{2}{3} + \\frac{2}{3} = 0\n$$\nThe orthogonality conditions are satisfied. The result is correct.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{8}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix}}$$", "id": "2403749"}, {"introduction": "Computational engineering constantly navigates the interface between continuous mathematical models and their discrete representations on a computer. A crucial question arises: do fundamental properties like orthogonality survive this transition? This exercise [@problem_id:2179886] delves into this critical issue by comparing a continuous inner product, defined by an integral, with its discrete approximation, a finite sum. You will quantify the degree to which orthogonality is compromised in a specific numerical case, providing a tangible understanding of discretization error and why seemingly \"perfect\" mathematical relationships require careful handling in numerical implementations.", "problem": "In numerical analysis, continuous mathematical constructs are often replaced by discrete approximations. This problem explores how such a replacement can affect fundamental properties like orthogonality.\n\nConsider the vector space of real-valued functions on the interval $[-1, 1]$. We define a continuous inner product, denoted by $\\langle \\cdot, \\cdot \\rangle_C$, as:\n$$\n\\langle u, v \\rangle_C = \\int_{-1}^{1} u(x)v(x) \\, dx\n$$\nWith respect to this inner product, the functions $f(x) = x$ and $g(x) = \\frac{3}{2}x^2 - \\frac{1}{2}$ are orthogonal.\n\nNow, let's approximate this inner product with a discrete sum. We partition the interval $[-1, 1]$ into $N$ subintervals of equal width $\\Delta x = 2/N$. We use the composite left-hand rule, sampling the functions at the left endpoint of each subinterval. The sample points are given by $x_k = -1 + k\\Delta x$ for $k = 0, 1, \\dots, N-1$. This defines a discrete inner product, denoted by $\\langle \\cdot, \\cdot \\rangle_D$, as:\n$$\n\\langle u, v \\rangle_D = \\sum_{k=0}^{N-1} u(x_k)v(x_k) \\Delta x\n$$\nThe \"angle\" $\\theta_D$ between two functions $u$ and $v$ in this discrete setting is defined via the relation $\\cos(\\theta_D) = \\frac{\\langle u, v \\rangle_D}{\\|u\\|_D \\|v\\|_D}$, where the discrete norm is given by $\\|u\\|_D = \\sqrt{\\langle u, u \\rangle_D}$.\n\nThe discrete approximation does not necessarily preserve the orthogonality observed in the continuous case. Your task is to quantify this discrepancy for a specific case.\n\nCalculate the value of $\\cos^2(\\theta_D)$ for the functions $f(x) = x$ and $g(x) = \\frac{3}{2}x^2 - \\frac{1}{2}$ using the discrete inner product $\\langle \\cdot, \\cdot \\rangle_D$ with $N=2$.\n\nExpress your answer as a rational fraction in simplest form.", "solution": "We are given $f(x)=x$ and $g(x)=\\frac{3}{2}x^{2}-\\frac{1}{2}$. For the composite left-hand rule with $N=2$ on the interval $[-1, 1]$, the step size is $\\Delta x=\\frac{1 - (-1)}{N}=\\frac{2}{2}=1$. The sample points are $x_k = -1 + k\\Delta x$, which gives $x_{0}=-1$ for $k=0$ and $x_{1}=0$ for $k=1$.\n\nFirst, evaluate the functions at the sample points:\n$$\nf(x_0) = f(-1)=-1, \\quad f(x_1) = f(0)=0\n$$\n$$\ng(x_0) = g(-1)=\\frac{3}{2}(-1)^2-\\frac{1}{2}=1, \\quad g(x_1) = g(0)=-\\frac{1}{2}\n$$\n\nNext, compute the discrete inner products according to the definition $\\langle u, v \\rangle_D = \\sum_{k=0}^{N-1} u(x_k)v(x_k) \\Delta x$.\n\nThe inner product of $f$ and $g$ is:\n$$\n\\langle f,g\\rangle_{D} = \\sum_{k=0}^{1} f(x_{k})g(x_{k})\\Delta x = (f(x_0)g(x_0) + f(x_1)g(x_1))\\Delta x = ((-1)(1) + (0)(-\\frac{1}{2})) \\cdot 1 = -1.\n$$\n\nThe inner products needed for the norms are:\n$$\n\\langle f,f\\rangle_{D} = \\sum_{k=0}^{1} f(x_{k})^2\\Delta x = (f(x_0)^2 + f(x_1)^2)\\Delta x = ((-1)^2 + 0^2) \\cdot 1 = 1.\n$$\n$$\n\\langle g,g\\rangle_{D} = \\sum_{k=0}^{1} g(x_{k})^2\\Delta x = (g(x_0)^2 + g(x_1)^2)\\Delta x = (1^2 + (-\\frac{1}{2})^2) \\cdot 1 = 1+\\frac{1}{4}=\\frac{5}{4}.\n$$\n\nNow, compute the discrete norms using $\\|u\\|_D = \\sqrt{\\langle u, u \\rangle_D}$:\n$$\n\\|f\\|_{D}=\\sqrt{1}=1\n$$\n$$\n\\|g\\|_{D}=\\sqrt{\\frac{5}{4}}=\\frac{\\sqrt{5}}{2}\n$$\n\nFinally, calculate $\\cos(\\theta_{D})$ and its square:\n$$\n\\cos(\\theta_{D})=\\frac{\\langle f,g\\rangle_{D}}{\\|f\\|_{D}\\|g\\|_{D}}=\\frac{-1}{1\\cdot \\frac{\\sqrt{5}}{2}}=-\\frac{2}{\\sqrt{5}}\n$$\n$$\n\\cos^{2}(\\theta_{D})=\\left(-\\frac{2}{\\sqrt{5}}\\right)^{2}=\\frac{4}{5}.\n$$", "answer": "$$\\boxed{\\frac{4}{5}}$$", "id": "2179886"}]}