{"hands_on_practices": [{"introduction": "A cornerstone of computational engineering is analyzing how structures respond to external forces. This practice simulates such a scenario, where a physical system is represented by a stiffness matrix $K$ and the response by a displacement vector $\\mathbf{u}$. By solving the linear system $K\\mathbf{u}=\\mathbf{f}$ for various load vectors $\\mathbf{f}$, you will gain hands-on experience with a fundamental task that is ubiquitous in fields ranging from structural analysis to electrical circuits. [@problem_id:2449826]", "problem": "You are given a linear system representative of a one-dimensional finite element discretization of a bar, encoded by a symmetric tridiagonal stiffness matrix. Let the stiffness matrix $K \\in \\mathbb{R}^{5 \\times 5}$ and load vectors $\\mathbf{f}^{(k)} \\in \\mathbb{R}^{5}$ be defined as follows:\n$$\nK = \\begin{bmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{bmatrix},\n$$\nand the test suite of load vectors is\n$$\n\\mathbf{f}^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n\\mathbf{f}^{(2)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n\\mathbf{f}^{(3)} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n\\mathbf{f}^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nFor each load vector $\\mathbf{f}^{(k)}$, compute the displacement vector $\\mathbf{u}^{(k)} \\in \\mathbb{R}^{5}$ that satisfies the linear system\n$$\nK \\,\\mathbf{u}^{(k)} = \\mathbf{f}^{(k)}.\n$$\nAll computations are purely numerical and dimensionless; no physical units are required. Angles are not involved. Percentages must not appear in any output.\n\nYour program must process the provided test suite and produce, in a single line, the list of the four displacement vectors $\\left[\\mathbf{u}^{(1)}, \\mathbf{u}^{(2)}, \\mathbf{u}^{(3)}, \\mathbf{u}^{(4)}\\right]$, where each vector is represented as a list of $5$ floating-point numbers rounded to $6$ decimal places. The final output format must be exactly one line of the form\n$$\n[[u^{(1)}_1,\\dots,u^{(1)}_5],[u^{(2)}_1,\\dots,u^{(2)}_5],[u^{(3)}_1,\\dots,u^{(3)}_5],[u^{(4)}_1,\\dots,u^{(4)}_5]],\n$$\nwith commas separating all entries and sublists, and with no extra text before or after.\n\nDesign for coverage:\n- General case: $\\mathbf{f}^{(3)}$ is a nontrivial dense load.\n- Boundary condition behavior: $\\mathbf{f}^{(2)}$ is the zero load.\n- Edge-localized load: $\\mathbf{f}^{(1)}$ loads only the boundary degrees of freedom.\n- Interior-localized load: $\\mathbf{f}^{(4)}$ loads only the center degree of freedom.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$), where in this problem the results are the four displacement vectors in the exact nested-list structure described above.", "solution": "The problem presented is a standard exercise in computational engineering, specifically involving the solution of a system of linear algebraic equations of the form $K \\mathbf{u} = \\mathbf{f}$. Here, $K$ represents the stiffness matrix of a discretized one-dimensional mechanical system, $\\mathbf{f}$ is an external load vector, and $\\mathbf{u}$ is the resulting displacement vector.\n\nBefore proceeding to a solution, we must first validate the problem's well-posedness. A unique solution $\\mathbf{u}$ exists for any given $\\mathbf{f}$ if and only if the matrix $K$ is non-singular, meaning its determinant is non-zero ($\\det(K) \\neq 0$). The given stiffness matrix is:\n$$\nK = \\begin{bmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{bmatrix}\n$$\nThis is a symmetric, tridiagonal, Toeplitz matrix. Such matrices arise frequently from the finite difference or finite element discretization of the one-dimensional negative Laplace operator, $-\\frac{d^2}{dx^2}$, with certain boundary conditions.\n\nThe invertibility of $K$ can be established by examining its eigenvalues. For an $n \\times n$ matrix of this form, the eigenvalues $\\lambda_j$ are given by the analytical formula:\n$$\n\\lambda_j = 2 - 2 \\cos\\left(\\frac{j\\pi}{n+1}\\right), \\quad \\text{for } j = 1, 2, \\dots, n\n$$\nIn our case, $n=5$. All arguments to the cosine function, $\\frac{j\\pi}{6}$, for $j \\in \\{1, 2, 3, 4, 5\\}$ lie strictly in the interval $(0, \\pi)$, where $\\cos(x) < 1$. Consequently, all eigenvalues $\\lambda_j = 2(1 - \\cos(\\frac{j\\pi}{6}))$ are strictly positive. A matrix with all positive eigenvalues is positive-definite, and a positive-definite matrix is always invertible. The determinant, which is the product of the eigenvalues, is therefore also positive. Specifically, $\\det(K) = n+1 = 6$.\n\nSince $K$ is invertible, the problem is well-posed. For each load vector $\\mathbf{f}^{(k)}$, there exists a unique displacement vector $\\mathbf{u}^{(k)}$ given by the formal solution:\n$$\n\\mathbf{u}^{(k)} = K^{-1} \\mathbf{f}^{(k)}\n$$\nThe task is to compute this solution for each of the four test cases:\n$$\n\\mathbf{f}^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n\\mathbf{f}^{(2)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n\\mathbf{f}^{(3)} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n\\mathbf{f}^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\nNumerically, it is more efficient and stable to solve the linear system directly rather than computing the inverse matrix $K^{-1}$ explicitly. Standard numerical methods such as LU decomposition followed by forward and backward substitution are employed for this purpose. The implementation will use a robust linear solver provided by a standard numerical library.\n\nThe procedure is as follows:\n1.  Define the matrix $K$ and the set of load vectors $\\mathbf{f}^{(k)}$.\n2.  Iterate through each load vector $\\mathbf{f}^{(k)}$.\n3.  For each $\\mathbf{f}^{(k)}$, solve the system $K \\mathbf{u}^{(k)} = \\mathbf{f}^{(k)}$ to find $\\mathbf{u}^{(k)}$.\n4.  Collect the resulting vectors $\\mathbf{u}^{(k)}$ and format them according to the output specification, rounding each component to $6$ decimal places.\n\nThe case of $\\mathbf{f}^{(2)} = \\mathbf{0}$ is trivial: since $K$ is invertible, the only solution to $K \\mathbf{u} = \\mathbf{0}$ is the zero vector $\\mathbf{u}^{(2)} = \\mathbf{0}$. The other cases represent different load distributions and will yield non-trivial displacement vectors. The problem is valid and can now be solved algorithmically.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a set of linear systems Ku=f for a given stiffness matrix K\n    and multiple load vectors f.\n    \"\"\"\n    # Define the symmetric tridiagonal stiffness matrix K.\n    K = np.array([\n        [2.0, -1.0,  0.0,  0.0,  0.0],\n        [-1.0, 2.0, -1.0,  0.0,  0.0],\n        [ 0.0, -1.0, 2.0, -1.0,  0.0],\n        [ 0.0,  0.0, -1.0, 2.0, -1.0],\n        [ 0.0,  0.0,  0.0, -1.0, 2.0]\n    ])\n\n    # Define the test suite of load vectors f^(k).\n    test_cases = [\n        np.array([1.0, 0.0, 0.0, 0.0, 1.0]),  # f^(1)\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),  # f^(2)\n        np.array([1.0, 2.0, 3.0, 4.0, 5.0]),  # f^(3)\n        np.array([0.0, 0.0, 1.0, 0.0, 0.0])   # f^(4)\n    ]\n\n    results = []\n    # Iterate through each load vector and solve the linear system.\n    for f_vector in test_cases:\n        # Use numpy's high-performance linear algebra solver.\n        u_vector = np.linalg.solve(K, f_vector)\n        results.append(u_vector)\n\n    # Format the results according to the problem specification.\n    # Each float is rounded to 6 decimal places.\n    # The final output must be a single-line string representation of a list of lists.\n    rounded_results_as_lists = [np.round(u, 6).tolist() for u in results]\n\n    # Manually construct the string to ensure no spaces and exact comma separation.\n    inner_strings = []\n    for res_list in rounded_results_as_lists:\n        # Format numbers to avoid trailing zeros where not needed, like `1.0` not `1.000000`\n        # `map(str, ...)` handles this correctly.\n        inner_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    \n    # Print the final result in the exact required format.\n    print(final_output_string)\n\nsolve()\n```", "id": "2449826"}, {"introduction": "Beyond solving equations, a key skill for a computational engineer is verifying that mathematical models are valid representations of reality. This exercise focuses on a fundamental tool in statistics and machine learning, the covariance matrix, which must satisfy specific mathematical properties like symmetry and positive definiteness. By implementing an algorithm to test these properties, you will directly see how abstract concepts from linear algebra ensure the physical and statistical soundness of a computational model. [@problem_id:2449839]", "problem": "You are given a finite set of real square matrices. A real square matrix $A \\in \\mathbb{R}^{n \\times n}$ is considered a valid covariance matrix in this task if and only if it satisfies all of the following conditions: (i) $A$ has only finite real entries, (ii) $A$ is symmetric, meaning $A = A^{\\top}$, and (iii) $A$ is positive definite, meaning $x^{\\top} A x > 0$ for every nonzero vector $x \\in \\mathbb{R}^{n}$. For each matrix in the test suite below, determine whether it is a valid covariance matrix under these conditions. There are no physical units involved in this task. The required output for the program is a single line containing a comma-separated list of boolean values enclosed in square brackets, in the same order as the test cases (for example: \"[True,False,True]\").\n\nThe test suite consists of the following matrices:\n\nCase $1$:\n$$\nA_1 = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n$$\n\nCase $2$:\n$$\nA_2 = \\begin{bmatrix}\n1 & 2 \\\\\n2 & 1\n\\end{bmatrix}\n$$\n\nCase $3$:\n$$\nA_3 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n$$\n\nCase $4$:\n$$\nA_4 = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 1\n\\end{bmatrix}\n$$\n\nCase $5$:\n$$\nA_5 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n$$\n\nCase $6$:\n$$\nA_6 = \\begin{bmatrix}\n3\n\\end{bmatrix}\n$$\n\nCase $7$:\n$$\nA_7 = \\begin{bmatrix}\n0\n\\end{bmatrix}\n$$\n\nCase $8$:\n$$\nA_8 = \\begin{bmatrix}\n\\text{NaN}\n\\end{bmatrix}\n$$\n\nYour program must evaluate each $A_i$ and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[\\text{True},\\text{False},\\dots]\"). The result for each case must be a boolean indicating whether $A_i$ is a valid covariance matrix according to the definition above. The test suite covers general cases, boundary conditions (such as size $1 \\times 1$ and singular matrices), non-symmetric matrices, and matrices with non-finite entries. The final output format must be exactly one line with the list notation as described, with no additional text.", "solution": "The problem requires the validation of a set of real square matrices against a provided definition of a valid covariance matrix. A matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as a valid covariance matrix if and only if it satisfies the following three conditions simultaneously:\n(i) $A$ has only finite real entries.\n(ii) $A$ is symmetric, which means $A = A^{\\top}$.\n(iii) $A$ is positive definite, which means $x^{\\top} A x > 0$ for every nonzero vector $x \\in \\mathbb{R}^{n}$.\n\nWe will analyze each matrix from the test suite against these three conditions. A matrix must satisfy all three to be considered valid. If any condition fails, the matrix is immediately invalid.\n\nFor condition (iii), we note that a symmetric matrix is positive definite if and only if all of its eigenvalues are strictly positive. This provides a computationally reliable method for verification. If a matrix is not symmetric, the question of it being positive definite under this problem's framework is moot, as it already fails condition (ii).\n\nLet us proceed with the analysis of each case.\n\nCase $1$: $A_1 = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n(i) Finite entries: All entries ($2, 1$) are finite real numbers. This condition is satisfied.\n(ii) Symmetry: The matrix is symmetric since the element at row $1$, column $2$ is $1$, which is equal to the element at row $2$, column $1$. Thus, $A_1 = A_1^{\\top}$. This condition is satisfied.\n(iii) Positive definiteness: Since $A_1$ is symmetric, we can inspect its eigenvalues. The characteristic equation is $\\det(A_1 - \\lambda I) = 0$.\n$$ \\det \\begin{pmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{pmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = (\\lambda-1)(\\lambda-3) = 0 $$\nThe eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 3$. Both are strictly positive. Therefore, $A_1$ is positive definite.\nVerdict for $A_1$: Valid (True).\n\nCase $2$: $A_2 = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$\n(i) Finite entries: All entries ($1, 2$) are finite real numbers. Satisfied.\n(ii) Symmetry: The matrix is symmetric. Satisfied.\n(iii) Positive definiteness: We compute the eigenvalues.\n$$ \\det \\begin{pmatrix} 1-\\lambda & 2 \\\\ 2 & 1-\\lambda \\end{pmatrix} = (1-\\lambda)^2 - 4 = \\lambda^2 - 2\\lambda - 3 = (\\lambda-3)(\\lambda+1) = 0 $$\nThe eigenvalues are $\\lambda_1 = 3$ and $\\lambda_2 = -1$. Since one eigenvalue is negative, the matrix is not positive definite.\nVerdict for $A_2$: Invalid (False).\n\nCase $3$: $A_3 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$\n(i) Finite entries: All entries ($1, 0$) are finite real numbers. Satisfied.\n(ii) Symmetry: The matrix is symmetric. Satisfied.\n(iii) Positive definiteness: The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 0$. A matrix is positive definite only if all eigenvalues are *strictly* positive. The presence of a $0$ eigenvalue means the matrix is positive semi-definite, but not positive definite. For a non-zero vector $x = [0, 1]^{\\top}$, we have $x^{\\top} A_3 x = [0 \\ 1] \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0$, violating the strict inequality $x^{\\top} A x > 0$.\nVerdict for $A_3$: Invalid (False).\n\nCase $4$: $A_4 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n(i) Finite entries: All entries ($1, 0$) are finite real numbers. Satisfied.\n(ii) Symmetry: The element at row $1$, column $2$ is $0$, while the element at row $2$, column $1$ is $1$. Since $0 \\neq 1$, the matrix is not symmetric ($A_4 \\neq A_4^{\\top}$). The matrix fails this condition.\nVerdict for $A_4$: Invalid (False).\n\nCase $5$: $A_5 = \\begin{bmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix}$\n(i) Finite entries: All entries are finite real numbers. Satisfied.\n(ii) Symmetry: The matrix is symmetric. Satisfied.\n(iii) Positive definiteness: We check the eigenvalues. The characteristic polynomial is $\\det(A_5 - \\lambda I) = (2-\\lambda)((2-\\lambda)^2-1) - (-1)(-(2-\\lambda)) = (2-\\lambda)^3 - 2(2-\\lambda) = (2-\\lambda)(\\lambda^2 - 4\\lambda + 2) = 0$. The roots are $\\lambda_1 = 2$ and, from the quadratic formula on $\\lambda^2 - 4\\lambda + 2 = 0$, $\\lambda = \\frac{4 \\pm \\sqrt{16-8}}{2} = 2 \\pm \\sqrt{2}$. The three eigenvalues are $2$, $2+\\sqrt{2}$, and $2-\\sqrt{2}$. Since $\\sqrt{2} \\approx 1.414$, all eigenvalues ($2$, $\\approx 3.414$, $\\approx 0.586$) are strictly positive. The matrix is positive definite.\nVerdict for $A_5$: Valid (True).\n\nCase $6$: $A_6 = \\begin{bmatrix} 3 \\end{bmatrix}$\n(i) Finite entries: The entry $3$ is a finite real number. Satisfied.\n(ii) Symmetry: A $1 \\times 1$ matrix is trivially symmetric. Satisfied.\n(iii) Positive definiteness: For a $1 \\times 1$ matrix $[a]$, the condition $x^{\\top} A x > 0$ for non-zero $x \\in \\mathbb{R}^1$ becomes $a x^2 > 0$. Since $x \\neq 0$, $x^2 > 0$. The inequality holds if and only if $a > 0$. Here, $a=3$, which is greater than $0$. The single eigenvalue is $3$, which is positive.\nVerdict for $A_6$: Valid (True).\n\nCase $7$: $A_7 = \\begin{bmatrix} 0 \\end{bmatrix}$\n(i) Finite entries: The entry $0$ is a finite real number. Satisfied.\n(ii) Symmetry: A $1 \\times 1$ matrix is trivially symmetric. Satisfied.\n(iii) Positive definiteness: Using the logic from Case $6$, we require $a>0$. Here, $a=0$. The condition is not satisfied, as for any non-zero $x$, $0 \\cdot x^2 = 0$, which is not strictly greater than $0$. The eigenvalue is $0$.\nVerdict for $A_7$: Invalid (False).\n\nCase $8$: $A_8 = \\begin{bmatrix} \\text{NaN} \\end{bmatrix}$\n(i) Finite entries: The entry is 'Not a Number' (NaN), which is not a finite real number. This condition is not satisfied.\nVerdict for $A_8$: Invalid (False).\n\nSummary of results:\n$A_1$: True\n$A_2$: False\n$A_3$: False\n$A_4$: False\n$A_5$: True\n$A_6$: True\n$A_7$: False\n$A_8$: False", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef is_valid_covariance(matrix: np.ndarray) -> bool:\n    \"\"\"\n    Checks if a given matrix is a valid covariance matrix based on the problem's definition.\n\n    A matrix A is a valid covariance matrix if it satisfies all of the following conditions:\n    (i) A has only finite real entries.\n    (ii) A is symmetric (A = A^T).\n    (iii) A is positive definite (x^T * A * x > 0 for all non-zero x).\n\n    Args:\n        matrix: A numpy array representing the square matrix.\n\n    Returns:\n        A boolean value: True if the matrix is a valid covariance matrix, False otherwise.\n    \"\"\"\n    # Condition (i): Check for finite real entries.\n    # np.isfinite checks for both NaN and infinity.\n    if not np.isfinite(matrix).all():\n        return False\n\n    # The problem statement guarantees square matrices. A robust check is included here.\n    if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:\n        return False\n\n    # Condition (ii): Check for symmetry.\n    # np.allclose is used for robust floating-point comparisons.\n    if not np.allclose(matrix, matrix.T):\n        return False\n        \n    # An empty matrix could be argued, but problem scope is n>=1.\n    if matrix.shape[0] == 0:\n        return False\n\n    # Condition (iii): Check for positive definiteness.\n    # For a real symmetric matrix, this is equivalent to all eigenvalues being strictly positive.\n    # We use np.linalg.eigvalsh, which is optimized for symmetric/Hermitian matrices\n    # and guarantees real eigenvalues.\n    try:\n        eigenvalues = np.linalg.eigvalsh(matrix)\n    except np.linalg.LinAlgError:\n        # This error can occur for malformed matrices, though unlikely for finite symmetric ones.\n        return False\n    \n    # All eigenvalues must be strictly greater than 0.\n    # For numerical stability with floating point numbers, a small tolerance might\n    # be used in general, but for this problem's integer inputs, > 0 is sufficient.\n    if not np.all(eigenvalues > 0):\n        return False\n\n    # If all conditions are met\n    return True\n\ndef solve():\n    \"\"\"\n    Defines the test suite of matrices, evaluates each, and prints the result.\n    \"\"\"\n    # Define the test cases from the problem statement as a list of numpy arrays.\n    test_cases = [\n        # Case 1\n        np.array([[2, 1], [1, 2]]),\n        # Case 2\n        np.array([[1, 2], [2, 1]]),\n        # Case 3\n        np.array([[1, 0], [0, 0]]),\n        # Case 4\n        np.array([[1, 0], [1, 1]]),\n        # Case 5\n        np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]]),\n        # Case 6\n        np.array([[3]]),\n        # Case 7\n        np.array([[0]]),\n        # Case 8\n        np.array([[np.nan]]),\n    ]\n\n    results = []\n    for matrix in test_cases:\n        # Determine if the current matrix is a valid covariance matrix.\n        is_valid = is_valid_covariance(matrix)\n        results.append(is_valid)\n\n    # Format the final output as a comma-separated list of booleans enclosed in square brackets.\n    # The str() of a boolean in Python is \"True\" or \"False\", which is what's required.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # The final print statement must produce only the specified output format.\n    print(output_str)\n\nsolve()\n```", "id": "2449839"}, {"introduction": "This final practice takes you to the frontier of data science by exploring the power of spectral methods to uncover hidden structures in data. You will construct a graph Laplacian matrix from a set of points and use its eigenvectors—its \"spectrum\"—to perform clustering, a task that is often difficult with traditional methods. This exercise demonstrates how a sophisticated application of matrix factorization can transform a complex geometric challenge into a tractable linear algebra problem, a core concept in modern machine learning and data analysis. [@problem_id:2449819]", "problem": "You are given finite sets of points in Euclidean space. For each set, you must construct a fully connected, weighted, undirected graph whose vertices correspond to the points and whose edge weights are defined by a Gaussian kernel with a prescribed positive scale parameter. Using the symmetric normalized graph Laplacian, you must obtain a low-dimensional representation from the eigenvectors associated with the smallest eigenvalues and then produce a partition of the points into a prescribed number of clusters by minimizing a within-cluster squared Euclidean objective in that representation. Your program must compute the final cluster labels for each test case and output all results on a single line in the format specified at the end of this statement.\n\nMathematical specification:\n\n- Let $X = \\{x_1, x_2, \\dots, x_n\\}$ be a set of points with $x_i \\in \\mathbb{R}^d$ and a scale parameter $\\tau \\in \\mathbb{R}_{>0}$. Define the similarity matrix $S \\in \\mathbb{R}^{n \\times n}$ by\n  $$ S_{ij} = \\begin{cases}\n  \\exp\\!\\left(-\\dfrac{\\|x_i - x_j\\|_2^2}{2 \\tau^2}\\right), & i \\neq j,\\\\\n  0, & i = j.\n  \\end{cases} $$\n- Define the degree matrix $D \\in \\mathbb{R}^{n \\times n}$ by $D_{ii} = \\sum_{j=1}^n S_{ij}$ and $D_{ij} = 0$ for $i \\neq j$.\n- Define the symmetric normalized Laplacian $L_{\\mathrm{sym}} \\in \\mathbb{R}^{n \\times n}$ by\n  $$ L_{\\mathrm{sym}} = I_n - D^{-1/2} S D^{-1/2}, $$\n  where $I_n$ is the $n \\times n$ identity matrix and $D^{-1/2}$ denotes the diagonal matrix with entries $D_{ii}^{-1/2}$ if $D_{ii} > 0$ and $0$ otherwise.\n- Let $k \\in \\mathbb{Z}_{\\ge 1}$ be the specified number of clusters. Let $(\\lambda_1, v_1), \\dots, (\\lambda_k, v_k)$ be the $k$ eigenpairs of $L_{\\mathrm{sym}}$ with the $k$ smallest eigenvalues in nondecreasing order. Select orthonormal eigenvectors and fix the sign of each column $v_j$ as follows: find an index $p \\in \\{1,\\dots,n\\}$ that achieves $\\max_i |(v_j)_i|$ (if multiple indices achieve the maximum, take the smallest such index), and if $(v_j)_p < 0$ then replace $v_j$ by $-v_j$.\n- Form the matrix $U \\in \\mathbb{R}^{n \\times k}$ with columns $v_1, \\dots, v_k$. Normalize each row: for $i \\in \\{1,\\dots,n\\}$, define\n  $$ \\tilde{u}_i = \\begin{cases}\n  \\dfrac{u_i}{\\|u_i\\|_2}, & \\|u_i\\|_2 > 0,\\\\\n  0, & \\text{otherwise},\n  \\end{cases} $$\n  where $u_i^\\top$ is the $i$-th row of $U$ and $\\tilde{u}_i^\\top$ is the $i$-th row of the row-normalized matrix $\\tilde{U} \\in \\mathbb{R}^{n \\times k}$.\n- Define labels $\\ell_i \\in \\{0, 1, \\dots, k-1\\}$ by solving the clustering problem\n  $$ \\min_{\\ell_1,\\dots,\\ell_n \\in \\{0,\\dots,k-1\\},\\, c_0,\\dots,c_{k-1} \\in \\mathbb{R}^k} \\sum_{i=1}^n \\left\\| \\tilde{u}_i - c_{\\ell_i} \\right\\|_2^2. $$\n  Among all minimizers, impose the following deterministic label indexing rule. Let $C = \\{c_0,\\dots,c_{k-1}\\}$ be any set of optimal centers. Sort the centers in ascending lexicographic order to obtain an ordered list; if two centers are exactly identical, break ties by the smaller mean of the original point indices assigned to that center (and if still tied, by the smaller original cluster index). Reindex clusters so that label $0$ corresponds to the first center in this ordered list, label $1$ to the second, and so on. The final labels $\\ell_i$ must be reported after applying this reindexing.\n\nTest suite:\n\nCompute labels $\\ell_i$ for each of the following cases. Coordinates are given in $\\mathbb{R}^2$.\n\n- Case A (happy path, two well-separated groups):\n  - $X_A = \\{(-2.0,\\, 0.0),\\; (-2.2,\\, 0.2),\\; (-1.8,\\, -0.3),\\; (2.0,\\, 0.0),\\; (2.1,\\, 0.1),\\; (1.9,\\, -0.2)\\}$.\n  - $\\tau_A = 0.9$.\n  - $k_A = 2$.\n- Case B (three compact groups, one per corner direction):\n  - $X_B = \\{(0.0,\\, 0.0),\\; (0.2,\\, -0.1),\\; (-0.1,\\, 0.1),\\; (3.0,\\, 0.0),\\; (2.9,\\, -0.2),\\; (3.2,\\, 0.1),\\; (0.0,\\, 3.0),\\; (0.1,\\, 2.8),\\; (-0.2,\\, 3.1)\\}$.\n  - $\\tau_B = 0.9$.\n  - $k_B = 3$.\n- Case C (nonconvex structure: dense center and surrounding ring):\n  - $X_C = \\{(0.1,\\, 0.1),\\; (-0.1,\\, -0.2),\\; (0.2,\\, -0.1),\\; (-0.2,\\, 0.0),\\; (2.0,\\, 0.0),\\; (1.4142,\\, 1.4142),\\; (0.0,\\, 2.0),\\; (-1.4142,\\, 1.4142),\\; (-2.0,\\, 0.0),\\; (-1.4142,\\, -1.4142),\\; (0.0,\\, -2.0),\\; (1.4142,\\, -1.4142)\\}$.\n  - $\\tau_C = 1.0$.\n  - $k_C = 2$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each case is the list of labels $\\ell_i$ in the same order as the points are listed above. For example, the overall output must be formatted exactly as\n[[labels for Case A],[labels for Case B],[labels for Case C]],\nusing square brackets and commas with no additional spaces or text. No angles or physical units are involved in this problem. All numerical values are in standard real numbers as given.", "solution": "The problem presented is a well-posed and standard task in computational science, specifically within the domain of machine learning and data analysis. It describes the spectral clustering algorithm proposed by Ng, Jordan, and Weiss, which is a powerful technique for identifying cluster structures in data, including non-convex ones. The problem is scientifically grounded, mathematically precise, and provides all necessary information to proceed. It requires the implementation of a deterministic procedure, from the construction of a graph Laplacian to a final, unambiguously indexed clustering. The problem is valid.\n\nThe solution proceeds by first constructing a representation of the data that is more amenable to clustering and then applying a standard partitioning algorithm in this new representation. This is the core principle of spectral clustering: transforming a problem of clustering in a potentially complex high-dimensional space into a simpler one in a low-dimensional embedding derived from the spectral properties of a similarity graph.\n\nThe algorithmic steps are executed as follows for each test case, which consists of a set of points $X = \\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, a scale parameter $\\tau > 0$, and a desired number of clusters $k$.\n\nFirst, we quantify the pairwise similarity between points. A fully connected, weighted, undirected graph is constructed with vertices corresponding to the points $x_i$. The weight of the edge between two distinct points $x_i$ and $x_j$ is given by a Gaussian kernel, which measures their affinity. The similarity matrix $S \\in \\mathbb{R}^{n \\times n}$ has entries:\n$$\nS_{ij} = \\exp\\!\\left(-\\frac{\\|x_i - x_j\\|_2^2}{2 \\tau^2}\\right) \\quad \\text{for } i \\neq j, \\quad \\text{and} \\quad S_{ii} = 0.\n$$\nThe parameter $\\tau$ controls the width of the kernel, defining the scale at which similarity is measured.\n\nFrom the similarity matrix, we define the degree matrix $D$ as the diagonal matrix whose entries $D_{ii}$ are the sum of similarities for point $x_i$: $D_{ii} = \\sum_{j=1}^n S_{ij}$. This represents the total affinity of each point to all other points.\n\nThe central object of analysis is the symmetric normalized graph Laplacian, $L_{\\mathrm{sym}}$, defined as:\n$$\nL_{\\mathrm{sym}} = I_n - D^{-1/2} S D^{-1/2}\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix. This specific form of the Laplacian has favorable properties, including real eigenvalues in the range $[0, 2]$. Its eigenvectors form an orthonormal basis and provide a low-dimensional embedding of the data that reveals cluster structure. The eigenvectors corresponding to the smallest eigenvalues (close to $0$) vary most slowly across the graph, making them ideal for identifying quasi-connected components, which are interpreted as clusters.\n\nWe compute the $k$ eigenvectors $v_1, \\dots, v_k$ corresponding to the $k$ smallest eigenvalues of $L_{\\mathrm{sym}}$. These eigenvectors are a basis for the embedding space. To ensure determinism, any sign ambiguity in the eigenvectors—since if $v$ is an eigenvector, so is $-v$—must be resolved. The prescribed rule achieves this: for each eigenvector $v_j$, we identify the element with the largest absolute value, and if this element is negative, we flip the sign of the entire eigenvector. This makes the choice of basis vectors canonical.\n\nThese $k$ sign-corrected eigenvectors are arranged as columns of a matrix $U \\in \\mathbb{R}^{n \\times k}$. The $i$-th row of $U$, denoted $u_i^\\top$, represents the new coordinates of the original point $x_i$ in the $k$-dimensional embedding space. A crucial step in the Ng-Jordan-Weiss algorithm is to normalize these row vectors to unit length, yielding a matrix $\\tilde{U}$ with rows $\\tilde{u}_i$:\n$$\n\\tilde{u}_i = \\frac{u_i}{\\|u_i\\|_2} \\quad \\text{if } \\|u_i\\|_2 > 0, \\quad \\text{and} \\quad \\tilde{u}_i = 0 \\text{ otherwise}.\n$$\nThis projects the embedded points onto the surface of a unit hypersphere in $\\mathbb{R}^k$. Theoretical analysis shows that for ideally separated data, all points from a single cluster will map to the same point on this hypersphere. For realistic data, points from one cluster will form a tight group.\n\nThe final step is to partition these new representations $\\{\\tilde{u}_i\\}_{i=1}^n$. This is a classical clustering problem, which the problem statement defines via the minimization of the within-cluster sum of squares:\n$$\n\\min_{\\{\\ell_i\\}, \\{c_j\\}} \\sum_{i=1}^n \\|\\tilde{u}_i - c_{\\ell_i}\\|_2^2\n$$\nThis is precisely the objective function for the k-means clustering algorithm. To guarantee a globally optimal and deterministic solution, which is essential for a scientific computation, we do not rely on a random initialization. Instead, for the small datasets provided, we exhaustively test all possible initializations where the $k$ initial cluster centroids are chosen from the set of points $\\{\\tilde{u}_i\\}$ themselves. The partition that yields the minimum sum of squares is selected.\n\nFinally, the resulting cluster labels must be indexed according to a deterministic rule. This resolves the permutation ambiguity of k-means labels. The optimal cluster centers $\\{c_j\\}_{j=0}^{k-1}$ are sorted first by their coordinates in lexicographical order. Any ties are broken by the mean of the original indices of the points assigned to the cluster, and further ties are broken by the original cluster index produced by the k-means algorithm. The final labels $\\{0, 1, \\dots, k-1\\}$ are assigned based on this sorted order. This rigorous procedure ensures that the output is unique and reproducible.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.linalg import eigh\nfrom scipy.cluster.vq import kmeans2\nfrom itertools import combinations\n\ndef compute_clusters(X: np.ndarray, tau: float, k: int) -> list[int]:\n    \"\"\"\n    Performs spectral clustering on a set of points according to the problem specification.\n\n    Args:\n        X: An (n, d) array of n points in d-dimensional space.\n        tau: The scale parameter for the Gaussian kernel.\n        k: The number of clusters to find.\n\n    Returns:\n        A list of n integer labels for the points.\n    \"\"\"\n    n = X.shape[0]\n\n    # Step 1: Compute the similarity matrix S\n    dist_sq = squareform(pdist(X, 'sqeuclidean'))\n    S = np.exp(-dist_sq / (2 * tau**2))\n    np.fill_diagonal(S, 0)\n\n    # Step 2: Compute the degree matrix D and the symmetric Laplacian L_sym\n    d = S.sum(axis=1)\n    \n    # Use np.divide or a 'where' clause for safe inverse square root\n    d_inv_sqrt_vals = np.power(d, -0.5, where=d > 0, out=np.zeros_like(d, dtype=float))\n    D_inv_sqrt = np.diag(d_inv_sqrt_vals)\n    \n    L_sym = np.identity(n) - D_inv_sqrt @ S @ D_inv_sqrt\n\n    # Step 3: Compute the first k eigenvectors of L_sym\n    # eigh returns eigenvalues in ascending order and corresponding eigenvectors as columns\n    _, eigvecs = eigh(L_sym)\n    U = eigvecs[:, :k]\n\n    # Step 4: Fix sign ambiguity of eigenvectors\n    for j in range(k):\n        v = U[:, j]\n        # np.argmax returns the first index in case of ties, as required\n        p = np.argmax(np.abs(v))\n        if v[p] < 0:\n            U[:, j] = -v\n\n    # Step 5: Normalize rows of U to get tilde_U\n    norms = np.linalg.norm(U, axis=1, keepdims=True)\n    tilde_U = np.divide(U, norms, out=np.zeros_like(U), where=norms != 0)\n\n    # Step 6: Perform k-means clustering on tilde_U\n    # To find the global minimum deterministically, we test all possible initial centroids from the dataset.\n    best_inertia = np.inf\n    best_centroids = None\n    best_labels = None\n    \n    point_indices = np.arange(n)\n    \n    # Iterate through all combinations of k points as initial centers\n    for init_indices in combinations(point_indices, k):\n        initial_centroids = tilde_U[list(init_indices), :]\n        \n        # Run kmeans2 with 'matrix' initialization\n        # iter=100 is a reasonable number of iterations for convergence\n        centroids, labels = kmeans2(tilde_U, initial_centroids, iter=100, minit='matrix')\n        \n        # Check if the number of resulting clusters is k\n        # kmeans2 might return fewer than k clusters if some initial centroids are very close\n        if len(centroids) < k:\n            # Reconstruct full centroid array if some clusters are empty\n            # This is a bit complex, and for these small problems, unlikely to be an issue.\n            # Simplified: we assume k centroids are returned.\n            pass\n\n        # Calculate inertia (within-cluster sum of squares)\n        inertia = 0.0\n        for j in range(len(centroids)):\n            cluster_points = tilde_U[labels == j]\n            if cluster_points.shape[0] > 0:\n                inertia += np.sum((cluster_points - centroids[j])**2)\n        \n        if inertia < best_inertia:\n            best_inertia = inertia\n            best_centroids = centroids\n            best_labels = labels\n\n    # Step 7: Re-index labels based on the deterministic sorting rule\n    sort_keys = []\n    for j in range(k):\n        center_coords = tuple(best_centroids[j])\n        indices_in_cluster = point_indices[best_labels == j]\n        # Handle empty clusters, although unlikely with our method\n        mean_orig_index = np.mean(indices_in_cluster) if len(indices_in_cluster) > 0 else -1.0\n        original_cluster_idx = j\n        sort_keys.append((center_coords, mean_orig_index, original_cluster_idx))\n\n    # Sort the keys to establish the canonical order of clusters\n    sorted_keys = sorted(sort_keys)\n    \n    # Create a mapping from old cluster indices to new ones\n    label_map = np.zeros(k, dtype=int)\n    for new_label, key_tuple in enumerate(sorted_keys):\n        old_label = key_tuple[2]\n        label_map[old_label] = new_label\n        \n    final_labels = label_map[best_labels]\n    \n    return final_labels.tolist()\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the spectral clustering algorithm, and prints the final output.\n    \"\"\"\n    test_cases = [\n        (\n            # Case A\n            np.array([\n                [-2.0, 0.0], [-2.2, 0.2], [-1.8, -0.3],\n                [2.0, 0.0], [2.1, 0.1], [1.9, -0.2]\n            ]), 0.9, 2\n        ),\n        (\n            # Case B\n            np.array([\n                [0.0, 0.0], [0.2, -0.1], [-0.1, 0.1],\n                [3.0, 0.0], [2.9, -0.2], [3.2, 0.1],\n                [0.0, 3.0], [0.1, 2.8], [-0.2, 3.1]\n            ]), 0.9, 3\n        ),\n        (\n            # Case C\n            np.array([\n                [0.1, 0.1], [-0.1, -0.2], [0.2, -0.1], [-0.2, 0.0],\n                [2.0, 0.0], [1.4142, 1.4142], [0.0, 2.0], [-1.4142, 1.4142],\n                [-2.0, 0.0], [-1.4142, -1.4142], [0.0, -2.0], [1.4142, -1.4142]\n            ]), 1.0, 2\n        )\n    ]\n\n    results_as_strings = []\n    for X, tau, k in test_cases:\n        labels = compute_clusters(X, tau, k)\n        # Format each list of labels as a string \"[l1,l2,...]\"\n        labels_str = f\"[{','.join(map(str, labels))}]\"\n        results_as_strings.append(labels_str)\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "2449819"}]}