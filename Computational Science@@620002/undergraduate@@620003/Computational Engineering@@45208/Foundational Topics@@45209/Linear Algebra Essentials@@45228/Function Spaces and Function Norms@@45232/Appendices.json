{"hands_on_practices": [{"introduction": "In engineering analysis, we often need to represent complex functions using a standardized set of simpler \"building block\" functions. This practice demonstrates how to construct such a set. You will apply the Gram-Schmidt process, a familiar algorithm from linear algebra, to transform a simple set of polynomials into an orthonormal basis within the $L_2([-1,1])$ function space, which is equipped with an inner product defined by an integral [@problem_id:2395880]. This hands-on procedure is not just a theoretical exercise; it is the method used to generate Legendre polynomials, a family of functions essential for the Finite Element Method and other advanced numerical techniques.", "problem": "In computational engineering, orthonormal polynomial bases on finite intervals are used to construct stable Galerkin approximations of boundary value problems. Consider the function space $L_{2}([-1,1])$ equipped with the standard inner product $\\langle f, g \\rangle = \\int_{-1}^{1} f(x)\\,g(x)\\,dx$ and induced norm $\\|f\\|_{2} = \\sqrt{\\langle f,f \\rangle}$. Starting from the linearly independent set $\\{1, x, x^{2}\\}$, use the Gram–Schmidt process with respect to this inner product to construct an orthonormal basis $\\{q_{0}(x), q_{1}(x), q_{2}(x)\\}$ of the span of $\\{1, x, x^{2}\\}$. Each basis function must be given explicitly as a polynomial in $x$ with real coefficients. Provide the final result as a single row vector containing the three polynomials in the order $q_{0}(x)$, $q_{1}(x)$, $q_{2}(x)$. The answer must be exact; do not approximate or round. Express the final answer as a single analytic expression as specified.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Function Space: $L_{2}([-1,1])$.\n- Inner Product: $\\langle f, g \\rangle = \\int_{-1}^{1} f(x)\\,g(x)\\,dx$.\n- Induced Norm: $\\|f\\|_{2} = \\sqrt{\\langle f,f \\rangle}$.\n- Initial Set: A linearly independent set of functions $\\{1, x, x^{2}\\}$.\n- Objective: Construct an orthonormal basis $\\{q_{0}(x), q_{1}(x), q_{2}(x)\\}$ from the span of the initial set using the Gram-Schmidt process.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is a standard application of the Gram-Schmidt orthogonalization process in the Hilbert space $L_{2}([-1,1])$. This is a fundamental concept in functional analysis and its applications in computational engineering, such as the finite element method. The polynomials to be constructed are the first three normalized Legendre polynomials, which are well-established special functions. The problem is scientifically sound.\n- **Well-Posedness**: The problem is well-posed. The Gram-Schmidt process, when applied to a set of linearly independent vectors (in this case, functions) in an inner product space, yields a unique orthonormal set that spans the same subspace. All necessary components (initial set, inner product definition) are provided.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n\n**Verdict and Action**\nThe problem is valid. It is a well-defined, standard exercise in applied mathematics. I will proceed with the solution.\n\nThe Gram-Schmidt process is an algorithm for constructing an orthonormal set of vectors from a given set of linearly independent vectors in an inner product space. We begin with the set of functions $\\{v_{0}(x), v_{1}(x), v_{2}(x)\\} = \\{1, x, x^{2}\\}$. We will construct an orthogonal set $\\{u_{0}(x), u_{1}(x), u_{2}(x)\\}$ and then normalize each function to obtain the orthonormal set $\\{q_{0}(x), q_{1}(x), q_{2}(x)\\}$.\n\n**Step 1: Construction of $q_{0}(x)$**\nLet $v_{0}(x) = 1$. The first orthogonal function $u_{0}(x)$ is simply $v_{0}(x)$.\n$$u_{0}(x) = v_{0}(x) = 1$$\nNext, we compute the norm squared of $u_{0}(x)$:\n$$\\|u_{0}\\|_{2}^{2} = \\langle u_{0}, u_{0} \\rangle = \\int_{-1}^{1} 1 \\cdot 1 \\,dx = [x]_{-1}^{1} = 1 - (-1) = 2$$\nThe norm is $\\|u_{0}\\|_{2} = \\sqrt{2}$.\nThe first orthonormal function $q_{0}(x)$ is obtained by normalizing $u_{0}(x)$:\n$$q_{0}(x) = \\frac{u_{0}(x)}{\\|u_{0}\\|_{2}} = \\frac{1}{\\sqrt{2}}$$\n\n**Step 2: Construction of $q_{1}(x)$**\nLet $v_{1}(x) = x$. The second orthogonal function $u_{1}(x)$ is found by subtracting the projection of $v_{1}(x)$ onto the subspace spanned by $u_{0}(x)$:\n$$u_{1}(x) = v_{1}(x) - \\frac{\\langle v_{1}, u_{0} \\rangle}{\\langle u_{0}, u_{0} \\rangle} u_{0}(x)$$\nWe compute the inner product $\\langle v_{1}, u_{0} \\rangle$:\n$$\\langle v_{1}, u_{0} \\rangle = \\int_{-1}^{1} x \\cdot 1 \\,dx = \\left[\\frac{x^{2}}{2}\\right]_{-1}^{1} = \\frac{1^{2}}{2} - \\frac{(-1)^{2}}{2} = 0$$\nSince the inner product is $0$, the functions $v_{1}(x)$ and $u_{0}(x)$ are already orthogonal. Thus:\n$$u_{1}(x) = v_{1}(x) - 0 = x$$\nNow, we compute the norm squared of $u_{1}(x)$:\n$$\\|u_{1}\\|_{2}^{2} = \\langle u_{1}, u_{1} \\rangle = \\int_{-1}^{1} x \\cdot x \\,dx = \\int_{-1}^{1} x^{2} \\,dx = \\left[\\frac{x^{3}}{3}\\right]_{-1}^{1} = \\frac{1^{3}}{3} - \\frac{(-1)^{3}}{3} = \\frac{1}{3} - \\left(-\\frac{1}{3}\\right) = \\frac{2}{3}$$\nThe norm is $\\|u_{1}\\|_{2} = \\sqrt{\\frac{2}{3}}$.\nThe second orthonormal function $q_{1}(x)$ is:\n$$q_{1}(x) = \\frac{u_{1}(x)}{\\|u_{1}\\|_{2}} = \\frac{x}{\\sqrt{\\frac{2}{3}}} = x\\sqrt{\\frac{3}{2}}$$\n\n**Step 3: Construction of $q_{2}(x)$**\nLet $v_{2}(x) = x^{2}$. The third orthogonal function $u_{2}(x)$ is found by subtracting the projections of $v_{2}(x)$ onto the subspace spanned by $u_{0}(x)$ and $u_{1}(x)$:\n$$u_{2}(x) = v_{2}(x) - \\frac{\\langle v_{2}, u_{0} \\rangle}{\\langle u_{0}, u_{0} \\rangle} u_{0}(x) - \\frac{\\langle v_{2}, u_{1} \\rangle}{\\langle u_{1}, u_{1} \\rangle} u_{1}(x)$$\nWe compute the required inner products:\n$$\\langle v_{2}, u_{0} \\rangle = \\int_{-1}^{1} x^{2} \\cdot 1 \\,dx = \\left[\\frac{x^{3}}{3}\\right]_{-1}^{1} = \\frac{1}{3} - \\left(-\\frac{1}{3}\\right) = \\frac{2}{3}$$\n$$\\langle v_{2}, u_{1} \\rangle = \\int_{-1}^{1} x^{2} \\cdot x \\,dx = \\int_{-1}^{1} x^{3} \\,dx = \\left[\\frac{x^{4}}{4}\\right]_{-1}^{1} = \\frac{1}{4} - \\frac{1}{4} = 0$$\nNow, we construct $u_{2}(x)$:\n$$u_{2}(x) = x^{2} - \\frac{2/3}{2} (1) - \\frac{0}{2/3} (x) = x^{2} - \\frac{1}{3}$$\nWe compute the norm squared of $u_{2}(x)$:\n$$\\|u_{2}\\|_{2}^{2} = \\int_{-1}^{1} \\left(x^{2} - \\frac{1}{3}\\right)^{2} dx = \\int_{-1}^{1} \\left(x^{4} - \\frac{2}{3}x^{2} + \\frac{1}{9}\\right) dx$$\n$$= \\left[\\frac{x^{5}}{5} - \\frac{2}{3}\\frac{x^{3}}{3} + \\frac{1}{9}x\\right]_{-1}^{1} = \\left[\\frac{x^{5}}{5} - \\frac{2x^{3}}{9} + \\frac{x}{9}\\right]_{-1}^{1}$$\n$$= \\left(\\frac{1}{5} - \\frac{2}{9} + \\frac{1}{9}\\right) - \\left(-\\frac{1}{5} + \\frac{2}{9} - \\frac{1}{9}\\right) = 2\\left(\\frac{1}{5} - \\frac{1}{9}\\right) = 2\\left(\\frac{9-5}{45}\\right) = \\frac{8}{45}$$\nThe norm is $\\|u_{2}\\|_{2} = \\sqrt{\\frac{8}{45}}$.\nThe third orthonormal function $q_{2}(x)$ is:\n$$q_{2}(x) = \\frac{u_{2}(x)}{\\|u_{2}\\|_{2}} = \\frac{x^{2} - \\frac{1}{3}}{\\sqrt{\\frac{8}{45}}} = \\left(x^{2} - \\frac{1}{3}\\right)\\sqrt{\\frac{45}{8}}$$\nTo simplify the expression:\n$$q_{2}(x) = \\left(\\frac{3x^{2}-1}{3}\\right)\\sqrt{\\frac{9 \\cdot 5}{4 \\cdot 2}} = \\left(\\frac{3x^{2}-1}{3}\\right)\\frac{3\\sqrt{5}}{2\\sqrt{2}} = (3x^{2}-1)\\frac{\\sqrt{5}}{2\\sqrt{2}} = \\frac{\\sqrt{10}}{4}(3x^{2}-1)$$\n\nThe resulting orthonormal basis is $\\{q_{0}(x), q_{1}(x), q_{2}(x)\\}$, where:\n$q_{0}(x) = \\frac{1}{\\sqrt{2}}$\n$q_{1}(x) = x\\sqrt{\\frac{3}{2}}$\n$q_{2}(x) = \\frac{\\sqrt{10}}{4}(3x^{2}-1)$\nThese are the first three orthonormal Legendre polynomials.\n\nThe final result is presented as a row vector containing these three polynomials.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  x\\sqrt{\\frac{3}{2}}  \\frac{\\sqrt{10}}{4}(3x^2-1)\n\\end{pmatrix}\n}\n$$", "id": "2395880"}, {"introduction": "Now that you have seen how to construct an orthonormal basis, let’s use the underlying principle of orthogonality to solve a common engineering problem: approximating a complicated function with a much simpler one. This exercise asks you to find the best linear approximation of the function $f(x) = \\sin(x)$ in the sense of minimizing the mean-square error, which corresponds to the $L_2$ norm [@problem_id:2395900]. By treating this as an orthogonal projection problem in a function space, you will see how abstract geometric concepts from Hilbert spaces provide a powerful and precise method for optimal function approximation.", "problem": "Let $f(x) = \\sin(x)$ on the interval $[0,\\pi]$, where $x$ is measured in radians. Consider the function space $L^{2}(0,\\pi)$ equipped with the standard $L^{2}$ inner product $\\langle g,h \\rangle = \\int_{0}^{\\pi} g(x)\\,h(x)\\,dx$. Determine the unique linear polynomial $p^{\\ast}(x) \\in \\operatorname{span}\\{1,x\\}$ that minimizes the $L^{2}$-norm of the approximation error $\\|f - p\\|_{2}$ over all $p(x) = \\alpha x + \\beta$. Provide your final answer as a single closed-form expression for $p^{\\ast}(x)$.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extracted Givens\n- Function to be approximated: $f(x) = \\sin(x)$.\n- Domain of definition: the interval $[0, \\pi]$.\n- The function space is $L^{2}(0, \\pi)$.\n- The inner product is defined as $\\langle g,h \\rangle = \\int_{0}^{\\pi} g(x)h(x)\\,dx$.\n- The approximation subspace is $W = \\operatorname{span}\\{1, x\\}$, which is the space of linear polynomials of the form $p(x) = \\alpha x + \\beta$.\n- The objective is to find the unique polynomial $p^{\\ast}(x) \\in W$ that minimizes the approximation error norm $\\|f - p\\|_{2}$.\n\nStep 2: Validation\nThe problem is scientifically grounded, being a standard problem in approximation theory within the context of Hilbert spaces, a cornerstone of functional analysis and computational engineering. It is well-posed, as the Projection Theorem guarantees the existence and uniqueness of such a best approximation in a finite-dimensional subspace. The problem statement is objective, complete, and contains no contradictions or ambiguities. All provided data and definitions are mathematically sound and sufficient for obtaining a solution.\n\nStep 3: Verdict\nThe problem is valid. We proceed with the solution.\n\nThe problem requires finding the best approximation of the function $f(x) = \\sin(x)$ in the subspace $W = \\operatorname{span}\\{1, x\\}$ with respect to the $L^2$-norm. In a Hilbert space, the best approximation of an element $f$ from a closed subspace $W$ is its orthogonal projection onto $W$. Let this projection be denoted by $p^*(x)$.\n\nThe defining property of the orthogonal projection is that the error vector, $f - p^*$, is orthogonal to every vector in the subspace $W$. It is sufficient to enforce orthogonality with respect to a basis of $W$. We select the standard monomial basis for $W$: $\\{u_1(x), u_2(x)\\}$, where $u_1(x) = 1$ and $u_2(x) = x$.\n\nThe polynomial we seek is $p^*(x) = \\beta u_1(x) + \\alpha u_2(x) = \\beta + \\alpha x$. The coefficients $\\alpha$ and $\\beta$ are determined by the orthogonality conditions:\n$1.$ $\\langle f - p^*, u_1 \\rangle = 0 \\implies \\langle f, u_1 \\rangle = \\langle p^*, u_1 \\rangle$\n$2.$ $\\langle f - p^*, u_2 \\rangle = 0 \\implies \\langle f, u_2 \\rangle = \\langle p^*, u_2 \\rangle$\n\nSubstituting $p^*(x)$ into these equations yields the Gram normal equations, a linear system for $\\alpha$ and $\\beta$:\n$$\n\\beta \\langle u_1, u_1 \\rangle + \\alpha \\langle u_2, u_1 \\rangle = \\langle f, u_1 \\rangle\n$$\n$$\n\\beta \\langle u_1, u_2 \\rangle + \\alpha \\langle u_2, u_2 \\rangle = \\langle f, u_2 \\rangle\n$$\n\nWe must compute the required inner products, which are definite integrals over the interval $[0, \\pi]$.\n\nThe components of the Gram matrix are:\n- $\\langle u_1, u_1 \\rangle = \\int_{0}^{\\pi} 1 \\cdot 1 \\, dx = [x]_{0}^{\\pi} = \\pi$.\n- $\\langle u_1, u_2 \\rangle = \\langle u_2, u_1 \\rangle = \\int_{0}^{\\pi} 1 \\cdot x \\, dx = \\left[\\frac{x^2}{2}\\right]_{0}^{\\pi} = \\frac{\\pi^2}{2}$.\n- $\\langle u_2, u_2 \\rangle = \\int_{0}^{\\pi} x \\cdot x \\, dx = \\int_{0}^{\\pi} x^2 \\, dx = \\left[\\frac{x^3}{3}\\right]_{0}^{\\pi} = \\frac{\\pi^3}{3}$.\n\nThe components of the right-hand side vector are:\n- $\\langle f, u_1 \\rangle = \\int_{0}^{\\pi} \\sin(x) \\cdot 1 \\, dx = [-\\cos(x)]_{0}^{\\pi} = -\\cos(\\pi) - (-\\cos(0)) = -(-1) - (-1) = 2$.\n- $\\langle f, u_2 \\rangle = \\int_{0}^{\\pi} \\sin(x) \\cdot x \\, dx$. This integral is solved using integration by parts, $\\int u \\, dv = uv - \\int v \\, du$. Let $u = x$ and $dv = \\sin(x)dx$, so $du = dx$ and $v = -\\cos(x)$.\n$$\n\\int_{0}^{\\pi} x \\sin(x) \\, dx = [-x\\cos(x)]_{0}^{\\pi} - \\int_{0}^{\\pi} (-\\cos(x)) \\, dx = (-\\pi\\cos(\\pi) - 0) + [\\sin(x)]_{0}^{\\pi} = \\pi + (0 - 0) = \\pi.\n$$\n\nWe now assemble the $2 \\times 2$ linear system for the coefficients $(\\beta, \\alpha)$:\n$$\n\\begin{pmatrix}\n\\pi  \\frac{\\pi^2}{2} \\\\\n\\frac{\\pi^2}{2}  \\frac{\\pi^3}{3}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta \\\\\n\\alpha\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 \\\\\n\\pi\n\\end{pmatrix}\n$$\n\nThe system of equations is:\n$1.$ $\\pi \\beta + \\frac{\\pi^2}{2} \\alpha = 2$\n$2.$ $\\frac{\\pi^2}{2} \\beta + \\frac{\\pi^3}{3} \\alpha = \\pi$\n\nTo solve this system, we can multiply the second equation by $\\frac{2}{\\pi}$:\n$$\n\\pi \\beta + \\frac{2}{\\pi} \\left( \\frac{\\pi^3}{3} \\right) \\alpha = \\frac{2}{\\pi} (\\pi) \\implies \\pi \\beta + \\frac{2\\pi^2}{3} \\alpha = 2\n$$\nWe now have two equations for $\\pi \\beta$:\n$1.'$ $\\pi \\beta = 2 - \\frac{\\pi^2}{2} \\alpha$\n$2.'$ $\\pi \\beta = 2 - \\frac{2\\pi^2}{3} \\alpha$\n\nEquating the right-hand sides gives:\n$$\n2 - \\frac{\\pi^2}{2} \\alpha = 2 - \\frac{2\\pi^2}{3} \\alpha\n$$\n$$\n-\\frac{\\pi^2}{2} \\alpha = -\\frac{2\\pi^2}{3} \\alpha\n$$\n$$\n\\left(\\frac{2\\pi^2}{3} - \\frac{\\pi^2}{2}\\right) \\alpha = 0\n$$\n$$\n\\left(\\frac{4\\pi^2 - 3\\pi^2}{6}\\right) \\alpha = \\frac{\\pi^2}{6} \\alpha = 0\n$$\nSince $\\pi^2 \\neq 0$, this requires that $\\alpha = 0$.\n\nSubstituting $\\alpha = 0$ into the first equation:\n$$\n\\pi \\beta + \\frac{\\pi^2}{2} (0) = 2 \\implies \\pi \\beta = 2 \\implies \\beta = \\frac{2}{\\pi}\n$$\nThe coefficients are $\\alpha = 0$ and $\\beta = \\frac{2}{\\pi}$.\nTherefore, the unique linear polynomial that minimizes the $L^2$-norm of the error is:\n$$\np^*(x) = 0 \\cdot x + \\frac{2}{\\pi} = \\frac{2}{\\pi}\n$$\nThe best approximation is a constant function. This is explained by the symmetry of the functions. The function $\\sin(x)$ is even about the center of the interval, $x = \\frac{\\pi}{2}$. The basis function $x$ can be made odd about $x = \\frac{\\pi}{2}$ by a constant shift ($x - \\frac{\\pi}{2}$). The projection of an even function onto an odd function over a symmetric domain is zero, which is why the coefficient for the non-constant part of the polynomial basis is zero.", "answer": "$$\n\\boxed{\\frac{2}{\\pi}}\n$$", "id": "2395900"}, {"introduction": "This final practice serves as a capstone, connecting the theory of function spaces and norms directly to the verification of a sophisticated computational model. In computational engineering, it is not enough to simply obtain a numerical solution; we must be confident in its accuracy and convergence properties. This problem requires you to implement a basic Finite Element Method (FEM) solver for a 1D boundary value problem and then use the $L_2$ norm to measure the error of your solution [@problem_id:2395840]. By running a computational experiment, you will verify that the error decreases at a rate predicted by FEM theory, a critical skill for anyone developing or using simulation software.", "problem": "Design and implement a complete program that, from first principles, verifies the rate of convergence in the $L_2$ norm for the conforming finite element method applied to a one-dimensional Poisson problem. Consider the boundary value problem on the open interval $(0,1)$: find $u \\in H_0^1(0,1)$ such that\n$$\n-\\frac{d^2 u}{dx^2} = f \\quad \\text{in } (0,1), \\qquad u(0)=0, \\quad u(1)=0,\n$$\nwith the exact solution prescribed as $u(x) = \\sin(\\pi x)$ so that $f(x) = \\pi^2 \\sin(\\pi x)$. For a polynomial degree $k \\in \\{1,2\\}$ and a uniform partition of $(0,1)$ into $N$ elements of size $h = 1/N$, let $V_h^k \\subset H_0^1(0,1)$ denote the space of continuous, piecewise polynomials of degree at most $k$ on the mesh. Let $u_h \\in V_h^k$ be the unique function satisfying the Galerkin variational formulation\n$$\n\\int_0^1 \\frac{d u_h}{dx}(x)\\, \\frac{d v_h}{dx}(x)\\, dx = \\int_0^1 f(x)\\, v_h(x)\\, dx \\quad \\text{for all } v_h \\in V_h^k,\n$$\nwith homogeneous Dirichlet boundary conditions incorporated in $V_h^k$. For a set of decreasing mesh sizes, define the $L_2$ error\n$$\nE(h) = \\left(\\int_0^1 \\left(u(x) - u_h(x)\\right)^2\\, dx \\right)^{1/2},\n$$\nand define the observed convergence rate $r$ to be the real number that best fits the power law $E(h) \\approx C h^r$ in the least-squares sense over the given set of mesh sizes, that is, the value of $r$ that minimizes $\\sum_i \\left(\\log E(h_i) - (a + r \\log h_i)\\right)^2$ over real $a$ and $r$.\n\nThe computational experiment must produce the following test suite results, each derived entirely from the mathematical definitions above:\n\n- Test case A (happy path): $k = 1$ with element counts $N \\in \\{10,20,40,80,160\\}$.\n- Test case B (higher-order space): $k = 2$ with element counts $N \\in \\{8,16,32,64\\}$.\n- Test case C (boundary of asymptotic regime): $k = 1$ with element counts $N \\in \\{2,4,8,16\\}$.\n\nFor each test case, compute the observed rate $r$ as defined above. Additionally, for each test case, output a boolean flag indicating whether the inequality $r \\geq k+1 - \\delta$ holds, where $\\delta = 0.1$ for Test case A, $\\delta = 0.2$ for Test case B, and $\\delta = 0.4$ for Test case C. All angles are in radians, and no physical units are involved.\n\nYour program must produce a single line of output containing all results aggregated into a single list in the following order:\n$[r_A, r_B, r_C, b_A, b_B, b_C]$,\nwhere $r_A$, $r_B$, and $r_C$ are the observed convergence rates for Test cases A, B, and C, respectively, and $b_A$, $b_B$, and $b_C$ are the corresponding boolean flags. For example, an output should look like a single line with the format $[r_1,r_2,r_3,b_1,b_2,b_3]$ with no additional text.", "solution": "The problem statement is a valid, well-posed exercise in computational engineering. It is scientifically grounded in the theory of the Finite Element Method (FEM) for solving partial differential equations. All parameters and objectives are defined with mathematical precision. I will now provide the solution.\n\nThe problem requires a numerical verification of the convergence rate of the conforming finite element method for the one-dimensional Poisson problem with homogeneous Dirichlet boundary conditions. The problem is stated as: find $u \\in H_0^1(0,1)$ such that\n$$\n-\\frac{d^2 u}{dx^2} = f(x) \\quad \\text{for } x \\in (0,1), \\quad u(0) = u(1) = 0.\n$$\nThe exact solution is provided as $u(x) = \\sin(\\pi x)$, which implies the forcing function must be $f(x) = \\pi^2 \\sin(\\pi x)$.\n\nThe core of the finite element method is the conversion of this differential equation into its weak, or variational, formulation. We find $u \\in H_0^1(0,1)$ such that for all test functions $v \\in H_0^1(0,1)$:\n$$\n\\int_0^1 \\frac{du}{dx}(x) \\frac{dv}{dx}(x) \\, dx = \\int_0^1 f(x) v(x) \\, dx.\n$$\nThis is achieved by multiplying the original equation by a test function $v$, integrating over the domain $(0,1)$, and applying integration by parts, incorporating the boundary conditions. We define the bilinear form $a(u,v) = \\int_0^1 u'v' \\,dx$ and the linear form $L(v) = \\int_0^1 fv \\,dx$. The problem is then to find $u \\in H_0^1(0,1)$ such that $a(u,v) = L(v)$ for all $v \\in H_0^1(0,1)$.\n\nTo solve this numerically, we introduce a finite-dimensional subspace $V_h^k \\subset H_0^1(0,1)$. The domain $(0,1)$ is partitioned into $N$ uniform elements of size $h=1/N$. The space $V_h^k$ consists of continuous functions that are piecewise polynomials of degree at most $k$ on this partition. The subscript $h$ indicates the dependence on the mesh size. The condition $V_h^k \\subset H_0^1(0,1)$ implies that any function in this space must be zero at the boundaries $x=0$ and $x=1$.\n\nThe Galerkin method seeks an approximate solution $u_h \\in V_h^k$ such that\n$$\na(u_h, v_h) = L(v_h) \\quad \\text{for all } v_h \\in V_h^k.\n$$\nLet $\\{\\phi_j\\}_{j=1}^{M}$ be a set of basis functions for $V_h^k$, where $M$ is the dimension of the space (the number of interior degrees of freedom). The approximate solution can be written as a linear combination of these basis functions:\n$$\nu_h(x) = \\sum_{j=1}^{M} c_j \\phi_j(x),\n$$\nwhere $c_j$ are unknown coefficients. Substituting this into the Galerkin formulation and choosing the test functions to be the basis functions themselves ($v_h = \\phi_i$ for $i=1,\\dots,M$), we obtain a system of linear algebraic equations:\n$$\n\\sum_{j=1}^{M} c_j a(\\phi_j, \\phi_i) = L(\\phi_i) \\quad \\text{for } i=1,\\dots,M.\n$$\nThis is a linear system $A\\mathbf{c} = \\mathbf{b}$, where the entries of the stiffness matrix $A$ are $A_{ij} = a(\\phi_j, \\phi_i)$, the entries of the load vector $\\mathbf{b}$ are $b_i = L(\\phi_i)$, and $\\mathbf{c} = [c_1, \\dots, c_M]^T$ is the vector of unknown coefficients.\n\nThe practical implementation involves the following steps:\n\n1.  **Assembly:** The integrals defining the entries of $A$ and $\\mathbf{b}$ are computed element by element. This is done by mapping each physical element $[x_e, x_{e+1}]$ to a canonical reference element, say $\\hat{I}=[-1,1]$. The basis functions are defined on this reference element. For degree $k$, there are $k+1$ local basis functions (Lagrange polynomials).\n    The integrals are transformed:\n    $$\n    A_{ij}^e = \\int_{x_e}^{x_{e+1}} \\frac{d\\phi_j}{dx} \\frac{d\\phi_i}{dx} \\, dx, \\quad b_i^e = \\int_{x_e}^{x_{e+1}} f(x) \\phi_i(x) \\, dx.\n    $$\n    Using a coordinate transform $x(\\xi) = x_e + \\frac{h}{2}(\\xi+1)$, these become integrals over $\\xi \\in [-1,1]$. Since the integrands may not be simple polynomials, numerical quadrature, specifically Gaussian quadrature, is used for accurate evaluation. The element-wise contributions $A_{ij}^e$ and $b_i^e$ are then added to the global system $A\\mathbf{c} = \\mathbf{b}$ according to a mapping between local and global-node indices.\n\n2.  **Solving:** The resulting linear system is symmetric and positive-definite. It can be solved for the coefficient vector $\\mathbf{c}$ using standard numerical linear algebra routines. Once $\\mathbf{c}$ is known, the approximate solution $u_h(x)$ is fully determined.\n\n3.  **Error Calculation:** The $L_2$ error, $E(h)$, is computed to assess the accuracy of the solution:\n    $$\n    E(h) = \\| u - u_h \\|_{L_2(0,1)} = \\left( \\int_0^1 (u(x) - u_h(x))^2 \\, dx \\right)^{1/2}.\n    $$\n    This integral is also computed numerically using element-wise Gaussian quadrature of sufficient order to ensure accuracy.\n\n4.  **Convergence Rate Estimation:** A priori error estimates for the FEM predict that, for a sufficiently smooth solution $u$, the $L_2$ error converges as $E(h) \\approx C h^{k+1}$ for some constant $C$ as $h \\to 0$. To verify this numerically, we take the logarithm of this relation:\n    $$\n    \\log E(h) \\approx \\log C + (k+1) \\log h.\n    $$\n    This shows a linear relationship between $\\log E(h)$ and $\\log h$. For a set of computed errors $\\{E(h_i)\\}$ corresponding to a set of mesh sizes $\\{h_i\\}$, we perform a linear least-squares fit on the data points $(\\log h_i, \\log E(h_i))$. The slope of the resulting line is the observed order of convergence, $r$. The problem defines $r$ as the value that minimizes the sum of squared residuals $\\sum_i (\\log E(h_i) - (a + r \\log h_i))^2$. This is a standard linear regression problem.\n\nThe computational procedure described above is executed for each test case specified in the problem statement. The resulting rate $r$ is then compared against the theoretical expectation $k+1$ using the provided tolerance $\\delta$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_legendre\nfrom typing import List, Dict, Any, Callable, Tuple\n\ndef get_basis_functions(k: int) - Tuple[int, np.ndarray, Callable, Callable]:\n    \"\"\"\n    Returns Lagrange basis functions and their derivatives on the reference element [-1, 1].\n\n    Args:\n        k: The polynomial degree (1 or 2).\n\n    Returns:\n        A tuple containing:\n        - Number of local basis functions.\n        - Nodal points on the reference element.\n        - A function that evaluates all basis functions at a point xi.\n        - A function that evaluates all basis function derivatives at a point xi.\n    \"\"\"\n    if k == 1:\n        num_basis = 2\n        nodes = np.array([-1.0, 1.0])\n        def basis_vals(xi: float) - np.ndarray:\n            return np.array([(1.0 - xi) / 2.0, (1.0 + xi) / 2.0])\n        def basis_ders(xi: float) - np.ndarray:\n            # Derivatives are constant for linear basis functions\n            return np.array([-0.5, 0.5])\n        return num_basis, nodes, basis_vals, basis_ders\n    elif k == 2:\n        num_basis = 3\n        nodes = np.array([-1.0, 0.0, 1.0])\n        def basis_vals(xi: float) - np.ndarray:\n            return np.array([\n                xi * (xi - 1.0) / 2.0,\n                1.0 - xi**2,\n                xi * (xi + 1.0) / 2.0\n            ])\n        def basis_ders(xi: float) - np.ndarray:\n            return np.array([\n                xi - 0.5,\n                -2.0 * xi,\n                xi + 0.5\n            ])\n        return num_basis, nodes, basis_vals, basis_ders\n    else:\n        raise ValueError(\"This implementation only supports polynomial degrees k=1 and k=2.\")\n\ndef solve_fem_poisson_1d(k: int, N: int, num_quad_points: int = 5) - float:\n    \"\"\"\n    Solves the 1D Poisson problem using the Finite Element Method.\n\n    Args:\n        k: The polynomial degree of the basis functions.\n        N: The number of elements in the uniform mesh.\n        num_quad_points: The number of points for Gaussian quadrature.\n\n    Returns:\n        The L2 error of the numerical solution.\n    \"\"\"\n    h = 1.0 / N\n    f = lambda x: np.pi**2 * np.sin(np.pi * x)\n    u_exact = lambda x: np.sin(np.pi * x)\n\n    num_local_basis, _, ref_basis_vals, ref_basis_ders = get_basis_functions(k)\n    num_dofs = k * N - 1\n\n    A = np.zeros((num_dofs, num_dofs))\n    b = np.zeros(num_dofs)\n\n    xi_q, w_q = roots_legendre(num_quad_points)\n\n    # Assembly loop over elements\n    for e in range(N):\n        x_e = e * h\n        A_elem = np.zeros((num_local_basis, num_local_basis))\n        b_elem = np.zeros(num_local_basis)\n\n        # Quadrature loop for element matrix and vector\n        for i in range(num_quad_points):\n            xi, w = xi_q[i], w_q[i]\n            x_phys = x_e + (h / 2.0) * (xi + 1.0)\n            jacobian = h / 2.0\n            \n            grads = ref_basis_ders(xi)\n            vals = ref_basis_vals(xi)\n            \n            # Element stiffness matrix\n            for p in range(num_local_basis):\n                for q in range(num_local_basis):\n                    A_elem[p, q] += (grads[q] / jacobian) * (grads[p] / jacobian) * w * jacobian\n            \n            # Element load vector\n            for p in range(num_local_basis):\n                b_elem[p] += f(x_phys) * vals[p] * w * jacobian\n\n        # Map local contributions to global system\n        for p_loc in range(num_local_basis):\n            p_glob_node = k * e + p_loc\n            if 0  p_glob_node  k * N:\n                p_dof = p_glob_node - 1\n                b[p_dof] += b_elem[p_loc]\n                for q_loc in range(num_local_basis):\n                    q_glob_node = k * e + q_loc\n                    if 0  q_glob_node  k * N:\n                        q_dof = q_glob_node - 1\n                        A[p_dof, q_dof] += A_elem[p_loc, q_loc]\n    \n    # Solve for coefficients\n    coeffs = np.linalg.solve(A, b)\n\n    # Compute L2 error\n    error_sq = 0.0\n    for e in range(N):\n        x_e = e * h\n        for i in range(num_quad_points):\n            xi, w = xi_q[i], w_q[i]\n            x_phys = x_e + (h / 2.0) * (xi + 1.0)\n            jacobian = h / 2.0\n            \n            u_val = u_exact(x_phys)\n            \n            uh_val = 0.0\n            vals = ref_basis_vals(xi)\n            for p_loc in range(num_local_basis):\n                p_glob_node = k * e + p_loc\n                node_coeff = 0.0\n                if 0  p_glob_node  k * N:\n                    p_dof = p_glob_node - 1\n                    node_coeff = coeffs[p_dof]\n                uh_val += node_coeff * vals[p_loc]\n            \n            error_sq += (u_val - uh_val)**2 * w * jacobian\n\n    return np.sqrt(error_sq)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'id': 'A', 'k': 1, 'N_list': [10, 20, 40, 80, 160], 'delta': 0.1},\n        {'id': 'B', 'k': 2, 'N_list': [8, 16, 32, 64], 'delta': 0.2},\n        {'id': 'C', 'k': 1, 'N_list': [2, 4, 8, 16], 'delta': 0.4}\n    ]\n\n    rates = {}\n    flags = {}\n\n    for case in test_cases:\n        k = case['k']\n        N_list = case['N_list']\n        delta = case['delta']\n        \n        h_values = 1.0 / np.array(N_list)\n        error_values = np.array([solve_fem_poisson_1d(k, N) for N in N_list])\n        \n        log_h = np.log(h_values)\n        log_e = np.log(error_values)\n        \n        # Calculate convergence rate r via linear regression slope\n        r, _ = np.polyfit(log_h, log_e, 1)\n        \n        # Check against theoretical rate k+1 with tolerance delta\n        passes_check = r = (k + 1.0 - delta)\n        \n        rates[case['id']] = r\n        flags[case['id']] = passes_check\n\n    # Assemble final list in the specified order: [r_A, r_B, r_C, b_A, b_B, b_C]\n    final_results = [\n        rates['A'], rates['B'], rates['C'],\n        flags['A'], flags['B'], flags['C']\n    ]\n    \n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2395840"}]}