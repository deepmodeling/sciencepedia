{"hands_on_practices": [{"introduction": "While theoretical formulas for the inverse exist, in computational practice, we rely on robust and efficient algorithms. This exercise guides you through implementing the Gauss-Jordan elimination method, the workhorse for solving linear systems and inverting matrices. By augmenting a matrix $A$ with the identity matrix $I$ and performing row operations to transform $A$ into $I$, you will simultaneously transform $I$ into $A^{-1}$, gaining a tangible understanding of how an inverse is computationally constructed and what happens when a matrix is singular [@problem_id:2400410].", "problem": "You are asked to implement a robust numerical procedure for determining whether a square matrix is invertible and, if so, computing its inverse, using only row operations. The method must be based on the Gauss–Jordan elimination procedure with partial pivoting.\n\nStart from foundational concepts in linear algebra: an $n \\times n$ matrix $A$ is invertible if and only if there exists a matrix $B$ such that $AB = BA = I_n$, where $I_n$ is the $n \\times n$ identity matrix. Elementary row operations correspond to left-multiplication by invertible elementary matrices. If a finite sequence of elementary row operations transforms $A$ into $I_n$, then the same transformations applied to $I_n$ yield $A^{-1}$. Conversely, if no such sequence exists because a pivot cannot be found at some step (that is, all entries in the pivot column below and including the current row are zero in exact arithmetic), then $A$ is singular.\n\nImplement the Gauss–Jordan algorithm as follows: given an $n \\times n$ real matrix $A$, construct the augmented matrix $[A \\mid I_n]$. For each column index $j \\in \\{0,1,\\dots,n-1\\}$, select as pivot the row $p \\in \\{j,\\dots,n-1\\}$ where the absolute value of the entry in column $j$ is maximal (partial pivoting). Swap rows $j$ and $p$. If the absolute value of the pivot is below a small tolerance $\\tau$, declare $A$ singular and stop. Otherwise, scale row $j$ to make the pivot equal to $1$, then eliminate all other entries in column $j$ by subtracting suitable multiples of row $j$ from the other rows. When the left block has been reduced to $I_n$, the right block equals $A^{-1}$.\n\nNumerical specification:\n- Use a pivot tolerance $\\tau = 10^{-12}$ to decide whether a pivot is effectively zero.\n- Use partial pivoting as described to improve numerical stability.\n- If the matrix is declared invertible, return the inverse as a flattened list in row-major order, with each entry rounded to six decimal places. If the matrix is singular, return an empty list for the inverse.\n\nTest suite:\nCompute the result for each of the following matrices:\n- Case $1$ (happy path, $3 \\times 3$): \n$$\nA_1 = \\begin{bmatrix}\n2 & 1 & 1 \\\\\n1 & 3 & 2 \\\\\n1 & 0 & 0\n\\end{bmatrix}.\n$$\n- Case $2$ (singular, $3 \\times 3$):\n$$\nA_2 = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n1 & 0 & 1\n\\end{bmatrix}.\n$$\n- Case $3$ (identity, $4 \\times 4$):\n$$\nA_3 = \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n- Case $4$ (requires row swap, $2 \\times 2$):\n$$\nA_4 = \\begin{bmatrix}\n0 & 1\\\\\n2 & 3\n\\end{bmatrix}.\n$$\n- Case $5$ (ill-conditioned but invertible, $2 \\times 2$):\n$$\nA_5 = \\begin{bmatrix}\n1 & 1\\\\\n1 & 1 + 10^{-10}\n\\end{bmatrix}.\n$$\n\nFinal output format:\n- For each case $k \\in \\{1,2,3,4,5\\}$, your program must produce a result of the form $[s_k, v_k]$ where $s_k$ is a boolean indicating whether $A_k$ is invertible under the tolerance $\\tau$, and $v_k$ is the row-major flattened inverse as a list of floats rounded to six decimal places when $s_k$ is true, or an empty list when $s_k$ is false.\n- Aggregate the five case results into a single list and print exactly one line containing this list, with items comma-separated and enclosed in square brackets, for example: \n\"[ [True,[...]], [False,[]], ... ]\".\nNo angles or physical units are involved in this task. All numerical answers are pure numbers. Ensure that floats are rounded to six decimal places in the output representation.", "solution": "The problem is determined to be valid. It is scientifically grounded in the principles of linear algebra, specifically the Gauss-Jordan elimination method for matrix inversion. The problem is well-posed, providing a complete algorithmic description, specific numerical parameters, and unambiguous test cases. The language used is objective and precise, avoiding any subjective or non-formalizable claims.\n\nThe task is to implement a procedure for computing the inverse of a square matrix $A$ of size $n \\times n$. The fundamental principle is that an elementary row operation on a matrix is equivalent to a left-multiplication by a corresponding elementary matrix. An elementary matrix is invertible. If a sequence of elementary matrices $E_1, E_2, \\dots, E_k$ transforms $A$ into the identity matrix $I_n$, then we have:\n$$ (E_k \\dots E_2 E_1) A = I_n $$\nBy definition of the inverse, this means:\n$$ A^{-1} = E_k \\dots E_2 E_1 $$\nIf we apply the same sequence of transformations to the identity matrix $I_n$, we obtain:\n$$ (E_k \\dots E_2 E_1) I_n = A^{-1} $$\nThis is the theoretical justification for the Gauss-Jordan elimination method on an augmented matrix. We start with the augmented matrix $[A \\mid I_n]$ and apply row operations to transform the left block $A$ into $I_n$. The same operations are simultaneously applied to the right block $I_n$, which is consequently transformed into $A^{-1}$. If at any point the left block cannot be transformed further towards $I_n$ because a non-zero pivot cannot be found, the matrix $A$ is singular and does not have an inverse.\n\nThe algorithm to be implemented is the Gauss-Jordan elimination with partial pivoting for numerical stability.\n\n**Algorithmic Steps:**\n\n1.  **Initialization**: Given an $n \\times n$ matrix $A$, construct the $n \\times 2n$ augmented matrix $M = [A \\mid I_n]$, where $I_n$ is the $n \\times n$ identity matrix.\n\n2.  **Forward Elimination and Back Substitution (Gauss-Jordan)**: Iterate through the columns of the left block, indexed by $j$ from $0$ to $n-1$. In each iteration $j$, perform the following steps:\n    a.  **Partial Pivoting**: To improve numerical stability and avoid division by small numbers, find the row index $p$ such that $j \\le p  n$ and $|M_{p,j}|$ is maximized. Swap row $j$ with row $p$. This operation affects the entire augmented matrix.\n    b.  **Singularity Check**: The element $M_{j,j}$ is now the pivot. If the absolute value of the pivot, $|M_{j,j}|$, is smaller than a pre-defined tolerance $\\tau$ (given as $10^{-12}$), the matrix is considered numerically singular. Terminate the algorithm and report that the matrix is not invertible.\n    c.  **Normalization**: Divide the entire row $j$ by the pivot value $M_{j,j}$. This makes the diagonal element $M_{j,j}$ equal to $1$.\n    $$ R_j \\leftarrow \\frac{1}{M_{j,j}} R_j $$\n    d.  **Elimination**: For every other row $i$ (where $i \\ne j$), eliminate the entry in column $j$. This is achieved by subtracting a multiple of the normalized pivot row $R_j$ from row $R_i$. The multiple is given by the value $M_{i,j}$.\n    $$ R_i \\leftarrow R_i - M_{i,j} R_j \\quad \\text{for } i = 0, \\dots, n-1, i \\ne j $$\n\n3.  **Result Extraction**: After iterating through all columns $j$ from $0$ to $n-1$, the left block of the augmented matrix will be the identity matrix $I_n$. The right block will be the inverse matrix $A^{-1}$.\n\n4.  **Output Formatting**:\n    - If the algorithm completed successfully, return a boolean `True` indicating invertibility. The computed inverse matrix $A^{-1}$ is then flattened into a one-dimensional list in row-major order, with each element rounded to six decimal places.\n    - If the matrix was determined to be singular, return a boolean `False` and an empty list.\n\nThis procedure will be applied to each of the five provided test matrices to generate the final output. The use of a small tolerance $\\tau$ is critical for handling floating-point arithmetic, as true zero may not be achieved due to precision errors. Partial pivoting is essential for robustness, especially for matrices that are ill-conditioned (like Case $5$) or have zeros on the diagonal (like Case $4$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_inverse_with_gauss_jordan(A_list, tolerance):\n    \"\"\"\n    Computes the inverse of a square matrix using Gauss-Jordan elimination\n    with partial pivoting.\n\n    Args:\n        A_list (list of lists): The input square matrix.\n        tolerance (float): A small number to check for singularity.\n\n    Returns:\n        tuple: A tuple (is_invertible, inverse_list).\n               is_invertible (bool): True if the matrix is invertible, False otherwise.\n               inverse_list (list): The flattened, row-major inverse rounded to 6\n                                    decimal places, or an empty list if singular.\n    \"\"\"\n    # Create a copy to avoid modifying the original list, and use float type.\n    matrix = np.array(A_list, dtype=float)\n    \n    n, m = matrix.shape\n    if n != m:\n        # This case is not expected based on the problem description.\n        return False, []\n\n    # Construct the augmented matrix [A | I]\n    identity = np.identity(n)\n    aug_matrix = np.hstack([matrix, identity])\n\n    # Iterate through columns (pivots)\n    for j in range(n):\n        # Step 1: Partial Pivoting\n        # Find the row with the largest pivot in the current column below the diagonal\n        pivot_row_idx = j + np.argmax(np.abs(aug_matrix[j:, j]))\n\n        # Swap the current row with the pivot row\n        if pivot_row_idx != j:\n            aug_matrix[[j, pivot_row_idx]] = aug_matrix[[pivot_row_idx, j]]\n\n        # Step 2: Singularity Check\n        pivot_value = aug_matrix[j, j]\n        if abs(pivot_value)  tolerance:\n            return False, []  # Matrix is singular\n\n        # Step 3: Normalization\n        # Divide the pivot row by the pivot value to make the pivot 1\n        aug_matrix[j, :] /= pivot_value\n\n        # Step 4: Elimination\n        # Eliminate all other entries in the current pivot column\n        for i in range(n):\n            if i != j:\n                factor = aug_matrix[i, j]\n                aug_matrix[i, :] -= factor * aug_matrix[j, :]\n\n    # The right part of the augmented matrix is now the inverse\n    inverse_matrix = aug_matrix[:, n:]\n\n    # Flatten the inverse matrix and round to 6 decimal places\n    flat_inverse = inverse_matrix.flatten().tolist()\n    rounded_inverse = [round(val, 6) for val in flat_inverse]\n\n    return True, rounded_inverse\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, 3x3)\n        [[2, 1, 1],\n         [1, 3, 2],\n         [1, 0, 0]],\n        # Case 2 (singular, 3x3)\n        [[1, 2, 3],\n         [2, 4, 6],\n         [1, 0, 1]],\n        # Case 3 (identity, 4x4)\n        [[1, 0, 0, 0],\n         [0, 1, 0, 0],\n         [0, 0, 1, 0],\n         [0, 0, 0, 1]],\n        # Case 4 (requires row swap, 2x2)\n        [[0, 1],\n         [2, 3]],\n        # Case 5 (ill-conditioned but invertible, 2x2)\n        [[1, 1],\n         [1, 1 + 1e-10]],\n    ]\n    \n    # Numerical specification\n    pivot_tolerance = 1e-12\n\n    results = []\n    for case_matrix in test_cases:\n        is_invertible, inverse = compute_inverse_with_gauss_jordan(case_matrix, pivot_tolerance)\n        results.append([is_invertible, inverse])\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list in Python naturally includes spaces\n    # after commas, which aligns with the provided example's styling.\n    # The f-string with join ensures a compact representation between the main list items.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2400410"}, {"introduction": "Beyond algorithmic approaches, a direct analytical formula for the inverse provides deep theoretical insight. This problem focuses on calculating the inverse using the adjugate matrix, defined via cofactors, and the determinant [@problem_id:2400385]. This classic method, $A^{-1} = \\frac{1}{\\det(A)}\\operatorname{adj}(A)$, makes the condition for invertibility crystal clear: the determinant must be non-zero. Working through this calculation by hand solidifies your understanding of the underlying algebraic structure of a matrix inverse.", "problem": "Consider the real $3 \\times 3$ matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2  -1  3\\\\\n0  4  -2\\\\\n1  5  1\n\\end{pmatrix}.\n$$\nA trusted computational tool reports the following candidate for the inverse of $A$:\n$$\nB \\;=\\;\n\\begin{pmatrix}\n\\frac{7}{9}  \\frac{8}{9}  -\\frac{5}{9}\\\\\n-\\frac{1}{9}  -\\frac{1}{18}  \\frac{2}{9}\\\\\n-\\frac{2}{9}  -\\frac{11}{18}  \\frac{4}{9}\n\\end{pmatrix}.\n$$\nUsing the adjugate-based definition of the inverse, where the adjugate matrix $\\operatorname{adj}(A)$ is the transpose of the cofactor matrix of $A$ and $A^{-1} = \\frac{1}{\\det(A)}\\,\\operatorname{adj}(A)$ for $\\det(A) \\neq 0$, compute $A^{-1}$ and verify it against $B$ via the identity $A A^{-1} = I_3$. Then report the scalar trace $\\operatorname{tr}(A^{-1})$. Provide the exact value (no rounding).", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The matrix $A$ is given as:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2  -1  3\\\\\n0  4  -2\\\\\n1  5  1\n\\end{pmatrix}\n$$\n- A candidate for the inverse of $A$ is given as:\n$$\nB \\;=\\;\n\\begin{pmatrix}\n\\frac{7}{9}  \\frac{8}{9}  -\\frac{5}{9}\\\\\n-\\frac{1}{9}  -\\frac{1}{18}  \\frac{2}{9}\\\\\n-\\frac{2}{9}  -\\frac{11}{18}  \\frac{4}{9}\n\\end{pmatrix}\n$$\n- The method for finding the inverse is specified: $A^{-1} = \\frac{1}{\\det(A)}\\,\\operatorname{adj}(A)$, where $\\operatorname{adj}(A)$ is the transpose of the cofactor matrix of $A$.\n- The task is to compute $A^{-1}$, verify it against $B$, verify the identity $A A^{-1} = I_3$, and report the trace $\\operatorname{tr}(A^{-1})$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in linear algebra, a core component of computational engineering. All definitions are standard, and the data provided are sufficient and consistent. The problem does not violate any scientific principles, is not ambiguous, and its solution is verifiable through computation.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe task is to compute the inverse of the matrix $A$ using the adjugate method. The inverse of a non-singular square matrix $A$ is given by the formula:\n$$\nA^{-1} = \\frac{1}{\\det(A)}\\operatorname{adj}(A)\n$$\nwhere $\\operatorname{adj}(A)$ is the adjugate of $A$, which is the transpose of its cofactor matrix.\n\nFirst, we must compute the determinant of $A$, $\\det(A)$. We use cofactor expansion along the first column, as it contains a zero entry which simplifies calculation.\n$$\n\\det(A) = 2 \\begin{vmatrix} 4  -2 \\\\ 5  1 \\end{vmatrix} - 0 \\begin{vmatrix} -1  3 \\\\ 5  1 \\end{vmatrix} + 1 \\begin{vmatrix} -1  3 \\\\ 4  -2 \\end{vmatrix}\n$$\n$$\n\\det(A) = 2(4 \\cdot 1 - (-2) \\cdot 5) + 1((-1) \\cdot (-2) - 3 \\cdot 4)\n$$\n$$\n\\det(A) = 2(4 + 10) + (2 - 12) = 2(14) - 10 = 28 - 10 = 18\n$$\nSince $\\det(A)=18 \\neq 0$, the matrix $A$ is invertible.\n\nNext, we compute the cofactor matrix, $C$. Each element $C_{ij}$ is given by $(-1)^{i+j} M_{ij}$, where $M_{ij}$ is the minor of the element $a_{ij}$.\n$$\nC_{11} = (-1)^{1+1} \\begin{vmatrix} 4  -2 \\\\ 5  1 \\end{vmatrix} = 1(4 - (-10)) = 14\n$$\n$$\nC_{12} = (-1)^{1+2} \\begin{vmatrix} 0  -2 \\\\ 1  1 \\end{vmatrix} = -1(0 - (-2)) = -2\n$$\n$$\nC_{13} = (-1)^{1+3} \\begin{vmatrix} 0  4 \\\\ 1  5 \\end{vmatrix} = 1(0 - 4) = -4\n$$\n$$\nC_{21} = (-1)^{2+1} \\begin{vmatrix} -1  3 \\\\ 5  1 \\end{vmatrix} = -1(-1 - 15) = 16\n$$\n$$\nC_{22} = (-1)^{2+2} \\begin{vmatrix} 2  3 \\\\ 1  1 \\end{vmatrix} = 1(2 - 3) = -1\n$$\n$$\nC_{23} = (-1)^{2+3} \\begin{vmatrix} 2  -1 \\\\ 1  5 \\end{vmatrix} = -1(10 - (-1)) = -11\n$$\n$$\nC_{31} = (-1)^{3+1} \\begin{vmatrix} -1  3 \\\\ 4  -2 \\end{vmatrix} = 1(2 - 12) = -10\n$$\n$$\nC_{32} = (-1)^{3+2} \\begin{vmatrix} 2  3 \\\\ 0  -2 \\end{vmatrix} = -1(-4 - 0) = 4\n$$\n$$\nC_{33} = (-1)^{3+3} \\begin{vmatrix} 2  -1 \\\\ 0  4 \\end{vmatrix} = 1(8 - 0) = 8\n$$\nThe cofactor matrix is:\n$$\nC = \\begin{pmatrix}\n14  -2  -4\\\\\n16  -1  -11\\\\\n-10  4  8\n\\end{pmatrix}\n$$\nThe adjugate matrix is the transpose of the cofactor matrix, $\\operatorname{adj}(A) = C^T$.\n$$\n\\operatorname{adj}(A) = \\begin{pmatrix}\n14  16  -10\\\\\n-2  -1  4\\\\\n-4  -11  8\n\\end{pmatrix}\n$$\nNow we construct the inverse matrix $A^{-1}$:\n$$\nA^{-1} = \\frac{1}{18} \\begin{pmatrix}\n14  16  -10\\\\\n-2  -1  4\\\\\n-4  -11  8\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{14}{18}  \\frac{16}{18}  -\\frac{10}{18}\\\\\n-\\frac{2}{18}  -\\frac{1}{18}  \\frac{4}{18}\\\\\n-\\frac{4}{18}  -\\frac{11}{18}  \\frac{8}{18}\n\\end{pmatrix}\n$$\nSimplifying the fractions gives:\n$$\nA^{-1} = \\begin{pmatrix}\n\\frac{7}{9}  \\frac{8}{9}  -\\frac{5}{9}\\\\\n-\\frac{1}{9}  -\\frac{1}{18}  \\frac{2}{9}\\\\\n-\\frac{2}{9}  -\\frac{11}{18}  \\frac{4}{9}\n\\end{pmatrix}\n$$\nThis computed inverse matches the provided candidate matrix $B$ exactly.\n\nTo verify the identity $A A^{-1} = I_3$, we perform the multiplication. It is more convenient to compute $A \\cdot \\operatorname{adj}(A)$ first. From theory, we know $A \\cdot \\operatorname{adj}(A) = \\det(A) \\cdot I_3$.\n$$\nA \\cdot \\operatorname{adj}(A) = \\begin{pmatrix}\n2  -1  3\\\\\n0  4  -2\\\\\n1  5  1\n\\end{pmatrix}\n\\begin{pmatrix}\n14  16  -10\\\\\n-2  -1  4\\\\\n-4  -11  8\n\\end{pmatrix}\n$$\nThe $(1,1)$ entry is $2(14) + (-1)(-2) + 3(-4) = 28 + 2 - 12 = 18$.\nThe $(2,2)$ entry is $0(16) + 4(-1) + (-2)(-11) = -4 + 22 = 18$.\nThe $(3,3)$ entry is $1(-10) + 5(4) + 1(8) = -10 + 20 + 8 = 18$.\nThe off-diagonal entries are all zero, as expected. For example, the $(1,2)$ entry is $2(16) + (-1)(-1) + 3(-11) = 32 + 1 - 33 = 0$.\nThus, the product is:\n$$\nA \\cdot \\operatorname{adj}(A) = \\begin{pmatrix}\n18  0  0\\\\\n0  18  0\\\\\n0  0  18\n\\end{pmatrix} = 18 \\cdot I_3\n$$\nTherefore, $A A^{-1} = A \\left( \\frac{1}{18}\\operatorname{adj}(A) \\right) = \\frac{1}{18} (A \\cdot \\operatorname{adj}(A)) = \\frac{1}{18}(18 \\cdot I_3) = I_3$. The calculation is correct.\n\nFinally, we compute the trace of $A^{-1}$, which is the sum of its diagonal elements.\n$$\n\\operatorname{tr}(A^{-1}) = \\frac{7}{9} + \\left(-\\frac{1}{18}\\right) + \\frac{4}{9}\n$$\nTo sum these fractions, we use a common denominator of $18$.\n$$\n\\operatorname{tr}(A^{-1}) = \\frac{14}{18} - \\frac{1}{18} + \\frac{8}{18} = \\frac{14 - 1 + 8}{18} = \\frac{21}{18}\n$$\nSimplifying the fraction by dividing the numerator and denominator by their greatest common divisor, $3$, gives:\n$$\n\\operatorname{tr}(A^{-1}) = \\frac{7}{6}\n$$", "answer": "$$\\boxed{\\frac{7}{6}}$$", "id": "2400385"}, {"introduction": "What happens when a matrix is not just singular, but also lacks a full set of linearly independent eigenvectors? This exercise challenges you to construct and analyze such a matrix—one that is both singular and non-diagonalizable [@problem_id:2400386]. By exploring the relationship between a matrix's eigenvalues, its characteristic polynomial, and the dimensions of its eigenspaces, you will uncover the necessity for the Jordan Normal Form. This practice provides a concrete look at the canonical structure of any linear transformation, which is essential for advanced topics like solving systems of differential equations via the matrix exponential.", "problem": "Let $\\mathbb{R}$ denote the field of real numbers. Construct an explicit real $3 \\times 3$ matrix $A$ that is singular (i.e., not invertible) and not diagonalizable over $\\mathbb{R}$, with characteristic polynomial $p_{A}(\\lambda)=\\lambda^{2}(\\lambda-1)$. Using only first principles (definitions of eigenvalues, eigenvectors, characteristic polynomial, minimal polynomial, and similarity), verify that your $A$ satisfies these properties and determine its Jordan normal form (JNF). Define $S=\\operatorname{tr}(\\exp(A))$, where $\\exp(A)$ is the matrix exponential. Provide $S$ as your final answer in exact form. No rounding is required and no units are involved.", "solution": "The problem statement is subjected to validation.\n\nGivens:\n1.  The matrix $A$ is a $3 \\times 3$ matrix with entries in $\\mathbb{R}$.\n2.  $A$ is singular.\n3.  $A$ is not diagonalizable over $\\mathbb{R}$.\n4.  The characteristic polynomial is $p_{A}(\\lambda)=\\lambda^{2}(\\lambda-1)$.\n5.  A verification using first principles is required.\n6.  The Jordan Normal Form (JNF) must be determined.\n7.  The quantity $S=\\operatorname{tr}(\\exp(A))$ must be calculated.\n\nValidation:\nThe problem is mathematically well-posed and self-consistent. The characteristic polynomial $p_{A}(\\lambda)=\\lambda^{2}(\\lambda-1)$ dictates the eigenvalues are $\\lambda_1 = 1$ with algebraic multiplicity $m_a(1) = 1$ and $\\lambda_2 = 0$ with algebraic multiplicity $m_a(0) = 2$.\nThe singularity of $A$ is a direct consequence of having an eigenvalue of $0$, since $\\det(A)$ is the product of its eigenvalues, which is $1 \\cdot 0 \\cdot 0 = 0$.\nThe condition of non-diagonalizability implies that for at least one eigenvalue, its geometric multiplicity $m_g(\\lambda)$ must be strictly less than its algebraic multiplicity $m_a(\\lambda)$. For $\\lambda=1$, we must have $1 \\le m_g(1) \\le m_a(1)=1$, so $m_g(1)=1$. For $\\lambda=0$, with $m_a(0)=2$, non-diagonalizability requires $m_g(0)  2$. Since an eigenspace cannot be zero-dimensional, $m_g(0)=1$. These conditions are consistent and sufficient to uniquely determine the Jordan normal form of such a matrix. The problem is scientifically grounded, objective, and its solution is verifiable.\n\nVerdict: The problem is valid.\n\nThe solution proceeds as follows. First, we construct a suitable matrix $A$. The conditions on the eigenvalues and their multiplicities determine the structure of the Jordan Normal Form (JNF) of $A$. As established, the eigenvalues are $\\lambda_1=1$ ($m_a=1, m_g=1$) and $\\lambda_2=0$ ($m_a=2, m_g=1$). The number of Jordan blocks corresponding to an eigenvalue is its geometric multiplicity.\nTherefore, there is one Jordan block for $\\lambda=1$, which must be of size $1 \\times 1$: $J_1(1) = [1]$.\nThere is one Jordan block for $\\lambda=0$, which must be of size $2 \\times 2$: $J_2(0) = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$.\nThe JNF of $A$, denoted $J$, is the block diagonal matrix formed by these blocks. We may write it as:\n$$ J = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} $$\nAny matrix $A$ satisfying the given conditions must be similar to this matrix $J$. For simplicity, we construct the most straightforward example by choosing $A=J$.\n$$ A = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} $$\nWe now verify that this matrix $A$ satisfies all the required properties using first principles.\n\n1.  **Characteristic Polynomial**: By definition, the characteristic polynomial is $p_A(\\lambda)=\\det(A-\\lambda I)$.\n    $$ p_A(\\lambda) = \\det \\begin{pmatrix} 1-\\lambda  0  0 \\\\ 0  -\\lambda  1 \\\\ 0  0  -\\lambda \\end{pmatrix} = (1-\\lambda)((-\\lambda)(-\\lambda) - 1 \\cdot 0) = (1-\\lambda)\\lambda^2 $$\n    The problem statement gives $p_A(\\lambda)=\\lambda^2(\\lambda-1)$. If we define the characteristic polynomial as $\\det(\\lambda I - A)$, which is a common convention to ensure it is monic, we get $(\\lambda-1)\\lambda^2$. Our matrix $A$ satisfies the condition.\n\n2.  **Singularity**: A matrix is singular if and only if its determinant is zero.\n    $$ \\det(A) = \\det \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = 1 \\cdot (0 \\cdot 0 - 1 \\cdot 0) = 0 $$\n    Since $\\det(A)=0$, the matrix $A$ is singular.\n\n3.  **Non-diagonalizability**: A matrix is diagonalizable if and only if, for every eigenvalue, its geometric and algebraic multiplicities are equal.\n    The eigenvalues are the roots of $\\lambda^2(1-\\lambda)=0$, which are $\\lambda=0$ (algebraic multiplicity $m_a(0)=2$) and $\\lambda=1$ (algebraic multiplicity $m_a(1)=1$).\n    The geometric multiplicity $m_g(\\lambda)$ is the dimension of the null space of $(A-\\lambda I)$.\n    For $\\lambda=0$: $m_g(0) = \\dim(\\ker(A-0I)) = \\dim(\\ker(A))$. We find the null space by solving $Av=0$:\n    $$ \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies v_1=0, v_3=0 $$\n    The eigenvectors are of the form $v = \\begin{pmatrix} 0 \\\\ v_2 \\\\ 0 \\end{pmatrix} = v_2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$. The eigenspace is spanned by a single vector, so its dimension is $m_g(0)=1$.\n    Since $m_g(0)=1  m_a(0)=2$, the matrix $A$ is not diagonalizable.\n    This can also be confirmed with the minimal polynomial, $m_A(\\lambda)$. For $A$ to be non-diagonalizable, $m_A(\\lambda)$ must have repeated roots. The minimal polynomial must divide the characteristic polynomial $p_A(\\lambda) = \\lambda^2(\\lambda-1)$. The candidates are $\\lambda(\\lambda-1)$ and $\\lambda^2(\\lambda-1)$.\n    We test $m(\\lambda)=\\lambda(\\lambda-1)$:\n    $$ m(A) = A(A-I) = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 0  0  0 \\\\ 0  -1  1 \\\\ 0  0  -1 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  0  -1 \\\\ 0  0  0 \\end{pmatrix} \\neq 0 $$\n    Therefore, the minimal polynomial must be $m_A(\\lambda)=\\lambda^2(\\lambda-1)$. The presence of the repeated root $\\lambda^2$ confirms that $A$ is not diagonalizable.\n\nThe Jordan Normal Form of $A$ has been determined to be the matrix $J$ above. Since we chose $A=J$, the JNF of our $A$ is $A$ itself.\n\nFinally, we must compute $S = \\operatorname{tr}(\\exp(A))$. Since $A$ is the JNF, we compute $\\exp(A)$ directly. Exponentiation of a block diagonal matrix results in a block diagonal matrix of the exponentials of the blocks.\n$$ A = J_1(1) \\oplus J_2(0) = [1] \\oplus \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} $$\n$$ \\exp(A) = \\exp(J_1(1)) \\oplus \\exp(J_2(0)) $$\nFor the first block:\n$$ \\exp(J_1(1)) = [\\exp(1)] $$\nFor the second block, $J_2(0)$ is a nilpotent matrix $N = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$ with $N^2=0$.\n$$ \\exp(N) = \\sum_{k=0}^{\\infty} \\frac{N^k}{k!} = \\frac{N^0}{0!} + \\frac{N^1}{1!} + \\frac{N^2}{2!} + \\dots = I + N + 0 + \\dots $$\n$$ \\exp(J_2(0)) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} $$\nCombining the blocks, we obtain the matrix exponential:\n$$ \\exp(A) = \\begin{pmatrix} \\exp(1)  0  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} $$\nThe trace, $S$, is the sum of the diagonal elements of this matrix.\n$$ S = \\operatorname{tr}(\\exp(A)) = \\exp(1) + 1 + 1 = \\exp(1) + 2 $$\nThis result is exact as required.", "answer": "$$\\boxed{\\exp(1) + 2}$$", "id": "2400386"}]}