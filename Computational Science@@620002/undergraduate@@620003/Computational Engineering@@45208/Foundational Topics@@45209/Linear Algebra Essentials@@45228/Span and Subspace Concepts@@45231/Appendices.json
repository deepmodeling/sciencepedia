{"hands_on_practices": [{"introduction": "Many problems in computational engineering are defined by constraints, such as the conservation of mass or energy. These constraints often define a subspace of valid solutions. This first exercise [@problem_id:2435999] is a fundamental practice in working from first principles: you will take a constraint equation, prove that the set of all vectors satisfying it forms a subspace, and then construct an explicit basis for it. This process solidifies the core definitions of span and linear independence and demonstrates how a high-dimensional space can be precisely characterized by just a few basis vectors.", "problem": "In a mass-conserving preprocessing step for a $5$-node discretization in computational engineering, a correction vector $\\mathbf{x} \\in \\mathbb{R}^{5}$ must satisfy the balance constraint that the sum of its components is zero. Let\n$$\nS \\equiv \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{5} \\,\\middle|\\, \\sum_{i=1}^{5} x_{i} = 0 \\right\\}.\n$$\nStarting only from the core definitions of subspace, span, and linear independence, and without invoking any prepackaged theorems beyond these definitions, do the following:\n1. Justify from first principles that $S$ is a subspace of $\\mathbb{R}^{5}$.\n2. Construct an explicit basis for $S$ by expressing a general vector in $S$ in terms of free components and rewriting it as a linear combination of a minimal set of fixed vectors. Establish minimality using the definition of linear independence.\n3. Using your construction, determine the dimension of $S$.\n\nReport only the dimension of $S$ as your final answer. No rounding is required and no units are involved.", "solution": "We begin with the definitions. A subset $U \\subseteq \\mathbb{R}^{n}$ is a subspace if and only if it contains the zero vector and is closed under addition and scalar multiplication. A set of vectors $\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k} \\}$ spans a subspace $U$ if every $\\mathbf{u} \\in U$ can be written as a linear combination of these vectors. Such a set is linearly independent if the only coefficients $a_{1},\\dots,a_{k}$ satisfying $a_{1}\\mathbf{v}_{1} + \\cdots + a_{k}\\mathbf{v}_{k} = \\mathbf{0}$ are $a_{1} = \\cdots = a_{k} = 0$. A basis is a spanning, linearly independent set, and its cardinality is the dimension.\n\nStep 1: Show that $S$ is a subspace. First, the zero vector $\\mathbf{0} \\in \\mathbb{R}^{5}$ has components summing to $0$, hence $\\mathbf{0} \\in S$. Let $\\mathbf{x}, \\mathbf{y} \\in S$ so that $\\sum_{i=1}^{5} x_{i} = 0$ and $\\sum_{i=1}^{5} y_{i} = 0$. Then\n$$\n\\sum_{i=1}^{5} (x_{i} + y_{i}) = \\sum_{i=1}^{5} x_{i} + \\sum_{i=1}^{5} y_{i} = 0 + 0 = 0,\n$$\nso $\\mathbf{x} + \\mathbf{y} \\in S$. For any scalar $\\alpha \\in \\mathbb{R}$ and $\\mathbf{x} \\in S$,\n$$\n\\sum_{i=1}^{5} (\\alpha x_{i}) = \\alpha \\sum_{i=1}^{5} x_{i} = \\alpha \\cdot 0 = 0,\n$$\nso $\\alpha \\mathbf{x} \\in S$. Therefore $S$ is a subspace of $\\mathbb{R}^{5}$.\n\nStep 2: Construct a basis. Let $\\mathbf{x} = (x_{1}, x_{2}, x_{3}, x_{4}, x_{5})^{\\top} \\in S$. The defining constraint is\n$$\nx_{1} + x_{2} + x_{3} + x_{4} + x_{5} = 0.\n$$\nWe can solve for one component in terms of the others using only algebra. Take $x_{5}$ as dependent:\n$$\nx_{5} = - (x_{1} + x_{2} + x_{3} + x_{4}).\n$$\nThen every $\\mathbf{x} \\in S$ can be parameterized by the four free variables $x_{1}, x_{2}, x_{3}, x_{4} \\in \\mathbb{R}$ as\n$$\n\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ - (x_{1} + x_{2} + x_{3} + x_{4}) \\end{pmatrix}\n= x_{1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{3} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{4} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nDefine the four vectors\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nBy the parameterization, any $\\mathbf{x} \\in S$ is a linear combination of $\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4} \\}$, so these vectors span $S$.\n\nTo verify linear independence using the definition, suppose\n$$\na_{1} \\mathbf{v}_{1} + a_{2} \\mathbf{v}_{2} + a_{3} \\mathbf{v}_{3} + a_{4} \\mathbf{v}_{4} = \\mathbf{0}.\n$$\nEquating components yields\n$$\n\\begin{aligned}\n\\text{Component } 1: & \\quad a_{1} = 0, \\\\\n\\text{Component } 2: & \\quad a_{2} = 0, \\\\\n\\text{Component } 3: & \\quad a_{3} = 0, \\\\\n\\text{Component } 4: & \\quad a_{4} = 0, \\\\\n\\text{Component } 5: & \\quad - (a_{1} + a_{2} + a_{3} + a_{4}) = 0,\n\\end{aligned}\n$$\nand the first four equations already force $a_{1} = a_{2} = a_{3} = a_{4} = 0$. Therefore the set $\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4} \\}$ is linearly independent.\n\nSince the set spans $S$ and is linearly independent, it is a basis for $S$.\n\nStep 3: Determine the dimension. By definition, the dimension of a subspace equals the number of vectors in any basis for it. The basis we constructed has $4$ vectors, hence\n$$\n\\dim(S) = 4.\n$$\nThis completes the construction and determination using only the fundamental definitions.", "answer": "$$\\boxed{4}$$", "id": "2435999"}, {"introduction": "Beyond simply defining subspaces, it is crucial to understand the subspaces associated with linear transformations, which model the evolution of systems. The eigenvectors of a matrix $A$ span a particularly important subspace that reveals the intrinsic modes of the system. This problem [@problem_id:2435980] challenges you to connect the concept of span to the property of diagonalizability, exploring why the dimension of the eigenvector subspace is a critical indicator of whether a matrix's behavior can be fully described through its eigenvalues and eigenvectors alone.", "problem": "In computational engineering, linear transformations are often represented by square matrices. Let $A \\in \\mathbb{C}^{n \\times n}$ act on $\\mathbb{C}^{n}$, and suppose $A$ is not diagonalizable over $\\mathbb{C}$. Define the subspace\n$$\nS \\;=\\; \\operatorname{span}\\{\\, v \\in \\mathbb{C}^{n} \\;:\\; \\exists\\, \\lambda \\in \\mathbb{C} \\text{ with } A v = \\lambda v \\,\\}.\n$$\nWhich one of the following statements is always true about $S$?\n\nA. $S$ is the direct sum of the eigenspaces of $A$, it is invariant under $A$, and because $A$ is not diagonalizable one has $\\dim S < n$. Moreover, $\\dim S$ equals the sum of the geometric multiplicities of the eigenvalues of $A$.\n\nB. The eigenvectors of $A$ span the entire space $\\mathbb{C}^{n}$ but are linearly dependent, which is why $A$ is not diagonalizable.\n\nC. The span $S$ need not be invariant under $A$ because generalized eigenvectors are required to obtain $A$-invariant subspaces.\n\nD. $\\dim S$ equals the number of distinct eigenvalues of $A$.\n\nE. For every non-diagonalizable $A \\in \\mathbb{C}^{n \\times n}$, one has $\\dim S = n - 1$.", "solution": "The problem statement must first be validated for correctness and completeness.\n\n**Problem Statement Validation**\n\nStep 1: Extract Givens\n-   A is a square matrix: $A \\in \\mathbb{C}^{n \\times n}$.\n-   The vector space is $\\mathbb{C}^n$.\n-   $A$ is not diagonalizable over $\\mathbb{C}$.\n-   A subspace $S$ is defined as $S \\;=\\; \\operatorname{span}\\{\\, v \\in \\mathbb{C}^{n} \\;:\\; \\exists\\, \\lambda \\in \\mathbb{C} \\text{ with } A v = \\lambda v \\,\\}$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is posed within the well-established framework of linear algebra.\nThe set $\\{\\, v \\in \\mathbb{C}^{n} \\;:\\; \\exists\\, \\lambda \\in \\mathbb{C} \\text{ with } A v = \\lambda v \\,\\}$ represents the union of all eigenspaces of the matrix $A$, including the zero vector. An eigenvector $v$ by definition must be non-zero, but the condition $A v = \\lambda v$ is satisfied by $v=0$ for any $\\lambda$. The span of the set of all eigenvectors is identical to the span of the union of all eigenspaces.\nThe concept of a non-diagonalizable matrix is standard. A matrix is non-diagonalizable over $\\mathbb{C}$ if and only if the sum of the geometric multiplicities of its eigenvalues is less than the dimension of the space, $n$.\nThe problem is mathematically well-posed, self-contained, and devoid of any scientific or logical flaws. The language is precise and unambiguous.\n\nStep 3: Verdict and Action\nThe problem statement is valid. Proceed to solution.\n\n**Derivation of Solution**\n\nFirst, we must correctly interpret the definition of the subspace $S$. The set $\\{\\, v \\in \\mathbb{C}^{n} \\;:\\; \\exists\\, \\lambda \\in \\mathbb{C} \\text{ with } A v = \\lambda v \\,\\}$ is the set of all vectors that are eigenvectors of $A$, plus the zero vector. This set is precisely the union of all eigenspaces of $A$. Let $\\sigma(A) = \\{\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\}$ be the set of distinct eigenvalues of $A$. For each $\\lambda_i \\in \\sigma(A)$, the corresponding eigenspace is $E_{\\lambda_i} = \\ker(A - \\lambda_i I)$. The set of all eigenvectors is the union of all non-zero vectors in these eigenspaces. The set defined in the problem is thus $\\bigcup_{i=1}^k E_{\\lambda_i}$.\n\nThe subspace $S$ is the span of this union:\n$$S = \\operatorname{span}\\left(\\bigcup_{i=1}^k E_{\\lambda_i}\\right)$$\nThe span of a union of subspaces is the sum of the subspaces. Therefore,\n$$S = E_{\\lambda_1} + E_{\\lambda_2} + \\dots + E_{\\lambda_k}$$\nA fundamental theorem of linear algebra states that eigenspaces corresponding to distinct eigenvalues are linearly independent. This implies that their sum is a direct sum:\n$$S = E_{\\lambda_1} \\oplus E_{\\lambda_2} \\oplus \\dots \\oplus E_{\\lambda_k}$$\nFrom this, the dimension of $S$ is the sum of the dimensions of the individual eigenspaces:\n$$\\dim S = \\sum_{i=1}^k \\dim(E_{\\lambda_i})$$\nThe dimension of an eigenspace, $\\dim(E_{\\lambda_i})$, is by definition the geometric multiplicity of the eigenvalue $\\lambda_i$. Let us denote it by $g_i$. So, $\\dim S = \\sum_{i=1}^k g_i$.\n\nA matrix $A \\in \\mathbb{C}^{n \\times n}$ is diagonalizable if and only if there exists a basis of $\\mathbb{C}^n$ consisting of eigenvectors of $A$. This is equivalent to the condition that the sum of the geometric multiplicities of all eigenvalues equals the dimension of the space, $n$. That is, $A$ is diagonalizable if and only if $\\sum_{i=1}^k g_i = n$.\nThe problem states that $A$ is **not** diagonalizable. This directly implies:\n$$\\dim S = \\sum_{i=1}^k g_i < n$$\nFinally, let us check if $S$ is an invariant subspace under $A$. A subspace $W$ is $A$-invariant if $A w \\in W$ for all $w \\in W$.\nEach eigenspace $E_{\\lambda_i}$ is an $A$-invariant subspace. If $v \\in E_{\\lambda_i}$, then $A v = \\lambda_i v$. Since $E_{\\lambda_i}$ is a subspace, $\\lambda_i v \\in E_{\\lambda_i}$.\nNow, let $v$ be an arbitrary vector in $S$. Since $S$ is the direct sum of the eigenspaces, $v$ can be uniquely written as a sum $v = v_1 + v_2 + \\dots + v_k$ where each $v_i \\in E_{\\lambda_i}$. Applying the linear transformation $A$:\n$$A v = A(v_1 + v_2 + \\dots + v_k) = A v_1 + A v_2 + \\dots + A v_k$$\nSince $v_i \\in E_{\\lambda_i}$, we have $A v_i = \\lambda_i v_i$. Thus,\n$$A v = \\lambda_1 v_1 + \\lambda_2 v_2 + \\dots + \\lambda_k v_k$$\nEach term $\\lambda_i v_i$ is a vector in $E_{\\lambda_i}$. Therefore, their sum is a vector in $E_{\\lambda_1} + \\dots + E_{\\lambda_k} = S$. This proves that $S$ is an $A$-invariant subspace.\n\nWith these properties established, we now evaluate each option.\n\n**Option-by-Option Analysis**\n\n**A. $S$ is the direct sum of the eigenspaces of $A$, it is invariant under $A$, and because $A$ is not diagonalizable one has $\\dim S < n$. Moreover, $\\dim S$ equals the sum of the geometric multiplicities of the eigenvalues of $A$.**\n-   \"$S$ is the direct sum of the eigenspaces of $A$\": As shown above, $S = \\bigoplus_{i=1}^k E_{\\lambda_i}$. This is correct.\n-   \"it is invariant under $A$\": As demonstrated, $S$ is an $A$-invariant subspace. This is correct.\n-   \"because $A$ is not diagonalizable one has $\\dim S < n$\": The condition for non-diagonalizability is that the sum of the geometric multiplicities is less than $n$. Since $\\dim S$ is this sum, we must have $\\dim S < n$. This is correct.\n-   \"$\\dim S$ equals the sum of the geometric multiplicities of the eigenvalues of $A$\": As derived, $\\dim S = \\sum_{i=1}^k \\dim(E_{\\lambda_i}) = \\sum_{i=1}^k g_i$. This is correct.\nEvery part of this statement is a direct and correct consequence of the problem statement and fundamental principles of linear algebra.\nVerdict: **Correct**.\n\n**B. The eigenvectors of $A$ span the entire space $\\mathbb{C}^{n}$ but are linearly dependent, which is why $A$ is not diagonalizable.**\nThe premise \"The eigenvectors of $A$ span the entire space $\\mathbb{C}^{n}$\" is the very definition of a diagonalizable matrix. Since $A$ is *not* diagonalizable, its eigenvectors *do not* span $\\mathbb{C}^{n}$. The span of the eigenvectors is $S$, and we have already established that $\\dim S < n$. The statement is factually incorrect from the start. The reason for non-diagonalizability is an insufficient number of linearly independent eigenvectors to form a basis for $\\mathbb{C}^n$, not their linear dependence in the way suggested.\nVerdict: **Incorrect**.\n\n**C. The span $S$ need not be invariant under $A$ because generalized eigenvectors are required to obtain $A$-invariant subspaces.**\nThe claim \"$S$ need not be invariant under $A$\" is false. We have rigorously proven that $S$, being the sum of $A$-invariant eigenspaces, is itself $A$-invariant. While generalized eigenvectors are indeed used to construct larger $A$-invariant subspaces (the generalized eigenspaces, whose direct sum is all of $\\mathbb{C}^n$), this does not negate the invariance of $S$. The reasoning provided is also misleading.\nVerdict: **Incorrect**.\n\n**D. $\\dim S$ equals the number of distinct eigenvalues of $A$.**\nLet $k$ be the number of distinct eigenvalues. This statement claims $\\dim S = k$. We know that $\\dim S = \\sum_{i=1}^k g_i$, where $g_i = \\dim(E_{\\lambda_i}) \\geq 1$. The claim is only true if $g_i=1$ for all $i=1, \\dots, k$. This is not generally true. Consider the non-diagonalizable matrix $A = \\begin{pmatrix} 2 & 1 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} \\in \\mathbb{C}^{4 \\times 4}$. The distinct eigenvalues are $\\lambda_1=2$ and $\\lambda_2=3$, so $k=2$. The geometric multiplicity of $\\lambda_1=2$ is $g_1=1$. The geometric multiplicity of $\\lambda_2=3$ is $g_2=2$. Then $\\dim S = g_1+g_2=1+2=3$. Here, $\\dim S = 3$ while the number of distinct eigenvalues is $k=2$. The statement is false.\nVerdict: **Incorrect**.\n\n**E. For every non-diagonalizable $A \\in \\mathbb{C}^{n \\times n}$, one has $\\dim S = n - 1$.**\nThis statement claims that the dimension of the subspace spanned by the eigenvectors is always exactly one less than the dimension of the space. This is false. Consider the matrix $A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\in \\mathbb{C}^{3 \\times 3}$. Here $n=3$. This is a Jordan block and is not diagonalizable. Its only eigenvalue is $\\lambda=0$. The eigenspace $E_0 = \\ker(A)$ is spanned by the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, so its dimension is $g_0=1$. Thus, $S=E_0$ and $\\dim S = 1$. The statement claims $\\dim S = n-1 = 3-1=2$. Since $1 \\neq 2$, this statement is false. The dimension \"defect\" $n - \\dim S$ can be any integer from $1$ to $n-1$.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2435980"}, {"introduction": "Theoretical concepts find their true power when they are translated into robust computational algorithms. This practice [@problem_id:2435961] moves from abstract theory to practical implementation by asking you to design a method for finding a vector that is guaranteed to be outside the span of a given set of vectors. This task requires you to think about the computational geometry of subspaces and their orthogonal complements, leading to powerful numerical techniques like the Singular Value Decomposition (SVD) that are cornerstones of modern computational engineering.", "problem": "You are given $k$ vectors in $\\mathbb{R}^n$ with $k < n$. By definition, the span of a finite set of vectors in $\\mathbb{R}^n$ is the set of all finite linear combinations of those vectors, and the span is a subspace of $\\mathbb{R}^n$. Your task is to design and implement a computational method that, for each provided test case, constructs a vector $\\mathbf{w} \\in \\mathbb{R}^n$ that is guaranteed to lie outside the span of the given vectors. Your method must be justified from first principles, starting from the core definitions of span, subspace, and orthogonality, and must ensure robustness to linear dependence among the input vectors. No physical units are involved. All angles, if any, should be considered in radians; however, no trigonometric quantities are required in this task.\n\nFor each test case, your program must:\n- Accept a set of $k$ vectors in $\\mathbb{R}^n$ with $k < n$ (these are hard-coded in your program; no user input is required).\n- Construct a vector $\\mathbf{w}$ that is guaranteed to be outside the span of the given vectors, based solely on the fundamental definitions of span and orthogonality between subspaces.\n- Compute the rank $r$ of the given set of vectors (the dimension of their span), the nullity $d = n - r$, and verify that $\\mathbf{w}$ is outside the span by computing the residual norm of $\\mathbf{w}$ after orthogonal projection onto the span. Use a numerical tolerance $\\tau = 10^{-10}$ to decide whether $\\mathbf{w}$ is outside the span, by declaring it outside if the residual norm is strictly greater than $\\tau \\cdot \\max(1, \\lVert \\mathbf{w} \\rVert_2)$, where $\\lVert \\cdot \\rVert_2$ is the Euclidean norm.\n- Produce a result for each test case as a list of the form $[r, d, b]$, where $r$ is an integer, $d$ is an integer, and $b$ is a boolean indicating whether $\\mathbf{w}$ is outside the span under the stated criterion.\n\nTest suite (each case lists $n$, $k$, and the $k$ column vectors in $\\mathbb{R}^n$):\n- Case $1$: $n = 3$, $k = 2$, vectors $\\left(1, 0, 0\\right)$ and $\\left(0, 1, 0\\right)$.\n- Case $2$: $n = 4$, $k = 3$, vectors $\\left(1, 1, 0, 0\\right)$, $\\left(2, 2, 0, 0\\right)$, and $\\left(0, 0, 1, 0\\right)$.\n- Case $3$: $n = 3$, $k = 2$, vectors $\\left(1, 0, 0\\right)$ and $\\left(1, 1 \\times 10^{-12}, 0\\right)$.\n- Case $4$: $n = 5$, $k = 4$, vectors $\\left(1, 0, 0, 0, 0\\right)$, $\\left(0, 1, 0, 0, 0\\right)$, $\\left(0, 0, 1, 0, 0\\right)$, and $\\left(0, 0, 0, 1, 0\\right)$.\n- Case $5$: $n = 3$, $k = 1$, vector $\\left(0, 0, 0\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case results enclosed in square brackets. Specifically, the output must be a single line of the form\n$[\\,[r_1,d_1,b_1],[r_2,d_2,b_2],\\dots,[r_5,d_5,b_5]\\,]$\nwith no additional text. Each $r_i$ and $d_i$ must be integers, and each $b_i$ must be a boolean. No user input is permitted, and no file input or output is permitted. The computation must be self-contained and reproducible.", "solution": "The problem as stated is valid. It is a well-posed question in computational linear algebra, firmly grounded in established mathematical principles. All necessary definitions, data, and conditions are provided, and no contradictions or ambiguities are present. The definition of \"nullity\" as $d = n - r$ is explicitly given, precluding any misinterpretation. We may therefore proceed with a formal solution.\n\nLet the given set of $k$ vectors in $\\mathbb{R}^n$ be denoted by $V = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$. The span of these vectors, $\\text{span}(V)$, is the set of all their linear combinations and forms a vector subspace of $\\mathbb{R}^n$. Let us designate this subspace as $S$. By definition, $S = \\{ \\mathbf{s} \\in \\mathbb{R}^n \\mid \\mathbf{s} = \\sum_{i=1}^k c_i \\mathbf{v}_i \\text{ for some scalars } c_i \\in \\mathbb{R} \\}$. The dimension of this subspace, $r = \\dim(S)$, is the rank of the set of vectors. Since the set contains $k$ vectors, it must be that $r \\le k$. The problem statement gives the critical condition $k < n$, which implies $r \\le k < n$. This guarantees that $S$ is a proper subspace of $\\mathbb{R}^n$.\n\nOur task is to construct a vector $\\mathbf{w}$ that is guaranteed to lie outside of $S$. A principled approach relies upon the concept of orthogonality. The fundamental theorem of linear algebra establishes that any vector space $\\mathbb{R}^n$ can be decomposed into the direct sum of a subspace $S$ and its orthogonal complement $S^\\perp$, such that $\\mathbb{R}^n = S \\oplus S^\\perp$. The orthogonal complement $S^\\perp$ is defined as the set of all vectors in $\\mathbb{R}^n$ that are orthogonal to every vector in $S$:\n$$ S^\\perp = \\{ \\mathbf{u} \\in \\mathbb{R}^n \\mid \\mathbf{u}^T \\mathbf{s} = 0 \\text{ for all } \\mathbf{s} \\in S \\} $$\nThe dimensions of these complementary subspaces are related by the equation $\\dim(S) + \\dim(S^\\perp) = \\dim(\\mathbb{R}^n) = n$. The problem defines a quantity $d = n - r$. By substituting $r = \\dim(S)$, we see that $d = n - \\dim(S) = \\dim(S^\\perp)$. The condition $r < n$ implies that $d = \\dim(S^\\perp) > 0$. This is a crucial result: the orthogonal complement $S^\\perp$ is a non-trivial subspace and contains non-zero vectors.\n\nAny non-zero vector $\\mathbf{w} \\in S^\\perp$ is guaranteed to not be an element of $S$. This can be proven by contradiction. Assume that $\\mathbf{w} \\in S^\\perp$, $\\mathbf{w} \\neq \\mathbf{0}$, and also that $\\mathbf{w} \\in S$. As an element of $S^\\perp$, $\\mathbf{w}$ must be orthogonal to every vector in $S$. Since $\\mathbf{w}$ is itself in $S$, it must be orthogonal to itself. This implies the inner product $\\mathbf{w}^T \\mathbf{w} = \\lVert \\mathbf{w} \\rVert_2^2 = 0$, which holds if and only if $\\mathbf{w}$ is the zero vector, $\\mathbf{w} = \\mathbf{0}$. This contradicts the assumption that $\\mathbf{w}$ is non-zero. Therefore, any non-zero vector found in $S^\\perp$ is definitively outside the span $S$.\n\nThe task is now reduced to finding a non-zero vector in $S^\\perp$. Computationally, we can achieve this by first arranging the given vectors $\\mathbf{v}_i$ as the columns of an $n \\times k$ matrix, $A = [\\mathbf{v}_1 | \\mathbf{v}_2 | \\dots | \\mathbf{v}_k]$. The subspace $S$ is then identical to the column space of $A$, written as $\\text{Col}(A)$. Its orthogonal complement, $S^\\perp$, is equivalent to the null space of the transpose of $A$, that is, $S^\\perp = (\\text{Col}(A))^\\perp = \\text{Nul}(A^T)$.\n\nThe most numerically robust method for analyzing these fundamental subspaces is the Singular Value Decomposition (SVD). The SVD of the matrix $A$ is its factorization into $A = U \\Sigma V^T$, where:\n- $U$ is an $n \\times n$ orthogonal matrix whose columns, $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_n\\}$, form an orthonormal basis for $\\mathbb{R}^n$.\n- $\\Sigma$ is an $n \\times k$ rectangular matrix with non-negative real numbers, the singular values $\\sigma_i$, on its main diagonal.\n- $V$ is a $k \\times k$ orthogonal matrix.\n\nThe rank $r$ of matrix $A$ corresponds to the number of non-zero singular values. The first $r$ columns of the matrix $U$, i.e., $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_r\\}$, constitute an orthonormal basis for the column space, $\\text{Col}(A) = S$. The subsequent $n-r$ columns of $U$, namely $\\{\\mathbf{u}_{r+1}, \\dots, \\mathbf{u}_n\\}$, form an orthonormal basis for the orthogonal complement, $S^\\perp$.\n\nThis provides a deterministic algorithm for constructing the vector $\\mathbf{w}$. We compute the full SVD of $A$ and select one of the last $n-r$ columns of $U$. We will consistently choose the last column, $\\mathbf{w} = \\mathbf{u}_n$. As columns of an orthogonal matrix, these basis vectors are of unit length, $\\lVert \\mathbf{w} \\rVert_2 = 1$, thus ensuring $\\mathbf{w}$ is non-zero.\n\nThe algorithmic procedure is as follows:\n$1$. Assemble the $n \\times k$ matrix $A$ from the input column vectors.\n$2$. Compute the numerical rank $r$ of $A$. The problem's \"nullity\" is then calculated as $d = n-r$.\n$3$. Perform a full SVD of $A$ to obtain the $n \\times n$ orthogonal matrix $U$.\n$4$. Select the vector $\\mathbf{w}$ to be the last column of $U$, $\\mathbf{w} = U[:, n-1]$.\n$5$. For verification, we must compute the norm of the residual after orthogonally projecting $\\mathbf{w}$ onto $S$. The orthogonal projection operator onto $S$ is $P_S = U_r U_r^T$, where $U_r = [\\mathbf{u}_1 | \\dots | \\mathbf{u}_r]$.\n$6$. For our chosen $\\mathbf{w}=\\mathbf{u}_n$, the projection is $P_S(\\mathbf{w}) = U_r U_r^T \\mathbf{u}_n$. Due to the mutual orthogonality of the columns of $U$, the product $U_r^T \\mathbf{u}_n$ is a zero vector. Consequently, the projection $P_S(\\mathbf{u}_n)$ is also the zero vector, $\\mathbf{0}$.\n$7$. The residual vector is $\\mathbf{w} - P_S(\\mathbf{w}) = \\mathbf{u}_n - \\mathbf{0} = \\mathbf{u}_n$. The norm of this residual is $\\lVert \\mathbf{u}_n \\rVert_2 = 1$.\n$8$. We then apply the specified test: is the residual norm strictly greater than $\\tau \\cdot \\max(1, \\lVert \\mathbf{w} \\rVert_2)$? Substituting the values, we check if $1 > 10^{-10} \\cdot \\max(1, 1)$, which simplifies to $1 > 10^{-10}$. This inequality is true. Therefore, the boolean outcome $b$ is `True`. This logic holds for all test cases where $r < n$, including the trivial case where the input is the zero vector, for which $r=0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a predefined test suite, constructing a vector outside the\n    span of a given set of vectors and performing the required analysis.\n    \"\"\"\n\n    # Test suite defined in the problem statement.\n    # Each case is a tuple: (n, list_of_column_vectors)\n    test_cases = [\n        (3, [(1, 0, 0), (0, 1, 0)]),\n        (4, [(1, 1, 0, 0), (2, 2, 0, 0), (0, 0, 1, 0)]),\n        (3, [(1, 0, 0), (1, 1e-12, 0)]),\n        (5, [(1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 1, 0)]),\n        (3, [(0, 0, 0)])\n    ]\n\n    all_results = []\n    \n    # Numerical tolerance for verification check\n    tau = 1e-10\n\n    for n, vectors in test_cases:\n        # Step 1: Construct the n x k matrix A from the input vectors.\n        # The input vectors are column vectors, so we transpose the array.\n        # Handle the edge case of an empty list of vectors.\n        if not vectors or not vectors[0]:\n            A = np.zeros((n, 0))\n        else:\n            A = np.array(vectors).T\n        \n        # Step 2: Compute the rank r and the problem-defined nullity d.\n        # numpy.linalg.matrix_rank uses SVD and is numerically robust.\n        r = np.linalg.matrix_rank(A)\n        d = n - r\n\n        # Step 3: Compute the full SVD of A.\n        # full_matrices=True is essential to get the full n x n U matrix.\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n\n        # Step 4: Select vector w from the orthogonal complement's basis.\n        # The last n-r columns of U form a basis for the orthogonal complement.\n        # For a deterministic choice, we take the last column of U.\n        w = U[:, -1]\n\n        # Step 5: Verify that w is outside the span S.\n        # The first r columns of U form an orthonormal basis for the span S.\n        Ur = U[:, :r]\n        \n        # Calculate the orthogonal projection of w onto the span S.\n        # If r is 0, the span is the zero vector, so the projection is zero.\n        # numpy handles matrix multiplication with shape (n, 0) correctly, resulting in a zero vector.\n        projection_w = Ur @ (Ur.T @ w)\n        \n        # Calculate the residual vector and its norm.\n        residual_vector = w - projection_w\n        residual_norm = np.linalg.norm(residual_vector)\n        \n        # Calculate the norm of w for the threshold calculation.\n        w_norm = np.linalg.norm(w)\n\n        # Apply the verification criterion from the problem statement.\n        threshold = tau * max(1.0, w_norm)\n        is_outside = residual_norm > threshold\n        \n        # Store the result [r, d, b] for this case.\n        all_results.append([int(r), int(d), is_outside])\n\n    # Format the final output string exactly as required.\n    # Individual list items are formatted as [r,d,b] without spaces.\n    def format_result_list(res_list):\n        r_val, d_val, b_val = res_list\n        return f\"[{r_val},{d_val},{b_val}]\"\n\n    formatted_results = [format_result_list(res) for res in all_results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2435961"}]}