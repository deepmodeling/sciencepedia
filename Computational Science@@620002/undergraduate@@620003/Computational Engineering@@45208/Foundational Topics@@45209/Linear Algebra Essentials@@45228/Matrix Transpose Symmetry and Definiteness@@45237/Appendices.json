{"hands_on_practices": [{"introduction": "In many computational engineering applications, from parameter estimation to finite element analysis, we often formulate objective functions that need to be minimized. This exercise [@problem_id:2412072] provides practice with a critical algebraic skill: transforming a complex-looking expression into the standard quadratic form $x^T Q x + c^T x + d$. Mastering this manipulation, particularly the transpose rule for matrix products $(AB)^T = B^T A^T$, is an essential first step for analyzing and solving these optimization problems.", "problem": "In a weighted residual computational algorithm for parameter estimation, a scalar objective function is defined by\n$$\nJ(x) = \\left(S B A x - d\\right)^{T} R \\left(S B A x - d\\right) + 2 \\left(G x\\right)^{T} H^{T} y,\n$$\nwhere $x \\in \\mathbb{R}^{n}$ is the decision variable, $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{p \\times m}$, $S \\in \\mathbb{R}^{k \\times p}$, $R \\in \\mathbb{R}^{k \\times k}$ is symmetric positive definite (SPD), $d \\in \\mathbb{R}^{k}$, $G \\in \\mathbb{R}^{\\ell \\times n}$, $H \\in \\mathbb{R}^{\\ell \\times \\ell}$, and $y \\in \\mathbb{R}^{\\ell}$. All matrix products are well-defined by the stated dimensions.\n\nRewrite $J(x)$ explicitly as a single closed-form analytical expression in $x$ that contains only sums of a quadratic term in $x$, a linear term in $x$, and a constant term, with all transposes applied to individual matrices only (not to matrix products). Your final answer must be this single analytical expression.", "solution": "The problem requires rewriting the scalar objective function $J(x)$ into a single closed-form analytical expression consisting of a sum of a quadratic term in $x$, a linear term in $x$, and a constant term, with the constraint that all matrix transposes are applied only to individual matrices.\n\nThe objective function is given as:\n$$\nJ(x) = \\left(S B A x - d\\right)^{T} R \\left(S B A x - d\\right) + 2 \\left(G x\\right)^{T} H^{T} y\n$$\nWe shall expand each of the two terms in this sum separately.\n\nLet the first term be $T_1 = \\left(S B A x - d\\right)^{T} R \\left(S B A x - d\\right)$.\nUsing the property of the transpose of a difference, $(U-V)^T = U^T - V^T$, we get:\n$$\nT_1 = \\left((S B A x)^T - d^T\\right) R \\left(S B A x - d\\right)\n$$\nApplying the distributive property of matrix multiplication:\n$$\nT_1 = (S B A x)^T R (S B A x) - (S B A x)^T R d - d^T R (S B A x) + d^T R d\n$$\nWe must now apply the transpose property for matrix products, which states $(P_1 P_2 \\dots P_k)^T = P_k^T \\dots P_2^T P_1^T$, to the terms containing products.\nThe quadratic term becomes:\n$$\n(S B A x)^T R (S B A x) = (x^T A^T B^T S^T) R (S B A x) = x^T A^T B^T S^T R S B A x\n$$\nThe linear terms (cross-terms) are $-(S B A x)^T R d$ and $-d^T R (S B A x)$. Let us expand the first one:\n$$\n(S B A x)^T R d = (x^T A^T B^T S^T) R d = x^T A^T B^T S^T R d\n$$\nThis is a $1 \\times 1$ matrix, a scalar. A scalar is equal to its own transpose. Let us compute the transpose of this scalar quantity:\n$$\n(x^T A^T B^T S^T R d)^T = d^T R^T (S^T)^T (B^T)^T (A^T)^T x\n$$\nUsing the properties $(P^T)^T=P$ and the given fact that $R$ is symmetric ($R = R^T$), this simplifies to:\n$$\nd^T R S B A x\n$$\nThis expression is identical to the second cross-term, $-d^T R (S B A x)$. Therefore, the two cross-terms are equal:\n$$\n(S B A x)^T R d = d^T R S B A x\n$$\nTheir sum is $-2 d^T R S B A x$.\nThe constant term is $d^T R d$.\nCombining these parts, the first term $T_1$ is fully expanded as:\n$$\nT_1 = x^T A^T B^T S^T R S B A x - 2 d^T R S B A x + d^T R d\n$$\nNow, let the second term be $T_2 = 2 \\left(G x\\right)^{T} H^{T} y$.\nApplying the transpose property for a product:\n$$\nT_2 = 2 (x^T G^T) H^T y = 2 x^T G^T H^T y\n$$\nThis is also a scalar, so it is equal to its transpose:\n$$\nT_2 = (2 x^T G^T H^T y)^T = 2 y^T (H^T)^T (G^T)^T x = 2 y^T H G x\n$$\nWe will use the form $2 y^T H G x$ to maintain consistency with the other linear term where $x$ appears at the end.\n\nFinally, we sum the expanded expressions for $T_1$ and $T_2$ to obtain the full expression for $J(x)$:\n$$\nJ(x) = T_1 + T_2 = (x^T A^T B^T S^T R S B A x - 2 d^T R S B A x + d^T R d) + 2 y^T H G x\n$$\nTo satisfy the requirement of grouping into quadratic, linear, and constant terms, we rearrange the expression:\n$$\nJ(x) = x^T A^T B^T S^T R S B A x + (2 y^T H G x - 2 d^T R S B A x) + d^T R d\n$$\nThe linear terms can be combined by factoring out the variable $x$:\n$$\nJ(x) = x^T A^T B^T S^T R S B A x + 2(y^T H G - d^T R S B A)x + d^T R d\n$$\nThis expression is in the required form. It is a sum of:\n1.  A quadratic term in $x$: $x^T A^T B^T S^T R S B A x$\n2.  A linear term in $x$: $2(y^T H G - d^T R S B A)x$\n3.  A constant term: $d^T R d$\n\nAll transposes are applied solely to individual matrices, as specified.", "answer": "$$\n\\boxed{J(x) = x^T A^T B^T S^T R S B A x + 2(y^T H G - d^T R S B A) x + d^T R d}\n$$", "id": "2412072"}, {"introduction": "A quadratic form $x^T A x$ often represents physical quantities like energy, but what part of the matrix $A$ truly defines this value? This exercise [@problem_id:2412126] delves into this fundamental question, guiding you to discover that only the symmetric component of a matrix contributes to its quadratic form. You will then apply this powerful insight to solve a practical design problem involving norm minimization, bridging the gap between abstract theory and concrete application.", "problem": "In computational engineering, particularly in the Finite Element Method (FEM), the energy associated with a stiffness-like operator is captured by the quadratic form $x^T A x$. Consider real matrices $A \\in \\mathbb{R}^{n \\times n}$ and the identity $x^T A x = 0$ holding for all $x \\in \\mathbb{R}^{n}$.\n\n1. Using only the definitions of transpose, symmetry, and the properties of the scalar $x^T A x$, derive from first principles the structural constraint on $A$ that is implied by the identity $x^T A x = 0$ for all $x \\in \\mathbb{R}^{n}$.\n\n2. Now specialize to $\\mathbb{R}^{3 \\times 3}$ and consider the design problem: among all real matrices $A \\in \\mathbb{R}^{3 \\times 3}$ that satisfy $x^T A x = 0$ for all $x \\in \\mathbb{R}^{3}$ and additionally satisfy the entry constraint $a_{12} = 2$, determine the minimal possible Frobenius norm $\\|A\\|_{F}$, where $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{3} \\sum_{j=1}^{3} a_{ij}^{2}}$. Report the minimal norm as an exact value.\n\nYour final answer must be the minimal Frobenius norm as a single real number. No rounding is required.", "solution": "We proceed from first principles.\n\nPart 1. Let $A \\in \\mathbb{R}^{n \\times n}$ and assume $x^T A x = 0$ for all $x \\in \\mathbb{R}^{n}$. For arbitrary $u, v \\in \\mathbb{R}^{n}$, expand\n\n$$\n(u+v)^T A (u+v) = u^T A u + u^T A v + v^T A u + v^T A v.\n$$\n\nBy the hypothesis, $u^T A u = 0$ and $v^T A v = 0$ for all $u, v$, so the expansion gives\n\n$$\n0 = (u+v)^T A (u+v) = u^T A v + v^T A u.\n$$\n\nNote that $v^T A u = (u^T A^T v)$. Therefore\n\n$$\n0 = u^T A v + u^T A^T v = u^T (A + A^T) v\n$$\n\nfor all $u, v \\in \\mathbb{R}^{n}$. Define the symmetric part $S = \\tfrac{1}{2}(A + A^T)$. Then the above implies $u^T S v = 0$ for all $u, v$. Choosing $u = e_{i}$ and $v = e_{j}$, the standard basis vectors, yields $e_{i}^T S e_{j} = s_{ij} = 0$ for all indices, hence $S = 0$. Therefore $A + A^T = 0$, i.e., $A$ is skew-symmetric. Thus, the identity $x^T A x = 0$ for all $x$ forces $A$ to be skew-symmetric.\n\nPart 2. We now minimize the Frobenius norm over all $A \\in \\mathbb{R}^{3 \\times 3}$ that are skew-symmetric and satisfy $a_{12} = 2$. A real $3 \\times 3$ skew-symmetric matrix has the form\n\n$$\nA = \\begin{pmatrix}\n0  a_{12}  a_{13} \\\\\n-a_{12}  0  a_{23} \\\\\n-a_{13}  -a_{23}  0\n\\end{pmatrix}.\n$$\n\nImposing $a_{12} = 2$ yields\n\n$$\nA = \\begin{pmatrix}\n0  2  a_{13} \\\\\n-2  0  a_{23} \\\\\n-a_{13}  -a_{23}  0\n\\end{pmatrix},\n$$\n\nwith free parameters $a_{13}, a_{23} \\in \\mathbb{R}$. The Frobenius norm squared is\n\n$$\n\\|A\\|_{F}^{2} = \\sum_{i=1}^{3} \\sum_{j=1}^{3} a_{ij}^{2} = 0^{2} + 2^{2} + a_{13}^{2} + (-2)^{2} + 0^{2} + a_{23}^{2} + (-a_{13})^{2} + (-a_{23})^{2} + 0^{2}.\n$$\n\nThis simplifies to\n\n$$\n\\|A\\|_{F}^{2} = 4 + 4 + 2 a_{13}^{2} + 2 a_{23}^{2} = 8 + 2\\left(a_{13}^{2} + a_{23}^{2}\\right).\n$$\n\nThe expression is minimized by choosing $a_{13} = 0$ and $a_{23} = 0$, which yields the minimal value\n\n$$\n\\|A\\|_{F,\\min} = \\sqrt{8} = 2 \\sqrt{2}.\n$$\n\nThe corresponding minimizing matrix\n\n$$\nA_{\\min} = \\begin{pmatrix}\n0  2  0 \\\\\n-2  0  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\n\nis real, non-zero, non-symmetric (indeed skew-symmetric), and satisfies $x^T A x = 0$ for all $x \\in \\mathbb{R}^{3}$ as required. Therefore, the minimal Frobenius norm is $2 \\sqrt{2}$.", "answer": "$$\\boxed{2\\sqrt{2}}$$", "id": "2412126"}, {"introduction": "The theoretical properties of matrices become truly powerful when they are used to write more efficient code. In large-scale computational engineering, where matrices can have millions of dimensions, exploiting structure like symmetry is not just an optimizationâ€”it is a necessity. This hands-on coding practice [@problem_id:2412069] challenges you to translate the mathematical property of symmetry into a practical, memory-efficient algorithm, a core competency in high-performance scientific computing.", "problem": "You are given the task of designing a memory-efficient representation and matrix-vector multiplication routine for a real, sparse, symmetric matrix that stores only its upper-triangular nonzero entries (including the diagonal). You must implement a complete, runnable program that constructs this representation for a provided test suite, computes the matrix-vector product using only the stored data, evaluates symmetry and definiteness properties, and reports a quantitative memory comparison to a naive unsymmetrized coordinate storage.\n\nDefinitions and requirements:\n\n- Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric, i.e., $A^T = A$. The input data for $A$ are given as a list of triplets $(i,j,v)$ with $0 \\le i \\le j \\le n-1$, representing $A_{ij} = v$ and, by symmetry, $A_{ji} = v$ when $i \\ne j$.\n- Given a vector $x \\in \\mathbb{R}^n$, the task is to compute $y = A x$ using only the upper-triangular storage. All arithmetic must adhere to real-number operations.\n- Determine whether $A$ is symmetric positive definite (SPD), defined by $x^T A x  0$ for all nonzero $x \\in \\mathbb{R}^n$. Report a boolean indicating whether this property holds.\n- Verify two correctness properties using a dense reconstruction of $A$:\n  1. $y$ computed from the upper-triangular storage equals the dense product $A x$ within an absolute tolerance of $10^{-12}$; report this as a boolean.\n  2. $y$ equals $A^T x$ within an absolute tolerance of $10^{-12}$; report this as a boolean. Note that for symmetric $A$, $A^T = A$.\n- Quantify memory usage as follows. Consider a naive unsymmetrized coordinate storage (Coordinate list) that stores every nonzero entry $(i,j,v)$ appearing in the full symmetric matrix, i.e., it stores both $(i,j,v)$ and $(j,i,v)$ for every strictly off-diagonal nonzero and a single $(i,i,v)$ for diagonal entries. Assume $64$-bit integers for indices and $64$-bit floats for values, so each stored triplet consumes $24$ bytes. Your upper-triangular storage stores each provided triplet once. For each test case, report the integer number of bytes saved:\n  $$\\text{bytes\\_saved} = \\left(\\text{naive\\_entry\\_count} - \\text{upper\\_entry\\_count}\\right) \\times 24.$$\n- Final output format: For each test case, produce a list\n  $$[\\;y\\_as\\_list\\_of\\_floats,\\; \\text{is\\_spd\\_boolean},\\; \\text{matches\\_dense\\_boolean},\\; \\text{transpose\\_consistency\\_boolean},\\; \\text{bytes\\_saved\\_integer}\\;].$$\n  Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, in the same order as the test suite below, for example:\n  $$[\\text{case1\\_result},\\text{case2\\_result},\\ldots].$$\n\nTest suite (indices are zero-based, i.e., they start at $0$; there are no duplicate triplets in any case):\n\n- Case $1$:\n  - $n = 5$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,4.0)$, $(1,1,5.0)$, $(2,2,6.0)$, $(3,3,5.0)$, $(4,4,4.0)$\n    - $(0,1,1.0)$, $(1,2,0.5)$, $(2,3,1.0)$, $(3,4,0.5)$, $(0,4,0.2)$\n  - $x = [1.0, 2.0, 3.0, 4.0, 5.0]$\n\n- Case $2$:\n  - $n = 4$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,2.0)$, $(1,1,3.0)$, $(2,2,4.0)$, $(3,3,5.0)$\n  - $x = [1.0, 0.0, -1.0, 2.0]$\n\n- Case $3$:\n  - $n = 3$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,2.0)$, $(1,1,-1.0)$, $(2,2,3.0)$\n    - $(0,2,0.5)$\n  - $x = [1.0, -2.0, 0.5]$\n\n- Case $4$:\n  - $n = 1$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,0.0)$\n  - $x = [3.0]$\n\nYour program must produce the single-line output described above, where each caseâ€™s result is a list with the five items in the specified order.", "solution": "The problem presented is valid, well-posed, and grounded in the fundamental principles of computational linear algebra. It addresses the practical task of implementing a memory-efficient matrix-vector multiplication for a real, sparse, symmetric matrix and subsequently analyzing its properties. We shall proceed with a rigorous, step-by-step solution.\n\nThe core of this problem is the computation of the matrix-vector product $y = Ax$ for a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, where only the non-zero entries in the upper triangle (including the diagonal) are stored. Let the set of stored index-value triplets be $U = \\{(i, j, v) \\,|\\, A_{ij} = v, 0 \\le i \\le j  n\\}$.\n\nThe $k$-th component of the product vector $y$ is defined as $y_k = \\sum_{l=0}^{n-1} A_{kl} x_l$. To compute this using only the upper-triangular entries, we must consider how each stored entry contributes to the product. For each stored triplet $(i, j, v) \\in U$:\n\n$1$. If $i = j$, the entry is on the diagonal. This term $A_{ii} = v$ contributes only to the product component $y_i$. The contribution is $v \\cdot x_i$.\n\n$2$. If $i  j$, the entry is in the strict upper triangle. This term $A_{ij} = v$ contributes $v \\cdot x_j$ to the component $y_i$. Due to the symmetry of $A$, we have $A_{ji} = A_{ij} = v$. This corresponding lower-triangular entry contributes $A_{ji} \\cdot x_i = v \\cdot x_i$ to the component $y_j$.\n\nTherefore, a correct and efficient algorithm can be formulated by initializing a zero vector $y \\in \\mathbb{R}^n$ and iterating through the stored triplets:\n- For each diagonal triplet $(i, i, v)$, update $y_i \\leftarrow y_i + v \\cdot x_i$.\n- For each off-diagonal triplet $(i, j, v)$ with $i  j$, perform two updates: $y_i \\leftarrow y_i + v \\cdot x_j$ and $y_j \\leftarrow y_j + v \\cdot x_i$.\nThis procedure correctly computes the full product $Ax$ by leveraging symmetry.\n\nNext, we must determine if $A$ is symmetric positive definite (SPD). A symmetric matrix is, by definition, positive definite if and only if all of its eigenvalues are strictly positive. This provides the most robust criterion for the check. The procedure is as follows:\n$1$. Reconstruct the full dense matrix $A$ from the provided upper-triangular triplets. For each triplet $(i, j, v)$, we set $A_{ij} = v$ and, if $i \\neq j$, also $A_{ji} = v$.\n$2$. Compute the eigenvalues of the resulting symmetric matrix $A$. Specialized numerical routines for symmetric matrices, such as those that compute eigenvalues of a real symmetric matrix, are appropriate here.\n$3$. Check if every computed eigenvalue $\\lambda$ satisfies the condition $\\lambda > 0$. If this holds for all eigenvalues, the matrix is SPD.\n\nThe verification of correctness is straightforward. The product vector $y$ computed via the efficient symmetric algorithm is compared against a reference product $y_{\\text{dense}} = A_{\\text{dense}} x$, where $A_{\\text{dense}}$ is the fully reconstructed matrix. The comparison must be performed using an absolute tolerance of $10^{-12}$. Additionally, because $A$ is symmetric ($A = A^T$), the identity $Ax = A^T x$ must hold. This provides a second, albeit related, consistency check.\n\nFinally, we quantify the memory savings. The problem specifies that each stored triplet $(i, j, v)$ consumes $24$ bytes ($2 \\times 8$ bytes for integer indices and $8$ bytes for a double-precision float value).\n- The upper-triangular storage requires $\\text{upper\\_entry\\_count}$ triplets, which is the count of input data.\n- A naive coordinate storage would store all non-zero entries explicitly. For each strictly off-diagonal entry $A_{ij} \\neq 0$ ($i \\neq j$), it stores two triplets, $(i, j, A_{ij})$ and $(j, i, A_{ji})$. For each diagonal entry $A_{ii} \\neq 0$, it stores one triplet.\nThe number of off-diagonal triplets in the input is $\\text{num\\_off\\_diagonal}$. The total number of triplets in the naive storage is thus $\\text{naive\\_entry\\_count} = (\\text{upper\\_entry\\_count} - \\text{num\\_off\\_diagonal}) + 2 \\cdot \\text{num\\_off\\_diagonal} = \\text{upper\\_entry\\_count} + \\text{num\\_off\\_diagonal}$.\nThe bytes saved are calculated as $(\\text{naive\\_entry\\_count} - \\text{upper\\_entry\\_count}) \\times 24$, which simplifies to $\\text{num\\_off\\_diagonal} \\times 24$. This reflects the fact that our efficient scheme saves one triplet storage for each non-zero, off-diagonal pair.\n\nThe implementation will process each test case according to these principles, generating the required five-element list for each.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5,\n            \"triplets\": [\n                (0, 0, 4.0), (1, 1, 5.0), (2, 2, 6.0), (3, 3, 5.0), (4, 4, 4.0),\n                (0, 1, 1.0), (1, 2, 0.5), (2, 3, 1.0), (3, 4, 0.5), (0, 4, 0.2)\n            ],\n            \"x\": [1.0, 2.0, 3.0, 4.0, 5.0]\n        },\n        {\n            \"n\": 4,\n            \"triplets\": [\n                (0, 0, 2.0), (1, 1, 3.0), (2, 2, 4.0), (3, 3, 5.0)\n            ],\n            \"x\": [1.0, 0.0, -1.0, 2.0]\n        },\n        {\n            \"n\": 3,\n            \"triplets\": [\n                (0, 0, 2.0), (1, 1, -1.0), (2, 2, 3.0),\n                (0, 2, 0.5)\n            ],\n            \"x\": [1.0, -2.0, 0.5]\n        },\n        {\n            \"n\": 1,\n            \"triplets\": [\n                (0, 0, 0.0)\n            ],\n            \"x\": [3.0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"n\"], case[\"triplets\"], case[\"x\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists.\n    print(str(results).replace(\" \", \"\"))\n\n\ndef process_case(n, triplets, x_vec):\n    \"\"\"\n    Processes a single test case according to the problem specification.\n\n    Args:\n        n (int): The dimension of the matrix.\n        triplets (list of tuple): The upper-triangular non-zero entries (i, j, v).\n        x_vec (list of float): The vector x.\n\n    Returns:\n        list: A list containing [y_as_list, is_spd, matches_dense, transpose_consistency, bytes_saved].\n    \"\"\"\n    x = np.array(x_vec, dtype=float)\n\n    # 1. Efficient matrix-vector multiplication y = Ax from upper-triangular storage\n    y = np.zeros(n, dtype=float)\n    for i, j, v in triplets:\n        if i == j:  # Diagonal element\n            y[i] += v * x[i]\n        else:  # Off-diagonal element, use symmetry\n            y[i] += v * x[j]\n            y[j] += v * x[i]\n\n    # 2. Reconstruct the full dense matrix for verification purposes\n    A_dense = np.zeros((n, n), dtype=float)\n    for i, j, v in triplets:\n        A_dense[i, j] = v\n        if i != j:\n            A_dense[j, i] = v\n\n    # 3. Determine if the matrix is Symmetric Positive Definite (SPD)\n    # A symmetric matrix is SPD iff all its eigenvalues are strictly positive.\n    is_spd = False\n    if n  0:\n        try:\n            eigenvalues = np.linalg.eigvalsh(A_dense)\n            if np.all(eigenvalues  0):\n                is_spd = True\n        except np.linalg.LinAlgError:\n            # This case shouldn't happen with the given data but is good practice.\n            is_spd = False\n\n    # 4. Perform correctness and consistency checks\n    tolerance = 1e-12\n    \n    # 4.1. Check against dense matrix-vector product\n    y_dense = A_dense @ x\n    matches_dense = np.allclose(y, y_dense, atol=tolerance, rtol=0)\n\n    # 4.2. Check consistency with A^T x (since A is symmetric, Ax = A^T x)\n    A_transpose = A_dense.T\n    y_transpose = A_transpose @ x\n    transpose_consistency = np.allclose(y, y_transpose, atol=tolerance, rtol=0)\n\n    # 5. Quantify memory savings\n    bytes_per_triplet = 24\n    num_off_diagonal = sum(1 for i, j, v in triplets if i != j)\n    # Savings come from storing each off-diagonal element once instead of twice\n    bytes_saved = num_off_diagonal * bytes_per_triplet\n    \n    # Format the results for the final output\n    return [y.tolist(), is_spd, matches_dense, transpose_consistency, int(bytes_saved)]\n\nsolve()\n```", "id": "2412069"}]}