{"hands_on_practices": [{"introduction": "Numerical differentiation is a fundamental tool in computational science, but its accuracy hinges on a delicate balance. This exercise explores the classic forward-difference formula, where you will discover the core trade-off between truncation error, which decreases as the step size $h$ shrinks, and round-off error, which becomes amplified. By deriving and empirically verifying an optimal step size $h^*$, you will gain a practical understanding of how to manage these competing errors to achieve the most accurate result possible [@problem_id:2447368].", "problem": "Implement a program that investigates truncation and round-off error in the forward-difference numerical differentiation of the exponential function. Consider the derivative of the function $f(x) = \\exp(x)$ at the point $x = 1$. Use the forward-difference approximation $f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$. Your tasks are to derive an error model from first principles, use it to obtain a theoretical optimal step size $h^*$, and then empirically validate this prediction by scanning a range of step sizes $h$ in floating-point arithmetic.\n\nBase your derivation only on the following widely accepted principles:\n- The Taylor expansion of a sufficiently smooth function about a point $x$: $f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$ as $h \\to 0$.\n- The standard model for floating-point rounding to nearest in the Institute of Electrical and Electronics Engineers (IEEE) 754 arithmetic: for any basic operation and number $y$, $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit round-off. For a given machine epsilon $\\varepsilon$ defined as the gap between $1$ and the next representable number, $u = \\varepsilon/2$.\n\nTasks to perform:\n1. Derive from the above principles a leading-order absolute error model for the forward-difference approximation at $x=1$ that combines truncation error and round-off error in terms of $h$, $f(1)$, $f''(1)$, and the machine epsilon $\\varepsilon$. Then, from this model, obtain the theoretical optimal step size $h^*$ that minimizes the leading-order error. Express $h^*$ explicitly for $f(x) = \\exp(x)$ at $x=1$ in terms of $\\varepsilon$. All quantities are dimensionless.\n2. Implement a program that:\n   - Computes the theoretical $h^*$ using the machine epsilon $\\varepsilon$ associated with a specified floating-point data type.\n   - Empirically estimates the optimal $h$ by scanning a logarithmically spaced set of $h$ values in a specified interval and selecting the $h$ that minimizes the absolute error $|D(h) - f'(1)|$, where $D(h) = \\dfrac{f(1+h) - f(1)}{h}$ is computed in the target floating-point data type. The reference value $f'(1)$ must be evaluated with sufficiently high precision so that it does not bias the empirical evaluation. All quantities are dimensionless.\n   - Reports, for each test case, a list of three floats: the theoretical optimal step size $h^*$, the empirically optimal step size $h_{\\mathrm{emp}}$, and the minimal absolute error attained across the scan.\n3. Use the following test suite, which varies the floating-point precision and the scan interval to probe different error regimes. For each case, scan $200$ logarithmically spaced values of $h$ over the specified interval:\n   - Test case A (happy path, double precision): data type $\\texttt{float64}$, $h \\in [10^{-16}, 10^{-1}]$.\n   - Test case B (single precision): data type $\\texttt{float32}$, $h \\in [10^{-10}, 10^{0}]$.\n   - Test case C (boundary, round-off dominated window in double precision): data type $\\texttt{float64}$, $h \\in [10^{-20}, 10^{-14}]$.\n4. Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a three-element list in the order [$h^*$, $h_{\\mathrm{emp}}$, min_h |D(h) - f'(1)|] for one test case. For example, the structure must be exactly like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$. All quantities are dimensionless real numbers.\n\nNo external input is required, and all quantities are dimensionless. Angles are not involved, and there are no percentages in the output.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\nStep 1: Extract Givens\n- Function to differentiate: $f(x) = \\exp(x)$.\n- Point of differentiation: $x = 1$.\n- Numerical differentiation formula: Forward-difference approximation, $f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$.\n- Principle 1 (Taylor expansion): $f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$ as $h \\to 0$.\n- Principle 2 (Floating-point error model): For a number $y$, its floating-point representation is $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\le u$, where $u = \\varepsilon/2$ is the unit round-off and $\\varepsilon$ is the machine epsilon.\n- Task 1: Derive a leading-order absolute error model for the approximation at $x=1$ combining truncation and round-off errors. From this model, derive the theoretical optimal step size $h^\\star$ for $f(x)=\\exp(x)$ at $x=1$ in terms of $\\varepsilon$.\n- Task 2: Implement a program to compute the theoretical $h^\\star$ and find an empirical optimal step size $h_{\\mathrm{emp}}$ by minimizing the absolute error over a scanned range of $h$ values.\n- Task 3: The program must be run for three test cases:\n    - Case A: `float64` precision, $h \\in [10^{-16}, 10^{-1}]$.\n    - Case B: `float32` precision, $h \\in [10^{-10}, 10^{0}]$.\n    - Case C: `float64` precision, $h \\in [10^{-20}, 10^{-14}]$.\n    - For all cases, the scan must use $200$ logarithmically spaced points.\n- Task 4: The final output must be a single string representing a list of lists, with each inner list being $[h^\\star, h_{\\mathrm{emp}}, \\text{min_error}]$. All quantities are dimensionless.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly located within the standard corpus of numerical analysis and computational engineering. It addresses the fundamental trade-off between truncation error and round-off error in finite difference methods. The function, principles, and error models are all standard and factually correct.\n- **Well-Posed:** The problem is well-defined. It specifies the function, the approximation method, the point of evaluation, the principles for derivation, and the precise tasks for computation and reporting. A unique analytical result for $h^\\star$ can be derived, and the empirical search is clearly specified.\n- **Objective:** The problem is stated in precise, objective language, free of ambiguity or subjective claims.\n\nStep 3: Verdict and Action\nThe problem statement is valid. It is a standard, well-posed problem in computational science. I will proceed with the derivation and implementation.\n\n**Derivation of the Error Model and Optimal Step Size**\n\nThe objective is to analyze the total error in the computed forward-difference approximation $\\hat{D}(h)$ of the derivative of $f(x) = \\exp(x)$ at $x=1$. The total error is the sum of two components: the truncation error, which arises from the approximation of the derivative by a finite difference, and the round-off error, which arises from the finite precision of floating-point arithmetic.\n\n1.  **Truncation Error Analysis**\n    The exact forward-difference quotient is $D(h) = \\dfrac{f(x+h) - f(x)}{h}$. We use the Taylor expansion of $f(x+h)$ around $x$:\n    $$f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3)$$\n    Substituting this into the expression for $D(h)$:\n    $$D(h) = \\frac{\\left( f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3) \\right) - f(x)}{h} = f'(x) + \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    The truncation error, $E_{\\text{trunc}}(h)$, is the difference between the approximation and the true derivative:\n    $$E_{\\text{trunc}}(h) = D(h) - f'(x) = \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    The leading-order absolute truncation error is therefore $|\\frac{h}{2}f''(x)|$.\n\n2.  **Round-off Error Analysis**\n    In floating-point arithmetic, we compute $\\hat{D}(h) = \\operatorname{fl}\\left(\\dfrac{\\operatorname{fl}(f(x+h)) - \\operatorname{fl}(f(x))}{h}\\right)$. Let us analyze the error in the numerator first. Let $y_1 = f(x)$ and $y_2 = f(x+h)$. Their computed values are:\n    $$\\hat{y}_1 = \\operatorname{fl}(y_1) = y_1(1+\\delta_1)$$\n    $$\\hat{y}_2 = \\operatorname{fl}(y_2) = y_2(1+\\delta_2)$$\n    where $|\\delta_1|, |\\delta_2| \\le u$, and $u = \\varepsilon/2$ is the unit round-off.\n    The subtraction is also subject to rounding:\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) = (\\hat{y}_2 - \\hat{y}_1)(1+\\delta_3) = (y_2(1+\\delta_2) - y_1(1+\\delta_1))(1+\\delta_3)$$\n    Expanding and retaining only first-order terms in $\\delta_i$:\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) \\approx (y_2 - y_1) + y_2\\delta_2 - y_1\\delta_1$$\n    The round-off error in the numerator is approximately $y_2\\delta_2 - y_1\\delta_1$. As $h \\to 0$, $y_2 = f(x+h) \\approx f(x) = y_1$. The error in the computed numerator is thus bounded by:\n    $$|y_2\\delta_2 - y_1\\delta_1| \\le |y_2||\\delta_2| + |y_1||\\delta_1| \\approx |f(x)|u + |f(x)|u = 2u|f(x)| = \\varepsilon|f(x)|$$\n    This error is then divided by $h$. The round-off error in the final result, $E_{\\text{round}}(h)$, is dominated by the error from the numerator. Thus, the leading-order absolute round-off error is:\n    $$|E_{\\text{round}}(h)| \\approx \\frac{\\varepsilon |f(x)|}{h}$$\n\n3.  **Total Error and Optimal Step Size**\n    The total absolute error, $\\mathcal{E}(h)$, is the sum of the magnitudes of the leading-order truncation and round-off errors:\n    $$\\mathcal{E}(h) \\approx |E_{\\text{trunc}}(h)| + |E_{\\text{round}}(h)| = \\frac{h}{2}|f''(x)| + \\frac{\\varepsilon |f(x)|}{h}$$\n    To find the step size $h^\\star$ that minimizes this total error, we differentiate $\\mathcal{E}(h)$ with respect to $h$ and set the result to zero:\n    $$\\frac{d\\mathcal{E}}{dh} = \\frac{1}{2}|f''(x)| - \\frac{\\varepsilon |f(x)|}{h^2} = 0$$\n    Solving for $h^2$:\n    $$h^2 = \\frac{2\\varepsilon |f(x)|}{|f''(x)|}$$\n    This gives the optimal step size:\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon |f(x)|}{|f''(x)|}}$$\n\n4.  **Application to $f(x) = \\exp(x)$ at $x=1$**\n    For the given function $f(x) = e^x$, we have $f'(x) = e^x$ and $f''(x) = e^x$. At the point $x=1$:\n    $$f(1) = e^1 = e$$\n    $$f''(1) = e^1 = e$$\n    Since $e > 0$, the absolute value signs can be removed. Substituting these into the formula for $h^\\star$:\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon \\cdot e}{e}} = \\sqrt{2\\varepsilon}$$\n    This is the theoretical optimal step size for the forward-difference approximation of the derivative of $e^x$ at $x=1$. It depends only on the machine epsilon $\\varepsilon$ of the floating-point arithmetic used. This completes the derivation. The implementation will now proceed to find this $h^\\star$ and compare it with empirically determined values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates truncation and round-off error in the forward-difference\n    numerical differentiation of f(x) = exp(x) at x = 1.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is (data_type_string, h_min, h_max).\n    test_cases = [\n        ('float64', 1e-16, 1e-1),\n        ('float32', 1e-10, 1e0),\n        ('float64', 1e-20, 1e-14),\n    ]\n\n    # Use a high-precision value for the true derivative f'(1) = e.\n    # np.longdouble provides higher precision than float64, preventing a bias\n    # in the error calculation.\n    true_derivative = np.exp(np.longdouble(1))\n\n    results = []\n    \n    for dtype_str, h_min, h_max in test_cases:\n        # 1. Set up the environment for the current test case.\n        if dtype_str == 'float64':\n            dtype = np.float64\n        elif dtype_str == 'float32':\n            dtype = np.float32\n        else:\n            raise ValueError(f\"Unsupported data type: {dtype_str}\")\n\n        # Get machine epsilon for the current data type.\n        eps = np.finfo(dtype).eps\n\n        # 2. Calculate the theoretical optimal step size h_star.\n        # As derived, h_star = sqrt(2 * epsilon).\n        h_star = np.sqrt(2 * eps)\n\n        # 3. Perform the empirical scan to find the optimal h.\n        \n        # Define the point of differentiation and the function f(x) = e^x,\n        # ensuring calculations use the specified data type.\n        x_val = dtype(1.0)\n        f = lambda val: np.exp(val, dtype=dtype)\n        \n        # Generate 200 logarithmically spaced values for h in the given interval.\n        # These values are cast to the target data type.\n        h_values = np.logspace(np.log10(h_min), np.log10(h_max), 200, dtype=dtype)\n        \n        min_abs_error = np.inf\n        h_emp = np.nan\n        \n        for h in h_values:\n            # Ensure h is not zero, which can happen with very small logspace ends.\n            if h == 0:\n                continue\n\n            # Calculate the forward-difference approximation D(h).\n            # The calculation is performed entirely in the target precision.\n            D_h = (f(x_val + h) - f(x_val)) / h\n            \n            # Calculate the absolute error. The subtraction is done with D_h promoted\n            # to the higher precision of true_derivative.\n            abs_error = np.abs(D_h - true_derivative)\n            \n            # Update the minimum error and corresponding h.\n            if abs_error < min_abs_error:\n                min_abs_error = abs_error\n                h_emp = h\n        \n        # Cast the minimum error back to a standard Python float for consistent output.\n        min_abs_error_float = float(min_abs_error)\n        \n        # 4. Store the results for this test case.\n        results.append([h_star, h_emp, min_abs_error_float])\n\n    # Final print statement in the exact required format.\n    # The output should look like [[a1,b1,c1],[a2,b2,c2],[a3,b3,c3]].\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2447368"}, {"introduction": "Even familiar formulas can hide numerical pitfalls, and this practice revisits the well-known quadratic formula to expose a major source of round-off error: \"catastrophic cancellation.\" This phenomenon describes the dramatic loss of precision that occurs when subtracting two nearly equal numbers. You will analyze why one root of a specific quadratic equation is vulnerable to this instability and use algebraic insight to derive a stable, equivalent expression [@problem_id:2435764]. This exercise powerfully demonstrates that analytical reformulation is a crucial technique for preserving numerical accuracy.", "problem": "Consider the quadratic polynomial $p(x) = x^2 - 10^8 x + 1$. Derive exact closed-form expressions for both real roots of $p(x) = 0$ from first principles. Then, using the basic model of rounding in finite precision floating-point arithmetic (as in the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision (binary$64$) standard), explain which of the two exact closed-form expressions is numerically unstable to evaluate and why, identifying the mechanism of catastrophic cancellation. Finally, rewrite the unstable root into an algebraically equivalent exact form that avoids subtractive cancellation. \n\nAnswer requirement: Provide the final answer as exact symbolic expressions for the two roots, without numerical approximation. No rounding is required. Express your final answer as a pair of exact expressions.", "solution": "The problem statement is subjected to validation before any attempt at a solution is made.\n\nFirst, we extract the givens of the problem.\nThe quadratic polynomial is given as $p(x) = x^2 - 10^8 x + 1$.\nThe problem requires finding the roots of the equation $p(x) = 0$.\nThe analysis of numerical stability must be performed within the context of finite precision floating-point arithmetic, specifically referencing the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision standard.\nThe task involves identifying which root expression derived from the standard quadratic formula is numerically unstable due to \"catastrophic cancellation\", and subsequently deriving an algebraically equivalent, but numerically stable, expression for that root.\n\nNext, we validate the problem statement.\nThe problem is scientifically grounded, as it deals with a fundamental topic in numerical analysis: the loss of precision when subtracting nearly equal numbers. This phenomenon, known as catastrophic cancellation, is a well-understood consequence of floating-point arithmetic. The polynomial and coefficients are mathematically sound.\nThe problem is well-posed. It provides all necessary information ($a=1$, $b=-10^8$, $c=1$) to find the roots and analyze their numerical properties. The question is unambiguous and leads to a unique set of stable expressions for the roots.\nThe problem is objective, stated in precise mathematical terms without any subjective or speculative content.\nTherefore, the problem is deemed valid and a solution will be furnished.\n\nThe quadratic equation to be solved is $x^2 - 10^8 x + 1 = 0$.\nWe apply the standard quadratic formula for the roots of $ax^2 + bx + c = 0$, which are given by $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$.\nFor the given polynomial, the coefficients are $a=1$, $b=-10^8$, and $c=1$.\nSubstituting these values into the formula yields:\n$$x = \\frac{-(-10^8) \\pm \\sqrt{(-10^8)^2 - 4(1)(1)}}{2(1)}$$\n$$x = \\frac{10^8 \\pm \\sqrt{10^{16} - 4}}{2}$$\nThis gives two exact, real roots:\n$$x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$$\n$$x_2 = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$$\n\nNow, we must analyze the numerical stability of these expressions in finite precision arithmetic. The source of potential instability lies in the subtraction of nearly equal quantities.\nLet us examine the magnitude of the terms involved. The term $10^{16}$ is vastly larger than $4$. Consequently, the value of $\\sqrt{10^{16} - 4}$ is very close to $\\sqrt{10^{16}} = 10^8$.\nTo see this more clearly, we can use a binomial approximation.\n$$\\sqrt{10^{16} - 4} = \\sqrt{10^{16}(1 - 4 \\times 10^{-16})} = 10^8 \\sqrt{1 - 4 \\times 10^{-16}}$$\nFor a small value $\\epsilon$, the approximation $(1 - \\epsilon)^{1/2} \\approx 1 - \\frac{1}{2}\\epsilon$ holds. Here, $\\epsilon = 4 \\times 10^{-16}$, which is very small.\nThus, $\\sqrt{10^{16} - 4} \\approx 10^8 (1 - \\frac{1}{2}(4 \\times 10^{-16})) = 10^8 (1 - 2 \\times 10^{-16}) = 10^8 - 2 \\times 10^{-8}$.\nThe value of $\\sqrt{10^{16} - 4}$ is only slightly less than $10^8$.\n\nConsider the expression for $x_1$: $x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$. This expression involves the addition of two large positive numbers of similar magnitude. In floating-point arithmetic, this operation is numerically stable. The relative error of the sum is small, on the order of the machine precision.\n\nConsider the expression for $x_2$: $x_2 = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$. This expression involves the subtraction of two very nearly equal numbers. Let $y = \\sqrt{10^{16} - 4}$. In floating-point computation, $10^8$ and the computed value of $y$ will agree in many of their leading significant digits. When the subtraction $10^8 - y$ is performed, these leading digits cancel, and the result is determined by the remaining, less significant digits, which are heavily influenced by the rounding errors incurred when computing $y$. This massive increase in relative error is the phenomenon of catastrophic cancellation. Therefore, the expression for $x_2$ is numerically unstable and would yield a highly inaccurate result if evaluated directly using standard double-precision arithmetic.\n\nTo find a numerically stable expression for $x_2$, we use Vieta's formulas, which relate the coefficients of a polynomial to its roots. For a quadratic equation $ax^2 + bx + c = 0$, the product of the roots is given by $x_1 x_2 = \\frac{c}{a}$.\nFor our equation, this gives $x_1 x_2 = \\frac{1}{1} = 1$.\nWe can compute the stable root $x_1$ accurately using its formula. Then, we can find $x_2$ from the relation $x_2 = \\frac{1}{x_1}$.\nSubstituting the stable expression for $x_1$:\n$$x_2 = \\frac{1}{\\frac{10^8 + \\sqrt{10^{16} - 4}}{2}} = \\frac{2}{10^8 + \\sqrt{10^{16} - 4}}$$\nThis revised expression for $x_2$ involves only the addition of positive numbers and a division, both of which are numerically stable operations. It avoids subtractive cancellation and is therefore the preferred form for numerical computation.\n\nWe verify that this new form is algebraically equivalent to the original unstable form for $x_2$ by rationalizing its denominator:\n$$\\frac{2}{10^8 + \\sqrt{10^{16} - 4}} \\cdot \\frac{10^8 - \\sqrt{10^{16} - 4}}{10^8 - \\sqrt{10^{16} - 4}} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{(10^8)^2 - (10^{16} - 4)} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{10^{16} - 10^{16} + 4} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{4} = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$$\nThis confirms the algebraic identity.\n\nThe final required answer consists of the exact symbolic expressions for both roots, rewritten in their numerically stable forms.\nThe larger root is $x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$.\nThe smaller root, in its stable form, is $x_2 = \\frac{2}{10^8 + \\sqrt{10^{16} - 4}}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{10^8 + \\sqrt{10^{16} - 4}}{2} & \\frac{2}{10^8 + \\sqrt{10^{16} - 4}} \\end{pmatrix}}$$", "id": "2435764"}, {"introduction": "The choice of algorithm can have a profound impact on the numerical stability of a computation, a principle this exercise makes tangible. Using the common statistical task of calculating variance, you will compare two distinct methods: the \"one-pass\" textbook formula, which is highly susceptible to catastrophic cancellation, and the more robust \"two-pass\" method [@problem_id:2447454]. By implementing both algorithms and measuring their error against a high-precision reference, you will learn to evaluate computational strategies not just for their theoretical correctness, but for their practical performance in finite-precision arithmetic.", "problem": "You are to implement a complete, runnable program that demonstrates truncation and round-off error in the computation of variance for datasets with a large mean and small deviations, comparing two computational formulas. The foundational base is the definition of variance as the second central moment of a real-valued random variable and the standard floating-point rounding model. Use the following facts as the starting point:\n- The variance of a real-valued random variable $X$ with finite second moment is defined by the second central moment: $\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big]$, where $\\mu = \\mathbb{E}[X]$.\n- For real numbers, the second raw moment satisfies $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + \\mu^2$.\n- Floating-point arithmetic in the Institute of Electrical and Electronics Engineers (IEEE) binary64 format (often called double precision) approximately obeys the rounding model $ \\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ with $|\\delta| \\le \\epsilon_{\\text{mach}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon, and $\\circ$ is an arithmetic operation. Subtracting nearly equal numbers can cause catastrophic cancellation, whereby significant digits are lost.\n\nYour program must:\n- Construct specified datasets whose elements have a large mean and small deviations.\n- Compute the variance using two methods in IEEE binary64 arithmetic:\n  1. The raw-moment one-pass form: compute $\\mathbb{E}[X]$ and $\\mathbb{E}[X^2]$ and then form $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n  2. The centered two-pass form: first compute $\\mu = \\mathbb{E}[X]$, then compute $\\mathbb{E}[(X-\\mu)^2]$ in a second pass.\n- Compute a high-precision reference variance using decimal arithmetic with sufficiently high precision so that round-off in the operations is negligible compared to IEEE binary64. Use the central-moment definition $\\mathbb{E}[(X-\\mu)^2]$ in high precision.\n- Quantify the absolute error of each floating-point method relative to the high-precision reference.\n- Produce a single line of output containing, for each dataset in the test suite and in order, a list of five values: the one-pass variance, the two-pass variance, the high-precision reference variance, the absolute error of the one-pass result, and the absolute error of the two-pass result.\n\nAll datasets are purely numeric; there are no physical units in this problem. Angles are not involved.\n\nTest suite to cover the happy path, boundary emphasis, and edge cases for catastrophic cancellation:\n- Test $1$ (symmetric small deviations around a large mean, integers to avoid input quantization): Let $M = 10^{8}$ and $D = \\{-3,-1,0,1,3\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals the average of squared deviations, which is $4$.\n- Test $2$ (non-symmetric small deviations with nonzero mean of deviations): Let $M = 10^{8}$ and $D = \\{0,1,2,3,4\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals $2$.\n- Test $3$ (larger sample with tiny, smoothly varying deviations): Let $M = 10^{8}$ and $D = \\left\\{ \\frac{k-500}{1000} \\;\\middle|\\; k=0,1,\\dots,999 \\right\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals is the population variance of this arithmetic grid; it is close to $1/12$ minus the square of the small mean of deviations and must be computed exactly by your high-precision routine.\n- Test $4$ (extreme small deviations near the limit of resolution relative to the mean): Let $M = 10^{8}$ and $D = \\{10^{-8},-10^{-8}\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals $10^{-16}$.\n\nHigh-precision reference requirement:\n- Build the reference using base-ten decimal arithmetic with at least $p = 100$ digits of precision. Construct the dataset using exact decimal values for $M$ and $D$ as specified above (e.g., use $M = 100000000$ exactly and rational deviations such as $(k-500)/1000$ as exact decimals). Compute the population variance using the two-pass central-moment definition $\\mathbb{E}[(X-\\mu)^2]$, where $\\mathbb{E}[\\cdot]$ is the arithmetic mean over the finite set.\n\nFloating-point computations requirement:\n- Use IEEE binary64 (double precision) via standard arrays in a numerical library to compute the one-pass raw-moment and two-pass centered-moment population variances. Do not apply any compensated summation or numerically stabilized tricks; use straightforward means and sums in the obvious ways so that truncation and round-off errors are visible.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-like list of four inner lists, one per test case in the order Tests $1$ through $4$. Each inner list must have the five floats: $[\\text{var\\_one\\_pass}, \\text{var\\_two\\_pass}, \\text{var\\_ref}, \\text{abs\\_err\\_one}, \\text{abs\\_err\\_two}]$. For example, a syntactically valid output line looks like $[[v_{11},v_{12},v_{13},e_{11},e_{12}],[v_{21},v_{22},v_{23},e_{21},e_{22}],\\dots]$ with numeric values filled in by your program.\n\nThere must be no user input and no external files. The program must fully determine the test data, perform the computations, and print the single required output line. The outputs must be floating-point numbers.", "solution": "The user has presented a problem in computational engineering that requires an analysis of numerical stability in variance calculations. The problem is valid, well-posed, and scientifically grounded. It addresses a fundamental issue in numerical methods: the loss of precision due to catastrophic cancellation in floating-point arithmetic.\n\nThe core task is to compare two formulas for computing the population variance of a dataset $X = \\{x_1, x_2, \\dots, x_N\\}$:\n\n1.  **The one-pass (or raw-moment) formula:** This method is derived from the algebraic identity $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Computationally, it involves a single pass through the data to compute the sum of values and the sum of squares, from which the means are calculated. The formula is:\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i\\right)^2 $$\n    While mathematically exact for real numbers, this formula is numerically unstable when the standard deviation $\\sigma$ is small compared to the mean $\\mu = \\mathbb{E}[X]$. The two terms, $\\mathbb{E}[X^2]$ and $(\\mathbb{E}[X])^2$, become very close to each other. Specifically, $\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2$, so the formula amounts to computing $\\sigma^2 = (\\sigma^2 + \\mu^2) - \\mu^2$. When evaluated using finite-precision floating-point arithmetic, such as IEEE binary64, this involves the subtraction of two very large, nearly identical numbers. This operation is a classic example of catastrophic cancellation. The leading digits of the two numbers cancel out, resulting in a loss of most or all significant digits of the small difference. The rounding errors in the computation of $\\mathbb{E}[X^2]$ and $(\\mathbb{E}[X])^2$, which are on the order of $\\mu^2 \\epsilon_{\\text{mach}}$, become the dominant part of the final result, potentially yielding a variance that is highly inaccurate, or even negative.\n\n2.  **The two-pass (or centered-moment) formula:** This method adheres more closely to the definition of variance as the mean of the squared deviations from the mean. It requires two passes over the data.\n    $$ \\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i $$\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2 $$\n    In the first pass, the mean $\\mu$ is computed. In the second pass, this computed mean is used to find the squared deviations $(x_i - \\mu)^2$, which are then averaged. This method is far more numerically robust. The subtraction $x_i - \\mu$ is still performed, but the result is a set of small numbers (the deviations). Squaring and summing these small numbers does not involve the subtraction of large quantities. The primary source of error is the initial computation of $\\mu$. The error in the computed mean, $\\hat{\\mu}$, propagates to the deviations. However, this error is typically small. For data with a large mean $M$ and small deviations, the error in the computed mean is on the order of $M\\epsilon_{\\text{mach}}$. As long as this error is small relative to the magnitude of the true deviations, the two-pass algorithm yields an accurate result.\n\nFor this problem, a high-precision reference calculation is also required. This will be performed using the Python `decimal` module, with a precision of $p=100$ digits. At this level of precision, the rounding errors are negligible compared to those in standard binary64 floating-point arithmetic, providing a \"ground truth\" against which the two other methods can be compared.\n\nThe program will be structured to process four specific test cases. Each case uses a dataset with a large mean ($M=10^8$) and small deviations, designed to expose the numerical flaws of the one-pass formula.\n\n-   **Test 1 & 2:** Deviations are small integers. The two-pass method is expected to be highly accurate. The one-pass method is expected to fail due to catastrophic cancellation.\n-   **Test 3:** A larger dataset with smoothly varying, small rational deviations. Similar behavior is expected.\n-   **Test 4:** Deviations are extremely small ($d = \\pm 10^{-8}$), near the resolution limit of binary64 relative to the mean. For $x = M+d$, the value of $d$ is smaller than the smallest possible increment for a number of magnitude $M$ (which is $M \\cdot \\epsilon_{\\text{mach}} \\approx 10^8 \\cdot 2.22 \\times 10^{-16} = 2.22 \\times 10^{-8}$). As a result, $fl(M+d)$ will be rounded to $M$ itself. This demonstrates a different source of error: loss of information in the initial data representation, which will cause both floating-point methods to compute a variance of $0$.\n\nThe implementation will proceed by defining a function that takes a dataset, computes the variance using the three methods (one-pass float, two-pass float, and high-precision reference), calculates the absolute errors for the float methods, and returns the results. This function will be called for each test case, and the collected results will be formatted into the specified output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares variance using three different methods to demonstrate\n    truncation and round-off errors.\n    \"\"\"\n    # Set the precision for decimal arithmetic to 100 digits, as required.\n    getcontext().prec = 100\n\n    def analyze_dataset(M_str: str, D_list: list):\n        \"\"\"\n        Performs the full analysis for a given dataset definition.\n\n        Args:\n            M_str: The large mean component as a string for exact Decimal conversion.\n            D_list: A list of deviation values (as Decimal objects).\n\n        Returns:\n            A list containing the five required values:\n            [var_one_pass, var_two_pass, var_ref, abs_err_one, abs_err_two]\n        \"\"\"\n        # 1. High-precision reference calculation using the decimal module.\n        # This serves as the ground truth.\n        M_dec = Decimal(M_str)\n        X_dec = [M_dec + d for d in D_list]\n        N_dec = Decimal(len(X_dec))\n        \n        # Use two-pass formula for the reference calculation.\n        mu_dec = sum(X_dec) / N_dec\n        var_ref = sum([(x - mu_dec)**2 for x in X_dec]) / N_dec\n\n        # 2. Floating-point calculations using numpy (IEEE binary64).\n        # Construct the dataset using standard float64.\n        # Note: float() conversion from Decimal can introduce small errors,\n        # but the dominant error source is the variance algorithm itself.\n        X_fp = np.array([float(x) for x in X_dec], dtype=np.float64)\n        \n        # Method 1: One-pass (raw-moment) formula. Prone to catastrophic cancellation.\n        # sigma^2 = E[X^2] - (E[X])^2\n        mean_of_squares = np.mean(X_fp**2)\n        square_of_mean = np.mean(X_fp)**2\n        var_one_pass = mean_of_squares - square_of_mean\n        \n        # Method 2: Two-pass (centered-moment) formula. More numerically stable.\n        # sigma^2 = E[(X - E[X])^2]\n        mu_fp = np.mean(X_fp)\n        var_two_pass = np.mean((X_fp - mu_fp)**2)\n        \n        # 3. Quantify absolute errors.\n        abs_err_one = abs(var_one_pass - float(var_ref))\n        abs_err_two = abs(var_two_pass - float(var_ref))\n        \n        return [var_one_pass, var_two_pass, float(var_ref), abs_err_one, abs_err_two]\n\n    # --- Define and run all test cases ---\n    test_cases_defs = [\n        # Test 1: Symmetric small integer deviations. True Var = 4.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['-3', '-1', '0', '1', '3']]},\n        \n        # Test 2: Non-symmetric small integer deviations. True Var = 2.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['0', '1', '2', '3', '4']]},\n        \n        # Test 3: Larger sample with small rational deviations.\n        {'M': '1e8', 'D': [(Decimal(k) - Decimal(500)) / Decimal(1000) for k in range(1000)]},\n        \n        # Test 4: Extremely small deviations at the limit of float64 resolution. True Var = 1e-16.\n        {'M': '1e8', 'D': [Decimal('1e-8'), Decimal('-1e-8')]}\n    ]\n\n    all_results = []\n    for case in test_cases_defs:\n        result = analyze_dataset(case['M'], case['D'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[v1,v2,...],[v1,v2,...]]\n    formatted_inner_lists = []\n    for res_list in all_results:\n        # Format each inner list as \"[v1,v2,v3,e1,e2]\"\n        formatted_list_str = f\"[{','.join(map(str, res_list))}]\"\n        formatted_inner_lists.append(formatted_list_str)\n    \n    # Join the inner lists into the final output format \"[ [...], [...], ... ]\"\n    final_output = f\"[{','.join(formatted_inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447454"}]}