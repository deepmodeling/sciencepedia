{"hands_on_practices": [{"introduction": "To build a strong intuition for problem conditioning, it is best to start with an ideal case. This exercise explores the conditioning of a rotation matrix, a fundamental operation in many engineering disciplines. By calculating its condition number [@problem_id:2428603], you will discover why such orthogonal transformations are considered the gold standard of numerical stability, representing perfectly-conditioned operations that preserve the geometry of a problem without amplifying errors.", "problem": "Consider the $2$-dimensional rotation matrix\n$$\nR(\\theta)=\\begin{pmatrix}\n\\cos(\\theta)  -\\sin(\\theta)\\\\\n\\sin(\\theta)  \\cos(\\theta)\n\\end{pmatrix},\n$$\nwhere $\\theta$ is a real angle measured in radians. Using the matrix norm induced by the Euclidean vector norm, determine the condition number $\\kappa_{2}\\!\\left(R(\\theta)\\right)$ of $R(\\theta)$ as a function of $\\theta$. Then, based solely on the value you obtain, interpret what this implies for the stability of solving linear systems that are pre-multiplied by $R(\\theta)$ (that is, replacing $A\\boldsymbol{x}=\\boldsymbol{b}$ by $R(\\theta)A\\boldsymbol{x}=R(\\theta)\\boldsymbol{b}$), in comparison to the original system.\n\nAnswer form: Provide the exact value of $\\kappa_{2}\\!\\left(R(\\theta)\\right)$ as a closed-form expression. No rounding is required. Do not include any units.", "solution": "The problem is subjected to validation and is found to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a rigorous solution. We proceed.\n\nThe condition number of a matrix $A$ with respect to the matrix norm induced by the a vector norm $\\|\\cdot\\|$ is defined as $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$. The problem specifies the Euclidean vector norm, which induces the spectral norm, or $2$-norm, for matrices. Thus, we must compute $\\kappa_{2}(R(\\theta)) = \\|R(\\theta)\\|_{2} \\|R(\\theta)^{-1}\\|_{2}$.\n\nThe matrix in question is the $2$-dimensional rotation matrix:\n$$\nR(\\theta) = \\begin{pmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{pmatrix}\n$$\nThe spectral norm of a matrix $A$ is given by $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{T}A)}$, where $\\lambda_{\\max}(A^{T}A)$ is the largest eigenvalue of the matrix $A^{T}A$.\n\nFirst, we compute the transpose of $R(\\theta)$:\n$$\nR(\\theta)^{T} = \\begin{pmatrix} \\cos(\\theta)  \\sin(\\theta) \\\\ -\\sin(\\theta)  \\cos(\\theta) \\end{pmatrix}\n$$\nNext, we compute the product $R(\\theta)^{T}R(\\theta)$:\n$$\nR(\\theta)^{T}R(\\theta) = \\begin{pmatrix} \\cos(\\theta)  \\sin(\\theta) \\\\ -\\sin(\\theta)  \\cos(\\theta) \\end{pmatrix} \\begin{pmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\cos^{2}(\\theta) + \\sin^{2}(\\theta)  -\\cos(\\theta)\\sin(\\theta) + \\sin(\\theta)\\cos(\\theta) \\\\ -\\sin(\\theta)\\cos(\\theta) + \\cos(\\theta)\\sin(\\theta)  \\sin^{2}(\\theta) + \\cos^{2}(\\theta) \\end{pmatrix}\n$$\nUsing the fundamental trigonometric identity $\\cos^{2}(\\theta) + \\sin^{2}(\\theta) = 1$, the product simplifies to the identity matrix:\n$$\nR(\\theta)^{T}R(\\theta) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I\n$$\nThis result demonstrates that $R(\\theta)$ is an orthogonal matrix.\n\nThe eigenvalues of $R(\\theta)^{T}R(\\theta) = I$ are the solutions to the characteristic equation $\\det(I - \\lambda I) = 0$, which is $(1-\\lambda)^{2} = 0$. The eigenvalues are $\\lambda_{1} = \\lambda_{2} = 1$. The largest eigenvalue is $\\lambda_{\\max}(R(\\theta)^{T}R(\\theta)) = 1$.\nTherefore, the spectral norm of $R(\\theta)$ is:\n$$\n\\|R(\\theta)\\|_{2} = \\sqrt{1} = 1\n$$\nNow, we must find the norm of the inverse matrix, $\\|R(\\theta)^{-1}\\|_{2}$. From the property $R(\\theta)^{T}R(\\theta) = I$, it follows directly that the inverse of $R(\\theta)$ is its transpose:\n$$\nR(\\theta)^{-1} = R(\\theta)^{T}\n$$\nSince $R(\\theta)^{T}$ is also an orthogonal matrix (as $(R(\\theta)^{T})^{T}R(\\theta)^{T} = R(\\theta)R(\\theta)^{T} = I$), its spectral norm must also be $1$. We can show this explicitly:\n$$\n\\|R(\\theta)^{-1}\\|_{2} = \\|R(\\theta)^{T}\\|_{2}\n$$\nThe norm of $R(\\theta)^{T}$ is $\\sqrt{\\lambda_{\\max}((R(\\theta)^{T})^{T}R(\\theta)^{T})} = \\sqrt{\\lambda_{\\max}(R(\\theta)R(\\theta)^{T})}$.\nThe product $R(\\theta)R(\\theta)^{T}$ is also the identity matrix, $I$. The eigenvalues are again $1$, so $\\lambda_{\\max}(R(\\theta)R(\\theta)^{T}) = 1$.\nThus, $\\|R(\\theta)^{-1}\\|_{2} = \\sqrt{1} = 1$.\n\nWith the norms of both $R(\\theta)$ and its inverse determined, the condition number is:\n$$\n\\kappa_{2}(R(\\theta)) = \\|R(\\theta)\\|_{2} \\|R(\\theta)^{-1}\\|_{2} = 1 \\cdot 1 = 1\n$$\nThe condition number of the rotation matrix $R(\\theta)$ is $1$ for any real angle $\\theta$. This is the minimum possible value for the condition number of any invertible matrix, indicating that the matrix is perfectly conditioned.\n\nFor the second part of the problem, we must interpret this result in the context of solving a linear system. We are comparing the stability of solving $A\\boldsymbol{x}=\\boldsymbol{b}$ to solving $A'\\boldsymbol{x}=\\boldsymbol{b}'$, where $A' = R(\\theta)A$ and $\\boldsymbol{b}'=R(\\theta)\\boldsymbol{b}$. The conditioning of the modified system is determined by $\\kappa_{2}(A')$. We can use the result $\\kappa_{2}(R(\\theta)) = 1$ to analyze $\\kappa_{2}(R(\\theta)A)$.\n\nThe condition number has the submultiplicative property $\\kappa(XY) \\le \\kappa(X)\\kappa(Y)$. Applying this property, we have:\n$$\n\\kappa_{2}(R(\\theta)A) \\le \\kappa_{2}(R(\\theta)) \\kappa_{2}(A)\n$$\nSince $\\kappa_{2}(R(\\theta)) = 1$, this inequality becomes:\n$$\n\\kappa_{2}(R(\\theta)A) \\le \\kappa_{2}(A)\n$$\nTo establish equality, we can write the original matrix $A$ as $A = R(\\theta)^{-1} (R(\\theta)A)$. Applying the submultiplicative property again:\n$$\n\\kappa_{2}(A) \\le \\kappa_{2}(R(\\theta)^{-1}) \\kappa_{2}(R(\\theta)A)\n$$\nThe inverse of $R(\\theta)$ is $R(\\theta)^{-1} = R(-\\theta)$, which is also a rotation matrix. Our calculation for $\\kappa_{2}(R(\\theta))$ is valid for any real angle, so $\\kappa_{2}(R(-\\theta)) = 1$. Therefore, $\\kappa_{2}(R(\\theta)^{-1})=1$. The inequality becomes:\n$$\n\\kappa_{2}(A) \\le \\kappa_{2}(R(\\theta)A)\n$$\nCombining the two inequalities, $\\kappa_{2}(R(\\theta)A) \\le \\kappa_{2}(A)$ and $\\kappa_{2}(A) \\le \\kappa_{2}(R(\\theta)A)$, we are forced to conclude that:\n$$\n\\kappa_{2}(R(\\theta)A) = \\kappa_{2}(A)\n$$\nThis means that pre-multiplying a linear system by a rotation matrix does not change the condition number of the system matrix with respect to the $2$-norm. A condition number of $1$ for $R(\\theta)$ implies that this transformation is an isometry and is perfectly stable. It preserves the geometry of the problem and, critically, does not amplify the sensitivity of the linear system to perturbations. The stability of solving the modified system $R(\\theta)A\\boldsymbol{x}=R(\\theta)\\boldsymbol{b}$ is identical to that of the original system $A\\boldsymbol{x}=\\boldsymbol{b}$.", "answer": "$$\n\\boxed{1}\n$$", "id": "2428603"}, {"introduction": "Having seen the ideal stability of a perfectly-conditioned matrix, we now turn to the opposite extreme to witness the practical dangers of ill-conditioning. In this problem [@problem_id:2428587], you will do more than just analyze a system; you will actively construct a scenario where a tiny, one-percent relative perturbation in the input vector $b$ of a system $A x = b$ causes the solution vector $x$ to jump to an entirely different quadrant. This exercise provides a stark, tangible demonstration of how ill-conditioning can lead to catastrophic failures in numerical computations.", "problem": "In computational engineering, the conditioning of a linear system is assessed by how sensitively its solution responds to small perturbations in the data. Consider the linear system $A x = b$ in $\\mathbb{R}^{2}$, where\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 1  1.05 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}.\n$$\nUse the Euclidean vector norm and the induced matrix norm (matrix $2$-norm) throughout.\n\n1) Starting from the definitions of solution sensitivity and induced norms, verify by explicit construction that there exists a perturbation $\\Delta b$ with relative size $\\|\\Delta b\\|_{2}/\\|b\\|_{2} = 0.01$ such that the solution $\\tilde{x}$ to $A \\tilde{x} = b + \\Delta b$ lies in a different open quadrant of $\\mathbb{R}^{2}$ than the solution $x$ to $A x = b$. You must build $\\Delta b$ explicitly and justify every step from first principles.\n\n2) Using the definition of the condition number in the matrix $2$-norm, and without invoking any unproven shortcut formulas, compute the condition number $\\kappa_{2}(A)$ for the given matrix $A$.\n\nProvide as your final answer only the numerical value of $\\kappa_{2}(A)$, rounded to four significant figures. No units are required.", "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n- Linear system: $A x = b$ in $\\mathbb{R}^{2}$.\n- Matrix: $A = \\begin{pmatrix} 1  1 \\\\ 1  1.05 \\end{pmatrix}$.\n- Vector: $b = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}$.\n- Norms: Euclidean vector norm ($\\|\\cdot\\|_{2}$) and the induced matrix norm ($2$-norm).\n- Perturbed system: $A \\tilde{x} = b + \\Delta b$.\n- Perturbation constraint: $\\|\\Delta b\\|_{2}/\\|b\\|_{2} = 0.01$.\n- Task 1: Construct $\\Delta b$ such that $\\tilde{x}$ and $x$ are in different open quadrants.\n- Task 2: Compute the condition number $\\kappa_{2}(A)$ from its definition.\n\nValidation Verdict:\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard exercise in numerical linear algebra concerning the sensitivity of linear systems, a core topic in computational engineering. The matrix $A$ is invertible as its determinant is $\\det(A) = (1)(1.05) - (1)(1) = 0.05 \\neq 0$, ensuring a unique solution exists. All data and constraints are provided, and there are no contradictions or ambiguities. The problem is valid.\n\nThe solution proceeds.\n\nPart 1: Construction of a suitable perturbation $\\Delta b$.\n\nFirst, we solve the unperturbed system $A x = b$ to find the original solution $x$.\n$$\n\\begin{pmatrix} 1  1 \\\\ 1  1.05 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n$x_1 + x_2 = 1$\n$x_1 + 1.05 x_2 = 1.015$\n\nSubtracting the first equation from the second yields:\n$(x_1 + 1.05 x_2) - (x_1 + x_2) = 1.015 - 1$\n$0.05 x_2 = 0.015$\n$x_2 = \\frac{0.015}{0.05} = \\frac{15}{50} = \\frac{3}{10} = 0.3$.\n\nSubstituting $x_2 = 0.3$ into the first equation:\n$x_1 + 0.3 = 1 \\implies x_1 = 0.7$.\n\nThe solution is $x = \\begin{pmatrix} 0.7 \\\\ 0.3 \\end{pmatrix}$. Since $x_1  0$ and $x_2  0$, this vector lies in the first open quadrant of $\\mathbb{R}^{2}$.\n\nNext, we analyze the perturbed system $A \\tilde{x} = b + \\Delta b$. The solution is $\\tilde{x} = A^{-1}(b + \\Delta b) = A^{-1}b + A^{-1}\\Delta b = x + \\Delta x$, where $\\Delta x = A^{-1}\\Delta b$. We must construct a perturbation $\\Delta b$ such that $\\tilde{x}$ lies in a different open quadrant. For example, we can aim for the fourth quadrant, which requires $\\tilde{x}_1  0$ and $\\tilde{x}_2  0$.\n\nThe new solution's components are $\\tilde{x}_1 = x_1 + \\Delta x_1 = 0.7 + \\Delta x_1$ and $\\tilde{x}_2 = x_2 + \\Delta x_2 = 0.3 + \\Delta x_2$. To move to the fourth quadrant, we need $\\tilde{x}_2  0$, which implies $0.3 + \\Delta x_2  0$, or $\\Delta x_2  -0.3$.\n\nTo find the relationship between $\\Delta x$ and $\\Delta b$, we must compute the inverse of $A$.\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 1.05  -1 \\\\ -1  1 \\end{pmatrix} = \\frac{1}{0.05} \\begin{pmatrix} 1.05  -1 \\\\ -1  1 \\end{pmatrix} = 20 \\begin{pmatrix} 1.05  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} 21  -20 \\\\ -20  20 \\end{pmatrix}.\n$$\nThe change in the solution is $\\Delta x = \\begin{pmatrix} \\Delta x_1 \\\\ \\Delta x_2 \\end{pmatrix} = A^{-1} \\Delta b = \\begin{pmatrix} 21  -20 \\\\ -20  20 \\end{pmatrix} \\begin{pmatrix} \\Delta b_1 \\\\ \\Delta b_2 \\end{pmatrix}$.\nThe second component is $\\Delta x_2 = -20 \\Delta b_1 + 20 \\Delta b_2 = 20(\\Delta b_2 - \\Delta b_1)$.\nOur condition $\\Delta x_2  -0.3$ becomes $20(\\Delta b_2 - \\Delta b_1)  -0.3$, which simplifies to $\\Delta b_2 - \\Delta b_1  -0.015$.\n\nThe perturbation $\\Delta b$ must also satisfy the relative size constraint $\\|\\Delta b\\|_{2} / \\|b\\|_{2} = 0.01$. First, we compute the norm of $b$.\n$$\n\\|b\\|_{2}^{2} = 1^{2} + 1.015^{2} = 1 + 1.030225 = 2.030225.\n$$\nSo, $\\|b\\|_{2} = \\sqrt{2.030225}$.\nThe constraint on $\\Delta b$ is $\\|\\Delta b\\|_{2} = 0.01 \\|b\\|_{2} = 0.01 \\sqrt{2.030225}$.\nThis implies $\\|\\Delta b\\|_{2}^{2} = (0.01)^{2} (2.030225) = 0.0002030225$.\n\nWe need to find a vector $\\Delta b = \\begin{pmatrix} \\Delta b_1 \\\\ \\Delta b_2 \\end{pmatrix}$ that satisfies both $\\Delta b_2 - \\Delta b_1  -0.015$ and $\\Delta b_1^2 + \\Delta b_2^2 = 0.0002030225$.\nTo satisfy the first condition as strongly as possible for a fixed norm, we should choose $\\Delta b$ such that the quantity $\\Delta b_2 - \\Delta b_1$ is minimized. This occurs when $\\Delta b$ is proportional to the vector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ but with a negative projection on the y-axis and positive on the x-axis. Thus, we choose $\\Delta b$ to be in the direction of the vector $\\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$.\n\nLet $\\Delta b = R \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$, where $R = \\|\\Delta b\\|_{2} = 0.01 \\sqrt{2.030225}$.\nSo, $\\Delta b_1 = \\frac{R}{\\sqrt{2}}$ and $\\Delta b_2 = -\\frac{R}{\\sqrt{2}}$.\nThe vector is explicitly:\n$$\n\\Delta b = \\frac{0.01 \\sqrt{2.030225}}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nLet us verify this construction. The norm is $\\|\\Delta b\\|_{2} = \\sqrt{(\\frac{R}{\\sqrt{2}})^2 + (-\\frac{R}{\\sqrt{2}})^2} = \\sqrt{\\frac{R^2}{2} + \\frac{R^2}{2}} = R$, which is correct by construction.\nNow check the condition on the components:\n$\\Delta b_2 - \\Delta b_1 = -\\frac{R}{\\sqrt{2}} - \\frac{R}{\\sqrt{2}} = -\\frac{2R}{\\sqrt{2}} = -R\\sqrt{2}$.\nSubstituting $R = 0.01 \\sqrt{2.030225}$:\n$$\n\\Delta b_2 - \\Delta b_1 = -(0.01 \\sqrt{2.030225})\\sqrt{2} = -0.01 \\sqrt{4.06045}.\n$$\nSince $2^2=4$, we have $\\sqrt{4.06045}  2$. Thus, $\\Delta b_2 - \\Delta b_1  -0.01 \\times 2 = -0.02$.\nAs $-0.02  -0.015$, our condition is satisfied.\nLet's compute the change in the solution vector $x$.\n$\\Delta x_2 = 20(\\Delta b_2 - \\Delta b_1) = 20(-0.01\\sqrt{4.06045}) = -0.2\\sqrt{4.06045}$.\nNumerically, $\\Delta x_2 \\approx -0.2 \\times 2.01505... \\approx -0.403$.\nThe new component $\\tilde{x}_2 = x_2 + \\Delta x_2 = 0.3 + \\Delta x_2 \\approx 0.3 - 0.403 = -0.103  0$.\n\nFor completeness, we check $\\tilde{x}_1$.\n$\\Delta x_1 = 21 \\Delta b_1 - 20 \\Delta b_2 = 21(\\frac{R}{\\sqrt{2}}) - 20(-\\frac{R}{\\sqrt{2}}) = \\frac{41R}{\\sqrt{2}} = \\frac{41(0.01\\sqrt{2.030225})}{\\sqrt{2}}  0$.\nSo $\\tilde{x}_1 = x_1 + \\Delta x_1  0.7  0$.\nThe new solution $\\tilde{x}$ has components $\\tilde{x}_1  0$ and $\\tilde{x}_2  0$, placing it in the fourth quadrant. The construction is valid.\n\nPart 2: Computation of the condition number $\\kappa_{2}(A)$.\n\nThe condition number in the matrix $2$-norm is defined as $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$.\nThe problem requires this to be computed from first principles, not by invoking unproven shortcut formulas. The matrix $2$-norm (or spectral norm) of a matrix $M$ is defined as its largest singular value, $\\|M\\|_{2} = \\sigma_{\\max}(M) = \\sqrt{\\lambda_{\\max}(M^{T}M)}$, where $\\lambda_{\\max}(M^{T}M)$ is the largest eigenvalue of $M^{T}M$.\n\nThe given matrix $A = \\begin{pmatrix} 1  1 \\\\ 1  1.05 \\end{pmatrix}$ is a symmetric matrix, i.e., $A = A^{T}$.\nFor a symmetric matrix $A$, the product $A^{T}A$ becomes $A^{2}$.\nThe eigenvalues of $A^{2}$ are the squares of the eigenvalues of $A$. If $\\lambda$ is an eigenvalue of $A$ with eigenvector $v$, then $A^{2}v = A(Av) = A(\\lambda v) = \\lambda(Av) = \\lambda^2 v$.\nThus, $\\lambda_{\\max}(A^{T}A) = \\lambda_{\\max}(A^{2}) = (\\max_i |\\lambda_i(A)|)^{2}$, where $\\lambda_i(A)$ are the eigenvalues of $A$.\nTherefore, for a symmetric matrix $A$, its $2$-norm is the maximum absolute eigenvalue:\n$$\n\\|A\\|_{2} = \\sqrt{(\\max_i |\\lambda_i(A)|)^{2}} = \\max_i |\\lambda_i(A)|.\n$$\nThe inverse $A^{-1}$ is also symmetric. Its eigenvalues are the reciprocals of the eigenvalues of $A$.\nSo, $\\|A^{-1}\\|_{2} = \\max_i |1/\\lambda_i(A)| = 1 / \\min_i |\\lambda_i(A)|$.\nThis holds provided that $A$ is invertible, which it is.\nCombining these results, the condition number for a symmetric matrix $A$ is the ratio of its largest to its smallest absolute eigenvalues:\n$$\n\\kappa_{2}(A) = \\frac{\\max_i |\\lambda_i(A)|}{\\min_i |\\lambda_i(A)|}.\n$$\nWe now find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 1-\\lambda  1 \\\\ 1  1.05-\\lambda \\end{pmatrix} = (1-\\lambda)(1.05-\\lambda) - 1 = 0.\n$$\n$$\n\\lambda^{2} - 1.05\\lambda - \\lambda + 1.05 - 1 = 0\n$$\n$$\n\\lambda^{2} - 2.05\\lambda + 0.05 = 0.\n$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$\n\\lambda = \\frac{2.05 \\pm \\sqrt{(-2.05)^{2} - 4(1)(0.05)}}{2} = \\frac{2.05 \\pm \\sqrt{4.2025 - 0.2}}{2} = \\frac{2.05 \\pm \\sqrt{4.0025}}{2}.\n$$\nBoth the trace ($2.05  0$) and determinant ($0.05  0$) of $A$ are positive, so $A$ is positive definite, and its eigenvalues are positive.\nThe maximum and minimum eigenvalues are:\n$$\n\\lambda_{\\max} = \\frac{2.05 + \\sqrt{4.0025}}{2}\n$$\n$$\n\\lambda_{\\min} = \\frac{2.05 - \\sqrt{4.0025}}{2}\n$$\nThe condition number is their ratio:\n$$\n\\kappa_{2}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{(2.05 + \\sqrt{4.0025})/2}{(2.05 - \\sqrt{4.0025})/2} = \\frac{2.05 + \\sqrt{4.0025}}{2.05 - \\sqrt{4.0025}}.\n$$\nTo compute the numerical value:\n$\\sqrt{4.0025} \\approx 2.0006249377...$\n$$\n\\kappa_{2}(A) \\approx \\frac{2.05 + 2.0006249377}{2.05 - 2.0006249377} = \\frac{4.0506249377}{0.0493750623} \\approx 82.038160...\n$$\nRounding to four significant figures, we get $82.04$.", "answer": "$$\\boxed{82.04}$$", "id": "2428587"}, {"introduction": "The condition number provides a powerful, but worst-case, upper bound on a problem's sensitivity. The actual error amplification depends on the alignment of the input data and its perturbations with the \"sensitive directions\" of the matrix, which are related to its singular vectors. This final practice [@problem_id:2428582] delves into this crucial nuance by examining a system that is extremely ill-conditioned, yet remains remarkably stable for perturbations along a specific direction, demonstrating that a high condition number flags a potential for instability but does not guarantee it for every possible input.", "problem": "Consider the linear system $A x = b$ with\n$$\nA = \\begin{pmatrix} 10^{6}  0 \\\\ 0  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nLet $\\|\\cdot\\|_{2}$ denote the Euclidean norm for vectors and the induced spectral norm for matrices. Let the spectral condition number be defined by $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$. For perturbations $\\delta b$ that are restricted to be parallel to $b$ (that is, $\\delta b = \\alpha b$ for some scalar $\\alpha$), define $C$ to be the smallest real number such that, for all nonzero $\\delta b$ parallel to $b$,\n$$\n\\frac{\\|\\delta x\\|_{2}}{\\|x\\|_{2}} \\le C \\,\\frac{\\|\\delta b\\|_{2}}{\\|b\\|_{2}},\n$$\nwhere $x$ solves $A x = b$ and $\\delta x$ solves $A (x + \\delta x) = b + \\delta b$. Compute the value\n$$\nR = \\frac{C}{\\kappa_{2}(A)}.\n$$\nProvide your final answer as an exact value (no rounding).", "solution": "The problem as stated is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We shall proceed with its formal derivation.\n\nThe problem investigates the sensitivity of the solution to the linear system $A x = b$ with respect to perturbations in the vector $b$. The given matrix and vector are:\n$$\nA = \\begin{pmatrix} 10^{6}  0 \\\\ 0  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe analysis is restricted to perturbations $\\delta b$ that are parallel to $b$, which means $\\delta b = \\alpha b$ for some scalar $\\alpha$.\n\nFirst, we solve for the exact solution $x$. Since $A$ is a diagonal matrix, its inverse is easily found:\n$$\nA^{-1} = \\begin{pmatrix} (10^{6})^{-1}  0 \\\\ 0  1^{-1} \\end{pmatrix} = \\begin{pmatrix} 10^{-6}  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe solution $x$ is given by $x = A^{-1} b$:\n$$\nx = \\begin{pmatrix} 10^{-6}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 10^{-6} \\\\ 0 \\end{pmatrix}\n$$\n\nNext, we determine the perturbation in the solution, $\\delta x$. The perturbed system is $A (x + \\delta x) = b + \\delta b$. Using the linearity of the matrix-vector product, we have $A x + A \\delta x = b + \\delta b$. Since $A x = b$, this simplifies to the equation for the perturbation:\n$$\nA \\delta x = \\delta b\n$$\nTherefore, $\\delta x = A^{-1} \\delta b$.\n\nThe problem specifies that the perturbation $\\delta b$ is parallel to $b$, so $\\delta b = \\alpha b$ for some nonzero scalar $\\alpha$. Substituting this into the expression for $\\delta x$:\n$$\n\\delta x = A^{-1} (\\alpha b) = \\alpha (A^{-1} b)\n$$\nSince we have already found that $x = A^{-1} b$, we can write:\n$$\n\\delta x = \\alpha x\n$$\n\nNow, we must find the constant $C$, which is defined as the smallest real number satisfying the inequality:\n$$\n\\frac{\\|\\delta x\\|_{2}}{\\|x\\|_{2}} \\le C \\,\\frac{\\|\\delta b\\|_{2}}{\\|b\\|_{2}}\n$$\nThis is equivalent to finding the supremum of the ratio of the relative errors over all allowed perturbations:\n$$\nC = \\sup_{\\delta b \\neq 0, \\delta b || b} \\left( \\frac{\\|\\delta x\\|_{2} / \\|x\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}} \\right)\n$$\nWe compute the norms involved. The norm used is the Euclidean norm, $\\|\\cdot\\|_{2}$.\n$$\n\\|x\\|_{2} = \\left\\| \\begin{pmatrix} 10^{-6} \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{(10^{-6})^{2} + 0^{2}} = 10^{-6}\n$$\n$$\n\\|\\delta x\\|_{2} = \\|\\alpha x\\|_{2} = |\\alpha| \\|x\\|_{2} = |\\alpha| \\times 10^{-6}\n$$\n$$\n\\|b\\|_{2} = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{1^{2} + 0^{2}} = 1\n$$\n$$\n\\|\\delta b\\|_{2} = \\|\\alpha b\\|_{2} = |\\alpha| \\|b\\|_{2} = |\\alpha| \\times 1 = |\\alpha|\n$$\nNow we compute the relative errors:\nThe relative error in the solution is $\\frac{\\|\\delta x\\|_{2}}{\\|x\\|_{2}} = \\frac{|\\alpha| \\times 10^{-6}}{10^{-6}} = |\\alpha|$.\nThe relative error in the right-hand side is $\\frac{\\|\\delta b\\|_{2}}{\\|b\\|_{2}} = \\frac{|\\alpha|}{1} = |\\alpha|$.\n\nSubstituting these into the expression for $C$:\n$$\nC = \\sup_{\\alpha \\neq 0} \\left( \\frac{|\\alpha|}{|\\alpha|} \\right) = \\sup_{\\alpha \\neq 0} (1) = 1\n$$\nThus, the specific condition number for perturbations in the direction of $b$ is $C = 1$.\n\nNext, we compute the standard spectral condition number $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$. For a symmetric (or diagonal) matrix, the spectral norm (induced $2$-norm) is the maximum absolute value of its eigenvalues.\nThe eigenvalues of $A$ are its diagonal entries, $\\lambda_{1} = 10^{6}$ and $\\lambda_{2} = 1$.\n$$\n\\|A\\|_{2} = \\max(|\\lambda_{1}|, |\\lambda_{2}|) = \\max(10^{6}, 1) = 10^{6}\n$$\nThe eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$, which are $10^{-6}$ and $1$.\n$$\n\\|A^{-1}\\|_{2} = \\max(|10^{-6}|, |1|) = 1\n$$\nThe spectral condition number is therefore:\n$$\n\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = (10^{6}) \\times (1) = 10^{6}\n$$\n\nFinally, we compute the required ratio $R$:\n$$\nR = \\frac{C}{\\kappa_{2}(A)} = \\frac{1}{10^{6}} = 10^{-6}\n$$\nThe result demonstrates that while the matrix $A$ is ill-conditioned in general (as shown by the large value of $\\kappa_{2}(A)$), the system is extremely well-conditioned with respect to perturbations in the specific direction of the eigenvector corresponding to the largest eigenvalue.", "answer": "$$\n\\boxed{1 \\times 10^{-6}}\n$$", "id": "2428582"}]}