{"hands_on_practices": [{"introduction": "In the world of finite-precision arithmetic, even a simple operation like summation can be fraught with peril. When adding a long sequence of numbers, especially those of differing magnitudes, small round-off errors can accumulate into a significant discrepancy. This practice [@problem_id:2389876] provides a dramatic demonstration of this phenomenon and allows you to test Kahan's compensated summation algorithm, a clever technique designed to track and correct for these otherwise lost bits of precision.", "problem": "You are asked to quantify how round-off errors propagate when summing an alternating sequence with a small constant bias in binary floating-point arithmetic, and to compare the error behavior of standard summation with Kahan compensated summation. Let the sequence be defined by\n$$\na_k = (-1)^{k+1} + s \\quad \\text{for} \\quad k=1,2,\\ldots,N,\n$$\nwhere $s>0$ is a real bias and $N$ is a positive integer. For each test case $(N,s)$ specified below, compute the following in IEEE $754$ binary $64$ arithmetic:\n- The standard left-to-right floating-point sum\n$$\nS_{\\text{std}}(N,s) = \\sum_{k=1}^{N} a_k.\n$$\n- The Kahan compensated floating-point sum\n$$\nS_{\\text{kah}}(N,s) = \\sum_{k=1}^{N} a_k,\n$$\nwhere the summation must be performed using the Kahan compensated summation method in binary $64$ arithmetic.\n\nLet the exact real sum be\n$$\nS_{\\text{exact}}(N,s) = \\sum_{k=1}^{N} \\left[(-1)^{k+1} + s\\right],\n$$\ninterpreted as a real number without floating-point rounding. For each test case, compute the absolute errors\n$$\nE_{\\text{std}} = \\left| S_{\\text{std}}(N,s) - S_{\\text{exact}}(N,s) \\right|, \n\\quad\nE_{\\text{kah}} = \\left| S_{\\text{kah}}(N,s) - S_{\\text{exact}}(N,s) \\right|.\n$$\n\nTest suite:\n- Case $1$: $(N,s)=(200000,\\,10^{-16})$.\n- Case $2$: $(N,s)=(200000,\\,2\\times 10^{-16})$.\n- Case $3$: $(N,s)=(200000,\\,10^{-12})$.\n- Case $4$: $(N,s)=(200001,\\,10^{-16})$.\n- Case $5$: $(N,s)=(2,\\,10^{-16})$.\n\nYour program must, for each of the $5$ cases, output the pair of floating-point numbers $(E_{\\text{std}}, E_{\\text{kah}})$, but aggregated into a single flat list in the following order:\n$$\n\\left[E_{\\text{std}}^{(1)},E_{\\text{kah}}^{(1)},E_{\\text{std}}^{(2)},E_{\\text{kah}}^{(2)},E_{\\text{std}}^{(3)},E_{\\text{kah}}^{(3)},E_{\\text{std}}^{(4)},E_{\\text{kah}}^{(4)},E_{\\text{std}}^{(5)},E_{\\text{kah}}^{(5)}\\right].\n$$\n\nFinal output format:\n- Produce exactly one line containing a single list enclosed in square brackets with comma-separated entries and no additional whitespace.\n- Each number must be printed in scientific notation with $12$ significant digits.\n- Example of the required style (illustrative only): $[1.234000000000e-03,4.560000000000e-07]$.\n\nNo physical units are involved. Angles are not used. Do not print any extra text before or after the list. The computations must be performed using binary $64$ floating-point arithmetic for $S_{\\text{std}}(N,s)$ and $S_{\\text{kah}}(N,s)$, and the exact real sum $S_{\\text{exact}}(N,s)$ must be used only as a reference for error calculation. The answer for each test case is a pair of floats $(E_{\\text{std}},E_{\\text{kah}})$ as defined above, and the final output aggregates all cases as specified.", "solution": "The problem statement is valid. It presents a well-defined numerical experiment in the field of computational physics, specifically concerning floating-point error analysis. The problem is scientifically grounded in the principles of numerical analysis, well-posed with clear objectives and constraints, and free of any factual or logical inconsistencies.\n\nThe problem requires a quantitative comparison between standard (naive) summation and Kahan compensated summation for an alternating sequence with a small positive bias, $a_k = (-1)^{k+1} + s$. The core numerical challenge arises from the properties of floating-point arithmetic, specifically the loss of significance when adding two numbers of vastly different magnitudes.\n\nFirst, let us establish the exact value of the sum, which will serve as our ground truth for error calculation. The sum is defined as:\n$$\nS_{\\text{exact}}(N,s) = \\sum_{k=1}^{N} \\left[(-1)^{k+1} + s\\right]\n$$\nThis can be separated into two parts:\n$$\nS_{\\text{exact}}(N,s) = \\left(\\sum_{k=1}^{N} (-1)^{k+1}\\right) + \\left(\\sum_{k=1}^{N} s\\right)\n$$\nThe second term is simply $N \\cdot s$. The first term is a sum of alternating $+1$ and $-1$.\nIf $N$ is an even integer, the sum is $(1-1) + (1-1) + \\ldots = 0$.\nIf $N$ is an odd integer, the sum is $(1-1) + \\ldots + (1-1) + 1 = 1$.\nThis can be expressed concisely as $N \\pmod 2$.\nTherefore, the exact sum, interpreted as a real number, is given by the analytical formula:\n$$\nS_{\\text{exact}}(N,s) = (N \\pmod 2) + Ns\n$$\nThis formula will be used as the reference value for computing the absolute errors $E_{\\text{std}}$ and $E_{\\text{kah}}$.\n\nNext, we analyze the behavior of the two summation methods in binary64 floating-point arithmetic.\n\nStandard Summation ($S_{\\text{std}}$):\nThe standard method computes the sum via a simple left-to-right loop: $S_k = S_{k-1} + a_k$. Let us trace the partial sums.\nThe terms of the sequence are $a_k \\approx 1$ for odd $k$ and $a_k \\approx -1$ for even $k$.\nThe partial sum for odd $k=2m-1$ is $S_{2m-1} = \\text{fl}(S_{2m-2} + a_{2m-1})$, where $\\text{fl}(\\cdot)$ denotes a floating-point operation. Here, $S_{2m-2}$ is the sum of $m-1$ pairs of terms $(a_{2j-1}+a_{2j})$, each pair summing to $2s$. Thus, $S_{2m-2} \\approx 2(m-1)s$, which is a small number for the given parameters. The term $a_{2m-1} \\approx 1$.\nThe addition is therefore $\\text{fl}(\\text{small\\_number} + \\text{large\\_number})$. In floating-point arithmetic, this operation loses most of the precision of the smaller number. The low-order bits of $S_{2m-2}$ are truncated.\nThe partial sum for even $k=2m$ is $S_{2m} = \\text{fl}(S_{2m-1} + a_{2m})$. Since $S_{2m-1} \\approx 1$ and $a_{2m} \\approx -1$, this operation is an example of catastrophic cancellation, where the subtraction of two nearly equal numbers results in a value with a large relative error.\nThe round-off error introduced at each step, particularly at the odd steps, accumulates. For a large number of terms $N$, this accumulated error is expected to become significant, causing $S_{\\text{std}}$ to deviate substantially from $S_{\\text{exact}}$.\n\nKahan Compensated Summation ($S_{\\text{kah}}$):\nThe Kahan summation algorithm is designed to mitigate this exact problem. It maintains a running compensation variable, $c$, which accumulates the round-off error from each addition. The algorithm for each step is:\n1. $y = a_k - c$\n2. $t = \\text{sum} + y$\n3. $c = (t - \\text{sum}) - y$\n4. $\\text{sum} = t$\n\nThe critical step is the calculation of the new compensation term, $c = (t - \\text{sum}) - y$. Algebraically, this should be zero. However, in floating-point arithmetic, `t - sum` recovers the high-order part of `y` that was successfully added to `sum`, and subtracting `y` from this isolates the negative of the low-order part of `y` that was lost in the addition `sum + y`. This lost part, $c$, is then subtracted from the *next* term $a_{k+1}$ before it is added to the running sum, effectively re-injecting the lost precision. This process prevents the systematic accumulation of round-off errors. Consequently, the error $E_{\\text{kah}}$ is expected to be very small, on the order of the machine precision of the final sum, and should not grow significantly with $N$.\n\nFor the special case where $N=2$, the Kahan algorithm does not provide any advantage. The compensation term $c$ is calculated after the second (and final) addition, but it is never used. Therefore, for $N=2$, $S_{\\text{std}}$ and $S_{\\text{kah}}$ will be identical, and thus $E_{\\text{std}} = E_{\\text{kah}}$.\n\nThe algorithm to be implemented is as follows:\nFor each test case $(N, s)$:\n1. Cast the input value $s$ to a binary64 floating-point number.\n2. Calculate the reference value $S_{\\text{exact}}$ using the analytical formula $S_{\\text{exact}} = (N \\pmod 2) + Ns$, performing the arithmetic using binary64 numbers.\n3. Implement a loop from $k=1$ to $N$ to compute $S_{\\text{std}}$. In each iteration, calculate $a_k = (-1)^{k+1} + s$ and add it to the running sum.\n4. Implement a second loop from $k=1$ to $N$ to compute $S_{\\text{kah}}$ using the Kahan summation algorithm as described above. All variables (`sum`, `c`, `y`, `t`) must be binary64.\n5. Compute the absolute errors $E_{\\text{std}} = |S_{\\text{std}} - S_{\\text{exact}}|$ and $E_{\\text{kah}} = |S_{\\text{kah}} - S_{\\text{exact}}|$.\n6. Collect the resulting pair of errors.\nAfter processing all test cases, the collected errors will be formatted into a single list as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the error propagation of standard vs. Kahan summation\n    for an alternating sequence with a small bias.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200000, 1e-16),\n        (200000, 2e-16),\n        (200000, 1e-12),\n        (200001, 1e-16),\n        (2, 1e-16),\n    ]\n\n    results = []\n    for N, s_val in test_cases:\n        # Use numpy.float64 to ensure computations are in IEEE 754 binary64.\n        s = np.float64(s_val)\n\n        # 1. Calculate the exact sum analytically.\n        # S_exact(N,s) = (N mod 2) + N*s\n        # The calculation is done in float64 to establish a consistent reference.\n        s_exact = np.float64(N % 2) + np.float64(N) * s\n\n        # 2. Compute the standard left-to-right floating-point sum.\n        s_std = np.float64(0.0)\n        for k in range(1, N + 1):\n            # Sequence term a_k = (-1)**(k+1) + s\n            # (-1)**(k+1) is 1 if k is odd, -1 if k is even.\n            term_sign = np.float64(1.0 if (k % 2 != 0) else -1.0)\n            a_k = term_sign + s\n            s_std += a_k\n\n        # 3. Compute the Kahan compensated floating-point sum.\n        s_kah = np.float64(0.0)\n        c = np.float64(0.0)  # A running compensation for lost low-order bits.\n        for k in range(1, N + 1):\n            term_sign = np.float64(1.0 if (k % 2 != 0) else -1.0)\n            a_k = term_sign + s\n            y = a_k - c\n            t = s_kah + y\n            # (t - s_kah) recovers the high-order part of y.\n            # Subtracting y recovers -(low part of y), which is the round-off error.\n            c = (t - s_kah) - y\n            s_kah = t\n\n        # 4. Compute the absolute errors.\n        e_std = np.abs(s_std - s_exact)\n        e_kah = np.abs(s_kah - s_exact)\n        \n        results.append(e_std)\n        results.append(e_kah)\n\n    # Final print statement in the exact required format.\n    # The format \"%.12e\" gives 1 digit before the decimal and 12 after,\n    # totaling 13 significant digits, as shown in the problem's example format style.\n    formatted_results = [f\"{r:.12e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2389876"}, {"introduction": "This exercise explores catastrophic cancellation, a rapid and devastating loss of significant figures that occurs when subtracting two nearly identical numbers. You will investigate a function that appears well-behaved but produces highly inaccurate results for small inputs when evaluated directly. Through this practice [@problem_id:2389878], you will see how a simple algebraic reformulation, guided by mathematical insight into the function's behavior, can restore numerical stability and produce accurate results.", "problem": "You are tasked with assessing numerical stability and error propagation when evaluating the function $f(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$ for values of $x$ near $0$ in radians. Your program must implement two different numerical evaluations of $f(x)$ for each test value of $x$, and quantify the error of each evaluation relative to a high-accuracy reference constructed from a truncated series expansion.\n\nDefinitions and requirements:\n\n- Work in radians.\n- Define the following three evaluations for any given nonzero $x$:\n  1. The direct evaluation $f_{\\mathrm{dir}}(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$.\n  2. A numerically stable evaluation obtained by the trigonometric rearrangement\n     $$f_{\\mathrm{stab}}(x) = \\dfrac{2 \\sin(x) \\sin^2\\!\\left(\\dfrac{x}{2}\\right)}{x^3 \\cos(x)},$$\n     which avoids subtractive cancellation by using $1 - \\cos(x) = 2\\sin^2\\!\\left(\\dfrac{x}{2}\\right)$.\n  3. A reference value constructed from the Maclaurin series of $f(x)$ truncated at order $x^6$:\n     $$f_{\\mathrm{ref}}(x) = \\dfrac{1}{2} + \\dfrac{x^2}{8} + \\dfrac{91}{1680} x^4 + \\dfrac{529}{24192} x^6.$$\n     This reference is valid for $|x| \\leq 10^{-1}$ with a remainder that is $\\mathcal{O}(x^8)$.\n\n- For each $x$, compute the absolute relative errors\n  $$E_{\\mathrm{dir}}(x) = \\dfrac{\\left| f_{\\mathrm{dir}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|}, \\quad E_{\\mathrm{stab}}(x) = \\dfrac{\\left| f_{\\mathrm{stab}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|}.$$\n\nTest suite:\n\nEvaluate and report the errors for the following inputs (all in radians): $x \\in \\{10^{-1}, 10^{-4}, 10^{-8}, -10^{-8}, 10^{-12}, 10^{-16}\\}$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each test value of $x$, first $E_{\\mathrm{dir}}(x)$ and then $E_{\\mathrm{stab}}(x)$. For the test values given above in the stated order, the output must therefore be\n$$[E_{\\mathrm{dir}}(10^{-1}), E_{\\mathrm{stab}}(10^{-1}), E_{\\mathrm{dir}}(10^{-4}), E_{\\mathrm{stab}}(10^{-4}), E_{\\mathrm{dir}}(10^{-8}), E_{\\mathrm{stab}}(10^{-8}), E_{\\mathrm{dir}}(-10^{-8}), E_{\\mathrm{stab}}(-10^{-8}), E_{\\mathrm{dir}}(10^{-12}), E_{\\mathrm{stab}}(10^{-12}), E_{\\mathrm{dir}}(10^{-16}), E_{\\mathrm{stab}}(10^{-16})].$$\nAll angles must be interpreted in radians. All reported quantities must be decimal floating-point numbers.", "solution": "The problem as stated is valid. It is a well-posed problem in computational physics that is scientifically grounded, internally consistent, and free of ambiguity. It addresses the fundamental numerical concept of catastrophic cancellation and its mitigation.\n\nThe core task is to evaluate the function $f(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$ for values of $x$ near $0$. This evaluation is performed using three distinct methods to demonstrate and quantify numerical error.\n\nThe first method is the direct evaluation, $f_{\\mathrm{dir}}(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$. This formulation is susceptible to catastrophic cancellation. For $x \\approx 0$, we have $\\tan(x) \\approx x$ and $\\sin(x) \\approx x$. In finite-precision floating-point arithmetic, the computation of $\\tan(x) - \\sin(x)$ involves the subtraction of two very close numbers. This operation causes the leading, significant digits to cancel, leaving a result dominated by representation errors. This loss of precision is then amplified by the division by a very small number, $x^3$.\n\nTo understand the behavior of $f(x)$ for small $x$, we examine the Maclaurin series expansions of the numerator's components:\n$$ \\tan(x) = x + \\frac{x^3}{3} + \\frac{2x^5}{15} + \\mathcal{O}(x^7) $$\n$$ \\sin(x) = x - \\frac{x^3}{6} + \\frac{x^5}{120} + \\mathcal{O}(x^7) $$\nSubtracting these series reveals the true behavior of the numerator:\n$$ \\tan(x) - \\sin(x) = \\left( \\frac{1}{3} - (-\\frac{1}{6}) \\right) x^3 + \\left( \\frac{2}{15} - \\frac{1}{120} \\right) x^5 + \\mathcal{O}(x^7) = \\frac{1}{2}x^3 + \\frac{1}{8}x^5 + \\mathcal{O}(x^7) $$\nThus, the function $f(x)$ approaches a finite limit as $x \\to 0$:\n$$ \\lim_{x \\to 0} f(x) = \\lim_{x \\to 0} \\frac{\\frac{1}{2}x^3 + \\frac{1}{8}x^5 + \\mathcal{O}(x^7)}{x^3} = \\frac{1}{2} $$\nThe direct formula attempts to compute a small leading term of order $\\mathcal{O}(x^3)$ by subtracting two much larger terms of order $\\mathcal{O}(x)$, which is the recipe for numerical disaster.\n\nThe second method, $f_{\\mathrm{stab}}(x)$, employs a trigonometric identity to reformulate the expression and avoid this cancellation. The derivation is as follows:\n$$ f(x) = \\frac{\\frac{\\sin(x)}{\\cos(x)} - \\sin(x)}{x^3} = \\frac{\\sin(x) \\left( \\frac{1}{\\cos(x)} - 1 \\right)}{x^3} = \\frac{\\sin(x) (1 - \\cos(x))}{x^3 \\cos(x)} $$\nThe term $1 - \\cos(x)$ is still a problematic subtraction for small $x$. However, it can be replaced using the half-angle identity $1 - \\cos(x) = 2 \\sin^2\\left(\\frac{x}{2}\\right)$. This yields the numerically stable form:\n$$ f_{\\mathrm{stab}}(x) = \\frac{2 \\sin(x) \\sin^2\\left(\\frac{x}{2}\\right)}{x^3 \\cos(x)} $$\nThis expression contains no subtractions of nearly equal quantities and is expected to be numerically stable.\n\nThe third method provides a high-accuracy reference value, $f_{\\mathrm{ref}}(x)$, using a truncated Maclaurin series of $f(x)$ itself. From the expansion of $\\tan(x) - \\sin(x)$, we derive the series for $f(x)$:\n$$ f(x) = \\frac{\\frac{1}{2}x^3 + \\frac{1}{8}x^5 + \\frac{13}{240}x^7 + \\dots}{x^3} = \\frac{1}{2} + \\frac{1}{8}x^2 + \\frac{13}{240}x^4 + \\dots $$\nThe problem provides a more accurate truncation:\n$$ f_{\\mathrm{ref}}(x) = \\frac{1}{2} + \\frac{x^2}{8} + \\frac{91}{1680} x^4 + \\frac{529}{24192} x^6 $$\nThis polynomial evaluation is inherently stable for small $x$ because it involves only additions of positive terms (since $x$ is squared) and multiplications. This makes it an ideal benchmark.\n\nThe numerical implementation will consist of three functions corresponding to $f_{\\mathrm{dir}}$, $f_{\\mathrm{stab}}$, and $f_{\\mathrm{ref}}$. We will then iterate through the test suite of $x$ values: $\\{10^{-1}, 10^{-4}, 10^{-8}, -10^{-8}, 10^{-12}, 10^{-16}\\}$. For each $x$, we compute the absolute relative errors for the direct and stable methods against the reference:\n$$ E_{\\mathrm{dir}}(x) = \\frac{\\left| f_{\\mathrm{dir}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|}, \\quad E_{\\mathrm{stab}}(x) = \\frac{\\left| f_{\\mathrm{stab}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|} $$\nSince $f_{\\mathrm{ref}}(x)$ is a sum of a positive constant ($1/2$) and non-negative terms, its value is always greater than or equal to $1/2$, so there is no risk of division by zero in the error calculation. The function $f(x)$ is an even function, meaning $f(x) = f(-x)$, so the results for $x=10^{-8}$ and $x=-10^{-8}$ are expected to be identical.\n\nThe implementation will use Horner's method for the polynomial evaluation of $f_{\\mathrm{ref}}(x)$ to optimize performance and maintain numerical accuracy. The final output will be a list of the computed errors, $E_{\\mathrm{dir}}(x)$ and $E_{\\mathrm{stab}}(x)$, for each test case in the specified order. We anticipate that $E_{\\mathrm{dir}}(x)$ will increase dramatically as $|x| \\to 0$, while $E_{\\mathrm{stab}}(x)$ will remain small, confirming the superior stability of the reformulated expression.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_dir(x: float) -> float:\n    \"\"\"\n    Direct evaluation of f(x) = (tan(x) - sin(x)) / x**3.\n    This form is prone to catastrophic cancellation for x near 0.\n    \"\"\"\n    if x == 0.0:\n        return 0.5  # The limit as x -> 0\n    return (np.tan(x) - np.sin(x)) / (x**3)\n\ndef f_stab(x: float) -> float:\n    \"\"\"\n    Numerically stable evaluation of f(x) using trigonometric rearrangement.\n    f_stab(x) = (2 * sin(x) * sin(x/2)**2) / (x**3 * cos(x)).\n    \"\"\"\n    if x == 0.0:\n        return 0.5  # The limit as x -> 0\n    \n    # Pre-calculate common terms\n    sin_x = np.sin(x)\n    x_half = x / 2.0\n    sin_x_half = np.sin(x_half)\n    cos_x = np.cos(x)\n    x_cubed = x**3\n    \n    # Avoid division by zero if cos(x) is zero, though not for test cases\n    if cos_x == 0.0:\n        return np.inf * np.sign(sin_x)\n        \n    numerator = 2.0 * sin_x * sin_x_half**2\n    denominator = x_cubed * cos_x\n    \n    return numerator / denominator\n\ndef f_ref(x: float) -> float:\n    \"\"\"\n    Reference evaluation of f(x) using its Maclaurin series expansion\n    truncated at the O(x^6) term.\n    f_ref(x) = 1/2 + x^2/8 + (91/1680)x^4 + (529/24192)x^6.\n    \"\"\"\n    if x == 0.0:\n        return 0.5\n        \n    # Use Horner's method for efficient and stable polynomial evaluation\n    y = x**2\n    c0 = 1.0 / 2.0\n    c1 = 1.0 / 8.0\n    c2 = 91.0 / 1680.0\n    c3 = 529.0 / 24192.0\n\n    return c0 + y * (c1 + y * (c2 + y * c3))\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's requirements.\n    It calculates and reports the relative errors for two numerical methods\n    against a reference value for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1e-1,\n        1e-4,\n        1e-8,\n        -1e-8,\n        1e-12,\n        1e-16\n    ]\n\n    results = []\n    for x in test_cases:\n        # Calculate the function value using all three methods\n        val_dir = f_dir(x)\n        val_stab = f_stab(x)\n        val_ref = f_ref(x)\n\n        # Calculate the absolute relative errors\n        # Note: abs(val_ref) is guaranteed to be >= 0.5, so no division by zero\n        error_dir = np.abs(val_dir - val_ref) / np.abs(val_ref)\n        error_stab = np.abs(val_stab - val_ref) / np.abs(val_ref)\n        \n        results.append(error_dir)\n        results.append(error_stab)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2389878"}, {"introduction": "We now scale up our analysis to a foundational algorithm in computational engineering: solving systems of linear equations using Gaussian elimination. The stability of such a multi-step process is paramount, as a single unstable step can corrupt the entire result. This hands-on problem [@problem_id:2375807] starkly contrasts a naive implementation with a robust one, demonstrating why the algorithmic strategy of partial pivoting is not an optional refinement but an essential component for reliable numerical computation.", "problem": "Write a complete, runnable program that, for a fixed set of linear systems, quantitatively compares the effect of roundoff error and catastrophic cancellation in Gaussian elimination without row interchanges versus Gaussian elimination with partial pivoting. All computations of the elimination and back substitution steps must be carried out in single-precision binary floating-point arithmetic (that is, using $32$-bit floating point), while any reference values you construct may be computed in double precision.\n\nDefine the relative forward error for an approximate solution $\\hat{\\mathbf{x}}$ to the linear system $A \\mathbf{x} = \\mathbf{b}$ with known exact solution $\\mathbf{x}_{\\mathrm{true}}$ as\n$$\nE(\\hat{\\mathbf{x}}) = \\frac{\\lVert \\hat{\\mathbf{x}} - \\mathbf{x}_{\\mathrm{true}} \\rVert_2}{\\lVert \\mathbf{x}_{\\mathrm{true}} \\rVert_2}.\n$$\nIf the method cannot proceed due to an exact zero pivot in the no-interchange variant, define the corresponding relative error to be $+\\infty$ and print it as the string \"inf\".\n\nYour program must, for each test case below, compute:\n- $\\hat{\\mathbf{x}}_{\\mathrm{np}}$: the solution obtained by Gaussian elimination without any row interchanges,\n- $\\hat{\\mathbf{x}}_{\\mathrm{pp}}$: the solution obtained by Gaussian elimination with partial pivoting (that is, at each elimination step, swap the current row with a lower row that has the largest absolute pivot in the current column),\n\nboth performed in $32$-bit floating point arithmetic. Then compute $E(\\hat{\\mathbf{x}}_{\\mathrm{np}})$ and $E(\\hat{\\mathbf{x}}_{\\mathrm{pp}})$ using the definition above.\n\nUse the following test suite. In every case, construct $\\mathbf{b}$ from $A$ and the specified $\\mathbf{x}_{\\mathrm{true}}$ as $\\mathbf{b} = A \\mathbf{x}_{\\mathrm{true}}$ in double precision, and then solve the system in single precision as required.\n\n- Test case $1$ (well-behaved $2 \\times 2$ system):\n  $$\n  A_1 = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad \\mathbf{x}_{\\mathrm{true},1} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}.\n  $$\n- Test case $2$ (catastrophic cancellation trigger without pivoting): Let $\\varepsilon = 10^{-20}$,\n  $$\n  A_2 = \\begin{bmatrix} \\varepsilon & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\mathbf{x}_{\\mathrm{true},2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n  $$\n- Test case $3$ (exact zero pivot without interchanges):\n  $$\n  A_3 = \\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}, \\quad \\mathbf{x}_{\\mathrm{true},3} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.\n  $$\n- Test case $4$ (ill-conditioned Hilbert system of size $5$): For indices $i,j \\in \\{1,2,3,4,5\\}$,\n  $$\n  \\left(A_4\\right)_{ij} = \\frac{1}{i + j - 1}, \\quad \\mathbf{x}_{\\mathrm{true},4} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.\n  $$\n\nAngle units do not apply. Physical units do not apply.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case $k \\in \\{1,2,3,4\\}$, report first $E(\\hat{\\mathbf{x}}_{\\mathrm{np}})$ and then $E(\\hat{\\mathbf{x}}_{\\mathrm{pp}})$, in that order. Thus the output must contain $8$ entries in total, ordered as\n$$\n\\left[E(\\hat{\\mathbf{x}}_{\\mathrm{np},1}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{pp},1}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{np},2}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{pp},2}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{np},3}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{pp},3}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{np},4}),\\,E(\\hat{\\mathbf{x}}_{\\mathrm{pp},4})\\right].\n$$\nEach finite float must be rounded to base-$10$ scientific notation with exactly $6$ digits after the decimal point (for example, $1.234000\\mathrm{e}{-02}$), and $+\\infty$ must be printed as \"inf\". Your program must not read any input and must not produce any additional text.", "solution": "The problem requires a quantitative comparison of Gaussian elimination with and without partial pivoting, focusing on the impact of catastrophic cancellation and roundoff error. The analysis will be performed on a defined suite of linear systems, $A \\mathbf{x} = \\mathbf{b}$, where the solution process is constrained to single-precision ($32$-bit) floating-point arithmetic. The quality of an approximate solution, $\\hat{\\mathbf{x}}$, is measured by its relative forward error, $E(\\hat{\\mathbf{x}})$, with respect to a known true solution, $\\mathbf{x}_{\\mathrm{true}}$.\n\nThe fundamental algorithm for solving a linear system $A \\mathbf{x} = \\mathbf{b}$ of size $n \\times n$ is Gaussian elimination. It consists of two stages: forward elimination and backward substitution. The forward elimination stage transforms the matrix $A$ into an upper triangular matrix $U$ by applying a sequence of elementary row operations. These same operations are applied to the vector $\\mathbf{b}$ to produce a modified vector $\\mathbf{c}$. The system is thereby transformed into an equivalent system $U \\mathbf{x} = \\mathbf{c}$, which is easily solved by backward substitution.\n\nThe forward elimination process for a column $k$ (from $0$ to $n-2$) involves using the pivot element $A_{kk}$ to eliminate the coefficients below it in the same column. For each row $i$ below row $k$ (i.e., $i > k$), the row is updated by subtracting a multiple of row $k$:\n$$\n\\text{Row}_i \\leftarrow \\text{Row}_i - m_{ik} \\cdot \\text{Row}_k\n$$\nwhere the multiplier is $m_{ik} = A_{ik} / A_{kk}$. This process is numerically stable only if the multipliers $m_{ik}$ are not excessively large. If the pivot element $A_{kk}$ is zero or very close to zero relative to other elements in its column, the multiplier $m_{ik}$ can become very large. This leads to catastrophic cancellation: when we compute $A_{ij} - m_{ik} A_{kj}$, if $m_{ik} A_{kj}$ is very large and of similar magnitude to $A_{ij}$, the subtraction can result in a loss of many significant digits. This introduces substantial roundoff error that propagates through subsequent calculations, ultimately corrupting the final solution $\\hat{\\mathbf{x}}$.\n\nThe two required algorithms are as follows:\n\n1.  **Gaussian Elimination without Pivoting**: This is the naive implementation described above. It proceeds without reordering the equations. Its primary vulnerability is its susceptibility to small or zero pivot elements. If an exact zero pivot $A_{kk}=0$ is encountered, the algorithm fails as the multiplier $m_{ik}$ is undefined. As per the problem specification, this failure results in an infinite error, denoted \"inf\".\n\n2.  **Gaussian Elimination with Partial Pivoting**: This is a refinement designed to enhance numerical stability. Before the elimination step for column $k$, the algorithm searches for the element with the largest absolute value in the sub-column $\\{A_{ik} | i \\ge k\\}$. Let this element be $A_{pk}$. The algorithm then swaps row $p$ with row $k$. This ensures that the pivot element $A_{kk}$ is the largest possible in its column (from the diagonal downwards), thereby ensuring that all multipliers $|m_{ik}| = |A_{ik}/A_{kk}|$ are less than or equal to $1$. This strategy prevents the multipliers from becoming large and significantly mitigates the risk of catastrophic cancellation.\n\nThe evaluation metric is the relative forward error, defined as:\n$$\nE(\\hat{\\mathbf{x}}) = \\frac{\\lVert \\hat{\\mathbf{x}} - \\mathbf{x}_{\\mathrm{true}} \\rVert_2}{\\lVert \\mathbf{x}_{\\mathrm{true}} \\rVert_2}\n$$\nwhere $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. This metric quantifies the deviation of the computed solution from the true solution, relative to the magnitude of the true solution.\n\nThe test suite is designed to expose the specific weaknesses of the no-pivoting approach:\n- **Test Case $1$**: A well-behaved $2 \\times 2$ system. Both methods are expected to produce highly accurate results, as the initial pivot is already the largest in its column.\n- **Test Case $2$**: This case features a matrix $A_2$ with a very small pivot element, $A_{11} = \\varepsilon = 10^{-20}$. While not exactly zero, this value is small enough that in single-precision arithmetic, its use as a pivot will cause a very large multiplier, $1/\\varepsilon$, leading to catastrophic cancellation in the update of the second row. The no-pivoting solution $\\hat{\\mathbf{x}}_{\\mathrm{np}}$ is expected to be highly inaccurate. Partial pivoting will swap the rows, using $1$ as the pivot, leading to a stable computation and an accurate solution $\\hat{\\mathbf{x}}_{\\mathrm{pp}}$.\n- **Test Case $3$**: This case presents an exact zero pivot, $A_{11} = 0$. The no-pivoting algorithm must fail, yielding an infinite error. Partial pivoting will swap row $1$ with either row $2$ or $3$, which have non-zero elements in the first column, allowing the algorithm to proceed and find a solution.\n- **Test Case $4$**: This involves the $5 \\times 5$ Hilbert matrix, which is famously ill-conditioned. Ill-conditioning means that small changes in $A$ or $\\mathbf{b}$ can lead to large changes in the solution $\\mathbf{x}$. While pivoting improves stability by controlling multiplier growth, it cannot eliminate the inherent sensitivity of an ill-conditioned system. Therefore, both algorithms are expected to produce solutions with significant error, but partial pivoting should still yield a more accurate result than the no-pivoting approach.\n\nThe implementation will consist of two primary functions, one for each variant of Gaussian elimination. These functions will perform all their internal arithmetic using $32$-bit floating-point numbers (`numpy.float32`). The main procedure will construct the test cases, calculate the right-hand side vector $\\mathbf{b} = A \\mathbf{x}_{\\mathrm{true}}$ in double precision (`float64`) for high-fidelity reference, call the single-precision solvers, and then compute the relative forward errors to generate the final report.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_no_pivot(A: np.ndarray, b: np.ndarray) -> np.ndarray | None:\n    \"\"\"\n    Solves Ax=b using Gaussian elimination without pivoting in single precision.\n    Returns the solution vector x, or None if a zero pivot is encountered.\n    \"\"\"\n    n = A.shape[0]\n    # Create an augmented matrix and cast to float32 for all computations\n    Aug = np.hstack([A, b.reshape(-1, 1)]).astype(np.float32)\n\n    # Forward elimination\n    for k in range(n - 1):\n        pivot = Aug[k, k]\n        if pivot == np.float32(0.0):\n            return None  # Failure due to exact zero pivot\n\n        for i in range(k + 1, n):\n            multiplier = Aug[i, k] / pivot\n            Aug[i, k:] -= multiplier * Aug[k, k:]\n\n    # Backward substitution\n    x_hat = np.zeros(n, dtype=np.float32)\n    for i in range(n - 1, -1, -1):\n        # Check for singularity detected after elimination\n        if Aug[i, i] == np.float32(0.0):\n            return None\n        \n        s = np.dot(Aug[i, i + 1:n], x_hat[i + 1:])\n        x_hat[i] = (Aug[i, n] - s) / Aug[i, i]\n\n    return x_hat\n\ndef solve_partial_pivot(A: np.ndarray, b: np.ndarray) -> np.ndarray | None:\n    \"\"\"\n    Solves Ax=b using Gaussian elimination with partial pivoting in single precision.\n    Returns the solution vector x, or None if the matrix is singular.\n    \"\"\"\n    n = A.shape[0]\n    # Create an augmented matrix and cast to float32 for all computations\n    Aug = np.hstack([A, b.reshape(-1, 1)]).astype(np.float32)\n\n    # Forward elimination with pivoting\n    for k in range(n - 1):\n        # Find the row with the largest pivot in the current column\n        max_row_idx = k + np.argmax(np.abs(Aug[k:, k]))\n\n        # Swap rows if necessary\n        if max_row_idx != k:\n            Aug[[k, max_row_idx]] = Aug[[max_row_idx, k]]\n\n        pivot = Aug[k, k]\n        # If the pivot is zero after swapping, the matrix is singular\n        if pivot == np.float32(0.0):\n            return None\n\n        for i in range(k + 1, n):\n            multiplier = Aug[i, k] / pivot\n            Aug[i, k:] -= multiplier * Aug[k, k:]\n\n    # Backward substitution\n    x_hat = np.zeros(n, dtype=np.float32)\n    for i in range(n - 1, -1, -1):\n        if Aug[i, i] == np.float32(0.0):\n            return None # Singular matrix\n        \n        s = np.dot(Aug[i, i + 1:n], x_hat[i + 1:])\n        x_hat[i] = (Aug[i, n] - s) / Aug[i, i]\n\n    return x_hat\n\ndef relative_forward_error(x_hat: np.ndarray, x_true: np.ndarray) -> float:\n    \"\"\"\n    Computes the relative forward error using double precision for accuracy.\n    \"\"\"\n    # Use float64 for higher precision error calculation\n    x_hat_64 = x_hat.astype(np.float64)\n    x_true_64 = x_true.astype(np.float64)\n    \n    norm_diff = np.linalg.norm(x_hat_64 - x_true_64, 2)\n    norm_true = np.linalg.norm(x_true_64, 2)\n    \n    if norm_true == 0:\n        return 0.0 if norm_diff == 0.0 else np.inf\n    \n    return norm_diff / norm_true\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # All matrices and vectors are initially defined in float64.\n    \n    # Test case 1\n    A1 = np.array([[2, 1], [1, 3]], dtype=np.float64)\n    x_true1 = np.array([1, -1], dtype=np.float64)\n\n    # Test case 2\n    eps = 1e-20\n    A2 = np.array([[eps, 1], [1, 1]], dtype=np.float64)\n    x_true2 = np.array([1, 1], dtype=np.float64)\n\n    # Test case 3\n    A3 = np.array([[0, 1, 1], [1, 1, 1], [1, 1, 2]], dtype=np.float64)\n    x_true3 = np.array([1, 2, 3], dtype=np.float64)\n\n    # Test case 4 (Hilbert matrix)\n    n4 = 5\n    A4 = np.zeros((n4, n4), dtype=np.float64)\n    for i in range(1, n4 + 1):\n        for j in range(1, n4 + 1):\n            A4[i-1, j-1] = 1.0 / (i + j - 1)\n    x_true4 = np.ones(n4, dtype=np.float64)\n\n    test_cases = [\n        (A1, x_true1),\n        (A2, x_true2),\n        (A3, x_true3),\n        (A4, x_true4),\n    ]\n\n    results = []\n    for A, x_true in test_cases:\n        # Construct b = A @ x_true in double precision\n        b = A @ x_true\n        \n        # --- No Pivoting ---\n        x_np = solve_no_pivot(A.copy(), b.copy())\n        if x_np is None:\n            err_np_str = \"inf\"\n        else:\n            err_np = relative_forward_error(x_np, x_true)\n            err_np_str = f\"{err_np:.6e}\"\n        results.append(err_np_str)\n        \n        # --- Partial Pivoting ---\n        x_pp = solve_partial_pivot(A.copy(), b.copy())\n        if x_pp is None:\n            # This case should not be reached with the given test problems\n            # but is included for robustness.\n            err_pp_str = \"inf\" \n        else:\n            err_pp = relative_forward_error(x_pp, x_true)\n            err_pp_str = f\"{err_pp:.6e}\"\n        results.append(err_pp_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2375807"}]}