{"hands_on_practices": [{"introduction": "Theoretical knowledge of numerical errors gains practical meaning when you see it in action. Our first exercise tackles a classic scenario of catastrophic cancellation, which occurs when subtracting two nearly equal floating-point numbers. This practice challenges you to reformulate a seemingly simple function that fails near a specific diagonal line, using a powerful algebraic technique—multiplying by the conjugate—to create a universally stable expression [@problem_id:2420071]. This is a fundamental skill for writing robust numerical code.", "problem": "You are asked to design and implement a numerically stable evaluation strategy for a function that exhibits catastrophic cancellation near a thin, diagonal line in the plane. Consider the scalar function defined for real inputs $x$ and $y$ by first forming $t = x - y$, and then the naive expression\n$$\nf_{\\text{naive}}(x,y) \\equiv \\frac{\\sqrt{1 + t} - 1}{t},\n$$\nwhich is only intended where $1 + t \\ge 0$. Along the diagonal line $y = x$, the naive expression becomes $0/0$ and yields a non-number (Not a Number), while for $t$ very close to $0$ it suffers from catastrophic cancellation in the numerator.\n\nYour task is to:\n- From the fact that finite precision arithmetic follows the rounding model of the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) and the definition of catastrophic cancellation in subtractive cancellation of nearly equal floating-point numbers, derive a numerically stable evaluation of $f(x,y)$ that:\n  1. Agrees with the mathematically correct value whenever $1 + (x - y) \\ge 0$.\n  2. Returns the correct limit on the diagonal $y = x$, that is, at $t = 0$.\n  3. Avoids generating Not a Number for any $(x,y)$ satisfying $1 + (x - y) \\ge 0$.\n- Implement this stable strategy in code for the given test suite and produce the stabilized function values as your program’s sole output.\n\nUse the following test suite of ordered pairs $(x,y)$, which is designed to exercise the diagonal $y=x$ exactly, approach it from both sides, test behavior near the boundary $t = -1$, and cover small and large magnitudes:\n- $(1, 1)$\n- $(1 + 10^{-16}, 1)$\n- $(1 - 10^{-16}, 1)$\n- $(0, 0)$\n- $(10^{-12}, 0)$\n- $(0, 10^{-12})$\n- $(2, 1)$\n- $(0, 1)$\n- $(10^{8}, 0)$\n- $(0, 0.999999999)$\n\nYour program must compute the stabilized value of $f(x,y)$ for each of the above inputs, in the given order, under the domain condition $1 + (x - y) \\ge 0$. All angles, if any appear in your derivation, must be in radians, but no angles are needed here. No physical units are involved. The final output format must be a single line containing a comma-separated list of the $10$ stabilized results enclosed in square brackets. Each number must be printed as a floating-point value with $15$ significant digits, for example,\n[...]\nand no additional text. If any test point were outside the domain $1 + (x - y) \\ge 0$, you would emit a Not a Number there, but all listed tests satisfy the domain condition, so your outputs must all be valid finite numbers.", "solution": "The problem requires the derivation and implementation of a numerically stable algorithm for the function\n$$\nf_{\\text{naive}}(x,y) = \\frac{\\sqrt{1 + t} - 1}{t}\n$$\nwhere $t = x - y$, and the domain of definition is restricted to $1 + t \\ge 0$.\n\nThe analysis begins with the identification of the source of numerical instability. The problem states that the naive expression suffers from catastrophic cancellation when $t$ is very close to $0$. This occurs because for $t \\approx 0$, the term $\\sqrt{1 + t}$ is very close to $1$. According to the principles of floating-point arithmetic (IEEE 754 standard), the subtraction of two nearly equal numbers, $\\sqrt{1+t}$ and $1$, results in a loss of significant digits. The resulting value has a large relative error, which is then magnified by division by the small number $t$.\n\nTo formulate a stable alternative, we must find a mathematically equivalent expression that avoids this subtractive cancellation. A standard and effective technique for expressions involving square roots is to multiply by the conjugate. The conjugate of the numerator, $\\sqrt{1 + t} - 1$, is $\\sqrt{1 + t} + 1$. We multiply the numerator and the denominator by this conjugate, which does not change the value of the expression for $t \\neq 0$:\n$$\nf(t) = \\frac{\\sqrt{1 + t} - 1}{t} \\times \\frac{\\sqrt{1 + t} + 1}{\\sqrt{1 + t} + 1}\n$$\nApplying the difference of squares formula, $(a-b)(a+b) = a^2 - b^2$, to the numerator yields:\n$$\nf(t) = \\frac{(\\sqrt{1 + t})^2 - 1^2}{t(\\sqrt{1 + t} + 1)} = \\frac{(1 + t) - 1}{t(\\sqrt{1 + t} + 1)} = \\frac{t}{t(\\sqrt{1 + t} + 1)}\n$$\nFor any $t \\neq 0$, we can cancel the factor of $t$ from the numerator and denominator, leading to the stabilized expression:\n$$\nf_{\\text{stable}}(t) = \\frac{1}{\\sqrt{1 + t} + 1}\n$$\nThis expression, $f_{\\text{stable}}(t)$, is numerically robust. For $t \\approx 0$, the denominator involves the addition of two positive numbers, $\\sqrt{1 + t}$ and $1$, which is a numerically stable operation. There is no subtraction of nearly equal quantities. This form is therefore suitable for computation for all $t$ in the domain, especially for $t$ near $0$.\n\nThe problem also requires that the function return the correct limiting value on the diagonal $y = x$, which corresponds to $t = 0$. At $t=0$, the naive expression is the indeterminate form $0/0$. We can find the limit using L'Hôpital's rule:\n$$\n\\lim_{t \\to 0} f_{\\text{naive}}(t) = \\lim_{t \\to 0} \\frac{\\frac{d}{dt}(\\sqrt{1 + t} - 1)}{\\frac{d}{dt}(t)} = \\lim_{t \\to 0} \\frac{\\frac{1}{2\\sqrt{1 + t}}}{1} = \\frac{1}{2\\sqrt{1 + 0}} = \\frac{1}{2}\n$$\nEvaluating our stable expression directly at $t=0$ gives:\n$$\nf_{\\text{stable}}(0) = \\frac{1}{\\sqrt{1 + 0} + 1} = \\frac{1}{1 + 1} = \\frac{1}{2}\n$$\nThis confirms that the stabilized formula correctly evaluates to the limit at $t=0$. It therefore handles the case of the diagonal $y=x$ without producing a `Not a Number` and without requiring a special conditional statement for $t=0$.\n\nThe derived expression $f_{\\text{stable}}(t)$ is numerically sound for the entire domain $1 + t \\ge 0$. For values of $t$ far from $0$, it remains mathematically equivalent and computationally accurate. For values near the boundary $t = -1$, for instance $t = -1 + \\epsilon$ for some small $\\epsilon > 0$, the denominator becomes $\\sqrt{\\epsilon} + 1$, which is well-behaved. Specifically at the boundary point $t = -1$, the function evaluates to $f_{\\text{stable}}(-1) = \\frac{1}{\\sqrt{1-1}+1} = \\frac{1}{0+1} = 1$, which is the correct limit as $t \\to -1^{+}$.\n\nTherefore, the single, globally stable evaluation strategy for all $(x,y)$ such that $1 + (x - y) \\ge 0$ is to first compute $t = x - y$, and then evaluate the expression:\n$$\nf(x,y) = \\frac{1}{\\sqrt{1 + t} + 1}\n$$\nThis single formula satisfies all conditions of the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerically stable values for the function f(x,y).\n    The naive form is f_naive = (sqrt(1 + t) - 1) / t, where t = x - y.\n    This suffers from catastrophic cancellation when t is near 0.\n    The stable form, derived by multiplying by the conjugate, is:\n    f_stable = 1 / (sqrt(1 + t) + 1).\n    This form is used for all calculations as it is robust across the valid domain.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0),\n        (1.0 + 1e-16, 1.0),\n        (1.0 - 1e-16, 1.0),\n        (0.0, 0.0),\n        (1e-12, 0.0),\n        (0.0, 1e-12),\n        (2.0, 1.0),\n        (0.0, 1.0),\n        (1e8, 0.0),\n        (0.0, 0.999999999),\n    ]\n\n    results = []\n    for x, y in test_cases:\n        # Calculate t = x - y\n        t = x - y\n        \n        # The domain condition 1 + t >= 0 is guaranteed by the problem statement.\n        # Use the numerically stable formula derived in the solution.\n        # This formula is valid and stable for all t in the domain, including t=0.\n        stable_value = 1.0 / (np.sqrt(1.0 + t) + 1.0)\n        results.append(stable_value)\n\n    # Final print statement in the exact required format.\n    # The format specifier '.15g' prints up to 15 significant digits.\n    print(f\"[{','.join(f'{r:.15g}' for r in results)}]\")\n\nsolve()\n```", "id": "2420071"}, {"introduction": "Many numerical methods, like those for approximating derivatives, involve a step size parameter, $h$. This introduces a fascinating trade-off: a smaller $h$ reduces the formula's inherent mathematical error (truncation error), but it can amplify the computer's finite-precision error (roundoff error). This exercise asks you to find the \"sweet spot\" by determining the optimal step size that balances these two competing error sources to achieve the most accurate result [@problem_id:2420015]. This analysis is central to the design and application of countless numerical algorithms.", "problem": "Consider the approximation of the derivative of the function $f(x) = \\sin(x)$ at the point $x = 1.0$ using the three-point central difference formula\n$$\nD(h) = \\frac{f(1.0+h) - f(1.0-h)}{2h}.\n$$\nAssume that all arithmetic is performed in floating-point according to the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard with rounding to nearest, with unit roundoff $u = 2^{-53}$. Assume further that each invocation of the elementary function $\\sin(\\cdot)$ adheres to the standard floating-point model, so that a computed value $\\widetilde{\\sin}(y)$ satisfies $\\widetilde{\\sin}(y) = \\sin(y)\\,(1+\\delta)$ with $|\\delta| \\le u$, and that each elementary arithmetic operation satisfies the same relative error bound. Treat $h$ as sufficiently small so that leading-order terms in $h$ and $u$ dominate, and evaluate $\\sin$ and $\\cos$ in radians. Determine the value of the step size $h$ that minimizes the leading-order bound on the absolute error in the computed $D(h)$ at $x=1.0$ arising from the combined effects of truncation and roundoff. Provide a single numerical value for $h$. Round your answer to $4$ significant figures. The answer is dimensionless.", "solution": "The problem statement is subjected to validation and is found to be scientifically grounded, well-posed, and objective. It is a standard problem in numerical analysis concerning the balance between truncation and roundoff errors. We shall proceed with a formal solution.\n\nThe total absolute error in approximating the derivative $f'(x)$ with the computed value of the central difference formula, $\\widetilde{D}(h)$, is bounded by the sum of the truncation error and the roundoff error:\n$$\n|\\widetilde{D}(h) - f'(x)| \\le |D(h) - f'(x)| + |\\widetilde{D}(h) - D(h)|\n$$\nThe first term, $|D(h) - f'(x)|$, represents the truncation error, $E_{\\text{trunc}}(h)$, inherent to the finite difference formula. The second term, $|\\widetilde{D}(h) - D(h)|$, is the roundoff error, $E_{\\text{round}}(h)$, which arises from floating-point arithmetic.\n\nFirst, we analyze the truncation error. The three-point central difference formula for the derivative of a function $f(x)$ is given by:\n$$\nD(h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nWe use Taylor series expansions for $f(x+h)$ and $f(x-h)$ around the point $x$:\n$$\nf(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\frac{h^3}{6} f'''(x) + \\frac{h^4}{24} f^{(4)}(x) + \\frac{h^5}{120} f^{(5)}(x) + O(h^6)\n$$\n$$\nf(x-h) = f(x) - h f'(x) + \\frac{h^2}{2} f''(x) - \\frac{h^3}{6} f'''(x) + \\frac{h^4}{24} f^{(4)}(x) - \\frac{h^5}{120} f^{(5)}(x) + O(h^6)\n$$\nSubtracting the second expansion from the first yields:\n$$\nf(x+h) - f(x-h) = 2h f'(x) + \\frac{h^3}{3} f'''(x) + \\frac{h^5}{60} f^{(5)}(x) + O(h^7)\n$$\nSubstituting this into the formula for $D(h)$:\n$$\nD(h) = \\frac{2h f'(x) + \\frac{h^3}{3} f'''(x) + O(h^5)}{2h} = f'(x) + \\frac{h^2}{6} f'''(x) + O(h^4)\n$$\nThe truncation error is $E_{\\text{trunc}}(h) = |D(h) - f'(x)|$. The leading-order term of this error is:\n$$\nE_{\\text{trunc}}(h) \\approx \\left| \\frac{h^2}{6} f'''(x) \\right|\n$$\nFor the given function $f(x) = \\sin(x)$, the third derivative is $f'''(x) = -\\cos(x)$. The evaluation point is $x=1.0$ radian. The truncation error is then:\n$$\nE_{\\text{trunc}}(h) \\approx \\frac{h^2}{6} |-\\cos(1.0)| = \\frac{h^2}{6} \\cos(1.0)\n$$\nsince $1.0$ radian is in the first quadrant, where $\\cos(1.0) > 0$.\n\nNext, we analyze the roundoff error. The formula involves the subtraction of two nearly equal numbers, $f(x+h)$ and $f(x-h)$, for small $h$, which leads to catastrophic cancellation. This is the dominant source of roundoff error. The computed function values are $\\widetilde{f}(x+h) = f(x+h)(1+\\delta_1)$ and $\\widetilde{f}(x-h) = f(x-h)(1+\\delta_2)$, where $|\\delta_1|, |\\delta_2| \\le u$, with $u = 2^{-53}$ being the unit roundoff. The absolute error in the computed numerator is bounded by:\n$$\n|f(x+h)(1+\\delta_1) - f(x-h)(1+\\delta_2) - (f(x+h) - f(x-h))| \\approx |f(x+h)\\delta_1 - f(x-h)\\delta_2|\n$$\nThe bound on this error is $\\le u|f(x+h)| + u|f(x-h)|$. For small $h$, we can approximate $f(x+h) \\approx f(x)$ and $f(x-h) \\approx f(x)$. The error in the numerator is thus bounded by approximately $2u|f(x)|$. This error is then propagated through the division by $2h$. The leading-order roundoff error is therefore:\n$$\nE_{\\text{round}}(h) \\approx \\frac{2u|f(x)|}{2h} = \\frac{u|f(x)|}{h}\n$$\nFor $f(x) = \\sin(x)$ at $x=1.0$, this becomes:\n$$\nE_{\\text{round}}(h) \\approx \\frac{u|\\sin(1.0)|}{h} = \\frac{u\\sin(1.0)}{h}\n$$\nsince $\\sin(1.0) > 0$.\n\nThe total error bound, $E(h)$, is the sum of the leading-order truncation and roundoff errors:\n$$\nE(h) \\approx E_{\\text{trunc}}(h) + E_{\\text{round}}(h) = \\frac{h^2}{6}\\cos(1.0) + \\frac{u\\sin(1.0)}{h}\n$$\nTo find the step size $h$ that minimizes this error bound, we differentiate $E(h)$ with respect to $h$ and set the result to zero:\n$$\n\\frac{dE}{dh} = \\frac{2h}{6}\\cos(1.0) - \\frac{u\\sin(1.0)}{h^2} = \\frac{h}{3}\\cos(1.0) - \\frac{u\\sin(1.0)}{h^2}\n$$\nSetting $\\frac{dE}{dh} = 0$:\n$$\n\\frac{h_{\\text{opt}}}{3}\\cos(1.0) = \\frac{u\\sin(1.0)}{h_{\\text{opt}}^2}\n$$\nSolving for $h_{\\text{opt}}$:\n$$\nh_{\\text{opt}}^3 = \\frac{3u\\sin(1.0)}{\\cos(1.0)} = 3u\\tan(1.0)\n$$\n$$\nh_{\\text{opt}} = (3u\\tan(1.0))^{1/3}\n$$\nNow, we substitute the numerical values $u = 2^{-53}$ and evaluate $\\tan(1.0)$ in radians.\n$$\nh_{\\text{opt}} = (3 \\times 2^{-53} \\times \\tan(1.0))^{1/3}\n$$\nUsing a calculator, $\\tan(1.0) \\approx 1.55740772$.\n$$\nh_{\\text{opt}} \\approx (3 \\times 2^{-53} \\times 1.55740772)^{1/3}\n$$\n$$\nh_{\\text{opt}} \\approx (4.67222317 \\times 2^{-53})^{1/3}\n$$\n$$\nh_{\\text{opt}} \\approx (4.67222317 \\times 1.11022302 \\times 10^{-16})^{1/3}\n$$\n$$\nh_{\\text{opt}} \\approx (5.18742014 \\times 10^{-16})^{1/3}\n$$\n$$\nh_{\\text{opt}} \\approx 8.03525 \\times 10^{-6}\n$$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$\nh_{\\text{opt}} \\approx 8.035 \\times 10^{-6}\n$$\nThis is the optimal step size that balances the decreasing truncation error and the increasing roundoff error.", "answer": "$$\n\\boxed{8.035 \\times 10^{-6}}\n$$", "id": "2420015"}, {"introduction": "We now scale up our analysis from single calculations to a full iterative algorithm in numerical linear algebra. The Gram-Schmidt process is a fundamental method for constructing a set of orthonormal vectors, but its most straightforward implementation, Classical Gram-Schmidt (CGS), is notoriously unstable. This practice dramatically illustrates how a subtle change in the order of operations, leading to the Modified Gram-Schmidt (MGS) algorithm, can completely restore numerical stability and accuracy [@problem_id:2419987]. This exercise provides a powerful lesson on how the design of an algorithm itself is a key strategy for roundoff control.", "problem": "You are given a family of column sets in real vector spaces constructed to be nearly collinear. For given integers $n \\ge k \\ge 1$ and a real parameter $\\epsilon \\ge 0$, define the standard basis $\\{e_1,\\dots,e_n\\}$ of $\\mathbb{R}^n$ and construct the $n \\times k$ matrix $A = [a_1,\\dots,a_k]$ with columns\n- $a_1 = e_1$,\n- for $2 \\le j \\le k$, $a_j = e_1 + \\epsilon^{\\,j-1} e_j$.\nAll computations are to be performed in double-precision floating-point arithmetic.\n\nTask:\n1. Using the matrix $A$ defined above, form two orthonormalization results for the columns of $A$:\n   - one using the Classical Gram-Schmidt (CGS) method, yielding a matrix $Q^{(c)} \\in \\mathbb{R}^{n \\times k}$,\n   - one using the Modified Gram-Schmidt (MGS) method, yielding a matrix $Q^{(m)} \\in \\mathbb{R}^{n \\times k}$.\n   For both methods, if a diagonal element that would normalize a column is zero, the corresponding column in $Q$ must be set to the zero vector.\n2. Define the orthogonality error for any $Q \\in \\mathbb{R}^{n \\times k}$ as follows. Let $q_j$ denote the $j$-th column of $Q$, let $\\delta = 10^{-14}$, and define the index set $J = \\{ j \\in \\{1,\\dots,k\\} : \\|q_j\\|_2 > \\delta \\}$. Let $r = |J|$, and let $Q_J \\in \\mathbb{R}^{n \\times r}$ be the submatrix containing only the columns of $Q$ indexed by $J$ (in increasing order of $j$). Define\n   $$E(Q) = \\begin{cases}\n   \\left\\| Q_J^{\\mathsf{T}} Q_J - I_r \\right\\|_F, & r \\ge 1, \\\\\n   0, & r = 0,\n   \\end{cases}$$\n   where $\\|\\cdot\\|_F$ denotes the Frobenius norm and $I_r$ is the $r \\times r$ identity matrix.\n3. For each test case in the test suite below, construct $A$ from $(n,k,\\epsilon)$, compute $Q^{(c)}$ and $Q^{(m)}$, and then compute the pair of orthogonality errors $\\big(E(Q^{(c)}), E(Q^{(m)})\\big)$.\n\nTest suite:\n- Case 1 (happy path): $(n,k,\\epsilon) = (6, 3, 10^{-8})$.\n- Case 2 (boundary, exact collinearity of all but one direction): $(n,k,\\epsilon) = (6, 3, 0)$.\n- Case 3 (deeper chain of near-collinearity): $(n,k,\\epsilon) = (10, 8, 10^{-12})$.\n- Case 4 (edge, perturbations near machine precision): $(n,k,\\epsilon) = (6, 5, 10^{-16})$.\n\nFinal output format:\n- Your program must produce a single line of output containing the results as a Python-style list of lists of floats:\n  [[E(Q^{(c)}_1), E(Q^{(m)}_1)], [E(Q^{(c)}_2), E(Q^{(m)}_2)], [E(Q^{(c)}_3), E(Q^{(m)}_3)], [E(Q^{(c)}_4), E(Q^{(m)}_4)]],\n  where the subscript denotes the test case number. The numbers should be printed in standard decimal or scientific notation without any additional text. The program must not read any input and must run as-is.", "solution": "The problem statement presented is subjected to validation and is found to be valid. It is a well-posed problem in computational engineering and numerical linear algebra, designed to demonstrate the differing numerical stability of the Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS) orthonormalization algorithms. The problem is scientifically grounded, self-contained, and all its components are rigorously defined.\n\nThe task is to compare the loss of orthogonality when applying CGS and MGS to a set of nearly collinear vectors. The degree of collinearity is controlled by a parameter $\\epsilon$. For small $\\epsilon$, the vectors become numerically difficult to distinguish, which exposes the weaknesses of numerically unstable algorithms.\n\nFirst, we define the matrix $A \\in \\mathbb{R}^{n \\times k}$ for given integers $n \\ge k \\ge 1$ and a real parameter $\\epsilon \\ge 0$. Its columns $a_j$ for $j=1, \\dots, k$ are constructed as follows:\n- $a_1 = e_1$\n- $a_j = e_1 + \\epsilon^{j-1} e_j$ for $2 \\le j \\le k$\nwhere $\\{e_1, \\dots, e_n\\}$ is the standard basis of $\\mathbb{R}^n$. As $\\epsilon \\to 0$, the vectors $a_j$ for $j \\ge 2$ approach $a_1$, creating a family of nearly linearly dependent vectors. This construction presents a stringent test for orthonormalization algorithms. All computations are performed in standard double-precision floating-point arithmetic, where machine epsilon is approximately $\\epsilon_{mach} \\approx 2.22 \\times 10^{-16}$.\n\nThe two algorithms for orthonormalization are as follows:\n\n**Classical Gram-Schmidt (CGS)**\nThe CGS algorithm generates a set of orthonormal vectors $\\{q_1, \\dots, q_k\\}$ from the input vectors $\\{a_1, \\dots, a_k\\}$. For each vector $a_j$, it subtracts the components that lie in the direction of the previously computed orthonormal vectors $\\{q_1, \\dots, q_{j-1}\\}$. The process is defined by:\n1. Initialize $v_j = a_j$.\n2. Compute the projection sums: $v_j = a_j - \\sum_{i=1}^{j-1} (q_i^{\\mathsf{T}} a_j) q_i$.\n3. Normalize: $q_j = v_j / \\|v_j\\|_2$.\n\nThe numerical instability of CGS arises from step 2. The term $(q_i^{\\mathsf{T}} a_j)$ is computed using the original vector $a_j$. If $a_j$ is nearly parallel to the subspace spanned by $\\{q_1, \\dots, q_{j-1}\\}$, the vector $v_j$ will be the result of subtracting a large vector from another nearly identical large vector. This operation, known as catastrophic cancellation, leads to a large loss of relative precision. The resulting computed vector $\\hat{v}_j$ may have significant components parallel to $\\{q_1, \\dots, q_{j-1}\\}$, meaning the final vector set $\\{\\hat{q}_1, \\dots, \\hat{q}_k\\}$ fails to be orthogonal.\n\n**Modified Gram-Schmidt (MGS)**\nThe MGS algorithm is a rearrangement of the CGS computation that is numerically more stable. Instead of projecting a single vector $a_j$ against all previous $q_i$, MGS takes each new orthonormal vector $q_j$ and immediately removes its component from all subsequent vectors $\\{a_{j+1}, \\dots, a_k\\}$.\nThe process is:\n1. Initialize $v_j = a_j$ for all $j=1, \\dots, k$.\n2. For $j=1, \\dots, k$:\n   a. Normalize the current vector: $q_j = v_j / \\|v_j\\|_2$.\n   b. Orthogonalize all subsequent vectors against the new $q_j$: $v_l = v_l - (q_j^{\\mathsf{T}} v_l) q_j$ for $l = j+1, \\dots, k$.\n\nThis procedure is mathematically equivalent to CGS but behaves very differently in finite precision. By orthogonalizing the vectors $v_l$ at each step, MGS effectively performs an iterative refinement of orthogonality, preventing the accumulation of errors seen in CGS. The vector $v_l$ on which the projection is based has already been made orthogonal to $\\{q_1, \\dots, q_{j-1}\\}$, making the computation more robust.\n\nFor both algorithms, the problem specifies that if the norm of a vector to be normalized, $\\|v_j\\|_2$, is zero, the resulting column $q_j$ should be a zero vector. In a floating-point environment, we implement this by checking if the norm is below a small tolerance (e.g., $10^{-20}$) to robustly handle values that are mathematically zero but numerically non-zero due to roundoff.\n\n**Orthogonality Error Metric**\nThe quality of the orthonormalization is measured by the orthogonality error $E(Q)$. Given a matrix $Q \\in \\mathbb{R}^{n \\times k}$, we first filter out any zero or near-zero columns. We define an index set $J = \\{ j \\in \\{1,\\dots,k\\} : \\|q_j\\|_2 > \\delta \\}$ with tolerance $\\delta = 10^{-14}$. If $r = |J|$ is the count of non-negligible columns, we form a submatrix $Q_J \\in \\mathbb{R}^{n \\times r}$ from these columns. The error is the Frobenius norm of the deviation of $Q_J^{\\mathsf{T}} Q_J$ from the identity matrix $I_r$:\n$$\nE(Q) = \\begin{cases}\n   \\left\\| Q_J^{\\mathsf{T}} Q_J - I_r \\right\\|_F, & r \\ge 1, \\\\\n   0, & r = 0.\n\\end{cases}\n$$\nFor a perfectly orthonormal set of columns in $Q_J$, this error would be $0$.\n\n**Analysis of Test Cases**\n- **Case 1: $(n, k, \\epsilon) = (6, 3, 10^{-8})$**. Here, $\\epsilon = 10^{-8}$. The vector $a_2$ is close to $a_1$, and $a_3$ is extremely close since its perturbation is $\\epsilon^2 = 10^{-16}$, which is at the limit of double precision. The value $\\epsilon=10^{-8}$ is approximately $\\sqrt{\\epsilon_{mach}}$, a known threshold where CGS begins to lose orthogonality significantly. We expect a noticeable error for CGS, while MGS should remain accurate.\n- **Case 2: $(n, k, \\epsilon) = (6, 3, 0)$**. With $\\epsilon=0$, the columns are exactly collinear: $A = [e_1, e_1, e_1]$. Both algorithms should correctly identify the linear dependence, producing $Q = [e_1, 0, 0, \\dots]$. The resulting error $E(Q)$ should be $0$ for both, as only one column will be non-zero.\n- **Case 3: $(n, k, \\epsilon) = (10, 8, 10^{-12})$**. The perturbation term $\\epsilon^{j-1}$ rapidly becomes smaller than machine precision. For $j=3$, $\\epsilon^2 = 10^{-24}$, so $a_3$ will be computationally identical to $e_1$. The matrix $A$ will be numerically $[e_1, e_1 + 10^{-12}e_2, e_1, e_1, \\dots]$. CGS will suffer severe loss of orthogonality due to error propagation. MGS will correctly orthogonalize $a_2$ against $a_1$ and then find that all subsequent vectors are in the span of $q_1$, resulting in zero vectors. The error for MGS should be small, while for CGS it will be large.\n- **Case 4: $(n, k, \\epsilon) = (6, 5, 10^{-16})$**. Here $\\epsilon$ is itself at the level of machine precision. The vector $a_2 = e_1 + 10^{-16} e_2$ is barely distinguishable from $a_1$. All subsequent vectors $a_j$ for $j \\ge 3$ will be numerically identical to $e_1$. This is an extreme case where CGS is expected to fail completely, producing columns that are not remotely orthogonal. MGS should handle this gracefully, yielding two orthonormal vectors and subsequent zero vectors, resulting in a very low error.\n\nThe implementation will follow these principles to compute the specified error pairs for each test case.", "answer": "```python\nimport numpy as np\n\ndef build_A(n, k, epsilon):\n    \"\"\"\n    Constructs the n x k matrix A with nearly collinear columns.\n    \n    Args:\n        n (int): Number of rows.\n        k (int): Number of columns.\n        epsilon (float): Parameter controlling collinearity.\n    \n    Returns:\n        np.ndarray: The n x k matrix A.\n    \"\"\"\n    A = np.zeros((n, k), dtype=np.float64)\n    # a_1 = e_1\n    A[0, 0] = 1.0\n    # a_j = e_1 + epsilon^(j-1) * e_j for j >= 2\n    for j in range(1, k):\n        A[0, j] = 1.0\n        if j < n:\n            A[j, j] = epsilon**j\n    return A\n\ndef classical_gram_schmidt(A):\n    \"\"\"\n    Orthonormalizes the columns of A using the Classical Gram-Schmidt method.\n    \n    Args:\n        A (np.ndarray): The matrix to orthonormalize.\n    \n    Returns:\n        np.ndarray: The matrix Q with orthonormal columns.\n    \"\"\"\n    n, k = A.shape\n    Q = np.zeros((n, k), dtype=np.float64)\n    # A small tolerance to check for zero norm\n    norm_tol = 1e-20 \n    \n    for j in range(k):\n        v = A[:, j].copy()\n        for i in range(j):\n            # CGS projects the original vector A[:, j] onto each q_i\n            proj_coeff = np.dot(Q[:, i].T, A[:, j])\n            v -= proj_coeff * Q[:, i]\n        \n        norm_v = np.linalg.norm(v)\n        if norm_v > norm_tol:\n            Q[:, j] = v / norm_v\n        # If norm_v is too small, Q[:, j] remains a zero vector.\n            \n    return Q\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Orthonormalizes the columns of A using the Modified Gram-Schmidt method.\n    \n    Args:\n        A (np.ndarray): The matrix to orthonormalize.\n    \n    Returns:\n        np.ndarray: The matrix Q with orthonormal columns.\n    \"\"\"\n    V = A.copy()\n    n, k = V.shape\n    Q = np.zeros((n, k), dtype=np.float64)\n    # A small tolerance to check for zero norm\n    norm_tol = 1e-20\n\n    for j in range(k):\n        norm_v = np.linalg.norm(V[:, j])\n        if norm_v > norm_tol:\n            Q[:, j] = V[:, j] / norm_v\n            # MGS orthogonalizes all subsequent vectors against the new q_j\n            for l in range(j + 1, k):\n                proj_coeff = np.dot(Q[:, j].T, V[:, l])\n                V[:, l] -= proj_coeff * Q[:, j]\n        # If norm_v is too small, Q[:, j] remains zero and no orthogonalization\n        # is performed against it.\n            \n    return Q\n\ndef orthogonality_error(Q):\n    \"\"\"\n    Calculates the orthogonality error E(Q) as defined in the problem.\n    \n    Args:\n        Q (np.ndarray): The matrix with putatively orthonormal columns.\n        \n    Returns:\n        float: The orthogonality error.\n    \"\"\"\n    n, k = Q.shape\n    delta = 1e-14\n    \n    J = [j for j in range(k) if np.linalg.norm(Q[:, j]) > delta]\n    r = len(J)\n    \n    if r == 0:\n        return 0.0\n    \n    Q_J = Q[:, J]\n    \n    I_r = np.identity(r, dtype=np.float64)\n    error_matrix = Q_J.T @ Q_J - I_r\n    \n    return np.linalg.norm(error_matrix, 'fro')\n\ndef solve():\n    \"\"\"\n    Runs the full test suite and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (6, 3, 1e-8),      # Case 1\n        (6, 3, 0.0),       # Case 2\n        (10, 8, 1e-12),    # Case 3\n        (6, 5, 1e-16),     # Case 4\n    ]\n\n    all_results = []\n    for n, k, epsilon in test_cases:\n        A = build_A(n, k, epsilon)\n        \n        Q_cgs = classical_gram_schmidt(A)\n        Q_mgs = modified_gram_schmidt(A)\n        \n        error_cgs = orthogonality_error(Q_cgs)\n        error_mgs = orthogonality_error(Q_mgs)\n        \n        all_results.append([error_cgs, error_mgs])\n\n    # The final print statement must follow the exact specified format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2419987"}]}