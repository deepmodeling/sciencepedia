{"hands_on_practices": [{"introduction": "To trust our numerical simulations, we must first verify that our code behaves as designed. The Method of Manufactured Solutions (MMS) is a rigorous and standard procedure for this verification. By inventing, or \"manufacturing,\" a smooth solution to our Partial Differential Equation (PDE), we create a test case where the exact error can be calculated, allowing us to numerically measure the scheme's convergence rate and confirm its order of consistency. This practice [@problem_id:2380147] will guide you through implementing the MMS for the Poisson equation, a fundamental skill for any computational scientist.", "problem": "You are to write a complete program that numerically verifies the order of consistency of a finite difference scheme by the Method of Manufactured Solutions (MMS). Consider the following boundary value problems for the Partial Differential Equation (PDE) and its one-dimensional analogue, posed on the unit domains. Denote the spatial domain in two dimensions by $\\Omega = (0,1) \\times (0,1)$ and in one dimension by $(0,1)$. The governing equations and boundary conditions are:\n\n- In two dimensions: find $u:\\overline{\\Omega}\\to\\mathbb{R}$ such that \n  $$\n  -\\Delta u = f \\ \\text{ in } \\ \\Omega, \n  \\quad \n  u = g \\ \\text{ on } \\ \\partial\\Omega,\n  $$\n  where $\\Delta$ is the Laplace operator.\n\n- In one dimension: find $u:[0,1]\\to\\mathbb{R}$ such that \n  $$\n  -u'' = f \\ \\text{ in } \\ (0,1), \n  \\quad \n  u(0)=g_0, \\ u(1)=g_1.\n  $$\n\nFor each test case below, a smooth manufactured solution $u_\\star$ is given. Define the source term $f$ by substituting $u_\\star$ into the continuous operator, i.e., use $f = -\\Delta u_\\star$ in two dimensions and $f = -u_\\star''$ in one dimension, and define boundary data from $g = u_\\star$ on $\\partial\\Omega$ (and $g_0 = u_\\star(0)$, $g_1 = u_\\star(1)$ in one dimension). Use the standard second-order central finite difference scheme on a uniform grid with mesh size $h$:\n- In two dimensions, the discrete operator for interior grid point $(i,j)$ with spacing $h$ is\n  $$\n  \\left(-\\Delta_h u\\right)_{i,j} \n  = \\frac{4 u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2}.\n  $$\n- In one dimension, the discrete operator for interior grid point $i$ with spacing $h$ is\n  $$\n  \\left(-\\delta_{xx} u\\right)_{i} \n  = \\frac{2 u_i - u_{i+1} - u_{i-1}}{h^2}.\n  $$\n\nFor each test case, solve the corresponding linear system for a sequence of refinements, compute the grid error as the difference between the numerical solution and the restriction of $u_\\star$ to the interior nodes, and measure the discrete two-norm (discrete $L^2$ norm) defined by\n- In two dimensions, for interior error values $e_{i,j}$:\n  $$\n  \\|e\\|_{2,h} = \\left( h^2 \\sum_{i=1}^{N} \\sum_{j=1}^{N} e_{i,j}^2 \\right)^{1/2}.\n  $$\n- In one dimension, for interior error values $e_{i}$:\n  $$\n  \\|e\\|_{2,h} = \\left( h \\sum_{i=1}^{N} e_{i}^2 \\right)^{1/2}.\n  $$\n\nLet $h_k$ denote the mesh sizes used and $E_k$ the corresponding discrete two-norms of the error. Define the observed order of consistency $p$ for each test case as the value that minimizes\n$$\n\\sum_{k} \\left( \\log(E_k) - \\alpha - p \\log(h_k) \\right)^2\n$$\nover real numbers $\\alpha$ and $p$. Report this $p$.\n\nTest Suite (each bullet is one test case to be processed independently; set $h = 1/(N+1)$):\n- Test case $1$ (two dimensions, trigonometric manufactured solution):\n  - Manufactured solution: $u_\\star(x,y) = \\sin(\\pi x)\\sin(\\pi y)$.\n  - Forcing term: $f(x,y) = -\\Delta u_\\star(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n  - Boundary data: $g(x,y) = u_\\star(x,y)$ on $\\partial\\Omega$.\n  - Interior grid sizes: $N \\in \\{8,16,32,64\\}$.\n\n- Test case $2$ (two dimensions, polynomial manufactured solution):\n  - Define $p(x) = x^2(1-x)^2$ and $p''(x) = 2 - 12x + 12x^2$.\n  - Manufactured solution: $u_\\star(x,y) = p(x)\\,p(y)$.\n  - Forcing term: $f(x,y) = -\\Delta u_\\star(x,y) = -\\big(p''(x)p(y) + p(x)p''(y)\\big)$.\n  - Boundary data: $g(x,y) = u_\\star(x,y)$ on $\\partial\\Omega$.\n  - Interior grid sizes: $N \\in \\{10,20,40,80\\}$.\n\n- Test case $3$ (one dimension, trigonometric manufactured solution):\n  - Manufactured solution: $u_\\star(x) = \\sin(5\\pi x)$.\n  - Forcing term: $f(x) = -u_\\star''(x) = 25\\pi^2 \\sin(5\\pi x)$.\n  - Boundary data: $u(0) = u_\\star(0)$ and $u(1) = u_\\star(1)$.\n  - Interior grid sizes: $N \\in \\{16,32,64,128\\}$.\n\nYour program must compute the observed order $p$ for each of the three test cases using the definition above and produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, $\\texttt{[2.000000,2.000000,2.000000]}$). No additional text should be printed.", "solution": "The problem presented is a standard exercise in the verification of numerical methods for partial differential equations, specifically using the Method of Manufactured Solutions (MMS). The problem is well-defined, scientifically sound, and internally consistent. All necessary data, equations, and definitions for its resolution are provided. Therefore, we proceed with the solution.\n\nThe objective is to numerically determine the order of consistency, $p$, for a standard finite difference scheme applied to the Poisson boundary value problem. For a scheme of order $p$, the discretization error, $E$, is expected to scale with the mesh size, $h$, according to the relation $E \\approx C h^p$, where $C$ is a constant that depends on the manufactured solution but not on $h$. Taking the natural logarithm of this relation yields:\n$$ \\ln(E) \\approx \\ln(C) + p \\ln(h) $$\nThis equation reveals a linear relationship between $\\ln(E)$ and $\\ln(h)$, with the slope being the order of consistency $p$. The problem requires us to compute $p$ for several test cases by performing a linear least-squares regression on a set of data points $(\\ln(h_k), \\ln(E_k))$, obtained by solving the problem on a sequence of refined grids. The value $p$ is the one that minimizes the sum of squared residuals $\\sum_{k} ( \\ln(E_k) - \\alpha - p \\ln(h_k) )^2$, where $\\alpha = \\ln(C)$.\n\nFor each test case, we must solve a linear system of the form $A \\mathbf{u}_h = \\mathbf{b}_h$, which arises from the finite difference discretization of the governing Poisson equation. Here, $\\mathbf{u}_h$ is the vector of unknown solution values at the interior grid nodes.\n\n**One-Dimensional Case**\n\nThe governing equation is $-u'' = f$ on $(0,1)$ with $u(0)=g_0$ and $u(1)=g_1$. We use a uniform grid with $N$ interior points $x_i = i h$ for $i=1, \\dots, N$, where the mesh size is $h=1/(N+1)$. The second-order central difference approximation for the negative second derivative at an interior node $x_i$ is:\n$$ (-\\delta_{xx} u)_i = \\frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} = f(x_i) $$\nThis set of $N$ equations for the $N$ unknowns $u_1, \\dots, u_N$ forms the linear system $A \\mathbf{u}_h = \\mathbf{b}_h$. The matrix $A$ is an $N \\times N$ matrix representing the discrete operator $(1/h^2)(-\\delta_{xx})$:\n$$ A = \\frac{1}{h^2} \n\\begin{pmatrix}\n2  -1    \\\\\n-1  2  -1   \\\\\n \\ddots  \\ddots  \\ddots  \\\\\n  -1  2  -1 \\\\\n   -1  2\n\\end{pmatrix}\n$$\nThe right-hand side vector $\\mathbf{b}_h$ incorporates the source term $f$ and the boundary conditions. The equation for the first interior node ($i=1$) is $(-u_0 + 2u_1 - u_2)/h^2 = f(x_1)$. Since $u_0 = g_0$ is known, we move this term to the right side: $(2u_1 - u_2)/h^2 = f(x_1) + g_0/h^2$. Similarly, for the last interior node ($i=N$), we have $(-u_{N-1} + 2u_N)/h^2 = f(x_N) + g_{N+1}/h^2$, where $u_{N+1} = g_1$. For the given test cases, the manufactured solutions are such that the boundary conditions are homogeneous, i.e., $g_0=g_1=0$. Thus, the right-hand side vector simplifies to $\\mathbf{b}_h = (f(x_1), f(x_2), \\dots, f(x_N))^T$.\n\n**Two-Dimensional Case**\n\nThe governing equation is $-\\Delta u = f$ on $\\Omega=(0,1)\\times(0,1)$. The interior grid points are $(x_i, y_j)$ where $x_i=ih, y_j=jh$ for $i,j=1, \\dots, N$, and $h=1/(N+1)$. The 5-point stencil for the negative Laplacian at an interior node $(i,j)$ is:\n$$ (-\\Delta_h u)_{i,j} = \\frac{4u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2} = f(x_i, y_j) $$\nThis gives a system of $N^2$ linear equations for the $N^2$ unknowns. The unknown values $u_{i,j}$ are flattened into a single vector $\\mathbf{u}_h$ of length $N^2$. Adopting a row-major ordering where the index $k$ corresponds to grid point $(i,j)$ via $k = jN+i$ (for $i,j=0,\\dots,N-1$), the system matrix $A$ has a block-tridiagonal structure. It can be constructed using the Kronecker product ($\\otimes$):\n$$ A = \\frac{1}{h^2} (I_N \\otimes T_N + T_N \\otimes I_N) $$\nwhere $T_N$ is the $N \\times N$ one-dimensional discretization matrix (the tridiagonal matrix shown previously, without the $1/h^2$ scaling factor) and $I_N$ is the $N \\times N$ identity matrix. The resulting matrix $A$ is of size $N^2 \\times N^2$. As in the 1D case, the manufactured solutions provided for the 2D test cases result in homogeneous Dirichlet boundary conditions ($u=0$ on $\\partial\\Omega$). Therefore, the right-hand side vector $\\mathbf{b}_h$ consists simply of the values of the source term $f(x_i, y_j)$ at the interior grid points, flattened in the same row-major order.\n\n**Error Calculation and Order Estimation**\n\nAfter solving the linear system $A \\mathbf{u}_h = \\mathbf{b}_h$ for the numerical solution vector $\\mathbf{u}_h$, we compute the error vector $\\mathbf{e}_h = \\mathbf{u}_h - \\mathbf{u}_{\\star,h}$, where $\\mathbf{u}_{\\star,h}$ is the vector of the manufactured solution evaluated at the interior grid nodes. The discrete $L^2$-norm of the error, $E_k = \\|\\mathbf{e}_{h_k}\\|_{2,h_k}$, is calculated for each mesh size $h_k$ using the formulas provided in the problem statement. Finally, the collection of pairs $(\\ln(h_k), \\ln(E_k))$ is used in a linear regression to find the slope $p$, which is our estimate for the order of consistency.\n\nThe procedure is implemented for each of the three test cases. The matrix $A$ in all cases is sparse, symmetric, and positive definite, so we employ efficient sparse linear algebra routines for the solution of the system.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, identity, kron\nfrom scipy.sparse.linalg import spsolve\n\ndef compute_order(dim, u_star, f, N_values):\n    \"\"\"\n    Computes the observed order of consistency for a given problem configuration.\n\n    Args:\n        dim (int): The spatial dimension of the problem (1 or 2).\n        u_star (callable): The manufactured solution function.\n        f (callable): The corresponding source term function.\n        N_values (list of int): A list of interior grid sizes for the convergence study.\n\n    Returns:\n        float: The computed order of consistency.\n    \"\"\"\n    h_list, E_list = [], []\n\n    for N in N_values:\n        h = 1.0 / (N + 1)\n\n        if dim == 1:\n            # 1D Solver\n            # Grid points (interior)\n            x_coords = np.linspace(h, 1 - h, N)\n\n            # Assemble matrix A for -u'' (scaled by 1/h^2)\n            # A is a tridiagonal matrix: [-1, 2, -1]\n            A = (1 / h**2) * diags([-1, 2, -1], [-1, 0, 1], shape=(N, N), format='csr')\n\n            # Assemble RHS vector b.\n            # For the given test cases, boundary conditions are homogeneous.\n            b = f(x_coords)\n            \n            # Solve the linear system\n            u_num = spsolve(A, b)\n\n            # Compute error\n            u_exact = u_star(x_coords)\n            error_vec = u_num - u_exact\n            error_norm = np.sqrt(h * np.sum(error_vec**2))\n\n        elif dim == 2:\n            # 2D Solver\n            # Assemble matrix A for -Delta_h using Kronecker products\n            # This assumes C-style (row-major) flattening of the grid.\n            T_N = diags([-1, 2, -1], [-1, 0, 1], shape=(N, N), format='csr')\n            I_N = identity(N, format='csr')\n            A = (1 / h**2) * (kron(I_N, T_N) + kron(T_N, I_N))\n            \n            # Create grid and assemble RHS vector b\n            x_coords = np.linspace(h, 1 - h, N)\n            y_coords = np.linspace(h, 1 - h, N)\n            # 'xy' indexing is consistent with C-style flattening.\n            X, Y = np.meshgrid(x_coords, y_coords, indexing='xy')\n            \n            # Boundary conditions are homogeneous for all test cases.\n            F = f(X, Y)\n            b = F.flatten()  # Default is 'C' order (row-major)\n            \n            # Solve the linear system\n            u_flat = spsolve(A, b)\n            u_num = u_flat.reshape((N, N)) # Reshape with 'C' order\n\n            # Compute error\n            u_exact = u_star(X, Y)\n            error_matrix = u_num - u_exact\n            error_norm = np.sqrt(h**2 * np.sum(error_matrix**2))\n        else:\n            raise ValueError(\"Dimension must be 1 or 2.\")\n            \n        h_list.append(h)\n        E_list.append(error_norm)\n    \n    # Perform linear regression on log-log data to find the order p\n    log_h = np.log(np.array(h_list))\n    log_E = np.log(np.array(E_list))\n    \n    # np.polyfit finds coefficients [p, alpha] for log(E) = p*log(h) + alpha\n    p, _ = np.polyfit(log_h, log_E, 1)\n    \n    return p\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"dim\": 2,\n            \"u_star\": lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"f\": lambda x, y: 2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"N_values\": [8, 16, 32, 64]\n        },\n        {\n            \"dim\": 2,\n            \"u_star\": lambda x, y: (x**2 * (1 - x)**2) * (y**2 * (1 - y)**2),\n            \"f\": lambda x, y: -((12*x**2 - 12*x + 2) * (y**2 * (1 - y)**2) + \\\n                                (x**2 * (1 - x)**2) * (12*y**2 - 12*y + 2)),\n            \"N_values\": [10, 20, 40, 80]\n        },\n        {\n            \"dim\": 1,\n            \"u_star\": lambda x: np.sin(5 * np.pi * x),\n            \"f\": lambda x: 25 * np.pi**2 * np.sin(5 * np.pi * x),\n            \"N_values\": [16, 32, 64, 128]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p = compute_order(case[\"dim\"], case[\"u_star\"], case[\"f\"], case[\"N_values\"])\n        results.append(p)\n\n    # Format output as specified: a list of floats with 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2380147"}, {"introduction": "A numerical scheme is only as strong as its weakest link. While we often focus on the high-order accuracy of an interior stencil, the treatment of boundary conditions is equally critical. This exercise [@problem_id:2380200] provides a powerful demonstration of this principle. By implementing a scheme that is second-order in the interior but contains a zeroth-order error at the boundary, you will witness firsthand how a single, localized, low-order error can dominate and degrade the global accuracy of the entire solution.", "problem": "Consider the two-point boundary value problem on the closed interval $\\left[0,1\\right]$:\n$$\nu''(x) = f(x), \\quad x \\in (0,1), \\qquad u(0) = 0,\\quad u(1) = 0.\n$$\nLet the exact solution be $u(x) = \\sin(\\pi x)$ so that $f(x) = -\\pi^2 \\sin(\\pi x)$. Design a discrete numerical scheme on a uniform grid that is second-order accurate in the interior but only zeroth-order accurate at the boundary $x=0$ by enforcing the following:\n- Use a uniform grid with $N$ subintervals and spacing $h = 1/N$ at nodes $x_i = i h$ for $i=0,1,\\dots,N$.\n- For interior nodes $i=1,2,\\dots,N-1$, discretize $u''(x_i)$ by the second-order central difference, i.e., require\n$$\n\\frac{U_{i-1} - 2U_i + U_{i+1}}{h^2} = f(x_i),\n$$\nwhere $U_i$ approximates $u(x_i)$.\n- At $x=1$, impose the exact Dirichlet boundary value $U_N = u(1) = 0$.\n- At $x=0$, impose a deliberately zeroth-order accurate boundary value by\n$$\nU_0 = u(0) + b = b,\n$$\nwhere $b$ is a constant that does not depend on $h$.\n\nYour program must compute the discrete solution $\\{U_i\\}_{i=0}^N$ for each test configuration below and quantify the effect of the zeroth-order boundary at $x=0$ on the global error.\n\nDefinitions and requirements:\n- For each grid, define the global maximum-norm error\n$$\nE_\\infty(N,b) = \\max_{0 \\le i \\le N} \\left| U_i - u(x_i) \\right|.\n$$\n- For two successive grids with $N$ and $2N$ subintervals and the same $b$, define the Experimental Order of Convergence (EOC) as\n$$\n\\mathrm{EOC}(N\\to 2N,b) = \\frac{\\log\\left(\\frac{E_\\infty(N,b)}{E_\\infty(2N,b)}\\right)}{\\log(2)}.\n$$\n\nTest suite:\n- Boundary offsets $b \\in \\{\\,0,\\ 1,\\ 0.1\\,\\}$.\n- Grids with $N \\in \\{\\,16,\\ 32,\\ 64,\\ 128\\,\\}$.\n\nTasks:\n- For each $b \\in \\{\\,0,\\ 1,\\ 0.1\\,\\}$, compute $E_\\infty(N,b)$ for all $N \\in \\{\\,16,\\ 32,\\ 64,\\ 128\\,\\}$.\n- For each $b$, compute $\\mathrm{EOC}(64\\to 128,b)$ using the errors at $N=64$ and $N=128$.\n- For each $b$, report two quantities in the following order:\n    1. $E_\\infty(128,b)$,\n    2. $\\mathrm{EOC}(64\\to 128,b)$.\n\nFinal output format:\n- Your program should produce a single line of output containing all results in a single comma-separated list enclosed in square brackets, ordered by the $b$ values in the sequence $b=0$, $b=1$, $b=0.1$. The six outputs must therefore be\n$$\n\\left[\\,E_\\infty(128,0),\\ \\mathrm{EOC}(64\\to 128,0),\\ E_\\infty(128,1),\\ \\mathrm{EOC}(64\\to 128,1),\\ E_\\infty(128,0.1),\\ \\mathrm{EOC}(64\\to 128,0.1)\\,\\right].\n$$\nAll reported values must be real numbers without units, printed in the specified order on a single line.", "solution": "The problem as stated is valid. It is a well-defined exercise in numerical analysis, specifically concerning the convergence properties of a finite difference scheme for a second-order boundary value problem. The problem is mathematically and scientifically sound, self-contained, and all terms are defined unambiguously. There are no contradictions or factual errors. We may therefore proceed with the solution.\n\nThe problem requires us to analyze the numerical solution of the two-point boundary value problem:\n$$\nu''(x) = f(x), \\quad x \\in (0,1)\n$$\nwith Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$. The exact solution is given as $u(x) = \\sin(\\pi x)$, which implies the forcing function must be $f(x) = u''(x) = -\\pi^2 \\sin(\\pi x)$.\n\nWe discretize the domain $[0,1]$ using a uniform grid with $N$ subintervals, step size $h = 1/N$, and nodes $x_i = i h$ for $i=0, 1, \\dots, N$. The numerical solution, denoted by $\\{U_i\\}_{i=0}^N$, is an approximation to the exact solution $\\{u(x_i)\\}_{i=0}^N$.\n\nThe core of the numerical scheme is the second-order central difference approximation for the second derivative at the interior nodes $i=1, \\dots, N-1$:\n$$\n\\frac{U_{i-1} - 2U_i + U_{i+1}}{h^2} = f(x_i)\n$$\nThis set of equations can be rearranged into a linear system. Let the vector of unknown interior grid values be $\\mathbf{U}_{\\text{int}} = [U_1, U_2, \\dots, U_{N-1}]^T$. The equations for these unknowns are:\nFor $i=1$: $U_0 - 2U_1 + U_2 = h^2 f(x_1)$\nFor $i=2, \\dots, N-2$: $U_{i-1} - 2U_i + U_{i+1} = h^2 f(x_i)$\nFor $i=N-1$: $U_{N-2} - 2U_{N-1} + U_N = h^2 f(x_{N-1})$\n\nThe boundary conditions are given as:\n$U_N = u(1) = 0$\n$U_0 = u(0) + b = b$\nHere, $b$ is a constant offset applied at the $x=0$ boundary.\n\nSubstituting these boundary values into the system of equations, we obtain a linear system $A \\mathbf{U}_{\\text{int}} = \\mathbf{d}$ of size $(N-1) \\times (N-1)$:\nThe matrix $A$ is a tridiagonal matrix defined by:\n$$\nA = \\begin{pmatrix}\n-2  1  0  \\cdots  0 \\\\\n1  -2  1  \\cdots  0 \\\\\n0  \\ddots  \\ddots  \\ddots  \\vdots \\\\\n\\vdots  \\cdots  1  -2  1 \\\\\n0  \\cdots  0  1  -2\n\\end{pmatrix}\n$$\nThis matrix is symmetric and negative definite, hence it is invertible, guaranteeing a unique solution for $\\mathbf{U}_{\\text{int}}$.\n\nThe right-hand side vector $\\mathbf{d} = [d_1, d_2, \\dots, d_{N-1}]^T$ is constructed as follows:\n$$\nd_i = h^2 f(x_i) \\quad \\text{for } i=2, \\dots, N-1\n$$\n$$\nd_1 = h^2 f(x_1) - U_0 = h^2 f(x_1) - b\n$$\n$$\nd_{N-1} = h^2 f(x_{N-1}) - U_N = h^2 f(x_{N-1})\n$$\nOnce the system is solved for $\\mathbf{U}_{\\text{int}}$, the full numerical solution is assembled as $\\{U_0, U_1, \\dots, U_{N-1}, U_N\\}$.\n\nWe must analyze the convergence of this scheme. The global error is measured in the maximum norm, $E_\\infty(N,b) = \\max_{0 \\le i \\le N} |U_i - u(x_i)|$. The experimental order of convergence (EOC) is calculated from solutions on two grids: $\\mathrm{EOC}(N\\to 2N,b) = \\log_2(E_\\infty(N,b) / E_\\infty(2N,b))$.\n\nCase 1: $b=0$.\nIn this case, $U_0 = 0$ and $U_N = 0$, which are the exact boundary values. The local truncation error (LTE) of the central difference scheme is $\\tau_i = \\frac{u(x_{i-1}) - 2u(x_i) + u(x_{i+1})}{h^2} - u''(x_i) = \\frac{h^2}{12}u^{(4)}(\\xi_i)$ for some $\\xi_i \\in (x_{i-1}, x_{i+1})$. The LTE is of order $O(h^2)$. Since the scheme is stable and the LTE is $O(h^2)$ at all interior points and zero at the boundaries, standard numerical analysis theory predicts that the global error will also be of order $O(h^2)$. Thus, $E_\\infty(N,0) \\propto h^2 \\propto N^{-2}$. We expect the EOC to be close to $2$.\n\nCase 2: $b \\neq 0$.\nHere, a deliberate error is introduced at the boundary $x=0$. The error at this single node is $e_0 = U_0 - u(0) = b - 0 = b$. This is a zeroth-order error, as it does not depend on $h$. The effect of a localized error at the boundary propagates into the domain. The global error $e_i = U_i - u(x_i)$ is the superposition of two components: one arising from the boundary error and another from the interior truncation errors. The solution to the homogeneous difference equation $e_{i-1} - 2e_i + e_{i+1} = 0$ with boundary conditions $e_0 = b$ and $e_N = 0$ is the linear function $e_i^{(h)} = b(1 - i/N) = b(1-x_i)$. The component due to the $O(h^2)$ interior truncation errors, $e_i^{(p)}$, is of order $O(h^2)$.\nThe total error is $e_i = e_i^{(h)} + e_i^{(p)} = b(1-x_i) + O(h^2)$.\nThe global error is $E_\\infty(N,b) = \\max_i |b(1-x_i) + O(h^2)|$. For $b \\neq 0$ and sufficiently large $N$ (small $h$), the $O(h^2)$ term is negligible compared to the $O(1)$ term $b(1-x_i)$. The maximum value of $|b(1-x_i)|$ on the grid $[0,1]$ occurs at $x_i=0$, where it is $|b|$. The error at other grid points, $|b(1-x_i)|$, is strictly less than $|b|$. Therefore, the maximum error will occur at $i=0$ and will be equal to $|b|$.\n$E_\\infty(N,b) = |b|$.\nSince the error does not decrease as $N$ increases, the scheme is zeroth-order convergent. The EOC is expected to be $\\log_2(|b|/|b|) = \\log_2(1) = 0$.\n\nThe program will implement the described finite difference method and compute the errors and EOCs to verify this analysis.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the BVP for different configurations and report results.\n    \"\"\"\n    \n    # Define the problem parameters and test suite.\n    b_values = [0.0, 1.0, 0.1]\n    N_values = [16, 32, 64, 128]\n    \n    # Store the final calculated values for printing.\n    final_results = []\n    \n    for b in b_values:\n        errors = {}\n        for N in N_values:\n            # 1. Set up the grid and functions.\n            h = 1.0 / N\n            # Grid points x_i = i*h for i = 0, ..., N.\n            x = np.linspace(0.0, 1.0, N + 1)\n            # Exact solution u(x) = sin(pi*x).\n            u_exact = np.sin(np.pi * x)\n            # Forcing function f(x) = -pi^2 * sin(pi*x).\n            f_vals = -np.pi**2 * np.sin(np.pi * x)\n\n            # 2. Construct the linear system A * U_int = d for interior points.\n            # The dimension of the system is (N-1) x (N-1).\n            dim = N - 1\n            if dim == 0:\n                errors[N] = np.max(np.abs(np.array([b, 0.0]) - u_exact))\n                continue\n\n            # Assemble the tridiagonal matrix A.\n            A = np.zeros((dim, dim))\n            # Main diagonal with -2.\n            np.fill_diagonal(A, -2.0)\n            # Sub-diagonal and super-diagonal with 1.\n            np.fill_diagonal(A[1:], 1.0)\n            np.fill_diagonal(A[:, 1:], 1.0)\n\n            # 3. Construct the right-hand side vector d.\n            # It incorporates f(x) and boundary conditions.\n            d = h**2 * f_vals[1:N]\n            d[0] -= b  # Contribution from U_0 = b.\n            # d[-1] -= 0, contribution from U_N = 0, no change needed.\n\n            # 4. Solve the linear system for the interior solution U_interior.\n            U_interior = np.linalg.solve(A, d)\n\n            # 5. Assemble the full numerical solution U.\n            U = np.zeros(N + 1)\n            U[0] = b\n            U[1:N] = U_interior\n            U[N] = 0.0\n\n            # 6. Calculate the maximum norm of the global error.\n            error = np.max(np.abs(U - u_exact))\n            errors[N] = error\n\n        # 7. Calculate required quantities for reporting.\n        # E_infinity for N=128.\n        E_inf_128 = errors[128]\n\n        # EOC using N=64 and N=128.\n        E_inf_64 = errors[64]\n\n        # Handle cases where error does not change or is zero to avoid log issues.\n        if E_inf_128 = 0.0 or E_inf_64 = 0.0 or E_inf_64 == E_inf_128:\n            eoc = 0.0\n        else:\n            eoc = np.log(E_inf_64 / E_inf_128) / np.log(2.0)\n\n        final_results.append(E_inf_128)\n        final_results.append(eoc)\n\n    # 8. Print the final results in the specified format.\n    # We use '.10g' for a clean representation of the floating point numbers.\n    print(f\"[{','.join(f'{val:.10g}' for val in final_results)}]\")\n\n# Execute the main function.\nsolve()\n```", "id": "2380200"}, {"introduction": "In the idealized world of mathematics, we can decrease the grid spacing $h$ indefinitely to drive the truncation error to zero. On a real computer, however, this is not the case. This exercise [@problem_id:2380203] explores the practical limits of consistency imposed by finite-precision arithmetic. You will analyze how round-off error, which grows as $h$ shrinks, competes with truncation error, leading to an \"optimal\" grid size and revealing that seeking ever-higher resolution can paradoxically increase the total error.", "problem": "A high-order finite difference scheme of spatial order $p$ with $p \\geq 6$ is used to approximate spatial derivatives in a smooth solution of a one-dimensional Partial Differential Equation (PDE). The grid spacing is $h  0$, and time stepping is chosen so that the temporal error is negligible compared to the spatial error over the range of $h$ considered. All computations are performed in floating-point arithmetic with machine unit roundoff $\\epsilon_{\\mathrm{mach}}$. In exact arithmetic, the scheme is consistent of order $p$ in space. In practice, for sufficiently small $h$, round-off errors may affect the observed convergence behavior.\n\nAssume that the implementation and algorithm are unchanged except for the arithmetic precision: either single-precision or double-precision floating-point arithmetic is used. The exact solution is smooth enough so that the truncation error model and stability constraints are valid, and the scheme remains linearly stable under the chosen refinement path.\n\nWhich statement best describes how using single-precision instead of double-precision floating-point arithmetic affects the practical consistency of this high-order scheme as $h \\to 0$?\n\nA. Using single-precision reduces the range of $h$ over which the observed error decays like $h^{p}$ and can cause an earlier onset of an error plateau or even error growth upon further refinement, so the scheme may appear practically inconsistent for small $h$ unless larger $h$ or mixed-precision techniques are used.\n\nB. Using single-precision does not affect practical consistency, because consistency depends only on the truncation error; the observed convergence rate remains $p$ down to arbitrarily small $h$.\n\nC. Using single-precision improves practical consistency, because larger round-off errors can cancel truncation errors, yielding faster error decay as $h \\to 0$.\n\nD. Using single-precision changes the formal consistency order from $p$ to $p/2$ for all $h$, because storing finite difference coefficients with fewer digits halves the order of accuracy.\n\nE. Using single-precision delays the onset of stiffness in the discretized PDE, thereby widening the $h$-interval over which truncation error dominates and improving the practical consistency regime.", "solution": "The user requests a critical validation of the problem statement, followed by a full solution and evaluation of options, adhering to the persona of a strict Russian STEM professor and specific formatting rules.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A high-order finite difference scheme of spatial order $p$ with $p \\geq 6$ is used.\n- The scheme approximates spatial derivatives in a smooth solution of a one-dimensional Partial Differential Equation (PDE).\n- The grid spacing is $h  0$.\n- Temporal error is negligible compared to the spatial error.\n- Computations are performed in floating-point arithmetic with machine unit roundoff $\\epsilon_{\\mathrm{mach}}$.\n- In exact arithmetic, the scheme is consistent of order $p$.\n- For sufficiently small $h$, round-off errors may affect the observed convergence behavior.\n- The comparison is between single-precision and double-precision floating-point arithmetic.\n- The exact solution is assumed to be smooth.\n- The scheme is assumed to remain linearly stable.\n- The question is: Which statement best describes how using single-precision instead of double-precision floating-point arithmetic affects the practical consistency of this high-order scheme as $h \\to 0$?\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scrutinized against the validation criteria.\n\n- **Scientifically Grounded (Critical)**: The problem is fundamentally sound. It describes a classic and critical topic in numerical analysis and computational science: the interplay between truncation error (discretization error) and round-off error in finite-precision arithmetic. The concepts of finite difference order $p$, grid spacing $h$, machine precision $\\epsilon_{\\mathrm{mach}}$, and their combined effect on the total error are all standard and well-established principles. The scenario is entirely realistic.\n- **Well-Posed**: The problem is well-posed. It asks for a qualitative analysis of the effect of changing $\\epsilon_{\\mathrm{mach}}$ on the observed error convergence curve $E(h)$. Given the standard model for total error, a unique and meaningful conclusion can be derived.\n- **Objective (Critical)**: The language used is precise, technical, and free of ambiguity or subjectivity. Terms like \"consistency,\" \"order $p$,\" and \"machine unit roundoff\" have rigorous definitions.\n- **Completeness and Consistency**: The problem provides sufficient assumptions (smooth solution, stable scheme, negligible temporal error) to isolate the phenomenon of interest, which is the balance between spatial truncation error and floating-point round-off error. The givens are self-consistent.\n- **Realism**: The situation described is not only realistic but is a common practical challenge faced by engineers and scientists when implementing and using high-order numerical methods.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically grounded, well-posed, objective, and presents a non-trivial question rooted in the fundamentals of computational mathematics. A solution will be derived.\n\n### Solution Derivation\n\nThe total error, $E_{\\text{total}}$, of a numerical approximation in floating-point arithmetic is the sum of two primary components: the truncation error, $E_{\\text{trunc}}$, and the round-off error, $E_{\\text{round}}$.\n\n$E_{\\text{total}}(h) = E_{\\text{trunc}}(h) + E_{\\text{round}}(h)$\n\n1.  **Truncation Error**: The problem states that the finite difference scheme is consistent of order $p$. This means the truncation error, which is the error inherent to approximating a continuous derivative with a discrete formula, behaves according to the following model for small $h$:\n    $$|E_{\\text{trunc}}(h)| \\approx C h^p$$\n    where $C$ is a constant that depends on the derivatives of the exact solution at the point of approximation but is independent of $h$. Since the solution is smooth and $p \\ge 6$, this error decreases very rapidly as $h$ is reduced.\n\n2.  **Round-off Error**: Round-off error arises because real numbers cannot be stored with infinite precision on a computer. Finite difference formulas compute derivatives by taking weighted sums of function values at nearby grid points, often involving division by powers of $h$. For example, a simple first derivative approximation is $\\frac{f(x+h) - f(x-h)}{2h}$. When $h$ is very small, this involves subtracting two nearly equal numbers, a classic source of catastrophic cancellation, and then dividing by the small number $h$, which amplifies the error. In general, for a scheme approximating a derivative of order $k$, the round-off error is modeled as:\n    $$|E_{\\text{round}}(h)| \\approx D \\frac{\\epsilon_{\\mathrm{mach}}}{h^k}$$\n    where $D$ is a constant related to the magnitude of the function values and the specific finite difference coefficients, and $\\epsilon_{\\mathrm{mach}}$ is the machine unit roundoff. For the purpose of this problem, we can assume $k \\ge 1$. As $h \\to 0$, round-off error grows.\n\n3.  **Total Error Behavior**: The magnitude of the total error can be approximated by the sum of the magnitudes of its components:\n    $$E_{\\text{total}}(h) \\approx C h^p + D \\frac{\\epsilon_{\\mathrm{mach}}}{h^k}$$\n    -   For large $h$, the term $C h^p$ dominates. The error decreases as $h$ is reduced, and plots of $\\log(E_{\\text{total}})$ versus $\\log(h)$ show a line with slope $p$. This is the **truncation-error-dominated regime**, where practical consistency is observed.\n    -   For very small $h$, the term $D \\frac{\\epsilon_{\\mathrm{mach}}}{h^k}$ dominates. The error increases as $h$ is further reduced. This is the **round-off-error-dominated regime**.\n    -   There exists an optimal grid spacing, $h_{\\text{opt}}$, that minimizes the total error. This occurs approximately when the two error contributions are equal:\n        $$C h_{\\text{opt}}^p \\approx D \\frac{\\epsilon_{\\mathrm{mach}}}{h_{\\text{opt}}^k} \\implies h_{\\text{opt}}^{p+k} \\approx \\frac{D}{C} \\epsilon_{\\mathrm{mach}}$$\n        $$h_{\\text{opt}} \\approx \\left(\\frac{D}{C} \\epsilon_{\\mathrm{mach}}\\right)^{1/(p+k)}$$\n\n4.  **Effect of Arithmetic Precision**: The key distinction between single- and double-precision arithmetic lies in the value of $\\epsilon_{\\mathrm{mach}}$:\n    -   Single-precision: $\\epsilon_{\\mathrm{mach, single}} \\approx 1.2 \\times 10^{-7}$\n    -   Double-precision: $\\epsilon_{\\mathrm{mach, double}} \\approx 2.2 \\times 10^{-16}$\n\n    Crucially, $\\epsilon_{\\mathrm{mach, single}} \\gg \\epsilon_{\\mathrm{mach, double}}$.\n    From the expression for $h_{\\text{opt}}$, since $h_{\\text{opt}}$ is proportional to $(\\epsilon_{\\mathrm{mach}})^{1/(p+k)}$, it is clear that $h_{\\text{opt, single}}  h_{\\text{opt, double}}$.\n\n    This means that when using single-precision arithmetic, the transition from the truncation-dominated regime to the round-off-dominated regime occurs at a **larger value of $h$** than it does for double-precision. The range of $h$ for which the theoretical convergence rate $p$ is observed in practice (i.e., $h  h_{\\text{opt}}$) is therefore **smaller** for single-precision. For values of $h$ below $h_{\\text{opt, single}}$, the total error will either level off to a minimum value (the error plateau) or begin to grow, masking the scheme's formal consistency. This minimum achievable error, $E_{\\min} \\approx E_{\\text{total}}(h_{\\text{opt}})$, is also much higher for single precision.\n\n### Option-by-Option Analysis\n\n**A. Using single-precision reduces the range of $h$ over which the observed error decays like $h^{p}$ and can cause an earlier onset of an error plateau or even error growth upon further refinement, so the scheme may appear practically inconsistent for small $h$ unless larger $h$ or mixed-precision techniques are used.**\nThis statement is a precise and accurate summary of the derivation above. \"Reduces the range of $h$\" is correct because $h_{\\text{opt, single}}  h_{\\text{opt, double}}$. \"Earlier onset\" correctly describes that the round-off floor is hit at a larger value of $h$. The term \"practically inconsistent\" correctly captures the phenomenon where the observed behavior (error growth) contradicts the theoretical expectation of convergence for that range of $h$. The mention of remediation strategies (using larger $h$ or mixed-precision) is also correct and relevant.\nVerdict: **Correct**.\n\n**B. Using single-precision does not affect practical consistency, because consistency depends only on the truncation error; the observed convergence rate remains $p$ down to arbitrarily small $h$.**\nThis statement incorrectly equates formal mathematical consistency (a limit as $h \\to 0$ in exact arithmetic) with practical, observed behavior in finite-precision arithmetic. The claim that the convergence rate remains $p$ for arbitrarily small $h$ is false, as it ignores the unavoidable growth of round-off error.\nVerdict: **Incorrect**.\n\n**C. Using single-precision improves practical consistency, because larger round-off errors can cancel truncation errors, yielding faster error decay as $h \\to 0$.**\nThis claim is fundamentally flawed. While it is possible for errors of opposite sign to cancel fortuitously at a single point, this is not a general or reliable phenomenon. Error analysis models errors by their magnitudes, assuming worst-case addition. The idea that larger random errors would systematically cancel a deterministic truncation error to produce *faster* convergence is nonsensical. In reality, larger round-off errors from single-precision arithmetic overwhelm the truncation error sooner, destroying the observed convergence.\nVerdict: **Incorrect**.\n\n**D. Using single-precision changes the formal consistency order from $p$ to $p/2$ for all $h$, because storing finite difference coefficients with fewer digits halves the order of accuracy.**\nThis is incorrect for multiple reasons. First, the *formal* order of a scheme is an algebraic property derived in exact arithmetic and is independent of the floating-point precision used for computation. Second, the effect of round-off is not a uniform change in convergence order for all $h$; it is a competing error source that dominates for small $h$. Third, the assertion that reduced precision \"halves the order of accuracy\" has no theoretical basis. It is a fabricated rule.\nVerdict: **Incorrect**.\n\n**E. Using single-precision delays the onset of stiffness in the discretized PDE, thereby widening the $h$-interval over which truncation error dominates and improving the practical consistency regime.**\nThis argument misinterprets stiffness and its relation to precision. Stiffness is determined by the spectrum of eigenvalues of the discretized operator matrix, which scales with powers of $1/h$. Floating-point precision does not change these eigenvalues in a systematic way that would \"delay\" stiffness. Furthermore, the conclusion that this widens the interval of practical consistency is the exact opposite of the correct analysis. Single-precision *narrows* this interval.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2380203"}]}