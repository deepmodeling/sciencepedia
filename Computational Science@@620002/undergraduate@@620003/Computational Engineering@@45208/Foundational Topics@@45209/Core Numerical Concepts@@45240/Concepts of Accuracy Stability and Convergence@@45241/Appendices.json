{"hands_on_practices": [{"introduction": "The Newton-Raphson method is celebrated for its fast, quadratic convergence. However, this impressive performance is conditional upon the properties of the function being analyzed. This exercise explores a scenario where this rapid convergence breaks down, demonstrating that the multiplicity of a root fundamentally alters the algorithm's behavior. By deriving the convergence rate from first principles for a function with a multiple root [@problem_id:2378389], you will gain a deeper appreciation for the interplay between a problem's analytical structure and the numerical accuracy of the method used to solve it.", "problem": "In computational engineering, the Newton–Raphson method is widely used to solve nonlinear equations. Consider applying the Newton–Raphson iteration\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\frac{p(x_k)}{p'(x_k)}\n$$\nto the polynomial\n$$\np(x) \\;=\\; (x-1)^{4}\\,(x+2),\n$$\nwith the goal of finding the root at $x=1$. Let $e_k = x_k - 1$ denote the error at iteration $k$, and assume an initial guess $x_0$ is chosen sufficiently close to $x=1$ so that the sequence $\\{x_k\\}$ converges to $1$. Using only the definition of the Newton–Raphson iteration, the definition of a multiple root, and basic first-order reasoning about smooth functions, determine the asymptotic linear error reduction factor $r$ defined by\n$$\nr \\;=\\; \\lim_{k\\to\\infty} \\frac{|e_{k+1}|}{|e_k|}.\n$$\nProvide your answer as an exact rational number. Do not compute any specific iterates.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim as follows:\n1.  Newton-Raphson iteration: $x_{k+1} = x_k - \\frac{p(x_k)}{p'(x_k)}$.\n2.  Polynomial function: $p(x) = (x-1)^{4}\\,(x+2)$.\n3.  Target root: $x=1$.\n4.  Error definition: $e_k = x_k - 1$.\n5.  Convergence assumption: The sequence $\\{x_k\\}$ generated from an initial guess $x_0$ converges to $1$.\n6.  Objective: Determine the asymptotic linear error reduction factor $r = \\lim_{k\\to\\infty} \\frac{|e_{k+1}|}{|e_k|}$.\n7.  Constraint: The derivation must use only the definitions of the Newton-Raphson method, multiple root, and basic first-order reasoning.\n8.  Answer format: Exact rational number.\n\nThe problem is scientifically grounded, as it deals with standard, well-established concepts in numerical analysis. It is well-posed, providing all necessary information to determine a unique, meaningful solution. The language is objective and precise. The problem is valid. We proceed with the solution.\n\nThe problem asks for the asymptotic error reduction factor for the Newton-Raphson method applied to a polynomial with a multiple root. The root of interest is $x=1$. Let the error at iteration $k$ be defined as $e_k = x_k - 1$. By this definition, we have $x_k = 1 + e_k$. The error at the next iteration, $e_{k+1}$, is given by $e_{k+1} = x_{k+1} - 1$.\n\nWe substitute the Newton-Raphson formula into the expression for $e_{k+1}$:\n$$\ne_{k+1} = \\left(x_k - \\frac{p(x_k)}{p'(x_k)}\\right) - 1\n$$\nUsing the relationship $x_k - 1 = e_k$, we can rewrite this as:\n$$\ne_{k+1} = e_k - \\frac{p(x_k)}{p'(x_k)}\n$$\nTo analyze the asymptotic behavior as $k \\to \\infty$, we must examine this relationship as $x_k \\to 1$, which implies $e_k \\to 0$. We express the function $p(x_k)$ and its derivative $p'(x_k)$ in terms of the error $e_k$ by substituting $x_k = 1 + e_k$.\n\nFirst, we compute the derivative of the polynomial $p(x) = (x-1)^{4}(x+2)$. Using the product rule for differentiation, we have:\n$$\np'(x) = \\frac{d}{dx}\\left[(x-1)^{4}(x+2)\\right] = 4(x-1)^{3}(x+2) + (x-1)^{4}(1)\n$$\nFactoring out the term $(x-1)^{3}$:\n$$\np'(x) = (x-1)^{3} \\left[ 4(x+2) + (x-1) \\right] = (x-1)^{3} (4x+8+x-1) = (x-1)^{3} (5x+7)\n$$\nNow, we evaluate $p(x_k)$ and $p'(x_k)$ at $x_k = 1 + e_k$:\n$$\np(x_k) = p(1+e_k) = ((1+e_k)-1)^{4}((1+e_k)+2) = (e_k)^{4}(e_k+3)\n$$\n$$\np'(x_k) = p'(1+e_k) = ((1+e_k)-1)^{3}(5(1+e_k)+7) = (e_k)^{3}(5e_k+5+7) = (e_k)^{3}(5e_k+12)\n$$\nThe problem statement indicates the root at $x=1$ has a multiplicity, which is evident from the factor $(x-1)^4$. For $p(x) = (x-1)^m g(x)$ with $g(1) \\neq 0$, the multiplicity is $m$. Here, $m=4$ and $g(x) = x+2$, with $g(1)=3 \\neq 0$.\n\nNow, substitute these expressions back into the equation for $e_{k+1}$:\n$$\ne_{k+1} = e_k - \\frac{(e_k)^{4}(e_k+3)}{(e_k)^{3}(5e_k+12)}\n$$\nFor $e_k \\neq 0$, we can simplify the fraction:\n$$\ne_{k+1} = e_k - e_k \\frac{e_k+3}{5e_k+12}\n$$\nFactor out $e_k$ to analyze the error reduction:\n$$\ne_{k+1} = e_k \\left( 1 - \\frac{e_k+3}{5e_k+12} \\right)\n$$\nTo combine the terms inside the parentheses, we find a common denominator:\n$$\ne_{k+1} = e_k \\left( \\frac{(5e_k+12) - (e_k+3)}{5e_k+12} \\right) = e_k \\left( \\frac{5e_k+12-e_k-3}{5e_k+12} \\right) = e_k \\left( \\frac{4e_k+9}{5e_k+12} \\right)\n$$\nThe asymptotic linear error reduction factor $r$ is defined as the limit of the ratio of successive errors:\n$$\nr = \\lim_{k\\to\\infty} \\frac{|e_{k+1}|}{|e_k|}\n$$\nUsing our derived expression for $e_{k+1}$:\n$$\n\\frac{e_{k+1}}{e_k} = \\frac{4e_k+9}{5e_k+12}\n$$\nThe assumption that the sequence $\\{x_k\\}$ converges to $1$ implies that $\\lim_{k\\to\\infty} e_k = 0$. We can now evaluate the limit:\n$$\nr = \\lim_{k\\to\\infty} \\left| \\frac{e_{k+1}}{e_k} \\right| = \\lim_{e_k\\to 0} \\left| \\frac{4e_k+9}{5e_k+12} \\right| = \\left| \\frac{4(0)+9}{5(0)+12} \\right| = \\left| \\frac{9}{12} \\right| = \\frac{3}{4}\n$$\nThis result is consistent with the general theory of Newton's method for a root of multiplicity $m$, which predicts a linear convergence rate of $r = \\frac{m-1}{m}$. For $m=4$, this gives $r = \\frac{4-1}{4} = \\frac{3}{4}$. Our derivation from first principles confirms this value. The asymptotic error reduction factor is an exact rational number.", "answer": "$$\n\\boxed{\\frac{3}{4}}\n$$", "id": "2378389"}, {"introduction": "When simulating physical systems over long durations, simply ensuring that the error is small at each step is not enough; we must also guarantee that the qualitative behavior of the system is preserved. This hands-on coding practice contrasts a standard integrator, the Explicit Euler method, with a structure-preserving geometric integrator, the Symplectic Euler method, on the classic Lotka-Volterra predator-prey model [@problem_id:2378425]. By implementing both, you will witness firsthand how respecting the underlying Hamiltonian structure of the equations leads to superior long-term stability and prevents the unphysical energy drift that plagues naive methods. This practice offers a crucial lesson in the importance of choosing an integrator that matches the geometric properties of the differential equation for reliable long-term predictions.", "problem": "You will study how time-integration choices affect long-term orbital stability, accuracy, and convergence for the classical predator-prey Lotka–Volterra system. Consider the coupled ordinary differential equations (ODEs)\n$$\n\\frac{dx}{dt} = a x - b x y,\\quad \\frac{dy}{dt} = -c y + d x y,\n$$\nwith strictly positive parameters and initial conditions. In this assignment, fix the parameters to $a = 1$, $b = 1$, $c = 1$, $d = 1$, and the initial condition to $x(0) = 1.5$, $y(0) = 1$. Your task is to implement two first-order time steppers, quantify their accuracy and stability over short and long horizons, and aggregate the numerical diagnostics in a single line of output as specified below. All angles are dimensionless and no physical units are involved.\n\nFundamental base: Use the basic definitions of ordinary differential equations, the explicit Euler method, the concept of a symplectic method on Hamiltonian systems, and standard notions of numerical accuracy (global error), stability (qualitative preservation of invariants and positivity), and convergence (observed order under time-step refinement). Use the following well-tested fact about the Lotka–Volterra system with $a, b, c, d > 0$: the function\n$$\nI(x,y) = d\\,x - c \\ln x + b\\,y - a \\ln y\n$$\nis constant along the exact trajectory for all $t$ such that $x(t) > 0$ and $y(t) > 0$.\n\nImplement the following components:\n\n1) Time steppers:\n- Explicit Euler (a non-structure-preserving scheme): Given time step $h  0$, advance $x$ and $y$ by applying the explicit Euler update to the right-hand side vector field.\n- Symplectic Euler (a structure-preserving scheme for Hamiltonian flows): Introduce the canonical change of variables $u = \\ln x$, $v = \\ln y$, for which the system can be written as a canonical Hamiltonian system with Hamiltonian\n$$\nH(u,v) = d\\,e^{u} - c\\,u + b\\,e^{v} - a\\,v,\n$$\nand canonical equations $u' = -\\partial H/\\partial v$, $v' = \\partial H/\\partial u$. Apply a first-order symplectic Euler scheme in the canonical variables $(u,v)$ and then map back to $(x,y)$ by $x = e^{u}$, $y = e^{v}$ per step.\n\n2) Diagnostics to quantify accuracy, stability, and convergence:\n- Invariant drift: For a given final time $T  0$, define the absolute drift of the invariant as\n$$\n\\Delta I = \\left| I\\!\\left(x(T), y(T)\\right) - I\\!\\left(x(0), y(0)\\right) \\right|.\n$$\nIf at any step the numerical $(x,y)$ leave the positive quadrant (that is, if `x = 0` or `y = 0`), then the invariant is not defined; in that case, report the drift as a floating-point not-a-number and separately flag positivity violation as described below.\n- Positivity flag: For a trajectory, return a boolean indicating whether $x_n  0$ and $y_n  0$ for every discrete step $n$ up to the final time.\n- Empirical order of convergence at final time: For a fixed $T  0$ and two step sizes $h$ and $h/2$, compute the numerical solutions at time $T$ with each step size. Use a high-accuracy reference solution computed by a robust ODE solver to approximate the exact state at time $T$. Define the errors $E(h)$ and $E(h/2)$ as the Euclidean norm of the difference between the numerical and reference states at time $T$. Then estimate the observed order by $p = \\log_2\\left(\\frac{E(h)}{E(h/2)}\\right)$.\nUse a high-accuracy reference with tight tolerances so that the reference error is negligible relative to the method errors.\n\n3) Test suite and required outputs:\nFix $a = 1$, $b = 1$, $c = 1$, $d = 1$, $x(0) = 1.5$, $y(0) = 1$. Implement the following five test cases, each producing a result to be aggregated:\n\n- Test $1$ (long-horizon drift and positivity, non-structure-preserving): explicit Euler with step size $h = 0.05$ on $[0,T]$ with $T = 50$. Output a two-element list $[\\Delta I, \\text{pos}]$ where $\\Delta I$ is the absolute invariant drift defined above (a float) and $\\text{pos}$ is the positivity flag (a boolean).\n- Test $2$ (long-horizon drift and positivity, structure-preserving): symplectic Euler with step size $h = 0.05$ on $[0,T]$ with $T = 50$. Output a two-element list $[\\Delta I, \\text{pos}]$ as above.\n- Test $3$ (empirical order for explicit Euler): compute the observed order at $T = 10$ using step sizes $h = 0.1$ and $h/2 = 0.05$. Use a high-accuracy reference solution for comparison. Output the scalar $p$ (a float).\n- Test $4$ (empirical order for symplectic Euler): compute the observed order at $T = 10$ using step sizes $h = 0.1$ and $h/2 = 0.05$. Use a high-accuracy reference solution for comparison. Output the scalar $p$ (a float).\n- Test $5$ (edge-case stability under coarse stepping): explicit Euler with step size $h = 0.5$ on $[0,T]$ with $T = 200$. Output the positivity flag (a boolean).\n\nImplementation details and constraints:\n- All simulations must use the fixed parameters $a = 1$, $b = 1$, $c = 1$, $d = 1$ and initial condition $x(0) = 1.5$, $y(0) = 1$.\n- For the high-accuracy reference solution, use an adaptive ODE integrator with tight tolerances (for example, relative tolerance $10^{-12}$ and absolute tolerance $10^{-14}$).\n- For all tests, use uniform time steps exactly dividing the final time so that the number of steps is an integer $N = T/h$.\n- Your program must implement both time steppers as described and compute the requested outputs.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$$\n\\big[ [\\Delta I_{\\text{EE}}, \\text{pos}_{\\text{EE}}], [\\Delta I_{\\text{SE}}, \\text{pos}_{\\text{SE}}], p_{\\text{EE}}, p_{\\text{SE}}, \\text{pos}_{\\text{EE,coarse}} \\big].\n$$\nFor example, a syntactically correct output would look like $[[0.123,True],[0.004,True],0.98,1.01,False]$ (the values shown here are illustrative; your program must compute the actual values). The output contains no units and no angles; booleans are written as in a standard programming language.", "solution": "The problem requires a comparative analysis of two numerical integration schemes, Explicit Euler and Symplectic Euler, for the Lotka-Volterra predator-prey model. The analysis will focus on the fundamental numerical concepts of stability, accuracy, and convergence. The problem statement has been validated and is found to be scientifically sound, well-posed, and complete. It constitutes a standard and instructive exercise in computational science. We shall now proceed with the solution.\n\nThe system is described by the coupled ordinary differential equations (ODEs):\n$$\n\\frac{dx}{dt} = a x - b x y\n$$\n$$\n\\frac{dy}{dt} = -c y + d x y\n$$\nwhere $x(t)$ and $y(t)$ represent the populations of prey and predator, respectively. The parameters are fixed at $a = 1$, $b = 1$, $c = 1$, and $d = 1$. The initial conditions are $x(0) = 1.5$ and $y(0) = 1.0$.\n\nA crucial feature of this system is the existence of a conserved quantity, or first integral, given by the function:\n$$\nI(x,y) = d\\,x - c \\ln x + b\\,y - a \\ln y\n$$\nFor the exact solution of the ODEs, $\\frac{dI}{dt} = 0$, which means that solution trajectories $(x(t), y(t))$ are confined to the level sets of $I(x,y)$. In the phase plane, these level sets form closed orbits around the non-trivial fixed point $(x_c, y_c) = (c/d, a/b)$. For the given parameters, this fixed point is at $(1, 1)$. The conservation of $I(x,y)$ is a key structural property of the system. A desirable numerical method should, at minimum, approximate this conservation property over long integration times.\n\nWe will implement and analyze two first-order numerical schemes.\n\n1.  **Explicit Euler Method**:\nThis is a standard, non-structure-preserving numerical method. Given a state $(x_n, y_n)$ at time $t_n$, the state at $t_{n+1} = t_n + h$ is approximated by a linear extrapolation along the tangent of the vector field:\n$$\nx_{n+1} = x_n + h (a x_n - b x_n y_n)\n$$\n$$\ny_{n+1} = y_n + h (-c y_n + d x_n y_n)\n$$\nThis method is simple to implement but is known to be dissipative or anti-dissipative for oscillatory systems. For Hamiltonian systems, it does not conserve the energy or any related invariant, and the numerical trajectory typically spirals away from the true, closed orbit. Furthermore, the explicit update can lead to non-physical negative populations if the time step $h$ is too large.\n\n2.  **Symplectic Euler Method**:\nThis is a structure-preserving integrator, specifically a geometric integrator designed for Hamiltonian systems. The Lotka-Volterra system can be transformed into a canonical Hamiltonian system. Let $u = \\ln x$ and $v = \\ln y$. The dynamics in these canonical coordinates are governed by the Hamiltonian:\n$$\nH(u,v) = d\\,e^{u} - c\\,u + b\\,e^{v} - a\\,v\n$$\nThe equations of motion are given as $\\dot{u} = -\\partial H / \\partial v = a - b e^v$ and $\\dot{v} = \\partial H / \\partial u = d e^u - c$. A first-order symplectic Euler scheme can be constructed by splitting the update for the conjugate variables. We will use the following variant:\n$$\nv_{n+1} = v_n + h (d e^{u_n} - c)\n$$\n$$\nu_{n+1} = u_n + h (a - b e^{v_{n+1}})\n$$\nAfter each step in $(u,v)$ coordinates, we transform back to the original variables: $x_{n+1} = e^{u_{n+1}}$ and $y_{n+1} = e^{v_{n+1}}$. By construction, this method ensures that $x$ and $y$ remain positive as long as $u$ and $v$ are real, which they will be. Symplectic integrators do not exactly conserve the Hamiltonian $H$; instead, they exactly conserve a nearby \"shadow\" Hamiltonian. This property results in the numerical energy error oscillating around zero, leading to excellent long-term stability and bounded drift of the invariant, in stark contrast to the secular drift observed with non-symplectic methods like Explicit Euler.\n\nThe evaluation of these methods is based on three diagnostics:\n\n-   **Invariant Drift ($\\Delta I$)**: Measures long-term stability. It is defined as the absolute difference $|I(x(T), y(T)) - I(x(0), y(0))|$. For a perfect integrator, $\\Delta I$ would be zero. We expect $\\Delta I$ for the Symplectic Euler method to be significantly smaller than for the Explicit Euler method over long time horizons.\n\n-   **Positivity**: A fundamental physical requirement for population models is that populations remain non-negative. We monitor whether $x_n > 0$ and $y_n > 0$ at all steps. Failure to maintain positivity indicates a catastrophic failure of the numerical scheme.\n\n-   **Empirical Order of Convergence ($p$)**: Measures accuracy. For a method with error $E(h) \\propto h^p$, we can estimate the order $p$ by comparing errors from simulations with two different step sizes, $h$ and $h/2$. The formula is $p = \\log_2(E(h)/E(h/2))$. For the first-order methods being tested, we expect to observe $p \\approx 1$.\n\nThe five specified tests are designed to highlight the differences between these two methods:\n-   Tests $1$ and $2$ compare the long-term drift and stability of Explicit Euler and Symplectic Euler.\n-   Tests $3$ and $4$ numerically verify the first-order convergence of both schemes.\n-   Test $5$ demonstrates the instability of Explicit Euler under a coarse time step, a situation where the explicit update can overshoot and violate positivity.\n\nThe following code implements these tests and computes the required numerical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the Lotka-Volterra problem by comparing Explicit and Symplectic Euler methods.\n    \"\"\"\n    # Fixed parameters and initial conditions\n    a, b, c, d = 1.0, 1.0, 1.0, 1.0\n    x0, y0 = 1.5, 1.0\n    state0 = np.array([x0, y0])\n\n    # === Helper Functions ===\n\n    def lotka_volterra_rhs(t, state, a, b, c, d):\n        \"\"\"Right-hand side of the Lotka-Volterra ODEs.\"\"\"\n        x, y = state\n        dxdt = a * x - b * x * y\n        dydt = -c * y + d * x * y\n        return np.array([dxdt, dydt])\n\n    def invariant(state, a, b, c, d):\n        \"\"\"Computes the conserved quantity I(x, y).\"\"\"\n        x, y = state\n        if x = 0 or y = 0:\n            return float('nan')\n        return d * x - c * np.log(x) + b * y - a * np.log(y)\n\n    def explicit_euler_step(state, h, a, b, c, d):\n        \"\"\"Performs one step of the Explicit Euler method.\"\"\"\n        x, y = state\n        x_new = x + h * (a * x - b * x * y)\n        y_new = y + h * (-c * y + d * x * y)\n        return np.array([x_new, y_new])\n\n    def symplectic_euler_step(state, h, a, b, c, d):\n        \"\"\"Performs one step of the Symplectic Euler method.\"\"\"\n        x, y = state\n        u, v = np.log(x), np.log(y)\n        v_new = v + h * (d * np.exp(u) - c)\n        u_new = u + h * (a - b * np.exp(v_new))\n        return np.array([np.exp(u_new), np.exp(v_new)])\n\n    def time_integrator(stepper_fn, state0, h, T, params):\n        \"\"\"\n        Integrates the ODEs using a given stepper function.\n        Returns the final state and a positivity flag.\n        \"\"\"\n        state = np.copy(state0)\n        positivity = True\n        num_steps = int(round(T / h))\n        \n        for _ in range(num_steps):\n            state = stepper_fn(state, h, *params)\n            if state[0] = 0 or state[1] = 0:\n                positivity = False\n                break\n        return state, positivity\n\n    # Initial invariant value\n    I0 = invariant(state0, a, b, c, d)\n    \n    results = []\n\n    # === Test 1: Explicit Euler, long-horizon drift and positivity ===\n    h1 = 0.05\n    T1 = 50.0\n    final_state_ee, pos_ee = time_integrator(explicit_euler_step, state0, h1, T1, (a, b, c, d))\n    if pos_ee:\n        I_final_ee = invariant(final_state_ee, a, b, c, d)\n        delta_I_ee = abs(I_final_ee - I0)\n    else:\n        delta_I_ee = float('nan')\n    results.append([delta_I_ee, pos_ee])\n\n    # === Test 2: Symplectic Euler, long-horizon drift and positivity ===\n    h2 = 0.05\n    T2 = 50.0\n    final_state_se, pos_se = time_integrator(symplectic_euler_step, state0, h2, T2, (a, b, c, d))\n    if pos_se:\n        I_final_se = invariant(final_state_se, a, b, c, d)\n        delta_I_se = abs(I_final_se - I0)\n    else:\n        delta_I_se = float('nan')\n    results.append([delta_I_se, pos_se])\n\n    # === Setup for Convergence Tests ===\n    T_conv = 10.0\n    h_conv = 0.1\n    \n    # High-accuracy reference solution\n    ref_sol = solve_ivp(\n        lotka_volterra_rhs, [0, T_conv], state0,\n        args=(a, b, c, d),\n        method='DOP853',\n        rtol=1e-12,\n        atol=1e-14\n    )\n    ref_state_T = ref_sol.y[:, -1]\n\n    # === Test 3: Empirical order for Explicit Euler ===\n    state_h_ee, _ = time_integrator(explicit_euler_step, state0, h_conv, T_conv, (a, b, c, d))\n    state_h2_ee, _ = time_integrator(explicit_euler_step, state0, h_conv / 2, T_conv, (a, b, c, d))\n    \n    error_h_ee = np.linalg.norm(state_h_ee - ref_state_T)\n    error_h2_ee = np.linalg.norm(state_h2_ee - ref_state_T)\n    \n    p_ee = np.log2(error_h_ee / error_h2_ee) if error_h2_ee  0 else float('nan')\n    results.append(p_ee)\n\n    # === Test 4: Empirical order for Symplectic Euler ===\n    state_h_se, _ = time_integrator(symplectic_euler_step, state0, h_conv, T_conv, (a, b, c, d))\n    state_h2_se, _ = time_integrator(symplectic_euler_step, state0, h_conv / 2, T_conv, (a, b, c, d))\n\n    error_h_se = np.linalg.norm(state_h_se - ref_state_T)\n    error_h2_se = np.linalg.norm(state_h2_se - ref_state_T)\n\n    p_se = np.log2(error_h_se / error_h2_se) if error_h2_se  0 else float('nan')\n    results.append(p_se)\n\n    # === Test 5: Explicit Euler, edge-case stability ===\n    h5 = 0.5\n    T5 = 200.0\n    _, pos_ee_coarse = time_integrator(explicit_euler_step, state0, h5, T5, (a, b, c, d))\n    results.append(pos_ee_coarse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2378425"}, {"introduction": "Numerical instability is not just a product of time-stepping; it can also arise from how we discretize space, especially when dealing with nonlinear Partial Differential Equations (PDEs). This exercise investigates aliasing instability, a phenomenon where nonlinear interactions create high-frequency information that the computational grid is too coarse to represent accurately, leading to catastrophic errors. In this coding challenge [@problem_id:2378405], you will use a Fourier pseudo-spectral method to solve the inviscid Burgers' equation and directly observe how failing to adequately resolve the solution in space causes an explosive, unphysical growth in the system's energy. This provides a powerful, practical illustration of the Nyquist-Shannon sampling theorem's importance in the context of computational stability.", "problem": "Consider the one-dimensional inviscid Burgers equation on a periodic domain, defined by the partial differential equation $$u_t(x,t) + \\partial_x \\left(\\frac{1}{2} u(x,t)^2\\right) = 0$$ on the interval $x \\in [0, 2\\pi]$ with angles in radians, and time $t \\ge 0$. The initial condition is a single Fourier mode $u(x,0) = A \\sin(m x)$ where $A > 0$ is the amplitude and $m \\in \\mathbb{Z}$ is the wavenumber. The exact solution for smooth regimes conserves the quadratic invariant $E(t) = \\frac{1}{2} \\int_{0}^{2\\pi} u(x,t)^2 \\, dx$ for as long as the solution remains smooth. In a Fourier pseudo-spectral discretization using a discrete Fourier transform with a finite number of grid points $N \\in \\mathbb{N}$, evaluating nonlinear products pointwise in physical space and transforming back to spectral space introduces aliasing. When the initial mode has fewer than two grid points per wavelength, that is $N / m  2$, the discretization necessarily aliases high-frequency content into lower wavenumbers. This can manifest as a numerical instability that amplifies discrete energy.\n\nImplement a program that performs the following for each test case:\n\n1. Use $N$ equispaced grid points on $[0, 2\\pi)$ with grid spacing $\\Delta x = 2\\pi / N$.\n2. Discretize space with a Fourier pseudo-spectral method using the Fast Fourier Transform (FFT). The spectral derivative is computed by multiplying Fourier coefficients by $ik$, where $k$ are the angular wavenumbers returned by $2\\pi \\cdot \\mathrm{fftfreq}(N, \\Delta x)$, with no de-aliasing.\n3. Discretize time with the classical fourth-order Runge–Kutta method. Use an adaptive time step $\\Delta t_n = c \\, \\Delta x / \\max_x |u^n(x)|$ with $c = 0.2$, updating $\\Delta t_n$ at each step, and ensure the final time is exactly $T$ by truncating the last step as needed. If $\\max_x |u^n(x)| = 0$, take $\\Delta t_n = T - t_n$.\n4. Evolve from $t=0$ to $t=T$ and compute the discrete energy amplification factor $R = E(T)/E(0)$ using the trapezoidal rule on the uniform grid, that is $E(t) = \\frac{1}{2} \\Delta x \\sum_{j=0}^{N-1} u(x_j,t)^2$.\n\nUse the fixed parameters $A = 0.1$ and $T = 0.1$ for all cases. The test suite consists of the following four cases, given as $(N, m)$ pairs:\n- Case $1$ (well-resolved, many points per wavelength): $(256, 16)$.\n- Case $2$ (Nyquist boundary, exactly two points per wavelength): $(64, 32)$.\n- Case $3$ (under-resolved, fewer than two points per wavelength): $(64, 40)$.\n- Case $4$ (severely under-resolved): $(32, 28)$.\n\nYour program must output a single line containing a list of four decimal numbers corresponding to $R$ for Cases $1$ through $4$ in order, each rounded to exactly $6$ digits after the decimal point. The required format is a single line:\n$[r_1,r_2,r_3,r_4]$\nwhere each $r_j$ is the rounded value of $R$ for Case $j$. No other text should be printed. The answer is dimensionless; do not include any units.", "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The problem addresses the numerical solution of the inviscid Burgers' equation using a Fourier pseudo-spectral method, which is a standard topic in computational engineering, specifically focusing on the concept of aliasing-induced numerical instability.\n\nThe governing partial differential equation (PDE) is the one-dimensional inviscid Burgers' equation in conservation form:\n$$\nu_t(x,t) + \\partial_x \\left(\\frac{1}{2} u(x,t)^2\\right) = 0\n$$\nThis equation is defined on a periodic spatial domain $x \\in [0, 2\\pi]$ with time $t \\ge 0$. The initial condition is a single sinusoidal mode:\n$$\nu(x,0) = A \\sin(m x)\n$$\nwhere the amplitude is $A = 0.1$ and the wavenumber $m$ is an integer. For the exact solution, the total energy, defined by the quadratic invariant $E(t) = \\frac{1}{2} \\int_{0}^{2\\pi} u(x,t)^2 \\, dx$, is conserved as long as the solution remains smooth (i.e., before shock formation). In all test cases, the final time $T=0.1$ is less than the shock formation time $t_{shock} = 1/(Am)$, so any change in the discrete energy is a numerical artifact.\n\nThe solution is obtained by first semi-discretizing the PDE in space, which converts the PDE into a system of ordinary differential equations (ODEs), and then integrating this system in time.\n\n**Spatial Discretization: Fourier Pseudo-spectral Method**\nThe spatial domain $[0, 2\\pi)$ is discretized using $N$ equispaced grid points $x_j = j \\Delta x$ for $j = 0, 1, \\dots, N-1$, where the grid spacing is $\\Delta x = 2\\pi/N$. The solution $u(x,t)$ is represented by a vector of its values at these grid points, $\\vec{u}(t) = [u(x_0,t), u(x_1,t), \\dots, u(x_{N-1},t)]^T$.\n\nThe semi-discretized form of the equation is:\n$$\n\\frac{d\\vec{u}}{dt} = \\vec{F}(\\vec{u})\n$$\nwhere $\\vec{F}$ is the discrete representation of the spatial operator $-\\partial_x \\left(\\frac{1}{2} u^2\\right)$. A pseudo-spectral method (also known as the collocation method) is used to compute $\\vec{F}(\\vec{u})$. The procedure for a given state vector $\\vec{u}$ is as follows:\n1.  Compute the nonlinear flux term $f(\\vec{u}) = \\frac{1}{2} \\vec{u}^2$ pointwise in physical space. This is an element-wise squaring operation.\n2.  Transform the flux vector to Fourier space using the Fast Fourier Transform (FFT): $\\widehat{f} = \\mathrm{FFT}(f)$.\n3.  Compute the spatial derivative in Fourier space. This is achieved by multiplying each Fourier coefficient $\\widehat{f}_k$ by $ik$, where $k$ are the discrete angular wavenumbers. The wavenumbers are given by $k = 2\\pi \\cdot \\mathrm{fftfreq}(N, \\Delta x)$, yielding integer wavenumbers for the $[0, 2\\pi]$ domain. The derivative in Fourier space is $\\widehat{f'}_k = ik \\widehat{f}_k$.\n4.  Transform the result back to physical space using the inverse FFT (IFFT): $f' = \\mathrm{IFFT}(\\widehat{f'})$.\n5.  The right-hand side of the ODE system is then $\\vec{F}(\\vec{u}) = -f'$.\n\nThis method is highly accurate for smooth functions. However, the nonlinear step $f(\\vec{u}) = \\frac{1}{2} \\vec{u}^2$ can create higher-frequency modes. If the initial data contains modes up to wavenumber $m$, the quadratic term will generate modes up to wavenumber $2m$. If $2m$ exceeds the Nyquist wavenumber of the grid, $k_{Nyquist} = N/2$, these high-frequency components are incorrectly represented as lower-frequency modes, a phenomenon known as aliasing. The problem specifies that no de-aliasing technique is to be used, which means this aliasing error will corrupt the solution and can lead to unphysical energy growth, i.e., numerical instability.\n\n**Temporal Discretization: Fourth-Order Runge-Kutta (RK4) Method**\nThe system of ODEs, $\\frac{d\\vec{u}}{dt} = \\vec{F}(\\vec{u})$, is advanced in time using the classical fourth-order Runge-Kutta method. Given the solution $\\vec{u}^n$ at time $t_n$, the solution at the next time step $t_{n+1} = t_n + \\Delta t_n$ is computed as:\n$$\n\\begin{aligned}\n    \\vec{k}_1 = \\vec{F}(\\vec{u}^n) \\\\\n    \\vec{k}_2 = \\vec{F}(\\vec{u}^n + \\frac{\\Delta t_n}{2} \\vec{k}_1) \\\\\n    \\vec{k}_3 = \\vec{F}(\\vec{u}^n + \\frac{\\Delta t_n}{2} \\vec{k}_2) \\\\\n    \\vec{k}_4 = \\vec{F}(\\vec{u}^n + \\Delta t_n \\vec{k}_3) \\\\\n    \\vec{u}^{n+1} = \\vec{u}^n + \\frac{\\Delta t_n}{6} (\\vec{k}_1 + 2\\vec{k}_2 + 2\\vec{k}_3 + \\vec{k}_4)\n\\end{aligned}\n$$\nThe time step $\\Delta t_n$ is adaptive, based on a CFL-like condition: $\\Delta t_n = c \\frac{\\Delta x}{\\max_j |u_j^n|}$, with a constant $c = 0.2$. This step size is recomputed at the beginning of each time step. To ensure the simulation ends precisely at time $T=0.1$, the final time step is truncated if necessary.\n\n**Energy Calculation**\nThe discrete energy at a given time $t$ is computed using the trapezoidal rule approximation of the integral on the uniform periodic grid:\n$$\nE(t) = \\frac{1}{2} \\Delta x \\sum_{j=0}^{N-1} u(x_j, t)^2\n$$\nThe objective is to compute the energy amplification factor $R = E(T) / E(0)$. For a stable, well-resolved numerical simulation, we expect $R \\approx 1$. For the under-resolved cases where aliasing instability occurs, we expect $R  1$.\n\nThe four test cases explore different resolution regimes:\n-   Case 1 ($N=256, m=16$): Well-resolved ($N/m = 16 \\gg 2$). The highest generated wavenumber is $2m = 32$, which is well below the Nyquist limit $N/2=128$. Aliasing is negligible; $R$ should be very close to $1$.\n-   Case 2 ($N=64, m=32$): Nyquist-limited ($N/m = 2$). The initial mode is at the Nyquist frequency. The nonlinear term generates a mode $2m=64$, which aliases to wavenumber $k=0$ on this grid. This will affect accuracy, but may not cause explosive growth.\n-   Case 3 ($N=64, m=40$): Under-resolved ($N/m = 1.6  2$). The initial mode $m=40$ is aliased to wavenumber $40 - 64 = -24$. The nonlinear interactions will be complex and are expected to cause energy growth, leading to $R  1$.\n-   Case 4 ($N=32, m=28$): Severely under-resolved ($N/m \\approx 1.14  2$). The initial mode $m=28$ is aliased to $28 - 32 = -4$. This severe under-resolution is expected to produce significant aliasing-induced instability and a value of $R$ substantially greater than $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 1D inviscid Burgers' equation using a Fourier pseudo-spectral method\n    and RK4 time integration to demonstrate aliasing instability.\n    \"\"\"\n    \n    # Fixed parameters for all test cases\n    A = 0.1\n    T = 0.1\n    CFL_CONST = 0.2\n\n    # Test cases: (N, m) pairs\n    test_cases = [\n        (256, 16),  # Case 1: Well-resolved\n        (64, 32),   # Case 2: Nyquist boundary\n        (64, 40),   # Case 3: Under-resolved\n        (32, 28),   # Case 4: Severely under-resolved\n    ]\n\n    results = []\n    for N, m in test_cases:\n        # 1. Setup grid, initial condition, and wavenumbers\n        dx = 2 * np.pi / N\n        x = np.arange(N) * dx\n        # Angular wavenumbers\n        k = 2 * np.pi * np.fft.fftfreq(N, d=dx)\n        \n        u = A * np.sin(m * x)\n        t = 0.0\n\n        # Function to compute the RHS of the semi-discretized equation:\n        # d(u)/dt = -d/dx(0.5 * u^2)\n        def rhs(u_vec):\n            # Compute flux in physical space\n            flux = 0.5 * u_vec**2\n            # Transform to Fourier space\n            flux_hat = np.fft.fft(flux)\n            # Differentiate in Fourier space\n            d_flux_hat = 1j * k * flux_hat\n            # Transform back to physical space. np.fft.ifft handles 1/N normalization.\n            d_flux = np.fft.ifft(d_flux_hat)\n            # The result should be real; discard small imaginary parts from numerical error\n            return -d_flux.real\n\n        # 2. Compute initial discrete energy\n        E0 = 0.5 * dx * np.sum(u**2)\n        \n        if E0 == 0.0:\n            # If initial energy is zero, amplification is undefined or 1.\n            # This case won't be hit with the given parameters.\n            results.append(1.0)\n            continue\n            \n        # 3. Time integration loop\n        while t  T:\n            # Adaptive time step based on CFL-like condition\n            u_max = np.max(np.abs(u))\n            if u_max == 0.0:\n                dt = T - t  # Take one step to the end\n            else:\n                dt = CFL_CONST * dx / u_max\n            \n            # Truncate the last step to end exactly at T\n            if t + dt  T:\n                dt = T - t\n            \n            # Classical 4th-order Runge-Kutta step\n            k1 = rhs(u)\n            k2 = rhs(u + 0.5 * dt * k1)\n            k3 = rhs(u + 0.5 * dt * k2)\n            k4 = rhs(u + dt * k3)\n            u = u + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n            \n            t += dt\n\n        # 4. Compute final energy and amplification factor\n        ET = 0.5 * dx * np.sum(u**2)\n        R = ET / E0\n        results.append(R)\n\n    # Final print statement in the exact required format.\n    # Each value is rounded to exactly 6 digits after the decimal point.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2378405"}]}