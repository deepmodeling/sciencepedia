{"hands_on_practices": [{"introduction": "Before we can use error norms to analyze our simulations, it is crucial to understand their practical implications. Different norms measure error in fundamentally different ways, and this also translates to different computational costs. This exercise explores the computational complexity of calculating the $L_\\infty$, $L_2$, and $H^1$ norms in the context of the Finite Element Method (FEM). By analyzing how the cost scales with the problem size, you will gain insight into the efficiency of these metrics and appreciate the benefits of the local computational structure inherent to FEM [@problem_id:2389331].", "problem": "Consider a scalar field $u_h$ represented in a conforming finite element method (FEM) discretization on a bounded domain $\\Omega \\subset \\mathbb{R}^d$ with $N$ global scalar degrees of freedom (DOF). Assume a fixed spatial dimension $d$, a fixed polynomial degree $p$ of the local finite element basis, and a shape-regular family of meshes. Let $M$ denote the number of elements, and assume $M$ scales linearly with $N$ due to fixed $p$ and $d$. The following discrete norms are to be computed:\n\n- The discrete $L_{\\infty}$ norm computed as the maximum absolute value over all nodal coefficients: $\\|u_h\\|_{L_{\\infty},\\text{disc}} = \\max_{i \\in \\{1,\\dots,N\\}} |u_i|$.\n\n- The $L_2$ norm computed by element-wise quadrature: $\\|u_h\\|_{L_2(\\Omega)} = \\left( \\int_{\\Omega} u_h(x)^2 \\,\\mathrm{d}x \\right)^{1/2}$, where the element integrals are evaluated using a fixed quadrature rule with $Q$ points per element, with $Q$ independent of $N$.\n\n- The $H^1$ norm computed by element-wise quadrature: $\\|u_h\\|_{H^1(\\Omega)} = \\left( \\int_{\\Omega} \\left[u_h(x)^2 + \\|\\nabla u_h(x)\\|^2 \\right] \\,\\mathrm{d}x \\right)^{1/2}$, using the same fixed quadrature rule. On each element, $u_h$ and $\\nabla u_h$ are evaluated at quadrature points as linear combinations of the local basis functions and their gradients with $n_{\\text{loc}}$ local DOF per element, where $n_{\\text{loc}}$ depends only on $p$ and $d$ and is independent of $N$.\n\nAdopt the following cost model:\n\n- Count floating-point additions and multiplications; ignore constant factors and lower-order terms.\n\n- Evaluation of basis functions and their gradients at quadrature points is assumed precomputed and reused, so the per-quadrature-point evaluation of $u_h$ or $\\nabla u_h$ costs a constant number of operations proportional to $n_{\\text{loc}}$, independent of $N$.\n\n- The cost of a final square root operation is counted as a constant $O(1)$ cost.\n\nUnder these assumptions, determine the asymptotic exponents $\\alpha_{\\infty}$, $\\alpha_{2}$, and $\\alpha_{H^1}$ such that the computational costs to evaluate the above $L_{\\infty}$, $L_2$, and $H^1$ norms, respectively, scale as $\\Theta(N^{\\alpha_{\\infty}})$, $\\Theta(N^{\\alpha_{2}})$, and $\\Theta(N^{\\alpha_{H^1}})$.\n\nProvide your final answer as a single row matrix $\\begin{pmatrix} \\alpha_{\\infty} & \\alpha_{2} & \\alpha_{H^1} \\end{pmatrix}$. No rounding is required.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded question in the field of computational engineering, specifically concerning the complexity analysis of finite element method (FEM) computations. All parameters and assumptions are clearly defined and consistent with standard literature on the topic. We will now proceed with the solution.\n\nThe objective is to determine the asymptotic computational cost for calculating three norms of a finite element solution $u_h$ as a function of the number of global degrees of freedom, $N$. The costs are to be expressed in the form $\\Theta(N^{\\alpha})$, and we must find the exponents $\\alpha_{\\infty}$, $\\alpha_{2}$, and $\\alpha_{H^1}$.\n\nLet us analyze each norm computation separately under the provided cost model.\n\n**1. Cost of the Discrete $L_{\\infty}$ Norm**\n\nThe discrete $L_{\\infty}$ norm is defined as:\n$$\n\\|u_h\\|_{L_{\\infty},\\text{disc}} = \\max_{i \\in \\{1,\\dots,N\\}} |u_i|\n$$\nHere, $\\{u_i\\}_{i=1}^N$ represents the vector of $N$ global scalar degrees of freedom (nodal coefficients). The computation involves two steps:\n1.  Calculating the absolute value $|u_i|$ for each of the $N$ coefficients. This requires $N$ operations.\n2.  Finding the maximum value among these $N$ positive numbers. This can be done by iterating through the list and keeping track of the maximum value found so far, which requires $N-1$ comparisons.\n\nThe total number of operations is a sum of terms linear in $N$. According to the cost model, which ignores constant factors and lower-order terms, the computational cost is dominated by the linear dependence on $N$. Therefore, the cost is $\\Theta(N)$.\nThis implies that the exponent $\\alpha_{\\infty}$ is $1$.\n$$\n\\alpha_{\\infty} = 1\n$$\n\n**2. Cost of the $L_2$ Norm**\n\nThe $L_2$ norm is computed via numerical quadrature over the domain $\\Omega$. The square of the norm is given by:\n$$\n\\|u_h\\|_{L_2(\\Omega)}^2 = \\int_{\\Omega} u_h(x)^2 \\,\\mathrm{d}x\n$$\nSince the integral is computed element-wise, we can write this as a sum over the $M$ elements $\\{K_e\\}_{e=1}^M$ of the mesh:\n$$\n\\|u_h\\|_{L_2(\\Omega)}^2 = \\sum_{e=1}^{M} \\int_{K_e} u_h(x)^2 \\,\\mathrm{d}x\n$$\nFor each element $K_e$, the integral is approximated using a quadrature rule with a fixed number of points, $Q$. Let the quadrature points be $\\{x_{q}\\}_{q=1}^Q$ and the corresponding weights be $\\{w_{q}\\}_{q=1}^Q$ on a reference element. The integral over element $K_e$ is then:\n$$\n\\int_{K_e} u_h(x)^2 \\,\\mathrm{d}x \\approx \\sum_{q=1}^{Q} u_h(x_{e,q})^2 \\, w_{e,q}\n$$\nwhere $x_{e,q}$ are the quadrature points mapped to element $K_e$ and $w_{e,q}$ are the corresponding weights incorporating the Jacobian of the transformation.\n\nLet us analyze the cost for a single element $K_e$. The computation involves a loop over the $Q$ quadrature points.\n- For each quadrature point $q \\in \\{1, \\dots, Q\\}$:\n  1.  Evaluate $u_h(x_{e,q})$. The problem states that this evaluation has a cost proportional to $n_{\\text{loc}}$, the number of local degrees of freedom. Since the polynomial degree $p$ and spatial dimension $d$ are fixed, $n_{\\text{loc}}$ is a constant independent of $N$. Thus, this evaluation has a constant cost, let us say $C_1$.\n  2.  Square the result $u_h(x_{e,q})^2$. This is $1$ multiplication.\n  3.  Multiply by the quadrature weight $w_{e,q}$. This is $1$ multiplication.\n  4.  Sum the result into a running total. This is $1$ addition.\n\nThe cost per quadrature point is constant. Since there are $Q$ points and $Q$ is a fixed number independent of $N$, the total cost to compute the integral over one element is also a constant, let us say $C_{\\text{elem}, 2}$.\n\nThe total cost to compute the sum $\\|u_h\\|_{L_2(\\Omega)}^2$ is the cost per element multiplied by the number of elements $M$:\n$$\n\\text{Cost}_{L_2^2} = M \\times C_{\\text{elem}, 2}\n$$\nThe problem states that for a fixed polynomial degree $p$ and a shape-regular family of meshes, the number of elements $M$ scales linearly with the number of degrees of freedom $N$. That is, $M = \\Theta(N)$.\nTherefore, the cost to compute the sum of squares is $\\Theta(N)$.\n\nFinally, the norm itself is obtained by taking the square root of the sum, which is specified to be an $O(1)$ operation. This does not change the overall asymptotic complexity.\nThe total cost is $\\Theta(N)$. This implies that the exponent $\\alpha_2$ is $1$.\n$$\n\\alpha_{2} = 1\n$$\n\n**3. Cost of the $H^1$ Norm**\n\nThe analysis for the $H^1$ norm follows a similar logic. The square of the $H^1$ norm is defined as:\n$$\n\\|u_h\\|_{H^1(\\Omega)}^2 = \\int_{\\Omega} \\left( u_h(x)^2 + \\|\\nabla u_h(x)\\|^2 \\right) \\,\\mathrm{d}x\n$$\nwhere $\\|\\nabla u_h(x)\\|^2$ is the squared Euclidean norm of the gradient vector. This integral is also computed via a sum over the $M$ elements and approximated using the same quadrature rule:\n$$\n\\|u_h\\|_{H^1(\\Omega)}^2 = \\sum_{e=1}^{M} \\int_{K_e} \\left( u_h(x)^2 + \\|\\nabla u_h(x)\\|^2 \\right) \\,\\mathrm{d}x \\approx \\sum_{e=1}^{M} \\sum_{q=1}^{Q} \\left( u_h(x_{e,q})^2 + \\|\\nabla u_h(x_{e,q})\\|^2 \\right) w_{e,q}\n$$\nLet us analyze the cost for a single element $K_e$.\n- For each quadrature point $q \\in \\{1, \\dots, Q\\}$:\n  1.  Evaluate $u_h(x_{e,q})$. This has a constant cost $C_1$.\n  2.  Evaluate the gradient vector $\\nabla u_h(x_{e,q})$. The problem states this also has a constant cost, say $C_2$, because it is proportional to $n_{\\text{loc}}$.\n  3.  Compute $u_h(x_{e,q})^2$. This is $1$ multiplication.\n  4.  Compute $\\|\\nabla u_h(x_{e,q})\\|^2$. The gradient is a vector in $\\mathbb{R}^d$. Computing its squared norm involves $d$ multiplications and $d-1$ additions. Since the spatial dimension $d$ is fixed, this is a constant number of operations.\n  5.  Add the two squared values: $u_h^2 + \\|\\nabla u_h\\|^2$. This is $1$ addition.\n  6.  Multiply by the quadrature weight $w_{e,q}$. This is $1$ multiplication.\n  7.  Sum the result into a running total. This is $1$ addition.\n\nAgain, the total cost per quadrature point is constant because $d$, $Q$, and $n_{\\text{loc}}$ are all fixed constants. The total cost to process one element, $C_{\\text{elem}, H^1}$, is therefore also constant.\n\nThe total cost to compute the sum $\\|u_h\\|_{H^1(\\Omega)}^2$ is the cost per element multiplied by the number of elements, $M$:\n$$\n\\text{Cost}_{H^{1,2}} = M \\times C_{\\text{elem}, H^1}\n$$\nUsing the given scaling $M = \\Theta(N)$, the cost to compute the sum is $\\Theta(N)$. The final square root operation is $O(1)$ and does not alter the asymptotic scaling.\nThe total cost is $\\Theta(N)$. This implies that the exponent $\\alpha_{H^1}$ is $1$.\n$$\n\\alpha_{H^1} = 1\n$$\n\nIn summary, all three norm computations have a computational cost that scales linearly with the number of degrees of freedom $N$. The linear scaling for the integral norms arises from the fact that the computation is local (element-wise), the cost per element is constant, and the total number of elements is proportional to $N$. The linear scaling for the discrete maximum norm is direct from its definition.\n\nThe resulting exponents are $\\alpha_{\\infty} = 1$, $\\alpha_{2} = 1$, and $\\alpha_{H^1} = 1$. The final answer is presented as the required row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & 1\n\\end{pmatrix}\n}\n$$", "id": "2389331"}, {"introduction": "One of the most vital tasks in computational science is verifying that a numerical method performs as theoretically expected, and a key part of this is measuring its order of convergence, $p$. This practice guides you through the standard procedure for experimentally determining $p$ by leveraging the asymptotic relationship $\\|e_h\\| \\approx C h^p$. By analyzing error norms computed at different mesh resolutions on a log-log scale, you can robustly estimate the convergence rate, a fundamental skill for validating code and ensuring your simulations produce reliable results [@problem_id:2389343].", "problem": "You are given a one-dimensional setting on the closed interval $[0,1]$ with a uniform grid of $N+1$ nodes $x_i = i h$, where $h = 1/N$ and $i \\in \\{0,1,\\dots,N\\}$. The exact smooth function is $u(x) = \\sin(2\\pi x)$. For a numerical method with mesh size $h$, denote the pointwise error by $e_h(x) = u_h(x) - u(x)$. You will use error norms to automatically detect the order of convergence $p$ of the method under the following assumptions.\n\nFundamental base:\n- Discrete $L^1$, $L^2$, and $L^\\infty$ norms on a uniform grid approximate their continuous counterparts using Riemann sums. For a vector of errors $e_i \\approx e_h(x_i)$,\n  - the discrete $L^1$ norm is $\\|e_h\\|_{1,h} = h \\sum_{i=0}^N |e_i|$,\n  - the discrete $L^2$ norm is $\\|e_h\\|_{2,h} = \\sqrt{h \\sum_{i=0}^N e_i^2}$,\n  - the discrete $L^\\infty$ norm is $\\|e_h\\|_{\\infty,h} = \\max_{0 \\le i \\le N} |e_i|$.\n- In the asymptotic regime of sufficiently small $h$, many consistent numerical methods exhibit an error-norm scaling of the form $\\|e_h\\| \\approx C h^p$ for some constants $C > 0$ and $p > 0$ that do not depend on $h$.\n\nTask:\n- Starting from the definitions above and the asymptotic scaling assumption, derive a principled estimator for the order of convergence $p$ that uses only error norms computed at multiple mesh sizes $h$. Your estimator must not assume knowledge of $C$ and must be robust to small higher-order contamination terms in the error.\n- Implement a complete program that:\n  1. Constructs a family of approximations $u_h(x)$ for a set of mesh sizes $h$ and computes the corresponding error norms.\n  2. Applies your estimator to obtain $\\widehat{p}$ from the finest levels in each case.\n  3. Aggregates the results for a given test suite and prints them in the required final output format.\n\nSynthetic “unknown” method model:\n- For each mesh size $h$, the approximate solution is defined as\n  $$u_h(x) = u(x) + C\\,h^p\\,w(x) + \\delta(x;h),$$\n  where $w(x)$ is a smooth “error shape” and $\\delta(x;h)$ is an optional higher-order contamination term modeling pre-asymptotic effects. Both $w(x)$ and $\\delta(x;h)$ are fully specified in the test suite below, but your estimator must not use any prior knowledge of $C$, $p$, $w(x)$, or $\\delta(x;h)$ beyond computing norms of $e_h(x)$.\n\nNorms to compute:\n- For each test case and each $h$, compute the discrete norm $\\|e_h\\|_{q,h}$ corresponding to the specified choice of $q \\in \\{1,2,\\infty\\}$ with the definitions given above, using the uniform grid with $N+1$ points and $h = 1/N$.\n\nEstimator application rule:\n- For each test case, estimate $p$ by applying your estimator to the three finest mesh sizes (i.e., the three smallest $h$ values in that case).\n\nTest suite:\n- Use the following mesh sizes in each case: $N \\in \\{10,20,40,80,160\\}$, so that $h \\in \\{1/10,1/20,1/40,1/80,1/160\\}$.\n- Case $1$ (happy path): $p = 2$, $C = 0.7$, $w(x) = \\cos(\\pi x)$, $\\delta(x;h) \\equiv 0$, norm $L^2$.\n- Case $2$ (different norm): $p = 1$, $C = 1.5$, $w(x) = e^{x}$, $\\delta(x;h) \\equiv 0$, norm $L^1$.\n- Case $3$ (fractional order and $L^\\infty$): $p = 1.5$, $C = 0.3$, $w(x) = \\sin(3\\pi x)$, $\\delta(x;h) \\equiv 0$, norm $L^\\infty$.\n- Case $4$ (pre-asymptotic contamination): $p = 3$, $C = 0.2$, $w(x) = \\cos(5\\pi x)$, $\\delta(x;h) = D\\,h^{p+1}\\,\\sin(7\\pi x)$ with $D = 5.0$, norm $L^2$.\n\nAlgorithmic requirements:\n- Use only the norm values and mesh sizes to estimate $p$ for each case.\n- To mitigate pre-asymptotic effects, apply your estimator only to the finest three mesh sizes, i.e., $N \\in \\{40,80,160\\}$, in every case.\n\nFinal output specification:\n- Your program must produce a single line of output containing a list with the four estimated orders of convergence, in the order of Cases $1$ through $4$, each rounded to $3$ decimal places. The format must be exactly a comma-separated list enclosed in square brackets, for example, $[a,b,c,d]$, where $a$, $b$, $c$, and $d$ are decimal numerals.\n\nNo physical units are involved. All angles are in radians. The final answer must be produced without requiring any user input or external files.", "solution": "The problem as stated is subjected to validation.\n\nStep 1: Extract Givens\n- Domain: Closed interval $[0,1]$.\n- Grid: Uniform grid of $N+1$ nodes $x_i = i h$, where $h = 1/N$ and $i \\in \\{0, 1, \\dots, N\\}$.\n- Exact function: $u(x) = \\sin(2\\pi x)$.\n- Pointwise error: $e_h(x) = u_h(x) - u(x)$.\n- Discrete $L^1$ norm: $\\|e_h\\|_{1,h} = h \\sum_{i=0}^N |e_i|$.\n- Discrete $L^2$ norm: $\\|e_h\\|_{2,h} = \\sqrt{h \\sum_{i=0}^N e_i^2}$.\n- Discrete $L^\\infty$ norm: $\\|e_h\\|_{\\infty,h} = \\max_{0 \\le i \\le N} |e_i|$.\n- Asymptotic error scaling: $\\|e_h\\| \\approx C h^p$ for constants $C > 0$ and $p > 0$.\n- Synthetic approximation model: $u_h(x) = u(x) + C\\,h^p\\,w(x) + \\delta(x;h)$.\n- Mesh sizes for analysis: $N \\in \\{10, 20, 40, 80, 160\\}$, which implies $h \\in \\{1/10, 1/20, 1/40, 1/80, 1/160\\}$.\n- Estimator application rule: Use the three finest mesh sizes, corresponding to $N \\in \\{40, 80, 160\\}$.\n- Test Case 1: $p = 2$, $C = 0.7$, $w(x) = \\cos(\\pi x)$, $\\delta(x;h) \\equiv 0$, norm is $L^2$.\n- Test Case 2: $p = 1$, $C = 1.5$, $w(x) = e^{x}$, $\\delta(x;h) \\equiv 0$, norm is $L^1$.\n- Test Case 3: $p = 1.5$, $C = 0.3$, $w(x) = \\sin(3\\pi x)$, $\\delta(x;h) \\equiv 0$, norm is $L^\\infty$.\n- Test Case 4: $p = 3$, $C = 0.2$, $w(x) = \\cos(5\\pi x)$, $\\delta(x;h) = D\\,h^{p+1}\\,\\sin(7\\pi x)$ with $D = 5.0$, norm is $L^2$. All angles are in radians.\n\nStep 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is based on fundamental concepts of numerical analysis, specifically the study of convergence rates for numerical methods, which is a standard topic in computational engineering and applied mathematics. The definitions and models are standard. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information—the exact function, the model for the numerical error, the definitions of norms, the mesh sizes to use, and a clear objective. The task is to derive and apply an estimator, for which a unique and meaningful solution exists.\n- **Objective**: The language is precise and quantitative. There are no subjective or opinion-based statements. All parameters are explicitly defined.\n- The problem is self-contained, consistent, and formalizable. There are no contradictions, missing data, or unrealistic requirements.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Convergence Order Estimator**\n\nThe problem is to determine the order of convergence, $p$, from the asymptotic relationship for the error norm, $\\|e_h\\|$, as a function of the mesh size, $h$. The governing model is given as:\n$$\n\\|e_h\\| \\approx C h^p\n$$\nwhere $C$ and $p$ are constants independent of $h$ for sufficiently small $h$. The constant $C$ is unknown. To eliminate $C$ and solve for $p$, we can transform the relation into a linear form by taking the natural logarithm of both sides:\n$$\n\\ln(\\|e_h\\|) \\approx \\ln(C h^p) = \\ln(C) + \\ln(h^p) = \\ln(C) + p \\ln(h)\n$$\nThis equation is of the form $Y = A + p X$, where $Y = \\ln(\\|e_h\\|)$, $X = \\ln(h)$, and the intercept $A = \\ln(C)$ is a constant. This reveals a linear relationship between the logarithm of the error norm and the logarithm of the mesh size. The order of convergence, $p$, is the slope of this line.\n\nThe problem requires using data from three mesh refinements, let us denote them $h_1$, $h_2$, and $h_3$, with corresponding computed error norms $E_1$, $E_2$, and $E_3$. This provides us with three data points $(X_i, Y_i) = (\\ln(h_i), \\ln(E_i))$ for $i \\in \\{1, 2, 3\\}$. A robust estimator for the slope $p$ can be obtained using linear least-squares regression. This method finds the line that minimizes the sum of the squared vertical distances from the data points to the line. For a set of $n$ points $(X_i, Y_i)$, the formula for the slope $\\hat{p}$ of the best-fit line is:\n$$\n\\hat{p} = \\frac{n \\sum_{i=1}^n X_i Y_i - \\left(\\sum_{i=1}^n X_i\\right) \\left(\\sum_{i=1}^n Y_i\\right)}{n \\sum_{i=1}^n X_i^2 - \\left(\\sum_{i=1}^n X_i\\right)^2}\n$$\nThis approach is principled as it uses all available information from the three specified mesh levels. It is also robust to small perturbations from the ideal linear model, such as those introduced by higher-order terms in the error (e.g., the $\\delta(x;h)$ term in Case $4$), as the regression process has an averaging effect.\n\n**Implementation Strategy**\n\nThe implementation will follow a systematic procedure for each test case.\n1. Define the parameters for the test case: the true order $p_{true}$, the constant $C$, the error shape function $w(x)$, the contamination term $\\delta(x;h)$, and the norm type $q \\in \\{1, 2, \\infty\\}$.\n2. For the three finest mesh resolutions, $N \\in \\{40, 80, 160\\}$, perform the following steps:\n    a. Calculate the mesh size $h=1/N$.\n    b. Generate the uniform grid of $N+1$ points, $x_i = i h$ for $i=0, \\dots, N$.\n    c. Evaluate the exact solution $u(x_i) = \\sin(2\\pi x_i)$ on the grid.\n    d. Construct the approximate solution $u_h(x_i) = u(x_i) + C\\,h^{p_{true}}\\,w(x_i) + \\delta(x_i;h, p_{true})$ on the grid. Note that the true order $p_{true}$ is used here only to generate the synthetic data, as specified.\n    e. Compute the pointwise error vector $e_i = u_h(x_i) - u(x_i)$.\n    f. Calculate the specified discrete error norm $\\|e_h\\|_{q,h}$ according to the provided formulas.\n3. After computing the three error norms ($E_1, E_2, E_3$) for the three mesh sizes ($h_1, h_2, h_3$), create two vectors: $X = [\\ln(h_1), \\ln(h_2), \\ln(h_3)]$ and $Y = [\\ln(E_1), \\ln(E_2), \\ln(E_3)]$.\n4. Apply a linear regression algorithm to the data $(X, Y)$ to find the slope. A standard numerical library function, such as `numpy.polyfit` with degree $1$, provides an efficient implementation of the least-squares formula, returning the slope as the first coefficient. This slope is the estimated order of convergence, $\\hat{p}$.\n5. This process is repeated for all four test cases. The resulting estimates are collected and formatted for the final output as a list of numbers rounded to $3$ decimal places.\n\nThis method adheres strictly to the problem constraints: it uses only the norm values and mesh sizes for the estimation and applies a principled, robust technique suitable for analyzing convergence data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by estimating the order of convergence for four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"case\": 1,\n            \"p_true\": 2.0,\n            \"C\": 0.7,\n            \"w_func\": lambda x: np.cos(np.pi * x),\n            \"delta_func\": lambda x, h, p: 0.0,\n            \"q\": 2\n        },\n        {\n            \"case\": 2,\n            \"p_true\": 1.0,\n            \"C\": 1.5,\n            \"w_func\": lambda x: np.exp(x),\n            \"delta_func\": lambda x, h, p: 0.0,\n            \"q\": 1\n        },\n        {\n            \"case\": 3,\n            \"p_true\": 1.5,\n            \"C\": 0.3,\n            \"w_func\": lambda x: np.sin(3 * np.pi * x),\n            \"delta_func\": lambda x, h, p: 0.0,\n            \"q\": np.inf\n        },\n        {\n            \"case\": 4,\n            \"p_true\": 3.0,\n            \"C\": 0.2,\n            \"w_func\": lambda x: np.cos(5 * np.pi * x),\n            \"delta_func\": lambda x, h, p: 5.0 * h**(p + 1) * np.sin(7 * np.pi * x),\n            \"q\": 2\n        }\n    ]\n\n    results = []\n    # Mesh resolutions to use for the estimator, as per the problem statement.\n    mesh_resolutions_N = [40, 80, 160]\n\n    for case in test_cases:\n        p_true = case[\"p_true\"]\n        C = case[\"C\"]\n        w_func = case[\"w_func\"]\n        delta_func = case[\"delta_func\"]\n        q = case[\"q\"]\n\n        mesh_sizes_h = []\n        error_norms = []\n\n        for N in mesh_resolutions_N:\n            h = 1.0 / N\n            mesh_sizes_h.append(h)\n            \n            # Create the uniform grid from x=0 to x=1 with N+1 points.\n            x = np.linspace(0.0, 1.0, N + 1)\n            \n            # Compute the exact solution u(x) = sin(2*pi*x).\n            u_exact = np.sin(2 * np.pi * x)\n            \n            # Compute the approximate solution using the synthetic model.\n            error_shape = C * h**p_true * w_func(x)\n            contamination = delta_func(x, h, p_true)\n            u_approx = u_exact + error_shape + contamination\n            \n            # Compute the pointwise error.\n            error_vector = u_approx - u_exact\n            \n            # Compute the specified discrete norm of the error.\n            norm = 0.0\n            if q == 1:\n                # Discrete L1 norm: h * sum(|e_i|)\n                norm = h * np.sum(np.abs(error_vector))\n            elif q == 2:\n                # Discrete L2 norm: sqrt(h * sum(e_i^2))\n                norm = np.sqrt(h * np.sum(error_vector**2))\n            elif q == np.inf:\n                # Discrete L-infinity norm: max(|e_i|)\n                norm = np.max(np.abs(error_vector))\n            \n            error_norms.append(norm)\n\n        # Estimate the order of convergence p using linear regression on the log-log data.\n        # The model is log(E) = log(C) + p * log(h).\n        # We find the slope 'p' of the best-fit line for (log(h), log(E)).\n        log_h = np.log(np.array(mesh_sizes_h))\n        log_E = np.log(np.array(error_norms))\n        \n        # np.polyfit with degree 1 fits a line and returns [slope, intercept].\n        # The slope is our estimate for the order of convergence p.\n        p_estimated, _ = np.polyfit(log_h, log_E, 1)        \n        results.append(p_estimated)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of results rounded to 3 decimal places, inside brackets.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "2389343"}, {"introduction": "After learning how to measure convergence rates, one might assume that a decreasing error norm always signifies a \"good\" solution. However, the choice of norm matters tremendously, as some norms can obscure critical flaws in the solution's quality. This hands-on experiment demonstrates a classic pitfall in numerical methods where a solution can appear to converge in an integral sense (e.g., the $L^2$ norm) while exhibiting completely non-physical pointwise behavior. This practice will help you develop a crucial, critical perspective on error analysis, learning to question what your metrics are truly measuring [@problem_id:2389318].", "problem": "Construct a fully specified computational experiment that demonstrates the contrast between integral-norm convergence and pointwise behavior for a stiff ordinary differential equation. Consider the initial value problem on the interval $[0,1]$:\n- Stiff linear ordinary differential equation: $y'(t) = -\\lambda y(t)$ with stiffness parameter $\\lambda = 100$.\n- Initial condition: $y(0) = 1$.\n- Exact solution: $y(t) = e^{-\\lambda t}$.\n\nFor each uniform time step count $N \\in \\mathbb{N}$, define $h = 1/N$ and the discrete approximation $\\{Y_n\\}_{n=0}^{N}$ by the Forward (Explicit) Euler time-stepping scheme\n$$\nY_{n+1} = Y_n + h(-\\lambda Y_n) = (1 - h\\lambda) Y_n,\\quad Y_0 = 1,\n$$\nand define the continuous, piecewise linear interpolant $y_h:[0,1]\\to\\mathbb{R}$ by linearly interpolating the nodal values $\\{(t_n,Y_n)\\}_{n=0}^{N}$ with $t_n = nh$.\n\nDefine the time-continuous $L^2$ error norm for a given $h$ as\n$$\nE_2(h) = \\left(\\int_0^1 \\lvert y_h(t) - y(t) \\rvert^2\\,dt\\right)^{1/2}.\n$$\nDefine a pointwise oscillation indicator for the discrete trajectory by counting sign changes in the sequence $\\{Y_n\\}_{n=0}^{N}$. Because the exact solution $y(t)$ is strictly positive on $[0,1]$, any sign change of the discrete solution is considered incorrect pointwise behavior.\n\nYour program must, for each test case listed below, evaluate the following two boolean metrics over the three step counts $N$ given in that test case:\n- Strictly decreasing integral-norm error across refinements: return $\\mathrm{True}$ if $E_2(h_1) > E_2(h_2) > E_2(h_3)$ holds for the three step sizes $h_i = 1/N_i$ in the order listed for the test case, and $\\mathrm{False}$ otherwise.\n- Presence of pointwise oscillations: return $\\mathrm{True}$ if at least one of the three discrete solutions $\\{Y_n\\}$, corresponding to the three step counts in the test case, exhibits at least one sign change, and $\\mathrm{False}$ otherwise.\n\nTest Suite (each line is a test case specifying a triple of step counts $N$):\n- Case A (stable but oscillatory regime): $N \\in \\{52,55,59\\}$.\n- Case B (well resolved regime): $N \\in \\{200,400,800\\}$.\n- Case C (stability boundary and beyond): $N \\in \\{50,49,48\\}$.\n- Case D (mixed regime): $N \\in \\{83,100,125\\}$.\n\nYour program must compute $E_2(h)$ exactly for the piecewise linear $y_h$ or by numerical integration with sufficient accuracy so that the boolean comparisons are correct to within an absolute tolerance of $10^{-8}$ on each $E_2(h)$ value, and must implement the sign-change test strictly on the discrete nodal values $\\{Y_n\\}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the eight boolean results aggregated as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\text{A\\_dec},\\text{A\\_osc},\\ \\text{B\\_dec},\\text{B\\_osc},\\ \\text{C\\_dec},\\text{C\\_osc},\\ \\text{D\\_dec},\\text{D\\_osc}],\n$$\nwhere, for example, $\\text{A\\_dec}$ is the boolean indicating whether $E_2$ is strictly decreasing across the three $N$ values of Case A, and $\\text{A\\_osc}$ indicates whether any sign change occurs in the discrete trajectories for Case A. The booleans must be printed as either $\\mathrm{True}$ or $\\mathrm{False}$ and nothing else on the line.", "solution": "We formalize the computational experiment entirely from definitions. The ordinary differential equation is $y'(t) = -\\lambda y(t)$ with $\\lambda = 100$, $t \\in [0,1]$, $y(0)=1$. The exact solution is $y(t) = e^{-\\lambda t}$, which is strictly positive and strictly decreasing on $[0,1]$. Stiffness arises because the time scale of decay is $1/\\lambda$, so for $\\lambda=100$ the solution exhibits rapid decay on the small scale $t=\\mathcal{O}(10^{-2})$ relative to the domain length $1$.\n\nFor a uniform grid with $N$ steps, $h = 1/N$ and $t_n = nh$, the Forward (Explicit) Euler scheme is defined from first principles by applying the definition of the derivative to the semidiscrete update:\n$$\n\\frac{Y_{n+1} - Y_n}{h} \\approx y'(t_n) = -\\lambda y(t_n),\n$$\nand by replacing $y(t_n)$ with $Y_n$, we obtain\n$$\nY_{n+1} = Y_n + h(-\\lambda Y_n) = (1 - h\\lambda) Y_n,\\quad Y_0 = 1.\n$$\nThis closed-form recursion yields\n$$\nY_n = (1 - h\\lambda)^n.\n$$\nThe continuous, piecewise linear interpolant $y_h(t)$ is then defined on each subinterval $[t_n,t_{n+1}]$ by the unique linear function matching the nodal values:\n$$\ny_h(t) = \\frac{t_{n+1}-t}{h} Y_n + \\frac{t - t_n}{h} Y_{n+1},\\qquad t \\in [t_n,t_{n+1}].\n$$\n\nWe measure the error in the time-continuous $L^2$ norm,\n$$\nE_2(h) = \\left(\\int_0^1 \\lvert y_h(t) - e^{-\\lambda t} \\rvert^2\\,dt\\right)^{1/2}.\n$$\nThis is a norm on the difference function and, by definition, integrates the squared pointwise error over time. Because $y_h$ is piecewise linear and $y$ is smooth, $E_2(h)$ can be evaluated either by exact integration on each subinterval (expansion of a quadratic minus an exponential with known antiderivatives) or by sufficiently accurate numerical quadrature so that comparisons between different $h$ values are correct within a small absolute tolerance.\n\nWe also quantify incorrect pointwise behavior by detecting sign changes in the discrete sequence $\\{Y_n\\}_{n=0}^{N}$. Since the exact solution satisfies $y(t)>0$ on $[0,1]$, any sign change in the numerical sequence is a nonphysical oscillation. For the Forward Euler scheme on the linear test equation, the amplification factor is\n$$\nG(h\\lambda) = 1 - h\\lambda.\n$$\nIf $0 < h\\lambda < 1$, then $G(h\\lambda) \\in (0,1)$, so $Y_{n+1} = G Y_n$ remains positive and monotonically decreasing, exhibiting no sign changes. If $1 < h\\lambda < 2$, then $G(h\\lambda) \\in (-1,0)$, so $Y_{n+1}$ alternates sign and decays in magnitude, producing pointwise oscillations. If $h\\lambda \\ge 2$, then $\\lvert G(h\\lambda) \\rvert \\ge 1$, and the method is at the stability boundary or unstable, typically yielding persistent or growing oscillations.\n\nTest Suite justification:\n- Case A uses $N \\in \\{52,55,59\\}$, i.e., $h\\lambda \\in \\{100/52,100/55,100/59\\} \\subset (1,2)$, so the discrete solutions are oscillatory but stable. As $N$ increases, $h$ decreases and the $L^2$ error $E_2(h)$ is expected to strictly decrease; the oscillation indicator should be positive because of sign alternation.\n- Case B uses $N \\in \\{200,400,800\\}$, i.e., $h\\lambda \\in \\{0.5,0.25,0.125\\} \\subset (0,1)$, so the method is stable and non-oscillatory. $E_2(h)$ should strictly decrease and the oscillation indicator should be negative (no sign changes).\n- Case C uses $N \\in \\{50,49,48\\}$, i.e., $h\\lambda \\in \\{2.0,\\approx 2.0408,\\approx 2.0833\\}$, spanning the stability boundary and an unstable regime. In this ordering, the $L^2$ errors are not strictly decreasing across the sequence (the first entry at the stability boundary is typically much smaller than the subsequent unstable entries), while the oscillation indicator is positive due to alternation of signs and instability.\n- Case D uses $N \\in \\{83,100,125\\}$, i.e., $h\\lambda \\in \\{\\approx 1.2048,1.0,0.8\\}$, transitioning from oscillatory stable to non-oscillatory stable. The $L^2$ error should strictly decrease and the oscillation indicator should be positive because at least the coarsest choice yields a sign change.\n\nAlgorithmic implementation from these principles:\n- For each $N$, compute the nodal values $\\{Y_n\\}$ via the recurrence and form $y_h$ by linear interpolation on $[0,1]$.\n- Evaluate $E_2(h)$ by numerically integrating $\\lvert y_h(t) - e^{-\\lambda t}\\rvert^2$ on a dense uniform partition of $[0,1]$, e.g., via the composite trapezoid rule with a sufficiently large even number of subintervals to achieve absolute accuracy better than $10^{-8}$.\n- Determine the oscillation indicator by checking whether there exists an index $n$ with $Y_n Y_{n+1} < 0$ (a sign change).\n- For each test case consisting of three $N$ values ordered as specified, compute the booleans for strict decrease of $E_2$ across the triple and the presence of oscillations across the triple.\n\nThe final output is the list\n$$\n[\\text{A\\_dec},\\text{A\\_osc},\\ \\text{B\\_dec},\\text{B\\_osc},\\ \\text{C\\_dec},\\text{C\\_osc},\\ \\text{D\\_dec},\\text{D\\_osc}],\n$$\nprinted on a single line with booleans.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_euler_linear_decay(lmbda: float, N: int):\n    \"\"\"\n    Solve y' = -lambda*y, y(0)=1 on [0,1] with Forward Euler using N steps.\n    Returns time nodes t (size N+1) and solution values Y (size N+1).\n    \"\"\"\n    h = 1.0 / N\n    t = np.linspace(0.0, 1.0, N + 1)\n    Y = np.empty(N + 1, dtype=float)\n    Y[0] = 1.0\n    factor = 1.0 - h * lmbda\n    # Use recurrence: Y_n = factor^n\n    # Iterative to avoid potential under/overflow in pow for large N and debug clarity\n    for n in range(N):\n        Y[n + 1] = Y[n] * factor\n    return t, Y\n\ndef piecewise_linear_interpolant_values(t_nodes: np.ndarray, y_nodes: np.ndarray, t_query: np.ndarray):\n    \"\"\"\n    Evaluate the piecewise linear interpolant through (t_nodes, y_nodes) at t_query.\n    Uses numpy's linear interpolation.\n    Assumes t_nodes is sorted ascending spanning [0,1].\n    \"\"\"\n    return np.interp(t_query, t_nodes, y_nodes)\n\ndef l2_error(lmbda: float, t_nodes: np.ndarray, y_nodes: np.ndarray, num_subintervals: int = 200000):\n    \"\"\"\n    Compute L2(0,1) error norm between piecewise-linear y_h and exact y(t)=exp(-lambda t).\n    Uses composite trapezoidal rule on a uniform grid with num_subintervals subintervals.\n    \"\"\"\n    # Ensure even number of subintervals for symmetric sampling (not required for trapz but good practice)\n    if num_subintervals % 2 == 1:\n        num_subintervals += 1\n    tq = np.linspace(0.0, 1.0, num_subintervals + 1)\n    yh = piecewise_linear_interpolant_values(t_nodes, y_nodes, tq)\n    y_exact = np.exp(-lmbda * tq)\n    err_sq = (yh - y_exact) ** 2\n    integral = np.trapz(err_sq, tq)\n    return np.sqrt(integral)\n\ndef has_sign_change(y_nodes: np.ndarray):\n    \"\"\"\n    Return True if the discrete sequence y_nodes exhibits any sign change between consecutive nodes.\n    \"\"\"\n    # Consider exact zero as non-negative for sign-change detection\n    prod = y_nodes[:-1] * y_nodes[1:]\n    return np.any(prod < 0.0)\n\ndef evaluate_case(lmbda: float, N_list):\n    \"\"\"\n    For a list of three N values, compute:\n    - strictly decreasing L2 error across the list order\n    - presence of oscillations (any sign change) among the three solutions\n    Returns tuple (dec_bool, osc_bool).\n    \"\"\"\n    E2_vals = []\n    osc_flags = []\n    for N in N_list:\n        t_nodes, y_nodes = forward_euler_linear_decay(lmbda, N)\n        E2 = l2_error(lmbda, t_nodes, y_nodes, num_subintervals=200000)\n        E2_vals.append(E2)\n        osc_flags.append(has_sign_change(y_nodes))\n    dec = (E2_vals[0] > E2_vals[1]) and (E2_vals[1] > E2_vals[2])\n    osc = any(osc_flags)\n    return dec, osc\n\ndef solve():\n    # Define parameters\n    lmbda = 100.0  # stiffness parameter\n    # Test cases: lists of N (number of steps); h = 1/N\n    case_A = [52, 55, 59]     # stable but oscillatory (1 < h*lambda < 2)\n    case_B = [200, 400, 800]  # well-resolved (h*lambda << 1)\n    case_C = [50, 49, 48]     # boundary and unstable (h*lambda >= 2), ordered to break monotonic decrease\n    case_D = [83, 100, 125]   # mixed: oscillatory -> boundary -> non-oscillatory\n\n    test_cases = [case_A, case_B, case_C, case_D]\n\n    results = []\n    for case in test_cases:\n        dec, osc = evaluate_case(lmbda, case)\n        results.append(dec)\n        results.append(osc)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2389318"}]}