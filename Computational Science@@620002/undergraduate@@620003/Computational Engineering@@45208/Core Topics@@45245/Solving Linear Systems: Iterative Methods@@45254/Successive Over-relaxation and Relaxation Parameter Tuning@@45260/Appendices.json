{"hands_on_practices": [{"introduction": "This first practice takes us back to fundamentals, providing a sharp, analytical demonstration of why the relaxation parameter is not just an optional tweak but a critical component of the SOR method. By working through a small, carefully chosen linear system, you will see firsthand how the standard Gauss-Seidel method (which is just SOR with $\\omega=1$) can fail to converge, while a well-chosen relaxation parameter $\\omega$ can rescue the iteration and lead to a solution. This exercise is invaluable for building a solid theoretical intuition for convergence analysis and the profound impact of parameter tuning [@problem_id:2441046].", "problem": "Consider the linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix} 1  2 \\\\ -1  1 \\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nUse only the definitions of the Gaussâ€“Seidel (GS) method and the Successive Over-Relaxation (SOR) method to analyze convergence.\n\nTasks:\n1. Starting from the componentwise SOR update definition for a general relaxation parameter $\\omega \\in (0,1]$, derive the $2 \\times 2$ linear error-propagation map $T_{\\omega}$ for this system. Then specialize your expression to $\\omega = 1$ to recover the GS error-propagation map $T_{1}$.\n2. Determine whether GS converges by computing the spectral radius of $T_{1}$ and justifying your conclusion.\n3. For $\\omega \\in (0,1)$, determine the value $\\omega^{\\star}$ that minimizes the spectral radius of $T_{\\omega}$ and state whether the corresponding SOR iteration converges. Report $\\omega^{\\star}$ rounded to four significant figures.\n\nYour final answer must be the single numerical value of $\\omega^{\\star}$ rounded to four significant figures. No units are required.", "solution": "The provided problem is subjected to validation.\n\n**Step 1: Extract Givens**\n- Linear system: $A x = b$\n- Matrix $A$: $A = \\begin{pmatrix} 1  2 \\\\ -1  1 \\end{pmatrix}$\n- Vector $b$: $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- Method: Successive Over-Relaxation (SOR) and Gauss-Seidel (GS) as a special case.\n- Parameter: Relaxation parameter $\\omega \\in (0, 1]$. The optimization is for $\\omega \\in (0, 1)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, focusing on the convergence analysis of the SOR iterative method. The concepts of error-propagation maps and spectral radius are fundamental and correctly applied.\n- **Well-Posed:** The problem is clearly stated with three distinct tasks. The objective to find an optimal relaxation parameter that minimizes the spectral radius is a well-defined mathematical optimization problem.\n- **Objective:** The problem is expressed in precise mathematical language, free from ambiguity or subjective content.\n- **Completeness and Consistency:** All necessary information (the matrix $A$) for analyzing the convergence of the iterative methods is provided. The vector $b$ is not required for the convergence analysis, which depends only on the iteration matrix, but its presence does not introduce any contradiction. The problem is self-contained.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe analysis begins with the definition of the SOR iteration. For a linear system $A x = b$, the matrix $A$ is decomposed as $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strict lower-triangular part of $A$, and $-U$ is the strict upper-triangular part of $A$. The SOR iteration is defined by the update rule:\n$$x^{(k+1)} = (D - \\omega L)^{-1} \\left( ((1-\\omega)D + \\omega U) x^{(k)} + \\omega b \\right)$$\nThe convergence of the method is determined by the spectral radius of the SOR iteration matrix, $T_{\\omega}$, defined as:\n$$T_{\\omega} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U)$$\nThe error vector $e^{(k)} = x^{(k)} - x^*$ (where $x^*$ is the exact solution) propagates according to $e^{(k+1)} = T_{\\omega} e^{(k)}$.\n\n**1. Derivation of the Error-Propagation Map $T_{\\omega}$**\n\nFor the given matrix $A = \\begin{pmatrix} 1  2 \\\\ -1  1 \\end{pmatrix}$, the decomposition is:\n$$D = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ -(-1)  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  -2 \\\\ 0  0 \\end{pmatrix}$$\nFirst, we compute the matrix $(D - \\omega L)$:\n$$D - \\omega L = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\omega \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ -\\omega  1 \\end{pmatrix}$$\nIts inverse is:\n$$(D - \\omega L)^{-1} = \\frac{1}{(1)(1) - (0)(-\\omega)} \\begin{pmatrix} 1  0 \\\\ \\omega  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ \\omega  1 \\end{pmatrix}$$\nNext, we compute the matrix $((1-\\omega)D + \\omega U)$:\n$$(1-\\omega)D + \\omega U = (1-\\omega) \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\omega \\begin{pmatrix} 0  -2 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1-\\omega  -2\\omega \\\\ 0  1-\\omega \\end{pmatrix}$$\nNow, we multiply these two matrices to find $T_{\\omega}$:\n$$T_{\\omega} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U) = \\begin{pmatrix} 1  0 \\\\ \\omega  1 \\end{pmatrix} \\begin{pmatrix} 1-\\omega  -2\\omega \\\\ 0  1-\\omega \\end{pmatrix}$$\n$$T_{\\omega} = \\begin{pmatrix} (1)(1-\\omega) + (0)(0)  (1)(-2\\omega) + (0)(1-\\omega) \\\\ (\\omega)(1-\\omega) + (1)(0)  (\\omega)(-2\\omega) + (1)(1-\\omega) \\end{pmatrix} = \\begin{pmatrix} 1-\\omega  -2\\omega \\\\ \\omega(1-\\omega)  1-\\omega - 2\\omega^2 \\end{pmatrix}$$\nThe Gauss-Seidel (GS) method is a special case of SOR with $\\omega = 1$. The GS error-propagation map, $T_1$, is obtained by substituting $\\omega=1$ into the expression for $T_{\\omega}$:\n$$T_{1} = \\begin{pmatrix} 1-1  -2(1) \\\\ 1(1-1)  1-1 - 2(1)^2 \\end{pmatrix} = \\begin{pmatrix} 0  -2 \\\\ 0  -2 \\end{pmatrix}$$\n\n**2. Convergence Analysis of the Gauss-Seidel Method**\n\nThe convergence of an iterative method is guaranteed if and only if the spectral radius $\\rho$ of its iteration matrix is strictly less than $1$. We compute the eigenvalues of $T_1$ by solving the characteristic equation $\\det(T_1 - \\lambda I) = 0$.\n$$\\det \\begin{pmatrix} 0 - \\lambda  -2 \\\\ 0  -2 - \\lambda \\end{pmatrix} = (-\\lambda)(-2-\\lambda) - (0)(-2) = \\lambda(2+\\lambda) = 0$$\nThe eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = -2$.\nThe spectral radius of $T_1$ is the maximum of the absolute values of its eigenvalues:\n$$\\rho(T_1) = \\max(|\\lambda_1|, |\\lambda_2|) = \\max(|0|, |-2|) = 2$$\nSince $\\rho(T_1) = 2 > 1$, the Gauss-Seidel method for this system does not converge.\n\n**3. Optimal Relaxation Parameter $\\omega^{\\star}$**\n\nTo find the optimal relaxation parameter $\\omega^{\\star}$ in the range $\\omega \\in (0, 1)$, we must find the value of $\\omega$ that minimizes the spectral radius $\\rho(T_{\\omega})$. We start by finding the eigenvalues of $T_{\\omega}$ from its characteristic equation $\\det(T_{\\omega} - \\lambda I) = 0$:\n$$\\det \\begin{pmatrix} 1-\\omega - \\lambda  -2\\omega \\\\ \\omega(1-\\omega)  1-\\omega - 2\\omega^2 - \\lambda \\end{pmatrix} = 0$$\n$$(1-\\omega - \\lambda)(1-\\omega - 2\\omega^2 - \\lambda) + 2\\omega^2(1-\\omega) = 0$$\n$$\\lambda^2 - (1-\\omega - 2\\omega^2 + 1-\\omega)\\lambda + (1-\\omega)(1-\\omega - 2\\omega^2) + 2\\omega^2(1-\\omega) = 0$$\n$$\\lambda^2 - (2 - 2\\omega - 2\\omega^2)\\lambda + (1-\\omega)(1-\\omega - 2\\omega^2 + 2\\omega^2) = 0$$\n$$\\lambda^2 - 2(1 - \\omega - \\omega^2)\\lambda + (1-\\omega)^2 = 0$$\nUsing the quadratic formula, the eigenvalues are:\n$$\\lambda = \\frac{2(1 - \\omega - \\omega^2) \\pm \\sqrt{4(1 - \\omega - \\omega^2)^2 - 4(1-\\omega)^2}}{2} = (1 - \\omega - \\omega^2) \\pm \\sqrt{(1 - \\omega - \\omega^2)^2 - (1-\\omega)^2}$$\nThe nature of the eigenvalues depends on the sign of the discriminant, $\\Delta_p = (1 - \\omega - \\omega^2)^2 - (1-\\omega)^2$.\n$$\\Delta_p = [(1 - \\omega - \\omega^2) - (1-\\omega)][(1 - \\omega - \\omega^2) + (1-\\omega)] = (-\\omega^2)(2 - 2\\omega - \\omega^2)$$\nFor $\\omega \\in (0, 1)$, $-\\omega^2$ is negative. The sign of $\\Delta_p$ is opposite to the sign of $f(\\omega) = 2 - 2\\omega - \\omega^2$. The roots of $f(\\omega) = 0$ are $\\omega = -1 \\pm \\sqrt{3}$. Since we are in the interval $\\omega \\in (0, 1)$, the critical point is $\\omega = \\sqrt{3} - 1 \\approx 0.732$.\n\nCase 1: $0  \\omega \\le \\sqrt{3}-1$.\nIn this interval, $f(\\omega) \\ge 0$, so $\\Delta_p \\le 0$. The eigenvalues are a complex conjugate pair (or real and equal at the boundary).\n$$\\lambda = (1 - \\omega - \\omega^2) \\pm i\\omega \\sqrt{2 - 2\\omega - \\omega^2}$$\nThe spectral radius is the modulus of these eigenvalues. For a complex number $z=a+ib$, $|z|^2=a^2+b^2$. The product of the complex conjugate roots is $\\lambda \\bar{\\lambda} = (1-\\omega)^2$, which is the constant term in the characteristic polynomial.\n$$\\rho(T_{\\omega})^2 = |\\lambda|^2 = (1-\\omega)^2$$\nSince $\\omega \\in (0, 1)$, $1-\\omega > 0$, so $\\rho(T_{\\omega}) = 1-\\omega$. This is a linearly decreasing function of $\\omega$.\n\nCase 2: $\\sqrt{3}-1  \\omega  1$.\nIn this interval, $f(\\omega)  0$, so $\\Delta_p  0$. The eigenvalues are real and distinct.\n$$\\lambda_{1,2} = (1 - \\omega - \\omega^2) \\pm \\omega\\sqrt{\\omega^2 + 2\\omega - 2}$$\nThe product of the roots is $\\lambda_1\\lambda_2 = (1-\\omega)^2 > 0$, so they have the same sign. Their sum is $\\lambda_1+\\lambda_2 = 2(1-\\omega-\\omega^2)$. The roots of $1-\\omega-\\omega^2=0$ are $\\omega = \\frac{\\sqrt{5}-1}{2} \\approx 0.618$ and a negative root. Since $\\sqrt{3}-1 > \\frac{\\sqrt{5}-1}{2}$, for any $\\omega$ in this case, $1-\\omega-\\omega^2  0$. Thus, both eigenvalues are negative.\nThe spectral radius is the maximum of their absolute values, which is the absolute value of the more negative root ($\\lambda_2$).\n$$\\rho(T_{\\omega}) = |\\lambda_2| = |(1 - \\omega - \\omega^2) - \\omega\\sqrt{\\omega^2 + 2\\omega - 2}| = -(1 - \\omega - \\omega^2) + \\omega\\sqrt{\\omega^2 + 2\\omega - 2}$$\n$$\\rho(T_{\\omega}) = \\omega^2 + \\omega - 1 + \\omega\\sqrt{\\omega^2 + 2\\omega - 2}$$\nTo find the minimum of this function, we examine its derivative with respect to $\\omega$:\n$$\\frac{d\\rho}{d\\omega} = 2\\omega + 1 + \\sqrt{\\omega^2 + 2\\omega - 2} + \\frac{\\omega(2\\omega+2)}{2\\sqrt{\\omega^2+2\\omega-2}} = 2\\omega + 1 + \\frac{(\\omega^2+2\\omega-2) + (\\omega^2+\\omega)}{\\sqrt{\\omega^2+2\\omega-2}}$$\n$$\\frac{d\\rho}{d\\omega} = 2\\omega + 1 + \\frac{2\\omega^2+3\\omega-2}{\\sqrt{\\omega^2+2\\omega-2}}$$\nFor $\\omega \\in (\\sqrt{3}-1, 1)$, all terms are positive:\n- $2\\omega+1 > 0$.\n- $\\sqrt{\\omega^2+2\\omega-2} > 0$.\n- The numerator $2\\omega^2+3\\omega-2$ has roots at $\\omega=1/2$ and $\\omega=-2$. It is positive for $\\omega > 1/2$. Since $\\sqrt{3}-1 \\approx 0.732 > 1/2$, this term is positive.\nTherefore, $\\frac{d\\rho}{d\\omega} > 0$ for $\\omega \\in (\\sqrt{3}-1, 1)$, which means $\\rho(T_{\\omega})$ is an increasing function in this interval.\n\nCombining the two cases, $\\rho(T_{\\omega})$ decreases linearly for $\\omega \\in (0, \\sqrt{3}-1]$ and increases for $\\omega \\in (\\sqrt{3}-1, 1)$. The minimum value of the spectral radius must occur at the point connecting these two regions, which is $\\omega^{\\star} = \\sqrt{3}-1$.\n\nAt this optimal value, the spectral radius is $\\rho(T_{\\omega^\\star}) = 1 - \\omega^{\\star} = 1 - (\\sqrt{3}-1) = 2 - \\sqrt{3}$.\nSince $1  \\sqrt{3}  2$, we have $0  2-\\sqrt{3}  1$. Specifically, $2-\\sqrt{3} \\approx 0.268$, which is less than $1$. Therefore, the SOR iteration with $\\omega = \\omega^{\\star}$ converges.\n\nThe problem asks for the numerical value of $\\omega^{\\star}$ rounded to four significant figures.\n$\\omega^{\\star} = \\sqrt{3}-1 \\approx 1.7320508... - 1 = 0.7320508...$\nRounding to four significant figures gives $0.7321$.", "answer": "$$\\boxed{0.7321}$$", "id": "2441046"}, {"introduction": "Building on our foundational understanding, we now move from abstract matrices to a tangible physical problem: modeling heat diffusion in a composite material. This practice challenges you to implement an SOR-type solver where the material properties are not uniform, a common scenario in engineering. It introduces an advanced concept where the relaxation parameter $\\omega$ is not a single scalar but a field, $\\omega_{i,j}$, which adapts to the local thermal conductivity, illustrating a powerful heuristic for tuning SOR in complex, heterogeneous systems [@problem_id:2441080].", "problem": "Consider steady-state heat diffusion in a two-dimensional composite medium on the unit square domain $\\Omega=[0,1]\\times[0,1]$. The temperature field is denoted by $T(x,y)$ and the thermal conductivity field is denoted by $k(x,y)$. The governing equation is the variable-coefficient elliptic equation $-\\nabla\\cdot(k\\nabla T)=0$ in the interior with Dirichlet boundary conditions $T=1$ on the left boundary $x=0$ and $T=0$ on the other three boundaries $x=1$, $y=0$, and $y=1$. Treat temperature as a dimensionless scalar.\n\nDiscretize $\\Omega$ with a uniform Cartesian grid of $N_x$ by $N_y$ nodes, including the boundary nodes. Let the grid spacings be $h_x=1/(N_x-1)$ and $h_y=1/(N_y-1)$, and assume $h_x=h_y=h$. Let $T_{i,j}$ denote the discrete temperature at node $(i,j)$ where $i\\in\\{0,1,\\dots,N_x-1\\}$ and $j\\in\\{0,1,\\dots,N_y-1\\}$. The interior node set is $\\{(i,j):1\\le i\\le N_x-2,\\ 1\\le j\\le N_y-2\\}$. Assign the boundary values by $T_{0,j}=1$ for all $j$, and $T_{N_x-1,j}=0$, $T_{i,0}=0$, $T_{i,N_y-1}=0$ for all admissible $i$ and $j$.\n\nDefine the piecewise-constant conductivity field by a vertical interface at $x=s\\in(0,1)$:\n- For each grid node with coordinate $x_i=i/(N_x-1)$ and any $j$, set $k_{i,j}=k_{\\text{left}}$ if $x_i\\le s$, and $k_{i,j}=k_{\\text{right}}$ otherwise, where $k_{\\text{left}}0$ and $k_{\\text{right}}0$.\n\nFor each interior node $(i,j)$, define face conductivities at half nodes by harmonic means:\n- $k_{i+\\frac{1}{2},j}=\\dfrac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j}+k_{i+1,j}}$, $k_{i-\\frac{1}{2},j}=\\dfrac{2\\,k_{i,j}\\,k_{i-1,j}}{k_{i,j}+k_{i-1,j}}$, $k_{i,j+\\frac{1}{2}}=\\dfrac{2\\,k_{i,j}\\,k_{i,j+1}}{k_{i,j}+k_{i,j+1}}$, $k_{i,j-\\frac{1}{2}}=\\dfrac{2\\,k_{i,j}\\,k_{i,j-1}}{k_{i,j}+k_{i,j-1}}$.\nThen set the discrete coefficients\n- $a_E=\\dfrac{k_{i+\\frac{1}{2},j}}{h^2}$, $a_W=\\dfrac{k_{i-\\frac{1}{2},j}}{h^2}$, $a_N=\\dfrac{k_{i,j+\\frac{1}{2}}}{h^2}$, $a_S=\\dfrac{k_{i,j-\\frac{1}{2}}}{h^2}$,\nand $A_P=a_E+a_W+a_N+a_S$. The discrete interior equation is\n$$\nA_P\\,T_{i,j}-a_E\\,T_{i+1,j}-a_W\\,T_{i-1,j}-a_N\\,T_{i,j+1}-a_S\\,T_{i,j-1}=0,\n$$\nwith the boundary values $T_{i,j}$ fixed as specified.\n\nIntroduce a spatially varying relaxation field $\\omega_{i,j}$ defined pointwise from the conductivity by\n$$\n\\omega_{i,j}=\n\\begin{cases}\n\\omega_{\\min}+\\left(\\omega_{\\max}-\\omega_{\\min}\\right)\\dfrac{k_{i,j}-k_{\\min}}{k_{\\max}-k_{\\min}},  \\text{if } k_{\\max}>k_{\\min} \\\\\n\\dfrac{\\omega_{\\min}+\\omega_{\\max}}{2},  \\text{if } k_{\\max}=k_{\\min}\n\\end{cases}\n$$\nwhere $k_{\\min}=\\min_{i,j} k_{i,j}$ and $k_{\\max}=\\max_{i,j} k_{i,j}$, and where $0\\omega_{\\min}\\omega_{\\max}2$.\n\nStarting from the initial interior guess $T_{i,j}=0$ for all interior nodes $(i,j)$, perform in-place lexicographic relaxation sweeps on the interior nodes by the pointwise update\n$$\nT_{i,j}\\leftarrow (1-\\omega_{i,j})\\,T_{i,j}+\\omega_{i,j}\\,\\frac{a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}}{A_P}.\n$$\nAfter each full sweep, compute the discrete residual at each interior node\n$$\nr_{i,j}=a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}-A_P\\,T_{i,j}\n$$\nand its infinity norm $\\|r\\|_{\\infty}=\\max_{(i,j)\\ \\text{interior}} |r_{i,j}|$. Stop when $\\|r\\|_{\\infty}\\tau$ for a given tolerance $\\tau>0$, or after a fixed maximum number of sweeps if convergence has not been achieved. The quantity to be reported for each test case is the integer number of full sweeps required to satisfy $\\|r\\|_{\\infty}\\tau$.\n\nTest suite. Use the following three test cases, each specified by $(N_x,N_y,s,k_{\\text{left}},k_{\\text{right}},\\omega_{\\min},\\omega_{\\max},\\tau)$:\n- Case $1$: $(N_x,N_y)=(3,3)$, $s=0.5$, $k_{\\text{left}}=1.0$, $k_{\\text{right}}=5.0$, $\\omega_{\\min}=1.0$, $\\omega_{\\max}=1.95$, $\\tau=1.0\\times 10^{-8}$.\n- Case $2$: $(N_x,N_y)=(3,3)$, $s=0.5$, $k_{\\text{left}}=3.0$, $k_{\\text{right}}=3.0$, $\\omega_{\\min}=1.0$, $\\omega_{\\max}=1.95$, $\\tau=1.0\\times 10^{-8}$.\n- Case $3$: $(N_x,N_y)=(3,3)$, $s=0.5$, $k_{\\text{left}}=1.0$, $k_{\\text{right}}=100.0$, $\\omega_{\\min}=1.0$, $\\omega_{\\max}=1.95$, $\\tau=1.0\\times 10^{-8}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above (for example, $[n_1,n_2,n_3]$). Each $n_i$ must be the integer number of sweeps required. No other text should be printed. All requested outputs are dimensionless integers with no units.", "solution": "The problem requires the implementation of a successive over-relaxation (SOR) type iterative solver for a steady-state heat diffusion equation on a two-dimensional composite medium. The solver employs a spatially varying relaxation parameter. The task is to find the number of iterations required for convergence for three specified test cases.\n\nFirst, the problem statement must be validated for correctness and solvability.\n\n**Problem Validation**\n\nThe givens are:\n- A governing partial differential equation: $-\\nabla\\cdot(k\\nabla T)=0$ on the unit square $\\Omega=[0,1]\\times[0,1]$.\n- Dirichlet boundary conditions: $T=1$ on $x=0$, and $T=0$ on $x=1$, $y=0$, and $y=1$.\n- A uniform Cartesian grid discretization ($N_x \\times N_y$ nodes, with spacing $h$).\n- A finite volume discretization scheme using a harmonic mean for face conductivities.\n- A definition for the piecewise-constant conductivity field $k(x,y)$.\n- A specific formula for a spatially varying relaxation field $\\omega_{i,j}$.\n- An initial guess of $T_{i,j}=0$ for interior nodes.\n- A pointwise iterative update rule (a form of SOR).\n- A convergence criterion based on the infinity norm of the residual, $\\|r\\|_{\\infty}  \\tau$.\n- Three distinct parameter sets for testing.\n\nThe problem is scientifically grounded, describing a standard boundary value problem in heat transfer, and the numerical method proposed is a valid, though non-standard, iterative technique. The problem is well-posed.\n\nHowever, the problem statement exhibits one point of sloppiness:\n\n1.  **Ambiguity in Boundary Conditions at Corners**: The problem specifies discrete boundary conditions as \"$T_{0,j}=1$ for all $j$, and $T_{N_x-1,j}=0$, $T_{i,0}=0$, $T_{i,N_y-1}=0$ for all admissible $i$ and $j$\". This creates ambiguity at the corners. For example, node $(0,0)$ is on the left boundary ($x=0$) where $T=1$ is prescribed, and on the bottom boundary ($y=0$) where $T=0$ is prescribed. The problem does not specify which condition takes precedence. Fortunately, for the given test cases with $N_x=3$ and $N_y=3$, there is only one interior node at $(1,1)$. The update for $T_{1,1}$ only requires its direct neighbors $T_{0,1}, T_{2,1}, T_{1,0}, T_{1,2}$, none of which are at the ambiguous corners. Therefore, this ambiguity does not affect the solution for the given test cases.\n\nDespite this minor ambiguity, the core problem is valid and can be solved.\n\n**Methodology**\n\nThe solution will be obtained by implementing the described algorithm. For each test case, the following steps are performed:\n\n1.  **Grid and Field Initialization**: A $N_x \\times N_y$ grid is established. The temperature field $T$ is initialized with the specified boundary conditions and an initial guess of $T_{i,j}=0$ for all interior nodes. The thermal conductivity field $k$ and the relaxation parameter field $\\omega$ are initialized according to their definitions for every node $(i,j)$ on the grid.\n\n2.  **Pre-computation of Coefficients**: For efficiency, the discrete coefficients $a_E, a_W, a_N, a_S,$ and $A_P$ are pre-computed and stored for each interior node, as they do not change during the iteration. The face conductivities $k_{i\\pm 1/2, j}$ and $k_{i, j\\pm 1/2}$ are calculated using the harmonic mean to ensure physical correctness at material interfaces.\n\n3.  **Iterative Solution**: The algorithm proceeds in discrete sweeps. A counter for the number of sweeps is initialized to $0$. The main loop is as follows:\n    a. A full sweep is performed over all interior nodes $(i,j)$ in lexicographic order. For each node, the temperature $T_{i,j}$ is updated in-place using the provided SOR-like formula:\n       $$ T_{i,j} \\leftarrow (1-\\omega_{i,j})\\,T_{i,j}+\\omega_{i,j}\\,\\frac{a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}}{A_P} $$\n    b. The sweep counter is incremented.\n    c. After the sweep, the residual $r_{i,j}$ is computed for all interior nodes using the correct formula.\n    d. The infinity norm of the residual, $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$, is calculated.\n    e. If $\\|r\\|_{\\infty}  \\tau$, the process has converged. The loop terminates, and the current sweep count is the result.\n\nThis procedure is repeated for each of the three test cases. The special grid size of $N_x=3, N_y=3$ results in a single interior node. This simplifies the problem significantly. For Cases $1$ and $3$, the relaxation parameter at the interior node evaluates to $\\omega_{1,1} = \\omega_{\\min} = 1.0$, which corresponds to a Gauss-Seidel iteration. For a system with a single variable, this converges in exactly one iteration. For Case $2$, the medium is homogeneous, and the relaxation parameter is a constant $\\omega > 1$, requiring multiple iterations to converge. The Python implementation in the final answer formalizes this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(Nx, Ny, s, k_left, k_right, omega_min, omega_max, tau):\n    \"\"\"\n    Solves the heat diffusion problem for a single test case.\n    \"\"\"\n    # Grid setup\n    if Nx = 2 or Ny = 2:\n        return 0  # No interior points to iterate on\n\n    h = 1.0 / (Nx - 1)\n    if not np.isclose(h, 1.0 / (Ny - 1)):\n        # Problem assumes hx=hy=h, this is a check.\n        # This part of code will not be reached for the given test cases.\n        raise ValueError(\"Grid spacing hx and hy must be equal.\")\n\n    # Initialize temperature field T[i, j]\n    T = np.zeros((Nx, Ny), dtype=np.float64)\n    \n    # Boundary conditions. Order of assignment sets corner values. Last one wins.\n    # Set T=0 on y=0, y=1, and x=1 boundaries.\n    T[:, 0] = 0.0\n    T[:, -1] = 0.0\n    T[-1, :] = 0.0\n    # Set T=1 on x=0 boundary.\n    T[0, :] = 1.0\n    \n    # Initial guess for interior points is already 0 from np.zeros.\n\n    # Conductivity field k[i, j]\n    k = np.zeros((Nx, Ny), dtype=np.float64)\n    x_coords = np.linspace(0, 1, Nx)\n    for i in range(Nx):\n        if x_coords[i] = s:\n            k[i, :] = k_left\n        else:\n            k[i, :] = k_right\n\n    # Relaxation parameter field omega[i, j]\n    omega = np.zeros((Nx, Ny), dtype=np.float64)\n    k_min_val = np.min(k)\n    k_max_val = np.max(k)\n    if not np.isclose(k_max_val, k_min_val):\n        omega = omega_min + (omega_max - omega_min) * (k - k_min_val) / (k_max_val - k_min_val)\n    else:\n        omega.fill((omega_min + omega_max) / 2.0)\n\n    # Pre-compute coefficients for interior nodes\n    num_interior_i = Nx - 2\n    num_interior_j = Ny - 2\n    aE = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    aW = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    aN = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    aS = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    AP = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n\n    for i_int in range(num_interior_i):\n        for j_int in range(num_interior_j):\n            i, j = i_int + 1, j_int + 1\n            # Harmonic mean for face conductivities\n            k_ip_half_j = 2 * k[i, j] * k[i + 1, j] / (k[i, j] + k[i + 1, j])\n            k_im_half_j = 2 * k[i, j] * k[i - 1, j] / (k[i, j] + k[i - 1, j])\n            k_i_jp_half = 2 * k[i, j] * k[i, j + 1] / (k[i, j] + k[i, j + 1])\n            k_i_jm_half = 2 * k[i, j] * k[i, j - 1] / (k[i, j] + k[i, j - 1])\n            \n            # Coefficients\n            aE[i_int, j_int] = k_ip_half_j / h**2\n            aW[i_int, j_int] = k_im_half_j / h**2\n            aN[i_int, j_int] = k_i_jp_half / h**2\n            aS[i_int, j_int] = k_i_jm_half / h**2\n            AP[i_int, j_int] = aE[i_int, j_int] + aW[i_int, j_int] + aN[i_int, j_int] + aS[i_int, j_int]\n\n    sweeps = 0\n    max_sweeps = 50000 # Safety limit\n\n    while sweeps  max_sweeps:\n        # Perform a sweep over interior nodes (in-place lexicographic update)\n        for i_int in range(num_interior_i):\n            for j_int in range(num_interior_j):\n                i, j = i_int + 1, j_int + 1\n                \n                ap_val = AP[i_int, j_int]\n                if ap_val > 0:\n                    gs_update = (aE[i_int, j_int] * T[i + 1, j] + \n                                 aW[i_int, j_int] * T[i - 1, j] + \n                                 aN[i_int, j_int] * T[i, j + 1] + \n                                 aS[i_int, j_int] * T[i, j - 1]) / ap_val\n                    T[i, j] = (1 - omega[i, j]) * T[i, j] + omega[i, j] * gs_update\n        \n        sweeps += 1\n\n        # After sweep, compute residual norm\n        max_r = 0.0\n        for i_int in range(num_interior_i):\n            for j_int in range(num_interior_j):\n                i, j = i_int + 1, j_int + 1\n                # Using the corrected residual formula\n                r_ij = (aE[i_int, j_int] * T[i + 1, j] + \n                        aW[i_int, j_int] * T[i - 1, j] + \n                        aN[i_int, j_int] * T[i, j + 1] + \n                        aS[i_int, j_int] * T[i, j - 1] - \n                        AP[i_int, j_int] * T[i, j])\n                \n                if abs(r_ij) > max_r:\n                    max_r = abs(r_ij)\n\n        if max_r  tau:\n            break\n            \n    return sweeps\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (Nx,Ny), s, k_left, k_right, omega_min, omega_max, tau\n        ((3, 3), 0.5, 1.0, 5.0, 1.0, 1.95, 1.0e-8),\n        # Case 2\n        ((3, 3), 0.5, 3.0, 3.0, 1.0, 1.95, 1.0e-8),\n        # Case 3\n        ((3, 3), 0.5, 1.0, 100.0, 1.0, 1.95, 1.0e-8),\n    ]\n\n    results = []\n    for case in test_cases:\n        (Nx, Ny), s, k_left, k_right, omega_min, omega_max, tau = case\n        num_sweeps = solve_case(Nx, Ny, s, k_left, k_right, omega_min, omega_max, tau)\n        results.append(num_sweeps)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441080"}, {"introduction": "Our final practice presents a crucial and counter-intuitive lesson that every computational scientist must learn: theoretical optimality does not always guarantee practical performance. This thought-provoking exercise guides you to construct a system where, due to the finite precision of computer arithmetic, the theoretically optimal relaxation parameter $\\omega_{\\mathrm{opt}}$ actually performs much worse than the simple, unrelaxed Gauss-Seidel method. This problem will sharpen your critical thinking and deepen your appreciation for the subtle yet profound interplay between numerical algorithms and the realities of machine computation [@problem_id:2441049].", "problem": "You are tasked with constructing and analyzing a linear system for which the theoretically optimal relaxation parameter for Successive Over-Relaxation (SOR) performs worse than the unrelaxed Gaussâ€“Seidel method in practical floating-point computation. Consider the family of symmetric positive definite matrices\n$$\nA_M \\;=\\; \\begin{pmatrix}\nM  -(M-1) \\\\\n-(M-1)  M\n\\end{pmatrix},\n$$\nwith the right-hand side\n$$\nb \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nand the initial guess\n$$\nx^{(0)} \\;=\\; \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nAssume the following.\n\n- In exact real arithmetic, analyze the spectral properties of the Jacobi iteration matrix associated with $A_M$. Using only core definitions of Jacobi and SOR iterations and their spectral radii, derive from first principles the theoretically optimal relaxation parameter $\\,\\omega_{\\mathrm{opt}}(M)\\,$ that minimizes the SOR spectral radius, expressed in closed form as a function of $M$.\n\n- Then, consider practical computation under the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) double-precision format with rounding to nearest. Treat the matrix entries as first stored in this format before any iteration begins. Using the standard floating-point spacing near $M$, argue that the stored matrix is effectively\n$$\n\\widetilde{A}_M \\;=\\; \\begin{pmatrix} M  -M \\\\ -M  M \\end{pmatrix}.\n$$\nUsing the exact SOR update equations applied to the stored $\\widetilde{A}_M$ and the given $b$ and $x^{(0)}$, compute and compare the first-step residuals $\\,r^{(1)}(\\omega) \\,=\\, b - \\widetilde{A}_M x^{(1)}(\\omega)\\,$ for $\\,\\omega = 1\\,$ and for $\\,\\omega = \\omega_{\\mathrm{opt}}(M)\\,$ obtained in the exact-arithmetic analysis. Explain why, due solely to floating-point representation of $A_M$, the theoretically optimal $\\,\\omega_{\\mathrm{opt}}(M)\\,$ performs worse than $\\,\\omega=1\\,$ on the stored problem.\n\nYour final answer must be the closed-form expression for $\\,\\omega_{\\mathrm{opt}}(M)\\,$ as a function of $M$, with no numerical evaluation. Express the final answer as a single analytic expression with no units. Do not include any additional commentary in the final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. There are no contradictions or missing information. We may proceed with the solution. The analysis is divided into two parts: first, the derivation under exact arithmetic, and second, the analysis under practical floating-point computation.\n\nPart 1: Exact Arithmetic Analysis and Derivation of $\\omega_{\\mathrm{opt}}(M)$\n\nWe are given the linear system $A_M x = b$, with the matrix\n$$\nA_M = \\begin{pmatrix}\nM  -(M-1) \\\\\n-(M-1)  M\n\\end{pmatrix}\n$$\nThe Successive Over-Relaxation (SOR) method is an iterative method for solving such systems. The convergence rate of SOR depends on the spectral radius of its iteration matrix, which in turn is related to the spectral radius of the Jacobi iteration matrix. We first derive the Jacobi matrix, $T_J$.\n\nThe matrix $A_M$ is decomposed as $A_M = D_M - L_M - U_M$, where $D_M$ is the diagonal part, $-L_M$ is the strictly lower triangular part, and $-U_M$ is the strictly upper triangular part of $A_M$.\n$$\nD_M = \\begin{pmatrix} M  0 \\\\ 0  M \\end{pmatrix}, \\quad L_M = \\begin{pmatrix} 0  0 \\\\ M-1  0 \\end{pmatrix}, \\quad U_M = \\begin{pmatrix} 0  M-1 \\\\ 0  0 \\end{pmatrix}\n$$\nThe Jacobi iteration matrix is defined as $T_J = D_M^{-1}(L_M + U_M)$.\n$$\nD_M^{-1} = \\begin{pmatrix} \\frac{1}{M}  0 \\\\ 0  \\frac{1}{M} \\end{pmatrix}\n$$\n$$\nL_M + U_M = \\begin{pmatrix} 0  M-1 \\\\ M-1  0 \\end{pmatrix}\n$$\nThus,\n$$\nT_J = \\begin{pmatrix} \\frac{1}{M}  0 \\\\ 0  \\frac{1}{M} \\end{pmatrix} \\begin{pmatrix} 0  M-1 \\\\ M-1  0 \\end{pmatrix} = \\begin{pmatrix} 0  \\frac{M-1}{M} \\\\ \\frac{M-1}{M}  0 \\end{pmatrix}\n$$\nTo find the optimal relaxation parameter $\\omega_{\\mathrm{opt}}$, we must first find the spectral radius of $T_J$, denoted $\\mu = \\rho(T_J)$. The spectral radius is the maximum of the absolute values of the eigenvalues of $T_J$. The eigenvalues $\\lambda_J$ of $T_J$ are the roots of the characteristic equation $\\det(T_J - \\lambda_J I) = 0$.\n$$\n\\det\\begin{pmatrix} -\\lambda_J  \\frac{M-1}{M} \\\\ \\frac{M-1}{M}  -\\lambda_J \\end{pmatrix} = (-\\lambda_J)^2 - \\left(\\frac{M-1}{M}\\right)^2 = 0\n$$\nThis gives $\\lambda_J^2 = \\left(\\frac{M-1}{M}\\right)^2$, so the eigenvalues are $\\lambda_J = \\pm \\frac{M-1}{M}$.\nAssuming $M1$, the spectral radius is\n$$\n\\mu = \\rho(T_J) = \\max\\left|\\pm \\frac{M-1}{M}\\right| = \\frac{M-1}{M}\n$$\nThe matrix $A_M$ is symmetric and has a \"Property A\" structure (more specifically, it is consistently ordered), which is a necessary condition for the standard theory of optimal SOR relaxation to apply. The theoretically optimal relaxation parameter $\\omega_{\\mathrm{opt}}$ that minimizes the spectral radius of the SOR iteration matrix is given by the formula:\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1 + \\sqrt{1 - \\mu^2}}\n$$\nWe substitute our expression for $\\mu$:\n$$\n1 - \\mu^2 = 1 - \\left(\\frac{M-1}{M}\\right)^2 = \\frac{M^2 - (M^2 - 2M + 1)}{M^2} = \\frac{2M - 1}{M^2}\n$$\nTaking the square root, we get:\n$$\n\\sqrt{1 - \\mu^2} = \\sqrt{\\frac{2M-1}{M^2}} = \\frac{\\sqrt{2M-1}}{M}\n$$\n(We assume $M > \\frac{1}{2}$ for the term to be real, which is consistent with the premise of large $M$).\nSubstituting this back into the formula for $\\omega_{\\mathrm{opt}}$:\n$$\n\\omega_{\\mathrm{opt}}(M) = \\frac{2}{1 + \\frac{\\sqrt{2M-1}}{M}} = \\frac{2M}{M + \\sqrt{2M-1}}\n$$\nThis is the closed-form expression for the theoretically optimal relaxation parameter based on exact arithmetic.\n\nPart 2: Analysis in Floating-Point Arithmetic\n\nThe problem posits that for a very large value of $M$, due to the limitations of IEEE 754 double-precision floating-point representation, the number $-(M-1)$ is indistinguishable from $-M$ and is stored as such. This transforms the matrix $A_M$ into a different matrix $\\widetilde{A}_M$ within the computer's memory:\n$$\nA_M = \\begin{pmatrix} M  -(M-1) \\\\ -(M-1)  M \\end{pmatrix} \\quad \\xrightarrow{\\text{storage}} \\quad \\widetilde{A}_M = \\begin{pmatrix} M  -M \\\\ -M  M \\end{pmatrix}\n$$\nThe computational task is therefore to solve the system $\\widetilde{A}_M x = b$ starting from $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Note that $\\det(\\widetilde{A}_M) = M^2 - (-M)(-M) = 0$, so the stored matrix is singular. The system is consistent because $b = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ lies in the column space of $\\widetilde{A}_M$, which is spanned by the vector $\\begin{pmatrix} M \\\\ -M \\end{pmatrix}$.\n\nWe now apply one step of the SOR iteration to this stored system. The component-wise SOR update formula is:\n$$\nx_i^{(k+1)} = (1-\\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij}x_j^{(k+1)} - \\sum_{ji} a_{ij}x_j^{(k)} \\right)\n$$\nUsing $\\widetilde{A}_M$, $b = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, and $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we compute $x^{(1)}(\\omega) = \\begin{pmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\end{pmatrix}$.\n\nFor $i=1$:\n$$\nx_1^{(1)} = (1-\\omega)x_1^{(0)} + \\frac{\\omega}{\\widetilde{a}_{11}} \\left( b_1 - \\widetilde{a}_{12}x_2^{(0)} \\right) = (1-\\omega)(0) + \\frac{\\omega}{M} \\left( 1 - (-M)(0) \\right) = \\frac{\\omega}{M}\n$$\nFor $i=2$:\n$$\nx_2^{(1)} = (1-\\omega)x_2^{(0)} + \\frac{\\omega}{\\widetilde{a}_{22}} \\left( b_2 - \\widetilde{a}_{21}x_1^{(1)} \\right) = (1-\\omega)(0) + \\frac{\\omega}{M} \\left( -1 - (-M)x_1^{(1)} \\right)\n$$\nSubstituting the value of $x_1^{(1)}$:\n$$\nx_2^{(1)} = \\frac{\\omega}{M} \\left( -1 + M\\left(\\frac{\\omega}{M}\\right) \\right) = \\frac{\\omega}{M}(-1 + \\omega) = \\frac{\\omega(\\omega-1)}{M}\n$$\nSo, the first iterate is $x^{(1)}(\\omega) = \\frac{1}{M}\\begin{pmatrix} \\omega \\\\ \\omega(\\omega-1) \\end{pmatrix}$.\n\nNext, we compute the residual after the first step, $r^{(1)}(\\omega) = b - \\widetilde{A}_M x^{(1)}(\\omega)$.\n$$\n\\widetilde{A}_M x^{(1)}(\\omega) = \\begin{pmatrix} M  -M \\\\ -M  M \\end{pmatrix} \\frac{1}{M} \\begin{pmatrix} \\omega \\\\ \\omega(\\omega-1) \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} \\omega \\\\ \\omega(\\omega-1) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\omega - \\omega(\\omega-1) \\\\ -\\omega + \\omega(\\omega-1) \\end{pmatrix} = \\begin{pmatrix} \\omega - \\omega^2 + \\omega \\\\ -\\omega + \\omega^2 - \\omega \\end{pmatrix} = \\begin{pmatrix} 2\\omega - \\omega^2 \\\\ \\omega^2 - 2\\omega \\end{pmatrix}\n$$\nThe residual is:\n$$\nr^{(1)}(\\omega) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 2\\omega - \\omega^2 \\\\ \\omega^2 - 2\\omega \\end{pmatrix} = \\begin{pmatrix} 1 - 2\\omega + \\omega^2 \\\\ -1 - (\\omega^2 - 2\\omega) \\end{pmatrix} = \\begin{pmatrix} (1-\\omega)^2 \\\\ -(1-\\omega)^2 \\end{pmatrix} = (1-\\omega)^2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nNow we compare the performance for $\\omega=1$ (Gauss-Seidel) and $\\omega=\\omega_{\\mathrm{opt}}(M)$.\n\nCase $\\omega=1$ (Gauss-Seidel):\nThe residual is $r^{(1)}(1) = (1-1)^2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The Gauss-Seidel method converges to an exact solution of the stored system $\\widetilde{A}_M x = b$ in a single iteration, yielding a zero residual.\n\nCase $\\omega=\\omega_{\\mathrm{opt}}(M)$:\nThe residual is $r^{(1)}(\\omega_{\\mathrm{opt}}) = (1-\\omega_{\\mathrm{opt}})^2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nFrom Part 1, $\\omega_{\\mathrm{opt}}(M) = \\frac{2M}{M + \\sqrt{2M-1}}$. For $M1$, $M > \\sqrt{2M-1}$, which implies $2M > M + \\sqrt{2M-1}$ and $\\omega_{\\mathrm{opt}}(M) > 1$. Specifically, $\\omega_{\\mathrm{opt}}(M)$ is never equal to $1$ for $M1/2$. Thus, $(1-\\omega_{\\mathrm{opt}})^2 \\neq 0$. The residual is non-zero. For large $M$, $\\omega_{\\mathrm{opt}}(M) \\to 2$, so $(1-\\omega_{\\mathrm{opt}})^2 \\to 1$, meaning the residual norm is almost unchanged from the initial residual norm.\n\nConclusion:\nThe parameter $\\omega_{\\mathrm{opt}}(M)$ was calculated to be optimal for the matrix $A_M$. However, the computation is performed with the matrix $\\widetilde{A}_M$. For this *actual* computational problem, the choice $\\omega=1$ results in a zero residual after one step, indicating convergence to an exact solution of the stored system. The choice $\\omega = \\omega_{\\mathrm{opt}}(M)$ results in a non-zero residual. Therefore, the theoretically optimal parameter performs significantly worse than the unrelaxed Gauss-Seidel method. This discrepancy arises because the theoretical optimization was performed on a model ($A_M$) that does not match the reality of the computation ($\\widetilde{A}_M$) due to the finite precision of floating-point arithmetic.", "answer": "$$\n\\boxed{\\frac{2M}{M + \\sqrt{2M-1}}}\n$$", "id": "2441049"}]}