{"hands_on_practices": [{"introduction": "Our journey into preconditioning begins with the fundamental question: what are we actually solving? Instead of tackling the original system $A\\mathbf{x} = \\mathbf{b}$ directly, preconditioning transforms it into an equivalent system that is easier for an iterative solver to handle. This first exercise [@problem_id:2179154] asks you to identify the correct form of a *left-preconditioned* system, a crucial first step for understanding and implementing methods like the Preconditioned Conjugate Gradient or GMRES.", "id": "2179154", "problem": "In scientific and engineering simulations, it is common to encounter the need to solve a large, sparse linear system of equations of the form $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a non-singular $n \\times n$ matrix, $\\mathbf{x}$ is the unknown vector of size $n$, and $\\mathbf{b}$ is a known vector of size $n$. Due to the size of the matrix $A$, direct methods like Gaussian elimination are often computationally prohibitive. Instead, iterative solvers such as the Generalized Minimal Residual (GMRES) method are used.\n\nTo accelerate the convergence of such iterative solvers, a technique called preconditioning is employed. This involves finding a matrix $M$, called the preconditioner, that is a \"good\" approximation of $A$ (i.e., $M \\approx A$) and for which the system $M\\mathbf{z} = \\mathbf{r}$ is easy to solve. One common method for constructing such a preconditioner for a sparse matrix $A$ is the Incomplete LU (ILU) factorization.\n\nConsider a scenario where we use a left preconditioner $M$ derived from an ILU factorization of $A$. The iterative solver is not applied to the original system $A\\mathbf{x} = \\mathbf{b}$, but to a mathematically equivalent, preconditioned system. Which one of the following options correctly represents the preconditioned system that the iterative solver is applied to?\n\nA. $A M^{-1} \\mathbf{y} = \\mathbf{b}$, where the original solution is recovered via $\\mathbf{x} = M^{-1}\\mathbf{y}$.\nB. $M^{-1} A \\mathbf{x} = M^{-1} \\mathbf{b}$.\nC. $M A \\mathbf{x} = M \\mathbf{b}$.\nD. $\\mathbf{x} = M^{-1} \\mathbf{b}$.\nE. $A \\mathbf{x} = M^{-1} \\mathbf{b}$.\n\n", "solution": "We start from the original linear system $A\\mathbf{x}=\\mathbf{b}$ with a nonsingular preconditioner $M\\approx A$ obtained, for example, from an ILU factorization. A left preconditioner means we apply $M^{-1}$ to the left of the system, yielding the mathematically equivalent system\n$$\nM^{-1}A\\mathbf{x}=M^{-1}\\mathbf{b}.\n$$\nIn practice, we never form $M^{-1}$ explicitly; instead, at each iteration of the Krylov method (e.g., GMRES), we apply $M^{-1}$ to a vector $\\mathbf{r}$ by solving the triangular system $M\\mathbf{z}=\\mathbf{r}$. Thus, the iterative solver is applied to the operator $M^{-1}A$ with right-hand side $M^{-1}\\mathbf{b}$, and the unknown remains $\\mathbf{x}$ of the original problem. This is precisely represented by option B.\n\nTo distinguish other options:\n- Option A corresponds to right preconditioning. Setting $\\mathbf{x}=M^{-1}\\mathbf{y}$ transforms $A\\mathbf{x}=\\mathbf{b}$ into $AM^{-1}\\mathbf{y}=\\mathbf{b}$, with recovery $\\mathbf{x}=M^{-1}\\mathbf{y}$. That is not left preconditioning.\n- Option C multiplies by $M$ on the left, yielding $MA\\mathbf{x}=M\\mathbf{b}$, which is not the standard preconditioned form used because iterative methods apply $M^{-1}$ via solves, not $M$ via multiplication.\n- Option D ignores $A$ and sets $\\mathbf{x}=M^{-1}\\mathbf{b}$, which is only valid if $M=A$ exactly.\n- Option E alters only the right-hand side to $M^{-1}\\mathbf{b}$ without changing the operator, which is not a standard preconditioning transformation and is not equivalent to left preconditioning.\n\nTherefore, the correct preconditioned system for a left preconditioner is $M^{-1}A\\mathbf{x}=M^{-1}\\mathbf{b}$, corresponding to option B.", "answer": "$$\\boxed{B}$$"}, {"introduction": "Incomplete LU (ILU) factorization is a powerful and widely used preconditioning technique, prized for its ability to approximate a matrix's inverse efficiently while preserving sparsity. However, the 'incompleteness'—the deliberate dropping of non-zero entries (fill-in)—can lead to numerical instability. This practice problem [@problem_id:2179131] provides a hands-on calculation to demonstrate how an ILU(0) factorization can fail by generating a zero pivot, even for a non-singular matrix where a full LU factorization would succeed.", "id": "2179131", "problem": "The Incomplete LU (ILU) factorization is a widely used preconditioning technique for iterative solvers of large sparse linear systems, $Ax=b$. The simplest variant, ILU(0), computes an approximate factorization $\\tilde{L}\\tilde{U} \\approx A$, where $\\tilde{L}$ is a unit lower triangular matrix and $\\tilde{U}$ is an upper triangular matrix. The key constraint of ILU(0) is that $\\tilde{L}$ and $\\tilde{U}$ must have the same sparsity pattern as the lower and upper triangular parts of the original matrix $A$, respectively. This means that if an entry $a_{ij}$ is zero in the original matrix, the corresponding entry in $\\tilde{L}$ or $\\tilde{U}$ is also forced to be zero. This is in contrast to a full LU factorization, where the process of Gaussian elimination can introduce new non-zero entries, known as \"fill-in\".\n\nA known issue with ILU(0) is that the factorization process can fail due to division by a zero pivot (a zero diagonal entry in $\\tilde{U}$), even for a non-singular matrix $A$ for which the exact LU factorization would proceed without encountering any zero pivots (assuming no row interchanges).\n\nConsider the following $3 \\times 3$ matrix $A$ which depends on a real parameter $k$:\n$$\nA = \\begin{pmatrix} 2 & 1 & 2 \\\\ 2 & 2 & 0 \\\\ 1 & 2 & k \\end{pmatrix}\n$$\nFind the value of $k$ for which the ILU(0) factorization of $A$ fails due to the emergence of a zero on the diagonal of $\\tilde{U}$, but for which the matrix $A$ is non-singular.\n\nA. $k = -1$\nB. $k = 0$\nC. $k = 1$\nD. $k = 2$\nE. $k = 4$\n\n", "solution": "We perform ILU(0) factorization of the given matrix\n$$\nA=\\begin{pmatrix}2 & 1 & 2\\\\ 2 & 2 & 0\\\\ 1 & 2 & k\\end{pmatrix},\n$$\nwith the constraint that the sparsity pattern of $\\tilde{L}$ and $\\tilde{U}$ matches the strictly lower and upper triangular parts of $A$, respectively. In particular, since $a_{2,3}=0$, ILU(0) enforces $\\tilde{U}_{2,3}=0$, even though standard LU would generally produce a nonzero fill-in there.\n\nLet $\\tilde{L}$ be unit lower triangular with entries\n$$\n\\tilde{L}=\\begin{pmatrix}1 & 0 & 0\\\\ \\ell_{21} & 1 & 0\\\\ \\ell_{31} & \\ell_{32} & 1\\end{pmatrix},\n$$\nand $\\tilde{U}$ be upper triangular with the same upper sparsity as $A$:\n$$\n\\tilde{U}=\\begin{pmatrix}u_{11} & u_{12} & u_{13}\\\\ 0 & u_{22} & 0\\\\ 0 & 0 & u_{33}\\end{pmatrix}.\n$$\nThe incomplete LU(0) formulas (Gauss elimination with dropped fill-ins outside the pattern) are:\n- For $j\\geq i$ and $(i,j)$ in the upper pattern of $A$,\n$$\nu_{ij}=a_{ij}-\\sum_{k=1}^{i-1}\\ell_{ik}u_{kj}.\n$$\n- For $j>i$ and $(j,i)$ in the lower pattern of $A$,\n$$\n\\ell_{ji}=\\frac{a_{ji}-\\sum_{k=1}^{i-1}\\ell_{jk}u_{ki}}{u_{ii}}.\n$$\n\nStep $i=1$:\n$$\nu_{11}=a_{11}=2,\\quad u_{12}=a_{12}=1,\\quad u_{13}=a_{13}=2,\n$$\n$$\n\\ell_{21}=\\frac{a_{21}}{u_{11}}=\\frac{2}{2}=1,\\quad \\ell_{31}=\\frac{a_{31}}{u_{11}}=\\frac{1}{2}.\n$$\n\nStep $i=2$:\n$$\nu_{22}=a_{22}-\\ell_{21}u_{12}=2-1\\cdot 1=1.\n$$\nFor position $(2,3)$, ILU(0) enforces the original zero:\n$$\nu_{23}=0\\quad\\text{(by sparsity constraint, since }a_{23}=0\\text{)}.\n$$\nCompute $\\ell_{32}$ using only the allowed sum:\n$$\n\\ell_{32}=\\frac{a_{32}-\\ell_{31}u_{12}}{u_{22}}=\\frac{2-\\frac{1}{2}\\cdot 1}{1}=\\frac{3}{2}.\n$$\n\nStep $i=3$:\n$$\nu_{33}=a_{33}-\\ell_{31}u_{13}-\\ell_{32}u_{23}=k-\\frac{1}{2}\\cdot 2-\\frac{3}{2}\\cdot 0=k-1.\n$$\n\nILU(0) fails if a zero pivot appears, i.e., when $u_{33}=0$, which gives\n$$\nk-1=0\\quad\\Rightarrow\\quad k=1.\n$$\nWe must also ensure that $A$ is nonsingular at this $k$. The determinant is\n$$\n\\det(A)=\\begin{vmatrix}2 & 1 & 2\\\\ 2 & 2 & 0\\\\ 1 & 2 & k\\end{vmatrix}\n=2\\begin{vmatrix}2 & 0\\\\ 2 & k\\end{vmatrix}\n-1\\begin{vmatrix}2 & 0\\\\ 1 & k\\end{vmatrix}\n+2\\begin{vmatrix}2 & 2\\\\ 1 & 2\\end{vmatrix}\n=2(2k)-1(2k)+2(4-2)=4k-2k+4=2(k+2).\n$$\nThus $A$ is nonsingular for all $k\\neq -2$, and in particular at $k=1$ we have $\\det(A)=6\\neq 0$.\n\nThis also highlights the contrast with full LU: without dropping fill-in, one would obtain $u_{23}=a_{23}-\\ell_{21}u_{13}=0-1\\cdot 2=-2$ and hence\n$$\nu_{33}^{\\text{(full LU)}}=k-\\ell_{31}u_{13}-\\ell_{32}u_{23}=k-1-\\frac{3}{2}(-2)=k+2,\n$$\nwhich only vanishes when $k=-2$, exactly the singular case. Therefore, ILU(0) fails at $k=1$ even though $A$ is nonsingular.\n\nHence the correct option is C.", "answer": "$$\\boxed{C}$$"}, {"introduction": "Theory and small-scale calculations provide the foundation, but the true test of a numerical method lies in its implementation and performance on realistic problems. This final, comprehensive exercise [@problem_id:2427808] challenges you to build a Preconditioned Conjugate Gradient solver and evaluate a suite of preconditioners, from the simple Jacobi method to the more sophisticated Incomplete Cholesky (IC) factorization. By experimenting with different floating-point precisions and increasingly challenging problems, you will gain practical insight into the critical trade-offs between computational cost, numerical stability, and convergence speed that define real-world scientific computing.", "id": "2427808", "problem": "You are given a sequence of symmetric positive definite linear systems of the form $A x = b$ arising from a $2$-dimensional, $5$-point finite difference discretization of an anisotropic diffusion operator on a square grid with Dirichlet boundary conditions. Let $m$ denote the number of interior points per coordinate direction (so the dimension is $n = m^2$). The coefficient matrix $A$ is formed as $A = I_m \\otimes T_x + T_y \\otimes I_m$, where $T_x$ and $T_y$ are tridiagonal matrices with constant diagonals defined by $T_x = \\text{tridiag}(-a_x, 2 a_x, -a_x)$ and $T_y = \\text{tridiag}(-a_y, 2 a_y, -a_y)$, for strictly positive $a_x$ and $a_y$. The right-hand side $b$ is the vector of all ones in $\\mathbb{R}^n$. Your task is to implement a Preconditioned Conjugate Gradient method with different preconditioners and investigate how floating point precision (single versus double versus mixed) affects preconditioner stability and utility as measured by iteration counts to a fixed accuracy.\n\nStarting from fundamental bases:\n- The Conjugate Gradient method solves $A x = b$ for symmetric positive definite $A$ by iteratively minimizing the $A$-energy norm of the error over expanding Krylov subspaces generated by $A$ and the residual, using orthogonality of residuals and $A$-conjugacy of directions. \n- A left preconditioner $M$ that is symmetric positive definite transforms the system to $M^{-1} A x = M^{-1} b$, changing the inner product to $\\langle u, v \\rangle_M = u^\\top M v$, which in practice reduces the condition number of the operator applied in the Krylov process and thus improves convergence rates when $M$ is a good approximation to $A$.\n- A standard sparse approximate factorization for symmetric positive definite matrices is incomplete Cholesky factorization with zero fill, denoted $\\text{IC}(0)$, which computes a lower triangular factor $L$ constrained to the lower-triangular sparsity pattern of $A$ such that $L L^\\top \\approx A$. Applying the preconditioner corresponds to computing $z = M^{-1} r$ by two triangular solves $L y = r$, $L^\\top z = y$.\n\nImplement the following preconditioners:\n- No preconditioner: $M = I$.\n- Jacobi preconditioner: $M = \\mathrm{diag}(A)$.\n- Incomplete Cholesky with zero fill computed in double precision: $M = L_{64} L_{64}^\\top$, where $L_{64}$ is computed using $64$-bit floating point arithmetic.\n- Incomplete Cholesky with zero fill computed in single precision with single-precision triangular solves: $M = L_{32} L_{32}^\\top$, where $L_{32}$ is computed using $32$-bit floating point arithmetic and applied using $32$-bit arithmetic in the triangular solves.\n- Mixed-precision incomplete Cholesky: compute $L_{32}$ as above but apply triangular solves in $64$-bit arithmetic (by casting the stored factor to $64$-bit once before solves).\n\nAll iterations, vector inner products, and convergence tests of the Preconditioned Conjugate Gradient method must be performed in $64$-bit arithmetic. Use the relative residual stopping criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\tau$, with $\\tau = 10^{-8}$. If the method fails to satisfy the stopping criterion within $k_{\\max} = 5000$ iterations, return $5000$ as the iteration count for that run.\n\nFor $\\text{IC}(0)$, if at any step a nonpositive diagonal is encountered due to finite precision effects, stabilize it by adding a small positive diagonal shift proportional to the local diagonal magnitude. Specifically, if the tentative value at step $i$ computed in the factorization is not strictly positive, replace it with $d_i + \\delta$, where $d_i$ is the diagonal entry of $A$ at row $i$, and $\\delta$ is a small positive quantity on the order of $\\sqrt{\\varepsilon}$ (where $\\varepsilon$ is machine epsilon in the working precision). Record how many such stabilizations occur when computing $L_{32}$.\n\nTest suite. For each of the following parameter sets $(m, a_x, a_y)$:\n- Case $1$: $m = 16$, $a_x = 1$, $a_y = 1$.\n- Case $2$: $m = 16$, $a_x = 1$, $a_y = 10^{-3}$.\n- Case $3$: $m = 16$, $a_x = 1$, $a_y = 10^{-6}$.\n\ndo the following for each case:\n- Assemble $A$ in compressed sparse row form.\n- Run the Preconditioned Conjugate Gradient method for each of the five preconditioners defined above, all with the same $b$ (the vector of ones), tolerance $\\tau = 10^{-8}$, and maximum iterations $k_{\\max} = 5000$.\n- Record the number of iterations taken to satisfy the stopping criterion for each preconditioner, and record the stabilization count used only for the single-precision incomplete Cholesky computation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of six integers in the order\n$[\\text{iters\\_none}, \\text{iters\\_jacobi}, \\text{iters\\_ic64}, \\text{iters\\_ic32}, \\text{iters\\_icmix}, \\text{stabilizations\\_ic32}]$.\nFor example, an output for three cases has the form\n$[[n_{1,1}, n_{1,2}, n_{1,3}, n_{1,4}, n_{1,5}, s_1],[n_{2,1}, n_{2,2}, n_{2,3}, n_{2,4}, n_{2,5}, s_2],[n_{3,1}, n_{3,2}, n_{3,3}, n_{3,4}, n_{3,5}, s_3]]$\nwhere each $n_{i,j}$ is an integer iteration count and each $s_i$ is the integer stabilization count for single-precision $\\text{IC}(0)$ in case $i$.\n\nAngle units do not apply. Physical units do not apply. All numerical quantities in the answer must be unitless.", "solution": "The problem posed is a well-defined exercise in computational science, requiring the implementation and comparison of preconditioning strategies for the Conjugate Gradient method. It is scientifically grounded, internally consistent, and requires no information beyond what is provided. The problem is valid.\n\nWe are tasked with solving the linear system $A x = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=m^2$ arises from a $5$-point finite difference discretization of an anisotropic diffusion operator on a uniform $m \\times m$ grid of interior points. The matrix $A$ is symmetric positive definite (SPD) and is given by the Kronecker sum $A = I_m \\otimes T_x + T_y \\otimes I_m$. The matrices $T_x, T_y \\in \\mathbb{R}^{m \\times m}$ are tridiagonal, specified as $T_x = \\text{tridiag}(-a_x, 2 a_x, -a_x)$ and $T_y = \\text{tridiag}(-a_y, 2 a_y, -a_y)$, with $a_x, a_y > 0$. The right-hand side vector $b$ is the vector of all ones.\n\nThe solution will proceed in three stages:\n1.  Construction of the matrix $A$ in a sparse format suitable for efficient computation.\n2.  Implementation of the Preconditioned Conjugate Gradient (PCG) algorithm.\n3.  Implementation of the various preconditioners, with particular attention to the Incomplete Cholesky factorization and its numerical stability under different floating-point precisions.\n\n**1. Matrix Assembly**\n\nThe matrix $A$ is constructed using the Kronecker product. Given $m$, $a_x$, and $a_y$, the matrices $T_x$ and $T_y$ are assembled as sparse tridiagonal matrices. Using the properties of the Kronecker product, $I_m \\otimes T_x$ yields a block-diagonal matrix where each block is $T_x$. Similarly, $T_y \\otimes I_m$ produces a block-tridiagonal matrix where the diagonal blocks are $2a_y I_m$ and the off-diagonal blocks are $-a_y I_m$. Their sum gives the matrix $A$, which has a block-tridiagonal structure and contains at most $5$ non-zero elements per row. All computations for assembling $A$ are performed in $64$-bit precision (double precision). The resulting matrix is stored in the Compressed Sparse Row (CSR) format.\n\n**2. The Preconditioned Conjugate Gradient (PCG) Method**\n\nThe PCG method is an iterative algorithm for solving SPD linear systems. It improves upon the Conjugate Gradient method by using a preconditioner $M \\approx A$ to transform the system, accelerating convergence by clustering the eigenvalues of the iteration matrix $M^{-1}A$. The algorithm, performed entirely in $64$-bit arithmetic, is as follows:\n\nGiven an initial guess $x_0$ (we use $x_0 = \\mathbf{0}$), a tolerance $\\tau = 10^{-8}$, and a maximum of $k_{\\max} = 5000$ iterations:\n1.  Initialize residual: $r_0 = b - A x_0$.\n2.  Apply preconditioner: $z_0 = M^{-1} r_0$.\n3.  Set initial search direction: $p_0 = z_0$.\n4.  For $k=0, 1, 2, \\dots$ until convergence or $k_{\\max}$:\n    a. Compute step size: $\\alpha_k = \\frac{r_k^\\top z_k}{p_k^\\top A p_k}$.\n    b. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    c. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n    d. Check for convergence: if $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\tau$, stop and return $k+1$.\n    e. Apply preconditioner: $z_{k+1} = M^{-1} r_{k+1}$.\n    f. Update search direction: $\\beta_k = \\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k}$, then $p_{k+1} = z_{k+1} + \\beta_k p_k$.\n\nThe core of the investigation lies in the implementation of the preconditioning step $z = M^{-1} r$.\n\n**3. Preconditioner Implementations**\n\nWe implement five different choices for the preconditioner $M$.\n\n-   **No Preconditioner**: $M = I$, the identity matrix. The operation $M^{-1}r$ is simply $z=r$. This corresponds to the standard Conjugate Gradient method.\n-   **Jacobi Preconditioner**: $M = \\mathrm{diag}(A)$. This is a simple diagonal scaling. For the given matrix $A$, the diagonal is constant and equal to $2(a_x+a_y)$. The operation $M^{-1}r$ is a component-wise division: $z_i = r_i / (2(a_x+a_y))$.\n-   **Incomplete Cholesky Factorization with Zero Fill (IC(0))**: This preconditioner computes an approximate Cholesky factorization $M = LL^\\top \\approx A$, where the lower triangular factor $L$ is constrained to have the same sparsity pattern as the lower part of $A$. The preconditioning step $z = M^{-1}r$ is performed by solving two triangular systems: $Ly=r$ (forward substitution) and $L^\\top z=y$ (backward substitution).\n\n    The IC(0) factor $L$ is computed via the following algorithm. For $i=0, \\dots, n-1$:\n    $$l_{ij} = \\frac{1}{l_{jj}} \\left( a_{ij} - \\sum_{k=0}^{j-1} l_{ik} l_{jk} \\right) \\quad \\text{for } j < i \\text{ and } a_{ij} \\ne 0$$\n    $$l_{ii} = \\sqrt{a_{ii} - \\sum_{k=0}^{i-1} l_{ik}^2}$$\n    For the graph of the $5$-point stencil matrix, which contains no cycles of length $3$, the sum $\\sum_{k<j} l_{ik} l_{jk}$ is always zero. This simplifies the computation of off-diagonal elements to $l_{ij} = a_{ij} / l_{jj}$.\n\n    In finite precision arithmetic, the term under the square root for $l_{ii}$ can become non-positive. To prevent breakdown, we stabilize the factorization. If $a_{ii} - \\sum_{k<i} l_{ik}^2 \\le 0$, we replace the diagonal value with $d_i + \\delta$, where $d_i$ is the original diagonal entry of $A$ and $\\delta = \\sqrt{\\varepsilon}$, with $\\varepsilon$ being the machine epsilon of the working precision.\n\n    We implement three variants of the IC(0) preconditioner:\n    1.  **Double Precision IC(0)**: The factor $L_{64}$ is computed and applied entirely in $64$-bit arithmetic.\n    2.  **Single Precision IC(0)**: The matrix $A$ is cast to $32$-bit precision. The factor $L_{32}$ is computed in $32$-bit precision (with stabilization using $\\varepsilon_{32}$), and the number of stabilizations is recorded. The triangular solves for $z=M^{-1}r$ are also performed in $32$-bit precision: the $64$-bit residual $r$ is cast to $32$-bit, the solves are performed, and the resulting $32$-bit vector $z$ is cast back to $64$-bit.\n    3.  **Mixed Precision IC(0)**: The factor $L_{32}$ is computed as in the single-precision case. However, it is then cast to a $64$-bit matrix before the PCG iteration begins. The triangular solves are subsequently performed entirely in $64$-bit arithmetic. This approach aims to benefit from the faster computation of the $L_{32}$ factor while maintaining the numerical stability of $64$-bit solves.\n\nThe following Python code implements this complete procedure for the specified test cases.", "answer": "```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import spsolve_triangular\n\ndef assemble_A(m, ax, ay):\n    \"\"\"\n    Assembles the 2D anisotropic diffusion matrix A using Kronecker products.\n    A = I_m kron T_x + T_y kron I_m\n    \"\"\"\n    n = m * m\n    \n    # Define T_x and T_y matrices\n    tx_diagonals = [-ax * np.ones(m - 1), 2 * ax * np.ones(m), -ax * np.ones(m - 1)]\n    ty_diagonals = [-ay * np.ones(m - 1), 2 * ay * np.ones(m), -ay * np.ones(m - 1)]\n    \n    Tx = sp.diags(tx_diagonals, [-1, 0, 1], shape=(m, m), format='csr', dtype=np.float64)\n    Ty = sp.diags(ty_diagonals, [-1, 0, 1], shape=(m, m), format='csr', dtype=np.float64)\n    \n    Im = sp.eye(m, dtype=np.float64)\n    \n    A = sp.kron(Im, Tx) + sp.kron(Ty, Im)\n    return A.tocsr()\n\ndef ic0_factor(A, precision):\n    \"\"\"\n    Computes the Incomplete Cholesky factorization with zero fill (IC(0)).\n    This implementation is correct for matrices where the graph has no cycles of length 3,\n    such as those from a 5-point finite difference stencil.\n    \n    Args:\n        A (csr_matrix): The symmetric positive definite matrix to factorize.\n        precision (dtype): The floating-point precision (np.float32 or np.float64).\n    \n    Returns:\n        L (csr_matrix): The lower triangular IC(0) factor.\n        stabilization_count (int): Number of times stabilization was applied.\n    \"\"\"\n    n = A.shape[0]\n    # Use LIL format for easier, albeit slower, element-wise modification\n    L = A.copy().tolil().astype(precision)\n    A_diag_orig = A.diagonal().astype(precision)\n    \n    eps = np.finfo(precision).eps\n    delta = np.sqrt(eps)\n    stabilization_count = 0\n\n    for i in range(n):\n        # The sum over k in the formula for l_ij is zero for 5-point stencil graph\n        # because there are no triangles (cycles of length 3).\n        # So, l_ij = a_ij / l_jj\n        row_i_cols_lt_i = sorted([j for j in L.rows[i] if j < i])\n        \n        for j in row_i_cols_lt_i:\n            if L[j, j] != 0:\n                L[i, j] = L[i, j] / L[j, j]\n\n        # Compute diagonal element l_ii\n        # l_ii^2 = a_ii - sum_{k<i} l_ik^2\n        sum_sq = 0.0\n        for k in row_i_cols_lt_i:\n             sum_sq += L[i, k]**2\n        \n        diag_val = L[i, i] - sum_sq # L[i,i] still holds original A[i,i]\n\n        if diag_val <= 0:\n            diag_val = A_diag_orig[i] + delta\n            stabilization_count += 1\n        \n        L[i, i] = np.sqrt(diag_val)\n\n    # Return a clean lower-triangular CSR matrix\n    return sp.tril(L, format='csr'), stabilization_count\n\ndef pcg(A, b, precon_func, tol, max_iter):\n    \"\"\"\n    Preconditioned Conjugate Gradient solver.\n    All vector operations and tests are in 64-bit precision.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    r = b - A.dot(x)\n    z = precon_func(r)\n    p = z.copy()\n    \n    rs_old = np.dot(r, z)\n    b_norm = np.linalg.norm(b)\n    \n    for i in range(max_iter):\n        Ap = A.dot(p)\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        if np.linalg.norm(r) / b_norm <= tol:\n            return i + 1\n            \n        z = precon_func(r)\n        rs_new = np.dot(r, z)\n        \n        beta = rs_new / rs_old\n        p = z + beta * p\n        \n        rs_old = rs_new\n        \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (16, 1.0, 1.0),\n        (16, 1.0, 1e-3),\n        (16, 1.0, 1e-6),\n    ]\n\n    results = []\n    \n    for m, ax, ay in test_cases:\n        case_results = []\n        A_64 = assemble_A(m, ax, ay)\n        n = m * m\n        b_64 = np.ones(n, dtype=np.float64)\n        tol = 1e-8\n        max_iter = 5000\n\n        # No preconditioner\n        precon_none = lambda r: r\n        iters_none = pcg(A_64, b_64, precon_none, tol, max_iter)\n        case_results.append(iters_none)\n\n        # Jacobi preconditioner\n        A_diag_64 = A_64.diagonal()\n        precon_jacobi = lambda r: r / A_diag_64\n        iters_jacobi = pcg(A_64, b_64, precon_jacobi, tol, max_iter)\n        case_results.append(iters_jacobi)\n\n        # IC(0) 64-bit\n        L64, _ = ic0_factor(A_64, np.float64)\n        precon_ic64 = lambda r: spsolve_triangular(L64.T, spsolve_triangular(L64, r, lower=True), lower=False)\n        iters_ic64 = pcg(A_64, b_64, precon_ic64, tol, max_iter)\n        case_results.append(iters_ic64)\n\n        # IC(0) 32-bit (factorization and solves in float32)\n        A_32 = A_64.astype(np.float32)\n        L32, stab_count = ic0_factor(A_32, np.float32)\n        def precon_ic32(r):\n            r_32 = r.astype(np.float32)\n            y_32 = spsolve_triangular(L32, r_32, lower=True)\n            z_32 = spsolve_triangular(L32.T, y_32, lower=False)\n            return z_32.astype(np.float64)\n        iters_ic32 = pcg(A_64, b_64, precon_ic32, tol, max_iter)\n        case_results.append(iters_ic32)\n\n        # Mixed-precision IC(0) (factor in float32, solves in float64)\n        L32_as_64 = L32.astype(np.float64)\n        precon_icmix = lambda r: spsolve_triangular(L32_as_64.T, spsolve_triangular(L32_as_64, r, lower=True), lower=False)\n        iters_icmix = pcg(A_64, b_64, precon_icmix, tol, max_iter)\n        case_results.append(iters_icmix)\n\n        case_results.append(stab_count)\n        results.append(case_results)\n\n    # Format the final output as specified\n    output_str = \"[\" + \",\".join([str(r) for r in results]) + \"]\"\n    print(output_str.replace(\" \", \"\"))\n\n\nsolve()\n```"}]}