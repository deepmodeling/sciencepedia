{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it is essential to master their fundamental building block: the Krylov subspace. This exercise provides direct, hands-on practice in constructing the vectors that define this subspace and then using the Gram-Schmidt process to generate an orthonormal basis. Mastering this foundational procedure is crucial, as it forms the core of the Arnoldi and Lanczos iterations that underpin methods like GMRES and Conjugate Gradient. [@problem_id:2214825]", "problem": "In numerical linear algebra, Krylov subspace methods are iterative techniques used to solve large linear systems of equations. A key component of algorithms like the Generalized Minimal Residual Method (GMRES) is the construction of an orthonormal basis for a Krylov subspace. The Krylov subspace of dimension $m$, denoted $\\mathcal{K}_m(A, b)$, is defined as the linear span of the vectors $\\{b, Ab, A^2b, \\dots, A^{m-1}b\\}$.\n\nConsider a linear system $Ax=b$ with the matrix $A$ and initial vector $b$ given by:\n$$A = \\begin{pmatrix} 1  2  0 \\\\ 0  1  3 \\\\ 1  0  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nConstruct an orthonormal basis $\\{q_1, q_2\\}$ for the Krylov subspace $\\mathcal{K}_2(A, b)$ using the Gram-Schmidt process. The basis is generated sequentially, starting with the vector $v_1 = b$ to generate $q_1$, and then using the vector $v_2 = Ab$ to generate $q_2$. To ensure a unique answer, the first non-zero component of each basis vector $q_i$ must be positive.\n\nYour task is to provide the matrix $Q_2 = [q_1 | q_2]$, where $q_1$ and $q_2$ are the column vectors forming the orthonormal basis. Express the elements of the matrix in their exact symbolic form using fractions and square roots as needed.", "solution": "We use the Euclidean inner product and the classical Gram-Schmidt process on $v_{1}=b$ and $v_{2}=Ab$.\n\nFirst, set $v_{1}=b=\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}$. Its norm is\n$$\n\\|v_{1}\\|=\\sqrt{1^{2}+1^{2}+0^{2}}=\\sqrt{2}.\n$$\nNormalize to obtain\n$$\nq_{1}=\\frac{v_{1}}{\\|v_{1}\\|}=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\\\ 0\\end{pmatrix},\n$$\nwhose first non-zero entry is positive.\n\nNext, compute $v_{2}=Ab$. With\n$$\nA=\\begin{pmatrix}120\\\\013\\\\101\\end{pmatrix},\\quad b=\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix},\n$$\nwe have\n$$\nv_{2}=Ab=\\begin{pmatrix}3\\\\1\\\\1\\end{pmatrix}.\n$$\nOrthogonalize $v_{2}$ against $q_{1}$:\n$$\nh_{1,2}=q_{1}^{\\top}v_{2}=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}}0\\end{pmatrix}\\begin{pmatrix}3\\\\1\\\\1\\end{pmatrix}=\\frac{4}{\\sqrt{2}}=2\\sqrt{2}.\n$$\nForm the orthogonal component\n$$\nu_{2}=v_{2}-h_{1,2}q_{1}=\\begin{pmatrix}3\\\\1\\\\1\\end{pmatrix}-2\\sqrt{2}\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\\\ 0\\end{pmatrix}=\\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix}.\n$$\nNormalize $u_{2}$:\n$$\n\\|u_{2}\\|=\\sqrt{1^{2}+(-1)^{2}+1^{2}}=\\sqrt{3},\\quad q_{2}=\\frac{u_{2}}{\\|u_{2}\\|}=\\begin{pmatrix}\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\\\ \\frac{1}{\\sqrt{3}}\\end{pmatrix},\n$$\nwhose first non-zero entry is also positive.\n\nTherefore, the orthonormal basis matrix is\n$$\nQ_{2}=\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{3}} \\\\\n0  \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{3}} \\\\ 0  \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2214825"}, {"introduction": "The Conjugate Gradient (CG) method is a remarkably efficient Krylov method, but its stellar performance hinges on a critical requirement: the system matrix $A$ must be symmetric and positive definite. This practice moves beyond theory by challenging you to implement the CG algorithm and observe its behavior when this core assumption is violated. By computationally diagnosing specific failure modes, like encountering non-positive curvature $p_k^T A p_k \\le 0$, you will develop a concrete intuition for why an algorithm's mathematical foundations are paramount for its practical application. [@problem_id:2407671]", "problem": "You are given linear systems of the form $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is real and symmetric. The standard Conjugate Gradient (CG) method is defined for symmetric positive definite matrices and produces search directions $p_k$ and step sizes $\\alpha_k$ that rely on the curvature quantity $p_k^\\top A p_k$. Consider the following computational task.\n\nGiven a real symmetric matrix $A$, a right-hand side $b$, an initial guess $x_0 = 0$, a tolerance $\\varepsilon$, and a maximum number of iterations $k_{\\max}$, write a program that attempts to solve $A x = b$ using the standard Conjugate Gradient method and returns a status code for each case according to the following rules:\n\n- Return $1$ if the method achieves $\\lVert r_k \\rVert_2 \\le \\varepsilon$ at some iteration $k \\le k_{\\max}$ without ever encountering a nonpositive curvature, where $r_k = b - A x_k$ is the residual and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n- Return $0$ if at any iteration $k$ the curvature quantity $p_k^\\top A p_k \\le 0$, which indicates a violation of the positive definiteness requirement inherent to the standard Conjugate Gradient method.\n- Return $-1$ if the method completes $k_{\\max}$ iterations without satisfying $\\lVert r_k \\rVert_2 \\le \\varepsilon$ and without encountering $p_k^\\top A p_k \\le 0$.\n\nUse the following test suite. In all cases, use the initial guess $x_0 = 0$, the tolerance $\\varepsilon = 10^{-10}$, and the maximum iteration count $k_{\\max} = 10$.\n\n- Test case $1$ (symmetric positive definite, expected to converge): \n  $$A_1 = \\begin{bmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n- Test case $2$ (symmetric positive semidefinite but singular, exhibits zero curvature): \n  $$A_2 = \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.$$\n- Test case $3$ (symmetric indefinite, exhibits negative curvature): \n  $$A_3 = \\begin{bmatrix} 1  2  0 \\\\ 2  1  0 \\\\ 0  0  3 \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example: $[s_1,s_2,s_3]$, where each $s_i \\in \\{-1,0,1\\}$ is the status code for test case $i$.", "solution": "The problem statement presented is valid. It is scientifically grounded in the well-established field of numerical linear algebra, specifically concerning Krylov subspace methods. The task is to implement the standard Conjugate Gradient (CG) algorithm and monitor its behavior when applied to symmetric matrices that are not necessarily positive definite. The problem is well-posed, objective, and self-contained, providing all necessary matrices, vectors, and parameters ($x_0$, $\\varepsilon$, $k_{\\max}$) to produce a unique, deterministic result for each test case. The specified return codes correspond to known failure modes or success conditions of the CG method. Therefore, a direct solution can be constructed.\n\nThe Conjugate Gradient method is an iterative algorithm for solving linear systems of the form $A x = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite (SPD). The method's convergence and stability depend critically on the positive definiteness, which ensures that the curvature quantity $p_k^\\top A p_k$ is always positive. This term appears in the denominator for the step size $\\alpha_k$, and its positivity guarantees that each step moves towards the minimum of the quadratic form $f(x) = \\frac{1}{2} x^\\top A x - b^\\top x$.\n\nThe standard CG algorithm is defined as follows. We are given the system $A x = b$, an initial guess $x_0$, a tolerance $\\varepsilon  0$, and a maximum number of iterations $k_{\\max}$.\n\nFirst, we initialize the state variables at iteration $k=0$:\nThe initial solution is given as $x_0 = 0$.\nThe initial residual is $r_0 = b - A x_0 = b$.\nThe initial search direction is set to the residual: $p_0 = r_0$.\n\nThe algorithm then enters an iterative loop for $k = 0, 1, 2, \\ldots, k_{\\max}-1$. At each iteration $k$, the following steps are performed:\n\n1.  Calculate the matrix-vector product $v_k = A p_k$.\n2.  Compute the curvature term, which is a scalar: $d_k = p_k^\\top v_k = p_k^\\top A p_k$. The problem requires us to check if $A$ is behaving as a positive definite matrix with respect to the current search direction. If $d_k \\le 0$, the assumption of positive definiteness is violated. The algorithm cannot proceed in a meaningful way, as the step size $\\alpha_k$ would be negative, zero, or undefined, leading away from the solution. In this case, we must terminate and return the status code $0$.\n3.  If the curvature is positive, calculate the optimal step size along the direction $p_k$:\n    $$ \\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k} $$\n4.  Update the solution vector:\n    $$ x_{k+1} = x_k + \\alpha_k p_k $$\n5.  Update the residual. To avoid an expensive recomputation ($r_{k+1} = b - A x_{k+1}$), we use a recursive formula:\n    $$ r_{k+1} = r_k - \\alpha_k A p_k = r_k - \\alpha_k v_k $$\n6.  Check for convergence. We compute the Euclidean norm of the new residual, $\\lVert r_{k+1} \\rVert_2$. If $\\lVert r_{k+1} \\rVert_2 \\le \\varepsilon$, the solution is considered sufficiently accurate. We terminate and return the status code $1$.\n7.  If the method has not converged, we prepare for the next iteration by computing a new search direction $p_{k+1}$ that is A-orthogonal to the previous one, $p_k$. This is done by first computing the coefficient $\\beta_k$:\n    $$ \\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} $$\n8.  Then, the new search direction is a linear combination of the new residual and the old search direction:\n    $$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n\nIf the loop completes after $k_{\\max}$ iterations without the residual norm falling below the tolerance $\\varepsilon$, and without ever encountering nonpositive curvature, the method has failed to converge within the given iteration limit. In this scenario, we return the status code $-1$.\n\nFor the implementation, we create a function that takes $A$, $b$, $x_0$, $\\varepsilon$, and $k_{\\max}$ as input. Before the main loop, we compute the initial residual $r_0$ and check its norm $\\lVert r_0 \\rVert_2$. If it is already less than or equal to $\\varepsilon$, we can immediately return the status code $1$. Otherwise, we proceed with the iterative process described above. Each of the three test cases provided is designed to trigger one of the three specified outcomes:\n-   Test Case $1$: $A_1$ is a symmetric positive definite matrix. All its eigenvalues are positive, which guarantees that $p^\\top A_1 p  0$ for any non-zero vector $p$. The CG method is expected to converge to the solution within a few iterations. The status code will be $1$.\n-   Test Case $2$: $A_2$ is a symmetric positive semidefinite matrix, as it has eigenvalues of $1$ and $0$. It is singular. The CG algorithm may generate a search direction that lies in the null space of $A_2$, which leads to a curvature term $p_k^\\top A_2 p_k = 0$. This will terminate the algorithm and yield the status code $0$.\n-   Test Case $3$: $A_3$ is a symmetric indefinite matrix, having both positive and negative eigenvalues. The CG method is not guaranteed to maintain positive curvature. A search direction $p_k$ can be generated such that $p_k^\\top A_3 p_k  0$. This condition also invalidates the premises of the standard CG method, requiring termination with status code $0$.\n\nThe program will execute this logic for each of the three test cases and report the resulting status codes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient_with_status(A, b, x0, tol, k_max):\n    \"\"\"\n    Attempts to solve Ax=b using the Conjugate Gradient method with specific status reporting.\n\n    Args:\n        A (np.ndarray): A real symmetric matrix.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The tolerance for the residual norm.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        int: Status code (1: converged, 0: non-positive curvature, -1: max iterations reached).\n    \"\"\"\n    x = x0.copy().astype(float)\n    r = b - A @ x\n    p = r.copy()\n    \n    # Check initial residual norm for trivial convergence\n    r_norm = np.linalg.norm(r)\n    if r_norm = tol:\n        return 1\n\n    rs_old = np.dot(r, r)\n\n    for k in range(k_max):\n        Ap = A @ p\n        \n        # Calculate curvature: p^T * A * p\n        curvature = np.dot(p, Ap)\n        \n        # Check for non-positive curvature (breakdown of CG)\n        if curvature = 0:\n            return 0  # Status for non-positive definite behavior\n        \n        # Update solution and residual\n        alpha = rs_old / curvature\n        x += alpha * p\n        r -= alpha * Ap\n        \n        # Check for convergence\n        r_norm = np.linalg.norm(r)\n        if r_norm = tol:\n            return 1  # Status for successful convergence\n        \n        # Update search direction\n        rs_new = np.dot(r, r)\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n        \n    return -1 # Status for exceeding max iterations without convergence\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases for the Conjugate Gradient problem.\n    \"\"\"\n    # Define global parameters for all test cases\n    x0_val = 0.0  # Scalar, will be broadcast to vector of zeros\n    tol = 1e-10\n    k_max = 10\n\n    # Test Case 1: Symmetric Positive Definite\n    A1 = np.array([[4, 1, 0], \n                   [1, 3, 1], \n                   [0, 1, 2]], dtype=float)\n    b1 = np.array([1, 2, 3], dtype=float)\n\n    # Test Case 2: Symmetric Positive Semidefinite (Singular)\n    A2 = np.array([[1, 0], \n                   [0, 0]], dtype=float)\n    b2 = np.array([1, 1], dtype=float)\n\n    # Test Case 3: Symmetric Indefinite\n    A3 = np.array([[1, 2, 0], \n                   [2, 1, 0], \n                   [0, 0, 3]], dtype=float)\n    b3 = np.array([1, 0, 0], dtype=float)\n    \n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n    ]\n\n    results = []\n    for A, b in test_cases:\n        n = A.shape[0]\n        x0 = np.full(n, x0_val, dtype=float)\n        status = conjugate_gradient_with_status(A, b, x0, tol, k_max)\n        results.append(status)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2407671"}, {"introduction": "In exact arithmetic, different methods for orthogonalization may be equivalent, but in the world of finite-precision computing, the choice of algorithm has profound consequences for stability and accuracy. This computational exercise investigates this critical issue by comparing the Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS) algorithms within the Arnoldi process. By numerically measuring the loss of orthogonality, you will gain first-hand insight into why MGS is the preferred choice for robust implementations of Krylov subspace methods like GMRES. [@problem_id:2407638]", "problem": "You are given square real matrices and an initial vector, and you must numerically quantify the loss of orthonormality in the Arnoldi basis generated when the orthogonalization inside the Arnoldi process uses either classical Gram–Schmidt (CGS) or modified Gram–Schmidt (MGS). Consider the Generalized Minimal Residual (GMRES) method, which builds an Arnoldi basis for the Krylov subspace. For a real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero starting vector $r_0 \\in \\mathbb{R}^n$, define the Krylov subspace of order $k$ as\n$$\n\\mathcal{K}_k(A,r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}.\n$$\nLet $Q_k \\in \\mathbb{R}^{n \\times m}$ be the matrix whose columns are the first $m$ Arnoldi vectors produced when attempting to generate an orthonormal basis of $\\mathcal{K}_k(A,r_0)$, with $m \\le k$ in case of early termination due to breakdown. Define the loss-of-orthonormality metric\n$$\n\\ell(Q_k) = \\left\\| I_m - Q_k^\\top Q_k \\right\\|_F,\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm and $I_m$ is the $m \\times m$ identity matrix. For each test case below, compute two real numbers: $\\ell_{\\mathrm{CGS}} = \\ell(Q_k^{\\mathrm{CGS}})$ and $\\ell_{\\mathrm{MGS}} = \\ell(Q_k^{\\mathrm{MGS}})$, obtained when the Arnoldi process uses classical Gram–Schmidt and modified Gram–Schmidt, respectively, to orthonormalize.\n\nUse double-precision arithmetic. For all test cases, use the initial residual $r_0 = b \\in \\mathbb{R}^n$ with entries $b_i = \\sin(i)$ for $i \\in \\{1,2,\\dots,n\\}$, where the angle unit for the sine function is radians.\n\nTest suite:\n- Case $1$ (symmetric positive definite tridiagonal): Let $n=50$, $k=25$, and $A \\in \\mathbb{R}^{n \\times n}$ be defined by $A_{i,i}=2$ for $1 \\le i \\le n$, $A_{i,i+1}=A_{i+1,i}=-1$ for $1 \\le i  n$, and all other entries equal to $0$.\n- Case $2$ (ill-conditioned diagonal): Let $n=60$, $k=30$, and $A=\\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$ with $\\lambda_i = 10^{\\alpha_i}$ and $\\alpha_i = -8 + \\dfrac{(i-1)\\cdot 8}{n-1}$ for $1 \\le i \\le n$.\n- Case $3$ (non-normal upper bidiagonal): Let $n=50$, $k=25$, and $A \\in \\mathbb{R}^{n \\times n}$ be defined by $A_{i,i}=1$ for $1 \\le i \\le n$, $A_{i,i+1}=0.9$ for $1 \\le i  n$, and all other entries equal to $0$.\n- Case $4$ (nearly defective upper bidiagonal): Let $n=40$, $k=35$, and $A \\in \\mathbb{R}^{n \\times n}$ be defined by $A_{i,i}=\\rho$ with $\\rho=0.999$ for $1 \\le i \\le n$, $A_{i,i+1}=1$ for $1 \\le i  n$, and all other entries equal to $0$.\n\nYour program must, for each case, attempt to build an Arnoldi basis of order $k$ starting from $r_0=b$, compute $\\ell_{\\mathrm{CGS}}$ and $\\ell_{\\mathrm{MGS}}$ for the matrix $Q_k$ actually constructed in each variant (using the first $m$ columns produced, with $m \\le k$ if breakdown occurs), and report the results in the specified format.\n\nFinal output format:\n- Produce a single line of output containing a list with four entries, one per test case, where each entry is a two-element list $[\\ell_{\\mathrm{CGS}},\\ell_{\\mathrm{MGS}}]$.\n- Each real number must be rounded to $10$ significant digits.\n- The line must contain no spaces.\n- For example, a valid shape is $[[x_{11},x_{12}],[x_{21},x_{22}],[x_{31},x_{32}],[x_{41},x_{42}]]$ with each $x_{ij}$ a rounded decimal string.\n\nThere are no physical units involved in this problem. All angles in the definition of $b$ are in radians. The final answers for each test case must be real numbers, and the final output must follow the single-line format described above.", "solution": "The problem requires a numerical comparison of the stability of two orthogonalization schemes, Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS), within the context of the Arnoldi iteration. The stability is quantified by measuring the loss of orthogonality in the generated basis for a set of well-defined test cases. The entire computation will be performed using double-precision floating-point arithmetic.\n\nThe Arnoldi iteration is a fundamental algorithm in numerical linear algebra for constructing an orthonormal basis for the Krylov subspace $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$. For a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and an initial vector $r_0 \\in \\mathbb{R}^n$, the process generates a sequence of orthonormal vectors $q_1, q_2, \\dots, q_k$ whose span is $\\mathcal{K}_k(A, r_0)$. These vectors form the columns of a matrix $Q_k = [q_1, q_2, \\dots, q_k] \\in \\mathbb{R}^{n \\times k}$. The process begins by normalizing the initial vector: $q_1 = r_0 / \\|r_0\\|_2$. Subsequently, for $j=1, 2, \\dots, k-1$, the vector $q_{j+1}$ is generated by taking the matrix-vector product $v = A q_j$ and orthogonalizing it with respect to the previously computed basis vectors $\\{q_1, q_2, \\dots, q_j\\}$. This is the Gram-Schmidt procedure. The resulting vector is then normalized.\n\nThe core of the problem lies in the implementation of this orthogonalization step.\n\nThe Classical Gram-Schmidt (CGS) approach computes the new vector by simultaneously subtracting all projections of $v=Aq_j$ onto the existing basis vectors. First, the projection coefficients $h_{i,j}$ are computed:\n$$\nh_{i,j} = q_i^T v \\quad \\text{for } i = 1, 2, \\dots, j.\n$$\nThen, a new vector $\\hat{v}$ is formed by subtracting the linear combination of basis vectors:\n$$\n\\hat{v} = v - \\sum_{i=1}^{j} h_{i,j} q_i.\n$$\nFinally, this vector is normalized to obtain the next basis vector, $q_{j+1} = \\hat{v} / \\|\\hat{v}\\|_2$, where the norm $\\|\\hat{v}\\|_2$ corresponds to the Hessenberg matrix entry $h_{j+1,j}$.\n\nThe Modified Gram-Schmidt (MGS) approach performs the orthogonalization sequentially. Instead of projecting the original vector $v$ onto each basis vector, it projects an intermediate vector that has already been made orthogonal to the preceding basis vectors. Let $w^{(0)} = v = A q_j$. The process iterates for $i=1, 2, \\dots, j$:\n$$\nh_{i,j} = q_i^T w^{(i-1)},\n$$\n$$\nw^{(i)} = w^{(i-1)} - h_{i,j} q_i.\n$$\nThe resulting vector after $j$ steps is $\\hat{v} = w^{(j)}$. This vector is then normalized to get $q_{j+1} = \\hat{v} / \\|\\hat{v}\\|_2$.\n\nIn exact arithmetic, CGS and MGS are equivalent. However, in finite-precision arithmetic, their numerical properties differ significantly. Due to rounding errors, the computed basis vectors $\\{q_i\\}$ are not perfectly orthogonal, i.e., $q_i^T q_j \\neq 0$ for $i \\neq j$. In CGS, all dot products $q_i^T v$ are computed with the same vector $v$. If the basis vectors have already lost some orthogonality, errors can accumulate, leading to a phenomenon known as catastrophic cancellation. The resulting vector $\\hat{v}$ may retain significant components along the directions of $q_1, \\dots, q_j$ that should have been removed. MGS is more stable because the vector being orthogonalized is updated at each step. The computation $q_i^T w^{(i-1)}$ uses a vector that is already numerically orthogonal to $\\{q_1, \\dots, q_{i-1}\\}$, which reduces the propagation of rounding errors and maintains the orthogonality of the basis vectors to a much higher degree.\n\nWe are asked to quantify this loss of orthogonality using the metric\n$$\n\\ell(Q_m) = \\| I_m - Q_m^T Q_m \\|_F,\n$$\nwhere $Q_m \\in \\mathbb{R}^{n \\times m}$ is the matrix of the $m$ generated Arnoldi vectors (with $m \\le k$ in case of breakdown), $I_m$ is the $m \\times m$ identity matrix, and $\\|\\cdot\\|_F$ is the Frobenius norm. For a perfectly orthonormal basis, $Q_m^T Q_m = I_m$ and $\\ell(Q_m) = 0$. Deviations from zero indicate a loss of orthogonality.\n\nThe solution strategy is as follows:\n1. For each test case, construct the specified matrix $A \\in \\mathbb{R}^{n \\times n}$ and the initial vector $r_0 = b \\in \\mathbb{R}^n$, where $b_i = \\sin(i)$ for $i=1,\\dots,n$.\n2. Implement the Arnoldi iteration for a specified number of steps, $k$. The implementation will include variants for both CGS and MGS orthogonalization.\n3. The process must detect breakdown, which occurs if the norm of the vector to be normalized, $h_{j+1,j}$, falls below a small tolerance. If breakdown occurs at step $j$, the process terminates, yielding a basis of dimension $m=j$. The problem specifies the maximum number of steps is $k$, so the final basis will have $m \\le k$ vectors.\n4. For each case and for each method (CGS and MGS), generate the basis matrix $Q_m$.\n5. Compute the loss-of-orthonormality metric $\\ell(Q_m)$ for both $Q_m^{\\mathrm{CGS}}$ and $Q_m^{\\mathrm{MGS}}$.\n6. The final output will be a formatted list containing the pairs of computed loss values $[\\ell_{\\mathrm{CGS}}, \\ell_{\\mathrm{MGS}}]$ for each test case.", "answer": "```python\nimport numpy as np\n\ndef format_num(n):\n    \"\"\"Formats a number to 10 significant digits for the final output.\"\"\"\n    return \"{:.10g}\".format(n)\n\ndef arnoldi_process(A, r0, k, method):\n    \"\"\"\n    Performs the Arnoldi iteration to generate an orthonormal basis Q for the\n    Krylov subspace K_k(A, r0).\n\n    Args:\n        A (np.ndarray): The matrix of size (n, n).\n        r0 (np.ndarray): The starting vector of size (n,).\n        k (int): The number of Arnoldi vectors to generate.\n        method (str): The orthogonalization method, either 'cgs' or 'mgs'.\n    \n    Returns:\n        np.ndarray: A matrix Q of size (n, m) with m = k orthonormal columns.\n    \"\"\"\n    n = A.shape[0]\n    # Use double precision explicitly as required\n    Q = np.zeros((n, k), dtype=np.float64)\n    # A small tolerance for breakdown detection.\n    tol = 1e-12\n\n    norm_r0 = np.linalg.norm(r0)\n    if norm_r0  tol:\n        # r0 is effectively the zero vector, Krylov subspace is trivial.\n        return np.zeros((n, 0), dtype=np.float64)\n\n    Q[:, 0] = r0 / norm_r0\n    m = 1  # Number of vectors generated so far\n\n    for j in range(k - 1):\n        v = A @ Q[:, j]\n        \n        # Orthogonalization against the current basis Q[:, :m], where m = j + 1\n        if method == 'cgs':\n            # Classical Gram-Schmidt\n            h = Q[:, :m].T @ v\n            w = v - Q[:, :m] @ h\n        \n        elif method == 'mgs':\n            # Modified Gram-Schmidt\n            w = v.copy()\n            for i in range(m):\n                # h_ij = q_i^T w\n                h_ij = Q[:, i].T @ w\n                # w = w - h_ij * q_i\n                w = w - h_ij * Q[:, i]\n        else:\n            raise ValueError(\"Method must be 'cgs' or 'mgs'\")\n            \n        h_next = np.linalg.norm(w)\n        \n        if h_next  tol:\n            # Breakdown: The Krylov subspace has dimension m and is invariant under A.\n            return Q[:, :m]\n        \n        Q[:, j + 1] = w / h_next\n        m += 1\n        \n    return Q[:, :m]\n\ndef solve():\n    \"\"\"\n    Solves the problem by running Arnoldi with CGS and MGS for each test case\n    and printing the results in the specified format.\n    \"\"\"\n    test_cases_params = [\n        {'n': 50, 'k': 25, 'id': 'case1'},\n        {'n': 60, 'k': 30, 'id': 'case2'},\n        {'n': 50, 'k': 25, 'id': 'case3'},\n        {'n': 40, 'k': 35, 'id': 'case4'}\n    ]\n\n    all_results = []\n\n    for params in test_cases_params:\n        n, k, case_id = params['n'], params['k'], params['id']\n        \n        # Construct matrix A for the current test case\n        if case_id == 'case1': # symmetric positive definite tridiagonal\n            A = np.diag(np.full(n, 2.0, dtype=np.float64)) + \\\n                np.diag(np.full(n - 1, -1.0, dtype=np.float64), k=1) + \\\n                np.diag(np.full(n - 1, -1.0, dtype=np.float64), k=-1)\n        elif case_id == 'case2': # ill-conditioned diagonal\n            i = np.arange(1, n + 1, dtype=np.float64)\n            alphas = -8.0 + (i - 1.0) * 8.0 / (n - 1.0)\n            lambdas = 10.0**alphas\n            A = np.diag(lambdas)\n        elif case_id == 'case3': # non-normal upper bidiagonal\n            A = np.diag(np.full(n, 1.0, dtype=np.float64)) + \\\n                np.diag(np.full(n - 1, 0.9, dtype=np.float64), k=1)\n        elif case_id == 'case4': # nearly defective upper bidiagonal\n            rho = 0.999\n            A = np.diag(np.full(n, rho, dtype=np.float64)) + \\\n                np.diag(np.full(n - 1, 1.0, dtype=np.float64), k=1)\n\n        # Construct initial vector b (r0)\n        i_vec = np.arange(1, n + 1, dtype=np.float64)\n        r0 = np.sin(i_vec)\n\n        case_results = []\n        for method in ['cgs', 'mgs']:\n            Q = arnoldi_process(A, r0, k, method)\n            m = Q.shape[1]\n            \n            if m == 0:\n                loss = 0.0\n            else:\n                I_m = np.eye(m, dtype=np.float64)\n                loss_matrix = I_m - Q.T @ Q\n                loss = np.linalg.norm(loss_matrix, 'fro')\n            case_results.append(loss)\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as required\n    list_of_strs = []\n    for pair in all_results:\n        cgs_str = format_num(pair[0])\n        mgs_str = format_num(pair[1])\n        list_of_strs.append(f\"[{cgs_str},{mgs_str}]\")\n    \n    final_output_str = f\"[{','.join(list_of_strs)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "2407638"}]}