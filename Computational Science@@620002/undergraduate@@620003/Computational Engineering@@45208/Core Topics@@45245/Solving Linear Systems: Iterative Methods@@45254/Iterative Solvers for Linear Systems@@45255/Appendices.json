{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the Gauss-Seidel method, a classic iterative solver, applied to a steady-state heat distribution problem. This exercise [@problem_id:2406634] requires you to implement the solver and compare the effect of two different update strategies: lexicographic and checkerboard (red-black) ordering. Through this, you will gain practical experience in implementing a foundational smoother and discover how the ordering of operations can influence performance and open possibilities for parallel computation.", "problem": "Consider the steady-state heat conduction problem (a two-dimensional (2D) Poisson equation) on the unit square with homogeneous Dirichlet boundary conditions. Let $u(x,y)$ satisfy $-\\Delta u = f(x,y)$ on $(0,1)\\times(0,1)$ with $u=0$ on the boundary. Discretize the domain using a uniform Cartesian grid with $n$ interior points in each coordinate direction, grid spacing $h = \\frac{1}{n+1}$, and the standard five-point finite difference stencil. This yields a linear system $A \\mathbf{u} = \\mathbf{b}$ of size $n^2 \\times n^2$, where, for each interior grid index pair $(i,j)$ with $i \\in \\{1,\\dots,n\\}$ and $j \\in \\{1,\\dots,n\\}$, the discrete operator applies\n$$\n\\left(A \\mathbf{u}\\right)_{i,j} = \\frac{1}{h^2}\\left(4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}\\right),\n$$\nwith $u_{0,j} = u_{n+1,j} = u_{i,0} = u_{i,n+1} = 0$ enforcing the boundary values. Take the source term to be identically zero, $f(x,y) \\equiv 0$, so that $\\mathbf{b} = \\mathbf{0}$ and the exact solution is $\\mathbf{u}^\\star = \\mathbf{0}$.\n\nDefine one Gauss–Seidel (GS) sweep as follows: for each interior grid point $(i,j)$, update $u_{i,j}$ by solving the discrete equation at that point for $u_{i,j}$ while holding the most recently available neighboring values fixed. Explicitly, the local update is\n$$\nu_{i,j} \\leftarrow \\frac{1}{4}\\left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - h^2 f_{i,j}\\right),\n$$\nwhere $f_{i,j} = 0$ for all interior indices.\n\nTwo orderings of unknowns are to be considered for performing each GS sweep:\n- Lexicographic ordering: visit points $(i,j)$ in increasing $i$ (row index) and, within each row, increasing $j$ (column index), that is, $(1,1)$, $(1,2)$, $\\dots$, $(1,n)$, $(2,1)$, $\\dots$, $(n,n)$.\n- Checkerboard (red–black) ordering: color the interior grid points by the parity of $i+j$; points with $(i+j)$ even are “red,” and points with $(i+j)$ odd are “black.” One sweep consists of updating all red points (in any order) and then all black points (in any order), using the most recently available neighbors.\n\nFor a given ordering, define the residual at sweep index $k$ as $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{u}^{(k)}$, and the Euclidean norm $\\|\\mathbf{r}^{(k)}\\|_2$. The asymptotic linear convergence factor for a given ordering and grid size $n$ is to be estimated empirically as the geometric mean of the successive residual norm ratios near the end of a fixed number of sweeps. Specifically, with the initial guess set to $u_{i,j}^{(0)} = 1$ for all interior $(i,j)$, perform $K$ full sweeps and record the ratios\n$$\n\\rho_k = \\frac{\\|\\mathbf{r}^{(k+1)}\\|_2}{\\|\\mathbf{r}^{(k)}\\|_2}, \\quad k = 0,1,\\dots,K-1.\n$$\nDefine the estimated convergence factor as\n$$\nq = \\exp\\left(\\frac{1}{T}\\sum_{k=K-T}^{K-1} \\log \\rho_k\\right),\n$$\nusing the last $T$ ratios, provided $\\|\\mathbf{r}^{(k)}\\|_2 \\neq 0$ for those indices. If at any point $\\|\\mathbf{r}^{(k)}\\|_2 = 0$, define $q = 0.0$ for that ordering and $n$.\n\nTest suite and required outputs:\n- Use $K = 80$ sweeps and $T = 30$ tail ratios in the definition above.\n- Use the three grid sizes $n \\in \\{1, 8, 31\\}$.\n- For each $n$ in the order $n=1$, $n=8$, $n=31$, compute two floating-point values: the estimated convergence factor $q_{\\text{lex}}$ for the lexicographic ordering, and the estimated convergence factor $q_{\\text{rb}}$ for the checkerboard (red–black) ordering, both rounded to four digits after the decimal point.\n- Edge-case convention: if for a given $n$ and ordering the residual norm becomes exactly $0$ at any sweep, output $0.0000$ for that case.\n\nFinal output format:\n- Your program should produce a single line of output containing the six results aggregated in order as a comma-separated list enclosed in square brackets, with no spaces, in the order $[q_{\\text{lex}}(1), q_{\\text{rb}}(1), q_{\\text{lex}}(8), q_{\\text{rb}}(8), q_{\\text{lex}}(31), q_{\\text{rb}}(31)]$. For example, an output line must look like $[0.1234,0.5678,0.9012,0.3456,0.7890,0.1234]$ but with the actual computed values for this problem.", "solution": "The problem statement is valid. It presents a well-defined numerical experiment to empirically estimate the convergence factor of the Gauss-Seidel iterative method for solving a discretized two-dimensional (2D) Poisson equation. The problem is scientifically sound, internally consistent, and contains all necessary parameters and definitions for its solution.\n\nThe problem lies in computational engineering, specifically the numerical solution of partial differential equations. The governing equation is the steady-state heat equation, or Poisson's equation, $-\\Delta u = f(x,y)$ on the unit square $(0,1)\\times(0,1)$ with a zero source term $f(x,y) \\equiv 0$ and homogeneous Dirichlet boundary conditions $u=0$ on the boundary. This setup implies an exact solution of $u(x,y) \\equiv 0$.\n\nDiscretization is performed using a standard five-point finite difference stencil on a uniform grid with $n$ interior points in each direction, resulting in a grid spacing of $h = 1/(n+1)$. This converts the continuous problem into a system of linear algebraic equations, $A \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the unknown values $u_{i,j}$ at the interior grid points, and the right-hand side is $\\mathbf{b} = \\mathbf{0}$. The matrix $A$ represents the negative discrete Laplacian operator. It is a well-known result that this matrix is symmetric and positive definite, which guarantees that the Gauss-Seidel iterative method converges for any initial guess.\n\nThe Gauss-Seidel method is an iterative smoother that updates each unknown $u_{i,j}$ using the most recently computed values of its neighbors. The update rule is derived from the discrete equation at point $(i,j)$:\n$$\n\\frac{1}{h^2}\\left(4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}\\right) = f_{i,j} = 0\n$$\nSolving for $u_{i,j}$ gives the local update formula:\n$$\nu_{i,j} \\leftarrow \\frac{1}{4}\\left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1}\\right)\n$$\nThe order in which the points $(i,j)$ are visited defines the specific variant of the method. The problem specifies two orderings:\n1.  Lexicographic ordering: Points are updated row by row, and column by column within each row. The update for $u_{i,j}$ uses newly computed values for $u_{i-1,j}$ and $u_{i,j-1}$ from the current sweep, and old values for $u_{i+1,j}$ and $u_{i,j+1}$ from the previous sweep.\n2.  Checkerboard (red-black) ordering: The grid points are partitioned into two sets, \"red\" for which $(i+j)$ is even and \"black\" for which $(i+j)$ is odd. A full sweep consists of first updating all red points, then all black points. All red points can be updated in parallel as their values depend only on their black neighbors. Subsequently, all black points are updated using the newly computed values of their red neighbors.\n\nThe convergence rate is estimated empirically. The residual vector after $k$ sweeps is $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{u}^{(k)} = -A \\mathbf{u}^{(k)}$. The ratio of successive Euclidean norms of the residual, $\\rho_k = \\|\\mathbf{r}^{(k+1)}\\|_2 / \\|\\mathbf{r}^{(k)}\\|_2$, is computed for $K=80$ sweeps. The asymptotic convergence factor $q$ is then estimated as the geometric mean of the last $T=30$ of these ratios:\n$$\nq = \\exp\\left(\\frac{1}{T}\\sum_{k=K-T}^{K-1} \\ln \\rho_k\\right)\n$$\nThis empirical factor $q$ approximates the spectral radius of the corresponding Gauss-Seidel iteration matrix, $\\rho(G)$, which theoretically governs the asymptotic rate of convergence. For the 2D model problem, the theoretical spectral radius is $\\rho(G) = \\cos^2(\\pi h)$ for both lexicographic and red-black orderings. However, the finite-iteration empirical estimate $q$ can differ between the orderings due to transient effects related to the non-normality of the iteration matrices.\n\nThe algorithm proceeds as follows for each specified grid size $n \\in \\{1, 8, 31\\}$ and for each of the two orderings:\n1.  Initialize an $(n+2) \\times (n+2)$ grid for $u$, with boundary cells set to $0$ and all $n^2$ interior cells initialized to $1$.\n2.  Calculate the initial residual norm $\\|\\mathbf{r}^{(0)}\\|_2$. A special case occurs for $n=1$, where a single iteration yields the exact solution $\\mathbf{u}=\\mathbf{0}$, thus making $\\|\\mathbf{r}^{(1)}\\|_2 = 0$. As per the problem's convention, this results in an estimated convergence factor of $q=0.0$.\n3.  Iterate for $K=80$ sweeps. In each sweep $k$:\n    a. Apply the Gauss-Seidel updates to all interior grid points according to the specified ordering (lexicographic or red-black).\n    b. Compute the new residual norm $\\|\\mathbf{r}^{(k+1)}\\|_2$.\n    c. If $\\|\\mathbf{r}^{(k+1)}\\|_2$ is zero, the process terminates for this case, yielding $q=0.0$. Otherwise, calculate and store the ratio $\\rho_k$.\n4.  After the iterations, if the solution has not converged to zero exactly, compute the geometric mean $q$ from the final $T=30$ stored ratios.\n5.  The computed values for $q_{\\text{lex}}$ and $q_{\\text{rb}}$ for each $n$ are rounded to four decimal places and aggregated into the final output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the computation for all test cases and print the final result.\n    \"\"\"\n    \n    def compute_factor(n, ordering, K, T):\n        \"\"\"\n        Computes the empirical convergence factor for a given grid size and GS ordering.\n\n        Args:\n            n (int): Number of interior grid points in each dimension.\n            ordering (str): 'lexicographic' or 'red_black'.\n            K (int): Total number of sweeps.\n            T (int): Number of tail ratios for the geometric mean.\n\n        Returns:\n            float: The estimated convergence factor q.\n        \"\"\"\n        # For n=1, the system has one unknown. GS converges in one step to the exact\n        # solution u=0. The residual becomes zero, so q=0.0 by problem definition.\n        if n == 1:\n            return 0.0\n\n        h = 1.0 / (n + 1)\n        \n        # Initialize grid u of size (n+2)x(n+2). Boundary is at index 0 and n+1.\n        u = np.zeros((n + 2, n + 2), dtype=np.float64)\n        # Set initial guess for interior points (1 to n).\n        u[1:n+1, 1:n+1] = 1.0\n\n        def calculate_residual_norm(u_grid):\n            \"\"\"Calculates the Euclidean norm of the residual vector r = -A*u.\"\"\"\n            # A*u applied to interior points\n            Au_interior = (4 * u_grid[1:n+1, 1:n+1]\n                           - u_grid[0:n, 1:n+1]    # u_{i-1,j}\n                           - u_grid[2:n+2, 1:n+1]  # u_{i+1,j}\n                           - u_grid[1:n+1, 0:n]    # u_{i,j-1}\n                           - u_grid[1:n+1, 2:n+2]) # u_{i,j+1}\n            Au_interior /= (h**2)\n            \n            return np.linalg.norm(Au_interior)\n\n        ratios = []\n        zero_residual_found = False\n\n        # Calculate initial residual norm.\n        norm_k = calculate_residual_norm(u)\n        if np.isclose(norm_k, 0.0):\n            zero_residual_found = True\n\n        if not zero_residual_found:\n            for _ in range(K):\n                # Perform one Gauss-Seidel sweep\n                if ordering == 'lexicographic':\n                    for i in range(1, n + 1):\n                        for j in range(1, n + 1):\n                            u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n                elif ordering == 'red_black':\n                    # Update \"red\" points (i+j is even)\n                    for i in range(1, n + 1):\n                        for j in range(1, n + 1):\n                            if (i + j) % 2 == 0:\n                                u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n                    # Update \"black\" points (i+j is odd)\n                    for i in range(1, n + 1):\n                        for j in range(1, n + 1):\n                            if (i + j) % 2 != 0:\n                                u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n\n                # Calculate post-sweep residual norm.\n                norm_k_plus_1 = calculate_residual_norm(u)\n                \n                if np.isclose(norm_k_plus_1, 0.0):\n                    zero_residual_found = True\n                    break\n                \n                # Store ratio and update norm for next iteration\n                ratios.append(norm_k_plus_1 / norm_k)\n                norm_k = norm_k_plus_1\n\n        if zero_residual_found:\n            return 0.0\n            \n        # Calculate the geometric mean of the last T ratios.\n        # Ratios are for k=0...K-1. We need k from K-T to K-1.\n        log_ratios_tail = np.log(ratios[K - T:])\n        q = np.exp(np.mean(log_ratios_tail))\n        \n        return q\n\n    # Define the test cases from the problem statement.\n    test_cases = [1, 8, 31]\n    K = 80\n    T = 30\n    \n    results = []\n    for n_val in test_cases:\n        # Lexicographic ordering\n        q_lex = compute_factor(n_val, 'lexicographic', K, T)\n        results.append(q_lex)\n        \n        # Red-Black ordering\n        q_rb = compute_factor(n_val, 'red_black', K, T)\n        results.append(q_rb)\n\n    # Format the results rounded to four decimal places.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406634"}, {"introduction": "While iterative methods can be powerful, they do not always converge out-of-the-box. This practice [@problem_id:2406651] presents a scenario where the basic Jacobi method fails and challenges you to diagnose and fix the issue. By experimentally determining an appropriate damping factor, you will directly engage with the core theory of convergence—the spectral radius of the iteration matrix—and learn how relaxation techniques can be used to ensure stability for a wider class of problems.", "problem": "You are asked to implement and analyze a damped Jacobi iteration to solve linear systems that are not strictly diagonally dominant. The objective is to determine, by numerical experimentation, damping factors that guarantee convergence. Your work must be grounded in the following foundational base: the linear system definition $A x = b$, the diagonal-offdiagonal split $A = D + R$ with $D$ the diagonal of $A$, and the general principle that fixed-point iterations converge when the spectral radius of the iteration matrix is less than $1$. You must not use any pre-packaged solver; all iteration logic must be implemented explicitly.\n\nTask requirements:\n- Implement a damped Jacobi iteration for a general square real matrix $A \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^n$. The method should start from $x^{(0)} = 0$ and iterate until the absolute residual norm criterion $\\lVert b - A x^{(k)} \\rVert_2 \\le \\tau$ is met or until a maximum number of iterations is reached. Use $\\tau = 10^{-10}$ and a fixed iteration limit of $10{,}000$.\n- For a given damping factor $\\omega \\in (0,1]$, apply the damped correction to the Jacobi update derived from $A = D + R$ and the residual $r^{(k)} = b - A x^{(k)}$ without invoking any formula beyond this foundational base. Numerical stability and correctness must be ensured for all provided test cases.\n- For each test case defined below, search over the discrete set $W = \\{0.05, 0.10, 0.15, \\dots, 1.00\\}$ and return the largest $\\omega \\in W$ such that the damped Jacobi iteration converges to the tolerance within the iteration limit. If no $\\omega$ in $W$ leads to convergence, return $0.0$ for that case.\n- The systems include matrices that are symmetric positive definite (SPD) but not strictly diagonally dominant. In all cases, all quantities are unitless; no physical units are involved.\n\nTest suite specification:\n- Across all cases, use $x^{(0)} = 0$, $\\tau = 10^{-10}$, $\\text{max\\_iter} = 10{,}000$, and the search set $W = \\{0.05, 0.10, \\dots, 1.00\\}$.\n- Case $1$ (non-strictly diagonally dominant, SPD, undamped Jacobi diverges due to a negative eigenvalue below $-1$ but damping can stabilize):\n  - Define the orthonormal matrix\n    $$Q_3 = \\begin{bmatrix}\n    \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}} \\\\\n    \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0 \\\\\n    \\frac{1}{\\sqrt{6}}  \\frac{1}{\\sqrt{6}}  -\\frac{2}{\\sqrt{6}}\n    \\end{bmatrix}.$$\n  - Define the diagonal matrix\n    $$\\Lambda_3 = \\mathrm{diag}(-1.1, 0.8, 0.6).$$\n  - Define\n    $$T_3 = Q_3 \\Lambda_3 Q_3^\\top, \\quad A_1 = I_3 - T_3, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n- Case $2$ (weakly diagonally dominant tridiagonal Toeplitz SPD; undamped Jacobi converges):\n  - Define $A_2 \\in \\mathbb{R}^{3 \\times 3}$ by $[A_2]_{ii} = 2$ for $i \\in \\{1,2,3\\}$ and $[A_2]_{i,i+1} = [A_2]_{i+1,i} = -1$ for $i \\in \\{1,2\\}$, with all other entries zero. Define\n    $$b_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n- Case $3$ (non-strictly diagonally dominant, SPD, undamped Jacobi diverges; damping can stabilize):\n  - Define the normalized Hadamard matrix\n    $$Q_4 = \\frac{1}{2}\\begin{bmatrix}\n    1  1  1  1 \\\\\n    1  1  -1  -1 \\\\\n    1  -1  1  -1 \\\\\n    1  -1  -1  1\n    \\end{bmatrix}.$$\n  - Define the diagonal matrix\n    $$\\Lambda_4 = \\mathrm{diag}(-1.3, 0.9, 0.7, 0.2).$$\n  - Define\n    $$T_4 = Q_4 \\Lambda_4 Q_4^\\top, \\quad A_3 = I_4 - T_4, \\quad b_3 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}.$$\n- Case $4$ (boundary-like case with a highly negative eigenvalue; testing near the stability edge):\n  - Use $Q_3$ as in Case $1$ and define\n    $$\\Lambda_3' = \\mathrm{diag}(-1.98, 0.0, 0.0).$$\n  - Define\n    $$T_3' = Q_3 \\Lambda_3' Q_3^\\top, \\quad A_4 = I_3 - T_3', \\quad b_4 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.$$\n\nRequired final output format:\n- Your program should produce a single line of output containing the largest damping factors found for the four cases, in order, as a comma-separated list enclosed in square brackets. For example, an output of the form\n  $$[0.95,1.0,0.85,0.65]$$\n  would be acceptable. The actual numeric values must be those obtained by your implementation using the parameters above.\n\nDesign for coverage:\n- Case $1$ verifies stabilization by damping for a non-strictly diagonally dominant matrix with a negative eigenvalue less than $-1$.\n- Case $2$ is a happy-path scenario where undamped Jacobi already converges; the largest admissible $\\omega$ should be $1.0$.\n- Case $3$ is another non-strictly diagonally dominant system with a more negative eigenvalue, testing a smaller largest admissible $\\omega$.\n- Case $4$ is a boundary case with a very negative eigenvalue close to $-2$, probing the edge of the stability region in the given grid.\n\nYour program must be a complete, runnable implementation that constructs $A$ and $b$ for each case exactly as defined above, executes the damped Jacobi method, searches over $W$, and prints the single required line with the resulting four numbers.", "solution": "We begin from the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$. Let $A = D + R$ where $D$ is the diagonal of $A$ and $R$ contains the off-diagonal entries. The Jacobi method is an instance of fixed-point iteration derived from rearranging $D x = b - R x$, yielding the map $x \\mapsto D^{-1}(b - R x)$. To fit the fundamental linear fixed-point framework $x^{(k+1)} = T x^{(k)} + c$, one writes the update in residual-correction form using the residual $r^{(k)} = b - A x^{(k)}$, and replaces the exact correction $A^{-1} r^{(k)}$ by the diagonal approximation $D^{-1} r^{(k)}$. This gives the undamped Jacobi step\n$$\nx^{(k+1)} = x^{(k)} + D^{-1} (b - A x^{(k)}).\n$$\nTo improve robustness for systems that are not strictly diagonally dominant, one applies a relaxation factor $\\omega \\in (0,1]$ to the correction, obtaining the damped Jacobi iteration\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - A x^{(k)}).\n$$\nThis is a fixed-point iteration with iteration matrix\n$$\nT_\\omega = I - \\omega D^{-1} A = (1 - \\omega) I + \\omega \\left( I - D^{-1} A \\right),\n$$\nso that if $T_J = I - D^{-1} A$ denotes the undamped Jacobi iteration matrix, we have\n$$\nT_\\omega = (1 - \\omega) I + \\omega T_J.\n$$\nThe fundamental convergence criterion for linear fixed-point iterations is that the spectral radius of the iteration matrix be less than $1$:\n$$\n\\rho(T_\\omega)  1.\n$$\nIf $\\lambda$ is an eigenvalue of $T_J$ with eigenvector $v$, then $T_\\omega v = \\left( 1 - \\omega + \\omega \\lambda \\right) v$, so the eigenvalues of $T_\\omega$ are\n$$\n\\mu(\\omega; \\lambda) = 1 - \\omega + \\omega \\lambda.\n$$\nTherefore,\n$$\n\\rho(T_\\omega) = \\max_{\\lambda \\in \\mathrm{spec}(T_J)} \\left| 1 - \\omega + \\omega \\lambda \\right|.\n$$\nThis immediately clarifies the role of damping. If an eigenvalue of $T_J$ satisfies $\\lambda  1$, then for any $\\omega \\in (0,1]$,\n$$\n\\left| 1 - \\omega + \\omega \\lambda \\right| = 1 + \\omega(\\lambda - 1)  1,\n$$\nso damping cannot cure divergence caused by eigenvalues greater than $1$. In contrast, if divergence is due to a negative eigenvalue with $\\lambda  -1$, damping can stabilize the iteration. Write $\\lambda = -c$ with $c  1$. Then\n$$\n\\left| 1 - \\omega + \\omega \\lambda \\right| = \\left| 1 - \\omega (1 + c) \\right|  1\n\\quad \\Longleftrightarrow \\quad 0  \\omega  \\frac{2}{1 + c}.\n$$\nThus, when all eigenvalues of $T_J$ are less than or equal to $1$ and any unstable eigenvalues satisfy $\\lambda  -1$, stability is achieved for sufficiently small $\\omega$; in fact, any $\\omega \\in \\left(0, \\min \\left\\{ 1, \\frac{2}{1 + c_{\\max}} \\right\\} \\right)$ where $c_{\\max} = \\max \\{ -\\lambda : \\lambda \\in \\mathrm{spec}(T_J), \\lambda  -1 \\}$ guarantees $\\rho(T_\\omega)  1$.\n\nAlgorithmic design:\n- For each test system, construct $A$ and $b$, initialize $x^{(0)} = 0$, select $\\omega$ from the discrete set $W = \\{0.05, 0.10, \\dots, 1.00\\}$, and iterate\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - A x^{(k)})\n$$\nuntil either the absolute residual norm $\\lVert b - A x^{(k)} \\rVert_2 \\le \\tau$ with $\\tau = 10^{-10}$ or the iteration count reaches $10{,}000$. Record whether the method converged. Over $W$, choose the largest $\\omega$ that converges.\n\nTest matrices and expectations:\n- Case $1$: Define $Q_3$, $\\Lambda_3 = \\mathrm{diag}(-1.1, 0.8, 0.6)$, $T_3 = Q_3 \\Lambda_3 Q_3^\\top$, and $A_1 = I_3 - T_3$, $b_1 = [1,2,3]^\\top$. By construction, the eigenvalues of $T_3$ are $-1.1$, $0.8$, and $0.6$. The instability is caused solely by $\\lambda_{\\min} = -1.1$. The admissible damping range for that mode is $0  \\omega  \\frac{2}{1 + 1.1} = \\frac{2}{2.1} \\approx 0.95238$. Therefore, $\\omega = 1.0$ diverges, whereas $\\omega = 0.95$ is theoretically stable. On the discrete grid $W$, we therefore expect the largest admissible $\\omega$ to be $0.95$, and the convergence factor for the slowest mode at $\\omega = 0.95$ is $\\left| 1 - 0.95(1 + 1.1) \\right| = \\left| 1 - 1.995 \\right| = 0.995$, which still allows convergence within $10{,}000$ iterations.\n- Case $2$: Tridiagonal Toeplitz $A_2$ with diagonal $2$ and off-diagonal $-1$ is symmetric positive definite (SPD) and weakly diagonally dominant. The undamped Jacobi iteration matrix has eigenvalues $\\lambda_k = \\cos \\left( \\frac{k \\pi}{n+1} \\right)$ for $k \\in \\{1,2,3\\}$, which lie in $(-1,1)$. For $\\omega = 1.0$, $\\rho(T_\\omega) = \\rho(T_J)  1$, so convergence occurs with the largest grid value $\\omega = 1.0$.\n- Case $3$: With $Q_4$ as the normalized Hadamard and $\\Lambda_4 = \\mathrm{diag}(-1.3, 0.9, 0.7, 0.2)$, the eigenvalues of $T_4$ are $-1.3$, $0.9$, $0.7$, $0.2$. The admissible damping range for the unstable negative mode is $0  \\omega  \\frac{2}{1 + 1.3} = \\frac{2}{2.3} \\approx 0.86956$. On the grid $W$, the largest admissible $\\omega$ is $0.85$. At $\\omega = 0.85$ the worst-factor is $\\left| 1 - 0.85(1 + 1.3) \\right| = \\left| 1 - 1.955 \\right| = 0.955$, which is acceptable for convergence within the iteration limit.\n- Case $4$: Using $Q_3$ and $\\Lambda_3' = \\mathrm{diag}(-1.98, 0.0, 0.0)$, the unstable negative mode requires $0  \\omega  \\frac{2}{1 + 1.98} = \\frac{2}{2.98} \\approx 0.67114$. On the grid $W$, the largest admissible $\\omega$ is $0.65$. At $\\omega = 0.65$ the worst factor is $\\left| 1 - 0.65(1 + 1.98) \\right| = \\left| 1 - 1.937 \\right| = 0.937$.\n\nPutting these together, the expected outputs are the largest admissible damping factors in $W$:\n- Case $1$: $0.95$.\n- Case $2$: $1.0$.\n- Case $3$: $0.85$.\n- Case $4$: $0.65$.\n\nThe program must construct the matrices exactly as specified from $Q_3$, $Q_4$, $\\Lambda_3$, $\\Lambda_4$, $\\Lambda_3'$, implement the damped Jacobi iteration with the stated stopping criteria, perform the grid search over $W$, and print the results in the required single-line list format.", "answer": "```python\nimport numpy as np\n\ndef damped_jacobi(A, b, omega, max_iter=10000, tol=1e-10):\n    \"\"\"\n    Perform damped Jacobi iteration:\n        x_{k+1} = x_k + omega * D^{-1} * (b - A x_k)\n    starting from x0 = 0. Returns (converged: bool, iterations: int).\n    \"\"\"\n    n = A.shape[0]\n    # Diagonal and its inverse\n    D = np.diag(A)\n    if np.any(D == 0):\n        return False, 0  # cannot apply Jacobi if zero diagonal\n    invD = 1.0 / D\n\n    x = np.zeros(n, dtype=float)\n    r = b - A @ x\n    rnorm = np.linalg.norm(r)\n    # Absolute tolerance\n    if rnorm = tol:\n        return True, 0\n\n    for k in range(1, max_iter + 1):\n        # Jacobi update with damping\n        r = b - A @ x\n        x = x + omega * (invD * r)\n        # Check convergence\n        r = b - A @ x\n        rnorm = np.linalg.norm(r)\n        if not np.isfinite(rnorm):\n            return False, k\n        if rnorm = tol:\n            return True, k\n        # Optional early bailout if the residual is exploding\n        if rnorm  1e20:\n            return False, k\n    return False, max_iter\n\n\ndef build_case1():\n    # Q3 orthonormal matrix\n    Q3 = np.array([\n        [1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0)],\n        [1.0/np.sqrt(2.0), -1.0/np.sqrt(2.0), 0.0],\n        [1.0/np.sqrt(6.0), 1.0/np.sqrt(6.0), -2.0/np.sqrt(6.0)]\n    ], dtype=float)\n    Lambda3 = np.diag([-1.1, 0.8, 0.6])\n    T3 = Q3 @ Lambda3 @ Q3.T\n    A1 = np.eye(3) - T3\n    b1 = np.array([1.0, 2.0, 3.0], dtype=float)\n    return A1, b1\n\n\ndef build_case2():\n    # Tridiagonal Toeplitz with diag=2, offdiag=-1, n=3\n    n = 3\n    A2 = np.zeros((n, n), dtype=float)\n    np.fill_diagonal(A2, 2.0)\n    for i in range(n - 1):\n        A2[i, i+1] = -1.0\n        A2[i+1, i] = -1.0\n    b2 = np.ones(n, dtype=float)\n    return A2, b2\n\n\ndef build_case3():\n    # Q4 normalized Hadamard matrix\n    Q4 = 0.5 * np.array([\n        [1.0,  1.0,  1.0,  1.0],\n        [1.0,  1.0, -1.0, -1.0],\n        [1.0, -1.0,  1.0, -1.0],\n        [1.0, -1.0, -1.0,  1.0]\n    ], dtype=float)\n    Lambda4 = np.diag([-1.3, 0.9, 0.7, 0.2])\n    T4 = Q4 @ Lambda4 @ Q4.T\n    A3 = np.eye(4) - T4\n    b3 = np.array([1.0, 2.0, 3.0, 4.0], dtype=float)\n    return A3, b3\n\n\ndef build_case4():\n    # Q3 as in case 1\n    Q3 = np.array([\n        [1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0)],\n        [1.0/np.sqrt(2.0), -1.0/np.sqrt(2.0), 0.0],\n        [1.0/np.sqrt(6.0), 1.0/np.sqrt(6.0), -2.0/np.sqrt(6.0)]\n    ], dtype=float)\n    Lambda3p = np.diag([-1.98, 0.0, 0.0])\n    T3p = Q3 @ Lambda3p @ Q3.T\n    A4 = np.eye(3) - T3p\n    b4 = np.array([0.0, 1.0, 0.0], dtype=float)\n    return A4, b4\n\n\ndef largest_convergent_omega(A, b, omegas, max_iter=10000, tol=1e-10):\n    last_ok = None\n    for w in omegas:\n        ok, _ = damped_jacobi(A, b, w, max_iter=max_iter, tol=tol)\n        if ok:\n            last_ok = w\n    return 0.0 if last_ok is None else float(np.round(last_ok, 10))\n\n\ndef solve():\n    # Define the test cases\n    test_cases = [\n        build_case1(),\n        build_case2(),\n        build_case3(),\n        build_case4()\n    ]\n    # Define omega grid W = {0.05, 0.10, ..., 1.00}\n    omegas = [0.05 * i for i in range(1, 21)]\n    results = []\n    for (A, b) in test_cases:\n        w_star = largest_convergent_omega(A, b, omegas, max_iter=10000, tol=1e-10)\n        results.append(w_star)\n    # Final output in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2406651"}, {"introduction": "We now advance from stationary methods to the Conjugate Gradient (CG) algorithm, the premier iterative solver for large, sparse, symmetric positive-definite systems. This exercise [@problem_id:2406599] demonstrates the transformative impact of preconditioning on the convergence speed of CG. You will compare the performance of standard CG against versions accelerated by both a simple diagonal preconditioner and a more sophisticated polynomial preconditioner, providing a clear, quantitative look at why preconditioning is essential in computational science.", "problem": "You are given a family of symmetric positive definite linear systems of the form $A_n x = b$ where $A_n \\in \\mathbb{R}^{n \\times n}$ is the standard second-order finite-difference discretization of the one-dimensional negative Laplacian on the open interval $(0,1)$ with homogeneous Dirichlet boundary conditions. Explicitly, $A_n$ is the tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals, and zeros elsewhere. Let $b \\in \\mathbb{R}^n$ be the vector with all entries equal to $1$, and let the initial guess be $x_0 = 0$. Denote by $D_n = \\mathrm{diag}(A_n)$ the diagonal matrix formed by the diagonal entries of $A_n$, and by $R_n = D_n - A_n$ the strictly off-diagonal part. For a nonnegative integer $m$, define the polynomial approximate inverse (truncated Neumann series) of order $m$ by\n$$\nP_m \\;=\\; \\sum_{k=0}^{m} \\left(D_n^{-1} R_n\\right)^k \\, D_n^{-1}.\n$$\nFor each test case specified below, consider the following three iteration counts, each defined as the smallest integer $k \\in \\{0,1,2,\\dots,n\\}$ such that the stated method produces an iterate $x_k$ with relative residual $\\|b - A_n x_k\\|_2 / \\|b\\|_2 \\le \\tau$, where $\\tau = 10^{-8}$ and the maximum number of iterations is $n$:\n$1)$ $k_{\\mathrm{unpre}}(n)$, obtained by the method that generates $A_n$-conjugate search directions and minimizes the $\\|b - A_n x\\|_2$ residual over successive affine Krylov subspaces $x_0 + \\mathcal{K}_k(A_n, r_0)$ with $r_0 = b - A_n x_0$.\n$2)$ $k_{\\mathrm{jac}}(n)$, obtained by the same method applied to the left-preconditioned system defined via the diagonal preconditioning operator $D_n^{-1}$, that is, using the auxiliary update $z = D_n^{-1} r$ at each iteration in place of $z = r$.\n$3)$ $k_{\\mathrm{poly}}(n,m)$, obtained by the same method applied to the left-preconditioned system defined via the polynomial operator $P_m$, that is, using the auxiliary update $z = P_m r$ at each iteration.\n\nTest suite. For each pair $(n,m)$ below, compute and report the triple of integers $[\\,k_{\\mathrm{unpre}}(n),\\,k_{\\mathrm{jac}}(n),\\,k_{\\mathrm{poly}}(n,m)\\,]$:\n- $(n,m) = (100,0)$\n- $(n,m) = (100,3)$\n- $(n,m) = (200,3)$\n- $(n,m) = (5,10)$\n\nYour program should produce a single line of output containing the results as a comma-separated list of these triples, enclosed in square brackets. Concretely, the output format must be\n$[\\,[k_{\\mathrm{unpre}}(n_1),k_{\\mathrm{jac}}(n_1),k_{\\mathrm{poly}}(n_1,m_1)],\\,[k_{\\mathrm{unpre}}(n_2),k_{\\mathrm{jac}}(n_2),k_{\\mathrm{poly}}(n_2,m_2)],\\,\\dots\\,]$ with no extra whitespace or text.", "solution": "**Methodology**\nThe core of the solution lies in implementing the Preconditioned Conjugate Gradient (PCG) algorithm. The standard (unpreconditioned) CG method is a special case of PCG where the preconditioner is the identity matrix.\n\nThe PCG algorithm for solving $Ax=b$ with a preconditioner $M$ is as follows:\n1.  Initialize: $k=0$, $x_0 = 0$, $r_0 = b - A x_0 = b$.\n2.  If $\\|r_0\\|_2 / \\|b\\|_2 \\le \\tau$, terminate with iteration count $k=0$.\n3.  Solve for $z_0$: $M z_0 = r_0$.\n4.  Set search direction: $p_0 = z_0$.\n5.  Iterate for $k = 0, 1, 2, \\dots$ up to a maximum of $n-1$:\n    a. Compute step size: $\\alpha_k = \\frac{r_k^T z_k}{p_k^T A p_k}$.\n    b. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    c. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n    d. Check for convergence: If $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\tau$, terminate with iteration count $k+1$.\n    e. Apply preconditioner: Solve $M z_{k+1} = r_{k+1}$.\n    f. Compute improvement factor: $\\beta_k = \\frac{r_{k+1}^T z_{k+1}}{r_k^T z_k}$.\n    g. Update search direction: $p_{k+1} = z_{k+1} + \\beta_k p_k$.\n\nThis single algorithm will be used to compute all required iteration counts by varying the 'solve' step ($M z = r$).\n\n**Preconditioner Implementation**\nThe three required scenarios correspond to three different definitions of the preconditioning step $z_k = M^{-1} r_k$.\n\n1.  **Unpreconditioned ($k_{\\mathrm{unpre}}$)**: This is equivalent to setting $M=I_n$, the identity matrix. The preconditioning step is trivial: $z_k = r_k$.\n\n2.  **Jacobi Preconditioning ($k_{\\mathrm{jac}}$)**: The preconditioner is $M=D_n = \\mathrm{diag}(A_n)$. For the given matrix $A_n$, $D_n = 2I_n$. The preconditioning step is a simple scaling: $z_k = D_n^{-1} r_k = \\frac{1}{2}r_k$.\n\n3.  **Polynomial Preconditioning ($k_{\\mathrm{poly}}$)**: The preconditioner is defined by its inverse, $M^{-1} = P_m$. The preconditioning step is a matrix-vector product: $z_k = P_m r_k$. The operator $P_m$ is given by\n    $$\n    P_m = \\sum_{j=0}^{m} (D_n^{-1} R_n)^j D_n^{-1}\n    $$\n    The application $z_k = P_m r_k$ is computed efficiently without explicitly forming the matrix $P_m$. Let $T = D_n^{-1} R_n$ and $v = D_n^{-1} r_k$. The problem reduces to computing $z_k = (\\sum_{j=0}^{m} T^j) v$. This can be implemented with an iterative loop:\n    -   Initialize sum $s = v$.\n    -   Initialize term $t = v$.\n    -   For $j = 1, \\dots, m$:\n        -   Update term: $t = Tt$.\n        -   Update sum: $s = s + t$.\n    -   The result is $z_k = s$.\n    Since $D_n = 2I_n$ and $R_n$ is a sparse matrix with $1$ on its first off-diagonals, the matrix-vector product $Tt$ is computationally inexpensive, with a cost of $O(n)$ floating-point operations. The total cost of applying the polynomial preconditioner is $O(m \\cdot n)$.\n\nFor the test case $(n,m)=(100,0)$, the polynomial preconditioner $P_0$ simplifies to $P_0 = (D_n^{-1}R_n)^0 D_n^{-1} = I_n D_n^{-1} = D_n^{-1}$. This is identical to the Jacobi preconditioner. Therefore, we expect $k_{\\mathrm{jac}}(100) = k_{\\mathrm{poly}}(100,0)$, which serves as a consistency check for the implementation.\n\nThe final implementation will construct the sparse matrices $A_n$ and $R_n$ and the vector $b$, then execute the PCG algorithm for each of the three preconditioning strategies for each test case $(n,m)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\n\ndef get_matrices(n):\n    \"\"\"Constructs the sparse matrices A_n and R_n.\"\"\"\n    # A_n is the 1D finite difference matrix: tridiagonal(-1, 2, -1)\n    diagonals_A = [-1 * np.ones(n - 1), 2 * np.ones(n), -1 * np.ones(n - 1)]\n    offsets_A = [-1, 0, 1]\n    A_n = sparse.diags(diagonals_A, offsets_A, shape=(n, n), format='csr')\n\n    # R_n = D_n - A_n. Since D_n = 2*I, R_n has 0 on diagonal, 1 on off-diagonals.\n    diagonals_R = [np.ones(n - 1), np.ones(n - 1)]\n    offsets_R = [-1, 1]\n    R_n = sparse.diags(diagonals_R, offsets_R, shape=(n, n), format='csr')\n    \n    return A_n, R_n\n\ndef poly_preconditioner_solve(r, R_matrix, m):\n    \"\"\"Computes z = P_m * r.\"\"\"\n    n = r.shape[0]\n    \n    # D_n = 2*I, so D_n_inv is a scaling by 0.5\n    d_inv_r = 0.5 * r\n    \n    # T = D_n_inv * R_n is a scaling of R_n by 0.5\n    T_mat = 0.5 * R_matrix\n\n    z = d_inv_r.copy()\n    t = d_inv_r.copy()\n    \n    for _ in range(m):\n        t = T_mat @ t\n        z += t\n        \n    return z\n\ndef pcg_solve(A, b, x_init, m_solve, tol, max_iter):\n    \"\"\"\n    Solves Ax=b using the Preconditioned Conjugate Gradient method.\n    \n    Args:\n        A: The system matrix (sparse).\n        b: The right-hand side vector.\n        x_init: The initial guess vector.\n        m_solve: A function that computes z = M_inv * r.\n        tol: The relative residual tolerance.\n        max_iter: The maximum number of iterations.\n        \n    Returns:\n        The number of iterations performed.\n    \"\"\"\n    x = x_init.copy()\n    r = b - A @ x\n    \n    norm_b = np.linalg.norm(b)\n    if norm_b == 0.0:\n        return 0\n\n    if np.linalg.norm(r) / norm_b = tol:\n        return 0\n\n    z = m_solve(r)\n    p = z.copy()\n    rz_old = r.dot(z)\n\n    for k in range(max_iter):\n        Ap = A @ p\n        alpha = rz_old / p.dot(Ap)\n        \n        x += alpha * p\n        r_new = r - alpha * Ap\n\n        if np.linalg.norm(r_new) / norm_b = tol:\n            return k + 1\n\n        z_new = m_solve(r_new)\n        rz_new = r_new.dot(z_new)\n        \n        beta = rz_new / rz_old\n        \n        p = z_new + beta * p\n        r = r_new\n        rz_old = rz_new\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (100, 0),\n        (100, 3),\n        (200, 3),\n        (5, 10),\n    ]\n\n    results = []\n    tau = 1e-8\n\n    for n, m in test_cases:\n        A_n, R_n = get_matrices(n)\n        b = np.ones(n)\n        x0 = np.zeros(n)\n        max_iter = n\n\n        # 1. Unpreconditioned CG\n        m_solve_unpre = lambda r: r\n        k_unpre = pcg_solve(A_n, b, x0, m_solve_unpre, tau, max_iter)\n        \n        # 2. Jacobi preconditioned CG\n        m_solve_jac = lambda r: 0.5 * r\n        k_jac = pcg_solve(A_n, b, x0, m_solve_jac, tau, max_iter)\n        \n        # 3. Polynomial preconditioned CG\n        m_solve_poly = lambda r: poly_preconditioner_solve(r, R_n, m)\n        k_poly = pcg_solve(A_n, b, x0, m_solve_poly, tau, max_iter)\n        \n        results.append([k_unpre, k_jac, k_poly])\n\n    # Format the output string as specified, with no extra whitespace.\n    inner_strings = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2406599"}]}