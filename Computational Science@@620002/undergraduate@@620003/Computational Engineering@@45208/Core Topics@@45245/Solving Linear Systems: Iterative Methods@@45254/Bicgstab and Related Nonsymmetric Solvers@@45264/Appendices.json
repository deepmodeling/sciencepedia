{"hands_on_practices": [{"introduction": "Understanding a complex algorithm like BiCGSTAB begins with demystifying its internal mechanics. This first exercise invites you to trace the algorithm's execution step-by-step on a small, manageable linear system [@problem_id:2374444]. By manually computing the residuals, search directions, and scalar coefficients in exact arithmetic, you will gain a concrete feel for how BiCGSTAB iteratively refines a solution and builds the Krylov subspace that underpins its operation.", "problem": "You will apply the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method to a nonsymmetric linear system to explicitly illustrate that, in exact arithmetic and with a standard choice of parameters, it requires two iterations to converge starting from the zero initial guess. Consider the linear system with\n- matrix $A = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$,\n- right-hand side $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$,\n- identity preconditioner, and\n- shadow residual $\\hat{r} = r_0$, where $r_0 = b - A x_0$.\n\nAll inner products are the standard Euclidean ones. Work in exact arithmetic (that is, do not round intermediate quantities). Perform the BiCGSTAB iterations step-by-step, starting from the core definitions of residuals, search directions, and scalar coefficients that define the BiCGSTAB recurrence, and show that:\n1. After the first iteration, the residual is nonzero.\n2. After the second iteration, the method reaches the exact solution (residual equal to the zero vector).\n\nExplicitly compute and report all intermediate vectors and scalars used by BiCGSTAB up to convergence to substantiate these claims. Finally, provide the exact value of the first smoothing parameter $\\omega_1$ produced during the first BiCGSTAB iteration for this system. Express your final answer as an exact rational number. Do not round.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Method:** Bi-Conjugate Gradient Stabilized (BiCGSTAB)\n- **Linear System:** $A x = b$\n- **Matrix:** $A = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$\n- **Right-hand side:** $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **Initial guess:** $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **Preconditioner:** Identity matrix ($M=I$)\n- **Shadow residual:** $\\hat{r}_0 = r_0$, where $r_0 = b - A x_0$\n- **Inner product:** Standard Euclidean inner product, denoted as $(u, v) = u^T v$.\n- **Arithmetic:** Exact arithmetic is required.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a direct and standard application of the BiCGSTAB algorithm, a well-established method in numerical linear algebra for solving nonsymmetric linear systems. It is scientifically sound.\n- **Well-Posedness:** The matrix $A$ has determinant $\\det(A) = (1)(4) - (2)(3) = 4 - 6 = -2 \\neq 0$. Thus, $A$ is invertible, and the linear system $Ax=b$ has a unique solution. The BiCGSTAB algorithm is a well-defined procedure. For a system of size $N \\times N$, Krylov subspace methods like BiCGSTAB are guaranteed to find the exact solution in at most $N$ iterations in exact arithmetic. For this $2 \\times 2$ system, convergence in exactly $2$ iterations is a plausible and verifiable claim.\n- **Objectivity:** The problem is stated using precise mathematical language, free of any subjectivity or ambiguity.\n\n**Verdict:** The problem is valid as it is scientifically grounded, well-posed, and objective. It is a formalizable problem within the field of computational engineering. We may proceed to the solution.\n\nThe solution will be constructed by applying the BiCGSTAB algorithm step-by-step. The algorithm is as follows:\n\n**Initialization:**\n1. Given an initial guess $x_0$.\n2. Compute the initial residual $r_0 = b - Ax_0$.\n3. Choose a shadow residual vector $\\hat{r}_0$, such that $(\\hat{r}_0, r_0) \\neq 0$. Here, we are given $\\hat{r}_0 = r_0$.\n4. Set initial parameters for the recurrence: $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n5. Set initial search directions: $p_0 = \\mathbf{0}$, $v_0 = \\mathbf{0}$.\n\n**Main Loop (for $k = 1, 2, \\dots$):**\n1. $\\rho_k = (\\hat{r}_0, r_{k-1})$\n2. $\\beta_k = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_{k-1}}{\\omega_{k-1}}$\n3. $p_k = r_{k-1} + \\beta_k (p_{k-1} - \\omega_{k-1} v_{k-1})$\n4. $v_k = A p_k$\n5. $\\alpha_k = \\frac{\\rho_k}{(\\hat{r}_0, v_k)}$\n6. $s_k = r_{k-1} - \\alpha_k v_k$\n7. $t_k = A s_k$\n8. $\\omega_k = \\frac{(t_k, s_k)}{(t_k, t_k)}$\n9. $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k$\n10. $r_k = s_k - \\omega_k t_k$\n\nWe will now apply this algorithm to the given system.\n\n**Initialization ($k=0$):**\n- The system is defined by $A = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$ and $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n- The initial residual is $r_0 = b - Ax_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The shadow residual is $\\hat{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The initial parameters are set to $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n- The initial direction vectors are $p_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $v_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Iteration 1 ($k=1$):**\n1. $\\rho_1 = (\\hat{r}_0, r_0) = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\n2. $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0} = \\frac{1}{1} \\frac{1}{1} = 1$.\n3. $p_1 = r_0 + \\beta_1(p_0 - \\omega_0 v_0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 1 \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n4. $v_1 = A p_1 = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$.\n5. The denominator for $\\alpha_1$ is $(\\hat{r}_0, v_1) = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = 1$. Thus, $\\alpha_1 = \\frac{\\rho_1}{(\\hat{r}_0, v_1)} = \\frac{1}{1} = 1$.\n6. $s_1 = r_0 - \\alpha_1 v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n7. $t_1 = A s_1 = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix}$.\n8. For $\\omega_1$, we compute the inner products:\n   $(t_1, s_1) = \\begin{pmatrix} -6  -12 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = (0)(-6) + (-3)(-12) = 36$.\n   $(t_1, t_1) = \\begin{pmatrix} -6  -12 \\end{pmatrix} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = (-6)^2 + (-12)^2 = 36 + 144 = 180$.\n   So, $\\omega_1 = \\frac{(t_1, s_1)}{(t_1, t_1)} = \\frac{36}{180} = \\frac{1}{5}$.\n9. $x_1 = x_0 + \\alpha_1 p_1 + \\omega_1 s_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n10. $r_1 = s_1 - \\omega_1 t_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{6}{5} \\\\ -\\frac{12}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n\nAfter the first iteration, the residual is $r_1 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$, which is not the zero vector. This substantiates the first claim.\n\n**Iteration 2 ($k=2$):**\n1. $\\rho_2 = (\\hat{r}_0, r_1) = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\frac{6}{5}$.\n2. $\\beta_2 = \\frac{\\rho_2}{\\rho_1} \\frac{\\alpha_1}{\\omega_1} = \\frac{6/5}{1} \\frac{1}{1/5} = \\frac{6}{5} \\cdot 5 = 6$.\n3. $p_2 = r_1 + \\beta_2(p_1 - \\omega_1 v_1) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\begin{pmatrix} \\frac{4}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} + \\frac{24}{5} \\\\ -\\frac{3}{5} - \\frac{18}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{5} \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix}$.\n4. $v_2 = A p_2 = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 - \\frac{42}{5} \\\\ 18 - \\frac{84}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30-42}{5} \\\\ \\frac{90-84}{5} \\end{pmatrix} = \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix}$.\n5. The denominator for $\\alpha_2$ is $(\\hat{r}_0, v_2) = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = -\\frac{12}{5}$. Thus, $\\alpha_2 = \\frac{\\rho_2}{(\\hat{r}_0, v_2)} = \\frac{6/5}{-12/5} = -\\frac{1}{2}$.\n6. $s_2 = r_1 - \\alpha_2 v_2 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n7. Since $s_2 = \\mathbf{0}$, we have $t_2 = A s_2 = A \\mathbf{0} = \\mathbf{0}$.\n8. The formula for $\\omega_2$ becomes $\\frac{(t_2, s_2)}{(t_2, t_2)} = \\frac{0}{0}$. This is a so-called \"lucky breakdown\", which indicates that the exact solution will be found at this step. We can set $\\omega_2 = 0$.\n9. $x_2 = x_1 + \\alpha_2 p_2 + \\omega_2 s_2 = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} + 0 \\cdot \\mathbf{0} = \\begin{pmatrix} 1 - 3 \\\\ -\\frac{3}{5} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -\\frac{6}{10} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{15}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix}$.\n10. $r_2 = s_2 - \\omega_2 t_2 = \\mathbf{0} - 0 \\cdot \\mathbf{0} = \\mathbf{0}$.\n\nThe residual $r_2$ is the zero vector, which means the algorithm has converged to the exact solution. This substantiates the second claim. To verify, we check the solution:\n$A x_2 = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1(-2) + 2(\\frac{3}{2}) \\\\ 3(-2) + 4(\\frac{3}{2}) \\end{pmatrix} = \\begin{pmatrix} -2 + 3 \\\\ -6 + 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = b$.\nThe solution is correct.\n\nThe problem asks for the value of the first smoothing parameter, $\\omega_1$. As calculated in the first iteration, this value is $\\omega_1 = \\frac{1}{5}$.", "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$", "id": "2374444"}, {"introduction": "Iterative solvers do not operate in a vacuum; their behavior is deeply connected to the fundamental properties of the system's matrix. This exercise explores one such connection by setting up a problem where the right-hand side vector $b$ is an eigenvector of the matrix $A$ [@problem_id:2374426]. By observing the algorithm's first step in this special case, you will uncover how underlying linear algebra concepts can directly influence the solver's convergence path, providing a deeper insight than mechanical calculation alone.", "problem": "Consider the linear system $A x = b$ with \n$$\nA = \\begin{pmatrix}\n2  5 \\\\\n0  3\n\\end{pmatrix}.\n$$\nChoose the right-hand side $b$ to be an eigenvector of $A$ associated with the eigenvalue $2$, specifically \n$$\nb = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nAssume the BiConjugate Gradient Stabilized (BiCGSTAB) method is used to solve $A x = b$ with the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the shadow residual $\\hat{r}$ chosen as $\\hat{r} = r_0$, where $r_0 = b - A x_0$. Work in exact arithmetic.\n\nDetermine the scalar $\\alpha_0$ computed during the first BiCGSTAB iteration. Provide your answer as an exact value with no rounding.", "solution": "The problem statement must first be validated for scientific soundness, completeness, and objectivity.\n\nStep 1: Extract Givens.\nThe problem provides the following information:\n- The linear system is $A x = b$.\n- The matrix $A$ is $A = \\begin{pmatrix} 2  5 \\\\ 0  3 \\end{pmatrix}$.\n- The right-hand side vector $b$ is $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- It is claimed that $b$ is an eigenvector of $A$ associated with the eigenvalue $2$.\n- The BiConjugate Gradient Stabilized (BiCGSTAB) method is to be used.\n- The initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n- The initial shadow residual is chosen as $\\hat{r} = r_0$, which should be interpreted as $\\hat{r}_0 = r_0$.\n- The initial residual is defined as $r_0 = b - A x_0$.\n- All calculations must be performed in exact arithmetic.\n- The objective is to determine the scalar $\\alpha_0$ from the first iteration of the BiCGSTAB algorithm.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is well-defined within the field of numerical linear algebra. The BiCGSTAB algorithm is a standard iterative method. We must verify the provided premise that $b$ is an eigenvector of $A$ with eigenvalue $2$.\nLet us compute the product $A b$:\n$$\nA b = \\begin{pmatrix} 2  5 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (5)(0) \\\\ (0)(1) + (3)(0) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\nNow, let us compute the product $\\lambda b$ with $\\lambda = 2$:\n$$\n\\lambda b = 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\nSince $A b = 2 b$, the claim is factually correct. The problem is scientifically grounded, self-contained, and posed with objective, unambiguous terms. It does not violate any of the invalidity criteria. Therefore, the problem is valid.\n\nStep 3: Proceed with Solution.\nWe will now solve the problem as stated. The BiCGSTAB algorithm begins with an initialization step, followed by iterations. We are concerned with the first iteration (indexed by i=0).\n\nThe initialization steps are:\n1.  Set the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n2.  Compute the initial residual $r_0$:\n    $$\n    r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2  5 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    $$\n3.  Choose the initial shadow residual $\\hat{r}_0$. The problem specifies to choose $\\hat{r}_0 = r_0$. Thus,\n    $$\n    \\hat{r}_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    $$\n4.  Set the initial search direction $p_0 = r_0$.\n    $$\n    p_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    $$\n5.  Compute the scalar $\\rho_0$. The general formula is $\\rho_i = \\hat{r}_0^T r_i$. For $i=0$, this is $\\rho_0 = \\hat{r}_0^T r_0$.\n    $$\n    \\rho_0 = \\hat{r}_0^T r_0 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (0)(0) = 1\n    $$\nThe condition $\\rho_0 \\neq 0$ must hold for the method to start. Here, $\\rho_0=1$, so the condition is satisfied.\n\nNow, we execute the first iteration ($i=0$) to find $\\alpha_0$.\n1.  Compute the vector $v_0 = A p_0$.\n    $$\n    v_0 = A p_0 = \\begin{pmatrix} 2  5 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n    $$\n2.  Compute the scalar $\\alpha_0$. The formula for $\\alpha_i$ is $\\alpha_i = \\frac{\\rho_i}{\\hat{r}_0^T v_i}$. For $i=0$:\n    $$\n    \\alpha_0 = \\frac{\\rho_0}{\\hat{r}_0^T v_0}\n    $$\n    We have $\\rho_0 = 1$. We need to compute the denominator, $\\hat{r}_0^T v_0$.\n    $$\n    \\hat{r}_0^T v_0 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = (1)(2) + (0)(0) = 2\n    $$\n    Therefore, $\\alpha_0$ is:\n    $$\n    \\alpha_0 = \\frac{1}{2}\n    $$\n\nAn alternative, more abstract derivation confirms this result. Since $r_0 = b$, $\\hat{r}_0 = r_0 = b$, and $p_0 = r_0 = b$, the formula for $\\alpha_0$ becomes:\n$$\n\\alpha_0 = \\frac{\\rho_0}{\\hat{r}_0^T v_0} = \\frac{r_0^T r_0}{(r_0)^T (A p_0)} = \\frac{b^T b}{b^T (A b)}\n$$\nWe have already verified that $b$ is an eigenvector of $A$ with eigenvalue $\\lambda=2$, so $A b = 2 b$. Substituting this into the expression for $\\alpha_0$:\n$$\n\\alpha_0 = \\frac{b^T b}{b^T (2b)} = \\frac{b^T b}{2 (b^T b)}\n$$\nSince $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, the inner product $b^T b = (1)^2 + (0)^2 = 1$, which is non-zero. We can therefore cancel the term $b^T b$ from the numerator and denominator, yielding:\n$$\n\\alpha_0 = \\frac{1}{2}\n$$\nThis confirms the result obtained through direct computation. The problem has a unique, exact solution.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2374426"}, {"introduction": "Moving from the idealized world of exact arithmetic to real-world computation exposes new challenges, primarily the effects of finite-precision floating-point arithmetic. This practice guides you through a coding exercise to demonstrate a critical pitfall known as 'wrong convergence' [@problem_id:2374413]. You will construct a system where the solver reports a tiny residual, suggesting success, yet the approximate solution remains far from the true answer—a phenomenon caused by severe ill-conditioning of the system matrix.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real, square, nonsingular matrix and let $x_{\\star} \\in \\mathbb{R}^{n}$ be the exact solution to the linear system $A x = b$, where $b = A x_{\\star}$. Consider computing an approximate solution $\\hat{x}$ in IEEE 754 double-precision (binary64) arithmetic by iterating from the initial guess $x_0 = 0$ using the BiConjugate Gradient Stabilized (BiCGSTAB) method. The iteration must terminate when the Euclidean norm of the residual $r_k = b - A x_k$ satisfies\n$$\n\\lVert r_k \\rVert_2 \\le \\max\\left( \\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\, \\tau_{\\mathrm{abs}} \\right),\n$$\nor when a prescribed maximum number of iterations $k_{\\max}$ is reached, whichever occurs first. Declare that the method has “converged” if and only if the residual-based stopping condition is met before reaching $k_{\\max}$. After termination, compute the relative forward error\n$$\n\\varepsilon_{\\mathrm{fwd}} = \\frac{\\lVert \\hat{x} - x_{\\star} \\rVert_2}{\\lVert x_{\\star} \\rVert_2}.\n$$\nFor a fixed threshold $\\theta = 10^{-2}$, define a test to “flag wrong convergence” if and only if the method has “converged” and simultaneously $\\varepsilon_{\\mathrm{fwd}}  \\theta$. For each test case, output the integer $1$ if wrong convergence is flagged and $0$ otherwise.\n\nAll arithmetic must be carried out in IEEE 754 double-precision (binary64). Angles are not used, and no physical units are involved.\n\nTest suite. Your program must evaluate exactly the following three cases:\n\n- Case 1 (well-conditioned, nonsymmetric, happy path): Let $n = 10$. Construct $A \\in \\mathbb{R}^{10 \\times 10}$ by first forming a matrix $T$ with entries sampled independently from the uniform distribution on $[-1,1]$ using a fixed pseudorandom seed $s = 123456$, then setting\n$$\nA = T + 10 I_{10},\n$$\nwhere $I_{10}$ is the $10 \\times 10$ identity matrix. Let $x_{\\star} = \\mathbf{1} \\in \\mathbb{R}^{10}$ be the vector of all ones, and set $b = A x_{\\star}$. Use $\\tau_{\\mathrm{rel}} = 10^{-8}$, $\\tau_{\\mathrm{abs}} = 0$, and $k_{\\max} = 200$.\n\n- Case 2 (severely ill-conditioned, nearly singular $2 \\times 2$ system that induces wrong convergence under absolute stopping): Let\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 1  1 + \\epsilon \\end{bmatrix}, \\quad \\epsilon = 10^{-14}.\n$$\nLet $x_{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ and set $b = A x_{\\star}$. Use $\\tau_{\\mathrm{rel}} = 0$, $\\tau_{\\mathrm{abs}} = 10^{-12}$, and $k_{\\max} = 200$.\n\n- Case 3 (boundary case, scalar system): Let $n = 1$, $A = [3]$, $x_{\\star} = [2]$, and $b = A x_{\\star} = [6]$. Use $\\tau_{\\mathrm{rel}} = 10^{-14}$, $\\tau_{\\mathrm{abs}} = 0$, and $k_{\\max} = 20$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[z_1, z_2, z_3]$, where $z_i \\in \\{0,1\\}$ is the integer flag for Case $i$ as defined above. For example, an output line could look like\n$[0,1,0]$.", "solution": "Before proceeding to a solution, the problem statement must be rigorously validated.\n\nFirst, we extract the givens presented in the problem statement.\nThe problem concerns the solution of a linear system $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is a real, square, nonsingular matrix, $x_{\\star} \\in \\mathbb{R}^{n}$ is the exact solution, and $b = A x_{\\star}$. An approximate solution $\\hat{x}$ is computed using the BiConjugate Gradient Stabilized (BiCGSTAB) method in IEEE $754$ double-precision arithmetic, starting from an initial guess $x_0 = 0$.\n\nThe iteration termination conditions are:\n1.  The Euclidean norm of the residual $r_k = b - A x_k$ satisfies $\\lVert r_k \\rVert_2 \\le \\max\\left( \\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\, \\tau_{\\mathrm{abs}} \\right)$.\n2.  A maximum number of iterations $k_{\\max}$ is reached.\n\nThe process has \"converged\" if and only if the residual-based stopping condition (1) is met before reaching $k_{\\max}$.\n\nAfter termination, the relative forward error is computed as $\\varepsilon_{\\mathrm{fwd}} = \\frac{\\lVert \\hat{x} - x_{\\star} \\rVert_2}{\\lVert x_{\\star} \\rVert_2}$.\n\nA test for \"wrong convergence\" is defined. The test is flagged (output $1$) if and only if the method has \"converged\" and simultaneously $\\varepsilon_{\\mathrm{fwd}}  \\theta$, where the threshold is fixed at $\\theta = 10^{-2}$. Otherwise, the output is $0$.\n\nThe test suite consists of three cases:\n-   Case 1: $n = 10$. $A = T + 10 I_{10}$, where $T_{ij} \\sim U[-1,1]$ generated with seed $s = 123456$. $x_{\\star} = \\mathbf{1} \\in \\mathbb{R}^{10}$. $b = A x_{\\star}$. Parameters: $\\tau_{\\mathrm{rel}} = 10^{-8}$, $\\tau_{\\mathrm{abs}} = 0$, $k_{\\max} = 200$.\n-   Case 2: $n = 2$. $A = \\begin{bmatrix} 1  1 \\\\ 1  1 + \\epsilon \\end{bmatrix}$ with $\\epsilon = 10^{-14}$. $x_{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. $b = A x_{\\star}$. Parameters: $\\tau_{\\mathrm{rel}} = 0$, $\\tau_{\\mathrm{abs}} = 10^{-12}$, $k_{\\max} = 200$.\n-   Case 3: $n = 1$. $A = [3]$, $x_{\\star} = [2]$, $b = A x_{\\star} = [6]$. Parameters: $\\tau_{\\mathrm{rel}} = 10^{-14}$, $\\tau_{\\mathrm{abs}} = 0$, $k_{\\max} = 20$.\n\nNext, we validate these givens. The problem is rooted in numerical linear algebra, a core discipline of computational engineering. The BiCGSTAB method is a standard, well-established algorithm for solving nonsymmetric linear systems. The concepts of residual, forward error, stopping criteria, and machine precision are fundamental to this field. The problem is therefore scientifically grounded. The definitions are precise, objective, and free of ambiguity. Each test case is fully specified with all necessary data ($A$, $x_{\\star}$, and solver parameters), ensuring the problem is self-contained and well-posed. The matrices are confirmed to be nonsingular as required. There are no internal contradictions or scientifically implausible conditions.\n\nThe analysis concludes that the problem is scientifically sound, well-posed, and objective. It is deemed valid, and we may proceed to construct a solution.\n\nThe solution requires the implementation of the BiCGSTAB algorithm. For a linear system $A x = b$ and an initial guess $x_0$, the algorithm iteratively refines the solution. We begin with $x_0 = 0$ and the corresponding initial residual $r_0 = b - A x_0 = b$. The standard choice for the initial shadow residual is $\\hat{r}_0 = r_0$. We initialize scalar parameters $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$, and vector parameters $p_0 = 0$, $v_0 = 0$.\n\nFor each iteration $k = 1, 2, \\dots, k_{\\max}$:\n1.  Compute $\\rho_k = \\hat{r}_0^T r_{k-1}$. A breakdown occurs if $\\rho_k = 0$.\n2.  Compute $\\beta = (\\rho_k / \\rho_{k-1}) \\cdot (\\alpha_{k-1} / \\omega_{k-1})$.\n3.  Update the search direction: $p_k = r_{k-1} + \\beta (p_{k-1} - \\omega_{k-1} v_{k-1})$.\n4.  Compute the matrix-vector product $v_k = A p_k$.\n5.  Compute the step length $\\alpha_k = \\rho_k / (\\hat{r}_0^T v_k)$. A breakdown occurs if the denominator is zero.\n6.  Compute an intermediate residual $s = r_{k-1} - \\alpha_k v_k$.\n7.  Compute the matrix-vector product $t = A s$.\n8.  Compute the stabilization parameter $\\omega_k = (t^T s) / (t^T t)$. A breakdown can occur if $t^T t = 0$.\n9.  Update the solution: $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s$.\n10. Update the residual: $r_k = s - \\omega_k t$.\n11. Check for convergence: test if $\\lVert r_k \\rVert_2 \\le \\max(\\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\tau_{\\mathrm{abs}})$. If true, terminate with `converged = True`.\n\nIf the loop completes without meeting the convergence criterion, the process terminates with `converged = False`. The final approximate solution is denoted $\\hat{x}$.\n\nFor each test case, this algorithm will be executed. After termination, we compute the relative forward error $\\varepsilon_{\\mathrm{fwd}} = \\lVert \\hat{x} - x_{\\star} \\rVert_2 / \\lVert x_{\\star} \\rVert_2$. The \"wrong convergence\" flag is set to $1$ if the algorithm reported convergence (`converged = True`) but the solution is poor ($\\varepsilon_{\\mathrm{fwd}}  10^{-2}$). Otherwise, the flag is $0$.\n\nCase 1 represents a typical well-conditioned problem. The condition number of $A$ is small, so we expect BiCGSTAB to converge rapidly and for the small final residual to correspond to a small forward error. The flag should be $0$.\n\nCase 2 is designed to expose a classic pathology. The matrix $A$ is severely ill-conditioned. The true solution is $x_{\\star} = [1, -1]^T$, resulting in a very small right-hand side vector $b = [\\epsilon, -\\epsilon]^T$. The initial residual $r_0=b$ has a norm of $\\sqrt{2}\\epsilon \\approx 1.414 \\times 10^{-14}$. This is smaller than the absolute tolerance $\\tau_{\\mathrm{abs}} = 10^{-12}$. Consequently, the algorithm will terminate at iteration $k=0$ with `converged = True` and $\\hat{x} = x_0 = 0$. The forward error will be $\\varepsilon_{\\mathrm{fwd}} = \\lVert 0 - x_{\\star} \\rVert_2 / \\lVert x_{\\star} \\rVert_2 = 1$, which is greater than $\\theta=10^{-2}$. Thus, the flag for this case will be $1$.\n\nCase 3 is a simple $1 \\times 1$ system. For such a simple case, BiCGSTAB is expected to find the exact solution in a single iteration. The forward error will be $0$, so the flag will be $0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_bicgstab(A, b, x0, tau_rel, tau_abs, k_max):\n    \"\"\"\n    Implements the BiConjugate Gradient Stabilized (BiCGSTAB) method.\n\n    Args:\n        A (np.ndarray): The square matrix of the linear system.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tau_rel (float): The relative tolerance for the stopping condition.\n        tau_abs (float): The absolute tolerance for the stopping condition.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The approximate solution vector.\n            - bool: A flag indicating if the method converged.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use IEEE 754 double precision (np.float64) for all arithmetic\n    x = x0.copy().astype(np.float64)\n    r = (b - A @ x).astype(np.float64)\n\n    norm_b = np.linalg.norm(b)\n    stop_tol = max(tau_rel * norm_b, tau_abs)\n\n    # Check for convergence at k=0\n    if np.linalg.norm(r) = stop_tol:\n        return x, True\n\n    # BiCGSTAB algorithm setup\n    r_hat0 = r.copy()\n    rho_prev = 1.0\n    alpha = 1.0\n    omega = 1.0\n    p = np.zeros(n, dtype=np.float64)\n    v = np.zeros(n, dtype=np.float64)\n\n    for k in range(1, k_max + 1):\n        rho_curr = np.dot(r_hat0, r)\n        \n        # Breakdown condition 1\n        if rho_curr == 0.0:\n            return x, False\n\n        # On the first iteration, beta calculation uses rho_prev=1, alpha=1, omega=1\n        # which effectively makes p = r, as p_prev and v_prev are zero.\n        beta = (rho_curr / rho_prev) * (alpha / omega)\n        p = r + beta * (p - omega * v)\n\n        v = A @ p\n        \n        r_hat0_dot_v = np.dot(r_hat0, v)\n        \n        # Breakdown condition 2\n        if r_hat0_dot_v == 0.0:\n            return x, False\n        \n        alpha = rho_curr / r_hat0_dot_v\n\n        s = r - alpha * v\n        t = A @ s\n        \n        t_dot_t = np.dot(t, t)\n        \n        # Breakdown condition 3: t is numerically zero.\n        if t_dot_t  np.finfo(np.float64).eps:\n            x = x + alpha * p\n            r = s\n            # The residual is now 's'. Check convergence and return.\n            return x, np.linalg.norm(r) = stop_tol\n\n        omega = np.dot(t, s) / t_dot_t\n\n        x = x + alpha * p + omega * s\n        r = s - omega * t\n        \n        # Check termination condition after iteration k\n        if np.linalg.norm(r) = stop_tol:\n            return x, True\n\n        # Update for next iteration\n        rho_prev = rho_curr\n\n    return x, False # Max iterations reached\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the BiCGSTAB problem.\n    \"\"\"\n    theta = 1e-2\n\n    # Case 1: Well-conditioned, nonsymmetric\n    n1 = 10\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    T1 = rng.uniform(-1, 1, (n1, n1))\n    A1 = T1 + 10 * np.eye(n1, dtype=np.float64)\n    x_star1 = np.ones(n1, dtype=np.float64)\n    b1 = A1 @ x_star1\n    tau_rel1 = 1e-8\n    tau_abs1 = 0.0\n    k_max1 = 200\n\n    # Case 2: Ill-conditioned, wrong convergence\n    epsilon = 1e-14\n    A2 = np.array([[1, 1], [1, 1 + epsilon]], dtype=np.float64)\n    x_star2 = np.array([1, -1], dtype=np.float64)\n    b2 = A2 @ x_star2\n    tau_rel2 = 0.0\n    tau_abs2 = 1e-12\n    k_max2 = 200\n\n    # Case 3: Scalar system\n    A3 = np.array([[3]], dtype=np.float64)\n    x_star3 = np.array([2], dtype=np.float64)\n    b3 = A3 @ x_star3\n    tau_rel3 = 1e-14\n    tau_abs3 = 0.0\n    k_max3 = 20\n\n    test_cases = [\n        (A1, x_star1, b1, tau_rel1, tau_abs1, k_max1),\n        (A2, x_star2, b2, tau_rel2, tau_abs2, k_max2),\n        (A3, x_star3, b3, tau_rel3, tau_abs3, k_max3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, x_star, b, tau_rel, tau_abs, k_max = case\n        n = A.shape[0]\n        x0 = np.zeros(n, dtype=np.float64)\n        \n        hat_x, converged = run_bicgstab(A, b, x0, tau_rel, tau_abs, k_max)\n        \n        norm_x_star = np.linalg.norm(x_star)\n        \n        # The problem guarantees x_star is not zero, so norm_x_star is non-zero.\n        eps_fwd = np.linalg.norm(hat_x - x_star) / norm_x_star\n\n        flag = 0\n        if converged and eps_fwd > theta:\n            flag = 1\n        \n        results.append(flag)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2374413"}]}