{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first isolate the core mathematical principle behind Richardson extrapolation. This exercise [@problem_id:456729] demonstrates how a clever linear combination of two approximations, one with a step size $h$ and another with $h/2$, can systematically cancel the leading error term. By working through this foundational example, you will develop an intuition for how \"accelerated\" estimates are constructed from an algebraic standpoint, before we apply this powerful idea to more complex numerical algorithms.", "problem": "Consider the function defined by $ f(h) = \\frac{\\arcsin(h) - h}{h^3} $ for $ h \\neq 0 $. The limit of $ f(h) $ as $ h \\to 0 $ is to be approximated using Richardson extrapolation. Richardson extrapolation is a sequence acceleration method that combines approximations from different step sizes to eliminate lower-order error terms.  \n\nAssume that the error in the approximation of the limit by $ f(h) $ has an asymptotic expansion in even powers of $ h $. Using step sizes $ h $ and $ h/2 $, apply Richardson extrapolation to $ f(h) $ and determine the limit as $ h \\to 0 $ of the resulting extrapolated expression.", "solution": "The Richardson extrapolation method assumes that $ f(h) $ has an asymptotic expansion of the form:  \n$$ f(h) = L + c_2 h^2 + c_4 h^4 + O(h^6), $$  \nwhere $ L $ is the desired limit, and $ c_2, c_4, \\ldots $ are constants.  \n\nEvaluate $ f $ at step sizes $ h $ and $ h/2 $:  \n$$ f(h) = L + c_2 h^2 + c_4 h^4 + O(h^6), $$  \n$$ f\\left(\\frac{h}{2}\\right) = L + c_2 \\left(\\frac{h}{2}\\right)^2 + c_4 \\left(\\frac{h}{2}\\right)^4 + O(h^6) = L + \\frac{c_2}{4} h^2 + \\frac{c_4}{16} h^4 + O(h^6). $$  \n\nForm a linear combination to eliminate the $ h^2 $ term:  \n$$ R(h) = a f(h) + b f\\left(\\frac{h}{2}\\right), $$  \nwhere $ a $ and $ b $ are chosen such that:  \n1. The constant term is $ L $: $ a + b = 1 $.  \n2. The coefficient of $ h^2 $ is zero: $ a c_2 h^2 + b \\left( \\frac{c_2}{4} h^2 \\right) = 0 $, which simplifies to $ a + \\frac{b}{4} = 0 $.  \n\nSolve the system of equations:  \n$$ a + b = 1, $$  \n$$ a + \\frac{b}{4} = 0. $$  \nSubtract the second equation from the first:  \n$$ (a + b) - \\left(a + \\frac{b}{4}\\right) = 1 - 0 \\implies \\frac{3b}{4} = 1 \\implies b = \\frac{4}{3}. $$  \nSubstitute into the first equation:  \n$$ a + \\frac{4}{3} = 1 \\implies a = 1 - \\frac{4}{3} = -\\frac{1}{3}. $$  \nThus, the extrapolated expression is:  \n$$ R(h) = -\\frac{1}{3} f(h) + \\frac{4}{3} f\\left(\\frac{h}{2}\\right) = \\frac{4 f\\left(\\frac{h}{2}\\right) - f(h)}{3}. $$  \n\nThe limit as $ h \\to 0 $ of $ R(h) $ is required. Since $ R(h) $ eliminates the $ h^2 $ term, it converges to $ L $ faster than $ f(h) $. However, to find $ L $ explicitly, use the Taylor series expansion of $ \\arcsin(h) $:  \n$$ \\arcsin(h) = h + \\frac{1}{6} h^3 + \\frac{3}{40} h^5 + O(h^7). $$  \nSubstitute into $ f(h) $:  \n$$ f(h) = \\frac{\\left( h + \\frac{1}{6} h^3 + \\frac{3}{40} h^5 + O(h^7) \\right) - h}{h^3} = \\frac{\\frac{1}{6} h^3 + \\frac{3}{40} h^5 + O(h^7)}{h^3} = \\frac{1}{6} + \\frac{3}{40} h^2 + O(h^4). $$  \nSimilarly,  \n$$ f\\left(\\frac{h}{2}\\right) = \\frac{1}{6} + \\frac{3}{40} \\left(\\frac{h}{2}\\right)^2 + O(h^4) = \\frac{1}{6} + \\frac{3}{40} \\cdot \\frac{h^2}{4} + O(h^4) = \\frac{1}{6} + \\frac{3}{160} h^2 + O(h^4). $$  \n\nNow substitute into $ R(h) $:  \n$$ 4 f\\left(\\frac{h}{2}\\right) = 4 \\left( \\frac{1}{6} + \\frac{3}{160} h^2 + O(h^4) \\right) = \\frac{4}{6} + \\frac{12}{160} h^2 + O(h^4) = \\frac{2}{3} + \\frac{3}{40} h^2 + O(h^4), $$  \n$$ f(h) = \\frac{1}{6} + \\frac{3}{40} h^2 + O(h^4). $$  \nThus,  \n$$ 4 f\\left(\\frac{h}{2}\\right) - f(h) = \\left( \\frac{2}{3} + \\frac{3}{40} h^2 \\right) - \\left( \\frac{1}{6} + \\frac{3}{40} h^2 \\right) + O(h^4) = \\frac{2}{3} - \\frac{1}{6} + \\left( \\frac{3}{40} - \\frac{3}{40} \\right) h^2 + O(h^4) = \\frac{4}{6} - \\frac{1}{6} + O(h^4) = \\frac{3}{6} + O(h^4) = \\frac{1}{2} + O(h^4). $$  \nTherefore,  \n$$ R(h) = \\frac{4 f\\left(\\frac{h}{2}\\right) - f(h)}{3} = \\frac{ \\frac{1}{2} + O(h^4) }{3} = \\frac{1}{6} + O(h^4). $$  \n\nTaking the limit as $ h \\to 0 $:  \n$$ \\lim_{h \\to 0} R(h) = \\frac{1}{6}. $$", "answer": "$$ \\boxed{\\dfrac{1}{6}} $$", "id": "456729"}, {"introduction": "Having grasped the principle of error cancellation, we now apply it to a common task in computational engineering: solving ordinary differential equations (ODEs). In this practice [@problem_id:2433093], you will enhance the simple but widely understood Forward Euler method by creating a composite solver that automatically performs one step of Richardson extrapolation. This task provides direct, hands-on experience in building a higher-order method from a lower-order one and quantifying the resulting improvement in accuracy, bridging the gap between theory and practical implementation.", "problem": "Design and implement a composite numerical method that, given an initial value problem for an ordinary differential equation of the form $y^{\\prime}(t) = f(t,y)$ with initial condition $y(t_0) = y_0$, automatically applies one step of Richardson extrapolation to a base first-order one-step method to accelerate the convergence of the global error at a specified final time $T$. The base method must be the forward Euler method, which advances from $t_n$ to $t_{n+1}$ using a uniform time step $h = (T - t_0)/N$ for some integer $N \\geq 1$. The composite method must perform two runs of the base method with step sizes $h$ and $h/2$ (that is, with $N$ and $2N$ steps, respectively), and then combine the two approximations at the same final time $T$ to cancel the leading-order term in the global error without using any pre-specified higher-order formula. Your implementation must not call any built-in ordinary differential equation solvers.\n\nPrincipled requirements:\n- Start from the fundamental definition of the initial value problem $y^{\\prime}(t) = f(t,y)$ with $y(t_0) = y_0$, and the definition of a one-step method’s global error at time $T$ as the difference between the numerical approximation and the exact solution $y(T)$.\n- Assume only well-tested facts about the order of accuracy: the forward Euler method has a global error that scales linearly with the step size $h$ for sufficiently smooth $f$, i.e., the global error is $\\mathcal{O}(h)$ as $h \\to 0$.\n- Apply a single step of the Richardson idea: use two approximations computed with step sizes $h$ and $h/2$ and combine them so that the leading $\\mathcal{O}(h)$ term in the global error is canceled, resulting in an approximation whose error scales as $\\mathcal{O}(h^2)$ under the same smoothness assumptions. Do not assume or use any “shortcut” formula in the problem statement; instead, your program must implement the general step-halving and cancellation principle.\n\nAngle unit requirement:\n- Whenever trigonometric functions appear, interpret their arguments in radians.\n\nTest suite:\nImplement your composite method and evaluate it on the following four initial value problems. For each case, compute:\n- the base forward Euler approximation at $T$ using $N$ steps,\n- the base forward Euler approximation at $T$ using $2N$ steps,\n- the extrapolated approximation at $T$ constructed by canceling the leading $\\mathcal{O}(h)$ global error term from the two approximations,\n- the absolute global error of the base method with $N$ steps, denoted $E_{\\mathrm{base}} = \\lvert Y_h(T) - y(T) \\rvert$,\n- the absolute global error of the extrapolated approximation, denoted $E_{\\mathrm{extra}} = \\lvert Y_{\\mathrm{extra}}(T) - y(T) \\rvert$,\n- the improvement factor $\\rho = E_{\\mathrm{base}}/E_{\\mathrm{extra}}$.\n\nThe four test cases are:\n1. $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, $N = 10$. Exact solution $y(t) = e^{-t}$.\n2. $f(t,y) = \\sin(t)$, $t_0 = 0$, $y_0 = 0$, $T = 1$, $N = 5$. Exact solution $y(t) = 1 - \\cos(t)$.\n3. $f(t,y) = -15\\,y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, $N = 10$. Exact solution $y(t) = e^{-15 t}$.\n4. $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, $N = 1$. Exact solution $y(t) = e^{-t}$.\n\nAll quantities involving trigonometric functions must use radians. No physical units are involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the four improvement factors $\\rho$ for the test cases, in the same order as listed above, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example $[\\rho_1,\\rho_2,\\rho_3,\\rho_4]$.\n- Each $\\rho$ must be a floating-point number.\n\nYour submission must be a single, complete, runnable program that adheres to the execution environment described later, takes no input, and writes exactly one line in the format above.", "solution": "The problem is determined to be valid. It is a well-posed and scientifically sound exercise in numerical analysis. We will proceed with a principled derivation and implementation.\n\nThe objective is to construct a numerical method for the initial value problem defined by the ordinary differential equation $y^{\\prime}(t) = f(t,y)$ with the initial condition $y(t_0) = y_0$. The goal is to approximate the solution $y(T)$ at a final time $T$.\n\nThe specified base algorithm is the forward Euler method. This is a one-step method that generates a sequence of approximations $Y_n$ to the true solution $y(t_n)$ at discrete time points $t_n = t_0 + n \\cdot h$. The step size $h$ is uniform, given by $h = (T - t_0)/N$ for a specified number of steps $N$. The recurrence relation for the forward Euler method is derived from a first-order Taylor expansion of the solution around $t_n$:\n$y(t_{n+1}) = y(t_n) + h \\cdot y'(t_n) + \\mathcal{O}(h^2)$.\nBy substituting $y'(t_n) = f(t_n, y(t_n))$ and truncating the higher-order terms, we obtain the numerical scheme:\n$$Y_{n+1} = Y_n + h \\cdot f(t_n, Y_n)$$\nwith $Y_0 = y_0$. After $N$ steps, this procedure yields an approximation $Y_N$ to the true solution $y(T)$. Let us denote this approximation as $Y_h(T)$.\n\nIt is a known result from numerical analysis that for a sufficiently smooth function $f$, the global error of the forward Euler method has an asymptotic expansion in powers of the step size $h$. Let $Y(T)$ be the exact solution at time $T$. The numerical approximation $Y_h(T)$ can be expressed as:\n$$Y_h(T) = Y(T) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots$$\nwhere the coefficients $C_k$ depend on the function $f$ and its derivatives but are independent of $h$. The leading term of the error is $C_1 h$, which establishes that the method has an order of accuracy of $1$, i.e., the global error is $\\mathcal{O}(h)$.\n\nThe task requires applying Richardson extrapolation to improve this accuracy. This is achieved by computing the solution with two different step sizes. We use the given step size $h$ (corresponding to $N$ steps) and a halved step size $h/2$ (corresponding to $2N$ steps). Let the respective approximations at time $T$ be $Y_h(T)$ and $Y_{h/2}(T)$. According to the error expansion, we have:\n$1$. $Y_h(T) = Y(T) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3)$\n$2$. $Y_{h/2}(T) = Y(T) + C_1 (h/2) + C_2 (h/2)^2 + \\mathcal{O}(h^3) = Y(T) + \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3)$\n\nThe objective is to find a linear combination of $Y_h(T)$ and $Y_{h/2}(T)$ that eliminates the leading error term, $C_1 h$. We seek an extrapolated approximation $Y_{\\mathrm{extra}}(T)$ of the form $\\alpha Y_{h/2}(T) + \\beta Y_h(T)$ that is a better approximation of $Y(T)$. For consistency, the approximation must be exact if the base method were exact, which implies $\\alpha + \\beta = 1$. To eliminate the $\\mathcal{O}(h)$ error term, the coefficients of $C_1 h$ in the combined error expansion must sum to zero:\n$\\alpha (\\frac{1}{2}) + \\beta (1) = 0$.\n\nWe have a system of two linear equations for the coefficients $\\alpha$ and $\\beta$:\n$$\n\\begin{cases}\n\\alpha + \\beta = 1 \\\\\n\\frac{1}{2}\\alpha + \\beta = 0\n\\end{cases}\n$$\nSubtracting the second equation from the first yields $\\frac{1}{2}\\alpha = 1$, which gives $\\alpha = 2$. Substituting this into the first equation gives $2 + \\beta = 1$, so $\\beta = -1$.\n\nThus, the extrapolated approximation is given by the formula:\n$$Y_{\\mathrm{extra}}(T) = 2 Y_{h/2}(T) - Y_h(T)$$\nLet us verify the error of this new approximation. The error is $Y_{\\mathrm{extra}}(T) - Y(T)$.\n$Y_{\\mathrm{extra}}(T) - Y(T) = (2 Y_{h/2}(T) - Y_h(T)) - Y(T)$\n$Y_{\\mathrm{extra}}(T) - Y(T) = 2(Y_{h/2}(T) - Y(T)) - (Y_h(T) - Y(T))$\nUsing the error expansions:\n$Y_{\\mathrm{extra}}(T) - Y(T) = 2 \\left( \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3) \\right) - (C_1 h + C_2 h^2 + \\mathcal{O}(h^3))$\n$Y_{\\mathrm{extra}}(T) - Y(T) = (C_1 h + \\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)) - (C_1 h + C_2 h^2 + \\mathcal{O}(h^3))$\n$Y_{\\mathrm{extra}}(T) - Y(T) = -\\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)$\n\nThe leading error term is now of order $\\mathcal{O}(h^2)$, confirming that the extrapolated method has an order of accuracy of $2$.\n\nThe implementation will consist of a function that performs the forward Euler integration for a given number of steps. This function will be called twice: once with $N$ steps and once with $2N$ steps. The resulting approximations, $Y_h(T)$ and $Y_{h/2}(T)$, are then combined using the derived formula to compute $Y_{\\mathrm{extra}}(T)$. Finally, the absolute global errors $E_{\\mathrm{base}} = \\lvert Y_h(T) - y(T) \\rvert$ and $E_{\\mathrm{extra}} = \\lvert Y_{\\mathrm{extra}}(T) - y(T) \\rvert$ are calculated, and their ratio $\\rho = E_{\\mathrm{base}} / E_{\\mathrm{extra}}$ is determined for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of applying Richardson extrapolation to the forward Euler method\n    for several ODE test cases and computes the error improvement factor.\n    \"\"\"\n\n    def forward_euler(f, t0, y0, T, N):\n        \"\"\"\n        Solves y'(t) = f(t, y) using the forward Euler method.\n        \n        Args:\n            f: The function defining the ODE, f(t, y).\n            t0: Initial time.\n            y0: Initial value.\n            T: Final time.\n            N: Number of steps.\n\n        Returns:\n            The numerical approximation of y(T).\n        \"\"\"\n        if N == 0:\n            return y0\n        \n        h = (T - t0) / N\n        t = t0\n        y = y0\n        \n        for _ in range(N):\n            y = y + h * f(t, y)\n            t = t + h\n            \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N\": 10,\n            \"y_exact\": lambda t: np.exp(-t)\n        },\n        {\n            \"f\": lambda t, y: np.sin(t),\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 1.0,\n            \"N\": 5,\n            \"y_exact\": lambda t: 1.0 - np.cos(t)\n        },\n        {\n            \"f\": lambda t, y: -15.0 * y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N\": 10,\n            \"y_exact\": lambda t: np.exp(-15.0 * t)\n        },\n        {\n            \"f\": lambda t, y: -y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N\": 1,\n            \"y_exact\": lambda t: np.exp(-t)\n        }\n    ]\n\n    rho_values = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        t0 = case[\"t0\"]\n        y0 = case[\"y0\"]\n        T = case[\"T\"]\n        N = case[\"N\"]\n        y_exact_func = case[\"y_exact\"]\n\n        # Compute the base approximation with N steps (step size h)\n        Y_h = forward_euler(f, t0, y0, T, N)\n        \n        # Compute the base approximation with 2N steps (step size h/2)\n        Y_h_div_2 = forward_euler(f, t0, y0, T, 2 * N)\n\n        # Compute the extrapolated approximation\n        Y_extra = 2.0 * Y_h_div_2 - Y_h\n\n        # Compute the exact value at the final time T\n        y_exact_at_T = y_exact_func(T)\n\n        # Compute the absolute global error of the base method\n        E_base = np.abs(Y_h - y_exact_at_T)\n        \n        # Compute the absolute global error of the extrapolated approximation\n        E_extra = np.abs(Y_extra - y_exact_at_T)\n\n        # Compute the improvement factor rho\n        if E_extra == 0.0:\n            # If extrapolated error is zero, improvement is infinite (assuming base error is not zero).\n            # This is unlikely with floating point arithmetic for these problems.\n            rho = float('inf') if E_base != 0.0 else 1.0\n        else:\n            rho = E_base / E_extra\n            \n        rho_values.append(rho)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{val:.6f}\" for val in rho_values]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2433093"}, {"introduction": "A true expert not only knows how to apply a technique but, more importantly, understands its limitations. This final practice [@problem_id:2435348] is a crucial test of your understanding of the foundational assumptions behind extrapolation, specifically in the context of Romberg integration. By analyzing why the method fails for an apparently simple integral of a non-smooth function, you will learn to identify the critical requirement of a well-behaved error series, a lesson that is vital for the robust application of any numerical method.", "problem": "Consider applying Romberg integration (successive Richardson extrapolation built on the composite trapezoidal rule with step sizes $h, h/2, h/4, \\ldots$) to evaluate the definite integral $\\int_{-1}^{1} |x| \\, dx$. The composite trapezoidal rule on a uniform grid of $N$ subintervals has step size $h = \\frac{2}{N}$. Your task is to reason, starting from the differentiability properties of the integrand and the structure of the composite trapezoidal rule, about whether and why Romberg integration does or does not accelerate convergence for this integral.\n\nWhich statement best explains the behavior?\n\nA. The integrand $|x|$ is not differentiable at $x=0$, so the even-power error expansion assumed by Romberg does not hold. For the composite trapezoidal rule, the global error is parity-dependent: it is $E_N = \\frac{h^2}{4}$ when $N$ is odd (the kink lies strictly inside one panel) and $E_N = 0$ when $N$ is even (the kink coincides with a node and each panel sees a linear function). Because the leading error coefficient is not a fixed constant across refinements, the first Richardson extrapolation from $h$ to $h/2$ can produce a value far from the exact integral.\n\nB. Romberg integration fails because the integral is improper and diverges at $x=0$, so no quadrature rule can be meaningfully applied.\n\nC. Romberg integration fails because $|x|$ is an odd function over symmetric limits, and cancellation near $x=0$ invalidates the extrapolation process.\n\nD. Romberg integration would succeed if the initial step size $h$ were made sufficiently small; the cusp at $x=0$ effectively disappears for small enough $h$, restoring the smooth even-power error expansion that Romberg requires.\n\nE. Romberg integration fails because the composite trapezoidal rule is only first-order accurate globally, and an even-power Richardson extrapolation is therefore inapplicable regardless of smoothness considerations.", "solution": "The problem requires an analysis of the performance of Romberg integration for the definite integral $I = \\int_{-1}^{1} |x| \\, dx$. The core of the analysis rests on the theoretical foundation of Romberg integration and the properties of the integrand $f(x) = |x|$.\n\nFirst, we establish the exact value of the integral. The integrand $f(x) = |x|$ is an even function, so the integral over the symmetric interval $[-1, 1]$ can be calculated as:\n$$ I = \\int_{-1}^{1} |x| \\, dx = 2 \\int_{0}^{1} x \\, dx = 2 \\left[ \\frac{x^2}{2} \\right]_{0}^{1} = 2 \\left( \\frac{1^2}{2} - \\frac{0^2}{2} \\right) = 1 $$\nThe exact value of the integral is $1$.\n\nNext, we analyze the basis of Romberg integration. Romberg integration is a method that uses Richardson extrapolation to accelerate the convergence of the composite trapezoidal rule. The composite trapezoidal rule approximation $T(h)$ for an integral $I$, with step size $h$, has an asymptotic error expansion given by the Euler-Maclaurin formula:\n$$ T(h) = I + C_1 h^2 + C_2 h^4 + C_3 h^6 + \\dots $$\nThis expansion is valid provided the integrand $f(x)$ is sufficiently smooth (i.e., has a sufficient number of continuous derivatives) on the integration interval $[a, b]$. Specifically, for the expansion to contain only even powers of $h$, the integrand and all its odd-order derivatives must vanish at the endpoints $a$ and $b$, or the function must be periodic with period $b-a$. For a general non-periodic function, the expansion is $T(h) = I + C_1 h^2 + C_2 h^4 + \\dots + K_p h^{2p} + O(h^{2p+2})$, which requires the integrand to be at least in the class $C^{2p+2}[a, b]$.\n\nThe integrand in this problem is $f(x) = |x|$. This function is continuous everywhere. However, its first derivative is the signum function, $f'(x) = \\text{sgn}(x)$, which has a jump discontinuity at $x=0$.\n$$ f'(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x > 0 \\end{cases} $$\nSince $f'(x)$ is not continuous at $x=0$, the function $f(x)=|x|$ is not in the class $C^1[-1, 1]$. Consequently, it is also not in $C^2[-1, 1]$. The fundamental smoothness requirement for the standard Euler-Maclaurin error expansion is violated. The error of the composite trapezoidal rule will not have the form with only even powers of $h$, and therefore, the premise for Romberg integration is invalid.\n\nWe must now analyze the specific behavior of the composite trapezoidal rule for this particular integrand. The rule is applied on a uniform grid of $N$ subintervals over $[-1, 1]$, with step size $h = \\frac{2}{N}$ and nodes $x_k = -1 + k h$ for $k=0, 1, \\dots, N$. The behavior depends critically on whether the point of non-differentiability, $x=0$, is a grid node.\n\nCase 1: $N$ is an even number.\nLet $N=2m$ for some integer $m \\ge 1$. The step size is $h = \\frac{2}{2m} = \\frac{1}{m}$. The grid nodes are $x_k = -1 + \\frac{k}{m}$. The node corresponding to $k=m$ is $x_m = -1 + \\frac{m}{m} = 0$. Thus, when $N$ is even, the point of non-differentiability $x=0$ is always a grid node. The composite trapezoidal rule sums the results over each subinterval. In the subintervals within $[-1, 0]$, the integrand is $f(x)=-x$, a linear function. In the subintervals within $[0, 1]$, the integrand is $f(x)=x$, also a linear function. The trapezoidal rule is exact for linear functions (its error term involves the second derivative of the integrand, which is zero for a linear function). Therefore, the approximation is exact for every subinterval, and the total computed integral is exact. The error $E_N = T_N - I = 0$ when $N$ is even.\n\nCase 2: $N$ is an odd number.\nLet $N=2m+1$ for some integer $m \\ge 0$. The step size is $h = \\frac{2}{2m+1}$. The grid nodes are $x_k = -1 + k \\frac{2}{2m+1}$. For $x_k=0$, we would need $-1 + k \\frac{2}{2m+1} = 0$, which implies $k = \\frac{2m+1}{2}$. This is never an integer. Thus, when $N$ is odd, the point $x=0$ always falls strictly inside one of the subintervals. This subinterval is $[x_m, x_{m+1}]$, where $x_m = -1 + m h = \\frac{-(2m+1)+2m}{2m+1} = \\frac{-1}{2m+1}$ and $x_{m+1} = -1 + (m+1)h = \\frac{-(2m+1)+2m+2}{2m+1} = \\frac{1}{2m+1}$. Notice that this interval is symmetric around $0$ and has length $h$. The error for the composite trapezoidal rule comes exclusively from this single subinterval, as the integrand is linear on all other subintervals.\nThe trapezoidal approximation for this subinterval is:\n$$ T_{[x_m, x_{m+1}]} = \\frac{h}{2} (f(x_m) + f(x_{m+1})) = \\frac{h}{2} \\left(\\left|\\frac{-1}{2m+1}\\right| + \\left|\\frac{1}{2m+1}\\right|\\right) = \\frac{h}{2} \\left(\\frac{1}{2m+1} + \\frac{1}{2m+1}\\right) = \\frac{h}{2} \\left(\\frac{2}{2m+1}\\right) = \\frac{h^2}{2} $$\nThe exact integral over this subinterval is:\n$$ I_{[x_m, x_{m+1}]} = \\int_{-1/(2m+1)}^{1/(2m+1)} |x|\\, dx = 2 \\int_0^{1/(2m+1)} x\\, dx = \\left[x^2\\right]_0^{1/(2m+1)} = \\left(\\frac{1}{2m+1}\\right)^2 = \\left(\\frac{h}{2}\\right)^2 = \\frac{h^2}{4} $$\nThe error for this subinterval, which is the total error $E_N$, is $E_N = T_{[x_m, x_{m+1}]} - I_{[x_m, x_{m+1}]} = \\frac{h^2}{2} - \\frac{h^2}{4} = \\frac{h^2}{4}$. When $N$ is odd, $E_N = \\frac{h^2}{4} \\ne 0$.\n\nThe Richardson extrapolation formula $R_{k,1} = \\frac{4R_{k,0} - R_{k-1,0}}{3}$ assumes that the error $E(h) = C h^2 + O(h^4)$ with a constant $C$. Our analysis shows the error sequence is $E_N=h^2/4$ for odd $N$ and $E_N=0$ for even $N$. As the step size is halved ($h \\to h/2$), the number of intervals is doubled ($N \\to 2N$), which changes the parity of $N$ if $N$ is odd. This means the \"constant\" $C$ in the error term is not constant but alternates. This invalidates the extrapolation. For example, if we start with an odd $N_0$ (error $\\sim h_0^2/4$) and go to $N_1=2N_0$ (error is $0$), extrapolation will produce a poor result.\n\nNow, we evaluate each option.\n\nA. The integrand $|x|$ is not differentiable at $x=0$, so the even-power error expansion assumed by Romberg does not hold. For the composite trapezoidal rule, the global error is parity-dependent: it is $E_N = \\frac{h^2}{4}$ when $N$ is odd (the kink lies strictly inside one panel) and $E_N = 0$ when $N$ is even (the kink coincides with a node and each panel sees a linear function). Because the leading error coefficient is not a fixed constant across refinements, the first Richardson extrapolation from $h$ to $h/2$ can produce a value far from the exact integral.\nThis statement is a perfect summary of our derivation. Every claim made is correct: the non-differentiability, the failure of the error expansion, the parity-dependent error of the trapezoidal rule ($E_N=h^2/4$ for odd $N$, $E_N=0$ for even $N$), the non-constant error coefficient, and the resulting failure of extrapolation.\nVerdict: **Correct**.\n\nB. Romberg integration fails because the integral is improper and diverges at $x=0$, so no quadrature rule can be meaningfully applied.\nThe integral $\\int_{-1}^{1} |x| \\, dx$ is a proper integral. The integrand $|x|$ is continuous and bounded on the finite interval $[-1, 1]$. The integral converges to $1$. The statement is factually false.\nVerdict: **Incorrect**.\n\nC. Romberg integration fails because $|x|$ is an odd function over symmetric limits, and cancellation near $x=0$ invalidates the extrapolation process.\nThe function $f(x)=|x|$ is an even function, because $f(-x) = |-x| = |x| = f(x)$. The premise of the statement is false.\nVerdict: **Incorrect**.\n\nD. Romberg integration would succeed if the initial step size $h$ were made sufficiently small; the cusp at $x=0$ effectively disappears for small enough $h$, restoring the smooth even-power error expansion that Romberg requires.\nThe non-differentiability at $x=0$ is an intrinsic property of the function $|x|$. It does not disappear at any scale. Reducing the step size $h$ does not make the function smoother or restore the required error expansion. The parity-dependent error behavior persists regardless of the scale of $h$.\nVerdict: **Incorrect**.\n\nE. Romberg integration fails because the composite trapezoidal rule is only first-order accurate globally, and an even-power Richardson extrapolation is therefore inapplicable regardless of smoothness considerations.\nThe composite trapezoidal rule is second-order accurate ($O(h^2)$) for sufficiently smooth ($C^2$) functions, not first-order. The failure of Romberg integration in this case is precisely due to smoothness considerations, which this statement dismisses. The premise of the statement is incorrect.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2435348"}]}