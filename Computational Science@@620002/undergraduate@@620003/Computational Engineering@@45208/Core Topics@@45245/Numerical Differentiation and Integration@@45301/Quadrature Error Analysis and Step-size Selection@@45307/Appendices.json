{"hands_on_practices": [{"introduction": "Textbook introductions to quadrature rules often highlight impressive convergence rates, like the $\\mathcal{O}(h^4)$ accuracy of Simpson's rule. However, these guarantees depend crucially on the smoothness of the function being integrated. This exercise challenges you to investigate what happens when this assumption is broken by applying Simpson's rule to a function with limited differentiability. By numerically estimating the convergence order, you will gain a deeper appreciation for the theoretical underpinnings of error analysis and learn why it is critical to match the numerical method to the properties of your problem [@problem_id:2430715].", "problem": "Consider the integral of a function with an interior cusp on a closed interval. Let the interval be $[a,b]=[0,1]$ and define, for a given parameter $c \\in (0,1)$, the function $f:[0,1]\\to\\mathbb{R}$ by\n$$\nf(x) = \\lvert x-c \\rvert^{\\alpha}, \\quad \\text{with } \\alpha=\\tfrac{3}{2}.\n$$\nThe function $f$ belongs to the class $C^1$ but not to the class $C^2$ on $[0,1]$. The exact value of the integral is given by\n$$\nI(c) = \\int_{0}^{1} \\lvert x-c \\rvert^{\\alpha}\\,dx = \\frac{(c-0)^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}.\n$$\nDefine the composite Simpson quadrature on a uniform mesh with $N$ subintervals (where $N$ is an even integer) as follows. Let $h = \\frac{b-a}{N}$ and $x_j = a + jh$ for $j=0,1,\\dots,N$. The composite Simpson approximation $S_N(f)$ to $\\int_a^b f(x)\\,dx$ is\n$$\nS_N(f) = \\frac{h}{3}\\left[f(x_0) + 4\\sum_{j=1,\\,j\\ \\text{odd}}^{N-1} f(x_j) + 2\\sum_{j=2,\\,j\\ \\text{even}}^{N-2} f(x_j) + f(x_N)\\right].\n$$\nFor each parameter value $c$, let the absolute error be\n$$\nE_N(c) = \\left\\lvert I(c) - S_N(f) \\right\\rvert.\n$$\nYou must write a complete, runnable program that, for each of the following test cases for $c$, estimates the observed convergence order $p$ such that $E_N(c) = \\Theta(h^{p})$ as $h \\to 0$, using the provided sequence of $N$ values:\n- Test case $1$: $c = 0.3$,\n- Test case $2$: $c = \\tfrac{3}{8}$,\n- Test case $3$: $c = 10^{-3}$.\n\nFor each test case, evaluate $E_N(c)$ for the sequence of even subinterval counts $N \\in \\{8,16,32,64,128,256,512\\}$, and from these values, determine a single representative estimate of the observed convergence order $p$ for that test case. The final output of your program must be a single line containing a list of three floating-point numbers corresponding to the estimated orders for the three test cases, in the order listed above. Express each floating-point number rounded to three decimal places. The required final output format is a single line with a comma-separated list enclosed in square brackets, for example,\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3].\n$$\nNo physical units are involved in this problem. Angles do not appear. Percentages are not required. All numerical values in the output must be floats.", "solution": "The problem statement is well-posed, mathematically consistent, and scientifically grounded. It presents a standard task in numerical analysis: the experimental determination of the convergence order for a quadrature rule when applied to a function with limited smoothness. We shall proceed with the solution.\n\nThe composite Simpson's rule is a numerical integration method of order four. Its absolute error $E_N$ for an integral on $[a,b]$ typically behaves as $E_N = \\mathcal{O}(h^4)$, where $h = (b-a)/N$ is the step size. This order of convergence is guaranteed only if the integrand $f$ is four times continuously differentiable, i.e., $f \\in C^4([a,b])$.\n\nThe function provided is $f(x) = \\lvert x-c \\rvert^{\\alpha}$ with $\\alpha=\\frac{3}{2}$. Its derivatives are:\n$$ f'(x) = \\frac{3}{2} \\operatorname{sgn}(x-c) \\lvert x-c \\rvert^{1/2} $$\n$$ f''(x) = \\frac{3}{4} \\lvert x-c \\rvert^{-1/2} $$\nThe first derivative $f'(x)$ is continuous on $[0,1]$, thus $f \\in C^1([0,1])$. However, the second derivative $f''(x)$ has a singularity at $x=c$, meaning $f \\notin C^2([0,1])$. The failure of the smoothness condition $f \\in C^4([0,1])$ leads to a degradation of the convergence order.\n\nFor integrands with an algebraic singularity of the form $|x-c|^{\\beta}$, the convergence order $p$ of an $m$-point Newton-Cotes rule, such as Simpson's rule, is given by the theory of quadrature for non-smooth functions. The order depends on whether the singularity at $c$ coincides with a quadrature node:\n1.  If the singularity at $c$ is not a quadrature node for any $N$ in the sequence, the order of convergence is $p = \\alpha+1$. For $\\alpha = \\frac{3}{2}$, this gives $p = \\frac{3}{2}+1 = 2.5$.\n2.  If the singularity at $c$ is a quadrature node for all sufficiently large $N$, the error is improved, and the order of convergence becomes $p = \\alpha+2$. For $\\alpha = \\frac{3}{2}$, this gives $p = \\frac{3}{2}+2 = 3.5$.\n\nWe analyze the test cases based on this theoretical foundation:\n- **Test case 1 ($c = 0.3$)**: The value $c=0.3 = \\frac{3}{10}$ is not a dyadic rational. The mesh points are $x_j = j/N$ where $N$ is a power of $2$ multiplied by $8$. Thus, $c$ will never be a mesh point. We expect a convergence order of $p \\approx 2.5$.\n- **Test case 2 ($c = \\frac{3}{8}$)**: The value $c=0.375=\\frac{3}{8}$ is a dyadic rational. For $N=8, 16, 32, \\dots$, the point $c$ is always a mesh point (e.g., for $N=8$, $x_3 = \\frac{3}{8}$). We expect a convergence order of $p \\approx 3.5$.\n- **Test case 3 ($c = 10^{-3}$)**: The value $c=0.001=\\frac{1}{1000}$ is not a dyadic rational. It will not be a mesh point. We expect a convergence order of $p \\approx 2.5$.\n\nTo estimate the order of convergence $p$ numerically, we assume the error follows the relation $E_N \\approx K h^p$ for some constant $K$, which can be rewritten as $E_N \\approx K' N^{-p}$. Taking the natural logarithm of this expression yields:\n$$ \\ln(E_N) \\approx \\ln(K') - p \\ln(N) $$\nThis equation shows a linear relationship between $\\ln(E_N)$ and $\\ln(N)$, with a slope of $-p$. To find a robust estimate for $p$, we will compute the errors $E_N(c)$ for the given sequence of $N \\in \\{8, 16, 32, 64, 128, 256, 512\\}$. We then perform a linear least-squares regression on the set of points $\\{(\\ln(N_i), \\ln(E_{N_i}))\\}_{i=1}^7$. The slope $m$ of the best-fit line is calculated as:\n$$ m = \\frac{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)}) (\\ln(E_{N_i}) - \\overline{\\ln(E)})}{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)})^2} $$\nThe estimated convergence order is then $p = -m$.\n\nThe following procedure is implemented for each test case:\n1.  Define the constants $\\alpha = \\frac{3}{2}$ and the test case parameter $c$.\n2.  Calculate the exact integral $I(c) = \\frac{c^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}$.\n3.  For each $N$ in the sequence, compute the composite Simpson's rule approximation $S_N(f)$.\n4.  Calculate the absolute error $E_N(c) = |I(c) - S_N(f)|$.\n5.  Using the set of calculated errors and corresponding $N$ values, determine the slope $m$ of the log-log plot via linear regression.\n6.  The convergence order is $p = -m$. The final value is rounded to three decimal places.\nThis entire procedure is encapsulated in the provided program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_convergence_order(c, N_values, alpha):\n    \"\"\"\n    Estimates the convergence order for Simpson's rule for the function\n    f(x) = |x-c|^alpha.\n\n    Args:\n        c (float): The location of the cusp in the function.\n        N_values (list or np.ndarray): A sequence of even integers representing\n                                      the number of subintervals.\n        alpha (float): The exponent in the function definition.\n\n    Returns:\n        float: The estimated order of convergence p.\n    \"\"\"\n    # 1. Define the function and its exact integral\n    f = lambda x: np.abs(x - c)**alpha\n    alpha_p1 = alpha + 1\n    I_exact = (c**alpha_p1 + (1 - c)**alpha_p1) / alpha_p1\n\n    errors = []\n    # 2. Loop through N values to calculate quadrature error\n    for N in N_values:\n        a, b = 0.0, 1.0\n        h = (b - a) / N\n        x = np.linspace(a, b, N + 1)\n        y = f(x)\n\n        # Composite Simpson's rule formula\n        # S_N = (h/3) * [f(x_0) + 4*sum(f(x_odd)) + 2*sum(f(x_even)) + f(x_N)]\n        S_N = (h / 3) * (y[0] + 4 * np.sum(y[1:-1:2]) + 2 * np.sum(y[2:-1:2]) + y[-1])\n        \n        # Absolute error\n        error = np.abs(I_exact - S_N)\n        errors.append(error)\n\n    # 3. Estimate convergence order using linear regression on log-log data\n    # The error model is E_N ≈ K * N^(-p).\n    # Taking logs: log(E_N) ≈ log(K) - p * log(N).\n    # This is a linear relationship between log(E_N) and log(N).\n    # The slope of this line is -p.\n    \n    log_N = np.log(np.array(N_values, dtype=float))\n    log_E = np.log(np.array(errors, dtype=float))\n\n    # Calculate the slope 'm' of the best-fit line for y = mx + b,\n    # where y = log_E and x = log_N.\n    # m = Cov(x, y) / Var(x)\n    x = log_N\n    y = log_E\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n    \n    # The convergence order p is the negative of the slope.\n    p = -slope\n    \n    return p\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    test_cases = [\n        0.3,\n        3.0 / 8.0,\n        1e-3\n    ]\n    \n    # Sequence of subintervals for convergence analysis\n    N_values = [8, 16, 32, 64, 128, 256, 512]\n    \n    # Exponent alpha\n    alpha = 1.5\n\n    results = []\n    for c in test_cases:\n        # Calculate the order p for the current test case\n        p_estimated = estimate_convergence_order(c, N_values, alpha)\n        \n        # Round the result to three decimal places\n        results.append(p_estimated)\n\n    # Format the final output as a string with 3 decimal places for each number\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2430715"}, {"introduction": "Given a fixed computational budget—a set number of function evaluations—how do you achieve the best accuracy? Is it better to use a high-order method on a coarse, uniform grid, or a lower-order method on a non-uniform grid that adapts to the function's features? This practice directly explores this fundamental trade-off by pitting a fixed-step Simpson's rule against an adaptive trapezoidal rule [@problem_id:2430732]. By integrating a function with a sharply localized peak, you will discover the remarkable efficiency of adaptive strategies in resolving complex behavior.", "problem": "You are given a family of integrals over the closed interval $\\left[0,1\\right]$ of the form\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx,\\quad \\text{where}\\quad f_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right).\n$$\nAll angles in the sine function must be interpreted in radians. For all computations in this problem, take $\\omega=20$ and $A=5$.\n\nFor each test case, you must compute two numerical approximations to $I(c,w)$ using at most $N$ evaluations of $f_{c,w}$ and then report their absolute errors with respect to a reference value that is accurate to within absolute error less than $10^{-12}$. The two required approximations are:\n\n1. A fixed-step composite Simpson’s rule on a uniform grid. This rule uses $M=\\left\\lfloor\\frac{N-1}{2}\\right\\rfloor$ composite subintervals and thus exactly $2M+1\\le N$ function evaluations at the nodes $x_i = a + i h$ for $i=0,1,\\dots,2M$, with $a=0$, $b=1$, and $h=\\frac{b-a}{2M}$. The approximation is\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right].\n$$\n\n2. An adaptive trapezoidal rule under a strict function-evaluation budget. Define the adaptive estimator $T_N$ as follows. Start with the single interval $\\left[a,b\\right]=\\left[0,1\\right]$. For any interval $\\left[x_L,x_R\\right]$ for which $f(x_L)$, $f(x_R)$, and $f(x_M)$ are known at the midpoint $x_M=\\frac{x_L+x_R}{2}$, define the coarse trapezoidal approximation\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h = x_R - x_L,\n$$\nand the refined trapezoidal approximation using the bisection $\\left[x_L,x_M\\right]\\cup\\left[x_M,x_R\\right]$,\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L) + 2 f(x_M) + f(x_R)\\right).\n$$\nUse the local error estimate\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}} - T_{\\text{coarse}}\\right|}{3}.\n$$\nInitialize by evaluating $f$ at $x=0$, $x=1$, and $x=\\frac{1}{2}$ (that is, $x_L=0$, $x_R=1$, and $x_M=\\frac{1}{2}$), which uses $3$ evaluations. Maintain the current set of disjoint leaf intervals, each represented by its triplet $\\left(x_L,x_M,x_R\\right)$ with known $f(x_L)$, $f(x_M)$, and $f(x_R)$ and an associated $T_{\\text{refined}}$ and $E$. At each step, if adding two new function evaluations would not exceed the budget $N$, choose the leaf interval with the largest $E$, bisect it into its two halves $\\left[x_L,x_M\\right]$ and $\\left[x_M,x_R\\right]$, evaluate $f$ at each new half-midpoint $\\frac{x_L+x_M}{2}$ and $\\frac{x_M+x_R}{2}$ (this increases the count of distinct evaluations by $2$), form their $T_{\\text{refined}}$ and $E$, and replace the parent interval by these two children in the leaf set. Stop when the next bisection would require more than $N$ total distinct evaluations of $f$. Define the final adaptive estimate $T_N$ as the sum of $T_{\\text{refined}}$ over all current leaf intervals. In counting the budget, only distinct evaluation points $x$ are counted; shared endpoints are not reevaluated.\n\nFor each test case below, compute a high-accuracy reference value $I^\\star(c,w)$ for $I(c,w)$ that is accurate to within absolute error less than $10^{-12}$, and then compute the absolute errors\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|, \\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|.\n$$\n\nTest suite (four cases):\n- Case A: $N=33$, $c=0.5$, $w=0.02$.\n- Case B: $N=33$, $c=0.9$, $w=0.01$.\n- Case C: $N=65$, $c=0.3$, $w=0.005$.\n- Case D: $N=129$, $c=0.5$, $w=0.05$.\n\nYour program must output a single line containing the results as a comma-separated list of lists in the order $\\left[\\text{Case A},\\text{Case B},\\text{Case C},\\text{Case D}\\right]$, where each inner list is of the form $\\left[E_S,E_T\\right]$. Each floating-point number must be formatted in scientific notation with exactly $10$ digits after the decimal point (for example, $1.2345678900e-03$). That is, the output must have the form\n$$\n\\left[ [E_{S,A},E_{T,A}], [E_{S,B},E_{T,B}], [E_{S,C},E_{T,C}], [E_{S,D},E_{T,D}] \\right],\n$$\nprinted as a single line with no spaces after commas.", "solution": "We formalize the two quadrature methods and the evaluation-budget-constrained adaptivity, then derive how to implement them from first principles of numerical integration and error analysis.\n\nThe integrand is\n$$\nf_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right),\n$$\nwith $\\omega=20$ and $A=5$, on $x\\in[0,1]$. The sine function uses radians. The integral is\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx.\n$$\n\nFixed-step composite Simpson’s rule. For a uniform grid on $[a,b]=[0,1]$ with $2M$ subintervals (so $2M+1$ nodes), the composite Simpson rule is, by definition of Newton–Cotes formulas,\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right],\n$$\nwhere $h=\\frac{b-a}{2M}$ and $x_i=a+ih$, $i=0,1,\\dots,2M$. The budget constraint is that we can use at most $N$ distinct function evaluations. The composite Simpson rule requires exactly $2M+1$ evaluations. Therefore we must choose\n$$\nM \\;=\\; \\left\\lfloor \\frac{N-1}{2} \\right\\rfloor,\n$$\nwhich guarantees $2M+1\\le N$ evaluations. The classical truncation error for Simpson’s rule for sufficiently smooth $f$ is $\\mathcal{O}(h^4)$, but here we do not assume a priori constants; we compute the approximation and measure the actual absolute error with respect to a high-accuracy reference.\n\nAdaptive trapezoidal rule under a strict budget. The trapezoidal rule on an interval $\\left[x_L,x_R\\right]$ has the approximation\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h=x_R-x_L.\n$$\nIf we bisect at the midpoint $x_M=\\frac{x_L+x_R}{2}$ and apply trapezoidal rule on the two halves, we obtain\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L)+2f(x_M)+f(x_R)\\right).\n$$\nFor sufficiently smooth $f$, the trapezoidal rule error on a single interval scales as $\\mathcal{O}(h^2)$. Under the assumption of an asymptotic error expansion $E(h)\\approx K h^2$ for some local constant $K$, Richardson extrapolation implies\n$$\nE(h) \\;\\approx\\; \\frac{T_{\\text{refined}} - T_{\\text{coarse}}}{3},\n$$\nso a local error estimate is\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}}-T_{\\text{coarse}}\\right|}{3}.\n$$\nAn adaptive refinement strategy aims to allocate smaller step sizes where this local error estimate is largest. We begin with the single interval $\\left[0,1\\right]$ and evaluate $f$ at $x=0$, $x=\\frac{1}{2}$, and $x=1$, consuming $3$ evaluations. For the current set of disjoint leaf intervals, each leaf is represented by its endpoints and midpoint $\\left(x_L,x_M,x_R\\right)$ with known $f(x_L)$, $f(x_M)$, $f(x_R)$, and associated $T_{\\text{refined}}$ and $E$. At each refinement step, if adding two new distinct function evaluations would keep the total at most $N$, we select the leaf interval with the largest $E$ and bisect it into its two halves. This creates two child intervals:\n- Left child $\\left[x_L,x_M\\right]$ with new midpoint $x_{LM}=\\frac{x_L+x_M}{2}$,\n- Right child $\\left[x_M,x_R\\right]$ with new midpoint $x_{MR}=\\frac{x_M+x_R}{2}$.\nWe evaluate $f$ at these two new midpoints (adding $2$ to the count of distinct samples), compute each child’s $T_{\\text{refined}}$ and $E$, and update the set of leaves. The current adaptive integral estimate is the sum of $T_{\\text{refined}}$ across all leaves. This greedy refinement continues until the next split would require more than $N$ total distinct evaluations. Because we begin with $2$ endpoints and $1$ midpoint (total $3$) and each bisection adds $2$ new midpoints, the number of distinct evaluations is $3+2k$ after $k$ bisections, which is always $\\le N$ by construction. This procedure concentrates subintervals where the local error is largest, which, for this problem, corresponds to regions with large local curvature such as the localized Gaussian feature near $x=c$ or regions of rapid oscillation.\n\nReference integral and absolute errors. We compute a high-accuracy reference value $I^\\star(c,w)$ for $I(c,w)$ to within absolute error less than $10^{-12}$ using a robust high-order quadrature routine so that the reported absolute errors\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|,\\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|\n$$\nare dominated by the discretization effects of the two methods being compared and not by the reference value.\n\nTest suite and expected outcomes. The four cases exercise different regimes:\n- Case A: $N=33$, $c=0.5$, $w=0.02$ (localized feature centered, moderate narrowness).\n- Case B: $N=33$, $c=0.9$, $w=0.01$ (localized feature near the boundary).\n- Case C: $N=65$, $c=0.3$, $w=0.005$ (very narrow feature, moderate budget).\n- Case D: $N=129$, $c=0.5$, $w=0.05$ (broader feature, larger budget).\n\nWe anticipate, from first principles, that fixed-step composite Simpson’s rule, being fourth order in the uniform step size $h$, will perform very well when the feature is not too narrow relative to the grid spacing and the function remains sufficiently smooth across cells. However, when the feature is sharply localized (small $w$) and especially when the budget $N$ is modest, the uniform mesh may underresolve the feature, leading to larger errors. The adaptive trapezoidal rule is only second order locally, but because the error estimator drives local refinement to the most challenging regions, it can outperform the fixed-step Simpson’s rule under the same evaluation budget by focusing evaluations within and around the localized Gaussian peak (and possibly near boundaries where asymmetry occurs). The results reported by the program will quantify these effects via the computed $E_S$ and $E_T$ for each test case. The final output format is a single line: a comma-separated list of four inner lists, each inner list being $\\left[E_S,E_T\\right]$ with each number formatted in scientific notation with exactly $10$ digits after the decimal point.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sin, exp\nfrom scipy.integrate import quad\n\ndef f_cw(x, c, w, omega=20.0, A=5.0):\n    # Sine uses radians; exponential is the localized feature.\n    return sin(omega * x) + A * exp(-((x - c) ** 2) / (2.0 * w ** 2))\n\ndef reference_integral(c, w):\n    # High-accuracy reference integral with stringent tolerances.\n    val, err = quad(lambda x: f_cw(x, c, w), 0.0, 1.0, epsabs=1e-13, epsrel=1e-13, limit=500)\n    return val\n\ndef composite_simpson_budget(c, w, N, a=0.0, b=1.0):\n    # Use M = floor((N-1)/2) composite Simpson subintervals (2M+1 nodes <= N).\n    M = int((N - 1) // 2)\n    if M < 1:\n        # Fallback: with too few points, use simple trapezoid on [a,b].\n        fa = f_cw(a, c, w)\n        fb = f_cw(b, c, w)\n        return 0.5 * (b - a) * (fa + fb)\n    h = (b - a) / (2 * M)\n    xs = a + h * np.arange(0, 2 * M + 1, dtype=float)\n    fs = np.array([f_cw(x, c, w) for x in xs])\n    s0 = fs[0] + fs[-1]\n    sodds = fs[1:-1:2].sum()  # odd indices: 1,3,...,2M-1\n    sevens = fs[2:-1:2].sum() if (2 * M - 1) >= 2 else 0.0  # even indices: 2,4,...,2M-2\n    return (h / 3.0) * (s0 + 4.0 * sodds + 2.0 * sevens)\n\nclass Interval:\n    __slots__ = (\"xL\",\"xM\",\"xR\",\"fL\",\"fM\",\"fR\",\"Tref\",\"E\",\"id\")\n    def __init__(self, xL, xM, xR, fL, fM, fR, Tref, E, idnum):\n        self.xL = xL; self.xM = xM; self.xR = xR\n        self.fL = fL; self.fM = fM; self.fR = fR\n        self.Tref = Tref; self.E = E; self.id = idnum\n\ndef adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0):\n    # Greedy adaptive trapezoidal rule under a strict evaluation budget of at most N distinct samples.\n    # Implementation matches the precise specification in the problem statement.\n    import heapq\n\n    # Map of distinct points to function values to avoid double counting.\n    values = {}\n\n    def eval_point(x):\n        # Evaluate f(x) if not already. Count distinct points only.\n        if x in values:\n            return values[x]\n        if len(values) >= N:\n            # Budget exhausted; should not happen if caller checks before adding new points.\n            return values.get(x, None)\n        fx = f_cw(x, c, w)\n        values[x] = fx\n        return fx\n\n    # Initialize with endpoints and midpoint\n    xL = a; xR = b; xM = 0.5 * (xL + xR)\n    fL = eval_point(xL); fR = eval_point(xR); fM = eval_point(xM)\n\n    # Compute initial refined trapezoid and error estimate\n    h = xR - xL\n    Tcoarse = 0.5 * h * (fL + fR)\n    Tref = 0.25 * h * (fL + 2.0 * fM + fR)\n    E = abs(Tref - Tcoarse) / 3.0\n\n    # Priority queue of intervals by negative error (max-heap behavior)\n    heap = []\n    counter = 0  # to break ties\n    first = Interval(xL, xM, xR, fL, fM, fR, Tref, E, counter)\n    heapq.heappush(heap, (-E, counter, first))\n    counter += 1\n\n    # Sum of refined trapezoid contributions across leaf intervals\n    integral_sum = Tref\n\n    # Current number of distinct evaluations is len(values)\n    # Each split requires adding exactly 2 new midpoints\n    while len(values) + 2 <= N and heap:\n        # Pop the interval with largest error\n        _, _, itv = heapq.heappop(heap)\n\n        # Prepare two children by bisecting\n        xL = itv.xL; xM = itv.xM; xR = itv.xR\n        fL = itv.fL; fM = itv.fM; fR = itv.fR\n\n        # Left child [xL, xM]\n        xLM = 0.5 * (xL + xM)\n        fLM = eval_point(xLM)\n        hL = xM - xL\n        Tcoarse_L = 0.5 * hL * (fL + fM)\n        Tref_L = 0.25 * hL * (fL + 2.0 * fLM + fM)\n        E_L = abs(Tref_L - Tcoarse_L) / 3.0\n\n        # Right child [xM, xR]\n        xMR = 0.5 * (xM + xR)\n        fMR = eval_point(xMR)\n        hR = xR - xM\n        Tcoarse_R = 0.5 * hR * (fM + fR)\n        Tref_R = 0.25 * hR * (fM + 2.0 * fMR + fR)\n        E_R = abs(Tref_R - Tcoarse_R) / 3.0\n\n        # Update integral sum: remove parent refined, add children refined\n        integral_sum += -itv.Tref + (Tref_L + Tref_R)\n\n        # Push children\n        left = Interval(xL, xLM, xM, fL, fLM, fM, Tref_L, E_L, counter); counter += 1\n        right = Interval(xM, xMR, xR, fM, fMR, fR, Tref_R, E_R, counter); counter += 1\n        heapq.heappush(heap, (-E_L, left.id, left))\n        heapq.heappush(heap, (-E_R, right.id, right))\n\n    return integral_sum\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (N, c, w)\n    test_cases = [\n        (33, 0.5, 0.02),   # Case A\n        (33, 0.9, 0.01),   # Case B\n        (65, 0.3, 0.005),  # Case C\n        (129, 0.5, 0.05),  # Case D\n    ]\n\n    results = []\n    for N, c, w in test_cases:\n        I_star = reference_integral(c, w)\n        S_N = composite_simpson_budget(c, w, N, a=0.0, b=1.0)\n        T_N = adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0)\n        err_S = abs(S_N - I_star)\n        err_T = abs(T_N - I_star)\n        results.append((err_S, err_T))\n\n    # Format: a single line with a comma-separated list of lists, each inner list [Es,Ea],\n    # with each float in scientific notation with exactly 10 digits after the decimal point.\n    formatted = \"[\" + \",\".join(f\"[{err_s:.10e},{err_t:.10e}]\" for (err_s, err_t) in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "2430732"}, {"introduction": "This capstone exercise moves from analyzing existing methods to constructing a modern, robust adaptive quadrature algorithm from first principles. Professional quadrature routines often use pairs of high-order rules, like Gauss-Legendre, to create a more reliable error estimate than one based on a single rule. This practice requires you to implement a complete adaptive routine using such an error estimator, along with a recursive logic for managing the error tolerance [@problem_id:2430740]. Tackling this challenge will solidify your understanding and equip you with a powerful tool applicable to a wide range of scientific computing tasks.", "problem": "Write a complete, runnable program that computes numerical approximations of definite integrals using an adaptive procedure based on rigorous local error control. Let a one-dimensional integral be denoted by $\\int_a^b f(x)\\,dx$ for a real-valued function $f$ on a closed interval $[a,b]$. The absolute error requirement is that the returned approximation $I$ satisfies $|I - \\int_a^b f(x)\\,dx| \\le \\tau$, where $\\tau > 0$ is a user-specified absolute tolerance.\n\nYour program must implement a routine that, given $f$, $a$, $b$, and $\\tau$, adaptively selects subintervals so that the absolute error requirement is met. The local error estimator for any subinterval $[u,v]$ must be the absolute difference between two Gaussian quadrature approximations on $[u,v]$, one using a $4$-point Gauss–Legendre rule and the other using a $5$-point Gauss–Legendre rule. Here, by an $n$-point Gauss–Legendre rule on $[u,v]$ we mean the unique quadrature obtained by an affine mapping of the standard $n$-point Gauss–Legendre rule on $[-1,1]$ to $[u,v]$. To ensure a global absolute error bound $\\le \\tau$, your routine must manage a subinterval-wise tolerance allocation so that when a subinterval $[u,v]$ does not meet its local tolerance (as diagnosed by the local error estimator), that subinterval is bisected and the two child subintervals each receive one-half of the parent tolerance budget. This error-budget halving must continue recursively until all leaf subintervals meet their local tolerance. If $a=b$, the value must be exactly $0$.\n\nAngles used inside trigonometric functions must be in radians.\n\nTest Suite:\nCompute approximations for the following cases, each with its own absolute tolerance $\\tau$:\n\n- Case $\\mathbf{1}$ (smooth, rapidly varying curvature): $f(x)=e^{-x^2}$ on $[0,1]$ with $\\tau = 10^{-9}$.\n- Case $\\mathbf{2}$ (rational decay over a long interval): $f(x)=\\dfrac{1}{1+x^2}$ on $[0,10]$ with $\\tau = 10^{-9}$.\n- Case $\\mathbf{3}$ (endpoint singular derivative but integrable): $f(x)=\\sqrt{x}$ on $[0,1]$ with $\\tau = 10^{-8}$.\n- Case $\\mathbf{4}$ (degenerate interval): $f(x)=\\sin(x)$ on $[1.234,1.234]$ with $\\tau = 10^{-12}$.\n- Case $\\mathbf{5}$ (high-degree polynomial): $f(x)=x^8$ on $[0,1]$ with $\\tau = 10^{-12}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $\\mathbf{1}$ through $\\mathbf{5}$. Each number must be printed in decimal notation with exactly $12$ digits after the decimal point. For example: $[r_1,r_2,r_3,r_4,r_5]$ where each $r_k$ is a floating-point number formatted to $12$ digits after the decimal point and no additional whitespace is included.", "solution": "The problem posed is to construct a numerical algorithm for approximating the definite integral $I = \\int_a^b f(x)\\,dx$ to within a prescribed absolute error tolerance $\\tau$. The method specified is an adaptive quadrature procedure, which is a standard and powerful technique in computational science. The validity of the problem statement is confirmed, as it is scientifically grounded, well-posed, and objective. It describes a classic recursive algorithm for numerical integration with error control. We shall proceed with a systematic derivation and implementation of the required algorithm.\n\nThe solution is founded on three core principles: a high-order base quadrature rule, a reliable local error estimator, and an adaptive strategy for interval subdivision and tolerance allocation.\n\nFirst, we establish the base numerical integration method. The problem specifies the use of Gauss-Legendre quadrature. For a given interval $[-1, 1]$, an $n$-point Gauss-Legendre rule approximates an integral as a weighted sum $\\int_{-1}^1 g(t) dt \\approx \\sum_{i=1}^n w_i g(t_i)$, where the nodes $\\{t_i\\}$ are the roots of the $n$-th degree Legendre polynomial and $\\{w_i\\}$ are the corresponding weights. This rule is optimal, being exact for all polynomials of degree up to $2n-1$. To apply this rule to a general interval $[u,v]$, we use the affine transformation $x(t) = \\frac{v-u}{2} t + \\frac{u+v}{2}$. The differential transforms as $dx = \\frac{v-u}{2} dt$. The integral over $[u,v]$ is then approximated as:\n$$\n\\int_u^v f(x)\\,dx = \\int_{-1}^1 f\\left(\\frac{v-u}{2} t + \\frac{u+v}{2}\\right) \\frac{v-u}{2} dt \\approx \\frac{v-u}{2} \\sum_{i=1}^n w_i f\\left(\\frac{v-u}{2} t_i + \\frac{u+v}{2}\\right)\n$$\nWe will denote the result of this $n$-point rule on interval $[u,v]$ as $I_n(f; [u,v])$. The problem requires us to use two such rules: a $4$-point rule, giving the approximation $I_4$, and a $5$-point rule, giving $I_5$.\n\nSecond, we formulate the local error estimator. The adaptive nature of the algorithm depends on its ability to assess the accuracy of the approximation on any given subinterval $[u,v]$. Since the $5$-point rule is exact for polynomials of degree up to $9$ and the $4$-point rule is exact up to degree $7$, $I_5$ is a significantly more accurate approximation of the true integral on $[u,v]$ than $I_4$. We can express the true integral $I_{\\text{true}}$ as $I_{\\text{true}} = I_5 + \\epsilon_5$ and $I_{\\text{true}} = I_4 + \\epsilon_4$, where $\\epsilon_5$ and $\\epsilon_4$ are the respective errors. Assuming the interval is small enough that the asymptotic behavior of the error holds, we have $|\\epsilon_5| \\ll |\\epsilon_4|$. It follows that the error of the less accurate rule, $\\epsilon_4$, can be estimated by the difference between the two approximations: $\\epsilon_4 = I_{\\text{true}} - I_4 \\approx I_5 - I_4$. Therefore, the local error estimator, $\\mathcal{E}$, is defined as the absolute difference:\n$$\n\\mathcal{E} = |I_5(f; [u,v]) - I_4(f; [u,v])|\n$$\nThis quantity provides a computable measure of how much the lower-order approximation is likely in error.\n\nThird, we design the adaptive refinement strategy. The global goal is to ensure the total error across $[a,b]$ is less than or equal to the global tolerance $\\tau$. The strategy specified is a recursive bisection with tolerance propagation. The algorithm begins with the entire interval $[a,b]$ and the full tolerance budget $\\tau$. For any sub-problem consisting of an interval $[u,v]$ and an associated tolerance $\\tau_{sub}$, the following procedure is applied:\n1.  Compute the two approximations $I_4(f; [u,v])$ and $I_5(f; [u,v])$.\n2.  Calculate the error estimate $\\mathcal{E} = |I_5 - I_4|$.\n3.  Compare the estimate to the tolerance:\n    *   If $\\mathcal{E} \\le \\tau_{sub}$, the interval is accepted. The approximation on this interval is considered sufficiently accurate, and its contribution to the total integral is taken to be the more precise value, $I_5(f; [u,v])$.\n    *   If $\\mathcal{E} > \\tau_{sub}$, the interval is rejected. The local accuracy is insufficient. The interval is bisected at its midpoint $m = (u+v)/2$. The procedure is then invoked on the two resulting subintervals, $[u,m]$ and $[m,v]$. Crucially, the tolerance budget is halved for each child, so each new sub-problem is assigned a tolerance of $\\tau_{sub}/2$.\n\nThe total integral approximation is the sum of the $I_5$ values from all subintervals that are eventually accepted. This recursive process guarantees that more computational effort, in the form of smaller subdivisions, is concentrated in regions where the function $f(x)$ is difficult to integrate (e.g., regions of high curvature or near singularities), while fewer subdivisions are used where the function is smooth and well-behaved.\n\nFor implementation, a stack-based approach is superior to direct recursion to avoid exceeding the system's recursion depth limit. The initial problem $(a, b, \\tau)$ is pushed onto a work stack. The algorithm then enters a loop, popping a problem from the stack, performing the error check, and either adding the result to the total or pushing two new sub-problems onto the stack. The process terminates when the stack is empty. A trivial case for $a=b$ must be handled, for which the integral is exactly $0$. This completes the logical design of the required program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_legendre\n\ndef solve():\n    \"\"\"\n    Computes numerical approximations of definite integrals using an adaptive\n    Gauss-Legendre quadrature method, following the problem specification.\n    \"\"\"\n    \n    # Pre-compute the standard Gauss-Legendre nodes and weights for orders 4 and 5.\n    # These are defined on the canonical interval [-1, 1].\n    try:\n        nodes_4, weights_4 = roots_legendre(4)\n        nodes_5, weights_5 = roots_legendre(5)\n    except ImportError:\n        # Fallback for environments where scipy might be installed incorrectly\n        # or for older scipy versions if roots_legendre is not available.\n        # These values are standard mathematical constants.\n        nodes_4 = np.array([-0.8611363115940526, -0.3399810435848563, 0.3399810435848563, 0.8611363115940526])\n        weights_4 = np.array([0.3478548451374538, 0.6521451548625461, 0.6521451548625461, 0.3478548451374538])\n        nodes_5 = np.array([-0.9061798459386640, -0.5384693101056831, 0.0, 0.5384693101056831, 0.9061798459386640])\n        weights_5 = np.array([0.2369268850561891, 0.4786286704993665, 0.5688888888888889, 0.4786286704993665, 0.2369268850561891])\n\n    def gauss_quadrature(f, u, v, nodes, weights):\n        \"\"\"\n        Applies an n-point Gauss-Legendre quadrature rule on the interval [u, v].\n\n        Args:\n            f: The function to integrate.\n            u, v: The interval of integration.\n            nodes, weights: The standard Gauss-Legendre nodes and weights on [-1, 1].\n\n        Returns:\n            The approximate value of the integral.\n        \"\"\"\n        # Affine transformation from [-1, 1] to [u, v]\n        # x = h * t + c, where t is in [-1, 1]\n        h = (v - u) / 2.0\n        c = (u + v) / 2.0\n        \n        # Transformed nodes\n        x_nodes = h * nodes + c\n        \n        # Evaluate function at transformed nodes\n        f_vals = f(x_nodes)\n        \n        # Compute the integral approximation\n        # The integral transformation includes a factor of h = (v-u)/2\n        integral = h * np.sum(weights * f_vals)\n        return integral\n\n    def adaptive_integral(f, a, b, tau):\n        \"\"\"\n        Performs adaptive quadrature to approximate the integral of f from a to b\n        with an absolute error tolerance tau.\n\n        Args:\n            f: The function to integrate.\n            a, b: The interval of integration.\n            tau: The absolute error tolerance.\n\n        Returns:\n            The integral approximation.\n        \"\"\"\n        if a == b:\n            return 0.0\n\n        # Use a stack for a non-recursive implementation to avoid recursion depth limits.\n        # Each element is a tuple (u, v, tolerance_for_subinterval).\n        work_stack = [(a, b, tau)]\n        total_integral = 0.0\n\n        while work_stack:\n            u, v, current_tol = work_stack.pop()\n            \n            # Compute approximations with 4-point and 5-point rules\n            i4 = gauss_quadrature(f, u, v, nodes_4, weights_4)\n            i5 = gauss_quadrature(f, u, v, nodes_5, weights_5)\n            \n            # Estimate the error as the absolute difference between the two approximations\n            error_estimate = abs(i5 - i4)\n            \n            # If the error is within the local tolerance, accept the subinterval\n            if error_estimate <= current_tol:\n                # Add the more accurate (5-point) approximation to the total\n                total_integral += i5\n            else:\n                # If tolerance is not met, bisect the interval\n                m = (u + v) / 2.0\n                # Split the tolerance budget between the two new subintervals\n                half_tol = current_tol / 2.0\n                \n                # Push the new sub-problems onto the stack.\n                # The order is chosen to mimic a depth-first traversal of the recursion tree.\n                work_stack.append((m, v, half_tol))\n                work_stack.append((u, m, half_tol))\n                \n        return total_integral\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'func': lambda x: np.exp(-x**2), 'a': 0.0, 'b': 1.0, 'tau': 1e-9},\n        {'func': lambda x: 1.0 / (1.0 + x**2), 'a': 0.0, 'b': 10.0, 'tau': 1e-9},\n        {'func': lambda x: np.sqrt(x), 'a': 0.0, 'b': 1.0, 'tau': 1e-8},\n        {'func': lambda x: np.sin(x), 'a': 1.234, 'b': 1.234, 'tau': 1e-12},\n        {'func': lambda x: x**8, 'a': 0.0, 'b': 1.0, 'tau': 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = adaptive_integral(case['func'], case['a'], case['b'], case['tau'])\n        results.append(f\"{result:.12f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2430740"}]}