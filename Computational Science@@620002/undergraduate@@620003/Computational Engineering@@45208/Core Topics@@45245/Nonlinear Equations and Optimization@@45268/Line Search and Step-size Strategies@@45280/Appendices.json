{"hands_on_practices": [{"introduction": "This first practice exercise focuses on the core mechanism of a line search: the backtracking algorithm. You will implement a procedure to find a step size that satisfies the Armijo sufficient decrease condition for the famous Rosenbrock function, a standard benchmark known for its challenging narrow, curved valley. This hands-on task [@problem_id:2409367] will solidify your understanding of how to prevent overshooting the minimum and ensure meaningful progress in an optimization routine.", "problem": "You are given the two-variable Rosenbrock function with an adjustable curvature parameter, defined for any $(x,y) \\in \\mathbb{R}^2$ by\n$$\nf(x,y) = (1 - x)^2 + \\kappa \\,(y - x^2)^2,\n$$\nwhere $\\kappa > 0$ controls the narrowness of the curved valley. For large $\\kappa$, the valley becomes very narrow around the curve $y = x^2$. Consider selecting a step length $\\alpha$ along the steepest descent direction $p = -\\nabla f(x,y)$ at a current point $x = (x,y)$ so that the following sufficient decrease condition holds:\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\,\\alpha \\,\\nabla f(x)^{\\mathsf{T}} p,\n$$\nwith fixed $c_1 \\in (0,1)$. If $\\nabla f(x)^{\\mathsf{T}} p \\ge 0$, define $\\alpha = 0$. Otherwise, among the values $\\alpha \\in \\{\\alpha_0, \\beta \\alpha_0, \\beta^2 \\alpha_0, \\dots\\}$ with fixed $\\alpha_0 > 0$ and $\\beta \\in (0,1)$, select the first $\\alpha$ that satisfies the sufficient decrease condition. If no such $\\alpha$ is found within at most $N_{\\max}$ reductions, define $\\alpha = 0$.\n\nImplement a program that, for each test case, computes the selected $\\alpha$ according to the rule above, using $p = -\\nabla f(x,y)$. You must compute any required derivatives from the definition of $f(x,y)$.\n\nUse the constants $c_1 = 10^{-4}$, $\\beta = \\tfrac{1}{2}$, and $N_{\\max} = 50$ in all cases. All computations are unitless real numbers.\n\nTest suite:\n- Case A (general narrow-valley case): $\\kappa = 100$, $(x,y) = (-1.2, 1.0)$, $\\alpha_0 = 1$.\n- Case B (very narrow valley): $\\kappa = 1000$, $(x,y) = (-1.2, 1.0)$, $\\alpha_0 = 1$.\n- Case C (boundary, stationary point): $\\kappa = 100$, $(x,y) = (1.0, 1.0)$, $\\alpha_0 = 1$.\n- Case D (initial step already sufficiently small): $\\kappa = 100$, $(x,y) = (-1.2, 1.0)$, $\\alpha_0 = 10^{-4}$.\n\nFinal output format:\nYour program must produce a single line of output containing a list of the four selected step lengths, in the same order as the test cases, rounded to six digits after the decimal point. The list must be printed as a comma-separated sequence enclosed in square brackets, for example, \"[a,b,c,d]\". Each entry must be a real number rounded to six decimal places.", "solution": "The problem statement has been subjected to rigorous validation and is found to be valid. It is scientifically grounded in the principles of numerical optimization, is well-posed with all necessary information provided, and is stated in objective, unambiguous terms. There are no logical contradictions, factual inaccuracies, or other flaws that would preclude a formal solution. We may therefore proceed with the analysis and implementation.\n\nThe objective is to compute a step length $\\alpha$ for an optimization algorithm using the backtracking line search method, specifically to satisfy the Armijo sufficient decrease condition. The function under consideration is the two-variable Rosenbrock function, a standard benchmark for unconstrained optimization algorithms.\n\nThe Rosenbrock function is given by:\n$$\nf(x,y) = (1 - x)^2 + \\kappa (y - x^2)^2\n$$\nwhere $\\mathbf{x} = (x,y)^{\\mathsf{T}} \\in \\mathbb{R}^2$ and $\\kappa > 0$ is a positive constant that controls the curvature of the function's valley.\n\nThe first step in any gradient-based method is to compute the gradient of the objective function, $\\nabla f(\\mathbf{x})$. The partial derivatives with respect to $x$ and $y$ are:\n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (1 - x)^2 + \\kappa (y - x^2)^2 \\right] = 2(1 - x)(-1) + \\kappa \\cdot 2(y - x^2)(-2x) = -2(1 - x) - 4\\kappa x(y - x^2)\n$$\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (1 - x)^2 + \\kappa (y - x^2)^2 \\right] = \\kappa \\cdot 2(y - x^2)(1) = 2\\kappa(y - x^2)\n$$\nThus, the gradient vector is:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} -2(1 - x) - 4\\kappa x(y - x^2) \\\\ 2\\kappa(y - x^2) \\end{pmatrix}\n$$\n\nThe problem specifies the use of the steepest descent direction, which is defined as the negative of the gradient:\n$$\n\\mathbf{p} = -\\nabla f(x,y)\n$$\nThis direction guarantees a local decrease in the function value, provided the gradient is non-zero.\n\nThe core of the problem is the Armijo sufficient decrease condition, which ensures that the step length $\\alpha$ provides a meaningful reduction in the function value. The condition is:\n$$\nf(\\mathbf{x} + \\alpha \\mathbf{p}) \\le f(\\mathbf{x}) + c_1 \\alpha \\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p}\n$$\nwhere $c_1 \\in (0,1)$ is a constant, here given as $c_1 = 10^{-4}$. The term $\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p}$ is the directional derivative of $f$ along $\\mathbf{p}$. Substituting $\\mathbf{p} = -\\nabla f(\\mathbf{x})$ gives:\n$$\n\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p} = \\nabla f(\\mathbf{x})^{\\mathsf{T}}(-\\nabla f(\\mathbf{x})) = -\\|\\nabla f(\\mathbf{x})\\|_2^2\n$$\nThis quantity is always non-positive. It is equal to zero only if $\\nabla f(\\mathbf{x}) = \\mathbf{0}$, which corresponds to a stationary point. The problem correctly specifies that if $\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p} \\ge 0$, then $\\alpha$ must be set to $0$. This handles the stationary point case, where no further progress in the direction of steepest descent is possible.\n\nThe algorithmic procedure for finding $\\alpha$ is a backtracking search:\n1.  For a given point $\\mathbf{x}=(x,y)$, and parameters $\\kappa$, $\\alpha_0$, $c_1$, $\\beta$, $N_{\\max}$:\n2.  Compute the gradient $\\mathbf{g} = \\nabla f(\\mathbf{x})$.\n3.  Compute the search direction $\\mathbf{p} = -\\mathbf{g}$.\n4.  Compute the directional derivative term $d = \\mathbf{g}^{\\mathsf{T}}\\mathbf{p} = -\\|\\mathbf{g}\\|_2^2$.\n5.  If $d \\ge 0$, set the final step length to $0$ and terminate.\n6.  Initialize the trial step length $\\alpha = \\alpha_0$.\n7.  Iterate for a maximum of $N_{\\max}$ reductions. For $j$ from $0$ to $N_{\\max}$:\n    a. Calculate the candidate point $\\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\alpha \\mathbf{p}$.\n    b. Evaluate the function at the current point, $f(\\mathbf{x})$, and at the candidate point, $f(\\mathbf{x}_{\\text{new}})$.\n    c. Check if the Armijo condition is satisfied: $f(\\mathbf{x}_{\\text{new}}) \\le f(\\mathbf{x}) + c_1 \\alpha d$.\n    d. If the condition is met, this $\\alpha$ is the desired step length. Terminate the search and return this value.\n    e. If the condition is not met, reduce the step length: $\\alpha \\leftarrow \\beta \\alpha$.\n8.  If the loop completes without satisfying the condition after testing $\\alpha_0, \\beta\\alpha_0, \\dots, \\beta^{N_{\\max}}\\alpha_0$, the search has failed. As per the problem statement, the step length is defined as $0$.\n\nThe implementation will apply this precise logic to each of the specified test cases using the provided constants $c_1 = 10^{-4}$, $\\beta = \\frac{1}{2}$, and $N_{\\max} = 50$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the step length using backtracking line search\n    for the Rosenbrock function, according to the specified problem statement.\n    \"\"\"\n\n    # Global constants as per the problem\n    C1 = 1e-4\n    BETA = 0.5\n    N_MAX = 50\n\n    def rosenbrock_f(x_vec, kappa):\n        \"\"\"\n        Computes the Rosenbrock function value.\n        f(x,y) = (1 - x)^2 + kappa * (y - x^2)^2\n        \"\"\"\n        x, y = x_vec[0], x_vec[1]\n        return (1 - x)**2 + kappa * (y - x**2)**2\n\n    def rosenbrock_grad(x_vec, kappa):\n        \"\"\"\n        Computes the gradient of the Rosenbrock function.\n        \"\"\"\n        x, y = x_vec[0], x_vec[1]\n        df_dx = -2 * (1 - x) - 4 * kappa * x * (y - x**2)\n        df_dy = 2 * kappa * (y - x**2)\n        return np.array([df_dx, df_dy])\n\n    def find_step_length(kappa, x_start, alpha_0):\n        \"\"\"\n        Implements the backtracking line search to find a suitable step length alpha.\n        \"\"\"\n        x_vec = np.array(x_start, dtype=float)\n        \n        grad_f = rosenbrock_grad(x_vec, kappa)\n        p = -grad_f\n        \n        # Directional derivative term\n        grad_f_dot_p = np.dot(grad_f, p)\n\n        # If direction is not a descent direction (or at a stationary point)\n        if grad_f_dot_p >= 0:\n            return 0.0\n\n        alpha = float(alpha_0)\n        f_x = rosenbrock_f(x_vec, kappa)\n\n        # Backtracking loop: test alpha_0, beta*alpha_0, ..., beta^N_max * alpha_0\n        for _ in range(N_MAX + 1):\n            x_new = x_vec + alpha * p\n            f_x_new = rosenbrock_f(x_new, kappa)\n            \n            # Armijo sufficient decrease condition\n            if f_x_new = f_x + C1 * alpha * grad_f_dot_p:\n                return alpha\n            \n            # Reduce alpha for the next iteration\n            alpha *= BETA\n        \n        # If no suitable alpha is found within N_MAX reductions\n        return 0.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (kappa, (x, y), alpha_0)\n        (100, (-1.2, 1.0), 1.0),       # Case A\n        (1000, (-1.2, 1.0), 1.0),      # Case B\n        (100, (1.0, 1.0), 1.0),        # Case C\n        (100, (-1.2, 1.0), 1e-4),      # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        kappa, x_start, alpha_0 = case\n        result = find_step_length(kappa, x_start, alpha_0)\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2409367"}, {"introduction": "An algorithm that works in theory can fail in practice due to the limitations of floating-point arithmetic. This exercise [@problem_id:2409357] dives into these numerical challenges, tasking you with building a robust line search that can handle situations where steps become too small to register a change in the variable's value. By addressing issues like arithmetic stagnation on very flat or large-magnitude landscapes, you will learn to write code that is not only correct but also resilient.", "problem": "You are given two differentiable scalar objective functions $f:\\mathbb{R}\\to\\mathbb{R}$ with their exact gradients, a starting point $x_0\\in\\mathbb{R}$, a fixed search direction $d\\in\\mathbb{R}$, and a set of parameters that define a step-size selection rule based on a sufficient decrease condition. Arithmetic must be performed in Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision. The goal is to return a step size $\\alpha$ that satisfies a sufficient decrease inequality or else deterministically returns a failure value if such a step cannot be realized due to arithmetic or logical constraints.\n\nLet $f_1(x) = (x-3)^2$ with $\\nabla f_1(x) = 2(x-3)$ and $f_2(x) = x^8$ with $\\nabla f_2(x) = 8x^7$. For a given pair $(f,\\nabla f)$, point $x_0$, direction $d$, initial trial step $\\alpha_0>0$, reduction factor $\\rho\\in(0,1)$, sufficient decrease constant $c_1\\in(0,1)$, minimal step $\\alpha_{\\min}>0$, and a maximum number of reductions $k_{\\max}\\in\\mathbb{N}$, define the candidate steps $\\alpha_k=\\alpha_0\\rho^k$ for $k\\in\\{0,1,2,\\dots,k_{\\max}\\}$. A step $\\alpha_k$ is acceptable if it satisfies the inequality\n$$\nf(x_0+\\alpha_k d) \\le f(x_0) + c_1 \\alpha_k \\nabla f(x_0) d,\n$$\nand the floating-point update produces a distinct point, namely the computed value $\\mathrm{fl}(x_0+\\alpha_k d)$ is not exactly equal to $x_0$ in IEEE $754$ double precision. If $\\nabla f(x_0)\\,d\\ge 0$, you must return $0.0$ without further checks. Otherwise, among the sequence $\\{\\alpha_k\\}$ you must return the first acceptable $\\alpha_k$. If for all $k\\in\\{0,1,\\dots,k_{\\max}\\}$ either the inequality fails or the computed update satisfies $\\mathrm{fl}(x_0+\\alpha_k d)=x_0$, or if $\\alpha_k\\alpha_{\\min}$, then return $0.0$.\n\nUse the following parameter values for all test cases: $\\alpha_0=1.0$, $\\rho=0.5$, $c_1=10^{-4}$, $\\alpha_{\\min}=10^{-16}$, $k_{\\max}=1000$.\n\nTest suite:\n- Case $1$ (general well-conditioned decrease): use $f=f_1$, $x_0=0.0$, and $d=-\\nabla f_1(x_0)$.\n- Case $2$ (very flat plateau causing microscopic steps): use $f=f_2$, $x_0=10^{-3}$, and $d=-\\nabla f_2(x_0)$.\n- Case $3$ (arithmetic stagnation from large magnitude state): use $f=f_1$, $x_0=10^{16}$, and $d=-1.0$.\n- Case $4$ (non-descent direction): use $f=f_1$, $x_0=0.0$, and $d=+\\nabla f_1(x_0)$.\n\nYour program must implement the above selection rule and produce, for each case, the returned step size $\\alpha$ as a floating-point number. The final output must aggregate the results of all cases in order as a single line: a comma-separated Python-style list with no spaces, for example $[a_1,a_2,a_3,a_4]$, where each $a_i$ is the step size returned for Case $i$.\n\nThere are no physical units or angles in this problem. All real numbers must be treated as unitless scalars. The required final output for the program is a single line containing the list $[a_1,a_2,a_3,a_4]$.", "solution": "The problem requires the implementation of a backtracking line search algorithm to find an acceptable step size $\\alpha$ that satisfies a sufficient decrease condition, while also being robust to the limitations of finite-precision floating-point arithmetic. The analysis and implementation must adhere to the provided parameters and test cases.\n\nThe core of the algorithm is the sufficient decrease condition, also known as the Armijo condition, which ensures that the step size $\\alpha$ leads to a meaningful reduction in the objective function $f$. For a starting point $x_0$ and a search direction $d$, an acceptable step size $\\alpha_k$ must satisfy:\n$$\nf(x_0 + \\alpha_k d) \\le f(x_0) + c_1 \\alpha_k \\nabla f(x_0) d\n$$\nwhere $c_1 \\in (0, 1)$ is a constant. This condition is only meaningful if $d$ is a descent direction, i.e., if the directional derivative $\\nabla f(x_0) d  0$. If $\\nabla f(x_0) d \\ge 0$, the direction is not a descent direction, and the search must be terminated.\n\nThe algorithm searches for an acceptable $\\alpha$ by starting with an initial guess $\\alpha_0$ and iteratively reducing it by a factor $\\rho \\in (0,1)$, thereby generating a sequence of trial steps $\\alpha_k = \\alpha_0 \\rho^k$ for $k = 0, 1, 2, \\dots$. The first $\\alpha_k$ in this sequence that satisfies the condition is chosen.\n\nTwo practical constraints rooted in numerical computation are introduced:\n$1$. A minimum step size $\\alpha_{\\min}$: The search terminates if $\\alpha_k$ becomes smaller than this threshold, indicating that further progress is negligible.\n$2$. Arithmetic stagnation: Due to the finite precision of IEEE $754$ double-precision arithmetic, the update $x_{new} = \\mathrm{fl}(x_0 + \\alpha_k d)$ may result in $x_{new}$ being exactly equal to $x_0$. This occurs when the change $|\\alpha_k d|$ is too small to be represented relative to the magnitude of $x_0$. Such a step is ineffective and must be rejected.\n\nThe complete specified procedure is as follows:\nFirst, verify that the search direction $d$ is a descent direction by checking if $\\nabla f(x_0) d \\ge 0$. If this holds, the algorithm must return $0.0$. Otherwise, proceed by iterating $k$ from $0$ to $k_{\\max}$. In each iteration, calculate $\\alpha_k = \\alpha_0 \\rho^k$. If $\\alpha_k  \\alpha_{\\min}$, terminate and return $0.0$. Next, check for stagnation by computing $x_{new} = \\mathrm{fl}(x_0 + \\alpha_k d)$ and testing if $x_{new} == x_0$. If they are equal, the step is too small; reject $\\alpha_k$ and continue to the next iteration. If there is no stagnation, check the sufficient decrease condition. If it is satisfied, $\\alpha_k$ is the desired step size, and the algorithm returns it. If the loop completes without finding an acceptable step, it returns $0.0$.\n\nWe apply this algorithm to the four test cases using the parameters $\\alpha_0=1.0$, $\\rho=0.5$, $c_1=10^{-4}$, $\\alpha_{\\min}=10^{-16}$, and $k_{\\max}=1000$.\n\nCase $1$: $f(x)=f_1(x)=(x-3)^2$, $x_0=0.0$, $d = -\\nabla f_1(x_0)$.\nFirst, we compute the gradient and direction: $\\nabla f_1(x_0) = 2(0.0 - 3) = -6.0$. The direction is $d = -(-6.0) = 6.0$.\nThe directional derivative is $\\nabla f_1(x_0) d = (-6.0)(6.0) = -36.0$. Since this is negative, we proceed. We have $f(x_0) = (0.0-3)^2 = 9.0$.\nFor $k=0$, $\\alpha_0=1.0$. The new point is $x_{new} = 0.0 + 1.0 \\cdot 6.0 = 6.0$. Stagnation does not occur. We check the condition:\n$f(6.0) \\le f(0.0) + c_1 \\alpha_0 \\nabla f_1(0.0) d \\implies (6.0-3)^2 \\le 9.0 + 10^{-4}(1.0)(-36.0) \\implies 9.0 \\le 8.9964$. This is false.\nFor $k=1$, $\\alpha_1=0.5$. The new point is $x_{new} = 0.0 + 0.5 \\cdot 6.0 = 3.0$. Stagnation does not occur. We check the condition:\n$f(3.0) \\le f(0.0) + c_1 \\alpha_1 \\nabla f_1(0.0) d \\implies (3.0-3)^2 \\le 9.0 + 10^{-4}(0.5)(-36.0) \\implies 0.0 \\le 8.9982$. This is true.\nThe first acceptable step size is $\\alpha_1=0.5$.\n\nCase $2$: $f(x)=f_2(x)=x^8$, $x_0=10^{-3}$, $d = -\\nabla f_2(x_0)$.\nThe gradient is $\\nabla f_2(x_0) = 8(10^{-3})^7 = 8 \\cdot 10^{-21}$. The direction is $d = -8 \\cdot 10^{-21}$.\nThe directional derivative is $\\nabla f_2(x_0) d = (8 \\cdot 10^{-21})(-8 \\cdot 10^{-21}) = -64 \\cdot 10^{-42}  0$.\nThe point update is $x_{new} = \\mathrm{fl}(10^{-3} - \\alpha_k \\cdot 8 \\cdot 10^{-21})$. The magnitude of the change is $|\\alpha_k d| = \\alpha_k \\cdot 8 \\cdot 10^{-21}$. For $\\alpha_k \\le 1.0$, this change is at most $8 \\cdot 10^{-21}$. The unit in the last place (ULP) of $x_0=10^{-3}$ in double precision is approximately $1.36 \\cdot 10^{-19}$. Since the magnitude of the change is much smaller than the ULP of $x_0$, the addition will be absorbed, resulting in $\\mathrm{fl}(x_0 + \\alpha_k d) = x_0$ for all trial steps. Therefore, the stagnation condition $x_{new} == x_0$ will be true for every $k \\in \\{0, \\dots, k_{\\max}\\}$. The loop will continue until all iterations are exhausted, and the function will return $0.0$.\n\nCase $3$: $f(x)=f_1(x)=(x-3)^2$, $x_0=10^{16}$, $d=-1.0$.\nThe gradient is $\\nabla f_1(x_0) = 2(10^{16}-3)$, which is approximately $2 \\cdot 10^{16}$.\nThe directional derivative is $\\nabla f_1(x_0) d \\approx (2 \\cdot 10^{16})(-1.0) = -2 \\cdot 10^{16}  0$.\nThe point update is $x_{new} = \\mathrm{fl}(10^{16} - \\alpha_k)$. The ULP of $x_0=10^{16}$ is $2.0$. The trial steps are $\\alpha_k = 0.5^k$, all of which are $\\le 1.0$. Since every $\\alpha_k$ is smaller than the ULP of $x_0$, the subtraction will be lost due to absorption, and $\\mathrm{fl}(10^{16} - \\alpha_k)$ will evaluate to $10^{16}$. Stagnation occurs for all trial steps. The loop completes without finding a valid step, so the function returns $0.0$.\n\nCase $4$: $f(x)=f_1(x)=(x-3)^2$, $x_0=0.0$, $d=+\\nabla f_1(x_0)$.\nThe gradient is $\\nabla f_1(x_0) = -6.0$, so the direction is $d=-6.0$.\nThe directional derivative is $\\nabla f_1(x_0) d = (-6.0)(-6.0) = 36.0$.\nSince the directional derivative $36.0 \\ge 0$, the direction $d$ is not a descent direction. According to the specified procedure, the algorithm must terminate immediately and return $0.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    ALPHA0 = 1.0\n    RHO = 0.5\n    C1 = 1e-4\n    ALPHA_MIN = 1e-16\n    K_MAX = 1000\n\n    # --- Objective Functions and Gradients ---\n    def f1(x: float) -> float:\n        return (x - 3.0)**2\n\n    def grad_f1(x: float) -> float:\n        return 2.0 * (x - 3.0)\n\n    def f2(x: float) -> float:\n        return x**8\n\n    def grad_f2(x: float) -> float:\n        return 8.0 * x**7\n\n    def line_search(f, grad_f, x0: float, d: float) -> float:\n        \"\"\"\n        Implements the backtracking line search algorithm.\n\n        Args:\n            f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The starting point.\n            d: The search direction.\n\n        Returns:\n            The acceptable step size alpha, or 0.0 on failure.\n        \"\"\"\n        # All arithmetic is performed in IEEE 754 double precision,\n        # which is the standard for Python's float type.\n        \n        directional_derivative = grad_f(x0) * d\n\n        # Condition: Must be a descent direction\n        if directional_derivative >= 0:\n            return 0.0\n\n        f_x0 = f(x0)\n\n        for k in range(K_MAX + 1):\n            alpha_k = ALPHA0 * (RHO**k)\n\n            # Condition: Step size must not be smaller than the minimum\n            if alpha_k  ALPHA_MIN:\n                return 0.0\n\n            # Compute the new point and check for arithmetic stagnation\n            x_new = x0 + alpha_k * d\n            if x_new == x0:\n                continue  # Step is too small to change x0, try a smaller alpha\n\n            # Condition: Sufficient decrease (Armijo condition)\n            if f(x_new) = f_x0 + C1 * alpha_k * directional_derivative:\n                return alpha_k  # Acceptable step size found\n\n        # Failure: No acceptable step size found within k_max iterations\n        return 0.0\n\n    # --- Test Suite Definition ---\n    # Each case is a tuple: (function, gradient, start_point, direction_lambda)\n    # The direction_lambda calculates d based on the start point x0.\n    test_cases = [\n        # Case 1: General well-conditioned decrease\n        (f1, grad_f1, 0.0, lambda x: -grad_f1(x)),\n        # Case 2: Very flat plateau causing microscopic steps\n        (f2, grad_f2, 1e-3, lambda x: -grad_f2(x)),\n        # Case 3: Arithmetic stagnation from large magnitude state\n        (f1, grad_f1, 1e16, lambda x: -1.0),\n        # Case 4: Non-descent direction\n        (f1, grad_f1, 0.0, lambda x: grad_f1(x)),\n    ]\n\n    results = []\n    for f_handle, grad_f_handle, x0_val, d_lambda in test_cases:\n        direction = d_lambda(x0_val)\n        alpha_result = line_search(f_handle, grad_f_handle, x0_val, direction)\n        results.append(alpha_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2409357"}, {"introduction": "Many real-world optimization problems are not unconstrained; variables must often remain within specific physical or operational bounds. This final practice [@problem_id:2409334] extends the line search concept to handle such box-constrained problems. You will explore and implement two powerful techniques—projection and truncation—to ensure that each step remains feasible, providing you with essential tools for tackling practical engineering optimization tasks.", "problem": "You are given a continuously differentiable objective function $f : \\mathbb{R}^n \\to \\mathbb{R}$ with gradient $\\nabla f$, a current feasible point $x \\in \\mathbb{R}^n$ satisfying box constraints $l \\le x \\le u$ componentwise, and a search direction $p \\in \\mathbb{R}^n$. Consider a discrete candidate set of step sizes\n$$\n\\mathcal{S} = \\{\\alpha_0 \\rho^k \\mid k \\in \\{0,1,\\dots,K\\}\\},\n$$\nwhere $\\alpha_0 \\in \\mathbb{R}_{>0}$, $\\rho \\in \\mathbb{R}$ with $0  \\rho  1$, and $K \\in \\mathbb{N}$ is finite. Define the componentwise projection onto the box $[l,u]$ by\n$$\n\\Pi_{[l,u]}(z)_i = \\min\\{\\max\\{z_i, l_i\\}, u_i\\}, \\quad i = 1,\\dots,n.\n$$\n\nTwo feasibility enforcement modes are considered for forming the trial point from a candidate step size $\\alpha \\in \\mathcal{S}$:\n\n- Projection mode: $s(\\alpha) = \\Pi_{[l,u]}(x + \\alpha p) - x$ and $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$.\n- Truncation mode: First define the largest feasible step along $p$ by\n$$\n\\alpha_{\\max}(x,p) = \\min_{i=1,\\dots,n} \\begin{cases}\n\\dfrac{u_i - x_i}{p_i},  \\text{if } p_i > 0, \\\\[6pt]\n\\dfrac{l_i - x_i}{p_i},  \\text{if } p_i  0, \\\\[6pt]\n+\\infty,  \\text{if } p_i = 0,\n\\end{cases}\n$$\nand then set $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} \\, p$ and $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$.\n\nFor either mode, accept a candidate $\\alpha \\in \\mathcal{S}$ if the Armijo sufficient decrease condition with the actual step vector $s(\\alpha)$ holds:\n$$\nf\\big(x_{\\text{trial}}(\\alpha)\\big) \\le f(x) + c_1 \\, \\nabla f(x)^\\top s(\\alpha),\n$$\nwhere $c_1 \\in \\mathbb{R}$ satisfies $0  c_1  1$. Among all accepted candidates in $\\mathcal{S}$, define $\\alpha^\\star$ to be the largest by value. If no $\\alpha \\in \\mathcal{S}$ is accepted, define $\\alpha^\\star$ to be the smallest element of $\\mathcal{S}$, that is $\\alpha^\\star = \\alpha_0 \\rho^K$.\n\nYour task is to write a complete program that, for a given test suite, computes $\\alpha^\\star$ for each test case and outputs all results in a single line. All quantities are nondimensional; no physical units are involved.\n\nUse the following objective functions and their gradients:\n- Quadratic function in two variables ($n = 2$):\n$$\nf_{\\text{quad}}(x) = \\tfrac{1}{2} x^\\top Q x + b^\\top x, \\quad \\nabla f_{\\text{quad}}(x) = Q x + b,\n$$\nwith\n$$\nQ = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n- Rosenbrock function in two variables ($n = 2$):\n$$\nf_{\\text{ros}}(x_1,x_2) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\nwith gradient\n$$\n\\nabla f_{\\text{ros}}(x_1,x_2) = \\begin{bmatrix}\n-400 x_1 (x_2 - x_1^2) - 2(1 - x_1) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}.\n$$\n\nCompute $\\alpha^\\star$ for each of the following test cases. Each test case specifies $(f,\\nabla f)$, the mode, the point $x$, the direction $p$, the bounds $l,u$, and the parameters $\\alpha_0$, $\\rho$, $c_1$, and $K$:\n\n- Test case $1$ (happy path, interior, projection mode): $f = f_{\\text{quad}}$, $\\nabla f = \\nabla f_{\\text{quad}}$, mode = projection, $x = [0, 0]^\\top$, $p = -\\nabla f(x)$, $l = [-5,-5]^\\top$, $u = [5,5]^\\top$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $c_1 = 1 \\times 10^{-4}$, $K = 20$.\n- Test case $2$ (boundary truncation, direction limited by box): $f = f_{\\text{quad}}$, $\\nabla f = \\nabla f_{\\text{quad}}$, mode = truncation, $x = [1.9, -1.9]^\\top$, $p = [1.0, -50.0]^\\top$, $l = [-2,-2]^\\top$, $u = [2,2]^\\top$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $c_1 = 1 \\times 10^{-4}$, $K = 20$.\n- Test case $3$ (nonconvex landscape, requires reduction, projection mode): $f = f_{\\text{ros}}$, $\\nabla f = \\nabla f_{\\text{ros}}$, mode = projection, $x = [-1.2, 1.0]^\\top$, $p = -\\nabla f(x)$, $l = [-2,-1]^\\top$, $u = [2,3]^\\top$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $c_1 = 1 \\times 10^{-4}$, $K = 30$.\n- Test case $4$ (zero direction edge case): $f = f_{\\text{quad}}$, $\\nabla f = \\nabla f_{\\text{quad}}$, mode = projection, $x = [0.5, -0.5]^\\top$, $p = [0.0, 0.0]^\\top$, $l = [-1,-1]^\\top$, $u = [1,1]^\\top$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $c_1 = 1 \\times 10^{-4}$, $K = 10$.\n- Test case $5$ (projection clips one coordinate at the bound): $f = f_{\\text{quad}}$, $\\nabla f = \\nabla f_{\\text{quad}}$, mode = projection, $x = [2.0, 0.0]^\\top$, $p = [1.0, -1.0]^\\top$, $l = [-2,-2]^\\top$, $u = [2,2]^\\top$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $c_1 = 1 \\times 10^{-4}$, $K = 20$.\n\nYour program must compute $\\alpha^\\star$ for each test case and produce a single line of output containing the results as a comma-separated list of decimal numbers rounded to exactly six digits after the decimal point, enclosed in square brackets. For example, the output format must be\n$$\n[\\alpha^\\star_1,\\alpha^\\star_2,\\alpha^\\star_3,\\alpha^\\star_4,\\alpha^\\star_5]\n$$\nwith each $\\alpha^\\star_i$ printed to six decimal places.", "solution": "The goal is to determine, for each test case, the largest step size in a discrete set that satisfies the Armijo sufficient decrease condition while respecting box constraints, enforced either by projection of the trial point or by truncation of the step to the maximal feasible value.\n\nWe start from first principles. Given $x \\in \\mathbb{R}^n$, a box $[l,u]$ with $l \\le u$ componentwise, and a direction $p \\in \\mathbb{R}^n$, feasibility of a step is defined via either projection or truncation. The projection operator $\\Pi_{[l,u]}$ is defined componentwise by\n$$\n\\Pi_{[l,u]}(z)_i = \\min\\{\\max\\{z_i,l_i\\},u_i\\}.\n$$\nThis is the orthogonal projection onto the closed convex set $[l,u]$ in the Euclidean space. It ensures $x_{\\text{trial}}(\\alpha) = \\Pi_{[l,u]}(x+\\alpha p)$ is feasible for any $\\alpha \\ge 0$.\n\nAlternatively, the maximal feasible step length along the ray $x + \\alpha p$ can be derived from the componentwise inequalities $l_i \\le x_i + \\alpha p_i \\le u_i$ for each component $i$. Solving these for $\\alpha$ yields the bound\n$$\n\\alpha \\le \\frac{u_i - x_i}{p_i} \\text{ if } p_i > 0, \\quad\n\\alpha \\le \\frac{l_i - x_i}{p_i} \\text{ if } p_i  0, \\quad\n\\alpha \\in \\mathbb{R}_{\\ge 0} \\text{ arbitrary if } p_i = 0.\n$$\nThe largest $\\alpha$ that maintains feasibility for all components simultaneously is then\n$$\n\\alpha_{\\max}(x,p) = \\min_{i=1,\\dots,n} \\begin{cases}\n\\dfrac{u_i - x_i}{p_i},  \\text{if } p_i > 0, \\\\[6pt]\n\\dfrac{l_i - x_i}{p_i},  \\text{if } p_i  0, \\\\[6pt]\n+\\infty,  \\text{if } p_i = 0.\n\\end{cases}\n$$\nIn truncation mode, one uses the actual step vector $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} p$, which guarantees $x + s(\\alpha)$ lies in the box.\n\nThe Armijo sufficient decrease condition is a first-order criterion ensuring that the decrease in the objective at the trial point is commensurate with the directional derivative at the current point, scaled by a factor $c_1 \\in (0,1)$. For an actual step vector $s(\\alpha)$ and trial point $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$, the condition is\n$$\nf\\big(x_{\\text{trial}}(\\alpha)\\big) \\le f(x) + c_1 \\nabla f(x)^\\top s(\\alpha).\n$$\nNote that the right-hand side uses the linear model of $f$ at $x$ along the specific displacement $s(\\alpha)$, which in projection mode may not be colinear with $p$ due to clipping. The above form is the natural generalization for both modes because it expresses the sufficient decrease relative to the actual step taken.\n\nTo ensure a finite, well-posed selection, we restrict candidate step sizes to a finite geometric set $\\mathcal{S} = \\{\\alpha_0 \\rho^k \\mid k = 0,1,\\dots,K\\}$ with $\\alpha_0 > 0$ and $0  \\rho  1$. The task is to choose\n$$\n\\alpha^\\star = \\max\\{\\alpha \\in \\mathcal{S} \\mid f(x_{\\text{trial}}(\\alpha)) \\le f(x) + c_1 \\nabla f(x)^\\top s(\\alpha)\\},\n$$\nwith the convention that if no such $\\alpha$ exists, then $\\alpha^\\star = \\alpha_0 \\rho^K$.\n\nWe implement this selection by evaluating the inequality for candidates in decreasing order of $\\alpha$ and accepting the first that satisfies the inequality; if none do, we return the smallest $\\alpha$.\n\nThe specific objective functions and gradients are:\n- Quadratic:\n$$\nf_{\\text{quad}}(x) = \\tfrac{1}{2} x^\\top Q x + b^\\top x, \\quad \\nabla f_{\\text{quad}}(x) = Q x + b, \\quad\nQ = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n- Rosenbrock:\n$$\nf_{\\text{ros}}(x_1,x_2) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\n$$\n\\nabla f_{\\text{ros}}(x_1,x_2) = \\begin{bmatrix}\n-400 x_1 (x_2 - x_1^2) - 2(1 - x_1) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}.\n$$\n\nKey properties used:\n- For projection mode, feasibility is guaranteed for any $\\alpha \\ge 0$ by definition of $\\Pi_{[l,u]}$.\n- For truncation mode, feasibility is guaranteed by construction of $\\alpha_{\\max}(x,p)$ and using $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} p$.\n- If $s(\\alpha) = 0$, the Armijo inequality holds with equality since $f(x_{\\text{trial}}(\\alpha)) = f(x)$ and $\\nabla f(x)^\\top s(\\alpha) = 0$.\n\nDiscussion of edge cases in the test suite:\n- Test case $1$ operates well within the box; the projection does not alter the step for sufficiently small $\\alpha$. The descent direction $p = -\\nabla f(x)$ ensures that the Armijo inequality will be satisfied for some $\\alpha \\in \\mathcal{S}$.\n- Test case $2$ showcases truncation at the boundary. The direction $p$ simultaneously attempts to increase the first coordinate and strongly decrease the second. The value $\\alpha_{\\max}(x,p)$ is dominated by the second coordinate’s lower bound, which determines the effective step length in truncation mode. If the Armijo condition does not hold at the truncated step with the largest candidates that still truncate, reducing $\\alpha$ further eventually reduces the actual step $s(\\alpha)$ and yields acceptance.\n- Test case $3$ is nonconvex; starting near a Rosenbrock valley, the full step may not satisfy Armijo. The discretized reduction by $\\rho$ ensures that a sufficiently small step will satisfy the inequality.\n- Test case $4$ has $p = 0$. Then $s(\\alpha) = 0$ for all $\\alpha$, and the Armijo condition holds trivially. The largest candidate is selected by definition, so $\\alpha^\\star = \\alpha_0$.\n- Test case $5$ starts at the upper bound in the first coordinate and moves outward in that coordinate, so projection clips the first component of the step to zero while allowing motion in the second component. The Armijo condition evaluates decrease along the actual displacement $s(\\alpha)$, which in this case is aligned with the negative gradient in the second component for small $\\alpha$.\n\nNumerical computation details for the program:\n- Evaluate $f$ and $\\nabla f$ at the current point $x$ once per test case.\n- For each $\\alpha \\in \\mathcal{S}$ in decreasing order, compute $s(\\alpha)$ per the specified mode and check the Armijo inequality.\n- Select the largest acceptable $\\alpha$ or the smallest candidate if none satisfy the inequality.\n- Report each $\\alpha^\\star$ rounded to exactly six digits after the decimal point, as required.\n\nThe program directly implements these definitions, ensuring correctness based on the mathematical statements above. For the truncation mode, $\\alpha_{\\max}(x,p)$ is computed componentwise and set to $+\\infty$ where $p_i = 0$. For projection mode, the projection operator is applied componentwise. In all cases, the Armijo condition is evaluated with the actual step vector $s(\\alpha)$, aligning with first-order sufficient decrease principles in constrained settings.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_quad(x):\n    Q = np.array([[3.0, 1.0],\n                  [1.0, 2.0]])\n    b = np.array([-1.0, 2.0])\n    return 0.5 * x @ (Q @ x) + b @ x\n\ndef grad_quad(x):\n    Q = np.array([[3.0, 1.0],\n                  [1.0, 2.0]])\n    b = np.array([-1.0, 2.0])\n    return Q @ x + b\n\ndef f_rosen(x):\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef grad_rosen(x):\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * x1 * (x2 - x1**2) - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2], dtype=float)\n\ndef project_box(z, l, u):\n    return np.minimum(np.maximum(z, l), u)\n\ndef alpha_max_truncation(x, p, l, u):\n    # Compute maximum feasible alpha along direction p from x within [l,u]\n    alpha_max = np.inf\n    for i in range(len(x)):\n        if p[i] > 0:\n            cand = (u[i] - x[i]) / p[i]\n            if cand  alpha_max:\n                alpha_max = cand\n        elif p[i]  0:\n            cand = (l[i] - x[i]) / p[i]\n            if cand  alpha_max:\n                alpha_max = cand\n        else:\n            # p[i] == 0: no restriction from this component\n            pass\n    if not np.isfinite(alpha_max):\n        alpha_max = np.inf\n    if alpha_max  0:\n        # If negative due to numerical issues (should not happen if x is feasible)\n        alpha_max = 0.0\n    return alpha_max\n\ndef line_search_alpha_star(f, grad, x, p, l, u, mode, alpha0, rho, c1, K):\n    # Precompute current function value and gradient\n    fx = f(x)\n    g = grad(x)\n    # Generate candidate alphas in decreasing order\n    alphas = [alpha0 * (rho ** k) for k in range(0, K + 1)]\n    accepted_alpha = None\n\n    # Tolerance to guard against tiny numerical violations\n    tol = 1e-12\n\n    if mode == 'truncate':\n        amax = alpha_max_truncation(x, p, l, u)\n\n    for alpha in alphas:\n        if mode == 'project':\n            x_trial = project_box(x + alpha * p, l, u)\n            s = x_trial - x\n        elif mode == 'truncate':\n            alpha_eff = min(alpha, amax)\n            s = alpha_eff * p\n            x_trial = x + s\n            # Make sure numerical drift stays within bounds\n            x_trial = project_box(x_trial, l, u)\n        else:\n            raise ValueError(\"Unknown mode\")\n\n        lhs = f(x_trial)\n        rhs = fx + c1 * (g @ s)\n        if lhs = rhs + tol:\n            accepted_alpha = alpha\n            break\n\n    if accepted_alpha is None:\n        accepted_alpha = alphas[-1]\n    return accepted_alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (f, grad, mode, x, p, l, u, alpha0, rho, c1, K)\n        # Test case 1\n        (f_quad, grad_quad, 'project',\n         np.array([0.0, 0.0]), None,\n         np.array([-5.0, -5.0]), np.array([5.0, 5.0]),\n         1.0, 0.5, 1e-4, 20),\n        # Test case 2\n        (f_quad, grad_quad, 'truncate',\n         np.array([1.9, -1.9]), np.array([1.0, -50.0]),\n         np.array([-2.0, -2.0]), np.array([2.0, 2.0]),\n         1.0, 0.5, 1e-4, 20),\n        # Test case 3\n        (f_rosen, grad_rosen, 'project',\n         np.array([-1.2, 1.0]), None,\n         np.array([-2.0, -1.0]), np.array([2.0, 3.0]),\n         1.0, 0.5, 1e-4, 30),\n        # Test case 4\n        (f_quad, grad_quad, 'project',\n         np.array([0.5, -0.5]), np.array([0.0, 0.0]),\n         np.array([-1.0, -1.0]), np.array([1.0, 1.0]),\n         1.0, 0.5, 1e-4, 10),\n        # Test case 5\n        (f_quad, grad_quad, 'project',\n         np.array([2.0, 0.0]), np.array([1.0, -1.0]),\n         np.array([-2.0, -2.0]), np.array([2.0, 2.0]),\n         1.0, 0.5, 1e-4, 20),\n    ]\n\n    results = []\n    for f, grad, mode, x, p, l, u, alpha0, rho, c1, K in test_cases:\n        # If p is None in projection cases where p = -grad(x)\n        if p is None:\n            p = -grad(x)\n        alpha_star = line_search_alpha_star(f, grad, x, p, l, u, mode, alpha0, rho, c1, K)\n        results.append(alpha_star)\n\n    # Format results to exactly six decimal places\n    formatted = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2409334"}]}