{"hands_on_practices": [{"introduction": "We begin our hands-on practice with a problem rooted in geometry, which serves as an excellent canvas for visualizing the core principles of constrained optimization. The task is to find the point on a specified curve—an astroid—that is furthest from the origin. This exercise provides a direct application of the Lagrange multiplier method, where we seek points where the gradient of our objective function (distance squared) is parallel to the gradient of the constraint function defining the curve [@problem_id:2380562]. Working through this problem will build your intuition for the geometric interpretation of Lagrange multipliers and develop your skills in setting up and solving the resulting system of algebraic equations.", "problem": "A planar profile in computational design is defined implicitly by the astroid curve given by $x^{2/3}+y^{2/3}=a^{2/3}$, where $a>0$ is a fixed constant. Consider only the portion of this curve in the first quadrant including its bounding axes (that is, with $x\\ge 0$ and $y\\ge 0$). Let the Euclidean distance from the origin to a point $(x,y)$ be $r(x,y)=\\sqrt{x^{2}+y^{2}}$. Determine the maximal possible value of $r(x,y)$ over all points $(x,y)$ on this portion of the astroid, expressed as a symbolic function of $a$. Report only the maximal value of $r(x,y)$ as your final answer. No rounding is required.", "solution": "The problem statement is parsed and validated.\n\n**Step 1: Extract Givens**\n- The planar profile is an astroid curve defined by the equation $x^{2/3}+y^{2/3}=a^{2/3}$.\n- The constant $a$ is positive, $a>0$.\n- The domain is restricted to the first quadrant, including the axes: $x \\ge 0$ and $y \\ge 0$.\n- The function to be maximized is the Euclidean distance from the origin, $r(x,y)=\\sqrt{x^{2}+y^{2}}$.\n- The task is to find the maximal value of $r(x,y)$ as a function of $a$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in multivariable calculus, specifically constrained optimization. The astroid curve and Euclidean distance are well-defined mathematical concepts. The problem is free from scientific falsehoods.\n- **Well-Posed:** The problem asks to maximize a continuous function, $r(x,y)$, over a compact set (the portion of the astroid in the first quadrant is a closed and bounded curve). The Extreme Value Theorem guarantees that a maximum and minimum exist. The problem is structured to find a unique maximum value.\n- **Objective:** The problem is stated using precise, unambiguous mathematical language.\n\nThe problem is deemed valid as it is mathematically sound, well-posed, and objective. There are no contradictions, missing information, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n**Solution Derivation**\nThe problem is to find the maximum value of the function $r(x,y) = \\sqrt{x^2+y^2}$ subject to the constraint $g(x,y) = x^{2/3}+y^{2/3} - a^{2/3} = 0$, with the additional constraints $x \\ge 0$ and $y \\ge 0$.\n\nMaximizing the distance $r(x,y)$ is equivalent to maximizing its square, $f(x,y) = r(x,y)^2 = x^2+y^2$, because the square root function is strictly monotonically increasing for non-negative arguments. This simplifies the differentiation required.\n\nThe constraint defines a compact set in the plane. A robust method for solving this problem is to parametrize the curve. The astroid $x^{2/3}+y^{2/3}=a^{2/3}$ has a standard parametrization given by:\n$$x(t) = a \\cos^3(t)$$\n$$y(t) = a \\sin^3(t)$$\nThe part of the curve in the first quadrant, where $x \\ge 0$ and $y \\ge 0$, corresponds to the parameter interval $t \\in [0, \\frac{\\pi}{2}]$.\n\nWe can now express the function $f(x,y)$ as a function of a single variable $t$:\n$$f(t) = (x(t))^2 + (y(t))^2 = (a \\cos^3(t))^2 + (a \\sin^3(t))^2 = a^2(\\cos^6(t) + \\sin^6(t))$$\nTo find the maximum value of $f(t)$ on the closed interval $[0, \\frac{\\pi}{2}]$, we must find the critical points by taking the derivative with respect to $t$ and setting it to zero. We must also evaluate the function at the endpoints of the interval, $t=0$ and $t=\\frac{\\pi}{2}$.\n\nThe derivative of $f(t)$ is:\n$$\\frac{df}{dt} = a^2 \\frac{d}{dt}(\\cos^6(t) + \\sin^6(t)) = a^2 [6\\cos^5(t)(-\\sin(t)) + 6\\sin^5(t)(\\cos(t))]$$\n$$\\frac{df}{dt} = 6a^2\\sin(t)\\cos(t)(\\sin^4(t) - \\cos^4(t))$$\nWe can factor the difference of squares:\n$$\\frac{df}{dt} = 6a^2\\sin(t)\\cos(t)(\\sin^2(t) - \\cos^2(t))(\\sin^2(t) + \\cos^2(t))$$\nUsing the identities $\\sin^2(t) + \\cos^2(t)=1$ and $\\cos^2(t) - \\sin^2(t) = \\cos(2t)$:\n$$\\frac{df}{dt} = -6a^2\\sin(t)\\cos(t)\\cos(2t)$$\nUsing the double angle identity $2\\sin(t)\\cos(t) = \\sin(2t)$:\n$$\\frac{df}{dt} = -3a^2\\sin(2t)\\cos(2t)$$\nUsing the double angle identity again for $\\sin(4t) = 2\\sin(2t)\\cos(2t)$:\n$$\\frac{df}{dt} = -\\frac{3}{2}a^2\\sin(4t)$$\nSetting the derivative to zero to find critical points in the open interval $(0, \\frac{\\pi}{2})$:\n$$-\\frac{3}{2}a^2\\sin(4t) = 0 \\implies \\sin(4t) = 0$$\nFor $t \\in (0, \\frac{\\pi}{2})$, the range of $4t$ is $(0, 2\\pi)$. The only solution in this range is $4t = \\pi$, which gives $t = \\frac{\\pi}{4}$.\n\nNow, we evaluate the function $r(x,y)$ (or its square $f(t)$) at the critical point $t=\\frac{\\pi}{4}$ and at the endpoints $t=0$ and $t=\\frac{\\pi}{2}$.\n\nCase 1: $t=0$ (endpoint)\n$x = a \\cos^3(0) = a$\n$y = a \\sin^3(0) = 0$\nThe point is $(a, 0)$. The distance is $r = \\sqrt{a^2 + 0^2} = a$.\n\nCase 2: $t=\\frac{\\pi}{2}$ (endpoint)\n$x = a \\cos^3(\\frac{\\pi}{2}) = 0$\n$y = a \\sin^3(\\frac{\\pi}{2}) = a$\nThe point is $(0, a)$. The distance is $r = \\sqrt{0^2 + a^2} = a$.\n\nCase 3: $t=\\frac{\\pi}{4}$ (critical point)\n$x = a \\cos^3(\\frac{\\pi}{4}) = a \\left(\\frac{1}{\\sqrt{2}}\\right)^3 = \\frac{a}{2\\sqrt{2}}$\n$y = a \\sin^3(\\frac{\\pi}{4}) = a \\left(\\frac{1}{\\sqrt{2}}\\right)^3 = \\frac{a}{2\\sqrt{2}}$\nThe point is $(\\frac{a}{2\\sqrt{2}}, \\frac{a}{2\\sqrt{2}})$. The squared distance is:\n$f(\\frac{\\pi}{4}) = \\left(\\frac{a}{2\\sqrt{2}}\\right)^2 + \\left(\\frac{a}{2\\sqrt{2}}\\right)^2 = \\frac{a^2}{8} + \\frac{a^2}{8} = \\frac{2a^2}{8} = \\frac{a^2}{4}$.\nThe distance is $r = \\sqrt{\\frac{a^2}{4}} = \\frac{a}{2}$.\n\nComparing the values of $r$ at the candidate points: $a$, $a$, and $\\frac{a}{2}$. Since $a>0$, the maximum value is $a$. This maximal distance occurs at the intersection of the astroid with the coordinate axes in the first quadrant, i.e., at points $(a,0)$ and $(0,a)$.", "answer": "$$\\boxed{a}$$", "id": "2380562"}, {"introduction": "Moving from pure geometry to practical application, this problem places you in the role of a production manager at a factory. Your goal is to minimize production costs, but you must operate within certain limitations: a minimum production target and a maximum emissions cap. These constraints are expressed as inequalities, a common feature in real-world engineering and economic problems. To solve this, you will employ the Karush-Kuhn-Tucker (KKT) conditions, a powerful generalization of the Lagrange multiplier method for problems involving inequality constraints [@problem_id:2380580]. This practice is invaluable for learning how to model resource allocation scenarios and find optimal solutions in a constrained decision space.", "problem": "A factory operates two parallel production lines with decision variables $x$ and $y$, representing the production quantities (in units) on lines $1$ and $2$, respectively. The total production cost is modeled by the quadratic function\n$$\nC(x,y) \\;=\\; x^{2} \\;+\\; y^{2} \\;+\\; \\frac{1}{2}\\,x\\,y.\n$$\nTotal output is $P(x,y)=x+y$ (in units), and total emissions are $E(x,y)=x+2y$ (in emission units). The factory must satisfy the production requirement $P(x,y)\\ge N$ with $N=10$, and the emission cap $E(x,y)\\le E_{\\max}$ with $E_{\\max}=14$. Production quantities are nonnegative: $x\\ge 0$ and $y\\ge 0$.\n\nDetermine the minimum achievable total cost $C_{\\min}$ subject to these constraints. Express your final answer in cost units (Cost Units (CU)). Provide an exact value (no rounding).", "solution": "The problem presented is a constrained optimization problem, which is a standard task in computational engineering and operations research. The problem is well-posed and scientifically grounded. I will now proceed with its formal solution.\n\nThe objective is to minimize the cost function $C(x,y) = x^2 + y^2 + \\frac{1}{2}xy$ subject to a set of linear inequality constraints.\nThe constraints are:\n$1.$ Production requirement: $x+y \\ge 10$\n$2.$ Emission cap: $x+2y \\le 14$\n$3.$ Non-negativity of production on line $1$: $x \\ge 0$\n$4.$ Non-negativity of production on line $2$: $y \\ge 0$\n\nFirst, let us characterize the feasible region defined by these inequalities. This region is a convex polygon in the $xy$-plane. The vertices of this polygon are found by the intersection of the boundary lines.\nThe boundary lines are $x+y=10$, $x+2y=14$, $x=0$, and $y=0$.\n- Intersection of $y=0$ and $x+y=10$: This gives the point $(10, 0)$. We verify feasibility: $10+2(0)=10 \\le 14$. This point is feasible. Let us call it $V_1 = (10, 0)$.\n- Intersection of $y=0$ and $x+2y=14$: This gives the point $(14, 0)$. We verify feasibility: $14+0=14 \\ge 10$. This point is feasible. Let us call it $V_2 = (14, 0)$.\n- Intersection of $x+y=10$ and $x+2y=14$: Subtracting the first equation from the second gives $(x+2y)-(x+y) = 14-10$, which simplifies to $y=4$. Substituting $y=4$ into the first equation gives $x+4=10$, so $x=6$. This gives the point $(6, 4)$. We verify feasibility: $x=6 \\ge 0$ and $y=4 \\ge 0$. This point is feasible. Let us call it $V_3 = (6, 4)$.\n\nThe feasible region is a triangle with vertices $V_1(10,0)$, $V_2(14,0)$, and $V_3(6,4)$.\n\nThe objective function is $C(x,y) = x^2 + y^2 + \\frac{1}{2}xy$. To determine if this function is convex, we examine its Hessian matrix, $H$:\n$$ H(x,y) = \\begin{pmatrix} \\frac{\\partial^2 C}{\\partial x^2} & \\frac{\\partial^2 C}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 C}{\\partial y \\partial x} & \\frac{\\partial^2 C}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\end{pmatrix} $$\nThe first principal minor is $2 > 0$. The second principal minor is the determinant, $\\det(H) = (2)(2) - (\\frac{1}{2})(\\frac{1}{2}) = 4 - \\frac{1}{4} = \\frac{15}{4} > 0$. Since the Hessian is positive-definite for all $(x,y)$, the function $C(x,y)$ is strictly convex.\n\nThe minimum of a strictly convex function over a compact convex set (our triangular feasible region) is unique and occurs at a point satisfying the Karush-Kuhn-Tucker (KKT) conditions. This point can be in the interior of the region or on its boundary.\n\nLet us rewrite the constraints in the standard form $g_i(x,y) \\le 0$:\n$g_1(x,y) = 10 - x - y \\le 0$\n$g_2(x,y) = x + 2y - 14 \\le 0$\n$g_3(x,y) = -x \\le 0$\n$g_4(x,y) = -y \\le 0$\n\nThe Lagrangian function is:\n$$ \\mathcal{L}(x, y, \\lambda_1, \\lambda_2, \\mu_1, \\mu_2) = x^2 + y^2 + \\frac{1}{2}xy + \\lambda_1(10-x-y) + \\lambda_2(x+2y-14) - \\mu_1 x - \\mu_2 y $$\nwhere $\\lambda_1, \\lambda_2, \\mu_1, \\mu_2$ are non-negative Lagrange multipliers.\n\nThe KKT stationarity conditions are:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x} = 2x + \\frac{1}{2}y - \\lambda_1 + \\lambda_2 - \\mu_1 = 0 $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial y} = 2y + \\frac{1}{2}x - \\lambda_1 + 2\\lambda_2 - \\mu_2 = 0 $$\n\nCase 1: The minimum is in the interior of the feasible region.\nIn this case, all inequality constraints are inactive, so $\\lambda_1 = \\lambda_2 = \\mu_1 = \\mu_2 = 0$. The stationarity conditions become:\n$2x + \\frac{1}{2}y = 0$\n$2y + \\frac{1}{2}x = 0$\nThe only real solution is $(x,y)=(0,0)$. However, this point is not in the feasible region, as it violates the constraint $x+y \\ge 10$. Thus, the minimum must lie on the boundary of the feasible region.\n\nThe minimum must lie on the boundary. We can evaluate the cost function $C(x,y)$ at each vertex of the feasible region, which are the primary candidates.\n\n- At $V_1(10,0)$:\n$C(10,0) = 10^2 + 0^2 + \\frac{1}{2}(10)(0) = 100$.\n\n- At $V_2(14,0)$:\n$C(14,0) = 14^2 + 0^2 + \\frac{1}{2}(14)(0) = 196$.\n\n- At $V_3(6,4)$:\n$C(6,4) = 6^2 + 4^2 + \\frac{1}{2}(6)(4) = 36 + 16 + 12 = 64$.\n\nComparing these values, the minimum cost is $64$, which occurs at the point $(x,y)=(6,4)$.\n\nTo be rigorous, we verify that the point $(6,4)$ satisfies the KKT conditions.\nAt $(x,y)=(6,4)$, the constraints $x+y \\ge 10$ and $x+2y \\le 14$ are active, meaning $g_1(6,4)=0$ and $g_2(6,4)=0$. The non-negativity constraints are inactive, so their corresponding multipliers are zero: $\\mu_1 = 0$ and $\\mu_2 = 0$.\nThe stationarity conditions become:\n$2(6) + \\frac{1}{2}(4) - \\lambda_1 + \\lambda_2 = 0 \\implies 14 - \\lambda_1 + \\lambda_2 = 0 \\implies \\lambda_1 - \\lambda_2 = 14$.\n$2(4) + \\frac{1}{2}(6) - \\lambda_1 + 2\\lambda_2 = 0 \\implies 11 - \\lambda_1 + 2\\lambda_2 = 0 \\implies \\lambda_1 - 2\\lambda_2 = 11$.\n\nWe solve this system for $\\lambda_1$ and $\\lambda_2$. Subtracting the second equation from the first yields:\n$(\\lambda_1 - \\lambda_2) - (\\lambda_1 - 2\\lambda_2) = 14 - 11 \\implies \\lambda_2 = 3$.\nSubstituting $\\lambda_2=3$ into the first equation: $\\lambda_1 - 3 = 14 \\implies \\lambda_1 = 17$.\n\nWe have found the multipliers $\\lambda_1=17$, $\\lambda_2=3$, $\\mu_1=0$, $\\mu_2=0$.\nAll multipliers are non-negative, satisfying the dual feasibility condition. The primal feasibility and complementary slackness conditions are satisfied by construction.\nSince $(x,y)=(6,4)$ satisfies the KKT conditions for a convex optimization problem, it is the global minimum.\n\nThe minimum achievable total cost is $C_{\\min} = C(6,4) = 64$.", "answer": "$$\n\\boxed{64}\n$$", "id": "2380580"}, {"introduction": "While analytical solutions are enlightening, many real-world optimization problems are too complex to be solved by hand and require numerical algorithms. This final exercise provides hands-on computational practice by guiding you to implement the augmented Lagrangian method, a cornerstone of modern optimization solvers. The method transforms a constrained problem into a sequence of simpler, unconstrained subproblems by blending the Lagrangian with a penalty term for constraint violation. By implementing this algorithm and observing how a key parameter, $\\rho$, affects its convergence rate, you will gain profound insight into the mechanics of computational optimization and the trade-offs involved in designing efficient and robust numerical methods [@problem_id:2380561].", "problem": "Consider the equality-constrained quadratic optimization problem in two real variables. Let the objective function be given by\n$$\nf(\\boldsymbol{x}) \\;=\\; \\tfrac{1}{2}\\,\\boldsymbol{x}^\\top Q\\,\\boldsymbol{x} \\;+\\; \\boldsymbol{c}^\\top \\boldsymbol{x},\n$$\nwhere $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite and $\\boldsymbol{c} \\in \\mathbb{R}^{2}$. The linear equality constraint is\n$$\nA\\,\\boldsymbol{x} \\;=\\; b,\n$$\nwhere $A \\in \\mathbb{R}^{1 \\times 2}$ and $b \\in \\mathbb{R}$. Use the specific data\n$$\nQ \\;=\\; \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad \\boldsymbol{c} \\;=\\; \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}, \\quad A \\;=\\; \\begin{bmatrix} 1 & 2 \\end{bmatrix}, \\quad b \\;=\\; 1.\n$$\nLet the initial primal iterate be\n$$\n\\boldsymbol{x}_0 \\;=\\; \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n$$\nand the initial dual iterate for the equality constraint (the Lagrange multiplier) be\n$$\n\\lambda_0 \\;=\\; 0.\n$$\nDefine the Karush–Kuhn–Tucker (KKT) residuals at an iterate $(\\boldsymbol{x}, \\lambda)$ as the primal residual\n$$\nr_p \\;=\\; \\lVert A\\,\\boldsymbol{x} - b \\rVert_2,\n$$\nand the stationarity residual\n$$\nr_s \\;=\\; \\lVert Q\\,\\boldsymbol{x} + \\boldsymbol{c} + A^\\top \\lambda \\rVert_2.\n$$\nAn algorithm is declared converged when\n$$\n\\max\\{\\, r_p,\\, r_s \\,\\} \\;\\le\\; \\varepsilon,\n$$\nwith tolerance\n$$\n\\varepsilon \\;=\\; 10^{-9}.\n$$\nImpose a maximum number of outer iterations\n$$\nk_{\\max} \\;=\\; 10000.\n$$\nFor the penalty parameter values (the test suite)\n$$\n\\boldsymbol{\\rho}_{\\text{test}} \\;=\\; \\big[\\, 0.01,\\, 0.1,\\, 1.0,\\, 10.0,\\, 1000.0 \\,\\big],\n$$\napply a numerical augmented Lagrangian approach for this problem, starting from $\\boldsymbol{x}_0$ and $\\lambda_0$ for each value of $\\rho \\in \\boldsymbol{\\rho}_{\\text{test}}$. For each $\\rho$, count the number of outer iterations required to satisfy the convergence criterion above. If convergence is not achieved within $k_{\\max}$ iterations, report $k_{\\max}$ for that case.\n\nYour program must produce a single line of output containing the iteration counts for the entries of $\\boldsymbol{\\rho}_{\\text{test}}$ in the same order, as a comma-separated list enclosed in square brackets. For example, if the counts were $k_1, k_2, k_3, k_4, k_5$, the output must be\n$$\n[\\,k_1,k_2,k_3,k_4,k_5\\,].\n$$\nAll numerical answers are dimensionless and must be returned as integers. No physical units are involved, and no angles are used in this problem.", "solution": "The problem presented is a standard, well-posed equality-constrained quadratic optimization problem. All necessary data, initial conditions, and convergence criteria are provided, and the problem is based on established principles of numerical optimization in computational engineering. Therefore, it is valid and admits a direct solution.\n\nThe task is to minimize the quadratic objective function\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top Q\\,\\boldsymbol{x} + \\boldsymbol{c}^\\top \\boldsymbol{x}\n$$\nsubject to the linear equality constraint\n$$\nA\\,\\boldsymbol{x} - b = 0.\n$$\nThis problem will be solved using the augmented Lagrangian method, also known as the method of multipliers. This iterative method combines the Lagrangian function with a quadratic penalty term for the constraint violation.\n\nThe augmented Lagrangian function $L_{\\rho}(\\boldsymbol{x}, \\lambda)$ for this problem is defined as:\n$$\nL_{\\rho}(\\boldsymbol{x}, \\lambda) = f(\\boldsymbol{x}) + \\lambda (A\\,\\boldsymbol{x} - b) + \\frac{\\rho}{2}(A\\,\\boldsymbol{x} - b)^2\n$$\nHere, $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier associated with the equality constraint, and $\\rho > 0$ is the penalty parameter. The augmented Lagrangian method proceeds via a sequence of \"outer\" iterations, indexed by $k$, starting from an initial guess $(\\boldsymbol{x}_0, \\lambda_0)$. Each iteration consists of two main steps:\n\n1.  **Primal Update (x-minimization):** For a given multiplier estimate $\\lambda_k$, the next primal iterate $\\boldsymbol{x}_{k+1}$ is found by minimizing the augmented Lagrangian function with respect to $\\boldsymbol{x}$:\n    $$\n    \\boldsymbol{x}_{k+1} = \\operatorname*{arg\\,min}_{\\boldsymbol{x}} L_{\\rho}(\\boldsymbol{x}, \\lambda_k)\n    $$\n    Since $L_{\\rho}(\\boldsymbol{x}, \\lambda_k)$ is a strictly convex quadratic function of $\\boldsymbol{x}$ (as $Q$ is positive definite and $\\rho > 0$), this minimization subproblem has a unique solution. This solution is found by setting the gradient of $L_{\\rho}$ with respect to $\\boldsymbol{x}$ to zero:\n    $$\n    \\nabla_{\\boldsymbol{x}} L_{\\rho}(\\boldsymbol{x}_{k+1}, \\lambda_k) = \\boldsymbol{0}\n    $$\n    Computing the gradient yields:\n    $$\n    \\nabla_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\boldsymbol{x}^\\top Q\\,\\boldsymbol{x} + \\boldsymbol{c}^\\top \\boldsymbol{x} + \\lambda_k (A\\,\\boldsymbol{x} - b) + \\frac{\\rho}{2}(A\\,\\boldsymbol{x} - b)^2 \\right) = Q\\boldsymbol{x} + \\boldsymbol{c} + A^\\top\\lambda_k + \\rho A^\\top(A\\boldsymbol{x} - b) = \\boldsymbol{0}\n    $$\n    To solve for $\\boldsymbol{x}_{k+1}$, we rearrange the terms into a linear system of equations:\n    $$\n    (Q + \\rho A^\\top A)\\boldsymbol{x}_{k+1} = - \\boldsymbol{c} - A^\\top\\lambda_k + \\rho A^\\top b\n    $$\n    The matrix $(Q + \\rho A^\\top A)$ is symmetric and positive definite, which guarantees that this linear system can be reliably solved for a unique $\\boldsymbol{x}_{k+1}$.\n\n2.  **Dual Update (Multiplier Update):** After finding $\\boldsymbol{x}_{k+1}$, the Lagrange multiplier is updated according to the rule:\n    $$\n    \\lambda_{k+1} = \\lambda_k + \\rho(A\\boldsymbol{x}_{k+1} - b)\n    $$\n    This update rule is designed to drive the primal residual $(A\\boldsymbol{x} - b)$ to zero as the iterations proceed.\n\nThe iterative process starts with the given initial values $\\boldsymbol{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and $\\lambda_0 = 0$. At each iteration $k$, we assess convergence by evaluating the Karush–Kuhn–Tucker (KKT) residuals. The primal residual $r_p = \\lVert A\\,\\boldsymbol{x}_k - b \\rVert_2$ measures the feasibility of the current iterate, while the stationarity residual $r_s = \\lVert Q\\,\\boldsymbol{x}_k + \\boldsymbol{c} + A^\\top \\lambda_k \\rVert_2$ measures how close the iterate is to satisfying the stationarity condition of the Lagrangian.\n\nThe algorithm terminates when the condition $\\max\\{r_p, r_s\\} \\le \\varepsilon$ is met, where the tolerance is $\\varepsilon = 10^{-9}$. The number of outer iterations, $k$, required to achieve this is recorded. If convergence is not reached within $k_{\\max} = 10000$ iterations, the process is stopped, and the iteration count is reported as $k_{\\max}$.\n\nThis entire procedure is executed independently for each penalty parameter $\\rho$ in the provided test suite $\\boldsymbol{\\rho}_{\\text{test}} = [0.01, 0.1, 1.0, 10.0, 1000.0]$. For each $\\rho$, the algorithm is reset to the initial conditions $(\\boldsymbol{x}_0, \\lambda_0)$, and the number of iterations to convergence is calculated. The final output is a list of these iteration counts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the equality-constrained quadratic optimization problem using\n    the augmented Lagrangian method for a range of penalty parameters.\n    \"\"\"\n    # Define problem data\n    Q = np.array([[4., 1.], [1., 3.]])\n    c = np.array([[-1.], [-2.]])\n    A = np.array([[1., 2.]])\n    b = 1.0\n\n    # Initial conditions\n    x0 = np.array([[0.], [0.]])\n    lambda0 = 0.0\n\n    # Algorithm parameters\n    epsilon = 1e-9\n    k_max = 10000\n    rho_test = [0.01, 0.1, 1.0, 10.0, 1000.0]\n\n    # Pre-compute constant matrices\n    At = A.T\n    AtA = At @ A\n\n    def run_augmented_lagrangian(rho):\n        \"\"\"\n        Executes the augmented Lagrangian algorithm for a given penalty parameter rho.\n\n        Returns the number of outer iterations required for convergence.\n        \"\"\"\n        x = x0.copy()\n        lam = lambda0\n        k = 0\n\n        # Pre-compute the system matrix M which depends on rho\n        M = Q + rho * AtA\n        \n        while k <= k_max:\n            # 1. Check for convergence at the current iterate (x, lam)\n            # Primal residual\n            primal_residual_vec = A @ x - b\n            rp = np.linalg.norm(primal_residual_vec)\n            \n            # Stationarity residual\n            stationarity_residual_vec = Q @ x + c + At * lam\n            rs = np.linalg.norm(stationarity_residual_vec)\n\n            if max(rp, rs) <= epsilon:\n                return k\n\n            # If max iterations are reached, stop and return k_max\n            if k == k_max:\n                break\n            \n            # 2. Perform the update step\n            # Primal update (x-minimization)\n            d = -c - lam * At + rho * b * At\n            x_next = np.linalg.solve(M, d)\n            \n            # Dual update (lambda-update)\n            # The expression A @ x_next - b results in a 1x1 array\n            lam_next = lam + rho * (A @ x_next - b)[0, 0]\n\n            # Update iterates for the next loop\n            x = x_next\n            lam = lam_next\n            k += 1\n            \n        return k_max\n\n    results = []\n    for rho in rho_test:\n        iterations = run_augmented_lagrangian(rho)\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2380561"}]}