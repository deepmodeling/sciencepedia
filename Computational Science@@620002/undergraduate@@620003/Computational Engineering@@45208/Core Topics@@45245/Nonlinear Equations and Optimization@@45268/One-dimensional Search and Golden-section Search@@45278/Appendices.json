{"hands_on_practices": [{"introduction": "True understanding of an algorithm comes from the ability to build it from the ground up. This exercise guides you through deriving the famous golden-section search from first principles, focusing on the logic that makes it so efficient. You will then implement the algorithm to solve a common engineering problem: optimizing a costly \"black-box\" function whose internal workings are unknown and time-consuming to evaluate [@problem_id:2421111]. This practice bridges the gap between theory and application, cementing your understanding of not just how the algorithm works, but why it is designed that way.", "problem": "You are tasked with implementing a one-dimensional search algorithm to minimize a scalar function that is only accessible through a separate, delayed black-box simulation. The algorithm must be golden-section search, derived from first principles of unimodality and interval reduction logic. Your program must be fully self-contained and produce the requested outputs without any user interaction.\n\nThe foundational base on which your design must rely includes the following facts and definitions:\n- A function $f(x)$ is unimodal on an interval $[a,b]$ if there exists a point $x^{\\star} \\in [a,b]$ such that $f(x)$ decreases on $[a,x^{\\star}]$ and increases on $[x^{\\star},b]$.\n- A one-dimensional search method aims to reduce an interval of uncertainty $[a,b]$ that provably contains the minimizer $x^{\\star}$, using only function evaluations $f(x)$.\n- The golden-section search is a bracketed search that chooses two interior points $c$ and $d$ within $[a,b]$ according to a constant ratio so that one of the points can be reused in the next iteration, thereby minimizing the number of function evaluations required.\n\nYour tasks:\n1. Derive the choice of the constant ratio for placing two interior points $c$ and $d$ within $[a,b]$ such that after discarding one sub-interval based on a single comparison of $f(c)$ and $f(d)$, one of the two points is reused and the new interval keeps the same proportion as the previous one. Do not assume the ratio; derive it from the invariant that the reduction factor is constant and one interior point is reused at each iteration. Base your derivation on the unimodality property and interval reduction logic.\n2. Implement the golden-section search algorithm resulting from your derivation. Your implementation must:\n   - Work on any unimodal function $f(x)$ supplied through a callable interface.\n   - Use only function evaluations $f(x)$ (no derivatives).\n   - Reuse one function evaluation at each iteration by design.\n   - Terminate when the interval length $(b-a)$ is less than or equal to a tolerance $\\varepsilon$, or when a maximum number of evaluations is reached.\n   - Return the estimate $\\hat{x} = (a+b)/2$ as the final minimizer.\n3. Model the delayed black-box simulation as follows:\n   - The callable returns the exact value of $f(x)$ for a given $x$, but it performs a deterministic, computationally costly internal loop whose results do not alter $f(x)$. This models a delay without changing the mathematics of $f(x)$.\n   - Implement simple memoization so repeated calls at the exact same $x$ do not re-run the internal loop. This reflects the practical need to minimize black-box calls.\n4. Use the following test suite of unimodal functions and intervals. For each test, run your golden-section search with tolerance $\\varepsilon = 10^{-8}$ and a maximum of $N_{\\max} = 200$ function evaluations. For each test, return the final estimate $\\hat{x}$ rounded to $8$ decimal places:\n   - Test A (happy path): $f(x) = (x - 1.234567)^{2} + 3$ on $[a,b] = [-5,5]$. The minimizer is interior.\n   - Test B (asymmetric convex): $f(x) = \\exp(0.3\\,x) + \\exp(-0.7\\,x)$ on $[a,b] = [-2,4]$. The minimizer is interior and not centered.\n   - Test C (flat-bottom unimodal): $f(x) = \\max(|x| - 1, 0)$ on $[a,b] = [-3,3]$. The set of minimizers is the interval $[-1,1]$. Any point $\\hat{x}$ in this set is acceptable; your algorithm should return the midpoint of the final bracket as specified above.\n   - Test D (boundary minimizer): $f(x) = (x + 2)^{2}$ on $[a,b] = [-10,-2]$. The minimizer lies at the boundary.\n5. Final Output Format:\n   - Your program should produce a single line of output containing the four minimizer estimates as a comma-separated list enclosed in square brackets, in the order A, B, C, D, with each value rounded to $8$ decimal places, for example: \"[xA,xB,xC,xD]\".\n   - All outputs are real numbers (floats) with exactly $8$ digits after the decimal point.\n\nNotes:\n- Angles are not involved in this problem.\n- There are no physical units involved.\n- Your program must be complete and runnable as is, with no user input.", "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded in the established principles of numerical optimization, is well-posed with all necessary information provided, and is stated in an objective, formal language. The tasks are coherent and build upon one another, from theoretical derivation to practical implementation and testing. There are no contradictions, ambiguities, or factual inaccuracies. Thus, I will proceed with the solution.\n\nThe problem demands the derivation and implementation of the golden-section search algorithm for minimizing a unimodal function on a given interval. This will be addressed in three parts: first, the derivation of the characteristic ratio from first principles; second, the design of the algorithm based on this derivation; and third, the implementation and testing against the specified cases.\n\n**1. Derivation of the Golden-Section Ratio**\n\nLet $f(x)$ be a unimodal function on the interval $[a, b]$. Our goal is to find a smaller interval that is guaranteed to contain the minimizer $x^{\\star}$. The method relies on evaluating the function at two interior points, $c$ and $d$, where $a < c < d < b$.\n\nLet the initial interval be $[a_0, b_0]$ with length $L_0 = b_0 - a_0$. We choose the interior points $c_0$ and $d_0$ to be symmetrically placed with respect to a ratio $\\tau \\in (0, 1)$. Specifically, we define them relative to the interval's endpoints. A common and effective convention is to place them such that the segment $[c_0, d_0]$ is centered within $[a_0, b_0]$, and the ratio of the length of the larger segment to the full interval is $\\tau$.\nLet the points be:\n$c_0 = a_0 + (1-\\tau)(b_0-a_0)$\n$d_0 = a_0 + \\tau(b_0-a_0)$\nFor $c_0 < d_0$, we must have $1 - \\tau < \\tau$, which implies $1 < 2\\tau$ or $\\tau > 1/2$. The length of the new interval after one step will be $\\tau L_0$.\n\nWe evaluate $f(c_0)$ and $f(d_0)$ and use the property of unimodality to discard a portion of the interval.\n\nCase 1: $f(c_0) < f(d_0)$.\nDue to unimodality, the minimum cannot be in the sub-interval $(d_0, b_0]$. Therefore, the new interval of uncertainty is $[a_1, b_1] = [a_0, d_0]$. The length of this new interval is $L_1 = d_0 - a_0 = \\tau L_0$.\nThe key requirement of the golden-section search is that one of the interior points from the previous iteration can be reused in the current iteration. The new interval $[a_1, b_1]$ contains the old interior point $c_0$. We must arrange our choice of $\\tau$ such that $c_0$ becomes one of the new interior points, $c_1$ or $d_1$.\nThe new interior points for the interval $[a_1, b_1]$ are:\n$c_1 = a_1 + (1-\\tau)L_1 = a_0 + (1-\\tau)(\\tau L_0)$\n$d_1 = a_1 + \\tau L_1 = a_0 + \\tau(\\tau L_0) = a_0 + \\tau^2 L_0$\nWe want to set one of these equal to the old point $c_0 = a_0 + (1-\\tau)L_0$.\nLet's try setting $d_1 = c_0$:\n$a_0 + \\tau^2 L_0 = a_0 + (1-\\tau)L_0$\n$\\tau^2 = 1 - \\tau$\n$\\tau^2 + \\tau - 1 = 0$\n\nCase 2: $f(c_0) \\geq f(d_0)$.\nBy unimodality, the minimum must be in $[c_0, b_0]$. The new interval is $[a_1, b_1] = [c_0, b_0]$. The length of this new interval is $L_1 = b_0 - c_0 = b_0 - (a_0 + (1-\\tau)L_0) = (b_0-a_0) - (1-\\tau)L_0 = L_0 - (1-\\tau)L_0 = \\tau L_0$.\nThe new interval contains the old interior point $d_0$.\nThe new interior points for $[a_1, b_1]$ are:\n$c_1 = a_1 + (1-\\tau)L_1 = c_0 + (1-\\tau)(\\tau L_0) = (a_0 + (1-\\tau)L_0) + (1-\\tau)\\tau L_0$\n$d_1 = a_1 + \\tau L_1 = c_0 + \\tau(\\tau L_0) = (a_0 + (1-\\tau)L_0) + \\tau^2 L_0$\nWe must equate one of these to the old point $d_0 = a_0 + \\tau L_0$.\nLet's try setting $c_1 = d_0$:\n$(a_0 + (1-\\tau)L_0) + (1-\\tau)\\tau L_0 = a_0 + \\tau L_0$\n$1-\\tau + \\tau - \\tau^2 = \\tau$\n$1 - \\tau^2 = \\tau$\n$\\tau^2 + \\tau - 1 = 0$\n\nBoth cases lead to the same quadratic equation: $\\tau^2 + \\tau - 1 = 0$.\nSolving for $\\tau$ using the quadratic formula, $\\tau = \\frac{-1 \\pm \\sqrt{1^2 - 4(1)(-1)}}{2}$:\n$$ \\tau = \\frac{-1 \\pm \\sqrt{5}}{2} $$\nSince $\\tau$ represents a ratio of lengths, it must be positive. Thus, we take the positive root:\n$$ \\tau = \\frac{\\sqrt{5} - 1}{2} \\approx 0.6180339887... $$\nThis number is the reciprocal of the golden ratio, $\\phi = \\frac{1+\\sqrt{5}}{2}$. The ratio for the smaller segment is $1-\\tau = \\tau^2 = \\frac{3-\\sqrt{5}}{2}$. This choice of $\\tau$ ensures that in each iteration, the interval is reduced by a factor of $\\tau$, and one of the interior points from the previous step is reused, saving one function evaluation per iteration after the initial setup.\n\n**2. Algorithm Design and Implementation**\n\nBased on the derivation, the algorithm proceeds as follows:\n\nLet $\\rho = \\frac{\\sqrt{5}-1}{2}$.\n1.  **Initialization:**\n    Given an interval $[a, b]$, a function $f$, a tolerance $\\varepsilon$, and a maximum number of evaluations $N_{\\max}$.\n    Calculate the initial interior points:\n    $c = b - \\rho(b-a)$\n    $d = a + \\rho(b-a)$\n    Evaluate the function at these two points: $f_c = f(c)$ and $f_d = f(d)$.\n    Initialize an evaluation counter, `eval_count = 2`.\n\n2.  **Iteration:**\n    Loop while the interval length $(b-a) > \\varepsilon$ and `eval_count` $< N_{\\max}$:\n    If $f_c < f_d$:\n        The new interval is $[a, d]$. Set $b \\leftarrow d$.\n        The old point $c$ becomes the new point $d$. Set $d \\leftarrow c$.\n        The corresponding function value is also reused: $f_d \\leftarrow f_c$.\n        Calculate the new point $c$: $c \\leftarrow b - \\rho(b-a)$.\n        Evaluate the function only at the new point $c$: $f_c \\leftarrow f(c)$.\n    Else ($f_c \\geq f_d$):\n        The new interval is $[c, b]$. Set $a \\leftarrow c$.\n        The old point $d$ becomes the new point $c$. Set $c \\leftarrow d$.\n        The corresponding function value is reused: $f_c \\leftarrow f_d$.\n        Calculate the new point $d$: $d \\leftarrow a + \\rho(b-a)$.\n        Evaluate the function only at the new point $d$: $f_d \\leftarrow f(d)$.\n    Increment `eval_count` by $1$.\n\n3.  **Termination:**\n    When the loop terminates, the minimizer is contained within the final interval $[a, b]$. The estimate for the minimizer is the midpoint of this interval: $\\hat{x} = (a+b)/2$.\n\n**3. Black-Box Simulation and Testing**\nA Python class will be used to encapsulate the black-box function. This class will implement memoization using a dictionary to store previously computed values, avoiding re-computation for identical inputs. A deterministic delay will simulate a costly evaluation. The derived algorithm will then be applied to the four test cases specified in the problem statement, using this black-box model.\n\nThe Python implementation in the `<final_answer>` section will strictly follow this design to compute the minimizers for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport time\n\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy, version: 1.23.5\n#     - name: scipy, version: 1.11.4\n\nclass BlackBoxFunction:\n    \"\"\"\n    A wrapper for a scalar function to simulate a delayed black-box evaluation with memoization.\n    - Simulates computational cost with a deterministic internal loop.\n    - Uses memoization to cache results and avoid re-running the expensive evaluation for the same input.\n    \"\"\"\n    def __init__(self, func):\n        self.func = func\n        self.memo = {}\n        self.evals = 0\n\n    def __call__(self, x):\n        \"\"\"\n        Evaluates the function at x, using cache if available.\n        \"\"\"\n        # Use a tuple representation of x as a key for hashability, handling potential float precision issues by rounding.\n        # For this problem, exact reuse is expected, so no rounding is necessary\n        x_key = x \n        if x_key in self.memo:\n            return self.memo[x_key]\n        else:\n            self.evals += 1\n            # Simulate a costly computation; this loop's result does not affect f(x)\n            # A simple busy-wait loop.\n            _ = sum(i for i in range(1000))\n            result = self.func(x)\n            self.memo[x_key] = result\n            return result\n\ndef golden_section_search(f_wrapped, a, b, tol, max_evals):\n    \"\"\"\n    Performs a golden-section search to find the minimum of a unimodal function.\n\n    Args:\n        f_wrapped: The BlackBoxFunction object to minimize.\n        a: The lower bound of the initial interval.\n        b: The upper bound of the initial interval.\n        tol: The tolerance for the interval length.\n        max_evals: The maximum number of function evaluations allowed.\n\n    Returns:\n        The estimated minimizer x_hat.\n    \"\"\"\n    rho = (np.sqrt(5) - 1) / 2  # Golden ratio conjugate, approx 0.618\n\n    # Initialize the two interior points\n    c = b - rho * (b - a)\n    d = a + rho * (b - a)\n\n    # Perform initial function evaluations\n    fc = f_wrapped(c)\n    fd = f_wrapped(d)\n\n    while (b - a) > tol and f_wrapped.evals < max_evals:\n        if fc < fd:\n            # The new interval is [a, d]\n            b = d\n            # The old c becomes the new d\n            d = c\n            fd = fc # Reuse function value\n            # Calculate the new c and its function value\n            c = b - rho * (b - a)\n            fc = f_wrapped(c)\n        else: # fc >= fd\n            # The new interval is [c, b]\n            a = c\n            # The old d becomes the new c\n            c = d\n            fc = fd # Reuse function value\n            # Calculate the new d and its function value\n            d = a + rho * (b - a)\n            fd = f_wrapped(d)\n    \n    # Return the midpoint of the final interval\n    return (a + b) / 2\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the golden-section search algorithm.\n    \"\"\"\n    # Define test functions\n    f_A = lambda x: (x - 1.234567)**2 + 3\n    f_B = lambda x: np.exp(0.3 * x) + np.exp(-0.7 * x)\n    f_C = lambda x: np.maximum(np.abs(x) - 1, 0)\n    f_D = lambda x: (x + 2)**2\n\n    test_cases = [\n        {\"func\": f_A, \"interval\": [-5.0, 5.0], \"name\": \"A\"},\n        {\"func\": f_B, \"interval\": [-2.0, 4.0], \"name\": \"B\"},\n        {\"func\": f_C, \"interval\": [-3.0, 3.0], \"name\": \"C\"},\n        {\"func\": f_D, \"interval\": [-10.0, -2.0], \"name\": \"D\"},\n    ]\n\n    results = []\n    \n    # Parameters from the problem statement\n    epsilon = 1e-8\n    N_max = 200\n\n    for case in test_cases:\n        f_callable = BlackBoxFunction(case[\"func\"])\n        a, b = case[\"interval\"]\n        \n        minimizer = golden_section_search(f_callable, a, b, tol=epsilon, max_evals=N_max)\n        \n        # Round to 8 decimal places as required for the output format\n        results.append(f\"{minimizer:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2421111"}, {"introduction": "A powerful tool is only effective when its limitations are understood. Golden-section search guarantees finding the global minimum only if the function is unimodal on the search interval. This practice [@problem_id:2421149] is a critical thinking exercise that explores what happens when this crucial prerequisite is not met. By analyzing the attempt to use this method for root-finding, you will discover how seemingly reasonable problem transformations can create \"spurious\" local minima, which can trap the search algorithm and lead to incorrect solutions.", "problem": "An engineer in computational engineering needs to solve the scalar equation $g(x)=c$ over a closed interval $[a,b]$. To do so, the engineer proposes to minimize the squared residual $f(x)=(g(x)-c)^2$ on $[a,b]$ using Golden-Section Search (GSS). Assume $g$ is continuous on $[a,b]$ and at least once differentiable on $(a,b)$. Select all statements that are correct about the validity of this approach and its pitfalls related to local minima.\n\nA. If $g$ is continuous and strictly monotone on $[a,b]$ and $c\\in g([a,b])$, then $f(x)$ is unimodal on $[a,b]$ with a unique global minimizer at the unique solution of $g(x)=c$, so applying GSS to $f$ on any bracket $[a,b]$ that contains that solution will return a root.\n\nB. The only stationary points of $f(x)$ are the solutions of $g(x)=c$.\n\nC. If $g$ is non-monotone on $[a,b]$, then $f(x)$ can have local minima at points where $g'(x)=0$ and $g(x)\\neq c$, so GSS run on $f$ over such a bracket can converge to a non-root local minimizer depending on the initial bracket.\n\nD. If $c\\notin g([a,b])$, then minimizing $f$ with GSS on $[a,b]$ will not produce a root of $g(x)=c$; instead, it will converge to a point in $[a,b]$ (possibly an endpoint) with nonzero residual.\n\nE. Squaring the residual makes $f(x)$ convex on $[a,b]$ for any differentiable $g$, so no spurious local minima exist and GSS will always find a root if one exists anywhere on the real line.", "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n- Equation to solve: $g(x) = c$\n- Domain: Closed interval $[a, b]$\n- Proposed method: Minimize the function $f(x) = (g(x) - c)^2$\n- Optimization algorithm: Golden-Section Search (GSS)\n- Properties of $g(x)$:\n    - Continuous on $[a, b]$\n    - At least once differentiable on $(a, b)$\n- Task: Evaluate statements about the validity and pitfalls of this approach.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard technique in numerical analysis: reformulating a root-finding problem as a minimization problem. The functions, concepts (continuity, differentiability, monotonicity, unimodality), and algorithm (GSS) are well-defined within mathematics and computational engineering. The approach is scientifically valid.\n- **Well-Posed:** The problem provides sufficient information to analyze the mathematical properties of the function $f(x)$ based on the given properties of $g(x)$. The questions posed in the options are precise and can be answered with mathematical rigor.\n- **Objective:** The problem and the statements to be evaluated are objective and devoid of subjective claims. Their correctness can be determined through mathematical proof or counterexample.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n**Derivation of Core Principles**\n\nThe problem proposes to find a root of $g(x) = c$ by minimizing the squared residual function $f(x) = (g(x) - c)^2$. The global minima of $f(x)$ occur where $f(x) = 0$, which is true if and only if $g(x) = c$. Therefore, the global minimizers of $f(x)$ are precisely the roots of $g(x) = c$.\n\nGolden-Section Search (GSS) is an algorithm guaranteed to find the minimum of a function over an interval only if the function is **unimodal** on that interval. A function is unimodal on an interval if it has exactly one local minimum in that interval. If a function has multiple local minima (i.e., is not unimodal), GSS may converge to a local minimum that is not the global minimum, depending on the initial search bracket.\n\nTo understand the behavior of GSS on $f(x)$, we must analyze the stationary points of $f(x)$, which determine its local minima and maxima. Using the chain rule, the first derivative of $f(x)$ is:\n$$ f'(x) = 2(g(x) - c) \\cdot g'(x) $$\nThe stationary points of $f(x)$ are the values of $x$ for which $f'(x) = 0$. This occurs if and only if:\n$$ g(x) - c = 0 \\quad \\text{or} \\quad g'(x) = 0 $$\nThis shows that the stationary points of $f(x)$ consist of two sets:\n$1$. The roots of $g(x) = c$.\n$2$. The stationary points of $g(x)$ itself.\n\nThese latter points, where $g'(x)=0$ but $g(x) \\neq c$, can introduce \"spurious\" local minima in $f(x)$, which are not roots of the original equation.\n\nTo determine if a stationary point $x_0$ is a local minimum, we can examine the second derivative, $f''(x_0)$:\n$$ f''(x) = 2(g'(x))^2 + 2(g(x) - c)g''(x) $$\nAt a stationary point $x_0$ where $g'(x_0) = 0$ and $g(x_0) \\neq c$, the second derivative simplifies to:\n$$ f''(x_0) = 2(g(x_0) - c)g''(x_0) $$\nFor $x_0$ to be a local minimum of $f$, we require $f''(x_0) > 0$. This condition can be met, as shown in the analysis of option C.\n\n**Option-by-Option Analysis**\n\n**A. If $g$ is continuous and strictly monotone on $[a,b]$ and $c\\in g([a,b])$, then $f(x)$ is unimodal on $[a,b]$ with a unique global minimizer at the unique solution of $g(x)=c$, so applying GSS to $f$ on any bracket $[a,b]$ that contains that solution will return a root.**\n\nIf $g(x)$ is continuous and strictly monotone on $[a,b]$, and $c$ is in the range of $g$ on this interval, the Intermediate Value Theorem guarantees there is a unique solution $x^*$ in $[a,b]$ such that $g(x^*) = c$. At this point, $f(x^*) = (g(x^*) - c)^2 = 0$. Since $f(x) \\ge 0$ for all $x$, $x^*$ is a global minimizer.\n\nBecause $g(x)$ is strictly monotone, $g'(x)$ does not change sign on $(a, b)$ and is non-zero (except possibly at isolated points that do not constitute local extrema of $g$). The only way for $f'(x) = 2(g(x) - c)g'(x)$ to be zero is if $g(x) - c = 0$, which happens only at $x = x^*$. Thus, $f(x)$ has only one stationary point in the interval.\nLet's assume $g(x)$ is strictly increasing, so $g'(x) > 0$.\n- For $x < x^*$, $g(x) < g(x^*) = c$, so $g(x) - c < 0$. Thus, $f'(x) = 2(\\text{negative})(\\text{positive}) < 0$.\n- For $x > x^*$, $g(x) > g(x^*) = c$, so $g(x) - c > 0$. Thus, $f'(x) = 2(\\text{positive})(\\text{positive}) > 0$.\nThis shows $f(x)$ is decreasing for $x < x^*$ and increasing for $x > x^*$. Therefore, $f(x)$ is unimodal on $[a,b]$ with its unique minimum at $x^*$. GSS is designed for unimodal functions and will correctly converge to this unique minimizer.\nVerdict: **Correct**.\n\n**B. The only stationary points of $f(x)$ are the solutions of $g(x)=c$.**\n\nAs derived above, the stationary points of $f(x)$ occur when $f'(x) = 2(g(x) - c) g'(x) = 0$. This equation is satisfied when $g(x) = c$ OR when $g'(x) = 0$. If there exists a point $x_0$ where $g(x)$ has a local extremum (so $g'(x_0) = 0$) and $g(x_0) \\neq c$, then $x_0$ is a stationary point of $f(x)$ but not a solution to $g(x) = c$. For example, let $g(x) = x^2$ and $c=4$. The solutions are $x=\\pm 2$. The function to minimize is $f(x) = (x^2 - 4)^2$. The derivative is $f'(x) = 2(x^2 - 4)(2x) = 4x(x-2)(x+2)$. The stationary points are $x=0$, $x=2$, and $x=-2$. The point $x=0$ is a stationary point of $f(x)$ because $g'(0)=0$, but $g(0) = 0 \\neq 4$, so it is not a solution.\nVerdict: **Incorrect**.\n\n**C. If $g$ is non-monotone on $[a,b]$, then $f(x)$ can have local minima at points where $g'(x)=0$ and $g(x)\\neq c$, so GSS run on $f$ over such a bracket can converge to a non-root local minimizer depending on the initial bracket.**\n\nIf $g(x)$ is non-monotone, there must be at least one point $x_0 \\in (a,b)$ where $g'(x_0)=0$. This point is a stationary point of $f(x)$. We must check if it can be a local minimum. As shown in the general derivation, $f''(x_0) = 2(g(x_0) - c)g''(x_0)$. We can make this positive.\nConsider $g(x) = x^3 - 4x$. Then $g'(x) = 3x^2 - 4$, which is zero at $x_0 = \\pm 2/\\sqrt{3}$. Let's pick $x_0 = 2/\\sqrt{3}$. At this point, $g(x)$ a local minimum: $g(2/\\sqrt{3}) = (8/3\\sqrt{3}) - (8/\\sqrt{3}) = -16/(3\\sqrt{3})$ and $g''(x) = 6x$, so $g''(2/\\sqrt{3}) = 12/\\sqrt{3} > 0$.\nLet's find a root for $c < g(x_0)$. For example, let $c = -6$. The equation is $x^3-4x = -6$. The function to minimize is $f(x)=(x^3-4x+6)^2$. A real root $x^*$ exists (e.g., $g(-3)=-15$, $g(-2)=0$, so root is between $-3$ and $-2$).\nAt the stationary point $x_0=2/\\sqrt{3}$ of $g(x)$, we have $g(x_0) = -16/(3\\sqrt{3}) \\approx -3.078$, which is not equal to $c=-6$. The second derivative of $f(x)$ at $x_0$ is $f''(x_0) = 2(g(x_0)-c)g''(x_0) = 2(-16/(3\\sqrt{3}) - (-6)) (12/\\sqrt{3}) = 2(-3.078+6)(6.928) > 0$.\nSo, $x_0 = 2/\\sqrt{3}$ is a local minimum of $f(x)$. Since $f(x_0) = (g(x_0)-c)^2 > 0$ while the global minimum is $f(x^*)=0$, $f(x)$ is not unimodal. GSS started on an interval containing $x_0$ but not $x^*$ (e.g., $[0,2]$) could converge to the local minimizer $x_0$, which is not a root of $g(x)=c$.\nVerdict: **Correct**.\n\n**D. If $c\\notin g([a,b])$, then minimizing $f$ with GSS on $[a,b]$ will not produce a root of $g(x)=c$; instead, it will converge to a point in $[a,b]$ (possibly an endpoint) with nonzero residual.**\n\nThe premise is that $c$ is not in the range of $g(x)$ on the interval $[a,b]$. This means there is no $x \\in [a,b]$ for which $g(x) = c$. Thus, no root exists in the interval. The function $f(x) = (g(x) - c)^2$ is therefore strictly positive for all $x \\in [a,b]$. As a continuous function on a compact set $[a,b]$, $f(x)$ must attain a global minimum value on this interval, and this minimum value will be greater than zero. The point $x_{min}$ where this minimum occurs represents the best \"least-squares\" approximation to a solution in $[a,b]$.\nSince no root exists, GSS cannot produce a root. What it will do is search for a minimum of $f(x)$. GSS iteratively narrows the search interval. Assuming it converges (which it will, as the interval shrinks), it will converge to a point corresponding to a minimum of $f(x)$ within the initial bracket. Since the minimum value of $f(x)$ is strictly positive, the point found will have a non-zero residual, i.e., $f(x_{min}) > 0$. Such a minimum can occur at an endpoint if, for example, $f(x)$ is monotonic on $[a,b]$. The statement accurately describes that the outcome is a least-squares solution, not a root.\nVerdict: **Correct**.\n\n**E. Squaring the residual makes $f(x)$ convex on $[a,b]$ for any differentiable $g$, so no spurious local minima exist and GSS will always find a root if one exists anywhere on the real line.**\n\nThis statement makes several incorrect claims.\n$1$. **Convexity**: $f(x)$ is not generally convex. As shown above, $f''(x) = 2(g'(x))^2 + 2(g(x) - c)g''(x)$. The second term, $2(g(x) - c)g''(x)$, can be negative and large enough to make $f''(x) < 0$. For a counterexample, let $g(x) = \\sin(x)$ and $c=0$. Then $f(x) = \\sin^2(x)$. $f''(x) = 2\\cos(2x)$, which is negative for $x \\in (\\pi/4, 3\\pi/4)$, so $f(x)$ is not convex on this interval.\n$2$. **No spurious local minima**: As demonstrated for option C, spurious (non-root) local minima can and do exist when $g(x)$ is non-monotone. Convexity is a sufficient (but not necessary) condition for unimodality. Since $f(x)$ is not always convex, the claim that no spurious minima exist is false.\n$3$. **Always find a root anywhere**: GSS is a local search method restricted to its initial search bracket $[a, b]$. It has no mechanism to find roots that lie outside this bracket.\nEach part of this statement is false.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "2421149"}, {"introduction": "The principles of golden-section search are more general than they first appear. While typically applied to a simple linear interval $[a, b]$, the core logic can be extended to more complex domains. This advanced practice [@problem_id:2421115] challenges you to adapt the algorithm for a function defined on a circular or periodic domain, such as finding an optimal angle. You will need to rethink fundamental concepts like \"interval\" and \"distance\" to correctly handle the wrap-around nature of the circle, fostering a deeper and more flexible command of algorithmic design.", "problem": "You are given a continuous, $2\\pi$-periodic function $f:\\mathbb{R}\\to\\mathbb{R}$ that is strictly unimodal on the unit circle: there exists a unique minimizer $\\theta^{\\star}\\in[0,2\\pi)$ modulo $2\\pi$, and on any geodesic (shortest) arc of angular length less than $\\pi$ that contains $\\theta^{\\star}$, the function $f$ decreases strictly up to $\\theta^{\\star}$ and then increases strictly. Angles are identified modulo $2\\pi$. Define the counterclockwise oriented arc from $L$ to $R$ by its angular span $\\Delta = (R-L)\\bmod 2\\pi \\in (0,2\\pi)$, and let the geodesic arc be the shorter of the two arcs connecting $L$ and $R$, so that a bracketing arc must satisfy $\\Delta\\in(0,\\pi)$.\n\nYou are asked to modify the golden-section search to minimize $f$ over the circle. A correct modification must specify how to initialize a bracket on the circle, how to select interior evaluation points each iteration, how to update the bracket while preserving correctness, how to handle wrap-around, and how to terminate.\n\nWhich option below correctly and sufficiently adapts the golden-section search to this circular setting?\n\nA. Maintain a bracketing geodesic arc $(L,R)$ with angular span $\\Delta\\in(0,\\pi)$ containing $\\theta^{\\star}$ such that $f$ is unimodal on that arc. At each iteration, choose two interior points along the counterclockwise direction by $c \\equiv L + (1-\\varphi)\\Delta \\pmod{2\\pi}$ and $d \\equiv L + \\varphi \\Delta \\pmod{2\\pi}$, where $\\varphi=(\\sqrt{5}-1)/2$. Evaluate $f(c)$ and $f(d)$. If $f(c) < f(d)$, set $R := d$; otherwise set $L := c$. Always interpret differences and updates modulo $2\\pi$ and keep the maintained arc as the geodesic one with span strictly less than $\\pi$. Terminate when $\\Delta \\le \\varepsilon$ for a prescribed tolerance $\\varepsilon>0$, and return any representative of the current arc (e.g., the midpoint along the arc) as the estimate of $\\theta^{\\star}$.\n\nB. Treat the circle as the fixed interval $[0,2\\pi]$ and apply the classical golden-section search on that interval with $a=0$, $b=2\\pi$, computing $c=b-\\varphi(b-a)$ and $d=a+\\varphi(b-a)$, ignoring wrap-around and without any restriction on the interval length relative to $\\pi$. Terminate when $b-a\\le\\varepsilon$.\n\nC. Initialize with any two points $L$ and $R$ whose counterclockwise separation satisfies $\\Delta\\in(\\pi,2\\pi)$ to avoid missing the minimizer. At each iteration, choose two interior points using the same golden ratios as on a line, but always measure along the longer arc. Update by keeping the longer subarc, because it is more likely to contain the minimizer on a circle. Terminate when the arc is shorter than a prescribed tolerance.\n\nD. Lift the circle to the real line by the tangent half-angle substitution $x=\\tan(\\theta/2)$ so that $\\theta=2\\arctan(x)$ maps $(-\\infty,\\infty)$ to $[0,2\\pi)$. Minimize $g(x)=f(2\\arctan(x))$ on $(-\\infty,\\infty)$ with the classical golden-section search, and map the minimizer back to the circle by $\\theta=2\\arctan(x_{\\min})$. No special treatment of wrap-around or arc length is needed on the circle.\n\nE. Maintain a bracketing geodesic arc $(L,R)$ with span $\\Delta\\in(0,\\pi)$ and, at each iteration, evaluate $f$ at two equally spaced interior points $c\\equiv L+\\Delta/3$ and $d\\equiv L+2\\Delta/3$ (all modulo $2\\pi$). Update by keeping the subarc with the lower interior value, and terminate when $\\Delta\\le\\varepsilon$.\n\nSelect the correct option.", "solution": "We begin from first principles. The golden-section search on a line assumes a continuous unimodal function on an interval and maintains a bracketing interval $[a,b]$ on which the function is unimodal. It chooses two interior points $c=b-\\varphi(b-a)$ and $d=a+\\varphi(b-a)$ with $\\varphi=(\\sqrt{5}-1)/2$, evaluates $f(c)$ and $f(d)$, and discards the half-interval that cannot contain the minimizer, preserving unimodality on the remaining interval. This relies on measuring distances along a single linear coordinate and on the existence of an interval on which there is a unique minimum and strictly monotone behavior on either side.\n\nOn a circle, angles are equivalent modulo $2\\pi$, and there is no global linear order. To adapt golden-section search, we must define a notion of an interval: the geodesic (shortest) arc between two angles. For two angles $L$ and $R$, their counterclockwise angular difference is $\\Delta = (R-L)\\bmod 2\\pi \\in (0,2\\pi)$. The geodesic arc is the shorter of the two arcs joining $L$ and $R$, which corresponds to requiring $\\Delta\\in(0,\\pi)$. Under the stated unimodality assumption, any geodesic arc of span less than $\\pi$ containing $\\theta^{\\star}$ will exhibit the same unimodality structure as in the linear case: strictly decreasing up to $\\theta^{\\star}$, then strictly increasing. Therefore, the correct adaptation is to maintain such a geodesic bracketing arc, choose interior points by fixed proportions of the arc length (as in the linear golden ratios), evaluate $f$ at those angles modulo $2\\pi$, and update the arc using the same logic as on the line, always preserving the geodesic property and handling wrap-around with modular arithmetic. Termination is based on the arc length $\\Delta$ being below a tolerance $\\varepsilon>0$.\n\nNow we evaluate each option.\n\nOption A: This option specifies maintaining a bracketing geodesic arc $(L,R)$ with span $\\Delta\\in(0,\\pi)$ on which $f$ is unimodal, selecting interior points $c$ and $d$ at golden-section positions along the counterclockwise direction: $c \\equiv L + (1-\\varphi)\\Delta \\pmod{2\\pi}$ and $d \\equiv L + \\varphi \\Delta \\pmod{2\\pi}$, with $\\varphi=(\\sqrt{5}-1)/2$. It compares $f(c)$ and $f(d)$; if $f(c) < f(d)$, the minimizer must lie in the geodesic subarc from $L$ to $d$, so $R$ is set to $d$; otherwise the minimizer lies in the subarc from $c$ to $R$, so $L$ is set to $c$. These updates mirror the line case and preserve that the new arc is geodesic with span less than $\\pi$, because each subarc has span $\\varphi\\Delta$ or $(1-\\varphi)\\Delta$, both strictly less than $\\Delta<\\pi$. The use of modulo $2\\pi$ handles wrap-around consistently. Termination when $\\Delta \\le \\varepsilon$ provides a standard accuracy guarantee in angular distance. This is a correct and sufficient adaptation of golden-section search to the circle.\n\nVerdict: Correct.\n\nOption B: This treats the domain as the fixed interval $[0,2\\pi]$ and applies the linear algorithm without accounting for circular identification. On $[0,2\\pi]$, a unimodal function on the circle need not be unimodal across the artificial cut at $0$ and $2\\pi$. If $\\theta^{\\star}$ is near $0$, then both neighborhoods near $0$ and near $2\\pi$ contain values close to the minimum, and the function on the interval may appear to have two “descending” sides, violating the single-peaked structure required by the algorithm. Starting with $a=0$, $b=2\\pi$ does not establish a bracketing interval in the sense needed for correctness, and the method can converge to a boundary or exhibit non-monotone progress. Without ensuring a geodesic arc of span less than $\\pi$ that contains the minimizer, the fundamental unimodality assumption is not guaranteed, so correctness is not ensured.\n\nVerdict: Incorrect.\n\nOption C: This deliberately uses the longer arc with span $\\Delta\\in(\\pi,2\\pi)$ and continues to keep the longer subarc. A longer arc on the circle necessarily contains both sides around the minimizer and, by construction, fails the geodesic requirement; on such arcs, the function cannot be guaranteed to be unimodal. The golden-section logic relies on discarding the half that cannot contain the minimizer under unimodality; keeping the longer subarc destroys the bracketing property and can retain regions that do not satisfy unimodality, breaking the convergence guarantee.\n\nVerdict: Incorrect.\n\nOption D: The tangent half-angle substitution $x=\\tan(\\theta/2)$ introduces singularities at $\\theta=\\pi$ (i.e., $x$ diverges) and maps the circle to the line non-uniformly. The composition $g(x)=f(2\\arctan(x))$ will be continuous but generally not unimodal on $(-\\infty,\\infty)$ even if $f$ is unimodal on geodesic arcs on the circle, because the mapping distorts distances and splits neighborhoods across the singularity. Without a guaranteed unimodality of $g$ on an interval, the golden-section search assumptions are violated, so correctness is not ensured.\n\nVerdict: Incorrect.\n\nOption E: This maintains a geodesic bracket and uses two equally spaced interior points, i.e., a ternary-search-like scheme. While such a method can converge under unimodality, it is not the golden-section search and does not reuse interior points to minimize function evaluations. The prompt requires a modification of the golden-section search algorithm, which specifically uses the golden ratio positions to preserve one interior point between iterations. This option does not implement the golden-section mechanism and therefore does not answer the question as posed.\n\nVerdict: Incorrect.\n\nTherefore, the only correct adaptation among the options is the one described in Option A.", "answer": "$$\\boxed{A}$$", "id": "2421115"}]}