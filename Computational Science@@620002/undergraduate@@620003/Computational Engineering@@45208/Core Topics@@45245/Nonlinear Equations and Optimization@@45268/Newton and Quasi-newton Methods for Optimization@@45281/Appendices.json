{"hands_on_practices": [{"introduction": "Newton's method is a cornerstone of optimization, celebrated for its rapid convergence. However, its raw, unmodified form can exhibit counterintuitive behavior on non-convex functions. This exercise provides a sharp, hands-on demonstration of this phenomenon, showing how a full Newton step can paradoxically increase the objective function value even as it moves closer to the stationary point [@problem_id:2417369]. Understanding this behavior is the first step toward appreciating the necessity of globalization strategies, such as line searches and trust regions, which ensure robust convergence.", "problem": "Let $f:\\mathbb{R}^2 \\to \\mathbb{R}$ be defined by $f(x_1,x_2)=\\tfrac{1}{2}x_1^2-\\tfrac{1}{2}x_2^2$. Starting from $x_0=(0,1)$, generate the Newton iterates for finding a stationary point using the full-step update $x_{k+1}=x_k-\\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$. Demonstrate that the Euclidean norm of the gradient $\\|\\nabla f(x_k)\\|_2$ decreases monotonically along the produced iterates, while the function value $f(x_k)$ increases at the first iteration, that is, $f(x_1)>f(x_0)$. Compute the exact value of $\\Delta f=f(x_1)-f(x_0)$. Provide your answer as an exact value with no rounding.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- Objective function: $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $f(x_1,x_2)=\\tfrac{1}{2}x_1^2-\\tfrac{1}{2}x_2^2$.\n- Initial point: $x_0=(0,1)$.\n- Iteration rule: Full-step Newton's method, $x_{k+1}=x_k-\\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$.\n- Task 1: Demonstrate that the Euclidean norm of the gradient, $\\|\\nabla f(x_k)\\|_2$, decreases monotonically.\n- Task 2: Demonstrate that $f(x_1)>f(x_0)$.\n- Task 3: Compute the exact value of $\\Delta f=f(x_1)-f(x_0)$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard exercise in numerical optimization, specifically concerning Newton's method. It is well-posed, providing all necessary information: the function, starting point, and iterative method are clearly defined. The function is infinitely differentiable, and its Hessian is constant and invertible, ensuring the Newton step is always well-defined. The problem is objective and contains no ambiguities. The tasks are to demonstrate specific, verifiable properties of the resulting iterates and to compute a precise quantity. The problem is a valid and instructive example of Newton's method applied to a non-convex quadratic function.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective function is given by $f(x_1, x_2) = \\frac{1}{2}x_1^2 - \\frac{1}{2}x_2^2$. We seek a stationary point using Newton's method. A stationary point $x^*$ satisfies $\\nabla f(x^*) = 0$.\n\nFirst, we compute the gradient vector $\\nabla f(x)$ and the Hessian matrix $\\nabla^2 f(x)$ for a general point $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\nThe gradient is:\n$$\n\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ -x_2 \\end{pmatrix}\n$$\nThe Hessian matrix is:\n$$\n\\nabla^2 f(x) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n$$\nThe Hessian matrix is constant for all $x \\in \\mathbb{R}^2$. The eigenvalues of $\\nabla^2 f$ are $\\lambda_1 = 1$ and $\\lambda_2 = -1$. Since one eigenvalue is positive and one is negative, the Hessian is indefinite. This implies that the stationary point of the function is a saddle point.\nThe inverse of the Hessian is:\n$$\n\\left[\\nabla^2 f(x)\\right]^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n$$\nThe Hessian is its own inverse.\n\nThe Newton update rule is $x_{k+1} = x_k - \\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$. For our specific function, this becomes:\n$$\nx_{k+1} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} x_{1,k} \\\\ -x_{2,k} \\end{pmatrix} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis shows that for any starting point $x_k \\neq (0,0)$, Newton's method converges to the stationary point $(0,0)$ in a single iteration.\n\nWe start from $x_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe first iterate, $x_1$, is:\n$$\nx_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nAll subsequent iterates will remain at the stationary point: $x_k = (0,0)$ for all $k \\ge 1$.\n\nNow, we address the tasks specified in the problem.\n\nTask 1: Demonstrate that $\\|\\nabla f(x_k)\\|_2$ decreases monotonically.\nWe compute the gradient at each iterate.\nFor $k=0$:\n$$\n\\nabla f(x_0) = \\nabla f(0,1) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|\\nabla f(x_0)\\|_2 = \\sqrt{0^2 + (-1)^2} = 1$.\nFor $k=1$:\n$$\n\\nabla f(x_1) = \\nabla f(0,0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|\\nabla f(x_1)\\|_2 = \\sqrt{0^2 + 0^2} = 0$.\nFor all $k > 1$, since $x_k = (0,0)$, we have $\\nabla f(x_k) = (0,0)$ and $\\|\\nabla f(x_k)\\|_2 = 0$.\nThe sequence of the norms of the gradients is $1, 0, 0, 0, \\ldots$. This is a monotonically non-increasing (and therefore, monotonically decreasing) sequence, as $1 > 0$ and $0 \\ge 0$. The condition is satisfied.\n\nTask 2: Demonstrate that $f(x_1) > f(x_0)$.\nWe compute the function value at $x_0$ and $x_1$.\nAt the starting point $x_0 = (0,1)$:\n$$\nf(x_0) = f(0,1) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(1)^2 = -\\frac{1}{2}\n$$\nAt the first iterate $x_1 = (0,0)$:\n$$\nf(x_1) = f(0,0) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(0)^2 = 0\n$$\nComparing the two values, we have $f(x_1) = 0$ and $f(x_0) = -1/2$. Indeed, $0 > -1/2$, which demonstrates that $f(x_1) > f(x_0)$. The function value increases. This occurs because the Hessian is not positive definite, and thus the Newton direction is not guaranteed to be a descent direction. The search direction is $p_0 = x_1 - x_0 = (0,0) - (0,1) = (0,-1)$. The directional derivative along $p_0$ from $x_0$ is $\\nabla f(x_0)^T p_0 = \\begin{pmatrix} 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = 1 > 0$, confirming that $p_0$ is an ascent direction.\n\nTask 3: Compute the exact value of $\\Delta f = f(x_1) - f(x_0)$.\nUsing the values calculated in the previous step:\n$$\n\\Delta f = f(x_1) - f(x_0) = 0 - \\left(-\\frac{1}{2}\\right) = \\frac{1}{2}\n$$\nThe exact value of the change in the function value after the first iteration is $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2417369"}, {"introduction": "Building on the insight that pure Newton's method requires safeguards, this practice guides you through implementing and comparing two workhorse algorithms: a modified Newton's method and the BFGS quasi-Newton method. You will apply these methods to the notoriously difficult Rosenbrock function, a classic benchmark for optimization routines [@problem_id:2417339]. This hands-on implementation will not only solidify your understanding of line-search methods but also illuminate the critical and nuanced impact of choosing different termination criteria on algorithm performance and reliability.", "problem": "You are to study the impact of alternative termination criteria on an ill-conditioned optimization problem by constructing and analyzing sequences generated by a second-order search direction and a quasi-second-order search direction. Consider the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by\n$$\nf(x) \\equiv f\\left(\\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix}\\right) = 100\\left(x_{2}-x_{1}^{2}\\right)^{2} + \\left(1-x_{1}\\right)^{2},\n$$\nwhich is known to have a narrow curved valley and a unique minimizer at $x^{\\star}=\\begin{bmatrix}1\\\\1\\end{bmatrix}$.\n\nDefine an iterative process from an initial point $x_{0}\\in\\mathbb{R}^{2}$ as follows. At iteration $k$, compute a search direction $p_{k}$ by solving\n$$\nB_{k}p_{k}=-\\nabla f(x_{k}),\n$$\nand then take a step\n$$\nx_{k+1}=x_{k}+\\alpha_{k}p_{k},\n$$\nwhere $\\alpha_{k}$ is chosen by an Armijo backtracking line-search with parameters $c_{1}=10^{-4}$ and contraction factor $\\beta=\\tfrac{1}{2}$, starting from $\\alpha_{k}=1$ and reducing $\\alpha_{k}\\leftarrow \\beta \\alpha_{k}$ until\n$$\nf(x_{k}+\\alpha_{k}p_{k}) \\le f(x_{k}) + c_{1}\\alpha_{k}\\nabla f(x_{k})^{\\top}p_{k},\n$$\nor until $\\alpha_{k}<10^{-16}$, in which case the current $\\alpha_{k}$ is accepted. For the matrix $B_{k}$, consider the following two cases:\n- Case H (exact second order): $B_{k}=\\nabla^{2} f(x_{k})$ with a diagonal shift $\\tau I$ if needed so that $B_{k}$ is symmetric positive definite, where $\\tau \\ge \\max\\{0,\\delta-\\lambda_{\\min}(\\nabla^{2}f(x_{k}))\\}$ with $\\delta=10^{-6}$ and $\\lambda_{\\min}(\\cdot)$ denotes the minimal eigenvalue. If $p_{k}$ computed from $B_{k}$ does not satisfy $\\nabla f(x_{k})^{\\top}p_{k}<0$, replace $p_{k}$ by $-\\nabla f(x_{k})$.\n- Case Q (quasi-second order): $B_{k}^{-1}$ is maintained by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) inverse-Hessian update starting from $B_{0}^{-1}=I$, using the standard inverse BFGS formula with $s_{k}=x_{k+1}-x_{k}$, $y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})$, provided $y_{k}^{\\top}s_{k}>0$, and skipping the update otherwise. The search direction is $p_{k}=-B_{k}^{-1}\\nabla f(x_{k})$.\n\nAdopt a maximum number of iterations $N_{\\max}=2000$. Investigate the following two termination criteria:\n- Criterion G: stop when $\\lVert \\nabla f(x_{k})\\rVert_{\\infty} < \\varepsilon$.\n- Criterion S: stop when $\\lVert x_{k}-x_{k-1}\\rVert_{2} < \\varepsilon$ (this criterion is applied only for $k\\ge 1$).\n\nYour program must run the iterative process for each test case, record the number of accepted iterations performed, the final objective value $f(x_{k})$, the final infinity-norm of the gradient $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}$, and the final step two-norm $\\lVert x_{k}-x_{k-1}\\rVert_{2}$ (take this as $0$ if no step was taken). If the maximum iteration count $N_{\\max}$ is reached without satisfying the termination criterion, return the current metrics.\n\nTest Suite. For each test case, the tuple consists of: method choice (H or Q), termination criterion (G or S), initial point $x_{0}$, and tolerance $\\varepsilon$. Use the following $6$ cases:\n- Case $1$: method H, criterion G, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-8}$.\n- Case $2$: method H, criterion S, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-4}$.\n- Case $3$: method Q, criterion G, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-8}$.\n- Case $4$: method Q, criterion S, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-4}$.\n- Case $5$: method H, criterion G, $x_{0}=\\begin{bmatrix}1.0\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-12}$.\n- Case $6$: method Q, criterion S, $x_{0}=\\begin{bmatrix}1.0\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-12}$.\n\nFinal output format. Your program should produce a single line of output containing a list of results, one per test case, in the same order as above. Each result must be the list\n$$\n[\\ \\text{method\\_id},\\ \\text{criterion\\_id},\\ \\text{iterations},\\ f(x_{k}),\\ \\lVert \\nabla f(x_{k})\\rVert_{\\infty},\\ \\lVert x_{k}-x_{k-1}\\rVert_{2}\\ ],\n$$\nwhere method identifiers are $0$ for H and $1$ for Q, criterion identifiers are $0$ for G and $1$ for S, the iteration count is an integer, and the remaining entries are real numbers. The full line must therefore be a single list of $6$ lists, for example\n$$\n[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots],\n$$\nwith no other characters printed.", "solution": "The problem presented is a well-posed task in the field of computational engineering, specifically in numerical optimization. It requires the implementation and comparison of two standard iterative methods for unconstrained nonlinear optimization—a modified Newton's method and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method—on the Rosenbrock function. The analysis focuses on the effect of different termination criteria. The problem is scientifically grounded, free of contradictions, and all necessary parameters are provided. Therefore, a rigorous solution can be constructed.\n\nThe objective is to find the minimizer of the Rosenbrock function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$, defined as:\n$$\nf(x) = f\\left(\\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix}\\right) = 100\\left(x_{2}-x_{1}^{2}\\right)^{2} + \\left(1-x_{1}\\right)^{2}\n$$\nThis function is a classic benchmark for optimization algorithms due to its non-convex nature and a narrow, curved valley leading to the global minimum, which makes convergence challenging. The unique minimizer is known to be $x^{\\star} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$, where $f(x^{\\star}) = 0$.\n\nFor the iterative methods, we require the gradient vector $\\nabla f(x)$ and, for Newton's method, the Hessian matrix $\\nabla^{2} f(x)$.\nThe gradient is given by:\n$$\n\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} -400x_{1}(x_{2}-x_{1}^{2}) - 2(1-x_{1}) \\\\ 200(x_{2}-x_{1}^{2}) \\end{bmatrix}\n$$\nThe Hessian matrix is:\n$$\n\\nabla^{2} f(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 1200x_{1}^{2} - 400x_{2} + 2 & -400x_{1} \\\\ -400x_{1} & 200 \\end{bmatrix}\n$$\nAll iterative methods under consideration follow the general structure:\n$$\nx_{k+1} = x_{k} + \\alpha_{k} p_{k}\n$$\nwhere $x_{k}$ is the current iterate, $p_{k}$ is the search direction, and $\\alpha_{k}$ is the step length.\n\n**Search Direction Calculation**\n\nThe primary difference between the methods lies in the computation of the search direction $p_{k}$. In general, this is based on solving a linear system of the form $B_{k}p_{k} = -\\nabla f(x_{k})$, where $B_k$ is a symmetric positive-definite matrix that is either the true Hessian or an approximation thereof.\n\n1.  **Case H: Modified Newton's Method**\n    This method uses second-order information by setting $B_{k}$ to the exact Hessian, $B_{k} = \\nabla^{2} f(x_{k})$. The search direction is then $p_{k} = -(\\nabla^{2} f(x_{k}))^{-1} \\nabla f(x_{k})$. This direction is optimal for the local quadratic model of the function. However, if the Hessian $\\nabla^{2} f(x_{k})$ is not positive definite, the quadratic model is not convex, and $p_{k}$ may not be a descent direction. To overcome this, a modification is employed. The matrix $B_k$ is adjusted to $B_k = \\nabla^{2} f(x_{k}) + \\tau I$, where $I$ is the identity matrix and $\\tau$ is a non-negative shift chosen to ensure $B_k$ is sufficiently positive definite. Specifically, $\\tau = \\max\\{0, \\delta - \\lambda_{\\min}(\\nabla^{2}f(x_{k}))\\}$ for a small constant $\\delta > 0$. This procedure, known as a modified Cholesky or eigenvalue-shifting approach, guarantees that the resulting search direction is a descent direction. As a further safeguard, if the computed $p_k$ fails the descent condition $\\nabla f(x_k)^\\top p_k < 0$, the algorithm reverts to the most basic descent direction, the steepest descent direction, $p_k = -\\nabla f(x_k)$.\n\n2.  **Case Q: BFGS Quasi-Newton Method**\n    This method avoids the computational expense and potential complexities of calculating and inverting the Hessian matrix at each iteration. Instead, it builds an approximation to the *inverse* Hessian, denoted here as $B_{k}^{-1}$. The search direction is then easily computed as $p_{k} = -B_{k}^{-1}\\nabla f(x_{k})$. Starting with an initial guess, typically $B_{0}^{-1} = I$, the approximation is updated at each step using information from the most recent step. The BFGS update formula for the inverse Hessian is:\n    $$\n    B_{k+1}^{-1} = \\left(I - \\frac{s_{k}y_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\\right) B_{k}^{-1} \\left(I - \\frac{y_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\\right) + \\frac{s_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\n    $$\n    where $s_{k} = x_{k+1} - x_{k}$ and $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$. A crucial requirement for this update is the curvature condition, $y_{k}^{\\top}s_{k} > 0$. This condition ensures that the updated matrix $B_{k+1}^{-1}$ remains positive definite if $B_{k}^{-1}$ was. If the condition is not met, the update is skipped, and $B_{k+1}^{-1}$ is set to $B_{k}^{-1}$.\n\n**Step Length Calculation**\n\nFor both methods, once a descent direction $p_k$ is found, the step length $\\alpha_k$ is determined by an Armijo backtracking line search. This search ensures sufficient decrease in the function value without taking excessively small steps. Starting with a full step $\\alpha_k = 1$, it is successively reduced by a factor $\\beta \\in (0, 1)$ until the Armijo condition is satisfied:\n$$\nf(x_{k}+\\alpha_{k}p_{k}) \\le f(x_{k}) + c_{1}\\alpha_{k}\\nabla f(x_{k})^{\\top}p_{k}\n$$\nThe parameter $c_{1}$ (e.g., $10^{-4}$) controls how much decrease is deemed sufficient. This prevents the algorithm from taking infinitesimally small steps that yield negligible progress. A lower bound on $\\alpha_k$ is imposed to prevent the line search from running indefinitely.\n\n**Termination Criteria**\n\nThe choice of when to stop the iterative process is critical. The problem investigates two common criteria:\n\n1.  **Criterion G: Gradient-norm based.** The algorithm terminates when $\\lVert \\nabla f(x_{k})\\rVert_{\\infty} < \\varepsilon$. This is theoretically sound, as a necessary condition for a point $x^{\\star}$ to be a local minimizer is $\\nabla f(x^{\\star}) = 0$. A small gradient norm indicates that the iterate is close to a stationary point.\n\n2.  **Criterion S: Step-size based.** The algorithm terminates when $\\lVert x_{k}-x_{k-1}\\rVert_{2} < \\varepsilon$. This criterion is often used in practice because it seems intuitive that steps should become small as we approach the solution. However, it can be unreliable. An algorithm might take very small steps far from the solution if it is struggling (e.g., in a narrow valley), leading to premature termination. Conversely, in regions of high curvature, even small changes in position can correspond to large changes in the gradient. This problem's design allows for a direct comparison of these two criteria on a challenging function. A maximum number of iterations, $N_{\\max} = 2000$, serves as a fail-safe against non-convergence.\n\nThe analysis of the six test cases will illuminate the practical performance, convergence speed, and robustness of these standard optimization schemes under different termination strategies.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problem for all test cases and print the results.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (method, criterion, x0, epsilon)\n        ('H', 'G', np.array([-1.2, 1.0]), 1e-8),\n        ('H', 'S', np.array([-1.2, 1.0]), 1e-4),\n        ('Q', 'G', np.array([-1.2, 1.0]), 1e-8),\n        ('Q', 'S', np.array([-1.2, 1.0]), 1e-4),\n        ('H', 'G', np.array([1.0, 1.0]), 1e-12),\n        ('Q', 'S', np.array([1.0, 1.0]), 1e-12),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        method, criterion, x0, epsilon = case\n        method_id = 0 if method == 'H' else 1\n        criterion_id = 0 if criterion == 'G' else 1\n        \n        iterations, final_f, final_grad_norm, final_step_norm = run_optimizer(\n            method, criterion, x0, epsilon\n        )\n        \n        result = [\n            method_id, \n            criterion_id, \n            iterations, \n            final_f, \n            final_grad_norm, \n            final_step_norm\n        ]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\n\ndef rosenbrock_f(x):\n    \"\"\"Rosenbrock function.\"\"\"\n    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n\ndef rosenbrock_grad(x):\n    \"\"\"Gradient of the Rosenbrock function.\"\"\"\n    g1 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n    g2 = 200 * (x[1] - x[0]**2)\n    return np.array([g1, g2])\n\ndef rosenbrock_hess(x):\n    \"\"\"Hessian of the Rosenbrock function.\"\"\"\n    h11 = 1200 * x[0]**2 - 400 * x[1] + 2\n    h12 = -400 * x[0]\n    h22 = 200\n    return np.array([[h11, h12], [h12, h22]])\n\n\ndef run_optimizer(method, criterion, x0, epsilon):\n    \"\"\"\n    Runs the optimization loop for a given configuration.\n    \"\"\"\n    \n    # Parameters\n    N_max = 2000\n    c1 = 1e-4\n    beta = 0.5\n    delta = 1e-6\n    alpha_min = 1e-16\n\n    x_k = np.copy(x0).astype(float)\n    x_prev = np.copy(x_k)\n    \n    # Initialize BFGS inverse Hessian approximation\n    if method == 'Q':\n        B_inv_k = np.identity(2)\n    \n    k = 0\n    while k < N_max:\n        grad_k = rosenbrock_grad(x_k)\n        \n        # --- Check termination criteria ---\n        if criterion == 'G':\n            if np.linalg.norm(grad_k, ord=np.inf) < epsilon:\n                break\n        elif criterion == 'S' and k > 0:\n            if np.linalg.norm(x_k - x_prev, ord=2) < epsilon:\n                break\n        \n        # --- Compute search direction p_k ---\n        if method == 'H':\n            H_k = rosenbrock_hess(x_k)\n            try:\n                # Eigenvalue modification to ensure positive definiteness\n                min_eig = np.min(np.linalg.eigvalsh(H_k))\n                tau = max(0, delta - min_eig)\n                B_k = H_k + tau * np.identity(2)\n                p_k = np.linalg.solve(B_k, -grad_k)\n                # Fallback to steepest descent if not a descent direction\n                if grad_k.T @ p_k >= 0:\n                    p_k = -grad_k\n            except np.linalg.LinAlgError:\n                # Fallback in case of singular matrix\n                p_k = -grad_k\n        \n        elif method == 'Q':\n            p_k = -B_inv_k @ grad_k\n\n        # --- Line search (Armijo backtracking) ---\n        alpha = 1.0\n        f_k = rosenbrock_f(x_k)\n        grad_dot_p = grad_k.T @ p_k\n        \n        while alpha >= alpha_min:\n            x_candidate = x_k + alpha * p_k\n            f_candidate = rosenbrock_f(x_candidate)\n            if f_candidate <= f_k + c1 * alpha * grad_dot_p:\n                break\n            alpha *= beta\n        else: # loop exhausted without break\n            alpha = alpha_min\n\n        x_prev = np.copy(x_k)\n        x_k = x_k + alpha * p_k\n        \n        # --- BFGS update for inverse Hessian (Method Q) ---\n        if method == 'Q':\n            s_k = x_k - x_prev\n            # Compute gradient at new point\n            grad_k_plus_1 = rosenbrock_grad(x_k)\n            y_k = grad_k_plus_1 - grad_k\n            \n            y_dot_s = y_k.T @ s_k\n            if y_dot_s > 0:\n                rho_k = 1.0 / y_dot_s\n                I = np.identity(2)\n                V = I - rho_k * np.outer(s_k, y_k)\n                B_inv_k = V @ B_inv_k @ V.T + rho_k * np.outer(s_k, s_k)\n\n        k += 1\n\n    # --- Collect final metrics ---\n    final_f = rosenbrock_f(x_k)\n    final_grad_norm = np.linalg.norm(rosenbrock_grad(x_k), ord=np.inf)\n    if k == 0:\n        final_step_norm = 0.0\n    else:\n        final_step_norm = np.linalg.norm(x_k - x_prev, ord=2)\n\n    return k, final_f, final_grad_norm, final_step_norm\n\nsolve()\n```", "id": "2417339"}, {"introduction": "The true power of quasi-Newton methods is most evident in large-scale applications, where efficiency is paramount. This capstone practice challenges you to implement the Limited-memory BFGS (L-BFGS) algorithm, a memory-efficient variant of BFGS, to solve a real-world machine learning problem: multiclass logistic regression [@problem_id:2417391]. By deriving the gradient and coding the famous L-BFGS two-loop recursion from scratch, you will gain invaluable experience in applying advanced optimization techniques to modern computational science.", "problem": "You must write a complete, runnable program that implements the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm from first principles to train a multiclass logistic regression model by minimizing the negative log-likelihood. The goal is to derive and implement the algorithm starting from core definitions, and then apply it to a small test suite. The final output must be a single line containing all test results in a specified format.\n\nThe model is defined as follows. Given data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, labels $\\mathbf{y} \\in \\{0, 1, \\dots, K-1\\}^{n}$, and the augmented features $\\tilde{\\mathbf{X}}$ formed by appending a bias feature $1$ to each row of $\\mathbf{X}$ (so $\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{n \\times (d+1)}$), the model parameters are collected in the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{(d+1) \\times K}$. For each sample index $i \\in \\{1,\\dots,n\\}$, the class scores are $\\mathbf{z}_i = \\tilde{\\mathbf{x}}_i^{\\top} \\mathbf{W} \\in \\mathbb{R}^{K}$, and the class probabilities are given by the softmax mapping\n$$\np_{i,k} = \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})}, \\quad \\text{for } k \\in \\{1,\\dots,K\\}.\n$$\nThe negative log-likelihood with $\\ell_{2}$ regularization (including the bias weights) is\n$$\nf(\\mathbf{W}) = - \\sum_{i=1}^{n} \\log p_{i, y_i} + \\frac{\\lambda}{2} \\lVert \\mathbf{W} \\rVert_{F}^{2},\n$$\nwhere $\\lambda \\ge 0$ is a given regularization coefficient and $\\lVert \\cdot \\rVert_{F}$ denotes the Frobenius norm.\n\nTasks:\n- From the above core definitions of the softmax function and the negative log-likelihood, derive the expression for the gradient $\\nabla f(\\mathbf{W})$ without invoking any pre-existing result beyond the chain rule, the product rule, and standard properties of the exponential function. Your derivation must rely on the definition of probabilities $p_{i,k}$, the one-hot encoding of labels, and the derivative of the logarithm, and must express the gradient in a matrix form that can be implemented efficiently.\n- Derive the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) search direction formula based on the secant condition and the positive-definite inverse Hessian update. Your derivation must start from the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update for the inverse Hessian, explain the limited-memory idea, and culminate in a two-loop recursion that computes the product of the inverse Hessian approximation with the current gradient using only a small window of recent curvature pairs. Explain the need for the curvature condition $s_k^{\\top} y_k > 0$ and how to handle violations in finite precision arithmetic.\n- Implement from scratch, without using any library optimizer, an L-BFGS solver for minimizing $f(\\mathbf{W})$. Your implementation must include:\n  - Two-loop recursion to compute the search direction using the last $m$ curvature pairs $(s_k, y_k)$ where $s_k = w_{k+1} - w_k$ and $y_k = \\nabla f(w_{k+1}) - \\nabla f(w_k)$ and $w$ is the vectorized form of $\\mathbf{W}$.\n  - An initial inverse Hessian scaling $H_0 = \\gamma_k \\mathbf{I}$ with $\\gamma_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$ when available, and $H_0 = \\mathbf{I}$ otherwise.\n  - A backtracking line search enforcing the Armijo condition $f(w_k + \\alpha p_k) \\le f(w_k) + c_1 \\alpha \\nabla f(w_k)^{\\top} p_k$ with parameters $c_1 \\in (0, 1)$, initial step size $\\alpha_0 = 1$, and contraction factor $\\tau \\in (0, 1)$.\n  - Safeguards: if $p_k^{\\top} \\nabla f(w_k) \\ge 0$ then fall back to the steepest descent direction $p_k = -\\nabla f(w_k)$; if $s_k^{\\top} y_k \\le \\epsilon$ for a small $\\epsilon > 0$, skip storing the new curvature pair to preserve positive definiteness.\n  - Stopping criteria based on the infinity norm of the gradient $\\lVert \\nabla f(w_k) \\rVert_{\\infty} \\le \\text{tol}$ or a maximum iteration count.\n- Apply your implementation to the following test suite. All features are two-dimensional with an appended bias, and there are $K = 3$ classes labeled as $0$, $1$, and $2$. The datasets are deterministic lists of points so that no pseudo-random generator is required.\n\nTest Suite:\n- Test case A (well-separated classes, moderate regularization):\n  - Data: $\\mathbf{X}_A$ with $n_A = 15$ samples, organized by class as follows (each row is $(x_1, x_2)$):\n    - Class $0$: $(0.0, 0.0)$, $(0.2, -0.1)$, $(-0.1, 0.1)$, $(0.1, 0.2)$, $(-0.2, 0.0)$.\n    - Class $1$: $(3.0, 3.0)$, $(2.9, 3.1)$, $(3.1, 2.8)$, $(3.2, 3.2)$, $(2.8, 2.9)$.\n    - Class $2$: $(-3.0, 3.0)$, $(-3.1, 3.2)$, $(-2.9, 2.8)$, $(-3.2, 2.9)$, $(-2.8, 3.1)$.\n  - Labels $\\mathbf{y}_A$: five zeros, followed by five ones, followed by five twos.\n  - Regularization: $\\lambda_A = 0.1$.\n  - L-BFGS hyperparameters: memory $m = 7$, gradient tolerance $\\text{tol} = 10^{-8}$, maximum iterations $200$, line search constants $c_1 = 10^{-4}$ and contraction $\\tau = 0.5$.\n  - Required output for this case: the minimized objective value $f(\\mathbf{W}^{\\star}_A)$ as a float rounded to six decimal places.\n- Test case B (same data, strong regularization):\n  - Data and labels identical to Test case A.\n  - Regularization: $\\lambda_B = 100$.\n  - Same L-BFGS hyperparameters as in Test case A.\n  - Required output for this case: the Frobenius norm $\\lVert \\mathbf{W}^{\\star}_B \\rVert_F$ of the optimized weight matrix as a float rounded to six decimal places.\n- Test case C (collinear features, mild regularization):\n  - Data: $\\mathbf{X}_C$ with $n_C = 9$ samples arranged by class:\n    - Class $0$: $(-2.0, -6.0)$, $(-1.0, -3.0)$, $(-1.5, -4.5)$.\n    - Class $1$: $(-0.2, -0.6)$, $(0.0, 0.0)$, $(0.5, 1.5)$.\n    - Class $2$: $(1.5, 4.5)$, $(2.0, 6.0)$, $(2.2, 6.6)$.\n  - Labels $\\mathbf{y}_C$: three zeros, then three ones, then three twos.\n  - Regularization: $\\lambda_C = 0.01$.\n  - Same L-BFGS hyperparameters as in Test case A.\n  - Required output for this case: a boolean indicating whether the infinity norm of the gradient at the returned solution satisfies $\\lVert \\nabla f(\\mathbf{W}^{\\star}_C) \\rVert_{\\infty} \\le 10^{-6}$.\n\nAngle units are not applicable. There are no physical units in this problem.\n\nFinal output format:\n- Your program should produce a single line of output containing the results of the three test cases as a comma-separated list enclosed in square brackets. The entries must appear in this order: $[ \\text{A-result}, \\text{B-result}, \\text{C-result} ]$, where A-result and B-result are floats rounded to six decimal places, and C-result is a boolean. For example: $[1.234000,0.567890,True]$.", "solution": "The problem as stated is well-defined, mathematically coherent, and scientifically grounded in the fields of numerical optimization and machine learning. It presents a standard, albeit non-trivial, task in computational engineering: the derivation and implementation of a core optimization algorithm for a fundamental statistical model. All definitions, parameters, and test cases are provided with sufficient precision to permit a unique and verifiable solution. Therefore, we proceed directly to the formal derivation and its subsequent implementation.\n\nThe task is to minimize the objective function $f(\\mathbf{W})$ for a multiclass logistic regression model:\n$$\nf(\\mathbf{W}) = - \\sum_{i=1}^{n} \\log p_{i, y_i} + \\frac{\\lambda}{2} \\lVert \\mathbf{W} \\rVert_{F}^{2}\n$$\nwhere $\\mathbf{W} \\in \\mathbb{R}^{(d+1) \\times K}$ are the model weights, $\\lambda \\geq 0$ is the regularization parameter, and $p_{i, y_i}$ is the model's predicted probability for the true class $y_i$ of sample $i$. The optimization will be performed using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm, which requires the gradient of the objective function, $\\nabla f(\\mathbf{W})$.\n\n**1. Derivation of the Gradient $\\nabla f(\\mathbf{W})$**\n\nThe objective function is a sum of two components: the negative log-likelihood $L(\\mathbf{W}) = - \\sum_{i=1}^{n} \\log p_{i, y_i}$ and the $\\ell_2$ regularization term $R(\\mathbf{W}) = \\frac{\\lambda}{2} \\lVert \\mathbf{W} \\rVert_{F}^{2}$. By linearity of differentiation, the gradient is $\\nabla f(\\mathbf{W}) = \\nabla L(\\mathbf{W}) + \\nabla R(\\mathbf{W})$.\n\nFirst, the gradient of the regularization term is elementary. The squared Frobenius norm is $\\lVert \\mathbf{W} \\rVert_{F}^{2} = \\sum_{j=1}^{d+1}\\sum_{k=1}^{K} W_{jk}^2$. The partial derivative with respect to an arbitrary weight $W_{ab}$ is:\n$$\n\\frac{\\partial R(\\mathbf{W})}{\\partial W_{ab}} = \\frac{\\partial}{\\partial W_{ab}} \\left( \\frac{\\lambda}{2} \\sum_{j,k} W_{jk}^2 \\right) = \\frac{\\lambda}{2} (2W_{ab}) = \\lambda W_{ab}\n$$\nIn matrix form, this gives $\\nabla R(\\mathbf{W}) = \\lambda \\mathbf{W}$.\n\nNext, we derive the gradient of the negative log-likelihood term, $\\nabla L(\\mathbf{W})$. We use the chain rule. The loss contribution from a single sample $i$ is $L_i = -\\log p_{i, y_i}$. The class probabilities $p_{i,k}$ depend on the scores $z_{i,k}$, which in turn depend on the weights $\\mathbf{W}$. The scores are given by $z_{i,k} = \\tilde{\\mathbf{x}}_i^{\\top} \\mathbf{w}_k$, where $\\tilde{\\mathbf{x}}_i \\in \\mathbb{R}^{d+1}$ is the augmented feature vector for sample $i$ and $\\mathbf{w}_k$ is the $k$-th column of $\\mathbf{W}$. In matrix notation, $\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{W}$. The probability for sample $i$ and class $k$ is given by the softmax function:\n$$\np_{i,k} = \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})}\n$$\nThe loss for sample $i$ can be rewritten as:\n$$\nL_i = -\\log p_{i, y_i} = -z_{i, y_i} + \\log\\left(\\sum_{j=1}^{K} \\exp(z_{i,j})\\right)\n$$\nWe first find the partial derivative of $L_i$ with respect to a score $z_{i,k}$:\n$$\n\\frac{\\partial L_i}{\\partial z_{i,k}} = -\\delta_{k, y_i} + \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})} = p_{i,k} - \\delta_{k, y_i}\n$$\nwhere $\\delta_{k, y_i}$ is the Kronecker delta, which is $1$ if $k=y_i$ and $0$ otherwise. This term can be represented by a one-hot encoded label matrix $\\mathbf{Y} \\in \\mathbb{R}^{n \\times K}$, where $Y_{ik} = \\delta_{k, y_i}$.\n\nNext, we find the partial derivative of a score $z_{i,k}$ with respect to a weight $W_{ab}$ (row $a$, column $b$ of $\\mathbf{W}$):\n$$\nz_{i,k} = \\sum_{j=1}^{d+1} \\tilde{x}_{ij} W_{jk} \\implies \\frac{\\partial z_{i,k}}{\\partial W_{ab}} = \\tilde{x}_{ia} \\delta_{kb}\n$$\nUsing the chain rule, the partial derivative of the total likelihood $L = \\sum_i L_i$ with respect to $W_{ab}$ is:\n$$\n\\frac{\\partial L}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\frac{\\partial L_i}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\frac{\\partial L_i}{\\partial z_{i,k}} \\frac{\\partial z_{i,k}}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\sum_{k=1}^{K} (p_{i,k} - Y_{ik}) (\\tilde{x}_{ia} \\delta_{kb})\n$$\nThe sum over $k$ contains only one non-zero term, for $k=b$. Thus:\n$$\n\\frac{\\partial L}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\tilde{x}_{ia} (p_{ib} - Y_{ib})\n$$\nThis expression is precisely the $(a,b)$-th element of the matrix product $\\tilde{\\mathbf{X}}^{\\top}(\\mathbf{P} - \\mathbf{Y})$, where $\\mathbf{P}$ is the $n \\times K$ matrix of probabilities $p_{ik}$.\nCombining the gradients of both terms, we arrive at the final expression for the gradient of $f(\\mathbf{W})$:\n$$\n\\nabla f(\\mathbf{W}) = \\tilde{\\mathbf{X}}^{\\top}(\\mathbf{P} - \\mathbf{Y}) + \\lambda \\mathbf{W}\n$$\nThis matrix form is essential for an efficient implementation.\n\n**2. The L-BFGS Algorithm**\n\nThe L-BFGS algorithm is a quasi-Newton method that approximates the inverse Hessian of the objective function to determine a search direction. It is designed for large-scale problems where forming, storing, or inverting the true or approximated Hessian is infeasible. Let $w$ be the vectorized form of the weight matrix $\\mathbf{W}$ and $g_k = \\nabla f(w_k)$ be the corresponding flattened gradient at iteration $k$.\n\nThe foundation of BFGS is the secant equation, $B_{k+1}s_k = y_k$, where $s_k = w_{k+1} - w_k$ is the step in the parameters and $y_k = g_{k+1} - g_k$ is the change in the gradient. The BFGS update for the inverse Hessian approximation $H_k \\approx (\\nabla^2 f(w_k))^{-1}$ is:\n$$\nH_{k+1} = (I - \\rho_k s_k y_k^{\\top}) H_k (I - \\rho_k y_k s_k^{\\top}) + \\rho_k s_k s_k^{\\top}, \\quad \\text{where } \\rho_k = \\frac{1}{y_k^{\\top} s_k}\n$$\nThis update preserves positive-definiteness if $H_k$ is positive-definite and the curvature condition $s_k^{\\top} y_k > 0$ holds. This condition is guaranteed for a strictly convex function with an exact line search, but in practice, it must be explicitly checked. If $s_k^{\\top} y_k \\le \\epsilon$ for some small tolerance $\\epsilon > 0$, the curvature pair $(s_k, y_k)$ is discarded to maintain stability.\n\nL-BFGS avoids storing the dense matrix $H_k$. Instead, it stores the last $m$ curvature pairs $\\{(s_i, y_i)\\}_{i=k-m}^{k-1}$. The search direction $p_k = -H_k g_k$ is computed using a procedure known as the two-loop recursion. This procedure implicitly applies the $m$ most recent BFGS updates to an initial Hessian approximation $H_k^0$.\n\nThe algorithm to compute the product $r = H_k g_k$ is as follows:\n1.  Initialize $q \\leftarrow g_k$.\n2.  **First Loop (backward pass):** Iterate from $i = k-1$ down to $k-m$.\n    -   Compute $\\rho_i = 1 / (y_i^{\\top} s_i)$.\n    -   Compute $\\alpha_i = \\rho_i s_i^{\\top} q$.\n    -   Update $q \\leftarrow q - \\alpha_i y_i$.\n3.  **Initial Hessian Scaling:** The initial inverse Hessian $H_k^0$ is chosen to be a simple scaled identity matrix, $H_k^0 = \\gamma_k I$. A common scaling factor, which helps to approximate the scale of the true inverse Hessian, is $\\gamma_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$. If $k=0$ or no previous step is available, $\\gamma_k = 1$ is used. Apply this scaling: $r \\leftarrow \\gamma_k q$.\n4.  **Second Loop (forward pass):** Iterate from $i = k-m$ up to $k-1$.\n    -   Fetch $\\rho_i = 1 / (y_i^{\\top} s_i)$.\n    -   Compute $\\beta = \\rho_i y_i^{\\top} r$.\n    -   Retrieve $\\alpha_i$ from the first loop.\n    -   Update $r \\leftarrow r + s_i (\\alpha_i - \\beta)$.\nThe resulting vector $r$ is the product $H_k g_k$, and the search direction is $p_k = -r$.\n\nA safeguard is necessary to ensure $p_k$ is a descent direction, i.e., $g_k^{\\top} p_k < 0$. If $g_k^{\\top} p_k \\ge 0$, which may occur due to numerical imprecision or the nature of the limited-memory approximation, the algorithm must fall back to a guaranteed descent direction, typically the steepest descent direction, $p_k = -g_k$.\n\nOnce a valid descent direction $p_k$ is found, a line search is performed to find a step size $\\alpha_k > 0$ that provides sufficient decrease in the objective function. A backtracking line search satisfying the Armijo condition is standard:\n$$\nf(w_k + \\alpha_k p_k) \\le f(w_k) + c_1 \\alpha_k g_k^{\\top} p_k\n$$\nfor a constant $c_1 \\in (0, 1)$. The search starts with an initial step size (e.g., $\\alpha=1$) and contracts it by a factor $\\tau \\in (0, 1)$ until the condition is met.\n\n**3. Implementation and Application**\n\nThe implementation follows the derived principles. The weight matrix $\\mathbf{W}$ and its gradient must be vectorized for use with the L-BFGS solver, which operates on one-dimensional vectors. A helper class for the logistic regression model will compute the objective value and gradient for a given vectorized weight. The L-BFGS solver will encapsulate the two-loop recursion, history management, line search, and convergence checks. For numerical stability, the softmax computation is stabilized by subtracting the maximum score from each row of a samples' scores before exponentiation.\n\nThe developed solver is then applied to the three specified test cases with the provided hyperparameters ($m=7$, $\\text{tol}=10^{-8}$, max iterations=$200$, $c_1=10^{-4}$, $\\tau=0.5$). The initial weights are set to zero. The required outputs for each case—minimized objective value, Frobenius norm of the optimal weights, and a boolean check on the final gradient norm—are calculated and formatted as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\nclass LogisticRegression:\n    \"\"\"\n    Encapsulates the multiclass logistic regression model, providing a function\n    to compute the objective value and its gradient.\n    \"\"\"\n    def __init__(self, X, y, K, lambda_reg):\n        \"\"\"\n        Initializes the model with data and hyperparameters.\n\n        Args:\n            X (np.ndarray): The n x d feature matrix.\n            y (np.ndarray): The n-dimensional label vector with integers 0 to K-1.\n            K (int): The number of classes.\n            lambda_reg (float): The L2 regularization coefficient.\n        \"\"\"\n        self.n, self.d = X.shape\n        self.K = K\n        self.lambda_reg = lambda_reg\n        \n        # Augment features with a bias term\n        self.X_tilde = np.c_[X, np.ones(self.n)]\n        \n        # Store original integer labels for indexing\n        self.y_indices = y\n        \n        # Create one-hot encoded label matrix\n        self.y_one_hot = np.zeros((self.n, self.K))\n        self.y_one_hot[np.arange(self.n), self.y_indices] = 1\n\n    def objective_and_grad(self, W_flat):\n        \"\"\"\n        Computes the objective function value and its gradient.\n\n        Args:\n            W_flat (np.ndarray): The flattened ((d+1)*K,) weight vector.\n\n        Returns:\n            tuple: A tuple containing:\n                - f_val (float): The objective function value.\n                - grad_flat (np.ndarray): The flattened gradient vector.\n        \"\"\"\n        W = W_flat.reshape((self.d + 1, self.K))\n\n        # (1) Compute scores\n        Z = self.X_tilde @ W\n\n        # (2) Compute probabilities using a numerically stable softmax\n        Z_max = np.max(Z, axis=1, keepdims=True)\n        exp_Z_shifted = np.exp(Z - Z_max)\n        P = exp_Z_shifted / np.sum(exp_Z_shifted, axis=1, keepdims=True)\n\n        # (3) Compute the objective function value (negative log-likelihood + L2 regularization)\n        log_probs = np.log(P[np.arange(self.n), self.y_indices])\n        log_likelihood = np.sum(log_probs)\n        frobenius_norm_sq = np.sum(W**2)\n        f_val = -log_likelihood + (self.lambda_reg / 2) * frobenius_norm_sq\n\n        # (4) Compute the gradient\n        grad_W = self.X_tilde.T @ (P - self.y_one_hot) + self.lambda_reg * W\n        grad_flat = grad_W.flatten()\n\n        return f_val, grad_flat\n\nclass LBFGSSolver:\n    \"\"\"\n    Implements the L-BFGS algorithm from first principles.\n    \"\"\"\n    def __init__(self, m, tol, max_iter, c1, tau, eps=1e-12):\n        \"\"\"\n        Initializes the L-BFGS solver with its hyperparameters.\n        \n        Args:\n            m (int): The memory size (number of curvature pairs to store).\n            tol (float): Gradient norm tolerance for convergence.\n            max_iter (int): Maximum number of iterations.\n            c1 (float): Armijo condition parameter for line search.\n            tau (float): Contraction factor for backtracking line search.\n            eps (float): Small tolerance for the curvature condition.\n        \"\"\"\n        self.m = m\n        self.tol = tol\n        self.max_iter = max_iter\n        self.c1 = c1\n        self.tau = tau\n        self.eps = eps\n\n    def solve(self, obj_grad_func, w0):\n        \"\"\"\n        Minimizes the objective function.\n\n        Args:\n            obj_grad_func (callable): A function that takes a vector w and returns (objective, gradient).\n            w0 (np.ndarray): The initial guess for the parameters.\n\n        Returns:\n            tuple: A tuple containing:\n                - w (np.ndarray): The optimized parameters.\n                - f_val (float): The objective value at the solution.\n                - g (np.ndarray): The gradient at the solution.\n        \"\"\"\n        w = w0.copy()\n        s_hist = deque(maxlen=self.m)\n        y_hist = deque(maxlen=self.m)\n        \n        f_val, g = obj_grad_func(w)\n\n        for k in range(self.max_iter):\n            if np.linalg.norm(g, ord=np.inf) < self.tol:\n                break\n\n            # 1. Compute search direction p_k using two-loop recursion\n            q = g.copy()\n            alphas = []\n            rhos = []\n\n            # First loop (backward)\n            for s, y in reversed(list(zip(s_hist, y_hist))):\n                rho = 1.0 / (y.T @ s)\n                rhos.insert(0, rho)\n                alpha = rho * (s.T @ q)\n                alphas.insert(0, alpha)\n                q -= alpha * y\n            \n            # Initial Hessian approximation scaling\n            gamma = 1.0\n            if s_hist:\n                s_last, y_last = s_hist[-1], y_hist[-1]\n                # Defensive check for y_last norm to avoid division by zero\n                y_norm_sq = y_last.T @ y_last\n                if y_norm_sq > self.eps:\n                    gamma = (s_last.T @ y_last) / y_norm_sq\n            \n            r = gamma * q\n\n            # Second loop (forward)\n            for i, (s, y) in enumerate(zip(s_hist, y_hist)):\n                beta = rhos[i] * (y.T @ r)\n                r += s * (alphas[i] - beta)\n\n            p = -r\n\n            # Safeguard: ensure p is a descent direction\n            g_dot_p = g.T @ p\n            if g_dot_p >= 0:\n                p = -g\n                g_dot_p = g.T @ p\n            \n            # 2. Backtracking line search for step size alpha_k\n            alpha = 1.0\n            w_new_temp = w + alpha * p\n            f_new_temp, _ = obj_grad_func(w_new_temp)\n            while f_new_temp > f_val + self.c1 * alpha * g_dot_p:\n                alpha *= self.tau\n                if alpha < 1e-15: # Safety break\n                    break\n                w_new_temp = w + alpha * p\n                f_new_temp, _ = obj_grad_func(w_new_temp)\n            \n            # 3. Update parameters and gradients\n            s_new = alpha * p\n            w_new = w + s_new\n            f_new, g_new = obj_grad_func(w_new)\n\n            y_new = g_new - g\n            \n            # 4. Update history if curvature condition is met\n            if s_new.T @ y_new > self.eps:\n                s_hist.append(s_new)\n                y_hist.append(y_new)\n            \n            w, f_val, g = w_new, f_new, g_new\n        \n        return w, f_val, g\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    K = 3\n    d = 2\n    lbfgs_params = {'m': 7, 'tol': 1e-8, 'max_iter': 200, 'c1': 1e-4, 'tau': 0.5}\n    solver = LBFGSSolver(**lbfgs_params)\n    w0 = np.zeros(((d + 1) * K,))\n    results = []\n\n    # --- Test Case A ---\n    XA = np.array([\n        [0.0, 0.0], [0.2, -0.1], [-0.1, 0.1], [0.1, 0.2], [-0.2, 0.0],\n        [3.0, 3.0], [2.9, 3.1], [3.1, 2.8], [3.2, 3.2], [2.8, 2.9],\n        [-3.0, 3.0], [-3.1, 3.2], [-2.9, 2.8], [-3.2, 2.9], [-2.8, 3.1]\n    ])\n    yA = np.array([0]*5 + [1]*5 + [2]*5)\n    lambda_A = 0.1\n    \n    model_A = LogisticRegression(XA, yA, K, lambda_A)\n    _, f_star_A, _ = solver.solve(model_A.objective_and_grad, w0)\n    results.append(f\"{f_star_A:.6f}\")\n\n    # --- Test Case B ---\n    yB = yA\n    lambda_B = 100.0\n    model_B = LogisticRegression(XA, yB, K, lambda_B)\n    w_star_B_flat, _, _ = solver.solve(model_B.objective_and_grad, w0)\n    W_star_B = w_star_B_flat.reshape((d + 1, K))\n    norm_W_star_B = np.linalg.norm(W_star_B, 'fro')\n    results.append(f\"{norm_W_star_B:.6f}\")\n\n    # --- Test Case C ---\n    XC = np.array([\n        [-2.0, -6.0], [-1.0, -3.0], [-1.5, -4.5],\n        [-0.2, -0.6], [0.0, 0.0], [0.5, 1.5],\n        [1.5, 4.5], [2.0, 6.0], [2.2, 6.6]\n    ])\n    yC = np.array([0]*3 + [1]*3 + [2]*3)\n    lambda_C = 0.01\n    \n    model_C = LogisticRegression(XC, yC, K, lambda_C)\n    _, _, g_star_C = solver.solve(model_C.objective_and_grad, w0)\n    grad_norm_inf = np.linalg.norm(g_star_C, ord=np.inf)\n    result_C = bool(grad_norm_inf <= 1e-6)\n    results.append(str(result_C))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2417391"}]}