{"hands_on_practices": [{"introduction": "The journey into numerical methods often begins with a fundamental question: if we rearrange an equation $f(x)=0$ into the form $x=g(x)$, will the iteration $x_{k+1}=g(x_k)$ actually find the solution? This practice provides a direct, hands-on answer by computationally exploring how different choices for $g(x)$ create drastically different convergence behaviors for the same root-finding problem. By implementing and testing various iteration maps, you will gain an intuitive understanding of the contraction mapping principle and the critical role of the derivative condition, $|g'(x)| < 1$, in defining the basin of attraction for a fixed point [@problem_id:2393371].", "problem": "You are to write a complete program that compares convergence behavior and basins of attraction for fixed-point iterations applied to the scalar nonlinear equation $e^{x} - 4x = 0$ under different rearrangements of the form $x = g(x)$. The program must be deterministic and must not require any user input.\n\nBegin from the following fundamental base:\n- Definition of a fixed point for a function $g$: a point $x^{\\star}$ such that $x^{\\star} = g(x^{\\star})$.\n- Fixed-point iteration: given an initial guess $x_0$, generate a sequence $x_{k+1} = g(x_k)$.\n- Sufficient local convergence criterion for fixed-point iteration from the Banach Fixed-Point Theorem (also known as the Contraction Mapping Principle): if $g$ is continuously differentiable and there exists a neighborhood of a fixed point $x^{\\star}$ in which $\\lvert g'(x) \\rvert \\leq q < 1$, then the iteration $x_{k+1} = g(x_k)$ converges to $x^{\\star}$ for any $x_0$ in that neighborhood.\n\nTasks for your program:\n1. Implement the nonlinear function $f(x) = e^{x} - 4x$ and its derivative $f'(x) = e^{x} - 4$.\n2. Implement the following three fixed-point maps $g$:\n   - $g_{\\mathrm{A}}(x) = \\dfrac{e^{x}}{4}$, defined for all real $x$.\n   - $g_{\\mathrm{B}}(x) = \\ln(4x)$, defined only for $x > 0$.\n   - $g_{\\mathrm{N}}(x) = x - \\dfrac{f(x)}{f'(x)}$ (Newton’s method written as a fixed-point map), defined when $f'(x) \\neq 0$.\n3. For each fixed-point map, implement the iteration $x_{k+1} = g(x_k)$ with the following termination and failure criteria:\n   - Convergence if either $\\lvert x_{k+1} - x_k \\rvert \\leq \\varepsilon_x$ or $\\lvert f(x_{k+1}) \\rvert \\leq \\varepsilon_f$, with $\\varepsilon_x = 10^{-12}$ and $\\varepsilon_f = 10^{-12}$.\n   - Failure if any of the following occurs:\n     - The next iterate leaves the domain of the map (for example, for $g_{\\mathrm{B}}$, if at any step $x_k \\leq 0$).\n     - A non-finite value (not-a-number or infinity) is produced at any step.\n     - The maximum number of iterations $N_{\\max} = 200$ is reached without convergence.\n     - For $g_{\\mathrm{N}}$, the derivative magnitude $\\lvert f'(x_k) \\rvert$ is below the threshold $\\tau = 10^{-12}$ at any step, so the update is ill-conditioned.\n4. Compute high-accuracy reference roots for classification. There are exactly two real roots of $f(x) = 0$: one in the interval $[0, 0.5]$ and one in the interval $[2, 3]$. Use a robust bracketed root-finding method (for example, bisection or Brent’s method) to compute reference values $r_1 \\in [0, 0.5]$ and $r_2 \\in [2, 3]$ with absolute accuracy at least $10^{-14}$.\n5. Basin-of-attraction classification: for a converged iterate $x^{\\star}$, classify it by proximity to the two reference roots using the rule\n   - Assign integer $0$ if $\\lvert x^{\\star} - r_1 \\rvert \\leq 10^{-8}$,\n   - Assign integer $1$ if $\\lvert x^{\\star} - r_2 \\rvert \\leq 10^{-8}$,\n   - Otherwise assign integer $-1$.\n   For a failed iteration, assign integer $-1$.\n6. Use the following test suite of $(g,\\ x_0)$ pairs to probe different basins, edge cases, and domains:\n   - $(g_{\\mathrm{A}},\\ x_0 = -1.0)$\n   - $(g_{\\mathrm{A}},\\ x_0 = 0.1)$\n   - $(g_{\\mathrm{A}},\\ x_0 = 0.6)$\n   - $(g_{\\mathrm{A}},\\ x_0 = 2.5)$\n   - $(g_{\\mathrm{B}},\\ x_0 = 0.2)$\n   - $(g_{\\mathrm{B}},\\ x_0 = 2.5)$\n   - $(g_{\\mathrm{B}},\\ x_0 = 1.0)$\n   - $(g_{\\mathrm{B}},\\ x_0 = -0.5)$\n   - $(g_{\\mathrm{N}},\\ x_0 = -1.0)$\n   - $(g_{\\mathrm{N}},\\ x_0 = 0.5)$\n   - $(g_{\\mathrm{N}},\\ x_0 = 4.0)$\n   - $(g_{\\mathrm{N}},\\ x_0 = \\ln 4)$\n7. Output specification: Your program should produce a single line of output containing the classification results for the test suite, in the same order as listed above, as a comma-separated list of integers enclosed in square brackets (for example, $[0,0,1,-1]$). No additional text should be printed.\n\nImportant notes:\n- Angles are not used in this problem, so no angle unit is required.\n- No physical units are involved.\n- Ensure numerical robustness by strictly applying the domain checks and failure criteria above.", "solution": "The problem statement has been subjected to rigorous validation and is found to be sound. It is a well-posed problem in computational engineering, grounded in the established principles of numerical analysis, specifically fixed-point iteration theory and Newton's method. All parameters, functions, and criteria are defined with sufficient precision to permit a unique, deterministic solution. We proceed with the analysis and derivation of the solution.\n\nThe objective is to analyze the convergence of three different fixed-point iteration schemes for finding the roots of the nonlinear equation $f(x) = e^{x} - 4x = 0$.\n\n**1. High-Accuracy Root Determination**\n\nThe equation $f(x) = 0$ is equivalent to finding the intersections of $y = e^x$ and $y = 4x$. A graphical analysis reveals two real roots. The problem statement correctly identifies the intervals containing these roots. To verify, we evaluate $f(x)$ at the interval boundaries:\n- For the interval $[0, 0.5]$:\n  $f(0) = e^{0} - 4(0) = 1 > 0$.\n  $f(0.5) = e^{0.5} - 4(0.5) = \\sqrt{e} - 2 \\approx 1.6487 - 2 = -0.3513 < 0$.\n- For the interval $[2, 3]$:\n  $f(2) = e^{2} - 4(2) = e^2 - 8 \\approx 7.3891 - 8 = -0.6109 < 0$.\n  $f(3) = e^{3} - 4(3) = e^3 - 12 \\approx 20.0855 - 12 = 8.0855 > 0$.\n\nSince $f(x)$ is continuous, the Intermediate Value Theorem guarantees the existence of a root in each interval. We denote these roots as $r_1 \\in [0, 0.5]$ and $r_2 \\in [2, 3]$. To serve as a benchmark for classification, these roots are computed to a high accuracy (absolute tolerance of at least $10^{-14}$) using a robust numerical method, specifically Brent's method, which is suitable for this purpose.\n\n**2. Convergence Analysis of Fixed-Point Mappings**\n\nThe convergence of a fixed-point iteration $x_{k+1} = g(x_k)$ to a fixed point $x^{\\star}$ is governed by the magnitude of the derivative of the iteration map, $\\lvert g'(x^{\\star}) \\rvert$. For local convergence, it is sufficient that $\\lvert g'(x^{\\star}) \\rvert < 1$. If $\\lvert g'(x^{\\star}) \\rvert > 1$, the fixed point is repelling, and the iteration will diverge. If $\\lvert g'(x^{\\star}) \\rvert = 1$, the test is inconclusive and a more detailed analysis is required.\n\nThe roots $r_1$ and $r_2$ of $f(x)=0$ are, by definition, fixed points of any valid rearrangement $x=g(x)$. At these points, $e^{x^\\star} = 4x^\\star$.\n\n- **Map A**: $g_{\\mathrm{A}}(x) = \\dfrac{e^x}{4}$.\n  The derivative is $g'_{\\mathrm{A}}(x) = \\dfrac{e^x}{4}$.\n  - At the first root $r_1$: $g'_{\\mathrm{A}}(r_1) = \\dfrac{e^{r_1}}{4} = \\dfrac{4r_1}{4} = r_1$. Numerically, $r_1 \\approx 0.3574$. Thus, $\\lvert g'_{\\mathrm{A}}(r_1) \\rvert \\approx 0.3574 < 1$. The fixed point $r_1$ is attracting.\n  - At the second root $r_2$: $g'_{\\mathrm{A}}(r_2) = \\dfrac{e^{r_2}}{4} = \\dfrac{4r_2}{4} = r_2$. Numerically, $r_2 \\approx 2.1533$. Thus, $\\lvert g'_{\\mathrm{A}}(r_2) \\rvert \\approx 2.1533 > 1$. The fixed point $r_2$ is repelling.\n  Therefore, the iteration with $g_{\\mathrm{A}}(x)$ is expected to converge to $r_1$ if started sufficiently close, and to diverge from the vicinity of $r_2$.\n\n- **Map B**: $g_{\\mathrm{B}}(x) = \\ln(4x)$, defined for $x > 0$.\n  The derivative is $g'_{\\mathrm{B}}(x) = \\dfrac{1}{4x} \\cdot 4 = \\dfrac{1}{x}$.\n  - At the first root $r_1$: $\\lvert g'_{\\mathrm{B}}(r_1) \\rvert = \\dfrac{1}{r_1} \\approx \\dfrac{1}{0.3574} \\approx 2.798 > 1$. The fixed point $r_1$ is repelling.\n  - At the second root $r_2$: $\\lvert g'_{\\mathrm{B}}(r_2) \\rvert = \\dfrac{1}{r_2} \\approx \\dfrac{1}{2.1533} \\approx 0.4644 < 1$. The fixed point $r_2$ is attracting.\n  This map exhibits behavior complementary to $g_{\\mathrm{A}}(x)$. The iteration is expected to converge to $r_2$ and diverge from $r_1$.\n\n- **Map N (Newton's Method)**: $g_{\\mathrm{N}}(x) = x - \\dfrac{f(x)}{f'(x)}$.\n  Here, $f(x) = e^x - 4x$ and $f'(x) = e^x - 4$. The second derivative is $f''(x) = e^x$.\n  The derivative of the fixed-point map is $g'_{\\mathrm{N}}(x) = \\dfrac{f(x)f''(x)}{(f'(x))^2}$.\n  At any simple root $x^{\\star}$ (where $f(x^{\\star}) = 0$ and $f'(x^{\\star}) \\neq 0$), the derivative is $g'_{\\mathrm{N}}(x^{\\star}) = 0$.\n  For our problem, $f'(r_1) = e^{r_1} - 4 = 4r_1 - 4 \\neq 0$ and $f'(r_2) = e^{r_2} - 4 = 4r_2 - 4 \\neq 0$, so both roots are simple.\n  The condition $g'_{\\mathrm{N}}(x^{\\star}) = 0$ implies quadratic convergence, which is extremely rapid. Both $r_1$ and $r_2$ are super-attracting fixed points for Newton's method. Failure occurs if the iteration encounters a point where $f'(x) = 0$, which is at $x = \\ln(4)$. This point corresponds to the local minimum of $f(x)$.\n\n**3. Algorithmic Implementation and Execution**\n\nThe solution will be implemented in a Python program following a structured, deterministic procedure.\n1.  **Initialization**: Define all constants: convergence tolerances $\\varepsilon_x = 10^{-12}$ and $\\varepsilon_f = 10^{-12}$, maximum iterations $N_{\\max} = 200$, Newton's method derivative threshold $\\tau = 10^{-12}$, and classification tolerance $10^{-8}$.\n2.  **Reference Root Calculation**: The roots $r_1$ and $r_2$ are computed using `scipy.optimize.brentq` to a precision greater than required by the problem.\n3.  **Function Definitions**: The functions $f(x)$, $f'(x)$, $g_{\\mathrm{A}}(x)$, $g_{\\mathrm{B}}(x)$, and $g_{\\mathrm{N}}(x)$ are implemented. The implementations for $g_{\\mathrm{B}}$ and $g_{\\mathrm{N}}$ include internal checks that raise exceptions for domain violations ($x_k \\leq 0$ for $g_{\\mathrm{B}}$) or an ill-conditioned update ($\\lvert f'(x_k) \\rvert < \\tau$ for $g_{\\mathrm{N}}$), as specified.\n4.  **Iteration Solver**: A master function executes the fixed-point iteration $x_{k+1} = g(x_k)$ for a given map $g$ and initial guess $x_0$. This function runs for a maximum of $N_{\\max}$ steps. In each step, it checks for convergence using the criteria $\\lvert x_{k+1} - x_k \\rvert \\leq \\varepsilon_x$ or $\\lvert f(x_{k+1}) \\rvert \\leq \\varepsilon_f$. It also handles failures due to non-finite results or exceptions raised by the map functions. The function returns a status ('converged' or 'failed') and the final iterate.\n5.  **Classification**: A separate classification function takes the output of the solver. If the status is 'failed', it returns $-1$. If 'converged', it compares the final iterate $x^{\\star}$ against the reference roots $r_1$ and $r_2$ using the $\\lvert x^{\\star} - r \\rvert \\leq 10^{-8}$ criterion and returns $0$, $1$, or $-1$ accordingly.\n6.  **Test Suite Execution**: The program iterates through the provided list of test cases. For each $(g, x_0)$ pair, it calls the solver and classifier, handling the edge case where an initial guess is already outside the domain of a map (e.g., $x_0 \\leq 0$ for $g_{\\mathrm{B}}$).\n7.  **Final Output**: The integer classification results are collected in a list and printed in the specified format: `[c_1,c_2,...,c_N]`.\n\nThis systematic approach ensures that all problem requirements are met precisely and robustly.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the given problem by comparing fixed-point iterations for f(x) = e^x - 4x = 0.\n    \"\"\"\n    # Define problem parameters\n    EPS_X = 1e-12\n    EPS_F = 1e-12\n    MAX_ITER = 200\n    DERIV_THRESHOLD = 1e-12\n    CLASSIFICATION_TOL = 1e-8\n    ROOT_FINDING_TOL = 1e-15\n\n    # Task 1  4: Implement f(x) and compute high-accuracy reference roots.\n    # The nonlinear function f(x) = e^x - 4x.\n    def f(x):\n        try:\n            val = np.exp(x) - 4.0 * x\n        except OverflowError:\n            val = np.inf\n        return val\n\n    # Its derivative f'(x) = e^x - 4.\n    def f_prime(x):\n        try:\n            val = np.exp(x) - 4.0\n        except OverflowError:\n            val = np.inf\n        return val\n\n    # Use a robust bracketed method to find reference roots r1 and r2.\n    r1 = brentq(f, 0.0, 0.5, xtol=ROOT_FINDING_TOL)\n    r2 = brentq(f, 2.0, 3.0, xtol=ROOT_FINDING_TOL)\n\n    # Task 2: Implement the three fixed-point maps.\n    # g_A(x) = e^x / 4\n    def g_A(x):\n        return np.exp(x) / 4.0\n\n    # g_B(x) = ln(4x), defined for x  0.\n    def g_B(x):\n        if x = 0:\n            raise ValueError(\"Domain error for g_B: x must be > 0.\")\n        return np.log(4.0 * x)\n\n    # g_N(x) = x - f(x)/f'(x) (Newton's method).\n    def g_N(x):\n        f_prime_val = f_prime(x)\n        if abs(f_prime_val)  DERIV_THRESHOLD:\n            raise ValueError(\"Derivative near zero for g_N.\")\n        return x - f(x) / f_prime_val\n\n    g_map = {'A': g_A, 'B': g_B, 'N': g_N}\n\n    # Task 3  5: Implement the iteration and classification logic.\n    def run_iteration_and_classify(g_name, x0):\n        # Handle cases where initial guess is invalid.\n        if g_name == 'B' and x0 = 0:\n            return -1\n        if g_name == 'N' and abs(f_prime(x0))  DERIV_THRESHOLD:\n            return -1\n\n        g_func = g_map[g_name]\n        x_k = x0\n\n        for _ in range(MAX_ITER):\n            try:\n                x_k_plus_1 = g_func(x_k)\n            except (ValueError, OverflowError):\n                # Catches domain errors from g_B, derivative error from g_N,\n                # and overflow in exp().\n                return -1\n\n            # Check for failure due to non-finite values.\n            if not np.isfinite(x_k_plus_1):\n                return -1\n\n            # Check for convergence.\n            converged_x_step = abs(x_k_plus_1 - x_k) = EPS_X\n            converged_f_val = abs(f(x_k_plus_1)) = EPS_F\n            \n            if converged_x_step or converged_f_val:\n                x_star = x_k_plus_1\n                # Classify the converged root.\n                if abs(x_star - r1) = CLASSIFICATION_TOL:\n                    return 0\n                elif abs(x_star - r2) = CLASSIFICATION_TOL:\n                    return 1\n                else:\n                    return -1 # Converged to a non-target or with insufficient precision for classification\n            \n            x_k = x_k_plus_1\n\n        # Failure due to reaching max iterations.\n        return -1\n\n    # Task 6: Define and run the test suite.\n    test_cases = [\n        ('A', -1.0),\n        ('A', 0.1),\n        ('A', 0.6),\n        ('A', 2.5),\n        ('B', 0.2),\n        ('B', 2.5),\n        ('B', 1.0),\n        ('B', -0.5), # Fails on initial condition\n        ('N', -1.0),\n        ('N', 0.5),\n        ('N', 4.0),\n        ('N', np.log(4.0)) # Fails on initial condition\n    ]\n\n    results = []\n    for g_name, x0 in test_cases:\n        result = run_iteration_and_classify(g_name, x0)\n        results.append(result)\n\n    # Task 7: Output a single line in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function.\nsolve()\n```", "id": "2393371"}, {"introduction": "Moving from abstract examples to real-world applications is a crucial step in mastering computational techniques. This exercise applies fixed-point iteration to solve the Colebrook-White equation, a cornerstone of hydraulics engineering for determining fluid friction in pipes, which cannot be solved analytically [@problem_id:2393339]. Furthermore, you will compare the standard iterative approach with Steffensen's method, a powerful acceleration technique that demonstrates how a clever algorithmic enhancement can dramatically reduce the number of iterations required for convergence. This practice underscores the importance of not just finding a solution, but finding it efficiently.", "problem": "Consider the Darcy friction factor, denoted by $f$, for fully turbulent internal flow in a circular pipe. The implicit Colebrook-White relation defines $f$ for a given Reynolds number $Re$ (dimensionless) and relative roughness $k$ (dimensionless), as\n$$\n\\frac{1}{\\sqrt{f}} = -2 \\log_{10}\\!\\left(\\frac{k}{3.7} + \\frac{2.51}{Re \\sqrt{f}}\\right).\n$$\nHere $\\log_{10}$ denotes the base-$10$ logarithm, $Re \\ge 4000$, and $k \\ge 0$. The unknown $f$ is dimensionless and strictly positive.\n\nWrite a complete, runnable program that, for each test case listed below, computes the unique physically relevant root $f$ of the Colebrook-White relation using two approaches:\n- a standard fixed-point iteration applied to a mathematically consistent fixed-point formulation of the implicit equation, and\n- Steffensen’s acceleration applied to the same fixed-point mapping.\n\nFor each approach and each test case, produce:\n- the converged value of $f$ rounded to $12$ decimal places,\n- the total number of iteration steps taken (an integer count), and\n- the total number of fixed-point mapping evaluations performed (an integer count).\n\nAdopt the following convergence and runtime controls for both approaches:\n- absolute difference criterion on successive friction factor estimates: terminate when $|f_{n+1} - f_n|  \\tau$ with $\\tau = 10^{-12}$,\n- a hard cap on the number of allowed iteration steps: at most $N_{\\max}^{(\\mathrm{std})} = 200$ for the standard fixed-point method and at most $N_{\\max}^{(\\mathrm{st})} = 100$ for Steffensen’s method.\n\nTest suite (each pair is $(Re,k)$, both dimensionless):\n- Case $1$: $(Re, k) = (4 \\times 10^{3}, 0)$,\n- Case $2$: $(Re, k) = (10^{5}, 10^{-4})$,\n- Case $3$: $(Re, k) = (10^{6}, 10^{-3})$,\n- Case $4$: $(Re, k) = (10^{7}, 5 \\times 10^{-3})$,\n- Case $5$: $(Re, k) = (5 \\times 10^{4}, 5 \\times 10^{-5})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case reported as a nested list in the order of the cases above.\n- For each test case, the nested list must be ordered as\n$[f_{\\mathrm{std}}, n_{\\mathrm{std}}, e_{\\mathrm{std}}, f_{\\mathrm{st}}, n_{\\mathrm{st}}, e_{\\mathrm{st}}]$,\nwhere $f_{\\mathrm{std}}$ and $f_{\\mathrm{st}}$ are the friction factors rounded to $12$ decimal places, $n_{\\mathrm{std}}$ and $n_{\\mathrm{st}}$ are iteration counts, and $e_{\\mathrm{std}}$ and $e_{\\mathrm{st}}$ are fixed-point mapping evaluation counts. The single output line must contain no whitespace. For example:\n\"[ [0.020000000000,12,12,0.020000000000,4,8], ... ]\" (this example is illustrative only).", "solution": "The problem as stated is valid. It concerns the numerical solution of the Colebrook-White equation, a well-established and fundamental relation in fluid mechanics for determining the friction factor in turbulent pipe flow. The problem is scientifically grounded, well-posed, and all parameters and conditions are specified with sufficient clarity to permit a unique and verifiable solution.\n\nThe task is to find the root $f$ of the implicit equation for a given Reynolds number $Re$ and relative roughness $k$:\n$$\n\\frac{1}{\\sqrt{f}} = -2 \\log_{10}\\!\\left(\\frac{k}{3.7} + \\frac{2.51}{Re \\sqrt{f}}\\right)\n$$\nThis must be accomplished using two iterative numerical methods.\n\n**1. Fixed-Point Formulation**\nTo apply fixed-point iteration, the equation must be rearranged into the form $f = g(f)$. A stable and convergent mapping $g(f)$ can be derived by isolating $f$ on the left-hand side. Let the right-hand side of the Colebrook-White equation be denoted as $A(f)$.\n$$\nA(f) = -2 \\log_{10}\\!\\left(\\frac{k}{3.7} + \\frac{2.51}{Re \\sqrt{f}}\\right)\n$$\nThen the equation is $\\frac{1}{\\sqrt{f}} = A(f)$. Solving for $f$ yields:\n$$\n\\sqrt{f} = \\frac{1}{A(f)} \\implies f = \\left(\\frac{1}{A(f)}\\right)^2\n$$\nThis gives the fixed-point mapping function $g(f)$:\n$$\ng(f) = \\left(\\frac{1}{-2 \\log_{10}\\!\\left(\\frac{k}{3.7} + \\frac{2.51}{Re \\sqrt{f}}\\right)}\\right)^2\n$$\nThe corresponding iterative scheme is $f_{n+1} = g(f_n)$, where $f_n$ is the estimate of the friction factor at iteration $n$.\n\n**2. Initial Guess**\nAn initial guess, $f_0$, is required to start the iteration. A robust choice for $f_0$ can be obtained from an explicit approximation of the Colebrook-White equation. The Haaland equation provides such an approximation:\n$$\n\\frac{1}{\\sqrt{f_0}} \\approx -1.8 \\log_{10}\\!\\left(\\left(\\frac{k}{3.7}\\right)^{1.11} + \\frac{6.9}{Re}\\right)\n$$\nSolving for $f_0$ gives a high-quality initial value that is close to the true root, ensuring rapid convergence.\n\n**3. Numerical Schemes and Convergence Criteria**\n\n**a) Standard Fixed-Point Iteration**\nThe standard fixed-point iteration algorithm proceeds as follows:\n1.  Start with an initial guess $f_0$.\n2.  For $n = 0, 1, 2, \\ldots$, compute the next estimate using the mapping: $f_{n+1} = g(f_n)$.\n3.  The process is terminated when the absolute difference between successive estimates is less than a specified tolerance $\\tau = 10^{-12}$, i.e., $|f_{n+1} - f_n|  \\tau$.\n4.  The iteration count, $n_{\\mathrm{std}}$, is the total number of times the mapping $f_{n+1} = g(f_n)$ is computed.\n5.  The evaluation count, $e_{\\mathrm{std}}$, is the total number of calls to the function $g(f)$. For this method, $e_{\\mathrm{std}} = n_{\\mathrm{std}}$. The process is also capped at $N_{\\max}^{(\\mathrm{std})} = 200$ iterations.\n\n**b) Steffensen's Method**\nSteffensen's method is an acceleration technique that typically converts the linear convergence of a fixed-point iteration into quadratic convergence. It uses three points from the underlying fixed-point sequence to extrapolate a better estimate.\n1.  Start with an initial guess $f_0$.\n2.  For each step $n$:\n    a. Let the current estimate be $p_0 = f_n$.\n    b. Generate two intermediate values using the mapping $g$: $p_1 = g(p_0)$ and $p_2 = g(p_1)$.\n    c. Compute the accelerated update using the Aitken's $\\Delta^2$ formula:\n    $$\n    f_{n+1} = p_0 - \\frac{(p_1 - p_0)^2}{p_2 - 2p_1 + p_0}\n    $$\n3.  Termination is based on the same criterion: $|f_{n+1} - f_n|  \\tau = 10^{-12}$.\n4.  One full cycle of the above constitutes a single iteration step for Steffensen's method. The iteration count is $n_{\\mathrm{st}}$.\n5.  Since each step requires two evaluations of the mapping function $g(f)$, the total evaluation count is $e_{\\mathrm{st}} = 2 \\times n_{\\mathrm{st}}$. The process is capped at $N_{\\max}^{(\\mathrm{st})} = 100$ iterations.\n\nThe implementation will proceed by defining functions for the mapping $g(f)$, the initial guess, and the two iterative solvers. These will then be applied to each test case to generate the required results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Colebrook-White equation for multiple test cases using both\n    standard fixed-point iteration and Steffensen's acceleration.\n    \"\"\"\n\n    # Test suite: each element is a tuple (Reynolds number, relative roughness)\n    test_cases = [\n        (4e3, 0.0),\n        (1e5, 1e-4),\n        (1e6, 1e-3),\n        (1e7, 5e-3),\n        (5e4, 5e-5),\n    ]\n\n    # Convergence and runtime controls\n    TOLERANCE = 1e-12\n    MAX_ITER_STD = 200\n    MAX_ITER_ST = 100\n\n    def g(f, Re, k):\n        \"\"\"\n        The fixed-point mapping function g(f) derived from the Colebrook-White equation.\n        \"\"\"\n        if f = 0:\n            # Physically, f must be positive. This avoids domain errors.\n            return np.inf\n        \n        log_arg = k / 3.7 + 2.51 / (Re * np.sqrt(f))\n        \n        if log_arg = 0:\n            # Argument to log10 must be positive.\n            return np.inf\n            \n        return (-2.0 * np.log10(log_arg))**-2.0\n\n    def haaland_initial_guess(Re, k):\n        \"\"\"\n        Calculates an initial guess for f using the Haaland equation.\n        \"\"\"\n        if Re  4000:\n            # Haaland is for turbulent flow\n            return 64.0 / Re  # Laminar flow friction factor, as a fallback\n        \n        # Note: (k/3.7)**1.11 is safe since k >= 0\n        rhs = -1.8 * np.log10((k / 3.7)**1.11 + 6.9 / Re)\n        f0 = (1.0 / rhs)**2\n        return f0\n\n    def standard_fixed_point(Re, k, f_initial):\n        \"\"\"\n        Performs standard fixed-point iteration.\n        Returns: (converged_f, iteration_count, evaluation_count)\n        \"\"\"\n        f_n = f_initial\n        eval_count = 0\n        for i in range(1, MAX_ITER_STD + 1):\n            f_n_plus_1 = g(f_n, Re, k)\n            eval_count += 1\n            if abs(f_n_plus_1 - f_n)  TOLERANCE:\n                return f_n_plus_1, i, eval_count\n            f_n = f_n_plus_1\n        # Return last value if not converged\n        return f_n, MAX_ITER_STD, eval_count\n\n    def steffensen_method(Re, k, f_initial):\n        \"\"\"\n        Performs Steffensen's accelerated fixed-point iteration.\n        Returns: (converged_f, iteration_count, evaluation_count)\n        \"\"\"\n        f_n = f_initial\n        eval_count = 0\n        for i in range(1, MAX_ITER_ST + 1):\n            p0 = f_n\n            \n            p1 = g(p0, Re, k)\n            p2 = g(p1, Re, k)\n            eval_count += 2\n            \n            denominator = p2 - 2 * p1 + p0\n            \n            if abs(denominator)  1e-20:  # Avoid division by zero, fallback to a standard step\n                f_n_plus_1 = p2\n            else:\n                f_n_plus_1 = p0 - (p1 - p0)**2 / denominator\n\n            if abs(f_n_plus_1 - f_n)  TOLERANCE:\n                return f_n_plus_1, i, eval_count\n            f_n = f_n_plus_1\n        # Return last value if not converged\n        return f_n, MAX_ITER_ST, eval_count\n\n    all_results_str = []\n    for Re, k in test_cases:\n        f0 = haaland_initial_guess(Re, k)\n        \n        # Standard Fixed-Point Iteration\n        f_std, n_std, e_std = standard_fixed_point(Re, k, f0)\n        \n        # Steffensen's Method\n        f_st, n_st, e_st = steffensen_method(Re, k, f0)\n        \n        # Format results for the current case\n        f_std_str = f\"{f_std:.12f}\"\n        f_st_str = f\"{f_st:.12f}\"\n        \n        case_str = f\"[{f_std_str},{n_std},{e_std},{f_st_str},{n_st},{e_st}]\"\n        all_results_str.append(case_str)\n        \n    # Final output string with no whitespace\n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2393339"}, {"introduction": "Most complex engineering systems are described not by a single equation, but by systems of coupled nonlinear equations. This advanced practice extends the concept of fixed-point iteration to the vector-valued case, revealing a fascinating and non-intuitive aspect of algorithm design [@problem_id:2393400]. You will implement and analyze two fundamental approaches, the Jacobi (simultaneous update) and Gauss-Seidel (sequential update) iterations, and discover a scenario where one converges while the other diverges spectacularly. This exercise provides a crucial lesson on how the structure of an iterative algorithm, particularly the flow of information between components, governs the stability and convergence of the entire system.", "problem": "You are asked to exhibit a concrete, explicit two-dimensional non-linear fixed-point problem and to analyze and verify, both theoretically and computationally, that the Jacobi-style update converges locally while the Gauss-Seidel-style update diverges locally. Work entirely in purely mathematical and algorithmic terms.\n\nConsider the parametric family of maps $g_{\\varepsilon} : \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by\n$$\ng_{\\varepsilon}(\\mathbf{x}) =\n\\begin{bmatrix}\ng_{1,\\varepsilon}(x_1,x_2) \\\\\ng_{2,\\varepsilon}(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na\\,x_1 + b\\,x_2 + \\varepsilon\\,x_1^2 \\\\\nc\\,x_1 + d\\,x_2 + \\varepsilon\\,x_2^2\n\\end{bmatrix},\n$$\nwith fixed coefficients $a=2$, $b=-1$, $c=2$, $d=-0.6$, and a scalar nonlinearity parameter $\\varepsilon \\ge 0$. Note that $\\mathbf{x}^{\\star}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ is a fixed point for all $\\varepsilon$, since $g_{\\varepsilon}(\\mathbf{0})=\\mathbf{0}$.\n\nDefine the two iteration styles for computing a fixed point of $g_{\\varepsilon}$:\n\n- Jacobi-style (simultaneous) fixed-point iteration: given $\\mathbf{x}^{(k)}=\\begin{bmatrix}x_1^{(k)} \\\\ x_2^{(k)}\\end{bmatrix}$, compute\n$$\n\\mathbf{x}^{(k+1)}=\n\\begin{bmatrix}\ng_{1,\\varepsilon}\\!\\left(x_1^{(k)},x_2^{(k)}\\right) \\\\\ng_{2,\\varepsilon}\\!\\left(x_1^{(k)},x_2^{(k)}\\right)\n\\end{bmatrix}.\n$$\n\n- Gauss-Seidel-style (sequential) fixed-point iteration: given $\\mathbf{x}^{(k)}=\\begin{bmatrix}x_1^{(k)} \\\\ x_2^{(k)}\\end{bmatrix}$, compute\n$$\nx_1^{(k+1)} = g_{1,\\varepsilon}\\!\\left(x_1^{(k)},x_2^{(k)}\\right), \\quad\nx_2^{(k+1)} = g_{2,\\varepsilon}\\!\\left(x_1^{(k+1)},x_2^{(k)}\\right).\n$$\n\nYour tasks are:\n\n1) Starting exclusively from core definitions of local linearization and fixed-point stability, derive the local convergence or divergence conditions near $\\mathbf{x}^{\\star}$ for both iteration styles. Express these conditions in terms of the Jacobian matrix of the Jacobi iteration map at $\\mathbf{x}^{\\star}$ and the Jacobian matrix of the Gauss-Seidel composition map at $\\mathbf{x}^{\\star}$. Do not assume any pre-packaged “shortcut formula”; derive what the relevant Jacobian matrices are and how their spectral radii govern local behavior.\n\n2) Specialize your derivation to the specific coefficients $a=2$, $b=-1$, $c=2$, $d=-0.6$, and show that there exists a neighborhood of $\\mathbf{x}^{\\star}$ in which the Jacobi-style iteration is locally convergent while the Gauss-Seidel-style iteration is locally divergent, for all sufficiently small $\\varepsilon \\ge 0$.\n\n3) Implement a program that:\n   - Encodes $g_{\\varepsilon}$ with the above coefficients.\n   - Implements both Jacobi-style and Gauss-Seidel-style iterations.\n   - Uses the Euclidean norm to assess convergence and divergence with the following decision rules:\n     • Converges if $\\|\\mathbf{x}^{(k)}\\|_2 \\le \\tau$ for some iteration $k$ within a maximum of $N_{\\max}$ steps.  \n     • Diverges if $\\|\\mathbf{x}^{(k)}\\|_2 \\ge M$ for some iteration $k$ within $N_{\\max}$ steps, or if any component becomes not-a-number.  \n     • If neither condition is met within $N_{\\max}$ steps, treat it as “no decision”; for the purpose of this problem, such a case must be reported as non-convergent and non-divergent in the requested boolean combinations below.\n   - For each test case below, returns a boolean that is true if and only if the Jacobi-style iteration converges and the Gauss-Seidel-style iteration diverges under the stated thresholds.\n\nUse the following fixed parameters for all tests: tolerance $\\tau = 10^{-10}$, maximum iterations $N_{\\max}=200$, divergence threshold $M=10^{6}$.\n\nTest suite:\n- Test $1$: $\\varepsilon=0.0$, initial vector $\\mathbf{x}^{(0)}=\\begin{bmatrix}10^{-3} \\\\ -10^{-3}\\end{bmatrix}$.\n- Test $2$: $\\varepsilon=0.01$, initial vector $\\mathbf{x}^{(0)}=\\begin{bmatrix}10^{-2} \\\\ 10^{-2}\\end{bmatrix}$.\n- Test $3$: $\\varepsilon=0.05$, initial vector $\\mathbf{x}^{(0)}=\\begin{bmatrix}5\\cdot 10^{-2} \\\\ -2\\cdot 10^{-2}\\end{bmatrix}$.\n- Test $4$: $\\varepsilon=0.01$, initial vector $\\mathbf{x}^{(0)}=\\begin{bmatrix}10^{-1} \\\\ 10^{-1}\\end{bmatrix}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests above. Each entry must be a boolean equal to true if and only if the Jacobi-style iteration converges and the Gauss-Seidel-style iteration diverges for that test, and false otherwise. For example, a valid output looks like\n“[True,False,True,True]”.\nThere are no physical units or angles involved in this problem.", "solution": "The problem as stated is mathematically and algorithmically well-defined, internally consistent, and scientifically grounded in the theory of numerical analysis. It requires the derivation of convergence criteria from first principles and their subsequent computational verification. The problem is valid and a complete solution will be provided.\n\nThe analysis is structured into two main parts: first, the theoretical derivation of local stability conditions for the Jacobi-style and Gauss-Seidel-style iterations; second, the specialization of these conditions to the given parametric map to demonstrate the required convergence-divergence behavior.\n\n**Part 1: Derivation of Local Convergence Conditions**\n\nThe local stability of a fixed-point iteration $\\mathbf{x}^{(k+1)} = G(\\mathbf{x}^{(k)})$ near a fixed point $\\mathbf{x}^{\\star}$ (where $G(\\mathbf{x}^{\\star}) = \\mathbf{x}^{\\star}$) is determined by the behavior of the error vector $\\mathbf{e}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^{\\star}$ over successive iterations.\n\nWe perform a first-order Taylor series expansion of the iteration map $G$ around the fixed point $\\mathbf{x}^{\\star}$:\n$$\n\\mathbf{x}^{(k+1)} = G(\\mathbf{x}^{(k)}) = G(\\mathbf{x}^{\\star} + \\mathbf{e}^{(k)}) \\approx G(\\mathbf{x}^{\\star}) + J_G(\\mathbf{x}^{\\star}) \\mathbf{e}^{(k)}\n$$\nwhere $J_G(\\mathbf{x}^{\\star})$ is the Jacobian matrix of the map $G$ evaluated at $\\mathbf{x}^{\\star}$.\n\nSince $G(\\mathbf{x}^{\\star}) = \\mathbf{x}^{\\star}$ and $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{\\star} + \\mathbf{e}^{(k+1)}$, the relation becomes:\n$$\n\\mathbf{x}^{\\star} + \\mathbf{e}^{(k+1)} \\approx \\mathbf{x}^{\\star} + J_G(\\mathbf{x}^{\\star}) \\mathbf{e}^{(k)}\n$$\nThis simplifies to the linearized error propagation equation:\n$$\n\\mathbf{e}^{(k+1)} \\approx J_G(\\mathbf{x}^{\\star}) \\mathbf{e}^{(k)}\n$$\nFor the iteration to be locally convergent, the error must vanish as $k \\to \\infty$. This requires the linear operator $J_G(\\mathbf{x}^{\\star})$ to be a contraction in a neighborhood of the origin. This condition is met if and only if the spectral radius of the Jacobian matrix, $\\rho(J_G(\\mathbf{x}^{\\star}))$, is strictly less than $1$. If $\\rho(J_G(\\mathbf{x}^{\\star}))  1$, the iteration is locally divergent.\n\nThe analysis proceeds by deriving the effective iteration map $G$ and its Jacobian for both the Jacobi and Gauss-Seidel schemes. The given map is:\n$$\ng_{\\varepsilon}(\\mathbf{x}) =\n\\begin{bmatrix}\ng_{1,\\varepsilon}(x_1, x_2) \\\\\ng_{2,\\varepsilon}(x_1, x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na\\,x_1 + b\\,x_2 + \\varepsilon\\,x_1^2 \\\\\nc\\,x_1 + d\\,x_2 + \\varepsilon\\,x_2^2\n\\end{bmatrix}\n$$\nThe fixed point is $\\mathbf{x}^{\\star} = \\mathbf{0}$. The Jacobian of $g_{\\varepsilon}$ is:\n$$\nJ_{g_\\varepsilon}(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial g_{1,\\varepsilon}}{\\partial x_1}  \\frac{\\partial g_{1,\\varepsilon}}{\\partial x_2} \\\\ \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_1}  \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} a + 2\\varepsilon x_1  b \\\\ c  d + 2\\varepsilon x_2 \\end{bmatrix}\n$$\nAt the fixed point $\\mathbf{x}^{\\star} = \\mathbf{0}$, this Jacobian simplifies to:\n$$\nJ_{g_\\varepsilon}(\\mathbf{0}) = \\begin{bmatrix} a  b \\\\ c  d \\end{bmatrix}\n$$\nThis matrix is independent of the parameter $\\varepsilon$.\n\n**Jacobi-Style Iteration**\nThe Jacobi iteration map is simply $G_J(\\mathbf{x}) = g_{\\varepsilon}(\\mathbf{x})$. Therefore, the relevant Jacobian for local stability analysis is the Jacobian of $g_{\\varepsilon}$ evaluated at the fixed point:\n$$\nJ_{G_J}(\\mathbf{x}^{\\star}) = J_{g_\\varepsilon}(\\mathbf{0}) = \\begin{bmatrix} a  b \\\\ c  d \\end{bmatrix}\n$$\nThe condition for local convergence is $\\rho\\left(\\begin{bmatrix} a  b \\\\ c  d \\end{bmatrix}\\right)  1$.\n\n**Gauss-Seidel-Style Iteration**\nThe Gauss-Seidel iteration is a composition of updates. The map $G_{GS}: \\mathbf{x}^{(k)} \\mapsto \\mathbf{x}^{(k+1)}$ is defined by:\n$$\n\\begin{cases}\nx_1^{(k+1)} = g_{1,\\varepsilon}(x_1^{(k)}, x_2^{(k)}) \\\\\nx_2^{(k+1)} = g_{2,\\varepsilon}(x_1^{(k+1)}, x_2^{(k)})\n\\end{cases}\n$$\nTo find the Jacobian of this composite map $G_{GS}$, we compute the partial derivatives of the components of $\\mathbf{x}^{(k+1)}$ with respect to the components of $\\mathbf{x}^{(k)}$, and evaluate them at $\\mathbf{x}^{(k)} = \\mathbf{x}^{\\star} = \\mathbf{0}$. Let $\\mathbf{x}' = G_{GS}(\\mathbf{x})$.\n\nThe partial derivatives of $x_1'$ are straightforward:\n$$\n\\frac{\\partial x_1'}{\\partial x_1} = \\frac{\\partial g_{1,\\varepsilon}}{\\partial x_1}, \\quad \\frac{\\partial x_1'}{\\partial x_2} = \\frac{\\partial g_{1,\\varepsilon}}{\\partial x_2}\n$$\nThe partial derivatives of $x_2'$ require the chain rule, as $x_2'$ depends on $x_1'$:\n$$\n\\frac{\\partial x_2'}{\\partial x_1} = \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_1'} \\frac{\\partial x_1'}{\\partial x_1} + \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_2} \\frac{\\partial x_2}{\\partial x_1} = \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_1'} \\frac{\\partial g_{1,\\varepsilon}}{\\partial x_1}\n$$\n$$\n\\frac{\\partial x_2'}{\\partial x_2} = \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_1'} \\frac{\\partial x_1'}{\\partial x_2} + \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_2} \\frac{\\partial x_2}{\\partial x_2} = \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_1'} \\frac{\\partial g_{1,\\varepsilon}}{\\partial x_2} + \\frac{\\partial g_{2,\\varepsilon}}{\\partial x_2}\n$$\nEvaluating these at $\\mathbf{x} = \\mathbf{x}^{\\star} = \\mathbf{0}$, we note that $x_1' = g_{1,\\varepsilon}(\\mathbf{0}) = 0$. Thus, all partial derivatives of $g_{1,\\varepsilon}$ and $g_{2,\\varepsilon}$ are evaluated at $\\mathbf{0}$. Using the notation $\\left. \\frac{\\partial g_{i,\\varepsilon}}{\\partial x_j} \\right|_{\\mathbf{0}} = J_{ij}(\\mathbf{0})$, the Jacobian of the Gauss-Seidel map at the fixed point is:\n$$\nJ_{G_{GS}}(\\mathbf{x}^{\\star}) =\n\\begin{bmatrix}\nJ_{11}(\\mathbf{0})  J_{12}(\\mathbf{0}) \\\\\nJ_{21}(\\mathbf{0}) J_{11}(\\mathbf{0})  J_{21}(\\mathbf{0}) J_{12}(\\mathbf{0}) + J_{22}(\\mathbf{0})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na  b \\\\\nca  cb + d\n\\end{bmatrix}\n$$\nThe condition for local convergence of the Gauss-Seidel iteration is $\\rho\\left(\\begin{bmatrix} a  b \\\\ ca  cb+d \\end{bmatrix}\\right)  1$.\n\n**Part 2: Specialization and Analysis**\n\nWe now apply these derived conditions to the specific coefficients: $a=2$, $b=-1$, $c=2$, $d=-0.6$. The analysis of local behavior for sufficiently small $\\varepsilon \\ge 0$ depends only on the Jacobians at $\\mathbf{x}^{\\star}=\\mathbf{0}$, which are independent of $\\varepsilon$.\n\n**Jacobi-Style Analysis**\nThe iteration matrix is:\n$$\nJ_{G_J}(\\mathbf{x}^{\\star}) = \\begin{bmatrix} 2  -1 \\\\ 2  -0.6 \\end{bmatrix}\n$$\nThe eigenvalues $\\lambda$ are found from the characteristic equation $\\det(J_{G_J}(\\mathbf{x}^{\\star}) - \\lambda I) = 0$:\n$$\n(2-\\lambda)(-0.6-\\lambda) - (-1)(2) = \\lambda^2 - 1.4\\lambda - 1.2 + 2 = \\lambda^2 - 1.4\\lambda + 0.8 = 0\n$$\nThe roots are:\n$$\n\\lambda = \\frac{1.4 \\pm \\sqrt{(-1.4)^2 - 4(1)(0.8)}}{2} = \\frac{1.4 \\pm \\sqrt{1.96 - 3.2}}{2} = \\frac{1.4 \\pm \\sqrt{-1.24}}{2} = 0.7 \\pm i\\sqrt{0.31}\n$$\nThe spectral radius is the modulus of these complex conjugate eigenvalues:\n$$\n\\rho(J_{G_J}(\\mathbf{x}^{\\star})) = |\\lambda| = \\sqrt{(0.7)^2 + (\\sqrt{0.31})^2} = \\sqrt{0.49 + 0.31} = \\sqrt{0.8}\n$$\nSince $\\rho(J_{G_J}(\\mathbf{x}^{\\star})) = \\sqrt{0.8} \\approx 0.8944  1$, the Jacobi-style iteration is locally convergent.\n\n**Gauss-Seidel-Style Analysis**\nThe iteration matrix is:\n$$\nJ_{G_{GS}}(\\mathbf{x}^{\\star}) = \\begin{bmatrix} a  b \\\\ ca  cb+d \\end{bmatrix} = \\begin{bmatrix} 2  -1 \\\\ (2)(2)  (2)(-1) + (-0.6) \\end{bmatrix} = \\begin{bmatrix} 2  -1 \\\\ 4  -2.6 \\end{bmatrix}\n$$\nThe eigenvalues $\\lambda$ are found from the characteristic equation $\\det(J_{G_{GS}}(\\mathbf{x}^{\\star}) - \\lambda I) = 0$:\n$$\n(2-\\lambda)(-2.6-\\lambda) - (-1)(4) = \\lambda^2 + 0.6\\lambda - 5.2 + 4 = \\lambda^2 + 0.6\\lambda - 1.2 = 0\n$$\nThe roots are:\n$$\n\\lambda = \\frac{-0.6 \\pm \\sqrt{(0.6)^2 - 4(1)(-1.2)}}{2} = \\frac{-0.6 \\pm \\sqrt{0.36 + 4.8}}{2} = \\frac{-0.6 \\pm \\sqrt{5.16}}{2}\n$$\nThe two real eigenvalues are approximately:\n$$\n\\lambda_1 = \\frac{-0.6 + \\sqrt{5.16}}{2} \\approx \\frac{-0.6 + 2.2716}{2} \\approx 0.8358\n$$\n$$\n\\lambda_2 = \\frac{-0.6 - \\sqrt{5.16}}{2} \\approx \\frac{-0.6 - 2.2716}{2} \\approx -1.4358\n$$\nThe spectral radius is the maximum of their absolute values:\n$$\n\\rho(J_{G_{GS}}(\\mathbf{x}^{\\star})) = \\max(|\\lambda_1|, |\\lambda_2|) = |-1.4358| \\approx 1.4358\n$$\nSince $\\rho(J_{G_{GS}}(\\mathbf{x}^{\\star}))  1$, the Gauss-Seidel-style iteration is locally divergent.\n\nThis theoretical analysis proves that for this specific system, for any sufficiently small initial vector $\\mathbf{x}^{(0)}$ and for all sufficiently small $\\varepsilon \\ge 0$, the Jacobi iteration converges to the fixed point $\\mathbf{x}^{\\star}=\\mathbf{0}$, while the Gauss-Seidel iteration diverges. The computational implementation will verify this behavior.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given problem by analyzing and simulating Jacobi and Gauss-Seidel iterations.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    a = 2.0\n    b = -1.0\n    c = 2.0\n    d = -0.6\n    \n    tau = 1e-10\n    N_max = 200\n    M = 1e6\n\n    # --- Test Suite ---\n    test_cases = [\n        # (epsilon, initial_x)\n        (0.0, np.array([1e-3, -1e-3])),\n        (0.01, np.array([1e-2, 1e-2])),\n        (0.05, np.array([5e-2, -2e-2])),\n        (0.01, np.array([1e-1, 1e-1])),\n    ]\n\n    def jacobi_iteration(epsilon, x0):\n        \"\"\"\n        Performs Jacobi-style fixed-point iteration.\n        Returns 'converges', 'diverges', or 'no_decision'.\n        \"\"\"\n        x = x0.copy()\n        for _ in range(N_max):\n            x1_k, x2_k = x[0], x[1]\n            \n            # Apply the function g_epsilon(x_k) simultaneously\n            x_next = np.array([\n                a * x1_k + b * x2_k + epsilon * x1_k**2,\n                c * x1_k + d * x2_k + epsilon * x2_k**2\n            ])\n            \n            norm = np.linalg.norm(x_next)\n\n            # Check for NaN, which indicates divergence\n            if np.isnan(norm):\n                return 'diverges'\n            \n            # Check for divergence threshold\n            if norm >= M:\n                return 'diverges'\n            \n            # Check for convergence\n            if norm = tau:\n                return 'converges'\n            \n            x = x_next\n            \n        return 'no_decision'\n\n    def gauss_seidel_iteration(epsilon, x0):\n        \"\"\"\n        Performs Gauss-Seidel-style fixed-point iteration.\n        Returns 'converges', 'diverges', or 'no_decision'.\n        \"\"\"\n        x = x0.copy()\n        for _ in range(N_max):\n            x1_k, x2_k = x[0], x[1]\n\n            # Apply the updates sequentially\n            # x1^(k+1) = g1(x1^k, x2^k)\n            x1_next = a * x1_k + b * x2_k + epsilon * x1_k**2\n            \n            # Check for intermediate blow-up\n            if np.isnan(x1_next) or abs(x1_next) >= M:\n                return 'diverges'\n\n            # x2^(k+1) = g2(x1^(k+1), x2^k)\n            x2_next = c * x1_next + d * x2_k + epsilon * x2_k**2\n\n            x_next = np.array([x1_next, x2_next])\n            norm = np.linalg.norm(x_next)\n\n            # Check for NaN, which indicates divergence\n            if np.isnan(norm):\n                return 'diverges'\n            \n            # Check for divergence threshold\n            if norm >= M:\n                return 'diverges'\n            \n            # Check for convergence\n            if norm = tau:\n                return 'converges'\n            \n            x = x_next\n\n        return 'no_decision'\n\n    results = []\n    for epsilon, x0 in test_cases:\n        jacobi_res = jacobi_iteration(epsilon, x0)\n        gs_res = gauss_seidel_iteration(epsilon, x0)\n        \n        # The condition is True if and only if Jacobi converges AND Gauss-Seidel diverges.\n        # A 'no_decision' state means the specific condition (convergence or divergence) was not met.\n        is_jacobi_conv_and_gs_div = (jacobi_res == 'converges' and gs_res == 'diverges')\n        results.append(is_jacobi_conv_and_gs_div)\n\n    # Format the output as a string representation of a list of booleans\n    output_str = f\"[{','.join(str(r).title() for r in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2393400"}]}