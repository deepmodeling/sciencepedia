{"hands_on_practices": [{"introduction": "The most direct way to appreciate the power of linearization is to see it solve a challenging nonlinear equation. In this exercise, you will apply Newton's method, which approximates a function with a tangent line at each step to find its root. We will use this fundamental technique to tackle an exciting problem from cryptography, demonstrating how iterative linearization can unravel a seemingly complex algebraic constraint derived from a hypothetical side-channel attack [@problem_id:2398877].", "problem": "A public-key cryptosystem uses the Rivest–Shamir–Adleman (RSA) modulus defined by $N = p\\,q$, where $p$ and $q$ are unknown primes with $p \\le q$. A side-channel measurement reveals a nonlinear algebraic constraint between the prime factors: the squared-sum $p^2 + q^2$ equals a leaked value $L$. Your task is to compute $p$ by solving a single-variable nonlinear equation derived from the definitions and to implement a Newton linearization procedure to find the root.\n\nFundamental base:\n- By definition of the modulus, $N = p\\,q$.\n- The leak provides $p^2 + q^2 = L$.\n- The goal is to eliminate $q$ and solve for $p$ as the positive root of a scalar nonlinear equation, using Newton’s method (first-order Taylor linearization) with an update based on the derivative of the scalar function with respect to the scalar iterate.\n\nAlgorithmic requirements:\n- Eliminate $q$ using the definition of $N$ to obtain a univariate equation in the unknown $p$ that is consistent with the leak $p^2 + q^2 = L$.\n- Define a continuously differentiable scalar function $f(x)$ such that the true $p$ is a simple root of $f(x) = 0$.\n- Derive the exact derivative $f'(x)$ from first principles (elementary calculus) to support Newton’s method.\n- Implement Newton’s method with the iterate $x_{\\text{new}} = x - f(x)/f'(x)$.\n- Use a strictly positive initial guess $x_0$ equal to $\\sqrt{N}$.\n- Enforce positivity of the iterate and provide a reasonable stopping rule based on the change in $x$ or the residual of $f$ (whichever you prefer). Use a finite maximum iteration count to guarantee termination.\n- After convergence, map the numerical root to the integer factor by rounding only as the final step, and return the smaller prime factor. If the Newton estimate is closer to $q$, use the relation $q = N/p$ to still return the smaller prime $p$.\n\nTest suite:\nYour program must solve the following four independent instances. For each instance, the input is a pair $(N, L)$, and the required output is the smaller prime factor $p$ as an integer.\n\n- Instance A: $N = 17473$, $L = 40130$.\n- Instance B: $N = 272953$, $L = 548210$.\n- Instance C: $N = 119989$, $L = 1539578$.\n- Instance D: $N = 1005973$, $L = 2012090$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[p_A, p_B, p_C, p_D]$. For example, if the four computed smaller factors are $a$, $b$, $c$, and $d$, print $[a,b,c,d]$ on a single line.", "solution": "The problem statement is subjected to validation before any attempt at a solution.\n\n### Step 1: Extract Givens\n\n- Public-key cryptosystem with RSA modulus $N = p\\,q$.\n- $p$ and $q$ are unknown primes with $p \\le q$.\n- A side-channel leak provides a nonlinear algebraic constraint: $p^2 + q^2 = L$.\n- The objective is to compute the prime factor $p$.\n- The method prescribed is to solve a single-variable nonlinear equation for $p$ using a Newton linearization procedure.\n- The scalar nonlinear equation $f(x)=0$, where $p$ is a root, must be derived by eliminating $q$.\n- Newton's method is defined by the iterative update $x_{\\text{new}} = x - f(x)/f'(x)$.\n- The derivative $f'(x)$ must be derived from first principles.\n- The initial guess for the iteration is $x_0 = \\sqrt{N}$.\n- The iterate $x$ must remain positive.\n- A stopping rule is required, based on iterate change or residual, with a maximum iteration count to ensure termination.\n- The final numerical root must be mapped to an integer, and the smaller prime factor must be returned. If the estimate is closer to $q$, the smaller factor $p$ should be computed as $p = N/q$.\n- Test Suite:\n    - Instance A: $(N = 17473, L = 40130)$\n    - Instance B: $(N = 272953, L = 548210)$\n    - Instance C: $(N = 119989, L = 1539578)$\n    - Instance D: $(N = 1005973, L = 2012090)$\n- Required Output Format: A single line with a comma-separated list of results for each instance, enclosed in square brackets: $[p_A, p_B, p_C, p_D]$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the established criteria for validity.\n\n- **Scientifically Grounded**: The problem is based on fundamental principles of algebra (solving polynomial equations) and numerical analysis (Newton's method). The context of RSA cryptography and side-channel attacks is a valid and established topic in computational engineering and computer science. The mathematical relationships are sound.\n- **Well-Posed**: The problem is clearly stated. It provides sufficient information ($N$, $L$) and a specific, well-defined numerical method (Newton's method) to find a unique solution (the smaller prime factor $p$).\n- **Objective**: The problem is expressed in precise, objective mathematical language. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. The provided data ($N, L$) are sufficient for the derivation and solution. There are no internal contradictions. For a solution to exist, we must have real roots for $p$ and $q$, which requires the discriminant of a related quadratic equation to be non-negative. This implies $L^2 - 4N^2 \\ge 0$, or $L \\ge 2N$. This condition holds for all test instances, confirming their physical consistency.\n    - A: $L=40130$, $2N=34946$. $L > 2N$.\n    - B: $L=548210$, $2N=545906$. $L > 2N$.\n    - C: $L=1539578$, $2N=239978$. $L > 2N$.\n    - D: $L=2012090$, $2N=2011946$. $L > 2N$.\n- **Formalizability**: The problem is not metaphorical and is directly formalizable into a mathematical procedure. It correctly falls under the topic of handling nonlinearities via linearization.\n\nThe problem does not exhibit any flaws related to scientific unsoundness, non-formalizability, incompleteness, infeasibility, or being ill-posed.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe solution requires deriving a scalar nonlinear function $f(x)=0$ whose root is the desired prime factor $p$, and then applying Newton's method.\n\n**1. Derivation of the Nonlinear Equation**\n\nWe are given two fundamental equations:\n$$\nN = p\\,q\n$$\n$$\np^2 + q^2 = L\n$$\nFrom the first equation, we can express $q$ in terms of $p$ and $N$, assuming $p \\neq 0$, which is guaranteed for a prime factor:\n$$\nq = \\frac{N}{p}\n$$\nSubstituting this expression for $q$ into the second equation allows for the elimination of $q$:\n$$\np^2 + \\left(\\frac{N}{p}\\right)^2 = L\n$$\n$$\np^2 + \\frac{N^2}{p^2} = L\n$$\nTo obtain a polynomial form, we multiply the entire equation by $p^2$:\n$$\np^4 + N^2 = L p^2\n$$\nRearranging the terms yields a single-variable nonlinear equation in $p$:\n$$\np^4 - L p^2 + N^2 = 0\n$$\nWe define a continuously differentiable scalar function $f(x)$ whose roots correspond to the prime factors:\n$$\nf(x) = x^4 - L x^2 + N^2\n$$\nThe problem is now reduced to finding the smaller positive root of $f(x) = 0$.\n\n**2. Derivation of the Derivative for Newton's Method**\n\nNewton's method requires the derivative of $f(x)$, denoted $f'(x)$. Applying the power rule for differentiation from elementary calculus:\n$$\nf'(x) = \\frac{d}{dx} \\left( x^4 - L x^2 + N^2 \\right)\n$$\n$$\nf'(x) = 4x^3 - 2Lx\n$$\n\n**3. Newton's Method Implementation**\n\nThe iterative formula for Newton's method is:\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} = x_k - \\frac{x_k^4 - L x_k^2 + N^2}{4x_k^3 - 2Lx_k}\n$$\nThe problem specifies an initial guess $x_0 = \\sqrt{N}$. This is a logical choice. Since $p \\le q$, it follows that $p \\le \\sqrt{N} \\le q$. The initial guess is therefore located between the two positive roots of $f(x)=0$. The function $f(x)$ is a 'W'-shaped quartic function with a local maximum at $x=0$ and local minima at $x = \\pm\\sqrt{L/2}$. As established in the validation, $L \\ge 2N$, which implies $\\sqrt{L/2} \\ge \\sqrt{N}$. The initial guess $x_0 = \\sqrt{N}$ is in the interval $(p, \\sqrt{L/2})$. In this interval, both $f(x)$ and $f'(x)$ are negative, causing the term $f(x)/f'(x)$ to be positive. The update $x_{k+1} = x_k - (\\text{positive value})$ ensures that the sequence of iterates $x_k$ decreases monotonically and converges to the smaller root, $p$.\n\nThe iteration proceeds as follows:\n1.  Initialize $x_k = \\sqrt{N}$.\n2.  Iterate up to a maximum number of steps (e.g., $100$) to guarantee termination.\n3.  In each step, calculate the update $\\Delta x = f(x_k)/f'(x_k)$.\n4.  Update the estimate: $x_{k+1} = x_k - \\Delta x$.\n5.  Terminate if the magnitude of the update, $|\\Delta x|$, falls below a specified tolerance $\\epsilon$ (e.g., $10^{-9}$).\n6.  Once the iteration converges to a root $x^*$, this value is a floating-point approximation of $p$.\n\n**4. Final Result Calculation**\n\nThe converged root $x^*$ must be converted to the final integer answer.\n1.  Round the numerical root to the nearest integer to get an integer estimate of the factor: $p_{\\text{est}} = \\text{round}(x^*)$.\n2.  Calculate the corresponding other factor: $q_{\\text{est}} = \\text{round}(N / p_{\\text{est}})$.\n3.  The problem requires returning the smaller prime factor, $p$. The robust way to determine this is to take the minimum of the two estimates: $\\min(p_{\\text{est}}, q_{\\text{est}})$. This correctly identifies the smaller factor regardless of which root the numerical method might have converged to, though our choice of $x_0$ ensures convergence to $p$.\n\nThe procedure will be applied to each $(N, L)$ pair in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the smaller prime factor p given N=pq and L=p^2+q^2\n    using Newton's method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (17473, 40130),    # Instance A\n        (272953, 548210),   # Instance B\n        (119989, 1539578),  # Instance C\n        (1005973, 2012090), # Instance D\n    ]\n\n    results = []\n    for N, L in test_cases:\n        # This function implements Newton's method to find a root of\n        # f(x) = x^4 - L*x^2 + N^2 = 0.\n        \n        # Initial guess as specified: x_0 = sqrt(N).\n        # This guess is between the two positive roots p and q,\n        # and closer to the smaller root p.\n        x = np.sqrt(N)\n        \n        # Parameters for Newton's method\n        max_iterations = 100\n        tolerance = 1e-9\n        \n        for i in range(max_iterations):\n            # Evaluate the function f(x) and its derivative f'(x).\n            # f(x) = x^4 - L*x^2 + N^2\n            # f'(x) = 4*x^3 - 2*L*x\n            x_sq = x * x\n            fx = x_sq * x_sq - L * x_sq + N * N\n            \n            dfx = 4.0 * x * x_sq - 2.0 * L * x\n            \n            # Avoid division by zero, though unlikely for this problem's setup.\n            if abs(dfx) < 1e-12:\n                break\n                \n            # Newton's method update step\n            step = fx / dfx\n            x = x - step\n            \n            # Check for convergence\n            if abs(step) < tolerance:\n                break\n        \n        # The iteration converges to a high-precision estimate of one of the roots.\n        # Since the initial guess x_0 = sqrt(N) is closer to p (p <= sqrt(N)),\n        # the method converges to p.\n        x_root = x\n        \n        # Map the numerical root to the integer factor.\n        # Round the result to the nearest integer to get the first factor candidate.\n        factor1 = int(round(x_root))\n        \n        # Compute the other factor candidate using the definition N = pq.\n        # Rounding here handles any small floating-point inaccuracies.\n        factor2 = int(round(N / factor1))\n        \n        # The smaller of the two factors is the required prime p.\n        p = min(factor1, factor2)\n        results.append(p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2398877"}, {"introduction": "Linearization extends from scalar functions to vector mappings through the Jacobian matrix, the cornerstone of multivariate analysis and optimization. This computational practice challenges you to compute the Jacobian of a complex, composed function using two different methods: the approximate finite-difference method and the exact forward-mode automatic differentiation [@problem_id:2398904]. By implementing and comparing these techniques, you will gain first-hand insight into the crucial trade-offs between approximation error and computational precision.", "problem": "You are asked to study how linearization is used to handle nonlinearities by computing the Jacobian matrix of a nonlinear composed mapping in two ways and comparing their numerical precision. Consider the composed function defined by the following components. Let $\\mathbf{x} \\in \\mathbb{R}^{3}$ with components $\\mathbf{x} = [x_{1}, x_{2}, x_{3}]^{\\top}$. Define an intermediate mapping $\\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$ by\n$$\n\\mathbf{g}(\\mathbf{x}) =\n\\begin{bmatrix}\n\\exp\\!\\big(x_{1} x_{2}\\big) \\\\\n\\sin\\!\\big(x_{2} + x_{3}\\big) \\\\\nx_{1}^{2} + x_{3}\n\\end{bmatrix},\n$$\nand a second mapping $\\mathbf{h}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$ by\n$$\n\\mathbf{h}(\\mathbf{u}) =\n\\begin{bmatrix}\nu_{1} \\cos(u_{2}) + u_{3}^{3} \\\\\n\\ln\\!\\big(1 + u_{1}^{2} + u_{2}^{2}\\big) + \\tanh(u_{3})\n\\end{bmatrix}.\n$$\nAngles for the trigonometric functions $\\sin(\\cdot)$ and $\\cos(\\cdot)$ are to be interpreted in radians. The composed function is $\\mathbf{F} = \\mathbf{h} \\circ \\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$.\n\nYour task is to compute the Jacobian matrix $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x}) \\in \\mathbb{R}^{2 \\times 3}$ at several inputs $\\mathbf{x}$ using both:\n- a forward-mode automatic differentiation approach based on dual numbers, and\n- a central finite-difference approximation with two step sizes.\n\nThe Jacobian $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})$ is defined by the entries $[\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})]_{ij} = \\partial F_{i}(\\mathbf{x}) / \\partial x_{j}$. Linearization refers to the first-order approximation of $\\mathbf{F}$ near $\\mathbf{x}$ using this Jacobian.\n\nFor the forward-mode automatic differentiation method, you must implement dual numbers representing pairs $(v, \\dot{v})$ that propagate values and directional derivatives through elementary operations and functions according to first principles (the product rule, the chain rule, and standard derivatives of elementary functions). For the finite difference method, approximate the $j$-th column of the Jacobian using the central difference formula\n$$\n\\frac{\\mathbf{F}(\\mathbf{x} + h \\mathbf{e}_{j}) - \\mathbf{F}(\\mathbf{x} - h \\mathbf{e}_{j})}{2 h},\n$$\nwhere $\\mathbf{e}_{j}$ is the $j$-th canonical basis vector in $\\mathbb{R}^{3}$, and $h$ is a positive scalar step size.\n\nTo quantify the precision difference, compute for each test input and each step size the relative Frobenius-norm error between the finite-difference Jacobian $\\mathbf{J}_{\\mathrm{fd}}$ and the automatic differentiation Jacobian $\\mathbf{J}_{\\mathrm{ad}}$:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\left\\| \\mathbf{J}_{\\mathrm{fd}} - \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}}{\\max\\!\\left(1, \\left\\| \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}\\right)},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. This normalization avoids division by values smaller than $1$ and keeps the metric well-scaled when the Jacobian norm is very small.\n\nTest suite:\n- Use the following four input vectors $\\mathbf{x}$ (dimensionless):\n  - $\\mathbf{x}^{(1)} = [0.2, -0.3, 0.5]^{\\top}$,\n  - $\\mathbf{x}^{(2)} = [10^{-8}, -10^{-8}, 10^{-8}]^{\\top}$,\n  - $\\mathbf{x}^{(3)} = [1.5, 0.7, -1.2]^{\\top}$,\n  - $\\mathbf{x}^{(4)} = [-2.0, 0.4, 0.3]^{\\top}$.\n- For the central differences, use two step sizes $h$:\n  - $h_{1} = 10^{-6}$,\n  - $h_{2} = 10^{-8}$.\n\nProgram requirements:\n- Implement the dual-number forward-mode automatic differentiation from first principles for the elementary operations and functions appearing in $\\mathbf{F}$.\n- Implement the central finite-difference Jacobian approximation as specified.\n- For each test input $\\mathbf{x}^{(k)}$, compute two values: $\\varepsilon_{\\mathrm{rel}}$ with $h = h_1$ and with $h = h_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. The list must contain the eight floating-point values in the following order:\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(1)}$ with $h_{1}$, then with $h_{2}$,\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(2)}$ with $h_{1}$, then with $h_{2}$,\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(3)}$ with $h_{1}$, then with $h_{2}$,\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(4)}$ with $h_{1}$, then with $h_{2}$.\nFor example, the output must look like\n$$\n[\\varepsilon_{1,1},\\varepsilon_{1,2},\\varepsilon_{2,1},\\varepsilon_{2,2},\\varepsilon_{3,1},\\varepsilon_{3,2},\\varepsilon_{4,1},\\varepsilon_{4,2}],\n$$\nwhere each $\\varepsilon_{k,\\ell}$ is a floating-point number. No additional text should be printed.", "solution": "The problem statement has been validated and is deemed valid. It presents a well-posed, scientifically grounded computational task within the field of computational engineering, specifically addressing the handling of nonlinearities via linearization. All functions, parameters, and evaluation metrics are defined with mathematical precision, and the problem is free of ambiguity or factual unsoundness.\n\nThe core task is to compute the Jacobian matrix $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})$ of a composed nonlinear function $\\mathbf{F} = \\mathbf{h} \\circ \\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$ using two distinct methods and to quantify their numerical precision. The methods are forward-mode automatic differentiation (AD) and central finite differences (FD). The AD result, being accurate to machine precision, will serve as the reference against which the FD approximation is compared.\n\n### 1. analytical Framework\n\nThe composed function is $\\mathbf{F}(\\mathbf{x}) = \\mathbf{h}(\\mathbf{g}(\\mathbf{x}))$, where $\\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$ and $\\mathbf{h}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$.\n$$\n\\mathbf{g}(\\mathbf{x}) =\n\\begin{bmatrix}\ng_{1}(\\mathbf{x}) \\\\ g_{2}(\\mathbf{x}) \\\\ g_{3}(\\mathbf{x})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\exp\\!\\big(x_{1} x_{2}\\big) \\\\\n\\sin\\!\\big(x_{2} + x_{3}\\big) \\\\\nx_{1}^{2} + x_{3}\n\\end{bmatrix}\n$$\n$$\n\\mathbf{h}(\\mathbf{u}) =\n\\begin{bmatrix}\nh_{1}(\\mathbf{u}) \\\\ h_{2}(\\mathbf{u})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nu_{1} \\cos(u_{2}) + u_{3}^{3} \\\\\n\\ln\\!\\big(1 + u_{1}^{2} + u_{2}^{2}\\big) + \\tanh(u_{3})\n\\end{bmatrix}\n$$\nThe Jacobian of the composed function is given by the multivariate chain rule:\n$$\n\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x}) = \\mathbf{J}_{\\mathbf{h}}(\\mathbf{g}(\\mathbf{x})) \\cdot \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x})\n$$\nwhere $\\mathbf{J}_{\\mathbf{h}} \\in \\mathbb{R}^{2 \\times 3}$ and $\\mathbf{J}_{\\mathbf{g}} \\in \\mathbb{R}^{3 \\times 3}$.\n\n### 2. Forward-Mode Automatic Differentiation (AD)\n\nForward-mode AD computes exact derivatives by systematically applying the chain rule at the level of elementary operations. This is achieved through the algebra of dual numbers. A dual number is an ordered pair $(v, \\dot{v})$, representing a value $v$ and its directional derivative $\\dot{v}$. It can be written as $v + \\epsilon \\dot{v}$, where $\\epsilon$ is an infinitesimal with the property $\\epsilon^2 = 0$.\n\nThe rules for arithmetic and elementary functions are derived from the standard rules of differentiation:\n- **Sum Rule**: $(u \\pm w)' = u' \\pm w'$ $\\implies$ $(u_{v}, u_{\\dot{v}}) \\pm (w_{v}, w_{\\dot{v}}) = (u_{v} \\pm w_{v}, u_{\\dot{v}} \\pm w_{\\dot{v}})$\n- **Product Rule**: $(u w)' = u'w + uw'$ $\\implies$ $(u_{v}, u_{\\dot{v}}) \\cdot (w_{v}, w_{\\dot{v}}) = (u_{v} w_{v}, u_{\\dot{v}} w_{v} + u_{v} w_{\\dot{v}})$\n- **Chain Rule**: $(f(u))' = f'(u) u'$ $\\implies$ $f((u_{v}, u_{\\dot{v}})) = (f(u_{v}), f'(u_{v}) u_{\\dot{v}})$\n\nTo compute the $j$-th column of the Jacobian $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})$, which contains the partial derivatives with respect to $x_j$, we compute the directional derivative of $\\mathbf{F}$ in the direction of the canonical basis vector $\\mathbf{e}_j$. This is done by setting the input derivatives (the \"seed\") to $\\dot{\\mathbf{x}} = \\mathbf{e}_j$.\n\nThe algorithm consists of the following steps for each column $j = 1, 2, 3$:\n1.  Initialize the input vector $\\mathbf{x}$ as a vector of dual numbers, where $x_{i}$ becomes $(x_{i}, \\delta_{ij})$, with $\\delta_{ij}$ being the Kronecker delta.\n2.  Evaluate the function $\\mathbf{F}$ using dual number arithmetic. The evaluation propagates the values and their derivatives through the composition $\\mathbf{h} \\circ \\mathbf{g}$.\n3.  The result is a vector of dual numbers, $\\mathbf{F}(\\mathbf{x}_{\\text{dual}}) = [(F_{1}, \\dot{F}_{1}), (F_{2}, \\dot{F}_{2})]^{\\top}$. The derivative components $(\\dot{F}_{1}, \\dot{F}_{2})^{\\top}$ form the $j$-th column of the Jacobian: $[\\mathbf{J}_{\\mathbf{F}}]_{ij} = \\dot{F}_i$.\n\nThis process is repeated for each input variable to construct the full Jacobian matrix $\\mathbf{J}_{\\mathrm{ad}}$.\n\n### 3. Central Finite-Difference (FD) Approximation\n\nThe finite difference method approximates derivatives by evaluating the function at perturbed points. The central difference formula for the $j$-th column of the Jacobian is given by:\n$$\n(\\mathbf{J}_{\\mathrm{fd}})_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h \\mathbf{e}_{j}) - \\mathbf{F}(\\mathbf{x} - h \\mathbf{e}_{j})}{2 h}\n$$\nwhere $h$ is a small step size. This formula is derived from the Taylor series expansion of $\\mathbf{F}(\\mathbf{x} \\pm h \\mathbf{e}_{j})$ around $\\mathbf{x}$. The truncation error of this approximation is of order $O(h^2)$, meaning the error is proportional to the square of the step size.\n\nHowever, the total error is a combination of this truncation error and the round-off error from floating-point arithmetic. Round-off error in the numerator's subtraction increases as $h$ decreases, leading to a loss of significance. This creates a trade-off: a smaller $h$ reduces truncation error but increases round-off error. We will investigate this by using two different step sizes, $h_1 = 10^{-6}$ and $h_2 = 10^{-8}$.\n\n### 4. Error Quantification\n\nTo compare the precision of the finite-difference method against the automatic differentiation result, we compute the relative Frobenius-norm error:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\left\\| \\mathbf{J}_{\\mathrm{fd}} - \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}}{\\max\\!\\left(1, \\left\\| \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}\\right)}\n$$\nThe Frobenius norm is defined as $\\|\\mathbf{A}\\|_{F} = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$. The normalization by $\\max(1, \\left\\| \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F})$ provides a stable error metric, preventing division by values near zero while scaling the error relative to the magnitude of the \"true\" Jacobian.\n\n### 5. Implementation Strategy\n\nThe solution is implemented in Python. A class `Dual` is defined to represent dual numbers and to overload standard arithmetic operators. Global functions for elementary mathematical operations (`exp`, `sin`, `cos`, `ln`, `tanh`) are implemented to be polymorphic, handling both standard floating-point numbers and `Dual` objects. The problem-specific functions $\\mathbf{g}$ and $\\mathbf{h}$ are then written using these polymorphic operations, allowing them to be used seamlessly in both AD and FD computations. The main program iterates through the specified test inputs and step sizes, computing the Jacobians via both methods and calculating the relative error for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A strict Russian professor insists on scientific correctness from first principles.\n\nclass Dual:\n    \"\"\"\n    Represents a dual number for forward-mode automatic differentiation.\n    A dual number has a real part (value) and an infinitesimal part (derivative).\n    d = value + derivative * epsilon, where epsilon^2 = 0.\n    \"\"\"\n    def __init__(self, value, derivative=0.0):\n        self.value = float(value)\n        self.derivative = float(derivative)\n\n    def __repr__(self):\n        return f\"Dual({self.value}, {self.derivative})\"\n\n    def __add__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.value + other.value, self.derivative + other.derivative)\n        return Dual(self.value + other, self.derivative)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.value - other.value, self.derivative - other.derivative)\n        return Dual(self.value - other, self.derivative)\n\n    def __rsub__(self, other):\n        return Dual(other - self.value, -self.derivative)\n\n    def __mul__(self, other):\n        if isinstance(other, Dual):\n            # Product rule: (uv)' = u'v + uv'\n            return Dual(self.value * other.value, \n                        self.derivative * other.value + self.value * other.derivative)\n        return Dual(self.value * other, self.derivative * other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n    \n    def __pow__(self, exponent):\n        if isinstance(exponent, (int, float)):\n            # Power rule for constant exponent: (u^c)' = c*u^(c-1)*u'\n            val = self.value ** exponent\n            der = exponent * (self.value ** (exponent - 1)) * self.derivative\n            return Dual(val, der)\n        raise NotImplementedError(\"Dual to the power of Dual is not implemented.\")\n\n    def __truediv__(self, other):\n        if isinstance(other, Dual):\n            # Quotient rule: (u/v)' = (u'v - uv') / v^2\n            val = self.value / other.value\n            der = (self.derivative * other.value - self.value * other.derivative) / (other.value ** 2)\n            return Dual(val, der)\n        return Dual(self.value / other, self.derivative / other)\n\n    def __rtruediv__(self, other):\n        val = other / self.value\n        der = (-other * self.derivative) / (self.value ** 2)\n        return Dual(val, der)\n    \n    def __neg__(self):\n        return Dual(-self.value, -self.derivative)\n\n# Polymorphic elementary functions that work with both floats and Dual numbers.\ndef exp(d):\n    if not isinstance(d, Dual): return np.exp(d)\n    val = np.exp(d.value)\n    der = val * d.derivative\n    return Dual(val, der)\n\ndef sin(d):\n    if not isinstance(d, Dual): return np.sin(d)\n    val = np.sin(d.value)\n    der = np.cos(d.value) * d.derivative\n    return Dual(val, der)\n\ndef cos(d):\n    if not isinstance(d, Dual): return np.cos(d)\n    val = np.cos(d.value)\n    der = -np.sin(d.value) * d.derivative\n    return Dual(val, der)\n\ndef log(d):\n    if not isinstance(d, Dual): return np.log(d)\n    val = np.log(d.value)\n    der = (1 / d.value) * d.derivative\n    return Dual(val, der)\n\ndef tanh(d):\n    if not isinstance(d, Dual): return np.tanh(d)\n    val = np.tanh(d.value)\n    # Derivative of tanh(x) is sech^2(x) = 1 - tanh^2(x)\n    der = (1 - val**2) * d.derivative\n    return Dual(val, der)\n\n# Problem-specific nonlinear mappings\ndef g(x_vec):\n    \"\"\" Intermediate mapping g: R^3 -> R^3 \"\"\"\n    x1, x2, x3 = x_vec[0], x_vec[1], x_vec[2]\n    return [\n        exp(x1 * x2),\n        sin(x2 + x3),\n        x1**2 + x3\n    ]\n\ndef h(u_vec):\n    \"\"\" Final mapping h: R^3 -> R^2 \"\"\"\n    u1, u2, u3 = u_vec[0], u_vec[1], u_vec[2]\n    return [\n        u1 * cos(u2) + u3**3,\n        log(1 + u1**2 + u2**2) + tanh(u3)\n    ]\n\ndef F(x_vec):\n    \"\"\" Composed mapping F = h(g(x)) \"\"\"\n    return h(g(x_vec))\n\ndef compute_jacobian_ad(x_val):\n    \"\"\"Computes the Jacobian using forward-mode automatic differentiation.\"\"\"\n    n_in = len(x_val)\n    # Dynamically determine output dimension by a sample evaluation\n    n_out = len(F(x_val))\n    J_ad = np.zeros((n_out, n_in))\n    \n    for j in range(n_in):\n        # Create dual number inputs with a seed for the j-th partial derivative\n        x_dual = [Dual(x_val[i], 1.0 if i == j else 0.0) for i in range(n_in)]\n        \n        # Evaluate the function with dual numbers\n        F_dual = F(x_dual)\n        \n        # The derivative part of the output is the j-th column of the Jacobian\n        for i in range(n_out):\n            J_ad[i, j] = F_dual[i].derivative\n            \n    return J_ad\n\ndef compute_jacobian_fd(x_val, h_step):\n    \"\"\"Computes the Jacobian using the central finite-difference formula.\"\"\"\n    x_val = np.array(x_val, dtype=float)\n    n_in = len(x_val)\n    n_out = len(F(x_val))\n    J_fd = np.zeros((n_out, n_in))\n    \n    for j in range(n_in):\n        e_j = np.zeros(n_in)\n        e_j[j] = 1.0\n        \n        x_fwd = x_val + h_step * e_j\n        x_bwd = x_val - h_step * e_j\n        \n        F_fwd = np.array(F(x_fwd))\n        F_bwd = np.array(F(x_bwd))\n        \n        column = (F_fwd - F_bwd) / (2 * h_step)\n        J_fd[:, j] = column\n        \n    return J_fd\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0.2, -0.3, 0.5],\n        [1e-8, -1e-8, 1e-8],\n        [1.5, 0.7, -1.2],\n        [-2.0, 0.4, 0.3]\n    ]\n    step_sizes = [1e-6, 1e-8]\n    \n    results = []\n    for x_vec in test_cases:\n        # Compute the \"exact\" Jacobian using Automatic Differentiation\n        J_ad = compute_jacobian_ad(x_vec)\n        norm_J_ad = np.linalg.norm(J_ad, 'fro')\n\n        for h in step_sizes:\n            # Compute the approximate Jacobian using Finite Differences\n            J_fd = compute_jacobian_fd(x_vec, h)\n            \n            # Calculate the relative Frobenius-norm error\n            norm_diff = np.linalg.norm(J_fd - J_ad, 'fro')\n            error = norm_diff / max(1.0, norm_J_ad)\n            results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2398904"}, {"introduction": "The success of linearization often depends not just on the algorithm, but on the initial problem formulation. This exercise explores this critical concept within the context of nonlinear parameter estimation, a common task in engineering [@problem_id:2398872]. By analyzing the Jacobian matrices for different mathematical descriptions of the same geometric object—an ellipse—you will discover how modeling choices can significantly impact the stability and solvability of the resulting linear systems.", "problem": "Consider a non-circular ellipse centered at the origin with principal semi-axes aligned with the Cartesian axes, parameterized by unknown semi-axis lengths $a>0$ and $b>0$ with $a \\neq b$. The ellipse satisfies the implicit Cartesian equation $x^{2}/a^{2} + y^{2}/b^{2} = 1$. Suppose you aim to estimate small parameter updates $\\delta a$ and $\\delta b$ about a nominal $(a_{0},b_{0})$ in a single iteration of Gauss-Newton (GN), using either Cartesian or polar measurements taken with respect to the origin.\n\nYou are given two types of measurement sets:\n- Cartesian points $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ that lie near the ellipse boundary.\n- Polar points $\\{(r_{i},\\theta_{i})\\}_{i=1}^{N}$ with the usual relations $x_{i}=r_{i}\\cos\\theta_{i}$ and $y_{i}=r_{i}\\sin\\theta_{i}$.\n\nYou consider three residual definitions:\n- Cartesian implicit residuals $f_{i}(a,b) = x_{i}^{2}/a^{2} + y_{i}^{2}/b^{2} - 1$.\n- Polar radial residuals $h_{i}(a,b) = r_{i} - \\left(\\cos^{2}\\theta_{i}/a^{2} + \\sin^{2}\\theta_{i}/b^{2}\\right)^{-1/2}$.\n- Polar implicit residuals $g_{i}(a,b) = r_{i}^{2}\\left(\\cos^{2}\\theta_{i}/a^{2} + \\sin^{2}\\theta_{i}/b^{2}\\right) - 1$.\n\nA single GN linearization step about $(a_{0},b_{0})$ uses the first-order expansion of the residual vector and forms a linear least-squares problem in $(\\delta a,\\delta b)$ with Jacobian equal to the matrix of partial derivatives of the chosen residuals with respect to $(a,b)$, all evaluated at $(a_{0},b_{0})$. Denote $\\hat r_{i} = \\left(\\cos^{2}\\theta_{i}/a_{0}^{2} + \\sin^{2}\\theta_{i}/b_{0}^{2}\\right)^{-1/2}$, i.e., the polar model prediction at $(a_{0},b_{0})$.\n\nWhich of the following statements about the resulting linear systems are correct?\n\nA. Using the Cartesian implicit residuals $f_{i}$, the Jacobian row for sample $i$ is $\\left[-2 x_{i}^{2}/a_{0}^{3},\\,-2 y_{i}^{2}/b_{0}^{3}\\right]$.\n\nB. Using the polar radial residuals $h_{i}$, the Jacobian row for sample $i$ is $\\left[-\\hat r_{i}^{3}\\cos^{2}\\theta_{i}/a_{0}^{3},\\,-\\hat r_{i}^{3}\\sin^{2}\\theta_{i}/b_{0}^{3}\\right]$.\n\nC. Using the polar implicit residuals $g_{i}$, after substituting $x_{i}=r_{i}\\cos\\theta_{i}$ and $y_{i}=r_{i}\\sin\\theta_{i}$, the Jacobian with respect to $(a,b)$ is identical to that obtained from the Cartesian implicit residuals $f_{i}$.\n\nD. The factor $\\hat r_{i}^{3}$ in the polar radial residual Jacobian guarantees that the corresponding normal matrix is always better conditioned than that of the Cartesian implicit residual linearization, regardless of how the samples $\\{(x_{i},y_{i})\\}$ or $\\{(r_{i},\\theta_{i})\\}$ are distributed.\n\nE. If all samples lie exactly on the minor axis of the ellipse (i.e., $x_{i}=0$ and equivalently $\\cos\\theta_{i}=0$ for all $i$), then at that iteration both the Cartesian implicit and the polar radial linearizations are singular for estimating $\\delta a$.", "solution": "The problem statement is a standard exercise in nonlinear least squares for parameter estimation, specifically the application of the Gauss-Newton method to the problem of fitting an ellipse. It involves analyzing the structure of the Jacobian matrix for different choices of residual functions.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Model:** A non-circular ellipse centered at the origin, with semi-axes $a>0$, $b>0$, $a \\neq b$. The implicit equation is $x^{2}/a^{2} + y^{2}/b^{2} = 1$.\n- **Objective:** Estimate small parameter updates $\\delta a$ and $\\delta b$ about a nominal point $(a_{0},b_{0})$ in a single Gauss-Newton (GN) iteration.\n- **Data Sets:**\n    1. Cartesian points $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ near the ellipse.\n    2. Polar points $\\{(r_{i},\\theta_{i})\\}_{i=1}^{N}$, with $x_{i}=r_{i}\\cos\\theta_{i}$ and $y_{i}=r_{i}\\sin\\theta_{i}$.\n- **Residual Definitions:**\n    1. Cartesian implicit: $f_{i}(a,b) = x_{i}^{2}/a^{2} + y_{i}^{2}/b^{2} - 1$.\n    2. Polar radial: $h_{i}(a,b) = r_{i} - \\left(\\cos^{2}\\theta_{i}/a^{2} + \\sin^{2}\\theta_{i}/b^{2}\\right)^{-1/2}$.\n    3. Polar implicit: $g_{i}(a,b) = r_{i}^{2}\\left(\\cos^{2}\\theta_{i}/a^{2} + \\sin^{2}\\theta_{i}/b^{2}\\right) - 1$.\n- **Methodology:** The GN method linearizes the residuals to form a linear least-squares problem. The Jacobian is the matrix of first partial derivatives of the residuals with respect to the parameters $(a,b)$, evaluated at the current estimate $(a_{0},b_{0})$.\n- **Notation:** $\\hat r_{i} = \\left(\\cos^{2}\\theta_{i}/a_{0}^{2} + \\sin^{2}\\theta_{i}/b_{0}^{2}\\right)^{-1/2}$ is the model's predicted radius at angle $\\theta_i$ for parameters $(a_0, b_0)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly based on standard principles of numerical optimization (Gauss-Newton method) and analytical geometry. The formulations are common in computational engineering and computer vision for geometric fitting tasks.\n- **Well-Posed:** The problem asks for an analysis of the structure of the Jacobian and the resulting linear systems, not for a solution to the fitting problem itself. The questions posed are specific and allow for a unique analytical verification.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n- **Flaw Checklist:** The problem does not violate any of the listed invalidity criteria. The setup is self-contained, consistent, and scientifically sound.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation and Option Analysis\n\nThe single Gauss-Newton step for estimating the parameter update $\\delta\\mathbf{p} = [\\delta a, \\delta b]^T$ is formulated as a linear least squares problem: $\\arg\\min_{\\delta\\mathbf{p}} \\| \\mathbf{J} \\delta\\mathbf{p} + \\mathbf{r}(\\mathbf{p}_0) \\|_2^2$, where $\\mathbf{r}(\\mathbf{p}_0)$ is the vector of residuals evaluated at the current parameter estimate $\\mathbf{p}_0=(a_0, b_0)$, and $\\mathbf{J}$ is the Jacobian matrix with entries $J_{ij} = \\frac{\\partial r_i}{\\partial p_j}$ evaluated at $\\mathbf{p}_0$. We will analyze the structure of the Jacobian for each residual type.\n\n**A. Using the Cartesian implicit residuals $f_{i}$, the Jacobian row for sample $i$ is $\\left[ -2 x_{i}^{2}/a_{0}^{3},\\,-2 y_{i}^{2}/b_{0}^{3} \\right]$.**\n\nThe residual for the $i$-th point is $f_{i}(a,b) = x_{i}^{2}a^{-2} + y_{i}^{2}b^{-2} - 1$. The Jacobian row is given by the partial derivatives with respect to $a$ and $b$.\nThe partial derivative with respect to $a$ is:\n$$ \\frac{\\partial f_{i}}{\\partial a} = x_{i}^{2} \\frac{\\partial}{\\partial a}(a^{-2}) = x_{i}^{2} (-2a^{-3}) = -\\frac{2x_{i}^{2}}{a^{3}} $$\nThe partial derivative with respect to $b$ is:\n$$ \\frac{\\partial f_{i}}{\\partial b} = y_{i}^{2} \\frac{\\partial}{\\partial b}(b^{-2}) = y_{i}^{2} (-2b^{-3}) = -\\frac{2y_{i}^{2}}{b^{3}} $$\nEvaluating these derivatives at the nominal point $(a_{0}, b_{0})$, the $i$-th row of the Jacobian is:\n$$ \\left[ \\frac{\\partial f_{i}}{\\partial a}\\bigg|_{(a_0, b_0)}, \\frac{\\partial f_{i}}{\\partial b}\\bigg|_{(a_0, b_0)} \\right] = \\left[ -\\frac{2x_{i}^{2}}{a_{0}^{3}}, -\\frac{2y_{i}^{2}}{b_{0}^{3}} \\right] $$\nThis matches the statement in the option.\n**Verdict: Correct.**\n\n**B. Using the polar radial residuals $h_{i}$, the Jacobian row for sample $i$ is $\\left[-\\hat r_{i}^{3}\\cos^{2}\\theta_{i}/a_{0}^{3},\\,-\\hat r_{i}^{3}\\sin^{2}\\theta_{i}/b_{0}^{3}\\right]$.**\n\nThe residual is $h_{i}(a,b) = r_{i} - \\left(\\cos^{2}\\theta_{i}a^{-2} + \\sin^{2}\\theta_{i}b^{-2}\\right)^{-1/2}$. Let $U_i(a,b) = \\cos^{2}\\theta_{i}a^{-2} + \\sin^{2}\\theta_{i}b^{-2}$. Then $h_i(a,b) = r_i - (U_i)^{-1/2}$.\nWe compute the partial derivatives using the chain rule. The term $r_i$ is a constant with respect to $a$ and $b$.\n$$ \\frac{\\partial h_{i}}{\\partial a} = -\\frac{\\partial}{\\partial a}(U_i^{-1/2}) = - \\left(-\\frac{1}{2}\\right) U_i^{-3/2} \\frac{\\partial U_i}{\\partial a} = \\frac{1}{2} U_i^{-3/2} \\left( \\cos^{2}\\theta_{i} (-2a^{-3}) \\right) = -U_i^{-3/2} \\frac{\\cos^{2}\\theta_{i}}{a^{3}} $$\nSimilarly, for $b$:\n$$ \\frac{\\partial h_{i}}{\\partial b} = -\\frac{\\partial}{\\partial b}(U_i^{-1/2}) = - \\left(-\\frac{1}{2}\\right) U_i^{-3/2} \\frac{\\partial U_i}{\\partial b} = \\frac{1}{2} U_i^{-3/2} \\left( \\sin^{2}\\theta_{i} (-2b^{-3}) \\right) = -U_i^{-3/2} \\frac{\\sin^{2}\\theta_{i}}{b^{3}} $$\nNow, evaluate at $(a_0, b_0)$ and use the given definition $\\hat r_{i} = \\left(\\cos^{2}\\theta_{i}/a_{0}^{2} + \\sin^{2}\\theta_{i}/b_{0}^{2}\\right)^{-1/2} = (U_i(a_0,b_0))^{-1/2}$.\nThis implies $(U_i(a_0,b_0))^{-3/2} = \\left((U_i(a_0,b_0))^{-1/2}\\right)^3 = \\hat r_i^3$.\nSubstituting this into the partial derivatives evaluated at $(a_0,b_0)$:\n$$ \\frac{\\partial h_{i}}{\\partial a}\\bigg|_{(a_0, b_0)} = -\\hat r_{i}^{3} \\frac{\\cos^{2}\\theta_{i}}{a_{0}^{3}} $$\n$$ \\frac{\\partial h_{i}}{\\partial b}\\bigg|_{(a_0, b_0)} = -\\hat r_{i}^{3} \\frac{\\sin^{2}\\theta_{i}}{b_{0}^{3}} $$\nThe Jacobian row is $\\left[-\\hat r_{i}^{3}\\cos^{2}\\theta_{i}/a_{0}^{3}, -\\hat r_{i}^{3}\\sin^{2}\\theta_{i}/b_{0}^{3}\\right]$. This matches the statement.\n**Verdict: Correct.**\n\n**C. Using the polar implicit residuals $g_{i}$, after substituting $x_{i}=r_{i}\\cos\\theta_{i}$ and $y_{i}=r_{i}\\sin\\theta_{i}$, the Jacobian with respect to $(a,b)$ is identical to that obtained from the Cartesian implicit residuals $f_{i}$.**\n\nThe polar implicit residual is given by $g_{i}(a,b) = r_{i}^{2}\\left(\\cos^{2}\\theta_{i}/a^{2} + \\sin^{2}\\theta_{i}/b^{2}\\right) - 1$.\nLet us perform the suggested substitution using the relations for polar coordinates: $x_i = r_i \\cos\\theta_i$ and $y_i = r_i \\sin\\theta_i$.\n$$ g_i(a,b) = \\frac{r_i^2 \\cos^2\\theta_i}{a^2} + \\frac{r_i^2 \\sin^2\\theta_i}{b^2} - 1 = \\frac{(r_i \\cos\\theta_i)^2}{a^2} + \\frac{(r_i \\sin\\theta_i)^2}{b^2} - 1 = \\frac{x_i^2}{a^2} + \\frac{y_i^2}{b^2} - 1 $$\nThis resulting expression is identical to the definition of the Cartesian implicit residual, $f_i(a,b)$. Since $g_i(a,b) \\equiv f_i(a,b)$, their partial derivatives with respect to $a$ and $b$ must be identical. Consequently, the Jacobian matrix derived from the set of residuals $\\{g_i\\}$ is identical to the Jacobian matrix derived from $\\{f_i\\}$.\n**Verdict: Correct.**\n\n**D. The factor $\\hat r_{i}^{3}$ in the polar radial residual Jacobian guarantees that the corresponding normal matrix is always better conditioned than that of the Cartesian implicit residual linearization, regardless of how the samples $\\{(x_{i},y_{i})\\}$ or $\\{(r_{i},\\theta_{i})\\}$ are distributed.**\n\nThis statement makes a universal claim about the conditioning of the normal matrices $J^T J$. The conditioning of a matrix is a complex property that depends on the entire structure of the matrix, which in turn depends on the sample distribution. A claim that holds \"always\" and \"regardless of the sample distribution\" is exceptionally strong and often false in numerical contexts.\nLet $J_f$ be the Jacobian for the Cartesian residuals and $J_h$ be the Jacobian for the polar radial residuals. From A and B, using $x_i = r_i\\cos\\theta_i$ and $y_i = r_i\\sin\\theta_i$, we can relate the rows of the two Jacobians:\n- Row $i$ of $J_f$: $\\left[ -2 r_i^2 \\cos^2\\theta_i / a_0^3, -2 r_i^2 \\sin^2\\theta_i / b_0^3 \\right]$\n- Row $i$ of $J_h$: $\\left[ -\\hat r_i^3 \\cos^2\\theta_i / a_0^3, -\\hat r_i^3 \\sin^2\\theta_i / b_0^3 \\right]$\nWe can see that the $i$-th row of $J_f$ is a multiple of the $i$-th row of $J_h$ if $\\cos^2\\theta_i$ and $\\sin^2\\theta_i$ are not both zero (which they cannot be). The multiplier is not constant across all rows.\nA crucial difference is that the entries of $J_h$ depend on the measurement data only through the angle $\\theta_i$, whereas the entries of $J_f$ depend on the measured radius $r_i$ as well. This makes the properties of $J_h$ more robust to radial outliers. However, better robustness to outliers is not equivalent to having a better-conditioned normal matrix in all cases.\nIt is possible to construct a situation where the implicit weighting in the Cartesian formulation $J_f$ could, by happenstance, improve the condition number compared to $J_h$. For example, if the columns of $J_h$ are nearly collinear (leading to an ill-conditioned $J_h^T J_h$), it is theoretically possible that applying a non-uniform row-wise scaling (as is the case when relating $J_h$ to $J_f$) could make the columns of the scaled matrix more orthogonal, thus improving the condition number. Such a scenario would constitute a counterexample to the claim.\nTherefore, a guarantee that one formulation *always* yields a better-conditioned system than another, irrespective of the data, is not plausible.\n**Verdict: Incorrect.**\n\n**E. If all samples lie exactly on the minor axis of the ellipse (i.e., $x_{i}=0$ and equivalently $\\cos\\theta_{i}=0$ for all $i$), then at that iteration both the Cartesian implicit and the polar radial linearizations are singular for estimating $\\delta a$.**\n\n\"Singular for estimating $\\delta a$\" means that the parameter update $\\delta a$ is not constrained by the data. In the context of the linear system $J\\delta\\mathbf{p} = -\\mathbf{r}$, this occurs if the first column of the Jacobian $J$ (corresponding to the parameter $a$) is identically zero. If the first column of $J$ is zero, the normal matrix $J^T J$ will have a first row and column of zeros and will be singular.\n\n1.  **Cartesian implicit linearization ($f_i$)**: The Jacobian row is $\\left[ -2x_{i}^{2}/a_{0}^{3}, -2y_{i}^{2}/b_{0}^{3} \\right]$. If all samples have $x_i = 0$, then for every row $i$, the first element is $-2(0)^2/a_0^3 = 0$. The entire first column of the Jacobian $J_f$ is zero. The system is singular for estimating $\\delta a$.\n\n2.  **Polar radial linearization ($h_i$)**: The Jacobian row is $\\left[ -\\hat r_{i}^{3}\\cos^{2}\\theta_{i}/a_{0}^{3}, -\\hat r_{i}^{3}\\sin^{2}\\theta_{i}/b_{0}^{3} \\right]$. If all samples have $\\cos\\theta_i=0$, then for every row $i$, the first element is $-\\hat r_i^3 (0)^2/a_0^3 = 0$. The entire first column of the Jacobian $J_h$ is zero. The system is singular for estimating $\\delta a$.\n\nIn both cases, having data only on the minor axis provides no information to determine the semi-axis length $a$ along the major axis. The Jacobians correctly reflect this geometric reality by becoming rank-deficient. Therefore, the statement is correct for both linearizations.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABCE}$$", "id": "2398872"}]}