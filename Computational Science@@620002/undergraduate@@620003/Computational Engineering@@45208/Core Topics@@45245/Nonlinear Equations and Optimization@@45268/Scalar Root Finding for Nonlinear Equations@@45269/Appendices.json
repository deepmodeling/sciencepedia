{"hands_on_practices": [{"introduction": "A fundamental skill in computational engineering is translating a physical problem into a mathematical one. This first practice exercise guides you through this crucial process, starting from a model of a system's impulse response. By determining a cutoff time that captures a specific fraction of the total signal energy, you will formulate a nonlinear scalar equation, practice justifying the existence and uniqueness of its solution, and ultimately solve it analytically, setting the stage for more complex numerical challenges. [@problem_id:2433840]", "problem": "In a computational engineering calibration task, a first-order linear time-invariant system exhibits a nonnegative impulse response modeled by the function $y(x)=\\exp(-x)$ for $x \\geq 0$. To select a cutoff time $c>0$ that captures a prescribed fraction of the total response, you are asked to determine the value of $c$ such that the area accumulated under the response from $x=0$ to $x=c$ equals exactly $\\frac{1}{2}$ of the total area from $x=0$ to $x=\\infty$. Starting from the definition of area under a curve as a definite integral and invoking the Fundamental Theorem of Calculus (FTC), do the following: formulate a scalar nonlinear equation in the unknown $c$ whose root corresponds to the required area condition; justify existence and uniqueness of the root by appealing to continuity and monotonicity; and solve analytically for the exact value of $c$. Provide your final answer as an exact symbolic expression; no rounding is required and no units are necessary.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n**Step 1: Extract Givens**\n- Impulse response function: $y(x) = \\exp(-x)$ for $x \\geq 0$.\n- Cutoff time: $c > 0$.\n- Condition: The area under the response from $x=0$ to $x=c$ must equal exactly $\\frac{1}{2}$ of the total area from $x=0$ to $x=\\infty$.\n- Task: (1) Formulate a scalar nonlinear equation for $c$. (2) Justify the existence and uniqueness of the root. (3) Solve for the exact value of $c$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on the standard mathematical model of an impulse response for a first-order linear time-invariant system, namely an exponential decay function. The concepts of area under a curve (integration), the Fundamental Theorem of Calculus, root finding, and properties of functions (continuity, monotonicity) are all fundamental principles in mathematics and engineering. The problem is scientifically sound.\n- **Well-Posed:** The problem provides all necessary information and a clear objective. The condition leads to a well-defined equation whose solution is sought.\n- **Objective:** The problem is stated using precise and unambiguous mathematical language, free from subjective or speculative content.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically grounded problem in computational engineering. A solution will be provided.\n\nThe task is to find a cutoff time $c > 0$ for the impulse response function $y(x) = \\exp(-x)$ such that the area accumulated from $x=0$ to $x=c$ is half of the total area accumulated from $x=0$ to $x=\\infty$.\n\nFirst, we must compute the total area, $A_{total}$, under the curve $y(x)$ from $x=0$ to $x=\\infty$. This is given by the improper integral:\n$$A_{total} = \\int_{0}^{\\infty} y(x) \\, dx = \\int_{0}^{\\infty} \\exp(-x) \\, dx$$\nTo evaluate this, we use the definition of an improper integral:\n$$A_{total} = \\lim_{b \\to \\infty} \\int_{0}^{b} \\exp(-x) \\, dx$$\nThe antiderivative of $\\exp(-x)$ is $-\\exp(-x)$. Applying the Fundamental Theorem of Calculus:\n$$A_{total} = \\lim_{b \\to \\infty} \\left[ -\\exp(-x) \\right]_{0}^{b} = \\lim_{b \\to \\infty} (-\\exp(-b) - (-\\exp(-0)))$$\n$$A_{total} = \\lim_{b \\to \\infty} (1 - \\exp(-b)) = 1 - 0 = 1$$\nThus, the total area under the impulse response curve is $1$.\n\nNext, we compute the partial area, $A_{partial}(c)$, accumulated from $x=0$ to the cutoff time $x=c$:\n$$A_{partial}(c) = \\int_{0}^{c} y(x) \\, dx = \\int_{0}^{c} \\exp(-x) \\, dx$$\nAgain, using the Fundamental Theorem of Calculus:\n$$A_{partial}(c) = \\left[ -\\exp(-x) \\right]_{0}^{c} = -\\exp(-c) - (-\\exp(-0)) = 1 - \\exp(-c)$$\n\nThe problem requires that the partial area be exactly one-half of the total area:\n$$A_{partial}(c) = \\frac{1}{2} A_{total}$$\nSubstituting the expressions for the areas, we obtain the scalar nonlinear equation for the unknown $c$:\n$$1 - \\exp(-c) = \\frac{1}{2} (1)$$\n$$1 - \\exp(-c) = \\frac{1}{2}$$\n\nThis is the required nonlinear equation. To find its root, we can define a function $f(c)$ such that we are looking for the root of $f(c)=0$:\n$$f(c) = (1 - \\exp(-c)) - \\frac{1}{2} = \\frac{1}{2} - \\exp(-c)$$\nWe must justify the existence and uniqueness of a root for $c > 0$.\n\nFor existence, we appeal to the Intermediate Value Theorem. The function $f(c) = \\frac{1}{2} - \\exp(-c)$ is a sum of a constant and an exponential function, both of which are continuous for all real $c$. Therefore, $f(c)$ is continuous on the interval $(0, \\infty)$. We examine the behavior of $f(c)$ at the boundaries of this interval.\nAs $c \\to 0^{+}$:\n$$\\lim_{c \\to 0^{+}} f(c) = \\frac{1}{2} - \\lim_{c \\to 0^{+}} \\exp(-c) = \\frac{1}{2} - \\exp(0) = \\frac{1}{2} - 1 = -\\frac{1}{2}$$\nAs $c \\to \\infty$:\n$$\\lim_{c \\to \\infty} f(c) = \\frac{1}{2} - \\lim_{c \\to \\infty} \\exp(-c) = \\frac{1}{2} - 0 = \\frac{1}{2}$$\nSince $f(c)$ is continuous on $(0, \\infty)$ and takes a negative value as $c \\to 0^{+}$ and a positive value as $c \\to \\infty$, the Intermediate Value Theorem guarantees that there exists at least one root $c_{root} \\in (0, \\infty)$ such that $f(c_{root}) = 0$.\n\nFor uniqueness, we examine the monotonicity of $f(c)$ by computing its derivative with respect to $c$:\n$$f'(c) = \\frac{d}{dc} \\left( \\frac{1}{2} - \\exp(-c) \\right) = 0 - (-\\exp(-c)) = \\exp(-c)$$\nFor any $c > 0$, the value of $\\exp(-c)$ is strictly positive. Since $f'(c) > 0$ for all $c \\in (0, \\infty)$, the function $f(c)$ is strictly monotonically increasing on its domain. A strictly monotonic function can cross the value $0$ at most once. Therefore, the root whose existence we have established is unique.\n\nFinally, we solve the equation analytically for the exact value of $c$:\n$$1 - \\exp(-c) = \\frac{1}{2}$$\nRearranging the terms to solve for $c$:\n$$\\exp(-c) = 1 - \\frac{1}{2}$$\n$$\\exp(-c) = \\frac{1}{2}$$\nTaking the natural logarithm of both sides:\n$$\\ln(\\exp(-c)) = \\ln\\left(\\frac{1}{2}\\right)$$\n$$-c = \\ln(1) - \\ln(2)$$\n$$-c = 0 - \\ln(2)$$\n$$-c = -\\ln(2)$$\n$$c = \\ln(2)$$\nThis value is positive, $c > 0$, consistent with the problem's constraint. This is the exact analytical solution.", "answer": "$$\n\\boxed{\\ln(2)}\n$$", "id": "2433840"}, {"introduction": "Numerical root-finding algorithms are powerful, but they are not infallible. To use them effectively, one must understand their limitations and potential failure modes. This exercise explores the behavior of gradient-based methods when applied to a specially designed function with pathological features near a root, illustrating why methods like Newton's can fail and highlighting the importance of analyzing a function's properties before attempting to solve it numerically. [@problem_id:2433765]", "problem": "Consider the scalar nonlinear function $f:[-0.1,0.1]\\to\\mathbb{R}$ defined by\n$$\nf(x)=\n\\begin{cases}\nx^2\\sin\\!\\left(\\dfrac{1}{x}\\right), & x\\neq 0,\\\\[6pt]\n0, & x=0,\n\\end{cases}\n$$\nso that $f$ is well-defined and continuous on $[-0.1,0.1]$. Select all options that are correct about non-trivial roots in $[-0.1,0.1]$ and about challenges for gradient-based methods (for example, Newton’s method and the secant method) applied near $x=0$.\n\nA. $x=\\dfrac{1}{4\\pi}$ is a non-trivial root of $f(x)$ in $[-0.1,0.1]$.\n\nB. $x=\\dfrac{1}{3\\pi}$ is a non-trivial root of $f(x)$ in $[-0.1,0.1]$.\n\nC. $x=-\\dfrac{1}{5\\pi}$ is a non-trivial root of $f(x)$ in $[-0.1,0.1]$.\n\nD. Near $x=0$, Newton’s method is not guaranteed to be reliable from a generic initialization because $f'(x)$ oscillates without approaching a limit as $x\\to 0$, $f'(0)=0$, and there are infinitely many closely spaced nearby roots; small changes in the starting guess can lead to convergence to different roots or to erratic behavior.\n\nE. A bracketing method such as the bisection method cannot be used on $[-0.1,0.1]$ because $f(x)$ is not continuous at $x=0$.\n\nF. The secant method necessarily converges linearly to the root at $x=0$ from any two starting points in $[-0.1,0.1]$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The function is $f:[-0.1,0.1]\\to\\mathbb{R}$, defined as\n$$f(x)=\n\\begin{cases}\nx^2\\sin\\!\\left(\\dfrac{1}{x}\\right), & x\\neq 0, \\\\\n0, & x=0.\n\\end{cases}\n$$\n- The problem asserts that $f$ is well-defined and continuous on the interval $[-0.1,0.1]$.\n- The objective is to identify all correct statements concerning non-trivial roots in $[-0.1,0.1]$ and the performance of gradient-based methods near $x=0$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses a standard function from real analysis, often used as a counterexample for certain properties related to differentiation. The concepts of roots, Newton's method, the secant method, and the bisection method are fundamental topics in numerical analysis and computational engineering. The problem is mathematically and scientifically sound.\n- **Well-Posed:** The function, its domain, and its properties are clearly defined. The task is to evaluate a set of propositions, which is a well-defined problem structure.\n- **Objective:** The statements to be evaluated are based on objective mathematical properties of the function and algorithms. There is no subjective or ambiguous language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full analysis will be performed.\n\n**Analysis of the Function and its Roots**\n\nThe roots of the function $f(x)$ are the values of $x$ for which $f(x)=0$.\nOne root is given by definition: $x=0$. This is the trivial root.\nFor non-trivial roots, we must have $x \\neq 0$. We solve $f(x) = x^2\\sin(\\frac{1}{x}) = 0$.\nSince $x \\neq 0$, $x^2 \\neq 0$. Therefore, we must have $\\sin(\\frac{1}{x}) = 0$.\nThe sine function is zero when its argument is an integer multiple of $\\pi$.\n$$ \\frac{1}{x} = k\\pi, \\quad \\text{for } k \\in \\mathbb{Z} \\setminus \\{0\\} $$\nThe case $k=0$ is excluded because it would imply $1/x = 0$, which is impossible.\nSolving for $x$, we find the non-trivial roots are:\n$$ x_k = \\frac{1}{k\\pi}, \\quad \\text{for } k \\in \\mathbb{Z} \\setminus \\{0\\} $$\nWe are interested in roots within the interval $[-0.1, 0.1]$.\n$$ -0.1 \\le \\frac{1}{k\\pi} \\le 0.1 $$\nThis is equivalent to two inequalities:\n1. $\\frac{1}{k\\pi} \\le 0.1 \\implies 1 \\le 0.1 k\\pi \\implies k \\ge \\frac{10}{\\pi}$.\n2. $\\frac{1}{k\\pi} \\ge -0.1 \\implies 1 \\ge -0.1 k\\pi \\implies k \\le -\\frac{10}{\\pi}$ (note the inequality flip when multiplying by negative $k$).\n\nThese can be combined into a single condition on the absolute value: $|\\frac{1}{k\\pi}| \\le 0.1 \\implies |k\\pi| \\ge 10 \\implies |k| \\ge \\frac{10}{\\pi}$.\nUsing the approximation $\\pi \\approx 3.14159$, we have $\\frac{10}{\\pi} \\approx 3.183$.\nSince $k$ must be an integer, the non-trivial roots in the interval $[-0.1,0.1]$ are given by $x_k = \\frac{1}{k\\pi}$ where $k$ is any integer such that $|k| \\ge 4$.\n\n**Option-by-Option Analysis**\n\n**A. $x=\\dfrac{1}{4\\pi}$ is a non-trivial root of $f(x)$ in $[-0.1,0.1]$.**\nThis corresponds to $k=4$. Since $|4| = 4 \\ge 4$, this is a root according to our derivation.\nLet's check if it lies in the interval. $x = \\frac{1}{4\\pi} \\approx \\frac{1}{12.566} \\approx 0.079577$.\nSince $-0.1 \\le 0.079577 \\le 0.1$, this root is indeed in the specified interval.\nVerdict: **Correct**.\n\n**B. $x=\\dfrac{1}{3\\pi}$ is a non-trivial root of $f(x)$ in $[-0.1,0.1]$.**\nThis corresponds to $k=3$. Since $|3|=3 < 4$, this is not a root within the interval.\nLet's verify its value: $x = \\frac{1}{3\\pi} \\approx \\frac{1}{9.4248} \\approx 0.1061$.\nSince $0.1061 > 0.1$, this value is outside the interval $[-0.1, 0.1]$.\nVerdict: **Incorrect**.\n\n**C. $x=-\\dfrac{1}{5\\pi}$ is a non-trivial root of $f(x)$ in $[-0.1,0.1]$.**\nThis corresponds to $x = \\frac{1}{-5\\pi}$, so $k=-5$. Since $|-5| = 5 \\ge 4$, this is a root.\nLet's check if it lies in the interval. $x = -\\frac{1}{5\\pi} \\approx -\\frac{1}{15.708} \\approx -0.06366$.\nSince $-0.1 \\le -0.06366 \\le 0.1$, this root is indeed in the specified interval.\nVerdict: **Correct**.\n\n**D. Near $x=0$, Newton’s method is not guaranteed to be reliable from a generic initialization because $f'(x)$ oscillates without approaching a limit as $x\\to 0$, $f'(0)=0$, and there are infinitely many closely spaced nearby roots; small changes in the starting guess can lead to convergence to different roots or to erratic behavior.**\nLet us analyze the derivative of $f(x)$. For $x \\neq 0$:\n$$ f'(x) = \\frac{d}{dx} \\left(x^2\\sin\\left(\\frac{1}{x}\\right)\\right) = 2x\\sin\\left(\\frac{1}{x}\\right) + x^2\\cos\\left(\\frac{1}{x}\\right)\\left(-\\frac{1}{x^2}\\right) = 2x\\sin\\left(\\frac{1}{x}\\right) - \\cos\\left(\\frac{1}{x}\\right) $$\nAt $x=0$, from the definition of the derivative:\n$$ f'(0) = \\lim_{h\\to 0} \\frac{f(h) - f(0)}{h} = \\lim_{h\\to 0} \\frac{h^2\\sin(1/h)}{h} = \\lim_{h\\to 0} h\\sin\\left(\\frac{1}{h}\\right) $$\nSince $|\\sin(1/h)| \\le 1$, we have $0 \\le |h\\sin(1/h)| \\le |h|$. By the Squeeze Theorem, as $h \\to 0$, $f'(0) = 0$.\nNow consider the limit of $f'(x)$ as $x \\to 0$:\n$$ \\lim_{x\\to 0} f'(x) = \\lim_{x\\to 0} \\left(2x\\sin\\left(\\frac{1}{x}\\right) - \\cos\\left(\\frac{1}{x}\\right)\\right) $$\nThe term $2x\\sin(1/x) \\to 0$ by the Squeeze Theorem. However, the term $\\cos(1/x)$ oscillates between $-1$ and $1$ and does not approach a limit. Thus, $\\lim_{x\\to 0} f'(x)$ does not exist.\nThe statement correctly asserts that $f'(x)$ oscillates without a limit as $x \\to 0$ and that $f'(0)=0$. It also correctly notes the presence of infinitely many roots clustering at $x=0$.\nThe iteration for Newton's method is $x_{n+1} = x_n - f(x_n)/f'(x_n)$. The non-existence of $\\lim_{x\\to 0} f'(x)$ and the fact that $f'(x)$ takes the value $0$ at infinitely many points near the origin (local extrema between roots) cause the tangent line's slope to change wildly. This makes the method's behavior extremely sensitive to the initial guess, leading to unpredictable convergence or divergence. The statement is an accurate and complete description of the pathology this function presents for Newton's method.\nVerdict: **Correct**.\n\n**E. A bracketing method such as the bisection method cannot be used on $[-0.1,0.1]$ because $f(x)$ is not continuous at $x=0$.**\nThe premise of this statement is false. The problem explicitly states, and it can be verified, that $f(x)$ is continuous on $[-0.1, 0.1]$.\nTo check the continuity at $x=0$:\n$$ \\lim_{x\\to 0} f(x) = \\lim_{x\\to 0} x^2\\sin\\left(\\frac{1}{x}\\right) $$\nSince $-1 \\le \\sin(1/x) \\le 1$, we have $-x^2 \\le x^2\\sin(1/x) \\le x^2$. As $x \\to 0$, both $-x^2$ and $x^2$ approach $0$. By the Squeeze Theorem, $\\lim_{x\\to 0} f(x) = 0$, which is equal to $f(0)$. Thus, $f$ is continuous at $x=0$.\nSince the reason given (\"because $f(x)$ is not continuous\") is false, the statement is flawed.\nFurthermore, let's check if the bisection method is applicable on $[-0.1, 0.1]$. We need $f(x)$ to be continuous, which it is, and $f(-0.1)f(0.1) < 0$.\n$f(0.1) = (0.1)^2 \\sin(10) = 0.01\\sin(10)$.\n$f(-0.1) = (-0.1)^2 \\sin(-10) = 0.01(-\\sin(10)) = -f(0.1)$.\nTo determine the sign, we locate $10$ radians. $3\\pi \\approx 9.425$ and $3.5\\pi \\approx 10.996$. Since $3\\pi < 10 < 3.5\\pi$, the angle $10$ is in the third quadrant of the unit circle, so $\\sin(10) < 0$.\nTherefore, $f(0.1) < 0$ and $f(-0.1) > 0$. The product $f(-0.1)f(0.1) < 0$.\nThe conditions for the bisection method are met. It can be used on $[-0.1, 0.1]$ to find a root. The statement is false both in its conclusion and its premise.\nVerdict: **Incorrect**.\n\n**F. The secant method necessarily converges linearly to the root at $x=0$ from any two starting points in $[-0.1,0.1]$.**\nThis statement makes several strong claims, all of which are suspect.\n1.  \"necessarily converges\": The secant method, like Newton's, is not guaranteed to converge for all functions and starting points. For this particular function, with its infinite oscillations near $x=0$, convergence is not at all guaranteed.\n2.  \"from any two starting points\": This is demonstrably false. Let's choose two starting points that are also roots, for example $x_0 = \\frac{1}{4\\pi}$ and $x_1 = \\frac{1}{5\\pi}$. Then $f(x_0)=0$ and $f(x_1)=0$. The secant iteration formula is $x_{n+1} = x_n - f(x_n)\\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}$. The denominator becomes $f(x_1) - f(x_0) = 0 - 0 = 0$. The method fails due to division by zero.\n3.  \"converges ... to the root at $x=0$\": Even if it converges, it might converge to one of the infinitely many other roots in the interval. The destination root is highly dependent on the starting points.\n4. \"converges linearly\": The order of convergence depends on properties of the root. Typically, convergence is superlinear for a simple root and linear for a multiple root. Here, $f(0)=0$ and $f'(0)=0$, suggesting a multiple root. While linear convergence might be expected *if* it converges to $x=0$, the claim that it *necessarily* happens from *any* starting points is false. The preceding points already invalidate the statement.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "2433765"}, {"introduction": "This final practice brings together theory and application in a realistic engineering design scenario. You will determine the precise firing angle $\\alpha$ for a thyristor to control the power delivered to a load, a common task in power electronics. The problem requires you to first derive the governing transcendental equation from an integral expression and then employ a robust numerical root-finding algorithm to solve it, demonstrating the end-to-end workflow of a computational engineering problem. [@problem_id:2433781]", "problem": "Let $f(\\alpha)$ denote the normalized integral defined by\n$$\nf(\\alpha) \\equiv \\frac{1}{\\pi}\\int_{\\alpha}^{\\pi}\\sin^2(\\theta)\\,d\\theta,\n$$\nwith $\\alpha \\in [0,\\pi]$. For a given target value $s \\in [0,\\tfrac{1}{2}]$, determine the angle $\\alpha$ in radians such that\n$$\nf(\\alpha) = s.\n$$\nYou must compute $\\alpha$ for each prescribed target $s$ to an absolute accuracy of at least $10^{-10}$ radians.\n\nAll angles must be expressed in radians. The final program output must be a single line containing a comma-separated list of the computed angles, enclosed in square brackets, with each value rounded to exactly $10$ decimal places.\n\nTest suite (targets $s$):\n- $s = 0.25$\n- $s = 0.5$\n- $s = 0$\n- $s = 10^{-6}$\n- $s = 0.499999$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[a_1,a_2,\\dots,a_5]$), where each $a_i$ is the solution $\\alpha$ for the corresponding $s$ in the order listed above, rounded to exactly $10$ decimal places.", "solution": "The problem as stated is valid. It is scientifically grounded in calculus and numerical analysis, well-posed, and expressed in objective mathematical language without ambiguity or contradiction. We shall proceed with a formal solution.\n\nWe are tasked with finding the value of $\\alpha \\in [0, \\pi]$ that satisfies the equation $f(\\alpha) = s$ for a given target value $s \\in [0, \\tfrac{1}{2}]$. The function $f(\\alpha)$ is defined as a normalized integral:\n$$f(\\alpha) \\equiv \\frac{1}{\\pi}\\int_{\\alpha}^{\\pi}\\sin^2(\\theta)\\,d\\theta.$$\nTo proceed, we must first evaluate the integral analytically. We use the trigonometric identity for power reduction: $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$. The indefinite integral is thus:\n$$\\int \\sin^2(\\theta)\\,d\\theta = \\int \\frac{1 - \\cos(2\\theta)}{2}\\,d\\theta = \\frac{1}{2}\\left(\\theta - \\frac{\\sin(2\\theta)}{2}\\right) + C.$$\nEvaluating the definite integral from $\\alpha$ to $\\pi$ yields:\n$$\\int_{\\alpha}^{\\pi}\\sin^2(\\theta)\\,d\\theta = \\left[\\frac{\\theta}{2} - \\frac{\\sin(2\\theta)}{4}\\right]_{\\alpha}^{\\pi} = \\left(\\frac{\\pi}{2} - \\frac{\\sin(2\\pi)}{4}\\right) - \\left(\\frac{\\alpha}{2} - \\frac{\\sin(2\\alpha)}{4}\\right) = \\frac{\\pi}{2} - \\frac{\\alpha}{2} + \\frac{\\sin(2\\alpha)}{4}.$$\nSubstituting this result into the definition of $f(\\alpha)$ gives the explicit form:\n$$f(\\alpha) = \\frac{1}{\\pi}\\left(\\frac{\\pi}{2} - \\frac{\\alpha}{2} + \\frac{\\sin(2\\alpha)}{4}\\right) = \\frac{1}{2} - \\frac{\\alpha}{2\\pi} + \\frac{\\sin(2\\alpha)}{4\\pi}.$$\nThe problem is now reduced to finding the root of the nonlinear scalar equation $g(\\alpha) = 0$, where $g(\\alpha)$ is defined as:\n$$g(\\alpha) = f(\\alpha) - s = \\frac{1}{2} - s - \\frac{\\alpha}{2\\pi} + \\frac{\\sin(2\\alpha)}{4\\pi}.$$\nBefore selecting a numerical method, we must analyze the properties of $f(\\alpha)$ on its domain $\\alpha \\in [0, \\pi]$. The derivative is found using the Fundamental Theorem of Calculus:\n$$f'(\\alpha) = \\frac{d}{d\\alpha}\\left(\\frac{1}{\\pi}\\int_{\\alpha}^{\\pi}\\sin^2(\\theta)\\,d\\theta\\right) = -\\frac{1}{\\pi}\\sin^2(\\alpha).$$\nSince $\\sin^2(\\alpha) \\ge 0$ for all real $\\alpha$, it follows that $f'(\\alpha) \\le 0$ on the domain. The function is strictly monotonically decreasing for $\\alpha \\in (0, \\pi)$. We evaluate the function at the boundaries of its domain:\n$f(0) = \\frac{1}{2}$.\n$f(\\pi) = 0$.\nThe range of $f(\\alpha)$ on $[0, \\pi]$ is therefore $[0, \\frac{1}{2}]$. The problem statement correctly constrains the target $s$ to this range. The strict monotonicity guarantees that for any $s \\in (0, \\frac{1}{2})$, there exists a unique solution $\\alpha \\in (0, \\pi)$.\n\nFor the boundary cases specified in the test suite:\nIf $s = 0.5$, the equation becomes $f(\\alpha) = 0.5$, which holds only for $\\alpha = 0$.\nIf $s = 0$, the equation is $f(\\alpha) = 0$, which holds only for $\\alpha = \\pi$.\nThese are trivial analytical solutions.\n\nFor a general $s \\in (0, \\frac{1}{2})$, the equation is transcendental and requires a numerical root-finding algorithm. We aim to solve $g(\\alpha) = 0$. We have established that for such $s$, $g(0) = f(0) - s = \\frac{1}{2} - s > 0$ and $g(\\pi) = f(\\pi) - s = -s < 0$. The change of sign confirms the existence of a root within the interval $(0, \\pi)$. This makes the interval $[0, \\pi]$ a suitable bracket for any bracketing root-finding method. We will employ a robust method such as Brent's method, which combines the speed of interpolation methods with the guaranteed convergence of bisection. This approach is superior to gradient-based methods like Newton-Raphson, as the derivative $f'(\\alpha)$ vanishes at the endpoints $\\alpha=0$ and $\\alpha=\\pi$, which degrades the convergence of such methods for roots near these points.\n\nThe computational procedure is as follows:\n$1$. For each target value $s$ from the test suite.\n$2$. Handle trivial cases: if $s = 0.5$, the solution is $\\alpha = 0$; if $s = 0$, the solution is $\\alpha = \\pi$.\n$3$. For other $s$, define the function $g(\\alpha) = \\frac{1}{2} - s - \\frac{\\alpha}{2\\pi} + \\frac{\\sin(2\\alpha)}{4\\pi}$ and numerically find its root within the interval $[0, \\pi]$ using a reliable solver implementing Brent's method.\n$4$. The required output accuracy is $10$ decimal places, corresponding to an absolute error less than $0.5 \\times 10^{-10}$. To ensure this, the solver's absolute tolerance on $\\alpha$ will be set to $10^{-12}$.\n$5$. The final results are collected and formatted as specified. For the case $s = 0.25$, one may confirm by substitution that the exact solution is $\\alpha = \\pi/2$, providing a useful check on the numerical implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Computes the angle alpha for given target values s by solving the equation f(alpha) = s,\n    where f(alpha) is a normalized integral.\n    \"\"\"\n    # Define the test cases for the target value s.\n    test_cases = [\n        0.25,\n        0.5,\n        0,\n        1e-6,\n        0.499999,\n    ]\n\n    results = []\n    \n    # The function f(alpha) is derived analytically as:\n    # f(alpha) = 1/2 - alpha/(2*pi) + sin(2*alpha)/(4*pi)\n    \n    for s in test_cases:\n        # Handle trivial edge cases analytically to ensure precision and avoid numerical issues.\n        if s == 0.5:\n            alpha = 0.0\n        elif s == 0.0:\n            alpha = np.pi\n        else:\n            # Define the function g(alpha) = f(alpha) - s whose root we need to find.\n            def g(alpha, target_s):\n                \"\"\"\n                Target function for the root-finding algorithm.\n                g(alpha) = (1/2 - s) - alpha/(2*pi) + sin(2*alpha)/(4*pi)\n                \"\"\"\n                return (0.5 - target_s) - alpha / (2 * np.pi) + np.sin(2 * alpha) / (4 * np.pi)\n\n            # Use a robust bracketing method (Brent's method is the default for root_scalar\n            # when a bracket is provided) to find the root in the interval [0, pi].\n            # A tight absolute tolerance (xtol) ensures the required output accuracy of 10^-10 radians.\n            try:\n                sol = root_scalar(g, args=(s,), bracket=[0, np.pi], xtol=1e-12, rtol=1e-12)\n                alpha = sol.root\n            except ValueError:\n                # This block should not be reached for valid s in [0, 0.5],\n                # but is good practice for robust code.\n                alpha = np.nan\n\n        # Format the result to exactly 10 decimal places as required.\n        results.append(f\"{alpha:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2433781"}]}