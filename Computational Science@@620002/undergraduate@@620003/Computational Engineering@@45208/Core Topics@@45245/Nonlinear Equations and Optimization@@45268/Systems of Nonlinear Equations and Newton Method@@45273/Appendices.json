{"hands_on_practices": [{"introduction": "The true power of a numerical algorithm is revealed when it is applied to solve tangible problems. This first practice moves Newton's method from abstract theory to a concrete application in computational economics [@problem_id:2441954]. You will determine the equilibrium prices in a market with nonlinear supply and demand functions, a classic problem that translates directly into finding the root of a system of nonlinear equations. This exercise will guide you through implementing the full Newton's method, including a backtracking line search to ensure robust convergence, providing a solid foundation for building practical solvers.", "problem": "Consider a two-good competitive market with nonlinear supply and demand. Let the price vector be $\\mathbf{p} = (p_1,p_2)$ with $p_1 > 0$ and $p_2 > 0$. The demand functions for goods $1$ and $2$ are given by\n$$\nQ_1^d(\\mathbf{p}) = a_1 - b_1\\, p_1^{\\alpha_1} + c_{12}\\, p_2^{\\gamma_{12}}, \\quad\nQ_2^d(\\mathbf{p}) = a_2 - b_2\\, p_2^{\\alpha_2} + c_{21}\\, p_1^{\\gamma_{21}}.\n$$\nThe supply functions for goods $1$ and $2$ are given by\n$$\nQ_1^s(p_1) = d_1 + e_1\\, p_1^{\\beta_1}, \\quad\nQ_2^s(p_2) = d_2 + e_2\\, p_2^{\\beta_2}.\n$$\nAn equilibrium is defined by the system of nonlinear equations enforcing market clearing in each good:\n$$\nF_1(\\mathbf{p}) = Q_1^d(\\mathbf{p}) - Q_1^s(p_1) = 0, \\quad\nF_2(\\mathbf{p}) = Q_2^d(\\mathbf{p}) - Q_2^s(p_2) = 0.\n$$\nYou must compute equilibrium prices and quantities using only first principles and general numerical analysis facts. Specifically:\n- Start from the definition of equilibrium $F(\\mathbf{p}) = \\mathbf{0}$, where $F(\\mathbf{p}) = (F_1(\\mathbf{p}),F_2(\\mathbf{p}))^\\top$.\n- Construct and implement an iterative root-finding method that uses a first-order local linearization of $F$ at the current iterate to generate a search direction, and apply a step length rule that guarantees all iterates remain strictly positive and that the residual norm decreases.\n- Stop the iteration when the Euclidean norm of the residual $\\lVert F(\\mathbf{p}) \\rVert_2$ is below a tolerance or when a maximum number of iterations is reached. If the maximum number of iterations is reached without meeting the tolerance, return the last iterate that satisfies positivity.\n\nUnits: Report prices in Currency Units (CU) and quantities in Quantity Units (QU). When printing results, round all reported numbers to $6$ decimal places.\n\nUse the following test suite of parameter sets. For each case, use the initial guess $\\mathbf{p}^{(0)} = (10,10)$ and the stopping tolerance $\\varepsilon = 10^{-10}$ with a maximum of $50$ outer iterations. In all cases, require iterates to satisfy $p_i \\ge 10^{-12}$, and use a backtracking factor of $1/2$ for step length reduction when needed.\n\n- Case $1$ (baseline, moderate cross-price effects):\n  - $a_1 = 120.0$, $b_1 = 1.2$, $\\alpha_1 = 1.5$, $c_{12} = 0.4$, $\\gamma_{12} = 1.2$, $d_1 = 10.0$, $e_1 = 0.9$, $\\beta_1 = 1.3$.\n  - $a_2 = 100.0$, $b_2 = 1.0$, $\\alpha_2 = 1.4$, $c_{21} = 0.3$, $\\gamma_{21} = 1.1$, $d_2 = 12.0$, $e_2 = 1.1$, $\\beta_2 = 1.2$.\n\n- Case $2$ (small-market scale, low prices):\n  - $a_1 = 35.0$, $b_1 = 0.9$, $\\alpha_1 = 1.2$, $c_{12} = 0.15$, $\\gamma_{12} = 1.3$, $d_1 = 5.0$, $e_1 = 0.8$, $\\beta_1 = 1.1$.\n  - $a_2 = 30.0$, $b_2 = 0.7$, $\\alpha_2 = 1.25$, $c_{21} = 0.2$, $\\gamma_{21} = 1.15$, $d_2 = 4.0$, $e_2 = 0.6$, $\\beta_2 = 1.05$.\n\n- Case $3$ (strong cross-price substitution, still well-conditioned):\n  - $a_1 = 80.0$, $b_1 = 1.1$, $\\alpha_1 = 1.6$, $c_{12} = 0.9$, $\\gamma_{12} = 1.05$, $d_1 = 8.0$, $e_1 = 1.0$, $\\beta_1 = 1.25$.\n  - $a_2 = 90.0$, $b_2 = 1.0$, $\\alpha_2 = 1.55$, $c_{21} = 0.8$, $\\gamma_{21} = 1.1$, $d_2 = 7.0$, $e_2 = 0.95$, $\\beta_2 = 1.2$.\n\nFor each parameter set, compute the equilibrium prices $\\hat{p}_1$ and $\\hat{p}_2$ in CU and the corresponding equilibrium quantities $\\hat{q}_1$ and $\\hat{q}_2$ in QU, where $\\hat{q}_1 = Q_1^d(\\hat{\\mathbf{p}}) = Q_1^s(\\hat{p}_1)$ and $\\hat{q}_2 = Q_2^d(\\hat{\\mathbf{p}}) = Q_2^s(\\hat{p}_2)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with numbers rounded to $6$ decimal places. Concatenate the results for the cases in the given order, appending for each case the four numbers $\\hat{p}_1$, $\\hat{p}_2$, $\\hat{q}_1$, $\\hat{q}_2$ in that order. For example, the output structure should be of the form\n$$\n[\\hat{p}_1^{(1)},\\hat{p}_2^{(1)},\\hat{q}_1^{(1)},\\hat{q}_2^{(1)},\\hat{p}_1^{(2)},\\hat{p}_2^{(2)},\\hat{q}_1^{(2)},\\hat{q}_2^{(2)},\\hat{p}_1^{(3)},\\hat{p}_2^{(3)},\\hat{q}_1^{(3)},\\hat{q}_2^{(3)}].\n$$", "solution": "The problem presented is scientifically sound and mathematically well-posed. It requires the computation of an economic equilibrium in a two-good market, which translates to finding the root of a system of two nonlinear equations. The prescribed method is a first-order iterative scheme, which is properly realized as Newton's method with a line search for globalization and to enforce physical constraints. We will construct the solution from first principles as required.\n\nThe system is defined by the market-clearing conditions:\n$$F_1(p_1, p_2) = Q_1^d(p_1, p_2) - Q_1^s(p_1) = 0$$\n$$F_2(p_1, p_2) = Q_2^d(p_1, p_2) - Q_2^s(p_2) = 0$$\nLet $\\mathbf{p} = (p_1, p_2)^\\top$ be the vector of prices. The system of equations can be written as $F(\\mathbf{p}) = \\mathbf{0}$, where $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$. Substituting the given supply and demand functions, we have:\n$$F_1(\\mathbf{p}) = (a_1 - d_1) - b_1 p_1^{\\alpha_1} - e_1 p_1^{\\beta_1} + c_{12} p_2^{\\gamma_{12}} = 0$$\n$$F_2(\\mathbf{p}) = (a_2 - d_2) - b_2 p_2^{\\alpha_2} - e_2 p_2^{\\beta_2} + c_{21} p_1^{\\gamma_{21}} = 0$$\nThese equations must be solved for $p_1 > 0$ and $p_2 > 0$.\n\nThe problem specifies an iterative method based on a first-order local linearization of $F(\\mathbf{p})$. This is precisely Newton's method. At an iterate $\\mathbf{p}^{(k)}$, we find a search direction $\\Delta\\mathbf{p}^{(k)}$ by solving the linear system that arises from the Taylor expansion of $F$ around $\\mathbf{p}^{(k)}$:\n$$F(\\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}) \\approx F(\\mathbf{p}^{(k)}) + J_F(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}^{(k)}$$\nSetting the left side to $\\mathbf{0}$ yields the Newton step equation:\n$$J_F(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}^{(k)} = -F(\\mathbf{p}^{(k)})$$\nwhere $J_F(\\mathbf{p})$ is the Jacobian matrix of $F$ with respect to $\\mathbf{p}$. The components of the Jacobian are the partial derivatives of $F_1$ and $F_2$:\n$$J_F(\\mathbf{p}) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial p_1} & \\frac{\\partial F_1}{\\partial p_2} \\\\ \\frac{\\partial F_2}{\\partial p_1} & \\frac{\\partial F_2}{\\partial p_2} \\end{pmatrix}$$\nThe required partial derivatives are:\n$$\\frac{\\partial F_1}{\\partial p_1} = -b_1 \\alpha_1 p_1^{\\alpha_1 - 1} - e_1 \\beta_1 p_1^{\\beta_1 - 1}$$\n$$\\frac{\\partial F_1}{\\partial p_2} = c_{12} \\gamma_{12} p_2^{\\gamma_{12} - 1}$$\n$$\\frac{\\partial F_2}{\\partial p_1} = c_{21} \\gamma_{21} p_1^{\\gamma_{21} - 1}$$\n$$\\frac{\\partial F_2}{\\partial p_2} = -b_2 \\alpha_2 p_2^{\\alpha_2 - 1} - e_2 \\beta_2 p_2^{\\beta_2 - 1}$$\nThe next iterate is then found by $\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}$. However, to ensure convergence from a starting point far from the solution and to maintain the positivity of prices, a damped Newton step is used:\n$$\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\lambda^{(k)} \\Delta\\mathbf{p}^{(k)}$$\nThe step length $\\lambda^{(k)} \\in (0, 1]$ is determined by a backtracking line search. Starting with $\\lambda=1$, we test the candidate point $\\mathbf{p}_{trial} = \\mathbf{p}^{(k)} + \\lambda \\Delta\\mathbf{p}^{(k)}$. We accept the step if it satisfies two conditions:\n$1$. Positivity: All components of $\\mathbf{p}_{trial}$ must be greater than or equal to a small positive threshold, specified as $10^{-12}$.\n$2$. Sufficient decrease: The Euclidean norm of the residual at the trial point must be less than the norm at the current point: $\\lVert F(\\mathbf{p}_{trial}) \\rVert_2 < \\lVert F(\\mathbf{p}^{(k)}) \\rVert_2$.\n\nIf either condition fails, the step length $\\lambda$ is reduced by a backtracking factor of $1/2$, and the trial is repeated. This process continues until an acceptable step is found.\n\nThe overall algorithm is as follows:\n$1$. Initialize the price vector $\\mathbf{p}^{(0)} = (10, 10)$, the iteration counter $k=0$, the tolerance $\\varepsilon = 10^{-10}$, the maximum number of iterations $N_{max} = 50$, the minimum price bound $p_{min} = 10^{-12}$, and the backtracking factor $\\rho = 1/2$.\n$2$. For $k = 0, 1, 2, \\dots, N_{max}-1$:\n    a. Evaluate the residual vector $F_k = F(\\mathbf{p}^{(k)})$ and its Euclidean norm $\\lVert F_k \\rVert_2$.\n    b. Check for convergence: If $\\lVert F_k \\rVert_2 < \\varepsilon$, terminate and set the equilibrium price $\\hat{\\mathbf{p}} = \\mathbf{p}^{(k)}$.\n    c. Evaluate the Jacobian matrix $J_k = J_F(\\mathbf{p}^{(k)})$.\n    d. Solve the linear system $J_k \\Delta\\mathbf{p}^{(k)} = -F_k$ for the Newton direction $\\Delta\\mathbf{p}^{(k)}$.\n    e. Initialize step length $\\lambda = 1$.\n    f. Perform backtracking line search:\n        i. Compute trial point $\\mathbf{p}_{trial} = \\mathbf{p}^{(k)} + \\lambda \\Delta\\mathbf{p}^{(k)}$.\n        ii. If all elements of $\\mathbf{p}_{trial}$ are $\\ge p_{min}$ and $\\lVert F(\\mathbf{p}_{trial}) \\rVert_2 < \\lVert F_k \\rVert_2$, then accept the step: set $\\mathbf{p}^{(k+1)} = \\mathbf{p}_{trial}$ and break the line search.\n        iii. Otherwise, reduce step length $\\lambda \\leftarrow \\rho \\lambda$. If $\\lambda$ becomes smaller than a machine precision threshold, break the line search to prevent an infinite loop, and the iteration proceeds with an unchanged $\\mathbf{p}$.\n$3$. If the loop completes without convergence, the last valid iterate $\\mathbf{p}^{(N_{max})}$ is taken as the result.\n$4$. Once the equilibrium price vector $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)$ is determined, the corresponding equilibrium quantities are calculated using the demand functions (or equivalently, the supply functions):\n$$\\hat{q}_1 = Q_1^d(\\hat{\\mathbf{p}}) = a_1 - b_1 \\hat{p}_1^{\\alpha_1} + c_{12} \\hat{p}_2^{\\gamma_{12}}$$\n$$\\hat{q}_2 = Q_2^d(\\hat{\\mathbf{p}}) = a_2 - b_2 \\hat{p}_2^{\\alpha_2} + c_{21} \\hat{p}_1^{\\gamma_{21}}$$\nThis procedure will be implemented for each of the three parameter sets provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve for market equilibrium for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains parameters in the order:\n    # (a1, b1, alpha1, c12, gamma12, d1, e1, beta1,\n    #  a2, b2, alpha2, c21, gamma21, d2, e2, beta2)\n    test_cases = [\n        (120.0, 1.2, 1.5, 0.4, 1.2, 10.0, 0.9, 1.3,\n         100.0, 1.0, 1.4, 0.3, 1.1, 12.0, 1.1, 1.2),\n        (35.0, 0.9, 1.2, 0.15, 1.3, 5.0, 0.8, 1.1,\n         30.0, 0.7, 1.25, 0.2, 1.15, 4.0, 0.6, 1.05),\n        (80.0, 1.1, 1.6, 0.9, 1.05, 8.0, 1.0, 1.25,\n         90.0, 1.0, 1.55, 0.8, 1.1, 7.0, 0.95, 1.2)\n    ]\n    \n    # Common numerical parameters for the solver\n    p0 = np.array([10.0, 10.0])\n    tol = 1e-10\n    max_iter = 50\n    min_p_val = 1e-12\n    backtrack_factor = 0.5\n    \n    results = []\n    for params in test_cases:\n        p_hat = find_equilibrium(params, p0, tol, max_iter, min_p_val, backtrack_factor)\n        \n        # Unpack parameters for quantity calculation\n        a1, b1, alpha1, c12, gamma12, _, _, _, \\\n        a2, b2, alpha2, c21, gamma21, _, _, _ = params\n        p1, p2 = p_hat\n        \n        # Calculate equilibrium quantities using demand functions\n        q1_hat = a1 - b1 * p1**alpha1 + c12 * p2**gamma12\n        q2_hat = a2 - b2 * p2**alpha2 + c21 * p1**gamma21\n        \n        results.extend([p1, p2, q1_hat, q2_hat])\n\n    # Format the final output string with numbers rounded to 6 decimal places.\n    output_str = f\"[{','.join([f'{x:.6f}' for x in results])}]\"\n    print(output_str)\n\ndef find_equilibrium(params, p_init, tol, max_iter, min_p, backtrack_factor):\n    \"\"\"\n    Implements Newton's method with backtracking to find equilibrium prices.\n    \n    Args:\n        params (tuple): A tuple of all model parameters.\n        p_init (np.ndarray): Initial guess for the price vector.\n        tol (float): Convergence tolerance for the residual norm.\n        max_iter (int): Maximum number of iterations.\n        min_p (float): Minimum allowed price value.\n        backtrack_factor (float): Factor for reducing step size in line search.\n\n    Returns:\n        np.ndarray: The equilibrium price vector.\n    \"\"\"\n    p = np.copy(p_init)\n    \n    # Unpack parameters\n    a1, b1, alpha1, c12, gamma12, d1, e1, beta1, \\\n    a2, b2, alpha2, c21, gamma21, d2, e2, beta2 = params\n    \n    def F(pr):\n        p1, p2 = pr[0], pr[1]\n        f1 = (a1 - d1) - b1 * p1**alpha1 - e1 * p1**beta1 + c12 * p2**gamma12\n        f2 = (a2 - d2) - b2 * p2**alpha2 - e2 * p2**beta2 + c21 * p1**gamma21\n        return np.array([f1, f2])\n\n    def J(pr):\n        p1, p2 = pr[0], pr[1]\n        j11 = -b1 * alpha1 * p1**(alpha1 - 1) - e1 * beta1 * p1**(beta1 - 1)\n        j12 = c12 * gamma12 * p2**(gamma12 - 1)\n        j21 = c21 * gamma21 * p1**(gamma21 - 1)\n        j22 = -b2 * alpha2 * p2**(alpha2 - 1) - e2 * beta2 * p2**(beta2 - 1)\n        return np.array([[j11, j12], [j21, j22]])\n\n    for _ in range(max_iter):\n        F_val = F(p)\n        res_norm = np.linalg.norm(F_val)\n        \n        if res_norm < tol:\n            break\n            \n        J_val = J(p)\n        try:\n            delta_p = np.linalg.solve(J_val, -F_val)\n        except np.linalg.LinAlgError:\n            # Jacobian is singular, cannot proceed with Newton step.\n            # This indicates a problem; break and return the current best guess.\n            break\n\n        # Backtracking line search\n        lambda_step = 1.0\n        while lambda_step > 1e-8: # Prevent excessively small steps\n            p_trial = p + lambda_step * delta_p\n            if np.all(p_trial >= min_p):\n                res_norm_trial = np.linalg.norm(F(p_trial))\n                if res_norm_trial < res_norm:\n                    p = p_trial\n                    break  # Step accepted\n            \n            lambda_step *= backtrack_factor\n    \n    return p\n\nsolve()\n```", "id": "2441954"}, {"introduction": "While Newton's method is powerful, its reliance on an analytically derived Jacobian matrix can be a significant hurdle in practice, as derivatives may be complex or unavailable. This exercise introduces a crucial technique to overcome this limitation: approximating the Jacobian using finite differences [@problem_id:2441924]. You will implement and compare a \"quasi-Newton\" method using this approximation against the standard Newton's method with its exact Jacobian. This hands-on comparison will provide deep insight into the practical trade-offs between implementation effort and the rate of convergence, a central theme in computational science.", "problem": "Consider the nonlinear system of equations defined by the vector-valued function $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ given by\n$$\n\\mathbf{F}(\\mathbf{x})=\n\\begin{bmatrix}\nf_1(x_1,x_2)\\\\\nf_2(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1-\\cos(x_2)\\\\\nx_2-\\sin(x_1)\n\\end{bmatrix},\n$$\nwhere $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ and the trigonometric functions use angles measured in radians. The exact Jacobian matrix $\\mathbf{J}(\\mathbf{x})$ of $\\mathbf{F}$ is\n$$\n\\mathbf{J}(\\mathbf{x})=\n\\begin{bmatrix}\n\\dfrac{\\partial f_1}{\\partial x_1} & \\dfrac{\\partial f_1}{\\partial x_2}\\\\\n\\dfrac{\\partial f_2}{\\partial x_1} & \\dfrac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & \\sin(x_2)\\\\\n-\\cos(x_1) & 1\n\\end{bmatrix}.\n$$\n\nDefine, for a given perturbation size $h>0$, the forward finite-difference Jacobian approximation $\\mathbf{J}_h(\\mathbf{x})$ by\n$$\n\\mathbf{J}_h(\\mathbf{x})=\\begin{bmatrix}\n\\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_1(\\mathbf{x})}{h} & \\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_1(\\mathbf{x})}{h}\\\\\n\\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_2(\\mathbf{x})}{h} & \\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_2(\\mathbf{x})}{h}\n\\end{bmatrix},\n$$\nwhere $\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ and $\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$.\n\nFor an initial guess $\\mathbf{x}^{(0)}\\in\\mathbb{R}^2$, define the sequence $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ recursively by\n$$\n\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big),\\qquad \\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)},\n$$\nwhere $\\mathbf{J}_\\star$ denotes either the exact Jacobian $\\mathbf{J}$ or the finite-difference Jacobian $\\mathbf{J}_h$, and the update $\\mathbf{s}^{(k)}$ is any solution of the linear system. Let the residual norm be $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$, and let the iteration terminate at the smallest index $k$ such that $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon$, with tolerance $\\varepsilon=10^{-10}$, or after $k_{\\max}=50$ iterations, whichever occurs first.\n\nFor each test case below, perform two runs starting from the given initial guess $\\mathbf{x}^{(0)}$:\n- Run A uses $\\mathbf{J}_\\star=\\mathbf{J}$.\n- Run B uses $\\mathbf{J}_\\star=\\mathbf{J}_h$ with the specified $h$.\n\nFor each run, record:\n- The number of iterations $n$ required to satisfy $\\|\\mathbf{F}(\\mathbf{x}^{(n)})\\|_2 \\le \\varepsilon$ (or $n=k_{\\max}$ if the tolerance is not met).\n- An estimate $\\hat{p}$ of the local convergence order computed from the last three available residual norms $\\{e_{m-2},e_{m-1},e_m\\}$ via\n$$\n\\hat{p}=\\frac{\\ln\\left(\\dfrac{e_m}{e_{m-1}}\\right)}{\\ln\\left(\\dfrac{e_{m-1}}{e_{m-2}}\\right)},\n$$\nwhere $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$, and $m$ is the final iteration index used in the run (use the last three residuals available at termination; all logarithms are natural logarithms). All angles in trigonometric functions are in radians.\n\nUse the following test suite, where each case is a pair $(\\mathbf{x}^{(0)},h)$:\n- Case $1$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$.\n- Case $2$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$.\n- Case $3$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$.\n- Case $4$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$.\n\nYour program must output a single line containing a list of results, one per test case, in the same order as listed. Each test case result must be a list of four entries $[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}]$, where $n_{\\text{exact}}$ and $\\hat{p}_{\\text{exact}}$ correspond to Run A, and $n_{\\text{fd}}$ and $\\hat{p}_{\\text{fd}}$ correspond to Run B. The final output format must be a single line that is a comma-separated list of these per-case lists enclosed in square brackets, for example, $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Nonlinear System**: A function $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ is defined as $\\mathbf{F}(\\mathbf{x})= \\begin{bmatrix} f_1(x_1,x_2)\\\\ f_2(x_1,x_2) \\end{bmatrix} = \\begin{bmatrix} x_1-\\cos(x_2)\\\\ x_2-\\sin(x_1) \\end{bmatrix}$ for $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$. Trigonometric functions use radians.\n- **Exact Jacobian**: The Jacobian matrix of $\\mathbf{F}$ is given as $\\mathbf{J}(\\mathbf{x}) = \\begin{bmatrix} 1 & \\sin(x_2)\\\\ -\\cos(x_1) & 1 \\end{bmatrix}$.\n- **Finite-Difference Jacobian**: An approximation $\\mathbf{J}_h(\\mathbf{x})$ is defined for a perturbation $h>0$ by its columns: the $j$-th column is $\\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j)-\\mathbf{F}(\\mathbf{x})}{h}$, where $\\mathbf{e}_j$ are the standard basis vectors.\n- **Iterative Scheme**: A sequence $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ is generated from an initial guess $\\mathbf{x}^{(0)}$ by solving $\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big)$ and setting $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)}$. Here, $\\mathbf{J}_\\star$ is either the exact Jacobian $\\mathbf{J}$ or the approximation $\\mathbf{J}_h$.\n- **Termination Criteria**: The iteration stops at the smallest index $k$ where the residual norm $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon = 10^{-10}$, or after $k_{\\max}=50$ iterations.\n- **Tasks**: For each test case, two runs are performed: Run A with $\\mathbf{J}_\\star=\\mathbf{J}$ and Run B with $\\mathbf{J}_\\star=\\mathbf{J}_h$. For each run, two quantities are to be recorded:\n    1. The number of iterations, $n$.\n    2. An estimate of the convergence order, $\\hat{p}=\\frac{\\ln(e_m/e_{m-1})}{\\ln(e_{m-1}/e_{m-2})}$, where $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$ and $m$ is the final iteration index.\n- **Test Cases**:\n    - Case 1: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$.\n    - Case 2: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$.\n    - Case 3: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$.\n    - Case 4: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$.\n- **Output Format**: A single-line list of lists: $[[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}], \\dots]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientific Grounding**: The problem is a standard exercise in numerical analysis, specifically in the field of computational methods for solving systems of nonlinear equations. Newton's method and its quasi-Newton variants (using finite-difference Jacobians) are fundamental, well-established algorithms. The mathematical formulations are correct.\n- **Well-Posedness**: The problem is clearly defined. The functions, Jacobians, iterative scheme, termination conditions, and required outputs are all specified unambiguously. The given system of equations has a unique solution in the domain of interest, and the Jacobian is non-singular near this solution, ensuring that the linear systems to be solved are well-posed. For example, the determinant of the Jacobian is $\\det(\\mathbf{J}) = 1 + \\cos(x_1)\\sin(x_2)$. For initial guesses like $(0.5, 0.5)$ or $(1.0, 1.0)$, the determinant is well away from zero, suggesting local convergence is achievable.\n- **Objectivity**: The language is formal and objective, free from subjective or non-scientific content.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. The calculation of $\\hat{p}$ requires at least three residual norms, corresponding to at least two iterations ($n\\ge 2$). Given the initial conditions and the nature of Newton's method, this is a reasonable expectation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n\nThe problem requires the implementation of two variants of Newton's method to solve the nonlinear system $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$. The first variant is the classical Newton-Raphson method, which uses the exact analytical Jacobian of $\\mathbf{F}$. The second is a quasi-Newton method where the Jacobian is approximated using a forward finite-difference scheme. We will compare the performance of these two methods in terms of the number of iterations required for convergence and the estimated local order of convergence.\n\nThe core of the method is the iterative update rule. At each step $k$, we approximate the nonlinear function $\\mathbf{F}$ with its linear Taylor expansion around the current iterate $\\mathbf{x}^{(k)}$:\n$$\n\\mathbf{F}(\\mathbf{x}) \\approx \\mathbf{F}(\\mathbf{x}^{(k)}) + \\mathbf{J}_\\star(\\mathbf{x}^{(k)})(\\mathbf{x} - \\mathbf{x}^{(k)})\n$$\nWe seek the next iterate $\\mathbf{x}^{(k+1)}$ by setting this approximation to zero, i.e., $\\mathbf{F}(\\mathbf{x}^{(k+1)}) = \\mathbf{0}$. Defining the update step as $\\mathbf{s}^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$, we obtain the linear system for the update:\n$$\n\\mathbf{J}_\\star(\\mathbf{x}^{(k)}) \\mathbf{s}^{(k)} = -\\mathbf{F}(\\mathbf{x}^{(k)})\n$$\nOnce $\\mathbf{s}^{(k)}$ is found by solving this system, the next iterate is computed as $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)}$. This process is repeated until the norm of the residual vector, $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$, falls below a specified tolerance $\\varepsilon = 10^{-10}$.\n\n**Run A: Exact Jacobian (Newton-Raphson Method)**\nIn this run, $\\mathbf{J}_\\star$ is the exact Jacobian matrix $\\mathbf{J}(\\mathbf{x})$:\n$$\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{bmatrix}\n1 & \\sin(x_2)\\\\\n-\\cos(x_1) & 1\n\\end{bmatrix}\n$$\nThis method is known to exhibit quadratic convergence (i.e., convergence order $p=2$) when the initial guess is sufficiently close to the solution and the Jacobian is non-singular at the solution. This means the number of correct digits in the solution roughly doubles with each iteration. The estimated order $\\hat{p}$ should therefore be close to $2$.\n\n**Run B: Finite-Difference Jacobian (Quasi-Newton Method)**\nIn this run, the Jacobian is approximated using the forward finite-difference formula. The $j$-th column of the approximate Jacobian $\\mathbf{J}_h(\\mathbf{x})$ is given by:\n$$\n[\\mathbf{J}_h(\\mathbf{x})]_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th standard basis vector and $h$ is a small perturbation parameter. For the given $2 \\times 2$ system, this yields:\n$$\n\\mathbf{J}_h(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{f_1(x_1+h, x_2) - f_1(x_1, x_2)}{h} & \\frac{f_1(x_1, x_2+h) - f_1(x_1, x_2)}{h} \\\\\n\\frac{f_2(x_1+h, x_2) - f_2(x_1, x_2)}{h} & \\frac{f_2(x_1, x_2+h) - f_2(x_1, x_2)}{h}\n\\end{bmatrix}\n$$\nThis approach avoids the need for analytical derivation of the Jacobian, which can be complex or impossible for some functions. However, it introduces an approximation error. The error in each element of $\\mathbf{J}_h$ is of order $O(h)$. This error perturbs the Newton step, affecting the convergence rate. For a very small $h$ (e.g., $10^{-6}$), the approximation is accurate, and the convergence should be nearly quadratic. As $h$ increases (e.g., $10^{-3}$ or $10^{-2}$), the approximation worsens, and the convergence rate is expected to degrade, potentially becoming linear ($p=1$), and requiring more iterations.\n\n**Algorithm and Implementation**\nFor each test case $(\\mathbf{x}^{(0)}, h)$, we perform the following steps for both Run A and Run B:\n$1$. Initialize $k=0$ and the current solution $\\mathbf{x} = \\mathbf{x}^{(0)}$. Create a list to store residual norms.\n$2$. Begin a loop that continues as long as $k \\le k_{\\max} = 50$.\n$3$. Compute the residual vector $\\mathbf{F}(\\mathbf{x})$ and its Euclidean norm $e_k = \\|\\mathbf{F}(\\mathbf{x})\\|_2$. Store $e_k$.\n$4$. Check for termination: if $e_k \\le \\varepsilon=10^{-10}$, set the final iteration count $n=k$ and exit the loop.\n$5$. If the loop is to continue (i.e., $k < k_{\\max}$), compute the appropriate Jacobian matrix $\\mathbf{J}_\\star(\\mathbf{x})$ (either exact or finite-difference).\n$6$. Solve the linear system $\\mathbf{J}_\\star(\\mathbf{x})\\mathbf{s} = -\\mathbf{F}(\\mathbf{x})$ for the step $\\mathbf{s}$.\n$7$. Update the solution: $\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}$.\n$8$. Increment the iteration counter: $k \\leftarrow k+1$.\n$9$. If the loop completes because $k$ reached $k_{\\max}$, set $n=k_{\\max}$.\n$10$. After the loop terminates, calculate the estimated convergence order $\\hat{p}$ using the last three available residual norms, $e_{n-2}, e_{n-1}, e_{n}$. If fewer than three norms are available (i.e., $n<2$), $\\hat{p}$ is considered not computable.\nThe results $(n, \\hat{p})$ from both runs are collected for each test case to form the final output. This process will demonstrate the theoretical properties of Newton-family methods in a practical computational context.", "answer": "```python\nimport numpy as np\n\n# Global constants as specified in the problem\nTOLERANCE = 1e-10\nK_MAX = 50\n\ndef F(x):\n    \"\"\"\n    Computes the vector-valued function F(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        x[0] - np.cos(x[1]),\n        x[1] - np.sin(x[0])\n    ])\n\ndef J_exact(x):\n    \"\"\"\n    Computes the exact Jacobian matrix J(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        [1.0, np.sin(x[1])],\n        [-np.cos(x[0]), 1.0]\n    ])\n\ndef J_fd(x, h):\n    \"\"\"\n    Computes the forward finite-difference approximation of the Jacobian.\n    x must be a NumPy array [x1, x2].\n    h is the perturbation size.\n    \"\"\"\n    n = len(x)\n    jac = np.zeros((n, n), dtype=float)\n    fx = F(x)\n    for j in range(n):\n        x_plus_h = x.copy()\n        x_plus_h[j] += h\n        fx_plus_h = F(x_plus_h)\n        jac[:, j] = (fx_plus_h - fx) / h\n    return jac\n\ndef newton_solver(x0, h, use_exact_jacobian):\n    \"\"\"\n    Solves the nonlinear system F(x)=0 using a Newton-like method.\n    \n    Args:\n        x0 (list or np.ndarray): The initial guess.\n        h (float): The perturbation size for the finite-difference Jacobian.\n        use_exact_jacobian (bool): If True, use the exact Jacobian. \n                                  If False, use the finite-difference approximation.\n\n    Returns:\n        tuple: (n_iter, p_hat) where n_iter is the number of iterations and\n               p_hat is the estimated convergence order.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    k = 0\n    residuals = []\n    n_iter = K_MAX\n\n    while k <= K_MAX:\n        F_val = F(x)\n        norm = np.linalg.norm(F_val)\n        residuals.append(norm)\n\n        if norm <= TOLERANCE:\n            n_iter = k\n            break\n        \n        if k == K_MAX:\n            break\n\n        if use_exact_jacobian:\n            Jacobian = J_exact(x)\n        else:\n            Jacobian = J_fd(x, h)\n\n        try:\n            # Solve the linear system J*s = -F\n            s = np.linalg.solve(Jacobian, -F_val)\n            x += s\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, the solver fails.\n            n_iter = K_MAX\n            break\n        \n        k += 1\n    \n    # Estimate convergence order p_hat from the last three residuals\n    p_hat = np.nan\n    if len(residuals) >= 3:\n        # Use residuals e_n, e_{n-1}, e_{n-2}\n        e_m, e_m1, e_m2 = residuals[-1], residuals[-2], residuals[-3]\n        \n        # Avoid division by zero or log of non-positive numbers.\n        # Ratios must be < 1 for convergence.\n        ratio1 = e_m / e_m1 if e_m1 != 0 else 0\n        ratio2 = e_m1 / e_m2 if e_m2 != 0 else 0\n\n        if 0 < ratio1 < 1 and 0 < ratio2 < 1:\n            p_hat = np.log(ratio1) / np.log(ratio2)\n            \n    return n_iter, p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        ([0.5, 0.5], 1e-6),\n        ([0.5, 0.5], 1e-3),\n        ([1.0, 1.0], 1e-6),\n        ([1.0, 1.0], 1e-2)\n    ]\n\n    all_results = []\n    for x0, h in test_cases:\n        # Run A: Exact Jacobian\n        n_exact, p_exact = newton_solver(x0, h, use_exact_jacobian=True)\n        \n        # Run B: Finite-Difference Jacobian\n        n_fd, p_fd = newton_solver(x0, h, use_exact_jacobian=False)\n        \n        # Assemble results for the current test case\n        case_result = [n_exact, n_fd, p_exact, p_fd]\n        all_results.append(case_result)\n\n    # The final print statement must match the format exactly.\n    # The default string representation of a list of lists is \"[...], [...]\"\n    # and the default representation of a float is what we need.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the main function\nsolve()\n```", "id": "2441924"}, {"introduction": "Efficiency is a primary concern in computational engineering, and the cost of repeatedly forming and factorizing the Jacobian matrix can be substantial. This advanced practice introduces Shamanskii's method, an intelligent modification of Newton's method designed to amortize this cost [@problem_id:2441976]. By reusing a single Jacobian for a fixed number of \"inner\" iterations, this algorithm strikes a balance between the rapid convergence of the standard Newton's method and the low per-iteration cost of a modified Newton scheme. Implementing this method will deepen your understanding of the broader family of quasi-Newton methods and the art of optimizing numerical algorithms for performance.", "problem": "You are to implement a solver for systems of nonlinear equations based on a variant of Newton’s method known as Shamanskii’s method. The computational task is purely mathematical and requires no physical units. All trigonometric functions must interpret angles in radians. The goal is to build the method starting from the definition of the root-finding problem for vector-valued functions and the first-order Taylor expansion. The method must compute a Jacobian matrix at an “outer” iterate, reuse this same Jacobian for a prescribed number of subsequent “inner” correction steps, and then refresh the Jacobian again, until a termination criterion is met.\n\nStart from the core definition: given a continuously differentiable function $F:\\mathbb{R}^n\\to\\mathbb{R}^n$, a root is a vector $x^\\star\\in\\mathbb{R}^n$ such that $F(x^\\star)=0$. Use the well-tested fact that a first-order Taylor expansion around a point $x\\in\\mathbb{R}^n$ approximates the function by $F(x+s)\\approx F(x)+J(x)s$, where $J(x)$ is the Jacobian matrix. In Newton-type methods, one constructs corrections $s$ by solving a linear system derived from this expansion to reduce the residual $F(x)$. In Shamanskii’s method, the Jacobian matrix is held fixed for several inner steps to amortize its computational cost: compute the Jacobian matrix and its factorization once at an outer iterate, then perform a prescribed number of inner correction steps using this fixed Jacobian, recomputing the Jacobian only after these inner steps or upon satisfaction of a termination criterion. You must implement robust stopping conditions that halt when either the residual norm or the step norm is sufficiently small.\n\nImplementation requirements:\n- Implement a function that, given $F$, its Jacobian $J$, an initial guess $x_0\\in\\mathbb{R}^n$, a reuse parameter $m\\in\\mathbb{N}$, a maximum total iteration count $k_{\\max}\\in\\mathbb{N}$, a residual tolerance $\\varepsilon_F>0$, and a step tolerance $\\varepsilon_s>0$, attempts to find an approximate root of $F(x)=0$.\n- The method must proceed in “outer” blocks: at the start of each block, evaluate and factorize the Jacobian at the current iterate. Then perform up to $m$ “inner” correction steps using this fixed Jacobian factorization, updating the right-hand sides with the current residuals. After $m$ inner steps, start a new outer block by recomputing the Jacobian, unless the method has already converged or the iteration limit is reached.\n- Use the Euclidean norm for both residual and step norms.\n- Terminate successfully when either the residual norm is less than or equal to $\\varepsilon_F$ or the step norm is less than or equal to $\\varepsilon_s$.\n- Angles in all trigonometric functions must be in radians.\n- Round each component of the final approximate solution to $10$ decimal places for output.\n\nTest suite:\nImplement your method and apply it to the following four test cases. For each case, provide an initial guess, the system $F$, its Jacobian $J$, the reuse parameter $m$, a maximum total iteration count $k_{\\max}$, and tolerances $(\\varepsilon_F,\\varepsilon_s)$.\n\n- Test case $\\#1$ (two variables, algebraic, baseline):\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}x^2+y^2-1\\\\ x-y\\end{bmatrix}.$$\n  - Jacobian,\n    $$J(x,y)=\\begin{bmatrix}2x & 2y\\\\ 1 & -1\\end{bmatrix}.$$\n  - Initial guess $x_0=\\begin{bmatrix}0.8\\\\ 0.6\\end{bmatrix}$.\n  - Reuse parameter $m=1$.\n  - Maximum iterations $k_{\\max}=50$.\n  - Tolerances $\\varepsilon_F=10^{-12}$ and $\\varepsilon_s=10^{-12}$.\n\n- Test case $\\#2$ (two variables with trigonometric nonlinearity, angles in radians):\n  - Let $x^\\star=0.8$ and $y^\\star=0.6$. Define $c=\\sin(x^\\star)+y^\\star$.\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}\\sin(x)+y-c\\\\ x^2+y^2-1\\end{bmatrix}.$$\n  - Jacobian,\n    $$J(x,y)=\\begin{bmatrix}\\cos(x) & 1\\\\ 2x & 2y\\end{bmatrix}.$$\n  - Initial guess $x_0=\\begin{bmatrix}0.7\\\\ 0.7\\end{bmatrix}$.\n  - Reuse parameter $m=3$.\n  - Maximum iterations $k_{\\max}=50$.\n  - Tolerances $\\varepsilon_F=10^{-12}$ and $\\varepsilon_s=10^{-12}$.\n\n- Test case $\\#3$ (three variables, coupled polynomial and exponential, known root):\n  - Let $e$ denote the base of the natural logarithm. Define $F:\\mathbb{R}^3\\to\\mathbb{R}^3$,\n    $$F(x,y,z)=\\begin{bmatrix}x+y+z-3\\\\ x^2+y^2+z^2-3\\\\ e^x+yz-(e+1)\\end{bmatrix}.$$\n  - Jacobian,\n    $$J(x,y,z)=\\begin{bmatrix}1 & 1 & 1\\\\ 2x & 2y & 2z\\\\ e^x & z & y\\end{bmatrix}.$$\n  - Initial guess $x_0=\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}$.\n  - Reuse parameter $m=2$.\n  - Maximum iterations $k_{\\max}=50$.\n  - Tolerances $\\varepsilon_F=10^{-12}$ and $\\varepsilon_s=10^{-12}$.\n\n- Test case $\\#4$ (two variables, Rosenbrock-like system):\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}10(y-x^2)\\\\ 1-x\\end{bmatrix}.$$\n  - Jacobian,\n    $$J(x,y)=\\begin{bmatrix}-20x & 10\\\\ -1 & 0\\end{bmatrix}.$$\n  - Initial guess $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$.\n  - Reuse parameter $m=5$.\n  - Maximum iterations $k_{\\max}=50$.\n  - Tolerances $\\varepsilon_F=10^{-12}$ and $\\varepsilon_s=10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the components of the final approximate solution, with each component rounded to $10$ decimal places. For example, the overall structure must look like\n  [[a11,a12],[a21,a22],[a31,a32,a33],[a41,a42]]\nwhere the bracket and comma placement matches exactly, and each a-value is a floating-point number printed with exactly $10$ digits after the decimal point.", "solution": "The user's problem is to implement Shamanskii's method for solving systems of nonlinear equations. The problem is scientifically sound, mathematically well-posed, and provides all necessary information for a complete and unambiguous solution. Thus, we proceed with the derivation and implementation.\n\nThe core objective is to find a vector $x^\\star \\in \\mathbb{R}^n$ that is a root of a continuously differentiable vector-valued function $F: \\mathbb{R}^n \\to \\mathbb{R}^n$, such that $F(x^\\star) = 0$.\n\nThe basis for this class of methods is the first-order Taylor series expansion of the function $F$ around a point $x_k$. For a small correction vector $s_k$, the function value at a new point $x_{k+1} = x_k + s_k$ can be approximated by a linear model:\n$$F(x_k + s_k) \\approx F(x_k) + J(x_k)s_k$$\nHere, $J(x_k)$ represents the Jacobian matrix of $F$ evaluated at the iterate $x_k$. The entry at row $i$ and column $j$ of the Jacobian is given by the partial derivative $J_{ij}(x) = \\frac{\\partial F_i}{\\partial x_j}(x)$.\n\nTo drive the function value towards zero, we set the linear approximation equal to the zero vector:\n$$F(x_k) + J(x_k)s_k = 0$$\nThis yields the fundamental linear system of Newton's method, which must be solved for the correction step $s_k$:\n$$J(x_k) s_k = -F(x_k)$$\nSolving this system provides the correction, and the next iterate is defined as $x_{k+1} = x_k + s_k$. In the standard Newton's method, this procedure, which includes the computationally expensive evaluation and factorization of the Jacobian matrix $J(x_k)$, is performed at every single iteration $k$.\n\nShamanskii's method offers a modification to reduce this computational burden. It is based on the observation that the Jacobian often varies slowly near a solution. Therefore, the method reuses a previously computed Jacobian for a fixed number of subsequent iterations. This is controlled by a reuse parameter $m \\in \\mathbb{N}$.\n\nThe algorithm for Shamanskii's method proceeds as follows:\n\nLet $x_0$ be the initial guess, $m$ the reuse parameter, $k_{\\max}$ the maximum number of iterations, and $\\varepsilon_F$ and $\\varepsilon_s$ the tolerances for the residual norm and step norm, respectively. The total iteration count begins at $k=0$.\n\n1.  **Initial State Check**: Before any iteration, the algorithm first checks if the initial guess $x_0$ is already a sufficiently accurate root. This is done by computing the Euclidean norm of the residual, $\\|F(x_0)\\|$. If $\\|F(x_0)\\| \\le \\varepsilon_F$, the process terminates immediately, returning $x_0$ as the solution.\n\n2.  **Iterative Process**: If the initial guess is not a solution, the main iterative loop begins and continues as long as the total iteration count $k$ is less than $k_{\\max}$.\n\n3.  **Jacobian Update (Outer Block)**: At the beginning of \"outer\" blocks of iterations, the Jacobian is updated. This occurs when the iteration counter $k$ is a multiple of the reuse parameter $m$ (i.e., at $k=0, m, 2m, \\ldots$). At these specific iterations, the Jacobian matrix is evaluated at the current iterate $x_k$:\n    $$J_{fixed} = J(x_k)$$\n    This matrix is immediately factorized using an LU decomposition with pivoting, $P J_{fixed} = LU$. The resulting factors $L$ and $U$ (and the permutation matrix $P$) are stored for use in the following \"inner\" steps.\n\n4.  **Correction Step (Inner Block)**: For every iteration $k$, including those where the Jacobian is updated, the following steps are performed:\n    a. The negative residual vector, $b_k = -F(x_k)$, is computed. This will serve as the right-hand side of the linear system.\n    b. The linear system $J_{fixed} s_k = b_k$ is solved for the correction vector $s_k$. This is done efficiently by using the stored LU factorization to perform forward and backward substitution.\n    c. The new iterate is computed by applying the correction: $x_{k+1} = x_k + s_k$.\n    d. The iteration counter is incremented: $k \\leftarrow k+1$.\n\n5.  **Termination Criteria Check**: After computing the step $s_{k-1}$ and updating the iterate to $x_k$, the algorithm checks for convergence. The process terminates successfully and returns the current iterate $x_k$ if either of the two conditions is satisfied:\n    - The Euclidean norm of the residual at the new point is within tolerance: $\\|F(x_k)\\| \\le \\varepsilon_F$.\n    - The Euclidean norm of the previous step is within tolerance: $\\|s_{k-1}\\| \\le \\varepsilon_s$.\n\n6.  **Failure Condition**: If the loop completes, meaning $k$ reaches $k_{\\max}$, without the convergence criteria being met, the algorithm terminates. It returns the final computed iterate $x_{k_{\\max}}$, indicating that the method did not converge within the specified iteration limit.\n\nFor the special case of $m=1$, Shamanskii's method is identical to the standard Newton's method. For large $m$, it approaches the modified Newton's method, where the Jacobian computed at $x_0$ is used for all steps. The parameter $m$ thus allows a trade-off between the convergence rate (typically faster for smaller $m$) and the computational cost per iteration (lower for larger $m$). The provided test cases explore this behavior with different systems and choices of $m$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve, LinAlgError\n\ndef shamanskii_solver(F, J, x0, m, k_max, eps_F, eps_s):\n    \"\"\"\n    Implements Shamanskii's method for solving systems of nonlinear equations F(x) = 0.\n\n    Args:\n        F (callable): The vector-valued function F(x).\n        J (callable): The Jacobian function J(x).\n        x0 (list or np.ndarray): The initial guess.\n        m (int): The Jacobian reuse parameter.\n        k_max (int): Maximum number of iterations.\n        eps_F (float): Tolerance for the residual norm.\n        eps_s (float): Tolerance for the step norm.\n\n    Returns:\n        np.ndarray: The approximate solution vector.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    \n    # Initial check: if the starting point is already a solution.\n    F_val = F(x)\n    if np.linalg.norm(F_val) <= eps_F:\n        return x\n\n    k = 0\n    lu_piv = None\n\n    while k < k_max:\n        # Recompute the Jacobian and its LU factorization periodically.\n        if k % m == 0:\n            J_val = J(x)\n            try:\n                # lu_factor returns (LU matrix, permutation indices)\n                lu_piv = lu_factor(J_val)\n            except LinAlgError:\n                # Jacobian is singular, cannot proceed.\n                # The method fails, return the last valid iterate.\n                return x\n\n        # F_val is from the previous iterate's end-of-loop calculation.\n        # It's -F(x_k) that forms the RHS of the linear system.\n        s = lu_solve(lu_piv, -F_val)\n\n        # Calculate the norm of the correction step.\n        step_norm = np.linalg.norm(s)\n        \n        # Update the solution vector. This is x_{k+1}.\n        x = x + s\n        k += 1\n\n        # Calculate the new residual at x_{k+1} for the next iteration's RHS\n        # and for the current iteration's convergence check.\n        F_val = F(x)\n        residual_norm = np.linalg.norm(F_val)\n\n        # Check for convergence based on residual norm or step norm.\n        if residual_norm <= eps_F or step_norm <= eps_s:\n            return x\n\n    # If the loop completes without convergence, return the last computed vector.\n    return x\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the Shamanskii solver.\n    \"\"\"\n    # Test Case 1\n    F1 = lambda v: np.array([v[0]**2 + v[1]**2 - 1.0, v[0] - v[1]])\n    J1 = lambda v: np.array([[2.0*v[0], 2.0*v[1]], [1.0, -1.0]])\n    x0_1 = [0.8, 0.6]\n    params1 = (F1, J1, x0_1, 1, 50, 1e-12, 1e-12)\n\n    # Test Case 2\n    x_star, y_star = 0.8, 0.6\n    c = np.sin(x_star) + y_star\n    F2 = lambda v: np.array([np.sin(v[0]) + v[1] - c, v[0]**2 + v[1]**2 - 1.0])\n    J2 = lambda v: np.array([[np.cos(v[0]), 1.0], [2.0*v[0], 2.0*v[1]]])\n    x0_2 = [0.7, 0.7]\n    params2 = (F2, J2, x0_2, 3, 50, 1e-12, 1e-12)\n\n    # Test Case 3\n    F3 = lambda v: np.array([v[0] + v[1] + v[2] - 3.0,\n                             v[0]**2 + v[1]**2 + v[2]**2 - 3.0,\n                             np.exp(v[0]) + v[1]*v[2] - (np.e + 1.0)])\n    J3 = lambda v: np.array([[1.0, 1.0, 1.0],\n                             [2.0*v[0], 2.0*v[1], 2.0*v[2]],\n                             [np.exp(v[0]), v[2], v[1]]])\n    x0_3 = [1.0, 1.0, 1.0]\n    params3 = (F3, J3, x0_3, 2, 50, 1e-12, 1e-12)\n\n    # Test Case 4\n    F4 = lambda v: np.array([10.0 * (v[1] - v[0]**2), 1.0 - v[0]])\n    J4 = lambda v: np.array([[-20.0 * v[0], 10.0], [-1.0, 0.0]])\n    x0_4 = [-1.2, 1.0]\n    params4 = (F4, J4, x0_4, 5, 50, 1e-12, 1e-12)\n\n    test_cases = [params1, params2, params3, params4]\n    \n    results = []\n    for F, J, x0, m, k_max, eps_F, eps_s in test_cases:\n        solution = shamanskii_solver(F, J, x0, m, k_max, eps_F, eps_s)\n        \n        # Format each component to 10 decimal places as a string.\n        rounded_solution = [f\"{comp:.10f}\" for comp in solution]\n        \n        # Format the list of components into the required string format, e.g., \"[1.0,2.0]\".\n        results.append(f\"[{','.join(rounded_solution)}]\")\n\n    # Print the final combined string of all results.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2441976"}]}