{"hands_on_practices": [{"introduction": "To truly master nonlinear solvers, we must move beyond simply using them and learn how to analyze their performance from first principles. This exercise challenges you to derive the convergence order for a family of iterative methods known as King's method [@problem_id:2381957]. By meticulously applying Taylor series expansions to the error term, you will gain hands-on experience with the fundamental analytical technique used to classify the efficiency of virtually any iterative solver.", "problem": "Consider a scalar nonlinear equation $f(x)=0$ with a simple root at $x=\\alpha$, that is, $f(\\alpha)=0$ and $f'(\\alpha)\\neq 0$. Assume $f$ is three times continuously differentiable in a neighborhood of $\\alpha$. For an initial guess $x_0$ sufficiently close to $\\alpha$, consider the one-parameter iteration\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\frac{f(x_k)}{f'(x_k)} \\;-\\; \\frac{f(x_k)^{2}\\,f''(x_k)}{2\\,f'(x_k)^{3}\\,\\bigl(1 + \\beta\\,f(x_k)/f'(x_k)\\bigr)},\n$$\nwhere $\\beta\\in\\mathbb{R}$ is a fixed parameter and the denominator is assumed nonzero over the iterates.\n\nUsing the definition of order of convergence $p$ for an error sequence $e_k=x_k-\\alpha$, namely that there exists a finite constant $C\\neq 0$ such that $\\lim_{k\\to\\infty}\\frac{|e_{k+1}|}{|e_k|^{p}}=|C|$, determine the order of convergence $p$ of this method for arbitrary real $\\beta$ under the above assumptions. Provide your final answer as a single integer. No rounding is needed, and no units are required.", "solution": "The validity of the problem statement is confirmed. It is a well-posed problem in numerical analysis, free of scientific or logical flaws.\n\nTo determine the order of convergence of the given iterative method, we analyze the relationship between the error at step $k+1$, denoted by $e_{k+1} = x_{k+1} - \\alpha$, and the error at step $k$, $e_k = x_k - \\alpha$. The order of convergence is an integer $p$ such that $e_{k+1} = C e_k^p + O(e_k^{p+1})$ for some constant $C \\neq 0$, as $k \\to \\infty$.\n\nThe iterative formula is given by:\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} - \\frac{f(x_k)^{2}\\,f''(x_k)}{2\\,f'(x_k)^{3}\\,\\bigl(1 + \\beta\\,f(x_k)/f'(x_k)\\bigr)}\n$$\nSubtracting the root $\\alpha$ from both sides gives the error recurrence relation:\n$$\ne_{k+1} = e_k - \\frac{f(x_k)}{f'(x_k)} - \\frac{f(x_k)^{2}\\,f''(x_k)}{2\\,f'(x_k)^{3}\\,\\bigl(1 + \\beta\\,f(x_k)/f'(x_k)\\bigr)}\n$$\nOur analysis relies on the Taylor series expansion of $f(x)$ and its derivatives around the simple root $\\alpha$, where $f(\\alpha)=0$ and $f'(\\alpha)\\neq 0$. We assume $f$ is at least three times continuously differentiable in a neighborhood of $\\alpha$. Let $e_k = x_k - \\alpha$. The Taylor expansions are:\n$$\nf(x_k) = f(\\alpha+e_k) = f'(\\alpha)e_k + \\frac{f''(\\alpha)}{2}e_k^2 + \\frac{f'''(\\alpha)}{6}e_k^3 + O(e_k^4)\n$$\n$$\nf'(x_k) = f'(\\alpha+e_k) = f'(\\alpha) + f''(\\alpha)e_k + \\frac{f'''(\\alpha)}{2}e_k^2 + O(e_k^3)\n$$\n$$\nf''(x_k) = f''(\\alpha+e_k) = f''(\\alpha) + f'''(\\alpha)e_k + O(e_k^2)\n$$\nTo simplify the expressions, let us denote $A=f'(\\alpha)$, $B=f''(\\alpha)$, and $C=f'''(\\alpha)$, with $A \\neq 0$.\n\nFirst, we analyze the term corresponding to Newton's method, $\\frac{f(x_k)}{f'(x_k)}$:\n$$\n\\frac{f(x_k)}{f'(x_k)} = \\frac{A e_k + \\frac{B}{2} e_k^2 + \\frac{C}{6} e_k^3 + O(e_k^4)}{A + B e_k + \\frac{C}{2} e_k^2 + O(e_k^3)}\n$$\nFactoring out $e_k$ from the numerator and $A$ from the denominator:\n$$\n\\frac{f(x_k)}{f'(x_k)} = \\frac{e_k(A + \\frac{B}{2} e_k + \\frac{C}{6} e_k^2 + \\dots)}{A(1 + \\frac{B}{A} e_k + \\frac{C}{2A} e_k^2 + \\dots)} = e_k \\left(1 + \\frac{B}{2A} e_k + \\frac{C}{6A} e_k^2 + \\dots\\right) \\left(1 - \\frac{B}{A} e_k + \\left(\\frac{B^2}{A^2}-\\frac{C}{2A}\\right)e_k^2 + \\dots\\right)\n$$\nExpanding and collecting terms up to $O(e_k^3)$:\n$$\n\\frac{f(x_k)}{f'(x_k)} = e_k \\left(1 + \\left(\\frac{B}{2A} - \\frac{B}{A}\\right)e_k + \\left(\\frac{C}{6A} - \\frac{B^2}{2A^2} + \\frac{B^2}{A^2} - \\frac{C}{2A}\\right)e_k^2 + \\dots\\right)\n$$\n$$\n\\frac{f(x_k)}{f'(x_k)} = e_k - \\frac{B}{2A}e_k^2 + \\left(\\frac{B^2}{2A^2} - \\frac{C}{3A}\\right)e_k^3 + O(e_k^4)\n$$\nThe first part of the error recurrence, the error of Newton's method, is:\n$$\ne_k - \\frac{f(x_k)}{f'(x_k)} = \\frac{B}{2A}e_k^2 - \\left(\\frac{B^2}{2A^2} - \\frac{C}{3A}\\right)e_k^3 + O(e_k^4)\n$$\nThis demonstrates that Newton's method itself is quadratically convergent ($p=2$).\n\nNow, we analyze the correction term:\n$$\n\\text{Correction Term} = \\frac{f(x_k)^{2}\\,f''(x_k)}{2\\,f'(x_k)^{3}\\,\\bigl(1 + \\beta\\,f(x_k)/f'(x_k)\\bigr)}\n$$\nWe expand each component of this term in powers of $e_k$.\n- Numerator:\n  $f(x_k)^2 = (A e_k + \\frac{B}{2} e_k^2 + O(e_k^3))^2 = A^2 e_k^2 + A B e_k^3 + O(e_k^4)$\n  $f''(x_k) = B + C e_k + O(e_k^2)$\n  $f(x_k)^2 f''(x_k) = (A^2 e_k^2 + A B e_k^3 + O(e_k^4))(B + C e_k + O(e_k^2)) = A^2 B e_k^2 + (A^2 C + A B^2) e_k^3 + O(e_k^4)$\n- Denominator:\n  $f'(x_k)^3 = (A + B e_k + O(e_k^2))^3 = A^3 + 3 A^2 B e_k + O(e_k^2)$\n  $f(x_k)/f'(x_k) = e_k - \\frac{B}{2A}e_k^2 + O(e_k^3)$\n  $1 + \\beta f(x_k)/f'(x_k) = 1 + \\beta e_k - \\frac{\\beta B}{2A} e_k^2 + O(e_k^3)$\n  The denominator is $2(A^3 + 3A^2 B e_k + O(e_k^2))(1 + \\beta e_k + O(e_k^2)) = 2A^3(1 + (\\frac{3B}{A} + \\beta)e_k + O(e_k^2))$.\n\nThe correction term becomes:\n$$\n\\frac{A^2 B e_k^2 + (A^2 C + A B^2) e_k^3 + O(e_k^4)}{2A^3(1 + (\\frac{3B}{A} + \\beta)e_k + O(e_k^2))} = \\frac{e_k^2(A^2 B + (A^2 C + A B^2) e_k + \\dots)}{2A^3} \\left(1 - (\\frac{3B}{A} + \\beta)e_k + \\dots\\right)\n$$\n$$\n= \\frac{1}{2A^3} \\left[ A^2 B e_k^2 + \\left(A^2 C + A B^2 - A^2 B(\\frac{3B}{A} + \\beta)\\right)e_k^3 + O(e_k^4) \\right]\n$$\n$$\n= \\frac{B}{2A} e_k^2 + \\frac{1}{2A^3} \\left(A^2 C + A B^2 - 3AB^2 - \\beta A^2 B\\right)e_k^3 + O(e_k^4)\n$$\n$$\n= \\frac{B}{2A} e_k^2 + \\left(\\frac{C}{2A} - \\frac{B^2}{A^2} - \\frac{\\beta B}{2A}\\right)e_k^3 + O(e_k^4)\n$$\nNow, we substitute these expansions back into the error recurrence for $e_{k+1}$:\n$$\ne_{k+1} = \\left[ \\frac{B}{2A}e_k^2 - \\left(\\frac{B^2}{2A^2} - \\frac{C}{3A}\\right)e_k^3 \\right] - \\left[ \\frac{B}{2A} e_k^2 + \\left(\\frac{C}{2A} - \\frac{B^2}{A^2} - \\frac{\\beta B}{2A}\\right)e_k^3 \\right] + O(e_k^4)\n$$\nThe terms of order $e_k^2$ cancel, which is a key observation. This means the order of convergence is at least $3$.\n$$\ne_{k+1} = \\left[ -\\frac{B^2}{2A^2} + \\frac{C}{3A} - \\frac{C}{2A} + \\frac{B^2}{A^2} + \\frac{\\beta B}{2A} \\right] e_k^3 + O(e_k^4)\n$$\nCollecting the coefficients for the $e_k^3$ term:\n$$\ne_{k+1} = \\left[ \\left(1 - \\frac{1}{2}\\right)\\frac{B^2}{A^2} + \\left(\\frac{1}{3} - \\frac{1}{2}\\right)\\frac{C}{A} + \\frac{\\beta B}{2A} \\right] e_k^3 + O(e_k^4)\n$$\n$$\ne_{k+1} = \\left( \\frac{B^2}{2A^2} - \\frac{C}{6A} + \\frac{\\beta B}{2A} \\right) e_k^3 + O(e_k^4)\n$$\nThe asymptotic error constant is $C_3 = \\frac{3B^2 - AC + 3\\beta AB}{6A^2}$. Rewriting in terms of the function derivatives at $\\alpha$:\n$$\nC_3 = \\frac{3(f''(\\alpha))^2 - f'(\\alpha)f'''(\\alpha) + 3\\beta f'(\\alpha)f''(\\alpha)}{6(f'(\\alpha))^2}\n$$\nThe order of convergence is $3$ provided that this constant $C_3$ is not generally zero. For any given real value of $\\beta$, we can construct a function $f(x)$ for which $C_3 \\neq 0$. For instance, for $f(x)=x^2-1$ at $\\alpha=1$, we have $f'(\\alpha)=2, f''(\\alpha)=2, f'''(\\alpha)=0$, leading to $C_3 = \\frac{12+12\\beta}{24} = \\frac{1+\\beta}{2}$, which is nonzero unless $\\beta=-1$. If we pick $f(x) = \\exp(x)-1$ at $\\alpha=0$, we have $f'(\\alpha)=f''(\\alpha)=f'''(\\alpha)=1$, leading to $C_3 = \\frac{3-1+3\\beta}{6} = \\frac{2+3\\beta}{6}$, which is nonzero unless $\\beta=-2/3$.\nSince for any given $\\beta$, it is always possible to find a function $f(x)$ such that $C_3 \\neq 0$, the order of convergence cannot be guaranteed to be higher than $3$. As the $e_k^2$ term is shown to vanish for any function $f$ (with $f'(\\alpha)\\neq 0$) and any $\\beta$, the order is at least $3$.\nThus, the order of convergence of the method for an arbitrary real parameter $\\beta$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "2381957"}, {"introduction": "While we often focus on convergence, understanding how and why a solver fails is equally crucial for robust engineering software. This practice takes an unconventional \"reverse-engineering\" approach to explore a common failure mode: periodic cycling [@problem_id:2381953]. By constructing a function for which the secant method becomes trapped in a 4-cycle, you will develop a more profound intuition for the method's behavior and the delicate conditions upon which convergence depends.", "problem": "Consider the secant method for solving a scalar nonlinear equation $f(x)=0$, defined by the iteration\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; f(x_k)\\,\\frac{x_k - x_{k-1}}{\\,f(x_k) - f(x_{k-1})\\,},\n$$\nfor $k \\geq 1$, with given initial points $x_0$ and $x_1$. Let $x_0 = -1$ and $x_1 = 0$. Suppose there exists a scalar function $f$ that is the unique cubic polynomial interpolating the values\n$$\nf(-1) = y_1,\\quad f(0) = y_2,\\quad f(1) = 1,\\quad f(c) = y_4,\n$$\nwhere $y_1, y_2, y_4$ are nonzero real numbers, and $c$ is a positive real number not equal to $1$. It is required that the secant iterates generated from $x_0=-1$ and $x_1=0$ satisfy\n$$\nx_2 = 1,\\quad x_3 = c,\\quad x_4 = -1,\\quad x_5 = 0,\n$$\nafter which the sequence repeats with period $4$ in the same order. Determine the unique positive value of $c$ for which such a function $f$ exists. Express your final answer as an exact expression.", "solution": "The problem requires finding a unique positive value for a parameter $c$ such that a specific periodic sequence of iterates is generated by the secant method for a particular cubic polynomial $f(x)$.\n\nThe secant method iteration is given by\n$$ x_{k+1} = x_k - f(x_k)\\,\\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} $$\nThis relation must hold for $k \\geq 1$. By algebraic manipulation, this equation is equivalent to the more convenient linear form:\n$$ f(x_{k-1})(x_{k+1} - x_k) = f(x_k)(x_{k+1} - x_{k-1}) $$\nThis form avoids denominators which might be zero, although the secant method itself presupposes $f(x_k) \\neq f(x_{k-1})$.\n\nThe problem specifies a periodic sequence of iterates with period $4$:\n$x_0 = -1$\n$x_1 = 0$\n$x_2 = 1$\n$x_3 = c$\n$x_4 = -1$\n$x_5 = 0$\n\nThe function $f(x)$ is a cubic polynomial defined by four interpolation conditions at distinct points. The function values at these points are:\n$f(-1) = y_1$\n$f(0) = y_2$\n$f(1) = 1$\n$f(c) = y_4$\n\nWe apply the secant relation for successive values of $k$.\n\nFor $k=1$:\nThe iterates are $x_0=-1$, $x_1=0$, and $x_2=1$. The function values are $f(x_0) = f(-1) = y_1$ and $f(x_1) = f(0) = y_2$.\nSubstituting these into the relation $f(x_0)(x_2 - x_1) = f(x_1)(x_2 - x_0)$:\n$$ y_1 (1 - 0) = y_2 (1 - (-1)) $$\n$$ y_1 = 2y_2 \\quad (1) $$\n\nFor $k=2$:\nThe iterates are $x_1=0$, $x_2=1$, and $x_3=c$. The function values are $f(x_1) = f(0) = y_2$ and $f(x_2) = f(1) = 1$.\nSubstituting into $f(x_1)(x_3 - x_2) = f(x_2)(x_3 - x_1)$:\n$$ y_2 (c - 1) = 1 (c - 0) $$\nSince $c \\neq 1$, we can write:\n$$ y_2 = \\frac{c}{c-1} \\quad (2) $$\n\nFor $k=3$:\nThe iterates are $x_2=1$, $x_3=c$, and $x_4=-1$. The function values are $f(x_2) = f(1) = 1$ and $f(x_3) = f(c) = y_4$.\nSubstituting into $f(x_2)(x_4 - x_3) = f(x_3)(x_4 - x_2)$:\n$$ 1 (-1 - c) = y_4 (-1 - 1) $$\n$$ -(c+1) = -2y_4 $$\n$$ y_4 = \\frac{c+1}{2} \\quad (3) $$\n\nFor $k=4$:\nThe iterates are $x_3=c$, $x_4=-1$, and $x_5=0$. The function values are $f(x_3) = f(c) = y_4$ and $f(x_4) = f(-1) = y_1$.\nSubstituting into $f(x_3)(x_5 - x_4) = f(x_4)(x_5 - x_3)$:\n$$ y_4 (0 - (-1)) = y_1 (0 - c) $$\n$$ y_4 = -c y_1 \\quad (4) $$\n\nWe have a system of four equations for the four unknowns $y_1, y_2, y_4, c$. We must solve this system for $c$.\nFrom $(1)$ and $(2)$, we express $y_1$ in terms of $c$:\n$$ y_1 = 2y_2 = 2 \\left(\\frac{c}{c-1}\\right) = \\frac{2c}{c-1} $$\nNow we substitute the expressions for $y_1$ and $y_4$ (from $(3)$) into equation $(4)$:\n$$ \\frac{c+1}{2} = -c \\left(\\frac{2c}{c-1}\\right) $$\n$$ \\frac{c+1}{2} = -\\frac{2c^2}{c-1} $$\nThe problem states that $c$ is a positive real number not equal to $1$, so $c-1 \\neq 0$. We can multiply both sides by $2(c-1)$ to clear the denominators:\n$$ (c+1)(c-1) = -2(2c^2) $$\n$$ c^2 - 1 = -4c^2 $$\n$$ 5c^2 = 1 $$\n$$ c^2 = \\frac{1}{5} $$\nThis gives two possible solutions for $c$: $c = \\frac{1}{\\sqrt{5}}$ and $c = -\\frac{1}{\\sqrt{5}}$.\nThe problem requires the unique positive value of $c$. Thus, we must have:\n$$ c = \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}}{5} $$\nThis value is positive and not equal to $1$, consistent with the problem statement.\nWe must also ensure that the premise of a unique cubic polynomial holds. This requires the interpolation abscissas, $x = -1, 0, 1, c$, to be distinct. For $c=\\frac{\\sqrt{5}}{5}$, the four points are distinct.\nFinally, we verify that $y_1, y_2, y_4$ are nonzero for this value of $c$.\n$y_2 = \\frac{c}{c-1} = \\frac{1/\\sqrt{5}}{1/\\sqrt{5}-1} = \\frac{1}{1-\\sqrt{5}} = -\\frac{1+\\sqrt{5}}{4} \\neq 0$.\n$y_1 = 2y_2 = -\\frac{1+\\sqrt{5}}{2} \\neq 0$.\n$y_4 = \\frac{c+1}{2} = \\frac{1/\\sqrt{5}+1}{2} = \\frac{1+\\sqrt{5}}{2\\sqrt{5}} \\neq 0$.\nAll conditions are satisfied. The existence of such a function $f$ is guaranteed, and the value of $c$ is uniquely determined.", "answer": "$$ \\boxed{\\frac{\\sqrt{5}}{5}} $$", "id": "2381953"}, {"introduction": "Theoretical analysis is powerful, but in real-world computational engineering, we often work with large systems where analytical derivations are intractable. This hands-on computational problem bridges that gap by having you implement a \"frozen\" Jacobian Newton method to solve a nonlinear partial differential equation, a common scenario in fields like heat transfer and fluid dynamics [@problem_id:2381909]. You will learn to numerically estimate the convergence order from the solver's output, a vital skill for validating and debugging complex simulation codes.", "problem": "Consider the nonlinear partial differential equation $u_t = u_{xx} + u^2$ on the spatial interval $x \\in [0,1]$ with homogeneous Dirichlet boundary conditions $u(0,t)=0$ and $u(1,t)=0$, and a smooth initial condition $u(x,0)=u_0(x)$. Discretize space using a uniform grid with $m$ interior points and spacing $h = \\frac{1}{m+1}$, and let $u \\in \\mathbb{R}^m$ denote the vector of interior values at a given time. Use the second-order central-difference approximation for the spatial second derivative, so that the discrete Laplacian is represented by the tridiagonal matrix $L \\in \\mathbb{R}^{m \\times m}$ with diagonal entries $-\\frac{2}{h^2}$ and off-diagonal entries $\\frac{1}{h^2}$. Discretize time with the backward Euler method with time step $\\Delta t > 0$, so that at a single time step from $u^n$ to $u^{n+1}$, the nonlinear system $F(u^{n+1})=0$ to be solved is defined by\n$$\nF(u) \\equiv u - \\Delta t \\left(Lu + u^{\\circ 2}\\right) - u^n \\in \\mathbb{R}^m,\n$$\nwhere $u^{\\circ 2}$ denotes the componentwise square.\n\nDefine the Jacobian matrix of $F$ at a vector $w \\in \\mathbb{R}^m$ by\n$$\nJ(w) \\equiv I - \\Delta t \\left(L + 2\\,\\mathrm{diag}(w)\\right) \\in \\mathbb{R}^{m \\times m},\n$$\nwhere $I$ is the identity matrix and $\\mathrm{diag}(w)$ is the diagonal matrix with the entries of $w$ on the diagonal.\n\nA \"frozen\" Jacobian Newton method for solving $F(u)=0$ at each time step is defined as follows: choose a fixed vector $w_{\\mathrm{freeze}}$ and compute Newton-like iterations $u^{(k+1)} = u^{(k)} + s^{(k)}$ by solving the linear system\n$$\nJ(w_{\\mathrm{freeze}})\\, s^{(k)} = -F\\!\\left(u^{(k)}\\right),\n$$\nwith the Jacobian $J(w_{\\mathrm{freeze}})$ held fixed for all inner iterations $k$. In this problem, for a single time step from time level $n$ to $n+1$, take $w_{\\mathrm{freeze}} = u^n$ and the initial guess $u^{(0)} = u^n$.\n\nLet the sequence $\\{u^{(k)}\\}_{k \\ge 0}$ denote the inner iterations of the frozen Jacobian method for one time step, and define the step differences $\\delta_k \\equiv \\lVert u^{(k+1)} - u^{(k)} \\rVert_2$ for $k \\ge 0$, where $\\lVert \\cdot \\rVert_2$ is the Euclidean norm. The asymptotic convergence order $p$ of an iterative solver is defined by the existence of a constant $C > 0$ such that, for errors $e_k \\equiv \\lVert u^{(k)} - u^\\star \\rVert_2$ sufficiently small, the relation $e_{k+1} \\approx C\\, e_k^p$ holds, where $u^\\star$ is the exact solution of $F(u)=0$. A standard estimator for $p$ that eliminates the unknown $u^\\star$ uses the step differences:\n$$\n\\widehat{p}_k \\equiv \\frac{\\log\\left(\\delta_{k}/\\delta_{k-1}\\right)}{\\log\\left(\\delta_{k-1}/\\delta_{k-2}\\right)}, \\quad \\text{for } k \\ge 2,\n$$\nprovided the method is in the asymptotic regime and $\\delta_{k-2},\\delta_{k-1},\\delta_k > 0$.\n\nTask: Implement a complete program that\n- Constructs $L$ and $F$ as described above for a single backward Euler time step.\n- Implements the frozen Jacobian Newton method with $w_{\\mathrm{freeze}} = u^n$ and $u^{(0)} = u^n$.\n- Iterates until either the stopping criterion $\\lVert s^{(k)} \\rVert_2 \\le \\varepsilon$ is met or a maximum number of iterations $k_{\\max}$ is reached. Use the Euclidean norm for $\\lVert s^{(k)} \\rVert_2$.\n- Estimates the asymptotic convergence order $p$ using $\\widehat{p}_k$ computed with the last three available step differences. If fewer than three step differences are available, return a not-a-number indicator (for example, the special floating-point value $\\mathrm{NaN}$).\n- Uses the initial condition $u^n_i = A \\sin(\\pi x_i)$ at grid points $x_i = i h$ for $i = 1,2,\\dots,m$, where $A$ is a given amplitude and $\\pi$ is the constant $3.14159\\dots$.\n\nUse the following test suite of parameter sets for a single time step:\n- Test $1$: $m = 100$, $\\Delta t = 10^{-3}$, $A = 1.0$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 100$.\n- Test $2$: $m = 100$, $\\Delta t = 5 \\times 10^{-3}$, $A = 2.0$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 100$.\n- Test $3$: $m = 50$, $\\Delta t = 10^{-3}$, $A = 0.1$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 100$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry equal to the estimated convergence order $\\widehat{p}$ for the corresponding test, rounded to six decimal places (for example, $[1.000000,0.998532,1.001247]$). There are no physical units involved. Angles, if any appear in your own auxiliary computations, must be in radians. Express all decimal numbers as standard decimal numerals in the output.", "solution": "The problem statement is first subjected to rigorous validation to ascertain its scientific and mathematical integrity.\n\n### Problem Validation\n\n#### Step 1: Extracted Givens\nThe problem provides the following explicit information:\n-   **Governing Equation**: A nonlinear partial differential equation $u_t = u_{xx} + u^2$ on the domain $x \\in [0,1]$.\n-   **Boundary Conditions**: Homogeneous Dirichlet conditions, $u(0,t)=0$ and $u(1,t)=0$.\n-   **Spatial Discretization**: A uniform grid with $m$ interior points and grid spacing $h = \\frac{1}{m+1}$. The vector of interior values is $u \\in \\mathbb{R}^m$.\n-   **Discrete Laplacian**: A second-order central-difference approximation for $u_{xx}$ is represented by a tridiagonal matrix $L \\in \\mathbb{R}^{m \\times m}$ with diagonal entries of $-\\frac{2}{h^2}$ and off-diagonal entries of $\\frac{1}{h^2}$.\n-   **Temporal Discretization**: The backward Euler method with a time step $\\Delta t > 0$.\n-   **Nonlinear System**: For a time step from $u^n$ to $u^{n+1}$, the system to solve is $F(u^{n+1})=0$, where the function $F: \\mathbb{R}^m \\to \\mathbb{R}^m$ is defined as $F(u) \\equiv u - \\Delta t \\left(Lu + u^{\\circ 2}\\right) - u^n$. The notation $u^{\\circ 2}$ signifies the componentwise square.\n-   **Jacobian Matrix**: The Jacobian of $F$ at a vector $w \\in \\mathbb{R}^m$ is $J(w) \\equiv I - \\Delta t \\left(L + 2\\,\\mathrm{diag}(w)\\right)$, where $I$ is the identity matrix.\n-   **Iterative Solver**: A \"frozen\" Jacobian Newton method where the Jacobian is fixed at $J(w_{\\mathrm{freeze}})$ with $w_{\\mathrm{freeze}} = u^n$. The initial guess for the iterations is $u^{(0)} = u^n$. The iterative update is $u^{(k+1)} = u^{(k)} + s^{(k)}$, where $s^{(k)}$ is the solution to the linear system $J(w_{\\mathrm{freeze}})\\, s^{(k)} = -F\\!\\left(u^{(k)}\\right)$.\n-   **Convergence Order Estimation**: The asymptotic order $p$ is estimated by $\\widehat{p}_k \\equiv \\frac{\\log\\left(\\delta_{k}/\\delta_{k-1}\\right)}{\\log\\left(\\delta_{k-1}/\\delta_{k-2}\\right)}$ for $k \\ge 2$. The step differences are $\\delta_k \\equiv \\lVert u^{(k+1)} - u^{(k)} \\rVert_2 = \\lVert s^{(k)} \\rVert_2$. If fewer than three step differences are available, the result is a Not-a-Number (NaN) indicator.\n-   **Initial Data**: For the single time step under consideration, the state $u^n$ is given by $u^n_i = A \\sin(\\pi x_i)$ at grid points $x_i = i h$ for $i = 1, 2, \\dots, m$.\n-   **Solver Parameters**: The stopping criterion is $\\lVert s^{(k)} \\rVert_2 \\le \\varepsilon$ or a maximum of $k_{\\max}$ iterations.\n-   **Test Cases**:\n    1.  $m = 100$, $\\Delta t = 10^{-3}$, $A = 1.0$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 100$.\n    2.  $m = 100$, $\\Delta t = 5 \\times 10^{-3}$, $A = 2.0$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 100$.\n    3.  $m = 50$, $\\Delta t = 10^{-3}$, $A = 0.1$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 100$.\n\n#### Step 2: Validation Using Extracted Givens\nThe problem is assessed against the criteria of scientific validity and well-posedness.\n-   **Scientific Grounding**: The problem addresses the numerical solution of a reaction-diffusion equation, a fundamental topic in computational science. The chosen numerical methods—finite differences for space, backward Euler for time, and a frozen Jacobian Newton solver for the resulting nonlinear system—are standard and rigorously established in the field of numerical analysis.\n-   **Well-Posedness and Objectivity**: The problem is formulated with mathematical precision. All quantities, operators, and algorithms are explicitly defined. The task is to implement a specified computational procedure and calculate a well-defined metric. The frozen Jacobian Newton method is a form of fixed-point iteration, whose convergence properties are well-understood. It is expected to exhibit linear convergence (order $p=1$), and the problem asks for a numerical estimation of this order. The question is unambiguous and possesses a unique, meaningful procedure for its solution.\n-   **Completeness**: All necessary parameters ($m, \\Delta t, A, \\varepsilon, k_{\\max}$), definitions, and initial conditions are provided for each test case. The problem is self-contained.\n\n#### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically sound, mathematically well-posed, and complete. A solution will be provided.\n\n### Solution\n\nThe solution requires the implementation of a numerical algorithm for a single backward Euler time step. The algorithm is a frozen Jacobian Newton method. For each specified test case, the convergence order of this method is to be estimated.\n\nFirst, the discrete problem is set up. For a given number of interior grid points $m$, the spatial step size is $h = \\frac{1}{m+1}$. The vector of spatial coordinates for the interior points is $x$, with components $x_i = i \\cdot h$ for $i=1, \\dots, m$.\n\nThe discrete Laplacian $L \\in \\mathbb{R}^{m \\times m}$ is a symmetric tridiagonal matrix defined by its non-zero elements:\n$$\nL_{i,j} = \\frac{1}{h^2} \\begin{cases}\n-2 & \\text{if } i=j \\\\\n1 & \\text{if } |i-j|=1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nThe initial vector for the time step, $u^n \\in \\mathbb{R}^m$, is constructed using the given amplitude $A$ and the spatial coordinates:\n$$\nu^n_i = A \\sin(\\pi x_i)\n$$\n\nThe core of the solver is the frozen Jacobian Newton iteration. The Jacobian is evaluated only once, using the state $u^n$ as the freezing point $w_{\\text{freeze}}$. This yields the constant iteration matrix:\n$$\nJ_{\\text{freeze}} = J(u^n) = I - \\Delta t \\left(L + 2\\,\\mathrm{diag}(u^n)\\right)\n$$\nwhere $I$ is the $m \\times m$ identity matrix and $\\mathrm{diag}(u^n)$ is the diagonal matrix whose diagonal entries are the elements of the vector $u^n$.\n\nThe iterative process to find $u^{n+1}$ begins with the initial guess $u^{(0)} = u^n$. In each iteration $k$, one performs the following steps:\n1.  Evaluate the residual function $F$ at the current iterate $u^{(k)}$:\n    $$\n    F(u^{(k)}) = u^{(k)} - \\Delta t (L u^{(k)} + (u^{(k)})^{\\circ 2}) - u^n\n    $$\n2.  Solve the linear system for the Newton update step $s^{(k)}$:\n    $$\n    J_{\\text{freeze}} s^{(k)} = -F(u^{(k)})\n    $$\n3.  Update the solution vector: $u^{(k+1)} = u^{(k)} + s^{(k)}$.\n4.  Compute the Euclidean norm of the update, $\\delta_k = \\lVert s^{(k)} \\rVert_2$, and store it.\nThe process is terminated when either $\\delta_k \\le \\varepsilon$ or $k = k_{\\max}$.\n\nUpon termination of the loop after $N$ iterations (producing step norms $\\delta_0, \\dots, \\delta_{N-1}$), the convergence order is estimated. If $N  3$, there are insufficient data points to apply the three-point formula, and the result is NaN. Otherwise, the estimate $\\widehat{p}$ is calculated using the final three available step norms:\n$$\n\\widehat{p} = \\frac{\\log(\\delta_{N-1}/\\delta_{N-2})}{\\log(\\delta_{N-2}/\\delta_{N-3})}\n$$\nThis calculation is performed for each test case. The expected theoretical convergence order for this method is $p=1$ (linear convergence), so the numerical estimate $\\widehat{p}$ should be close to $1$.\n\nThe implementation will utilize the `numpy` library for all array, vector, and matrix operations, including `np.linalg.solve` for the linear system solution.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_frozen_newton_solver(m, delta_t, A, epsilon, k_max):\n    \"\"\"\n    Solves the nonlinear system for one backward Euler step using a frozen Jacobian\n    Newton method and estimates the convergence order.\n    \n    Args:\n        m (int): Number of interior spatial grid points.\n        delta_t (float): Time step size.\n        A (float): Amplitude of the initial sine wave profile.\n        epsilon (float): Tolerance for the stopping criterion.\n        k_max (int): Maximum number of iterations.\n        \n    Returns:\n        float: The estimated convergence order p, or np.nan if not computable.\n    \"\"\"\n    # 1. Setup grid and initial condition for the time step\n    h = 1.0 / (m + 1.0)\n    x = np.arange(1, m + 1) * h\n    u_n = A * np.sin(np.pi * x)\n\n    # 2. Construct discrete Laplacian L\n    h_sq_inv = 1.0 / (h**2)\n    main_diag_L = -2.0 * h_sq_inv * np.ones(m)\n    off_diag_L = h_sq_inv * np.ones(m - 1)\n    L = np.diag(main_diag_L) + np.diag(off_diag_L, k=1) + np.diag(off_diag_L, k=-1)\n    \n    # 3. Construct the frozen Jacobian matrix J_freeze\n    I = np.identity(m)\n    J_freeze = I - delta_t * (L + 2.0 * np.diag(u_n))\n    \n    # 4. Perform frozen Jacobian Newton iterations\n    u_k = np.copy(u_n)  # Initial guess u^(0) = u^n\n    step_diffs = []\n\n    for _ in range(k_max):\n        # Calculate the residual F(u_k)\n        Lu_k = L @ u_k\n        u_k_sq = np.square(u_k)\n        F_u_k = u_k - delta_t * (Lu_k + u_k_sq) - u_n\n        \n        # Solve the linear system for the step s_k\n        s_k = np.linalg.solve(J_freeze, -F_u_k)\n        \n        # Calculate the norm of the step (delta_k)\n        delta_k = np.linalg.norm(s_k, 2)\n        step_diffs.append(delta_k)\n        \n        # Update the solution\n        u_k += s_k\n        \n        # Check stopping criterion\n        if delta_k = epsilon:\n            break\n            \n    # 5. Estimate the convergence order p\n    if len(step_diffs)  3:\n        return np.nan\n        \n    # Use the last three step differences for estimation\n    # delta_k / delta_{k-1}\n    # delta_{k-1} / delta_{k-2}\n    d_last = step_diffs[-1]\n    d_mid = step_diffs[-2]\n    d_first = step_diffs[-3]\n\n    # Guard against division by zero or log of non-positive numbers\n    if d_mid = 0 or d_first = 0:\n        return np.nan\n\n    ratio1 = d_last / d_mid\n    ratio2 = d_mid / d_first\n\n    if ratio1 = 0 or ratio2 = 0 or ratio2 == 1.0:\n        return np.nan\n        \n    p_hat = np.log(ratio1) / np.log(ratio2)\n    \n    return p_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, delta_t, A, epsilon, k_max)\n        (100, 1e-3, 1.0, 1e-12, 100),\n        (100, 5e-3, 2.0, 1e-12, 100),\n        (50, 1e-3, 0.1, 1e-12, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        p_estimate = run_frozen_newton_solver(*case)\n        results.append(p_estimate)\n\n    # Format the final output string\n    formatted_results = []\n    for res in results:\n        if np.isnan(res):\n            formatted_results.append('nan')\n        else:\n            # Round to six decimal places\n            formatted_results.append(f'{res:.6f}')\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2381909"}]}