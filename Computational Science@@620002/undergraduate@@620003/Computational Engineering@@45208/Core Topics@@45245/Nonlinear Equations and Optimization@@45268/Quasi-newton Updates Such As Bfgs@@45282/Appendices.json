{"hands_on_practices": [{"introduction": "The Broyden-Fletcher-Goldfarb-Shanno (BFGS) update is not the only quasi-Newton formula, with the Symmetric Rank-1 (SR1) update offering a seemingly simpler form. However, algorithmic design requires not just simplicity but also numerical robustness. This exercise [@problem_id:2431086] provides a direct, hands-on comparison by guiding you to construct scenarios where the SR1 update's denominator approaches zero, causing it to become unstable or fail entirely. By observing how the BFGS update remains well-defined and effective under the same conditions, you will gain a crucial appreciation for the superior stability that has made BFGS a cornerstone of modern optimization software.", "problem": "Develop a complete, runnable program that, for a small set of explicitly specified cases, compares the behavior of the Symmetric Rank-1 (SR1) quasi-Newton update and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton update on quadratic models where the SR1 denominator can become arbitrarily small. For each case, the program must take as given a symmetric positive definite matrix $H \\in \\mathbb{R}^{n \\times n}$, a symmetric matrix $B_k \\in \\mathbb{R}^{n \\times n}$, and a nonzero vector $s_k \\in \\mathbb{R}^n$, and must define $y_k = H s_k$. The program must then perform the following computations for each case:\n\n1. Compute the SR1 denominator $\\Delta_k = s_k^T\\left(y_k - B_k s_k\\right)$. Define the relative near-zero test parameter $r = 10^{-8}$ and declare the boolean\n   $$\\text{SR1\\_near\\_zero} := \\left(|\\Delta_k| \\le r \\, \\|s_k\\|_2 \\, \\|y_k - B_k s_k\\|_2\\right).$$\n   This flag is intended to identify when the SR1 update\n   $$B_{k+1}^{\\text{SR1}} = B_k + \\frac{\\left(y_k - B_k s_k\\right)\\left(y_k - B_k s_k\\right)^T}{\\left(y_k - B_k s_k\\right)^T s_k}$$\n   would be considered numerically unsafe to apply due to a near-zero denominator. If the denominator is exactly zero, the inequality is satisfied and the flag must be set to true.\n\n2. Compute the BFGS update\n   $$B_{k+1}^{\\text{BFGS}} = B_k \\;-\\; \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} \\;+\\; \\frac{y_k y_k^T}{s_k^T y_k}.$$\n   Then test whether $B_{k+1}^{\\text{BFGS}}$ is numerically positive definite by checking if its minimum eigenvalue exceeds the positive-definiteness tolerance $\\epsilon_{\\text{pd}} = 10^{-12}$. Denote this boolean by $\\text{BFGS\\_PD}$.\n\n3. Compute the Frobenius-norm error between the BFGS-updated matrix and the true $H$,\n   $$E_{\\text{F}} = \\left\\|B_{k+1}^{\\text{BFGS}} - H\\right\\|_F.$$\n\nYour program must apply these computations to each of the following three test cases, where every matrix and vector is explicitly specified:\n\n- Test case $\\#1$ (a well-conditioned, non-pathological case):\n  - $n = 2$.\n  - $H^{(1)} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$.\n  - $B_k^{(1)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$.\n  - $s_k^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n  - $y_k^{(1)} = H^{(1)} s_k^{(1)}$.\n\n- Test case $\\#2$ (near-orthogonality between $s_k$ and $y_k - B_k s_k$ makes the SR1 denominator relatively near zero while $y_k - B_k s_k$ is not small):\n  - $n = 2$.\n  - $H^{(2)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}$.\n  - Let $\\varepsilon = 10^{-12}$.\n  - $B_k^{(2)} = \\begin{bmatrix} 2 - \\varepsilon & 0 \\\\ 0 & 1 \\end{bmatrix}$.\n  - $s_k^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n  - $y_k^{(2)} = H^{(2)} s_k^{(2)}$.\n\n- Test case $\\#3$ (exact orthogonality makes the SR1 denominator exactly zero):\n  - $n = 2$.\n  - $H^{(3)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}$.\n  - $B_k^{(3)} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$.\n  - $s_k^{(3)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n  - $y_k^{(3)} = H^{(3)} s_k^{(3)}$.\n\nFor each test case $i \\in \\{1,2,3\\}$, your program must output a list\n$$\\left[\\text{SR1\\_near\\_zero}^{(i)},\\, \\text{BFGS\\_PD}^{(i)},\\, E_{\\text{F}}^{(i)}\\right],$$\nwhere the first two entries are booleans and the last entry is a floating-point number. The floating-point number must be rounded to exactly $8$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results for all three test cases as a comma-separated list of the three lists, with no spaces anywhere. For example, the required structure is\n$$\\texttt{[[b11,b12,f1],[b21,b22,f2],[b31,b32,f3]]},$$\nwhere each $\\texttt{bij}$ is either $\\texttt{True}$ or $\\texttt{False}$ and each $\\texttt{fi}$ is a decimal with exactly $8$ digits after the decimal point. There are no physical units or angles in this problem; all numeric outputs are dimensionless real numbers.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in the principles of numerical optimization, is well-posed with all necessary information provided, and is stated with objective, unambiguous language. We shall proceed with a complete solution.\n\nThe core of this problem is to investigate and contrast the numerical behavior of two prominent quasi-Newton update formulas, the Symmetric Rank-1 (SR1) and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) updates, under conditions that are known to be problematic for the former. The quasi-Newton methods iteratively build an approximation $B_k$ to the Hessian matrix of the objective function. The update from $B_k$ to $B_{k+1}$ uses information from the most recent step, encapsulated by the vectors $s_k = x_{k+1} - x_k$ (the change in position) and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ (the change in gradient). For a quadratic objective function $f(x) = \\frac{1}{2} x^T H x + c^T x + d$, the Hessian is constant, $H$, and we have the relationship $y_k = H s_k$. The update formulas aim to satisfy the secant equation, $B_{k+1}s_k = y_k$.\n\nThe SR1 update is given by:\n$$B_{k+1}^{\\text{SR1}} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^T}{(y_k - B_k s_k)^T s_k}$$\nIts primary deficiency is the denominator, $\\Delta_k = (y_k - B_k s_k)^T s_k$. If this term is zero or very close to zero, the update becomes undefined or numerically unstable, leading to a potentially large and meaningless change in $B_k$. The condition $\\Delta_k = 0$ implies that the vector $y_k - B_k s_k$, which represents the discrepancy between the true gradient change and that predicted by the current model $B_k$, is orthogonal to the step direction $s_k$.\n\nThe BFGS update is given by:\n$$B_{k+1}^{\\text{BFGS}} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{s_k^T y_k}$$\nA crucial property of the BFGS update is that if $B_k$ is positive definite and the curvature condition $s_k^T y_k > 0$ is satisfied, then $B_{k+1}^{\\text{BFGS}}$ is also positive definite. The curvature condition is typically ensured by line search algorithms conforming to the Wolfe conditions. In this problem, since $y_k = H s_k$ and $H$ is specified as positive definite, $s_k^T y_k = s_k^T H s_k > 0$ for any non-zero $s_k$. Thus, the BFGS update is well-defined and expected to maintain positive definiteness.\n\nWe now analyze the three specified test cases. The parameters are $r = 10^{-8}$ for the SR1 near-zero test and $\\epsilon_{\\text{pd}} = 10^{-12}$ for the positive-definiteness check.\n\n**Test Case 1: Well-conditioned case**\nGiven:\n- $H^{(1)} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $B_k^{(1)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $s_k^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nFirst, we compute the necessary vectors:\n- $y_k^{(1)} = H^{(1)} s_k^{(1)} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}$.\n- $B_k^{(1)} s_k^{(1)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- The discrepancy vector is $v_k^{(1)} = y_k^{(1)} - B_k^{(1)} s_k^{(1)} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$.\n\n1.  **SR1 denominator check**:\n    - $\\Delta_k^{(1)} = (s_k^{(1)})^T v_k^{(1)} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = 5$.\n    - Norms: $\\|s_k^{(1)}\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$, $\\|v_k^{(1)}\\|_2 = \\sqrt{3^2+2^2} = \\sqrt{13}$.\n    - Test: Is $|\\Delta_k^{(1)}| \\le r \\|s_k^{(1)}\\|_2 \\|v_k^{(1)}\\|_2$?\n      $5 \\le 10^{-8} \\sqrt{2} \\sqrt{13} \\approx 5.099 \\times 10^{-8}$. This is false.\n    - Thus, $\\text{SR1\\_near\\_zero}^{(1)} = \\text{False}$.\n\n2.  **BFGS update and positive definiteness**:\n    - Denominators: $(s_k^{(1)})^T B_k^{(1)} s_k^{(1)} = 2$, $(s_k^{(1)})^T y_k^{(1)} = 7$.\n    - $B_{k+1}^{\\text{BFGS}, (1)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\begin{bmatrix} 1 & 1 \\end{bmatrix} + \\frac{1}{7}\\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}\\begin{bmatrix} 4 & 3 \\end{bmatrix} = \\begin{bmatrix} 19.5/7 & 8.5/7 \\\\ 8.5/7 & 12.5/7 \\end{bmatrix} \\approx \\begin{bmatrix} 2.7857 & 1.2143 \\\\ 1.2143 & 1.7857 \\end{bmatrix}$.\n    - Eigenvalues are $\\lambda = \\frac{16}{7} \\pm \\frac{\\sqrt{338}}{14}$. The minimum eigenvalue is $\\lambda_{\\min} = \\frac{32 - 13\\sqrt{2}}{14} \\approx 0.972$.\n    - Since $0.972 > 10^{-12}$, the matrix is positive definite.\n    - Thus, $\\text{BFGS\\_PD}^{(1)} = \\text{True}$.\n\n3.  **Frobenius-norm error**:\n    - $B_{k+1}^{\\text{BFGS}, (1)} - H^{(1)} = \\begin{bmatrix} 19.5/7 - 3 & 8.5/7 - 1 \\\\ 8.5/7 - 1 & 12.5/7 - 2 \\end{bmatrix} = \\begin{bmatrix} -1.5/7 & 1.5/7 \\\\ 1.5/7 & -1.5/7 \\end{bmatrix}$.\n    - $E_{\\text{F}}^{(1)} = \\left\\|B_{k+1}^{\\text{BFGS}, (1)} - H^{(1)}\\right\\|_F = \\sqrt{4 \\times (1.5/7)^2} = \\frac{3}{7} \\approx 0.42857143$.\n\nResult for Case 1: $[\\text{False}, \\text{True}, 0.42857143]$.\n\n---\n\n**Test Case 2: Near-zero SR1 denominator**\nGiven:\n- $\\varepsilon = 10^{-12}$.\n- $H^{(2)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}$, $B_k^{(2)} = \\begin{bmatrix} 2 - \\varepsilon & 0 \\\\ 0 & 1 \\end{bmatrix}$, $s_k^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nComputed vectors:\n- $y_k^{(2)} = H^{(2)} s_k^{(2)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n- $B_k^{(2)} s_k^{(2)} = \\begin{bmatrix} 2 - \\varepsilon & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 - \\varepsilon \\\\ 0 \\end{bmatrix}$.\n- Discrepancy vector: $v_k^{(2)} = y_k^{(2)} - B_k^{(2)} s_k^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 2 - \\varepsilon \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\varepsilon \\\\ 1 \\end{bmatrix}$.\n\n1.  **SR1 denominator check**:\n    - $\\Delta_k^{(2)} = (s_k^{(2)})^T v_k^{(2)} = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} \\varepsilon \\\\ 1 \\end{bmatrix} = \\varepsilon = 10^{-12}$.\n    - Norms: $\\|s_k^{(2)}\\|_2 = 1$, $\\|v_k^{(2)}\\|_2 = \\sqrt{\\varepsilon^2+1^2} = \\sqrt{10^{-24}+1} \\approx 1$.\n    - Test: Is $|\\Delta_k^{(2)}| \\le r \\|s_k^{(2)}\\|_2 \\|v_k^{(2)}\\|_2$?\n      $10^{-12} \\le 10^{-8} \\times 1 \\times \\sqrt{1+10^{-24}}$. This is true.\n    - Thus, $\\text{SR1\\_near\\_zero}^{(2)} = \\text{True}$.\n\n2.  **BFGS update and positive definiteness**:\n    - Denominators: $(s_k^{(2)})^T B_k^{(2)} s_k^{(2)} = 2 - \\varepsilon$, $(s_k^{(2)})^T y_k^{(2)} = 2$.\n    - $B_{k+1}^{\\text{BFGS}, (2)} = \\begin{bmatrix} 2-\\varepsilon & 0 \\\\ 0 & 1 \\end{bmatrix} - \\frac{1}{2-\\varepsilon}\\begin{bmatrix} 2-\\varepsilon \\\\ 0 \\end{bmatrix}\\begin{bmatrix} 2-\\varepsilon & 0 \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\begin{bmatrix} 2 & 1 \\end{bmatrix}$.\n    - $B_{k+1}^{\\text{BFGS}, (2)} = \\begin{bmatrix} 2-\\varepsilon & 0 \\\\ 0 & 1 \\end{bmatrix} - \\begin{bmatrix} 2-\\varepsilon & 0 \\\\ 0 & 0 \\end{bmatrix} + \\begin{bmatrix} 2 & 1 \\\\ 1 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1.5 \\end{bmatrix}$.\n    - Eigenvalues of this matrix are $\\lambda = \\frac{3.5 \\pm \\sqrt{4.25}}{2}$. The minimum eigenvalue is $\\lambda_{\\min} = \\frac{3.5 - \\sqrt{4.25}}{2} \\approx 0.719$.\n    - Since $0.719 > 10^{-12}$, the matrix is positive definite.\n    - Thus, $\\text{BFGS\\_PD}^{(2)} = \\text{True}$.\n\n3.  **Frobenius-norm error**:\n    - $B_{k+1}^{\\text{BFGS}, (2)} - H^{(2)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1.5 \\end{bmatrix} - \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & -1.5 \\end{bmatrix}$.\n    - $E_{\\text{F}}^{(2)} = \\left\\|B_{k+1}^{\\text{BFGS}, (2)} - H^{(2)}\\right\\|_F = \\sqrt{0^2+0^2+0^2+(-1.5)^2} = 1.5$.\n\nResult for Case 2: $[\\text{True}, \\text{True}, 1.50000000]$.\n\n---\n\n**Test Case 3: Exactly zero SR1 denominator**\nThis case is identical to case 2, setting $\\varepsilon=0$.\nGiven:\n- $H^{(3)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}$, $B_k^{(3)} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $s_k^{(3)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nComputed vectors:\n- $y_k^{(3)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n- $B_k^{(3)} s_k^{(3)} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$.\n- Discrepancy vector: $v_k^{(3)} = y_k^{(3)} - B_k^{(3)} s_k^{(3)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n\n1.  **SR1 denominator check**:\n    - $\\Delta_k^{(3)} = (s_k^{(3)})^T v_k^{(3)} = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0$.\n    - The condition $|\\Delta_k^{(3)}| \\le ...$ is trivially satisfied as $0$ is less than or equal to any non-negative number.\n    - Thus, $\\text{SR1\\_near\\_zero}^{(3)} = \\text{True}$.\n\n2.  **BFGS update and positive definiteness**:\n    - The computation is identical to case 2 with $\\varepsilon=0$.\n    - $B_{k+1}^{\\text{BFGS}, (3)} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1.5 \\end{bmatrix}$.\n    - The matrix and its eigenvalues are identical to case 2. The minimum eigenvalue is $\\approx 0.719 > 10^{-12}$.\n    - Thus, $\\text{BFGS\\_PD}^{(3)} = \\text{True}$.\n\n3.  **Frobenius-norm error**:\n    - The resulting matrix $B_{k+1}^{\\text{BFGS}, (3)}$ and the true Hessian $H^{(3)}$ are identical to case 2.\n    - $E_{\\text{F}}^{(3)} = 1.5$.\n\nResult for Case 3: $[\\text{True}, \\text{True}, 1.50000000]$.\n\nThe results clearly demonstrate the robustness of the BFGS update in situations where the SR1 update fails. In cases 2 and 3, the SR1 denominator becomes numerically small and then exactly zero, rendering the update unsafe or undefined. In contrast, the BFGS update proceeds without issue, producing a positive definite matrix in all three scenarios, thereby justifying its widespread use in practical optimization software.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quasi-Newton update comparison problem for three test cases.\n    \"\"\"\n\n    # Define constants from the problem statement.\n    r = 1e-8\n    eps_pd = 1e-12\n\n    # Define the test cases. Each case is a dictionary for clarity.\n    test_cases = [\n        {\n            \"H\": np.array([[3.0, 1.0], [1.0, 2.0]]),\n            \"Bk\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"sk\": np.array([1.0, 1.0]),\n        },\n        {\n            \"H\": np.array([[2.0, 1.0], [1.0, 3.0]]),\n            \"Bk\": np.array([[2.0 - 1e-12, 0.0], [0.0, 1.0]]),\n            \"sk\": np.array([1.0, 0.0]),\n        },\n        {\n            \"H\": np.array([[2.0, 1.0], [1.0, 3.0]]),\n            \"Bk\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"sk\": np.array([1.0, 0.0]),\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        H, Bk, sk = case[\"H\"], case[\"Bk\"], case[\"sk\"]\n\n        # Define yk = H * sk\n        yk = H @ sk\n\n        # --- Part 1: SR1 Denominator Check ---\n        # vector vk = yk - Bk * sk\n        vk = yk - Bk @ sk\n        # SR1 denominator delta_k = sk.T * (yk - Bk * sk)\n        delta_k = sk.T @ vk\n        \n        # Norms for the near-zero test\n        norm_sk = np.linalg.norm(sk, 2)\n        norm_vk = np.linalg.norm(vk, 2)\n        \n        # The inequality handles the case where delta_k is exactly zero.\n        sr1_near_zero = abs(delta_k) <= r * norm_sk * norm_vk\n\n        # --- Part 2: BFGS Update and Positive-Definiteness Test ---\n        # Compute denominators for BFGS update\n        sk_T_Bk_sk = sk.T @ Bk @ sk\n        sk_T_yk = sk.T @ yk\n\n        # BFGS update formula\n        term1 = Bk\n        term2 = np.outer(Bk @ sk, Bk @ sk) / sk_T_Bk_sk\n        term3 = np.outer(yk, yk) / sk_T_yk\n        \n        B_k_plus_1_bfgs = term1 - term2 + term3\n\n        # Check for positive definiteness. Using eigvalsh as matrix is symmetric.\n        eigenvalues = np.linalg.eigvalsh(B_k_plus_1_bfgs)\n        min_eigenvalue = np.min(eigenvalues)\n        bfgs_pd = min_eigenvalue > eps_pd\n\n        # --- Part 3: Frobenius-norm Error ---\n        E_f = np.linalg.norm(B_k_plus_1_bfgs - H, 'fro')\n\n        # Collect results for the current case\n        # The float must be rounded to exactly 8 decimal places for the output format.\n        current_result = [sr1_near_zero, bfgs_pd, round(E_f, 8)]\n        all_results.append(current_result)\n\n    # --- Final Output Formatting ---\n    # Construct the output string manually to ensure no spaces and correct format.\n    # e.g., \"[[False,True,0.42857143],[True,True,1.50000000],[True,True,1.50000000]]\"\n    result_strings = []\n    for res in all_results:\n        # Format the boolean and float components as strings\n        b1 = str(res[0])\n        b2 = str(res[1])\n        f_val = f\"{res[2]:.8f}\"\n        # Create the inner list string \"[b1,b2,f_val]\"\n        result_strings.append(f\"[{b1},{b2},{f_val}]\")\n    \n    # Join the inner list strings with a comma and wrap in brackets\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2431086"}, {"introduction": "The performance of a quasi-Newton method is often sensitive to the initial approximation of the *inverse* Hessian matrix. While starting with the identity matrix, $H_0 = I$, is a simple and robust choice, it may not reflect the scale of the problem's curvature, potentially leading to slow initial progress on ill-conditioned functions. This practice [@problem_id:2431054] challenges you to implement a popular heuristic for scaling the initial matrix, which uses information from a preliminary probing step to better align with the local geometry. By comparing the progress of the scaled and unscaled methods, you will see firsthand how a more informed initialization can dramatically accelerate convergence.", "problem": "Consider unconstrained minimization of a smooth function $f:\\mathbb{R}^n \\to \\mathbb{R}$ using a Quasi-Newton method with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update. The method maintains an approximation $H_k$ to the inverse Hessian matrix (the inverse of the matrix of second derivatives), and computes a descent direction $p_k$ at iteration $k$ via $p_k = - H_k \\nabla f(x_k)$. The line search then chooses a step length $\\alpha_k$ along $p_k$ to satisfy the strong Wolfe conditions with constants $c_1 = 10^{-4}$ and $c_2 = 0.9$. The initial inverse Hessian approximation $H_0$ strongly influences the first step $p_0$ and hence the initial progress.\n\nYour task is to construct and evaluate a reproducible scenario where the choice $H_0 = I$ (the identity matrix) leads to very slow initial progress compared to a scaled identity $H_0 = \\gamma I$ with\n$$\n\\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0}, \\quad s_0 = -\\eta \\, \\nabla f(x_0), \\quad y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0),\n$$\nwhere $\\eta > 0$ is a small probing step. This scaled choice attempts to tune the initial inverse Hessian to the local curvature measured along the gradient direction.\n\nUse as objective the generalized two-parameter Rosenbrock function in even dimension $n$:\n$$\nf(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right],\n$$\nwith gradient components\n$$\n\\frac{\\partial f}{\\partial x_{2i-1}} = -4\\beta\\,x_{2i-1}\\left(x_{2i} - x_{2i-1}^2\\right) + 2\\left(x_{2i-1} - 1\\right), \\quad\n\\frac{\\partial f}{\\partial x_{2i}} = 2\\beta\\left(x_{2i} - x_{2i-1}^2\\right),\n$$\nfor $i=1,\\dots,n/2$.\n\nImplement a single BFGS iteration with two initializations:\n- Case A (unscaled): $H_0 = I$.\n- Case B (scaled): $H_0 = \\gamma I$ with $\\gamma$ given above, using the probing step $s_0 = -\\eta \\nabla f(x_0)$.\n\nFor each case, compute $p_0 = -H_0 \\nabla f(x_0)$ and select $\\alpha_0$ via a strong Wolfe line search with $c_1 = 10^{-4}$ and $c_2 = 0.9$ starting from $\\alpha = 1$. If the strong Wolfe line search fails to return a step length, fall back to Armijo backtracking (sufficient decrease only) with reduction factor $0.5$, the same $c_1$, and the same starting $\\alpha = 1$. Then form $x_1 = x_0 + \\alpha_0 p_0$ for each case. Define the initial progress as the objective reduction $f(x_0) - f(x_1)$.\n\nYour program must compute, for each test case, the ratio\n$$\nR = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)},\n$$\nwhere $x_1^{\\text{scaled}}$ is obtained with the scaled $H_0 = \\gamma I$ and $x_1^{\\text{identity}}$ with $H_0 = I$. A value $R > 1$ indicates that the scaled initialization made more progress than the unscaled one on the first iteration. Report $R$ rounded to six decimal places.\n\nTest suite:\n- Test 1 (high curvature, two dimensions): $n=2$, $\\beta = 10^4$, $x_0 = (-1.2,\\,1.0)$, $\\eta = 10^{-3}$.\n- Test 2 (moderate curvature, four dimensions): $n=4$, $\\beta = 100$, $x_0 = (-1.2,\\,1.0,\\,-1.2,\\,1.0)$, $\\eta = 10^{-3}$.\n- Test 3 (edge case near minimizer): $n=2$, $\\beta = 100$, $x_0 = (1.0,\\,1.0001)$, $\\eta = 10^{-3}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3]\". Each $r_i$ must be the floating-point value of $R$ for the corresponding test, rounded to six decimal places. No other output is permitted. There are no physical units involved and no angles; all quantities are dimensionless real numbers.", "solution": "The posed problem is subjected to rigorous validation. All givens, including the objective function, its gradient, algorithmic parameters, and test cases, have been extracted. The problem is found to be scientifically grounded in the established field of numerical optimization. It is well-posed, with all necessary information provided for a unique, deterministic solution. The language is objective and precise. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe problem requires an analysis of the first iteration of a Quasi-Newton optimization method, specifically the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. We are to compare the initial progress for two distinct choices of the initial inverse Hessian approximation, $H_0$.\n\nThe general iterative scheme for a Quasi-Newton method is given by:\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\nwhere $p_k$ is the search direction and $\\alpha_k$ is the step length. The search direction is computed using the current approximation of the inverse Hessian matrix, $H_k$, and the gradient of the objective function, $\\nabla f(x_k)$:\n$$ p_k = -H_k \\nabla f(x_k) $$\nThe step length $\\alpha_k$ is determined by a line search procedure to ensure sufficient decrease in the function value and satisfaction of curvature conditions. The problem specifies the strong Wolfe conditions:\n1. Armijo (sufficient decrease) condition: $f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$\n2. Curvature condition: $|\\nabla f(x_k + \\alpha_k p_k)^\\top p_k| \\le c_2 |\\nabla f(x_k)^\\top p_k|$\nwith specified constants $c_1 = 10^{-4}$ and $c_2 = 0.9$.\n\nThe core of this problem lies in the choice of the initial approximation, $H_0$, which dictates the very first step of the optimization process, $p_0 = -H_0 \\nabla f(x_0)$. We examine two cases.\n\nCase A: The Unscaled Identity Initialization\nThe choice $H_0 = I$, where $I$ is the identity matrix, is the simplest possible. This results in an initial search direction $p_0 = -\\nabla f(x_0)$, which is the direction of steepest descent. While intuitive, this direction can be very inefficient for ill-conditioned problems where the level sets of the function are highly eccentric, leading to slow, zigzagging convergence.\n\nCase B: The Scaled Identity Initialization\nA more sophisticated approach is to scale the initial identity matrix, $H_0 = \\gamma I$. The scaling factor $\\gamma$ is chosen to approximate the curvature of the function. The problem prescribes a specific method to find $\\gamma$ based on a probing step. We first compute a trial step $s_0 = -\\eta \\nabla f(x_0)$ for a small $\\eta > 0$. We then measure the change in the gradient, $y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0)$. The scaling factor is then given by:\n$$ \\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0} $$\nThis formula for $\\gamma$ is derived from finding a scalar that best satisfies the secant equation $s_0 = H_0 y_0$ in a least-squares sense, i.e., by minimizing $\\|\\ s_0 - \\gamma y_0 \\|_2^2$ with respect to $\\gamma$. This seeks to imbue $H_0$ with some information about the function's curvature, potentially leading to a much better-scaled initial step $p_0 = -\\gamma \\nabla f(x_0)$. For this to be a descent direction, we require $\\gamma > 0$, which is true if the function is convex enough along the probing direction $s_0$ such that $y_0^\\top s_0 > 0$.\n\nThe test function is the generalized Rosenbrock function for even dimension $n$:\n$$ f(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right] $$\nThis function is a classic benchmark for optimization algorithms due to its non-convexity and the presence of a narrow, parabolic valley. For large $\\beta$, the problem becomes very ill-conditioned, making it an excellent candidate to demonstrate the superiority of a well-scaled initial step over the naive steepest descent direction.\n\nThe computational procedure is as follows:\n1.  For each test case ($n, \\beta, x_0, \\eta$), we will implement the Rosenbrock function and its gradient.\n2.  We will execute one iteration for Case A ($H_0 = I$) and Case B ($H_0 = \\gamma I$).\n3.  For Case B, the factor $\\gamma$ is computed first, requiring an evaluation of the gradient at $x_0$ and at a probed point $x_0 + s_0$. A safeguard is necessary for the case where the denominator $y_0^\\top y_0$ is close to zero.\n4.  For both cases, the search direction $p_0$ is computed. We must verify that it is a descent direction (i.e., $\\nabla f(x_0)^\\top p_0 < 0$).\n5.  A line search is performed along $p_0$ starting from $\\alpha = 1$ to find a step length $\\alpha_0$ satisfying the strong Wolfe conditions. We will use the `line_search` function from the `scipy.optimize` library.\n6.  As per the problem, if the strong Wolfe line search fails, we must fall back to a manual implementation of Armijo backtracking, where we iteratively reduce $\\alpha$ by a factor of $0.5$ until the sufficient decrease condition is met.\n7.  With $\\alpha_0$ determined, we compute the next iterate $x_1 = x_0 + \\alpha_0 p_0$ and the corresponding function value $f(x_1)$.\n8.  The progress for each case is defined as the reduction in the objective function, $f(x_0) - f(x_1)$.\n9.  Finally, we compute the ratio $R$ of the progress made by the scaled method to that of the unscaled method:\n$$ R = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)} $$\nThe denominator is regularized to prevent division by zero or numerical instability. The resulting value of $R$ for each test case will be rounded to six decimal places.\n\nThis procedure will be encapsulated in a Python program, which constitutes the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef rosenbrock(x, beta):\n    \"\"\"\n    Computes the value of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n    \n    val = 0.0\n    for i in range(n // 2):\n        # x-indices are 2*i and 2*i+1, corresponding to problem's x_2i-1 and x_2i for i=1..n/2\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        term1 = beta * (x[idx2] - x[idx1]**2)**2\n        term2 = (1 - x[idx1])**2\n        val += term1 + term2\n    return val\n\ndef rosenbrock_grad(x, beta):\n    \"\"\"\n    Computes the gradient of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n        \n    grad = np.zeros(n)\n    for i in range(n // 2):\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        common_term = 2 * beta * (x[idx2] - x[idx1]**2)\n        grad[idx1] = -2 * x[idx1] * common_term + 2 * (x[idx1] - 1)\n        grad[idx2] = common_term\n    return grad\n\ndef compute_progress(x0, beta, eta, initialization, c1, c2):\n    \"\"\"\n    Computes the progress f(x0) - f(x1) for a single Quasi-Newton iteration.\n    Handles both identity and scaled initializations, and includes line search logic.\n    \"\"\"\n    # Create lambda functions to pass beta parameter\n    f = lambda x: rosenbrock(x, beta)\n    grad = lambda x: rosenbrock_grad(x, beta)\n\n    f0 = f(x0)\n    g0 = grad(x0)\n\n    # If gradient is virtually zero, no progress can be made.\n    if np.linalg.norm(g0) < 1e-12:\n        return 0.0\n\n    if initialization == 'identity':\n        p0 = -g0\n    elif initialization == 'scaled':\n        s0 = -eta * g0\n        \n        # Probing step to compute the scaling factor gamma\n        x_probe = x0 + s0\n        g_probe = grad(x_probe)\n        y0 = g_probe - g0\n\n        y0_dot_y0 = np.dot(y0, y0)\n        \n        # Guard against division by zero for ill-defined gamma\n        if y0_dot_y0 < 1e-20:\n            gamma = 1.0 # Fallback to identity scaling\n        else:\n            y0_dot_s0 = np.dot(y0, s0)\n            gamma = y0_dot_s0 / y0_dot_y0\n        \n        p0 = -gamma * g0\n    else:\n        raise ValueError(f\"Unknown initialization type: {initialization}\")\n\n    # The search direction must be a descent direction.\n    # If gamma <= 0, this will not hold, and progress should be zero.\n    pk_dot_g0 = np.dot(g0, p0)\n    if pk_dot_g0 >= 0:\n        return 0.0\n\n    # Perform strong Wolfe line search using SciPy\n    alpha, _, _, f_new, _, _ = line_search(\n        f=f,\n        myfprime=grad,\n        xk=x0,\n        pk=p0,\n        gfk=g0,\n        old_fval=f0,\n        c1=c1,\n        c2=c2\n    )\n\n    # Fallback to Armijo backtracking if strong Wolfe search fails\n    if alpha is None:\n        alpha = 1.0\n        rho = 0.5\n        \n        # Limit backtracking steps to prevent infinite loops\n        for _ in range(100):\n            x_new_check = x0 + alpha * p0\n            f_new_check = f(x_new_check)\n            if f_new_check <= f0 + c1 * alpha * pk_dot_g0:\n                f_new = f_new_check\n                break\n            alpha *= rho\n        else:\n            # If Armijo loop completes without break, step is negligible.\n            return 0.0\n\n    return f0 - f_new\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, beta, x0, eta)\n        (2, 1e4, np.array([-1.2, 1.0]), 1e-3),\n        (4, 100.0, np.array([-1.2, 1.0, -1.2, 1.0]), 1e-3),\n        (2, 100.0, np.array([1.0, 1.0001]), 1e-3)\n    ]\n    \n    # Line search parameters\n    c1 = 1e-4\n    c2 = 0.9\n\n    results = []\n    for n, beta, x0, eta in test_cases:\n        # Calculate progress for the unscaled (identity) case\n        progress_identity = compute_progress(x0, beta, eta, 'identity', c1, c2)\n        \n        # Calculate progress for the scaled case\n        progress_scaled = compute_progress(x0, beta, eta, 'scaled', c1, c2)\n\n        # Compute the ratio R, with a safeguard for the denominator\n        denominator = max(progress_identity, 1e-30)\n        R = progress_scaled / denominator\n        \n        results.append(round(R, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431054"}, {"introduction": "The standard BFGS algorithm's requirement to store and update a dense $n \\times n$ matrix approximation is prohibitive for large-scale optimization, where the number of variables $n$ can be in the thousands or millions. The Limited-Memory BFGS (L-BFGS) method elegantly overcomes this by implicitly representing the matrix using only a small, fixed number of recent curvature pairs. This exercise [@problem_id:2431082] focuses on implementing the famous \"two-loop recursion,\" the computational engine of L-BFGS that computes the search direction without ever forming the dense matrix. Mastering this technique is fundamental to understanding how quasi-Newton methods are scaled to solve high-dimensional problems in fields like machine learning and data science.", "problem": "You are given finite sequences of curvature pairs $\\{(s_i,y_i)\\}$, an initial scalar $\\gamma$ defining $H_0=\\gamma I$, and a current gradient $g$ of a smooth objective function. Consider the symmetric positive definite linear operator $H$ defined as follows: among all symmetric positive definite matrices that satisfy the secant conditions $H y_i = s_i$ for the most recent $m$ pairs $\\{(s_i,y_i)\\}_{i=k-m+1}^k$ with $s_i^\\top y_i &gt; 0$, the operator $H$ is the unique one obtained by starting from $H_0=\\gamma I$ and enforcing these secant conditions in increasing index order in the sense of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update. For each test case below, compute the search direction $p=-H g$.\n\nAll vectors and matrices are over the real numbers. All components are to be treated as dimensionally consistent pure numbers (no physical units). Use the following test suite; in each case, only the last $m$ pairs are to be enforced.\n\nTest case $1$ (boundary, no curvature pairs):\n- Dimension $n=3$.\n- Memory size $m=0$ (no pairs).\n- Initial scaling $\\gamma = 0.5$.\n- Gradient $g = \\left[1,-2,3\\right]$.\n\nTest case $2$ (single curvature pair):\n- Dimension $n=2$.\n- Memory size $m=1$.\n- Initial scaling $\\gamma = 1$.\n- Curvature pair $\\left(s_1,y_1\\right) = \\left(\\left[1,2\\right],\\left[3,1\\right]\\right)$ with $s_1^\\top y_1 = 5$.\n- Gradient $g = \\left[4,-1\\right]$.\n\nTest case $3$ (multiple pairs spanning the space, exact recovery for a quadratic):\n- Dimension $n=3$.\n- Memory size $m=3$.\n- Initial scaling $\\gamma = 1$.\n- Define $A=\\mathrm{diag}\\!\\left(2,3,4\\right)$. Let $s_1=\\left[1,0,0\\right]$, $s_2=\\left[0,1,0\\right]$, $s_3=\\left[0,0,1\\right]$, and $y_i = A s_i$, that is $y_1=\\left[2,0,0\\right]$, $y_2=\\left[0,3,0\\right]$, $y_3=\\left[0,0,4\\right]$.\n- Gradient $g = \\left[5,-6,7\\right]$.\n\nTest case $4$ (limited memory using only the most recent pairs):\n- Dimension $n=4$.\n- Memory size $m=2$ (use only the last two pairs listed below).\n- Initial scaling $\\gamma = 1$.\n- Define the symmetric positive definite matrix\n$$\nA=\\begin{bmatrix}\n4 & 1 & 0 & 0\\\\\n1 & 3 & 0 & 0\\\\\n0 & 0 & 2 & 0\\\\\n0 & 0 & 0 & 1.5\n\\end{bmatrix}.\n$$\n- Define pairs from $y_i = A s_i$ for\n$s_1=\\left[1,0,0,0\\right]$ with $y_1=\\left[4,1,0,0\\right]$,\n$s_2=\\left[0,1,0,0\\right]$ with $y_2=\\left[1,3,0,0\\right]$,\n$s_3=\\left[0,0,1,0\\right]$ with $y_3=\\left[0,0,2,0\\right]$,\n$s_4=\\left[0,0,0,1\\right]$ with $y_4=\\left[0,0,0,1.5\\right]$.\n- Gradient $g = \\left[1,2,3,4\\right]$.\n- Only the last $m=2$ pairs, namely $\\left(s_3,y_3\\right)$ and $\\left(s_4,y_4\\right)$, are to be enforced.\n\nYour program must compute the corresponding search direction $p=-H g$ for each test case. Output formatting requirement: your program should produce a single line of output containing a list of the four search direction vectors in order, with no whitespace, using decimal notation. Specifically, the output must have the form\n$[[p^{(1)}_1,\\dots,p^{(1)}_{n_1}],[p^{(2)}_1,\\dots,p^{(2)}_{n_2}],[p^{(3)}_1,\\dots,p^{(3)}_{n_3}],[p^{(4)}_1,\\dots,p^{(4)}_{n_4}]]$,\nwhere $p^{(j)}$ is the vector for test case $j$. Each number must be printed as a decimal (for example, $-2$, $-1.5$, or $-2.6666666667$ are acceptable). The final output must be a single line exactly in this bracketed, comma-separated format.", "solution": "The problem statement is critically examined and found to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard computational task from numerical optimization: the calculation of a search direction using the Limited-Memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm.\n\nThe problem defines a symmetric positive definite matrix $H$ through a constructive procedure. Starting with an initial matrix $H_0 = \\gamma I$, where $\\gamma$ is a positive scalar and $I$ is the identity matrix, a sequence of $m$ BFGS updates is applied using the provided curvature pairs $\\{(s_i, y_i)\\}$. The problem specifies that the updates are applied in increasing order of their indices. The BFGS update formula for the inverse Hessian approximation $H_k$ to obtain $H_{k+1}$ is:\n$$ H_{k+1} = (I - \\rho_k s_k y_k^\\top) H_k (I - \\rho_k y_k s_k^\\top) + \\rho_k s_k s_k^\\top $$\nwhere $\\rho_k = (y_k^\\top s_k)^{-1}$. The condition $s_k^\\top y_k > 0$ ensures that $\\rho_k$ is positive and that positive definiteness is maintained throughout the updates.\n\nThe task is to compute the search direction $p = -Hg$, where $g$ is a given gradient vector. The direct computation by first forming the $n \\times n$ matrix $H$ and then performing the matrix-vector multiplication is computationally prohibitive for large-scale problems, for which L-BFGS is designed. The standard and efficient method is the L-BFGS two-loop recursion, which computes the product $Hg$ without explicitly forming $H$. This procedure utilizes only the $m$ stored curvature pairs and the initial scaling factor $\\gamma$. This is the principle guiding the solution design.\n\nThe algorithm to compute $r=Hg$ is as follows. Let the $m$ most recent curvature pairs be denoted $\\{(s_j, y_j)\\}_{j=1}^m$, ordered from the oldest to the newest in the memory window. Let $\\rho_j = (y_j^\\top s_j)^{-1}$ for each pair.\n\n1.  Initialize a vector $q \\leftarrow g$.\n2.  Perform a backward pass (first loop) from the newest pair ($j=m$) to the oldest ($j=1$):\n    For $j = m, m-1, \\dots, 1$:\n    -   Compute and store $\\alpha_j \\leftarrow \\rho_j s_j^\\top q$.\n    -   Update $q \\leftarrow q - \\alpha_j y_j$.\n3.  Scale the intermediate vector by the initial inverse Hessian approximation: $r \\leftarrow H_0 q = \\gamma q$.\n4.  Perform a forward pass (second loop) from the oldest pair ($j=1$) to the newest ($j=m$):\n    For $j = 1, 2, \\dots, m$:\n    -   Compute $\\beta_j \\leftarrow \\rho_j y_j^\\top r$.\n    -   Update $r \\leftarrow r + s_j(\\alpha_j - \\beta_j)$.\n\nThe resulting vector $r$ is the product $Hg$. The final search direction is $p = -r$. This procedure is implemented to solve each of the specified test cases.\n\n-   **Test Case 1**: With memory $m=0$, no curvature pairs are used. The loops are not executed. The calculation simplifies to $p = -H_0 g = -\\gamma g$. For $\\gamma = 0.5$ and $g = [1, -2, 3]^\\top$, we have $p = -0.5 \\times [1, -2, 3]^\\top = [-0.5, 1, -1.5]^\\top$.\n\n-   **Test Case 2**: With $m=1$ pair $(s_1, y_1)$, the two-loop recursion is applied. We calculate $\\rho_1 = (y_1^\\top s_1)^{-1} = 1/5 = 0.2$. The algorithm yields $p = [-1.8, 3.4]^\\top$.\n\n-   **Test Case 3**: With $m=n=3$ and the pairs $(s_i, y_i)$ where $s_i$ are standard basis vectors and $y_i=As_i$ for a diagonal matrix $A$, the L-BFGS procedure is known to recover the exact inverse Hessian, $H=A^{-1}$, after $n$ updates. Thus, the search direction is $p = -A^{-1}g$. For $A=\\mathrm{diag}(2,3,4)$ and $g = [5, -6, 7]^\\top$, this results in $p = -[\\mathrm{diag}(0.5, 1/3, 0.25)] [5, -6, 7]^\\top = [-2.5, 2, -1.75]^\\top$. The two-loop recursion confirms this result.\n\n-   **Test Case 4**: With memory $m=2$, only the last two pairs, $(s_3, y_3)$ and $(s_4, y_4)$, are used. These pairs are orthogonal. The initial matrix is $H_0 = I$ ($\\gamma=1$). The L-BFGS updates effectively modify only the components of the inverse Hessian approximation corresponding to the subspace spanned by $\\{s_3, s_4\\}$. The resulting search direction is calculated as $p = [-1, -2, -1.5, -8/3]^\\top$.", "answer": "```python\nimport numpy as np\n\ndef compute_lbfgs_direction(m, gamma, s_pairs, y_pairs, g):\n    \"\"\"\n    Computes the L-BFGS search direction p = -Hg using the two-loop recursion.\n\n    Args:\n        m (int): The memory size.\n        gamma (float): The initial scaling factor for H_0.\n        s_pairs (list of np.ndarray): List of 's' vectors {s_k}.\n        y_pairs (list of np.ndarray): List of 'y' vectors {y_k}.\n        g (np.ndarray): The current gradient vector.\n\n    Returns:\n        np.ndarray: The search direction vector p.\n    \"\"\"\n    if m == 0:\n        return -gamma * g\n\n    rhos = [1.0 / (y.T @ s) for s, y in zip(s_pairs, y_pairs)]\n    alphas = np.zeros(m)\n    \n    q = g.copy()\n\n    # First loop: from newest to oldest pair\n    for i in range(m - 1, -1, -1):\n        alphas[i] = rhos[i] * (s_pairs[i].T @ q)\n        q = q - alphas[i] * y_pairs[i]\n\n    r = gamma * q\n\n    # Second loop: from oldest to newest pair\n    for i in range(m):\n        beta = rhos[i] * (y_pairs[i].T @ r)\n        r = r + s_pairs[i] * (alphas[i] - beta)\n        \n    p = -r\n    return p\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement and computes the results.\n    \"\"\"\n    # Test case 1\n    case1 = {\n        \"m\": 0, \"gamma\": 0.5, \"s_pairs\": [], \"y_pairs\": [],\n        \"g\": np.array([1., -2., 3.])\n    }\n\n    # Test case 2\n    case2 = {\n        \"m\": 1, \"gamma\": 1.0,\n        \"s_pairs\": [np.array([1., 2.])],\n        \"y_pairs\": [np.array([3., 1.])],\n        \"g\": np.array([4., -1.])\n    }\n\n    # Test case 3\n    s1_c3 = np.array([1., 0., 0.])\n    s2_c3 = np.array([0., 1., 0.])\n    s3_c3 = np.array([0., 0., 1.])\n    A_c3 = np.diag([2., 3., 4.])\n    y1_c3 = A_c3 @ s1_c3\n    y2_c3 = A_c3 @ s2_c3\n    y3_c3 = A_c3 @ s3_c3\n    case3 = {\n        \"m\": 3, \"gamma\": 1.0,\n        \"s_pairs\": [s1_c3, s2_c3, s3_c3],\n        \"y_pairs\": [y1_c3, y2_c3, y3_c3],\n        \"g\": np.array([5., -6., 7.])\n    }\n\n    # Test case 4\n    A_c4 = np.array([[4., 1., 0., 0.],\n                     [1., 3., 0., 0.],\n                     [0., 0., 2., 0.],\n                     [0., 0., 0., 1.5]])\n    s1_c4 = np.array([1., 0., 0., 0.])\n    s2_c4 = np.array([0., 1., 0., 0.])\n    s3_c4 = np.array([0., 0., 1., 0.])\n    s4_c4 = np.array([0., 0., 0., 1.])\n    y1_c4 = A_c4 @ s1_c4\n    y2_c4 = A_c4 @ s2_c4\n    y3_c4 = A_c4 @ s3_c4\n    y4_c4 = A_c4 @ s4_c4\n    m_c4 = 2\n    case4 = {\n        \"m\": m_c4, \"gamma\": 1.0,\n        \"s_pairs\": [s3_c4, s4_c4], # Only last m=2 pairs\n        \"y_pairs\": [y3_c4, y4_c4],\n        \"g\": np.array([1., 2., 3., 4.])\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        p = compute_lbfgs_direction(case[\"m\"], case[\"gamma\"], case[\"s_pairs\"], case[\"y_pairs\"], case[\"g\"])\n        results.append(p)\n\n    str_results = [str(p.tolist()) for p in results]\n    final_output = f\"[{','.join(str_results)}]\"\n    \n    # Remove all whitespace to match the required output format.\n    print(final_output.replace(\" \", \"\"))\n\nsolve()\n```", "id": "2431082"}]}