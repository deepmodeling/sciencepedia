{"hands_on_practices": [{"introduction": "The heart of any adaptive algorithm is its ability to estimate the local error of a numerical step. This practice contrasts two fundamental strategies for obtaining an error estimate: the general-purpose but computationally expensive step-doubling method, and the highly efficient embedded Runge-Kutta method. By implementing and comparing both, you will gain a deep, practical understanding of the trade-offs between computational cost and algorithmic design, and see firsthand why embedded methods form the bedrock of modern ODE solvers [@problem_id:2372273].", "problem": "You are to write a complete program that compares two adaptive step-size strategies for integrating a scalar initial value problem of an ordinary differential equation. Consider an initial value problem of the form $y'(t)=f(t,y)$ with $y(t_0)=y_0$ on a finite interval $[t_0,t_f]$. For each test case below, integrate from $t_0$ to $t_f$ and produce two summaries, one for an embedded Runge–Kutta–Fehlberg $4(5)$ method and one for a step-doubling strategy built on the classical fourth-order Runge–Kutta method. For each strategy, report the total number of function evaluations and the arithmetic mean of the accepted step sizes.\n\nMathematical specifications that must be followed exactly for both strategies:\n- Error scaling: For a trial step from $(t,y)$ with stepsize $h$, define the scalar scale $s=\\max\\{|y|,|y_{\\mathrm{trial}}|\\}$, the tolerance $T=\\text{atol}+\\text{rtol}\\cdot s$, and an error estimate $E\\ge 0$ computed by the strategy. A trial step is accepted if and only if $E\\le T$. If accepted, update $(t,y)\\leftarrow (t+h,y_{\\mathrm{accept}})$. If the trial step would overshoot $t_f$, replace $h$ by $t_f-t$ before the trial.\n- Step-size update: Let the method’s effective local error order be $q=5$ so that the local error behaves as $\\mathcal{O}(h^q)$. After any trial (accepted or rejected), define the growth factor\n$$\ng=\n\\begin{cases}\n\\max(g_{\\min},\\min(g_{\\max},\\,\\sigma\\,(T/\\max(E,\\varepsilon))^{1/q} )), & E>0,\\\\\ng_{\\max}, & E=0,\n\\end{cases}\n$$\nwith fixed constants $\\sigma=0.9$, $g_{\\min}=0.2$, $g_{\\max}=5$, and $\\varepsilon=10^{-30}$. Then set the next trial step size to $h\\leftarrow \\max(h_{\\min},\\min(h_{\\max},g\\,h))$ with $h_{\\min}=10^{-12}$ and $h_{\\max}=t_f-t_0$. The initial step size must be $h_0=(t_f-t_0)/50$ for all test cases.\n- Angle unit: All occurrences of trigonometric functions use radians.\n\nEmbedded Runge–Kutta–Fehlberg $4(5)$ method details:\n- Use the classical Fehlberg coefficients with $6$ stages. Given $h$, the internal stages are\n$$\n\\begin{aligned}\nk_1&=f(t, y),\\\\\nk_2&=f\\!\\left(t+\\tfrac{1}{4}h,\\,y+h\\left(\\tfrac{1}{4}k_1\\right)\\right),\\\\\nk_3&=f\\!\\left(t+\\tfrac{3}{8}h,\\,y+h\\left(\\tfrac{3}{32}k_1+\\tfrac{9}{32}k_2\\right)\\right),\\\\\nk_4&=f\\!\\left(t+\\tfrac{12}{13}h,\\,y+h\\left(\\tfrac{1932}{2197}k_1-\\tfrac{7200}{2197}k_2+\\tfrac{7296}{2197}k_3\\right)\\right),\\\\\nk_5&=f\\!\\left(t+h,\\,y+h\\left(\\tfrac{439}{216}k_1-8k_2+\\tfrac{3680}{513}k_3-\\tfrac{845}{4104}k_4\\right)\\right),\\\\\nk_6&=f\\!\\left(t+\\tfrac{1}{2}h,\\,y+h\\left(-\\tfrac{8}{27}k_1+2k_2-\\tfrac{3544}{2565}k_3+\\tfrac{1859}{4104}k_4-\\tfrac{11}{40}k_5\\right)\\right).\n\\end{aligned}\n$$\nThe fourth- and fifth-order trial solutions are\n$$\n\\begin{aligned}\ny_4&=y+h\\left(\\tfrac{25}{216}k_1+\\tfrac{1408}{2565}k_3+\\tfrac{2197}{4104}k_4-\\tfrac{1}{5}k_5\\right),\\\\\ny_5&=y+h\\left(\\tfrac{16}{135}k_1+\\tfrac{6656}{12825}k_3+\\tfrac{28561}{56430}k_4-\\tfrac{9}{50}k_5+\\tfrac{2}{55}k_6\\right).\n\\end{aligned}\n$$\nUse $y_{\\mathrm{trial}}=y_5$, $y_{\\mathrm{accept}}=y_5$, and error estimate $E=|y_5-y_4|$. Count exactly $6$ function evaluations per trial step (regardless of acceptance).\n\nStep-doubling strategy (based on the classical fourth-order Runge–Kutta method):\n- The classical fourth-order Runge–Kutta step for step size $h$ is\n$$\n\\begin{aligned}\nK_1&=f(t,y),\\quad K_2=f\\!\\left(t+\\tfrac{h}{2},y+\\tfrac{h}{2}K_1\\right),\\quad K_3=f\\!\\left(t+\\tfrac{h}{2},y+\\tfrac{h}{2}K_2\\right),\\\\\nK_4&=f\\!\\left(t+h,y+hK_3\\right),\\quad \\Phi_h(y)=y+\\tfrac{h}{6}\\left(K_1+2K_2+2K_3+K_4\\right).\n\\end{aligned}\n$$\nFor each trial step, compute $y^{(1)}=\\Phi_h(y)$ and $y^{(2)}=\\Phi_{h/2}(\\Phi_{h/2}(y))$. Use $y_{\\mathrm{trial}}=y^{(2)}$, $y_{\\mathrm{accept}}=y^{(2)}$, and the Richardson error estimate\n$$\nE=\\frac{|\\,y^{(2)}-y^{(1)}\\,|}{2^4-1}=\\frac{|\\,y^{(2)}-y^{(1)}\\,|}{15}.\n$$\nCount exactly $12$ function evaluations per trial step (one full step costing $4$ evaluations plus two half steps costing $8$ evaluations), regardless of acceptance.\n\nTest suite:\nFor each case $i\\in\\{1,2,3\\}$ below, the program must run both strategies with the same $(t_0,t_f,y_0,\\text{rtol},\\text{atol})$ and produce the outputs specified under “Required final output format.”\n- Case $1$ (smooth linear): $f(t,y)=-2y+t$, $t_0=0$, $t_f=10$, $y_0=1$, $\\text{rtol}=10^{-6}$, $\\text{atol}=10^{-9}$.\n- Case $2$ (oscillatory forcing, radians): $f(t,y)=50\\cos(50t)-y$, $t_0=0$, $t_f=2$, $y_0=0$, $\\text{rtol}=10^{-5}$, $\\text{atol}=10^{-7}$.\n- Case $3$ (logistic growth): $f(t,y)=y(1-y)$, $t_0=0$, $t_f=10$, $y_0=10^{-6}$, $\\text{rtol}=10^{-7}$, $\\text{atol}=10^{-12}$.\n\nRequired final output format:\nYour program should produce a single line of output containing a list with three entries (one per test case), where each entry is the list\n$$\n[\\;N_{\\mathrm{RKF45}},\\;N_{\\mathrm{SD}},\\;\\overline{h}_{\\mathrm{RKF45}},\\;\\overline{h}_{\\mathrm{SD}}\\;],\n$$\nwith $N_{\\mathrm{RKF45}}$ the total number of function evaluations used by the embedded Runge–Kutta–Fehlberg method, $N_{\\mathrm{SD}}$ the total number of function evaluations used by the step-doubling strategy, $\\overline{h}_{\\mathrm{RKF45}}$ the arithmetic mean of all accepted step sizes for the embedded Runge–Kutta–Fehlberg method over $[t_0,t_f]$, and $\\overline{h}_{\\mathrm{SD}}$ the arithmetic mean of all accepted step sizes for the step-doubling strategy over $[t_0,t_f]$. The output must be printed as a single line in the exact format of a Python list, for example:\n[[N1_RKF45,N1_SD,mean_h1_RKF45,mean_h1_SD],[N2_RKF45,N2_SD,mean_h2_RKF45,mean_h2_SD],[N3_RKF45,N3_SD,mean_h3_RKF45,mean_h3_SD]]\n\nNo additional text should be printed.", "solution": "The task is to implement and compare two adaptive step-size control strategies for solving scalar ordinary differential equations (ODEs). The fundamental principle of adaptive integration is to dynamically adjust the step size $h$ to ensure that the local truncation error per step remains within a user-defined tolerance. This approach is significantly more efficient than using a fixed step size, as it allows the integrator to take large steps when the solution is smooth and small steps when it changes rapidly.\n\nThe control mechanism for both strategies is governed by a common framework. At each time $t$ with solution $y$, a trial step of size $h$ is attempted. This yields a trial solution $y_{\\mathrm{trial}}$ and an estimate of the local error, $E \\ge 0$. The step is deemed acceptable if $E \\le T$, where $T$ is the tolerance, defined as a combination of relative and absolute tolerances: $T = \\text{atol} + \\text{rtol} \\cdot s$, with the scale factor $s = \\max\\{|y|, |y_{\\mathrm{trial}}|\\}$. If the step is accepted, the solution is advanced to $(t+h, y_{\\mathrm{accept}})$.\n\nRegardless of whether the step is accepted or rejected, the step size for the next trial, $h_{\\text{new}}$, is computed based on the observed error. The problem specifies a standard update rule:\n$$h_{\\text{new}} = g \\cdot h_{\\text{old}}$$\nwhere $g$ is a growth factor. For a method whose local error estimate behaves as $\\mathcal{O}(h^q)$, the ideal factor would be $(T/E)^{1/q}$. To ensure stability, this factor is moderated:\n$$\ng =\n\\begin{cases}\n\\max\\left(g_{\\min}, \\min\\left(g_{\\max}, \\sigma \\left(\\frac{T}{\\max(E, \\varepsilon)}\\right)^{1/q}\\right)\\right), & E > 0 \\\\\ng_{\\max}, & E = 0\n\\end{cases}\n$$\nHere, $\\sigma  1$ is a safety factor, $\\varepsilon$ is a very small number to prevent division by zero, and $g_{\\min}, g_{\\max}$ are bounds to prevent drastic changes in step size. The problem specifies $q=5$, $\\sigma=0.9$, $g_{\\min}=0.2$, $g_{\\max}=5$, and $\\varepsilon=10^{-30}$. The new step size is also clamped within $[h_{\\min}, h_{\\max}]$.\n\nThe two strategies differ solely in how they compute the trial solution and the error estimate $E$.\n\n**1. Embedded Runge–Kutta–Fehlberg $4(5)$ Method (RKF45)**\n\nThis is an \"embedded\" method, which uses a pair of Runge-Kutta formulas of different orders ($p=4$ and $p+1=5$) that share intermediate function evaluations (stages) to minimize computational cost. For a step size $h$, the method computes six stages, $k_1, \\dots, k_6$, which are then linearly combined to produce two approximations: a fourth-order solution $y_4$ and a fifth-order solution $y_5$.\n$$y_4 = y + h \\sum_{i=1}^6 b_i k_i, \\quad y_5 = y + h \\sum_{i=1}^6 b_i^* k_i$$\nThe problem specifies using the higher-order solution to advance the integration, so $y_{\\mathrm{trial}} = y_5$ and $y_{\\mathrm{accept}} = y_5$. The difference between the two solutions provides an asymptotically correct estimate of the local truncation error of the lower-order method:\n$$E = |y_5 - y_4| = \\mathcal{O}(h^5)$$\nThis error estimate is used in the step-size control logic. Since the error estimate is of order $5$, the specified controller parameter $q=5$ is appropriate. The entire procedure requires $6$ function evaluations per trial step.\n\n**2. Step-Doubling with Classical Fourth-Order Runge-Kutta (RK4)**\n\nThis strategy, based on Richardson extrapolation, uses a single underlying method (RK4, order $p=4$) to generate an error estimate. For a desired step size $h$, the solution is advanced from $t$ to $t+h$ in two different ways:\n- One single step of size $h$, yielding $y^{(1)} = \\Phi_h(y)$.\n- Two consecutive steps of size $h/2$, yielding $y^{(2)} = \\Phi_{h/2}(\\Phi_{h/2}(y))$.\n\nThe local truncation error of the more accurate solution, $y^{(2)}$, can be estimated from the difference between the two results:\n$$E = \\frac{|y^{(2)} - y^{(1)}|}{2^p - 1} = \\frac{|y^{(2)} - y^{(1)}|}{15}$$\nThis error estimate is also of order $\\mathcal{O}(h^{p+1}) = \\mathcal{O}(h^5)$, making $q=5$ the correct choice for the step-size controller. The problem specifies advancing the solution with the more accurate result, $y_{\\text{accept}} = y^{(2)}$. This technique is computationally intensive; a single trial requires one full step ($4$ function evaluations) and two half-steps ($2 \\times 4 = 8$ evaluations), for a total of $12$ function evaluations, as no stage computations are shared.\n\nThe program is structured around a general-purpose adaptive integration function that implements the main control loop. This function accepts a \"stepper\" function as an argument, which encapsulates the logic specific to either RKF45 or step-doubling. This design cleanly separates the control logic from the error estimation method. The main script defines the three specified ODE problems and, for each one, calls the integrator twice—once with the RKF45 stepper and once with the step-doubling stepper—to compute and report the required performance metrics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# --- Global constants for step-size control ---\nSIGMA = 0.9\nG_MIN = 0.2\nG_MAX = 5.0\nEPSILON = 1e-30\nH_MIN = 1e-12\nQ_ORDER = 5.0\n\n# --- Stepper implementation for RKF45 ---\ndef rkf45_step(f, t, y, h):\n    \"\"\"\n    Performs one trial step using the Runge-Kutta-Fehlberg 4(5) method.\n    \"\"\"\n    # Fehlberg coefficients\n    k1 = f(t, y)\n    k2 = f(t + 1/4 * h, y + h * (1/4 * k1))\n    k3 = f(t + 3/8 * h, y + h * (3/32 * k1 + 9/32 * k2))\n    k4 = f(t + 12/13 * h, y + h * (1932/2197 * k1 - 7200/2197 * k2 + 7296/2197 * k3))\n    k5 = f(t + h, y + h * (439/216 * k1 - 8 * k2 + 3680/513 * k3 - 845/4104 * k4))\n    k6 = f(t + 1/2 * h, y + h * (-8/27 * k1 + 2 * k2 - 3544/2565 * k3 + 1859/4104 * k4 - 11/40 * k5))\n\n    # 4th and 5th order solutions\n    y4 = y + h * (25/216 * k1 + 1408/2565 * k3 + 2197/4104 * k4 - 1/5 * k5)\n    y5 = y + h * (16/135 * k1 + 6656/12825 * k3 + 28561/56430 * k4 - 9/50 * k5 + 2/55 * k6)\n\n    # Use 5th order for trial and acceptance\n    y_trial = y5\n    y_accept = y5\n\n    # Error estimate is the difference between the two\n    error_est = abs(y5 - y4)\n    \n    # 6 function evaluations per trial step\n    f_evals = 6\n    \n    return y_trial, y_accept, error_est, f_evals\n\n# --- Stepper implementation for Step-Doubling RK4 ---\ndef rk4_single_step(f, t, y, h):\n    \"\"\"\n    Performs a single step of the classical 4th order Runge-Kutta method.\n    \"\"\"\n    k1 = f(t, y)\n    k2 = f(t + h/2.0, y + h/2.0 * k1)\n    k3 = f(t + h/2.0, y + h/2.0 * k2)\n    k4 = f(t + h, y + h * k3)\n    return y + h/6.0 * (k1 + 2*k2 + 2*k3 + k4)\n\ndef sd_rk4_step(f, t, y, h):\n    \"\"\"\n    Performs one trial step using step-doubling with the RK4 method.\n    \"\"\"\n    # One step of size h\n    y1 = rk4_single_step(f, t, y, h)\n    \n    # Two steps of size h/2\n    y_mid = rk4_single_step(f, t, y, h/2.0)\n    y2 = rk4_single_step(f, t + h/2.0, y_mid, h/2.0)\n\n    # Use the more accurate solution for trial and acceptance\n    y_trial = y2\n    y_accept = y2\n    \n    # Richardson error estimate\n    error_est = abs(y2 - y1) / 15.0\n    \n    # 12 function evaluations per trial step (4 for full step, 8 for two half-steps)\n    f_evals = 12\n    \n    return y_trial, y_accept, error_est, f_evals\n\n# --- Generic adaptive ODE integrator ---\ndef integrate(f, t0, tf, y0, rtol, atol, stepper_func):\n    \"\"\"\n    Integrates an ODE using an adaptive step-size strategy.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    \n    h_max = float(tf - t0)\n    h = h_max / 50.0\n\n    total_f_evals = 0\n    accepted_h_sum = 0.0\n    accepted_steps_count = 0\n\n    while t  tf:\n        if t + h  tf:\n            h = tf - t\n\n        y_trial, y_accept, E, f_evals = stepper_func(f, t, y, h)\n        total_f_evals += f_evals\n        \n        s = max(abs(y), abs(y_trial))\n        T = atol + rtol * s\n\n        accepted = (E = T)\n\n        if accepted:\n            accepted_h_sum += h\n            accepted_steps_count += 1\n            t += h\n            y = y_accept\n\n        # Step-size update logic\n        if E  0:\n            g = max(G_MIN, min(G_MAX, SIGMA * (T / max(E, EPSILON))**(1.0/Q_ORDER)))\n        else: # E == 0 implies error is smaller than machine precision\n            g = G_MAX\n        \n        h = max(H_MIN, min(h_max, g * h))\n\n    mean_h = accepted_h_sum / accepted_steps_count if accepted_steps_count  0 else 0.0\n    \n    return total_f_evals, mean_h\n\n# --- Main solution function ---\ndef solve():\n    \"\"\"\n    Defines test cases and runs the comparison, printing the final result.\n    \"\"\"\n    # Define the ODE functions for the test cases\n    def f1(t, y): return -2.0 * y + t\n    def f2(t, y): return 50.0 * math.cos(50.0 * t) - y\n    def f3(t, y): return y * (1.0 - y)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: smooth linear\n        (f1, 0.0, 10.0, 1.0, 1e-6, 1e-9),\n        # Case 2: oscillatory forcing\n        (f2, 0.0, 2.0, 0.0, 1e-5, 1e-7),\n        # Case 3: logistic growth\n        (f3, 0.0, 10.0, 1e-6, 1e-7, 1e-12)\n    ]\n\n    results = []\n    for case in test_cases:\n        f, t0, tf, y0, rtol, atol = case\n        \n        # Run RKF45 strategy\n        n_rkf45, h_avg_rkf45 = integrate(f, t0, tf, y0, rtol, atol, rkf45_step)\n        \n        # Run Step-Doubling RK4 strategy\n        n_sd, h_avg_sd = integrate(f, t0, tf, y0, rtol, atol, sd_rk4_step)\n        \n        results.append([n_rkf45, n_sd, h_avg_rkf45, h_avg_sd])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```", "id": "2372273"}, {"introduction": "Real-world engineering problems are almost always described by systems of ODEs, not single equations, and often involve variables with vastly different scales or accuracy requirements. This exercise challenges you to extend the adaptive control logic from a single scalar error to a vector of errors by implementing a per-component tolerance scheme. Mastering this technique is essential for building robust solvers that can efficiently handle the complex, multi-scale physical models common in computational engineering [@problem_id:2370767].", "problem": "You are asked to design and implement an adaptive explicit one-step method with per-component error control for solving systems of ordinary differential equations of the form $\\dot{\\mathbf{y}}(t) = \\mathbf{f}(t,\\mathbf{y}(t))$ with an initial condition $\\mathbf{y}(t_0) = \\mathbf{y}_0 \\in \\mathbb{R}^n$. The goal is to enforce a vector of per-component absolute error tolerances $\\mathbf{tol} = [tol_1,\\dots,tol_n]^T$, where each $tol_i > 0$ is specified by the user. The method must be based on an embedded Runge–Kutta pair so that, on each trial step, you obtain two approximations $\\mathbf{y}^{[high]}$ and $\\mathbf{y}^{[low]}$ of different orders and estimate a local truncation error vector $\\mathbf{e} = \\mathbf{y}^{[high]} - \\mathbf{y}^{[low]}$. The step is accepted if and only if the infinity norm of the componentwise normalized error satisfies $\\max_i \\left( \\frac{|e_i|}{tol_i} \\right) \\le 1$. Otherwise, the step is rejected and retried with a smaller step size. You must derive a principled step-size controller from the fundamental scaling law of the local truncation error for one-step methods: if the local error behaves like $C h^{p+1}$ for some constant $C$ and method order $p$, then the step size $h$ should be adjusted to drive the normalized error towards unity, while using a multiplicative safety factor strictly less than one and bounding the change of $h$ between a minimum and maximum growth factor. You must not use any black-box ordinary differential equation solvers from external libraries.\n\nBase your implementation on a concrete, widely used embedded Runge–Kutta pair of orders $(5,4)$. Use the Dormand–Prince $(5,4)$ pair with the following Butcher tableau coefficients (all omitted entries are zero). The stage nodes $\\{c_j\\}$ are: $c_2 = \\frac{1}{5}$, $c_3 = \\frac{3}{10}$, $c_4 = \\frac{4}{5}$, $c_5 = \\frac{8}{9}$, $c_6 = 1$, $c_7 = 1$. The internal weights $\\{a_{j\\ell}\\}$ for stages $j=2,\\dots,7$ are:\n- $a_{21} = \\frac{1}{5}$.\n- $a_{31} = \\frac{3}{40}$, $a_{32} = \\frac{9}{40}$.\n- $a_{41} = \\frac{44}{45}$, $a_{42} = -\\frac{56}{15}$, $a_{43} = \\frac{32}{9}$.\n- $a_{51} = \\frac{19372}{6561}$, $a_{52} = -\\frac{25360}{2187}$, $a_{53} = \\frac{64448}{6561}$, $a_{54} = -\\frac{212}{729}$.\n- $a_{61} = \\frac{9017}{3168}$, $a_{62} = -\\frac{355}{33}$, $a_{63} = \\frac{46732}{5247}$, $a_{64} = \\frac{49}{176}$, $a_{65} = -\\frac{5103}{18656}$.\n- $a_{71} = \\frac{35}{384}$, $a_{72} = 0$, $a_{73} = \\frac{500}{1113}$, $a_{74} = \\frac{125}{192}$, $a_{75} = -\\frac{2187}{6784}$, $a_{76} = \\frac{11}{84}$.\nThe fifth-order weights $\\{b^{[5]}_\\ell\\}$ are $b^{[5]}_1 = \\frac{35}{384}$, $b^{[5]}_2 = 0$, $b^{[5]}_3 = \\frac{500}{1113}$, $b^{[5]}_4 = \\frac{125}{192}$, $b^{[5]}_5 = -\\frac{2187}{6784}$, $b^{[5]}_6 = \\frac{11}{84}$, $b^{[5]}_7 = 0$. The embedded fourth-order weights $\\{b^{[4]}_\\ell\\}$ are $b^{[4]}_1 = \\frac{5179}{57600}$, $b^{[4]}_2 = 0$, $b^{[4]}_3 = \\frac{7571}{16695}$, $b^{[4]}_4 = \\frac{393}{640}$, $b^{[4]}_5 = -\\frac{92097}{339200}$, $b^{[4]}_6 = \\frac{187}{2100}$, $b^{[4]}_7 = \\frac{1}{40}$. Use the difference between the two formulas to compute the error estimate.\n\nDesign the controller using the following constants: method order for error scaling $p = 4$, safety factor $s = 0.9$, minimum and maximum growth factors $g_{\\min} = 0.1$ and $g_{\\max} = 5.0$, and hard bounds on the step size $h_{\\min} = 10^{-12}$ and $h_{\\max} = 10^{0}$. You must start from a user-provided initial step $h_0$ and ensure that you never step past the final time by more than one step; if a proposed step would cross the final time, clip it to land exactly on the final time. If a step is rejected and the adjusted step size would fall below $h_{\\min}$, you must still proceed with $h = h_{\\min}$ to guarantee forward progress. If the normalized error equals zero, choose the maximum growth factor for the next step.\n\nFor verification, compute a final-time global error diagnostic for each test case by comparing the numerical solution $\\mathbf{y}(t_f)$ to the exact solution $\\mathbf{y}_{\\text{exact}}(t_f)$. For each case, report the single scalar\n$$\nR = \\max_i \\frac{|y_i(t_f) - y_{\\text{exact},i}(t_f)|}{tol_i}.\n$$\nYour program must solve all the following test cases and output their $R$ values:\n\n- Case A (happy path, decoupled linear system): dimension $n = 3$, time interval $[t_0,t_f] = [0,5]$, initial condition $\\mathbf{y}_0 = [1,1,1]^T$, dynamics $\\dot{y}_1 = -y_1$, $\\dot{y}_2 = -2 y_2$, $\\dot{y}_3 = -3 y_3$, per-component tolerances $\\mathbf{tol} = [10^{-8},10^{-9},10^{-10}]^T$, initial step $h_0 = 10^{-1}$. The exact solution is $y_1(t) = e^{-t}$, $y_2(t) = e^{-2t}$, $y_3(t) = e^{-3t}$.\n- Case B (oscillatory, moderate interval): dimension $n = 2$, time interval $[t_0,t_f] = [0,2\\pi]$, initial condition $\\mathbf{y}_0 = [1,0]^T$, dynamics $\\dot{y}_1 = y_2$, $\\dot{y}_2 = -y_1$, per-component tolerances $\\mathbf{tol} = [10^{-8},10^{-8}]^T$, initial step $h_0 = 5\\times 10^{-2}$. The exact solution is $y_1(t) = \\cos t$, $y_2(t) = -\\sin t$. Use radians for the angle unit.\n- Case C (mixed scales across components): dimension $n = 3$, time interval $[t_0,t_f] = [0,2]$, initial condition $\\mathbf{y}_0 = [10^{3},10^{-3},0]^T$, dynamics $\\dot{y}_1 = -10^{-1} y_1$, $\\dot{y}_2 = -5 y_2$, $\\dot{y}_3 = \\cos t$, per-component tolerances $\\mathbf{tol} = [10^{-3},10^{-9},10^{-6}]^T$, initial step $h_0 = 10^{-2}$. The exact solution is $y_1(t) = 10^{3} e^{-10^{-1} t}$, $y_2(t) = 10^{-3} e^{-5 t}$, $y_3(t) = \\sin t$.\n- Case D (edge case with a constant component): dimension $n = 2$, time interval $[t_0,t_f] = [0,5]$, initial condition $\\mathbf{y}_0 = [1,1]^T$, dynamics $\\dot{y}_1 = 0$, $\\dot{y}_2 = -y_2$, per-component tolerances $\\mathbf{tol} = [10^{-12},10^{-8}]^T$, initial step $h_0 = 10^{-1}$. The exact solution is $y_1(t) = 1$, $y_2(t) = e^{-t}$.\n\nYour program should produce a single line of output containing the values $[R_A,R_B,R_C,R_D]$ as a comma-separated list enclosed in square brackets. Each $R$ must be printed in scientific notation with exactly $6$ digits after the decimal point (for example, $1.234567e-03$).", "solution": "The problem statement has been validated and is deemed valid. It presents a well-posed, scientifically grounded task in computational engineering, providing all necessary parameters and specifications to design and implement an adaptive ordinary differential equation solver.\n\nThe objective is to construct an adaptive step-size solver for a system of first-order ordinary differential equations (ODEs), $\\dot{\\mathbf{y}}(t) = \\mathbf{f}(t, \\mathbf{y}(t))$, with initial conditions $\\mathbf{y}(t_0) = \\mathbf{y}_0$. The solver must use the Dormand–Prince $(5,4)$ embedded Runge–Kutta pair and a principled step-size control mechanism to meet a vector of per-component absolute error tolerances, $\\mathbf{tol}$.\n\nFirst, we formulate the general structure of an explicit Runge–Kutta method. Given a solution $(\\mathbf{y}_k, t_k)$ and a step size $h$, the method computes an approximation $\\mathbf{y}_{k+1}$ at time $t_{k+1} = t_k + h$. This is achieved by computing a series of intermediate stage derivatives, $\\mathbf{k}_j$, as follows:\n$$\n\\mathbf{k}_j = \\mathbf{f}\\left(t_k + c_j h, \\mathbf{y}_k + h \\sum_{\\ell=1}^{j-1} a_{j\\ell} \\mathbf{k}_\\ell\\right) \\quad \\text{for } j=1, \\dots, s\n$$\nwhere $s$ is the number of stages. The solution is then advanced using a weighted sum of these stages:\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_k + h \\sum_{j=1}^s b_j \\mathbf{k}_j\n$$\nThe coefficients $c_j$, $a_{j\\ell}$, and $b_j$ define the specific method and are given by its Butcher tableau.\n\nAn embedded Runge–Kutta pair utilizes two sets of weights, $\\mathbf{b}^{[high]}$ and $\\mathbf{b}^{[low]}$, corresponding to methods of different orders, say $p$ and $p-1$. In our case, the Dormand–Prince $(5,4)$ pair provides a fifth-order method and an embedded fourth-order method. After computing the stages $\\mathbf{k}_j$, we can form two approximations:\n$$\n\\mathbf{y}_{k+1}^{[high]} = \\mathbf{y}_k + h \\sum_{j=1}^7 b_j^{[5]} \\mathbf{k}_j \\quad (\\text{Order } 5)\n$$\n$$\n\\mathbf{y}_{k+1}^{[low]} = \\mathbf{y}_k + h \\sum_{j=1}^7 b_j^{[4]} \\mathbf{k}_j \\quad (\\text{Order } 4)\n$$\nThe difference between these two solutions provides an estimate of the local truncation error of the lower-order method:\n$$\n\\mathbf{e}_{k+1} = \\mathbf{y}_{k+1}^{[high]} - \\mathbf{y}_{k+1}^{[low]} = h \\sum_{j=1}^7 (b_j^{[5]} - b_j^{[4]}) \\mathbf{k}_j\n$$\nThis error estimate $\\mathbf{e}_{k+1}$ is the foundation for adaptive step-size control. The controller's goal is to adjust the step size $h$ so that this error meets a specified tolerance. Following the principle of local extrapolation, the more accurate fifth-order solution, $\\mathbf{y}_{k+1}^{[high]}$, is used to propagate the solution if the step is accepted.\n\nThe step-size control logic is based on a scalar normalized error, $E$, defined as the maximum of the component-wise errors normalized by their respective absolute tolerances:\n$$\nE = \\max_i \\left( \\frac{|e_i|}{tol_i} \\right)\n$$\nA trial step is deemed acceptable if and only if $E \\le 1$. If a step is accepted, we proceed to the next integration interval. If it is rejected ($E > 1$), the current step is discarded, and the step is retried with a smaller step size $h$.\n\nThe core of the controller is the algorithm for selecting the next step size, $h_{new}$. The local truncation error of the fourth-order method scales with the fifth power of the step size, i.e., $\\|\\mathbf{e}\\| \\propto h^{p+1}$ with $p=4$. We want to find a new step size $h_{new}$ such that the resulting error $E_{new}$ would be approximately equal to $1$. This leads to the relation:\n$$\n1 \\approx E \\left( \\frac{h_{new}}{h_{old}} \\right)^{p+1}\n$$\nSolving for $h_{new}$ yields $h_{new} \\approx h_{old} (1/E)^{1/(p+1)}$. To ensure stability and avoid aggressive step size changes, we introduce a safety factor $s  1$ (given as $s=0.9$) and bound the step size growth/shrinkage factor by $g_{min}$ and $g_{max}$ (given as $g_{min}=0.1$ and $g_{max}=5.0$). The proposed step-size update factor is therefore:\n$$\ng = \\min(g_{max}, \\max(g_{min}, s \\cdot E^{-1/(p+1)}))\n$$\nwhere $p=4$. The new step size is then $h_{new} = h_{old} \\cdot g$. Per the problem, if the normalized error $E$ is exactly zero, we must use the maximal growth factor, so $h_{new} = h_{old} \\cdot g_{max}$.\n\nThe complete integration algorithm is as follows:\n1.  Initialize time $t \\leftarrow t_0$, solution $\\mathbf{y} \\leftarrow \\mathbf{y}_0$, and step size $h \\leftarrow h_0$.\n2.  While $t  t_f$:\n    a. Ensure the step does not overshoot the final time: $h \\leftarrow \\min(h, t_f - t)$.\n    b. Enter a loop for the current step (to handle rejections):\n        i. Compute the seven stages $\\mathbf{k}_1, \\dots, \\mathbf{k}_7$ using the given Dormand–Prince coefficients.\n        ii. Calculate the error estimate vector $\\mathbf{e} = h \\sum_{j=1}^7 (b_j^{[5]} - b_j^{[4]}) \\mathbf{k}_j$.\n        iii. Compute the normalized error $E = \\max_i (|e_i| / tol_i)$.\n        iv. If $E \\le 1$, the step is accepted. Break the inner loop.\n        v. If $E > 1$, the step is rejected. Calculate a new, smaller step size $h_{new} = h \\cdot \\min(g_{max}, \\max(g_{min}, s \\cdot E^{-1/5}))$. We must guarantee forward progress by ensuring the new step size is not below $h_{min}=10^{-12}$: set $h \\leftarrow \\max(h_{new}, h_{min})$ and retry the step (continue the inner loop).\n    c. Update the state using the higher-order solution: $\\mathbf{y} \\leftarrow \\mathbf{y} + h \\sum_{j=1}^7 b_j^{[5]} \\mathbf{k}_j$ and $t \\leftarrow t + h$.\n    d. Calculate the step size for the next iteration. If $E = 0$, set the growth factor to $g_{max}$. Otherwise, use the formula $g = s \\cdot E^{-1/5}$. Clamp this within $[g_{min}, g_{max}]$. Let this optimal factor be $g_{opt}$.\n    e. Propose the next step-size $h_{next} = h \\cdot g_{opt}$.\n    f. Clamp the proposed step size within the absolute bounds $[h_{min}, h_{max}] = [10^{-12}, 1.0]$. Set $h \\leftarrow \\min(h_{max}, \\max(h_{min}, h_{next}))$.\n3. Upon completion, the final vector $\\mathbf{y}$ is the numerical solution at $t_f$. The diagnostic $R$ is then computed by comparing this numerical solution to the provided exact solution.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and report on all test cases.\n    \"\"\"\n\n    # Dormand-Prince (5,4) Butcher Tableau and controller parameters\n    DP5_C = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1], dtype=np.float64)\n    DP5_A = np.array([\n        [0, 0, 0, 0, 0, 0, 0],\n        [1/5, 0, 0, 0, 0, 0, 0],\n        [3/40, 9/40, 0, 0, 0, 0, 0],\n        [44/45, -56/15, 32/9, 0, 0, 0, 0],\n        [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0, 0],\n        [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0, 0],\n        [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]\n    ], dtype=np.float64)\n    # Weights for the 5th order solution (higher order)\n    DP5_B5 = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0], dtype=np.float64)\n    # Weights for the 4th order solution (for error estimation)\n    DP5_B4 = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40], dtype=np.float64)\n    # Difference of weights for efficient error calculation\n    DP5_E = DP5_B5 - DP5_B4\n    # Method order for error scaling\n    P = 4\n\n    # Controller parameters\n    params = {\n        'C': DP5_C, 'A': DP5_A, 'B': DP5_B5, 'E': DP5_E, 'P': P,\n        's': 0.9, 'g_min': 0.1, 'g_max': 5.0,\n        'h_min': 1e-12, 'h_max': 1.0\n    }\n\n    # --- Test Case Definitions ---\n\n    # Case A: Decoupled linear system\n    f_A = lambda t, y: np.array([-y[0], -2*y[1], -3*y[2]])\n    y_exact_A = lambda t: np.array([np.exp(-t), np.exp(-2*t), np.exp(-3*t)])\n    case_A = {\n        'f': f_A, 't_span': [0.0, 5.0], 'y0': np.array([1.0, 1.0, 1.0]),\n        'tol': np.array([1e-8, 1e-9, 1e-10]), 'h0': 1e-1,\n        'y_exact': y_exact_A\n    }\n\n    # Case B: Oscillatory system\n    f_B = lambda t, y: np.array([y[1], -y[0]])\n    y_exact_B = lambda t: np.array([np.cos(t), -np.sin(t)])\n    case_B = {\n        'f': f_B, 't_span': [0.0, 2*np.pi], 'y0': np.array([1.0, 0.0]),\n        'tol': np.array([1e-8, 1e-8]), 'h0': 5e-2,\n        'y_exact': y_exact_B\n    }\n\n    # Case C: Mixed scales\n    f_C = lambda t, y: np.array([-0.1*y[0], -5*y[1], np.cos(t)])\n    y_exact_C = lambda t: np.array([1e3 * np.exp(-0.1*t), 1e-3 * np.exp(-5*t), np.sin(t)])\n    case_C = {\n        'f': f_C, 't_span': [0.0, 2.0], 'y0': np.array([1e3, 1e-3, 0.0]),\n        'tol': np.array([1e-3, 1e-9, 1e-6]), 'h0': 1e-2,\n        'y_exact': y_exact_C\n    }\n\n    # Case D: Constant component\n    f_D = lambda t, y: np.array([0.0, -y[1]])\n    y_exact_D = lambda t: np.array([1.0, np.exp(-t)])\n    case_D = {\n        'f': f_D, 't_span': [0.0, 5.0], 'y0': np.array([1.0, 1.0]),\n        'tol': np.array([1e-12, 1e-8]), 'h0': 1e-1,\n        'y_exact': y_exact_D\n    }\n\n    test_cases = [case_A, case_B, case_C, case_D]\n    results = []\n\n    for case in test_cases:\n        y_final = adaptive_rk_solver(case['f'], case['t_span'], case['y0'],\n                                     case['tol'], case['h0'], params)\n        y_exact_final = case['y_exact'](case['t_span'][1])\n        \n        # Calculate the final diagnostic R\n        R = np.max(np.abs(y_final - y_exact_final) / case['tol'])\n        results.append(R)\n\n    # Print results in the specified format\n    print(f\"[{','.join([f'{r:.6e}' for r in results])}]\")\n\ndef adaptive_rk_solver(f, t_span, y0, tol, h0, params):\n    \"\"\"\n    Implements an adaptive step-size Runge-Kutta solver based on the\n    Dormand-Prince (5,4) pair as specified.\n    \"\"\"\n    t0, tf = t_span\n    t = t0\n    y = np.copy(y0)\n    h = h0\n    \n    # Unpack parameters\n    C, A, B, E_w = params['C'], params['A'], params['B'], params['E']\n    p = params['P']\n    s, g_min, g_max = params['s'], params['g_min'], params['g_max']\n    h_min, h_max = params['h_min'], params['h_max']\n    \n    num_stages = len(C)\n    \n    while t  tf:\n        # Clip step size to not overshoot tf\n        if t + h  tf:\n            h = tf - t\n            \n        accepted = False\n        while not accepted:\n            # Compute stages\n            k = np.zeros((num_stages, len(y)), dtype=np.float64)\n            k[0] = f(t, y)\n            for i in range(1, num_stages):\n                y_stage = y + h * A[i, :i] @ k[:i, :]\n                k[i] = f(t + C[i] * h, y_stage)\n\n            # Calculate error estimate\n            error_est = h * (E_w @ k)\n            \n            # Calculate normalized error scalar\n            # Adding a small epsilon to tol to prevent division by zero in theory,\n            # though problem constraints guarantee tol_i  0\n            err_norm = np.max(np.abs(error_est) / tol)\n            \n            if err_norm = 1.0:\n                # Step is accepted\n                accepted = True\n                \n                # Update solution and time\n                t = t + h\n                y = y + h * (B @ k)\n                \n                # Calculate step size for the next step\n                if err_norm == 0.0:\n                    growth_factor = g_max\n                else:\n                    growth_factor = s * (err_norm ** (-1.0 / (p + 1.0)))\n                \n                h_opt_factor = min(g_max, max(g_min, growth_factor))\n                h = min(h_max, max(h_min, h * h_opt_factor))\n                \n            else:\n                # Step is rejected\n                growth_factor = s * (err_norm ** (-1.0 / (p + 1.0)))\n                h_opt_factor = max(g_min, growth_factor) # Only shrink\n                h_new = h * h_opt_factor\n                \n                # Enforce minimum step size to guarantee progress\n                h = max(h_min, h_new) \n                \n                # Check for possible stall at tf\n                if t + h == t:\n                    raise RuntimeError(\"Step size underflow at t={t}\")\n\n    return y\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2370767"}, {"introduction": "Standard ODE solvers perform best when the derivative function $f(t, y)$ is smooth, but many physical systems involve abrupt changes such as switches activating or forces suddenly appearing. This practice focuses on building a robust integrator that can handle known discontinuities in the ODE's right-hand side. You will learn to modify the solver's main loop to treat each break time as a mandatory stopping point, a crucial technique for ensuring accuracy and efficiency when faced with non-smooth problems [@problem_id:2370750].", "problem": "You are given initial value problems (IVPs) of the form $$\\frac{dy}{dt} = f(t,y), \\quad y(t_0) = y_0,$$ where the derivative function $f(t,y)$ is only piecewise continuous with respect to $t$. For each IVP, the time interval of integration is $[t_0, T]$. A finite set of break times $\\mathcal{B} = \\{b_1, b_2, \\ldots, b_m\\}$ is provided at which $f(t, y)$ may be discontinuous. You must design a program that computes an approximation to $y(T)$ using an adaptive step-size control strategy that enforces the following fundamental requirements: (i) the local error at each accepted step must be controlled relative to an absolute tolerance $\\mathrm{atol}$ and a relative tolerance $\\mathrm{rtol}$ using a scale of the form $$\\mathrm{scale} = \\mathrm{atol} + \\mathrm{rtol} \\cdot \\max\\{|y_{\\text{old}}|, |y_{\\text{new}}|\\},$$ and (ii) no single numerical step is allowed to span across any $b_i \\in \\mathcal{B}$; that is, each $b_i$ must be treated as an endpoint of a subinterval for the numerical approximation. If $T \\in \\mathcal{B}$, you must return the left-limit value at $T$, namely the value obtained by integrating on $[t_0, T)$ without crossing $T$. The output for each IVP must be a single real number equal to the approximation of $y(T)$, expressed as a floating-point number rounded to $10$ decimal places.\n\nUse the following test suite of IVPs with all mathematical quantities expressed in standard units without any physical dimensions:\n\n- Test case $1$:\n  - Differential equation: $$f_1(t,y) = -2\\,y + 1.$$\n  - Interval: $$t \\in [0, 1].$$\n  - Initial condition: $$y(0) = 0.$$\n  - Break times: $$\\mathcal{B} = \\varnothing.$$\n  - Target time: $$T = 1.$$\n  - Tolerances: $$\\mathrm{atol} = 10^{-10}, \\quad \\mathrm{rtol} = 10^{-10}.$$\n\n- Test case $2$:\n  - Differential equation: $$f_2(t,y) = -y + 2\\,H(t - 0.5),$$ where $H(\\tau)$ is the Heaviside step function defined by $H(\\tau) = 0$ for $\\tau  0$ and $H(\\tau) = 1$ for $\\tau \\ge 0$.\n  - Interval: $$t \\in [0, 1].$$\n  - Initial condition: $$y(0) = 0.$$\n  - Break times: $$\\mathcal{B} = \\{0.5\\}.$$\n  - Target time: $$T = 1.$$\n  - Tolerances: $$\\mathrm{atol} = 10^{-10}, \\quad \\mathrm{rtol} = 10^{-10}.$$\n\n- Test case $3$:\n  - Define $$g(t) = \\begin{cases}\n  0,  t  0.3,\\\\\n  3,  0.3 \\le t  0.3001,\\\\\n  0,  0.3001 \\le t  0.7,\\\\\n  -1,  t \\ge 0.7,\n  \\end{cases}$$\n  and $$f_3(t,y) = -y + g(t).$$\n  - Interval: $$t \\in [0, 1].$$\n  - Initial condition: $$y(0) = 0.$$\n  - Break times: $$\\mathcal{B} = \\{0.3, 0.3001, 0.7\\}.$$\n  - Target time: $$T = 1.$$\n  - Tolerances: $$\\mathrm{atol} = 10^{-10}, \\quad \\mathrm{rtol} = 10^{-10}.$$\n\n- Test case $4$:\n  - Differential equation: $$f_4(t,y) = -y + H(t - 1).$$\n  - Interval: $$t \\in [0, 1].$$\n  - Initial condition: $$y(0) = 1.$$\n  - Break times: $$\\mathcal{B} = \\{1\\}.$$\n  - Target time: $$T = 1.$$ If $T$ coincides with a break time, return the left-limit value at $T$ as described above.\n  - Tolerances: $$\\mathrm{atol} = 10^{-10}, \\quad \\mathrm{rtol} = 10^{-10}.$$\n\nYour program must solve all four test cases and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases. Each entry must be rounded to $10$ decimal places. For example, the output format must be exactly of the form\n\"[r1,r2,r3,r4]\"\nwhere $r1$, $r2$, $r3$, and $r4$ are the floating-point results corresponding to the test cases $1$ through $4$.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and complete. It describes a standard problem in computational engineering: the numerical solution of ordinary differential equations (ODEs) with piecewise continuous forcing terms. The requirements for an adaptive step-size control strategy and the handling of discontinuities are clearly specified, forming a well-defined computational task.\n\nThe problem requires the design of a numerical integrator for an initial value problem (IVP) of the form\n$$\n\\frac{dy}{dt} = f(t,y), \\quad y(t_0) = y_0\n$$\nover an interval $[t_0, T]$, where the function $f(t,y)$ may have a finite number of jump discontinuities with respect to time $t$ at specified break times $\\mathcal{B} = \\{b_1, b_2, \\ldots, b_m\\}$.\n\nThe core of the solution lies in a two-level strategy. At the higher level, a driver routine partitions the total integration interval $[t_0, T]$ into a sequence of subintervals whose endpoints are the initial time $t_0$, the final time $T$, and the break times in $\\mathcal{B}$. At the lower level, a robust adaptive step-size ODE solver integrates the solution across each continuous subinterval.\n\nFor the core integrator, an explicit embedded Runge-Kutta method is a suitable choice. We will use the Dormand-Prince 5(4) pair, hereafter referred to as DOPRI5(4). This method is widely used due to its excellent stability and efficiency. For each step of size $h$, it computes two approximations of the solution: a fifth-order accurate solution $y_{n+1}$ and a fourth-order accurate solution $\\hat{y}_{n+1}$. The difference between these two provides an estimate of the local truncation error, $E_{n+1} = y_{n+1} - \\hat{y}_{n+1}$.\n\nThe adaptive step-size control algorithm proceeds as follows. At each step $n$, starting from $(t_n, y_n)$ with a step size $h_n$:\n1.  Compute the fifth-order solution $y_{n+1}$ and the fourth-order solution $\\hat{y}_{n+1}$ using the DOPRI5(4) formulas.\n2.  Calculate the required tolerance scale for the current step, defined as $\\mathrm{scale} = \\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_n|, |y_{n+1}|)$, where $\\mathrm{atol}$ and $\\mathrm{rtol}$ are the prescribed absolute and relative tolerances.\n3.  Compute the normalized error, $err = |y_{n+1} - \\hat{y}_{n+1}| / \\mathrm{scale}$.\n4.  If $err \\le 1$, the step is accepted. The new state is $(t_{n+1}, y_{n+1}) = (t_n + h_n, y_{n+1})$.\n5.  If $err  1$, the step is rejected. The state remains $(t_n, y_n)$.\n6.  The step size for the next attempt (or the next accepted step) is adjusted using a standard formula:\n    $$\n    h_{\\text{new}} = h_n \\cdot S \\cdot \\left(\\frac{1}{err}\\right)^{1/(p+1)}\n    $$\n    where $p=4$ is the order of the lower-order method, and $S$ is a safety factor (typically $S \\approx 0.9$) to promote stability. The change in step size is usually bounded to prevent overly aggressive adjustments.\n\nThe high-level driver routine manages the integration across discontinuities. It first constructs a sorted list of unique time points by combining $\\{t_0, T\\}$ and the set of break times $\\mathcal{B}$. Let these points be $\\tau_0, \\tau_1, \\ldots, \\tau_k$, where $\\tau_0 = t_0$ and $\\tau_k \\le T$. The driver then iteratively calls the adaptive integrator for each subinterval $[\\tau_i, \\tau_{i+1}]$. The final state $y(\\tau_i)$ from one sub-integration serves as the initial condition for the next sub-integration starting at $\\tau_i$. This ensures that no numerical step crosses a discontinuity, as required.\n\nA special condition applies if the final time $T$ is itself a break time, as in Test Case 4. The problem requires returning the left-limit of the solution at $T$, denoted $y(T^-)$. This is interpreted as performing the integration on the final subinterval, say $[b_k, T]$, using the definition of the derivative function $f(t,y)$ that is valid for $t  T$. For Test Case 4 with $f_4(t,y) = -y + H(t - 1)$ and $T=1$, this means integrating the equation $y' = -y$ over the entire interval $[0,1]$, as $H(t-1)=0$ for all $t1$.\n\nThis structured approach is implemented for each test case as follows:\n-   **Test Case 1:** $f_1(t,y) = -2y + 1$. No break times. The adaptive integrator is called once on $[0, 1]$.\n-   **Test Case 2:** $f_2(t,y) = -y + 2H(t - 0.5)$. Break time at $t=0.5$. First, integrate $y' = -y$ on $[0, 0.5]$. Then, using the resulting $y(0.5)$, integrate $y' = -y + 2$ on $[0.5, 1]$.\n-   **Test Case 3:** $f_3(t,y) = -y + g(t)$. Break times at $0.3, 0.3001, 0.7$. The integration is performed in four stages:\n    1.  On $[0, 0.3]$ with $f(t,y) = -y$.\n    2.  On $[0.3, 0.3001]$ with $f(t,y) = -y + 3$.\n    3.  On $[0.3001, 0.7]$ with $f(t,y) = -y$.\n    4.  On $[0.7, 1]$ with $f(t,y) = -y - 1$.\n-   **Test Case 4:** $f_4(t,y) = -y + H(t - 1)$. $T=1$ is a break time. To find the left-limit, we integrate $y' = -y$ (the form of $f_4$ for $t1$) over the interval $[0, 1]$.\n\nThis methodology correctly addresses all constraints of the problem, providing a precise and robust numerical solution.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a suite of IVPs with piecewise continuous derivatives using a custom\n    adaptive Runge-Kutta integrator.\n    \"\"\"\n\n    # Dormand-Prince 5(4) Butcher Tableau coefficients\n    C = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1], dtype=np.float64)\n    A = np.array([\n        [0, 0, 0, 0, 0, 0, 0],\n        [1/5, 0, 0, 0, 0, 0, 0],\n        [3/40, 9/40, 0, 0, 0, 0, 0],\n        [44/45, -56/15, 32/9, 0, 0, 0, 0],\n        [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0, 0],\n        [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0, 0],\n        [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]\n    ], dtype=np.float64)\n    \n    # 5th order solution coefficients\n    B5 = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0], dtype=np.float64)\n    # 4th order solution coefficients (for error estimation)\n    B4 = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40], dtype=np.float64)\n    E = B5 - B4\n\n    def adaptive_integrator(f, t_span, y0, atol, rtol):\n        \"\"\"\n        Integrates a single continuous IVP using an adaptive DOPRI5(4) method.\n        \"\"\"\n        t0, t_end = t_span\n        t = t0\n        y = y0\n\n        h = 1e-6  # Initial step size suggestion\n        \n        # Integrator parameters\n        safety_factor = 0.9\n        min_scale = 0.2\n        max_scale = 5.0\n\n        while t  t_end:\n            if t + h  t_end:\n                h = t_end - t\n\n            # Compute stages\n            k = np.zeros(7, dtype=np.float64)\n            for i in range(7):\n                k_sum = np.dot(A[i, :i], k[:i])\n                k[i] = f(t + C[i] * h, y + h * k_sum)\n\n            # Compute 5th order solution and error estimate\n            y_new = y + h * np.dot(B5, k)\n            err_est = h * np.dot(E, k)\n\n            # Error control\n            scale = atol + rtol * max(abs(y), abs(y_new))\n            err_norm = abs(err_est) / scale\n\n            if err_norm = 1.0: # Step accepted\n                t += h\n                y = y_new\n            \n            # Step size adaptation\n            if err_norm  1e-15: # Avoid division by zero\n                h_new_factor = safety_factor * (err_norm ** -0.2)\n                h *= min(max_scale, max(min_scale, h_new_factor))\n            else: # Error is very small, can increase step size aggressively\n                h *= max_scale\n\n            if h  1e-15: # Prevent step size from becoming too small\n                 raise RuntimeError(\"Step size underflow\")\n        \n        return y\n\n    test_cases = [\n        {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        atol = 1e-10\n        rtol = 1e-10\n\n        if case['id'] == 1:\n            f1 = lambda t, y: -2 * y + 1\n            y0 = 0.0\n            t_span = [0.0, 1.0]\n            result = adaptive_integrator(f1, t_span, y0, atol, rtol)\n        \n        elif case['id'] == 2:\n            f2a = lambda t, y: -y\n            f2b = lambda t, y: -y + 2\n            y0 = 0.0\n            # Interval 1: [0, 0.5]\n            y_mid = adaptive_integrator(f2a, [0.0, 0.5], y0, atol, rtol)\n            # Interval 2: [0.5, 1.0]\n            result = adaptive_integrator(f2b, [0.5, 1.0], y_mid, atol, rtol)\n\n        elif case['id'] == 3:\n            y0 = 0.0\n            # Interval 1: [0, 0.3]\n            f3a = lambda t, y: -y\n            y1 = adaptive_integrator(f3a, [0.0, 0.3], y0, atol, rtol)\n            # Interval 2: [0.3, 0.3001]\n            f3b = lambda t, y: -y + 3\n            y2 = adaptive_integrator(f3b, [0.3, 0.3001], y1, atol, rtol)\n            # Interval 3: [0.3001, 0.7]\n            f3c = lambda t, y: -y\n            y3 = adaptive_integrator(f3c, [0.3001, 0.7], y2, atol, rtol)\n            # Interval 4: [0.7, 1.0]\n            f3d = lambda t, y: -y - 1\n            result = adaptive_integrator(f3d, [0.7, 1.0], y3, atol, rtol)\n\n        elif case['id'] == 4:\n            # Left-limit case: integrate y' = -y for t in [0, 1]\n            f4 = lambda t, y: -y\n            y0 = 1.0\n            t_span = [0.0, 1.0]\n            result = adaptive_integrator(f4, t_span, y0, atol, rtol)\n\n        results.append(round(result, 10))\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2370750"}]}