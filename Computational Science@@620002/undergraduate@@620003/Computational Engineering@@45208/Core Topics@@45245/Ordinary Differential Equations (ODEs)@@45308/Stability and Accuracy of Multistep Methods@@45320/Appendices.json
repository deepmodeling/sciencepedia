{"hands_on_practices": [{"introduction": "The Dahlquist equivalence theorem forms the bedrock of our understanding of linear multistep methods, stating that a method is convergent if and only if it is both consistent and zero-stable. This first exercise [@problem_id:2437365] serves as a fundamental check of these two core properties. You will analyze a given two-step method to determine its convergence by systematically testing for consistency, which ensures the method approximates the differential equation, and zero-stability, which guarantees that errors do not grow uncontrollably.", "problem": "Consider a $k$-step linear multistep method (LMM) for solving the ordinary differential equation (ODE) $y'(t) = f(t,y(t))$ with constant step size $h>0$. The LMM is characterized by the pair of polynomials $\\rho(z)$ and $\\sigma(z)$ associated with the methodâ€™s coefficients. For a particular method, the characteristic polynomials are given by\n$$\\rho(z) = z^{2} - 1.1\\,z + 0.1, \\qquad \\sigma(z) = 1.5\\,z - 0.5.$$\nDetermine whether this method is convergent in the usual LMM sense (that is, whether its global error tends to zero as $h \\to 0$ for sufficiently smooth problems). Report your result as a single indicator value $C$, where $C=1$ if the method is convergent and $C=0$ if it is not. Provide the value of $C$ exactly (no rounding). No units are required.", "solution": "The problem requires an assessment of the convergence of a given linear multistep method (LMM). The method is defined by its characteristic polynomials:\n$$ \\rho(z) = z^{2} - 1.1\\,z + 0.1 $$\n$$ \\sigma(z) = 1.5\\,z - 0.5 $$\nThe degree of the polynomial $\\rho(z)$ is $k=2$, which indicates this is a $2$-step method.\n\nThe convergence of any LMM is determined by the Dahlquist equivalence theorem. This fundamental theorem states that a linear multistep method is convergent if and only if it is both consistent and zero-stable. We will examine each of these properties in turn.\n\nFirst, we analyze the consistency of the method. An LMM is consistent if and only if two conditions are met:\n1. $\\rho(1) = 0$\n2. $\\rho'(1) = \\sigma(1)$\n\nThe first condition, $\\rho(1)=0$, is known as the condition for order of accuracy $p \\geq 1$. Let us evaluate $\\rho(z)$ at $z=1$:\n$$ \\rho(1) = (1)^{2} - 1.1(1) + 0.1 = 1 - 1.1 + 0.1 = 0 $$\nThe first condition for consistency is satisfied.\n\nNext, we check the second condition, $\\rho'(1) = \\sigma(1)$. This is the primary condition for consistency. First, we compute the derivative of $\\rho(z)$:\n$$ \\rho'(z) = \\frac{d}{dz} \\left( z^{2} - 1.1\\,z + 0.1 \\right) = 2z - 1.1 $$\nEvaluating this derivative at $z=1$:\n$$ \\rho'(1) = 2(1) - 1.1 = 0.9 $$\nNow, we evaluate the second polynomial $\\sigma(z)$ at $z=1$:\n$$ \\sigma(1) = 1.5(1) - 0.5 = 1.0 $$\nWe now compare the two results. For the method to be consistent, we must have $\\rho'(1) = \\sigma(1)$. However, we find that:\n$$ 0.9 \\neq 1.0 $$\nThe second condition is not satisfied. Therefore, the method is **not consistent**.\n\nAccording to the Dahlquist equivalence theorem, consistency is a necessary condition for convergence. Since the method fails the consistency test, it cannot be convergent.\n\nFor the sake of completeness, we shall also analyze the zero-stability of the method. A method is zero-stable if and only if it satisfies the root condition: all roots of the first characteristic polynomial $\\rho(z)$ must lie within or on the closed unit disk in the complex plane, and any roots with modulus equal to $1$ must be simple.\nWe must find the roots of the equation $\\rho(z) = 0$:\n$$ z^{2} - 1.1\\,z + 0.1 = 0 $$\nThis is a simple quadratic equation. The roots can be found by factoring the polynomial:\n$$ (z - 1)(z - 0.1) = z^{2} - 1.1z + 0.1 = 0 $$\nThe roots are therefore $z_1 = 1$ and $z_2 = 0.1$.\nNow we check the root condition:\n1. The modulus of each root must be less than or equal to $1$.\n   - For the first root, $|z_1| = |1| = 1$. This root lies on the unit circle.\n   - For the second root, $|z_2| = |0.1| = 0.1$. This root lies strictly inside the unit disk.\n   Both roots satisfy the condition $|z_j| \\leq 1$.\n2. Any root lying on the unit circle must be simple.\n   The only root on the unit circle is $z_1=1$. Since the two roots $z_1$ and $z_2$ are distinct, the root $z_1=1$ has a multiplicity of $1$, and is therefore a simple root.\n\nBoth parts of the root condition are satisfied. Thus, the method is **zero-stable**.\n\nIn summary:\n- The method is **not consistent**.\n- The method is **zero-stable**.\n\nFor convergence, a method must be both consistent and zero-stable. Because this method is not consistent, it is **not convergent**.\n\nThe problem asks for an indicator value $C$, where $C=1$ for a convergent method and $C=0$ for a non-convergent method. As our analysis has proven the method is not convergent, the correct indicator value is $0$.", "answer": "$$\\boxed{0}$$", "id": "2437365"}, {"introduction": "Theory predicts that a method lacking zero-stability will fail to converge, regardless of its formal order of accuracy. In this practice [@problem_id:2437356], you will move from analysis to synthesis by designing a method that is intentionally flawed. By constructing a third-order method that violates the Dahlquist root condition, and then applying it to a simple initial value problem, you will witness firsthand the catastrophic error growth that provides a powerful, practical demonstration of this theoretical principle.", "problem": "You are to study stability and accuracy of linear multistep methods by constructing a method that is accurate of order three but intentionally not zero-stable, and then applying it to the scalar linear ordinary differential equation $y'=-y$ to observe numerical failure.\n\nConsider a $k$-step linear multistep method written in the form\n$$\\sum_{j=0}^{k}\\alpha_j\\,y_{n-j} \\;=\\; h\\sum_{j=0}^{k}\\beta_j\\,f_{n-j},$$\nwhere $h>0$ is the step size, $t_n = t_0 + n h$, $y_n \\approx y(t_n)$, and $f_{n-j} = f(t_{n-j}, y_{n-j})$. Define the characteristic polynomial associated with the left-hand side as\n$$\\rho(r) \\;=\\; \\sum_{j=0}^{k}\\alpha_j\\,r^{k-j}.$$\nZero-stability requires that all roots of $\\rho(r)$ lie in the closed unit disk and any root on the unit circle is simple.\n\nTask A (method design): Impose the non-zero-stability by requiring that $\\rho(r)$ has a repeated unit root and a third root strictly outside the unit disk:\n$$\\rho(r) \\;=\\; (r-1)^2(r-q),\\quad \\text{with } q = 2.$$\nUse $k=3$ and the backward-indexed form\n$$\\alpha_0\\,y_n + \\alpha_1\\,y_{n-1} + \\alpha_2\\,y_{n-2} + \\alpha_3\\,y_{n-3} \\;=\\; h\\left(\\beta_0\\,f_n + \\beta_1\\,f_{n-1} + \\beta_2\\,f_{n-2} + \\beta_3\\,f_{n-3}\\right).$$\nDetermine the coefficients $\\{\\alpha_j\\}_{j=0}^3$ from the above $\\rho(r)$ and then determine $\\{\\beta_j\\}_{j=0}^3$ by enforcing that the method has order $3$ in the sense of local truncation error (that is, it is exact for $y(t)$ equal to polynomials up to degree $3$ when $f=y'$). To make the method uniquely determined and explicit, enforce $\\beta_0 = 0$.\n\nTask B (application): Apply the designed method to the initial value problem $y'=-y$, $y(0)=1$, on the interval $[0,T]$ with $T=1$. Use exact starting values $y(0)$, $y(h)$, and $y(2h)$ taken from the exact solution $y(t)=e^{-t}$ to initialize the three-step method. For each step size $h$ in the test suite below, run the method to $t=T$ and compute the absolute error at the final time, defined as $E(h)=\\lvert y_N - e^{-T}\\rvert$, where $N = T/h$.\n\nTest suite:\n- Case $1$: $h=0.2$ and $T=1$.\n- Case $2$: $h=0.1$ and $T=1$.\n- Case $3$: $h=0.05$ and $T=1$.\n\nFinal output format: Your program should produce a single line of output containing the three absolute errors as a comma-separated list of decimal numbers enclosed in square brackets, in the order of the cases listed above, each rounded to $6$ decimal places (for example, $[x_1,x_2,x_3]$).", "solution": "The problem requires the design and application of a linear multistep method constructed to be of order three but not zero-stable. This exercise serves to demonstrate the critical role of zero-stability (the Dahlquist root condition) for the convergence of a numerical scheme. A method that is consistent but not zero-stable will not converge, and this example is engineered to exhibit such failure.\n\nFirst, we design the method as specified. A linear $k$-step method has the general form:\n$$ \\sum_{j=0}^{k}\\alpha_j\\,y_{n-j} \\;=\\; h\\sum_{j=0}^{k}\\beta_j\\,f_{n-j} $$\nFor this problem, the number of steps is $k=3$. The coefficients $\\{\\alpha_j\\}$ are defined through the characteristic polynomial $\\rho(r) = \\sum_{j=0}^{k}\\alpha_j\\,r^{k-j}$. To deliberately violate zero-stability, this polynomial is prescribed to have a repeated root on the unit circle and a root outside the unit disk:\n$$ \\rho(r) \\;=\\; (r-1)^2(r-q) $$\nwith the specific value $q=2$. We expand this polynomial to determine the $\\alpha_j$ coefficients, assuming the standard normalization $\\alpha_0=1$:\n$$ \\rho(r) = (r^2 - 2r + 1)(r-2) = r^3 - 2r^2 + r - 2r^2 + 4r - 2 = r^3 - 4r^2 + 5r - 2 $$\nBy comparing this with the form $\\rho(r) = \\alpha_0 r^3 + \\alpha_1 r^2 + \\alpha_2 r + \\alpha_3$, we find:\n$$ \\alpha_0 = 1, \\quad \\alpha_1 = -4, \\quad \\alpha_2 = 5, \\quad \\alpha_3 = -2 $$\nThe presence of the root $|r|=2 > 1$ violates the root condition, guaranteeing instability.\n\nNext, we determine the coefficients $\\{\\beta_j\\}_{j=0}^3$ by enforcing an order of accuracy $p=3$. The problem states the method must be explicit, which means $y_n$ can be computed without solving an equation involving $f_n$, thus we must have $\\beta_0=0$. The remaining coefficients $\\beta_1$, $\\beta_2$, and $\\beta_3$ are determined by requiring the method to be exact for the polynomials $y(t)=t^m$ for $m \\in \\{0, 1, 2, 3\\}$. This is equivalent to setting the first four coefficients of the local truncation error to zero.\nThis condition is expressed as $\\sum_{j=0}^3 \\alpha_j y(t_{n-j}) = h \\sum_{j=0}^3 \\beta_j y'(t_{n-j})$. Let's test for $y(t) = t^m$, taking $t_{n-j} = -jh$ for algebraic simplicity.\n\nFor $m=0$: $y(t)=1, y'(t)=0$. The condition is $\\sum_{j=0}^3 \\alpha_j = 0$. Using our values: $1-4+5-2 = 0$. This is satisfied, as required for consistency, since $\\rho(1)=0$.\n\nFor $m=1$: $y(t)=t, y'(t)=1$. The condition is $\\sum_{j=0}^3 \\alpha_j (-jh) = h\\sum_{j=0}^3 \\beta_j$.\n$$ -h(\\alpha_1 + 2\\alpha_2 + 3\\alpha_3) = h(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3) $$\nThe left side is $-h(-4 + 2(5) + 3(-2)) = -h(-4+10-6) = 0$. Thus, we must have $\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 = 0$. This is the second condition for consistency, $\\rho'(1) = \\sigma(1)$, and since $\\rho'(1)=0$, we need $\\sigma(1)=0$.\n\nFor $m=2$: $y(t)=t^2, y'(t)=2t$. The condition is $\\sum_{j=0}^3 \\alpha_j (-jh)^2 = h\\sum_{j=0}^3 \\beta_j (2(-jh))$.\n$$ h^2(\\alpha_1 + 4\\alpha_2 + 9\\alpha_3) = -2h^2(\\beta_1 + 2\\beta_2 + 3\\beta_3) $$\nSubstituting the $\\alpha_j$ values: $(-4 + 4(5) + 9(-2)) = -4+20-18=-2$.\nThis gives $-2 = -2(\\beta_1 + 2\\beta_2 + 3\\beta_3)$, which simplifies to $\\beta_1 + 2\\beta_2 + 3\\beta_3 = 1$.\n\nFor $m=3$: $y(t)=t^3, y'(t)=3t^2$. The condition is $\\sum_{j=0}^3 \\alpha_j (-jh)^3 = h\\sum_{j=0}^3 \\beta_j (3(-jh)^2)$.\n$$ -h^3(\\alpha_1 + 8\\alpha_2 + 27\\alpha_3) = 3h^3(\\beta_1 + 4\\beta_2 + 9\\beta_3) $$\nThe term with $\\alpha_j$ is $-(-4 + 8(5) + 27(-2)) = -(-4+40-54) = 18$.\nThis gives $18 = 3(\\beta_1 + 4\\beta_2 + 9\\beta_3)$, which simplifies to $\\beta_1 + 4\\beta_2 + 9\\beta_3 = 6$.\n\nWe now have a system of linear equations for $\\beta_1, \\beta_2, \\beta_3$:\n1. $\\beta_1 + \\beta_2 + \\beta_3 = 0$\n2. $\\beta_1 + 2\\beta_2 + 3\\beta_3 = 1$\n3. $\\beta_1 + 4\\beta_2 + 9\\beta_3 = 6$\nSubtracting equation (1) from (2) gives $\\beta_2 + 2\\beta_3 = 1$. Subtracting equation (2) from (3) gives $2\\beta_2 + 6\\beta_3 = 5$.\nFrom $\\beta_2 + 2\\beta_3 = 1$, we have $\\beta_2 = 1 - 2\\beta_3$. Substituting this into the second derived equation: $2(1-2\\beta_3) + 6\\beta_3 = 5 \\implies 2-4\\beta_3+6\\beta_3=5 \\implies 2\\beta_3=3 \\implies \\beta_3 = 3/2$.\nThen, $\\beta_2 = 1 - 2(3/2) = -2$.\nFinally, from equation (1), $\\beta_1 = -\\beta_2 - \\beta_3 = -(-2) - 3/2 = 1/2$.\nSo, the coefficients are $\\beta_0=0$, $\\beta_1=1/2$, $\\beta_2=-2$, and $\\beta_3=3/2$.\n\nThe final explicit three-step method is:\n$$ y_n - 4y_{n-1} + 5y_{n-2} - 2y_{n-3} = h\\left(\\frac{1}{2} f_{n-1} - 2f_{n-2} + \\frac{3}{2}f_{n-3}\\right) $$\nRearranging to compute $y_n$:\n$$ y_n = 4y_{n-1} - 5y_{n-2} + 2y_{n-3} + h\\left(\\frac{1}{2} f_{n-1} - 2f_{n-2} + \\frac{3}{2}f_{n-3}\\right) $$\n\nFor Task B, we apply this method to the initial value problem $y' = -y$ with $y(0)=1$ on the interval $[0, T]$ with $T=1$. Here $f_{n-j} = -y_{n-j}$.\nThe recurrence relation becomes:\n$$ y_n = 4y_{n-1} - 5y_{n-2} + 2y_{n-3} - h\\left(\\frac{1}{2}y_{n-1} - 2y_{n-2} + \\frac{3}{2}y_{n-3}\\right) $$\n$$ y_n = \\left(4 - \\frac{h}{2}\\right)y_{n-1} + \\left(-5 + 2h\\right)y_{n-2} + \\left(2 - \\frac{3h}{2}\\right)y_{n-3} $$\nThe method is initialized with exact values from the solution $y(t) = e^{-t}$: $y_0 = e^0=1$, $y_1=e^{-h}$, and $y_2=e^{-2h}$. We iterate for $n=3, \\dots, N$ where $N=T/h$. The final error is $E(h)=\\lvert y_N - e^{-T}\\rvert$. Because the method is not zero-stable, the error is expected to grow as $h$ decreases, as the number of steps $N$ increases and the unstable mode proportional to $(2)^n$ dominates the solution. The following program implements this computation for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and applies a non-zero-stable linear multistep method to demonstrate\n    numerical instability.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (h, T)\n        (0.2, 1.0),\n        (0.1, 1.0),\n        (0.05, 1.0),\n    ]\n\n    results = []\n    for h, T in test_cases:\n        # Number of steps\n        # Use int(round(...)) to avoid floating point inaccuracies for N\n        N = int(round(T / h))\n\n        # Array to store the solution y_n\n        y = np.zeros(N + 1)\n\n        # Initialize with exact starting values from the solution y(t) = exp(-t)\n        # y_0, y_1, y_2\n        y[0] = np.exp(0.0)\n        y[1] = np.exp(-h)\n        y[2] = np.exp(-2 * h)\n\n        # Coefficients for the recurrence relation:\n        # y_n = c1*y_{n-1} + c2*y_{n-2} + c3*y_{n-3}\n        c1 = 4.0 - h / 2.0\n        c2 = -5.0 + 2.0 * h\n        c3 = 2.0 - 1.5 * h\n\n        # Apply the method from n=3 to N\n        for n in range(3, N + 1):\n            y[n] = c1 * y[n-1] + c2 * y[n-2] + c3 * y[n-3]\n\n        # Exact solution at the final time T\n        y_exact_T = np.exp(-T)\n\n        # Absolute error at the final time\n        error = np.abs(y[N] - y_exact_T)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    # The output format requires rounding to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2437356"}, {"introduction": "This final practice [@problem_id:2437328] explores a fascinating and practical scenario where a theoretically unstable method appears to behave correctly for thousands of steps before suddenly diverging. You will set up an experiment where the initial conditions are carefully tuned to follow the decaying, stable solution mode of a test problem. This exercise reveals the critical interplay between mathematical theory and computational reality, demonstrating how tiny, inevitable floating-point round-off errors can act as seeds for a latent instability, which grows silently until it ultimately overwhelms and destroys the numerical solution.", "problem": "Consider the linear ordinary differential equation $y^{\\prime}(t)=\\lambda y(t)$ with $\\lambda=-1$ and the constant stepsize grid $t_n=n h$ for $n\\in\\mathbb{N}_0$ with $h=0.01$. Define a two-step linear multistep method by the coefficients\n$$\n\\alpha_2=1,\\quad \\alpha_1=-(1+r_u),\\quad \\alpha_0=r_u,\\quad \\beta_2=1-r_u,\\quad \\beta_1=0,\\quad \\beta_0=0,\n$$\nwhere $r_u=1+\\delta$ with a given $\\delta>0$. This method takes the form\n$$\n\\alpha_2 y_{n+2}+\\alpha_1 y_{n+1}+\\alpha_0 y_n = h\\left(\\beta_2 f_{n+2}+\\beta_1 f_{n+1}+\\beta_0 f_n\\right),\n$$\nwith $f_{n+j}=f(t_{n+j},y_{n+j})=\\lambda y_{n+j}$. Let the characteristic polynomials be\n$$\n\\rho(\\zeta)=\\alpha_2 \\zeta^2+\\alpha_1 \\zeta+\\alpha_0,\\qquad \\sigma(\\zeta)=\\beta_2 \\zeta^2+\\beta_1 \\zeta+\\beta_0,\n$$\nand let $\\zeta$ satisfy the root condition for the linear test equation,\n$$\n\\rho(\\zeta)-h\\lambda \\sigma(\\zeta)=0.\n$$\n\nInitial conditions are defined as follows. Set $y_0=1$. Let $\\zeta_p$ be the root of $\\rho(\\zeta)-h\\lambda \\sigma(\\zeta)=0$ with modulus strictly less than $1$. Set $y_1=\\zeta_p\\,y_0$. These initial conditions place the discrete solution on the decaying mode associated with $\\zeta_p$ in exact arithmetic.\n\nFor each parameter set in the test suite given below, evolve the discrete solution $\\{y_n\\}$ on the grid $t_n$ and determine the first index $n\\ge n_{\\min}$ such that $|y_n|\\ge \\tau$, where\n$$\nn_{\\min}=\\min\\{n\\in\\mathbb{N}_0:\\ |\\exp(\\lambda n h)|<\\tau\\},\n$$\nso that the initial large value at $n=0$ is excluded, and $\\tau=10^{-2}$. If no such $n$ exists up to and including an imposed maximum index $N_{\\max}$, report $-1$ for that parameter set.\n\nTest suite (each parameter set is $(\\delta, h, \\lambda, N_{\\max}, \\tau)$):\n- $(2\\times 10^{-3},\\ 0.01,\\ -1,\\ 120000,\\ 10^{-2})$\n- $(10^{-3},\\ 0.01,\\ -1,\\ 120000,\\ 10^{-2})$\n- $(5\\times 10^{-4},\\ 0.01,\\ -1,\\ 120000,\\ 10^{-2})$\n- $(2\\times 10^{-2},\\ 0.01,\\ -1,\\ 120000,\\ 10^{-2})$\n\nYour program should produce a single line of output containing the four integers corresponding to the earliest indices for each parameter set, aggregated as a comma-separated list enclosed in square brackets (for example, $[n_1,n_2,n_3,n_4]$). The required output types are integers; use $-1$ if the threshold is not reached by $N_{\\max}$.", "solution": "The problem presented is a well-posed exercise in computational science, specifically in the numerical analysis of ordinary differential equations. It requires an investigation into the stability of a given linear two-step method. The method is deliberately constructed to be non-zero-stable, which introduces a parasitic, unstable solution mode. The task is to simulate the evolution of the discrete solution, which is initialized to follow the stable mode, and to determine when the inevitable growth of the unstable mode, seeded by floating-point round-off error, causes the solution's magnitude to exceed a given threshold. This is a standard and important concept in the study of numerical methods.\n\nFirst, let us analyze the problem setup. We are given the linear test equation $y'(t) = \\lambda y(t)$, with $\\lambda=-1$. The solution is computed on an equispaced grid $t_n = n h$, with step size $h=0.01$. The continuous solution is $y(t) = y(0)e^{\\lambda t}$, which decays exponentially.\n\nThe two-step linear multistep method (LMM) is defined by\n$$\n\\alpha_2 y_{n+2} + \\alpha_1 y_{n+1} + \\alpha_0 y_n = h \\left( \\beta_2 f_{n+2} + \\beta_1 f_{n+1} + \\beta_0 f_n \\right)\n$$\nwhere $f_{n+j} = \\lambda y_{n+j}$. Substituting the given coefficients, we have:\n$$\n\\alpha_2=1,\\quad \\alpha_1=-(1+r_u),\\quad \\alpha_0=r_u,\\quad \\beta_2=1-r_u,\\quad \\beta_1=0,\\quad \\beta_0=0\n$$\nwith $r_u = 1+\\delta$ for some $\\delta>0$. The method takes the specific form:\n$$\ny_{n+2} - (1+r_u) y_{n+1} + r_u y_n = h \\lambda (1-r_u) y_{n+2}\n$$\nThis is an implicit method, as $y_{n+2}$ appears on both sides. We can rearrange it into an explicit recurrence relation for the discrete solution $\\{y_n\\}$:\n$$\n(1 - h\\lambda(1-r_u)) y_{n+2} = (1+r_u) y_{n+1} - r_u y_n\n$$\n$$\ny_{n+2} = \\frac{(1+r_u) y_{n+1} - r_u y_n}{1 - h\\lambda(1-r_u)}\n$$\nThe behavior of solutions to this linear recurrence relation is governed by the roots of its characteristic polynomial. Substituting $y_n = \\zeta^n$ into the recurrence relation gives the stability polynomial equation:\n$$\n\\rho(\\zeta) - h\\lambda\\sigma(\\zeta) = 0\n$$\nwhere $\\rho(\\zeta) = \\zeta^2 - (1+r_u)\\zeta + r_u$ and $\\sigma(\\zeta) = (1-r_u)\\zeta^2$. The equation is:\n$$\n\\zeta^2 - (1+r_u)\\zeta + r_u - h\\lambda(1-r_u)\\zeta^2 = 0\n$$\nThis is a quadratic equation in $\\zeta$:\n$$\n(1 - h\\lambda(1-r_u))\\zeta^2 - (1+r_u)\\zeta + r_u = 0\n$$\nLet the two roots of this equation be $\\zeta_p$ and $\\zeta_s$. The general solution to the recurrence is of the form $y_n = c_1 \\zeta_p^n + c_2 \\zeta_s^n$.\n\nThe method's zero-stability is determined by the roots of $\\rho(\\zeta)=0$, which are $\\zeta=1$ and $\\zeta=r_u = 1+\\delta$. Since $\\delta>0$, the root $r_u$ has a modulus greater than $1$. This violates the root condition, rendering the method not zero-stable and therefore not convergent. This is the central feature of the problem.\n\nOne root of the full stability polynomial, which we call the principal root $\\zeta_p$, approximates the exact propagator $e^{h\\lambda}$. Since $h\\lambda = -0.01$, $|\\zeta_p|$ will be slightly less than $1$. The other root, the spurious or parasitic root $\\zeta_s$, arises from the multistep nature of the method. Due to the inherent instability of the method, we expect $|\\zeta_s| > 1$.\n\nThe problem specifies initial conditions $y_0=1$ and $y_1=\\zeta_p y_0 = \\zeta_p$, where $\\zeta_p$ is the root with modulus strictly less than $1$. In exact arithmetic, these initial conditions ensure that $c_2=0$ and the solution is purely $y_n = \\zeta_p^n$, which would decay to zero. However, in finite-precision floating-point arithmetic, round-off errors at each step of the recurrence will introduce a small component of the spurious solution, so the computed solution $\\hat{y}_n$ will be approximately $\\hat{y}_n \\approx \\zeta_p^n + \\epsilon \\zeta_s^n$, where $\\epsilon$ is a small value on the order of machine precision. Since $|\\zeta_s|>1$, the term $\\epsilon \\zeta_s^n$ will grow exponentially and eventually dominate the decaying term $\\zeta_p^n$.\n\nThe task is to find the first time step $n$, beyond an initial transient phase, where this instability becomes macroscopic, i.e., $|\\hat{y}_n| \\ge \\tau = 10^{-2}$. The search must begin from $n \\ge n_{\\min}$, where $n_{\\min}$ is the first integer $n$ for which the exact solution's magnitude $|\\exp(\\lambda n h)|$ drops below $\\tau$. This condition is given by $|\\exp(\\lambda n h)| < \\tau$, which, for $\\lambda < 0$, is equivalent to $\\lambda n h < \\ln(\\tau)$, or $n > \\ln(\\tau)/(\\lambda h)$. Thus, $n_{\\min} = \\lfloor \\ln(\\tau)/(\\lambda h) \\rfloor + 1$. For the given parameters, $n_{\\min} = \\lfloor \\ln(10^{-2})/(-0.01) \\rfloor + 1 = \\lfloor 460.517... \\rfloor + 1 = 461$.\n\nThe algorithm for each test case will be:\n1.  Calculate the coefficients of the quadratic stability polynomial $A\\zeta^2+B\\zeta+C=0$, where $A = 1 - h\\lambda(1-r_u)$, $B = -(1+r_u)$, and $C = r_u$.\n2.  Solve the quadratic equation to find the two roots.\n3.  Identify the principal root $\\zeta_p$ as the one with modulus less than $1$.\n4.  Initialize the solution array with $y_0 = 1$ and $y_1 = \\zeta_p$.\n5.  Iteratively compute the solution $y_n$ up to $N_{\\max}$ using the recurrence relation $y_{n+2} = -(B/A)y_{n+1} - (C/A)y_n$.\n6.  Starting from $n=n_{\\min}$, search for the first index $n$ where $|y_n| \\ge \\tau$.\n7.  If such an index is found before or at $N_{\\max}$, report it. Otherwise, report $-1$.\n\nThis computational procedure will be executed for each parameter set provided in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_one_case(delta, h, lam, N_max, tau):\n    \"\"\"\n    Solves for the instability onset time for a single parameter set.\n    \"\"\"\n    # 1. Calculate method parameters\n    r_u = 1.0 + delta\n    q = h * lam\n\n    # 2. Coefficients of the quadratic stability polynomial: A*zeta^2 + B*zeta + C = 0\n    A = 1.0 - q * (1.0 - r_u)\n    B = -(1.0 + r_u)\n    C = r_u\n\n    # 3. Solve the quadratic equation to find the roots\n    # The roots are guaranteed to be real for the given problem parameters.\n    # D = B^2 - 4AC = (1-r_u)^2 + 4*q*r_u*(1-r_u) = delta^2 - 4*q*(1+delta)*delta > 0\n    # since q = h*lam  0.\n    roots = np.roots([A, B, C])\n\n    # 4. Identify the principal root zeta_p (modulus  1)\n    # The problem guarantees one such root exists.\n    if np.abs(roots[0])  1.0:\n        zeta_p = roots[0]\n    else:\n        zeta_p = roots[1]\n        # Sanity check that the other root is > 1\n        if np.abs(roots[0]) = 1.0:\n             # This case should not be reached given the problem description.\n             # You may want to handle this as an error. For this problem, it's safe.\n             pass\n\n    # 5. Initialize solution array\n    # We use complex numbers as a safeguard, although roots are real.\n    # Array size is N_max + 1 for indices 0 to N_max.\n    y = np.zeros(N_max + 1, dtype=np.complex128)\n    y[0] = 1.0 + 0.0j\n    y[1] = complex(zeta_p)\n\n    # 6. Calculate n_min\n    # n_min is the first integer n such that |exp(lambda*n*h)|  tau\n    # Since lambda  0, this is exp(lam*n*h)  tau => lam*n*h  log(tau)\n    # => n > log(tau) / (lam*h)\n    n_min = int(np.floor(np.log(tau) / (lam * h))) + 1\n\n    # 7. Evolve the solution using the recurrence relation\n    # y_{n+2} = (-(1+r_u)/A)*y_{n+1} - (-r_u/A)*y_n = (-B/A)*y_{n+1} - (C/A)*y_n\n    # Note: this is a slight rewriting to use A,B,C directly\n    coeff1 = -B / A\n    coeff2 = -C / A\n\n    for n in range(N_max - 1):\n        y[n+2] = coeff1 * y[n+1] + coeff2 * y[n]\n\n    # 8. Find the first index n >= n_min where |y_n| >= tau\n    for n in range(n_min, N_max + 1):\n        if np.abs(y[n]) >= tau:\n            return n\n\n    # 9. If no such index is found, return -1\n    return -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (delta, h, lambda, N_max, tau)\n        (2e-3, 0.01, -1, 120000, 1e-2),\n        (1e-3, 0.01, -1, 120000, 1e-2),\n        (5e-4, 0.01, -1, 120000, 1e-2),\n        (2e-2, 0.01, -1, 120000, 1e-2),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_one_case(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2437328"}]}