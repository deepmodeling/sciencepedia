{"hands_on_practices": [{"introduction": "To truly understand the Local Truncation Error (LTE), we must first go beyond its abstract definition and compute it directly. This exercise challenges you to find the exact LTE for the Forward Euler method when applied to a specific initial value problem [@problem_id:2185646]. By deriving the exact analytical solution and comparing it to the one-step Euler approximation, you will gain a concrete, first-principles understanding of the error introduced in a single numerical step.", "problem": "Consider the initial value problem (IVP) given by the first-order ordinary differential equation:\n$$\ny'(t) = \\alpha y(t) - \\beta t^2\n$$\nwith the initial condition $y(0) = y_0$. In this equation, $\\alpha$, $\\beta$, and $y_0$ are given non-zero real constants.\n\nThis IVP is to be solved numerically using Euler's method with a step size $h$. The local truncation error for Euler's method at the $(i+1)$-th step is defined as $\\tau_{i+1} = y(t_{i+1}) - \\left[y(t_i) + h f(t_i, y(t_i))\\right]$, where $y(t)$ represents the exact solution to the IVP.\n\nCalculate the exact value of the local truncation error, $\\tau_1$, that occurs in the very first step of the method (from $t_0 = 0$ to $t_1 = h$). Express your answer as a single closed-form analytic expression in terms of $\\alpha$, $\\beta$, $y_0$, and $h$.", "solution": "The problem asks for the exact local truncation error, $\\tau_1$, for the first step of Euler's method applied to the IVP $y'(t) = \\alpha y(t) - \\beta t^2$ with $y(0) = y_0$.\n\nThe definition of the local truncation error for the first step (from $t_0=0$ to $t_1=h$) is given by:\n$$\n\\tau_1 = y(t_1) - \\left[y(t_0) + h f(t_0, y(t_0))\\right]\n$$\nwhere $y(t)$ is the exact solution to the ODE.\n\nFirst, we identify the terms in this definition:\n- $t_0 = 0$ and $t_1 = h$.\n- $y(t_0) = y(0) = y_0$.\n- $f(t,y) = \\alpha y - \\beta t^2$. Therefore, $f(t_0, y(t_0)) = f(0, y_0) = \\alpha y_0 - \\beta (0)^2 = \\alpha y_0$.\n\nSubstituting these into the definition of $\\tau_1$, we get:\n$$\n\\tau_1 = y(h) - \\left[y_0 + h(\\alpha y_0)\\right] = y(h) - y_0(1 + \\alpha h)\n$$\nTo find the exact value of $\\tau_1$, we must first find the exact solution $y(t)$ to the ODE, and then evaluate it at $t=h$.\n\nThe ODE is $y'(t) = \\alpha y(t) - \\beta t^2$, which is a first-order linear ODE. We can rewrite it in the standard form $y' - \\alpha y = -\\beta t^2$.\nThe integrating factor, $I(t)$, is given by:\n$$\nI(t) = \\exp\\left(\\int -\\alpha \\, dt\\right) = \\exp(-\\alpha t)\n$$\nMultiplying the standard form of the ODE by $I(t)$:\n$$\n\\exp(-\\alpha t)y' - \\alpha \\exp(-\\alpha t)y = -\\beta t^2 \\exp(-\\alpha t)\n$$\nThe left side is the derivative of the product $I(t)y(t)$:\n$$\n\\frac{d}{dt}\\left(y(t) \\exp(-\\alpha t)\\right) = -\\beta t^2 \\exp(-\\alpha t)\n$$\nIntegrate both sides with respect to $t$:\n$$\ny(t) \\exp(-\\alpha t) = -\\beta \\int t^2 \\exp(-\\alpha t) \\, dt\n$$\nThe integral on the right side requires integration by parts twice. Let $J = \\int t^2 \\exp(-\\alpha t) \\, dt$.\nFor the first integration by parts, let $u = t^2$ and $dv = \\exp(-\\alpha t) dt$. Then $du = 2t \\, dt$ and $v = -\\frac{1}{\\alpha}\\exp(-\\alpha t)$.\n$$\nJ = t^2 \\left(-\\frac{1}{\\alpha}\\exp(-\\alpha t)\\right) - \\int \\left(-\\frac{1}{\\alpha}\\exp(-\\alpha t)\\right) (2t \\, dt) = -\\frac{t^2}{\\alpha}\\exp(-\\alpha t) + \\frac{2}{\\alpha} \\int t \\exp(-\\alpha t) \\, dt\n$$\nFor the remaining integral, $\\int t \\exp(-\\alpha t) \\, dt$, we use integration by parts again. Let $u = t$ and $dv = \\exp(-\\alpha t) dt$. Then $du = dt$ and $v = -\\frac{1}{\\alpha}\\exp(-\\alpha t)$.\n$$\n\\int t \\exp(-\\alpha t) \\, dt = t\\left(-\\frac{1}{\\alpha}\\exp(-\\alpha t)\\right) - \\int \\left(-\\frac{1}{\\alpha}\\exp(-\\alpha t)\\right) dt = -\\frac{t}{\\alpha}\\exp(-\\alpha t) - \\frac{1}{\\alpha^2}\\exp(-\\alpha t)\n$$\nSubstituting this back into the expression for $J$:\n$$\nJ = -\\frac{t^2}{\\alpha}\\exp(-\\alpha t) + \\frac{2}{\\alpha} \\left(-\\frac{t}{\\alpha}\\exp(-\\alpha t) - \\frac{1}{\\alpha^2}\\exp(-\\alpha t)\\right) = -\\left(\\frac{t^2}{\\alpha} + \\frac{2t}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right)\\exp(-\\alpha t)\n$$\nNow we can write the solution for $y(t) \\exp(-\\alpha t)$:\n$$\ny(t) \\exp(-\\alpha t) = -\\beta J + C = \\beta\\left(\\frac{t^2}{\\alpha} + \\frac{2t}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right)\\exp(-\\alpha t) + C\n$$\nwhere $C$ is the constant of integration.\nMultiplying by $\\exp(\\alpha t)$ gives the general solution for $y(t)$:\n$$\ny(t) = \\beta\\left(\\frac{t^2}{\\alpha} + \\frac{2t}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right) + C \\exp(\\alpha t)\n$$\nWe use the initial condition $y(0) = y_0$ to find $C$:\n$$\ny(0) = y_0 = \\beta\\left(0 + 0 + \\frac{2}{\\alpha^3}\\right) + C \\exp(0) = \\frac{2\\beta}{\\alpha^3} + C\n$$\nSo, $C = y_0 - \\frac{2\\beta}{\\alpha^3}$.\n\nThe exact solution to the IVP is:\n$$\ny(t) = \\beta\\left(\\frac{t^2}{\\alpha} + \\frac{2t}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right) + \\left(y_0 - \\frac{2\\beta}{\\alpha^3}\\right) \\exp(\\alpha t)\n$$\nNow, we evaluate this exact solution at $t = h$:\n$$\ny(h) = \\beta\\left(\\frac{h^2}{\\alpha} + \\frac{2h}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right) + \\left(y_0 - \\frac{2\\beta}{\\alpha^3}\\right) \\exp(\\alpha h)\n$$\nFinally, we substitute this expression for $y(h)$ into our formula for the local truncation error $\\tau_1$:\n$$\n\\tau_1 = y(h) - y_0(1 + \\alpha h)\n$$\n$$\n\\tau_1 = \\left[ \\beta\\left(\\frac{h^2}{\\alpha} + \\frac{2h}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right) + \\left(y_0 - \\frac{2\\beta}{\\alpha^3}\\right) \\exp(\\alpha h) \\right] - y_0(1 + \\alpha h)\n$$\nThis is the final expression for the exact local truncation error. We can rearrange it slightly for clarity:\n$$\n\\tau_1 = \\left(y_0 - \\frac{2\\beta}{\\alpha^3}\\right) \\exp(\\alpha h) + \\beta\\left(\\frac{h^2}{\\alpha} + \\frac{2h}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right) - y_0 - y_0 \\alpha h\n$$", "answer": "$$\\boxed{\\left(y_0 - \\frac{2\\beta}{\\alpha^3}\\right) \\exp(\\alpha h) + \\beta\\left(\\frac{h^2}{\\alpha} + \\frac{2h}{\\alpha^2} + \\frac{2}{\\alpha^3}\\right) - y_0(1 + \\alpha h)}$$", "id": "2185646"}, {"introduction": "While analytical calculations are insightful, most real-world problems lack exact solutions, making direct error calculation impossible. This practice introduces a powerful computational technique to verify a solver's accuracy empirically [@problem_id:2395168]. You will write a program that treats the Forward Euler solver as a \"black box\" and uses its outputs to numerically estimate the order of its local truncation error, a core skill for validating any numerical code.", "problem": "Consider initial value problems of the form $\\dfrac{dy}{dt} = f(t,y)$ with $y(t_0)=y_0$ and a one-step solver that maps $(t_n,y_n)$ to $(t_{n+1},y_{n+1})$ with $t_{n+1}=t_n+h$. The Local Truncation Error (LTE) is defined as the error incurred over a single step when starting from the exact state, that is, the difference between the exact solution at $t_0+h$ and the solver’s one-step output from $(t_0,y(t_0))$. You are given access only to the solver’s outputs for various step sizes and must determine, from first principles, whether the LTE scales as $O(h^2)$ as $h \\to 0$, which is characteristic of the forward Euler method.\n\nYour task is to:\n- Construct an observable that depends only on the solver’s outputs at step sizes $h$ and $h/2$, both starting from the same initial data at $t_0$, in a way that reflects the single-step inaccuracy over $[t_0,t_0+h]$.\n- For a sequence of step sizes $(h_j)$ with $h_{j+1} = h_j/2$, empirically fit a power law of the form $\\lVert \\text{observable}(h) \\rVert \\approx C h^r$ as $h \\to 0$ and estimate the exponent $r$ by a least-squares fit on logarithmic scales. Use the absolute value for scalar problems and the Euclidean norm for vector-valued problems.\n- Conclude that the LTE is $O(h^2)$ if and only if the estimated exponent $r$ is approximately $2$ (to within numerical tolerance).\n\nDesign your program to carry out this analysis on four test cases. In all cases, angles (if any) are in radians.\n\nTest Suite:\n- Test $1$ (scalar, linear): $f(t,y) = -2y$, $t_0=0$, $y_0=1$, step sizes $h \\in \\{0.2, 0.1, 0.05, 0.025, 0.0125\\}$.\n- Test $2$ (scalar, nonlinear): $f(t,y) = y^2 - y$, $t_0=0$, $y_0=0.3$, step sizes $h \\in \\{0.2, 0.1, 0.05, 0.025, 0.0125\\}$.\n- Test $3$ (vector, linear rotation): $f(t,y) = \\begin{bmatrix}-y_2 \\\\ y_1\\end{bmatrix}$ for $y=\\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}$, $t_0=0$, $y_0=\\begin{bmatrix}1\\\\0\\end{bmatrix}$, step sizes $h \\in \\{0.2, 0.1, 0.05, 0.025, 0.0125\\}$.\n- Test $4$ (scalar, small-step stress): $f(t,y) = -2y$, $t_0=0$, $y_0=1$, step sizes $h \\in \\{2^{-10}, 2^{-11}, 2^{-12}, 2^{-13}, 2^{-14}\\}$.\n\nImplementation requirements:\n- Treat the solver as a black box; you may only use its one-step outputs at specified step sizes starting from $(t_0,y_0)$ to build your observable for each $h$.\n- For each test, compute the estimated exponent $r$ by performing a least-squares fit of $\\log(\\lVert \\text{observable}(h) \\rVert)$ versus $\\log(h)$ across all given $h$.\n- Your program must produce a single line of output containing the four estimated exponents as a comma-separated list enclosed in square brackets, with each value rounded to three decimal places (e.g., $[2.000,2.000,2.000,2.000]$).\n\nYour output must be numerical and unitless. No external inputs are provided; all computations must be performed within the program as specified above. The final output format is strictly one line with the list notation described.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical analysis for ordinary differential equations, is well-posed with a clear objective and methodology, and is free of contradiction or ambiguity. We shall proceed with a formal solution.\n\nThe objective is to devise and implement a numerical procedure to verify the order of the local truncation error (LTE) for a one-step solver, which is posited to be the forward Euler method. The hypothesis is that the LTE is of order $O(h^2)$, where $h$ is the step size.\n\nAn initial value problem (IVP) is given by the differential equation $\\dfrac{dy}{dt} = f(t,y)$ with an initial condition $y(t_0)=y_0$. The forward Euler method generates a sequence of approximations $y_n$ to the exact solution $y(t_n)$ at discrete time points $t_n = t_0 + nh$. The update rule is:\n$$y_{n+1} = y_n + h f(t_n, y_n)$$\n\nThe Local Truncation Error, which we denote $\\tau(h)$, is the error incurred in a single step, assuming the step begins with the exact solution, $y_n = y(t_n)$. It is defined as:\n$$\\tau(h) = y(t_n + h) - y_{n+1}$$\nTo determine the order of this error, we perform a Taylor series expansion of the exact solution $y(t_n + h)$ around $t_n$:\n$$y(t_n + h) = y(t_n) + h y'(t_n) + \\frac{h^2}{2} y''(t_n) + O(h^3)$$\nBy definition of the IVP, $y'(t_n) = f(t_n, y(t_n))$. Substituting this and the forward Euler formula for $y_{n+1}$ into the LTE definition, we obtain:\n$$ \\tau(h) = \\left( y(t_n) + h f(t_n, y(t_n)) + \\frac{h^2}{2} y''(t_n) + O(h^3) \\right) - \\left( y(t_n) + h f(t_n, y(t_n)) \\right) $$\n$$ \\tau(h) = \\frac{h^2}{2} y''(t_n) + O(h^3) $$\nThis confirms that the LTE for the forward Euler method is indeed of order $O(h^2)$.\n\nThe core of the problem is to verify this property without access to the exact solution $y(t_n+h)$ and its derivatives. We are tasked to construct an observable quantity using only the outputs of the \"black-box\" solver. A standard technique is to compare the result of a single step of size $h$ with the result of two consecutive steps of size $h/2$, both starting from the same initial state $(t_0, y_0)$.\n\nLet us define two approximations to the solution at time $t_0+h$:\n$1$. A single step of size $h$:\n$$y_h = y_0 + h f(t_0, y_0)$$\n$2$. Two successive steps of size $h/2$:\n$$y'_{h/2} = y_0 + \\frac{h}{2} f(t_0, y_0)$$\n$$y_{h/2,2} = y'_{h/2} + \\frac{h}{2} f\\left(t_0 + \\frac{h}{2}, y'_{h/2}\\right)$$\nOur observable, $E(h)$, is the difference between these two approximations:\n$$E(h) = y_h - y_{h/2,2}$$\nTo find the order of $E(h)$, we expand $y_{h/2,2}$ using a Taylor series for $f$ around $(t_0, y_0)$:\n$$f\\left(t_0 + \\frac{h}{2}, y'_{h/2}\\right) = f(t_0, y_0) + \\frac{h}{2} \\frac{\\partial f}{\\partial t} + (y'_{h/2} - y_0) \\frac{\\partial f}{\\partial y} + O(h^2)$$\nSubstituting $y'_{h/2} - y_0 = \\frac{h}{2} f(t_0, y_0)$ gives:\n$$f\\left(t_0 + \\frac{h}{2}, y'_{h/2}\\right) = f_0 + \\frac{h}{2} \\left( \\frac{\\partial f}{\\partial t} + f_0 \\frac{\\partial f}{\\partial y} \\right) + O(h^2) = f_0 + \\frac{h}{2} y''(t_0) + O(h^2)$$\nwhere $f_0 = f(t_0, y_0)$. Substituting this back into the expression for $y_{h/2,2}$:\n$$y_{h/2,2} = \\left(y_0 + \\frac{h}{2}f_0\\right) + \\frac{h}{2}\\left(f_0 + \\frac{h}{2} y''(t_0) + O(h^2)\\right) = y_0 + h f_0 + \\frac{h^2}{4} y''(t_0) + O(h^3)$$\nNow we compute the observable $E(h)$:\n$$E(h) = (y_0 + h f_0) - \\left(y_0 + h f_0 + \\frac{h^2}{4} y''(t_0) + O(h^3)\\right) = - \\frac{h^2}{4} y''(t_0) + O(h^3)$$\nThis demonstrates that for small $h$, the norm of our observable scales with the second power of $h$:\n$$\\lVert E(h) \\rVert \\approx K h^2$$\nwhere $K = \\frac{1}{4} \\lVert y''(t_0) \\rVert$. To estimate the exponent, which we call $r$, we analyze the relationship $\\lVert E(h) \\rVert \\approx K h^r$. By taking the natural logarithm of both sides, we obtain a linear relationship:\n$$\\log(\\lVert E(h) \\rVert) \\approx \\log(K) + r \\log(h)$$\nThis is an equation of the form $Y = A + rX$, with $Y = \\log(\\lVert E(h) \\rVert)$, $X = \\log(h)$, and $A = \\log(K)$. Given a set of measurements $(h_j, \\lVert E(h_j) \\rVert)$ for a sequence of decreasing step sizes $h_j$, we can perform a linear least-squares regression on the transformed data points $(\\log(h_j), \\log(\\lVert E(h_j) \\rVert))$ to find the slope $r$. If $r$ is approximately $2$, we can conclude that the observable scales as $h^2$, which in turn supports the hypothesis that the solver's LTE is $O(h^2)$.\n\nThe program will implement this procedure for the four specified test cases. For each case, it will:\n$1$. Iterate through the given sequence of step sizes, $h_j$.\n$2$. For each $h_j$, compute $y_{h_j}$ and $y_{h_j/2, 2}$ using the forward Euler rule.\n$3$. Calculate the norm of the observable $E(h_j) = y_{h_j} - y_{h_j/2, 2}$. The Euclidean norm is used, which corresponds to the absolute value for scalar problems.\n$4$. Collect the logarithm of the norms and the logarithm of the step sizes.\n$5$. Perform a first-degree polynomial (linear) fit to the log-log data to find the slope, $r$.\n$6$. Report the estimated exponent $r$ for each test case, rounded to three decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs an analysis to numerically estimate the order of the local\n    truncation error for the forward Euler method on four test cases.\n    \"\"\"\n\n    # Define the right-hand side functions for the ODEs dy/dt = f(t, y)\n    def f_case1(t, y):\n        return -2 * y\n\n    def f_case2(t, y):\n        return y**2 - y\n\n    def f_case3(t, y):\n        # y is expected to be a numpy array [y1, y2]\n        return np.array([-y[1], y[0]])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": f_case1,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"h_values\": np.array([0.2, 0.1, 0.05, 0.025, 0.0125]),\n        },\n        {\n            \"f\": f_case2,\n            \"t0\": 0.0,\n            \"y0\": 0.3,\n            \"h_values\": np.array([0.2, 0.1, 0.05, 0.025, 0.0125]),\n        },\n        {\n            \"f\": f_case3,\n            \"t0\": 0.0,\n            \"y0\": np.array([1.0, 0.0]),\n            \"h_values\": np.array([0.2, 0.1, 0.05, 0.025, 0.0125]),\n        },\n        {\n            \"f\": f_case1,  # Same f as Test 1\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"h_values\": np.array([2**-10, 2**-11, 2**-12, 2**-13, 2**-14]),\n        },\n    ]\n\n    estimated_exponents = []\n    for case in test_cases:\n        f = case[\"f\"]\n        t0 = case[\"t0\"]\n        y0 = case[\"y0\"]\n        h_values = case[\"h_values\"]\n        \n        observable_norms = []\n        \n        for h in h_values:\n            # The 'black-box' solver is the forward Euler method. We apply it\n            # once for a step of size h, and twice for steps of size h/2.\n            \n            # 1. One step of size h\n            y_h = y0 + h * f(t0, y0)\n            \n            # 2. Two steps of size h/2\n            h_half = h / 2.0\n            y_h_half_1 = y0 + h_half * f(t0, y0)  # First half-step\n            y_h_half_2 = y_h_half_1 + h_half * f(t0 + h_half, y_h_half_1) # Second half-step\n            \n            # 3. Construct the observable and compute its norm.\n            # The observable is the difference between the two approximations.\n            observable = y_h - y_h_half_2\n            \n            # The norm is the Euclidean norm (absolute value for scalars).\n            norm_obs = np.linalg.norm(observable)\n            observable_norms.append(norm_obs)\n        \n        # 4. Perform least-squares fit on logarithmic scales.\n        # The equation is log(norm) = r * log(h) + log(C).\n        # We find the slope 'r' of this linear relationship.\n        log_h = np.log(h_values)\n        log_err = np.log(np.array(observable_norms))\n        \n        # np.polyfit for a degree 1 polynomial returns [slope, intercept].\n        # The slope is the desired exponent 'r'.\n        slope, _ = np.polyfit(log_h, log_err, 1)\n        \n        estimated_exponents.append(slope)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.3f}\" for r in estimated_exponents]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2395168"}, {"introduction": "The ultimate goal of estimating the LTE is to actively control it, leading to more efficient and reliable simulations. In this capstone practice, you will implement an adaptive step-size algorithm for the Forward Euler method [@problem_id:2395159]. By using an error estimate to intelligently adjust the step size $h$ on the fly, you will create a solver that automatically balances accuracy and computational cost, a key principle behind modern ordinary differential equation software.", "problem": "Write a complete, runnable program that advances solutions of initial value problems for ordinary differential equations using the Forward Euler method with adaptive step-size control based on a local truncation error estimate. For an attempted step from time $t$ to $t + h$, let $y^{[h]}$ denote the one-step Forward Euler update and let $y^{[h/2]}$ denote the composition of two Forward Euler half-steps of size $h/2$. Define the local error estimate for that attempted step as $e = \\| y^{[h/2]} - y^{[h]} \\|$, where for scalar states the absolute value is used and for vector states the Euclidean norm is used. A step is accepted if and only if $e \\le \\epsilon$, in which case the solution is advanced to $t \\leftarrow t + h$ with state $y \\leftarrow y^{[h/2]}$. A step is rejected if $e > \\epsilon$, in which case $(t,y)$ are unchanged and a smaller step size must be retried. The tentative next step size is updated after each acceptance or rejection according to\n$$\nh_{\\text{new}} \\;=\\; h \\cdot \\sigma \\cdot \\mathrm{clip}\\!\\left(\\left(\\frac{\\epsilon}{\\max(e, e_{\\min})}\\right)^{1/2}, \\gamma_{\\min}, \\gamma_{\\max}\\right),\n$$\nwith fixed parameters $\\sigma = 0.9$, $e_{\\min} = 10^{-16}$, $\\gamma_{\\min} = 0.2$, and $\\gamma_{\\max} = 5.0$. On every attempt, enforce the bounds $h \\ge h_{\\min}$ with $h_{\\min} = 10^{-12}$ and do not overshoot the final time by replacing $h$ with $\\min(h, T_{\\text{end}} - t)$ before the step attempt. Use the Forward Euler updates\n$$\ny^{[h]} \\;=\\; y \\;+\\; h\\, f(t, y), \\qquad\ny^{[h/2]} \\;=\\; y \\;+\\; \\frac{h}{2}\\, f(t, y) \\;+\\; \\frac{h}{2}\\, f\\!\\left(t+\\frac{h}{2},\\, y + \\frac{h}{2}\\, f(t, y)\\right).\n$$\nTerminate when $t = T_{\\text{end}}$.\n\nApply the above to the following test suite of initial value problems. For each case, compute the absolute error (scalar) or Euclidean norm of the error (vector) at the final time by comparing with the exact solution specified.\n\n- Test case A (scalar, nonlinear):\n  - Differential equation: $y'(t) = -\\,y(t)^2$.\n  - Initial condition: $y(0) = 1$.\n  - Time interval: $t \\in [0, 1]$.\n  - Tolerance: $\\epsilon = 10^{-6}$.\n  - Initial step size: $h_0 = 0.4$.\n  - Exact solution at time $t$: $y(t) = \\dfrac{1}{1 + t}$.\n\n- Test case B (scalar, linear, non-autonomous):\n  - Differential equation: $y'(t) = \\cos(t) - y(t)$.\n  - Initial condition: $y(0) = 0$.\n  - Time interval: $t \\in [0, 3]$.\n  - Tolerance: $\\epsilon = 10^{-5}$.\n  - Initial step size: $h_0 = 0.3$.\n  - Exact solution at time $t$: $y(t) = \\dfrac{\\sin t + \\cos t}{2} \\;-\\; \\dfrac{1}{2}\\, e^{-t}$.\n\n- Test case C (vector, linear, decoupled/forced):\n  - Differential equation: \n    $$\n    \\begin{bmatrix} y_1'(t) \\\\[4pt] y_2'(t) \\end{bmatrix}\n    \\;=\\;\n    \\begin{bmatrix} -2\\, y_1(t) \\\\[4pt] -\\tfrac{1}{2}\\, y_2(t) + \\sin t \\end{bmatrix}.\n    $$\n  - Initial condition: $\\begin{bmatrix} y_1(0) \\\\[2pt] y_2(0) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\[2pt] 0 \\end{bmatrix}$.\n  - Time interval: $t \\in [0, 2]$.\n  - Tolerance: $\\epsilon = 10^{-6}$.\n  - Initial step size: $h_0 = 0.2$.\n  - Exact solution at time $t$: $y_1(t) = e^{-2 t}$ and $y_2(t) = \\dfrac{ \\tfrac{1}{2} \\sin t - \\cos t + e^{-t/2}}{1.25}$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases A, B, C. Each result must be a floating-point number equal to the absolute error (for scalar problems) or the Euclidean norm of the error (for the vector problem) at the final time $T_{\\text{end}}$, rounded to $10$ decimal places using standard rounding. For example, the required output format is\n$$\n[\\text{err\\_A},\\text{err\\_B},\\text{err\\_C}],\n$$\nwith each of $\\text{err\\_A}$, $\\text{err\\_B}$, and $\\text{err\\_C}$ shown with exactly $10$ digits after the decimal point and no additional whitespace.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n\nThe provided problem statement specifies the following:\n\n- **Numerical Method**: Adaptive Forward Euler method for solving initial value problems for ordinary differential equations (ODEs).\n- **One-Step Forward Euler Update**: $y^{[h]} = y + h\\, f(t, y)$.\n- **Two Half-Step Update (Explicit Midpoint Method)**: $y^{[h/2]} = y + \\frac{h}{2}\\, f(t, y) + \\frac{h}{2}\\, f(t+\\frac{h}{2},\\, y + \\frac{h}{2}\\, f(t, y))$.\n- **Local Error Estimate**: $e = \\| y^{[h/2]} - y^{[h]} \\|$, using the absolute value for scalar states and the Euclidean norm for vector states.\n- **Step Acceptance Criterion**: A step of size $h$ is accepted if $e \\le \\epsilon$.\n- **Solution Advancement on Acceptance**: If a step is accepted, the time and state are updated as $t \\leftarrow t + h$ and $y \\leftarrow y^{[h/2]}$.\n- **Step Rejection Criterion**: A step is rejected if $e > \\epsilon$. The state $(t, y)$ remains unchanged, and a new, smaller step size is attempted.\n- **Step-Size Update Formula**: The new tentative step size $h_{\\text{new}}$ is calculated after each attempt (accepted or rejected) using the formula:\n$$\nh_{\\text{new}} = h \\cdot \\sigma \\cdot \\mathrm{clip}\\!\\left(\\left(\\frac{\\epsilon}{\\max(e, e_{\\min})}\\right)^{1/2}, \\gamma_{\\min}, \\gamma_{\\max}\\right)\n$$\n- **Fixed Control Parameters**:\n  - Safety factor: $\\sigma = 0.9$.\n  - Minimum error for division: $e_{\\min} = 10^{-16}$.\n  - Minimum step-size change factor: $\\gamma_{\\min} = 0.2$.\n  - Maximum step-size change factor: $\\gamma_{\\max} = 5.0$.\n- **Step-Size Constraints**:\n  - Minimum allowed step size: $h \\ge h_{\\min} = 10^{-12}$.\n  - Final step adjustment: Before each attempt, the step size $h$ is replaced by $\\min(h, T_{\\text{end}} - t)$ to avoid overshooting the final time $T_{\\text{end}}$.\n- **Termination Condition**: Integration stops when the time $t$ reaches $T_{\\text{end}}$.\n\n- **Test Suite**:\n  - **Test Case A (scalar, nonlinear)**:\n    - ODE: $y'(t) = -y(t)^2$\n    - Initial Condition: $y(0) = 1$\n    - Time Interval: $t \\in [0, 1]$\n    - Tolerance: $\\epsilon = 10^{-6}$\n    - Initial Step Size: $h_0 = 0.4$\n    - Exact Solution: $y(t) = \\frac{1}{1 + t}$\n  - **Test Case B (scalar, linear, non-autonomous)**:\n    - ODE: $y'(t) = \\cos(t) - y(t)$\n    - Initial Condition: $y(0) = 0$\n    - Time Interval: $t \\in [0, 3]$\n    - Tolerance: $\\epsilon = 10^{-5}$\n    - Initial Step Size: $h_0 = 0.3$\n    - Exact Solution: $y(t) = \\frac{\\sin t + \\cos t}{2} - \\frac{1}{2} e^{-t}$\n  - **Test Case C (vector, linear, decoupled/forced)**:\n    - ODE: $\\begin{bmatrix} y_1'(t) \\\\ y_2'(t) \\end{bmatrix} = \\begin{bmatrix} -2 y_1(t) \\\\ -\\frac{1}{2} y_2(t) + \\sin t \\end{bmatrix}$\n    - Initial Condition: $\\begin{bmatrix} y_1(0) \\\\ y_2(0) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n    - Time Interval: $t \\in [0, 2]$\n    - Tolerance: $\\epsilon = 10^{-6}$\n    - Initial Step Size: $h_0 = 0.2$\n    - Exact Solution: $y_1(t) = e^{-2t}$, $y_2(t) = \\frac{ \\frac{1}{2} \\sin t - \\cos t + e^{-t/2}}{1.25}$\n\n- **Output Requirement**: The program must output a single line containing a comma-separated list of the final absolute errors (for scalar cases) or Euclidean norm of the error (for the vector case) at $T_{\\text{end}}$, rounded to $10$ decimal places, in the format `[err_A,err_B,err_C]`.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The problem describes a standard numerical method for solving ODEs, a fundamental task in computational science and engineering. The method for error estimation (comparing a first-order method with a second-order method) and the formula for adaptive step-size control are well-established techniques in numerical analysis. The test cases are standard in the literature and have known analytical solutions, which is appropriate for verification.\n- **Well-Posedness**: The problem is self-contained. It provides all necessary information: the algorithm, all parameters, initial conditions, time intervals, and the exact form of the ODEs to be solved. The criteria for termination and output formatting are clearly defined.\n- **Objectivity**: The problem is stated in precise mathematical and algorithmic language, free of ambiguity, subjectivity, or non-scientific claims.\n\nThe problem does not violate any of the invalidity criteria. It is a well-defined computational task.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Principle-Based Solution\n\nThe task is to implement an adaptive-step integration scheme for initial value problems of the form $y'(t) = f(t, y)$ with $y(t_0) = y_0$. The core of the method is the first-order Forward Euler method, enhanced with a mechanism for automatic step-size adjustment to control the local error.\n\nThe Forward Euler method approximates the solution at time $t_{n+1} = t_n + h$ using the Taylor expansion truncated after the first-order term:\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\nThe local truncation error of this method is of order $O(h^2)$.\n\nTo adapt the step size $h$, we need an estimate of this local error. The problem specifies a common technique where the result from a single step of size $h$ is compared with the result from two successive steps of size $h/2$.\n\nLet $y_n$ be the solution at time $t_n$.\n1.  We compute an approximation to $y(t_n+h)$ using one Forward Euler step of size $h$:\n    $$\n    y^{[h]} = y_n + h f(t_n, y_n)\n    $$\n2.  We compute another approximation by taking two consecutive half-steps:\n    First half-step from $t_n$ to $t_n+h/2$:\n    $$\n    y_{n+1/2} = y_n + \\frac{h}{2} f(t_n, y_n)\n    $$\n    Second half-step from $t_n+h/2$ to $t_n+h$:\n    $$\n    y^{[h/2]} = y_{n+1/2} + \\frac{h}{2} f(t_n + \\frac{h}{2}, y_{n+1/2})\n    $$\n    Substituting $y_{n+1/2}$ into the second equation yields the formula for $y^{[h/2]}$ as given in the problem statement. This two-step process is equivalent to a single step of the second-order Runge-Kutta method known as the explicit midpoint rule, which has a local truncation error of $O(h^3)$.\n\nThe difference between these two approximations, $e = \\|y^{[h/2]} - y^{[h]}\\|$, serves as an estimate for the error of the lower-order method, $y^{[h]}$. Specifically, since the exact solution $y_{\\text{exact}}(t_n+h)$ can be written as $y_{\\text{exact}} \\approx y^{[h]} + C h^2$ and $y_{\\text{exact}} \\approx y^{[h/2]} + O(h^3)$, the difference $e$ is a good proxy for the term $C h^2$.\n\nWith this error estimate, we apply step-size control. For a given tolerance $\\epsilon$, a step is successful if $e \\le \\epsilon$. If so, we advance the solution using the more accurate approximation: $t_{n+1} = t_n + h$ and $y_{n+1} = y^{[h/2]}$. This is a form of local extrapolation. If the step is rejected ($e > \\epsilon$), we must retry from $t_n$ with a smaller step size.\n\nThe new step size, $h_{\\text{new}}$, is determined with the goal of making the error in the next step approximately equal to $\\epsilon$. Since $e \\approx C h^2$, we desire $h_{\\text{new}}$ such that $\\epsilon \\approx C h_{\\text{new}}^2$. This leads to the relation $h_{\\text{new}} \\approx h (\\epsilon/e)^{1/2}$. The problem provides a more robust control formula that includes a safety factor $\\sigma$ and clips the step-size change factor to prevent overly aggressive adjustments:\n$$\nh_{\\text{new}} = h \\cdot \\sigma \\cdot \\mathrm{clip}\\!\\left(\\left(\\frac{\\epsilon}{\\max(e, e_{\\min})}\\right)^{1/2}, \\gamma_{\\min}, \\gamma_{\\max}\\right)\n$$\nThis update is performed after every attempt, whether it was accepted or rejected, using the attempted step size $h$ and the resulting error $e$.\n\nThe overall algorithm is implemented as a loop that advances the solution from the initial time $t_0$ to the final time $T_{\\text{end}}$, adjusting the step size $h$ at each iteration based on the estimated local error. The implementation will handle both scalar and vector ODEs and apply the specified constraints on the step size. Finally, the accuracy of the method is evaluated by comparing the computed solution at $T_{\\text{end}}$ to the provided exact analytical solution.", "answer": "```python\nimport numpy as np\n\ndef adaptive_euler_solver(f, t_span, y0, h0, eps, params):\n    \"\"\"\n    Solves an IVP using Forward Euler with adaptive step-size control.\n    \"\"\"\n    t_start, t_end = t_span\n    t = t_start\n    # Ensure y is a numpy array for consistent vector/scalar operations\n    y = np.array(y0, dtype=np.float64)\n    h = h0\n\n    sigma = params['sigma']\n    e_min = params['e_min']\n    gamma_min = params['gamma_min']\n    gamma_max = params['gamma_max']\n    h_min = params['h_min']\n\n    while t < t_end:\n        # Prevent floating point precision issues near the end\n        if np.isclose(t, t_end):\n            break\n        \n        # Adjust step size to not overshoot T_end and respect h_min\n        h_try = min(h, t_end - t)\n        h_try = max(h_try, h_min)\n\n        # One full step (Forward Euler)\n        # y_h = y + h_try * f(t, y)\n        k1 = f(t, y)\n        y_h = y + h_try * k1\n\n        # Two half-steps (Explicit Midpoint)\n        # y_h_half = y + h/2*f + h/2*f(t+h/2, y+h/2*f)\n        y_mid = y + (h_try / 2.0) * k1\n        k2 = f(t + h_try / 2.0, y_mid)\n        y_h_half = y_mid + (h_try / 2.0) * k2\n        \n        # Error estimate\n        error_vec = y_h_half - y_h\n        e = np.linalg.norm(error_vec)\n\n        # Step acceptance/rejection\n        if e <= eps:\n            # Accept step and advance solution with the more accurate result\n            t = t + h_try\n            y = y_h_half\n\n        # Update step size for the next attempt (always)\n        ratio = eps / max(e, e_min)\n        factor = np.clip(np.sqrt(ratio), gamma_min, gamma_max)\n        h = h_try * sigma * factor\n        \n    return y\n\ndef solve():\n    \"\"\"\n    Sets up and solves the test cases, then prints the final errors.\n    \"\"\"\n    # Universal parameters for the solver\n    solver_params = {\n        'sigma': 0.9,\n        'e_min': 1e-16,\n        'gamma_min': 0.2,\n        'gamma_max': 5.0,\n        'h_min': 1e-12,\n    }\n\n    # Test Case A\n    def f_a(t, y):\n        return -y[0]**2\n    def y_exact_a(t):\n        return np.array([1.0 / (1.0 + t)])\n\n    # Test Case B\n    def f_b(t, y):\n        return np.cos(t) - y[0]\n    def y_exact_b(t):\n        return np.array([(np.sin(t) + np.cos(t) - np.exp(-t)) / 2.0])\n\n    # Test Case C\n    def f_c(t, y):\n        dy1_dt = -2.0 * y[0]\n        dy2_dt = -0.5 * y[1] + np.sin(t)\n        return np.array([dy1_dt, dy2_dt])\n    def y_exact_c(t):\n        y1 = np.exp(-2.0 * t)\n        y2 = (0.5 * np.sin(t) - np.cos(t) + np.exp(-t / 2.0)) / 1.25\n        return np.array([y1, y2])\n\n    test_cases = [\n        {\n            'f': f_a, 'y_exact': y_exact_a, 't_span': [0.0, 1.0],\n            'y0': [1.0], 'h0': 0.4, 'eps': 1e-6\n        },\n        {\n            'f': f_b, 'y_exact': y_exact_b, 't_span': [0.0, 3.0],\n            'y0': [0.0], 'h0': 0.3, 'eps': 1e-5\n        },\n        {\n            'f': f_c, 'y_exact': y_exact_c, 't_span': [0.0, 2.0],\n            'y0': [1.0, 0.0], 'h0': 0.2, 'eps': 1e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        t_end = case['t_span'][1]\n        \n        y_final = adaptive_euler_solver(\n            case['f'], case['t_span'], case['y0'],\n            case['h0'], case['eps'], solver_params\n        )\n        \n        y_true = case['y_exact'](t_end)\n        error = np.linalg.norm(y_final - y_true)\n        results.append(error)\n\n    # Format output as specified\n    formatted_results = [f\"{res:.10f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2395159"}]}