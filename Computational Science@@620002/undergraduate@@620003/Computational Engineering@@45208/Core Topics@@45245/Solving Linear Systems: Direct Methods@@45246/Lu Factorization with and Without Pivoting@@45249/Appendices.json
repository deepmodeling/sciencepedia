{"hands_on_practices": [{"introduction": "Mastering a new computational method begins with the fundamentals. This first exercise is designed to build your proficiency with the LU factorization process. By applying the standard algorithm without pivoting [@problem_id:1029988], you will gain a concrete understanding of how a matrix $A$ is systematically decomposed into its lower ($L$) and upper ($U$) triangular components, a foundational skill for the more advanced topics to follow.", "problem": "Consider the 3×3 matrix $A$ given by:\n\n$$\nA = \\begin{bmatrix}\n1 & 3 & 2 \\\\\n4 & 2 & 1 \\\\\n3 & 1 & 2\n\\end{bmatrix}.\n$$\n\nCompute the LU decomposition of $A$ without pivoting, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. Provide the element $u_{33}$ (the entry in the third row and third column) of the matrix $U$.", "solution": "We seek $A=LU$ with \n$$L=\\begin{bmatrix}1&0&0\\\\l_{21}&1&0\\\\l_{31}&l_{32}&1\\end{bmatrix},\\quad\nU=\\begin{bmatrix}u_{11}&u_{12}&u_{13}\\\\0&u_{22}&u_{23}\\\\0&0&u_{33}\\end{bmatrix}.$$\n\nStep 1: First row of $U$ from $A$:\n$$u_{11}=a_{11}=1,\\quad u_{12}=a_{12}=3,\\quad u_{13}=a_{13}=2.$$\n\nStep 2: First column of $L$ below the diagonal:\n$$l_{21}=\\frac{a_{21}}{u_{11}}=\\frac{4}{1}=4,\\quad\nl_{31}=\\frac{a_{31}}{u_{11}}=\\frac{3}{1}=3.$$\n\nStep 3: Second row of $U$:\n$$u_{22}=a_{22}-l_{21}u_{12}=2-4\\cdot3=2-12=-10,$$\n$$u_{23}=a_{23}-l_{21}u_{13}=1-4\\cdot2=1-8=-7.$$\n\nStep 4: Subdiagonal element $l_{32}$:\n$$l_{32}=\\frac{a_{32}-l_{31}u_{12}}{u_{22}}\n=\\frac{1-3\\cdot3}{-10}=\\frac{1-9}{-10}=\\frac{-8}{-10}=\\frac{4}{5}.$$\n\nStep 5: Finally, $u_{33}$:\n$$u_{33}=a_{33}-l_{31}u_{13}-l_{32}u_{23}\n=2-3\\cdot2-\\frac{4}{5}\\,(-7)\n=2-6+\\frac{28}{5}\n=-4+\\frac{28}{5}\n=\\frac{-20+28}{5}\n=\\frac{8}{5}.$$", "answer": "$$\\boxed{\\frac{8}{5}}$$", "id": "1029988"}, {"introduction": "While powerful, the basic LU factorization algorithm has a critical weakness that pivoting is designed to solve. This problem explores a specific scenario where the naive algorithm fails, not because the matrix is singular, but due to a zero in a pivot position. By analyzing this well-posed yet problematic case [@problem_id:2407904], you will uncover the fundamental reason why pivoting is not just an optimization but a crucial component for achieving numerical stability.", "problem": "An economist is modeling a stylized network of cross-exposures among $3$ financial sectors where each sector’s endogenous funding cost depends only on the other sectors’ short-term rates, not on its own. This deliberate modeling choice corresponds to an extreme case of cross-dependence with zero own-effect, which can arise under a numeraire normalization or a binding no-self-lending rule. The resulting linear system is represented as $A \\mathbf{x} = \\mathbf{b}$, with\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix},\n$$\nwhere the zero diagonal entries encode the absence of direct self-dependence and the off-diagonal ones encode symmetric cross-dependence. \n\nStarting from the core definitions of Gaussian elimination and Lower-Upper (LU) decomposition, explain why a naive LU factorization without pivoting fails for $A$ even though the underlying system is well-posed. Then, using only fundamental properties of determinants and elimination, determine the exact value of $\\det(A)$. Express your final answer in exact form with no rounding.", "solution": "The problem asks for two things: first, an explanation for the failure of a naive Lower-Upper ($LU$) decomposition for the given matrix $A$, and second, the calculation of the determinant of $A$ using fundamental principles.\n\nThe matrix in question is:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n$$\n\n**Part 1: Failure of Naive LU Decomposition without Pivoting**\n\nThe $LU$ decomposition of a matrix $A$ is the process of finding a lower triangular matrix $L$ and an upper triangular matrix $U$ such that $A = LU$. Typically, $L$ is constrained to be unit lower triangular, meaning its diagonal elements are all $1$. The matrix $U$ is found through the process of Gaussian elimination, and the matrix $L$ stores the multipliers used to create zeros in the lower part of $A$.\n\nThe Gaussian elimination algorithm proceeds by using the diagonal elements, called pivots, to eliminate the non-zero entries below them in each column. Let us attempt this for matrix $A$.\n\nThe first step is to use the pivot element $a_{11}$ to eliminate the elements $a_{21}$ and $a_{31}$. The standard row operations are:\n$R_2 \\rightarrow R_2 - l_{21} R_1$\n$R_3 \\rightarrow R_3 - l_{31} R_1$\nwhere the multipliers are $l_{21} = \\frac{a_{21}}{a_{11}}$ and $l_{31} = \\frac{a_{31}}{a_{11}}$.\n\nIn our matrix $A$, the first pivot element is $a_{11} = 0$. The calculation of the first multiplier, $l_{21}$, requires the operation:\n$$\nl_{21} = \\frac{a_{21}}{a_{11}} = \\frac{1}{0}\n$$\nThis operation constitutes division by zero, which is an undefined mathematical operation. Consequently, the standard Gaussian elimination algorithm cannot proceed past the first step. Since the naive $LU$ factorization algorithm is a direct implementation of this elimination process, it fails at the outset. This failure occurs precisely because the pivot element is zero. A remedy for this issue in practice is pivoting, which involves row interchanges to place a non-zero element in the pivot position. The problem specifies a \"naive\" factorization, which by definition excludes such pivoting strategies.\n\nThus, even though the system $A\\mathbf{x}=\\mathbf{b}$ is well-posed (which we will confirm by showing $\\det(A) \\neq 0$), the naive computational algorithm for $LU$ decomposition fails due to the presence of a zero on the diagonal in a pivot position.\n\n**Part 2: Calculation of $\\det(A)$ using Fundamental Properties**\n\nThe problem requires a determination of $\\det(A)$ using the properties of determinants under elementary row operations. The key properties are:\n1.  Swapping two rows of a matrix multiplies its determinant by $-1$.\n2.  Adding a multiple of one row to another row does not change the determinant.\n3.  The determinant of an upper triangular matrix is the product of its diagonal elements.\n\nWe will transform $A$ into an upper triangular form, keeping track of how the determinant changes.\n$$\nA = \\begin{pmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n$$\nAs established, the pivot $a_{11} = 0$ is problematic. We perform a row swap, for instance $R_1 \\leftrightarrow R_2$. According to property $1$, this introduces a factor of $-1$.\n$$\n\\det(A) = - \\det \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n$$\nThe new pivot is $1$, which is non-zero. The element $a_{21}$ is already $0$. We proceed to eliminate $a_{31} = 1$ by performing the row operation $R_3 \\rightarrow R_3 - R_1$. According to property $2$, this operation does not change the determinant.\n$$\n\\det(A) = - \\det \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1-1 & 1-0 & 0-1\n\\end{pmatrix}\n= - \\det \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n$$\nNow, we use the second pivot, $a_{22} = 1$, to eliminate the element $a_{32} = 1$. We perform the row operation $R_3 \\rightarrow R_3 - R_2$. Again, this does not alter the determinant.\n$$\n\\det(A) = - \\det \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0-0 & 1-1 & -1-1\n\\end{pmatrix}\n= - \\det \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & -2\n\\end{pmatrix}\n$$\nThe resulting matrix is an upper triangular matrix, let us call it $U$. According to property $3$, its determinant is the product of its diagonal elements:\n$$\n\\det(U) = (1)(1)(-2) = -2\n$$\nTherefore, the determinant of the original matrix $A$ is:\n$$\n\\det(A) = - \\det(U) = -(-2) = 2\n$$\nSince $\\det(A) = 2 \\neq 0$, the matrix $A$ is invertible, and the linear system $A\\mathbf{x}=\\mathbf{b}$ has a unique solution for any vector $\\mathbf{b}$. This confirms the problem's own assertion that the underlying system is well-posed, despite the failure of the naive factorization algorithm.", "answer": "$$\\boxed{2}$$", "id": "2407904"}, {"introduction": "Once you have the $PA=LU$ factorization of a matrix, what can you do with it? This practice moves beyond computing the factors to applying them for their primary purpose: efficiently solving linear systems. You will derive and implement a procedure to solve the transposed system $A^{\\top} x = b$ using only the $P$, $L$, and $U$ factors, demonstrating the true power and flexibility of this decomposition [@problem_id:2410733].", "problem": "In computational engineering, the Lower–Upper (LU) factorization with row pivoting represents a matrix by a permutation and triangular factors, enabling efficient solving of linear systems. Suppose you are given a permutation matrix $P \\in \\mathbb{R}^{n \\times n}$, a unit lower-triangular matrix $L \\in \\mathbb{R}^{n \\times n}$, and an upper-triangular matrix $U \\in \\mathbb{R}^{n \\times n}$ such that $P A = L U$ for an unknown nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$. Using only fundamental properties of permutation matrices, triangular matrices, and transposition, derive an algorithm to solve the transposed system $A^{\\top} x = b$ without explicitly forming $A$ or $A^{\\top}$. Your derivation must start from the defining relation $P A = L U$ and the identities $P^{\\top} = P^{-1}$ and $\\left(X Y\\right)^{\\top} = Y^{\\top} X^{\\top}$, and must justify the order and type of triangular solves used.\n\nThen apply your derived algorithm to the concrete data below and compute the exact solution vector $x$:\n$$\nP = \\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 0\n\\end{pmatrix}, \\quad\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n2 & 1 & 0 \\\\\n-1 & 3 & 1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\n2 & -1 & 1 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & 5\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}.\n$$\nDo not explicitly form $A$ or $A^{\\top}$ in your computations; use only $P$, $L$, $U$, and $b$. Express your final answer as a single row vector with exact rational entries. No rounding is required.", "solution": "**Derivation of the Algorithm**\nThe objective is to solve the linear system $A^{\\top} x = b$ for the unknown vector $x$. The starting point is the given factorization $P A = L U$.\n\nFirst, we must express $A^{\\top}$ in terms of the given factors $P$, $L$, and $U$. From the initial equation, we isolate $A$:\n$$ P A = L U $$\nSince $P$ is a permutation matrix, its inverse is equal to its transpose, $P^{-1} = P^{\\top}$. Left-multiplying by $P^{\\top}$ yields:\n$$ P^{\\top} P A = P^{\\top} L U $$\n$$ I A = P^{\\top} L U $$\n$$ A = P^{\\top} L U $$\nNow, we take the transpose of this expression for $A$. Using the property $(XYZ)^{\\top} = Z^{\\top}Y^{\\top}X^{\\top}$, we obtain:\n$$ A^{\\top} = (P^{\\top} L U)^{\\top} = U^{\\top} L^{\\top} (P^{\\top})^{\\top} $$\nSince $(P^{\\top})^{\\top} = P$, the expression for $A^{\\top}$ simplifies to:\n$$ A^{\\top} = U^{\\top} L^{\\top} P $$\nSubstitute this into the system $A^{\\top} x = b$:\n$$ (U^{\\top} L^{\\top} P) x = b $$\nTo solve this system for $x$, we proceed by introducing intermediate vectors. This avoids matrix-matrix multiplications and allows us to solve a sequence of simpler systems. Let us define a vector $y$ as:\n$$ y = P x $$\nSubstituting this into the equation gives:\n$$ U^{\\top} L^{\\top} y = b $$\nNext, let us define a vector $z$ as:\n$$ z = L^{\\top} y $$\nSubstituting this gives:\n$$ U^{\\top} z = b $$\nThis sequence of substitutions defines a three-step algorithm for finding $x$:\n\n1.  **Solve for $z$**: Solve the system $U^{\\top} z = b$. Since $U$ is an upper-triangular matrix, its transpose $U^{\\top}$ is a lower-triangular matrix. This system is therefore solved efficiently using **forward substitution**.\n2.  **Solve for $y$**: Solve the system $L^{\\top} y = z$. Since $L$ is a unit lower-triangular matrix, its transpose $L^{\\top}$ is a unit upper-triangular matrix. This system is solved efficiently using **backward substitution**.\n3.  **Solve for $x$**: From the definition $y = P x$, we find $x$ by applying the inverse of $P$: $x = P^{-1} y$. Using the property $P^{-1} = P^{\\top}$, we get $x = P^{\\top} y$. This final step is a simple permutation of the elements of vector $y$.\n\nThis algorithm solves for $x$ without ever explicitly computing the matrix $A$ or its transpose $A^{\\top}$.\n\n**Application to Provided Data**\nWe now apply this algorithm to the given matrices and vector.\nThe provided data are:\n$$ P = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}, \\quad L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & 3 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & 3 & 4 \\\\ 0 & 0 & 5 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nWe require the transposes of $U$, $L$, and $P$:\n$$ U^{\\top} = \\begin{pmatrix} 2 & 0 & 0 \\\\ -1 & 3 & 0 \\\\ 1 & 4 & 5 \\end{pmatrix}, \\quad L^{\\top} = \\begin{pmatrix} 1 & 2 & -1 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad P^{\\top} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} $$\n**Step 1: Solve $U^{\\top} z = b$ by forward substitution.**\n$$ \\begin{pmatrix} 2 & 0 & 0 \\\\ -1 & 3 & 0 \\\\ 1 & 4 & 5 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nFrom the first row: $2z_1 = 1 \\implies z_1 = \\frac{1}{2}$.\nFrom the second row: $-z_1 + 3z_2 = 2 \\implies -(\\frac{1}{2}) + 3z_2 = 2 \\implies 3z_2 = \\frac{5}{2} \\implies z_2 = \\frac{5}{6}$.\nFrom the third row: $z_1 + 4z_2 + 5z_3 = 3 \\implies \\frac{1}{2} + 4(\\frac{5}{6}) + 5z_3 = 3 \\implies \\frac{1}{2} + \\frac{10}{3} + 5z_3 = 3 \\implies \\frac{23}{6} + 5z_3 = 3 \\implies 5z_3 = 3 - \\frac{23}{6} = -\\frac{5}{6} \\implies z_3 = -\\frac{1}{6}$.\nSo, the intermediate vector $z$ is $z = \\begin{pmatrix} 1/2 \\\\ 5/6 \\\\ -1/6 \\end{pmatrix}$.\n\n**Step 2: Solve $L^{\\top} y = z$ by backward substitution.**\n$$ \\begin{pmatrix} 1 & 2 & -1 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 5/6 \\\\ -1/6 \\end{pmatrix} $$\nFrom the third row: $y_3 = -\\frac{1}{6}$.\nFrom the second row: $y_2 + 3y_3 = \\frac{5}{6} \\implies y_2 + 3(-\\frac{1}{6}) = \\frac{5}{6} \\implies y_2 - \\frac{1}{2} = \\frac{5}{6} \\implies y_2 = \\frac{5}{6} + \\frac{3}{6} = \\frac{8}{6} = \\frac{4}{3}$.\nFrom the first row: $y_1 + 2y_2 - y_3 = \\frac{1}{2} \\implies y_1 + 2(\\frac{4}{3}) - (-\\frac{1}{6}) = \\frac{1}{2} \\implies y_1 + \\frac{8}{3} + \\frac{1}{6} = \\frac{1}{2} \\implies y_1 + \\frac{16}{6} + \\frac{1}{6} = \\frac{3}{6} \\implies y_1 + \\frac{17}{6} = \\frac{3}{6} \\implies y_1 = \\frac{3-17}{6} = -\\frac{14}{6} = -\\frac{7}{3}$.\nSo, the intermediate vector $y$ is $y = \\begin{pmatrix} -7/3 \\\\ 4/3 \\\\ -1/6 \\end{pmatrix}$.\n\n**Step 3: Compute $x = P^{\\top} y$.**\n$$ x = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} -7/3 \\\\ 4/3 \\\\ -1/6 \\end{pmatrix} = \\begin{pmatrix} -1/6 \\\\ 4/3 \\\\ -7/3 \\end{pmatrix} $$\nThe final solution vector $x$ contains exact rational entries as required.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{1}{6} & \\frac{4}{3} & -\\frac{7}{3}\n\\end{pmatrix}\n}\n$$", "id": "2410733"}]}