{"hands_on_practices": [{"introduction": "The theoretical study of linear algebra often operates in an idealized world of infinite precision. However, real-world computational engineering relies on finite-precision floating-point arithmetic, where every calculation introduces a tiny rounding error. This exercise demonstrates the profound and often surprising impact of these small errors when dealing with an ill-conditioned system, showing how different levels of computational precision can lead to vastly different answers for the same problem. By manually performing Gaussian elimination, you will gain a first-hand appreciation for how ill-conditioning amplifies numerical noise. [@problem_id:2203807]", "problem": "In computational science, a common task is to solve a system of linear equations of the form $Ax = b$. However, digital computers use finite-precision floating-point arithmetic, which can introduce errors, especially when the system is ill-conditioned (i.e., small changes in the input data can lead to large changes in the solution).\n\nConsider the following ill-conditioned linear system $Ax=b$:\n$$\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1.00001\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\\n1.0000066667\n\\end{pmatrix}\n$$\nThe vector $b$ was constructed such that the exact analytical solution to this system is known to be $x_{\\text{true}} = \\begin{pmatrix} 1/3 \\\\ 2/3 \\end{pmatrix}$.\n\nYour task is to investigate the effect of numerical precision on the solution. You will solve this system using Gaussian elimination without pivoting under two different simulated floating-point arithmetic models:\n\n1.  **Single Precision**: All initial values, intermediate calculations, and final results are rounded to 6 significant figures.\n2.  **Double Precision**: All initial values, intermediate calculations, and final results are rounded to 12 significant figures.\n\nAfter obtaining the numerical solutions $x_{\\text{single}}$ and $x_{\\text{double}}$, calculate the relative error for each solution with respect to the true solution $x_{\\text{true}}$. The relative error is defined using the infinity norm ($\\| \\cdot \\|_{\\infty}$) as:\n$$\nE = \\frac{\\|x_{\\text{computed}} - x_{\\text{true}}\\|_{\\infty}}{\\|x_{\\text{true}}\\|_{\\infty}}\n$$\nwhere for a vector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$, the infinity norm is $\\|v\\|_{\\infty} = \\max(|v_1|, |v_2|)$.\n\nDetermine the relative error for the single-precision case, $E_{\\text{single}}$, and the relative error for the double-precision case, $E_{\\text{double}}$. Provide your answers for $E_{\\text{single}}$ and $E_{\\text{double}}$, in that order, each rounded to three significant figures.", "solution": "We solve the system by Gaussian elimination without pivoting, rounding all stored values after every arithmetic operation to the specified number of significant figures.\n\nGiven\n$$\nA=\\begin{pmatrix}1 & 1\\\\ 1 & 1.00001\\end{pmatrix},\\quad\nb=\\begin{pmatrix}1\\\\ 1.0000066667\\end{pmatrix},\\quad\nx_{\\text{true}}=\\begin{pmatrix}\\frac{1}{3}\\\\ \\frac{2}{3}\\end{pmatrix}.\n$$\n\nSingle precision (6 significant figures):\n- Round initial data to 6 significant figures:\n$$\na_{11}=1,\\ a_{12}=1,\\ a_{21}=1,\\ a_{22}=1.00001,\\ b_{1}=1,\\ b_{2}=\\operatorname{round}_{6\\text{sf}}(1.0000066667)=1.00001.\n$$\n- Elimination multiplier:\n$$\nm_{21}=\\operatorname{round}_{6\\text{sf}}\\!\\left(\\frac{a_{21}}{a_{11}}\\right)=\\operatorname{round}_{6\\text{sf}}(1)=1.\n$$\n- Row update for row 2:\n$$\na_{21}'=\\operatorname{round}_{6\\text{sf}}(a_{21}-m_{21}a_{11})=\\operatorname{round}_{6\\text{sf}}(0)=0,\n$$\n$$\na_{22}'=\\operatorname{round}_{6\\text{sf}}(a_{22}-m_{21}a_{12})=\\operatorname{round}_{6\\text{sf}}(1.00001-1)=\\operatorname{round}_{6\\text{sf}}(0.00001)=0.00001,\n$$\n$$\nb_{2}'=\\operatorname{round}_{6\\text{sf}}(b_{2}-m_{21}b_{1})=\\operatorname{round}_{6\\text{sf}}(1.00001-1)=0.00001.\n$$\n- Back substitution:\n$$\nx_{2}=\\operatorname{round}_{6\\text{sf}}\\!\\left(\\frac{b_{2}'}{a_{22}'}\\right)=\\operatorname{round}_{6\\text{sf}}\\!\\left(\\frac{0.00001}{0.00001}\\right)=1,\n$$\n$$\nx_{1}=\\operatorname{round}_{6\\text{sf}}\\!\\left(\\frac{b_{1}-a_{12}x_{2}}{a_{11}}\\right)=\\operatorname{round}_{6\\text{sf}}(1-1)=0.\n$$\nThus $x_{\\text{single}}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. The relative error in the infinity norm is\n$$\n\\|x_{\\text{single}}-x_{\\text{true}}\\|_{\\infty}=\\max\\!\\left(\\left|0-\\frac{1}{3}\\right|,\\left|1-\\frac{2}{3}\\right|\\right)=\\frac{1}{3},\n$$\n$$\n\\|x_{\\text{true}}\\|_{\\infty}=\\max\\!\\left(\\left|\\frac{1}{3}\\right|,\\left|\\frac{2}{3}\\right|\\right)=\\frac{2}{3},\n$$\n$$\nE_{\\text{single}}=\\frac{\\frac{1}{3}}{\\frac{2}{3}}=\\frac{1}{2}=0.5.\n$$\n\nDouble precision (12 significant figures):\n- Round initial data to 12 significant figures:\n$$\na_{11}=1,\\ a_{12}=1,\\ a_{21}=1,\\ a_{22}=1.00001,\\ b_{1}=1,\\ b_{2}=1.0000066667.\n$$\n- Elimination multiplier:\n$$\nm_{21}=\\operatorname{round}_{12\\text{sf}}(1)=1.\n$$\n- Row update for row 2:\n$$\na_{21}'=\\operatorname{round}_{12\\text{sf}}(0)=0,\\quad\na_{22}'=\\operatorname{round}_{12\\text{sf}}(1.00001-1)=\\operatorname{round}_{12\\text{sf}}(0.00001)=0.00001,\n$$\n$$\nb_{2}'=\\operatorname{round}_{12\\text{sf}}(1.0000066667-1)=\\operatorname{round}_{12\\text{sf}}(0.0000066667)=0.0000066667.\n$$\n- Back substitution:\n$$\nx_{2}=\\operatorname{round}_{12\\text{sf}}\\!\\left(\\frac{0.0000066667}{0.00001}\\right)=\\operatorname{round}_{12\\text{sf}}(0.66667)=0.66667=\\frac{66667}{100000},\n$$\n$$\nx_{1}=\\operatorname{round}_{12\\text{sf}}(1-0.66667)=\\operatorname{round}_{12\\text{sf}}(0.33333)=0.33333=\\frac{33333}{100000}.\n$$\nThus $x_{\\text{double}}=\\begin{pmatrix}\\frac{33333}{100000}\\\\ \\frac{66667}{100000}\\end{pmatrix}$. The infinity-norm error is\n$$\n\\|x_{\\text{double}}-x_{\\text{true}}\\|_{\\infty}\n=\\max\\!\\left(\\left|\\frac{33333}{100000}-\\frac{1}{3}\\right|,\\left|\\frac{66667}{100000}-\\frac{2}{3}\\right|\\right)\n=\\max\\!\\left(\\frac{1}{300000},\\frac{1}{300000}\\right)=\\frac{1}{300000}.\n$$\nTherefore\n$$\nE_{\\text{double}}=\\frac{\\frac{1}{300000}}{\\frac{2}{3}}=\\frac{1}{300000}\\cdot\\frac{3}{2}=\\frac{1}{200000}=5\\times 10^{-6}.\n$$\n\nFinally, rounding each to three significant figures gives\n$$\nE_{\\text{single}}=0.500,\\quad E_{\\text{double}}=5.00\\times 10^{-6}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.500 & 5.00 \\times 10^{-6}\\end{pmatrix}}$$", "id": "2203807"}, {"introduction": "Having seen how ill-conditioning can amplify errors, the next step is to diagnose its origin. Is the problem fundamental, or is it an artifact of how we've set up our equations? This exercise tackles this crucial question by presenting two ill-conditioned systems that are problematic for different reasons. Your task is to distinguish between ill-conditioning that arises from poor scaling of variables—which can often be cured with a simple change of basis—and ill-conditioning that is intrinsic to the underlying physical or mathematical problem itself. [@problem_id:2400703]", "problem": "In computational engineering practice, linear systems can be numerically ill-conditioned either because the variables are poorly scaled or because the underlying operator is intrinsically nearly singular. Consider two square linear systems with unknown vector $x \\in \\mathbb{R}^2$ and given right-hand side $b \\in \\mathbb{R}^2$:\nSystem $\\mathrm{S1}$: $A_1 x = b$ with\n$$\nA_1 = Q D, \\quad Q = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ -1 & 1 \\end{bmatrix}, \\quad D = \\begin{bmatrix} \\alpha & 0 \\\\ 0 & 1/\\alpha \\end{bmatrix}, \\quad \\alpha = 10^6.\n$$\nSystem $\\mathrm{S2}$: $A_2 x = b$ with\n$$\nA_2 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1+\\epsilon \\end{bmatrix}, \\quad \\epsilon = 10^{-8}.\n$$\nYou may use only the following foundational facts:\n- The matrix $2$-norm is defined by $\\|A\\|_2 = \\max_{\\|x\\|_2 = 1} \\|A x\\|_2$. The $2$-norm condition number is $\\kappa_2(A) = \\|A\\|_2 \\,\\|A^{-1}\\|_2$ for nonsingular $A$.\n- Orthogonal matrices $Q$ satisfy $Q^\\top Q = I$ and preserve the Euclidean norm: $\\|Q x\\|_2 = \\|x\\|_2$ for all $x$.\n- A change of basis in the unknowns of the form $x = T y$ with nonsingular $T$ produces the equivalent system $(A T) y = b$; after solving for $y$, one recovers $x = T y$. The mapping from $b$ to the original $x$ remains $x = A^{-1} b$.\n- For symmetric positive definite matrices, singular values equal eigenvalues.\n\nWhich of the following statements are true? Select all that apply.\n\nA. In $\\mathrm{S1}$, $\\kappa_2(A_1)$ is large, but there exists a simple diagonal change of variables that yields a transformed system matrix with $\\kappa_2$ equal to $1$ while leaving the mapping $b \\mapsto x$ unchanged.\n\nB. In $\\mathrm{S2}$, any orthogonal change of basis applied to the unknowns cannot reduce the $2$-norm condition number of the coefficient matrix.\n\nC. In $\\mathrm{S2}$, there exists a nonsingular change of basis in the unknowns that makes the coefficient matrix have $\\kappa_2 = \\mathcal{O}(1)$, and this would imply the original mapping $b \\mapsto x$ becomes well-conditioned in the Euclidean norm.\n\nD. In $\\mathrm{S1}$, even if one finds a change of variables that makes the transformed coefficient matrix perfectly conditioned, the sensitivity of the original solution $x$ to perturbations in $b$ measured in relative Euclidean norms remains proportional to $\\kappa_2(A_1)$.\n\nE. In $\\mathrm{S2}$, $\\kappa_2(A_2) \\approx 4/\\epsilon$ for small $\\epsilon$, reflecting an inherent near-rank-one structure that cannot be eliminated by any unit-preserving (orthogonal) change of basis.", "solution": "The problem statement is scientifically grounded in the principles of numerical linear algebra, is well-posed, objective, and self-contained. The provided data and definitions are standard and consistent. Therefore, the problem is valid and we may proceed with a full analysis.\n\nThe core of this problem is the distinction between two types of ill-conditioning: one arising from poor scaling of variables, which can be remedied by a change of basis (preconditioning), and another arising from the intrinsic near-singularity of an operator, which cannot be fixed by a simple change of basis.\n\nFirst, we analyze System $\\mathrm{S1}$: $A_1 x = b$.\nThe matrix is $A_1 = Q D$, where $Q = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ -1 & 1 \\end{bmatrix}$ is an orthogonal matrix and $D = \\begin{bmatrix} \\alpha & 0 \\\\ 0 & 1/\\alpha \\end{bmatrix}$ is a diagonal matrix with $\\alpha = 10^6$.\nThe structure $A_1 = Q D$ is related to the singular value decomposition (SVD). Since $Q$ is orthogonal, the singular values of $A_1$ are the absolute values of the diagonal entries of $D$.\nThe singular values are $\\sigma_{\\max} = \\alpha = 10^6$ and $\\sigma_{\\min} = 1/\\alpha = 10^{-6}$.\nThe $2$-norm condition number is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. For any matrix, $\\|A\\|_2 = \\sigma_{\\max}$ and $\\|A^{-1}\\|_2 = 1/\\sigma_{\\min}$.\nThus, the condition number of $A_1$ is:\n$$ \\kappa_2(A_1) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{\\alpha}{1/\\alpha} = \\alpha^2 = (10^6)^2 = 10^{12} $$\nThis system is extremely ill-conditioned.\n\nSecond, we analyze System $\\mathrm{S2}$: $A_2 x = b$.\nThe matrix is $A_2 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1+\\epsilon \\end{bmatrix}$ with $\\epsilon = 10^{-8}$.\nThis matrix is symmetric. For a symmetric positive definite (SPD) matrix, the singular values are its eigenvalues. We check if $A_2$ is SPD. The trace is $\\text{tr}(A_2) = 1 + (1+\\epsilon) = 2+\\epsilon > 0$. The determinant is $\\det(A_2) = 1(1+\\epsilon) - 1^2 = \\epsilon > 0$. Since both are positive, the eigenvalues are positive, and the matrix is SPD.\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(A_2 - \\lambda I) = 0$:\n$$ (1-\\lambda)(1+\\epsilon-\\lambda) - 1 = 0 \\implies \\lambda^2 - (2+\\epsilon)\\lambda + \\epsilon = 0 $$\nThe roots are:\n$$ \\lambda = \\frac{2+\\epsilon \\pm \\sqrt{(2+\\epsilon)^2 - 4\\epsilon}}{2} = \\frac{2+\\epsilon \\pm \\sqrt{4+4\\epsilon+\\epsilon^2 - 4\\epsilon}}{2} = \\frac{2+\\epsilon \\pm \\sqrt{4+\\epsilon^2}}{2} $$\nFor small $\\epsilon$, we use the Taylor expansion $\\sqrt{4+\\epsilon^2} = 2\\sqrt{1+\\epsilon^2/4} \\approx 2(1 + \\frac{1}{2}\\frac{\\epsilon^2}{4}) = 2+\\frac{\\epsilon^2}{4}$.\nThe eigenvalues are approximately:\n$$ \\lambda_{\\max} \\approx \\frac{2+\\epsilon + (2+\\epsilon^2/4)}{2} = 2 + \\frac{\\epsilon}{2} + \\frac{\\epsilon^2}{8} \\approx 2 $$\n$$ \\lambda_{\\min} \\approx \\frac{2+\\epsilon - (2+\\epsilon^2/4)}{2} = \\frac{\\epsilon}{2} - \\frac{\\epsilon^2}{8} \\approx \\frac{\\epsilon}{2} $$\nThe condition number of $A_2$ is:\n$$ \\kappa_2(A_2) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\approx \\frac{2}{\\epsilon/2} = \\frac{4}{\\epsilon} = \\frac{4}{10^{-8}} = 4 \\times 10^8 $$\nThis system is also extremely ill-conditioned. This ill-conditioning arises because as $\\epsilon \\to 0$, $A_2$ approaches a singular matrix $\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$.\n\nNow we evaluate each statement.\n\n**A. In $\\mathrm{S1}$, $\\kappa_2(A_1)$ is large, but there exists a simple diagonal change of variables that yields a transformed system matrix with $\\kappa_2$ equal to $1$ while leaving the mapping $b \\mapsto x$ unchanged.**\nWe have established that $\\kappa_2(A_1) = 10^{12}$ is large. The ill-conditioning is due to the scaling matrix $D$. Let's introduce a diagonal change of variables $x=Ty$. The new system is $(A_1 T) y = b$. We seek a diagonal matrix $T$ that conditions the system. A logical choice is one that counteracts the effect of $D$. Let's choose $T = D^{-1} = \\begin{bmatrix} 1/\\alpha & 0 \\\\ 0 & \\alpha \\end{bmatrix}$. This is a simple diagonal matrix.\nThe new system matrix is $\\tilde{A}_1 = A_1 T = (Q D) D^{-1} = Q I = Q$.\nThe matrix $Q$ is orthogonal. For any orthogonal matrix, its singular values are all $1$. Thus, $\\|Q\\|_2=1$ and $\\|Q^{-1}\\|_2=1$. The condition number is $\\kappa_2(Q) = 1$. So we have found a change of variables that makes the new system perfectly conditioned.\nThe mapping from $b$ to $x$ is given by $x=A_1^{-1}b$. A change of variables is a technique to solve the system; it does not alter this fundamental relationship. After solving for $y = (A_1 T)^{-1} b$, one recovers $x$ via $x = T y = T (A_1 T)^{-1} b = T T^{-1} A_1^{-1} b = A_1^{-1} b$. The mapping is unchanged. The statement is entirely correct.\nVerdict: **Correct**.\n\n**B. In $\\mathrm{S2}$, any orthogonal change of basis applied to the unknowns cannot reduce the $2$-norm condition number of the coefficient matrix.**\nAn orthogonal change of basis in the unknowns has the form $x = Ty$, where $T$ is an orthogonal matrix ($T^\\top T = I$). The new coefficient matrix is $\\tilde{A}_2 = A_2 T$.\nThe $2$-norm of $\\tilde{A}_2$ is $\\|\\tilde{A}_2\\|_2 = \\|A_2 T\\|_2$. Since the $2$-norm is invariant under right-multiplication by an orthogonal matrix, $\\|A_2 T\\|_2 = \\|A_2\\|_2$.\nThe inverse is $\\tilde{A}_2^{-1} = (A_2 T)^{-1} = T^{-1} A_2^{-1} = T^\\top A_2^{-1}$. The norm is $\\|\\tilde{A}_2^{-1}\\|_2 = \\|T^\\top A_2^{-1}\\|_2$. The $2$-norm is also invariant under left-multiplication by an orthogonal matrix, so $\\|T^\\top A_2^{-1}\\|_2 = \\|A_2^{-1}\\|_2$.\nTherefore, the condition number of the transformed matrix is:\n$$ \\kappa_2(\\tilde{A}_2) = \\|\\tilde{A}_2\\|_2 \\|\\tilde{A}_2^{-1}\\|_2 = \\|A_2\\|_2 \\|A_2^{-1}\\|_2 = \\kappa_2(A_2) $$\nAn orthogonal change of basis does not change the $2$-norm condition number at all. Thus, it cannot reduce it.\nVerdict: **Correct**.\n\n**C. In $\\mathrm{S2}$, there exists a nonsingular change of basis in the unknowns that makes the coefficient matrix have $\\kappa_2 = \\mathcal{O}(1)$, and this would imply the original mapping $b \\mapsto x$ becomes well-conditioned in the Euclidean norm.**\nThe first part of the statement is true. For any nonsingular matrix $A$, one can choose the change of basis $x=Ty$ with $T=A^{-1}$. The new system matrix becomes $AT = A A^{-1} = I$. The identity matrix $I$ has $\\kappa_2(I)=1$, which is $\\mathcal{O}(1)$.\nThe second part claims this implies the original mapping $b \\mapsto x$ becomes well-conditioned. This is a fundamental misunderstanding. The mapping $b \\mapsto x$ is uniquely defined by the operator $A_2^{-1}$, as $x = A_2^{-1} b$. The sensitivity of this mapping to perturbations in $b$ is measured by $\\kappa_2(A_2)$, which we found to be large ($\\approx 4 \\times 10^8$). This high sensitivity is an intrinsic property of the problem $A_2 x = b$. A change of variables is a computational technique (a preconditioner) to find the solution $x$ more accurately in the presence of floating-point arithmetic errors. It makes the system for the *new* variable $y$ well-conditioned, but it does not change the conditioning of the original problem itself. The original mapping remains ill-conditioned. The statement's implication is false.\nVerdict: **Incorrect**.\n\n**D. In $\\mathrm{S1}$, even if one finds a change of variables that makes the transformed coefficient matrix perfectly conditioned, the sensitivity of the original solution $x$ to perturbations in $b$ measured in relative Euclidean norms remains proportional to $\\kappa_2(A_1)$.**\nThis statement correctly identifies the concept discussed in C. The sensitivity of the solution $x$ to perturbations $\\delta b$ in the data $b$ for the system $A_1 x = b$ is fundamentally governed by the condition number of $A_1$. The classical error bound is:\n$$ \\frac{\\|\\delta x\\|_2}{\\|x\\|_2} \\leq \\kappa_2(A_1) \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} $$\nThis inequality shows that the relative error in the solution can be as large as $\\kappa_2(A_1)$ times the relative error in the data. This property is inherent to the matrix $A_1$ and the mapping $A_1^{-1}$ that it defines. As we showed in A, we can find a change of variables that makes the transformed system matrix perfectly conditioned, which aids in the numerical computation of the solution. However, this does not alter the underlying sensitivity of the original variables $x$ to the input data $b$. That sensitivity is, and always will be, dictated by $\\kappa_2(A_1) = 10^{12}$. The statement is therefore correct.\nVerdict: **Correct**.\n\n**E. In $\\mathrm{S2}$, $\\kappa_2(A_2) \\approx 4/\\epsilon$ for small $\\epsilon$, reflecting an inherent near-rank-one structure that cannot be eliminated by any unit-preserving (orthogonal) change of basis.**\nOur analysis of System $\\mathrm{S2}$ showed that for small $\\epsilon$, $\\kappa_2(A_2) = \\lambda_{\\max}/\\lambda_{\\min} \\approx 2/(\\epsilon/2) = 4/\\epsilon$. The first part of the statement is correct.\nThe reason for this large condition number is that for $\\epsilon \\to 0$, the matrix $A_2$ approaches $\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$, which is a rank-1 (and thus singular) matrix. This property is what is meant by \"inherent near-rank-one structure\" or more accurately, near-rank-deficiency.\nThe final clause states this cannot be eliminated by a \"unit-preserving (orthogonal) change of basis\". As proven in the analysis for option B, an orthogonal transformation preserves the $2$-norm and thus the $2$-norm condition number. The large condition number of $A_2$ is a consequence of its singular value spectrum $(\\sigma_{\\max} \\approx 2, \\sigma_{\\min} \\approx \\epsilon/2)$, which is invariant under orthogonal transformations. This type of ill-conditioning is intrinsic to the operator $A_2$ and cannot be removed by simply rotating the coordinate system.\nVerdict: **Correct**.", "answer": "$$\\boxed{ABDE}$$", "id": "2400703"}, {"introduction": "In large-scale computational engineering, linear systems are frequently solved using iterative methods like the Generalized Minimal Residual (GMRES) method. For ill-conditioned systems, these methods can slow to a crawl or fail to converge entirely. This final practice moves from diagnosis to treatment, demonstrating how a technique called preconditioning can dramatically improve the performance of iterative solvers. By implementing and comparing an unpreconditioned and a preconditioned GMRES solver, you will see how a simple transformation of the system can make an otherwise intractable problem computationally feasible. [@problem_id:2400723]", "problem": "Consider solving linear systems of the form $A x = b$ for a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $b \\in \\mathbb{R}^{n}$ using the Generalized Minimal Residual method (GMRES). Let $x_0 = 0$ be the initial guess, and let the Krylov subspace of order $k$ be $ \\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{ r_0, A r_0, \\dots, A^{k-1} r_0 \\}$, where $r_0 = b - A x_0 = b$. The GMRES iterate $x_k$ is the vector in $x_0 + \\mathcal{K}_k(A,r_0)$ that minimizes the residual norm $\\| b - A x \\|_2$. Define the relative residual at iteration $k$ as $\\rho_k = \\| b - A x_k \\|_2 / \\|b\\|_2$. For a prescribed tolerance $\\tau > 0$ and an iteration cap $K \\in \\mathbb{N}$, define the achieved iteration count as the smallest $k \\in \\{1,2,\\dots,K\\}$ for which $\\rho_k \\le \\tau$. If no such $k$ exists up to $K$, report an achieved iteration count of $-1$ and report the final $\\rho_K$ achieved at $K$ iterations. Consider also left preconditioning with the diagonal (Jacobi) preconditioner $M = \\mathrm{diag}(A)$, i.e., solve $M^{-1} A x = M^{-1} b$ with GMRES, still with $x_0 = 0$, and define the preconditioned GMRES iterate $x_k^{(M)}$ analogously, together with the relative residual $\\rho_k^{(M)} = \\| b - A x_k^{(M)} \\|_2 / \\|b\\|_2$ and its achieved iteration count with the same $\\tau$ and $K$. For each problem instance below, compute both the unpreconditioned and preconditioned achieved iteration counts and the corresponding final relative residuals.\n\nTest suite. For each case, construct $A$ and $b$ exactly as specified, with $b = A x^{\\star}$ and $x^{\\star} = \\mathbf{1}$ (the vector of all ones of appropriate length). Use $x_0 = 0$ in all cases.\n\n- Case A (ill-conditioned symmetric positive definite): Let $n = 12$, and let $A = H_n$ be the Hilbert matrix with entries $[H_n]_{i,j} = 1/(i+j-1)$ for $i,j \\in \\{1,\\dots,n\\}$. Use tolerance $\\tau = 10^{-8}$ and iteration cap $K = 12$.\n\n- Case B (badly scaled, non-normal upper-triangular plus diagonal): Let $n = 40$. Define the diagonal $D \\in \\mathbb{R}^{n \\times n}$ by $D_{i,i} = 10^{\\,8 \\cdot (1 - (i-1)/(n-1))}$ for $i \\in \\{1,\\dots,n\\}$, and let $U \\in \\mathbb{R}^{n \\times n}$ be the strictly upper-triangular matrix with $U_{i,j} = 1$ if $i < j$ and $U_{i,j} = 0$ otherwise. Let $A = D + U$. Use tolerance $\\tau = 10^{-6}$ and iteration cap $K = 40$.\n\n- Case C (boundary sanity, diagonal with few distinct eigenvalues): Let $n = 15$. Let the diagonal entries of $A$ cycle through the three values $\\{1, 10^{2}, 10^{4}\\}$ in order, repeating as needed to length $n$; i.e., $A = \\mathrm{diag}(1, 10^{2}, 10^{4}, 1, 10^{2}, 10^{4}, \\dots)$ of size $n \\times n$. Use tolerance $\\tau = 10^{-12}$ and iteration cap $K = 15$.\n\nFor each case, you must output a list with four entries: $[\\kappa_{\\mathrm{un}}, \\kappa_{\\mathrm{pre}}, \\rho_{\\mathrm{un}}, \\rho_{\\mathrm{pre}}]$, where $\\kappa_{\\mathrm{un}}$ is the achieved iteration count for unpreconditioned GMRES (or $-1$ if not achieved within $K$), $\\kappa_{\\mathrm{pre}}$ is the achieved iteration count for preconditioned GMRES (or $-1$ if not achieved within $K$), $\\rho_{\\mathrm{un}}$ is the final relative residual achieved by the unpreconditioned run (equal to $\\rho_{\\kappa_{\\mathrm{un}}}$ if $\\kappa_{\\mathrm{un}} \\neq -1$, else equal to $\\rho_K$), and $\\rho_{\\mathrm{pre}}$ is the analogous quantity for the preconditioned run.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the three cases A, B, C. Each case’s result must itself be a bracketed, comma-separated list as above. The final output should therefore be of the form $[[\\kappa_{\\mathrm{un}}^{(A)}, \\kappa_{\\mathrm{pre}}^{(A)}, \\rho_{\\mathrm{un}}^{(A)}, \\rho_{\\mathrm{pre}}^{(A)}], [\\kappa_{\\mathrm{un}}^{(B)}, \\kappa_{\\mathrm{pre}}^{(B)}, \\rho_{\\mathrm{un}}^{(B)}, \\rho_{\\mathrm{pre}}^{(B)}], [\\kappa_{\\mathrm{un}}^{(C)}, \\kappa_{\\mathrm{pre}}^{(C)}, \\rho_{\\mathrm{un}}^{(C)}, \\rho_{\\mathrm{pre}}^{(C)}]]$, where each $\\rho$ must be printed as a floating-point number in scientific notation with at least three significant digits, and each $\\kappa$ as an integer.", "solution": "The problem is validated as follows.\n\n### Step 1: Extract Givens\n- **Linear System**: $A x = b$, with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$.\n- **Method**: Generalized Minimal Residual (GMRES).\n- **Initial Guess**: $x_0 = 0$.\n- **Krylov Subspace**: $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{ r_0, A r_0, \\dots, A^{k-1} r_0 \\}$, where $r_0 = b - A x_0 = b$.\n- **GMRES Iterate**: $x_k$ is the vector in $x_0 + \\mathcal{K}_k(A,r_0)$ that minimizes $\\| b - A x \\|_2$.\n- **Relative Residual (unpreconditioned)**: $\\rho_k = \\| b - A x_k \\|_2 / \\|b\\|_2$.\n- **Convergence Criterion**: Find the smallest $k \\in \\{1,2,\\dots,K\\}$ where $\\rho_k \\le \\tau$. This is the \"achieved iteration count\".\n- **Failure Condition**: If no such $k$ exists up to the iteration cap $K$, the achieved iteration count is $-1$, and the final relative residual is $\\rho_K$.\n- **Preconditioning**: Left preconditioning with the Jacobi preconditioner $M = \\mathrm{diag}(A)$. The system solved is $M^{-1} A x = M^{-1} b$.\n- **Preconditioned Iterate**: $x_k^{(M)}$ is defined analogously for the preconditioned system.\n- **Relative Residual (preconditioned)**: $\\rho_k^{(M)} = \\| b - A x_k^{(M)} \\|_2 / \\|b\\|_2$. Note that the residual is measured with respect to the original system, not the preconditioned one.\n- **Problem Instances**:\n    - **Case A**: $n = 12$, $A = H_n$ (Hilbert matrix, $[H_n]_{i,j} = 1/(i+j-1)$), tolerance $\\tau = 10^{-8}$, iteration cap $K = 12$.\n    - **Case B**: $n = 40$, $A = D + U$, where $D_{i,i} = 10^{\\,8 \\cdot (1 - (i-1)/(n-1))}$ and $U$ is strictly upper-triangular with ones. Tolerance $\\tau = 10^{-6}$, iteration cap $K = 40$.\n    - **Case C**: $n = 15$, $A = \\mathrm{diag}(1, 10^{2}, 10^{4}, 1, \\dots)$. Tolerance $\\tau = 10^{-12}$, iteration cap $K = 15$.\n- **Right-hand side**: In all cases, $b = A x^{\\star}$ where $x^{\\star} = \\mathbf{1}$ (vector of all ones).\n- **Output**: For each case, a list $[\\kappa_{\\mathrm{un}}, \\kappa_{\\mathrm{pre}}, \\rho_{\\mathrm{un}}, \\rho_{\\mathrm{pre}}]$. $\\kappa$ is the achieved iteration count, $\\rho$ is the final relative residual.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the well-established GMRES method for solving linear systems, a fundamental topic in numerical linear algebra and computational engineering. All matrices and procedures are standard. The problem is scientifically sound.\n- **Well-Posed**: Each test case is clearly defined with all necessary parameters ($A, b, x_0, \\tau, K$). The definition of GMRES and its convergence criteria are standard, ensuring a unique and meaningful solution exists for the requested outputs.\n- **Objective**: The problem statement is precise, quantitative, and free of subjective language. The definitions are mathematical and unambiguous.\n- **Other criteria**: The problem does not violate any other validation criteria. It is a complete, consistent, and formalizable computational task.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe Generalized Minimal Residual (GMRES) method is an iterative algorithm for solving the linear system of equations $Ax=b$. It is particularly effective for non-symmetric matrices $A$. The core principle of GMRES is to find an approximate solution $x_k$ at iteration $k$ from an affine subspace $x_0 + \\mathcal{K}_k(A, r_0)$ that minimizes the Euclidean norm of the residual, $\\|r_k\\|_2 = \\|b-Ax_k\\|_2$. Here, $x_0$ is an initial guess and $\\mathcal{K}_k(A, r_0)$ is the $k$-th order Krylov subspace generated by $A$ and the initial residual $r_0 = b-Ax_0$.\n\nThe minimization is performed over an expanding subspace. A key component of GMRES is the Arnoldi iteration, which generates an orthonormal basis $V_{k+1} = [v_1, v_2, \\dots, v_{k+1}]$ for the Krylov subspace $\\mathcal{K}_{k+1}(A, r_0)$. The first vector is $v_1 = r_0 / \\|r_0\\|_2$. The Arnoldi process yields the matrix relation $AV_k = V_{k+1}\\tilde{H}_k$, where $\\tilde{H}_k$ is a $(k+1) \\times k$ upper Hessenberg matrix.\n\nAn iterate $x_k$ can be written as $x_k = x_0 + z_k$ where $z_k \\in \\mathcal{K}_k(A, r_0)$. Since $V_k$ is a basis for $\\mathcal{K}_k(A, r_0)$, we can write $z_k = V_k y$ for some vector $y \\in \\mathbb{R}^k$. The minimization problem becomes:\n$$ \\min_{y \\in \\mathbb{R}^k} \\| b - A(x_0 + V_k y) \\|_2 = \\min_{y \\in \\mathbb{R}^k} \\| r_0 - AV_k y \\|_2 $$\nUsing $r_0 = \\|r_0\\|_2 v_1 = \\beta v_1$ and the Arnoldi relation, we substitute to obtain:\n$$ \\min_{y \\in \\mathbb{R}^k} \\| \\beta v_1 - V_{k+1} \\tilde{H}_k y \\|_2 $$\nSince the columns of $V_{k+1}$ are orthonormal, this is equivalent to solving a small least-squares problem:\n$$ \\min_{y \\in \\mathbb{R}^k} \\| \\beta e_1 - \\tilde{H}_k y \\|_2 $$\nwhere $e_1$ is the first standard basis vector in $\\mathbb{R}^{k+1}$. This subproblem is efficiently solved, for example, using QR factorization of $\\tilde{H}_k$. Once $y$ is found, the solution is $x_k = x_0 + V_k y$.\n\nFor the preconditioned case, we solve the left-preconditioned system $M^{-1}Ax = M^{-1}b$, where $M = \\mathrm{diag}(A)$ is the Jacobi preconditioner. GMRES is applied to the operator $A' = M^{-1}A$ and the right-hand side $b' = M^{-1}b$. The Krylov subspace is now $\\mathcal{K}_k(M^{-1}A, M^{-1}r_0)$. The algorithm minimizes the norm of the preconditioned residual, $\\|r'_k\\|_2 = \\|M^{-1}(b-Ax_k)\\|_2$.\n\nA critical detail in the problem statement is that the convergence criterion for the preconditioned case is based on the **true** relative residual, $\\rho_k^{(M)} = \\|b - Ax_k^{(M)}\\|_2 / \\|b\\|_2$, not the preconditioned one. This requires explicitly computing the iterate $x_k^{(M)}$ and the true residual at each step to check for convergence, rather than using the computationally cheaper residual norm from the least-squares subproblem.\n\nThe implementation will follow this logic. For each iteration $k$ from $1$ to $K$, we will:\n1.  Extend the Arnoldi factorization to obtain $V_{k+1}$ and $\\tilde{H}_k$.\n2.  Solve the $(k+1) \\times k$ least-squares problem for $y$.\n3.  Construct the solution $x_k = x_0 + V_k y$.\n4.  Compute the true relative residual $\\|b - Ax_k\\|_2 / \\|b\\|_2$.\n5.  If the residual is below the tolerance $\\tau$, we record the iteration count $k$ and the final residual, then terminate.\n6.  If the loop completes without convergence, we record $-1$ for the iteration count and the residual at iteration $K$.\n\nThe three test cases explore different sources of ill-conditioning:\n- **Case A**: A Hilbert matrix, which is intrinsically ill-conditioned due to nearly collinear column vectors. Its condition number grows exponentially with size.\n- **Case B**: A badly scaled matrix. The diagonal entries span many orders of magnitude. Jacobi preconditioning is specifically designed to counteract this by scaling the rows of the system.\n- **Case C**: A diagonal matrix with a small number of distinct eigenvalues. For such a matrix, GMRES is guaranteed to find the exact solution in a number of iterations equal to the degree of the minimal polynomial of $A$ with respect to $r_0$, which is at most the number of distinct eigenvalues. With preconditioning by $M=A$, the system operator becomes the identity matrix, for which GMRES converges in a single iteration.", "answer": "```\n[[-1,-1,2.028e-01,1.860e-01],[-1,2,1.156e-01,7.245e-07],[3,1,1.066e-15,1.758e-16]]\n```", "id": "2400723"}]}