{"hands_on_practices": [{"introduction": "Many fundamental laws of physics and engineering are expressed as differential equations. To solve these equations on a computer, we often transform them from a continuous problem into a discrete one through a process called discretization. This practice provides a concrete example of this crucial procedure, where you will use the finite difference method to approximate the solution to the famous Airy differential equation [@problem_id:2373161]. You will see firsthand how this approach naturally gives rise to a tridiagonal system of linear equations, building a tangible bridge between calculus, numerical methods, and linear algebra.", "problem": "Consider the boundary value problem for the Airy differential equation on a finite interval: find a function $y(x)$ such that\n$$\ny''(x) - x\\,y(x) = 0 \\quad \\text{for} \\quad x \\in [x_{\\mathrm{L}},x_{\\mathrm{R}}],\n$$\nsubject to Dirichlet boundary conditions $y(x_{\\mathrm{L}}) = \\alpha$ and $y(x_{\\mathrm{R}}) = \\beta$. The exact solution has the form\n$$\ny(x) = c_1\\,\\operatorname{Ai}(x) + c_2\\,\\operatorname{Bi}(x),\n$$\nwhere $\\operatorname{Ai}(x)$ and $\\operatorname{Bi}(x)$ are the Airy functions of the first and second kinds, respectively, and the constants $c_1$ and $c_2$ are uniquely determined by the boundary conditions.\n\nDefine a uniform grid with $n$ interior points over $[x_{\\mathrm{L}},x_{\\mathrm{R}}]$ by $x_i = x_{\\mathrm{L}} + i\\,h$ for $i \\in \\{0,1,\\dots,n+1\\}$, where $h = (x_{\\mathrm{R}} - x_{\\mathrm{L}})/(n+1)$, and $x_0 = x_{\\mathrm{L}}$, $x_{n+1} = x_{\\mathrm{R}}$. Let $y_i$ denote an approximation to $y(x_i)$ at the grid points. For each interior index $i \\in \\{1,2,\\dots,n\\}$, impose the linear relation\n$$\n\\frac{y_{i-1} - 2 y_i + y_{i+1}}{h^2} - x_i\\,y_i = 0.\n$$\nTreat the boundary values as known data: $y_0 = \\alpha$ and $y_{n+1} = \\beta$. This yields a tridiagonal linear system for the unknown vector $\\mathbf{u} = (y_1,y_2,\\dots,y_n)^\\top$ of the form $\\mathbf{A}\\,\\mathbf{u} = \\mathbf{d}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ has constant sub- and super-diagonals equal to $1/h^2$ and diagonal entries $-2/h^2 - x_i$ at row $i$, and the right-hand side $\\mathbf{d} \\in \\mathbb{R}^{n}$ is zero everywhere except for the boundary contributions $d_1 = -\\alpha/h^2$ and $d_n = -\\beta/h^2$.\n\nYour task is to compute, for each specified test case below, the maximum absolute grid error\n$$\nE_{\\infty} = \\max_{0 \\le j \\le n+1} \\left| y_j - y(x_j) \\right|,\n$$\nwhere $y_j$ denotes the numerical approximation at $x_j$ with $y_0 = \\alpha$ and $y_{n+1} = \\beta$, and $y(x)$ is the exact solution with constants $c_1$ and $c_2$ determined by the boundary conditions:\n$$\n\\begin{bmatrix}\n\\operatorname{Ai}(x_{\\mathrm{L}}) & \\operatorname{Bi}(x_{\\mathrm{L}}) \\\\\n\\operatorname{Ai}(x_{\\mathrm{R}}) & \\operatorname{Bi}(x_{\\mathrm{R}})\n\\end{bmatrix}\n\\begin{bmatrix}\nc_1 \\\\ c_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha \\\\ \\beta\n\\end{bmatrix}.\n$$\n\nTest suite. For each of the following parameter sets, set the boundary values to match the Airy function of the first kind at the endpoints, that is, $\\alpha = \\operatorname{Ai}(x_{\\mathrm{L}})$ and $\\beta = \\operatorname{Ai}(x_{\\mathrm{R}})$:\n- Case $1$ (general case): $x_{\\mathrm{L}} = 0$, $x_{\\mathrm{R}} = 1$, $n = 100$.\n- Case $2$ (single interior unknown): $x_{\\mathrm{L}} = 0$, $x_{\\mathrm{R}} = 1$, $n = 1$.\n- Case $3$ (larger interval): $x_{\\mathrm{L}} = 0$, $x_{\\mathrm{R}} = 5$, $n = 500$.\n- Case $4$ (interval including negative values): $x_{\\mathrm{L}} = -2$, $x_{\\mathrm{R}} = 2$, $n = 400$.\n\nFinal output format. Your program should produce a single line of output containing the four values of $E_{\\infty}$, one for each case in the order listed above, as a comma-separated list enclosed in square brackets, for example, $[e_1,e_2,e_3,e_4]$. Each $e_k$ must be a real number (decimal representation). No other text should be printed.", "solution": "The problem presented is a valid and well-posed boundary value problem from the field of computational science and engineering. It requires the numerical solution of the Airy differential equation using a finite difference scheme, which results in a tridiagonal linear system. The task is to compute the maximum grid error against the known exact solution for several parameter configurations. The problem is scientifically grounded, self-contained, and objective. There are no contradictions, ambiguities, or violations of fundamental principles. We proceed to the solution.\n\nThe problem is to find a numerical approximation to the solution $y(x)$ of the Airy differential equation, given by\n$$\ny''(x) - x\\,y(x) = 0,\n$$\non a finite domain $x \\in [x_{\\mathrm{L}}, x_{\\mathrm{R}}]$, subject to the Dirichlet boundary conditions $y(x_{\\mathrm{L}}) = \\alpha$ and $y(x_{\\mathrm{R}}) = \\beta$.\n\nFirst, we establish a uniform computational grid. The interval $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ is discretized into $n+1$ subintervals of equal width $h = (x_{\\mathrm{R}} - x_{\\mathrm{L}})/(n+1)$. This defines a set of $n+2$ grid points $x_i = x_{\\mathrm{L}} + i\\,h$ for $i \\in \\{0, 1, \\dots, n+1\\}$, where $x_0 = x_{\\mathrm{L}}$ and $x_{n+1} = x_{\\mathrm{R}}$. Let $y_i$ be the numerical approximation of the exact solution $y(x_i)$ at each grid point. The values at the boundaries are fixed by the given conditions: $y_0 = \\alpha$ and $y_{n+1} = \\beta$. The unknowns are the values at the $n$ interior grid points, $y_1, y_2, \\dots, y_n$.\n\nTo transform the continuous differential equation into a system of algebraic equations, we employ a finite difference approximation. The second derivative $y''(x)$ at an interior grid point $x_i$ is approximated using a second-order accurate central difference formula:\n$$\ny''(x_i) \\approx \\frac{y(x_{i-1}) - 2y(x_i) + y(x_{i+1})}{h^2}.\n$$\nSubstituting this approximation into the Airy equation for each interior point $x_i$, where $i \\in \\{1, 2, \\dots, n\\}$, we obtain a system of $n$ linear equations:\n$$\n\\frac{y_{i-1} - 2y_i + y_{i+1}}{h^2} - x_i y_i = 0.\n$$\nRearranging this equation to group the unknown terms ($y_1, \\dots, y_n$) on the left-hand side gives:\n$$\n\\frac{1}{h^2} y_{i-1} + \\left(-\\frac{2}{h^2} - x_i\\right) y_i + \\frac{1}{h^2} y_{i+1} = 0.\n$$\nThis set of equations for $i = 1, \\dots, n$ forms a linear system of the form $\\mathbf{A} \\mathbf{u} = \\mathbf{d}$, where $\\mathbf{u} = (y_1, y_2, \\dots, y_n)^\\top$ is the vector of unknown interior values.\n\nFor the first equation ($i=1$), the term $y_0$ is known ($\\alpha$), so it is moved to the right-hand side:\n$$\n\\left(-\\frac{2}{h^2} - x_1\\right) y_1 + \\frac{1}{h^2} y_2 = -\\frac{1}{h^2} y_0 = -\\frac{\\alpha}{h^2}.\n$$\nFor the last equation ($i=n$), the term $y_{n+1}$ is known ($\\beta$), and is also moved to the right-hand side:\n$$\n\\frac{1}{h^2} y_{n-1} + \\left(-\\frac{2}{h^2} - x_n\\right) y_n = -\\frac{1}{h^2} y_{n+1} = -\\frac{\\beta}{h^2}.\n$$\nFor any other interior equation where $i \\in \\{2, \\dots, n-1\\}$, the right-hand side is $0$.\n\nThe coefficient matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is thus tridiagonal, with the following structure:\n- Main diagonal entries: $A_{i,i} = -2/h^2 - x_i$\n- Sub-diagonal and super-diagonal entries: $A_{i, i+1} = A_{i+1, i} = 1/h^2$\n\nThe right-hand side vector $\\mathbf{d} \\in \\mathbb{R}^n$ is given by:\n$$\nd_i = \\begin{cases}\n    -\\alpha/h^2 & \\text{if } i = 1, \\\\\n    0 & \\text{if } 1 < i < n, \\\\\n    -\\beta/h^2 & \\text{if } i = n \\text{ and } n > 1, \\\\\n    -(\\alpha+\\beta)/h^2 & \\text{if } i = 1 \\text{ and } n = 1.\n\\end{cases}\n$$\nThis structure is efficiently solved using specialized algorithms for banded matrices, such as those available in the SciPy library.\n\nTo compute the error, we need the exact solution. The general solution to the Airy equation is $y(x) = c_1 \\operatorname{Ai}(x) + c_2 \\operatorname{Bi}(x)$. The constants $c_1$ and $c_2$ are determined by the boundary conditions. For all test cases, the boundary values are set to $\\alpha = \\operatorname{Ai}(x_{\\mathrm{L}})$ and $\\beta = \\operatorname{Ai}(x_{\\mathrm{R}})$. The boundary conditions thus become:\n$$\nc_1 \\operatorname{Ai}(x_{\\mathrm{L}}) + c_2 \\operatorname{Bi}(x_{\\mathrm{L}}) = \\operatorname{Ai}(x_{\\mathrm{L}})\n$$\n$$\nc_1 \\operatorname{Ai}(x_{\\mathrm{R}}) + c_2 \\operatorname{Bi}(x_{\\mathrm{R}}) = \\operatorname{Ai}(x_{\\mathrm{R}})\n$$\nBy inspection, the unique solution to this system is $c_1=1$ and $c_2=0$, because the Airy functions $\\operatorname{Ai}(x)$ and $\\operatorname{Bi}(x)$ form a fundamental set of solutions. Therefore, the exact solution for all test cases is simply $y(x) = \\operatorname{Ai}(x)$.\n\nThe numerical procedure for each test case is as follows:\n1.  Define the parameters $x_{\\mathrm{L}}$, $x_{\\mathrm{R}}$, and $n$.\n2.  Calculate the grid spacing $h$, the interior grid points $x_i$, and the boundary values $\\alpha = \\operatorname{Ai}(x_{\\mathrm{L}})$ and $\\beta = \\operatorname{Ai}(x_{\\mathrm{R}})$.\n3.  Construct the tridiagonal matrix $\\mathbf{A}$ in a banded format and the right-hand side vector $\\mathbf{d}$.\n4.  Solve the linear system $\\mathbf{A}\\mathbf{u} = \\mathbf{d}$ to find the vector of numerical approximations $\\mathbf{u} = (y_1, \\dots, y_n)^\\top$.\n5.  Form the complete numerical solution vector $(y_0, y_1, \\dots, y_n, y_{n+1}) = (\\alpha, u_1, \\dots, u_n, \\beta)$.\n6.  Evaluate the exact solution $y(x_j) = \\operatorname{Ai}(x_j)$ at all grid points $x_j$, for $j \\in \\{0, \\dots, n+1\\}$.\n7.  Compute the maximum absolute error $E_{\\infty} = \\max_{0 \\le j \\le n+1} |y_j - y(x_j)|$. Since the errors at the boundaries are zero by construction, this simplifies to calculating the maximum error over the interior points.\n\nThis procedure is implemented for each of the four specified test cases to obtain the required error values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import airy\nfrom scipy.linalg import solve_banded\n\ndef solve_airy_bvp(x_L: float, x_R: float, n: int) -> float:\n    \"\"\"\n    Solves the Airy BVP using finite differences and returns the max grid error.\n\n    Args:\n        x_L (float): Left boundary of the interval.\n        x_R (float): Right boundary of the interval.\n        n (int): Number of interior grid points.\n\n    Returns:\n        float: The maximum absolute error E_infinity.\n    \"\"\"\n    if n < 0:\n        raise ValueError(\"Number of interior points n must be non-negative.\")\n\n    # 1. Setup grid\n    # The total number of points is n+2. The number of intervals is n+1.\n    h = (x_R - x_L) / (n + 1)\n    \n    # Generate the full grid from x_0 to x_{n+1}\n    x_full = np.linspace(x_L, x_R, n + 2)\n    \n    # The interior points are x_1 to x_n\n    x_interior = x_full[1:-1]\n\n    # 2. Define boundary conditions based on the problem statement\n    # The exact solution is y(x) = Ai(x).\n    # alpha = y(x_L), beta = y(x_R)\n    alpha, _, _, _ = airy(x_L)\n    beta, _, _, _ = airy(x_R)\n\n    # If there are no interior points, the error is 0 as boundaries are exact.\n    if n == 0:\n        return 0.0\n\n    # 3. Construct the tridiagonal linear system A*u = d\n    # A is represented in a banded format for scipy.linalg.solve_banded.\n    # The banded matrix `ab` has 3 rows for a tridiagonal matrix:\n    # row 0: super-diagonal (shifted)\n    # row 1: main-diagonal\n    # row 2: sub-diagonal (shifted)\n    ab = np.zeros((3, n))\n    h2_inv = 1.0 / (h * h)\n\n    # Populate the main, sub-, and super-diagonals of matrix A\n    ab[0, 1:] = h2_inv  # Super-diagonal\n    ab[1, :] = -2.0 * h2_inv - x_interior  # Main-diagonal\n    ab[2, :-1] = h2_inv  # Sub-diagonal\n\n    # Construct the right-hand side vector d\n    d = np.zeros(n)\n    d[0] = -alpha * h2_inv\n    # The beta contribution is added to the last element.\n    # This correctly handles the n=1 case where the first and last elements are the same.\n    d[-1] += -beta * h2_inv\n\n    # 4. Solve the linear system for u = (y_1, ..., y_n)\n    u = solve_banded((1, 1), ab, d)\n\n    # 5. Assemble the full numerical solution and find the exact solution\n    # The full numerical solution includes the boundary values.\n    y_numerical = np.concatenate(([alpha], u, [beta]))\n    \n    # The exact solution is Ai(x) evaluated at all grid points.\n    y_exact, _, _, _ = airy(x_full)\n\n    # 6. Compute the maximum absolute grid error E_infinity\n    error = np.max(np.abs(y_numerical - y_exact))\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (x_L, x_R, n).\n    test_cases = [\n        (0.0, 1.0, 100),    # Case 1\n        (0.0, 1.0, 1),      # Case 2\n        (0.0, 5.0, 500),    # Case 3\n        (-2.0, 2.0, 400),   # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        x_L, x_R, n = case\n        e_infinity = solve_airy_bvp(x_L, x_R, n)\n        results.append(e_infinity)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2373161"}, {"introduction": "Now that we've seen how tridiagonal systems appear in practice, the next step is to solve them efficiently and reliably. While a general-purpose Gaussian elimination solver would work, it would ignore the sparse structure of the matrix. This exercise [@problem_id:2447586] introduces the Thomas algorithm, an exceptionally fast method tailored for tridiagonal systems. More importantly, you will confront cases where the basic algorithm fails and learn to implement a more robust version using partial pivoting, a cornerstone technique for ensuring numerical stability in scientific computing.", "problem": "You are given strictly tridiagonal linear systems of the form $A \\mathbf{x} = \\mathbf{d}$, where $A \\in \\mathbb{R}^{n \\times n}$ has nonzero entries only on the main diagonal, the first subdiagonal, and the first superdiagonal. The tridiagonal matrix is specified by three arrays: the subdiagonal $\\{a_i\\}_{i=1}^{n}$ with $a_1 = 0$, the diagonal $\\{b_i\\}_{i=1}^{n}$, and the superdiagonal $\\{c_i\\}_{i=1}^{n}$ with $c_n = 0$. For each test case below, the right-hand side $\\mathbf{d}$ is constructed using a known solution vector $\\mathbf{x}^{\\mathrm{true}}$ via $\\mathbf{d} = A \\mathbf{x}^{\\mathrm{true}}$.\n\nYour task is to write a complete program that, for each test case, computes the solution $\\mathbf{x}$ to $A \\mathbf{x} = \\mathbf{d}$ using a robust direct method that remains correct even when some $b_i$ equals zero. The program must then output, for each test case, the computed solution vector $\\mathbf{x}$.\n\nNo physical units are involved. Angles are not involved. All outputs must be numerical.\n\nTest suite (use these exact cases):\n\n- Case $1$ (happy path, strictly diagonally dominant): $n = 5$. Coefficients: $a_1 = 0$, and for $i \\in \\{2,3,4,5\\}$, $a_i = -1$; for $i \\in \\{1,2,3,4,5\\}$, $b_i = 2$; for $i \\in \\{1,2,3,4\\}$, $c_i = -1$, and $c_5 = 0$. Let $\\mathbf{x}^{\\mathrm{true}} = [1,1,1,1,1]^\\top$. Compute $\\mathbf{d} = A \\mathbf{x}^{\\mathrm{true}}$.\n- Case $2$ (edge case, zero on the main diagonal at the first equation, but the system is nonsingular): $n = 4$. Coefficients: $a_1 = 0$, $a_2 = 1$, $a_3 = 1$, $a_4 = 1$; $b_1 = 0$, $b_2 = 2$, $b_3 = 2$, $b_4 = 2$; $c_1 = 1$, $c_2 = 1$, $c_3 = 1$, $c_4 = 0$. Let $\\mathbf{x}^{\\mathrm{true}} = [1,1,1,1]^\\top$. Compute $\\mathbf{d} = A \\mathbf{x}^{\\mathrm{true}}$.\n- Case $3$ (near-breakdown without interchanges, numerically delicate pivot): $n = 6$. Coefficients: $a_1 = 0$, and for $i \\in \\{2,3,4,5,6\\}$, $a_i = -1$; $b_1 = 10^{-16}$ and for $i \\in \\{2,3,4,5,6\\}$, $b_i = 2$; for $i \\in \\{1,2,3,4,5\\}$, $c_i = -1$, and $c_6 = 0$. Let $\\mathbf{x}^{\\mathrm{true}} = [1,1,1,1,1,1]^\\top$. Compute $\\mathbf{d} = A \\mathbf{x}^{\\mathrm{true}}$.\n\nFinal output format:\n\n- For each test case $k$, compute the numerical solution $\\mathbf{x}^{(k)}$ to $A^{(k)} \\mathbf{x}^{(k)} = \\mathbf{d}^{(k)}$.\n- Your program should produce a single line of output containing the results as a comma-separated list of three lists, where the $k$-th inner list is the solution $\\mathbf{x}^{(k)}$ rounded to $10$ decimal places. For example, the output format is\n  \"[[$1.0000000000,1.0000000000],[...],[...]]\".\n- The required answers (the entries of each $\\mathbf{x}^{(k)}$) are floats. The three test cases together must be aggregated into a single line as described above.", "solution": "The problem requires the solution of a strictly tridiagonal linear system of equations, denoted $A \\mathbf{x} = \\mathbf{d}$, for three distinct cases. The matrix $A \\in \\mathbb{R}^{n \\times n}$ has non-zero entries only on its main diagonal ($b_i$), subdiagonal ($a_i$), and superdiagonal ($c_i$). A critical requirement is that the chosen solution method must be a robust direct method, capable of handling cases where diagonal elements $b_i$ are zero, a condition under which the standard Thomas algorithm fails.\n\nFirst, the given problem is validated. The problem statement is scientifically grounded in numerical linear algebra, a core component of computational science. It is well-posed, as each test case corresponds to a non-singular linear system, guaranteeing a unique solution. The definitions are objective and unambiguous, and all necessary information to reproduce the test cases is provided. The problem is therefore valid.\n\nA standard direct method for tridiagonal systems is the Thomas algorithm, which is a specialized form of Gaussian elimination. The algorithm consists of a forward elimination pass followed by a backward substitution pass. In the forward pass, the system is transformed into an upper bidiagonal one. For the $i$-th equation, $a_i x_{i-1} + b_i x_i + c_i x_{i+1} = d_i$, this is achieved by eliminating $x_{i-1}$. This leads to recurrence relations for modified coefficients. For example, one common formulation modifies the diagonal and right-hand side as follows:\n$$ b'_1 = b_1, \\quad d'_1 = d_1 $$\nFor $i = 2, \\dots, n$:\n$$ m_i = \\frac{a_i}{b'_{i-1}} $$\n$$ b'_i = b_i - m_i c_{i-1} $$\n$$ d'_i = d_i - m_i d'_{i-1} $$\nThe system is then solved by backward substitution:\n$$ x_n = \\frac{d'_n}{b'_n} $$\n$$ x_i = \\frac{d'_i - c_i x_{i+1}}{b'_i} \\quad \\text{for } i = n-1, \\dots, 1 $$\nThis algorithm is efficient, with a time complexity of $\\mathcal{O}(n)$. However, its critical flaw is the division by the pivot element $b'_{i-1}$ at each step. If any $b'_{i-1}$ is zero or numerically close to zero, the algorithm fails or becomes numerically unstable. This is precisely the issue highlighted by Test Case 2, where $b_1 = 0$, and Test Case 3, where $b_1 = 10^{-16}$.\n\nTo address this deficiency, a robust direct method is required. The canonical choice for such problems is Gaussian Elimination with Partial Pivoting (GEPP). The principle of GEPP is to mitigate numerical instability by ensuring the pivot element is as large as possible in magnitude. At each step $j$ of the elimination, the algorithm inspects all entries in the current column $j$ from row $j$ downwards. It then swaps the current row $j$ with the row containing the entry of largest absolute value (the pivot).\n\nFor a tridiagonal matrix, the pivoting step at column $j$ is simplified. The only potentially non-zero entries at or below the diagonal in column $j$ are $A_{j,j}$ (which is $b_j$) and $A_{j+1,j}$ (which is $a_{j+1}$). Therefore, partial pivoting only requires comparing $|A_{j,j}|$ with $|A_{j+1,j}|$ and swapping row $j$ with row $j+1$ if the latter is larger. Such a swap may introduce a non-zero element outside the tridiagonal structure (a \"fill-in\"), specifically at position $(j, j+2)$ if row $j$ is swapped with row $j+1$. The resulting matrix becomes upper triangular with an upper bandwidth of $2$.\n\nThe GEPP algorithm proceeds as follows:\n1.  **Forward Elimination with Partial Pivoting**: For each column $j$ from $1$ to $n-1$:\n    a.  **Pivoting**: Find the row index $p \\ge j$ such that $|A_{p,j}|$ is maximized. For an initially tridiagonal system, $p$ will be either $j$ or $j+1$. Swap row $j$ and row $p$. Also swap the corresponding elements in the vector $\\mathbf{d}$.\n    b.  **Elimination**: For each row $i$ from $j+1$ to $n$, calculate the multiplier $m = A_{i,j} / A_{j,j}$. Update row $i$ by subtracting $m$ times the pivot row $j$: $R_i \\leftarrow R_i - m R_j$. This operation zeros out the element $A_{i,j}$. The same operation is applied to the right-hand side vector: $d_i \\leftarrow d_i - m d_j$. For an initially tridiagonal system, only row $j+1$ will have a non-zero $A_{j+1,j}$ to be eliminated at step $j$.\n\n2.  **Backward Substitution**: After the forward elimination process, the matrix $A$ is converted into an upper triangular matrix $U$. The system $U \\mathbf{x} = \\mathbf{d'}$ is then solved for $\\mathbf{x}$ starting from the last variable $x_n$ and proceeding backwards to $x_1$:\n    $$ x_n = \\frac{d'_n}{U_{n,n}} $$\n    $$ x_i = \\frac{1}{U_{i,i}} \\left( d'_i - \\sum_{k=i+1}^{n} U_{i,k} x_k \\right) \\quad \\text{for } i = n-1, \\dots, 1 $$\nThis algorithm is robust against zero diagonal elements (provided the matrix is non-singular) and is numerically stable. For the small problem sizes specified ($n \\le 6$), implementing this algorithm using a dense matrix representation for $A$ is straightforward and computationally acceptable, avoiding the complexities of managing a banded matrix structure with fill-in. The provided solution code will therefore implement Gaussian Elimination with Partial Pivoting on a dense matrix representation.", "answer": "```python\nimport numpy as np\n\ndef solve_gepp(A_in, d_in):\n    \"\"\"\n    Solves the linear system Ax=d using Gaussian Elimination with Partial Pivoting.\n\n    Args:\n        A_in (np.ndarray): A square n x n matrix.\n        d_in (np.ndarray): A vector of length n.\n\n    Returns:\n        np.ndarray: The solution vector x.\n    \"\"\"\n    A = np.copy(A_in).astype(np.float64)\n    d = np.copy(d_in).astype(np.float64)\n    n = len(d)\n\n    # Forward elimination with partial pivoting\n    for j in range(n - 1):\n        # Find the row with the largest pivot in column j (from j downwards)\n        pivot_row_index = j + np.argmax(np.abs(A[j:, j]))\n\n        # Swap rows if a larger pivot is found\n        if pivot_row_index != j:\n            A[[j, pivot_row_index], :] = A[[pivot_row_index, j], :]\n            d[j], d[pivot_row_index] = d[pivot_row_index], d[j]\n\n        # The pivot element is A[j, j]. For a non-singular matrix, it must be non-zero\n        # after pivoting. The problems are guaranteed to be non-singular.\n        if A[j, j] == 0:\n            # This path should not be taken for the given problems.\n            continue\n\n        # Elimination step: zero out entries in column j below the pivot\n        for i in range(j + 1, n):\n            if A[i, j] != 0:\n                multiplier = A[i, j] / A[j, j]\n                A[i, j:] -= multiplier * A[j, j:]\n                d[i] -= multiplier * d[j]\n\n    # Backward substitution to solve the upper triangular system\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        # The problems are non-singular, so division by zero is not expected.\n        if A[i, i] == 0:\n            # This indicates a singular matrix\n            raise ValueError(\"Matrix is singular and cannot be solved.\")\n        \n        sum_ax = np.dot(A[i, i + 1:], x[i + 1:])\n        x[i] = (d[i] - sum_ax) / A[i, i]\n        \n    return x\n\ndef create_tridiagonal_matrix(n, a_coeffs, b_coeffs, c_coeffs):\n    \"\"\"Helper function to create the dense tridiagonal matrix.\"\"\"\n    A = np.zeros((n, n), dtype=np.float64)\n    for i in range(n):\n        A[i, i] = b_coeffs[i]\n        if i > 0:\n            A[i, i-1] = a_coeffs[i-1]\n        if i < n - 1:\n            A[i, i+1] = c_coeffs[i]\n    return A\n\ndef solve():\n    \"\"\"\n    Solves the tridiagonal systems for the specified test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5,\n            \"a\": [-1., -1., -1., -1.],  # indices i=2 to n, so n-1 elements\n            \"b\": [2., 2., 2., 2., 2.],\n            \"c\": [-1., -1., -1., -1.],  # indices i=1 to n-1, so n-1 elements\n            \"xtrue\": np.array([1., 1., 1., 1., 1.])\n        },\n        {\n            \"n\": 4,\n            \"a\": [1., 1., 1.],\n            \"b\": [0., 2., 2., 2.],\n            \"c\": [1., 1., 1.],\n            \"xtrue\": np.array([1., 1., 1., 1.])\n        },\n        {\n            \"n\": 6,\n            \"a\": [-1., -1., -1., -1., -1.],\n            \"b\": [1e-16, 2., 2., 2., 2., 2.],\n            \"c\": [-1., -1., -1., -1., -1.],\n            \"xtrue\": np.array([1., 1., 1., 1., 1., 1.])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        a = case[\"a\"]\n        b = case[\"b\"]\n        c = case[\"c\"]\n        xtrue = case[\"xtrue\"]\n        \n        # Construct the dense matrix A\n        A = create_tridiagonal_matrix(n, a, b, c)\n\n        # Compute the right-hand side vector d\n        d = A @ xtrue\n        \n        # Solve the system Ax = d using the robust GEPP method\n        x_computed = solve_gepp(A, d)\n        results.append(x_computed)\n\n    # Format the output as specified\n    formatted_results = []\n    for res in results:\n        formatted_res = '[' + ','.join([f\"{val:.10f}\" for val in res]) + ']'\n        formatted_results.append(formatted_res)\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447586"}, {"introduction": "The structural advantages we exploit in tridiagonal systems extend to a broader class of sparse matrices. This final practice [@problem_id:2373172] challenges you to generalize your skills by creating a solver for banded matrices with any specified lower bandwidth $p$ and upper bandwidth $q$. By developing a banded LU solver from scratch, you will learn to manage more complex sparse data structures and algorithms, a vital skill for tackling large-scale problems in areas like finite element analysis and computational fluid dynamics.", "problem": "You are asked to write a complete and runnable program that implements a solver for banded linear systems using Lower-Upper (LU) factorization without pivoting for non-centered bands. A square matrix $A \\in \\mathbb{R}^{n \\times n}$ is called banded with lower bandwidth $p$ and upper bandwidth $q$ if $A_{i,j} = 0$ whenever $j < i - p$ or $j > i + q$. In other words, the nonzero pattern is confined to $p$ subdiagonals below the main diagonal and $q$ superdiagonals above it. The goal is to solve $A x = b$ by exploiting the band structure to restrict operations to the band.\n\nStarting from the definition of Gaussian elimination and the notion of LU factorization $A = L U$ with a unit lower triangular matrix $L$ (diagonal entries equal to $1$) and an upper triangular matrix $U$, derive an algorithm that:\n- Performs elimination steps only within the band implied by $p$ and $q$, assuming no pivoting is needed.\n- Stores multipliers below the diagonal as entries of $L$ and the transformed upper part as entries of $U$.\n- Uses forward substitution to solve $L y = b$ followed by backward substitution to solve $U x = y$.\n\nYour implementation must not rely on any library routine that directly solves banded systems; instead, it must implement the LU factorization and triangular solves explicitly, using only the bandwidth parameters $p$ and $q$ to limit the operations.\n\nTest Suite. Your program must apply the solver to the following five test cases. In each case, construct $A$, $x_{\\text{true}}$, and $b = A x_{\\text{true}}$ exactly as specified, then compute the numerical solution $\\hat{x}$ using your banded LU solver, and report the infinity norm of the error $\\lVert \\hat{x} - x_{\\text{true}} \\rVert_{\\infty} = \\max_i | \\hat{x}_i - (x_{\\text{true}})_i |$ as a floating-point number.\n\n- Case $1$ (general non-centered band): $n=7$, $p=2$, $q=1$. Define $A \\in \\mathbb{R}^{7 \\times 7}$ by\n  - $A_{i,i} = 5 + i$ for $1 \\le i \\le 7$,\n  - $A_{i,i+1} = -1$ for $1 \\le i \\le 6$,\n  - $A_{i,i-1} = -2$ for $2 \\le i \\le 7$,\n  - $A_{i,i-2} = 0.5$ for $3 \\le i \\le 7$,\n  - all other entries $A_{i,j} = 0$.\n  Let $x_{\\text{true}} = [1,-2,3,-4,5,-6,7]^{\\top}$.\n\n- Case $2$ (tridiagonal): $n=6$, $p=1$, $q=1$. Define $A \\in \\mathbb{R}^{6 \\times 6}$ by\n  - $A_{i,i} = 4$ for $1 \\le i \\le 6$,\n  - $A_{i,i+1} = -1$ for $1 \\le i \\le 5$,\n  - $A_{i,i-1} = -1$ for $2 \\le i \\le 6$,\n  - all other entries $A_{i,j} = 0$.\n  Let $x_{\\text{true}} = [1,1,1,1,1,1]^{\\top}$.\n\n- Case $3$ (upper-banded with width $q=3$): $n=5$, $p=0$, $q=3$. Define $A \\in \\mathbb{R}^{5 \\times 5}$ by\n  - $A_{i,i} = 4 + i$ for $1 \\le i \\le 5$,\n  - $A_{i,i+1} = 0.5$ for $1 \\le i \\le 4$,\n  - $A_{i,i+2} = -0.25$ for $1 \\le i \\le 3$,\n  - $A_{i,i+3} = 0.125$ for $1 \\le i \\le 2$,\n  - all other entries $A_{i,j} = 0$.\n  Let $x_{\\text{true}} = [2,-1,0.5,-0.5,1]^{\\top}$.\n\n- Case $4$ (lower-banded with width $p=2$): $n=5$, $p=2$, $q=0$. Define $A \\in \\mathbb{R}^{5 \\times 5}$ by\n  - $A_{i,i} = 5$ for $1 \\le i \\le 5$,\n  - $A_{i,i-1} = -1$ for $2 \\le i \\le 5$,\n  - $A_{i,i-2} = 0.3$ for $3 \\le i \\le 5$,\n  - all other entries $A_{i,j} = 0$.\n  Let $x_{\\text{true}} = [1,2,3,4,5]^{\\top}$.\n\n- Case $5$ (diagonal): $n=6$, $p=0$, $q=0$. Define $A \\in \\mathbb{R}^{6 \\times 6}$ by\n  - $A_{i,i} = 2 + 0.1 i$ for $1 \\le i \\le 6$,\n  - all other entries $A_{i,j} = 0$.\n  Let $x_{\\text{true}} = [-1,2,-3,4,-5,6]^{\\top}$.\n\nIn all cases, form $b = A x_{\\text{true}}$, solve $A \\hat{x} = b$ with your banded LU solver, compute the infinity norm error $\\lVert \\hat{x} - x_{\\text{true}} \\rVert_{\\infty}$, and collect the five errors.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases $1$ through $5$, for example, $[e_1,e_2,e_3,e_4,e_5]$, where each $e_k$ is the computed infinity norm error for case $k$ as a floating-point number.", "solution": "The problem presented is a standard, well-defined exercise in numerical linear algebra, specifically in the domain of computational engineering. It asks for the derivation and implementation of a solver for a linear system $A x = b$, where $A$ is a banded matrix, using LU factorization without pivoting. All components required for a unique solution are provided: the definition of a banded matrix, the required method, and a set of five distinct, numerically specified test cases. The problem is scientifically grounded, objective, and complete. There are no logical contradictions, ambiguities, or factual inaccuracies. Therefore, the problem is deemed valid and a full solution will be provided.\n\nThe task is to solve the linear system $A x = b$ for a square matrix $A \\in \\mathbb{R}^{n \\times n}$ with lower bandwidth $p$ and upper bandwidth $q$. This means $A_{i,j} = 0$ if $j < i - p$ or $j > i + q$. The solution method is LU factorization, decomposing $A$ into a product of a unit lower triangular matrix $L$ and an upper triangular matrix $U$, such that $A = LU$. The system is then solved by a two-step process: first solving $L y = b$ (forward substitution) and then $U x = y$ (backward substitution).\n\nA critical observation is that if no row permutations (pivoting) are performed, the band structure is preserved during Gaussian elimination. The resulting matrix $L$ will have a lower bandwidth of $p$, and $U$ will have an upper bandwidth of $q$. This allows for significant computational and storage savings.\n\nTo exploit the band structure, we avoid storing the full $n \\times n$ matrix $A$. Instead, we store only the non-zero bands in a compact matrix, denoted as $A_{band}$, of size $(p+q+1) \\times n$. An element $A_{i,j}$ from the original matrix is mapped to an element in the compact storage. A standard convention, which we adopt here, maps $A_{i,j}$ to the element $A_{band}[i-j+q, j]$ (using $0$-based indexing). In this scheme, the main diagonal of $A$ resides in row $q$ of $A_{band}$, the $q$ superdiagonals are in rows $0, \\dots, q-1$, and the $p$ subdiagonals are in rows $q+1, \\dots, q+p$.\n\nThe LU factorization process is an in-place modification of the $A_{band}$ matrix. The algorithm is derived from standard Gaussian elimination. The process iterates through columns $k$ from $0$ to $n-2$, using the pivot $A_{k,k}$ to eliminate the non-zero entries below it in the same column.\n\nFor each pivot column $k \\in \\{0, 1, \\dots, n-2\\}$:\nThe elements to be eliminated are $A_{i,k}$ for rows $i$ where $k+1 \\le i \\le \\min(n-1, k+p)$.\nFor each such row $i$, the multiplier is calculated as $m_{i,k} = A_{i,k} / A_{k,k}$.\nThis multiplier $m_{i,k}$ becomes the entry $L_{i,k}$ of the matrix $L$. We store it in the position previously occupied by $A_{i,k}$.\nThe elimination update rule is $A_{i,j} \\leftarrow A_{i,j} - m_{i,k} A_{k,j}$. This update only needs to be applied to columns $j$ where both $A_{i,j}$ and $A_{k,j}$ can be non-zero. This corresponds to columns $j \\in \\{k+1, \\dots, \\min(n-1, k+q)\\}$.\n\nIn terms of the compact storage $A_{band}$, the factorization algorithm is as follows for $k \\in \\{0, \\dots, n-2\\}$:\n1. For each row index $i \\in \\{k+1, \\dots, \\min(n-1, k+p)\\}$:\n   a. The pivot is $U_{k,k} = A_{band}[q, k]$.\n   b. The element to be eliminated is $A_{i,k}$, located at $A_{band}[i-k+q, k]$.\n   c. Calculate the multiplier $m_{i,k} = \\frac{A_{band}[i-k+q, k]}{A_{band}[q, k]}$.\n   d. Store this multiplier in place: $A_{band}[i-k+q, k] = m_{i,k}$. This entry now represents $L_{i,k}$.\n   e. For each column index $j \\in \\{k+1, \\dots, \\min(n-1, k+q)\\}$, update the corresponding element of row $i$:\n      $$ A_{band}[i-j+q, j] \\leftarrow A_{band}[i-j+q, j] - m_{i,k} \\times A_{band}[k-j+q, j] $$\nAfter this process completes, the upper part of $A_{band}$ (rows $0$ to $q$) contains the matrix $U$, and the strictly lower part (rows $q+1$ to $q+p$) contains the non-diagonal entries of $L$.\n\nWith the factorization $A=LU$ completed in-place, we proceed to solve the two triangular systems.\n\nFirst, forward substitution to solve $L y = b$:\nSince $L$ is a unit lower triangular matrix with lower bandwidth $p$, the formula for each component $y_i$ is:\n$$ y_i = b_i - \\sum_{j=\\max(0, i-p)}^{i-1} L_{i,j} y_j \\quad \\text{for } i = 0, \\dots, n-1 $$\nThe entries $L_{i,j}$ are the multipliers stored in $A_{band}[i-j+q, j]$. The vector $y$ can be computed by iterating $i$ from $0$ to $n-1$.\n\nSecond, backward substitution to solve $U x = y$:\nSince $U$ is an upper triangular matrix with upper bandwidth $q$, we solve for $x$ by iterating backwards from $i = n-1$ down to $0$:\n$$ x_i = \\frac{1}{U_{i,i}} \\left( y_i - \\sum_{j=i+1}^{\\min(n-1, i+q)} U_{i,j} x_j \\right) \\quad \\text{for } i = n-1, \\dots, 0 $$\nThe entries $U_{i,j}$ are stored in $A_{band}[i-j+q, j]$. The diagonal entry $U_{i,i}$ is at $A_{band}[q, i]$.\n\nThis completes the derivation of the algorithm. A program implementing these steps will be constructed to solve the given test cases. For each case, the matrix $A$ and vector $x_{\\text{true}}$ are defined, $b = A x_{\\text{true}}$ is computed, the numerical solution $\\hat{x}$ is found using the derived banded LU solver, and the infinity norm of the error, $\\lVert \\hat{x} - x_{\\text{true}} \\rVert_{\\infty}$, is reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef full_to_banded(A, p, q):\n    \"\"\"\n    Converts a full numpy matrix A to its compact banded representation.\n    \"\"\"\n    n = A.shape[0]\n    A_band = np.zeros((p + q + 1, n))\n    for i in range(n):\n        for j in range(max(0, i - p), min(n, i + q + 1)):\n            # Mapping A[i, j] to A_band[row, col]\n            # row = i - j + q, col = j\n            A_band[i - j + q, j] = A[i, j]\n    return A_band\n\ndef banded_lu_factorization(A_band, p, q):\n    \"\"\"\n    Performs LU factorization in-place on a banded matrix.\n    The upper part of A_band will store U, the lower part will store L.\n    \"\"\"\n    n = A_band.shape[1]\n    # k is the pivot column (0-indexed)\n    for k in range(n - 1):\n        # Pivot element U_kk is at A_band[q, k]\n        pivot = A_band[q, k]\n        if pivot == 0:\n            # This would require pivoting, which is not implemented as per problem spec.\n            # For the given test cases, this should not occur.\n            raise ValueError(\"Zero pivot encountered.\")\n        \n        # i is the row to be updated\n        for i in range(k + 1, min(n, k + p + 1)):\n            # Multiplier L_ik = A_ik / U_kk\n            # Element A_ik is at A_band[i-k+q, k]\n            multiplier = A_band[i - k + q, k] / pivot\n            A_band[i - k + q, k] = multiplier  # Store L_ik\n\n            # Update row i from column k+1 to k+q\n            # A_ij <- A_ij - L_ik * U_kj\n            for j in range(k + 1, min(n, k + q + 1)):\n                # U_kj is at A_band[k-j+q, j]\n                # A_ij is at A_band[i-j+q, j]\n                A_band[i - j + q, j] -= multiplier * A_band[k - j + q, j]\n\ndef solve_banded_lu(A_band, p, q, b):\n    \"\"\"\n    Solves A x = b for a banded system A, given its in-place LU factorization A_band.\n    \"\"\"\n    n = A_band.shape[1]\n    y = b.copy()\n\n    # Forward substitution to solve L y = b\n    # L is unit lower triangular with lower bandwidth p\n    for i in range(n):\n        s = 0.0\n        # Sum over L_ij * y_j\n        for j in range(max(0, i - p), i):\n            # L_ij is at A_band[i-j+q, j]\n            s += A_band[i - j + q, j] * y[j]\n        y[i] -= s\n\n    # Backward substitution to solve U x = y\n    # U is upper triangular with upper bandwidth q\n    x = y.copy()\n    for i in range(n - 1, -1, -1):\n        s = 0.0\n        # Sum over U_ij * x_j\n        for j in range(i + 1, min(n, i + q + 1)):\n            # U_ij is at A_band[i-j+q, j]\n            s += A_band[i - j + q, j] * x[j]\n        \n        # U_ii is at A_band[q, i]\n        x[i] = (y[i] - s) / A_band[q, i]\n        \n    return x\n\ndef get_test_cases():\n    \"\"\"\n    Generates the five test cases as specified in the problem.\n    Returns a list of tuples, each containing (A, p, q, x_true).\n    \"\"\"\n    cases = []\n\n    # Case 1\n    n1, p1, q1 = 7, 2, 1\n    A1 = np.zeros((n1, n1))\n    for i in range(n1):\n        A1[i, i] = 5.0 + (i + 1)\n        if i + 1 < n1: A1[i, i + 1] = -1.0\n        if i - 1 >= 0: A1[i, i - 1] = -2.0\n        if i - 2 >= 0: A1[i, i - 2] = 0.5\n    x_true1 = np.array([1.0, -2.0, 3.0, -4.0, 5.0, -6.0, 7.0])\n    cases.append((A1, p1, q1, x_true1))\n\n    # Case 2\n    n2, p2, q2 = 6, 1, 1\n    A2 = np.zeros((n2, n2))\n    for i in range(n2):\n        A2[i, i] = 4.0\n        if i + 1 < n2: A2[i, i + 1] = -1.0\n        if i - 1 >= 0: A2[i, i - 1] = -1.0\n    x_true2 = np.ones(n2)\n    cases.append((A2, p2, q2, x_true2))\n\n    # Case 3\n    n3, p3, q3 = 5, 0, 3\n    A3 = np.zeros((n3, n3))\n    for i in range(n3):\n        A3[i, i] = 4.0 + (i + 1)\n        if i + 1 < n3: A3[i, i + 1] = 0.5\n        if i + 2 < n3: A3[i, i + 2] = -0.25\n        if i + 3 < n3: A3[i, i + 3] = 0.125\n    x_true3 = np.array([2.0, -1.0, 0.5, -0.5, 1.0])\n    cases.append((A3, p3, q3, x_true3))\n\n    # Case 4\n    n4, p4, q4 = 5, 2, 0\n    A4 = np.zeros((n4, n4))\n    for i in range(n4):\n        A4[i, i] = 5.0\n        if i - 1 >= 0: A4[i, i - 1] = -1.0\n        if i - 2 >= 0: A4[i, i - 2] = 0.3\n    x_true4 = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    cases.append((A4, p4, q4, x_true4))\n\n    # Case 5\n    n5, p5, q5 = 6, 0, 0\n    A5 = np.zeros((n5, n5))\n    for i in range(n5):\n        A5[i, i] = 2.0 + 0.1 * (i + 1)\n    x_true5 = np.array([-1.0, 2.0, -3.0, 4.0, -5.0, 6.0])\n    cases.append((A5, p5, q5, x_true5))\n\n    return cases\n\ndef solve():\n    test_cases = get_test_cases()\n    results = []\n\n    for A_full, p, q, x_true in test_cases:\n        # 1. Create the banded system\n        b = A_full @ x_true\n        A_band = full_to_banded(A_full, p, q)\n\n        # 2. Perform LU factorization on the banded matrix\n        banded_lu_factorization(A_band, p, q)\n\n        # 3. Solve using forward/backward substitution\n        x_hat = solve_banded_lu(A_band, p, q, b)\n\n        # 4. Compute the infinity norm of the error\n        error = np.linalg.norm(x_hat - x_true, ord=np.inf)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2373172"}]}