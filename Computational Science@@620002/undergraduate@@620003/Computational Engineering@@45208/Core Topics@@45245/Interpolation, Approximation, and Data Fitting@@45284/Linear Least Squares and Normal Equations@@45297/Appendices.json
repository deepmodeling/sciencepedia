{"hands_on_practices": [{"introduction": "One of the most fundamental applications of linear least squares is in data fitting and parameter estimation. This exercise places you in the role of an engineer calibrating a new sensor, a common task where you must find the best linear model to describe a set of noisy measurements. By working through this problem [@problem_id:2218047], you will practice the complete workflow of setting up the normal equations from experimental data, solving for the model parameters, and quantifying the \"goodness of fit\" by calculating the residual error.", "problem": "An engineer is calibrating a novel thermal sensor. The sensor's output voltage, $V$, is assumed to be a linear function of the ambient temperature, $T$. The relationship is modeled by the equation $V(T) = c_0 + c_1 T$, where $c_0$ and $c_1$ are the calibration constants to be determined. To find these constants, four measurements are taken in a controlled environment:\n\n*   At a temperature of $T=10$ degrees Celsius, the measured voltage is $V=2.6$ volts.\n*   At a temperature of $T=20$ degrees Celsius, the measured voltage is $V=3.4$ volts.\n*   At a temperature of $T=30$ degrees Celsius, the measured voltage is $V=4.7$ volts.\n*   At a temperature of $T=40$ degrees Celsius, the measured voltage is $V=5.4$ volts.\n\nThe parameters $c_0$ and $c_1$ are to be determined such that the sum of the squared differences between the measured voltages and the voltages predicted by the linear model is minimized. Let the resulting best-fit line be $\\hat{V}(T) = \\hat{c}_0 + \\hat{c}_1 T$.\n\nYour task is to calculate the Euclidean norm of the residual vector, where the components of the residual vector are the differences between the individually measured voltages and the corresponding voltages predicted by this best-fit line.\n\nExpress your final answer in volts, rounded to three significant figures.", "solution": "We model the voltage as a linear function of temperature, $V(T)=c_{0}+c_{1}T$, and determine $(\\hat{c}_{0},\\hat{c}_{1})$ by least squares using the four measurements $(T_{i},V_{i})=(10,2.6),(20,3.4),(30,4.7),(40,5.4)$. Let the design matrix be $X=\\begin{pmatrix}1 & 10 \\\\ 1 & 20 \\\\ 1 & 30 \\\\ 1 & 40\\end{pmatrix}$ and the observation vector be $\\boldsymbol{V}=\\begin{pmatrix}2.6 \\\\ 3.4 \\\\ 4.7 \\\\ 5.4\\end{pmatrix}$. The least-squares estimate satisfies\n$$\n\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=(X^{T}X)^{-1}X^{T}\\boldsymbol{V},\n$$\nequivalently the normal equations\n$$\n\\begin{pmatrix}n & \\sum T_{i} \\\\ \\sum T_{i} & \\sum T_{i}^{2}\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}\\sum V_{i} \\\\ \\sum T_{i}V_{i}\\end{pmatrix}.\n$$\nCompute the sums: $n=4$, $\\sum T_{i}=10+20+30+40=100$, $\\sum T_{i}^{2}=10^{2}+20^{2}+30^{2}+40^{2}=3000$, $\\sum V_{i}=2.6+3.4+4.7+5.4=16.1$, and $\\sum T_{i}V_{i}=10\\cdot 2.6+20\\cdot 3.4+30\\cdot 4.7+40\\cdot 5.4=451$. Thus we solve\n$$\n\\begin{pmatrix}4 & 100 \\\\ 100 & 3000\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}16.1 \\\\ 451\\end{pmatrix}.\n$$\nThe determinant is $4\\cdot 3000-100\\cdot 100=2000$, so\n$$\n\\hat{c}_{0}=\\frac{3000\\cdot 16.1-100\\cdot 451}{2000}=1.6,\\quad \\hat{c}_{1}=\\frac{-100\\cdot 16.1+4\\cdot 451}{2000}=0.097.\n$$\nHence the best-fit line is $\\hat{V}(T)=1.6+0.097\\,T$.\n\nCompute the residuals $r_{i}=V_{i}-\\hat{V}(T_{i})$ at the four temperatures:\n$$\n\\hat{V}(10)=1.6+0.097\\cdot 10=2.57,\\quad r_{1}=2.6-2.57=0.03,\n$$\n$$\n\\hat{V}(20)=1.6+0.097\\cdot 20=3.54,\\quad r_{2}=3.4-3.54=-0.14,\n$$\n$$\n\\hat{V}(30)=1.6+0.097\\cdot 30=4.51,\\quad r_{3}=4.7-4.51=0.19,\n$$\n$$\n\\hat{V}(40)=1.6+0.097\\cdot 40=5.48,\\quad r_{4}=5.4-5.48=-0.08.\n$$\nThe Euclidean norm of the residual vector $\\boldsymbol{r}$ is\n$$\n\\|\\boldsymbol{r}\\|_{2}=\\sqrt{\\sum_{i=1}^{4}r_{i}^{2}}=\\sqrt{(0.03)^{2}+(-0.14)^{2}+(0.19)^{2}+(-0.08)^{2}}=\\sqrt{0.063}.\n$$\nEvaluating the square root and rounding to three significant figures gives\n$$\n\\|\\boldsymbol{r}\\|_{2}\\approx 0.251.\n$$\nThis value is in volts because each residual is a voltage difference.", "answer": "$$\\boxed{0.251}$$", "id": "2218047"}, {"introduction": "Beyond data fitting, the linear least squares method has a profound geometric interpretation: it finds the orthogonal projection of a vector onto a subspace. This practice [@problem_id:2218040] uses an intuitive scenario from robotics to make this abstract concept concrete. You will determine the closest point a robot's end-effector can get to a desired target, reinforcing the idea that the least-squares solution is not just an algebraic curiosity but the \"best\" approximation in a geometric sense.", "problem": "You are a control systems engineer working on a simple 2-Degree-Of-Freedom (2-DOF) robotic manipulator. The robot's end-effector moves in 3D space, and its position relative to its base is described by a coordinate vector $\\mathbf{p} = [x, y, z]^T$. The robot's motion is generated by two independent actuators. The first actuator moves the end-effector along the direction vector $\\mathbf{u}_1 = [1, 0, 1]^T$, and the second actuator moves it along the direction vector $\\mathbf{u}_2 = [0, 1, 1]^T$. Consequently, any reachable position $\\mathbf{p}_{reach}$ must be a linear combination of these two vectors: $\\mathbf{p}_{reach} = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2$, for some scalar coefficients $c_1$ and $c_2$ representing the actuator displacements. This means the set of all reachable points forms a plane passing through the origin.\n\nA task requires the robot's end-effector to move to a target position $\\mathbf{b} = [1, 2, 2]^T$. This point may not lie within the robot's reachable plane. The control system's objective is to find the point $\\mathbf{p}^*$ within the reachable plane that is geometrically closest to the target point $\\mathbf{b}$. This optimal point is the orthogonal projection of $\\mathbf{b}$ onto the subspace spanned by the actuator direction vectors.\n\nUsing the method of normal equations, determine the coordinate vector of this optimal reachable point $\\mathbf{p}^*$. Express your answer as a column vector with exact fractional components.", "solution": "The problem asks for the orthogonal projection of the vector $\\mathbf{b}$ onto the subspace spanned by the vectors $\\mathbf{u}_1$ and $\\mathbf{u}_2$. This subspace is the column space of the matrix $A$ whose columns are $\\mathbf{u}_1$ and $\\mathbf{u}_2$. The projection $\\mathbf{p}^*$ is the vector in the column space of $A$ that is closest to $\\mathbf{b}$.\n\nFirst, we define the matrix $A$ and the vector $\\mathbf{b}$:\n$$ A = \\begin{bmatrix} \\mathbf{u}_1 & \\mathbf{u}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} $$\n\nThe projection $\\mathbf{p}^*$ can be expressed as a linear combination of the columns of $A$, i.e., $\\mathbf{p}^* = A\\mathbf{c}^*$, where $\\mathbf{c}^* = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}$ is the vector of coefficients that minimizes the squared Euclidean distance $\\|A\\mathbf{c} - \\mathbf{b}\\|^2$. This is a linear least squares problem. The solution $\\mathbf{c}^*$ is found by solving the normal equations:\n$$ A^T A \\mathbf{c}^* = A^T \\mathbf{b} $$\n\nWe proceed by computing the components of this equation.\n\nStep 1: Compute the matrix $A^T A$.\nThe transpose of $A$ is:\n$$ A^T = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} $$\nNow, we compute the product $A^T A$:\n$$ A^T A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(0)+(1)(1) & (1)(0)+(0)(1)+(1)(1) \\\\ (0)(1)+(1)(0)+(1)(1) & (0)(0)+(1)(1)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $$\n\nStep 2: Compute the vector $A^T \\mathbf{b}$.\n$$ A^T \\mathbf{b} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(2)+(1)(2) \\\\ (0)(1)+(1)(2)+(1)(2) \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$\n\nStep 3: Solve the normal equations for $\\mathbf{c}^*$.\nThe system of equations is:\n$$ \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$\nThis corresponds to the two linear equations:\n1) $2c_1 + c_2 = 3$\n2) $c_1 + 2c_2 = 4$\n\nFrom equation (1), we can express $c_2$ in terms of $c_1$:\n$c_2 = 3 - 2c_1$\n\nSubstitute this into equation (2):\n$c_1 + 2(3 - 2c_1) = 4$\n$c_1 + 6 - 4c_1 = 4$\n$-3c_1 = -2$\n$c_1 = \\frac{2}{3}$\n\nNow, substitute the value of $c_1$ back to find $c_2$:\n$c_2 = 3 - 2\\left(\\frac{2}{3}\\right) = 3 - \\frac{4}{3} = \\frac{9}{3} - \\frac{4}{3} = \\frac{5}{3}$\nSo, the coefficient vector is $\\mathbf{c}^* = \\begin{bmatrix} 2/3 \\\\ 5/3 \\end{bmatrix}$.\n\nStep 4: Compute the projection vector $\\mathbf{p}^*$.\nThe optimal reachable point $\\mathbf{p}^*$ is given by $A\\mathbf{c}^*$:\n$$ \\mathbf{p}^* = A\\mathbf{c}^* = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\end{bmatrix} = \\begin{bmatrix} (1)\\left(\\frac{2}{3}\\right) + (0)\\left(\\frac{5}{3}\\right) \\\\ (0)\\left(\\frac{2}{3}\\right) + (1)\\left(\\frac{5}{3}\\right) \\\\ (1)\\left(\\frac{2}{3}\\right) + (1)\\left(\\frac{5}{3}\\right) \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\\\ \\frac{2}{3} + \\frac{5}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\\\ \\frac{7}{3} \\end{bmatrix} $$\n\nThe coordinate vector of the optimal reachable point is $\\mathbf{p}^* = [2/3, 5/3, 7/3]^T$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} \\\\\n\\frac{5}{3} \\\\\n\\frac{7}{3}\n\\end{pmatrix}\n}\n$$", "id": "2218040"}, {"introduction": "A theoretical solution and its practical implementation can behave very differently, especially in computational science. This final exercise [@problem_id:2409733] moves from pen-and-paper to a computational setting, exploring the crucial concept of numerical stability. By writing a program to test matrices with increasingly collinear columns, you will directly observe how the conditioning of a matrix $A$ drastically affects the sensitivity of the least-squares solution to small perturbations, a phenomenon known as ill-conditioning.", "problem": "Create a complete, runnable program that evaluates the sensitivity of the linear least-squares minimizer to perturbations in the right-hand side for a family of explicitly defined matrices with nearly collinear columns. Let the linear least-squares solution $x_{LS}$ be the minimizer of the Euclidean norm objective $\\lVert A x - b \\rVert_2$ for a given real matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$. Consider the following deterministic setup in $\\mathbb{R}^3$ with two columns, $m = 3$ and $n = 2$, based on the orthonormal vectors $c_1 = \\frac{1}{\\sqrt{2}}[1,1,0]^\\top$ and $w = \\frac{1}{\\sqrt{2}}[1,-1,0]^\\top$.\n\nFor each test case below, construct $A \\in \\mathbb{R}^{3 \\times 2}$, define $x_{\\text{true}} \\in \\mathbb{R}^2$, form $b = A x_{\\text{true}}$, and form a perturbation $\\Delta b$ with magnitude specified by a relative factor $r$ in the direction $w$; that is, $\\lVert \\Delta b \\rVert_2 = r \\, \\lVert b \\rVert_2$ and $\\Delta b = \\alpha \\, w$ with $\\alpha = r \\, \\lVert b \\rVert_2$. For each case, compute the least-squares solutions $x_{LS}(b)$ and $x_{LS}(b + \\Delta b)$ and then compute the amplification factor of the solution with respect to the perturbation in $b$ defined by\n$$\nR = \\frac{\\lVert x_{LS}(b + \\Delta b) - x_{LS}(b) \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2}.\n$$\nIf any denominator is zero in the above expression, interpret the corresponding ratio using a limiting value consistent with continuity, and in the implementation prevent division by zero using a strictly positive negligible constant.\n\nUse the following test suite, where all numbers are exact and dimensionless:\n- Test case $1$ (well-conditioned baseline): $A = [c_1,\\, w]$ (columns), $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n- Test case $2$ (moderate near collinearity): $A = [c_1,\\, c_1 + \\epsilon w]$ with $\\epsilon = 10^{-3}$, $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n- Test case $3$ (extreme near collinearity): $A = [c_1,\\, c_1 + \\epsilon w]$ with $\\epsilon = 10^{-9}$, $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n- Test case $4$ (exact collinearity and rank deficiency): $A = [c_1,\\, c_1]$, $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n\nFor each test case, the required answer is the single real number $R$ as defined above. Your program must process all four test cases in the specified order and produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, for example, $[R_1,R_2,R_3,R_4]$. No physical units are involved. Angles are not used. All numerical outputs must be real numbers (in decimal or scientific notation) on that single line only.", "solution": "The problem requires an analysis of the sensitivity of the linear least-squares solution to perturbations in the right-hand side vector $b$. We are asked to compute an amplification factor, $R$, for four specific test cases involving matrices with progressively more collinear columns.\n\nThe linear least-squares problem is defined as finding a vector $x_{LS} \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual, $\\lVert Ax - b \\rVert_2$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$.\n\nThe solution to this problem can be formally expressed using the Moore-Penrose pseudoinverse of $A$, denoted $A^\\dagger$, as $x_{LS} = A^\\dagger b$. When the matrix $A$ has full column rank (i.e., $\\text{rank}(A) = n$), the pseudoinverse is given by $A^\\dagger = (A^\\top A)^{-1} A^\\top$, and the solution $x_{LS}$ is unique and can be found by solving the system of normal equations:\n$$\nA^\\top A x = A^\\top b\n$$\nIf $A$ is rank-deficient, the least-squares problem still has a solution, but it is not unique. In this situation, a unique solution is conventionally selected by requiring it to have the minimum Euclidean norm among all minimizers. The `numpy.linalg.lstsq` function, as is standard, returns this minimum-norm solution, which is also given by $x_{LS} = A^\\dagger b$.\n\nThe sensitivity of the solution $x_{LS}$ to a perturbation $\\Delta b$ in $b$ is central to this problem. Let the perturbed vector be $b' = b + \\Delta b$. The new solution is $x_{LS}(b + \\Delta b) = A^\\dagger (b + \\Delta b)$. The change in the solution, $\\Delta x$, is therefore:\n$$\n\\Delta x = x_{LS}(b + \\Delta b) - x_{LS}(b) = A^\\dagger (b + \\Delta b) - A^\\dagger b = A^\\dagger \\Delta b\n$$\nThe problem defines the amplification factor $R$ as the ratio of the relative change in the solution to the relative change in the right-hand side vector:\n$$\nR = \\frac{\\lVert \\Delta x \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2} = \\frac{\\lVert A^\\dagger \\Delta b \\rVert_2 / \\lVert A^\\dagger b \\rVert_2}{\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2}\n$$\nIn this problem, the relative perturbation in $b$ is fixed: $\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2 = r = 10^{-6}$. Thus, the formula simplifies to:\n$$\nR = \\frac{\\lVert x_{LS}(b + \\Delta b) - x_{LS}(b) \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{r}\n$$\nThe procedure for each test case is to construct the matrix $A$, the true solution $x_{\\text{true}}$, the vector $b = A x_{\\text{true}}$, and the perturbation $\\Delta b$. Then we compute the least-squares solutions for $b$ and $b+\\Delta b$ to find $R$. The base vectors are the orthonormal vectors $c_1 = \\frac{1}{\\sqrt{2}}[1,1,0]^\\top$ and $w = \\frac{1}{\\sqrt{2}}[1,-1,0]^\\top$.\n\nCase 1: $A = [c_1, w]$, $x_{\\text{true}} = [2, 1]^\\top$, $r = 10^{-6}$.\nThe matrix $A$ has orthonormal columns. Thus, $A^\\top A = I$, where $I$ is the $2 \\times 2$ identity matrix. The condition number of $A$ is $\\kappa_2(A) = 1$. The system is perfectly well-conditioned.\n$b = A x_{\\text{true}} = 2c_1 + 1w$. Since $b$ is constructed to be in the column space of $A$ and $A$ has full rank, $x_{LS}(b) = x_{\\text{true}}$.\n$\\lVert x_{LS}(b) \\rVert_2 = \\lVert x_{\\text{true}} \\rVert_2 = \\sqrt{2^2 + 1^2} = \\sqrt{5}$.\nThe perturbation is $\\Delta b = r \\lVert b \\rVert_2 w$.\nThe change in solution is $\\Delta x = A^\\dagger \\Delta b = (A^\\top A)^{-1} A^\\top \\Delta b = A^\\top \\Delta b$.\n$\\Delta x = A^\\top (r \\lVert b \\rVert_2 w) = r \\lVert b \\rVert_2 A^\\top w = r \\lVert b \\rVert_2 \\begin{pmatrix} c_1^\\top w \\\\ w^\\top w \\end{pmatrix} = r \\lVert b \\rVert_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n$\\lVert \\Delta x \\rVert_2 = r \\lVert b \\rVert_2$.\n$\\lVert b \\rVert_2 = \\lVert 2c_1 + w \\rVert_2 = \\sqrt{4\\lVert c_1 \\rVert_2^2 + \\lVert w \\rVert_2^2} = \\sqrt{4+1} = \\sqrt{5}$.\nSo, $\\lVert \\Delta x \\rVert_2 = r\\sqrt{5}$.\nThe amplification factor is $R = \\frac{\\lVert \\Delta x \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{r} = \\frac{(r\\sqrt{5}) / \\sqrt{5}}{r} = 1$.\n\nCase 2 and 3: $A = [c_1, c_1 + \\epsilon w]$ with $\\epsilon = 10^{-3}$ and $\\epsilon = 10^{-9}$.\nThe columns of $A$ are nearly collinear as $\\epsilon \\to 0$. The matrix $A$ becomes increasingly ill-conditioned.\nFrom analysis, $x_{LS}(b) = x_{\\text{true}}$.\nThe change in solution is $\\Delta x = \\frac{r}{\\epsilon}\\sqrt{9+\\epsilon^2} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nThe norm is $\\lVert \\Delta x \\rVert_2 = \\frac{r\\sqrt{2}}{\\epsilon}\\sqrt{9+\\epsilon^2}$.\nThen $R = \\frac{\\lVert \\Delta x \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{r} = \\frac{(\\frac{r\\sqrt{2}}{\\epsilon}\\sqrt{9+\\epsilon^2}) / \\sqrt{5}}{r} = \\frac{\\sqrt{2}\\sqrt{9+\\epsilon^2}}{\\epsilon\\sqrt{5}}$.\nAs $\\epsilon \\to 0$, $R$ grows proportionally to $1/\\epsilon$. For small $\\epsilon$, $R \\approx \\frac{3\\sqrt{2}}{\\epsilon\\sqrt{5}}$. This demonstrates extreme sensitivity to perturbations for ill-conditioned systems.\n\nCase 4: $A = [c_1, c_1]$.\nThe matrix $A$ is rank-deficient with $\\text{rank}(A)=1$. The columns are exactly collinear ($\\epsilon = 0$).\n$b = A x_{\\text{true}} = 2c_1 + 1c_1 = 3c_1$. $b$ is in the column space of $A$.\nThe set of exact solutions to $Ax=b$ is the line $x_1+x_2 = 3$. The minimum-norm solution is the point on this line closest to the origin, which is $x_{LS}(b) = [1.5, 1.5]^\\top$.\nThe perturbation is $\\Delta b = r \\lVert b \\rVert_2 w = 3rw$. This perturbation vector $w$ is orthogonal to the column space of $A$, which is $\\text{span}\\{c_1\\}$.\nThe least-squares solution projects the right-hand side onto the column space of $A$.\n$P_A(b+\\Delta b) = P_A(b) + P_A(\\Delta b) = b + 0 = b$.\nThe problem of finding $x_{LS}(b+\\Delta b)$ is therefore equivalent to finding $x_{LS}(b)$, because the perturbation is projected out.\nThus, $x_{LS}(b+\\Delta b) = x_{LS}(b) = [1.5, 1.5]^\\top$.\nThis means $\\Delta x = 0$, and consequently $\\lVert \\Delta x \\rVert_2 = 0$.\nThe amplification factor is $R = \\frac{0 / \\lVert x_{LS}(b) \\rVert_2}{r} = 0$.\nThe value $R=0$ shows a discontinuity from the limit of Case 2/3 as $\\epsilon \\to 0$. This occurs because for any $\\epsilon > 0$, the column space of $A$ is $\\text{span}\\{c_1, w\\}$, and the perturbation $\\Delta b \\propto w$ lies within it. At $\\epsilon = 0$, the column space collapses to $\\text{span}\\{c_1\\}$, and the same perturbation $\\Delta b$ becomes orthogonal to it. The behavior of the pseudoinverse solution is fundamentally different for perturbations inside versus outside the column space. The problem statement is valid and this result is correct.\n\nThe implementation will compute these values numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Evaluates the sensitivity of the linear least-squares minimizer to perturbations\n    for a family of matrices with increasingly collinear columns.\n    \"\"\"\n    \n    # Define the orthonormal basis vectors\n    c1 = (1 / np.sqrt(2)) * np.array([1.0, 1.0, 0.0])\n    w = (1 / np.sqrt(2)) * np.array([1.0, -1.0, 0.0])\n    \n    # Define x_true and r as specified in the problem\n    x_true = np.array([2.0, 1.0])\n    r = 1e-6\n    \n    # Test cases parameters: (name, epsilon)\n    # Epsilon defines the matrix A. A special value of None is used for the first case.\n    test_params = [\n        (\"well-conditioned baseline\", None),\n        (\"moderate near collinearity\", 1e-3),\n        (\"extreme near collinearity\", 1e-9),\n        (\"exact collinearity\", 0.0)\n    ]\n    \n    results = []\n\n    # Small constant to prevent division by zero, as per problem instructions\n    ZERO_GUARD = 1e-15\n\n    for name, epsilon in test_params:\n        # Step 1: Construct matrix A\n        if epsilon is None:\n            # Case 1: A = [c1, w]\n            A = np.c_[c1, w]\n        else:\n            # Cases 2, 3, 4: A = [c1, c1 + epsilon * w]\n            col2 = c1 + epsilon * w\n            A = np.c_[c1, col2]\n            \n        # Step 2: Form b and the perturbation delta_b\n        b = A @ x_true\n        \n        norm_b = np.linalg.norm(b)\n        safe_norm_b = norm_b if norm_b > ZERO_GUARD else ZERO_GUARD\n        \n        # Perturbation is in the direction of w\n        delta_b = r * norm_b * w\n        \n        b_perturbed = b + delta_b\n\n        # Step 3: Compute the least-squares solutions\n        # Use rcond=None to adopt the new default behavior of numpy.linalg.lstsq\n        x_ls_b = np.linalg.lstsq(A, b, rcond=None)[0]\n        x_ls_b_perturbed = np.linalg.lstsq(A, b_perturbed, rcond=None)[0]\n        \n        # Step 4: Compute the amplification factor R\n        delta_x = x_ls_b_perturbed - x_ls_b\n        \n        norm_delta_x = np.linalg.norm(delta_x)\n        norm_x_ls_b = np.linalg.norm(x_ls_b)\n        \n        safe_norm_x_ls_b = norm_x_ls_b if norm_x_ls_b > ZERO_GUARD else ZERO_GUARD\n\n        # The relative error in x\n        rel_err_x = norm_delta_x / safe_norm_x_ls_b\n        \n        # The relative error in b is, by definition, r.\n        # R = rel_err_x / (||delta_b|| / ||b||) = rel_err_x / r\n        # We must handle the case where r is zero, though problem states r=10^-6\n        safe_r = r if r > ZERO_GUARD else ZERO_GUARD\n        \n        R = rel_err_x / safe_r\n        \n        results.append(R)\n\n    # Final print statement in the exact required format.\n    # The output format is a list of strings representing the numbers.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2409733"}]}