{"hands_on_practices": [{"introduction": "Many problems in engineering and science are 'ill-posed,' meaning small changes in input data can cause large, unphysical swings in the solution. This exercise introduces Tikhonov regularization, a powerful technique to combat this instability by reformulating the problem into an augmented least squares system. By applying QR factorization to this augmented system [@problem_id:2430326], you will learn a numerically robust method to find stable and meaningful solutions to otherwise intractable problems.", "problem": "You are to implement a program that computes Tikhonov-regularized least squares solutions using a factorization based on the orthogonal-triangular decomposition, also known as the QR decomposition. The problem addresses ill-posed or ill-conditioned linear systems by solving a regularized least squares problem via an augmented overdetermined system.\n\nStarting point and fundamental base: The regularized least squares problem seeks, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$, data vector $b \\in \\mathbb{R}^{m}$, and regularization parameter $\\lambda \\ge 0$, a vector $x \\in \\mathbb{R}^{n}$ that minimizes the objective function $\\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2$, where $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. This can be reformulated as a standard least squares problem by considering the augmented system $\\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x \\approx \\begin{pmatrix} b \\\\ 0 \\end{pmatrix}$, where $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix and $0 \\in \\mathbb{R}^{n}$ is the zero vector.\n\nYour task is to:\n1. Implement a function that, given $A$, $b$, and $\\lambda$, constructs the augmented matrix $C = \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix}$ and the augmented right-hand side $d = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix}$, computes the economy-size QR factorization $C = Q R$ with $Q \\in \\mathbb{R}^{(m+n) \\times n}$ having orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ upper triangular, and then solves the triangular system $R x = Q^\\top d$ for $x$ by back substitution. Do not form or solve the normal equations, and do not use singular value decomposition.\n2. For each test case listed below, compute the Euclidean norm of the augmented residual, defined as $\\lVert C x - d \\rVert_2 = \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2$, using the solution $x$ obtained by your QR-based method.\n3. Produce a single line of output containing the results as a comma-separated list of floats, enclosed in square brackets, with each float rounded to six decimal places, in the order of the test cases described below.\n\nAlgorithmic constraints:\n- Use an orthogonal-triangular decomposition applied to the augmented system, followed by back substitution on the triangular factor. This leverages orthogonality to avoid squaring the condition number, which would occur when forming normal equations.\n\nTest suite:\nImplement and evaluate your solver on the following four deterministic test cases. In all definitions below, indices start at $0$.\n\n- Test case $1$ (overdetermined, ill-conditioned, unregularized):\n  - Dimensions: $m = 30$, $n = 10$.\n  - Matrix $A \\in \\mathbb{R}^{30 \\times 10}$ defined by entries $A_{i,j} = \\dfrac{1}{i + j + 1}$ for $0 \\le i \\le 29$ and $0 \\le j \\le 9$.\n  - True vector $x_{\\text{true}} \\in \\mathbb{R}^{10}$ defined by $(x_{\\text{true}})_j = 1$ if $j$ is even and $(x_{\\text{true}})_j = -1$ if $j$ is odd, for $0 \\le j \\le 9$.\n  - Noise vector $\\varepsilon \\in \\mathbb{R}^{30}$ defined by $\\varepsilon_i = 10^{-4} \\cos(i)$ for $0 \\le i \\le 29$.\n  - Right-hand side $b = A x_{\\text{true}} + \\varepsilon$.\n  - Regularization parameter $\\lambda = 0$.\n\n- Test case $2$ (same system as test case $1$ but with regularization):\n  - Use the same $A$, $x_{\\text{true}}$, $\\varepsilon$, and $b$ as in test case $1$.\n  - Regularization parameter $\\lambda = 10^{-2}$.\n\n- Test case $3$ (square, rank-deficient, exact data, slight regularization):\n  - Dimensions: $m = 8$, $n = 6$.\n  - Construct columns $c_k \\in \\mathbb{R}^{8}$ as follows:\n    - $c_0$ is the vector with entries $c_{0,i} = 1 + \\dfrac{i}{7}$ for $0 \\le i \\le 7$.\n    - $c_1 = c_0$ (duplicate column to ensure rank deficiency).\n    - $c_2$ has entries $c_{2,i} = \\sin\\!\\left(\\dfrac{\\pi i}{7}\\right)$.\n    - $c_3$ has entries $c_{3,i} = \\cos\\!\\left(\\dfrac{\\pi i}{3}\\right)$.\n    - $c_4 = c_2 + c_3$.\n    - $c_5$ is the constant vector with entries $1$ for all $i$.\n  - Form $A = [c_0\\, c_1\\, c_2\\, c_3\\, c_4\\, c_5] \\in \\mathbb{R}^{8 \\times 6}$.\n  - Define $x_{\\text{true}} \\in \\mathbb{R}^{6}$ with entries $(x_{\\text{true}})_j = [0.5, -0.5, 1.0, -1.0, 0.0, 2.0]_j$ for $0 \\le j \\le 5$.\n  - Right-hand side $b = A x_{\\text{true}}$.\n  - Regularization parameter $\\lambda = 10^{-6}$.\n\n- Test case $4$ (underdetermined, noisy, regularized):\n  - Dimensions: $m = 5$, $n = 10$.\n  - Matrix $A \\in \\mathbb{R}^{5 \\times 10}$ with entries $A_{i,j} = \\sin(i + j) + \\cos(2 i + 3 j)$ for $0 \\le i \\le 4$ and $0 \\le j \\le 9$, where the trigonometric functions take arguments in radians.\n  - True vector $x_{\\text{true}} \\in \\mathbb{R}^{10}$ with entries $(x_{\\text{true}})_j = \\sin(j)$ for $0 \\le j \\le 9$.\n  - Noise vector $\\varepsilon \\in \\mathbb{R}^{5}$ with entries $\\varepsilon_i = 10^{-5} \\sin(i)$ for $0 \\le i \\le 4$.\n  - Right-hand side $b = A x_{\\text{true}} + \\varepsilon$.\n  - Regularization parameter $\\lambda = 10^{-1}$.\n\nOutput specification:\n- For each test case $k \\in \\{1,2,3,4\\}$, compute the scalar $r_k = \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2$ using your QR-based solution $x$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[r_1, r_2, r_3, r_4]$, with each $r_k$ rounded to six decimal places, for example, $[0.123456,0.234567,0.345678,0.456789]$.\n\nAngle units:\n- All trigonometric functions use radian arguments.\n\nNo physical units are involved in this problem.", "solution": "The problem presented is valid and well-posed. It addresses a fundamental task in computational science: the stable solution of ill-conditioned or underdetermined linear systems using Tikhonov regularization. The specified methodology, which relies on the orthogonal-triangular ($QR$) decomposition of an augmented system, is a standard and numerically robust technique.\n\nThe objective is to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the Tikhonov-regularized objective function:\n$$ \\underset{x}{\\text{minimize}} \\quad \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2 $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the system matrix, $b \\in \\mathbb{R}^{m}$ is the data vector, and $\\lambda \\ge 0$ is the regularization parameter. The term $\\lambda^2 \\lVert x \\rVert_2^2$ penalizes solutions with large norms, which is crucial for stabilizing the problem when $A$ is ill-conditioned or rank-deficient.\n\nThis objective function can be expressed as the squared Euclidean norm of a single vector, which transforms the problem into a standard linear least squares format. We construct an augmented matrix $C \\in \\mathbb{R}^{(m+n) \\times n}$ and an augmented vector $d \\in \\mathbb{R}^{m+n}$:\n$$ C = \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix}, \\quad d = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} $$\nHere, $I$ is the $n \\times n$ identity matrix, and $0$ is the zero vector in $\\mathbb{R}^{n}$. The original minimization problem is equivalent to:\n$$ \\underset{x}{\\text{minimize}} \\quad \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 = \\underset{x}{\\text{minimize}} \\quad \\lVert C x - d \\rVert_2^2 $$\nThe solution $x$ to this least squares problem satisfies the normal equations:\n$$ C^\\top C x = C^\\top d $$\nHowever, the problem statement correctly forbids forming the matrix $C^\\top C$ explicitly. This is because the condition number of $C^\\top C$ is the square of the condition number of $C$, i.e., $\\kappa(C^\\top C) = \\kappa(C)^2$. Forming the normal equations can therefore introduce unnecessary numerical instability, especially for ill-conditioned systems.\n\nA numerically superior method is to use an orthogonal-triangular decomposition of $C$. We compute the economy-size $QR$ factorization of the augmented matrix $C$:\n$$ C = QR $$\nwhere $Q \\in \\mathbb{R}^{(m+n) \\times n}$ is a matrix with orthonormal columns (i.e., $Q^\\top Q = I_n$), and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix.\n\nSubstituting $C = QR$ into the least squares problem, we seek to minimize $\\lVert QRx - d \\rVert_2$. Since $Q$ has orthonormal columns, multiplying by $Q^\\top$ preserves the Euclidean norm of the residual component in the column space of $Q$. The solution $x$ is found by solving the transformed system. Substituting into the normal equations gives:\n$$ (QR)^\\top (QR) x = (QR)^\\top d $$\n$$ R^\\top Q^\\top Q R x = R^\\top Q^\\top d $$\n$$ R^\\top R x = R^\\top Q^\\top d $$\nFor the problem to have a unique solution, the matrix $R$ must be invertible. If $\\lambda > 0$, the matrix $C$ is guaranteed to have full column rank, even if $A$ does not. This is because the rows corresponding to $\\lambda I$ ensure linear independence of the columns of $C$. Consequently, the upper triangular matrix $R$ will be full rank and thus invertible. If $\\lambda = 0$ and $A$ has full column rank, $R$ is also invertible. In these cases, we can multiply by $(R^\\top)^{-1}$ to obtain the upper triangular system:\n$$ R x = Q^\\top d $$\nThis system is solved efficiently for $x$ using back substitution.\n\nOnce the solution vector $x$ is computed, the final step is to calculate the Euclidean norm of the augmented residual, as required. This is computed directly:\n$$ r = \\lVert C x - d \\rVert_2 = \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2 $$\nThis procedure is applied to each of the four specified test cases. The constructions of the matrices and vectors for each case are executed precisely as defined in the problem statement. The final output consists of the computed residual norms.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve_regularized_ls_qr(A, b, lam):\n    \"\"\"\n    Solves the Tikhonov-regularized least squares problem min ||Ax-b||^2 + lam^2||x||^2\n    using QR factorization of the augmented system.\n\n    Args:\n        A (np.ndarray): The matrix A of shape (m, n).\n        b (np.ndarray): The vector b of shape (m,).\n        lam (float): The regularization parameter lambda.\n\n    Returns:\n        float: The Euclidean norm of the augmented residual, ||Cx - d||_2.\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Construct the augmented matrix C and augmented vector d.\n    if lam > 0:\n        C = np.vstack((A, lam * np.eye(n)))\n    else:\n        # Handle lambda = 0 case to avoid creating a zero matrix and then multiplying\n        C = np.vstack((A, np.zeros((n, n))))\n        \n    d = np.hstack((b, np.zeros(n)))\n\n    # 2. Compute the economy-size QR factorization of C.\n    Q, R = np.linalg.qr(C, mode='reduced')\n\n    # 3. Solve the upper triangular system Rx = Q^T d by back substitution.\n    qT_d = Q.T @ d\n    # SciPy's solver is efficient for this.\n    x = solve_triangular(R, qT_d, check_finite=False)\n\n    # 4. Compute the Euclidean norm of the augmented residual.\n    residual_norm = np.linalg.norm(C @ x - d)\n    \n    return residual_norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = []\n\n    # Test case 1: overdetermined, ill-conditioned, unregularized\n    m1, n1 = 30, 10\n    lam1 = 0.0\n    i1 = np.arange(m1)[:, np.newaxis]\n    j1 = np.arange(n1)\n    A1 = 1.0 / (i1 + j1 + 1)\n    x_true1 = np.array([(-1)**k for k in range(n1)])\n    epsilon1 = 1e-4 * np.cos(np.arange(m1))\n    b1 = A1 @ x_true1 + epsilon1\n    test_cases.append({'A': A1, 'b': b1, 'lam': lam1})\n\n    # Test case 2: same system as test case 1 but with regularization\n    lam2 = 1e-2\n    test_cases.append({'A': A1, 'b': b1, 'lam': lam2})\n\n    # Test case 3: square, rank-deficient, exact data, slight regularization\n    m3, n3 = 8, 6\n    lam3 = 1e-6\n    i3 = np.arange(m3)\n    c0 = 1 + i3 / 7.0\n    c1 = c0\n    c2 = np.sin(np.pi * i3 / 7.0)\n    c3 = np.cos(np.pi * i3 / 3.0)\n    c4 = c2 + c3\n    c5 = np.ones(m3)\n    A3 = np.column_stack([c0, c1, c2, c3, c4, c5])\n    x_true3 = np.array([0.5, -0.5, 1.0, -1.0, 0.0, 2.0])\n    b3 = A3 @ x_true3\n    test_cases.append({'A': A3, 'b': b3, 'lam': lam3})\n\n    # Test case 4: underdetermined, noisy, regularized\n    m4, n4 = 5, 10\n    lam4 = 1e-1\n    i4 = np.arange(m4)[:, np.newaxis]\n    j4 = np.arange(n4)\n    A4 = np.sin(i4 + j4) + np.cos(2 * i4 + 3 * j4)\n    x_true4 = np.sin(np.arange(n4))\n    epsilon4 = 1e-5 * np.sin(np.arange(m4))\n    b4 = A4 @ x_true4 + epsilon4\n    test_cases.append({'A': A4, 'b': b4, 'lam': lam4})\n\n    results = []\n    for case in test_cases:\n        residual = solve_regularized_ls_qr(case['A'], case['b'], case['lam'])\n        results.append(residual)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2430326"}, {"introduction": "Predicting future behavior from past observations is a cornerstone of computational engineering, from financial modeling to control systems. This practice challenges you to fit an autoregressive (AR) model to time-series data, a task that translates directly into a least squares problem [@problem_id:2430292]. You will construct the necessary data matrices and employ QR factorization with column pivoting, a vital technique for handling the near-collinearity and potential rank deficiency often found in real-world data.", "problem": "You are given time series data and asked to fit an autoregressive (AR) model using a numerically stable linear least squares formulation based on orthogonal-triangular (QR) factorization with column pivoting. The model is the autoregressive (AR) model of order $p$, defined by\n$$\nx_t \\;=\\; \\sum_{i=1}^{p} \\phi_i \\, x_{t-i} \\;+\\; \\varepsilon_t,\n$$\nwhere $x_t$ is the observed scalar time series, $\\phi_i$ are the AR coefficients to be estimated, and $\\varepsilon_t$ is an unobserved disturbance.\n\nFundamental starting points:\n- The least squares estimate minimizes the sum of squared residuals\n$$\n\\min_{\\phi \\in \\mathbb{R}^p} \\;\\left\\| H \\, \\phi - y \\right\\|_2^2,\n$$\nwhere $H$ is the data matrix of lagged observations and $y$ is the vector of current observations. Specifically, for a series $\\{x_1,\\dots,x_N\\}$ and order $p$, define $m = N - p$, then for each $t \\in \\{p+1,\\dots,N\\}$ the row $t-p$ of $H \\in \\mathbb{R}^{m \\times p}$ is $[x_{t-1}, x_{t-2}, \\dots, x_{t-p}]$ and the corresponding entry of $y \\in \\mathbb{R}^{m}$ is $x_t$.\n- Orthogonal-triangular (QR) factorization with column pivoting is a well-tested and numerically stable approach to solve least squares problems, including rank-deficient cases. If $H \\, P = Q \\, R$ with $Q$ orthonormal, $R$ upper-triangular, and $P$ a column permutation, then the least squares solution with minimal Euclidean norm in rank-deficient cases is obtained by truncating to the numerical rank.\n\nYour task is to write a complete program that:\n1. Constructs $H$ and $y$ from each provided time series $\\{x_t\\}$ and AR order $p$ as described above.\n2. Computes the least squares AR coefficient vector $\\hat{\\phi}$ using QR factorization with column pivoting. Determine the numerical rank $r$ by a tolerance\n$$\n\\tau \\;=\\; \\max(m,p)\\, \\epsilon \\, |R_{11}|,\n$$\nwhere $\\epsilon$ is the double precision machine epsilon and $R_{11}$ is the top-left entry of $R$. If $|R_{11}| = 0$, take $r = 0$. If $r > 0$, let $R_1 \\in \\mathbb{R}^{r \\times r}$ be the leading triangular block and $Q_1 \\in \\mathbb{R}^{m \\times r}$ the corresponding orthonormal columns; then the minimal-norm solution is\n$$\n\\hat{z}_1 \\;=\\; R_1^{-1} \\, Q_1^\\top \\, y, \n\\quad\n\\hat{z}_2 \\;=\\; 0 \\in \\mathbb{R}^{p-r},\n\\quad\n\\hat{z} \\;=\\; \\begin{bmatrix}\\hat{z}_1 \\\\ \\hat{z}_2\\end{bmatrix},\n\\quad\n\\hat{\\phi} \\;=\\; P \\, \\hat{z}.\n$$\nIf $r = 0$, set $\\hat{\\phi} = 0 \\in \\mathbb{R}^p$.\n3. For each test case, output the estimated coefficient vector $\\hat{\\phi}$, with each component rounded to six decimal places.\n\nTest suite:\n- Test case $1$ (general well-conditioned case, exact AR of order $2$):\n  - Order: $p = 2$.\n  - Time series $\\{x_t\\}_{t=1}^{7}$: $[100.0, 110.0, 80.0, 67.5, 53.75, 43.75, 35.3125]$.\n- Test case $2$ (boundary case with minimal sample for $p = 1$):\n  - Order: $p = 1$.\n  - Time series $\\{x_t\\}_{t=1}^{2}$: $[100.0, 102.0]$.\n- Test case $3$ (nearly rank-deficient design for $p = 2$, geometric growth):\n  - Order: $p = 2$.\n  - Time series $\\{x_t\\}_{t=1}^{8}$: $[100.0, 102.0, 104.04, 106.1208, 108.243216, 110.40808032, 112.6162419264, 114.868566764928]$.\n- Test case $4$ (over-parameterized model $p = 3$ fitted to exact first-order dynamics, rank-deficient):\n  - Order: $p = 3$.\n  - Time series $\\{x_t\\}_{t=1}^{8}$: $[100.0, 50.0, 25.0, 12.5, 6.25, 3.125, 1.5625, 0.78125]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the coefficient vectors for the four test cases, as a comma-separated list enclosed in square brackets. Each coefficient vector must itself be a bracketed, comma-separated list with each floating-point entry rounded to six decimal places. For example:\n$ [\\,[\\phi^{(1)}_1,\\dots,\\phi^{(1)}_{p_1}],\\,[\\phi^{(2)}_1,\\dots],\\,[\\phi^{(3)}_1,\\dots],\\,[\\phi^{(4)}_1,\\dots]\\,] $.\n- Concretely, the program must print exactly one line of the form\n$ [\\,[\\cdot,\\cdot],\\,[\\cdot],\\,[\\cdot,\\cdot],\\,[\\cdot,\\cdot,\\cdot]\\,] $,\nwith no spaces between numbers or brackets and each number formatted to six decimal places.", "solution": "The problem requires the estimation of coefficients for an autoregressive (AR) model of order $p$, specified as\n$$\nx_t = \\sum_{i=1}^{p} \\phi_i x_{t-i} + \\varepsilon_t\n$$\nwhere $\\{x_t\\}$ is a time series, $\\phi = [\\phi_1, \\dots, \\phi_p]^\\top$ is the vector of coefficients to be determined, and $\\varepsilon_t$ is a random disturbance term. The estimation is formulated as a linear least squares problem, which must be solved using QR factorization with column pivoting for numerical stability.\n\nFirst, we formalize the problem. Given a time series of length $N$, $\\{x_1, \\dots, x_N\\}$, and an AR order $p$, we construct a linear system. Let $m = N - p$. If $m \\le 0$, there is insufficient data to form the problem; all provided test cases satisfy $m > 0$. We define a response vector $y \\in \\mathbb{R}^m$ and a data matrix $H \\in \\mathbb{R}^{m \\times p}$. The vector $y$ consists of the observations to be predicted, and the matrix $H$ contains the lagged observations used as predictors.\n$$\ny = \\begin{bmatrix} x_{p+1} \\\\ x_{p+2} \\\\ \\vdots \\\\ x_N \\end{bmatrix}, \\quad\nH = \\begin{bmatrix}\nx_p & x_{p-1} & \\cdots & x_1 \\\\\nx_{p+1} & x_p & \\cdots & x_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{N-1} & x_{N-2} & \\cdots & x_{N-p}\n\\end{bmatrix}\n$$\nThe AR model can then be expressed in matrix form as $y \\approx H\\phi$. The least squares estimate $\\hat{\\phi}$ is the vector that minimizes the sum of squared residuals, which is the squared Euclidean norm of the residual vector:\n$$\n\\hat{\\phi} = \\arg\\min_{\\phi \\in \\mathbb{R}^p} \\| H\\phi - y \\|_2^2\n$$\nA common method to solve this is via the normal equations, $H^\\top H \\phi = H^\\top y$. However, this is numerically unwise. The condition number of $H^\\top H$ is the square of the condition number of $H$, $\\kappa(H^\\top H) = \\kappa(H)^2$. If $H$ is ill-conditioned, $H^\\top H$ will be even more so, leading to large errors in the computed solution due to floating-point arithmetic.\n\nA numerically superior approach is to use an orthogonal factorization of $H$. The problem specifies QR factorization with column pivoting. This computes the decomposition $HP = QR$, where $P$ is a $p \\times p$ permutation matrix, $Q$ is an $m \\times m$ orthogonal matrix (its columns are orthonormal, $Q^\\top Q=I$), and $R$ is an $m \\times p$ upper trapezoidal matrix. The permutation matrix $P$ is chosen to ensure that the diagonal elements of $R$ are non-increasing in magnitude, which is crucial for reliably determining the numerical rank of $H$.\n\nSubstituting $H = QRP^{-1}$ into the objective function, and using the fact that the Euclidean norm is invariant under orthogonal transformations (i.e., $\\|Qz\\|_2 = \\|z\\|_2$), we get:\n$$\n\\| H\\phi - y \\|_2^2 = \\| QRP^{-1}\\phi - y \\|_2^2 = \\| Q(RP^{-1}\\phi - Q^\\top y) \\|_2^2 = \\| RP^{-1}\\phi - Q^\\top y \\|_2^2\n$$\nLet $z = P^{-1}\\phi$, which implies $\\phi = Pz$. The vector $z$ represents the coefficients corresponding to the permuted columns of $H$. The problem becomes minimizing $\\|Rz - Q^\\top y\\|_2^2$.\n\nThe matrix $H$ may be rank-deficient or nearly so, especially if the underlying time series exhibits strong trends or periodicities, causing the lagged observation vectors (columns of $H$) to be nearly linearly dependent. Column-pivoted QR is robust to this situation. We determine the numerical rank $r$ of $H$ by examining the diagonal elements of $R$. A tolerance $\\tau = \\max(m,p) \\cdot \\epsilon \\cdot |R_{11}|$ is used, where $\\epsilon$ is the machine epsilon. The rank $r$ is the number of diagonal elements $|R_{ii}|$ such that $|R_{ii}| > \\tau$. If $|R_{11}|=0$, the matrix is zero, so $r=0$.\n\nBased on rank $r$, we partition $R$, $z$, and $c = Q^\\top y$:\n$$\nR = \\begin{bmatrix} R_{11} & R_{12} \\\\ 0 & R_{22} \\end{bmatrix}, \\quad z = \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}, \\quad c = Q^\\top y = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}\n$$\nHere, $R_{11}$ is an $r \\times r$ upper-triangular matrix with non-zero diagonal elements, $z_1 \\in \\mathbb{R}^r$, and $c_1 \\in \\mathbb{R}^r$. The norm to be minimized becomes:\n$$\n\\| Rz - c \\|_2^2 = \\| R_{11}z_1 + R_{12}z_2 - c_1 \\|_2^2 + \\| R_{22}z_2 - c_2 \\|_2^2\n$$\nThe diagonal elements of the $(m-r) \\times (p-r)$ block $R_{22}$ are numerically zero (less than or equal to $\\tau$). For a minimal-norm solution, we set $z_2 = 0 \\in \\mathbb{R}^{p-r}$. This choice nullifies the contribution of the most uncertain components. The problem reduces to solving the full-rank upper-triangular system for $z_1$:\n$$\nR_{11} z_1 = c_1\n$$\nThis system is solved efficiently via back substitution. The resulting solution for the permuted coefficients is $\\hat{z} = [\\hat{z}_1^\\top, 0, \\dots, 0]^\\top \\in \\mathbb{R}^p$.\n\nFinally, the coefficient vector $\\hat{\\phi}$ for the original, unpermuted model is recovered by applying the permutation $P$. If the permutation indices from the QR routine are `piv`, such that `H_perm = H[:, piv]`, then the relationship between the coefficients is $\\hat{\\phi}_{\\text{piv}[k]} = \\hat{z}_k$.\nIn the edge case where the numerical rank $r=0$ (e.g., $H$ is a zero matrix), the minimal norm solution is simply $\\hat{\\phi}=0$.\n\nThe overall algorithm is:\n$1$. From time series $\\{x_t\\}$ and order $p$, construct the matrix $H$ and vector $y$.\n$2$. Compute the QR factorization with column pivoting: $H P = Q R$. Most numerical libraries return $Q$, $R$, and a permutation index vector.\n$3$. Determine the numerical rank $r$ using the specified tolerance $\\tau$.\n$4$. If $r=0$, set $\\hat{\\phi}=0$.\n$5$. If $r>0$, solve the $r \\times r$ triangular system $R_{1:r, 1:r} \\, \\hat{z}_{1:r} = (Q^\\top y)_{1:r}$ for $\\hat{z}_{1:r}$.\n$6$. Construct $\\hat{z} \\in \\mathbb{R}^p$ by padding $\\hat{z}_{1:r}$ with $p-r$ zeros.\n$7$. Apply the inverse permutation to $\\hat{z}$ to obtain the final coefficient vector $\\hat{\\phi}$.\nThis procedure yields a robust, numerically stable estimate of the AR coefficients, providing the unique minimal-norm solution in cases of rank deficiency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the autoregressive model fitting problem\n    for all test cases as specified.\n    \"\"\"\n\n    # Test cases as defined in the problem description.\n    test_cases = [\n        # (order p, time series x_t)\n        (2, [100.0, 110.0, 80.0, 67.5, 53.75, 43.75, 35.3125]),\n        (1, [100.0, 102.0]),\n        (2, [100.0, 102.0, 104.04, 106.1208, 108.243216, 110.40808032, 112.6162419264, 114.868566764928]),\n        (3, [100.0, 50.0, 25.0, 12.5, 6.25, 3.125, 1.5625, 0.78125]),\n    ]\n\n    results = []\n    for p, x_ts in test_cases:\n        phi_hat = estimate_ar_coeffs(np.array(x_ts, dtype=float), p)\n        results.append(phi_hat)\n\n    # Format the final output string as required.\n    # e.g., [[c1,c2],[c1],[c1,c2],[c1,c2,c3]]\n    # with 6 decimal places and no spaces.\n    output_parts = []\n    for res in results:\n        formatted_coeffs = \",\".join([f\"{c:.6f}\" for c in res])\n        output_parts.append(f\"[{formatted_coeffs}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\ndef estimate_ar_coeffs(x, p):\n    \"\"\"\n    Estimates AR(p) coefficients for a time series x using QR factorization\n    with column pivoting.\n\n    Args:\n        x (np.ndarray): The time series data.\n        p (int): The order of the autoregressive model.\n\n    Returns:\n        np.ndarray: The estimated AR coefficients phi_hat.\n    \"\"\"\n    N = len(x)\n    m = N - p\n\n    if m <= 0:\n        # Not enough data points to form the least squares problem.\n        # This case is not in the test suite but is a necessary check.\n        return np.zeros(p)\n\n    # 1. Construct the Hankel matrix H and response vector y.\n    # H is m x p, y is m x 1\n    H = np.zeros((m, p))\n    y = x[p:]\n    \n    for i in range(m):\n        # The i-th row of H corresponds to predicting x_{p+i+1}\n        # using [x_{p+i}, x_{p+i-1}, ..., x_{i+1}]\n        # In array indexing, this is y[i] from x[p+i-1:i-1:-1]\n        H[i, :] = x[i:i+p][::-1]\n\n    # 2. Compute QR factorization with column pivoting: H P = Q R\n    Q, R, piv = qr(H, pivoting=True)\n    \n    # 3. Determine numerical rank r.\n    m, p_eff = H.shape\n    eps = np.finfo(float).eps\n    \n    # Check for the case where H is the zero matrix.\n    if np.abs(R[0, 0]) == 0:\n        r = 0\n    else:\n        tau = np.max(H.shape) * eps * np.abs(R[0, 0])\n        diag_R = np.abs(np.diag(R))\n        r = np.sum(diag_R > tau)\n    \n    if r == 0:\n        # If rank is 0, the minimal norm solution is phi_hat = 0.\n        return np.zeros(p)\n\n    # 4. Solve the least squares problem using the factorization.\n    # We want to solve H * phi = y, which is (Q R P^T) * phi = y\n    # Let z = P^T * phi. Then Q R z = y, or R z = Q^T y.\n    # Scipy's output `piv` is such that H[:,piv] = Q @ R.\n    # We solve for z in (H[:,piv]) @ z = y.\n    # The coefficients are then `phi[piv] = z`.\n\n    # Calculate c = Q^T * y\n    c = Q.T @ y\n\n    # 5. Solve the upper triangular system for the first r components of z.\n    # R_sub @ z_sub = c_sub\n    R1 = R[:r, :r]\n    c1 = c[:r]\n    z_hat_r = solve_triangular(R1, c1, lower=False)\n\n    # 6. Construct full z_hat vector with zeros for components beyond rank r.\n    z_hat = np.zeros(p)\n    z_hat[:r] = z_hat_r\n    \n    # 7. Apply inverse permutation to get phi_hat.\n    # The `piv` array maps new column index `j` to old column index `piv[j]`.\n    # z_hat[j] is the coefficient for old column `piv[j]`.\n    # So, phi_hat[piv[j]] = z_hat[j]\n    phi_hat = np.zeros(p)\n    phi_hat[piv] = z_hat\n        \n    return phi_hat\n\nsolve()\n```", "id": "2430292"}, {"introduction": "A computed solution is only as reliable as its sensitivity to measurement errors. This exercise provides a method to precisely quantify how a single outlier in your data vector can perturb the least squares solution [@problem_id:2430336]. By deriving a sensitivity factor directly from the QR factorization of the system matrix, you will gain critical insight into the stability of a linear system and appreciate why robust numerical methods are indispensable.", "problem": "You are given a set of independent linear regression test cases, each described by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank, a baseline measurement vector $b \\in \\mathbb{R}^{m}$, an index $p \\in \\{0,1,\\dots,m-1\\}$ interpreted with zero-based indexing, and a scalar outlier magnitude $\\delta \\in \\mathbb{R}$. For each test case, consider the baseline least squares (LS) solution $x \\in \\mathbb{R}^{n}$ that minimizes $\\lVert A x - b \\rVert_{2}$, and the perturbed solution $\\tilde{x} \\in \\mathbb{R}^{n}$ that minimizes $\\lVert A x - \\tilde{b} \\rVert_{2}$ where $\\tilde{b} = b + \\delta e_{p}$ and $e_{p} \\in \\mathbb{R}^{m}$ is the $p$-th canonical basis vector (with $1$ at index $p$ and $0$ elsewhere). Define the sensitivity amplification for the test case as the scalar\n$$\ns \\;=\\; \\frac{\\lVert \\tilde{x} - x \\rVert_{2}}{|\\delta|}.\n$$\nYour task is to compute $s$ for each of the following test cases.\n\nTest case $1$:\n- $A = \\begin{bmatrix}\n1 & 0 & 1\\\\\n0 & 1 & 1\\\\\n1 & 1 & 0\\\\\n2 & 1 & 1\\\\\n1 & 2 & 1\n\\end{bmatrix}$, $b = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4\\\\ 5 \\end{bmatrix}$, $p = 2$, $\\delta = 10$.\n\nTest case $2$:\n- $A = \\begin{bmatrix}\n1 & 1.0000 & 2.0000\\\\\n2 & 2.0005 & 4.0009\\\\\n3 & 3.0008 & 6.0012\\\\\n4 & 4.0011 & 8.0015\\\\\n5 & 5.0014 & 10.0018\n\\end{bmatrix}$, $b = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix}$, $p = 0$, $\\delta = 50$.\n\nTest case $3$:\n- $A = \\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 0\\\\\n0 & 0 & 0\n\\end{bmatrix}$, $b = \\begin{bmatrix} 2\\\\ -1\\\\ 3\\\\ 10\\\\ -100 \\end{bmatrix}$, $p = 4$, $\\delta = 1000$.\n\nTest case $4$:\n- $A = \\begin{bmatrix}\n3 & 0 & 0\\\\\n0 & 2 & 0\\\\\n0 & 0 & 1\\\\\n1 & 1 & 1\\\\\n4 & 5 & 6\n\\end{bmatrix}$, $b = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4\\\\ 5 \\end{bmatrix}$, $p = 1$, $\\delta = 10^{-6}$.\n\nCompute $s$ for each test case as a real number rounded to $6$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example\n[$s_1,s_2,s_3,s_4$].", "solution": "The problem statement must first be validated for scientific and logical consistency.\n\n**Step 1: Extract Givens**\nFor a set of test cases, each is defined by:\n- A matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank.\n- A vector $b \\in \\mathbb{R}^{m}$.\n- A zero-based index $p \\in \\{0, 1, \\dots, m-1\\}$.\n- A scalar perturbation magnitude $\\delta \\in \\mathbb{R}$.\n\nThe following quantities are defined:\n- The baseline least squares solution $x$, which minimizes $\\lVert A x - b \\rVert_{2}$.\n- The perturbed measurement vector $\\tilde{b} = b + \\delta e_{p}$, where $e_p$ is the $p$-th canonical basis vector.\n- The perturbed least squares solution $\\tilde{x}$, which minimizes $\\lVert A x - \\tilde{b} \\rVert_{2}$.\n- The sensitivity amplification $s = \\frac{\\lVert \\tilde{x} - x \\rVert_{2}}{|\\delta|}$.\n\nThe task is to compute $s$ for four specified test cases.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientific Grounding**: The problem is set in the context of linear least squares, a fundamental topic in numerical linear algebra and computational science. The concepts of least squares solutions, matrix rank, vector norms, and sensitivity analysis are standard and well-established. The problem is scientifically sound.\n2.  **Well-Posedness**: The problem states that the matrix $A$ in each test case has full column rank. For a matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, full column rank implies that the columns of $A$ are linearly independent. This ensures that the Gram matrix $A^T A$ is invertible, which guarantees the existence of a unique least squares solution for both the baseline and perturbed problems. An examination of the matrices provided for the test cases confirms they all possess full column rank, even in Case 2 where the columns are nearly linearly dependent, leading to an ill-conditioned but still full-rank matrix. Thus, each problem is well-posed.\n3.  **Objectivity**: The problem is formulated using precise mathematical language and definitions, free of any subjectivity or ambiguity.\n\nThe problem does not violate any of the invalidity criteria. It is complete, consistent, and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\n**Derivation of the Solution**\n\nThe solution to the linear least squares problem $\\min_x \\lVert Ax - y \\rVert_2$ for a full-rank matrix $A$ is given by the normal equations:\n$$\nx = (A^T A)^{-1} A^T y\n$$\nApplying this to the baseline and perturbed problems:\n$$\nx = (A^T A)^{-1} A^T b\n$$\n$$\n\\tilde{x} = (A^T A)^{-1} A^T \\tilde{b}\n$$\nThe difference between the perturbed and baseline solutions, $\\Delta x = \\tilde{x} - x$, is derived as follows:\n$$\n\\Delta x = (A^T A)^{-1} A^T \\tilde{b} - (A^T A)^{-1} A^T b = (A^T A)^{-1} A^T (\\tilde{b} - b)\n$$\nSubstituting $\\tilde{b} = b + \\delta e_p$:\n$$\n\\Delta x = (A^T A)^{-1} A^T (b + \\delta e_p - b) = \\delta (A^T A)^{-1} A^T e_p\n$$\nThe sensitivity amplification $s$ is then:\n$$\ns = \\frac{\\lVert \\Delta x \\rVert_{2}}{|\\delta|} = \\frac{\\lVert \\delta (A^T A)^{-1} A^T e_p \\rVert_{2}}{|\\delta|} = \\frac{|\\delta| \\lVert (A^T A)^{-1} A^T e_p \\rVert_{2}}{|\\delta|} = \\lVert (A^T A)^{-1} A^T e_p \\rVert_{2}\n$$\nThis result demonstrates that the sensitivity $s$ is independent of both the measurement vector $b$ and the perturbation magnitude $\\delta$. It is an intrinsic property determined solely by the matrix $A$ and the index $p$ of the perturbation.\n\nWhile the formula involving the normal equations is analytically correct, its direct computation by forming $A^T A$ is numerically unstable, especially for ill-conditioned matrices. The condition number of $A^T A$ is the square of the condition number of $A$, i.e., $\\kappa(A^T A) = \\kappa(A)^2$. This squaring can lead to significant loss of precision.\n\nA numerically superior method is to use the QR decomposition of $A$. Let us use the reduced (or thin) QR decomposition, where $A = QR$, with $Q \\in \\mathbb{R}^{m \\times n}$ having orthonormal columns ($Q^T Q = I_n$) and $R \\in \\mathbb{R}^{n \\times n}$ being an upper triangular, invertible matrix.\n\nThe least squares solution can be expressed as:\n$$\nx = R^{-1} Q^T b\n$$\nFollowing the same logic, the difference in solutions is:\n$$\n\\Delta x = \\tilde{x} - x = R^{-1} Q^T \\tilde{b} - R^{-1} Q^T b = R^{-1} Q^T (\\delta e_p) = \\delta (R^{-1} Q^T e_p)\n$$\nThe sensitivity $s$ is therefore:\n$$\ns = \\frac{\\lVert \\Delta x \\rVert_{2}}{|\\delta|} = \\lVert R^{-1} (Q^T e_p) \\rVert_{2}\n$$\nThe term $Q^T e_p$ represents the $p$-th column of the matrix $Q^T$. This is equivalent to the transpose of the $p$-th row of $Q$. Let us denote the vector corresponding to the $p$-th row of $Q$ as $q_p \\in \\mathbb{R}^n$. Then $Q^T e_p = q_p$.\nThe final expression for sensitivity is:\n$$\ns = \\lVert R^{-1} q_p \\rVert_{2}\n$$\nThis expression is numerically stable and forms the basis of our computational algorithm.\n\n**Algorithm**\nFor each test case specified by matrix $A$ and index $p$:\n1.  Compute the reduced QR decomposition of $A$ to obtain $Q \\in \\mathbb{R}^{m \\times n}$ and $R \\in \\mathbb{R}^{n \\times n}$.\n2.  Extract the $p$-th row of $Q$ to form the vector $q_p$.\n3.  Solve the upper triangular system $Rv = q_p$ for the vector $v = R^{-1}q_p$. This is efficiently done using back substitution.\n4.  Compute the Euclidean norm $s = \\lVert v \\rVert_2$.\n5.  The result is rounded to $6$ decimal places as required.\n\nThis procedure is applied to each of the four test cases. For Case 3, where $A = [I_3; 0_{2\\times3}]$, the QR decomposition is trivial with $Q=A$ and $R=I_3$. The $p=4$ index corresponds to the fifth row of $Q$, which is a zero vector. Consequently, $q_4 = (0,0,0)$, and $s = \\lVert I_3^{-1} \\cdot 0 \\rVert_2 = 0$, as expected. A perturbation to a measurement corresponding to a zero row in $A$ has no influence on the solution vector $x$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Computes the sensitivity amplification for a series of linear regression problems.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (A, p), as b and delta are not needed for the calculation of s.\n    test_cases = [\n        (\n            np.array([\n                [1., 0., 1.],\n                [0., 1., 1.],\n                [1., 1., 0.],\n                [2., 1., 1.],\n                [1., 2., 1.]\n            ]),\n            2\n        ),\n        (\n            np.array([\n                [1., 1.0000, 2.0000],\n                [2., 2.0005, 4.0009],\n                [3., 3.0008, 6.0012],\n                [4., 4.0011, 8.0015],\n                [5., 5.0014, 10.0018]\n            ]),\n            0\n        ),\n        (\n            np.array([\n                [1., 0., 0.],\n                [0., 1., 0.],\n                [0., 0., 1.],\n                [0., 0., 0.],\n                [0., 0., 0.]\n            ]),\n            4\n        ),\n        (\n            np.array([\n                [3., 0., 0.],\n                [0., 2., 0.],\n                [0., 0., 1.],\n                [1., 1., 1.],\n                [4., 5., 6.]\n            ]),\n            1\n        )\n    ]\n\n    results = []\n    for A, p in test_cases:\n        # The sensitivity amplification s is given by ||inv(R) * q_p||,\n        # where A = QR (thin QR decomposition) and q_p is the p-th row of Q.\n        \n        # Step 1: Compute the reduced QR decomposition of A.\n        # mode='reduced' gives Q (m x n) and R (n x n).\n        Q, R = np.linalg.qr(A, mode='reduced')\n\n        # Step 2: Extract the p-th row of Q to form the vector q_p.\n        q_p = Q[p, :]\n\n        # Step 3: Solve the upper triangular system Rv = q_p for the vector v.\n        # This is equivalent to calculating v = inv(R) * q_p.\n        # lower=False indicates that R is an upper triangular matrix.\n        v = solve_triangular(R, q_p, lower=False)\n\n        # Step 4: Compute the Euclidean norm (L2 norm) of v.\n        s = np.linalg.norm(v)\n        \n        # Round the result to 6 decimal places.\n        results.append(round(s, 6))\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2430336"}]}