{"hands_on_practices": [{"introduction": "A common intuition is that using more data points to build a model should always lead to a better result. This practice challenges that notion by exploring the Runge phenomenon, a cornerstone concept illustrating the dangers of high-degree polynomial interpolation. By computationally seeking the \"crossover degree\" for the famous Runge function, you will directly observe how increasing the number of equispaced interpolation nodes can paradoxically increase the total approximation error, providing a vivid, hands-on understanding of this theoretical limitation [@problem_id:2436036].", "id": "2436036", "problem": "Consider the Runge function $f(x)=\\dfrac{1}{1+a x^{2}}$ on the closed interval $[-1,1]$ for a given parameter $a>0$. For each integer degree $n \\ge 0$, let $P_{n}(x)$ denote the unique polynomial of degree at most $n$ that interpolates $f(x)$ at the $n+1$ equispaced nodes $x_{k}=-1+\\dfrac{2k}{n}$ for $k=0,1,\\dots,n$. Define the integrated absolute interpolation error\n$$\nE(n)=\\int_{-1}^{1}\\left|f(x)-P_{n}(x)\\right|\\,dx.\n$$\nDefine the crossover degree $N_{\\text{crit}}$ to be the smallest integer $n$ in a prescribed search range such that adding one more equispaced interpolation node strictly increases the integrated error in the following sense:\n$$\nE(n+1) > \\left(1+\\tau\\right) E(n),\n$$\nwhere $\\tau$ is a fixed relative margin. If no such $n$ exists within the prescribed range, set $N_{\\text{crit}}=-1$.\n\nYour task is to compute $N_{\\text{crit}}$ for each of the test cases listed below. The integral defining $E(n)$ must be evaluated numerically to an absolute error no larger than $10^{-6}$ on $[-1,1]$.\n\nTest Suite (each item is a triple $(a,n_{\\min},n_{\\max})$ specifying the Runge parameter and the inclusive degree search range):\n- Case $1$: $(a,n_{\\min},n_{\\max})=(25,2,80)$\n- Case $2$: $(a,n_{\\min},n_{\\max})=(5,2,80)$\n- Case $3$: $(a,n_{\\min},n_{\\max})=(100,2,80)$\n- Case $4$: $(a,n_{\\min},n_{\\max})=(1,2,10)$\n\nUse the relative margin $\\tau=10^{-4}$ in the definition of $N_{\\text{crit}}$. For each case, determine the smallest integer $n$ with $n_{\\min}\\le n < n_{\\max}$ for which $E(n+1)>\\left(1+\\tau\\right)E(n)$; if none exists in the specified range, report $-1$.\n\nFinal output format: Your program should produce a single line of output containing the four results in order as a comma-separated list enclosed in square brackets, for example $[N_{\\text{crit},1},N_{\\text{crit},2},N_{\\text{crit},3},N_{\\text{crit},4}]$, where each $N_{\\text{crit},j}$ is an integer.", "solution": "The problem presented is a valid exercise in computational physics and numerical analysis. It is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The task concerns the investigation of Runge's phenomenon for polynomial interpolation of the Runge function, $f(x)=\\dfrac{1}{1+ax^2}$, on the interval $[-1,1]$. Specifically, we are to determine the \"crossover degree\" $N_{\\text{crit}}$, defined as the smallest integer degree $n$ within a specified range $[n_{\\min}, n_{\\max}-1]$ for which the integrated absolute error $E(n+1)$ shows a significant increase over $E(n)$. This phenomenon, where interpolation error with high-degree polynomials on equispaced nodes diverges near the interval boundaries, is a fundamental concept illustrating the limitations of naive interpolation strategies.\n\nThe methodology for solving this problem is a direct computational implementation of the definitions provided. For each test case, characterized by the parameter $a$ and the search range $[n_{\\min}, n_{\\max}]$, we must systematically compute the integrated error $E(n)$ for degrees $n$ and $n+1$ and check the specified condition.\n\nFirst, we define the core quantity, the integrated absolute interpolation error, for a given degree $n$:\n$$\nE(n) = \\int_{-1}^{1} \\left|f(x) - P_{n}(x)\\right|\\,dx\n$$\nHere, $f(x) = \\dfrac{1}{1+ax^2}$ is the function to be interpolated. The term $P_n(x)$ represents the unique interpolating polynomial of degree at most $n$ that passes through $n+1$ specific points on the curve of $f(x)$. These points are determined by the set of $n+1$ equispaced nodes:\n$$\nx_k = -1 + \\frac{2k}{n}, \\quad k = 0, 1, \\dots, n\n$$\nThe corresponding values are $y_k = f(x_k)$. The polynomial $P_n(x)$ can be constructed using these pairs $(x_k, y_k)$. While the Lagrange form is the classical representation, the Barycentric form is numerically more stable and efficient for evaluation, and is a standard choice in high-quality numerical libraries.\n\nThe integral defining $E(n)$ does not, in general, have a simple analytical closed form. Therefore, its evaluation must be performed numerically. A robust adaptive quadrature algorithm, such as the one implemented in `scipy.integrate.quad` which is based on a QUADPACK routine, is suitable. To satisfy the problem's requirement that the integral be evaluated to an absolute error no larger than $10^{-6}$, the numerical integration subroutine must be configured with a sufficiently small tolerance, for instance, an absolute tolerance of $10^{-9}$.\n\nThe overall algorithm proceeds as follows for each test case $(a, n_{\\min}, n_{\\max})$ with the fixed relative margin $\\tau = 10^{-4}$:\n1. Initialize the crossover degree $N_{\\text{crit}} = -1$.\n2. Iterate through integer degrees $n$ from $n_{\\min}$ to $n_{\\max}-1$.\n3. In each iteration, compute the error integrals $E(n)$ and $E(n+1)$. To optimize, the value of $E(n)$ from the previous iteration can be reused. For the first iteration, $n=n_{\\min}$, both $E(n_{\\min})$ and $E(n_{\\min}+1)$ must be computed. For subsequent iterations $n > n_{\\min}$, the value of $E(n)$ will have been computed as $E((n-1)+1)$ in the prior step.\n4. Check the crossover condition:\n   $$\n   E(n+1) > (1+\\tau) E(n)\n   $$\n5. If this condition is met, the current degree $n$ is the smallest integer that satisfies it. We set $N_{\\text{crit}} = n$ and terminate the search for this test case.\n6. If the loop completes without the condition ever being met, $N_{\\text{crit}}$ remains at its initial value of $-1$.\n\nThis computational procedure is repeated for all four test cases, and the resulting values of $N_{\\text{crit}}$ are collected. The parameter $a$ controls the sharpness of the peak in $f(x)$. A larger value of $a$ leads to a more pronounced Runge phenomenon, and we expect the error to start diverging at a lower degree $n$, leading to a smaller $N_{\\text{crit}}$. Conversely, a smaller $a$ results in a smoother function, delaying the onset of divergence, which should correspond to a larger $N_{\\text{crit}}$ or no crossover within the given range.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import BarycentricInterpolator\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves for the crossover degree N_crit for the Runge function interpolation error\n    across several test cases.\n    \"\"\"\n\n    def calculate_error(n: int, a: float) -> float:\n        \"\"\"\n        Calculates the integrated absolute interpolation error E(n) for a given degree n and parameter a.\n\n        Args:\n            n: The degree of the interpolating polynomial.\n            a: The parameter of the Runge function.\n\n        Returns:\n            The value of the integral E(n).\n        \"\"\"\n        if n < 0:\n            # A degree cannot be negative.\n            raise ValueError(\"Degree n must be non-negative.\")\n\n        # Define the Runge function f(x)\n        runge_func = lambda x: 1.0 / (1.0 + a * x**2)\n\n        # Generate n+1 equispaced nodes in [-1, 1]\n        x_nodes = np.linspace(-1.0, 1.0, n + 1)\n        y_nodes = runge_func(x_nodes)\n\n        # Construct the barycentric interpolating polynomial P_n(x)\n        poly = BarycentricInterpolator(x_nodes, y_nodes)\n\n        # Define the absolute error function |f(x) - P_n(x)|\n        error_func = lambda x: np.abs(runge_func(x) - poly(x))\n\n        # Numerically integrate the error function from -1 to 1.\n        # Set a small absolute tolerance to ensure the result is accurate to 1e-6 as required.\n        integral_val, _ = quad(error_func, -1.0, 1.0, epsabs=1e-9)\n\n        return integral_val\n\n    def find_Ncrit(a: float, n_min: int, n_max: int, tau: float) -> int:\n        \"\"\"\n        Finds the smallest integer n in [n_min, n_max-1] such that E(n+1) > (1+tau)E(n).\n        \n        Args:\n            a: The parameter of the Runge function.\n            n_min: The minimum degree to start the search from.\n            n_max: The exclusive upper bound for the search degree n.\n            tau: The relative margin for error increase.\n            \n        Returns:\n            The crossover degree N_crit, or -1 if not found.\n        \"\"\"\n        N_crit = -1\n\n        # Check for valid range\n        if n_min >= n_max:\n            return N_crit\n\n        # Use an error cache to avoid recomputing integrals\n        error_cache = {}\n\n        def get_E(n_val):\n            if n_val not in error_cache:\n                error_cache[n_val] = calculate_error(n_val, a)\n            return error_cache[n_val]\n        \n        # Loop n from n_min to n_max-1\n        for n in range(n_min, n_max):\n            E_n = get_E(n)\n            E_n_plus_1 = get_E(n + 1)\n\n            if E_n_plus_1 > (1.0 + tau) * E_n:\n                N_crit = n\n                break  # Found the smallest n, terminate search for this case\n        \n        return N_crit\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (25, 2, 80),   # Case 1\n        (5, 2, 80),    # Case 2\n        (100, 2, 80),  # Case 3\n        (1, 2, 10),    # Case 4\n    ]\n    \n    # Relative margin\n    tau = 1e-4\n\n    results = []\n    for a, n_min, n_max in test_cases:\n        result = find_Ncrit(a, n_min, n_max, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "The failures of high-degree interpolation extend beyond simple function approximation; they become critically severe in many engineering applications, such as calculating velocity from displacement data, which requires differentiation. This practice delves into how small noises in measurement data or the act of differentiation itself can be catastrophically amplified by a high-degree polynomial interpolant. You will analyze the stark contrast in error propagation between naive equispaced nodes and the much more robust Chebyshev nodes, highlighting a crucial strategy for reliable numerical differentiation [@problem_id:2409024].", "id": "2409024", "problem": "A computational engineer records a one-dimensional displacement signal $s(t)$ from a mechanical system over a time interval $[-1,1]$. The signal is known to be sufficiently smooth, specifically $s \\in C^{m}([-1,1])$ for some $m \\ge 2$. The engineer samples $s(t)$ at $n+1$ distinct nodes $x_0, x_1, \\dots, x_n \\in [-1,1]$ and constructs the unique polynomial $p_n(t)$ of degree at most $n$ that interpolates the measured data $\\{(x_i, y_i)\\}_{i=0}^n$, where $y_i = s(x_i) + \\eta_i$. Two noise regimes are considered:\n- Bounded noise: $|\\eta_i| \\le \\varepsilon$ for a fixed $\\varepsilon > 0$.\n- Stochastic noise: $\\eta_i$ are independent, zero-mean random variables with variance $\\operatorname{Var}(\\eta_i) = \\sigma^2$.\n\nThe engineer uses the derivative $p_n'(t)$ as a proxy for the true velocity $v(t) = s'(t)$ and, in a force-calculation context, as a proxy for acceleration if differentiated again. Assume the engineer is interested in $p_n'(x_0)$ at a fixed interior point $x_0 \\in (-1,1)$. Consider both the choice of equispaced nodes and Chebyshev nodes of the first kind on $[-1,1]$.\n\nWhich of the following statements about the propagation of error into $p_n'(x_0)$ is true? Select all that apply.\n\nA. For equispaced nodes on $[-1,1]$ and a fixed interior $x_0$, the map from data perturbations $\\{\\eta_i\\}$ with $|\\eta_i| \\le \\varepsilon$ to the derivative error $|p_n'(x_0) - \\tilde p_n'(x_0)|$ (where $\\tilde p_n$ is the interpolant of the perturbed data) has an operator norm that grows without bound as $n$ increases.\n\nB. Switching from equispaced nodes to Chebyshev nodes reduces the worst-case amplification of data perturbations into $p_n'(x_0)$; in particular, the amplification with Chebyshev nodes grows at most polynomially (not exponentially) in $n$.\n\nC. Even with exact data ($\\eta_i = 0$), if $s$ is analytic on $[-1,1]$ and nodes are equispaced, then the maximum pointwise derivative error $\\max_{x \\in [-1,1]} |s'(x) - p_n'(x)|$ necessarily decreases monotonically to $0$ as $n$ increases.\n\nD. Under the stochastic noise model with independent, zero-mean noise of common variance $\\sigma^2$, one has $\\operatorname{Var}(p_n'(x_0)) = \\sigma^2 \\sum_{i=0}^n \\left(\\ell_i'(x_0)\\right)^2$, where $\\ell_i$ are the Lagrange basis polynomials for the chosen nodes. Consequently, the standard deviation of $p_n'(x_0)$ can grow significantly with $n$ for equispaced nodes.", "solution": "The problem statement must first be validated for scientific soundness and consistency.\n\n### Step 1: Extract Givens\n- A displacement signal $s(t)$, which is a function in the class $C^{m}([-1,1])$ for some integer $m \\ge 2$.\n- The time interval is $[-1,1]$.\n- A set of $n+1$ distinct interpolation nodes $\\{x_i\\}_{i=0}^n$ within the interval $[-1,1]$.\n- The measured data are $(x_i, y_i)$ for $i=0, \\dots, n$.\n- The relationship between the measured data and the true signal is $y_i = s(x_i) + \\eta_i$, where $\\eta_i$ is a noise term.\n- Two noise models are considered:\n    1. Bounded noise: $|\\eta_i| \\le \\varepsilon$ for a constant $\\varepsilon > 0$.\n    2. Stochastic noise: $\\eta_i$ are independent, zero-mean ($\\mathbb{E}[\\eta_i] = 0$) random variables with common variance $\\operatorname{Var}(\\eta_i) = \\sigma^2$.\n- An interpolating polynomial $p_n(t)$ of degree at most $n$ is constructed through the data points $\\{(x_i, y_i)\\}_{i=0}^n$.\n- The derivative of the interpolant, $p_n'(t)$, is used as an approximation for the true velocity $s'(t)$.\n- The analysis focuses on the error in the derivative at a fixed interior point $x_0 \\in (-1,1)$. The problem appears to imply that this point of evaluation, $x_0$, is also one of the interpolation nodes. This is a notational choice that we will respect.\n- Two sets of nodes are considered: equispaced nodes and Chebyshev nodes of the first kind on $[-1,1]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a classic scenario in numerical analysis and computational science: the differentiation of noisy data using polynomial interpolation. This task is fundamental to many engineering applications where derivatives must be estimated from sampled measurements.\n\n- **Scientific Grounding**: The problem is firmly rooted in the theory of polynomial interpolation, numerical differentiation, and error analysis. The concepts of Lagrange polynomials, Chebyshev nodes, Runge's phenomenon, and error propagation are all standard, well-established topics in mathematics and engineering. The problem is scientifically sound.\n- **Well-Posedness**: The existence and uniqueness of the interpolating polynomial $p_n(t)$ for $n+1$ distinct nodes is guaranteed. The questions posed are about the asymptotic behavior of error metrics as the number of nodes $n$ increases, which are well-defined mathematical questions.\n- **Objectivity**: The problem is stated in precise, objective mathematical language. There are no subjective or ambiguous terms. The notational choice of using $x_0$ to denote both a specific node and the first element in a sequence of nodes is slightly confusing but does not create a logical contradiction, as nodes can be arbitrarily labeled. For any set of nodes on $[-1,1]$ with $n>1$, both equispaced and Chebyshev sets contain interior points. One such point can be designated as $x_0$ for the analysis.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically grounded problem in numerical analysis. The solution can now be derived.\n\nThe interpolating polynomial $p_n(t)$ passing through the points $\\{(x_i, y_i)\\}_{i=0}^n$ can be written using the Lagrange basis as:\n$$p_n(t) = \\sum_{i=0}^n y_i \\ell_i(t)$$\nwhere $\\ell_i(t)$ are the Lagrange basis polynomials, defined by\n$$\\ell_i(t) = \\prod_{j=0, j \\neq i}^n \\frac{t-x_j}{x_i-x_j}$$\nThe derivative of the interpolating polynomial is:\n$$p_n'(t) = \\sum_{i=0}^n y_i \\ell_i'(t)$$\nLet $p_n^*(t)$ be the hypothetical interpolant of the exact, noise-free data, i.e., $p_n^*(t) = \\sum_{i=0}^n s(x_i) \\ell_i(t)$. The polynomial interpolating the noisy data is $p_n(t) = \\sum_{i=0}^n (s(x_i) + \\eta_i) \\ell_i(t)$. The error in the polynomial's derivative due to the data noise $\\eta_i$ is:\n$$\\Delta p_n'(t) \\equiv p_n'(t) - p_n^{*\\prime}(t) = \\sum_{i=0}^n \\eta_i \\ell_i'(t)$$\nThe problem asks about the magnitude of this error at a fixed interior point $x_0 \\in (-1,1)$. The options refer to $\\tilde{p}_n$ as the interpolant of perturbed data; we identify $p_n$ with $\\tilde{p}_n$ from the problem statement and $p_n^*$ with the unperturbed one. Thus, the quantity $|p_n'(x_0) - \\tilde{p}_n'(x_0)|$ in option A corresponds to $|\\Delta p_n'(x_0)| = |\\sum_{i=0}^n \\eta_i \\ell_i'(x_0)|$, where one set of data has $\\eta_i=0$.\n\nNow we evaluate each option.\n\n**A. For equispaced nodes on $[-1,1]$ and a fixed interior $x_0$, the map from data perturbations $\\{\\eta_i\\}$ with $|\\eta_i| \\le \\varepsilon$ to the derivative error $|p_n'(x_0) - \\tilde p_n'(x_0)|$ (where $\\tilde p_n$ is the interpolant of the perturbed data) has an operator norm that grows without bound as $n$ increases.**\n\nThe error in the derivative is $\\Delta p_n'(x_0) = \\sum_{i=0}^n \\eta_i \\ell_i'(x_0)$. Under the bounded noise model, $|\\eta_i| \\le \\varepsilon$. The worst-case error is found by maximizing over all possible perturbations:\n$$\\max_{|\\eta_i| \\le \\varepsilon} |\\Delta p_n'(x_0)| = \\max_{|\\eta_i| \\le \\varepsilon} \\left| \\sum_{i=0}^n \\eta_i \\ell_i'(x_0) \\right| = \\varepsilon \\sum_{i=0}^n |\\ell_i'(x_0)|$$\nThe operator norm of the linear map from the vector of perturbations $\\vec{\\eta} = (\\eta_0, \\dots, \\eta_n)$ (with the $\\ell_\\infty$ norm $\\|\\vec{\\eta}\\|_\\infty = \\max_i |\\eta_i|$) to the scalar output $\\Delta p_n'(x_0)$ is the amplification factor $\\sum_{i=0}^n |\\ell_i'(x_0)|$. This quantity is the condition number for differentiation at point $x_0$.\nFor equispaced nodes on $[-1,1]$, it is a well-established result in numerical analysis that this condition number grows exponentially with $n$. The underlying reason is related to Runge's phenomenon; the derivatives of the Lagrange basis functions become very large, especially near the boundaries, but the growth is exponential even at interior points. Since exponential growth is unbounded, the statement is true.\n**Verdict: Correct.**\n\n**B. Switching from equispaced nodes to Chebyshev nodes reduces the worst-case amplification of data perturbations into $p_n'(x_0)$; in particular, the amplification with Chebyshev nodes grows at most polynomially (not exponentially) in $n$.**\n\nThe worst-case amplification factor is $\\sum_{i=0}^n |\\ell_i'(x_0)|$. As established in A, for equispaced nodes this factor grows exponentially. For Chebyshev nodes of the first kind ($x_i = \\cos(\\frac{(2i+1)\\pi}{2(n+1)})$), the behavior of both the interpolant and its derivatives is much better. The Lebesgue constant $\\Lambda_n = \\max_{x \\in [-1,1]} \\sum_{i=0}^n |\\ell_i(x)|$ grows only as $O(\\log n)$. The condition number for differentiation, $\\sum_{i=0}^n |\\ell_i'(x_0)|$, also behaves much better. For any fixed point $x_0 \\in [-1,1]$, this sum is known to be bounded by a polynomial in $n$. Specifically, for an interior point $x_0 \\in (-1,1)$, the sum grows as $O(n^2)$.\nSwitching from exponential growth for equispaced nodes to polynomial ($O(n^2)$) growth for Chebyshev nodes is a significant reduction in the amplification of errors. The statement is therefore true.\n**Verdict: Correct.**\n\n**C. Even with exact data ($\\eta_i = 0$), if $s$ is analytic on $[-1,1]$ and nodes are equispaced, then the maximum pointwise derivative error $\\max_{x \\in [-1,1]} |s'(x) - p_n'(x)|$ necessarily decreases monotonically to $0$ as $n$ increases.**\n\nThis statement concerns the approximation error, not the propagation of data noise. It claims that for any analytic function $s(t)$, the derivative of the interpolant $p_n'(t)$ based on equispaced nodes converges uniformly and monotonically to the true derivative $s'(t)$. This is false.\nIt is famous that for equispaced nodes, polynomial interpolation does not guarantee uniform convergence for all analytic functions. The classic counterexample is the Runge function, $s(x) = 1/(1+25x^2)$, for which the maximum interpolation error $\\max_{x \\in [-1,1]}|s(x) - p_n(x)|$ diverges as $n \\to \\infty$. The error in the derivative, $|s'(x) - p_n'(x)|$, generally fares even worse, as differentiation amplifies the high-frequency oscillations of the error $s(x) - p_n(x)$. Therefore, uniform convergence of the derivative to zero is not guaranteed.\nThe additional claim that the error \"necessarily decreases monotonically\" is also false. Error in numerical methods rarely exhibits strict monotonic behavior; it often oscillates as the parameter $n$ changes.\n**Verdict: Incorrect.**\n\n**D. Under the stochastic noise model with independent, zero-mean noise of common variance $\\sigma^2$, one has $\\operatorname{Var}(p_n'(x_0)) = \\sigma^2 \\sum_{i=0}^n \\left(\\ell_i'(x_0)\\right)^2$, where $\\ell_i$ are the Lagrange basis polynomials for the chosen nodes. Consequently, the standard deviation of $p_n'(x_0)$ can grow significantly with $n$ for equispaced nodes.**\n\nThe derivative of the interpolant is $p_n'(x_0) = \\sum_{i=0}^n y_i \\ell_i'(x_0)$. The data points are $y_i = s(x_i) + \\eta_i$. Since $s(x_i)$ and $\\ell_i'(x_0)$ are deterministic constants, the variance of $p_n'(x_0)$ is due entirely to the noise terms $\\eta_i$.\n$$\\operatorname{Var}(p_n'(x_0)) = \\operatorname{Var}\\left(\\sum_{i=0}^n (s(x_i) + \\eta_i) \\ell_i'(x_0)\\right) = \\operatorname{Var}\\left(\\sum_{i=0}^n \\eta_i \\ell_i'(x_0)\\right)$$\nSince the noise terms $\\eta_i$ are independent, the variance of the sum is the sum of the variances:\n$$\\operatorname{Var}(p_n'(x_0)) = \\sum_{i=0}^n \\operatorname{Var}(\\eta_i \\ell_i'(x_0)) = \\sum_{i=0}^n (\\ell_i'(x_0))^2 \\operatorname{Var}(\\eta_i)$$\nGiven that $\\operatorname{Var}(\\eta_i) = \\sigma^2$ for all $i$, we get:\n$$\\operatorname{Var}(p_n'(x_0)) = \\sum_{i=0}^n (\\ell_i'(x_0))^2 \\sigma^2 = \\sigma^2 \\sum_{i=0}^n (\\ell_i'(x_0))^2$$\nThe first part of the statement is correct.\nThe standard deviation is $\\operatorname{StdDev}(p_n'(x_0)) = \\sqrt{\\operatorname{Var}(p_n'(x_0))} = \\sigma \\sqrt{\\sum_{i=0}^n (\\ell_i'(x_0))^2}$.\nThe term $\\sqrt{\\sum_{i=0}^n (\\ell_i'(x_0))^2}$ is the Euclidean norm ($\\ell_2$-norm) of the vector of derivative basis functions evaluated at $x_0$. As established in part A, the $\\ell_1$-norm of this vector, $\\sum_{i=0}^n |\\ell_i'(x_0)|$, grows exponentially for equispaced nodes. By the equivalence of norms in a finite-dimensional vector space ($\\|\\cdot\\|_1$ and $\\|\\cdot\\|_2$ on $\\mathbb{R}^{n+1}$), we have $\\|\\mathbf{v}\\|_2 \\le \\|\\mathbf{v}\\|_1 \\le \\sqrt{n+1}\\|\\mathbf{v}\\|_2$. Since $\\|\\mathbf{v}\\|_1$ grows exponentially, and $\\sqrt{n+1}$ grows only polynomially, $\\|\\mathbf{v}\\|_2$ must also grow exponentially. Therefore, the standard deviation of the derivative error grows exponentially, which is \"significantly\". The second part of the statement is also correct.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABD}$$"}, {"introduction": "Separate from theoretical limitations like the Runge phenomenon, a practical barrier arises from the finite-precision arithmetic of computers. This hands-on coding exercise demonstrates how the choice of interpolation nodes can lead to numerical instability, causing catastrophic loss of precision in the calculation of Newton's divided differences. You will investigate how different scenarios, from the well-behaved to the pathologically clustered, push the limits of double-precision arithmetic, revealing how high-degree interpolation can fail not just in theory, but in practice on a machine [@problem_id:2409001].", "id": "2409001", "problem": "A computational task is to quantify how finite precision arithmetic affects the construction and evaluation of high-degree interpolating polynomials when represented in the Newton basis using divided differences. Work in standard double-precision arithmetic as specified by the Institute of Electrical and Electronics Engineers (IEEE) $754$ binary64 format, and treat all operations and results as occurring in that model. For a given set of nodes $\\{x_k\\}_{k=0}^n \\subset \\mathbb{R}$ and a function $f:\\mathbb{R}\\to\\mathbb{R}$, the unique interpolating polynomial $p_n$ of degree at most $n$ in the Newton basis is defined by\n$$\np_n(x) \\;=\\; \\sum_{k=0}^{n} c_k \\prod_{j=0}^{k-1} (x - x_j),\n$$\nwhere $c_k$ is the $k$-th divided difference $f[x_0,\\dots,x_k]$ in the standard mathematical sense. All quantities must be computed and evaluated numerically in double precision.\n\nFor each test case below, construct $p_n$ for the specified $f$ and nodes, then evaluate $p_n$ and $f$ on a uniform evaluation grid of $m$ points in the stated interval and compute the maximum absolute evaluation error\n$$\nE \\;=\\; \\max_{t \\in T} \\big|p_n(t) - f(t)\\big|,\n$$\nwhere $T$ is the evaluation grid. If at any stage non-finite values arise (for example, any coefficient $c_k$ is not finite or any evaluation of $p_n$ is not finite), report $E = +\\infty$ for that test case.\n\nTest suite:\n- Case A (general behavior with equidistant nodes and a function known to oscillate under equidistant interpolation): $f(x) = \\dfrac{1}{1+25x^2}$ on $[-1,1]$, nodes $x_k$ equidistant on $[-1,1]$ with $n=20$ (that is, $21$ nodes), and $m=1001$ evaluation points uniformly spaced on $[-1,1]$.\n- Case B (well-conditioned nodes for an analytic function): $f(x) = e^x$ on $[-1,1]$, nodes $x_k = \\cos\\!\\left(\\dfrac{k\\pi}{n}\\right)$ (Chebyshev nodes of the second kind) with $n=30$, and $m=1001$ evaluation points uniformly spaced on $[-1,1]$.\n- Case C (strong node clustering to probe sensitivity of divided differences): $f(x) = \\sin(x)$ on $[0,1]$, nodes $x_k = 10^{-k}$ for $k=18,17,\\dots,0$ (that is, strictly increasing from $10^{-18}$ to $10^0$) with $n=18$, and $m=1001$ evaluation points uniformly spaced on $[0,1]$.\n- Case D (very high degree with well-conditioned nodes for an analytic function): $f(x) = e^x$ on $[-1,1]$, nodes $x_k = \\cos\\!\\left(\\dfrac{k\\pi}{n}\\right)$ with $n=80$, and $m=1001$ evaluation points uniformly spaced on $[-1,1]$.\n\nYour program must produce four results corresponding to the above cases, in order, each being the scalar $E$ for the case as defined above. The final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $[E_A,E_B,E_C,E_D]$.", "solution": "The problem presented requires the validation of polynomial interpolation methods under the constraints of finite precision arithmetic, a fundamental topic in computational engineering and numerical analysis. The task is to construct an interpolating polynomial $p_n(x)$ in the Newton basis for given functions and node sets, and then to quantify the maximum absolute error $E$ against the true function on a fine evaluation grid. All computations are to be performed using standard IEEE $754$ double-precision floating-point arithmetic.\n\nThe interpolating polynomial $p_n(x)$ of degree at most $n$ that passes through the $n+1$ points $\\{(x_k, f(x_k))\\}_{k=0}^n$ is uniquely defined. In the Newton basis, it takes the form:\n$$\np_n(x) \\;=\\; \\sum_{k=0}^{n} c_k \\prod_{j=0}^{k-1} (x - x_j)\n$$\nThis representation is generally preferred for numerical construction over the monomial basis, as the latter often leads to a severely ill-conditioned Vandermonde matrix.\n\nThe coefficients $c_k$ are the divided differences, defined as $c_k = f[x_0, \\dots, x_k]$. They are computed recursively:\n$$\nf[x_i] \\;=\\; f(x_i)\n$$\n$$\nf[x_i, \\dots, x_j] \\;=\\; \\frac{f[x_{i+1}, \\dots, x_j] - f[x_i, \\dots, x_{j-1}]}{x_j - x_i}\n$$\nNumerically, these coefficients are generated by constructing a divided difference table. The required coefficients $c_k = f[x_0, \\dots, x_k]$ form the top diagonal of this table.\n\nOnce the coefficients $\\{c_k\\}_{k=0}^n$ are determined, the polynomial $p_n(x)$ is evaluated at each point $t$ in the specified evaluation grid $T$. For numerical stability and efficiency, evaluation is performed using Horner's method (nested evaluation):\n$$\np_n(t) = ((\\dots(c_n(t-x_{n-1}) + c_{n-1})(t-x_{n-2}) + \\dots) + c_1)(t-x_0) + c_0\n$$\nThis method minimizes the number of arithmetic operations and reduces the accumulation of floating-point error compared to a direct summation of terms.\n\nThe final error is calculated as $E = \\max_{t \\in T} |p_n(t) - f(t)|$. If at any stage a non-finite value (such as `Infinity` or `NaN`) is produced, the error for that case is reported as $E = +\\infty$.\n\nThe four test cases are designed to probe different aspects of this numerical process:\n\nCase A: $f(x) = \\frac{1}{1+25x^2}$ with $n=20$ equidistant nodes on $[-1, 1]$. This is the canonical example of Runge's phenomenon. For this function, high-degree polynomial interpolation with equally spaced nodes diverges near the endpoints of the interval, leading to large oscillations and a substantial error $E$. This is a property of the approximation itself, which is exacerbated by finite-precision effects.\n\nCase B: $f(x) = e^x$ with $n=30$ Chebyshev nodes on $[-1, 1]$. The function $e^x$ is analytic on the entire complex plane. Interpolation of an analytic function at Chebyshev nodes is known to converge exponentially fast in exact arithmetic. The Chebyshev nodes $x_k = \\cos(k\\pi/n)$ cluster near the endpoints, which is precisely what is needed to counteract Runge's phenomenon. Therefore, a very small error $E$ is expected, demonstrating a well-conditioned problem.\n\nCase C: $f(x) = \\sin(x)$ with $n=18$ and nodes $x_k = 10^{-k}$ for $k=18,17,\\dots,0$. These nodes are extremely clustered near $x=0$. The recursive computation of divided differences involves denominators of the form $x_j - x_i$, which will be extremely small. Furthermore, the numerators, representing differences of nearly equal values, will suffer from catastrophic cancellation, leading to a profound loss of relative precision. This severe numerical instability is expected to corrupt the coefficients $c_k$, potentially rendering them non-finite or producing a very large, meaningless error $E$.\n\nCase D: $f(x) = e^x$ with $n=80$ Chebyshev nodes on $[-1, 1]$. This case extends Case B to a much higher degree. While theory predicts continued convergence, this test pushes the limits of stability in double-precision arithmetic. Even with optimal nodes, the accumulation of small floating-point errors over many operations for a high degree like $n=80$ might become significant. This case will reveal the practical limits of high-degree interpolation even under favorable conditions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_newton_coeffs(x_nodes: np.ndarray, y_nodes: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the coefficients of the Newton interpolating polynomial.\n    \n    Args:\n        x_nodes: The interpolation nodes (n+1 points).\n        y_nodes: The function values at the interpolation nodes (n+1 values).\n    \n    Returns:\n        An array of the n+1 Newton coefficients.\n    \"\"\"\n    n = len(x_nodes)\n    # Use a copy to avoid modifying the original y_nodes array if it was passed.\n    # Ensure all calculations are done in float64.\n    coeffs = np.copy(y_nodes).astype(np.float64)\n    \n    # Iterate to compute the divided differences\n    for j in range(1, n):\n        for i in range(n - 1, j - 1, -1):\n            denominator = x_nodes[i] - x_nodes[i - j]\n            # The problem statement guarantees distinct nodes, but in a general\n            # setting, a zero denominator would lead to non-finite results.\n            # numpy handles division by zero, resulting in inf, which is\n            # handled later as per the problem requirements.\n            coeffs[i] = (coeffs[i] - coeffs[i - 1]) / denominator\n            \n    return coeffs\n\ndef evaluate_newton_poly(x_nodes: np.ndarray, coeffs: np.ndarray, t_eval: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Evaluates the Newton polynomial at given points using Horner's method.\n    \n    Args:\n        x_nodes: The interpolation nodes.\n        coeffs: The Newton coefficients.\n        t_eval: The points at which to evaluate the polynomial.\n        \n    Returns:\n        The polynomial values at the evaluation points.\n    \"\"\"\n    n = len(coeffs)\n    p_values = np.zeros_like(t_eval, dtype=np.float64)\n    \n    for i, t in enumerate(t_eval):\n        # Horner's method for a single evaluation point t\n        val = coeffs[n - 1]\n        for k in range(n - 2, -1, -1):\n            val = val * (t - x_nodes[k]) + coeffs[k]\n        p_values[i] = val\n        \n    return p_values\n\ndef solve():\n    \"\"\"\n    Solves the interpolation problem for all four specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case A: Runge's phenomenon\n        {\n            'f': lambda x: 1.0 / (1.0 + 25.0 * x**2),\n            'interval': np.array([-1.0, 1.0], dtype=np.float64),\n            'n': 20,\n            'node_type': 'equidistant',\n            'm': 1001\n        },\n        # Case B: Well-conditioned problem\n        {\n            'f': lambda x: np.exp(x),\n            'interval': np.array([-1.0, 1.0], dtype=np.float64),\n            'n': 30,\n            'node_type': 'chebyshev',\n            'm': 1001\n        },\n        # Case C: Extreme node clustering\n        {\n            'f': lambda x: np.sin(x),\n            'interval': np.array([0.0, 1.0], dtype=np.float64),\n            'n': 18,\n            'node_type': 'custom_clustered',\n            'm': 1001\n        },\n        # Case D: Very high degree with good nodes\n        {\n            'f': lambda x: np.exp(x),\n            'interval': np.array([-1.0, 1.0], dtype=np.float64),\n            'n': 80,\n            'node_type': 'chebyshev',\n            'm': 1001\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n = case['n']\n        f = case['f']\n        interval = case['interval']\n        m = case['m']\n\n        # Step 1: Generate interpolation nodes\n        if case['node_type'] == 'equidistant':\n            x_nodes = np.linspace(interval[0], interval[1], n + 1, dtype=np.float64)\n        elif case['node_type'] == 'chebyshev':\n            k = np.arange(n + 1, dtype=np.float64)\n            # Chebyshev nodes of the second kind on [-1, 1]\n            x_nodes = np.cos(k * np.pi / n)\n        elif case['node_type'] == 'custom_clustered':\n            k = np.arange(18, -1, -1, dtype=np.float64)\n            x_nodes = 10.0**(-k)\n        \n        y_nodes = f(x_nodes)\n\n        # Step 2: Compute Newton coefficients\n        coeffs = compute_newton_coeffs(x_nodes, y_nodes)\n\n        # Check for non-finite coefficients\n        if not np.all(np.isfinite(coeffs)):\n            results.append(np.inf)\n            continue\n            \n        # Step 3: Evaluate polynomial and true function on the grid\n        t_eval = np.linspace(interval[0], interval[1], m, dtype=np.float64)\n        p_values = evaluate_newton_poly(x_nodes, coeffs, t_eval)\n\n        # Check for non-finite evaluation results\n        if not np.all(np.isfinite(p_values)):\n            results.append(np.inf)\n            continue\n            \n        f_values = f(t_eval)\n\n        # Step 4: Compute the maximum absolute error\n        error = np.max(np.abs(p_values - f_values))\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}