{"hands_on_practices": [{"introduction": "Understanding new concepts often starts by connecting them to what you already know. This first practice builds a bridge between the Newton form of a polynomial and the more familiar standard power series form ($c_n x^n + \\dots + c_1 x + c_0$). By algebraically expanding the Newton representation, you will see exactly how the coefficients and nodes of the Newton form combine to produce the standard coefficients, clarifying that these are just two different descriptions of the same underlying polynomial. [@problem_id:2189970]", "problem": "In numerical analysis, a polynomial can be represented in various forms. Consider a quadratic polynomial $P_2(x)$ written in Newton's form, which is constructed using interpolation nodes $x_0$ and $x_1$:\n$$P_2(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1)$$\nHere, $a_0, a_1$, and $a_2$ are the coefficients in the Newton basis.\n\nThe same polynomial can also be written in the standard power series form:\n$$P_2(x) = c_2x^2 + c_1x + c_0$$\nwhere $c_2, c_1$, and $c_0$ are the coefficients in the standard monomial basis $\\{x^2, x, 1\\}$.\n\nYour task is to perform an algebraic conversion to find the expressions for the coefficients $c_2, c_1$, and $c_0$ in terms of the Newton form parameters $a_0, a_1, a_2, x_0$, and $x_1$. Present your final answer as a single row matrix containing the expressions for $c_2, c_1,$ and $c_0$, in that specific order.", "solution": "Start from the given Newton form:\n$$P_2(x)=a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1).$$\nExpand the quadratic factor:\n$$(x-x_0)(x-x_1)=x^2-(x_0+x_1)x+x_0x_1.$$\nSubstitute this back and distribute:\n$$P_2(x)=a_0+a_1x-a_1x_0+a_2\\left[x^2-(x_0+x_1)x+x_0x_1\\right].$$\nGroup like powers of $x$ to match $P_2(x)=c_2x^2+c_1x+c_0$:\n$$P_2(x)=\\underbrace{a_2}_{c_2}x^2+\\underbrace{\\left(a_1-a_2(x_0+x_1)\\right)}_{c_1}x+\\underbrace{\\left(a_0-a_1x_0+a_2x_0x_1\\right)}_{c_0}.$$\nTherefore,\n$$c_2=a_2,\\quad c_1=a_1-a_2(x_0+x_1),\\quad c_0=a_0-a_1x_0+a_2x_0x_1.$$", "answer": "$$\\boxed{\\begin{pmatrix} a_2 & a_1-a_2(x_0+x_1) & a_0-a_1x_0+a_2x_0x_1 \\end{pmatrix}}$$", "id": "2189970"}, {"introduction": "The coefficients of a Newton polynomial are its \"divided differences,\" which are calculated using a recursive formula. While visualizing this as a large triangular table is helpful, a more memory-efficient \"in-place\" algorithm is crucial for practical computation. This exercise asks you to manually trace this clever algorithm for a given set of data points, providing a concrete, step-by-step understanding of how the final coefficients are generated. [@problem_id:2189914]", "problem": "In numerical analysis, Newton's form of the interpolating polynomial is a powerful tool for constructing a polynomial that passes through a given set of data points. The coefficients of this polynomial are the divided differences, denoted $f[x_0, \\dots, x_k]$. A common method for computing these coefficients involves constructing a large two-dimensional table.\n\nConsider a memory-efficient, in-place algorithm designed to compute these coefficients using only a single one-dimensional array of size $n+1$. This algorithm operates on a set of $n+1$ data points $(x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)$.\n\nThe algorithm proceeds as follows:\n1.  Initialize a one-dimensional array, let's call it $A$, of size $n+1$ with the dependent variable values. That is, $A_i$ is set to $y_i$ for $i = 0, 1, \\dots, n$.\n2.  Execute a nested loop. The outer loop iterates with an index $k$ from $1$ to $n$. The inner loop iterates with an index $j$ from $n$ down to $k$.\n3.  Inside the inner loop, update the array element $A_j$ using the formula:\n    $$A_j \\leftarrow \\frac{A_j - A_{j-1}}{x_j - x_{j-k}}$$\n\nAfter the algorithm terminates, the array $A$ will contain the required coefficients for the Newton polynomial, i.e., $A_k = f[x_0, \\dots, x_k]$ for $k=0, \\dots, n$.\n\nSuppose you are given the following four data points (so $n=3$):\n- $(x_0, y_0) = (-2, -5)$\n- $(x_1, y_1) = (-1, 1)$\n- $(x_2, y_2) = (1, 1)$\n- $(x_3, y_3) = (2, 7)$\n\nApply the memory-efficient algorithm described above to these data points. What are the final contents of the array $A$ after the algorithm completes its execution? Present your answer as a sequence of four numbers representing the elements $A_0, A_1, A_2, A_3$.", "solution": "We are given data points $(x_0,y_0)=(-2,-5)$, $(x_1,y_1)=(-1,1)$, $(x_2,y_2)=(1,1)$, $(x_3,y_3)=(2,7)$ and must run the in-place divided-differences algorithm:\n1) Initialize $A_i=y_i$ for $i=0,1,2,3$, so\n$$A_0=-5,\\quad A_1=1,\\quad A_2=1,\\quad A_3=7.$$\n\n2) For $k=1$ to $3$, for $j=3$ down to $k$, update\n$$A_j\\leftarrow \\frac{A_j-A_{j-1}}{x_j-x_{j-k}}.$$\n\nOuter loop $k=1$:\n- For $j=3$:\n$$A_3\\leftarrow \\frac{A_3-A_2}{x_3-x_2}=\\frac{7-1}{2-1}=6.$$\n- For $j=2$:\n$$A_2\\leftarrow \\frac{A_2-A_1}{x_2-x_1}=\\frac{1-1}{1-(-1)}=\\frac{0}{2}=0.$$\n- For $j=1$:\n$$A_1\\leftarrow \\frac{A_1-A_0}{x_1-x_0}=\\frac{1-(-5)}{-1-(-2)}=\\frac{6}{1}=6.$$\nArray after $k=1$:\n$$A=\\left(-5,\\ 6,\\ 0,\\ 6\\right).$$\n\nOuter loop $k=2$:\n- For $j=3$:\n$$A_3\\leftarrow \\frac{A_3-A_2}{x_3-x_1}=\\frac{6-0}{2-(-1)}=\\frac{6}{3}=2.$$\n- For $j=2$:\n$$A_2\\leftarrow \\frac{A_2-A_1}{x_2-x_0}=\\frac{0-6}{1-(-2)}=\\frac{-6}{3}=-2.$$\nArray after $k=2$:\n$$A=\\left(-5,\\ 6,\\ -2,\\ 2\\right).$$\n\nOuter loop $k=3$:\n- For $j=3$:\n$$A_3\\leftarrow \\frac{A_3-A_2}{x_3-x_0}=\\frac{2-(-2)}{2-(-2)}=\\frac{4}{4}=1.$$\nArray after $k=3$:\n$$A=\\left(-5,\\ 6,\\ -2,\\ 1\\right).$$\n\nThus the final contents are $A_0=-5$, $A_1=6$, $A_2=-2$, $A_3=1$.", "answer": "$$\\boxed{\\begin{pmatrix}-5 & 6 & -2 & 1\\end{pmatrix}}$$", "id": "2189914"}, {"introduction": "Polynomial interpolation is powerful, but not without its pitfalls. This hands-on coding challenge addresses one of the most famous cautionary tales in numerical analysis: the Runge phenomenon, where high-degree interpolants using evenly spaced nodes can produce wild oscillations. By implementing the Newton interpolation algorithm for the Runge function, you will computationally verify this phenomenon and discover how a strategic choice of interpolation points—the Chebyshev nodes—can dramatically improve accuracy and tame these oscillations. [@problem_id:2426405]", "problem": "You are to implement polynomial interpolation in Newton form for the Runge function using two node-generation strategies. Start from the fundamental definition that for $n+1$ distinct nodes $\\{x_0,\\dots,x_n\\}$ and function values $\\{f(x_0),\\dots,f(x_n)\\}$, there exists a unique polynomial $p_n(x)$ of degree at most $n$ that satisfies $p_n(x_i)=f(x_i)$ for all $i$. Use the recursive definition of divided differences and the Newton basis to derive a numerically stable evaluation algorithm. The target function is the Runge function $f(x)=\\dfrac{1}{1+25x^2}$ on $[-1,1]$. All angles must be in radians.\n\nYour tasks are:\n- Implement a function to compute the divided differences for nodes $\\{x_i\\}_{i=0}^n$ and values $\\{f(x_i)\\}_{i=0}^n$, producing the Newton-form coefficients $\\{c_0,\\dots,c_n\\}$ for $p_n(x)$.\n- Implement a function to evaluate the Newton-form interpolant at arbitrary $x$ using nested multiplication based on the Newton basis $\\{1,(x-x_0),(x-x_0)(x-x_1),\\dots\\}$ and the previously computed coefficients $\\{c_k\\}_{k=0}^n$.\n- Generate two sets of interpolation nodes on $[-1,1]$ for each degree $n$:\n  - Equally spaced nodes: $x_i=-1+\\dfrac{2i}{n}$ for $i=0,\\dots,n$.\n  - Chebyshev nodes of the first kind (extrema) on $[-1,1]$: $x_i=\\cos\\!\\left(\\dfrac{i\\pi}{n}\\right)$ for $i=0,\\dots,n$. Use radians.\n- For each interpolant, evaluate $p_n(x)$ on a dense grid of $M$ points uniformly spaced on $[-1,1]$ and compute the maximum absolute error $E_{\\max}=\\max_{x\\in\\mathcal{G}}|p_n(x)-f(x)|$, where $\\mathcal{G}$ is the evaluation grid.\n- Do all arithmetic in double precision.\n\nTest suite specification:\n- Use the Runge function $f(x)=\\dfrac{1}{1+25x^2}$ on $[-1,1]$.\n- Use an evaluation grid $\\mathcal{G}$ of $M=10001$ equally spaced points on $[-1,1]$.\n- Use the following degrees (with $n+1$ nodes each): $n\\in\\{0,1,5,10,20\\}$.\n- For each $n$ in the above set, compute two numbers: $E_{\\max}^{\\text{eq}}(n)$ for equally spaced nodes and $E_{\\max}^{\\text{ch}}(n)$ for Chebyshev nodes (with angles in radians).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain real numbers in the following order:\n  - $\\big[E_{\\max}^{\\text{eq}}(0),E_{\\max}^{\\text{ch}}(0),E_{\\max}^{\\text{eq}}(1),E_{\\max}^{\\text{ch}}(1),E_{\\max}^{\\text{eq}}(5),E_{\\max}^{\\text{ch}}(5),E_{\\max}^{\\text{eq}}(10),E_{\\max}^{\\text{ch}}(10),E_{\\max}^{\\text{eq}}(20),E_{\\max}^{\\text{ch}}(20)\\big]$.\n- The output must be a single line and must not contain any additional text.", "solution": "The problem starts from the core definition of interpolation: given $n+1$ distinct nodes $\\{x_0,\\dots,x_n\\}$ and data values $\\{y_0,\\dots,y_n\\}$ with $y_i=f(x_i)$, there exists a unique polynomial $p_n(x)$ of degree at most $n$ such that $p_n(x_i)=y_i$ for $i=0,\\dots,n$. One constructive representation is the Newton form, which combines two fundamental pieces: divided differences and the Newton basis. The Newton basis is defined recursively as $N_0(x)=1$, $N_k(x)=(x-x_{k-1})N_{k-1}(x)$ for $k\\ge 1$, which yields $N_k(x)=\\prod_{j=0}^{k-1}(x-x_j)$. The interpolant can be expressed as $p_n(x)=\\sum_{k=0}^n c_k N_k(x)$, where the coefficients $c_k$ are the divided differences $c_k=f[x_0,\\dots,x_k]$. The divided differences are determined from the data via the recursion $f[x_i]=y_i$ and, for $k\\ge 1$, $f[x_i,\\dots,x_{i+k}]=\\dfrac{f[x_{i+1},\\dots,x_{i+k}]-f[x_i,\\dots,x_{i+k-1}]}{x_{i+k}-x_i}$. This construction directly encodes the data constraints $p_n(x_i)=y_i$ by induction on $i$, ensuring existence and uniqueness.\n\nAlgorithmic design proceeds as follows. First, compute the divided differences. We can implement the recursion in-place on an array $d$ initialized as $d_i^{(0)}=y_i$. For each order $k=1,\\dots,n$ and indices $i=0,\\dots,n-k$, update $d_i^{(k)}=\\dfrac{d_{i+1}^{(k-1)}-d_i^{(k-1)}}{x_{i+k}-x_i}$. After completing all orders, the Newton coefficients are $c_k=d_0^{(k)}$ for $k=0,\\dots,n$. This is an $O(n^2)$ operation and uses only the core recursion.\n\nSecond, evaluate the interpolant efficiently and stably by nested multiplication (a Horner-like scheme adapted to the Newton basis). Starting with $v=c_n$, accumulate $v \\leftarrow c_{k}+ (x-x_k)\\,v$ for $k=n-1,\\dots,0$. This corresponds to evaluating the polynomial in its nested form: $p_n(x)=c_0+(x-x_0)\\left(c_1+(x-x_1)\\left(\\dots+(x-x_{n-1})c_n\\right)\\right)$. This evaluation is $O(n)$ per point and numerically stable relative to a naive basis expansion.\n\nFor node placement, we compare two strategies on $[-1,1]$. Equally spaced nodes use $x_i=-1+\\dfrac{2i}{n}$ for $i=0,\\dots,n$. Chebyshev nodes of the first kind (extrema) are $x_i=\\cos\\!\\left(\\dfrac{i\\pi}{n}\\right)$ for $i=0,\\dots,n$, which cluster near the endpoints and are known to reduce the maximum interpolation error for analytic functions. All trigonometric computations use radians, as specified. The Runge function $f(x)=\\dfrac{1}{1+25x^2}$ is analytic on and around $[-1,1]$, yet it famously exhibits the Runge phenomenon when interpolated with equally spaced nodes: as $n$ increases, the maximum error $E_{\\max}$ may worsen due to oscillations near $x=\\pm 1$. Chebyshev nodes mitigate this by minimizing the Lebesgue constant growth and distributing node density where it is most needed.\n\nTo quantify the behavior, we evaluate on a dense grid $\\mathcal{G}$ of $M=10001$ equispaced points on $[-1,1]$. For each $n\\in\\{0,1,5,10,20\\}$ and each node strategy, we compute the interpolant $p_n(x)$ on $\\mathcal{G}$ and then the maximum absolute error $E_{\\max}=\\max_{x\\in\\mathcal{G}}|p_n(x)-f(x)|$. The program outputs the sequence $[E_{\\max}^{\\text{eq}}(0),E_{\\max}^{\\text{ch}}(0),E_{\\max}^{\\text{eq}}(1),E_{\\max}^{\\text{ch}}(1),E_{\\max}^{\\text{eq}}(5),E_{\\max}^{\\text{ch}}(5),E_{\\max}^{\\text{eq}}(10),E_{\\max}^{\\text{ch}}(10),E_{\\max}^{\\text{eq}}(20),E_{\\max}^{\\text{ch}}(20)]$ on a single line. We expect $E_{\\max}^{\\text{eq}}(n)$ to initially decrease and then increase with $n$ due to the Runge phenomenon, while $E_{\\max}^{\\text{ch}}(n)$ should decrease more steadily.\n\nImplementation details ensure numerical robustness:\n- Divided differences are computed in place using double-precision arrays to avoid unnecessary copies.\n- The evaluation uses vectorized nested multiplication over the grid for efficiency.\n- For the Chebyshev node generator, $n=0$ is handled explicitly, yielding the single node $x_0=\\cos(0)=1$. For $n\\ge 1$, nodes $\\cos\\!\\left(\\dfrac{i\\pi}{n}\\right)$ for $i=0,\\dots,n$ are generated and sorted to ascending order; sorting does not change the interpolating polynomial when coefficients are recomputed for the sorted order, and it yields consistent ordering across strategies.\n\nThis principled approach directly reflects the core definitions of interpolation and divided differences, employs the Newton basis to obtain an efficient algorithm, and computes the requested error metrics on a fixed grid for objective comparison across node sets and degrees.", "answer": "```python\nimport numpy as np\n\ndef runge_function(x: np.ndarray) -> np.ndarray:\n    # f(x) = 1 / (1 + 25 x^2)\n    return 1.0 / (1.0 + 25.0 * x * x)\n\ndef divided_differences(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute Newton divided differences coefficients.\n    x: nodes (n+1,)\n    y: values at nodes (n+1,)\n    Returns coefficients c such that p(x) = c0 + c1*(x-x0) + ... in Newton form.\n    \"\"\"\n    n = x.size - 1\n    dd = y.astype(float).copy()\n    # In-place computation: dd[i] overwritten by higher-order divided differences\n    for k in range(1, n + 1):\n        # Update dd[0..n-k]\n        denom = x[k:] - x[:-k]\n        # Avoid division by zero; nodes are distinct by construction\n        dd[: n - k + 1] = (dd[1: n - k + 2] - dd[: n - k + 1]) / denom\n    # Coefficients are dd[0] at each order; we need to reconstruct them\n    # We can recompute to capture dd[0] at each stage by re-running but more efficient:\n    # Build the table once and collect c's\n    # Rebuild more explicitly:\n    dd_table = y.astype(float).copy()\n    coeffs = [dd_table[0]]\n    for k in range(1, n + 1):\n        dd_table[: n - k + 1] = (dd_table[1: n - k + 2] - dd_table[: n - k + 1]) / (x[k:] - x[:-k])\n        coeffs.append(dd_table[0])\n    return np.array(coeffs, dtype=float)\n\ndef newton_evaluate(x_eval: np.ndarray, x_nodes: np.ndarray, coeffs: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Evaluate Newton-form polynomial with given nodes and coefficients at x_eval.\n    Uses nested multiplication (Horner-like) for Newton basis.\n    \"\"\"\n    # Start from highest-order coefficient\n    p = np.full_like(x_eval, fill_value=coeffs[-1], dtype=float)\n    # Iterate backwards over nodes\n    for k in range(len(coeffs) - 2, -1, -1):\n        p = coeffs[k] + (x_eval - x_nodes[k]) * p\n    return p\n\ndef equispaced_nodes(n: int) -> np.ndarray:\n    # n+1 nodes from -1 to 1 inclusive\n    return np.linspace(-1.0, 1.0, n + 1, dtype=float)\n\ndef chebyshev_extrema_nodes(n: int) -> np.ndarray:\n    # Chebyshev nodes of the first kind (extrema): x_i = cos(i*pi/n), i=0..n\n    if n == 0:\n        nodes = np.array([1.0], dtype=float)\n    else:\n        i = np.arange(0, n + 1, dtype=float)\n        nodes = np.cos(np.pi * i / float(n))\n    # Sort ascending for consistency\n    nodes.sort()\n    return nodes\n\ndef max_abs_error_on_grid(n: int, node_strategy: str, grid: np.ndarray) -> float:\n    if node_strategy == \"equispaced\":\n        x_nodes = equispaced_nodes(n)\n    elif node_strategy == \"chebyshev\":\n        x_nodes = chebyshev_extrema_nodes(n)\n    else:\n        raise ValueError(\"Unknown node strategy\")\n\n    y_nodes = runge_function(x_nodes)\n    coeffs = divided_differences(x_nodes, y_nodes)\n    p_vals = newton_evaluate(grid, x_nodes, coeffs)\n    f_vals = runge_function(grid)\n    err = np.abs(p_vals - f_vals)\n    return float(np.max(err))\n\ndef solve():\n    # Define the test cases: degrees n\n    n_values = [0, 1, 5, 10, 20]\n    # Evaluation grid of M=10001 points on [-1,1]\n    M = 10001\n    grid = np.linspace(-1.0, 1.0, M, dtype=float)\n\n    results = []\n    for n in n_values:\n        e_eq = max_abs_error_on_grid(n, \"equispaced\", grid)\n        e_ch = max_abs_error_on_grid(n, \"chebyshev\", grid)\n        results.append(e_eq)\n        results.append(e_ch)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2426405"}]}