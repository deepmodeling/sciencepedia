{"hands_on_practices": [{"introduction": "This practice bridges the gap between fundamental engineering mechanics and computational data analysis. By deriving a beam deflection model from the principles of Euler-Bernoulli theory and fitting it to hypothetical experimental data, you will gain hands-on experience in estimating a key physical parameter—the Young's Modulus, $E$. This exercise is a cornerstone of parameter estimation, demonstrating how physical laws can be transformed into regression models to characterize material properties from measurements [@problem_id:2383151].", "problem": "You are given the task of estimating the Young's Modulus $E$ of a material by fitting deflection data of a prismatic cantilever beam subjected to static loads. The modeling assumptions must be consistent with Euler–Bernoulli beam theory. Start from the following fundamental base and core definitions only, and do not use any pre-derived deflection formulas in your derivation.\n\nFundamental base:\n- For small deflections of a prismatic beam with constant second moment of area, Euler–Bernoulli beam theory states that the curvature satisfies $\\,\\kappa(x) = \\dfrac{d^2 y}{dx^2} = \\dfrac{M(x)}{E I}\\,$, where $y(x)$ is the transverse deflection, $M(x)$ is the internal bending moment, $E$ is Young's Modulus, and $I$ is the area second moment of inertia about the neutral axis.\n- For a cantilever beam fixed at $x=0$ and free at $x=L$, the boundary conditions at the fixed end are $\\,y(0)=0\\,$ and $\\,y'(0)=0\\,$.\n- For a rectangular cross section of width $b$ (in the out-of-plane direction) and thickness $h$ (in the bending direction), the area second moment of inertia is $\\,I = \\dfrac{b h^3}{12}\\,$.\n- The internal bending moment functions $M(x)$ for the two load cases of interest are:\n  1. Tip point load of magnitude $F$ applied downward at the free end $x=L$: $\\,M(x) = F\\,(L-x)\\,$ (sign convention chosen so that downward deflection is positive).\n  2. Uniformly distributed load of intensity $q$ (force per unit length) applied downward along the entire span $[0,L]$: $\\,M(x) = \\dfrac{q}{2}\\,(L-x)^2\\,$.\n\nTask:\n1. From $\\,\\dfrac{d^2 y}{dx^2} = \\dfrac{M(x)}{E I}\\,$ with the cantilever boundary conditions at $x=0$, derive expressions for the deflection $y(x)$ for both load cases listed above, expressed purely in terms of $x$, $L$, the corresponding load parameter ($F$ or $q$), and the unknown material parameter $E$ (with $I$ known from $b$ and $h$).\n2. Show that for each load case the deflection can be written in the linear-in-parameter form $\\,y(x) = a\\,\\phi(x)\\,$ with $\\,a = \\dfrac{1}{E I}\\,$ and a known shape function $\\,\\phi(x)\\,$ determined solely by $x$, $L$, and the load parameters ($F$ or $q$).\n3. Using the linear least squares principle, given measured deflections $\\{(x_i, y_i)\\}_{i=1}^n$, determine the estimator $\\,\\hat{a}\\,$ that minimizes $\\sum_{i=1}^n \\big(y_i - a\\,\\phi(x_i)\\big)^2$. Then compute $\\,\\hat{E} = \\dfrac{1}{\\hat{a}\\,I}\\,$.\n4. Implement a complete, runnable program that:\n   - Encodes the three test cases below,\n   - Computes $I$ from $b$ and $h$,\n   - Constructs the appropriate $\\phi(x_i)$ for each test case from your derived expressions,\n   - Fits $a$ by least squares and returns $\\,\\hat{E}\\,$ for each case.\n\nPhysical units and answer format:\n- All geometric quantities are in meters, forces in newtons, and loads in newtons per meter. Young's Modulus must be reported in pascals.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is $\\,\\hat{E}\\,$ in pascals using scientific notation with three digits after the decimal (for example, $\\,2.100\\mathrm{e}{+11}\\,$). The required final output format is thus exactly like $\\,\\big[\\hat{E}_1,\\hat{E}_2,\\hat{E}_3\\big]\\,$ where each $\\,\\hat{E}_k\\,$ is a float in scientific notation with three digits after the decimal.\n\nTest suite (implement these parameter sets exactly as given; for each case, fit $\\,\\hat{E}\\,$ from the provided measurements):\n- Case A (tip point load):\n  - $L = 0.5\\,\\mathrm{m}$, $b = 0.02\\,\\mathrm{m}$, $h = 0.004\\,\\mathrm{m}$, $F = 5.0\\,\\mathrm{N}$.\n  - Positions $x$ in meters: $[\\,0.1,\\,0.2,\\,0.3,\\,0.4,\\,0.5\\,]$.\n  - Measured deflections $y$ in meters: $[\\,0.001634905658,\\,0.005786792453,\\,0.012306415076,\\,0.019904528282,\\,0.028301886750\\,]$.\n- Case B (uniformly distributed load):\n  - $L = 1.0\\,\\mathrm{m}$, $b = 0.03\\,\\mathrm{m}$, $h = 0.01\\,\\mathrm{m}$, $q = 200.0\\,\\mathrm{N/m}$.\n  - Positions $x$ in meters: $[\\,0.2,\\,0.4,\\,0.6,\\,0.8,\\,1.0\\,]$.\n  - Measured deflections $y$ in meters: $[\\,0.003307,\\,0.011611,\\,0.02263,\\,0.03495,\\,0.04766905\\,]$.\n- Case C (tip point load, minimal data):\n  - $L = 0.3\\,\\mathrm{m}$, $b = 0.015\\,\\mathrm{m}$, $h = 0.003\\,\\mathrm{m}$, $F = 0.2\\,\\mathrm{N}$.\n  - Positions $x$ in meters: $[\\,0.15,\\,0.3\\,]$.\n  - Measured deflections $y$ in meters: $[\\,0.005188,\\,0.0166767\\,]$.\n\nDesign for coverage:\n- Case A is a well-conditioned multi-point fit for a tip load.\n- Case B exercises a different load case with a larger bending stiffness and UDL.\n- Case C is an edge case with only two measurements to test the one-parameter regression at minimal sample size.\n\nYour program must output a single line in the exact format: $[E_1,E_2,E_3]$, where each entry is $\\,\\hat{E}\\,$ in pascals as a float in scientific notation with three digits after the decimal, and no additional text. Ensure numerical stability by using double-precision arithmetic.", "solution": "The problem as stated is valid. It is scientifically grounded in the fundamental principles of solid mechanics, specifically Euler–Bernoulli beam theory, and employs a standard parameter estimation technique, linear least squares. The problem is well-posed, providing all necessary physical parameters, boundary conditions, and measurement data to uniquely determine the desired quantity, Young's Modulus $E$. The language is objective and the definitions are precise. Therefore, we proceed with the solution.\n\nThe task is to estimate the Young's Modulus $E$ from deflection data. This requires expressing the deflection $y(x)$ as a function that is linear in the parameter $a = \\frac{1}{EI}$, where $I$ is the area second moment of inertia. We will first derive the deflection equations for the two specified load cases.\n\n**1. Derivation of Deflection Equations**\n\nThe governing differential equation for the beam deflection $y(x)$ is given as:\n$$\n\\frac{d^2 y}{dx^2} = \\frac{M(x)}{E I}\n$$\nFor a cantilever beam fixed at $x=0$, the boundary conditions are zero deflection and zero slope at the fixed end:\n$$\ny(0) = 0 \\quad \\text{and} \\quad \\frac{dy}{dx}(0) = 0\n$$\n\n**Case I: Tip Point Load $F$**\n\nThe internal bending moment function is given as $M(x) = F(L-x)$. Substituting this into the governing equation:\n$$\n\\frac{d^2 y}{dx^2} = \\frac{F(L-x)}{E I}\n$$\nWe integrate with respect to $x$ to find the slope, $\\frac{dy}{dx}$:\n$$\n\\frac{dy}{dx}(x) = \\int \\frac{F(L-x)}{E I} dx = \\frac{F}{E I} \\left( Lx - \\frac{x^2}{2} \\right) + C_1\n$$\nApplying the boundary condition $\\frac{dy}{dx}(0) = 0$:\n$$\n\\frac{dy}{dx}(0) = \\frac{F}{E I} \\left( L(0) - \\frac{0^2}{2} \\right) + C_1 = 0 \\implies C_1 = 0\n$$\nSo, the slope is $\\frac{dy}{dx}(x) = \\frac{F}{E I} \\left( Lx - \\frac{x^2}{2} \\right)$. Next, we integrate again to find the deflection, $y(x)$:\n$$\ny(x) = \\int \\frac{F}{E I} \\left( Lx - \\frac{x^2}{2} \\right) dx = \\frac{F}{E I} \\left( L\\frac{x^2}{2} - \\frac{x^3}{6} \\right) + C_2\n$$\nApplying the boundary condition $y(0) = 0$:\n$$\ny(0) = \\frac{F}{E I} \\left( L\\frac{0^2}{2} - \\frac{0^3}{6} \\right) + C_2 = 0 \\implies C_2 = 0\n$$\nThus, the deflection equation for a tip point load is:\n$$\ny(x) = \\frac{F}{E I} \\left( \\frac{Lx^2}{2} - \\frac{x^3}{6} \\right)\n$$\n\n**Case II: Uniformly Distributed Load $q$**\n\nThe internal bending moment function is $M(x) = \\frac{q}{2}(L-x)^2$. Substituting into the governing equation:\n$$\n\\frac{d^2 y}{dx^2} = \\frac{q}{2E I}(L-x)^2\n$$\nIntegrating once to find the slope:\n$$\n\\frac{dy}{dx}(x) = \\int \\frac{q}{2E I}(L-x)^2 dx = \\frac{q}{2E I} \\left( -\\frac{(L-x)^3}{3} \\right) + C_1\n$$\nApplying the boundary condition $\\frac{dy}{dx}(0) = 0$:\n$$\n\\frac{dy}{dx}(0) = \\frac{q}{2E I} \\left( -\\frac{L^3}{3} \\right) + C_1 = 0 \\implies C_1 = \\frac{qL^3}{6EI}\n$$\nSo, the slope is $\\frac{dy}{dx}(x) = \\frac{q}{6EI} \\left( L^3 - (L-x)^3 \\right)$. Integrating again for deflection:\n$$\ny(x) = \\int \\frac{q}{6EI} \\left( L^3 - (L-x)^3 \\right) dx = \\frac{q}{6EI} \\left( L^3x + \\frac{(L-x)^4}{4} \\right) + C_2\n$$\nApplying the boundary condition $y(0) = 0$:\n$$\ny(0) = \\frac{q}{6EI} \\left( 0 + \\frac{L^4}{4} \\right) + C_2 = 0 \\implies C_2 = -\\frac{qL^4}{24EI}\n$$\nThe deflection equation is:\n$$\ny(x) = \\frac{q}{6EI} \\left( L^3x + \\frac{(L-x)^4}{4} \\right) - \\frac{qL^4}{24EI} = \\frac{q}{24EI} \\left( 4L^3x + (L-x)^4 - L^4 \\right)\n$$\nExpanding the polynomial $(L-x)^4 = L^4 - 4L^3x + 6L^2x^2 - 4Lx^3 + x^4$ and substituting gives the simplified form:\n$$\ny(x) = \\frac{q}{24EI} \\left( 6L^2x^2 - 4Lx^3 + x^4 \\right) = \\frac{qx^2}{24EI} \\left( 6L^2 - 4Lx + x^2 \\right)\n$$\n\n**2. Linear-in-Parameter Model and Least Squares Estimation**\n\nFor both cases, the deflection equation can be written in the form $y(x) = a \\phi(x)$, where $a = \\frac{1}{EI}$.\n- For the tip load $F$: $\\phi_F(x) = F \\left( \\frac{Lx^2}{2} - \\frac{x^3}{6} \\right)$.\n- For the uniform load $q$: $\\phi_q(x) = \\frac{qx^2}{24} \\left( 6L^2 - 4Lx + x^2 \\right)$.\n\nGiven a set of $n$ noisy deflection measurements $\\{(x_i, y_i)\\}_{i=1}^n$, we seek the parameter estimate $\\hat{a}$ that minimizes the sum of squared errors, $S(a)$:\n$$\nS(a) = \\sum_{i=1}^n \\left( y_i - a \\phi(x_i) \\right)^2\n$$\nTo find the minimum, we set the derivative of $S(a)$ with respect to $a$ to zero:\n$$\n\\frac{dS}{da} = \\sum_{i=1}^n 2 \\left( y_i - a\\phi(x_i) \\right) (-\\phi(x_i)) = -2 \\sum_{i=1}^n \\left( y_i\\phi(x_i) - a\\phi(x_i)^2 \\right) = 0\n$$\nThis simplifies to:\n$$\n\\sum_{i=1}^n y_i\\phi(x_i) = \\hat{a} \\sum_{i=1}^n \\phi(x_i)^2\n$$\nSolving for the estimator $\\hat{a}$:\n$$\n\\hat{a} = \\frac{\\sum_{i=1}^n y_i \\phi(x_i)}{\\sum_{i=1}^n \\phi(x_i)^2}\n$$\nThis is the standard formula for the slope of a linear regression model through the origin.\n\n**3. Computational Procedure**\n\nThe program will implement the following steps for each test case:\n1.  Calculate the area second moment of inertia $I = \\frac{b h^3}{12}$ from the given beam geometry.\n2.  Identify the load case and construct the vector of basis function values, $\\mathbf{\\Phi} = [\\phi(x_1), \\phi(x_2), \\dots, \\phi(x_n)]^T$, using the appropriate derived expression for $\\phi(x)$.\n3.  Using the vector of measured deflections $\\mathbf{y} = [y_1, y_2, \\dots, y_n]^T$, compute the estimate $\\hat{a}$ using the dot product formulation: $\\hat{a} = \\frac{\\mathbf{y}^T \\mathbf{\\Phi}}{\\mathbf{\\Phi}^T \\mathbf{\\Phi}}$.\n4.  Calculate the estimated Young's Modulus $\\hat{E}$ from $\\hat{a}$ and $I$:\n    $$\n    \\hat{E} = \\frac{1}{\\hat{a} I}\n    $$\nThe result for each test case will be formatted and printed as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for Young's Modulus E for three test cases of cantilever beam deflection.\n    The solution follows the derivation of beam deflection from Euler-Bernoulli theory\n    and uses linear least squares to fit the model parameter a = 1/(EI).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"case_id\": \"A\",\n            \"type\": \"point_load\",\n            \"L\": 0.5,  # m\n            \"b\": 0.02, # m\n            \"h\": 0.004, # m\n            \"F\": 5.0,  # N\n            \"x_data\": np.array([0.1, 0.2, 0.3, 0.4, 0.5]), # m\n            \"y_data\": np.array([0.001634905658, 0.005786792453, 0.012306415076, 0.019904528282, 0.028301886750]) # m\n        },\n        {\n            \"case_id\": \"B\",\n            \"type\": \"uniform_load\",\n            \"L\": 1.0,  # m\n            \"b\": 0.03, # m\n            \"h\": 0.01, # m\n            \"q\": 200.0, # N/m\n            \"x_data\": np.array([0.2, 0.4, 0.6, 0.8, 1.0]), # m\n            \"y_data\": np.array([0.003307, 0.011611, 0.02263, 0.03495, 0.04766905]) # m\n        },\n        {\n            \"case_id\": \"C\",\n            \"type\": \"point_load\",\n            \"L\": 0.3,  # m\n            \"b\": 0.015, # m\n            \"h\": 0.003, # m\n            \"F\": 0.2,  # N\n            \"x_data\": np.array([0.15, 0.3]), # m\n            \"y_data\": np.array([0.005188, 0.0166767]) # m\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Step 1: Calculate area second moment of inertia (I)\n        b, h = case[\"b\"], case[\"h\"]\n        I = (b * h**3) / 12\n\n        # Step 2: Construct the basis function phi(x)\n        L = case[\"L\"]\n        x = case[\"x_data\"]\n        y_measured = case[\"y_data\"]\n\n        if case[\"type\"] == \"point_load\":\n            F = case[\"F\"]\n            # phi_F(x) = F * (L*x^2/2 - x^3/6)\n            phi_values = F * (L * x**2 / 2 - x**3 / 6)\n        elif case[\"type\"] == \"uniform_load\":\n            q = case[\"q\"]\n            # phi_q(x) = (q*x^2/24) * (6*L^2 - 4*L*x + x^2)\n            phi_values = (q * x**2 / 24) * (6 * L**2 - 4 * L * x + x**2)\n        else:\n            # This case should not be reached with the given test suite\n            continue\n\n        # Step 3: Fit 'a' by linear least squares\n        # The estimator a_hat = sum(y_i * phi_i) / sum(phi_i^2)\n        # This is equivalent to (y^T * phi) / (phi^T * phi)\n        numerator = np.dot(y_measured, phi_values)\n        denominator = np.dot(phi_values, phi_values)\n        a_hat = numerator / denominator\n\n        # Step 4: Compute Young's Modulus E\n        # a = 1/(E*I) => E = 1/(a*I)\n        E_hat = 1 / (a_hat * I)\n        \n        results.append(E_hat)\n\n    # Format the final list of results into the required string format\n    # Example format required: [2.100e+11,7.000e+10,3.150e+09]\n    formatted_results = [f\"{res:.3e}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2383151"}, {"introduction": "Standard regression methods, while powerful, can be surprisingly sensitive to single errant data points or outliers. This exercise provides a dramatic, head-to-head comparison between the classic Ordinary Least Squares (OLS) estimator, which minimizes the sum of squared errors, and a robust alternative using the Huber loss function. Completing this practice will give you a concrete understanding of why and how to build models that are resilient to the imperfections common in real-world data [@problem_id:2383160].", "problem": "You are asked to implement and compare two estimators for a simple linear model with one predictor and an intercept: the Ordinary Least Squares (OLS) estimator and the Huber-loss-based robust estimator. The model is\n$$\ny_i = a\\,x_i + b + \\varepsilon_i,\\quad i=1,\\dots,N,\n$$\nwhere $a$ is the slope, $b$ is the intercept, and $\\varepsilon_i$ are residuals. For OLS, the parameter estimate $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ is defined as the minimizer of the sum of squared residuals\n$$\n(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) \\in \\arg\\min_{a,b} \\sum_{i=1}^{N} \\left(y_i - a\\,x_i - b\\right)^2.\n$$\nFor the robust estimator, define the Huber loss with parameter $\\delta>0$ by\n$$\n\\phi_\\delta(r) = \\begin{cases}\n\\dfrac{1}{2} r^2, & \\text{if } |r|\\le \\delta,\\\\\n\\delta\\left(|r| - \\dfrac{1}{2}\\delta\\right), & \\text{if } |r| > \\delta,\n\\end{cases}\n$$\nand set\n$$\n(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) \\in \\arg\\min_{a,b} \\sum_{i=1}^{N} \\phi_\\delta\\!\\left(y_i - a\\,x_i - b\\right).\n$$\n\nImplement a program that constructs the following three deterministic test cases and computes $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ and $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ for each. Use the same Huber parameter $\\delta$ for all tests, namely $\\delta = 0.1$.\n\nTest suite (each test specifies $x$, $y$, and $N$ explicitly):\n\n- Test $1$ (happy path, no outlier):\n  - $x_i = i$ for $i \\in \\{-5,-4,\\dots,4,5\\}$, hence $N=11$.\n  - $y_i = 2\\,x_i + 1$ for every $i$.\n\n- Test $2$ (single high-leverage outlier):\n  - Start from Test $1$ and append one additional point $(x_o, y_o) = (50, -100)$, hence $N=12$.\n  - Thus, $x$ is the sequence from Test $1$ with an appended $50$, and $y$ is the corresponding sequence with an appended $-100$.\n\n- Test $3$ (small-sample boundary case):\n  - $x = [0, 1]$, hence $N=2$.\n  - $y = [1, 3]$.\n\nYour program must, for each test case, compute both $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ and $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ and report them as real numbers. There are no physical units or angles involved.\n\nFinal output format: Your program should produce a single line of output containing an outer list with three inner lists, one per test case, in the fixed order\n$$\n[\\,[\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}, \\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}]_{\\text{Test }1},\\; [\\cdot]_{\\text{Test }2},\\; [\\cdot]_{\\text{Test }3}\\,],\n$$\nprinted as a comma-separated list enclosed in square brackets and with each numerical entry rounded to six digits after the decimal point. For example, the outer list contains three inner lists, and each inner list contains four real numbers in the order specified.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The task is a standard exercise in computational statistics, comparing the classical Ordinary Least Squares (OLS) estimator with a modern robust alternative based on the Huber loss function.\n\nWe are to find parameter estimates $(\\hat a, \\hat b)$ for the linear model $y_i = a x_i + b + \\varepsilon_i$.\n\n**Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator minimizes the sum of squared residuals, a loss function defined as $L_{\\mathrm{OLS}}(a, b) = \\sum_{i=1}^{N} (y_i - a\\,x_i - b)^2$. This is a quadratic, convex, and smooth function of the parameters $(a, b)$. The unique minimum is found by setting the partial derivatives with respect to $a$ and $b$ to zero. This yields the normal equations:\n$$\n\\begin{pmatrix}\n\\sum_{i=1}^{N} x_i^2 & \\sum_{i=1}^{N} x_i \\\\\n\\sum_{i=1}^{N} x_i & N\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat a_{\\mathrm{OLS}} \\\\ \\hat b_{\\mathrm{OLS}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{N} x_i y_i \\\\\n\\sum_{i=1}^{N} y_i\n\\end{pmatrix}\n$$\nSolving this $2 \\times 2$ linear system provides the closed-form analytical solutions for the parameters:\n$$\n\\hat a_{\\mathrm{OLS}} = \\frac{N \\sum x_i y_i - (\\sum x_i)(\\sum y_i)}{N \\sum x_i^2 - (\\sum x_i)^2}\n$$\n$$\n\\hat b_{\\mathrm{OLS}} = \\bar{y} - \\hat a_{\\mathrm{OLS}} \\bar{x}\n$$\nwhere $\\bar{x} = \\frac{1}{N}\\sum x_i$ and $\\bar{y} = \\frac{1}{N}\\sum y_i$. These formulas will be implemented directly for computation.\n\n**Huber-Loss-Based Robust Estimator**\n\nThe robust estimator minimizes the sum of Huber losses, $L_{\\mathrm{Huber}}(a, b) = \\sum_{i=1}^{N} \\phi_\\delta\\!\\left(y_i - a\\,x_i - b\\right)$, with the Huber loss function $\\phi_\\delta(r)$ defined as:\n$$\n\\phi_\\delta(r) = \\begin{cases}\n\\dfrac{1}{2} r^2, & \\text{if } |r|\\le \\delta,\\\\\n\\delta\\left(|r| - \\dfrac{1}{2}\\delta\\right), & \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nThis loss function behaves quadratically for small residuals (like OLS) and linearly for large residuals. This property makes the estimator robust to outliers, as large errors do not contribute quadratically to the total loss. The function $L_{\\mathrm{Huber}}(a, b)$ is convex but not continuously differentiable (it has \"kinks\" where $|y_i - a x_i - b| = \\delta$). Therefore, a closed-form solution like that for OLS does not exist.\n\nThis is a convex optimization problem, which can be solved using numerical methods. We will utilize a general-purpose numerical optimization routine, specifically `scipy.optimize.minimize`, to find the parameters $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ that minimize the total Huber loss. The OLS estimates will serve as a suitable initial guess for the iterative solver. The parameter $\\delta$ is given as $0.1$.\n\n**Implementation Across Test Cases**\n\nFor each test case, we will first construct the data vectors $x$ and $y$. Then, we will compute the OLS estimates using the analytical formulas. Subsequently, we will define the Huber loss objective function and use `scipy.optimize.minimize` to find the Huber estimates.\n\n- **Test 1:** The data points lie perfectly on the line $y_i = 2x_i + 1$. The residuals for the exact parameters $(a,b) = (2,1)$ are all zero. Both the OLS and Huber loss functions achieve their global minimum of zero at this point. Thus, we expect $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) = (\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) = (2, 1)$.\n\n- **Test 2:** An outlier at $(x_o, y_o) = (50, -100)$ is added to the data from Test $1$. This outlier has high leverage (its $x$-value is far from the mean of other $x$-values) and a large residual. OLS is known to be highly sensitive to such points; the quadratic penalty on the large residual will \"pull\" the regression line significantly toward the outlier, drastically altering the estimates from $(2, 1)$. In contrast, the Huber estimator's linear penalty for large residuals will limit the outlier's influence. We expect $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ to remain much closer to the true underlying parameters $(2, 1)$ of the other $11$ data points, demonstrating the principle of robust regression.\n\n- **Test 3:** With only two data points, $(0, 1)$ and $(1, 3)$, there exists a unique line that passes through both. The equation of this line is $y = 2x + 1$. For the parameters $(a,b) = (2,1)$, the residuals are zero. As in Test $1$, both OLS and Huber estimators will identify these exact parameters, yielding $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) = (\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) = (2, 1)$.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements and compares OLS and Huber estimators for linear regression\n    on three specified test cases.\n    \"\"\"\n\n    # --- Estimator Implementations ---\n\n    def calculate_ols(x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n        \"\"\"\n        Calculates the Ordinary Least Squares (OLS) estimator for a and b.\n        y = a*x + b\n        \"\"\"\n        N = len(x)\n        if N < 2:\n            return (np.nan, np.nan)\n\n        sum_x = np.sum(x)\n        sum_y = np.sum(y)\n        sum_xy = np.sum(x * y)\n        sum_x2 = np.sum(x**2)\n\n        denominator = N * sum_x2 - sum_x**2\n        if np.abs(denominator) < 1e-12: # Check for collinearity (all x values are the same)\n            return (np.nan, np.nan)\n\n        a_ols = (N * sum_xy - sum_x * sum_y) / denominator\n        b_ols = (sum_y - a_ols * sum_x) / N\n        \n        return a_ols, b_ols\n\n    def calculate_huber(x: np.ndarray, y: np.ndarray, delta: float) -> tuple[float, float]:\n        \"\"\"\n        Calculates the Huber-loss-based robust estimator for a and b.\n        y = a*x + b\n        \"\"\"\n        \n        def huber_loss_objective(params: np.ndarray, x_data: np.ndarray, y_data: np.ndarray, d: float) -> float:\n            \"\"\"\n            Objective function: sum of Huber losses for a given set of parameters.\n            \"\"\"\n            a, b = params\n            residuals = y_data - (a * x_data + b)\n            abs_residuals = np.abs(residuals)\n            \n            # Quadratic part for small residuals\n            quadratic_loss = 0.5 * residuals[abs_residuals <= d]**2\n            \n            # Linear part for large residuals\n            linear_loss = d * (abs_residuals[abs_residuals > d] - 0.5 * d)\n            \n            return np.sum(quadratic_loss) + np.sum(linear_loss)\n\n        # Use OLS estimates as a good initial guess\n        a_ols, b_ols = calculate_ols(x, y)\n        initial_guess = np.array([a_ols, b_ols])\n\n        result = minimize(\n            huber_loss_objective,\n            x0=initial_guess,\n            args=(x, y, delta),\n            method='BFGS' # A standard quasi-Newton method suitable for this problem\n        )\n\n        a_huber, b_huber = result.x\n        return a_huber, b_huber\n\n    # --- Test Cases Definition ---\n    \n    # Test 1: Happy path, no outlier\n    x1 = np.arange(-5, 6, 1, dtype=float)\n    y1 = 2 * x1 + 1\n    \n    # Test 2: Single high-leverage outlier\n    x2 = np.append(x1, 50.0)\n    y2 = np.append(y1, -100.0)\n    \n    # Test 3: Small-sample boundary case\n    x3 = np.array([0.0, 1.0])\n    y3 = np.array([1.0, 3.0])\n    \n    test_cases = [\n        (x1, y1),\n        (x2, y2),\n        (x3, y3)\n    ]\n    \n    delta = 0.1\n    all_results = []\n    \n    for x_data, y_data in test_cases:\n        a_ols, b_ols = calculate_ols(x_data, y_data)\n        a_huber, b_huber = calculate_huber(x_data, y_data, delta)\n        \n        case_results = [a_ols, b_ols, a_huber, b_huber]\n        all_results.append(case_results)\n        \n    # --- Format and Print Output ---\n    \n    outer_list_str = []\n    for inner_list in all_results:\n        inner_list_str = [f\"{val:.6f}\" for val in inner_list]\n        outer_list_str.append(f\"[{','.join(inner_list_str)}]\")\n        \n    final_output = f\"[{','.join(outer_list_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2383160"}, {"introduction": "Moving beyond standard techniques, this advanced practice challenges you to implement a modern regression algorithm designed to find sparse solutions—models where many coefficients are exactly zero. You will derive a solver for a non-convex, $L_{0.5}$-regularized model from a first-principles optimization strategy known as the Majorization-Minimization framework. This exercise provides deep insight into the computational machinery behind cutting-edge machine learning methods used for feature selection and building interpretable models [@problem_id:2383204].", "problem": "You are asked to implement and analyze a regression model with a non-convex quasi-norm regularizer within a principled optimization framework. The goal is to fit a linear model to data while penalizing the coefficients using the quasi-norm corresponding to the exponent $0.5$, often informally referred to as the $L_{0.5}$ norm, which induces stronger sparsity than the standard $L_{1}$ norm. The design must begin from core definitions of least-squares data fitting and proceed using fundamental inequalities and concavity properties to produce a convergent, implementable numerical scheme that can be run on a computer.\n\nProblem setting. Given a data matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^{n}$, consider the following regularized least-squares objective\n$$\n\\min_{w \\in \\mathbb{R}^{d}} \\; \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{d} \\left( \\lvert w_j \\rvert^{2} + \\varepsilon \\right)^{p/2},\n$$\nwhere $p = 0.5$, $\\lambda > 0$ is the regularization parameter, and $\\varepsilon > 0$ is a smoothing parameter that makes the penalty finite and differentiable at $w_j = 0$. All angles in trigonometric functions are in radians. You must use the smoothing parameter $\\varepsilon = 10^{-2}$ and $p = 0.5$.\n\nTasks.\n1) Derive, from first principles, an iterative reweighted least-squares scheme grounded in a Majorization–Minimization (MM) argument. Start from the definition of least squares, the chain rule for differentiation, and the concavity of the function $t \\mapsto (t + \\varepsilon)^{p/2}$ for $t \\ge 0$ and $p \\in (0,1)$. Construct a quadratic majorizer in $w$ and obtain a linear system update for $w$ at each iteration. Do not assume a pre-existing formula; derive it.\n2) Implement the derived algorithm. Use the following settings for the numerical method: initialize with $w^{(0)} = 0$, use the stopping criterion $\\lVert w^{(k+1)} - w^{(k)} \\rVert_{2} \\le \\text{tol} \\cdot \\left(\\lVert w^{(k)} \\rVert_{2} + 10^{-12}\\right)$ with $\\text{tol} = 10^{-10}$ or stop after a maximum of $500$ iterations, whichever occurs first. For numerical stability, you may add a ridge term $10^{-12} I$ to any positive semidefinite system matrix you need to invert.\n3) For each test case described below, compute the minimized objective value\n$$\nJ^\\star = \\frac{1}{2}\\lVert y - X w^\\star \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{d} \\left( \\lvert w^\\star_j \\rvert^{2} + \\varepsilon \\right)^{p/2}\n$$\nattained by your algorithm.\n4) Output formatting: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$. Each $r_i$ must be the minimized objective value $J^\\star$ for test case $i$, rounded to $6$ decimal places. No other text should be printed.\n\nTest suite. Construct each $(X,y,\\lambda)$ deterministically as instructed below. For all definitions, indices $i$ and $j$ are $1$-based in the mathematical description. Angles are in radians.\n\n- Test Case A (happy path, moderately sparse ground truth):\n  - Dimensions: $n = 20$, $d = 8$.\n  - For $1 \\le i \\le 20$, $1 \\le j \\le 8$:\n    $$\n    X_{ij} = \\sin(0.7\\, i + 0.3\\, j) \\;+\\; 0.1 \\cos(2\\, i - 0.2\\, j).\n    $$\n  - Ground-truth coefficients:\n    $$\n    w_{\\text{true}} = [\\,1.5,\\, 0,\\, -2.0,\\, 0,\\, 0,\\, 0.5,\\, 0,\\, 0\\,]^\\top.\n    $$\n  - Response:\n    $$\n    y_i = \\sum_{j=1}^{8} X_{ij}\\, (w_{\\text{true}})_j \\;+\\; 0.02 \\sin(3\\, i).\n    $$\n  - Regularization: $\\lambda = 0.1$.\n\n- Test Case B (ill-conditioned design with near-collinear features):\n  - Dimensions: $n = 16$, $d = 6$.\n  - Define sequences for $1 \\le i \\le 16$:\n    $$\n    b_i = \\sin(0.2\\, i),\\quad c_i = \\cos(0.3\\, i),\\quad s_i = \\sin(0.5\\, i),\\quad t_i = \\cos(0.05\\, i).\n    $$\n  - Columns of $X$ for $1 \\le i \\le 16$:\n    $$\n    X_{i1} = b_i,\\quad\n    X_{i2} = 0.995\\, b_i + 0.01\\, c_i,\\quad\n    X_{i3} = c_i,\\quad\n    X_{i4} = s_i,\\quad\n    X_{i5} = X_{i1} + X_{i3},\\quad\n    X_{i6} = t_i.\n    $$\n  - Ground-truth coefficients:\n    $$\n    w_{\\text{true}} = [\\,1.0,\\, -1.0,\\, 0.0,\\, 0.5,\\, 0.0,\\, 0.0\\,]^\\top.\n    $$\n  - Response:\n    $$\n    y_i = \\sum_{j=1}^{6} X_{ij}\\, (w_{\\text{true}})_j \\;+\\; 0.01 \\cos(4\\, i).\n    $$\n  - Regularization: $\\lambda = 0.05$.\n\n- Test Case C (high-regularization regime to induce strong shrinkage):\n  - Dimensions: $n = 12$, $d = 5$.\n  - For $1 \\le i \\le 12$, $1 \\le j \\le 5$:\n    $$\n    X_{ij} = \\sin(0.4\\, i - 0.2\\, j) \\;+\\; 0.05\\, j.\n    $$\n  - Ground-truth coefficients:\n    $$\n    w_{\\text{true}} = [\\,0.2,\\, -0.1,\\, 0.0,\\, 0.0,\\, 0.3\\,]^\\top.\n    $$\n  - Response:\n    $$\n    y_i = \\sum_{j=1}^{5} X_{ij}\\, (w_{\\text{true}})_j \\;+\\; 0.05 \\sin(2.5\\, i).\n    $$\n  - Regularization: $\\lambda = 1.0$.\n\nRequired final output format. Your program must print exactly one line:\n- A single list with $3$ floating-point numbers $[J_A,J_B,J_C]$, where $J_A$, $J_B$, and $J_C$ are the minimized objective values for Test Cases A, B, and C, respectively, each rounded to exactly $6$ decimal places, with no additional whitespace beyond commas and brackets.", "solution": "The problem is to devise and implement a numerical algorithm to solve a non-convex regularized least-squares optimization problem. The objective function to be minimized is given by\n$$\nJ(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{d} \\left( w_j^2 + \\varepsilon \\right)^{p/2}\n$$\nwhere $w \\in \\mathbb{R}^d$ is the vector of coefficients, $X \\in \\mathbb{R}^{n \\times d}$ is the data matrix, $y \\in \\mathbb{R}^n$ is the response vector, $\\lambda > 0$ is the regularization parameter, $\\varepsilon = 10^{-2}$ is a smoothing parameter, and the exponent is $p=0.5$. The penalty term is a quasi-norm that promotes sparsity. Due to its non-convex nature, direct minimization is difficult. We are instructed to derive a solution using a Majorization-Minimization (MM) framework.\n\nThe MM principle is an iterative procedure for optimization. At each iteration $k$, the difficult objective function $J(w)$ is replaced by a simpler surrogate function $Q(w | w^{(k)})$, called the majorizer, which satisfies two conditions:\n1.  $J(w) \\le Q(w | w^{(k)})$ for all $w$ (Majorization).\n2.  $J(w^{(k)}) = Q(w^{(k)} | w^{(k)})$ (Tangency).\n\nThe next iterate is found by minimizing the simpler majorizer: $w^{(k+1)} = \\arg\\min_{w} Q(w | w^{(k)})$. This procedure guarantees that the objective function value is non-increasing, i.e., $J(w^{(k+1)}) \\le J(w^{(k)})$.\n\nLet us decompose the objective function $J(w)$ into two parts: a data-fidelity term $f(w) = \\frac{1}{2}\\lVert y - Xw \\rVert_2^2$ and a regularization term $g(w) = \\lambda \\sum_{j=1}^{d} (w_j^2 + \\varepsilon)^{p/2}$. The term $f(w)$ is a convex quadratic and does not require majorization. We will construct a majorizer for the non-convex term $g(w)$.\n\nThe regularizer $g(w)$ is a sum of separable functions, $g(w) = \\sum_{j=1}^d g_j(w_j)$, where $g_j(w_j) = \\lambda (w_j^2 + \\varepsilon)^{p/2}$. Let us introduce a scalar function $h(t) = (t + \\varepsilon)^{p/2}$ for $t \\ge 0$, so that $g_j(w_j) = \\lambda h(w_j^2)$.\n\nThe problem directs us to use the concavity of $h(t)$. We verify this by examining its second derivative with respect to $t$. The first and second derivatives are:\n$$\nh'(t) = \\frac{p}{2} (t + \\varepsilon)^{\\frac{p}{2}-1}\n$$\n$$\nh''(t) = \\frac{p}{2}\\left(\\frac{p}{2}-1\\right) (t + \\varepsilon)^{\\frac{p}{2}-2}\n$$\nFor the given parameter $p=0.5$, we have $p/2 = 0.25$ and $p/2-1 = -0.75$. Both factors are non-zero. Since $t \\ge 0$ and $\\varepsilon = 10^{-2} > 0$, the term $(t + \\varepsilon)$ is strictly positive. The sign of $h''(t)$ is determined by the sign of the product $\\frac{p}{2}(\\frac{p}{2}-1)$. With $p=0.5$, this product is $0.25 \\times (-0.75) = -0.1875$, which is negative. Thus, $h''(t) < 0$ for all $t \\ge 0$, proving that $h(t)$ is a strictly concave function.\n\nFor a concave function, its tangent line at any point provides a global upper bound. That is, for any $t$ and $t_k$ in its domain:\n$$\nh(t) \\le h(t_k) + h'(t_k)(t - t_k)\n$$\nWe apply this inequality to majorize each term $h(w_j^2)$ around the value at the current iterate, $w^{(k)}$. Let $t = w_j^2$ and $t_k = (w_j^{(k)})^2$. The inequality becomes:\n$$\n(w_j^2 + \\varepsilon)^{p/2} \\le ((w_j^{(k)})^2 + \\varepsilon)^{p/2} + h'((w_j^{(k)})^2) (w_j^2 - (w_j^{(k)})^2)\n$$\nLet us define the weight $\\gamma_j^{(k)} = h'((w_j^{(k)})^2) = \\frac{p}{2}((w_j^{(k)})^2 + \\varepsilon)^{\\frac{p}{2}-1}$. The majorization for the $j$-th penalty component is:\n$$\n\\lambda (w_j^2 + \\varepsilon)^{p/2} \\le \\lambda \\gamma_j^{(k)} w_j^2 + C_j^{(k)}\n$$\nwhere $C_j^{(k)} = \\lambda (h((w_j^{(k)})^2) - \\gamma_j^{(k)} (w_j^{(k)})^2)$ is a constant with respect to $w_j$.\n\nSumming over all components $j=1, \\dots, d$, we obtain a majorizer for the full regularization term $g(w)$. Combining this with the exact data-fidelity term $f(w)$, we form the global majorizing function $Q(w | w^{(k)})$ for the objective $J(w)$:\n$$\nQ(w | w^{(k)}) = \\frac{1}{2}\\lVert y - Xw \\rVert_2^2 + \\lambda \\sum_{j=1}^d \\gamma_j^{(k)} w_j^2 + C^{(k)}\n$$\nwhere $C^{(k)} = \\sum_j C_j^{(k)}$ is a constant with respect to $w$. This majorizer $Q$ is a simple quadratic function of $w$.\n\nThe next iterate, $w^{(k+1)}$, is found by minimizing $Q(w | w^{(k)})$. We find the minimum by setting the gradient of $Q$ with respect to $w$ to zero. First, we express $Q$ in matrix notation:\n$$\nQ(w | w^{(k)}) = \\frac{1}{2}(y - Xw)^T(y - Xw) + \\lambda w^T \\Gamma^{(k)} w + C^{(k)}\n$$\nwhere $\\Gamma^{(k)}$ is a diagonal matrix with diagonal entries $\\Gamma_{jj}^{(k)} = \\gamma_j^{(k)}$. The gradient is:\n$$\n\\nabla_w Q(w | w^{(k)}) = -X^T(y - Xw) + 2\\lambda \\Gamma^{(k)} w = (X^T X + 2\\lambda \\Gamma^{(k)})w - X^T y\n$$\nSetting the gradient to zero, we obtain a linear system for the optimal $w$:\n$$\n(X^T X + 2\\lambda \\Gamma^{(k)}) w = X^T y\n$$\nThis gives the update rule for the MM algorithm:\n$$\nw^{(k+1)} = (X^T X + 2\\lambda \\Gamma^{(k)})^{-1} X^T y\n$$\nThis is an instance of an Iteratively Reweighted Least Squares (IRLS) algorithm. The \"weights\" are the diagonal elements of $2\\lambda \\Gamma^{(k)}$, which are updated at each iteration based on the current estimate $w^{(k)}$.\n\nThe complete algorithm is as follows:\n1.  **Initialization**: Set the iteration counter $k=0$ and initialize the coefficient vector $w^{(0)}=0$. Define constants: $p=0.5$, $\\varepsilon=10^{-2}$, tolerance $\\text{tol}=10^{-10}$, and maximum iterations $N_{\\text{max}}=500$.\n2.  **Iteration**: For $k=0, 1, 2, \\dots, N_{\\text{max}}-1$:\n    a.  Compute the weights for $j=1, \\dots, d$:\n        $$\n        \\gamma_j^{(k)} = \\frac{p}{2} \\left( (w_j^{(k)})^2 + \\varepsilon \\right)^{\\frac{p}{2}-1} = 0.25 \\left( (w_j^{(k)})^2 + 10^{-2} \\right)^{-0.75}\n        $$\n    b.  Construct the diagonal matrix of weights $\\Gamma^{(k)} = \\text{diag}(\\gamma_1^{(k)}, \\dots, \\gamma_d^{(k)})$.\n    c.  Solve the linear system for $w^{(k+1)}$:\n        $$\n        (X^T X + 2\\lambda \\Gamma^{(k)} + \\delta I) w^{(k+1)} = X^T y\n        $$\n        where $\\delta I$ is a small ridge term (with $\\delta=10^{-12}$) added for numerical stability, ensuring the system matrix is always well-conditioned and invertible.\n    d.  Check for convergence: If $\\lVert w^{(k+1)} - w^{(k)} \\rVert_{2} \\le \\text{tol} \\cdot (\\lVert w^{(k)} \\rVert_{2} + 10^{-12})$, terminate the loop.\n3.  **Output**: The final converged vector $w^\\star$ is the result. We then compute the objective value $J(w^\\star)$.\nThis principled derivation provides a robust and convergent numerical scheme to find a stationary point of the non-convex objective function.", "answer": "```python\nimport numpy as np\n\ndef mm_solver(X, y, lambda_reg):\n    \"\"\"\n    Solves the L0.5-regularized least squares problem using a\n    Majorization-Minimization (MM) algorithm, which results in an\n    Iteratively Reweighted Least Squares (IRLS) scheme.\n    \"\"\"\n    n, d = X.shape\n    \n    # Parameters from the problem statement\n    p = 0.5\n    epsilon = 1e-2\n    tol = 1e-10\n    max_iter = 500\n    ridge_term = 1e-12\n\n    # Initialize weights vector\n    w = np.zeros(d)\n    \n    # Pre-compute constant parts of the linear system\n    XtX = X.T @ X\n    Xty = X.T @ y\n\n    for k in range(max_iter):\n        w_old = w.copy()\n        \n        # 1. Compute weights for the current iterate w\n        w_sq = w_old**2\n        gamma_vals = (p / 2) * (w_sq + epsilon)**(p / 2 - 1)\n        Gamma_k = np.diag(gamma_vals)\n        \n        # 2. Form the system matrix and solve for the new w\n        # (XtX + 2*lambda*Gamma_k + delta*I) w_new = Xty\n        system_matrix = XtX + 2 * lambda_reg * Gamma_k + ridge_term * np.identity(d)\n        \n        try:\n            w = np.linalg.solve(system_matrix, Xty)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if solve fails, though unlikely with ridge\n            w = np.linalg.pinv(system_matrix) @ Xty\n\n        # 3. Check for convergence\n        norm_w_old = np.linalg.norm(w_old)\n        norm_diff = np.linalg.norm(w - w_old)\n        \n        if norm_diff <= tol * (norm_w_old + 1e-12):\n            break\n            \n    return w\n\ndef calculate_objective(X, y, w, lambda_reg):\n    \"\"\"\n    Calculates the value of the objective function.\n    \"\"\"\n    p = 0.5\n    epsilon = 1e-2\n    \n    residual = y - X @ w\n    least_squares_term = 0.5 * np.sum(residual**2)\n    \n    penalty_term = lambda_reg * np.sum((w**2 + epsilon)**(p/2))\n    \n    return least_squares_term + penalty_term\n\ndef generate_test_case_A():\n    n, d, lambda_reg = 20, 8, 0.1\n    X = np.zeros((n, d))\n    i_vals = np.arange(1, n + 1)\n    j_vals = np.arange(1, d + 1)\n    \n    for i_idx, i in enumerate(i_vals):\n        for j_idx, j in enumerate(j_vals):\n            X[i_idx, j_idx] = np.sin(0.7 * i + 0.3 * j) + 0.1 * np.cos(2 * i - 0.2 * j)\n            \n    w_true = np.array([1.5, 0, -2.0, 0, 0, 0.5, 0, 0])\n    noise = 0.02 * np.sin(3 * i_vals)\n    y = X @ w_true + noise\n    \n    return X, y, lambda_reg\n\ndef generate_test_case_B():\n    n, d, lambda_reg = 16, 6, 0.05\n    X = np.zeros((n, d))\n    i_vals = np.arange(1, n + 1)\n    \n    b = np.sin(0.2 * i_vals)\n    c = np.cos(0.3 * i_vals)\n    s = np.sin(0.5 * i_vals)\n    t = np.cos(0.05 * i_vals)\n    \n    X[:, 0] = b\n    X[:, 1] = 0.995 * b + 0.01 * c\n    X[:, 2] = c\n    X[:, 3] = s\n    X[:, 4] = X[:, 0] + X[:, 2]\n    X[:, 5] = t\n    \n    w_true = np.array([1.0, -1.0, 0.0, 0.5, 0.0, 0.0])\n    noise = 0.01 * np.cos(4 * i_vals)\n    y = X @ w_true + noise\n    \n    return X, y, lambda_reg\n    \ndef generate_test_case_C():\n    n, d, lambda_reg = 12, 5, 1.0\n    X = np.zeros((n, d))\n    i_vals = np.arange(1, n + 1)\n    j_vals = np.arange(1, d + 1)\n    \n    for i_idx, i in enumerate(i_vals):\n        for j_idx, j in enumerate(j_vals):\n            X[i_idx, j_idx] = np.sin(0.4 * i - 0.2 * j) + 0.05 * j\n            \n    w_true = np.array([0.2, -0.1, 0.0, 0.0, 0.3])\n    noise = 0.05 * np.sin(2.5 * i_vals)\n    y = X @ w_true + noise\n    \n    return X, y, lambda_reg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases_generators = [\n        generate_test_case_A,\n        generate_test_case_B,\n        generate_test_case_C\n    ]\n    \n    results = []\n    \n    for generator in test_cases_generators:\n        X, y, lambda_reg = generator()\n        w_star = mm_solver(X, y, lambda_reg)\n        objective_value = calculate_objective(X, y, w_star, lambda_reg)\n        results.append(f\"{objective_value:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2383204"}]}