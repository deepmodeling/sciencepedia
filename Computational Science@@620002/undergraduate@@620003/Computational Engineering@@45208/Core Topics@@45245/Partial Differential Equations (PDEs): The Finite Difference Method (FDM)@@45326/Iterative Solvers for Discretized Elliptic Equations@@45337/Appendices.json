{"hands_on_practices": [{"introduction": "This first practice focuses on implementing the Conjugate Gradient (CG) method, a cornerstone of modern iterative solvers for symmetric positive definite systems. You will learn the crucial \"matrix-free\" technique, where the system matrix $A$ is never explicitly formed, but its action on a vector is provided as a function call. This approach is fundamental for solving the large-scale linear systems that arise from PDE discretizations, where storing the full matrix would be prohibitively expensive in terms of memory. [@problem_id:2406207]", "problem": "Consider the linear system arising from the standard five-point finite difference discretization of the Dirichlet problem for the Poisson equation on the unit square. Let the continuous problem be given by $-\\Delta u = f$ on $(0,1)\\times(0,1)$ with $u=0$ on the boundary. For a uniform grid with $n$ interior points per coordinate direction, the grid spacing is $h = \\frac{1}{n+1}$, and the unknowns can be ordered lexicographically into a vector in $\\mathbb{R}^{n^2}$. Define the linear operator $A:\\mathbb{R}^{n^2}\\to\\mathbb{R}^{n^2}$ corresponding to the discrete negative Laplacian with homogeneous Dirichlet boundary conditions by the standard five-point stencil: for each interior grid point with indices $(i,j)$ (where $i\\in\\{1,\\dots,n\\}$ and $j\\in\\{1,\\dots,n\\}$), the action $(A u)_{i,j}$ is:\n$$(A u)_{i,j} \\;=\\; \\frac{1}{h^2}\\left(4\\,u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}\\right),$$\nwith the convention that $u_{0,j}=u_{n+1,j}=u_{i,0}=u_{i,n+1}=0$ due to the Dirichlet boundary conditions. The vector form of $A u$ is obtained by flattening the two-dimensional array $u_{i,j}$ to a one-dimensional array in row-major order. The operator $A$ is symmetric positive definite.\n\nYour task is to write a complete, runnable program that, for a set of test cases, solves $A u = b$ using a matrix-free implementation of the Conjugate Gradient (CG) method, where the only access to $A$ is through a function that computes $v \\mapsto A v$; no explicit sparse or dense matrix representation of $A$ is allowed. Use the initial guess $u^{(0)}=0$ and terminate when the relative residual norm satisfies\n$$\\frac{\\|r^{(k)}\\|_2}{\\|b\\|_2} \\le \\mathrm{tol},$$\nor when the iteration count exceeds a prescribed maximum. Angles for any trigonometric functions must be in radians.\n\nTo make the right-hand side compatible with a known exact discrete solution, use the manufactured discrete solution:\n$$u_{i,j}^{\\star} \\;=\\; \\sin(\\pi x_i)\\,\\sin(\\pi y_j), \\quad x_i = i h,\\; y_j = j h,$$\nand define $b := A u^{\\star}$ by applying the operator $A$ to $u^{\\star}$ as specified above. For each test case, compute the approximate solution $u$ by the matrix-free solver and then verify both of the following:\n- The final relative residual is at most the specified tolerance $\\mathrm{tol}$.\n- The relative $2$-norm error with respect to $u^{\\star}$ satisfies\n$$\\frac{\\|u - u^{\\star}\\|_2}{\\|u^{\\star}\\|_2} \\le 100\\,\\mathrm{tol}.$$\n\nTest Suite:\nEvaluate your program on the following parameter pairs $(n,\\mathrm{tol})$:\n- $(1,\\,10^{-12})$,\n- $(8,\\,10^{-10})$,\n- $(32,\\,10^{-8})$,\n- $(64,\\,10^{-8})$.\n\nFor each test case, return a boolean indicating whether both verification conditions above are satisfied. Use a maximum iteration count equal to $n^2$ for each case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above. For example, if all test cases pass, the output must be exactly\n\"[True,True,True,True]\".", "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, and objective. It presents a standard problem in computational science: solving a linear system derived from a finite difference discretization of a partial differential equation using an iterative method. All parameters and conditions are specified with sufficient clarity to permit a unique and verifiable solution.\n\nThe task is to solve the linear system of equations $A u = b$ using a matrix-free implementation of the Conjugate Gradient (CG) method. This system arises from the five-point finite difference discretization of the Poisson equation, $-\\Delta u = f$, on the unit square $\\Omega = (0,1) \\times (0,1)$ with homogeneous Dirichlet boundary conditions, $u=0$ on $\\partial\\Omega$.\n\nFor a uniform grid with $n$ interior points in each coordinate direction, the grid spacing is $h = \\frac{1}{n+1}$. The discrete operator $A$ represents the negative Laplacian, mapping a function on the grid points to another function. Its action on a grid function $u$, at an interior point $(i,j)$ where $i,j \\in \\{1, \\ldots, n\\}$, is given by the five-point stencil:\n$$ (A u)_{i,j} = \\frac{1}{h^2} \\left( 4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} \\right). $$\nThe boundary conditions $u=0$ are incorporated by setting $u_{i,j}=0$ if $i$ or $j$ is $0$ or $n+1$. The resulting operator $A$ is symmetric and positive definite (SPD), which is a necessary condition for the convergence of the Conjugate Gradient method.\n\nThe problem is constructed using the Method of Manufactured Solutions to provide a case with a known exact discrete solution. The designated exact solution is:\n$$ u_{i,j}^{\\star} = \\sin(\\pi x_i) \\sin(\\pi y_j), \\quad \\text{where } x_i = i h \\text{ and } y_j = j h. $$\nThe right-hand side vector $b$ is then defined as $b = A u^{\\star}$, which is computed by applying the discrete operator $A$ to the known solution $u^{\\star}$.\n\nThe core of the solution is the Conjugate Gradient algorithm. The algorithm is iterative and is particularly suitable for large, sparse systems where the matrix $A$ is SPD. A key requirement here is that the implementation must be \"matrix-free,\" meaning the matrix $A$ is never explicitly constructed or stored. Instead, its action $v \\mapsto Av$ is provided by a function.\n\nThe algorithm proceeds as follows:\n$1$. Initialize the solution vector $u^{(0)} = 0$.\n$2$. Compute the initial residual $r^{(0)} = b - A u^{(0)} = b$.\n$3$. Set the initial search direction $p^{(0)} = r^{(0)}$.\n$4$. For $k = 0, 1, 2, \\dots$ until convergence:\n    a. Compute the matrix-vector product $v^{(k)} = A p^{(k)}$.\n    b. Compute the step size $\\alpha_k = \\frac{r^{(k)T} r^{(k)}}{p^{(k)T} v^{(k)}}$.\n    c. Update the solution: $u^{(k+1)} = u^{(k)} + \\alpha_k p^{(k)}$.\n    d. Update the residual: $r^{(k+1)} = r^{(k)} - \\alpha_k v^{(k)}$.\n    e. Check for convergence: if $\\frac{\\|r^{(k+1)}\\|_2}{\\|b\\|_2} \\le \\mathrm{tol}$, terminate.\n    f. Compute the improvement factor for the search direction: $\\beta_k = \\frac{r^{(k+1)T} r^{(k+1)}}{r^{(k)T} r^{(k)}}$.\n    g. Update the search direction: $p^{(k+1)} = r^{(k+1)} + \\beta_k p^{(k)}$.\n\nThe implementation encapsulates the action of $A$ in a dedicated function. This function accepts a one-dimensional vector of size $n^2$, internally reshapes it into an $n \\times n$ two-dimensional grid representing the solution values at the interior points. To apply the five-point stencil efficiently, this grid is padded with zeros to represent the homogeneous Dirichlet boundary conditions. The stencil operation is then applied using vectorized array operations on this padded grid. The resulting $n \\times n$ grid is flattened back into a one-dimensional vector of size $n^2$ and returned.\n\nFor each test case defined by a pair $(n, \\mathrm{tol})$, the program first sets up the grid, the matrix-free operator function, and the manufactured solution $u^{\\star}$ with its corresponding right-hand side $b$. Then, the CG solver is invoked with a maximum of $n^2$ iterations.\n\nUpon termination of the solver, two verification conditions are checked:\n$1$. The final relative residual norm $\\frac{\\|r^{(\\text{final})}\\|_2}{\\|b\\|_2}$ must be less than or equal to the prescribed tolerance $\\mathrm{tol}$.\n$2$. The relative error between the computed solution $u$ and the exact discrete solution $u^{\\star}$, measured in the Euclidean norm, $\\frac{\\|u - u^{\\star}\\|_2}{\\|u^{\\star}\\|_2}$, must be no more than $100 \\times \\mathrm{tol}$.\n\nA boolean result, `True` if both conditions are met and `False` otherwise, is determined for each test case. The final output is a list of these boolean values.", "answer": "```python\nimport numpy as np\n\ndef create_operator(n, h):\n    \"\"\"\n    Creates a matrix-free function for the 5-point discrete Laplacian operator A.\n\n    Args:\n        n (int): Number of interior grid points per dimension.\n        h (float): Grid spacing.\n\n    Returns:\n        A function that computes the matrix-vector product A*u.\n    \"\"\"\n    def A_op(u_vec):\n        \"\"\"\n        Applies the discrete Laplacian operator A to a vector u_vec.\n\n        Args:\n            u_vec (np.ndarray): A 1D vector of size n*n representing grid values.\n\n        Returns:\n            np.ndarray: The result of A*u_vec as a 1D vector.\n        \"\"\"\n        if n == 0:\n            return np.array([])\n        \n        # Reshape the 1D vector to a 2D grid\n        u_grid = u_vec.reshape((n, n))\n        \n        # Pad the grid with zeros to handle boundary conditions\n        u_padded = np.zeros((n + 2, n + 2))\n        u_padded[1:-1, 1:-1] = u_grid\n        \n        # Apply the 5-point stencil using vectorized operations\n        # The stencil is (4*u_ij - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1})\n        Au_grid = (4 * u_padded[1:-1, 1:-1] -\n                   u_padded[0:-2, 1:-1] -  # u_{i-1,j}\n                   u_padded[2:, 1:-1]   -  # u_{i+1,j}\n                   u_padded[1:-1, 0:-2] -  # u_{i,j-1}\n                   u_padded[1:-1, 2:])      # u_{i,j+1}\n        \n        # Scale by 1/h^2\n        Au_grid /= h**2\n        \n        # Flatten the resulting 2D grid back to a 1D vector\n        return Au_grid.flatten()\n    \n    return A_op\n\ndef cg_solver(A_op, b, tol, max_iter):\n    \"\"\"\n    Solves A*u = b using the Conjugate Gradient method.\n\n    Args:\n        A_op (callable): Matrix-free operator for A.\n        b (np.ndarray): Right-hand side vector.\n        tol (float): Convergence tolerance for the relative residual.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the solution vector u and the final residual vector r.\n    \"\"\"\n    u = np.zeros_like(b)\n    r = b.copy()\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        return u, r  # Trivial case: if b is zero, solution is zero.\n\n    for _ in range(max_iter):\n        Ap = A_op(p)\n        alpha = rs_old / np.dot(p, Ap)\n        \n        u += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) / b_norm <= tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return u, r\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (1, 1e-12),\n        (8, 1e-10),\n        (32, 1e-8),\n        (64, 1e-8),\n    ]\n\n    results = []\n    for n, tol in test_cases:\n        h = 1.0 / (n + 1)\n        max_iter = n * n\n        \n        # 1. Create the matrix-free operator for A\n        A_op = create_operator(n, h)\n        \n        # 2. Create the manufactured solution u_star and right-hand side b = A*u_star\n        # The problem states u*_ij = sin(pi*x_i)*sin(pi*y_j) where x_i=ih, y_j=jh.\n        # This implies relating the first grid index to x and second to y.\n        i_coords = np.arange(1, n + 1) * h\n        j_coords = np.arange(1, n + 1) * h\n        # Use 'ij' indexing so grid[i,j] corresponds to (i_coords[i], j_coords[j])\n        xx, yy = np.meshgrid(i_coords, j_coords, indexing='ij')\n        \n        u_star_grid = np.sin(np.pi * xx) * np.sin(np.pi * yy)\n        u_star_vec = u_star_grid.flatten()\n        \n        b_vec = A_op(u_star_vec)\n        \n        # 3. Solve the system A*u = b using the CG solver\n        u_sol, r_final = cg_solver(A_op, b_vec, tol, max_iter)\n        \n        # 4. Perform verification\n        b_norm = np.linalg.norm(b_vec)\n        u_star_norm = np.linalg.norm(u_star_vec)\n\n        # Verification 1: Final relative residual\n        final_rel_res = np.linalg.norm(r_final) / b_norm if b_norm > 0 else 0.0\n        check1 = final_rel_res <= tol\n\n        # Verification 2: Relative error with respect to manufactured solution\n        rel_error = np.linalg.norm(u_sol - u_star_vec) / u_star_norm if u_star_norm > 0 else 0.0\n        check2 = rel_error <= 100 * tol\n        \n        # Both checks must pass\n        results.append(check1 and check2)\n\n    # Print the final result in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406207"}, {"introduction": "While the Conjugate Gradient method is powerful, its convergence rate depends heavily on the conditioning of the system matrix. This exercise introduces preconditioning, a technique to dramatically accelerate convergence by transforming the original system into an equivalent one that is easier to solve. You will implement the Symmetric Successive Over-Relaxation (SSOR) method as a preconditioner and directly compare the performance of the standard CG method against its preconditioned counterpart, observing the significant potential for reduction in iteration counts. [@problem_id:2406195]", "problem": "Consider the elliptic boundary value problem for the scalar field $u(x,y)$ on the unit square $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions:\n$$\n-\\Delta u(x,y) = f(x,y)\\ \\text{in}\\ \\Omega,\\qquad u(x,y)=0\\ \\text{on}\\ \\partial\\Omega.\n$$\nDiscretize this problem using the standard five-point finite difference method on a uniform grid with $n$ interior points in each coordinate direction. Let $h = \\frac{1}{n+1}$ denote the grid spacing. At each interior grid point $(i,j)$, where $i,j\\in\\{1,2,\\dots,n\\}$ and $(x_i,y_j) = (ih,jh)$, the discrete equation is\n$$\n4\\,u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = h^2 f(x_i,y_j).\n$$\nAfter lexicographic ordering of the unknowns, this yields a linear system $A \\mathbf{u} = \\mathbf{b}$ of dimension $N \\times N$ with $N = n^2$, where $A$ is symmetric positive definite. In this problem, take $f(x,y) \\equiv 1$, so that each entry of $\\mathbf{b}$ equals $h^2$.\n\nDefine the Symmetric Successive Over-Relaxation (SSOR) preconditioner for a given relaxation parameter $\\omega \\in (0,2)$ as follows. Let the matrix $A$ be split as $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strict lower triangular part of $A$, and $U$ is the strict upper triangular part of $A$. The SSOR preconditioner $M(\\omega)$ is\n$$\nM(\\omega) = (D + \\omega L)\\, D^{-1}\\, (D + \\omega U).\n$$\n\nYour task is to implement a program that, for each specified test case $(n,\\omega)$, solves the system $A \\mathbf{u} = \\mathbf{b}$ using:\n- the Conjugate Gradient (CG) method without preconditioning, and\n- the preconditioned Conjugate Gradient method using the SSOR preconditioner $M(\\omega)$.\n\nFor both solvers, use the zero vector as the initial guess and stop when the relative residual norm satisfies\n$$\n\\frac{\\lVert \\mathbf{r}_k \\rVert_2}{\\lVert \\mathbf{b} \\rVert_2} \\le 10^{-8},\n$$\nor when the iteration count reaches the maximum allowed iterations $N = n^2$, whichever occurs first. Report, for each test case, the number of iterations required by unpreconditioned CG and SSOR-preconditioned CG to satisfy the stopping criterion.\n\nUse real arithmetic. All results are dimensionless; no physical units are required.\n\nTest suite:\n- Case $1$: $(n,\\omega) = (16, 1.0)$.\n- Case $2$: $(n,\\omega) = (16, 1.5)$.\n- Case $3$: $(n,\\omega) = (32, 1.5)$.\n- Case $4$: $(n,\\omega) = (8, 1.9)$.\n- Case $5$: $(n,\\omega) = (8, 0.5)$.\n\nYour program should produce a single line of output containing the results for all cases as a comma-separated list of lists, where each inner list contains exactly two integers corresponding to $[\\text{iterations of unpreconditioned CG}, \\text{iterations of SSOR-preconditioned CG}]$ for that case, in the exact order of the test suite. For example, the output format must be\n$$\n[[k_1^{\\mathrm{CG}},k_1^{\\mathrm{SSOR}}],[k_2^{\\mathrm{CG}},k_2^{\\mathrm{SSOR}}],\\dots,[k_5^{\\mathrm{CG}},k_5^{\\mathrm{SSOR}}]].\n$$", "solution": "The problem statement is critically examined and found to be valid. It is a standard, well-posed problem in the field of computational engineering, specifically concerning the iterative solution of linear systems arising from the discretization of elliptic partial differential equations. All definitions, parameters, and objectives are stated with scientific and mathematical precision. No contradictions, ambiguities, or unsound premises are present.\n\nThe task is to solve the linear system $A\\mathbf{u}=\\mathbf{b}$ using two methods: the Conjugate Gradient (CG) method and the Preconditioned Conjugate Gradient (PCG) method with a Symmetric Successive Over-Relaxation (SSOR) preconditioner. The system results from a five-point finite difference discretization of the Poisson equation, $-\\Delta u = 1$, on a unit square with homogeneous Dirichlet boundary conditions.\n\nThe solution is implemented by adhering to the following principled design:\n\n1.  **System Characterization**: The matrix $A$ resulting from the five-point stencil with lexicographic ordering is a large, sparse, block-tridiagonal, symmetric positive-definite (SPD) matrix. Its diagonal entries are $4$, and off-diagonal entries corresponding to grid neighbors are $-1$. The vector $\\mathbf{b}$ has all entries equal to $h^2$, where $h = \\frac{1}{n+1}$ is the grid spacing.\n\n2.  **Matrix-Free Implementation**: To handle the large dimension $N=n^2$ efficiently, the matrix $A$ is not constructed explicitly. Instead, a function is implemented to compute the matrix-vector product $A\\mathbf{v}$. This function reshapes the $N$-dimensional vector $\\mathbf{v}$ into an $n \\times n$ grid, applies the five-point stencil operator while enforcing the zero boundary conditions, and returns the resulting $N$-dimensional vector.\n\n3.  **Conjugate Gradient (CG) Algorithm**: A standard implementation of the CG method is employed. This method is appropriate for SPD systems like $A\\mathbf{u}=\\mathbf{b}$. Starting from an initial guess $\\mathbf{u}_0 = \\mathbf{0}$, the algorithm generates a sequence of iterates that minimizes the $A$-norm of the error. The process terminates when the relative L2-norm of the residual, $\\frac{\\lVert \\mathbf{r}_k \\rVert_2}{\\lVert \\mathbf{b} \\rVert_2}$, falls below a tolerance of $10^{-8}$ or the number of iterations reaches the maximum of $N$.\n\n4.  **SSOR Preconditioning**: The SSOR preconditioner is given by $M(\\omega) = (D + \\omega L) D^{-1} (D + \\omega U)$, where $A = D+L+U$ is the splitting of $A$ into its diagonal ($D$), strict lower triangular ($L$), and strict upper triangular ($U$) parts. For this problem, $D=4I$. The core of the PCG algorithm is the application of the inverse preconditioner, i.e., solving the system $M\\mathbf{z}=\\mathbf{r}$ for $\\mathbf{z}$. This is performed in two steps:\n    a. A forward substitution to solve $(D + \\omega L) \\mathbf{v} = \\mathbf{r}$ for an intermediate vector $\\mathbf{v}$. On the grid, this corresponds to a forward sweep:\n    $$v_{i,j} = \\frac{1}{4} (r_{i,j} + \\omega(v_{i-1,j} + v_{i,j-1}))$$\n    b. A backward substitution to solve $(D + \\omega U) \\mathbf{z} = D \\mathbf{v}$ for the result $\\mathbf{z}$. On the grid, this is a backward sweep:\n    $$z_{i,j} = v_{i,j} + \\frac{\\omega}{4}(z_{i+1,j} + z_{i,j+1})$$\n    A dedicated function implements these two sweeps to compute $\\mathbf{z} = M^{-1}\\mathbf{r}$.\n\n5.  **Preconditioned Conjugate Gradient (PCG) Algorithm**: The PCG method is implemented by integrating the SSOR preconditioner solve step into the standard CG framework. This is equivalent to applying CG to the better-conditioned system $M^{-1}A\\mathbf{u} = M^{-1}\\mathbf{b}$. The stopping criteria are identical to those for the unpreconditioned CG method.\n\nFor each test case $(n, \\omega)$, the number of iterations required by both CG and PCG are computed and recorded. The final output aggregates these results into the specified list-of-lists format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _matvec_A(v_1d, n):\n    \"\"\"\n    Computes the matrix-vector product A*v for the 2D Poisson problem.\n    The matrix A is not formed explicitly.\n    \"\"\"\n    v_2d = v_1d.reshape((n, n))\n    # Pad with 0 for homogeneous Dirichlet boundary conditions.\n    v_padded = np.pad(v_2d, pad_width=1, mode='constant', constant_values=0)\n    # Apply the 5-point stencil corresponding to the negative Laplacian.\n    av_2d = (4 * v_2d \n             - v_padded[1:-1, 0:-2]  # Left neighbor\n             - v_padded[1:-1, 2:]    # Right neighbor\n             - v_padded[0:-2, 1:-1]  # Bottom neighbor\n             - v_padded[2:, 1:-1])   # Top neighbor\n    return av_2d.flatten()\n\ndef _solve_ssor(r_1d, n, omega):\n    \"\"\"\n    Solves the SSOR preconditioning system Mz=r, returning z = M^-1 * r.\n    The preconditioner is M = (D + w*L) * D^-1 * (D + w*U), with D=4I.\n    This is solved via two sweeps: a forward substitution followed by a backward substitution.\n    \"\"\"\n    r_2d = r_1d.reshape((n, n))\n    \n    # --- Step 1: Forward substitution ---\n    # Solves (D + w*L)v = r, which is (4I + w*L)v = r.\n    # On the grid, this is: 4*v_ij - w*v_{i-1,j} - w*v_{i,j-1} = r_ij\n    v_2d = np.zeros((n, n))\n    for j in range(n):\n        for i in range(n):\n            v_left = v_2d[i - 1, j] if i > 0 else 0.0\n            v_down = v_2d[i, j - 1] if j > 0 else 0.0\n            v_2d[i, j] = (r_2d[i, j] + omega * (v_left + v_down)) / 4.0\n            \n    # --- Step 2: Backward substitution ---\n    # Solves (D + w*U)z = D*v, which is (4I + w*U)z = 4v.\n    # On the grid, this is: 4*z_ij - w*z_{i+1,j} - w*z_{i,j+1} = 4*v_ij\n    z_2d = np.zeros((n, n))\n    for j in range(n - 1, -1, -1):\n        for i in range(n - 1, -1, -1):\n            z_right = z_2d[i + 1, j] if i < n - 1 else 0.0\n            z_up = z_2d[i, j + 1] if j < n - 1 else 0.0\n            z_2d[i, j] = v_2d[i, j] + (omega / 4.0) * (z_right + z_up)\n            \n    return z_2d.flatten()\n\ndef _run_cg(n, b, b_norm, tol, max_iter):\n    \"\"\"Runs the unpreconditioned Conjugate Gradient solver.\"\"\"\n    x = np.zeros_like(b)\n    r = np.copy(b)  # Since x_0 is zero, r_0 = b - A*0 = b\n    p = np.copy(r)\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old) / b_norm <= tol:\n        return 0\n\n    for i in range(max_iter):\n        ap = _matvec_A(p, n)\n        alpha = rs_old / np.dot(p, ap)\n        x += alpha * p\n        r -= alpha * ap\n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) / b_norm <= tol:\n            return i + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return max_iter\n\ndef _run_pcg(n, omega, b, b_norm, tol, max_iter):\n    \"\"\"Runs the Preconditioned Conjugate Gradient solver with SSOR.\"\"\"\n    x = np.zeros_like(b)\n    r = np.copy(b)  # Since x_0 is zero, r_0 = b\n    \n    if b_norm == 0 or np.linalg.norm(r) / b_norm <= tol:\n        return 0\n\n    z = _solve_ssor(r, n, omega)\n    p = np.copy(z)\n    rz_old = np.dot(r, z)\n    \n    for i in range(max_iter):\n        ap = _matvec_A(p, n)\n        alpha = rz_old / np.dot(p, ap)\n        x += alpha * p\n        r -= alpha * ap\n        \n        if np.linalg.norm(r) / b_norm <= tol:\n            return i + 1\n            \n        z = _solve_ssor(r, n, omega)\n        rz_new = np.dot(r, z)\n        \n        if rz_old == 0:\n            return i + 1\n\n        p = z + (rz_new / rz_old) * p\n        rz_old = rz_new\n    \n    return max_iter\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, omega)\n        (16, 1.0),\n        (16, 1.5),\n        (32, 1.5),\n        (8, 1.9),\n        (8, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, omega = case\n        \n        # Setup problem parameters\n        N = n * n\n        h = 1.0 / (n + 1)\n        tol = 1e-8\n        max_iter = N\n\n        # Right-hand side vector b\n        b = (h**2) * np.ones(N)\n        b_norm = np.linalg.norm(b)\n\n        # Run unpreconditioned CG\n        cg_iters = _run_cg(n, b, b_norm, tol, max_iter)\n        \n        # Run SSOR-preconditioned CG\n        pcg_iters = _run_pcg(n, omega, b, b_norm, tol, max_iter)\n        \n        results.append([cg_iters, pcg_iters])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406195"}, {"introduction": "Many real-world engineering problems involve multiple coupled physical fields, leading to linear systems with a distinct block structure. This final practice extends the concepts of classical iterative methods to these more complex systems. You will implement and compare the block Jacobi and block Gauss-Seidel methods to solve a coupled elliptic problem, gaining practical experience in handling the block-structured matrices that are common in multi-physics simulations. [@problem_id:2406140]", "problem": "Consider the block linear system that arises from a two-field discretized elliptic operator on a uniform square grid with homogeneous Dirichlet boundary conditions. Let $m \\in \\mathbb{N}$ be the number of interior grid points per spatial direction, and let $h = \\frac{1}{m+1}$. Define the one-dimensional second-difference matrix $T \\in \\mathbb{R}^{m \\times m}$ by\n$$\nT = \\frac{1}{h^2}\\operatorname{tridiag}(-1,\\,2,\\,-1),\n$$\nand the two-dimensional discrete negative Laplacian $L \\in \\mathbb{R}^{m^2 \\times m^2}$ by\n$$\nL = I_m \\otimes T \\;+\\; T \\otimes I_m,\n$$\nwhere $I_m$ denotes the identity matrix of size $m$ and $\\otimes$ denotes the Kronecker product. For a fixed scalar $\\sigma > 0$, let\n$$\nK = L + \\sigma I_{m^2}.\n$$\nGiven a coupling parameter $\\alpha \\in \\mathbb{R}$, define the block matrix $A \\in \\mathbb{R}^{(2m^2)\\times(2m^2)}$ by\n$$\nA \\;=\\;\n\\begin{bmatrix}\nK & \\alpha I_{m^2}\\\\\n\\alpha I_{m^2} & K\n\\end{bmatrix}.\n$$\nLet the exact discrete fields be prescribed componentwise on the interior grid points by\n$$\nu^{\\star}(x_i,y_j) = \\sin(\\pi x_i)\\sin(\\pi y_j), \\quad v^{\\star}(x_i,y_j) = \\sin(2\\pi x_i)\\sin(\\pi y_j),\n$$\nfor $x_i = i h$, $y_j = j h$, with $i,j \\in \\{1,2,\\dots,m\\}$. Stack these values in lexicographic order to obtain vectors $u^{\\star}, v^{\\star} \\in \\mathbb{R}^{m^2}$, and define the right-hand side $b \\in \\mathbb{R}^{2m^2}$ by\n$$\n\\begin{bmatrix}b_1 \\\\ b_2\\end{bmatrix}\n=\nA\n\\begin{bmatrix}u^{\\star} \\\\ v^{\\star}\\end{bmatrix},\n\\quad \\text{that is,}\\quad\nb_1 = Ku^{\\star} + \\alpha v^{\\star}, \\;\\; b_2 = \\alpha u^{\\star} + K v^{\\star}.\n$$\n\nLet the initial guess be the zero vector in $\\mathbb{R}^{2m^2}$, and let the stopping tolerance be the relative residual threshold $\\tau = 10^{-8}$, where the relative residual at iterate $x^{(k)} \\in \\mathbb{R}^{2m^2}$ is\n$$\n\\rho^{(k)} \\;=\\; \\frac{\\|b - A x^{(k)}\\|_2}{\\|b\\|_2}.\n$$\nTerminate the iteration at the smallest $k$ such that $\\rho^{(k)} \\le \\tau$, or when a maximum of $k_{\\max} = 10000$ iterations has been reached.\n\nTwo block fixed-point iterations are to be considered. Denote a generic iterate as $x^{(k)} = \\begin{bmatrix} u^{(k)} \\\\ v^{(k)} \\end{bmatrix}$ with $u^{(k)}, v^{(k)} \\in \\mathbb{R}^{m^2}$.\n\n- Option A (block-diagonal splitting): For each iteration index $k \\ge 0$, compute\n$$\nu^{(k+1)} = K^{-1}\\left(b_1 - \\alpha v^{(k)}\\right), \\qquad\nv^{(k+1)} = K^{-1}\\left(b_2 - \\alpha u^{(k)}\\right).\n$$\n\n- Option B (block lower-triangular splitting): For each iteration index $k \\ge 0$, compute\n$$\nu^{(k+1)} = K^{-1}\\left(b_1 - \\alpha v^{(k)}\\right), \\qquad\nv^{(k+1)} = K^{-1}\\left(b_2 - \\alpha u^{(k+1)}\\right).\n$$\n\nIn both options, $K^{-1}(\\cdot)$ stands for solving a linear system with coefficient matrix $K$ exactly (up to numerical precision). The quantities $b_1$ and $b_2$ are as defined above.\n\nTest Suite. Use $\\sigma = 1$ for all cases, the zero initial guess, and the tolerance and maximum iterations defined above. Evaluate the following five test cases, each specified by the pair $(m,\\alpha)$ and the option label:\n1. $(m,\\alpha,\\text{Option}) = (8,\\,0.5,\\,\\text{A})$,\n2. $(m,\\alpha,\\text{Option}) = (8,\\,0.5,\\,\\text{B})$,\n3. $(m,\\alpha,\\text{Option}) = (1,\\,0.0,\\,\\text{A})$,\n4. $(m,\\alpha,\\text{Option}) = (12,\\,0.9,\\,\\text{A})$,\n5. $(m,\\alpha,\\text{Option}) = (12,\\,0.9,\\,\\text{B})$.\n\nFor each test case, report a list containing two values: the number of iterations $k$ at termination (an integer), and the final relative residual $\\rho^{(k)}$ (a floating-point number). Your program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets, with each individual test case result shown as a two-element list in the same order as above. For example, the required overall output format is\n$$\n[\\,[k_1,\\rho_1],[k_2,\\rho_2],[k_3,\\rho_3],[k_4,\\rho_4],[k_5,\\rho_5]\\,].\n$$\nNo physical units are involved in this problem, and any angles that appear implicitly in trigonometric functions must be interpreted in radians.", "solution": "The problem is subjected to a rigorous validation process before a solution is attempted.\n\n**Step 1: Extract Givens**\nThe givens extracted verbatim from the problem statement are as follows:\n- Grid parameter: $m \\in \\mathbb{N}$\n- Grid spacing: $h = \\frac{1}{m+1}$\n- One-dimensional matrix: $T = \\frac{1}{h^2}\\operatorname{tridiag}(-1,\\,2,\\,-1) \\in \\mathbb{R}^{m \\times m}$\n- Two-dimensional matrix: $L = I_m \\otimes T \\;+\\; T \\otimes I_m \\in \\mathbb{R}^{m^2 \\times m^2}$\n- Shift parameter: $\\sigma > 0$\n- Shifted matrix: $K = L + \\sigma I_{m^2}$\n- Coupling parameter: $\\alpha \\in \\mathbb{R}$\n- Block matrix: $A \\;=\\; \\begin{bmatrix} K & \\alpha I_{m^2}\\\\ \\alpha I_{m^2} & K \\end{bmatrix} \\in \\mathbb{R}^{(2m^2)\\times(2m^2)}$\n- Exact discrete fields: $u^{\\star}(x_i,y_j) = \\sin(\\pi x_i)\\sin(\\pi y_j)$ and $v^{\\star}(x_i,y_j) = \\sin(2\\pi x_i)\\sin(\\pi y_j)$ for $x_i = i h$, $y_j = j h$, with $i,j \\in \\{1,2,\\dots,m\\}$. These fields are stacked into vectors $u^{\\star}, v^{\\star} \\in \\mathbb{R}^{m^2}$.\n- Right-hand side vector: $b = A\\begin{bmatrix}u^{\\star} \\\\ v^{\\star}\\end{bmatrix}$, with components $b_1 = Ku^{\\star} + \\alpha v^{\\star}$ and $b_2 = \\alpha u^{\\star} + K v^{\\star}$.\n- Initial condition: $x^{(0)} = 0 \\in \\mathbb{R}^{2m^2}$\n- Termination tolerance: $\\tau = 10^{-8}$\n- Maximum iterations: $k_{\\max} = 10000$\n- Relative residual formula: $\\rho^{(k)} = \\frac{\\|b - A x^{(k)}\\|_2}{\\|b\\|_2}$\n- Iteration Option A (Block-Jacobi): $u^{(k+1)} = K^{-1}(b_1 - \\alpha v^{(k)})$, $v^{(k+1)} = K^{-1}(b_2 - \\alpha u^{(k)})$\n- Iteration Option B (Block-Gauss-Seidel): $u^{(k+1)} = K^{-1}(b_1 - \\alpha v^{(k)})$, $v^{(k+1)} = K^{-1}(b_2 - \\alpha u^{(k+1)})$\n- Constants for all test cases: $\\sigma = 1$.\n- Test Suite:\n  1. $(m,\\alpha,\\text{Option}) = (8,\\,0.5,\\,\\text{A})$\n  2. $(m,\\alpha,\\text{Option}) = (8,\\,0.5,\\,\\text{B})$\n  3. $(m,\\alpha,\\text{Option}) = (1,\\,0.0,\\,\\text{A})$\n  4. $(m,\\alpha,\\text{Option}) = (12,\\,0.9,\\,\\text{A})$\n  5. $(m,\\alpha,\\text{Option}) = (12,\\,0.9,\\,\\text{B})$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding (Critical):** The problem is fundamentally sound. It describes the numerical solution of a system of coupled elliptic partial differential equations discretized via the finite difference method on a uniform grid. The matrices $T$ and $L$ are the standard discrete representations of the one-dimensional and two-dimensional negative Laplacian operators, respectively. The block matrix $A$ is characteristic of coupled-field problems. The iterative solution strategies, Option A and Option B, are immediately identifiable as the block-Jacobi and block-Gauss-Seidel methods, respectively. These are canonical subjects within numerical analysis and computational engineering.\n- **Well-Posedness:** The matrix $T$ is a symmetric Toeplitz matrix known to be positive definite. The Kronecker sum $L = I_m \\otimes T + T \\otimes I_m$ is therefore also symmetric and positive definite. With $\\sigma = 1 > 0$, the matrix $K = L + \\sigma I_{m^2}$ is symmetric and strictly positive definite, guaranteeing its invertibility. The linear solves involving $K$ are thus well-posed. The full matrix $A$ is symmetric. Its eigenvalues are the eigenvalues of $K \\pm \\alpha I_{m^2}$. For all specified test cases, the parameters $(m, \\alpha)$ ensure that $|\\alpha|$ is less than the minimum eigenvalue of $K$, which implies that $A$ is also symmetric and positive definite. Consequently, the linear system $Ax=b$ has a unique solution.\n- **Objectivity (Critical):** The problem is defined with mathematical precision and rigor, free of subjective, ambiguous, or non-scientific language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically grounded, well-posed, and objective. A complete solution will be constructed.\n\n**Methodology and Solution Derivation**\nThe task is to solve the linear system $Ax=b$ for $x=\\begin{bmatrix} u \\\\ v \\end{bmatrix}$ using two different block-iterative methods. The solution process is implemented by first setting up the discrete system and then executing the specified iterative algorithm until a termination criterion is met.\n\n1.  **System Construction:**\n    For a given grid parameter $m$, the mesh size $h = 1/(m+1)$ is calculated. The size of the sub-problems is $N = m^2$.\n    The $m \\times m$ matrix $T$ is constructed.\n    The $N \\times N$ discrete Laplacian $L$ is formed using the Kronecker product rule $L=I_m \\otimes T + T \\otimes I_m$.\n    The shifted matrix $K = L + \\sigma I_N$ is assembled with $\\sigma=1$. Since $K$ is symmetric positive definite (SPD), its Cholesky factorization $K=R^T R$ (where $R$ is upper triangular) is computed once for efficiency. The operation $K^{-1}y$ is then performed by solving $R^Tz=y$ (forward substitution) and $Rx=z$ (backward substitution).\n    The full $2N \\times 2N$ system matrix $A$ is constructed.\n\n2.  **Right-Hand Side (RHS) Formulation:**\n    This is a manufactured solution problem. The exact solution $x^{\\star} = \\begin{bmatrix}u^{\\star} \\\\ v^{\\star}\\end{bmatrix}$ is known.\n    A grid of points $(x_i, y_j)=(ih, jh)$ for $i,j \\in \\{1,\\dots,m\\}$ is generated.\n    The exact solution fields $u^{\\star}$ and $v^{\\star}$ are evaluated on this grid. These $m \\times m$ arrays of values are flattened into $N \\times 1$ vectors using lexicographical ordering.\n    The RHS vector $b$ is computed as $b = A x^{\\star}$. This ensures that $x^{\\star}$ is the true solution to the discrete system $Ax=b$. The norm $\\|b\\|_2$ is pre-calculated for use in the relative residual computation.\n\n3.  **Iterative Solution:**\n    The iteration starts with the zero vector $x^{(0)} = 0$. The process for computing iterate $x^{(k)}$ from $x^{(k-1)}$ for $k \\ge 1$ depends on the chosen option. Let $x^{(k)} = \\begin{bmatrix} u^{(k)} \\\\ v^{(k)} \\end{bmatrix}$.\n\n    - **Option A (Block-Jacobi):** The updates for $u^{(k)}$ and $v^{(k)}$ depend only on the values from the previous iteration, $x^{(k-1)}$.\n    $$\n    \\begin{aligned}\n    u^{(k)} = K^{-1}\\left(b_1 - \\alpha v^{(k-1)}\\right) \\\\\n    v^{(k)} = K^{-1}\\left(b_2 - \\alpha u^{(k-1)}\\right)\n    \\end{aligned}\n    $$\n    This corresponds to the splitting $A=M-N$ with $M_{\\text{J}} = \\operatorname{diag}(K,K)$.\n\n    - **Option B (Block-Gauss-Seidel):** The update for $v^{(k)}$ uses the most recently computed value, $u^{(k)}$.\n    $$\n    \\begin{aligned}\n    u^{(k)} = K^{-1}\\left(b_1 - \\alpha v^{(k-1)}\\right) \\\\\n    v^{(k)} = K^{-1}\\left(b_2 - \\alpha u^{(k)}\\right)\n    \\end{aligned}\n    $$\n    This corresponds to the splitting $A=M-N$ with $M_{\\text{GS}}$ being the block lower-triangular part of $A$.\n\n4.  **Termination Criteria:**\n    For each iteration $k=1, 2, \\dots, k_{\\max}$, the new iterate $x^{(k)}$ is computed. The relative residual $\\rho^{(k)} = \\|b - A x^{(k)}\\|_2 / \\|b\\|_2$ is evaluated. If $\\rho^{(k)} \\le \\tau = 10^{-8}$, the iteration terminates and the pair $[k, \\rho^{(k)}]$ is reported. If the loop completes up to $k=k_{\\max}$ without meeting the tolerance, the final values $[k_{\\max}, \\rho^{(k_{\\max})}]$ are reported.\n\nThe implementation will follow these steps precisely for each test case specified in the problem.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        (8, 0.5, 'A'),\n        (8, 0.5, 'B'),\n        (1, 0.0, 'A'),\n        (12, 0.9, 'A'),\n        (12, 0.9, 'B'),\n    ]\n\n    sigma = 1.0\n    tau = 1e-8\n    k_max = 10000\n\n    results = []\n    for m, alpha, option in test_cases:\n        result = run_iteration(m, alpha, option, sigma, tau, k_max)\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # str() on a list uses single quotes, which is fine, but we will be robust.\n    formatted_results = [f\"[{k},{res}]\" for k, res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_iteration(m, alpha, option, sigma, tau, k_max):\n    \"\"\"\n    Performs the iterative solution for a single test case.\n    \n    Args:\n        m (int): Number of interior grid points per direction.\n        alpha (float): Coupling parameter.\n        option (str): 'A' for block-Jacobi, 'B' for block-Gauss-Seidel.\n        sigma (float): Shift parameter for matrix K.\n        tau (float): Relative residual tolerance.\n        k_max (int): Maximum number of iterations.\n        \n    Returns:\n        list: A list containing [number_of_iterations, final_relative_residual].\n    \"\"\"\n    m_sq = m * m\n    h = 1.0 / (m + 1.0)\n    \n    # 1. System Construction\n    \n    # 1D second-difference matrix T\n    diag_T = np.full(m, 2.0)\n    offdiag_T = np.full(m - 1, -1.0)\n    T = (np.diag(diag_T) + np.diag(offdiag_T, k=1) + np.diag(offdiag_T, k=-1)) / h**2\n    \n    # 2D discrete Laplacian L\n    I_m = np.identity(m)\n    L = np.kron(I_m, T) + np.kron(T, I_m)\n    \n    # Shifted matrix K\n    I_msq = np.identity(m_sq)\n    K = L + sigma * I_msq\n    \n    # Full system matrix A\n    A = np.block([\n        [K, alpha * I_msq],\n        [alpha * I_msq, K]\n    ])\n    \n    # Pre-compute Cholesky factorization of K for efficient solves\n    cho_K = cho_factor(K)\n\n    # 2. Right-Hand Side (RHS) Formulation\n    \n    # Grid points\n    i = np.arange(1, m + 1)\n    grid_pts = i * h\n    x_grid, y_grid = np.meshgrid(grid_pts, grid_pts)\n\n    # Exact solution fields u_star, v_star\n    u_star_grid = np.sin(np.pi * x_grid) * np.sin(np.pi * y_grid)\n    v_star_grid = np.sin(2 * np.pi * x_grid) * np.sin(np.pi * y_grid)\n    \n    # Flatten to vectors in lexicographic order\n    u_star_vec = u_star_grid.flatten()\n    v_star_vec = v_star_grid.flatten()\n    \n    # Full exact solution vector and RHS\n    x_star = np.concatenate([u_star_vec, v_star_vec])\n    b = A @ x_star\n    b1 = b[:m_sq]\n    b2 = b[m_sq:]\n    b_norm = np.linalg.norm(b, 2)\n    \n    if b_norm == 0: # Avoid division by zero\n        return [0, 0.0]\n\n    # 3. Iterative Solution\n    \n    x_k = np.zeros(2 * m_sq) # Initial guess x^(0)\n    \n    # Check initial residual (for k=0)\n    res_vec = b - A @ x_k\n    rho = np.linalg.norm(res_vec, 2) / b_norm\n    if rho = tau:\n        return [0, rho]\n        \n    for k in range(1, k_max + 1):\n        u_prev = x_k[:m_sq]\n        v_prev = x_k[m_sq:]\n        \n        # Compute x^(k)\n        if option == 'A': # Block-Jacobi\n            u_next = cho_solve(cho_K, b1 - alpha * v_prev)\n            v_next = cho_solve(cho_K, b2 - alpha * u_prev)\n        elif option == 'B': # Block-Gauss-Seidel\n            u_next = cho_solve(cho_K, b1 - alpha * v_prev)\n            v_next = cho_solve(cho_K, b2 - alpha * u_next)\n        else: # Should not happen\n            raise ValueError(\"Invalid option specified.\")\n            \n        x_k = np.concatenate([u_next, v_next])\n        \n        # Check termination criteria\n        res_vec = b - A @ x_k\n        rho = np.linalg.norm(res_vec, 2) / b_norm\n        \n        if rho = tau:\n            return [k, rho]\n            \n    # If loop finishes, return result at k_max\n    return [k_max, rho]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2406140"}]}