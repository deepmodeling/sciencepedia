{"hands_on_practices": [{"introduction": "In computational science, we often translate the abstract operation of differentiation into the concrete language of linear algebra. This practice guides you through the process of constructing a \"differentiation matrix\" for the second-order central difference scheme [@problem_id:2391158]. By representing the derivative as a matrix, we can solve complex differential equations by transforming them into systems of linear algebraic equations, a task at which computers excel.", "problem": "A one-dimensional, uniform computational grid on the interval $[0,L]$ consists of $N+1$ nodes located at $x_i = i\\,h$ for $i \\in \\{0,1,\\dots,N\\}$, where $h = L/N$. Let $f:[0,L]\\to\\mathbb{R}$ be sufficiently smooth, with Dirichlet boundary conditions $f(0)=0$ and $f(L)=0$. Denote the nodal values by $f_i = f(x_i)$, so $f_0=f_N=0$. Consider approximating the first derivative $f'(x)$ at the interior nodes $x_i$ for $i \\in \\{1,2,\\dots,N-1\\}$ by a linear combination of nearby nodal values that is second-order accurate in $h$.\n\nUsing only Taylor series expansions of $f(x)$ about an interior node and the definition of the first derivative, derive the second-order accurate, centered finite difference approximation for $f'(x_i)$ and assemble the corresponding linear mapping $D \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ that sends the interior value vector $\\mathbf{f} = [\\,f_1,\\;f_2,\\;\\dots,\\;f_{N-1}\\,]^{\\top}$ to the vector of derivative approximations $\\mathbf{g} = [\\,g_1,\\;g_2,\\;\\dots,\\;g_{N-1}\\,]^{\\top}$, where $g_i \\approx f'(x_i)$. Carefully account for the Dirichlet boundary values $f_0=f_N=0$ when constructing the first and last rows.\n\nFinally, for the specific case $L=1$ and $N=5$, provide the explicit matrix $D$. Your final answer must be a single analytical expression. No rounding is required, and no units are involved.", "solution": "The problem as stated is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It is a standard exercise in numerical analysis. We shall proceed with the derivation.\n\nThe objective is to find a second-order accurate finite difference approximation for the first derivative, $f'(x_i)$, at an interior node $x_i$ of a uniform grid. We are given a sufficiently smooth function $f(x)$ and utilize Taylor series expansions around the point $x_i$. The grid points are defined as $x_j = j\\,h$, where $h = L/N$. The nodal values are denoted $f_j = f(x_j)$.\n\nLet us consider the Taylor series expansions for $f(x)$ at the neighboring points $x_{i+1} = x_i + h$ and $x_{i-1} = x_i - h$:\n$$f(x_{i+1}) = f(x_i) + h f'(x_i) + \\frac{h^2}{2!} f''(x_i) + \\frac{h^3}{3!} f'''(x_i) + \\mathcal{O}(h^4)$$\n$$f(x_{i-1}) = f(x_i) - h f'(x_i) + \\frac{h^2}{2!} f''(x_i) - \\frac{h^3}{3!} f'''(x_i) + \\mathcal{O}(h^4)$$\nUsing the notation $f_j = f(x_j)$, these can be written as:\n$$f_{i+1} = f_i + h f'(x_i) + \\frac{h^2}{2} f''(x_i) + \\frac{h^3}{6} f'''(x_i) + \\dots$$\n$$f_{i-1} = f_i - h f'(x_i) + \\frac{h^2}{2} f''(x_i) - \\frac{h^3}{6} f'''(x_i) + \\dots$$\n\nTo isolate the first derivative term, $f'(x_i)$, we subtract the second expansion from the first:\n$$f_{i+1} - f_{i-1} = (f_i - f_i) + (h - (-h))f'(x_i) + \\left(\\frac{h^2}{2} - \\frac{h^2}{2}\\right)f''(x_i) + \\left(\\frac{h^3}{6} - \\left(-\\frac{h^3}{6}\\right)\\right)f'''(x_i) + \\dots$$\n$$f_{i+1} - f_{i-1} = 2h f'(x_i) + \\frac{h^3}{3} f'''(x_i) + \\mathcal{O}(h^5)$$\nSolving for $f'(x_i)$, we obtain:\n$$f'(x_i) = \\frac{f_{i+1} - f_{i-1}}{2h} - \\frac{h^2}{6} f'''(x_i) + \\mathcal{O}(h^4)$$\nThe finite difference approximation, which we denote as $g_i$, is the first term on the right-hand side:\n$$g_i = \\frac{f_{i+1} - f_{i-1}}{2h}$$\nThis is the centered finite difference formula for the first derivative. The leading term of the truncation error, $T_i = f'(x_i) - g_i$, is $-\\frac{h^2}{6} f'''(x_i)$. Since the error is proportional to $h^2$, the approximation is second-order accurate, as required.\n\nNext, we construct the differentiation matrix $D \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ that maps the vector of interior nodal values $\\mathbf{f} = [f_1, f_2, \\dots, f_{N-1}]^{\\top}$ to the vector of derivative approximations $\\mathbf{g} = [g_1, g_2, \\dots, g_{N-1}]^{\\top}$, such that $\\mathbf{g} = D\\mathbf{f}$. The components of $\\mathbf{g}$ are given by $g_i = \\sum_{j=1}^{N-1} D_{ij}f_j$.\n\nFor a generic interior node $i \\in \\{2, 3, \\dots, N-2\\}$, the formula $g_i = \\frac{1}{2h}(-f_{i-1} + f_{i+1})$ involves only interior nodal values. The corresponding row $i$ of the matrix $D$ will have non-zero elements only at columns $j=i-1$ and $j=i+1$:\n$$D_{i,i-1} = -\\frac{1}{2h}, \\quad D_{i,i} = 0, \\quad D_{i,i+1} = \\frac{1}{2h}$$\n\nWe must carefully handle the first and last rows of the matrix, which correspond to $i=1$ and $i=N-1$, incorporating the Dirichlet boundary conditions $f_0=0$ and $f_N=0$.\n\nFor the first row ($i=1$):\n$$g_1 = \\frac{f_{1+1} - f_{1-1}}{2h} = \\frac{f_2 - f_0}{2h}$$\nGiven $f_0=0$, this simplifies to:\n$$g_1 = \\frac{f_2}{2h} = (0)f_1 + \\left(\\frac{1}{2h}\\right)f_2 + (0)f_3 + \\dots$$\nThus, the first row of $D$ is $[0, \\frac{1}{2h}, 0, \\dots, 0]$. Specifically, $D_{1,1}=0$ and $D_{1,2}=\\frac{1}{2h}$.\n\nFor the last row ($i=N-1$):\n$$g_{N-1} = \\frac{f_{(N-1)+1} - f_{(N-1)-1}}{2h} = \\frac{f_N - f_{N-2}}{2h}$$\nGiven $f_N=0$, this simplifies to:\n$$g_{N-1} = \\frac{-f_{N-2}}{2h} = \\dots + (0)f_{N-3} + \\left(-\\frac{1}{2h}\\right)f_{N-2} + (0)f_{N-1}$$\nThus, the last row of $D$ is $[0, \\dots, 0, -\\frac{1}{2h}, 0]$. Specifically, $D_{N-1,N-2}=-\\frac{1}{2h}$ and $D_{N-1,N-1}=0$.\n\nThe general structure of the $(N-1) \\times (N-1)$ matrix $D$ is:\n$$\nD = \\frac{1}{2h}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 & 0 & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & 0 \\\\\n0 & 0 & 0 & \\cdots & -1 & 0 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & -1 & 0\n\\end{pmatrix}\n$$\n\nFinally, we are asked to provide the explicit matrix for the specific case $L=1$ and $N=5$.\nThe step size is $h = L/N = 1/5$.\nThe pre-factor is $\\frac{1}{2h} = \\frac{1}{2(1/5)} = \\frac{5}{2}$.\nThe number of interior nodes is $N-1 = 4$, so the matrix $D$ will be of size $4 \\times 4$.\nThe interior vector is $\\mathbf{f} = [f_1, f_2, f_3, f_4]^{\\top}$.\nUsing the structure derived above for $N-1=4$:\n$$\nD = \\frac{5}{2}\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1 \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n$$\nThis is the required differentiation matrix.", "answer": "$$\n\\boxed{\n\\frac{5}{2}\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1 \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n}\n$$", "id": "2391158"}, {"introduction": "The accuracy of our finite difference formulas relies on the elegant predictability of smooth functions, as captured by Taylor series. But what happens when a function is not smooth, such as one with a sharp corner or \"cusp\"? This exercise [@problem_id:2391178] explores this exact scenario, revealing how and why standard formulas can produce misleading results and emphasizing the importance of understanding the theoretical assumptions behind our numerical methods.", "problem": "You are asked to analyze finite difference approximations for the first derivative of the function $f(x) = \\lvert x - c \\rvert$ at and near the non-differentiable point $x = c$. Work from first principles: use the definition of the derivative and the notion of one-sided limits, and recall that a finite difference approximation is obtained by replacing the limit by a small, positive step size $h$ and using nearby sample points. Do not assume any specific canned finite difference formula; instead, justify the structure of the forward, backward, and central approximations directly from the derivative definition and symmetry arguments.\n\nYour tasks are:\n\n1. Theoretical analysis from first principles:\n   - Using the definition of the derivative $f'(x) = \\lim_{h \\to 0} \\dfrac{f(x+h) - f(x)}{h}$ when the limit exists, derive the standard first-order forward and first-order backward approximations for $f'(x)$ by considering small positive $h$ and one-sided sampling. Separately, justify the central approximation by considering symmetric sampling $x \\pm h$ and the idea of cancellation of odd error terms when $f$ is smooth at the point of interest.\n   - Apply these ideas to $f(x) = \\lvert x - c \\rvert$ at $x = c$. Determine, as functions of $h$, what the forward, backward, and central approximations evaluate to when $h > 0$. Based on this, explain the sense in which the central approximation “fails” at the cusp (that is, it produces a seemingly stable value that is not a one-sided derivative and does not reflect the non-differentiability).\n   - Analyze the behavior away from the cusp. For $x \\neq c$, determine conditions on $h$ under which the sampling points used by the approximation remain on the same side of $c$ (so that $f$ coincides locally with a linear function), and deduce the consequence for the numerical derivative produced by each approximation. Also analyze what happens when $h$ is so large that the sampling stencil crosses the cusp ($x-h < c < x+h$), and explain the impact on the central and backward approximations.\n\n2. Computational study:\n   - Use the fixed parameter $c = 0.3$.\n   - Define the function $f(x) = \\lvert x - c \\rvert$ and implement the following three approximations from your derivations: the first-order forward approximation, the first-order backward approximation, and the symmetric central approximation, each with a positive step size $h$.\n   - Use the following test suite, designed to probe distinct behaviors.\n     - Cusp test at $x = c$: use a single very small step $h_{\\text{cusp}} = 10^{-12}$ to evaluate the three approximations at $x = c$.\n     - Smooth-region stability test: choose $x_{\\text{smooth}} = c + 0.4$ and the step set $\\{2^{-3}, 2^{-4}, 2^{-5}, 2^{-6}\\}$. For each step in this set, evaluate the central and forward approximations at $x_{\\text{smooth}}$, and measure the maximum absolute deviation from the correct one-sided derivative there.\n     - Crossing-cusp test: choose $x_{\\text{cross}} = c + 0.01$ with $h_{\\text{cross}} = 0.02$. Evaluate the central and forward approximations at $x_{\\text{cross}}$ to illustrate how the central approximation degrades when its stencil crosses the cusp, while the forward approximation with a forward stencil may remain on one side.\n   - Your program must aggregate the following eight results, in this exact order, as a single line of output:\n     $[$\n     central approximation at $x=c$ with $h = 10^{-12}$,\n     forward approximation at $x=c$ with $h = 10^{-12}$,\n     backward approximation at $x=c$ with $h = 10^{-12}$,\n     absolute difference between the forward and backward approximations at $x=c$ with $h = 10^{-12}$,\n     maximum absolute deviation of the central approximation from the correct derivative at $x_{\\text{smooth}}$ over $h \\in \\{2^{-3}, 2^{-4}, 2^{-5}, 2^{-6}\\}$,\n     maximum absolute deviation of the forward approximation from the correct derivative at $x_{\\text{smooth}}$ over $h \\in \\{2^{-3}, 2^{-4}, 2^{-5}, 2^{-6}\\}$,\n     central approximation at $x_{\\text{cross}}$ with $h = 0.02$,\n     forward approximation at $x_{\\text{cross}}$ with $h = 0.02$\n     $]$.\n   - The final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,\\dots,r_8]$. All outputs are unitless real numbers.\n\nScientific realism and self-consistency are required in your reasoning and implementation. The numerical values above are chosen so that double-precision arithmetic suffices and the behavior is qualitatively robust.", "solution": "The problem presented is a valid exercise in computational mathematics, requiring both theoretical analysis from first principles and a numerical implementation. It is scientifically grounded, well-posed, and objective. We shall proceed with a rigorous solution.\n\nThe analysis begins with the definition of the derivative of a function $f(x)$ at a point $x$:\n$$ f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x+\\Delta x) - f(x)}{\\Delta x} $$\nThis limit must exist for the derivative to be defined. Finite difference approximations are derived by replacing this limit process with a small, finite step size, denoted here by $h > 0$.\n\nFirst, we derive the standard finite difference formulas.\nThe first-order forward difference approximation is obtained by setting $\\Delta x = h$, where $h$ is a small positive number:\n$$ D_{+h}f(x) = \\frac{f(x+h) - f(x)}{h} $$\nThis formula samples the function at $x$ and $x+h$. For a sufficiently smooth function, a Taylor series expansion of $f(x+h)$ around $x$ gives $f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\mathcal{O}(h^3)$. Substituting this into the formula yields:\n$$ D_{+h}f(x) = \\frac{(f(x) + hf'(x) + \\mathcal{O}(h^2)) - f(x)}{h} = f'(x) + \\mathcal{O}(h) $$\nThe approximation error is of order $h$, making it a first-order method.\n\nThe first-order backward difference approximation is obtained by setting $\\Delta x = -h$:\n$$ D_{-h}f(x) = \\frac{f(x-h) - f(x)}{-h} = \\frac{f(x) - f(x-h)}{h} $$\nThis formula samples the function at $x-h$ and $x$. The Taylor expansion $f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) + \\mathcal{O}(h^3)$ gives:\n$$ D_{-h}f(x) = \\frac{f(x) - (f(x) - hf'(x) + \\mathcal{O}(h^2))}{h} = f'(x) + \\mathcal{O}(h) $$\nThis is also a first-order method.\n\nThe central difference approximation is constructed using a symmetric stencil around $x$, sampling at $x-h$ and $x+h$. The effective step size is $2h$:\n$$ D_{0h}f(x) = \\frac{f(x+h) - f(x-h)}{2h} $$\nIts higher accuracy for smooth functions is revealed by subtracting the Taylor series for $f(x-h)$ from that of $f(x+h)$:\n$$ f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + \\mathcal{O}(h^4) $$\n$$ f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + \\mathcal{O}(h^4) $$\n$$ f(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + \\mathcal{O}(h^5) $$\nDividing by $2h$ gives:\n$$ D_{0h}f(x) = f'(x) + \\frac{h^2}{6}f'''(x) + \\mathcal{O}(h^4) = f'(x) + \\mathcal{O}(h^2) $$\nThe leading error term is of order $h^2$, making this a second-order method. This higher accuracy relies on the cancellation of even-powered terms in the Taylor series, which is predicated on the function being sufficiently smooth (at least $C^3$) at $x$.\n\nNow, we apply these approximations to the function $f(x) = \\lvert x-c \\rvert$. This function is not smooth at $x=c$; its derivative is discontinuous there. The Taylor series expansions used above are not valid at this point.\n\nBehavior at the cusp, $x=c$:\nThe function value at the cusp is $f(c) = \\lvert c-c \\rvert = 0$. For any $h > 0$:\nThe forward approximation is:\n$$ D_{+h}f(c) = \\frac{f(c+h) - f(c)}{h} = \\frac{\\lvert(c+h)-c\\rvert - 0}{h} = \\frac{\\lvert h \\rvert}{h} = \\frac{h}{h} = 1 $$\nThis value corresponds to the right-hand derivative, $f'_+(c) = 1$.\nThe backward approximation is:\n$$ D_{-h}f(c) = \\frac{f(c) - f(c-h)}{h} = \\frac{0 - \\lvert(c-h)-c\\rvert}{h} = \\frac{-\\lvert -h \\rvert}{h} = \\frac{-h}{h} = -1 $$\nThis value corresponds to the left-hand derivative, $f'_-(c) = -1$.\nThe central approximation is:\n$$ D_{0h}f(c) = \\frac{f(c+h) - f(c-h)}{2h} = \\frac{\\lvert h \\rvert - \\lvert -h \\rvert}{2h} = \\frac{h - h}{2h} = 0 $$\nThe central difference \"fails\" in the sense that it yields a stable value of $0$, which is not a derivative of the function at that point (as the derivative does not exist). This result arises from the perfect symmetry of the stencil around the perfectly symmetric cusp, causing the function values $f(c+h)$ and $f(c-h)$ to be identical. The result $0$ hides the non-differentiability and gives a misleading impression of a zero slope. This value happens to be the average of the one-sided derivatives.\n\nBehavior away from the cusp, $x \\neq c$:\nIf the finite difference stencil does not cross the point $x=c$, the function $f(x)$ behaves locally like a linear function.\nSuppose $x > c$. The derivative is $f'(x) = 1$. The stencil avoids the cusp if all its points are greater than $c$. For the central and backward stencils, this requires $x-h > c$, or $h < x-c$. For the forward stencil, $[x, x+h]$, this is always true for $h > 0$. If $h < x-c$, then for any point $u$ in the stencil, $u-c > 0$, so $f(u) = u-c$.\nThe approximations become:\n$$ D_{+h}f(x) = \\frac{(x+h-c) - (x-c)}{h} = \\frac{h}{h} = 1 $$\n$$ D_{-h}f(x) = \\frac{(x-c) - (x-h-c)}{h} = \\frac{h}{h} = 1 $$\n$$ D_{0h}f(x) = \\frac{(x+h-c) - (x-h-c)}{2h} = \\frac{2h}{2h} = 1 $$\nAll three methods give the exact derivative, $1$.\nSuppose $x < c$. The derivative is $f'(x) = -1$. The stencil avoids the cusp if $x+h < c$, or $h < c-x$. If this holds, $f(u) = c-u$ for all points $u$ in the stencil. The approximations become:\n$$ D_{+h}f(x) = \\frac{(c-(x+h)) - (c-x)}{h} = \\frac{-h}{h} = -1 $$\n$$ D_{-h}f(x) = \\frac{(c-x) - (c-(x-h))}{h} = \\frac{-h}{h} = -1 $$\n$$ D_{0h}f(x) = \\frac{(c-(x+h)) - (c-(x-h))}{2h} = \\frac{-2h}{2h} = -1 $$\nAll three methods again give the exact derivative, $-1$.\nThus, as long as the stencil does not span the singularity, the finite difference methods are exact for this piecewise linear function.\n\nBehavior when the stencil crosses the cusp:\nConsider the case where $x > c$ but the step size $h$ is large enough that $x-h < c < x+h$. This violates the condition $h < x-c$. The stencils for the backward and central approximations now sample points from both sides of the cusp.\nThe forward stencil $[x, x+h]$ remains entirely in the region $u>c$, so the forward approximation remains exact: $D_{+h}f(x) = 1$.\nThe backward stencil $[x-h, x]$ crosses the cusp. We have $f(x) = x-c$ but $f(x-h) = c - (x-h) = c-x+h$. The approximation is:\n$$ D_{-h}f(x) = \\frac{(x-c) - (c-x+h)}{h} = \\frac{2x - 2c - h}{h} $$\nThe central stencil $[x-h, x+h]$ also crosses the cusp. We have $f(x+h) = x+h-c$ and $f(x-h) = c-x+h$. The approximation is:\n$$ D_{0h}f(x) = \\frac{(x+h-c) - (c-x+h)}{2h} = \\frac{2x - 2c}{2h} = \\frac{x-c}{h} $$\nIn both cases, the result is not the correct derivative $f'(x)=1$. The numerical methods are polluted by sampling across the singularity, breaking the assumptions of local smoothness upon which their derivations are based.\n\nThis completes the required theoretical analysis. The computational study will now implement these findings.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs theoretical and computational analysis of finite difference\n    approximations for the derivative of f(x) = |x - c|.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    # Global constant c\n    c = 0.3\n\n    # Test Case 1: Cusp test\n    x_cusp = c\n    h_cusp = 1e-12\n\n    # Test Case 2: Smooth-region stability test\n    x_smooth = c + 0.4\n    h_smooth_set = np.array([2**-3, 2**-4, 2**-5, 2**-6])\n\n    # Test Case 3: Crossing-cusp test\n    x_cross = c + 0.01\n    h_cross = 0.02\n\n    # --- Function and Derivative Approximations ---\n    def f(x_val):\n        \"\"\"The function f(x) = |x - c|.\"\"\"\n        return np.abs(x_val - c)\n\n    def forward_diff(func, x, h):\n        \"\"\"First-order forward difference approximation.\"\"\"\n        return (func(x + h) - func(x)) / h\n\n    def backward_diff(func, x, h):\n        \"\"\"First-order backward difference approximation.\"\"\"\n        return (func(x) - func(x - h)) / h\n\n    def central_diff(func, x, h):\n        \"\"\"Second-order central difference approximation.\"\"\"\n        return (func(x + h) - func(x - h)) / (2 * h)\n\n    # --- Calculations ---\n    results = []\n\n    # 1. Central approximation at x=c\n    r1 = central_diff(f, x_cusp, h_cusp)\n    results.append(r1)\n\n    # 2. Forward approximation at x=c\n    r2 = forward_diff(f, x_cusp, h_cusp)\n    results.append(r2)\n\n    # 3. Backward approximation at x=c\n    r3 = backward_diff(f, x_cusp, h_cusp)\n    results.append(r3)\n\n    # 4. Absolute difference between forward and backward at x=c\n    r4 = np.abs(r2 - r3)\n    results.append(r4)\n\n    # 5. Maximum absolute deviation of central approximation at x_smooth\n    # The correct derivative at x_smooth = 0.7 is 1.0\n    correct_deriv_smooth = 1.0\n    central_devs = []\n    for h_val in h_smooth_set:\n        # Stencil is [x-h, x+h].\n        # x_smooth - c = 0.4. Largest h is 0.125.\n        # h < x_smooth - c, so stencil does not cross cusp. Approximation should be exact.\n        approx = central_diff(f, x_smooth, h_val)\n        central_devs.append(np.abs(approx - correct_deriv_smooth))\n    r5 = np.max(central_devs)\n    results.append(r5)\n\n    # 6. Maximum absolute deviation of forward approximation at x_smooth\n    forward_devs = []\n    for h_val in h_smooth_set:\n        # Stencil is [x, x+h]. Both points are to the right of c.\n        # Approximation should be exact.\n        approx = forward_diff(f, x_smooth, h_val)\n        forward_devs.append(np.abs(approx - correct_deriv_smooth))\n    r6 = np.max(forward_devs)\n    results.append(r6)\n\n    # 7. Central approximation at x_cross\n    # Here, x_cross - c = 0.01 and h_cross = 0.02.\n    # Since h_cross > x_cross - c, the stencil [x-h, x+h] crosses the cusp.\n    r7 = central_diff(f, x_cross, h_cross)\n    results.append(r7)\n\n    # 8. Forward approximation at x_cross\n    # The forward stencil [x, x+h] does not cross the cusp as x > c and h > 0.\n    r8 = forward_diff(f, x_cross, h_cross)\n    results.append(r8)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2391178"}, {"introduction": "While theoretical problems often deal with perfect, smooth functions, real-world data from experiments and sensors is invariably contaminated with noise. This practice [@problem_id:2391185] delves into the critical challenge of differentiating noisy signals. You will empirically discover the fundamental trade-off between truncation error, which shrinks with step size $h$, and noise amplification, which grows, revealing that a smaller step size is not always better.", "problem": "Consider the scalar function of time $y(t) = \\cos(t)$ with time measured in radians. A sensor reports noisy samples $y(t_i) = \\cos(t_i) + \\epsilon_i$ at uniformly spaced times $t_i = i h$ for $i = 0, 1, \\dots, M$, where $h = \\dfrac{2\\pi}{M}$ and $\\epsilon_i$ are independent and identically distributed (i.i.d.) zero-mean Gaussian random variables with standard deviation $\\sigma$. The exact velocity is $y'(t) = -\\sin(t)$. Define the first-order forward difference approximation to the derivative at an index $i$ by $D_f y_i = \\dfrac{y_{i+1} - y_i}{h}$, valid for $i = 0, 1, \\dots, M-1$, and the second-order centered difference approximation by $D_c y_i = \\dfrac{y_{i+1} - y_{i-1}}{2h}$, valid for $i = 1, 2, \\dots, M-1$. For any set of valid indices $\\mathcal{I}$, define the root-mean-square (RMS) error by\n$$\nE = \\sqrt{\\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\left(D y_i - y'(t_i)\\right)^2 }.\n$$\nYou must quantify how the RMS error scales with respect to the grid spacing $h$ and the noise magnitude $\\sigma$.\n\nUse the following test suite. For all items involving random noise, use a fixed pseudorandom number generator seed to ensure reproducibility as specified.\n\n1. Noiseless centered-difference order test (expected scaling in $h$): Use $\\sigma = 0$ and $M \\in \\{40, 80, 160, 320, 640\\}$. For each $M$, compute $E$ for the centered difference using valid indices $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$. Let $p_{\\mathrm{cd},0}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} h$ over these five points.\n\n2. Noiseless forward-difference order test (expected scaling in $h$): Use $\\sigma = 0$ and $M \\in \\{40, 80, 160, 320, 640\\}$. For each $M$, compute $E$ for the forward difference using valid indices $\\mathcal{I} = \\{0,1,\\dots,M-1\\}$. Let $p_{\\mathrm{fd},0}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} h$ over these five points.\n\n3. Noise scaling at fixed grid (expected linear scaling in $\\sigma$): Fix $M = 2000$ and let $\\sigma \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\\}$. Generate a single i.i.d. standard normal base sequence $z_i$ for $i = 0,1,\\dots,M$ using the seed $2025$, and for each $\\sigma$ set $\\epsilon_i = \\sigma z_i$. For each $\\sigma$, compute $E$ for the centered difference using valid indices $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$. Let $q_{\\sigma}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} \\sigma$ over these five points.\n\n4. Noise-dominated scaling in $h$ (expected inverse scaling in $h$): Fix $\\sigma = 10^{-3}$ and $M \\in \\{500, 800, 1200, 2000\\}$. For each $M$, generate an i.i.d. standard normal base sequence $z_i$ for $i = 0,1,\\dots,M$ using the seed $3000 + M$, set $\\epsilon_i = \\sigma z_i$, and compute $E$ for the centered difference using valid indices $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$. Let $r_{h,\\mathrm{noise}}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} h$ over these four points.\n\nAll trigonometric functions must use angles in radians. Your program must compute the four exponents $p_{\\mathrm{cd},0}$, $p_{\\mathrm{fd},0}$, $q_{\\sigma}$, and $r_{h,\\mathrm{noise}}$, in that order, and produce a single line of output containing these four values rounded to three decimal places as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like\n[1.234,2.345,3.456,4.567]\nand must contain no other text.", "solution": "The problem presented is a well-posed exercise in computational engineering, specifically in the error analysis of finite difference methods. It is scientifically sound, and all parameters and procedures are defined with sufficient rigor to permit a unique, verifiable solution. We shall therefore proceed with the analysis and solution.\n\nThe core of the problem lies in understanding the two primary sources of error when numerically differentiating sampled data: truncation error, which is intrinsic to the finite difference approximation, and noise error, which arises from imperfections in the data itself. The total error is a combination of these two components.\n\nLet the true, smooth function be $f(t) = \\cos(t)$, and the sampled data at times $t_i = ih$ be $y_i = f(t_i) + \\epsilon_i$, where $\\epsilon_i$ is a random noise term with mean zero and standard deviation $\\sigma$. The exact derivative is $f'(t_i)$. The error of a finite difference approximation $D y_i$ is $e_i = D y_i - f'(t_i)$.\n\nWe begin with the centered difference approximation, $D_c y_i$:\n$$\nD_c y_i = \\frac{y_{i+1} - y_{i-1}}{2h} = \\frac{(f(t_{i+1}) + \\epsilon_{i+1}) - (f(t_{i-1}) + \\epsilon_{i-1})}{2h}\n$$\n$$\nD_c y_i = \\left( \\frac{f(t_{i+1}) - f(t_{i-1})}{2h} \\right) + \\left( \\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h} \\right)\n$$\nThe first term can be analyzed using Taylor series expansion of $f(t)$ around $t_i$:\n$$\nf(t_i \\pm h) = f(t_i) \\pm h f'(t_i) + \\frac{h^2}{2} f''(t_i) \\pm \\frac{h^3}{6} f'''(t_i) + \\mathcal{O}(h^4)\n$$\nSubtracting these expansions gives:\n$$\nf(t_{i+1}) - f(t_{i-1}) = f(t_i+h) - f(t_i-h) = 2h f'(t_i) + \\frac{h^3}{3} f'''(t_i) + \\mathcal{O}(h^5)\n$$\nSubstituting this back, we find the expression for the numerical derivative:\n$$\nD_c y_i = \\left( f'(t_i) + \\frac{h^2}{6} f'''(t_i) + \\mathcal{O}(h^4) \\right) + \\left( \\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h} \\right)\n$$\nThe error at point $i$ is therefore:\n$$\ne_i = D_c y_i - f'(t_i) = \\underbrace{\\frac{h^2}{6} f'''(t_i) + \\mathcal{O}(h^4)}_{\\text{Truncation Error } (E_T)} + \\underbrace{\\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h}}_{\\text{Noise Error } (E_N)}\n$$\nThe root-mean-square (RMS) error is $E = \\sqrt{\\mathbb{E}[e_i^2]}$, where $\\mathbb{E}[\\cdot]$ denotes averaging over the sample points and noise realizations. Assuming the truncation and noise errors are uncorrelated, the mean squared error is the sum of the mean squared errors of the components:\n$$\nE^2 \\approx \\mathbb{E}[E_T^2] + \\mathbb{E}[E_N^2]\n$$\nThe truncation error term scales as $E_T \\propto h^2$, so its contribution to the mean square error is $\\mathbb{E}[E_T^2] \\propto h^4$.\nFor the noise error, since the $\\epsilon_i$ are i.i.d. with variance $\\sigma^2$, the variance of the noise term is $\\text{Var}(\\epsilon_{i+1} - \\epsilon_{i-1}) = \\text{Var}(\\epsilon_{i+1}) + \\text{Var}(\\epsilon_{i-1}) = 2\\sigma^2$. The mean squared noise error is thus:\n$$\n\\mathbb{E}[E_N^2] = \\frac{\\mathbb{E}[(\\epsilon_{i+1} - \\epsilon_{i-1})^2]}{4h^2} = \\frac{\\text{Var}(\\epsilon_{i+1} - \\epsilon_{i-1})}{4h^2} = \\frac{2\\sigma^2}{4h^2} = \\frac{\\sigma^2}{2h^2}\n$$\nSo, the total RMS error for the centered difference scheme behaves as:\n$$\nE_{\\text{cd}} \\approx \\sqrt{A h^4 + B \\frac{\\sigma^2}{h^2}}\n$$\nwhere $A$ and $B$ are constants dependent on the function's derivatives and statistical properties.\n\nA similar analysis for the first-order forward difference, $D_f y_i = (y_{i+1} - y_i)/h$, yields a truncation error $E_T = \\frac{h}{2} f''(t_i) + \\mathcal{O}(h^2)$ and a noise error $E_N = (\\epsilon_{i+1} - \\epsilon_i)/h$. The RMS error behaves as:\n$$\nE_{\\text{fd}} \\approx \\sqrt{C h^2 + D \\frac{\\sigma^2}{h^2}}\n$$\nwhere the truncation error contribution is now first order in $h$.\n\nWith this theoretical framework, we can predict the outcomes of the four experiments.\n1.  **Noiseless centered-difference ($p_{\\mathrm{cd},0}$)**: Here, $\\sigma=0$, so $E_{\\text{cd}} \\approx \\sqrt{A h^4} \\propto h^2$. On a log-log plot of $E$ versus $h$, we have $\\log_{10} E = 2 \\log_{10} h + \\text{const}$. The slope $p_{\\mathrm{cd},0}$ is expected to be $2$.\n2.  **Noiseless forward-difference ($p_{\\mathrm{fd},0}$)**: Here, $\\sigma=0$, so $E_{\\text{fd}} \\approx \\sqrt{C h^2} \\propto h^1$. On a log-log plot, $\\log_{10} E = 1 \\log_{10} h + \\text{const}$. The slope $p_{\\mathrm{fd},0}$ is expected to be $1$.\n3.  **Noise scaling at fixed grid ($q_{\\sigma}$)**: With a fixed, small $h$, the truncation error term $A h^4$ is a small constant. For sufficiently large $\\sigma$, the noise term $B \\sigma^2/h^2$ will dominate. Thus, $E_{\\text{cd}} \\approx \\sqrt{B \\sigma^2/h^2} = \\frac{\\sqrt{B}}{h}\\sigma \\propto \\sigma$. On a log-log plot of $E$ versus $\\sigma$, we have $\\log_{10} E = 1 \\log_{10} \\sigma + \\text{const}$. The slope $q_{\\sigma}$ is expected to be $1$.\n4.  **Noise-dominated scaling in $h$ ($r_{h,\\mathrm{noise}}$)**: With fixed $\\sigma$ and small $h$, the noise term again dominates. $E_{\\text{cd}} \\approx \\sqrt{B \\sigma^2/h^2} \\propto \\frac{1}{h} = h^{-1}$. On a log-log plot of $E$ versus $h$, we have $\\log_{10} E = -1 \\log_{10} h + \\text{const}$. The slope $r_{h,\\mathrm{noise}}$ is expected to be $-1$.\n\nThe following Python code implements these four tests to numerically verify these theoretical scaling laws.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef compute_rms_error(approx_deriv, exact_deriv):\n    \"\"\"Computes the root-mean-square error.\"\"\"\n    return np.sqrt(np.mean((approx_deriv - exact_deriv)**2))\n\ndef run_noiseless_centered_test():\n    \"\"\"\n    Test 1: Noiseless centered-difference order test.\n    Computes the scaling exponent of error with respect to h for a\n    noiseless centered-difference scheme. Expected value is 2.\n    \"\"\"\n    M_values = np.array([40, 80, 160, 320, 640])\n    h_values = 2 * np.pi / M_values\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y = np.cos(t)\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y[indices + 1] - y[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n\n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef run_noiseless_forward_test():\n    \"\"\"\n    Test 2: Noiseless forward-difference order test.\n    Computes the scaling exponent of error with respect to h for a\n    noiseless forward-difference scheme. Expected value is 1.\n    \"\"\"\n    M_values = np.array([40, 80, 160, 320, 640])\n    h_values = 2 * np.pi / M_values\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y = np.cos(t)\n\n        # Valid indices for forward difference are 0, ..., M-1\n        indices = np.arange(0, M)\n        y_fd = (y[indices + 1] - y[indices]) / h\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_fd, y_prime_exact)\n        error_values.append(error)\n        \n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef run_noise_sigma_scaling_test():\n    \"\"\"\n    Test 3: Noise scaling at fixed grid.\n    Computes the scaling exponent of error with respect to sigma for a\n    fixed grid. Expected value is 1.\n    \"\"\"\n    M = 2000\n    sigma_values = np.array([1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n    error_values = []\n    \n    h = 2 * np.pi / M\n    t = np.linspace(0, 2 * np.pi, M + 1)\n    y_true = np.cos(t)\n\n    # Generate a single base noise sequence\n    rng = np.random.default_rng(seed=2025)\n    z = rng.normal(0, 1, size=M + 1)\n\n    for sigma in sigma_values:\n        epsilon = sigma * z\n        y_noisy = y_true + epsilon\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y_noisy[indices + 1] - y_noisy[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n        \n    log_sigma = np.log10(sigma_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_sigma, log_E)\n    return slope\n\ndef run_noise_h_scaling_test():\n    \"\"\"\n    Test 4: Noise-dominated scaling in h.\n    Computes the scaling exponent of error with respect to h in the\n    noise-dominated regime. Expected value is -1.\n    \"\"\"\n    M_values = np.array([500, 800, 1200, 2000])\n    h_values = 2 * np.pi / M_values\n    sigma = 1e-3\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y_true = np.cos(t)\n        \n        # Generate new noise for each M with specified seed\n        rng = np.random.default_rng(seed=3000 + M)\n        epsilon = sigma * rng.normal(0, 1, size=M + 1)\n        y_noisy = y_true + epsilon\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y_noisy[indices + 1] - y_noisy[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n\n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef solve():\n    \"\"\"\n    Executes the four test cases and prints the results in the\n    specified format.\n    \"\"\"\n    p_cd_0 = run_noiseless_centered_test()\n    p_fd_0 = run_noiseless_forward_test()\n    q_sigma = run_noise_sigma_scaling_test()\n    r_h_noise = run_noise_h_scaling_test()\n\n    results = [p_cd_0, p_fd_0, q_sigma, r_h_noise]\n    \n    # Format the results to three decimal places and print\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Run the solution\nsolve()\n```", "id": "2391185"}]}