{"hands_on_practices": [{"introduction": "The foundation of analyzing any numerical scheme is understanding its truncation error, which quantifies the discrepancy between the discrete approximation and the true continuous operator. This exercise provides fundamental practice in using Taylor series expansions to derive the leading-order truncation error for two different central-difference stencils. By comparing the error coefficients of a standard stencil and a wider, \"hole\" stencil, you will gain direct insight into how the geometric properties of a finite difference operator influence its accuracy [@problem_id:2440670].", "problem": "Consider a uniform, one-dimensional structured grid with nodes at positions $x_i = x_0 + i h$, where $h > 0$ is the constant grid spacing and $i$ is an integer index. Let $f(x)$ be a scalar field with continuous derivatives up to at least order $5$ in a neighborhood of $x_i$. Two discrete approximations to the first derivative $f^{\\prime}(x_i)$ are defined as follows:\n\n- The standard central-difference stencil using immediate neighbors:\n$$D_{\\mathrm{std}} f\\big|_{x_i} = \\frac{f(x_{i+1}) - f(x_{i-1})}{2 h}.$$\n\n- A modified stencil that enforces a connectivity \"hole\" by linking only every other node:\n$$D_{\\mathrm{hole}} f\\big|_{x_i} = \\frac{f(x_{i+2}) - f(x_{i-2})}{4 h}.$$\n\nAssuming $f$ is sufficiently smooth and ignoring round-off errors, each approximation incurs a leading truncation error term proportional to $h^2 f^{(3)}(x_i)$. Define $C_{\\mathrm{std}}$ and $C_{\\mathrm{hole}}$ as the scalar coefficients multiplying $h^2 f^{(3)}(x_i)$ in the leading truncation error for $D_{\\mathrm{std}}$ and $D_{\\mathrm{hole}}$, respectively, with signs included. Determine the ratio $R = \\left|\\dfrac{C_{\\mathrm{hole}}}{C_{\\mathrm{std}}}\\right|$ as a single real number. No rounding is required.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard problem in numerical analysis concerning the derivation and comparison of truncation errors for finite difference schemes. We will proceed with a rigorous solution based on Taylor series expansions.\n\nThe core of the analysis is to expand the function $f(x)$ in a Taylor series around the grid point $x_i$. The function is assumed to be sufficiently smooth, allowing for expansion up to the necessary order. The Taylor series for $f(x)$ at a point $x_i+\\delta$ is given by:\n$$f(x_i + \\delta) = f(x_i) + \\delta f'(x_i) + \\frac{\\delta^2}{2!} f''(x_i) + \\frac{\\delta^3}{3!} f^{(3)}(x_i) + \\frac{\\delta^4}{4!} f^{(4)}(x_i) + \\dots$$\nwhere primes and parenthetical superscripts denote derivatives with respect to $x$.\n\nFirst, we determine the coefficient $C_{\\mathrm{std}}$ for the standard central difference approximation, $D_{\\mathrm{std}} f\\big|_{x_i}$. The formula is:\n$$D_{\\mathrm{std}} f\\big|_{x_i} = \\frac{f(x_{i+1}) - f(x_{i-1})}{2 h}$$\nWe require expansions for $f(x_{i+1}) = f(x_i+h)$ and $f(x_{i-1}) = f(x_i-h)$.\nSetting $\\delta = h$:\n$$f(x_{i+1}) = f(x_i) + h f'(x_i) + \\frac{h^2}{2} f''(x_i) + \\frac{h^3}{6} f^{(3)}(x_i) + \\frac{h^4}{24} f^{(4)}(x_i) + O(h^5)$$\nSetting $\\delta = -h$:\n$$f(x_{i-1}) = f(x_i) - h f'(x_i) + \\frac{h^2}{2} f''(x_i) - \\frac{h^3}{6} f^{(3)}(x_i) + \\frac{h^4}{24} f^{(4)}(x_i) + O(h^5)$$\nSubtracting the second expansion from the first yields the numerator of the stencil:\n$$f(x_{i+1}) - f(x_{i-1}) = (f(x_i) - f(x_i)) + (h - (-h)) f'(x_i) + (\\frac{h^2}{2} - \\frac{h^2}{2}) f''(x_i) + (\\frac{h^3}{6} - (-\\frac{h^3}{6})) f^{(3)}(x_i) + O(h^5)$$\n$$f(x_{i+1}) - f(x_{i-1}) = 2 h f'(x_i) + \\frac{2 h^3}{6} f^{(3)}(x_i) + O(h^5) = 2 h f'(x_i) + \\frac{h^3}{3} f^{(3)}(x_i) + O(h^5)$$\nNow, substituting this into the expression for $D_{\\mathrm{std}} f\\big|_{x_i}$:\n$$D_{\\mathrm{std}} f\\big|_{x_i} = \\frac{2 h f'(x_i) + \\frac{h^3}{3} f^{(3)}(x_i) + O(h^5)}{2 h} = f'(x_i) + \\frac{h^2}{6} f^{(3)}(x_i) + O(h^4)$$\nThe truncation error, $\\mathcal{E}_{\\mathrm{std}}$, is the difference between the approximation and the true derivative:\n$$\\mathcal{E}_{\\mathrm{std}} = D_{\\mathrm{std}} f\\big|_{x_i} - f'(x_i) = \\frac{h^2}{6} f^{(3)}(x_i) + O(h^4)$$\nThe leading term of this error is defined as $C_{\\mathrm{std}} h^2 f^{(3)}(x_i)$. By direct comparison, we identify the coefficient:\n$$C_{\\mathrm{std}} = \\frac{1}{6}$$\n\nNext, we perform the same analysis for the modified stencil, $D_{\\mathrm{hole}} f\\big|_{x_i}$. The formula is:\n$$D_{\\mathrm{hole}} f\\big|_{x_i} = \\frac{f(x_{i+2}) - f(x_{i-2})}{4 h}$$\nThis stencil involves points $x_{i+2} = x_i+2h$ and $x_{i-2} = x_i-2h$. We can use the Taylor series expansion with a step of $2h$. Let us define an effective grid spacing $H = 2h$. The stencil can be rewritten as:\n$$D_{\\mathrm{hole}} f\\big|_{x_i} = \\frac{f(x_i+H) - f(x_i-H)}{2 H}$$\nThis is structurally identical to the standard central difference formula, but with step size $H$ instead of $h$. The leading truncation error will therefore also be structurally identical, with $h$ replaced by $H$:\n$$\\mathcal{E}_{\\mathrm{hole}} = \\frac{H^2}{6} f^{(3)}(x_i) + O(H^4)$$\nSubstituting $H = 2h$ back into the expression:\n$$\\mathcal{E}_{\\mathrm{hole}} = \\frac{(2h)^2}{6} f^{(3)}(x_i) + O((2h)^4) = \\frac{4h^2}{6} f^{(3)}(x_i) + O(h^4) = \\frac{2}{3} h^2 f^{(3)}(x_i) + O(h^4)$$\nThe leading term of this error is defined as $C_{\\mathrm{hole}} h^2 f^{(3)}(x_i)$. By comparison, we identify the coefficient:\n$$C_{\\mathrm{hole}} = \\frac{2}{3}$$\n\nFinally, we are required to compute the ratio $R = \\left|\\dfrac{C_{\\mathrm{hole}}}{C_{\\mathrm{std}}}\\right|$. Using the derived coefficients:\n$$R = \\left|\\frac{2/3}{1/6}\\right| = \\left|\\frac{2}{3} \\cdot \\frac{6}{1}\\right| = \\left|\\frac{12}{3}\\right| = |4|$$\nThus, the resulting ratio is:\n$$R = 4$$\nThis result demonstrates that increasing the stencil width from $2h$ to $4h$ while maintaining the structure of the central difference approximation increases the coefficient of the leading truncation error by a factor of $4$.", "answer": "$$\\boxed{4}$$", "id": "2440670"}, {"introduction": "While theoretical analysis on uniform grids provides essential insights, practical applications often involve grids with imperfections. This computational practice bridges theory and practice by asking you to run a numerical experiment to measure the convergence rate of a finite difference scheme on a randomly \"jittered\" grid. By analyzing how the error $E(N,\\rho)$ scales with the grid spacing $\\Delta x$, you will empirically discover and verify the critical principle that a nominally second-order scheme degrades to first-order accuracy on a non-uniform grid, highlighting the profound impact of grid quality on numerical accuracy [@problem_id:2440669].", "problem": "Consider the open interval domain $\\left(0,1\\right)$ and a continuously differentiable function $u:\\left[0,1\\right]\\to\\mathbb{R}$ given by $u(x)=\\sin\\!\\left(2\\pi x\\right)$, where angles are measured in radians. Its exact derivative is $u'(x)=2\\pi\\cos\\!\\left(2\\pi x\\right)$. For a given positive integer $N$, define the nominal uniform spacing $\\Delta x = 1/N$, and define a one-dimensional structured grid with randomly perturbed node locations by\n$$\nx_i = i\\,\\Delta x + \\epsilon_i,\\quad i=0,1,\\dots,N,\n$$\nwhere $\\epsilon_0 = 0$ and the perturbations satisfy the increment condition\n$$\n\\epsilon_{i+1}-\\epsilon_i \\sim \\text{Uniform}\\!\\left(-\\rho\\,\\Delta x,\\;\\rho\\,\\Delta x\\right)\n$$\nwith a fixed jitter parameter $\\rho \\in [0,1)$, independently for $i=0,1,\\dots,N-1$. This construction guarantees strictly increasing nodes $x_0<x_1<\\cdots<x_N$ for any fixed $\\rho\\in[0,1)$.\n\nDefine the three-point symmetric central difference operator at interior nodes $i=1,2,\\dots,N-1$ by\n$$\n\\left(Du\\right)\\!\\left(x_i\\right) \\;=\\; \\frac{u\\!\\left(x_{i+1}\\right) - u\\!\\left(x_{i-1}\\right)}{x_{i+1} - x_{i-1}}.\n$$\nLet the error at resolution $N$ be the interior maximum norm\n$$\nE(N,\\rho) \\;=\\; \\max_{i=1,\\dots,N-1} \\left| \\left(Du\\right)\\!\\left(x_i\\right) - u'(x_i) \\right|.\n$$\n\nYour task is to analyze the observed convergence rate $p(\\rho)$ of the above central difference operator as the nominal spacing $\\Delta x$ is refined. Use the least-squares slope of the line that fits $\\{\\left(\\log \\Delta x_k, \\log E(N_k,\\rho)\\right)\\}$ to estimate $p(\\rho)$, where the logarithm is the natural logarithm and $\\Delta x_k = 1/N_k$.\n\nUse the following test suite of jitter parameters and refinements:\n- Jitter parameters: $\\rho \\in \\{0,\\;0.25,\\;0.5\\}$.\n- Refinement levels: $N \\in \\{50,\\;100,\\;200,\\;400,\\;800\\}$.\n\nFor reproducibility, for each fixed $\\rho$ use a base seed $s(\\rho)$ defined by\n$$\ns(0) = 314159,\\quad s(0.25) = 271828,\\quad s(0.5) = 161803,\n$$\nand for each $N$ within the same $\\rho$ case, use the seed $s(\\rho)+N$ to generate the independent perturbation sequence $\\{\\epsilon_i\\}_{i=0}^N$ as specified above.\n\nYour program must compute, for each $\\rho$ in the test suite, the least-squares estimate of the convergence rate $p(\\rho)$ from the data pairs $\\left(\\log \\Delta x, \\log E(N,\\rho)\\right)$ over the given refinements. Report the three values $p(0)$, $p(0.25)$, and $p(0.5)$ rounded to three decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[p(0),p(0.25),p(0.5)]$. For example, an output with three numbers $a$, $b$, and $c$ must appear exactly as $[a,b,c]$ on a single line.", "solution": "The problem statement is parsed and validated. It is determined to be scientifically sound, well-posed, and an unambiguous exercise in computational numerical analysis. All necessary conditions, parameters, and definitions for a unique, verifiable solution are provided. The problem is therefore valid.\n\nThe task is to determine the observed order of convergence, $p(\\rho)$, for a three-point central difference scheme on a randomly perturbed one-dimensional grid. The order of convergence quantifies how the numerical error $E$ decreases as the grid spacing $\\Delta x$ is refined, typically following a power law $E \\propto (\\Delta x)^p$. By taking the natural logarithm, we obtain a linear relationship $\\log E = p \\log \\Delta x + C$, where the slope $p$ is the desired order of convergence.\n\nThe theoretical foundation for this analysis rests on the Taylor series expansion of the function $u(x)$ being approximated. For the given difference operator,\n$$\n\\left(Du\\right)\\!\\left(x_i\\right) = \\frac{u(x_{i+1}) - u(x_{i-1})}{x_{i+1} - x_{i-1}},\n$$\nthe local truncation error $\\tau_i = \\left(Du\\right)\\!\\left(x_i\\right) - u'(x_i)$ at an interior node $x_i$ is examined.\n\nCase 1: Uniform Grid ($\\rho = 0$).\nIn this case, the grid nodes are $x_i = i \\Delta x$. The spacing is uniform, such that $x_{i+1} - x_i = \\Delta x$ and $x_i - x_{i-1} = \\Delta x$. The operator simplifies to the standard second-order central difference formula:\n$$\n\\left(Du\\right)\\!\\left(x_i\\right) = \\frac{u(x_i + \\Delta x) - u(x_i - \\Delta x)}{2 \\Delta x}.\n$$\nTaylor series expansion of $u(x_i \\pm \\Delta x)$ around $x_i$ shows that the truncation error is\n$$\n\\tau_i = \\frac{u'''(x_i)}{6} (\\Delta x)^2 + O((\\Delta x)^4).\n$$\nThe error is proportional to $(\\Delta x)^2$. Therefore, we expect the order of convergence to be $p(0) = 2$.\n\nCase 2: Perturbed Grid ($\\rho > 0$).\nHere, the grid spacing is non-uniform. Let $h_i = x_{i+1} - x_i$ and $h_{i-1} = x_i - x_{i-1}$. The truncation error is\n$$\n\\tau_i = \\frac{u''(x_i)}{2} (h_i - h_{i-1}) + O(\\max(h_i^2, h_{i-1}^2)).\n$$\nThe difference in adjacent cell sizes is given by $h_i - h_{i-1} = (\\Delta x + \\epsilon_{i+1} - \\epsilon_i) - (\\Delta x + \\epsilon_i - \\epsilon_{i-1}) = (\\epsilon_{i+1} - \\epsilon_i) - (\\epsilon_i - \\epsilon_{i-1})$. Since each increment $\\epsilon_{j+1} - \\epsilon_j$ is a random variable of order $O(\\rho \\Delta x)$, their difference is also of order $O(\\rho \\Delta x)$. The leading error term is thus proportional to $\\rho \\Delta x$, and the scheme's accuracy degrades to first order. We expect the order of convergence to be $p(\\rho) = 1$ for any $\\rho > 0$.\n\nThe computational procedure to verify these theoretical rates is as follows. For each specified jitter parameter $\\rho \\in \\{0, 0.25, 0.5\\}$, we perform a grid refinement study using the resolutions $N \\in \\{50, 100, 200, 400, 800\\}$. For each pair $(N, \\rho)$:\n1.  The random number generator is seeded with $s(\\rho) + N$ for reproducibility, where $s(\\rho)$ is the given base seed.\n2.  The nominal spacing is $\\Delta x = 1/N$.\n3.  The perturbations $\\{\\epsilon_i\\}_{i=0}^N$ are generated. The increments $\\Delta \\epsilon_i = \\epsilon_{i+1} - \\epsilon_i$ are sampled from $\\text{Uniform}(-\\rho \\Delta x, \\rho \\Delta x)$. The cumulative sum of these increments, starting from $\\epsilon_0 = 0$, yields the full set of perturbations.\n4.  The grid node locations $x_i = i \\Delta x + \\epsilon_i$ are computed.\n5.  The function $u(x) = \\sin(2\\pi x)$ is evaluated at each node $x_i$.\n6.  At each interior node $i=1, \\dots, N-1$, the numerical derivative $\\left(Du\\right)\\!\\left(x_i\\right)$ is calculated using the three-point formula, and the exact derivative $u'(x_i) = 2\\pi \\cos(2\\pi x)$ is calculated.\n7.  The absolute error $|\\left(Du\\right)\\!\\left(x_i\\right) - u'(x_i)|$ is computed at each interior node, and the maximum of these errors is stored as $E(N, \\rho)$.\n8.  This process generates five data points $(\\log(\\Delta x_k), \\log(E(N_k, \\rho)))$ for each $\\rho$.\n9.  A linear least-squares regression is applied to these data points to find the slope, which is our estimate for the convergence rate $p(\\rho)$.\n\nThe final result is obtained by rounding the computed rates $p(0)$, $p(0.25)$, and $p(0.5)$ to three decimal places. The following code implements this specified procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the observed convergence rate for a central difference operator\n    on a randomly perturbed grid for different jitter parameters.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (jitter_parameter, base_seed)\n        (0.0, 314159),\n        (0.25, 271828),\n        (0.5, 161803),\n    ]\n\n    # Refinement levels\n    N_values = [50, 100, 200, 400, 800]\n\n    # Define the function and its exact derivative\n    u = lambda x: np.sin(2 * np.pi * x)\n    u_prime = lambda x: 2 * np.pi * np.cos(2 * np.pi * x)\n\n    convergence_rates = []\n\n    for rho, base_seed in test_cases:\n        log_dx_data = []\n        log_E_data = []\n\n        for N in N_values:\n            # Step 1: Set seed for reproducibility\n            seed = base_seed + N\n            rng = np.random.default_rng(seed)\n\n            # Step 2: Generate the perturbed grid\n            dx = 1.0 / N\n            \n            # Generate increments for epsilon: delta_eps_i = epsilon_{i+1} - epsilon_i\n            delta_eps = rng.uniform(low=-rho * dx, high=rho * dx, size=N)\n            \n            # Compute epsilon_i from increments, with epsilon_0 = 0\n            # epsilon_i = sum_{j=0}^{i-1} delta_eps_j\n            eps = np.zeros(N + 1)\n            eps[1:] = np.cumsum(delta_eps)\n\n            # Define grid nodes x_i = i*dx + epsilon_i\n            x_ideal = np.arange(N + 1) * dx\n            x = x_ideal + eps\n\n            # Step 3: Evaluate the function on the grid\n            u_vals = u(x)\n\n            # Step 4: Compute numerical derivative at interior nodes i=1,...,N-1\n            # (Du)(x_i) = (u(x_{i+1}) - u(x_{i-1})) / (x_{i+1} - x_{i-1})\n            x_interior = x[1:-1]\n            Du_vals = (u_vals[2:] - u_vals[:-2]) / (x[2:] - x[:-2])\n\n            # Step 5: Compute exact derivative at interior nodes\n            u_prime_vals = u_prime(x_interior)\n\n            # Step 6: Calculate the interior maximum norm error E(N, rho)\n            errors = np.abs(Du_vals - u_prime_vals)\n            E_N_rho = np.max(errors)\n            \n            # Step 7: Store log-log data for regression\n            log_dx_data.append(np.log(dx))\n            log_E_data.append(np.log(E_N_rho))\n\n        # Step 8: Perform linear least-squares regression to find the convergence rate p(rho)\n        # We are fitting log(E) = p * log(dx) + c\n        # numpy.polyfit returns the coefficients [p, c] of the fitted polynomial.\n        p_rho, _ = np.polyfit(log_dx_data, log_E_data, 1)\n\n        convergence_rates.append(p_rho)\n\n    # Final print statement in the exact required format.\n    # Round results to three decimal places.\n    rounded_rates = [round(p, 3) for p in convergence_rates]\n    print(f\"[{','.join(map(str, rounded_rates))}]\")\n\nsolve()\n```", "id": "2440669"}, {"introduction": "Implementing accurate boundary conditions is one of the most challenging aspects of computational modeling, especially on non-uniform grids designed to resolve boundary layers. This advanced practice focuses on the rigorous analysis of a Neumann boundary condition implemented on a highly \"stretched\" grid, where the ratio of adjacent cell sizes $r = s/t$ becomes very large. By deriving the truncation error and evaluating its behavior in this limit, you will determine whether the numerical scheme remains consistent, a crucial test for the robustness of boundary condition implementations in practical simulations [@problem_id:2440694].", "problem": "Consider a sufficiently smooth function $u \\in C^{3}([0,L])$ that solves the one-dimensional partial differential equation (PDE) $u''(x)=f(x)$ on $[0,L]$ with a Neumann boundary condition at the right endpoint, $u'(L)=g$, and a Dirichlet boundary condition at the left endpoint, $u(0)=u_{0}$. Let $\\{x_{i}\\}_{i=0}^{N}$ be a structured but nonuniform grid with $x_{0}=0$, $x_{N}=L$, and spacings $\\Delta x_{i}=x_{i}-x_{i-1}$ for $i \\in \\{1,\\dots,N\\}$. Focus on the right boundary $x_{N}$ and denote $s=\\Delta x_{N}$ and $t=\\Delta x_{N-1}$. \n\nAt $x_{N}$, the Neumann boundary condition is imposed by approximating $u'(x_{N})$ with a finite difference (FD) expression that uses the three-point stencil $\\{x_{N-2},x_{N-1},x_{N}\\}$ and is exact for all polynomials of degree at most $2$ in $x$. This uniquely determines a linear combination $A u(x_{N-2}) + B u(x_{N-1}) + C u(x_{N})$ that approximates $u'(x_{N})$.\n\nDefine the local truncation error at the boundary node $x_{N}$, denoted $\\tau_{N}$, as the difference between this three-point FD approximation and the exact derivative $u'(x_{N})$. Consider a family of grids for which $s \\to 0$, $t \\to 0$, and the ratio $r=\\frac{s}{t} \\to \\infty$.\n\nCompute the exact value of the limit\n$$\nC_{\\infty} \\;=\\; \\lim_{r \\to \\infty} \\; \\frac{\\tau_{N}}{u^{(3)}(x_{N})\\, s^{2}} .\n$$\n\nReport your answer as a single exact real number (no units).", "solution": "The problem as stated is well-posed, scientifically grounded, and contains sufficient information for a unique solution. It is a standard exercise in the analysis of finite difference methods. We proceed with the derivation.\n\nOur first objective is to determine the coefficients $A$, $B$, and $C$ for the finite difference approximation of $u'(x_{N})$. The approximation uses the three-point stencil $\\{x_{N-2}, x_{N-1}, x_{N}\\}$ and has the form $A u(x_{N-2}) + B u(x_{N-1}) + C u(x_{N})$. The stencil points are located at coordinates $x_{N}$, $x_{N-1} = x_{N} - s$, and $x_{N-2} = x_{N-1} - t = x_{N} - s - t$. The defining property of this approximation is that it is exact for all polynomials of degree at most $2$. We enforce this property by testing it on a suitable polynomial basis, for example, $\\{1, x-x_{N}, (x-x_{N})^2\\}$.\n\nLet $p(x)$ be a polynomial. The approximation $D[p]$ is given by $D[p] = A p(x_{N-2}) + B p(x_{N-1}) + C p(x_{N})$. The condition is $D[p] = p'(x_N)$ for $\\deg(p) \\le 2$.\n\nCase $1$: $p(x) = p_{0}(x) = 1$. The derivative is $p'_{0}(x_{N}) = 0$.\n$A p_{0}(x_{N-2}) + B p_{0}(x_{N-1}) + C p_{0}(x_{N}) = A(1) + B(1) + C(1) = 0$.\nThis gives our first equation:\n$$A + B + C = 0 \\quad (1)$$\n\nCase $2$: $p(x) = p_{1}(x) = x-x_{N}$. The derivative is $p'_{1}(x_{N}) = 1$.\n$A p_{1}(x_{N-2}) + B p_{1}(x_{N-1}) + C p_{1}(x_{N}) = A(-s-t) + B(-s) + C(0) = 1$.\nThis gives our second equation:\n$$-A(s+t) - B s = 1 \\quad (2)$$\n\nCase $3$: $p(x) = p_{2}(x) = (x-x_{N})^2$. The derivative is $p'_{2}(x_{N}) = 2(x_{N}-x_{N}) = 0$.\n$A p_{2}(x_{N-2}) + B p_{2}(x_{N-1}) + C p_{2}(x_{N}) = A(-s-t)^2 + B(-s)^2 + C(0)^2 = 0$.\nThis gives our third equation:\n$$A(s+t)^2 + B s^2 = 0 \\quad (3)$$\n\nWe now solve this linear system for $A$, $B$, and $C$. From equation $(3)$:\n$B = -A \\frac{(s+t)^2}{s^2}$.\nSubstitute this expression for $B$ into equation $(2)$:\n$-A(s+t) - s \\left( -A \\frac{(s+t)^2}{s^2} \\right) = 1$\n$-A(s+t) + A \\frac{(s+t)^2}{s} = 1$\n$A \\left( \\frac{(s+t)^2 - s(s+t)}{s} \\right) = 1$\n$A \\left( \\frac{(s+t)(s+t-s)}{s} \\right) = 1$\n$A \\frac{t(s+t)}{s} = 1 \\implies A = \\frac{s}{t(s+t)}$.\n\nNow, we find $B$:\n$B = - \\frac{s}{t(s+t)} \\cdot \\frac{(s+t)^2}{s^2} = -\\frac{s+t}{st}$.\n\nFinally, from equation $(1)$, we find $C$:\n$C = -A - B = -\\frac{s}{t(s+t)} - \\left( -\\frac{s+t}{st} \\right) = \\frac{s+t}{st} - \\frac{s}{t(s+t)}$\n$C = \\frac{(s+t)^2 - s^2}{st(s+t)} = \\frac{s^2+2st+t^2-s^2}{st(s+t)} = \\frac{2st+t^2}{st(s+t)} = \\frac{t(2s+t)}{st(s+t)} = \\frac{2s+t}{s(s+t)}$.\n\nThe next step is to analyze the local truncation error, $\\tau_{N}$, defined as the difference between the finite difference approximation and the exact derivative:\n$\\tau_{N} = \\left( A u(x_{N-2}) + B u(x_{N-1}) + C u(x_{N}) \\right) - u'(x_{N})$.\nSince $u \\in C^{3}([0,L])$, we can use Taylor series expansions of $u(x_{N-2})$ and $u(x_{N-1})$ around $x_{N}$:\n$u(x_{N-1}) = u(x_N - s) = u(x_N) - s u'(x_N) + \\frac{s^2}{2} u''(x_N) - \\frac{s^3}{6} u^{(3)}(x_N) + O(s^4)$.\n$u(x_{N-2}) = u(x_N - s - t) = u(x_N) - (s+t) u'(x_N) + \\frac{(s+t)^2}{2} u''(x_N) - \\frac{(s+t)^3}{6} u^{(3)}(x_N) + O((s+t)^4)$.\n\nSubstituting these into the expression for $\\tau_{N}$ and collecting terms by derivatives of $u$ at $x_{N}$:\n$$\\tau_N = u(x_N) (A+B+C) + u'(x_N) (-A(s+t) - Bs - 1) + u''(x_N) \\left( A\\frac{(s+t)^2}{2} + B\\frac{s^2}{2} \\right) - u^{(3)}(x_N) \\left( A\\frac{(s+t)^3}{6} + B\\frac{s^3}{6} \\right) + \\dots$$\nBy construction, the coefficients of $u(x_N)$, $u'(x_N)$, and $u''(x_N)$ are zero.\n- $A+B+C = 0$.\n- $-A(s+t) - Bs = 1$, so the coefficient of $u'(x_N)$ is $1-1=0$.\n- $A(s+t)^2 + Bs^2 = 0$, so the coefficient of $u''(x_N)$ is $0$.\n\nThe leading term of the truncation error is determined by the $u^{(3)}(x_N)$ term:\n$\\tau_{N} = - \\left( A\\frac{(s+t)^3}{6} + B\\frac{s^3}{6} \\right) u^{(3)}(x_N) + \\dots$\nLet's evaluate the coefficient $K = - \\frac{1}{6} \\left( A(s+t)^3 + B s^3 \\right)$:\n$K = - \\frac{1}{6} \\left[ \\frac{s}{t(s+t)} (s+t)^3 - \\frac{s+t}{st} s^3 \\right]$\n$K = - \\frac{1}{6} \\left[ \\frac{s(s+t)^2}{t} - \\frac{s^2(s+t)}{t} \\right]$\nThis can be factored:\n$K = - \\frac{s(s+t)}{6t} \\left[ (s+t) - s \\right]$\n$K = - \\frac{s(s+t)}{6t} [t] = - \\frac{s(s+t)}{6}$.\n\nThus, the truncation error is $\\tau_N = -\\frac{s(s+t)}{6} u^{(3)}(x_N) + \\dots$.\nWe are required to compute the limit $C_{\\infty} = \\lim_{r \\to \\infty} \\frac{\\tau_N}{u^{(3)}(x_N) s^2}$. Substituting the leading term of $\\tau_N$:\n$C_{\\infty} = \\lim_{r \\to \\infty} \\frac{-\\frac{s(s+t)}{6} u^{(3)}(x_N)}{u^{(3)}(x_N) s^2}$.\nAssuming $u^{(3)}(x_N) \\neq 0$, we can simplify the expression:\n$C_{\\infty} = \\lim_{r \\to \\infty} \\left( -\\frac{s(s+t)}{6s^2} \\right) = \\lim_{r \\to \\infty} \\left( -\\frac{s+t}{6s} \\right) = \\lim_{r \\to \\infty} \\left( -\\frac{1}{6} \\left(1 + \\frac{t}{s}\\right) \\right)$.\nThe problem defines the grid ratio as $r = \\frac{s}{t}$. Therefore, $\\frac{t}{s} = \\frac{1}{r}$.\nSubstituting this into the limit expression:\n$C_{\\infty} = \\lim_{r \\to \\infty} \\left( -\\frac{1}{6} \\left(1 + \\frac{1}{r}\\right) \\right)$.\nAs $r \\to \\infty$, the term $\\frac{1}{r}$ approaches $0$. The limit is:\n$C_{\\infty} = -\\frac{1}{6} (1+0) = -\\frac{1}{6}$.\nThe higher-order terms in the Taylor expansion, when divided by $s^2$, will vanish in the limit as $s \\to 0$ and $t \\to 0$, justifying the use of the leading error term for this calculation.", "answer": "$$ \\boxed{-\\frac{1}{6}} $$", "id": "2440694"}]}