{"hands_on_practices": [{"introduction": "We begin by empirically verifying one of the most important theoretical results in Monte Carlo simulation: the convergence rate. This exercise guides you through a numerical experiment to estimate the rate at which the error of a standard Monte Carlo estimator decreases as the number of samples, $n$, increases. By performing this analysis for several different underlying probability distributions, you will gain a concrete understanding of the famous $\\mathcal{O}(n^{-1/2})$ convergence behavior that is predicted by the Central Limit Theorem [@problem_id:2414889].", "problem": "A two-dimensional metal plate occupies the unit square domain with total area equal to $1$. Time is measured in discrete steps indexed by $k \\in \\mathbb{N}$. At each time index $k$, the plate’s boundary temperature is spatially uniform and equal to a random temperature $X_k$ (in Kelvin), where $\\{X_k\\}_{k \\ge 1}$ are independent and identically distributed random variables with a distribution specified per test case. Assume that thermal conduction is sufficiently fast relative to the time between boundary changes so that, at each time index $k$, the plate’s internal temperature field is spatially uniform and equal to the boundary temperature $X_k$. Consequently, the spatially averaged temperature of the plate at time index $k$ equals $X_k$, and the long-time average plate temperature equals the mathematical expectation $\\mu = \\mathbb{E}[X_1]$ (in Kelvin). The estimation of $\\mu$ is a Monte Carlo integration problem over the distribution of $X_1$.\n\nFor each test case below, consider the following definitions. For a positive integer $n$, define the estimator $\\widehat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, where $X_1, \\dots, X_n$ are independent samples drawn from the law of $X_1$ specified by the test case. For a list of sample sizes $\\{n_j\\}_{j=1}^{J}$ and a positive integer $R$, define, for each $n_j$, the empirical mean absolute error\n$$\nM_{n_j} \\;=\\; \\frac{1}{R} \\sum_{r=1}^{R} \\left| \\widehat{\\mu}^{(r)}_{n_j} - \\mu \\right|,\n$$\nwhere $\\widehat{\\mu}^{(r)}_{n_j}$ denotes an independent replicate of $\\widehat{\\mu}_{n_j}$. Define the empirical convergence rate exponent $\\widehat{r}$ as the least-squares slope of the linear fit $y = a + \\widehat{r}\\,x$ to the points $\\left(x_j, y_j\\right)$ with $x_j = \\ln(n_j)$ and $y_j = \\ln(M_{n_j})$, where $\\ln(\\cdot)$ denotes the natural logarithm. The quantity $\\widehat{r}$ is dimensionless. Each test case also specifies a seed $s$ for a pseudorandom number generator to ensure reproducibility.\n\nTest Suite:\n- Test case $1$: $X_1 \\sim \\mathrm{Uniform}([0,1])$. The exact mean is $\\mu = \\frac{1}{2}$ (in Kelvin). Use the sample sizes $n \\in \\{100, 300, 1000, 3000, 10000\\}$, the number of independent replicates $R = 400$, and the seed $s = 12345$.\n- Test case $2$: $X_1 \\sim \\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha = \\frac{1}{2}$ and $\\beta = \\frac{1}{2}$, supported on $[0,1]$. The exact mean is $\\mu = \\frac{\\alpha}{\\alpha+\\beta} = \\frac{1}{2}$ (in Kelvin). Use the sample sizes $n \\in \\{100, 300, 1000, 3000, 10000\\}$, the number of independent replicates $R = 400$, and the seed $s = 2023$.\n- Test case $3$: $X_1$ is a two-point mixture: $X_1 = 0$ with probability $p = \\frac{9}{10}$ and $X_1 = 1$ with probability $1-p = \\frac{1}{10}$. The exact mean is $\\mu = \\frac{1}{10}$ (in Kelvin). Use the sample sizes $n \\in \\{100, 300, 1000, 3000, 10000\\}$, the number of independent replicates $R = 400$, and the seed $s = 777$.\n- Test case $4$: $X_1 \\sim \\mathrm{Uniform}([0.49, 0.51])$. The exact mean is $\\mu = 0.5$ (in Kelvin). Use the sample sizes $n \\in \\{30, 100, 300, 1000, 3000\\}$, the number of independent replicates $R = 400$, and the seed $s = 4242$.\n\nAngle units are not applicable. All temperatures are in Kelvin. The final reported quantities are the four real numbers $\\widehat{r}_1, \\widehat{r}_2, \\widehat{r}_3, \\widehat{r}_4$, one per test case, each equal to the empirical convergence rate exponent defined above. Express each reported number as a decimal (no percent sign), rounded to $3$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$, for example, $[\\widehat{r}_1,\\widehat{r}_2,\\widehat{r}_3,\\widehat{r}_4]$.", "solution": "The problem statement is evaluated and found to be valid. It is scientifically grounded in the theory of Monte Carlo methods, well-posed with all necessary information provided, and objective in its formulation. No inconsistencies, ambiguities, or factual unsoundness are present.\n\nThe problem concerns the numerical estimation of the convergence rate for a Monte Carlo estimator of a mean. The physical framing involving a metal plate is an analogy; the core task is a statistical one. For each test case, we are asked to estimate the mean $\\mu = \\mathbb{E}[X_1]$ of a random variable $X_1$ using the sample mean estimator $\\widehat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, based on $n$ independent and identically distributed samples $X_1, \\dots, X_n$.\n\nThe rate of convergence of this estimator is a central result in computational statistics. According to the Central Limit Theorem (CLT), for a sequence of i.i.d. random variables $\\{X_i\\}$ with finite mean $\\mu$ and finite, non-zero variance $\\sigma^2 = \\mathrm{Var}(X_1)$, the distribution of the sample mean estimator converges to a normal distribution. Specifically, the random variable $\\sqrt{n}(\\widehat{\\mu}_n - \\mu)$ converges in distribution to a normal random variable $\\mathcal{N}(0, \\sigma^2)$ as $n \\to \\infty$.\n\nThis implies that the error of the estimator, $\\widehat{\\mu}_n - \\mu$, is approximately distributed as $\\mathcal{N}(0, \\sigma^2/n)$ for large $n$. The problem defines the mean absolute error, whose theoretical value is $M_n = \\mathbb{E}[|\\widehat{\\mu}_n - \\mu|]$. Using the normal approximation, we can relate $M_n$ to the sample size $n$:\n$$\nM_n = \\mathbb{E}[|\\widehat{\\mu}_n - \\mu|] \\approx \\mathbb{E}\\left[\\left|\\frac{\\sigma}{\\sqrt{n}} Z\\right|\\right] = \\frac{\\sigma}{\\sqrt{n}} \\mathbb{E}[|Z|]\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable. The quantity $\\mathbb{E}[|Z|] = \\sqrt{2/\\pi}$ is a constant. Therefore, we have the asymptotic relationship:\n$$\nM_n \\propto n^{-1/2}\n$$\nThis proportionality demonstrates that the mean absolute error of the standard Monte Carlo estimator converges to zero at a rate of $n^{-1/2}$.\n\nThe problem requires us to find an empirical estimate of this convergence rate exponent. This is done by performing a linear least-squares fit to the logarithmically transformed data points $(x_j, y_j)$, where $x_j = \\ln(n_j)$ and $y_j = \\ln(M_{n_j})$. Taking the natural logarithm of the asymptotic relationship gives:\n$$\n\\ln(M_n) \\approx \\ln(C) - \\frac{1}{2} \\ln(n)\n$$\nwhere $C$ is a constant of proportionality. This is a linear equation of the form $y = a + \\widehat{r} x$, with a theoretical slope (convergence rate exponent) of $\\widehat{r} = -1/2 = -0.5$.\n\nThis theoretical rate of $-0.5$ is universal for Monte Carlo integration of functions with respect to distributions that have a finite variance. All four distributions specified in the test cases possess a finite variance:\n$1$. For $X_1 \\sim \\mathrm{Uniform}([0,1])$, $\\sigma^2 = 1/12$.\n$2$. For $X_1 \\sim \\mathrm{Beta}(1/2, 1/2)$, $\\sigma^2 = 1/8$.\n$3$. For the two-point mixture (a Bernoulli trial), $\\sigma^2 = p(1-p) = (1/10)(9/10) = 9/100$.\n$4$. For $X_1 \\sim \\mathrm{Uniform}([0.49, 0.51])$, $\\sigma^2 = (0.51-0.49)^2/12 = (0.02)^2/12 \\approx 3.33 \\times 10^{-5}$.\n\nSince all variances are finite, the theoretical convergence rate exponent is $-0.5$ in all four test cases. The procedure described in the problem is a numerical experiment to verify this theoretical rate. We will implement this procedure for each test case.\n\nThe algorithm for each test case is as follows:\n$1$. Set the pseudorandom number generator seed $s$ for reproducibility.\n$2$. For each sample size $n_j$ in the provided list:\n    a. Perform $R$ independent simulation replicates.\n    b. In each replicate $r \\in \\{1, \\dots, R\\}$, generate $n_j$ random samples from the specified distribution for $X_1$.\n    c. Compute the sample mean $\\widehat{\\mu}^{(r)}_{n_j}$.\n    d. Calculate the absolute error $|\\widehat{\\mu}^{(r)}_{n_j} - \\mu|$, where $\\mu$ is the given exact mean.\n    e. After $R$ replicates, compute the empirical mean absolute error $M_{n_j} = \\frac{1}{R} \\sum_{r=1}^{R} |\\widehat{\\mu}^{(r)}_{n_j} - \\mu|$.\n$3$. Create a set of data points $(x_j, y_j)$ where $x_j = \\ln(n_j)$ and $y_j = \\ln(M_{n_j})$.\n$4$. Perform a simple linear regression on these points to find the slope $\\widehat{r}$ of the best-fit line $y = a + \\widehat{r}x$. This slope is the empirical convergence rate exponent.\n$5$. The result is then rounded to $3$ decimal places.\n\nThis procedure will be executed for all four test cases to obtain the values $\\widehat{r}_1, \\widehat{r}_2, \\widehat{r}_3, \\widehat{r}_4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Monte Carlo convergence rate problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Uniform(0,1)\",\n            \"dist_gen\": lambda rng, size: rng.uniform(0, 1, size),\n            \"mu\": 0.5,\n            \"n_values\": [100, 300, 1000, 3000, 10000],\n            \"R\": 400,\n            \"seed\": 12345\n        },\n        {\n            \"name\": \"Beta(0.5, 0.5)\",\n            \"dist_gen\": lambda rng, size: rng.beta(0.5, 0.5, size),\n            \"mu\": 0.5,\n            \"n_values\": [100, 300, 1000, 3000, 10000],\n            \"R\": 400,\n            \"seed\": 2023\n        },\n        {\n            \"name\": \"Two-point mixture\",\n            \"dist_gen\": lambda rng, size: rng.choice([0, 1], size=size, p=[0.9, 0.1]),\n            \"mu\": 0.1,\n            \"n_values\": [100, 300, 1000, 3000, 10000],\n            \"R\": 400,\n            \"seed\": 777\n        },\n        {\n            \"name\": \"Uniform(0.49, 0.51)\",\n            \"dist_gen\": lambda rng, size: rng.uniform(0.49, 0.51, size),\n            \"mu\": 0.5,\n            \"n_values\": [30, 100, 300, 1000, 3000],\n            \"R\": 400,\n            \"seed\": 4242\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        dist_gen = case[\"dist_gen\"]\n        mu = case[\"mu\"]\n        n_values = case[\"n_values\"]\n        R = case[\"R\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n        \n        log_n_values = []\n        log_M_n_values = []\n\n        for n in n_values:\n            # Store absolute errors for R replicates\n            absolute_errors = np.zeros(R)\n            \n            for r in range(R):\n                # Generate n samples\n                samples = dist_gen(rng, n)\n                # Compute sample mean\n                mu_hat = np.mean(samples)\n                # Compute absolute error\n                absolute_errors[r] = np.abs(mu_hat - mu)\n            \n            # Compute empirical mean absolute error M_n\n            M_n = np.mean(absolute_errors)\n            \n            # Store log-transformed values for regression\n            log_n_values.append(np.log(n))\n            log_M_n_values.append(np.log(M_n))\n            \n        # Perform linear least-squares regression on (log(n), log(M_n))\n        # np.polyfit returns [slope, intercept] for degree 1\n        x = np.array(log_n_values)\n        y = np.array(log_M_n_values)\n        \n        # The slope r_hat is the first element of the result\n        r_hat = np.polyfit(x, y, 1)[0]\n        \n        results.append(r_hat)\n\n    # Format the final output as specified\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2414889"}, {"introduction": "While standard Monte Carlo is a powerful and general tool, it has significant limitations, particularly when the integrand is concentrated in a very small region of a high-dimensional space. This thought experiment presents a carefully constructed \"worst-case\" scenario designed to highlight this weakness, where a naive estimator is overwhelmingly likely to produce a completely incorrect result [@problem_id:2414934]. Analyzing such a pathological case is crucial for appreciating *why* and *when* we need more sophisticated methods, motivating the transition from naive sampling to advanced variance reduction techniques.", "problem": "Let $d \\ge 1$ and $N \\in \\mathbb{N}$. Consider naive Monte Carlo integration of the integral $I_N = \\int_{[0,1]^d} f_N(x) \\, dx$, where the estimator is $\\widehat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f_N(U_i)$ and the random vectors $U_1, \\dots, U_N$ are independent and identically distributed with the uniform distribution on $[0,1]^d$. Define $\\ell = N^{-2/d}$ and fix a smooth function $\\psi: \\mathbb{R} \\to [0,1]$ such that $\\psi$ is $C^{\\infty}$, $\\psi(t) = 1$ for $|t| \\le \\frac{1}{2}$, and $\\psi(t) = 0$ for $|t| \\ge 1$. Define the smooth, bounded function $f_N: [0,1]^d \\to [0,1]$ by\n$$\nf_N(x_1,\\dots,x_d) \\;=\\; \\prod_{i=1}^{d} \\psi\\!\\left(\\frac{x_i - \\frac{1}{2}}{\\ell/2}\\right).\n$$\nThen $f_N$ is supported in the cube $[\\frac{1}{2}-\\frac{\\ell}{2}, \\frac{1}{2}+\\frac{\\ell}{2}]^d$ (of volume $\\ell^d$) and equals $1$ on the inner cube $[\\frac{1}{2}-\\frac{\\ell}{4}, \\frac{1}{2}+\\frac{\\ell}{4}]^d$ (of volume $(\\ell/2)^d$). Determine the exact value of the probability $\\mathbb{P}(\\widehat{I}_N = 0)$ as a closed-form analytic expression in terms of $N$ and $d$. No rounding is required and no units are involved; provide a single expression.", "solution": "The problem asks for the probability $\\mathbb{P}(\\widehat{I}_N = 0)$. We begin by a systematic analysis of the conditions under which the Monte Carlo estimator $\\widehat{I}_N$ is equal to zero.\n\nThe estimator is defined as a sum:\n$$\n\\widehat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f_N(U_i)\n$$\nThe function $f_N(x)$ is constructed as a product of functions $\\psi$, where the codomain of $\\psi$ is $[0,1]$. Therefore, $f_N(x)$ is non-negative for all $x$, i.e., $f_N(x) \\ge 0$. A sum of non-negative terms is zero if and only if each term is individually zero. This leads to the fundamental equivalence:\n$$\n\\widehat{I}_N = 0 \\quad \\iff \\quad f_N(U_i) = 0 \\text{ for all } i \\in \\{1, 2, \\ldots, N\\}\n$$\nThe random vectors $U_1, \\dots, U_N$ are specified to be independent and identically distributed. The probability of the joint event is therefore the product of the probabilities of the individual events:\n$$\n\\mathbb{P}(\\widehat{I}_N = 0) = \\mathbb{P}\\left(\\bigcap_{i=1}^{N} \\{f_N(U_i) = 0\\}\\right) = \\prod_{i=1}^{N} \\mathbb{P}(f_N(U_i) = 0)\n$$\nAs the distributions of $U_i$ are identical, the probability $\\mathbb{P}(f_N(U_i) = 0)$ is constant for all $i$. Let this probability be denoted by $p$. The expression simplifies to:\n$$\n\\mathbb{P}(\\widehat{I}_N = 0) = p^N, \\quad \\text{where } p = \\mathbb{P}(f_N(U) = 0) \\text{ for any } U \\sim \\text{Uniform}([0,1]^d)\n$$\nThe next step is to determine the value of $p$. This requires identifying the set of points $x$ for which $f_N(x) = 0$. The function is defined as:\n$$\nf_N(x) = f_N(x_1,\\dots,x_d) = \\prod_{i=1}^{d} \\psi\\left(\\frac{x_i - \\frac{1}{2}}{\\ell/2}\\right)\n$$\nA product equals zero if and only if at least one of its factors is zero. From the properties of $\\psi$, we know that $\\psi(t) = 0$ for all $t$ such that $|t| \\ge 1$. Consequently, $f_N(x)=0$ if and only if there exists at least one coordinate index $j \\in \\{1, \\ldots, d\\}$ for which:\n$$\n\\left| \\frac{x_j - \\frac{1}{2}}{\\ell/2} \\right| \\ge 1\n$$\nThis inequality is equivalent to $|x_j - \\frac{1}{2}| \\ge \\frac{\\ell}{2}$.\nThe function $f_N(x)$ is non-zero only if for all coordinate indices $j \\in \\{1, \\ldots, d\\}$, the strict inequality $|\\frac{x_j - 1/2}{\\ell/2}| < 1$ holds. This defines the open hypercube $S_{open} = (\\frac{1}{2} - \\frac{\\ell}{2}, \\frac{1}{2} + \\frac{\\ell}{2})^d$. The support of $f_N$, which is the closure of this set, is the closed hypercube $S = [\\frac{1}{2} - \\frac{\\ell}{2}, \\frac{1}{2} + \\frac{\\ell}{2}]^d$.\n\nThe event $f_N(U) = 0$ occurs if the random vector $U$ falls outside of the open set $S_{open}$. Since $U$ is drawn from a continuous uniform distribution, the probability of it landing on the boundary of $S_{open}$ (which has measure zero) is $0$. Therefore, the probability $p$ can be calculated as:\n$$\np = \\mathbb{P}(f_N(U) = 0) = \\mathbb{P}(U \\notin S_{open}) = 1 - \\mathbb{P}(U \\in S_{open})\n$$\nThe random vector $U$ is uniformly distributed over the unit hypercube $[0,1]^d$, which has a volume of $1^d=1$. The probability $\\mathbb{P}(U \\in S_{open})$ is equal to the volume of $S_{open}$, provided $S_{open} \\subseteq [0,1]^d$. The hypercube $S_{open}$ is centered at $(\\frac{1}{2}, \\ldots, \\frac{1}{2})$ and has side length $\\ell$. For it to be contained in $[0,1]^d$, we need $0 \\le \\frac{1}{2} - \\frac{\\ell}{2}$ and $\\frac{1}{2} + \\frac{\\ell}{2} \\le 1$. Both conditions simplify to $\\ell \\le 1$. We are given $\\ell = N^{-2/d}$, where $N \\in \\mathbb{N}$ and $d \\ge 1$. Since $N \\ge 1$, we have $N^{2/d} \\ge 1$, which implies $\\ell = N^{-2/d} \\le 1$. The condition is satisfied, so $S \\subset [0,1]^d$.\n\nThe volume of the hypercube $S$ (and $S_{open}$) is the side length raised to the power of the dimension:\n$$\n\\text{Volume}(S) = \\ell^d\n$$\nThe probability of a single sample falling within the support of $f_N$ is:\n$$\n\\mathbb{P}(U \\in S) = \\frac{\\text{Volume}(S)}{\\text{Volume}([0,1]^d)} = \\frac{\\ell^d}{1} = \\ell^d\n$$\nThe probability $p$ is then:\n$$\np = 1 - \\mathbb{P}(U \\in S) = 1 - \\ell^d\n$$\nWe now substitute this expression for $p$ back into the equation for $\\mathbb{P}(\\widehat{I}_N = 0)$:\n$$\n\\mathbb{P}(\\widehat{I}_N = 0) = (1 - \\ell^d)^N\n$$\nThe final step is to substitute the definition of $\\ell = N^{-2/d}$:\n$$\n\\ell^d = \\left(N^{-2/d}\\right)^d = N^{-2}\n$$\nThis gives the final closed-form expression for the probability:\n$$\n\\mathbb{P}(\\widehat{I}_N = 0) = \\left(1 - N^{-2}\\right)^N\n$$\nThis result demonstrates a specific rate of convergence for the probability of obtaining a zero estimate, which notably becomes independent of the dimension $d$ due to the chosen scaling of $\\ell$ with $N$ and $d$.", "answer": "$$\n\\boxed{\\left(1 - N^{-2}\\right)^{N}}\n$$", "id": "2414934"}, {"introduction": "Having established the need for improvement, we now explore a powerful and practical variance reduction technique known as control variates. This method involves approximating our complex integrand with a simpler function—in this case, a polynomial—whose integral can be computed exactly. By applying Monte Carlo to the *difference* between the original function and its simple approximation, we can dramatically reduce the estimator's variance and achieve higher accuracy with fewer samples [@problem_id:2414893]. This exercise provides hands-on experience in implementing one of the most widely used strategies for accelerating Monte Carlo integration.", "problem": "You are given the task of approximating integrals of functions over the unit hypercube $[0,1]^d$ using Monte Carlo (MC) integration, and of accelerating convergence by using a control variate based on a low-order polynomial approximation of the integrand. For each test case in the test suite below, your program must construct a total-degree polynomial control variate and evaluate its impact on accuracy relative to plain Monte Carlo.\n\nDefinitions and setup:\n- Let $f: [0,1]^d \\to \\mathbb{R}$ be an integrable function, and let the target integral be $I = \\int_{[0,1]^d} f(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x}$.\n- A total-degree polynomial of degree at most $k$ in $d$ variables is any polynomial of the form $p(\\boldsymbol{x}) = \\sum_{\\lvert \\boldsymbol{\\alpha} \\rvert \\le k} c_{\\boldsymbol{\\alpha}} \\,\\boldsymbol{x}^{\\boldsymbol{\\alpha}}$, where $\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d$, $\\lvert \\boldsymbol{\\alpha} \\rvert = \\alpha_1 + \\cdots + \\alpha_d$, and $\\boldsymbol{x}^{\\boldsymbol{\\alpha}} = \\prod_{j=1}^d x_j^{\\alpha_j}$.\n- The exact integral of a monomial over $[0,1]^d$ is $\\int_{[0,1]^d} \\boldsymbol{x}^{\\boldsymbol{\\alpha}} \\,\\mathrm{d}\\boldsymbol{x} = \\prod_{j=1}^d \\frac{1}{\\alpha_j + 1}$, hence for $p(\\boldsymbol{x})$ the exact integral is $\\mu_p = \\int_{[0,1]^d} p(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x} = \\sum_{\\lvert \\boldsymbol{\\alpha} \\rvert \\le k} c_{\\boldsymbol{\\alpha}} \\prod_{j=1}^d \\frac{1}{\\alpha_j + 1}$.\n- Angles used inside trigonometric functions must be interpreted in radians.\n\nFor each test case:\n- Use $m_{\\text{fit}}$ independent draws $\\{\\boldsymbol{Z}_j\\}_{j=1}^{m_{\\text{fit}}}$ from the uniform distribution on $[0,1]^d$ to determine the coefficients $\\{c_{\\boldsymbol{\\alpha}}\\}$ of the total-degree polynomial $p$ of degree at most $k$ that best fits the data $\\{(\\boldsymbol{Z}_j, f(\\boldsymbol{Z}_j))\\}$ in the least-squares sense.\n- Use an independent set of $n$ draws $\\{\\boldsymbol{Y}_i\\}_{i=1}^n$ from the uniform distribution on $[0,1]^d$ to compute:\n  - the plain Monte Carlo estimator $\\hat{I}_{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n f(\\boldsymbol{Y}_i)$,\n  - the control-variate estimator $\\hat{I}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(f(\\boldsymbol{Y}_i) - p(\\boldsymbol{Y}_i)\\big) + \\mu_p$.\n- For each case, compute the absolute error of each estimator using the exact integral $I$ of the given $f$. Then compute the ratio $r = \\frac{\\lvert \\hat{I}_{\\mathrm{CV}} - I \\rvert}{\\lvert \\hat{I}_{\\mathrm{MC}} - I \\rvert}$.\n\nIntegrands and their exact integrals:\n- In one dimension ($d = 1$): $f_1(x) = e^{-x} \\cos(7x) + x^3$, with exact integral\n  $$\n  I_1 = \\int_0^1 e^{-x} \\cos(7x)\\,\\mathrm{d}x + \\int_0^1 x^3\\,\\mathrm{d}x = \\Re\\!\\left(\\frac{e^{-1 + i\\,7} - 1}{-1 + i\\,7}\\right) + \\frac{1}{4}.\n  $$\n- In two dimensions ($d = 2$): $f_2(x,y) = \\sin(\\pi x)\\sin(\\pi y) + x\\,y$, with exact integral\n  $$\n  I_2 = \\left(\\int_0^1 \\sin(\\pi x)\\,\\mathrm{d}x\\right)^2 + \\left(\\int_0^1 x\\,\\mathrm{d}x\\right)\\left(\\int_0^1 y\\,\\mathrm{d}y\\right) = \\frac{4}{\\pi^2} + \\frac{1}{4}.\n  $$\n\nTest suite:\n- Case $1$: $f_1$, $d = 1$, polynomial degree $k = 3$, $m_{\\text{fit}} = 200$, $n = 20000$, seed $= 123456$.\n- Case $2$: $f_2$, $d = 2$, polynomial degree $k = 2$, $m_{\\text{fit}} = 500$, $n = 30000$, seed $= 202311$.\n- Case $3$: $f_1$, $d = 1$, polynomial degree $k = 0$, $m_{\\text{fit}} = 30$, $n = 5000$, seed $= 7777$.\n- Case $4$: $f_2$, $d = 2$, polynomial degree $k = 1$, $m_{\\text{fit}} = 100$, $n = 20000$, seed $= 8888$.\n\nRequirements:\n- For each test case, initialize a pseudorandom number generator with the given seed and use it for all sampling in that test case.\n- The final reported value for each test case is the ratio $r$ defined above, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and in the order of the cases listed, for example $[r_1,r_2,r_3,r_4]$.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent set of requirements for a standard problem in computational engineering. The task is to evaluate the effectiveness of a control variate method for Monte Carlo integration, where the control variate is a polynomial approximation of the integrand.\n\nThe fundamental principle behind Monte Carlo integration is the law of large numbers. For a random variable $\\boldsymbol{X}$ uniformly distributed in the unit hypercube $[0,1]^d$, the expected value of $f(\\boldsymbol{X})$ is the integral we wish to compute: $E[f(\\boldsymbol{X})] = \\int_{[0,1]^d} f(\\boldsymbol{x}) \\, \\mathrm{d}\\boldsymbol{x} = I$. By drawing $n$ independent and identically distributed samples $\\{\\boldsymbol{Y}_i\\}_{i=1}^n$ from this distribution, we can form the plain Monte Carlo estimator, which is the sample mean:\n$$\n\\hat{I}_{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n f(\\boldsymbol{Y}_i)\n$$\nThe central limit theorem states that the error of this estimator, $\\hat{I}_{\\mathrm{MC}} - I$, is approximately normally distributed with mean $0$ and variance $\\sigma_f^2/n$, where $\\sigma_f^2 = \\text{Var}(f(\\boldsymbol{X}))$. The standard error thus decreases as $n^{-1/2}$, which can be a slow rate of convergence.\n\nTo accelerate convergence, we can use variance reduction techniques. The control variate method is one such technique. It involves finding a function $p(\\boldsymbol{x})$ that is a good approximation to $f(\\boldsymbol{x})$ and whose integral $\\mu_p = \\int_{[0,1]^d} p(\\boldsymbol{x}) \\, \\mathrm{d}\\boldsymbol{x}$ is known analytically. We then rewrite the integral $I$ as:\n$$\nI = \\int_{[0,1]^d} (f(\\boldsymbol{x}) - p(\\boldsymbol{x})) \\, \\mathrm{d}\\boldsymbol{x} + \\int_{[0,1]^d} p(\\boldsymbol{x}) \\, \\mathrm{d}\\boldsymbol{x} = \\int_{[0,1]^d} (f(\\boldsymbol{x}) - p(\\boldsymbol{x})) \\, \\mathrm{d}\\boldsymbol{x} + \\mu_p\n$$\nWe can apply Monte Carlo integration to the new, hopefully better-behaved, integrand $g(\\boldsymbol{x}) = f(\\boldsymbol{x}) - p(\\boldsymbol{x})$. This leads to the control variate estimator:\n$$\n\\hat{I}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n (f(\\boldsymbol{Y}_i) - p(\\boldsymbol{Y}_i)) + \\mu_p\n$$\nThis is an unbiased estimator of $I$, just like $\\hat{I}_{\\mathrm{MC}}$. Its variance is $\\text{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{n}\\text{Var}(f(\\boldsymbol{X}) - p(\\boldsymbol{X}))$. If $p(\\boldsymbol{x})$ is strongly correlated with $f(\\boldsymbol{x})$, the variance of their difference will be small, leading to a more accurate estimator for the same sample size $n$.\n\nThe problem specifies using a total-degree polynomial of degree at most $k$ as the control variate $p(\\boldsymbol{x})$. Such a polynomial is a linear combination of monomial basis functions: $p(\\boldsymbol{x}) = \\sum_{|\\boldsymbol{\\alpha}| \\le k} c_{\\boldsymbol{\\alpha}} \\boldsymbol{x}^{\\boldsymbol{\\alpha}}$. This choice is motivated by the Weierstrass approximation theorem, which ensures that polynomials can approximate any continuous function on a compact domain like $[0,1]^d$. The coefficients $\\{c_{\\boldsymbol{\\alpha}}\\}$ are determined by a least-squares fit. We generate $m_{\\text{fit}}$ samples $\\{\\boldsymbol{Z}_j\\}_{j=1}^{m_{\\text{fit}}}$ and find the coefficients $\\boldsymbol{c}$ that minimize the sum of squared errors $\\sum_{j=1}^{m_{\\text{fit}}} (f(\\boldsymbol{Z}_j) - p(\\boldsymbol{Z}_j))^2$. This is a standard linear regression problem, formulated as minimizing $\\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{b}\\|_2^2$, where the design matrix $\\boldsymbol{A}$ has entries $A_{j, \\boldsymbol{\\alpha}} = \\boldsymbol{Z}_j^{\\boldsymbol{\\alpha}}$ and the target vector $\\boldsymbol{b}$ has entries $b_j = f(\\boldsymbol{Z}_j)$. This system is solved for $\\boldsymbol{c}$ using numerical linear algebra routines.\n\nOnce the coefficients $\\boldsymbol{c}$ are found, the exact integral of the polynomial, $\\mu_p$, is computed using the provided formula:\n$$\n\\mu_p = \\sum_{|\\boldsymbol{\\alpha}| \\le k} c_{\\boldsymbol{\\alpha}} \\left( \\int_{[0,1]^d} \\boldsymbol{x}^{\\boldsymbol{\\alpha}} \\, \\mathrm{d}\\boldsymbol{x} \\right) = \\sum_{|\\boldsymbol{\\alpha}| \\le k} c_{\\boldsymbol{\\alpha}} \\prod_{j=1}^d \\frac{1}{\\alpha_j + 1}\n$$\n\nThe solution procedure for each test case is as follows:\n1.  Initialize a pseudorandom number generator with the specified seed.\n2.  Define the integrand $f(\\boldsymbol{x})$, its dimension $d$, its exact integral $I$, the polynomial degree $k$, and the sample sizes $m_{\\text{fit}}$ and $n$.\n3.  Generate the set of all multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d$ such that $|\\boldsymbol{\\alpha}| \\le k$. These define the polynomial basis.\n4.  Generate $m_{\\text{fit}}$ random points $\\{\\boldsymbol{Z}_j\\}_{j=1}^{m_{\\text{fit}}}$ from $U([0,1]^d)$.\n5.  Construct the design matrix $\\boldsymbol{A}$ by evaluating each basis monomial at each point $\\boldsymbol{Z}_j$.\n6.  Construct the target vector $\\boldsymbol{b}$ by evaluating $f(\\boldsymbol{Z}_j)$.\n7.  Solve the least-squares problem $\\boldsymbol{A}\\boldsymbol{c} \\approx \\boldsymbol{b}$ to find the coefficient vector $\\boldsymbol{c}$.\n8.  Calculate $\\mu_p$, the exact integral of the resulting polynomial $p(\\boldsymbol{x})$.\n9.  Generate a second, independent set of $n$ random points $\\{\\boldsymbol{Y}_i\\}_{i=1}^n$ from $U([0,1]^d)$.\n10. Evaluate $f(\\boldsymbol{Y}_i)$ and $p(\\boldsymbol{Y}_i)$ for all points in this new set. The values of $p(\\boldsymbol{Y}_i)$ are found by building a design matrix for the points $\\boldsymbol{Y}$ and multiplying by the coefficient vector $\\boldsymbol{c}$.\n11. Compute the estimators $\\hat{I}_{\\mathrm{MC}}$ and $\\hat{I}_{\\mathrm{CV}}$.\n12. Calculate the absolute errors $|\\hat{I}_{\\mathrm{MC}} - I|$ and $|\\hat{I}_{\\mathrm{CV}} - I|$.\n13. Compute the ratio $r = \\frac{|\\hat{I}_{\\mathrm{CV}} - I|}{|\\hat{I}_{\\mathrm{MC}} - I|}$ and round it to $6$ decimal places. A ratio less than $1$ indicates an improvement. The case $k=0$ serves as a control where $p(\\boldsymbol{x})$ is a constant. In this specific setup, the control variate estimator $\\hat{I}_{\\mathrm{CV}}$ becomes mathematically identical to the plain Monte Carlo estimator $\\hat{I}_{\\mathrm{MC}}$. Therefore, their errors are identical, and the ratio $r$ will be exactly 1, which confirms the implementation's logic.\n\nThis systematic process is applied to each test case, leveraging numerical libraries for random number generation and solving the linear least-squares problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\nfrom typing import List, Tuple, Callable\n\ndef solve():\n    \"\"\"\n    Solves the Monte Carlo integration problem with control variates for all test cases.\n    \"\"\"\n\n    # Define functions and their exact integrals\n    def f1(x: np.ndarray) -> np.ndarray:\n        # x is assumed to be a column vector or a 2D array of shape (N, 1)\n        return np.exp(-x[:, 0]) * np.cos(7 * x[:, 0]) + x[:, 0]**3\n\n    # I1 = Re[ (e^(-1+7i) - 1) / (-1+7i) ] + 1/4\n    # Re part = (1 - e^-1*cos(7) + 7*e^-1*sin(7)) / 50\n    I1_re_part = (1 - np.exp(-1) * np.cos(7) + 7 * np.exp(-1) * np.sin(7)) / 50\n    I1 = I1_re_part + 0.25\n\n    def f2(x: np.ndarray) -> np.ndarray:\n        # x is assumed to be an array of shape (N, 2)\n        return np.sin(np.pi * x[:, 0]) * np.sin(np.pi * x[:, 1]) + x[:, 0] * x[:, 1]\n    \n    # I2 = (integral sin(pi*x) dx)^2 + (integral x dx)^2 = (2/pi)^2 + (1/2)^2\n    I2 = 4 / (np.pi**2) + 0.25\n\n    def generate_total_degree_exponents(d: int, k: int) -> List[Tuple[int, ...]]:\n        \"\"\"\n        Generates all multi-indices alpha for d variables with total degree <= k.\n        \"\"\"\n        if d == 1:\n            return [(i,) for i in range(k + 1)]\n        \n        exponents = []\n        for i in range(k + 1):\n            sub_exponents = generate_total_degree_exponents(d - 1, k - i)\n            for sub_exp in sub_exponents:\n                exponents.append((i,) + sub_exp)\n        return exponents\n\n    def evaluate_monomials(points: np.ndarray, exponents: List[Tuple[int, ...]]) -> np.ndarray:\n        \"\"\"\n        Evaluates a basis of monomials at a set of points.\n        points: (n_points, d)\n        exponents: list of alpha tuples\n        Returns: design matrix (n_points, n_exponents)\n        \"\"\"\n        n_points, d = points.shape\n        n_exponents = len(exponents)\n        design_matrix = np.zeros((n_points, n_exponents))\n        \n        for i, alpha in enumerate(exponents):\n            # np.prod(points**alpha, axis=1) is a concise way to compute x^alpha\n            design_matrix[:, i] = np.prod(np.power(points, alpha), axis=1)\n            \n        return design_matrix\n\n    test_cases = [\n        {'f': f1, 'I': I1, 'd': 1, 'k': 3, 'm_fit': 200, 'n': 20000, 'seed': 123456},\n        {'f': f2, 'I': I2, 'd': 2, 'k': 2, 'm_fit': 500, 'n': 30000, 'seed': 202311},\n        {'f': f1, 'I': I1, 'd': 1, 'k': 0, 'm_fit': 30, 'n': 5000, 'seed': 7777},\n        {'f': f2, 'I': I2, 'd': 2, 'k': 1, 'm_fit': 100, 'n': 20000, 'seed': 8888},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        f, I, d, k, m_fit, n, seed = \\\n            case['f'], case['I'], case['d'], case['k'], case['m_fit'], case['n'], case['seed']\n            \n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate basis and fit the polynomial control variate\n        exponents = generate_total_degree_exponents(d, k)\n        \n        # Generate samples for fitting\n        Z_points = rng.random((m_fit, d))\n        \n        # Build design matrix and target vector for least-squares\n        A_fit = evaluate_monomials(Z_points, exponents)\n        b_fit = f(Z_points)\n        \n        # Solve for polynomial coefficients\n        c_coeffs, _, _, _ = np.linalg.lstsq(A_fit, b_fit, rcond=None)\n\n        # Step 2: Calculate the exact integral of the polynomial\n        integral_of_monomials = np.array([1.0 / np.prod([exp + 1 for exp in alpha]) for alpha in exponents])\n        mu_p = np.dot(c_coeffs, integral_of_monomials)\n\n        # Step 3: Use an independent sample set for MC estimation\n        Y_points = rng.random((n, d))\n        \n        # Evaluate f and p at the new samples\n        f_Y = f(Y_points)\n        A_eval = evaluate_monomials(Y_points, exponents)\n        p_Y = A_eval @ c_coeffs\n\n        # Step 4: Compute estimators and errors\n        I_hat_mc = np.mean(f_Y)\n        I_hat_cv = np.mean(f_Y - p_Y) + mu_p\n        \n        error_mc = np.abs(I_hat_mc - I)\n        error_cv = np.abs(I_hat_cv - I)\n\n        if error_mc == 0:\n            # This edge case is highly unlikely but robust code should handle it.\n            # If plain MC is perfect, the ratio is either 0 (if CV is also perfect) or infinite.\n            # In the context of performance improvement, if the baseline is perfect, \n            # any non-zero error from the new method is a degradation. If CV is also perfect,\n            # it's no better or worse, ratio could be 1. Let's assume non-zero error for CV.\n            # Given the problem's nature, error_mc will not be zero.\n            ratio = float('inf') if error_cv > 0 else 1.0\n        else:\n            ratio = error_cv / error_mc\n        \n        results.append(round(ratio, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2414893"}]}