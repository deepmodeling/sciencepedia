{"hands_on_practices": [{"introduction": "Before we can harness stochastic differential equations to model complex systems, we must first master the art of simulating them reliably. This exercise delves into the most fundamental numerical algorithm, the Euler-Maruyama method, and uncovers a crucial concept: mean-square stability. By deriving the stability boundary, you will gain a hands-on appreciation for why simulating SDEs requires more care than their deterministic counterparts and learn how to ensure your simulations remain physically meaningful [@problem_id:2439998].", "problem": "Consider the scalar linear stochastic differential equation (SDE) in the Itô sense\n$$\ndX_t = \\lambda X_t \\, dt + \\mu X_t \\, dW_t,\\quad X_0 = x_0,\n$$\nwhere $W_t$ denotes a standard Wiener process (standard Brownian motion), and $\\lambda \\in \\mathbb{R}$, $\\mu \\in \\mathbb{R}$ are constants. The explicit Euler–Maruyama (EM) method for a uniform time step $h>0$ applied to this SDE yields the recursion\n$$\nX_{n+1} = X_n + \\lambda h X_n + \\mu X_n \\Delta W_n,\n$$\nwhere $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ are independent Gaussian random variables with distribution $\\mathcal{N}(0,h)$.\n\nDefine mean-square stability of the zero solution for the discrete-time method as follows: for a fixed step size $h>0$, the zero solution is mean-square stable if \n$$\n\\mathbb{E}\\big[|X_n|^2\\big] \\to 0 \\text{ as } n \\to \\infty\n$$\nfor any initial condition $X_0$ with finite second moment. The mean-square stability boundary $h^\\star(\\lambda,\\mu)$ is the supremum of all $h>0$ such that the Euler–Maruyama method is mean-square stable.\n\nTask: Starting from first principles and the definitions above, derive a closed-form expression for $h^\\star(\\lambda,\\mu)$ as a function of $\\lambda$ and $\\mu$, covering all cases, including when no positive step size yields mean-square stability. Then, write a program that evaluates $h^\\star(\\lambda,\\mu)$ for each parameter pair in the test suite specified below, rounds each result to $6$ decimal places, and outputs the list of these values.\n\nTest suite (each entry is a pair $(\\lambda,\\mu)$):\n- $(-2.0, 1.0)$\n- $(-1.0, 0.0)$\n- $(-0.5, 1.0)$\n- $(0.5, 0.1)$\n- $(-3.0, 2.0)$\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_k$ is the value of $h^\\star(\\lambda,\\mu)$ for the $k$-th test case, rounded to $6$ decimal places. No physical units are involved, and all numerical results must be reported as real numbers.", "solution": "The problem requires the derivation of the mean-square stability boundary, denoted $h^\\star(\\lambda, \\mu)$, for the explicit Euler-Maruyama method applied to a scalar linear stochastic differential equation. The derivation must proceed from first principles.\n\nThe SDE in question is\n$$\ndX_t = \\lambda X_t \\, dt + \\mu X_t \\, dW_t, \\quad X_0 = x_0,\n$$\nwhere $\\lambda \\in \\mathbb{R}$, $\\mu \\in \\mathbb{R}$ are constants and $W_t$ is a standard Wiener process.\n\nThe Euler-Maruyama (EM) method provides a discrete-time approximation. For a uniform time step $h > 0$, the recursion is given as\n$$\nX_{n+1} = X_n + \\lambda h X_n + \\mu X_n \\Delta W_n,\n$$\nwhere $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$. These increments are independent and identically distributed Gaussian random variables, $\\Delta W_n \\sim \\mathcal{N}(0, h)$. We can factor the expression for $X_{n+1}$ as\n$$\nX_{n+1} = (1 + \\lambda h + \\mu \\Delta W_n) X_n.\n$$\nMean-square stability of the zero solution is defined by the condition $\\mathbb{E}\\big[|X_n|^2\\big] \\to 0$ as $n \\to \\infty$. Let us define $M_n = \\mathbb{E}\\big[|X_n|^2\\big]$ and derive a recurrence relation for it.\n$$\nM_{n+1} = \\mathbb{E}\\big[|X_{n+1}|^2\\big] = \\mathbb{E}\\Big[\\big|(1 + \\lambda h + \\mu \\Delta W_n) X_n\\big|^2\\Big] = \\mathbb{E}\\Big[|1 + \\lambda h + \\mu \\Delta W_n|^2 |X_n|^2\\Big].\n$$\nThe value of $X_n$ is determined by the history of the Wiener process up to time $t_n$, so it is $\\mathcal{F}_{t_n}$-measurable. The increment $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is independent of the $\\sigma$-algebra $\\mathcal{F}_{t_n}$. Consequently, $X_n$ and $\\Delta W_n$ are independent. This allows us to separate the expectation:\n$$\nM_{n+1} = \\mathbb{E}\\Big[|1 + \\lambda h + \\mu \\Delta W_n|^2\\Big] \\cdot \\mathbb{E}\\big[|X_n|^2\\big] = R(\\lambda, \\mu, h) M_n,\n$$\nwhere $R(\\lambda, \\mu, h) = \\mathbb{E}\\big[|1 + \\lambda h + \\mu \\Delta W_n|^2\\big]$ is the mean-square amplification factor.\n\nTo find $R(\\lambda, \\mu, h)$, we expand the expression inside the expectation:\n$$\nR(\\lambda, \\mu, h) = \\mathbb{E}\\big[(1 + \\lambda h)^2 + 2(1 + \\lambda h)\\mu \\Delta W_n + \\mu^2 (\\Delta W_n)^2\\big].\n$$\nUsing the linearity of expectation, we get\n$$\nR(\\lambda, \\mu, h) = (1 + \\lambda h)^2 + 2(1 + \\lambda h)\\mu \\mathbb{E}[\\Delta W_n] + \\mu^2 \\mathbb{E}[(\\Delta W_n)^2].\n$$\nThe properties of the Wiener increment are $\\mathbb{E}[\\Delta W_n] = 0$ and $\\mathbb{E}[(\\Delta W_n)^2] = \\text{Var}(\\Delta W_n) = h$. Substituting these values into the expression for $R$ yields\n$$\nR(\\lambda, \\mu, h) = (1 + \\lambda h)^2 + 2(1 + \\lambda h)\\mu \\cdot 0 + \\mu^2 h = (1 + \\lambda h)^2 + \\mu^2 h.\n$$\nExpanding this, we have\n$$\nR(\\lambda, \\mu, h) = 1 + 2\\lambda h + \\lambda^2 h^2 + \\mu^2 h.\n$$\nThe recurrence relation $M_{n+1} = R(\\lambda, \\mu, h) M_n$ describes a geometric progression for the mean-square value. For $M_n \\to 0$ as $n \\to \\infty$, it is necessary and sufficient that the amplification factor satisfies $|R(\\lambda, \\mu, h)| < 1$.\nSince $h > 0$ and $\\mu^2 \\ge 0$, we have $R(\\lambda, \\mu, h) = (1+\\lambda h)^2 + \\mu^2 h \\ge 0$. Therefore, the stability condition simplifies to $0 \\le R(\\lambda, \\mu, h) < 1$. This is equivalent to\n$$\n1 + 2\\lambda h + \\lambda^2 h^2 + \\mu^2 h < 1.\n$$\nSubtracting $1$ from both sides gives\n$$\n2\\lambda h + \\lambda^2 h^2 + \\mu^2 h < 0.\n$$\nSince we seek a solution for $h > 0$, we can divide the inequality by $h$:\n$$\n2\\lambda + \\mu^2 + \\lambda^2 h < 0.\n$$\nWe must now solve this inequality for $h$.\n\nCase 1: $\\lambda = 0$.\nThe inequality becomes $2(0) + \\mu^2 + (0)^2 h < 0$, which simplifies to $\\mu^2 < 0$. This inequality has no solution for any real $\\mu$. Therefore, if $\\lambda = 0$, there is no step size $h > 0$ for which the EM method is mean-square stable. The set of stable step sizes is the empty set, and its supremum is $h^\\star(0, \\mu) = 0$.\n\nCase 2: $\\lambda \\ne 0$.\nWe rearrange the inequality to isolate $h$:\n$$\n\\lambda^2 h < -(2\\lambda + \\mu^2).\n$$\nSince $\\lambda \\ne 0$, $\\lambda^2 > 0$. Dividing by $\\lambda^2$ preserves the inequality direction:\n$$\nh < -\\frac{2\\lambda + \\mu^2}{\\lambda^2}.\n$$\nFor there to be any positive $h$ satisfying this condition, the right-hand side must be positive. This requires\n$$\n-\\frac{2\\lambda + \\mu^2}{\\lambda^2} > 0 \\implies -(2\\lambda + \\mu^2) > 0 \\implies 2\\lambda + \\mu^2 < 0.\n$$\nIf $2\\lambda + \\mu^2 < 0$, then any step size $h$ in the interval $\\left(0, -\\frac{2\\lambda + \\mu^2}{\\lambda^2}\\right)$ will lead to a mean-square stable scheme. The stability boundary $h^\\star(\\lambda, \\mu)$ is the supremum of this set of stable step sizes, which is the upper bound of the interval:\n$$\nh^\\star(\\lambda, \\mu) = -\\frac{2\\lambda + \\mu^2}{\\lambda^2}.\n$$\nIf, on the other hand, $2\\lambda + \\mu^2 \\ge 0$, then the right-hand side of the inequality for $h$ is less than or equal to zero. No positive $h$ can satisfy the condition. The set of stable step sizes is empty, and its supremum is $h^\\star(\\lambda, \\mu) = 0$.\n\nCombining these results, we derive the final closed-form expression for the mean-square stability boundary:\n$$\nh^\\star(\\lambda, \\mu) =\n\\begin{cases}\n    -\\frac{2\\lambda + \\mu^2}{\\lambda^2} & \\text{if } \\lambda \\neq 0 \\text{ and } 2\\lambda + \\mu^2 < 0 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nThis expression is implemented in the provided program to evaluate $h^\\star(\\lambda, \\mu)$ for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the mean-square stability boundary h_star for the Euler-Maruyama\n    method applied to a scalar linear SDE for a given set of parameters.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each entry is a tuple (lambda, mu).\n    test_cases = [\n        (-2.0, 1.0),\n        (-1.0, 0.0),\n        (-0.5, 1.0),\n        (0.5, 0.1),\n        (-3.0, 2.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_val, mu_val = case\n        \n        # The stability boundary h_star is derived from the condition\n        # for mean-square stability of the Euler-Maruyama method:\n        # h_star = sup{h > 0 | (1 + lambda*h)^2 + mu^2*h < 1}.\n        # This simplifies to h < -(2*lambda + mu^2) / lambda^2,\n        # which requires 2*lambda + mu^2 < 0 for a positive solution h to exist.\n\n        # The closed-form expression for h_star is:\n        # h_star = - (2*lambda + mu^2) / lambda^2   if 2*lambda + mu^2 < 0 and lambda != 0\n        # h_star = 0                                otherwise\n\n        h_star = 0.0\n        \n        # Check if lambda is non-zero to avoid division by zero.\n        if lambda_val != 0.0:\n            stability_condition = 2 * lambda_val + mu_val**2\n            \n            # If the condition is met, a positive stability boundary exists.\n            if stability_condition < 0.0:\n                h_star = -stability_condition / (lambda_val**2)\n\n        # Round the result to 6 decimal places as required.\n        # Using f-string for formatting ensures the trailing zeros are kept.\n        rounded_result = f\"{h_star:.6f}\"\n        results.append(rounded_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2439998"}, {"introduction": "A theoretical model becomes powerful only when it can be connected to real-world data. This practice bridges that gap, guiding you through the process of parameter estimation for the widely used Geometric Brownian Motion (GBM) model. Starting from first principles with Itô's Lemma, you will derive the Maximum Likelihood Estimators (MLE) for the drift and volatility, transforming a complex SDE problem into a tractable statistical one and learning a core skill in computational finance and engineering [@problem_id:2440008].", "problem": "Consider a scalar asset price modeled as a geometric Brownian motion (GBM), that is, it satisfies the stochastic differential equation (SDE) $dS_t = \\mu S_t \\, dt + \\sigma S_t \\, dW_t$, where $S_t$ is the price at time $t$, $\\mu$ is the drift, $\\sigma$ is the volatility, and $W_t$ is a standard Brownian motion (BM). Starting from the SDE and using only the fundamental definitions from Itô calculus (in particular, Itô's lemma applied to the logarithm), derive a statistically principled estimator based on maximum likelihood estimation (MLE) for the unknown parameters $\\mu$ and $\\sigma$ from a discrete time series of observations $S_0,S_1,\\dots,S_n$ sampled at uniform time step $\\Delta t$.\n\nYour derivation must begin from the SDE definition, use Itô's lemma to obtain the dynamics of $\\log S_t$, and then proceed to a likelihood-based argument for independent and identically distributed (IID) increments to produce explicit estimators for $\\mu$ and $\\sigma$ in terms of the observed series $S_0,\\dots,S_n$ and the sampling interval $\\Delta t$. Do not invoke any shortcut formulas beyond those obtained directly from these principles.\n\nThen, implement these estimators in a complete, runnable program that, for each test case below, reconstructs the observed price series from a given initial value and a prescribed sequence of log-returns, and computes the MLE estimates of $\\mu$ and $\\sigma$ using only the reconstructed price series (not the given returns). For numerical reproducibility across implementations, round every reported $\\mu$ and $\\sigma$ to six decimal places.\n\nTest suite specification:\n- Each test case provides an initial price $S_0$, a time step $\\Delta t$, and a list of log-returns $Y_1,\\dots,Y_n$. The observed prices are defined recursively by $S_k = S_{k-1} \\exp(Y_k)$ for $k=1,\\dots,n$. Your program must first reconstruct the price series from these definitions, then compute the MLEs from the reconstructed series.\n- Test cases:\n  1. General case with moderate variability:\n     - $S_0 = 100$, $\\Delta t = 0.1$, $[Y_1,\\dots,Y_{10}] = [0.02,-0.05,0.03,0.1,-0.08,0.04,0.0,-0.02,0.06,-0.03]$.\n  2. Boundary case with a single increment and zero variability:\n     - $S_0 = 50$, $\\Delta t = 0.5$, $[Y_1] = [0.0]$.\n  3. Negative drift with small variability:\n     - $S_0 = 80$, $\\Delta t = 0.25$, $[Y_1,\\dots,Y_8] = [-0.1,0.02,-0.03,-0.04,0.01,-0.019,-0.05,0.03]$.\n  4. High variability with mixed signs:\n     - $S_0 = 10$, $\\Delta t = 0.5$, $[Y_1,\\dots,Y_5] = [0.15,0.0,-0.2,0.25,-0.1]$.\n\nOutput specification:\n- For each test case, compute the MLEs $\\hat{\\mu}$ and $\\hat{\\sigma}$ from the reconstructed series.\n- Aggregate all results in a single flat list in the order $[\\hat{\\mu}_1,\\hat{\\sigma}_1,\\hat{\\mu}_2,\\hat{\\sigma}_2,\\hat{\\mu}_3,\\hat{\\sigma}_3,\\hat{\\mu}_4,\\hat{\\sigma}_4]$, where the subscript indexes the test case.\n- Round each number to six decimal places before output.\n- Your program should produce a single line of output containing the rounded results as a comma-separated list enclosed in square brackets, for example, $[\\hat{\\mu}_1,\\hat{\\sigma}_1,\\dots]$, with each entry formatted to six decimal places.\n\nAll answers are real numbers without physical units, and angles are not involved. Your final outputs must be floats formatted to six decimal places. The program must be self-contained and use no input.", "solution": "The problem statement is evaluated and found to be valid. It is a well-posed problem in stochastic calculus and statistical inference, grounded in established financial engineering principles. The data are complete, and the objectives are clear. We proceed with the derivation and solution.\n\nThe asset price $S_t$ is modeled by the stochastic differential equation (SDE) for a geometric Brownian motion (GBM):\n$$ dS_t = \\mu S_t \\, dt + \\sigma S_t \\, dW_t $$\nwhere $\\mu$ is the drift parameter, $\\sigma$ is the volatility parameter, and $W_t$ is a standard Wiener process or Brownian motion. Our objective is to derive the maximum likelihood estimators (MLE) for $\\mu$ and $\\sigma$ given a discrete time series of observations $S_0, S_1, \\dots, S_n$ sampled at a uniform time interval $\\Delta t$.\n\nThe derivation begins by applying Itô's lemma to the function $f(S_t) = \\log S_t$. Let $X_t = \\log S_t$. The partial derivatives of $f(S_t)$ with respect to $t$ and $S_t$ are:\n$$ \\frac{\\partial f}{\\partial t} = 0, \\quad \\frac{\\partial f}{\\partial S_t} = \\frac{1}{S_t}, \\quad \\frac{\\partial^2 f}{\\partial S_t^2} = -\\frac{1}{S_t^2} $$\nAccording to Itô's lemma, the differential $dX_t$ is given by:\n$$ dX_t = \\left( \\frac{\\partial f}{\\partial t} + \\mu S_t \\frac{\\partial f}{\\partial S_t} + \\frac{1}{2} \\sigma^2 S_t^2 \\frac{\\partial^2 f}{\\partial S_t^2} \\right) dt + \\left( \\sigma S_t \\frac{\\partial f}{\\partial S_t} \\right) dW_t $$\nSubstituting the partial derivatives into this expression yields:\n$$ dX_t = \\left( 0 + \\mu S_t \\left(\\frac{1}{S_t}\\right) + \\frac{1}{2} \\sigma^2 S_t^2 \\left(-\\frac{1}{S_t^2}\\right) \\right) dt + \\left( \\sigma S_t \\left(\\frac{1}{S_t}\\right) \\right) dW_t $$\nSimplifying the terms, we obtain the SDE for the log-price process $X_t$:\n$$ dX_t = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\sigma dW_t $$\nThis is an arithmetic Brownian motion with a constant drift $\\mu - \\frac{1}{2}\\sigma^2$ and constant volatility $\\sigma$.\n\nTo analyze the discrete observations, we integrate this SDE over a time interval $[t_{k-1}, t_k]$, where $t_k = k \\Delta t$. The change in log-price over this interval is:\n$$ X_{t_k} - X_{t_{k-1}} = \\int_{t_{k-1}}^{t_k} \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\int_{t_{k-1}}^{t_k} \\sigma dW_t $$\nLet $Y_k = X_{t_k} - X_{t_{k-1}} = \\log(S_{t_k}) - \\log(S_{t_{k-1}})$ be the log-return over the $k$-th interval. The first integral is deterministic, while the second is an Itô integral.\n$$ Y_k = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) (t_k - t_{k-1}) + \\sigma (W_{t_k} - W_{t_{k-1}}) $$\nSince $t_k - t_{k-1} = \\Delta t$, this becomes:\n$$ Y_k = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) \\Delta t + \\sigma (W_{t_k} - W_{t_{k-1}}) $$\nThe increments of a standard Brownian motion, $W_{t_k} - W_{t_{k-1}}$, are independent and normally distributed with mean $0$ and variance $\\Delta t$. That is, $W_{t_k} - W_{t_{k-1}} \\sim \\mathcal{N}(0, \\Delta t)$.\nConsequently, the log-returns $Y_k$ for $k=1, \\dots, n$ are independent and identically distributed (IID) random variables from a normal distribution:\n$$ Y_k \\sim \\mathcal{N}\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) \\Delta t, \\sigma^2 \\Delta t \\right) $$\nLet us define new parameters for this normal distribution: the mean $\\nu = (\\mu - \\frac{1}{2}\\sigma^2) \\Delta t$ and the variance $\\tau^2 = \\sigma^2 \\Delta t$. Thus, $Y_k \\sim \\mathcal{N}(\\nu, \\tau^2)$.\n\nWe now derive the MLEs for $\\nu$ and $\\tau^2$ based on the $n$ observations $Y_1, \\dots, Y_n$. The likelihood function is the product of the probability density functions for each observation:\n$$ L(\\nu, \\tau^2; Y_1, \\dots, Y_n) = \\prod_{k=1}^n \\frac{1}{\\sqrt{2\\pi \\tau^2}} \\exp\\left( -\\frac{(Y_k - \\nu)^2}{2\\tau^2} \\right) $$\nIt is more convenient to work with the log-likelihood function, $\\mathcal{L} = \\log L$:\n$$ \\mathcal{L}(\\nu, \\tau^2) = \\sum_{k=1}^n \\left( -\\frac{1}{2} \\log(2\\pi) - \\frac{1}{2} \\log(\\tau^2) - \\frac{(Y_k - \\nu)^2}{2\\tau^2} \\right) $$\n$$ \\mathcal{L}(\\nu, \\tau^2) = -\\frac{n}{2} \\log(2\\pi\\tau^2) - \\frac{1}{2\\tau^2} \\sum_{k=1}^n (Y_k - \\nu)^2 $$\nTo find the estimators that maximize this function, we compute the partial derivatives with respect to $\\nu$ and $\\tau^2$ and set them to zero.\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\nu} = \\frac{1}{\\tau^2} \\sum_{k=1}^n (Y_k - \\nu) = 0 \\implies \\sum_{k=1}^n Y_k - n\\nu = 0 $$\nThis yields the MLE for $\\nu$:\n$$ \\hat{\\nu} = \\frac{1}{n} \\sum_{k=1}^n Y_k = \\bar{Y} $$\nThe MLE for the mean of a normal distribution is its sample mean.\nNext, we differentiate with respect to $\\tau^2$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial (\\tau^2)} = -\\frac{n}{2\\tau^2} + \\frac{1}{2(\\tau^2)^2} \\sum_{k=1}^n (Y_k - \\nu)^2 = 0 $$\nSubstituting $\\hat{\\nu}$ for $\\nu$ and solving for $\\tau^2$:\n$$ \\frac{n}{2\\hat{\\tau}^2} = \\frac{1}{2(\\hat{\\tau}^2)^2} \\sum_{k=1}^n (Y_k - \\hat{\\nu})^2 \\implies \\hat{\\tau}^2 = \\frac{1}{n} \\sum_{k=1}^n (Y_k - \\bar{Y})^2 $$\nThis is the sample variance of the log-returns, which we denote as $s_Y^2$.\n\nFinally, we use the invariance property of maximum likelihood estimators to find the estimators for our original parameters, $\\mu$ and $\\sigma$. The relationships are:\n$$ \\tau^2 = \\sigma^2 \\Delta t \\quad \\text{and} \\quad \\nu = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) \\Delta t $$\nFrom the first relation, we estimate $\\sigma^2$:\n$$ \\hat{\\sigma}^2 = \\frac{\\hat{\\tau}^2}{\\Delta t} = \\frac{s_Y^2}{\\Delta t} = \\frac{1}{n \\Delta t} \\sum_{k=1}^n (Y_k - \\bar{Y})^2 $$\nThe estimator for the volatility $\\sigma$ is therefore:\n$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n \\Delta t} \\sum_{k=1}^n (Y_k - \\bar{Y})^2} $$\nFrom the second relation, we solve for $\\mu$:\n$$ \\mu = \\frac{\\nu}{\\Delta t} + \\frac{1}{2}\\sigma^2 $$\nSubstituting the estimators $\\hat{\\nu}$ and $\\hat{\\sigma}^2$, we obtain the estimator for the drift $\\mu$:\n$$ \\hat{\\mu} = \\frac{\\hat{\\nu}}{\\Delta t} + \\frac{1}{2}\\hat{\\sigma}^2 = \\frac{\\bar{Y}}{\\Delta t} + \\frac{1}{2}\\hat{\\sigma}^2 $$\nThese are the explicit estimators for $\\mu$ and $\\sigma$ derived from first principles as required. The implementation will compute these quantities from the reconstructed price series.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (S0, delta_t, list_of_log_returns)\n    test_cases = [\n        (100.0, 0.1, [0.02, -0.05, 0.03, 0.1, -0.08, 0.04, 0.0, -0.02, 0.06, -0.03]),\n        (50.0, 0.5, [0.0]),\n        (80.0, 0.25, [-0.1, 0.02, -0.03, -0.04, 0.01, -0.019, -0.05, 0.03]),\n        (10.0, 0.5, [0.15, 0.0, -0.2, 0.25, -0.1])\n    ]\n\n    all_results = []\n    \n    for s0, delta_t, yk_list in test_cases:\n        # Reconstruct the price series S_k from S_0 and log-returns Y_k\n        # S_k = S_{k-1} * exp(Y_k)\n        price_series = [s0]\n        current_price = s0\n        for yk in yk_list:\n            current_price *= np.exp(yk)\n            price_series.append(current_price)\n        \n        # As per problem, compute estimators using ONLY the reconstructed price series.\n        # This means we must re-calculate log-returns from the price series.\n        price_series_np = np.array(price_series)\n        \n        # n is the number of returns, which is number of prices - 1.\n        n = len(price_series_np) - 1\n        \n        if n > 0:\n            # Calculate log-returns Yk = log(S_k) - log(S_{k-1})\n            log_returns = np.log(price_series_np[1:]) - np.log(price_series_np[:-1])\n            \n            # Compute sample mean of log-returns\n            y_mean = np.mean(log_returns)\n            \n            # Compute sample variance of log-returns (MLE version, ddof=0 is default in np.var)\n            y_var_mle = np.var(log_returns)\n            \n            # Estimate sigma^2 using the derived formula\n            sigma_sq_hat = y_var_mle / delta_t\n            \n            # Estimate sigma\n            sigma_hat = np.sqrt(sigma_sq_hat)\n            \n            # Estimate mu using the derived formula\n            mu_hat = (y_mean / delta_t) + 0.5 * sigma_sq_hat\n        else:\n            # If n=0 (only S0 given), estimators are undefined from returns.\n            # Handle this as an edge case, though not expected by the problem setup.\n            # The narrowest case in test suite is n=1.\n            mu_hat = 0.0\n            sigma_hat = 0.0\n\n        all_results.append(f\"{mu_hat:.6f}\")\n        all_results.append(f\"{sigma_hat:.6f}\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "2440008"}, {"introduction": "Having learned to simulate SDEs and calibrate them with data, we now advance to a core engineering task: optimization and control under uncertainty. This problem challenges you to design an optimal harvesting strategy for a population modeled by a stochastic logistic equation, maximizing its long-term yield. By leveraging the Fokker-Planck equation to find the system's stationary behavior, you will see how analytical insights from Itô calculus can directly inform a practical and robust optimal control policy [@problem_id:2439927].", "problem": "You are given a stochastic population model for a single fish species under per-capita harvesting. The biomass process $\\{X_t\\}_{t \\ge 0}$ is modeled by the stochastic differential equation\n$$\ndX_t \\;=\\; \\Big(r\\,X_t\\big(1 - \\tfrac{X_t}{K}\\big) \\;-\\; u\\,X_t\\Big)\\,dt \\;+\\; \\sigma\\,X_t\\,dW_t,\\quad X_0>0,\n$$\nwhere $r>0$ is the intrinsic growth rate, $K>0$ is the carrying capacity, $\\sigma \\ge 0$ is the environmental noise intensity, $u \\ge 0$ is a constant per-capita harvesting rate, and $\\{W_t\\}_{t \\ge 0}$ is a standard Wiener process (Brownian motion). The long-run performance criterion is the ergodic average yield\n$$\nJ(u) \\;=\\; \\liminf_{T\\to\\infty}\\,\\frac{1}{T}\\,\\mathbb{E}\\!\\left[\\int_0^T u\\,X_t\\,dt\\right].\n$$\nWithin the class of constant harvesting rates $u \\in [0,u_{\\max}]$, design an optimal constant harvesting strategy that maximizes $J(u)$, and compute the corresponding stationary mean biomass and yield for given parameter sets.\n\nStart from fundamental definitions of Itô diffusion and the associated Fokker–Planck (forward Kolmogorov) equation. Carefully justify any assumptions needed for the existence of a stationary density and the ergodic average. Do not assume the target formulas; instead, derive the stationary density shape from the zero-flux stationary Fokker–Planck equation and then obtain the stationary mean biomass. Use these to express $J(u)$ purely in terms of $(r,K,\\sigma,u)$, and then maximize with respect to $u$ subject to the admissible bounds.\n\nYour program must implement the derived formula for the optimal constant harvesting rate $u^\\star$ and report, for each test case, the triplet\n$$\n\\big[u^\\star,\\ \\mathbb{E}[X]_{\\mathrm{stat}}(u^\\star),\\ J(u^\\star)\\big],\n$$\nas floating-point numbers rounded to six decimal places.\n\nAssume all parameters are given as dimensionless positive real numbers with $r>0$, $K>0$, $u_{\\max}\\ge 0$, and $\\sigma\\ge 0$. If conditions to ensure a normalizable stationary distribution are not met, your control should correctly default to the best admissible choice implied by your derivation, and you must return consistent values for $\\mathbb{E}[X]_{\\mathrm{stat}}$ and $J$ under that limiting policy.\n\nTest Suite:\n- Case A (interior optimum): $(r,K,\\sigma,u_{\\max})=(1.0,1000.0,0.4,10.0)$.\n- Case B (control bound active): $(r,K,\\sigma,u_{\\max})=(1.0,1000.0,0.4,0.3)$.\n- Case C (no positive feasible stationary harvest): $(r,K,\\sigma,u_{\\max})=(0.5,800.0,1.2,10.0)$.\n- Case D (small positive effective growth, bound active): $(r,K,\\sigma,u_{\\max})=(0.6,750.0,1.0,0.03)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the test cases, in order A, B, C, D, as a list of lists with each inner list formatted as\n$$\n[u^\\star,\\ \\mathbb{E}[X]_{\\mathrm{stat}}(u^\\star),\\ J(u^\\star)],\n$$\nrounded to six decimal places, and the outer list enclosing the four inner lists. For example, the output must look like\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]\n$$\nwith no extra whitespace or text.", "solution": "The problem statement is found to be valid upon critical inspection. It is scientifically grounded in the established theory of stochastic differential equations and their application to population dynamics, it is well-posed, and all provided information is self-contained and consistent. We may therefore proceed with a formal derivation of the solution.\n\nThe core of the problem is to determine the optimal constant harvesting rate $u^\\star$ that maximizes the long-run average yield from a fish population whose biomass $X_t$ is governed by the stochastic differential equation (SDE):\n$$\ndX_t \\;=\\; \\Big(r\\,X_t\\big(1 - \\tfrac{X_t}{K}\\big) \\;-\\; u\\,X_t\\Big)\\,dt \\;+\\; \\sigma\\,X_t\\,dW_t\n$$\nThis is an Itô diffusion process of the form $dX_t = a(X_t)dt + b(X_t)dW_t$, with the state space for biomass being $x>0$. The drift and diffusion coefficients are identified by rearranging the equation:\n$$\na(x) = (r-u)x - \\frac{r}{K}x^2\n$$\n$$\nb(x) = \\sigma x\n$$\nThe evolution of the probability density function $p(x,t)$ of the process $X_t$ is described by the Fokker-Planck (or forward Kolmogorov) equation:\n$$\n\\frac{\\partial p(x,t)}{\\partial t} = -\\frac{\\partial}{\\partial x} \\big[ a(x)p(x,t) \\big] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\big[ b(x)^2 p(x,t) \\big]\n$$\nWe seek a stationary solution, $p_s(x)$, corresponding to the long-run behavior of the system, by setting $\\frac{\\partial p}{\\partial t} = 0$. This implies that the probability flux, $S(x)$, must be constant:\n$$\nS(x) = a(x)p_s(x) - \\frac{1}{2}\\frac{d}{dx}\\big[ b(x)^2 p_s(x) \\big] = \\text{constant}\n$$\nFor a stationary distribution to exist on the domain $(0, \\infty)$, there must be no net flow of probability mass across the boundaries. The boundary at $x=0$ is an exit boundary if the population can go extinct, and the boundary at $x=\\infty$ must not have probability escaping to it. A non-trivial stationary state corresponds to a zero-flux condition, $S(x) = 0$ for all $x>0$. This gives a first-order ordinary differential equation for $p_s(x)$:\n$$\na(x)p_s(x) = \\frac{1}{2}\\frac{d}{dx}\\big[ b(x)^2 p_s(x) \\big]\n$$\nSeparating variables and integrating yields the general form of the stationary density:\n$$\np_s(x) = \\frac{N}{b(x)^2} \\exp\\left( \\int \\frac{2a(x)}{b(x)^2} dx \\right)\n$$\nwhere $N$ is a normalization constant. We now substitute our specific drift and diffusion coefficients:\n$$\n\\frac{2a(x)}{b(x)^2} = \\frac{2\\left((r-u)x - \\frac{r}{K}x^2\\right)}{(\\sigma x)^2} = \\frac{2(r-u)}{\\sigma^2 x} - \\frac{2r}{\\sigma^2 K}\n$$\nThe integral is:\n$$\n\\int \\left( \\frac{2(r-u)}{\\sigma^2 x} - \\frac{2r}{\\sigma^2 K} \\right) dx = \\frac{2(r-u)}{\\sigma^2}\\ln x - \\frac{2r}{\\sigma^2 K} x\n$$\nSubstituting this back into the expression for $p_s(x)$:\n$$\np_s(x) = \\frac{N}{(\\sigma x)^2} \\exp\\left( \\frac{2(r-u)}{\\sigma^2}\\ln x - \\frac{2r}{\\sigma^2 K} x \\right) \\propto x^{-2} x^{\\frac{2(r-u)}{\\sigma^2}} \\exp\\left( -\\frac{2r}{\\sigma^2 K} x \\right)\n$$\n$$\np_s(x) = N' x^{\\left(\\frac{2(r-u)}{\\sigma^2}-2\\right)} \\exp\\left( -\\left(\\frac{2r}{\\sigma^2 K}\\right) x \\right)\n$$\nThis is the functional form of a Gamma distribution, $p(x;\\alpha,\\beta) \\propto x^{\\alpha-1}e^{-\\beta x}$. By comparison, we identify the shape parameter $\\alpha$ and the rate parameter $\\beta$:\n$$\n\\alpha - 1 = \\frac{2(r-u)}{\\sigma^2} - 2 \\implies \\alpha = \\frac{2(r-u)}{\\sigma^2} - 1\n$$\n$$\n\\beta = \\frac{2r}{\\sigma^2 K}\n$$\nFor this density to be normalizable, i.e., for the integral $\\int_0^\\infty p_s(x) dx$ to be finite, we require $\\alpha > 0$. This imposes a critical condition on the harvesting rate $u$:\n$$\n\\frac{2(r-u)}{\\sigma^2} - 1 > 0 \\implies 2(r-u) > \\sigma^2 \\implies u < r - \\frac{\\sigma^2}{2}\n$$\nIf this condition is not met, i.e., $u \\ge r - \\frac{\\sigma^2}{2}$, the integral for normalization diverges, implying that the probability mass accumulates at $x=0$. The population goes to extinction almost surely. In this case, the stationary state is trivial, with $\\mathbb{E}[X]_{\\mathrm{stat}} = 0$ and consequently the long-run average yield $J(u) = 0$.\n\nIf the survival condition $u < r - \\sigma^2/2$ holds, a non-trivial stationary distribution exists and the process is ergodic. Ergodicity implies that the long-run time average of any function of the state converges to the expectation of that function with respect to the stationary distribution. The performance criterion is thus given by:\n$$\nJ(u) = u \\, \\mathbb{E}[X]_{\\mathrm{stat}}(u)\n$$\nThe mean of a Gamma distribution is $\\mathbb{E}[X] = \\alpha/\\beta$. Thus, the stationary mean biomass is:\n$$\n\\mathbb{E}[X]_{\\mathrm{stat}}(u) = \\frac{\\alpha}{\\beta} = \\frac{\\frac{2(r-u)}{\\sigma^2} - 1}{\\frac{2r}{\\sigma^2 K}} = \\frac{K}{2r}\\left(2(r-u)-\\sigma^2\\right) = K\\left(1 - \\frac{u}{r} - \\frac{\\sigma^2}{2r}\\right)\n$$\nThe long-run yield is then:\n$$\nJ(u) = u K \\left(1 - \\frac{u}{r} - \\frac{\\sigma^2}{2r}\\right)\n$$\nThis expression is valid for $u < r - \\sigma^2/2$. Let us define the effective intrinsic growth rate $r_{eff} = r - \\sigma^2/2$.\n\nThe optimization problem is to maximize $J(u)$ for $u \\in [0, u_{\\max}]$.\nFirst, consider the case where $r_{eff} \\le 0$, or $r \\le \\sigma^2/2$. In this scenario, for any non-negative harvesting rate $u \\ge 0$, the condition $u < r_{eff}$ can never be met. Therefore, for all $u \\ge 0$, the population goes extinct, and $J(u) = 0$. The maximum possible yield is $0$, which is achieved at $u^\\star = 0$.\n\nNext, consider the case where $r_{eff} > 0$. The yield function $J(u)$ is a downward-opening parabola in $u$ on the interval $[0, r_{eff})$. To find the unconstrained maximum, we differentiate $J(u)$ with respect to $u$ and set the result to zero:\n$$\n\\frac{dJ}{du} = K\\left(1 - \\frac{u}{r} - \\frac{\\sigma^2}{2r}\\right) + uK\\left(-\\frac{1}{r}\\right) = K\\left(1 - \\frac{2u}{r} - \\frac{\\sigma^2}{2r}\\right) = 0\n$$\n$$\n1 - \\frac{\\sigma^2}{2r} = \\frac{2u}{r} \\implies u = \\frac{r}{2}\\left(1 - \\frac{\\sigma^2}{2r}\\right) = \\frac{r}{2} - \\frac{\\sigma^2}{4}\n$$\nLet this candidate optimum be $u_{cand} = \\frac{r}{2} - \\frac{\\sigma^2}{4}$. Since $r_{eff}>0$ implies $r>\\sigma^2/2$, we have $u_{cand} > 0$. Also, $u_{cand} < r - \\sigma^2/2 = r_{eff}$, so this maximum lies within the valid domain of the parabolic yield function.\nThe final optimal harvesting rate $u^\\star$ must also respect the constraint $u \\in [0, u_{\\max}]$. Since $J(u)$ is increasing for $u \\in [0, u_{cand}]$, the optimal constrained control is:\n$$\nu^\\star = \\min(u_{cand}, u_{\\max}) = \\min\\left(\\frac{r}{2} - \\frac{\\sigma^2}{4}, u_{\\max}\\right)\n$$\nBased on this derivation, the complete algorithm is as follows:\n1. Given $(r, K, \\sigma, u_{\\max})$, calculate the survival threshold $r_{eff} = r - \\sigma^2/2$.\n2. If $r_{eff} \\le 0$: The system is not sustainable. The optimal strategy is no harvesting. Set $u^\\star=0$, $\\mathbb{E}[X]_{\\mathrm{stat}}(u^\\star)=0$, and $J(u^\\star)=0$.\n3. If $r_{eff} > 0$: The system can sustain harvesting.\n    a. Calculate the unconstrained optimal rate $u_{cand} = r/2 - \\sigma^2/4$.\n    b. The optimal constrained rate is $u^\\star = \\min(u_{cand}, u_{\\max})$.\n    c. The corresponding stationary mean biomass is $\\mathbb{E}[X]_{\\mathrm{stat}}(u^\\star) = K(1 - u^\\star/r - \\sigma^2/(2r))$.\n    d. The maximal yield is $J(u^\\star) = u^\\star \\cdot \\mathbb{E}[X]_{\\mathrm{stat}}(u^\\star)$.\n\nThis algorithm is implemented to solve the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the optimal constant harvesting strategy for a stochastic\n    population model and computes the corresponding stationary mean biomass and yield.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (r, K, sigma, u_max)\n    test_cases = [\n        (1.0, 1000.0, 0.4, 10.0),  # Case A\n        (1.0, 1000.0, 0.4, 0.3),   # Case B\n        (0.5, 800.0, 1.2, 10.0),   # Case C\n        (0.6, 750.0, 1.0, 0.03)    # Case D\n    ]\n\n    results = []\n    for r, K, sigma, u_max in test_cases:\n        sigma_sq = sigma**2\n        \n        # Calculate the effective growth rate condition for population survival.\n        # If r_eff_survival <= 0, the population goes extinct even with no harvesting.\n        r_eff_survival = r - sigma_sq / 2.0\n        \n        if r_eff_survival <= 0:\n            # No sustainable positive yield is possible.\n            # Optimal strategy is to not harvest.\n            u_star = 0.0\n            E_X_stat = 0.0\n            J_u_star = 0.0\n        else:\n            # A sustainable population is possible.\n            # Calculate the candidate for the optimal harvesting rate, which maximizes the yield parabola.\n            u_cand = r / 2.0 - sigma_sq / 4.0\n            \n            # The optimal harvesting rate is the candidate rate, capped by the maximum allowed rate.\n            u_star = np.minimum(u_cand, u_max)\n            \n            # Calculate the stationary mean biomass at the optimal harvesting rate.\n            E_X_stat = K * (1.0 - u_star / r - sigma_sq / (2.0 * r))\n            if E_X_stat < 0: # Should not happen with correct logic, but as safeguard\n                E_X_stat = 0.0\n\n            # Calculate the corresponding optimal long-run average yield.\n            J_u_star = u_star * E_X_stat\n\n        # Store the triplet of results, rounded to six decimal places.\n        results.append([\n            round(u_star, 6),\n            round(E_X_stat, 6),\n            round(J_u_star, 6)\n        ])\n\n    # Format the final output according to the problem specification:\n    # [[a1,b1,c1],[a2,b2,c2],...] with no extraneous whitespace.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2439927"}]}