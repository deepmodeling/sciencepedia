{"hands_on_practices": [{"introduction": "The fundamental motivation for using the Multilevel Monte Carlo (MLMC) method is its superior computational efficiency compared to the standard Monte Carlo approach. This exercise provides a direct, quantitative comparison by guiding you through a work calculation for a common scenario in computational engineering. By working through this problem, you will see precisely how MLMC achieves significant cost savings by strategically distributing computational effort across a hierarchy of model fidelities. This practice is essential for building a foundational intuition for the method's core advantage [@problem_id:2416330].", "problem": "A computational engineer uses Multilevel Monte Carlo (MLMC) and standard Monte Carlo (MC) to estimate the mean of a dimensionless quantity of interest (QoI) arising from a stochastic partial differential equation solved numerically on a hierarchy of meshes. The numerical approximation at level $l \\in \\{0,1,\\dots,L\\}$ uses mesh size $h_l \\propto 2^{-l}$. The absolute discretization bias satisfies $|\\mathbb{E}[Q - Q_L]| \\leq c_b 2^{-\\alpha L}$ with $c_b = 0.5$ and $\\alpha = 2$. The cost of one sample at level $l$ is $C_l = 2^{2l}$ (in arbitrary but identical units for both methods).\n\nFor MLMC, the level differences are $Y_0 = Q_0$ and $Y_l = Q_l - Q_{l-1}$ for $l \\ge 1$, sampled with independent realizations. The level-difference variances are $V_0 = \\operatorname{Var}(Y_0) = 1.0$ and, for $l \\ge 1$, $V_l = \\operatorname{Var}(Y_l) = 2^{-2l}$. For standard MC on the finest level $L$, the QoI variance is $s^2 = \\operatorname{Var}(Q_L) = 1.0$. Assume all samples are independent across all levels and realizations.\n\nBoth methods must produce a two-sided $95$ percent confidence interval for the continuum mean $\\mathbb{E}[Q]$ of total width $W = 1.0 \\times 10^{-2}$. Use the conservative requirement that the discretization bias magnitude is bounded by $W/4$, and that the statistical half-width equals $W/4$. Take the minimal integer $L$ that satisfies the bias requirement and compare the minimal expected computational work of standard MC and MLMC needed to meet the statistical half-width requirement at that $L$.\n\nWhat is the ratio of the minimal expected work of standard MC to that of MLMC under these assumptions? Round your answer to three significant figures.", "solution": "The problem requires a comparison of the minimal computational work for standard Monte Carlo (MC) and Multilevel Monte Carlo (MLMC) methods to estimate the mean of a quantity of interest, $\\mathbb{E}[Q]$, subject to a specified total error tolerance.\n\nFirst, we validate the problem statement.\nThe givens are:\n- Hierarchy of levels $l \\in \\{0, 1, \\dots, L\\}$.\n- Mesh size $h_l \\propto 2^{-l}$.\n- Discretization bias bound: $|\\mathbb{E}[Q - Q_L]| \\le c_b 2^{-\\alpha L}$ with $c_b = 0.5$ and $\\alpha = 2$.\n- Cost per sample at level $l$: $C_l = 2^{2l}$.\n- MLMC differences: $Y_0 = Q_0$, $Y_l = Q_l - Q_{l-1}$ for $l \\ge 1$.\n- MLMC variances: $V_0 = \\operatorname{Var}(Y_0) = 1.0$, $V_l = \\operatorname{Var}(Y_l) = 2^{-2l}$ for $l \\ge 1$.\n- Standard MC variance: $s^2 = \\operatorname{Var}(Q_L) = 1.0$.\n- Total confidence interval width for $\\mathbb{E}[Q]$: $W = 1.0 \\times 10^{-2}$.\n- Bias constraint: $|\\mathbb{E}[Q - Q_L]| \\le W/4$.\n- Statistical half-width constraint: statistical half-width $= W/4$.\n\nThe problem is scientifically grounded, describing a standard application of Monte Carlo methods in computational engineering. It is well-posed, with a clear objective and sufficient data. The language is objective and precise. Therefore, the problem is deemed valid.\n\nThe solution proceeds in three steps:\n1.  Determine the finest resolution level $L$.\n2.  Calculate the minimal work for standard MC, $\\text{Work}_{MC}$.\n3.  Calculate the minimal work for MLMC, $\\text{Work}_{MLMC}$.\n4.  Compute the ratio $\\frac{\\text{Work}_{MC}}{\\text{Work}_{MLMC}}$.\n\nStep 1: Determine the finest level $L$.\nThe level $L$ is chosen to satisfy the discretization bias constraint. We are given the bias is bounded by $c_b 2^{-\\alpha L}$ and this must be less than or equal to $W/4$. We must find the smallest integer $L$ that satisfies this inequality.\n$$c_b 2^{-\\alpha L} \\le \\frac{W}{4}$$\nSubstituting the given values $c_b = 0.5$, $\\alpha = 2$, and $W = 1.0 \\times 10^{-2}$:\n$$0.5 \\times 2^{-2L} \\le \\frac{1.0 \\times 10^{-2}}{4}$$\n$$0.5 \\times (2^2)^{-L} \\le 0.0025$$\n$$0.5 \\times 4^{-L} \\le 0.0025$$\n$$4^{-L} \\le \\frac{0.0025}{0.5} = 0.005$$\n$$4^L \\ge \\frac{1}{0.005} = 200$$\nWe test integer values for $L$: $4^3 = 64$ and $4^4 = 256$. The smallest integer $L$ satisfying $4^L \\ge 200$ is $L=4$.\n\nStep 2: Minimal work for standard MC.\nThe standard MC estimator uses $N_{MC}$ samples at the finest level $L=4$. The estimator is $\\hat{E}_{MC} = \\frac{1}{N_{MC}} \\sum_{i=1}^{N_{MC}} Q_L^{(i)}$. The variance of this estimator is:\n$$\\operatorname{Var}(\\hat{E}_{MC}) = \\frac{\\operatorname{Var}(Q_L)}{N_{MC}} = \\frac{s^2}{N_{MC}}$$\nWe are given $s^2 = 1.0$.\nThe problem states that the total width of a $95\\%$ confidence interval for $\\mathbb{E}[Q]$ should be $W$. This total error is composed of the discretization bias and the statistical error. The problem provides a budget for each: the bias $|\\mathbb{E}[Q - Q_L]|$ is bounded by $W/4$, and the statistical half-width of the confidence interval for $\\mathbb{E}[Q_L]$ is $W/4$.\nA two-sided $95\\%$ confidence interval has a half-width of $z \\sqrt{\\operatorname{Var}(\\text{Estimator})}$, where $z \\approx 1.96$ is the $z$-score. Therefore, the statistical constraint is:\n$$z \\sqrt{\\operatorname{Var}(\\hat{E}_{MC})} = \\frac{W}{4}$$\n$$z \\sqrt{\\frac{s^2}{N_{MC}}} = \\frac{W}{4}$$\nSolving for the required number of samples $N_{MC}$:\n$$N_{MC} = \\frac{s^2 z^2}{(W/4)^2}$$\nThe computational work (cost) for standard MC is the number of samples multiplied by the cost per sample at level $L$:\n$$\\text{Work}_{MC} = N_{MC} \\times C_L = \\frac{s^2 z^2}{(W/4)^2} C_L$$\nWith $L=4$, the cost per sample is $C_4 = 2^{2 \\times 4} = 2^8 = 256$.\n\nStep 3: Minimal work for MLMC.\nThe MLMC estimator is $\\hat{E}_{MLMC} = \\sum_{l=0}^{L} \\frac{1}{N_l} \\sum_{i=1}^{N_l} Y_l^{(i)}$. Its variance is:\n$$\\operatorname{Var}(\\hat{E}_{MLMC}) = \\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} = \\sum_{l=0}^{L} \\frac{V_l}{N_l}$$\nThe statistical error constraint is the same as for MC:\n$$z \\sqrt{\\operatorname{Var}(\\hat{E}_{MLMC})} = \\frac{W}{4} \\implies \\sum_{l=0}^{L} \\frac{V_l}{N_l} = \\frac{(W/4)^2}{z^2}$$\nThe total work for MLMC is $\\text{Work}_{MLMC} = \\sum_{l=0}^{L} N_l C_l$. We minimize this work subject to the variance constraint using Lagrange multipliers. The optimal number of samples $N_l$ for each level $l$ is proportional to $\\sqrt{V_l / C_l}$.\nThe resulting minimal work is given by the formula:\n$$\\text{Work}_{MLMC} = \\frac{z^2}{(W/4)^2} \\left( \\sum_{l=0}^{L} \\sqrt{V_l C_l} \\right)^2$$\nWe need to calculate the sum $\\sum_{l=0}^{L} \\sqrt{V_l C_l}$ for $L=4$.\nThe given values are $C_l = 2^{2l}$, $V_0 = 1.0$, and $V_l = 2^{-2l}$ for $l \\ge 1$.\nFor level $l=0$:\n$$\\sqrt{V_0 C_0} = \\sqrt{1.0 \\times 2^{2 \\times 0}} = \\sqrt{1.0 \\times 1} = 1$$\nFor levels $l \\ge 1$:\n$$\\sqrt{V_l C_l} = \\sqrt{2^{-2l} \\times 2^{2l}} = \\sqrt{1} = 1$$\nThe sum for $L=4$ is:\n$$\\sum_{l=0}^{4} \\sqrt{V_l C_l} = \\sqrt{V_0 C_0} + \\sum_{l=1}^{4} \\sqrt{V_l C_l} = 1 + (1+1+1+1) = 5$$\nSo, the minimal work for MLMC is:\n$$\\text{Work}_{MLMC} = \\frac{z^2}{(W/4)^2} (5)^2 = \\frac{25 z^2}{(W/4)^2}$$\n\nStep 4: Compute the ratio.\nWe now compute the ratio of the minimal work of standard MC to that of MLMC.\n$$\\frac{\\text{Work}_{MC}}{\\text{Work}_{MLMC}} = \\frac{\\frac{s^2 z^2}{(W/4)^2} C_L}{\\frac{z^2}{(W/4)^2} \\left( \\sum_{l=0}^{L} \\sqrt{V_l C_l} \\right)^2} = \\frac{s^2 C_L}{\\left( \\sum_{l=0}^{L} \\sqrt{V_l C_l} \\right)^2}$$\nNote that the terms involving $z$ and $W$ cancel out.\nSubstituting the known values: $s^2=1.0$, $L=4$, $C_L=C_4=256$, and $\\sum_{l=0}^4 \\sqrt{V_l C_l} = 5$:\n$$\\frac{\\text{Work}_{MC}}{\\text{Work}_{MLMC}} = \\frac{1.0 \\times 256}{5^2} = \\frac{256}{25} = 10.24$$\nThe problem asks to round the answer to three significant figures.\n$$10.24 \\approx 10.2$$", "answer": "$$\\boxed{10.2}$$", "id": "2416330"}, {"introduction": "The remarkable efficiency of MLMC is not unconditional; it relies on a favorable relationship between how quickly the variance of level differences decays and how rapidly the computational cost per sample grows. This problem explores a \"pathological\" but theoretically important regime where this relationship is unfavorable, causing the method's performance to degrade. Analyzing this scenario [@problem_id:2416392] will deepen your understanding of the MLMC complexity theorem and equip you with the critical knowledge to diagnose why the method might be underperforming in a practical application.", "problem": "Consider the one-dimensional stochastic differential equation (SDE) $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ on $t \\in [0,T]$, with $a(\\cdot)$ and $b(\\cdot)$ globally Lipschitz and linear-growth, and a scalar payoff $\\varphi(X_T)$ that is globally Lipschitz. Let $\\{h_\\ell\\}_{\\ell=0}^L$ be a geometric sequence of time steps with $h_\\ell = 2^{-\\ell}T$, and let $X_T^{(\\ell)}$ denote the Euler–Maruyama approximation on level $\\ell$. Assume that the Euler–Maruyama method has strong convergence rate $\\alpha \\in (0,1)$ in the sense that there exists $C0$ such that $\\mathbb{E}\\big[\\lvert X_T - X_T^{(\\ell)}\\rvert\\big] \\le C\\, h_\\ell^{\\alpha}$ for all $\\ell$. Assume also the classical weak order $1$ for smooth functionals, i.e., there is $C'0$ such that $\\lvert \\mathbb{E}[\\varphi(X_T)] - \\mathbb{E}[\\varphi(X_T^{(\\ell)})]\\rvert \\le C' \\, h_\\ell$ for all $\\ell$. In a one-dimensional setting, the computational work to generate a single Euler–Maruyama path on step size $h_\\ell$ scales as $\\Theta(h_\\ell^{-1})$.\n\nYou form the Multilevel Monte Carlo (MLMC) estimator for $\\mathbb{E}[\\varphi(X_T)]$ based on the telescoping identity with level corrections $Y_\\ell = \\varphi\\!\\left(X_T^{(\\ell)}\\right) - \\varphi\\!\\left(X_T^{(\\ell-1)}\\right)$ for $\\ell \\ge 1$ (with $Y_0 = \\varphi\\!\\left(X_T^{(0)}\\right)$), using standard Brownian-bridge coupling between levels. You target a root-mean-square error $\\epsilon \\in (0,1)$, balancing squared bias and variance to be of the same order.\n\nFocus on the regime where the strong convergence rate satisfies $\\alpha \\in (0,\\tfrac{1}{2})$. Which of the following statements are correct in this regime?\n\nA. For $\\alpha \\in (0,\\tfrac{1}{2})$, the optimal MLMC computational complexity to achieve error $\\epsilon$ in this $1$D setting scales as $O\\!\\left(\\epsilon^{-(3 - 2\\alpha)}\\right)$.\n\nB. For $\\alpha \\in (0,\\tfrac{1}{2})$, MLMC offers no asymptotic improvement over single-level Monte Carlo on the finest grid; both have cost $O\\!\\left(\\epsilon^{-3}\\right)$.\n\nC. The deterioration occurs because the variance decay rate of level corrections $\\beta = 2\\alpha$ is smaller than the per-sample cost growth rate $\\gamma = 1$, so under the optimal allocation most work is spent on the finest levels.\n\nD. The multilevel telescoping identity $\\mathbb{E}[\\varphi(X_T^{(L)})] = \\sum_{\\ell=0}^{L} \\mathbb{E}[Y_\\ell]$ fails when $\\alpha  \\tfrac{1}{2}$, causing the method to break down even with coupling.\n\nE. Under weak order $1$, the finest step size required to control the bias at level $L$ satisfies $h_L = \\Theta(\\epsilon)$.", "solution": "The problem statement is subjected to validation before proceeding with a solution.\n\n### Step 1: Extract Givens\n-   **Stochastic Differential Equation (SDE):** $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ for $t \\in [0,T]$.\n-   **Coefficient Properties:** $a(\\cdot)$ and $b(\\cdot)$ are globally Lipschitz and have linear-growth.\n-   **Payoff Function:** $\\varphi(X_T)$ is a scalar, with $\\varphi(\\cdot)$ being globally Lipschitz.\n-   **Numerical Scheme:** Euler–Maruyama approximation, $X_T^{(\\ell)}$, on a geometric sequence of time steps $h_\\ell = 2^{-\\ell}T$.\n-   **Strong Convergence Rate:** $\\mathbb{E}\\big[\\lvert X_T - X_T^{(\\ell)}\\rvert\\big] \\le C\\, h_\\ell^{\\alpha}$ for some $\\alpha \\in (0,1)$. The analysis focuses on the regime $\\alpha \\in (0,\\tfrac{1}{2})$.\n-   **Weak Convergence Rate:** $\\lvert \\mathbb{E}[\\varphi(X_T)] - \\mathbb{E}[\\varphi(X_T^{(\\ell)})]\\rvert \\le C' \\, h_\\ell$. This corresponds to a weak order of $\\delta_{\\text{weak}} = 1$.\n-   **Computational Cost:** The work for one path on level $\\ell$ is $C_\\ell = \\Theta(h_\\ell^{-1})$, which implies a cost growth rate $\\gamma = 1$.\n-   **Estimator:** Multilevel Monte Carlo (MLMC) based on the telescoping sum with corrections $Y_\\ell = \\varphi(X_T^{(\\ell)}) - \\varphi(X_T^{(\\ell-1)})$ for $\\ell \\ge 1$ and $Y_0 = \\varphi(X_T^{(0)})$.\n-   **Error Target:** Root-mean-square error (RMSE) is $\\epsilon \\in (0,1)$.\n-   **Error Partitioning:** The squared bias and variance are balanced to be of the same order, i.e., $(\\text{Bias})^2 = O(\\epsilon^2)$ and $\\text{Variance} = O(\\epsilon^2)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard theoretical setup for the analysis of the Multilevel Monte Carlo method for approximating expectations of SDE solutions.\n-   **Scientifically Grounded:** The problem is based on established principles of numerical analysis for SDEs and the MLMC method. The assumptions about convergence rates and costs are standard in the literature.\n-   **Well-Posed:** The problem provides sufficient information to determine the MLMC complexity and analyze the given statements. The properties of the SDE coefficients ensure a unique solution exists.\n-   **Objective:** The language is formal and precise.\n\nAll conditions for a valid problem are met. The assumptions, while forming a specific theoretical case (i.e., $\\alpha  1/2$), are self-consistent and allow for a rigorous mathematical analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be derived.\n\n### Derivation and Analysis\nThe goal is to analyze the computational complexity of the MLMC estimator to achieve a root-mean-square error (RMSE) of $\\epsilon$. The MSE is given by $\\text{MSE} = (\\text{Bias})^2 + \\text{Variance}$. We require $\\text{MSE} \\le \\epsilon^2$. The problem states we balance the terms such that $(\\text{Bias})^2 = O(\\epsilon^2)$ and $\\text{Variance} = O(\\epsilon^2)$.\n\n**1. Bias Analysis**\nThe bias of the MLMC estimator on $L$ levels is $\\text{Bias} = \\mathbb{E}[\\varphi(X_T^{(L)})] - \\mathbb{E}[\\varphi(X_T)]$.\nThe given weak convergence rate is $\\delta_{\\text{weak}} = 1$, so $|\\text{Bias}| \\le C' h_L^1$.\nTo satisfy the bias constraint $|\\text{Bias}| = O(\\epsilon)$, we must have $h_L = O(\\epsilon)$. To minimize computational cost, we choose the largest possible step size $h_L$ (smallest number of levels $L$), which leads to the scaling $h_L = \\Theta(\\epsilon)$.\n\n**2. Variance and Cost Analysis**\nThe total variance of the MLMC estimator is $\\text{Var}[\\widehat{P}_L^{\\text{ML}}] = \\sum_{\\ell=0}^L \\frac{V_\\ell}{N_\\ell}$, where $V_\\ell = \\text{Var}[Y_\\ell]$ and $N_\\ell$ is the number of samples at level $\\ell$.\nThe total computational cost is $\\text{Cost} = \\sum_{\\ell=0}^L N_\\ell C_\\ell$.\n-   **Variance of corrections ($V_\\ell$):** Since $\\varphi$ is Lipschitz, the variance of the correction term $Y_\\ell = \\varphi(X_T^{(\\ell)}) - \\varphi(X_T^{(\\ell-1)})$ for $\\ell \\ge 1$ is bounded by its second moment:\n    $V_\\ell \\le \\mathbb{E}[(\\varphi(X_T^{(\\ell)}) - \\varphi(X_T^{(\\ell-1)}))^2] \\le K_\\varphi^2 \\mathbb{E}[(X_T^{(\\ell)} - X_T^{(\\ell-1)})^2]$.\n    The strong convergence rate $\\alpha$ implies that $\\mathbb{E}[(X_T^{(\\ell)} - X_T^{(\\ell-1)})^2] = O(h_\\ell^{2\\alpha})$.\n    Thus, the variance decay rate is $\\beta = 2\\alpha$, i.e., $V_\\ell = O(h_\\ell^\\beta) = O(h_\\ell^{2\\alpha})$ for $\\ell \\ge 1$. For $\\ell=0$, $V_0=O(1)$.\n-   **Cost per sample ($C_\\ell$):** It is given that $C_\\ell = \\Theta(h_\\ell^{-1})$, so the cost growth rate is $\\gamma=1$.\n\n**3. Complexity Theorem**\nTo minimize the cost for a target variance $\\sum V_\\ell/N_\\ell = O(\\epsilon^2)$, the optimal number of samples is $N_\\ell \\propto \\sqrt{V_\\ell/C_\\ell}$. The total cost is given by:\n$$ \\text{Cost} \\approx \\frac{1}{\\epsilon^2} \\left( \\sum_{\\ell=0}^L \\sqrt{V_\\ell C_\\ell} \\right)^2 $$\nWe have $\\sqrt{V_\\ell C_\\ell} \\approx \\sqrt{h_\\ell^\\beta \\cdot h_\\ell^{-\\gamma}} = h_\\ell^{(\\beta-\\gamma)/2}$. The problem specifies $\\alpha \\in (0, \\frac{1}{2})$, which means $\\beta=2\\alpha \\in (0, 1)$. Since $\\gamma=1$, we are in the regime $\\beta  \\gamma$.\nIn this regime, the sum $\\sum_{\\ell=0}^L h_\\ell^{(\\beta-\\gamma)/2} = \\sum_{\\ell=0}^L (2^{-\\ell})^{(\\beta-\\gamma)/2}$ is a geometric series with a ratio greater than $1$, dominated by the final term $h_L^{(\\beta-\\gamma)/2}$.\nThe cost is then:\n$$ \\text{Cost} \\approx \\frac{1}{\\epsilon^2} (h_L^{(\\beta-\\gamma)/2})^2 = \\frac{1}{\\epsilon^2} h_L^{\\beta-\\gamma} $$\nSubstituting $h_L = \\Theta(\\epsilon)$ (from bias analysis with $\\delta_{\\text{weak}}=1$), $\\gamma=1$, and $\\beta=2\\alpha$:\n$$ \\text{Cost} = O\\left(\\epsilon^{-2} \\cdot \\epsilon^{2\\alpha-1}\\right) = O\\left(\\epsilon^{-2+2\\alpha-1}\\right) = O\\left(\\epsilon^{-(3-2\\alpha)}\\right) $$\n\nNow we evaluate each option.\n\n**A. For $\\alpha \\in (0,\\tfrac{1}{2})$, the optimal MLMC computational complexity to achieve error $\\epsilon$ in this 1D setting scales as $O\\!\\left(\\epsilon^{-(3 - 2\\alpha)}\\right)$.**\nThis matches our derived result. The analysis shows that when the variance decay rate $\\beta=2\\alpha$ is smaller than the cost growth rate $\\gamma=1$, and the weak error rate is 1, the complexity is given by the formula $\\text{Cost} = O(\\epsilon^{-(2+\\gamma-\\beta)})$, which for the given parameters is $O(\\epsilon^{-(3-2\\alpha)})$.\nVerdict: **Correct**.\n\n**B. For $\\alpha \\in (0,\\tfrac{1}{2})$, MLMC offers no asymptotic improvement over single-level Monte Carlo on the finest grid; both have cost $O\\!\\left(\\epsilon^{-3}\\right)$.**\nFor a single-level Monte Carlo (MC) method, the bias must be controlled by choosing a fine grid $L$ such that $h_L = O(\\epsilon)$. The variance of the estimator is $V_L/N_L$, which needs to be $O(\\epsilon^2)$. As $L\\to\\infty$, $V_L \\to \\text{Var}[\\varphi(X_T)] = O(1)$, so we need $N_L = O(\\epsilon^{-2})$ samples. The total cost is $\\text{Cost}_{\\text{MC}} = N_L C_L = O(\\epsilon^{-2}) \\cdot O(h_L^{-1}) = O(\\epsilon^{-2} \\cdot \\epsilon^{-1}) = O(\\epsilon^{-3})$.\nThe MLMC cost is $O(\\epsilon^{-(3-2\\alpha)})$. Since $\\alpha \\in (0, \\tfrac{1}{2})$, we have $2\\alpha  0$, and the exponent $3-2\\alpha$ is strictly less than $3$. Therefore, MLMC offers an asymptotic improvement over single-level MC. The statement is false on two counts: MLMC is better, and its cost is not $O(\\epsilon^{-3})$.\nVerdict: **Incorrect**.\n\n**C. The deterioration occurs because the variance decay rate of level corrections $\\beta = 2\\alpha$ is smaller than the per-sample cost growth rate $\\gamma = 1$, so under the optimal allocation most work is spent on the finest levels.**\nThe ideal MLMC complexity is $O(\\epsilon^{-2})$, achieved when $\\beta\\gamma$. The complexity \"deteriorates\" (becomes worse) when $\\beta \\le \\gamma$.\nIn this problem, $\\beta=2\\alpha$ and $\\gamma=1$. The regime $\\alpha \\in (0, \\tfrac{1}{2})$ corresponds exactly to $\\beta  \\gamma$. This condition is the reason for the non-optimal complexity.\nThe work on level $\\ell$ is $W_\\ell = N_\\ell C_\\ell$. With optimal allocation, $W_\\ell \\propto \\sqrt{V_\\ell C_\\ell} \\propto h_\\ell^{(\\beta-\\gamma)/2}$. Since $\\beta-\\gamma  0$, the exponent is negative, meaning $W_\\ell$ increases as $h_\\ell$ decreases (i.e., as $\\ell$ increases). Thus, the total work is dominated by the work on the finest levels. The statement provides a correct and complete explanation.\nVerdict: **Correct**.\n\n**D. The multilevel telescoping identity $\\mathbb{E}[\\varphi(X_T^{(L)})] = \\sum_{\\ell=0}^{L} \\mathbb{E}[Y_\\ell]$ fails when $\\alpha  \\tfrac{1}{2}$, causing the method to break down even with coupling.**\nThe telescoping identity is $\\mathbb{E}[\\varphi(X_T^{(L)})] = \\mathbb{E}[\\varphi(X_T^{(0)})] + \\sum_{\\ell=1}^{L} \\mathbb{E}[\\varphi(X_T^{(\\ell)}) - \\varphi(X_T^{(\\ell-1)})]$. This is a purely algebraic identity resulting from the linearity of the expectation operator. It holds for any set of random variables, irrespective of convergence rates like $\\alpha$ or any coupling strategy. The method does not \"break down\"; its computational efficiency is simply reduced.\nVerdict: **Incorrect**.\n\n**E. Under weak order 1, the finest step size required to control the bias at level $L$ satisfies $h_L = \\Theta(\\epsilon)$.**\nAs established in the initial bias analysis, the MLMC estimator's bias is controlled by the finest level $L$: $|\\text{Bias}| \\approx C'h_L^{\\delta_{\\text{weak}}}$. The problem states $\\delta_{\\text{weak}}=1$. To achieve an overall RMSE of $\\epsilon$, the bias must be controlled to be $O(\\epsilon)$. This requires $C'h_L^1 = O(\\epsilon)$, which implies $h_L = O(\\epsilon)$. For an efficient algorithm, one chooses the largest possible $h_L$ (i.e., the smallest $L$) that satisfies this, making the relationship tight: $h_L = \\Theta(\\epsilon)$.\nVerdict: **Correct**.", "answer": "$$\\boxed{ACE}$$", "id": "2416392"}, {"introduction": "This final practice serves as a capstone exercise, challenging you to apply the full MLMC methodology to a realistic engineering problem: estimating signal delay in a VLSI interconnect under manufacturing uncertainty. You will build a complete simulation pipeline, from modeling the underlying physics with a resistive-capacitive (RC) ladder network to implementing a numerical solver and embedding it within an adaptive MLMC framework. This hands-on implementation [@problem_id:2416335] synthesizes the concepts of physical modeling, numerical discretization, and statistical estimation, providing an authentic experience in solving complex uncertainty propagation problems in computational engineering.", "problem": "You are to implement a complete, runnable program that estimates the expected signal delay in a Very Large Scale Integration (VLSI) interconnect line subject to manufacturing variability using Multilevel Monte Carlo (MLMC). The estimation target is the expected time for the far-end node voltage to reach a specified threshold fraction of a unit step input, expressed in seconds, rounded to floating-point precision.\n\nThe physical model shall be based on the following fundamental principles and core definitions:\n- Ohm’s law in resistive media: for a uniform conductor of length $L$ and cross-sectional area $A$, the resistance is $R = \\rho \\, L / A$, where $\\rho$ is the electrical resistivity.\n- Parallel-plate capacitance: for a plate of area $A$ separated by a dielectric of thickness $h$ and permittivity $\\varepsilon$, the capacitance is $C = \\varepsilon \\, A / h$. A common engineering correction for edge (fringing) fields is to use an effective plate width $W_{\\text{eff}} = W + 2h/\\pi$, where $W$ is the physical width.\n- First-order system step response: a single resistor-capacitor network obeys the linear differential equation $C \\, \\mathrm{d}v/\\mathrm{d}t + (1/R)\\,v = (1/R)\\,u(t)$ for unit step input $u(t)$, and the time to reach a fixed threshold (e.g., $v^\\star = 0.5$) is determined by solving for $t$ in the step response. In a distributed interconnect, the line is modeled as a resistive-capacitive ladder obtained by spatial discretization; the governing semi-discrete linear differential system can be integrated in time using unconditionally stable backward Euler.\n\nMathematical and numerical model:\n- Geometry and materials: an interconnect of length $L$ and rectangular cross section of width $W$ and thickness $T$ runs over a ground plane separated by a dielectric of thickness $h$ and permittivity $\\varepsilon = \\varepsilon_{0}\\varepsilon_{r}$, where $\\varepsilon_{0}$ is the vacuum permittivity and $\\varepsilon_{r}$ is the relative permittivity. Electrical resistivity is $\\rho$.\n- Random inputs: due to manufacturing variability, width $W$ and thickness $T$ are independent, normally distributed with means $\\mu_{W}$ and $\\mu_{T}$ and standard deviations $\\sigma_{W}$ and $\\sigma_{T}$, truncated below at strictly positive lower bounds $w_{\\min}$ and $t_{\\min}$ to enforce physical realism. That is, draw $W \\sim \\max(\\mathcal{N}(\\mu_{W}, \\sigma_{W}^{2}), w_{\\min})$ and $T \\sim \\max(\\mathcal{N}(\\mu_{T}, \\sigma_{T}^{2}), t_{\\min})$ by rejection sampling.\n- Spatial discretization hierarchy: construct $L+1$ levels of uniform spatial grids with $N_{\\ell} = N_{0}\\, 2^{\\ell}$ segments for level $\\ell \\in \\{0,1,\\dots,L\\}$, with $N_{0} \\in \\mathbb{N}$. Let $\\Delta x_{\\ell} = L/N_{\\ell}$. On each level, represent the interconnect as a resistive-capacitive ladder: each segment has resistance $r_{\\text{seg}} = \\rho \\, \\Delta x_{\\ell} /(W T)$, each interior node has capacitance to ground $c_{\\text{node}} = \\varepsilon \\, W_{\\text{eff}} \\, \\Delta x_{\\ell} / h$ with $W_{\\text{eff}} = W + 2h/\\pi$.\n- Temporal integration: apply backward Euler with time step $\\Delta t_{\\ell}$ to the semi-discrete linear system for node voltages. The driver node at the input end is a Dirichlet boundary set to a unit step of amplitude $1$; the far end is open (no load). Use a fixed threshold $v^{\\star} = 0.5$ to define the delay observable $P_{\\ell}(W,T)$ as the first time $t$ such that the far-end node voltage reaches $v^{\\star}$, linearly interpolated between time steps. Choose the simulation horizon $T_{\\max}$ proportional to the diffusive time scale $r' c' L^{2}$ with $r' = \\rho/(W T)$ and $c' = \\varepsilon W_{\\text{eff}}/h$, namely $T_{\\max} = \\alpha \\, r' c' L^{2}$ for a fixed factor $\\alpha0$. Use a level-dependent number of time steps $M_{\\ell} = M_{0}\\,2^{\\ell}$ and $\\Delta t_{\\ell} = T_{\\max}/M_{\\ell}$.\n- Quantity of interest: for any realization $(W,T)$ and level $\\ell$, compute $Y_{\\ell}(W,T) := P_{\\ell}(W,T)$, the estimated $v^{\\star}$-crossing time in seconds.\n\nMultilevel Monte Carlo (MLMC) estimator:\n- Use the telescoping sum representation for the expectation at the finest level $L$: $\\mathbb{E}[Y_{L}] = \\mathbb{E}[Y_{0}] + \\sum_{\\ell=1}^{L} \\mathbb{E}[Y_{\\ell} - Y_{\\ell-1}]$, and estimate each expectation by Monte Carlo sample averages with level-wise couplings that use shared random inputs $(W,T)$ for $Y_{\\ell}$ and $Y_{\\ell-1}$ at the same sample index.\n- Variance-driven sample allocation: let $V_{\\ell}$ denote the variance of $Y_{0}$ for $\\ell=0$ and of $(Y_{\\ell}-Y_{\\ell-1})$ for $\\ell \\ge 1$, and $C_{\\ell}$ the work per sample at level $\\ell$ (proportional to $N_{\\ell} M_{\\ell}$). Given a root-mean-square error tolerance $\\varepsilon$, split it into variance and bias targets. Use pilot estimates of $V_{\\ell}$ and $C_{\\ell}$ to allocate the number of samples $N_{\\ell}$ per level according to the standard asymptotically optimal rule $N_{\\ell} \\propto \\sqrt{V_{\\ell}/C_{\\ell}}$, normalized to achieve the variance target. Control the bias by limiting the maximum level $L$ and by ensuring that the mean of $(Y_{L}-Y_{L-1})$ is sufficiently small relative to $\\varepsilon$.\n\nImplementation requirements:\n- Your program must implement the above model and estimator. It must use a fixed pseudorandom number generator seed to ensure reproducibility across runs.\n- For numerical robustness, ensure $W \\ge w_{\\min}$ and $T \\ge t_{\\min}$ by rejection sampling.\n- The final answers must be expressed in seconds, as floating-point numbers.\n\nTest suite:\nImplement your program to run the MLMC estimator for the following three cases. In all cases, use $v^{\\star} = 0.5$, $\\alpha = 10$, $N_{0} = 8$, $M_{0} = 80$, $L_{\\max} = 3$, $w_{\\min} = \\mu_{W}/3$, and $t_{\\min} = \\mu_{T}/3$. The vacuum permittivity is $\\varepsilon_{0} = 8.8541878128 \\times 10^{-12}\\ \\text{F/m}$. The final output must be a single line containing a Python-style list of the three expected delays in seconds, in the order of the cases below, without any additional text.\n\n- Case A (happy path):\n  - $L = 5.0 \\times 10^{-4}\\ \\text{m}$, $h = 2.0 \\times 10^{-7}\\ \\text{m}$, $\\rho = 1.68 \\times 10^{-8}\\ \\Omega\\cdot\\text{m}$, $\\varepsilon_{r} = 3.9$,\n  - $\\mu_{W} = 1.2 \\times 10^{-7}\\ \\text{m}$, $\\sigma_{W} = 1.2 \\times 10^{-8}\\ \\text{m}$,\n  - $\\mu_{T} = 6.0 \\times 10^{-8}\\ \\text{m}$, $\\sigma_{T} = 8.0 \\times 10^{-9}\\ \\text{m}$,\n  - MLMC tolerance $\\varepsilon = 1.0 \\times 10^{-12}\\ \\text{s}$.\n\n- Case B (thin metal, larger delay; edge case for higher resistance):\n  - $L = 5.0 \\times 10^{-4}\\ \\text{m}$, $h = 2.0 \\times 10^{-7}\\ \\text{m}$, $\\rho = 1.68 \\times 10^{-8}\\ \\Omega\\cdot\\text{m}$, $\\varepsilon_{r} = 3.9$,\n  - $\\mu_{W} = 1.2 \\times 10^{-7}\\ \\text{m}$, $\\sigma_{W} = 1.2 \\times 10^{-8}\\ \\text{m}$,\n  - $\\mu_{T} = 4.0 \\times 10^{-8}\\ \\text{m}$, $\\sigma_{T} = 6.0 \\times 10^{-9}\\ \\text{m}$,\n  - MLMC tolerance $\\varepsilon = 1.5 \\times 10^{-12}\\ \\text{s}$.\n\n- Case C (low-$k$ dielectric, smaller delay; edge case for reduced capacitance):\n  - $L = 5.0 \\times 10^{-4}\\ \\text{m}$, $h = 2.0 \\times 10^{-7}\\ \\text{m}$, $\\rho = 1.68 \\times 10^{-8}\\ \\Omega\\cdot\\text{m}$, $\\varepsilon_{r} = 2.5$,\n  - $\\mu_{W} = 1.2 \\times 10^{-7}\\ \\text{m}$, $\\sigma_{W} = 1.2 \\times 10^{-8}\\ \\text{m}$,\n  - $\\mu_{T} = 6.0 \\times 10^{-8}\\ \\text{m}$, $\\sigma_{T} = 8.0 \\times 10^{-9}\\ \\text{m}$,\n  - MLMC tolerance $\\varepsilon = 1.0 \\times 10^{-12}\\ \\text{s}$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of [Case A result, Case B result, Case C result], for example, \"[resultA,resultB,resultC]\". Each result must be a single floating-point number in seconds.\n\nAll angles, if any, must be in radians. There are no percentages in this problem; any fractional quantities must be expressed as decimals.\n\nYour program must be completely self-contained, must not require any user input, and must not access files or the network. It must adhere to the specified execution environment.", "solution": "The problem requires the estimation of the expected signal propagation delay in a VLSI interconnect using the Multilevel Monte Carlo (MLMC) method. The interconnect's geometric parameters, width $W$ and thickness $T$, are subject to manufacturing variability and are modeled as random variables.\n\nThe solution is developed by first establishing the physical and numerical model for a single realization of the interconnect, and then embedding this model within the MLMC statistical framework.\n\n### 1. Physical and Numerical Model\n\nAn interconnect of length $L$, width $W$, and thickness $T$ over a ground plane is modeled as a distributed resistive-capacitive (RC) line. For the purpose of numerical simulation, this continuous line is discretized into a ladder network of $N_{\\ell}$ segments for a given refinement level $\\ell$.\n\n**1.1. RC Ladder Parameters**\n\nFor a level $\\ell$, the number of uniform spatial segments is $N_{\\ell} = N_{0} 2^{\\ell}$, where $N_{0}$ is the base number of segments. The length of each segment is $\\Delta x_{\\ell} = L / N_{\\ell}$.\n\n- **Resistance per segment ($r_{\\text{seg}}$)**: Based on Ohm's law, the resistance of a single segment is given by:\n$$r_{\\text{seg}} = \\frac{\\rho \\Delta x_{\\ell}}{W T}$$\nwhere $\\rho$ is the electrical resistivity, and $W$ and $T$ are the random width and thickness for a given sample.\n\n- **Capacitance per node ($c_{\\text{node}}$)**: The capacitance to the ground plane is modeled using the parallel-plate formula with a fringe-field correction. The total capacitance of a segment is attributed to its nodes. We adopt the standard $\\Pi$-model, where a segment's capacitance is split equally between its two end nodes. An interior node of the ladder network thus accumulates capacitance from two adjacent segments, while an end node receives capacitance from only one.\nThe effective width $W_{\\text{eff}}$ for capacitance calculation is $W_{\\text{eff}} = W + 2h/\\pi$, where $h$ is the dielectric thickness. The capacitance for a length $\\Delta x_{\\ell}$ is:\n$$c_{\\text{node}} = \\frac{\\varepsilon W_{\\text{eff}} \\Delta x_{\\ell}}{h}$$\nwhere $\\varepsilon = \\varepsilon_{0}\\varepsilon_{r}$ is the dielectric permittivity.\nAccording to the $\\Pi$-model, each interior node ($i=1, \\dots, N_{\\ell}-1$) has a capacitance $C_i = c_{\\text{node}}$, and the far-end node ($i=N_{\\ell}$) has a capacitance $C_{N_{\\ell}} = c_{\\text{node}}/2$.\n\n**1.2. Semi-Discrete System of Equations**\n\nThe RC ladder consists of $N_{\\ell}$ nodes with unknown voltages, labeled $v_1, \\dots, v_{N_{\\ell}}$. The input node $v_0$ is driven by a unit step voltage source, $v_0(t)=1$ for $t \\ge 0$. Applying Kirchhoff's current law at each node $i$ yields a system of first-order ordinary differential equations (ODEs):\n- For an interior node $i \\in \\{1, \\dots, N_{\\ell}-1\\}$:\n$$C_i \\frac{\\mathrm{d}v_i}{\\mathrm{d}t} = \\frac{v_{i-1} - v_i}{r_{\\text{seg}}} - \\frac{v_i - v_{i+1}}{r_{\\text{seg}}}$$\n- For the open-circuited far-end node $i = N_{\\ell}$:\n$$C_{N_{\\ell}} \\frac{\\mathrm{d}v_{N_{\\ell}}}{\\mathrm{d}t} = \\frac{v_{N_{\\ell}-1} - v_{N_{\\ell}}}{r_{\\text{seg}}}$$\n\nThis system can be written in matrix form: $\\mathbf{C} \\frac{\\mathrm{d}\\mathbf{v}}{\\mathrm{d}t} = -\\mathbf{K}\\mathbf{v} + \\mathbf{b}$, where $\\mathbf{v} = [v_1, \\dots, v_{N_{\\ell}}]^T$, $\\mathbf{C}$ is a diagonal matrix of node capacitances, $\\mathbf{K}$ is the symmetric positive definite conductance matrix, and $\\mathbf{b}$ is the source vector from the input $v_0$.\n\n**1.3. Temporal Integration**\n\nThe system of ODEs is integrated in time using the backward Euler method, which is unconditionally stable and thus suitable for potentially stiff RC networks. The time domain is discretized into $M_{\\ell} = M_{0} 2^{\\ell}$ steps of size $\\Delta t_{\\ell} = T_{\\max} / M_{\\ell}$. The simulation horizon $T_{\\max} = \\alpha r' c' L^2$ is chosen to be sufficiently long to observe the delay, where $r' = \\rho/(WT)$ and $c' = \\varepsilon W_{\\text{eff}}/h$.\n\nThe backward Euler update from time step $k$ to $k+1$ is given by:\n$$\\mathbf{C} \\frac{\\mathbf{v}^{k+1} - \\mathbf{v}^k}{\\Delta t_{\\ell}} = -\\mathbf{K}\\mathbf{v}^{k+1} + \\mathbf{b}$$\nRearranging gives a linear system to be solved for $\\mathbf{v}^{k+1}$ at each time step:\n$$(\\mathbf{C} + \\Delta t_{\\ell} \\mathbf{K}) \\mathbf{v}^{k+1} = \\mathbf{C}\\mathbf{v}^k + \\Delta t_{\\ell} \\mathbf{b}$$\nStarting from the initial condition $\\mathbf{v}^0 = \\mathbf{0}$, we iterate this equation. The matrix $\\mathbf{A}_{\\text{BE}} = \\mathbf{C} + \\Delta t_{\\ell} \\mathbf{K}$ is tridiagonal, so the system can be solved efficiently in $O(N_{\\ell})$ operations.\n\nThe quantity of interest, $P_{\\ell}(W,T)$, is the time at which the far-end node voltage $v_{N_{\\ell}}(t)$ first reaches the threshold $v^{\\star}=0.5$. If $v_{N_{\\ell}}$ crosses the threshold between time steps $k$ and $k+1$, the delay is computed by linear interpolation.\n\n### 2. Multilevel Monte Carlo (MLMC) Estimator\n\nThe goal is to compute $\\mathbb{E}[Y_L]$, where $Y_\\ell = P_\\ell(W,T)$ is the delay computed at level $\\ell$ and $L=L_{\\max}$ is the finest level. MLMC is based on the telescoping sum:\n$$\\mathbb{E}[Y_L] = \\mathbb{E}[Y_0] + \\sum_{\\ell=1}^{L} \\mathbb{E}[Y_\\ell - Y_{\\ell-1}]$$\nThe MLMC estimator $\\hat{Y}_{L}^{\\text{MLMC}}$ for $\\mathbb{E}[Y_L]$ is:\n$$\\hat{Y}_{L}^{\\text{MLMC}} = \\frac{1}{K_0}\\sum_{i=1}^{K_0} Y_0^{(i)} + \\sum_{\\ell=1}^{L} \\frac{1}{K_\\ell}\\sum_{i=1}^{K_\\ell} (Y_\\ell^{(i)} - Y_{\\ell-1}^{(i)})$$\nHere, $K_\\ell$ is the number of Monte Carlo samples at level $\\ell$. The crucial aspect is the coupling: for each sample $i$ in the sum for level $\\ell \\ge 1$, the same random inputs $(W^{(i)}, T^{(i)})$ are used to compute both $Y_\\ell^{(i)}$ and $Y_{\\ell-1}^{(i)}$. This ensures that the variance of the difference, $V_\\ell = \\text{Var}[Y_\\ell - Y_{\\ell-1}]$, decreases as the level $\\ell$ increases.\n\n**2.1. Optimal Sample Allocation**\n\nThe total Mean Squared Error (MSE) of the estimator is $\\text{MSE} = \\text{Var}[\\hat{Y}_{L}^{\\text{MLMC}}] + (\\text{Bias})^2 \\le \\varepsilon^2$, where $\\varepsilon$ is the desired root-mean-square error. The variance is $\\text{Var}[\\hat{Y}_{L}^{\\text{MLMC}}] = \\sum_{\\ell=0}^{L} V_\\ell/K_\\ell$, with $V_0 = \\text{Var}[Y_0]$. The bias is due to the finite discretization at level $L$.\n\nWe allocate half of the error budget to variance, $\\sum_{\\ell=0}^{L} V_\\ell/K_\\ell \\le \\varepsilon^2/2$. The number of samples $K_\\ell$ at each level is chosen to minimize the total computational cost, $\\sum_{\\ell=0}^{L} K_\\ell C_\\ell$, where $C_\\ell \\propto N_\\ell M_\\ell$ is the cost per sample at level $\\ell$. The optimal number of samples is given by:\n$$K_\\ell = \\left\\lceil \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\sqrt{\\frac{V_\\ell}{C_\\ell}} \\right\\rceil$$\n\n**2.2. Algorithm**\n\nThe practical algorithm proceeds as follows:\n1.  **Pilot Stage**: A small number of samples ($K_{\\text{pilot}}$) are simulated on all levels $\\ell=0, \\dots, L$. This is used to obtain initial estimates for the means $\\mathbb{E}[Y_\\ell - Y_{\\ell-1}]$ and variances $V_\\ell$. The costs $C_\\ell = N_\\ell M_\\ell$ are known a priori.\n2.  **Sample Allocation**: Using the pilot estimates for $V_\\ell$ and the known costs $C_\\ell$, the optimal number of samples $K_\\ell$ for each level is calculated using the formula above.\n3.  **Main Simulation**: For each level $\\ell$, additional simulations are run until the total number of samples reaches $K_\\ell$. The sample means of $Y_0$ and the differences $(Y_\\ell - Y_{\\ell-1})$ are computed.\n4.  **Final Estimate**: The final MLMC estimate is obtained by summing the sample means calculated in the previous step, as per the telescoping sum formula.\n\nThe random inputs $W$ and $T$ are drawn from truncated normal distributions using rejection sampling to ensure they are physically realistic ($W \\ge w_{\\min}, T \\ge t_{\\min}$).", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\nfrom math import pi\n\ndef solve():\n    \"\"\"\n    Main function to solve the VLSI delay estimation problem for all test cases.\n    \"\"\"\n    # Physical and numerical constants\n    EPS0 = 8.8541878128e-12\n    V_STAR = 0.5\n    ALPHA = 10.0\n    N0 = 8\n    M0 = 80\n    L_MAX = 3\n    \n    test_cases = [\n        # Case A\n        {\n            'L': 5.0e-4, 'h': 2.0e-7, 'rho': 1.68e-8, 'eps_r': 3.9,\n            'mu_W': 1.2e-7, 'sigma_W': 1.2e-8,\n            'mu_T': 6.0e-8, 'sigma_T': 8.0e-9,\n            'mlmc_tol': 1.0e-12\n        },\n        # Case B\n        {\n            'L': 5.0e-4, 'h': 2.0e-7, 'rho': 1.68e-8, 'eps_r': 3.9,\n            'mu_W': 1.2e-7, 'sigma_W': 1.2e-8,\n            'mu_T': 4.0e-8, 'sigma_T': 6.0e-9,\n            'mlmc_tol': 1.5e-12\n        },\n        # Case C\n        {\n            'L': 5.0e-4, 'h': 2.0e-7, 'rho': 1.68e-8, 'eps_r': 2.5,\n            'mu_W': 1.2e-7, 'sigma_W': 1.2e-8,\n            'mu_T': 6.0e-8, 'sigma_T': 8.0e-9,\n            'mlmc_tol': 1.0e-12\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        # Add constants to the parameter dictionary\n        params.update({'v_star': V_STAR, 'alpha': ALPHA, 'N0': N0, 'M0': M0, 'eps0': EPS0})\n        result = mlmc_estimator(params, L_MAX)\n        results.append(result)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_wt_sample(params, rng):\n    \"\"\"\n    Generates a sample of (W, T) using rejection sampling from a truncated normal distribution.\n    \"\"\"\n    mu_W, sigma_W = params['mu_W'], params['sigma_W']\n    w_min = mu_W / 3.0\n    while True:\n        W = rng.normal(mu_W, sigma_W)\n        if W >= w_min:\n            break\n            \n    mu_T, sigma_T = params['mu_T'], params['sigma_T']\n    t_min = mu_T / 3.0\n    while True:\n        T = rng.normal(mu_T, sigma_T)\n        if T >= t_min:\n            break\n            \n    return W, T\n\ndef compute_delay(params, W, T, level):\n    \"\"\"\n    Computes the signal delay for a single sample (W, T) at a given discretization level.\n    \"\"\"\n    S_l = params['N0'] * (2**level)\n    M_l = params['M0'] * (2**level)\n    \n    eps = params['eps0'] * params['eps_r']\n    W_eff = W + 2 * params['h'] / pi\n    \n    r_prime = params['rho'] / (W * T)\n    c_prime = eps * W_eff / params['h']\n    \n    delta_x = params['L'] / S_l\n    r_seg = r_prime * delta_x\n    c_node = c_prime * delta_x\n    \n    T_max = params['alpha'] * r_prime * c_prime * params['L']**2\n    delta_t = T_max / M_l\n    \n    if r_seg == 0 or c_node == 0: return 0.0\n    g = 1.0 / r_seg\n\n    # Coefficients for the backward Euler tridiagonal system\n    k1 = delta_t * g / c_node\n    k2 = delta_t * g / (c_node / 2.0)\n    \n    # SciPy's banded solver format: ab[u+i-j, j] = A[i,j]\n    # For u=1, l=1: ab[0,:]=super, ab[1,:]=main, ab[2,:]=sub\n    ab = np.zeros((3, S_l))\n    # Super-diagonal (A[i, i+1]) -> ab[0, i+1]\n    ab[0, 1:] = -k1\n    # Main-diagonal (A[i, i]) -> ab[1, i]\n    ab[1, 0:S_l-1] = 1.0 + 2.0 * k1\n    ab[1, S_l-1] = 1.0 + k2\n    # Sub-diagonal (A[i+1, i]) -> ab[2, i]\n    ab[2, 0:S_l-2] = -k1\n    if S_l > 1:\n        ab[2, S_l-2] = -k1 \n        ab[2, S_l-2] = -k2 # A[S_l-1, S_l-2] = -k2\n\n    # Time stepping\n    v = np.zeros(S_l)\n    \n    for k in range(int(M_l)):\n        v_old = v.copy()\n        \n        rhs = v_old\n        rhs[0] += k1 # Add source term\n        \n        v = solve_banded((1, 1), ab, rhs, check_finite=False)\n        \n        if v[-1] >= params['v_star']:\n            v_far_end_old = v_old[-1]\n            v_far_end_new = v[-1]\n            \n            t_old = k * delta_t\n            t_new = (k + 1) * delta_t\n\n            if v_far_end_new == v_far_end_old: return t_new\n            \n            delay = t_old + (t_new - t_old) * (params['v_star'] - v_far_end_old) / (v_far_end_new - v_far_end_old)\n            return delay\n\n    return T_max # Should not be reached with adequate alpha\n\ndef mlmc_estimator(params, L_max):\n    \"\"\"\n    Implements the Multilevel Monte Carlo estimator for a given test case.\n    \"\"\"\n    K_pilot = 200\n    eps = params['mlmc_tol']\n    rng = np.random.default_rng(seed=12345)\n\n    # --- Pilot Stage ---\n    means = [0.0] * (L_max + 1)\n    variances = [0.0] * (L_max + 1)\n    costs = [0.0] * (L_max + 1)\n\n    Y_samples = [np.zeros(K_pilot) for _ in range(L_max + 1)]\n    for i in range(K_pilot):\n        W, T = generate_wt_sample(params, rng)\n        for l in range(L_max + 1):\n            Y_samples[l][i] = compute_delay(params, W, T, l)\n\n    for l in range(L_max + 1):\n        if l == 0:\n            diff_samples = Y_samples[0]\n        else:\n            diff_samples = Y_samples[l] - Y_samples[l-1]\n        \n        means[l] = np.mean(diff_samples)\n        variances[l] = np.var(diff_samples)\n        S_l = params['N0'] * (2**l)\n        M_l = params['M0'] * (2**l)\n        costs[l] = float(S_l * M_l)\n\n    # --- Sample Allocation ---\n    optimal_K = [0] * (L_max + 1)\n    sum_sqrt_V_C = sum(np.sqrt(variances[l] * costs[l]) for l in range(L_max + 1) if variances[l] > 0)\n    \n    for l in range(L_max + 1):\n        if variances[l] > 1e-30: # If variance is non-negligible\n            num = 2.0 / (eps**2) * sum_sqrt_V_C * np.sqrt(variances[l] / costs[l])\n            optimal_K[l] = int(np.ceil(num))\n        else:\n            optimal_K[l] = 0\n    \n    # --- Main Simulation ---\n    final_estimate = 0.0\n    for l in range(L_max + 1):\n        K_l = optimal_K[l]\n        if K_l  2 and variances[l] > 1e-30:\n            K_l = 2\n        \n        # If variance is zero, the pilot mean is exact for the difference term\n        if K_l == 0:\n            final_estimate += means[l]\n            continue\n\n        sum_of_diffs = 0.0\n        for _ in range(K_l):\n            W, T = generate_wt_sample(params, rng)\n            y_fine = compute_delay(params, W, T, l)\n            if l == 0:\n                y_coarse = 0.0\n            else:\n                y_coarse = compute_delay(params, W, T, l-1)\n            sum_of_diffs += y_fine - y_coarse\n        \n        final_estimate += sum_of_diffs / K_l\n        \n    return final_estimate\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2416335"}]}