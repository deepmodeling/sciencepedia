{"hands_on_practices": [{"introduction": "At the heart of advanced uncertainty quantification methods like the Stochastic Galerkin and Stochastic Collocation methods lies the concept of generalized polynomial chaos (gPC). This powerful framework uses special sets of orthogonal polynomials that are tailored to the probability distributions of the random inputs in a system. Mastering the construction of these basis functions is the essential first step toward building robust stochastic models. This foundational exercise [@problem_id:2439629] provides direct practice in creating an orthonormal tensor-product basis for a system with multiple inputs following different probability distributions, a crucial skill for analyzing complex, real-world systems.", "problem": "In computational engineering uncertainty quantification, both stochastic collocation (SC) and stochastic Galerkin (SG) methods commonly employ generalized polynomial chaos (gPC) expansions. Consider two independent random inputs: a normal random variable $\\xi$ with distribution $\\mathcal{N}(0,1)$ and a uniform random variable $\\eta$ on the interval $[-1,1]$. Let the joint probability density be the product of the marginals. An orthonormal gPC basis with respect to this joint measure is formed by tensor products of one-dimensional orthonormal polynomial families adapted to the marginals. Specifically, use the probabilists’ Hermite polynomials for the normal input and the Legendre polynomials for the uniform input, each normalized to be orthonormal with respect to their probability measures.\n\nLet $\\psi_{i,j}(\\xi,\\eta)$ denote the orthonormal tensor-product basis function of degree $i$ in $\\xi$ and degree $j$ in $\\eta$. Provide the explicit closed-form analytical expression for $\\psi_{2,3}(\\xi,\\eta)$ as a single simplified function of $\\xi$ and $\\eta$. Do not include any units. Do not round; give your answer in exact form.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the theory of uncertainty quantification, specifically generalized polynomial chaos (gPC) expansions. It is well-posed, objective, and contains all necessary information to derive a unique analytical solution.\n\nThe task is to find the explicit expression for the orthonormal tensor-product basis function $\\psi_{2,3}(\\xi,\\eta)$. Given that the random variables $\\xi$ and $\\eta$ are independent, the joint probability measure is the product of their marginals. The orthonormal basis $\\psi_{i,j}(\\xi,\\eta)$ is formed by the tensor product of the one-dimensional orthonormal polynomials corresponding to each random variable. Let $\\phi_i(\\xi)$ be the $i$-th degree orthonormal polynomial for the standard normal variable $\\xi$, and let $\\chi_j(\\eta)$ be the $j$-th degree orthonormal polynomial for the uniform variable $\\eta$. The tensor-product basis function is then:\n$$\n\\psi_{i,j}(\\xi,\\eta) = \\phi_i(\\xi) \\chi_j(\\eta)\n$$\nWe need to determine the specific functions $\\phi_2(\\xi)$ and $\\chi_3(\\eta)$.\n\nFirst, consider the random variable $\\xi \\sim \\mathcal{N}(0,1)$. The probability density function (PDF) is $w_{\\xi}(\\xi) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{\\xi^2}{2})$. The corresponding orthogonal polynomials are the probabilists' Hermite polynomials, $He_n(\\xi)$. The orthonormality condition for the gPC basis is defined with respect to the probability measure. The inner product of two functions $f(\\xi)$ and $g(\\xi)$ is their expectation $\\mathbb{E}[f(\\xi)g(\\xi)]$:\n$$\n\\langle f, g \\rangle_{\\xi} = \\int_{-\\infty}^{\\infty} f(\\xi)g(\\xi) w_{\\xi}(\\xi) d\\xi\n$$\nThe squared norm of the $n$-th degree Hermite polynomial under this inner product is known to be $\\langle He_n, He_n \\rangle_{\\xi} = n!$.\nThe one-dimensional orthonormal polynomials $\\phi_n(\\xi)$ are thus obtained by normalizing $He_n(\\xi)$:\n$$\n\\phi_n(\\xi) = \\frac{He_n(\\xi)}{\\sqrt{\\langle He_n, He_n \\rangle_{\\xi}}} = \\frac{He_n(\\xi)}{\\sqrt{n!}}\n$$\nWe are interested in the degree $i=2$. The second probabilists' Hermite polynomial is $He_2(\\xi) = \\xi^2 - 1$. The normalization constant is $\\sqrt{2!} = \\sqrt{2}$. Therefore, the orthonormal polynomial of degree $2$ is:\n$$\n\\phi_2(\\xi) = \\frac{\\xi^2 - 1}{\\sqrt{2}}\n$$\n\nSecond, consider the random variable $\\eta \\sim \\mathcal{U}(-1,1)$. Its PDF is $w_{\\eta}(\\eta) = \\frac{1}{2}$ for $\\eta \\in [-1,1]$. The corresponding orthogonal polynomials are the Legendre polynomials, $P_n(\\eta)$, which are orthogonal with respect to a constant weight function of $1$ on the interval $[-1,1]$. The inner product for the gPC basis is defined with respect to the uniform probability measure:\n$$\n\\langle f, g \\rangle_{\\eta} = \\int_{-1}^{1} f(\\eta)g(\\eta) w_{\\eta}(\\eta) d\\eta = \\frac{1}{2} \\int_{-1}^{1} f(\\eta)g(\\eta) d\\eta\n$$\nThe standard orthogonality relation for Legendre polynomials is $\\int_{-1}^{1} P_m(\\eta)P_n(\\eta) d\\eta = \\frac{2}{2n+1} \\delta_{mn}$. The squared norm of $P_n(\\eta)$ with respect to our probability measure is therefore:\n$$\n\\langle P_n, P_n \\rangle_{\\eta} = \\frac{1}{2} \\int_{-1}^{1} (P_n(\\eta))^2 d\\eta = \\frac{1}{2} \\left( \\frac{2}{2n+1} \\right) = \\frac{1}{2n+1}\n$$\nThe one-dimensional orthonormal polynomials $\\chi_n(\\eta)$ are obtained by normalizing $P_n(\\eta)$:\n$$\n\\chi_n(\\eta) = \\frac{P_n(\\eta)}{\\sqrt{\\langle P_n, P_n \\rangle_{\\eta}}} = \\frac{P_n(\\eta)}{\\sqrt{1/(2n+1)}} = \\sqrt{2n+1} P_n(\\eta)\n$$\nWe are interested in degree $j=3$. The third Legendre polynomial is $P_3(\\eta) = \\frac{1}{2}(5\\eta^3 - 3\\eta)$. The normalization constant is $\\sqrt{2(3)+1} = \\sqrt{7}$. Therefore, the orthonormal polynomial of degree $3$ is:\n$$\n\\chi_3(\\eta) = \\sqrt{7} P_3(\\eta) = \\frac{\\sqrt{7}}{2}(5\\eta^3 - 3\\eta)\n$$\n\nFinally, we construct the desired tensor-product basis function $\\psi_{2,3}(\\xi,\\eta)$ by multiplying $\\phi_2(\\xi)$ and $\\chi_3(\\eta)$:\n$$\n\\psi_{2,3}(\\xi,\\eta) = \\phi_2(\\xi) \\chi_3(\\eta) = \\left( \\frac{\\xi^2 - 1}{\\sqrt{2}} \\right) \\left( \\frac{\\sqrt{7}}{2}(5\\eta^3 - 3\\eta) \\right)\n$$\nCombining the constant terms, we get:\n$$\n\\psi_{2,3}(\\xi,\\eta) = \\frac{\\sqrt{7}}{2\\sqrt{2}} (\\xi^2 - 1)(5\\eta^3 - 3\\eta)\n$$\nTo present this in a simplified form with a rational denominator, we multiply the numerator and denominator by $\\sqrt{2}$:\n$$\n\\psi_{2,3}(\\xi,\\eta) = \\frac{\\sqrt{14}}{4} (\\xi^2 - 1)(5\\eta^3 - 3\\eta)\n$$\nThis is the final, explicit closed-form expression.", "answer": "$$\n\\boxed{\\frac{\\sqrt{14}}{4} (\\xi^2 - 1)(5\\eta^3 - 3\\eta)}\n$$", "id": "2439629"}, {"introduction": "With an understanding of the polynomial bases, we can now apply them to solve physical problems involving uncertainty. The one-dimensional heat equation with a random thermal diffusivity serves as a classic model problem for comparing the two dominant spectral approaches: the intrusive Stochastic Galerkin (SG) method and the non-intrusive Stochastic Collocation (SC) method. The SG method transforms the stochastic partial differential equation (PDE) into a larger, coupled system of deterministic PDEs, while the SC method relies on repeatedly solving the original deterministic PDE at specific parameter values. This comprehensive practice [@problem_id:2439592] offers a direct, hands-on comparison of these two techniques, revealing the fundamental trade-offs between the analytical effort of SG and the implementation simplicity of SC.", "problem": "Consider the one-dimensional heat equation with homogeneous Dirichlet boundary conditions and a random thermal diffusivity,\n$$\nu_t(x,t,\\alpha) = \\alpha \\, u_{xx}(x,t,\\alpha), \\quad x \\in [0,1], \\ t \\ge 0,\n$$\n$$\nu(0,t,\\alpha)=0, \\quad u(1,t,\\alpha)=0, \\quad u(x,0,\\alpha)=\\sin(\\pi x),\n$$\nwhere the thermal diffusivity $\\alpha$ is a scalar random variable that is uniformly distributed on $[a,b]$, with $0<a<b<\\infty$. Introduce the standardized random variable $\\xi \\sim \\mathcal{U}([-1,1])$ through the affine map\n$$\n\\alpha(\\xi) = \\frac{a+b}{2} + \\frac{b-a}{2}\\,\\xi.\n$$\nYou will compare two uncertainty quantification approaches: the stochastic Galerkin method (SG) using polynomial chaos expansion with Legendre polynomials, and the stochastic collocation method (SC) using Gauss–Legendre quadrature.\n\nFundamental bases you may use include separation of variables for linear partial differential equations, orthogonality and completeness of Legendre polynomials on $[-1,1]$ with respect to the uniform measure, and standard properties of expectations and variances.\n\nTask requirements:\n- Derive, from first principles, the coupled deterministic system obtained by projecting the heat equation onto the orthonormal Legendre polynomial chaos basis $\\{\\psi_k(\\xi)\\}_{k=0}^p$ with respect to the probability measure of $\\xi$. Assume\n$$\n\\psi_k(\\xi) = \\sqrt{2k+1}\\,P_k(\\xi),\n$$\nwhere $P_k$ is the degree-$k$ Legendre polynomial on $[-1,1]$, so that $\\mathbb{E}[\\psi_i(\\xi)\\psi_j(\\xi)] = \\delta_{ij}$ with respect to the uniform probability measure on $[-1,1]$. Use the ansatz\n$$\nu(x,t,\\xi) \\approx \\sum_{k=0}^{p} u_k(x,t) \\, \\psi_k(\\xi),\n$$\nand show how the modal coefficients are coupled through expectations of the form $\\mathbb{E}[\\alpha(\\xi)\\psi_i(\\xi)\\psi_j(\\xi)]$.\n- Specialize the derivation to the given initial condition $u(x,0,\\xi)=\\sin(\\pi x)$ and homogeneous Dirichlet boundaries, using separation of variables to identify the spatial dependence. Reduce the stochastic Galerkin system to a linear system of ordinary differential equations in time for the stochastic amplitudes, and express the mean and variance of $u(x,t,\\xi)$ at a fixed point $x^\\star \\in (0,1)$ in terms of the polynomial chaos coefficients.\n- For the stochastic collocation method, use Gauss–Legendre quadrature with $Q$ points $\\{(\\xi_q,w_q)\\}_{q=1}^Q$ on $[-1,1]$ to approximate expectations with respect to the uniform distribution on $[-1,1]$. Explicitly state how to compute the approximate mean and variance at $x^\\star$ and time $t$ in terms of the deterministic solutions evaluated at the collocation nodes.\n\nNumerical task and output specification:\n- Implement a complete program that:\n  1) Constructs the stochastic Galerkin coupling matrix using numerical quadrature over $[-1,1]$,\n  2) Evolves the stochastic Galerkin system in time for the amplitudes starting from the derived initial condition,\n  3) Computes the stochastic collocation estimates of the mean and variance using Gauss–Legendre quadrature,\n  4) Returns, for each test case, the absolute error in the mean and the absolute error in the variance at the specified point $x^\\star$ and time $t$, comparing SG against SC.\n- Use the following test suite of parameter sets $(a,b,p,Q,t,x^\\star)$:\n  - Test $1$: $(a,b,p,Q,t,x^\\star)= (0.1, 0.5, 3, 8, 0.3, 0.5)$.\n  - Test $2$: $(a,b,p,Q,t,x^\\star)= (0.1, 0.5, 5, 6, 0.0, 0.5)$.\n  - Test $3$: $(a,b,p,Q,t,x^\\star)= (0.01, 1.0, 5, 16, 0.2, 0.3)$.\n  - Test $4$: $(a,b,p,Q,t,x^\\star)= (0.1, 0.9, 7, 20, 2.0, 0.5)$.\n- For expectations with respect to $\\xi \\sim \\mathcal{U}([-1,1])$, remember that $\\mathbb{E}[f(\\xi)] = \\frac{1}{2}\\int_{-1}^{1} f(\\xi)\\, d\\xi$, and approximate the integral with Gauss–Legendre quadrature as needed.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list of floats: the absolute error in the mean and the absolute error in the variance at $(x^\\star,t)$. The exact format must be\n$$\n[[e_{1,\\mathrm{mean}},e_{1,\\mathrm{var}}],[e_{2,\\mathrm{mean}},e_{2,\\mathrm{var}}],[e_{3,\\mathrm{mean}},e_{3,\\mathrm{var}}],[e_{4,\\mathrm{mean}},e_{4,\\mathrm{var}}]]\n$$\nwith no spaces.", "solution": "The problem presented is a valid, well-posed problem in computational engineering, specifically in the field of uncertainty quantification. It is scientifically grounded in the physics of heat transfer and the mathematics of stochastic analysis and numerical methods. All necessary data and conditions are provided. I will therefore proceed with a complete solution.\n\nThe analysis begins with the one-dimensional heat equation with a random thermal diffusivity $\\alpha$:\n$$\nu_t(x,t,\\alpha) = \\alpha \\, u_{xx}(x,t,\\alpha), \\quad x \\in [0,1], \\ t \\ge 0\n$$\nsubject to homogeneous Dirichlet boundary conditions $u(0,t,\\alpha)=0, u(1,t,\\alpha)=0$ and initial condition $u(x,0,\\alpha)=\\sin(\\pi x)$. The diffusivity $\\alpha$ is uniformly distributed on $[a,b]$, which is mapped from a standard uniform random variable $\\xi \\sim \\mathcal{U}([-1,1])$ via the affine transformation:\n$$\n\\alpha(\\xi) = \\bar{\\alpha} + \\hat{\\alpha}\\xi = \\frac{a+b}{2} + \\frac{b-a}{2}\\xi\n$$\nHere, $\\bar{\\alpha} = \\mathbb{E}[\\alpha]$ is the mean and $\\hat{\\alpha} = (b-a)/2$ is a scaling factor.\n\n**Stochastic Galerkin Method Derivation**\n\nThe Stochastic Galerkin (SG) method seeks an approximate solution in the form of a Polynomial Chaos Expansion (PCE):\n$$\nu(x,t,\\xi) \\approx u_p(x,t,\\xi) = \\sum_{k=0}^{p} u_k(x,t) \\psi_k(\\xi)\n$$\nwhere $\\{\\psi_k(\\xi)\\}$ are orthonormal Legendre polynomials on $[-1,1]$ with respect to the uniform probability measure, satisfying $\\mathbb{E}[\\psi_i(\\xi)\\psi_j(\\xi)] = \\frac{1}{2}\\int_{-1}^{1} \\psi_i(\\xi)\\psi_j(\\xi) d\\xi = \\delta_{ij}$.\n\nWe substitute this ansatz into the governing PDE:\n$$\n\\frac{\\partial}{\\partial t} \\sum_{k=0}^{p} u_k(x,t) \\psi_k(\\xi) = \\alpha(\\xi) \\frac{\\partial^2}{\\partial x^2} \\sum_{k=0}^{p} u_k(x,t) \\psi_k(\\xi)\n$$\nBy linearity of the operators, this becomes:\n$$\n\\sum_{k=0}^{p} \\frac{\\partial u_k(x,t)}{\\partial t} \\psi_k(\\xi) = \\alpha(\\xi) \\sum_{k=0}^{p} \\frac{\\partial^2 u_k(x,t)}{\\partial x^2} \\psi_k(\\xi)\n$$\nThe Galerkin projection is performed by taking the inner product of this equation with each basis function $\\psi_j(\\xi)$ for $j=0, \\dots, p$. The inner product is defined by the expectation operator $\\mathbb{E}[\\cdot]$.\n$$\n\\mathbb{E}\\left[ \\psi_j(\\xi) \\sum_{k=0}^{p} \\frac{\\partial u_k}{\\partial t} \\psi_k(\\xi) \\right] = \\mathbb{E}\\left[ \\psi_j(\\xi) \\alpha(\\xi) \\sum_{k=0}^{p} \\frac{\\partial^2 u_k}{\\partial x^2} \\psi_k(\\xi) \\right]\n$$\nUsing the linearity of the expectation and the orthogonality of the basis functions, the left-hand side simplifies:\n$$\n\\sum_{k=0}^{p} \\frac{\\partial u_k}{\\partial t} \\mathbb{E}[\\psi_j \\psi_k] = \\sum_{k=0}^{p} \\frac{\\partial u_k}{\\partial t} \\delta_{jk} = \\frac{\\partial u_j}{\\partial t}\n$$\nThe right-hand side becomes:\n$$\n\\sum_{k=0}^{p} \\frac{\\partial^2 u_k}{\\partial x^2} \\mathbb{E}[\\alpha(\\xi) \\psi_j(\\xi) \\psi_k(\\xi)]\n$$\nThis results in a coupled system of $p+1$ deterministic PDEs for the modal coefficients $u_j(x,t)$:\n$$\n\\frac{\\partial u_j}{\\partial t}(x,t) = \\sum_{k=0}^{p} C_{jk} \\frac{\\partial^2 u_k}{\\partial x^2}(x,t), \\quad j = 0, \\dots, p\n$$\nwhere the coupling matrix entries are given by $C_{jk} = \\mathbb{E}[\\alpha(\\xi) \\psi_j(\\xi) \\psi_k(\\xi)]$. Substituting the expression for $\\alpha(\\xi)$:\n$$\nC_{jk} = \\mathbb{E}[(\\bar{\\alpha} + \\hat{\\alpha}\\xi) \\psi_j \\psi_k] = \\bar{\\alpha} \\mathbb{E}[\\psi_j \\psi_k] + \\hat{\\alpha} \\mathbb{E}[\\xi \\psi_j \\psi_k] = \\bar{\\alpha} \\delta_{jk} + \\hat{\\alpha} \\mathbb{E}[\\xi \\psi_j \\psi_k]\n$$\nThe matrix with entries $\\mathbb{E}[\\xi \\psi_j \\psi_k]$ is tridiagonal due to the three-term recurrence relation for Legendre polynomials. This makes the coupling matrix $\\mathbf{C}$ a symmetric tridiagonal matrix.\n\n**Simplification for the Specific Problem**\n\nThe form of the initial and boundary conditions allows for separation of variables. The solution retains the spatial form $\\sin(\\pi x)$ for all time. Thus, we can posit a solution of the form $u(x,t,\\xi) = \\hat{u}(t,\\xi) \\sin(\\pi x)$. Substituting this into the original PDE yields a stochastic ordinary differential equation for the amplitude $\\hat{u}(t,\\xi)$:\n$$\n\\frac{d\\hat{u}}{dt} = -\\pi^2 \\alpha(\\xi) \\hat{u}\n$$\nThe initial condition $u(x,0,\\xi)=\\sin(\\pi x)$ implies $\\hat{u}(0,\\xi)=1$. The exact solution is $\\hat{u}(t,\\xi) = \\exp(-\\pi^2 \\alpha(\\xi) t)$. The full stochastic solution is $u(x,t,\\xi) = \\exp(-\\pi^2 \\alpha(\\xi) t) \\sin(\\pi x)$.\n\nThis separation enables writing the PCE coefficients as $u_k(x,t) = \\hat{u}_k(t) \\sin(\\pi x)$. Substituting this into the coupled PDE system and noting that $\\frac{\\partial^2}{\\partial x^2}(\\hat{u}_k \\sin(\\pi x)) = -\\pi^2 \\hat{u}_k \\sin(\\pi x)$, we obtain a system of linear ODEs for the temporal coefficients $\\hat{u}_k(t)$:\n$$\n\\frac{d \\hat{u}_j(t)}{dt} = -\\pi^2 \\sum_{k=0}^{p} C_{jk} \\hat{u}_k(t)\n$$\nIn vector form, this is $\\frac{d\\hat{\\mathbf{u}}}{dt} = -\\pi^2 \\mathbf{C} \\hat{\\mathbf{u}}(t)$, where $\\hat{\\mathbf{u}}(t) = [\\hat{u}_0(t), \\dots, \\hat{u}_p(t)]^T$.\n\nThe initial condition for this system is found by projecting $\\hat{u}(0,\\xi)=1$ onto the basis:\n$$\n\\hat{u}_k(0) = \\mathbb{E}[1 \\cdot \\psi_k(\\xi)] = \\mathbb{E}[\\psi_0(\\xi) \\psi_k(\\xi)] = \\delta_{0k}\n$$\nSo, the initial vector is $\\hat{\\mathbf{u}}(0) = [1, 0, \\dots, 0]^T$. The solution to the ODE system is given by the matrix exponential:\n$$\n\\hat{\\mathbf{u}}(t) = e^{-\\pi^2 \\mathbf{C} t} \\hat{\\mathbf{u}}(0)\n$$\n\n**Mean and Variance from Stochastic Galerkin**\n\nThe mean of the solution is:\n$$\n\\mathbb{E}[u_p(x,t,\\xi)] = \\mathbb{E}\\left[\\sin(\\pi x) \\sum_{k=0}^{p} \\hat{u}_k(t) \\psi_k(\\xi)\\right] = \\sin(\\pi x) \\sum_{k=0}^{p} \\hat{u}_k(t) \\mathbb{E}[\\psi_k(\\xi)]\n$$\nSince $\\mathbb{E}[\\psi_k(\\xi)]=\\delta_{0k}$, the mean is determined solely by the zeroth-order coefficient:\n$$\n\\mu_{SG}(x,t) = \\mathbb{E}[u_p(x,t,\\xi)] = \\hat{u}_0(t) \\sin(\\pi x)\n$$\nThe variance is $\\mathrm{Var}(u_p) = \\mathbb{E}[u_p^2] - (\\mathbb{E}[u_p])^2$. The expected value of the square is:\n$$\n\\mathbb{E}[u_p^2] = \\sin^2(\\pi x) \\mathbb{E}\\left[\\left(\\sum_{k=0}^{p} \\hat{u}_k(t) \\psi_k(\\xi)\\right)^2\\right] = \\sin^2(\\pi x) \\sum_{i=0}^{p}\\sum_{j=0}^{p} \\hat{u}_i(t)\\hat{u}_j(t) \\mathbb{E}[\\psi_i\\psi_j] = \\sin^2(\\pi x) \\sum_{k=0}^{p} \\hat{u}_k(t)^2\n$$\nThus, the variance is given by the sum of squares of the higher-order coefficients:\n$$\n\\sigma^2_{SG}(x,t) = \\left( \\sin^2(\\pi x) \\sum_{k=0}^{p} \\hat{u}_k(t)^2 \\right) - \\left( \\hat{u}_0(t) \\sin(\\pi x) \\right)^2 = \\sin^2(\\pi x) \\sum_{k=1}^{p} \\hat{u}_k(t)^2\n$$\n\n**Stochastic Collocation Method**\n\nThe Stochastic Collocation (SC) method approximates expectations by numerical quadrature. For a given function $f(\\xi)$, its expectation is approximated using a $Q$-point Gauss-Legendre quadrature rule:\n$$\n\\mathbb{E}[f(\\xi)] = \\frac{1}{2}\\int_{-1}^1 f(\\xi) d\\xi \\approx \\sum_{q=1}^{Q} \\frac{w'_q}{2} f(\\xi_q) = \\sum_{q=1}^{Q} w_q f(\\xi_q)\n$$\nwhere $(\\xi_q, w'_q)$ are the standard Gauss-Legendre nodes and weights on $[-1,1]$, and $w_q = w'_q/2$ are the corresponding probability weights.\n\nThe procedure is as follows:\n1.  For each quadrature node $\\xi_q$, determine the physical parameter $\\alpha_q = \\alpha(\\xi_q)$.\n2.  Solve the deterministic problem for each $\\alpha_q$. In this case, we evaluate the exact solution at the point $(x^\\star, t)$:\n    $$\n    U_q = u(x^\\star, t; \\alpha_q) = e^{-\\pi^2 \\alpha_q t} \\sin(\\pi x^\\star)\n    $$\n3.  The mean is approximated as the weighted sum of these solutions:\n    $$\n    \\mu_{SC}(x^\\star, t) \\approx \\sum_{q=1}^{Q} w_q U_q\n    $$\n4.  The variance is approximated as:\n    $$\n    \\sigma^2_{SC}(x^\\star, t) \\approx \\mathbb{E}[u^2] - (\\mathbb{E}[u])^2 \\approx \\left(\\sum_{q=1}^{Q} w_q U_q^2\\right) - \\mu_{SC}^2\n    $$\nFor a sufficiently large number of collocation points $Q$, this method provides a highly accurate estimate of the true statistics, serving as a reliable benchmark against which the SG approximation can be compared.\n\n**Numerical Implementation**\n\nThe numerical task involves implementing both methods.\nFor SG, the matrix $\\mathbf{C}$ is constructed using high-order numerical quadrature. The ODE system is solved by diagonalizing $\\mathbf{C} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^T$ and computing $\\hat{\\mathbf{u}}(t) = \\mathbf{V} e^{-\\pi^2 \\mathbf{\\Lambda} t} \\mathbf{V}^T \\hat{\\mathbf{u}}(0)$.\nFor SC, the deterministic solution is evaluated at each of the $Q$ collocation nodes, and the results are combined using the quadrature rule.\nThe absolute errors $| \\mu_{SG} - \\mu_{SC} |$ and $| \\sigma^2_{SG} - \\sigma^2_{SC} |$ are then computed for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_legendre, eval_legendre\nfrom scipy.linalg import eigh, expm\n\ndef solve():\n    \"\"\"\n    Solves the given problem by comparing Stochastic Galerkin (SG) and\n    Stochastic Collocation (SC) methods for a 1D heat equation with a\n    random parameter.\n    \"\"\"\n    test_cases = [\n        # (a, b, p, Q, t, x_star)\n        (0.1, 0.5, 3, 8, 0.3, 0.5),\n        (0.1, 0.5, 5, 6, 0.0, 0.5),\n        (0.01, 1.0, 5, 16, 0.2, 0.3),\n        (0.1, 0.9, 7, 20, 2.0, 0.5),\n    ]\n\n    results = []\n\n    for a, b, p, Q, t, x_star in test_cases:\n        # --- Stochastic Galerkin (SG) Method ---\n        \n        # 1. Construct the coupling matrix C using numerical quadrature.\n        # The integrand is a polynomial of degree up to 1+2p.\n        # Gauss-Legendre quadrature with N_quad points is exact for polynomials\n        # of degree up to 2*N_quad - 1. We need 2*N_quad - 1 >= 1+2p => N_quad >= p+1.\n        # We choose a safe number of points.\n        N_quad_C = 2 * p + 2\n        nodes_C, weights_C = roots_legendre(N_quad_C)\n        \n        # Orthonormal Legendre polynomials: psi_k(xi) = sqrt(2k+1) * P_k(xi)\n        psi_vals = np.zeros((p + 1, N_quad_C))\n        for k in range(p + 1):\n            psi_vals[k, :] = np.sqrt(2 * k + 1) * eval_legendre(k, nodes_C)\n            \n        alpha_vals = (a + b) / 2.0 + (b - a) / 2.0 * nodes_C\n        \n        C = np.zeros((p + 1, p + 1))\n        # C_jk = E[alpha * psi_j * psi_k] = 0.5 * integral(alpha * psi_j * psi_k, dxi)\n        for j in range(p + 1):\n            for k in range(j, p + 1): # Matrix is symmetric\n                integrand = alpha_vals * psi_vals[j, :] * psi_vals[k, :]\n                integral_val = np.sum(weights_C * integrand)\n                C[j, k] = 0.5 * integral_val\n                if j != k:\n                    C[k, j] = C[j, k]\n\n        # 2. Evolve the system of ODEs for the coefficients\n        u_hat_0 = np.zeros(p + 1)\n        u_hat_0[0] = 1.0\n\n        # Solution: u_hat(t) = expm(-pi^2 * C * t) @ u_hat_0\n        # For symmetric C, diagonalization is numerically stable and efficient.\n        evals, V = eigh(C)\n        exp_lambda_t = np.exp(-np.pi**2 * evals * t)\n        u_hat_t = V @ (exp_lambda_t * (V.T @ u_hat_0))\n\n        # 3. Compute SG mean and variance\n        sin_x_star = np.sin(np.pi * x_star)\n        mean_sg = u_hat_t[0] * sin_x_star\n        var_sg = np.sum(u_hat_t[1:]**2) * sin_x_star**2\n\n        # --- Stochastic Collocation (SC) Method ---\n        \n        # 1. Get Gauss-Legendre nodes and weights for SC\n        nodes_sc, weights_sc = roots_legendre(Q)\n        prob_weights_sc = weights_sc / 2.0\n        \n        # 2. Evaluate deterministic solution at each node\n        alpha_sc = (a + b) / 2.0 + (b - a) / 2.0 * nodes_sc\n        sol_vals_sc = np.exp(-np.pi**2 * alpha_sc * t) * sin_x_star\n        \n        # 3. Compute SC mean and variance\n        mean_sc = np.sum(prob_weights_sc * sol_vals_sc)\n        \n        # Var = E[u^2] - (E[u])^2\n        var_sc = np.sum(prob_weights_sc * sol_vals_sc**2) - mean_sc**2\n\n        # --- Comparison ---\n        error_mean = np.abs(mean_sg - mean_sc)\n        error_var = np.abs(var_sg - var_sc)\n\n        results.append(f\"[{error_mean},{error_var}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2439592"}, {"introduction": "In most engineering applications, we do not have an exact analytical solution to verify the accuracy of our numerical approximations. Therefore, we need reliable methods to estimate the error of our simulation using only the computational results themselves. A common and effective technique is a-posteriori error estimation, where we infer the accuracy by comparing solutions from different levels of approximation, such as those obtained using successive polynomial degrees. This exercise [@problem_id:2439622] guides you through implementing a practical error indicator for a Stochastic Collocation solution, a skill essential for building confidence in your numerical results and for developing adaptive algorithms that automatically refine the solution to meet a desired accuracy.", "problem": "Consider the following parametric model problem defined for a scalar random input. Let the random variable be denoted by $\\xi \\sim \\mathcal{U}([-1,1])$ (uniform distribution on $[-1,1]$). Define the parameterized coefficient by $a(\\xi) = a_0 + a_1 \\xi$ with $a_0 \\in \\mathbb{R}$ and $a_1 \\in \\mathbb{R}$, and assume $a_0 > |a_1|$ so that $a(\\xi) &gt; 0$ for all $\\xi \\in [-1,1]$. Consider the linear ordinary differential equation\n$$\n\\frac{dy}{dt}(t;\\xi) + a(\\xi)\\,y(t;\\xi) = 1,\\quad t \\in (0,T], \\quad y(0;\\xi) = y_0,\n$$\nwith $T \\in \\mathbb{R}_{&gt;0}$ and $y_0 \\in \\mathbb{R}$. The unique solution at time $T$ is\n$$\nQ(\\xi) = y(T;\\xi) = y_0 e^{-a(\\xi)\\,T} + \\frac{1 - e^{-a(\\xi)\\,T}}{a(\\xi)}.\n$$\nFor a given nonnegative integer polynomial degree $p \\in \\mathbb{Z}_{\\ge 0}$, define:\n- The stochastic collocation approximation $u_p(\\xi)$ as the unique polynomial in $\\xi$ of degree at most $p$ that interpolates $Q(\\xi)$ at the $p+1$ distinct collocation points given by the zeros of the Legendre polynomial of degree $p+1$ on $(-1,1)$.\n- The higher-degree approximation $u_{p+1}(\\xi)$ analogously as the unique polynomial of degree at most $p+1$ interpolating $Q(\\xi)$ at the $p+2$ zeros of the Legendre polynomial of degree $p+2$.\n\nDefine the a-posteriori error indicator by the root-mean-square (with respect to the law of $\\xi$) of the difference between these two approximations:\n$$\n\\eta_p \\equiv \\left( \\mathbb{E}\\left[ \\left(u_p(\\xi)-u_{p+1}(\\xi)\\right)^2 \\right] \\right)^{1/2}\n= \\left( \\frac{1}{2} \\int_{-1}^1 \\left(u_p(\\xi)-u_{p+1}(\\xi)\\right)^2 \\, d\\xi \\right)^{1/2}.\n$$\n\nYour task is to write a complete program that, for each parameter set in the test suite below, computes the scalar error indicator $\\eta_p$ as defined above and reports it as a floating-point number.\n\nTest suite (each case is $(a_0,a_1,y_0,T,p)$):\n- Case $1$: $(1.0, 0.3, 0.5, 1.0, 2)$.\n- Case $2$: $(1.0, 0.8, 1.0, 2.0, 0)$.\n- Case $3$: $(1.4, 0.0, 0.7, 0.3, 3)$.\n- Case $4$: $(2.0, 0.99, 1.0, 3.0, 4)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the cases above as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the computed floating-point value of $\\eta_p$ for the corresponding case.\n- No additional text or lines should be printed.", "solution": "The problem presented is a well-defined and standard exercise in the field of computational engineering, specifically in uncertainty quantification using stochastic spectral methods. It is scientifically grounded, self-contained, and algorithmically solvable. All provided constants and functions are clearly defined, and the constraints ensure the problem is well-posed. Therefore, the problem is valid, and a solution will be provided.\n\nThe objective is to compute an a-posteriori error indicator, $\\eta_p$, for a stochastic collocation approximation of the solution to a parametric ordinary differential equation (ODE).\n\nThe governing ODE is:\n$$\n\\frac{dy}{dt}(t;\\xi) + a(\\xi)\\,y(t;\\xi) = 1, \\quad y(0;\\xi) = y_0\n$$\nwhere $\\xi$ is a random variable following a uniform distribution on $[-1,1]$, denoted $\\xi \\sim \\mathcal{U}([-1,1])$. The coefficient $a(\\xi)$ is an affine function of $\\xi$:\n$$\na(\\xi) = a_0 + a_1 \\xi\n$$\nThe problem states that $a_0 > |a_1|$, which guarantees that $a(\\xi) > 0$ for all $\\xi \\in [-1,1]$. This condition is critical as it ensures the solution is well-behaved and free of singularities. The analytical solution for the quantity of interest $Q(\\xi) = y(T;\\xi)$ is given by:\n$$\nQ(\\xi) = y_0 e^{-a(\\xi)\\,T} + \\frac{1 - e^{-a(\\xi)\\,T}}{a(\\xi)}\n$$\n\nThe core of the problem involves two polynomial approximations to $Q(\\xi)$:\n1.  $u_p(\\xi)$: A polynomial of degree at most $p$, which is constructed by interpolating the exact solution $Q(\\xi)$ at a specific set of $p+1$ points. These points, known as collocation points, are the roots of the Legendre polynomial of degree $p+1$, $P_{p+1}(\\xi)$.\n2.  $u_{p+1}(\\xi)$: Similarly, a polynomial of degree at most $p+1$ that interpolates $Q(\\xi)$ at the $p+2$ roots of the Legendre polynomial of degree $p+2$, $P_{p+2}(\\xi)$.\n\nThe error indicator, $\\eta_p$, is defined as the root-mean-square norm of the difference between these two successive approximations. The expectation $\\mathbb{E}[\\cdot]$ for a function $f(\\xi)$ with $\\xi \\sim \\mathcal{U}([-1,1])$ is given by $\\frac{1}{2}\\int_{-1}^{1} f(\\xi) \\,d\\xi$. Thus, $\\eta_p$ is computed as:\n$$\n\\eta_p = \\left( \\mathbb{E}\\left[ \\left(u_p(\\xi)-u_{p+1}(\\xi)\\right)^2 \\right] \\right)^{1/2} = \\left( \\frac{1}{2} \\int_{-1}^1 \\left(u_p(\\xi)-u_{p+1}(\\xi)\\right)^2 \\, d\\xi \\right)^{1/2}\n$$\n\nThe computational strategy to calculate $\\eta_p$ for each test case is as follows:\n1.  **Parameter Instantiation**: For a given set of parameters $(a_0, a_1, y_0, T, p)$, define the specific functions $a(\\xi)$ and $Q(\\xi)$.\n2.  **Construction of $u_p(\\xi)$**:\n    a. Determine the $p+1$ collocation points by finding the roots of the Legendre polynomial $P_{p+1}(\\xi)$. This is accomplished using a standard numerical routine, such as `numpy.polynomial.legendre.leggauss(p+1)`.\n    b. Evaluate the exact solution $Q(\\xi)$ at these $p+1$ points to obtain the pairs $(\\xi_i, Q(\\xi_i))$.\n    c. Construct the unique interpolating polynomial $u_p(\\xi)$ of degree $p$ that passes through these points. A robust method is to use Lagrange interpolation, for which `scipy.interpolate.lagrange` provides a convenient implementation, returning a polynomial object.\n3.  **Construction of $u_{p+1}(\\xi)$**: Repeat the process described in step 2 for the next higher degree. Find the $p+2$ roots of $P_{p+2}(\\xi)$, evaluate $Q(\\xi)$ at these new points, and construct the interpolating polynomial $u_{p+1}(\\xi)$.\n4.  **Error Calculation**:\n    a. Define the difference polynomial $d(\\xi) = u_p(\\xi) - u_{p+1}(\\xi)$. Since $u_p$ and $u_{p+1}$ are represented as polynomial objects, their difference is also a polynomial object.\n    b. The integrand is $(d(\\xi))^2$, which is also a polynomial. The integral of a polynomial can be computed exactly. We form the polynomial $(d(\\xi))^2$ and find its antiderivative. The definite integral over $[-1, 1]$ is then calculated by evaluating the antiderivative at the limits of integration.\n    c. Finally, $\\eta_p$ is computed by taking the square root of half the value of this integral.\n\nA special case arises when $a_1 = 0$, as in Case 3. Here, $a(\\xi) = a_0$ is a constant. Consequently, the solution $Q(\\xi)$ is also a constant, independent of $\\xi$. Any polynomial interpolation of a constant function results in that same constant. Therefore, $u_p(\\xi) = u_{p+1}(\\xi) = Q$, their difference is zero, and $\\eta_p$ is exactly $0$. This case serves as a sanity check for the correctness of the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import lagrange\nfrom numpy.polynomial.legendre import leggauss\n\ndef solve():\n    \"\"\"\n    Computes the a-posteriori error indicator for a stochastic collocation method\n    applied to a parametric ODE.\n    \"\"\"\n    # Test suite: (a_0, a_1, y_0, T, p)\n    test_cases = [\n        (1.0, 0.3, 0.5, 1.0, 2),\n        (1.0, 0.8, 1.0, 2.0, 0),\n        (1.4, 0.0, 0.7, 0.3, 3),\n        (2.0, 0.99, 1.0, 3.0, 4),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a0, a1, y0, T, p = case\n\n        # A special case where the coefficient a(xi) is deterministic.\n        # In this situation, the solution Q is a constant, so any polynomial\n        # interpolant will also be that constant. The difference u_p - u_{p+1}\n        # is identically zero, hence the error indicator eta_p is zero.\n        if a1 == 0.0:\n            results.append(0.0)\n            continue\n\n        # Define the parameterized coefficient a(xi)\n        def a(xi):\n            return a0 + a1 * xi\n\n        # Define the exact solution Q(xi). The condition a0 > |a1| ensures\n        # the denominator a(xi) is never zero for xi in [-1, 1].\n        def Q(xi):\n            a_val = a(xi)\n            exp_term = np.exp(-a_val * T)\n            # The term (1 - exp(-x))/x is evaluated robustly. While a_val > 0 is\n            # guaranteed, this form would be stable even if a_val were close to 0.\n            term2 = (1.0 - exp_term) / a_val\n            return y0 * exp_term + term2\n\n        # --- Construct u_p(xi), the polynomial of degree p ---\n        p_degree = p\n        # Collocation points are the p+1 zeros of the Legendre polynomial of degree p+1.\n        num_points_p = p_degree + 1\n        nodes_p, _ = leggauss(num_points_p)\n        # Evaluate the exact solution Q at these nodes.\n        q_values_p = Q(nodes_p)\n        # Create the interpolating polynomial using Lagrange interpolation.\n        # The result is a numpy.poly1d object.\n        u_p = lagrange(nodes_p, q_values_p)\n\n        # --- Construct u_{p+1}(xi), the polynomial of degree p+1 ---\n        p1_degree = p + 1\n        # Collocation points are the p+2 zeros of the Legendre polynomial of degree p+2.\n        num_points_p1 = p1_degree + 1\n        nodes_p1, _ = leggauss(num_points_p1)\n        # Evaluate the exact solution Q at these new nodes.\n        q_values_p1 = Q(nodes_p1)\n        # Create the interpolating polynomial.\n        u_p1 = lagrange(nodes_p1, q_values_p1)\n\n        # --- Compute the error indicator eta_p ---\n        # The difference d(xi) = u_p(xi) - u_{p+1}(xi) is a polynomial.\n        d = u_p - u_p1\n        \n        # The squared difference is also a polynomial.\n        d_squared = d * d\n        \n        # We integrate d_squared from -1 to 1 exactly.\n        # poly.integ() returns the antiderivative polynomial.\n        integral_poly = d_squared.integ()\n        integral_value = integral_poly(1.0) - integral_poly(-1.0)\n        \n        # Compute eta_p according to its definition.\n        eta_p = np.sqrt(integral_value / 2.0)\n        \n        results.append(eta_p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "2439622"}]}