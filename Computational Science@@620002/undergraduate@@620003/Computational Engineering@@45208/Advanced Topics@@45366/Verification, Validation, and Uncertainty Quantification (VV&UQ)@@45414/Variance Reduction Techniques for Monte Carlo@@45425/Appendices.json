{"hands_on_practices": [{"introduction": "The control variates technique is a powerful method for reducing variance by leveraging the correlation between our quantity of interest and another random variable with a known expectation. This practice asks you to move beyond qualitative understanding and derive the exact, quantitative benefit of an optimally chosen control variate. By working through this analytical exercise [@problem_id:2449189], you will gain a deeper appreciation for the mathematical underpinnings of this method and see how its effectiveness is directly tied to the statistical properties of the system being studied.", "problem": "Let $X$ be a normally distributed random variable with mean $\\,\\mu\\,$ and variance $\\,\\sigma^{2}\\,$, where $\\,\\mu \\neq 0\\,$ and $\\,\\sigma^{2} > 0\\,$. The goal is to estimate $\\,\\theta = \\mathbb{E}[X^{2}]\\,$ via Monte Carlo using $\\,n\\,$ independent and identically distributed (i.i.d.) samples $\\,X_{1},\\dots,X_{n}\\,$.\n\nConsider the following unbiased estimators of $\\,\\theta\\,$:\n- The direct estimator $\\,\\hat{\\theta}_{\\mathrm{dir}} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}\\,$.\n- The control variate (CV) estimator $\\,\\hat{\\theta}_{\\mathrm{cv}} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(X_{i}^{2} - \\beta\\,(X_{i}-\\mu)\\right)\\,$, where $\\,\\mu = \\mathbb{E}[X]\\,$ is known and the coefficient $\\,\\beta\\,$ is chosen to minimize the estimator variance.\n\nDerive the exact ratio $\\,\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})}\\,$ in closed form as a simplified expression in terms of $\\,\\mu\\,$ and $\\,\\sigma^{2}\\,$ only (it must not depend on $\\,n\\,$). Provide your final answer as a single analytic expression. Do not approximate.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The random variable $X$ is normally distributed with mean $\\mu$ and variance $\\sigma^{2}$.\n- Constraints: $\\mu \\neq 0$ and $\\sigma^{2} > 0$.\n- The parameter to be estimated is $\\theta = \\mathbb{E}[X^{2}]$.\n- The estimation is performed using $n$ independent and identically distributed (i.i.d.) samples $X_{1}, \\dots, X_{n}$.\n- The direct estimator is $\\hat{\\theta}_{\\mathrm{dir}} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}$.\n- The control variate (CV) estimator is $\\hat{\\theta}_{\\mathrm{cv}} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(X_{i}^{2} - \\beta\\,(X_{i}-\\mu)\\right)$.\n- The mean $\\mu = \\mathbb{E}[X]$ is known.\n- The coefficient $\\beta$ is chosen to minimize $\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})$.\n- The objective is to derive the exact ratio $\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})}$ as a simplified expression in terms of $\\mu$ and $\\sigma^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in computational statistics, dealing with variance reduction for Monte Carlo estimators using control variates. All concepts—normal distribution, expectation, variance, and estimators—are fundamentally sound.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information to determine the optimal control variate coefficient $\\beta$ and subsequently calculate the required variance ratio. The existence and uniqueness of the solution are guaranteed by the principles of linear regression and variance minimization.\n- **Objective:** The problem is stated using precise, objective mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard, well-defined problem in computational engineering and statistics. I will proceed with the derivation.\n\nThe objective is to compute the ratio of the variances of two estimators for $\\theta = \\mathbb{E}[X^2]$.\nThe direct estimator is $\\hat{\\theta}_{\\mathrm{dir}} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}$. Since the samples $X_i$ are i.i.d., its variance is:\n$$\n\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n} \\mathrm{Var}(X_{i}^{2}) = \\frac{1}{n}\\mathrm{Var}(X^{2})\n$$\nThe control variate estimator is $\\hat{\\theta}_{\\mathrm{cv}} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$, where $Y_{i} = X_{i}^{2} - \\beta(X_{i}-\\mu)$. Its variance is:\n$$\n\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}}) = \\frac{1}{n}\\mathrm{Var}(Y_1) = \\frac{1}{n}\\mathrm{Var}(X^{2} - \\beta(X-\\mu))\n$$\nThe ratio to be computed is therefore:\n$$\n\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})} = \\frac{\\frac{1}{n}\\mathrm{Var}(X^{2} - \\beta(X-\\mu))}{\\frac{1}{n}\\mathrm{Var}(X^{2})} = \\frac{\\mathrm{Var}(X^{2} - \\beta(X-\\mu))}{\\mathrm{Var}(X^{2})}\n$$\nThe coefficient $\\beta$ must be chosen to minimize $\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})$. The optimal coefficient, denoted $\\beta^{*}$, is given by the well-known formula:\n$$\n\\beta^{*} = \\frac{\\mathrm{Cov}(X^{2}, X-\\mu)}{\\mathrm{Var}(X-\\mu)}\n$$\nWe must compute the components of this expression. The denominator is simply $\\mathrm{Var}(X-\\mu) = \\mathrm{Var}(X) = \\sigma^{2}$.\nFor the numerator, we compute the covariance:\n$$\n\\mathrm{Cov}(X^{2}, X-\\mu) = \\mathbb{E}[X^{2}(X-\\mu)] - \\mathbb{E}[X^{2}]\\mathbb{E}[X-\\mu]\n$$\nSince $\\mathbb{E}[X-\\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0$, the second term vanishes.\n$$\n\\mathrm{Cov}(X^{2}, X-\\mu) = \\mathbb{E}[X^{3} - \\mu X^{2}] = \\mathbb{E}[X^{3}] - \\mu \\mathbb{E}[X^{2}]\n$$\nTo compute these moments, let us define a standard normal variable $Z = \\frac{X-\\mu}{\\sigma}$, so $X = \\mu + \\sigma Z$. The moments of $Z$ are $\\mathbb{E}[Z]=0$, $\\mathbb{E}[Z^2]=1$, $\\mathbb{E}[Z^3]=0$, and $\\mathbb{E}[Z^4]=3$.\nThe second moment of $X$ is $\\mathbb{E}[X^{2}] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^{2} = \\sigma^{2} + \\mu^{2}$.\nThe third moment of $X$ is:\n$$\n\\mathbb{E}[X^{3}] = \\mathbb{E}[(\\mu + \\sigma Z)^{3}] = \\mathbb{E}[\\mu^{3} + 3\\mu^{2}\\sigma Z + 3\\mu\\sigma^{2}Z^{2} + \\sigma^{3}Z^{3}]\n$$\nUsing linearity of expectation and the moments of $Z$:\n$$\n\\mathbb{E}[X^{3}] = \\mu^{3} + 3\\mu^{2}\\sigma\\mathbb{E}[Z] + 3\\mu\\sigma^{2}\\mathbb{E}[Z^{2}] + \\sigma^{3}\\mathbb{E}[Z^{3}] = \\mu^{3} + 0 + 3\\mu\\sigma^{2}(1) + 0 = \\mu^{3} + 3\\mu\\sigma^{2}\n$$\nNow, substitute the moments back into the covariance expression:\n$$\n\\mathrm{Cov}(X^{2}, X-\\mu) = (\\mu^{3} + 3\\mu\\sigma^{2}) - \\mu(\\sigma^{2} + \\mu^{2}) = \\mu^{3} + 3\\mu\\sigma^{2} - \\mu\\sigma^{2} - \\mu^{3} = 2\\mu\\sigma^{2}\n$$\nThe optimal coefficient is therefore:\n$$\n\\beta^{*} = \\frac{2\\mu\\sigma^{2}}{\\sigma^{2}} = 2\\mu\n$$\nWith the optimal $\\beta^{*}$, the variance of the control variate term is given by:\n$$\n\\mathrm{Var}(X^{2} - \\beta^{*}(X-\\mu)) = \\mathrm{Var}(X^{2}) - \\frac{(\\mathrm{Cov}(X^{2}, X-\\mu))^{2}}{\\mathrm{Var}(X-\\mu)}\n$$\nWe need to compute $\\mathrm{Var}(X^{2})$.\n$$\n\\mathrm{Var}(X^{2}) = \\mathbb{E}[X^{4}] - (\\mathbb{E}[X^{2}])^{2}\n$$\nFirst, calculate the fourth moment of $X$:\n$$\n\\mathbb{E}[X^{4}] = \\mathbb{E}[(\\mu + \\sigma Z)^{4}] = \\mathbb{E}[\\mu^{4} + 4\\mu^{3}\\sigma Z + 6\\mu^{2}\\sigma^{2}Z^{2} + 4\\mu\\sigma^{3}Z^{3} + \\sigma^{4}Z^{4}]\n$$\n$$\n\\mathbb{E}[X^{4}] = \\mu^{4} + 4\\mu^{3}\\sigma\\mathbb{E}[Z] + 6\\mu^{2}\\sigma^{2}\\mathbb{E}[Z^{2}] + 4\\mu\\sigma^{3}\\mathbb{E}[Z^{3}] + \\sigma^{4}\\mathbb{E}[Z^{4}]\n$$\n$$\n\\mathbb{E}[X^{4}] = \\mu^{4} + 0 + 6\\mu^{2}\\sigma^{2}(1) + 0 + \\sigma^{4}(3) = \\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}\n$$\nNow we can compute $\\mathrm{Var}(X^{2})$:\n$$\n\\mathrm{Var}(X^{2}) = (\\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}) - (\\sigma^{2} + \\mu^{2})^{2}\n$$\n$$\n\\mathrm{Var}(X^{2}) = (\\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}) - (\\sigma^{4} + 2\\mu^{2}\\sigma^{2} + \\mu^{4}) = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}\n$$\nWith this result, we find the minimized variance for the control variate term:\n$$\n\\mathrm{Var}(X^{2} - \\beta^{*}(X-\\mu)) = (4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}) - \\frac{(2\\mu\\sigma^{2})^{2}}{\\sigma^{2}}\n$$\n$$\n= (4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}) - \\frac{4\\mu^{2}\\sigma^{4}}{\\sigma^{2}} = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4} - 4\\mu^{2}\\sigma^{2} = 2\\sigma^{4}\n$$\nFinally, we assemble the ratio of variances:\n$$\n\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})} = \\frac{\\mathrm{Var}(X^{2} - \\beta^{*}(X-\\mu))}{\\mathrm{Var}(X^{2})} = \\frac{2\\sigma^{4}}{4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}}\n$$\nSince $\\sigma^{2} > 0$, we can simplify the expression by dividing the numerator and denominator by $2\\sigma^{2}$:\n$$\n\\frac{2\\sigma^{4}}{2\\sigma^{2}(2\\mu^{2} + \\sigma^{2})} = \\frac{\\sigma^{2}}{2\\mu^{2} + \\sigma^{2}}\n$$\nThis is the final, simplified expression for the variance ratio. It depends only on $\\mu$ and $\\sigma^{2}$, as required.", "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{2\\mu^{2} + \\sigma^{2}}}\n$$", "id": "2449189"}, {"introduction": "While control variates exploit existing correlations, the common random numbers (CRN) technique allows us to *induce* positive correlation to our advantage, especially when comparing two similar systems. This computational practice [@problem_id:3005265] will guide you through a practical implementation of CRN in the context of financial option pricing. You will directly compare the variance of an estimate using CRN against one using independent sampling, providing a concrete demonstration of how synchronizing randomness can lead to dramatic improvements in estimator precision.", "problem": "Let a geometric Brownian motion (GBM) under the risk-neutral probability measure be defined by the stochastic differential equation $dS_t = r S_t \\, dt + \\sigma S_t \\, dW_t$, where $S_t$ is the asset price at time $t$, $r$ is the continuously compounded interest rate, $\\sigma$ is the volatility, and $W_t$ is a standard Brownian motion. Consider a European call option with strike $K$ and maturity $T$ whose discounted payoff at time $T$ under a given interest rate $r$ is $e^{-rT} \\max(S_T - K, 0)$.\n\nYour task is to construct a comparative Monte Carlo estimator using the variance reduction technique known as common random numbers to estimate the variance of the paired pathwise difference in discounted call payoffs at two different interest rates. Specifically, for a fixed sample size $N$ and a specified integer seed $s$, generate a single set of $N$ standard normal random variables and use it to drive both terminal values $S_T^{(r_1)}$ and $S_T^{(r_2)}$ of the GBM at interest rates $r_1$ and $r_2$, respectively. For each outcome, compute the discounted call payoffs $C^{(r_1)}$ and $C^{(r_2)}$, form the paired difference $D = C^{(r_1)} - C^{(r_2)}$, and compute the empirical unbiased sample variance of $D$. Additionally, for comparison, estimate the variance of the difference-of-means estimator using independent sampling for each rate: draw $N$ independent standard normal variates with seed $s+1$ for $r_1$ and $N$ independent standard normal variates with seed $s+2$ for $r_2$, compute the corresponding discounted call payoffs, and estimate $\\operatorname{Var}(\\bar{C}^{(r_1)} - \\bar{C}^{(r_2)})$ via the sum of the two sample variances divided by $N$. In summary, for each test case you must output the following four quantities:\n- the unbiased empirical sample variance $s_D^2$ of the paired differences $D$ (using common random numbers),\n- the estimated variance of the difference-of-means estimator under common random numbers, $s_D^2/N$,\n- the estimated variance of the difference-of-means estimator under independent sampling, $s_{C^{(r_1)}}^2/N + s_{C^{(r_2)}}^2/N$,\n- the ratio of the independent-sampling estimator variance to the common-random-numbers estimator variance. If the common-random-numbers estimator variance is numerically zero, output the ratio as the floating-point infinity.\n\nBase your construction on the core definitions of GBM, the risk-neutral valuation principle, and Monte Carlo simulation. Do not use shortcut formulas in the problem statement; derive any necessary relationships from these foundations in your solution. All quantities must be expressed as dimensionless real numbers.\n\nUse the following test suite of parameter values, where each tuple represents $(S_0, K, \\sigma, T, r_1, r_2, N, s)$:\n- Case A (general comparison): $(100.0, 100.0, 0.2, 1.0, 0.01, 0.05, 200000, 271828)$,\n- Case B (boundary with identical rates): $(100.0, 100.0, 0.2, 1.0, 0.03, 0.03, 100000, 314159)$,\n- Case C (short maturity, large rate difference): $(100.0, 100.0, 0.2, 0.01, 0.0, 0.10, 300000, 161803)$,\n- Case D (deep out-of-the-money): $(50.0, 100.0, 0.4, 2.0, 0.02, 0.08, 250000, 141421)$.\n\nYour program must produce a single line of output containing all results in the following order as a comma-separated list enclosed in square brackets:\n$[s_{D,A}^2, v_{\\text{CRN},A}, v_{\\text{IND},A}, \\rho_A, s_{D,B}^2, v_{\\text{CRN},B}, v_{\\text{IND},B}, \\rho_B, s_{D,C}^2, v_{\\text{CRN},C}, v_{\\text{IND},C}, \\rho_C, s_{D,D}^2, v_{\\text{CRN},D}, v_{\\text{IND},D}, \\rho_D]$,\nwhere $s_{D,\\cdot}^2$ denotes the unbiased sample variance of paired differences for the specified case, $v_{\\text{CRN},\\cdot} = s_{D,\\cdot}^2/N$ denotes the common-random-numbers estimator variance, $v_{\\text{IND},\\cdot}$ denotes the independent-sampling estimator variance, and $\\rho_{\\cdot} = v_{\\text{IND},\\cdot} / v_{\\text{CRN},\\cdot}$ denotes the variance ratio for the specified case.", "solution": "The problem requires a comparison of variance for an estimator of the difference between two European call option prices, where the prices differ only by the risk-free interest rate, $r$. The comparison is between two Monte Carlo simulation schemes: one using independent random number streams for each option price estimation, and one using common random numbers (CRN), a variance reduction technique.\n\nFirst, we must establish the simulation formula for the terminal asset price, $S_T$. The asset price is stipulated to follow a geometric Brownian motion (GBM) under the risk-neutral measure, described by the stochastic differential equation (SDE):\n$$\ndS_t = r S_t \\, dt + \\sigma S_t \\, dW_t\n$$\nwhere $S_t$ is the asset price at time $t$, $r$ is the risk-free rate, $\\sigma$ is the volatility, and $W_t$ is a standard Wiener process (Brownian motion).\n\nTo find a solution for $S_T$, we apply Itô's lemma to the function $f(S_t) = \\ln(S_t)$. The derivatives are $f'(S_t) = 1/S_t$ and $f''(S_t) = -1/S_t^2$. According to Itô's lemma, the differential $d(\\ln S_t)$ is:\n$$\nd(\\ln S_t) = f'(S_t) dS_t + \\frac{1}{2} f''(S_t) (dS_t)^2\n$$\nThe quadratic variation term $(dS_t)^2$ is found by squaring the SDE, keeping only the lowest order terms in $dt$ according to Itô calculus rules ($dt^2 \\to 0$, $dt dW_t \\to 0$, $(dW_t)^2 \\to dt$):\n$$\n(dS_t)^2 = (r S_t \\, dt + \\sigma S_t \\, dW_t)^2 = \\sigma^2 S_t^2 (dW_t)^2 = \\sigma^2 S_t^2 dt\n$$\nSubstituting $dS_t$ and $(dS_t)^2$ into the lemma gives:\n$$\nd(\\ln S_t) = \\frac{1}{S_t}(r S_t \\, dt + \\sigma S_t \\, dW_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right)(\\sigma^2 S_t^2 dt)\n$$\n$$\nd(\\ln S_t) = (r \\, dt + \\sigma \\, dW_t) - \\frac{1}{2} \\sigma^2 dt = \\left(r - \\frac{1}{2}\\sigma^2\\right)dt + \\sigma dW_t\n$$\nIntegrating both sides from $t=0$ to $t=T$:\n$$\n\\int_0^T d(\\ln S_t) = \\int_0^T \\left(r - \\frac{1}{2}\\sigma^2\\right)dt + \\int_0^T \\sigma dW_t\n$$\n$$\n\\ln(S_T) - \\ln(S_0) = \\left(r - \\frac{1}{2}\\sigma^2\\right)T + \\sigma (W_T - W_0)\n$$\nGiven that $W_0=0$ and the random variable $W_T$ is normally distributed with mean $0$ and variance $T$, we can write $W_T = \\sqrt{T}Z$, where $Z$ is a standard normal random variable, $Z \\sim N(0, 1)$. Exponentiating both sides yields the solution for the terminal asset price:\n$$\nS_T = S_0 \\exp\\left( \\left(r - \\frac{1}{2}\\sigma^2\\right)T + \\sigma\\sqrt{T}Z \\right)\n$$\nThis formula is the basis for the Monte Carlo simulation of terminal asset prices.\n\nThe price of a European call option is the expected value of its discounted payoff under the risk-neutral measure:\n$$\nC(r) = E\\left[ e^{-rT} \\max(S_T - K, 0) \\right]\n$$\nA Monte Carlo estimate for this price is the sample average over $N$ simulated paths:\n$$\n\\bar{C}(r) = \\frac{1}{N} \\sum_{i=1}^N C_i(r) = \\frac{1}{N} \\sum_{i=1}^N e^{-rT} \\max(S_{T,i} - K, 0)\n$$\nwhere each $S_{T,i}$ is generated using an independent draw $Z_i$ from $N(0,1)$.\n\nOur objective is to estimate the difference in option prices for two interest rates, $r_1$ and $r_2$, i.e., $\\Delta = C(r_1) - C(r_2)$. The natural estimator is $\\hat{\\Delta} = \\bar{C}(r_1) - \\bar{C}(r_2)$. We are interested in the variance of this estimator, $\\operatorname{Var}(\\hat{\\Delta})$.\n\nFirst, consider the case of **independent sampling**. We generate two independent sets of $N$ standard normal variates, $\\{Z_i^{(1)}\\}_{i=1}^N$ and $\\{Z_i^{(2)}\\}_{i=1}^N$, to compute $\\bar{C}(r_1)$ and $\\bar{C}(r_2)$ respectively. Because the estimators are independent, the variance of their difference is the sum of their variances:\n$$\n\\operatorname{Var}(\\hat{\\Delta})_{\\text{IND}} = \\operatorname{Var}(\\bar{C}(r_1) - \\bar{C}(r_2)) = \\operatorname{Var}(\\bar{C}(r_1)) + \\operatorname{Var}(\\bar{C}(r_2))\n$$\nThe variance of a sample mean is the population variance divided by the sample size, $\\operatorname{Var}(\\bar{C}(r)) = \\operatorname{Var}(C(r))/N$. We estimate this using the unbiased sample variance, $s_{C(r)}^2 = \\frac{1}{N-1}\\sum_{i=1}^N(C_i(r) - \\bar{C}(r))^2$. Thus, the estimated variance of the difference-of-means estimator is:\n$$\nv_{\\text{IND}} = \\frac{s_{C(r_1)}^2}{N} + \\frac{s_{C(r_2)}^2}{N}\n$$\n\nNext, consider the **common random numbers (CRN)** technique. We use a single set of $N$ standard normal variates, $\\{Z_i\\}_{i=1}^N$, to generate both sequences of payoffs, $\\{C_i(r_1)\\}_{i=1}^N$ and $\\{C_i(r_2)\\}_{i=1}^N$. We then estimate $\\Delta$ by computing the mean of the paired differences, $\\bar{D} = \\frac{1}{N} \\sum_{i=1}^N D_i$, where $D_i = C_i(r_1) - C_i(r_2)$. The variance of this estimator is:\n$$\n\\operatorname{Var}(\\bar{D}) = \\frac{1}{N} \\operatorname{Var}(D) = \\frac{1}{N} \\operatorname{Var}(C(r_1) - C(r_2))\n$$\n$$\nv_{\\text{CRN}} = \\frac{1}{N} \\operatorname{Var}(C(r_1) - C(r_2)) = \\frac{1}{N} \\left( \\operatorname{Var}(C(r_1)) + \\operatorname{Var}(C(r_2)) - 2\\operatorname{Cov}(C(r_1), C(r_2)) \\right)\n$$\nWe estimate this with the sample variance of the differences, $s_D^2 = \\frac{1}{N-1}\\sum_{i=1}^N(D_i - \\bar{D})^2$. The estimated variance of the estimator is then $v_{\\text{CRN}} = s_D^2 / N$. Note that $s_D^2$ is one of the required outputs.\n\nThe effectiveness of CRN depends on the covariance term. For a fixed shock $Z$, the terminal price $S_T(r) = S_0 \\exp\\left( (r - \\frac{1}{2}\\sigma^2)T + \\sigma\\sqrt{T}Z \\right)$ is a monotonically increasing function of $r$. The call payoff $\\max(S_T - K, 0)$ is monotonic in $S_T$. The discounting factor $e^{-rT}$ is monotonic decreasing in $r$. The combined function $C(r, Z) = e^{-rT} \\max(S_T(r,Z) - K, 0)$ also tends to be monotonic in $r$ over a significant part of its domain. This induces a strong positive correlation between the payoff sequences $C_i(r_1)$ and $C_i(r_2)$. As a result, $\\operatorname{Cov}(C(r_1), C(r_2))$ is positive and large, which significantly reduces the variance $v_{\\text{CRN}}$ compared to $v_{\\text{IND}}$. The ratio $\\rho = v_{\\text{IND}} / v_{\\text{CRN}}$ quantifies this variance reduction and is expected to be greater than $1$.\n\nFor the special case where $r_1=r_2$ (Case B), for any given shock $Z_i$, we will have $S_{T,i}^{(r_1)} = S_{T,i}^{(r_2)}$ and thus $C_i^{(r_1)} = C_i^{(r_2)}$. This means every paired difference $D_i = C_i^{(r_1)} - C_i^{(r_2)}$ will be exactly $0$. Consequently, the sample variance of the differences, $s_D^2$, will be $0$. The CRN estimator variance, $v_{\\text{CRN}} = s_D^2/N$, is also $0$. In this scenario, the ratio $\\rho = v_{\\text{IND}} / v_{\\text{CRN}}$ becomes infinite, indicating perfect variance reduction, as expected.\n\nThe computational procedure is as follows for each test case $(S_0, K, \\sigma, T, r_1, r_2, N, s)$:\n1.  **Common Random Numbers:**\n    a. Initialize a random number generator with seed $s$. Generate $N$ standard normal variates $Z_i$.\n    b. For each $Z_i$, compute $S_{T,i}^{(r_1)}$, $S_{T,i}^{(r_2)}$, and then the discounted payoffs $C_i^{(r_1)}$, $C_i^{(r_2)}$.\n    c. Form the differences $D_i = C_i^{(r_1)} - C_i^{(r_2)}$.\n    d. Compute the unbiased sample variance $s_D^2 = \\operatorname{Var}(D_1, \\dots, D_N)$ (with $N-1$ in the denominator).\n    e. Compute the estimator variance $v_{\\text{CRN}} = s_D^2 / N$.\n2.  **Independent Sampling:**\n    a. Initialize a generator with seed $s+1$. Generate $N$ variates $Z_i^{(1)}$ and compute the corresponding payoffs $C_i^{(r_1)}$. Calculate their sample variance $s_{C(r_1)}^2$.\n    b. Initialize a generator with seed $s+2$. Generate $N$ variates $Z_i^{(2)}$ and compute the corresponding payoffs $C_i^{(r_2)}$. Calculate their sample variance $s_{C(r_2)}^2$.\n    c. Compute the estimator variance $v_{\\text{IND}} = s_{C(r_1)}^2/N + s_{C(r_2)}^2/N$.\n3.  **Ratio:** Calculate $\\rho = v_{\\text{IND}} / v_{\\text{CRN}}$. If $v_{\\text{CRN}} = 0$, the ratio is positive infinity.\n\nThese steps are implemented for each test case to produce the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating and comparing variances for Monte Carlo\n    estimators using both common random numbers and independent sampling.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, K, sigma, T, r1, r2, N, s)\n        (100.0, 100.0, 0.2, 1.0, 0.01, 0.05, 200000, 271828), # Case A\n        (100.0, 100.0, 0.2, 1.0, 0.03, 0.03, 100000, 314159), # Case B\n        (100.0, 100.0, 0.2, 0.01, 0.0, 0.10, 300000, 161803), # Case C\n        (50.0, 100.0, 0.4, 2.0, 0.02, 0.08, 250000, 141421),  # Case D\n    ]\n\n    results = []\n    \n    for S0, K, sigma, T, r1, r2, N, s in test_cases:\n        \n        # --- 1. Common Random Numbers (CRN) Calculation ---\n        rng_crn = np.random.default_rng(s)\n        Z = rng_crn.standard_normal(N)\n\n        # Calculate terminal prices for both rates using the same random numbers\n        drift1 = (r1 - 0.5 * sigma**2) * T\n        drift2 = (r2 - 0.5 * sigma**2) * T\n        diffusion = sigma * np.sqrt(T) * Z\n        \n        ST1_crn = S0 * np.exp(drift1 + diffusion)\n        ST2_crn = S0 * np.exp(drift2 + diffusion)\n        \n        # Calculate discounted call payoffs\n        C1_crn = np.exp(-r1 * T) * np.maximum(ST1_crn - K, 0)\n        C2_crn = np.exp(-r2 * T) * np.maximum(ST2_crn - K, 0)\n        \n        # Calculate paired differences\n        D = C1_crn - C2_crn\n        \n        # Unbiased empirical sample variance of the paired differences\n        s_D_sq = np.var(D, ddof=1)\n        \n        # Estimated variance of the difference-of-means estimator under CRN\n        v_crn = s_D_sq / N\n        \n        # --- 2. Independent Sampling Calculation ---\n        # Rate 1\n        rng_ind1 = np.random.default_rng(s + 1)\n        Z1_ind = rng_ind1.standard_normal(N)\n        ST1_ind = S0 * np.exp(drift1 + sigma * np.sqrt(T) * Z1_ind)\n        C1_ind = np.exp(-r1 * T) * np.maximum(ST1_ind - K, 0)\n        s_C1_sq = np.var(C1_ind, ddof=1)\n\n        # Rate 2\n        rng_ind2 = np.random.default_rng(s + 2)\n        Z2_ind = rng_ind2.standard_normal(N)\n        ST2_ind = S0 * np.exp(drift2 + sigma * np.sqrt(T) * Z2_ind)\n        C2_ind = np.exp(-r2 * T) * np.maximum(ST2_ind - K, 0)\n        s_C2_sq = np.var(C2_ind, ddof=1)\n        \n        # Estimated variance of the difference-of-means estimator under independent sampling\n        v_ind = (s_C1_sq / N) + (s_C2_sq / N)\n        \n        # --- 3. Ratio Calculation ---\n        if v_crn == 0:\n            rho = float('inf')\n        else:\n            rho = v_ind / v_crn\n            \n        results.extend([s_D_sq, v_crn, v_ind, rho])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3005265"}, {"introduction": "Importance sampling is arguably the most powerful variance reduction technique, but its power comes with significant risks. A poorly chosen proposal distribution can not only be unhelpful but can catastrophically increase the variance, sometimes to infinity. This final practice [@problem_id:2402925] serves as a critical lesson in the responsible use of importance sampling, forcing you to analyze why some proposal distributions fail. By identifying sampling strategies that are provably worse than a simple uniform approach, you will build the theoretical intuition needed to design and diagnose importance samplers effectively in your own work.", "problem": "You are asked to estimate the integral of the function $f(x)=x^2$ over the interval $[0,1]$ using importance sampling. Let $g(x)$ denote a normalized proposal probability density function on $[0,1]$. For the purpose of this question, define “worse than uniform sampling” to mean that the importance sampling estimator using $g(x)$ has a variance that is strictly larger than (or not finite compared to) the variance of the standard Monte Carlo estimator that samples $x$ uniformly on $[0,1]$. Which of the following choices for $g(x)$ makes importance sampling provably worse than uniform sampling for estimating $\\int_{0}^{1} x^2\\,dx$?\n\nA. $g(x)=1$ for $x\\in[0,1]$.\n\nB. $g(x)=\\dfrac{3}{2}\\,(1-x)^{1/2}$ for $x\\in[0,1]$.\n\nC. $g(x)=3x^2$ for $x\\in[0,1]$.\n\nD. $g(x)=2(1-x)$ for $x\\in[0,1]$.\n\nSelect all that apply. Your justification should rely on a variance comparison grounded in first principles of importance sampling, not on heuristic intuition.", "solution": "The problem statement must first be validated for scientific and logical integrity before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The integral to be estimated is $I = \\int_{0}^{1} f(x)\\,dx$, where the integrand is $f(x) = x^2$.\n- The integration interval is $[0,1]$.\n- A normalized proposal probability density function (PDF) on $[0,1]$ is denoted by $g(x)$.\n- Uniform sampling uses the PDF $p(x) = 1$ for $x \\in [0,1]$.\n- The definition of “worse than uniform sampling” is that the variance of the importance sampling estimator using $g(x)$ is strictly larger than, or not finite compared to, the variance of the standard Monte Carlo estimator (uniform sampling).\n- The task is to identify which of the provided functions $g(x)$ result in an importance sampling estimator that is worse than the uniform sampling estimator.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a standard application of the theory of Monte Carlo integration and variance reduction techniques, specifically importance sampling. The concepts are fundamental to computational physics and statistics. The problem is scientifically sound.\n- **Well-Posedness:** The problem is clearly formulated. The quantities to be compared—variances of estimators—are well-defined. The provided functions $g(x)$ are all valid, non-negative, and normalized probability density functions on the interval $[0,1]$.\n    - A: $\\int_0^1 1 \\,dx = 1$.\n    - B: $\\int_0^1 \\frac{3}{2}(1-x)^{1/2} \\,dx = \\frac{3}{2} [-\\frac{2}{3}(1-x)^{3/2}]_0^1 = 1$.\n    - C: $\\int_0^1 3x^2 \\,dx = [x^3]_0^1 = 1$.\n    - D: $\\int_0^1 2(1-x) \\,dx = 2[x - \\frac{x^2}{2}]_0^1 = 1$.\n- **Objectivity:** The criterion for \"worse\" is given as a strict inequality of variances, which is an objective mathematical condition. There is no ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, self-contained, and scientifically grounded problem. A solution will now be derived.\n\n### Derivation of Solution\n\nThe integral in question is $I = \\int_0^1 x^2 \\,dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}$.\n\n**1. Variance of the Standard (Uniform) Monte Carlo Estimator**\n\nFor uniform sampling, samples $x_i$ are drawn from the PDF $p(x)=1$ on $[0,1]$. The estimator for the integral is $\\hat{I}_{unif} = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$. The single-sample variance is $\\sigma^2_{unif} = \\text{Var}_{p}(f(x))$.\nThis variance is given by:\n$$\n\\sigma^2_{unif} = E_p[f(x)^2] - (E_p[f(x)])^2\n$$\nThe expectation of $f(x)$ is the integral itself:\n$$\nE_p[f(x)] = \\int_0^1 f(x) p(x) \\,dx = \\int_0^1 x^2 \\cdot 1 \\,dx = I = \\frac{1}{3}\n$$\nThe expectation of $f(x)^2$ is:\n$$\nE_p[f(x)^2] = \\int_0^1 (f(x))^2 p(x) \\,dx = \\int_0^1 (x^2)^2 \\cdot 1 \\,dx = \\int_0^1 x^4 \\,dx = \\left[\\frac{x^5}{5}\\right]_0^1 = \\frac{1}{5}\n$$\nThus, the variance for uniform sampling is:\n$$\n\\sigma^2_{unif} = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{5} - \\frac{1}{9} = \\frac{9 - 5}{45} = \\frac{4}{45}\n$$\n\n**2. Variance of the Importance Sampling Estimator**\n\nFor importance sampling, samples $x_i$ are drawn from a proposal PDF $g(x)$. The integral is expressed as $I = \\int_0^1 \\frac{f(x)}{g(x)} g(x) \\,dx = E_g\\left[\\frac{f(x)}{g(x)}\\right]$. The single-sample variance is $\\sigma^2_g = \\text{Var}_g\\left(\\frac{f(x)}{g(x)}\\right)$.\nThis variance is given by:\n$$\n\\sigma^2_g = E_g\\left[\\left(\\frac{f(x)}{g(x)}\\right)^2\\right] - \\left(E_g\\left[\\frac{f(x)}{g(x)}\\right]\\right)^2\n$$\nThe second term is $(I)^2 = (1/3)^2 = 1/9$, since the estimator must be unbiased. The first term is:\n$$\nE_g\\left[\\left(\\frac{f(x)}{g(x)}\\right)^2\\right] = \\int_0^1 \\left(\\frac{f(x)}{g(x)}\\right)^2 g(x) \\,dx = \\int_0^1 \\frac{f(x)^2}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{g(x)} \\,dx\n$$\nSo, the variance for the importance sampling estimator is:\n$$\n\\sigma^2_g = \\int_0^1 \\frac{x^4}{g(x)} \\,dx - \\frac{1}{9}\n$$\n\n**3. Condition for \"Worse than Uniform Sampling\"**\n\nAccording to the problem definition, the importance sampling is worse if $\\sigma^2_g > \\sigma^2_{unif}$ or if $\\sigma^2_g$ is infinite.\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx - \\frac{1}{9} > \\frac{4}{45}\n$$\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx > \\frac{4}{45} + \\frac{1}{9} = \\frac{4+5}{45} = \\frac{9}{45} = \\frac{1}{5}\n$$\nSo, a choice of $g(x)$ is worse if $\\int_0^1 \\frac{x^4}{g(x)} \\,dx > \\frac{1}{5}$ or if this integral diverges.\n\n### Option-by-Option Analysis\n\n**A. $g(x)=1$ for $x\\in[0,1]$.**\nThis is uniform sampling. We check the condition:\n$$\n\\int_0^1 \\frac{x^4}{1} \\,dx = \\int_0^1 x^4 \\,dx = \\frac{1}{5}\n$$\nThe condition is $\\frac{1}{5} > \\frac{1}{5}$, which is false. The variance is equal to that of uniform sampling, not strictly larger.\n**Verdict: Incorrect.**\n\n**B. $g(x)=\\dfrac{3}{2}\\,(1-x)^{1/2}$ for $x\\in[0,1]$.**\nWe evaluate the integral:\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{\\frac{3}{2}(1-x)^{1/2}} \\,dx = \\frac{2}{3} \\int_0^1 x^4 (1-x)^{-1/2} \\,dx\n$$\nThis is related to the Beta function, $B(z_1, z_2) = \\int_0^1 t^{z_1-1} (1-t)^{z_2-1} \\,dt$. Here, $z_1-1=4$ and $z_2-1=-1/2$, so $z_1=5$ and $z_2=1/2$.\n$$\n\\int_0^1 x^4 (1-x)^{-1/2} \\,dx = B(5, 1/2) = \\frac{\\Gamma(5)\\Gamma(1/2)}{\\Gamma(5+1/2)} = \\frac{4! \\sqrt{\\pi}}{\\frac{9}{2} \\cdot \\frac{7}{2} \\cdot \\frac{5}{2} \\cdot \\frac{3}{2} \\cdot \\frac{1}{2}\\sqrt{\\pi}} = \\frac{24}{\\frac{945}{32}} = \\frac{24 \\cdot 32}{945} = \\frac{768}{945} = \\frac{256}{315}\n$$\nSo, the full term is:\n$$\n\\frac{2}{3} \\cdot \\frac{256}{315} = \\frac{512}{945}\n$$\nNow we check the condition $\\frac{512}{945} > \\frac{1}{5}$.\n$$\n\\frac{1}{5} = \\frac{189}{945}\n$$\nSince $512 > 189$, the inequality $\\frac{512}{945} > \\frac{189}{945}$ is true. The variance is strictly larger.\n**Verdict: Correct.**\n\n**C. $g(x)=3x^2$ for $x\\in[0,1]$.**\nThis proposal PDF is proportional to the integrand $f(x)=x^2$. This is not the ideal choice, which is proportional to $|f(x)|$. The ideal choice for minimum variance is proportional to $|f(x)|$ which is $g(x) \\propto x^2$, leading to $g(x)=3x^2$. This should yield minimum, not maximum, variance. Let's re-read the theory. The ideal importance sampling density is proportional to $|f(x)|$, i.e. $g(x) = |f(x)| / \\int |f(x)| dx$. Here $f(x)=x^2$ is positive, so $g(x)=x^2 / \\int_0^1 x^2 dx = x^2 / (1/3) = 3x^2$. For this choice, the variance is zero.\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{3x^2} \\,dx = \\frac{1}{3} \\int_0^1 x^2 \\,dx = \\frac{1}{3} \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{9}\n$$\nWe check the condition $\\frac{1}{9} > \\frac{1}{5}$. This is false.\nIndeed, the variance is $\\sigma^2_g = \\frac{1}{9} - \\frac{1}{9} = 0$, the minimum possible variance. This is far better, not worse, than uniform sampling.\n**Verdict: Incorrect.**\n\n**D. $g(x)=2(1-x)$ for $x\\in[0,1]$.**\nWe evaluate the integral for the variance calculation:\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{2(1-x)} \\,dx = \\frac{1}{2} \\int_0^1 \\frac{x^4}{1-x} \\,dx\n$$\nWe must analyze the convergence of this integral. As $x \\to 1^-$, let $u = 1-x$. The integrand behaves like $\\frac{(1-u)^4}{u} \\approx \\frac{1}{u}$. The integral $\\int \\frac{1}{u} \\,du$ diverges logarithmically at $u=0$.\nTherefore, the integral $\\int_0^1 \\frac{x^4}{1-x} \\,dx$ is divergent. This means the variance $\\sigma^2_g$ is infinite. An infinite variance is, by the problem's definition, \"worse than uniform sampling\". This catastrophic failure occurs because the proposal distribution $g(x)$ goes to zero at $x=1$, where the function $f(x)^2=x^4$ is non-zero.\n**Verdict: Correct.**", "answer": "$$\\boxed{BD}$$", "id": "2402925"}]}