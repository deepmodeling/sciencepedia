{"hands_on_practices": [{"introduction": "Before building complex algorithms, it is crucial to develop a firm intuition for the core mechanism of barrier methods. This exercise guides you through an analytical exploration of a simple scalar problem using a logarithmic barrier function [@problem_id:2423438]. By deriving the exact solution path, known as the central path, you will see precisely how the minimizer of the barrier-augmented objective converges to the true constrained optimum as the barrier parameter $\\rho$ is adjusted, providing a clear window into the fundamental behavior of interior-point methods.", "problem": "You are given a scalar constrained optimization problem designed to illustrate how a constraint can transition from inactive (strictly satisfied) to active (approximately binding) as a tunable parameter is sequentially increased within a barrier method framework. Consider the convex optimization problem of minimizing a smooth objective subject to a single inequality constraint. Let the objective be $f(x) = (x - 2)^2$ and the inequality constraint be $c(x) = x - 1 \\le 0$ (equivalently, $x \\le 1$). To enforce the constraint via a logarithmic barrier, consider the barrier-augmented objective\n$$\n\\psi_{\\rho}(x) = \\rho\\, f(x) - \\log(1 - x),\n$$\ndefined on the open domain $x  1$, where $\\rho  0$ is a tunable weighting parameter. The barrier term $-\\log(1 - x)$ encodes the constraint $x \\le 1$ and ensures iterates remain strictly feasible. As $\\rho$ is increased sequentially, the barrier influence diminishes relative to $f(x)$, and the minimizer of $\\psi_{\\rho}$ approaches the boundary where the constraint becomes active in the sense of approximate equality.\n\nStarting from fundamental principles (first-order optimality conditions for unconstrained minimization over the open domain $x  1$ and properties of convexity), derive the unique minimizer $x(\\rho)$ for $\\rho  0$ in closed form and use it to detect when the constraint is “active” in the approximate sense. Define the slack $s(\\rho) = 1 - x(\\rho)$. For a given tolerance $\\tau  0$, say that the constraint is active if $s(\\rho) \\le \\tau$ and inactive if $s(\\rho)  \\tau$.\n\nYour task is to write a complete, runnable program that:\n- Uses the derived closed-form $x(\\rho)$ to evaluate $s(\\rho)$ for each test value of $\\rho$ in the test suite.\n- Compares $s(\\rho)$ against a fixed tolerance $\\tau$ to produce an activity indicator: output $1$ if the constraint is active and $0$ otherwise.\n\nFundamental base you may use: first-order necessary and sufficient conditions for convex smooth minimization (vanishing gradient characterizes the minimizer on the open domain), the definition of the logarithmic barrier for an inequality constraint $x \\le 1$, and basic algebra to solve the resulting optimality equation.\n\nTest suite:\n- Barrier weights $\\rho \\in \\{0.1, 1, 10, 50, 100\\}$.\n- Tolerance $\\tau = 0.01$.\n\nFinal output format:\n- Your program should produce a single line of output containing the activity results for the test suite as a comma-separated list of integers enclosed in square brackets, in the same order as the given $\\rho$ values. For example, an output could look like $[0,1,1,1,1]$ but with the correct results for this problem.\n\nNo physical units are involved in this problem. Angles are not used. Percentages are not used; when comparing to the tolerance $\\tau$, all values are pure real numbers. The answer for each test case must be an integer in $\\{0,1\\}$, and the program must aggregate these into a single list as specified.", "solution": "The problem presented is a well-posed exercise in constrained optimization, specifically illustrating the mechanics of a logarithmic barrier method. Its components are mathematically precise, scientifically grounded in the principles of convex optimization, and constitute a complete and consistent setup. The problem is therefore deemed valid. We shall proceed with its formal solution.\n\nThe task is to find the unique minimizer of the barrier-augmented objective function $\\psi_{\\rho}(x)$ for a given parameter $\\rho  0$, where\n$$\n\\psi_{\\rho}(x) = \\rho f(x) - \\log(1 - x)\n$$\nwith the objective $f(x) = (x - 2)^2$. The domain of definition is the open interval $(-\\infty, 1)$, which enforces strict satisfaction of the constraint $x - 1  0$.\n\nThe function $\\psi_{\\rho}(x)$ is the sum of two functions: $\\rho(x-2)^2$ and $-\\log(1-x)$. The first term, being a scaled quadratic with a positive leading coefficient since $\\rho  0$, is a convex function. The second term, the logarithmic barrier, is also convex on its domain $x  1$. The sum of convex functions is convex. To demonstrate strict convexity, we examine the second derivative.\n\nThe first derivative of $\\psi_{\\rho}(x)$ with respect to $x$ is:\n$$\n\\frac{d\\psi_{\\rho}}{dx} = \\frac{d}{dx} \\left[ \\rho (x - 2)^2 - \\log(1 - x) \\right] = 2\\rho(x - 2) - \\frac{-1}{1 - x} = 2\\rho(x - 2) + \\frac{1}{1 - x}.\n$$\nThe second derivative is:\n$$\n\\frac{d^2\\psi_{\\rho}}{dx^2} = \\frac{d}{dx} \\left[ 2\\rho(x - 2) + (1 - x)^{-1} \\right] = 2\\rho + (-1)(1 - x)^{-2}(-1) = 2\\rho + \\frac{1}{(1 - x)^2}.\n$$\nFor any $\\rho  0$ and any $x$ in the domain $x  1$, both terms $2\\rho$ and $\\frac{1}{(1 - x)^2}$ are strictly positive. Therefore, $\\frac{d^2\\psi_{\\rho}}{dx^2}  0$ for all $x  1$, which confirms that $\\psi_{\\rho}(x)$ is a strictly convex function on its domain.\n\nFor a strictly convex and differentiable function on an open set, the first-order necessary condition for a minimum, that the gradient (in this scalar case, the derivative) vanishes, is also a sufficient condition. Thus, the unique minimizer $x(\\rho)$ is found by solving the equation $\\frac{d\\psi_{\\rho}}{dx} = 0$:\n$$\n2\\rho(x - 2) + \\frac{1}{1 - x} = 0.\n$$\nTo solve for $x$, we first clear the denominator by multiplying by $(1 - x)$, which is non-zero in the domain:\n$$\n2\\rho(x - 2)(1 - x) + 1 = 0\n$$\n$$\n2\\rho(x - x^2 - 2 + 2x) + 1 = 0\n$$\n$$\n2\\rho(-x^2 + 3x - 2) + 1 = 0\n$$\n$$\n-2\\rho x^2 + 6\\rho x - 4\\rho + 1 = 0.\n$$\nThis is a quadratic equation of the form $ax^2 + bx + c = 0$, with coefficients $a = 2\\rho$, $b = -6\\rho$, and $c = 4\\rho - 1$. We apply the quadratic formula to find the roots:\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{6\\rho \\pm \\sqrt{(-6\\rho)^2 - 4(2\\rho)(4\\rho - 1)}}{2(2\\rho)}\n$$\n$$\nx = \\frac{6\\rho \\pm \\sqrt{36\\rho^2 - 8\\rho(4\\rho - 1)}}{4\\rho} = \\frac{6\\rho \\pm \\sqrt{36\\rho^2 - 32\\rho^2 + 8\\rho}}{4\\rho}\n$$\n$$\nx = \\frac{6\\rho \\pm \\sqrt{4\\rho^2 + 8\\rho}}{4\\rho} = \\frac{6\\rho \\pm 2\\sqrt{\\rho^2 + 2\\rho}}{4\\rho} = \\frac{3\\rho \\pm \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}.\n$$\nThis gives two potential solutions. We must select the one that respects the domain constraint $x  1$.\nConsider the root with the plus sign:\n$x_1 = \\frac{3\\rho + \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}$. Since $\\rho  0$, we have $\\sqrt{\\rho^2 + 2\\rho}  \\sqrt{\\rho^2} = \\rho$. Thus, the numerator is greater than $3\\rho + \\rho = 4\\rho$, and $x_1  \\frac{4\\rho}{2\\rho} = 2$. This root is invalid as it lies outside the domain $x  1$.\n\nConsider the root with the minus sign:\n$x_2 = \\frac{3\\rho - \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}$. Since $\\rho  \\sqrt{\\rho^2 + 2\\rho}$, the numerator is $3\\rho - \\sqrt{\\rho^2 + 2\\rho}  3\\rho - \\rho = 2\\rho$. Therefore, $x_2  \\frac{2\\rho}{2\\rho} = 1$. This root is always within the feasible domain.\nThe unique minimizer is thus given by the closed-form expression:\n$$\nx(\\rho) = \\frac{3\\rho - \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}.\n$$\nThe problem defines the slack variable as $s(\\rho) = 1 - x(\\rho)$. Substituting the expression for $x(\\rho)$:\n$$\ns(\\rho) = 1 - \\left( \\frac{3\\rho - \\sqrt{\\rho^2 + 2\\rho}}{2\\rho} \\right) = \\frac{2\\rho - (3\\rho - \\sqrt{\\rho^2 + 2\\rho})}{2\\rho} = \\frac{\\sqrt{\\rho^2 + 2\\rho} - \\rho}{2\\rho}.\n$$\nThe constraint is defined as \"active\" if $s(\\rho) \\le \\tau$ and \"inactive\" otherwise. With $\\tau = 0.01$, we must evaluate this condition for each given value of $\\rho$. The program will implement this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained optimization problem using a logarithmic barrier method\n    and determines constraint activity for a suite of test cases.\n    \"\"\"\n    # The test suite consists of a set of barrier weights `rho`\n    # and a fixed tolerance `tau`.\n    test_cases = {\n        \"rho_values\": [0.1, 1, 10, 50, 100],\n        \"tau\": 0.01\n    }\n\n    rho_values = test_cases[\"rho_values\"]\n    tau = test_cases[\"tau\"]\n\n    results = []\n\n    # For each barrier weight `rho`, calculate the slack and determine if the\n    # constraint is active.\n    for rho in rho_values:\n        # The minimizer x(rho) of the barrier-augmented objective is derived from\n        # the first-order optimality condition d/dx [rho*(x-2)^2 - log(1-x)] = 0.\n        # This leads to the quadratic equation 2*rho*x^2 - 6*rho*x + (4*rho - 1) = 0.\n        # The valid root (satisfying x  1) is x(rho) = (3*rho - sqrt(rho^2 + 2*rho)) / (2*rho).\n\n        # The slack is s(rho) = 1 - x(rho).\n        # A simpler expression for the slack is derived as:\n        # s(rho) = (sqrt(rho^2 + 2*rho) - rho) / (2*rho)\n        \n        # We must use floating-point numbers for the calculations.\n        rho_f = float(rho)\n        \n        # Calculate the slack s(rho).\n        numerator = np.sqrt(rho_f**2 + 2 * rho_f) - rho_f\n        denominator = 2 * rho_f\n        slack = numerator / denominator\n\n        # The constraint is \"active\" if the slack is less than or equal to the tolerance.\n        # Activity indicator is 1 for active, 0 for inactive.\n        if slack = tau:\n            activity_indicator = 1\n        else:\n            activity_indicator = 0\n        \n        results.append(activity_indicator)\n\n    # The final output must be a single line containing the activity results\n    # as a comma-separated list of integers enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2423438"}, {"introduction": "Transitioning from theory to practice, this exercise challenges you to implement a complete sequential quadratic penalty method, a cornerstone algorithm for constrained optimization [@problem_id:2423453]. You will construct a gradient-based solver and investigate the practical concept of \"warm-starting,\" where the solution from one penalty subproblem is used as the initial guess for the next. This practice not only solidifies your understanding of the algorithmic pipeline but also allows you to experimentally quantify a critical technique used to accelerate convergence in real-world applications.", "problem": "You are asked to implement a sequential quadratic penalty method for constrained optimization and to experimentally quantify the benefit of warm-starting by reusing the previous subproblem’s solution as the initial guess for the next penalty parameter. Implement a gradient-based solver with a backtracking Armijo line search to minimize a sequence of unconstrained penalized subproblems. For a fixed sequence of penalty parameters $\\{\\rho_k\\}_{k=1}^K$ with $\\rho_1  \\rho_2  \\dots  \\rho_K$, compare two strategies for each test case: (i) cold-starting each subproblem from the same initial point, and (ii) warm-starting subproblem $k+1$ from the computed minimizer of subproblem $k$. Report the speedup factor defined as the ratio of the total number of gradient descent iterations taken by cold-starting to that taken by warm-starting across the full penalty sequence.\n\nFundamental base and definitions to use:\n- A constrained minimization problem has objective $f:\\mathbb{R}^n\\to\\mathbb{R}$, inequality constraints $g_i(x)\\le 0$ for $i\\in\\{1,\\dots,m\\}$, and equality constraints $h_j(x)=0$ for $j\\in\\{1,\\dots,p\\}$.\n- The classical quadratic penalty for inequalities is applied to the violation as $\\max\\{0, g_i(x)\\}^2$ and for equalities as $h_j(x)^2$.\n- The penalized subproblem for a given $\\rhogt;0$ is to minimize \n$$\n\\Phi_\\rho(x)=f(x)+\\rho\\left(\\sum_{i=1}^m \\max\\{0,g_i(x)\\}^2+\\sum_{j=1}^p h_j(x)^2\\right).\n$$\n- Use gradient descent with a backtracking Armijo rule: given current $x$, gradient $\\nabla\\Phi_\\rho(x)$, initial step size $t_0$, shrinkage factor $\\beta\\in(0,1)$, and Armijo parameter $c\\in(0,1)$, choose the largest $t$ from the sequence $\\{t_0, \\beta t_0, \\beta^2 t_0,\\dots\\}$ satisfying\n$$\n\\Phi_\\rho(x - t \\nabla \\Phi_\\rho(x)) \\le \\Phi_\\rho(x) - c\\,t\\,\\|\\nabla \\Phi_\\rho(x)\\|_2^2.\n$$\n- Stop the inner solver when $\\|\\nabla \\Phi_\\rho(x)\\|_2\\le \\varepsilon$.\n\nImplementation requirements:\n- Implement the quadratic penalty method and gradient descent with backtracking Armijo line search exactly as defined above.\n- For inequality constraints, treat only the positive violation by using the $\\max\\{0,\\cdot\\}$ structure in both the penalty value and its gradient. For equality constraints, penalize the squared residual.\n- Use the penalty sequence $\\rho\\in\\{10,10^2,10^3\\}$, i.e., $\\rho \\in \\{10,100,1000\\}$.\n- Use gradient tolerance $\\varepsilon=10^{-6}$, Armijo parameter $c=10^{-4}$, shrinkage factor $\\beta=\\tfrac{1}{2}$, and initial step size $t_0=1$ for all subproblems. Cap the maximum number of gradient iterations per subproblem at $N_{\\max}=10^4$.\n- Count the number of outer gradient descent iterations (each accepted step after line search) taken to converge a subproblem; do not count line search backtracking steps separately.\n\nTest suite:\nImplement and solve the following three two-dimensional test cases. In each case, return the speedup factor\n$$\nS=\\frac{N_{\\mathrm{cold}}}{N_{\\mathrm{warm}}},\n$$\nwhere $N_{\\mathrm{cold}}$ is the total number of gradient descent iterations summed over all penalty parameters when cold-starting each subproblem from the specified initial point, and $N_{\\mathrm{warm}}$ is the total when warm-starting each subproblem from the previous subproblem’s solution.\n\n- Case $\\mathbf{A}$ (convex quadratic with a binding linear inequality):\n  - Objective: $f(x,y)=(x-1)^2+2\\,(y+2)^2$.\n  - Inequality: $g_1(x,y)=1-x-y\\le 0$.\n  - No equalities.\n  - Initial point: $x_0=(0,0)$.\n\n- Case $\\mathbf{B}$ (convex quadratic with an equality):\n  - Objective: $f(x,y)=(x-3)^2+(y-1)^2$.\n  - Equality: $h_1(x,y)=x-y=0$.\n  - No inequalities.\n  - Initial point: $x_0=(0,0)$.\n\n- Case $\\mathbf{C}$ (convex quadratic with a curved inequality):\n  - Objective: $f(x,y)=(x+2)^2+y^2$.\n  - Inequality: $g_1(x,y)=x^2+y^2-1\\le 0$.\n  - No equalities.\n  - Initial point: $x_0=(0,0)$.\n\nOutput specification:\n- For each case, compute the speedup factor $S$ as defined above.\n- Your program should produce a single line of output containing the three speedup factors as a comma-separated list enclosed in square brackets, in the order $\\left[S_A,S_B,S_C\\right]$, where $S_A$ corresponds to Case $\\mathbf{A}$, $S_B$ to Case $\\mathbf{B}$, and $S_C$ to Case $\\mathbf{C}$. For example, output of the form $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$ with numeric values.\n- Express each speedup factor as a floating-point number. You may round internally, but the printed values must be standard decimal floats.\n\nNo physical units are involved. Angles are not used. Percentages are not used.\n\nThe final program must be self-contained, require no input, and adhere to the specified runtime environment. The correctness will be assessed by verifying that the implementation follows the definitions and that warm-starting yields strictly fewer iterations or at least not more, producing meaningful speedup factors for the specified cases. The output must be exactly one line in the specified format.", "solution": "The problem requires the implementation of a sequential quadratic penalty method to solve constrained optimization problems. The core of the task is to compare the computational efficiency of two initialization strategies for the sequence of unconstrained subproblems: a cold-start strategy versus a warm-start strategy. The efficiency is to be quantified by a speedup factor, defined as the ratio of total gradient descent iterations.\n\nThe general form of the constrained optimization problem is to minimize an objective function $f(x)$ subject to a set of inequality constraints $g_i(x) \\le 0$ for $i \\in \\{1, \\dots, m\\}$ and equality constraints $h_j(x) = 0$ for $j \\in \\{1, \\dots, p\\}$, where $x \\in \\mathbb{R}^n$.\n\nThe quadratic penalty method approximates the solution to this problem by solving a sequence of unconstrained minimization problems. For a given penalty parameter $\\rho > 0$, the penalized objective function $\\Phi_\\rho(x)$ is constructed by adding terms to the original objective function that penalize violations of the constraints. The specific form of the penalized function is:\n$$\n\\Phi_\\rho(x) = f(x) + \\rho \\left( \\sum_{i=1}^m \\left(\\max\\{0, g_i(x)\\}\\right)^2 + \\sum_{j=1}^p \\left(h_j(x)\\right)^2 \\right)\n$$\nThis function, $\\Phi_\\rho(x)$, is then minimized with respect to $x$. By solving this unconstrained problem for a sequence of increasing penalty parameters, $\\rho_1  \\rho_2  \\dots  \\rho_K$, the sequence of minimizers $x^*(\\rho_k)$ converges to the solution of the original constrained problem.\n\nTo minimize each unconstrained subproblem $\\min_x \\Phi_\\rho(x)$, a gradient-based method is required. The gradient of the penalized objective function, $\\nabla \\Phi_\\rho(x)$, is derived using the chain rule. For an inequality constraint term $P_i(x) = \\rho (\\max\\{0, g_i(x)\\})^2$, the gradient is $\\nabla P_i(x) = 2 \\rho \\max\\{0, g_i(x)\\} \\nabla g_i(x)$. For an equality constraint term $Q_j(x) = \\rho (h_j(x))^2$, the gradient is $\\nabla Q_j(x) = 2 \\rho h_j(x) \\nabla h_j(x)$. Combining these with the gradient of the objective function, the full gradient is:\n$$\n\\nabla \\Phi_\\rho(x) = \\nabla f(x) + 2\\rho \\left( \\sum_{i=1}^m \\max\\{0, g_i(x)\\} \\nabla g_i(x) + \\sum_{j=1}^p h_j(x) \\nabla h_j(x) \\right)\n$$\nThe unconstrained minimization is performed using gradient descent. Starting from a point $x_k$, the next point $x_{k+1}$ is found by moving in the direction of the negative gradient:\n$$\nx_{k+1} = x_k - t \\nabla \\Phi_\\rho(x_k)\n$$\nThe step size $t > 0$ is determined by a backtracking line search employing the Armijo condition. For a given descent direction $d_k = -\\nabla \\Phi_\\rho(x_k)$, we seek the largest $t$ from the sequence $\\{t_0, \\beta t_0, \\beta^2 t_0, \\dots\\}$ that satisfies:\n$$\n\\Phi_\\rho(x_k + t d_k) \\le \\Phi_\\rho(x_k) + c \\, t \\, \\nabla \\Phi_\\rho(x_k)^T d_k\n$$\nwhich, using $d_k = -\\nabla \\Phi_\\rho(x_k)$, simplifies to the form given in the problem statement:\n$$\n\\Phi_\\rho(x_k - t \\nabla \\Phi_\\rho(x_k)) \\le \\Phi_\\rho(x_k) - c \\, t \\, \\|\\nabla \\Phi_\\rho(x_k)\\|_2^2\n$$\nThe algorithm iterates until the norm of the gradient is below a specified tolerance $\\varepsilon$, i.e., $\\|\\nabla \\Phi_\\rho(x)\\|_2 \\le \\varepsilon$. The parameters for this solver are fixed: initial step size $t_0=1$, Armijo parameter $c=10^{-4}$, shrinkage factor $\\beta=0.5$, and gradient norm tolerance $\\varepsilon=10^{-6}$. The maximum number of iterations per subproblem is capped at $N_{\\max}=10^4$.\n\nThe experiment compares two strategies over the penalty parameter sequence $\\rho \\in \\{10, 100, 1000\\}$:\n1.  **Cold-Start:** Each subproblem for $\\rho_k$ is initialized from the same starting point $x_0$. The total number of iterations, $N_{\\mathrm{cold}}$, is the sum of iterations required to solve each subproblem independently.\n2.  **Warm-Start:** The first subproblem (for $\\rho_1=10$) is initialized from $x_0$. The subproblem for each subsequent $\\rho_{k+1}$ is initialized using the solution obtained from the previous subproblem for $\\rho_k$. The total number of iterations, $N_{\\mathrm{warm}}$, is the sum of iterations across this sequence.\n\nThe rationale for warm-starting is that the solution $x^*(\\rho_k)$ is expected to be a good initial guess for the minimizer of $\\Phi_{\\rho_{k+1}}(x)$, especially when $\\rho_{k+1}$ is not drastically larger than $\\rho_k$. This should lead to faster convergence. The performance gain is measured by the speedup factor $S = N_{\\mathrm{cold}} / N_{\\mathrm{warm}}$.\n\nThe implementation will proceed by defining Python functions for each test case's objective and constraint functions and their respective gradients. A generalized solver function will execute the gradient descent with Armijo line search. A top-level function will manage the sequence of penalty parameters, apply both cold-start and warm-start strategies, count the total iterations for each, and compute the speedup. This process will be repeated for all three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    # --- Solver Parameters ---\n    SOLVER_PARAMS = {\n        'epsilon': 1e-6,\n        'c_armijo': 1e-4,\n        'beta': 0.5,\n        't0': 1.0,\n        'n_max': 10000\n    }\n    PENALTY_PARAMS = [10.0, 100.0, 1000.0]\n\n    # --- Test Case Definitions ---\n    \n    # Case A: (x-1)^2 + 2(y+2)^2, s.t. 1-x-y = 0\n    case_A = {\n        'f': lambda x: (x[0] - 1.0)**2 + 2.0 * (x[1] + 2.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 1.0), 4.0 * (x[1] + 2.0)]),\n        'g': [lambda x: 1.0 - x[0] - x[1]],\n        'grad_g': [lambda x: np.array([-1.0, -1.0])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    # Case B: (x-3)^2 + (y-1)^2, s.t. x-y = 0\n    case_B = {\n        'f': lambda x: (x[0] - 3.0)**2 + (x[1] - 1.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 3.0), 2.0 * (x[1] - 1.0)]),\n        'g': [],\n        'grad_g': [],\n        'h': [lambda x: x[0] - x[1]],\n        'grad_h': [lambda x: np.array([1.0, -1.0])],\n        'x0': np.array([0.0, 0.0])\n    }\n    \n    # Case C: (x+2)^2 + y^2, s.t. x^2+y^2-1 = 0\n    case_C = {\n        'f': lambda x: (x[0] + 2.0)**2 + x[1]**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] + 2.0), 2.0 * x[1]]),\n        'g': [lambda x: x[0]**2 + x[1]**2 - 1.0],\n        'grad_g': [lambda x: np.array([2.0 * x[0], 2.0 * x[1]])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    \n    def get_penalized_funcs(case, rho):\n        \"\"\"Creates the penalized function and its gradient for a given case and rho.\"\"\"\n        \n        def phi(x):\n            f_val = case['f'](x)\n            g_sum = sum(max(0, g_func(x))**2 for g_func in case['g'])\n            h_sum = sum(h_func(x)**2 for h_func in case['h'])\n            return f_val + rho * (g_sum + h_sum)\n\n        def grad_phi(x):\n            grad_f_val = case['grad_f'](x)\n            \n            grad_g_sum = np.zeros_like(x, dtype=float)\n            for g_func, grad_g_func in zip(case['g'], case['grad_g']):\n                g_val = g_func(x)\n                if g_val  0:\n                    grad_g_sum += 2.0 * g_val * grad_g_func(x)\n\n            grad_h_sum = np.zeros_like(x, dtype=float)\n            for h_func, grad_h_func in zip(case['h'], case['grad_h']):\n                h_val = h_func(x)\n                grad_h_sum += 2.0 * h_val * grad_h_func(x)\n                \n            return grad_f_val + rho * (grad_g_sum + grad_h_sum)\n        \n        return phi, grad_phi\n\n    def gradient_descent(phi, grad_phi, x_init, params):\n        \"\"\"\n        Performs gradient descent with backtracking Armijo line search.\n        \"\"\"\n        x = np.copy(x_init)\n        n_iters = 0\n        \n        for k in range(params['n_max']):\n            grad = grad_phi(x)\n            grad_norm_sq = np.dot(grad, grad)\n\n            if np.sqrt(grad_norm_sq) = params['epsilon']:\n                break\n            \n            # Backtracking line search\n            t = params['t0']\n            phi_x = phi(x)\n            \n            while True:\n                x_new = x - t * grad\n                phi_new = phi(x_new)\n                armijo_check = phi_x - params['c_armijo'] * t * grad_norm_sq\n                \n                if phi_new = armijo_check:\n                    break\n                t *= params['beta']\n            \n            x = x_new\n            n_iters += 1\n        \n        return x, n_iters\n\n    def run_penalty_method(case, solver_params, penalty_params):\n        \"\"\"\n        Runs the full sequential penalty method for a case,\n        calculating iterations for both cold and warm starts.\n        \"\"\"\n        # Cold start\n        total_iters_cold = 0\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            _, n_iters = gradient_descent(phi, grad_phi, case['x0'], solver_params)\n            total_iters_cold += n_iters\n            \n        # Warm start\n        total_iters_warm = 0\n        x_warm = np.copy(case['x0'])\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            x_sol, n_iters = gradient_descent(phi, grad_phi, x_warm, solver_params)\n            total_iters_warm += n_iters\n            x_warm = x_sol\n            \n        if total_iters_warm == 0:\n             # This case should not happen in this problem, but is a safeguard.\n             # If cold is also 0, speedup is 1. If cold > 0, speedup is \"infinite\".\n            return 1.0 if total_iters_cold == 0 else float('inf')\n            \n        return float(total_iters_cold) / float(total_iters_warm)\n\n    results = []\n    for case in test_cases:\n        speedup = run_penalty_method(case, SOLVER_PARAMS, PENALTY_PARAMS)\n        results.append(speedup)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2423453"}, {"introduction": "A hallmark of a skilled computational engineer is the ability to create algorithms that are not only theoretically sound but also numerically robust. This advanced practice confronts the real-world challenge of numerical overflow, a common pitfall when implementing penalty methods with exponential functions [@problem_id:2423412]. You will design and code a solver that anticipates and avoids floating-point errors by integrating overflow-safe evaluation strategies into a root-finding algorithm, honing your ability to develop reliable and production-quality scientific software.", "problem": "Design and implement a robust exterior-penalty solver that avoids numerical overflow while computing the sequence of penalized minimizers that approach a constrained solution from the infeasible side. Consider the one-dimensional constrained optimization problem in purely mathematical terms:\n- Minimize the objective $f(x) = (x - a)^2$ subject to the inequality constraint $c(x) \\le 0$, where $c(x) = e^{b x} - 1$ and $a  0$, $b  0$. The feasible set is $\\{x \\in \\mathbb{R} : x \\le 0\\}$ and the constrained solution is $x^* = 0$.\n- For an exterior quadratic penalty parameter sequence $\\{\\mu_k\\}_{k=1}^{\\infty}$ with $\\mu_k  0$, define the penalized objective\n$$\nF_k(x) = (x - a)^2 + \\mu_k \\,\\big(\\max\\{0, e^{b x} - 1\\}\\big)^2.\n$$\nFor each $k$, let $x_k^*$ be a global minimizer of $F_k(x)$ over $x \\in \\mathbb{R}$. It is known from first principles of calculus and convexity that $x_k^*  0$ for all finite $\\mu_k$ when $a  0$, and that $x_k^* \\to 0^+$ as $\\mu_k \\to \\infty$; hence the sequence approaches $x^*$ from the infeasible side $x  0$.\nThe difficulty: Evaluations of $e^{b x}$ on the infeasible side can overflow for moderately large $b$ or $x$, even though $x_k^*$ itself stays close to $0$. A robust numerical design must ensure that the computation of $x_k^*$ never triggers overflow, while remaining accurate.\n\nYour tasks:\n1) Starting from fundamental definitions in calculus, reason about the monotonicity and convexity properties that guarantee the existence, uniqueness, and right-sided approach ($x_k^* \\to 0^+$) of penalized minimizers $x_k^*$ for $a  0$.\n\n2) Devise an algorithm to compute $x_k^*$ for given $(a,b,\\mu)$ that:\n- Uses the first-order optimality condition on the right side $x  0$,\n$$\n\\frac{d}{dx}F(x) = 2(x - a) + 2 \\mu b \\, e^{b x}\\,(e^{b x} - 1) = 0 \\quad \\text{for } x  0,\n$$\nand the fact that for $x \\le 0$ the penalty vanishes.\n- Exploits strict monotonicity of $\\frac{d}{dx}F(x)$ in $x$ on $(0,\\infty)$ to apply bracketing and bisection safely on the derivative equation without evaluating $e^{b x}$ at dangerously large arguments.\n- Implements overflow-safe evaluations using elementary techniques such as:\n  • guarding the exponential argument $s = b x$ to avoid computing $e^{s}$ when $2 s$ exceeds the floating-point overflow threshold,\n  • using the function $e^{s}-1$ via a numerically stable form when $|s|$ is small.\nYou must ensure that the code never evaluates any operation that produces an overflow, while still computing accurate roots.\n\n3) Implement a complete program that, for each specified $(a,b,\\mu)$, returns the unique minimizer $x_k^*$ as a real number. Your program must hard-code the test suite below, compute the corresponding minimizers, and print them as the final output.\n\nTest suite:\n- Case $1$: $(a,b,\\mu) = (1,\\;50,\\;10^{-6})$.\n- Case $2$: $(a,b,\\mu) = (1,\\;200,\\;10^{3})$.\n- Case $3$: $(a,b,\\mu) = (1,\\;500,\\;10^{12})$.\n- Case $4$: $(a,b,\\mu) = (1,\\;1000,\\;10^{20})$.\n\nAngle units do not arise. There are no physical units in this problem.\n\nOutput specification:\n- For each case, compute the minimizer $x_k^*$ as a floating-point number and round to $10$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.0123456789,0.0012345678,0.0001234567,0.0000123456]\").\n\nYour implementation must be a complete, runnable program. It must not read any input and must not access any external files or networks. The only allowed libraries are the Python standard library, NumPy, and SciPy as specified by the execution environment. The numerical method you implement must be justified by the reasoning in item $1$ and must be robust against overflow in the evaluation of $e^{b x}$ on the infeasible side $x  0$.", "solution": "The problem provided is a valid and well-posed exercise in computational engineering, specifically concerning the robust implementation of an exterior penalty method for constrained optimization. The problem is scientifically grounded in calculus and numerical analysis, and is free of any inconsistencies or ambiguities.\n\nHerein, I present the complete reasoning and derivation for the specified tasks. All mathematical entities strictly adhere to LaTeX formatting.\n\nFirst, we analyze the properties of the penalized objective function to formally establish the existence and uniqueness of the minimizer, as requested. The problem is to minimize $f(x) = (x - a)^2$ subject to $c(x) = e^{b x} - 1 \\le 0$. Given $a  0$ and $b  0$, the constraint $e^{bx} \\le 1$ is equivalent to $x \\le 0$. The true constrained minimizer is clearly $x^* = 0$, located at the boundary of the feasible set.\n\nThe exterior penalty method utilizes the penalized objective function, which for a penalty parameter $\\mu  0$, is given by:\n$$\nF(x) = (x - a)^2 + \\mu \\left( \\max\\{0, e^{bx} - 1\\} \\right)^2\n$$\nWe analyze this function by splitting its domain into the feasible region ($x \\le 0$) and the infeasible region ($x  0$).\n\n1.  **Analysis in the Feasible Region ($x \\le 0$):**\n    For $x \\le 0$, the constraint is satisfied, so $\\max\\{0, e^{bx} - 1\\} = 0$. The penalized objective simplifies to:\n    $$\n    F(x) = (x - a)^2\n    $$\n    The first derivative is $F'(x) = 2(x - a)$. Since $a  0$ and $x \\le 0$, it follows that $(x - a)$ is strictly negative, and thus $F'(x)  0$. This implies that $F(x)$ is strictly decreasing over the interval $(-\\infty, 0]$. The minimum value on this interval is attained at the right endpoint, $x=0$, where $F(0) = a^2$. The second derivative is $F''(x) = 2  0$, confirming the function is strictly convex on this interval.\n\n2.  **Analysis in the Infeasible Region ($x  0$):**\n    For $x  0$, the constraint is violated, and the penalty term is active:\n    $$\n    F(x) = (x - a)^2 + \\mu (e^{bx} - 1)^2\n    $$\n    The first-order necessary condition for a minimum is $F'(x) = 0$. We define $g(x) = F'(x)$:\n    $$\n    g(x) = \\frac{dF}{dx} = 2(x - a) + 2\\mu(e^{bx} - 1) \\cdot \\frac{d}{dx}(e^{bx} - 1) = 2(x - a) + 2\\mu b e^{bx}(e^{bx} - 1)\n    $$\n    To guarantee a unique minimizer in this region, we analyze the properties of $g(x)$.\n    -   **Behavior at boundaries:**\n        As $x \\to 0^+$, $g(x) \\to 2(0 - a) + 2\\mu b e^0(e^0 - 1) = -2a$. Since $a  0$, $g(0^+)  0$.\n        As $x \\to \\infty$, both terms $2(x - a)$ and $2\\mu b e^{bx}(e^{bx} - 1)$ approach $+\\infty$. Thus, $\\lim_{x\\to\\infty} g(x) = +\\infty$.\n    -   **Monotonicity:** We examine the second derivative of $F(x)$, which is the first derivative of $g(x)$:\n        $$\n        F''(x) = g'(x) = \\frac{d}{dx} \\left[ 2(x - a) + 2\\mu b(e^{2bx} - e^{bx}) \\right] = 2 + 2\\mu b(2be^{2bx} - be^{bx}) = 2 + 2\\mu b^2 e^{bx}(2e^{bx} - 1)\n        $$\n        For $x  0$ and $b  0$, we have $e^{bx}  1$, which implies $(2e^{bx} - 1)  (2 \\cdot 1 - 1) = 1  0$. Given that $\\mu  0$ and $b^2  0$, the entire penalty-related term is strictly positive. Therefore, $F''(x)  2$ for all $x  0$.\n\n    Since $g(x)$ is continuous, starts negative at $x=0^+$, and tends to infinity, the Intermediate Value Theorem guarantees the existence of at least one root $x_k^* \\in (0, \\infty)$. Because $g'(x) = F''(x)  0$, $g(x)$ is strictly increasing on $(0, \\infty)$, which ensures this root is unique. This root corresponds to the unique global minimum of $F(x)$ on $(0, \\infty)$.\n\n3.  **Global Minimizer and Convergence:**\n    The global minimizer of $F(x)$ over $\\mathbb{R}$ is either the minimum on $(-\\infty, 0]$ (which is at $x=0$) or the minimum on $(0, \\infty)$ (at $x_k^*$). Since $F(0) = a^2$ and $F'(0^+) = -2a  0$, the function value decreases as we move from $x=0$ into the positive domain. This necessitates that the minimum value $F(x_k^*)$ must be strictly less than $F(0)$. Hence, the unique global minimizer of the penalized function $F(x)$ is $x_k^*$, and it lies in the interval $(0, \\infty)$, approaching the true solution from the infeasible side.\n    \n    The condition $F'(x_k^*) = 0$ implies $2(x_k^* - a) = -2\\mu_k b e^{bx_k^*}(e^{bx_k^*}-1)$. The right-hand side is negative, so $x_k^* - a  0$, which yields $0  x_k^*  a$. This provides a secure bracketing interval $[0, a]$ for finding the root.\n    \n    As the penalty parameter $\\mu_k \\to \\infty$, the equality can only hold if the non-penalty term is balanced by an infinitely large term, which requires the factor $e^{bx_k^*}(e^{bx_k^*}-1)$ to approach zero. This occurs only as $x_k^* \\to 0^+$, confirming that the sequence of penalized minimizers converges to the constrained solution.\n\n4.  **Robust Numerical Algorithm:**\n    To find the minimizer $x_k^*$, we must solve the nonlinear equation $g(x) = 0$ for $x \\in (0, a)$. The numerical challenge arises from the exponential term $e^{bx}$, which can cause floating-point overflow for large values of $b$ or $x$.\n    Our proposed algorithm employs a bracketing root-finder, specifically the Brent-Dekker method implemented as `scipy.optimize.brentq`, which is robust and efficient. The key is to provide it with a numerically safe function to evaluate $g(x)$.\n\n    The overflow-safe function, let us call it `g_safe(x, a, b, mu)`, will be designed as follows:\n    -   It takes $x, a, b, \\mu$ as input.\n    -   It computes the argument of the exponential, $s = bx$.\n    -   It checks for potential overflow. A standard 64-bit float `exp` function overflows when its argument exceeds approximately $709.78$. The dominant term in $g(x)$ behaves like $e^{2s}$. Therefore, if $2s$ is close to this threshold (e.g., $s  350$), the function's value is guaranteed to be a very large positive number. In this case, `g_safe` will return a pre-defined large value, like `numpy.finfo(float).max`, to signal this to the root-finder without performing the overflowing calculation.\n    -   If $s$ is not in the overflow-prone region, the function proceeds with the calculation. For superior accuracy when $s$ is close to zero, the term $e^s - 1$ is computed using the numerically stable function `numpy.expm1(s)`. The full expression is $g(x) = 2(x - a) + 2\\mu b \\cdot \\text{numpy.exp}(s) \\cdot \\text{numpy.expm1}(s)$.\n\n    By calling `scipy.optimize.brentq(g_safe, 0, a, args=(a, b, mu))`, we leverage a powerful solver that, guided by our safe evaluation function, will efficiently and reliably find the unique root $x_k^*$ without encountering numerical overflow, even when parameters $b$ and $\\mu$ are very large. The search algorithm naturally avoids the region where $x$ is large enough to cause overflow, as the function value there is enormous and positive, pushing the search bracket towards zero.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves for the minimizer of a penalized objective function for a suite of test cases.\n\n    The problem is to minimize f(x) = (x - a)^2 subject to c(x) = exp(b*x) - 1 = 0.\n    The penalized objective is F(x) = (x - a)^2 + mu * (max{0, c(x)})^2.\n    The minimizer x* is found by solving the first-order optimality condition F'(x) = 0\n    for x  0 using a robust, overflow-safe root-finding method.\n    \"\"\"\n\n    # Practical overflow threshold for np.exp can be determined from np.finfo.\n    # np.log(np.finfo(np.float64).max) is approximately 709.78.\n    # The penalty term behaves like exp(2*b*x), so we check if b*x  709.78 / 2.\n    # We use a safe margin for the check.\n    EXP_ARG_SAFE_LIMIT = 350.0\n\n    def g_safe(x, a, b, mu):\n        \"\"\"\n        Calculates the derivative of the penalized objective function F(x) in a numerically\n        robust and overflow-safe manner.\n\n        g(x) = F'(x) = 2(x - a) + 2*mu*b*exp(b*x)*(exp(b*x) - 1) for x  0.\n\n        Args:\n            x (float): The point at which to evaluate the derivative.\n            a (float): The parameter 'a' from the objective function.\n            b (float): The parameter 'b' from the constraint function.\n            mu (float): The penalty parameter.\n\n        Returns:\n            float: The value of F'(x), or a large positive number if overflow is anticipated.\n        \"\"\"\n        # For the region x = 0, the penalty is 0, so F'(x) = 2(x-a).\n        # We are solving for the root in (0, a), so g_safe(0) defines the lower bound value.\n        if x = 0:\n            return 2.0 * (x - a)\n\n        s = b * x\n\n        # Pre-emptive overflow check. If s is large, the exponential term dominates\n        # and will be a large positive number. Returning np.finfo(float).max ensures\n        # the root-finder correctly interprets the sign of g(x) without overflow.\n        if s  EXP_ARG_SAFE_LIMIT:\n            return np.finfo(np.float64).max\n\n        # Use np.expm1(s) for numerically stable computation of exp(s) - 1,\n        # especially important when s is close to 0.\n        penalty_derivative = 2.0 * mu * b * np.exp(s) * np.expm1(s)\n        \n        return 2.0 * (x - a) + penalty_derivative\n\n    # Hard-coded test suite as specified in the problem statement.\n    test_cases = [\n        # (a, b, mu)\n        (1.0, 50.0, 1e-6),\n        (1.0, 200.0, 1e3),\n        (1.0, 500.0, 1e12),\n        (1.0, 1000.0, 1e20),\n    ]\n\n    results = []\n    for a, b, mu in test_cases:\n        # The minimizer x_k^* is guaranteed to be in the interval (0, a).\n        # g_safe(0) = -2a  0\n        # g_safe(a)  0 (or inf, which is handled correctly)\n        # Therefore, we can use a bracketing root-finder.\n        # brentq is a robust and efficient choice for this task.\n        try:\n            minimizer_x = brentq(g_safe, 0.0, a, args=(a, b, mu))\n            results.append(round(minimizer_x, 10))\n        except (ValueError, RuntimeError) as e:\n            # This block should not be reached if the analysis is correct.\n            # It is included as a safeguard.\n            results.append(f\"Error: {e}\")\n\n    # Format the final output as specified.\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2423412"}]}