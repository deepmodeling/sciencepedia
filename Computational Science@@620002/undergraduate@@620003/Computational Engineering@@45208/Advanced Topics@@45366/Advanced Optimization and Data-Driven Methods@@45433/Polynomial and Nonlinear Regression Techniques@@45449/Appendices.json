{"hands_on_practices": [{"introduction": "Many relationships in engineering and science are not simple straight lines. This first exercise demonstrates how to model such nonlinearities using polynomial regression, a powerful technique that transforms the problem into a familiar linear least-squares framework by creating new features from the original variables. By predicting a compound's boiling point from its molecular properties, you will gain hands-on experience with feature engineering and the foundational mechanics of fitting a multivariate polynomial model. [@problem_id:2425201]", "problem": "Consider the task of learning a nonlinear mapping from molecular descriptors to the normal boiling point using a multivariate polynomial regression model. You are given a training set of compounds with three descriptors: molecular weight $M$ in grams per mole, topological polar surface area $A$ in square angstroms, and hydrogen bond donor count $H$ (unitless). The target is the normal boiling point $T_b$ in Kelvin. Your goal is to design and implement a program that, starting from first principles of least squares estimation, fits a second-degree multivariate polynomial model to the training data and then predicts the boiling points for a set of test compounds.\n\nFundamental base and constraints:\n- Start from the definition of least squares estimation as the minimization of the sum of squared residuals between observed and predicted targets.\n- Treat the nonlinear relationship by constructing a polynomial feature map of total degree up to $2$ on standardized inputs. Specifically, for a descriptor vector $\\mathbf{x} = [M,A,H]^\\top$, standardize to $\\mathbf{z} = ( \\mathbf{x} - \\boldsymbol{\\mu} ) \\oslash \\boldsymbol{\\sigma}$, where $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$ are the component-wise mean and standard deviation computed from the training descriptors, and $\\oslash$ denotes elementwise division. Then construct the feature vector $\\boldsymbol{\\phi}(\\mathbf{z})$ containing the constant term, all linear terms, all unique pairwise interaction terms, and all squared terms up to degree $2$.\n- Fit the model parameters by minimizing the squared residuals over the training set. Do not include any regularization term.\n- Use the fitted model to predict $T_b$ for the provided test descriptors.\n\nTraining data (each tuple is $(M,A,H,T_b)$; $M$ in grams per mole, $A$ in square angstroms, $T_b$ in Kelvin):\n- $\\left(60,\\,20,\\,1,\\,369.0\\right)$\n- $\\left(80,\\,35,\\,0,\\,382.6\\right)$\n- $\\left(120,\\,40,\\,2,\\,440.0\\right)$\n- $\\left(150,\\,70,\\,1,\\,503.95\\right)$\n- $\\left(90,\\,25,\\,2,\\,401.9\\right)$\n- $\\left(110,\\,60,\\,0,\\,446.45\\right)$\n- $\\left(140,\\,50,\\,1,\\,449.5\\right)$\n- $\\left(70,\\,15,\\,0,\\,343.6\\right)$\n- $\\left(180,\\,80,\\,2,\\,557.4\\right)$\n- $\\left(200,\\,30,\\,3,\\,433.7\\right)$\n- $\\left(100,\\,45,\\,1,\\,429.95\\right)$\n- $\\left(130,\\,55,\\,2,\\,478.4\\right)$\n\nFeature construction requirements:\n- Compute the training-set mean $\\boldsymbol{\\mu}$ and standard deviation $\\boldsymbol{\\sigma}$ of $M$, $A$, and $H$ using only the training descriptors.\n- For each descriptor vector, compute standardized descriptors $\\mathbf{z} = \\left[z_1, z_2, z_3\\right]^\\top$ as $z_i = \\left(x_i - \\mu_i\\right)/\\sigma_i$ for $i \\in \\{1,2,3\\}$.\n- Construct the degree-$2$ polynomial feature vector\n$$\n\\boldsymbol{\\phi}(\\mathbf{z}) = \\left[1,\\, z_1,\\, z_2,\\, z_3,\\, z_1^2,\\, z_2^2,\\, z_3^2,\\, z_1 z_2,\\, z_1 z_3,\\, z_2 z_3 \\right]^\\top.\n$$\n\nModel fitting requirement:\n- Fit a linear model in the feature space $\\boldsymbol{\\phi}(\\mathbf{z})$ to predict $T_b$ by minimizing the sum of squared residuals over the training set, using ordinary least squares without regularization.\n\nTest suite:\n- Predict $T_b$ for the following four descriptor triplets $(M,A,H)$:\n  1. $\\left(95,\\,38,\\,1\\right)$\n  2. $\\left(160,\\,65,\\,2\\right)$\n  3. $\\left(50,\\,10,\\,0\\right)$\n  4. $\\left(210,\\,85,\\,3\\right)$\n\nAnswer specification and units:\n- Report each predicted normal boiling point in Kelvin.\n- Round each prediction to the nearest $0.01$ Kelvin.\n\nFinal output format:\n- Your program should produce a single line of output containing the four rounded predictions as a comma-separated list enclosed in square brackets, with no additional text. For example, the required format is [$r_1$,$r_2$,$r_3$,$r_4$], where each $r_i$ is a float rounded to the nearest $0.01$ Kelvin.", "solution": "The problem statement submitted for consideration is valid. It presents a well-posed problem in computational engineering, specifically in the domain of quantitative structure-property relationships (QSPR). The task is to construct a predictive model for the normal boiling point of chemical compounds based on a set of molecular descriptors. The problem provides all necessary data, a clear definition of the model structure, and a precise methodology for parameter estimation based on ordinary least squares. The problem is scientifically grounded, logically consistent, and free of ambiguity. Therefore, a rigorous solution can be derived.\n\nThe core of the problem is to fit a multivariate polynomial regression model. Although the relationship between the target variable, boiling point $T_b$, and the original descriptors $\\mathbf{x} = [M, A, H]^\\top$ is nonlinear, the problem is transformed into a linear regression problem by constructing a new set of features, $\\boldsymbol{\\phi}(\\mathbf{z})$. The model hypothesis is linear in the coefficients $\\mathbf{w}$ with respect to these engineered features:\n$$\n\\hat{T}_b = f(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{z}(\\mathbf{x}))\n$$\nwhere $\\mathbf{z}$ is the standardized descriptor vector and $\\mathbf{w}$ is the vector of model parameters to be determined.\n\nThe parameters $\\mathbf{w}$ are found by minimizing the sum of squared residuals (SSR) over the training set of $N$ samples. The SSR, which serves as the cost function $J(\\mathbf{w})$, is given by:\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^{N} (T_{b,i} - \\hat{T}_{b,i})^2 = \\sum_{i=1}^{N} (T_{b,i} - \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{z}_i))^2\n$$\nwhere $T_{b,i}$ is the observed boiling point for the $i$-th sample.\n\nTo facilitate the derivation, we express this in matrix notation. Let $\\mathbf{y}$ be the $N \\times 1$ vector of observed boiling points, and let $\\Phi$ be the $N \\times D$ design matrix, where each row $i$ is the feature vector $\\boldsymbol{\\phi}(\\mathbf{z}_i)^\\top$ for the $i$-th training sample, and $D$ is the number of features (including the constant term). In this problem, $N=12$ and $D=10$. The cost function becomes:\n$$\nJ(\\mathbf{w}) = (\\mathbf{y} - \\Phi\\mathbf{w})^\\top (\\mathbf{y} - \\Phi\\mathbf{w})\n$$\nExpanding this expression yields:\n$$\nJ(\\mathbf{w}) = \\mathbf{y}^\\top\\mathbf{y} - \\mathbf{y}^\\top\\Phi\\mathbf{w} - (\\Phi\\mathbf{w})^\\top\\mathbf{y} + (\\Phi\\mathbf{w})^\\top(\\Phi\\mathbf{w}) = \\mathbf{y}^\\top\\mathbf{y} - 2\\mathbf{w}^\\top\\Phi^\\top\\mathbf{y} + \\mathbf{w}^\\top\\Phi^\\top\\Phi\\mathbf{w}\n$$\nTo find the minimum of this quadratic function, we compute its gradient with respect to $\\mathbf{w}$ and set it to the zero vector:\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2\\Phi^\\top\\mathbf{y} + 2\\Phi^\\top\\Phi\\mathbf{w} = \\mathbf{0}\n$$\nThis leads to the normal equations:\n$$\n(\\Phi^\\top\\Phi)\\mathbf{w} = \\Phi^\\top\\mathbf{y}\n$$\nAssuming the matrix $\\Phi^\\top\\Phi$ is invertible (which is generally true if the columns of $\\Phi$ are linearly independent), the unique solution for the optimal weight vector $\\mathbf{w}_{\\text{OLS}}$ is:\n$$\n\\mathbf{w}_{\\text{OLS}} = (\\Phi^\\top\\Phi)^{-1} \\Phi^\\top\\mathbf{y}\n$$\nThis equation provides the analytical solution for ordinary least squares. The term $(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top$ is known as the Moore-Penrose pseudoinverse of $\\Phi$.\n\nThe implementation will follow these procedural steps:\n1.  **Data Standardization:**\n    First, the training descriptor data, an $N \\times 3$ matrix $\\mathbf{X}_{\\text{train}}$, is used to compute the component-wise mean vector $\\boldsymbol{\\mu}$ and standard deviation vector $\\boldsymbol{\\sigma}$. Standard deviation is calculated using $N$ in the denominator, corresponding to the population standard deviation.\n    $$\n    \\mu_j = \\frac{1}{N} \\sum_{i=1}^{N} X_{ij}, \\quad \\sigma_j = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (X_{ij} - \\mu_j)^2}\n    $$\n    Each training descriptor vector $\\mathbf{x}_i$ is then standardized: $\\mathbf{z}_i = (\\mathbf{x}_i - \\boldsymbol{\\mu}) \\oslash \\boldsymbol{\\sigma}$.\n\n2.  **Feature Engineering:**\n    For each standardized vector $\\mathbf{z}_i = \\left[z_{i,1}, z_{i,2}, z_{i,3}\\right]^\\top$, the feature vector $\\boldsymbol{\\phi}(\\mathbf{z}_i)$ is constructed as specified:\n    $$\n    \\boldsymbol{\\phi}(\\mathbf{z}_i) = \\left[1,\\, z_{i,1},\\, z_{i,2},\\, z_{i,3},\\, z_{i,1}^2,\\, z_{i,2}^2,\\, z_{i,3}^2,\\, z_{i,1} z_{i,2},\\, z_{i,1} z_{i,3},\\, z_{i,2} z_{i,3} \\right]^\\top\n    $$\n    These $N$ feature vectors are stacked as rows to form the $N \\times D$ design matrix $\\Phi$, where $N=12$ and $D=10$.\n\n3.  **Solving the Normal Equations:**\n    The vector of training targets $\\mathbf{y}$ is formed from the given $T_b$ values. The linear system $(\\Phi^\\top\\Phi)\\mathbf{w} = \\Phi^\\top\\mathbf{y}$ is solved for $\\mathbf{w}$. While direct computation of the inverse $(\\Phi^\\top\\Phi)^{-1}$ is possible, it is numerically less stable than using a dedicated linear system solver.\n\n4.  **Prediction:**\n    For each test descriptor triplet $\\mathbf{x}_{\\text{test}}$, it must first be standardized using the same $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$ calculated from the training set. Then, the corresponding feature vector $\\boldsymbol{\\phi}(\\mathbf{z}_{\\text{test}})$ is constructed. The prediction is computed as the dot product:\n    $$\n    \\hat{T}_{b, \\text{test}} = \\mathbf{w}_{\\text{OLS}}^\\top \\boldsymbol{\\phi}(\\mathbf{z}_{\\text{test}})\n    $$\n    The final results are rounded to two decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multivariate polynomial regression problem using ordinary least squares.\n    \"\"\"\n    # Training data: (M, A, H, T_b)\n    # M in g/mol, A in Angstrom^2, H is unitless, T_b in Kelvin.\n    training_data = np.array([\n        (60, 20, 1, 369.0),\n        (80, 35, 0, 382.6),\n        (120, 40, 2, 440.0),\n        (150, 70, 1, 503.95),\n        (90, 25, 2, 401.9),\n        (110, 60, 0, 446.45),\n        (140, 50, 1, 449.5),\n        (70, 15, 0, 343.6),\n        (180, 80, 2, 557.4),\n        (200, 30, 3, 433.7),\n        (100, 45, 1, 429.95),\n        (130, 55, 2, 478.4)\n    ])\n\n    # Test data: (M, A, H)\n    test_cases = np.array([\n        (95, 38, 1),\n        (160, 65, 2),\n        (50, 10, 0),\n        (210, 85, 3)\n    ])\n\n    # Step 1: Separate descriptors and targets from training data\n    X_train = training_data[:, :3]\n    y_train = training_data[:, 3]\n\n    # Step 2: Compute standardization parameters from the training set\n    mu = np.mean(X_train, axis=0)\n    # ddof=0 for population standard deviation, as is standard when not specified\n    sigma = np.std(X_train, axis=0, ddof=0)\n\n    # Standardize the training descriptors\n    Z_train = (X_train - mu) / sigma\n\n    # Step 3: Construct the design matrix Phi from the standardized training data\n    def construct_feature_matrix(Z):\n        \"\"\"Constructs the polynomial feature matrix from standardized descriptors.\"\"\"\n        n_samples = Z.shape[0]\n        z1, z2, z3 = Z[:, 0], Z[:, 1], Z[:, 2]\n        \n        # The feature vector is [1, z1, z2, z3, z1^2, z2^2, z3^2, z1*z2, z1*z3, z2*z3]\n        phi = np.zeros((n_samples, 10))\n        phi[:, 0] = 1\n        phi[:, 1] = z1\n        phi[:, 2] = z2\n        phi[:, 3] = z3\n        phi[:, 4] = z1**2\n        phi[:, 5] = z2**2\n        phi[:, 6] = z3**2\n        phi[:, 7] = z1 * z2\n        phi[:, 8] = z1 * z3\n        phi[:, 9] = z2 * z3\n        return phi\n\n    Phi_train = construct_feature_matrix(Z_train)\n\n    # Step 4: Solve the normal equations to find the weight vector w\n    # (Phi^T * Phi) * w = Phi^T * y\n    # Use np.linalg.solve for numerical stability instead of direct inversion.\n    try:\n        A = Phi_train.T @ Phi_train\n        b = Phi_train.T @ y_train\n        w = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Fallback to least-squares solver if matrix is singular.\n        # This is more robust and directly minimizes the sum of squares.\n        w, _, _, _ = np.linalg.lstsq(Phi_train, y_train, rcond=None)\n\n    # Step 5: Predict T_b for the test cases\n    # Standardize test data using training set parameters\n    Z_test = (test_cases - mu) / sigma\n    \n    # Construct the feature matrix for the test data\n    Phi_test = construct_feature_matrix(Z_test)\n    \n    # Perform prediction\n    predictions = Phi_test @ w\n\n    # Round the predictions to the nearest 0.01 Kelvin\n    rounded_predictions = np.round(predictions, 2)\n    \n    # Format the final output string\n    result_str = \",\".join([f\"{p:.2f}\" for p in rounded_predictions])\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```", "id": "2425201"}, {"introduction": "The standard least-squares method assumes that the error variance is constant for all observations, a condition known as homoscedasticity. This practice tackles the more realistic scenario of heteroscedasticity, where the error variance changes, by implementing an iteratively reweighted least squares (IRLS) algorithm. You will learn how to assign weights to data points to improve model accuracy and build a more robust polynomial regression model. [@problem_id:2425232]", "problem": "You are given independent paired observations $\\{(x_i,y_i)\\}_{i=1}^{n}$ and asked to fit a polynomial regression model under heteroscedastic noise in which the measurement error variance is proportional to the magnitude of the mean response. Concretely, assume a polynomial of degree $d$,\n$$\n\\mu(x;\\boldsymbol{\\beta}) \\equiv \\beta_0 + \\beta_1 x + \\cdots + \\beta_d x^d,\n$$\nand suppose the errors satisfy $y_i = \\mu(x_i;\\boldsymbol{\\beta}) + \\varepsilon_i$ with $\\mathbb{E}[\\varepsilon_i]=0$ and $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2 \\,|\\mu(x_i;\\boldsymbol{\\beta})|$, where $\\sigma^2$ is an unknown constant and $|\\,\\cdot\\,|$ denotes absolute value. The weighting function proportional to the inverse variance thus depends on the unknown mean. Use a feasible weighted least squares procedure based on iteratively reweighted least squares to estimate $\\boldsymbol{\\beta}$:\n- Start with an unweighted ordinary least squares fit to obtain an initial estimate $\\widehat{\\boldsymbol{\\beta}}^{(0)}$.\n- At iteration $t \\in \\{1,2,\\dots\\}$, form predicted means $\\widehat{\\mu}_i^{(t-1)} = \\mu(x_i;\\widehat{\\boldsymbol{\\beta}}^{(t-1)})$ and set weights\n$$\nw_i^{(t)} = \\frac{1}{\\max\\left(|\\widehat{\\mu}_i^{(t-1)}|,\\tau\\right)},\n$$\nwhere $\\tau$ is a small positive threshold to avoid division by zero. Then compute the weighted least squares estimate\n$$\n\\widehat{\\boldsymbol{\\beta}}^{(t)} \\in \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} w_i^{(t)}\\left(y_i - \\mu(x_i;\\boldsymbol{\\beta})\\right)^2.\n$$\n- Stop when the relative change in the parameter vector is small:\n$$\n\\frac{\\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} - \\widehat{\\boldsymbol{\\beta}}^{(t-1)} \\rVert_2}{\\max\\left(1,\\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} \\rVert_2\\right)} < \\text{tol},\n$$\nor when a maximum number of iterations is reached.\n\nUse the following fixed algorithmic hyperparameters for all test cases: threshold $\\tau = 10^{-3}$, tolerance $\\text{tol} = 10^{-10}$, and maximum iterations $T_{\\max} = 100$.\n\nFor each test case, your program must:\n- Construct the design matrix with columns $[1,x,x^2,\\dots,x^d]$.\n- Run the feasible weighted least squares iterations as defined above.\n- After termination, report:\n  1. The fitted coefficient vector $\\widehat{\\boldsymbol{\\beta}}$ as a list of $d+1$ floating-point numbers.\n  2. The final weighted residual sum of squares\n     $$\n     S = \\sum_{i=1}^{n} w_i^{(\\text{final})}\\left(y_i - \\mu(x_i;\\widehat{\\boldsymbol{\\beta}})\\right)^2,\n     $$\n     where $w_i^{(\\text{final})}$ are the weights of the last iteration computed from the final predicted means.\n  3. The integer number of iterations $k$ actually performed.\n\nRound each floating-point output to $6$ decimal places. The integer output $k$ is not rounded.\n\nTest suite:\n- Case 1 (general, degree 2):\n  - $d = 2$,\n  - $x = [0, 1, 2, 3, 4, 5, 6, 7]$,\n  - $y = [1.00, 0.72, 0.77, 1.34, 2.15, 3.60, 5.05, 7.50]$.\n- Case 2 (includes negative and near-zero responses, degree 2):\n  - $d = 2$,\n  - $x = [-3, -2, -1, 0, 1, 2, 3]$,\n  - $y = [-2.10, -1.55, -0.60, -0.20, 0.12, 0.17, 0.11]$.\n- Case 3 (includes an exact zero measurement, degree 2):\n  - $d = 2$,\n  - $x = [0, 1, 2, 3]$,\n  - $y = [0.00, 0.05, 0.20, 0.45]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form\n$$\n[\\,[\\beta_0,\\beta_1,\\dots,\\beta_d],\\,S,\\,k\\,],\n$$\nwith all floating-point entries rounded to $6$ decimals. For the three cases above, the required output format is\n$$\n\\big[\\,[\\,[\\beta_0,\\beta_1,\\beta_2],S,k],\\;[\\,[\\beta_0,\\beta_1,\\beta_2],S,k],\\;[\\,[\\beta_0,\\beta_1,\\beta_2],S,k]\\,\\big],\n$$\nprinted with no spaces, for example\n$$\n[[[\\beta_0,\\beta_1,\\beta_2],S,k],[[\\beta_0,\\beta_1,\\beta_2],S,k],[[\\beta_0,\\beta_1,\\beta_2],S,k]].\n$$\n\nNo physical units or angles are involved. All outputs must be pure numbers as specified and must be formatted exactly as a single line with no extra characters or whitespace.", "solution": "The problem statement is assessed to be valid and well-posed. It describes a standard procedure in computational statistics for fitting a polynomial regression model in the presence of heteroscedastic noise, where the variance of the error term is proportional to the magnitude of the mean response. The proposed method, feasible weighted least squares (FWLS), implemented via iteratively reweighted least squares (IRLS), is a correct and established technique for this class of problems. All necessary data, model specifications, algorithmic steps, hyperparameters, and output formats are provided, allowing for a unique and verifiable solution.\n\nThe core of the problem is to estimate the parameter vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$ for the polynomial model $\\mu(x;\\boldsymbol{\\beta}) = \\sum_{j=0}^{d} \\beta_j x^j$. The observed data $y_i$ are assumed to follow the model $y_i = \\mu(x_i;\\boldsymbol{\\beta}) + \\varepsilon_i$, with error variance $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2 |\\mu(x_i;\\boldsymbol{\\beta})|$.\n\nThe optimal estimator for such a model is the weighted least squares (WLS) estimator, which minimizes the weighted sum of squared residuals. The weights $w_i$ should be inversely proportional to the variance of the observations, i.e., $w_i \\propto 1/|\\mu(x_i;\\boldsymbol{\\beta})|$. However, the true mean $\\mu(x_i;\\boldsymbol{\\beta})$ is unknown. The IRLS algorithm resolves this by iteratively approximating the optimal weights.\n\nThe procedure is as follows:\n\nLet the given data be $\\{(x_i, y_i)\\}_{i=1}^{n}$. First, we construct the $n \\times (d+1)$ design matrix $\\mathbf{X}$, where the $i$-th row is given by $[1, x_i, x_i^2, \\dots, x_i^d]$. The model can then be expressed in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$.\n\n**Step 0: Initialization**\nAn initial estimate $\\widehat{\\boldsymbol{\\beta}}^{(0)}$ is obtained by ignoring the heteroscedasticity and performing an ordinary least squares (OLS) fit. This minimizes the unweighted sum of squared errors, $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$. The solution is given by the normal equations:\n$$\n\\widehat{\\boldsymbol{\\beta}}^{(0)} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n**Step 1: Iterative Refinement**\nFor each iteration $t=1, 2, \\dots, T_{\\max}$:\n1.  **Estimate Mean Response**: Using the parameter estimate from the previous iteration, $\\widehat{\\boldsymbol{\\beta}}^{(t-1)}$, we compute the predicted mean for each observation:\n    $$\n    \\widehat{\\boldsymbol{\\mu}}^{(t-1)} = \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^{(t-1)}\n    $$\n2.  **Update Weights**: We update the weights based on the estimated mean. The weight for the $i$-th observation is:\n    $$\n    w_i^{(t)} = \\frac{1}{\\max\\left(|\\widehat{\\mu}_i^{(t-1)}|, \\tau\\right)}\n    $$\n    where $\\tau = 10^{-3}$ is a small positive threshold to prevent division by zero or excessively large weights when the predicted mean is close to zero. These weights are assembled into a diagonal matrix $\\mathbf{W}^{(t)}$.\n\n3.  **Solve WLS Problem**: A new parameter estimate $\\widehat{\\boldsymbol{\\beta}}^{(t)}$ is found by solving the WLS problem, which minimizes the weighted sum of squared errors:\n    $$\n    \\widehat{\\boldsymbol{\\beta}}^{(t)} = \\arg\\min_{\\boldsymbol{\\beta}} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{W}^{(t)} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n    $$\n    The solution is given by the weighted normal equations:\n    $$\n    \\widehat{\\boldsymbol{\\beta}}^{(t)} = (\\mathbf{X}^T\\mathbf{W}^{(t)}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}^{(t)}\\mathbf{y}\n    $$\n    Numerically, this is solved robustly by transforming the system. Let $\\mathbf{D}^{(t)}$ be a diagonal matrix with diagonal entries $\\sqrt{w_i^{(t)}}$. The problem is equivalent to solving an OLS problem for the transformed variables $\\mathbf{y}' = \\mathbf{D}^{(t)}\\mathbf{y}$ and $\\mathbf{X}' = \\mathbf{D}^{(t)}\\mathbf{X}$.\n\n**Step 2: Convergence Check**\nThe iterative process is terminated when the relative change in the parameter vector is smaller than a specified tolerance $\\text{tol} = 10^{-10}$:\n$$\n\\frac{\\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} - \\widehat{\\boldsymbol{\\beta}}^{(t-1)} \\rVert_2}{\\max\\left(1, \\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} \\rVert_2\\right)} < \\text{tol}\n$$\nor when the maximum number of iterations, $T_{\\max} = 100$, is reached. Let the final iteration count be $k$ and the final parameter estimate be $\\widehat{\\boldsymbol{\\beta}}$.\n\n**Step 3: Final Output Calculation**\nUpon termination, the following quantities are calculated:\n1.  The final coefficient vector $\\widehat{\\boldsymbol{\\beta}}$.\n2.  The final weighted residual sum of squares, $S$. This requires computing the final predicted means $\\widehat{\\boldsymbol{\\mu}} = \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}$ and the corresponding final weights $w_i^{(\\text{final})} = 1/\\max(|\\widehat{\\mu}_i|, \\tau)$. The sum $S$ is then:\n    $$\n    S = \\sum_{i=1}^{n} w_i^{(\\text{final})}\\left(y_i - \\widehat{\\mu}_i\\right)^2\n    $$\n3.  The total number of iterations performed, $k$.\n\nThis procedure is applied to each test case specified in the problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of test cases for feasible weighted least squares\n    polynomial regression.\n    \"\"\"\n\n    def solve_case(d, x_data, y_data):\n        \"\"\"\n        Implements the Iteratively Reweighted Least Squares (IRLS) algorithm for a\n        single case.\n        \"\"\"\n        # Algorithmic hyperparameters\n        tau = 1e-3\n        tol = 1e-10\n        T_max = 100\n\n        x_np = np.asarray(x_data, dtype=float)\n        y_np = np.asarray(y_data, dtype=float)\n\n        # Construct the design matrix X with columns [1, x, x^2, ..., x^d]\n        X = np.vander(x_np, d + 1, increasing=True)\n        \n        # Step 0: Initial OLS fit\n        beta_current, _, _, _ = np.linalg.lstsq(X, y_np, rcond=None)\n        \n        k = 0\n        for t in range(1, T_max + 1):\n            k = t\n            beta_previous = beta_current.copy()\n\n            # a. Calculate predicted means\n            mu_hat_previous = X @ beta_previous\n\n            # b. Calculate weights\n            weights = 1.0 / np.maximum(np.abs(mu_hat_previous), tau)\n            \n            # c. Update beta using a weighted least squares fit.\n            # This is solved by transforming the system and using OLS.\n            D = np.diag(np.sqrt(weights))\n            X_prime = D @ X\n            y_prime = D @ y_np\n            \n            beta_current, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n\n            # d. Check for convergence\n            norm_beta_current = np.linalg.norm(beta_current)\n            diff_norm = np.linalg.norm(beta_current - beta_previous)\n            \n            relative_change = diff_norm / np.maximum(1.0, norm_beta_current)\n\n            if relative_change < tol:\n                break\n        \n        # After termination, compute final outputs\n        beta_final = beta_current\n        \n        # Calculate final predicted means from the final beta\n        mu_final = X @ beta_final\n        \n        # Calculate final weights based on the final predicted means\n        w_final = 1.0 / np.maximum(np.abs(mu_final), tau)\n        \n        # Calculate final weighted residual sum of squares (S)\n        residuals = y_np - mu_final\n        S = np.sum(w_final * (residuals**2))\n        \n        return beta_final, S, k\n\n    test_cases = [\n        # Case 1 (general, degree 2)\n        (2, [0., 1., 2., 3., 4., 5., 6., 7.], [1.00, 0.72, 0.77, 1.34, 2.15, 3.60, 5.05, 7.50]),\n        # Case 2 (includes negative and near-zero responses, degree 2)\n        (2, [-3., -2., -1., 0., 1., 2., 3.], [-2.10, -1.55, -0.60, -0.20, 0.12, 0.17, 0.11]),\n        # Case 3 (includes an exact zero measurement, degree 2)\n        (2, [0., 1., 2., 3.], [0.00, 0.05, 0.20, 0.45])\n    ]\n\n    results = []\n    for d_val, x_val, y_val in test_cases:\n        beta, S, k_val = solve_case(d_val, x_val, y_val)\n        \n        # Format results as specified: round floats to 6 decimal places.\n        beta_rounded = [round(b, 6) for b in beta]\n        S_rounded = round(S, 6)\n        \n        results.append([beta_rounded, S_rounded, k_val])\n\n    # Print the final result in the exact single-line format with no spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2425232"}, {"introduction": "While polynomial models are versatile, many physical processes are described by equations that are intrinsically nonlinear in their parameters. This exercise introduces you to fitting such models using the Gauss–Newton method, a cornerstone of nonlinear least-squares optimization. By modeling pharmacokinetic data, you will derive the analytical Jacobian matrix and implement a parameter update step, providing fundamental skills for tackling a wide range of advanced regression problems. [@problem_id:2425266]", "problem": "You are tasked with deriving and implementing the Jacobian matrix for a nonlinear least-squares fit of a two-compartment pharmacokinetic concentration-time model. The model is given by the sum of two decaying exponentials: for time $t$ in hours and concentration $C(t)$ in milligrams per liter (mg/L),\n$$\nC(t) = A e^{-\\alpha t} + B e^{-\\beta t},\n$$\nwhere $A$ and $B$ are amplitudes in milligrams per liter, and $\\alpha$ and $\\beta$ are decay rates in inverse hours. Starting from the standard definitions of nonlinear least squares and Jacobian matrices, derive the Jacobian of the residuals vector with respect to the parameter vector $\\theta = [A, B, \\alpha, \\beta]^T$, and implement it in a program. Use the Jacobian to perform a single Gauss–Newton step with Levenberg–Marquardt damping for a set of test cases.\n\nBase your derivation solely on the following fundamental facts:\n- The residual vector for data $\\{(t_i, y_i)\\}_{i=1}^N$ is $r_i(\\theta) = y_i - C(t_i; \\theta)$.\n- The Jacobian matrix is defined by $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$.\n- The Gauss–Newton step $\\Delta \\theta$ approximately minimizes the sum of squared residuals by solving the linear system\n$$\n\\left(J^T J + \\lambda I\\right) \\Delta \\theta = J^T r,\n$$\nwhere $\\lambda$ is a damping parameter and $I$ is the identity matrix.\n\nImplementation requirements:\n1. Derive the Jacobian analytically from the definitions above. Do not assume any pre-derived formulas beyond the model definition.\n2. Implement a function to compute $C(t; \\theta)$ and the Jacobian of the residuals with respect to $\\theta$.\n3. Implement one Gauss–Newton iteration with Levenberg–Marquardt damping to compute $\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta$, using a fixed damping $\\lambda = 10^{-6}$.\n4. Use the following test suite. In each case, generate synthetic observations $y_i$ exactly from the model with the given true parameters (no noise), then start from the given initial guess and compute exactly one update step. All time inputs are in hours and all concentrations are in milligrams per liter. Output parameter estimates $A$ and $B$ in milligrams per liter and $\\alpha$ and $\\beta$ in inverse hours.\n   - Case 1 (general case with $t=0$ included): \n     - Times: $0$, $0.5$, $1$, $2$, $4$, $8$.\n     - True parameters: $A = 5$, $B = 2$, $\\alpha = 0.8$, $\\beta = 0.1$.\n     - Initial guess: $A = 4.5$, $B = 1$, $\\alpha = 0.6$, $\\beta = 0.2$.\n   - Case 2 (nearly collinear exponentials, tests conditioning):\n     - Times: $0$, $1$, $2$, $3$, $6$.\n     - True parameters: $A = 3$, $B = 3$, $\\alpha = 0.2$, $\\beta = 0.2001$.\n     - Initial guess: $A = 2.5$, $B = 3.5$, $\\alpha = 0.22$, $\\beta = 0.19$.\n   - Case 3 (one amplitude equal to zero):\n     - Times: $0$, $1$, $3$, $5$, $10$.\n     - True parameters: $A = 2$, $B = 0$, $\\alpha = 0.5$, $\\beta = 0.2$.\n     - Initial guess: $A = 1$, $B = 0.5$, $\\alpha = 0.3$, $\\beta = 0.3$.\n   - Case 4 (slow decays over long times):\n     - Times: $0$, $6$, $12$, $18$, $24$.\n     - True parameters: $A = 1.5$, $B = 0.5$, $\\alpha = 0.05$, $\\beta = 0.01$.\n     - Initial guess: $A = 1.2$, $B = 0.4$, $\\alpha = 0.06$, $\\beta = 0.008$.\n5. Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concatenate, in order, the updated parameters for each case as $[A_1, B_1, \\alpha_1, \\beta_1, A_2, B_2, \\alpha_2, \\beta_2, A_3, B_3, \\alpha_3, \\beta_3, A_4, B_4, \\alpha_4, \\beta_4]$, where the subscript denotes the case number. Express $A$ and $B$ in milligrams per liter and $\\alpha$ and $\\beta$ in inverse hours. Print each numerical value rounded to six decimal places. Angles are not involved. No percentages appear in the final output.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Model for concentration $C(t)$: $C(t) = A e^{-\\alpha t} + B e^{-\\beta t}$.\n- Parameter vector: $\\theta = [A, B, \\alpha, \\beta]^T$.\n- Residual vector component: $r_i(\\theta) = y_i - C(t_i; \\theta)$ for data $\\{(t_i, y_i)\\}_{i=1}^N$.\n- Jacobian matrix definition: $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$.\n- Gauss-Newton linear system with Levenberg-Marquardt damping: $(J^T J + \\lambda I) \\Delta \\theta = J^T r$.\n- Damping parameter: $\\lambda = 10^{-6}$.\n- Parameter update rule: $\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta$.\n- Test Cases: Four cases are provided, each with a set of time points $t_i$, true parameters for generating synthetic data $y_i$, and an initial parameter guess $\\theta_{\\text{old}}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on the two-compartment pharmacokinetic model, a standard model in pharmacology. The solution method, Gauss-Newton with Levenberg-Marquardt damping, is a fundamental and widely used algorithm for nonlinear least squares optimization. All definitions are standard and correct.\n- **Well-Posed:** The problem is well-posed. It requires the computation of a single, well-defined step of an iterative algorithm. All necessary data and constants ($\\lambda$, initial guesses, time points) are provided. The Levenberg-Marquardt modification $(J^T J + \\lambda I)$ ensures the matrix is positive definite and thus invertible for $\\lambda > 0$, guaranteeing a unique solution for the parameter update $\\Delta \\theta$.\n- **Objective:** The problem is stated using precise, objective mathematical language.\n- **Conclusion:** The problem is scientifically sound, well-posed, objective, and contains sufficient information for a unique solution. It is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation and Implementation**\n\nThe objective is to compute a single parameter update for a nonlinear least-squares problem using the Gauss-Newton method with Levenberg-Marquardt damping. This requires the analytical derivation of the Jacobian matrix of the residuals.\n\nThe parameter vector is $\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4]^T = [A, B, \\alpha, \\beta]^T$.\nThe model function for concentration at time $t_i$ is:\n$$C(t_i; \\theta) = A e^{-\\alpha t_i} + B e^{-\\beta t_i}$$\nThe residual for the $i$-th observation $(t_i, y_i)$ is:\n$$r_i(\\theta) = y_i - C(t_i; \\theta)$$\nThe Jacobian matrix $J$ has elements $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$. Since the observed data $y_i$ are constant with respect to the parameters $\\theta$, the differentiation applies only to the model function:\n$$J_{ij} = \\frac{\\partial}{\\partial \\theta_j} (y_i - C(t_i; \\theta)) = -\\frac{\\partial C(t_i; \\theta)}{\\partial \\theta_j}$$\nThe Jacobian matrix will have $N$ rows, corresponding to the $N$ data points, and $4$ columns, corresponding to the four parameters in $\\theta$. We now compute the partial derivatives of $C(t_i; \\theta)$ with respect to each parameter.\n\n1.  **Partial derivative with respect to $A$ ($\\theta_1$):**\n    $$\\frac{\\partial C(t_i)}{\\partial A} = \\frac{\\partial}{\\partial A} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = e^{-\\alpha t_i}$$\n    Thus, the first column of the Jacobian is $J_{i1} = -e^{-\\alpha t_i}$.\n\n2.  **Partial derivative with respect to $B$ ($\\theta_2$):**\n    $$\\frac{\\partial C(t_i)}{\\partial B} = \\frac{\\partial}{\\partial B} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = e^{-\\beta t_i}$$\n    Thus, the second column of the Jacobian is $J_{i2} = -e^{-\\beta t_i}$.\n\n3.  **Partial derivative with respect to $\\alpha$ ($\\theta_3$):**\n    $$\\frac{\\partial C(t_i)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = A \\cdot (e^{-\\alpha t_i} \\cdot (-t_i)) = -A t_i e^{-\\alpha t_i}$$\n    Thus, the third column of the Jacobian is $J_{i3} = -(-A t_i e^{-\\alpha t_i}) = A t_i e^{-\\alpha t_i}$.\n\n4.  **Partial derivative with respect to $\\beta$ ($\\theta_4$):**\n    $$\\frac{\\partial C(t_i)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = B \\cdot (e^{-\\beta t_i} \\cdot (-t_i)) = -B t_i e^{-\\beta t_i}$$\n    Thus, the fourth column of the Jacobian is $J_{i4} = -(-B t_i e^{-\\beta t_i}) = B t_i e^{-\\beta t_i}$.\n\nCombining these results, the $i$-th row of the Jacobian matrix $J$ is given by:\n$$[J]_{i,:} = \\begin{bmatrix} -e^{-\\alpha t_i} & -e^{-\\beta t_i} & A t_i e^{-\\alpha t_i} & B t_i e^{-\\beta t_i} \\end{bmatrix}$$\nThis Jacobian is evaluated using the current parameter estimates, $\\theta_{\\text{old}}$.\n\nThe Gauss-Newton algorithm proceeds as follows for each test case:\n1.  Generate synthetic data points $y_i$ using the provided true parameters $(\\theta_{\\text{true}})$ and time points $t_i$: $y_i = C(t_i; \\theta_{\\text{true}})$.\n2.  Set the current parameters $\\theta$ to the initial guess $\\theta_{\\text{old}}$.\n3.  Compute the residual vector $r$, where each element is $r_i = y_i - C(t_i; \\theta_{\\text{old}})$.\n4.  Compute the Jacobian matrix $J$ using the derived formula, evaluated at $\\theta_{\\text{old}}$ and for all $t_i$.\n5.  Construct the approximate Hessian matrix $H = J^T J + \\lambda I$, where $J^T$ is the transpose of $J$, $\\lambda = 10^{-6}$ is the damping parameter, and $I$ is the $4 \\times 4$ identity matrix.\n6.  Construct the right-hand side vector $g = J^T r$.\n7.  Solve the linear system of normal equations $H \\Delta \\theta = g$ for the parameter update vector $\\Delta \\theta$.\n8.  Calculate the new parameter estimate $\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta$.\n\nThis procedure will be implemented and executed for each of the four test cases specified in the problem statement. The resulting $\\theta_{\\text{new}}$ vectors for each case are concatenated to form the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements the Jacobian for a two-compartment pharmacokinetic model,\n    and performs a single Gauss-Newton step with Levenberg-Marquardt damping\n    for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case with t=0 included)\n        {\n            \"times\": np.array([0, 0.5, 1, 2, 4, 8], dtype=np.float64),\n            \"true_params\": np.array([5.0, 2.0, 0.8, 0.1], dtype=np.float64),\n            \"initial_guess\": np.array([4.5, 1.0, 0.6, 0.2], dtype=np.float64),\n        },\n        # Case 2 (nearly collinear exponentials, tests conditioning)\n        {\n            \"times\": np.array([0, 1, 2, 3, 6], dtype=np.float64),\n            \"true_params\": np.array([3.0, 3.0, 0.2, 0.2001], dtype=np.float64),\n            \"initial_guess\": np.array([2.5, 3.5, 0.22, 0.19], dtype=np.float64),\n        },\n        # Case 3 (one amplitude equal to zero)\n        {\n            \"times\": np.array([0, 1, 3, 5, 10], dtype=np.float64),\n            \"true_params\": np.array([2.0, 0.0, 0.5, 0.2], dtype=np.float64),\n            \"initial_guess\": np.array([1.0, 0.5, 0.3, 0.3], dtype=np.float64),\n        },\n        # Case 4 (slow decays over long times)\n        {\n            \"times\": np.array([0, 6, 12, 18, 24], dtype=np.float64),\n            \"true_params\": np.array([1.5, 0.5, 0.05, 0.01], dtype=np.float64),\n            \"initial_guess\": np.array([1.2, 0.4, 0.06, 0.008], dtype=np.float64),\n        }\n    ]\n\n    lambda_damping = 1e-6\n    all_results = []\n\n    def model(t, params):\n        \"\"\"Computes C(t) = A*exp(-alpha*t) + B*exp(-beta*t)\"\"\"\n        A, B, alpha, beta = params\n        return A * np.exp(-alpha * t) + B * np.exp(-beta * t)\n\n    def jacobian(t, params):\n        \"\"\"\n        Computes the Jacobian of the residuals vector with respect to the parameters.\n        The i-th row is [d(r_i)/dA, d(r_i)/dB, d(r_i)/d_alpha, d(r_i)/d_beta].\n        Since r_i = y_i - C_i, d(r_i)/d(theta_j) = -d(C_i)/d(theta_j).\n        \"\"\"\n        A, B, alpha, beta = params\n        num_points = len(t)\n        J = np.zeros((num_points, 4), dtype=np.float64)\n\n        exp_alpha_t = np.exp(-alpha * t)\n        exp_beta_t = np.exp(-beta * t)\n\n        # dC/dA = exp(-alpha*t)\n        J[:, 0] = -exp_alpha_t\n        # dC/dB = exp(-beta*t)\n        J[:, 1] = -exp_beta_t\n        # dC/d_alpha = -A*t*exp(-alpha*t)\n        J[:, 2] = -(-A * t * exp_alpha_t)\n        # dC/d_beta = -B*t*exp(-beta*t)\n        J[:, 3] = -(-B * t * exp_beta_t)\n        \n        return J\n\n    for case in test_cases:\n        times = case[\"times\"]\n        true_params = case[\"true_params\"]\n        theta_old = case[\"initial_guess\"]\n\n        # Step 1: Generate synthetic observation data (without noise)\n        y_obs = model(times, true_params)\n\n        # Step 2: Calculate residuals based on the initial guess\n        y_pred = model(times, theta_old)\n        residuals = y_obs - y_pred\n\n        # Step 3: Calculate the Jacobian matrix at the initial guess\n        J = jacobian(times, theta_old)\n\n        # Step 4: Form and solve the Gauss-Newton system with LM damping\n        # (J^T J + lambda I) * delta_theta = J^T * r\n        J_T = J.T\n        J_T_J = J_T @ J\n        \n        # Add damping factor\n        H = J_T_J + lambda_damping * np.identity(4)\n        \n        rhs = J_T @ residuals\n\n        delta_theta = np.linalg.solve(H, rhs)\n\n        # Step 5: Update the parameters for one iteration\n        theta_new = theta_old + delta_theta\n        all_results.extend(theta_new)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\nsolve()\n```", "id": "2425266"}]}