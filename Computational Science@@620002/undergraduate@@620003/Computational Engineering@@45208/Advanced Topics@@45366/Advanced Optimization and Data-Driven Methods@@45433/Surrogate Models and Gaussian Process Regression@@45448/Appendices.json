{"hands_on_practices": [{"introduction": "This first exercise provides a foundational hands-on experience with Gaussian Process regression. You will model a simple one-dimensional relationship, calculating the posterior predictive mean and variance, which are the cornerstones of GP analysis [@problem_id:2441367]. This practice is crucial for developing an intuition for how GP models make predictions and quantify uncertainty, particularly in response to changes in key hyperparameters like the length scale, $\\ell$.", "problem": "A scalar-valued latent function $f$ maps a runner's pace $p$ (in minutes per kilometer) to heart rate $f(p)$ (in beats per minute). You are given noisy observations $\\{(p_i, y_i)\\}_{i=1}^n$ where $y_i$ are measured heart rates in beats per minute. Assume an additive noise model $y_i = f(p_i) + \\varepsilon_i$ with independent Gaussian noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$.\n\nAssume a zero-mean Gaussian process prior for $f$ with covariance function\n$$\nk(p,p') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(p - p')^2}{2\\ell^2}\\right),\n$$\nwhere $\\sigma_f$ is the signal scale (in beats per minute), and $\\ell$ is the length scale (in minutes per kilometer).\n\nThe training data are the following $n = 7$ pace-heart rate pairs:\n- Paces (minutes per kilometer): $[3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5]$.\n- Measured heart rates (beats per minute): $[176, 168, 160, 150, 140, 130, 122]$.\n\nAssume $\\sigma_n = 2.0$ (in beats per minute) unless otherwise stated below. For each of the test cases listed below, compute the posterior predictive mean $\\mu_\\star$ (in beats per minute) and the posterior predictive standard deviation $s_\\star$ (in beats per minute) for a single test pace $p_\\star$, under the specified hyperparameters. Report $\\mu_\\star$ and $s_\\star$ rounded to three decimal places.\n\nTest suite (each case is independent):\n- Case $1$: $p_\\star = 5.2$, $\\sigma_f = 60.0$, $\\ell = 0.6$, $\\sigma_n = 2.0$.\n- Case $2$: $p_\\star = 3.2$, $\\sigma_f = 60.0$, $\\ell = 0.6$, $\\sigma_n = 2.0$.\n- Case $3$: $p_\\star = 7.0$, $\\sigma_f = 60.0$, $\\ell = 0.6$, $\\sigma_n = 2.0$.\n- Case $4$: $p_\\star = 5.2$, $\\sigma_f = 60.0$, $\\ell = 0.2$, $\\sigma_n = 2.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\mu_\\star^{(1)}, s_\\star^{(1)}, \\mu_\\star^{(2)}, s_\\star^{(2)}, \\mu_\\star^{(3)}, s_\\star^{(3)}, \\mu_\\star^{(4)}, s_\\star^{(4)}]$. All eight numbers must be expressed in beats per minute and rounded to three decimal places, with no spaces. For example: $[150.123,2.345,...]$ (note: the actual values will differ).", "solution": "The problem statement is first subjected to rigorous validation. All givens are checked for scientific soundness, self-consistency, and objectivity.\n\nThe problem specifies a regression task to model a latent function $f(p)$ which maps a runner's pace $p$ (in minutes per kilometer) to heart rate $f(p)$ (in beats per minute). The model is a Gaussian Process with a zero-mean prior, $f \\sim \\mathcal{GP}(0, k(p, p'))$. The covariance function is the squared exponential kernel:\n$$k(p,p') = \\sigma_f^2 \\exp\\left(-\\frac{(p - p')^2}{2\\ell^2}\\right)$$\nThe observations $y_i$ are noisy measurements of the latent function, described by the model $y_i = f(p_i) + \\varepsilon_i$, where the noise $\\varepsilon_i$ is independent and identically distributed as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nThe training data consists of $n=7$ pairs of pace $p_i$ and observed heart rate $y_i$:\n-   Paces $P = [3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5]$ min/km.\n-   Measured heart rates $Y = [176, 168, 160, 150, 140, 130, 122]$ bpm.\n\nThe task is to compute the posterior predictive mean $\\mu_\\star$ and posterior predictive standard deviation $s_\\star$ for a single test pace $p_\\star$ for four distinct test cases. For all cases, the noise standard deviation is given as $\\sigma_n = 2.0$ bpm.\n\nValidation Verdict: The problem is valid. It constitutes a well-posed and scientifically grounded problem in the field of surrogate modeling, a sub-discipline of computational engineering. The problem provides all necessary data and model specifications, is free from internal contradictions, and is formulated with objective, unambiguous language. We may proceed with the analytical solution.\n\nThe theoretical foundation for the solution is the derivation of the posterior predictive distribution for a Gaussian Process model. Let the training inputs be denoted by the vector $X = [p_1, \\dots, p_n]^T$ and the corresponding training outputs by $\\mathbf{y} = [y_1, \\dots, y_n]^T$. The value of the latent function at a new test point $p_\\star$ is denoted by $f_\\star = f(p_\\star)$.\n\nAccording to the definition of a Gaussian Process, the latent function values at the training points, $\\mathbf{f} = [f(p_1), \\dots, f(p_n)]^T$, and the latent value at the test point, $f_\\star$, follow a joint Gaussian distribution. Since the prior mean is zero, this distribution is:\n$$ \\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) & K(X, p_\\star) \\\\ K(p_\\star, X) & k(p_\\star, p_\\star) \\end{pmatrix} \\right) $$\nwhere $K(X, X)$ is the $n \\times n$ covariance matrix with entries $K_{ij} = k(p_i, p_j)$, $K(X, p_\\star)$ is the $n \\times 1$ vector of covariances between training points and the test point, and $k(p_\\star, p_\\star)$ is the prior variance at the test point.\n\nThe observation model $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$, implies that the joint distribution of the observed training outputs $\\mathbf{y}$ and the latent test value $f_\\star$ is also Gaussian:\n$$ \\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, p_\\star) \\\\ K(p_\\star, X) & k(p_\\star, p_\\star) \\end{pmatrix} \\right) $$\nFor notational convenience, let $K = K(X, X)$, $\\mathbf{k}_\\star = K(X, p_\\star)$, and $k_{\\star\\star} = k(p_\\star, p_\\star)$. The matrix $K_y = K + \\sigma_n^2 I$ represents the covariance of the noisy observations.\n\nThe posterior predictive distribution $p(f_\\star | X, \\mathbf{y}, p_\\star)$ is derived by applying the rules for conditioning on a multivariate Gaussian distribution. This yields a Gaussian posterior with the following mean and variance:\n\nPosterior predictive mean:\n$$ \\mu_\\star = \\mathbb{E}[f_\\star | X, \\mathbf{y}, p_\\star] = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y} $$\nPosterior predictive variance:\n$$ \\sigma_\\star^2 = \\text{Var}[f_\\star | X, \\mathbf{y}, p_\\star] = k_{\\star\\star} - \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_\\star $$\nThe problem requires the posterior predictive standard deviation, which is $s_\\star = \\sqrt{\\sigma_\\star^2}$. This quantity measures the uncertainty in the estimate of the true function value $f(p_\\star)$, not the noisy observation $y_\\star$.\n\nThe step-by-step computational procedure for each test case is as follows:\n$1$. Define the training data vectors $X$ and $\\mathbf{y}$, of size $n=7$.\n$2$. For a given test case with hyperparameters $\\sigma_f, \\ell$, noise $\\sigma_n$, and test point $p_\\star$:\n$3$. Construct the $7 \\times 7$ training covariance matrix $K$ with entries $K_{ij} = \\sigma_f^2 \\exp(-\\frac{(p_i - p_j)^2}{2\\ell^2})$.\n$4$. Construct the $7 \\times 1$ test covariance vector $\\mathbf{k}_\\star$ with entries $k_{\\star,i} = k(p_i, p_\\star) = \\sigma_f^2 \\exp(-\\frac{(p_i - p_\\star)^2}{2\\ell^2})$.\n$5$. The prior variance at the test point is $k_{\\star\\star} = k(p_\\star, p_\\star) = \\sigma_f^2$.\n$6$. Form the noisy covariance matrix $K_y = K + \\sigma_n^2 I$, where $I$ is the $7 \\times 7$ identity matrix and $\\sigma_n^2 = 2.0^2 = 4.0$.\n$7$. To ensure numerical stability, direct matrix inversion is avoided. Instead, we solve two systems of linear equations. First, solve $(K_y) \\boldsymbol{\\alpha} = \\mathbf{y}$ for the vector $\\boldsymbol{\\alpha}$. The posterior mean is then given by the dot product $\\mu_\\star = \\mathbf{k}_\\star^T \\boldsymbol{\\alpha}$.\n$8$. Second, solve $(K_y) \\mathbf{v} = \\mathbf{k}_\\star$ for the vector $\\mathbf{v}$. The posterior variance is then computed as $\\sigma_\\star^2 = k_{\\star\\star} - \\mathbf{k}_\\star^T \\mathbf{v}$. The most robust method for solving these systems is via Cholesky decomposition of $K_y$, which is guaranteed to be symmetric and positive-definite.\n$9$. The posterior standard deviation is $s_\\star = \\sqrt{\\sigma_\\star^2}$. Care must be taken to handle potential minor negative values for $\\sigma_\\star^2$ arising from floating-point arithmetic errors by ensuring the argument of the square root is non-negative.\n$10$. The resulting values for $\\mu_\\star$ and $s_\\star$ are rounded to three decimal places.\n\nThis complete procedure is implemented programmatically for each of the four specified test cases. The results are assembled into the final output format as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Computes the posterior predictive mean and standard deviation for a Gaussian Process\n    regression model based on the provided problem statement.\n    \"\"\"\n    \n    # Define the training data from the problem statement.\n    p_train = np.array([3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5])\n    y_train = np.array([176, 168, 160, 150, 140, 130, 122])\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (p_star, sigma_f, l, sigma_n).\n    test_cases = [\n        (5.2, 60.0, 0.6, 2.0),\n        (3.2, 60.0, 0.6, 2.0),\n        (7.0, 60.0, 0.6, 2.0),\n        (5.2, 60.0, 0.2, 2.0),\n    ]\n\n    results = []\n    \n    # Reshape training inputs to be a column vector for broadcasting\n    p_train_col = p_train[:, np.newaxis]\n\n    for case in test_cases:\n        p_star, sigma_f, l, sigma_n = case\n\n        # Main logic to calculate the result for one case goes here.\n        \n        # 1. Define the squared exponential covariance function (kernel).\n        def kernel(a, b, sf, ls):\n            \"\"\"Squared exponential kernel.\"\"\"\n            # Using scipy.spatial.distance.cdist would be cleaner but not allowed.\n            # We use broadcasting to compute the squared Euclidean distances.\n            sqdist = (a - b.T)**2\n            return (sf**2) * np.exp(-sqdist / (2 * ls**2))\n\n        # 2. Compute the covariance matrices and vectors.\n        # K(X, X): Covariance of training inputs\n        K = kernel(p_train_col, p_train_col, sigma_f, l)\n        \n        # K_y = K(X, X) + sigma_n^2 * I: Covariance of noisy observations\n        K_y = K + (sigma_n**2) * np.eye(len(p_train))\n        \n        # k_star = K(X, x_star): Covariance between training and test inputs\n        k_star = kernel(p_train_col, np.array([[p_star]]), sigma_f, l).flatten()\n        \n        # k_star_star = k(x_star, x_star): Prior variance at the test point\n        k_star_star = sigma_f**2\n\n        # 3. Compute posterior predictive mean and variance.\n        # Use Cholesky decomposition for stable and efficient linear system solving.\n        # K_y is symmetric and positive definite. L is its lower-triangular Cholesky factor.\n        try:\n            L = cholesky(K_y, lower=True)\n        except np.linalg.LinAlgError:\n            # This should not happen with a valid kernel and non-zero noise.\n            # Handle as an error if it occurs.\n            results.extend([np.nan, np.nan])\n            continue\n            \n        # Solve (K + sigma_n^2*I) * alpha = y for alpha.\n        # This is equivalent to alpha = inv(K + sigma_n^2*I) @ y.\n        alpha = cho_solve((L, True), y_train)\n        \n        # Calculate posterior mean: mu_star = k_star.T @ alpha\n        mu_star = k_star.T @ alpha\n        \n        # Solve (K + sigma_n^2*I) * v = k_star for v.\n        # This is equivalent to v = inv(K + sigma_n^2*I) @ k_star.\n        v = cho_solve((L, True), k_star)\n        \n        # Calculate posterior variance: sigma_star^2 = k_star_star - k_star.T @ v\n        var_star = k_star_star - k_star.T @ v\n        \n        # Ensure variance is non-negative due to potential floating point errors.\n        s_star = np.sqrt(max(0, var_star))\n        \n        # 4. Append rounded results to the list.\n        results.append(round(mu_star, 3))\n        results.append(round(s_star, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441367"}, {"introduction": "Building on the fundamentals, this next practice elevates the challenge to a two-dimensional problem, a scenario more typical of real-world engineering applications. You will use an anisotropic kernel, which assigns a unique length scale to each input dimension, to model the response of a biological system [@problem_id:2441369]. This exercise is key to understanding how to adapt GP models for multidimensional inputs that have different physical characteristics and scales of variation.", "problem": "A laboratory characterizes the specific growth rate of a bacterial colony as a function of nutrient concentration and temperature. Let the input be the two-dimensional vector $x = (n,t)$, where $n$ is nutrient concentration in milligrams per milliliter and $t$ is temperature in degrees Celsius. The response is the specific growth rate $g(x)$ in inverse hours. You must construct a surrogate model for $g(x)$ using Gaussian Process (GP) regression with a zero-mean prior, an anisotropic squared-exponential covariance, and homoscedastic Gaussian observation noise. The covariance function is\n$$\nk\\big((n,t),(n',t')\\big) = \\sigma_f^2 \\exp\\left( -\\frac{1}{2}\\left(\\frac{(n-n')^2}{\\ell_n^2} + \\frac{(t-t')^2}{\\ell_t^2}\\right) \\right),\n$$\nwith hyperparameters $\\sigma_f = 0.9$, $\\ell_n = 3.0$, $\\ell_t = 5.0$. Observations have independent additive Gaussian noise with standard deviation $\\sigma_n = 0.05$.\n\nYou are given the following laboratory experiments, each reporting $(n,t)$ and the measured growth rate $g$:\n- $(n,t) = (0.5,25.0)$, $g = 0.10$.\n- $(n,t) = (1.0,30.0)$, $g = 0.25$.\n- $(n,t) = (2.0,30.0)$, $g = 0.40$.\n- $(n,t) = (4.0,35.0)$, $g = 0.80$.\n- $(n,t) = (6.0,37.0)$, $g = 1.00$.\n- $(n,t) = (8.0,40.0)$, $g = 0.85$.\n- $(n,t) = (10.0,42.0)$, $g = 0.60$.\n\nUsing the Gaussian Process regression model specified above and the provided experiments, compute the posterior predictive mean of $g$ at the following query conditions $(n,t)$:\n- $(5.0,37.0)$,\n- $(0.5,20.0)$,\n- $(10.0,42.0)$,\n- $(2.0,30.0)$,\n- $(7.0,33.0)$,\n- $(0.0,37.0)$.\n\nExpress each predicted mean growth rate in inverse hours and round each value to six decimal places. The test suite of queries covers an interior point, boundary conditions, exact training inputs, and extrapolation beyond the observed nutrient range, respectively.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the queries above (for example, $[r_1,r_2,r_3,r_4,r_5,r_6]$), where each $r_i$ is a float rounded to six decimal places in inverse hours.", "solution": "The problem requires the construction of a surrogate model using Gaussian Process (GP) regression to predict the specific growth rate of a bacterial colony, $g$, as a function of nutrient concentration, $n$, and temperature, $t$. The task is to compute the posterior predictive mean of the growth rate at several query points, given a set of experimental observations and specified model hyperparameters.\n\nA Gaussian Process defines a distribution over functions. We model the unknown growth rate function $g(x)$ as a draw from a GP. The model is specified by a mean function $\\mu(x)$ and a covariance function (or kernel) $k(x,x')$. The problem states a zero-mean prior, so $\\mu(x) = 0$. The observations are assumed to be corrupted by independent and identically distributed Gaussian noise, such that if $f(x)$ is the true underlying function value, the observed value is $y = f(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nFor a set of $N$ training points $X = \\{x_1, \\dots, x_N\\}$ with corresponding observations $y = \\{y_1, \\dots, y_N\\}^T$, and a set of $M$ test points $X_* = \\{x_{*1}, \\dots, x_{*M}\\}$, the joint distribution of the observed values $y$ and the function values $f_*$ at the test points is Gaussian:\n$$\n\\begin{pmatrix} y \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N} \\left( 0,\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I & K(X, X_*) \\\\\nK(X_*, X) & K(X_*, X_*)\n\\end{pmatrix}\n\\right)\n$$\nwhere $K(A, B)$ is the matrix of covariances computed between all pairs of points in sets $A$ and $B$, $I$ is the $N \\times N$ identity matrix, and $\\sigma_n^2$ is the noise variance.\n\nBy conditioning the joint distribution on the known observations $y$, we obtain the posterior predictive distribution for $f_*$, which is also a Gaussian, $p(f_* | X, y, X_*) = \\mathcal{N}(\\bar{f}_*, \\text{cov}(f_*))$. The mean of this posterior distribution, which is the desired prediction, is given by the standard GP regression equation:\n$$\n\\bar{f}_* = K(X_*, X) [K(X, X) + \\sigma_n^2 I]^{-1} y\n$$\nThe computational procedure follows from this equation.\n\nThe specific inputs for this problem are:\n- Hyperparameters: Signal standard deviation $\\sigma_f = 0.9$, noise standard deviation $\\sigma_n = 0.05$, and anisotropic length-scales $\\ell_n = 3.0$ and $\\ell_t = 5.0$.\n- Covariance function: The anisotropic squared-exponential kernel,\n$$\nk\\big((n,t),(n',t')\\big) = \\sigma_f^2 \\exp\\left( -\\frac{1}{2}\\left(\\frac{(n-n')^2}{\\ell_n^2} + \\frac{(t-t')^2}{\\ell_t^2}\\right) \\right)\n$$\n- Training data ($N=7$ points):\n  The inputs are $X = \\begin{bmatrix} 0.5 & 25.0 \\\\ 1.0 & 30.0 \\\\ 2.0 & 30.0 \\\\ 4.0 & 35.0 \\\\ 6.0 & 37.0 \\\\ 8.0 & 40.0 \\\\ 10.0 & 42.0 \\end{bmatrix}$ and the observed outputs are $y = \\begin{bmatrix} 0.10, 0.25, 0.40, 0.80, 1.00, 0.85, 0.60 \\end{bmatrix}^T$.\n- Query points ($M=6$ points):\n  The query inputs are $X_* = \\begin{bmatrix} 5.0 & 37.0 \\\\ 0.5 & 20.0 \\\\ 10.0 & 42.0 \\\\ 2.0 & 30.0 \\\\ 7.0 & 33.0 \\\\ 0.0 & 37.0 \\end{bmatrix}$.\n\nThe calculation proceeds as follows:\n\nStep 1: Compute the $N \\times N$ training covariance matrix, $K(X, X)$.\nThe element $(i, j)$ of this matrix is calculated using the kernel function $k(x_i, x_j)$. For example, the diagonal elements are $k(x_i, x_i) = \\sigma_f^2 \\exp(0) = 0.9^2 = 0.81$. The off-diagonal elements measure the correlation between different training points.\n\nStep 2: Form the noisy observation covariance matrix, $K_y$.\nThis matrix is formed by adding the noise variance $\\sigma_n^2$ to the diagonal of $K(X, X)$.\n$$K_y = K(X, X) + \\sigma_n^2 I$$\nHere, $\\sigma_n^2 = 0.05^2 = 0.0025$. This term regularizes the matrix, making it invertible, and accounts for the uncertainty in the observations.\n\nStep 3: Solve for the weight vector $\\alpha$.\nFor numerical stability and efficiency, we avoid explicit matrix inversion. Instead, we solve the linear system of equations $K_y \\alpha = y$ for the vector $\\alpha$. This vector can be interpreted as a set of weights corresponding to each training point.\n$$\\alpha = [K(X, X) + \\sigma_n^2 I]^{-1} y$$\n\nStep 4: Compute the $M \\times N$ cross-covariance matrix, $K(X_*, X)$.\nThe element $(j, i)$ of this matrix is $k(x_{*j}, x_i)$, representing the covariance between the $j$-th test point and the $i$-th training point.\n\nStep 5: Compute the posterior predictive mean, $\\bar{f}_*$.\nThe final predictions are obtained by taking a weighted sum of the columns of $K(X_*, X)$, where the weights are given by the vector $\\alpha$. This is a simple matrix-vector product:\n$$\\bar{f}_* = K(X_*, X) \\alpha$$\nThis yields a vector of $M$ predicted mean values, one for each query point in $X_*$.\n\nThe numerical implementation of these steps is performed using matrix operations. Vectorized computation of the squared-distance matrices, which are arguments to the exponential function in the kernel, provides an efficient method to construct the covariance matrices $K(X, X)$ and $K(X_*, X)$. The resulting vector of predictive means is then rounded to six decimal places to produce the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a Gaussian Process regression model and computes the posterior\n    predictive mean for a given set of query points.\n    \"\"\"\n    # Define the hyperparameters for the GP model.\n    # sigma_f: Signal standard deviation for the squared-exponential kernel.\n    # l_n, l_t: Anisotropic length-scales for nutrient and temperature.\n    # sigma_n: Standard deviation of the homoscedastic Gaussian noise.\n    sigma_f = 0.9\n    l_n = 3.0\n    l_t = 5.0\n    sigma_n = 0.05\n\n    # Define the training data from laboratory experiments.\n    # X_train: (N x 2) matrix of input features (n, t).\n    # y_train: (N,) vector of observed growth rates g.\n    X_train = np.array([\n        [0.5, 25.0],\n        [1.0, 30.0],\n        [2.0, 30.0],\n        [4.0, 35.0],\n        [6.0, 37.0],\n        [8.0, 40.0],\n        [10.0, 42.0]\n    ])\n    y_train = np.array([0.10, 0.25, 0.40, 0.80, 1.00, 0.85, 0.60])\n\n    # Define the query points for which predictions are required.\n    # X_query: (M x 2) matrix of query inputs (n, t).\n    test_cases = [\n        (5.0, 37.0),\n        (0.5, 20.0),\n        (10.0, 42.0),\n        (2.0, 30.0),\n        (7.0, 33.0),\n        (0.0, 37.0),\n    ]\n    X_query = np.array(test_cases)\n\n    # Anisotropic length-scales organized into a vector.\n    length_scales = np.array([l_n, l_t])\n\n    def kernel(X1, X2, sigma_f_val, length_scales_val):\n        \"\"\"\n        Computes the anisotropic squared-exponential covariance matrix between\n        two sets of input points X1 and X2.\n        \n        Args:\n            X1 (np.ndarray): An (N x D) array of N points in D dimensions.\n            X2 (np.ndarray): An (M x D) array of M points in D dimensions.\n            sigma_f_val (float): The signal variance hyperparameter.\n            length_scales_val (np.ndarray): A (D,) array of length-scales.\n        \n        Returns:\n            np.ndarray: The (N x M) covariance matrix.\n        \"\"\"\n        # Scale inputs by their respective length scales for anisotropic kernel.\n        X1_scaled = X1 / length_scales_val\n        X2_scaled = X2 / length_scales_val\n\n        # Efficiently compute the matrix of squared Euclidean distances\n        # between all pairs of points from X1_scaled and X2_scaled.\n        sq_dists = np.sum(X1_scaled**2, axis=1)[:, np.newaxis] + \\\n                   np.sum(X2_scaled**2, axis=1) - \\\n                   2 * (X1_scaled @ X2_scaled.T)\n        \n        # Apply the squared-exponential formula.\n        return sigma_f_val**2 * np.exp(-0.5 * sq_dists)\n\n    # Step 1: Compute K(X, X), the covariance matrix of training data.\n    K_XX = kernel(X_train, X_train, sigma_f, length_scales)\n\n    # Step 2: Add observation noise variance to the diagonal to get K_y.\n    N = X_train.shape[0]\n    K_y = K_XX + (sigma_n**2) * np.eye(N)\n\n    # Step 3: Solve the linear system K_y * alpha = y_train for alpha.\n    # This is numerically more stable than computing K_y^-1 directly.\n    alpha = np.linalg.solve(K_y, y_train)\n\n    # Step 4: Compute K(X_*, X), the cross-covariance between query and training points.\n    K_Xstar_X = kernel(X_query, X_train, sigma_f, length_scales)\n    \n    # Step 5: Compute the posterior predictive mean by multiplying K(X_*, X) with alpha.\n    f_star_mean = K_Xstar_X @ alpha\n\n    # Format the results: round to six decimal places as specified.\n    results = [round(val, 6) for val in f_star_mean]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441369"}, {"introduction": "Our final exercise moves into one of the most powerful aspects of Gaussian Processes: custom kernel design. Standard kernels assume properties like stationarity, but many real-world functions have abrupt changes or other complex structures. In this problem, you will engineer a non-stationary kernel to explicitly model a known discontinuity, learning how to encode specific prior knowledge directly into your model [@problem_id:2441430].", "problem": "Consider a one-dimensional regression problem modeled by a zero-mean Gaussian process with a nonstationary covariance designed to accommodate a known discontinuity at a point $x=c$. Let $c=0.0$. Define two independent latent Gaussian processes $f_{\\mathrm{L}}(x)$ and $f_{\\mathrm{R}}(x)$ with covariance functions $k_{\\mathrm{L}}(x,x')$ and $k_{\\mathrm{R}}(x,x')$, respectively. The observed function is defined as\n$$\nf(x) = w(x)\\,f_{\\mathrm{L}}(x) + \\bigl(1-w(x)\\bigr)\\,f_{\\mathrm{R}}(x),\n$$\nwhere $w(x)$ is a known weighting function that transitions between the left and right processes. This induces a valid covariance function\n$$\nk(x,x') = w(x)\\,w(x')\\,k_{\\mathrm{L}}(x,x') + \\bigl(1-w(x)\\bigr)\\bigl(1-w(x')\\bigr)\\,k_{\\mathrm{R}}(x,x').\n$$\nAssume independent and identically distributed Gaussian observation noise with variance $\\sigma_n^2$.\n\nUse squared-exponential base kernels on each side:\n$$\nk_{\\mathrm{SE}}(x,x';\\sigma^2,\\ell) = \\sigma^2 \\exp\\!\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right).\n$$\nSpecify\n- Left-side kernel: $k_{\\mathrm{L}}(x,x') = k_{\\mathrm{SE}}(x,x';\\sigma_{\\mathrm{L}}^2,\\ell_{\\mathrm{L}})$ with $\\sigma_{\\mathrm{L}}^2=1.0$ and $\\ell_{\\mathrm{L}}=0.25$.\n- Right-side kernel: $k_{\\mathrm{R}}(x,x') = k_{\\mathrm{SE}}(x,x';\\sigma_{\\mathrm{R}}^2,\\ell_{\\mathrm{R}})$ with $\\sigma_{\\mathrm{R}}^2=0.8$ and $\\ell_{\\mathrm{R}}=0.5$.\n\nConsider two definitions of the weighting function $w(x)$:\n- Hard step (piecewise): \n$$\nw_{\\mathrm{step}}(x;c)=\\begin{cases}\n1 & \\text{if } x<c,\\\\\n0 & \\text{if } x\\ge c.\n\\end{cases}\n$$\n- Sigmoid (smooth): for steepness parameter $s>0$,\n$$\nw_{\\mathrm{sig}}(x;c,s)=\\frac{1}{1+\\exp\\!\\bigl(s(x-c)\\bigr)}.\n$$\n\nTraining data are given at input locations\n$$\n\\mathbf{X}=[-1.0,-0.6,-0.2,0.2,0.6,1.0],\n$$\nwith noiseless targets generated from known piecewise functions at $c=0.0$:\n$$\nf_{\\mathrm{L}}(x) = 1 + 0.5\\sin(2\\pi x),\\quad f_{\\mathrm{R}}(x) = -1 + 0.5\\cos(2\\pi x),\n$$\nand\n$$\ny_i = \\begin{cases}\nf_{\\mathrm{L}}(x_i) & \\text{if } x_i < c,\\\\\nf_{\\mathrm{R}}(x_i) & \\text{if } x_i \\ge c.\n\\end{cases}\n$$\nUse a Gaussian observation noise variance of $\\sigma_n^2 = 10^{-10}$.\n\nUse the above prior and likelihood to compute the Gaussian process posterior predictive mean at the query points\n$$\n\\mathbf{X}_\\star=[-0.75,0.0,0.75].\n$$\n\nTest Suite. Evaluate the posterior mean at $\\mathbf{X}_\\star$ for each of the following three parameter sets:\n- Test $1$: weighting $w(x)=w_{\\mathrm{step}}(x;c)$ with $c=0.0$.\n- Test $2$: weighting $w(x)=w_{\\mathrm{sig}}(x;c,s)$ with $c=0.0$ and $s=5$.\n- Test $3$: weighting $w(x)=w_{\\mathrm{sig}}(x;c,s)$ with $c=0.0$ and $s=50$.\n\nYour program must:\n- Implement the covariance $k(x,x')$ given above using the specified $w(x)$ and base kernels.\n- Construct the Gaussian process posterior given the training data and $\\sigma_n^2$.\n- Produce the posterior mean at each point of $\\mathbf{X}_\\star$ for each test in the order Test $1$, Test $2$, Test $3$.\n\nFinal Output Format. Your program should produce a single line of output containing the nine posterior mean values, ordered as $[\\mu_\\star^{(1)}(x_{\\star,1}),\\mu_\\star^{(1)}(x_{\\star,2}),\\mu_\\star^{(1)}(x_{\\star,3}),\\mu_\\star^{(2)}(x_{\\star,1}),\\dots,\\mu_\\star^{(3)}(x_{\\star,3})]$, where superscripts index the test cases and subscripts index the query points in $\\mathbf{X}_\\star$ as listed above. Print the results as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places (for example, $[0.123456,0.000000,-1.234568,...]$). No other text should be printed.", "solution": "The problem statement is valid. It is self-contained, scientifically grounded in the theory of Gaussian processes, and possesses a unique, computable solution. We proceed with the derivation.\n\nThe objective is to compute the posterior predictive mean $\\boldsymbol{\\mu}_\\star$ of a Gaussian process (GP) model at a set of query points $\\mathbf{X}_\\star$. The model is defined with a zero-mean prior. For a zero-mean GP, the posterior predictive mean is given by the standard expression:\n$$\n\\boldsymbol{\\mu}_\\star = K(\\mathbf{X}_\\star, \\mathbf{X}) \\left( K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 I \\right)^{-1} \\mathbf{y}\n$$\nwhere $\\mathbf{X}$ is the matrix of $N$ training input locations, $\\mathbf{y}$ is the vector of corresponding target observations, $\\sigma_n^2$ is the variance of the i.i.d. Gaussian observation noise, $I$ is the $N \\times N$ identity matrix, and $K(\\cdot, \\cdot)$ is the covariance function, also known as the kernel.\n\nThe central feature of this problem is the specialized non-stationary covariance function designed to model a discontinuity at a point $c$. This function is constructed by blending two independent latent Gaussian processes, $f_{\\mathrm{L}}(x)$ and $f_{\\mathrm{R}}(x)$, each with its own covariance function, $k_{\\mathrm{L}}(x,x')$ and $k_{\\mathrm{R}}(x,x')$, respectively. The blending is controlled by a known weighting function $w(x)$:\n$$\nk(x,x') = w(x)\\,w(x')\\,k_{\\mathrm{L}}(x,x') + \\bigl(1-w(x)\\bigr)\\bigl(1-w(x')\\bigr)\\,k_{\\mathrm{R}}(x,x')\n$$\nThe base kernels for the left and right processes are the squared-exponential (SE) kernels, defined as:\n$$\nk_{\\mathrm{SE}}(x,x';\\sigma^2,\\ell) = \\sigma^2 \\exp\\!\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)\n$$\nThe parameters for these kernels are given as $(\\sigma_{\\mathrm{L}}^2, \\ell_{\\mathrm{L}}) = (1.0, 0.25)$ for $k_{\\mathrm{L}}$ and $(\\sigma_{\\mathrm{R}}^2, \\ell_{\\mathrm{R}}) = (0.8, 0.5)$ for $k_{\\mathrm{R}}$.\n\nThe solution is obtained by following a systematic computational procedure:\n\n1.  **Generate Training Data**: The training targets $\\mathbf{y}$ are first computed for the given training inputs $\\mathbf{X} = \\bigl[-1.0,\\,-0.6,\\,-0.2,\\,0.2,\\,0.6,\\,1.0\\bigr]$ and discontinuity at $c=0.0$. The values are determined by the piecewise function:\n    $$\n    y_i = \\begin{cases}\n    1 + 0.5\\sin(2\\pi x_i) & \\text{if } x_i < 0.0 \\\\\n    -1 + 0.5\\cos(2\\pi x_i) & \\text{if } x_i \\ge 0.0\n    \\end{cases}\n    $$\n\n2.  **Iterate Through Test Cases**: The subsequent steps are performed for each of the three specified test scenarios, which differ only in the choice of the weighting function $w(x)$.\n    -   Test 1: $w(x) = w_{\\mathrm{step}}(x; 0.0) = \\begin{cases} 1 & \\text{if } x<0.0,\\\\ 0 & \\text{if } x\\ge 0.0. \\end{cases}$\n    -   Test 2: $w(x) = w_{\\mathrm{sig}}(x; 0.0, 5) = \\frac{1}{1+\\exp(5x)}$.\n    -   Test 3: $w(x) = w_{\\mathrm{sig}}(x; 0.0, 50) = \\frac{1}{1+\\exp(50x)}$.\n\n3.  **Construct Covariance Matrices**: For each test case, the necessary covariance matrices are assembled.\n    -   The $N \\times N$ training covariance matrix $K(\\mathbf{X}, \\mathbf{X})$ and the $N_\\star \\times N$ test-training cross-covariance matrix $K(\\mathbf{X}_\\star, \\mathbf{X})$ are computed. For two sets of inputs $\\mathbf{A}$ (size $M$) and $\\mathbf{B}$ (size $N$), the composite covariance matrix $K(\\mathbf{A}, \\mathbf{B})$ is calculated as follows:\n        a. Compute the base kernel matrices $K_{\\mathrm{L}}(\\mathbf{A}, \\mathbf{B})$ and $K_{\\mathrm{R}}(\\mathbf{A}, \\mathbf{B})$.\n        b. Compute the weight vectors $\\mathbf{w_A}$ and $\\mathbf{w_B}$ by applying the selected $w(x)$ to each element of $\\mathbf{A}$ and $\\mathbf{B}$.\n        c. The final matrix is formed by the Hadamard (element-wise) product of the respective matrices:\n           $$\n           K(\\mathbf{A}, \\mathbf{B}) = (\\mathbf{w_A} \\mathbf{w_B}^T) \\odot K_{\\mathrm{L}}(\\mathbf{A}, \\mathbf{B}) + \\left((\\mathbf{1}-\\mathbf{w_A})(\\mathbf{1}-\\mathbf{w_B})^T\\right) \\odot K_{\\mathrm{R}}(\\mathbf{A}, \\mathbf{B})\n           $$\n           where $\\mathbf{w_A} \\mathbf{w_B}^T$ is the outer product of the weight vectors.\n\n4.  **Solve for Posterior Mean**: With the matrices constructed, the posterior mean is found.\n    a. The noisy covariance matrix of the training data is formed: $K_y = K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 I$. The small noise term $\\sigma_n^2 = 10^{-10}$ ensures numerical stability.\n    b. To avoid the explicit computation of a matrix inverse, which is a numerically inferior practice, we solve the linear system of equations $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ for the weight vector $\\boldsymbol{\\alpha}$. This is equivalent to setting $\\boldsymbol{\\alpha} = K_y^{-1}\\mathbf{y}$.\n    c. The posterior predictive mean vector $\\boldsymbol{\\mu}_\\star$ is then obtained by the matrix-vector product $\\boldsymbol{\\mu}_\\star = K(\\mathbf{X}_\\star, \\mathbf{X}) \\boldsymbol{\\alpha}$.\n\nThis procedure is executed for all three test cases, and the resulting nine values for the posterior means are concatenated to form the final result.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian process regression problem with a non-stationary kernel.\n    \"\"\"\n\n    # --- 1. GIVENS: Define problem parameters and data ---\n\n    # Discontinuity point\n    c = 0.0\n\n    # Kernel parameters\n    sigma_L_sq = 1.0\n    l_L = 0.25\n    sigma_R_sq = 0.8\n    l_R = 0.5\n\n    # Observation noise variance\n    sigma_n_sq = 1e-10\n\n    # Training and query points\n    X_train = np.array([-1.0, -0.6, -0.2, 0.2, 0.6, 1.0])\n    X_star = np.array([-0.75, 0.0, 0.75])\n\n    # --- 2. GENERATE TRAINING DATA (y) ---\n\n    def f_L(x):\n        return 1.0 + 0.5 * np.sin(2.0 * np.pi * x)\n\n    def f_R(x):\n        return -1.0 + 0.5 * np.cos(2.0 * np.pi * x)\n\n    y_train = np.array([\n        f_L(x) if x < c else f_R(x) for x in X_train\n    ])\n\n    # --- 3. KERNEL AND WEIGHTING FUNCTIONS ---\n\n    def k_se(X1, X2, sigma_sq, l):\n        \"\"\"Squared-exponential kernel.\"\"\"\n        sqdist = np.subtract.outer(X1, X2)**2\n        return sigma_sq * np.exp(-0.5 * sqdist / l**2)\n\n    def w_step(x, c):\n        \"\"\"Hard step weighting function.\"\"\"\n        return np.where(x < c, 1.0, 0.0)\n\n    def w_sig(x, c, s):\n        \"\"\"Sigmoid weighting function.\"\"\"\n        return 1.0 / (1.0 + np.exp(s * (x - c)))\n\n    def k_composite(X1, X2, w_func, w_params):\n        \"\"\"Composite non-stationary kernel.\"\"\"\n        # Evaluate base kernels\n        k_L_matrix = k_se(X1, X2, sigma_L_sq, l_L)\n        k_R_matrix = k_se(X1, X2, sigma_R_sq, l_R)\n\n        # Evaluate weighting function at input points\n        w1 = w_func(X1, **w_params)\n        w2 = w_func(X2, **w_params)\n\n        # Compute outer products of weights\n        W_L_outer = np.outer(w1, w2)\n        W_R_outer = np.outer(1.0 - w1, 1.0 - w2)\n\n        # Combine using the formula with Hadamard product\n        return W_L_outer * k_L_matrix + W_R_outer * k_R_matrix\n\n    # --- 4. MAIN COMPUTATION LOOP ---\n    \n    test_cases = [\n        {\"w_func\": w_step, \"params\": {\"c\": c}},\n        {\"w_func\": w_sig, \"params\": {\"c\": c, \"s\": 5.0}},\n        {\"w_func\": w_sig, \"params\": {\"c\": c, \"s\": 50.0}},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        w_func = case[\"w_func\"]\n        w_params = case[\"params\"]\n\n        # Compute covariance matrices\n        K_train = k_composite(X_train, X_train, w_func, w_params)\n        K_cross = k_composite(X_star, X_train, w_func, w_params)\n\n        # Add noise to the training covariance matrix for numerical stability\n        K_y = K_train + sigma_n_sq * np.eye(len(X_train))\n\n        # Solve the linear system K_y * alpha = y_train for alpha\n        # This is more stable than computing the inverse of K_y\n        alpha = np.linalg.solve(K_y, y_train)\n\n        # Compute the posterior mean\n        mu_star = K_cross @ alpha\n        \n        all_results.extend(mu_star)\n\n    # --- 5. FORMAT AND PRINT OUTPUT ---\n    \n    # Format the results to 6 decimal places as required\n    output_str = \",\".join([f\"{r:.6f}\" for r in all_results])\n    print(f\"[{output_str}]\")\n\n\nsolve()\n```", "id": "2441430"}]}