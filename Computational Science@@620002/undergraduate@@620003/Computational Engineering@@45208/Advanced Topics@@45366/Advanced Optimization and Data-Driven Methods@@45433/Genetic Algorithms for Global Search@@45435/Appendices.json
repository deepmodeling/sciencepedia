{"hands_on_practices": [{"introduction": "Our first practice dives into the world of digital typography, a problem that is both visually intuitive and computationally challenging. You will implement a foundational Genetic Algorithm to evolve the pixel representation of a letterform, aiming to maximize its legibility based on a complex fitness function that balances shape matching, pixel connectivity, and other aesthetic criteria. This exercise demonstrates how a standard GA [@problem_id:2396561] can effectively navigate a vast and rugged discrete search space where an exhaustive search would be impossible.", "problem": "You are given a discrete global search problem that formalizes low-resolution letterform legibility as a combinatorial optimization task. For a given integer grid size $N \\in \\mathbb{N}$, define a binary image as a matrix $X \\in \\{0,1\\}^{N \\times N}$, where entry $X_{r,c}=1$ indicates a lit pixel at row $r$ and column $c$, and $X_{r,c}=0$ otherwise. Let $T \\in \\{0,1\\}^{N \\times N}$ be a fixed target binary image of the same shape. For a candidate $X$, define the following terms:\n- Match term $m(X,T)$:\n$$\nm(X,T) \\;=\\; 1 \\;-\\; \\frac{1}{N^2} \\sum_{r=0}^{N-1}\\sum_{c=0}^{N-1} \\left| X_{r,c} - T_{r,c} \\right|.\n$$\n- Connectivity term $c(X)$: Let $S(X)=\\{(r,c)\\,:\\,X_{r,c}=1\\}$ and define the $4$-neighborhood of $(r,c)$ as $\\{(r-1,c),(r+1,c),(r,c-1),(r,c+1)\\}$ when indices are within bounds. Let $h_{r,c}(X)=1$ if there exists a $4$-neighbor $(r',c')$ with $X_{r',c'}=1$, and $h_{r,c}(X)=0$ otherwise. Define\n$$\nc(X) \\;=\\; \\begin{cases}\n\\displaystyle \\frac{1}{|S(X)|}\\sum_{(r,c)\\in S(X)} h_{r,c}(X), & \\text{if } |S(X)|>0,\\\\[6pt]\n0, & \\text{if } |S(X)|=0.\n\\end{cases}\n$$\n- Isolation term $i(X)$: Let the $8$-neighborhood of $(r,c)$ be all $(r',c')$ with $\\max(|r'-r|,|c'-c|)=1$ when indices are within bounds. Let $g_{r,c}(X)=1$ if there exists an $8$-neighbor $(r',c')$ with $X_{r',c'}=1$, and $g_{r,c}(X)=0$ otherwise. Define the fraction of isolated lit pixels\n$$\ni(X) \\;=\\; \\begin{cases}\n\\displaystyle \\frac{1}{|S(X)|}\\sum_{(r,c)\\in S(X)} \\big(1 - g_{r,c}(X)\\big), & \\text{if } |S(X)|>0,\\\\[6pt]\n0, & \\text{if } |S(X)|=0.\n\\end{cases}\n$$\n- Area-deviation term $a(X,T)$:\n$$\na(X,T) \\;=\\; \\frac{\\big|\\;|S(X)| - |S(T)|\\;\\big|}{N^2}.\n$$\nGiven nonnegative weights $w_m,w_c,w_i,w_a \\in \\mathbb{R}_{\\ge 0}$, the objective to be maximized is\n$$\nF(X;T,w_m,w_c,w_i,w_a) \\;=\\; w_m\\, m(X,T) \\;+\\; w_c\\, c(X) \\;-\\; w_i\\, i(X) \\;-\\; w_a\\, a(X,T).\n$$\n\nYour program must, for each specified test case, search the domain $\\{0,1\\}^{N \\times N}$ subject to a strict evaluation budget $B \\in \\mathbb{N}$, and report the maximum objective value found within that budget. An evaluation is the computation of $F(X;T,w_m,w_c,w_i,w_a)$ for a specific $X$. You must round each reported value to $6$ decimal places.\n\nTest suite specification. In all cases, indices are zero-based with rows $r \\in \\{0,1,\\dots,N-1\\}$ and columns $c \\in \\{0,1,\\dots,N-1\\}$. Each target $T$ is defined explicitly as the set of lit pixels $S(T) \\subset \\{0,\\dots,N-1\\} \\times \\{0,\\dots,N-1\\}$.\n\n- Case $1$ (happy path): $N=8$, weights $(w_m,w_c,w_i,w_a)=(0.6,\\,0.3,\\,0.4,\\,0.2)$, budget $B=6000$. Target $T$ encodes a blocky letter resembling an $E$ with a vertical spine and three horizontal bars:\n  1. Vertical spine: all $(r,c)$ with $r \\in \\{1,2,3,4,5,6\\}$ and $c=1$.\n  2. Top bar: all $(r,c)$ with $r=1$ and $c \\in \\{1,2,3,4,5\\}$.\n  3. Middle bar: all $(r,c)$ with $r=3$ and $c \\in \\{1,2,3,4\\}$.\n  4. Bottom bar: all $(r,c)$ with $r=6$ and $c \\in \\{1,2,3,4,5\\}$.\n\n- Case $2$ (boundary size and stronger regularization): $N=5$, weights $(w_m,w_c,w_i,w_a)=(0.5,\\,0.2,\\,0.7,\\,0.6)$, budget $B=2000$. Target $T$ is a minimal $E$:\n  1. Vertical spine: all $(r,c)$ with $r \\in \\{1,2,3\\}$ and $c=1$.\n  2. Top bar: all $(r,c)$ with $r=1$ and $c \\in \\{1,2,3\\}$.\n  3. Middle bar: all $(r,c)$ with $r=2$ and $c \\in \\{1,2\\}$.\n  4. Bottom bar: all $(r,c)$ with $r=3$ and $c \\in \\{1,2,3\\}$.\n\n- Case $3$ (diagonal-like structure and connectivity emphasis): $N=8$, weights $(w_m,w_c,w_i,w_a)=(0.5,\\,0.6,\\,0.3,\\,0.2)$, budget $B=8000$. Target $T$ encodes a blocky letter resembling an $A$ with two vertical legs, a top bar, and a crossbar:\n  1. Left leg: all $(r,c)$ with $r \\in \\{2,3,4,5,6\\}$ and $c=1$.\n  2. Right leg: all $(r,c)$ with $r \\in \\{2,3,4,5,6\\}$ and $c=6$.\n  3. Top bar: all $(r,c)$ with $r=1$ and $c \\in \\{2,3,4,5\\}$.\n  4. Crossbar: all $(r,c)$ with $r=4$ and $c \\in \\{2,3,4,5\\}$.\n\nFinal output format. Your program should produce a single line of output containing the three rounded objective values for Cases $1$, $2$, and $3$, in that order, as a comma-separated list enclosed in square brackets. For example, it must print exactly one line in the form\n[$v_1, v_2, v_3$], where each $v_k$ is a real number rounded to $6$ decimal places with standard rounding.", "solution": "The problem presented is a discrete global search task, which requires maximizing a complex objective function over a high-dimensional binary space. The search space for a binary image of size $N \\times N$ is $\\{0,1\\}^{N \\times N}$, which contains $2^{N^2}$ possible configurations. For the given values of $N=5$ and $N=8$, the search spaces have $2^{25} \\approx 3.3 \\times 10^7$ and $2^{64} \\approx 1.8 \\times 10^{19}$ members, respectively. An exhaustive search is computationally infeasible. The problem imposes a strict evaluation budget, which frames it as a black-box optimization task where the goal is to find the best possible solution within a limited number of function evaluations. This structure necessitates the use of a metaheuristic search algorithm.\n\nA Genetic Algorithm (GA) is a well-suited methodology for this type of problem. GAs are inspired by the process of natural selection and are effective for exploring large, complex, and poorly understood search spaces. The objective function $F(X;T,w_m,w_c,w_i,w_a)$ is a weighted sum of several components measuring similarity, connectivity, and other structural properties, likely resulting in a rugged fitness landscape with many local optima. GAs are robust against being trapped in such local optima due to their population-based approach and stochastic operators like crossover and mutation.\n\nThe implemented solution is a standard Genetic Algorithm tailored to the specifics of this problem.\n\n**1. Representation and Population**\nAn individual in the population, representing a candidate solution, is an $N \\times N$ binary matrix $X$. The population consists of a fixed number of such individuals. For reproducibility, the pseudo-random number generator is seeded with a constant value. The initial population is seeded with the target image $T$ itself, several mutated versions of $T$, and a majority of randomly generated images to ensure a balance between exploiting a known good region and exploring the broader search space.\n\n**2. Fitness Evaluation**\nThe fitness of an individual $X$ is determined by the objective function $F(X;T,w_m,w_c,w_i,w_a)$. The components of this function are calculated as follows:\n- The match term $m(X,T) = 1 - \\frac{1}{N^2} \\sum_{r,c} |X_{r,c} - T_{r,c}|$ is the normalized agreement between the candidate and target images.\n- The area-deviation term $a(X,T) = \\frac{|\\sum X_{r,c} - \\sum T_{r,c}|}{N^2}$ penalizes differences in the number of lit pixels.\n- The connectivity term $c(X)$ and isolation term $i(X)$ require analyzing pixel neighborhoods. These are calculated efficiently using 2D convolution. The number of lit neighbors for each pixel across the entire grid is computed in a single operation using `scipy.signal.convolve2d` with appropriate kernels. For $c(X)$, which uses a $4$-neighborhood, the kernel is $\\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$. For $i(X)$, which is based on an $8$-neighborhood, the kernel is $\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}$. This vectorized approach is significantly more efficient than iterating through pixels. An `Evaluator` class encapsulates fitness calculation and strictly enforces the evaluation budget $B$.\n\n**3. Selection**\nTournament selection is used to choose parents for the next generation. In a $k$-way tournament, $k$ individuals are chosen at random from the current population, and the one with the highest fitness is selected as a parent. This method provides a good balance between selection pressure and population diversity.\n\n**4. Genetic Operators**\n- **Crossover**: Uniform crossover is employed. For each pixel location, a random choice determines which of the two parents contributes its pixel value to the offspring. This operator effectively combines features from both parents.\n- **Mutation**: Bit-flip mutation is applied to offspring after crossover. Each pixel value (bit) has a small, independent probability of being flipped. The mutation rate is set to $1/N^2$, which on average results in one mutation per individual, introducing new genetic material and preventing premature convergence.\n\n**5. Generational Cycle and Elitism**\nThe GA proceeds in generations. In each cycle, a new population is created from the old one. Elitism is implemented, meaning a small number of the best-performing individuals from the current generation are guaranteed to survive, preserving high-quality solutions. The remainder of the new population is filled with offspring generated through selection, crossover, and mutation. The algorithm terminates once the evaluation budget $B$ is expended, and the highest fitness value found during the entire run is reported as the result.\n\nThis systematic, principle-based design ensures a robust search process capable of navigating the complex fitness landscape to find high-quality solutions for the given letterform legibility problem within the specified computational constraints.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\n# Static seed for reproducibility across runs\nRNG = np.random.default_rng(42)\n\nclass FitnessEvaluator:\n    \"\"\"Calculates the fitness of a candidate image and manages the evaluation budget.\"\"\"\n    def __init__(self, target_matrix, weights, budget):\n        self.T = target_matrix\n        self.N = self.T.shape[0]\n        self.w_m, self.w_c, self.w_i, self.w_a = weights\n        \n        self.target_pixel_count = np.sum(self.T)\n        self.N_squared = self.N * self.N\n        \n        self.kernel_4_conn = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=np.uint8)\n        self.kernel_8_conn = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=np.uint8)\n        \n        self.eval_budget = budget\n        self.eval_count = 0\n        self.max_fitness_found = -np.inf\n\n    def calculate_fitness(self, X):\n        \"\"\"Computes the objective function F for a given image X.\"\"\"\n        if self.eval_count >= self.eval_budget:\n            return None\n\n        # Match term m(X, T)\n        match_term = 1.0 - np.sum(np.abs(X - self.T)) / self.N_squared\n        \n        pixel_count = np.sum(X)\n        \n        if pixel_count == 0:\n            conn_term = 0.0\n            iso_term = 0.0\n        else:\n            # Connectivity term c(X) using 4-neighborhood\n            neighbors_4 = convolve2d(X, self.kernel_4_conn, mode='same', boundary='fill', fillvalue=0)\n            connected_pixels = (neighbors_4 > 0) * X\n            conn_term = np.sum(connected_pixels) / pixel_count\n            \n            # Isolation term i(X) using 8-neighborhood\n            neighbors_8 = convolve2d(X, self.kernel_8_conn, mode='same', boundary='fill', fillvalue=0)\n            isolated_pixels = (neighbors_8 == 0) * X\n            iso_term = np.sum(isolated_pixels) / pixel_count\n            \n        # Area-deviation term a(X, T)\n        area_dev_term = np.abs(pixel_count - self.target_pixel_count) / self.N_squared\n        \n        fitness = (self.w_m * match_term + \n                   self.w_c * conn_term - \n                   self.w_i * iso_term - \n                   self.w_a * area_dev_term)\n        \n        self.eval_count += 1\n        if fitness > self.max_fitness_found:\n            self.max_fitness_found = fitness\n            \n        return fitness\n\ndef tournament_selection_idx(fitnesses, k):\n    \"\"\"Selects an individual's index using a tournament.\"\"\"\n    pop_size = len(fitnesses)\n    contender_indices = RNG.choice(pop_size, k, replace=False)\n    \n    best_contender_idx = -1\n    best_fitness = -np.inf\n    \n    for idx in contender_indices:\n        if fitnesses[idx] > best_fitness:\n            best_fitness = fitnesses[idx]\n            best_contender_idx = idx\n            \n    return best_contender_idx\n\ndef uniform_crossover(p1, p2):\n    \"\"\"Performs uniform crossover on two parents.\"\"\"\n    mask = RNG.integers(0, 2, size=p1.shape, dtype=np.uint8)\n    child1 = p1 * mask + p2 * (1 - mask)\n    child2 = p2 * mask + p1 * (1 - mask)\n    return child1, child2\n\ndef mutate(individual, mutation_rate):\n    \"\"\"Applies bit-flip mutation to an individual.\"\"\"\n    mutation_mask = RNG.random(individual.shape) < mutation_rate\n    individual[mutation_mask] = 1 - individual[mutation_mask]\n\ndef run_genetic_algorithm(N, target_pixels, weights, budget):\n    \"\"\"Main GA loop for a single test case.\"\"\"\n    \n    target_matrix = np.zeros((N, N), dtype=np.uint8)\n    for r, c in target_pixels:\n        target_matrix[r, c] = 1\n        \n    evaluator = FitnessEvaluator(target_matrix, weights, budget)\n    \n    # GA Parameters\n    pop_size = 100\n    elite_size = 2\n    tournament_k = 3\n    mutation_rate = 1.0 / (N * N)\n    crossover_prob = 0.9\n\n    # Initialization\n    population = []\n    fitnesses = []\n    \n    # Seed population with target, its mutations, and random individuals\n    initial_seeds = [(target_matrix.copy(), 0), (target_matrix.copy(), int(0.05 * N*N)), (target_matrix.copy(), int(0.1 * N*N))]\n    for ind, num_flips in initial_seeds:\n        if num_flips > 0:\n            indices_to_flip = RNG.choice(N * N, num_flips, replace=False)\n            flat_ind = ind.flatten()\n            flat_ind[indices_to_flip] = 1 - flat_ind[indices_to_flip]\n            ind = flat_ind.reshape((N, N))\n        \n        fit = evaluator.calculate_fitness(ind)\n        if fit is not None:\n            population.append(ind)\n            fitnesses.append(fit)\n\n    while len(population) < pop_size:\n        individual = RNG.integers(0, 2, size=(N, N), dtype=np.uint8)\n        fit = evaluator.calculate_fitness(individual)\n        if fit is None: break\n        population.append(individual)\n        fitnesses.append(fit)\n\n    if not population: return -np.inf\n\n    # Main generational loop\n    while evaluator.eval_count < evaluator.eval_budget:\n        # Sort population by fitness for elitism\n        sorted_indices = np.argsort(fitnesses)\n        new_population = [population[i] for i in sorted_indices[-elite_size:]]\n        new_fitnesses = [fitnesses[i] for i in sorted_indices[-elite_size:]]\n\n        # Generate offspring\n        while len(new_population) < pop_size:\n            parent1_idx = tournament_selection_idx(fitnesses, tournament_k)\n            parent2_idx = tournament_selection_idx(fitnesses, tournament_k)\n            parent1 = population[parent1_idx]\n            parent2 = population[parent2_idx]\n\n            if RNG.random() < crossover_prob:\n                child1, child2 = uniform_crossover(parent1, parent2)\n            else:\n                child1, child2 = parent1.copy(), parent2.copy()\n            \n            mutate(child1, mutation_rate)\n            mutate(child2, mutation_rate)\n\n            fit1 = evaluator.calculate_fitness(child1)\n            if fit1 is None: break\n            new_population.append(child1)\n            new_fitnesses.append(fit1)\n            \n            if len(new_population) < pop_size:\n                fit2 = evaluator.calculate_fitness(child2)\n                if fit2 is None: break\n                new_population.append(child2)\n                new_fitnesses.append(fit2)\n            \n        if evaluator.eval_count >= evaluator.eval_budget: break\n\n        population = new_population\n        fitnesses = new_fitnesses\n\n    return evaluator.max_fitness_found\n\ndef solve():\n    test_cases = [\n        {\n            \"N\": 8, \"weights\": (0.6, 0.3, 0.4, 0.2), \"budget\": 6000,\n            \"target_pixels\": set(\n                [(r, 1) for r in range(1, 7)] + [(1, c) for c in range(1, 6)] +\n                [(3, c) for c in range(1, 5)] + [(6, c) for c in range(1, 6)]\n            )\n        },\n        {\n            \"N\": 5, \"weights\": (0.5, 0.2, 0.7, 0.6), \"budget\": 2000,\n            \"target_pixels\": set(\n                [(r, 1) for r in range(1, 4)] + [(1, c) for c in range(1, 4)] +\n                [(2, c) for c in range(1, 3)] + [(3, c) for c in range(1, 4)]\n            )\n        },\n        {\n            \"N\": 8, \"weights\": (0.5, 0.6, 0.3, 0.2), \"budget\": 8000,\n            \"target_pixels\": set(\n                [(r, 1) for r in range(2, 7)] + [(r, 6) for r in range(2, 7)] +\n                [(1, c) for c in range(2, 6)] + [(4, c) for c in range(2, 6)]\n            )\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        max_val = run_genetic_algorithm(\n            case[\"N\"], case[\"target_pixels\"], case[\"weights\"], case[\"budget\"]\n        )\n        results.append(round(max_val, 6))\n    \n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2396561"}, {"introduction": "Having explored a discrete domain, we now shift our focus to continuous parameter optimization, a common task in engineering design. This practice challenges you to use a real-coded GA to discover the optimal geometric parameters of a metamaterial's unit cell, aiming to minimize its thermal expansion based on a surrogate model. By implementing operators like arithmetic crossover and Gaussian mutation, you will learn how GAs can efficiently search continuous spaces and handle design constraints using penalty functions, a crucial skill for real-world engineering problems [@problem_id:2396545].", "problem": "You are tasked with implementing a real-coded Genetic Algorithm (GA) to perform a global search over a simplified, dimensionless surrogate model of the effective thermal expansion response of a metamaterial unit cell. The goal is to minimize a scalar objective that approximates an effective thermal expansion coefficient. All angles must be handled in radians. The final answers must be dimensionless real numbers.\n\nFundamental base for this task includes: (i) the definition of optimization as the search for an argument that minimizes a scalar function over a domain, (ii) the definition of a population-based randomized search (genetic algorithm) with reproduction via selection, crossover, and mutation, and (iii) penalty methods for constrained optimization.\n\nDecision variables and domain:\n- Let the chromosome be a vector $\\mathbf{x} = (t, s, \\phi)$ where $t$ is a thickness ratio, $s$ is a hinge compliance ratio, and $\\phi$ is a re-entrant angle in radians. These are dimensionless.\n- Nominal variable domains are:\n  - $t \\in [0.05, 0.95]$,\n  - $s \\in [0.10, 0.90]$,\n  - $\\phi \\in [0.10, 1.30]$ radians.\n- A derived relative density proxy is $r_d(t,s) = 0.5\\,t + 0.5\\,(1 - s)$, which must satisfy $r_d \\in [0.20, 0.80]$.\n\nObjective (to be minimized):\nDefine the surrogate objective $f(t,s,\\phi)$ by\n$$\nf(t,s,\\phi) = c_0 + c_1 t + c_2 s + c_3 \\sin(\\phi) + c_4 t s - c_5 s \\sin(\\phi) - c_6 (1 - t) \\sin(\\phi) + \\Pi_{\\text{dens}}(t,s) + \\Pi_{\\text{box}}(t,s,\\phi),\n$$\nwith constants\n$$\nc_0=0.2,\\quad c_1=0.1,\\quad c_2=0.15,\\quad c_3=0.3,\\quad c_4=0.05,\\quad c_5=0.6,\\quad c_6=0.5.\n$$\nPenalty terms enforce constraints,\n$$\n\\Pi_{\\text{dens}}(t,s) = M \\left(\\max\\{0, r_d(t,s) - 0.80\\}\\right)^2 + M \\left(\\max\\{0, 0.20 - r_d(t,s)\\}\\right)^2,\n$$\n$$\n\\Pi_{\\text{box}}(t,s,\\phi) = B\\left(\\max\\{0, t - \\overline{t}\\}\\right)^2 + B\\left(\\max\\{0, \\underline{t} - t\\}\\right)^2 + B\\left(\\max\\{0, s - \\overline{s}\\}\\right)^2 + B\\left(\\max\\{0, \\underline{s} - s\\}\\right)^2 + B\\left(\\max\\{0, \\phi - \\overline{\\phi}\\}\\right)^2 + B\\left(\\max\\{0, \\underline{\\phi} - \\phi\\}\\right)^2,\n$$\nwith $M=10$, $B=100$, and $(\\underline{t},\\overline{t})=(0.05,0.95)$, $(\\underline{s},\\overline{s})=(0.10,0.90)$, $(\\underline{\\phi},\\overline{\\phi})=(0.10,1.30)$ unless a test case specifies different bounds for $\\phi$.\n\nGenetic Algorithm design requirement:\n- Chromosome encoding: real-valued vector $\\mathbf{x}\\in\\mathbb{R}^3$.\n- Initialization: sample uniformly within the box bounds for each decision variable.\n- Fitness: identical to $f(t,s,\\phi)$; minimization objective.\n- Selection: tournament selection with tournament size $k=3$.\n- Crossover: arithmetic crossover, where two parents $\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$ produce two children\n  $\\mathbf{y}^{(1)} = \\omega \\mathbf{x}^{(1)} + (1-\\omega)\\mathbf{x}^{(2)}$, $\\mathbf{y}^{(2)} = \\omega \\mathbf{x}^{(2)} + (1-\\omega)\\mathbf{x}^{(1)}$,\n  with $\\omega \\sim \\text{Uniform}[0,1]$, applied with probability $p_c$.\n- Mutation: independent Gaussian mutation per gene applied with probability $p_m$; for gene $j$,\n  $y_j \\leftarrow y_j + \\mathcal{N}\\!\\left(0, \\sigma^2 (\\overline{b}_j - \\underline{b}_j)^2\\right)$, where $[\\underline{b}_j,\\overline{b}_j]$ are the bounds for gene $j$ and $\\sigma$ is a scalar scale factor. After mutation, clip to the box bounds.\n- Elitism: copy the best $E=\\max\\{1,\\lfloor 0.05\\,N \\rfloor\\}$ individuals unchanged to the next generation, where $N$ is the population size.\n- Termination: fixed number of generations $G$.\n- Randomness: all random number generation must be seeded as specified to ensure deterministic outputs.\n\nAngle unit:\n- All angles $\\phi$ are in radians.\n\nTest suite:\nImplement and run the GA on the following three test cases. Each test case is fully specified by its bounds, algorithmic hyperparameters, and random seed.\n\n- Test case A (general “happy path”):\n  - Bounds: $t \\in [0.05,0.95]$, $s \\in [0.10,0.90]$, $\\phi \\in [0.10,1.30]$ radians.\n  - Population size $N=60$, generations $G=120$, crossover probability $p_c=0.9$, mutation probability per gene $p_m=0.2$, mutation scale $\\sigma=0.05$.\n  - Random seed: $12345$.\n\n- Test case B (boundary in angle; small re-entrant angles):\n  - Bounds: $t \\in [0.05,0.95]$, $s \\in [0.10,0.90]$, $\\phi \\in [0.10,0.25]$ radians.\n  - Population size $N=80$, generations $G=180$, crossover probability $p_c=0.9$, mutation probability per gene $p_m=0.2$, mutation scale $\\sigma=0.04$.\n  - Random seed: $2023$.\n\n- Test case C (edge case with small population but large angles):\n  - Bounds: $t \\in [0.05,0.95]$, $s \\in [0.10,0.90]$, $\\phi \\in [1.10,1.30]$ radians.\n  - Population size $N=25$, generations $G=300$, crossover probability $p_c=0.8$, mutation probability per gene $p_m=0.3$, mutation scale $\\sigma=0.06$.\n  - Random seed: $314159$.\n\nRequired output:\n- For each test case, compute the best (lowest) objective value $f_{\\min}$ found by the GA at termination.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, in the order A, B, C. For example, the exact format must be of the form [$x_1, x_2, x_3$], where $x_1$, $x_2$, and $x_3$ are real numbers rounded to $6$ decimal places.", "solution": "This problem requires implementing a real-coded Genetic Algorithm (GA) to find the minimum value of a surrogate objective function, $f(t,s,\\phi)$, which models the thermal expansion of a metamaterial. The search space is continuous, defined by three dimensionless variables: a thickness ratio $t$, a hinge compliance ratio $s$, and a re-entrant angle $\\phi$. The solution is constructed by implementing the specified GA.\n\nThe fitness of a solution is its objective function value, which the algorithm seeks to minimize. This function is composed of a base surrogate model and penalty terms to enforce constraints:\n$$\nf(t,s,\\phi) = c_0 + c_1 t + c_2 s + c_3 \\sin(\\phi) + c_4 t s - c_5 s \\sin(\\phi) - c_6 (1 - t) \\sin(\\phi) + \\Pi_{\\text{dens}}(t,s) + \\Pi_{\\text{box}}(t,s,\\phi)\n$$\nwith given constants $c_i$. The constraints are handled using an exterior penalty method. A constraint on the relative density proxy, $r_d(t,s)$, and box constraints on the decision variables are enforced by the quadratic penalty terms $\\Pi_{\\text{dens}}$ and $\\Pi_{\\text{box}}$, respectively.\n\nThe Genetic Algorithm is implemented as a generational model with the following components as specified:\n1.  **Chromosome and Population:** Each individual (chromosome) is represented by a real-valued vector $\\mathbf{x} = (t, s, \\phi) \\in \\mathbb{R}^3$. A population consists of $N$ such individuals. The initial population is generated by sampling each variable for each individual uniformly at random from its specified domain.\n2.  **Fitness Evaluation:** The fitness of an individual is simply its objective function value, $f(\\mathbf{x})$. The algorithm seeks to minimize this value.\n3.  **Evolutionary Cycle:** The algorithm proceeds for a fixed number of generations, $G$.\n4.  **Elitism:** To ensure the preservation of the best solutions found so far, a set of elite individuals is carried over directly to the next generation. The number of elites is $E=\\max\\{1,\\lfloor 0.05\\,N \\rfloor\\}$.\n5.  **Reproduction:** The remaining $N-E$ individuals for the new generation are produced through selection, crossover, and mutation.\n    -   **Selection:** Parents are selected from the current population using tournament selection. For each parent needed, a tournament of size $k=3$ is conducted: $3$ individuals are chosen at random from the population, and the one with the best (lowest) fitness is declared the winner and becomes a parent.\n    -   **Crossover:** Selected parents are paired. With a crossover probability $p_c$, a pair of parents $\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$ produces two offspring, $\\mathbf{y}^{(1)}$ and $\\mathbf{y}^{(2)}$, via arithmetic crossover:\n        $$\n        \\mathbf{y}^{(1)} = \\omega \\mathbf{x}^{(1)} + (1-\\omega)\\mathbf{x}^{(2)}\n        $$\n        $$\n        \\mathbf{y}^{(2)} = \\omega \\mathbf{x}^{(2)} + (1-\\omega)\\mathbf{x}^{(1)}\n        $$\n        where $\\omega$ is a random variable drawn from $\\text{Uniform}[0,1]$. If crossover does not occur, the offspring are direct copies of the parents.\n    -   **Mutation:** Each gene (component) of each new offspring is subjected to mutation with probability $p_m$. If a gene $y_j$ is selected for mutation, its value is altered by adding Gaussian noise:\n        $$\n        y_j \\leftarrow y_j + \\mathcal{N}\\!\\left(0, \\sigma^2 (\\overline{b}_j - \\underline{b}_j)^2\\right)\n        $$\n        where $[\\underline{b}_j, \\overline{b}_j]$ is the valid range for gene $j$ and $\\sigma$ is a given mutation scale factor. After mutation, the gene's value is clipped to ensure it remains within its bounds $[\\underline{b}_j, \\overline{b}_j]$.\n\nThis entire procedure is deterministic due to the use of a specified random seed for each test case. The algorithm is executed for the three distinct test cases provided, and the best fitness value found upon termination is reported for each.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and report results for all test cases.\n    \"\"\"\n    # Global constants for the objective function as per the problem statement.\n    CONSTANTS = {\n        'c': [0.2, 0.1, 0.15, 0.3, 0.05, 0.6, 0.5], # c0 through c6\n        'M': 10.0,\n        'B': 100.0\n    }\n\n    def objective_function(x, bounds):\n        \"\"\"\n        Calculates the objective function value for a given individual x.\n        x is a numpy array [t, s, phi].\n        bounds is a dictionary {'t':(min,max), 's':(min,max), 'phi':(min,max)}.\n        \"\"\"\n        t, s, phi = x[0], x[1], x[2]\n        c0, c1, c2, c3, c4, c5, c6 = CONSTANTS['c']\n        M = CONSTANTS['M']\n        B = CONSTANTS['B']\n        \n        t_min, t_max = bounds['t']\n        s_min, s_max = bounds['s']\n        phi_min, phi_max = bounds['phi']\n\n        # Base surrogate objective function F(t,s,phi)\n        f_base = (c0 + c1 * t + c2 * s + c3 * math.sin(phi) + \n                  c4 * t * s - c5 * s * math.sin(phi) - \n                  c6 * (1.0 - t) * math.sin(phi))\n\n        # Derived relative density proxy and its penalty\n        r_d = 0.5 * t + 0.5 * (1.0 - s)\n        pi_dens = M * (max(0.0, r_d - 0.8)**2) + M * (max(0.0, 0.2 - r_d)**2)\n\n        # Box constraint penalty. This is included for formal correctness, though\n        # clipping in the GA ensures individuals are always within bounds.\n        pi_box_t = B * (max(0.0, t - t_max)**2) + B * (max(0.0, t_min - t)**2)\n        pi_box_s = B * (max(0.0, s - s_max)**2) + B * (max(0.0, s_min - s)**2)\n        pi_box_phi = B * (max(0.0, phi - phi_max)**2) + B * (max(0.0, phi_min - phi)**2)\n        pi_box = pi_box_t + pi_box_s + pi_box_phi\n\n        return f_base + pi_dens + pi_box\n\n    class GeneticAlgorithm:\n        \"\"\"\n        Implements the specified real-coded Genetic Algorithm.\n        \"\"\"\n        def __init__(self, bounds, N, G, pc, pm, sigma, k, seed):\n            self.bounds = bounds\n            self.bounds_arr = np.array([bounds['t'], bounds['s'], bounds['phi']])\n            self.N = N\n            self.G = G\n            self.pc = pc\n            self.pm = pm\n            self.sigma = sigma\n            self.k = k\n            self.rng = np.random.default_rng(seed)\n            self.E = max(1, math.floor(0.05 * self.N))\n            self.population = None\n            self.best_fitness_overall = float('inf')\n\n        def _initialize_population(self):\n            pop = np.zeros((self.N, 3))\n            pop[:, 0] = self.rng.uniform(self.bounds['t'][0], self.bounds['t'][1], self.N)\n            pop[:, 1] = self.rng.uniform(self.bounds['s'][0], self.bounds['s'][1], self.N)\n            pop[:, 2] = self.rng.uniform(self.bounds['phi'][0], self.bounds['phi'][1], self.N)\n            return pop\n\n        def _evaluate_population(self, pop):\n            return np.array([objective_function(ind, self.bounds) for ind in pop])\n\n        def _tournament_selection(self, pop, fitnesses):\n            indices = self.rng.choice(self.N, size=self.k, replace=False)\n            tournament_fitnesses = fitnesses[indices]\n            winner_idx_in_tournament = np.argmin(tournament_fitnesses)\n            winner_idx_in_pop = indices[winner_idx_in_tournament]\n            return pop[winner_idx_in_pop]\n\n        def _mutate_individual(self, ind):\n            for j in range(3):\n                if self.rng.random() < self.pm:\n                    gene_range = self.bounds_arr[j, 1] - self.bounds_arr[j, 0]\n                    std_dev = self.sigma * gene_range\n                    noise = self.rng.normal(loc=0.0, scale=std_dev)\n                    ind[j] += noise\n            return np.clip(ind, self.bounds_arr[:, 0], self.bounds_arr[:, 1])\n\n        def run(self):\n            self.population = self._initialize_population()\n\n            for _ in range(self.G):\n                fitnesses = self._evaluate_population(self.population)\n                \n                # Track the best fitness found in any generation\n                min_fitness_current_gen = np.min(fitnesses)\n                if min_fitness_current_gen < self.best_fitness_overall:\n                    self.best_fitness_overall = min_fitness_current_gen\n\n                # Elitism: preserve the best E individuals\n                elite_indices = np.argsort(fitnesses)[:self.E]\n                elites = self.population[elite_indices]\n                \n                new_population = [None] * self.N\n                new_population[:self.E] = elites\n\n                # Reproduction: create N-E new individuals\n                num_offspring = self.N - self.E\n                \n                offspring_list = []\n                for _ in range(num_offspring // 2):\n                    p1 = self._tournament_selection(self.population, fitnesses)\n                    p2 = self._tournament_selection(self.population, fitnesses)\n\n                    if self.rng.random() < self.pc:\n                        omega = self.rng.random()\n                        c1 = omega * p1 + (1 - omega) * p2\n                        c2 = omega * p2 + (1 - omega) * p1\n                    else:\n                        c1, c2 = p1.copy(), p2.copy()\n                    \n                    offspring_list.append(self._mutate_individual(c1))\n                    offspring_list.append(self._mutate_individual(c2))\n\n                if num_offspring % 2 == 1:\n                    p1 = self._tournament_selection(self.population, fitnesses)\n                    c1 = p1.copy()\n                    offspring_list.append(self._mutate_individual(c1))\n\n                new_population[self.E:] = offspring_list\n                self.population = np.array(new_population)\n            \n            # Final evaluation at termination\n            final_fitnesses = self._evaluate_population(self.population)\n            final_min_fitness = np.min(final_fitnesses)\n            if final_min_fitness < self.best_fitness_overall:\n                 self.best_fitness_overall = final_min_fitness\n            \n            return self.best_fitness_overall\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"bounds\": {'t': (0.05, 0.95), 's': (0.10, 0.90), 'phi': (0.10, 1.30)},\n            \"N\": 60, \"G\": 120, \"pc\": 0.9, \"pm\": 0.2, \"sigma\": 0.05, \"k\": 3, \"seed\": 12345\n        },\n        {\n            \"bounds\": {'t': (0.05, 0.95), 's': (0.10, 0.90), 'phi': (0.10, 0.25)},\n            \"N\": 80, \"G\": 180, \"pc\": 0.9, \"pm\": 0.2, \"sigma\": 0.04, \"k\": 3, \"seed\": 2023\n        },\n        {\n            \"bounds\": {'t': (0.05, 0.95), 's': (0.10, 0.90), 'phi': (1.10, 1.30)},\n            \"N\": 25, \"G\": 300, \"pc\": 0.8, \"pm\": 0.3, \"sigma\": 0.06, \"k\": 3, \"seed\": 314159\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        ga = GeneticAlgorithm(\n            bounds=case[\"bounds\"], N=case[\"N\"], G=case[\"G\"],\n            pc=case[\"pc\"], pm=case[\"pm\"], sigma=case[\"sigma\"],\n            k=case[\"k\"], seed=case[\"seed\"]\n        )\n        best_f = ga.run()\n        results.append(best_f)\n\n    # Final print statement in the exact required format.\n    rounded_results = [round(r, 6) for r in results]\n    print(f\"[{','.join(map(str, rounded_results))}]\")\n\nsolve()\n```", "id": "2396545"}, {"introduction": "Our final practice moves beyond standard operators to the art of advanced GA design. Using the classic NP-hard Minimum Vertex Cover problem as a testbed, you are tasked with creating a specialized crossover operator that incorporates knowledge of the problem's underlying graph structure. This exercise [@problem_id:2396605] demonstrates a critical concept: that the power of a GA can be significantly enhanced by designing 'intelligent' operators that preserve important building blocks and guide the search more effectively than generic ones.", "problem": "You are given a finite, undirected, simple graph $G = (V,E)$ with $|V| = n$ and $|E| = m$. A vertex cover is a subset $C \\subseteq V$ such that every edge $(u,v) \\in E$ has $u \\in C$ or $v \\in C$. The minimum vertex cover problem is to find a vertex cover of minimum cardinality. A chromosome is a binary vector $x \\in \\{0,1\\}^n$, where index $i$ corresponds to vertex $i \\in V$ and $x_i = 1$ if and only if vertex $i$ is included in the candidate cover. The size of the cover represented by $x$ is $|x| = \\sum_{i=0}^{n-1} x_i$. Define the uncovered edge count for a chromosome $x$ as $U(x) = |\\{(u,v)\\in E \\mid x_u = 0 \\wedge x_v = 0\\}|$. Define the penalized fitness as $f(x) = |x| + M \\cdot U(x)$, with $M = n + 1$.\n\nDefine critical sub-graphs as follows. A triangle is any $3$-vertex subset $\\{i,j,k\\} \\subseteq V$ such that all three edges $(i,j)$, $(j,k)$, and $(i,k)$ are in $E$. A star centered at $c \\in V$ is the induced sub-graph on $\\{c\\} \\cup N(c)$, where $N(c)$ is the neighborhood of $c$, under the constraint $\\deg(c) \\ge 3$. Construct a disjoint family $\\mathcal{H}$ of critical sub-graphs by the following rules, applied in order: first include all triangles; then, in non-increasing order of degree, include a star centered at $c$ only if its vertex set is disjoint from all vertex sets of sub-graphs already in $\\mathcal{H}$; stop when all centers with degree at least $3$ have been considered. Thus, every $H \\in \\mathcal{H}$ is either a triangle or a star, and the vertex sets of sub-graphs in $\\mathcal{H}$ are pairwise disjoint.\n\nA crossover operator $X$ takes as input the graph $G$, the disjoint critical sub-graphs family $\\mathcal{H}$, and two parent chromosomes $p^{(1)}, p^{(2)} \\in \\{0,1\\}^n$, and produces a child chromosome $c \\in \\{0,1\\}^n$ satisfying all of the following properties:\n\n- Preservation over critical sub-graphs: For each $H \\in \\mathcal{H}$ with vertex set $V(H)$, define the local uncovered count $U_H(p) = |\\{(u,v)\\in E \\mid u \\in V(H), v \\in V(H), p_u = 0, p_v = 0\\}|$, and the local size $|p|_H = \\sum_{i \\in V(H)} p_i$. Define the local cost $g_H(p) = |p|_H + M \\cdot U_H(p)$. The child must satisfy $c_i = p^{(k)}_i$ for all $i \\in V(H)$, where $k \\in \\{1,2\\}$ is chosen to minimize $g_H(p^{(k)})$; in case of a tie, either parent may be chosen with equal probability.\n- Outside critical sub-graphs: For all $j \\in V \\setminus \\bigcup_{H \\in \\mathcal{H}} V(H)$, the child must satisfy $c_j \\in \\{p^{(1)}_j, p^{(2)}_j\\}$, with $c_j = p^{(1)}_j$ and $c_j = p^{(2)}_j$ occurring with equal probability and independently across such positions.\n\nAfter crossover, a deterministic feasibility repair must transform $c$ into a vertex cover, followed by a deterministic pruning step that iteratively removes any redundant selected vertex (sets $x_i$ to $0$) if removal maintains feasibility, until no such removal is possible. The feasibility repair and pruning may only modify the child $c$ and must terminate in finite time.\n\nTask. Implement a complete program that:\n\n- For each test graph listed below, performs a population-based search over $\\{0,1\\}^n$ that applies the above crossover operator $X$ as the sole recombination mechanism and uses the penalized fitness $f(x)$ to guide search. The search must be stochastic but reproducible by fixing a random seed. It must include random initialization and pointwise bit mutation with per-bit mutation probability equal to $1/n$, applied to offspring before repair and pruning.\n- For each test graph, after a fixed computational budget that is the same for all graphs, return the best chromosome found under $f(x)$, together with whether it is a valid vertex cover.\n\nTest suite. Use the following graphs; vertices are labeled from $0$ to $n-1$ and each edge $(u,v)$ is undirected:\n\n- Test $1$ (non-bipartite, contains a triangle): $n = 3$, $E = \\{(0,1),(1,2),(0,2)\\}$.\n- Test $2$ (star, high-degree center): $n = 6$, $E = \\{(0,1),(0,2),(0,3),(0,4),(0,5)\\}$.\n- Test $3$ (cycle with a diagonal, contains a triangle): $n = 4$, $E = \\{(0,1),(1,2),(2,3),(3,0),(0,2)\\}$.\n- Test $4$ (boundary case, empty): $n = 5$, $E = \\emptyset$.\n\nComputational budget and reproducibility. Use the same fixed random seed for all tests. For each test, use the same population size and the same number of generations. Mutation must be applied with per-bit probability $1/n$. The feasibility repair and pruning must be applied to all individuals before evaluating $f(x)$.\n\nFinal output. Your program must produce a single line of output aggregating all test results as a comma-separated list enclosed in square brackets. For each test in the order listed above, output two integers: the best cover size found $|x^\\star|$ and an indicator of feasibility defined as $1$ if $U(x^\\star) = 0$ and $0$ otherwise. The final output format is therefore a single list of length $8$: $[|x^\\star_1|,b_1,|x^\\star_2|,b_2,|x^\\star_3|,b_3,|x^\\star_4|,b_4]$, where $b_i \\in \\{0,1\\}$ for each test $i$.", "solution": "The task is to implement a genetic algorithm with a highly specialized operator for the NP-hard Minimum Vertex Cover problem. The solution faithfully implements the specified algorithm, which enhances a standard GA framework with problem-specific knowledge.\n\n**1. Chromosome and Fitness**\nA solution is represented by a binary vector $x \\in \\{0,1\\}^n$, where $x_i=1$ means vertex $i$ is in the cover. The algorithm's search is guided by the penalized fitness function $f(x) = |x| + M \\cdot U(x)$, where $|x|$ is the cover size, $U(x)$ is the count of uncovered edges, and the penalty $M=n+1$ ensures any valid cover is fitter than any invalid one. However, since all individuals in the main population are maintained as valid covers, fitness evaluation for selection simplifies to just the cover size $|x|$.\n\n**2. Critical Sub-graph Identification**\nBefore the evolution starts, the graph is pre-processed to identify a disjoint family of \"critical sub-graphs\" $\\mathcal{H}$. This is done deterministically:\n- First, all triangles are found. They are sorted lexicographically and added greedily to $\\mathcal{H}$ if they don't overlap with already added sub-graphs.\n- Second, vertices are sorted by degree (descending). Stars of degree $\\ge 3$ are considered and added to $\\mathcal{H}$ if they are disjoint from all sub-graphs already included.\nThis process partitions the graph's vertices into those within critical sub-graphs and those outside.\n\n**3. Evolutionary Process**\nThe algorithm uses a generational model with elitism, ensuring the best solution is never lost.\n- **Initialization:** An initial population of random chromosomes is created. Each is immediately made a valid, minimal cover using the repair and prune operators.\n- **Selection:** Parents are chosen using tournament selection, where the winner is the individual with the smallest cover size.\n- **Crossover:** The specialized crossover operator creates a child from two parents. For each critical sub-graph in $\\mathcal{H}$, it copies the entire corresponding block of genes from the parent who handles that sub-graph more effectively (i.e., has a lower local cost). For all other vertices, it uses standard uniform crossover.\n- **Mutation:** The child chromosome undergoes bit-flip mutation with a per-bit probability of $1/n$.\n- **Repair and Prune:** After crossover and mutation, the resulting chromosome is deterministically transformed back into a valid and locally minimal vertex cover. First, a repair step adds vertices to cover any uncovered edges. Then, a pruning step iteratively removes any redundant vertices as long as the cover remains valid.\n- **Replacement:** The new generation is formed by the elite individual(s) and the newly created, repaired, and pruned offspring.\n\nThis design combines the global search of a GA with problem-specific heuristics encoded in the crossover and repair operators to effectively search for small vertex covers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A complete and runnable Python 3.12 program.\n# This program implements the specified genetic algorithm for the minimum vertex cover problem.\n# It uses only the numpy library (version 1.23.5) as permitted.\n\n# Define GA parameters not specified in the problem statement.\n# These are kept constant across all test cases.\nPOPULATION_SIZE = 50\nGENERATIONS = 50\nTOURNAMENT_SIZE = 3\nRANDOM_SEED = 42\n\nclass VertexCoverGA:\n    \"\"\"\n    Implements the specialized genetic algorithm for the Minimum Vertex Cover problem\n    as described in the problem statement.\n    \"\"\"\n    def __init__(self, n, edges, pop_size, generations, seed):\n        self.n = n\n        # Ensure edges are consistently ordered, e.g., (min, max)\n        self.edges = sorted([tuple(sorted(e)) for e in edges])\n        self.pop_size = pop_size\n        self.generations = generations\n        self.rng = np.random.default_rng(seed)\n        self.M = n + 1\n        \n        self.adj = [set() for _ in range(n)]\n        for u, v in self.edges:\n            self.adj[u].add(v)\n            self.adj[v].add(u)\n        \n        self.H, self.non_h_vertices = self._find_critical_subgraphs()\n\n    def _find_critical_subgraphs(self):\n        \"\"\"\n        Constructs the disjoint family of critical sub-graphs H, following the\n        specified rules: triangles first, then stars.\n        \"\"\"\n        H = []\n        used_vertices = set()\n        \n        # 1. Greedily find all disjoint triangles\n        all_triangles = set()\n        if self.n >= 3:\n            for i in range(self.n):\n                # Sort neighbors to ensure deterministic processing and triangle representation\n                neighbors = sorted(list(self.adj[i]))\n                for j_idx in range(len(neighbors)):\n                    for k_idx in range(j_idx + 1, len(neighbors)):\n                        v, w = neighbors[j_idx], neighbors[k_idx]\n                        if v < i and w < i: # Avoid re-discovering triangles\n                            continue\n                        if w in self.adj[v]:\n                            triangle = tuple(sorted((i, v, w)))\n                            all_triangles.add(triangle)\n\n        # Sort triangles to have a deterministic order for greedy selection\n        sorted_triangles = sorted(list(all_triangles))\n        \n        for t_vertices in sorted_triangles:\n             t_set = set(t_vertices)\n             if not t_set.intersection(used_vertices):\n                 H.append({'type': 'triangle', 'vertices': t_set})\n                 used_vertices.update(t_set)\n\n        # 2. Greedily find disjoint stars\n        degrees = [(i, len(self.adj[i])) for i in range(self.n)]\n        degrees.sort(key=lambda x: (-x[1], x[0])) # Sort by degree desc, then index asc\n\n        for i, deg in degrees:\n            if deg < 3:\n                break\n            star_vertices = {i} | self.adj[i]\n            if not star_vertices.intersection(used_vertices):\n                H.append({'type': 'star', 'vertices': star_vertices})\n                used_vertices.update(star_vertices)\n        \n        non_h_vertices = sorted([v for v in range(self.n) if v not in used_vertices])        \n        return H, non_h_vertices\n\n    def _local_cost(self, p, h_block_vertices):\n        size_h = sum(p[i] for i in h_block_vertices)\n        uncovered_h = 0\n        h_v_list = list(h_block_vertices)\n        for i in range(len(h_v_list)):\n            for j in range(i + 1, len(h_v_list)):\n                u, v = h_v_list[i], h_v_list[j]\n                if v in self.adj[u] and p[u] == 0 and p[v] == 0:\n                    uncovered_h += 1\n        return size_h + self.M * uncovered_h\n\n    def crossover(self, p1, p2):\n        child = np.zeros(self.n, dtype=np.int8)\n        \n        for h_block in self.H:\n            h_vertices = h_block['vertices']\n            cost1 = self._local_cost(p1, h_vertices)\n            cost2 = self._local_cost(p2, h_vertices)\n            \n            winner = p1 if cost1 < cost2 else p2 if cost2 < cost1 else (p1 if self.rng.random() < 0.5 else p2)\n            \n            for v_idx in h_vertices:\n                child[v_idx] = winner[v_idx]\n\n        for j in self.non_h_vertices:\n            child[j] = p1[j] if self.rng.random() < 0.5 else p2[j]\n\n        return child\n\n    def mutate(self, chromosome):\n        if self.n == 0: return chromosome\n        mutation_prob = 1.0 / self.n if self.n > 0 else 0.0\n        if mutation_prob > 0:\n            for i in range(self.n):\n                if self.rng.random() < mutation_prob:\n                    chromosome[i] = 1 - chromosome[i]\n        return chromosome\n\n    def is_cover(self, chromosome):\n        return all(chromosome[u] == 1 or chromosome[v] == 1 for u, v in self.edges)\n\n    def repair(self, chromosome):\n        for u, v in self.edges:\n            if chromosome[u] == 0 and chromosome[v] == 0:\n                chromosome[u] = 1 # Deterministic rule: add vertex with smaller index\n        return chromosome\n\n    def prune(self, chromosome):\n        changed = True\n        node_order = list(range(self.n))\n        while changed:\n            changed = False\n            for i in node_order:\n                if chromosome[i] == 1:\n                    chromosome[i] = 0\n                    if not self.is_cover(chromosome):\n                        chromosome[i] = 1\n                    else:\n                        changed = True\n        return chromosome\n\n    def run(self):\n        population = self.rng.integers(0, 2, size=(self.pop_size, self.n), dtype=np.int8)\n        \n        for i in range(self.pop_size):\n            population[i] = self.repair(population[i])\n            population[i] = self.prune(population[i])\n\n        # Handle case of n=0 graph\n        if self.n == 0:\n            return 0, 1\n\n        best_chromosome = population[0].copy()\n        best_fitness = np.sum(best_chromosome)\n\n        for _ in range(self.generations):\n            fitnesses = np.sum(population, axis=1)\n\n            current_best_idx = np.argmin(fitnesses)\n            if fitnesses[current_best_idx] < best_fitness:\n                best_fitness = fitnesses[current_best_idx]\n                best_chromosome = population[current_best_idx].copy()\n            \n            new_population = np.zeros((self.pop_size, self.n), dtype=np.int8)\n            new_population[0] = best_chromosome.copy() # Elitism\n            \n            for i in range(1, self.pop_size):\n                tourn_indices1 = self.rng.choice(self.pop_size, TOURNAMENT_SIZE, replace=False)\n                p1_idx = tourn_indices1[np.argmin(fitnesses[tourn_indices1])]\n                \n                tourn_indices2 = self.rng.choice(self.pop_size, TOURNAMENT_SIZE, replace=False)\n                p2_idx = tourn_indices2[np.argmin(fitnesses[tourn_indices2])]\n\n                parent1 = population[p1_idx]\n                parent2 = population[p2_idx]\n                \n                child = self.crossover(parent1, parent2)\n                child = self.mutate(child)\n                child = self.repair(child)\n                child = self.prune(child)\n                \n                new_population[i] = child\n            \n            population = new_population\n\n        best_size = np.sum(best_chromosome)\n        is_feasible_flag = 1 if self.is_cover(best_chromosome) else 0\n        \n        return int(best_size), is_feasible_flag\n\ndef solve():\n    \"\"\"\n    Main function to run the GA on the test suite and print the results\n    in the specified format.\n    \"\"\"\n    test_cases = [\n        {'n': 3, 'edges': [(0, 1), (1, 2), (0, 2)]},\n        {'n': 6, 'edges': [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]},\n        {'n': 4, 'edges': [(0, 1), (1, 2), (2, 3), (3, 0), (0, 2)]},\n        {'n': 5, 'edges': []},\n    ]\n\n    results = []\n    for case in test_cases:\n        ga = VertexCoverGA(\n            n=case['n'],\n            edges=case['edges'],\n            pop_size=POPULATION_SIZE,\n            generations=GENERATIONS,\n            seed=RANDOM_SEED\n        )\n        best_size, is_feasible = ga.run()\n        results.extend([best_size, is_feasible])\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2396605"}]}