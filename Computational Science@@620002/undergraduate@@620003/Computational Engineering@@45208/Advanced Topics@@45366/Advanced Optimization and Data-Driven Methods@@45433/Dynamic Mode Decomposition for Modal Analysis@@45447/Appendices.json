{"hands_on_practices": [{"introduction": "Let's begin our hands-on exploration with the foundational principle of Dynamic Mode Decomposition. This first exercise [@problem_id:2387371] challenges you to verify the 'exact' property of DMD, which states that for noiseless data generated by a linear time-invariant system, the algorithm can perfectly reconstruct the underlying dynamics. By implementing the core DMD procedure and testing it on synthetic data, you will build confidence in the method and establish a baseline understanding of how it operates in an ideal setting.", "problem": "You are given the task of assessing whether a data-driven linear model obtained via the exact Dynamic Mode Decomposition (DMD) achieves zero reconstruction error when applied to noiseless snapshots generated by a linear, time-invariant discrete-time system. Consider a state sequence $\\{x_k\\}_{k=0}^{m-1}$ in $\\mathbb{C}^n$ produced by the recursion $x_{k+1} = A x_k$, where $A \\in \\mathbb{C}^{n \\times n}$ is a constant matrix, and define the snapshot matrices $X_1 = [x_0, x_1, \\dots, x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}$ and $X_2 = [x_1, x_2, \\dots, x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}$. Let $\\widehat{A}$ denote the linear operator produced by the exact Dynamic Mode Decomposition from the pair $(X_1, X_2)$ using the full numerical rank of $X_1$. Define the normalized reconstruction error as\n$$\n\\varepsilon = \\frac{\\lVert X_2 - \\widehat{A} X_1 \\rVert_F}{\\lVert X_2 \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. A dataset satisfies the conditions for exact DMD if it is noise-free, generated by a linear time-invariant model as above, and the operator $\\widehat{A}$ is formed using the full numerical rank of $X_1$.\n\nConstruct three synthetic datasets by specifying $A$, $x_0$, and $m$ that meet these conditions. For each dataset, generate the snapshots $\\{x_k\\}$ by $x_{k+1} = A x_k$ and form $X_1$ and $X_2$ as defined above. For each dataset, compute $\\varepsilon$ and return a boolean indicating whether $\\varepsilon \\leq 10^{-12}$.\n\nYour program must implement this for the following test suite. In all cases where complex numbers appear, angles are in radians, and all computations are over $\\mathbb{C}$.\n\nTest Suite (each bullet fully specifies one dataset):\n\n- Dataset 1 (diagonalizable with real eigenvalues):\n  - Dimension $n = 3$, number of snapshots $m = 6$.\n  - Choose $W_1 = \\begin{bmatrix} 1  2  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$ and $\\Lambda_1 = \\mathrm{diag}(0.8,\\, 1.2,\\, -0.5)$, and define $A_1 = W_1 \\Lambda_1 W_1^{-1}$.\n  - Initial state $x_0^{(1)} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$.\n\n- Dataset 2 (includes an oscillatory complex-conjugate pair):\n  - Dimension $n = 3$, number of snapshots $m = 7$.\n  - Choose $W_2 = I_3$ (the $3 \\times 3$ identity), $\\Lambda_2 = \\mathrm{diag}\\!\\left(\\mathrm{e}^{\\mathrm{i}\\pi/6},\\, \\mathrm{e}^{-\\mathrm{i}\\pi/6},\\, 0.9\\right)$, and define $A_2 = \\Lambda_2$.\n  - Initial state $x_0^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}$.\n\n- Dataset 3 (rank-deficient snapshots due to unexcited modes):\n  - Dimension $n = 4$, number of snapshots $m = 5$.\n  - Choose $W_3 = I_4$ (the $4 \\times 4$ identity), $\\Lambda_3 = \\mathrm{diag}(0.7,\\, 0.7,\\, 0.3,\\, 1.1)$, and define $A_3 = \\Lambda_3$.\n  - Initial state $x_0^{(3)} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\nFor each dataset, you must:\n1. Construct $A$, generate $x_k$ via $x_{k+1} = A x_k$ for $k = 0, 1, \\dots, m-2$, and form $X_1$ and $X_2$.\n2. Compute $\\widehat{A}$ by applying the exact Dynamic Mode Decomposition to $(X_1, X_2)$ using the full numerical rank of $X_1$.\n3. Compute $\\varepsilon$ as defined above and compare it to $10^{-12}$ to obtain a boolean result.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one boolean per dataset in the order given above. For example, a valid output looks like \"[True,True,True]\".", "solution": "The goal is to verify that the exact Dynamic Mode Decomposition (DMD) produces zero reconstruction error for noiseless snapshots generated by a linear time-invariant system when the method uses the full numerical rank of the data. The data construction and the error metric are specified as follows. Given $A \\in \\mathbb{C}^{n \\times n}$, an initial state $x_0 \\in \\mathbb{C}^n$, and an integer $m \\ge 2$, define $x_{k+1} = A x_k$ for $k = 0, \\dots, m-2$, and set\n$$\nX_1 = [x_0, x_1, \\dots, x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}, \\quad\nX_2 = [x_1, x_2, \\dots, x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}.\n$$\nBy construction, $X_2 = A X_1$.\n\nThe exact DMD operator $\\widehat{A}$ is the data-driven linear map on the column space of $X_1$ that maps each $x_k$ in $X_1$ to $x_{k+1}$ in $X_2$. To construct $\\widehat{A}$ from data, one uses the singular value decomposition (SVD) of $X_1$. Let $X_1$ have the (thin) SVD\n$$\nX_1 = U_r S_r V_r^*,\n$$\nwhere $U_r \\in \\mathbb{C}^{n \\times r}$, $S_r \\in \\mathbb{R}^{r \\times r}$ is diagonal with strictly positive diagonal entries, $V_r \\in \\mathbb{C}^{(m-1) \\times r}$ has orthonormal columns, and $r = \\mathrm{rank}(X_1)$ is the full numerical rank determined from the singular values. The exact DMD operator is then defined by\n$$\n\\widehat{A} = X_2 V_r S_r^{-1} U_r^*.\n$$\nWe now show that this choice guarantees zero reconstruction error in exact arithmetic for noiseless sequential data.\n\nFirst, observe that\n$$\n\\widehat{A} X_1 = X_2 V_r S_r^{-1} U_r^* \\, (U_r S_r V_r^*) = X_2 V_r V_r^*.\n$$\nThe matrix $V_r V_r^*$ is the orthogonal projector in $\\mathbb{C}^{(m-1) \\times (m-1)}$ onto the row space of $X_1$. For sequential noiseless data, $X_2 = A X_1$, which implies that the row space of $X_2$ is contained in the row space of $X_1$ because left multiplication by $A$ forms linear combinations of the rows of $X_1$. Therefore, projecting $X_2$ onto the row space of $X_1$ leaves it unchanged:\n$$\nX_2 V_r V_r^* = X_2.\n$$\nHence,\n$$\n\\widehat{A} X_1 = X_2,\n$$\nwhich implies the reconstruction error defined by\n$$\n\\varepsilon = \\frac{\\lVert X_2 - \\widehat{A} X_1 \\rVert_F}{\\lVert X_2 \\rVert_F}\n$$\nis exactly zero in exact arithmetic.\n\nIn finite-precision arithmetic, numerical roundoff leads to a small nonzero value. Using a strict tolerance such as $10^{-12}$ captures this and validates the theoretical result.\n\nNext, we justify that each dataset in the test suite satisfies the conditions required:\n\n- Dataset 1: $A_1 = W_1 \\Lambda_1 W_1^{-1}$ with $W_1$ invertible and $\\Lambda_1$ diagonal with distinct real eigenvalues. This ensures $A_1$ is diagonalizable. The snapshots are noiseless and satisfy $X_2 = A_1 X_1$. The numerical rank of $X_1$ equals the dimension of the invariant subspace excited by $x_0^{(1)}$, which here will be full due to the choice of $x_0^{(1)}$ and distinct eigenvalues.\n\n- Dataset 2: $A_2 = \\Lambda_2$ with $\\Lambda_2 = \\mathrm{diag}\\!\\left(\\mathrm{e}^{\\mathrm{i}\\pi/6}, \\mathrm{e}^{-\\mathrm{i}\\pi/6}, 0.9\\right)$. This is diagonal over $\\mathbb{C}$ and thus diagonalizable, with an oscillatory complex-conjugate eigenpair and a real decaying eigenvalue. The snapshots are noiseless and generated by $x_{k+1} = A_2 x_k$, hence $X_2 = A_2 X_1$. The numerical rank is detected from the singular values of $X_1$.\n\n- Dataset 3: $A_3 = \\Lambda_3$ with $\\Lambda_3 = \\mathrm{diag}(0.7, 0.7, 0.3, 1.1)$ and $x_0^{(3)} = [3, -2, 0, 0]^T$. Only the first two eigenmodes are excited, so the snapshots lie in a two-dimensional invariant subspace, making $X_1$ rank-deficient (rank equal to $2$) even though $n = 4$. The exact DMD constructed with the full numerical rank $r = 2$ still satisfies $\\widehat{A} X_1 = X_2$ exactly, because the data remain noiseless and $X_2 = A_3 X_1$ with row space of $X_2$ contained in that of $X_1$.\n\nAlgorithmic plan for the program:\n1. For each dataset, construct $A$ as specified, generate $x_k$ iteratively, and form $X_1$ and $X_2$.\n2. Compute the thin SVD of $X_1$ to obtain $U, S, V^*$, determine the numerical rank $r$ using the threshold $S_i > \\tau$ with $\\tau = \\max(n, m-1) \\cdot \\epsilon \\cdot S_1$, where $\\epsilon$ is machine precision.\n3. Truncate to $U_r, S_r, V_r$ and compute $\\widehat{A} = X_2 V_r S_r^{-1} U_r^*$.\n4. Compute $\\varepsilon$ and compare to $10^{-12}$ to produce the boolean result for each dataset.\n5. Output the list of booleans in the required single-line format.\n\nBecause the datasets satisfy the exact DMD conditions, the reconstruction should be numerically zero within the specified tolerance for all three cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef numerical_rank(S, shape, eps=None):\n    \"\"\"\n    Determine the numerical rank based on singular values S and matrix shape.\n    Uses threshold tau = max(shape) * eps * S[0].\n    \"\"\"\n    if eps is None:\n        eps = np.finfo(float).eps\n    if S.size == 0:\n        return 0\n    tau = max(shape) * eps * S[0]\n    return int(np.sum(S > tau))\n\ndef exact_dmd_operator(X1, X2):\n    \"\"\"\n    Compute the exact DMD operator A_hat = X2 V_r S_r^{-1} U_r^* using the full numerical rank.\n    \"\"\"\n    # Compute SVD of X1\n    U, s, Vh = np.linalg.svd(X1, full_matrices=False)\n    # Determine numerical rank\n    r = numerical_rank(s, X1.shape)\n    if r == 0:\n        # Degenerate case: no information\n        return np.zeros((X2.shape[0], X1.shape[0]), dtype=X1.dtype)\n    U_r = U[:, :r]\n    S_r_inv = np.diag(1.0 / s[:r])\n    V_r = Vh.conj().T[:, :r]\n    # Exact DMD operator\n    A_hat = X2 @ V_r @ S_r_inv @ U_r.conj().T\n    return A_hat\n\ndef generate_snapshots(A, x0, m):\n    \"\"\"\n    Generate snapshots x_0, x_1, ..., x_{m-1} using x_{k+1} = A x_k.\n    Returns X1 = [x0 ... x_{m-2}] and X2 = [x1 ... x_{m-1}].\n    \"\"\"\n    n = A.shape[0]\n    X = np.zeros((n, m), dtype=complex)\n    X[:, 0] = x0\n    for k in range(m - 1):\n        X[:, k + 1] = A @ X[:, k]\n    X1 = X[:, :-1]\n    X2 = X[:, 1:]\n    return X1, X2\n\ndef build_dataset_1():\n    # Dataset 1: A = W * Lambda * W^{-1}, real diagonalizable\n    W = np.array([[1, 2, 0],\n                  [0, 1, 1],\n                  [1, 0, 1]], dtype=float)\n    Lambda = np.diag([0.8, 1.2, -0.5])\n    Winv = np.linalg.inv(W)\n    A = (W @ Lambda @ Winv).astype(complex)\n    x0 = np.array([1, -1, 2], dtype=complex)\n    m = 6\n    return A, x0, m\n\ndef build_dataset_2():\n    # Dataset 2: complex conjugate pair and a real eigenvalue\n    lam1 = np.exp(1j * np.pi / 6.0)\n    lam2 = np.exp(-1j * np.pi / 6.0)\n    lam3 = 0.9 + 0j\n    A = np.diag([lam1, lam2, lam3]).astype(complex)\n    x0 = np.array([2, 1, -1], dtype=complex)\n    m = 7\n    return A, x0, m\n\ndef build_dataset_3():\n    # Dataset 3: rank-deficient snapshots (only first two modes excited)\n    A = np.diag([0.7, 0.7, 0.3, 1.1]).astype(complex)\n    x0 = np.array([3, -2, 0, 0], dtype=complex)\n    m = 5\n    return A, x0, m\n\ndef reconstruction_boolean(X1, X2, tol=1e-12):\n    A_hat = exact_dmd_operator(X1, X2)\n    diff = X2 - (A_hat @ X1)\n    num = np.linalg.norm(diff, ord='fro')\n    den = np.linalg.norm(X2, ord='fro')\n    # Handle the degenerate case where X2 is zero matrix\n    rel_err = 0.0 if den == 0.0 else (num / den)\n    return rel_err = tol\n\ndef solve():\n    # Define the test cases from the problem statement.\n    datasets = [\n        build_dataset_1(),\n        build_dataset_2(),\n        build_dataset_3(),\n    ]\n\n    results = []\n    for A, x0, m in datasets:\n        X1, X2 = generate_snapshots(A, x0, m)\n        results.append(reconstruction_boolean(X1, X2, tol=1e-12))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2387371"}, {"introduction": "In practice, data is often contaminated with noise, and we are typically interested in only the most dominant dynamic modes. The key to handling this is Singular Value Decomposition (SVD) rank truncation, which acts as a powerful filter. This exercise [@problem_id:2387367] delves into the art of selecting the truncation rank $r$, demonstrating how this single parameter critically affects the ability of DMD to identify modes, especially in challenging cases with closely spaced frequencies or modes with low energy.", "problem": "You are asked to design and implement a program that investigates how truncating the rank in a low-rank factorization influences modal identification using Dynamic Mode Decomposition (DMD). Begin from the discrete-time linear time-invariant evolution model and the core linear algebra definition that a best-fit linear operator advancing one snapshot to the next can be inferred from data. Use this foundation to derive a computational procedure that, given a user-specified truncation rank $r$, estimates the dominant modes and their associated discrete-time eigenvalues from snapshot data. Your program must quantify how many true modes are correctly identified for several carefully constructed test cases, including systems with closely spaced frequencies and systems with a low-energy mode. All angles must be handled in radians.\n\nThe task must be framed purely in mathematical and algorithmic terms:\n\n- Consider a sequence of complex-valued snapshots $\\{x_k\\}_{k=0}^{m-1}$ with $x_k \\in \\mathbb{C}^n$ generated by a linear superposition of $p$ modes,\n  $$x_k = \\sum_{j=1}^{p} a_j \\, \\phi_j \\, e^{(\\sigma_j + \\mathrm{i}\\,\\omega_j)\\,k},$$\n  where $\\phi_j \\in \\mathbb{C}^n$ are spatial mode shapes, $a_j \\in \\mathbb{R}_+$ are mode amplitudes (energy proxies), $\\sigma_j \\in \\mathbb{R}$ are discrete-time growth rates per sample, and $\\omega_j \\in \\mathbb{R}$ are angular frequencies in radians per sample. The discrete-time eigenvalue associated with mode $j$ is $\\lambda_j^\\star = e^{\\sigma_j + \\mathrm{i}\\,\\omega_j}$. The snapshots are stacked into data matrices $X = [x_0,\\dots,x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}$ and $Y = [x_1,\\dots,x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}$.\n- Starting from the definition of least-squares fitting of a linear operator $A$ such that $Y \\approx A X$, and the use of Singular Value Decomposition (SVD) to stabilize and enable low-rank approximation, derive a computational procedure to estimate a reduced operator and its eigenvalues, under a user-chosen rank truncation $r$. Do not introduce any other assumptions beyond the linear time-invariant model and standard linear algebra.\n- Implement a matching rule to count correct identifications: A computed eigenvalue $\\mu$ is said to match a ground-truth $\\lambda_j^\\star$ if both of the following hold simultaneously:\n  1) The wrapped absolute angular difference between their arguments is at most a tolerance $\\tau_\\omega$, i.e., the smallest absolute difference between $\\arg(\\mu)$ and $\\omega_j$ modulo $2\\pi$ is $\\le \\tau_\\omega$ (angles in radians).\n  2) The magnitude difference satisfies $\\big|\\,|\\mu| - e^{\\sigma_j}\\,\\big| \\le \\tau_m$.\n  Use $\\tau_\\omega = 10^{-3}$ and $\\tau_m = 10^{-3}$.\n- For reproducibility, when random quantities are used (e.g., spatial modes $\\phi_j$), fix the random number generator seed to $12345$ before constructing the test cases.\n\nTest Suite. Your program must construct the following five test cases, generate the corresponding snapshots, execute the rank-$r$ DMD procedure, and compute the integer number of correctly identified modes for each case. Use $n=6$ sensors and complex-valued snapshots. In all cases, set the additive noise level to zero. Angles are in radians, and time is counted in integer sample steps.\n\n- Case $1$ (baseline, well-separated): $m=300$, $p=3$, $\\omega=[0.40,\\,1.10,\\,2.20]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,0.80,\\,0.50]$, $r=3$.\n- Case $2$ (closely spaced pair, adequate rank): $m=400$, $p=3$, $\\omega=[0.20,\\,0.205,\\,1.00]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,1.00,\\,0.30]$, $r=3$.\n- Case $3$ (closely spaced pair, insufficient rank): $m=400$, $p=3$, $\\omega=[0.20,\\,0.205,\\,1.00]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,1.00,\\,0.30]$, $r=2$.\n- Case $4$ (low-energy, decaying mode): $m=300$, $p=3$, $\\omega=[0.50,\\,1.20,\\,1.80]$, $\\sigma=[0.00,\\,0.00,\\,-0.02]$, $a=[1.00,\\,0.80,\\,0.05]$, $r=3$.\n- Case $5$ (boundary rank): $m=300$, $p=3$, $\\omega=[0.25,\\,0.80,\\,1.60]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,0.80,\\,0.50]$, $r=1$.\n\nFor each case:\n- Construct $p$ random spatial modes $\\phi_j \\in \\mathbb{C}^n$ with unit $2$-norm.\n- Form snapshots $x_k$ for $k=0,\\dots,m-1$ using the above superposition (with no noise).\n- Build $X$ and $Y$ from consecutive snapshots.\n- Apply your rank-$r$ DMD procedure to compute the set of eigenvalues $\\{\\mu_\\ell\\}$.\n- Count the integer number of correctly identified modes by one-to-one matching between $\\{\\mu_\\ell\\}$ and $\\{\\lambda_j^\\star\\}$ under the stated tolerances.\n\nFinal Output Format. Your program should produce a single line of output containing a Python-style list of $5$ integers, where the $i$-th integer is the number of correctly identified modes for Case $i$. The output must be exactly of the form\n\"[v1,v2,v3,v4,v5]\"\nwith no extra spaces or text. All angle computations must be in radians. No physical units are required beyond the instruction that all angles are in radians.", "solution": "The problem as stated is scientifically sound, mathematically well-posed, and contains all necessary information for a unique and verifiable solution. It presents a standard task in the field of computational engineering: the implementation and analysis of the Dynamic Mode Decomposition (DMD) algorithm. The investigation is focused on how rank truncation, a key parameter in DMD, affects the identification of dynamic modes, particularly in challenging scenarios involving closely spaced frequencies, low-energy modes, and severely constrained rank. I shall therefore proceed with the derivation of the required algorithm from first principles, followed by its implementation.\n\nThe fundamental premise of DMD is that the evolution of a system, represented by a sequence of state vectors (snapshots) $\\{x_k\\}_{k=0}^{m-1}$ with $x_k \\in \\mathbb{C}^n$, can be approximated by a linear time-invariant operator $A \\in \\mathbb{C}^{n \\times n}$. This implies the relationship:\n$$\nx_{k+1} \\approx A x_k\n$$\nThis relationship is extended to the entire sequence of snapshots by forming two data matrices, $X = [x_0, x_1, \\dots, x_{m-2}]$ and $Y = [x_1, x_2, \\dots, x_{m-1}]$, both in $\\mathbb{C}^{n \\times (m-1)}$. The system dynamics are then compactly expressed as:\n$$\nY \\approx A X\n$$\nThe operator $A$ that best models the transition from $X$ to $Y$ is found by solving the least-squares problem $\\min_A \\| Y - A X \\|_F$, where $\\| \\cdot \\|_F$ is the Frobenius norm. The solution is given by $A = Y X^+$, where $X^+$ denotes the Moore-Penrose pseudoinverse of $X$.\n\nFor systems with large state dimension $n$, forming and analyzing the $n \\times n$ matrix $A$ is computationally expensive and often intractable. The goal is to determine the spectral properties of $A$ (its eigenvalues and eigenvectors), which characterize the system's dynamic modes, without explicitly forming $A$. This is achieved through a projection-based approach using the Singular Value Decomposition (SVD) of the data matrix $X$.\n\nThe SVD of $X$ is given by $X = U \\Sigma V^H$, where $U \\in \\mathbb{C}^{n \\times q}$ and $V \\in \\mathbb{C}^{(m-1) \\times q}$ are matrices with orthonormal columns, and $\\Sigma \\in \\mathbb{R}^{q \\times q}$ is a diagonal matrix of positive singular values $\\sigma_i$, with $q = \\min(n, m-1)$. The columns of $U$ form a basis that optimally captures the energy in the snapshots. The core idea of reduced-rank DMD is to project the dynamics onto a lower-dimensional subspace of rank $r \\le q$ spanned by the first $r$ columns of $U$. This is achieved by truncating the SVD:\n$$\nX \\approx X_r = U_r \\Sigma_r V_r^H\n$$\nwhere $U_r \\in \\mathbb{C}^{n \\times r}$, $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$, and $V_r \\in \\mathbb{C}^{(m-1) \\times r}$ are the truncated SVD matrices. The pseudoinverse of the rank-$r$ approximated data matrix is $X_r^+ = V_r \\Sigma_r^{-1} U_r^H$. The low-rank approximation of the operator $A$ is then:\n$$\nA \\approx Y X_r^+ = Y V_r \\Sigma_r^{-1} U_r^H\n$$\nThe dynamics are projected onto the subspace spanned by the POD modes (columns of $U_r$). The reduced operator $\\tilde{A} \\in \\mathbb{C}^{r \\times r}$, which represents the action of $A$ in this projected subspace, is defined as $\\tilde{A} = U_r^H A U_r$. The eigenvalues of $\\tilde{A}$ approximate the dominant eigenvalues of $A$. Substituting the expression for $A$ and using the property that $U_r^H U_r = I_r$ (the identity matrix of size $r$), we obtain the final expression for the reduced operator:\n$$\n\\tilde{A} = U_r^H (Y V_r \\Sigma_r^{-1} U_r^H) U_r = U_r^H Y V_r \\Sigma_r^{-1}\n$$\nThe eigenvalues of this small $r \\times r$ matrix $\\tilde{A}$ are the rank-$r$ DMD eigenvalues.\n\nThe computational procedure is thus as follows:\n$1$. Construct the data matrices $X, Y$ from the time-series of snapshots.\n$2$. Compute the SVD of $X = U \\Sigma V^H$.\n$3$. Truncate the SVD components to a specified rank $r$ to obtain $U_r$, $\\Sigma_r$, and $V_r$.\n$4$. Compute the low-rank system operator representation $\\tilde{A} = U_r^H Y V_r \\Sigma_r^{-1}$.\n$5$. Compute the eigenvalues of $\\tilde{A}$, which are the desired DMD eigenvalues $\\{\\mu_\\ell\\}_{\\ell=1}^r$.\n\nFor the given problem, synthetic data is generated from a known superposition of modes, $x_k = \\sum_{j=1}^{p} a_j \\phi_j \\lambda_j^{\\star k}$, where $\\lambda_j^\\star = e^{\\sigma_j + \\mathrm{i}\\omega_j}$ are the true discrete-time eigenvalues. The computed DMD eigenvalues $\\{\\mu_\\ell\\}$ are compared against the set of true eigenvalues $\\{\\lambda_j^\\star\\}$. A match is declared if both the magnitude difference $|\\,|\\mu_\\ell| - |\\lambda_j^\\star|\\,| \\le \\tau_m$ and the wrapped angular difference $\\min(|\\arg(\\mu_\\ell) - \\arg(\\lambda_j^\\star)|, 2\\pi - |\\arg(\\mu_\\ell) - \\arg(\\lambda_j^\\star)|) \\le \\tau_\\omega$ are satisfied, using the specified tolerances $\\tau_m=10^{-3}$ and $\\tau_\\omega=10^{-3}$. A one-to-one matching protocol is used to count the number of correctly identified modes. The following code implements this procedure for the five specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DMD problem for a suite of test cases.\n    Derives DMD eigenvalues for different ranks and physical parameters,\n    and counts the number of correctly identified modes.\n    \"\"\"\n\n    # Global parameters as defined in the problem statement\n    n_sensors = 6\n    tau_omega = 1e-3\n    tau_m = 1e-3\n    rng_seed = 12345\n\n    # Define the five test cases\n    test_cases = [\n        # Case 1 (baseline, well-separated)\n        {'m': 300, 'p': 3, 'omega': np.array([0.40, 1.10, 2.20]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 0.80, 0.50]), 'r': 3},\n        # Case 2 (closely spaced pair, adequate rank)\n        {'m': 400, 'p': 3, 'omega': np.array([0.20, 0.205, 1.00]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 1.00, 0.30]), 'r': 3},\n        # Case 3 (closely spaced pair, insufficient rank)\n        {'m': 400, 'p': 3, 'omega': np.array([0.20, 0.205, 1.00]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 1.00, 0.30]), 'r': 2},\n        # Case 4 (low-energy, decaying mode)\n        {'m': 300, 'p': 3, 'omega': np.array([0.50, 1.20, 1.80]), \n         'sigma': np.array([0.00, 0.00, -0.02]), 'a': np.array([1.00, 0.80, 0.05]), 'r': 3},\n        # Case 5 (boundary rank)\n        {'m': 300, 'p': 3, 'omega': np.array([0.25, 0.80, 1.60]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 0.80, 0.50]), 'r': 1},\n    ]\n\n    # Fix the random number generator seed for reproducibility\n    np.random.seed(rng_seed)\n\n    # Generate spatial modes once, as they are used across all cases\n    # The problem implies new random modes for each case. The seed is reset before each construction.\n    # To be safe and compliant, let's create random modes inside the loop.\n\n    results = []\n    for case_params in test_cases:\n        # Unpack parameters for the current case\n        m = case_params['m']\n        p = case_params['p']\n        omega_true = case_params['omega']\n        sigma_true = case_params['sigma']\n        a_true = case_params['a']\n        r = case_params['r']\n\n        # --- 1. Generate Data ---\n        # Generate p random, orthonormal spatial modes phi_j in C^n\n        # Set seed before each case to ensure case-specific reproducibility\n        np.random.seed(rng_seed)\n        phi_modes = np.zeros((n_sensors, p), dtype=np.complex128)\n        for j in range(p):\n            vec = np.random.randn(n_sensors) + 1j * np.random.randn(n_sensors)\n            phi_modes[:, j] = vec / np.linalg.norm(vec)\n\n        # True discrete-time eigenvalues\n        lambda_true = np.exp(sigma_true + 1j * omega_true)\n        \n        # Time steps\n        k_steps = np.arange(m)\n        \n        # Temporal evolution of each mode (Vandermonde matrix)\n        temporal_dynamics = np.exp(np.outer(k_steps, sigma_true + 1j * omega_true))\n        \n        # Superposition of modes to generate snapshots\n        # snapshots = phi * diag(a) * V.T\n        snapshots = phi_modes @ np.diag(a_true) @ temporal_dynamics.T\n\n        # Create data matrices X and Y\n        X = snapshots[:, :-1]\n        Y = snapshots[:, 1:]\n\n        # --- 2. Apply Rank-r DMD ---\n        # SVD of X\n        U, s, Vh = np.linalg.svd(X, full_matrices=False)\n\n        # Truncate to rank r\n        Ur = U[:, :r]\n        Sr = s[:r]\n        Vr = Vh[:r, :].conj().T\n        \n        # Compute reduced operator A_tilde\n        # A_tilde = Ur^H * Y * Vr * inv(Sr)\n        Sr_inv = np.diag(1.0 / Sr)\n        A_tilde = Ur.conj().T @ Y @ Vr @ Sr_inv\n        \n        # Compute DMD eigenvalues\n        dmd_eigvals = np.linalg.eig(A_tilde)[0]\n        \n        # --- 3. Match Eigenvalues and Count ---\n        true_mags = np.exp(sigma_true)\n        true_angles = omega_true\n\n        computed_mags = np.abs(dmd_eigvals)\n        computed_angles = np.angle(dmd_eigvals)\n\n        count = 0\n        matched_computed_indices = set()\n        \n        # One-to-one matching: for each true mode, find one unique matching computed mode\n        for j in range(p):\n            for i in range(r):\n                if i in matched_computed_indices:\n                    continue\n\n                # Check magnitude condition\n                mag_diff = np.abs(computed_mags[i] - true_mags[j])\n                is_mag_match = mag_diff = tau_m\n\n                # Check angle condition (wrapped difference)\n                angle_diff = computed_angles[i] - true_angles[j]\n                wrapped_angle_diff = np.abs((angle_diff + np.pi) % (2 * np.pi) - np.pi)\n                is_angle_match = wrapped_angle_diff = tau_omega\n                \n                if is_mag_match and is_angle_match:\n                    count += 1\n                    matched_computed_indices.add(i)\n                    break # Move to the next true mode\n\n        results.append(count)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2387367"}, {"introduction": "The standard DMD algorithm assumes that data snapshots are collected at uniform time intervals, a condition often unmet in experimental or operational settings. This advanced practice [@problem_id:2387428] challenges you to overcome this limitation by modifying the DMD framework to handle non-uniform sampling. You will move from estimating a discrete-time operator to fitting a continuous-time system generator, a more fundamental representation of the dynamics, by solving a nonlinear optimization problem.", "problem": "Implement a program that estimates continuous-time modal dynamics from state snapshots collected at non-uniform time intervals by modifying the standard Dynamic Mode Decomposition (DMD) to fit a continuous-time generator. The task must be solved from first principles grounded in the following base: for a Linear Time-Invariant (LTI) system defined by the ordinary differential equation $\\dot{\\mathbf{x}}(t) = A \\mathbf{x}(t)$, the exact flow map over a time increment $\\Delta t$ is given by $\\mathbf{x}(t+\\Delta t) = \\exp(A \\Delta t)\\,\\mathbf{x}(t)$, where $\\exp(\\cdot)$ is the matrix exponential. You may also use the definition of the Singular Value Decomposition (SVD), which states that any data matrix $X \\in \\mathbb{R}^{n \\times m}$ can be factored as $X = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{n \\times n}$ orthogonal, $\\Sigma \\in \\mathbb{R}^{n \\times m}$ diagonal and nonnegative, and $V \\in \\mathbb{R}^{m \\times m}$ orthogonal. Do not assume or use any discrete-time DMD formula that presumes uniform sampling.\n\nYou are given multiple test cases of synthetic snapshot data $\\{\\mathbf{x}_k\\}_{k=0}^{m}$ generated by known continuous-time linear systems with state dimension $n$, initial condition $\\mathbf{x}_0 \\in \\mathbb{R}^n$, and strictly increasing non-uniform time stamps $\\{t_k\\}_{k=0}^{m}$ with time unit seconds ($s$). The goal is to recover the continuous-time eigenvalues of the underlying generator $A$ by estimating a reduced-order generator $G \\in \\mathbb{R}^{r \\times r}$ on a low-dimensional subspace spanned by leading left singular vectors of the snapshot matrix. Specifically, let $X = [\\mathbf{x}_0,\\mathbf{x}_1,\\dots,\\mathbf{x}_m] \\in \\mathbb{R}^{n \\times (m+1)}$ be the snapshot matrix and let $U_r \\in \\mathbb{R}^{n \\times r}$ be the first $r$ columns of $U$ from the SVD of $X$. Define reduced coordinates $\\mathbf{y}_k = U_r^{\\top}\\mathbf{x}_k \\in \\mathbb{R}^{r}$. Use the continuous-time flow relation to pose the following Nonlinear Least Squares (NLS) problem for $G$:\n$$\n\\min_{G \\in \\mathbb{R}^{r \\times r}} \\; \\sum_{k=0}^{m-1} \\left\\lVert \\mathbf{y}_{k+1} - \\exp\\!\\big(G \\,\\Delta t_k\\big)\\,\\mathbf{y}_k \\right\\rVert_2^2, \\quad \\text{where } \\Delta t_k = t_{k+1} - t_k \\;.\n$$\nFrom the optimal $\\widehat{G}$, compute the estimated continuous-time eigenvalues as the spectrum $\\{\\widehat{\\lambda}_i\\}_{i=1}^{r}$ of $\\widehat{G}$. Compare these to the true continuous-time eigenvalues $\\{\\lambda_i\\}_{i=1}^{r}$ of the known system generator $A$ by computing a minimal pairing between sets that minimizes the total absolute difference in the complex plane. Report, for each test case, the maximum absolute eigenvalue discrepancy\n$$\ne_{\\max} = \\max_{i \\in \\{1,\\dots,r\\}} \\left|\\lambda_{\\pi(i)} - \\widehat{\\lambda}_i\\right|\n$$\nover the optimal pairing $\\pi$. Each $e_{\\max}$ must be expressed in reciprocal seconds ($s^{-1}$) and rounded to six decimal places.\n\nYour program must implement the above procedure and produce results for the following test suite. In every case, the time unit is seconds ($s$), and angles in any trigonometric functions are in radians.\n\n- Test case $1$ (two-dimensional oscillatory with decay, non-uniform fine sampling):\n  - State dimension $n = 2$.\n  - True generator\n    $$\n    A = \\begin{bmatrix}\n    -0.1  -5.0 \\\\\n     5.0  -0.1\n    \\end{bmatrix}.\n    $$\n  - Initial condition $\\mathbf{x}_0 = [1.0,\\; 0.0]^{\\top}$.\n  - Number of transitions $m = 60$ (so $61$ snapshots).\n  - Non-uniform increments $\\Delta t_k = 0.05 + 0.02 \\sin^2(0.3 k)$ for $k \\in \\{0,1,\\dots,m-1\\}$.\n  - Reduction rank $r = 2$.\n\n- Test case $2$ (three-dimensional strictly real, varied time scales):\n  - State dimension $n = 3$.\n  - True generator\n    $$\n    A = \\mathrm{diag}(-0.2,\\; -1.0,\\; -3.0).\n    $$\n  - Initial condition $\\mathbf{x}_0 = [1.0,\\; 1.0,\\; 1.0]^{\\top}$.\n  - Number of transitions $m = 80$ (so $81$ snapshots).\n  - Non-uniform increments $\\Delta t_k = 0.02 \\big(1 + 4 \\sin^2(0.17 k)\\big)$ for $k \\in \\{0,1,\\dots,m-1\\}$.\n  - Reduction rank $r = 3$.\n\n- Test case $3$ (two-dimensional nearly repeated real rates, long horizon):\n  - State dimension $n = 2$.\n  - True generator\n    $$\n    A = \\mathrm{diag}(-0.05,\\; -0.051).\n    $$\n  - Initial condition $\\mathbf{x}_0 = [1.0,\\; 0.5]^{\\top}$.\n  - Number of transitions $m = 100$ (so $101$ snapshots).\n  - Non-uniform increments $\\Delta t_k = 0.5 + 0.5 \\sin^2(0.11 k)$ for $k \\in \\{0,1,\\dots,m-1\\}$.\n  - Reduction rank $r = 2$.\n\n- Test case $4$ (two-dimensional mild oscillation with intermittent very small steps):\n  - State dimension $n = 2$.\n  - True generator\n    $$\n    A = \\begin{bmatrix}\n    -0.01  -2.0 \\\\\n     2.0  -0.01\n    \\end{bmatrix}.\n    $$\n  - Initial condition $\\mathbf{x}_0 = [0.3,\\; -0.7]^{\\top}$.\n  - Number of transitions $m = 24$ (so $25$ snapshots).\n  - Non-uniform increments\n    $$\n    \\Delta t_k =\n    \\begin{cases}\n    0.001,  \\text{if } k \\bmod 4 = 0, \\\\\n    0.25 + 0.05 \\sin^2(0.5 k),  \\text{otherwise,}\n    \\end{cases}\n    $$\n    for $k \\in \\{0,1,\\dots,m-1\\}$.\n  - Reduction rank $r = 2$.\n\nImplementation requirements and numerical specifications:\n- Use the matrix exponential $\\exp(\\cdot)$ in the objective exactly as defined above.\n- Use the Singular Value Decomposition (SVD) to construct the reduced basis $U_r$ from the data matrix $X$.\n- Solve the stated NLS problem over the entries of $G \\in \\mathbb{R}^{r \\times r}$ using any standard numerical method for nonlinear least squares initialized sensibly. One acceptable initialization is to form reduced data matrices $Y_- = [\\mathbf{y}_0,\\dots,\\mathbf{y}_{m-1}]$ and $Y_+ = [\\mathbf{y}_1,\\dots,\\mathbf{y}_m]$, compute a discrete map $A_d$ by least squares, and obtain an initial guess via a matrix logarithm scaled by the average time step. Any imaginary numerical roundoff in this initialization should be discarded to enforce a real initial $G$.\n- Compute the final eigenvalues from the optimized $\\widehat{G}$ and compare to the true eigenvalues from $A$ by solving the minimal pairing problem that minimizes the sum of absolute complex differences. Then report the maximal absolute discrepancy $e_{\\max}$ over the paired eigenvalues.\n- Each $e_{\\max}$ must be reported in $s^{-1}$, rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, for example, $\\text{[}e_1,e_2,e_3,e_4\\text{]}$, where each $e_i$ is the rounded maximal absolute eigenvalue discrepancy for test case $i$ in $s^{-1}$.", "solution": "The posed problem is valid. It is scientifically grounded in the theory of linear time-invariant systems, is well-posed as a nonlinear least-squares optimization problem, and is specified with objective, unambiguous mathematical language. The task is to estimate the continuous-time eigenvalues of a system generator $A$ from a sequence of state snapshots $\\{\\mathbf{x}_k\\}_{k=0}^{m}$ collected at non-uniform time instances $\\{t_k\\}_{k=0}^{m}$.\n\nThe fundamental principle is that for a linear system governed by the ordinary differential equation $\\dot{\\mathbf{x}}(t) = A \\mathbf{x}(t)$, the state at time $t_{k+1}$ is related to the state at time $t_k$ by the exact flow map $\\mathbf{x}_{k+1} = \\exp(A \\Delta t_k) \\mathbf{x}_k$, where $\\Delta t_k = t_{k+1} - t_k$ and $\\exp(\\cdot)$ is the matrix exponential. The matrix $A$ is unknown and is the target of our estimation.\n\nThe procedure involves the following steps:\n\n1.  **Data Collection and Representation:** The sequence of $m+1$ state snapshots, each of dimension $n$, is assembled into a single data matrix $X = [\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_m] \\in \\mathbb{R}^{n \\times (m+1)}$.\n\n2.  **Dimensionality Reduction:** High-dimensional state data often exhibits low-rank dynamics. We identify the dominant modes of the system by applying Singular Value Decomposition (SVD) to the snapshot matrix: $X = U \\Sigma V^{\\top}$. The columns of $U \\in \\mathbb{R}^{n \\times n}$ form an orthonormal basis for the state space. We select the first $r$ columns of $U$, denoted $U_r \\in \\mathbb{R}^{n \\times r}$, corresponding to the $r$ largest singular values. This matrix $U_r$ forms a basis for an $r$-dimensional subspace that optimally captures the energy of the signal. The full-state data is then projected onto this subspace to obtain reduced-order coordinates: $\\mathbf{y}_k = U_r^{\\top} \\mathbf{x}_k \\in \\mathbb{R}^{r}$.\n\n3.  **Reduced-Order Model Formulation:** We assume that the dynamics in this reduced coordinate system can be approximated by a similar linear evolution, governed by a reduced-order generator matrix $G \\in \\mathbb{R}^{r \\times r}$. The evolution of the reduced state is thus modeled as $\\mathbf{y}_{k+1} \\approx \\exp(G \\Delta t_k) \\mathbf{y}_k$. The eigenvalues of this matrix $G$ will serve as our estimates for the dominant eigenvalues of the original system generator $A$.\n\n4.  **Nonlinear Least-Squares (NLS) Estimation:** To find the optimal generator $G$, we solve the following NLS optimization problem, which minimizes the squared error between the observed and predicted reduced states over all measured time steps:\n    $$\n    \\widehat{G} = \\arg\\min_{G \\in \\mathbb{R}^{r \\times r}} \\; \\sum_{k=0}^{m-1} \\left\\lVert \\mathbf{y}_{k+1} - \\exp\\!\\big(G \\,\\Delta t_k\\big)\\,\\mathbf{y}_k \\right\\rVert_2^2\n    $$\n    This is a non-convex optimization problem that must be solved numerically. We employ an iterative solver, such as a Levenberg-Marquardt algorithm. The optimization variable is the matrix $G$, which has $r^2$ independent entries.\n\n5.  **Initialization:** The convergence of the NLS solver depends on a good initial guess for $G$. A reasonable starting point, $G_0$, is obtained by first finding an approximate discrete-time propagator $A_d$ that maps $Y_-$ to $Y_+$, where $Y_- = [\\mathbf{y}_0, \\dots, \\mathbf{y}_{m-1}]$ and $Y_+ = [\\mathbf{y}_1, \\dots, \\mathbf{y}_m]$. $A_d$ is found via the normal equations: $A_d = Y_+ Y_-^{\\dagger}$, where $Y_-^{\\dagger}$ is the Moore-Penrose pseudoinverse. This discrete map is then converted to a continuous-time generator by taking the matrix logarithm and scaling by the average time step $\\overline{\\Delta t} = \\frac{1}{m}\\sum_{k=0}^{m-1} \\Delta t_k$. Thus, the initial guess is $G_0 = \\frac{1}{\\overline{\\Delta t}} \\log(A_d)$. Any small imaginary components arising from numerical error are discarded to ensure a real initial guess.\n\n6.  **Eigenvalue Analysis and Comparison:** After the NLS solver converges to an optimal $\\widehat{G}$, we compute its eigenvalues, $\\{\\widehat{\\lambda}_i\\}_{i=1}^{r}$. These are the estimated continuous-time eigenvalues. To evaluate the accuracy of the estimation, we compare this set to the true eigenvalues of the known generator $A$, $\\{\\lambda_i\\}_{i=1}^{r}$. The comparison requires finding an optimal pairing between the two sets of complex numbers. This is a linear assignment problem, which we solve to find a permutation $\\pi$ that minimizes the total absolute difference $\\sum_{i=1}^{r} |\\lambda_{\\pi(i)} - \\widehat{\\lambda}_i|$.\n\n7.  **Error Metric:** With the optimal pairing established, the final performance metric is the maximum absolute eigenvalue discrepancy, which represents the worst-case estimation error among all dominant modes:\n    $$\n    e_{\\max} = \\max_{i \\in \\{1,\\dots,r\\}} \\left|\\lambda_{\\pi(i)} - \\widehat{\\lambda}_i\\right|\n    $$\n    This procedure is implemented and executed for each of the four specified test cases to compute the corresponding $e_{\\max}$ values.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm, logm, eigvals\nfrom scipy.optimize import least_squares, linear_sum_assignment\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def run_case(A, x0, m, dt_func, r):\n        \"\"\"\n        Processes a single test case for continuous-time DMD.\n\n        Args:\n            A (np.ndarray): The true n x n generator matrix.\n            x0 (np.ndarray): The initial state vector of size n.\n            m (int): The number of transitions (m+1 snapshots).\n            dt_func (callable): A function lambda k: ... that returns Delta_t_k.\n            r (int): The rank for truncation.\n\n        Returns:\n            float: The maximum absolute eigenvalue discrepancy e_max.\n        \"\"\"\n        n = A.shape[0]\n\n        # 1. Generate snapshot data\n        delta_ts = np.array([dt_func(k) for k in range(m)])\n        X = np.zeros((n, m + 1))\n        X[:, 0] = x0\n        for k in range(m):\n            X[:, k + 1] = expm(A * delta_ts[k]) @ X[:, k]\n\n        # 2. Dimensionality reduction via SVD\n        U, s, Vt = np.linalg.svd(X, full_matrices=False)\n        Ur = U[:, :r]\n\n        # 3. Project data onto reduced basis\n        Y = Ur.T @ X  # Y is r x (m+1)\n\n        # 4. Nonlinear Least Squares (NLS) Estimation\n        # Define the residual function for the optimizer\n        def residuals(g_flat, r_dim, y_data, dt_data, num_transitions):\n            G = g_flat.reshape((r_dim, r_dim))\n            res_vectors = []\n            for k in range(num_transitions):\n                yk = y_data[:, k]\n                yk1 = y_data[:, k + 1]\n                \n                # Propagate yk forward in time\n                yk1_pred = expm(G * dt_data[k]) @ yk\n                \n                res_vectors.append(yk1 - yk1_pred)\n            \n            return np.concatenate(res_vectors)\n\n        # 5. Initialization for G\n        Y_minus = Y[:, :-1]\n        Y_plus = Y[:, 1:]\n        \n        A_d = Y_plus @ np.linalg.pinv(Y_minus)\n        \n        avg_dt = np.mean(delta_ts)\n        G0 = np.real(logm(A_d) / avg_dt)\n        g0_flat = G0.flatten()\n\n        # Solve the NLS problem\n        opt_result = least_squares(\n            residuals,\n            g0_flat,\n            args=(r, Y, delta_ts, m),\n            method='lm'\n        )\n        G_hat = opt_result.x.reshape((r, r))\n        \n        # 6. Eigenvalue Analysis and Comparison\n        true_eigs = eigvals(A)\n        estimated_eigs = eigvals(G_hat)\n\n        # Sort eigenvalues for consistent pairing results (optional but good practice)\n        true_eigs = np.sort_complex(true_eigs)\n        estimated_eigs = np.sort_complex(estimated_eigs)\n\n        # Create a cost matrix for the assignment problem\n        cost_matrix = np.abs(np.subtract.outer(true_eigs, estimated_eigs))\n        \n        # Solve the linear assignment problem to find the optimal pairing\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # 7. Error Metric\n        # The discrepancies for the optimal pairs\n        discrepancies = cost_matrix[row_ind, col_ind]\n        e_max = np.max(discrepancies)\n\n        return e_max\n\n    # Test Case 1\n    A1 = np.array([[-0.1, -5.0], [5.0, -0.1]])\n    x0_1 = np.array([1.0, 0.0])\n    m1 = 60\n    dt_func1 = lambda k: 0.05 + 0.02 * math.sin(0.3 * k)**2\n    r1 = 2\n    e1 = run_case(A1, x0_1, m1, dt_func1, r1)\n\n    # Test Case 2\n    A2 = np.diag([-0.2, -1.0, -3.0])\n    x0_2 = np.array([1.0, 1.0, 1.0])\n    m2 = 80\n    dt_func2 = lambda k: 0.02 * (1 + 4 * math.sin(0.17 * k)**2)\n    r2 = 3\n    e2 = run_case(A2, x0_2, m2, dt_func2, r2)\n    \n    # Test Case 3\n    A3 = np.diag([-0.05, -0.051])\n    x0_3 = np.array([1.0, 0.5])\n    m3 = 100\n    dt_func3 = lambda k: 0.5 + 0.5 * math.sin(0.11 * k)**2\n    r3 = 2\n    e3 = run_case(A3, x0_3, m3, dt_func3, r3)\n\n    # Test Case 4\n    A4 = np.array([[-0.01, -2.0], [2.0, -0.01]])\n    x0_4 = np.array([0.3, -0.7])\n    m4 = 24\n    def dt_func4(k):\n        if k % 4 == 0:\n            return 0.001\n        else:\n            return 0.25 + 0.05 * math.sin(0.5 * k)**2\n    r4 = 2\n    e4 = run_case(A4, x0_4, m4, dt_func4, r4)\n\n    results = [e1, e2, e3, e4]\n    \n    # Format the final output string\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n\n```", "id": "2387428"}]}