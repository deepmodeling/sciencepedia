{"hands_on_practices": [{"introduction": "This first exercise serves as a clear and accessible entry point into the world of the Karush-Kuhn-Tucker (KKT) conditions. By considering a simple one-dimensional quadratic problem, we can isolate and examine each of the four core KKT components: primal feasibility, dual feasibility, stationarity, and complementary slackness. Working through this foundational example [@problem_id:2167418] will build your intuition for how these conditions interact to define the optimal solution of a constrained problem.", "problem": "In a simplified machine learning scenario, the cost associated with a model parameter, $x$, is described by the quadratic function $f(x) = \\frac{1}{2}x^2$. To prevent underfitting and ensure the model has a minimum level of complexity, a domain expert imposes a constraint that the parameter $x$ must be at least 2. This leads to a constrained optimization problem.\n\nThe Lagrangian function for this problem is defined as $L(x, \\lambda) = f(x) + \\lambda g(x)$, where $g(x) \\le 0$ represents the inequality constraint and $\\lambda \\ge 0$ is the Lagrange multiplier.\n\nYour task is to find the saddle point $(x^*, \\lambda^*)$ of the Lagrangian. This point corresponds to the optimal parameter value $x^*$ that minimizes the cost function subject to the constraint, and the associated optimal Lagrange multiplier $\\lambda^*$.\n\nReport the saddle point as an ordered pair $(x^*, \\lambda^*)$.", "solution": "We are given the convex quadratic objective $f(x) = \\frac{1}{2}x^{2}$ and the inequality constraint $x \\geq 2$. To express this in the standard form $g(x) \\leq 0$, define $g(x) = 2 - x$, so the constraint is $g(x) \\leq 0$. The Lagrangian is\n$$\nL(x, \\lambda) = \\frac{1}{2}x^{2} + \\lambda(2 - x),\n$$\nwith dual feasibility $\\lambda \\geq 0$.\n\nFor a convex problem with affine constraint, the Karush–Kuhn–Tucker conditions are necessary and sufficient for optimality and for characterizing a saddle point. The KKT conditions are:\n- Primal feasibility: $g(x^*) \\leq 0$, i.e., $x^* \\geq 2$.\n- Dual feasibility: $\\lambda^* \\geq 0$.\n- Stationarity: $\\frac{\\partial L}{\\partial x}(x^*, \\lambda^*) = 0$.\n- Complementary slackness: $\\lambda^* g(x^*) = 0$.\n\nCompute the stationarity condition:\n$$\n\\frac{\\partial L}{\\partial x} = x - \\lambda \\quad \\Rightarrow \\quad x^* - \\lambda^* = 0 \\quad \\Rightarrow \\quad x^* = \\lambda^*.\n$$\nApply complementary slackness:\n$$\n\\lambda^*(2 - x^*) = 0.\n$$\nConsider the cases:\n- If $\\lambda^* = 0$, then from stationarity $x^* = 0$, which violates primal feasibility $x^* \\geq 2$; reject this case.\n- Therefore $2 - x^* = 0 \\Rightarrow x^* = 2$. Then by stationarity $\\lambda^* = x^* = 2$, which satisfies $\\lambda^* \\geq 0$ and $x^* \\geq 2$.\n\nThus, the unique KKT point is $(x^*, \\lambda^*) = (2, 2)$. Because the problem is convex with an affine constraint, this KKT point is the saddle point of the Lagrangian. Equivalently, verifying the saddle property: $L(x, 2)$ is minimized at $x = 2$, and $L(2, \\lambda) = 2$ for all $\\lambda \\geq 0$, with the dual maximizer at $\\lambda^* = 2$.", "answer": "$$\\boxed{\\begin{pmatrix} 2 & 2 \\end{pmatrix}}$$", "id": "2167418"}, {"introduction": "Building on the foundational concepts, we now advance to a multi-dimensional scenario common in machine learning and data science: finding the projection of a point onto a feasible set. This problem [@problem_id:2407313] requires a systematic, component-wise application of the KKT conditions, showcasing their power in handling multiple constraints simultaneously. It provides an excellent opportunity to master the logic of complementary slackness, which acts as a switch to determine which constraints are active at the optimal solution.", "problem": "Let $p \\in \\mathbb{R}^{3}$ be the point $p = (-1,-2,3)$. Consider the problem of finding the point $x \\in \\mathbb{R}^{3}$ in the first orthant that is closest to $p$ in the Euclidean sense. Formulate this as the constrained optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\; \\frac{1}{2}\\|x - p\\|_{2}^{2} \\quad \\text{subject to} \\quad x_{i} \\ge 0 \\text{ for } i=1,2,3.\n$$\nDetermine the Karush–Kuhn–Tucker (KKT) point $(x^*,\\lambda^*)$, where $\\lambda^* \\in \\mathbb{R}^{3}$ are the Lagrange multipliers associated with the inequality constraints. Provide your final answer as a single row vector $(x_{1}^*,x_{2}^*,x_{3}^*,\\lambda_{1}^*,\\lambda_{2}^*,\\lambda_{3}^*)$. No rounding is required.", "solution": "We are asked to minimize the squared Euclidean distance from $x$ to $p$ subject to the first-orthant constraints. The optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\; f(x) = \\frac{1}{2}\\|x - p\\|_{2}^{2} \\quad \\text{subject to} \\quad x_{i} \\ge 0 \\text{ for } i=1,2,3,\n$$\nwith $p = (-1,-2,3)$. We express the inequality constraints in the standard form $g_{i}(x) \\le 0$ by defining\n$$\ng_{i}(x) = -x_{i} \\le 0, \\quad i=1,2,3.\n$$\nThe Lagrangian $L:\\mathbb{R}^{3} \\times \\mathbb{R}^{3} \\to \\mathbb{R}$ is\n$$\nL(x,\\lambda) = \\frac{1}{2}\\|x - p\\|_{2}^{2} + \\sum_{i=1}^{3} \\lambda_{i} g_{i}(x) = \\frac{1}{2}\\|x - p\\|_{2}^{2} - \\lambda^{\\top} x,\n$$\nwhere $\\lambda \\in \\mathbb{R}^{3}$ are the Lagrange multipliers. The Karush–Kuhn–Tucker (KKT) conditions consist of:\n1. Stationarity:\n$$\n\\nabla_{x} L(x,\\lambda) = x - p - \\lambda = 0 \\;\\; \\Longrightarrow \\;\\; \\lambda = x - p.\n$$\n2. Primal feasibility:\n$$\nx_{i} \\ge 0, \\quad i=1,2,3.\n$$\n3. Dual feasibility:\n$$\n\\lambda_{i} \\ge 0, \\quad i=1,2,3.\n$$\n4. Complementary slackness:\n$$\n\\lambda_{i} \\, x_{i} = 0, \\quad i=1,2,3.\n$$\nSubstitute $p = (-1,-2,3)$ into the stationarity relation $\\lambda = x - p$ to obtain componentwise\n$$\n\\lambda_{1} = x_{1} + 1, \\quad \\lambda_{2} = x_{2} + 2, \\quad \\lambda_{3} = x_{3} - 3.\n$$\nWe now enforce complementary slackness and feasibility for each coordinate.\n\nFor $i=1$:\n- If $x_{1} > 0$, then $\\lambda_{1} = 0$, which implies $x_{1} + 1 = 0 \\Rightarrow x_{1} = -1$, contradicting $x_{1} > 0$.\n- Therefore, $x_{1} = 0$, and then $\\lambda_{1} = 0 + 1 = 1 \\ge 0$.\n\nFor $i=2$:\n- If $x_{2} > 0$, then $\\lambda_{2} = 0$, which implies $x_{2} + 2 = 0 \\Rightarrow x_{2} = -2$, contradicting $x_{2} > 0$.\n- Therefore, $x_{2} = 0$, and then $\\lambda_{2} = 0 + 2 = 2 \\ge 0$.\n\nFor $i=3$:\n- If $x_{3} > 0$, then $\\lambda_{3} = 0$, which implies $x_{3} - 3 = 0 \\Rightarrow x_{3} = 3$, consistent with $x_{3} > 0$.\n- This choice satisfies complementary slackness since $\\lambda_{3} x_{3} = 0 \\cdot 3 = 0$ and dual feasibility since $\\lambda_{3} = 0 \\ge 0$.\n\nCollecting the components, the unique KKT point is\n$$\nx^* = (0,0,3), \\quad \\lambda^* = (1,2,0).\n$$\nBecause the objective function is strictly convex and the feasible set is convex, these KKT conditions are sufficient for global optimality, confirming that $(x^*,\\lambda^*)$ is the KKT point for the problem.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & 0 & 3 & 1 & 2 & 0 \\end{pmatrix}}$$", "id": "2407313"}, {"introduction": "The final practice bridges the gap between theoretical understanding and practical computational skill. In this exercise [@problem_id:2407334], you are tasked with creating a programmatic function to verify whether a candidate point satisfies the KKT conditions for a given nonlinear optimization problem. This is a crucial skill for any computational engineer, as it demands a precise, algorithmic interpretation of each condition and introduces the practical necessity of numerical tolerances, such as $\\epsilon$, when implementing optimization solvers.", "problem": "You are given a concrete nonlinear optimization problem and asked to implement a programmatic verifier for the Karush–Kuhn–Tucker (KKT) optimality conditions. Consider the following minimization problem in computational engineering form: minimize the objective function $f(x) = (x_1 - 1)^2 + (x_2 - 2)^2$ subject to the inequality constraints $g_1(x) = x_1 + x_2 - 2 \\le 0$, $g_2(x) = -x_1 \\le 0$, $g_3(x) = -x_2 \\le 0$, and the equality constraint $h_1(x) = x_1 - x_2 = 0$, where $x \\in \\mathbb{R}^2$. Let $\\lambda \\in \\mathbb{R}^3$ denote the vector of Lagrange multipliers for the inequality constraints and $\\mu \\in \\mathbb{R}$ denote the multiplier for the equality constraint. The KKT verification must include the four canonical components: primal feasibility, dual feasibility, complementary slackness, and stationarity. Use the Euclidean vector norm for stationarity residuals and enforce all checks up to a prescribed tolerance $\\epsilon > 0$.\n\nYour task is to write a complete program that implements a function named `check_kkt(x, lam, mu, eps)` which returns a boolean indicating whether a given triple ($x$, $\\lambda$, $\\mu$) satisfies the KKT conditions for the specified problem within the tolerance $\\epsilon$. The function must perform the following checks:\n- Primal feasibility: all inequality constraints satisfy $g_i(x) \\le \\epsilon$ and equality constraints satisfy $|h_j(x)| \\le \\epsilon$.\n- Dual feasibility: all multipliers for inequality constraints satisfy $\\lambda_i \\ge -\\epsilon$.\n- Complementary slackness: all products satisfy $|\\lambda_i \\, g_i(x)| \\le \\epsilon$.\n- Stationarity: the Euclidean norm of the Lagrangian gradient satisfies $\\|\\nabla f(x) + \\sum_{i=1}^{3} \\lambda_i \\nabla g_i(x) + \\mu \\nabla h_1(x)\\|_2 \\le \\epsilon$.\n\nThe gradients for this problem are:\n- $\\nabla f(x) = \\big(2(x_1 - 1), \\, 2(x_2 - 2)\\big)$,\n- $\\nabla g_1(x) = (1, \\, 1)$, $\\nabla g_2(x) = (-1, \\, 0)$, $\\nabla g_3(x) = (0, \\, -1)$,\n- $\\nabla h_1(x) = (1, \\, -1)$.\n\nYour program must evaluate the function `check_kkt` on the following test suite of inputs and aggregate the boolean results. For each test case, $x$ is given as a length-2 real array, $\\lambda$ is a length-3 real array, $\\mu$ is a real scalar, and $\\epsilon$ is a positive real scalar:\n- Test case 1 (canonical KKT point): $x = [1, 1]$, $\\lambda = [1, 0, 0]$, $\\mu = -1$, $\\epsilon = 10^{-8}$. Expected: True.\n- Test case 2 (stationarity violated): $x = [1, 1]$, $\\lambda = [0.5, 0, 0]$, $\\mu = -0.5$, $\\epsilon = 10^{-8}$. Expected: False.\n- Test case 3 (primal infeasible): $x = [1.2, 1.2]$, $\\lambda = [1, 0, 0]$, $\\mu = -1$, $\\epsilon = 10^{-8}$. Expected: False.\n- Test case 4 (near-KKT within tolerance): $x = [1, 1 + 10^{-7}]$, $\\lambda = [1, -10^{-7}, 0]$, $\\mu = -1 - 10^{-7}$, $\\epsilon = 10^{-6}$. Expected: True.\n- Test case 5 (complementary slackness violated): $x = [1, 1]$, $\\lambda = [1, 0.1, 0]$, $\\mu = -1$, $\\epsilon = 10^{-8}$. Expected: False.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False,True]\"). No additional text must be printed. No physical units are involved. Angles are not involved. Percentages are not involved. The only accepted output types are boolean values aggregated as specified.", "solution": "The supplied problem is subjected to rigorous validation and is deemed valid. It is a well-posed, scientifically grounded problem from the field of computational engineering, specifically concerning the verification of Karush-Kuhn-Tucker (KKT) optimality conditions for a nonlinear optimization problem. All functions, variables, and conditions are explicitly defined, and the task is to implement a numerical verifier.\n\nThe problem is to minimize an objective function $f(x)$ subject to a set of inequality and equality constraints. This is a standard nonlinear programming (NLP) problem of the form:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & f(x) \\\\\n\\text{subject to} \\quad & g_i(x) \\le 0, \\quad i = 1, \\dots, m \\\\\n& h_j(x) = 0, \\quad j = 1, \\dots, p\n\\end{aligned}\n$$\nFor the specific case provided:\n-   Objective function: $f(x) = (x_1 - 1)^2 + (x_2 - 2)^2$ where $x = (x_1, x_2) \\in \\mathbb{R}^2$.\n-   Inequality constraints ($m=3$):\n    -   $g_1(x) = x_1 + x_2 - 2 \\le 0$\n    -   $g_2(x) = -x_1 \\le 0$\n    -   $g_3(x) = -x_2 \\le 0$\n-   Equality constraint ($p=1$):\n    -   $h_1(x) = x_1 - x_2 = 0$\n\nThe Karush-Kuhn-Tucker conditions are first-order necessary conditions for a solution in NLP to be optimal, provided some regularity conditions hold. To state these conditions, we first introduce the Lagrangian function, $L(x, \\lambda, \\mu)$, associated with the problem:\n$$\nL(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^{m} \\lambda_i g_i(x) + \\sum_{j=1}^{p} \\mu_j h_j(x)\n$$\nwhere $\\lambda = (\\lambda_1, \\dots, \\lambda_m)$ are the Lagrange multipliers for the inequality constraints (also known as dual variables) and $\\mu = (\\mu_1, \\dots, \\mu_p)$ are the multipliers for the equality constraints.\n\nFor the given problem, the Lagrangian is:\n$$\nL(x, \\lambda, \\mu) = (x_1 - 1)^2 + (x_2 - 2)^2 + \\lambda_1(x_1 + x_2 - 2) + \\lambda_2(-x_1) + \\lambda_3(-x_2) + \\mu_1(x_1 - x_2)\n$$\nSince there is only one equality constraint, we denote its multiplier by $\\mu$. The KKT conditions for a point $(x^*, \\lambda^*, \\mu^*)$ to be a candidate for optimum are:\n\n1.  **Primal Feasibility**: The point $x^*$ must satisfy all original constraints.\n    -   $g_i(x^*) \\le 0$ for $i \\in \\{1, 2, 3\\}$\n    -   $h_1(x^*) = 0$\n2.  **Dual Feasibility**: The Lagrange multipliers for the inequality constraints must be non-negative.\n    -   $\\lambda_i^* \\ge 0$ for $i \\in \\{1, 2, 3\\}$\n3.  **Complementary Slackness**: For each inequality constraint, either the constraint is active (i.e., holds with equality) or its corresponding multiplier is zero.\n    -   $\\lambda_i^* g_i(x^*) = 0$ for $i \\in \\{1, 2, 3\\}$\n4.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $x$ must be zero at the point $(x^*, \\lambda^*, \\mu^*)$.\n    -   $\\nabla_x L(x^*, \\lambda^*, \\mu^*) = 0$\n\nFor numerical verification, these exact conditions must be relaxed using a small, positive tolerance $\\epsilon$. The programmatic checks are structured as follows:\n\n-   **Primal Feasibility Check**:\n    -   $g_i(x) \\le \\epsilon$ for $i \\in \\{1, 2, 3\\}$\n    -   $|h_1(x)| \\le \\epsilon$\n-   **Dual Feasibility Check**:\n    -   $\\lambda_i \\ge -\\epsilon$ for $i \\in \\{1, 2, 3\\}$. This allows for small negative values that are numerically zero.\n-   **Complementary Slackness Check**:\n    -   $|\\lambda_i g_i(x)| \\le \\epsilon$ for $i \\in \\{1, 2, 3\\}$.\n-   **Stationarity Check**:\n    The gradient of the Lagrangian with respect to $x$ is:\n    $$\n    \\nabla_x L(x, \\lambda, \\mu) = \\nabla f(x) + \\sum_{i=1}^{3} \\lambda_i \\nabla g_i(x) + \\mu \\nabla h_1(x)\n    $$\n    Substituting the given gradients:\n    -   $\\nabla f(x) = (2(x_1 - 1), 2(x_2 - 2))$\n    -   $\\nabla g_1(x) = (1, 1)$, $\\nabla g_2(x) = (-1, 0)$, $\\nabla g_3(x) = (0, -1)$\n    -   $\\nabla h_1(x) = (1, -1)$\n    The components of the Lagrangian gradient are:\n    $$\n    \\frac{\\partial L}{\\partial x_1} = 2(x_1 - 1) + \\lambda_1(1) + \\lambda_2(-1) + \\lambda_3(0) + \\mu(1) = 2(x_1 - 1) + \\lambda_1 - \\lambda_2 + \\mu\n    $$\n    $$\n    \\frac{\\partial L}{\\partial x_2} = 2(x_2 - 2) + \\lambda_1(1) + \\lambda_2(0) + \\lambda_3(-1) + \\mu(-1) = 2(x_2 - 2) + \\lambda_1 - \\lambda_3 - \\mu\n    $$\n    The stationarity condition is then that the Euclidean norm of this gradient vector, known as the stationarity residual, must be close to zero:\n    $$\n    \\|\\nabla_x L(x, \\lambda, \\mu)\\|_2 \\le \\epsilon\n    $$\n\nThe implemented function `check_kkt` will systematically perform these four checks. If any single check fails, the function immediately returns `False`. If all checks pass, it returns `True`.\n\nLet us demonstrate with the first test case: $x = [1, 1]$, $\\lambda = [1, 0, 0]$, $\\mu = -1$, and $\\epsilon = 10^{-8}$.\n-   Variables: $x_1 = 1$, $x_2 = 1$, $\\lambda_1 = 1$, $\\lambda_2 = 0$, $\\lambda_3 = 0$, $\\mu = -1$.\n\n-   **Primal Feasibility**:\n    -   $g_1(x) = 1 + 1 - 2 = 0$. $0 \\le 10^{-8}$ is true.\n    -   $g_2(x) = -1$. $-1 \\le 10^{-8}$ is true.\n    -   $g_3(x) = -1$. $-1 \\le 10^{-8}$ is true.\n    -   $h_1(x) = 1 - 1 = 0$. $|0| \\le 10^{-8}$ is true.\n    The condition is satisfied.\n\n-   **Dual Feasibility**:\n    -   $\\lambda_1 = 1 \\ge -10^{-8}$ is true.\n    -   $\\lambda_2 = 0 \\ge -10^{-8}$ is true.\n    -   $\\lambda_3 = 0 \\ge -10^{-8}$ is true.\n    The condition is satisfied.\n\n-   **Complementary Slackness**:\n    -   $|\\lambda_1 g_1(x)| = |1 \\cdot 0| = 0$. $0 \\le 10^{-8}$ is true.\n    -   $|\\lambda_2 g_2(x)| = |0 \\cdot (-1)| = 0$. $0 \\le 10^{-8}$ is true.\n    -   $|\\lambda_3 g_3(x)| = |0 \\cdot (-1)| = 0$. $0 \\le 10^{-8}$ is true.\n    The condition is satisfied.\n\n-   **Stationarity**:\n    -   $\\frac{\\partial L}{\\partial x_1} = 2(1 - 1) + 1 - 0 + (-1) = 0$.\n    -   $\\frac{\\partial L}{\\partial x_2} = 2(1 - 2) + 1 - 0 - (-1) = -2 + 1 + 1 = 0$.\n    -   The gradient vector is $(0, 0)$.\n    -   $\\|\\nabla_x L\\|_2 = \\sqrt{0^2 + 0^2} = 0$. $0 \\le 10^{-8}$ is true.\n    The condition is satisfied.\n\nSince all four conditions are satisfied within the tolerance $\\epsilon$, the point $(x, \\lambda, \\mu)$ is a valid KKT point, and the function correctly returns `True`. The other test cases are evaluated analogously, and the program will aggregate the boolean outcomes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef check_kkt(x, lam, mu, eps):\n    \"\"\"\n    Verifies the KKT conditions for a given point (x, lam, mu).\n\n    Args:\n        x (np.ndarray): Primal variables, a 2-element array [x1, x2].\n        lam (np.ndarray): Lagrange multipliers for inequality constraints, a 3-element array [lam1, lam2, lam3].\n        mu (float): Lagrange multiplier for the equality constraint.\n        eps (float): Tolerance for all numerical checks.\n\n    Returns:\n        bool: True if the KKT conditions are satisfied, False otherwise.\n    \"\"\"\n    x1, x2 = x[0], x[1]\n    lam1, lam2, lam3 = lam[0], lam[1], lam[2]\n\n    # --- 1. Primal Feasibility ---\n    # g1(x) = x1 + x2 - 2 <= 0\n    g1 = x1 + x2 - 2\n    if g1 > eps:\n        return False\n\n    # g2(x) = -x1 <= 0\n    g2 = -x1\n    if g2 > eps:\n        return False\n\n    # g3(x) = -x2 <= 0\n    g3 = -x2\n    if g3 > eps:\n        return False\n\n    # h1(x) = x1 - x2 = 0\n    h1 = x1 - x2\n    if abs(h1) > eps:\n        return False\n        \n    # --- 2. Dual Feasibility ---\n    # lam_i >= 0 for all i\n    if np.any(lam < -eps):\n        return False\n\n    # --- 3. Complementary Slackness ---\n    # lam1 * g1(x) = 0\n    if abs(lam1 * g1) > eps:\n        return False\n\n    # lam2 * g2(x) = 0\n    if abs(lam2 * g2) > eps:\n        return False\n\n    # lam3 * g3(x) = 0\n    if abs(lam3 * g3) > eps:\n        return False\n\n    # --- 4. Stationarity ---\n    # Gradient of the Lagrangian with respect to x must be zero.\n    # grad_L_x1 = 2*(x1 - 1) + lam1 - lam2 + mu\n    # grad_L_x2 = 2*(x2 - 2) + lam1 - lam3 - mu\n    grad_L_x1 = 2 * (x1 - 1) + lam1 - lam2 + mu\n    grad_L_x2 = 2 * (x2 - 2) + lam1 - lam3 - mu\n    \n    grad_L = np.array([grad_L_x1, grad_L_x2])\n    \n    # Check the Euclidean norm of the gradient\n    stationarity_residual = np.linalg.norm(grad_L)\n    \n    if stationarity_residual > eps:\n        return False\n        \n    # If all conditions pass\n    return True\n\ndef solve():\n    \"\"\"\n    Executes the KKT verification for a predefined suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (canonical KKT point)\n        {'x': np.array([1.0, 1.0]), 'lam': np.array([1.0, 0.0, 0.0]), 'mu': -1.0, 'eps': 1e-8},\n        # Test case 2 (stationarity violated)\n        {'x': np.array([1.0, 1.0]), 'lam': np.array([0.5, 0.0, 0.0]), 'mu': -0.5, 'eps': 1e-8},\n        # Test case 3 (primal infeasible)\n        {'x': np.array([1.2, 1.2]), 'lam': np.array([1.0, 0.0, 0.0]), 'mu': -1.0, 'eps': 1e-8},\n        # Test case 4 (near-KKT within tolerance)\n        {'x': np.array([1.0, 1.0 + 1e-7]), 'lam': np.array([1.0, -1e-7, 0.0]), 'mu': -1.0 - 1e-7, 'eps': 1e-6},\n        # Test case 5 (complementary slackness violated)\n        {'x': np.array([1.0, 1.0]), 'lam': np.array([1.0, 0.1, 0.0]), 'mu': -1.0, 'eps': 1e-8},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = check_kkt(case['x'], case['lam'], case['mu'], case['eps'])\n        results.append(result)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2407334"}]}