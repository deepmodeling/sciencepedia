{"hands_on_practices": [{"introduction": "The simplest linear programs can be solved by starting from the origin. However, many real-world problems include equality or \"greater-than-or-equal-to\" constraints, which means the origin is not a feasible starting point. This exercise guides you through the Two-Phase Simplex Method, a powerful and systematic procedure for first finding a valid starting basic feasible solution (Phase I) and then proceeding to find the optimal solution (Phase II) [@problem_id:2446110].", "problem": "Consider the following Linear Program (LP) in standard decision variables, designed so that the naive all-slack-variable basis is not directly usable and a feasibility-seeking procedure is required to obtain an initial basic feasible solution. Determine the optimal value of the objective function.\nMaximize $z = x_{1} + 3 x_{2}$\nsubject to\n$x_{1} + 2 x_{2} = 4$,\n$2 x_{1} + x_{2} \\ge 3$,\n$x_{1} + x_{2} \\le 6$,\n$x_{1} \\ge 0$, $x_{2} \\ge 0$.\nProvide the exact optimal value of $z$. No rounding is required.", "solution": "The problem statement constitutes a standard Linear Program (LP). Before proceeding to a solution, its validity must be established.\n\n**Problem Validation**\n\nThe givens are:\nObjective Function: Maximize $z = x_{1} + 3 x_{2}$\nConstraints:\n$1$. $x_{1} + 2 x_{2} = 4$\n$2$. $2 x_{1} + x_{2} \\ge 3$\n$3$. $x_{1} + x_{2} \\le 6$\n$4$. $x_{1} \\ge 0$, $x_{2} \\ge 0$\n\nThe problem is scientifically grounded as it is a well-defined mathematical optimization problem. It is well-posed, objective, and self-contained. The constraints are not contradictory, and a feasible region exists. For instance, the point $(x_1, x_2) = (4, 0)$ satisfies all constraints: $4+2(0)=4$, $2(4)+0=8 \\ge 3$, $4+0=4 \\le 6$, $4 \\ge 0$, $0 \\ge 0$. The problem is therefore valid, and a solution can be determined.\n\nThe presence of an equality constraint and a 'greater than or equal to' constraint necessitates a feasibility-seeking procedure, as the origin $(x_1, x_2) = (0, 0)$ is not a feasible point. The Two-Phase Simplex Method is the appropriate procedure.\n\n**Phase I: Find a Basic Feasible Solution**\n\nFirst, the LP is converted to standard form by introducing a surplus variable $s_1$, a slack variable $s_2$, and artificial variables $a_1$ and $a_2$.\nThe constraints become:\n$x_{1} + 2 x_{2} + a_{1} = 4$\n$2 x_{1} + x_{2} - s_{1} + a_{2} = 3$\n$x_{1} + x_{2} + s_{2} = 6$\nwith all variables $x_{1}, x_{2}, s_{1}, s_{2}, a_{1}, a_{2} \\ge 0$.\n\nThe Phase I objective is to minimize the sum of artificial variables, $w = a_{1} + a_{2}$, which is equivalent to maximizing $z' = -w = -a_{1} - a_{2}$. We must express $z'$ in terms of the non-basic variables.\nFrom the first two constraints:\n$a_{1} = 4 - x_{1} - 2 x_{2}$\n$a_{2} = 3 - 2 x_{1} - x_{2} + s_{1}$\nSubstituting these into the expression for $z'$:\n$z' = -(4 - x_{1} - 2 x_{2}) - (3 - 2 x_{1} - x_{2} + s_{1}) = -7 + 3 x_{1} + 3 x_{2} - s_{1}$\nThe objective function row for the initial tableau is thus $z' - 3 x_{1} - 3 x_{2} + s_{1} = -7$.\n\nThe initial Phase I simplex tableau is:\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis} & z' & x_1 & x_2 & s_1 & s_2 & a_1 & a_2 & \\text{RHS} \\\\\n\\hline\nz' & 1 & -3 & -3 & 1 & 0 & 0 & 0 & -7 \\\\\n\\hline\na_1 & 0 & 1 & 2 & 0 & 0 & 1 & 0 & 4 \\\\\na_2 & 0 & 2 & 1 & -1 & 0 & 0 & 1 & 3 \\\\\ns_2 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 6\n\\end{array}\n$$\nThe most negative coefficient in the $z'$ row is $-3$, corresponding to $x_1$ and $x_2$. We choose $x_1$ as the entering variable. The ratio test determines the leaving variable: $\\min\\{ \\frac{4}{1}, \\frac{3}{2}, \\frac{6}{1} \\} = \\frac{3}{2}$. The pivot row is the $a_2$ row. The pivot element is $2$.\nAfter performing row operations to make $x_1$ a basic variable, the tableau becomes:\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis} & z' & x_1 & x_2 & s_1 & s_2 & a_1 & a_2 & \\text{RHS} \\\\\n\\hline\nz' & 1 & 0 & -\\frac{3}{2} & -\\frac{1}{2} & 0 & 0 & \\frac{3}{2} & -\\frac{5}{2} \\\\\n\\hline\na_1 & 0 & 0 & \\frac{3}{2} & \\frac{1}{2} & 0 & 1 & -\\frac{1}{2} & \\frac{5}{2} \\\\\nx_1 & 0 & 1 & \\frac{1}{2} & -\\frac{1}{2} & 0 & 0 & \\frac{1}{2} & \\frac{3}{2} \\\\\ns_2 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 1 & 0 & -\\frac{1}{2} & \\frac{9}{2}\n\\end{array}\n$$\nThe next entering variable is $x_2$, with a coefficient of $-\\frac{3}{2}$. The ratio test: $\\min\\{ \\frac{5/2}{3/2}, \\frac{3/2}{1/2}, \\frac{9/2}{1/2} \\} = \\min\\{ \\frac{5}{3}, 3, 9 \\} = \\frac{5}{3}$. The pivot row is the $a_1$ row. The pivot element is $\\frac{3}{2}$.\nAfter row operations, the tableau is:\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis} & z' & x_1 & x_2 & s_1 & s_2 & a_1 & a_2 & \\text{RHS} \\\\\n\\hline\nz' & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\\\\n\\hline\nx_2 & 0 & 0 & 1 & \\frac{1}{3} & 0 & \\frac{2}{3} & -\\frac{1}{3} & \\frac{5}{3} \\\\\nx_1 & 0 & 1 & 0 & -\\frac{2}{3} & 0 & -\\frac{1}{3} & \\frac{2}{3} & \\frac{2}{3} \\\\\ns_2 & 0 & 0 & 0 & \\frac{1}{3} & 1 & -\\frac{1}{3} & -\\frac{1}{3} & \\frac{11}{3}\n\\end{array}\n$$\nThe Phase I objective function value is $0$, and there are no negative coefficients in the $z'$ row. Phase I is complete. The artificial variables $a_1$ and $a_2$ have been driven to $0$ and can be removed from the problem. We have found a basic feasible solution: $x_1 = \\frac{2}{3}$, $x_2 = \\frac{5}{3}$, $s_2 = \\frac{11}{3}$, with non-basic variables $s_1=0$.\n\n**Phase II: Solve the Original Problem**\n\nWe now use the final tableau of Phase I to begin Phase II. The artificial variable columns are discarded, and the objective function is replaced with the original one, $z = x_{1} + 3 x_{2}$. The objective row must be expressed in terms of the current non-basic variables ($s_1$).\nThe basic variables are:\n$x_{1} = \\frac{2}{3} + \\frac{2}{3} s_{1}$\n$x_{2} = \\frac{5}{3} - \\frac{1}{3} s_{1}$\nSubstituting into the objective function $z$:\n$z = (\\frac{2}{3} + \\frac{2}{3} s_{1}) + 3 (\\frac{5}{3} - \\frac{1}{3} s_{1}) = \\frac{2}{3} + \\frac{2}{3} s_{1} + 5 - s_{1} = \\frac{17}{3} - \\frac{1}{3} s_{1}$\nThe Phase II objective function row is $z + \\frac{1}{3} s_{1} = \\frac{17}{3}$.\n\nThe initial Phase II tableau is:\n$$\n\\begin{array}{c|c|cccc|c}\n\\text{Basis} & z & x_1 & x_2 & s_1 & s_2 & \\text{RHS} \\\\\n\\hline\nz & 1 & 0 & 0 & \\frac{1}{3} & 0 & \\frac{17}{3} \\\\\n\\hline\nx_2 & 0 & 0 & 1 & \\frac{1}{3} & 0 & \\frac{5}{3} \\\\\nx_1 & 0 & 1 & 0 & -\\frac{2}{3} & 0 & \\frac{2}{3} \\\\\ns_2 & 0 & 0 & 0 & \\frac{1}{3} & 1 & \\frac{11}{3}\n\\end{array}\n$$\nThe objective row equation is $z + \\frac{1}{3}s_1 = \\frac{17}{3}$, which can be written as $z = \\frac{17}{3} - \\frac{1}{3}s_1$. For a maximization problem, the solution is optimal if the reduced costs of all non-basic variables are non-positive. Here, the reduced cost of the non-basic variable $s_1$ is $-\\frac{1}{3}$. Since this is non-positive, the current basic feasible solution is optimal. The optimal value of the objective function is found in the RHS column of the $z$ row.\n\nThe optimal value is $z = \\frac{17}{3}$. The optimal solution is $x_1 = \\frac{2}{3}$ and $x_2 = \\frac{5}{3}$.", "answer": "$$\n\\boxed{\\frac{17}{3}}\n$$", "id": "2446110"}, {"introduction": "The standard simplex algorithm moves between feasible vertices to find the optimum. This practice introduces a powerful alternative: the Dual Simplex Method. Here, you will start from a \"super-optimal\" but infeasible solution and pivot towards the feasible region, providing a different perspective on the geometry of linear programming and a crucial tool for advanced topics like sensitivity analysis [@problem_id:2446085].", "problem": "Consider the following Linear Programming (LP) problem in standard form for maximization with decision variables $x_1$ and $x_2$ and slack variables $s_1$ and $s_2$:\nMaximize $-x_1 - 3 x_2$\nsubject to\n$-2 x_1 + x_2 + s_1 = -2$,\n$x_1 + x_2 + s_2 = 4$,\n$x_1 \\ge 0$, $x_2 \\ge 0$, $s_1 \\ge 0$, $s_2 \\ge 0$.\nTake the initial basis to be the slack variables $\\{s_1, s_2\\}$, so the initial basic solution is given by $x_1 = 0$, $x_2 = 0$, $s_1 = -2$, $s_2 = 4$. This basis is dual-feasible and primal-infeasible. Perform exactly one iteration of the Dual Simplex Method (DSM), updating the basis once and obtaining the new basic solution and objective value. Then, interpret the geometric meaning of this single dual simplex step in terms of active constraints and movement to an adjacent basis.\nProvide only the updated objective value $z$ after this single dual simplex pivot. No rounding is required; provide the exact value.", "solution": "The user has provided a Linear Programming (LP) problem and requested a single iteration of the Dual Simplex Method (DSM). I will first validate the problem statement.\n\nStep 1: Extract Givens\nThe problem is to maximize the objective function $z = -x_1 - 3x_2$.\nThe constraints are:\n$c_1: -2 x_1 + x_2 + s_1 = -2$\n$c_2: x_1 + x_2 + s_2 = 4$\nThe non-negativity conditions are $x_1 \\ge 0$, $x_2 \\ge 0$, $s_1 \\ge 0$, $s_2 \\ge 0$.\nThe initial basis is given as $\\{s_1, s_2\\}$.\nThe initial basic solution is given as $x_1 = 0$, $x_2 = 0$, $s_1 = -2$, $s_2 = 4$.\nThe problem states this basis is dual-feasible and primal-infeasible.\n\nStep 2: Validate Using Extracted Givens\nThe problem is a standard LP formulation. It is scientifically grounded in the field of computational engineering and optimization theory. The language is objective and precise. I must verify the claims about feasibility.\n\nPrimal Feasibility Check: A basic solution is primal-feasible if all variables have non-negative values. The initial solution has $s_1 = -2$, which violates the non-negativity constraint $s_1 \\ge 0$. Therefore, the initial basic solution is indeed primal-infeasible.\n\nDual Feasibility Check: For a maximization problem, a basis is dual-feasible if the reduced costs of all non-basic variables are non-positive. Alternatively, in a tableau where the objective row is written as $z - \\sum c'_j x_j = z_0$, dual feasibility requires all $c'_j \\le 0$. The common convention used here, which I will adopt, is to write the objective function as $z + \\sum \\bar{c}_j x_j = z_0$, where $\\bar{c}_j$ are the negatives of the reduced costs. For a maximization problem, this formulation requires all $\\bar{c}_j \\ge 0$ for dual feasibility (optimality condition).\nThe objective function is $z = -x_1 - 3x_2$, which can be rewritten as $z + x_1 + 3x_2 = 0$.\nThe non-basic variables are $x_1$ and $x_2$. Their coefficients in the objective row are $\\bar{c}_{x_1} = 1$ and $\\bar{c}_{x_2} = 3$. Both are non-negative. Therefore, the initial basic solution is indeed dual-feasible.\n\nThe problem is self-contained, consistent, well-posed, and scientifically grounded. It requires a standard, verifiable computational procedure.\n\nStep 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe initial simplex tableau is constructed from the given system of equations. The objective row is derived from $z + x_1 + 3x_2 = 0$. The basic variables are $s_1$ and $s_2$.\n\nInitial Tableau:\n$$\n\\begin{array}{c|c|cccc|c}\n\\text{Basis} & z & x_1 & x_2 & s_1 & s_2 & \\text{RHS} \\\\\n\\hline\nz & 1 & 1 & 3 & 0 & 0 & 0 \\\\\n\\hline\ns_1 & 0 & -2 & 1 & 1 & 0 & -2 \\\\\ns_2 & 0 & 1 & 1 & 0 & 1 & 4 \\\\\n\\end{array}\n$$\n\nOne iteration of the Dual Simplex Method consists of the following steps:\n\n1.  Select the Leaving Variable (Pivot Row): The leaving variable is the basic variable with the most negative value in the Right-Hand Side (RHS) column. Here, the only basic variable with a negative value is $s_1 = -2$. Thus, $s_1$ is the leaving variable, and the row corresponding to $s_1$ is the pivot row.\n\n2.  Select the Entering Variable (Pivot Column): The entering variable is chosen from the non-basic variables ($x_1, x_2$) by applying the dual ratio test. The ratio is computed as $|\\frac{\\bar{c}_j}{a_{ij}}|$, where $i$ is the index of the pivot row, $\\bar{c}_j$ are the coefficients in the objective row, and $a_{ij}$ are the coefficients in the pivot row. This test is only applied to columns $j$ where $a_{ij} < 0$.\nIn the pivot row ($s_1$ row), the coefficient for $x_1$ is $a_{1,x_1} = -2$, and for $x_2$ is $a_{1,x_2} = 1$. We only consider $x_1$ since its coefficient is negative.\nThe ratio for $x_1$ is:\n$$ \\left| \\frac{\\bar{c}_{x_1}}{a_{1,x_1}} \\right| = \\left| \\frac{1}{-2} \\right| = \\frac{1}{2} $$\nSince $x_1$ is the only candidate, it becomes the entering variable. The pivot element is $-2$, located at the intersection of the $s_1$ row and the $x_1$ column.\n\n3.  Perform the Pivot Operation: We update the tableau by making the pivot element $1$ and all other elements in the pivot column $0$.\nLet $R_0, R_1, R_2$ be the objective row, $s_1$ row, and $s_2$ row, respectively. Let the new rows be $R'_0, R'_1, R'_2$.\nThe new pivot row $R'_1$ is obtained by dividing $R_1$ by the pivot element, $-2$.\n$$ R'_1 = \\frac{R_1}{-2} = \\frac{1}{-2} \\begin{pmatrix} 0 & -2 & 1 & 1 & 0 & -2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 & 1 \\end{pmatrix} $$\nThe new objective row is $R'_0 = R_0 - (1) R'_1$.\n$$ R'_0 = \\begin{pmatrix} 1 & 1 & 3 & 0 & 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & \\frac{7}{2} & \\frac{1}{2} & 0 & -1 \\end{pmatrix} $$\nThe new $s_2$ row is $R'_2 = R_2 - (1) R'_1$.\n$$ R'_2 = \\begin{pmatrix} 0 & 1 & 1 & 0 & 1 & 4 \\end{pmatrix} - \\begin{pmatrix} 0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & \\frac{3}{2} & \\frac{1}{2} & 1 & 3 \\end{pmatrix} $$\n\nThe updated tableau after one iteration is:\n$$\n\\begin{array}{c|c|cccc|c}\n\\text{Basis} & z & x_1 & x_2 & s_1 & s_2 & \\text{RHS} \\\\\n\\hline\nz & 1 & 0 & \\frac{7}{2} & \\frac{1}{2} & 0 & -1 \\\\\n\\hline\nx_1 & 0 & 1 & -\\frac{1}{2} & -\\frac{1}{2} & 0 & 1 \\\\\ns_2 & 0 & 0 & \\frac{3}{2} & \\frac{1}{2} & 1 & 3 \\\\\n\\end{array}\n$$\n\nThe new basic solution is found by setting the non-basic variables $x_2$ and $s_1$ to $0$. From the tableau, we read the values of the basic variables: $x_1 = 1$, $s_2 = 3$. The full solution is $(x_1, x_2, s_1, s_2) = (1, 0, 0, 3)$. This solution is now primal-feasible as all variables are non-negative. The updated objective value is read from the RHS of the objective row. The objective row represents the equation $z + \\frac{7}{2}x_2 + \\frac{1}{2}s_1 = -1$. With the non-basic variables equal to zero, we get $z = -1$.\nWe can verify this with the original objective function: $z = -x_1 - 3x_2 = -(1) - 3(0) = -1$.\n\nGeometric Interpretation:\nThe initial basic solution $(x_1, x_2, s_1, s_2) = (0, 0, -2, 4)$ corresponds to the vertex $(x_1, x_2) = (0, 0)$ in the decision variable space. The active constraints are $x_1=0$ and $x_2=0$. This point is primal-infeasible because it violates the constraint $-2x_1 + x_2 \\le -2$ (since $0 \\not\\le -2$). This infeasibility is represented by the negative slack variable $s_1=-2$.\nThe Dual Simplex step performs a basis change. The leaving variable $s_1$ implies that the constraint associated with it, $-2x_1 + x_2 = -2$ (i.e., $s_1=0$), becomes active. The entering variable $x_1$ implies that the constraint $x_1=0$ becomes inactive.\nThe new basis $\\{x_1, s_2\\}$ corresponds to a new vertex where the new non-basic variables are zero: $x_2=0$ and $s_1=0$. The active constraints are now $x_2=0$ and $-2x_1+x_2 = -2$. Solving this system gives $x_1 = 1$ and $x_2 = 0$.\nGeometrically, the single dual simplex step moved from the primal-infeasible vertex $(0,0)$ to the adjacent vertex $(1,0)$. The new vertex $(1,0)$ is primal-feasible, as it satisfies all constraints: $-2(1)+0 = -2 \\le -2$ and $1+0=1 \\le 4$. The method moved from a \"super-optimal\" but infeasible point towards the feasible region, arriving at a basic feasible solution. The objective value changed from $z=0$ to $z=-1$, which is a decrease, as expected when moving from an infeasible region to a feasible one in a maximization problem.\n\nThe problem asks for the updated objective value $z$ after this single pivot. That value is $-1$.", "answer": "$$ \\boxed{-1} $$", "id": "2446085"}, {"introduction": "While simplex tableaus are excellent for learning, large-scale practical implementations rely on what's known as the Revised Simplex Method, an approach that focuses on efficiently updating the inverse of the basis matrix, $B^{-1}$. This coding exercise challenges you to implement this crucial computational step using the Sherman-Morrison-Woodbury formula. By doing so, you will directly experience the link between abstract linear algebra and the computational engine that makes the simplex method so powerful [@problem_id:2446121].", "problem": "You are to write a complete program that, for a set of predefined basis-update scenarios arising in the simplex method, computes the inverse of the updated basis from the current inverse, and verifies its correctness against direct inversion. The setting is as follows: a basis matrix $B \\in \\mathbb{R}^{n \\times n}$ is nonsingular, and a subset of its columns is replaced by new vectors to form an updated matrix $B_{\\text{new}}$. The positions of the replaced columns are given by an index set $J = \\{j_{1}, \\dots, j_{k}\\}$ with $j_{p} \\in \\{1, \\dots, n\\}$, and the corresponding replacement vectors are $a^{(p)} \\in \\mathbb{R}^{n}$ for $p \\in \\{1, \\dots, k\\}$. You are given $B$, $B^{-1}$, $J$, and the list of replacement vectors. Your task is to compute an updated inverse $B_{\\text{new}}^{-1}$ from $B^{-1}$ by any mathematically valid method, and to quantify the accuracy of your result by comparing to a direct inversion of $B_{\\text{new}}$.\n\nFor each test case described below, your program must:\n- Form $B_{\\text{new}}$ by replacing the columns of $B$ at the indices in $J$ with the provided replacement vectors.\n- Compute an updated inverse $B_{\\text{new}}^{-1}$ from the given $B^{-1}$ without directly inverting $B_{\\text{new}}$.\n- Compute a reference inverse $\\widehat{B_{\\text{new}}^{-1}}$ by inverting $B_{\\text{new}}$ directly using standard dense linear algebra.\n- Compute the maximum absolute entry-wise discrepancy\n$$\n\\Delta \\;=\\; \\max_{1 \\le i \\le n,\\, 1 \\le j \\le n} \\left| \\left(B_{\\text{new}}^{-1}\\right)_{ij} - \\left(\\widehat{B_{\\text{new}}^{-1}}\\right)_{ij} \\right|.\n$$\n\nTest Suite:\nYou must hard-code the following four test cases. In each case, indices in $J$ are given in $1$-based notation; when implementing in code, you may use any consistent internal indexing. All matrices and vectors are over the real numbers.\n\n- Test case $1$ (single-column update, moderate conditioning):\n  - $n = 3$,\n  - $B^{(1)} = \\begin{bmatrix} 2 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$,\n  - $J^{(1)} = \\{2\\}$,\n  - Replacement $a^{(1)} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}$ for column $2$.\n\n- Test case $2$ (two-column update in a banded matrix):\n  - $n = 4$,\n  - $B^{(2)} = \\begin{bmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 4 & 1 & 0 \\\\ 0 & 1 & 4 & 1 \\\\ 0 & 0 & 1 & 3 \\end{bmatrix}$,\n  - $J^{(2)} = \\{1, 3\\}$,\n  - Replacements:\n    - $a^{(2,1)} = \\begin{bmatrix} 5 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ for column $1$,\n    - $a^{(2,3)} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 5 \\\\ 0 \\end{bmatrix}$ for column $3$.\n\n- Test case $3$ (near-singular update from the identity):\n  - $n = 3$,\n  - $B^{(3)} = I_{3}$, where $I_{3}$ is the $3 \\times 3$ identity matrix,\n  - $J^{(3)} = \\{2\\}$,\n  - Replacement $a^{(3)} = \\begin{bmatrix} 10^{-8} \\\\ 10^{-8} \\\\ 0 \\end{bmatrix}$ for column $2$.\n\n- Test case $4$ (mixed update with a zero-difference column):\n  - $n = 5$,\n  - $B^{(4)} = \\operatorname{diag}(2, 3, 4, 5, 6)$,\n  - $J^{(4)} = \\{2, 5\\}$,\n  - Replacements:\n    - $a^{(4,2)} = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$ for column $2$ (i.e., no change),\n    - $a^{(4,5)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 6 \\end{bmatrix}$ for column $5$.\n\nFor each test case, the required answer is the single real number $\\Delta$ defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$, $2$, $3$, $4$. For example, an output line has the form\n\"[r1,r2,r3,r4]\"\nwhere $r1$, $r2$, $r3$, $r4$ are the computed values of $\\Delta$ for the four cases. No units are involved and no angles appear; all quantities are dimensionless real numbers.", "solution": "The problem presented is well-posed and addresses a fundamental operation in computational linear algebra: updating the inverse of a matrix after a low-rank modification. This procedure is central to the efficiency of iterative methods in optimization, most notably the simplex algorithm for linear programming, where basis matrices are updated one column at a time. The correct and computationally efficient approach is to apply the Sherman-Morrison-Woodbury formula, which avoids the expensive re-computation of the inverse from scratch.\n\nLet the nonsingular basis matrix be $B \\in \\mathbb{R}^{n \\times n}$. We are given its inverse, $B^{-1}$. The updated basis, $B_{\\text{new}}$, is formed by replacing $k$ columns of $B$. Let the set of indices of the columns to be replaced be $J = \\{j_{1}, \\dots, j_{k}\\}$, where each $j_{p} \\in \\{1, \\dots, n\\}$. The $j_{p}$-th column of $B$, denoted $b_{j_p}$, is replaced by a new vector $a^{(p)} \\in \\mathbb{R}^{n}$.\n\nThis replacement operation can be expressed as a rank-$k$ update to the matrix $B$. Let $e_{j_p}$ be the standard basis vector with a $1$ at position $j_p$ and zeros elsewhere. The original column $b_{j_p}$ can be written as $B e_{j_p}$. The updated matrix $B_{\\text{new}}$ is given by:\n$$\nB_{\\text{new}} = B - \\sum_{p=1}^{k} (B e_{j_p}) e_{j_p}^T + \\sum_{p=1}^{k} a^{(p)} e_{j_p}^T = B + \\sum_{p=1}^{k} (a^{(p)} - B e_{j_p}) e_{j_p}^T\n$$\nThis can be written in a compact form as $B_{\\text{new}} = B + UV^T$, where $U$ and $V$ are matrices of size $n \\times k$. The columns of $U$ are the update vectors $u_p = a^{(p)} - B e_{j_p}$, and the columns of $V$ are the standard basis vectors $v_p = e_{j_p}$.\n\nThe Sherman-Morrison-Woodbury formula provides an expression for the inverse of such a rank-$k$ updated matrix:\n$$\n(B + UV^T)^{-1} = B^{-1} - B^{-1} U (I_k + V^T B^{-1} U)^{-1} V^T B^{-1}\n$$\nHere, $I_k$ is the $k \\times k$ identity matrix. This formula is advantageous because it requires the inversion of a much smaller matrix, $C = I_k + V^T B^{-1} U$, which is of size $k \\times k$. In the context of the simplex method, typically only one column is updated ($k=1$), in which case $C$ is a scalar and its inversion is trivial.\n\nTo implement the solution, we follow these steps for each test case:\n\n$1$. **Initialization**: Given $B$, $B^{-1}$ (which we compute from $B$ as it is not explicitly provided), the set of $1$-based indices $J$, and the replacement vectors $\\{a^{(p)}\\}_{p=1}^k$. We convert $J$ to $0$-based indices for computation. The number of replacements is $k = |J|$.\n\n$2$. **Construct Update Matrices**: We form the $n \\times k$ matrix $U$ whose columns are $u_p = a^{(p)} - b_{j_p}$. We do not need to explicitly form the matrix $V$, as its operations can be performed by indexing.\n\n$3$. **Apply the Woodbury Formula**: We compute the components of the formula:\n    - Let $W = B^{-1} U$. This is an $n \\times k$ matrix.\n    - Let $Z = V^T B^{-1}$. Since the rows of $V^T$ are $e_{j_p}^T$, the rows of $Z$ are simply the rows of $B^{-1}$ corresponding to the indices in $J$. So, $Z$ is a $k \\times n$ matrix.\n    - The small $k \\times k$ matrix to be inverted is $C = I_k + V^T B^{-1} U = I_k + ZU$. Alternatively, $C = I_k + V^T W$. The $(p,q)$-th entry of $V^T W$ is $e_{j_p}^T w_q$, which is the $j_p$-th entry of the vector $w_q$. Thus, $V^T W$ is formed by selecting the rows of $W$ corresponding to the indices in $J$.\n    - We compute $C^{-1}$.\n    - The updated inverse is then $B_{\\text{new}}^{-1} = B^{-1} - W C^{-1} Z$.\n\n$4$. **Verification**:\n    - We construct the matrix $B_{\\text{new}}$ explicitly by replacing the specified columns of $B$.\n    - We compute a reference inverse, $\\widehat{B_{\\text{new}}^{-1}}$, by directly inverting $B_{\\text{new}}$ using a standard numerical linear algebra library routine.\n    - We compute the maximum absolute entry-wise discrepancy $\\Delta$ between our calculated inverse and the reference inverse:\n    $$\n    \\Delta \\;=\\; \\max_{1 \\le i \\le n,\\, 1 \\le j \\le n} \\left| \\left(B_{\\text{new}}^{-1}\\right)_{ij} - \\left(\\widehat{B_{\\text{new}}^{-1}}\\right)_{ij} \\right|\n    $$\nThis procedure is applied to all provided test cases. Note that for Case $4$, one of the replacement vectors is identical to the column it replaces. This results in a zero column in the update matrix $U$, which is handled correctly by the formula without any special logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the basis update problem for a suite of test cases.\n    For each case, it computes the inverse of an updated basis matrix using the\n    Sherman-Morrison-Woodbury formula and compares it to a direct inversion.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (B, J, replacements)\n    # J contains 1-based indices.\n    # replacements is a list of column vectors corresponding to indices in J.\n    test_cases = [\n        # Test case 1 (single-column update, moderate conditioning)\n        (\n            np.array([[2., 1., 0.], [1., 3., 1.], [0., 1., 2.]]),\n            [2],\n            [np.array([1., 2., 1.])]\n        ),\n        # Test case 2 (two-column update in a banded matrix)\n        (\n            np.array([[4., 1., 0., 0.], [1., 4., 1., 0.], [0., 1., 4., 1.], [0., 0., 1., 3.]]),\n            [1, 3],\n            [np.array([5., 1., 0., 0.]), np.array([0., 2., 5., 0.])]\n        ),\n        # Test case 3 (near-singular update from the identity)\n        (\n            np.eye(3),\n            [2],\n            [np.array([1e-8, 1e-8, 0.])]\n        ),\n        # Test case 4 (mixed update with a zero-difference column)\n        (\n            np.diag([2., 3., 4., 5., 6.]),\n            [2, 5],\n            [np.array([0., 3., 0., 0., 0.]), np.array([1., 0., 0., 0., 6.])]\n        )\n    ]\n\n    results = []\n    for B, J_one_based, replacements in test_cases:\n        n = B.shape[0]\n        k = len(J_one_based)\n\n        # Convert 1-based indices to 0-based for numpy\n        J_zero_based = [j - 1 for j in J_one_based]\n\n        # Step 1: Compute given inverse B_inv and form B_new\n        B_inv = np.linalg.inv(B)\n        B_new = B.copy()\n        for i, j in enumerate(J_zero_based):\n            B_new[:, j] = replacements[i]\n\n        # Step 2: Compute reference inverse by direct inversion\n        B_new_inv_ref = np.linalg.inv(B_new)\n\n        # Step 3: Compute updated inverse using Sherman-Morrison-Woodbury formula\n        # B_new = B + U @ V.T\n        \n        # Construct the n x k matrix U\n        U_cols = []\n        for i, j in enumerate(J_zero_based):\n            b_j = B[:, j]\n            a_p = replacements[i]\n            u_p = a_p - b_j\n            U_cols.append(u_p)\n        U = np.column_stack(U_cols)\n\n        # Apply the formula: B_new_inv = B_inv - B_inv @ U @ inv(I + V.T @ B_inv @ U) @ V.T @ B_inv\n        # Let W = B_inv @ U\n        # Let Z = V.T @ B_inv\n        \n        W = B_inv @ U\n        \n        # The matrix V consists of standard basis vectors e_j.\n        # V.T @ M selects rows from M.\n        # So, Z = B_inv[J_zero_based, :]\n        Z = B_inv[J_zero_based, :]\n        \n        # The core k x k matrix to invert is C = I_k + V.T @ W\n        # V.T @ W selects rows from W. So C = I_k + W[J_zero_based, :]\n        C = np.eye(k) + W[J_zero_based, :]\n        C_inv = np.linalg.inv(C)\n        \n        # Compute the final updated inverse\n        B_new_inv_calc = B_inv - W @ C_inv @ Z\n        \n        # Step 4: Compute the maximum absolute entry-wise discrepancy\n        discrepancy = np.max(np.abs(B_new_inv_calc - B_new_inv_ref))\n        results.append(discrepancy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2446121"}]}