{"hands_on_practices": [{"introduction": "The fundamental building block of any trust-region algorithm is the trust-region subproblem. In each iteration, we don't try to minimize the true, complex objective function directly, but rather a simpler quadratic model that we believe is accurate within a local \"trust region.\" This exercise provides a hands-on look at solving this core subproblem in a clear, one-dimensional setting, forcing you to decide between taking a full, unconstrained step or moving to the boundary of the trusted area [@problem_id:2224504].", "problem": "In the context of unconstrained optimization, trust-region methods iteratively approximate a complex function with a simpler model function $m(p)$ around the current point. The next step $p$ is then determined by solving the trust-region subproblem, which involves minimizing this model within a \"trust region\" of radius $\\Delta > 0$, where the model is believed to be a reliable approximation of the original function. The subproblem is formally stated as:\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nConsider a one-dimensional optimization scenario where the model function for a step $p \\in \\mathbb{R}$ is a quadratic given by:\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\nwith a gradient term $g = 2$, a Hessian term $H = 6$, and an arbitrary constant $c$. The step is constrained by a trust-region radius of $\\Delta = 0.1$.\n\nDetermine the optimal step $p$ that solves this one-dimensional trust-region subproblem. Provide your answer as an exact decimal number.", "solution": "We must minimize the quadratic model $m(p) = g p + \\frac{1}{2} H p^{2} + c$ subject to the trust-region constraint $|p| \\le \\Delta$, with given values $g=2$, $H=6$, and $\\Delta=0.1$. The constant $c$ does not affect the minimizer and can be ignored.\n\nConsider the Lagrangian for the trust-region subproblem:\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\nwith $\\lambda \\ge 0$. The Karush-Kuhn-Tucker conditions are:\n1. Stationarity: \n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. Primal feasibility: $|p| \\le \\Delta$.\n3. Dual feasibility: $\\lambda \\ge 0$.\n4. Complementary slackness: $\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$.\n\nCase 1 (interior solution): If $|p| < \\Delta$, then $\\lambda = 0$ and stationarity gives\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\nFor $g=2$ and $H=6$, this gives $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$. Check feasibility: $|p_{u}| = \\frac{1}{3} > 0.1 = \\Delta$, so the interior solution is infeasible.\n\nCase 2 (boundary solution): Then $p^{2} = \\Delta^{2}$, so $p = \\pm \\Delta$. Stationarity becomes\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\nSince $H + 2 \\lambda \\ge 0$, the sign of $p$ must match the sign of $-g$. With $g = 2 > 0$, we must take $p = -\\Delta = -0.1$. To verify dual feasibility, solve for $\\lambda$:\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\nSubstituting $g=2$, $\\Delta=0.1$, and $H=6$ gives\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\nwhich satisfies dual feasibility and complementary slackness.\n\nTherefore, the optimal trust-region step is the boundary step in the negative gradient direction:\n$$\np^{\\star} = -\\Delta = -0.1.\n$$", "answer": "$$\\boxed{-0.1}$$", "id": "2224504"}, {"introduction": "A single step is just one part of the puzzle; a robust optimization algorithm must link these steps together into an iterative process. This practice challenges you to implement a complete one-dimensional trust-region method from the ground up, moving beyond just solving a single subproblem. By coding the logic for step evaluation and trust-radius updates, you will see firsthand how the algorithm dynamically adapts to the landscape of the objective function, ensuring reliable convergence where simpler methods might fail [@problem_id:2461247].", "problem": "Consider the univariate objective function in dimensionless units given by $f(x)=x^4-x^2$ with first derivative $f'(x)=4x^3-2x$ and second derivative $f''(x)=12x^2-2$. The stationary points are the local maxima at $x=0$ and the local minima at $x_{\\pm}=\\pm 1/\\sqrt{2}$. For an iterate $x_k\\in\\mathbb{R}$, define the quadratic model $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ and a trust-region radius $\\Delta_k>0$. The trust-region subproblem is to find a step $s_k\\in\\mathbb{R}$ that minimizes $m_k(s)$ subject to the constraint $\\lvert s\\rvert\\le \\Delta_k$. Let the predicted reduction be $\\operatorname{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$ and the actual reduction be $\\operatorname{ared}_k=f(x_k)-f(x_k+s_k)$. Define the acceptance ratio $\\rho_k=\\operatorname{ared}_k/\\operatorname{pred}_k$ when $\\operatorname{pred}_k>0$; if $\\operatorname{pred}_k\\le 0$, set $\\rho_k=-\\infty$. A step is accepted if $\\rho_k\\ge \\eta_1$ and rejected otherwise. The trust-region radius is updated by the following rule with fixed parameters $\\eta_1\\in(0,1)$, $\\eta_2\\in(\\eta_1,1)$, $\\gamma_1\\in(0,1)$, and $\\gamma_2>1$:\n- If $\\rho_k<\\eta_1$, set $\\Delta_{k+1}=\\gamma_1\\Delta_k$.\n- If $\\rho_k\\ge \\eta_2$ and $\\lvert s_k\\rvert\\ge 0.8\\,\\Delta_k$, set $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$.\n- Otherwise, set $\\Delta_{k+1}=\\Delta_k$.\nIf a step is rejected, keep $x_{k+1}=x_k$; if it is accepted, set $x_{k+1}=x_k+s_k$. The iteration terminates when either $\\lvert f'(x_k)\\rvert\\le \\varepsilon_g$, $\\lvert s_k\\rvert\\le \\varepsilon_s$ for an accepted step, $\\Delta_k\\le \\varepsilon_s$, or when a fixed iteration cap is reached.\n\nIn one spatial dimension, the unique global minimizer of the quadratic model subject to $\\lvert s\\rvert\\le \\Delta$ is characterized as follows. For given $g\\in\\mathbb{R}$ and $h\\in\\mathbb{R}$ with $g=f'(x)$ and $h=f''(x)$,\n- If $h>0$ and the unconstrained minimizer $s_N=-g/h$ satisfies $\\lvert s_N\\rvert\\le \\Delta$, then $s^\\star=s_N$; otherwise $s^\\star=-\\operatorname{sign}(g)\\,\\Delta$.\n- If $h\\le 0$, then $s^\\star=\\Delta$ when $g<0$, $s^\\star=-\\Delta$ when $g>0$, and $s^\\star=\\Delta$ when $g=0$.\n\nFix the numerical parameters $\\eta_1=0.1$, $\\eta_2=0.9$, $\\gamma_1=0.25$, $\\gamma_2=2$, $\\Delta_{\\max}=10$, gradient tolerance $\\varepsilon_g=10^{-8}$, step tolerance $\\varepsilon_s=10^{-10}$, and a maximum of $100$ iterations. For each test case below, start from the given initial point $x_0$ with the given initial radius $\\Delta_0$, and apply the above iteration using the one-dimensional model minimizer $s_k$ at each step.\n\nDefine three scalar outputs for each test case:\n- $b_1$: the logical value that is true if and only if no accepted step ever increases the objective, that is, for all accepted steps $k$, $f(x_{k+1})\\le f(x_k)$.\n- $b_2$: the logical value that is true if and only if the full unconstrained Newton step at the first iterate, $s_N=-f'(x_0)/f''(x_0)$ (when $f''(x_0)\\ne 0$; define $b_2$ as false if $f''(x_0)=0$), produces a higher objective value, that is, $f(x_0+s_N)>f(x_0)$.\n- $d$: the absolute distance from the final accepted iterate $x_{\\mathrm{final}}$ to the nearest local minimizer, $d=\\min\\{\\lvert x_{\\mathrm{final}}-1/\\sqrt{2}\\rvert,\\lvert x_{\\mathrm{final}}+1/\\sqrt{2}\\rvert\\}$.\n\nTest suite parameters to evaluate:\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list $[b_1,b_2,d]$ where $b_1$ and $b_2$ are the lowercase strings \"true\" or \"false\", and $d$ is a decimal-rounded float with exactly six digits after the decimal point. The final output must therefore be a single line of the form\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$,\nwith no spaces anywhere in the line.", "solution": "The problem is valid. It presents a well-posed, self-contained, and scientifically sound exercise in numerical optimization, specifically the application of a trust-region algorithm to a one-dimensional potential energy function. All necessary parameters, algorithmic rules, functions, and termination criteria are provided with mathematical precision. We shall proceed with a complete solution.\n\nThe core of this problem is to implement and analyze a trust-region optimization algorithm. This class of methods is fundamental in computational sciences, particularly in computational chemistry for locating stable molecular geometries, which correspond to minima on a potential energy surface. The objective function $f(x) = x^4 - x^2$ is a canonical one-dimensional double-well potential, representing a system with two stable states (minima) at $x_{\\pm} = \\pm 1/\\sqrt{2}$ and an unstable transition state (maximum) at $x=0$.\n\nA trust-region algorithm iteratively finds a minimum by constructing a simpler model of the objective function, which is trusted only within a neighborhood of the current iterate $x_k$. This neighborhood is a \"trust region\" of radius $\\Delta_k$. The model is a quadratic function, $m_k(s)$, derived from a second-order Taylor expansion of $f(x)$ around $x_k$:\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\nwhere $s$ is the step from $x_k$. This model is minimized with respect to $s$ subject to the constraint $\\lvert s \\rvert \\le \\Delta_k$. This constrained minimization is called the trust-region subproblem.\n\nThe solution to the one-dimensional subproblem, as provided, depends on the curvature of the model, given by the second derivative $h = f''(x_k)$.\n1.  If $h > 0$, the model is convex (a parabola opening upwards). The unconstrained minimizer is the Newton step $s_N = -g/h$, where $g = f'(x_k)$. If this step lies within the trust region, i.e., $\\lvert s_N \\rvert \\le \\Delta_k$, it is the optimal step $s_k$. Otherwise, the model is minimized at the boundary of the trust region, $s_k = -\\operatorname{sign}(g)\\Delta_k$, moving as far as possible in the direction of steepest descent.\n2.  If $h \\le 0$, the model is locally concave or linear. Its minimum over the interval $[-\\Delta_k, \\Delta_k]$ must lie at one of the boundaries. The selection between $s = \\Delta_k$ and $s = -\\Delta_k$ is determined by the sign of the gradient $g$, which dictates the direction of descent.\n\nOnce the trial step $s_k$ is computed, its quality is assessed by comparing the *actual reduction* in the objective function, $\\operatorname{ared}_k = f(x_k) - f(x_k + s_k)$, to the *predicted reduction* from the model, $\\operatorname{pred}_k = m_k(0) - m_k(s_k)$. Their ratio, $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$, measures the fidelity of the model.\n\n-   If $\\rho_k$ is close to $1$, the model is an excellent predictor. The step is accepted, and we may expand the trust region ($\\Delta_{k+1} = \\gamma_2 \\Delta_k$) to allow for more aggressive steps, provided the current step was already near the trust region boundary.\n-   If $\\rho_k$ is positive but not large, the model is adequate. The step is accepted, but the trust region size is maintained ($\\Delta_{k+1} = \\Delta_k$).\n-   If $\\rho_k$ is small or negative, the model is poor. The step is rejected ($x_{k+1} = x_k$), and the trust region is shrunk ($\\Delta_{k+1} = \\gamma_1 \\Delta_k$) to improve model accuracy in the subsequent iteration.\n\nThe step acceptance rule is $\\rho_k \\ge \\eta_1$. Since $\\eta_1 = 0.1 > 0$ and the solution to the subproblem ensures $\\operatorname{pred}_k \\ge 0$, any accepted step must have $\\operatorname{ared}_k \\ge \\eta_1 \\operatorname{pred}_k \\ge 0$. If $\\operatorname{pred}_k > 0$, then $\\operatorname{ared}_k > 0$, guaranteeing that $f(x_{k+1}) < f(x_k)$. A step can only be accepted if the model predicts descent ($\\operatorname{pred}_k > 0$). Therefore, the condition for $b_1$—that no accepted step ever increases the objective function—is guaranteed to be true by the very construction of this algorithm. Any deviation would indicate a flawed implementation.\n\nThe output $b_2$ probes the limitation of the pure Newton-Raphson method. The unconstrained Newton step, $s_N = -f'(x_0)/f''(x_0)$, finds the extremum of the quadratic model. Near a maximum, such as at $x_0=0.0$ or $x_0=0.1$, the Hessian $f''(x_0)$ is negative. The Newton step thus seeks the *maximum* of the local quadratic model, which is a poor strategy for minimizing the global function $f(x)$ and is likely to result in an ascent step, i.e., $f(x_0+s_N) > f(x_0)$. The trust-region framework corrects this deficiency by constraining the step size.\n\nThe final output, $d$, measures the accuracy of convergence to one of the true minimizers, $x_{\\pm} = \\pm 1/\\sqrt{2}$. Given the initial points are all non-negative, the algorithm is expected to converge to the positive minimizer, $x_+ = 1/\\sqrt{2}$.\n\nThe implementation will proceed by first defining the objective function and its derivatives. Then, for each test case, the value of $b_2$ is computed. The main iterative loop is then executed, which involves, at each step $k$: checking for termination, solving the trust-region subproblem for $s_k$, evaluating the step quality via $\\rho_k$, and updating the state variables $x_k$ and $\\Delta_k$ according to the specified rules. The value of $b_1$ is tracked throughout the iteration. Upon termination, the final distance $d$ is computed.", "answer": "[[true,true,0.000000],[true,true,0.000000],[true,false,0.000000],[true,true,0.000000]]", "id": "2461247"}, {"introduction": "Real-world optimization problems often present challenging geometries, such as saddle points, where the curvature is not uniformly positive. Trust-region methods are specifically designed to handle these situations gracefully. This advanced exercise delves into the so-called \"hard case\" of the trust-region subproblem, a scenario that arises when the model's Hessian is indefinite and requires a more sophisticated solution than simply moving along the gradient. Mastering this case is key to understanding the full power and robustness of trust-region frameworks in complex engineering applications [@problem_id:2447650].", "problem": "Consider the two-dimensional quadratic objective function $f(x) = \\frac{1}{2} x^{\\mathsf{T}} H x + g^{\\mathsf{T}} x$ with $H = \\begin{pmatrix} -2 & 0 \\\\ 0 & 3 \\end{pmatrix}$ and $g = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$. Let the associated trust-region subproblem be to minimize the quadratic model $q(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p$ subject to the Euclidean-norm trust-region constraint $\\|p\\|_{2} \\leq \\Delta$, with $\\Delta = 1$. Determine the exact minimum value of $q(p)$ over the set $\\{p \\in \\mathbb{R}^{2} : \\|p\\|_{2} \\leq 1\\}$. Provide your answer in exact form as a single real number (no rounding).", "solution": "The problem presented is a standard trust-region subproblem in the field of numerical optimization. We are asked to find the minimum value of a quadratic model $q(p)$ within a spherical trust region of radius $\\Delta = 1$. The problem is formally stated as:\n$$\n\\text{minimize} \\quad q(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p\n$$\n$$\n\\text{subject to} \\quad \\|p\\|_{2} \\leq \\Delta\n$$\nwhere $p = \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$, and the given parameters are:\n$$\nH = \\begin{pmatrix} -2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad \\Delta = 1\n$$\nThe objective function can be written explicitly as:\n$$\nq(p_1, p_2) = \\begin{pmatrix} 0 & 2 \\end{pmatrix} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} p_1 & p_2 \\end{pmatrix} \\begin{pmatrix} -2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} = 2 p_2 + \\frac{1}{2}(-2 p_1^2 + 3 p_2^2) = -p_1^2 + \\frac{3}{2} p_2^2 + 2 p_2\n$$\nThe constraint is $p_1^2 + p_2^2 \\leq 1$.\n\nTo solve this constrained optimization problem, we utilize the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\nL(p, \\lambda) = q(p) + \\frac{\\lambda}{2} (p^{\\mathsf{T}} p - \\Delta^2)\n$$\nwhere $\\lambda \\geq 0$ is the Lagrange multiplier associated with the inequality constraint. The KKT conditions for a solution $p^*$ are:\n1. Stationarity: $\\nabla_p L(p^*, \\lambda) = \\nabla q(p^*) + \\lambda p^* = (H + \\lambda I) p^* = -g$\n2. Primal feasibility: $\\|p^*\\|_2 \\leq \\Delta$\n3. Dual feasibility: $\\lambda \\geq 0$\n4. Complementary slackness: $\\lambda (\\|p^*\\|_2^2 - \\Delta^2) = 0$\n5. Second-order necessity: The matrix $(H + \\lambda I)$ must be positive semidefinite.\n\nWe analyze the possible cases.\n\nCase 1: The solution is in the interior of the trust region, i.e., $\\|p^*\\|_2 < \\Delta$.\nBy the complementary slackness condition, this implies $\\lambda = 0$. The stationarity condition simplifies to $H p^* = -g$. For such a point to be a minimizer, the Hessian $H$ must be positive semidefinite. The eigenvalues of $H$ are the diagonal entries, $\\lambda_1(H) = -2$ and $\\lambda_2(H) = 3$. Since one eigenvalue is negative, $H$ is an indefinite matrix. The unconstrained quadratic $q(p)$ has a saddle point, not a minimum, and is unbounded below. Therefore, an interior solution is impossible. The solution must lie on the boundary of the trust region.\n\nCase 2: The solution lies on the boundary of the trust region, i.e., $\\|p^*\\|_2 = \\Delta = 1$.\nThis implies from the complementary slackness condition that $\\lambda \\geq 0$. We must also satisfy the second-order condition that $(H + \\lambda I)$ is positive semidefinite. The eigenvalues of $(H + \\lambda I)$ are $(-2 + \\lambda)$ and $(3 + \\lambda)$. For positive semidefiniteness, both must be non-negative:\n$$\n-2 + \\lambda \\geq 0 \\implies \\lambda \\geq 2\n$$\n$$\n3 + \\lambda \\geq 0 \\implies \\lambda \\geq -3\n$$\nBoth conditions together require $\\lambda \\geq 2$. From the stationarity condition $(H + \\lambda I) p^* = -g$:\n$$\n\\begin{pmatrix} -2+\\lambda & 0 \\\\ 0 & 3+\\lambda \\end{pmatrix} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} = -\\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}\n$$\nThis matrix equation yields a system of linear equations:\n$$\n(-2 + \\lambda) p_1 = 0\n$$\n$$\n(3 + \\lambda) p_2 = -2\n$$\nWe consider two subcases based on the value of $\\lambda$.\n\nSubcase 2a: $\\lambda > 2$.\nIn this case, $(-2 + \\lambda) \\neq 0$, so the first equation implies $p_1 = 0$. From the second equation, $p_2 = \\frac{-2}{3+\\lambda}$. We use the boundary condition $\\|p\\|_2 = 1$:\n$$\np_1^2 + p_2^2 = 0^2 + \\left(\\frac{-2}{3+\\lambda}\\right)^2 = 1\n$$\n$$\n\\frac{4}{(3+\\lambda)^2} = 1 \\implies (3+\\lambda)^2 = 4 \\implies 3+\\lambda = \\pm 2\n$$\nIf $3+\\lambda = 2$, then $\\lambda = -1$, which contradicts $\\lambda > 2$.\nIf $3+\\lambda = -2$, then $\\lambda = -5$, which also contradicts $\\lambda > 2$.\nThus, there is no solution with $\\lambda > 2$.\n\nSubcase 2b: $\\lambda = 2$.\nThis value satisfies the condition $\\lambda \\geq 2$. This situation, where $\\lambda$ equals the negative of the smallest eigenvalue of $H$ ($\\lambda = -\\lambda_{\\text{min}}(H)$), is known as the \"hard case\". The first equation becomes $( -2 + 2) p_1 = 0 \\cdot p_1 = 0$, which is satisfied for any $p_1 \\in \\mathbb{R}$. The second equation gives $(3 + 2) p_2 = -2$, so $5 p_2 = -2$, which implies $p_2 = -\\frac{2}{5}$.\nWe now use the boundary condition $\\|p\\|_2 = 1$:\n$$\np_1^2 + p_2^2 = 1 \\implies p_1^2 + \\left(-\\frac{2}{5}\\right)^2 = 1\n$$\n$$\np_1^2 + \\frac{4}{25} = 1 \\implies p_1^2 = 1 - \\frac{4}{25} = \\frac{21}{25}\n$$\nThis gives two possible values for $p_1$: $p_1 = \\pm \\frac{\\sqrt{21}}{5}$.\nWe have found two candidate solutions:\n$$\np^{(1)} = \\begin{pmatrix} \\frac{\\sqrt{21}}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} \\quad \\text{and} \\quad p^{(2)} = \\begin{pmatrix} -\\frac{\\sqrt{21}}{5} \\\\ -\\frac{2}{5} \\end{pmatrix}\n$$\nThe problem asks for the minimum value of $q(p)$. We evaluate $q(p)$ at these points. Notice that the function $q(p_1, p_2) = -p_1^2 + \\frac{3}{2} p_2^2 + 2 p_2$ depends on $p_1^2$, so the value of $q$ will be identical for both candidate points.\nUsing $p_1^2 = \\frac{21}{25}$ and $p_2 = -\\frac{2}{5}$:\n$$\nq_{\\text{min}} = - \\left(\\frac{21}{25}\\right) + \\frac{3}{2}\\left(-\\frac{2}{5}\\right)^2 + 2\\left(-\\frac{2}{5}\\right)\n$$\n$$\nq_{\\text{min}} = -\\frac{21}{25} + \\frac{3}{2}\\left(\\frac{4}{25}\\right) - \\frac{4}{5}\n$$\n$$\nq_{\\text{min}} = -\\frac{21}{25} + \\frac{12}{50} - \\frac{4}{5}\n$$\n$$\nq_{\\text{min}} = -\\frac{21}{25} + \\frac{6}{25} - \\frac{20}{25}\n$$\n$$\nq_{\\text{min}} = \\frac{-21 + 6 - 20}{25} = \\frac{-15 - 20}{25} = \\frac{-35}{25}\n$$\nSimplifying the fraction gives the exact minimum value:\n$$\nq_{\\text{min}} = -\\frac{7}{5}\n$$\nThis is the global minimum of the trust-region subproblem.", "answer": "$$\n\\boxed{-\\frac{7}{5}}\n$$", "id": "2447650"}]}