{"hands_on_practices": [{"introduction": "Our journey into the practical application of eigenvalue problems begins with a fundamental task: computing a specific eigenpair. While methods exist to find all eigenvalues of a matrix, many engineering scenarios, like analyzing resonance, require us to find only the eigenvalue closest to a specific target value. This exercise [@problem_id:2442752] challenges you to implement the inverse iteration with shift algorithm, a powerful technique that elegantly transforms this problem into one solvable by standard linear system solvers.", "problem": "Implement a program that computes an approximate eigenpair for each of several real matrices using inverse iteration with a real shift. The goal is to approximate the eigenpair corresponding to the eigenvalue of a matrix $A$ that is closest to a given target resonance frequency (the shift) $\\sigma$. The program must follow a principled algorithmic design rooted in the algebraic eigenvalue problem and spectral decomposition.\n\nStart from the following fundamentals:\n- An eigenpair $(\\lambda, v)$ of a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ satisfies $A v = \\lambda v$ with $v \\neq 0$.\n- If $A$ is diagonalizable, there exists a basis of eigenvectors $\\{v_i\\}_{i=1}^n$ with corresponding eigenvalues $\\{\\lambda_i\\}_{i=1}^n$, so that for any vector $x \\in \\mathbb{R}^n$ one can write $x = \\sum_{i=1}^n \\alpha_i v_i$.\n- For a real scalar shift $\\sigma \\in \\mathbb{R}$ such that $A - \\sigma I$ is invertible, the matrix $(A - \\sigma I)^{-1}$ has the same eigenvectors as $A$ and eigenvalues $\\{(\\lambda_i - \\sigma)^{-1}\\}_{i=1}^n$.\n\nYou must implement inverse iteration with shift $\\sigma$ as follows:\n- Use the shifted linear system $(A - \\sigma I) y^{(k)} = x^{(k-1)}$ at iteration $k$, starting from $x^{(0)}$ equal to the all-ones vector normalized to unit two-norm (Euclidean norm).\n- To solve the linear system robustly and efficiently across iterations, factor the same coefficient matrix once using LU (Lower-Upper) factorization and reuse it. If $(A - \\sigma I)$ is singular or numerically ill-conditioned, replace it with $(A - \\sigma I + \\delta I)$ for the smallest nonnegative $\\delta$ in the sequence $\\{0, 10^{-12}, 10^{-10}, 10^{-8}, 10^{-6}, 10^{-4}\\}$ that yields a condition number less than $10^{12}$ in the two-norm.\n- Normalize $x^{(k)} = y^{(k)} / \\| y^{(k)} \\|_2$ at every iteration.\n- At each iteration, compute the Rayleigh quotient $\\mu^{(k)} = (x^{(k)})^\\top A x^{(k)}$ as the eigenvalue estimate and the residual two-norm $r^{(k)} = \\| A x^{(k)} - \\mu^{(k)} x^{(k)} \\|_2$.\n- Terminate when $r^{(k)} \\le \\tau$ with $\\tau = 10^{-10}$, or when the iteration count reaches $k_{\\max} = 100$.\n\nYour program must produce, for each test case, the final eigenvalue estimate $\\mu^{(*)}$ as a floating-point number rounded to six decimal places. The eigenvector estimate need not be printed.\n\nAngle units are not involved in this problem. There are no physical units; all computations are in real arithmetic and the outputs are dimensionless.\n\nTest Suite (matrices $A$ and shifts $\\sigma$):\n- Case $1$ (symmetric tridiagonal):\n  $$A_1 = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}, \\quad \\sigma_1 = 3.2.$$\n- Case $2$ (upper triangular, non-symmetric; exact-shift singularity):\n  $$A_2 = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 5 \\end{bmatrix}, \\quad \\sigma_2 = 3.0.$$\n- Case $3$ (diagonal, clustered eigenvalues near the shift):\n  $$A_3 = \\operatorname{diag}(2.0, 2.1, 10.0), \\quad \\sigma_3 = 2.08.$$\n- Case $4$ (symmetric dense):\n  $$A_4 = \\begin{bmatrix} 6 & 2 & 1 \\\\ 2 & 3 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}, \\quad \\sigma_4 = 6.5.$$\n\nImplementation requirements:\n- Use a single LU (Lower-Upper) factorization of the chosen coefficient matrix per test case and reuse it across iterations.\n- Select the smallest $\\delta$ from the specified sequence that ensures the condition number of $A - \\sigma I + \\delta I$ is less than $10^{12}$ in the two-norm. If none satisfies this, use the last one $\\delta = 10^{-4}$.\n- Initialize with $x^{(0)}$ equal to the vector of all ones, normalized to unit two-norm. Use real arithmetic throughout.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example:\n  $$[\\mu_1,\\mu_2,\\mu_3,\\mu_4],$$\n  where each $\\mu_i$ is the final eigenvalue estimate for case $i$ rounded to six decimal places as a decimal number (not a fraction).\n\nYour program must be self-contained and require no input. It must execute the four specified test cases and print exactly one line in the format above.", "solution": "The problem presented is a standard task in computational linear algebra: to find an approximation for an eigenpair $(\\lambda, v)$ of a square matrix $A$ where the eigenvalue $\\lambda$ is closest to a specified real number $\\sigma$, known as the shift. The prescribed method is the inverse iteration algorithm with a shift, a robust and widely used numerical technique.\n\nThe foundation of this method rests on the properties of eigenvalues under matrix inversion and shifting. The standard eigenvalue problem is defined by the equation $A v = \\lambda v$, where $\\lambda$ is a scalar eigenvalue and $v$ is the corresponding non-zero eigenvector. If we consider a shifted matrix $(A - \\sigma I)$, where $\\sigma \\in \\mathbb{R}$ is the shift and $I$ is the identity matrix, its eigenvalues are $(\\lambda_i - \\sigma)$, where $\\{\\lambda_i\\}$ are the eigenvalues of $A$. The eigenvectors remain unchanged.\n\nAssuming the shifted matrix $(A - \\sigma I)$ is invertible, we can consider its inverse, $(A - \\sigma I)^{-1}$. The eigenvalues of this inverse matrix are the reciprocals of the eigenvalues of $(A - \\sigma I)$, which are $\\{(\\lambda_i - \\sigma)^{-1}\\}$. The power method, when applied to a matrix, iteratively converges to the eigenvector associated with the eigenvalue of largest magnitude. Therefore, applying the power method to $(A - \\sigma I)^{-1}$ will converge to the eigenvector corresponding to the eigenvalue $(\\lambda_j - \\sigma)^{-1}$ that has the largest absolute value. This occurs when the denominator $|\\lambda_j - \\sigma|$ is minimized. Consequently, this procedure isolates the eigenvector $v_j$ corresponding to the eigenvalue $\\lambda_j$ of the original matrix $A$ that is closest to the shift $\\sigma$. This is the principle of the shift-and-invert or shifted inverse iteration method.\n\nThe iterative process is given by $x^{(k)} = c_k (A - \\sigma I)^{-1} x^{(k-1)}$, where $c_k$ is a normalization constant. In practice, calculating the matrix inverse is computationally expensive and numerically unstable. It is far more efficient to solve the equivalent linear system of equations:\n$$\n(A - \\sigma I) y^{(k)} = x^{(k-1)}\n$$\nThe new iterate is then obtained by normalizing the solution vector $y^{(k)}$:\n$$\nx^{(k)} = \\frac{y^{(k)}}{\\|y^{(k)}\\|_2}\n$$\nTo solve the linear system efficiently across multiple iterations, the coefficient matrix, let us call it $M = A - \\sigma I$, is factored only once. The problem specifies the use of LU factorization. This decomposition finds a lower triangular matrix $L$, an upper triangular matrix $U$, and a permutation matrix $P$ such that $PM = LU$. The linear system $My^{(k)} = x^{(k-1)}$ is then rewritten as $P^{-1}LU y^{(k)} = x^{(k-1)}$, or $LU y^{(k)} = P x^{(k-1)}$. This is solved in two stages: first, forward substitution for $z$ in $Lz = P x^{(k-1)}$, and second, backward substitution for $y^{(k)}$ in $Uy^{(k)} = z$. This two-step process is significantly faster than re-solving the system from scratch at each iteration.\n\nA critical consideration is when the shift $\\sigma$ is very close or equal to an eigenvalue of $A$. In this case, the matrix $M = A - \\sigma I$ becomes ill-conditioned or singular, rendering the linear system unsolvable or its solution highly sensitive to errors. The problem mandates a specific, robust protocol to handle this. We must compute the condition number of $M$ in the two-norm, $\\kappa_2(M)$. If $\\kappa_2(M) \\ge 10^{12}$, the matrix is considered ill-conditioned. To regularize it, we introduce a small perturbation $\\delta > 0$ and use the matrix $M' = A - \\sigma I + \\delta I$. We must select the smallest $\\delta$ from the provided sequence $\\{0, 10^{-12}, 10^{-10}, 10^{-8}, 10^{-6}, 10^{-4}\\}$ that results in $\\kappa_2(M') < 10^{12}$. The LU factorization is then performed on this regularized matrix $M'$.\n\nThe algorithm proceeds as follows for each test case $(A, \\sigma)$:\n$1.$ Define the sequence of perturbations $\\Delta = \\{0, 10^{-12}, 10^{-10}, 10^{-8}, 10^{-6}, 10^{-4}\\}$.\n$2.$ For each $\\delta \\in \\Delta$, form the matrix $M = A - \\sigma I + \\delta I$. Compute its condition number $\\kappa_2(M)$. The first $\\delta$ for which $\\kappa_2(M) < 10^{12}$ is chosen, and this matrix $M$ is used for the iteration. If no such $\\delta$ exists, the last one, $\\delta = 10^{-4}$, is used.\n$3.$ Compute the LU factorization of the chosen matrix $M$.\n$4.$ Initialize the iteration vector $x^{(0)}$ as the vector of all ones, normalized to have a Euclidean norm of $1$.\n$5.$ For $k = 1, 2, \\ldots, k_{\\max}$:\n    a. Solve the linear system $M y^{(k)} = x^{(k-1)}$ using the pre-computed LU factorization.\n    b. Normalize the resulting vector: $x^{(k)} = y^{(k)} / \\| y^{(k)} \\|_2$.\n    c. Compute the eigenvalue estimate using the Rayleigh quotient: $\\mu^{(k)} = (x^{(k)})^\\top A x^{(k)}$.\n    d. Calculate the residual norm: $r^{(k)} = \\| A x^{(k)} - \\mu^{(k)} x^{(k)} \\|_2$.\n    e. Check for convergence: if $r^{(k)} \\le 10^{-10}$, the iteration terminates.\n$6.$ The iteration also terminates if the maximum number of iterations, $k_{\\max} = 100$, is reached.\n$7.$ The final eigenvalue estimate $\\mu^{(*)}$ is the last computed value of $\\mu^{(k)}$, which is then rounded to six decimal places for the output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Computes an approximate eigenpair for several matrices using inverse iteration with a real shift.\n    - An eigenpair (lambda, v) satisfies A v = lambda v.\n    - Inverse iteration with shift sigma finds the eigenvalue of A closest to sigma.\n    - The method iteratively solves (A - sigma I) y_k = x_{k-1} and normalizes x_k = y_k / ||y_k||.\n    - LU factorization is used for efficient solving of the linear system.\n    - Ill-conditioning is handled by perturbing the matrix with a small delta.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (symmetric tridiagonal)\n        (np.array([[4, 1, 0], [1, 3, 1], [0, 1, 2]], dtype=float), 3.2),\n        # Case 2 (upper triangular, non-symmetric; exact-shift singularity)\n        (np.array([[1, 2, 0], [0, 3, 1], [0, 0, 5]], dtype=float), 3.0),\n        # Case 3 (diagonal, clustered eigenvalues near the shift)\n        (np.diag([2.0, 2.1, 10.0]), 2.08),\n        # Case 4 (symmetric dense)\n        (np.array([[6, 2, 1], [2, 3, 1], [1, 1, 1]], dtype=float), 6.5)\n    ]\n\n    results = []\n    \n    # Constants defined in the problem\n    k_max = 100\n    tau = 1e-10\n    cond_threshold = 1e12\n    deltas = [0, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4]\n\n    for A, sigma in test_cases:\n        n = A.shape[0]\n        I = np.identity(n)\n        \n        # Step 1: Select coefficient matrix M by handling ill-conditioning\n        M = None\n        chosen_delta = None\n        \n        for delta in deltas:\n            M_candidate = A - sigma * I + delta * I\n            # The problem specifies condition number in the two-norm\n            cond_num = np.linalg.cond(M_candidate, p=2)\n            \n            if cond_num < cond_threshold:\n                M = M_candidate\n                chosen_delta = delta\n                break\n        \n        # If no delta satisfied the condition, use the last one\n        if M is None:\n            delta = deltas[-1]\n            M = A - sigma * I + delta * I\n            chosen_delta = delta\n            \n        # Step 2: Perform LU factorization once\n        lu_factor = linalg.lu_factor(M)\n        \n        # Step 3: Initialize the vector x_0\n        x = np.ones(n, dtype=float)\n        x = x / np.linalg.norm(x, 2)\n        \n        mu_final = 0.0\n        \n        # Step 4: Inverse Iteration Loop\n        for k in range(k_max):\n            # Solve (A - sigma*I + delta*I) y_k = x_{k-1} using LU decomposition\n            y = linalg.lu_solve(lu_factor, x)\n            \n            # Normalize to get the next iterate x_k\n            x = y / np.linalg.norm(y, 2)\n            \n            # Compute Rayleigh quotient as eigenvalue estimate\n            mu = (x.T @ A @ x)\n            mu_final = mu\n            \n            # Compute residual norm for convergence check\n            residual_norm = np.linalg.norm(A @ x - mu * x, 2)\n            \n            # Termination condition\n            if residual_norm <= tau:\n                break\n        \n        results.append(round(mu_final, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2442752"}, {"introduction": "Having explored a method for computing eigenvalues, we now turn to their interpretive power in computational engineering. The stability of numerical simulations, such as those solving differential equations over time, is not guaranteed and depends critically on the chosen parameters. This practice [@problem_id:2442744] uses eigenvalue analysis of the system's amplification matrix to provide a rigorous prediction of whether a simulation will remain stable or diverge, connecting an abstract mathematical property to a concrete computational outcome.", "problem": "Consider the linear autonomous system $ \\dot{\\mathbf{u}}(t) = A \\mathbf{u}(t) $ where $ A \\in \\mathbb{C}^{n \\times n} $. The explicit forward Euler time-stepping scheme with step size $ \\Delta t \\in \\mathbb{R}_{>0} $ yields the recurrence $ \\mathbf{u}^{k+1} = G \\mathbf{u}^{k} $ with amplification matrix\n$$\nG = I + \\Delta t \\, A,\n$$\nwhere $ I $ is the identity matrix of size $ n $. Let $ \\{ \\lambda_i(G) \\}_{i=1}^n $ denote the eigenvalues of $ G $, and define the spectral radius\n$$\n\\rho(G) = \\max_{1 \\le i \\le n} |\\lambda_i(G)|.\n$$\n\nAdopt the following mathematically precise classification for linear instability of the explicit scheme:\n- Declare the scheme \"unstable\" if $ \\rho(G) > 1 $.\n- If $ \\rho(G) = 1 $, declare the scheme \"unstable\" if and only if $ G $ is not diagonalizable over $ \\mathbb{C} $.\n- Otherwise, declare the scheme \"not unstable.\"\n\nAll decisions involving equality to $ 1 $ and diagonalizability must account for finite precision using the tolerance $ \\varepsilon = 10^{-10} $, as follows:\n- Treat a real number $ x $ as equal to $ 1 $ if $ |x - 1| \\le \\varepsilon $.\n- Treat $ \\rho(G) > 1 $ if $ \\rho(G) \\ge 1 + \\varepsilon $.\n- Treat $ \\rho(G) < 1 $ if $ \\rho(G) \\le 1 - \\varepsilon $.\n- Treat $ G $ as diagonalizable over $ \\mathbb{C} $ if and only if the eigenvector matrix has full column rank $ n $ (rank test evaluated with tolerance $ \\varepsilon $).\n\nFor each test case below, compute:\n$ (i) $ the spectral radius $ \\rho(G) $,\n$ (ii) $ a boolean flag indicating whether the scheme is unstable according to the above criteria, and\n$ (iii) $ a boolean flag indicating whether $ G $ is diagonalizable over $ \\mathbb{C} $.\n\nTest suite (each case is a pair $ (A, \\Delta t) $ with $ A \\in \\mathbb{R}^{2 \\times 2} $ and $ \\Delta t \\in \\mathbb{R}_{>0} $):\n- Case $ 1 $:\n$$\nA_1 = \\begin{bmatrix} -2 & 0 \\\\ 0 & -5 \\end{bmatrix}, \\quad \\Delta t_1 = 0.2.\n$$\n- Case $ 2 $:\n$$\nA_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad \\Delta t_2 = 0.6.\n$$\n- Case $ 3 $:\n$$\nA_3 = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}, \\quad \\Delta t_3 = 0.5.\n$$\n- Case $ 4 $:\n$$\nA_4 = \\begin{bmatrix} -4 & 0 \\\\ 0 & 0 \\end{bmatrix}, \\quad \\Delta t_4 = 0.5.\n$$\n\nFinal output format:\nYour program must produce a single line of output containing a list of results for the $ 4 $ test cases, in order. Each test case result must be a list $ [\\rho(G),\\ \\text{is\\_unstable},\\ \\text{is\\_diagonalizable}] $. The entire output must be a single bracketed list of these $ 4 $ lists, printed on one line without additional text. For example, an output with $ 2 $ cases would look like:\n[[1.0,False,True],[1.2,True,True]]", "solution": "The problem requires the stability analysis of the explicit forward Euler method for a system of linear ordinary differential equations $ \\dot{\\mathbf{u}}(t) = A \\mathbf{u}(t) $. The analysis is to be performed for several given test cases.\n\nThe forward Euler scheme approximates the time derivative by a finite difference, $ \\dot{\\mathbf{u}}(t) \\approx (\\mathbf{u}^{k+1} - \\mathbf{u}^{k}) / \\Delta t $, where $ \\mathbf{u}^k $ represents the numerical solution at time $ t_k = k \\Delta t $ and $ \\Delta t $ is the time step size. Substituting this into the governing differential equation yields the recurrence relation:\n$$ \\frac{\\mathbf{u}^{k+1} - \\mathbf{u}^{k}}{\\Delta t} = A \\mathbf{u}^{k} $$\n$$ \\mathbf{u}^{k+1} = \\mathbf{u}^{k} + \\Delta t A \\mathbf{u}^{k} = (I + \\Delta t A) \\mathbf{u}^{k} $$\nThe matrix $ G = I + \\Delta t A $ is the amplification matrix, which maps the solution at step $ k $ to step $ k+1 $. The stability of the numerical scheme is determined by the properties of this matrix $ G $.\n\nThe behavior of the solution as $ k \\to \\infty $ is governed by the eigenvalues of $ G $. If $ \\left\\{ \\mu_j \\right\\}_{j=1}^n $ are the eigenvalues of the matrix $ A $, then the eigenvalues of $ G $, denoted $ \\left\\{ \\lambda_j \\right\\}_{j=1}^n $, are given by the relation $ \\lambda_j = 1 + \\Delta t \\, \\mu_j $. The stability of the scheme depends on the magnitude of these eigenvalues.\n\nThe problem specifies a set of precise criteria for classifying the scheme's stability, based on the spectral radius of $ G $, $ \\rho(G) = \\max_j |\\lambda_j(G)| $, and its diagonalizability. A numerical tolerance of $ \\varepsilon = 10^{-10} $ is mandated for all floating-point comparisons.\n\nThe stability criteria are as follows:\n1.  If $ \\rho(G) \\ge 1 + \\varepsilon $, the scheme is declared \"unstable\". At least one eigenvalue lies outside the unit circle in the complex plane, leading to an exponentially growing numerical solution.\n2.  If $ |\\rho(G) - 1| \\le \\varepsilon $, the stability depends on the structure of the eigenspace of $ G $.\n    - If $ G $ is not diagonalizable, it possesses at least one Jordan block of size greater than $1$ for an eigenvalue with magnitude $1$. This results in secular growth (e.g., of the form $ c k \\lambda^k $), and the scheme is \"unstable\".\n    - If $ G $ is diagonalizable, all modes with eigenvalues of magnitude $1$ are non-growing, and the scheme is \"not unstable\".\n3.  If $ \\rho(G) \\le 1 - \\varepsilon $, all eigenvalues lie strictly inside the unit circle, ensuring that the numerical solution decays to zero. The scheme is \"not unstable\".\n\nA matrix $ G \\in \\mathbb{C}^{n \\times n} $ is diagonalizable if and only if it possesses a set of $ n $ linearly independent eigenvectors. This is equivalent to its matrix of eigenvectors, $V$, having full rank, i.e., $ \\text{rank}(V) = n $. Numerically, the rank is robustly computed by counting the number of singular values of $V$ that are greater than the given tolerance $ \\varepsilon $.\n\nWe now analyze each test case based on these principles.\n\n**Case 1:** $ A_1 = \\begin{bmatrix} -2 & 0 \\\\ 0 & -5 \\end{bmatrix} $, $ \\Delta t_1 = 0.2 $.\n- The amplification matrix is $ G_1 = I + 0.2 A_1 = \\begin{bmatrix} 1+0.2(-2) & 0 \\\\ 0 & 1+0.2(-5) \\end{bmatrix} = \\begin{bmatrix} 0.6 & 0 \\\\ 0 & 0 \\end{bmatrix} $.\n- The eigenvalues of this diagonal matrix are visible on the diagonal: $ \\lambda_1 = 0.6 $ and $ \\lambda_2 = 0 $.\n- The spectral radius is $ \\rho(G_1) = \\max(|0.6|, |0|) = 0.6 $.\n- Since $ 0.6 \\le 1 - \\varepsilon $, the scheme is \"not unstable\".\n- $G_1$ is a diagonal matrix, and all diagonal matrices are diagonalizable.\n- Result: $ [\\rho(G_1) = 0.6, \\text{is\\_unstable} = \\text{False}, \\text{is\\_diagonalizable} = \\text{True}] $.\n\n**Case 2:** $ A_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} $, $ \\Delta t_2 = 0.6 $.\n- The amplification matrix is $ G_2 = I + 0.6 A_2 = \\begin{bmatrix} 1 & 0.6 \\\\ 0.6 & 1 \\end{bmatrix} $.\n- The characteristic equation is $ \\det(G_2 - \\lambda I) = (1-\\lambda)^2 - 0.6^2 = 0 $, which gives eigenvalues $ \\lambda = 1 \\pm 0.6 $. Thus, $ \\lambda_1 = 1.6 $ and $ \\lambda_2 = 0.4 $.\n- The spectral radius is $ \\rho(G_2) = \\max(|1.6|, |0.4|) = 1.6 $.\n- Since $ 1.6 \\ge 1 + \\varepsilon $, the scheme is \"unstable\".\n- $G_2$ is a real symmetric matrix, which guarantees that it is diagonalizable over $ \\mathbb{R} $, and therefore over $ \\mathbb{C} $.\n- Result: $ [\\rho(G_2) = 1.6, \\text{is\\_unstable} = \\text{True}, \\text{is\\_diagonalizable} = \\text{True}] $.\n\n**Case 3:** $ A_3 = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} $, $ \\Delta t_3 = 0.5 $.\n- The amplification matrix is $ G_3 = I + 0.5 A_3 = \\begin{bmatrix} 1 & 0.5 \\\\ 0 & 1 \\end{bmatrix} $.\n- This matrix is upper triangular, so its eigenvalues are its diagonal entries: $ \\lambda_1 = \\lambda_2 = 1 $.\n- The spectral radius is $ \\rho(G_3) = |1| = 1.0 $.\n- We now assess diagonalizability. The eigenspace for $ \\lambda=1 $ is the null space of $ G_3 - 1I = \\begin{bmatrix} 0 & 0.5 \\\\ 0 & 0 \\end{bmatrix} $. The equation $ (G_3 - I)\\mathbf{v} = \\mathbf{0} $ requires $ 0.5 v_2 = 0 $. This implies the eigenspace is spanned by the single vector $ [1, 0]^T $. The geometric multiplicity ($1$) is less than the algebraic multiplicity ($2$), so the matrix $ G_3 $ is not diagonalizable.\n- According to the criteria, since $ \\rho(G_3) = 1 $ and $ G_3 $ is not diagonalizable, the scheme is \"unstable\".\n- Result: $ [\\rho(G_3) = 1.0, \\text{is\\_unstable} = \\text{True}, \\text{is\\_diagonalizable} = \\text{False}] $.\n\n**Case 4:** $ A_4 = \\begin{bmatrix} -4 & 0 \\\\ 0 & 0 \\end{bmatrix} $, $ \\Delta t_4 = 0.5 $.\n- The amplification matrix is $ G_4 = I + 0.5 A_4 = \\begin{bmatrix} 1+0.5(-4) & 0 \\\\ 0 & 1+0.5(0) \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix} $.\n- The eigenvalues are $ \\lambda_1 = -1 $ and $ \\lambda_2 = 1 $.\n- The spectral radius is $ \\rho(G_4) = \\max(|-1|, |1|) = 1.0 $.\n- $G_4$ is a diagonal matrix and is therefore diagonalizable.\n- Since $ \\rho(G_4) = 1 $ and $ G_4 $ is diagonalizable, the scheme is \"not unstable\".\n- Result: $ [\\rho(G_4) = 1.0, \\text{is\\_unstable} = \\text{False}, \\text{is\\_diagonalizable} = \\text{True}] $.\n\nThe results are computed algorithmically based on these derivations.", "answer": "```python\nimport numpy as np\n\ndef analyze_stability(A, dt, epsilon):\n    \"\"\"\n    Analyzes the stability of the forward Euler scheme for a given matrix A and time step dt.\n\n    Args:\n        A (np.ndarray): The matrix from the ODE system.\n        dt (float): The time step size.\n        epsilon (float): The tolerance for floating-point comparisons.\n\n    Returns:\n        list: A list containing [rho(G), is_unstable, is_diagonalizable].\n    \"\"\"\n    n = A.shape[0]\n    I = np.identity(n)\n    G = I + dt * A\n\n    # (i) Compute eigenvalues of G and its spectral radius\n    eigenvalues_G = np.linalg.eigvals(G)\n    rho_G = np.max(np.abs(eigenvalues_G))\n\n    # (iii) Check diagonalizability of G\n    # A matrix is diagonalizable if and only if it has a full set of n linearly independent eigenvectors.\n    # We compute the eigenvector matrix V and check its rank.\n    # The rank is determined by counting singular values greater than the tolerance.\n    _, V = np.linalg.eig(G)\n    singular_values = np.linalg.svd(V, compute_uv=False)\n    rank_V = np.sum(singular_values > epsilon)\n    is_diagonalizable = (rank_V == n)\n\n    # (ii) Determine instability based on the given criteria\n    is_unstable = False\n    if rho_G >= 1 + epsilon:\n        is_unstable = True\n    elif np.abs(rho_G - 1) <= epsilon:\n        if not is_diagonalizable:\n            is_unstable = True\n    # Otherwise, rho_G is <= 1 - epsilon, for which the scheme is \"not unstable\".\n\n    return [rho_G, is_unstable, is_diagonalizable]\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    epsilon = 1e-10\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        (np.array([[-2.0, 0.0], [0.0, -5.0]]), 0.2), # Case 1\n        (np.array([[0.0, 1.0], [1.0, 0.0]]), 0.6),   # Case 2\n        (np.array([[0.0, 1.0], [0.0, 0.0]]), 0.5),   # Case 3\n        (np.array([[-4.0, 0.0], [0.0, 0.0]]), 0.5),   # Case 4\n    ]\n\n    results = []\n    for A, dt in test_cases:\n        result = analyze_stability(A, dt, epsilon)\n        results.append(result)\n\n    # Format the output string as required by the problem.\n    # e.g., [[rho1,False,True],[rho2,True,True]]\n    # Python's str() for booleans produces capitalized True/False, which matches the example.\n    # The list-to-string conversion with f-strings and join creates the exact format.\n    inner_lists_str = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "2442744"}, {"introduction": "In our final practice, we advance from computing and interpreting eigenvalues to actively using the spectral decomposition to manipulate a system's state. In fields like structural dynamics, the eigenvectors represent fundamental modes of vibration, and it is often desirable to filter out unwanted modes from a system's response. This exercise [@problem_id:2442802] guides you through the design of a 'spectral filter' based on projection in a generalized inner product space, a technique with direct applications in vibration control and system design.", "problem": "A finite-dimensional, undamped mechanical system with symmetric positive definite (SPD) mass matrix $M \\in \\mathbb{R}^{n \\times n}$ and stiffness matrix $K \\in \\mathbb{R}^{n \\times n}$ exhibits small free vibrations governed by the algebraic generalized eigenvalue problem: find nonzero $v \\in \\mathbb{R}^{n}$ and scalar $\\lambda \\in \\mathbb{R}$ such that $K v = \\lambda M v$. In computational engineering, the vibrational modes are the eigenvectors of this problem, and a spectral decomposition can be carried out using an $M$-orthonormal basis of eigenvectors.\n\nYour task is to design a spectral filter that removes a prescribed subset of vibrational modes by projection in the $M$-inner product, using only fundamental definitions from linear algebra:\n\n- The $M$-inner product of vectors $x,y \\in \\mathbb{R}^{n}$ is $\\langle x, y \\rangle_{M} = x^{\\mathsf{T}} M y$.\n- A set of eigenvectors $\\{v_{i}\\}_{i=1}^{n}$ can be chosen $M$-orthonormal so that $v_{i}^{\\mathsf{T}} M v_{j} = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n- A projection operator onto a subspace, with respect to an inner product, is linear, idempotent (i.e., $P^{2} = P$), and leaves vectors in the target subspace invariant while annihilating vectors in its orthogonal complement (defined via that inner product).\n\nStarting from these definitions (and without introducing any unproven shortcut formulas), derive the linear operator that acts as the spectral filter which removes the components of a vector along a selected subset of eigenvectors (with respect to the $M$-inner product) and leaves untouched the components in the $M$-orthogonal complement of that subset. Then implement it algorithmically by computing the generalized eigen-decomposition, selecting the appropriate eigenvectors, and applying your derived projection.\n\nUse the following specific, dimensionless matrices and data for a fully specified and testable implementation:\n\n- Dimension $n = 4$.\n- Mass matrix\n$$\nM = \\begin{bmatrix}\n2 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1.5 & 0 \\\\\n0 & 0 & 0 & 1.2\n\\end{bmatrix}.\n$$\n- Stiffness matrix\n$$\nK = \\begin{bmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{bmatrix}.\n$$\n- Let the generalized eigenpairs $(\\lambda_{i}, v_{i})$ be ordered by ascending eigenvalue $\\lambda_{i}$, with indices $i \\in \\{0,1,2,3\\}$. Ensure your eigenvectors are scaled so that $v_{i}^{\\mathsf{T}} M v_{i} = 1$ for all $i$ and $v_{i}^{\\mathsf{T}} M v_{j} = 0$ for $i \\neq j$.\n\nFor each test case below, you must:\n- Construct the spectral filter that removes the set of modes indicated by the index set $S$.\n- Apply the filter to the given input vector $u$ to obtain the filtered vector $\\hat{u}$.\n- Compute the requested scalar diagnostic.\n\nTest suite (covering a typical case, mixed-mode removal, and two edge cases):\n\n- Test case $1$: $u = \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$, $S = \\{0\\}$. Output the scalar\n$$\nr_{1} = \\max_{i \\in S} \\left| \\alpha_{i} \\right|, \\quad \\text{where } \\alpha = V^{\\mathsf{T}} M \\hat{u} \\text{ are the modal coordinates after filtering and } V = [v_{0}\\ v_{1}\\ v_{2}\\ v_{3}].\n$$\n- Test case $2$: $u = \\begin{bmatrix}1 \\\\ 2 \\\\ -1 \\\\ 0.5\\end{bmatrix}$, $S = \\{0,2\\}$. Output the scalar\n$$\nr_{2} = \\max_{i \\in S} \\left| \\alpha_{i} \\right| \\text{ computed as above}.\n$$\n- Test case $3$ (boundary: remove none): $u = \\begin{bmatrix}-0.2 \\\\ 0.4 \\\\ 0.6 \\\\ -0.8\\end{bmatrix}$, $S = \\varnothing$. Output the scalar\n$$\nr_{3} = \\left\\| \\hat{u} - u \\right\\|_{2}.\n$$\n- Test case $4$ (boundary: remove all): $u = \\begin{bmatrix}3 \\\\ -1 \\\\ 0.5 \\\\ 2\\end{bmatrix}$, $S = \\{0,1,2,3\\}$. Output the scalar\n$$\nr_{4} = \\left\\| \\hat{u} \\right\\|_{2}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4}]$). All computations are dimensionless; no physical unit reporting is required. The final numeric answers for $r_{1}$, $r_{2}$, $r_{3}$, and $r_{4}$ must be floating-point numbers computed by your program. The design must be general and based strictly on the above fundamental principles and definitions, not on any pre-supplied projection formulas in this statement.", "solution": "The problem statement has been rigorously evaluated and is deemed to be valid. It is scientifically grounded, well-posed, and contains all necessary information for a unique and meaningful solution. We shall proceed with the derivation and implementation.\n\nThe fundamental context is a finite-dimensional vector space $\\mathbb{R}^n$ equipped with an inner product defined by the symmetric positive definite (SPD) mass matrix $M$. This is the $M$-inner product, given by $\\langle x, y \\rangle_M = x^{\\mathsf{T}} M y$ for any vectors $x,y \\in \\mathbb{R}^n$.\n\nThe generalized eigenvalue problem $K v = \\lambda M v$ with symmetric matrix $K$ and SPD matrix $M$ guarantees the existence of $n$ real eigenvalues $\\lambda_i$ and a corresponding set of eigenvectors $\\{v_i\\}_{i=0}^{n-1}$. These eigenvectors can be chosen to form an $M$-orthonormal basis for $\\mathbb{R}^n$. This means they satisfy the condition $\\langle v_i, v_j \\rangle_M = v_i^{\\mathsf{T}} M v_j = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nAny vector $u \\in \\mathbb{R}^n$ can be expressed uniquely as a linear combination of these basis vectors:\n$$\nu = \\sum_{i=0}^{n-1} \\alpha_i v_i\n$$\nThe coefficients $\\alpha_i$, known as the modal coordinates of $u$, are determined by projecting $u$ onto each basis vector using the $M$-inner product. Taking the $M$-inner product of the equation above with a basis vector $v_j$:\n$$\n\\langle u, v_j \\rangle_M = \\left\\langle \\sum_{i=0}^{n-1} \\alpha_i v_i, v_j \\right\\rangle_M\n$$\nBy the linearity of the inner product, this becomes:\n$$\n\\langle u, v_j \\rangle_M = \\sum_{i=0}^{n-1} \\alpha_i \\langle v_i, v_j \\rangle_M = \\sum_{i=0}^{n-1} \\alpha_i \\delta_{ij} = \\alpha_j\n$$\nThus, the $j$-th modal coordinate is $\\alpha_j = \\langle u, v_j \\rangle_M = u^{\\mathsf{T}} M v_j$.\n\nThe task is to design a spectral filter that removes the components of a vector $u$ corresponding to a prescribed set of modes, indexed by the set $S$. Let $V_S$ be the subspace spanned by the eigenvectors $\\{v_i\\}_{i \\in S}$ that are to be removed, and let $V_{S^\\perp}$ be its $M$-orthogonal complement, spanned by the eigenvectors $\\{v_j\\}_{j \\notin S}$ that are to be kept.\n$$\nV_S = \\text{span}\\{v_i \\mid i \\in S\\}\n$$\n$$\nV_{S^\\perp} = \\text{span}\\{v_j \\mid j \\notin S\\}\n$$\nThe space $\\mathbb{R}^n$ is the direct sum of these two subspaces, $\\mathbb{R}^n = V_S \\oplus V_{S^\\perp}$. Consequently, any vector $u$ can be uniquely decomposed as $u = u_S + u_{S^\\perp}$, where $u_S \\in V_S$ and $u_{S^\\perp} \\in V_{S^\\perp}$. From the modal expansion, these components are:\n$$\nu_S = \\sum_{i \\in S} \\alpha_i v_i \\quad \\text{and} \\quad u_{S^\\perp} = \\sum_{j \\notin S} \\alpha_j v_j\n$$\nThe problem requires that the filter \"removes\" the components in $V_S$ and \"leaves untouched\" the components in $V_{S^\\perp}$. This is precisely the definition of a projection onto the subspace $V_{S^\\perp}$ with respect to the $M$-inner product. Let this projection operator be denoted by $P$. The filtered vector, $\\hat{u}$, is therefore given by $\\hat{u} = P(u)$.\n\nBy definition, the projection of $u$ onto $V_{S^\\perp}$ is the vector $\\hat{u} \\in V_{S^\\perp}$ such that the error vector, $u - \\hat{u}$, is $M$-orthogonal to every vector in $V_{S^\\perp}$.\nSince $\\hat{u} \\in V_{S^\\perp}$, it has an expansion of the form $\\hat{u} = \\sum_{j \\notin S} \\beta_j v_j$ for some coefficients $\\beta_j$.\nThe orthogonality condition is $\\langle u - \\hat{u}, v_k \\rangle_M = 0$ for all $k \\notin S$.\nSubstituting the expansions for $u$ and $\\hat{u}$:\n$$\n\\left\\langle \\sum_{i=0}^{n-1} \\alpha_i v_i - \\sum_{j \\notin S} \\beta_j v_j, v_k \\right\\rangle_M = 0 \\quad \\text{for } k \\notin S\n$$\nUsing linearity and $M$-orthonormality:\n$$\n\\alpha_k - \\beta_k = 0 \\implies \\beta_k = \\alpha_k \\quad \\text{for all } k \\notin S\n$$\nThis demonstrates that the coefficients of the projected vector in the basis $\\{v_j\\}_{j \\notin S}$ are simply the original modal coordinates $\\alpha_j$. Therefore, the filtered vector is:\n$$\n\\hat{u} = \\sum_{j \\notin S} \\alpha_j v_j\n$$\nTo construct the operator $P$ such that $\\hat{u} = P u$, we substitute the expression for $\\alpha_j$:\n$$\n\\hat{u} = \\sum_{j \\notin S} (v_j^{\\mathsf{T}} M u) v_j = \\sum_{j \\notin S} v_j (v_j^{\\mathsf{T}} M u) = \\left( \\sum_{j \\notin S} v_j v_j^{\\mathsf{T}} M \\right) u\n$$\nThe explicit matrix form of the spectral filtering operator is thus:\n$$\nP = \\sum_{j \\notin S} v_j v_j^{\\mathsf{T}} M\n$$\nThis operator is linear, idempotent ($P^2=P$), and projects any vector onto the subspace $V_{S^\\perp}$ along its $M$-orthogonal complement $V_S$, as required.\n\nAlgorithmically, forming and applying the matrix $P$ is inefficient. A more direct procedure is:\n1.  Solve the generalized eigenvalue problem $K v = \\lambda M v$ to find the eigenvalues $\\lambda_i$ and the matrix of $M$-orthonormal eigenvectors $V = [v_0, v_1, \\dots, v_{n-1}]$.\n2.  For a given input vector $u$, compute its full vector of modal coordinates: $\\alpha = V^{\\mathsf{T}} M u$.\n3.  Construct the filtered modal coordinate vector $\\hat{\\alpha}$ by setting to zero the coordinates corresponding to the modes to be removed: $\\hat{\\alpha}_i = 0$ if $i \\in S$, and $\\hat{\\alpha}_i = \\alpha_i$ if $i \\notin S$.\n4.  Reconstruct the filtered vector in the original basis by multiplying by the eigenvector matrix: $\\hat{u} = V \\hat{\\alpha}$.\n\nApplying this logic to the specific test cases:\n- For Test Cases $1$ and $2$, we filter the vector $u$ to get $\\hat{u}$. The diagnostic is $r = \\max_{i \\in S} |(V^{\\mathsf{T}} M \\hat{u})_i|$. Since $V^{\\mathsf{T}} M \\hat{u} = V^{\\mathsf{T}} M (V \\hat{\\alpha}) = (V^{\\mathsf{T}} M V) \\hat{\\alpha} = I \\hat{\\alpha} = \\hat{\\alpha}$, and we have explicitly set $\\hat{\\alpha}_i = 0$ for all $i \\in S$, the diagnostic must be $0$.\n- For Test Case $3$, $S = \\varnothing$. No modes are removed. The operator $P$ is the identity operator, so $\\hat{u} = u$. The diagnostic $r_3 = \\|\\hat{u} - u\\|_2$ must therefore be $0$.\n- For Test Case $4$, $S = \\{0, 1, 2, 3\\}$. All modes are removed. The operator $P$ is the zero operator, so $\\hat{u} = 0$. The diagnostic $r_4 = \\|\\hat{u}\\|_2$ must be $0$.\n\nThe implementation will demonstrate these theoretical consequences. The numerical results should be zero, or a value on the order of machine precision due to floating-point arithmetic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Solves the spectral filtering problem by deriving and applying a projection operator.\n    \"\"\"\n    # Define problem parameters for n=4 as specified.\n    M = np.array([\n        [2.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.5, 0.0],\n        [0.0, 0.0, 0.0, 1.2]\n    ])\n\n    K = np.array([\n        [2.0, -1.0, 0.0, 0.0],\n        [-1.0, 2.0, -1.0, 0.0],\n        [0.0, -1.0, 2.0, -1.0],\n        [0.0, 0.0, -1.0, 2.0]\n    ])\n\n    # Define the test suite.\n    test_cases = [\n        {'u': np.array([1.0, 0.0, 0.0, 0.0]), 'S': {0}},\n        {'u': np.array([1.0, 2.0, -1.0, 0.5]), 'S': {0, 2}},\n        {'u': np.array([-0.2, 0.4, 0.6, -0.8]), 'S': set()},\n        {'u': np.array([3.0, -1.0, 0.5, 2.0]), 'S': {0, 1, 2, 3}},\n    ]\n\n    # Step 1: Solve the generalized eigenvalue problem Kv = lambda*M*v.\n    # eigh returns eigenvalues in ascending order and M-orthonormal eigenvectors.\n    # That is, for eigenvectors V, the condition V.T @ M @ V = I holds.\n    eigenvalues, V = eigh(K, b=M)\n    \n    results = []\n\n    for i, case in enumerate(test_cases):\n        u = case['u']\n        S = case['S']\n        \n        # Step 2: Compute the full vector of modal coordinates.\n        alpha = V.T @ M @ u\n        \n        # Step 3: Filter by setting modal coordinates in S to zero.\n        alpha_hat = alpha.copy()\n        if S: # If S is not empty\n            indices_to_remove = list(S)\n            alpha_hat[indices_to_remove] = 0.0\n            \n        # Step 4: Reconstruct the filtered vector.\n        u_hat = V @ alpha_hat\n\n        # Compute the requested scalar diagnostic for each test case.\n        if i == 0 or i == 1: # Test cases 1 and 2\n            # Diagnostic: max(|alpha_i|) for i in S, where alpha are modal coordinates of u_hat.\n            # As derived, modal coordinates of u_hat are alpha_hat.\n            alpha_post_filter = V.T @ M @ u_hat\n            indices_to_check = list(S)\n            result = np.max(np.abs(alpha_post_filter[indices_to_check]))\n            results.append(result)\n\n        elif i == 2: # Test case 3\n            # Diagnostic: ||u_hat - u||_2\n            result = np.linalg.norm(u_hat - u)\n            results.append(result)\n            \n        elif i == 3: # Test case 4\n            # Diagnostic: ||u_hat||_2\n            result = np.linalg.norm(u_hat)\n            results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2442802"}]}