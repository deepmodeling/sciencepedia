{"hands_on_practices": [{"introduction": "A fundamental challenge in computational fields like fluid dynamics is managing the vast amount of data generated by simulations. This first exercise allows you to directly compare two data reduction techniques: a simple spatial averaging method and the more sophisticated Singular Value Decomposition (SVD). By implementing both and measuring their reconstruction accuracy, you will gain a practical understanding of why SVD is considered an optimal method for data compression, as it excels at capturing the most significant global features of the data [@problem_id:2371486].", "problem": "Consider a two-component velocity field from computational fluid dynamics (CFD), defined over a rectangular grid with $m$ rows and $n$ columns. Let the spatial domain be $[0,1] \\times [0,1]$, with grid point coordinates $x_j = \\frac{j}{n-1}$ for $j \\in \\{0,1,\\dots,n-1\\}$ and $y_i = \\frac{i}{m-1}$ for $i \\in \\{0,1,\\dots,m-1\\}$, with the convention that if $n=1$ then $x_0 = 0$ and if $m=1$ then $y_0 = 0$. Define the horizontal and vertical components of the velocity at each grid point by\n$$\nu(i,j) = \\sin\\!\\big(2\\pi x_j\\big)\\cos\\!\\big(2\\pi y_i\\big) + 0.3 \\cos\\!\\big(4\\pi x_j + 0.1\\big)\\sin\\!\\big(2\\pi y_i\\big) + 0.1\\, y_i,\n$$\n$$\nv(i,j) = -\\cos\\!\\big(2\\pi x_j\\big)\\sin\\!\\big(2\\pi y_i\\big) + 0.25 \\sin\\!\\big(2\\pi x_j\\big)\\sin\\!\\big(4\\pi y_i + 0.3\\big) + 0.1\\, x_j,\n$$\nfor all valid indices $(i,j)$. Angles in the trigonometric functions are in radians.\n\nAggregate the field into a single real matrix $M \\in \\mathbb{R}^{(2m)\\times n}$ by stacking the components vertically:\n$$\nM = \\begin{bmatrix} U \\\\ V \\end{bmatrix}, \\quad U_{i,j} = u(i,j), \\quad V_{i,j} = v(i,j).\n$$\n\nTwo data reduction strategies and their reconstructions are to be compared:\n\n1. Singular value decomposition (SVD) truncation of rank $r$: Let the singular value decomposition (SVD) of $M$ be $M = Q \\Sigma W^\\top$ with $Q \\in \\mathbb{R}^{(2m)\\times(2m)}$, $\\Sigma \\in \\mathbb{R}^{(2m)\\times n}$, and $W \\in \\mathbb{R}^{n\\times n}$. The rank-$r$ truncated reconstruction $M_r$ is\n$$\nM_r = Q_{[:,1:r]} \\,\\Sigma_{[1:r,1:r]} \\, W_{[:,1:r]}^\\top,\n$$\nwhere $r \\in \\mathbb{N}$ satisfies $1 \\le r \\le \\min(2m,n)$ and the notation $A_{[:,1:r]}$ selects the first $r$ columns and $A_{[1:r,1:r]}$ the leading $r\\times r$ principal submatrix.\n\n2. Spatial coarsening with block-averaged reconstruction: Given positive integers $s_y$ and $s_x$, partition the $m\\times n$ grids of $U$ and $V$ into non-overlapping blocks of size $s_y \\times s_x$, except possibly at the boundaries where blocks may be smaller. For each block, replace all entries in that block by the arithmetic mean of the entries originally in that block. Perform this operation independently on $U$ and $V$ to obtain $\\widehat{U}$ and $\\widehat{V}$, and define the coarsened reconstruction $\\widehat{M} = \\begin{bmatrix} \\widehat{U} \\\\ \\widehat{V} \\end{bmatrix}$.\n\nFor each reconstruction $\\widetilde{M} \\in \\{M_r, \\widehat{M}\\}$, define the relative reconstruction error using the Frobenius norm by\n$$\n\\varepsilon(\\widetilde{M}) = \\frac{\\lVert M - \\widetilde{M} \\rVert_F}{\\lVert M \\rVert_F}.\n$$\n\nYour task is to implement a program that, for each test case specified below, constructs $M$ from the given $(m,n)$, computes the SVD-truncated reconstruction $M_r$ of rank $r$, computes the spatially coarsened reconstruction $\\widehat{M}$ using block sizes $(s_y,s_x)$, evaluates the errors $\\varepsilon(M_r)$ and $\\varepsilon(\\widehat{M})$, and returns, for each test case, the single floating-point value\n$$\n\\Delta = \\varepsilon(\\widehat{M}) - \\varepsilon(M_r).\n$$\nA positive value of $\\Delta$ indicates that the rank-$r$ SVD reconstruction has a smaller relative error than the coarsened reconstruction for that test case.\n\nTest suite (angles in radians):\n- Case $1$: $(m,n,r,s_y,s_x) = (48,64,8,4,4)$.\n- Case $2$: $(m,n,r,s_y,s_x) = (32,30,1,64,64)$.\n- Case $3$: $(m,n,r,s_y,s_x) = (64,24,6,8,6)$.\n- Case $4$: $(m,n,r,s_y,s_x) = (24,96,5,6,8)$.\n- Case $5$: $(m,n,r,s_y,s_x) = (40,50,3,1,1)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets, ordered by the cases above, that is,\n$$\n[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5].\n$$\nNo additional text should be printed on any line. There are no physical units in this problem, and all angles are specified in radians. Each $\\Delta_k$ must be output as a floating-point number. The answer must be computed exactly as specified by the definitions above without introducing any alternative normalization or weighting.", "solution": "We formalize both reconstructions and the error metric from first principles. The singular value decomposition (SVD) of a real matrix $M \\in \\mathbb{R}^{(2m)\\times n}$ is a factorization $M = Q \\Sigma W^\\top$ with $Q \\in \\mathbb{R}^{(2m)\\times(2m)}$ and $W \\in \\mathbb{R}^{n\\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{(2m)\\times n}$ diagonal in the sense of having nonnegative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$ on its main diagonal (with possible zero padding off its square core). For any rank parameter $r$ satisfying $1 \\le r \\le \\min(2m,n)$, the rank-$r$ truncated reconstruction is\n$$\nM_r = \\sum_{k=1}^{r} \\sigma_k \\, q_k \\, w_k^\\top = Q_{[:,1:r]}\\,\\Sigma_{[1:r,1:r]}\\,W_{[:,1:r]}^\\top,\n$$\nwhere $q_k$ and $w_k$ are the $k$-th columns of $Q$ and $W$, respectively. By the Eckart–Young–Mirsky theorem, $M_r$ is a best approximation to $M$ among all matrices of rank at most $r$ with respect to the Frobenius norm. Therefore, its relative Frobenius error is\n$$\n\\varepsilon(M_r) = \\frac{\\left\\|M - M_r\\right\\|_F}{\\left\\|M\\right\\|_F} = \\frac{\\sqrt{\\sum_{k=r+1}^{\\rho} \\sigma_k^2}}{\\sqrt{\\sum_{k=1}^{\\rho} \\sigma_k^2}},\n$$\nwhere $\\rho = \\operatorname{rank}(M)$.\n\nThe spatial coarsening reconstruction is defined via a blockwise averaging operator. For given block sizes $(s_y,s_x)$ with $s_y \\in \\mathbb{N}$ and $s_x \\in \\mathbb{N}$, we partition the index set $\\{0,1,\\dots,m-1\\} \\times \\{0,1,\\dots,n-1\\}$ into Cartesian blocks\n$$\nB_{\\alpha,\\beta} = \\{(i,j) \\,\\mid\\, \\alpha s_y \\le i \\le \\min((\\alpha+1)s_y-1,m-1),\\ \\beta s_x \\le j \\le \\min((\\beta+1)s_x-1,n-1) \\},\n$$\nfor $\\alpha \\in \\{0,1,\\dots,\\lceil m/s_y \\rceil - 1\\}$ and $\\beta \\in \\{0,1,\\dots,\\lceil n/s_x \\rceil - 1\\}$. For any matrix $A \\in \\mathbb{R}^{m\\times n}$, define the piecewise-constant projection $\\mathcal{P}_{s_y,s_x}(A)$ by assigning, for each block $B_{\\alpha,\\beta}$, the value\n$$\n\\left(\\mathcal{P}_{s_y,s_x}(A)\\right)_{i,j} = \\frac{1}{|B_{\\alpha,\\beta}|}\\sum_{(p,q)\\in B_{\\alpha,\\beta}} A_{p,q} \\quad \\text{for all } (i,j)\\in B_{\\alpha,\\beta}.\n$$\nThis operator is the orthogonal projector (with respect to the Frobenius inner product) onto the subspace of matrices that are constant on each block $B_{\\alpha,\\beta}$. Applying this operator to each component yields $\\widehat{U}=\\mathcal{P}_{s_y,s_x}(U)$ and $\\widehat{V}=\\mathcal{P}_{s_y,s_x}(V)$, and thus the reconstructed stacked matrix $\\widehat{M} = \\begin{bmatrix}\\widehat{U} \\\\ \\widehat{V}\\end{bmatrix}$. The associated relative error is\n$$\n\\varepsilon(\\widehat{M}) = \\frac{\\left\\|M - \\widehat{M}\\right\\|_F}{\\left\\|M\\right\\|_F}.\n$$\n\nThe velocity field is prescribed deterministically by\n$$\nu(i,j) = \\sin(2\\pi x_j)\\cos(2\\pi y_i) + 0.3 \\cos(4\\pi x_j + 0.1)\\sin(2\\pi y_i) + 0.1\\, y_i,\n$$\n$$\nv(i,j) = -\\cos(2\\pi x_j)\\sin(2\\pi y_i) + 0.25 \\sin(2\\pi x_j)\\sin(4\\pi y_i + 0.3) + 0.1\\, x_j,\n$$\nwith $x_j = \\frac{j}{n-1}$ for $n>1$ (and $x_0=0$ if $n=1$) and $y_i = \\frac{i}{m-1}$ for $m>1$ (and $y_0=0$ if $m=1$). Constructing $U$ and $V$ as $m\\times n$ arrays and stacking yields $M \\in \\mathbb{R}^{(2m)\\times n}$.\n\nFor each test tuple $(m,n,r,s_y,s_x)$, the computational steps follow directly from these definitions:\n- Form $M$ from the specified $m$ and $n$ by evaluating $u(i,j)$ and $v(i,j)$ at the grid points and stacking.\n- Compute the rank-$r$ SVD truncation $M_r$ and the relative Frobenius error $\\varepsilon(M_r)$.\n- Compute the block-averaged reconstructions $\\widehat{U}$ and $\\widehat{V}$ with the given $(s_y,s_x)$, stack to $\\widehat{M}$, and compute $\\varepsilon(\\widehat{M})$.\n- Report the scalar difference $\\Delta = \\varepsilon(\\widehat{M}) - \\varepsilon(M_r)$.\n\nThe test suite covers a typical case with moderate block size and rank, a boundary case where block sizes exceed the domain (single-block averaging), a tall matrix case with $2m \\gg n$, a wide matrix case with $n \\gg 2m$, and an identity coarsening case with $(s_y,s_x)=(1,1)$ where coarsening error is exactly zero. The final output is a single list $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$ corresponding to the cases in order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_velocity_field(m: int, n: int):\n    # Coordinates in [0,1], handle degenerate sizes\n    if n > 1:\n        x = np.linspace(0.0, 1.0, n)\n    else:\n        x = np.array([0.0])\n    if m > 1:\n        y = np.linspace(0.0, 1.0, m)\n    else:\n        y = np.array([0.0])\n\n    X, Y = np.meshgrid(x, y, indexing='xy')\n\n    # Define u and v components as specified\n    u = np.sin(2.0 * np.pi * X) * np.cos(2.0 * np.pi * Y) \\\n        + 0.3 * np.cos(4.0 * np.pi * X + 0.1) * np.sin(2.0 * np.pi * Y) \\\n        + 0.1 * Y\n\n    v = -np.cos(2.0 * np.pi * X) * np.sin(2.0 * np.pi * Y) \\\n        + 0.25 * np.sin(2.0 * np.pi * X) * np.sin(4.0 * np.pi * Y + 0.3) \\\n        + 0.1 * X\n\n    return u, v\n\ndef stack_components(u: np.ndarray, v: np.ndarray) -> np.ndarray:\n    # Stack u and v vertically: shape (2m, n)\n    return np.vstack([u, v])\n\ndef truncated_svd_reconstruction(M: np.ndarray, r: int) -> np.ndarray:\n    # Compute rank-r truncated SVD reconstruction\n    U, s, Vh = np.linalg.svd(M, full_matrices=False)\n    r = int(r)\n    Ur = U[:, :r]\n    sr = s[:r]\n    Vhr = Vh[:r, :]\n    # Equivalent to Ur @ np.diag(sr) @ Vhr but more efficient:\n    return (Ur * sr) @ Vhr\n\ndef block_mean_reconstruction(A: np.ndarray, sy: int, sx: int) -> np.ndarray:\n    m, n = A.shape\n    R = np.empty_like(A)\n    # Iterate over blocks\n    for r0 in range(0, m, sy):\n        r1 = min(r0 + sy, m)\n        for c0 in range(0, n, sx):\n            c1 = min(c0 + sx, n)\n            block = A[r0:r1, c0:c1]\n            mean_val = block.mean() if block.size > 0 else 0.0\n            R[r0:r1, c0:c1] = mean_val\n    return R\n\ndef relative_frobenius_error(A: np.ndarray, B: np.ndarray) -> float:\n    diff = A - B\n    num = np.linalg.norm(diff, ord='fro')\n    den = np.linalg.norm(A, ord='fro')\n    # In our construction, den should be > 0, but guard just in case\n    if den == 0.0:\n        return 0.0 if num == 0.0 else float('inf')\n    return float(num / den)\n\ndef solve():\n    # Define the test cases from the problem statement as (m, n, r, s_y, s_x)\n    test_cases = [\n        (48, 64, 8, 4, 4),\n        (32, 30, 1, 64, 64),\n        (64, 24, 6, 8, 6),\n        (24, 96, 5, 6, 8),\n        (40, 50, 3, 1, 1),\n    ]\n\n    results = []\n    for m, n, r, sy, sx in test_cases:\n        # Generate field and stack\n        u, v = generate_velocity_field(m, n)\n        M = stack_components(u, v)\n\n        # Truncated SVD reconstruction and error\n        Mr = truncated_svd_reconstruction(M, r)\n        err_svd = relative_frobenius_error(M, Mr)\n\n        # Block coarsening reconstruction and error (apply per component)\n        u_hat = block_mean_reconstruction(u, sy, sx)\n        v_hat = block_mean_reconstruction(v, sy, sx)\n        M_hat = stack_components(u_hat, v_hat)\n        err_coarse = relative_frobenius_error(M, M_hat)\n\n        # Delta = coarsening error - SVD error\n        delta = err_coarse - err_svd\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    # Format with a reasonable precision for readability.\n    print(\"[\" + \",\".join(f\"{val:.10f}\" for val in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2371486"}, {"introduction": "Moving from data compression to problem-solving, we now tackle ill-posed linear systems, a common hurdle in engineering where solutions are overly sensitive to noise. In this practice, you will use Truncated SVD (TSVD) as a regularization method to find a stable and meaningful solution to a system based on the notoriously ill-conditioned Hilbert matrix. This exercise [@problem_id:2371492] illuminates the critical trade-off at the heart of regularization: how discarding information associated with small singular values can stabilize the solution against noise, often bringing it closer to the true underlying answer.", "problem": "Consider a linear inverse problem in computational engineering modeled as $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ is ill-conditioned, $x \\in \\mathbb{R}^{n}$ is an unknown parameter vector, and $b \\in \\mathbb{R}^{m}$ is the given data. Ill-conditioning causes small perturbations in $b$ to induce large changes in solutions, making the problem effectively ill-posed in finite-precision computation. You will investigate a data reduction approach using the Singular Value Decomposition (SVD) to construct a truncated SVD solution and compare the solution residue and the solution error.\n\nFundamental base:\n- Use the definition of the Singular Value Decomposition (SVD): any real matrix $A \\in \\mathbb{R}^{m \\times n}$ admits a factorization $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ diagonal with nonnegative entries arranged in nonincreasing order.\n- Use the least-squares principle: for overdetermined systems, the solution minimizes the Euclidean norm of the residue $\\|A \\tilde{x} - b\\|_{2}$.\n- Use the Euclidean norm properties and orthogonal invariance of the Euclidean norm.\n\nYour task:\n- For each test case, build a matrix $A$, a ground-truth vector $x$, and a data vector $b = A x + e$ where $e$ is a deterministic perturbation vector playing the role of measurement noise.\n- Compute the truncated SVD (TSVD) solution $\\tilde{x}_{k}$ by retaining only the $k$ largest singular components of $A$.\n- For each case, compute:\n  1. The residue norm $r = \\|A \\tilde{x}_{k} - b\\|_{2}$.\n  2. The solution error norm $e_{x} = \\|\\tilde{x}_{k} - x\\|_{2}$.\n- Use the Euclidean norm for all norms and express all results as real numbers rounded to exactly $6$ decimal places.\n\nConstruction rules for all test cases:\n- Define the Hilbert-type matrix $A \\in \\mathbb{R}^{m \\times n}$ entrywise by $A_{ij} = \\dfrac{1}{i + j - 1}$ for $1 \\le i \\le m$, $1 \\le j \\le n$.\n- Define the ground-truth $x \\in \\mathbb{R}^{n}$ by $x_{i} = \\dfrac{(-1)^{i}}{i}$ for $1 \\le i \\le n$.\n- Define the deterministic perturbation vector $e \\in \\mathbb{R}^{m}$ by $e_{i} = \\sigma \\cdot (-1)^{i}$ for $1 \\le i \\le m$, with scalar $\\sigma \\ge 0$ given per test.\n- Form $b = A x + e$.\n- Construct the truncated SVD solution as follows: compute an SVD of $A$ and retain only the $k$ largest singular components to build $\\tilde{x}_{k}$. If $k = 0$, define $\\tilde{x}_{0}$ to be the zero vector in $\\mathbb{R}^{n}$.\n- Compute $r = \\|A \\tilde{x}_{k} - b\\|_{2}$ and $e_{x} = \\|\\tilde{x}_{k} - x\\|_{2}$.\n- Round $r$ and $e_{x}$ to exactly $6$ decimal places.\n\nTest suite:\n- Case $1$: $m = 8$, $n = 8$, $k = 0$, $\\sigma = 10^{-6}$.\n- Case $2$: $m = 8$, $n = 8$, $k = 2$, $\\sigma = 10^{-3}$.\n- Case $3$: $m = 8$, $n = 8$, $k = 4$, $\\sigma = 10^{-3}$.\n- Case $4$: $m = 8$, $n = 8$, $k = 4$, $\\sigma = 0$.\n- Case $5$: $m = 8$, $n = 8$, $k = 8$, $\\sigma = 10^{-3}$.\n- Case $6$: $m = 10$, $n = 6$, $k = 3$, $\\sigma = 10^{-4}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of pairs in a single set of square brackets. Each pair corresponds to a test case, in the same order as listed, and is formatted as $[r,e_{x}]$ with both numbers rounded to $6$ decimal places. No spaces are allowed anywhere in the line.\n- For example, the overall output should look like $[[r_{1},e_{x,1}],[r_{2},e_{x,2}],\\dots,[r_{6},e_{x,6}]]$ where each $r_{i}$ and $e_{x,i}$ are decimal numbers with exactly $6$ digits after the decimal point.", "solution": "The problem under consideration is the solution of a linear system of equations $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ is an ill-conditioned matrix, $x \\in \\mathbb{R}^{n}$ is the vector of unknown parameters, and $b \\in \\mathbb{R}^{m}$ is the data vector, which is contaminated with noise. The data vector is modeled as $b = A x_{\\text{true}} + e$, where $x_{\\text{true}}$ is the ground-truth solution and $e$ is a perturbation vector representing measurement noise. The ill-conditioning of $A$ means that small perturbations in $b$ can lead to large, unphysical oscillations in the solution obtained by naively inverting the system.\n\nA standard method to address this ill-posedness is regularization, and a powerful technique for this is the Truncated Singular Value Decomposition (TSVD). The SVD of the matrix $A$ is given by:\n$$ A = U \\Sigma V^{\\top} $$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^{\\top}U = I_m$, $V^{\\top}V = I_n$), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r = \\text{rank}(A)$. Let the columns of $U$ be $\\{u_i\\}_{i=1}^m$ and the columns of $V$ be $\\{v_i\\}_{i=1}^n$.\n\nThe standard least-squares solution, which minimizes $\\|Ax - b\\|_2$, can be expressed via the Moore-Penrose pseudoinverse $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$:\n$$ \\hat{x} = A^{\\dagger}b = \\sum_{i=1}^{r} \\frac{u_i^{\\top}b}{\\sigma_i} v_i $$\nFor an ill-conditioned matrix, many singular values $\\sigma_i$ are very small. If the numerator $u_i^{\\top}b$ is not correspondingly small, the division by small $\\sigma_i$ amplifies noise from the data vector $b$, corrupting the solution.\n\nThe TSVD method regularizes the solution by truncating the summation, including only the components associated with the $k$ largest singular values, where $k \\le r$ is the truncation parameter. The TSVD solution, denoted $\\tilde{x}_k$, is defined as:\n$$ \\tilde{x}_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} v_i $$\nFor the case where the truncation parameter $k=0$, the summation is empty, and the solution is defined as the zero vector, $\\tilde{x}_0 = \\mathbf{0} \\in \\mathbb{R}^n$.\n\nWe are tasked to compute two key metrics: the residue norm, $r = \\|A\\tilde{x}_k - b\\|_2$, and the solution error norm, $e_x = \\|\\tilde{x}_k - x_{\\text{true}}\\|_2$.\n\nThe residue vector is $A\\tilde{x}_k - b$. Using $Av_i = \\sigma_i u_i$, we have:\n$$ A\\tilde{x}_k = A \\left(\\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} v_i\\right) = \\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} (Av_i) = \\sum_{i=1}^{k} (u_i^{\\top}b) u_i $$\nThe data vector $b$ can be expanded in the orthonormal basis of $U$ as $b = \\sum_{j=1}^{m} (u_j^{\\top}b) u_j$. Thus, the residue is:\n$$ A\\tilde{x}_k - b = \\sum_{i=1}^{k} (u_i^{\\top}b) u_i - \\sum_{j=1}^{m} (u_j^{\\top}b) u_j = - \\sum_{j=k+1}^{m} (u_j^{\\top}b) u_j $$\nBy the Pythagorean theorem for orthogonal vectors, the squared norm of the residue is:\n$$ r^2 = \\|A\\tilde{x}_k - b\\|_2^2 = \\sum_{j=k+1}^{m} (u_j^{\\top}b)^2 $$\n\nThe solution error vector is $\\tilde{x}_k - x_{\\text{true}}$. We expand $\\tilde{x}_k$ using $b = Ax_{\\text{true}} + e$:\n$$ \\tilde{x}_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top}(Ax_{\\text{true}} + e)}{\\sigma_i} v_i = \\sum_{i=1}^{k} \\frac{u_i^{\\top}Ax_{\\text{true}}}{\\sigma_i} v_i + \\sum_{i=1}^{k} \\frac{u_i^{\\top}e}{\\sigma_i} v_i $$\nUsing the relation $u_i^{\\top}A = (A^{\\top}u_i)^{\\top} = (V\\Sigma^{\\top}U^{\\top}u_i)^{\\top} = (\\sigma_i v_i)^{\\top} = \\sigma_i v_i^{\\top}$, the first term becomes:\n$$ \\sum_{i=1}^{k} \\frac{\\sigma_i v_i^{\\top}x_{\\text{true}}}{\\sigma_i} v_i = \\sum_{i=1}^{k} (v_i^{\\top}x_{\\text{true}}) v_i $$\nThe true solution $x_{\\text{true}}$ can be expanded in the orthonormal basis of $V$ as $x_{\\text{true}} = \\sum_{j=1}^{n} (v_j^{\\top}x_{\\text{true}}) v_j$. The error vector is therefore:\n$$ \\tilde{x}_k - x_{\\text{true}} = \\left( \\sum_{i=1}^{k} (v_i^{\\top}x_{\\text{true}}) v_i - \\sum_{j=1}^{n} (v_j^{\\top}x_{\\text{true}}) v_j \\right) + \\sum_{i=1}^{k} \\frac{u_i^{\\top}e}{\\sigma_i} v_i $$\n$$ \\tilde{x}_k - x_{\\text{true}} = \\underbrace{- \\sum_{j=k+1}^{n} (v_j^{\\top}x_{\\text{true}}) v_j}_{\\text{truncation error}} + \\underbrace{\\sum_{i=1}^{k} \\frac{u_i^{\\top}e}{\\sigma_i} v_i}_{\\text{perturbation error}} $$\nThese two error components are orthogonal, as they are linear combinations of disjoint subsets of the orthonormal basis $\\{v_i\\}$. The squared norm of the solution error is the sum of the squared norms of these components:\n$$ e_x^2 = \\|\\tilde{x}_k - x_{\\text{true}}\\|_2^2 = \\sum_{j=k+1}^{n} (v_j^{\\top}x_{\\text{true}})^2 + \\sum_{i=1}^{k} \\left(\\frac{u_i^{\\top}e}{\\sigma_i}\\right)^2 $$\nThe optimal choice of the truncation parameter $k$ involves a trade-off: increasing $k$ reduces the truncation error but increases the perturbation error, especially when the noise $e$ is significant and the singular values $\\sigma_i$ are small.\n\nFor each test case, we apply the following procedure:\n1.  Construct the Hilbert matrix $A \\in \\mathbb{R}^{m \\times n}$ with entries $A_{ij} = \\frac{1}{i + j - 1}$ for $1$-based indices $i,j$.\n2.  Construct the ground-truth vector $x \\in \\mathbb{R}^{n}$ with entries $x_i = \\frac{(-1)^{i}}{i}$ for $1$-based index $i$.\n3.  Construct the perturbation vector $e \\in \\mathbb{R}^{m}$ with entries $e_i = \\sigma \\cdot (-1)^{i}$ for $1$-based index $i$.\n4.  Form the data vector $b = Ax + e$.\n5.  Compute the SVD of $A$: $A = U \\Sigma V^{\\top}$.\n6.  If $k=0$, set $\\tilde{x}_0 = \\mathbf{0}$. Otherwise, compute the TSVD solution $\\tilde{x}_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} v_i$.\n7.  Calculate the residue norm $r = \\|A\\tilde{x}_k - b\\|_2$ and the solution error norm $e_x = \\|\\tilde{x}_k - x\\|_2$.\n8.  Round both results to $6$ decimal places and report.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of linear inverse problems using Truncated SVD (TSVD).\n\n    For each test case, it constructs a Hilbert matrix A, a ground-truth\n    solution x, and a perturbed data vector b. It then computes the TSVD\n    solution x_tilde_k for a given truncation level k. Finally, it calculates\n    the residue norm ||A*x_tilde_k - b||_2 and the solution error norm\n    ||x_tilde_k - x||_2.\n    \"\"\"\n\n    test_cases = [\n        # (m, n, k, sigma)\n        (8, 8, 0, 1e-6),\n        (8, 8, 2, 1e-3),\n        (8, 8, 4, 1e-3),\n        (8, 8, 4, 0.0),\n        (8, 8, 8, 1e-3),\n        (10, 6, 3, 1e-4),\n    ]\n\n    results_list = []\n\n    for m, n, k, sigma in test_cases:\n        # Step 1: Construct the Hilbert matrix A\n        # Using 1-based indexing in formula, A_ij = 1/(i+j-1)\n        # In 0-based numpy, this is 1/((i+1)+(j+1)-1) = 1/(i+j+1)\n        i_indices, j_indices = np.meshgrid(np.arange(m), np.arange(n), indexing='ij')\n        A = 1.0 / (i_indices + j_indices + 1)\n\n        # Step 2: Construct the ground-truth vector x\n        # Using 1-based indexing in formula, x_i = (-1)^i/i for i=1..n\n        idx_n = np.arange(1, n + 1)\n        x_true = ((-1)**idx_n) / idx_n\n\n        # Step 3: Construct the perturbation vector e\n        # Using 1-based indexing in formula, e_i = sigma * (-1)^i for i=1..m\n        idx_m = np.arange(1, m + 1)\n        e = sigma * ((-1)**idx_m)\n\n        # Step 4: Form the data vector b\n        b = A @ x_true + e\n\n        # Step 5: Compute the SVD of A\n        # full_matrices=True to match the problem statement's definition\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n        # Vh is V.T\n\n        # Step 6: Compute the TSVD solution x_tilde_k\n        if k == 0:\n            x_tilde_k = np.zeros(n)\n        else:\n            # Slices for the truncated components\n            s_k = s[:k]\n            U_k = U[:, :k]\n            Vh_k = Vh[:k, :]\n\n            # Compute x_tilde_k = V_k * (S_k^-1 * (U_k^T * b))\n            # This is numerically more stable than forming the pseudoinverse matrix.\n            c = U_k.T @ b\n            w = c / s_k\n            x_tilde_k = Vh_k.T @ w\n\n        # Step 7: Calculate residue and solution error norms\n        residue_norm = np.linalg.norm(A @ x_tilde_k - b)\n        solution_error_norm = np.linalg.norm(x_tilde_k - x_true)\n        \n        # Step 8: Format results to 6 decimal places\n        r_str = f\"{residue_norm:.6f}\"\n        e_x_str = f\"{solution_error_norm:.6f}\"\n        \n        results_list.append(f\"[{r_str},{e_x_str}]\")\n\n    # Final print statement in the exact required format\n    final_output = f\"[{','.join(results_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2371492"}, {"introduction": "Our final practice ventures into the modern domain of data recovery, addressing the common issue of incomplete datasets from sources like sensor arrays. You will implement an iterative algorithm that uses SVD at its core to 'inpaint' or fill in missing values, under the powerful assumption that the complete data has a low-rank structure. This exercise [@problem_id:2371448] showcases how SVD is not just a one-shot analysis tool but can be the engine within advanced algorithms for solving complex problems like matrix completion, a technique with applications from recommendation systems to image restoration.", "problem": "You are given a data-reduction task arising from a sensor array with correlated channels. The data from $m$ sensors recorded at $n$ time steps is represented as a real matrix $X_{\\text{true}} \\in \\mathbb{R}^{m \\times n}$. Due to communication losses and corruption, only a subset of entries is observed. Let $\\Omega \\subset \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$ denote the index set of observed entries, and let $P_{\\Omega}$ be the sampling operator defined by $(P_{\\Omega}(Z))_{ij} = Z_{ij}$ if $(i,j) \\in \\Omega$ and $(P_{\\Omega}(Z))_{ij} = 0$ otherwise. The goal is to reconstruct a low-rank matrix $X \\in \\mathbb{R}^{m \\times n}$ that approximates $X_{\\text{true}}$ by exploiting the assumption that $X_{\\text{true}}$ has low rank (reflecting correlated sensor behavior) and that the best rank-$k$ approximation in Frobenius norm is obtained by truncating the Singular Value Decomposition (SVD).\n\nFundamental definitions and facts to use:\n- The Singular Value Decomposition (SVD) states that any $Z \\in \\mathbb{R}^{m \\times n}$ can be written as $Z = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ orthogonal, $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ diagonal and nonnegative.\n- The Frobenius norm $\\|Z\\|_{F} = \\sqrt{\\sum_{i,j} Z_{ij}^{2}}$.\n- The Eckart–Young theorem: for a given $k \\in \\mathbb{N}$, the best rank-$k$ approximation to $Z$ in Frobenius norm is obtained by truncating the SVD of $Z$ to its top $k$ singular values and corresponding singular vectors.\n\nConsider the following fixed-point iterative scheme for inpainting via low-rank approximation:\n- Initialize $X^{(0)} = 0$.\n- For iteration $t = 0,1,2,\\dots$:\n  1. Form the filled matrix $Y^{(t)} = P_{\\Omega}(Y) + P_{\\Omega^{c}}(X^{(t)})$, where $Y = P_{\\Omega}(X_{\\text{true}} + N)$ is the observed data (possibly noisy), $N$ is a noise matrix (possibly zero), and $P_{\\Omega^{c}}$ is the complement sampling operator that keeps entries not in $\\Omega$ and zeros out entries in $\\Omega$.\n  2. Compute the rank-$k$ truncated SVD approximation $X^{(t+1)}$ of $Y^{(t)}$ by retaining the top $k$ singular values and corresponding singular vectors.\n- Stop when $\\|X^{(t+1)} - X^{(t)}\\|_{F} / \\max(1, \\|X^{(t)}\\|_{F}) \\le \\varepsilon$ or when $t$ reaches a prescribed maximum number of iterations, and return $X^{(t+1)}$.\n\nImplement this algorithm and evaluate it on the following test suite. For each test, compute the relative Frobenius error $\\|X_{\\text{est}} - X_{\\text{true}}\\|_{F} / \\|X_{\\text{true}}\\|_{F}$ as a floating-point number.\n\nTest suite (all numbers are real scalars, and every matrix is given explicitly):\n\n- Case A (happy path, moderately sampled, exact low rank):\n  - Dimensions: $m = 6$, $n = 5$.\n  - Construct $X_{\\text{true}} = A_{A} B_{A}^{\\top}$ with $A_{A} \\in \\mathbb{R}^{6 \\times 2}$ and $B_{A} \\in \\mathbb{R}^{5 \\times 2}$:\n    - $A_{A} = \\begin{bmatrix}\n      1 & 0 \\\\\n      0 & 1 \\\\\n      1 & 1 \\\\\n      2 & -1 \\\\\n      -1 & 2 \\\\\n      0.5 & 1.5\n      \\end{bmatrix}$,\n      $B_{A} = \\begin{bmatrix}\n      2 & 1 \\\\\n      1 & -1 \\\\\n      0 & 2 \\\\\n      -1 & 0.5 \\\\\n      1.5 & -0.5\n      \\end{bmatrix}$.\n  - Mask $M_{A} \\in \\{0,1\\}^{6 \\times 5}$ with ones for observed entries:\n    - $M_{A} = \\begin{bmatrix}\n      1 & 1 & 0 & 1 & 0 \\\\\n      0 & 1 & 1 & 0 & 1 \\\\\n      1 & 0 & 1 & 1 & 0 \\\\\n      1 & 1 & 0 & 0 & 1 \\\\\n      0 & 1 & 1 & 1 & 0 \\\\\n      1 & 0 & 0 & 1 & 1\n      \\end{bmatrix}$.\n  - Noise $N_{A} = 0$ (all entries zero).\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 1000$, tolerance $\\varepsilon = 10^{-10}$.\n\n- Case B (boundary, fully observed, exact recovery at true rank):\n  - Dimensions: $m = 4$, $n = 4$.\n  - Construct $X_{\\text{true}} = A_{B} B_{B}^{\\top}$ with $A_{B} \\in \\mathbb{R}^{4 \\times 2}$ and $B_{B} \\in \\mathbb{R}^{4 \\times 2}$:\n    - $A_{B} = \\begin{bmatrix}\n      2 & 0 \\\\\n      0 & 1 \\\\\n      1 & -1 \\\\\n      3 & 2\n      \\end{bmatrix}$,\n      $B_{B} = \\begin{bmatrix}\n      1 & 2 \\\\\n      0.5 & -1 \\\\\n      2 & 0 \\\\\n      1 & 1\n      \\end{bmatrix}$.\n  - Mask $M_{B}$ is all ones in $\\mathbb{R}^{4 \\times 4}$.\n  - Noise $N_{B} = 0$.\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 1000$, tolerance $\\varepsilon = 10^{-12}$.\n\n- Case C (edge case, one entire row missing):\n  - Dimensions: $m = 5$, $n = 5$.\n  - Construct $X_{\\text{true}} = A_{C} B_{C}^{\\top}$ with $A_{C} \\in \\mathbb{R}^{5 \\times 2}$ and $B_{C} \\in \\mathbb{R}^{5 \\times 2}$:\n    - $A_{C} = \\begin{bmatrix}\n      1 & 0 \\\\\n      0 & 1 \\\\\n      1 & 1 \\\\\n      2 & 1 \\\\\n      -1 & 2\n      \\end{bmatrix}$,\n      $B_{C} = \\begin{bmatrix}\n      1 & 1 \\\\\n      2 & -1 \\\\\n      -1 & 0.5 \\\\\n      0 & 2 \\\\\n      1 & -2\n      \\end{bmatrix}$.\n  - Mask $M_{C} \\in \\{0,1\\}^{5 \\times 5}$:\n    - $M_{C} = \\begin{bmatrix}\n      1 & 0 & 1 & 1 & 0 \\\\\n      1 & 1 & 0 & 0 & 1 \\\\\n      0 & 0 & 0 & 0 & 0 \\\\\n      1 & 1 & 1 & 0 & 1 \\\\\n      0 & 1 & 0 & 1 & 1\n      \\end{bmatrix}$.\n  - Noise $N_{C} = 0$.\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 2000$, tolerance $\\varepsilon = 10^{-12}$.\n\n- Case D (noisy observations, moderately sampled):\n  - Dimensions: $m = 5$, $n = 4$.\n  - Construct $X_{\\text{true}} = A_{D} B_{D}^{\\top}$ with $A_{D} \\in \\mathbb{R}^{5 \\times 2}$ and $B_{D} \\in \\mathbb{R}^{4 \\times 2}$:\n    - $A_{D} = \\begin{bmatrix}\n      1 & 0 \\\\\n      0 & 1 \\\\\n      1 & -1 \\\\\n      2 & 1 \\\\\n      -1 & 2\n      \\end{bmatrix}$,\n      $B_{D} = \\begin{bmatrix}\n      1 & 2 \\\\\n      2 & 1 \\\\\n      -1 & 1 \\\\\n      0.5 & -0.5\n      \\end{bmatrix}$.\n  - Mask $M_{D} \\in \\{0,1\\}^{5 \\times 4}$:\n    - $M_{D} = \\begin{bmatrix}\n      1 & 1 & 0 & 1 \\\\\n      1 & 0 & 1 & 1 \\\\\n      1 & 1 & 0 & 0 \\\\\n      0 & 1 & 1 & 1 \\\\\n      1 & 0 & 1 & 0\n      \\end{bmatrix}$.\n  - Additive noise $N_{D} \\in \\mathbb{R}^{5 \\times 4}$:\n    - $N_{D} = \\begin{bmatrix}\n      0.01 & -0.02 & 0 & 0 \\\\\n      -0.03 & 0 & 0.02 & -0.01 \\\\\n      0.02 & 0.01 & 0 & 0 \\\\\n      0 & -0.02 & 0.03 & -0.01 \\\\\n      0.01 & 0 & -0.02 & 0\n      \\end{bmatrix}$.\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 1500$, tolerance $\\varepsilon = 10^{-10}$.\n\nImplementation requirements:\n- Implement the iterative scheme exactly as stated, using the rank-$k$ truncated SVD at each iteration on the filled matrix $Y^{(t)}$. Use the Frobenius norm for measuring convergence.\n- For each case, compute the relative Frobenius error $\\|X_{\\text{est}} - X_{\\text{true}}\\|_{F} / \\|X_{\\text{true}}\\|_{F}$ as a floating-point number rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_{A},r_{B},r_{C},r_{D}]$), where $r_{A}$, $r_{B}$, $r_{C}$, and $r_{D}$ are the rounded relative errors for Cases A, B, C, and D, respectively. No other text should be printed.", "solution": "The problem describes a task known as low-rank matrix completion, which aims to reconstruct a data matrix from incomplete and potentially noisy observations under the assumption that the underlying true data is of low rank. The proposed method is an iterative algorithm that uses Singular Value Decomposition (SVD) at its core, a standard and powerful technique for low-rank approximation.\n\nThe objective is to implement and evaluate this iterative algorithm. It functions by repeatedly enforcing two properties: consistency with the observed data and adherence to a low-rank model. The core iterative step can be expressed as:\n$$X^{(t+1)} = \\mathcal{S}_k(P_{\\Omega}(Y) + P_{\\Omega^{c}}(X^{(t)}))$$\nwhere $\\mathcal{S}_k(Z)$ denotes the operation of computing the best rank-$k$ approximation of a matrix $Z$. By the Eckart-Young theorem, if the SVD of $Z$ is $Z = U \\Sigma V^{\\top}$, then $\\mathcal{S}_k(Z) = U_k \\Sigma_k V_k^{\\top}$, where $U_k$ and $V_k$ are the matrices formed by the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ is the diagonal matrix of the first $k$ singular values.\n\nUsing the provided mask matrices $M \\in \\{0,1\\}^{m \\times n}$ (where $M_{ij}=1$ for observed entries), the sampling operators can be written using the element-wise (Hadamard) product $\\circ$. The observed data is $Y_{\\text{obs}} = M \\circ (X_{\\text{true}} + N)$, and the iterative update proceeds as follows:\n1.  **Initialize:** The estimate is initialized to the zero matrix: $X^{(0)} = 0 \\in \\mathbb{R}^{m \\times n}$.\n2.  **Iterate** for $t = 0, 1, ..., t_{\\max}-1$:\n    a. **Store previous estimate:** $X_{\\text{prev}} = X^{(t)}$.\n    b. **Fill matrix:** A temporary matrix $Y^{(t)}$ is formed by combining the known observed entries with the current estimate for the unknown entries:\n       $$Y^{(t)} = Y_{\\text{obs}} + (J-M) \\circ X^{(t)}$$\n       where $J$ is the matrix of all ones.\n    c. **Project to low rank:** The new estimate $X^{(t+1)}$ is obtained by computing the rank-$k$ truncated SVD of $Y^{(t)}$:\n       $$X^{(t+1)} = \\mathcal{S}_k(Y^{(t)})$$\n    d. **Check for convergence:** The process stops if the relative change between consecutive estimates is below the tolerance $\\varepsilon$:\n       $$\\frac{\\|X^{(t+1)} - X_{\\text{prev}}\\|_F}{\\max(1, \\|X_{\\text{prev}}\\|_F)} \\le \\varepsilon$$\n3.  **Output:** The final computed matrix $X_{\\text{est}} = X^{(t+1)}$ is used for evaluation.\n\nThe final evaluation for each test case is the relative Frobenius error between the estimated matrix $X_{\\text{est}}$ and the ground truth $X_{\\text{true}}$:\n$$ \\text{Error} = \\frac{\\|X_{\\text{est}} - X_{\\text{true}}\\|_F}{\\|X_{\\text{true}}\\|_F} $$\n\n- **Case A** represents a standard application where a significant fraction of entries is missing, but the underlying matrix is exactly rank-$2$. The algorithm is expected to converge to a solution with low error.\n- **Case B** is a trivial case where all entries are observed ($M=J$). The algorithm should recover $X_{\\text{true}}$ perfectly in the first iteration. $Y^{(0)} = M \\circ X_{\\text{true}} + (J-M) \\circ X^{(0)} = X_{\\text{true}} + 0 = X_{\\text{true}}$. Since $X_{\\text{true}}$ is rank-$2$, $\\mathcal{S}_2(X_{\\text{true}}) = X_{\\text{true}}$. Thus, $X^{(1)} = X_{\\text{true}}$ and the error should be near machine precision.\n- **Case C** is an edge case where an entire row is unobserved. It is a known result in matrix completion theory that recovery is not possible in this situation, as there is no information to constrain the values in that row. The algorithm will converge, but the resulting error for the missing row will be arbitrary from the perspective of the ground truth, leading to a high overall reconstruction error.\n- **Case D** includes additive noise on the observations. The algorithm will attempt to find a rank-$2$ matrix that best fits the noisy data. The reconstruction will not be exact, as the algorithm will denoise the data by projecting it onto a low-rank subspace. The final error is expected to be non-zero but small, reflecting the algorithm's ability to reject some of the noise.\n\nThe implementation will follow this logic for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and evaluate the test cases for matrix completion.\n    \"\"\"\n\n    def run_completion_algorithm(X_true, mask, noise, k, t_max, epsilon):\n        \"\"\"\n        Implements the iterative SVD-based matrix completion algorithm.\n        \n        Args:\n            X_true (np.ndarray): The ground truth matrix.\n            mask (np.ndarray): The observation mask (1s for observed, 0s for missing).\n            noise (np.ndarray): The additive noise matrix.\n            k (int): The target rank.\n            t_max (int): The maximum number of iterations.\n            epsilon (float): The convergence tolerance.\n\n        Returns:\n            np.ndarray: The estimated matrix X_est.\n        \"\"\"\n        m, n = X_true.shape\n        X_est = np.zeros((m, n))\n        Y_obs = mask * (X_true + noise)\n        \n        for _ in range(t_max):\n            X_prev = X_est.copy()\n            \n            # 1. Form the filled matrix\n            Y_filled = Y_obs + (1 - mask) * X_est\n            \n            # 2. Compute the rank-k truncated SVD approximation\n            try:\n                U, s, Vt = np.linalg.svd(Y_filled, full_matrices=False)\n                # Reconstruct from top k singular values/vectors\n                X_est = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n            except np.linalg.LinAlgError:\n                # In case of non-convergence of SVD, though unlikely for these test cases.\n                break\n\n            # 3. Check for convergence\n            norm_prev = np.linalg.norm(X_prev, 'fro')\n            diff_norm = np.linalg.norm(X_est - X_prev, 'fro')\n            \n            if diff_norm / max(1.0, norm_prev) < epsilon:\n                break\n                \n        return X_est\n\n    # Define Test Cases\n    \n    # Case A\n    A_A = np.array([\n        [1, 0], [0, 1], [1, 1],\n        [2, -1], [-1, 2], [0.5, 1.5]\n    ])\n    B_A = np.array([\n        [2, 1], [1, -1], [0, 2],\n        [-1, 0.5], [1.5, -0.5]\n    ])\n    X_true_A = A_A @ B_A.T\n    M_A = np.array([\n        [1, 1, 0, 1, 0], [0, 1, 1, 0, 1], [1, 0, 1, 1, 0],\n        [1, 1, 0, 0, 1], [0, 1, 1, 1, 0], [1, 0, 0, 1, 1]\n    ])\n    N_A = np.zeros_like(X_true_A)\n    params_A = {'X_true': X_true_A, 'mask': M_A, 'noise': N_A, 'k': 2, 't_max': 1000, 'epsilon': 1e-10}\n\n    # Case B\n    A_B = np.array([[2, 0], [0, 1], [1, -1], [3, 2]])\n    B_B = np.array([[1, 2], [0.5, -1], [2, 0], [1, 1]])\n    X_true_B = A_B @ B_B.T\n    M_B = np.ones((4, 4))\n    N_B = np.zeros_like(X_true_B)\n    params_B = {'X_true': X_true_B, 'mask': M_B, 'noise': N_B, 'k': 2, 't_max': 1000, 'epsilon': 1e-12}\n\n    # Case C\n    A_C = np.array([[1, 0], [0, 1], [1, 1], [2, 1], [-1, 2]])\n    B_C = np.array([[1, 1], [2, -1], [-1, 0.5], [0, 2], [1, -2]])\n    X_true_C = A_C @ B_C.T\n    M_C = np.array([\n        [1, 0, 1, 1, 0], [1, 1, 0, 0, 1], [0, 0, 0, 0, 0],\n        [1, 1, 1, 0, 1], [0, 1, 0, 1, 1]\n    ])\n    N_C = np.zeros_like(X_true_C)\n    params_C = {'X_true': X_true_C, 'mask': M_C, 'noise': N_C, 'k': 2, 't_max': 2000, 'epsilon': 1e-12}\n\n    # Case D\n    A_D = np.array([[1, 0], [0, 1], [1, -1], [2, 1], [-1, 2]])\n    B_D = np.array([[1, 2], [2, 1], [-1, 1], [0.5, -0.5]])\n    X_true_D = A_D @ B_D.T\n    M_D = np.array([\n        [1, 1, 0, 1], [1, 0, 1, 1], [1, 1, 0, 0],\n        [0, 1, 1, 1], [1, 0, 1, 0]\n    ])\n    N_D = np.array([\n        [0.01, -0.02, 0, 0], [-0.03, 0, 0.02, -0.01],\n        [0.02, 0.01, 0, 0], [0, -0.02, 0.03, -0.01],\n        [0.01, 0, -0.02, 0]\n    ])\n    params_D = {'X_true': X_true_D, 'mask': M_D, 'noise': N_D, 'k': 2, 't_max': 1500, 'epsilon': 1e-10}\n\n    test_cases = [params_A, params_B, params_C, params_D]\n    results = []\n\n    for case_params in test_cases:\n        X_est = run_completion_algorithm(**case_params)\n        X_true = case_params['X_true']\n        \n        # Calculate relative Frobenius error\n        error = np.linalg.norm(X_est - X_true, 'fro') / np.linalg.norm(X_true, 'fro')\n        \n        # Round to six decimal places\n        rounded_error = f\"{error:.6f}\"\n        results.append(rounded_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2371448"}]}