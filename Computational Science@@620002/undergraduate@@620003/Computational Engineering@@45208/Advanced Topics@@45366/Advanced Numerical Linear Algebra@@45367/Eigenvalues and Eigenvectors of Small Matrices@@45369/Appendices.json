{"hands_on_practices": [{"introduction": "Understanding eigenvalues often starts with finding them from a given matrix. This practice flips the script, challenging you to construct a matrix that exhibits a specific behavior—rotation and scaling—defined by a pair of complex conjugate eigenvalues. This exercise solidifies the deep connection between a matrix's algebraic properties (its spectrum) and its geometric action on a vector space, a fundamental concept in modeling oscillatory or spiraling systems in engineering. [@problem_id:2387674]", "problem": "In computational engineering, two-dimensional linear updates that combine uniform scaling and in-plane rotation are modeled by real $2 \\times 2$ matrices whose eigenvalues occur as a complex conjugate pair. Let $a$ and $b$ be real numbers with $b \\neq 0$, and suppose you want a real $2 \\times 2$ matrix whose spectrum is exactly the pair $a \\pm bi$.\n\nUsing only core definitions of eigenvalues via the characteristic polynomial and the real-linear representation of multiplication by a complex number on the plane, do the following: construct an explicit real $2 \\times 2$ matrix that has eigenvalues $a \\pm bi$, justify that it has those eigenvalues from first principles, and then derive a closed-form analytic expression for its determinant in terms of $a$ and $b$.\n\nProvide your final answer as a single simplified expression in terms of $a$ and $b$. No rounding is required, and no units are involved.", "solution": "We start from two foundational facts. First, an eigenvalue $\\lambda$ of a real $2 \\times 2$ matrix $A$ is a root of its characteristic polynomial, defined by $\\chi_{A}(\\lambda) = \\det(\\lambda I - A)$. Second, multiplication by a complex number on the complex plane $\\mathbb{C}$ defines a real-linear map on $\\mathbb{R}^{2}$ when we identify a complex number $x + i y$ with the vector $\\begin{pmatrix} x \\\\ y \\end{pmatrix}$.\n\nLet $z = a + b i$ with $a, b \\in \\mathbb{R}$ and $b \\neq 0$. Consider the real-linear map $T: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ that corresponds to multiplication by $z$ on $\\mathbb{C}$ under the identification $(x, y) \\leftrightarrow x + i y$. Explicitly, for any $(x, y) \\in \\mathbb{R}^{2}$,\n$$\nz \\cdot (x + i y) = (a + b i)(x + i y) = (a x - b y) + i(b x + a y).\n$$\nTherefore, in the standard basis of $\\mathbb{R}^{2}$, the matrix $A$ of $T$ is\n$$\nA = \\begin{pmatrix}\na & -b \\\\\nb & a\n\\end{pmatrix}.\n$$\nWe now verify that $A$ has eigenvalues $a \\pm b i$ by computing its characteristic polynomial:\n$$\n\\chi_{A}(\\lambda) = \\det(\\lambda I - A) = \\det\\!\\begin{pmatrix}\n\\lambda - a & b \\\\\n- b & \\lambda - a\n\\end{pmatrix} = (\\lambda - a)^{2} + b^{2}.\n$$\nThe roots of $\\chi_{A}(\\lambda)$ satisfy\n$$\n(\\lambda - a)^{2} + b^{2} = 0 \\quad \\Longleftrightarrow \\quad \\lambda - a = \\pm i b \\quad \\Longleftrightarrow \\quad \\lambda = a \\pm b i.\n$$\nThus $A$ indeed has the desired eigenvalues.\n\nFinally, we compute the determinant of $A$ directly from its entries using the definition of the determinant for a $2 \\times 2$ matrix:\n$$\n\\det(A) = a \\cdot a - (-b) \\cdot b = a^{2} + b^{2}.\n$$\nAlternatively, since the determinant equals the product of eigenvalues for any square matrix, we have\n$$\n\\det(A) = (a + b i)(a - b i) = a^{2} + b^{2},\n$$\nwhich agrees with the direct computation. Therefore, the determinant expressed in terms of $a$ and $b$ is $a^{2} + b^{2}$.", "answer": "$$\\boxed{a^{2}+b^{2}}$$", "id": "2387674"}, {"introduction": "One of the most powerful applications of eigenvalues and eigenvectors is in solving systems of linear differential equations, which model countless phenomena from thermal networks to electrical circuits. By changing to the basis of eigenvectors, we can elegantly decouple the system's state variables and find a closed-form solution for its evolution over time. This practice demonstrates this cornerstone technique by asking you to solve for the transient behavior of a simple two-state linear model. [@problem_id:2387684]", "problem": "In a reduced $2$-state linear model arising in computational engineering (for example, a coarse two-node thermal network), the state vector $u(t) \\in \\mathbb{R}^{2}$ evolves according to the linear time-invariant ordinary differential equation (ODE) $\\dot{u}(t) = A u(t)$ with initial condition $u(0) = u_{0}$. Consider\n$$\nA = \\begin{pmatrix} 4 & -2 \\\\ 1 & 1 \\end{pmatrix}, \\qquad u_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nStarting only from fundamental definitions and facts appropriate to this setting—specifically, the power-series definition of the matrix exponential and the concept of eigendecomposition of a matrix—derive an explicit expression for $u(t)$ by diagonalizing $A$. Then, report the closed-form analytic expression for the first component $u_{1}(t)$ as a function of $t$. Provide your final answer as a single analytic expression in terms of $t$. No numerical evaluation or rounding is required.", "solution": "The problem is first subjected to validation.\n\nStep 1: Extracted Givens.\nThe system is governed by the linear time-invariant ordinary differential equation $\\dot{u}(t) = A u(t)$ for the state vector $u(t) \\in \\mathbb{R}^{2}$.\nThe system matrix is $A = \\begin{pmatrix} 4 & -2 \\\\ 1 & 1 \\end{pmatrix}$.\nThe initial condition is $u(0) = u_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe derivation of the solution $u(t)$ must proceed by diagonalizing $A$ and must be based on the power-series definition of the matrix exponential and the concept of eigendecomposition.\nThe required output is the closed-form analytic expression for the first component of the solution vector, $u_{1}(t)$.\n\nStep 2: Validation.\nThe problem is a standard initial value problem for a system of linear first-order ordinary differential equations. Such models are fundamental in computational engineering and other STEM fields. The given matrix $A$ is a constant $2 \\times 2$ matrix, and the initial condition $u_0$ is a constant vector.\nThe problem is:\n- **Scientifically Grounded**: It is based on established principles of linear algebra and differential equations.\n- **Well-Posed**: It is a standard initial value problem with a constant coefficient matrix, guaranteeing a unique solution exists for all time $t$. The eigenvalues of $A$ are distinct, which means $A$ is diagonalizable, and the requested method is applicable.\n- **Objective**: The problem is stated in precise mathematical terms, free of ambiguity or subjective claims.\n- **Complete**: All necessary data ($A$, $u_0$) and constraints are provided.\n- **Feasible**: The calculations are straightforward and do not involve any unrealistic physical or mathematical conditions.\n\nVerdict: The problem is valid.\n\nThe solution proceeds as follows. The problem is a linear initial value problem of the form $\\dot{u}(t) = A u(t)$ with $u(0) = u_0$. The formal solution is given by $u(t) = \\exp(At) u_0$. The problem requires this to be justified from the power series definition of the matrix exponential, $\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{M^k}{k!}$.\nLet $u(t) = \\exp(At) u_0 = \\left(\\sum_{k=0}^{\\infty} \\frac{(At)^k}{k!}\\right) u_0$.\nDifferentiating term by term with respect to $t$:\n$$\n\\frac{d}{dt} u(t) = \\frac{d}{dt} \\left( \\sum_{k=0}^{\\infty} \\frac{A^k t^k}{k!} \\right) u_0 = \\left( \\sum_{k=1}^{\\infty} \\frac{A^k k t^{k-1}}{k!} \\right) u_0 = \\left( \\sum_{k=1}^{\\infty} \\frac{A^k t^{k-1}}{(k-1)!} \\right) u_0\n$$\nFactoring out one $A$ from the summation:\n$$\n\\frac{d}{dt} u(t) = A \\left( \\sum_{k=1}^{\\infty} \\frac{A^{k-1} t^{k-1}}{(k-1)!} \\right) u_0\n$$\nBy re-indexing with $j = k-1$, the sum becomes $\\sum_{j=0}^{\\infty} \\frac{(At)^j}{j!} = \\exp(At)$.\nThus, $\\frac{d}{dt} u(t) = A \\left( \\exp(At) u_0 \\right) = A u(t)$, which satisfies the differential equation.\nAt $t=0$, $u(0) = \\exp(A \\cdot 0) u_0 = I u_0 = u_0$, which satisfies the initial condition.\n\nTo compute $\\exp(At)$, we diagonalize the matrix $A$. The eigendecomposition of $A$ is $A = PDP^{-1}$, where $D$ is a diagonal matrix of eigenvalues and $P$ is a matrix whose columns are the corresponding eigenvectors.\nFirst, we find the eigenvalues by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 4-\\lambda & -2 \\\\ 1 & 1-\\lambda \\end{pmatrix} = (4-\\lambda)(1-\\lambda) - (-2)(1) = 4 - 5\\lambda + \\lambda^2 + 2 = \\lambda^2 - 5\\lambda + 6 = 0\n$$\nFactoring the quadratic equation gives $(\\lambda - 2)(\\lambda - 3) = 0$. The eigenvalues are $\\lambda_1 = 2$ and $\\lambda_2 = 3$.\n\nNext, we find the corresponding eigenvectors by solving $(A - \\lambda I)v = 0$ for each eigenvalue.\nFor $\\lambda_1 = 2$:\n$$\n(A - 2I)v_1 = \\begin{pmatrix} 4-2 & -2 \\\\ 1 & 1-2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 2 & -2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the equation $x-y=0$. A suitable eigenvector is $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nFor $\\lambda_2 = 3$:\n$$\n(A - 3I)v_2 = \\begin{pmatrix} 4-3 & -2 \\\\ 1 & 1-3 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 1 & -2 \\\\ 1 & -2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the equation $x-2y=0$. A suitable eigenvector is $v_2 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\nThe matrix of eigenvectors $P$ and the diagonal matrix of eigenvalues $D$ are:\n$$\nP = \\begin{pmatrix} v_1 & v_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 1 & 1 \\end{pmatrix}, \\qquad D = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}\n$$\nWe need the inverse of $P$:\n$$\nP^{-1} = \\frac{1}{\\det(P)} \\begin{pmatrix} 1 & -2 \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{1 \\cdot 1 - 2 \\cdot 1} \\begin{pmatrix} 1 & -2 \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{-1} \\begin{pmatrix} 1 & -2 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 \\\\ 1 & -1 \\end{pmatrix}\n$$\nThe utility of diagonalization is that $\\exp(At) = P \\exp(Dt) P^{-1}$. This arises from the power series:\n$\\exp(At) = \\sum_{k=0}^{\\infty} \\frac{(PDP^{-1}t)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{P(Dt)^k P^{-1}}{k!} = P\\left(\\sum_{k=0}^{\\infty} \\frac{(Dt)^k}{k!}\\right)P^{-1} = P\\exp(Dt)P^{-1}$.\nThe exponential of a diagonal matrix is the diagonal matrix of the exponentials of its elements:\n$$\n\\exp(Dt) = \\exp\\left(\\begin{pmatrix} 2t & 0 \\\\ 0 & 3t \\end{pmatrix}\\right) = \\begin{pmatrix} \\exp(2t) & 0 \\\\ 0 & \\exp(3t) \\end{pmatrix}\n$$\nNow, we compute $\\exp(At)$:\n$$\n\\exp(At) = P \\exp(Dt) P^{-1} = \\begin{pmatrix} 1 & 2 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\exp(2t) & 0 \\\\ 0 & \\exp(3t) \\end{pmatrix} \\begin{pmatrix} -1 & 2 \\\\ 1 & -1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\exp(2t) & 2\\exp(3t) \\\\ \\exp(2t) & \\exp(3t) \\end{pmatrix} \\begin{pmatrix} -1 & 2 \\\\ 1 & -1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -\\exp(2t) + 2\\exp(3t) & 2\\exp(2t) - 2\\exp(3t) \\\\ -\\exp(2t) + \\exp(3t) & 2\\exp(2t) - \\exp(3t) \\end{pmatrix}\n$$\nFinally, we compute the solution $u(t) = \\exp(At) u_0$:\n$$\nu(t) = \\begin{pmatrix} -\\exp(2t) + 2\\exp(3t) & 2\\exp(2t) - 2\\exp(3t) \\\\ -\\exp(2t) + \\exp(3t) & 2\\exp(2t) - \\exp(3t) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\nu(t) = \\begin{pmatrix} u_1(t) \\\\ u_2(t) \\end{pmatrix} = \\begin{pmatrix} -\\exp(2t) + 2\\exp(3t) \\\\ -\\exp(2t) + \\exp(3t) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(3t) - \\exp(2t) \\\\ \\exp(3t) - \\exp(2t) \\end{pmatrix}\n$$\nThe problem asks for the first component, $u_1(t)$.\n$$\nu_1(t) = 2\\exp(3t) - \\exp(2t)\n$$\nThis is the closed-form analytic expression for the first component of the state vector as a function of time $t$.", "answer": "$$\\boxed{2\\exp(3t) - \\exp(2t)}$$", "id": "2387684"}, {"introduction": "While many systems can be fully understood through diagonalization, some important cases are \"defective,\" meaning they lack a full set of linearly independent eigenvectors for a repeated eigenvalue. These non-diagonalizable systems, which often represent critically damped or shearing phenomena, require a different tool for analysis: the Jordan normal form. This final practice guides you in identifying the canonical structure of such a system, a crucial step for accurately characterizing its stability and long-term behavior. [@problem_id:2387707]", "problem": "In computational engineering, the stability of a discretized two-state linear system can be investigated via the eigenstructure of its state-transition matrix. Consider the real matrix $A \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n5 & -3 \\\\\n12 & -7\n\\end{pmatrix}.\n$$\nBy the Jordan normal form over $\\mathbb{R}$, we mean a real canonical form consisting of Jordan blocks with the convention that each Jordan block has ones on its superdiagonal. Using only definitions of eigenvalues, eigenvectors, and generalized eigenvectors, determine the value of the single superdiagonal entry in the Jordan normal form of $A$ under similarity over $\\mathbb{R}$. Express your final answer as a real number. No rounding is required.", "solution": "The problem is subjected to validation before a solution is attempted.\n\nFirst, the givens are extracted from the problem statement:\n- The matrix is $A \\in \\mathbb{R}^{2 \\times 2}$ defined as $A = \\begin{pmatrix} 5 & -3 \\\\ 12 & -7 \\end{pmatrix}$.\n- The objective is to determine the value of the single superdiagonal entry in the Jordan normal form of $A$ over $\\mathbb{R}$.\n- The convention for a Jordan block is that it has ones on its superdiagonal.\n- The derivation must \"use only definitions of eigenvalues, eigenvectors, and generalized eigenvectors\".\n\nThe problem is evaluated for validity. It is scientifically grounded as it pertains to the well-established theory of linear algebra, specifically the Jordan normal form. It is well-posed, as the Jordan form of a square matrix is unique up to the permutation of its Jordan blocks, and the value of superdiagonal entries is a deterministic property of the matrix. The language is objective and precise. The problem is self-contained and does not violate any fundamental principles. Therefore, the problem is deemed valid and a solution will be furnished.\n\nThe Jordan normal form of a matrix is determined by its eigenvalues and the structure of their corresponding eigenspaces and generalized eigenspaces. The first step is to compute the eigenvalues of the matrix $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n\nThe characteristic polynomial is:\n$$ \\det(A - \\lambda I) = \\det\\begin{pmatrix} 5-\\lambda & -3 \\\\ 12 & -7-\\lambda \\end{pmatrix} $$\n$$ = (5-\\lambda)(-7-\\lambda) - (-3)(12) $$\n$$ = -35 - 5\\lambda + 7\\lambda + \\lambda^2 + 36 $$\n$$ = \\lambda^2 + 2\\lambda + 1 $$\n$$ = (\\lambda + 1)^2 $$\nSetting the characteristic polynomial to zero, $(\\lambda + 1)^2 = 0$, reveals a single eigenvalue $\\lambda = -1$ with an algebraic multiplicity of $m_a = 2$.\n\nNext, we determine the geometric multiplicity, $m_g$, of this eigenvalue. The geometric multiplicity is the dimension of the eigenspace associated with $\\lambda$, which is the dimension of the null space of the matrix $(A - \\lambda I)$.\nFor $\\lambda = -1$, we have:\n$$ A - \\lambda I = A - (-1)I = A + I = \\begin{pmatrix} 5+1 & -3 \\\\ 12 & -7+1 \\end{pmatrix} = \\begin{pmatrix} 6 & -3 \\\\ 12 & -6 \\end{pmatrix} $$\nTo find the null space, we solve the system $(A+I)v = 0$ for a vector $v = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$:\n$$ \\begin{pmatrix} 6 & -3 \\\\ 12 & -6 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis system of linear equations is equivalent to the single equation $6x - 3y = 0$, which simplifies to $y = 2x$. The eigenspace is spanned by a single vector, for example, $v_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe dimension of this null space is $1$. Therefore, the geometric multiplicity is $m_g = 1$.\n\nSince the geometric multiplicity $m_g = 1$ is less than the algebraic multiplicity $m_a = 2$, the matrix $A$ is not diagonalizable. The Jordan normal form of $A$ must contain at least one non-diagonal Jordan block. Given that $A$ is a $2 \\times 2$ matrix with a single eigenvalue of algebraic multiplicity $2$, its Jordan form must consist of a single $2 \\times 2$ Jordan block.\n\nAccording to the definition and the convention specified in the problem statement, a Jordan block of size $k \\times k$ for an eigenvalue $\\lambda$ is an upper triangular matrix of the form:\n$$ J_k(\\lambda) = \\begin{pmatrix}\n\\lambda & 1 & 0 & \\dots & 0 \\\\\n0 & \\lambda & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\lambda & 1 \\\\\n0 & 0 & \\dots & 0 & \\lambda\n\\end{pmatrix} $$\nThe superdiagonal entries are all $1$.\n\nFor the matrix $A$, the Jordan normal form is a single $2 \\times 2$ block corresponding to the eigenvalue $\\lambda = -1$. This block is:\n$$ J = J_2(-1) = \\begin{pmatrix} -1 & 1 \\\\ 0 & -1 \\end{pmatrix} $$\nThis structure is a direct consequence of the matrix having only one linearly independent eigenvector for an eigenvalue of algebraic multiplicity two. A basis for the Jordan decomposition is formed by an eigenvector $v_1$ and a generalized eigenvector $v_2$, which satisfies $(A-\\lambda I)v_2 = v_1$. The existence of such a chain of length two confirms the structure of the Jordan block.\n\nThe Jordan form $J$ has a single superdiagonal entry. Inspecting the matrix $J$, this entry is located at row $1$, column $2$. The value of this entry is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "2387707"}]}