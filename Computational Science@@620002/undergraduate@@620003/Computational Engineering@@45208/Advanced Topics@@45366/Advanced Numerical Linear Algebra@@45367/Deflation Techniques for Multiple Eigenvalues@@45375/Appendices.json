{"hands_on_practices": [{"introduction": "Our journey into hands-on practices begins with a critical examination of a common pitfall in eigenvalue computations. While Hotelling's deflation is a straightforward method for finding subsequent eigenvalues, its effectiveness is not universal. This exercise uses a carefully constructed non-diagonalizable (defective) matrix to demonstrate a scenario where the standard deflation procedure fails to fully account for an eigenvalue's structure [@problem_id:2383519]. By working through this example, you will gain a deeper appreciation for the distinction between algebraic and geometric multiplicity and understand why simple rank-one deflation is insufficient for handling the generalized eigenspaces associated with defective eigenvalues.", "problem": "Consider the $4 \\times 4$ matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}.\n$$\nThe eigenvalue $\\lambda=1$ is multiple for $A$. Let $v$ be a unit right eigenvector of $A$ associated with $\\lambda=1$. Define the standard Hotelling deflation (rank-one deflation) by\n$$\nA_{\\mathrm{def}} \\;=\\; A \\;-\\; \\lambda\\, v v^{\\top}.\n$$\nUsing only the definitions of eigenvalue, eigenvector, generalized eigenspace, and determinant, perform the following:\n\n- Verify that $\\lambda=1$ is an eigenvalue of $A$ with algebraic multiplicity greater than $1$, and determine a unit right eigenvector $v$ for $\\lambda=1$.\n- Form $A_{\\mathrm{def}}$ and justify, in terms of the structure of $A$ and the definition of generalized eigenvectors, why this standard deflation fails to uncover the full basis for the generalized eigenspace corresponding to $\\lambda=1$.\n- Compute $\\det\\!\\big(A_{\\mathrm{def}} - \\lambda I\\big)$.\n\nAnswer specification:\n- Provide your final answer as a single real number.\n- No rounding is required.", "solution": "The problem statement is first validated against the required criteria.\n\n**Step 1: Extract Givens**\n- The matrix $A$:\n$$\nA = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\n- An eigenvalue $\\lambda = 1$.\n- The definition of Hotelling deflation: $A_{\\mathrm{def}} = A - \\lambda v v^{\\top}$, where $v$ is a unit right eigenvector of $A$ associated with $\\lambda$.\n- The tasks are:\n    1. Verify that $\\lambda=1$ is a multiple eigenvalue of $A$ and find a corresponding unit right eigenvector $v$.\n    2. Form $A_{\\mathrm{def}}$ and explain why standard deflation fails to uncover the full basis for the generalized eigenspace corresponding to $\\lambda=1$.\n    3. Compute $\\det(A_{\\mathrm{def}} - \\lambda I)$ with $\\lambda=1$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard, well-established concepts from linear algebra and numerical analysis, namely eigenvalues, eigenvectors, generalized eigenspaces, determinants, and matrix deflation techniques. These are fundamental topics in computational engineering.\n- **Well-Posed:** The problem provides a specific matrix and clear definitions. The tasks are unambiguous and lead to a unique, verifiable solution.\n- **Objective:** The problem is stated using precise mathematical language, free from subjective or speculative content.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard, well-posed problem in numerical linear algebra. I will now proceed with the solution.\n\n**Part 1: Eigenvalue Multiplicity and Eigenvector**\n\nTo determine the eigenvalues of $A$, we compute the characteristic polynomial $p(\\lambda) = \\det(A - \\lambda I)$, where $I$ is the $4 \\times 4$ identity matrix.\n$$\nA - \\lambda I = \\begin{pmatrix}\n1-\\lambda & 1 & 0 & 0 \\\\\n0 & 1-\\lambda & 1 & 0 \\\\\n0 & 0 & 1-\\lambda & 0 \\\\\n0 & 0 & 0 & 2-\\lambda\n\\end{pmatrix}\n$$\nSince this is an upper triangular matrix, its determinant is the product of its diagonal elements:\n$$\n\\det(A - \\lambda I) = (1-\\lambda)(1-\\lambda)(1-\\lambda)(2-\\lambda) = (1-\\lambda)^{3}(2-\\lambda)\n$$\nThe roots of the characteristic equation $\\det(A - \\lambda I) = 0$ are the eigenvalues. The eigenvalues are $\\lambda_1 = 1$ with algebraic multiplicity $m_a(\\lambda_1) = 3$, and $\\lambda_2 = 2$ with algebraic multiplicity $m_a(\\lambda_2) = 1$. This verifies that $\\lambda=1$ is an eigenvalue with multiplicity greater than $1$.\n\nNext, we find a right eigenvector $x$ corresponding to $\\lambda=1$ by solving the system $(A - 1 \\cdot I)x = 0$.\n$$\n(A - I)x = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\nThis matrix-vector equation yields the system of linear equations:\n$x_2 = 0$\n$x_3 = 0$\n$x_4 = 0$\nThe variable $x_1$ is a free variable. Thus, any eigenvector corresponding to $\\lambda=1$ is of the form $c \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$ for any non-zero scalar $c$. The eigenspace is one-dimensional, so the geometric multiplicity is $m_g(1) = 1$. The problem requires a unit eigenvector $v$. We select the basis vector for the eigenspace and normalize it.\nLet $x = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$. The Euclidean norm is $\\|x\\|_2 = \\sqrt{1^2 + 0^2 + 0^2 + 0^2} = 1$.\nTherefore, a unit right eigenvector is $v = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$.\n\n**Part 2: Deflated Matrix and Explanation of Failure**\n\nWe form the deflated matrix $A_{\\mathrm{def}}$ using $\\lambda=1$ and $v = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$.\nFirst, we compute the rank-one matrix $v v^{\\top}$:\n$$\nv v^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nNow, we compute $A_{\\mathrm{def}} = A - \\lambda v v^{\\top} = A - 1 \\cdot v v^{\\top}$:\n$$\nA_{\\mathrm{def}} = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix} - \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\nThe failure of this deflation method lies in the fact that the matrix $A$ is defective for the eigenvalue $\\lambda=1$. A matrix is defective for an eigenvalue if its algebraic multiplicity is greater than its geometric multiplicity. Here, $m_a(1)=3$ while $m_g(1)=1$.\n\nThis defectiveness implies the existence of a generalized eigenspace of dimension $3$, which is spanned not just by the single eigenvector $v$, but also by two generalized eigenvectors. A generalized eigenvector $x_k$ is a vector in a chain defined by $(A - \\lambda I)x_k = x_{k-1}$, where $x_1$ is the true eigenvector. The space $\\ker((A-\\lambda I)^k)$ for $k > 1$ contains the generalized eigenvectors.\n\nThe standard Hotelling deflation, $A_{\\mathrm{def}} = A - \\lambda v v^{\\top}$, is designed to shift the eigenvalue $\\lambda$ corresponding to the eigenvector $v$ to $0$, such that $A_{\\mathrm{def}}v = (A - \\lambda v v^{\\top})v = Av - \\lambda v(v^{\\top}v) = \\lambda v - \\lambda v(1) = 0$. However, this rank-one modification only utilizes information about the one-dimensional eigenspace spanned by $v$. It does not account for the higher-dimensional structure of the generalized eigenspace (which is an invariant subspace under $A$). The deflation fails to correctly transform the entire Jordan block associated with $\\lambda=1$. Consequently, the influence of the other vectors in the Jordan chain is not removed from the matrix, and the deflated matrix $A_{\\mathrm{def}}$ retains components of the original eigensystem associated with $\\lambda=1$. The goal of deflation is to find eigenvectors for other eigenvalues, but in this defective case, subsequent power iterations on $A_{\\mathrm{def}}$ would converge to another vector within the same generalized eigenspace for $\\lambda=1$.\n\nTo see this explicitly, let's find the eigenvalues of $A_{\\mathrm{def}}$. The characteristic polynomial is $\\det(A_{\\mathrm{def}} - \\mu I)$:\n$$\n\\det\\begin{pmatrix}\n-\\mu & 1 & 0 & 0 \\\\\n0 & 1-\\mu & 1 & 0 \\\\\n0 & 0 & 1-\\mu & 0 \\\\\n0 & 0 & 0 & 2-\\mu\n\\end{pmatrix} = (-\\mu)(1-\\mu)^2(2-\\mu)\n$$\nThe eigenvalues of $A_{\\mathrm{def}}$ are $\\{0, 1, 1, 2\\}$. The original eigenvalues of $A$ were $\\{1, 1, 1, 2\\}$. The deflation only shifted one instance of the eigenvalue $1$ to $0$, while two instances of the eigenvalue $1$ remain. A successful deflation would simplify the eigenproblem, but here it has failed to remove the multiplicity of $\\lambda=1$, which is the primary reason for deflation in the first place.\n\n**Part 3: Final Computation**\n\nThe problem asks for the computation of $\\det(A_{\\mathrm{def}} - \\lambda I)$ with $\\lambda = 1$. This is equivalent to asking for the value of the characteristic polynomial of $A_{\\mathrm{def}}$ at $\\mu=1$. From the analysis above, we know that $1$ is an eigenvalue of $A_{\\mathrm{def}}$. By definition, if $\\mu$ is an eigenvalue of a matrix $M$, then $\\det(M - \\mu I) = 0$.\n\nLet's compute it directly for completeness.\n$$\nA_{\\mathrm{def}} - 1 \\cdot I = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix} - \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe resulting matrix has a row consisting entirely of zeros (the third row). The determinant of any matrix with a zero row or column is zero.\nTherefore, $\\det(A_{\\mathrm{def}} - I) = 0$.", "answer": "$$\\boxed{0}$$", "id": "2383519"}, {"introduction": "Having seen how deflation can fail for defective matrices, we now turn our attention to a more subtle but equally important issue: numerical stability. Even for well-behaved symmetric matrices, the choice of deflation method can dramatically impact the accuracy of your results, especially when eigenvalues are closely clustered. This practice provides a theoretical framework for comparing the numerical robustness of Hotelling's deflation against Wielandt's deflation [@problem_id:2383542]. By analyzing a hypothetical case with eigenvalues separated by machine precision, you will see how subtractive cancellation can cripple one method while another remains accurate, a vital lesson in computational science.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with two clustered eigenvalues separated by machine precision. Throughout, assume the standard floating-point (FP) rounding model for a single scalar operation (addition, subtraction, or multiplication): if $z$ denotes the exact result of an operation on FP inputs, the computed result is $\\operatorname{fl}(z) = z (1 + \\delta)$ with $|\\delta| \\leq \\varepsilon_{m}$, where $\\varepsilon_{m}$ is the unit roundoff (often called the machine epsilon). Assume that the inputs are exactly representable in FP, and that the matrix norm is the spectral norm.\n\nLet $A$ be given in exact arithmetic by\n$$\nA \\;=\\; Q \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda + \\delta \\end{pmatrix} Q^{\\top},\n$$\nwhere $Q$ is orthogonal, $\\lambda > 0$, and $0 < \\delta \\ll \\lambda$. Assume the first eigenpair $(\\lambda, v)$ is known exactly, with $v$ the first column of $Q$. You will compare two deflation strategies to estimate the second eigenvalue $\\lambda + \\delta$ in finite precision when the eigenvalue separation equals machine precision, that is, when $\\delta = \\varepsilon_{m}$.\n\nHotelling’s deflation forms the deflated matrix\n$$\nA_{H} \\;=\\; A \\;-\\; \\lambda \\, v v^{\\top},\n$$\nand then estimates the remaining eigenvalue as the nonzero eigenvalue of $A_{H}$. Wielandt’s deflation uses the orthogonal projector\n$$\nP \\;=\\; I \\;-\\; v v^{\\top},\n$$\nand works only on the orthogonal complement of $v$, restricting $P A P$ to $\\mathrm{span}\\{v\\}^{\\perp}$ to estimate the remaining eigenvalue. In exact arithmetic both approaches recover the same second eigenvalue $\\lambda + \\delta$.\n\nIn FP arithmetic, however, the two routes incur different backward errors at the point where the large $O(\\lambda)$ contribution is removed. Model each route as follows.\n\n- Hotelling: The explicitly formed FP-deflated matrix is\n$$\n\\widehat{A}_{H} \\;=\\; \\operatorname{fl}\\!\\left(A - \\lambda v v^{\\top}\\right) \\;=\\; \\left(A - \\lambda v v^{\\top}\\right) \\;+\\; E_{H},\n$$\nwith an additive perturbation $E_{H}$ satisfying $\\|E_{H}\\|_{2} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda$ for a modest constant $c_{H}$ independent of $\\lambda$ and $\\delta$.\n\n- Wielandt: The action of $P A P$ on $\\mathrm{span}\\{v\\}^{\\perp}$ is implemented without forming $P A P$ explicitly; that is, vectors $x \\in \\mathrm{span}\\{v\\}^{\\perp}$ are mapped by the computed operator\n$$\n\\widehat{B}\\,x \\;=\\; \\operatorname{fl}\\!\\big( P\\,\\operatorname{fl}(A\\,\\operatorname{fl}(P x)) \\big),\n$$\nwhich in exact arithmetic equals $B x := (P A P)x$ and, on $\\mathrm{span}\\{v\\}^{\\perp}$, reduces to $B x = (\\lambda + \\delta)x$. Under the FP model and using $x \\in \\mathrm{span}\\{v\\}^{\\perp}$ (so $P x = x$ exactly), show that the induced operator perturbation on $\\mathrm{span}\\{v\\}^{\\perp}$ satisfies $\\|\\widehat{B} - B\\|_{2} \\approx c_{W}\\,\\varepsilon_{m}\\,(\\lambda + \\delta)$ when forming $A x$, but that the subsequent restriction to the orthogonal complement makes the $O(\\lambda)$ part cancel in the spectral estimate, leaving an effective eigenvalue perturbation controlled by the $\\delta$-scaled part, namely of size $\\approx c_{W}\\,\\varepsilon_{m}\\,\\delta$ with a modest constant $c_{W}$.\n\nAssume further that first-order eigenvalue perturbation for symmetric matrices applies: if $\\mu$ is a simple eigenvalue of a symmetric matrix $M$ with unit eigenvector $w$, then under a small symmetric perturbation $E$ the first-order eigenvalue error is $w^{\\top} E w$, and hence bounded in magnitude by $\\|E\\|_{2}$.\n\nUsing this model, compute the ratio\n$$\nR(\\varepsilon_{m}) \\;=\\; \\frac{\\text{absolute error in the Hotelling estimate of } \\lambda + \\delta}{\\text{absolute error in the Wielandt estimate of } \\lambda + \\delta}\n$$\nfor the normalized case $\\|A\\|_{2} = \\lambda + \\delta = 1$ with $\\delta = \\varepsilon_{m}$. Your final answer must be a single closed-form expression in terms of $\\varepsilon_{m}$ only. No rounding is required, and no units are needed.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the analysis of floating-point errors in eigenvalue algorithms. It is well-posed, providing all necessary models and data, and is expressed in objective, unambiguous language. I shall proceed with the solution.\n\nThe objective is to compute the ratio, $R(\\varepsilon_{m})$, of the absolute errors in estimating the eigenvalue $\\lambda+\\delta$ using Hotelling's and Wielandt's deflation methods, under the specified finite-precision model.\n\nFirst, we analyze the error for Hotelling's deflation. The method involves the explicit formation of the matrix $\\widehat{A}_{H} = \\operatorname{fl}(A - \\lambda v v^{\\top})$. The problem models this as $\\widehat{A}_{H} = A_{H} + E_{H}$, where $A_{H} = A - \\lambda v v^{\\top}$ is the exact deflated matrix and $E_{H}$ is the perturbation due to floating-point arithmetic. The eigenvalues of the symmetric matrix $A_H$ are $0$ and $\\lambda + \\delta$. We seek to estimate $\\lambda + \\delta$.\n\nAccording to first-order eigenvalue perturbation theory for symmetric matrices, the error in an eigenvalue is bounded by the spectral norm of the perturbation matrix. The problem provides the magnitude of this perturbation as $\\|E_{H}\\|_{2} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda$. The absolute error in the Hotelling estimate, which we denote $\\Delta\\lambda_{H}$, is therefore of this order:\n$$\n\\Delta\\lambda_{H} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda\n$$\nThis error is proportional to the larger eigenvalue $\\lambda$, which is a characteristic result for methods involving subtractive cancellation of large quantities.\n\nNext, we analyze the error for Wielandt's deflation. This method operates on the subspace $\\mathrm{span}\\{v\\}^{\\perp}$ by effectively applying the operator $B = PAP$, where $P = I - vv^{\\top}$. For any vector $x \\in \\mathrm{span}\\{v\\}^{\\perp}$, we have $Px=x$, and thus $Bx = PAx = P((\\lambda+\\delta)x) = (\\lambda+\\delta)x$. The exact operator on this subspace is simple scaling by the desired eigenvalue, $\\lambda+\\delta$.\n\nThe problem provides a refined model for the error in this procedure. It states that while the computed action $\\widehat{B}x$ induces an operator perturbation $\\|\\widehat{B} - B\\|_{2}$ proportional to $\\lambda+\\delta$, the \"effective eigenvalue perturbation\" is much smaller. This is because the projection $P$ nullifies the dominant part of the error associated with the large eigenvalue $\\lambda$. The resulting absolute error in the Wielandt estimate, $\\Delta\\lambda_{W}$, is given to be of the order:\n$$\n\\Delta\\lambda_{W} \\approx c_{W}\\,\\varepsilon_{m}\\,\\delta\n$$\nThis error is proportional to the small eigenvalue separation $\\delta$, indicating superior numerical stability.\n\nWe can now form the ratio $R(\\varepsilon_{m})$ of these two absolute errors:\n$$\nR(\\varepsilon_{m}) = \\frac{\\Delta\\lambda_{H}}{\\Delta\\lambda_{W}} \\approx \\frac{c_{H}\\,\\varepsilon_{m}\\,\\lambda}{c_{W}\\,\\varepsilon_{m}\\,\\delta} = \\frac{c_{H}}{c_{W}} \\frac{\\lambda}{\\delta}\n$$\nThe factors $c_{H}$ and $c_{W}$ are described as \"modest constants\" that depend on the details of the floating-point implementation. In a high-level comparison of algorithmic stability, it is standard to assume these constants are of the same order of magnitude, thus we take the approximation $c_{H}/c_{W} \\approx 1$. This is a reasonable step to isolate the structural difference in the error propagation of the two methods. The ratio then simplifies to:\n$$\nR(\\varepsilon_{m}) \\approx \\frac{\\lambda}{\\delta}\n$$\nThe problem requires this ratio to be computed for a specific case. The conditions are:\n$1$. The spectral norm of $A$ is normalized: $\\|A\\|_{2} = \\lambda + \\delta = 1$.\n$2$. The eigenvalue separation is equal to the machine epsilon: $\\delta = \\varepsilon_{m}$.\n\nFrom the first condition, we express $\\lambda$ in terms of $\\delta$:\n$$\n\\lambda = 1 - \\delta\n$$\nSubstituting the second condition, $\\delta = \\varepsilon_{m}$, into this expression yields:\n$$\n\\lambda = 1 - \\varepsilon_{m}\n$$\nFinally, we substitute these expressions for $\\lambda$ and $\\delta$ into the formula for the error ratio $R(\\varepsilon_{m})$:\n$$\nR(\\varepsilon_{m}) \\approx \\frac{1 - \\varepsilon_{m}}{\\varepsilon_{m}}\n$$\nThis result demonstrates the significant superiority of Wielandt's deflation over Hotelling's method for finding a small eigenvalue when it is clustered with a large one. The error in Hotelling's method is larger by a factor of approximately $1/\\varepsilon_{m}$.", "answer": "$$\n\\boxed{\\frac{1 - \\varepsilon_{m}}{\\varepsilon_{m}}}\n$$", "id": "2383542"}, {"introduction": "Building on our understanding of the challenges in deflation, we now explore a powerful and practical alternative known as 'soft deflation'. Instead of sequentially removing eigen-information from the matrix, this technique recasts the search for the next eigenvector as a penalized optimization problem. This hands-on coding exercise will guide you through the process of deriving and implementing this method, transforming a penalized Rayleigh quotient into a standard eigenvalue problem [@problem_id:2383531]. Through a comprehensive test suite, you will explore the method's robustness and see firsthand how a well-chosen penalty can effectively steer the computation towards the desired solution.", "problem": "You are given a real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a unit vector $v_1 \\in \\mathbb{R}^n$ that approximates an eigenvector associated with the smallest eigenvalue $\\lambda_1$ of $A$. Consider the penalized Rayleigh-quotient objective\n$$\nq(x) \\;=\\; \\frac{x^{\\top} A x}{x^{\\top} x} \\;+\\; \\gamma \\, (v_1^{\\top} x)^2,\n$$\nwith penalty parameter $\\gamma > 0$, and the constrained minimization problem $\\min_{\\|x\\|_2 = 1} q(x)$. Starting only from the fundamental definitions of eigenpairs and Rayleigh quotient for symmetric matrices, derive how to compute a vector $x_\\star$ that solves this problem by transforming it into an equivalent eigenvalue problem, and then explain how to convert $x_\\star$ into a numerical estimate $\\widehat{\\lambda}_2$ of the “next” eigenvalue of $A$ after $\\lambda_1$ (that is, the smallest eigenvalue of $A$ among vectors approximately orthogonal to $v_1$). Your program must implement this transformation and compute $\\widehat{\\lambda}_2$ as the Rayleigh quotient of $A$ at $x_\\star$.\n\nYour program must implement the following algorithmic task with numerical reliability:\n- Construct the rank-$1$ update $B \\;=\\; A \\;+\\; \\gamma \\, v_1 v_1^{\\top}$.\n- Compute a unit-norm eigenvector $x_\\star$ corresponding to the smallest eigenvalue of $B$.\n- Return the estimate $\\widehat{\\lambda}_2 \\;=\\; x_\\star^{\\top} A x_\\star$.\n\nUse the following test suite. For each case, compute and report the required output as specified.\n\n- Test $1$ (happy path, distinct spectrum):\n  - $A_1 \\,=\\, \\mathrm{diag}(1,\\,2,\\,4) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,1} \\,=\\, [1,\\,0,\\,0]^{\\top}$,\n  - $\\gamma_1 \\,=\\, 3$.\n  - Output: a single float equal to $\\widehat{\\lambda}_2$.\n\n- Test $2$ (near-degenerate first two eigenvalues):\n  - $A_2 \\,=\\, \\mathrm{diag}(1.0,\\,1.001,\\,5.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,2} \\,=\\, [1,\\,0,\\,0]^{\\top}$,\n  - $\\gamma_2 \\,=\\, 0.01$.\n  - Output: a single float equal to $\\widehat{\\lambda}_2$.\n\n- Test $3$ (exact multiplicity in the smallest eigenvalue):\n  - $A_3 \\,=\\, \\mathrm{diag}(2.0,\\,2.0,\\,7.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,3} \\,=\\, [1,\\,0,\\,0]^{\\top}$,\n  - $\\gamma_3 \\,=\\, 5$.\n  - Output: a single float equal to $\\widehat{\\lambda}_2$.\n\n- Test $4$ (inexact deflation direction):\n  - $A_4 \\,=\\, \\mathrm{diag}(1.0,\\,2.0,\\,3.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - Let the angle be $\\theta \\,=\\, 5$ degrees (angles must be interpreted in degrees),\n  - $v_{1,4} \\,=\\, [\\cos(\\theta),\\,\\sin(\\theta),\\,0]^{\\top}$ normalized to unit length,\n  - $\\gamma_4 \\,=\\, 10$.\n  - Output: a single float equal to $\\widehat{\\lambda}_2$.\n\n- Test $5$ (failure case with too-small penalty):\n  - $A_5 \\,=\\, \\mathrm{diag}(1.0,\\,1.001,\\,3.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,5} \\,=\\, [1,\\,0,\\,0]^{\\top}$,\n  - $\\gamma_5 \\,=\\, 0.0001$.\n  - Output: a single boolean defined as follows. Let $\\widehat{\\lambda}_2$ be computed as above, and let $\\lambda_1 \\,=\\, 1.0$ and $\\lambda_2 \\,=\\, 1.001$. Output the boolean value of the expression $\\big|\\widehat{\\lambda}_2 - \\lambda_2\\big| < \\big|\\widehat{\\lambda}_2 - \\lambda_1\\big|$.\n\nImportant implementation details:\n- All matrices are real and symmetric. All vectors must be normalized to unit length where required.\n- You must not rely on any randomness; use deterministic linear algebra routines.\n- Angles, where present, must be interpreted in degrees as specified.\n- There are no physical units in this problem.\n\nFinal output format:\n- Your program should produce a single line of output containing the results of Tests $1$ through $5$ as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$. For Tests $1$–$4$, each $\\text{result}_k$ is a float. For Test $5$, $\\text{result}_5$ is a boolean. No additional text must be printed.", "solution": "The problem presented is a valid exercise in numerical linear algebra, specifically concerning deflation techniques for eigenvalue problems. We will proceed with a formal derivation.\n\nThe stated objective is to solve the constrained minimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^n, \\|x\\|_2 = 1} q(x)\n$$\nwhere the objective function is the penalized Rayleigh quotient:\n$$\nq(x) = \\frac{x^{\\top} A x}{x^{\\top} x} + \\gamma (v_1^{\\top} x)^2\n$$\nHere, $A \\in \\mathbb{R}^{n \\times n}$ is a real, symmetric matrix, $v_1 \\in \\mathbb{R}^n$ is a given unit vector approximating the eigenvector for the smallest eigenvalue of $A$, and $\\gamma > 0$ is a scalar penalty parameter.\n\nFirst, we formalize the problem. The constraint $\\|x\\|_2 = 1$ implies $x^{\\top} x = 1$. Substituting this into the objective function simplifies the problem to minimizing the following function on the unit sphere:\n$$\nf(x) = x^{\\top} A x + \\gamma (v_1^{\\top} x)^2 \\quad \\text{subject to} \\quad x^{\\top} x = 1\n$$\nThe penalty term, $\\gamma (v_1^{\\top} x)^2$, can be rewritten as a quadratic form. Since $v_1^{\\top} x$ is a scalar, its square is $(v_1^{\\top} x)(v_1^{\\top} x) = (x^{\\top} v_1)(v_1^{\\top} x)$. By associativity of matrix multiplication, this expression is equivalent to $x^{\\top} (v_1 v_1^{\\top}) x$. The term $v_1 v_1^{\\top}$ is the outer product of the vector $v_1$ with itself, which is a symmetric rank-one matrix.\n\nSubstituting this back into the expression for $f(x)$, we can combine the two quadratic forms:\n$$\nf(x) = x^{\\top} A x + \\gamma x^{\\top} (v_1 v_1^{\\top}) x = x^{\\top} (A + \\gamma v_1 v_1^{\\top}) x\n$$\nLet us define a new matrix $B = A + \\gamma v_1 v_1^{\\top}$. Since $A$ is symmetric and $v_1 v_1^{\\top}$ is symmetric, their sum $B$ is also a real, symmetric matrix. The minimization problem is now transformed into:\n$$\n\\min_{x^{\\top} x = 1} x^{\\top} B x\n$$\nThis is the standard problem of finding the minimum value of the Rayleigh quotient for the matrix $B$. By the Rayleigh-Ritz theorem, for any symmetric matrix $B$, the minimum value of the quotient $\\frac{x^{\\top} B x}{x^{\\top} x}$ is the smallest eigenvalue of $B$, denoted $\\mu_{\\min}(B)$. This minimum is achieved if and only if $x$ is a corresponding eigenvector. Therefore, the vector $x_\\star$ that solves our original minimization problem is the unit-norm eigenvector of $B$ associated with its smallest eigenvalue.\n\nNow, we must understand how this procedure relates to finding the \"next\" eigenvalue of $A$. Let the eigenpairs of $A$ be $(\\lambda_i, u_i)$ for $i=1, \\dots, n$, ordered such that $\\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$. The vector $v_1$ is given as an approximation to $u_1$.\n\nConsider the ideal case where the approximation is exact, i.e., $v_1 = u_1$. The matrix is $B = A + \\gamma u_1 u_1^{\\top}$. Let us examine the action of $B$ on the eigenvectors of $A$:\n1. For any eigenvector $u_i$ of $A$ with $i > 1$, we assume it is orthogonal to $u_1$ (which is always possible for symmetric matrices, even with repeated eigenvalues). Then $u_1^{\\top} u_i = 0$. Applying $B$ to $u_i$ yields:\n$$\nB u_i = A u_i + \\gamma u_1 (u_1^{\\top} u_i) = \\lambda_i u_i + \\gamma u_1 (0) = \\lambda_i u_i\n$$\nThis shows that $(\\lambda_i, u_i)$ for $i=2, \\dots, n$ are also eigenpairs of $B$.\n\n2. For the eigenvector $u_1$, we have $u_1^{\\top} u_1 = 1$ since it is a unit vector. Applying $B$ to $u_1$ gives:\n$$\nB u_1 = A u_1 + \\gamma u_1 (u_1^{\\top} u_1) = \\lambda_1 u_1 + \\gamma u_1(1) = (\\lambda_1 + \\gamma) u_1\n$$\nSo, $(\\lambda_1 + \\gamma, u_1)$ is an eigenpair of $B$.\n\nThe spectrum of $B$ is therefore $\\{\\lambda_1 + \\gamma, \\lambda_2, \\lambda_3, \\dots, \\lambda_n\\}$. The purpose of the penalty term is to \"shift\" the eigenvalue corresponding to the known eigenvector direction $u_1$ upwards, exposing the next eigenvalue, $\\lambda_2$, as the minimum. For this to occur, the smallest eigenvalue of $B$ must be $\\lambda_2$. This requires that $\\lambda_2 < \\lambda_1 + \\gamma$. Rearranging, the penalty parameter $\\gamma$ must be chosen to satisfy $\\gamma > \\lambda_2 - \\lambda_1$. If this condition holds, the smallest eigenvalue of $B$ is $\\lambda_2$, and its corresponding eigenvector is $u_2$.\n\nIn the general case where $v_1$ is only an approximation to $u_1$, the eigenvectors of $A$ (other than $u_1$) are not exactly eigenvectors of $B$. However, for a sufficiently large $\\gamma$, the eigenvalue of $B$ associated with the direction $v_1$ is significantly increased. The eigenvector $x_\\star$ corresponding to the smallest eigenvalue of $B$ will then be an approximation of $u_2$, the eigenvector of $A$ for the second-smallest eigenvalue $\\lambda_2$. The quality of this approximation depends on the accuracy of $v_1$ and the gap between eigenvalues.\n\nFinally, to obtain a numerical estimate $\\widehat{\\lambda}_2$ of the eigenvalue $\\lambda_2$, we use the vector $x_\\star$ we have found. The best estimate for an eigenvalue of $A$ given an approximate eigenvector $x_\\star$ is its Rayleigh quotient with respect to $A$. As $x_\\star$ is a unit vector, this is simply:\n$$\n\\widehat{\\lambda}_2 = x_\\star^{\\top} A x_\\star\n$$\nThis completes the derivation. The algorithm is precisely as stated in the problem: construct $B = A + \\gamma v_1 v_1^{\\top}$, find the eigenvector $x_\\star$ corresponding to the smallest eigenvalue of $B$, and compute the Rayleigh quotient $\\widehat{\\lambda}_2 = x_\\star^{\\top} A x_\\star$ as the desired estimate. Test $5$ demonstrates the failure of the method when $\\gamma$ is too small to satisfy the condition $\\gamma > \\lambda_2 - \\lambda_1$, in which case the minimizer $x_\\star$ remains aligned with the direction of $v_1$ instead of shifting to approximate $u_2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the series of eigenvalue deflation problems.\n    \"\"\"\n\n    test_cases = [\n        # Test 1: Happy path\n        {\n            'A': np.diag([1.0, 2.0, 4.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 3.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 2: Near-degenerate eigenvalues\n        {\n            'A': np.diag([1.0, 1.001, 5.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 0.01,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 3: Exact multiplicity\n        {\n            'A': np.diag([2.0, 2.0, 7.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 5.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 4: Inexact deflation direction\n        {\n            'A': np.diag([1.0, 2.0, 3.0]),\n            'theta_deg': 5.0,\n            'gamma': 10.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 5: Failure case with too-small penalty\n        {\n            'A': np.diag([1.0, 1.001, 3.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 0.0001,\n            'task': 'check_closeness',\n            'lambda1': 1.0,\n            'lambda2': 1.001\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case['A']\n        gamma = case['gamma']\n\n        # Construct the deflation vector v1\n        if 'v1_unnormalized' in case:\n            v1_unnormalized = case['v1_unnormalized']\n        else:  # Test 4, construct from angle\n            theta_rad = np.deg2rad(case['theta_deg'])\n            v1_unnormalized = np.array([np.cos(theta_rad), np.sin(theta_rad), 0.0])\n        \n        # Normalize v1 to ensure it is a unit vector\n        norm_v1 = np.linalg.norm(v1_unnormalized)\n        if norm_v1 == 0:\n            # Handle potential zero vector, though not expected from problem spec\n            v1 = v1_unnormalized\n        else:\n            v1 = v1_unnormalized / norm_v1\n\n        # 1. Construct the rank-1 updated matrix B\n        # B = A + gamma * v1 * v1^T\n        B = A + gamma * np.outer(v1, v1)\n\n        # 2. Compute the eigenvector x_star for the smallest eigenvalue of B.\n        # np.linalg.eigh returns eigenvalues in ascending order and corresponding\n        # eigenvectors as columns.\n        eigenvalues_B, eigenvectors_B = np.linalg.eigh(B)\n        x_star = eigenvectors_B[:, 0]\n\n        # 3. Compute the estimate lambda_hat_2 as the Rayleigh quotient of A at x_star.\n        # Since x_star is a unit vector, this is x_star^T * A * x_star.\n        lambda_hat_2 = x_star.T @ A @ x_star\n        \n        if case['task'] == 'compute_lambda_hat_2':\n            results.append(lambda_hat_2)\n        elif case['task'] == 'check_closeness':\n            lambda1 = case['lambda1']\n            lambda2 = case['lambda2']\n            # Evaluate the boolean expression |lambda_hat_2 - lambda2| < |lambda_hat_2 - lambda1|\n            is_closer = np.abs(lambda_hat_2 - lambda2) < np.abs(lambda_hat_2 - lambda1)\n            results.append(bool(is_closer))\n\n    # Format the final output string as specified in the problem\n    # Example: [2.0,1.001,2.0,2.0076..,False]\n    # The map(str, ...) correctly handles float and bool types.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2383531"}]}