{"hands_on_practices": [{"introduction": "The power of Singular Value Decomposition in data approximation stems from the Eckart-Young-Mirsky theorem, which guarantees that the truncated SVD provides the optimal low-rank approximation for a given matrix. This theorem also gives us a beautifully simple formula for the approximation error, stating that the error in the spectral norm is simply the first truncated singular value, $\\sigma_{k+1}$. This practice [@problem_id:1071275] provides a direct, hands-on calculation to solidify your understanding of this fundamental error bound by applying it to a well-structured matrix.", "problem": "Let $A$ be a real $m \\times n$ matrix. The Singular Value Decomposition (SVD) of $A$ is a factorization of the form $A = U\\Sigma V^T$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal. The diagonal entries $\\sigma_i$ of $\\Sigma$ are known as the singular values of $A$ and are conventionally ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r = \\text{rank}(A)$.\n\nThe truncated SVD provides a low-rank approximation of $A$. The best rank-$k$ approximation of $A$, denoted $A_k$, is given by $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$, where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nThe Eckart-Young-Mirsky theorem states that $A_k$ is the optimal rank-$k$ approximation of $A$ with respect to the spectral norm (matrix 2-norm), $\\| \\cdot \\|_2$. The error of this approximation is given by the first truncated singular value:\n$$\n\\|A - A_k\\|_2 = \\sigma_{k+1}\n$$\n\nConsider the 3x3 symmetric Pascal matrix, $P_3$, defined by the binomial coefficients $P_{ij} = \\binom{i+j-2}{i-1}$ for $i,j \\in \\{1, 2, 3\\}$.\n\nDetermine the spectral norm of the error, $\\|P_3 - (P_3)_k\\|_2$, when approximating the matrix $P_3$ with its best rank-$k$ approximation $(P_3)_k$ for $k=2$.", "solution": "1. The Eckart–Young–Mirsky theorem gives \n$$\\|P_3 - (P_3)_k\\|_2 = \\sigma_{k+1},$$ \nwhere $\\sigma_i$ are the singular values of $P_3$, ordered $\\sigma_1\\ge\\sigma_2\\ge\\sigma_3>0$.\n2. For the symmetric matrix \n$$P_3=\\begin{pmatrix}1&1&1\\\\1&2&3\\\\1&3&6\\end{pmatrix}$$ \nthe singular values equal its eigenvalues.  The characteristic polynomial is\n$$\\det(P_3-\\lambda I)=\\lambda^3-9\\lambda^2+9\\lambda-1=0.$$\n3. Factoring out $(\\lambda-1)$ yields\n$$\\lambda^3-9\\lambda^2+9\\lambda-1=(\\lambda-1)(\\lambda^2-8\\lambda+1),$$\nso the eigenvalues are \n$$\\lambda=1,\\quad \\lambda=4\\pm\\sqrt{15}.$$\n4. Ordering $\\sigma_1=4+\\sqrt{15}\\ge\\sigma_2=1\\ge\\sigma_3=4-\\sqrt{15}$, for $k=2$ the error norm is\n$$\\|P_3-(P_3)_2\\|_2=\\sigma_3=4-\\sqrt{15}.$$", "answer": "$$\\boxed{4-\\sqrt{15}}$$", "id": "1071275"}, {"introduction": "Beyond quantifying the global error, it is crucial to understand the structural properties of the low-rank approximation itself. A rank-$k$ approximation, $A_k$, captures the most significant energy of the original matrix $A$, but does it preserve other features like sparsity? This computational exercise [@problem_id:2435626] lets you explore this question by analyzing how the energy of a rank-1 approximation, $A_1 = \\sigma_1 u_1 v_1^T$, can \"leak\" into regions where the original matrix was zero, a critical insight for applications in image and signal processing.", "problem": "You are given the mathematical task of evaluating how the best rank-$1$ approximation of an image-like matrix may introduce content outside the original nonzero support. Let $A \\in \\mathbb{R}^{m \\times n}$ be a real matrix. Define its best rank-$1$ approximation $A_1$ to be any minimizer of the Frobenius-norm error\n$$\n\\lVert A - X \\rVert_F \\quad \\text{over all matrices } X \\in \\mathbb{R}^{m \\times n} \\text{ with } \\operatorname{rank}(X) = 1.\n$$\nLet $M \\in \\{0,1\\}^{m \\times n}$ denote the support indicator of $A$, defined by $M_{ij} = 1$ if and only if $A_{ij} \\neq 0$, and $M_{ij} = 0$ otherwise. Define the misleading energy ratio $r(A)$ as\n$$\nr(A) \\coloneqq \\begin{cases}\n\\dfrac{\\lVert ( \\mathbf{1} - M ) \\odot A_1 \\rVert_F^2}{\\lVert A_1 \\rVert_F^2}, & \\text{if } \\lVert A_1 \\rVert_F \\neq 0, \\\\\n0, & \\text{if } \\lVert A_1 \\rVert_F = 0,\n\\end{cases}\n$$\nwhere $\\odot$ denotes the Hadamard product and $\\mathbf{1}$ denotes the all-ones matrix of size $m \\times n$. The ratio $r(A)$ quantifies the fraction of the energy of the best rank-$1$ approximation $A_1$ that is located strictly outside the original support of $A$. All computations are to be carried out over the real numbers.\n\nYour task is to compute $r(A)$ for the following test suite of four matrices, each of size $m = n = 8$:\n\n- Test case $1$ ($8 \\times 8$): A vertical bar. Define $A$ by $A_{ij} = 1$ for all row indices $i \\in \\{2,3,4,5,6,7\\}$ and column index $j = 3$, and $A_{ij} = 0$ otherwise.\n\n- Test case $2$ ($8 \\times 8$): A $2 \\times 2$ pattern embedded in the top-left corner. Set the entries\n$$\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 \\\\\n1 & 1\n\\end{bmatrix},\n$$\nand $A_{ij} = 0$ for all other indices.\n\n- Test case $3$ ($8 \\times 8$): A different $2 \\times 2$ pattern embedded in the top-left corner. Set the entries\n$$\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{bmatrix},\n$$\nand $A_{ij} = 0$ for all other indices.\n\n- Test case $4$ ($8 \\times 8$): The zero matrix. Set $A_{ij} = 0$ for all indices.\n\nAll indices are one-based in the above description. There are no physical units involved. Angles do not appear. Percentages must not be used anywhere in your output.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$. Each number must be rounded to $6$ decimal places as a decimal (for example, $0.123456$). The required format is\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n$$", "solution": "The problem is well-defined and grounded in the established principles of linear algebra. The core of the task lies in applying the Eckart-Young-Mirsky theorem, which provides the solution to the low-rank approximation problem. I will proceed with a formal solution.\n\nThe problem asks for the computation of the misleading energy ratio $r(A)$ for four given matrices. The ratio is defined as:\n$$\nr(A) \\coloneqq \\begin{cases}\n\\dfrac{\\lVert ( \\mathbf{1} - M ) \\odot A_1 \\rVert_F^2}{\\lVert A_1 \\rVert_F^2}, & \\text{if } \\lVert A_1 \\rVert_F \\neq 0,\\\\\n0, & \\text{if } \\lVert A_1 \\rVert_F = 0,\n\\end{cases}\n$$\nwhere $A_1$ is the best rank-$1$ approximation of a matrix $A \\in \\mathbb{R}^{m \\times n}$, $M$ is the support indicator of $A$, $\\mathbf{1}$ is the all-ones matrix, and $\\odot$ is the Hadamard product.\n\nThe foundation for finding $A_1$ is the Eckart-Young-Mirsky theorem. For any matrix $A$ with singular value decomposition (SVD) $A = U \\Sigma V^T$, where the singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$ are the diagonal entries of $\\Sigma$, the best rank-$k$ approximation in the Frobenius norm is given by $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$. For this problem, we require the best rank-$1$ approximation, which corresponds to the largest singular value $\\sigma_1$ and its associated left and right singular vectors $u_1$ and $v_1$. Thus,\n$$\nA_1 = \\sigma_1 u_1 v_1^T.\n$$\nThe Frobenius norm of $A_1$ is directly related to $\\sigma_1$. Since $u_1$ and $v_1$ are orthonormal vectors, we have:\n$$\n\\lVert A_1 \\rVert_F^2 = \\lVert \\sigma_1 u_1 v_1^T \\rVert_F^2 = \\sigma_1^2 \\operatorname{Tr}((u_1 v_1^T)^T (u_1 v_1^T)) = \\sigma_1^2 \\operatorname{Tr}(v_1 u_1^T u_1 v_1^T) = \\sigma_1^2 \\operatorname{Tr}(v_1 (u_1^T u_1) v_1^T) = \\sigma_1^2 \\operatorname{Tr}(v_1 v_1^T) = \\sigma_1^2 \\operatorname{Tr}(v_1^T v_1) = \\sigma_1^2.\n$$\nThe denominator in the expression for $r(A)$ is simply $\\sigma_1^2$. The problem reduces to computing the SVD of each matrix $A$, constructing $A_1$, and then calculating the required norms.\n\nAll matrices are of size $m=n=8$. One-based indexing in the problem description is converted to zero-based for computation.\n\n**Test Case 1: Vertical bar**\nThe matrix $A$ has entries $A_{ij} = 1$ for $i \\in \\{1, 2, 3, 4, 5, 6\\}$ and $j=2$ (zero-based), and $A_{ij}=0$ otherwise. This matrix can be written as the outer product of two vectors: $A = c e_2^T$, where $c$ is a column vector with ones in entries $1$ through $6$, and $e_2$ is the standard basis vector for column index $2$. As $A$ is already a rank-$1$ matrix, its best rank-$1$ approximation is itself, i.e., $A_1 = A$. The support indicator $M$ has ones precisely where $A$ has ones. Consequently, the matrix $(\\mathbf{1} - M)$ has zeros where $A_1$ has non-zero values. The Hadamard product $(\\mathbf{1} - M) \\odot A_1$ is therefore the zero matrix. The numerator $\\lVert (\\mathbf{1} - M) \\odot A_1 \\rVert_F^2$ is $0$. Therefore, $r(A) = 0$.\n\n**Test Case 2: $2 \\times 2$ pattern**\nThe matrix $A$ has a non-zero block in the top-left corner given by $A' = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$. The singular values of $A$ are those of $A'$ plus additional zeros. To find the singular values of $A'$, we compute the eigenvalues of $A' A'^T$:\n$$\nA' A'^T = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nThe characteristic equation is $\\lambda^2 - 3\\lambda + 1 = 0$, yielding eigenvalues $\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}$. The largest singular value squared is $\\sigma_1^2 = \\frac{3+\\sqrt{5}}{2}$, so $\\sigma_1 = \\frac{1+\\sqrt{5}}{2}$, which is the golden ratio $\\phi$. The best rank-$1$ approximation $A_1$ will have its non-zero entries confined to the same $2 \\times 2$ block, which we call $A'_1$.\nThe SVD components for $A'$ are $u'_1 \\propto \\begin{pmatrix} 1 \\\\ \\sigma_1 \\end{pmatrix}$ and $v'_1 \\propto \\begin{pmatrix} \\sigma_1 \\\\ 1 \\end{pmatrix}$.\nThe approximation is $A'_1 = \\frac{\\sigma_1}{1+\\sigma_1^2} \\begin{pmatrix} \\sigma_1 & 1 \\\\ \\sigma_1^2 & \\sigma_1 \\end{pmatrix}$.\nThe original matrix $A$ has a zero at entry $(0,1)$ (zero-based). The support $M$ has $M_{0,1}=0$. The \"misleading\" energy is thus the squared value of the entry $(A_1)_{0,1}$.\n$$\n(A_1)_{0,1} = (A'_1)_{0,1} = \\frac{\\sigma_1}{1+\\sigma_1^2}.\n$$\nThe ratio $r(A)$ is the squared value of this entry divided by the total energy $\\sigma_1^2$:\n$$\nr(A) = \\frac{ \\left( \\frac{\\sigma_1}{1+\\sigma_1^2} \\right)^2 }{ \\sigma_1^2 } = \\frac{1}{(1+\\sigma_1^2)^2}.\n$$\nSubstituting $\\sigma_1^2 = \\frac{3+\\sqrt{5}}{2}$, we get $1+\\sigma_1^2 = \\frac{5+\\sqrt{5}}{2}$.\n$$\nr(A) = \\frac{1}{\\left(\\frac{5+\\sqrt{5}}{2}\\right)^2} = \\frac{4}{(5+\\sqrt{5})^2} = \\frac{4}{30+10\\sqrt{5}} = \\frac{2}{15+5\\sqrt{5}} = \\frac{3-\\sqrt{5}}{10}.\n$$\nNumerically, this value is approximately $0.076393$.\n\n**Test Case 3: Transposed $2 \\times 2$ pattern**\nThe matrix $A^{(3)}$ in this case has its non-zero block as $A'^{(3)} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$, which is the transpose of the block from Case 2, i.e., $A'^{(3)} = (A'^{(2)})^T$. The SVD of a transposed matrix $A^T = (U\\Sigma V^T)^T = V \\Sigma^T U^T$ shows that it has the same singular values. Thus, $\\sigma_1$ for this case is identical to that of Case 2. The best rank-$1$ approximation is $A_1^{(3)} = (A_1^{(2)})^T$. The support $M^{(3)}$ is the transpose of $M^{(2)}$. The zero in the $2 \\times 2$ block of $A^{(3)}$ is at position $(1,0)$, whereas for $A^{(2)}$ it was at $(0,1)$. The misleading energy for $A^{(3)}$ comes from the entry $(A_1^{(3)})_{1,0}$. By the transpose property, $(A_1^{(3)})_{1,0} = (A_1^{(2)})_{0,1}$. This is the same value that generated the misleading energy in Case 2. Since the numerator (squared leaked energy) and the denominator ($\\sigma_1^2$) are identical for both cases, the ratio $r(A)$ must also be identical.\n$$\nr(A) = \\frac{3-\\sqrt{5}}{10} \\approx 0.076393.\n$$\n\n**Test Case 4: The zero matrix**\nThe matrix $A$ is the $8 \\times 8$ zero matrix. All its singular values are $0$. Thus, $\\sigma_1=0$. The best rank-$1$ approximation is $A_1 = 0 \\cdot u_1 v_1^T = 0$. The Frobenius norm $\\lVert A_1 \\rVert_F = 0$. According to the problem's definition for this scenario, $r(A) = 0$.\n\nThe final results are therefore $[0.0, \\frac{3-\\sqrt{5}}{10}, \\frac{3-\\sqrt{5}}{10}, 0.0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the misleading energy ratio r(A) for a suite of test matrices.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Note: Problem uses 1-based indexing, Python uses 0-based.\n    \n    # Test case 1: A vertical bar.\n    A1 = np.zeros((8, 8))\n    A1[1:7, 2] = 1.0  # Rows 2 to 7 (exclusive) and column 3 (1-based)\n                       # corresponds to rows 1 to 6 and column 2 (0-based)\n                       \n    # Test case 2: A 2x2 pattern.\n    A2 = np.zeros((8, 8))\n    A2[0, 0] = 1.0\n    A2[1, 0] = 1.0\n    A2[1, 1] = 1.0\n    \n    # Test case 3: A different 2x2 pattern.\n    A3 = np.zeros((8, 8))\n    A3[0, 0] = 1.0\n    A3[0, 1] = 1.0\n    A3[1, 1] = 1.0\n\n    # Test case 4: The zero matrix.\n    A4 = np.zeros((8, 8))\n\n    test_cases = [A1, A2, A3, A4]\n    results = []\n\n    for A in test_cases:\n        # Check if the matrix is a zero matrix.\n        if not np.any(A):\n            results.append(0.0)\n            continue\n\n        # Step 1: Compute the Singular Value Decomposition (SVD).\n        # U has shape (m, m), s has shape (k,), Vh has shape (n, n)\n        # where k = min(m, n).\n        U, s, Vh = np.linalg.svd(A)\n\n        # Step 2: Identify the components for the rank-1 approximation.\n        sigma_1 = s[0]\n        \n        # If the largest singular value is zero, the matrix A was zero,\n        # and A1 is also zero. The ratio is 0. This is handled by the initial check.\n        # This is for numerical stability, though not strictly needed for given cases.\n        if np.isclose(sigma_1, 0):\n            results.append(0.0)\n            continue\n            \n        u_1 = U[:, 0]\n        vh_1 = Vh[0, :]\n\n        # Step 3: Construct the best rank-1 approximation A_1.\n        A_1 = sigma_1 * np.outer(u_1, vh_1)\n        \n        # Step 4: Construct the support indicator M and its complement.\n        M = (A != 0).astype(float)\n        One_minus_M = 1.0 - M\n\n        # Step 5: Compute the misleading energy.\n        # The Hadamard product is the element-wise '*' operator in numpy.\n        B = One_minus_M * A_1\n        \n        # Numerator: squared Frobenius norm of the energy outside the support.\n        numerator = np.linalg.norm(B, 'fro')**2\n        \n        # Denominator: squared Frobenius norm of A_1, which is sigma_1^2.\n        denominator = sigma_1**2\n\n        # Step 6: Compute the ratio r(A).\n        ratio = numerator / denominator\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    # Round each result to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2435626"}, {"introduction": "Low-rank approximation is the engine behind Principal Component Analysis (PCA), a ubiquitous technique for dimensionality reduction and feature extraction. However, the theoretical optimality of PCA holds for idealized data; in the real world, datasets are often contaminated with outliers. This advanced exercise [@problem_id:2435636] is a thought experiment that guides you to analytically determine how a single, large outlier can dominate the SVD and fundamentally alter the computed principal components, revealing the non-robust nature of classical PCA.", "problem": "Consider a dataset in $\\mathbb{R}^d$ constructed as follows. There are $N$ baseline samples $x_i \\in \\mathbb{R}^d$ of the form $x_i = s_i a$ for $i \\in \\{1,\\dots,N\\}$, where $a \\in \\mathbb{R}^d$ satisfies $\\|a\\|_2 = 1$, the scalars $s_i \\in \\mathbb{R}$ satisfy $\\sum_{i=1}^N s_i = 0$, and $\\sum_{i=1}^N s_i^2$ is finite. An additional outlier sample $x_{N+1} \\in \\mathbb{R}^d$ is appended with $x_{N+1} = M c$, where $c \\in \\mathbb{R}^d$ satisfies $\\|c\\|_2 = 1$ and $a^\\top c = 0$, and $M > 0$ is a scalar parameter. Let $X \\in \\mathbb{R}^{(N+1)\\times d}$ be the data matrix whose ($i$-th) row is $x_i^\\top$. Let $\\bar{x} \\in \\mathbb{R}^d$ denote the sample mean $\\bar{x} = \\frac{1}{N+1}\\sum_{i=1}^{N+1} x_i$, and define the column-centered data matrix $X' = X - \\mathbf{1}\\bar{x}^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^{N+1}$ is the all-ones vector. Let the Singular Value Decomposition (SVD) be $X' = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{(N+1)\\times (N+1)}$, $\\Sigma \\in \\mathbb{R}^{(N+1)\\times d}$, and $V \\in \\mathbb{R}^{d\\times d}$, with nonnegative singular values in nonincreasing order on the diagonal of $\\Sigma$. In Principal Component Analysis (PCA), the first principal component is the first column of $V$ (the dominant right singular vector), and the largest singular value equals the square root of the dominant eigenvalue of $(X')^\\top X'$.\n\nWhich of the following statements about how the outlier affects the principal components and the low-rank structure of $X'$ are true?\n\nA. For sufficiently large $M$, the first principal component aligns with $c$.\n\nB. After centering by subtracting $\\bar{x}$, the outlier’s contribution cancels, so the first principal component necessarily remains aligned with $a$ for all $M$.\n\nC. There is a switching threshold determined by $M^2 \\frac{N}{N+1} = \\sum_{i=1}^N s_i^2$ at which the first principal component changes alignment between $a$ and $c$.\n\nD. For large $M$, the largest singular value of $X'$ scales as $\\sqrt{\\frac{N}{N+1}}\\,M$ up to lower-order terms in $M$.\n\nE. If $a^\\top c = 0$, then the top two right singular vectors of $X'$ need not be orthogonal.\n\nF. As $M \\to \\infty$, the squared error of the best rank-$1$ approximation (in the Frobenius norm induced by $X'$) converges to $\\sum_{i=1}^N s_i^2$.", "solution": "We begin by analyzing the structure of the centered data matrix, $X'$. The principal components of the data are the eigenvectors of the covariance-like matrix $S = (X')^\\top X'$. The analysis proceeds as follows.\n\nFirst, we compute the sample mean, $\\bar{x}$:\n$$ \\bar{x} = \\frac{1}{N+1} \\sum_{i=1}^{N+1} x_i = \\frac{1}{N+1} \\left( \\sum_{i=1}^N x_i + x_{N+1} \\right) $$\nGiven $x_i = s_i a$ for $i \\in \\{1,\\dots,N\\}$, $x_{N+1} = M c$, and the condition $\\sum_{i=1}^N s_i = 0$, this simplifies to:\n$$ \\bar{x} = \\frac{1}{N+1} \\left( \\left(\\sum_{i=1}^N s_i\\right) a + M c \\right) = \\frac{1}{N+1} (0 \\cdot a + M c) = \\frac{M}{N+1} c $$\n\nNext, we define the centered data points, $x_i' = x_i - \\bar{x}$.\nFor $i \\in \\{1, \\dots, N\\}$:\n$$ x_i' = s_i a - \\frac{M}{N+1} c $$\nFor $i = N+1$:\n$$ x_{N+1}' = M c - \\frac{M}{N+1} c = \\left( M - \\frac{M}{N+1} \\right) c = \\frac{M(N+1) - M}{N+1} c = \\frac{MN}{N+1} c $$\nThe centered data matrix $X'$ has rows $(x_i')^\\top$.\n\nThe principal components are the eigenvectors of $S = (X')^\\top X'$. We compute this matrix:\n$$ S = \\sum_{i=1}^{N+1} x_i' (x_i')^\\top = \\sum_{i=1}^{N} x_i' (x_i')^\\top + x_{N+1}' (x_{N+1}')^\\top $$\nThe sum over the first $N$ points is:\n$$ \\sum_{i=1}^{N} \\left( s_i a - \\frac{M}{N+1} c \\right) \\left( s_i a - \\frac{M}{N+1} c \\right)^\\top $$\n$$ = \\sum_{i=1}^{N} \\left( s_i^2 a a^\\top - \\frac{s_i M}{N+1} a c^\\top - \\frac{s_i M}{N+1} c a^\\top + \\left(\\frac{M}{N+1}\\right)^2 c c^\\top \\right) $$\nUsing $\\sum_{i=1}^N s_i = 0$ and distributing the summation:\n$$ = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top - \\frac{M}{N+1}\\left(\\sum_{i=1}^{N} s_i\\right) (a c^\\top + c a^\\top) + N \\left(\\frac{M}{N+1}\\right)^2 c c^\\top $$\n$$ = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + N \\frac{M^2}{(N+1)^2} c c^\\top $$\nNow, adding the term for $x_{N+1}'$:\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + N \\frac{M^2}{(N+1)^2} c c^\\top + \\left(\\frac{MN}{N+1}\\right)^2 c c^\\top $$\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + \\left( N \\frac{M^2}{(N+1)^2} + \\frac{M^2 N^2}{(N+1)^2} \\right) c c^\\top $$\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + \\frac{M^2 N(1+N)}{(N+1)^2} c c^\\top $$\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + \\frac{M^2 N}{N+1} c c^\\top $$\nLet $\\lambda_a = \\sum_{i=1}^N s_i^2$ and $\\lambda_c(M) = M^2 \\frac{N}{N+1}$. The matrix is $S = \\lambda_a a a^\\top + \\lambda_c(M) c c^\\top$.\nSince $a$ and $c$ are orthonormal ($a^\\top c = 0$, $\\|a\\|_2 = 1$, $\\|c\\|_2=1$), they are eigenvectors of $S$:\n$S a = (\\lambda_a a a^\\top + \\lambda_c(M) c c^\\top) a = \\lambda_a a (a^\\top a) = \\lambda_a a$.\n$S c = (\\lambda_a a a^\\top + \\lambda_c(M) c c^\\top) c = \\lambda_c(M) c (c^\\top c) = \\lambda_c(M) c$.\nThe eigenvalues are $\\lambda_a$ and $\\lambda_c(M)$. Any vector $v$ in the orthogonal complement of the span of $\\{a, c\\}$ is an eigenvector with eigenvalue $0$. The principal components are the eigenvectors of $S$ corresponding to nonzero eigenvalues, ordered by the magnitude of the eigenvalue. The first principal component corresponds to the largest eigenvalue.\n\nNow we evaluate each statement.\n\nA. For sufficiently large $M$, the first principal component aligns with $c$.\nThe eigenvalue associated with vector $a$ is $\\lambda_a = \\sum_{i=1}^N s_i^2$, which is a finite constant. The eigenvalue associated with vector $c$ is $\\lambda_c(M) = M^2 \\frac{N}{N+1}$. As $M \\to \\infty$, $\\lambda_c(M)$ grows without bound, while $\\lambda_a$ remains constant. Therefore, for a sufficiently large $M$, $\\lambda_c(M) > \\lambda_a$. The largest eigenvalue will be $\\lambda_c(M)$, and the corresponding eigenvector (the first principal component) will be $c$. This statement is **Correct**.\n\nB. After centering by subtracting $\\bar{x}$, the outlier’s contribution cancels, so the first principal component necessarily remains aligned with $a$ for all $M$.\nThis assertion is false. The contribution of the outlier does not cancel. The centered outlier is $x_{N+1}' = \\frac{MN}{N+1} c$, which is non-zero. As shown in the analysis for option A, for large $M$, the first principal component aligns with $c$, not $a$. This statement is **Incorrect**.\n\nC. There is a switching threshold determined by $M^2 \\frac{N}{N+1} = \\sum_{i=1}^N s_i^2$ at which the first principal component changes alignment between $a$ and $c$.\nThe first principal component is determined by the larger of the two eigenvalues $\\lambda_a$ and $\\lambda_c(M)$. A switch in the identity of the first principal component occurs when these eigenvalues are equal, i.e., $\\lambda_a = \\lambda_c(M)$. Substituting the expressions for the eigenvalues, we get $\\sum_{i=1}^N s_i^2 = M^2 \\frac{N}{N+1}$. This equation defines the threshold value of $M$ for the switch. This statement is **Correct**.\n\nD. For large $M$, the largest singular value of $X'$ scales as $\\sqrt{\\frac{N}{N+1}}\\,M$ up to lower-order terms in $M$.\nThe singular values $\\sigma_j$ of $X'$ are the square roots of the eigenvalues of $S = (X')^\\top X'$. For large $M$, the largest eigenvalue is $\\lambda_{max} = \\lambda_c(M) = M^2 \\frac{N}{N+1}$. The corresponding largest singular value is $\\sigma_1 = \\sqrt{\\lambda_c(M)} = \\sqrt{M^2 \\frac{N}{N+1}} = M \\sqrt{\\frac{N}{N+1}}$. This expression is exactly what the statement claims, with no lower-order terms. This statement is **Correct**.\n\nE. If $a^\\top c = 0$, then the top two right singular vectors of $X'$ need not be orthogonal.\nThe right singular vectors of $X'$ are the columns of the matrix $V$ in the SVD $X' = U \\Sigma V^\\top$. By definition of the Singular Value Decomposition, the matrix $V$ is an orthogonal matrix. Its columns therefore form an orthonormal set of vectors. Consequently, any two distinct right singular vectors must be orthogonal. The statement is a direct contradiction of the properties of SVD. This statement is **Incorrect**.\n\nF. As $M \\to \\infty$, the squared error of the best rank-$1$ approximation (in the Frobenius norm induced by $X'$) converges to $\\sum_{i=1}^N s_i^2$.\nAccording to the Eckart-Young-Mirsky theorem, the squared Frobenius norm error of the best rank-$k$ approximation of $X'$ is the sum of the squares of the singular values from $k+1$ to the rank of the matrix. For a rank-$1$ approximation ($k=1$), this error is $\\sum_{j=2}^{\\text{rank}} \\sigma_j^2$. The squared singular values are the eigenvalues of $S$. As $M \\to \\infty$, the largest eigenvalue is $\\lambda_1 = \\lambda_c(M) = M^2 \\frac{N}{N+1}$ and the second-largest is $\\lambda_2 = \\lambda_a = \\sum_{i=1}^N s_i^2$. All other eigenvalues are $0$. The squared error is therefore $\\lambda_2 + \\lambda_3 + \\dots = \\lambda_a + 0 + \\dots = \\sum_{i=1}^N s_i^2$. This is a constant value. Thus, the error converges to this value. This statement is **Correct**.", "answer": "$$\\boxed{ACDF}$$", "id": "2435636"}]}