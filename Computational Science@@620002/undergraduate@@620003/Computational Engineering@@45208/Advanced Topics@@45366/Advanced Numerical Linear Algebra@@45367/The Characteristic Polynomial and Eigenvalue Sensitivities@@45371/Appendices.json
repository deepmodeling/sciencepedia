{"hands_on_practices": [{"introduction": "The properties of a matrix and its eigenvalues are deeply connected, but not always in the way one might first assume. For instance, while a real symmetric matrix is guaranteed to have real eigenvalues, the converse is not true. This exercise begins by having you confront this common misconception with a concrete counterexample, reinforcing the importance of rigorous proof over intuition. You will then practice a fundamental skill in sensitivity analysis: calculating how a specific eigenvalue responds to a small, targeted perturbation in the matrix [@problem_id:2443300].", "problem": "A real square matrix $A \\in \\mathbb{R}^{n \\times n}$ has a characteristic polynomial whose roots are the eigenvalues of $A$. Consider the following claim: if the characteristic polynomial of a real matrix $A$ has only real roots, then $A$ must be symmetric. Decide whether this claim is true or false and justify your conclusion.\n\nAs a concrete study, analyze the real, non-symmetric matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 0 & 6 \\\\\n1 & 0 & -11 \\\\\n0 & 1 & 6\n\\end{pmatrix}.\n$$\nFirst, compute the characteristic polynomial of $A$ and verify that it has only real roots. Then consider the one-parameter perturbation\n$$\nA(\\varepsilon) \\;=\\; A \\;+\\; \\varepsilon\\,B, \\quad \\text{where} \\quad B \\in \\mathbb{R}^{3 \\times 3} \\text{ has entries } B_{1,3}=1 \\text{ and } B_{i,j}=0 \\text{ for all other } (i,j).\n$$\nLet $\\lambda(\\varepsilon)$ denote the eigenvalue of $A(\\varepsilon)$ for which $\\lambda(0)=2$. Using only first principles and definitions, determine the value of the sensitivity $\\dfrac{d\\lambda}{d\\varepsilon}\\big|_{\\varepsilon=0}$.\n\nReport the value of $\\dfrac{d\\lambda}{d\\varepsilon}\\big|_{\\varepsilon=0}$ as your final answer. No rounding is required and no units are involved.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information. It is a valid problem in matrix analysis.\n\nThe first part of the problem requires an evaluation of the claim: \"if the characteristic polynomial of a real matrix $A$ has only real roots, then $A$ must be symmetric.\" This claim is false. A symmetric matrix ($A = A^T$) is a sufficient condition for all its eigenvalues to be real, but it is not a necessary condition. We can demonstrate the falsity of this claim using the provided matrix $A$ as a counterexample.\n\nThe matrix is given as:\n$$\nA = \\begin{pmatrix}\n0 & 0 & 6 \\\\\n1 & 0 & -11 \\\\\n0 & 1 & 6\n\\end{pmatrix}\n$$\nThe transpose of $A$ is:\n$$\nA^T = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n6 & -11 & 6\n\\end{pmatrix}\n$$\nSince $A \\neq A^T$, the matrix $A$ is non-symmetric.\n\nNext, we compute the characteristic polynomial $p(\\lambda) = \\det(A - \\lambda I)$:\n$$\np(\\lambda) = \\det \\begin{pmatrix}\n-\\lambda & 0 & 6 \\\\\n1 & -\\lambda & -11 \\\\\n0 & 1 & 6-\\lambda\n\\end{pmatrix}\n$$\nExpanding the determinant along the first row gives:\n$$\np(\\lambda) = (-\\lambda) \\begin{vmatrix} -\\lambda & -11 \\\\ 1 & 6-\\lambda \\end{vmatrix} - 0 \\begin{vmatrix} 1 & -11 \\\\ 0 & 6-\\lambda \\end{vmatrix} + 6 \\begin{vmatrix} 1 & -\\lambda \\\\ 0 & 1 \\end{vmatrix}\n$$\n$$\np(\\lambda) = -\\lambda(-\\lambda(6-\\lambda) - (-11)(1)) + 6(1(1) - 0)\n$$\n$$\np(\\lambda) = -\\lambda(-6\\lambda + \\lambda^2 + 11) + 6\n$$\n$$\np(\\lambda) = -\\lambda^3 + 6\\lambda^2 - 11\\lambda + 6\n$$\nTo find the roots of this polynomial, we can test integer factors of the constant term $6$. Testing $\\lambda=1$:\n$p(1) = -1^3 + 6(1)^2 - 11(1) + 6 = -1 + 6 - 11 + 6 = 0$. So, $\\lambda=1$ is a root.\nTesting $\\lambda=2$:\n$p(2) = -2^3 + 6(2)^2 - 11(2) + 6 = -8 + 24 - 22 + 6 = 0$. So, $\\lambda=2$ is a root.\nTesting $\\lambda=3$:\n$p(3) = -3^3 + 6(3)^2 - 11(3) + 6 = -27 + 54 - 33 + 6 = 0$. So, $\\lambda=3$ is a root.\nThe eigenvalues of $A$ are $\\lambda_1=1$, $\\lambda_2=2$, and $\\lambda_3=3$. All roots are real.\nSince the real, non-symmetric matrix $A$ has only real eigenvalues, the claim is proven false.\n\nThe second part of the problem concerns the sensitivity of the eigenvalue $\\lambda=2$ to a perturbation. The perturbed matrix is $A(\\varepsilon) = A + \\varepsilon B$, where\n$$\nB = \\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\quad \\implies \\quad\nA(\\varepsilon) = \\begin{pmatrix}\n0 & 0 & 6+\\varepsilon \\\\\n1 & 0 & -11 \\\\\n0 & 1 & 6\n\\end{pmatrix}\n$$\nThe eigenvalue problem for the perturbed system is $A(\\varepsilon) x(\\varepsilon) = \\lambda(\\varepsilon) x(\\varepsilon)$, where $\\lambda(\\varepsilon)$ and $x(\\varepsilon)$ are the eigenvalue and corresponding right eigenvector, which depend on the parameter $\\varepsilon$. We are given $\\lambda(0)=2$.\nWe seek to compute the sensitivity $\\frac{d\\lambda}{d\\varepsilon}$ evaluated at $\\varepsilon=0$. Differentiating the eigenvalue equation with respect to $\\varepsilon$ gives:\n$$\n\\frac{dA}{d\\varepsilon} x(\\varepsilon) + A(\\varepsilon) \\frac{dx}{d\\varepsilon} = \\frac{d\\lambda}{d\\varepsilon} x(\\varepsilon) + \\lambda(\\varepsilon) \\frac{dx}{d\\varepsilon}\n$$\nAt $\\varepsilon=0$, we have $A(0)=A$, $\\lambda(0)=\\lambda_0=2$, and $x(0)=x_0$, where $x_0$ is the right eigenvector of $A$ for $\\lambda_0=2$. Also, $\\frac{dA}{d\\varepsilon} = B$. Substituting $\\varepsilon=0$:\n$$\nB x_0 + A \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = \\frac{d\\lambda}{d\\varepsilon}\\bigg|_{\\varepsilon=0} x_0 + \\lambda_0 \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0}\n$$\nTo isolate the scalar term $\\frac{d\\lambda}{d\\varepsilon}$, we introduce the left eigenvector $y_0$ corresponding to $\\lambda_0$, which satisfies $y_0^T A = \\lambda_0 y_0^T$. Multiplying the equation from the left by $y_0^T$:\n$$\ny_0^T B x_0 + y_0^T A \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = \\frac{d\\lambda}{d\\varepsilon}\\bigg|_{\\varepsilon=0} y_0^T x_0 + \\lambda_0 y_0^T \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0}\n$$\nUsing the property of the left eigenvector, $y_0^T A = \\lambda_0 y_0^T$, the equation simplifies:\n$$\ny_0^T B x_0 + \\lambda_0 y_0^T \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = \\frac{d\\lambda}{d\\varepsilon}\\bigg|_{\\varepsilon=0} y_0^T x_0 + \\lambda_0 y_0^T \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0}\n$$\nThe terms involving $\\frac{dx}{d\\varepsilon}$ cancel, leaving:\n$$\ny_0^T B x_0 = \\frac{d\\lambda}{d\\varepsilon}\\bigg|_{\\varepsilon=0} y_0^T x_0\n$$\nSince $\\lambda_0=2$ is a simple eigenvalue, the inner product $y_0^T x_0$ is non-zero. Thus, we can solve for the sensitivity:\n$$\n\\frac{d\\lambda}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = \\frac{y_0^T B x_0}{y_0^T x_0}\n$$\nWe must now compute the right eigenvector $x_0$ and left eigenvector $y_0$ for $\\lambda_0=2$.\n\nThe right eigenvector $x_0$ is found by solving $(A - 2I)x_0 = 0$:\n$$\n\\begin{pmatrix} -2 & 0 & 6 \\\\ 1 & -2 & -11 \\\\ 0 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row: $-2x_1 + 6x_3 = 0 \\implies x_1 = 3x_3$.\nFrom the third row: $x_2 + 4x_3 = 0 \\implies x_2 = -4x_3$.\nChoosing $x_3=1$, we get the right eigenvector $x_0 = \\begin{pmatrix} 3 \\\\ -4 \\\\ 1 \\end{pmatrix}$.\n\nThe left eigenvector $y_0$ is found by solving $(A^T - 2I)y_0 = 0$:\n$$\n(A^T - 2I)y_0 = \\begin{pmatrix} -2 & 1 & 0 \\\\ 0 & -2 & 1 \\\\ 6 & -11 & 4 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row: $-2y_1 + y_2 = 0 \\implies y_2 = 2y_1$.\nFrom the second row: $-2y_2 + y_3 = 0 \\implies y_3 = 2y_2 = 2(2y_1) = 4y_1$.\nChoosing $y_1=1$, we get the left eigenvector $y_0 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}$.\n\nNow we compute the terms for the sensitivity formula.\nThe numerator is $y_0^T B x_0$:\n$$\ny_0^T B x_0 = \\begin{pmatrix} 1 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -4 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1\n$$\nThe denominator is $y_0^T x_0$:\n$$\ny_0^T x_0 = \\begin{pmatrix} 1 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -4 \\\\ 1 \\end{pmatrix} = 1(3) + 2(-4) + 4(1) = 3 - 8 + 4 = -1\n$$\nFinally, the sensitivity is:\n$$\n\\frac{d\\lambda}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = \\frac{1}{-1} = -1\n$$", "answer": "$$\n\\boxed{-1}\n$$", "id": "2443300"}, {"introduction": "While understanding eigenvalue sensitivity to a single, specific perturbation is a great start, in many engineering applications we need a more general measure of robustness. This leads to the concept of an eigenvalue's condition number, $\\kappa(\\lambda)$, which quantifies the maximum possible change in an eigenvalue for any perturbation of a given size. This practice guides you through the process of first constructing a non-diagonal matrix with specified eigenvalues and then deriving and applying the formula for the condition number to identify which of its eigenvalues is the most sensitive to arbitrary perturbations [@problem_id:2443319].", "problem": "You are given the three distinct real eigenvalues $\\lambda_{1} = 1$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 3$. \n\n1. Construct a real, non-diagonal $3 \\times 3$ matrix $A$ that has exactly these eigenvalues by performing a similarity transform of a diagonal matrix. Specifically, let\n$$\n\\Lambda = \\mathrm{diag}(1, 2, 3), \\quad V = \\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix},\n$$\nand define \n$$\nA = V \\Lambda V^{-1}.\n$$\nCompute $A$ explicitly and verify, using the characteristic polynomial, that the eigenvalues of $A$ are $\\{1,2,3\\}$.\n\n2. For each eigenvalue $\\lambda$ of $A$, let $x$ be a corresponding right eigenvector and $y$ be a corresponding left eigenvector, defined by $A x = \\lambda x$ and $y^{\\mathsf{T}} A = \\lambda y^{\\mathsf{T}}$. Consider a perturbation $A(\\varepsilon) = A + \\varepsilon E$ with $\\|E\\|_{2} = 1$, where $\\|\\cdot\\|_{2}$ denotes the operator $2$-norm. Starting from the defining relation $A(\\varepsilon) x(\\varepsilon) = \\lambda(\\varepsilon) x(\\varepsilon)$ and without quoting any pre-established perturbation formulas, derive an expression for the first-order sensitivity (the $2$-norm condition number) \n$$\n\\kappa(\\lambda) = \\sup_{\\|E\\|_{2} = 1} \\left| \\left.\\frac{d\\lambda(\\varepsilon)}{d\\varepsilon}\\right|_{\\varepsilon = 0} \\right|,\n$$\nin terms of $x$ and $y$. Then compute $\\kappa(\\lambda)$ for each of the three eigenvalues of your matrix $A$ and identify the most sensitive eigenvalue.\n\nReport as your final answer the numerical value of the largest $\\kappa(\\lambda)$ among the three eigenvalues. Express your final answer as an exact number. No rounding is needed.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a standard exercise in computational linear algebra concerning eigenvalue sensitivity. We shall proceed with the solution.\n\nThe problem is divided into two parts. First, we construct the matrix $A$ and verify its eigenvalues. Second, we derive the formula for the eigenvalue condition number and compute it for each eigenvalue of $A$.\n\nPart 1: Construction and Verification of Matrix $A$\n\nWe are given the diagonal matrix of eigenvalues $\\Lambda$ and a matrix $V$ for the similarity transform.\n$$\n\\Lambda = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad V = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe matrix $A$ is defined as $A = V \\Lambda V^{-1}$. First, we must compute the inverse of $V$. The determinant of $V$ is $\\det(V) = 1 \\cdot (1 \\cdot 1 - 1 \\cdot 0) - 1 \\cdot (0) + 0 = 1$. The inverse $V^{-1}$ is given by the adjugate matrix, since $\\det(V)=1$.\nThe matrix of cofactors is:\n$$\nC = \\begin{pmatrix}\n+\\begin{vmatrix} 1 & 1 \\\\ 0 & 1 \\end{vmatrix} & -\\begin{vmatrix} 0 & 1 \\\\ 0 & 1 \\end{vmatrix} & +\\begin{vmatrix} 0 & 1 \\\\ 0 & 0 \\end{vmatrix} \\\\\n-\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} & +\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} & -\\begin{vmatrix} 1 & 1 \\\\ 0 & 0 \\end{vmatrix} \\\\\n+\\begin{vmatrix} 1 & 0 \\\\ 1 & 1 \\end{vmatrix} & -\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} & +\\begin{vmatrix} 1 & 1 \\\\ 0 & 1 \\end{vmatrix}\n\\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 1 & -1 & 1 \\end{pmatrix}\n$$\nThe adjugate matrix is the transpose of the cofactor matrix, $\\mathrm{adj}(V) = C^{\\mathsf{T}}$.\n$$\nV^{-1} = \\frac{1}{\\det(V)} \\mathrm{adj}(V) = \\begin{pmatrix} 1 & -1 & 1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nNow, we compute $A$:\n$$\nA = V \\Lambda V^{-1} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 2 & 3 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+2(0)+0(0) & 1(-1)+2(1)+0(0) & 1(1)+2(-1)+0(1) \\\\ 0(1)+2(0)+3(0) & 0(-1)+2(1)+3(0) & 0(1)+2(-1)+3(1) \\\\ 0(1)+0(0)+3(0) & 0(-1)+0(1)+3(0) & 0(1)+0(-1)+3(1) \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1 & 1 & -1 \\\\ 0 & 2 & 1 \\\\ 0 & 0 & 3 \\end{pmatrix}\n$$\nThis matrix is real and non-diagonal as required. To verify its eigenvalues, we compute the characteristic polynomial $p(\\lambda) = \\det(A - \\lambda I)$:\n$$\np(\\lambda) = \\det \\begin{pmatrix} 1-\\lambda & 1 & -1 \\\\ 0 & 2-\\lambda & 1 \\\\ 0 & 0 & 3-\\lambda \\end{pmatrix}\n$$\nFor an upper triangular matrix, the determinant is the product of the diagonal entries.\n$$\np(\\lambda) = (1-\\lambda)(2-\\lambda)(3-\\lambda)\n$$\nThe roots of the characteristic equation $p(\\lambda) = 0$ are $\\lambda_1 = 1$, $\\lambda_2 = 2$, and $\\lambda_3 = 3$, which are the specified eigenvalues.\n\nPart 2: Eigenvalue Sensitivity Analysis\n\nWe must first derive the expression for the eigenvalue condition number $\\kappa(\\lambda)$. We start with the perturbed eigenvalue problem:\n$$\nA(\\varepsilon) x(\\varepsilon) = \\lambda(\\varepsilon) x(\\varepsilon)\n$$\nwhere $A(\\varepsilon) = A + \\varepsilon E$, $\\lambda(0) = \\lambda$, and $x(0) = x$. We assume Taylor series expansions for $\\lambda(\\varepsilon)$ and $x(\\varepsilon)$ around $\\varepsilon=0$:\n$$\n\\lambda(\\varepsilon) = \\lambda + \\varepsilon \\lambda^{(1)} + O(\\varepsilon^2)\n$$\n$$\nx(\\varepsilon) = x + \\varepsilon x^{(1)} + O(\\varepsilon^2)\n$$\nwhere $\\lambda^{(1)} = \\left.\\frac{d\\lambda}{d\\varepsilon}\\right|_{\\varepsilon=0}$.\nSubstituting these into the perturbed equation:\n$$\n(A + \\varepsilon E)(x + \\varepsilon x^{(1)}) = (\\lambda + \\varepsilon \\lambda^{(1)})(x + \\varepsilon x^{(1)}) + O(\\varepsilon^2)\n$$\nExpanding and collecting terms of order $\\varepsilon^1$:\n$$\nAx + \\varepsilon(Ex + Ax^{(1)}) = \\lambda x + \\varepsilon(\\lambda^{(1)} x + \\lambda x^{(1)}) + O(\\varepsilon^2)\n$$\nThe terms of order $\\varepsilon^0$ give the unperturbed equation $Ax = \\lambda x$. The first-order terms give:\n$$\nEx + Ax^{(1)} = \\lambda^{(1)} x + \\lambda x^{(1)}\n$$\nRearranging gives:\n$$\n(A - \\lambda I)x^{(1)} = \\lambda^{(1)} x - Ex\n$$\nTo solve for the scalar $\\lambda^{(1)}$, we introduce the left eigenvector $y$ corresponding to $\\lambda$, which satisfies $y^{\\mathsf{T}}A = \\lambda y^{\\mathsf{T}}$, or $y^{\\mathsf{T}}(A - \\lambda I) = 0^{\\mathsf{T}}$. Left-multiplying the equation for $x^{(1)}$ by $y^{\\mathsf{T}}$:\n$$\ny^{\\mathsf{T}}(A - \\lambda I)x^{(1)} = y^{\\mathsf{T}}(\\lambda^{(1)} x - Ex)\n$$\nThe left side is $0^{\\mathsf{T}}x^{(1)} = 0$. Therefore:\n$$\n0 = \\lambda^{(1)} y^{\\mathsf{T}}x - y^{\\mathsf{T}}Ex\n$$\nSince the eigenvalues are distinct, the matrix $A$ is not defective, and for a simple eigenvalue, $y^{\\mathsf{T}}x \\neq 0$. We can thus solve for $\\lambda^{(1)}$:\n$$\n\\lambda^{(1)} = \\frac{y^{\\mathsf{T}}Ex}{y^{\\mathsf{T}}x}\n$$\nThe condition number $\\kappa(\\lambda)$ is defined as the maximum absolute value of this sensitivity over all perturbations $E$ with operator norm $\\|E\\|_2 = 1$:\n$$\n\\kappa(\\lambda) = \\sup_{\\|E\\|_{2} = 1} |\\lambda^{(1)}| = \\sup_{\\|E\\|_{2} = 1} \\left| \\frac{y^{\\mathsf{T}}Ex}{y^{\\mathsf{T}}x} \\right| = \\frac{1}{|y^{\\mathsf{T}}x|} \\sup_{\\|E\\|_{2} = 1} |y^{\\mathsf{T}}Ex|\n$$\nThe term $|y^{\\mathsf{T}}Ex|$ can be bounded using the Cauchy-Schwarz inequality and properties of the induced $2$-norm:\n$$\n|y^{\\mathsf{T}}Ex| \\le \\|y\\|_2 \\|Ex\\|_2 \\le \\|y\\|_2 \\|E\\|_2 \\|x\\|_2\n$$\nFor $\\|E\\|_2 = 1$, we have $|y^{\\mathsf{T}}Ex| \\le \\|y\\|_2 \\|x\\|_2$. This upper bound is attained for the rank-one matrix $E_0 = \\frac{y x^{\\mathsf{T}}}{\\|y\\|_2 \\|x\\|_2}$. A more direct choice is $E_0 = \\frac{y x^{\\mathsf{T}}}{\\|x\\|_2^2} \\frac{\\|y\\|_2}{\\|y\\|_2} \\dots$ no. Let $u = y/\\|y\\|_2$ and $v=x/\\|x\\|_2$. The matrix $E_0=uv^{\\mathsf{T}}$ has $\\|E_0\\|_2=1$. With this choice, $y^{\\mathsf{T}}E_0 x = y^{\\mathsf{T}}(uv^{\\mathsf{T}})x = (y^{\\mathsf{T}}u)(v^{\\mathsf{T}}x) = (\\|y\\|_2)(\\|x\\|_2)$. Thus, the supremum is indeed $\\|y\\|_2 \\|x\\|_2$.\n$$\n\\kappa(\\lambda) = \\frac{\\|y\\|_2 \\|x\\|_2}{|y^{\\mathsf{T}}x|}\n$$\nNow we compute the right and left eigenvectors for $A$. The right eigenvectors $x$ solve $(A - \\lambda I)x=0$, and the left eigenvectors $y$ solve $(A^{\\mathsf{T}} - \\lambda I)y=0$.\n\nFor $\\lambda_1 = 1$:\nRight eigenvector $x_1$: $(A-1I)x_1 = \\begin{pmatrix} 0 & 1 & -1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix}x_1 = 0$. This gives $x_1 = c \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. We choose $x_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nLeft eigenvector $y_1$: $(A^{\\mathsf{T}}-1I)y_1 = \\begin{pmatrix} 0 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ -1 & 1 & 2 \\end{pmatrix}y_1 = 0$. This gives $y_1 = c \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$. We choose $y_1 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n$\\|x_1\\|_2 = 1$, $\\|y_1\\|_2 = \\sqrt{1^2 + (-1)^2 + 1^2} = \\sqrt{3}$.\n$y_1^{\\mathsf{T}}x_1 = 1(1) - 1(0) + 1(0) = 1$.\n$\\kappa(\\lambda_1) = \\frac{1 \\cdot \\sqrt{3}}{|1|} = \\sqrt{3}$.\n\nFor $\\lambda_2 = 2$:\nRight eigenvector $x_2$: $(A-2I)x_2 = \\begin{pmatrix} -1 & 1 & -1 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix}x_2 = 0$. This gives $x_2 = c \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$. We choose $x_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nLeft eigenvector $y_2$: $(A^{\\mathsf{T}}-2I)y_2 = \\begin{pmatrix} -1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ -1 & 1 & 1 \\end{pmatrix}y_2 = 0$. This gives $y_2 = c \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$. We choose $y_2 = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n$\\|x_2\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$, $\\|y_2\\|_2 = \\sqrt{(-1)^2+1^2} = \\sqrt{2}$.\n$y_2^{\\mathsf{T}}x_2 = 0(1) - 1(1) + 1(0) = -1$.\n$\\kappa(\\lambda_2) = \\frac{\\sqrt{2} \\cdot \\sqrt{2}}{|-1|} = 2$.\n\nFor $\\lambda_3 = 3$:\nRight eigenvector $x_3$: $(A-3I)x_3 = \\begin{pmatrix} -2 & 1 & -1 \\\\ 0 & -1 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}x_3 = 0$. This gives $x_3 = c \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$. We choose $x_3 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\nLeft eigenvector $y_3$: $(A^{\\mathsf{T}}-3I)y_3 = \\begin{pmatrix} -2 & 0 & 0 \\\\ 1 & -1 & 0 \\\\ -1 & 1 & 0 \\end{pmatrix}y_3 = 0$. This gives $y_3 = c \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We choose $y_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n$\\|x_3\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$, $\\|y_3\\|_2 = 1$.\n$y_3^{\\mathsf{T}}x_3 = 0(0) + 0(1) + 1(1) = 1$.\n$\\kappa(\\lambda_3) = \\frac{\\sqrt{2} \\cdot 1}{|1|} = \\sqrt{2}$.\n\nFinally, we compare the condition numbers:\n$\\kappa(\\lambda_1) = \\sqrt{3} \\approx 1.732$\n$\\kappa(\\lambda_2) = 2$\n$\\kappa(\\lambda_3) = \\sqrt{2} \\approx 1.414$\nThe largest condition number is $\\kappa(\\lambda_2) = 2$. Therefore, the eigenvalue $\\lambda_2=2$ is the most sensitive to perturbations.\nThe largest value of $\\kappa(\\lambda)$ is $2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2443319"}, {"introduction": "The sensitivity we have explored so far, where the change in an eigenvalue is linearly proportional to the perturbation, applies to simple (non-defective) eigenvalues. However, the situation changes dramatically for defective matrices, which contain Jordan blocks. This final exercise demonstrates the extreme ill-conditioning of defective eigenvalues, where a perturbation of size $\\epsilon$ can cause a much larger shift in the eigenvalue, often on the order of $\\epsilon^{1/p}$ for a Jordan block of size $p$. Understanding this phenomenon is critical for numerical stability analysis [@problem_id:2443358].", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the defective matrix\n$$\nA = \\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix},\n$$\nwhich has a size-$2$ Jordan block at the eigenvalue $0$. Consider the perturbed matrix $A_{\\epsilon} = A + E(\\epsilon)$ with a perturbation $E(\\epsilon) \\in \\mathbb{R}^{2 \\times 2}$ defined by\n$$\nE(\\epsilon) = \\epsilon \\begin{pmatrix}\n0 & 0 \\\\\n1 & 0\n\\end{pmatrix},\n$$\nwhere $\\epsilon > 0$. The perturbation $E(\\epsilon)$ has induced $2$-norm equal to $\\epsilon$.\n\nDetermine, as an explicit function of $\\epsilon$, the leading-order asymptotic expression (as $\\epsilon \\to 0^{+}$) for the eigenvalue of $A_{\\epsilon}$ with positive real part. Provide your answer as a single analytic expression in terms of $\\epsilon$ with no units. No rounding is required.", "solution": "The problem requires the determination of the leading-order asymptotic expression for the eigenvalue of a perturbed matrix $A_{\\epsilon}$ that possesses a positive real part.\n\nThe unperturbed matrix is given by:\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis matrix is in Jordan canonical form. Its characteristic polynomial is $\\det(A - \\lambda I) = \\det \\begin{pmatrix} -\\lambda & 1 \\\\ 0 & -\\lambda \\end{pmatrix} = \\lambda^2$. This shows that $A$ has a single eigenvalue $\\lambda = 0$ with algebraic multiplicity $2$. To find the geometric multiplicity, we compute the dimension of the null space of $A - 0I = A$. The rank of $A$ is $1$, so by the rank-nullity theorem, the nullity is $2 - 1 = 1$. Since the geometric multiplicity ($1$) is less than the algebraic multiplicity ($2$), the matrix $A$ is defective. It consists of a single Jordan block of size $2$ for the eigenvalue $0$.\n\nThe perturbation is defined as $E(\\epsilon) = \\epsilon \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$, where $\\epsilon > 0$. The perturbed matrix is $A_{\\epsilon} = A + E(\\epsilon)$. We construct this matrix explicitly:\n$$\nA_{\\epsilon} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + \\epsilon \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ \\epsilon & 0 \\end{pmatrix}\n$$\nTo find the eigenvalues of $A_{\\epsilon}$, we must solve its characteristic equation, $\\det(A_{\\epsilon} - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix. First, we form the matrix $A_{\\epsilon} - \\lambda I$:\n$$\nA_{\\epsilon} - \\lambda I = \\begin{pmatrix} 0 & 1 \\\\ \\epsilon & 0 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda & 1 \\\\ \\epsilon & -\\lambda \\end{pmatrix}\n$$\nThe determinant of this matrix provides the characteristic polynomial of $A_{\\epsilon}$:\n$$\n\\det(A_{\\epsilon} - \\lambda I) = (-\\lambda)(-\\lambda) - (1)(\\epsilon) = \\lambda^2 - \\epsilon\n$$\nSetting the characteristic polynomial to zero gives the characteristic equation:\n$$\n\\lambda^2 - \\epsilon = 0\n$$\nWe solve this equation for the eigenvalues $\\lambda$:\n$$\n\\lambda^2 = \\epsilon\n$$\nThe solutions are $\\lambda = \\pm\\sqrt{\\epsilon}$.\nThus, the two eigenvalues of the perturbed matrix $A_{\\epsilon}$ are $\\lambda_1 = \\sqrt{\\epsilon}$ and $\\lambda_2 = -\\sqrt{\\epsilon}$.\n\nThe problem demands the eigenvalue with a positive real part. Given that $\\epsilon > 0$, the quantity $\\sqrt{\\epsilon}$ is a real and positive number.\nThe first eigenvalue, $\\lambda_1 = \\sqrt{\\epsilon}$, has a positive real part, as $\\Re(\\sqrt{\\epsilon}) = \\sqrt{\\epsilon} > 0$.\nThe second eigenvalue, $\\lambda_2 = -\\sqrt{\\epsilon}$, has a negative real part, as $\\Re(-\\sqrt{\\epsilon}) = -\\sqrt{\\epsilon} < 0$.\nTherefore, the eigenvalue of interest is $\\sqrt{\\epsilon}$.\n\nThe final step is to determine the leading-order asymptotic expression for this eigenvalue as $\\epsilon \\to 0^{+}$. The expression obtained, $\\sqrt{\\epsilon}$, is an exact analytic formula. An asymptotic expansion for a function $f(\\epsilon)$ as $\\epsilon \\to 0$ is a series of the form $c_1 \\epsilon^{p_1} + c_2 \\epsilon^{p_2} + \\dots$ where $p_1 < p_2 < \\dots$. The leading-order term is the first term, $c_1 \\epsilon^{p_1}$. In this case, the function is simply $\\sqrt{\\epsilon}$, or $1 \\cdot \\epsilon^{1/2}$. There are no other terms in the expression. Consequently, the expression itself is its own leading-order asymptotic form.\n\nThis result demonstrates the high sensitivity of defective eigenvalues. A perturbation of order $O(\\epsilon)$ to a matrix with a size-$p$ Jordan block can induce a change in the eigenvalue of order $O(\\epsilon^{1/p})$. For this problem, $p=2$, and the change in the eigenvalue is of order $O(\\epsilon^{1/2}) = O(\\sqrt{\\epsilon})$, which is of a much larger magnitude than $O(\\epsilon)$ for small $\\epsilon$.\nThe requested expression is thus $\\sqrt{\\epsilon}$.", "answer": "$$\n\\boxed{\\sqrt{\\epsilon}}\n$$", "id": "2443358"}]}