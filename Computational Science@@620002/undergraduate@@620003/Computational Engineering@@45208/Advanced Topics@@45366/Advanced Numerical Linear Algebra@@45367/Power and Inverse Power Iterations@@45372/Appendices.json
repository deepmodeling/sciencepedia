{"hands_on_practices": [{"introduction": "Before diving into complex implementations, it's essential to build a solid intuition for why iterative methods work. This first exercise explores the core principle of the power method in a theoretically perfect scenario. By applying the iteration to a projection matrix $P$, you will see how the process converges to the dominant eigenvector in a single step, providing a crystal-clear illustration of the underlying mechanics [@problem_id:2427094]. This practice will strengthen your understanding of eigenvectors and the function of the Rayleigh quotient in extracting the corresponding eigenvalue.", "problem": "Let $v \\in \\mathbb{R}^{n}$ be a nonzero vector and define the orthogonal projector onto the one-dimensional subspace spanned by $v$ by $P = \\dfrac{v v^{T}}{v^{T} v} \\in \\mathbb{R}^{n \\times n}$. Consider the power iteration with normalization applied to $P$, given an initial vector $x_{0} \\in \\mathbb{R}^{n}$ satisfying $v^{T} x_{0} \\neq 0$. One iteration produces $y_{1} = P x_{0}$ and the normalized iterate $x_{1} = \\dfrac{y_{1}}{\\|y_{1}\\|_{2}}$. Define the Rayleigh quotient of $x_{1}$ with respect to $P$ by $r_{1} = \\dfrac{x_{1}^{T} P x_{1}}{x_{1}^{T} x_{1}}$. Determine the exact value of $r_{1}$. The final answer must be a single real number. No rounding is required.", "solution": "The problem requires the calculation of the Rayleigh quotient $r_{1}$ for the first iterate of the power method applied to an orthogonal projector matrix $P$.\n\nFirst, we must validate the problem statement.\nThe givens are:\n- A nonzero vector $v \\in \\mathbb{R}^{n}$.\n- The orthogonal projector matrix $P = \\dfrac{v v^{T}}{v^{T} v} \\in \\mathbb{R}^{n \\times n}$.\n- An initial vector $x_{0} \\in \\mathbb{R}^{n}$ with the property $v^{T} x_{0} \\neq 0$.\n- The first unnormalized iterate is $y_{1} = P x_{0}$.\n- The first normalized iterate is $x_{1} = \\dfrac{y_{1}}{\\|y_{1}\\|_{2}}$.\n- The Rayleigh quotient is $r_{1} = \\dfrac{x_{1}^{T} P x_{1}}{x_{1}^{T} x_{1}}$.\n\nThe problem is scientifically grounded in the principles of linear algebra and numerical analysis. The definitions of the orthogonal projector, power iteration, and Rayleigh quotient are standard. The problem is well-posed; the condition $v^{T} x_{0} \\neq 0$ ensures that the first iterate $y_{1}$ is not the zero vector, so its normalization $x_{1}$ is well-defined. The problem is objective and contains all necessary information for a unique solution. Therefore, the problem is deemed valid and a solution will be constructed.\n\nThe objective is to compute $r_{1}$. We begin by analyzing the structure of the first iterate $x_{1}$.\nThe unnormalized iterate $y_{1}$ is given by:\n$$y_{1} = P x_{0} = \\left(\\dfrac{v v^{T}}{v^{T} v}\\right) x_{0} = \\dfrac{v (v^{T} x_{0})}{v^{T} v}$$\nLet us define the scalar $\\alpha = v^{T} x_{0}$. By the problem statement, $\\alpha \\neq 0$. Also, let us denote the scalar $v^{T} v = \\|v\\|_{2}^{2}$. Since $v$ is a nonzero vector, $v^{T} v > 0$.\nWith these definitions, the expression for $y_{1}$ simplifies to:\n$$y_{1} = \\dfrac{\\alpha}{v^{T} v} v$$\nThis expression shows that $y_{1}$ is a non-zero scalar multiple of the vector $v$.\n\nNext, we normalize $y_{1}$ to obtain $x_{1}$. The normalization is with respect to the Euclidean norm, $\\| \\cdot \\|_{2}$.\nThe norm of $y_{1}$ is:\n$$\\|y_{1}\\|_{2} = \\left\\| \\dfrac{\\alpha}{v^{T} v} v \\right\\|_{2} = \\left| \\dfrac{\\alpha}{v^{T} v} \\right| \\|v\\|_{2} = \\dfrac{|\\alpha|}{v^{T} v} \\sqrt{v^{T} v} = \\dfrac{|\\alpha|}{\\sqrt{v^{T} v}}$$\nNow, we compute $x_{1}$:\n$$x_{1} = \\dfrac{y_{1}}{\\|y_{1}\\|_{2}} = \\dfrac{\\frac{\\alpha}{v^{T} v} v}{\\frac{|\\alpha|}{\\sqrt{v^{T} v}}} = \\dfrac{\\alpha}{|\\alpha|} \\dfrac{\\sqrt{v^{T} v}}{v^{T} v} v = \\dfrac{\\alpha}{|\\alpha|} \\dfrac{1}{\\sqrt{v^{T} v}} v$$\nRecognizing that $\\sqrt{v^{T} v} = \\|v\\|_{2}$, we can write $x_{1}$ as:\n$$x_{1} = \\text{sgn}(\\alpha) \\dfrac{v}{\\|v\\|_{2}}$$\nwhere $\\text{sgn}(\\alpha) = \\frac{\\alpha}{|\\alpha|}$ is the sign of $\\alpha$. This shows that $x_{1}$ is a unit vector pointing in the same or opposite direction as $v$. In other words, $x_{1}$ lies in the one-dimensional subspace spanned by $v$.\n\nNow we must evaluate the Rayleigh quotient $r_{1} = \\dfrac{x_{1}^{T} P x_{1}}{x_{1}^{T} x_{1}}$.\nBy definition of a normalized vector, the denominator is $x_{1}^{T} x_{1} = \\|x_{1}\\|_{2}^{2} = 1$.\nSo, the Rayleigh quotient simplifies to $r_{1} = x_{1}^{T} P x_{1}$.\n\nTo compute this, we first evaluate the action of $P$ on $x_{1}$.\n$$P x_{1} = P \\left( \\text{sgn}(\\alpha) \\dfrac{v}{\\|v\\|_{2}} \\right) = \\text{sgn}(\\alpha) \\dfrac{1}{\\|v\\|_{2}} (P v)$$\nLet us compute $P v$:\n$$P v = \\left(\\dfrac{v v^{T}}{v^{T} v}\\right) v = \\dfrac{v (v^{T} v)}{v^{T} v} = v$$\nThis confirms that $v$ is an eigenvector of the projector $P$ with a corresponding eigenvalue of $\\lambda = 1$.\nSubstituting this result back into the expression for $P x_{1}$:\n$$P x_{1} = \\text{sgn}(\\alpha) \\dfrac{1}{\\|v\\|_{2}} (v) = x_{1}$$\nThis shows that $x_{1}$ is also an eigenvector of $P$ corresponding to the eigenvalue $\\lambda = 1$. This is expected, as the power iteration has converged in a single step to an eigenvector associated with the dominant eigenvalue of $P$, which is $1$. The eigenvalues of $P$ are $1$ (with eigenvector $v$) and $0$ (with eigenspace being the orthogonal complement of the span of $v$).\n\nFinally, we calculate $r_{1}$:\n$$r_{1} = x_{1}^{T} (P x_{1}) = x_{1}^{T} x_{1}$$\nSince $x_{1}$ is a unit vector, $x_{1}^{T} x_{1} = 1$.\nTherefore, the value of the Rayleigh quotient is:\n$$r_{1} = 1$$\nThis result is independent of the choice of initial vector $x_{0}$, as long as $x_{0}$ has a non-zero component in the direction of $v$.", "answer": "$$\\boxed{1}$$", "id": "2427094"}, {"introduction": "While the basic power method is excellent for finding the dominant eigenvalue, many engineering applications require finding eigenvalues that aren't the largest. This is achieved using the powerful shift-and-invert strategy, which forms the basis of inverse iteration. This hands-on coding challenge guides you through implementing inverse iteration to find an eigenvalue near a specific shift $\\sigma$ for a sparse tridiagonal matrix, a structure that frequently appears in physics and engineering simulations [@problem_id:2427100]. A key takeaway will be a practical one: the importance of efficiently solving the linear system $(A - \\sigma I)z = x$ at each step, rather than explicitly computing a matrix inverse.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be the sparse tridiagonal matrix with main diagonal entries equal to $2$ and first sub- and super-diagonal entries equal to $-1$, that is, $A = \\operatorname{tridiag}([-1], [2], [-1])$ with Dirichlet-type structure. For a given real shift $\\sigma \\in \\mathbb{R}$, a nonzero initial vector $x^{(0)} \\in \\mathbb{R}^{n}$, a convergence tolerance $\\varepsilon > 0$, and a maximum iteration count $m \\in \\mathbb{N}$, compute an approximation to an eigenvalue $\\lambda \\in \\mathbb{R}$ of $A$ that is closest to $\\sigma$, together with a corresponding unit-norm eigenvector $v \\in \\mathbb{R}^{n}$, by iteratively solving linear systems with the shifted matrix $A - \\sigma I$ and normalizing the iterates. At each iteration, define the scalar approximation using the Rayleigh quotient $\\lambda^{(k)} = (x^{(k)})^{\\top} A x^{(k)}$, and declare convergence when the absolute change in successive Rayleigh quotients satisfies $\\lvert \\lambda^{(k)} - \\lambda^{(k-1)} \\rvert \\le \\varepsilon \\max(1, \\lvert \\lambda^{(k)} \\rvert)$ or when the iteration count reaches $m$. The initial vector is specified by $x^{(0)}_i = i+1$ for indices $i = 0, 1, 2, \\dots, n-1$, and the normalization is with respect to the Euclidean norm $\\lVert \\cdot \\rVert_2$. All computations must be carried out in double-precision floating-point arithmetic. Report, for each test case, only the converged Rayleigh quotient approximation to the eigenvalue, rounded to exactly $10$ decimal places.\n\nTest suite. For each parameter tuple $(n, \\sigma, \\varepsilon, m)$ below, construct $A$ as above and compute the requested approximation:\n- Case $1$: $(n, \\sigma, \\varepsilon, m) = (10, 0.1, 10^{-12}, 100)$.\n- Case $2$: $(n, \\sigma, \\varepsilon, m) = (10, 1.6, 10^{-12}, 100)$.\n- Case $3$: $(n, \\sigma, \\varepsilon, m) = (10, 3.9, 10^{-12}, 100)$.\n- Case $4$: $(n, \\sigma, \\varepsilon, m) = (40, 10.0, 10^{-12}, 200)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, with each value rounded to exactly $10$ decimal places (for example, [$x_1,x_2,x_3,x_4$]). The final output must therefore be a list of $4$ floating-point numbers.", "solution": "The problem requires the computation of an eigenvalue $\\lambda$ of a specific symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ that is closest to a given real shift $\\sigma$. The matrix $A$ is defined by its diagonals: the main diagonal entries are all $2$, and the first sub-diagonal and super-diagonal entries are all $-1$. This matrix is a standard finite difference discretization of the second derivative operator with Dirichlet boundary conditions. The method prescribed is the inverse power iteration with shift, also known as shift-and-invert iteration.\n\nThis method is designed to find the eigenvalue of $A$ closest to $\\sigma$. The fundamental principle is that if $\\lambda$ is an eigenvalue of $A$ with corresponding eigenvector $v$, then $(\\lambda - \\sigma)$ is an eigenvalue of the shifted matrix $A - \\sigma I$ with the same eigenvector $v$. Consequently, $(\\lambda - \\sigma)^{-1}$ is an eigenvalue of the inverse matrix $(A - \\sigma I)^{-1}$. The power iteration method, when applied to a matrix $B$, converges to the eigenvector corresponding to the eigenvalue with the largest magnitude. By applying power iteration to $B = (A - \\sigma I)^{-1}$, we find the eigenvector corresponding to its eigenvalue of largest magnitude. This eigenvalue is $(\\lambda_{\\text{closest}} - \\sigma)^{-1}$, where $\\lambda_{\\text{closest}}$ is the eigenvalue of $A$ that is nearest to the shift $\\sigma$, thus minimizing $\\lvert \\lambda - \\sigma \\rvert$.\n\nThe iterative procedure is as follows. We begin with a normalized initial vector $x^{(0)} \\in \\mathbb{R}^n$, where $\\lVert x^{(0)} \\rVert_2 = 1$. The problem specifies an initial vector with components $(x^{(0)}_{\\text{unnorm}})_i = i+1$ for $i=0, \\dots, n-1$; this vector must first be normalized. The first Rayleigh quotient is then computed as $\\lambda^{(0)} = (x^{(0)})^{\\top} A x^{(0)}$.\n\nFor each subsequent iteration $k=1, 2, \\dots, m$:\n$1$. The core step is to solve the linear system of equations $(A - \\sigma I) z^{(k)} = x^{(k-1)}$ for the vector $z^{(k)}$. This is equivalent to applying the inverse matrix, $z^{(k)} = (A - \\sigma I)^{-1} x^{(k-1)}$, but is numerically far more stable and efficient than computing the matrix inverse explicitly. The matrix $A - \\sigma I$ is tridiagonal, so this system can be solved very efficiently in $O(n)$ operations using a specialized algorithm such as the Thomas algorithm.\n\n$2$. The resulting vector $z^{(k)}$ is then normalized to have unit Euclidean norm, yielding the next iterate for the eigenvector: $x^{(k)} = z^{(k)} / \\lVert z^{(k)} \\rVert_2$. The sequence of vectors $x^{(k)}$ converges to the eigenvector $v$ corresponding to the eigenvalue $\\lambda$ of $A$ closest to $\\sigma$.\n\n$3$. The corresponding eigenvalue is approximated at each step using the Rayleigh quotient: $\\lambda^{(k)} = (x^{(k)})^{\\top} A x^{(k)}$. Since $x^{(k)}$ is a unit vector, this simplifies from the general form $\\frac{(x^{(k)})^{\\top} A x^{(k)}}{(x^{(k)})^{\\top} x^{(k)}}$. The sequence $\\lambda^{(k)}$ converges to the eigenvalue $\\lambda$.\n\n$4$. Convergence is declared when the change between successive eigenvalue approximations is sufficiently small. The specified criterion is $\\lvert \\lambda^{(k)} - \\lambda^{(k-1)} \\rvert \\le \\varepsilon \\max(1, \\lvert \\lambda^{(k)} \\rvert)$. This is a mixed relative and absolute error tolerance. The iteration also terminates if the maximum number of iterations, $m$, is reached.\n\nThe implementation will construct the matrix $A - \\sigma I$ in a banded format suitable for an efficient linear solver, such as `scipy.linalg.solve_banded`. The matrix has a main diagonal of $(2-\\sigma)$, and sub- and super-diagonals of $-1$. The Rayleigh quotient calculation will also be optimized for a tridiagonal matrix $A$, where the matrix-vector product $A x^{(k)}$ can be calculated in $O(n)$ time. The entire procedure must be executed using double-precision floating-point arithmetic as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n\n    def apply_A(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Efficiently computes the matrix-vector product y = Ax for the\n        tridiagonal matrix A = tridiag([-1], [2], [-1]).\n        \"\"\"\n        n = len(x)\n        if n == 0:\n            return np.array([])\n        if n == 1:\n            return np.array([2.0 * x[0]])\n        \n        y = np.zeros_like(x, dtype=np.float64)\n        \n        # First element\n        y[0] = 2.0 * x[0] - x[1]\n        \n        # Middle elements\n        y[1:-1] = -x[0:-2] + 2.0 * x[1:-1] - x[2:]\n        \n        # Last element\n        y[-1] = -x[-2] + 2.0 * x[-1]\n        \n        return y\n\n    def inverse_power_iteration(n: int, sigma: float, epsilon: float, m: int) -> float:\n        \"\"\"\n        Computes the eigenvalue of A closest to a shift sigma using the\n        inverse power iteration method.\n\n        Args:\n            n: The dimension of the matrix A.\n            sigma: The real shift.\n            epsilon: The convergence tolerance.\n            m: The maximum number of iterations.\n\n        Returns:\n            The converged eigenvalue approximation.\n        \"\"\"\n        # Construct the banded representation of the shifted matrix B = A - sigma*I\n        # The matrix B is tridiagonal with diagonals [-1, 2-sigma, -1].\n        # For scipy.linalg.solve_banded, the banded matrix `ab` has shape (l+u+1, n).\n        # Here, l=1 (sub-diagonal), u=1 (super-diagonal).\n        # ab[0, 1:] is the super-diagonal\n        # ab[1, :] is the main diagonal\n        # ab[2, :-1] is the sub-diagonal\n        ab = np.zeros((3, n), dtype=np.float64)\n        ab[0, 1:] = -1.0\n        ab[1, :] = 2.0 - sigma\n        ab[2, :-1] = -1.0\n\n        # Initialize the vector x_k\n        # As per problem, initial vector components are x_i = i + 1\n        x_k_unnorm = np.arange(1, n + 1, dtype=np.float64)\n        norm_x = np.linalg.norm(x_k_unnorm)\n        x_k = x_k_unnorm / norm_x\n        \n        # Compute initial Rayleigh quotient lambda_k\n        lambda_k = np.dot(x_k, apply_A(x_k))\n\n        for _ in range(m):\n            lambda_prev = lambda_k\n            \n            # Solve (A - sigma*I) * z = x_k\n            z_k = solve_banded((1, 1), ab, x_k)\n\n            # Normalize to get the next iterate\n            norm_z = np.linalg.norm(z_k)\n            # This case should not be reached with a nonsingular matrix and nonzero vector\n            if norm_z == 0.0:\n                break\n                \n            x_k = z_k / norm_z\n\n            # Compute the new Rayleigh quotient\n            lambda_k = np.dot(x_k, apply_A(x_k))\n\n            # Check for convergence\n            # Criterion: |lambda_k - lambda_{k-1}| <= eps * max(1, |lambda_k|)\n            if np.abs(lambda_k - lambda_prev) <= epsilon * np.max([1.0, np.abs(lambda_k)]):\n                break\n        \n        return lambda_k\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 0.1, 1e-12, 100),\n        (10, 1.6, 1e-12, 100),\n        (10, 3.9, 1e-12, 100),\n        (40, 10.0, 1e-12, 200),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_val, sigma_val, eps_val, m_val = case\n        result = inverse_power_iteration(n_val, sigma_val, eps_val, m_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2427100"}, {"introduction": "With a grasp of both power and inverse iteration, we can now stage a 'race' between algorithms to see how they perform in practice. This comprehensive implementation challenge pits three methods against each other: the basic power iteration, inverse iteration with a fixed shift, and the highly advanced Rayleigh Quotient Iteration (RQI) which uses an adaptive shift [@problem_id:2427128]. By testing these methods on matrices with different properties—including one with very close eigenvalues—you will gain invaluable, first-hand insight into their drastically different convergence rates and learn why RQI is often the method of choice for its remarkable speed.", "problem": "Implement an algorithm to approximate eigenpairs of a real symmetric matrix using inverse iteration with a variable shift given by the Rayleigh quotient. Let the Rayleigh quotient for a nonzero vector $x \\in \\mathbb{R}^n$ be defined as $R(x) = \\dfrac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$. Consider the following three iterative schemes applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with a given nonzero initial vector $x_0 \\in \\mathbb{R}^n$ and tolerance $\\varepsilon > 0$:\n\n- Power iteration: $x_{k+1} \\leftarrow \\dfrac{A x_k}{\\lVert A x_k \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a fixed shift: with $\\sigma_0 = R(x_0)$ fixed, compute $x_{k+1}$ by solving $(A - \\sigma_0 I) y = x_k$ and setting $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a variable shift equal to the Rayleigh quotient (Rayleigh quotient iteration): at each iteration, compute $\\sigma_k = R(x_k)$, solve $(A - \\sigma_k I) y = x_k$, and set $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n\nFor each scheme, stop when the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$ or when a prescribed maximum number of iterations is reached. All vector norms are the Euclidean norm, and $I$ denotes the identity matrix of size $n$.\n\nUse the following test suite. In every case, set $n = 5$, the tolerance $\\varepsilon = 10^{-10}$, and the maximum number of iterations to $1000$.\n\n- Test case $\\#1$ (tri-diagonal symmetric positive definite matrix):\n  - Matrix $A_1 \\in \\mathbb{R}^{5 \\times 5}$:\n    $$\n    A_1 =\n    \\begin{bmatrix}\n    6 & 2 & 0 & 0 & 0 \\\\\n    2 & 5 & 2 & 0 & 0 \\\\\n    0 & 2 & 4 & 2 & 0 \\\\\n    0 & 0 & 2 & 3 & 2 \\\\\n    0 & 0 & 0 & 2 & 2\n    \\end{bmatrix}.\n    $$\n  - Initial vector $x_0^{(1)} = \\dfrac{1}{\\sqrt{5}} [1, 1, 1, 1, 1]^\\mathsf{T}$.\n\n- Test case $\\#2$ (symmetric matrix with two very close eigenvalues):\n  - Define diagonal matrix $D = \\mathrm{diag}(1, 1 + 10^{-6}, 2, 3, 4)$.\n  - Define the planar rotation with angle $\\theta$ such that $\\cos \\theta = \\dfrac{4}{5}$ and $\\sin \\theta = \\dfrac{3}{5}$, and set\n    $$\n    Q = \\begin{bmatrix}\n    \\cos \\theta & -\\sin \\theta & 0 & 0 & 0 \\\\\n    \\sin \\theta & \\phantom{-}\\cos \\theta & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 1\n    \\end{bmatrix}.\n    $$\n  - Matrix $A_2 = Q^\\mathsf{T} D Q$.\n  - Initial vector $x_0^{(2)} = [1, 0, 0, 0, 0]^\\mathsf{T}$.\n\n- Test case $\\#3$ (Hilbert matrix):\n  - Matrix $A_3 \\in \\mathbb{R}^{5 \\times 5}$ with entries $(A_3)_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, 3, 4, 5\\}$.\n  - Initial vector $x_0^{(3)} = \\dfrac{1}{\\sqrt{5}} [1, -1, 1, -1, 1]^\\mathsf{T}$.\n\nFor each test case, run the three schemes independently with the same $A$ and $x_0$ and record the smallest iteration count $k$ at which the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$. If convergence does not occur within the maximum number of iterations, record the maximum number of iterations.\n\nYour program must output a single line containing a comma-separated list of $9$ integers enclosed in square brackets, in the following order:\n$[k_{\\mathrm{RQI}}^{(1)}, k_{\\mathrm{fixed}}^{(1)}, k_{\\mathrm{power}}^{(1)}, k_{\\mathrm{RQI}}^{(2)}, k_{\\mathrm{fixed}}^{(2)}, k_{\\mathrm{power}}^{(2)}, k_{\\mathrm{RQI}}^{(3)}, k_{\\mathrm{fixed}}^{(3)}, k_{\\mathrm{power}}^{(3)}]$, where $k_{\\mathrm{RQI}}^{(i)}$ is the iteration count for Rayleigh quotient iteration on test case $i$, $k_{\\mathrm{fixed}}^{(i)}$ is the iteration count for inverse iteration with a fixed shift $\\sigma_0 = R(x_0^{(i)})$, and $k_{\\mathrm{power}}^{(i)}$ is the iteration count for power iteration. The output must be exactly one line in this format, with no additional characters or whitespace beyond those structurally required by the list representation.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically iterative methods for eigenvalue problems. The problem is well-posed, with all necessary parameters, matrices, initial conditions, and stopping criteria explicitly and unambiguously defined. The language is objective and formal. Therefore, a solution will be provided.\n\nThe problem requires the implementation and comparison of three iterative algorithms for approximating an eigenpair $(\\lambda, v)$ of a real symmetric matrix $A$, where $A v = \\lambda v$. An eigenpair consists of an eigenvalue $\\lambda$ and its corresponding eigenvector $v$. The methods under consideration are power iteration, inverse iteration with a fixed shift, and inverse iteration with a variable shift, also known as Rayleigh quotient iteration (RQI). For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, all its eigenvalues are real, and there exists an orthonormal basis of eigenvectors. The Rayleigh quotient, defined for a nonzero vector $x \\in \\mathbb{R}^n$ as $R(x) = \\frac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$, provides an estimate for an eigenvalue. If $x$ is an eigenvector, then $R(x)$ is the corresponding exact eigenvalue. For all algorithms, we start with an initial vector $x_0$ and generate a sequence of vectors $\\{x_k\\}$ that converges to an eigenvector, and a sequence of Rayleigh quotients $\\{\\lambda_k = R(x_k)\\}$ that converges to the corresponding eigenvalue.\n\n1.  **Power Iteration**\n\n    The power iteration method is the simplest algorithm for finding the dominant eigenpair of a matrix, i.e., the eigenpair $(\\lambda_1, v_1)$ where $|\\lambda_1|$ is the largest magnitude among all eigenvalues. The iterative step is defined as:\n    $$\n    x_{k+1} = \\frac{A x_k}{\\lVert A x_k \\rVert_2}\n    $$\n    Starting with an initial vector $x_0$ that has a non-zero component in the direction of the dominant eigenvector $v_1$, the sequence $x_k$ converges to $v_1$. The convergence is linear, with a rate determined by the ratio $|\\lambda_2 / \\lambda_1|$, where $\\lambda_2$ is the eigenvalue with the second-largest magnitude. If this ratio is close to $1$, convergence can be very slow. The eigenvalue is approximated at each step by the Rayleigh quotient, $\\lambda_k = R(x_k)$.\n\n2.  **Inverse Iteration with Fixed Shift**\n\n    Inverse iteration is a method to find the eigenpair corresponding to the eigenvalue closest to a given shift $\\sigma$. It applies the power method to the matrix $(A - \\sigma I)^{-1}$. The eigenvalues of $(A - \\sigma I)^{-1}$ are $(\\lambda_i - \\sigma)^{-1}$, where $\\lambda_i$ are the eigenvalues of $A$. The dominant eigenvalue of $(A - \\sigma I)^{-1}$ corresponds to the smallest value of $|\\lambda_i - \\sigma|$, which means $\\lambda_i$ is the eigenvalue of $A$ closest to $\\sigma$. The iterative step is:\n    $$\n    x_{k+1} = \\frac{(A - \\sigma I)^{-1} x_k}{\\lVert (A - \\sigma I)^{-1} x_k \\rVert_2}\n    $$\n    In practice, computing the matrix inverse is avoided. Instead, we solve the linear system $(A - \\sigma I) y_k = x_k$ for $y_k$, and then normalize:\n    $$\n    x_{k+1} = \\frac{y_k}{\\lVert y_k \\rVert_2}\n    $$\n    In this problem, a fixed shift $\\sigma_0 = R(x_0)$ is used throughout the process. Convergence is linear, but the rate is determined by the ratio of the two eigenvalues of $(A-\\sigma_0 I)^{-1}$ with largest magnitude. If $\\sigma_0$ is much closer to one eigenvalue $\\lambda_j$ than to any other, convergence to the eigenvector $v_j$ is very rapid.\n\n3.  **Rayleigh Quotient Iteration (RQI)**\n\n    Rayleigh quotient iteration is a powerful refinement of inverse iteration where the shift is updated at each step using the best current estimate for the eigenvalue: the Rayleigh quotient. The iterative process is defined by:\n    1.  Compute the shift: $\\sigma_k = R(x_k) = \\frac{x_k^\\mathsf{T} A x_k}{x_k^\\mathsf{T} x_k}$.\n    2.  Solve for $y_{k+1}$: $(A - \\sigma_k I) y_{k+1} = x_k$.\n    3.  Normalize: $x_{k+1} = \\frac{y_{k+1}}{\\lVert y_{k+1} \\rVert_2}$.\n\n    For a symmetric matrix, RQI exhibits cubic convergence once the iterate $x_k$ is sufficiently close to an eigenvector. This means that the number of correct digits in the approximation roughly triples with each iteration, leading to extremely fast convergence.\n\n**Stopping Criterion**\n\nFor all three methods, the iteration terminates when the norm of the residual vector, $\\lVert A x_k - \\lambda_k x_k \\rVert_2$, falls below a specified tolerance $\\varepsilon$, where $\\lambda_k = R(x_k)$. This residual measures how well the current approximate pair $( \\lambda_k, x_k )$ satisfies the eigenvalue equation. The iteration count $k$ at which this condition is first met is the desired output. If the condition is not met within a maximum number of iterations, that maximum number is recorded.\n\nThe implementation will proceed by defining three functions, one for each algorithm. Each function will iteratively generate the vector sequence and check the stopping criterion at each step, returning the iteration count. These functions will then be applied to the three specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates a dominant eigenpair using Power Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    for k in range(1, max_iter + 1):\n        # Calculate x_k\n        v = A @ x\n        x_k = v / np.linalg.norm(v)\n\n        # Check residual for x_k\n        # Since x_k is normalized, its L2 norm squared is 1.\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm <= tol:\n            return k\n\n        # Prepare for the next iteration\n        x = x_k\n\n    return max_iter\n\ndef inverse_iteration_fixed_shift(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using inverse iteration with a fixed shift.\n    The shift is the Rayleigh quotient of the initial vector.\n    \"\"\"\n    # Normalize initial vector for stability, although given vectors are normalized.\n    # The problem specifies sigma0 = R(x0), where x0 is the given initial vector.\n    # Since all given x0 are unit norm, x0.T @ x0 = 1.\n    sigma0 = x0.T @ A @ x0\n    \n    try:\n        M = A - sigma0 * np.eye(A.shape[0])\n    except np.linalg.LinAlgError:\n        return max_iter # Fails if shift is an exact eigenvalue\n\n    x = x0 / np.linalg.norm(x0) # Start iteration with normalized vector\n\n    for k in range(1, max_iter + 1):\n        try:\n            # Solve (A - sigma0*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # Shift is an eigenvalue or matrix is numerically singular\n            return max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm <= tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef rayleigh_quotient_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using Rayleigh Quotient Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    \n    for k in range(1, max_iter + 1):\n        # Update shift at each step using the Rayleigh quotient of x_{k-1}\n        sigma = x.T @ A @ x\n\n        try:\n            M = A - sigma * np.eye(A.shape[0])\n            # Solve (A - sigma_{k-1}*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # If the shift is an eigenvalue, the previous iterate was the eigenvector.\n            # Its residual should be zero or very small.\n            # The loop condition will have caught it in the previous iteration.\n            # This indicates a numerical breakdown or an exact hit.\n            return k-1 if k > 1 else max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n        \n        if residual_norm <= tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    n = 5\n    tol = 1e-10\n    max_iter = 1000\n\n    # Test Case 1\n    A1 = np.array([\n        [6, 2, 0, 0, 0],\n        [2, 5, 2, 0, 0],\n        [0, 2, 4, 2, 0],\n        [0, 0, 2, 3, 2],\n        [0, 0, 0, 2, 2]\n    ], dtype=float)\n    x0_1 = np.ones(n) / np.sqrt(n)\n\n    # Test Case 2\n    D = np.diag([1.0, 1.0 + 1e-6, 2.0, 3.0, 4.0])\n    cos_theta = 4.0 / 5.0\n    sin_theta = 3.0 / 5.0\n    Q = np.eye(n)\n    Q[0, 0] = cos_theta\n    Q[0, 1] = -sin_theta\n    Q[1, 0] = sin_theta\n    Q[1, 1] = cos_theta\n    A2 = Q.T @ D @ Q\n    x0_2 = np.zeros(n)\n    x0_2[0] = 1.0\n\n    # Test Case 3\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (n, n), dtype=float)\n    x0_3 = np.array([1, -1, 1, -1, 1]) / np.sqrt(n)\n    \n    test_cases = [\n        (A1, x0_1),\n        (A2, x0_2),\n        (A3, x0_3)\n    ]\n\n    results = []\n    for A, x0 in test_cases:\n        k_rqi = rayleigh_quotient_iteration(A, x0, tol, max_iter)\n        k_fixed = inverse_iteration_fixed_shift(A, x0, tol, max_iter)\n        k_power = power_iteration(A, x0, tol, max_iter)\n        \n        results.extend([k_rqi, k_fixed, k_power])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2427128"}]}