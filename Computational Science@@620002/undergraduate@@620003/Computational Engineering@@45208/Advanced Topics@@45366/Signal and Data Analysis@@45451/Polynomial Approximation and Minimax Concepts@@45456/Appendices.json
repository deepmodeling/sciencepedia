{"hands_on_practices": [{"introduction": "The journey into minimax approximation begins with its theoretical cornerstone, the Chebyshev Equioscillation Theorem. This exercise serves as a perfect illustration of the theorem, presenting a special case where the unique best polynomial approximation can be discovered and verified analytically. By working through this problem [@problem_id:2425564], you will develop a deep intuition for the characteristic alternating behavior of the error function, which is the signature of a true minimax solution.", "problem": "Let $f(x)=4x^{3}-3x$ on the interval $[-1,1]$. Let $\\mathcal{P}_{2}$ denote the set of all real polynomials of degree at most $2$. With the uniform norm $\\|g\\|_{\\infty,[-1,1]}=\\max_{x\\in[-1,1]}|g(x)|$, determine the unique minimax quadratic approximant $p^{\\star}\\in\\mathcal{P}_{2}$ that minimizes $\\|f-p\\|_{\\infty,[-1,1]}$ over all $p\\in\\mathcal{P}_{2}$. Provide $p^{\\star}(x)$ explicitly as a simplified exact expression. Your final answer must be a single analytic expression.", "solution": "The problem statement must first be validated for scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n- The function to be approximated is $f(x)=4x^{3}-3x$.\n- The interval of approximation is $[-1,1]$.\n- The space of approximating polynomials is $\\mathcal{P}_{2}$, the set of all real polynomials of degree at most $2$.\n- The norm for measuring the error is the uniform norm, $\\|g\\|_{\\infty,[-1,1]}=\\max_{x\\in[-1,1]}|g(x)|$.\n- The objective is to determine the unique minimax quadratic approximant $p^{\\star}\\in\\mathcal{P}_{2}$ that minimizes $\\|f-p\\|_{\\infty,[-1,1]}$ over all $p\\in\\mathcal{P}_{2}$.\n- The required output is an explicit, simplified exact expression for $p^{\\star}(x)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard problem in approximation theory, a subfield of numerical analysis and computational engineering. It is well-posed; the existence and uniqueness of such a polynomial approximant are guaranteed by fundamental theorems. The problem is stated in objective, precise mathematical language without ambiguity or missing information. There are no contradictions or scientifically unsound premises.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe problem asks for the best approximation of the function $f(x) = 4x^{3}-3x$ on the interval $[-1,1]$ by a polynomial $p(x)$ of degree at most $2$, in the sense of minimizing the maximum absolute error $\\|f - p\\|_{\\infty,[-1,1]}$. This is a classic minimax approximation problem.\n\nThe cornerstone for solving this problem is the Chebyshev Equioscillation Theorem. This theorem states that a polynomial $p^{\\star} \\in \\mathcal{P}_{n}$ is the unique best uniform approximation to a continuous function $f(x)$ on an interval $[a,b]$ if and only if the error function $e(x) = f(x) - p^{\\star}(x)$ exhibits at least $n+2$ points of equioscillation. That is, there must exist at least $n+2$ points $x_0 < x_1 < \\dots < x_{n+1}$ in $[a,b]$ such that $|e(x_i)| = \\|e\\|_{\\infty, [a,b]}$, and the sign of the error alternates, i.e., $e(x_i) = -e(x_{i+1})$ for $i=0, 1, \\dots, n$.\n\nIn this specific problem, the space of approximating polynomials is $\\mathcal{P}_{2}$, so the degree is at most $n=2$. The interval is $[-1,1]$. Therefore, we must find a polynomial $p^{\\star}(x) \\in \\mathcal{P}_2$ such that the error function $e(x) = f(x) - p^{\\star}(x)$ has at least $n+2 = 4$ points of equioscillation on $[-1,1]$.\n\nThe crucial first step is to recognize the function $f(x)$. The given function is $f(x) = 4x^{3}-3x$. This is the definition of the Chebyshev polynomial of the first kind of degree $3$, denoted as $T_3(x)$. So, we are tasked with finding the best quadratic approximation to $T_3(x)$ on $[-1,1]$.\n\nLet us examine the properties of $T_3(x)$. By definition, $T_n(x) = \\cos(n \\arccos(x))$. For $x \\in [-1,1]$, the value of $T_n(x)$ is bounded by $[-1,1]$. The extrema of $T_n(x)$ on $[-1,1]$ occur at the points $x_k = \\cos\\left(\\frac{k\\pi}{n}\\right)$ for $k = 0, 1, \\dots, n$. At these points, $T_n(x_k) = \\cos(k\\pi) = (-1)^k$.\n\nFor our function $f(x) = T_3(x)$, we have $n=3$. The extrema occur at $3+1=4$ points:\n$x_k = \\cos\\left(\\frac{k\\pi}{3}\\right)$ for $k = 0, 1, 2, 3$.\nLet's list these points and the corresponding values of $f(x)$:\n- For $k=0$: $x_0 = \\cos(0) = 1$. The function value is $f(1) = T_3(1) = (-1)^0 = 1$.\n- For $k=1$: $x_1 = \\cos\\left(\\frac{\\pi}{3}\\right) = \\frac{1}{2}$. The function value is $f\\left(\\frac{1}{2}\\right) = T_3\\left(\\frac{1}{2}\\right) = (-1)^1 = -1$.\n- For $k=2$: $x_2 = \\cos\\left(\\frac{2\\pi}{3}\\right) = -\\frac{1}{2}$. The function value is $f\\left(-\\frac{1}{2}\\right) = T_3\\left(-\\frac{1}{2}\\right) = (-1)^2 = 1$.\n- For $k=3$: $x_3 = \\cos(\\pi) = -1$. The function value is $f(-1) = T_3(-1) = (-1)^3 = -1$.\n\nLet us propose a candidate for the best approximating polynomial $p^{\\star}(x)$. A simple but logical choice is the zero polynomial, $p^{\\star}(x) = 0$. This polynomial is clearly an element of $\\mathcal{P}_2$.\n\nNow, we must verify if this choice satisfies the conditions of the Equioscillation Theorem. The error function is:\n$e(x) = f(x) - p^{\\star}(x) = (4x^{3}-3x) - 0 = 4x^{3}-3x = T_3(x)$.\n\nThe uniform norm of the error is:\n$\\|e\\|_{\\infty, [-1,1]} = \\|T_3\\|_{\\infty, [-1,1]} = \\max_{x \\in [-1,1]} |T_3(x)| = 1$.\n\nWe have identified four points in the interval $[-1,1]$, namely $\\{-1, -1/2, 1/2, 1\\}$, where the error function $|e(x)|$ attains this maximum value of $1$. The number of these points is $4$, which satisfies the requirement of being at least $n+2 = 2+2=4$.\n\nLet us check the alternating sign condition at these points, ordered as $x_3 < x_2 < x_1 < x_0$:\n- $e(-1) = T_3(-1) = -1$\n- $e\\left(-\\frac{1}{2}\\right) = T_3\\left(-\\frac{1}{2}\\right) = 1$\n- $e\\left(\\frac{1}{2}\\right) = T_3\\left(\\frac{1}{2}\\right) = -1$\n- $e(1) = T_3(1) = 1$\n\nThe values of the error at these four points are $-1, 1, -1, 1$. They are equal in magnitude to the maximum error and alternate in sign.\n\nAll conditions of the Chebyshev Equioscillation Theorem are satisfied for $p^{\\star}(x)=0$ and $n=2$. Therefore, $p^{\\star}(x)=0$ is the unique minimax polynomial approximation of degree at most $2$ to $f(x)=4x^3-3x$ on the interval $[-1,1]$.", "answer": "$$\\boxed{0}$$", "id": "2425564"}, {"introduction": "Moving from pure theory to practical application, this exercise addresses how minimax approximation is performed in the real world, where functions are often represented by discrete data sets. This practice [@problem_id:2425556] introduces a powerful and widely used technique: formulating the discrete minimax problem as a Linear Program (LP), which can then be solved using standard optimization software. You'll apply this method to a tangible engineering scenario involving the degradation of a turbine blade, demonstrating how to compute the best polynomial model that minimizes the maximum prediction error.", "problem": "You are given a deterministic degradation law for the remaining useful life (RUL) of a turbine blade as a function of operating hours and load cycles. Let $h$ denote operating hours (in hours) and $c$ denote the number of high-stress cycles (dimensionless count). The true, unknown RUL function is modeled for this exercise by a known map $R(h,c)$ defined by\n$$\nR(h,c) \\;=\\; \\max\\!\\Big(0,\\; L_0 \\;-\\; \\alpha\\,h \\;-\\; \\beta\\,c \\;-\\; \\gamma\\,\\sqrt{h+1} \\;-\\; \\delta\\,h\\,c \\Big),\n$$\nwith constants $L_0 = 5000$, $\\alpha = 0.8$, $\\beta = 0.2$, $\\gamma = 30$, and $\\delta = 8\\times 10^{-5}$. The output $R(h,c)$ is in hours. All numbers above represent fixed scalars.\n\nYou must construct, for each specified case, a bivariate polynomial model of total degree at most $d$ in the normalized, dimensionless variables\n$$\nx \\;=\\; \\frac{h - h_{\\min}}{h_{\\max}-h_{\\min}}, \\qquad\ny \\;=\\; \\frac{c - c_{\\min}}{c_{\\max}-c_{\\min}},\n$$\nover a rectangular domain $[h_{\\min},h_{\\max}]\\times [c_{\\min},c_{\\max}]$, where $h_{\\min}$, $h_{\\max}$, $c_{\\min}$, $c_{\\max}$ are the case-specific bounds given below. The polynomial has the form\n$$\np(x,y) \\;=\\; \\sum_{\\substack{i,j\\ge 0\\\\ i+j\\le d}} a_{ij}\\,x^{i}\\,y^{j},\n$$\nwith real coefficients $a_{ij}$. For each case, determine the coefficients $\\{a_{ij}\\}$ that minimize the maximum absolute modeling error on a finite training set,\n$$\n\\min_{\\{a_{ij}\\}} \\; \\max_{(h_k,c_k)\\in \\mathcal{T}} \\; \\big|\\,p(x_k,y_k) \\;-\\; R(h_k,c_k)\\,\\big| ,\n$$\nwhere $\\mathcal{T}$ is the uniformly spaced training grid of size $N_h\\times N_c$ over the domain (grid includes endpoints), and $(x_k,y_k)$ are the normalized coordinates of $(h_k,c_k)$.\n\nFor evaluation, compute the maximum absolute error over a separate uniformly spaced validation grid $\\mathcal{V}$ of size $N_h^{\\mathrm{val}}\\times N_c^{\\mathrm{val}}$ on the same domain,\n$$\nE_{\\max} \\;=\\; \\max_{(h,c)\\in \\mathcal{V}} \\; \\big|\\,p(x,y) \\;-\\; R(h,c)\\,\\big| .\n$$\nReport $E_{\\max}$ in hours for each case.\n\nTest suite specification (each case provides $(d,\\;[h_{\\min},h_{\\max}],\\;[c_{\\min},c_{\\max}],\\;N_h\\times N_c,\\;N_h^{\\mathrm{val}}\\times N_c^{\\mathrm{val}})$):\n- Case $1$: $d=2$, $[0,2000]$, $[0,4000]$, $21\\times 21$, $41\\times 41$.\n- Case $2$: $d=1$, $[0,200]$, $[0,200]$, $11\\times 11$, $41\\times 41$.\n- Case $3$: $d=3$, $[0,3500]$, $[0,7000]$, $25\\times 25$, $51\\times 51$.\n- Case $4$: $d=2$, $[3000,4500]$, $[6000,9000]$, $15\\times 15$, $41\\times 41$.\n\nYour program must, for each case, compute the coefficients $\\{a_{ij}\\}$ that minimize the maximum absolute error on the training grid and then evaluate $E_{\\max}$ on the validation grid. The final outputs are the four validation maximum absolute errors, each expressed in hours as floating-point numbers rounded to three decimals.\n\nFinal output format: Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, for example $[e_1,e_2,e_3,e_4]$, where each $e_k$ is the rounded maximum absolute error (in hours) for Case $k$.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational engineering, specifically in the area of function approximation using minimax principles. All necessary data and definitions are provided, and there are no internal contradictions or logical flaws.\n\nThe core task is to determine the coefficients $\\{a_{ij}\\}$ of a bivariate polynomial $p(x,y)$ that best approximates a given function $R(h,c)$ on a finite training set $\\mathcal{T}$. The criterion for \"best\" is the minimization of the maximum absolute error, which is an application of Chebyshev or $L_{\\infty}$ approximation. This minimax problem can be rigorously transformed into a linear programming (LP) problem.\n\nLet the bivariate polynomial of total degree at most $d$ be\n$$\np(x,y) = \\sum_{\\substack{i,j\\ge 0\\\\ i+j\\le d}} a_{ij}\\,x^{i}\\,y^{j}.\n$$\nFor computational purposes, we flatten the coefficients $\\{a_{ij}\\}$ into a single vector $\\boldsymbol{\\alpha}$ of length $N_{coeffs} = (d+1)(d+2)/2$. The polynomial can then be expressed as\n$$\np(x,y) = \\sum_{k=1}^{N_{coeffs}} \\alpha_k \\phi_k(x,y),\n$$\nwhere $\\{\\phi_k(x,y) = x^{i_k}y^{j_k}\\}$ is an ordered basis for the polynomial space. The problem is to find the coefficient vector $\\boldsymbol{\\alpha}$ that solves the optimization problem:\n$$\n\\min_{\\boldsymbol{\\alpha}} \\max_{(h_m,c_m)\\in\\mathcal{T}} |p(x_m,y_m) - R(h_m,c_m)|,\n$$\nwhere the index $m$ runs over the $M = N_h \\times N_c$ points in the training set $\\mathcal{T}$, and $(x_m, y_m)$ are the normalized coordinates corresponding to $(h_m,c_m)$.\n\nThis minimax problem is equivalent to finding the minimum value $\\epsilon$ for which a set of coefficients $\\boldsymbol{\\alpha}$ exists that satisfies\n$$\n|p(x_m,y_m) - R(h_m,c_m)| \\le \\epsilon, \\quad \\forall (h_m,c_m) \\in \\mathcal{T}.\n$$\nThe problem is thus reformulated as:\n$$\n\\min_{\\boldsymbol{\\alpha}, \\epsilon} \\epsilon\n$$\nsubject to the $2M$ linear inequalities:\n$$\n- \\epsilon \\le p(x_m,y_m) - R(h_m,c_m) \\le \\epsilon, \\quad \\text{for } m=1, \\dots, M.\n$$\nSubstituting the polynomial expression, these inequalities become:\n$$\n\\sum_{k=1}^{N_{coeffs}} \\alpha_k \\phi_k(x_m,y_m) - \\epsilon \\le R(h_m,c_m)\n$$\n$$\n-\\sum_{k=1}^{N_{coeffs}} \\alpha_k \\phi_k(x_m,y_m) - \\epsilon \\le -R(h_m,c_m)\n$$\nThis is a linear program for the unknown variables $\\mathbf{u} = [\\alpha_1, \\dots, \\alpha_{N_{coeffs}}, \\epsilon]^T$. In the standard form $\\min \\mathbf{c}^T\\mathbf{u}$ subject to $A_{ub}\\mathbf{u} \\le \\mathbf{b}_{ub}$, the components are:\n1.  The vector of unknowns $\\mathbf{u}$ has dimension $N_{coeffs}+1$. The coefficients $\\alpha_k$ are unbounded, while the error $\\epsilon$ must be non-negative.\n2.  The objective vector is $\\mathbf{c} = [0, \\dots, 0, 1]^T$, of dimension $N_{coeffs}+1$, to minimize $\\epsilon$.\n3.  Let $V$ be the $M \\times N_{coeffs}$ generalized Vandermonde matrix where $V_{mk} = \\phi_k(x_m, y_m)$. Let $\\mathbf{r}$ be the $M \\times 1$ vector where $r_m = R(h_m, c_m)$. The constraint matrix $A_{ub}$ of size $(2M) \\times (N_{coeffs}+1)$ and the vector $\\mathbf{b}_{ub}$ of size $2M$ are constructed as:\n$$\nA_{ub} = \\begin{pmatrix} V & -\\mathbf{1} \\\\ -V & -\\mathbf{1} \\end{pmatrix}, \\quad \\mathbf{b}_{ub} = \\begin{pmatrix} \\mathbf{r} \\\\ -\\mathbf{r} \\end{pmatrix},\n$$\nwhere $\\mathbf{1}$ is an $M \\times 1$ column vector of ones.\n\nThe computational procedure for each test case is as follows:\n1.  Define the case parameters: degree $d$, domain bounds $[h_{\\min}, h_{\\max}]$ and $[c_{\\min}, c_{\\max}]$, and grid sizes $N_h \\times N_c$ and $N_h^{\\mathrm{val}} \\times N_c^{\\mathrm{val}}$.\n2.  Generate the $M = N_h \\times N_c$ training points, normalize their coordinates, and evaluate the true function $R(h,c)$ to form the vector $\\mathbf{r}$.\n3.  Construct the polynomial basis and the corresponding Vandermonde matrix $V$.\n4.  Set up and solve the LP problem using a numerical solver, specifying that the coefficient variables $\\alpha_k$ are unbounded and the error variable $\\epsilon$ is non-negative. This yields the optimal coefficient vector $\\boldsymbol{\\alpha}^*$.\n5.  Construct the optimal polynomial $p^*(x,y)$ using $\\boldsymbol{\\alpha}^*$.\n6.  Generate the validation grid $\\mathcal{V}$ and evaluate the model error at each validation point.\n7.  The final result for the case, $E_{\\max}$, is the maximum absolute error found on the validation grid:\n$$\nE_{\\max} = \\max_{(h,c) \\in \\mathcal{V}} |p^*(x,y) - R(h,c)|.\n$$\nThis procedure is applied to all four specified cases to obtain the required validation errors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves the polynomial minimax approximation problem for a set of test cases.\n    \"\"\"\n    # Define problem constants for the RUL function\n    L0 = 5000.0\n    alpha = 0.8\n    beta = 0.2\n    gamma = 30.0\n    delta = 8e-5\n\n    def R(h, c):\n        \"\"\"\n        Calculates the true Remaining Useful Life (RUL) based on the given degradation law.\n        The function is vectorized to handle numpy arrays for h and c.\n        \"\"\"\n        val = L0 - alpha * h - beta * c - gamma * np.sqrt(h + 1.0) - delta * h * c\n        return np.maximum(0, val)\n\n    # Test suite specification\n    # Each case: (d, [h_min, h_max], [c_min, c_max], (N_h, N_c), (N_h_val, N_c_val))\n    test_cases = [\n        (2, [0.0, 2000.0], [0.0, 4000.0], (21, 21), (41, 41)),\n        (1, [0.0, 200.0], [0.0, 200.0], (11, 11), (41, 41)),\n        (3, [0.0, 3500.0], [0.0, 7000.0], (25, 25), (51, 51)),\n        (2, [3000.0, 4500.0], [6000.0, 9000.0], (15, 15), (41, 41)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        d, h_bounds, c_bounds, train_grid_size, val_grid_size = case\n        h_min, h_max = h_bounds\n        c_min, c_max = c_bounds\n        N_h, N_c = train_grid_size\n        N_h_val, N_c_val = val_grid_size\n\n        # 1. Generate polynomial basis exponents (i, j) for total degree <= d\n        exponents = []\n        for total_degree in range(d + 1):\n            for i in range(total_degree + 1):\n                j = total_degree - i\n                exponents.append((i, j))\n        num_coeffs = len(exponents)\n\n        # 2. Generate training grid and evaluate the RUL function\n        h_train_pts = np.linspace(h_min, h_max, N_h)\n        c_train_pts = np.linspace(c_min, c_max, N_c)\n        h_train_grid, c_train_grid = np.meshgrid(h_train_pts, c_train_pts)\n        \n        h_train_flat = h_train_grid.flatten()\n        c_train_flat = c_train_grid.flatten()\n        num_train_pts = len(h_train_flat)\n\n        x_train_flat = (h_train_flat - h_min) / (h_max - h_min)\n        y_train_flat = (c_train_flat - c_min) / (c_max - c_min)\n        R_train_flat = R(h_train_flat, c_train_flat)\n\n        # 3. Construct the Linear Programming problem\n        # Generalized Vandermonde matrix V for training points\n        V_train = np.zeros((num_train_pts, num_coeffs))\n        for i, (p, q) in enumerate(exponents):\n            V_train[:, i] = x_train_flat**p * y_train_flat**q\n        \n        # LP objective: minimize epsilon (the last variable)\n        c_lp = np.zeros(num_coeffs + 1)\n        c_lp[num_coeffs] = 1.0\n\n        # LP inequality constraints matrix A_ub\n        A_ub = np.zeros((2 * num_train_pts, num_coeffs + 1))\n        A_ub[:num_train_pts, :num_coeffs] = V_train\n        A_ub[num_train_pts:, :num_coeffs] = -V_train\n        A_ub[:, -1] = -1.0\n\n        # LP inequality constraints vector b_ub\n        b_ub = np.concatenate([R_train_flat, -R_train_flat])\n        \n        # Bounds for variables: coefficients are unbounded, epsilon >= 0\n        bounds = [(None, None)] * num_coeffs + [(0, None)]\n\n        # 4. Solve the LP problem\n        res = linprog(c_lp, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        \n        coeffs = res.x[:num_coeffs]\n\n        # 5. Evaluate the obtained polynomial on the validation grid\n        h_val_pts = np.linspace(h_min, h_max, N_h_val)\n        c_val_pts = np.linspace(c_min, c_max, N_c_val)\n        h_val_grid, c_val_grid = np.meshgrid(h_val_pts, c_val_pts)\n        \n        h_val_flat = h_val_grid.flatten()\n        c_val_flat = c_val_grid.flatten()\n\n        x_val_flat = (h_val_flat - h_min) / (h_max - h_min)\n        y_val_flat = (c_val_flat - c_min) / (c_max - c_min)\n        R_val_flat = R(h_val_flat, c_val_flat)\n\n        # Vandermonde matrix for validation points\n        V_val = np.zeros((len(x_val_flat), num_coeffs))\n        for i, (p, q) in enumerate(exponents):\n            V_val[:, i] = x_val_flat**p * y_val_flat**q\n\n        # Predictions from the fitted polynomial\n        p_val = V_val @ coeffs\n        \n        # Compute maximum absolute error on the validation set\n        E_max = np.max(np.abs(p_val - R_val_flat))\n        \n        results.append(round(E_max, 3))\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2425556"}, {"introduction": "This final practice elevates the concept of approximation to a more complex, geometric domain common in computer-aided design. Rather than fitting a single polynomial, you will approximate a complex curve using multiple piecewise cubic Bézier segments. The optimization goal is inverted: instead of minimizing error for a fixed number of pieces, you must find the minimum number of segments required to stay within a given error tolerance, measured by the Hausdorff distance. This problem [@problem_id:2425561] showcases how to solve such problems using dynamic programming, a key algorithmic technique for tackling sequential optimization tasks.", "problem": "You are given a planar curve representing a single-stroke \"glyph-like\" contour that qualitatively resembles a stylized Fraktur letter. The curve is defined as a parametric mapping $\\gamma:[0,1]\\to\\mathbb{R}^2$ sampled at $N$ equally spaced parameters $t_k=\\frac{k}{N-1}$ for $k\\in\\{0,1,\\dots,N-1\\}$. The sampled points are $P_k=\\gamma(t_k)=(x(t_k),y(t_k))$. The glyph-like curve is synthesized by the following well-posed and smooth construction:\n- Let $u(t)=2t-1$.\n- Let the envelope be $e(t)=0.9-0.4\\cos(2\\pi t)$.\n- Define $x(t)=u(t)+0.06\\sin(8\\pi t)+0.02\\sin(14\\pi t)$ and $y(t)=0.9\\,e(t)\\sin(\\pi u(t))+0.2\\sin(3\\pi u(t))+0.08\\sin(5\\pi u(t))$.\n- Use $N=140$ sample points.\n\nA piecewise cubic Bézier approximation is a sequence of cubic Bézier segments $\\{B_m\\}_{m=1}^M$ with control points $\\{Q_{m,0},Q_{m,1},Q_{m,2},Q_{m,3}\\}$, where each segment is parameterized by\n$$\nB_m(\\tau)=(1-\\tau)^3 Q_{m,0}+3\\tau(1-\\tau)^2 Q_{m,1}+3\\tau^2(1-\\tau)Q_{m,2}+\\tau^3 Q_{m,3},\\quad \\tau\\in[0,1],\n$$\nand consecutive segments satisfy $Q_{m,3}=Q_{m+1,0}$ for $m\\in\\{1,\\dots,M-1\\}$ (ensuring $C^0$ continuity). The approximation must start at $P_0$ and end at $P_{N-1}$, i.e., $Q_{1,0}=P_0$ and $Q_{M,3}=P_{N-1}$.\n\nError metric (discrete symmetric Hausdorff distance): for any candidate segment covering consecutive samples $P_i,\\dots,P_j$ with $0\\le i<j\\le N-1$, define two discrete point sets:\n- A densified polyline set $A_{i:j}$ obtained by sampling $q$ equally spaced points along each straight subsegment $[P_k,P_{k+1}]$ for $k\\in\\{i,\\dots,j-1\\}$, including endpoints, with $q=4$.\n- A Bézier set $B_{i:j}$ obtained by evaluating the fitted cubic Bézier segment at $M_B$ uniformly spaced parameters $\\tau_\\ell=\\frac{\\ell}{M_B-1}$ for $\\ell\\in\\{0,1,\\dots,M_B-1\\}$, with $M_B=20$.\n\nGiven two finite sets $A$ and $B$ in $\\mathbb{R}^2$, the discrete symmetric Hausdorff distance is\n$$\nd_H(A,B)=\\max\\left\\{\\max_{a\\in A}\\min_{b\\in B}\\|a-b\\|_2,\\ \\max_{b\\in B}\\min_{a\\in A}\\|a-b\\|_2\\right\\}.\n$$\n\nFitting rule for a candidate segment: for indices $i<j$, fit a single cubic Bézier $B_{i:j}$ to the data $P_i,\\dots,P_j$ by setting $Q_{0}=P_i$, $Q_{3}=P_j$, and choosing $Q_1,Q_2\\in\\mathbb{R}^2$ to minimize the sum of squared residuals between the Bézier and the data at chord-length parameters. Specifically, let $s_k$ be cumulative chord lengths along $P_i,\\dots,P_j$, normalized to $t_k=\\frac{s_k-s_i}{s_j-s_i}\\in[0,1]$. Then solve in the least-squares sense for $Q_1$ and $Q_2$ from\n$$\n(1-t_k)^3 Q_0+3t_k(1-t_k)^2 Q_1+3t_k^2(1-t_k) Q_2+t_k^3 Q_3\\approx P_k\n$$\nfor all $k\\in\\{i,\\dots,j\\}$. If $j=i+1$, use the straight-line special case $Q_1=Q_0+\\frac{1}{3}(Q_3-Q_0)$ and $Q_2=Q_0+\\frac{2}{3}(Q_3-Q_0)$.\n\nOptimization goal (minimax constraint): For a given tolerance $\\varepsilon>0$, find the minimal number $M$ of cubic Bézier segments and a partition $0=i_0<i_1<\\dots<i_M=N-1$ such that for each $m\\in\\{1,\\dots,M\\}$, the fitted Bézier $B_{i_{m-1}:i_m}$ satisfies\n$$\nd_H\\!\\left(A_{i_{m-1}:i_m}, B_{i_{m-1}:i_m}\\right)\\le \\varepsilon.\n$$\n\nYour task is to implement a program that:\n- Constructs the $N$ sample points of the glyph-like curve as specified.\n- For every pair $(i,j)$ with $0\\le i<j\\le N-1$, computes the fitted cubic Bézier $B_{i:j}$ by the rule above and the corresponding discrete symmetric Hausdorff error $E_{i,j}=d_H\\!\\left(A_{i:j}, B_{i:j}\\right)$.\n- For each tolerance $\\varepsilon$ in the test suite, finds the minimal number $M(\\varepsilon)$ of segments needed so that a partition exists with all segment errors $\\le \\varepsilon$. If no such partition exists, return $-1$ for that $\\varepsilon$.\n\nTest suite:\n- Use the tolerance values $\\varepsilon\\in\\{0.12,\\ 0.08,\\ 0.05,\\ 0.03\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered in the same order as the test suite tolerances. Each entry must be the integer minimal segment count $M(\\varepsilon)$ for that tolerance. For example, an output of the form $[M(0.12),M(0.08),M(0.05),M(0.03)]$.\n- Angles are not used. No physical units are involved. All numeric answers must be integers in the final output.", "solution": "The problem posed is a well-defined exercise in computational engineering, specifically in the domain of geometric approximation and optimization. It requires finding the minimum number of piecewise cubic Bézier segments to approximate a given planar curve, subject to a constraint on the approximation error defined by the discrete symmetric Hausdorff distance. The problem is valid as it is mathematically sound, self-contained, and algorithmically solvable.\n\nThe solution is a systematic, multi-step process:\n1.  **Curve Discretization**: First, the target parametric curve $\\gamma(t)$ is sampled at $N=140$ equidistant parameter values $t_k = \\frac{k}{N-1}$ for $k \\in \\{0, 1, \\dots, N-1\\}$. The parametric equations are given as:\n    $$u(t) = 2t-1$$\n    $$e(t) = 0.9-0.4\\cos(2\\pi t)$$\n    $$x(t) = u(t)+0.06\\sin(8\\pi t)+0.02\\sin(14\\pi t)$$\n    $$y(t) = 0.9\\,e(t)\\sin(\\pi u(t))+0.2\\sin(3\\pi u(t))+0.08\\sin(5\\pi u(t))$$\n    This produces a set of $N$ ordered points $P_k = (x(t_k), y(t_k))$ that constitute the discrete representation of the curve to be approximated.\n\n2.  **Error Precomputation**: The core of the problem is to decide how to partition the sequence of points $\\{P_k\\}_{k=0}^{N-1}$. To make this decision efficiently, we precompute the approximation error $E_{i,j}$ for fitting a single cubic Bézier segment to every possible sub-sequence of points from $P_i$ to $P_j$, where $0 \\le i < j \\le N-1$.\n\n    a.  **Bézier Fitting**: For each sub-sequence $P_i, \\dots, P_j$, a cubic Bézier curve is fitted. The control points are denoted $\\{Q_0, Q_1, Q_2, Q_3\\}$. The endpoints are fixed: $Q_0 = P_i$ and $Q_3 = P_j$. The inner control points, $Q_1$ and $Q_2$, are determined by minimizing the sum of squared errors. The parameterization for the fitting process is based on normalized cumulative chord lengths of the polyline $P_i, \\dots, P_j$. Let $s_k$ be the cumulative chord length from $P_i$ to $P_k$. The normalized parameter for point $P_k$ is $t_k = (s_k-s_i)/(s_j-s_i)$. The fitting objective is to solve the linear least-squares problem for $Q_1$ and $Q_2$ that minimizes $\\sum_{k=i+1}^{j-1} \\|B(t_k) - P_k\\|^2$, where $B(t)$ is the Bézier curve equation. This can be expressed as $A\\mathbf{x} \\approx \\mathbf{b}$, where the unknowns are the coordinates of $Q_1$ and $Q_2$. For the simple case where $j=i+1$, the control points are chosen to form a straight line segment represented as a degenerate cubic Bézier.\n\n    b.  **Error Calculation**: The error $E_{i,j}$ is the discrete symmetric Hausdorff distance $d_H(A_{i:j}, B_{i:j})$. The set $A_{i:j}$ is generated by densifying the polyline $P_i, \\dots, P_j$, sampling $q=4$ points on each sub-segment $[P_k, P_{k+1}]$. The set $B_{i:j}$ is generated by sampling $M_B=20$ points from the fitted Bézier curve. The distance $d_H(A, B)$ is defined as $\\max\\left\\{\\max_{a\\in A}\\min_{b\\in B}\\|a-b\\|_2, \\max_{b\\in B}\\min_{a\\in A}\\|a-b\\|_2\\right\\}$. Storing these errors in a matrix $E$ of size $N \\times N$ facilitates the final optimization step.\n\n3.  **Optimal Partitioning via Dynamic Programming**: The main optimization goal is to find the minimum number of segments, $M$, for a given tolerance $\\varepsilon$. This is a classic shortest path problem on a directed acyclic graph (DAG). The nodes of the graph are the indices of the points, $\\{0, 1, \\dots, N-1\\}$. A directed edge exists from node $j$ to node $k$ (with $j < k$) if the error of fitting a single segment from $P_j$ to $P_k$, i.e., $E_{j,k}$, is less than or equal to the tolerance $\\varepsilon$. Each such edge has a weight of $1$. The problem then becomes finding the shortest path from node $0$ to node $N-1$.\n\n    Let $M[k]$ be the minimum number of segments required to approximate the curve from $P_0$ to $P_k$. We can formulate a dynamic programming recurrence:\n    $$M[k] = \\min_{0 \\le j  k, \\text{ s.t. } E_{j,k} \\le \\varepsilon} \\{M[j] + 1\\}$$\n    The base case is $M[0] = 0$. We compute $M[k]$ for $k=1, \\dots, N-1$. The final answer for a given $\\varepsilon$ is $M[N-1]$. If $M[N-1]$ remains at its initial infinite value, it signifies that no valid partition exists, and the result is $-1$. This procedure is repeated for each tolerance $\\varepsilon$ in the provided test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to solve the glyph approximation problem.\n    It orchestrates curve generation, error precomputation, and optimization.\n    \"\"\"\n    \n    # Define problem constants.\n    N = 140\n    Q_DENSITY = 4\n    BEZIER_SAMPLES = 20\n    TEST_TOLERANCES = [0.12, 0.08, 0.05, 0.03]\n\n    def generate_curve_points(num_points):\n        \"\"\"Generates the N sample points of the glyph-like curve.\"\"\"\n        t = np.linspace(0, 1, num_points)\n        u = 2 * t - 1\n        envelope = 0.9 - 0.4 * np.cos(2 * np.pi * t)\n        \n        x = u + 0.06 * np.sin(8 * np.pi * t) + 0.02 * np.sin(14 * np.pi * t)\n        y = (0.9 * envelope * np.sin(np.pi * u) + \n             0.2 * np.sin(3 * np.pi * u) + \n             0.08 * np.sin(5 * np.pi * u))\n        \n        return np.vstack((x, y)).T\n\n    def fit_bezier(points_segment):\n        \"\"\"\n        Fits a cubic Bézier curve to a segment of points.\n        Returns the four control points.\n        \"\"\"\n        n_pts = len(points_segment)\n        q0 = points_segment[0]\n        q3 = points_segment[-1]\n        \n        if n_pts == 2:\n            # Special case for j = i + 1, a straight line\n            q1 = q0 + (q3 - q0) / 3.0\n            q2 = q0 + 2.0 * (q3 - q0) / 3.0\n            return np.array([q0, q1, q2, q3])\n\n        # Chord length parameterization for non-linear segments\n        chord_lengths = np.linalg.norm(np.diff(points_segment, axis=0), axis=1)\n        s = np.concatenate(([0], np.cumsum(chord_lengths)))\n        \n        # Avoid division by zero for coincident points\n        if s[-1] == 0:\n            t = np.linspace(0, 1, n_pts)\n        else:\n            t = s / s[-1]\n\n        t_interior = t[1:-1]\n        p_interior = points_segment[1:-1]\n        \n        # Bernstein basis polynomials\n        c1 = 3 * t_interior * (1 - t_interior)**2\n        c2 = 3 * t_interior**2 * (1 - t_interior)\n        \n        # System matrix A for Ax = b\n        a_mat = np.vstack([c1, c2]).T\n        \n        # RHS vector b\n        rhs = (p_interior - \n               (1 - t_interior)[:, np.newaxis]**3 * q0 - \n               t_interior[:, np.newaxis]**3 * q3)\n        \n        # Solve least-squares for inner control points Q1, Q2\n        try:\n            q12, _, _, _ = np.linalg.lstsq(a_mat, rhs, rcond=None)\n            q1, q2 = q12[0], q12[1]\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix (e.g., collinear points)\n            q1 = q0 + (q3 - q0) / 3.0\n            q2 = q0 + 2.0 * (q3 - q0) / 3.0\n        \n        return np.array([q0, q1, q2, q3])\n\n    def evaluate_bezier(ctrl_pts, num_points):\n        \"\"\"Evaluates a Bézier curve at uniformly spaced parameters.\"\"\"\n        tau = np.linspace(0, 1, num_points)\n        tau_ = 1 - tau\n        \n        b = (tau_[:, np.newaxis]**3 * ctrl_pts[0] + \n             3 * tau_[:, np.newaxis]**2 * tau[:, np.newaxis] * ctrl_pts[1] + \n             3 * tau_[:, np.newaxis] * tau[:, np.newaxis]**2 * ctrl_pts[2] + \n             tau[:, np.newaxis]**3 * ctrl_pts[3])\n        return b\n\n    def get_densified_polyline(points_segment, q_val):\n        \"\"\"Densifies a polyline segment by sampling each sub-segment.\"\"\"\n        points_list = []\n        for k in range(len(points_segment) - 1):\n            points_list.append(np.linspace(points_segment[k], points_segment[k+1], q_val, endpoint=False))\n        points_list.append(points_segment[-1].reshape(1, 2))\n        return np.vstack(points_list)\n\n    def symmetric_hausdorff(set_a, set_b):\n        \"\"\"Computes the discrete symmetric Hausdorff distance between two point sets.\"\"\"\n        dists_ab = cdist(set_a, set_b, 'euclidean')\n        h_a_b = np.max(np.min(dists_ab, axis=1))\n        h_b_a = np.max(np.min(dists_ab, axis=0))\n        return np.max([h_a_b, h_b_a])\n\n    def precompute_errors(p_all):\n        \"\"\"Precomputes the fitting error for all possible segments (i, j).\"\"\"\n        num_points = len(p_all)\n        error_matrix = np.full((num_points, num_points), np.inf)\n        \n        for i in range(num_points):\n            for j in range(i + 1, num_points):\n                segment = p_all[i:j+1]\n                ctrl_pts = fit_bezier(segment)\n                \n                a_set = get_densified_polyline(segment, Q_DENSITY)\n                b_set = evaluate_bezier(ctrl_pts, BEZIER_SAMPLES)\n                \n                error_matrix[i, j] = symmetric_hausdorff(a_set, b_set)\n        return error_matrix\n\n    def find_min_segments(errors, epsilon):\n        \"\"\"Finds the minimum number of segments using dynamic programming.\"\"\"\n        num_points = errors.shape[0]\n        dp = np.full(num_points, np.inf)\n        dp[0] = 0\n        \n        for k in range(1, num_points):\n            for j in range(k):\n                if errors[j, k] = epsilon:\n                    if dp[j] != np.inf:\n                        dp[k] = min(dp[k], dp[j] + 1)\n        \n        result = dp[-1]\n        return int(result) if result != np.inf else -1\n\n    # Main execution flow\n    points = generate_curve_points(N)\n    errors = precompute_errors(points)\n    \n    results = []\n    for eps in TEST_TOLERANCES:\n        min_segs = find_min_segments(errors, eps)\n        results.append(min_segs)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2425561"}]}