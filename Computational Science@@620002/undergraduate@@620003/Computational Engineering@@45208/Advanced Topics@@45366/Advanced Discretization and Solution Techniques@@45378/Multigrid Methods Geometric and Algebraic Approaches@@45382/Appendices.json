{"hands_on_practices": [{"introduction": "The most effective way to grasp the power of multigrid methods is to build one from the ground up. This practice [@problem_id:2415666] guides you through the implementation of a complete geometric multigrid (GMG) V-cycle for the classic Poisson equation. By assembling the solver from its fundamental components—smoothing, restriction, and prolongation—and testing different strategies for the coarse-grid solve, you will gain a concrete understanding of how these parts work in concert to achieve remarkable efficiency.", "problem": "Construct a complete program that compares two geometric multigrid solvers for the discrete two-dimensional Poisson problem on the unit square. Consider the boundary value problem given by $-\\Delta u = f$ on $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions $u=0$ on $\\partial\\Omega$. Discretize the operator using the standard five-point finite difference stencil on a uniform Cartesian grid with $N$ interior points in each spatial direction (grid spacing $h = 1/(N+1)$). The discrete linear system is $A \\mathbf{u} = \\mathbf{b}$, where $A$ is the $N^2 \\times N^2$ sparse matrix corresponding to the five-point stencil with coefficients $4$ on the diagonal and $-1$ on each of the four nearest neighbors (the factor $1/h^2$ is absorbed into the right-hand side by setting $\\mathbf{b} = h^2 f$ sampled at interior grid points). For the manufactured forcing, use $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$. For the zero-forcing edge case, use $f(x,y) \\equiv 0$.\n\nDefine two solvers that both employ a geometric multigrid V-cycle with the following common specifications: coarsening halves $N$ in each spatial direction at each level; the coarsest grid has $N_{\\min} = 4$ interior points per direction; the restriction operator is full-weighting, the prolongation operator is bilinear interpolation; the smoother is weighted Jacobi with relaxation weight $\\omega = 2/3$; the number of pre-smoothing sweeps is $\\nu_1 = 2$ and the number of post-smoothing sweeps is $\\nu_2 = 2$; each V-cycle uses a zero initial correction on the coarse solve. The hierarchy at each level uses a rediscretized operator $A_{\\ell}$ of the same five-point finite difference type corresponding to that level’s grid size.\n\nThe two solvers differ only at the coarsest grid:\n- Solver S (standard baseline): on the coarsest grid, instead of solving exactly, perform $\\nu_0 = 20$ additional weighted Jacobi iterations to approximately solve the coarse-grid linear system for the current right-hand side.\n- Solver L (Lower–Upper factorization variant): on the coarsest grid, solve the coarse-grid linear system exactly using a full Lower–Upper (LU) decomposition.\n\nFor both solvers, the initial guess on the finest grid is the zero vector. Let the Euclidean norm be denoted by $\\lVert \\cdot \\rVert_2$. For a given tolerance $\\tau > 0$, declare convergence when the relative residual satisfies $\\lVert \\mathbf{b} - A \\mathbf{u}^{(k)} \\rVert_2 / \\lVert \\mathbf{b} \\rVert_2 \\leq \\tau$, where $\\mathbf{u}^{(k)}$ is the current iterate after $k$ V-cycles. If $\\lVert \\mathbf{b} \\rVert_2 = 0$, treat the problem as already satisfied with $k=0$. Impose a maximum number of V-cycles $M = 200$; if the solver has not met the convergence condition within $M$ V-cycles, report non-convergence.\n\nTest Suite. Implement and run the program on the following four test cases, each specified by a tuple $(N,\\tau,\\text{rhs})$:\n- Case $1$: $(N,\\tau,\\text{rhs}) = (16, 10^{-8}, \\text{manufactured})$ where $\\text{rhs}$ uses $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n- Case $2$: $(N,\\tau,\\text{rhs}) = (64, 10^{-8}, \\text{manufactured})$.\n- Case $3$: $(N,\\tau,\\text{rhs}) = (8, 10^{-8}, \\text{zero})$ where $\\text{rhs}$ uses $f(x,y) \\equiv 0$.\n- Case $4$: $(N,\\tau,\\text{rhs}) = (32, 10^{-12}, \\text{manufactured})$.\n\nFor each case, run Solver S and Solver L. For each solver, record the number of V-cycles $k_S$ and $k_L$ used until convergence, with the convention that $k$ equals $M$ if convergence is not achieved within $M$ V-cycles. Also record the boolean convergence indicators $c_S$ and $c_L$ that are true if and only if the solver converged within $M$ V-cycles. Finally, define a speedup metric $s$ for each case as follows:\n- If $c_S$ and $c_L$ are both true and $k_L > 0$, let $s = k_S / k_L$.\n- If $k_S = 0$ and $k_L = 0$, let $s = 1.0$.\n- Otherwise, let $s = -1.0$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each case must be a list in the form $[k_S, k_L, c_S, c_L, s]$. The final output must therefore be a list of four lists corresponding to the four cases, for example, $[[k_{S,1}, k_{L,1}, c_{S,1}, c_{L,1}, s_1],[k_{S,2}, k_{L,2}, c_{S,2}, c_{L,2}, s_2],[k_{S,3}, k_{L,3}, c_{S,3}, c_{L,3}, s_3],[k_{S,4}, k_{L,4}, c_{S,4}, c_{L,4}, s_4]]$.", "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded, well-posed, internally consistent, and provides an unambiguous set of instructions for implementing and comparing two geometric multigrid solvers. The problem is a standard exercise in computational science and engineering. We shall proceed with the derivation and implementation of the solution.\n\nThe problem under consideration is the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the unit square $\\Omega = (0,1) \\times (0,1)$:\n$$ -\\Delta u(x,y) = f(x,y) \\quad \\text{for } (x,y) \\in \\Omega $$\n$$ u(x,y) = 0 \\quad \\text{for } (x,y) \\in \\partial\\Omega $$\n\nWe discretize this problem on a uniform Cartesian grid with $N$ interior points in each direction. The grid spacing is $h = 1/(N+1)$. The grid points are $(x_i, y_j) = (i h, j h)$ for $i,j \\in \\{1, \\dots, N\\}$. The continuous operator $-\\Delta$ is approximated using the standard five-point finite difference stencil. This yields a linear system of equations $A \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the solution values $u(x_i,y_j)$ at the interior grid points, lexicographically ordered. The matrix $A$ is an $N^2 \\times N^2$ block-tridiagonal matrix derived from the stencil. The problem specifies that the factor $1/h^2$ from the stencil is absorbed into the right-hand side, so the entries of $A$ are:\n$$\nA_{k,k} = 4, \\quad \\text{and} \\quad A_{k,l} = -1 \\text{ if point } l \\text{ is a direct neighbor of point } k\n$$\nThe right-hand side vector is $\\mathbf{b}$, where its components corresponding to point $(i,j)$ are given by $b_{ij} = h^2 f(x_i, y_j)$.\n\nThe solution is found using a geometric multigrid V-cycle. A hierarchy of grids is constructed by coarsening. A fine grid with $N_f$ interior points in each direction is coarsened to a grid with $N_c = N_f/2$ points. This process is repeated until a minimum grid size of $N_{\\min}=4$ is reached. At each level $\\ell$ of the hierarchy, a discrete operator $A_\\ell$ of the same five-point structure is re-derived for the corresponding grid size $N_\\ell$.\n\nThe components of the multigrid cycle are as follows:\n\n1.  **Smoother**: A weighted Jacobi smoother with relaxation weight $\\omega = 2/3$ is used for pre- and post-smoothing. Given a current solution iterate $\\mathbf{u}^{(k)}$, the next iterate $\\mathbf{u}^{(k+1)}$ is computed as:\n    $$ \\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\omega D^{-1}(\\mathbf{b} - A\\mathbf{u}^{(k)}) $$\n    Since the diagonal entries of $A$ are all $4$, the diagonal matrix $D$ is $4I$, where $I$ is the identity matrix. The update rule simplifies to:\n    $$ \\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\frac{\\omega}{4}(\\mathbf{b} - A\\mathbf{u}^{(k)}) $$\n    The specified number of pre-smoothing sweeps is $\\nu_1 = 2$, and the number of post-smoothing sweeps is $\\nu_2 = 2$.\n\n2.  **Restriction**: The residual is transferred from a fine grid to a coarse grid using a full-weighting restriction operator $I_h^{2h}$. For a fine-grid residual $r_h$ and a coarse-grid residual $r_{2h}$, with coarse-grid point $(i,j)$ corresponding to fine-grid point $(2i, 2j)$ (in a coordinate system where indices span the whole grid including boundaries), the operation is defined by the stencil:\n    $$ (I_h^{2h} r_h)_{i,j} = \\frac{1}{16} \\begin{pmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 1 \\end{pmatrix} r_h $$\n    This means the value at a coarse-grid point is a weighted average of the values at the $9$ corresponding fine-grid points centered around it.\n\n3.  **Prolongation**: The correction computed on the coarse grid is transferred to the fine grid using a bilinear interpolation operator $I_{2h}^h$. This operator is the transpose of the full-weighting restriction operator, scaled such that $(I_{2h}^h) = 4(I_h^{2h})^T$. For a coarse-grid correction $e_{2h}$, the fine-grid correction $e_h$ is computed as follows, where coarse-grid nodes $(i,j)$ are aligned with fine-grid nodes $(2i,2j)$:\n    \\begin{itemize}\n        \\item $e_h(2i, 2j) = e_{2h}(i,j)$ (Injection)\n        \\item $e_h(2i+1, 2j) = \\frac{1}{2}(e_{2h}(i,j) + e_{2h}(i+1,j))$\n        \\item $e_h(2i, 2j+1) = \\frac{1}{2}(e_{2h}(i,j) + e_{2h}(i,j+1))$\n        \\item $e_h(2i+1, 2j+1) = \\frac{1}{4}(e_{2h}(i,j) + e_{2h}(i+1,j) + e_{2h}(i,j+1) + e_{2h}(i+1,j+1))$\n    \\end{itemize}\n    Boundary values of the coarse-grid correction are taken as $0$.\n\nThe V-cycle algorithm proceeds as follows for a given level $\\ell$ with system $A_\\ell \\mathbf{u}_\\ell = \\mathbf{b}_\\ell$:\n1.  **Base Case**: If on the coarsest grid ($N_\\ell = N_{\\min}=4$), approximately or exactly solve $A_\\ell \\mathbf{e}_\\ell = \\mathbf{b}_\\ell$.\n2.  **Recursive Step**:\n    a. **Pre-smoothing**: Apply $\\nu_1=2$ weighted Jacobi sweeps to $A_\\ell \\mathbf{u}_\\ell = \\mathbf{b}_\\ell$ starting with the current estimate $\\mathbf{u}_\\ell$.\n    b. **Compute Residual**: Calculate the residual $\\mathbf{r}_\\ell = \\mathbf{b}_\\ell - A_\\ell \\mathbf{u}_\\ell$.\n    c. **Restriction**: Restrict the residual to the next coarser grid: $\\mathbf{r}_{\\ell+1} = I_h^{2h} \\mathbf{r}_\\ell$.\n    d. **Coarse-Grid Correction**: Solve the coarse-grid problem $A_{\\ell+1} \\mathbf{e}_{\\ell+1} = \\mathbf{r}_{\\ell+1}$ by a recursive call to the V-cycle, starting with a zero initial guess for the correction $\\mathbf{e}_{\\ell+1}$.\n    e. **Prolongation**: Interpolate the correction back to the fine grid: $\\mathbf{e}_\\ell = I_{2h}^h \\mathbf{e}_{\\ell+1}$.\n    f. **Correction**: Update the solution: $\\mathbf{u}_\\ell \\leftarrow \\mathbf{u}_\\ell + \\mathbf{e}_\\ell$.\n    g. **Post-smoothing**: Apply $\\nu_2=2$ weighted Jacobi sweeps to $A_\\ell \\mathbf{u}_\\ell = \\mathbf{b}_\\ell$.\n\nTwo solvers are implemented, differing only in the base case (coarsest grid solve):\n-   **Solver S**: Approximately solves the coarsest-grid system using $\\nu_0 = 20$ weighted Jacobi iterations.\n-   **Solver L**: Solves the coarsest-grid system exactly using an LU factorization of the $16 \\times 16$ coarse-grid matrix $A_{\\min}$. The LU factorization is pre-computed for efficiency.\n\nFor each test case, both solvers start with a zero vector as the initial guess on the finest grid. Iterations continue until the relative residual $\\lVert \\mathbf{b} - A \\mathbf{u}^{(k)} \\rVert_2 / \\lVert \\mathbf{b} \\rVert_2$ falls below a tolerance $\\tau$, or a maximum of $M=200$ V-cycles is reached. If $\\lVert\\mathbf{b}\\rVert_2 = 0$, the problem is considered solved in $k=0$ cycles. The number of cycles $k_S, k_L$, convergence status $c_S, c_L$, and a speedup metric $s$ are reported.", "answer": "```python\nimport numpy as np\nimport scipy.sparse\nimport scipy.sparse.linalg\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef create_A(N):\n    \"\"\"Creates the N^2 x N^2 sparse matrix for the 2D Poisson problem.\"\"\"\n    if N == 0:\n        return scipy.sparse.csr_matrix((0,0))\n    D_1d = scipy.sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(N, N), format='csr')\n    I_N = scipy.sparse.identity(N, format='csr')\n    A = scipy.sparse.kron(D_1d, I_N) + scipy.sparse.kron(I_N, D_1d)\n    return A.tocsr()\n\ndef jacobi_sweep(u, b, omega):\n    \"\"\"Performs one weighted Jacobi sweep on a 2D grid.\"\"\"\n    u_padded = np.pad(u, 1, mode='constant', constant_values=0)\n    # A*u term without the diagonal\n    laplacian_u_no_D = -(u_padded[:-2, 1:-1] +\n                       u_padded[2:, 1:-1] +\n                       u_padded[1:-1, :-2] +\n                       u_padded[1:-1, 2:])\n    Au = 4 * u + laplacian_u_no_D\n    residual = b - Au\n    u_new = u + (omega / 4.0) * residual\n    return u_new\n\ndef restrict(r_f):\n    \"\"\"Full-weighting restriction from a fine grid to a coarse grid.\"\"\"\n    Nf = r_f.shape[0]\n    Nc = Nf // 2\n    \n    # Pad fine residual for easier stencil application\n    r_f_p = np.pad(r_f, 1, mode='constant', constant_values=0)\n\n    # coarse grid node (i,j) is centered at fine grid node (2i,2j)\n    # Python indices: coarse (i, j) corresponds to fine (2i, 2j)\n    # Stencil center: fine_padded (2i+1, 2j+1)\n    \n    r_c = np.zeros((Nc, Nc))\n    for i in range(Nc):\n        for j in range(Nc):\n            pi, pj = 2 * i + 1, 2 * j + 1 # Padded indices\n            r_c[i, j] = (\n                4.0 * r_f_p[pi, pj] +\n                2.0 * (r_f_p[pi-1, pj] + r_f_p[pi+1, pj] + r_f_p[pi, pj-1] + r_f_p[pi, pj+1]) +\n                1.0 * (r_f_p[pi-1, pj-1] + r_f_p[pi-1, pj+1] + r_f_p[pi+1, pj-1] + r_f_p[pi+1, pj+1])\n            ) / 16.0\n            \n    return r_c\n\ndef prolongate(e_c):\n    \"\"\"Bilinear interpolation from a coarse grid to a fine grid.\"\"\"\n    Nc = e_c.shape[0]\n    Nf = 2 * Nc\n    e_f = np.zeros((Nf, Nf))\n    \n    e_c_p = np.pad(e_c, ((0, 1), (0, 1)), mode='constant')\n\n    # Direct injection\n    e_f[::2, ::2] = e_c\n    \n    # Interpolation on axes\n    e_f[1::2, ::2] = 0.5 * (e_c_p[:-1, :-1] + e_c_p[1:, :-1])\n    e_f[::2, 1::2] = 0.5 * (e_c_p[:-1, :-1] + e_c_p[:-1, 1:])\n\n    # Interpolation at center of cells\n    e_f[1::2, 1::2] = 0.25 * (e_c_p[:-1, :-1] + e_c_p[1:, :-1] + e_c_p[:-1, 1:] + e_c_p[1:, 1:])\n    \n    return e_f\n\nclass MultigridSolver:\n    def __init__(self, N, N_min=4, nu_1=2, nu_2=2, omega=2/3):\n        self.N_min = N_min\n        self.nu_1 = nu_1\n        self.nu_2 = nu_2\n        self.omega = omega\n        self.grids = self._build_grid_hierarchy(N)\n\n    def _build_grid_hierarchy(self, N):\n        grids = []\n        curr_N = N\n        while curr_N >= self.N_min:\n            grid_info = {'N': curr_N, 'A': create_A(curr_N)}\n            if curr_N == self.N_min:\n                # Pre-compute LU for Solver L\n                A_coarse_dense = grid_info['A'].toarray()\n                grid_info['LU'] = lu_factor(A_coarse_dense)\n            grids.append(grid_info)\n            if curr_N == self.N_min:\n                break\n            curr_N //= 2\n        return grids\n\n    def v_cycle(self, u, b, level, solver_type, nu_0=20):\n        N = self.grids[level]['N']\n        A = self.grids[level]['A']\n\n        if N == self.N_min:\n            u_coarse = np.zeros_like(b)\n            if solver_type == 'S':\n                for _ in range(nu_0):\n                    u_coarse = jacobi_sweep(u_coarse, b, self.omega)\n                return u_coarse\n            else: # solver_type == 'L'\n                lu, piv = self.grids[level]['LU']\n                sol_flat = lu_solve((lu, piv), b.flatten())\n                return sol_flat.reshape((N, N))\n        \n        # Pre-smoothing\n        u_smoothed = u.copy()\n        for _ in range(self.nu_1):\n            u_smoothed = jacobi_sweep(u_smoothed, b, self.omega)\n        \n        # Compute residual and restrict\n        res_fine_flat = b.flatten() - A.dot(u_smoothed.flatten())\n        res_fine = res_fine_flat.reshape((N, N))\n        res_coarse = restrict(res_fine)\n        \n        # Recursive call for coarse-grid correction\n        e_coarse = self.v_cycle(np.zeros_like(res_coarse), res_coarse, level + 1, solver_type, nu_0)\n        \n        # Prolongate correction and update solution\n        e_fine = prolongate(e_coarse)\n        u_corrected = u_smoothed + e_fine\n        \n        # Post-smoothing\n        u_final = u_corrected.copy()\n        for _ in range(self.nu_2):\n            u_final = jacobi_sweep(u_final, b, self.omega)\n        \n        return u_final\n\ndef solve_case(N, tau, rhs_type, M=200):\n    solver = MultigridSolver(N)\n    h = 1.0 / (N + 1)\n    \n    if rhs_type == 'manufactured':\n        x = np.linspace(h, 1.0 - h, N)\n        y = np.linspace(h, 1.0 - h, N)\n        xx, yy = np.meshgrid(x, y)\n        f = 2 * np.pi**2 * np.sin(np.pi * xx) * np.sin(np.pi * yy)\n        b_grid = h**2 * f\n    else: # 'zero'\n        b_grid = np.zeros((N, N))\n        \n    b_flat = b_grid.flatten()\n    norm_b = np.linalg.norm(b_flat)\n\n    if norm_b == 0:\n        return (0, 0, True, True, 1.0)\n    \n    # Run Solver S\n    u_s = np.zeros((N, N))\n    k_S = M\n    c_S = False\n    for k in range(1, M + 1):\n        u_s = solver.v_cycle(u_s, b_grid, 0, 'S')\n        residual_flat = b_flat - solver.grids[0]['A'].dot(u_s.flatten())\n        if np.linalg.norm(residual_flat) / norm_b = tau:\n            k_S = k\n            c_S = True\n            break\n            \n    # Run Solver L\n    u_l = np.zeros((N, N))\n    k_L = M\n    c_L = False\n    for k in range(1, M + 1):\n        u_l = solver.v_cycle(u_l, b_grid, 0, 'L')\n        residual_flat = b_flat - solver.grids[0]['A'].dot(u_l.flatten())\n        if np.linalg.norm(residual_flat) / norm_b = tau:\n            k_L = k\n            c_L = True\n            break\n\n    # Compute speedup\n    s = -1.0\n    if c_S and c_L:\n        if k_L > 0:\n            s = k_S / k_L\n        # k_L = 0 handled by initial norm_b==0 check\n    \n    return [k_S, k_L, c_S, c_L, s]\n\ndef solve():\n    test_cases = [\n        (16, 1e-8, 'manufactured'),\n        (64, 1e-8, 'manufactured'),\n        (8, 1e-8, 'zero'),\n        (32, 1e-12, 'manufactured'),\n    ]\n\n    all_results = []\n    for N, tau, rhs_type in test_cases:\n        result = solve_case(N=N, tau=tau, rhs_type=rhs_type)\n        # Format boolean and float for output\n        formatted_result = [\n            result[0], result[1],\n            bool(result[2]), bool(result[3]),\n            float(result[4])\n        ]\n        all_results.append(str(formatted_result).replace(\"'\", \"\"))\n    \n    print(f\"[{','.join(all_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2415666"}, {"introduction": "A functional solver is a great start, but a truly skilled practitioner also understands how and why a method can fail. This exercise [@problem_id:2415629] focuses on debugging by isolating the coarse-grid correction step, the heart of the multigrid algorithm. You will explore what happens when the crucial algebraic consistency between operators is broken, learning to diagnose issues that can cause convergence to stall or even diverge. This practice illuminates the theoretical underpinnings, such as the Galerkin principle $A_H = R A_h P$, that ensure multigrid's robustness.", "problem": "Consider the linear system defined by the two-dimensional discrete Poisson operator with homogeneous Dirichlet boundary conditions. Let the fine grid have $n_f \\times n_f$ interior unknowns with $n_f = 2 n_c + 1$, where $n_c$ is the number of coarse interior unknowns in one dimension. The fine-grid mesh width is $h = \\frac{1}{n_f+1}$, and the coarse-grid mesh width is $H = 2h = \\frac{1}{n_c+1}$. The fine-grid operator $A_h \\in \\mathbb{R}^{n_f^2 \\times n_f^2}$ is the standard five-point stencil discrete Laplacian given by\n$$\nA_h = \\frac{1}{h^2} \\left( I_{n_f} \\otimes T_{n_f} + T_{n_f} \\otimes I_{n_f} \\right),\n$$\nwhere $I_{n_f}$ is the $n_f \\times n_f$ identity matrix and $T_{n_f} \\in \\mathbb{R}^{n_f \\times n_f}$ is the tridiagonal matrix with diagonal entries $2$ and sub-/super-diagonal entries $-1$.\n\nLet the full-weighting restriction operator $R_{\\mathrm{fw}} \\in \\mathbb{R}^{n_c^2 \\times n_f^2}$ be defined by applying the $3 \\times 3$ stencil with weights\n$$\n\\frac{1}{16} \\begin{bmatrix}\n1  2  1 \\\\\n2  4  2 \\\\\n1  2  1\n\\end{bmatrix}\n$$\ncentered at the fine-grid point aligned with the coarse-grid point; that is, for each coarse-grid index $(i,j)$ with $i,j \\in \\{0,\\dots,n_c-1\\}$, the aligned fine-grid index is $(2i+1,2j+1)$ in zero-based indexing over interior points. Define the prolongation operator $P \\in \\mathbb{R}^{n_f^2 \\times n_c^2}$ by $P = 2 R_{\\mathrm{fw}}^{\\top}$.\n\nFor a coarse-grid vector $e_H \\in \\mathbb{R}^{n_c^2}$, define the fine-grid right-hand side by\n$$\nb = A_h \\, P \\, e_H.\n$$\nConsider a single coarse-grid correction step without any smoothing, starting from the fine-grid initial guess $x_0 = 0$. For a given choice of restriction operator $R \\in \\mathbb{R}^{n_c^2 \\times n_f^2}$ and coarse-grid operator $A_H \\in \\mathbb{R}^{n_c^2 \\times n_c^2}$, compute\n$$\nr_0 = b - A_h x_0 = b, \\quad y = A_H^{-1} \\, R \\, r_0, \\quad x_1 = x_0 + P \\, y, \\quad r_1 = b - A_h x_1.\n$$\nFor each test case below, compute the scalar\n$$\nq = \\frac{\\lVert r_1 \\rVert_2}{\\lVert r_0 \\rVert_2}.\n$$\n\nYou must implement the operators exactly as defined above using the stated finite-difference discretization and transfer stencils. The coarse-grid operator may be either the Galerkin product $A_H = R \\, A_h \\, P$ or the rediscretized operator\n$$\nA_H^{\\text{redi}} = \\frac{1}{H^2} \\left( I_{n_c} \\otimes T_{n_c} + T_{n_c} \\otimes I_{n_c} \\right).\n$$\nAll matrices must be formed in a manner consistent with the above definitions, and all linear solves must be performed exactly with respect to floating-point arithmetic (for example, by direct solution on the coarse grid). No smoothing iterations are to be applied.\n\nTest suite. For each test case, specify $n_c$, the coarse-grid vector $e_H$ (standard basis vector at the center index), the restriction operator $R$, and the coarse-grid operator $A_H$:\n- Test $1$ (baseline, consistent Galerkin): $n_c = 15$, $e_H$ equals the Kronecker delta at the central coarse node, $R = R_{\\mathrm{fw}}$, $A_H = R \\, A_h \\, P$.\n- Test $2$ (scaled coarse-grid operator bug): $n_c = 15$, same $e_H$ and $R = R_{\\mathrm{fw}}$, but $A_H = \\alpha (R A_h P)$ with $\\alpha = 0.1$.\n- Test $3$ (mismatched restriction scaling versus rediscretization): $n_c = 15$, same $e_H$, $R = c R_{\\mathrm{fw}}$ with $c = 3$, and $A_H = A_H^{\\text{redi}}$ constructed on the coarse grid with mesh width $H$.\n- Test $4$ (small-grid edge case, scaled Galerkin): $n_c = 1$, $e_H$ equals the single coarse basis vector, $R = R_{\\mathrm{fw}}$, and $A_H = \\alpha (R A_h P)$ with $\\alpha = 2$.\n\nYour program must compute $q$ for each test case in the order listed and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[q_1,q_2,q_3,q_4]$. Each $q$ must be a real number (floating-point). No input is provided to the program, and no units are involved in this problem. Angles do not appear in this problem, and no percentages are to be used.", "solution": "The problem presented is a standard, well-posed exercise in the field of computational engineering, specifically concerning the analysis of multigrid methods. It requires the construction of discrete operators for the two-dimensional Poisson equation and the evaluation of a single coarse-grid correction step under various configurations. The problem is scientifically grounded, self-contained, and all terms are defined with sufficient precision for a unique, verifiable solution to be computed. All definitions for operators and grid hierarchies are standard in the literature on multigrid methods. We will proceed with the solution.\n\nThe core of the problem is to compute the reduction in the norm of the residual after one coarse-grid correction step, for a linear system $A_h x = b$. The process, starting from an initial guess $x_0 = 0$, is defined as:\n$1$. Compute initial residual: $r_0 = b - A_h x_0 = b$.\n$2$. Restrict the residual to the coarse grid: $r_H = R r_0$.\n$3$. Solve the coarse-grid error equation: $A_H y = r_H$, yielding $y = A_H^{-1} r_H$.\n$4$. Prolongate the coarse-grid correction to the fine grid: $e_h = P y$.\n$5$. Update the solution: $x_1 = x_0 + e_h = P y$.\n$6$. Compute the new residual: $r_1 = b - A_h x_1$.\n$7$. Evaluate the residual reduction factor: $q = \\frac{\\lVert r_1 \\rVert_2}{\\lVert r_0 \\rVert_2}$.\n\nThe operators are defined as follows:\n- The fine-grid operator $A_h \\in \\mathbb{R}^{n_f^2 \\times n_f^2}$ is the discrete Laplacian on an $n_f \\times n_f$ interior grid with mesh width $h = \\frac{1}{n_f+1}$. It is given by $A_h = \\frac{1}{h^2} ( I_{n_f} \\otimes T_{n_f} + T_{n_f} \\otimes I_{n_f} )$, where $T_{n_f}$ is the $n_f \\times n_f$ tridiagonal matrix with $2$ on the diagonal and $-1$ on the off-diagonals.\n- The full-weighting restriction operator $R_{\\mathrm{fw}} \\in \\mathbb{R}^{n_c^2 \\times n_f^2}$ uses the stencil $\\frac{1}{16} \\begin{bmatrix} 1  2  1 \\\\ 2  4  2 \\\\ 1  2  1 \\end{bmatrix}$. The coarse grid size $n_c$ is related to the fine grid size $n_f$ by $n_f = 2 n_c + 1$.\n- The prolongation operator is defined as $P = 2 R_{\\mathrm{fw}}^{\\top}$.\n- The coarse-grid operator $A_H$ can be either the Galerkin operator $A_H = R A_h P$ (where $R$ could be a scaled version of $R_{\\mathrm{fw}}$) or the rediscretized operator $A_H^{\\text{redi}} = \\frac{1}{H^2} ( I_{n_c} \\otimes T_{n_c} + T_{n_c} \\otimes I_{n_c} )$ with coarse mesh width $H = 2h = \\frac{1}{n_c+1}$.\n\nThe right-hand side is specifically chosen to be in the range of the operator $A_h P$, i.e., $b = A_h P e_H$, where $e_H$ is a specific coarse-grid vector. This choice simplifies the analysis.\n\nSubstituting the definitions, the new residual $r_1$ can be expressed in terms of the initial residual $r_0$:\n$$\nr_1 = b - A_h x_1 = b - A_h (P y) = b - A_h P A_H^{-1} R r_0 = (I - A_h P A_H^{-1} R) r_0\n$$\nThe matrix $C = I - A_h P A_H^{-1} R$ is the coarse-grid correction operator. The quantity to compute, $q$, is the norm of the new residual relative to the old, given this specific $r_0$.\n\nWe analyze each test case based on this framework.\n\n**Test Case 1: Baseline Galerkin Coarse Grid**\n- Parameters: $n_c = 15$, $R = R_{\\mathrm{fw}}$, $A_H = R A_h P$.\n- In this case, the coarse-grid operator is constructed via the Galerkin principle with consistent interpolation and restriction operators. The setup is self-consistent.\n- The new residual is $r_1 = (I - A_h P (R A_h P)^{-1} R) r_0$.\n- The right-hand side is $r_0 = b = A_h P e_H$. Substituting this into the expression for $r_1$:\n$$\nr_1 = (I - A_h P (R A_h P)^{-1} R) (A_h P e_H) = A_h P e_H - A_h P (R A_h P)^{-1} R (A_h P e_H)\n$$\n- Assuming $R A_h P$ is invertible, we can simplify the term $(R A_h P)^{-1} (R A_h P)$, which becomes the identity matrix on the coarse space.\n$$\nr_1 = A_h P e_H - A_h P (I_{n_c^2}) e_H = A_h P e_H - A_h P e_H = 0\n$$\n- Therefore, the new residual is the zero vector. The ratio $q$ is $\\frac{\\lVert 0 \\rVert_2}{\\lVert r_0 \\rVert_2} = 0$, provided $r_0 \\neq 0$. The operator $A_h P$ has full column rank, so for a non-zero $e_H$, $r_0$ is non-zero.\n- The expected result is $q_1 = 0.0$.\n\n**Test Case 2: Scaled Galerkin Operator (Incorrect Scaling)**\n- Parameters: $n_c = 15$, $R=R_{\\mathrm{fw}}$, $A_H = \\alpha (R A_h P)$ with $\\alpha = 0.1$.\n- This case introduces a deliberate scaling error in the coarse-grid operator.\n- The analysis follows the same path, but with the scaled operator:\n$$\nA_H^{-1} = (\\alpha R A_h P)^{-1} = \\frac{1}{\\alpha} (R A_h P)^{-1}\n$$\n- The new residual is:\n$$\nr_1 = (I - A_h P (\\frac{1}{\\alpha} (R A_h P)^{-1}) R) (A_h P e_H)\n$$\n$$\nr_1 = A_h P e_H - \\frac{1}{\\alpha} A_h P (R A_h P)^{-1} R A_h P e_H = A_h P e_H - \\frac{1}{\\alpha} A_h P e_H = (1 - \\frac{1}{\\alpha}) A_h P e_H\n$$\n- Since $r_0 = A_h P e_H$, we have $r_1 = (1 - \\frac{1}{\\alpha}) r_0$.\n- With $\\alpha = 0.1$, the factor is $1 - \\frac{1}{0.1} = 1 - 10 = -9$.\n- So, $r_1 = -9 r_0$. The ratio of norms is:\n$$\nq = \\frac{\\lVert -9 r_0 \\rVert_2}{\\lVert r_0 \\rVert_2} = |-9| \\frac{\\lVert r_0 \\rVert_2}{\\lVert r_0 \\rVert_2} = 9\n$$\n- The expected result is $q_2 = 9.0$.\n\n**Test Case 3: Mismatched Operators**\n- Parameters: $n_c = 15$, $R = c R_{\\mathrm{fw}}$ with $c = 3$, $A_H = A_H^{\\text{redi}}$.\n- Here, the coarse-grid operator is not the Galerkin product. Instead, it is the operator obtained by rediscretizing the Poisson problem on the coarse grid. Furthermore, the restriction operator is scaled by a factor $c=3$. Note that the prolongation operator $P$ remains fixed as $P=2R_{\\mathrm{fw}}^{\\top}$.\n- The new residual is $r_1 = (I - A_h P (A_H^{\\text{redi}})^{-1} (c R_{\\mathrm{fw}})) r_0$.\n- There is no simple algebraic cancellation as in the previous cases because $A_H^{\\text{redi}} \\neq c R_{\\mathrm{fw}} A_h P$. While $A_H^{\\text{redi}}$ is a good approximation of the unscaled Galerkin operator $R_{\\mathrm{fw}} A_h P$, the mismatch, amplified by the factor $c=3$, will lead to a non-zero residual.\n- The value of $q_3 = \\frac{\\lVert r_1 \\rVert_2}{\\lVert r_0 \\rVert_2}$ must be determined by numerical computation. This involves constructing all specified matrices ($A_h$, $R_{\\mathrm{fw}}$, $P$, $A_H^{\\text{redi}}$), performing the matrix-vector operations and the coarse-grid solve, and finally computing the vector norms.\n\n**Test Case 4: Small Grid, Scaled Galerkin**\n- Parameters: $n_c = 1$, $R=R_{\\mathrm{fw}}$, $A_H = \\alpha(R A_h P)$ with $\\alpha = 2$.\n- This case is structurally identical to Test Case $2$, but with different parameters for grid size and scaling factor. For $n_c=1$, we have $n_f = 2(1)+1 = 3$. The coarse grid has $1$ unknown, and the fine grid has $3 \\times 3 = 9$ unknowns.\n- The analytical result is the same: $r_1 = (1 - \\frac{1}{\\alpha}) r_0$.\n- With $\\alpha = 2$, this becomes $r_1 = (1 - \\frac{1}{2}) r_0 = 0.5 r_0$.\n- The ratio of norms is:\n$$\nq = \\frac{\\lVert 0.5 r_0 \\rVert_2}{\\lVert r_0 \\rVert_2} = 0.5\n$$\n- The expected result is $q_4 = 0.5$.\n\nThe implementation will construct these operators using sparse matrix formats and perform the calculations as described to find the numerical value for all four cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, kron, csc_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef create_T_matrix(n):\n    \"\"\"\n    Creates the tridiagonal matrix T_n used in the 2D discrete Laplacian.\n    T_n has 2 on the diagonal and -1 on the sub/super-diagonals.\n    \"\"\"\n    if n == 0:\n        return csc_matrix((0, 0))\n    diagonals = [-1 * np.ones(n - 1), 2 * np.ones(n), -1 * np.ones(n - 1)]\n    return diags(diagonals, [-1, 0, 1], format='csc')\n\ndef create_laplacian_operator(n, h):\n    \"\"\"\n    Creates the 2D discrete Laplacian operator A for an n x n interior grid.\n    \"\"\"\n    if n == 0:\n        return csc_matrix((0, 0))\n    T_n = create_T_matrix(n)\n    I_n = csc_matrix(np.eye(n))\n    A = kron(I_n, T_n) + kron(T_n, I_n)\n    return (1.0 / (h**2)) * A\n\ndef create_full_weighting_restriction(nc):\n    \"\"\"\n    Creates the full-weighting restriction operator R_fw.\n    \"\"\"\n    nf = 2 * nc + 1\n    num_coarse_nodes = nc * nc\n    num_fine_nodes = nf * nf\n\n    if nc == 0:\n        return csc_matrix((0, num_fine_nodes))\n\n    rows = []\n    cols = []\n    data = []\n\n    stencil = (1.0 / 16.0) * np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]])\n\n    for ic in range(nc):\n        for jc in range(nc):\n            kc = ic * nc + jc  # Coarse grid 1D index (row of R)\n            \n            ic_f_center = 2 * ic + 1\n            jc_f_center = 2 * jc + 1\n            \n            for di in range(-1, 2):\n                for dj in range(-1, 2):\n                    if_ = ic_f_center + di\n                    jf_ = jc_f_center + dj\n                    \n                    kf = if_ * nf + jf_ # Fine grid 1D index (col of R)\n                    weight = stencil[di + 1, dj + 1]\n                    \n                    rows.append(kc)\n                    cols.append(kf)\n                    data.append(weight)\n\n    return csc_matrix((data, (rows, cols)), shape=(num_coarse_nodes, num_fine_nodes))\n\ndef solve():\n    \"\"\"\n    Main function to run the multigrid coarse-grid correction simulations.\n    \"\"\"\n    # Test suite definition: (nc, e_H_spec, R_spec, AH_spec)\n    # e_H_spec: 'center' or 'single'\n    # R_spec: (scaling_factor, type), e.g., (1.0, 'fw')\n    # AH_spec: (scaling_factor, type), e.g., (1.0, 'galerkin') or (1.0, 'redi')\n    test_cases = [\n        (15, 'center', (1.0, 'fw'), (1.0, 'galerkin')),\n        (15, 'center', (1.0, 'fw'), (0.1, 'galerkin')),\n        (15, 'center', (3.0, 'fw'), (1.0, 'redi')),\n        (1, 'single', (1.0, 'fw'), (2.0, 'galerkin')),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        nc, e_H_spec, R_spec, AH_spec = case\n\n        # Grid parameters\n        nf = 2 * nc + 1\n        h = 1.0 / (nf + 1)\n        H = 1.0 / (nc + 1)\n        \n        num_coarse_nodes = nc * nc\n        num_fine_nodes = nf * nf\n\n        # Coarse-grid source vector e_H\n        e_H = np.zeros(num_coarse_nodes)\n        if e_H_spec == 'center':\n            center_idx_1d = (nc - 1) // 2\n            center_idx_flat = center_idx_1d * nc + center_idx_1d\n            e_H[center_idx_flat] = 1.0\n        elif e_H_spec == 'single':\n             e_H[0] = 1.0\n\n        # Build operators\n        A_h = create_laplacian_operator(nf, h)\n        R_fw_base = create_full_weighting_restriction(nc)\n        P = (2.0 * R_fw_base.T).tocsc()\n\n        # Restriction operator R for the current case\n        c_R, _ = R_spec\n        R = c_R * R_fw_base\n\n        # Coarse-grid operator A_H for the current case\n        alpha_AH, AH_type = AH_spec\n        if AH_type == 'galerkin':\n            A_H = alpha_AH * (R @ A_h @ P)\n        elif AH_type == 'redi':\n            A_H = alpha_AH * create_laplacian_operator(nc, H)\n\n        # Coarse-grid correction step\n        x0 = np.zeros(num_fine_nodes)\n        b = A_h @ (P @ e_H)\n        \n        r0 = b - A_h @ x0\n        \n        r_H = R @ r0\n        \n        # Solve coarse system\n        y = spsolve(A_H.tocsc(), r_H)\n        \n        x1 = x0 + P @ y\n        \n        r1 = b - A_h @ x1\n        \n        # Compute the ratio q\n        norm_r0 = np.linalg.norm(r0)\n        norm_r1 = np.linalg.norm(r1)\n        \n        if norm_r0 == 0:\n            q = 0.0 if norm_r1 == 0 else np.inf\n        else:\n            q = norm_r1 / norm_r0\n            \n        results.append(q)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2415629"}, {"introduction": "Once you have a working and reliable solver, a final practical question emerges: when do you stop the iteration? This exercise [@problem_id:2415645] challenges you to think critically about designing a robust termination criterion. You will evaluate various strategies for monitoring convergence and learn to create a stopping rule that is resilient to common issues like problem scaling, false convergence, and iteration stagnation. Mastering this aspect is key to deploying iterative solvers confidently in practical scientific and engineering applications.", "problem": "You are solving a linear system $A u = b$ that arises from a standard second-order finite-difference discretization of the Poisson equation $-\\nabla^2 u = f$ on a bounded domain with appropriate boundary conditions. The matrix $A$ is Symmetric Positive Definite (SPD). You use either a Geometric Multigrid (GMG) method or an Algebraic Multigrid (AMG) method as a stand-alone iterative solver that produces a sequence of iterates $\\{u_k\\}_{k \\geq 0}$ with residuals $r_k = b - A u_k$.\n\nThe target is to develop a robust termination criterion that is based on the ratio $\\|r_k\\| / \\|r_0\\|$, where $\\|\\cdot\\|$ denotes a chosen computable vector norm. Your criterion should be:\n- insensitive to scaling of the right-hand side $b$,\n- meaningful across mesh refinements,\n- safe when $\\|r_0\\|$ is small or when the residual stagnates transiently,\n- implementable without requiring unavailable quantities (such as the exact error).\n\nUse the fundamental definitions $r_k = A e_k$ with $e_k = u - u_k$ and standard norm relations for SPD matrices to argue what makes the criterion robust. Assume there exists a mesh-independent contraction factor $\\rho \\in (0,1)$ in the $A$-energy norm $\\|e\\|_A = \\sqrt{e^T A e}$ for a properly designed multigrid cycle.\n\nWhich of the following is the most robust termination criterion under these requirements?\n\nA. Stop at the first $k$ satisfying $\\|r_k\\|_2 \\leq \\max\\{\\tau_{\\mathrm{rel}} \\|r_0\\|_2,\\ \\tau_{\\mathrm{abs}}\\}$ with user-chosen tolerances $\\tau_{\\mathrm{rel}} \\in (0,1)$ and $\\tau_{\\mathrm{abs}} > 0$, and also enforce a minimum cycle count $k \\geq k_{\\min}$ and a maximum $k \\leq k_{\\max}$. Additionally, if the average residual reduction factor over the last $s$ iterations,\n$$\\phi_k = \\left(\\frac{\\|r_k\\|_2}{\\|r_{k-s}\\|_2}\\right)^{1/s},$$\nexceeds a user threshold $\\eta \\in (0,1)$ (indicating stagnation), terminate with a flagged failure.\n\nB. Stop at the first $k$ satisfying $\\|r_k\\|_2 \\leq \\tau_{\\mathrm{abs}}$ for a fixed $\\tau_{\\mathrm{abs}} > 0$, because an absolute residual threshold directly reflects accuracy.\n\nC. Stop as soon as a single-step decrease is observed, i.e., when $\\|r_k\\|_2 / \\|r_{k-1}\\|_2 \\leq \\tau_{\\mathrm{step}}$ for a chosen $\\tau_{\\mathrm{step}} \\in (0,1)$, because multigrid reductions are typically geometric.\n\nD. Stop when the relative change in the iterate is small, i.e., when $\\|u_{k+1} - u_k\\|_2 / \\|u_k\\|_2 \\leq \\tau_{\\mathrm{rel}}$, because small iterate updates imply proximity to the solution.\n\nE. Stop when the residual on the current coarse grid level satisfies $\\|r_k^{(c)}\\|_2 / \\|r_0^{(c)}\\|_2 \\leq \\tau_{\\mathrm{rel}}$, because the coarse grid captures the smooth error that dominates late in the solve.\n\nSelect all that apply.", "solution": "The problem asks to identify the most robust termination criterion for a multigrid solver applied to a linear system $A u = b$ arising from the discretization of the Poisson equation. The matrix $A$ is Symmetric Positive Definite (SPD). A robust criterion must be insensitive to problem scaling, meaningful across different mesh sizes, safe against pathologies like small initial residuals or convergence stagnation, and implementable using available quantities.\n\nThe foundation of our analysis rests on the relationship between the error $e_k = u - u_k$ and the residual $r_k = b - A u_k$. For an SPD matrix $A$, these are linked via $r_k = A e_k$. The problem states that multigrid provides a mesh-independent contraction factor $\\rho \\in (0,1)$ in the $A$-energy norm, defined as $\\|e\\|_A = \\sqrt{e^T A e}$. That is, $\\|e_{k+1}\\|_A \\leq \\rho \\|e_k\\|_A$.\n\nThe energy norm of the error is related to a weighted norm of the residual:\n$$ \\|e_k\\|_A^2 = e_k^T A e_k = (A^{-1} r_k)^T A (A^{-1} r_k) = r_k^T A^{-1} r_k = \\|r_k\\|_{A^{-1}}^2 $$\nWhile convergence is guaranteed in this norm, it is not directly computable as it requires $A^{-1}$. We must use a computable norm, typically the Euclidean norm $\\| \\cdot \\|_2$. The norms are related by the eigenvalues of $A$:\n$$ \\frac{1}{\\sqrt{\\lambda_{\\max}(A)}} \\|r_k\\|_2 \\leq \\|e_k\\|_A \\leq \\frac{1}{\\sqrt{\\lambda_{\\min}(A)}} \\|r_k\\|_2 $$\nwhere $\\lambda_{\\min}(A)$ and $\\lambda_{\\max}(A)$ are the minimum and maximum eigenvalues of $A$. Since these eigenvalues depend on the mesh size $h$ (for a 2D Poisson problem, $\\lambda_{\\max} \\propto h^{-2}$ while $\\lambda_{\\min}$ is constant), a criterion based on $\\|r_k\\|_2$ might have mesh-dependent behavior. However, the *relative* residual, $\\|r_k\\|_2 / \\|r_0\\|_2$, is a standard and practical choice. It is insensitive to scaling of the equation: if we solve for $\\alpha u$ using right-hand side $\\alpha b$, the new residual is $\\alpha r_k$, and the ratio remains unchanged.\n\nWe will now evaluate each proposed criterion against the requirements.\n\n**A. Stop at the first $k$ satisfying $\\|r_k\\|_2 \\leq \\max\\{\\tau_{\\mathrm{rel}} \\|r_0\\|_2,\\ \\tau_{\\mathrm{abs}}\\}$ with user-chosen tolerances $\\tau_{\\mathrm{rel}} \\in (0,1)$ and $\\tau_{\\mathrm{abs}} > 0$, and also enforce a minimum cycle count $k \\geq k_{\\min}$ and a maximum $k \\leq k_{\\max}$. Additionally, if the average residual reduction factor over the last $s$ iterations, $\\phi_k = (\\|r_k\\|_2 / \\|r_{k-s}\\|_2)^{1/s}$, exceeds a user threshold $\\eta \\in (0,1)$ (indicating stagnation), terminate with a flagged failure.**\n\n*   **Analysis**: This is a composite criterion incorporating multiple safeguards, which is characteristic of robust software.\n    1.  The core is a relative residual check, $\\|r_k\\|_2 / \\|r_0\\|_2 \\leq \\tau_{\\mathrm{rel}}$, which is insensitive to scaling of the right-hand side $b$. This addresses the first requirement.\n    2.  The inclusion of an absolute tolerance, $\\tau_{\\mathrm{abs}}$, makes the criterion safe when the initial residual $\\|r_0\\|_2$ is already very small (e.g., due to a good initial guess). Without $\\tau_{\\mathrm{abs}}$, the solver might stop prematurely before achieving a meaningful level of accuracy. This directly addresses the safety requirement concerning small $\\|r_0\\|$.\n    3.  The check for stagnation, $\\phi_k > \\eta$, explicitly detects when the solver ceases to make effective progress. This prevents wasting computation on a diverging or stalled iteration and correctly flags a failure, which is a critical feature for a robust implementation. The averaging over $s$ iterations avoids reacting to transient, single-iteration slowdowns.\n    4.  The minimum iteration count $k_{\\min}$ prevents termination based on initial transient behavior. The maximum iteration count $k_{\\max}$ guarantees termination.\n    5.  All quantities used are computable at runtime.\n\nThis criterion systematically addresses all the stated requirements for robustness. The slight violation of perfect scaling insensitivity by $\\tau_{\\mathrm{abs}}$ is a deliberate and necessary design choice to ensure safety, a higher-order concern for robustness.\n*   **Verdict**: **Correct**.\n\n**B. Stop at the first $k$ satisfying $\\|r_k\\|_2 \\leq \\tau_{\\mathrm{abs}}$ for a fixed $\\tau_{\\mathrm{abs}} > 0$, because an absolute residual threshold directly reflects accuracy.**\n\n*   **Analysis**: This criterion is fundamentally flawed.\n    1.  It is not insensitive to scaling. If the equation $A u = b$ is scaled by a factor $\\alpha$, the condition becomes $\\|\\alpha r_k\\|_2 \\leq \\tau_{\\mathrm{abs}}$, which is equivalent to $\\|r_k\\|_2 \\leq \\tau_{\\mathrm{abs}} / |\\alpha|$. The behavior depends entirely on the scaling of the problem.\n    2.  It is unsafe. If the initial guess is good such that $\\|r_0\\|_2 \\leq \\tau_{\\mathrm{abs}}$, the solver stops at iteration $k=0$ without performing any work.\n    3.  The justification \"directly reflects accuracy\" is false. The relationship between the residual norm $\\|r_k\\|_2$ and the error norm $\\|e_k\\|_A$ is mediated by mesh-dependent eigenvalues. A fixed absolute residual tolerance does not correspond to a fixed level of accuracy across different meshes.\n*   **Verdict**: **Incorrect**.\n\n**C. Stop as soon as a single-step decrease is observed, i.e., when $\\|r_k\\|_2 / \\|r_{k-1}\\|_2 \\leq \\tau_{\\mathrm{step}}$ for a chosen $\\tau_{\\mathrm{step}} \\in (0,1)$, because multigrid reductions are typically geometric.**\n\n*   **Analysis**: This criterion is myopic and unreliable. It measures the instantaneous rate of convergence, not the total reduction achieved.\n    1.  It is possible to satisfy this condition long before the residual has been meaningfully reduced. For example, if $\\|r_0\\|_2 = 10^6$ and after one iteration $\\|r_1\\|_2 = 10^5$, the ratio is $0.1$. If $\\tau_{\\mathrm{step}} = 0.5$, the solver would terminate, even though the residual is still very large relative to any reasonable measure of accuracy.\n    2.  It fails to provide any guarantee on the final error or residual relative to their initial values. It only guarantees that at least one step was \"good\". This is not a robust measure of convergence.\n*   **Verdict**: **Incorrect**.\n\n**D. Stop when the relative change in the iterate is small, i.e., when $\\|u_{k+1} - u_k\\|_2 / \\|u_k\\|_2 \\leq \\tau_{\\mathrm{rel}}$, because small iterate updates imply proximity to the solution.**\n\n*   **Analysis**: This criterion is notoriously unreliable.\n    1.  The denominator $\\|u_k\\|_2$ can be zero or close to zero, causing numerical overflow or instability. This is especially problematic if the true solution $u$ is the zero vector.\n    2.  The premise is false. A small update $\\|u_{k+1} - u_k\\|_2$ has two possible interpretations: either the residual $r_k$ is small (indicating proximity to the solution), or the iterative method has stalled (stagnation). This criterion cannot distinguish between convergence and stagnation. A stalled solver will produce small updates, leading to a false declaration of convergence. A residual-based criterion does not suffer from this ambiguity, as a large residual unambiguously signals a lack of convergence.\n*   **Verdict**: **Incorrect**.\n\n**E. Stop when the residual on the current coarse grid level satisfies $\\|r_k^{(c)}\\|_2 / \\|r_0^{(c)}\\|_2 \\leq \\tau_{\\mathrm{rel}}$, because the coarse grid captures the smooth error that dominates late in the solve.**\n\n*   **Analysis**: This criterion is incomplete. A multigrid method works by addressing different frequency components of the error on different grid levels.\n    1.  The smoother reduces high-frequency error on the fine grid.\n    2.  The coarse-grid correction reduces low-frequency (smooth) error.\n    A full measure of convergence must account for all error components. The fine-grid residual $r_k$ represents the total error. The coarse-grid residual $r_k^{(c)}$ only reflects the state of the smooth error components. It is entirely possible for the smooth error to be small (small $r_k^{(c)}$) while the oscillatory, high-frequency error remains large (due to an ineffective smoother), resulting in a large total residual $r_k$. Terminating based only on the coarse-grid state is unsafe as it ignores a potentially large part of the error.\n*   **Verdict**: **Incorrect**.\n\nIn conclusion, only Option A presents a comprehensive and multi-faceted strategy that addresses the practical challenges of terminating an iterative solver robustly. It combines the strengths of relative and absolute criteria while adding explicit checks for stagnation and other pathological behaviors. It represents the standard of good practice in computational science.", "answer": "$$\\boxed{A}$$", "id": "2415645"}]}