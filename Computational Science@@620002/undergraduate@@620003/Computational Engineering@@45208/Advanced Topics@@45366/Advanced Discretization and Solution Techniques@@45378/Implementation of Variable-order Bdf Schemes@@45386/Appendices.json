{"hands_on_practices": [{"introduction": "To truly master a numerical method, there is no substitute for building it from the ground up. This exercise guides you through the complete construction of a variable-order BDF solver, starting from the foundational principle of polynomial interpolation [@problem_id:2401859]. You will implement the core time-stepping loop, develop an adaptive order selection strategy, and analyze the subtle but important distinction between truncation error and rounding error introduced by finite-precision arithmetic.", "problem": "Implement a variable-order Backward Differentiation Formula (BDF) time integrator for the scalar linear initial value problem $y'(t)=\\lambda\\,y(t)+s(t)$ over a uniform grid with step size $h$. The numerical method must be based on the definition of backward finite differences and polynomial interpolation and must respect the following constraints.\n\n- Accuracy/stability base: Start from the core definition of an initial value problem and the concept of approximating $y(t)$ by a polynomial that interpolates $k$ previous points on a uniform grid. Use the definition of the derivative and polynomial exactness to obtain an implicit linear multistep rule of order $k$ (the BDF method of order $k$). Do not start from any pre-given BDF formula in your derivation.\n- Variable order: At each step $t_{n+1}=t_n+h$, select an order $k\\in\\{1,2,3,4,5\\}$ using only data from the current uniform history $\\{y_n,y_{n-1},\\dots\\}$, as follows. For each admissible $k$ (limited by history length), compute the $k$-order BDF prediction $y_{n+1}^{(k)}$ by solving the single implicit linear equation that arises for $y_{n+1}$. For $k\\ge 2$, define a local truncation error proxy $e_k=\\lvert y_{n+1}^{(k)}-y_{n+1}^{(k-1)}\\rvert$. Choose the order that minimizes $e_k$ among the admissible $k$, but constrain the chosen order to change by at most $\\pm 1$ relative to the order used on the previous step. When only $k=1$ is admissible, use $k=1$.\n- Linear solve per step: Because $f(t,y)=\\lambda\\,y+s(t)$ is linear in $y$, the implicit BDF step at order $k$ must reduce to a single linear solve for $y_{n+1}$ with a scalar denominator. No nonlinear iterations are allowed or needed.\n- Rational coefficients: Store every BDF coefficient (for each order $k\\in\\{1,2,3,4,5\\}$) as an exact rational number. Your program must perform two distinct runs:\n  1. A “rational-coefficient” run that uses these exact rationals converted to standard floating-point at use time (no pre-rounding of coefficients to a finite number of decimal digits).\n  2. A “float-coefficient” run that first rounds each rational coefficient to a single-precision floating-point number (convert each rational to a $32$-bit floating-point number) and then performs the computation using those rounded coefficients (they may be upcast to standard precision during arithmetic, but their values must be those after single-precision rounding).\n- Error decomposition targets at the final time $T$: Define\n  - The rounding error proxy at step size $h$ as $E_{\\mathrm{round}}(h)=\\lvert y^{\\mathrm{float-coef}}(T;h)-y^{\\mathrm{rational-coef}}(T;h)\\rvert$.\n  - The truncation error proxy at step size $h$ as $E_{\\mathrm{trunc}}(h)=\\lvert y^{\\mathrm{rational-coef}}(T;h)-y^{\\mathrm{rational-coef}}(T;h/2)\\rvert$.\n- Angle units: When a cosine source $s(t)=\\cos(t)$ appears, the angle must be interpreted in radians.\n\nImplement the solver and compute both $E_{\\mathrm{round}}(h)$ and $E_{\\mathrm{trunc}}(h)$ for each test case in the suite below. Assume $t_0=0$ and uniform step sizes. The final time $T$ is exactly divisible by $h$ and by $h/2$ in all cases.\n\nTest Suite:\n- Case A (happy path, moderately stiff decay): $\\lambda=-1$, $s(t)\\equiv 0$, $y(0)=1$, $T=10$, $h=0.1$.\n- Case B (stiffer decay): $\\lambda=-50$, $s(t)\\equiv 0$, $y(0)=1$, $T=1$, $h=0.02$.\n- Case C (forced decay): $\\lambda=-5$, $s(t)=\\cos(t)$ with $t$ in radians, $y(0)=0$, $T=10$, $h=0.1$.\n\nProgram requirements:\n- Implement a variable-order BDF with maximum order $5$ using the procedure above.\n- Store all BDF coefficients as exact rationals and generate both coefficient sets (rational-at-use and single-precision-rounded) from those rationals.\n- For each case, compute the pair $(E_{\\mathrm{round}}(h), E_{\\mathrm{trunc}}(h))$ defined above.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$[E_{\\mathrm{round}}^{(A)},E_{\\mathrm{trunc}}^{(A)},E_{\\mathrm{round}}^{(B)},E_{\\mathrm{trunc}}^{(B)},E_{\\mathrm{round}}^{(C)},E_{\\mathrm{trunc}}^{(C)}]$,\nwhere each entry is a floating-point number. For example, a syntactically correct output looks like $[0.0,1.0,2.5,3.75,4.0,6.25]$.", "solution": "The problem requires the implementation of a variable-order Backward Differentiation Formula (BDF) solver for the scalar linear ordinary differential equation (ODE) $y'(t) = \\lambda y(t) + s(t)$. This is a well-posed problem in computational science. First, we must derive the BDF method of order $k$ from fundamental principles, as stipulated.\n\n**1. Derivation of BDF-k Formulas**\n\nA BDF method of order $k$ approximates the derivative $y'(t_{n+1})$ by differentiating a polynomial $P(t)$ of degree $k$ that interpolates the $k+1$ most recent solution points: $(t_{n+1}, y_{n+1}), (t_n, y_n), \\dots, (t_{n+1-k}, y_{n+1-k})$. For a uniform step size $h$, we have $t_j = jh$. The approximation is then $y'(t_{n+1}) \\approx P'(t_{n+1})$.\n\nA systematic way to derive the coefficients is to use backward differences. The formula for the derivative of a function $g$ at a point $x$ in terms of its backward differences at that point is given by:\n$$g'(x) = \\frac{1}{h} \\sum_{j=1}^{\\infty} \\frac{\\nabla^j g(x)}{j}$$\nwhere $\\nabla$ is the backward difference operator, $\\nabla g(x) = g(x) - g(x-h)$. Truncating this series at $j=k$ gives a $k$-th order approximation. Applying this to our sequence of numerical solutions $\\{y_i\\}$, we approximate $y'(t_{n+1})$ as:\n$$y'(t_{n+1}) \\approx \\frac{1}{h} \\sum_{j=1}^{k} \\frac{1}{j} \\nabla^j y_{n+1}$$\nHere, $\\nabla^j y_{n+1}$ is the $j$-th backward difference using the points $y_{n+1}, y_n, \\dots$. Substituting this into the ODE $y'(t_{n+1})=\\lambda y_{n+1}+s(t_{n+1})$, we obtain the BDF-k formula:\n$$\\frac{1}{h} \\sum_{j=1}^{k} \\frac{1}{j} \\nabla^j y_{n+1} = \\lambda y_{n+1} + s(t_{n+1})$$\nLet us expand this for $k=1, \\dots, 5$. The backward difference operator is defined as $\\nabla y_i = y_i - y_{i-1}$, and $\\nabla^j y_i = \\nabla(\\nabla^{j-1} y_i)$.\n\nFor $k=1$:\n$$ \\frac{1}{h} \\nabla y_{n+1} = \\frac{1}{h}(y_{n+1} - y_n) = \\lambda y_{n+1} + s_{n+1} $$\n$$ \\implies y_{n+1} - y_n = h(\\lambda y_{n+1} + s_{n+1}) $$\n\nFor $k=2$:\n$$ \\frac{1}{h} \\left(\\nabla y_{n+1} + \\frac{1}{2}\\nabla^2 y_{n+1}\\right) = \\lambda y_{n+1} + s_{n+1} $$\n$$ \\nabla^2 y_{n+1} = (y_{n+1} - y_n) - (y_n - y_{n-1}) = y_{n+1} - 2y_n + y_{n-1} $$\n$$ \\implies (y_{n+1} - y_n) + \\frac{1}{2}(y_{n+1} - 2y_n + y_{n-1}) = h(\\lambda y_{n+1} + s_{n+1}) $$\n$$ \\implies \\frac{3}{2}y_{n+1} - 2y_n + \\frac{1}{2}y_{n-1} = h(\\lambda y_{n+1} + s_{n+1}) $$\n\nContinuing this process, we can express the BDF-k method in the general form:\n$$\\sum_{i=0}^{k} c_{k,i} y_{n+1-i} = h(\\lambda y_{n+1} + s_{n+1})$$\nThe coefficients $c_{k,i}$ are rational numbers. They are derived as follows:\n- For $k=1$: $\\{c_{1,0}, c_{1,1}\\} = \\{1, -1\\}$\n- For $k=2$: $\\{c_{2,0}, c_{2,1}, c_{2,2}\\} = \\{\\frac{3}{2}, -2, \\frac{1}{2}\\}$\n- For $k=3$: $\\{c_{3,0}, \\dots, c_{3,3}\\} = \\{\\frac{11}{6}, -3, \\frac{3}{2}, -\\frac{1}{3}\\}$\n- For $k=4$: $\\{c_{4,0}, \\dots, c_{4,4}\\} = \\{\\frac{25}{12}, -4, 3, -\\frac{4}{3}, \\frac{1}{4}\\}$\n- For $k=5$: $\\{c_{5,0}, \\dots, c_{5,5}\\} = \\{\\frac{137}{60}, -5, 5, -\\frac{10}{3}, \\frac{5}{4}, -\\frac{1}{5}\\}$\n\n**2. Solving for $y_{n+1}$**\n\nThe BDF formula is implicit. For the given linear ODE, we can solve for $y_{n+1}$ algebraically.\n$$c_{k,0} y_{n+1} + \\sum_{i=1}^{k} c_{k,i} y_{n+1-i} = h \\lambda y_{n+1} + h s_{n+1}$$\n$$(c_{k,0} - h\\lambda) y_{n+1} = h s_{n+1} - \\sum_{i=1}^{k} c_{k,i} y_{n+1-i}$$\n$$y_{n+1} = \\frac{h s_{n+1} - \\sum_{i=1}^{k} c_{k,i} y_{n-i+1}}{c_{k,0} - h\\lambda}$$\nThe summation term involves the $k$ previous values of $y$. Let the history vector be $Y_n = [y_n, y_{n-1}, \\dots]$. Then the summation is a dot product of the coefficient vector $[c_{k,1}, \\dots, c_{k,k}]$ and the history vector $[y_n, \\dots, y_{n-k+1}]$.\n\n**3. Variable-Order Selection Algorithm**\n\nThe problem specifies a particular algorithm for varying the order $k$ at each time step. Let $k_{prev}$ be the order used in the previous step. At the current step to compute $y_{n+1}$:\n\n1.  **Identify History-Admissible Orders:** Determine the set of orders $K_{hist} = \\{ k \\in \\mathbb{Z} \\mid 1 \\le k \\le \\min(n+1, 5) \\}$, where $n+1$ is the number of points in the history including the initial condition $y_0$.\n2.  **Compute Predictions:** For each order $k \\in K_{hist}$, compute a predicted value $y_{n+1}^{(k)}$ using the BDF-k formula derived above.\n3.  **Compute Error Proxies:** For each order $k \\ge 2$ that is in $K_{hist}$, calculate the error proxy $e_k = \\lvert y_{n+1}^{(k)} - y_{n+1}^{(k-1)} \\rvert$.\n4.  **Select Optimal Unconstrained Order:** If only $k=1$ is admissible, the choice must be $k=1$. Otherwise, find the order $k_{opt}$ that minimizes the error proxy $e_k$ among all $k$ for which it is defined:\n    $$k_{opt} = \\underset{k \\in K_{hist}, k \\ge 2}{\\arg\\min} \\{e_k\\}$$\n5.  **Apply Constraint:** The new order, $k_{new}$, is then determined by constraining $k_{opt}$ to be at most one step away from the previous order, $k_{prev}$.\n    $$k_{new} = \\max(k_{prev}-1, \\min(k_{opt}, k_{prev}+1))$$\n    We must ensure $k_{new} \\geq 1$. Since $k_{prev} \\ge 1$, $k_{prev}-1 \\ge 0$. Thus, a final guard is $k_{new} = \\max(1, k_{new})$.\n6.  **Update Solution:** The solution for the current step is the one corresponding to the chosen order: $y_{n+1} = y_{n+1}^{(k_{new})}$. The value of $k_{new}$ becomes $k_{prev}$ for the next step.\n\nThe initial step must use $k=1$, so we initialize $k_{prev}=1$. The history of solution values must be maintained to compute subsequent steps.\n\n**4. Error Metric Calculation**\nThe program must perform three runs for each test case to compute the required error metrics by obtaining solutions at the final time $T$:\n1.  A solution $y^{\\mathrm{rational-coef}}(T;h)$ from the `rational-coef` run with step size $h$.\n2.  A solution $y^{\\mathrm{float-coef}}(T;h)$ from the `float-coef` run with step size $h$.\n3.  A solution $y^{\\mathrm{rational-coef}}(T;h/2)$ from the `rational-coef` run with step size $h/2$.\n\nThe `float-coef` run requires rounding the exact rational BDF coefficients to single-precision ($32$-bit) floating-point numbers before use in the solver.\n\nThe errors are then computed as defined in the problem:\n- Rounding error proxy: $E_{\\mathrm{round}}(h) = \\lvert y^{\\mathrm{float-coef}}(T;h) - y^{\\mathrm{rational-coef}}(T;h) \\rvert$\n- Truncation error proxy: $E_{\\mathrm{trunc}}(h) = \\lvert y^{\\mathrm{rational-coef}}(T;h) - y^{\\mathrm{rational-coef}}(T;h/2) \\rvert$\n\nThe implementation will follow this logic precisely.", "answer": "```python\nimport numpy as np\nfrom fractions import Fraction\nimport math\n\ndef get_bdf_coeffs(coeff_type='rational'):\n    \"\"\"\n    Generates BDF coefficients for orders 1-5.\n    \n    Args:\n        coeff_type (str): 'rational' for exact fractions, 'float' for\n                          single-precision rounded floats.\n\n    Returns:\n        A dictionary mapping order k to a list of its coefficients.\n    \"\"\"\n    # Coefficients c_{k,i} for sum_{i=0 to k} c_{k,i} y_{n+1-i} = h f_{n+1}\n    base_coeffs = {\n        1: [Fraction(1), Fraction(-1)],\n        2: [Fraction(3, 2), Fraction(-2), Fraction(1, 2)],\n        3: [Fraction(11, 6), Fraction(-3), Fraction(3, 2), Fraction(-1, 3)],\n        4: [Fraction(25, 12), Fraction(-4), Fraction(3), Fraction(-4, 3), Fraction(1, 4)],\n        5: [Fraction(137, 60), Fraction(-5), Fraction(5), Fraction(-10, 3), Fraction(5, 4), Fraction(-1, 5)],\n    }\n\n    if coeff_type == 'rational':\n        return {k: [float(c) for c in v] for k, v in base_coeffs.items()}\n    elif coeff_type == 'float':\n        # Round to single-precision float (float32) and then use as standard float (float64)\n        float_coeffs = {}\n        for k, v in base_coeffs.items():\n            float_coeffs[k] = [float(np.float32(c)) for c in v]\n        return float_coeffs\n    else:\n        raise ValueError(\"Invalid coefficient type\")\n\ndef solve_ode_variable_bdf(params, h, coeff_type):\n    \"\"\"\n    Solves y'(t) = lambda*y(t) + s(t) with a variable-order BDF method.\n    \"\"\"\n    lam, s_func, y0, T = params['lambda'], params['s_func'], params['y0'], params['T']\n    \n    # Ensure T is a multiple of h\n    num_steps = round(T / h)\n    \n    bdf_coeffs = get_bdf_coeffs(coeff_type)\n    \n    # History of y values, y_hist[0] = y_n, y_hist[1] = y_{n-1}, ...\n    y_hist = [y0]\n    k_prev = 1\n    max_order = 5\n\n    for n in range(num_steps):\n        t_next = (n + 1) * h\n        \n        # 1. Identify history-admissible orders\n        k_hist_max = min(len(y_hist), max_order)\n        \n        # 2. Compute predictions for all admissible orders\n        y_preds = {}\n        for k in range(1, k_hist_max + 1):\n            coeffs = bdf_coeffs[k]\n            c_k0 = coeffs[0]\n            \n            # History term: - sum_{i=1 to k} c_{k,i} * y_{n+1-i}\n            history_sum = -sum(coeffs[i] * y_hist[i-1] for i in range(1, k + 1))\n            \n            numerator = h * s_func(t_next) + history_sum\n            denominator = c_k0 - h * lam\n            \n            y_preds[k] = numerator / denominator\n\n        # 3.  4. Select optimal order based on error proxies\n        k_opt = 1 \n        if k_hist_max > 1:\n            # Orders for which e_k can be computed\n            k_err_domain = range(2, k_hist_max + 1)\n            \n            errors = {k: abs(y_preds[k] - y_preds[k-1]) for k in k_err_domain}\n            \n            if errors:\n                # Find the order k that minimizes e_k\n                k_opt = min(errors, key=errors.get)\n\n        # 5. Apply +/- 1 constraint\n        k_new = k_opt\n        if k_new > k_prev + 1:\n            k_new = k_prev + 1\n        if k_new  k_prev - 1:\n            k_new = k_prev - 1\n        k_new = max(1, k_new)\n        \n        # 6. Update solution and history\n        y_next = y_preds[k_new]\n        y_hist.insert(0, y_next)\n        \n        # Trim history to max required length\n        if len(y_hist) > max_order:\n            y_hist.pop()\n            \n        k_prev = k_new\n        \n    return y_hist[0]\n\ndef solve():\n    test_cases = [\n        # Case A\n        {'params': {'lambda': -1.0, 's_func': lambda t: 0.0, 'y0': 1.0, 'T': 10.0}, 'h': 0.1},\n        # Case B\n        {'params': {'lambda': -50.0, 's_func': lambda t: 0.0, 'y0': 1.0, 'T': 1.0}, 'h': 0.02},\n        # Case C\n        {'params': {'lambda': -5.0, 's_func': lambda t: math.cos(t), 'y0': 0.0, 'T': 10.0}, 'h': 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        params = case['params']\n        h = case['h']\n        \n        # Run with rational coeffs at h\n        y_rat_h = solve_ode_variable_bdf(params, h, 'rational')\n        \n        # Run with float coeffs at h\n        y_float_h = solve_ode_variable_bdf(params, h, 'float')\n        \n        # Run with rational coeffs at h/2\n        y_rat_h2 = solve_ode_variable_bdf(params, h / 2.0, 'rational')\n        \n        # Compute error proxies\n        e_round = abs(y_float_h - y_rat_h)\n        e_trunc = abs(y_rat_h - y_rat_h2)\n        \n        results.extend([e_round, e_trunc])\n    \n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "2401859"}, {"introduction": "The power of BDF methods for stiff problems lies in their implicit nature, but this requires solving a complex algebraic equation at every time step. This practice zooms in on this critical challenge, using Newton's method to solve the nonlinear equation arising from a BDF discretization [@problem_id:2401862]. You will investigate how the convergence of the Newton solver is impacted by the accuracy of the finite-difference Jacobian, revealing a crucial trade-off between computational cost and robustness.", "problem": "Consider the scalar initial value problem defined for all real times by the ordinary differential equation\n$$\ny'(t) = \\cos(t) + \\lambda\\,(y(t) - \\sin(t))^3,\\quad y(0)=0,\n$$\nwhere $\\lambda$ is a real parameter and $\\sin(\\cdot)$ and $\\cos(\\cdot)$ denote the sine and cosine functions with the angle measured in radians. The exact solution is $y(t)=\\sin(t)$ for all $t$.\n\nLet a variable-order backward differentiation formula (BDF) method of order $p\\in\\{1,2,3\\}$ with constant time step $\\Delta t0$ be used to advance one step from time $t_n$ to $t_{n+1}=t_n+\\Delta t$. The BDF residual for the $(n+1)$-th step is defined as\n$$\nR(y_{n+1}) = \\sum_{j=0}^{p} \\alpha_j\\, y_{n+1-j} - \\Delta t\\; f(t_{n+1}, y_{n+1}),\n$$\nwhere $f(t,y)=\\cos(t) + \\lambda\\,(y - \\sin(t))^3$, and the BDF coefficients $\\{\\alpha_j\\}_{j=0}^{p}$ for uniform steps are:\n- for $p=1$: $\\alpha_0 = 1$, $\\alpha_1 = -1$,\n- for $p=2$: $\\alpha_0 = \\tfrac{3}{2}$, $\\alpha_1 = -2$, $\\alpha_2 = \\tfrac{1}{2}$,\n- for $p=3$: $\\alpha_0 = \\tfrac{11}{6}$, $\\alpha_1 = -3$, $\\alpha_2 = \\tfrac{3}{2}$, $\\alpha_3 = -\\tfrac{1}{3}$.\n\nAssume prior values are exact and given by $y_{n+1-j}=\\sin(t_{n+1}-j\\,\\Delta t)$ for $j=1,\\dots,p$, and take the initial guess for the nonlinear solve to be $y_{n+1}^{(0)}=y_n=\\sin(t_n)$, where $t_{n+1}=p\\,\\Delta t$ and $t_n=(p-1)\\,\\Delta t$.\n\nLet the Jacobian of the residual with respect to $y_{n+1}$ be approximated by a forward finite difference with increment $h_J0$,\n$$\nJ(y) \\approx \\frac{R(y+h_J)-R(y)}{h_J}.\n$$\nAt each test case, compute the number of iterations $k$ taken by Newton's method applied to $R(y_{n+1})=0$ with the above finite-difference Jacobian approximation and initial guess, until the stopping condition $\\lvert R(y_{n+1}^{(k)})\\rvert \\le \\varepsilon$ is reached or a maximum of $k_{\\max}$ iterations is exceeded. Use $\\varepsilon = 10^{-12}$ and $k_{\\max}=50$. Also report the absolute solution error at termination,\n$$\ne = \\lvert y_{n+1}^{(k)} - \\sin(t_{n+1})\\rvert,\n$$\nwhere $y_{n+1}^{(k)}$ is the last iterate (either at convergence or after reaching $k_{\\max}$).\n\nTest Suite:\nEvaluate the above for the following parameter sets $(p,\\Delta t,\\lambda,h_J)$:\n- Case $1$: $(p=\\;1,\\;\\Delta t=\\;0.1,\\;\\lambda=\\;10,\\;h_J=\\;10^{-6})$,\n- Case $2$: $(p=\\;2,\\;\\Delta t=\\;0.05,\\;\\lambda=\\;100,\\;h_J=\\;10^{-6})$,\n- Case $3$: $(p=\\;3,\\;\\Delta t=\\;0.01,\\;\\lambda=\\;1000,\\;h_J=\\;10^{-6})$,\n- Case $4$: $(p=\\;2,\\;\\Delta t=\\;0.05,\\;\\lambda=\\;100,\\;h_J=\\;10^{-12})$,\n- Case $5$: $(p=\\;2,\\;\\Delta t=\\;0.05,\\;\\lambda=\\;100,\\;h_J=\\;10^{-2})$,\n- Case $6$: $(p=\\;1,\\;\\Delta t=\\;0.2,\\;\\lambda=\\;0,\\;h_J=\\;10^{-6})$.\n\nFor each case, produce a result which is the list $[k,e]$, where $k$ is the integer number of Newton iterations used and $e$ is the floating-point absolute error defined above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of the six case results, each case result itself being a two-element list, with no spaces, enclosed in square brackets. For example, the format must be like\n$$\n\\big[\\,[k_1,e_1],[k_2,e_2],[k_3,e_3],[k_4,e_4],[k_5,e_5],[k_6,e_6]\\,\\big].\n$$", "solution": "The objective is to simulate a single step of a variable-order BDF method for a given nonlinear ODE and report the performance of the internal Newton solver. The procedure for each test case $(p, \\Delta t, \\lambda, h_J)$ is as follows.\n\nFirst, we establish the context for the BDF integration step. The BDF method is an implicit linear multistep method. For an ODE of the form $y'(t) = f(t, y(t))$, the BDF scheme of order $p$ requires solving the following nonlinear algebraic equation for the unknown $y_{n+1}$ at each time step $t_{n+1}$:\n$$\n\\sum_{j=0}^{p} \\alpha_j y_{n+1-j} - \\Delta t f(t_{n+1}, y_{n+1}) = 0\n$$\nTo facilitate the solution process, we define the residual function $R(y)$, where $y$ is a candidate for the solution $y_{n+1}$:\n$$\nR(y) = \\alpha_0 y + \\sum_{j=1}^{p} \\alpha_j y_{n+1-j} - \\Delta t f(t_{n+1}, y)\n$$\nThe problem specifies that the history values $y_{n+1-j}$ for $j \\in \\{1, \\dots, p\\}$ are known and exact, i.e., $y_{n+1-j} = \\sin(t_{n+1-j})$. The times are defined by $t_{n+1} = p \\Delta t$ and $t_{n+1-j} = t_{n+1} - j \\Delta t = (p-j)\\Delta t$. The historical part of the sum can be pre-computed as a constant for the current step:\n$$\nH_n = \\sum_{j=1}^{p} \\alpha_j \\sin((p-j)\\Delta t)\n$$\nThe residual function can then be written more compactly as:\n$$\nR(y) = \\alpha_0 y + H_n - \\Delta t \\left( \\cos(t_{n+1}) + \\lambda(y - \\sin(t_{n+1}))^3 \\right)\n$$\nWe must find the root of $R(y)=0$ using Newton's method. The iterative scheme for Newton's method is given by:\n$$\ny_{n+1}^{(k+1)} = y_{n+1}^{(k)} - \\left[ J\\left(y_{n+1}^{(k)}\\right) \\right]^{-1} R\\left(y_{n+1}^{(k)}\\right)\n$$\nwhere $y_{n+1}^{(k)}$ is the $k$-th iterate. The Jacobian $J(y)$ is the derivative of the residual with respect to $y$, $J(y) = \\frac{dR}{dy}$. The problem mandates the use of a forward finite-difference approximation for this Jacobian:\n$$\nJ(y) \\approx \\frac{R(y+h_J) - R(y)}{h_J}\n$$\nThe iterative process begins with the initial guess $y_{n+1}^{(0)} = y_n = \\sin(t_n) = \\sin((p-1)\\Delta t)$. Iterations proceed until the stopping condition $|R(y_{n+1}^{(k)})| \\le \\varepsilon = 10^{-12}$ is satisfied, or the number of iterations $k$ exceeds the maximum $k_{\\max}=50$.\n\nThe algorithm for a single test case is as follows:\n$1$. Identify the parameters $(p, \\Delta t, \\lambda, h_J)$ and constants $\\varepsilon=10^{-12}$, $k_{\\max}=50$.\n$2$. Select the BDF coefficients $\\{\\alpha_j\\}$ corresponding to the order $p$.\n$3$. Set the time for the current step: $t_{n+1} = p \\Delta t$.\n$4$. Compute the constant history term $H_n = \\sum_{j=1}^{p} \\alpha_j \\sin((p-j)\\Delta t)$.\n$5$. Define the residual function $R(y) = \\alpha_0 y + H_n - \\Delta t (\\cos(t_{n+1}) + \\lambda(y - \\sin(t_{n+1}))^3)$.\n$6$. Initialize the Newton iteration: $k=0$ and $y_{cur} = \\sin((p-1)\\Delta t)$.\n$7$. Begin the iteration loop: Initialize $k=0$, $y_k = y_{n+1}^{(0)}$.\n   WHILE $k  k_{\\max}$:\n     a. Calculate $R(y_k)$.\n     b. IF $|R(y_k)| \\le \\varepsilon$, break.\n     c. Calculate the approximate Jacobian $J(y_k)$.\n     d. $y_{k+1} = y_k - R(y_k)/J(y_k)$.\n     e. $k \\leftarrow k+1$.\n$8$. Upon termination, the final iteration count is $k$ and the final approximation is $y_{n+1}^{(k)}$.\n$9$. Compute the absolute error: $e = |y_{n+1}^{(k)} - \\sin(t_{n+1})|$.\n$10$. The result for the test case is the pair $[k, e]$.\n\nThis complete procedure must be executed for each of the six specified test cases. The implementation will use floating-point arithmetic with standard double precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given problem by running the specified test suite and printing\n    the results in the required format.\n    \"\"\"\n\n    test_cases = [\n        # (p, Δt, λ, h_J)\n        (1, 0.1, 10, 1e-6),\n        (2, 0.05, 100, 1e-6),\n        (3, 0.01, 1000, 1e-6),\n        (2, 0.05, 100, 1e-12),\n        (2, 0.05, 100, 1e-2),\n        (1, 0.2, 0, 1e-6),\n    ]\n\n    results = []\n    for case in test_cases:\n        k, e = run_newton_bdf_step(*case)\n        results.append(f\"[{k},{e}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef run_newton_bdf_step(p, dt, lam, h_j):\n    \"\"\"\n    Performs one BDF step using Newton's method for a given set of parameters.\n\n    Args:\n        p (int): Order of the BDF method.\n        dt (float): Time step size Δt.\n        lam (float): Parameter λ in the ODE.\n        h_j (float): Increment for finite-difference Jacobian.\n\n    Returns:\n        tuple: A tuple containing:\n            - k (int): Number of Newton iterations.\n            - error (float): Absolute error at termination.\n    \"\"\"\n    epsilon = 1e-12\n    k_max = 50\n\n    bdf_coeffs = {\n        1: {'alpha': [1.0, -1.0]},\n        2: {'alpha': [3.0/2.0, -2.0, 1.0/2.0]},\n        3: {'alpha': [11.0/6.0, -3.0, 3.0/2.0, -1.0/3.0]}\n    }\n\n    if p not in bdf_coeffs:\n        raise ValueError(f\"Invalid order p={p}. Must be 1, 2, or 3.\")\n\n    alpha = bdf_coeffs[p]['alpha']\n    alpha_0 = alpha[0]\n    \n    # Time points\n    t_next = p * dt\n    t_prev = (p - 1) * dt\n\n    # Pre-compute history term\n    history_sum = 0.0\n    for j in range(1, p + 1):\n        t_hist = (p - j) * dt\n        history_sum += alpha[j] * np.sin(t_hist)\n    \n    sin_t_next = np.sin(t_next)\n    cos_t_next = np.cos(t_next)\n\n    def f(y, t):\n        return cos_t_next + lam * (y - sin_t_next)**3\n\n    def residual(y):\n        return alpha_0 * y + history_sum - dt * f(y, t_next)\n    \n    # Newton's method\n    k = 0\n    y_k = np.sin(t_prev)  # Initial guess y_n+1^(0) = y_n\n\n    while k  k_max:\n        res_k = residual(y_k)\n\n        if np.abs(res_k) = epsilon:\n            break\n\n        # Finite difference Jacobian\n        res_h = residual(y_k + h_j)\n        jac_k = (res_h - res_k) / h_j\n        \n        # Avoid division by zero if Jacobian is pathologically small\n        if np.abs(jac_k)  1e-14:\n            # This indicates a problem, e.g., stagnation. We stop.\n            k = k_max\n            break\n\n        # Newton update\n        y_k = y_k - res_k / jac_k\n        k += 1\n    \n    # Final error calculation\n    error = np.abs(y_k - sin_t_next)\n\n    return k, error\n    \nsolve()\n```", "id": "2401862"}, {"introduction": "Production-quality ODE solvers must gracefully handle events—sudden changes or specified conditions—that interrupt the integration. This exercise tackles the advanced practical problem of restarting a high-order BDF method after such an event without reverting to a low-order scheme [@problem_id:2401895]. By using polynomial interpolation to construct a \"synthetic history,\" you will connect the theoretical underpinnings of BDF methods to an essential technique for building robust and efficient adaptive solvers.", "problem": "You are given the scalar initial value problem defined by the ordinary differential equation (ODE) $y^{\\prime}(t) = \\lambda\\, y(t)$ with the exact solution $y(t) = \\exp(\\lambda t)$. Suppose an event is detected at time $t = t_e$, and the integration must restart at $t_e$ using a backward differentiation formula (BDF) of order $k$ with a constant post-event step size $h$. To restart without losing the high order $k$, one needs a set of $k$ history values aligned with the new step size, namely $y(t_e), y(t_e - h), \\dots, y\\left(t_e - (k-1)h\\right)$, which we will call the synthetic history.\n\nFor each test case below, use the following definitions and tasks that are grounded in fundamental polynomial interpolation and the definition of the backward differentiation formula:\n\n- Let $\\{\\tau_i\\}_{i=1}^{k}$ be $k$ distinct pre-event sampling times satisfying $\\tau_i  t_e$ for all $i$. Define the set of $k+1$ interpolation nodes as $\\{(\\tau_i, y(\\tau_i))\\}_{i=1}^{k} \\cup \\{(t_e, y(t_e))\\}$. Let $p_k(t)$ be the unique polynomial of degree at most $k$ that interpolates these $k+1$ points. Define the synthetic history aligned to the new step as $y_e^{(j)} := p_k\\left(t_e - j h\\right)$ for $j \\in \\{0,1,\\dots,k-1\\}$, where $y_e^{(0)} = p_k(t_e)$.\n\n- The order-$k$ backward differentiation formula (BDF) with constant step size $h$ is defined by coefficients $\\{\\alpha_j\\}_{j=0}^{k}$ that make the discrete derivative at $t_{n+1}$ exact for all polynomials of degree at most $k$. These coefficients are uniquely determined by the moment conditions on the equispaced grid $s_j = -j$ for $j \\in \\{0,1,\\dots,k\\}$:\n  1. $\\sum_{j=0}^{k} \\alpha_j s_j^m = 0$ for all integers $m$ with $m \\in \\{0,2,3,\\dots,k\\}$,\n  2. $\\sum_{j=0}^{k} \\alpha_j s_j = 1$.\n  The BDF update at $t_{n+1} = t_e + h$ is then given by\n  $$\\sum_{j=0}^{k} \\alpha_j\\, y_{n+1-j} = h\\, y^{\\prime}(t_{n+1}),$$\n  where $y_{n+1} = y(t_e+h)$ and $y_{n+1-j}$ for $j \\in \\{1,\\dots,k\\}$ are the synthetic history values $y_e^{(j-1)}$ defined above. For the ODE $y^{\\prime} = \\lambda y$, this becomes the scalar implicit equation\n  $$\\alpha_0\\, y(t_e+h) + \\sum_{j=1}^{k} \\alpha_j\\, y_e^{(j-1)} = h\\, \\lambda\\, y(t_e+h),$$\n  which has the unique solution\n  $$y(t_e+h) = \\frac{-\\sum_{j=1}^{k} \\alpha_j\\, y_e^{(j-1)}}{\\alpha_0 - h\\, \\lambda}.$$\n\nYour program must, for each test case, construct $p_k(t)$ from the specified interpolation nodes, evaluate the synthetic history $y_e^{(j)}$, determine the order-$k$ BDF coefficients $\\{\\alpha_j\\}_{j=0}^{k}$ by solving the above linear moment conditions, compute the one-step restart value $y(t_e+h)$ using the implicit formula, and report the absolute error with respect to the exact solution $y_{\\text{exact}}(t_e+h) = \\exp(\\lambda (t_e+h))$. The result for each test case is the floating-point absolute error $|y(t_e+h) - y_{\\text{exact}}(t_e+h)|$.\n\nTest Suite:\n- Case A (general happy path): $\\lambda = -2.0$, $k = 3$, $h = 0.05$, $t_e = 0.9$, pre-event sampling times $\\tau = [0.2, 0.5, 0.7]$.\n- Case B (boundary order): $\\lambda = 1.1$, $k = 1$, $h = 0.03$, $t_e = 0.2$, pre-event sampling times $\\tau = [0.0]$.\n- Case C (higher order): $\\lambda = -4.0$, $k = 5$, $h = 0.02$, $t_e = 0.9$, pre-event sampling times $\\tau = [0.0, 0.12, 0.25, 0.44, 0.68]$.\n- Case D (short step, clustered data): $\\lambda = 0.5$, $k = 2$, $h = 0.005$, $t_e = 0.92$, pre-event sampling times $\\tau = [0.88, 0.9]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above. For example, the output must be of the form $[r_A,r_B,r_C,r_D]$, where each $r_{\\cdot}$ is the absolute error for the corresponding case expressed as a floating-point number.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of numerical analysis for ordinary differential equations, is well-posed, and all terms are defined objectively and without contradiction. I will now provide the rigorous solution.\n\nThe task is to compute the one-step error of a backward differentiation formula (BDF) integrator when it is restarted at a specific time $t_e$. This restart requires the construction of a \"synthetic history\"—a set of solution values at equispaced points in the past—which did not previously exist because the pre-event steps were not aligned to the new, post-event step size $h$. The problem correctly formalizes this procedure using polynomial interpolation.\n\nThe solution is a direct, multi-step computation.\n\nFirst, we must construct the synthetic history. For a BDF method of order $k$, we require $k$ history points. The method must be exact for polynomials of degree up to $k$. Therefore, the most natural choice for a history-generating function is a polynomial of degree at most $k$, which we will denote $p_k(t)$. A unique polynomial of degree at most $k$ is determined by $k+1$ distinct points. The problem provides these points: the $k$ pre-event data points $\\{(\\tau_i, y(\\tau_i))\\}_{i=1}^{k}$ and the value at the event time itself, $(t_e, y(t_e))$. The function $y(t)$ is given as the exact solution to the test equation $y'(t) = \\lambda y(t)$, which is $y(t) = \\exp(\\lambda t)$. Thus, we have the $k+1$ interpolation nodes $\\{(\\tau, \\exp(\\lambda \\tau))\\}_{\\tau \\in \\{\\tau_1, \\dots, \\tau_k, t_e\\}}$. We find the unique polynomial $p_k(t)$ that passes through these nodes. Computationally, this is achieved by solving a linear system for the polynomial's coefficients. Once $p_k(t)$ is determined, we can generate the synthetic history values, $y_e^{(j)}$, by evaluating this polynomial at the required equispaced grid points:\n$$y_e^{(j)} = p_k(t_e - j h) \\quad \\text{for } j \\in \\{0, 1, \\dots, k-1\\}$$\n\nSecond, we must determine the coefficients $\\{\\alpha_j\\}_{j=0}^{k}$ of the order-$k$ BDF. The problem provides the defining moment conditions. These conditions state that the discrete differentiation operator must be exact for all monomials $t^m$ up to degree $k$, which leads to a system of $k+1$ linear equations for the $k+1$ unknown coefficients. The system is specified as:\n$$\n\\sum_{j=0}^{k} \\alpha_j s_j^m =\n\\begin{cases}\n1  \\text{if } m=1 \\\\\n0  \\text{if } m \\in \\{0, 2, 3, \\dots, k\\}\n\\end{cases}\n$$\nwhere $s_j = -j$. This can be written in matrix form as $M \\mathbf{\\alpha} = \\mathbf{b}$, where $M$ is a Vandermonde matrix with entries $M_{m,j} = (-j)^m$ for rows $m \\in \\{0, \\dots, k\\}$ and columns $j \\in \\{0, \\dots, k\\}$, the unknown vector is $\\mathbf{\\alpha} = [\\alpha_0, \\dots, \\alpha_k]^T$, and the right-hand side vector is $\\mathbf{b} = [0, 1, 0, \\dots, 0]^T$. Solving this non-singular system yields the unique set of BDF coefficients for the given order $k$.\n\nThird, we compute the numerical solution at the first post-event time step, $t_{e}+h$. The BDF formula applied to the differential equation $y' = \\lambda y$ at time $t_e+h$ is:\n$$\\alpha_0 y(t_e+h) + \\sum_{j=1}^{k} \\alpha_j y_e^{(j-1)} = h \\lambda y(t_e+h)$$\nHere, the values $y_e^{(j-1)}$ are the synthetic history points calculated in the first step. Note that for $j \\in \\{1, \\dots, k\\}$, the index $j-1$ ranges from $0$ to $k-1$, matching our set of synthetic values. This is an implicit equation for the unknown $y(t_e+h)$, which is easily solved algebraically:\n$$y(t_e+h) = \\frac{-\\sum_{j=1}^{k} \\alpha_j y_e^{(j-1)}}{\\alpha_0 - h \\lambda}$$\nThe numerator is a weighted sum of the synthetic history, and the denominator is the stability function of the method evaluated at $h\\lambda$.\n\nFinally, we quantify the accuracy of the procedure by computing the absolute error. We compare the numerically computed value $y(t_e+h)$ against the exact analytical solution at the same point, $y_{\\text{exact}}(t_e+h) = \\exp(\\lambda(t_e+h))$. The error is therefore:\n$$\\text{Error} = |y(t_e+h) - y_{\\text{exact}}(t_e+h)|$$\n\nThis complete sequence of operations is performed for each test case specified in the problem statement. The implementation will use robust numerical library functions for polynomial fitting and solving the linear system for the BDF coefficients.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the BDF restart problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (general happy path)\n        {'lambda_val': -2.0, 'k': 3, 'h': 0.05, 't_e': 0.9, 'tau': [0.2, 0.5, 0.7]},\n        # Case B (boundary order)\n        {'lambda_val': 1.1, 'k': 1, 'h': 0.03, 't_e': 0.2, 'tau': [0.0]},\n        # Case C (higher order)\n        {'lambda_val': -4.0, 'k': 5, 'h': 0.02, 't_e': 0.9, 'tau': [0.0, 0.12, 0.25, 0.44, 0.68]},\n        # Case D (short step, clustered data)\n        {'lambda_val': 0.5, 'k': 2, 'h': 0.005, 't_e': 0.92, 'tau': [0.88, 0.9]},\n    ]\n\n    results = []\n    for case in test_cases:\n        error = calculate_restart_error(**case)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_restart_error(lambda_val, k, h, t_e, tau):\n    \"\"\"\n    Calculates the absolute error for a single BDF restart test case.\n\n    Args:\n        lambda_val (float): The lambda parameter of the ODE y' = lambda*y.\n        k (int): The order of the BDF method.\n        h (float): The post-event step size.\n        t_e (float): The time of the event and restart.\n        tau (list[float]): A list of k pre-event sampling times.\n\n    Returns:\n        float: The absolute error |y_computed(t_e+h) - y_exact(t_e+h)|.\n    \"\"\"\n    # Step 1: Construct the interpolating polynomial p_k(t)\n    # The interpolation nodes are the k pre-event points plus the event point.\n    interp_times = np.array(tau + [t_e])\n    \n    # The corresponding y-values are from the exact solution y(t) = exp(lambda*t)\n    interp_values = np.exp(lambda_val * interp_times)\n    \n    # Find the coefficients of the unique polynomial of degree at most k.\n    # With k+1 points, polyfit of degree k gives the exact interpolating polynomial.\n    poly_coeffs = np.polyfit(interp_times, interp_values, k)\n\n    # Step 2: Evaluate p_k(t) to get the synthetic history\n    # The synthetic history points are at t_e, t_e - h, ..., t_e - (k-1)h\n    synth_eval_times = t_e - np.arange(k) * h\n    synthetic_history = np.polyval(poly_coeffs, synth_eval_times) # y_e^(0), ..., y_e^(k-1)\n\n    # Step 3: Determine the BDF coefficients {alpha_j}\n    # This requires solving a (k+1)x(k+1) linear system M*alpha = b\n    # M_mj = (-j)^m for m, j in {0, ..., k}\n    m_powers = np.arange(k + 1).reshape(-1, 1)\n    j_bases = -np.arange(k + 1)\n    M = np.power(j_bases, m_powers)\n    \n    # The RHS vector b is [0, 1, 0, ..., 0]^T\n    b = np.zeros(k + 1)\n    b[1] = 1.0\n    \n    alpha_coeffs = np.linalg.solve(M, b) # [alpha_0, alpha_1, ..., alpha_k]\n\n    # Step 4: Compute the one-step restart value y(t_e+h)\n    # y(t_e+h) = (-sum_{j=1 to k} alpha_j * y_e^(j-1)) / (alpha_0 - h*lambda)\n    # The sum is alpha_1*y_e^0 + alpha_2*y_e^1 + ... + alpha_k*y_e^(k-1)\n    numerator_sum = np.dot(alpha_coeffs[1:], synthetic_history)\n    numerator = -numerator_sum\n    \n    denominator = alpha_coeffs[0] - h * lambda_val\n    \n    y_computed = numerator / denominator\n\n    # Step 5: Calculate the absolute error\n    y_exact = np.exp(lambda_val * (t_e + h))\n    error = np.abs(y_computed - y_exact)\n    \n    return error\n\nsolve()\n```", "id": "2401895"}]}