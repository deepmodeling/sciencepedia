{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will dissect the fundamental mechanics of the additive Schwarz method. This first exercise [@problem_id:2552509] provides a concrete scenario where you can perform one full iteration by hand, revealing how local problems are solved on overlapping subdomains and how their solutions are combined to update the global approximation. By comparing the result to a single step of the classic Jacobi iteration, you will gain a tangible feel for how the additive Schwarz method operates.", "problem": "Consider the scalar diffusion problem $-u''=f$ on the interval $[0,6]$ with homogeneous Dirichlet boundary conditions $u(0)=u(6)=0$. Discretize by continuous piecewise-linear finite elements on a uniform mesh with spacing $h=1$, so that the assembled symmetric positive definite (SPD) stiffness matrix $A\\in\\mathbb{R}^{5\\times 5}$ on the $5$ interior nodes $\\{1,2,3,4,5\\}$ is the standard tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals. Let the right-hand side be the vector $b=e_{3}\\in\\mathbb{R}^{5}$, where $e_{3}$ is the unit vector with a $1$ at node $3$ and zeros elsewhere, and start from the initial guess $u^{0}=0$.\n\nDefine an overlapping three-subdomain decomposition with subdomain index sets\n- $\\Omega_{1}=\\{1,2,3\\}$,\n- $\\Omega_{2}=\\{3,4,5\\}$,\n- $\\Omega_{3}=\\{2,3,4\\}$.\n\nFor each subdomain $\\Omega_{i}$, let $R_{i}\\in\\mathbb{R}^{3\\times 5}$ be the Boolean restriction operator that extracts the components on $\\Omega_{i}$ in the natural order, and let the local operator be $A_{i}=R_{i}AR_{i}^{\\top}\\in\\mathbb{R}^{3\\times 3}$. Consider the classical one-level additive Schwarz stationary iteration (with simple injection, no partition-of-unity weighting),\n$$\nu^{k+1}=u^{k}+\\sum_{i=1}^{3}R_{i}^{\\top}A_{i}^{-1}R_{i}\\,\\bigl(b-Au^{k}\\bigr),\n$$\nand the Jacobi iteration on the global system,\n$$\nu^{k+1}=u^{k}+D^{-1}\\bigl(b-Au^{k}\\bigr),\n$$\nwhere $D=\\operatorname{diag}(A)$ is the diagonal of $A$. Using $u^{0}=0$, perform one iteration of each method to obtain $u_{\\mathrm{AS}}^{1}$ and $u_{\\mathrm{J}}^{1}$, and compute the ratio of Euclidean norms of the corresponding residuals,\n$$\n\\frac{\\|b-Au_{\\mathrm{AS}}^{1}\\|_{2}}{\\|b-Au_{\\mathrm{J}}^{1}\\|_{2}}.\n$$\n\nProvide your answer as a single exact expression (no rounding, no decimals).", "solution": "The problem statement is a well-posed exercise in numerical linear algebra, specifically concerning iterative methods for linear systems arising from the finite element discretization of a differential equation. It is scientifically grounded, formally defined, and contains all necessary information to proceed with a solution. No inconsistencies or invalid premises are present. We shall proceed with the calculation.\n\nThe global stiffness matrix $A \\in \\mathbb{R}^{5\\times 5}$ and the right-hand side vector $b \\in \\mathbb{R}^{5}$ are given as:\n$$\nA = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix}, \\quad b = e_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe initial guess is $u^{0} = 0 \\in \\mathbb{R}^{5}$.\n\nFirst, we compute one iteration of the Jacobi method. The Jacobi iteration is defined as $u^{k+1} = u^{k} + D^{-1}(b - Au^{k})$, where $D = \\operatorname{diag}(A)$.\nFor the first iteration, with $k=0$ and $u^{0} = 0$, we have:\n$$\nu_{\\mathrm{J}}^{1} = u^{0} + D^{-1}(b - Au^{0}) = D^{-1}b\n$$\nThe diagonal of $A$ is $D = \\operatorname{diag}(2, 2, 2, 2, 2) = 2I$, where $I$ is the $5 \\times 5$ identity matrix. Thus, its inverse is $D^{-1} = \\frac{1}{2}I$.\nThe first Jacobi iterate is:\n$$\nu_{\\mathrm{J}}^{1} = \\frac{1}{2}I b = \\frac{1}{2}b = \\frac{1}{2}e_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{2} \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe corresponding residual is $r_{\\mathrm{J}}^{1} = b - Au_{\\mathrm{J}}^{1}$. We first compute $Au_{\\mathrm{J}}^{1}$:\n$$\nA u_{\\mathrm{J}}^{1} = A \\left(\\frac{1}{2}e_{3}\\right) = \\frac{1}{2} (A e_{3}) = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ -1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\\\ 1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\nThe residual is then:\n$$\nr_{\\mathrm{J}}^{1} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\\\ 1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm of the Jacobi residual is:\n$$\n\\|r_{\\mathrm{J}}^{1}\\|_{2} = \\left\\| \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{0^{2} + \\left(\\frac{1}{2}\\right)^{2} + 0^{2} + \\left(\\frac{1}{2}\\right)^{2} + 0^{2}} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{1}{2}}\n$$\n\nNext, we compute one iteration of the one-level additive Schwarz method. The iteration is $u^{k+1} = u^{k} + \\sum_{i=1}^{3} R_{i}^{\\top}A_{i}^{-1}R_{i}\\,(b - Au^{k})$.\nFor the first iteration, with $k=0$ and $u^{0}=0$, we have:\n$$\nu_{\\mathrm{AS}}^{1} = \\sum_{i=1}^{3} R_{i}^{\\top}A_{i}^{-1}R_{i}b\n$$\nWe analyze each subdomain contribution. The local matrices are $A_{i} = R_{i}AR_{i}^{\\top}$. For the subdomains $\\Omega_{1} = \\{1,2,3\\}$, $\\Omega_{2} = \\{3,4,5\\}$, and $\\Omega_{3} = \\{2,3,4\\}$, the corresponding local stiffness matrices are the principal submatrices of $A$:\n$$\nA_{1} = A_{2} = A_{3} = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}\n$$\nThe inverse of this matrix is required. The determinant is $\\det(A_{1}) = 2(2 \\cdot 2 - (-1)(-1)) - (-1)(-1 \\cdot 2 - 0) = 2(3) - 2 = 4$. The inverse is:\n$$\nA_{1}^{-1} = A_{2}^{-1} = A_{3}^{-1} = \\frac{1}{4} \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix}\n$$\nNow we compute the local solves. The right-hand side $b=e_{3}$ is restricted to each subdomain, yielding local vectors $b_{i} = R_{i}b$:\n$$\nb_{1} = R_{1}b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad b_{2} = R_{2}b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad b_{3} = R_{3}b = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe local solutions are $w_{i} = A_{i}^{-1}b_{i}$:\n$$\nw_{1} = A_{1}^{-1}b_{1} = \\frac{1}{4} \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\n$$\nw_{2} = A_{2}^{-1}b_{2} = \\frac{1}{4} \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n$$\nw_{3} = A_{3}^{-1}b_{3} = \\frac{1}{4} \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nThese local solutions are extended back to the global vector space using $R_{i}^{\\top}$ and summed to obtain the first additive Schwarz iterate:\n$$\nu_{\\mathrm{AS}}^{1} = R_{1}^{\\top}w_{1} + R_{2}^{\\top}w_{2} + R_{3}^{\\top}w_{3} = \\frac{1}{4}\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4}\\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\\\ 2 \\\\ 1 \\end{pmatrix} + \\frac{1}{4}\\begin{pmatrix} 0 \\\\ 2 \\\\ 4 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\frac{1}{4}\\begin{pmatrix} 1 \\\\ 4 \\\\ 10 \\\\ 4 \\\\ 1 \\end{pmatrix}\n$$\nThe corresponding residual is $r_{\\mathrm{AS}}^{1} = b - Au_{\\mathrm{AS}}^{1}$. We first compute $Au_{\\mathrm{AS}}^{1}$:\n$$\nAu_{\\mathrm{AS}}^{1} = \\frac{1}{4} \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 4 \\\\ 10 \\\\ 4 \\\\ 1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2-4 \\\\ -1+8-10 \\\\ -4+20-4 \\\\ -10+8-1 \\\\ -4+2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} -2 \\\\ -3 \\\\ 12 \\\\ -3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ -3/4 \\\\ 3 \\\\ -3/4 \\\\ -1/2 \\end{pmatrix}\n$$\nThe residual is then:\n$$\nr_{\\mathrm{AS}}^{1} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -1/2 \\\\ -3/4 \\\\ 3 \\\\ -3/4 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 3/4 \\\\ -2 \\\\ 3/4 \\\\ 1/2 \\end{pmatrix}\n$$\nThe Euclidean norm of the additive Schwarz residual is:\n$$\n\\|r_{\\mathrm{AS}}^{1}\\|_{2} = \\sqrt{\\left(\\frac{1}{2}\\right)^{2} + \\left(\\frac{3}{4}\\right)^{2} + (-2)^{2} + \\left(\\frac{3}{4}\\right)^{2} + \\left(\\frac{1}{2}\\right)^{2}} = \\sqrt{\\frac{1}{4} + \\frac{9}{16} + 4 + \\frac{9}{16} + \\frac{1}{4}}\n$$\n$$\n\\|r_{\\mathrm{AS}}^{1}\\|_{2} = \\sqrt{2\\left(\\frac{1}{4}\\right) + 2\\left(\\frac{9}{16}\\right) + 4} = \\sqrt{\\frac{1}{2} + \\frac{9}{8} + 4} = \\sqrt{\\frac{4}{8} + \\frac{9}{8} + \\frac{32}{8}} = \\sqrt{\\frac{45}{8}} = \\frac{3\\sqrt{5}}{2\\sqrt{2}} = \\frac{3\\sqrt{10}}{4}\n$$\nFinally, we compute the required ratio of the norms of the residuals:\n$$\n\\frac{\\|b-Au_{\\mathrm{AS}}^{1}\\|_{2}}{\\|b-Au_{\\mathrm{J}}^{1}\\|_{2}} = \\frac{\\|r_{\\mathrm{AS}}^{1}\\|_{2}}{\\|r_{\\mathrm{J}}^{1}\\|_{2}} = \\frac{\\frac{3\\sqrt{10}}{4}}{\\sqrt{\\frac{1}{2}}} = \\frac{3\\sqrt{10}}{4} \\cdot \\sqrt{2} = \\frac{3\\sqrt{20}}{4} = \\frac{3 \\cdot 2\\sqrt{5}}{4} = \\frac{6\\sqrt{5}}{4} = \\frac{3\\sqrt{5}}{2}\n$$", "answer": "$$\n\\boxed{\\frac{3\\sqrt{5}}{2}}\n$$", "id": "2552509"}, {"introduction": "While the additive Schwarz method is conceptually straightforward, its convergence is not always guaranteed. This next practice [@problem_id:2387017] presents a carefully constructed example to highlight the crucial differences between the additive (parallel) and multiplicative (sequential) variants of the Schwarz algorithm. By computing the spectral radii $\\rho$ of the respective iteration matrices, you will discover a scenario where one method converges beautifully while the other fails spectacularly, providing a powerful lesson on the importance of formal convergence analysis.", "problem": "Consider the linear system $A x = b$ in $\\mathbb{R}^{2}$ with $A = I$, where $I$ is the $2 \\times 2$ identity matrix and $b \\in \\mathbb{R}^{2}$ is arbitrary. Define three one-dimensional overlapping subspaces\n$V_{1} = \\operatorname{span}\\{v_{1}\\}$, $V_{2} = \\operatorname{span}\\{v_{2}\\}$, and $V_{3} = \\operatorname{span}\\{v_{3}\\}$, where\n$v_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v_{2} = \\begin{pmatrix} \\sqrt{3}/2 \\\\ 1/2 \\end{pmatrix}$, and $v_{3} = \\begin{pmatrix} 1/2 \\\\ \\sqrt{3}/2 \\end{pmatrix}$. For each $i \\in \\{1,2,3\\}$, the local solver restricted to $V_{i}$ is defined by $A_{i}^{-1}$, where $A_{i} = R_{i} A R_{i}^{\\top}$ and $R_{i}$ is the restriction to $V_{i}$.\n\nDefine the classical additive Schwarz iteration with unit relaxation as\n$x^{k+1} = x^{k} + \\sum_{i=1}^{3} R_{i}^{\\top} A_{i}^{-1} R_{i} \\left(b - A x^{k}\\right)$,\nand the classical multiplicative Schwarz iteration as the sequential application of the three local corrections:\n$x^{k+1} = \\left(I - R_{3}^{\\top} A_{3}^{-1} R_{3} A\\right)\\left(I - R_{2}^{\\top} A_{2}^{-1} R_{2} A\\right)\\left(I - R_{1}^{\\top} A_{1}^{-1} R_{1} A\\right) x^{k} + \\text{(appropriate affine term in $b$)}$.\nLet $T_{\\mathrm{add}}$ and $T_{\\mathrm{mult}}$ be the corresponding iteration matrices for the error propagation, so that the errors $e^{k} = x^{k} - x^{\\ast}$ satisfy $e^{k+1} = T_{\\mathrm{add}} e^{k}$ for the additive method and $e^{k+1} = T_{\\mathrm{mult}} e^{k}$ for the multiplicative method, where $x^{\\ast}$ is the exact solution.\n\nCompute the spectral radii $\\rho\\!\\left(T_{\\mathrm{add}}\\right)$ and $\\rho\\!\\left(T_{\\mathrm{mult}}\\right)$ for this setup, and then compute the single quantity\n$\\Delta = \\rho\\!\\left(T_{\\mathrm{add}}\\right) - \\rho\\!\\left(T_{\\mathrm{mult}}\\right)$.\nExpress your final answer as an exact value. No rounding is required.", "solution": "The problem requires the computation of the spectral radii of the iteration matrices for the classical additive and multiplicative Schwarz methods, and then the difference between these two radii.\n\nFirst, we must validate the problem statement.\nThe problem is stated as:\n- Linear system: $A x = b$ in $\\mathbb{R}^{2}$ with $A = I$, the $2 \\times 2$ identity matrix.\n- Subspaces: $V_{1} = \\operatorname{span}\\{v_{1}\\}$, $V_{2} = \\operatorname{span}\\{v_{2}\\}$, and $V_{3} = \\operatorname{span}\\{v_{3}\\}$, with $v_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v_{2} = \\begin{pmatrix} \\sqrt{3}/2 \\\\ 1/2 \\end{pmatrix}$, and $v_{3} = \\begin{pmatrix} 1/2 \\\\ \\sqrt{3}/2 \\end{pmatrix}$.\n- Local solvers: $A_{i}^{-1}$, where $A_{i} = R_{i} A R_{i}^{\\top}$ and $R_{i}$ is the restriction to $V_{i}$.\n- Additive Schwarz iteration: $x^{k+1} = x^{k} + \\sum_{i=1}^{3} R_{i}^{\\top} A_{i}^{-1} R_{i} (b - A x^{k})$.\n- Multiplicative Schwarz iteration: $x^{k+1} = (I - R_{3}^{\\top} A_{3}^{-1} R_{3} A)\\dots(I - R_{1}^{\\top} A_{1}^{-1} R_{1} A) x^{k} + \\text{affine term}$.\n- The task is to compute $\\Delta = \\rho(T_{\\mathrm{add}}) - \\rho(T_{\\mathrm{mult}})$.\n\nThe problem is scientifically grounded in the theory of iterative methods for linear systems, specifically domain decomposition. It is well-posed, objective, and provides all necessary information. The setup is a standard academic example used to analyze the convergence properties of Schwarz methods. The problem is therefore deemed valid.\n\nThe error propagation for the additive method is given by $e^{k+1} = T_{\\mathrm{add}} e^{k}$, where the iteration matrix is $T_{\\mathrm{add}} = I - \\sum_{i=1}^{3} R_{i}^{\\top} A_{i}^{-1} R_{i} A$.\nLet us define the operators $P_i = R_{i}^{\\top} A_{i}^{-1} R_{i} A$. Since $A=I$ (the $2 \\times 2$ identity matrix), these simplify to $P_i = R_{i}^{\\top} A_{i}^{-1} R_{i}$. The vectors $v_1$, $v_2$, and $v_3$ are unit vectors:\n$\\|v_1\\|^2 = 1^2 + 0^2 = 1$.\n$\\|v_2\\|^2 = (\\frac{\\sqrt{3}}{2})^2 + (\\frac{1}{2})^2 = \\frac{3}{4} + \\frac{1}{4} = 1$.\n$\\|v_3\\|^2 = (\\frac{1}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{1}{4} + \\frac{3}{4} = 1$.\n\nFor a one-dimensional subspace $V_i = \\operatorname{span}\\{v_i\\}$ with $\\|v_i\\|=1$, the restriction operator $R_i: \\mathbb{R}^2 \\to \\mathbb{R}$ is $R_i x = v_i^\\top x$. Its transpose $R_i^\\top: \\mathbb{R} \\to \\mathbb{R}^2$ is $R_i^\\top \\alpha = \\alpha v_i$. The local matrix $A_i = R_i A R_i^\\top = R_i I R_i^\\top = R_i R_i^\\top$ is a $1 \\times 1$ matrix (a scalar). Its action on a scalar $\\alpha$ is $A_i \\alpha = R_i(\\alpha v_i) = v_i^\\top (\\alpha v_i) = \\alpha (v_i^\\top v_i) = \\alpha \\|v_i\\|^2 = \\alpha$. Thus, $A_i = [1]$ and $A_i^{-1} = [1]$.\n\nThe operator $P_i = R_i^\\top A_i^{-1} R_i$ acts on a vector $x \\in \\mathbb{R}^2$ as $P_i x = R_i^\\top(1 \\cdot (v_i^\\top x)) = (v_i^\\top x) v_i$. This is the orthogonal projection onto the subspace $V_i$. The matrix representation of $P_i$ is $v_i v_i^\\top$.\n\nWe compute the matrices for $P_1$, $P_2$, and $P_3$:\n$P_1 = v_1 v_1^\\top = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n$P_2 = v_2 v_2^\\top = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{1}{4} \\end{pmatrix}$.\n$P_3 = v_3 v_3^\\top = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{3}{4} \\end{pmatrix}$.\n\nThe additive iteration matrix is $T_{\\mathrm{add}} = I - \\sum_{i=1}^{3} P_i$.\nFirst, we compute the sum $\\sum_{i=1}^{3} P_i$:\n$$ \\sum_{i=1}^{3} P_i = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{3}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{1}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{3}{4}+\\frac{1}{4} & \\frac{\\sqrt{3}}{4}+\\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4}+\\frac{\\sqrt{3}}{4} & \\frac{1}{4}+\\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} 2 & \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} & 1 \\end{pmatrix} $$\nThen, $T_{\\mathrm{add}} = I - \\sum P_i$:\n$$ T_{\\mathrm{add}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 2 & \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} & 1 \\end{pmatrix} = \\begin{pmatrix} -1 & -\\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2} & 0 \\end{pmatrix} $$\nTo find the spectral radius $\\rho(T_{\\mathrm{add}})$, we compute its eigenvalues $\\lambda$ from the characteristic equation $\\det(T_{\\mathrm{add}} - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} -1-\\lambda & -\\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2} & -\\lambda \\end{pmatrix} = (-1-\\lambda)(-\\lambda) - \\left(-\\frac{\\sqrt{3}}{2}\\right)\\left(-\\frac{\\sqrt{3}}{2}\\right) = \\lambda(1+\\lambda) - \\frac{3}{4} = 0 $$\n$$ \\lambda^2 + \\lambda - \\frac{3}{4} = 0 $$\nUsing the quadratic formula, the eigenvalues are:\n$$ \\lambda = \\frac{-1 \\pm \\sqrt{1^2 - 4(1)(-\\frac{3}{4})}}{2} = \\frac{-1 \\pm \\sqrt{1+3}}{2} = \\frac{-1 \\pm 2}{2} $$\nThe eigenvalues are $\\lambda_1 = \\frac{-1+2}{2} = \\frac{1}{2}$ and $\\lambda_2 = \\frac{-1-2}{2} = -\\frac{3}{2}$.\nThe spectral radius is the maximum of the absolute values of the eigenvalues:\n$\\rho(T_{\\mathrm{add}}) = \\max\\left(\\left|\\frac{1}{2}\\right|, \\left|-\\frac{3}{2}\\right|\\right) = \\frac{3}{2}$.\n\nNext, we consider the multiplicative iteration. The error propagation matrix is $T_{\\mathrm{mult}} = (I-P_3)(I-P_2)(I-P_1)$.\nThe matrices $(I-P_i)$ are also projectors, projecting onto the space orthogonal to $V_i$.\n$I - P_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n$I - P_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{\\sqrt{3}}{4} \\\\ -\\frac{\\sqrt{3}}{4} & \\frac{3}{4} \\end{pmatrix}$.\n$I - P_3 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} & -\\frac{\\sqrt{3}}{4} \\\\ -\\frac{\\sqrt{3}}{4} & \\frac{1}{4} \\end{pmatrix}$.\n\nNow, we compute the product $T_{\\mathrm{mult}}$:\nFirst, $(I-P_2)(I-P_1)$:\n$$ \\begin{pmatrix} \\frac{1}{4} & -\\frac{\\sqrt{3}}{4} \\\\ -\\frac{\\sqrt{3}}{4} & \\frac{3}{4} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{\\sqrt{3}}{4} \\\\ 0 & \\frac{3}{4} \\end{pmatrix} $$\nThen, $T_{\\mathrm{mult}} = (I-P_3) \\left[ (I-P_2)(I-P_1) \\right]$:\n$$ T_{\\mathrm{mult}} = \\begin{pmatrix} \\frac{3}{4} & -\\frac{\\sqrt{3}}{4} \\\\ -\\frac{\\sqrt{3}}{4} & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 0 & -\\frac{\\sqrt{3}}{4} \\\\ 0 & \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} 0 & \\frac{3}{4}\\left(-\\frac{\\sqrt{3}}{4}\\right) - \\frac{\\sqrt{3}}{4}\\left(\\frac{3}{4}\\right) \\\\ 0 & \\left(-\\frac{\\sqrt{3}}{4}\\right)\\left(-\\frac{\\sqrt{3}}{4}\\right) + \\frac{1}{4}\\left(\\frac{3}{4}\\right) \\end{pmatrix} $$\n$$ T_{\\mathrm{mult}} = \\begin{pmatrix} 0 & -\\frac{3\\sqrt{3}}{16} - \\frac{3\\sqrt{3}}{16} \\\\ 0 & \\frac{3}{16} + \\frac{3}{16} \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{6\\sqrt{3}}{16} \\\\ 0 & \\frac{6}{16} \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{3\\sqrt{3}}{8} \\\\ 0 & \\frac{3}{8} \\end{pmatrix} $$\nThe eigenvalues of an upper triangular matrix are its diagonal entries. The eigenvalues of $T_{\\mathrm{mult}}$ are $\\lambda_1 = 0$ and $\\lambda_2 = \\frac{3}{8}$.\nThe spectral radius is $\\rho(T_{\\mathrm{mult}}) = \\max\\left(|0|, \\left|\\frac{3}{8}\\right|\\right) = \\frac{3}{8}$.\n\nFinally, we compute the required quantity $\\Delta$:\n$$ \\Delta = \\rho(T_{\\mathrm{add}}) - \\rho(T_{\\mathrm{mult}}) = \\frac{3}{2} - \\frac{3}{8} = \\frac{12}{8} - \\frac{3}{8} = \\frac{9}{8} $$\nThe result is an exact value as requested.", "answer": "$$\n\\boxed{\\frac{9}{8}}\n$$", "id": "2387017"}, {"introduction": "Our final practice moves from idealized pencil-and-paper exercises to the realm of practical computation, addressing a key challenge in large-scale applications. In real scenarios, solving subdomain problems exactly can be prohibitively expensive. This coding exercise [@problem_id:2387028] allows you to explore the concept of \"inexact solves,\" where the local problems are only solved approximately. You will investigate how the amount of work spent on the inner subdomain solves impacts the convergence rate of the outer iteration, revealing a critical trade-off in the design of efficient parallel preconditioners.", "problem": "Consider the linear system that results from a standard centered finite-difference discretization of the one-dimensional Poisson equation on the unit interval with homogeneous Dirichlet boundary conditions. The continuous problem is $-u''(x) = g(x)$ for $x \\in (0,1)$ with $u(0) = 0$ and $u(1) = 0$. Discretize the interval into $N$ interior points with uniform spacing $h = 1/(N+1)$, yielding the symmetric positive definite linear system $A u = f$, where $A \\in \\mathbb{R}^{N \\times N}$ has $2/h^2$ on the diagonal and $-1/h^2$ on the first sub- and super-diagonals, and $f \\in \\mathbb{R}^{N}$ is the discretized right-hand side. Throughout this problem, set $f$ to the constant vector with all entries equal to $1$ (unitless). The goal is to solve $A u = f$ by a two-subdomain multiplicative Schwarz method with inexact subdomain solves.\n\nPartition the set of unknowns into two contiguous subdomains with possible overlap. Define the midpoint index $m = \\lfloor N/2 \\rfloor$. For a given nonnegative integer overlap parameter $o$, define the index sets\n$$\nI_1 = \\{0,1,\\dots,m+o-1\\}, \\quad I_2 = \\{m-o, m-o+1, \\dots, N-1\\},\n$$\nwith the understanding that $I_1$ and $I_2$ are intersected with $\\{0,1,\\dots,N-1\\}$ to remain within valid bounds. The two-subdomain multiplicative Schwarz iteration is defined as follows: starting from a global iterate $x^{(k)} \\in \\mathbb{R}^{N}$, perform in order\n$$\n\\text{solve on } I_1: \\quad A_{11} \\, \\delta x_1 = r_1, \\quad \\text{then update } x_{I_1}^{(k+\\frac{1}{2})} = x_{I_1}^{(k)} + \\delta x_1,\n$$\n$$\n\\text{solve on } I_2: \\quad A_{22} \\, \\delta x_2 = r_2, \\quad \\text{then update } x_{I_2}^{(k+1)} = x_{I_2}^{(k+\\frac{1}{2})} + \\delta x_2,\n$$\nwhere $A_{ii}$ are the principal submatrices of $A$ restricted to $I_i$, and $r_i$ are the restricted residuals incorporating fixed values from the complement, defined by $r_i = b_i - A_{i, I_i} x_{I_i}$ with right-hand side $b_i = f_{I_i} - A_{i, \\overline{I_i}} x_{\\overline{I_i}}$. Here $A_{i, I_i}$ denotes the block of $A$ with rows in $I_i$ and columns in $I_i$, and $A_{i, \\overline{I_i}}$ denotes the block with rows in $I_i$ and columns in the complement of $I_i$.\n\nIn this problem, the subdomain solves are not exact. Instead, they are carried out inexactly by applying a fixed number $m_{\\text{in}} \\in \\mathbb{N}_0$ of weighted Jacobi iterations to the restricted system $A_{ii} x_{I_i} = b_i$ with relaxation weight $\\omega \\in (0,1)$. One weighted Jacobi sweep on $A_{ii} x_{I_i} = b_i$ has the form\n$$\nx_{I_i}^{\\text{new}} = x_{I_i}^{\\text{old}} + \\omega D_{ii}^{-1} \\left(b_i - A_{ii} x_{I_i}^{\\text{old}}\\right),\n$$\nwhere $D_{ii}$ is the diagonal of $A_{ii}$. The initial guess for each subdomain solve is the current subvector of the global iterate. After $m_{\\text{in}}$ sweeps, the subdomain subvector is injected back into the global iterate (overwriting entries on that subdomain). One full outer iteration consists of the $I_1$ update followed by the $I_2$ update.\n\nDefine the global residual $r^{(k)} = f - A x^{(k)}$ and its Euclidean norm $\\|r^{(k)}\\|_2$. The observed contraction factor per outer iteration is quantified by the geometric mean\n$$\n\\rho_{\\text{obs}} = \\exp\\left( \\frac{1}{K} \\sum_{k=1}^{K} \\log \\frac{\\|r^{(k)}\\|_2}{\\|r^{(k-1)}\\|_2} \\right),\n$$\ncomputed over $K$ outer iterations starting from $x^{(0)} = 0$.\n\nTask: Implement a program that, for each test case below, constructs $A$ and $f$ as described, partitions the indices into two subdomains with overlap $o$, performs $K$ outer multiplicative Schwarz iterations with inexact subdomain solves using $m_{\\text{in}}$ weighted Jacobi sweeps with relaxation weight $\\omega$, and returns the observed contraction factor $\\rho_{\\text{obs}}$ rounded to six decimal places.\n\nUse only dimensionless quantities for all computations. Angles are not involved. There are no physical units to report.\n\nTest suite (each tuple is $(N, o, m_{\\text{in}}, \\omega, K)$):\n- Case $1$: $(127, 2, 0, 2/3, 20)$.\n- Case $2$: $(127, 2, 1, 2/3, 20)$.\n- Case $3$: $(127, 2, 3, 2/3, 20)$.\n- Case $4$: $(127, 0, 10, 2/3, 20)$.\n- Case $5$: $(63, 1, 2, 0.8, 20)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each contraction factor rounded to six decimal places and without any additional text, for example, \"[0.950000,0.750000,0.500000]\".", "solution": "The problem has been subjected to rigorous validation and is determined to be valid. It constitutes a well-posed and scientifically grounded problem within the domain of computational engineering and numerical linear algebra. The task is to analyze the convergence of a two-subdomain multiplicative Schwarz method with inexact subdomain solves for a specific linear system. The formulation is devoid of ambiguity, contradiction, or factual error. A complete solution will now be furnished.\n\nThe method will be implemented algorithmically, following a precise sequence of steps derived from the problem statement.\n\n1.  **System Construction**: The problem is defined on a one-dimensional domain, discretized into $N$ interior points with mesh spacing $h = 1/(N+1)$. This leads to a linear system $A u = f$. The matrix $A \\in \\mathbb{R}^{N \\times N}$ is a symmetric positive definite matrix representing the centered finite-difference approximation of the negative second derivative operator. It is a tridiagonal matrix with entries:\n    $$\n    A_{ij} =\n    \\begin{cases}\n    2/h^2 & \\text{if } i = j \\\\\n    -1/h^2 & \\text{if } |i - j| = 1 \\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n    $$\n    The right-hand side vector $f \\in \\mathbb{R}^{N}$ is given as a constant vector where every entry is $1$.\n\n2.  **Domain Decomposition**: The set of $N$ indices $\\{0, 1, \\dots, N-1\\}$ is partitioned into two overlapping subdomains, $I_1$ and $I_2$. With the midpoint index defined as $m = \\lfloor N/2 \\rfloor$ and a given non-negative integer overlap parameter $o$, the index sets are:\n    $$\n    I_1 = \\{i \\in \\mathbb{Z} \\mid 0 \\le i < m+o \\} \\cap \\{0, \\dots, N-1\\}\n    $$\n    $$\n    I_2 = \\{i \\in \\mathbb{Z} \\mid m-o \\le i < N \\} \\cap \\{0, \\dots, N-1\\}\n    $$\n    For each subdomain $I_i$, we define the corresponding principal submatrix $A_{ii}$ of $A$ by restricting $A$ to the rows and columns indexed by $I_i$.\n\n3.  **Multiplicative Schwarz Iteration with Inexact Solves**: The core of the problem is the iterative solution of $A u = f$ starting from an initial guess $x^{(0)} = 0$. One outer iteration, which computes $x^{(k+1)}$ from $x^{(k)}$, consists of two sequential multiplicative steps.\n\n    a.  **Subdomain 1 Solve**: First, an update is computed for the variables in subdomain $I_1$. The system to be solved is for a correction $\\delta x_1$, given by:\n        $$\n        A_{11} \\delta x_1 = r_1\n        $$\n        Here, $r_1$ is the global residual $r^{(k)} = f - A x^{(k)}$ restricted to the indices in $I_1$. This system is solved inexactly using $m_{\\text{in}}$ iterations of the weighted Jacobi method, starting with an initial guess $\\delta x_1 = 0$:\n        $$\n        (\\delta x_1)^{(j+1)} = (\\delta x_1)^{(j)} + \\omega D_{11}^{-1} (r_1 - A_{11} (\\delta x_1)^{(j)}) \\quad \\text{for } j=0, \\dots, m_{\\text{in}}-1\n        $$\n        where $D_{11}$ is the diagonal of $A_{11}$ and $\\omega$ is the relaxation weight. The final correction is used to update the solution vector, yielding an intermediate solution $x^{(k+1/2)}$:\n        $$\n        x_{I_1}^{(k+1/2)} = x_{I_1}^{(k)} + \\delta x_1\n        $$\n        The components of $x$ outside $I_1$ remain unchanged.\n\n    b.  **Subdomain 2 Solve**: Second, the process is repeated for subdomain $I_2$. A new correction $\\delta x_2$ is computed by inexactly solving:\n        $$\n        A_{22} \\delta x_2 = r_2\n        $$\n        where $r_2$ is the new global residual $r^{(k+1/2)} = f - A x^{(k+1/2)}$ restricted to $I_2$. Again, $m_{\\text{in}}$ weighted Jacobi iterations are used. The final solution for the outer iteration $k+1$ is then obtained by updating the intermediate solution:\n        $$\n        x_{I_2}^{(k+1)} = x_{I_2}^{(k+1/2)} + \\delta x_2\n        $$\n\n4.  **Convergence Analysis**: The effectiveness of the iterative method is measured by the observed contraction factor $\\rho_{\\text{obs}}$. After performing $K$ outer iterations, we have a sequence of residual norms $\\|r^{(k)}\\|_2$ for $k=0, \\dots, K$. The factor $\\rho_{\\text{obs}}$ is the geometric mean of the per-iteration reduction factors:\n    $$\n    \\rho_{\\text{obs}} = \\exp\\left( \\frac{1}{K} \\sum_{k=1}^{K} \\log \\frac{\\|r^{(k)}\\|_2}{\\|r^{(k-1)}\\|_2} \\right)\n    $$\n    This expression simplifies algebraically to a more numerically stable form:\n    $$\n    \\rho_{\\text{obs}} = \\left( \\frac{\\|r^{(K)}\\|_2}{\\|r^{(0)}\\|_2} \\right)^{1/K}\n    $$\n    This formula will be used for the final computation. For the special case where $m_{\\text{in}} = 0$, no Jacobi iterations are performed, the correction $\\delta x_i$ is always zero, and the solution never changes. Consequently, the residual norm remains constant, leading to $\\rho_{\\text{obs}} = 1$.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags\n\ndef solve():\n    \"\"\"\n    Computes the observed contraction factor for a two-subdomain multiplicative\n    Schwarz method with inexact solves for the 1D Poisson equation.\n    \"\"\"\n    test_cases = [\n        (127, 2, 0, 2/3, 20),  # Case 1\n        (127, 2, 1, 2/3, 20),  # Case 2\n        (127, 2, 3, 2/3, 20),  # Case 3\n        (127, 0, 10, 2/3, 20), # Case 4\n        (63, 1, 2, 0.8, 20),    # Case 5\n    ]\n\n    results = []\n    for N, o, m_in, omega, K in test_cases:\n        # Step 1: System Construction\n        h = 1.0 / (N + 1)\n        h2_inv = 1.0 / (h * h)\n        \n        main_diag = np.full(N, 2.0 * h2_inv)\n        off_diag = np.full(N - 1, -1.0 * h2_inv)\n        \n        A_sparse = diags(\n            [main_diag, off_diag, off_diag], \n            [0, -1, 1], \n            shape=(N, N), \n            format='csr'\n        )\n        A = A_sparse.toarray()\n        f = np.ones(N)\n\n        # Step 2: Domain Decomposition\n        m = N // 2\n        I1 = list(range(m + o))\n        I2 = list(range(m - o, N))\n        \n        A11 = A[np.ix_(I1, I1)]\n        A22 = A[np.ix_(I2, I2)]\n        \n        # The diagonal of A_ii is constant. D_ii_inv is a scalar.\n        D_inv_val = 1.0 / (2.0 * h2_inv)\n\n        # Initialize iteration\n        x = np.zeros(N)\n        residual_norms = []\n        \n        r_initial = f - A @ x\n        residual_norms.append(np.linalg.norm(r_initial))\n        \n        if m_in == 0:\n            # Special case: no inner iterations means no update to x.\n            # Residual norm remains constant, rho = 1.\n            rho_obs = 1.0\n            results.append(f\"{rho_obs:.6f}\")\n            continue\n\n        # Step 3: Multiplicative Schwarz Iterations\n        for _ in range(K):\n            # Subdomain 1 solve\n            r_global = f - A @ x\n            r1 = r_global[I1]\n            delta_x1 = np.zeros(len(I1))\n            for _ in range(m_in):\n                delta_x1 += omega * D_inv_val * (r1 - A11 @ delta_x1)\n            x[I1] += delta_x1\n            \n            # Subdomain 2 solve\n            r_global = f - A @ x\n            r2 = r_global[I2]\n            delta_x2 = np.zeros(len(I2))\n            for _ in range(m_in):\n                delta_x2 += omega * D_inv_val * (r2 - A22 @ delta_x2)\n            x[I2] += delta_x2\n            \n            r_final = f - A @ x\n            residual_norms.append(np.linalg.norm(r_final))\n\n        # Step 4: Convergence Analysis\n        rho_obs = (residual_norms[-1] / residual_norms[0])**(1.0 / K)\n        results.append(f\"{rho_obs:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2387028"}]}