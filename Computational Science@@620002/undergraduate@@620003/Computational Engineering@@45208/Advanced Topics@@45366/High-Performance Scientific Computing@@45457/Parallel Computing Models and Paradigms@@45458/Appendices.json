{"hands_on_practices": [{"introduction": "The first step in parallel programming is ensuring correctness. This exercise confronts one of the most common and subtle bugs in concurrent systems: the data race. By implementing a shared hash table, you will first deterministically trigger a \"lost update\" problem, making a theoretical hazard a tangible failure. You will then correct this flaw using two fundamental synchronization paradigms, mutual exclusion and atomic operations, providing a practical understanding of how to build reliable concurrent data structures. [@problem_id:2422625]", "problem": "Create a complete, runnable program that demonstrates a data race in a hash table and then eliminates it using two different synchronization paradigms. The program must implement a fixed-capacity, open-addressed hash table that stores integer keys and integer values. Let the capacity be a positive integer $m$. The hash function is $h(k) = k \\bmod m$, and collisions are resolved by linear probing. The table must support an operation that, given a key $k$, increments its associated value by $1$.\n\nPrecisely specify the semantics as follows:\n- The hash table is a mapping from integers to integers. For a given key $k$, the mathematically correct effect of $R$ concurrent increments performed by each of $T$ threads is that the final stored value equals $T \\times R$, assuming the value is initialized to $0$ and no other operations occur.\n- An unsynchronized implementation of the increment operation performs a read of the current value, computes the successor by adding $1$, and writes the result back, with no synchronization protecting the read–modify–write sequence.\n- A mutual exclusion (mutex) implementation uses a single global mutual exclusion lock to ensure that each increment executes as an indivisible critical section.\n- An atomic operation implementation uses a per-key atomic fetch-and-add primitive with the following linearizable specification: calling `atomic_fetch_add(x, \\Delta)` returns the previous value stored at $x$ and sets $x \\leftarrow x + \\Delta$ as if the operation were instantaneous and indivisible. In this variant, increments are performed by `atomic_fetch_add(x, 1)`.\n\nYour program must construct a deterministic data race in the unsynchronized variant by ensuring that two concurrent increment operations on the same key both read the same prior value before either writes. The test must be constructed so that both threads read the value $0$ and both write back $1$, making the final value $1$ while the mathematically correct value is $2$.\n\nUse the following test suite. For all tests, use a fixed capacity $m = 8$ and the same key $k_0 = 7$, with the key inserted into the table before any increments and its value initialized to $0$.\n\n- Test $1$ (deterministic data race):\n  - Variant: unsynchronized.\n  - Threads: $T = 2$.\n  - Increments per thread: $R = 1$.\n  - Schedule the two threads so that both read before either writes, ensuring a data race and a lost update.\n  - Expected mathematically correct value: $2$.\n  - The test result is a boolean indicating whether the observed final value equals $2$.\n\n- Test $2$ (coarse-grained mutual exclusion, general case):\n  - Variant: mutex.\n  - Threads: $T = 4$.\n  - Increments per thread: $R = 1000$.\n  - Expected mathematically correct value: $4000$.\n  - The test result is a boolean indicating whether the observed final value equals $4000$.\n\n- Test $3$ (atomic fetch-and-add, general case):\n  - Variant: atomic operations.\n  - Threads: $T = 8$.\n  - Increments per thread: $R = 1000$.\n  - Expected mathematically correct value: $8000$.\n  - The test result is a boolean indicating whether the observed final value equals $8000$.\n\n- Test $4$ (boundary condition with zero work):\n  - Variant: mutex.\n  - Threads: $T = 5$.\n  - Increments per thread: $R = 0$.\n  - Expected mathematically correct value: $0$.\n  - The test result is a boolean indicating whether the observed final value equals $0$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Tests $1$ through $4$. For example, if all tests pass, the output must be exactly of the form \"[True,True,True,True]\" with no spaces. No physical units are involved. Angles are not used. Percentages are not used.", "solution": "The provided problem statement is valid. It is scientifically grounded in the principles of concurrent programming, well-posed with a clear objective and sufficient data, and objective in its language. The problem requires the implementation of a concurrent hash table and a demonstration of a data race, along with its resolution using standard synchronization techniques, which is a classic and formalizable problem in computational engineering.\n\nThe solution is implemented by first designing a hash table class, `ConcurrentHashTable`, capable of operating in three distinct modes: `unsynchronized`, `mutex`-based, and `atomic`. The program then executes a suite of four tests to demonstrate and verify the behavior of these modes.\n\n**1. Hash Table Design**\n\nThe hash table uses open addressing with linear probing. Its storage consists of two NumPy arrays: `_keys` and `_values`, each of size equal to the capacity $m$. Empty slots are marked by a special sentinel object in the `_keys` array. The hash function is $h(k) = k \\bmod m$, as specified. The `_find_slot` method implements linear probing to locate the correct slot for a given key $k$, starting at index $h(k)$ and searching sequentially until the key or an empty slot is found.\n\n**2. Increment Operation and Synchronization Variants**\n\nThe core logic resides in the `increment` method, which modifies the value associated with a key $k$. Its behavior is determined by the `variant` specified during the table's instantiation.\n\n-   **Unsynchronized Variant**: This implementation performs a non-atomic read-modify-write sequence. It reads the current value, increments it, and writes it back without any locks. This version is intentionally vulnerable to data races. For the purpose of deterministic testing, this method is augmented with an optional `barrier` argument. When a `threading.Barrier` is provided, the thread will pause after reading the value, allowing for precise control over the execution schedule to force a data race.\n\n-   **Mutex (Coarse-Grained Locking) Variant**: This implementation uses a single, global `threading.Lock` instance for the entire hash table. Any thread wishing to perform an increment must first acquire this lock, making the entire increment operation a critical section. This ensures that only one thread can modify the table at any given time, preventing all data races but potentially limiting concurrency.\n\n-   **Atomic Operations (Fine-Grained Locking) Variant**: The problem specifies a \"per-key atomic fetch-and-add primitive\". This is implemented by creating an array of `threading.Lock` objects, with one lock corresponding to each slot in the hash table. When incrementing a key $k$, the thread first finds its designated slot $idx$ and then acquires the lock `_slot_locks[idx]` before modifying `_values[idx]`. This provides fine-grained synchronization, as operations on keys that map to different slots can proceed in parallel. This correctly models the semantics of per-key atomicity.\n\n**3. Test Suite Execution**\n\nThe `solve` function orchestrates the four specified tests.\n\n-   **Test 1 (Deterministic Data Race)**: This test demonstrates a lost update. It uses the `unsynchronized` variant with $T=2$ threads and $R=1$ increment per thread for key $k_0=7$. A `threading.Barrier(2)` is passed to the `increment` method. Both threads are started. Each thread reads the initial value, which is $0$. They then both wait at the barrier. Once both have reached the barrier, they are unblocked simultaneously. Both threads then attempt to write the value $0+1=1$ to the same location. The final value in the table is $1$. The mathematically correct value is $T \\times R = 2 \\times 1 = 2$. The test compares the observed value ($1$) with the correct value ($2$), yielding `False`, which correctly indicates the failure due to the data race.\n\n-   **Test 2 (Mutex Synchronization)**: This test verifies the correctness of the coarse-grained locking approach under a higher load. With $T=4$ threads and $R=1000$ increments each, the global lock ensures that all $4000$ increments are applied sequentially, although the order is non-deterministic. The final value is guaranteed to be the mathematically correct value of $4000$. The test result is `True`.\n\n-   **Test 3 (Atomic Operations)**: This test verifies the fine-grained locking mechanism. With $T=8$ threads and $R=1000$ increments each, the slot-specific lock protects the updates to the key's value. The final value will be the correct sum of all increments, which is $8000$. The test result is `True`.\n\n-   **Test 4 (Boundary Condition)**: This test uses the `mutex` variant with $T=5$ threads and $R=0$ increments. Since the number of increments is $0$, the worker threads perform no operations on the hash table. The initial value of $0$ remains unchanged. The expected value is $T \\times R = 5 \\times 0 = 0$. The test result is `True`, confirming correct behavior in the zero-work case.\n\nThe program collects the boolean result from each test and prints them in the specified format `[result1,result2,result3,result4]`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport threading\n\nclass ConcurrentHashTable:\n    \"\"\"\n    A fixed-capacity, open-addressed hash table supporting concurrent increments.\n    \n    The table can be configured to use different synchronization paradigms:\n    - 'unsynchronized': No locking, vulnerable to data races.\n    - 'mutex': A single global lock for coarse-grained synchronization.\n    - 'atomic': A lock per hash slot for fine-grained synchronization.\n    \"\"\"\n\n    def __init__(self, capacity, variant):\n        if not isinstance(capacity, int) or capacity <= 0:\n            raise ValueError(\"Capacity must be a positive integer.\")\n        if variant not in ['unsynchronized', 'mutex', 'atomic']:\n            raise ValueError(\"Invalid variant specified.\")\n\n        self.capacity = capacity\n        self.variant = variant\n        \n        # Sentinel object to mark empty slots in the key array.\n        self._EMPTY = object()\n        \n        self._keys = np.full(self.capacity, self._EMPTY, dtype=object)\n        self._values = np.zeros(self.capacity, dtype=np.int64)\n\n        if self.variant == 'mutex':\n            self._lock = threading.Lock()\n        elif self.variant == 'atomic':\n            self._slot_locks = [threading.Lock() for _ in range(self.capacity)]\n\n    def _hash(self, key):\n        return key % self.capacity\n\n    def _find_slot(self, key):\n        \"\"\"Finds the slot for a key using linear probing.\"\"\"\n        start_idx = self._hash(key)\n        for i in range(self.capacity):\n            idx = (start_idx + i) % self.capacity\n            # Return slot if key is found or an empty slot is available for insertion.\n            if self._keys[idx] == key or self._keys[idx] == self._EMPTY:\n                return idx\n        raise RuntimeError(\"Hash table is full and key not found.\")\n\n    def insert(self, key, value=0):\n        \"\"\"Inserts a key-value pair. Assumes non-concurrent insertion.\"\"\"\n        idx = self._find_slot(key)\n        if self._keys[idx] == self._EMPTY:\n            self._keys[idx] = key\n            self._values[idx] = value\n        else: # Key already exists, update value.\n            self._values[idx] = value\n            \n    def get_value(self, key):\n        \"\"\"Retrieves the value for a given key.\"\"\"\n        idx = self._find_slot(key)\n        if self._keys[idx] == key:\n            return self._values[idx]\n        raise KeyError(\"Key not found in table.\")\n\n    def increment(self, key, barrier=None):\n        \"\"\"\n        Increments the value associated with a key by 1.\n        The synchronization behavior depends on the table's variant.\n        A barrier can be passed for deterministic testing of race conditions.\n        \"\"\"\n        idx = self._find_slot(key)\n        if self._keys[idx] != key:\n            raise KeyError(\"Key not found in table for increment.\")\n\n        if self.variant == 'unsynchronized':\n            # Non-atomic read-modify-write.\n            old_val = self._values[idx]\n            # Barrier for deterministic scheduling in Test 1.\n            if barrier:\n                try:\n                    barrier.wait()\n                except threading.BrokenBarrierError:\n                    pass # Barrier might be reset between threads\n            self._values[idx] = old_val + 1\n        \n        elif self.variant == 'mutex':\n            # Coarse-grained lock on the entire table.\n            with self._lock:\n                self._values[idx] += 1\n        \n        elif self.variant == 'atomic':\n            # Fine-grained lock on the specific hash slot.\n            with self._slot_locks[idx]:\n                self._values[idx] += 1\n\ndef solve():\n    \"\"\"\n    Executes the test suite to demonstrate data races and synchronization.\n    \"\"\"\n    test_params = [\n        {'id': 1, 'variant': 'unsynchronized', 'T': 2, 'R': 1, 'm': 8, 'k0': 7, 'correct': 2},\n        {'id': 2, 'variant': 'mutex',          'T': 4, 'R': 1000, 'm': 8, 'k0': 7, 'correct': 4000},\n        {'id': 3, 'variant': 'atomic',         'T': 8, 'R': 1000, 'm': 8, 'k0': 7, 'correct': 8000},\n        {'id': 4, 'variant': 'mutex',          'T': 5, 'R': 0, 'm': 8, 'k0': 7, 'correct': 0},\n    ]\n\n    results = []\n    \n    # --- Test 1: Deterministic Data Race ---\n    case = test_params[0]\n    table = ConcurrentHashTable(capacity=case['m'], variant=case['variant'])\n    table.insert(key=case['k0'], value=0)\n    \n    barrier = threading.Barrier(case['T'])\n    \n    def race_worker(tbl, key, bar):\n        tbl.increment(key, barrier=bar)\n\n    threads = []\n    for _ in range(case['T']):\n        thread = threading.Thread(target=race_worker, args=(table, case['k0'], barrier))\n        threads.append(thread)\n        thread.start()\n    \n    for thread in threads:\n        thread.join()\n        \n    observed = table.get_value(case['k0'])\n    results.append(observed == case['correct']) # Expected: False\n\n    # --- Tests 2, 3, 4: General Cases ---\n    def general_worker(tbl, key, num_increments):\n        for _ in range(num_increments):\n            tbl.increment(key)\n\n    for case in test_params[1:]:\n        table = ConcurrentHashTable(capacity=case['m'], variant=case['variant'])\n        table.insert(key=case['k0'], value=0)\n\n        threads = []\n        for _ in range(case['T']):\n            thread = threading.Thread(target=general_worker, args=(table, case['k0'], case['R']))\n            threads.append(thread)\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n            \n        observed = table.get_value(case['k0'])\n        results.append(observed == case['correct'])\n    \n    # Final print statement in the exact required format.\n    # The str() of a bool in Python is 'True' or 'False' (capitalized).\n    # The problem example shows \"[True,True,True,True]\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2422625"}, {"introduction": "Once correctness is guaranteed, the focus shifts to performance. This practice dives deep into the micro-architectural details of modern GPUs, exploring how memory access patterns can dramatically impact the efficiency of parallel algorithms. You will model a parallel reduction and quantify the performance penalty of shared memory bank conflicts, a common bottleneck in CUDA programming. By comparing a naive memory layout to a conflict-avoiding padded layout, you will gain first-hand experience in architecture-aware optimization, a critical skill for high-performance computing. [@problem_id:2422580]", "problem": "You are given a formal model of Single Instruction Multiple Threads (SIMT) reduction on a Graphics Processing Unit (GPU) shared memory system. A cooperative thread block consists of $T$ threads, partitioned into warps of size $w$. Shared memory is organized into $B$ banks. Each shared memory element occupies $e_b$ bytes, the shared memory bank word width is $b_w$ bytes, and the bank index for an element at logical index $i \\in \\mathbb{Z}_{\\ge 0}$ is defined by\n$$\n\\operatorname{bank}(i) = \\left\\lfloor \\frac{i \\cdot e_b}{b_w} \\right\\rfloor \\bmod B.\n$$\nAssume base address alignment such that the first element has byte address $0$. A parallel reduction over $M$ elements, where $M = \\min(N, T)$, proceeds by the standard stride-halving binary tree pattern with in-place updates in shared memory as follows. For each stride $s \\in \\{1, 2, 4, \\dots\\}$ while $s < M$, the set of active thread indices is\n$$\nA_s = \\{\\, t \\in \\mathbb{Z} \\mid 0 \\le t < M,\\ t \\bmod (2s) = 0,\\ t + s < M \\,\\}.\n$$\nEach active thread $t \\in A_s$ performs, conceptually, three shared-memory operations in order: it reads the element at index $t$, reads the element at index $t+s$, computes their sum, and writes the result back to index $t$. Let $f:\\mathbb{Z}_{\\ge 0} \\to \\mathbb{Z}_{\\ge 0}$ be an index remapping that models optional padding of shared memory to mitigate bank conflicts:\n- Without padding, $f(i) = i$.\n- With conflict-avoidance padding, $f(i) = i + \\left\\lfloor \\frac{i}{B} \\right\\rfloor$.\n\nFor a given stride $s$, define three sub-phases corresponding to the address sets accessed by the active threads in a warp: phase $1$ uses addresses $\\{ f(t) \\mid t \\in A_s \\cap W \\}$, phase $2$ uses addresses $\\{ f(t+s) \\mid t \\in A_s \\cap W \\}$, and phase $3$ uses addresses $\\{ f(t) \\mid t \\in A_s \\cap W \\}$ again for the write, where $W$ is the set of thread indices in a specific warp. For a warp $j \\in \\{0,1,\\dots,\\lceil T/w \\rceil - 1\\}$, define its thread index set as\n$$\nW_j = \\{\\, jw, jw+1, \\dots, \\min(T-1, jw + w - 1) \\,\\}.\n$$\nWithin any sub-phase and warp, if multiple active threads address elements whose bank indices are identical, these accesses are serialized; the sub-phase cost for that warp equals the maximum number of active threads in the warp that map to the same bank during that sub-phase. If a warp has no active threads in a sub-phase, its cost for that sub-phase is $0$. The step cost for stride $s$ is the sum over warps of their three sub-phase costs, and the total cost of the reduction is the sum of step costs over all strides $s$ with $s < M$.\n\nYour task is to write a complete, runnable program that, for each test case, computes two integers:\n- $C_{\\text{naive}}$: the total cost with $f(i) = i$.\n- $C_{\\text{padded}}$: the total cost with $f(i) = i + \\left\\lfloor \\frac{i}{B} \\right\\rfloor$.\n\nAssumptions and requirements:\n- Treat $N$, $T$, $w$, $B$, $e_b$, and $b_w$ as given integers for each test case.\n- The reduction only considers $M = \\min(N, T)$ elements.\n- Angles and physical units are not involved; all outputs are pure integers.\n- Warps are aggregated by summing their costs; there is no overlap between different warps or sub-phases in the cost model.\n\nTest Suite (each tuple lists $(N, T, w, B, e_b, b_w)$ with all numbers in base ten):\n1. $(N, T, w, B, e_b, b_w) = (\\, $1024$, $256$, $32$, $32$, $4$, $4$ \\,)$\n2. $(N, T, w, B, e_b, b_w) = (\\, $1$, $1$, $32$, $32$, $4$, $4$ \\,)$\n3. $(N, T, w, B, e_b, b_w) = (\\, $1000$, $256$, $32$, $32$, $4$, $4$ \\,)$\n4. $(N, T, w, B, e_b, b_w) = (\\, $512$, $256$, $32$, $16$, $4$, $4$ \\,)$\n5. $(N, T, w, B, e_b, b_w) = (\\, $192$, $64$, $32$, $32$, $4$, $4$ \\,)$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list $[C_{\\text{naive}},C_{\\text{padded}}]$. There must be no whitespace in the output. For example, a possible output structure is\n$[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4],[x_5,y_5]]$\nwhere each $x_i$ and $y_i$ is an integer computed by your program for test case $i$.", "solution": "The provided problem is a well-defined exercise in computational modeling, requiring the simulation of a parallel reduction algorithm on a simplified Graphics Processing Unit (GPU) shared memory architecture. The problem statement is scientifically grounded in the principles of parallel computing, logically consistent, and complete. It is therefore deemed valid and a solution will be constructed as specified.\n\nThe task is to calculate the total cost, defined in terms of memory access serialization, for a parallel reduction operation under two different memory layout schemes. The calculation must precisely follow the formal model described. The solution can be implemented by methodically simulating the reduction process step-by-step and aggregating the costs as defined.\n\nThe overall cost is the sum of costs for each step of the reduction. A reduction on $M$ elements proceeds in stages, indexed by a stride $s$ that doubles at each stage, starting from $s=1$ and continuing as long as $s < M$. The total cost $C$ is thus:\n$$\nC = \\sum_{s \\in \\{1, 2, 4, \\dots\\} \\text{ s.t. } s < M} \\text{StepCost}(s)\n$$\nwhere $M = \\min(N, T)$ is the number of elements participating in the reduction.\n\nThe cost for a single step with stride $s$, $\\text{StepCost}(s)$, is the sum of costs incurred by all warps in the thread block. A thread block of $T$ threads is partitioned into $\\lceil T/w \\rceil$ warps of size $w$.\n$$\n\\text{StepCost}(s) = \\sum_{j=0}^{\\lceil T/w \\rceil - 1} \\text{WarpCost}(W_j, s)\n$$\nwhere $W_j$ is the set of thread indices in warp $j$.\n\nThe cost for a single warp $W_j$ at stride $s$, $\\text{WarpCost}(W_j, s)$, is the sum of costs from three distinct sub-phases corresponding to the memory operations performed by the active threads. The set of active threads for stride $s$ is $A_s = \\{\\, t \\in \\mathbb{Z} \\mid 0 \\le t < M,\\ t \\bmod (2s) = 0,\\ t + s < M \\,\\}$. Within a warp $W_j$, the active threads are $A_{s,j} = A_s \\cap W_j$.\nThe three sub-phases are:\n1.  Read from logical index $t$ for each $t \\in A_{s,j}$.\n2.  Read from logical index $t+s$ for each $t \\in A_{s,j}$.\n3.  Write to logical index $t$ for each $t \\in A_{s,j}$.\n\nThe cost of a sub-phase for a warp is the degree of bank conflict, defined as the maximum number of active threads in that warp accessing the same memory bank simultaneously. Let $\\mathcal{I}$ be the set of logical indices accessed in a sub-phase. The cost is:\n$$\n\\text{SubPhaseCost}(\\mathcal{I}) = \\max_{k \\in \\{0, \\dots, B-1\\}} \\left| \\{\\, i \\in \\mathcal{I} \\mid \\operatorname{bank}(f(i)) = k \\,\\} \\right|\n$$\nIf $\\mathcal{I}$ is empty, the cost is $0$. The bank index for a logical index $k$ is given by $\\operatorname{bank}(k) = \\left\\lfloor \\frac{k \\cdot e_b}{b_w} \\right\\rfloor \\bmod B$. The function $f(i)$ maps a logical index to a memory index, modeling padding. We must compute the total cost for two cases:\n-   **Naive layout**: $C_{\\text{naive}}$, where $f(i) = i$.\n-   **Padded layout**: $C_{\\text{padded}}$, where $f(i) = i + \\left\\lfloor \\frac{i}{B} \\right\\rfloor$.\n\nThe algorithm to solve the problem is a direct implementation of this model. It involves a main loop over strides $s$, an inner loop over warps $j$, and for each warp, a calculation of the three sub-phase costs. For each sub-phase, we identify the set of logical indices accessed by the active threads in the warp, apply the appropriate index remapping function $f$, calculate the bank index for each resulting memory index, and find the maximum frequency of any single bank index. Summing these costs over all sub-phases, warps, and strides yields the final total cost. This procedure is executed once for $C_{\\text{naive}}$ and once for $C_{\\text{padded}}$. For all test cases provided, $e_b = b_w = 4$, which simplifies the bank index calculation to $\\operatorname{bank}(k) = k \\bmod B$.\n\nThe implementation will consist of a primary function that computes the total cost for a given set of parameters and a specified padding scheme. This function will be called for each test case for both naive and padded schemes. A helper function will compute the sub-phase cost by determining the bank indices for a given set of logical indices and calculating the maximum collision count.", "answer": "```python\nimport math\nfrom collections import Counter\n\ndef solve():\n    \"\"\"\n    Solves the GPU shared memory reduction cost problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        (1024, 256, 32, 32, 4, 4),\n        (1, 1, 32, 32, 4, 4),\n        (1000, 256, 32, 32, 4, 4),\n        (512, 256, 32, 16, 4, 4),\n        (192, 64, 32, 32, 4, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, T, w, B, eb, bw = case\n        c_naive = compute_total_cost(N, T, w, B, eb, bw, 'naive')\n        c_padded = compute_total_cost(N, T, w, B, eb, bw, 'padded')\n        results.append([c_naive, c_padded])\n    \n    case_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(case_strings)}]\")\n\ndef compute_subphase_cost(indices, padding_type, B, eb, bw):\n    \"\"\"\n    Computes the cost of a single sub-phase for a warp, which is the maximum\n    number of threads accessing the same memory bank.\n    \"\"\"\n    if not indices:\n        return 0\n\n    bank_indices = []\n    for i in indices:\n        mapped_i = 0\n        if padding_type == 'naive':\n            mapped_i = i\n        elif padding_type == 'padded':\n            mapped_i = i + (i // B)\n\n        # Bank index is defined by floor((i * e_b) / b_w) mod B.\n        bank_idx = (mapped_i * eb // bw) % B\n        bank_indices.append(bank_idx)\n\n    if not bank_indices:\n        return 0\n    \n    # The cost is the max number of accesses to any single bank.\n    counts = Counter(bank_indices)\n    return max(counts.values())\n\ndef compute_total_cost(N, T, w, B, eb, bw, padding_type):\n    \"\"\"\n    Computes the total cost of a parallel reduction based on the given model.\n    \"\"\"\n    M = min(N, T)\n    if M < 2:\n        return 0\n        \n    total_cost = 0\n    s = 1\n    num_warps = (T + w - 1) // w\n\n    while s < M:\n        step_cost = 0\n        for j in range(num_warps):\n            warp_start_tid = j * w\n            warp_end_tid = min(T, (j + 1) * w)\n\n            active_threads_in_warp = []\n            for t in range(warp_start_tid, warp_end_tid):\n                # An active thread t must satisfy:\n                # 1. Be part of the reduction: t < M\n                # 2. Be the thread responsible for the sum: t % (2s) == 0\n                # 3. Have a partner to sum with: t + s < M\n                if t < M and t % (2 * s) == 0 and t + s < M:\n                    active_threads_in_warp.append(t)\n            \n            if not active_threads_in_warp:\n                continue\n\n            # Phase 1: Read from logical index t\n            cost1 = compute_subphase_cost(active_threads_in_warp, padding_type, B, eb, bw)\n            \n            # Phase 2: Read from logical index t+s\n            indices_phase2 = [t + s for t in active_threads_in_warp]\n            cost2 = compute_subphase_cost(indices_phase2, padding_type, B, eb, bw)\n\n            # Phase 3: Write to logical index t (same addresses as phase 1)\n            cost3 = cost1\n\n            step_cost += (cost1 + cost2 + cost3)\n\n        total_cost += step_cost\n        s *= 2\n        \n    return total_cost\n\nsolve()\n```", "id": "2422580"}, {"introduction": "Scaling from a single node to a distributed cluster introduces a new primary bottleneck: network communication. This exercise focuses on a common and practical challenge in scientific computing—efficiently transferring non-contiguous data, such as a sub-block of a matrix, between processes. Using a first-principles performance model, you will analyze the trade-offs between manually packing data into a contiguous buffer versus leveraging MPI's powerful derived datatypes. This practice provides essential skills for modeling communication costs and optimizing data transfer, which are fundamental to writing scalable applications with the Message Passing Interface. [@problem_id:2422623]", "problem": "You are asked to model and compare two parallel communication strategies for sending a non-contiguous sub-block of a matrix between two processes in the Message Passing Interface (MPI). The comparison must be done using a first-principles performance model grounded in the Hockney latency–bandwidth model for communication and a simple bandwidth-based model for memory operations.\n\nA non-contiguous sub-block is defined on a dense matrix stored in row-major order. The sub-block is described by its upper-left corner column index, its width, and its height. When the width is strictly smaller than the number of columns of the matrix, each row of the sub-block contributes one contiguous segment of memory; this yields a strided layout across rows.\n\nYour program must compute the predicted end-to-end time, in seconds, for two methods:\n- Method A (Manual Pack/Unpack): The sender manually packs the sub-block into a contiguous buffer, sends it, and the receiver manually unpacks it into the destination sub-block.\n- Method B (MPI Derived Datatype): The sender and receiver use a derived datatype to describe the non-contiguous sub-block, transmit it in a single message, and rely on the MPI library to perform any internal packing and unpacking.\n\nFoundational performance model assumptions:\n- The Hockney latency–bandwidth model states that the time to send a contiguous message of size $n$ bytes over a network is $T_{\\mathrm{net}} = \\alpha + \\dfrac{n}{B_{\\mathrm{net}}}$, where $\\alpha$ is the latency (in seconds) and $B_{\\mathrm{net}}$ is the sustained network bandwidth (in bytes per second).\n- Memory operations are modeled as bandwidth-bound. For a contiguous copy of size $n$ bytes using a highly optimized path, the time is $T = \\dfrac{n}{B_{\\mathrm{copy}}}$. For strided gather or scatter operations of size $n$ bytes, the time is $T = \\dfrac{n}{B_{\\mathrm{gather}}}$ or $T = \\dfrac{n}{B_{\\mathrm{scatter}}}$, respectively. Different effective bandwidths reflect locality and implementation differences.\n- For derived datatypes, there is an additional fixed setup cost $L_{\\mathrm{ddt}}$ (in seconds) per message and a per-segment overhead $s_{\\mathrm{seg}}$ (in seconds) per contiguous segment described by the type on the sender and receiver. A segment is a maximal contiguous piece of the application buffer; for a sub-block with width $w$ inside a matrix with row length $N$ and starting column index $j_0$, the number of segments is:\n  - $1$ if and only if $w = N$ and $j_0 = 0$ (the sub-block is a single contiguous region), and\n  - otherwise $h$, one segment per sub-block row.\n\nDefinitions and required formulas to implement:\n- The matrix is stored in row-major order with $N$ columns. The sub-block starts at column $j_0$, has width $w$ and height $h$. The element size is $e$ bytes. The total number of bytes to transfer is $n = e \\cdot w \\cdot h$.\n- Network communication time for both methods is $T_{\\mathrm{net}} = \\alpha + \\dfrac{n}{B_{\\mathrm{net}}}$.\n- For Method A (Manual Pack/Unpack), the total time is\n  $$T_{\\mathrm{manual}} = \\left(\\frac{n}{B_{\\mathrm{gather,man}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,man}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right).$$\n  If the sub-block is contiguous (that is, $w = N$ and $j_0 = 0$), then use $B_{\\mathrm{gather,man}} = B_{\\mathrm{copy}}$ and $B_{\\mathrm{scatter,man}} = B_{\\mathrm{copy}}$.\n- For Method B (MPI Derived Datatype), the total time is\n  $$T_{\\mathrm{ddt}} = \\left(\\frac{n}{B_{\\mathrm{gather,ddt}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,ddt}}}\\right) + \\left(L_{\\mathrm{ddt}} + s_{\\mathrm{seg}} \\cdot S\\right),$$\n  where $S$ is the number of segments as defined above. If the sub-block is contiguous, then use $B_{\\mathrm{gather,ddt}} = B_{\\mathrm{copy}}$ and $B_{\\mathrm{scatter,ddt}} = B_{\\mathrm{copy}}$.\n\nYour program must compute, for each test case, the ratio\n$$r = \\frac{T_{\\mathrm{manual}}}{T_{\\mathrm{ddt}}}.$$\nA value $r > 1$ indicates that the derived datatype approach is faster under the model; a value $r < 1$ indicates the manual approach is faster.\n\nUnits:\n- All times must be computed in seconds.\n- All bandwidths must be in bytes per second.\n- All sizes ($N$, $w$, $h$, $j_0$) are unitless counts of elements; the element size $e$ is in bytes.\n\nUse the following fixed parameters across all test cases:\n- Latency: $\\alpha = 1.0 \\times 10^{-6}$.\n- Network bandwidth: $B_{\\mathrm{net}} = 1.2 \\times 10^{10}$.\n- Contiguous copy bandwidth: $B_{\\mathrm{copy}} = 2.4 \\times 10^{10}$.\n- Manual gather bandwidth: $B_{\\mathrm{gather,man}} = 8.0 \\times 10^{9}$.\n- Manual scatter bandwidth: $B_{\\mathrm{scatter,man}} = 7.0 \\times 10^{9}$.\n- Derived datatype gather bandwidth: $B_{\\mathrm{gather,ddt}} = 1.4 \\times 10^{10}$.\n- Derived datatype scatter bandwidth: $B_{\\mathrm{scatter,ddt}} = 1.2 \\times 10^{10}$.\n- Derived datatype fixed setup: $L_{\\mathrm{ddt}} = 5.0 \\times 10^{-7}$.\n- Derived datatype per-segment overhead: $s_{\\mathrm{seg}} = 5.0 \\times 10^{-8}$.\n- Element size: $e = 8$.\n\nTest suite:\n- Case 1 (typical moderately sized, non-contiguous): $N = 2048$, $w = 256$, $h = 256$, $j_0 = 128$.\n- Case 2 (very small, latency-dominated, non-contiguous): $N = 2048$, $w = 8$, $h = 8$, $j_0 = 100$.\n- Case 3 (highly strided: many short segments): $N = 8192$, $w = 8$, $h = 1024$, $j_0 = 16$.\n- Case 4 (contiguous rectangle): $N = 2048$, $w = 2048$, $h = 256$, $j_0 = 0$.\n- Case 5 (large, non-contiguous): $N = 4096$, $w = 1024$, $h = 1024$, $j_0 = 0$.\n\nFinal output format:\n- Your program must produce a single line containing the five ratios for the cases above, as a comma-separated list enclosed in square brackets. Each ratio must be rounded to exactly six digits after the decimal point.", "solution": "The problem requires the computation and comparison of the end-to-end transfer time for a non-contiguous sub-block of a matrix between two processes using two distinct strategies within the Message Passing Interface (MPI) framework. The comparison is to be performed by computing the ratio of the total time for a manual pack/unpack method to that of a method using MPI derived datatypes. The analysis is based on a first-principles performance model.\n\nThe primary objective is to calculate the ratio $r = \\frac{T_{\\mathrm{manual}}}{T_{\\mathrm{ddt}}}$ for a given set of test cases. Here, $T_{\\mathrm{manual}}$ is the total time for Method A (Manual Pack/Unpack) and $T_{\\mathrm{ddt}}$ is the total time for Method B (MPI Derived Datatype). All calculations must adhere strictly to the provided formulas and parameters.\n\nFirst, we define the common parameters and variables.\nThe matrix has $N$ columns and elements of size $e$ bytes. The sub-block is defined by its height $h$, width $w$, and the column index of its upper-left corner, $j_0$. The total size of the data to be transferred is $n = e \\cdot w \\cdot h$ bytes.\nThe fixed performance parameters are:\n- Network latency, $\\alpha = 1.0 \\times 10^{-6}$ s.\n- Network bandwidth, $B_{\\mathrm{net}} = 1.2 \\times 10^{10}$ bytes/s.\n- Contiguous memory copy bandwidth, $B_{\\mathrm{copy}} = 2.4 \\times 10^{10}$ bytes/s.\n- Manual gather bandwidth, $B_{\\mathrm{gather,man}} = 8.0 \\times 10^{9}$ bytes/s.\n- Manual scatter bandwidth, $B_{\\mathrm{scatter,man}} = 7.0 \\times 10^{9}$ bytes/s.\n- Derived datatype gather bandwidth, $B_{\\mathrm{gather,ddt}} = 1.4 \\times 10^{10}$ bytes/s.\n- Derived datatype scatter bandwidth, $B_{\\mathrm{scatter,ddt}} = 1.2 \\times 10^{10}$ bytes/s.\n- Derived datatype fixed setup cost, $L_{\\mathrm{ddt}} = 5.0 \\times 10^{-7}$ s.\n- Derived datatype per-segment overhead, $s_{\\mathrm{seg}} = 5.0 \\times 10^{-8}$ s/segment.\n- Element size, $e = 8$ bytes.\n\nA critical condition is whether the sub-block is a single contiguous block of memory. This occurs if and only if the sub-block spans the entire width of the matrix and starts at the first column, i.e., $w = N$ and $j_0 = 0$.\n\nLet us analyze the total time for each method.\n\nFor Method A (Manual Pack/Unpack), the total time $T_{\\mathrm{manual}}$ is given by the sum of sender-side packing costs, network transfer time, and receiver-side unpacking costs. The provided model is:\n$$T_{\\mathrm{manual}} = \\left(\\frac{n}{B_{\\mathrm{gather,man,eff}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,man,eff}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right)$$\nHere, $B_{\\mathrm{gather,man,eff}}$ and $B_{\\mathrm{scatter,man,eff}}$ are the effective bandwidths for the manual gather and scatter operations, respectively. Their values depend on the memory layout of the sub-block.\n- If the sub-block is non-contiguous ($w \\neq N$ or $j_0 \\neq 0$), then $B_{\\mathrm{gather,man,eff}} = B_{\\mathrm{gather,man}}$ and $B_{\\mathrm{scatter,man,eff}} = B_{\\mathrm{scatter,man}}$.\n- If the sub-block is contiguous ($w = N$ and $j_0 = 0$), the gather and scatter operations are equivalent to a contiguous memory copy, so $B_{\\mathrm{gather,man,eff}} = B_{\\mathrm{copy}}$ and $B_{\\mathrm{scatter,man,eff}} = B_{\\mathrm{copy}}$.\n\nFor Method B (MPI Derived Datatype), the total time $T_{\\mathrm{ddt}}$ is the sum of effective memory operation costs handled by the MPI library, network transfer time, and datatype-specific overheads:\n$$T_{\\mathrm{ddt}} = \\left(\\frac{n}{B_{\\mathrm{gather,ddt,eff}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,ddt,eff}}}\\right) + \\left(L_{\\mathrm{ddt}} + s_{\\mathrm{seg}} \\cdot S\\right)$$\nThe effective bandwidths $B_{\\mathrm{gather,ddt,eff}}$ and $B_{\\mathrm{scatter,ddt,eff}}$ are determined similarly:\n- If the sub-block is non-contiguous, $B_{\\mathrm{gather,ddt,eff}} = B_{\\mathrm{gather,ddt}}$ and $B_{\\mathrm{scatter,ddt,eff}} = B_{\\mathrm{scatter,ddt}}$.\n- If the sub-block is contiguous, $B_{\\mathrm{gather,ddt,eff}} = B_{\\mathrm{copy}}$ and $B_{\\mathrm{scatter,ddt,eff}} = B_{\\mathrm{copy}}$.\n\nThe parameter $S$ represents the number of contiguous segments that compose the sub-block. Based on the problem definition for a row-major matrix:\n- If the sub-block is contiguous ($w = N$ and $j_0 = 0$), it forms a single segment, so $S=1$.\n- If the sub-block is non-contiguous, each of its $h$ rows constitutes a separate segment, so $S=h$.\n\nThe procedure for each test case $(N, w, h, j_0)$ is as follows:\n1.  Calculate the total data size: $n = e \\cdot w \\cdot h$.\n2.  Determine if the sub-block is contiguous by checking if $w = N$ and $j_0 = 0$.\n3.  Based on contiguity, determine the number of segments $S$, and select the appropriate effective bandwidths for both $T_{\\mathrm{manual}}$ and $T_{\\mathrm{ddt}}$ calculations.\n4.  Substitute the values into the formulas for $T_{\\mathrm{manual}}$ and $T_{\\mathrm{ddt}}$ to compute their numerical values.\n5.  Compute the final ratio $r = T_{\\mathrm{manual}} / T_{\\mathrm{ddt}}$.\n\nThis process will be executed for each of the five specified test cases, and the resulting ratios will be reported.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the performance ratio of manual packing vs. MPI derived datatypes\n    for transferring a non-contiguous matrix sub-block based on a first-principles model.\n    \"\"\"\n    \n    # Fixed performance parameters\n    alpha = 1.0e-6  # Network latency (s)\n    B_net = 1.2e10  # Network bandwidth (bytes/s)\n    B_copy = 2.4e10  # Contiguous copy bandwidth (bytes/s)\n    B_gather_man = 8.0e9  # Manual gather bandwidth (bytes/s)\n    B_scatter_man = 7.0e9  # Manual scatter bandwidth (bytes/s)\n    B_gather_ddt = 1.4e10  # Derived datatype gather bandwidth (bytes/s)\n    B_scatter_ddt = 1.2e10  # Derived datatype scatter bandwidth (bytes/s)\n    L_ddt = 5.0e-7  # Derived datatype fixed setup cost (s)\n    s_seg = 5.0e-8  # Derived datatype per-segment overhead (s/segment)\n    e = 8  # Element size (bytes)\n\n    # Test suite\n    test_cases = [\n        # (N, w, h, j_0)\n        (2048, 256, 256, 128),  # Case 1: typical moderately sized, non-contiguous\n        (2048, 8, 8, 100),      # Case 2: very small, latency-dominated, non-contiguous\n        (8192, 8, 1024, 16),    # Case 3: highly strided: many short segments\n        (2048, 2048, 256, 0),   # Case 4: contiguous rectangle\n        (4096, 1024, 1024, 0),  # Case 5: large, non-contiguous\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N, w, h, j_0 = case\n\n        # Total data size in bytes\n        n = e * w * h\n\n        # Determine if the sub-block is contiguous\n        is_contiguous = (w == N and j_0 == 0)\n\n        # Determine effective bandwidths and number of segments\n        if is_contiguous:\n            eff_B_gather_man = B_copy\n            eff_B_scatter_man = B_copy\n            eff_B_gather_ddt = B_copy\n            eff_B_scatter_ddt = B_copy\n            S = 1\n        else:\n            eff_B_gather_man = B_gather_man\n            eff_B_scatter_man = B_scatter_man\n            eff_B_gather_ddt = B_gather_ddt\n            eff_B_scatter_ddt = B_scatter_ddt\n            S = h\n\n        # Calculate T_manual\n        time_pack = n / eff_B_gather_man\n        time_copy_send = n / B_copy\n        time_net = alpha + n / B_net\n        time_unpack = n / eff_B_scatter_man\n        time_copy_recv = n / B_copy\n        \n        T_manual = time_pack + time_copy_send + time_net + time_unpack + time_copy_recv\n\n        # Calculate T_ddt\n        time_gather_ddt = n / eff_B_gather_ddt\n        time_scatter_ddt = n / eff_B_scatter_ddt\n        time_overhead_ddt = L_ddt + s_seg * S\n        \n        T_ddt = time_gather_ddt + time_net + time_scatter_ddt + time_overhead_ddt\n        \n        # Calculate the ratio\n        ratio = T_manual / T_ddt\n        \n        results.append(f\"{ratio:.6f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2422623"}]}