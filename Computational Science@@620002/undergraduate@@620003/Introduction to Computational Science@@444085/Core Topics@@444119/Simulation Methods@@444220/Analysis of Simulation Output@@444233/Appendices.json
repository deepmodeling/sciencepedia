{"hands_on_practices": [{"introduction": "Before analyzing the scientific meaning of a simulation's output, we must first verify that the code correctly implements its underlying mathematical model. A cornerstone of this verification process is the convergence study, which confirms that the numerical error decreases at a predictable rate as the simulation's resolution is refined. This exercise [@problem_id:3097495] provides hands-on practice in performing a convergence study for finite-difference methods, a fundamental skill for establishing confidence in any numerical simulation.", "problem": "You are given a periodic scalar field $u(x)$ defined on the interval $[0,2\\pi]$ with $u(x)=\\sin(x)$, where angles are in radians. Consider numerical simulations that approximate the spatial derivative $u'(x)$ on uniform periodic grids using three finite-difference stencils: a forward difference, a second-order central difference, and a fourth-order central difference. Your task is to analyze the simulation outputs across multiple grid resolutions to verify the observed order of accuracy by computing the discrete $L^2$ and $L^\\infty$ norms of the output errors and fitting slopes on a log-log scale.\n\nFundamental base and core definitions to use:\n- The uniform grid has $N$ points with spacing $h=\\frac{2\\pi}{N}$ and grid points $x_i = i h$ for $i=0,1,\\dots,N-1$, with periodic boundary conditions.\n- The exact derivative is $u'(x)=\\cos(x)$.\n- The forward-difference stencil for the derivative is $D_f u_i = \\frac{u_{i+1}-u_i}{h}$ with periodic wrap-around $u_{N}\\equiv u_0$.\n- The second-order central-difference stencil for the derivative is $D_c u_i = \\frac{u_{i+1}-u_{i-1}}{2h}$ with periodic wrap-around.\n- The fourth-order central-difference stencil for the derivative is $D_4 u_i = \\frac{-u_{i+2}+8u_{i+1}-8u_{i-1}+u_{i-2}}{12h}$ with periodic wrap-around.\n- The discrete $L^2$ error norm over the periodic grid is defined as $\\lVert e \\rVert_{2,h} = \\sqrt{h \\sum_{i=0}^{N-1} e_i^2}$, which consistently approximates the continuous $L^2$ norm on $[0,2\\pi]$ for uniform $h$.\n- The discrete $L^\\infty$ error norm is defined as $\\lVert e \\rVert_{\\infty} = \\max_{0 \\le i \\le N-1} |e_i|$.\n- If a method of order $p$ exhibits error scaling $E(h) \\approx C h^p$ for some constant $C$, then taking natural logarithms yields $\\ln(E(h)) \\approx \\ln(C) + p \\ln(h)$. Thus, fitting a straight line to points $\\left(\\ln(h), \\ln(E(h))\\right)$ via least squares yields a slope equal to the observed order $p$.\n\nYour program must implement the three stencils above to generate simulation outputs (numerical derivatives) for $u(x)=\\sin(x)$ on periodic grids, compute error arrays $e_i = D u_i - \\cos(x_i)$ for each grid, evaluate $\\lVert e \\rVert_{2,h}$ and $\\lVert e \\rVert_{\\infty}$ for each grid resolution, and then estimate the order $p$ by fitting the slope of $\\ln(E)$ versus $\\ln(h)$ for both norms.\n\nTest suite:\n- Test case $1$ (happy path, first-order method): forward difference with grid sizes $\\{32,64,128,256\\}$.\n- Test case $2$ (happy path, second-order method): second-order central difference with grid sizes $\\{16,32,64,128\\}$.\n- Test case $3$ (higher-order method): fourth-order central difference with grid sizes $\\{8,16,32,64,128\\}$.\n- Test case $4$ (fine-grid coverage for stability): second-order central difference with grid sizes $\\{128,256,512,1024,2048,4096\\}$.\n\nFor each test case, compute two floats: the estimated slope $p$ from the $L^2$ norm and the estimated slope $p$ from the $L^\\infty$ norm, both obtained by least-squares fitting of $\\ln(E)$ versus $\\ln(h)$ across the given grid sizes. Angles must be treated in radians. No physical unit conversion is required.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list of floats $[p_{L^2},p_{L^\\infty}]$, with each float rounded to three decimal places. For example, the output should look like $[[p_{11},p_{12}],[p_{21},p_{22}],\\dots]$ with no spaces between commas.\n\nThe answer for each test case is a list of two floats. The entire program must be a complete, runnable program that produces the single-line output described above without requiring any user input.", "solution": "The problem requires us to empirically verify the order of accuracy for three finite-difference stencils used to approximate the first derivative of a periodic function. This is a fundamental procedure in computational science for validating simulation code. The analysis involves computing numerical derivatives on grids of varying resolutions, calculating the error against the exact solution, and determining the rate of error convergence as the grid spacing $h$ decreases.\n\nThe methodology consists of four main steps: grid discretization, application of numerical stencils, error computation, and order of accuracy estimation via linear regression on a log-log scale.\n\n**1. Discretization of the Domain and Function**\nThe problem is set on the one-dimensional periodic domain $[0, 2\\pi]$. This domain is discretized into a uniform grid with $N$ points, indexed from $i=0$ to $i=N-1$. The grid points are located at $x_i = i h$, where the grid spacing (or resolution) is $h = \\frac{2\\pi}{N}$.\nThe given scalar field is $u(x) = \\sin(x)$, which is evaluated at each grid point to form a discrete vector of function values, $u_i = u(x_i) = \\sin(i h)$. The analytical derivative, which serves as the ground truth for our error analysis, is $u'(x) = \\cos(x)$, giving the exact discrete values $u'_i = u'(x_i) = \\cos(i h)$.\n\n**2. Finite-Difference Stencils for the First Derivative**\nWe approximate the derivative $u'(x_i)$ using the discrete values $u_i$ on the grid. The problem specifies three stencils. Periodicity is enforced by ensuring that grid indices are handled with wrap-around, i.e., an index $j$ is treated as $j \\pmod N$.\n\n- **Forward Difference ($D_f$)**: A one-sided, first-order accurate stencil.\n$$ D_f u_i = \\frac{u_{i+1} - u_i}{h} $$\nThe leading term in the truncation error for this method is proportional to $h$, so its expected order of accuracy is $p=1$.\n\n- **Second-Order Central Difference ($D_c$)**: A symmetric, second-order accurate stencil.\n$$ D_c u_i = \\frac{u_{i+1} - u_{i-1}}{2h} $$\nIts truncation error is proportional to $h^2$, so its expected order of accuracy is $p=2$.\n\n- **Fourth-Order Central Difference ($D_4$)**: A wider, symmetric, fourth-order accurate stencil.\n$$ D_4 u_i = \\frac{-u_{i+2} + 8u_{i+1} - 8u_{i-1} + u_{i-2}}{12h} $$\nIts truncation error is proportional to $h^4$, so its expected order of accuracy is $p=4$.\n\nFor each stencil, a vector of numerical derivative values, $(D u)_i$, is computed for all points $i=0, \\dots, N-1$.\n\n**3. Error Calculation and Norms**\nThe error of the numerical approximation is captured by the error vector $e$, where each component is the pointwise difference between the numerical and exact derivatives:\n$$ e_i = (D u)_i - u'(x_i) $$\nTo quantify the overall magnitude of this error vector, we compute two different norms for each grid resolution $N$.\n\n- **Discrete $L^2$ Norm ($\\lVert e \\rVert_{2,h}$)**: This norm is a discrete analogue of the continuous $L^2$ norm, measuring the root-mean-square error across the grid. It is defined as:\n$$ \\lVert e \\rVert_{2,h} = \\sqrt{h \\sum_{i=0}^{N-1} e_i^2} $$\nThe factor of $\\sqrt{h}$ ensures that this discrete norm is a consistent approximation of the continuous integral norm $\\left(\\int_0^{2\\pi} e(x)^2 dx\\right)^{1/2}$.\n\n- **$L^\\infty$ Norm ($\\lVert e \\rVert_{\\infty}$)**: Also known as the maximum norm, this measures the worst-case pointwise error on the grid:\n$$ \\lVert e \\rVert_{\\infty} = \\max_{0 \\le i  N} |e_i| $$\n\n**4. Order of Accuracy Verification**\nA numerical method has an order of accuracy $p$ if its error $E$ scales with the grid spacing $h$ according to the relation $E(h) \\approx C h^p$ for some constant $C$ as $h \\to 0$. To determine $p$ from simulation data, we compute the error $E$ (using both the $L^2$ and $L^\\infty$ norms) for a sequence of simulations with progressively smaller grid spacings $h_j$.\n\nBy taking the natural logarithm of the scaling relation, we obtain a linear equation:\n$$ \\ln(E(h)) \\approx \\ln(C) + p \\ln(h) $$\nThis indicates that a plot of $\\ln(E)$ versus $\\ln(h)$ will be approximately a straight line with slope $p$. We can therefore estimate the observed order of accuracy by performing a linear least-squares fit to the set of data points $(\\ln(h_j), \\ln(E_j))$ generated from the simulations. The slope of the resulting best-fit line provides the empirical value of $p$.\n\nThe specified test cases provide different stencils and sets of grid sizes $\\{N\\}$, allowing us to perform this analysis and compare the observed order with the theoretical expectation for each method. For example, for Test Case 1 (forward difference), we expect the slopes for both norms to be close to $1$. For Test Case 3 (fourth-order central difference), we expect the slopes to be close to $4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the observed order of accuracy for three finite-difference stencils\n    by analyzing simulation error norms on a log-log scale.\n    \"\"\"\n\n    # Define the finite-difference stencil functions\n    def forward_diff(u, h):\n        \"\"\"Computes the derivative using a first-order forward-difference stencil.\"\"\"\n        return (np.roll(u, -1) - u) / h\n\n    def central_diff2(u, h):\n        \"\"\"Computes the derivative using a second-order central-difference stencil.\"\"\"\n        return (np.roll(u, -1) - np.roll(u, 1)) / (2 * h)\n\n    def central_diff4(u, h):\n        \"\"\"Computes the derivative using a fourth-order central-difference stencil.\"\"\"\n        return (-np.roll(u, -2) + 8 * np.roll(u, -1) - 8 * np.roll(u, 1) + np.roll(u, 2)) / (12 * h)\n\n    # Define the test cases as a list of tuples: (stencil_function, list_of_N_values)\n    test_cases = [\n        (forward_diff, [32, 64, 128, 256]),\n        (central_diff2, [16, 32, 64, 128]),\n        (central_diff4, [8, 16, 32, 64, 128]),\n        (central_diff2, [128, 256, 512, 1024, 2048, 4096])\n    ]\n\n    all_results = []\n\n    # Process each test case\n    for stencil_func, N_values in test_cases:\n        h_vals = []\n        l2_errors = []\n        linf_errors = []\n\n        # Run simulation for each grid size N\n        for N in N_values:\n            h = 2 * np.pi / N\n            h_vals.append(h)\n            \n            # Create the grid and evaluate the function and its exact derivative\n            x = np.linspace(0, 2 * np.pi, N, endpoint=False)\n            u = np.sin(x)\n            u_prime_exact = np.cos(x)\n            \n            # Compute the numerical derivative using the specified stencil\n            u_prime_numerical = stencil_func(u, h)\n            \n            # Compute the error vector\n            error_vec = u_prime_numerical - u_prime_exact\n            \n            # Compute and store the L2 and L-infinity error norms\n            l2_error = np.sqrt(h * np.sum(error_vec**2))\n            l2_errors.append(l2_error)\n            \n            linf_error = np.max(np.abs(error_vec))\n            linf_errors.append(linf_error)\n\n        # Convert lists to numpy arrays for vectorized operations\n        h_vals_np = np.array(h_vals)\n        l2_errors_np = np.array(l2_errors)\n        linf_errors_np = np.array(linf_errors)\n\n        # Take the natural log of h and the error norms\n        log_h = np.log(h_vals_np)\n        log_l2_err = np.log(l2_errors_np)\n        log_linf_err = np.log(linf_errors_np)\n\n        # Perform linear regression to find the slope (order of accuracy, p)\n        # np.polyfit(x, y, 1) returns [slope, intercept]\n        p_l2 = np.polyfit(log_h, log_l2_err, 1)[0]\n        p_linf = np.polyfit(log_h, log_linf_err, 1)[0]\n        \n        all_results.append([p_l2, p_linf])\n        \n    # Format the final output string exactly as required\n    formatted_results = []\n    for p_l2, p_linf in all_results:\n        formatted_results.append(f\"[{p_l2:.3f},{p_linf:.3f}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3097495"}, {"introduction": "Many scientific simulations incorporate randomness to model complex processes, leading to variability in their output even with identical parameters. To draw reliable conclusions, it is crucial to quantify this stochasticity and establish clear criteria for reproducibility. This practice [@problem_id:3097442] introduces a rigorous statistical framework, grounded in the Central Limit Theorem, to assess run-to-run variability and make objective decisions about whether simulation results are stable and reproducible.", "problem": "You are analyzing the output of a stochastic simulation that, for a fixed parameterization, produces scalar observations whose distribution can be assumed independent and identically distributed. To quantify run-to-run variability across distinct random seeds and to test reproducibility thresholds for key output metrics, implement the following procedure grounded in the Law of Large Numbers and the Central Limit Theorem (CLT).\n\nFundamental definitions:\n- For each random seed $s$ in a finite set $S$, the simulation produces $K$ independent scalar observations $x_{s,1}, x_{s,2}, \\dots, x_{s,K}$ drawn from a normal distribution with true mean $\\mu$ and true standard deviation $\\sigma$. In mathematical notation, $x_{s,i} \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and $\\{x_{s,i}\\}_{i=1}^K$ are independent for fixed $s$.\n- For each seed $s$, define the seed-level sample mean and variance by\n  $$ m_s = \\frac{1}{K} \\sum_{i=1}^{K} x_{s,i}, \\quad v_s = \\frac{1}{K-1} \\sum_{i=1}^{K} \\left(x_{s,i} - m_s\\right)^2, \\quad s_s = \\sqrt{v_s}. $$\n- Across seeds, define the grand mean of seed means and the across-seed standard deviation of seed means by\n  $$ \\bar{m} = \\frac{1}{|S|} \\sum_{s \\in S} m_s, \\quad sd_m = \\sqrt{\\frac{1}{|S| - 1} \\sum_{s \\in S} \\left(m_s - \\bar{m}\\right)^2}. $$\n- Across seeds, define the grand mean of seed standard deviations and the across-seed standard deviation of seed standard deviations by\n  $$ \\bar{s} = \\frac{1}{|S|} \\sum_{s \\in S} s_s, \\quad sd_{sd} = \\sqrt{\\frac{1}{|S| - 1} \\sum_{s \\in S} \\left(s_s - \\bar{s}\\right)^2}. $$\n- Define the relative across-seed variability of the seed means as\n  $$ rv_m = \\begin{cases} \\frac{sd_m}{|\\bar{m}|},  \\text{if } |\\bar{m}| \\ge \\varepsilon \\\\ \\frac{sd_m}{\\bar{s}},  \\text{if } |\\bar{m}|  \\varepsilon \\end{cases} $$\n  where $\\varepsilon > 0$ is a specified small threshold.\n- Define the relative across-seed variability of the seed standard deviations as\n  $$ rv_{sd} = \\frac{sd_{sd}}{\\bar{s}}. $$\n- Define the pairwise agreement fractions for means and standard deviations, respectively, as\n  $$ f_m = \\frac{1}{\\binom{|S|}{2}} \\left|\\left\\{ \\{s,t\\} \\subset S, s \\ne t : |m_s - m_t| \\le \\Delta_m \\right\\}\\right|, $$\n  $$ f_{sd} = \\frac{1}{\\binom{|S|}{2}} \\left|\\left\\{ \\{s,t\\} \\subset S, s \\ne t : |s_s - s_t| \\le \\Delta_{sd} \\right\\}\\right|. $$\n- By the Central Limit Theorem (CLT), the seed-level sample mean $m_s$ is approximately normal with standard error $s_s / \\sqrt{K}$. Use the two-sided nominal $95\\%$ confidence interval for each seed $s$ defined by\n  $$ CI_s = \\left[m_s - z \\cdot \\frac{s_s}{\\sqrt{K}},\\; m_s + z \\cdot \\frac{s_s}{\\sqrt{K}}\\right], $$\n  where $z$ is the $97.5$th percentile of the standard normal distribution, i.e., $z \\approx 1.959963984540054$. Define the coverage fraction\n  $$ c = \\frac{1}{|S|} \\left|\\left\\{ s \\in S : \\bar{m} \\in CI_s \\right\\}\\right|. $$\n\nReproducibility decision rules:\n- The mean metric is declared reproducible if and only if $rv_m \\le \\tau_m$ and $f_m \\ge p_m$.\n- The variability metric (standard deviation across observations within each seed) is declared reproducible if and only if $rv_{sd} \\le \\tau_{sd}$ and $f_{sd} \\ge p_{sd}$.\n- The CLT coverage test is declared reproducible if and only if $c \\ge q$.\n\nImplementation requirements:\n- For each test case, you must generate the seed-level observations exactly as $x_{s,i} \\sim \\mathcal{N}(\\mu, \\sigma^2)$ using a pseudo-random number generator seeded by the given $s \\in S$ to ensure deterministic behavior.\n- Use the formulas above to compute $m_s$, $s_s$, $\\bar{m}$, $sd_m$, $\\bar{s}$, $sd_{sd}$, $rv_m$, $rv_{sd}$, $f_m$, $f_{sd}$, and $c$, then evaluate the decision rules.\n- The final answer for each test case must be a list of three integers $[M, D, C]$, where $M$ is $1$ if the mean metric is reproducible and $0$ otherwise, $D$ is $1$ if the variability metric is reproducible and $0$ otherwise, and $C$ is $1$ if the CLT coverage test passes and $0$ otherwise.\n\nTest suite:\n- Test case A (happy path):\n  - Parameters: $\\mu = 5.0$, $\\sigma = 2.0$, $K = 1000$, $S = \\{0, 1, 2, \\dots, 19\\}$, $\\varepsilon = 10^{-8}$.\n  - Thresholds: $\\tau_m = 0.03$, $\\Delta_m = 0.15$, $p_m = 0.9$, $\\tau_{sd} = 0.05$, $\\Delta_{sd} = 0.12$, $p_{sd} = 0.9$, $q = 0.9$.\n- Test case B (low-sample, tight thresholds):\n  - Parameters: $\\mu = 5.0$, $\\sigma = 2.0$, $K = 30$, $S = \\{0, 1, 2, \\dots, 9\\}$, $\\varepsilon = 10^{-8}$.\n  - Thresholds: $\\tau_m = 0.03$, $\\Delta_m = 0.25$, $p_m = 0.9$, $\\tau_{sd} = 0.05$, $\\Delta_{sd} = 0.20$, $p_{sd} = 0.9$, $q = 1.0$.\n- Test case C (mean near zero, fallback normalization engaged if needed):\n  - Parameters: $\\mu = 0.0$, $\\sigma = 1.0$, $K = 500$, $S = \\{100, 101, 102, \\dots, 114\\}$, $\\varepsilon = 10^{-8}$.\n  - Thresholds: $\\tau_m = 0.06$, $\\Delta_m = 0.10$, $p_m = 0.8$, $\\tau_{sd} = 0.06$, $\\Delta_{sd} = 0.10$, $p_{sd} = 0.8$, $q = 0.9$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of the three per-test-case result vectors in order A, B, C, as a comma-separated list enclosed in square brackets. Each per-test-case vector must itself be a comma-separated list enclosed in square brackets. For example, your output must look like\n  - $[[M_A, D_A, C_A],[M_B, D_B, C_B],[M_C, D_C, C_C]]$,\n  where each symbol is an integer $0$ or $1$ as defined above.", "solution": "The problem requires the implementation of a statistical procedure to assess the reproducibility of a stochastic simulation's output across multiple runs, each initialized with a different random seed. The assessment is based on three criteria: the reproducibility of the mean output, the reproducibility of the output's standard deviation, and the consistency of confidence intervals as predicted by the Central Limit Theorem (CLT). The solution involves generating simulated data, calculating a series of statistical metrics, and applying a set of prescribed decision rules.\n\nThe process for each test case is as follows:\n\n1.  **Data Generation**: For a given test case with parameters for the true mean $\\mu$ and true standard deviation $\\sigma$, number of observations per run $K$, and a set of random seeds $S$, we first generate the raw data. For each seed $s \\in S$, we produce $K$ independent and identically distributed (i.i.d.) observations $\\{x_{s,1}, x_{s,2}, \\dots, x_{s,K}\\}$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$. The pseudo-random number generator must be seeded with $s$ to ensure deterministic and reproducible results.\n\n2.  **Within-Seed Statistics**: For each set of observations corresponding to a seed $s$, we compute the sample mean $m_s$ and the unbiased sample standard deviation $s_s$. The formulas are:\n    $$ m_s = \\frac{1}{K} \\sum_{i=1}^{K} x_{s,i} $$\n    $$ s_s = \\sqrt{\\frac{1}{K-1} \\sum_{i=1}^{K} \\left(x_{s,i} - m_s\\right)^2} $$\n    This step yields $|S|$ values for the mean, $\\{m_s\\}_{s \\in S}$, and $|S|$ values for the standard deviation, $\\{s_s\\}_{s \\in S}$.\n\n3.  **Across-Seed Aggregate Statistics**: Using the collections of seed-level means and standard deviations, we compute four aggregate statistics.\n    - The grand mean of the seed means, $\\bar{m}$:\n      $$ \\bar{m} = \\frac{1}{|S|} \\sum_{s \\in S} m_s $$\n    - The across-seed sample standard deviation of the seed means, $sd_m$:\n      $$ sd_m = \\sqrt{\\frac{1}{|S| - 1} \\sum_{s \\in S} \\left(m_s - \\bar{m}\\right)^2} $$\n    - The grand mean of the seed standard deviations, $\\bar{s}$:\n      $$ \\bar{s} = \\frac{1}{|S|} \\sum_{s \\in S} s_s $$\n    - The across-seed sample standard deviation of the seed standard deviations, $sd_{sd}$:\n      $$ sd_{sd} = \\sqrt{\\frac{1}{|S| - 1} \\sum_{s \\in S} \\left(s_s - \\bar{s}\\right)^2} $$\n    The use of $|S|-1$ in the denominator for $sd_m$ and $sd_{sd}$ corresponds to the computation of an unbiased sample standard deviation of the seed-level statistics.\n\n4.  **Relative Variability Metrics**: We then quantify the relative variability of the seed-level statistics.\n    - The relative variability of the mean, $rv_m$, is defined with a conditional normalization to avoid division by a near-zero mean. Given a small threshold $\\varepsilon > 0$:\n      $$ rv_m = \\begin{cases}\n      \\frac{sd_m}{|\\bar{m}|},  \\text{if } |\\bar{m}| \\ge \\varepsilon \\\\\n      \\frac{sd_m}{\\bar{s}},  \\text{if } |\\bar{m}|  \\varepsilon\n      \\end{cases} $$\n    - The relative variability of the standard deviation, $rv_{sd}$, is normalized by the mean standard deviation:\n      $$ rv_{sd} = \\frac{sd_{sd}}{\\bar{s}} $$\n    where we can assume $\\bar{s} > 0$ since the observations are drawn from a distribution with $\\sigma > 0$.\n\n5.  **Pairwise Agreement Fractions**: To measure the consistency between pairs of runs, we calculate the fraction of pairs whose statistics agree within a given tolerance.\n    - The total number of unique pairs of seeds is $\\binom{|S|}{2}$.\n    - The agreement fraction for the means, $f_m$, is the fraction of pairs $\\{s, t\\} \\subset S$ for which the absolute difference $|m_s - m_t|$ does not exceed a tolerance $\\Delta_m$:\n      $$ f_m = \\frac{1}{\\binom{|S|}{2}} \\left|\\left\\{ \\{s,t\\} \\subset S, s \\ne t : |m_s - m_t| \\le \\Delta_m \\right\\}\\right| $$\n    - Similarly, the agreement fraction for the standard deviations, $f_{sd}$, uses a tolerance $\\Delta_{sd}$:\n      $$ f_{sd} = \\frac{1}{\\binom{|S|}{2}} \\left|\\left\\{ \\{s,t\\} \\subset S, s \\ne t : |s_s - s_t| \\le \\Delta_{sd} \\right\\}\\right| $$\n\n6.  **CLT Coverage Fraction**: The Central Limit Theorem suggests that for a sufficiently large $K$, the sample mean $m_s$ is approximately normally distributed. A nominal $95\\%$ confidence interval ($CI_s$) for the true mean is constructed for each seed $s$:\n    $$ CI_s = \\left[m_s - z \\cdot \\frac{s_s}{\\sqrt{K}},\\; m_s + z \\cdot \\frac{s_s}{\\sqrt{K}}\\right] $$\n    where $z \\approx 1.959963984540054$ is the $97.5$th percentile of the standard normal distribution. The problem defines a coverage test based on how many of these intervals contain the grand mean $\\bar{m}$. The coverage fraction, $c$, is:\n    $$ c = \\frac{1}{|S|} \\left|\\left\\{ s \\in S : \\bar{m} \\in CI_s \\right\\}\\right| $$\n    This is equivalent to checking if $|\\bar{m} - m_s| \\le z \\cdot \\frac{s_s}{\\sqrt{K}}$ for each seed $s$.\n\n7.  **Reproducibility Decision Rules**: Finally, we apply the specified decision rules to determine the reproducibility status for the mean, standard deviation, and CLT coverage.\n    - The mean is reproducible ($M=1$) if $rv_m \\le \\tau_m$ AND $f_m \\ge p_m$. Otherwise, $M=0$.\n    - The standard deviation is reproducible ($D=1$) if $rv_{sd} \\le \\tau_{sd}$ AND $f_{sd} \\ge p_{sd}$. Otherwise, $D=0$.\n    - The CLT coverage test passes ($C=1$) if $c \\ge q$. Otherwise, $C=0$.\n\nThese steps are executed for each test case, and the resulting integer vectors $[M, D, C]$ are collected to form the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef calculate_reproducibility(params, thresholds):\n    \"\"\"\n    Performs the full reproducibility analysis for a single test case.\n    \"\"\"\n    # Unpack parameters\n    mu, sigma, K, S, epsilon = params\n    \n    # Unpack thresholds\n    tau_m, delta_m, p_m = thresholds['mean']\n    tau_sd, delta_sd, p_sd = thresholds['std_dev']\n    q = thresholds['clt']\n    \n    num_seeds = len(S)\n\n    # Step 1 and 2: Data Generation and Within-Seed Statistics\n    m_s_list = np.zeros(num_seeds)\n    s_s_list = np.zeros(num_seeds)\n\n    for i, seed in enumerate(S):\n        rng = np.random.default_rng(seed)\n        observations = rng.normal(loc=mu, scale=sigma, size=K)\n        m_s_list[i] = np.mean(observations)\n        # ddof=1 for unbiased sample standard deviation\n        s_s_list[i] = np.std(observations, ddof=1)\n\n    # Step 3: Across-Seed Aggregate Statistics\n    m_bar = np.mean(m_s_list)\n    # ddof=1 as per formula for sd_m\n    sd_m = np.std(m_s_list, ddof=1) if num_seeds > 1 else 0.0\n\n    s_bar = np.mean(s_s_list)\n    # ddof=1 as per formula for sd_sd\n    sd_sd = np.std(s_s_list, ddof=1) if num_seeds > 1 else 0.0\n\n    # Step 4: Relative Variability Metrics\n    if np.abs(m_bar) >= epsilon:\n        rv_m = sd_m / np.abs(m_bar)\n    else:\n        # Fallback normalization\n        rv_m = sd_m / s_bar if s_bar > 0 else np.inf\n    \n    rv_sd = sd_sd / s_bar if s_bar > 0 else np.inf\n\n    # Step 5: Pairwise Agreement Fractions\n    num_pairs = num_seeds * (num_seeds - 1) / 2\n    if num_pairs > 0:\n        agreement_count_m = 0\n        agreement_count_sd = 0\n        \n        for i, j in combinations(range(num_seeds), 2):\n            if np.abs(m_s_list[i] - m_s_list[j]) = delta_m:\n                agreement_count_m += 1\n            if np.abs(s_s_list[i] - s_s_list[j]) = delta_sd:\n                agreement_count_sd += 1\n        \n        f_m = agreement_count_m / num_pairs\n        f_sd = agreement_count_sd / num_pairs\n    else:\n        f_m = 1.0 # Vacuously true for  2 seeds\n        f_sd = 1.0\n\n    # Step 6: CLT Coverage Fraction\n    z = 1.959963984540054\n    # Standard error of the mean for each seed\n    se_m = s_s_list / np.sqrt(K)\n    \n    # Check if m_bar is within the CI for each seed\n    # |m_bar - m_s| = z * se_m\n    is_covered = np.abs(m_bar - m_s_list) = z * se_m\n    coverage_count = np.sum(is_covered)\n    c = coverage_count / num_seeds if num_seeds > 0 else 0.0\n\n    # Step 7: Reproducibility Decision Rules\n    M = 1 if rv_m = tau_m and f_m >= p_m else 0\n    D = 1 if rv_sd = tau_sd and f_sd >= p_sd else 0\n    C = 1 if c >= q else 0\n\n    return [M, D, C]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Test case A\",\n            \"params\": (5.0, 2.0, 1000, list(range(20)), 1e-8),\n            \"thresholds\": {\n                \"mean\": (0.03, 0.15, 0.9),    # tau_m, delta_m, p_m\n                \"std_dev\": (0.05, 0.12, 0.9), # tau_sd, delta_sd, p_sd\n                \"clt\": 0.9,                  # q\n            }\n        },\n        {\n            \"name\": \"Test case B\",\n            \"params\": (5.0, 2.0, 30, list(range(10)), 1e-8),\n            \"thresholds\": {\n                \"mean\": (0.03, 0.25, 0.9),\n                \"std_dev\": (0.05, 0.20, 0.9),\n                \"clt\": 1.0,\n            }\n        },\n        {\n            \"name\": \"Test case C\",\n            \"params\": (0.0, 1.0, 500, list(range(100, 115)), 1e-8),\n            \"thresholds\": {\n                \"mean\": (0.06, 0.10, 0.8),\n                \"std_dev\": (0.06, 0.10, 0.8),\n                \"clt\": 0.9\n            }\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result_vector = calculate_reproducibility(case['params'], case['thresholds'])\n        all_results.append(result_vector)\n\n    # Format the final output string exactly as required.\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3097442"}, {"introduction": "Modern simulations often produce vast, high-dimensional datasets, where key relationships can be obscured by the sheer volume of information. Principal Component Analysis (PCA) is a powerful technique for uncovering the dominant patterns of variation within such complex outputs. This exercise [@problem_id:3097441] guides you through implementing PCA from first principles to reduce the dimensionality of multivariate simulation data and interpret how input parameters drive the most significant output modes.", "problem": "You are given the task of analyzing multivariate outputs from repeated simulation runs using Principal Component Analysis (PCA). The goal is to identify dominant modes in the output space and to interpret how input parameters drive those modes. You must implement PCA starting from core definitions of variation and covariance, without calling any black-box PCA routines.\n\nYou are to design a program that, for each test case described below, does the following:\n\n1. Data generation from a linear simulator.\n   - For a given number of runs $N$, number of outputs $M$, and number of parameters $K$, generate parameter vectors $p_i \\in \\mathbb{R}^K$ for $i \\in \\{1,\\dots,N\\}$ from a specified zero-mean multivariate normal distribution with covariance matrix $\\Sigma \\in \\mathbb{R}^{K \\times K}$.\n   - Let $B \\in \\mathbb{R}^{M \\times K}$ be a fixed output-basis matrix whose columns encode latent output modes.\n   - For each run $i$, generate the output $y_i \\in \\mathbb{R}^M$ as $y_i = B p_i + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I_M)$ with $I_M$ the $M \\times M$ identity matrix and $\\sigma \\ge 0$ is a specified noise standard deviation.\n   - Stack all $y_i^\\top$ rows into the data matrix $X \\in \\mathbb{R}^{N \\times M}$.\n\n2. PCA on the outputs.\n   - Center each output (column) of $X$ by subtracting its sample mean, producing the centered matrix $X_c$.\n   - Compute the sample covariance in the output space based on $X_c$. From the covariance structure, compute the principal directions in the output space and the associated variances using a decomposition consistent with the definition that principal directions are orthonormal directions maximizing projected variance.\n   - Let the singular value decomposition of $X_c$ be $X_c = U S V^\\top$ with $U \\in \\mathbb{R}^{N \\times r}$, $S \\in \\mathbb{R}^{r \\times r}$ diagonal with nonnegative entries $(s_1,\\dots,s_r)$, $V \\in \\mathbb{R}^{M \\times r}$, and $r = \\min(N, M)$. The first principal direction in the output space is the first column $v_1$ of $V$, and the first principal component scores are the first column of $U S$.\n   - The explained variance ratio of the first component is $\\mathrm{EVR}_1 = \\dfrac{s_1^2}{\\sum_{j=1}^r s_j^2}$.\n\n3. Mode identification and parameter interpretation.\n   - Compute an alignment score in $[0,1]$ between the first principal direction $v_1$ and the columns of $B$. For each column $b_j$ of $B$, define $\\hat{b}_j = b_j / \\lVert b_j \\rVert_2$. The alignment with $b_j$ is $|\\hat{b}_j^\\top v_1|$. Report the maximum alignment across $j \\in \\{0,\\dots,K-1\\}$.\n   - Compute the absolute Pearson correlation between the first principal component scores (an $N$-vector) and each parameter sequence across runs (each is an $N$-vector). Report the index $j \\in \\{0,\\dots,K-1\\}$ of the parameter with the largest absolute correlation (breaking ties by choosing the smallest index).\n\n4. Numerical outputs per test case.\n   - For each test case, produce a list with three entries: $[\\mathrm{EVR}_1, \\mathrm{Alignment}, \\mathrm{DominantIndex}]$.\n   - Round all floating-point outputs ($\\mathrm{EVR}_1$ and $\\mathrm{Alignment}$) to exactly $6$ decimal places. The dominant index is an integer with no rounding.\n\n5. Final output format.\n   - Your program should produce a single line of output containing the results for all test cases as a comma-separated list of the per-test lists, with no spaces, enclosed in square brackets. For example, a valid output for two test cases looks like $[[0.912345,0.998765,0],[0.673210,0.812345,1]]$.\n\nFundamental base for the derivation to be followed in your solution:\n- Sample centering and sample covariance of multivariate data.\n- Definition of principal components as orthonormal directions that maximize projected sample variance, which are given by eigenvectors of the sample covariance and can be computed via singular value decomposition (SVD).\n\nDefinitions and conventions:\n- For a vector $v$, $\\lVert v \\rVert_2 = \\sqrt{\\sum_i v_i^2}$.\n- The normalization operator is $\\mathrm{normalize}(v) = v / \\lVert v \\rVert_2$ for $\\lVert v \\rVert_2 \\ne 0$.\n- The Pearson correlation between two vectors $x$ and $y$ (each of length $N$) is $\\dfrac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^N (y_i - \\bar{y})^2}}$.\n\nAngle units do not apply. There are no physical units in this problem. All answers must be real numbers (for the first two entries) and integers (for the third entry).\n\nTest suite:\n- Shared conventions for all cases:\n  - Let $\\mathrm{rng}$ be the NumPy default random number generator initialized with the given seed. Draw parameters as $P \\sim \\mathcal{N}(0, \\Sigma)$ where $P \\in \\mathbb{R}^{N \\times K}$ has independent rows. Let noise $E \\in \\mathbb{R}^{N \\times M}$ have independent entries $\\mathcal{N}(0, \\sigma^2)$. Form $X = P B^\\top + E$.\n  - Let $\\mathrm{normalize}$ be applied to the raw integer vectors below to form columns of $B$.\n\n- Test case $1$ (happy path, dominant single mode):\n  - $N = 200$, $M = 6$, $K = 3$, seed $= 12345$.\n  - Columns of $B$:\n    - $b_0 = \\mathrm{normalize}([2,1,0,0,0,0])$,\n    - $b_1 = \\mathrm{normalize}([0,1,2,0,0,0])$,\n    - $b_2 = \\mathrm{normalize}([0,0,0,1,-1,0])$.\n  - Covariance $\\Sigma = \\mathrm{diag}([2.0, 0.7, 0.3])$.\n  - Noise standard deviation $\\sigma = 0.1$.\n\n- Test case $2$ (two comparable output modes with correlated parameters):\n  - $N = 150$, $M = 6$, $K = 3$, seed $= 2021$.\n  - Columns of $B$:\n    - $b_0 = \\mathrm{normalize}([1,1,0,0,0,0])$,\n    - $b_1 = \\mathrm{normalize}([-1,1,0,0,0,0])$,\n    - $b_2 = 0.5 \\times \\mathrm{normalize}([0,0,0,1,1,1])$.\n  - Covariance $\\Sigma = \\begin{bmatrix} 1.5  0.9  0 \\\\ 0.9  1.5  0 \\\\ 0  0  0.1 \\end{bmatrix}$.\n  - Noise standard deviation $\\sigma = 0.2$.\n\n- Test case $3$ (near noise-free, clear first mode):\n  - $N = 60$, $M = 5$, $K = 2$, seed $= 7$.\n  - Columns of $B$:\n    - $b_0 = \\mathrm{normalize}([1,2,0,0,0])$,\n    - $b_1 = \\mathrm{normalize}([0,0,1,1,0])$.\n  - Covariance $\\Sigma = \\mathrm{diag}([3.0, 0.1])$.\n  - Noise standard deviation $\\sigma = 0.01$.\n\nYour program must implement the following computations for each test case:\n- Compute $\\mathrm{EVR}_1$ from the singular values of the centered data matrix.\n- Compute the maximum alignment $ \\max_j |\\hat{b}_j^\\top v_1| $ where $\\hat{b}_j$ are the normalized columns of $B$ and $v_1$ is the first right singular vector of the centered data matrix.\n- Compute the index of the parameter with the largest absolute Pearson correlation with the first principal component scores, breaking ties by choosing the smallest index.\n\nFinal output format:\n- Produce exactly one line containing a single list with one triple per test case, in the order of the test cases above.\n- Each triple must be of the form $[\\mathrm{EVR}_1,\\mathrm{Alignment},\\mathrm{DominantIndex}]$ with no spaces and floating-point values rounded to exactly $6$ decimal places.", "solution": "We start from fundamental definitions of variation, covariance, and orthogonal decompositions. Given $N$ runs and $M$ outputs, the data matrix $X \\in \\mathbb{R}^{N \\times M}$ is formed by row-stacking output vectors $y_i^\\top$. The sample mean vector for outputs is $\\bar{x} \\in \\mathbb{R}^M$ with components $\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N X_{ij}$. Centered data is $X_c = X - \\mathbf{1} \\bar{x}^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^N$ is the vector of ones. The sample covariance in the output space is $S = \\frac{1}{N-1} X_c^\\top X_c \\in \\mathbb{R}^{M \\times M}$.\n\nBy the definition of Principal Component Analysis (PCA), principal directions are the orthonormal directions in the output space that maximize the sample variance of the projected data. These are given by the eigenvectors of $S$ associated with descending eigenvalues. Numerically, these eigenvectors can be equivalently obtained via the singular value decomposition (SVD) of $X_c$, $X_c = U S_V V^\\top$, where $S_V$ is a diagonal matrix with nonnegative singular values $(s_1, s_2, \\dots, s_r)$, $r = \\min(N, M)$. The right singular vectors (columns of $V$) are the principal directions in the output space, and the left singular vectors scaled by singular values ($U S_V$) yield the principal component scores. The variance explained by component $j$ is $\\lambda_j = \\frac{s_j^2}{N-1}$, hence the explained variance ratio for component $j$ is\n$$\n\\mathrm{EVR}_j = \\frac{\\lambda_j}{\\sum_{k=1}^r \\lambda_k} = \\frac{s_j^2}{\\sum_{k=1}^r s_k^2}.\n$$\nThis follows from the definition of sample covariance and the spectral theorem.\n\nFor interpreting the first principal direction in terms of the simulator’s known latent output modes (columns of $B$), consider the cosine of the angle between the first principal direction $v_1$ and each normalized simulator column $\\hat{b}_j = b_j / \\lVert b_j \\rVert_2$:\n$$\n\\text{alignment}_j = |\\hat{b}_j^\\top v_1| \\in [0,1].\n$$\nBecause both $v_1$ and $\\hat{b}_j$ are unit vectors, this equals the absolute cosine of the angle between them, with $1$ indicating perfect alignment (or perfect anti-alignment, but we use absolute value to remove sign ambiguity inherent in eigenvectors).\n\nTo relate the first principal component to the input parameters, let $z \\in \\mathbb{R}^N$ be the vector of first principal component scores, which is the first column of $U S_V$. For each parameter $j \\in \\{0, \\dots, K-1\\}$, consider the sequence $p^{(j)} \\in \\mathbb{R}^N$ formed by the $j$-th component of parameter vectors across runs. The Pearson correlation between $z$ and $p^{(j)}$ is\n$$\nr_j = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(p^{(j)}_i - \\overline{p}^{(j)})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (p^{(j)}_i - \\overline{p}^{(j)})^2}},\n$$\nwith $\\bar{z}$ and $\\overline{p}^{(j)}$ denoting sample means. We report $\\arg\\max_j |r_j|$ as the dominant parameter index, breaking ties by choosing the smallest index. This interpretation aligns with the notion that PCA scores capture maximal variance directions in output space, and parameters most strongly linearly associated with those scores are the most influential drivers.\n\nAlgorithmic procedure for each test case:\n1. Construct $B$ by normalizing the provided integer vectors to unit length to obtain columns $b_j$, and applying any specified scalar multipliers.\n2. Initialize a random number generator with the specified seed. Draw $N$ independent parameter vectors from the specified multivariate normal distribution with covariance $\\Sigma$ to form $P \\in \\mathbb{R}^{N \\times K}$.\n3. Draw noise $E \\in \\mathbb{R}^{N \\times M}$ with independent entries from $\\mathcal{N}(0, \\sigma^2)$.\n4. Form $X = P B^\\top + E$ and center columns to obtain $X_c$.\n5. Compute the SVD of $X_c$ as $X_c = U S_V V^\\top$ with $V \\in \\mathbb{R}^{M \\times r}$ and singular values $(s_1, \\dots, s_r)$.\n6. Compute $\\mathrm{EVR}_1 = s_1^2 / \\sum_{k=1}^r s_k^2$.\n7. Extract $v_1$, the first column of $V$. Compute $\\mathrm{Alignment} = \\max_j |\\hat{b}_j^\\top v_1|$ where $\\hat{b}_j = b_j / \\lVert b_j \\rVert_2$ for each column of $B$.\n8. Compute the scores $z$ as the first column of $U S_V$ (i.e., $z = U[:,0] \\cdot s_1$). For each parameter column $P[:, j]$, compute the absolute Pearson correlation between $z$ and $P[:, j]$. Determine $\\mathrm{DominantIndex} = \\arg\\max_j |r_j|$ using smallest-index tie-breaking.\n9. Round $\\mathrm{EVR}_1$ and $\\mathrm{Alignment}$ to exactly $6$ decimal places. Keep $\\mathrm{DominantIndex}$ as an integer.\n10. Output the results for all test cases as a single list of triples with no spaces, as specified.\n\nJustification of correctness:\n- Centering ensures that the covariance captures variations around the mean.\n- The SVD-based computation yields principal directions in the output space (columns of $V$) and singular values whose squares are proportional to sample variances along those directions, by the equivalence between SVD and eigen-decomposition of the sample covariance $S$.\n- The alignment metric uses the cosine similarity, which is the natural measure of directional similarity in Euclidean space, and is invariant to scaling of $b_j$ and the sign ambiguity of $v_1$.\n- The correlation-based interpretation reflects linear association between the latent driver (scores) and parameters, a standard approach in interpreting PCA with known covariates.\n\nThe implementation adheres to the definitions above and computes the requested quantities for the three specified test cases. The final output aggregates results as required, with floating-point values rounded to six decimal places and exact formatting with no spaces.", "answer": "```python\nimport numpy as np\n\ndef normalize(v):\n    v = np.asarray(v, dtype=float)\n    n = np.linalg.norm(v)\n    if n == 0.0:\n        return v.copy()\n    return v / n\n\ndef build_B(columns, scales=None):\n    \"\"\"\n    columns: list of 1D arrays/lists to be normalized\n    scales: optional list of scalars to multiply each normalized column\n    \"\"\"\n    cols = []\n    for i, c in enumerate(columns):\n        vc = normalize(np.array(c, dtype=float))\n        if scales is not None:\n            vc = vc * float(scales[i])\n        cols.append(vc)\n    return np.column_stack(cols)\n\ndef generate_data(N, M, K, B, Sigma, sigma_noise, seed):\n    rng = np.random.default_rng(seed)\n    # Parameters: N x K from multivariate normal\n    P = rng.multivariate_normal(mean=np.zeros(K), cov=Sigma, size=N)\n    # Noise: N x M\n    E = rng.normal(loc=0.0, scale=sigma_noise, size=(N, M))\n    # Outputs: X = P @ B.T + E\n    X = P @ B.T + E\n    return X, P\n\ndef center_columns(X):\n    mean = X.mean(axis=0, keepdims=True)\n    return X - mean, mean.ravel()\n\ndef pca_first_component(Xc):\n    # SVD of centered data matrix\n    U, S, VT = np.linalg.svd(Xc, full_matrices=False)\n    # First right singular vector (principal direction in output space)\n    v1 = VT.T[:, 0]\n    # Explained variance ratio of first component\n    S2 = S**2\n    evr1 = float(S2[0] / S2.sum()) if S2.sum() > 0 else 0.0\n    # First principal component scores: first column of U*S\n    scores1 = U[:, 0] * S[0] if S.size > 0 else np.zeros(Xc.shape[0])\n    return v1, evr1, scores1\n\ndef max_alignment_with_B(v1, B):\n    # Normalize columns of B\n    norms = np.linalg.norm(B, axis=0)\n    # Avoid division by zero\n    norms[norms == 0.0] = 1.0\n    Bn = B / norms\n    # v1 should already be unit norm from SVD, but ensure stability\n    v1n = v1 / (np.linalg.norm(v1) if np.linalg.norm(v1) > 0 else 1.0)\n    dots = np.abs(Bn.T @ v1n)\n    return float(dots.max())\n\ndef pearson_abs_correlations(z, P):\n    # z: N-vector of scores\n    # P: N x K matrix of parameters\n    zc = z - z.mean()\n    z_norm = np.linalg.norm(zc)\n    K = P.shape[1]\n    abs_rs = np.zeros(K, dtype=float)\n    for j in range(K):\n        pj = P[:, j]\n        pjc = pj - pj.mean()\n        pj_norm = np.linalg.norm(pjc)\n        if z_norm == 0.0 or pj_norm == 0.0:\n            r = 0.0\n        else:\n            r = float((zc @ pjc) / (z_norm * pj_norm))\n        abs_rs[j] = abs(r)\n    return abs_rs\n\ndef format_results_no_spaces(results):\n    # results: list of [float, float, int]\n    parts = []\n    for triple in results:\n        f1, f2, idx = triple\n        # Ensure proper formatting for floats and ints\n        if isinstance(f1, (np.floating,)):\n            f1 = float(f1)\n        if isinstance(f2, (np.floating,)):\n            f2 = float(f2)\n        s = f\"[{format(f1, '.6f')},{format(f2, '.6f')},{int(idx)}]\"\n        parts.append(s)\n    return \"[\" + \",\".join(parts) + \"]\"\n\ndef solve():\n    test_cases = []\n\n    # Test case 1\n    # N=200, M=6, K=3, seed=12345\n    N1, M1, K1, seed1 = 200, 6, 3, 12345\n    cols1 = [\n        [2, 1, 0, 0, 0, 0],\n        [0, 1, 2, 0, 0, 0],\n        [0, 0, 0, 1, -1, 0],\n    ]\n    B1 = build_B(cols1)  # unit-norm columns\n    Sigma1 = np.diag([2.0, 0.7, 0.3])\n    sigma_noise1 = 0.1\n    test_cases.append((N1, M1, K1, B1, Sigma1, sigma_noise1, seed1))\n\n    # Test case 2\n    # N=150, M=6, K=3, seed=2021\n    N2, M2, K2, seed2 = 150, 6, 3, 2021\n    cols2 = [\n        [1, 1, 0, 0, 0, 0],\n        [-1, 1, 0, 0, 0, 0],\n        [0, 0, 0, 1, 1, 1],\n    ]\n    scales2 = [1.0, 1.0, 0.5]\n    B2 = build_B(cols2, scales=scales2)\n    Sigma2 = np.array([[1.5, 0.9, 0.0],\n                       [0.9, 1.5, 0.0],\n                       [0.0, 0.0, 0.1]])\n    sigma_noise2 = 0.2\n    test_cases.append((N2, M2, K2, B2, Sigma2, sigma_noise2, seed2))\n\n    # Test case 3\n    # N=60, M=5, K=2, seed=7\n    N3, M3, K3, seed3 = 60, 5, 2, 7\n    cols3 = [\n        [1, 2, 0, 0, 0],\n        [0, 0, 1, 1, 0],\n    ]\n    B3 = build_B(cols3)\n    Sigma3 = np.diag([3.0, 0.1])\n    sigma_noise3 = 0.01\n    test_cases.append((N3, M3, K3, B3, Sigma3, sigma_noise3, seed3))\n\n    results = []\n    for (N, M, K, B, Sigma, sigma_noise, seed) in test_cases:\n        X, P = generate_data(N, M, K, B, Sigma, sigma_noise, seed)\n        Xc, _ = center_columns(X)\n        v1, evr1, scores1 = pca_first_component(Xc)\n        alignment = max_alignment_with_B(v1, B)\n        abs_rs = pearson_abs_correlations(scores1, P)\n        dominant_idx = int(np.argmax(abs_rs))\n        # Round floats to 6 decimals for final output\n        results.append([round(evr1, 6), round(alignment, 6), dominant_idx])\n\n    print(format_results_no_spaces(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3097441"}]}