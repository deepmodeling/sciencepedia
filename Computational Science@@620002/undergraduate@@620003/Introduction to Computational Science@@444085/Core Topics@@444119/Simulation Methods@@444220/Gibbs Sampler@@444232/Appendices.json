{"hands_on_practices": [{"introduction": "The engine of a Gibbs sampler is the set of full conditional distributions for each variable in a model. This first practice focuses on the essential skill of deriving these distributions from a joint probability function [@problem_id:1338680]. By working through this example, you will see how to find the conditional probability for one variable, which is the foundational step for constructing any Gibbs sampler.", "problem": "Consider two interacting binary random variables, $X$ and $Y$, which can each take a value of either 0 or 1. Their joint probability mass function is specified by an unnormalized relationship of the form $P(X=x, Y=y) \\propto f(x, y)$, where the function $f(x, y)$ is defined for $x, y \\in \\{0, 1\\}$. This type of distribution is fundamental in modeling interacting systems in fields like statistical mechanics and machine learning.\n\nFor a specific system, the interaction is described by the function:\n$$f(x, y) = \\exp(5xy - 2x - y)$$\nSo, the joint probability is given by:\n$$P(X=x, Y=y) \\propto \\exp(5xy - 2x - y)$$\n\nYour task is to determine the conditional probability that the variable $X$ takes the value 1, given the state of the variable $Y$. Derive a symbolic expression for $P(X=1 | Y=y)$ as a function of $y$.", "solution": "We are given a joint distribution specified up to normalization as $P(X=x, Y=y) \\propto f(x,y)$ with $f(x,y) = \\exp(5xy - 2x - y)$. For discrete random variables, the conditional probability is given by\n$$\nP(X=1 \\mid Y=y) = \\frac{P(X=1, Y=y)}{\\sum_{x \\in \\{0,1\\}} P(X=x, Y=y)}.\n$$\nBecause $P$ is proportional to $f$, the common proportionality constant cancels in the ratio, so we can compute using $f$ directly:\n$$\nP(X=1 \\mid Y=y) = \\frac{f(1,y)}{f(0,y) + f(1,y)}.\n$$\nCompute the required terms:\n$$\nf(1,y) = \\exp(5 \\cdot 1 \\cdot y - 2 \\cdot 1 - y) = \\exp(4y - 2),\n$$\n$$\nf(0,y) = \\exp(5 \\cdot 0 \\cdot y - 2 \\cdot 0 - y) = \\exp(-y).\n$$\nHence,\n$$\nP(X=1 \\mid Y=y) = \\frac{\\exp(4y - 2)}{\\exp(-y) + \\exp(4y - 2)}.\n$$\nDivide numerator and denominator by $\\exp(4y - 2)$ to simplify:\n$$\nP(X=1 \\mid Y=y) = \\frac{1}{1 + \\exp\\big((-y) - (4y - 2)\\big)} = \\frac{1}{1 + \\exp(2 - 5y)}.\n$$\nThis gives the desired symbolic expression as a function of $y$.", "answer": "$$\\boxed{\\frac{1}{1 + \\exp(2 - 5y)}}$$", "id": "1338680"}, {"introduction": "Once the full conditional distributions are known, we can put the sampler into motion. This exercise [@problem_id:1338677] demonstrates the mechanics of the first update step in a Gibbs sampling chain, moving from an initial state $(X^{(0)}, Y^{(0)})$ to a new state for the first variable, $X^{(1)}$. This practice solidifies the understanding of how the algorithm iteratively generates new samples based on the most recent values of the other variables.", "problem": "A two-dimensional random variable $(X, Y)$ is characterized by its conditional distributions. The conditional distribution of $X$ given $Y=y$ is a continuous uniform distribution on the interval $[0, y]$. The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean of $x$.\n\nA Markov chain Monte Carlo (MCMC) simulation based on the Gibbs sampler is used to generate samples from the joint distribution of $(X, Y)$. The iterative process for generating the sequence of samples $(X^{(t)}, Y^{(t)})$ for $t = 0, 1, 2, \\dots$ is defined as follows:\n1.  Sample $X^{(t+1)}$ from the conditional distribution of $X$ given $Y=Y^{(t)}$.\n2.  Sample $Y^{(t+1)}$ from the conditional distribution of $Y$ given $X=X^{(t+1)}$.\n\nThe process starts from the initial state $(X^{(0)}, Y^{(0)}) = (4, 4)$.\n\nCalculate the probability $P(1 < X^{(1)} < 2.5)$. Express your answer as an exact fraction.", "solution": "We are given the conditional distributions:\n- For any $y>0$, $X \\mid Y=y \\sim \\text{Uniform}(0,y)$, so the conditional density is\n$$\nf_{X\\mid Y}(x \\mid y) = \\begin{cases}\n\\frac{1}{y}, & 0 \\leq x \\leq y, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n- For any $x \\geq 0$, $Y \\mid X=x \\sim \\text{Poisson}(x)$.\n\nIn the Gibbs sampler, step 1 updates $X^{(1)}$ from the conditional distribution of $X$ given $Y=Y^{(0)}$. Since the process starts at $(X^{(0)}, Y^{(0)})=(4,4)$, we have\n$$\nX^{(1)} \\mid Y^{(0)}=4 \\sim \\text{Uniform}(0,4),\n$$\nso its conditional density is $f_{X^{(1)} \\mid Y^{(0)}}(x \\mid 4) = \\frac{1}{4}$ for $0 \\leq x \\leq 4$ and $0$ otherwise.\n\nTherefore,\n$$\nP\\bigl(1 < X^{(1)} < \\tfrac{5}{2}\\bigr) = \\int_{1}^{5/2} \\frac{1}{4} \\, dx = \\frac{1}{4}\\left(\\frac{5}{2} - 1\\right) = \\frac{1}{4} \\cdot \\frac{3}{2} = \\frac{3}{8}.\n$$\nBecause $Y^{(0)}=4$ is deterministic in this setup, this conditional probability equals the unconditional $P(1 < X^{(1)} < 5/2)$.", "answer": "$$\\boxed{\\frac{3}{8}}$$", "id": "1338677"}, {"introduction": "A powerful algorithm is only useful if we understand its limitations. This thought experiment [@problem_id:1338719] explores a critical scenario where the Gibbs sampler fails to converge to the true distribution because it gets stuck in one place. By analyzing a system with a strict dependency between variables, you will discover why the sampler cannot explore the entire state space, highlighting the importance of the chain's ability to move freely between all possible states.", "problem": "A monitoring system for a specialized chemical reactor tracks two correlated temperature readings, $X$ and $Y$. Due to a strict physical constraint imposed by the reactor's design, the joint probability density function $p(x, y)$ for these two readings is non-zero only on the line segment defined by the equation $x + y = 10$, where $x$ and $y$ are both non-negative (i.e., $x \\ge 0$ and $y \\ge 0$). Along this accessible line segment, the probability distribution is uniform.\n\nA data scientist decides to use a Gibbs sampler to generate synthetic data points $(X, Y)$ that follow this distribution. The sampler is initialized at the point $(X_0, Y_0) = (4.5, 5.5)$. The sampling procedure for generating the next state $(X_{t}, Y_{t})$ from the current state $(X_{t-1}, Y_{t-1})$ for $t = 1, 2, 3, \\dots$ is as follows:\n1. Draw a new value for the first variable, $X_{t}$, from the conditional distribution $p(x | Y=Y_{t-1})$.\n2. Draw a new value for the second variable, $Y_{t}$, from the conditional distribution $p(y | X=X_{t})$.\n\nDetermine the state of the sampler, $(X_{100}, Y_{100})$, after 100 full iterations. Express your answer as a pair of numbers.", "solution": "The joint support is the line segment $S=\\{(x,y): x\\ge 0,\\ y\\ge 0,\\ x+y=10\\}$. Since $p(x,y)$ is uniform along this one-dimensional set, for any $y\\in[0,10]$ the conditional $p(x\\mid Y=y)$ concentrates all its mass on the unique $x$ satisfying $x+y=10$, i.e.,\n$$\np(x\\mid Y=y)=\\delta\\!\\left(x-(10-y)\\right),\n$$\nand similarly, for any $x\\in[0,10]$,\n$$\np(y\\mid X=x)=\\delta\\!\\left(y-(10-x)\\right).\n$$\nTherefore, the Gibbs updates are deterministic:\n$$\nX_{t}=10-Y_{t-1},\\qquad Y_{t}=10-X_{t}.\n$$\nStarting from $(X_{0},Y_{0})=(4.5,5.5)$, the first iteration yields\n$$\nX_{1}=10-5.5=4.5,\\qquad Y_{1}=10-4.5=5.5,\n$$\nso the state is unchanged. By induction, if $(X_{t},Y_{t})=(4.5,5.5)$, then\n$$\nX_{t+1}=10-5.5=4.5,\\qquad Y_{t+1}=10-4.5=5.5,\n$$\nhence the chain remains fixed at $(4.5,5.5)$ for all $t$. In particular,\n$$\n(X_{100},Y_{100})=(4.5,5.5).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4.5 & 5.5 \\end{pmatrix}}$$", "id": "1338719"}]}