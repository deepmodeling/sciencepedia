{"hands_on_practices": [{"introduction": "The true power of importance sampling lies not just in changing the distribution, but in choosing a new proposal distribution $q(x)$ that significantly reduces the variance of the estimate. This exercise provides direct practice in this core concept. By working through a concrete example, you will learn how to mathematically derive the optimal parameter for a proposal distribution to make your Monte Carlo estimation as efficient as possible. [@problem_id:767870]", "problem": "Importance sampling is a variance reduction technique used in Monte Carlo methods to estimate properties of a particular distribution, while drawing samples from a different distribution. Suppose we wish to estimate the expectation $I = E_p[f(X)] = \\int f(x) p(x) dx$, where $p(x)$ is the target probability density function. Instead of sampling from $p(x)$, we can draw samples $\\{X_i\\}_{i=1}^N$ from a proposal distribution $q(x)$ and construct the estimator:\n$$ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N f(X_i) w(X_i) $$\nwhere $w(x) = \\frac{p(x)}{q(x)}$ are the importance weights. This estimator is unbiased, i.e., $E_q[\\hat{I}_N] = I$. The efficiency of this method depends on the variance of the estimator, which is given by $\\text{Var}_q(\\hat{I}_N) = \\frac{1}{N} \\text{Var}_q(f(X)w(X))$. To minimize the variance of the estimator for a fixed sample size $N$, one must minimize $\\text{Var}_q(f(X)w(X))$. Since $\\text{Var}_q(Y) = E_q[Y^2] - (E_q[Y])^2$ and $E_q[f(X)w(X)] = I$ is a constant with respect to the choice of $q(x)$, minimizing the variance is equivalent to minimizing the second moment, $E_q[(f(X)w(X))^2]$.\n\nConsider a scenario where the target distribution $p(x)$ is a Rayleigh distribution with scale parameter $\\sigma_p$. The probability density function of a Rayleigh distribution with scale parameter $\\sigma$ is given by:\n$$ g(x; \\sigma) = \\frac{x}{\\sigma^2} e^{-x^2 / (2\\sigma^2)} \\quad \\text{for } x \\ge 0 $$\nWe wish to estimate the mean of this target distribution, so $f(x)=x$. As our proposal distribution $q(x)$, we choose another Rayleigh distribution with a tunable scale parameter $\\sigma_q$.\n\nDerive the optimal value of the proposal scale parameter $\\sigma_q$ that minimizes the variance of the importance sampling estimator. Express your answer in terms of $\\sigma_p$.", "solution": "We wish to minimize the second moment\n$$M_2(\\sigma_q)=E_q[(f(X)w(X))^2]\n=\\int_0^\\infty x^2\\Bigl(\\frac{p(x)}{q(x)}\\Bigr)^2q(x)\\,dx\n=\\int_0^\\infty\\frac{x^2p(x)^2}{q(x)}\\,dx$$\nwith \n$$p(x)=\\frac{x}{\\sigma_p^2}e^{-x^2/(2\\sigma_p^2)},\\quad\nq(x)=\\frac{x}{\\sigma_q^2}e^{-x^2/(2\\sigma_q^2)}.$$\nSubstitute to get\n$$M_2\n=\\int_0^\\infty\\frac{x^2\\,(x/\\sigma_p^2)^2e^{-x^2/\\sigma_p^2}}\n{(x/\\sigma_q^2)e^{-x^2/(2\\sigma_q^2)}}\\,dx\n=\\frac{\\sigma_q^2}{\\sigma_p^4}\\int_0^\\infty x^3e^{-ax^2}\\,dx,$$\nwhere \n$$a=\\frac1{\\sigma_p^2}-\\frac1{2\\sigma_q^2}\n=\\frac{2\\sigma_q^2-\\sigma_p^2}{2\\sigma_p^2\\sigma_q^2}.$$\nUsing \n$$\\int_0^\\infty x^3e^{-ax^2}dx=\\frac1{2a^2},$$\nwe obtain\n$$M_2\n=\\frac{\\sigma_q^2}{\\sigma_p^4}\\,\\frac1{2a^2}\n=2\\,\\frac{\\sigma_q^6}{(2\\sigma_q^2-\\sigma_p^2)^2}.$$\nSet $x=\\sigma_q^2$ and minimize\n$$F(x)=\\frac{x^3}{(2x-\\sigma_p^2)^2}$$\nby solving $dF/dx=0$.  One finds the admissible critical point\n$$\\sigma_q^2=\\tfrac32\\,\\sigma_p^2,\n\\quad\\sigma_q=\\sigma_p\\sqrt{\\tfrac32}.$$", "answer": "$$\\boxed{\\sigma_p\\sqrt{\\frac{3}{2}}}$$", "id": "767870"}, {"introduction": "While a well-chosen proposal distribution can dramatically improve an estimate, a poorly chosen one can be disastrous, leading to biased or even infinitely wrong results. This problem tackles the most critical pitfall in importance sampling: ensuring the support of the proposal distribution $q(x)$ covers the entire support of the target distribution $p(x)$. This conceptual exercise will sharpen your understanding of why this condition is non-negotiable and how to remedy the situation when it is violated. [@problem_id:3143026]", "problem": "You are tasked with estimating the quantity $\\mu = \\mathbb{E}_{p}[g(X)]$ for a bounded measurable function $g$, where $X$ is a real-valued random variable with probability density function $p$. Consider the following scientifically plausible setting: the target density $p$ is uniform on the interval $[0, 2]$, and the proposal density $q$ used for importance sampling is uniform on the interval $[0, 1.5]$. Concretely,\n$$\np(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x), \\qquad q(x) = \\frac{2}{3}\\,\\mathbf{1}_{[0, 1.5]}(x),\n$$\nwhere $\\mathbf{1}_{A}(x)$ is the indicator function of the set $A$. The standard importance sampling method uses samples $X_{1}, \\dots, X_{n} \\overset{\\text{i.i.d.}}{\\sim} q$ and weights $w(x) = \\frac{p(x)}{q(x)}$ to form an estimator of $\\mu$. In this setting, the interval $[1.5, 2]$ has positive mass under $p$ but zero mass under $q$, creating a gap in proposal support.\n\nTo remedy support gaps, one common strategy is to replace $q$ with a mixture proposal that guarantees support coverage. Let $r$ be the uniform density on $[0, 2]$, i.e.,\n$$\nr(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x),\n$$\nand consider the mixture proposal\n$$\nq_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x),\n$$\nwith $\\varepsilon \\in (0, 1)$.\n\nSelect all statements that are correct in this setting.\n\nA. For $g(x) = 1$, the standard importance sampling estimator using $q$ is unbiased for $\\mu$.\n\nB. The condition that $p$ is absolutely continuous with respect to $q$ (i.e., $p \\ll q$) fails here. Consequently, the standard importance sampling estimator using $q$ is biased unless $g(x) = 0$ on $[1.5, 2]$.\n\nC. Using the mixture proposal $q_{\\varepsilon}$ restores unbiasedness for any bounded $g$ and yields a finite variance importance sampling estimator; however, as $\\varepsilon \\to 0$, the variance can become arbitrarily large.\n\nD. Multiplying the weights $\\frac{p(x)}{q(x)}$ by a fixed constant to cap their magnitude prevents bias and ensures finite variance even when $q$ has zero density on parts of the support of $p$.\n\nE. Switching to the self-normalized importance sampling (SNIS) estimator removes the need for support coverage; unbiasedness holds even if $q$ assigns zero density where $p(x) > 0$.", "solution": "The fundamental requirement for an unbiased importance sampling estimator is that the support of the proposal distribution $q$ must contain the support of the target distribution $p$. This is written as $p \\ll q$ ($p$ is absolutely continuous with respect to $q$). In this problem, $\\text{supp}(p) = [0, 2]$ while $\\text{supp}(q) = [0, 1.5]$, so the condition is violated. The region $(1.5, 2]$ has positive probability under $p$ but is never sampled by $q$.\n\n*   **A is incorrect:** The estimator is biased because it completely misses the contribution to the integral from the interval $(1.5, 2]$. For $g(x)=1$, the true mean is $\\mu=1$, but the estimator's expectation is $\\mathbb{E}[\\hat{\\mu}] = \\int_{0}^{1.5} p(x)dx = \\int_{0}^{1.5} (1/2) dx = 0.75$.\n*   **B is correct:** The statement correctly identifies that the absolute continuity condition fails. As a result, the estimator is biased. The bias comes from the ignored region: $-\\int_{1.5}^{2} g(x)p(x)dx$. This bias is zero only if $g(x)$ is zero (almost everywhere) on $[1.5, 2]$.\n*   **C is correct:** The mixture proposal $q_{\\varepsilon}$ has support on $[0, 2]$, which matches the support of $p$. This restores unbiasedness. The variance involves an integral containing the term $1/q_{\\varepsilon}(x)$. On the interval $(1.5, 2]$, $q_{\\varepsilon}(x) = \\varepsilon r(x) = \\varepsilon/2$. As $\\varepsilon \\to 0$, this term blows up, causing the variance of the estimator to become arbitrarily large.\n*   **D is incorrect:** Capping or truncating weights is a technique to reduce variance at the cost of introducing bias. It does not prevent or fix the bias caused by a support mismatch.\n*   **E is incorrect:** The self-normalized (SNIS) estimator is also inconsistent if the support condition is violated. It will converge to the expectation over a truncated version of the distribution, not the true expectation.\n\nTherefore, statements B and C are the correct ones.", "answer": "$$\\boxed{BC}$$", "id": "3143026"}, {"introduction": "With a solid grasp of the theory, it's time to apply importance sampling to a practical computational task where its benefits truly shine: rare-event simulation. This hands-on coding problem challenges you to estimate an extremely small probability within a large combinatorial spaceâ€”a task where simple random sampling would fail. You will implement a complete importance sampling workflow, from designing a suitable proposal distribution to handling the numerical stability issues that arise in real-world applications. [@problem_id:3241899]", "problem": "Consider a combinatorial search space defined as the set $\\mathcal{X} = \\{1,2,\\dots,m\\}^n$ of all sequences $x = (x_1,x_2,\\dots,x_n)$ where each coordinate $x_i$ is an integer between $1$ and $m$. A randomized search algorithm that samples uniformly from $\\mathcal{X}$ induces a target distribution $p(x)$ over $\\mathcal{X}$ where $p(x) = m^{-n}$ for every $x \\in \\mathcal{X}$. Define a real-valued cost function $C(x)$ on $\\mathcal{X}$ of the form\n$$\nC(x) = \\sum_{i=1}^{n} a_i x_i,\n$$\nwhere $a_i > 0$ are fixed coefficients. A solution $x$ is called satisfactory if $C(x) \\le T$ for a given threshold $T > 0$. The quantity of interest is the probability\n$$\n\\pi = \\mathbb{P}_p\\big(C(X) \\le T\\big),\n$$\nwhere $X \\sim p$.\n\nDerive, from the definitions of expectation and probability and the change of measure principle, a computable importance sampling estimator for $\\pi$ that uses a tractable proposal distribution $q(x)$ over $\\mathcal{X}$, constructed as a product of independent, coordinate-wise distributions. In particular, let $q(x)$ be defined by independent coordinates with\n$$\nq(x) = \\prod_{i=1}^{n} q_i(x_i),\n$$\nand for a given parameter $\\beta \\ge 0$,\n$$\nq_i(k) \\propto \\exp\\big(-\\beta\\, a_i\\, k\\big), \\quad k \\in \\{1,2,\\dots,m\\}.\n$$\nThe normalization constants for each coordinate must be included to make $q_i$ a valid probability mass function on $\\{1,2,\\dots,m\\}$. Using this proposal, implement a program that:\n- Samples $N$ independent points $X^{(1)}, X^{(2)}, \\dots, X^{(N)}$ from $q$.\n- Computes a theoretically justified importance weight $w(x)$ for each sampled point.\n- Produces an estimator $\\hat{\\pi}$ for $\\pi$ based on the sampled points and their weights.\n\nYou must ensure numerical stability for large $n$ and $m$ by working in logarithmic space where appropriate. Use a fixed pseudo-random seed so that the results are reproducible. All sampling and estimation should be purely mathematical and unit-free.\n\nYour program must solve the following test suite of parameter settings. For each test case, return the single importance sampling estimate $\\hat{\\pi}$ as a floating-point number:\n\n- Test case $1$ (general case): $n = 12$, $m = 10$, $a_i = i$ for $i = 1,2,\\dots,12$, $T = 300$, $\\beta = 0.2$, $N = 30000$, seed $= 42$.\n- Test case $2$ (uniform baseline via $\\beta = 0$): $n = 12$, $m = 10$, $a_i = i$ for $i = 1,2,\\dots,12$, $T = 300$, $\\beta = 0.0$, $N = 30000$, seed $= 43$.\n- Test case $3$ (rare-event emphasis): $n = 20$, $m = 20$, $a_i = ((i-1) \\bmod 5) + 1$ for $i = 1,2,\\dots,20$, $T = 200$, $\\beta = 0.3$, $N = 40000$, seed $= 44$.\n- Test case $4$ (boundary where every sample is satisfactory): $n = 8$, $m = 15$, $a_i = 2$ for all $i$, $T = 240$, $\\beta = 0.5$, $N = 10000$, seed $= 45$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each floating-point number rounded to six decimal places, for example, $[r_1,r_2,r_3,r_4]$ where each $r_j$ corresponds to the estimate for test case $j$ in the order given above.", "solution": "The probability $\\pi$ can be written as an expectation of an indicator function $\\mathbb{I}(\\cdot)$ under the target distribution $p(x)$:\n$$\n\\pi = \\mathbb{E}_p[\\mathbb{I}(C(X) \\le T)] = \\sum_{x \\in \\mathcal{X}} p(x) \\mathbb{I}(C(x) \\le T)\n$$\nUsing the importance sampling principle, we can express this as an expectation with respect to the proposal distribution $q(x)$:\n$$\n\\pi = \\mathbb{E}_q\\left[w(X) \\mathbb{I}(C(X) \\le T)\\right], \\quad \\text{where } w(x) = \\frac{p(x)}{q(x)}\n$$\nThe corresponding Monte Carlo estimator, using $N$ samples $X^{(j)} \\sim q(x)$, is:\n$$\n\\hat{\\pi} = \\frac{1}{N} \\sum_{j=1}^{N} w(X^{(j)}) \\mathbb{I}(C(X^{(j)}) \\le T)\n$$\nTo implement this, we need the explicit form of the weight $w(x)$. The target distribution is uniform, $p(x) = m^{-n}$. The proposal distribution is a product of independent coordinate-wise distributions, $q(x) = \\prod_{i=1}^{n} q_i(x_i)$, where each $q_i$ is a normalized probability mass function:\n$$\nq_i(k) = \\frac{\\exp(-\\beta a_i k)}{Z_i}, \\quad \\text{with normalization constant } Z_i = \\sum_{k=1}^{m} \\exp(-\\beta a_i k)\n$$\nThe full proposal is $q(x) = \\frac{\\exp(-\\beta \\sum a_i x_i)}{\\prod Z_i} = \\frac{\\exp(-\\beta C(x))}{\\prod Z_i}$. The importance weight is therefore:\n$$\nw(x) = \\frac{p(x)}{q(x)} = \\frac{m^{-n}}{\\exp(-\\beta C(x)) / \\prod Z_i} = m^{-n} \\exp(\\beta C(x)) \\prod_{i=1}^n Z_i\n$$\nFor numerical stability, we compute the logarithm of the weight:\n$$\n\\log w(x) = -n \\log m + \\beta C(x) + \\sum_{i=1}^n \\log Z_i\n$$\nThe algorithm proceeds as follows:\n1.  For each coordinate $i=1,\\dots,n$, compute the normalization constant $Z_i$ and the probability mass function (PMF) for $q_i$. The term $\\log Z_i = \\log(\\sum_k \\exp(-\\beta a_i k))$ should be computed using the log-sum-exp trick.\n2.  Generate $N$ independent samples $X^{(1)}, \\dots, X^{(N)}$ from $q(x)$ by sampling each coordinate $X_i^{(j)}$ from its respective PMF.\n3.  For each sample $X^{(j)}$, compute its cost $C(X^{(j)})$.\n4.  If $C(X^{(j)}) \\le T$, compute its log-weight $\\log w(X^{(j)})$ using the stable formula above.\n5.  The final estimate is the average of the weights for the satisfactory samples: $\\hat{\\pi} = \\frac{1}{N} \\sum_{j \\text{ s.t. } C(X^{(j)}) \\le T} \\exp(\\log w(X^{(j)}))$.\n\nThis approach ensures that all calculations involving products of many small probabilities are handled in log space, preventing underflow and maintaining numerical precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a set of importance sampling problems.\n    \"\"\"\n    test_cases = [\n        # (n, m, a_rule, T, beta, N, seed)\n        (12, 10, 'i',      300, 0.2, 30000, 42),\n        (12, 10, 'i',      300, 0.0, 30000, 43),\n        (20, 20, 'mod',    200, 0.3, 40000, 44),\n        ( 8, 15, 'const',  240, 0.5, 10000, 45),\n    ]\n\n    results = []\n    \n    for n, m, a_rule, T, beta, N, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate 'a' vector based on rule\n        if a_rule == 'i':\n            a = np.arange(1, n + 1, dtype=np.float64)\n        elif a_rule == 'mod':\n            a = ((np.arange(n)) % 5) + 1.0\n        elif a_rule == 'const':\n            a = np.full(n, 2.0)\n\n        # 2. Pre-compute log_Z and sampling distributions (PMFs)\n        log_Z = np.zeros(n)\n        all_pmfs = []\n        k_vals = np.arange(1, m + 1, dtype=np.float64)\n\n        for i in range(n):\n            if beta == 0.0:\n                log_Z[i] = np.log(m)\n                pmf = np.full(m, 1.0 / m)\n            else:\n                # Use log-sum-exp trick for numerical stability\n                v = -beta * a[i] * k_vals\n                # v_max is the first element since k_vals is increasing and arg is negative\n                v_max = v[0] \n                log_Z[i] = v_max + np.log(np.sum(np.exp(v - v_max)))\n                \n                # Compute Probability Mass Function (PMF)\n                log_pmf = v - log_Z[i]\n                pmf = np.exp(log_pmf)\n            \n            # Normalize to correct for minute floating-point inaccuracies\n            pmf /= np.sum(pmf)\n            all_pmfs.append(pmf)\n\n        # 3. Generate N samples from the proposal distribution q(x)\n        samples = np.zeros((N, n), dtype=int)\n        for i in range(n):\n            samples[:, i] = rng.choice(k_vals, size=N, p=all_pmfs[i])\n            \n        # 4. Compute costs for all N samples (vectorized)\n        costs = samples @ a\n        \n        # 5. Filter for satisfactory samples\n        satisfactory_mask = costs = T\n        satisfactory_costs = costs[satisfactory_mask]\n        \n        pi_hat = 0.0\n        if satisfactory_costs.size > 0:\n            # 6. Compute importance weights for satisfactory samples\n            log_w_const = -n * np.log(m) + np.sum(log_Z)\n            log_weights = beta * satisfactory_costs + log_w_const\n            \n            # 7. Sum weights and compute the final estimator\n            # The sum is only over the satisfactory samples as per the estimator formula\n            total_sum_of_weights = np.sum(np.exp(log_weights))\n            pi_hat = total_sum_of_weights / N\n            \n        results.append(f\"{pi_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3241899"}]}