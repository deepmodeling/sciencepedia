{"hands_on_practices": [{"introduction": "This first practice is a cornerstone of computational physics, guiding you to build a complete N-body simulator from first principles. You will implement the gravitational dynamics using a stable symplectic integrator and incorporate a realistic physical process: inelastic collisions. By tracking the conservation of momentum and the dissipation of energy during mergers, this exercise [@problem_id:3163488] provides deep insight into how complex astrophysical systems evolve.", "problem": "You are asked to write a complete, runnable program that simulates a two-dimensional Newtonian $N$-body system with an inelastic merge rule for pairwise separations. The scientific base for this problem is Newton's law of universal gravitation and Newton's second law of motion. Your implementation must derive the forces and update the system state from these fundamentals, and it must quantify how collisions alter the mass spectrum and the mechanical energy.\n\nThe system must be modeled in dimensionless units. Use gravitational constant $G=1$, numerical softening $\\epsilon=10^{-3}$ for force and potential computations, and a symplectic time-stepping scheme. Use a two-dimensional position space and treat all bodies as point masses.\n\nImplement the following from first principles:\n\n- Continuous-time dynamics: Acceleration is determined by pairwise gravitational interaction consistent with Newton's law and Newton's second law, and the position and velocity updates must be performed by a time-reversible, symplectic method.\n- Collision and merge rule: Whenever the instantaneous separation between bodies $i$ and $j$, denoted $r_{ij}$, satisfies $r_{ij}<r_{\\mathrm{coll}}$, replace the colliding bodies by a single body that conserves total mass and total linear momentum of the colliding set. The merged body must be placed at the center-of-mass position of the colliding set. The rule applies transitively to any cluster of mutually colliding bodies within a single time step. The merge operation is fully inelastic.\n\nInitial conditions are fixed and identical across test cases:\n- Number of bodies $N=6$.\n- Masses $[\\,1.0,\\,1.5,\\,0.8,\\,0.6,\\,1.2,\\,1.0\\,]$.\n- Initial positions lie on a unit-radius hexagon centered at the origin. Use the ordered list of positions $[\\, (1.0,\\,0.0),\\, (0.5,\\,0.8660254037844386),\\, (-0.5,\\,0.8660254037844386),\\, (-1.0,\\,0.0),\\, (-0.5,\\,-0.8660254037844386),\\, (0.5,\\,-0.8660254037844386) \\,]$.\n- Initial velocities point radially inward with magnitude $v_{0}=0.1$, i.e., $ \\mathbf{v}_{i}(0) = -v_{0}\\,\\mathbf{r}_{i}(0) $ for each body $i$.\n- Time step $dt=0.001$ and number of steps $n_{\\mathrm{steps}}=3000$.\n\nAll quantities are dimensionless, and no angles beyond those implicit in the given coordinates are required. Energy must be computed in mechanical form as the sum of kinetic and gravitational potential energy using the same softening parameter $\\epsilon$ as in the force computation. The inelastic merge rule removes relative kinetic energy and mutual potential energy associated with merged pairs from the mechanical energy budget; your program must compute the mechanical energy consistently before any evolution and after the full integration.\n\nTest suite:\n- Case $1$: $r_{\\mathrm{coll}}=0.0$ (boundary case: no collisions possible).\n- Case $2$: $r_{\\mathrm{coll}}=0.05$ (moderate collision radius).\n- Case $3$: $r_{\\mathrm{coll}}=0.15$ (large collision radius).\n\nFor each test case, after integrating for $n_{\\mathrm{steps}}$ steps, compute and return the following list in the specified order:\n- Final number of bodies $N_{\\mathrm{final}}$ (an integer).\n- Final total mass $M_{\\mathrm{final}}$ (a float).\n- Total number of merges performed $n_{\\mathrm{merge}}$ (an integer, counting the number of pair-merges implied by cluster merges, i.e., merging $k$ bodies counts as $k-1$).\n- Final kinetic energy $K_{\\mathrm{final}}$ (a float).\n- Final potential energy $U_{\\mathrm{final}}$ (a float).\n- Initial mechanical energy $E_{\\mathrm{initial}}$ (a float).\n- Final mechanical energy $E_{\\mathrm{final}}$ (a float).\n- Fractional mechanical-energy change $\\Delta E / |E_{\\mathrm{initial}}|$ where $\\Delta E = E_{\\mathrm{final}} - E_{\\mathrm{initial}}$ (a float).\n- Final mass mean $\\mu_{m}$ (a float).\n- Final mass standard deviation $\\sigma_{m}$ (a float).\n- Final mass minimum $m_{\\min}$ (a float).\n- Final mass maximum $m_{\\max}$ (a float).\n\nAll floats must be rounded to six decimal places in the output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself represented as a comma-separated list enclosed in square brackets and with no spaces, for example, $[[\\dots],[\\dots],[\\dots]]$.\n\nYour solution must not require any user input. It must be reproducible with the specified initial conditions. Use only dimensionless units throughout and report all results in dimensionless units.", "solution": "The user wants to solve an N-body simulation problem.\n\nHere's my approach:\n1.  **Problem Validation**: I first analyze the problem statement to ensure it is scientifically sound, well-posed, and contains all necessary information. The problem describes a standard N-body simulation with Newtonian gravity and an inelastic collision-merge rule. The physics (Newton's laws, conservation of mass and momentum), numerical methods (symplectic integrator, gravitational softening), and initial conditions are all clearly and correctly specified. The required outputs are well-defined metrics that quantify the system's evolution. The problem is valid and can be solved as stated.\n\n2.  **Core Physics and Mathematics**:\n    *   **Newton's Law of Universal Gravitation**: The force exerted on a body `i` by a body `j` is given by $\\mathbf{F}_{ij} = -G \\frac{m_i m_j}{|\\mathbf{r}_{ij}|^3} \\mathbf{r}_{ij}$, where $\\mathbf{r}_{ij} = \\mathbf{r}_i - \\mathbf{r}_j$ is the displacement vector. To prevent numerical singularities when two bodies are very close, a softening parameter $\\epsilon$ is introduced. The force magnitude becomes $|\\mathbf{F}_{ij}| = G \\frac{m_i m_j}{r_{ij}^2 + \\epsilon^2}$. The force vector is thus $\\mathbf{F}_{ij} = -G \\frac{m_i m_j}{(r_{ij}^2 + \\epsilon^2)^{3/2}} \\mathbf{r}_{ij}$.\n    *   **Newton's Second Law**: The acceleration of body `i` is the net force acting on it divided by its mass: $\\mathbf{a}_i = \\frac{1}{m_i} \\sum_{j \\neq i} \\mathbf{F}_{ij}$.\n    *   **Mechanical Energy**: The total mechanical energy $E$ is the sum of the total kinetic energy $K$ and the total potential energy $U$.\n        *   Kinetic Energy: $K = \\sum_i K_i = \\sum_i \\frac{1}{2} m_i |\\mathbf{v}_i|^2$.\n        *   Potential Energy: With softening, the potential energy for a pair $(i, j)$ is $U_{ij} = -G \\frac{m_i m_j}{\\sqrt{r_{ij}^2 + \\epsilon^2}}$. The total potential energy is the sum over all unique pairs: $U = \\sum_{i<j} U_{ij}$.\n    *   **Inelastic Collision**: When a set of bodies collides, they merge into a single new body. This process must conserve:\n        *   Total Mass: $M_{\\text{new}} = \\sum_{k \\in \\text{cluster}} m_k$.\n        *   Total Linear Momentum: $\\mathbf{P}_{\\text{new}} = \\sum_{k \\in \\text{cluster}} m_k \\mathbf{v}_k$. The velocity of the new body is $\\mathbf{V}_{\\text{new}} = \\mathbf{P}_{\\text{new}} / M_{\\text{new}}$.\n        *   The new body is placed at the center of mass of the cluster: $\\mathbf{R}_{\\text{new}} = \\frac{1}{M_{\\text{new}}} \\sum_{k \\in \\text{cluster}} m_k \\mathbf{r}_k$. This process is inelastic because the kinetic energy associated with the relative motion of the colliding bodies is dissipated and removed from the system's mechanical energy budget.\n\n3.  **Numerical Implementation**:\n    *   **Time Integration**: A second-order Leapfrog integrator, specifically the \"Kick-Drift-Kick\" (KDK) variant, is chosen as it is symplectic and time-reversible, as required. A single time step `dt` from time $t$ to $t+dt$ proceeds as follows:\n        1.  **Kick (half-step)**: Update velocities using the acceleration at time $t$: $\\mathbf{v}(t + dt/2) = \\mathbf{v}(t) + \\mathbf{a}(t) \\cdot dt/2$.\n        2.  **Drift (full-step)**: Update positions using the new half-step velocities: $\\mathbf{r}(t + dt) = \\mathbf{r}(t) + \\mathbf{v}(t + dt/2) \\cdot dt$.\n        3.  **Kick (half-step)**: Compute acceleration $\\mathbf{a}(t + dt)$ at the new positions and complete the velocity update: $\\mathbf{v}(t + dt) = \\mathbf{v}(t + dt/2) + \\mathbf{a}(t + dt) \\cdot dt/2$.\n    *   **Collision Handling**: After each full integration step, the system state is checked for collisions.\n        1.  **Detection**: The pairwise distances $r_{ij}$ between all bodies are computed. A collision is flagged for any pair where $r_{ij} < r_{\\text{coll}}$.\n        2.  **Clustering**: The problem requires the merge rule to apply transitively. This means if body A collides with B, and B with C, all three (A, B, C) must merge. This is equivalent to finding the connected components of a graph where bodies are vertices and an edge exists if $r_{ij} < r_{\\text{coll}}$. I will implement this using a Breadth-First Search (BFS) to identify these collision clusters.\n        3.  **Merging**: For each cluster containing more than one body, the bodies are removed and replaced by a single new body with properties (mass, position, velocity) calculated according to the conservation laws described above. The total number of pairwise merges is tracked.\n    *   **Data Structures**: `NumPy` arrays are used to store the state vectors (masses, positions, velocities) for efficient vectorized computations, particularly for the $O(N^2)$ force and energy calculations. The `scipy.spatial.distance` module is used for efficient pairwise distance calculation.\n\n4.  **Program Structure**:\n    *   A main function `solve()` orchestrates the entire process.\n    *   It defines the test cases based on the provided collision radii $r_{\\text{coll}}$.\n    *   For each test case, it sets up the initial conditions and runs the simulation loop for the specified number of steps.\n    *   Helper functions are defined for calculating accelerations, computing energy, and handling the collision/merge logic.\n    *   After each simulation run, it computes the required list of final metrics.\n    *   Finally, it formats all results according to the strict output specification (comma-separated lists within a main list, with floats rounded to six decimal places) and prints the single-line output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to run the N-body simulation for all test cases and print the results.\n    \"\"\"\n    # Global constants for the simulation\n    G = 1.0\n    EPSILON = 1e-3\n    DT = 0.001\n    N_STEPS = 3000\n\n    # Initial conditions (fixed for all test cases)\n    initial_masses = np.array([1.0, 1.5, 0.8, 0.6, 1.2, 1.0])\n    initial_positions = np.array([\n        [1.0, 0.0],\n        [0.5, 0.8660254037844386],\n        [-0.5, 0.8660254037844386],\n        [-1.0, 0.0],\n        [-0.5, -0.8660254037844386],\n        [0.5, -0.8660254037844386]\n    ])\n    initial_velocities = -0.1 * initial_positions\n\n    def get_accelerations(masses, positions):\n        \"\"\"Calculates gravitational acceleration for all bodies.\"\"\"\n        n_bodies = len(masses)\n        if n_bodies <= 1:\n            return np.zeros((n_bodies, 2))\n\n        pos_diff = positions[:, np.newaxis, :] - positions[np.newaxis, :, :]\n        dist_sq = np.sum(pos_diff**2, axis=-1)\n        \n        # Softened inverse cube law\n        inv_r3 = (dist_sq + EPSILON**2)**(-1.5)\n        np.fill_diagonal(inv_r3, 0.0)\n\n        # a_i = sum_j G * m_j * (r_j - r_i) / |r_ij|^3 (softened)\n        # Note: pos_diff = r_i - r_j, so we use -pos_diff for (r_j - r_i)\n        accel_factor = G * masses[np.newaxis, :] * inv_r3\n        accelerations = np.sum(accel_factor[:, :, np.newaxis] * (-pos_diff), axis=1)\n        \n        return accelerations\n    \n    def calculate_energy(masses, positions, velocities):\n        \"\"\"Calculates kinetic, potential, and total mechanical energy.\"\"\"\n        n_bodies = len(masses)\n        \n        # Kinetic Energy\n        vel_sq = np.sum(velocities**2, axis=1)\n        kinetic_energy = 0.5 * np.sum(masses * vel_sq)\n\n        # Potential Energy\n        potential_energy = 0.0\n        if n_bodies > 1:\n            # Create indices for upper triangle of pairwise interactions\n            i_indices, j_indices = np.triu_indices(n_bodies, k=1)\n            \n            pos_diff = positions[i_indices] - positions[j_indices]\n            dist = np.sqrt(np.sum(pos_diff**2, axis=1) + EPSILON**2)\n            \n            mass_prods = masses[i_indices] * masses[j_indices]\n            \n            potential_energy = -G * np.sum(mass_prods / dist)\n\n        total_energy = kinetic_energy + potential_energy\n        return kinetic_energy, potential_energy, total_energy\n\n    def handle_collisions(masses, positions, velocities, r_coll):\n        \"\"\"Detects and merges colliding bodies.\"\"\"\n        n_bodies = len(masses)\n        if n_bodies <= 1:\n            return masses, positions, velocities, 0\n\n        # Build adjacency matrix for collisions\n        dist_matrix = squareform(pdist(positions))\n        adj_matrix = dist_matrix < r_coll\n        np.fill_diagonal(adj_matrix, False)\n        \n        # Find connected components (collision clusters) using BFS\n        visited = [False] * n_bodies\n        clusters = []\n        for i in range(n_bodies):\n            if not visited[i]:\n                cluster = []\n                queue = [i]\n                visited[i] = True\n                while queue:\n                    u = queue.pop(0)\n                    cluster.append(u)\n                    for v in range(n_bodies):\n                        if adj_matrix[u, v] and not visited[v]:\n                            visited[v] = True\n                            queue.append(v)\n                clusters.append(cluster)\n\n        if len(clusters) == n_bodies:  # No merges\n            return masses, positions, velocities, 0\n\n        new_masses, new_positions, new_velocities = [], [], []\n        num_merges = 0\n\n        for cluster in clusters:\n            if len(cluster) == 1:\n                idx = cluster[0]\n                new_masses.append(masses[idx])\n                new_positions.append(positions[idx])\n                new_velocities.append(velocities[idx])\n            else:\n                cluster_indices = np.array(cluster)\n                cluster_masses = masses[cluster_indices]\n                \n                total_mass = np.sum(cluster_masses)\n                \n                # Center of mass position\n                com_pos = np.sum(cluster_masses[:, np.newaxis] * positions[cluster_indices], axis=0) / total_mass\n                \n                # Center of mass velocity (momentum conservation)\n                com_vel = np.sum(cluster_masses[:, np.newaxis] * velocities[cluster_indices], axis=0) / total_mass\n                \n                new_masses.append(total_mass)\n                new_positions.append(com_pos)\n                new_velocities.append(com_vel)\n                \n                num_merges += len(cluster) - 1\n\n        return np.array(new_masses), np.array(new_positions), np.array(new_velocities), num_merges\n\n    def run_simulation(r_coll):\n        \"\"\"Runs a single simulation for a given collision radius.\"\"\"\n        # Initialize state from fixed initial conditions\n        m = initial_masses.copy()\n        r = initial_positions.copy()\n        v = initial_velocities.copy()\n\n        _, _, e_initial = calculate_energy(m, r, v)\n        total_merges = 0\n\n        # Main integration loop (KDK Leapfrog)\n        for _ in range(N_STEPS):\n            a = get_accelerations(m, r)\n            v_half = v + a * DT / 2.0\n            r_new = r + v_half * DT\n            \n            # For merging, we need full-step velocities to conserve momentum at the time of collision\n            a_new = get_accelerations(m, r_new)\n            v_new = v_half + a_new * DT / 2.0\n            \n            r, v = r_new, v_new\n            \n            # Check for collisions and merge bodies\n            if r_coll > 0.0:\n                  m, r, v, merges_in_step = handle_collisions(m, r, v, r_coll)\n                  total_merges += merges_in_step\n\n        # Final calculations\n        n_final = len(m)\n        m_final = np.sum(m)\n        k_final, u_final, e_final = calculate_energy(m, r, v)\n        \n        delta_e_frac = (e_final - e_initial) / np.abs(e_initial) if e_initial != 0 else 0.0\n        \n        # Mass statistics\n        if n_final > 0:\n            m_mean = np.mean(m)\n            m_std = np.std(m)\n            m_min = np.min(m)\n            m_max = np.max(m)\n        else: # Should not happen in this problem\n            m_mean = m_std = m_min = m_max = 0.0\n            \n        return (n_final, m_final, total_merges, k_final, u_final, e_initial, e_final,\n                delta_e_frac, m_mean, m_std, m_min, m_max)\n\n    test_cases = [\n        0.0,  # Case 1\n        0.05, # Case 2\n        0.15, # Case 3\n    ]\n\n    all_results = []\n    for r_coll in test_cases:\n        result_tuple = run_simulation(r_coll)\n        formatted_case = []\n        for item in result_tuple:\n            if isinstance(item, float):\n                formatted_case.append(f\"{item:.6f}\")\n            else:\n                formatted_case.append(str(item))\n        all_results.append(f\"[{','.join(formatted_case)}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3163488"}, {"introduction": "Having built a simulator, we now use it to explore one of the most profound features of N-body systems: chaos. This exercise [@problem_id:3163501] challenges you to numerically investigate the shadowing lemma by quantifying how tiny rounding errors, controlled by an emulated machine precision $\\epsilon_{\\mathrm{mach}}$, cause trajectories to diverge over time. This practice reveals the fundamental limits on the predictability of chaotic systems and highlights the critical role of numerical precision in scientific computing.", "problem": "You are tasked with writing a complete, runnable program that numerically estimates the shadowing time in a chaotic gravitational $N$-body system as a function of machine epsilon $\\,\\epsilon_{\\mathrm{mach}}\\,$. The shadowing time is defined as the earliest time at which a trajectory computed under a given finite precision $\\,\\epsilon_{\\mathrm{mach}}\\,$ deviates from a high-precision reference trajectory by more than a prescribed threshold. The work must be grounded in first principles and focus on how numerical precision interacts with chaotic dynamics.\n\nStart from the following fundamental base:\n- Newton’s Second Law relates acceleration $\\,\\mathbf{a}_i(t)\\,$ to net force $\\,\\mathbf{F}_i(t)\\,$: \n$$m_i\\,\\mathbf{a}_i(t) = \\mathbf{F}_i(t).$$\n- For point masses under Newtonian gravity in two spatial dimensions, the acceleration of body $\\,i\\,$ due to body $\\,j\\,$ can be modeled as \n$$\\mathbf{a}_i(t) = \\sum_{j \\neq i} G\\,m_j\\,\\frac{\\mathbf{r}_j(t) - \\mathbf{r}_i(t)}{\\left(\\lVert \\mathbf{r}_j(t) - \\mathbf{r}_i(t)\\rVert^2 + \\varepsilon_{\\text{soft}}^2\\right)^{3/2}},$$\nwhere $\\,G\\,$ is the gravitational constant and $\\,\\varepsilon_{\\text{soft}} > 0\\,$ is a small softening length to avoid singular accelerations during close approaches.\n- Kinematics define velocity $\\,\\mathbf{v}_i(t)\\,$ and position $\\,\\mathbf{r}_i(t)\\,$ via $\\,\\mathbf{v}_i(t) = \\frac{d\\mathbf{r}_i}{dt}\\,$ and $\\,\\mathbf{a}_i(t) = \\frac{d\\mathbf{v}_i}{dt}\\,$.\n\nDiscretize time using a symplectic method appropriate for Hamiltonian systems, such as the velocity-Verlet scheme. Use a fixed time step $\\,\\Delta t\\,$ and compute $\\,\\mathbf{a}_i(t)\\,$ from the positions at each step. The reference trajectory must be computed in standard double-precision arithmetic without emulated quantization. The finite-precision trajectory must emulate a machine epsilon $\\,\\epsilon_{\\mathrm{mach}}\\,$ by rounding all intermediate state variables to a binary grid corresponding to that $\\,\\epsilon_{\\mathrm{mach}}\\,$. To emulate $\\,\\epsilon_{\\mathrm{mach}}\\,$, treat each real number $\\,x\\,$ in base-$\\,2\\,$ representation as $\\,x = m \\cdot 2^e\\,$ with mantissa $\\,m\\,$ and exponent $\\,e\\,$, and round $\\,m\\,$ to the nearest multiple of $\\,\\epsilon_{\\mathrm{mach}}\\,$, adjusting $\\,e\\,$ if rounding produces $\\,|m| = 1\\,$. Apply this emulated rounding to positions, velocities, and accelerations at every update in the finite-precision trajectory. The Institute of Electrical and Electronics Engineers (IEEE) 754 standard motivates this representation, but do not assume any specific hardware format; implement the rounding explicitly as described.\n\nDefine the shadowing time $\\,T_{\\text{shad}}(\\epsilon_{\\mathrm{mach}})\\,$ as follows. Let $\\,\\mathbf{r}_i^{\\text{ref}}(t)\\,$ denote the reference positions and $\\,\\mathbf{r}_i^{\\epsilon}(t)\\,$ denote the positions under emulated precision $\\,\\epsilon_{\\mathrm{mach}}\\,$. At each discrete time, compute the separation\n$$D(t;\\epsilon_{\\mathrm{mach}}) = \\max_{i \\in \\{1,\\dots,N\\}} \\left\\lVert \\mathbf{r}_i^{\\epsilon}(t) - \\mathbf{r}_i^{\\text{ref}}(t) \\right\\rVert.$$\nThen \n$$T_{\\text{shad}}(\\epsilon_{\\mathrm{mach}}) = \\min\\{t_k : D(t_k;\\epsilon_{\\mathrm{mach}}) > D_{\\text{thr}}\\},$$\nwhere $\\,t_k = k\\,\\Delta t\\,$ are the discrete times and $\\,D_{\\text{thr}}\\,$ is a fixed threshold. If this threshold is never exceeded within the simulation horizon $\\,T_{\\max}\\,$, set $\\,T_{\\text{shad}}(\\epsilon_{\\mathrm{mach}}) = T_{\\max}\\,$.\n\nImplement the following numerical experiment in dimensionless units:\n- Number of bodies $\\,N = 5\\,$.\n- Gravitational constant $\\,G = 1\\,$.\n- Masses $\\,m_i = 1\\,$ for all $\\,i\\,$.\n- Softening length $\\,\\varepsilon_{\\text{soft}} = 10^{-3}\\,$.\n- Time step $\\,\\Delta t = 5 \\times 10^{-3}\\,$.\n- Maximum simulated time $\\,T_{\\max} = 50\\,$.\n- Shadowing threshold $\\,D_{\\text{thr}} = 5 \\times 10^{-2}\\,$.\n- Initial positions $\\,\\mathbf{r}_i(0)\\,$ and velocities $\\,\\mathbf{v}_i(0)\\,$ in two dimensions:\n  - $\\,\\mathbf{r}_1(0) = (-1,\\,0)\\,$, $\\,\\mathbf{v}_1(0) = (0,\\,0.3)\\,$,\n  - $\\,\\mathbf{r}_2(0) = (1,\\,0)\\,$, $\\,\\mathbf{v}_2(0) = (0,\\,-0.3)\\,$,\n  - $\\,\\mathbf{r}_3(0) = (0,\\,0.9)\\,$, $\\,\\mathbf{v}_3(0) = (-0.35,\\,0)\\,$,\n  - $\\,\\mathbf{r}_4(0) = (0,\\,-0.9)\\,$, $\\,\\mathbf{v}_4(0) = (0.35,\\,0)\\,$,\n  - $\\,\\mathbf{r}_5(0) = (0.2,\\,0.2)\\,$, $\\,\\mathbf{v}_5(0) = (-0.15,\\,0.2)\\,$.\n- Use the velocity-Verlet update rule to advance $\\,\\mathbf{r}_i(t)\\,$ and $\\,\\mathbf{v}_i(t)\\,$ over time.\n\nConstruct a test suite over a range of emulated machine epsilons to probe different regimes:\n- $\\,\\epsilon_{\\mathrm{mach}} = 2^{-10}\\,$,\n- $\\,\\epsilon_{\\mathrm{mach}} = 2^{-15}\\,$,\n- $\\,\\epsilon_{\\mathrm{mach}} = 2^{-20}\\,$,\n- $\\,\\epsilon_{\\mathrm{mach}} = 2^{-25}\\,$,\n- $\\,\\epsilon_{\\mathrm{mach}} = 2^{-30}\\,$.\n\nFor each test case, compute $\\,T_{\\text{shad}}(\\epsilon_{\\mathrm{mach}})\\,$. The final program output must be a single line containing the list of shadowing times for the test suite as a comma-separated list enclosed in square brackets, with each value rounded to $\\,6\\,$ decimal places, for example $\\,\\left[\\ldots\\right]\\,$. No other text may be printed.\n\nYour program must be self-contained and runnable without user input, using standard double-precision for the reference trajectory. All quantities are dimensionless, so no physical units are required in the output. Ensure that your numerical setup is scientifically realistic and self-consistent.", "solution": "The objective is to numerically investigate the shadowing time, $T_{\\text{shad}}$, in a chaotic $5$-body gravitational system. This time quantifies how long a numerical trajectory computed with finite precision remains close to a high-precision reference trajectory. The analysis hinges on implementing a custom numerical simulation from first principles, focusing on the interplay between chaotic dynamics and computational rounding errors, which are systematically controlled by an emulated machine epsilon, $\\epsilon_{\\mathrm{mach}}$.\n\nThe physical system consists of $N=5$ bodies in a two-dimensional plane, interacting via Newtonian gravity. The acceleration $\\mathbf{a}_i$ of a body $i$ with mass $m_i$ at position $\\mathbf{r}_i$ is determined by the sum of gravitational forces from all other bodies $j$:\n$$\n\\mathbf{a}_i(t) = \\sum_{j \\neq i} G\\,m_j\\,\\frac{\\mathbf{r}_j(t) - \\mathbf{r}_i(t)}{\\left(\\lVert \\mathbf{r}_j(t) - \\mathbf{r}_i(t)\\rVert^2 + \\varepsilon_{\\text{soft}}^2\\right)^{3/2}}\n$$\nHere, $G$ is the gravitational constant, and $\\varepsilon_{\\text{soft}}$ is a softening parameter introduced to prevent the denominator from becoming zero during close encounters, which would lead to unphysical, infinite accelerations. For this problem, we use dimensionless units where $G=1$, all masses $m_i=1$, and $\\varepsilon_{\\text{soft}} = 10^{-3}$.\n\nTo simulate the system's evolution, we must numerically integrate the equations of motion, which form a system of second-order ordinary differential equations: $\\frac{d^2\\mathbf{r}_i}{dt^2} = \\mathbf{a}_i(\\{\\mathbf{r}_j\\})$. The gravitational N-body problem is a Hamiltonian system, meaning certain quantities like the total energy are conserved in the true dynamics. For long-term simulations of such systems, it is crucial to use a symplectic integrator, which is designed to preserve the geometric properties of the phase space and exhibits better long-term energy conservation than non-symplectic methods. The velocity-Verlet algorithm is a widely used, second-order symplectic method that is both accurate and computationally efficient. Given the state $(\\mathbf{r}_i(t), \\mathbf{v}_i(t))$ at time $t$ and a fixed time step $\\Delta t$, the state at time $t+\\Delta t$ is computed in three steps:\n1.  Compute a half-step velocity update: $\\mathbf{v}_i(t + \\Delta t/2) = \\mathbf{v}_i(t) + \\frac{1}{2} \\mathbf{a}_i(t) \\Delta t$, where $\\mathbf{a}_i(t)$ is calculated from positions $\\mathbf{r}_i(t)$.\n2.  Update positions to the full new time step: $\\mathbf{r}_i(t + \\Delta t) = \\mathbf{r}_i(t) + \\mathbf{v}_i(t + \\Delta t/2) \\Delta t$.\n3.  Compute the full-step velocity using the new acceleration: $\\mathbf{v}_i(t + \\Delta t) = \\mathbf{v}_i(t + \\Delta t/2) + \\frac{1}{2} \\mathbf{a}_i(t + \\Delta t)$, where $\\mathbf{a}_i(t + \\Delta t)$ is calculated from the new positions $\\mathbf{r}_i(t + \\Delta t)$.\n\nThe core of the investigation is to compare two parallel simulations: a reference trajectory computed using standard double-precision floating-point arithmetic (emulating a very high precision) and a perturbed trajectory computed under an emulated, larger machine epsilon $\\epsilon_{\\mathrm{mach}}$. To emulate finite precision, a custom rounding function is applied. Any floating-point number $x$ can be represented as $x = m \\cdot 2^e$, where $m$ is the mantissa and $e$ is the exponent. The problem specifies rounding the mantissa $m$ to the nearest multiple of the desired $\\epsilon_{\\mathrm{mach}}$. This is achieved by computing the rounded mantissa as $m' = \\text{round}(m / \\epsilon_{\\mathrm{mach}}) \\cdot \\epsilon_{\\mathrm{mach}}$. The rounded number is then $x' = m' \\cdot 2^e$. A special case arises if rounding causes $|m'|=1$, which violates the standard normalization where $|m| \\in [0.5, 1)$. In this case, the representation is renormalized; for instance, $1.0 \\cdot 2^e$ becomes $0.5 \\cdot 2^{e+1}$. This rounding procedure is applied to all state variables (positions, velocities) and intermediate quantities (accelerations) at every update step of the perturbed trajectory's velocity-Verlet integration.\n\nThe shadowing time $T_{\\text{shad}}(\\epsilon_{\\mathrm{mach}})$ is determined by tracking the divergence between the two trajectories. At each time step $t_k = k \\Delta t$, we compute the maximum positional deviation over all bodies:\n$$\nD(t_k; \\epsilon_{\\mathrm{mach}}) = \\max_{i \\in \\{1,\\dots,N\\}} \\left\\lVert \\mathbf{r}_i^{\\epsilon}(t_k) - \\mathbf{r}_i^{\\text{ref}}(t_k) \\right\\rVert\n$$\nwhere $\\mathbf{r}_i^{\\epsilon}$ and $\\mathbf{r}_i^{\\text{ref}}$ are the positions in the perturbed and reference simulations, respectively. The shadowing time is the first time $t_k$ for which this deviation exceeds a predefined threshold, $D(t_k; \\epsilon_{\\mathrm{mach}}) > D_{\\text{thr}}$. If the deviation never exceeds the threshold within the total simulation time $T_{\\max}$, the shadowing time is reported as $T_{\\max}$. For this experiment, the parameters are set to $\\Delta t = 5 \\times 10^{-3}$, $T_{\\max} = 50$, and $D_{\\text{thr}} = 5 \\times 10^{-2}$.\n\nThe algorithm proceeds by iterating through a set of specified $\\epsilon_{\\mathrm{mach}}$ values. For each $\\epsilon_{\\mathrm{mach}}$, two simulations are run in lock-step, starting from identical initial conditions. The reference simulation uses standard arithmetic, while the perturbed simulation applies the custom rounding at each stage. The deviation $D(t_k)$ is checked at every time step until it surpasses $D_{\\text{thr}}$, at which point the current time $t_k$ is recorded as $T_{\\text{shad}}$ for that $\\epsilon_{\\mathrm{mach}}$, and the simulation for that test case terminates. This process is repeated for all test cases, and the resulting shadowing times are collected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef round_to_precision(x, eps_mach):\n    \"\"\"\n    Rounds a number or a NumPy array to a specified machine epsilon.\n    This function emulates finite precision arithmetic by rounding the mantissa\n    of the floating-point representation.\n    \"\"\"\n    # Handle the zero case to avoid issues with np.frexp(0)\n    if np.all(x == 0):\n        return x\n\n    # Decompose x into mantissa and exponent such that x = mantissa * 2**exponent\n    # with 0.5 <= |mantissa| < 1.0 for non-zero x.\n    mantissa, exponent = np.frexp(x)\n    \n    # Round the mantissa to the nearest multiple of eps_mach\n    rounded_mantissa = np.round(mantissa / eps_mach) * eps_mach\n\n    # Handle cases where rounding results in |mantissa| = 1.0, which needs\n    # renormalization to stay within the [0.5, 1.0) range for the mantissa.\n    # Ex: 1.0 * 2^e becomes 0.5 * 2^(e+1).\n    # This check must be done element-wise for arrays.\n    if isinstance(rounded_mantissa, np.ndarray):\n        overflow_mask = np.abs(rounded_mantissa) == 1.0\n        rounded_mantissa[overflow_mask] /= 2.0\n        exponent[overflow_mask] += 1\n    elif abs(rounded_mantissa) == 1.0:\n        rounded_mantissa /= 2.0\n        exponent += 1\n        \n    # Reconstruct the number from the rounded mantissa and adjusted exponent.\n    return np.ldexp(rounded_mantissa, exponent)\n\ndef calculate_accelerations(positions, masses, G, eps_soft_sq):\n    \"\"\"\n    Calculates the acceleration of each body due to gravitational interaction.\n    \"\"\"\n    N = positions.shape[0]\n    # Reshape for broadcasting:\n    # positions[:, np.newaxis, :] -> shape (N, 1, 2)\n    # positions[np.newaxis, :, :] -> shape (1, N, 2)\n    # diffs will have shape (N, N, 2), where diffs[i, j] = r_i - r_j\n    diffs = positions[:, np.newaxis, :] - positions[np.newaxis, :, :]\n    \n    # Calculate squared distances, shape (N, N)\n    dist_sq = np.sum(diffs**2, axis=-1)\n    \n    # Calculate the inverse cube of the softened distance\n    inv_r3 = (dist_sq + eps_soft_sq)**(-1.5)\n    # Set diagonal to zero to avoid self-interaction (i=j)\n    np.fill_diagonal(inv_r3, 0.0)\n    \n    # Calculate accelerations. The summation is over axis j=1.\n    # masses[np.newaxis, :, np.newaxis] broadcasts masses for j.\n    # The result has shape (N, 2). The formula is a_i = sum_j G*m_j*(r_j-r_i)/|...|^3\n    # which is sum_j G*m_j*(-diffs_ij)/|...|^3\n    accelerations = -G * np.sum(masses[np.newaxis, :, np.newaxis] * diffs * inv_r3[:, :, np.newaxis], axis=1)\n    \n    return accelerations\n\ndef run_simulation(eps_mach, N, G, masses, eps_soft, dt, T_max, D_thr, r0, v0):\n    \"\"\"\n    Runs a single N-body simulation for a given machine epsilon and returns the shadowing time.\n    \"\"\"\n    eps_soft_sq = eps_soft**2\n    num_steps = int(T_max / dt)\n\n    # --- Initialize Reference Trajectory ---\n    r_ref = r0.copy()\n    v_ref = v0.copy()\n    \n    # --- Initialize Finite-Precision Trajectory ---\n    # Start with identical initial conditions, then apply initial rounding.\n    r_eps = r0.copy()\n    v_eps = v0.copy()\n    r_eps = round_to_precision(r_eps, eps_mach)\n    v_eps = round_to_precision(v_eps, eps_mach)\n    \n    # Calculate initial accelerations\n    a_ref = calculate_accelerations(r_ref, masses, G, eps_soft_sq)\n    a_eps = calculate_accelerations(r_eps, masses, G, eps_soft_sq)\n    a_eps = round_to_precision(a_eps, eps_mach)\n\n    for k in range(num_steps):\n        # --- Update Reference Trajectory (Velocity-Verlet) ---\n        v_ref_half = v_ref + 0.5 * a_ref * dt\n        r_ref = r_ref + v_ref_half * dt\n        a_ref_new = calculate_accelerations(r_ref, masses, G, eps_soft_sq)\n        v_ref = v_ref_half + 0.5 * a_ref_new * dt\n        a_ref = a_ref_new\n\n        # --- Update Finite-Precision Trajectory (Velocity-Verlet with rounding) ---\n        v_eps_half = v_eps + 0.5 * a_eps * dt\n        v_eps_half = round_to_precision(v_eps_half, eps_mach)\n        \n        r_eps = r_eps + v_eps_half * dt\n        r_eps = round_to_precision(r_eps, eps_mach)\n        \n        a_eps_new = calculate_accelerations(r_eps, masses, G, eps_soft_sq)\n        a_eps_new = round_to_precision(a_eps_new, eps_mach)\n        \n        v_eps = v_eps_half + 0.5 * a_eps_new * dt\n        v_eps = round_to_precision(v_eps, eps_mach)\n        \n        a_eps = a_eps_new\n        \n        # --- Check for Shadowing ---\n        current_time = (k + 1) * dt\n        separation = np.linalg.norm(r_eps - r_ref, axis=1)\n        max_separation = np.max(separation)\n        \n        if max_separation > D_thr:\n            return current_time\n            \n    return T_max\n\ndef solve():\n    # Define the parameters for the numerical experiment\n    N = 5\n    G = 1.0\n    masses = np.ones(N)\n    eps_soft = 1.0e-3\n    dt = 5.0e-3\n    T_max = 50.0\n    D_thr = 5.0e-2\n\n    # Define initial positions and velocities\n    r0 = np.array([\n        [-1.0, 0.0],\n        [1.0, 0.0],\n        [0.0, 0.9],\n        [0.0, -0.9],\n        [0.2, 0.2]\n    ])\n    v0 = np.array([\n        [0.0, 0.3],\n        [0.0, -0.3],\n        [-0.35, 0.0],\n        [0.35, 0.0],\n        [-0.15, 0.2]\n    ])\n    \n    # Define the test cases for machine epsilon\n    test_cases = [2**-10, 2**-15, 2**-20, 2**-25, 2**-30]\n\n    results = []\n    for eps_mach in test_cases:\n        shadowing_time = run_simulation(eps_mach, N, G, masses, eps_soft, dt, T_max, D_thr, r0, v0)\n        results.append(shadowing_time)\n\n    # Format the final output as specified\n    formatted_results = ','.join([f\"{res:.6f}\" for res in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "3163501"}, {"introduction": "A correct simulation is only the first step; an efficient one is often the goal. This final practice [@problem_id:3163576] shifts focus from the physical model to computational performance using the industry-standard Roofline model. By carefully analyzing the arithmetic operations and memory traffic of the core force calculation, you will learn to predict whether your code is bottlenecked by the processor's computational speed or by the system's memory bandwidth, a crucial skill for optimizing scientific codes.", "problem": "Consider the classical pairwise interaction kernel used in an $N$-body gravitational simulation. Starting from Newton's laws of motion and universal gravitation, the per-interaction computation between a particle $i$ and a particle $j$ is implemented in a standard algorithmic pattern that, for each pair $(i,j)$, performs the following arithmetic steps on floating-point scalars: subtract to form relative coordinates $dx$, $dy$, $dz$; square and sum to form $r^2$; add a small softening parameter; take a square root and a reciprocal to obtain $1/r$; multiply to obtain $1/r^3$; scale by $G$ and the source mass; and finally accumulate three components of acceleration for particle $i$. Assume each arithmetic operation type, including addition, subtraction, multiplication, division, and square root, counts as exactly one floating-point operation. Let the data layout be structure-of-arrays with positions stored as separate arrays and masses as a separate array.\n\nAssume the following data movement model with respect to main memory traffic for a single interaction: the target particle $i$'s position and accumulators are held in registers across the inner loop, and their memory traffic is amortized across many source particles $j$, hence neglected at the per-interaction level. For each source particle $j$, the algorithm loads $x_j$, $y_j$, $z_j$, and $m_j$ from main memory. If a software-managed cache or tiling scheme enables reuse of each loaded $j$ across multiple different $i$ particles, model this with a reuse factor $r$ meaning that, on average, each $j$'s four scalars are loaded from main memory once but used in $r$ distinct pairwise interactions before being evicted. Under this model, the effective main-memory bytes per interaction is the total bytes of those four scalars divided by $r$.\n\nUsing the Roofline performance model, you must classify whether the kernel is limited by peak floating-point throughput or by main-memory bandwidth on a given hardware, and predict the attainable floating-point rate. The relevant inputs per test case are: the floating-point precision in bytes per scalar, the reuse factor $r$, the hardware peak floating-point rate $P_{\\text{peak}}$ in gigafloating-point-operations per second, and the peak main-memory bandwidth $B_{\\text{peak}}$ in gigabytes per second. Your program must, for each test case, compute the arithmetic intensity $I$ in floating-point-operations per byte, the ridge point $R$ defined as the threshold separating bandwidth-bound and compute-bound regimes, the predicted attainable floating-point rate $P_{\\text{att}}$ in gigafloating-point-operations per second, and a regime flag defined as $0$ if bandwidth-bound and $1$ if compute-bound. Use the following principled bases only: the definitions of velocity and acceleration in classical mechanics, Newton's law of universal gravitation, the definition of floating-point operation counting as described above, the definition that arithmetic intensity is floating-point-operations per transferred byte, and the idea that a computation's attainable rate is jointly constrained by peak floating-point throughput and peak bandwidth mediated by arithmetic intensity.\n\nYou must not use any shortcut formulas beyond what can be derived from the above definitions. Carefully account for the number of floating-point operations per pairwise interaction implied by the algorithmic steps, and the number of bytes transferred per interaction implied by the memory model and reuse factor $r$.\n\nFor numerical evaluation, use the following test suite of six independent hardware-and-algorithm settings, where all quantities in gigafloating-point-operations per second and gigabytes per second are to be treated as real numbers:\n\n- Test case $1$: double precision with $8$ bytes per scalar, reuse factor $r = 1$, $P_{\\text{peak}} = 200$, $B_{\\text{peak}} = 50$.\n- Test case $2$: double precision with $8$ bytes per scalar, reuse factor $r = 1$, $P_{\\text{peak}} = 7000$, $B_{\\text{peak}} = 900$.\n- Test case $3$: double precision with $8$ bytes per scalar, reuse factor $r = 8$, $P_{\\text{peak}} = 200$, $B_{\\text{peak}} = 50$.\n- Test case $4$: single precision with $4$ bytes per scalar, reuse factor $r = 1$, $P_{\\text{peak}} = 14000$, $B_{\\text{peak}} = 900$.\n- Test case $5$: double precision with $8$ bytes per scalar, reuse factor $r = 32$, $P_{\\text{peak}} = 200$, $B_{\\text{peak}} = 50$.\n- Test case $6$: double precision with $8$ bytes per scalar, reuse factor $r = 1$, $P_{\\text{peak}} = 200$, $B_{\\text{peak}} = 304.7619047619$.\n\nAngle units are not applicable. No physical units are required. All floating-point outputs must be expressed as real numbers rounded to six decimal places. For each test case, output a list containing, in this order: arithmetic intensity $I$, ridge point $R$, predicted attainable rate $P_{\\text{att}}$, and the regime flag, with the regime flag equal to $1$ if and only if $I$ is greater than or equal to $R$. Your program should produce a single line of output containing the results as a comma-separated list of these per-test-case lists enclosed in a single pair of square brackets (for example, $[[\\dots],[\\dots],\\dots]$). The output must be fully determined by the test suite above and the principled bases stated, without any additional input.", "solution": "The problem requires an analysis of a standard gravitational $N$-body interaction kernel using the Roofline performance model. The analysis is predicated on a meticulous accounting of arithmetic operations and data movement from main memory, which are derived from first principles as outlined in the problem statement.\n\nThe solution proceeds in three stages:\n1.  Quantification of the arithmetic work (floating-point operations) per pairwise particle interaction.\n2.  Quantification of the main-memory data transfer per interaction.\n3.  Application of the Roofline model using these quantities to determine the performance characteristics.\n\n### 1. Arithmetic Workload Analysis (FLOPs per Interaction)\n\nThe number of floating-point operations (FLOPs) is determined by summing the operations described in the algorithmic pattern for a single interaction between a target particle $i$ and a source particle $j$. Each arithmetic operation (addition, subtraction, multiplication, division, square root) is counted as one FLOP.\n\nThe force on particle $i$ due to particle $j$ is given by Newton's law of universal gravitation, $\\vec{F}_{ij} \\propto m_i m_j \\vec{r}_{ij} / ||\\vec{r}_{ij}||^3$, where $\\vec{r}_{ij} = \\vec{r}_j - \\vec{r}_i$. The acceleration of particle $i$ is $\\vec{a}_i = \\vec{F}_{ij}/m_i$. The algorithm computes $\\vec{a}_i \\mathrel{+}= G m_j \\vec{r}_{ij} / ||\\vec{r}_{ij}||^3$. The steps are:\n\n1.  **Compute relative position vector**: $\\vec{\\Delta r} = (dx, dy, dz) = \\vec{r}_j - \\vec{r}_i$. This requires $3$ subtractions for the three components.\n    `FLOPs = 3`\n\n2.  **Compute squared distance**: $r^2 = dx^2 + dy^2 + dz^2$. This involves $3$ multiplications (squarings) and $2$ additions.\n    `FLOPs = 3 + 2 = 5`\n\n3.  **Apply softening**: $r_s^2 = r^2 + \\epsilon^2$, where $\\epsilon^2$ is the softening parameter. This is $1$ addition.\n    `FLOPs = 1`\n\n4.  **Compute inverse distance**: The algorithm computes $1/r_s$. This is described as \"take a square root and a reciprocal\".\n    - $r_s = \\sqrt{r_s^2}$: $1$ square root operation.\n    - $1/r_s$: $1$ division operation.\n    `FLOPs = 1 + 1 = 2`\n\n5.  **Compute inverse cubed distance**: To get $1/r_s^3$ from $1/r_s$, two multiplications are needed: $(1/r_s) \\cdot (1/r_s) \\to 1/r_s^2$, and $(1/r_s^2) \\cdot (1/r_s) \\to 1/r_s^3$.\n    `FLOPs = 2`\n\n6.  **Scale by physical constants and mass**: The term $1/r_s^3$ is scaled by the gravitational constant $G$ and the source mass $m_j$. This results in a scalar $s = G \\cdot m_j \\cdot (1/r_s^3)$. This requires $2$ multiplications.\n    `FLOPs = 2`\n\n7.  **Accumulate acceleration**: The acceleration vector of particle $i$, $\\vec{a}_i$, is updated: $\\vec{a}_i \\mathrel{+}= s \\cdot \\vec{\\Delta r}$. This involves scaling each component of $\\vec{\\Delta r}$ by $s$ and adding it to the corresponding component of $\\vec{a}_i$. This is $3$ multiplications and $3$ additions.\n    `FLOPs = 3 + 3 = 6`\n\nSumming the FLOP counts from all steps gives the total FLOPs per interaction, $F$:\n$$ F = 3 + 5 + 1 + 2 + 2 + 2 + 6 = 21 \\text{ FLOPs} $$\n\n### 2. Data Transfer Analysis (Bytes per Interaction)\n\nThe memory model specifies that for each source particle $j$, its position $(x_j, y_j, z_j)$ and mass $m_j$ are loaded from main memory. This constitutes a load of $4$ scalar values. The data for the target particle $i$ are assumed to reside in registers and do not contribute to main-memory traffic on a per-interaction basis.\n\nLet $S$ be the floating-point precision in bytes per scalar. The total bytes loaded from main memory for one source particle $j$ is $4S$.\n\nThe model includes a reuse factor, $r$, which signifies that the data for particle $j$ are used in $r$ distinct pairwise interactions before being evicted from a cache. Therefore, the memory-access cost is amortized over these $r$ uses. The effective number of bytes transferred from main memory per interaction, $M$, is:\n$$ M = \\frac{4S}{r} \\text{ bytes} $$\n\n### 3. Roofline Model Application\n\nThe Roofline model predicts the attainable performance, $P_{\\text{att}}$, based on the kernel's arithmetic intensity and the hardware's peak capabilities.\n\n-   **Arithmetic Intensity ($I$)**: This is the ratio of arithmetic work to data movement.\n    $$ I = \\frac{F}{M} = \\frac{21}{4S/r} = \\frac{21r}{4S} \\quad [\\text{FLOPs/byte}] $$\n\n-   **Hardware Parameters**: The hardware is characterized by its peak floating-point rate, $P_{\\text{peak}}$ (in GFLOPS), and its peak main-memory bandwidth, $B_{\\text{peak}}$ (in GB/s).\n\n-   **Ridge Point ($R$)**: This is the critical arithmetic intensity at which the performance limitation transitions from being memory-bound to compute-bound. It is the ratio of peak performance to peak bandwidth.\n    $$ R = \\frac{P_{\\text{peak}}}{B_{\\text{peak}}} \\quad [\\text{FLOPs/byte}] $$\n\n-   **Attainable Performance ($P_{\\text{att}}$)**: The performance is constrained by both the compute and memory ceilings. The attainable performance is the minimum of these two limits.\n    - Compute-limited performance: $P_{\\text{peak}}$\n    - Memory-limited performance: $I \\cdot B_{\\text{peak}}$\n    $$ P_{\\text{att}} = \\min(P_{\\text{peak}}, I \\cdot B_{\\text{peak}}) \\quad [\\text{GFLOPS}] $$\n\n-   **Regime Classification**: The kernel is classified as compute-bound if its arithmetic intensity is sufficient to saturate the floating-point units, and bandwidth-bound otherwise. The problem specifies the condition for the regime flag as:\n    - Compute-bound (flag = $1$) if $I \\geq R$.\n    - Bandwidth-bound (flag = $0$) if $I < R$.\n\nThese derived formulae are sufficient to calculate the required quantities for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the N-body Roofline model problem for a suite of test cases.\n    \"\"\"\n    \n    # Test cases defined as tuples of (precision_bytes, reuse_factor, P_peak, B_peak)\n    test_cases = [\n        # Test case 1: double precision, r=1, CPU-like\n        (8, 1, 200, 50),\n        # Test case 2: double precision, r=1, GPU-like\n        (8, 1, 7000, 900),\n        # Test case 3: double precision, r=8, CPU-like with tiling\n        (8, 8, 200, 50),\n        # Test case 4: single precision, r=1, GPU-like\n        (4, 1, 14000, 900),\n        # Test case 5: double precision, r=32, CPU-like with heavy tiling\n        (8, 32, 200, 50),\n        # Test case 6: double precision, r=1, knee-point case\n        (8, 1, 200, 304.7619047619),\n    ]\n\n    # Constant derived from the problem description: total FLOPs per interaction.\n    # 3(sub) + 5(r_sq) + 1(soften) + 2(inv_r) + 2(inv_r_cubed) + 2(scale_G_m) + 6(accum)\n    FLOPS_PER_INTERACTION = 21.0\n\n    results = []\n    \n    for case in test_cases:\n        s_bytes, r_reuse, p_peak, b_peak = case\n\n        # 1. Calculate Bytes per Interaction\n        # Load x_j, y_j, z_j, m_j (4 scalars) with reuse factor r.\n        bytes_per_interaction = (4.0 * s_bytes) / r_reuse\n\n        # 2. Calculate Arithmetic Intensity (I)\n        # I = FLOPs / Byte\n        arithmetic_intensity = FLOPS_PER_INTERACTION / bytes_per_interaction\n\n        # 3. Calculate Ridge Point (R)\n        # R = P_peak / B_peak\n        ridge_point = p_peak / b_peak\n        \n        # 4. Determine Regime and Attainable Performance (P_att)\n        # The performance is the minimum of the compute ceiling (P_peak)\n        # and the memory bandwidth ceiling (I * B_peak).\n        \n        # P_att = min(P_peak, I * B_peak)\n        attainable_rate = min(p_peak, arithmetic_intensity * b_peak)\n        \n        # Regime flag is 1 if compute-bound (I >= R), 0 if bandwidth-bound (I < R).\n        regime_flag = 1 if arithmetic_intensity >= ridge_point else 0\n        \n        # Store the results for this case.\n        # The problem requires rounding floats to six decimal places.\n        case_result = [\n            round(arithmetic_intensity, 6),\n            round(ridge_point, 6),\n            round(attainable_rate, 6),\n            regime_flag\n        ]\n        results.append(case_result)\n\n    # Format the final output string as a list of lists, without spaces.\n    inner_strs = []\n    for res_list in results:\n        # Format floats to 6 decimal places, int as is.\n        formatted_vals = [f\"{res_list[0]:.6f}\", f\"{res_list[1]:.6f}\", f\"{res_list[2]:.6f}\", str(res_list[3])]\n        inner_strs.append(f\"[{','.join(formatted_vals)}]\")\n    \n    final_output_str = f\"[{','.join(inner_strs)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "3163576"}]}