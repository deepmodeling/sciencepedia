{"hands_on_practices": [{"introduction": "In computational science, our view of a continuous process is often through discrete samples. This exercise explores aliasing, a fundamental pitfall where sampling too infrequently can create misleading patterns. By simulating a simple sine wave and analyzing its Autocorrelation Function (ACF) under different sampling rates [@problem_id:3098943], you will gain a concrete understanding of how improper data collection can lead to incorrect conclusions about a system's dynamics.", "problem": "You will implement a complete, runnable program that demonstrates how undersampling a fast oscillatory signal causes aliasing, which in turn produces a misleading Autocorrelation Function (ACF) suggesting slower dynamics. Work entirely in a purely mathematical and algorithmic setting.\n\nA continuous-time scalar signal is defined as $x(t) = \\sin\\!\\big(2\\pi f_0 t + \\varphi\\big)$, where $f_0$ is frequency in Hertz (cycles per second), and $\\varphi$ is a fixed phase in radians. This signal is sampled uniformly at sampling interval $\\Delta t$ to form the discrete sequence $x_n = x(n\\,\\Delta t)$ for integer $n \\in \\{0,1,\\dots,N-1\\}$. Define the sample mean $\\mu = \\frac{1}{N}\\sum_{n=0}^{N-1} x_n$. The normalized, finite-length, one-sided sample Autocorrelation Function (ACF) is defined for integer lag $k \\in \\{0,1,\\dots,N-1\\}$ by\n$$\nr[k] \\;=\\; \\frac{\\sum_{n=0}^{N-1-k}\\big(x_n - \\mu\\big)\\big(x_{n+k} - \\mu\\big)}{\\sum_{n=0}^{N-1}\\big(x_n - \\mu\\big)^2}.\n$$\nNote that $r[0] = 1$ by construction. For a purely sinusoidal input, the dominant time scale revealed by the ACF can be estimated as the time at the first nonzero-lag positive local maximum. In other words, define $k^\\star$ to be the smallest integer $k \\ge 1$ such that $r[k] > 0$ and $r[k] \\ge r[k-1]$ and $r[k] \\ge r[k+1]$. Then define the estimated dominant time scale\n$$\n\\tau_{\\mathrm{est}} \\;=\\; k^\\star \\,\\Delta t \\quad \\text{(in seconds)}.\n$$\n\nYour task is to write a program that, for each parameter set in the test suite below, constructs $x_n$, computes $r[k]$ for $k \\ge 0$, finds $k^\\star$ as described, and returns $\\tau_{\\mathrm{est}}$ in seconds. The purpose is to observe that when the sampling frequency $f_s = 1/\\Delta t$ is too low relative to $f_0$, aliasing occurs and the ACF can indicate a slower apparent dynamic, that is, a larger $\\tau_{\\mathrm{est}}$ than the true period $T_0 = 1/f_0$.\n\nFundamental base you may rely on: the definition of uniform sampling $x_n = x(n\\,\\Delta t)$, the definition of the normalized sample ACF given above, and the widely tested fact that if the sampling frequency $f_s = 1/\\Delta t$ does not exceed twice the highest signal frequency, the sampled discrete-time sinusoid encodes an aliased frequency in the interval $[0, f_s/2]$, which the ACF reflects.\n\nTest suite (each case specifies $(f_0, \\Delta t, N, \\varphi)$):\n- Case $1$ (well-sampled, happy path): $f_0 = 3.0$ Hz, $\\Delta t = 0.01$ s, $N = 1024$, $\\varphi = 0.3$ rad.\n- Case $2$ (undersampled, aliasing to slower apparent dynamics): $f_0 = 45.0$ Hz, $\\Delta t = 0.02$ s, $N = 1024$, $\\varphi = 0.2$ rad.\n- Case $3$ (Nyquist boundary behavior with phase avoiding degeneracy): $f_0 = 25.0$ Hz, $\\Delta t = 0.02$ s, $N = 1024$, $\\varphi = \\pi/2$ rad.\n\nRequirements for the program:\n- For each case, generate $x_n$ exactly as $x_n = \\sin\\!\\big(2\\pi f_0 n \\Delta t + \\varphi\\big)$ for $n \\in \\{0,\\dots,N-1\\}$.\n- Compute $r[k]$ exactly by the definition above for $k \\in \\{0,\\dots,N-1\\}$.\n- Determine $k^\\star$ as the smallest $k \\ge 1$ with $r[k] > 0$ and $r[k] \\ge r[k-1]$ and $r[k] \\ge r[k+1]$; if no such $k$ exists, use the $k \\ge 1$ that maximizes $r[k]$.\n- Return $\\tau_{\\mathrm{est}} = k^\\star \\Delta t$ for each case, expressed in seconds, rounded to $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing a comma-separated list of the three $\\tau_{\\mathrm{est}}$ values enclosed in square brackets and with no spaces (for example, `[0.123456,0.234567,0.345678]`).", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. All necessary parameters and definitions for a unique solution are provided. The task is to implement an algorithm that demonstrates the effect of aliasing on the autocorrelation function of a sampled sinusoidal signal.\n\nThe solution proceeds in four main steps for each test case:\n1.  Generation of the discrete-time signal $x_n$.\n2.  Computation of the normalized sample Autocorrelation Function (ACF), $r[k]$.\n3.  Determination of the characteristic lag, $k^\\star$.\n4.  Calculation of the estimated dominant time scale, $\\tau_{\\mathrm{est}}$.\n\nLet us consider a given test case with parameters $(f_0, \\Delta t, N, \\varphi)$.\n\n**Step 1: Signal Generation**\nThe continuous-time signal is given by $x(t) = \\sin(2\\pi f_0 t + \\varphi)$. We sample this signal at a uniform interval $\\Delta t$ to generate a discrete-time sequence $x_n$ of length $N$. The time instances are $t_n = n \\Delta t$ for $n \\in \\{0, 1, \\dots, N-1\\}$. The discrete signal is thus:\n$$\nx_n = x(t_n) = \\sin(2\\pi f_0 n \\Delta t + \\varphi)\n$$\nThis sequence is generated for all $n$ from $0$ to $N-1$.\n\n**Step 2: Autocorrelation Function (ACF) Calculation**\nThe problem provides a specific formula for the normalized sample ACF, $r[k]$. The calculation requires several sub-steps.\n\nFirst, we compute the sample mean $\\mu$ of the signal $x_n$:\n$$\n\\mu = \\frac{1}{N} \\sum_{n=0}^{N-1} x_n\n$$\nNext, we center the signal by subtracting the mean: $x'_n = x_n - \\mu$.\n\nThe denominator of the ACF is the total sum of squares of the centered signal, which corresponds to the autocovariance at lag $k=0$ and serves as the normalization factor.\n$$\nS = \\sum_{n=0}^{N-1} (x_n - \\mu)^2 = \\sum_{n=0}^{N-1} (x'_n)^2\n$$\nThe numerator is the unnormalized autocovariance at a given integer lag $k$, where $k \\in \\{0, 1, \\dots, N-1\\}$:\n$$\nC[k] = \\sum_{n=0}^{N-1-k} (x_n - \\mu)(x_{n+k} - \\mu) = \\sum_{n=0}^{N-1-k} x'_n x'_{n+k}\n$$\nFinally, the normalized sample ACF, $r[k]$, is the ratio of the autocovariance at lag $k$ to the normalization factor $S$:\n$$\nr[k] = \\frac{C[k]}{S} = \\frac{\\sum_{n=0}^{N-1-k} (x_n - \\mu)(x_{n+k} - \\mu)}{\\sum_{n=0}^{N-1} (x_n - \\mu)^2}\n$$\nNote that for $k=0$, the numerator becomes $\\sum_{n=0}^{N-1} (x_n - \\mu)^2$, which is identical to the denominator $S$, ensuring $r[0] = 1$ as stated. This entire function $r[k]$ is computed for $k \\in \\{0, 1, \\dots, N-1\\}$.\n\n**Step 3: Determination of the Characteristic Lag ($k^\\star$)**\nThe characteristic lag $k^\\star$ is used to estimate the dominant period in the signal as revealed by the ACF. It is defined as the smallest integer lag $k \\ge 1$ that represents a positive local maximum. The conditions are:\n1.  $k \\ge 1$\n2.  $r[k] > 0$\n3.  $r[k] \\ge r[k-1]$\n4.  $r[k] \\ge r[k+1]$\n\nTo find $k^\\star$, we iterate through $k$ starting from $1$ up to $N-2$ (to ensure $r[k+1]$ is accessible). The first value of $k$ that satisfies all three conditions is chosen as $k^\\star$.\n\nIf this search completes without finding such a $k$, a fallback rule is applied: $k^\\star$ is chosen as the lag $k \\ge 1$ that maximizes the value of $r[k]$. This is found by searching for the maximum value in the subarray $r[1], r[2], \\dots, r[N-1]$.\n\n**Step 4: Calculation of the Estimated Time Scale ($\\tau_{\\mathrm{est}}$)**\nOnce $k^\\star$ has been determined, the estimated dominant time scale $\\tau_{\\mathrm{est}}$ is calculated by converting the lag from sample units back to time units (seconds):\n$$\n\\tau_{\\mathrm{est}} = k^\\star \\cdot \\Delta t\n$$\nThis value is computed for each test case.\n\n**Conceptual Analysis of Test Cases:**\n- **Case 1 (Well-sampled):** $f_0 = 3.0$ Hz, $\\Delta t = 0.01$ s. The sampling frequency is $f_s = 1/\\Delta t = 100$ Hz. The Nyquist frequency is $f_s/2 = 50$ Hz. Since $f_0 \\ll f_s/2$, the signal is well-sampled. The true period is $T_0 = 1/f_0 \\approx 0.3333$ s. We expect $\\tau_{\\mathrm{est}} \\approx T_0$.\n- **Case 2 (Undersampled):** $f_0 = 45.0$ Hz, $\\Delta t = 0.02$ s. The sampling frequency is $f_s = 1/\\Delta t = 50$ Hz. The Nyquist frequency is $f_s/2 = 25$ Hz. Since $f_0 > f_s/2$, aliasing will occur. The aliased frequency is $f_a = |f_0 - f_s| = |45 - 50| = 5.0$ Hz. The apparent period is $T_a = 1/f_a = 0.2$ s. We expect the ACF to reflect this aliased dynamic, so $\\tau_{\\mathrm{est}} \\approx T_a$.\n- **Case 3 (Nyquist boundary):** $f_0 = 25.0$ Hz, $\\Delta t = 0.02$ s. Here, $f_0$ is exactly the Nyquist frequency. The sampled signal will be $x_n = \\sin(2\\pi \\cdot 25 \\cdot n \\cdot 0.02 + \\pi/2) = \\sin(n\\pi + \\pi/2) = \\cos(n\\pi)$, which results in the sequence $\\{1, -1, 1, -1, \\dots\\}$. The signal is perfectly anticorrelated at lag $k=1$ and perfectly correlated at lag $k=2$. Thus, the first positive peak in the ACF should occur at $k^\\star=2$, leading to $\\tau_{\\mathrm{est}} = 2 \\cdot 0.02 = 0.04$ s, corresponding to an apparent frequency of $1/0.04 = 25$ Hz.\n\nThe implementation will follow these steps precisely.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the estimated dominant time scale from the\n    autocorrelation function of a sampled sinusoidal signal for three test cases,\n    demonstrating the effect of aliasing.\n    \"\"\"\n    # Test suite (f0, dt, N, phi)\n    test_cases = [\n        (3.0, 0.01, 1024, 0.3),         # Case 1: Well-sampled\n        (45.0, 0.02, 1024, 0.2),        # Case 2: Undersampled (aliasing)\n        (25.0, 0.02, 1024, np.pi/2),   # Case 3: Nyquist boundary\n    ]\n\n    results = []\n    \n    for f0, dt, N, phi in test_cases:\n        # Step 1: Generate the discrete-time signal\n        n = np.arange(N)\n        t = n * dt\n        x = np.sin(2 * np.pi * f0 * t + phi)\n\n        # Step 2: Compute the Autocorrelation Function (ACF)\n        \n        # Center the signal by subtracting its mean\n        mu = np.mean(x)\n        x_centered = x - mu\n        \n        # The denominator is the sum of squared deviations, which is also\n        # the unnormalized autocorrelation at lag 0.\n        # Efficiently computed using np.dot or np.sum(x_centered**2)\n        denominator = np.dot(x_centered, x_centered)\n        \n        # Handle the edge case where the denominator is zero (e.g., a constant signal)\n        if denominator == 0:\n            # For a constant signal, ACF is ill-defined. We can set r to zeros.\n            # However, for a sine wave, this is not expected.\n            r = np.zeros(N)\n            r[0] = 1.0\n        else:\n            # The numerator is the autocovariance at each lag k.\n            # np.correlate computes C[k] = sum_n(a[n+k] * v[n]).\n            # Using mode 'full' provides all lags. The second half corresponds\n            # to non-negative lags.\n            unnormalized_acf = np.correlate(x_centered, x_centered, mode='full')\n            \n            # The unnormalized ACF at non-negative lags k=0,1,...,N-1\n            # is the second half of the full correlation result.\n            autocovariance = unnormalized_acf[N - 1:]\n            \n            # Normalize to get the ACF r[k]\n            r = autocovariance / denominator\n\n        # Step 3: Determine the characteristic lag k_star\n        k_star = -1  # Sentinel value indicating not found\n\n        # Search for the smallest k >= 1 that is a positive local maximum.\n        # Loop up to N-2 to allow checking r[k+1].\n        for k in range(1, N - 1):\n            is_positive = r[k] > 0\n            is_local_max = (r[k] >= r[k - 1]) and (r[k] >= r[k + 1])\n            \n            if is_positive and is_local_max:\n                k_star = k\n                break  # Found the smallest k, so we can stop.\n\n        # If no such k was found, apply the fallback rule.\n        if k_star == -1:\n            # Find the k >= 1 that globally maximizes r[k].\n            # We search in r[1:] because k must be >= 1.\n            # np.argmax returns the index relative to the sliced array r[1:].\n            # We add 1 to get the index relative to the original array r.\n            k_star = np.argmax(r[1:]) + 1\n\n        # Step 4: Calculate the estimated dominant time scale\n        tau_est = k_star * dt\n        results.append(tau_est)\n\n    # Final print statement in the exact required format.\n    # We use a format specifier to ensure 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3098943"}, {"introduction": "Many simulated systems, from climate models to economic forecasts, exhibit predictable cyclical patterns or \"seasonality.\" While this structure is often a key feature, it can also induce strong autocorrelation that obscures the underlying random fluctuations we may wish to study. This practice demonstrates how to identify such spurious correlation and then remove it using seasonal differencing [@problem_id:3098918], a foundational technique in time series analysis.", "problem": "You are to implement a complete, runnable program that demonstrates how unremoved seasonality can induce misleading dependence in simulation data and how seasonal differencing mitigates such spurious short-lag dependence. Work strictly from core definitions. Consider a discrete-time series $\\{X_t\\}_{t=0}^{N-1}$ constructed as the sum of a seasonal deterministic component and additive noise. The seasonal component is a sinusoid with a known period, and the additive noise is independent and identically distributed Gaussian noise. Use the sine function with angles in radians.\n\nFundamental base and definitions:\n- A discrete-time series is a sequence $\\{X_t\\}$ indexed by integer time $t$.\n- The population autocovariance at lag $k$ is $\\gamma(k) = \\mathrm{Cov}(X_t, X_{t+k})$ when this is time-invariant and well-defined.\n- The sample mean is $\\bar{X} = \\frac{1}{N}\\sum_{t=0}^{N-1}X_t$.\n- The unbiased sample autocovariance estimator at lag $k$ is\n$$\\hat{\\gamma}(k) = \\frac{1}{N-k} \\sum_{t=0}^{N-1-k} \\left(X_t - \\bar{X}\\right)\\left(X_{t+k} - \\bar{X}\\right), \\quad 0 \\le k \\le N-1.$$\n- The sample autocorrelation function (ACF) at lag $k$ is\n$$\\hat{\\rho}(k) = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)}.$$\n\nSeasonal model and differencing:\n- The simulated series is\n$$X_t = A \\sin\\!\\left(\\frac{2\\pi t}{s}\\right) + \\varepsilon_t,$$\nwhere $A$ is amplitude, $s$ is the seasonal period measured in time steps, and $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ are independent across $t$.\n- Seasonal differencing at period $d$ produces\n$$Y_t = X_t - X_{t-d}, \\quad t = d, d+1, \\dots, N-1.$$\n\nDetection criterion:\n- For each simulation, compute $\\hat{\\rho}(1)$ for the raw series $\\{X_t\\}$ and for the differenced series $\\{Y_t\\}$.\n- Use the large-sample Gaussian approximation for the ACF under white noise to define a two-sided $95\\%$ confidence bound as\n$$b_N = \\frac{1.96}{\\sqrt{N}},$$\nwhere $N$ is the sample size of the series for which the ACF is computed. Treat $\\left|\\hat{\\rho}(1)\\right| > b_N$ as “statistically significant” dependence at lag $1$; otherwise treat it as not significant.\n- Declare that “spurious short-lag autocorrelation due to seasonality is removed” if and only if $\\left|\\hat{\\rho}_{X}(1)\\right| > b_{N_X}$ and $\\left|\\hat{\\rho}_{Y}(1)\\right| < b_{N_Y}$, where $N_X = N$ is the length of $\\{X_t\\}$ and $N_Y = N - d$ is the length of $\\{Y_t\\}$.\n\nTasks to implement:\n1. For each test case, generate $\\{X_t\\}$ using the model above with the specified parameters and a fixed random seed for reproducibility, then compute $\\hat{\\rho}(1)$ for $\\{X_t\\}$.\n2. Apply seasonal differencing with the specified $d$ to obtain $\\{Y_t\\}$, then compute $\\hat{\\rho}(1)$ for $\\{Y_t\\}$.\n3. Compute the bounds $b_{N_X}$ and $b_{N_Y}$.\n4. For each test case, output a boolean indicating whether the differencing removed spurious short-lag autocorrelation according to the detection criterion.\n\nAngle unit:\n- All angles in the sine function are in radians.\n\nTest suite:\n- Each tuple is $(N, s, A, \\sigma, d, \\text{seed})$.\n- Use the following parameter sets:\n  - Case $1$ (general “happy path”): $(720, 24, 2.0, 0.5, 24, 2023)$.\n  - Case $2$ (wrong differencing period): $(720, 24, 2.0, 0.5, 12, 2023)$.\n  - Case $3$ (no seasonality; boundary against false detection): $(720, 24, 0.0, 1.0, 24, 42)$.\n  - Case $4$ (small sample near the seasonal scale): $(36, 24, 1.5, 0.1, 24, 7)$.\n  - Case $5$ (seasonality weak relative to noise): $(720, 24, 0.2, 1.0, 24, 123)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the boolean results for the five cases as a comma-separated list enclosed in square brackets, for example, `[True,False,True,True,False]`.\n- The program must be self-contained, require no user input, and use the specified runtime environment.", "solution": "The problem requires the implementation of a computational experiment to demonstrate how seasonal differencing can remove spurious short-lag autocorrelation induced by a deterministic seasonal component in a time series. The solution involves generating time series data, computing sample autocorrelations, applying seasonal differencing, and evaluating the outcome against a specified statistical criterion.\n\nThe procedural steps are as follows:\n\n1.  **Time Series Generation**: For each test case, a discrete-time series $\\{X_t\\}_{t=0}^{N-1}$ of length $N$ is synthesized according to the model:\n    $$X_t = S_t + \\varepsilon_t = A \\sin\\left(\\frac{2\\pi t}{s}\\right) + \\varepsilon_t$$\n    Here, $S_t = A \\sin(\\frac{2\\pi t}{s})$ represents a deterministic seasonal component with amplitude $A$ and period $s$. The term $\\varepsilon_t$ represents additive noise, which is modeled as independent and identically distributed (i.i.d.) random variables from a normal distribution with mean $0$ and variance $\\sigma^2$, denoted $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$. The sine function's argument is in radians. A fixed random seed is used for each case to ensure reproducibility of the noise component $\\varepsilon_t$.\n\n2.  **Sample Autocorrelation Function (ACF) Calculation**: The core of the analysis is the sample autocorrelation function (ACF), $\\hat{\\rho}(k)$, which estimates the correlation between observations at lag $k$. The problem specifies the following definitions, which must be implemented precisely. First, the sample mean $\\bar{X}$ is computed:\n    $$\\bar{X} = \\frac{1}{N}\\sum_{t=0}^{N-1}X_t$$\n    Next, the unbiased sample autocovariance estimator at lag $k$, $\\hat{\\gamma}(k)$, is calculated. For the required lag $k=1$ and the variance at lag $k=0$, the formulas are:\n    $$\\hat{\\gamma}(1) = \\frac{1}{N-1} \\sum_{t=0}^{N-2} \\left(X_t - \\bar{X}\\right)\\left(X_{t+1} - \\bar{X}\\right)$$\n    $$\\hat{\\gamma}(0) = \\frac{1}{N} \\sum_{t=0}^{N-1} \\left(X_t - \\bar{X}\\right)^2$$\n    Note that $\\hat{\\gamma}(0)$ is the (biased) sample variance. The sample autocorrelation at lag $k=1$, denoted $\\hat{\\rho}_X(1)$, is then the ratio:\n    $$\\hat{\\rho}_X(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)}$$\n    This procedure is applied to the original series $\\{X_t\\}$.\n\n3.  **Seasonal Differencing**: To remove the seasonality, seasonal differencing is applied with a specified differencing period $d$. This creates a new time series $\\{Y_t\\}$:\n    $$Y_t = X_t - X_{t-d}, \\quad \\text{for } t = d, d+1, \\dots, N-1$$\n    The resulting differenced series $\\{Y_t\\}$ has a shorter length of $N_Y = N - d$. When the differencing period $d$ matches the true seasonal period $s$, the deterministic sinusoidal component is analytically removed:\n    $$S_t - S_{t-s} = A \\sin\\left(\\frac{2\\pi t}{s}\\right) - A \\sin\\left(\\frac{2\\pi (t-s)}{s}\\right) = A \\sin\\left(\\frac{2\\pi t}{s}\\right) - A \\sin\\left(\\frac{2\\pi t}{s} - 2\\pi\\right) = 0$$\n    The differenced series then becomes $Y_t = \\varepsilon_t - \\varepsilon_{t-s}$. This is a moving-average process whose true lag-$1$ autocorrelation is zero for $s > 1$. The sample ACF of $\\{Y_t\\}$, $\\hat{\\rho}_Y(1)$, is computed using the same formulas as for $\\{X_t\\}$, but applied to the data $\\{Y_t\\}$ with its corresponding length $N_Y$.\n\n4.  **Detection Criterion**: The effectiveness of seasonal differencing is evaluated using a statistical criterion. For a time series of length $M$ consisting of pure white noise, the sample ACF values $\\hat{\\rho}(k)$ for $k>0$ are approximately normally distributed with mean $0$ and variance $1/M$. This gives rise to a two-sided $95\\%$ confidence bound:\n    $$b_M = \\frac{1.96}{\\sqrt{M}}$$\n    An observed ACF value $|\\hat{\\rho}(1)| > b_M$ is deemed \"statistically significant.\" The problem defines that \"spurious short-lag autocorrelation due to seasonality is removed\" if and only if both of the following conditions are met:\n    1.  The original series shows significant lag-$1$ autocorrelation: $|\\hat{\\rho}_X(1)| > b_{N_X}$, where $N_X = N$.\n    2.  The differenced series does not show significant lag-$1$ autocorrelation: $|\\hat{\\rho}_Y(1)| < b_{N_Y}$, where $N_Y = N - d$.\n\n    For each test case, we compute $\\hat{\\rho}_X(1)$ and $\\hat{\\rho}_Y(1)$, their respective bounds $b_{N_X}$ and $b_{N_Y}$, and evaluate this two-part logical condition to produce a final boolean result. The choice of test cases examines scenarios including correct differencing ($d=s$), incorrect differencing ($d \\neq s$), absence of seasonality ($A=0$), small sample size, and a low signal-to-noise ratio.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_rho1(series: np.ndarray) -> float:\n    \"\"\"\n    Calculates the sample autocorrelation at lag 1 based on the problem's definitions.\n\n    Args:\n        series: A 1D numpy array representing the time series.\n\n    Returns:\n        The sample autocorrelation at lag 1.\n    \"\"\"\n    N = len(series)\n    if N <= 1:\n        # Not enough data to compute lag-1 ACF.\n        return 0.0\n\n    mean_val = np.mean(series)\n    demeaned_series = series - mean_val\n\n    # Unbiased sample autocovariance at lag k=0, per problem definition:\n    # gamma(0) = (1/N) * sum((X_t - X_bar)^2), which is the biased sample variance.\n    gamma_0 = np.var(series)\n\n    # Unbiased sample autocovariance at lag k=1, per problem definition:\n    # gamma(1) = (1/(N-1)) * sum((X_t - X_bar)*(X_{t+1} - X_bar))\n    # The sum is over t from 0 to N-2.\n    sum_of_products = np.sum(demeaned_series[:-1] * demeaned_series[1:])\n    gamma_1 = sum_of_products / (N - 1)\n\n    if gamma_0 == 0.0:\n        # If the series has zero variance, correlation is undefined or 0.\n        return 0.0\n\n    # Sample autocorrelation at lag 1\n    rho_1 = gamma_1 / gamma_0\n    \n    return rho_1\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and validation for all test cases.\n    \"\"\"\n    # Test suite: Each tuple is (N, s, A, sigma, d, seed)\n    test_cases = [\n        # Case 1 (general “happy path”): (720, 24, 2.0, 0.5, 24, 2023)\n        (720, 24, 2.0, 0.5, 24, 2023),\n        # Case 2 (wrong differencing period): (720, 24, 2.0, 0.5, 12, 2023)\n        (720, 24, 2.0, 0.5, 12, 2023),\n        # Case 3 (no seasonality; boundary against false detection): (720, 24, 0.0, 1.0, 24, 42)\n        (720, 24, 0.0, 1.0, 24, 42),\n        # Case 4 (small sample near the seasonal scale): (36, 24, 1.5, 0.1, 24, 7)\n        (36, 24, 1.5, 0.1, 24, 7),\n        # Case 5 (seasonality weak relative to noise): (720, 24, 0.2, 1.0, 24, 123)\n        (720, 24, 0.2, 1.0, 24, 123),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, s, A, sigma, d, seed = case\n\n        # 1. Generate the time series {X_t}\n        np.random.seed(seed)\n        t = np.arange(N)\n        seasonal_component = A * np.sin(2 * np.pi * t / s)\n        noise_component = np.random.normal(loc=0.0, scale=sigma, size=N)\n        X = seasonal_component + noise_component\n        \n        # Compute ACF for {X_t}\n        rho_X_1 = calculate_rho1(X)\n        N_X = N\n        \n        # 2. Apply seasonal differencing to get {Y_t}\n        Y = X[d:] - X[:-d]\n        \n        # Compute ACF for {Y_t}\n        rho_Y_1 = calculate_rho1(Y)\n        N_Y = N - d\n\n        # 3. Compute the confidence bounds\n        b_N_X = 1.96 / np.sqrt(N_X)\n        b_N_Y = 1.96 / np.sqrt(N_Y)\n\n        # 4. Apply the detection criterion\n        is_X_autocorrelated = np.abs(rho_X_1) > b_N_X\n        is_Y_not_autocorrelated = np.abs(rho_Y_1) < b_N_Y\n        \n        is_removed = is_X_autocorrelated and is_Y_not_autocorrelated\n        results.append(is_removed)\n\n    # Format the final output as a comma-separated list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3098918"}, {"introduction": "After applying techniques like differencing or regression to detrend a time series, how can we be confident that no significant autocorrelation remains in the residuals? This exercise introduces the Ljung-Box test, a powerful statistical tool for formally testing the \"whiteness\" of a data series. By applying this test to the residuals of a simulated queuing system [@problem_id:3098986], you will learn to move beyond visual inspection and quantitatively assess whether your model has adequately captured the temporal dependencies in the data.", "problem": "You are given a scenario in which a developer has simulated the throughput of a single-server queuing system over discrete time steps. The observed throughput at time index $t$ is denoted by $y_t$ for $t = 1, 2, \\dots, n$. The simulator constructs $y_t$ as a count generated from a time-varying rate with a deterministic trend and a stochastic component that may exhibit autocorrelation. Your task is to test whether, after removing a linear trend from $y_t$, the residuals still exhibit autocorrelation, using the Ljung–Box test.\n\nStarting from the core definitions of autocovariance and autocorrelation in time series and the properties of white noise, derive the test statistic that aggregates sample autocorrelations across multiple lags to assess whether residuals behave as white noise. The null hypothesis is that the residuals are white noise up to a specified maximum lag. Use the distributional result under the null hypothesis to compute a $p$-value, and decide whether or not to reject the null at a given significance level.\n\nThe simulator produces $y_t$ via the following process:\n1. A latent Autoregressive (AR) model of order one, defined by $a_t = \\phi a_{t-1} + \\eta_t$, with $a_0 = 0$, and innovations $\\eta_t \\sim \\mathcal{N}(0, \\sigma^2)$ independently across $t$.\n2. A time-varying rate $\\lambda_t = \\lambda_0 \\left(1 + s \\frac{t}{n}\\right) \\exp(a_t)$.\n3. Observed throughput $y_t$ is generated as a Poisson count with rate $\\lambda_t$.\n\nDetrending is performed by fitting an ordinary least squares (OLS) linear model $y_t \\approx \\beta_0 + \\beta_1 t$ over $t = 1, 2, \\dots, n$, and forming residuals $r_t = y_t - (\\hat{\\beta}_0 + \\hat{\\beta}_1 t)$.\n\nCompute the sample autocorrelation $\\hat{\\rho}_k$ of the residuals at lag $k$ for each $k = 1, 2, \\dots, h$, compute the test statistic that aggregates these $\\hat{\\rho}_k$, obtain the $p$-value from its null distribution, and decide whether residual autocorrelation is present based on whether the $p$-value is less than a specified significance level $\\alpha$.\n\nYour program must implement the above procedure and produce results for the following test suite. Each test case is specified by the tuple $(n, \\lambda_0, s, \\phi, \\sigma, h, \\alpha)$:\n- Test case $1$: $(n = 200, \\lambda_0 = 10, s = 0.5, \\phi = 0, \\sigma = 0.2, h = 20, \\alpha = 0.05)$.\n- Test case $2$: $(n = 200, \\lambda_0 = 10, s = 0.5, \\phi = 0.7, \\sigma = 0.2, h = 20, \\alpha = 0.05)$.\n- Test case $3$: $(n = 120, \\lambda_0 = 6, s = 0, \\phi = 0.9, \\sigma = 0.25, h = 15, \\alpha = 0.01)$.\n- Test case $4$: $(n = 40, \\lambda_0 = 8, s = 0.4, \\phi = 0, \\sigma = 0.5, h = 10, \\alpha = 0.05)$.\n\nFor each test case, your program should output a boolean indicating whether the null hypothesis of white noise residuals is rejected at the level $\\alpha$, that is, output $\\text{True}$ if residual autocorrelation is detected and $\\text{False}$ otherwise. Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, for example, `[False,True,True,False]`. No physical units are involved in this problem, and all angles, if any were present, would be in radians, but none are required here. Express all numerical results as booleans as specified.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of time series analysis and statistical hypothesis testing, is well-posed with a clear objective and a complete set of parameters, and contains no internal contradictions or ambiguities.\n\nThe task is to determine if residual autocorrelation exists in a simulated time series after removing a linear trend. This involves a three-step process: data simulation, detrending via Ordinary Least Squares (OLS), and hypothesis testing using the Ljung-Box test.\n\n**1. Data Generation Process**\n\nThe observed data, $y_t$, represents a count process simulated over $t = 1, 2, \\dots, n$. The value of $y_t$ is a random variable drawn from a Poisson distribution with a time-varying rate $\\lambda_t$:\n$$ y_t \\sim \\text{Poisson}(\\lambda_t) $$\nThe rate $\\lambda_t$ is constructed to include both a deterministic trend and a stochastic, potentially autocorrelated component:\n$$ \\lambda_t = \\lambda_0 \\left(1 + s \\frac{t}{n}\\right) \\exp(a_t) $$\nHere, $\\lambda_0$ is a baseline rate, the term $(1 + s \\frac{t}{n})$ introduces a linear trend over the time interval scaled by parameter $s$, and $\\exp(a_t)$ introduces stochastic fluctuations. The stochastic term $a_t$ is generated by a first-order autoregressive, AR($1$), process:\n$$ a_t = \\phi a_{t-1} + \\eta_t $$\nwith initial condition $a_0 = 0$ and innovations $\\eta_t$ being independent and identically distributed draws from a normal distribution with mean $0$ and variance $\\sigma^2$, denoted $\\eta_t \\sim \\mathcal{N}(0, \\sigma^2)$. The parameter $\\phi$ is the autocorrelation coefficient of the AR($1$) process. If $\\phi=0$, $a_t = \\eta_t$ is a white noise process. If $\\phi \\neq 0$, the sequence $\\{a_t\\}$ is serially correlated, which in turn induces autocorrelation in the observed series $\\{y_t\\}$.\n\n**2. Detrending and Residuals**\n\nThe problem requires removing a linear trend from the observed data $\\{y_t\\}$. This is accomplished by fitting a simple linear regression model using Ordinary Least Squares (OLS):\n$$ y_t = \\beta_0 + \\beta_1 t + \\epsilon_t $$\nThe OLS method finds the estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the sum of squared errors. The fitted values are $\\hat{y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 t$. The residuals, which represent the data after the linear trend has been removed, are then calculated as:\n$$ r_t = y_t - \\hat{y}_t = y_t - (\\hat{\\beta}_0 + \\hat{\\beta}_1 t) $$\nThe core question is whether this residual series $\\{r_t\\}$ behaves like white noise.\n\n**3. The Ljung-Box Test for Autocorrelation**\n\nThe Ljung-Box test is used to test the null hypothesis ($H_0$) that a series of observations is independently distributed (i.e., is white noise). The alternative hypothesis ($H_1$) is that the observations are not independently distributed and exhibit serial correlation.\n\nThe test begins by calculating the sample autocorrelations of the residual series $\\{r_t\\}$ for a specified number of lags, $k = 1, 2, \\dots, h$. The sample autocorrelation at lag $k$ is defined as:\n$$ \\hat{\\rho}_k = \\frac{\\sum_{t=k+1}^n (r_t - \\bar{r})(r_{t-k} - \\bar{r})}{\\sum_{t=1}^n (r_t - \\bar{r})^2} $$\nwhere $n$ is the number of residuals and $\\bar{r}$ is the sample mean of the residuals. A property of OLS with an intercept term is that the sum of residuals is zero, so $\\bar{r} = 0$, simplifying the calculation.\n\nThe Ljung-Box test statistic, $Q$, aggregates the squared sample autocorrelations across the first $h$ lags:\n$$ Q = n(n+2) \\sum_{k=1}^h \\frac{\\hat{\\rho}_k^2}{n-k} $$\nThis statistic provides a single measure of the overall autocorrelation up to lag $h$. The term $(n-k)$ in the denominator is a finite-sample correction over the earlier Box-Pierce statistic.\n\nUnder the null hypothesis that the residuals $\\{r_t\\}$ are white noise, the $Q$ statistic is approximately distributed as a chi-squared ($\\chi^2$) random variable. The degrees of freedom ($df$) of this distribution is $h$. While adjustments to the degrees of freedom are necessary when testing residuals from fitted ARMA models (by subtracting the number of fitted ARMA parameters), such adjustments are generally not applied for residuals from a regression on deterministic variables like a time trend. Therefore, we use $df = h$.\n\nThe decision to reject or fail to reject the null hypothesis is based on the $p$-value. The $p$-value is the probability of observing a $Q$ statistic at least as large as the one calculated, assuming $H_0$ is true:\n$$ p\\text{-value} = P(\\chi^2_h > Q_{obs}) $$\nwhere $Q_{obs}$ is the value of the statistic computed from the data. If the $p$-value is less than the predetermined significance level $\\alpha$, we reject the null hypothesis and conclude that the residuals exhibit significant serial correlation. If $p\\text{-value} \\ge \\alpha$, we fail to reject $H_0$, meaning there is insufficient evidence to conclude that the residuals are not white noise. The program will output $\\text{True}$ if $H_0$ is rejected and $\\text{False}$ otherwise.\n\nThis complete procedure is implemented for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Simulates time series data, detrends it, and performs the Ljung-Box test\n    for autocorrelation on the residuals.\n    \"\"\"\n    # Set a random seed for reproducibility of the stochastic simulation.\n    np.random.seed(0)\n    \n    # Test cases defined as (n, lambda_0, s, phi, sigma, h, alpha)\n    test_cases = [\n        (200, 10, 0.5, 0.0, 0.2, 20, 0.05),\n        (200, 10, 0.5, 0.7, 0.2, 20, 0.05),\n        (120, 6, 0.0, 0.9, 0.25, 15, 0.01),\n        (40, 8, 0.4, 0.0, 0.5, 10, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, lambda_0, s, phi, sigma, h, alpha = case\n\n        # Step 1: Simulate the time series data\n        # Generate innovations eta_t ~ N(0, sigma^2) for the AR(1) process\n        eta = np.random.normal(loc=0.0, scale=sigma, size=n)\n\n        # Generate the latent AR(1) process a_t = phi * a_{t-1} + eta_t with a_0=0\n        a = np.zeros(n)\n        if n > 0:\n            # For t=1 (index 0), a_1 = phi*a_0 + eta_1 = eta_1\n            a[0] = eta[0]\n            for t in range(1, n):\n                a[t] = phi * a[t-1] + eta[t]\n        \n        # Generate the time-varying rate lambda_t\n        t_vals = np.arange(1, n + 1)\n        lambda_t = lambda_0 * (1.0 + s * t_vals / n) * np.exp(a)\n\n        # Generate the observed throughput y_t ~ Poisson(lambda_t)\n        y_t = np.random.poisson(lam=lambda_t)\n\n        # Step 2: Detrend the series using Ordinary Least Squares (OLS)\n        # Create the design matrix X for the linear model y_t = beta_0 + beta_1*t\n        X = np.vstack([np.ones(n), t_vals]).T\n        \n        # Fit the OLS model using np.linalg.lstsq\n        try:\n            beta = np.linalg.lstsq(X, y_t, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            # Handle cases where the fit fails, though unlikely here\n            results.append(False) \n            continue\n        \n        # Calculate the fitted values and the residuals\n        y_hat = X @ beta\n        residuals = y_t - y_hat\n\n        # Step 3: Perform the Ljung-Box test on the residuals\n        n_res = len(residuals)\n        \n        # Center the residuals (mean is computationally near zero for OLS with intercept)\n        r_centered = residuals - np.mean(residuals)\n        \n        # Calculate sample autocorrelations rho_k for lags k=1,...,h\n        # First, calculate autocovariances using np.correlate\n        autocov = np.correlate(r_centered, r_centered, mode='full')\n        \n        # The autocovariance at lag 0 (gamma_0) is the middle element\n        gamma_0 = autocov[n_res - 1]\n        \n        rho_hat_sq = np.zeros(h)\n        if gamma_0 > 1e-12:  # Avoid division by zero if variance is zero\n            # Autocovariances for lags 1 to h are at indices n to n+h-1\n            autocov_lags = autocov[n_res : n_res + h]\n            rho_hats = autocov_lags / gamma_0\n            rho_hat_sq = rho_hats**2\n\n        # Calculate the Ljung-Box Q statistic\n        k_vals = np.arange(1, h + 1)\n        \n        # The divisor term n-k; ensure no division by a non-positive number\n        divisors = n_res - k_vals\n        valid_indices = divisors > 0\n\n        reject_null = False\n        if np.any(valid_indices):\n            # Sum only over lags k where n-k > 0\n            Q = n_res * (n_res + 2) * np.sum(rho_hat_sq[valid_indices] / divisors[valid_indices])\n            \n            # Degrees of freedom for the chi-squared distribution is h\n            # for residuals from regression on deterministic variables.\n            df = h\n            \n            # Compute the p-value from the chi-squared distribution's survival function\n            p_value = chi2.sf(Q, df)\n            \n            # Step 4: Make a decision based on the significance level alpha\n            if p_value < alpha:\n                reject_null = True\n\n        results.append(reject_null)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3098986"}]}