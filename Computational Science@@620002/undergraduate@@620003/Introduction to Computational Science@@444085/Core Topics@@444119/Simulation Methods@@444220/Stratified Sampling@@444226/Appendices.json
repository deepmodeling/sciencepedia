{"hands_on_practices": [{"introduction": "A key advantage of stratified sampling is the ability to allocate sampling effort efficiently. Instead of sampling uniformly, we can invest more resources in strata that are more variable or more important. This exercise guides you through the foundational theory of optimal allocation, where you will use basic principles of optimization to derive the ideal sample sizes for minimizing estimation error across different strata.[@problem_id:3198743]", "problem": "Consider a computational experiment to estimate a histogram of a binary property over a partition of the domain into $3$ disjoint bins indexed by $k \\in \\{1,2,3\\}$. In bin $k$, each sampled item produces an independent Bernoulli outcome with success probability $p_{k} \\in (0,1)$, and the per-sample cost is identical across bins. You plan to collect $n_{k}$ samples in bin $k$, with the total sampling budget constrained by $\\sum_{k=1}^{3} n_{k} = N$, where $N$ is a fixed positive integer. For each bin $k$, you estimate the bin-wise success probability $p_{k}$ by the sample mean $\\hat{p}_{k}$ of the Bernoulli outcomes collected in that bin. You aim to choose the allocation $(n_{1}, n_{2}, n_{3})$ to minimize the sum, over bins, of the Mean Squared Error (MSE) of the estimators $\\hat{p}_{k}$.\n\nStarting from fundamental properties of Bernoulli sampling and the definition of Mean Squared Error (MSE), derive the allocation that minimizes the total MSE subject to the constraint $\\sum_{k=1}^{3} n_{k} = N$. Then, express the optimal allocation for bin $2$, namely the closed-form expression for $n_{2}$, in terms of $N$, $p_{1}$, $p_{2}$, and $p_{3}$. Provide your final expression for $n_{2}$ in exact form. No rounding is required.", "solution": "The problem is to find the optimal allocation of a total sample size $N$ among $3$ disjoint bins, denoted by indices $k \\in \\{1, 2, 3\\}$, to minimize the total Mean Squared Error (MSE) of the estimators for the bin-wise success probabilities $p_k$. The problem is well-posed and scientifically grounded in the principles of statistics and optimization. We will proceed with a full derivation.\n\nFirst, let us define the estimator for the success probability $p_k$ in bin $k$. We collect $n_k$ samples, where each sample is an independent Bernoulli trial. Let $X_{k,i}$ for $i=1, \\dots, n_k$ be the random variable for the $i$-th outcome in bin $k$, where $X_{k,i}=1$ for a success and $X_{k,i}=0$ for a failure. The probability of success is given as $P(X_{k,i}=1) = p_k$. The estimator for $p_k$, denoted $\\hat{p}_k$, is the sample mean:\n$$\n\\hat{p}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} X_{k,i}\n$$\nThe objective is to minimize the sum of the Mean Squared Errors for each of these estimators. The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$. The MSE can also be expressed as the sum of the variance and the squared bias of the estimator: $\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + (\\text{Bias}(\\hat{\\theta}))^2$.\n\nLet us first determine the MSE of $\\hat{p}_k$. We begin by calculating its expected value to find the bias.\n$$\nE[\\hat{p}_k] = E\\left[\\frac{1}{n_k} \\sum_{i=1}^{n_k} X_{k,i}\\right] = \\frac{1}{n_k} \\sum_{i=1}^{n_k} E[X_{k,i}]\n$$\nSince each $X_{k,i}$ is a Bernoulli random variable with parameter $p_k$, its expected value is $E[X_{k,i}] = p_k$.\n$$\nE[\\hat{p}_k] = \\frac{1}{n_k} \\sum_{i=1}^{n_k} p_k = \\frac{1}{n_k} (n_k p_k) = p_k\n$$\nThe bias of the estimator is $\\text{Bias}(\\hat{p}_k) = E[\\hat{p}_k] - p_k = p_k - p_k = 0$. The estimator $\\hat{p}_k$ is unbiased.\nSince the bias is zero, the MSE is equal to the variance of the estimator:\n$$\n\\text{MSE}(\\hat{p}_k) = \\text{Var}(\\hat{p}_k)\n$$\nNow, we calculate the variance of $\\hat{p}_k$. Since the samples $X_{k,i}$ are independent, the variance of their sum is the sum of their variances.\n$$\n\\text{Var}(\\hat{p}_k) = \\text{Var}\\left(\\frac{1}{n_k} \\sum_{i=1}^{n_k} X_{k,i}\\right) = \\frac{1}{n_k^2} \\text{Var}\\left(\\sum_{i=1}^{n_k} X_{k,i}\\right) = \\frac{1}{n_k^2} \\sum_{i=1}^{n_k} \\text{Var}(X_{k,i})\n$$\nThe variance of a Bernoulli random variable with parameter $p_k$ is $\\text{Var}(X_{k,i}) = p_k(1-p_k)$.\n$$\n\\text{Var}(\\hat{p}_k) = \\frac{1}{n_k^2} \\sum_{i=1}^{n_k} p_k(1-p_k) = \\frac{1}{n_k^2} n_k p_k(1-p_k) = \\frac{p_k(1-p_k)}{n_k}\n$$\nSo, the MSE for the estimator in bin $k$ is:\n$$\n\\text{MSE}(\\hat{p}_k) = \\frac{p_k(1-p_k)}{n_k}\n$$\nThe total MSE, which we aim to minimize, is the sum over all bins:\n$$\nL(n_1, n_2, n_3) = \\sum_{k=1}^{3} \\text{MSE}(\\hat{p}_k) = \\sum_{k=1}^{3} \\frac{p_k(1-p_k)}{n_k}\n$$\nThis minimization is subject to the constraint on the total number of samples:\n$$\n\\sum_{k=1}^{3} n_k = N\n$$\nThis is a constrained optimization problem. We can solve it using the method of Lagrange multipliers. We treat the sample sizes $n_k$ as continuous positive real variables. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(n_1, n_2, n_3, \\lambda) = \\sum_{k=1}^{3} \\frac{p_k(1-p_k)}{n_k} + \\lambda \\left(\\sum_{k=1}^{3} n_k - N\\right)\n$$\nTo find the minimum, we take the partial derivative of $\\mathcal{L}$ with respect to each $n_k$ and set it to zero. For each $k \\in \\{1, 2, 3\\}$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_k} = -\\frac{p_k(1-p_k)}{n_k^2} + \\lambda = 0\n$$\nThis implies:\n$$\n\\lambda = \\frac{p_k(1-p_k)}{n_k^2}\n$$\nFrom this, we can express $n_k$ in terms of $\\lambda$:\n$$\nn_k^2 = \\frac{p_k(1-p_k)}{\\lambda} \\implies n_k = \\frac{\\sqrt{p_k(1-p_k)}}{\\sqrt{\\lambda}}\n$$\nWe take the positive square root since $n_k$ must be positive. This result shows that the optimal sample size for a given bin, $n_k$, is proportional to the standard deviation of the Bernoulli process in that bin, $\\sigma_k = \\sqrt{p_k(1-p_k)}$.\n\nNow, we use the constraint $\\sum_{k=1}^{3} n_k = N$ to determine the value of the Lagrange multiplier $\\lambda$.\n$$\n\\sum_{k=1}^{3} \\frac{\\sqrt{p_k(1-p_k)}}{\\sqrt{\\lambda}} = N\n$$\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^{3} \\sqrt{p_k(1-p_k)} = N\n$$\nSolving for $\\frac{1}{\\sqrt{\\lambda}}$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} = \\frac{N}{\\sum_{j=1}^{3} \\sqrt{p_j(1-p_j)}}\n$$\nNow substitute this back into the expression for $n_k$:\n$$\nn_k = \\sqrt{p_k(1-p_k)} \\left( \\frac{1}{\\sqrt{\\lambda}} \\right) = \\sqrt{p_k(1-p_k)} \\left( \\frac{N}{\\sum_{j=1}^{3} \\sqrt{p_j(1-p_j)}} \\right)\n$$\nThis gives the general formula for the optimal allocation $n_k$:\n$$\nn_k = N \\frac{\\sqrt{p_k(1-p_k)}}{\\sum_{j=1}^{3} \\sqrt{p_j(1-p_j)}}\n$$\nThe problem specifically asks for the closed-form expression for the optimal allocation for bin $2$, which is $n_2$. Setting $k=2$ in the general formula yields:\n$$\nn_2 = N \\frac{\\sqrt{p_2(1-p_2)}}{\\sqrt{p_1(1-p_1)} + \\sqrt{p_2(1-p_2)} + \\sqrt{p_3(1-p_3)}}\n$$\nThis is the final expression for the optimal number of samples in bin $2$ in terms of the total sample size $N$ and the bin-wise success probabilities $p_1, p_2, p_3$. This allocation strategy is a variant of Neyman allocation for stratified sampling.", "answer": "$$\n\\boxed{N \\frac{\\sqrt{p_2(1-p_2)}}{\\sqrt{p_1(1-p_1)} + \\sqrt{p_2(1-p_2)} + \\sqrt{p_3(1-p_3)}}}\n$$", "id": "3198743"}, {"introduction": "Theory comes to life when applied to a practical problem. In this hands-on exercise, you will implement stratified sampling for a common computational task: estimating an integral using the Monte Carlo method. By comparing a simple equal-width stratification with a more sophisticated scheme based on the function's derivative, you will gain a concrete understanding of how intelligent stratum design leads to significant variance reduction and more accurate results.[@problem_id:3198834]", "problem": "You are to design and implement a complete, runnable program that performs Monte Carlo integration using stratified sampling to estimate the integral of the function $f(x)=\\exp(x)$ over the interval $[0,1]$, and to compare two stratification schemes: equal-width strata and nonuniform-width strata determined by the magnitude of the derivative. The program must follow a principled algorithm derived from first principles of integration and probability. The computational framework is as follows.\n\nBegin from the fundamental equivalence between Riemann integration and expectation under a uniform distribution: for any integrable function $f$ on $[a,b]$, the integral satisfies\n$$\n\\int_a^b f(x)\\,dx \\;=\\; (b-a)\\,\\mathbb{E}\\big[f(U)\\big],\n$$\nwhere $U$ is a random variable uniformly distributed on $[a,b]$. This identity justifies Monte Carlo (randomized) estimation of integrals via the expected value of the function under the uniform distribution. Stratified sampling partitions the interval into subintervals (strata) and estimates each stratumâ€™s contribution separately, combining these contributions as a weighted sum. The key idea is that partitioning can reduce variance when strata are constructed to capture heterogeneity of the integrand.\n\nYour implementation must:\n- Use $K$ strata to partition $[0,1]$ under two schemes:\n  1. Equal-width scheme: choose boundaries $0=b_0<b_1<\\dots<b_K=1$ with $b_i=i/K$.\n  2. Derivative-weighted scheme: let $g(x)=|f'(x)|$ on $[0,1]$. Choose boundaries $0=b_0<b_1<\\dots<b_K=1$ such that the integral of $g$ over each stratum is equal, that is, $\\int_{b_{i-1}}^{b_i} g(x)\\,dx$ is the same constant for all $i\\in\\{1,\\dots,K\\}$. This makes strata narrower where $g$ is large and wider where $g$ is small.\n- Within each stratum $[b_{i-1},b_i]$, draw a fixed number of independent samples uniformly at random and compute the average of $f$ over those samples. Multiply that average by the stratum width $w_i=b_i-b_{i-1}$ and sum over strata to obtain a Monte Carlo estimator of the integral. This procedure must be identical for both stratification schemes.\n- Allocate samples per stratum deterministically using equal counts per stratum as follows: given a total of $N$ samples and $K$ strata, assign $n_i=\\lfloor N/K\\rfloor$ samples to every stratum, and distribute the remainder $r=N-K\\lfloor N/K\\rfloor$ by giving $1$ additional sample to each of the first $r$ strata (i.e., $n_i=\\lfloor N/K\\rfloor+1$ for $i\\in\\{1,\\dots,r\\}$ and $n_i=\\lfloor N/K\\rfloor$ otherwise).\n- Use a fixed pseudorandom generator seed to ensure reproducibility: initialize your Random Number Generator (RNG) with seed $12345$.\n- Quantify error for each scheme by the absolute difference between the estimator and the true integral value. Express all error values as decimal floats.\n- Compute the true integral of $f(x)=\\exp(x)$ over $[0,1]$ analytically.\n- Report, for each test case, three outputs: the absolute error for the equal-width stratification, the absolute error for the derivative-weighted stratification, and a boolean indicating whether the derivative-weighted stratification achieved strictly smaller error than the equal-width scheme. Floats must be rounded to $8$ decimal places. The boolean must be either $\\mathrm{True}$ or $\\mathrm{False}$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each test case result is itself a comma-separated list of its three outputs, for example, $[[e_1,d_1,t_1],[e_2,d_2,t_2],\\dots]$.\n\nImplement and run your program on the following test suite, which covers typical behavior and edge conditions:\n- Case $1$: $N=1000$, $K=5$.\n- Case $2$: $N=1000$, $K=1$.\n- Case $3$: $N=100$, $K=10$.\n- Case $4$: $N=5000$, $K=50$.\n- Case $5$: $N=1024$, $K=7$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). For this problem, the full output must be the list of lists described above, with floats rounded to $8$ decimal places and booleans as specified, and with no spaces anywhere in the output.", "solution": "The user wants to solve a computational science problem involving Monte Carlo integration using stratified sampling.\n\n### Step 1: Extract Givens\n\n-   **Function to integrate**: $f(x)=\\exp(x)$\n-   **Integration interval**: $[0,1]$\n-   **Monte Carlo integration principle**: $\\int_a^b f(x)\\,dx = (b-a)\\,\\mathbb{E}\\big[f(U)\\big]$, where $U \\sim \\text{Uniform}(a,b)$.\n-   **Number of strata**: $K$\n-   **Stratification Scheme 1 (Equal-width)**: Boundaries $0=b_0<b_1<\\dots<b_K=1$ with $b_i=i/K$.\n-   **Stratification Scheme 2 (Derivative-weighted)**: Let $g(x)=|f'(x)|$ on $[0,1]$. Boundaries $0=b_0<b_1<\\dots<b_K=1$ such that $\\int_{b_{i-1}}^{b_i} g(x)\\,dx$ is a constant for all $i\\in\\{1,\\dots,K\\}$.\n-   **Sampling within strata**:\n    -   A fixed number of independent samples drawn uniformly at random from within each stratum $[b_{i-1},b_i]$.\n-   **Integral estimator**: Sum of (stratum width * average of $f$ over samples).\n-   **Sample allocation**:\n    -   Total samples: $N$\n    -   Base samples per stratum: $n_{base} = \\lfloor N/K\\rfloor$.\n    -   Remainder: $r=N-K\\lfloor N/K\\rfloor$.\n    -   Sample count for stratum $i$: $n_i=\\lfloor N/K\\rfloor+1$ for $i\\in\\{1,\\dots,r\\}$; $n_i=\\lfloor N/K\\rfloor$ for $i\\in\\{r+1,\\dots,K\\}$.\n-   **RNG Seed**: $12345$.\n-   **Error metric**: Absolute difference between the estimator and the true integral value.\n-   **True integral**: Must be computed analytically.\n-   **Output format per test case**: `[absolute_error_equal_width, absolute_error_derivative_weighted, boolean_is_deriv_better]`.\n-   **Rounding**: Floats rounded to $8$ decimal places.\n-   **Boolean format**: $\\mathrm{True}$ or $\\mathrm{False}$.\n-   **Final output format**: `[[e1,d1,t1],[e2,d2,t2],...]` with no spaces.\n-   **Test cases**:\n    -   Case 1: $N=1000$, $K=5$.\n    -   Case 2: $N=1000$, $K=1$.\n    -   Case 3: $N=100$, $K=10$.\n    -   Case 4: $N=5000$, $K=50$.\n    -   Case 5: $N=1024$, $K=7$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded (Critical)**: The problem is constructed upon the standard mathematical and statistical principles of integral calculus, Monte Carlo methods, and stratified sampling. The use of a derivative-based partitioning is a well-established variance reduction technique. The problem is scientifically sound.\n2.  **Well-Posed**: All parameters ($f(x)$, interval, $N$, $K$), procedures (stratification rules, sample allocation), and constraints (RNG seed, output format) are explicitly defined. This ensures that a unique and reproducible solution can be computed.\n3.  **Objective (Critical)**: The problem is stated using formal mathematical language and is devoid of subjective or ambiguous terminology.\n4.  **Scientific or Factual Unsoundness**: None. The underlying principles are correct.\n5.  **Non-Formalizable or Irrelevant**: The problem is a standard exercise in computational science and is fully formalizable.\n6.  **Incomplete or Contradictory Setup**: The problem is self-contained and free of contradictions. The edge case of $K=1$ is handled consistently by the general formulas for both stratification schemes, resulting in identical partitions (the full interval $[0,1]$), though the sequence of random numbers used will differ, leading to distinct estimates.\n7.  **Unrealistic or Infeasible**: The computational task is well within the capabilities of a standard computer.\n8.  **Ill-Posed or Poorly Structured**: The problem is well-structured, with a clear objective and path to a unique solution.\n9.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial; it requires a correct implementation of two different stratification schemes and their comparison, demonstrating an understanding of variance reduction.\n10. **Outside Scientific Verifiability**: The solution is deterministic and verifiable given the fixed RNG seed.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe solution is developed from first principles as follows.\n\n**1. Analytical Integral Value**\nThe exact value of the integral serves as the ground truth for evaluating the accuracy of the Monte Carlo estimators. The function to integrate is $f(x) = \\exp(x)$ over the interval $[0, 1]$. The analytical integral is:\n$$\nI_{true} = \\int_{0}^{1} \\exp(x) \\,dx = \\left[ \\exp(x) \\right]_{0}^{1} = \\exp(1) - \\exp(0) = e - 1\n$$\nThis value will be calculated using high-precision floating-point arithmetic.\n\n**2. Stratified Sampling Estimator**\nThe principle of stratified sampling involves partitioning the integration domain $[a, b]$ into $K$ disjoint subintervals (strata), $[b_{i-1}, b_i]$, such that $a = b_0 < b_1 < \\dots < b_K = b$. The total integral is the sum of the integrals over each stratum:\n$$\nI = \\sum_{i=1}^{K} \\int_{b_{i-1}}^{b_i} f(x) \\,dx\n$$\nEach stratum integral is estimated using the Monte Carlo method. For a stratum $[b_{i-1}, b_i]$ with width $w_i = b_i - b_{i-1}$, the integral is approximated by drawing $n_i$ independent uniform random samples $x_{i,j}$ from this interval. The estimator for the total integral is then the sum of the estimates for each stratum:\n$$\n\\hat{I}_{\\text{stratified}} = \\sum_{i=1}^{K} w_i \\cdot \\left( \\frac{1}{n_i} \\sum_{j=1}^{n_i} f(x_{i,j}) \\right)\n$$\nThe problem specifies a deterministic rule for allocating the total $N$ samples among the $K$ strata: $n_{\\text{base}} = \\lfloor N/K \\rfloor$ samples are assigned to each stratum, and the remaining $r = N \\pmod K$ samples are distributed one by one to the first $r$ strata.\n\n**3. Stratification Schemes**\nThe effectiveness of stratified sampling depends on the choice of strata boundaries $b_i$. The problem requires a comparison of two schemes.\n\n**Scheme 1: Equal-Width Stratification**\nThis is the most straightforward method. The interval $[0, 1]$ is divided into $K$ subintervals of equal width. The boundaries are given by:\n$$\nb_i = \\frac{i}{K} \\quad \\text{for } i = 0, 1, \\dots, K\n$$\nThe width of each stratum is constant, $w_i = 1/K$.\n\n**Scheme 2: Derivative-Weighted Stratification**\nThis scheme aims to reduce variance by creating narrower strata where the integrand $f(x)$ changes most rapidly. This is achieved by partitioning the domain such that the integral of the derivative's magnitude, $g(x) = |f'(x)|$, is constant across all strata. For $f(x) = \\exp(x)$, the derivative is $f'(x) = \\exp(x)$. On the interval $[0, 1]$, $\\exp(x) > 0$, so $g(x) = \\exp(x)$.\n\nThe total integral of $g(x)$ over $[0,1]$ is $\\int_0^1 \\exp(x) \\,dx = e-1$. We want to find boundaries $b_i$ such that for each stratum $[b_{i-1}, b_i]$:\n$$\n\\int_{b_{i-1}}^{b_i} \\exp(x) \\,dx = \\frac{1}{K} \\int_{0}^{1} \\exp(x) \\,dx = \\frac{e-1}{K}\n$$\nEvaluating the integral on the left gives $\\exp(b_i) - \\exp(b_{i-1})$. This yields a recurrence relation:\n$$\n\\exp(b_i) - \\exp(b_{i-1}) = \\frac{e-1}{K}\n$$\nWith the starting boundary $b_0 = 0$, we can solve for successive boundaries. For $i=1$, $\\exp(b_1) - \\exp(0) = \\frac{e-1}{K}$, which implies $\\exp(b_1) = 1 + \\frac{e-1}{K}$.\nBy induction, we find the general expression for $\\exp(b_i)$:\n$$\n\\exp(b_i) = \\exp(b_0) + i \\cdot \\frac{e-1}{K} = 1 + i \\cdot \\frac{e-1}{K}\n$$\nSolving for $b_i$ by taking the natural logarithm gives the formula for the boundaries:\n$$\nb_i = \\ln\\left(1 + i \\cdot \\frac{e-1}{K}\\right) \\quad \\text{for } i = 0, 1, \\dots, K\n$$\nThis formula correctly yields $b_0 = \\ln(1) = 0$ and $b_K = \\ln(1 + K \\cdot \\frac{e-1}{K}) = \\ln(e) = 1$.\n\n**4. Algorithm and Implementation**\nThe overall algorithm proceeds as follows:\n1. Initialize a single pseudorandom number generator with the specified seed $12345$ to ensure reproducibility.\n2. For each test case specified by a total sample count $N$ and stratum count $K$:\n   a. Calculate the per-stratum sample counts $n_i$.\n   b. For the equal-width scheme, generate boundaries $b_i=i/K$. For the derivative-weighted scheme, generate boundaries $b_i = \\ln(1 + i(e-1)/K)$.\n   c. A core function implements the stratified sampling estimator. This function takes the integrand, a set of boundaries, the per-stratum sample counts, and the RNG as input. It iterates through the strata, generates uniform random points within each, evaluates the function, computes the stratum's integral estimate, and sums the results.\n   d. Call this core function for each of the two stratification schemes to obtain their respective integral estimates, $\\hat{I}_{eq}$ and $\\hat{I}_{dw}$. The RNG state evolves between calls, ensuring that different sets of random numbers are used for each estimation.\n   e. Compute the absolute error for each scheme: $E_{eq} = |\\hat{I}_{eq} - I_{true}|$ and $E_{dw} = |\\hat{I}_{dw} - I_{true}|$.\n   f. Determine the boolean result by checking if $E_{dw} < E_{eq}$.\n   g. Format the two error values (rounded to $8$ decimal places) and the boolean into the specified list format for the test case.\n3. Consolidate the results from all test cases into a final list-of-lists structure and print it as a single-line string with no spaces.", "answer": "```python\nimport numpy as np\n\ndef stratified_monte_carlo(f, boundaries, sample_counts, rng):\n    \"\"\"\n    Performs stratified Monte Carlo integration.\n\n    Args:\n        f (callable): The function to integrate.\n        boundaries (np.ndarray): An array of stratum boundaries, size K+1.\n        sample_counts (list): A list of sample counts per stratum, size K.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        float: The estimated value of the integral.\n    \"\"\"\n    total_integral_estimate = 0.0\n    num_strata = len(sample_counts)\n    \n    for i in range(num_strata):\n        b_lower = boundaries[i]\n        b_upper = boundaries[i+1]\n        width = b_upper - b_lower\n        n_samples = sample_counts[i]\n        \n        if n_samples == 0:\n            continue\n            \n        # Generate n_samples uniform samples in [0, 1]\n        u_samples = rng.uniform(size=n_samples)\n        \n        # Transform samples to the stratum interval [b_lower, b_upper]\n        x_samples = b_lower + u_samples * width\n        \n        # Evaluate the function at the sample points\n        y_samples = f(x_samples)\n        \n        # Estimate the integral for the current stratum and add to total\n        total_integral_estimate += width * np.mean(y_samples)\n        \n    return total_integral_estimate\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo integration comparison.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1000, 5),\n        (1000, 1),\n        (100, 10),\n        (5000, 50),\n        (1024, 7),\n    ]\n\n    # Initialize the random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(12345)\n\n    # Define the function to integrate, f(x) = exp(x).\n    f = lambda x: np.exp(x)\n\n    # Analytically compute the true value of the integral from 0 to 1.\n    true_integral = np.exp(1.0) - 1.0\n\n    # Store formatted results for all test cases.\n    all_results_str = []\n\n    for N, K in test_cases:\n        # Step 1: Determine sample allocation per stratum.\n        if K > 0:\n            n_base = N // K\n            remainder = N % K\n            sample_counts = [n_base + 1] * remainder + [n_base] * (K - remainder)\n        else:\n            sample_counts = []\n\n        # -- Scheme 1: Equal-width stratification --\n        boundaries_eq = np.linspace(0.0, 1.0, K + 1)\n        integral_eq = stratified_monte_carlo(f, boundaries_eq, sample_counts, rng)\n        error_eq = np.abs(integral_eq - true_integral)\n\n        # -- Scheme 2: Derivative-weighted stratification --\n        # For f(x)=exp(x), |f'(x)|=exp(x). The boundaries b_i are chosen\n        # such that integral_{b_{i-1}}^{b_i} exp(x) dx is constant.\n        # This leads to b_i = ln(1 + i * (e-1)/K).\n        if K > 0:\n            integral_of_g = true_integral\n            const = integral_of_g / K\n            i_vals = np.arange(K + 1)\n            boundaries_dw = np.log(1.0 + i_vals * const)\n        else:\n            boundaries_dw = np.array([])\n        \n        integral_dw = stratified_monte_carlo(f, boundaries_dw, sample_counts, rng)\n        error_dw = np.abs(integral_dw - true_integral)\n\n        # -- Step 3: Comparison and Formatting --\n        is_dw_better = error_dw  error_eq\n        \n        error_eq_rounded = f\"{error_eq:.8f}\"\n        error_dw_rounded = f\"{error_dw:.8f}\"\n        \n        # Build the result string for the current test case in the format [e,d,t].\n        case_result_str = f\"[{error_eq_rounded},{error_dw_rounded},{is_dw_better}]\"\n        all_results_str.append(case_result_str)\n\n    # Final print statement in the exact required format [[...],[...],...].\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3198834"}, {"introduction": "In real-world applications, our models and assumptions are rarely perfect. This practice moves beyond idealized scenarios to explore a critical question: what happens to our stratified sampling estimator if the stratum boundaries are incorrect? You will analyze the effects of stratum misclassification, a common problem in survey data, by deriving the resulting increase in variance. This advanced exercise will deepen your understanding of the method's robustness and the practical consequences of model error.[@problem_id:3198761]", "problem": "Consider stratified sampling of a large population partitioned into two strata with true population proportions $W_1$ and $W_2$ satisfying $W_1 + W_2 = 1$. In stratum $h \\in \\{1,2\\}$, the true stratum mean is $\\mu_h$ and the within-stratum variance is $\\sigma_h^2$. A stratified estimator of the overall population mean $\\mu$ uses independent and identically distributed (IID) samples of sizes $n_1$ and $n_2$ drawn within strata $1$ and $2$, respectively, and computes the weighted sum of the stratum sample means using the (possibly observed) stratum weights. Suppose stratum boundaries are imperfect: a symmetric fraction $\\epsilon \\in [0,1]$ of units from stratum $1$ are erroneously assigned to stratum $2$ and, symmetrically, a fraction $\\epsilon$ from stratum $2$ are assigned to stratum $1$. Define $\\epsilon$ as a fraction (express it as a decimal, not with a percentage sign). Under this misalignment, the sampling still draws $n_1$ units from the set labeled as stratum $1$ and $n_2$ from the set labeled as stratum $2$, and the estimator uses the observed stratum proportions after misclassification.\n\nStarting from the following fundamental bases:\n- The definition of expectation and variance for random variables.\n- The Law of Total Variance (LTV): for any random variable $X$ and any partition indicator $Z$, $\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid Z)] + \\mathrm{Var}(\\mathbb{E}[X \\mid Z])$.\n- For IID sampling within a stratum of true variance $\\sigma_h^2$, the variance of the stratum sample mean is $\\sigma_h^2 / n_h$.\n\nDerive an explicit and computable upper bound in terms of $\\epsilon$, $W_1$, $W_2$, $n_1$, $n_2$, $\\mu_1$, $\\mu_2$, $\\sigma_1^2$, and $\\sigma_2^2$ for the increase in the variance of the stratified estimator of the mean caused by the misalignment. Specifically, bound the quantity\n$$\n\\Delta \\mathrm{Var} \\;=\\; \\mathrm{Var}_\\epsilon(\\hat{\\mu}) \\;-\\; \\mathrm{Var}_0(\\hat{\\mu}),\n$$\nwhere $\\mathrm{Var}_0(\\hat{\\mu})$ is the variance under correct boundaries and $\\mathrm{Var}_\\epsilon(\\hat{\\mu})$ is the variance under the misaligned boundaries described above. Your bound must be a function $U(\\epsilon)$ such that for all valid parameter values,\n$$\n\\Delta \\mathrm{Var} \\;\\le\\; U(\\epsilon).\n$$\n\nYour program must implement the derived bound $U(\\epsilon)$ and also compute the exact $\\Delta \\mathrm{Var}$ using the mixture interpretation of the misaligned strata derived from the fundamental bases. Then, for each test case below, your program must output whether the inequality $\\Delta \\mathrm{Var} \\le U(\\epsilon)$ holds.\n\nUse the following test suite of parameter sets, where each tuple is $(W_1,W_2,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,n_1,n_2,\\epsilon)$:\n1. $(0.4, 0.6, 0.0, 1.0, 1.0, 1.0, 100, 150, 0.1)$\n2. $(0.5, 0.5, 3.0, 7.0, 2.0, 2.0, 50, 50, 0.0)$\n3. $(0.7, 0.3, 0.0, 5.0, 1.5, 0.5, 80, 40, 0.5)$\n4. $(0.6, 0.4, 2.0, 2.0, 0.5, 1.5, 120, 60, 0.2)$\n5. $(0.2, 0.8, 1.0, 4.0, 0.1, 10.0, 20, 20, 0.3)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each entry is a boolean indicating whether the bound holds (i.e., whether $\\Delta \\mathrm{Var} \\le U(\\epsilon)$). No other text should be printed.", "solution": "The user wants a detailed derivation of an upper bound for the increase in variance of a stratified mean estimator due to stratum boundary misalignment, followed by an implementation that verifies this bound.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Population partitioned into two strata, $h \\in \\{1, 2\\}$.\n- True population proportions (weights): $W_1, W_2$, with $W_1 + W_2 = 1$.\n- True stratum means: $\\mu_1, \\mu_2$.\n- True within-stratum variances: $\\sigma_1^2, \\sigma_2^2$.\n- Misclassification error: A fraction $\\epsilon \\in [0, 1]$ of units from stratum $1$ are misassigned to stratum $2$, and a fraction $\\epsilon$ of units from stratum $2$ are misassigned to stratum $1$.\n- Sampling: Independent and identically distributed (IID) samples of sizes $n_1$ and $n_2$ are drawn from the observed (misaligned) strata.\n- Estimator: The stratified estimator of the overall mean $\\mu$ uses the observed stratum proportions, $W_1'$ and $W_2'$.\n- Quantity to bound: The increase in variance, $\\Delta \\mathrm{Var} = \\mathrm{Var}_\\epsilon(\\hat{\\mu}) - \\mathrm{Var}_0(\\hat{\\mu})$.\n- Fundamental bases: Definition of expectation and variance, Law of Total Variance (LTV), variance of an IID sample mean.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in statistical sampling theory, a core topic in statistics and computational science. All concepts (stratified sampling, variance, expectation, LTV) are standard.\n- **Well-Posed:** The problem provides all necessary parameters and a clear objective. The derivation of an upper bound is a standard mathematical exercise, and a solution is expected to exist.\n- **Objective:** The language is precise and quantitative. No subjective or opinion-based statements are present.\n- **Completeness and Consistency:** The problem is self-contained. The definitions are clear and there are no apparent contradictions. The condition $W_1 + W_2 = 1$ is consistent.\n- **Realism:** Stratum misclassification is a practical and well-studied problem in survey sampling. The setup is a simplified but realistic model of this issue.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-defined mathematical problem in statistics. I will proceed with the derivation and solution.\n\n### Derivation and Solution\n\nThe solution requires a step-by-step derivation of the variance of the stratified estimator under misclassification, from which an upper bound can be formulated.\n\n**1. Characterization of Observed Strata**\nDue to the symmetric misclassification, the observed stratum proportions, which we denote by $W_1'$ and $W_2'$, are mixtures of the true strata.\nThe proportion of the population observed as stratum $1$ is composed of the $(1-\\epsilon)$ fraction of true stratum $1$ and the $\\epsilon$ fraction of true stratum $2$.\n$$ W_1' = (1-\\epsilon)W_1 + \\epsilon W_2 $$\nSimilarly, for observed stratum $2$:\n$$ W_2' = \\epsilon W_1 + (1-\\epsilon)W_2 $$\nIt can be verified that $W_1' + W_2' = (1-\\epsilon)W_1 + \\epsilon W_2 + \\epsilon W_1 + (1-\\epsilon)W_2 = W_1 + W_2 = 1$.\n\n**2. Properties of a Sample from an Observed Stratum**\nEach observed stratum is a mixture of the two true populations. Let's analyze the properties of a single random draw, $Y_h'$, from an observed stratum $h' \\in \\{1', 2'\\}$.\nThe mean and variance of $Y_h'$ are found using the laws of total expectation and total variance. Let $Z$ be an indicator for the true stratum of origin ($Z=1$ or $Z=2$).\n\nFor observed stratum $1'$, the conditional probabilities of a unit's true origin are:\n$P(Z=1|Y_1') = \\frac{(1-\\epsilon)W_1}{W_1'}$ and $P(Z=2|Y_1') = \\frac{\\epsilon W_2}{W_1'}$.\nThe mean of a draw from stratum $1'$ is:\n$$ \\mu_1' = \\mathbb{E}[Y_1'] = \\mathbb{E}[\\mathbb{E}[Y_1'|Z]] = P(Z=1|Y_1')\\mu_1 + P(Z=2|Y_1')\\mu_2 = \\frac{(1-\\epsilon)W_1\\mu_1 + \\epsilon W_2\\mu_2}{W_1'} $$\nThe variance of a draw from stratum $1'$, using the Law of Total Variance $\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X|Z)] + \\mathrm{Var}(\\mathbb{E}[X|Z])$, is:\n$$ \\sigma_{1'}^2 = \\mathrm{Var}(Y_1') = \\left(\\frac{(1-\\epsilon)W_1\\sigma_1^2 + \\epsilon W_2\\sigma_2^2}{W_1'}\\right) + \\left(\\frac{(1-\\epsilon)W_1\\mu_1^2 + \\epsilon W_2\\mu_2^2}{W_1'} - (\\mu_1')^2\\right) $$\nThe second term simplifies to $\\frac{(1-\\epsilon)W_1}{W_1'} \\frac{\\epsilon W_2}{W_1'} (\\mu_1 - \\mu_2)^2$. So,\n$$ \\sigma_{1'}^2 = \\frac{(1-\\epsilon)W_1\\sigma_1^2 + \\epsilon W_2\\sigma_2^2}{W_1'} + \\frac{\\epsilon(1-\\epsilon)W_1 W_2}{(W_1')^2}(\\mu_1 - \\mu_2)^2 $$\nBy symmetry, similar expressions hold for $\\mu_2'$ and $\\sigma_{2'}^2$ by swapping $(1-\\epsilon)$ with $\\epsilon$.\n\n**3. Variance of the Stratified Estimator**\nThe stratified estimator for the population mean $\\mu = W_1\\mu_1 + W_2\\mu_2$ is $\\hat{\\mu} = W_1'\\bar{y}_1' + W_2'\\bar{y}_2'$, where $\\bar{y}_h'$ is the sample mean of $n_h$ draws from observed stratum $h'$.\nThe estimator is unbiased: $\\mathbb{E}[\\hat{\\mu}_\\epsilon] = W_1'\\mu_1' + W_2'\\mu_2' = ((1-\\epsilon)W_1\\mu_1 + \\epsilon W_2\\mu_2) + (\\epsilon W_1\\mu_1 + (1-\\epsilon)W_2\\mu_2) = W_1\\mu_1 + W_2\\mu_2 = \\mu$.\n\nThe variance of the estimator is given by:\n$$ \\mathrm{Var}_\\epsilon(\\hat{\\mu}) = (W_1')^2\\mathrm{Var}(\\bar{y}_1') + (W_2')^2\\mathrm{Var}(\\bar{y}_2') = \\frac{(W_1')^2\\sigma_{1'}^2}{n_1} + \\frac{(W_2')^2\\sigma_{2'}^2}{n_2} $$\nSubstituting the expression for $\\sigma_{h'}^2$:\n$$ (W_1')^2\\sigma_{1'}^2 = W_1'((1-\\epsilon)W_1\\sigma_1^2 + \\epsilon W_2\\sigma_2^2) + \\epsilon(1-\\epsilon)W_1 W_2(\\mu_1 - \\mu_2)^2 $$\n$$ (W_2')^2\\sigma_{2'}^2 = W_2'(\\epsilon W_1\\sigma_1^2 + (1-\\epsilon)W_2\\sigma_2^2) + \\epsilon(1-\\epsilon)W_1 W_2(\\mu_1 - \\mu_2)^2 $$\nThe variance under misclassification is therefore:\n$$ \\mathrm{Var}_\\epsilon(\\hat{\\mu}) = \\frac{W_1'}{n_1}((1-\\epsilon)W_1\\sigma_1^2 + \\epsilon W_2\\sigma_2^2) + \\frac{W_2'}{n_2}(\\epsilon W_1\\sigma_1^2 + (1-\\epsilon)W_2\\sigma_2^2) + \\epsilon(1-\\epsilon)W_1W_2(\\mu_1-\\mu_2)^2\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right) $$\nThe variance under perfect stratification ($\\epsilon=0$) is:\n$$ \\mathrm{Var}_0(\\hat{\\mu}) = \\frac{W_1^2\\sigma_1^2}{n_1} + \\frac{W_2^2\\sigma_2^2}{n_2} $$\n\n**4. Exact Increase in Variance, $\\Delta \\mathrm{Var}$**\nThe increase in variance is $\\Delta \\mathrm{Var} = \\mathrm{Var}_\\epsilon(\\hat{\\mu}) - \\mathrm{Var}_0(\\hat{\\mu})$. After substituting the expressions for $W_1' = (1-\\epsilon)W_1 + \\epsilon W_2$ and $W_2' = \\epsilon W_1 + (1-\\epsilon)W_2$ into the formula for $\\mathrm{Var}_\\epsilon(\\hat{\\mu})$ and performing substantial algebraic simplification, we arrive at the following exact expression for the change in variance:\n$$ \\Delta \\mathrm{Var} = \\epsilon(\\epsilon-2)\\left(\\frac{W_1^2\\sigma_1^2}{n_1} + \\frac{W_2^2\\sigma_2^2}{n_2}\\right) + \\epsilon^2\\left(\\frac{W_2^2\\sigma_2^2}{n_1} + \\frac{W_1^2\\sigma_1^2}{n_2}\\right) + \\epsilon(1-\\epsilon)W_1W_2(\\sigma_1^2+\\sigma_2^2+(\\mu_1-\\mu_2)^2)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right) $$\n\n**5. Derivation of the Upper Bound $U(\\epsilon)$**\nThe expression for $\\Delta \\mathrm{Var}$ consists of three additive terms. Let's analyze them:\n1.  Term A: $A = \\epsilon(\\epsilon-2)\\left(\\frac{W_1^2\\sigma_1^2}{n_1} + \\frac{W_2^2\\sigma_2^2}{n_2}\\right) = \\epsilon(\\epsilon-2)\\mathrm{Var}_0(\\hat{\\mu})$. Since $\\epsilon \\in [0, 1]$, the factor $\\epsilon(\\epsilon-2)$ is non-positive. As $\\mathrm{Var}_0(\\hat{\\mu})$ is non-negative, Term A is always non-positive ($A \\le 0$).\n2.  Term B: $B = \\epsilon^2\\left(\\frac{W_2^2\\sigma_2^2}{n_1} + \\frac{W_1^2\\sigma_1^2}{n_2}\\right)$. This term is non-negative.\n3.  Term C: $C = \\epsilon(1-\\epsilon)W_1W_2(\\sigma_1^2+\\sigma_2^2+(\\mu_1-\\mu_2)^2)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)$. This term is also non-negative.\n\nWe seek an upper bound $U(\\epsilon)$ such that $\\Delta \\mathrm{Var} \\le U(\\epsilon)$. We can construct a simple and valid upper bound by dropping the non-positive term $A$:\n$$ \\Delta \\mathrm{Var} = A + B + C \\le B + C $$\nWe define our upper bound as $U(\\epsilon) = B + C$. This yields:\n$$ U(\\epsilon) = \\epsilon^2\\left(\\frac{W_2^2\\sigma_2^2}{n_1} + \\frac{W_1^2\\sigma_1^2}{n_2}\\right) + \\epsilon(1-\\epsilon)W_1W_2(\\sigma_1^2+\\sigma_2^2+(\\mu_1-\\mu_2)^2)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right) $$\nThis function is an explicit and computable upper bound for $\\Delta \\mathrm{Var}$. By its construction, the inequality $\\Delta \\mathrm{Var} \\le U(\\epsilon)$ is guaranteed to hold for all valid parameters, as it is equivalent to $A \\le 0$, which is true for $\\epsilon \\in [0,1]$.\nThe program will compute both $\\Delta \\mathrm{Var}$ and $U(\\epsilon)$ and verify that this inequality holds for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the exact variance increase (Delta_Var) due to stratum misalignment\n    and a derived upper bound (U_epsilon) for it, then checks if Delta_Var = U_epsilon.\n    \"\"\"\n    test_cases = [\n        # (W1, W2, mu1, mu2, sigma1_sq, sigma2_sq, n1, n2, epsilon)\n        (0.4, 0.6, 0.0, 1.0, 1.0, 1.0, 100, 150, 0.1),\n        (0.5, 0.5, 3.0, 7.0, 2.0, 2.0, 50, 50, 0.0),\n        (0.7, 0.3, 0.0, 5.0, 1.5, 0.5, 80, 40, 0.5),\n        (0.6, 0.4, 2.0, 2.0, 0.5, 1.5, 120, 60, 0.2),\n        (0.2, 0.8, 1.0, 4.0, 0.1, 10.0, 20, 20, 0.3),\n    ]\n\n    results = []\n    for case in test_cases:\n        W1, W2, mu1, mu2, s1sq, s2sq, n1, n2, e = case\n\n        # Part 1: Compute the exact increase in variance, Delta_Var.\n\n        # The term corresponding to the variance under perfect stratification (Var0).\n        var0 = (W1**2 * s1sq) / n1 + (W2**2 * s2sq) / n2\n        \n        # The three additive terms in the exact expression for Delta_Var.\n        # Term A: A = e(e-2) * Var0\n        term_A = e * (e - 2) * var0\n\n        # Term B: The term proportional to e^2\n        term_B_factor = (W2**2 * s2sq) / n1 + (W1**2 * s1sq) / n2\n        term_B = e**2 * term_B_factor\n        \n        # Term C: The term proportional to e(1-e)\n        delta_mu_sq = (mu1 - mu2)**2\n        term_C_factor = W1 * W2 * (s1sq + s2sq + delta_mu_sq) * (1/n1 + 1/n2)\n        term_C = e * (1 - e) * term_C_factor\n        \n        # Exact increase in variance\n        delta_var = term_A + term_B + term_C\n\n        # Part 2: Compute the upper bound U(epsilon).\n        # The upper bound is derived by dropping the non-positive term_A.\n        u_epsilon = term_B + term_C\n        \n        # Part 3: Verify the inequality.\n        # By construction, delta_var = u_epsilon because term_A is non-positive for e in [0, 1].\n        # The check handles potential floating point arithmetic issues, though they are unlikely here.\n        is_le = delta_var = u_epsilon\n        \n        results.append(is_le)\n    \n    # Print the results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3198761"}]}