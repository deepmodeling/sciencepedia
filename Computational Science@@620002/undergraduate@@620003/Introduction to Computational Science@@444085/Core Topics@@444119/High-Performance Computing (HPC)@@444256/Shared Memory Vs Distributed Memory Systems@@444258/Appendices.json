{"hands_on_practices": [{"introduction": "A core decision in parallel computing is how to manage large datasets: do we copy the data to every processing unit, or do we partition it? This practice guides you through a fundamental design choice between replication and sharding, a common scenario in computational science. By performing a practical calculation based on system memory constraints, you will determine when a distributed memory approach becomes necessary and quantify the resulting communication costs [@problem_id:3191832].", "problem": "A computational scientist is evaluating two parallelization strategies for a single large one-dimensional array on a cluster: replicating the entire array on every node versus sharding the array across nodes with nearest-neighbor ghost cells. The goal is to decide whether replication is feasible and, if not, determine how many nodes are required under sharding to satisfy per-node memory constraints, and to quantify the communication time per iteration for ghost exchanges.\n\nFundamental definitions and facts to use:\n- In a shared memory design, all threads can access a single memory address space, whereas in a distributed memory design, each process has private memory and any data not locally available must be communicated.\n- Under replication in a distributed memory setting, each node stores the entire array locally, so the per-node memory footprint equals the full array size.\n- Under sharding with one-dimensional block decomposition across $p$ nodes, each node stores its local block and ghost cells for two neighbors (left and right). If each boundary requires $g$ ghost elements, then each node stores an additional $2g$ elements beyond its local block.\n- In a bandwidth-only communication model, ignoring latency, the communication time is proportional to the bytes moved: $T = \\beta \\times B$, where $B$ is the total bytes transmitted and $\\beta$ is the seconds per byte coefficient.\n\nGiven data:\n- Array length is $A = 6.0 \\times 10^{9}$ elements.\n- Each element is $s = 8$ bytes.\n- Per-node Random Access Memory (RAM) is $M = 24 \\times 10^{9}$ bytes.\n- Each node requires $g = 5.0 \\times 10^{6}$ ghost elements per neighbor per iteration, for two neighbors.\n- The bandwidth model scaling is $\\beta = 2.5 \\times 10^{-10}$ seconds per byte.\n\nTasks:\n1. Decide whether replication is feasible by comparing the full array size to the per-node memory $M$.\n2. If replication is not feasible, determine the minimal integer $p$ such that a sharded arrangement fits in per-node memory, accounting for both the local block and ghost cells.\n3. Using the bandwidth-only model, compute the per-node communication time per iteration to exchange ghost cells with the two neighbors (send only; assume symmetric exchange, and ignore latency). Express time in seconds.\n\nRound the communication time to four significant figures. Report your final answer as two values in the order: minimal $p$ and per-node ghost exchange time in seconds.", "solution": "The problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- Array length: $A = 6.0 \\times 10^{9}$ elements\n- Size per element: $s = 8$ bytes\n- Per-node RAM: $M = 24 \\times 10^{9}$ bytes\n- Ghost elements per neighbor: $g = 5.0 \\times 10^{6}$ elements\n- Number of neighbors for ghost exchange: $2$\n- Bandwidth model coefficient: $\\beta = 2.5 \\times 10^{-10}$ seconds per byte\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem uses standard concepts from high-performance and parallel computing, including memory constraints, domain decomposition (sharding), ghost cells for boundary data exchange, and a simplified bandwidth-only communication model. These are fundamental and well-established principles in computational science. The provided numerical values are large but plausible for a modern high-performance computing cluster.\n- **Well-Posed**: The problem is structured with a clear set of tasks that lead to a unique, quantifiable solution. All necessary data are provided.\n- **Objective**: The problem is stated in precise, quantitative terms, free from any subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. I will proceed with the full solution.\n\n### Task 1: Feasibility of Replication\nFirst, we determine if replicating the entire array on each node is a feasible strategy. This requires comparing the total size of the array to the available memory on a single node.\n\nThe total size of the array, $S_{total}$, is the product of the number of elements, $A$, and the size of each element, $s$.\n$$S_{total} = A \\times s$$\nSubstituting the given values:\n$$S_{total} = (6.0 \\times 10^{9} \\text{ elements}) \\times (8 \\text{ bytes/element}) = 48 \\times 10^{9} \\text{ bytes}$$\n\nThe available memory per node is given as $M = 24 \\times 10^{9}$ bytes.\nWe compare $S_{total}$ with $M$:\n$$48 \\times 10^{9} \\text{ bytes} > 24 \\times 10^{9} \\text{ bytes}$$\nSince the total array size $S_{total}$ is greater than the per-node memory $M$, it is not feasible to replicate the entire array on each node.\n\n### Task 2: Minimal Number of Nodes for Sharding\nSince replication is not feasible, we must use a sharded (domain decomposition) strategy. We need to find the minimum number of nodes, $p$, such that the portion of the array on each node, including ghost cells, fits into the node's memory $M$.\n\nFor a one-dimensional array sharded across $p$ nodes, each node stores a local block of size $A/p$ elements. In addition, each node stores ghost cells for its two neighbors (left and right). The problem states that each boundary requires $g$ ghost elements. Therefore, each node must store an additional $2g$ elements.\n\nThe total number of elements, $N_p$, stored on a single node is the sum of its local block and its ghost cells:\n$$N_p = \\frac{A}{p} + 2g$$\n\nThe total memory footprint per node, $M_p$, is the number of elements $N_p$ multiplied by the size per element $s$:\n$$M_p = \\left(\\frac{A}{p} + 2g\\right) \\times s$$\n\nFor this arrangement to be feasible, the per-node memory footprint $M_p$ must be less than or equal to the available per-node memory $M$:\n$$M_p \\leq M$$\n$$\\left(\\frac{A}{p} + 2g\\right) \\times s \\leq M$$\n\nWe solve this inequality for $p$:\n$$\\frac{A}{p} + 2g \\leq \\frac{M}{s}$$\n$$\\frac{A}{p} \\leq \\frac{M}{s} - 2g$$\n$$p \\geq \\frac{A}{\\frac{M}{s} - 2g}$$\n\nNow, we substitute the given values:\n$A = 6.0 \\times 10^{9}$\n$M = 24 \\times 10^{9}$\n$s = 8$\n$g = 5.0 \\times 10^{6}$\n\nFirst, calculate the term in the denominator:\n$$\\frac{M}{s} - 2g = \\frac{24 \\times 10^{9}}{8} - 2 \\times (5.0 \\times 10^{6})$$\n$$\\frac{M}{s} - 2g = 3.0 \\times 10^{9} - 1.0 \\times 10^{7}$$\n$$\\frac{M}{s} - 2g = 300 \\times 10^{7} - 1.0 \\times 10^{7} = 299 \\times 10^{7} = 2.99 \\times 10^{9}$$\n\nNow, substitute this back into the inequality for $p$:\n$$p \\geq \\frac{6.0 \\times 10^{9}}{2.99 \\times 10^{9}}$$\n$$p \\geq \\frac{6.0}{2.99} \\approx 2.00668896...$$\n\nSince the number of nodes $p$ must be an integer, the minimal value for $p$ is the smallest integer that satisfies this condition, which is the ceiling of $2.00668896...$.\n$$p_{min} = 3$$\n\n### Task 3: Per-Node Communication Time\nThe communication time per iteration, $T$, is calculated using the bandwidth-only model, $T = \\beta \\times B$, where $B$ is the total bytes transmitted per node.\n\nThe problem asks for the time to \"exchange ghost cells with the two neighbors (send only)\". This means we calculate the amount of data one node sends. A typical ghost cell exchange involves a node sending its left-boundary data to its left neighbor and its right-boundary data to its right neighbor. Each of these transfers involves $g$ elements.\n\nTotal number of elements sent by one node = $g (\\text{to left}) + g (\\text{to right}) = 2g$.\nTotal bytes sent by one node, $B$, is:\n$$B = 2g \\times s$$\n$$B = 2 \\times (5.0 \\times 10^{6}) \\times 8 = 1.0 \\times 10^{7} \\times 8 = 8.0 \\times 10^{7} \\text{ bytes}$$\n\nNow, we calculate the communication time $T$:\n$$T = \\beta \\times B$$\n$$T = (2.5 \\times 10^{-10} \\text{ s/byte}) \\times (8.0 \\times 10^{7} \\text{ bytes})$$\n$$T = (2.5 \\times 8.0) \\times (10^{-10} \\times 10^{7}) \\text{ s}$$\n$$T = 20.0 \\times 10^{-3} \\text{ s} = 0.02 \\text{ s}$$\n\nThe problem requires rounding the communication time to four significant figures. The exact calculated value is $0.02$. To express this with four significant figures, we write it as $0.02000$, or in scientific notation, $2.000 \\times 10^{-2}$.\n\nThe final answer requires two values: the minimal integer $p$, and the communication time in seconds.\n- Minimal $p = 3$\n- Communication time $T = 2.000 \\times 10^{-2}$ s", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 2.000 \\times 10^{-2}\n\\end{pmatrix}\n}\n$$", "id": "3191832"}, {"introduction": "When data is partitioned across processors or memory hierarchies, communication is required at the boundaries, which is often managed using \"ghost\" or \"halo\" cells. This exercise challenges you to analytically derive and compare the storage overhead associated with these boundary regions in both distributed-memory and shared-memory contexts. Understanding this relationship between a partition's volume (computation) and its surface area (communication) is crucial for designing efficient algorithms on parallel architectures [@problem_id:3191809].", "problem": "You are modeling an explicit time-stepping finite-difference update of a scalar field on a two-dimensional uniform Cartesian grid with spacing $h$ that discretizes a rectangular subdomain of $n_{1}$ cells in the horizontal direction and $n_{2}$ cells in the vertical direction. Each subdomain is owned by one process in a distributed-memory setting and is assumed to be strictly interior (it has four neighbors). The numerical stencil is radius one (a five-point stencil), and each process allocates a one-cell-thick ghost layer around its $n_{1} \\times n_{2}$ interior cells. For ease of indexing and buffer packing, the ghost layer includes the corner ghost cells, so the local allocation is $(n_{1}+2) \\times (n_{2}+2)$. Within a single-node shared-memory setting, you instead tile computation into tiles of $t_{1} \\times t_{2}$ interior cells, and for each tile you stage a one-cell-thick halo (including corners) into fast memory before updating the $t_{1} \\times t_{2}$ interior.\n\nStarting only from basic geometric facts for rectangles (perimeter equals twice the sum of side lengths; area equals the product of side lengths) and the definition of a one-cell-thick halo as the set difference between a padded rectangle and its interior rectangle, perform the following reasoning steps:\n\n- Derive an expression for the total physical length $H$ of the interprocess interfaces surrounding a single distributed-memory subdomain in terms of $h$, $n_{1}$, and $n_{2}$.\n- From the discrete structure, derive the count $G_{D}$ of ghost cells stored per distributed-memory subdomain (include all corners), and the corresponding dimensionless storage overhead fraction $f_{D} = G_{D}/(n_{1} n_{2})$.\n- For the shared-memory tile, derive the count $G_{S}$ of extra elements staged beyond the $t_{1} \\times t_{2}$ interior (include corners), and the corresponding dimensionless staging overhead fraction $f_{S} = G_{S}/(t_{1} t_{2})$.\n- Define the ratio $R$ of distributed-memory halo overhead fraction to shared-memory tile boundary overhead fraction by $R = f_{D}/f_{S}$, and simplify $R$ to a closed-form expression in terms of $n_{1}$, $n_{2}$, $t_{1}$, and $t_{2}$.\n\nProvide your final answer as the simplified analytic expression for $R$. Do not substitute numerical values. Express your final answer without any units.", "solution": "The problem involves two settings: a distributed-memory subdomain with a ghost layer and a shared-memory tile with a staged halo. The derivations can be built from basic geometry and the combinatorics of rectangular grids.\n\nFirst, consider the physical geometry of the subdomain. The interior region comprises $n_{1}$ cells horizontally and $n_{2}$ cells vertically. Each cell has width $h$ and height $h$, so the physical side lengths of the interior rectangle are $n_{1} h$ and $n_{2} h$. The total physical length of the interprocess interfaces that surround this interior subdomain is the perimeter of this rectangle. By the elementary fact that the perimeter of a rectangle equals twice the sum of its side lengths, we obtain\n$$\nH = 2\\big(n_{1} h + n_{2} h\\big) = 2 h \\big(n_{1} + n_{2}\\big).\n$$\n\nSecond, consider the discrete halo for the distributed-memory subdomain when a one-cell-thick layer including corners is allocated. The padded array is $(n_{1}+2) \\times (n_{2}+2)$, and the interior is $n_{1} \\times n_{2}$. The ghost region is the set difference, so the total number of ghost cells is the difference in cell counts:\n$$\nG_{D} = (n_{1}+2)(n_{2}+2) - n_{1} n_{2} = 2 n_{1} + 2 n_{2} + 4.\n$$\nThe dimensionless storage overhead fraction is the ghost count normalized by the interior area in cells:\n$$\nf_{D} = \\frac{G_{D}}{n_{1} n_{2}} = \\frac{2 n_{1} + 2 n_{2} + 4}{n_{1} n_{2}}.\n$$\n\nThird, for the shared-memory tile of size $t_{1} \\times t_{2}$ interior cells with a one-cell-thick staged halo (including corners), the same padded-versus-interior logic applies. The total staged region has $(t_{1}+2)(t_{2}+2)$ elements, of which $t_{1} t_{2}$ are interior. Therefore, the extra elements staged are\n$$\nG_{S} = (t_{1}+2)(t_{2}+2) - t_{1} t_{2} = 2 t_{1} + 2 t_{2} + 4,\n$$\nand the dimensionless staging overhead fraction is\n$$\nf_{S} = \\frac{G_{S}}{t_{1} t_{2}} = \\frac{2 t_{1} + 2 t_{2} + 4}{t_{1} t_{2}}.\n$$\n\nFinally, define the ratio of the distributed-memory halo overhead fraction to the shared-memory tile boundary overhead fraction:\n$$\nR = \\frac{f_{D}}{f_{S}} = \\frac{\\dfrac{2 n_{1} + 2 n_{2} + 4}{n_{1} n_{2}}}{\\dfrac{2 t_{1} + 2 t_{2} + 4}{t_{1} t_{2}}}.\n$$\nSimplifying this complex fraction by multiplying numerator and denominator appropriately yields\n$$\nR = \\frac{(2 n_{1} + 2 n_{2} + 4)\\, t_{1} t_{2}}{(2 t_{1} + 2 t_{2} + 4)\\, n_{1} n_{2}}.\n$$\nThis closed-form expression compares how large the fractional halo overhead is in distributed memory versus the fractional boundary staging overhead in shared-memory tiling, purely as a function of the interior dimensions $n_{1}$, $n_{2}$, $t_{1}$, and $t_{2}$, derived from fundamental perimeter and set-difference counting principles.", "answer": "$$\\boxed{\\frac{(2 n_{1} + 2 n_{2} + 4)\\, t_{1} t_{2}}{(2 t_{1} + 2 t_{2} + 4)\\, n_{1} n_{2}}}$$", "id": "3191809"}, {"introduction": "The performance of parallel systems is often dictated by how they handle contention for shared resources, which manifests differently in shared versus distributed architectures. This exercise models the classic \"ping-pong\" effect to provide a quantitative comparison between cache coherence traffic in shared memory and network messaging in distributed memory. By analyzing this access pattern, you will gain a deeper intuition for the distinct performance bottlenecks inherent to each programming model [@problem_id:3191797].", "problem": "A developer is analyzing the performance differences between a shared memory system and a distributed memory system. In the shared memory system, two threads on separate Central Processing Unit (CPU) cores alternately write to the same memory location, assumed to be in the same cache line, provoking cache coherence traffic. In the distributed memory system, two processes alternately send fixed-size messages to each other over a network. The aim is to reason from fundamental principles to construct a quantitative model and implement a program that computes the total time for a given number of events and counts the coherence or messaging events.\n\nUse the following foundational base:\n\n- Cache coherence in a shared memory system enforces a single-writer invariant per cache line. When a thread without ownership attempts to write a cache line owned by another core, a coherence protocol performs invalidation and transfers ownership, incurring additional latency relative to a local write. This is a well-tested observation in modern multiprocessors. Let a cache line have size $C$ bytes. Let the per-event local write time be $t_{w}$ (in nanoseconds). Let the coherence transfer incur a latency $L_{\\text{coh}}$ (in nanoseconds) plus a data movement time determined by a sustained memory system bandwidth $B_{\\text{sm}}$ given in gigabytes per second, using the gigabyte definition $1\\,\\text{GB}=10^{9}$ bytes. The model must reflect that coherence transfers only occur when the writer is not the current owner of the cache line.\n\n- Message passing over a network has a latency-bandwidth behavior: the time to send a message of size $M$ bytes is the sum of a fixed latency $L_{\\text{net}}$ (in nanoseconds), a transmission time governed by link bandwidth $B_{\\text{net}}$ in gigabytes per second (with $1\\,\\text{GB}=10^{9}$ bytes), and a per-message software overhead $o$ (in nanoseconds). This is a widely accepted latency-bandwidth model.\n\nThe developer must consider a synthetic access pattern in both systems:\n\n- Shared memory ping-pong: two threads alternately write to the same logical location. The location is at an address stride $s$ bytes apart within an array. If $s<C$ then the two writes target the same cache line (shared cache line), creating false sharing and coherence transfers; if $s \\ge C$ then the writes target different cache lines, avoiding false sharing and coherence transfers. The number of alternating write events is $E$.\n\n- Distributed ping-pong: two processes alternately send messages of size $M$ bytes back and forth for a total of $E$ events.\n\nYour tasks:\n\n1. From the base above, derive a model for the shared memory total time $T_{\\text{sm}}(E)$ that accounts for local writes and coherence transfers under the alternating write pattern, and a count $X_{\\text{sm}}(E)$ of coherence transfer events. Clearly define the conditions under which a coherence transfer occurs in this alternation.\n\n2. From the latency-bandwidth base, derive a model for the distributed memory total time $T_{\\text{dist}}(E)$ and the count $X_{\\text{dist}}(E)$ of messages under alternating sends.\n\n3. Implement a program that, for each test case in the suite below, computes $[T_{\\text{sm}}(E), X_{\\text{sm}}(E), T_{\\text{dist}}(E), X_{\\text{dist}}(E)]$. All times must be expressed in nanoseconds, as floating-point numbers. Event counts must be integers.\n\n4. The final output format must be a single line containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[T_{\\text{sm}}(E), X_{\\text{sm}}(E), T_{\\text{dist}}(E), X_{\\text{dist}}(E)]$, with no spaces. For example, the overall output should look like $[[a_{1},b_{1},c_{1},d_{1}],[a_{2},b_{2},c_{2},d_{2}],\\dots]$.\n\nPhysical units: all time quantities must be reported in nanoseconds. Bandwidths $B_{\\text{sm}}$ and $B_{\\text{net}}$ are given in gigabytes per second, where $1\\,\\text{GB}=10^{9}$ bytes. Angles are not used. Percentages are not used.\n\nTest suite (each case is a tuple $(E, C, s, t_{w}, L_{\\text{coh}}, B_{\\text{sm}}, L_{\\text{net}}, B_{\\text{net}}, o, M)$):\n\n- Case $1$ (general case with false sharing): $(E=\\;1000,\\;C=\\;64,\\;s=\\;8,\\;t_{w}=\\;3,\\;L_{\\text{coh}}=\\;100,\\;B_{\\text{sm}}=\\;40,\\;L_{\\text{net}}=\\;5000,\\;B_{\\text{net}}=\\;10,\\;o=\\;200,\\;M=\\;64)$.\n\n- Case $2$ (boundary, single event): $(E=\\;1,\\;C=\\;64,\\;s=\\;8,\\;t_{w}=\\;3,\\;L_{\\text{coh}}=\\;100,\\;B_{\\text{sm}}=\\;40,\\;L_{\\text{net}}=\\;5000,\\;B_{\\text{net}}=\\;10,\\;o=\\;200,\\;M=\\;64)$.\n\n- Case $3$ (no false sharing, stride crosses cache line): $(E=\\;1000,\\;C=\\;64,\\;s=\\;128,\\;t_{w}=\\;3,\\;L_{\\text{coh}}=\\;100,\\;B_{\\text{sm}}=\\;40,\\;L_{\\text{net}}=\\;5000,\\;B_{\\text{net}}=\\;10,\\;o=\\;200,\\;M=\\;64)$.\n\n- Case $4$ (fast shared memory link, slower network): $(E=\\;1000,\\;C=\\;64,\\;s=\\;8,\\;t_{w}=\\;2,\\;L_{\\text{coh}}=\\;10,\\;B_{\\text{sm}}=\\;200,\\;L_{\\text{net}}=\\;20000,\\;B_{\\text{net}}=\\;5,\\;o=\\;100,\\;M=\\;64)$.\n\n- Case $5$ (larger message size effect): $(E=\\;500,\\;C=\\;64,\\;s=\\;8,\\;t_{w}=\\;3,\\;L_{\\text{coh}}=\\;100,\\;B_{\\text{sm}}=\\;40,\\;L_{\\text{net}}=\\;5000,\\;B_{\\text{net}}=\\;10,\\;o=\\;200,\\;M=\\;1024)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, exactly ordered as the test suite and with each test case’s result formatted as $[T_{\\text{sm}}(E), X_{\\text{sm}}(E), T_{\\text{dist}}(E), X_{\\text{dist}}(E)]$.", "solution": "The user has provided a problem statement that requires validation and, if valid, a complete solution.\n\n### Step 1: Extract Givens\nThe following parameters and definitions are provided in the problem statement:\n- System parameters for shared memory:\n    - $C$: cache line size in bytes.\n    - $t_{w}$: per-event local write time in nanoseconds.\n    - $L_{\\text{coh}}$: coherence transfer latency in nanoseconds.\n    - $B_{\\text{sm}}$: sustained memory system bandwidth in gigabytes per second ($1\\,\\text{GB}=10^{9}$ bytes).\n- System parameters for distributed memory:\n    - $M$: message size in bytes.\n    - $L_{\\text{net}}$: network latency in nanoseconds.\n    - $B_{\\text{net}}$: network link bandwidth in gigabytes per second ($1\\,\\text{GB}=10^{9}$ bytes).\n    - $o$: per-message software overhead in nanoseconds.\n- Access pattern parameters:\n    - $E$: total number of alternating write or send events.\n    - $s$: address stride in bytes for the shared memory case.\n- Conditions for shared memory coherence:\n    - A coherence transfer occurs if $s < C$.\n    - No coherence transfer occurs if $s \\ge C$.\n- Modeling requirements:\n    - Derive models for total time $T_{\\text{sm}}(E)$ and event count $X_{\\text{sm}}(E)$ for the shared memory system.\n    - Derive models for total time $T_{\\text{dist}}(E)$ and event count $X_{\\text{dist}}(E)$ for the distributed memory system.\n- Output requirements:\n    - For each test case, compute $[T_{\\text{sm}}(E), X_{\\text{sm}}(E), T_{\\text{dist}}(E), X_{\\text{dist}}(E)]$.\n    - Times must be floating-point numbers in nanoseconds.\n    - Event counts must be integers.\n- Test suite: A set of $5$ test cases providing numerical values for all parameters.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the specified criteria:\n- **Scientifically Grounded**: The problem is based on established, albeit simplified, performance models in computer architecture and high-performance computing. The latency-bandwidth model for message passing and the latency-plus-transfer-time model for cache coherence are standard first-order approximations. The concept of false sharing due to cache line granularity is a fundamental topic in parallel programming. The problem is scientifically sound.\n- **Well-Posed**: All necessary parameters are defined, the objectives are clear, and the required outputs are specified. The problem is self-contained and structured to lead to a unique, stable solution for each test case.\n- **Objective**: The problem is stated using precise, technical language, free from subjectivity or ambiguity.\n\nThe problem does not exhibit any of the invalidity flags. It is a well-defined exercise in computational performance modeling.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be developed.\n\n### Principle-Based Solution Derivation\n\nThe objective is to derive quantitative models for the total time and event count for two different parallel programming paradigms under a \"ping-pong\" access pattern.\n\n#### 1. Shared Memory System Model ($T_{\\text{sm}}$, $X_{\\text{sm}}$)\nIn the shared memory ping-pong scenario, two threads alternately write to a memory location. The key factor is whether these writes contend for the same cache line.\n\n**Condition for Coherence latency:** A coherence event is triggered when a thread attempts to write to a cache line for which it does not have ownership. This happens if the two threads' write targets fall within the same cache line. Given the cache line size $C$ and the address stride $s$ between the two threads' write locations, a coherence-inducing \"false sharing\" or \"true sharing\" scenario occurs if $s < C$. If $s \\ge C$, the write locations are guaranteed to be in different cache lines, and the threads operate independently without provoking coherence traffic between their cores.\n\n**Coherence Event Count ($X_{\\text{sm}}$):** We consider an alternating sequence of $E$ write events. Let's assume the first write (event $1$) by Thread A establishes its ownership of the cache line. This initial write does not involve a transfer from another core. The second write (event $2$) by Thread B requires ownership to be transferred from Thread A's cache to Thread B's cache. The third write (event $3$) by Thread A requires ownership to be transferred back from B to A. This pattern continues for all subsequent writes. Therefore, for $E$ total write events, a coherence transfer occurs for events $2, 3, \\dots, E$. The number of coherence transfers is $E-1$. If $E \\le 1$, no transfers occur. This logic applies only if $s < C$.\n\nThe number of coherence events, $X_{\\text{sm}}$, is thus:\n$$ X_{\\text{sm}}(E) = \\begin{cases} \\max(0, E-1) & \\text{if } s < C \\\\ 0 & \\text{if } s \\ge C \\end{cases} $$\nThe result must be an integer.\n\n**Total Time ($T_{\\text{sm}}$):** The total time is the sum of the time for all local writes and the time for all coherence transfers.\nEach of the $E$ events involves a local write, costing $t_w$.\nThe cost of a single coherence transfer is the sum of a fixed latency $L_{\\text{coh}}$ and a data movement time. The data moved is one cache line of size $C$. The bandwidth $B_{\\text{sm}}$ is given in GB/s. We convert this to bytes per nanosecond. Since $1\\,\\text{GB} = 10^9\\,\\text{bytes}$ and $1\\,\\text{s} = 10^9\\,\\text{ns}$, a bandwidth of $B_{\\text{sm}}\\,\\text{GB/s}$ is numerically equal to $B_{\\text{sm}}\\,\\text{bytes/ns}$.\nThe time to transfer one cache line is $t_{\\text{transfer}} = \\frac{C}{B_{\\text{sm}}}$.\nThe cost of one coherence event is therefore $L_{\\text{coh}} + \\frac{C}{B_{\\text{sm}}}$.\n\nThe total time $T_{\\text{sm}}(E)$ is the sum of $E$ local writes and $X_{\\text{sm}}(E)$ coherence events:\n$$ T_{\\text{sm}}(E) = (E \\cdot t_w) + X_{\\text{sm}}(E) \\cdot \\left(L_{\\text{coh}} + \\frac{C}{B_{\\text{sm}}}\\right) $$\nSubstituting the expression for $X_{\\text{sm}}(E)$:\n$$ T_{\\text{sm}}(E) = E \\cdot t_w + \\begin{cases} \\max(0, E-1) \\cdot \\left(L_{\\text{coh}} + \\frac{C}{B_{\\text{sm}}}\\right) & \\text{if } s < C \\\\ 0 & \\text{if } s \\ge C \\end{cases} $$\n\n#### 2. Distributed Memory System Model ($T_{\\text{dist}}$, $X_{\\text{dist}}$)\nIn the distributed memory ping-pong, two processes alternately send messages to each other.\n\n**Message Event Count ($X_{\\text{dist}}$):** The problem states that the processes engage in a total of $E$ send events. Each send constitutes a distinct message passing event. Thus, the total count of messages is simply $E$.\n$$ X_{\\text{dist}}(E) = E $$\nThe result must be an integer.\n\n**Total Time ($T_{\\text{dist}}$):** The total time is the sum of the costs of all $E$ message sends. The problem describes a latency-bandwidth model for a single message send. The time to send one message of size $M$ is the sum of a fixed network latency $L_{\\text{net}}$, a per-message software overhead $o$, and a transmission time.\nThe transmission time is determined by the message size $M$ and the network bandwidth $B_{\\text{net}}$. As with $B_{\\text{sm}}$, $B_{\\text{net}}$ in GB/s is numerically equal to $B_{\\text{net}}$ in bytes/ns.\nThe transmission time is $t_{\\text{transmit}} = \\frac{M}{B_{\\text{net}}}$.\nThe total cost per message is $L_{\\text{net}} + o + \\frac{M}{B_{\\text{net}}}$.\n\nSince the $E$ events are sequential in a ping-pong pattern, the total time is $E$ multiplied by the cost of a single event.\n$$ T_{\\text{dist}}(E) = E \\cdot \\left( L_{\\text{net}} + o + \\frac{M}{B_{\\text{net}}} \\right) $$\n\nThese derived models will be implemented to compute the results for the provided test suite.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total time and event counts for shared and distributed memory\n    ping-pong scenarios based on provided performance models.\n    \"\"\"\n\n    # Test suite: (E, C, s, t_w, L_coh, B_sm, L_net, B_net, o, M)\n    test_cases = [\n        # Case 1: general case with false sharing\n        (1000, 64, 8, 3, 100, 40, 5000, 10, 200, 64),\n        # Case 2: boundary, single event\n        (1, 64, 8, 3, 100, 40, 5000, 10, 200, 64),\n        # Case 3: no false sharing, stride crosses cache line\n        (1000, 64, 128, 3, 100, 40, 5000, 10, 200, 64),\n        # Case 4: fast shared memory link, slower network\n        (1000, 64, 8, 2, 10, 200, 20000, 5, 100, 64),\n        # Case 5: larger message size effect\n        (500, 64, 8, 3, 100, 40, 5000, 10, 200, 1024),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        E, C, s, t_w, L_coh, B_sm, L_net, B_net, o, M = case\n\n        # Shared Memory Calculation\n        # Bandwidth B_sm is in GB/s, which is numerically equivalent to bytes/ns.\n        # Check for false sharing condition s  C\n        if s  C:\n            # Coherence transfers occur for every event except the first one.\n            X_sm = max(0, E - 1)\n            # Cost of a single coherence event = latency + data transfer time\n            coherence_cost = float(L_coh) + float(C) / float(B_sm)\n            # Total time = sum of all local write times + sum of all coherence costs\n            T_sm = float(E * t_w) + float(X_sm) * coherence_cost\n        else:\n            # No false sharing means no coherence transfers.\n            X_sm = 0\n            # Total time is just the sum of local write times.\n            T_sm = float(E * t_w)\n\n        # Distributed Memory Calculation\n        # Bandwidth B_net is in GB/s, which is numerically equivalent to bytes/ns.\n        # Number of message events is the total number of events E.\n        X_dist = E\n        # Cost of a single message = latency + overhead + transmission time\n        message_cost = float(L_net) + float(o) + float(M) / float(B_net)\n        # Total time is the number of events multiplied by the cost per event.\n        T_dist = float(E) * message_cost\n\n        # Ensure event counts are integers\n        X_sm = int(X_sm)\n        X_dist = int(X_dist)\n\n        # Append results for this case\n        all_results.append([T_sm, X_sm, T_dist, X_dist])\n\n    # Format the final output string precisely as specified, without spaces.\n    inner_results_str = [\n        f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results\n    ]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3191797"}]}