{"hands_on_practices": [{"introduction": "A GPU's massive parallelism is constrained by the finite resources on each of its Streaming Multiprocessors (SMs). This exercise provides a practical guide to calculating \"occupancy,\" a critical metric that measures how effectively a kernel utilizes the available parallel execution capacity. By working through this problem, you will learn how to identify the limiting resource for a given kernel—be it registers, shared memory, or thread capacity—and understand how this directly impacts the potential for latency hiding and overall performance [@problem_id:3139038].", "problem": "Consider a Graphics Processing Unit (GPU) composed of identical Streaming Multiprocessors (SMs). Each SM has finite hardware resources: a register file of capacity $R_{\\mathrm{SM}}$ (counted as the number of $32$-bit registers), a shared memory capacity of $S_{\\mathrm{SM}}$ bytes, a thread slot capacity of $T_{\\mathrm{SM}}$ threads, and an architectural limit of at most $B_{\\max}$ resident blocks at any time. A thread block with $b$ threads consumes registers because each thread uses $r$ registers, and it consumes shared memory because each block allocates $s$ bytes in shared memory. The basic constraints are: the total registers used by all resident blocks on an SM cannot exceed $R_{\\mathrm{SM}}$, the total shared memory used cannot exceed $S_{\\mathrm{SM}}$, the total threads cannot exceed $T_{\\mathrm{SM}}$, and the number of blocks cannot exceed $B_{\\max}$. All counts are integers, and an SM schedules whole blocks only. Occupancy is the fraction of thread slots on an SM that are filled by resident threads and is defined as the number of resident threads divided by $T_{\\mathrm{SM}}$, with the understanding that it cannot exceed $1$.\n\nYou are given a specific GPU with $R_{\\mathrm{SM}}=65536$, $S_{\\mathrm{SM}}=98304\\,\\mathrm{bytes}$, $T_{\\mathrm{SM}}=2048$, and $B_{\\max}=16$. Consider two kernels:\n\n- Kernel A: per-thread registers $r=64$, per-block shared memory $s=12288\\,\\mathrm{bytes}$, threads per block $b=256$.\n- Kernel B (baseline): per-thread registers $r_{0}=32$, per-block shared memory $s_{0}=8192\\,\\mathrm{bytes}$, threads per block $b_{0}=128$.\n\nUsing first principles and the resource limits described above, determine for each kernel the maximum number of resident blocks per SM that can be scheduled concurrently. From these, compute the occupancy of each kernel on a single SM. Finally, assume a simplified performance model in which throughput is proportional to occupancy and runtime is inversely proportional to throughput. Under this model, compute the ratio of runtime of Kernel A to the runtime of Kernel B (that is, $\\text{runtime}_{A}/\\text{runtime}_{B}$). Round your final answer to four significant figures and express it as a dimensionless decimal.", "solution": "The problem is first validated against the given criteria.\n\n**Step 1: Extract Givens**\nThe following data and definitions are provided:\n- GPU Streaming Multiprocessor (SM) resources:\n  - Register file capacity: $R_{\\mathrm{SM}} = 65536$ registers.\n  - Shared memory capacity: $S_{\\mathrm{SM}} = 98304$ bytes.\n  - Thread slot capacity: $T_{\\mathrm{SM}} = 2048$ threads.\n  - Maximum resident blocks: $B_{\\max} = 16$.\n- Kernel resource consumption per block:\n  - Registers per thread: $r$.\n  - Threads per block: $b$.\n  - Shared memory per block: $s$.\n- Constraints for $N$ resident blocks:\n  1. Total registers: $N \\times b \\times r \\le R_{\\mathrm{SM}}$.\n  2. Total shared memory: $N \\times s \\le S_{\\mathrm{SM}}$.\n  3. Total threads: $N \\times b \\le T_{\\mathrm{SM}}$.\n  4. Total blocks: $N \\le B_{\\max}$.\n- Kernel A parameters:\n  - $r_A = 64$ registers per thread.\n  - $s_A = 12288$ bytes per block.\n  - $b_A = 256$ threads per block.\n- Kernel B parameters (baseline):\n  - $r_B = 32$ registers per thread.\n  - $s_B = 8192$ bytes per block.\n  - $b_B = 128$ threads per block.\n- Occupancy definition: $\\frac{\\text{Number of resident threads}}{T_{\\mathrm{SM}}}$.\n- Performance Model: Throughput is proportional to occupancy, and runtime is inversely proportional to throughput.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed for validity:\n- **Scientifically Grounded:** The problem uses a standard, simplified model for GPU resource allocation and occupancy calculation. The concepts of registers, shared memory, thread blocks, and SM limits are fundamental to GPU architecture and programming (e.g., in CUDA). The provided values for hardware resources and kernel configurations are realistic for modern GPUs.\n- **Well-Posed:** All necessary variables, constants, and relationships are explicitly defined. The problem is self-contained and provides a clear path to a unique numerical solution.\n- **Objective:** The problem is stated in precise, quantitative terms, free from any subjective or ambiguous language.\n\nThe problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, contradiction, or infeasibility). It is a well-defined computational problem in the domain of computational science.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be provided.\n\n**Solution Derivation**\n\nThe core of the problem is to determine the maximum number of blocks, $N$, that can be concurrently resident on a single SM for each kernel. This number is constrained by the SM's finite resources. Since an SM can only schedule whole blocks, the maximum number of blocks supported by each resource limit is the floor of the ratio of total resource capacity to the per-block resource consumption. The overall maximum number of resident blocks, $N$, is the minimum of these individual limits.\n\nFor a kernel with $b$ threads per block, $r$ registers per thread, and $s$ bytes of shared memory per block, the maximum number of blocks, $N$, is given by:\n$N = \\min( N_{\\text{reg}}, N_{\\text{smem}}, N_{\\text{thread}}, N_{\\text{block}} )$\nwhere:\n- $N_{\\text{reg}} = \\lfloor \\frac{R_{\\mathrm{SM}}}{b \\times r} \\rfloor$ (limit from registers)\n- $N_{\\text{smem}} = \\lfloor \\frac{S_{\\mathrm{SM}}}{s} \\rfloor$ (limit from shared memory)\n- $N_{\\text{thread}} = \\lfloor \\frac{T_{\\mathrm{SM}}}{b} \\rfloor$ (limit from thread slots)\n- $N_{\\text{block}} = B_{\\max}$ (architectural limit on blocks)\n\nOnce $N$ is found for each kernel, the occupancy can be calculated. The number of resident threads is $N \\times b$. Occupancy, $O$, is then:\n$O = \\frac{N \\times b}{T_{\\mathrm{SM}}}$\n\nFinally, the runtime ratio is derived from the performance model.\nRuntime $\\propto \\frac{1}{\\text{Throughput}}$ and Throughput $\\propto O$.\nTherefore, Runtime $\\propto \\frac{1}{O}$.\nThe ratio of runtimes is $\\frac{\\text{runtime}_A}{\\text{runtime}_B} = \\frac{1/O_A}{1/O_B} = \\frac{O_B}{O_A}$.\n\n**Analysis for Kernel A**\n- Parameters: $r_A=64$, $s_A=12288$, $b_A=256$.\n- SM limits: $R_{\\mathrm{SM}}=65536$, $S_{\\mathrm{SM}}=98304$, $T_{\\mathrm{SM}}=2048$, $B_{\\max}=16$.\n\n1.  Calculate individual block limits for Kernel A:\n    - Register limit: $N_{A, \\text{reg}} = \\lfloor \\frac{65536}{256 \\times 64} \\rfloor = \\lfloor \\frac{65536}{16384} \\rfloor = \\lfloor 4 \\rfloor = 4$.\n    - Shared memory limit: $N_{A, \\text{smem}} = \\lfloor \\frac{98304}{12288} \\rfloor = \\lfloor 8 \\rfloor = 8$.\n    - Thread limit: $N_{A, \\text{thread}} = \\lfloor \\frac{2048}{256} \\rfloor = \\lfloor 8 \\rfloor = 8$.\n    - Architectural block limit: $N_{A, \\text{block}} = 16$.\n\n2.  Determine maximum resident blocks for Kernel A:\n    $N_A = \\min(4, 8, 8, 16) = 4$ blocks.\n    The limiting factor for Kernel A is the register file capacity.\n\n3.  Calculate occupancy for Kernel A:\n    Number of resident threads = $N_A \\times b_A = 4 \\times 256 = 1024$.\n    $O_A = \\frac{1024}{T_{\\mathrm{SM}}} = \\frac{1024}{2048} = 0.5$.\n\n**Analysis for Kernel B**\n- Parameters: $r_B=32$, $s_B=8192$, $b_B=128$.\n- SM limits are the same.\n\n1.  Calculate individual block limits for Kernel B:\n    - Register limit: $N_{B, \\text{reg}} = \\lfloor \\frac{65536}{128 \\times 32} \\rfloor = \\lfloor \\frac{65536}{4096} \\rfloor = \\lfloor 16 \\rfloor = 16$.\n    - Shared memory limit: $N_{B, \\text{smem}} = \\lfloor \\frac{98304}{8192} \\rfloor = \\lfloor 12 \\rfloor = 12$.\n    - Thread limit: $N_{B, \\text{thread}} = \\lfloor \\frac{2048}{128} \\rfloor = \\lfloor 16 \\rfloor = 16$.\n    - Architectural block limit: $N_{B, \\text{block}} = 16$.\n\n2.  Determine maximum resident blocks for Kernel B:\n    $N_B = \\min(16, 12, 16, 16) = 12$ blocks.\n    The limiting factor for Kernel B is the shared memory capacity.\n\n3.  Calculate occupancy for Kernel B:\n    Number of resident threads = $N_B \\times b_B = 12 \\times 128 = 1536$.\n    $O_B = \\frac{1536}{T_{\\mathrm{SM}}} = \\frac{1536}{2048} = \\frac{3}{4} = 0.75$.\n\n**Calculation of Runtime Ratio**\nThe ratio of the runtime of Kernel A to the runtime of Kernel B is:\n$\\frac{\\text{runtime}_A}{\\text{runtime}_B} = \\frac{O_B}{O_A} = \\frac{0.75}{0.5} = 1.5$.\n\nThe problem requires the answer to be rounded to four significant figures.\n$1.5$ is expressed as $1.500$.", "answer": "$$\\boxed{1.500}$$", "id": "3139038"}, {"introduction": "High occupancy is necessary but not sufficient for good performance; threads must also access memory efficiently. GPUs achieve high memory throughput by \"coalescing\" individual memory requests from threads in a warp into a minimal number of transactions. This practice demonstrates the significant performance penalty of uncoalesced, or strided, memory access, and you will quantify the number of wasted transactions and calculate the precise data padding needed to restore optimal, aligned access [@problem_id:3138937].", "problem": "A Graphics Processing Unit (GPU) executes memory operations in warps of $32$ threads. Global memory is served in fixed-size segments of $128$ bytes, and a warp generates exactly one memory transaction for each distinct $128$-byte segment that contains any byte of the warp’s requested words. Consider a kernel where each thread in a warp loads a single $4$-byte floating-point value from an array in global memory. The address accessed by thread $t \\in \\{0,1,\\ldots,31\\}$ is\n$$\nA(t) \\;=\\; A_{0} \\;+\\; \\delta \\;+\\; t \\, s \\, b,\n$$\nwhere $A_{0}$ is aligned to a $128$-byte boundary, $\\delta$ is the byte offset from that boundary, $s$ is the stride in elements between consecutive threads, and $b=4$ bytes is the element size. Assume that no individual $4$-byte access straddles a $128$-byte boundary for the parameters below.\n\nSuppose $s=3$ and $\\delta=44$ bytes.\n\nUsing only the definitions above and elementary integer arithmetic, determine the number of $128$-byte transactions generated by one warp for these parameters. Then, determine the minimal nonnegative padding $p$ (in bytes) that must be added to the base address (i.e., replace $\\delta$ by $\\delta+p$) so that the first accessed address is aligned to a $128$-byte segment boundary, and recompute the number of $128$-byte transactions under this padding.\n\nReport your reasoning in full, but for grading purposes provide only the final minimal padding amount $p$ as your answer. Express the final padding in bytes. No rounding is needed.", "solution": "The problem asks for the minimal non-negative padding $p$ required to align the starting address of a memory access pattern to a $128$-byte segment boundary, and to determine the number of memory transactions both with and without this padding.\n\nFirst, let us formalize the given information. A warp consists of $32$ threads, indexed by $t \\in \\{0,\n1, \\ldots, 31\\}$. Each thread accesses a $4$-byte word. The memory segment size is $128$ bytes. The starting byte address accessed by thread $t$ is given by the function:\n$$A(t) = A_0 + \\delta + t \\cdot s \\cdot b$$\nwhere $A_0$ is a base address aligned to a $128$-byte boundary (i.e., $A_0 \\pmod{128} = 0$), $\\delta$ is an initial byte offset, $s$ is the stride in elements, and $b=4$ bytes is the size of one element. A memory transaction is generated for each unique $128$-byte segment accessed by any thread in the warp. The segment containing a byte at address $A$ is identified by the index $\\lfloor A / 128 \\rfloor$.\n\nThe specific parameters for the initial case are $s=3$ and $\\delta=44$ bytes. The address accessed by thread $t$ is therefore:\n$$A(t) = A_0 + 44 + t \\cdot 3 \\cdot 4 = A_0 + 44 + 12t$$\nSince $A_0$ is aligned to a $128$-byte boundary, the segment index for address $A(t)$ is $\\lfloor (A_0 + 44 + 12t) / 128 \\rfloor = A_0/128 + \\lfloor (44 + 12t) / 128 \\rfloor$. The constant term $A_0/128$ merely shifts all segment indices by an integer amount and does not affect the number of unique segments accessed. We can therefore analyze the memory access pattern by considering the addresses relative to $A_0$, which we denote by $A_{rel}(t) = 44 + 12t$.\n\nTo find the number of transactions, we must determine the number of unique segment indices, $\\lfloor A_{rel}(t) / 128 \\rfloor$, for $t \\in \\{0, 1, \\ldots, 31\\}$.\nThe first thread ($t=0$) accesses the address $A_{rel}(0) = 44 + 12(0) = 44$.\nThe segment index for this access is $\\lfloor 44 / 128 \\rfloor = 0$.\n\nThe last thread ($t=31$) accesses the address $A_{rel}(31) = 44 + 12(31) = 44 + 372 = 416$.\nThe segment index for this access is $\\lfloor 416 / 128 \\rfloor = 3$.\n\nThe addresses accessed by the warp are monotonic, starting from $44$ and increasing with a step of $12$ bytes between consecutive threads. Since the stride in bytes ($12$) is less than the segment size ($128$), the threads will access all intermediate segments between the first and last segment. Therefore, the total number of unique segments accessed is the difference between the last and first segment indices, plus one.\nNumber of transactions = $(\\text{last segment index}) - (\\text{first segment index}) + 1 = 3 - 0 + 1 = 4$.\nSo, for the initial parameters, the warp generates $4$ memory transactions.\n\nNext, we must determine the minimal non-negative padding $p$ (in bytes) that must be added to the base address such that the first accessed address is aligned to a $128$-byte segment boundary. The new first address, for thread $t=0$, will be $A'(0) = A_0 + \\delta + p$.\nGiven $\\delta = 44$, the new first address is $A'(0) = A_0 + 44 + p$.\nFor $A'(0)$ to be aligned to a $128$-byte boundary, it must be a multiple of $128$.\n$$A_0 + 44 + p = k \\cdot 128$$\nfor some integer $k$. Since $A_0$ is already a multiple of $128$, we can write $A_0 = m \\cdot 128$ for some integer $m$.\n$$m \\cdot 128 + 44 + p = k \\cdot 128$$\n$$44 + p = (k-m) \\cdot 128$$\nLet $k' = k-m$. We need to find the smallest non-negative integer $p$ such that $44 + p = k' \\cdot 128$ for some integer $k'$.\nTo ensure $p \\ge 0$, we require $k' \\cdot 128 \\ge 44$. Since $k'$ must be an integer, the smallest possible value for $k'$ is $1$.\nFor $k' = 1$, we have:\n$$44 + p = 128 \\implies p = 128 - 44 = 84$$\nThe minimal non-negative padding required is $p=84$ bytes.\n\nFinally, we recompute the number of memory transactions with this padding. The new offset is $\\delta' = \\delta + p = 44 + 84 = 128$.\nThe new address function, relative to $A_0$, is:\n$$A'_{rel}(t) = \\delta' + t \\cdot s \\cdot b = 128 + t \\cdot 3 \\cdot 4 = 128 + 12t$$\nThe first thread ($t=0$) now accesses $A'_{rel}(0) = 128$. The segment index is $\\lfloor 128 / 128 \\rfloor = 1$.\nThe last thread ($t=31$) accesses $A'_{rel}(31) = 128 + 12(31) = 128 + 372 = 500$. The segment index is $\\lfloor 500 / 128 \\rfloor = 3$.\nThe number of transactions is again the difference in segment indices plus one:\nNumber of transactions = $3 - 1 + 1 = 3$.\nBy adding the padding of $84$ bytes, the number of memory transactions is reduced from $4$ to $3$, improving memory access efficiency.\n\nThe problem asks for the minimal padding amount $p$. Based on our calculation, this value is $84$.", "answer": "$$\n\\boxed{84}\n$$", "id": "3138937"}, {"introduction": "Effective GPU programming requires designing algorithms that align with the hardware's execution model. This case study compares two common strategies for a parallel reduction, a fundamental pattern in scientific computing. By analyzing the work and span of each approach, you will discover the crucial performance difference between expensive block-level synchronization and efficient, implicit warp-level cooperation, a key insight for any parallel algorithm designer [@problem_id:3138934].", "problem": "A Graphics Processing Unit (GPU) executes threads in Single Instruction Multiple Threads (SIMT) fashion: threads within a warp execute in lockstep, and a block-level barrier synchronization (for example, the Compute Unified Device Architecture (CUDA) function `__syncthreads()`) ensures all threads in a block reach the same point before any proceeds. In parallel algorithm analysis, the total work is the aggregate number of primitive operations, and the span (also called the critical path length) is the length of the longest chain of dependent steps that must be executed sequentially. Consider reducing $B$ floating-point values to a single sum within one block using shared memory. Assume each floating-point addition costs $T_{\\text{add}}$ cycles, each block-level barrier costs $T_{\\text{sync}}$ cycles, and you may ignore global memory transfer costs and any instruction-level overhead beyond these two costs.\n\nYou will compare two intra-block reduction strategies for $B=256$ threads and warp size $W=32$:\n\n1. A binary tree reduction implemented entirely in shared memory, where at each reduction level active threads perform additions of pairs of shared-memory elements, and a block-level barrier is used so that threads reconverge before the next level. Assume one barrier after the initial shared-memory write followed by one barrier per reduction level.\n\n2. A warp-centric reduction where each warp first reduces its $W$ values using only warp-synchronous instructions (no block-level barrier needed within a warp), then the lane-$0$ thread of each warp writes its partial sum to shared memory, a single block-level barrier ensures visibility of these partial sums, and finally a single warp reduces the $B/W$ partial sums using warp-synchronous instructions with no further block-level barriers.\n\nStarting from the core definitions of work and span, and the SIMT execution model described above, do the following:\n\n- Derive the work and span for each strategy as functions of $B$, $W$, $T_{\\text{add}}$, and $T_{\\text{sync}}$.\n- From those derivations, determine the number of block-level barrier synchronizations executed by each strategy and the peak shared-memory footprint measured as the number of $4$-byte slots concurrently occupied by partial sums at the most memory-intensive step.\n- Using your span expressions, compute the ratio $R$ of the predicted span time of the binary tree strategy to that of the warp-centric strategy for $B=256$, $W=32$, $T_{\\text{add}}=4$ cycles, and $T_{\\text{sync}}=80$ cycles.\n\nRound your final ratio $R$ to four significant figures. Report $R$ as a pure number (no units).", "solution": "The problem statement is critically reviewed and validated against the established criteria.\n\n### Step 1: Extract Givens\n- **Execution Model**: Single Instruction Multiple Threads (SIMT).\n- **Entities**: Threads are grouped into warps, which are grouped into blocks.\n- **Synchronization**: Warp-level execution is lockstep (implicitly synchronous). Block-level synchronization is explicit via a barrier (e.g., `__syncthreads()`).\n- **Definitions**:\n    - **Work**: Aggregate number of primitive operations.\n    - **Span**: Length of the longest chain of dependent steps (critical path length).\n- **Problem Setup**: Reduction of $B$ floating-point values to a single sum within one block.\n- **Cost Model**:\n    - Floating-point addition cost: $T_{\\text{add}}$ cycles.\n    - Block-level barrier cost: $T_{\\text{sync}}$ cycles.\n    - Global memory and other instruction overheads are to be ignored.\n- **Parameters**:\n    - Number of threads (and values): $B = 256$.\n    - Warp size: $W = 32$.\n    - $T_{\\text{add}} = 4$ cycles.\n    - $T_{\\text{sync}} = 80$ cycles.\n- **Strategy 1 (Binary Tree Reduction)**:\n    - Implemented entirely in shared memory.\n    - At each reduction level, active threads perform additions of pairs.\n    - A block-level barrier is used to reconverge threads before the next level.\n    - Assumption: One barrier after the initial shared-memory write, followed by one barrier per reduction level.\n- **Strategy 2 (Warp-Centric Reduction)**:\n    - Each warp reduces its $W$ values using warp-synchronous instructions (no block-level barriers).\n    - Lane-$0$ thread of each warp writes its partial sum to shared memory.\n    - A single block-level barrier ensures visibility of partial sums.\n    - A single warp reduces the $B/W$ partial sums using warp-synchronous instructions.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the principles of parallel computing and GPU architecture. The SIMT model, work-span analysis, and the two reduction strategies are standard concepts in computational science.\n- **Well-Posed**: The problem is clearly defined with specific parameters, cost models, and algorithmic descriptions. It asks for a unique, computable ratio.\n- **Objective**: The language is precise and technical, free of subjective claims. The assumptions for the cost model are stated explicitly.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined exercise in parallel algorithm analysis. A complete solution will be provided.\n\n### Solution Derivation\n\nWe begin by deriving the symbolic expressions for work, span, number of barriers, and shared memory footprint for each strategy.\n\nThe total work for reducing $B$ numbers to a single sum is the number of additions required, which is always $B-1$. Therefore, for both strategies, the work is $(B-1)T_{\\text{add}}$. We will focus on the span, which differs significantly.\n\n**Strategy 1: Binary Tree Reduction**\n\n1.  **Work ($W_1$)**: The total number of additions is $B-1$. The total work is $W_1 = (B-1) T_{\\text{add}}$.\n\n2.  **Span ($S_1$)**: The reduction of $B$ elements occurs in $\\log_2(B)$ levels. According to the problem statement, there is an initial barrier after data is loaded into shared memory, and then one barrier after each reduction level. The critical path consists of a sequence of additions and barrier synchronizations.\n    - A thread on the critical path first waits at an initial barrier: cost $T_{\\text{sync}}$.\n    - It then participates in $\\log_2(B)$ levels of reduction. Each level involves one addition followed by a barrier. The cost of each level for a thread on the critical path is $T_{\\text{add}} + T_{\\text{sync}}$.\n    The total span is the sum of these sequential costs:\n    $$S_1 = T_{\\text{sync}} (\\text{initial}) + \\sum_{i=1}^{\\log_2(B)} (T_{\\text{add}} + T_{\\text{sync}})$$\n    $$S_1 = T_{\\text{sync}} + \\log_2(B) \\cdot (T_{\\text{add}} + T_{\\text{sync}})$$\n    $$S_1 = (\\log_2(B)) T_{\\text{add}} + (1 + \\log_2(B)) T_{\\text{sync}}$$\n\n3.  **Number of Block-Level Barriers ($N_{\\text{sync},1}$)**: As per the derivation, there is one initial barrier and one for each of the $\\log_2(B)$ reduction levels.\n    $$N_{\\text{sync},1} = 1 + \\log_2(B)$$\n\n4.  **Peak Shared Memory Footprint ($M_1$)**: The strategy requires all $B$ initial values to be stored in shared memory at the beginning of the reduction. This is the point of maximum memory usage.\n    $$M_1 = B \\text{ slots}$$\n\n**Strategy 2: Warp-Centric Reduction**\n\n1.  **Work ($W_2$)**: The total number of additions remains $B-1$. The work is $W_2 = (B-1) T_{\\text{add}}$.\n\n2.  **Span ($S_2$)**: The span is the sum of the spans of its three sequential phases.\n    - **Phase 1: Intra-warp reductions.** All $B/W$ warps operate in parallel. Each warp reduces $W$ values to one partial sum. This requires $\\log_2(W)$ sequential additions using warp-synchronous operations (e.g., shuffles), which do not incur the $T_{\\text{sync}}$ cost. The span of this phase is determined by a single warp's reduction time.\n      Span (Phase 1) = $(\\log_2(W)) T_{\\text{add}}$.\n    - **Phase 2: Block-level synchronization.** The $B/W$ partial sums are written to shared memory. A single block-level barrier is then called to ensure all these writes are visible to all threads.\n      Span (Phase 2) = $T_{\\text{sync}}$.\n    - **Phase 3: Final reduction.** A single warp reads the $B/W$ partial sums from shared memory and reduces them to the final value. This reduction takes $\\log_2(B/W)$ sequential steps using warp-synchronous operations.\n      Span (Phase 3) = $(\\log_2(B/W)) T_{\\text{add}}$.\n    The total span $S_2$ is the sum of the spans of these three phases:\n    $$S_2 = (\\log_2(W)) T_{\\text{add}} + T_{\\text{sync}} + (\\log_2(B/W)) T_{\\text{add}}$$\n    Using the logarithm property $\\log(a) + \\log(b) = \\log(ab)$, we simplify:\n    $$S_2 = (\\log_2(W) + \\log_2(B/W)) T_{\\text{add}} + T_{\\text{sync}}$$\n    $$S_2 = (\\log_2(B)) T_{\\text{add}} + T_{\\text{sync}}$$\n\n3.  **Number of Block-Level Barriers ($N_{\\text{sync},2}$)**: The strategy explicitly uses only one such barrier.\n    $$N_{\\text{sync},2} = 1$$\n\n4.  **Peak Shared Memory Footprint ($M_2$)**: Shared memory is used only to store the intermediate partial sums from each warp. There are $B/W$ such sums.\n    $$M_2 = B/W \\text{ slots}$$\n\n### Numerical Calculation\n\nNow we substitute the given values: $B=256$, $W=32$, $T_{\\text{add}}=4$ cycles, and $T_{\\text{sync}}=80$ cycles.\n\nFirst, we find the values of the logarithms:\n- $\\log_2(B) = \\log_2(256) = \\log_2(2^8) = 8$.\n- $\\log_2(W) = \\log_2(32) = \\log_2(2^5) = 5$.\n- $\\log_2(B/W) = \\log_2(256/32) = \\log_2(8) = 3$.\n\n**Summary of derived quantities:**\n\n| Metric                      | Strategy 1 (Symbolic)                                           | Strategy 1 (Numerical)                          | Strategy 2 (Symbolic)                            | Strategy 2 (Numerical)                    |\n|-----------------------------|-----------------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|-------------------------------------------|\n| Span                        | $(\\log_2 B) T_{\\text{add}} + (1+\\log_2 B) T_{\\text{sync}}$        | $8 \\cdot 4 + (1+8) \\cdot 80 = 752$ cycles       | $(\\log_2 B) T_{\\text{add}} + T_{\\text{sync}}$     | $8 \\cdot 4 + 80 = 112$ cycles             |\n| Num. Barriers               | $1 + \\log_2 B$                                                  | $1+8=9$                                         | $1$                                              | $1$                                       |\n| Shmem Footprint             | $B$ slots                                                       | $256$ slots                                     | $B/W$ slots                                      | $256/32 = 8$ slots                        |\n\n**Compute the Ratio $R$**\n\nThe problem asks for the ratio $R$ of the span of the binary tree strategy ($S_1$) to that of the warp-centric strategy ($S_2$).\n\n$$R = \\frac{S_1}{S_2} = \\frac{(\\log_2(B)) T_{\\text{add}} + (1 + \\log_2(B)) T_{\\text{sync}}}{(\\log_2(B)) T_{\\text{add}} + T_{\\text{sync}}}$$\n\nUsing the calculated numerical values for the spans:\n\n$$R = \\frac{752}{112}$$\n\nSimplifying the fraction:\n$$R = \\frac{376}{56} = \\frac{188}{28} = \\frac{94}{14} = \\frac{47}{7}$$\n\nNow, we compute the decimal value and round to four significant figures:\n$$R = \\frac{47}{7} \\approx 6.7142857...$$\nRounding to four significant figures gives $6.714$.", "answer": "$$\\boxed{6.714}$$", "id": "3138934"}]}