{"hands_on_practices": [{"introduction": "Understanding the performance trade-offs between different parallel programming models is a cornerstone of computational science. This exercise challenges you to analyze a fundamental parallel algorithm, the prefix sum (or scan), under two distinct paradigms: a shared-memory model using threads and a distributed-memory model using the Message Passing Interface (MPI) [@problem_id:3169829]. By deriving and comparing the critical path length, or span, for each implementation, you will gain hands-on experience in applying abstract cost models to quantify the real-world impact of synchronization and communication overhead.", "problem": "You are studying the parallel prefix sum (scan) over $N$ double-precision elements using two programming models: a thread-based shared-memory implementation and a process-based Message Passing Interface (MPI) implementation. Assume the associative operation is addition. Your goal is to compare the total work and the critical path length (span) under a simple cost model, and then quantify the constant-factor difference in the $\\Theta(\\log_2 P)$ span terms arising from the different coordination patterns.\n\nFundamental bases and assumptions:\n- A sequential scan over $N$ items performs $N-1$ additions.\n- In the fork-join model, the work $T_1$ is the total number of primitive operations performed across all processing elements, and the span $T_{\\infty}$ is the length of the longest chain of dependent operations (the critical path).\n- A balanced binary tree of $P$ leaves has height $\\log_2 P$.\n\nTwo implementations and their cost models:\n- Thread-based shared-memory implementation: Data are partitioned into $P$ contiguous blocks, one per thread. Threads compute local scans of their blocks, then perform an inter-thread scan on the $P$ block totals using a balanced tree up-sweep followed by a down-sweep, and finally add per-block offsets to local elements. Each tree level incurs one barrier of latency $b$ and a constant-time local combine cost $c$ on the critical path.\n- MPI-based implementation using recursive doubling: Each process first performs a local scan of its block, then participates in $\\log_2 P$ rounds of point-to-point message exchanges (recursive doubling) to compute the prefix of block totals. Each round incurs a message latency $L$, a per-byte transfer time $g$ for a payload of $s$ bytes (one partial sum), and a local combine cost $c$ on the critical path.\n\nIn comparing spans, focus on the coordination component that yields the $\\Theta(\\log_2 P)$ term. You may ignore the identical local per-block scan and offset-application contributions in both models when forming the ratio, because those contributions cancel and do not affect the $\\Theta(\\log_2 P)$ coordination term or its constant factors.\n\nGiven parameters:\n- Number of processing elements $P = 1024$.\n- Thread barrier latency $b = 0.3$ microseconds.\n- Local combine cost $c = 0.01$ microseconds.\n- MPI per-message latency $L = 4.8$ microseconds.\n- MPI inverse bandwidth $g = 0.003$ microseconds per byte.\n- Payload size $s = 8$ bytes.\n\nTasks:\n1) Using the core definitions above, characterize symbolically the leading-order work $T_1$ for each implementation in terms of $N$ and $P$, retaining any asymptotically non-negligible dependence on $P$ that differs between the two.\n2) Derive the asymptotic form and constant factors of the coordination span $T_{\\infty}$ as a function of $P$ for each implementation under the given cost model, justifying the $\\Theta(\\log_2 P)$ dependence from first principles.\n3) Using the given numerical parameters, compute the ratio $R$ of the coordination spans,\n$$\nR \\equiv \\frac{T_{\\infty}^{\\text{threads}}}{T_{\\infty}^{\\text{MPI}}}.\n$$\nRound your answer to four significant figures. Report $R$ as a pure number with no units.", "solution": "The problem requires an analysis of two parallel prefix sum implementations, one using shared-memory threads and the other using MPI. The analysis involves determining the total work ($T_1$) and the coordination span ($T_{\\infty}$) for each, followed by a numerical comparison of their coordination spans.\n\n### Task 1: Characterization of Work ($T_1$)\n\nThe work, $T_1$, represents the total number of additions performed across all processing elements. The input consists of $N$ elements, partitioned into $P$ blocks of size $N/P$.\n\n**1. Thread-based Shared-Memory Implementation:**\nThe algorithm proceeds in three stages. We calculate the work for each stage.\n- **Local Scans:** Each of the $P$ threads performs a sequential prefix sum on its local block of $N/P$ elements. A sequential scan on $k$ items takes $k-1$ additions. Thus, the work for one thread is $(N/P) - 1$. For all $P$ threads, the total work is $W_{\\text{local}} = P \\left( \\frac{N}{P} - 1 \\right) = N - P$.\n- **Inter-thread Scan:** A parallel scan is performed on the $P$ block totals (the last element of each local scan) using a balanced binary tree structure involving an up-sweep and a down-sweep (Blelloch scan). The work for a parallel scan on $P$ elements using this method is $2(P-1)$ additions. So, $W_{\\text{inter}} = 2(P-1)$.\n- **Offset Application:** Each of the threads from $1$ to $P-1$ adds its corresponding prefix sum of block totals to all $N/P$ elements in its local block. Thread $0$ requires no offset. This results in $(P-1)$ threads each performing $N/P$ additions. The total work is $W_{\\text{offset}} = (P-1)\\frac{N}{P} = N - \\frac{N}{P}$.\n\nThe total work for the thread-based model, $T_{1}^{\\text{threads}}$, is the sum of the work in all stages:\n$$T_{1}^{\\text{threads}} = W_{\\text{local}} + W_{\\text{inter}} + W_{\\text{offset}}$$\n$$T_{1}^{\\text{threads}} = (N-P) + 2(P-1) + \\left(N - \\frac{N}{P}\\right)$$\n$$T_{1}^{\\text{threads}} = 2N + P - \\frac{N}{P} - 2$$\n\n**2. MPI-based Implementation:**\nThis algorithm also has three stages.\n- **Local Scans:** This stage is identical to the thread-based model. The work is $W_{\\text{local}} = N - P$.\n- **Inter-process Scan (Recursive Doubling):** The prefix sums of the $P$ block totals are computed using a recursive doubling algorithm. This requires $\\log_2 P$ rounds of communication. In each round $k$ (where $k$ ranges from $0$ to $\\log_2 P - 1$), processes $i \\in [2^k, P-1]$ receive a partial sum and perform one addition. The number of additions in round $k$ is $P - 2^k$. The total work is the sum over all rounds:\n$$W_{\\text{inter}} = \\sum_{k=0}^{\\log_2 P - 1} (P - 2^k) = P \\sum_{k=0}^{\\log_2 P - 1} 1 - \\sum_{k=0}^{\\log_2 P - 1} 2^k = P\\log_2 P - (2^{\\log_2 P} - 1) = P\\log_2 P - P + 1$$\n- **Offset Application:** This stage is identical to the thread-based model. The work is $W_{\\text{offset}} = N - \\frac{N}{P}$.\n\nThe total work for the MPI-based model, $T_{1}^{\\text{MPI}}$, is:\n$$T_{1}^{\\text{MPI}} = W_{\\text{local}} + W_{\\text{inter}} + W_{\\text{offset}}$$\n$$T_{1}^{\\text{MPI}} = (N-P) + (P\\log_2 P - P + 1) + \\left(N - \\frac{N}{P}\\right)$$\n$$T_{1}^{\\text{MPI}} = 2N + P\\log_2 P - 2P - \\frac{N}{P} + 1$$\n\n### Task 2: Derivation of Coordination Span ($T_{\\infty}$)\n\nThe span, $T_{\\infty}$, represents the critical path length. We are asked to analyze the coordination component only.\n\n**1. Thread-based Coordination Span:**\nThe coordination phase consists of an up-sweep and a down-sweep on a balanced binary tree of height $\\log_2 P$.\n- **Up-sweep:** The critical path goes from a leaf to the root, traversing $\\log_2 P$ levels. Each level requires synchronization (a barrier of latency $b$) followed by a local combine operation (cost $c$). The span for one level is $b+c$. Thus, the span for the up-sweep is $(\\log_2 P)(b+c)$.\n- **Down-sweep:** The critical path goes from the root to a leaf, again traversing $\\log_2 P$ levels. Each level incurs the same cost of $b+c$. The span for the down-sweep is also $(\\log_2 P)(b+c)$.\n\nThe total coordination span, $T_{\\infty}^{\\text{threads}}$, is the sum of the spans of the two sequential phases:\n$$T_{\\infty}^{\\text{threads}} = (\\log_2 P)(b+c) + (\\log_2 P)(b+c) = 2(b+c)\\log_2 P$$\nThis has the required $\\Theta(\\log_2 P)$ dependence, justified by the traversal of a tree of depth $\\log_2 P$ twice, once up and once down.\n\n**2. MPI-based Coordination Span:**\nThe coordination phase uses recursive doubling, which consists of $\\log_2 P$ sequential rounds.\n- **Span of one round:** In each round, a critical-path process waits for a message, receives it, and performs a local combination. The time for this is the sum of the message latency $L$, the transfer time $g \\cdot s$ (where $s$ is the payload size and $g$ is the inverse bandwidth), and the local combine cost $c$. So, the span per round is $L+gs+c$.\n\nThe total coordination span, $T_{\\infty}^{\\text{MPI}}$, is the span per round multiplied by the number of rounds:\n$$T_{\\infty}^{\\text{MPI}} = (L+gs+c)\\log_2 P$$\nThis has the required $\\Theta(\\log_2 P)$ dependence, justified by the $\\log_2 P$ sequential stages of communication inherent to the recursive doubling algorithm.\n\n### Task 3: Computation of the Ratio $R$\n\nThe ratio $R$ of the coordination spans is defined as:\n$$R = \\frac{T_{\\infty}^{\\text{threads}}}{T_{\\infty}^{\\text{MPI}}}$$\nSubstituting the expressions derived in Task 2:\n$$R = \\frac{2(b+c)\\log_2 P}{(L+gs+c)\\log_2 P}$$\nThe $\\log_2 P$ terms cancel, yielding:\n$$R = \\frac{2(b+c)}{L+gs+c}$$\nNow, we substitute the given numerical parameters:\n- $b = 0.3 \\, \\mu s$\n- $c = 0.01 \\, \\mu s$\n- $L = 4.8 \\, \\mu s$\n- $g = 0.003 \\, \\mu s / \\text{byte}$\n- $s = 8 \\, \\text{bytes}$\n\nThe numerator is:\n$$2(b+c) = 2(0.3 + 0.01) = 2(0.31) = 0.62$$\nThe denominator is:\n$$L+gs+c = 4.8 + (0.003 \\times 8) + 0.01 = 4.8 + 0.024 + 0.01 = 4.834$$\nThe ratio $R$ is:\n$$R = \\frac{0.62}{4.834} \\approx 0.1282579644$$\nRounding to four significant figures, we get:\n$$R \\approx 0.1283$$", "answer": "$$\\boxed{0.1283}$$", "id": "3169829"}, {"introduction": "Effective parallelization often involves more than just distributing work; it requires tuning scheduling parameters to match the workload's characteristics. This practice focuses on a classic problem in loop-level parallelism: determining the optimal chunk size for OpenMP's dynamic scheduling when faced with a load-imbalanced loop [@problem_id:3169831]. By modeling the trade-off between scheduling overhead and idle time, you will use calculus to derive a formula for the ideal chunk size, translating theoretical analysis into a practical performance optimization.", "problem": "Consider a shared-memory loop executed with Open Multi-Processing (OpenMP) dynamic scheduling on $P$ hardware threads. The loop has $N$ iterations indexed by $i \\in \\{1,2,\\dots,N\\}$, and the compute time per iteration is modeled as $c(i)=a+bi$, where $a>0$ and $b\\geq 0$ are constants. The OpenMP runtime uses a fixed chunk size $s$ (iterations per chunk). Each chunk assignment incurs a scheduling overhead of $\\delta$ time units; there are no other sources of overhead or synchronization beyond this cost per chunk. The dynamic scheduler assigns chunks to threads as they become free.\n\nAdopt the following scientifically reasonable modeling assumptions consistent with dynamic scheduling and a monotone increasing per-iteration cost profile:\n- The dynamic scheduler eliminates most load imbalance except near the end of the loop, where the indivisibility of chunks causes at most $P-1$ threads to be idle while the last chunk is processed.\n- The average idle time per idle thread during this tail is approximately one-half of the compute time of a representative tail chunk.\n- When $s \\ll N$, the compute time of a representative tail chunk can be modeled by the chunk size multiplied by the per-iteration cost near $i \\approx N$.\n\nUsing these assumptions and first principles, construct a model for the total inefficiency time (defined as the sum of total idle time across all threads plus total scheduling overhead), as a function of the chunk size $s$ and parameters $a$, $b$, $N$, $P$, and $\\delta$. Then derive the real-valued chunk size $s^{\\ast}$ that minimizes this inefficiency time.\n\nFinally, evaluate your expression for the following parameter values: $N=10{,}000$, $P=8$, $a=2$, $b=0.005$, and $\\delta=3$. Report the optimal real-valued chunk size $s^{\\ast}$ in iterations per chunk, rounded to three significant figures. Do not include any units in your final numerical answer.", "solution": "The scenario is a parallel loop with Open Multi-Processing (OpenMP) dynamic scheduling. By definition, dynamic scheduling assigns work in chunks of $s$ iterations to threads as they become available. Each assignment incurs an overhead of $\\delta$ time units. The total number of chunks executed is $N/s$, assuming $s$ divides $N$; for modeling and optimization, we can treat $s$ as a positive real variable and interpret $N/s$ continuously.\n\nFundamental modeling steps:\n1. Scheduling overhead. Since each chunk assignment costs $\\delta$, and a total of $N/s$ chunks are issued, the total scheduling overhead is\n$$\nT_{\\text{overhead}}(s) = \\delta \\frac{N}{s}.\n$$\n\n2. Tail idle time. With dynamic scheduling and a monotone increasing per-iteration cost $c(i)=a+bi$, the primary imbalance occurs at the end of the loop because chunks are indivisible. In the final tail period, up to $P-1$ threads can be idle while one thread completes the last chunk. A standard queueing and scheduling argument for tasks with indivisible granularity is that the average idle time per idle thread over the tail interval is approximately one-half of the last chunk’s compute time. Therefore, the total idle time across threads in the tail can be modeled as\n$$\nT_{\\text{idle}}(s) \\approx \\frac{P-1}{2} \\, T_{\\text{chunk,tail}}(s).\n$$\nWhen $s \\ll N$ and $c(i)$ is slowly varying over the chunk, a representative tail chunk compute time can be approximated by the chunk size multiplied by the per-iteration cost near $i \\approx N$, namely\n$$\nT_{\\text{chunk,tail}}(s) \\approx s \\big(a + bN\\big).\n$$\nCombining these,\n$$\nT_{\\text{idle}}(s) \\approx \\frac{P-1}{2} \\, s \\big(a + bN\\big).\n$$\n\n3. Total inefficiency time. Summing the two contributions,\n$$\nL(s) = T_{\\text{idle}}(s) + T_{\\text{overhead}}(s) \\approx \\frac{P-1}{2} \\, s \\big(a + bN\\big) + \\delta \\frac{N}{s}.\n$$\nWe seek the real-valued chunk size $s>0$ minimizing $L(s)$.\n\nOptimization by calculus:\nCompute the derivative with respect to $s$:\n$$\n\\frac{dL}{ds} = \\frac{P-1}{2} \\big(a + bN\\big) - \\delta \\frac{N}{s^{2}}.\n$$\nSet $\\frac{dL}{ds}=0$ for optimality:\n$$\n\\frac{P-1}{2} \\big(a + bN\\big) = \\delta \\frac{N}{s^{2}}.\n$$\nSolve for $s^{2}$:\n$$\ns^{2} = \\frac{\\delta N}{\\frac{P-1}{2} \\big(a + bN\\big)} = \\frac{2 \\delta N}{(P-1)\\big(a + bN\\big)}.\n$$\nThus the optimal real-valued chunk size is\n$$\ns^{\\ast} = \\sqrt{\\frac{2 \\delta N}{(P-1)\\big(a + bN\\big)}}.\n$$\n\nNumerical evaluation:\nGiven $N=10{,}000$, $P=8$, $a=2$, $b=0.005$, and $\\delta=3$,\n- Compute $a+bN$:\n$$\na + bN = 2 + 0.005 \\times 10{,}000 = 2 + 50 = 52.\n$$\n- Compute $(P-1)\\big(a + bN\\big)$:\n$$\n(P-1)\\big(a + bN\\big) = 7 \\times 52 = 364.\n$$\n- Compute $2 \\delta N$:\n$$\n2 \\delta N = 2 \\times 3 \\times 10{,}000 = 60{,}000.\n$$\nTherefore,\n$$\ns^{\\ast} = \\sqrt{\\frac{60{,}000}{364}} \\approx \\sqrt{164.8351648} \\approx 12.8388\\ldots\n$$\nRounded to three significant figures,\n$$\ns^{\\ast} \\approx 12.8.\n$$\n\nThis $s^{\\ast}$ is a real-valued optimum under the model. It satisfies the assumption $s \\ll N$ and therefore is consistent with the tail-chunk approximation used.", "answer": "$$\\boxed{12.8}$$", "id": "3169831"}, {"introduction": "A common pitfall in parallel programming is assuming that more threads always lead to better performance. This exercise provides a computational model to explore the phenomenon of oversubscription, where the number of active threads exceeds the available CPU cores, leading to significant performance degradation [@problem_id:3169824]. By implementing models for context-switching overhead and cache contention, you will quantify the slowdown and learn to identify an optimal thread count, a critical skill for writing efficient and scalable applications.", "problem": "You are asked to design and implement a self-contained computational experiment that models the impact of oversubscription in a threading context and the role of thread affinity on performance. The model must be based on fundamental definitions of throughput, speedup, and scheduling, and must not rely on properties of any particular programming language runtime. Your experiment must quantify slowdown when the number of threads exceeds available cores and propose an adaptive strategy to cap the number of threads to avoid oversubscription.\n\nFundamental base, definitions, and modeling assumptions:\n- A Central Processing Unit (CPU) with $c$ cores can run up to $c$ independent threads simultaneously without preemptive time sharing.\n- A thread performs $W$ units of work, where each unit takes $1$ abstract time unit when executed uninterrupted on a dedicated core. The total workload is $t$ identical threads, each with $W$ units.\n- Speedup is defined as $S(t) = T(1) \\,/\\, T(t)$, where $T(t)$ is the makespan (the time to finish all $t$ threads).\n- The ideal batch makespan under perfect scaling and no overhead is $$T_{\\text{ideal}}(t,c,W) = \\max\\!\\left(W, \\frac{t\\,W}{c}\\right).$$ This follows from the observation that when $t \\le c$ all threads can run concurrently and finish in $W$, and when $t > c$ the CPU time is shared across $c$ cores, yielding a lower bound of $(t\\,W)/c$ for total completion time under perfect pipelining.\n- Under time-sliced scheduling, each active thread is given a quantum of $q$ work units per slice, incurring a context-switch overhead of $s$ time units whenever switching between different threads on the same core. When only a single thread runs on a core (i.e., no oversubscription on that core), assume no context switch overhead ($s$ applies only when $k > 1$), where $k$ is the number of threads assigned to a core.\n- With thread affinity (pinned scheduling), each thread is assigned to a fixed core. If $t > c$, each core hosts $k = \\lceil t / c \\rceil$ threads and cycles through them round-robin. The number of slices per thread is $L = \\lceil W / q \\rceil$.\n- Without thread affinity (unpinned scheduling), threads may migrate between cores. Migration introduces additional expected overhead per slice of $p \\cdot a$ time units, where $p$ is the probability of a migration per slice and $a$ is the cost of a migration. Unpinned scheduling also degrades effective per-slice execution due to lost locality and contention. Model this cache and contention penalty as a multiplicative factor $$r(t,c,\\beta) = 1 + \\beta \\cdot \\max\\!\\left(\\frac{t}{c} - 1,\\, 0\\right),$$ where $\\beta \\ge 0$ is a given penalty coefficient. The effective time to execute $q$ work units in a slice becomes $q \\cdot r(t,c,\\beta)$.\n- The pinned makespan is modeled by $$T_{\\text{pinned}}(t,c,W,q,s) \\approx L \\cdot k \\cdot \\big(q + s_{\\text{eff}}\\big),$$ where $k = \\lceil t / c \\rceil$, $L = \\lceil W / q \\rceil$, and $s_{\\text{eff}} = s$ if $k > 1$ and $s_{\\text{eff}} = 0$ otherwise.\n- The unpinned makespan is modeled by $$T_{\\text{unpinned}}(t,c,W,q,s,p,a,\\beta) \\approx L \\cdot k \\cdot \\big(q \\cdot r(t,c,\\beta) + s_{\\text{eff}} + p \\cdot a\\big),$$ using the same $k$, $L$, and $s_{\\text{eff}}$.\n- Define slowdown relative to the ideal batch completion time by $$\\text{slowdown}(t) = \\frac{T_{\\text{actual}}(t)}{T_{\\text{ideal}}(t,c,W)}.$$ This dimensionless quantity captures inefficiency due to oversubscription and overhead.\n\nTasks:\n- Implement the above model to compute, for each test case, the pinned slowdown and the unpinned slowdown at the provided thread count $t$.\n- Propose an adaptive capping strategy: given a maximum allowable thread count $t_{\\max}$ and fixed parameters $(c,W,q,s,p,a,\\beta)$, choose $$t^{\\star} = \\operatorname*{arg\\,min}_{u \\in \\{1,2,\\dots,t_{\\max}\\}} T_{\\text{unpinned}}(u,c,W,q,s,p,a,\\beta).$$ Report the recommended cap as $\\min(c, t^{\\star})$ to avoid oversubscription by design.\n- Your program must implement these computations in a deterministic way and produce the outputs described below.\n\nUnits and numerical representation:\n- All times are in abstract time units based on the above definitions. Slowdowns are dimensionless.\n- Report all floating-point slowdowns rounded to $6$ decimal places.\n\nTest suite:\nProvide results for the following parameter sets, which cover a general case, boundary conditions, and edge cases. Each test case is a tuple $(c,t,W,q,s,p,a,\\beta)$.\n\n- Case $1$ (happy path, moderate oversubscription): $(c,t,W,q,s,p,a,\\beta) = (4,8,1000,50,2,0.1,5,0.05)$.\n- Case $2$ (boundary, $t = c$): $(c,t,W,q,s,p,a,\\beta) = (4,4,1000,50,2,0.1,5,0.05)$.\n- Case $3$ (heavy oversubscription): $(c,t,W,q,s,p,a,\\beta) = (4,32,1000,20,2,0.3,5,0.1)$.\n- Case $4$ (serial baseline): $(c,t,W,q,s,p,a,\\beta) = (4,1,1000,50,2,0.0,5,0.05)$.\n- Case $5$ (single core with oversubscription): $(c,t,W,q,s,p,a,\\beta) = (1,8,1000,50,2,0.1,5,0.05)$.\n\nRequired final output format:\n- For each test case, output a list of three values: $[\\text{slowdown}_{\\text{pinned}}, \\text{slowdown}_{\\text{unpinned}}, t_{\\text{cap}}]$, where the slowdowns are floats rounded to $6$ decimal places, and $t_{\\text{cap}}$ is an integer given by $\\min(c,t^{\\star})$ with $t^{\\star}$ computed over $u \\in \\{1,2,\\dots,t\\}$ for that case.\n- Aggregate all test case results into a single list in the given order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[[x_1,y_1,z_1],[x_2,y_2,z_2],\\dots]$ with no extra whitespace or text.", "solution": "The posed problem requires the design and implementation of a computational experiment to model and quantify the performance effects of thread oversubscription and scheduling policies (pinned vs. unpinned). The analysis is to be based on a set of provided mathematical models for execution time (makespan). The problem is well-defined, scientifically grounded in the principles of computational performance modeling, and internally consistent. All variables, constants, and functional relationships are explicitly provided, enabling a direct and formal implementation. The approximate nature of the models, indicated by `$\\approx$`, is understood to mean that for the purposes of this problem, the expressions are to be treated as exact equalities.\n\nThe solution proceeds in three logical steps:\n$1$. Implementing the mathematical models for ideal, pinned, and unpinned makespans.\n$2$. Computing the slowdown for both pinned and unpinned scheduling relative to the ideal makespan for each specified test case.\n$3$. Determining a recommended thread cap, $t_{\\text{cap}}$, by finding the thread count $t^{\\star}$ that minimizes the unpinned makespan and then constraining this value by the number of cores $c$.\n\nFirst, we formalize the component functions based on the provided definitions. Let $c$ be the number of cores, $t$ the number of threads, $W$ the work per thread, $q$ the time quantum, and $s$ the context-switch overhead.\n\nThe number of slices per thread, $L$, is the total work $W$ divided by the work per slice $q$, rounded up to the nearest integer:\n$$L = \\lceil W / q \\rceil$$\nThe maximum number of threads assigned to any single core, $k$, assuming an even distribution, is:\n$$k = \\lceil t / c \\rceil$$\nThe effective context-switch overhead, $s_{\\text{eff}}$, applies only when a core manages more than one thread ($k > 1$):\n$$s_{\\text{eff}} = \\begin{cases} s & \\text{if } k > 1 \\\\ 0 & \\text{if } k \\le 1 \\end{cases}$$\n\nWith these fundamental quantities, we can define the makespan models. The ideal makespan, $T_{\\text{ideal}}$, represents the theoretical minimum time, assuming perfect parallelism and no overhead. It is the greater of the single-thread execution time ($W$) and the time if all work ($t \\cdot W$) were perfectly distributed across $c$ cores:\n$$T_{\\text{ideal}}(t,c,W) = \\max\\left(W, \\frac{t \\cdot W}{c}\\right)$$\n\nThe makespan for pinned scheduling, $T_{\\text{pinned}}$, where each thread is fixed to a core, is determined by the most heavily loaded core. This core runs $k$ threads in a round-robin fashion. The total time is the number of slices per thread ($L$) multiplied by the number of threads on the core ($k$) times the duration of a slice, which includes execution ($q$) and potential context-switching ($s_{\\text{eff}}$):\n$$T_{\\text{pinned}}(t,c,W,q,s) = L \\cdot k \\cdot (q + s_{\\text{eff}})$$\n\nThe makespan for unpinned scheduling, $T_{\\text{unpinned}}$, incorporates additional overheads. Let $p$ be the migration probability, $a$ the migration cost, and $\\beta$ the contention penalty coefficient. A multiplicative penalty factor, $r$, models performance degradation due to lost cache locality and contention when threads exceed cores:\n$$r(t,c,\\beta) = 1 + \\beta \\cdot \\max\\left(\\frac{t}{c} - 1, 0\\right)$$\nThe unpinned makespan includes the costs of slice execution scaled by contention ($q \\cdot r$), context switching ($s_{\\text{eff}}$), and expected migration overhead ($p \\cdot a$):\n$$T_{\\text{unpinned}}(t,c,W,q,s,p,a,\\beta) = L \\cdot k \\cdot (q \\cdot r(t,c,\\beta) + s_{\\text{eff}} + p \\cdot a)$$\n\nThe performance degradation is quantified by the slowdown, defined as the ratio of the actual makespan to the ideal makespan:\n$$\\text{slowdown}(t) = \\frac{T_{\\text{actual}}(t)}{T_{\\text{ideal}}(t,c,W)}$$\nWe will compute this for both $T_{\\text{pinned}}$ and $T_{\\text{unpinned}}$.\n\nFinally, we must propose an adaptive threading cap. This requires finding the optimal number of threads, $t^{\\star}$, that minimizes the unpinned makespan for a given set of system parameters. The search is conducted over the range of a an allowable thread count, $u \\in \\{1, 2, \\dots, t_{\\max}\\}$, where $t_{\\max}$ is specified as the thread count $t$ from the corresponding test case.\n$$t^{\\star} = \\operatorname*{arg\\,min}_{u \\in \\{1, 2, \\dots, t\\}} T_{\\text{unpinned}}(u,c,W,q,s,p,a,\\beta)$$\nAn analytical examination of the $T_{\\text{unpinned}}$ model reveals that for $u \\in \\{1, \\dots, c\\}$, the makespan is constant, as overheads from oversubscription ($s$ and the $\\beta$-dependent term) are zero. For $u > c$, both the factor $k = \\lceil u/c \\rceil$ and the per-slice time increase, leading to a monotonically increasing makespan. Consequently, the minimum time occurs for any $u \\in \\{1, \\dots, c\\}$, and the `argmin` is $t^{\\star}=1$. The recommended cap, $t_{\\text{cap}}$, is designed to prevent oversubscription by taking the minimum of $c$ and this optimal value, $t^{\\star}$:\n$$t_{\\text{cap}} = \\min(c, t^{\\star})$$\n\nThe computational procedure involves implementing these functions, iterating through each test case tuple $(c,t,W,q,s,p,a,\\beta)$, calculating the pinned and unpinned slowdowns at the given $t$, determining $t_{\\text{cap}}$ via the minimization search, and reporting the three resulting values: $[\\text{slowdown}_{\\text{pinned}}, \\text{slowdown}_{\\text{unpinned}}, t_{\\text{cap}}]$. All floating-point results are rounded to $6$ decimal places as required. The Python code below implements this procedure.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the computational scheduling problem by implementing the provided models.\n    \"\"\"\n\n    test_cases = [\n        # (c, t, W, q, s, p, a, beta)\n        (4, 8, 1000, 50, 2, 0.1, 5, 0.05),    # Case 1: happy path, moderate oversubscription\n        (4, 4, 1000, 50, 2, 0.1, 5, 0.05),    # Case 2: boundary, t = c\n        (4, 32, 1000, 20, 2, 0.3, 5, 0.1),     # Case 3: heavy oversubscription\n        (4, 1, 1000, 50, 2, 0.0, 5, 0.05),    # Case 4: serial baseline\n        (1, 8, 1000, 50, 2, 0.1, 5, 0.05),    # Case 5: single core with oversubscription\n    ]\n\n    # --- Helper functions for the model ---\n\n    def calculate_L(W, q):\n        \"\"\"Calculates L, the number of slices per thread.\"\"\"\n        return np.ceil(W / q)\n\n    def calculate_k(t, c):\n        \"\"\"Calculates k, the max threads per core.\"\"\"\n        return np.ceil(t / c)\n\n    def calculate_s_eff(k, s):\n        \"\"\"Calculates effective context-switch overhead.\"\"\"\n        return s if k > 1 else 0\n\n    def calculate_r(t, c, beta):\n        \"\"\"Calculates r, the cache/contention penalty factor.\"\"\"\n        return 1 + beta * max(t / c - 1, 0)\n\n    def T_ideal(t, c, W):\n        \"\"\"Calculates the ideal makespan.\"\"\"\n        return max(W, t * W / c)\n\n    def T_pinned(t, c, W, q, s):\n        \"\"\"Calculates the pinned makespan.\"\"\"\n        L = calculate_L(W, q)\n        k = calculate_k(t, c)\n        s_eff = calculate_s_eff(k, s)\n        return L * k * (q + s_eff)\n\n    def T_unpinned(t, c, W, q, s, p, a, beta):\n        \"\"\"Calculates the unpinned makespan.\"\"\"\n        L = calculate_L(W, q)\n        k = calculate_k(t, c)\n        s_eff = calculate_s_eff(k, s)\n        r = calculate_r(t, c, beta)\n        migration_overhead = p * a\n        return L * k * (q * r + s_eff + migration_overhead)\n\n    def compute_metrics(case):\n        \"\"\"\n        Computes all required metrics for a single test case.\n        Returns (slowdown_pinned, slowdown_unpinned, t_cap).\n        \"\"\"\n        c, t, W, q, s, p, a, beta = case\n\n        # --- Task 1: Compute slowdowns for the given t ---\n        ideal_time = T_ideal(t, c, W)\n        \n        pinned_time = T_pinned(t, c, W, q, s)\n        slowdown_pinned = pinned_time / ideal_time\n\n        unpinned_time = T_unpinned(t, c, W, q, s, p, a, beta)\n        slowdown_unpinned = unpinned_time / ideal_time\n\n        # --- Task 2: Propose adaptive capping strategy ---\n        t_max_for_search = t\n        \n        # Search for t_star that minimizes T_unpinned\n        min_unpinned_time = float('inf')\n        t_star = -1\n\n        # The search range for u is {1, 2, ..., t} for the given case\n        for u in range(1, t_max_for_search + 1):\n            current_unpinned_time = T_unpinned(u, c, W, q, s, p, a, beta)\n            if current_unpinned_time  min_unpinned_time:\n                min_unpinned_time = current_unpinned_time\n                t_star = u\n        \n        # In case t=0 (not in tests but for robustness)\n        if t_star == -1 and t_max_for_search > 0:\n            t_star = 1\n        elif t_max_for_search == 0:\n             t_star = 0\n\n\n        t_cap = min(c, t_star)\n\n        return slowdown_pinned, slowdown_unpinned, int(t_cap)\n\n    results = []\n    for case in test_cases:\n        sp, su, tc = compute_metrics(case)\n        results.append([sp, su, tc])\n\n    # Final print statement in the exact required format.\n    # Manually construct the string to avoid whitespace added by str(list).\n    result_strings = []\n    for res in results:\n        # Format slowdowns to 6 decimal places.\n        slowdown_p_str = f\"{res[0]:.6f}\"\n        slowdown_u_str = f\"{res[1]:.6f}\"\n        t_cap_str = str(res[2])\n        result_strings.append(f\"[{slowdown_p_str},{slowdown_u_str},{t_cap_str}]\")\n    \n    return f\"[{','.join(result_strings)}]\"\n\n# The output of this function will be placed in the answer tag.\n# print(solve())\n```", "answer": "`[[1.040000,1.100000,1],[1.000000,1.010000,1],[1.100000,1.875000,1],[1.000000,1.000000,1],[1.040000,1.400000,1]]`", "id": "3169824"}]}