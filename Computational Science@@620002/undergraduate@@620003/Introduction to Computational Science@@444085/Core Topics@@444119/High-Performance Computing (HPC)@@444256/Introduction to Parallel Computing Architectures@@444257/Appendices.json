{"hands_on_practices": [{"introduction": "Understanding the performance limits of a parallel architecture is the first step toward writing efficient code. The Roofline model provides an intuitive yet powerful way to visualize these limits, showing that an application's performance is ultimately capped by either the processor's peak computational rate or the memory system's bandwidth. This exercise challenges you to apply the Roofline model to a multicore system, calculating the maximum achievable speedup for a memory-bound application and revealing how hardware bottlenecks can limit the benefits of parallelization [@problem_id:3145387].", "problem": "A programmer is evaluating a shared-memory multicore system using the roofline performance perspective for a memory-bound kernel. The system has up to $N_{\\max} = 32$ identical cores, each with a per-core peak compute rate of $P_{c} = 80$ giga floating-point operations per second (GFLOP/s). The memory subsystem provides a sustained bandwidth cap of $B = 60$ gigabytes per second (GB/s), but a single core sustains only $b_{1} = 12$ GB/s due to limited parallelism in memory accesses. The kernel has constant arithmetic intensity $I = 0.8$ floating-point operations per byte. Assume the following are valid: arithmetic intensity is defined as floating-point operations per byte moved; sustained performance is bounded by both the compute peak and the product of arithmetic intensity with the achievable bandwidth; and the achievable bandwidth scales approximately linearly with the number of active cores until reaching the cap $B$. The programmer defines parallel speedup $S(N)$ as the ratio of the achievable performance with $N$ cores to the achievable performance with one core. Under these constraints, deduce the maximum possible speedup $S_{\\max}$ over all integer $N$ with $1 \\leq N \\leq N_{\\max}$, and provide it as a dimensionless decimal number. No rounding is needed; provide the exact value.", "solution": "The problem asks for the maximum possible parallel speedup $S_{\\max}$ for a memory-bound kernel on a shared-memory multicore system. The analysis will be conducted using the roofline performance model, which posits that the achievable performance is the minimum of the peak compute performance and the peak memory performance.\n\nFirst, let us define the achievable performance for $N$ cores, denoted as $P(N)$. According to the roofline model:\n$$ P(N) = \\min(P_{\\text{compute}}(N), P_{\\text{memory}}(N)) $$\nwhere $P_{\\text{compute}}(N)$ is the total peak compute rate for $N$ cores, and $P_{\\text{memory}}(N)$ is the performance achievable given the memory bandwidth for $N$ cores.\n\nThe givens are:\n- Maximum number of cores: $N_{\\max} = 32$\n- Per-core peak compute rate: $P_{c} = 80$ GFLOP/s\n- System sustained memory bandwidth cap: $B = 60$ GB/s\n- Single-core sustained memory bandwidth: $b_{1} = 12$ GB/s\n- Kernel arithmetic intensity: $I = 0.8$ FLOP/byte\n\nThe total peak compute rate for $N$ identical cores is the sum of their individual peak rates:\n$$ P_{\\text{compute}}(N) = N \\times P_{c} $$\n\nThe peak memory performance is the product of the kernel's arithmetic intensity $I$ and the achievable memory bandwidth for $N$ cores, $B(N)$:\n$$ P_{\\text{memory}}(N) = I \\times B(N) $$\n\nThe problem states that the achievable bandwidth $B(N)$ scales linearly with the number of active cores $N$ until it reaches the system cap $B$. The single-core bandwidth is $b_1$. Therefore, the linearly scaled bandwidth is $N \\times b_1$. This leads to the following model for $B(N)$:\n$$ B(N) = \\min(N \\times b_1, B) $$\n\nCombining these expressions, the overall performance $P(N)$ is:\n$$ P(N) = \\min(N \\times P_{c}, I \\times \\min(N \\times b_1, B)) $$\nUsing the property $\\min(a, \\min(b, c)) = \\min(a, b, c)$, we can write:\n$$ P(N) = \\min(N \\times P_{c}, N \\times I \\times b_1, I \\times B) $$\n\nLet's substitute the given values into the terms of the expression for $P(N)$:\n- $N \\times P_{c} = N \\times 80$ GFLOP/s\n- $N \\times I \\times b_1 = N \\times 0.8 \\frac{\\text{FLOP}}{\\text{byte}} \\times 12 \\frac{\\text{GB}}{\\text{s}} = N \\times 9.6$ GFLOP/s\n- $I \\times B = 0.8 \\frac{\\text{FLOP}}{\\text{byte}} \\times 60 \\frac{\\text{GB}}{\\text{s}} = 48$ GFLOP/s\n\nSo, the performance in GFLOP/s is:\n$$ P(N) = \\min(80N, 9.6N, 48) $$\nFor any positive number of cores $N \\ge 1$, we have $9.6N < 80N$. Thus, the term $80N$ is never the minimum, and the expression simplifies to:\n$$ P(N) = \\min(9.6N, 48) $$\nThis confirms that for the given arithmetic intensity, the kernel's performance is always limited by the memory subsystem, not the peak computational capability of the cores, for any number of cores $N$ in the specified range.\n\nThe problem defines the parallel speedup $S(N)$ as the ratio of the achievable performance with $N$ cores to that with one core. First, we must calculate the single-core performance, $P(1)$:\n$$ P(1) = \\min(9.6 \\times 1, 48) = \\min(9.6, 48) = 9.6 \\text{ GFLOP/s} $$\n\nNow, we can express the speedup function $S(N)$:\n$$ S(N) = \\frac{P(N)}{P(1)} = \\frac{\\min(9.6N, 48)}{9.6} $$\nWe can distribute the division into the $\\min$ function:\n$$ S(N) = \\min\\left(\\frac{9.6N}{9.6}, \\frac{48}{9.6}\\right) $$\n$$ S(N) = \\min(N, 5) $$\n\nThe problem asks for the maximum possible speedup, $S_{\\max}$, for an integer number of cores $N$ in the range $1 \\leq N \\leq N_{\\max}$, where $N_{\\max} = 32$.\nThe function $S(N) = \\min(N, 5)$ behaves as follows:\n- For $N < 5$, $S(N) = N$. The speedup is linear and increases with $N$.\n- For $N \\ge 5$, $S(N) = 5$. The speedup saturates at a constant value of $5$.\n\nThe function $S(N)$ is a monotonically non-decreasing function of $N$. Its maximum value over the domain $1 \\leq N \\leq 32$ will be the value it reaches at saturation.\n- $S(1) = 1$\n- $S(2) = 2$\n- $S(3) = 3$\n- $S(4) = 4$\n- $S(5) = 5$\n- $S(6) = 5$\n- ...\n- $S(32) = 5$\n\nThe maximum value of $S(N)$ for $N$ in the given range is $5$. This maximum is first achieved with $N=5$ cores and is maintained for all a greater number of cores up to $N_{\\max}=32$. This limitation on speedup is a classic example of Amdahl's Law, where the bottleneck (in this case, the system memory bus bandwidth $B$) limits the achievable parallel performance improvement.\n\nThe maximum possible speedup $S_{\\max}$ is therefore $5$.", "answer": "$$\\boxed{5}$$", "id": "3145387"}, {"introduction": "Beyond theoretical performance peaks, true parallel efficiency depends on mastering the subtleties of the memory hierarchy. A notorious performance pitfall in shared-memory systems is \"false sharing,\" where logically separate data items, accessed by different cores, inadvertently occupy the same cache line and trigger costly, unnecessary coherence traffic. This problem asks you to diagnose a classic case of false sharing and evaluate common data layout strategies, such as padding, to eliminate this \"ping-pong\" effect and unlock performance [@problem_id:3145329].", "problem": "A shared-memory multicore system uses an invalidation-based cache coherence protocol consistent with Modified, Exclusive, Shared, Invalid (MESI). The coherence unit is a cache line of size $L = 64$ bytes. Consider $T$ threads pinned one per core on a processor whose private caches also have line size $L = 64$ bytes. Each thread $i$ increments a per-thread counter in a shared array $c[]$ at index $i$ inside a tight loop. The type of $c[]$ is a $64$-bit unsigned integer, so each element has size $s = 8$ bytes. The array base is aligned to $L = 64$ bytes. The inter-thread communication pattern requires no sharing of the counters’ logical values during the loop; each thread only updates its own counter and a reduction is performed after the loop to combine results.\n\nEmpirically, the increment loop achieves lower throughput than expected, and performance improves when threads are reduced to $T = 1$. You are asked to reason from first principles of cache coherence to explain whether the observed slowdown is consistent with false sharing and, if so, to determine which data layout redesigns guarantee elimination of cache line ping-pong due to false sharing, without changing the loop’s semantics.\n\nSelect all options that, under these assumptions, guarantee that distinct threads updating their own counters will not cause false sharing during the increment loop for any $T \\ge 1$.\n\nA. Pad each counter so that successive counters are separated by at least $L = 64$ bytes, and align the array base to $L = 64$ bytes, so each thread’s counter resides on a distinct cache line.\n\nB. Keep the original contiguous array of $s = 8$-byte counters but align only the array base to $L = 64$ bytes; rely on base alignment to prevent false sharing.\n\nC. Replace the struct-of-arrays layout with an array-of-structs in which each per-thread struct contains the counter and is padded so the struct size is exactly $L = 64$ bytes; align the array base to $L = 64$ bytes.\n\nD. Use atomic increment operations (e.g., atomic fetch-and-add) on the original contiguous array of $s = 8$-byte counters to prevent cache line ping-pong.\n\nE. Use a struct-of-arrays layout that groups all counters contiguously to improve spatial locality for the counter field; rely on improved locality to reduce coherence traffic.\n\nProvide your selection and justify it by deriving the conditions under which false sharing occurs for the original layout and the conditions required to avoid it, grounded in the core definitions of cache lines and invalidation-based coherence.", "solution": "The problem statement describes a classic performance issue in parallel computing known as false sharing. It is scientifically grounded, well-posed, and provides sufficient, consistent information to derive a solution from first principles of computer architecture.\n\n**1. Validation of the Problem Statement**\n\nThe givens are:\n- A shared-memory multicore system with $T$ threads, one per core.\n- An invalidation-based cache coherence protocol, consistent with MESI.\n- The coherence unit (and private cache line size) is $L = 64$ bytes.\n- Each thread $i$ increments its own counter, $c[i]$, in a tight loop.\n- The counters are $64$-bit unsigned integers, so their size is $s = 8$ bytes.\n- The array $c$ is contiguous in memory, and its base address is aligned to a $64$-byte boundary.\n- There is no logical need for threads to share counter values during the loop.\n\nThe observed performance degradation for $T > 1$ is consistent with the phenomenon of false sharing. Let us analyze why from first principles.\n\n**2. Derivation from First Principles**\n\nCache coherence protocols operate on the granularity of a cache line. In an invalidation-based protocol like MESI, when a core writes to a memory location, it must first gain exclusive ownership of the cache line containing that location. This is typically achieved by sending an invalidation signal over the interconnect to all other cores that may have a copy of that same cache line. Those cores must then mark their copies as invalid (I state). Any subsequent attempt by those cores to read from or write to that line will result in a cache miss, forcing a fetch of the updated line from memory or directly from the modifying core's cache.\n\nThe problem arises when multiple distinct data items, updated by different threads, happen to reside on the same physical cache line. This is called **false sharing**. The data items are logically independent, but their physical proximity in memory couples their performance through the cache coherence mechanism.\n\nIn the given scenario:\n- The size of a single counter element is $s = 8$ bytes.\n- The size of a cache line is $L = 64$ bytes.\n- Therefore, the number of counters that can fit within a single cache line is $N = L/s = 64/8 = 8$.\n\nThe array $c$ is contiguous and its base is aligned to a $64$-byte boundary. This means that the memory addresses for the first $8$ counters, $c[0], c[1], \\dots, c[7]$, will occupy the same cache line. Specifically, if the base address of $c$ is $A_{base}$, then $c[i]$ is at address $A_{base} + i \\times s$. For $i \\in \\{0, 1, \\dots, 7\\}$, these addresses all fall within the first cache line, which spans memory $[A_{base}, A_{base} + L - 1]$.\n\nLet's consider two threads, Thread $0$ and Thread $1$, running on different cores and updating $c[0]$ and $c[1]$ respectively.\n1. Thread $0$ executes `c[0]++`. This is a read-modify-write operation. To perform the write, Core $0$ must acquire exclusive ownership of the cache line. Let's say it brings the line into its private cache in the 'Modified' (M) state.\n2. Thread $1$ executes `c[1]++`. Since $c[1]$ is on the same cache line, Core $1$ must also acquire exclusive ownership. It sends a request for the line.\n3. This request causes Core $0$ to lose exclusive ownership. It might write the line back to memory and/or forward it to Core $1$. Core $0$'s copy of the line is invalidated (I state).\n4. Core $1$ now has the line in the 'Modified' state and increments $c[1]$.\n5. When Thread $0$ loops and attempts to increment $c[0]$ again, it finds its copy is invalid, resulting in a cache miss. It must re-acquire the cache line, which in turn invalidates the copy in Core $1$'s cache.\n\nThis back-and-forth transfer of the cache line between cores is known as \"cache line ping-pong.\" It is a costly process that involves stalls and traffic on the memory bus or interconnect, explaining the observed low throughput. The issue disappears for $T=1$ because there is no other core to contend with for the cache line.\n\nTo **guarantee** the elimination of false sharing, we must ensure that no two counters modified by different threads reside on the same cache line. Since each thread $i$ modifies only $c[i]$, the condition is that for any two distinct threads $i$ and $j$, $c[i]$ and $c[j]$ must be on different cache lines. This can be achieved by structuring the data such that each `c[i]` is located in a memory block of at least $L=64$ bytes that is not shared with any other $c[j]$.\n\n**3. Evaluation of Options**\n\n**A. Pad each counter so that successive counters are separated by at least $L = 64$ bytes, and align the array base to $L = 64$ bytes, so each thread’s counter resides on a distinct cache line.**\nThis approach explicitly enforces the necessary condition. If the base address of the array is aligned to $64$ bytes, and each element is placed at an offset that is a multiple of $64$ bytes, then each element will start at the beginning of a new cache line. For example, the address of $c[i]$ would be $A_{base} + i \\times P$, where $P \\ge L = 64$. Since a cache line cannot span across two elements, and each element's start is at least $L$ bytes from the next, each $c[i]$ will be on a unique cache line (relative to other counters). This layout directly prevents false sharing.\nVerdict: **Correct**.\n\n**B. Keep the original contiguous array of $s = 8$-byte counters but align only the array base to $L = 64$ bytes; rely on base alignment to prevent false sharing.**\nThis describes the original problematic setup. As demonstrated in the derivation, aligning the base of a contiguous array of small elements does not prevent those elements from sharing a cache line. The first $8$ counters, $c[0]$ through $c[7]$, will still occupy the first cache line, leading to false sharing among the first $8$ threads.\nVerdict: **Incorrect**.\n\n**C. Replace the struct-of-arrays layout with an array-of-structs in which each per-thread struct contains the counter and is padded so the struct size is exactly $L = 64$ bytes; align the array base to $L = 64$ bytes.**\nThis is an alternative, and common, implementation of the strategy in option A. An array of structs like `struct { uint64_t counter; char padding[56]; } data[T];` would have a `sizeof` equal to $64$ bytes. Since the array is contiguous and its base is aligned to $64$ bytes, the address of the $i$-th struct, `data[i]`, will be $A_{base} + i \\times 64$. This means each struct, and thus each counter within it, will begin on a new cache line boundary. The counter for thread $i$ is on a different cache line from the counter for thread $j$ ($i \\neq j$). This guarantees the elimination of false sharing.\nVerdict: **Correct**.\n\n**D. Use atomic increment operations (e.g., atomic fetch-and-add) on the original contiguous array of $s = 8$-byte counters to prevent cache line ping-pong.**\nAtomic operations ensure that a read-modify-write sequence is performed as an indivisible unit from the perspective of the CPU's instruction stream. This is crucial for preventing data races in cases of *true sharing* (multiple threads updating the same variable). However, they do not alter the underlying memory layout or the behavior of the cache coherence protocol. An atomic write to $c[i]$ is still a write to a memory location within a cache line. If other threads are writing to other variables (like $c[j]$) on the same cache line, the hardware must still invalidate their copies to give the writing core exclusive access. Atomic operations do not and cannot prevent the cache line ping-pong caused by false sharing.\nVerdict: **Incorrect**.\n\n**E. Use a struct-of-arrays layout that groups all counters contiguously to improve spatial locality for the counter field; rely on improved locality to reduce coherence traffic.**\nThe initial problem\nsetup *is* a struct-of-arrays layout (or its equivalent for a simple array), where all counters are grouped contiguously. This design provides excellent spatial locality for a *sequential* access pattern (one thread iterating over all counters). However, for the given *parallel* access pattern where each thread repeatedly accesses its own single, distinct counter, this very spatial locality is the cause of false sharing. Placing the counters contiguously ensures they will share cache lines, which in turn *maximizes*, rather than reduces, coherence traffic.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3145329"}, {"introduction": "In distributed-memory systems, performance is often dictated by the efficiency of communication across the network. The standard latency/bandwidth model reveals that sending many small messages can be inefficient due to the high startup cost, often denoted as $\\alpha$, incurred by each message. This exercise guides you through a practical optimization problem: determining the optimal number of small messages to bundle into a single, larger transmission to minimize overall latency while respecting system constraints on memory and network overhead [@problem_id:3145373].", "problem": "A developer is optimizing a Message Passing Interface (MPI) communication path on a homogeneous cluster interconnect. The application generates small payloads destined to a single remote rank. Each payload is a fixed-size message of $s$ bytes. Messages arrive according to a Poisson process with rate $\\lambda$ messages per second. The developer considers bundling an integer threshold of $k$ arriving messages into one aggregated send operation using a single buffer, then transmitting the aggregated payload as one message. The buffer memory available for bundling at this destination is $B$ bytes. The communication cost of transmitting a message of size $m$ bytes over the interconnect is accurately modeled by the standard startup–bandwidth relation, where the time to send is the sum of a startup latency $\\alpha$ and a size-dependent term given by an inverse bandwidth factor $\\beta$ multiplied by $m$. The end-to-end latency for any message is defined to be the time from the instant the message arrives at the sender until the instant the entire aggregated send containing that message has been delivered to the receiver. Assume that transmission begins immediately upon the arrival of the $k$-th message in a bundle and that messages are processed instantly after arrival except for aggregation and transmission.\n\nTo control the startup (latency) cost, the system imposes a budget on startup time per unit wall-clock time: across the ongoing stream, the total startup time consumed per second must not exceed $C_{\\mathrm{max}}$ microseconds per second. The developer must choose the bundling threshold $k$ to minimize the expected end-to-end latency per message while satisfying both the startup budget and the buffer memory constraint.\n\nUse the following parameter values: $s = 128$ bytes, $\\lambda = 2400$ messages per second, $\\alpha = 3.5$ microseconds, $\\beta = 0.04$ microseconds per byte, $B = 16384$ bytes, and $C_{\\mathrm{max}} = 3000$ microseconds per second. Determine the integer bundling threshold $k$ that minimizes the expected end-to-end latency per message subject to the given constraints. Express your final answer as the integer $k$ with no units.", "solution": "The user wants to find the integer bundling threshold, $k$, that minimizes the expected end-to-end latency per message, subject to constraints on buffer memory and startup cost.\n\nFirst, we must formalize the constraints on $k$. The variable $k$ must be a positive integer.\n\nThe first constraint is the buffer memory size. A bundle comprises $k$ messages, each of size $s$. The total size of the aggregated payload is $m = k \\cdot s$. This aggregated message must fit within the available buffer of size $B$.\n$$k \\cdot s \\le B$$\nWe can solve for the upper bound on $k$:\n$$k \\le \\frac{B}{s}$$\nUsing the given parameter values, $s = 128$ bytes and $B = 16384$ bytes:\n$$k \\le \\frac{16384}{128} = \\frac{2^{14}}{2^7} = 2^7 = 128$$\nThus, the bundling threshold $k$ must be less than or equal to $128$.\n\nThe second constraint is the budget on startup time per unit wall-clock time. Messages arrive at a rate of $\\lambda$ messages per second. Since we bundle $k$ messages into a single send operation, the rate of these send operations is $\\frac{\\lambda}{k}$. Each send operation incurs a startup latency cost of $\\alpha$. The total startup time consumed per second is the product of the send rate and the cost per send:\n$$\\text{Startup time per second} = \\frac{\\lambda}{k} \\cdot \\alpha$$\nThis value must not exceed the maximum allowed budget, $C_{\\mathrm{max}}$:\n$$\\frac{\\lambda \\alpha}{k} \\le C_{\\mathrm{max}}$$\nWe can solve for the lower bound on $k$:\n$$k \\ge \\frac{\\lambda \\alpha}{C_{\\mathrm{max}}}$$\nUsing the given parameter values, $\\lambda = 2400$ messages/s, $\\alpha = 3.5$ $\\mu$s, and $C_{\\mathrm{max}} = 3000$ $\\mu$s/s:\n$$k \\ge \\frac{2400 \\times 3.5}{3000} = \\frac{8400}{3000} = 2.8$$\nSince $k$ must be an integer, the smallest possible value for $k$ that satisfies this constraint is $k=3$.\n\nCombining the constraints, the valid range for the integer bundling threshold $k$ is $3 \\le k \\le 128$.\n\nNext, we formulate the objective function, which is the expected end-to-end latency per message, denoted as $L(k)$. The latency for any message is the sum of its waiting time in the buffer, $T_{\\text{wait}}$, and the transmission time of the aggregated bundle, $T_{\\text{transmit}}$.\n$$L(k) = E[T_{\\text{wait}}] + T_{\\text{transmit}}$$\nThe transmission time, $T_{\\text{transmit}}$, is for an aggregated message of size $m = sk$. According to the startup–bandwidth model, this time is:\n$$T_{\\text{transmit}}(k) = \\alpha + \\beta m = \\alpha + \\beta s k$$\nThe waiting time depends on the arrival process. Messages arrive according to a Poisson process with rate $\\lambda$. This implies that the time intervals between successive message arrivals are independent and identically distributed exponential random variables with a mean of $\\frac{1}{\\lambda}$.\nA bundle is complete and transmission begins upon the arrival of the $k$-th message. Consider the messages in a bundle, indexed from $i=1$ to $k$ in order of arrival. The $i$-th message must wait for the subsequent $k-i$ messages to arrive. Therefore, the waiting time for the $i$-th message is the sum of $k-i$ inter-arrival times. The expected waiting time for the $i$-th message is:\n$$E[T_{\\text{wait},i}] = (k-i) \\times \\frac{1}{\\lambda}$$\nTo find the average expected waiting time per message in the bundle, we sum the expected waiting times for all $k$ messages and divide by $k$:\n$$E[T_{\\text{wait}}] = \\frac{1}{k} \\sum_{i=1}^{k} E[T_{\\text{wait},i}] = \\frac{1}{k} \\sum_{i=1}^{k} \\frac{k-i}{\\lambda} = \\frac{1}{k\\lambda} \\sum_{i=1}^{k} (k-i)$$\nThe summation term is the sum of integers from $0$ to $k-1$:\n$$\\sum_{i=1}^{k} (k-i) = (k-1) + (k-2) + \\dots + 0 = \\frac{(k-1)k}{2}$$\nSubstituting this back into the expression for the average expected waiting time:\n$$E[T_{\\text{wait}}] = \\frac{1}{k\\lambda} \\cdot \\frac{(k-1)k}{2} = \\frac{k-1}{2\\lambda}$$\nNow, we can write the full expression for the expected end-to-end latency per message, $L(k)$:\n$$L(k) = \\frac{k-1}{2\\lambda} + \\alpha + \\beta s k$$\nTo find the value of $k$ that minimizes $L(k)$, we can rearrange the terms to analyze its dependence on $k$:\n$$L(k) = \\frac{k}{2\\lambda} - \\frac{1}{2\\lambda} + \\alpha + \\beta s k = k \\left(\\beta s + \\frac{1}{2\\lambda}\\right) + \\left(\\alpha - \\frac{1}{2\\lambda}\\right)$$\nThis expression shows that $L(k)$ is a linear function of $k$ of the form $f(k) = M k + C$, where the slope is $M = \\beta s + \\frac{1}{2\\lambda}$ and the intercept is $C = \\alpha - \\frac{1}{2\\lambda}$.\nThe given parameters are $\\beta = 0.04 > 0$, $s = 128 > 0$, and $\\lambda = 2400 > 0$. The slope $M$ is the sum of two positive terms:\n$$M = (0.04 \\times 128) + \\frac{1}{2 \\times 2400} = 5.12 + \\frac{1}{4800} > 0$$\nSince the slope $M$ is positive, $L(k)$ is a strictly increasing function of $k$.\nTo minimize a strictly increasing function over a closed interval, we must choose the smallest value in that interval. The valid domain for $k$ is the set of integers in the interval $[3, 128]$.\nThe minimum value of $L(k)$ will therefore occur at the smallest valid integer value for $k$, which is $k=3$.", "answer": "$$\\boxed{3}$$", "id": "3145373"}]}