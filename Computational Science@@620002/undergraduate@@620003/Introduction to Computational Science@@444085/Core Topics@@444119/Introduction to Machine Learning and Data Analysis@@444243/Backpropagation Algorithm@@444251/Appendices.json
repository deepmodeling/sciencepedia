{"hands_on_practices": [{"introduction": "At its core, backpropagation is a specific application of a more general and powerful algorithm known as reverse-mode Automatic Differentiation (AD). This exercise takes you back to first principles, tasking you with building the AD engine itself [@problem_id:3100018]. By creating a \"tape\" to record a sequence of elementary operations and then replaying it in reverse, you will implement the fundamental mechanism for accumulating gradients, providing the deepest possible insight into how backpropagation efficiently computes derivatives for any complex computational graph.", "problem": "You are tasked with implementing reverse-mode Automatic Differentiation (AD) for a small scalar expression language and demonstrating how to compute adjoint variables, denoted by $\\bar{x} = \\partial L / \\partial x$, through a reverse replay of a tape of operations. Reverse-mode AD is synonymous with the backpropagation algorithm in computational graphs. Your implementation must start from first principles, specifically the chain rule of differentiation for composed functions, and the definition of a computational graph as a directed acyclic graph of primitive operations with a scalar loss $L$ at the root. You must design a tape structure that records the forward execution of primitive operations and then replay this tape in reverse to accumulate adjoints using the chain rule.\n\nYour small expression language must support scalar variables and constants, and the following primitive operations: binary addition $+$, binary subtraction $-$, binary multiplication $\\cdot$, binary division $\\div$, unary sine $\\sin(\\cdot)$, unary exponential $\\exp(\\cdot)$, and unary natural logarithm $\\log(\\cdot)$. All angles in trigonometric functions must be in radians. The domain constraints must be respected, for example the input to $\\log(\\cdot)$ must be strictly positive. You must design the computational tape to record each non-leaf operation with its operands and forward value to ensure a correct reverse replay.\n\nYour program must:\n- Build an internal computational graph and tape when evaluating a scalar loss $L$.\n- Compute the adjoint for each input variable $x_i$, that is $\\partial L / \\partial x_i$, via a single reverse replay of the tape starting from $\\bar{L} = \\partial L / \\partial L = 1$.\n- Produce, for each test case, a list whose first element is the scalar loss value $L$ and whose subsequent elements are the adjoints for the variables in the order they were introduced.\n\nStart from fundamental principles only: the chain rule for composed functions, the definition of adjoints $\\bar{v} = \\partial L / \\partial v$ for intermediate values $v$, and the semantics of a computational graph. Do not rely on pre-packaged differentiation formulas that skip the derivation path; instead derive and implement the local partial derivatives needed for the reverse replay using basic calculus for each primitive operation.\n\nImplement and run the following test suite. In each case, define the variables in the specified order, construct the expression using the primitive operations, and compute the outputs. All angles are in radians, and there are no physical units involved in this problem.\n\n- Test case $1$ (general composition): variables $x, y$, loss $L = \\sin(x \\cdot y) + \\exp(y)$, with $x = 0.5$, $y = -1.0$. Output format for this case: $[L, \\partial L / \\partial x, \\partial L / \\partial y]$.\n- Test case $2$ (boundary with zero and constants): variable $x$, loss $L = x \\cdot 0 + \\sin(0) + \\log(1)$, with $x = 2.0$. Output format: $[L, \\partial L / \\partial x]$.\n- Test case $3$ (repeated variable usage): variable $x$, loss $L = (x \\cdot x) \\cdot x$, with $x = 2.0$. Output format: $[L, \\partial L / \\partial x]$.\n- Test case $4$ (division and logarithm): variables $x, y$, loss $L = x \\div y + \\log(y)$, with $x = 1.0$, $y = 1.5$. Output format: $[L, \\partial L / \\partial x, \\partial L / \\partial y]$.\n- Test case $5$ (nested unary composition): variable $x$, loss $L = \\exp(\\sin(x))$, with $x = 0.0$. Output format: $[L, \\partial L / \\partial x]$.\n\nYour program should produce a single line of output containing the results of all test cases as a comma-separated list enclosed in square brackets, with each test case result itself being a comma-separated list enclosed in square brackets. For example, an output for two test cases would look like $[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$. Your final output must follow this format exactly, using standard floating-point numbers.", "solution": "The problem requires the implementation of reverse-mode Automatic Differentiation (AD), colloquially known as backpropagation, from first principles. This method computes the gradient of a scalar loss function $L$ with respect to a set of input variables $x_i$ by first performing a forward evaluation of the expression for $L$ to compute intermediate values and record a computational graph, followed by a reverse traversal of this graph to propagate gradients based on the chain rule.\n\n**Fundamental Principles: The Chain Rule and Adjoints**\n\nThe foundation of reverse-mode AD is the chain rule of calculus. If a scalar loss $L$ is a function of an intermediate variable $v_j$, which itself is a function of other variables $v_i$, the gradient of $L$ with respect to $v_i$ is given by the sum of contributions through all paths from $v_i$ to $L$. For a single path $L \\to v_j \\to v_i$, the chain rule states:\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\nIn the language of AD, we define the \"adjoint\" of a variable $v$ as $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$. Using this notation, the chain rule becomes:\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\nThe reverse-mode AD algorithm leverages this relationship by first computing the value of $L$ and then propagating the adjoints backward from $L$ to the input variables. The process starts by seeding the adjoint of the loss function itself, $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$.\n\n**The Computational Graph and Tape**\n\nAny scalar expression can be decomposed into a sequence of primitive operations (e.g., addition, multiplication, sine). This decomposition naturally forms a Directed Acyclic Graph (DAG), where nodes represent numerical values (input variables, constants, and intermediate results) and edges represent the primitive operations.\n\nThe forward evaluation of the expression, from inputs to the final loss $L$, is used to construct this graph. For our implementation, we use a \"tape\" data structure, which is a linearized representation of the graph. The tape is an ordered list of operations recorded during the forward pass. Each entry on the tape stores the type of operation, references to its input nodes, and a reference to its output node. This recording ensures we have the complete structure and all necessary intermediate values for the reverse pass.\n\n**The Forward Pass: Evaluation and Tape Recording**\n\nThe forward pass proceeds as follows:\n$1$. Input variables and constants are initialized as the starting nodes in our graph.\n$2$. The expression is evaluated sequentially. Each time a primitive operation is applied, two things happen:\n    a. The numerical result of the operation is computed and stored as a new node in the graph.\n    b. An entry is added to the tape, recording the operation type, its input node(s), and the newly created output node.\n\nFor example, for the expression $z = x \\cdot y$, we would compute the value of $z$ using the current values of $x$ and $y$, create a new node for $z$, and record `('mul', [x_node, y_node], z_node)` on the tape.\n\n**The Reverse Pass: Adjoint Accumulation**\n\nOnce the forward pass is complete and the final loss value $L$ is computed, the reverse pass begins. It traverses the tape in the reverse order of its creation.\n$1$. An array of adjoints, corresponding to each node in the graph, is initialized to zero.\n$2$. The adjoint of the final loss node is set to $1$, i.e., $\\bar{L} = 1$.\n$3$. For each operation $z = f(x_1, \\dots, x_n)$ on the tape (processed in reverse order):\n    a. We retrieve the already computed adjoint of the output, $\\bar{z}$.\n    b. We use the chain rule to calculate the contribution of $\\bar{z}$ to the adjoints of the inputs. The adjoint of each input $x_i$ is updated by accumulating this contribution:\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    The use of accumulation ($\\mathrel{+}=$) is crucial because a single variable might be used in multiple operations (i.e., it can be a parent to multiple children in the graph). Its total adjoint is the sum of the gradient signals flowing back from all its children. Reversing the tape guarantees that a node's adjoint ($\\bar{z}$) is fully calculated before it is propagated to its own inputs ($x_i$).\n\n**Adjoint Update Rules for Primitive Operations**\n\nThe local partial derivatives $\\frac{\\partial z}{\\partial x_i}$ are known for each primitive operation. The values of the inputs required for these derivatives (e.g., for $z = x \\cdot y$, $\\frac{\\partial z}{\\partial x} = y$) are available from the forward pass.\n\n- **Addition:** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= \\bar{z}$.\n\n- **Subtraction:** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= -\\bar{z}$.\n\n- **Multiplication:** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$.\n\n- **Division:** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$.\n\n- **Sine:** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$.\n\n- **Exponential:** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$.\n\n- **Natural Logarithm:** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$.\n\n**Implementation Design**\n\nThe implementation uses two main classes: `Graph` and `Node`. The `Graph` class manages the state of the computation: it stores the `values` of all nodes, the operation `tape`, and the computed `adjoints`. The `Node` class acts as a wrapper around a node's ID, providing an intuitive interface by overloading Python's arithmetic operators (`+`, `*`, etc.). When an operation like `c = a + b` is performed on `Node` objects, it transparently calls a method on the associated `Graph` object, which performs the forward calculation, records the operation on the tape, and returns a new `Node` for the result `c`. This object-oriented design allows for the construction of expressions in a natural way while correctly building the computational graph in the background. After the final loss `Node` is computed, a call to `Graph.compute_gradients()` executes the reverse pass as described above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "With an understanding of the core gradient calculation engine, we now apply it to the fundamental building block of a neural network: a single neuron. This exercise connects the abstract mechanics of backpropagation to the concrete goal of optimization [@problem_id:3099996]. You will not only use the chain rule to find parameters that perfectly fit a small dataset but also analyze the geometry of the loss surface using the Hessian matrix to verify that you have found a true local minimum, clarifying why computing gradients is the key to training models.", "problem": "You are given a single-neuron model with a linear activation, defined by the parametric function $f(x; \\theta) = W x + b$, where $\\theta = (W, b)$, $W \\in \\mathbb{R}$, and $b \\in \\mathbb{R}$. The training set consists of three input-output pairs $(x_i, y_i)$ for $i = 1, 2, 3$, specifically $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The empirical risk is the half-sum of squared errors, defined by\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\nStarting from the fundamental definition of the chain rule of calculus and the definition of the gradient and Hessian (the matrix of second-order partial derivatives), do the following:\n1. Choose parameters $W$ and $b$ that exactly fit the three data points, meaning that $f(x_i; \\theta) = y_i$ for every $i \\in \\{1, 2, 3\\}$.\n2. Using backpropagation (that is, the chain rule applied to the computational graph of the model), derive the gradient $\\nabla_{\\theta} J(\\theta)$ and evaluate it at the exact-fit parameters you chose in part $1$.\n3. Derive the Hessian $H(\\theta)$ of $J(\\theta)$ with respect to $\\theta$ and evaluate it at the exact-fit parameters. Compute the minimum eigenvalue $\\lambda_{\\min}(H)$.\n4. Based on the sign of $\\lambda_{\\min}(H)$, briefly state whether the exact-fit point is a local minimum or a saddle point for $J(\\theta)$.\n\nProvide your final answer as the exact value of $\\lambda_{\\min}(H)$ at the solution. No rounding is required.", "solution": "The problem has been validated and is scientifically sound, well-posed, objective, and contains sufficient information for a unique solution.\n\nThe task is to analyze the empirical risk function $J(\\theta)$ for a single linear neuron model $f(x; \\theta) = W x + b$ with parameters $\\theta = (W, b)$. The risk is defined as the half-sum of squared errors over three data points: $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The risk function is:\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\nSubstituting the given data points:\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. Find the exact-fit parameters $\\theta^* = (W, b)$**\n\nFor an exact fit, the model must satisfy $f(x_i; \\theta) = y_i$ for all $i \\in \\{1, 2, 3\\}$. This yields a system of linear equations for $W$ and $b$:\n\\begin{enumerate}\n    \\item For $(x_1, y_1) = (0, 1)$: $W(0) + b = 1 \\implies b = 1$.\n    \\item For $(x_2, y_2) = (1, 3)$: $W(1) + b = 3 \\implies W + b = 3$.\n    \\item For $(x_3, y_3) = (2, 5)$: $W(2) + b = 5 \\implies 2W + b = 5$.\n\\end{enumerate}\nSubstituting $b = 1$ from the first equation into the second gives $W + 1 = 3$, which implies $W = 2$.\nWe must verify that these values satisfy the third equation: $2W + b = 2(2) + 1 = 4 + 1 = 5$, which is consistent with $y_3 = 5$.\nThus, the parameters for an exact fit are $W = 2$ and $b = 1$. Let us denote this point as $\\theta^* = (2, 1)$.\n\n**2. Derive and evaluate the gradient $\\nabla_{\\theta} J(\\theta)$ at $\\theta^*$**\n\nThe gradient of $J(\\theta)$ with respect to $\\theta = (W, b)$ is $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$.\nUsing the chain rule, as specified by the backpropagation methodology, we define the error for each point as $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$. The loss is $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$.\nThe partial derivatives are:\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\nAt the exact-fit point $\\theta^* = (2, 1)$, by definition, the error terms are zero: $e_i(\\theta^*) = Wx_i + b - y_i = 0$ for all $i$.\nTherefore, evaluating the gradient at $\\theta^*$:\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\nThe gradient at the exact-fit point is the zero vector: $\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This confirms that $\\theta^*$ is a critical point of the loss function $J(\\theta)$.\n\n**3. Derive the Hessian $H(\\theta)$ and compute its minimum eigenvalue**\n\nThe Hessian matrix $H(\\theta)$ contains the second-order partial derivatives of $J(\\theta)$:\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2}  \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W}  \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\nWe compute these by differentiating the first-order partial derivatives:\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\nNote that $\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$, as expected. The Hessian is constant and does not depend on $W$ or $b$. We evaluate the sums using the given inputs $x_1=0$, $x_2=1$, $x_3=2$:\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\nThe Hessian matrix is:\n$$\nH = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $H$ are the roots of the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 5-\\lambda  3 \\\\ 3  3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\nUsing the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\nSimplifying $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$:\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\nThe two eigenvalues are $\\lambda_1 = 4 + \\sqrt{10}$ and $\\lambda_2 = 4 - \\sqrt{10}$. The minimum eigenvalue is $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$.\n\n**4. Classify the critical point $\\theta^*$**\n\nTo classify the critical point $\\theta^*$, we examine the signs of the eigenvalues of the Hessian matrix evaluated at that point. Since $H$ is constant, we use the eigenvalues just computed.\nWe know that $3 = \\sqrt{9}  \\sqrt{10}  \\sqrt{16} = 4$.\nTherefore, the minimum eigenvalue $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ is positive, as $4  \\sqrt{10}$.\nThe maximum eigenvalue $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ is also clearly positive.\nSince both eigenvalues of the Hessian are positive, the Hessian matrix is positive definite. According to the second partial derivative test, a critical point at which the Hessian is positive definite is a local minimum. For this quadratic loss function, it is the unique global minimum. The exact-fit point is a local minimum.\nThe final answer is the value of the minimum eigenvalue.", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}, {"introduction": "Modern neural networks rely on non-linear activation functions, but their mathematical properties can introduce practical training challenges. This exercise explores a classic pitfall known as the \"dying ReLU\" problem, where a neuron can become stuck in an inactive state and cease to learn [@problem_id:3100021]. By analyzing the conditions that cause the gradient to vanish and investigating how alternative designs like the leaky ReLU provide a remedy, you will gain a deeper appreciation for how the choice of activation function directly interacts with the flow of gradients during backpropagation.", "problem": "Consider a single neuron with input vector $x \\in \\mathbb{R}^d$, weight vector $w \\in \\mathbb{R}^d$, bias $b \\in \\mathbb{R}$, pre-activation $z = w^\\top x + b$, and activation $a = \\phi(z)$. Assume training data are drawn from a distribution supported on the hypercube $[0,1]^d$ and that the scalar training loss $\\mathcal{L}$ depends differentiably on $a$ and possibly on a target $y$. The activation $\\phi$ is either the Rectified Linear Unit (ReLU), defined by $\\phi(z) = \\max\\{0,z\\}$, or the leaky variant with negative slope parameter $\\alpha \\in (0,1)$, defined by $\\phi(z) = \\max\\{\\alpha z, z\\}$. There is no explicit parameter regularization unless stated. Using fundamental definitions (linearity of $z$, the support constraint on $x$, the definition of the activation, and the multivariate chain rule), analyze when the neuron becomes inactive on the data distribution, and how this affects the gradient during backpropagation.\n\nSelect all statements that are correct:\n\nA. If inputs $x$ have support contained in $[0,1]^d$, then the condition\n$$\nb + \\sum_{i=1}^d \\max\\{w_i, 0\\}  0\n$$\nis sufficient to ensure $\\Pr(z0)=1$ under the data distribution.\n\nB. Under the condition in option A and with the Rectified Linear Unit activation, for any differentiable data loss that depends on $a$ only through $z$ via the activation, the gradients of the data loss with respect to $w$ and $b$ produced by backpropagation are zero almost surely on the data distribution.\n\nC. If the activation is replaced by the leaky variant with slope $\\alpha \\in (0,1)$, then whenever $z0$ one has\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\alpha \\,\\frac{\\partial \\mathcal{L}}{\\partial a}\\, x\n\\quad\\text{and}\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\alpha \\,\\frac{\\partial \\mathcal{L}}{\\partial a},\n$$\nso that nonzero upstream gradient $\\partial \\mathcal{L}/\\partial a$ yields nonzero parameter gradients even when $z0$.\n\nD. Applying batch normalization to $z$ before the activation makes it impossible to have $\\Pr(z0)=1$ for any choice of $w$ and $b$.\n\nE. Adding an $\\ell_2$ penalty $\\lambda \\lVert w \\rVert_2^2$ to the loss always guarantees a nonzero gradient with respect to $w$ even if $z0$ for all inputs under the Rectified Linear Unit activation.", "solution": "The problem statement is a valid theoretical exercise in the analysis of artificial neurons and the backpropagation algorithm. It is scientifically grounded in the principles of computational science and machine learning, well-posed, objective, and contains all necessary information to evaluate the provided statements.\n\nThe analysis hinges on fundamental principles: the definition of the neuron's pre-activation, the properties of the specified activation functions, and the multivariate chain rule for computing gradients.\n\nLet the neuron's parameters be the weight vector $w \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$. For an input vector $x \\in \\mathbb{R}^d$, the pre-activation is $z = w^\\top x + b$. The activation is $a = \\phi(z)$. The training loss $\\mathcal{L}$ is a differentiable function of the activation $a$.\n\nThe gradients of the loss with respect to the parameters are computed via the chain rule (backpropagation):\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w_i}\n\\quad \\implies \\quad\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} x\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} (1)\n$$\nwhere $\\frac{\\partial \\mathcal{L}}{\\partial a}$ is the upstream gradient, and $\\frac{\\partial a}{\\partial z} = \\phi'(z)$ is the local gradient of the activation function.\n\nA neuron is considered \"inactive\" on the data distribution if its pre-activation $z$ falls into a regime where $\\phi'(z)$ is zero or very small for all inputs $x$ from the distribution's support. The problem specifies that the support of the input data distribution is contained in the hypercube $[0,1]^d$.\n\n### Option A Evaluation\n\nThis statement provides a condition for the neuron's pre-activation $z$ to be negative for all possible inputs from the data distribution. This implies that the neuron is always in the regime $z  0$. To verify this, we need to find the maximum possible value of $z$ given that $x \\in [0,1]^d$.\nThe pre-activation is $z = w^\\top x + b = \\sum_{i=1}^d w_i x_i + b$. To maximize this linear function of $x$ over the hypercube $[0,1]^d$, we must choose the components $x_i$ at their bounds to maximize each term $w_i x_i$:\n- If $w_i  0$, we choose $x_i = 1$ to get a contribution of $w_i$.\n- If $w_i \\le 0$, we choose $x_i = 0$ to get a contribution of $0$.\n\nThe maximum value of the term $w_i x_i$ over $x_i \\in [0,1]$ is therefore $\\max\\{w_i, 0\\}$. The maximum value of $z$ is:\n$$\nz_{\\max} = \\max_{x \\in [0,1]^d} (w^\\top x + b) = \\left( \\sum_{i=1}^d \\max_{x_i \\in [0,1]} (w_i x_i) \\right) + b = \\left( \\sum_{i=1}^d \\max\\{w_i, 0\\} \\right) + b\n$$\nIf $z_{\\max}  0$, then for any input vector $x$ from the support, $z = w^\\top x + b \\le z_{\\max}  0$. This ensures that $z$ is always negative. The condition given in the option, $b + \\sum_{i=1}^d \\max\\{w_i, 0\\}  0$, is precisely $z_{\\max}  0$. If this condition holds, then for any data point $x$ drawn from the distribution, $z(x)  0$, which means $\\Pr(z0)=1$.\nTherefore, the statement is **Correct**.\n\n### Option B Evaluation\n\nThis statement examines the consequence of the condition in A when using the Rectified Linear Unit (ReLU) activation, $\\phi(z) = \\max\\{0, z\\}$. The derivative of the ReLU function is:\n$$\n\\phi'(z) = \\frac{d}{dz}\\max\\{0, z\\} = \\begin{cases} 1  \\text{if } z  0 \\\\ 0  \\text{if } z  0 \\end{cases}\n$$\n(At $z=0$, the function is not differentiable, but a subgradient of $0$ is commonly used in implementations).\nThe condition from option A ensures that $z  0$ for all inputs $x$ from the data support. Thus, for any such input, the gradient of the activation is $\\frac{\\partial a}{\\partial z} = \\phi'(z) = 0$.\nSubstituting this into the chain rule expressions for the parameter gradients:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot (0) \\cdot x = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot (0) \\cdot (1) = 0\n$$\nSince this holds for every point in the data distribution's support, the gradients of the data loss with respect to $w$ and $b$ are zero almost surely. This phenomenon is known as the \"dying ReLU\" problem.\nTherefore, the statement is **Correct**.\n\n### Option C Evaluation\n\nThis statement considers the leaky ReLU activation, $\\phi(z) = \\max\\{\\alpha z, z\\}$ with $\\alpha \\in (0,1)$. The derivative is:\n$$\n\\phi'(z) = \\frac{d}{dz}\\max\\{\\alpha z, z\\} = \\begin{cases} 1  \\text{if } z  0 \\\\ \\alpha  \\text{if } z  0 \\end{cases}\n$$\nThe statement considers the case where $z0$. In this regime, the local gradient is $\\frac{\\partial a}{\\partial z} = \\phi'(z) = \\alpha$.\nUsing the chain rule:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot \\alpha \\cdot x\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot \\alpha \\cdot 1\n$$\nThese expressions exactly match the formulas given in the option. Since $\\alpha \\in (0,1)$, $\\alpha$ is nonzero. If the upstream gradient $\\frac{\\partial \\mathcal{L}}{\\partial a}$ is nonzero, then the gradient $\\frac{\\partial \\mathcal{L}}{\\partial b}$ is guaranteed to be nonzero. The gradient $\\frac{\\partial \\mathcal{L}}{\\partial w}$ will also be nonzero, provided the input vector $x$ is not the zero vector. A nonzero gradient \"flows\" back through the neuron even when it is in its negative pre-activation regime.\nTherefore, the statement is **Correct**.\n\n### Option D Evaluation\n\nThis statement claims that applying batch normalization (BN) to $z$ makes it impossible to have $\\Pr(z0)=1$. This statement is ambiguous about which variable is being referred to in $\\Pr(z0)=1$.\nInterpretation 1: The variable is the original pre-activation $z = w^\\top x + b$. The application of a BN layer after $z$ does not change the mathematical properties of $z$ itself. We can still choose parameters $w$ and $b$ satisfying the condition from option A (e.g., $w$ as a vector of $-1$s and $b=-1$), which makes $z$ always negative. The BN layer being present in the architecture does not make this choice of parameters impossible. Under this interpretation, the statement is false.\nInterpretation 2: The variable is the input to the activation function, which, after applying BN, is the batch-normalized pre-activation, let's call it $z_{\\text{BN}}$. The claim is that `Pr(z_{\\text{BN}}0)=1` is impossible. At training time, BN operates on a mini-batch of pre-activations $\\{z^{(1)}, \\dots, z^{(m)}\\}$. Let $\\mu_B$ and $\\sigma_B^2$ be the mini-batch mean and variance. The output is $z_{\\text{BN}}^{(i)} = \\gamma \\hat{z}^{(i)} + \\beta$, where $\\hat{z}^{(i)} = (z^{(i)} - \\mu_B)/\\sqrt{\\sigma_B^2 + \\epsilon}$, and $\\gamma, \\beta$ are learnable parameters. The standardized values $\\hat{z}^{(i)}$ have zero mean over the mini-batch, i.e., $\\sum_{i=1}^m \\hat{z}^{(i)} = 0$. For any non-trivial batch (where not all $z^{(i)}$ are identical), this requires that some $\\hat{z}^{(i)}$ are positive and some are negative. Let $\\hat{z}_{\\max}(B) = \\max_{i \\in B} \\hat{z}^{(i)}  0$. The condition $z_{\\text{BN}}^{(i)}  0$ for all $i$ in the batch would require $\\gamma \\hat{z}_{\\max}(B) + \\beta  0$. This is equivalent to $\\beta  -\\gamma \\hat{z}_{\\max}(B)$. Since $\\beta$ is a learnable parameter, it is possible for an optimization algorithm to drive it to a sufficiently negative value such that this inequality holds for any mini-batch $B$ (since the inputs $x$ are bounded, the values of $\\hat{z}_{\\max}(B)$ are also bounded). Therefore, it is not \"impossible\" to have $z_{BN}  0$ for all inputs.\nUnder both reasonable interpretations, the claim of impossibility is false.\nTherefore, the statement is **Incorrect**.\n\n### Option E Evaluation\n\nThis statement considers adding an $\\ell_2$ penalty $\\lambda \\lVert w \\rVert_2^2$ to the loss function. The total loss is $\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda \\lVert w \\rVert_2^2$.\nThe gradient of the total loss with respect to $w$ is the sum of the data loss gradient and the regularization gradient:\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{total}}}{\\partial w} = \\frac{\\partial \\mathcal{L}_{\\text{data}}}{\\partial w} + \\frac{\\partial}{\\partial w} (\\lambda \\lVert w \\rVert_2^2) = \\frac{\\partial \\mathcal{L}_{\\text{data}}}{\\partial w} + 2\\lambda w\n$$\nThe premise is that the neuron is inactive for all inputs under the ReLU activation, i.e., $z  0$ for all $x$ in the support. As established in the analysis of option B, this implies that the data-dependent part of the gradient is zero: $\\frac{\\partial \\mathcal{L}_{\\text{data}}}{\\partial w} = 0$.\nThe total gradient thus simplifies to:\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{total}}}{\\partial w} = 2\\lambda w\n$$\nThe statement claims this gradient is \"always\" nonzero. This is true if and only if $w \\neq 0$ (assuming $\\lambda0$). However, it is possible for the weight vector $w$ to be the zero vector, $w=0$.\nIf $w=0$, the pre-activation becomes $z = 0^\\top x + b = b$. The condition $z0$ for all inputs simplifies to $b0$. This is a valid state for the neuron's parameters. For example, if $w=0$ and $b=-1$, the neuron is inactive for all inputs. In this specific case, the total gradient with respect to $w$ is $\\frac{\\partial \\mathcal{L}_{\\text{total}}}{\\partial w} = 2\\lambda(0) = 0$.\nSince there exists a valid case where the gradient is zero, the claim that it is \"always\" nonzero is false.\nTherefore, the statement is **Incorrect**.", "answer": "$$\\boxed{ABC}$$", "id": "3100021"}]}