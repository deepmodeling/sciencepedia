{"hands_on_practices": [{"introduction": "A fundamental challenge in modeling is selecting an appropriate level of model complexity. A model that is too simple may fail to capture the underlying patterns in the data (underfitting), while a model that is too complex may fit the noise in the training data instead of the true signal (overfitting). This practice explores the classic bias-variance tradeoff by analyzing the \"generalization gap\" — the difference between validation and training error — to quantify overfitting risk and systematically select an optimal polynomial degree for a regression model [@problem_id:3107026].", "problem": "You are given observations from polynomial regression models of degree $d$ fitted to the same dataset. For each test case, you are provided, for a set of degrees $d \\in \\mathbb{N}$, the empirical training Mean Squared Error (MSE) $E_{\\text{train}}(d)$ and the empirical cross-validation MSE $E_{\\text{val}}(d)$. Your task is to predict overfitting risk by regressing the generalization gap $g(d)$ versus $d$, and to classify the optimal degree that minimizes an estimate of the expected generalization error.\n\nFundamental base and definitions:\n- The generalization error at complexity $d$ can be expressed as $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$, where $g(d)$ is the generalization gap defined by $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$. In practice, $E_{\\text{test}}(d)$ is unknown and cross-validation is used as a well-tested proxy, so we estimate the gap as $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- Overfitting risk increasing with model complexity is reflected by a positive slope of $g(d)$ as a function of $d$.\n\nAlgorithmic requirement:\n- For each test case, compute $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$ at each observed degree $d_k$.\n- Fit a straight line $g(d) \\approx a + b\\,d$ to the observed pairs $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS).\n- Form the estimated expected generalization error at each observed degree as $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n- Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. If multiple degrees attain the same minimum within numerical tolerance, choose the smallest such degree (prefer lower complexity).\n- Report two quantities per test case: the estimated slope $\\widehat{b}$ of the gap-versus-degree regression, rounded to $4$ decimal places, and the classified optimal degree $\\widehat{d}^{\\star}$ as an integer.\n\nTest suite:\n- Test case $1$: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n- Test case $2$: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n- Test case $3$: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n- Test case $4$: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n- Test case $5$: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n\nOutput specification:\n- For each test case, output a two-element list $[\\widehat{b}, \\widehat{d}^{\\star}]$, where $\\widehat{b}$ is the slope rounded to $4$ decimal places, and $\\widehat{d}^{\\star}$ is an integer.\n- Your program should produce a single line of output containing one list that aggregates the per-test-case outputs in order, with no spaces. Each element must itself be a two-element list as described. The final printout must be exactly one line.\n\nNo physical units are involved. All angles, if any, are irrelevant here. No percentages are required.\n\nYour program must be a complete, runnable program that defines the test cases above internally and produces the specified single-line output.", "solution": "### Step 1: Extract Givens\n- **Models**: Polynomial regression models of degree $d \\in \\mathbb{N}$.\n- **Data per test case**: A set of degrees $d_k$, corresponding training Mean Squared Error (MSE) $E_{\\text{train}}(d_k)$, and cross-validation MSE $E_{\\text{val}}(d_k)$.\n- **Definitions**:\n    - Generalization error: $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$.\n    - Generalization gap: $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$.\n    - Empirical generalization gap estimate: $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- **Algorithmic Requirements**:\n    1.  Compute the empirical gap for each degree: $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$.\n    2.  Fit a straight line $g(d) \\approx a + b\\,d$ to the data points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS) to find the estimated intercept $\\widehat{a}$ and slope $\\widehat{b}$.\n    3.  Estimate the expected generalization error at each degree: $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n    4.  Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. The tie-breaking rule is to choose the smallest degree if multiple degrees yield the same minimum.\n    5.  Report the slope $\\widehat{b}$ rounded to $4$ decimal places and the integer optimal degree $\\widehat{d}^{\\star}$.\n- **Test Suite**:\n    - **Test case 1**: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n    - **Test case 2**: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n    - **Test case 3**: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n    - **Test case 4**: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n    - **Test case 5**: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n- **Output Specification**: A single-line list of two-element lists $[\\widehat{b}, \\widehat{d}^{\\star}]$ for each test case, with no spaces in the final output string.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is based on fundamental concepts in statistical learning theory, including the bias-variance trade-off, overfitting, training error, validation error, and generalization error. The use of Ordinary Least Squares to model the relationship between model complexity and the generalization gap is a standard and sound analytical technique.\n- **Well-Posed**: The problem is well-posed. For each test case, the number of distinct data points for the OLS regression is at least $2$, ensuring a unique solution for the line parameters. The search for the optimal degree is a minimization over a finite set, which is guaranteed to have a solution. The tie-breaking rule ensures this solution is unique.\n- **Objective**: The problem is stated using precise mathematical definitions and a clear, unambiguous algorithmic procedure. The inputs are numerical, and the required outputs are well-defined.\n- **Flaw Checklist**: The problem does not violate any of the specified flaws. It is scientifically sound, formalizable, complete, realistic, and well-posed. It requires non-trivial computation and reasoning.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires an analysis of model performance as a function of complexity, specifically the degree $d$ of a polynomial regression model. The core of the analysis lies in understanding and modeling the generalization gap, $g(d)$, which represents the difference between a model's performance on unseen data (approximated by validation error, $E_{\\text{val}}$) and its performance on the training data ($E_{\\text{train}}$). A growing gap with increasing complexity is a hallmark of overfitting.\n\nThe specified algorithm aims to create a smoothed estimate of the generalization error to make a more robust model selection decision than simply picking the model with the lowest validation error. The validation error itself can be noisy, and modeling the trend of the generalization gap can help filter out this noise.\n\nThe procedure for each test case is as follows:\n\n1.  **Compute the Empirical Generalization Gap**: For each given polynomial degree $d_k$, we calculate the empirical generalization gap, $g(d_k)$, using the provided training and validation errors:\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **Model the Generalization Gap via OLS**: We hypothesize a linear relationship between the model complexity $d$ and the generalization gap $g(d)$. We fit a line, $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$, to the set of observed points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS). The slope $\\widehat{b}$ and intercept $\\widehat{a}$ are chosen to minimize the sum of squared differences $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$. For a set of $n$ points, the OLS estimators are given by:\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    where $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ and $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ are the sample means. The slope $\\widehat{b}$ serves as a direct measure of overfitting risk; a positive $\\widehat{b}$ indicates that the gap between training and validation performance widens as model complexity increases.\n\n3.  **Estimate the Generalization Error**: Using the linear model for the gap, we construct a smoothed estimate of the true generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$:\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    This estimate combines the directly observed training error, which reflects how well the model fits the data, with a regularized estimate of the generalization penalty, which accounts for complexity.\n\n4.  **Determine the Optimal Degree**: The optimal degree, $\\widehat{d}^{\\star}$, is selected as the degree from the given set $\\{d_k\\}$ that minimizes our estimated generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$.\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    The problem specifies that if a tie occurs, the smallest degree among those that achieve the minimum error should be chosen. This reflects the principle of parsimony (Occam's razor): when all else is equal, prefer the simpler model.\n\nFinally, for each test case, we report the calculated slope $\\widehat{b}$ (a measure of overfitting risk) and the determined optimal degree $\\widehat{d}^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3107026"}, {"introduction": "The effectiveness of standard Ordinary Least Squares (OLS) regression hinges on several key assumptions, including the assumption of constant error variance, known as homoscedasticity. This exercise provides a complete, hands-on workflow for what to do when this assumption is violated. You will first learn to formally classify whether a dataset exhibits non-constant variance (heteroscedasticity) using the Breusch-Pagan test, and then proceed to correct for it by implementing a more powerful technique called Weighted Least Squares (WLS) [@problem_id:3107027].", "problem": "You are asked to write a complete, runnable program that, for several synthetic datasets, performs three tasks grounded in the classical linear model: classify the presence of heteroscedasticity of the error term conditional variance $\\operatorname{Var}(\\epsilon \\mid x)$ using residual diagnostics formalized by a Lagrange Multiplier principle, estimate a parametric variance function $\\sigma^{2}(x)$ by regression, and refit the regression by weighted least squares. Your implementation must follow a principle-based method derived from fundamental definitions and well-tested results, as specified below.\n\nThe base model is the simple linear regression\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n,\n$$\nwhere the errors satisfy $\\mathbb{E}[\\epsilon_i \\mid x_i] = 0$. Under homoscedasticity, the conditional variance is constant, $\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2$, while under heteroscedasticity it varies with $x_i$, $\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2(x_i)$. Ordinary least squares (OLS) chooses coefficients that minimize the sum of squared residuals,\n$$\n(\\widehat{\\beta}_0,\\widehat{\\beta}_1) = \\arg\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2.\n$$\nDefine residuals $r_i = y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 x_i$.\n\nTask A (Heteroscedasticity classification via residual plots formalized as a test): Use the Breusch–Pagan Lagrange Multiplier approach as a formal proxy for residual-pattern diagnostics. Regress the squared residuals on the regressors and a constant,\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i,\n$$\nand compute the coefficient of determination $R^2$ of this auxiliary regression. The Breusch–Pagan test statistic is\n$$\n\\mathrm{LM} = n R^2,\n$$\nwhich, under the null hypothesis of homoscedasticity, follows approximately a chi-squared distribution with $k$ degrees of freedom, where $k$ is the number of non-constant regressors in the auxiliary regression. For this problem, $k = 1$. Classify a dataset as heteroscedastic if the chi-squared survival probability satisfies\n$$\np\\text{-value} = \\Pr\\left(\\chi^2_k \\ge \\mathrm{LM}\\right) < 0.05.\n$$\n\nTask B (Variance function regression): Using the OLS residuals, fit a positive variance model by regressing the log-squared residuals on a fixed feature of $x$:\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i,\n$$\nwhere $\\varepsilon$ is a small positive constant to avoid taking the logarithm of zero. Then define the fitted variance at each $x_i$ as\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right).\n$$\nThis ensures $\\widehat{\\sigma}_i^2 > 0$ for all $i$.\n\nTask C (Weighted least squares refit): With weights $w_i = 1/\\widehat{\\sigma}_i^2$, compute the weighted least squares (WLS) estimator that minimizes\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2,\n$$\nand report the refitted coefficients. Also compute the mean squared error (MSE) of OLS and WLS fits, defined as\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2,\\quad\n\\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2.\n$$\n\nImplementation details you must follow:\n- Use the following test suite with a fixed random seed for reproducibility. For each case, draw $x_i$ independently and uniformly as specified, generate $\\epsilon_i$ as independent Gaussian noise with mean $0$ and the specified standard deviation function, and set $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$.\n- Random seed: use seed value $1337$ for all random generation.\n- For each dataset, use the exact parameters:\n\n1. Case $1$ (homoscedastic, “happy path”): $n = 200$, $\\beta_0 = 1.25$, $\\beta_1 = -0.75$, $x_i \\sim \\mathrm{Uniform}[-2,2]$, noise standard deviation $\\sigma(x) \\equiv 0.5$ (constant).\n2. Case $2$ (clearly heteroscedastic, increasing with $|x|$): $n = 200$, $\\beta_0 = 0.5$, $\\beta_1 = 1.5$, $x_i \\sim \\mathrm{Uniform}[-3,3]$, noise standard deviation $\\sigma(x) = 0.3 + 0.7|x|$.\n3. Case $3$ (edge case with moderate sample size and curved variance): $n = 60$, $\\beta_0 = 0.0$, $\\beta_1 = 2.0$, $x_i \\sim \\mathrm{Uniform}[-2,2]$, noise standard deviation $\\sigma(x) = 0.2 + 0.4(x+1)^2$.\n\n- In the variance-regression step, use $\\varepsilon = 10^{-8}$.\n- In the Breusch–Pagan auxiliary regression, include an intercept and the regressor $x_i$. Use $k=1$ degrees of freedom for the chi-squared reference distribution.\n\nRequired outputs for each dataset:\n- A list of the form\n$$\n[\\text{hetero\\_boolean},\\; p\\_value,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_1,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_1,\\; \\mathrm{MSE}_{\\mathrm{OLS}},\\; \\mathrm{MSE}_{\\mathrm{WLS}}].\n$$\n- The boolean must be capitalized as in Python ($\\mathrm{True}$ or $\\mathrm{False}$). All floats must be rounded to exactly $6$ decimal places.\n\nFinal program requirements:\n- Your program must construct all datasets internally using the above test suite, perform Tasks A–C for each dataset, and produce a single line of output containing a list with three elements (one per test case), where each element is the list described above. The format must be a single line:\n\"[$\\dots$]\" with comma-separated entries and no extra text. For example: \"[[True,0.012345,1.234000, ...],[...],[...]]\".\n- No user input, no external files, and no network access are permitted. The only allowed libraries are Numerical Python (NumPy) and Scientific Python (SciPy).", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and objective problem in computational statistics. It outlines a standard procedure for identifying and correcting for heteroscedasticity in a simple linear regression context. All necessary parameters, models, and evaluation criteria are specified unambiguously.\n\nThe solution will be developed by systematically implementing the three specified tasks for each of the three synthetic datasets. The methodology relies on fundamental principles of linear regression, statistical testing, and weighted least squares estimation.\n\n### Principle-Based Method\n\nThe core of the problem is to address heteroscedasticity, a condition where the variance of the error term $\\epsilon_i$ is not constant across observations. The standard Ordinary Least Squares (OLS) estimator, while still unbiased under heteroscedasticity, is no longer the Best Linear Unbiased Estimator (BLUE). The provided tasks outline a common workflow to diagnose and mitigate this issue.\n\n#### 1. Data Generation and Initial OLS Fit\n\nFor each test case, we first generate a synthetic dataset according to the specified model:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n\n$$\nThe independent variable $x_i$ is drawn from a uniform distribution, and the error term $\\epsilon_i$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2(x_i))$, where the standard deviation $\\sigma(x_i)$ is specific to each case.\n\nWe begin by fitting this model using Ordinary Least Squares (OLS). The OLS estimators $(\\widehat{\\beta}^{\\mathrm{OLS}}_0, \\widehat{\\beta}^{\\mathrm{OLS}}_1)$ are found by minimizing the sum of squared residuals. In matrix form, with a design matrix $\\mathbf{X}$ (where the first column is all ones and the second is the vector of $x_i$ values) and response vector $\\mathbf{y}$, the OLS coefficient vector $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}$ is given by:\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nFrom this fit, we compute the OLS residuals $r_i = y_i - (\\widehat{\\beta}^{\\mathrm{OLS}}_0 + \\widehat{\\beta}^{\\mathrm{OLS}}_1 x_i)$. These residuals are crucial for the subsequent diagnostic and modeling steps.\n\n#### 2. Task A: Heteroscedasticity Classification (Breusch-Pagan Test)\n\nThe first task is to formally test for the presence of heteroscedasticity. The Breusch-Pagan test is based on the idea that if heteroscedasticity is present, the squared residuals $r_i^2$ (which are proxies for the true error variance $\\sigma_i^2$) should be systematically related to the independent variables.\n\nWe perform an auxiliary OLS regression:\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i\n$$\nThe test statistic is the Lagrange Multiplier (LM) statistic, calculated using the coefficient of determination, $R^2$, from this auxiliary regression:\n$$\n\\mathrm{LM} = n R^2\n$$\nUnder the null hypothesis of homoscedasticity, this statistic follows a chi-squared distribution with $k$ degrees of freedom, where $k$ is the number of independent variables in the auxiliary regression (excluding the constant). In this problem, $k=1$.\n\nWe then compute the $p$-value:\n$$\np\\text{-value} = \\Pr\\left(\\chi^2_1 \\ge \\mathrm{LM}\\right)\n$$\nA small $p$-value (specifically, $p < 0.05$ as per the problem) provides evidence against the null hypothesis, leading us to classify the dataset as heteroscedastic.\n\n#### 3. Task B: Parametric Variance Function Regression\n\nIf heteroscedasticity is suspected (or even if not, for the purpose of this exercise), we proceed to model the variance function $\\sigma^2(x_i)$. The problem specifies a flexible and robust log-linear model to ensure the estimated variance is always positive. We regress the logarithm of the squared residuals on a function of $x_i$:\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i\n$$\nHere, $\\varepsilon = 10^{-8}$ is a small constant to prevent taking the logarithm of zero in cases where a residual is exactly zero. After estimating the parameters $\\widehat{\\gamma}_0$ and $\\widehat{\\gamma}_1$ via OLS, we can form an estimate of the variance for each observation:\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right)\n$$\n\n#### 4. Task C: Weighted Least Squares (WLS) Refit\n\nWith the estimated variances $\\widehat{\\sigma}_i^2$ in hand, we can improve upon the initial OLS fit by using Weighted Least Squares (WLS). WLS accounts for heteroscedasticity by assigning a weight to each observation that is inversely proportional to its error variance. The weights are defined as $w_i = 1/\\widehat{\\sigma}_i^2$. Observations with smaller variance (and thus more information) receive higher weight.\n\nThe WLS estimator is the vector $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}}$ that minimizes the weighted sum of squared residuals:\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n$$\nIn matrix form, with $\\mathbf{W}$ being a diagonal matrix of the weights $w_i$, the WLS solution is:\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nThis estimator is efficient under the assumed form of heteroscedasticity.\n\nFinally, we compute the Mean Squared Error (MSE) for both the OLS and WLS fits to compare their performance on the original scale:\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 \\quad \\text{and} \\quad \\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2\n$$\n\nThis completes the entire pipeline. The implementation will follow these steps precisely for each test case, gathering the required\neight output values: the boolean classification, the $p$-value, the two OLS coefficients, the two WLS coefficients, and the two MSE values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing OLS, Breusch-Pagan test, variance modeling,\n    and WLS for three synthetic datasets.\n    \"\"\"\n    \n    # Define test cases as per the problem description.\n    test_cases = [\n        # Case 1: Homoscedastic\n        {'n': 200, 'beta0': 1.25, 'beta1': -0.75, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.5},\n        # Case 2: Clearly heteroscedastic\n        {'n': 200, 'beta0': 0.5, 'beta1': 1.5, 'x_range': [-3, 3], 'sigma_func': lambda x: 0.3 + 0.7 * np.abs(x)},\n        # Case 3: Edge case, curved variance\n        {'n': 60, 'beta0': 0.0, 'beta1': 2.0, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.2 + 0.4 * (x + 1)**2}\n    ]\n\n    # Global constants and random number generator\n    RANDOM_SEED = 1337\n    EPSILON_LOG = 1e-8\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Data Generation ---\n        n = case['n']\n        beta0_true, beta1_true = case['beta0'], case['beta1']\n        x = rng.uniform(case['x_range'][0], case['x_range'][1], size=n)\n        sigma = case['sigma_func'](x)\n        epsilon = rng.normal(0, sigma, size=n)\n        y = beta0_true + beta1_true * x + epsilon\n\n        # --- Task A: OLS and Breusch-Pagan Test ---\n        \n        # 1. Initial OLS fit\n        X_ols = np.c_[np.ones(n), x]\n        try:\n            beta_ols = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y\n        except np.linalg.LinAlgError:\n            # Fallback to lstsq if matrix is singular, though unlikely for this problem.\n            beta_ols, _, _, _ = np.linalg.lstsq(X_ols, y, rcond=None)\n\n        res_ols = y - X_ols @ beta_ols\n        squared_res = res_ols**2\n        \n        # 2. Auxiliary regression for Breusch-Pagan test: r_i^2 on x_i\n        X_aux = np.c_[np.ones(n), x]\n        try:\n            delta = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ squared_res\n        except np.linalg.LinAlgError:\n            delta, _, _, _ = np.linalg.lstsq(X_aux, squared_res, rcond=None)\n            \n        y_pred_aux = X_aux @ delta\n        ss_res_aux = np.sum((squared_res - y_pred_aux)**2)\n        ss_tot_aux = np.sum((squared_res - np.mean(squared_res))**2)\n        \n        # Guard against ss_tot_aux being zero\n        r_squared_aux = 1 - (ss_res_aux / ss_tot_aux) if ss_tot_aux > 0 else 0.0\n\n        # 3. LM statistic and p-value\n        lm_stat = n * r_squared_aux\n        p_value = chi2.sf(lm_stat, df=1)\n        is_heteroscedastic = p_value < 0.05\n        \n        # --- Task B: Variance Function Regression ---\n        log_squared_res = np.log(squared_res + EPSILON_LOG)\n        log_x_feature = np.log(1 + x**2)\n        \n        X_var = np.c_[np.ones(n), log_x_feature]\n        try:\n            gamma = np.linalg.inv(X_var.T @ X_var) @ X_var.T @ log_squared_res\n        except np.linalg.LinAlgError:\n            gamma, _, _, _ = np.linalg.lstsq(X_var, log_squared_res, rcond=None)\n\n        sigma2_hat = np.exp(X_var @ gamma)\n        \n        # --- Task C: Weighted Least Squares Refit ---\n        weights = 1.0 / sigma2_hat\n        W = np.diag(weights)\n        \n        try:\n            # Direct WLS formula\n            X_wls = X_ols\n            beta_wls = np.linalg.inv(X_wls.T @ W @ X_wls) @ X_wls.T @ W @ y\n        except np.linalg.LinAlgError:\n            # Fallback using transformed OLS, which is more stable\n            sqrt_w = np.sqrt(weights)\n            y_prime = y * sqrt_w\n            X_prime = X_wls * sqrt_w[:, np.newaxis]\n            beta_wls, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n\n        # --- MSE Calculation ---\n        mse_ols = np.mean(res_ols**2)\n        res_wls = y - X_wls @ beta_wls\n        mse_wls = np.mean(res_wls**2)\n        \n        # --- Collect and Format Results ---\n        result_list = [\n            is_heteroscedastic,\n            p_value,\n            beta_ols[0],\n            beta_ols[1],\n            beta_wls[0],\n            beta_wls[1],\n            mse_ols,\n            mse_wls,\n        ]\n        all_results.append(result_list)\n\n    # Final print statement in the exact required format.\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        formatted_res = (\n            f\"[{res[0]},\"\n            f\"{res[1]:.6f},\"\n            f\"{res[2]:.6f},\"\n            f\"{res[3]:.6f},\"\n            f\"{res[4]:.6f},\"\n            f\"{res[5]:.6f},\"\n            f\"{res[6]:.6f},\"\n            f\"{res[7]:.6f}]\"\n        )\n        output_str += formatted_res\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3107027"}, {"introduction": "While machine learning models can be powerful, their predictive capabilities are often limited to the domain of the data on which they were trained. Using a model to make predictions far outside this domain, a process known as extrapolation, is fraught with risk. This practice provides a concrete framework for classifying whether an extrapolation is \"safe\" by checking for the preservation of logical properties like monotonicity and adherence to physical bounds, teaching a critical lesson in understanding a model's operational limits [@problem_id:3107029].", "problem": "You are given a learned univariate regression model represented as a polynomial function $\\hat{f}(x) = \\sum_{k=0}^{K} c_k x^k$ and a training domain $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$. You must write a complete, runnable program that, for several specified test cases, performs two tasks: (1) classify whether extrapolating $\\hat{f}(x)$ to a specified interval outside the training domain is safe according to logical monotonicity and physical bounds criteria; and (2) estimate the rate at which symmetric Gaussian confidence interval widths widen as a function of distance beyond the training domain by linear regression. The program must output one result per test case and aggregate all results into a single line in the exact format specified below.\n\nFoundational definitions to be used as the starting point:\n- A function is monotone nondecreasing on an interval if for every pair $x_1 \\le x_2$ in that interval we have $\\hat{f}(x_1) \\le \\hat{f}(x_2)$. It is monotone nonincreasing if $\\hat{f}(x_1) \\ge \\hat{f}(x_2)$ for all such pairs. A constant function is both monotone nondecreasing and monotone nonincreasing.\n- A physical bound for the response is given by a closed interval $[y_{\\min}, y_{\\max}]$ that the model output must not exceed.\n- For a two-sided Gaussian confidence interval with nominal coverage $q \\in (0,1)$ under Gaussian noise with standard deviation $\\sigma > 0$, the width $w$ of a symmetric interval is related to the quantile of the Standard Normal Distribution (SND) via the well-tested fact that a symmetric two-sided interval at level $q$ uses the SND quantile $z_q$ defined by $z_q = \\Phi^{-1}\\!\\left(1 - \\frac{1-q}{2}\\right)$, where $\\Phi$ is the SND cumulative distribution function. The corresponding width contribution from a known standard deviation $\\sigma$ is $2 z_q \\sigma$.\n- For linear regression of a response $W$ on a scalar predictor $D$ with an intercept, the ordinary least squares slope equals $\\mathrm{Cov}(D,W)/\\mathrm{Var}(D)$ under the usual assumptions when $\\mathrm{Var}(D) > 0$.\n\nYour program must implement the following logic for each test case:\n\n1) Safety classification rule based on monotonicity and bounds:\n- Determine the monotonicity type of $\\hat{f}$ on the training interval $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ by checking whether it is monotone nondecreasing, monotone nonincreasing, or (within a numerical tolerance) constant. Use a uniform grid of $N$ points for this check with $N = 1000$ and a tolerance $\\varepsilon = 10^{-8}$. You may assess monotonicity by inspecting the sign of the discrete derivative across the grid or by any equivalent numerical criterion that adheres to the definitions above. Declare the training monotonicity type as one of: increasing, decreasing, or flat. If no monotonicity type applies within tolerance, then classification must be unsafe.\n- Check that $\\hat{f}$ has the same monotonicity type on the extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ as on $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$, using the same $N$ and $\\varepsilon$. If the training type is flat, then the extrapolation must be flat within tolerance as well.\n- Confirm that $\\hat{f}(x) \\in [y_{\\min}, y_{\\max}]$ at every point of a uniform grid of $N$ points over $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$, inclusive of the endpoints, within a tolerance of $\\varepsilon$.\n- The extrapolation is classified as safe if and only if both the monotonicity condition and the bounds condition are satisfied.\n\n2) Regression of confidence interval widening rate:\n- For a given coverage level $q$ and noise standard deviation $\\sigma$, define the synthetic symmetric confidence interval width at a point $x$ beyond the training domain by\n$$\nw(x) = 2 z_q \\sigma \\left(1 + \\lambda \\, d(x)\\right),\n$$\nwhere $d(x)$ is the nonnegative distance from $x$ to the nearest point of the training domain $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$, that is,\n$$\nd(x) = \\begin{cases}\nx_{\\mathrm{L}} - x, & \\text{if } x < x_{\\mathrm{L}},\\\\\n0, & \\text{if } x \\in [x_{\\mathrm{L}}, x_{\\mathrm{R}}],\\\\\nx - x_{\\mathrm{R}}, & \\text{if } x > x_{\\mathrm{R}}.\n\\end{cases}\n$$\nThe parameter $\\lambda \\ge 0$ controls how quickly the width increases per unit distance. For each test case, construct $M = 5$ evenly spaced points in the extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$, compute $d(x)$ and $w(x)$ at those points, and fit the ordinary least squares linear model $w = \\alpha + r \\, d$. Report the estimated slope $r$ as the widening rate. Round the reported $r$ to six decimal places.\n\nTest suite:\nProvide results for the following four test cases. In each case, the polynomial coefficients are listed from lowest degree to highest degree, so $c_0$ is the constant term.\n\n- Case $1$:\n  - $c = \\{1, 2\\}$, so $\\hat{f}(x) = 1 + 2 x$.\n  - Training domain $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-1, 1]$.\n  - Extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [1, 2]$.\n  - Physical bounds $[y_{\\min}, y_{\\max}] = [0, 5]$.\n  - Noise standard deviation $\\sigma = 0.5$.\n  - Coverage level $q = 0.95$.\n  - Widening parameter $\\lambda = 0.1$.\n\n- Case $2$:\n  - $c = \\{1, 4\\}$, so $\\hat{f}(x) = 1 + 4 x$.\n  - Training domain $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-1, 1]$.\n  - Extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [1, 3]$.\n  - Physical bounds $[y_{\\min}, y_{\\max}] = [0, 10]$.\n  - Noise standard deviation $\\sigma = 0.3$.\n  - Coverage level $q = 0.90$.\n  - Widening parameter $\\lambda = 0.2$.\n\n- Case $3$:\n  - $c = \\{0, -1, 0, 1\\}$, so $\\hat{f}(x) = x^3 - x$.\n  - Training domain $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-0.5, 0.5]$.\n  - Extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [0.5, 1.0]$.\n  - Physical bounds $[y_{\\min}, y_{\\max}] = [-10, 10]$.\n  - Noise standard deviation $\\sigma = 0.4$.\n  - Coverage level $q = 0.95$.\n  - Widening parameter $\\lambda = 0.15$.\n\n- Case $4$:\n  - $c = \\{3\\}$, so $\\hat{f}(x) = 3$.\n  - Training domain $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-2, 2]$.\n  - Extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [2, 3]$.\n  - Physical bounds $[y_{\\min}, y_{\\max}] = [0, 5]$.\n  - Noise standard deviation $\\sigma = 0.2$.\n  - Coverage level $q = 0.99$.\n  - Widening parameter $\\lambda = 0.0$.\n\nFinal output format:\nYour program should produce a single line of output containing all test case results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list in the form $[s, r]$ where $s$ is a boolean (True if safe, False if unsafe) and $r$ is the slope rounded to six decimals. For example, an output with two hypothetical results would look like \"[[True,0.123456],[False,0.000000]]\". Your program must print exactly one such line and nothing else.", "solution": "The user has provided a computational problem requiring the implementation of a two-part analysis for a given polynomial regression model, $\\hat{f}(x)$. The first task is to classify the safety of extrapolating the model beyond its training domain based on logical criteria. The second is to estimate the rate at which a synthetic confidence interval widens as a function of distance from the training domain. The problem is well-posed, scientifically grounded in standard numerical and statistical principles, and provides all necessary data and definitions for a unique, verifiable solution.\n\nThe solution proceeds by implementing the specified logic for each test case.\n\n**Part 1: Safety Classification**\n\nThe safety of an extrapolation is contingent on two conditions: preservation of monotonicity and adherence to physical bounds.\n\n1.  **Monotonicity Analysis**: A function is defined as monotone nondecreasing on an interval if for any $x_1 \\le x_2$ in that interval, $\\hat{f}(x_1) \\le \\hat{f}(x_2)$. It is monotone nonincreasing if $\\hat{f}(x_1) \\ge \\hat{f}(x_2)$. A constant function satisfies both conditions. To assess this numerically, we evaluate the function $\\hat{f}(x)$ on a fine, uniform grid of $N=1000$ points within a given interval. Let these evaluated points be $y_i = \\hat{f}(x_i)$. We then examine the sequence of first differences, $\\Delta y_i = y_{i+1} - y_i$.\n    -   The function is considered numerically nondecreasing if all $\\Delta y_i \\ge -\\varepsilon$, for a small tolerance $\\varepsilon=10^{-8}$.\n    -   It is numerically nonincreasing if all $\\Delta y_i \\le \\varepsilon$.\n    -   If both conditions hold, the function is classified as `flat`. If only the first holds, it is `increasing`. If only the second holds, it is `decreasing`. If neither holds, it is non-monotone, or `none`.\n\n    The safety rule requires that the monotonicity type observed on the training interval $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ must be preserved on the extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$. If the function is not monotone on the training interval, the extrapolation is immediately deemed unsafe.\n\n2.  **Physical Bounds Check**: The model's output $\\hat{f}(x)$ must remain within the specified physical bounds $[y_{\\min}, y_{\\max}]$ for all $x$ in the extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$. Numerically, this is verified by checking that for all points $y_i$ on the evaluation grid over $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$, the condition $y_{\\min} - \\varepsilon \\le y_i \\le y_{\\max} + \\varepsilon$ holds.\n\nAn extrapolation is classified as `safe` (True) if and only if both the monotonicity is preserved and the physical bounds are respected. Otherwise, it is `unsafe` (False).\n\n**Part 2: Confidence Interval Widening Rate**\n\nThis part involves a regression analysis on a synthetically generated dataset.\n\n1.  **Synthetic Data Generation**: The problem defines a model for the width of a symmetric confidence interval at a point $x$ as a function of its distance from the training domain:\n    $$\n    w(x) = 2 z_q \\sigma \\left(1 + \\lambda \\, d(x)\\right)\n    $$\n    Here, $\\sigma$ is the noise standard deviation, $\\lambda$ is a widening parameter, and $d(x)$ is the distance from $x$ to the interval $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$. The term $z_q$ is the quantile of the Standard Normal Distribution (SND) corresponding to a two-sided confidence level $q$, given by $z_q = \\Phi^{-1}((1+q)/2)$, where $\\Phi$ is the SND's cumulative distribution function. For the analysis, we generate $M=5$ evenly spaced points $\\{x_i\\}$ in the extrapolation interval $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ and compute the corresponding pairs $(d_i, w_i)$, where $d_i = d(x_i)$ and $w_i = w(x_i)$.\n\n2.  **Linear Regression**: We fit a simple linear regression model of the form $w = \\alpha + r d$ to the generated $(d_i, w_i)$ pairs using ordinary least squares (OLS). The slope of this regression, $r$, represents the estimated rate at which the confidence interval width widens per unit of distance from the training domain. The OLS estimate for the slope $r$ is given by the formula:\n    $$\n    r = \\frac{\\mathrm{Cov}(d, w)}{\\mathrm{Var}(d)} = \\frac{\\sum_{i=1}^{M} (d_i - \\bar{d})(w_i - \\bar{w})}{\\sum_{i=1}^{M} (d_i - \\bar{d})^2}\n    $$\n    where $\\bar{d}$ and $\\bar{w}$ are the sample means of the $d_i$ and $w_i$ values, respectively. This value is computed for each test case. Note that since the synthetic data $w_i$ is an exact linear function of $d_i$, $w_i = (2z_q \\sigma) + (2z_q \\sigma \\lambda) d_i$, the OLS regression will exactly recover the theoretical slope, $r = 2z_q \\sigma \\lambda$, provided $\\mathrm{Var}(d) > 0$.\n\nThe final program iterates through the specified test cases, applies the logic described above, and aggregates the results into the required output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It encapsulates all logic and helper functions for self-contained execution.\n    \"\"\"\n\n    def check_monotonicity(c_coeffs, interval, N, eps):\n        \"\"\"\n        Numerically determines the monotonicity of a polynomial on an interval.\n        Returns 'increasing', 'decreasing', 'flat', or 'none'.\n        \"\"\"\n        # np.polyval expects coefficients for highest power first.\n        c_rev = c_coeffs[::-1]\n        \n        x_grid = np.linspace(interval[0], interval[1], N)\n        y_vals = np.polyval(c_rev, x_grid)\n        \n        diffs = np.diff(y_vals)\n        \n        is_non_decreasing = np.all(diffs >= -eps)\n        is_non_increasing = np.all(diffs <= eps)\n        \n        if is_non_decreasing and is_non_increasing:\n            return 'flat'\n        elif is_non_decreasing:\n            return 'increasing'\n        elif is_non_increasing:\n            return 'decreasing'\n        else:\n            return 'none'\n\n    def check_bounds(c_coeffs, interval, N, y_min, y_max, eps):\n        \"\"\"\n        Checks if the polynomial's output stays within physical bounds on an interval.\n        \"\"\"\n        c_rev = c_coeffs[::-1]\n        \n        x_grid = np.linspace(interval[0], interval[1], N)\n        y_vals = np.polyval(c_rev, x_grid)\n        \n        # Check if all y_vals are within [y_min, y_max] with tolerance\n        return np.all((y_vals >= y_min - eps) & (y_vals <= y_max + eps))\n\n    def calculate_distance(x_points, x_L, x_R):\n        \"\"\"\n        Vectorized calculation of the distance from points 'x' to the interval [x_L, x_R].\n        \"\"\"\n        return np.maximum(0, x_points - x_R) + np.maximum(0, x_L - x_points)\n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: c, [x_L, x_R], [x_A, x_B], [y_min, y_max], sigma, q, lambda\n        (np.array([1, 2]), [-1, 1], [1, 2], [0, 5], 0.5, 0.95, 0.1),\n        # Case 2\n        (np.array([1, 4]), [-1, 1], [1, 3], [0, 10], 0.3, 0.90, 0.2),\n        # Case 3\n        (np.array([0, -1, 0, 1]), [-0.5, 0.5], [0.5, 1.0], [-10, 10], 0.4, 0.95, 0.15),\n        # Case 4\n        (np.array([3]), [-2, 2], [2, 3], [0, 5], 0.2, 0.99, 0.0),\n    ]\n\n    results = []\n    \n    # Constants for numerical checks\n    N = 1000\n    M = 5\n    eps = 1e-8\n\n    for case in test_cases:\n        c, train_domain, extrap_interval, phys_bounds, sigma, q, lam = case\n        x_L, x_R = train_domain\n        x_A, x_B = extrap_interval\n        y_min, y_max = phys_bounds\n\n        # --- Part 1: Safety Classification ---\n        train_mono_type = check_monotonicity(c, train_domain, N, eps)\n        \n        is_safe = False\n        if train_mono_type != 'none':\n            extrap_mono_type = check_monotonicity(c, extrap_interval, N, eps)\n            mono_ok = (train_mono_type == extrap_mono_type)\n            \n            if mono_ok:\n                bounds_ok = check_bounds(c, extrap_interval, N, y_min, y_max, eps)\n                if bounds_ok:\n                    is_safe = True\n\n        # --- Part 2: Widening Rate Regression ---\n        z_q = norm.ppf(1 - (1 - q) / 2.0)\n        \n        x_points_regr = np.linspace(x_A, x_B, M)\n        d_vals = calculate_distance(x_points_regr, x_L, x_R)\n        w_vals = 2 * z_q * sigma * (1 + lam * d_vals)\n        \n        var_d = np.var(d_vals, ddof=1)\n        \n        r = 0.0\n        # OLS slope r = Cov(d, w) / Var(d). Check Var(d) > 0.\n        if var_d > 1e-12:\n            cov_dw = np.cov(d_vals, w_vals, ddof=1)[0, 1]\n            r = cov_dw / var_d\n        \n        results.append([is_safe, r])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{s},{r:.6f}]\" for s, r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3107029"}]}