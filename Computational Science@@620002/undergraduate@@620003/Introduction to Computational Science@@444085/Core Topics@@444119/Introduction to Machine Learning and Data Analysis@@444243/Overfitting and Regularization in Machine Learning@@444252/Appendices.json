{"hands_on_practices": [{"introduction": "How can we precisely measure the complexity of a highly flexible, non-parametric model? This exercise introduces the concept of \"effective degrees of freedom,\" $d_{\\mathrm{eff}}$, which provides a concrete answer by analyzing the eigenvalues of the kernel matrix. By implementing Kernel Ridge Regression, you will see firsthand how the regularization parameter $\\lambda$ and the kernel's length-scale $\\ell$ interact to control this quantifiable measure of model complexity, offering a powerful tool for diagnosing and preventing overfitting [@problem_id:3168571].", "problem": "You are given a one-dimensional training design with inputs $\\mathbf{x} \\in \\mathbb{R}^{n}$ defined by $n=20$ equispaced points on the closed interval $\\left[0,1\\right]$, specifically $x_i = \\dfrac{i-1}{n-1}$ for $i \\in \\{1,2,\\dots,n\\}$. Consider Kernel Ridge Regression (KRR) with a Radial Basis Function (RBF) kernel. The RBF kernel with length-scale $\\ell>0$ is defined by $k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$, and the associated Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ has entries $K_{ij} = k_{\\ell}(x_i,x_j)$. KRR estimates the response by minimizing a Tikhonov-regularized squared loss over the Reproducing Kernel Hilbert Space, with regularization parameter $\\lambda \\ge 0$.\n\nStarting from fundamental definitions, derive the linear smoother that maps training outputs $\\mathbf{y} \\in \\mathbb{R}^n$ to fitted values $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ for KRR and use this to define the effective degrees of freedom $d_{\\mathrm{eff}}$ as the trace of the smoother. Your program must implement a numerically stable computation of $d_{\\mathrm{eff}}$ for given $\\lambda$ and $\\ell$ for the fixed design $\\mathbf{x}$ described above, without performing an explicit matrix inversion.\n\nDemonstrate overfitting control by reporting $d_{\\mathrm{eff}}$ across the following test suite of $(\\lambda,\\ell)$ pairs:\n$(10^{-6}, \\, 0.05)$,\n$(10^{-6}, \\, 5.0)$,\n$(10^{-1}, \\, 0.2)$,\n$(10^{-1}, \\, 1.0)$,\n$(10^{1}, \\, 0.05)$,\n$(10^{1}, \\, 5.0)$.\n\nYour program should compute $d_{\\mathrm{eff}}$ for each test case using the fixed $\\mathbf{x}$ and the RBF kernel specified, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$. Each $d_{\\mathrm{eff}}$ must be printed as a decimal rounded to $6$ places. No physical units are involved, and angles are not used.", "solution": "The Kernel Ridge Regression (KRR) problem seeks to find a function $f$ within a Reproducing Kernel Hilbert Space (RKHS), denoted $\\mathcal{H}_k$, that minimizes the regularized squared loss. For a given training set $\\{ (x_i, y_i) \\}_{i=1}^n$, this objective is formulated as:\n$$\n\\min_{f \\in \\mathcal{H}_k} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}_k}^2\n$$\nHere, $\\lambda \\ge 0$ is the regularization parameter. The Representer Theorem states that the optimal solution $f(x)$ can be written as a linear combination of kernel functions evaluated at the training points, $f(x) = \\sum_{j=1}^n \\alpha_j k(x, x_j)$.\n\nBy substituting this form into the objective and expressing it in matrix notation, the problem becomes minimizing with respect to the coefficients $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_n)^T$:\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\|\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}\\|_2^2 + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}\n$$\nwhere $\\mathbf{K}$ is the Gram matrix with entries $K_{ij} = k(x_i, x_j)$. The solution for the coefficients is found by setting the gradient to zero, which yields:\n$$\n\\boldsymbol{\\alpha} = (\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\nThe fitted values at the training points are $\\hat{\\mathbf{y}} = \\mathbf{K}\\boldsymbol{\\alpha}$. This leads to the definition of the linear smoother matrix $\\mathbf{S}$:\n$$\n\\hat{\\mathbf{y}} = \\underbrace{\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}}_{\\mathbf{S}} \\mathbf{y}\n$$\nThe effective degrees of freedom, $d_{\\mathrm{eff}}$, is defined as the trace of this smoother matrix:\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}(\\mathbf{S}) = \\mathrm{Tr}\\left(\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\right)\n$$\nFor a numerically stable computation that avoids explicit matrix inversion, we use the spectral decomposition of the symmetric Gram matrix $\\mathbf{K} = \\mathbf{U\\Sigma U}^T$, where $\\mathbf{\\Sigma}$ is the diagonal matrix of eigenvalues $\\sigma_i$. Using the cyclic property of the trace, the expression simplifies to:\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right) = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i + \\lambda}\n$$\nThis final formula is implemented in the provided code. It calculates $d_{\\mathrm{eff}}$ by first computing the eigenvalues of the Gram matrix for each $(\\lambda, \\ell)$ pair and then summing the resulting terms.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective degrees of freedom for Kernel Ridge Regression\n    with an RBF kernel for a set of (lambda, ell) parameters.\n    \"\"\"\n    \n    # Define fixed problem parameters\n    n = 20\n    x = np.linspace(0, 1, n)\n\n    # Define the test suite of (lambda, ell) pairs\n    test_cases = [\n        (1e-6, 0.05),\n        (1e-6, 5.0),\n        (1e-1, 0.2),\n        (1e-1, 1.0),\n        (1e+1, 0.05),\n        (1e+1, 5.0)\n    ]\n\n    results = []\n    \n    # Use broadcasting to create a matrix of squared distances\n    # x_col has shape (n, 1), x has shape (n,). Broadcasting creates an (n, n) matrix.\n    x_col = x.reshape(-1, 1)\n    sq_dist_matrix = (x_col - x)**2\n\n    for lmbda, ell in test_cases:\n        # Construct the RBF Gram matrix K\n        # K_ij = exp(-||x_i - x_j||^2 / (2 * ell^2))\n        K = np.exp(-sq_dist_matrix / (2 * ell**2))\n\n        # Compute the eigenvalues of the symmetric matrix K.\n        # np.linalg.eigvalsh is numerically stable and optimized for Hermitian matrices.\n        eigenvalues = np.linalg.eigvalsh(K)\n\n        # Compute the effective degrees of freedom using the derived formula.\n        # d_eff = sum(sigma_i / (sigma_i + lambda))\n        d_eff = np.sum(eigenvalues / (eigenvalues + lmbda))\n        \n        # Round the result to 6 decimal places and store as a string\n        results.append(f\"{d_eff:.6f}\")\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3168571"}, {"introduction": "Regularization is typically taught as the minimization of a single, combined objective function. This practice challenges you to reframe the concept as a multi-objective optimization problem, where you simultaneously seek to minimize training error and the regularization penalty. By implementing Ridge Regression and plotting the resulting Pareto front, you will develop a powerful visual intuition for the trade-off between model simplicity and data fidelity, and understand why some hyperparameter choices are fundamentally superior to others [@problem_id:3168619].", "problem": "You will study the biasâ€“variance trade-off through a two-objective view of linear models with quadratic regularization. Work within the following mathematical framework grounded in the core definition of Empirical Risk Minimization: a linear predictor with parameters $w \\in \\mathbb{R}^p$ is trained by minimizing the sum of the empirical squared error and a quadratic regularization penalty. For each regularization strength, consider the two objectives separately: training mean squared error and the value of the regularization penalty term itself. The goal is to construct a discrete approximation of the trade-off curve across a set of regularization strengths and identify Pareto-optimal models.\n\nDefinitions you must use:\n- The training data matrix is $X \\in \\mathbb{R}^{n \\times p}$ and the response vector is $y \\in \\mathbb{R}^n$.\n- A linear model predicts $\\hat{y} = X w$ for $w \\in \\mathbb{R}^p$.\n- The training Mean Squared Error (MSE) is $E_{\\text{train}}(w) = \\frac{1}{n}\\lVert X w - y \\rVert_2^2$.\n- The quadratic (ridge) regularization penalty term value for a given regularization strength $\\lambda > 0$ is $P_\\lambda(w) = \\lambda \\lVert w \\rVert_2^2$.\n- For a fixed dataset and a fixed list of $\\lambda$ values, training a separate model for each $\\lambda$ yields a set of pairs $\\left(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda)\\right)$. Consider these as points in the objective space.\n- A model $i$ with objective pair $(E_i, P_i)$ is Pareto dominated by a model $j$ with objective pair $(E_j, P_j)$ if and only if $E_j \\le E_i$ and $P_j \\le P_i$ with at least one of the two inequalities being strict. A model is Pareto optimal if it is not dominated by any other model in the set.\n\nYour program must, for each test case defined below, do the following:\n1. Generate a synthetic dataset using the stated generative model.\n2. For each given $\\lambda$ in the test case, train the regularized linear model and compute the pair $\\left(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda)\\right)$.\n3. Determine the set of indices (zero-based, aligned with the order of $\\lambda$ values listed) that are Pareto optimal.\n4. Count how many models are dominated.\n\nData generation protocol (deterministic and scientifically sound):\n- For a given seed $s$, dimension $p$, and sample size $n$, draw the design matrix $X \\in \\mathbb{R}^{n \\times p}$ with entries independently from the Normal distribution with mean $0$ and variance $1$, denoted $\\mathcal{N}(0,1)$. The draws must be Independent and Identically Distributed (i.i.d.).\n- Draw the ground-truth parameter vector $w^\\star \\in \\mathbb{R}^p$ with entries i.i.d. from $\\mathcal{N}(0,1)$ using the same seed $s$ but advanced deterministically so that $X$ and $w^\\star$ are not identical draws.\n- Draw noise $\\epsilon \\in \\mathbb{R}^n$ with entries i.i.d. from $\\mathcal{N}(0,\\sigma^2)$, for the specified $\\sigma$.\n- Form the response as $y = X w^\\star + \\epsilon$.\n\nImportant implementation notes:\n- All regularization strengths $\\lambda$ will be strictly positive. Do not include $\\lambda = 0$.\n- Your algorithm for identifying Pareto-optimal points must adhere to the definition above. In floating-point arithmetic, use a reasonable small tolerance when comparing real numbers to mitigate spurious dominance caused by numerical roundoff, but preserve strict improvement in at least one coordinate.\n\nTest suite to cover typical and edge scenarios:\n- Test case A (well-posed, low noise):\n  - Seed $s = 7$\n  - Samples $n = 40$\n  - Features $p = 6$\n  - Noise standard deviation $\\sigma = 10^{-1}$\n  - Regularization strengths (in order): $\\lambda \\in \\{10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10\\}$\n- Test case B (underdetermined, very low noise):\n  - Seed $s = 101$\n  - Samples $n = 20$\n  - Features $p = 50$\n  - Noise standard deviation $\\sigma = 5 \\times 10^{-2}$\n  - Regularization strengths (in order): $\\lambda \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\}$\n- Test case C (well-posed, high noise):\n  - Seed $s = 2023$\n  - Samples $n = 50$\n  - Features $p = 10$\n  - Noise standard deviation $\\sigma = 1$\n  - Regularization strengths (in order): $\\lambda \\in \\{10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10^2\\}$\n\nRequired final output format:\n- Your program should produce a single line of output containing a list with one element per test case, in the same order A, B, C.\n- For each test case, output a list with two entries:\n  1. A list of the zero-based indices (integers) of the Pareto-optimal models, sorted in ascending order.\n  2. An integer equal to the number of dominated models.\n- Concretely, the output must be a single line in the form\n  - $[ [\\text{pareto\\_A}, \\text{dominated\\_count\\_A}], [\\text{pareto\\_B}, \\text{dominated\\_count\\_B}], [\\text{pareto\\_C}, \\text{dominated\\_count\\_C}] ]$\n  where each $\\text{pareto\\_X}$ is a list of integers as specified.\n- The output must contain no units and no additional text, and the elements must be comma-separated and enclosed in square brackets exactly as shown.", "solution": "This problem analyzes Ridge regression through the lens of bi-objective optimization, considering the trade-off between minimizing training error and minimizing model complexity (represented by the regularization penalty).\n\n**1. Ridge Regression Solution**\nThe standard Ridge regression objective function minimizes a combination of squared error and an $\\ell_2$-norm penalty on the model parameters $w$:\n$$\nL(w) = \\lVert Xw - y \\rVert_2^2 + \\lambda \\lVert w \\rVert_2^2\n$$\nTo find the optimal parameter vector $w_\\lambda$, we take the gradient of $L(w)$ with respect to $w$ and set it to zero:\n$$\n\\nabla_w L(w) = 2 X^T (Xw - y) + 2 \\lambda w = 0\n$$\nRearranging the terms gives the normal equations for Ridge regression:\n$$\n(X^T X + \\lambda I) w = X^T y\n$$\nSince $\\lambda > 0$, the matrix $(X^T X + \\lambda I)$ is always invertible. The unique solution for the parameter vector is therefore:\n$$\nw_\\lambda = (X^T X + \\lambda I)^{-1} X^T y\n$$\n\n**2. Bi-Objective Evaluation**\nFor each model trained with a specific $\\lambda$, we compute two separate objective values:\n- **Training Mean Squared Error ($E_{\\text{train}}$):** Measures the goodness of fit.\n$$\nE_{\\text{train}}(w_\\lambda) = \\frac{1}{n} \\lVert Xw_\\lambda - y \\rVert_2^2\n$$\n- **Regularization Penalty Value ($P_\\lambda$):** The value of the penalty term itself.\n$$\nP_\\lambda(w_\\lambda) = \\lambda \\lVert w_\\lambda \\rVert_2^2\n$$\nThis process yields a set of objective pairs $\\{(E_k, P_k)\\}$, one for each $\\lambda_k$.\n\n**3. Pareto Optimality**\nThe core of the analysis is to identify the Pareto-optimal models from this set. A model $j$ is said to *dominate* model $i$ if it is better or equal on both objectives and strictly better on at least one. Since we want to minimize both objectives, this means:\n$$\nE_j \\le E_i \\quad \\text{and} \\quad P_j \\le P_i\n$$\nwith at least one inequality being strict. A model is *Pareto optimal* if it is not dominated by any other model in the set. The algorithm implemented in the solution iterates through all pairs of models, marks any model that is dominated, and finally reports the indices of the non-dominated (i.e., Pareto-optimal) models and the total count of dominated ones.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(s, n, p, sigma, lambdas):\n    \"\"\"\n    Generates data, trains models, and performs Pareto analysis for one test case.\n\n    Args:\n        s (int): Seed for the random number generator.\n        n (int): Number of samples.\n        p (int): Number of features.\n        sigma (float): Standard deviation of the noise.\n        lambdas (list[float]): List of regularization strengths.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of zero-based indices of Pareto-optimal models.\n              2. The integer count of dominated models.\n    \"\"\"\n    # 1. Data Generation using a deterministic protocol\n    rng = np.random.default_rng(seed=s)\n    X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n    # The RNG state advances, so w_star is drawn from a different state than X\n    w_star = rng.normal(loc=0.0, scale=1.0, size=p)\n    epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = X @ w_star + epsilon\n\n    # 2. Model Training and Objective Calculation for each lambda\n    objectives = []\n    XTX = X.T @ X\n    XTy = X.T @ y\n    I = np.identity(p)\n    \n    for lmbda in lambdas:\n        # Solve (X'X + lambda*I)w = X'y for w\n        w_lambda = np.linalg.solve(XTX + lmbda * I, XTy)\n\n        # Calculate the two objectives: training MSE and penalty value\n        pred_error = X @ w_lambda - y\n        E_train = (1.0 / n) * np.sum(pred_error**2)\n        P_lambda_val = lmbda * np.sum(w_lambda**2)\n        \n        objectives.append((E_train, P_lambda_val))\n\n    # 3. Pareto Optimality Analysis\n    num_models = len(lambdas)\n    is_dominated = [False] * num_models\n    tol = 1e-12  # Tolerance for floating-point comparisons\n\n    for i in range(num_models):\n        for j in range(num_models):\n            if i == j:\n                continue\n            \n            # Check if model j dominates model i\n            E_i, P_i = objectives[i]\n            E_j, P_j = objectives[j]\n            \n            # Dominance condition: E_j <= E_i and P_j <= P_i, with one being strict\n            is_le_in_both = (E_j <= E_i + tol) and (P_j <= P_i + tol)\n            is_lt_in_one = (E_j < E_i - tol) or (P_j < P_i - tol)\n            \n            if is_le_in_both and is_lt_in_one:\n                is_dominated[i] = True\n                break  # Model i is dominated, no need to check other j's\n\n    pareto_indices = [i for i, dominated in enumerate(is_dominated) if not dominated]\n    dominated_count = sum(is_dominated)\n\n    return [pareto_indices, dominated_count]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case A (well-posed, low noise)\n        {'s': 7, 'n': 40, 'p': 6, 'sigma': 1e-1, 'lambdas': [1e-6, 1e-4, 1e-2, 1e-1, 1.0, 10.0]},\n        \n        # Test case B (underdetermined, very low noise)\n        {'s': 101, 'n': 20, 'p': 50, 'sigma': 5e-2, 'lambdas': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]},\n        \n        # Test case C (well-posed, high noise)\n        {'s': 2023, 'n': 50, 'p': 10, 'sigma': 1.0, 'lambdas': [1e-5, 1e-3, 1e-2, 1e-1, 1.0, 1e2]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['s'], case['n'], case['p'], case['sigma'], case['lambdas'])\n        results.append(result)\n\n    # Format the output string exactly as required, without extra spaces.\n    outer_list_parts = []\n    for res_pair in results:\n        pareto_indices, dominated_count = res_pair\n        indices_str = '[' + ','.join(map(str, pareto_indices)) + ']'\n        pair_str = f\"[{indices_str},{dominated_count}]\"\n        outer_list_parts.append(pair_str)\n    \n    final_output = '[' + ','.join(outer_list_parts) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3168619"}, {"introduction": "Controlling model complexity goes beyond adding explicit penalty terms to a loss function. This advanced practice explores the concept of *implicit regularization*, where properties of the optimization algorithm itself can prevent overfitting. You will implement the Iterative Shrinkage-Thresholding Algorithm (ISTA) for the LASSO problem and compare the effect of explicit $\\ell_1$ penalization against the implicit regularization provided by simply stopping the optimization process early, a technique foundational to modern large-scale machine learning [@problem_id:3168592].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective as a composite function with a smooth data-fit term and a non-smooth sparsity-promoting penalty. Let the training design matrix be $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$, the validation design matrix be $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$, and the corresponding response vectors be $y_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ and $y_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$. The empirical risk minimization problem with an $\\ell_1$ penalty is\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1,\n$$\nwhere $\\lambda \\ge 0$ balances the fit to data and sparsity. The Iterative Shrinkage-Thresholding Algorithm (ISTA) is obtained by applying proximal gradient descent to the composite objective, which alternates a gradient step on the smooth term with an application of the proximal operator of the $\\ell_1$ norm. In this problem, you will unroll a fixed number of ISTA iterations $T$ as a layer-wise computation graph (interpreting $T$ as early stopping), and compare its generalization behavior to a long-run ISTA with explicit $\\lambda$ regularization.\n\nYour program must:\n- Construct synthetic linear regression datasets with sparse ground truth using the following fundamental and well-tested specifications:\n  1. The ground-truth parameter $x^\\star \\in \\mathbb{R}^d$ is $k$-sparse, with exactly $k$ nonzero entries drawn independently from a zero-mean unit-variance distribution. The support (locations of nonzero entries) is chosen uniformly at random.\n  2. The training and validation matrices are generated to have controllable column-wise correlation. For a specified correlation parameter $\\rho \\in [0,1)$, let $Z_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ and $Z_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$ have independent and identically distributed (i.i.d.) standard normal entries. For each dataset, draw a latent vector $u_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ and $u_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$ with i.i.d. standard normal entries. Define\n  $$\n  A_{\\text{train}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{train}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{train}}, \\quad\n  A_{\\text{val}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{val}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{val}},\n  $$\n  for each column index $j \\in \\{1,\\dots,d\\}$. This construction yields feature columns with correlation controlled by $\\rho$ and is scientifically sound for studying the effects of collinearity.\n  3. The training and validation responses satisfy\n  $$\n  y_{\\text{train}} \\;=\\; A_{\\text{train}} x^\\star \\;+\\; \\varepsilon_{\\text{train}}, \\quad\n  y_{\\text{val}} \\;=\\; A_{\\text{val}} x^\\star \\;+\\; \\varepsilon_{\\text{val}},\n  $$\n  where $\\varepsilon_{\\text{train}}$ and $\\varepsilon_{\\text{val}}$ have i.i.d. Gaussian entries with zero mean and standard deviation $\\sigma$ (noise level).\n- Starting from the core definitions of the squared loss and the $\\ell_1$ penalty, implement ISTA by:\n  1. Computing the gradient of the smooth term using first principles from multivariate calculus.\n  2. Choosing a constant step size $s$ that is at most the reciprocal of the Lipschitz constant of the gradient of the smooth term, where the Lipschitz constant equals the largest eigenvalue of $\\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$.\n  3. Applying the proximal operator of the $\\ell_1$ norm to the gradient-updated iterate.\n- Unroll exactly $T$ iterations from the zero vector $x^{(0)} = 0$ to obtain $x^{(T)}$ and interpret finite $T$ as early stopping, an implicit form of regularization arising from halting the optimization before convergence.\n\nFor each dataset, construct two estimators:\n- Early-stopped estimator with no explicit $\\ell_1$ penalty: set $\\lambda = 0$ and run ISTA for exactly $T_{\\text{small}}$ iterations to obtain $x^{(T_{\\text{small}})}$.\n- Explicitly regularized estimator: set a prescribed $\\lambda > 0$ and run ISTA for $T_{\\text{large}}$ iterations to approximate convergence and obtain $x^{(T_{\\text{large}})}$.\n\nEvaluate generalization by computing the Mean Squared Error (MSE) on the validation set for both estimators:\n$$\n\\text{MSE}_{\\text{val}}(x) \\;=\\; \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2.\n$$\nReport, for each dataset, whether early stopping achieves strictly lower validation error than explicit $\\lambda$ regularization. Represent this comparison as an integer: $1$ if $\\text{MSE}_{\\text{val}}(x^{(T_{\\text{small}})}) < \\text{MSE}_{\\text{val}}(x^{(T_{\\text{large}})})$, otherwise $0$.\n\nUse the following test suite to ensure coverage of distinct regimes:\n- Case $1$ (happy path, moderate noise, mild correlation): $d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.2$, $\\rho = 0.2$, $T_{\\text{small}} = 10$, $T_{\\text{large}} = 500$, $\\lambda = 0.05$.\n- Case $2$ (boundary condition with $T=0$ and higher noise): $d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 1.0$, $\\rho = 0.0$, $T_{\\text{small}} = 0$, $T_{\\text{large}} = 500$, $\\lambda = 0.1$.\n- Case $3$ (strong feature correlation, low noise): $d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.05$, $\\rho = 0.9$, $T_{\\text{small}} = 3$, $T_{\\text{large}} = 500$, $\\lambda = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is the integer comparison result for Case $i$. No physical units or angle units are involved. Ensure all random choices are reproducible by fixing seeds per case inside the program.", "solution": "This exercise compares explicit regularization ($\\ell_1$ penalty) with implicit regularization (early stopping) for the LASSO problem, using the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n\n**1. The LASSO Objective and ISTA**\nThe LASSO objective function is a composite of a smooth data-fit term and a non-smooth $\\ell_1$ penalty:\n$$ F(x) = \\underbrace{\\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2}_{f(x)} \\; + \\; \\underbrace{\\lambda \\left\\| x \\right\\|_1}_{g(x)} $$\nISTA is a proximal gradient method that solves this by iterating two steps:\n1.  A gradient descent step on the smooth term $f(x)$.\n2.  Application of the proximal operator for the non-smooth term $g(x)$.\n\nThe ISTA update rule is:\n$$ x^{(t+1)} = \\text{prox}_{s g} \\left( x^{(t)} - s \\nabla f(x^{(t)}) \\right) $$\nwhere $s$ is the step size. The gradient is $\\nabla f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top (A_{\\text{train}} x - y_{\\text{train}})$. The proximal operator for the $\\ell_1$ norm is the soft-thresholding function, $S_{s\\lambda}(v) = \\text{sign}(v) \\max(|v| - s\\lambda, 0)$. A guaranteed convergence requires the step size $s$ to be no larger than the reciprocal of the Lipschitz constant of the gradient, $L = \\left\\| \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}} \\right\\|_2$. The code correctly calculates this step size.\n\n**2. Regularization Strategies**\nTwo estimators are constructed to compare regularization methods:\n- **Early-Stopped Estimator (Implicit Regularization):** The explicit penalty is removed by setting $\\lambda=0$. The ISTA algorithm, which now acts like standard gradient descent, is started from $x^{(0)}=0$ and run for a small number of iterations, $T_{\\text{small}}$. Stopping the optimization before it converges to the (overfit) least-squares solution provides an implicit regularizing effect, keeping the parameter norms small.\n- **Explicitly Regularized Estimator:** A non-zero penalty $\\lambda > 0$ is used. The ISTA algorithm is run for a large number of iterations, $T_{\\text{large}}$, to allow it to converge near the true LASSO solution.\n\n**3. Evaluation**\nThe performance of both resulting estimators, $x^{(T_{\\text{small}})}$ and $x^{(T_{\\text{large}})}$, is measured by their Mean Squared Error (MSE) on an unseen validation set. The comparison determines if the implicit regularization from early stopping can yield a better generalizing model (lower validation MSE) than the explicitly penalized model. The code implements this procedure for three distinct data-generating scenarios to test the comparison under different conditions of noise and feature correlation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(d, n_train, n_val, k, sigma, rho, rng):\n    \"\"\"\n    Generates synthetic data for a sparse linear regression problem.\n    \n    Args:\n        d (int): Number of features.\n        n_train (int): Number of training samples.\n        n_val (int): Number of validation samples.\n        k (int): Sparsity level of the true parameter vector.\n        sigma (float): Standard deviation of the noise.\n        rho (float): Correlation parameter for the design matrix columns.\n        rng (numpy.random.Generator): NumPy random number generator instance.\n        \n    Returns:\n        tuple: A_train, y_train, A_val, y_val\n    \"\"\"\n    # 1. Generate k-sparse ground-truth parameter vector x_star\n    x_star = np.zeros(d)\n    support = rng.choice(d, k, replace=False)\n    x_star[support] = rng.standard_normal(k)\n\n    # 2. Generate training and validation design matrices A\n    # Training data\n    Z_train = rng.standard_normal((n_train, d))\n    u_train = rng.standard_normal(n_train)\n    A_train = np.sqrt(1 - rho) * Z_train + np.sqrt(rho) * u_train[:, np.newaxis]\n    \n    # Validation data\n    Z_val = rng.standard_normal((n_val, d))\n    u_val = rng.standard_normal(n_val)\n    A_val = np.sqrt(1 - rho) * Z_val + np.sqrt(rho) * u_val[:, np.newaxis]\n\n    # 3. Generate response vectors y\n    eps_train = sigma * rng.standard_normal(n_train)\n    y_train = A_train @ x_star + eps_train\n    \n    eps_val = sigma * rng.standard_normal(n_val)\n    y_val = A_val @ x_star + eps_val\n\n    return A_train, y_train, A_val, y_val\n\ndef ista(A, y, lambda_reg, T, step_size):\n    \"\"\"\n    Implements the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n    \n    Args:\n        A (np.ndarray): Design matrix.\n        y (np.ndarray): Response vector.\n        lambda_reg (float): L1 regularization parameter.\n        T (int): Number of iterations.\n        step_size (float): Step size for the gradient descent step.\n        \n    Returns:\n        np.ndarray: The estimated parameter vector x after T iterations.\n    \"\"\"\n    n, d = A.shape\n    x = np.zeros(d)\n    \n    if T == 0:\n        return x\n\n    for _ in range(T):\n        # Gradient of the smooth term\n        grad = (1 / n) * A.T @ (A @ x - y)\n        \n        # Gradient update step\n        z = x - step_size * grad\n        \n        # Proximal operator (soft-thresholding)\n        x = np.sign(z) * np.maximum(np.abs(z) - step_size * lambda_reg, 0)\n        \n    return x\n\ndef calculate_mse(A, y, x):\n    \"\"\"Calculates the Mean Squared Error.\"\"\"\n    n = A.shape[0]\n    error = A @ x - y\n    return (1 / n) * np.sum(error**2)\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda\n        (50, 80, 80, 5, 0.2, 0.2, 10, 500, 0.05),\n        (50, 80, 80, 5, 1.0, 0.0, 0, 500, 0.1),\n        (50, 80, 80, 5, 0.05, 0.9, 3, 500, 0.01),\n    ]\n\n    results = []\n    \n    for i, params in enumerate(test_cases):\n        d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda_val = params\n        \n        # Ensure reproducibility for each case by fixing the seed\n        rng = np.random.default_rng(seed=i)\n\n        # Generate data\n        A_train, y_train, A_val, y_val = generate_data(d, n_train, n_val, k, sigma, rho, rng)\n        \n        # Calculate step size\n        C = (1 / n_train) * (A_train.T @ A_train)\n        L = np.max(np.linalg.eigvalsh(C))\n        step_size = 1.0 / L\n\n        # Early-stopped estimator (lambda = 0)\n        x_early = ista(A_train, y_train, lambda_reg=0.0, T=T_small, step_size=step_size)\n        \n        # Explicitly regularized estimator (lambda > 0)\n        x_reg = ista(A_train, y_train, lambda_reg=lambda_val, T=T_large, step_size=step_size)\n        \n        # Evaluate validation MSE for both\n        mse_early = calculate_mse(A_val, y_val, x_early)\n        mse_reg = calculate_mse(A_val, y_val, x_reg)\n        \n        # Compare and store result\n        result = 1 if mse_early < mse_reg else 0\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3168592"}]}