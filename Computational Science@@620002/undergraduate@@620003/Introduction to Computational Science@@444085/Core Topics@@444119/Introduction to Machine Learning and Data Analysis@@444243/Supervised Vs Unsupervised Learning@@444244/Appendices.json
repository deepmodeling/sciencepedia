{"hands_on_practices": [{"introduction": "This first exercise is designed to build a strong intuition for the fundamental difference between supervised and unsupervised learning objectives. By constructing datasets where supervised accuracy and unsupervised cluster quality diverge, you will see firsthand that the two paradigms are asking different questions of the data. This practice [@problem_id:3199424] demonstrates that a geometrically \"good\" clustering does not always correspond to a predictively useful classification, a crucial concept when choosing the right tool for a data analysis task.", "problem": "In supervised learning, a classifier is trained to map feature vectors to given labels to minimize empirical error, while in unsupervised learning, structure is inferred from the data alone without labels. Consider points in two-dimensional Euclidean space with features denoted by $x = (x_1, x_2) \\in \\mathbb{R}^2$ and binary class labels $y \\in \\{0, 1\\}$. The supervised objective is to choose predictions $\\hat{y}$ for each $x$ that minimize the empirical classification error on the training set, a direct instance of Empirical Risk Minimization (ERM). The unsupervised objective in clustering is to partition points into $k$ groups to optimize a cohesion and separation criterion; a widely used option is the silhouette score based on pairwise Euclidean distances. This problem operationalizes the comparison: you will compute a supervised accuracy using a simple threshold classifier trained on labels, and an unsupervised silhouette score using K-Means Clustering (K-Means) run on the raw features with a deterministic initialization, then evaluate three datasets designed to highlight when the two objectives diverge.\n\nDefinitions and required computations:\n- Supervised classifier. Restrict to a one-dimensional decision on the first coordinate $x_1$. For a threshold $t \\in \\mathbb{R}$ and a left-side class $c_{\\text{left}} \\in \\{0, 1\\}$, the classifier predicts\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}}, & \\text{if } x_1 \\le t,\\\\\n1 - c_{\\text{left}}, & \\text{if } x_1 > t.\n\\end{cases}\n$$\nAmong all thresholds formed by midpoints between consecutive sorted $x_1$ values (including values just below the minimum and just above the maximum), choose the $(t, c_{\\text{left}})$ that maximizes the training accuracy\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right),\n$$\nwhere $N$ is the number of samples and $\\mathbf{1}(\\cdot)$ is the indicator function. In ties, choose $c_{\\text{left}} = 0$ and the smallest $t$ achieving the maximum. Report $\\text{Acc}$ as a decimal in $[0,1]$.\n- Unsupervised clustering. Run K-Means with $k = 2$ clusters on the feature vectors $x^{(i)}$ using Euclidean distance. Initialize the two centroids deterministically as the points with minimal and maximal $x_1$ values. Iteratively reassign points to the nearest centroid and update centroids as the mean of assigned points until assignments stabilize or until $100$ iterations elapse. If a cluster becomes empty, reinitialize its centroid as the point farthest (in Euclidean distance) from the other centroid. After convergence, compute the silhouette score. For each point $i$, let $a(i)$ be the average Euclidean distance from $x^{(i)}$ to all other points in its assigned cluster (if that cluster has size $1$, let $a(i) = 0$). Let $b(i)$ be the minimum, over all other clusters, of the average Euclidean distance from $x^{(i)}$ to points in that cluster. The silhouette value is\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}, & \\text{if } \\max\\{a(i), b(i)\\} > 0,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nand the silhouette score is the mean of $s(i)$ over all points.\n\nTest suite datasets (each sample is given as $(x_1, x_2)$ with an associated label $y$; the order is arbitrary and all numbers are real):\n1. Dataset A (moderate separation with slight overlap along $x_1$):\n   - Class $0$: $(-0.4, 0.0)$, $(-0.2, 0.1)$, $(0.2, -0.1)$, $(0.3, 0.05)$\n   - Class $1$: $(0.2, 0.0)$, $(0.6, -0.1)$, $(0.8, 0.1)$, $(1.0, -0.05)$\n2. Dataset B (greater separation along $x_1$ but large noise along $x_2$):\n   - Class $0$: $(-0.4, 5.0)$, $(-0.2, -5.0)$, $(0.2, 7.0)$, $(0.3, -7.0)$\n   - Class $1$: $(1.2, 6.0)$, $(1.6, -6.0)$, $(1.8, 8.0)$, $(2.0, -8.0)$\n3. Dataset C (near-complete overlap: identical distributions by construction):\n   - Class $0$: $(-0.1, 0.05)$, $(0.0, -0.02)$, $(0.1, 0.03)$, $(-0.05, -0.04)$\n   - Class $1$: $(-0.08, 0.01)$, $(0.02, -0.03)$, $(0.09, 0.04)$, $(-0.02, -0.05)$\n\nYour program must implement the above procedures and, for each dataset, output the pair $[\\text{Acc}, \\text{Sil}]$, where $\\text{Acc}$ is the supervised training accuracy and $\\text{Sil}$ is the silhouette score of the final K-Means clustering. Express each value as a decimal rounded to six places. Final output format: a single line containing a top-level list of three pairs, in the dataset order A, B, C, with comma-separated entries, for example, $[[a_1,s_1],[a_2,s_2],[a_3,s_3]]$ with numerals to six decimal places.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of machine learning, is well-posed with deterministic procedures ensuring a unique solution, and is expressed in objective, formal language. All data and definitions required for a solution are present and consistent. I will now proceed with a complete solution.\n\nThe problem requires a comparative analysis of supervised and unsupervised learning on three datasets. For each dataset, we must compute two metrics: the accuracy of a simple supervised classifier and the silhouette score of an unsupervised clustering algorithm.\n\nA dataset consists of $N$ points in a two-dimensional space, where each point $x^{(i)} = (x_1^{(i)}, x_2^{(i)}) \\in \\mathbb{R}^2$ is associated with a binary label $y^{(i)} \\in \\{0, 1\\}$.\n\n### Supervised Classification: 1D Threshold Classifier\n\nThe supervised learning task is to find the optimal one-dimensional threshold classifier that minimizes classification error on the training set. This is an instance of Empirical Risk Minimization (ERM).\n\nThe classifier's prediction $\\hat{y}(x)$ for a point $x = (x_1, x_2)$ is defined based on a threshold $t$ on the first feature $x_1$ and a class assignment $c_{\\text{left}} \\in \\{0, 1\\}$ for the \"left\" side of the decision boundary:\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}}, & \\text{if } x_1 \\le t, \\\\\n1 - c_{\\text{left}}, & \\text{if } x_1 > t.\n\\end{cases}\n$$\nThe goal is to select the pair $(t, c_{\\text{left}})$ that maximizes the training accuracy, defined as:\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right)\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function which is $1$ if its argument is true and $0$ otherwise.\n\nTo find the optimal classifier, we must search over a set of candidate thresholds $t$ and both possible values for $c_{\\text{left}}$. The classification boundary only needs to be checked at locations that change the partitioning of the data. Let the unique sorted values of the first feature be $u_1 < u_2 < \\dots < u_m$. The set of meaningful thresholds consists of the midpoints between these consecutive values, $t_j = (u_j + u_{j+1})/2$ for $j=1, \\dots, m-1$. Additionally, to account for cases where all points are classified into a single group, we test thresholds below the minimum $x_1$ value (e.g., $u_1 - 1$) and above the maximum $x_1$ value (e.g., $u_m + 1$).\n\nFor each candidate threshold $t$, we evaluate the accuracy for both $c_{\\text{left}} = 0$ and $c_{\\text{left}} = 1$. We select the pair $(t, c_{\\text{left}})$ that yields the highest accuracy. The problem specifies a tie-breaking rule: in case of a tie in accuracy, we must choose the classifier with $c_{\\text{left}} = 0$. If there is still a tie, the one with the smallest threshold $t$ is chosen.\n\n### Unsupervised Clustering: K-Means with Silhouette Score\n\nThe unsupervised task is to partition the data points $x^{(i)}$ into $k=2$ clusters using the K-Means algorithm and then evaluate the quality of this partition using the silhouette score.\n\n**K-Means Algorithm**\nThe algorithm proceeds as follows:\n1.  **Initialization**: The two centroids, $\\mu_1$ and $\\mu_2$, are deterministically initialized to the data points with the minimum and maximum values of the first feature, $x_1$, respectively.\n2.  **Iteration**: The algorithm iterates between two steps until cluster assignments stabilize or a maximum of $100$ iterations is reached.\n    a.  **Assignment Step**: Each data point $x^{(i)}$ is assigned to the cluster corresponding to the nearest centroid, based on Euclidean distance: $C_j \\leftarrow \\{x^{(i)} \\mid \\|x^{(i)} - \\mu_j\\|_2 \\le \\|x^{(i)} - \\mu_l\\|_2 \\text{ for all } l=1, \\dots, k\\}$.\n    b.  **Update Step**: Each centroid $\\mu_j$ is re-computed as the mean of all points assigned to its cluster: $\\mu_j \\leftarrow \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$.\n3.  **Empty Cluster Handling**: If a cluster $C_j$ becomes empty during the update step, its centroid is reinitialized. The new centroid is set to be the data point in the entire dataset that is farthest (in Euclidean distance) from the other (non-empty) cluster's centroid.\n\n**Silhouette Score**\nAfter the K-Means algorithm converges, the resulting clustering is evaluated using the silhouette score. For each point $x^{(i)}$, we calculate:\n-   $a(i)$: The average Euclidean distance from $x^{(i)}$ to all other points within its own cluster. If the cluster contains only one point, $a(i) = 0$.\n-   $b(i)$: The average Euclidean distance from $x^{(i)}$ to all points in the nearest neighboring cluster. Since $k=2$, this is simply the average distance to all points in the other cluster.\n\nThe silhouette value for point $i$ is then:\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}, & \\text{if } \\max\\{a(i), b(i)\\} > 0, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe silhouette score for the entire clustering is the mean of the silhouette values $s(i)$ over all data points. A score near $1$ indicates well-separated clusters, a score near $0$ indicates overlapping clusters, and a negative score suggests points might have been assigned to the wrong cluster.\n\n### Application to Datasets\n\nThe described procedures are applied to each of the three provided datasets. For each dataset, we first construct the feature matrix $X$ and the label vector $y$. Then, we execute the supervised accuracy calculation and the unsupervised K-Means/silhouette score calculation. The resulting pair of values, $[\\text{Acc}, \\text{Sil}]$, is computed for each dataset, with each value rounded to six decimal places.\n\n- **Dataset A** presents a scenario with moderate separation along $x_1$, but some overlap, making the supervised task non-trivial. The unsupervised clustering will depend on the geometric structure of the point cloud.\n- **Dataset B** is designed such that the labels are perfectly separable along $x_1$, but a large variance along $x_2$ acts as a distractor. This tests whether the unsupervised algorithm is misled by the noisy feature $x_2$, which dominates the Euclidean distance metric.\n- **Dataset C** features heavily overlapping distributions for the two classes. Both supervised and unsupervised methods are expected to perform poorly, resulting in a low accuracy (near random chance, $0.5$) and a low silhouette score (near $0$).\n\nThe final output is a list containing the $[\\text{Acc}, \\text{Sil}]$ pairs for datasets A, B, and C, in order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_accuracy(X, y):\n    \"\"\"\n    Calculates the maximum accuracy of a 1D threshold classifier on x1.\n    \"\"\"\n    x1 = X[:, 0]\n    \n    unique_x1 = np.unique(x1)\n    \n    thresholds = []\n    if len(unique_x1) > 1:\n        # Midpoints between unique consecutive sorted x1 values\n        thresholds.extend((unique_x1[:-1] + unique_x1[1:]) / 2)\n    # Add thresholds to classify all points into one group\n    thresholds.append(unique_x1[0] - 1.0)\n    thresholds.append(unique_x1[-1] + 1.0)\n    \n    # Use a tuple (acc, c_left, t) for lexicographical comparison\n    # Goal: max(acc), min(c_left), min(t)\n    # This is equivalent to finding max of (acc, 1-c_left, -t)\n    best_params = (-1.0, 2, float('inf')) # (acc, c_left, t)\n\n    for t in sorted(list(set(thresholds))):\n        for c_left in [0, 1]:\n            c_right = 1 - c_left\n            y_pred = np.where(x1 <= t, c_left, c_right)\n            acc = np.mean(y_pred == y)\n            \n            # Tie-breaking logic\n            current_params = (acc, c_left, t)\n            if current_params[0] > best_params[0]:\n                best_params = current_params\n            elif current_params[0] == best_params[0]:\n                if current_params[1] < best_params[1]:\n                    best_params = current_params\n                elif current_params[1] == best_params[1] and current_params[2] < best_params[2]:\n                    best_params = current_params\n                    \n    return best_params[0]\n\ndef _calculate_silhouette(X):\n    \"\"\"\n    Performs K-Means clustering (k=2) and calculates the silhouette score.\n    \"\"\"\n    N, D = X.shape\n    k = 2\n\n    if N < k:\n        return 0.0\n\n    # 1. Deterministic Initialization\n    idx_min_x1 = np.argmin(X[:, 0])\n    idx_max_x1 = np.argmax(X[:, 0])\n    \n    if idx_min_x1 == idx_max_x1:\n        # Pathological case not present in test data but good practice to handle.\n        # If all x1 are same, initialization would give one centroid.\n        # For this problem's scope, we assume this does not happen.\n        if N > 1:\n             idx_max_x1 = (idx_min_x1 + 1) % N # Pick another point\n        else:\n             return 0.0 # Single point has 0 silhouette score.\n    \n    centroids = np.array([X[idx_min_x1], X[idx_max_x1]], dtype=np.float64)\n    \n    assignments = np.zeros(N, dtype=int)\n    \n    for _ in range(100):\n        # 2. Assignment step (vectorized)\n        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n        new_assignments = np.argmin(distances, axis=1)\n            \n        # 3. Check for convergence\n        if np.array_equal(assignments, new_assignments):\n            break\n        assignments = new_assignments\n        \n        # 4. Update step with empty cluster handling\n        new_centroids = np.zeros_like(centroids)\n        cluster_sizes = np.bincount(assignments, minlength=k)\n\n        # First, compute centroids for non-empty clusters\n        non_empty_updated = [False, False]\n        for j in range(k):\n            if cluster_sizes[j] > 0:\n                new_centroids[j] = np.mean(X[assignments == j], axis=0)\n                non_empty_updated[j] = True\n\n        # Then, handle any empty clusters\n        for j in range(k):\n            if cluster_sizes[j] == 0:\n                other_j = 1 - j\n                # Use the newly computed centroid of the other cluster\n                other_centroid = new_centroids[other_j]\n                \n                # Reinitialize as the point farthest from the other centroid\n                dists_from_other_centroid = np.linalg.norm(X - other_centroid, axis=1)\n                farthest_point_idx = np.argmax(dists_from_other_centroid)\n                new_centroids[j] = X[farthest_point_idx]\n        \n        centroids = new_centroids\n    \n    # After convergence, use the final assignments\n    assignments = new_assignments\n    \n    # 5. Silhouette score calculation\n    if len(np.unique(assignments)) < 2:\n        return 0.0\n\n    silhouette_values = np.zeros(N)\n    for i in range(N):\n        point_i = X[i]\n        cluster_idx = assignments[i]\n        \n        # Intra-cluster distance a(i)\n        mask_a = (assignments == cluster_idx) & (np.arange(N) != i)\n        if not np.any(mask_a):\n            a_i = 0.0\n        else:\n            a_i = np.mean(np.linalg.norm(X[mask_a] - point_i, axis=1))\n\n        # Inter-cluster distance b(i)\n        other_cluster_idx = 1 - cluster_idx\n        mask_b = assignments == other_cluster_idx\n        \n        if not np.any(mask_b): # Should not happen if len(unique(assignments)) > 1\n             b_i = 0.0\n        else:\n             b_i = np.mean(np.linalg.norm(X[mask_b] - point_i, axis=1))\n\n        if max(a_i, b_i) == 0:\n            silhouette_values[i] = 0.0\n        else:\n            silhouette_values[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(silhouette_values)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"class_0\": [(-0.4, 0.0), (-0.2, 0.1), (0.2, -0.1), (0.3, 0.05)],\n            \"class_1\": [(0.2, 0.0), (0.6, -0.1), (0.8, 0.1), (1.0, -0.05)]\n        },\n        # Dataset B\n        {\n            \"class_0\": [(-0.4, 5.0), (-0.2, -5.0), (0.2, 7.0), (0.3, -7.0)],\n            \"class_1\": [(1.2, 6.0), (1.6, -6.0), (1.8, 8.0), (2.0, -8.0)]\n        },\n        # Dataset C\n        {\n            \"class_0\": [(-0.1, 0.05), (0.0, -0.02), (0.1, 0.03), (-0.05, -0.04)],\n            \"class_1\": [(-0.08, 0.01), (0.02, -0.03), (0.09, 0.04), (-0.02, -0.05)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        points_0 = np.array(case[\"class_0\"])\n        points_1 = np.array(case[\"class_1\"])\n        \n        X = np.concatenate((points_0, points_1), axis=0)\n        y = np.concatenate((np.zeros(len(points_0), dtype=int), np.ones(len(points_1), dtype=int)), axis=0)\n\n        accuracy = _calculate_accuracy(X, y)\n        silhouette = _calculate_silhouette(X)\n        \n        results.append([accuracy, silhouette])\n\n    # Final print statement in the exact required format.\n    results_as_strings = [f\"[{acc:.6f},{sil:.6f}]\" for acc, sil in results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3199424"}, {"introduction": "Moving from conceptual foundations to a practical application, this next problem places you in the role of a computational biologist. You will compare a sophisticated supervised model, a Recurrent Neural Network (RNN), against a thoughtfully designed unsupervised clustering method for predicting protein secondary structure [@problem_id:2432793]. This exercise highlights a key trade-off in machine learning: the power of general-purpose, data-driven models versus the effectiveness of simpler methods guided by domain-specific feature engineering, especially when model architectures have inherent limitations.", "problem": "You are given a binary, position-wise prediction task representative of protein secondary structure assignment. Let the alphabet of amino acids be the standard set of $20$ one-letter codes. For any amino acid $a$, define a scalar propensity function $\\phi(a)$ by\n- $\\phi(a) = +1$ if $a \\in \\{\\text{A}, \\text{L}, \\text{M}, \\text{Q}, \\text{E}, \\text{K}, \\text{R}, \\text{H}\\}$ (commonly helix-preferring),\n- $\\phi(a) = -1$ if $a \\in \\{\\text{V}, \\text{I}, \\text{Y}, \\text{F}, \\text{W}, \\text{T}\\}$ (commonly sheet-preferring),\n- $\\phi(a) = 0$ otherwise (neutral set $\\{\\text{C}, \\text{D}, \\text{N}, \\text{P}, \\text{G}, \\text{S}\\}$).\n\nFor a protein sequence $s = (a_1, a_2, \\dots, a_T)$, define the ground-truth secondary structure label at each position $i \\in \\{1,\\dots,T\\}$ as $y_i \\in \\{0,1\\}$ with $1$ denoting alpha-helix and $0$ denoting beta-sheet, by the local-context rule\n$$\n\\tilde{z}_i \\;=\\; \\beta_0 + \\beta_1 \\,\\phi(a_i) + \\beta_2 \\,\\phi(a_{i-1}) + \\beta_3 \\,\\phi(a_{i+1}),\n$$\nwith boundary convention $\\phi(a_0)=\\phi(a_{T+1})=0$. Then\n$$\ny_i \\;=\\; \\begin{cases}\n1 & \\text{if } \\tilde{z}_i \\ge 0,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nUse fixed coefficients $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 0.5$, $\\beta_3 = 0.5$.\n\nConsider a supervised model that maps the one-hot encoding $x_i \\in \\{0,1\\}^{20}$ of $a_i$ to a hidden state $h_i \\in \\mathbb{R}^d$ and an output $\\hat{y}_i \\in (0,1)$ via a Recurrent Neural Network (RNN) defined as follows. Let $h_0 = 0 \\in \\mathbb{R}^d$, then for $i=1,\\dots,T$,\n$$\nh_i \\;=\\; \\tanh\\!\\left(W_x x_i + W_h h_{i-1} + b\\right), \\quad s_i \\;=\\; U h_i + c, \\quad \\hat{y}_i \\;=\\; \\sigma(s_i),\n$$\nwhere $\\sigma(u) = \\frac{1}{1+e^{-u}}$, $W_x \\in \\mathbb{R}^{d \\times 20}$, $W_h \\in \\mathbb{R}^{d \\times d}$, $b \\in \\mathbb{R}^d$, $U \\in \\mathbb{R}^{1 \\times d}$, and $c \\in \\mathbb{R}$. The supervised objective over a set of labeled sequences is the average binary cross-entropy with $\\ell_2$-regularization,\n$$\n\\mathcal{L} \\;=\\; \\frac{1}{N}\\sum_{n=1}^N \\frac{1}{T_n} \\sum_{i=1}^{T_n} \\left[-y_i^{(n)} \\log \\hat{y}_i^{(n)} - \\left(1-y_i^{(n)}\\right)\\log\\left(1-\\hat{y}_i^{(n)}\\right)\\right] \\;+\\; \\lambda \\left(\\lVert W_x\\rVert_F^2 + \\lVert W_h\\rVert_F^2 + \\lVert U\\rVert_F^2\\right),\n$$\nwith regularization coefficient $\\lambda = 10^{-4}$ and hidden dimension $d = 6$.\n\nAlso consider an unsupervised baseline that ignores labels and performs clustering in a one-dimensional feature space using $k$-means with $k=2$. For any position $i$, define the scalar feature\n$$\nf_i \\;=\\; \\phi(a_i) + 0.5\\,\\phi(a_{i-1}) + 0.5\\,\\phi(a_{i+1}),\n$$\nwith the same boundary conventions. The clustering yields two centroids $m_1, m_2 \\in \\mathbb{R}$; assign the label $\\hat{y}^{\\text{unsup}}_i = 1$ to the cluster whose centroid is larger and $\\hat{y}^{\\text{unsup}}_i = 0$ to the other. At evaluation time, a position $i$ is assigned to the nearest centroid in Euclidean distance.\n\nTraining and evaluation data are specified as follows. The supervised model must be trained only on the training set sequences, using the labels $y_i$ defined above. The unsupervised model must be fit only on training data features $\\{f_i\\}$ constructed from the training sequences. Then both models must be evaluated on the test suite sequences below by computing the fraction of correctly predicted labels (accuracy) relative to the ground-truth $y_i$ defined above.\n\nTraining set sequences:\n- S-train-1: \"ALMEKRALMEKRAALMQEKRHALMEKRA\"\n- S-train-2: \"VIVTWIYVIVTWYVIVTWIYVIVTWI\"\n- S-train-3: \"ALMEKRGPGSNNDCALMEKRGPGS\"\n- S-train-4: \"GPGSNDNCGPGSGPGSALVIALVIALVIA\"\n- S-train-5: \"ALMEKRALVIALMEKRVIYTWALMEKRV\"\n\nTest suite sequences:\n- S1 (general mixed case): \"ALMEKRVIVTWTALMEQKRVIV\"\n- S2 (boundary length-$1$ case): \"A\"\n- S3 (all helix-favoring bias): \"ALMEKRAALMEKR\"\n- S4 (all sheet-favoring bias): \"VIVTWIYVIVTWI\"\n\nYour program must implement the following and produce the specified output:\n- Train the supervised recurrent model described above on the training set and then, for each test sequence, compute the accuracy\n$$\nA_{\\text{sup}}(\\text{S}j) \\;=\\; \\frac{1}{T_j} \\sum_{i=1}^{T_j} \\mathbb{I}\\left[\\mathbb{I}\\left(\\hat{y}_i^{(\\text{S}j)} \\ge 0.5\\right) = y_i^{(\\text{S}j)}\\right],\n$$\nfor $j \\in \\{1,2,3,4\\}$, where $T_j$ is the length of sequence $\\text{S}j$.\n- Fit the unsupervised baseline on training features $\\{f_i\\}$ and then, for each test sequence, compute the accuracy\n$$\nA_{\\text{unsup}}(\\text{S}j) \\;=\\; \\frac{1}{T_j} \\sum_{i=1}^{T_j} \\mathbb{I}\\left[\\hat{y}^{\\text{unsup}}_i(\\text{S}j) = y_i^{(\\text{S}j)}\\right].\n$$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $8$ floating-point numbers in the following order:\n$$\n\\big[ A_{\\text{sup}}(\\text{S}1),\\, A_{\\text{unsup}}(\\text{S}1),\\, A_{\\text{sup}}(\\text{S}2),\\, A_{\\text{unsup}}(\\text{S2),\\, A_{\\text{sup}}(\\text{S}3),\\, A_{\\text{unsup}}(\\text{S}3),\\, A_{\\text{sup}}(\\text{S}4),\\, A_{\\text{unsup}}(\\text{S}4) \\big].\n$$\nAll accuracies must be expressed as decimal fractions in $[0,1]$ with no percentage signs. The program must not read any input and must use only the specified training and test sequences above.", "solution": "The problem statement presented is a well-defined computational task in bioinformatics, comparing a supervised learning model against an unsupervised baseline for a simplified protein secondary structure prediction problem. All components—data, models, parameters, and evaluation metrics—are specified with sufficient clarity. The problem is scientifically grounded, logically consistent, and formally structured. It is therefore deemed valid.\n\nThe solution will be developed in three principal stages:\n1.  Implementation of the ground-truth label and feature generation logic.\n2.  Implementation and evaluation of the unsupervised $k$-means clustering model.\n3.  Implementation, training, and evaluation of the supervised Recurrent Neural Network (RNN) model.\n\n**1. Ground-Truth Data Generation**\n\nThe foundation of this problem is the deterministic rule for generating ground-truth labels $y_i \\in \\{0, 1\\}$. This rule depends on a scalar propensity function $\\phi(a)$ for each amino acid $a$.\n-   $\\phi(a) = +1$ for helix-preferring residues: $\\{\\text{A}, \\text{L}, \\text{M}, \\text{Q}, \\text{E}, \\text{K}, \\text{R}, \\text{H}\\}$.\n-   $\\phi(a) = -1$ for sheet-preferring residues: $\\{\\text{V}, \\text{I}, \\text{Y}, \\text{F}, \\text{W}, \\text{T}\\}$.\n-   $\\phi(a) = 0$ for neutral residues: $\\{\\text{C}, \\text{D}, \\text{N}, \\text{P}, \\text{G}, \\text{S}\\}$.\n\nFor a protein sequence of length $T$, the real-valued score $\\tilde{z}_i$ at each position $i$ is a linear combination of propensities in a local window of size $3$:\n$$\n\\tilde{z}_i = \\beta_0 + \\beta_1 \\phi(a_i) + \\beta_2 \\phi(a_{i-1}) + \\beta_3 \\phi(a_{i+1})\n$$\nWith the given coefficients $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 0.5$, $\\beta_3 = 0.5$, and boundary conditions $\\phi(a_0) = \\phi(a_{T+1}) = 0$, this simplifies to:\n$$\n\\tilde{z}_i = \\phi(a_i) + 0.5 \\phi(a_{i-1}) + 0.5 \\phi(a_{i+1})\n$$\nThe binary ground-truth label $y_i$ is then determined by the sign of $\\tilde{z}_i$:\n$$\ny_i = \\begin{cases} 1 & \\text{if } \\tilde{z}_i \\ge 0 \\\\ 0 & \\text{if } \\tilde{z}_i < 0 \\end{cases}\n$$\nThis logic will be encapsulated in a function to generate labels for any given amino acid sequence.\n\n**2. Unsupervised Baseline Model**\n\nThe unsupervised model is based on $k$-means clustering with $k=2$. Crucially, the feature used for clustering at position $i$ is defined as:\n$$\nf_i = \\phi(a_i) + 0.5 \\phi(a_{i-1}) + 0.5 \\phi(a_{i+1})\n$$\nThis is identical to the score $\\tilde{z}_i$ used to generate the ground truth. The model operates as follows:\n-   **Fitting:** The set of all features $\\{f_i\\}$ is computed from all training sequences. A $1$-dimensional $k$-means algorithm is applied to this dataset to find two cluster centroids, $m_1$ and $m_2$.\n-   **Labeling Convention:** The cluster with the larger centroid (e.g., $\\max(m_1, m_2)$) is assigned the label $1$ (helix), and the other is assigned label $0$ (sheet).\n-   **Prediction:** For a feature $f_j$ from a test sequence, the prediction $\\hat{y}^{\\text{unsup}}_j$ is determined by assigning $f_j$ to the cluster with the nearest centroid. If we assume $m_1 < m_2$, the decision boundary is at $\\frac{m_1 + m_2}{2}$. A position $j$ is predicted as helix (label $1$) if $f_j > \\frac{m_1 + m_2}{2}$ and sheet (label $0$) otherwise.\n\nThe accuracy of this model depends on how the data-driven decision boundary $\\frac{m_1 + m_2}{2}$ compares to the ground-truth boundary, which is fixed at $0$. Since the features $f_i$ are the same as the ground-truth scores $\\tilde{z}_i$, the training data for clustering naturally separates into a group of non-negative values (true label $1$) and a group of negative values (true label $0$). The $k$-means algorithm is expected to find one positive centroid and one negative centroid, resulting in a decision boundary close to $0$ and thus high accuracy.\n\n**3. Supervised RNN Model**\n\nThe supervised model is a standard Recurrent Neural Network (RNN).\n-   **Architecture:**\n    $$\n    h_i = \\tanh(W_x x_i + W_h h_{i-1} + b) \\\\\n    \\hat{y}_i = \\sigma(U h_i + c)\n    $$\n    The input $x_i \\in \\{0, 1\\}^{20}$ is the one-hot encoding of amino acid $a_i$. The hidden dimension is $d=6$.\n-   **Limitation:** This is a unidirectional RNN, meaning the state $h_i$ and prediction $\\hat{y}_i$ depend only on the input sequence up to position $i$, $(a_1, \\dots, a_i)$. However, the ground-truth label $y_i$ depends on the future amino acid $a_{i+1}$. This architectural mismatch means the model cannot, in principle, perfectly replicate the ground-truth function on arbitrary sequences. It can only succeed by learning statistical patterns from the training data that allow it to \"anticipate\" $a_{i+1}$ based on the history $(a_1, \\dots, a_i)$.\n-   **Training:** The model parameters ($W_x, W_h, b, U, c$) are optimized by minimizing the regularized binary cross-entropy loss function over the training set.\n    $$\n    \\mathcal{L} = \\text{BCE} + \\lambda \\cdot (\\lVert W_x\\rVert_F^2 + \\lVert W_h\\rVert_F^2 + \\lVert U\\rVert_F^2)\n    $$\n    Optimization is performed using stochastic gradient descent with gradients computed via the backpropagation through time (BPTT) algorithm. The regularization coefficient is $\\lambda = 10^{-4}$. We will use a fixed learning rate and a set number of training epochs, which are standard hyperparameters for such a procedure.\n-   **Prediction:** After training, the model predicts a label of $1$ if its output $\\hat{y}_i \\ge 0.5$ and $0$ otherwise.\n\n**4. Implementation and Evaluation**\n\nThe program will implement both models. For the unsupervised model, a simple iterative $k$-means algorithm is sufficient. For the supervised model, we will implement the forward pass and BPTT to train the network. Both trained models will then be evaluated on the four test sequences to compute their respective accuracies, defined as the fraction of correctly predicted labels. The final output will be a list of these eight accuracy values in the specified order. The parameters for the RNN will be initialized randomly, and the training process involves inherent stochasticity. However, given the deterministic nature of the ground truth and the small size of the problem, the training process is expected to converge to a stable and representative solution.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Implements and evaluates supervised and unsupervised models for a protein secondary structure task.\n    \"\"\"\n    \n    # ------------------- PROBLEM DEFINITION -------------------\n    \n    AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n    AA_TO_IX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n    VOCAB_SIZE = len(AMINO_ACIDS)\n\n    HELIX_PREF = set(\"ALMQEKRH\")\n    SHEET_PREF = set(\"VIYFWT\")\n\n    BETA_0, BETA_1, BETA_2, BETA_3 = 0.0, 1.0, 0.5, 0.5\n    \n    D_HIDDEN = 6\n    LAMBDA_REG = 1e-4\n    LEARNING_RATE = 0.01\n    EPOCHS = 1000\n    \n    # Using a fixed seed for reproducibility of RNN initialization and training.\n    np.random.seed(42)\n\n    TRAIN_SEQS = [\n        \"ALMEKRALMEKRAALMQEKRHALMEKRA\",\n        \"VIVTWIYVIVTWYVIVTWIYVIVTWI\",\n        \"ALMEKRGPGSNNDCALMEKRGPGS\",\n        \"GPGSNDNCGPGSGPGSALVIALVIALVIA\",\n        \"ALMEKRALVIALMEKRVIYTWALMEKRV\",\n    ]\n    \n    TEST_SUITE = {\n        \"S1\": \"ALMEKRVIVTWTALMEQKRVIV\",\n        \"S2\": \"A\",\n        \"S3\": \"ALMEKRAALMEKR\",\n        \"S4\": \"VIVTWIYVIVTWI\",\n    }\n    \n    # ------------------- HELPER FUNCTIONS -------------------\n\n    def get_propensity(aa):\n        if aa in HELIX_PREF: return 1.0\n        if aa in SHEET_PREF: return -1.0\n        return 0.0\n\n    def generate_truth(sequence):\n        T = len(sequence)\n        propensities = [get_propensity(aa) for aa in sequence]\n        z_tilde = np.zeros(T)\n        for i in range(T):\n            phi_i = propensities[i]\n            phi_prev = propensities[i-1] if i > 0 else 0.0\n            phi_next = propensities[i+1] if i < T - 1 else 0.0\n            z_tilde[i] = BETA_0 + BETA_1 * phi_i + BETA_2 * phi_prev + BETA_3 * phi_next\n        \n        labels = (z_tilde >= 0).astype(int)\n        features = z_tilde\n        return features, labels\n\n    def one_hot_encode(sequence):\n        T = len(sequence)\n        x = np.zeros((T, VOCAB_SIZE))\n        for i, aa in enumerate(sequence):\n            x[i, AA_TO_IX[aa]] = 1\n        return x\n\n    # ------------------- UNSUPERVISED MODEL -------------------\n    \n    class KMeans1D:\n        def __init__(self, k=2):\n            self.k = k\n            self.centroids = None\n            self.cluster_labels = None\n\n        def fit(self, data):\n            # Initialize centroids\n            centroids = np.random.choice(np.unique(data), self.k, replace=False)\n            \n            for _ in range(100): # Max iterations\n                clusters = [[] for _ in range(self.k)]\n                for point in data:\n                    distances = [np.abs(point - c) for c in centroids]\n                    closest_idx = np.argmin(distances)\n                    clusters[closest_idx].append(point)\n                \n                new_centroids = np.array([np.mean(c) if c else centroids[i] for i, c in enumerate(clusters)])\n                \n                if np.all(new_centroids == centroids):\n                    break\n                centroids = new_centroids\n            \n            self.centroids = centroids\n            # Assign label 1 to larger centroid, 0 to smaller\n            self.cluster_labels = (self.centroids == np.max(self.centroids)).astype(int)\n\n        def predict(self, data):\n            preds = []\n            for point in data:\n                distances = [np.abs(point - c) for c in self.centroids]\n                closest_idx = np.argmin(distances)\n                preds.append(self.cluster_labels[closest_idx])\n            return np.array(preds)\n            \n    # ------------------- SUPERVISED MODEL (RNN) -------------------\n    \n    class SimpleRNN:\n        def __init__(self, input_dim, hidden_dim, output_dim, reg_lambda):\n            self.input_dim = input_dim\n            self.hidden_dim = hidden_dim\n            self.reg_lambda = reg_lambda\n\n            # Xavier/Glorot initialization\n            self.Wx = np.random.randn(hidden_dim, input_dim) * np.sqrt(1.0 / input_dim)\n            self.Wh = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(1.0 / hidden_dim)\n            self.b = np.zeros(hidden_dim)\n            self.U = np.random.randn(output_dim, hidden_dim) * np.sqrt(1.0 / hidden_dim)\n            self.c = np.zeros(output_dim)\n\n        def forward(self, x_seq):\n            T = x_seq.shape[0]\n            h = np.zeros((T + 1, self.hidden_dim))\n            s = np.zeros(T)\n            y_hat = np.zeros(T)\n            \n            for t in range(T):\n                h[t+1] = np.tanh(self.Wx @ x_seq[t] + self.Wh @ h[t] + self.b)\n                s[t] = (self.U @ h[t+1] + self.c).item()\n                y_hat[t] = sigmoid(s[t])\n            return h, s, y_hat\n        \n        def train_step(self, x_seq, y_seq, learning_rate):\n            T = x_seq.shape[0]\n            \n            # Forward pass\n            h, s, y_hat = self.forward(x_seq)\n\n            # --- Backward pass (BPTT) ---\n            # Initialize gradients\n            d_Wx, d_Wh, d_b = np.zeros_like(self.Wx), np.zeros_like(self.Wh), np.zeros_like(self.b)\n            d_U, d_c = np.zeros_like(self.U), np.zeros_like(self.c)\n            \n            # Gradient of loss w.r.t. pre-sigmoid output\n            d_s = y_hat - y_seq\n            \n            d_h_next = np.zeros(self.hidden_dim)\n            \n            for t in reversed(range(T)):\n                # Output layer gradients\n                d_U += d_s[t] * h[t+1].reshape(1, -1)\n                d_c += d_s[t]\n                \n                # Propagate gradient back to hidden state\n                d_h = d_s[t] * self.U.flatten() + d_h_next\n                \n                # Propagate through tanh non-linearity\n                d_tanh = d_h * (1 - h[t+1]**2)\n                \n                # Recurrent layer gradients\n                d_b += d_tanh\n                d_Wh += np.outer(d_tanh, h[t])\n                d_Wx += np.outer(d_tanh, x_seq[t])\n                \n                # Pass gradient to previous time step\n                d_h_next = d_tanh @ self.Wh\n            \n            # Add L2 regularization gradients\n            d_Wx += 2 * self.reg_lambda * self.Wx\n            d_Wh += 2 * self.reg_lambda * self.Wh\n            d_U += 2 * self.reg_lambda * self.U\n\n            # Update parameters\n            self.Wx -= learning_rate * d_Wx\n            self.Wh -= learning_rate * d_Wh\n            self.b -= learning_rate * d_b\n            self.U -= learning_rate * d_U\n            self.c -= learning_rate * d_c\n    \n        def predict(self, x_seq):\n            _, _, y_hat = self.forward(x_seq)\n            return (y_hat >= 0.5).astype(int)\n\n    # ------------------- MAIN EXECUTION LOGIC -------------------\n\n    # 1. Prepare training data\n    train_features_list = []\n    train_labels_list = []\n    train_onehot_list = []\n    \n    for seq in TRAIN_SEQS:\n        features, labels = generate_truth(seq)\n        train_features_list.append(features)\n        train_labels_list.append(labels)\n        train_onehot_list.append(one_hot_encode(seq))\n\n    all_train_features = np.concatenate(train_features_list)\n\n    # 2. Fit and evaluate unsupervised model\n    unsup_model = KMeans1D()\n    unsup_model.fit(all_train_features)\n    \n    unsup_accuracies = {}\n    for name, seq in TEST_SUITE.items():\n        test_features, test_labels = generate_truth(seq)\n        preds = unsup_model.predict(test_features)\n        acc = np.mean(preds == test_labels) if len(test_labels) > 0 else 1.0\n        unsup_accuracies[name] = acc\n        \n    # 3. Train and evaluate supervised model\n    rnn = SimpleRNN(\n        input_dim=VOCAB_SIZE, \n        hidden_dim=D_HIDDEN, \n        output_dim=1,\n        reg_lambda=LAMBDA_REG\n    )\n\n    for epoch in range(EPOCHS):\n        indices = np.random.permutation(len(TRAIN_SEQS))\n        for i in indices:\n            x_seq = train_onehot_list[i]\n            y_seq = train_labels_list[i]\n            rnn.train_step(x_seq, y_seq, LEARNING_RATE)\n\n    sup_accuracies = {}\n    for name, seq in TEST_SUITE.items():\n        x_seq = one_hot_encode(seq)\n        _, test_labels = generate_truth(seq)\n        preds = rnn.predict(x_seq)\n        acc = np.mean(preds == test_labels) if len(test_labels) > 0 else 1.0\n        sup_accuracies[name] = acc\n\n    # 4. Assemble and print results\n    results = []\n    for name in sorted(TEST_SUITE.keys()):\n        results.append(sup_accuracies[name])\n        results.append(unsup_accuracies[name])\n    \n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\n\nsolve()\n```", "id": "2432793"}, {"introduction": "Our final practice delves into a more subtle and advanced interaction between supervised and unsupervised paradigms. Through a carefully designed simulation, you will investigate how underlying data structures, which can be uncovered by unsupervised analysis, can create misleading statistical signals that deceive supervised feature selection algorithms [@problem_id:3199415]. This exercise serves as a powerful cautionary tale, demonstrating that a feature's strong correlation with a target variable does not always imply a direct predictive relationship, a phenomenon often caused by hidden confounding variables.", "problem": "You are given the task of constructing a synthetic dataset to analyze the interaction between supervised and unsupervised learning. Use the following fundamental base: the definition of supervised linear regression with target $Y$ given by $Y = \\alpha^\\top X + \\epsilon$ where $\\epsilon$ is additive noise, and the definition of an unsupervised mixture model where the feature vector $X$ is drawn from a mixture of multivariate Gaussian components. The goal is to reason from these foundations to test whether multimodal structure in $X$ (from clustering) can mislead a supervised, univariate, correlation-based feature selection rule.\n\nDefine the data-generating process as follows. Let $X \\in \\mathbb{R}^d$ be a random vector drawn from a mixture with $K$ clusters. For a cluster index $C \\in \\{1, \\dots, K\\}$ sampled with probabilities $\\{p_1,\\dots,p_K\\}$ such that $\\sum_{k=1}^K p_k = 1$, draw\n$$\nX \\mid C = k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k),\n$$\nwhere $\\mu_k \\in \\mathbb{R}^d$ is the mean vector for cluster $k$ and $\\Sigma_k$ is a diagonal covariance matrix with diagonal entries given by per-feature within-cluster variances. Then, define the supervised regression target\n$$\nY = \\alpha^\\top X + \\epsilon,\n$$\nwith $\\alpha \\in \\mathbb{R}^d$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ independent of $X$.\n\nConsider a feature selection rule that ranks features by the absolute value of the sample Pearson correlation coefficient between each individual feature $X_j$ and the target $Y$. The Pearson correlation for feature $j$ is\n$$\n\\rho_j = \\frac{\\widehat{\\operatorname{Cov}}(X_j, Y)}{\\widehat{\\sigma}(X_j)\\,\\widehat{\\sigma}(Y)},\n$$\nwhere $\\widehat{\\operatorname{Cov}}$ is the unbiased sample covariance and $\\widehat{\\sigma}$ is the unbiased sample standard deviation. A feature is called \"predictive\" if its corresponding coefficient in $\\alpha$ is nonzero, and \"non-predictive\" otherwise.\n\nYour program must:\n1. For each test case, generate $n$ independent samples of $(X,Y)$ according to the mixture model and linear regression target described above, using a diagonal $\\Sigma_k$ with specified per-feature within-cluster standard deviations and the same diagonal for all clusters.\n2. Compute the absolute sample Pearson correlation coefficient $|\\rho_j|$ for each feature $j \\in \\{1,\\dots,d\\}$.\n3. Determine whether clustering structure can mislead correlation-based supervised feature selection, using the following quantifiable criterion for \"misled\": a test case is labeled as misled if there exists at least one non-predictive feature whose absolute correlation with $Y$ is strictly greater than the absolute correlation of at least one predictive feature. Formally, let $P = \\{j:\\alpha_j \\neq 0\\}$ and $N = \\{j:\\alpha_j = 0\\}$. The case is misled if\n$$\n\\max_{j \\in N} |\\rho_j| > \\min_{i \\in P} |\\rho_i|.\n$$\n\nUse the following test suite to ensure coverage of different scenarios:\n\n- Test Case $1$ (happy path mislead via aligned cluster means):\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, 0.1, 0.0]$,\n  - Cluster means: $\\mu_1 = [2.0, 0.0, 2.0]$, $\\mu_2 = [-2.0, 0.0, -2.0]$,\n  - Within-cluster standard deviations: per-feature $\\sigma = [0.5, 0.1, 0.5]$,\n  - Cluster probabilities: $p_1 = 0.5$, $p_2 = 0.5$,\n  - Noise standard deviation: $\\sigma_\\epsilon = 0.2$,\n  - Random seed: $42$.\n\n- Test Case $2$ (mislead under imbalanced clusters):\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, 0.1, 0.0]$,\n  - Cluster means: $\\mu_1 = [2.0, 0.0, 2.0]$, $\\mu_2 = [-2.0, 0.0, -2.0]$,\n  - Within-cluster standard deviations: per-feature $\\sigma = [0.5, 0.1, 0.5]$,\n  - Cluster probabilities: $p_1 = 0.9$, $p_2 = 0.1$,\n  - Noise standard deviation: $\\sigma_\\epsilon = 0.2$,\n  - Random seed: $123$.\n\n- Test Case $3$ (boundary case with no cluster separation):\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, 0.1, 0.0]$,\n  - Cluster means: $\\mu_1 = [0.0, 0.0, 0.0]$, $\\mu_2 = [0.0, 0.0, 0.0]$,\n  - Within-cluster standard deviations: per-feature $\\sigma = [0.5, 0.1, 0.5]$,\n  - Cluster probabilities: $p_1 = 0.5$, $p_2 = 0.5$,\n  - Noise standard deviation: $\\sigma_\\epsilon = 0.2$,\n  - Random seed: $7$.\n\n- Test Case $4$ (edge case where cluster shifts are orthogonal to the supervised signal):\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, -1.0, 0.0]$,\n  - Cluster means: $\\mu_1 = [2.0, 2.0, 2.0]$, $\\mu_2 = [-2.0, -2.0, -2.0]$,\n  - Within-cluster standard deviations: per-feature $\\sigma = [0.5, 0.5, 0.5]$,\n  - Cluster probabilities: $p_1 = 0.5$, $p_2 = 0.5$,\n  - Noise standard deviation: $\\sigma_\\epsilon = 0.2$,\n  - Random seed: $2024$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean for a test case in the order $[1,2,3,4]$. For example, the output format must be exactly like $[r_1,r_2,r_3,r_4]$ where each $r_i$ is either $True$ or $False$.\n- No physical units are involved in this problem.", "solution": "The problem requires us to investigate a scenario where feature selection based on a supervised criterion (correlation with a target variable) can be misled by an underlying unsupervised structure (clustering) in the feature space. We are asked to construct a synthetic dataset, apply a specific correlation-based feature selection rule, and determine if the rule is \"misled\" according to a precise mathematical definition.\n\nThe core of the problem lies in understanding how the statistical properties of the features $X$ and their relationship with the target $Y$ are affected by the data generating process. The feature vector $X \\in \\mathbb{R}^d$ is drawn from a Gaussian Mixture Model (GMM), and the target $Y$ is a linear function of $X$ with additive noise.\n\nLet's first analyze the theoretical relationship between a feature $X_j$ and the target $Y$. The Pearson correlation coefficient $\\rho_j$ is a normalized version of the covariance $\\operatorname{Cov}(X_j, Y)$. Let's analyze this covariance.\nGiven $Y = \\alpha^\\top X + \\epsilon = \\sum_{i=1}^d \\alpha_i X_i + \\epsilon$, the covariance is:\n$$\n\\operatorname{Cov}(X_j, Y) = \\operatorname{Cov}\\left(X_j, \\sum_{i=1}^d \\alpha_i X_i + \\epsilon\\right)\n$$\nBy linearity of covariance and the independence of $\\epsilon$ from $X$ (which implies $\\operatorname{Cov}(X_j, \\epsilon) = 0$), we have:\n$$\n\\operatorname{Cov}(X_j, Y) = \\sum_{i=1}^d \\alpha_i \\operatorname{Cov}(X_j, X_i)\n$$\nThe term $\\operatorname{Cov}(X_j, X_i)$ is the total covariance between features $j$ and $i$. The problem states that $X$ is drawn from a mixture model. We can decompose the total covariance using the law of total covariance:\n$$\n\\operatorname{Cov}(X_j, X_i) = \\mathbb{E}[\\operatorname{Cov}(X_j, X_i \\mid C)] + \\operatorname{Cov}(\\mathbb{E}[X_j \\mid C], \\mathbb{E}[X_i \\mid C])\n$$\nHere, $C$ is the random variable for the cluster assignment.\nThe first term, $\\mathbb{E}[\\operatorname{Cov}(X_j, X_i \\mid C)]$, is the average within-cluster covariance. The problem specifies that the within-cluster covariance matrices $\\Sigma_k$ are diagonal, with entries $\\sigma_{k,j}^2$. Further, all clusters share the same diagonal covariance, so $\\sigma_{k,j}^2 = \\sigma_j^2$ for all $k$. Thus, $\\operatorname{Cov}(X_j, X_i \\mid C=k) = \\delta_{ij} \\sigma_j^2$, where $\\delta_{ij}$ is the Kronecker delta. The expectation over $C$ does not change this, so $\\mathbb{E}[\\operatorname{Cov}(X_j, X_i \\mid C)] = \\delta_{ij} \\sigma_j^2$.\n\nThe second term, $\\operatorname{Cov}(\\mathbb{E}[X_j \\mid C], \\mathbb{E}[X_i \\mid C])$, is the covariance of the cluster means. Let $M_j = \\mathbb{E}[X_j \\mid C]$ be the random variable representing the mean of feature $j$, which takes the value $\\mu_{kj}$ (the $j$-th component of $\\mu_k$) with probability $p_k$. The covariance of these means introduces off-diagonal terms into the total covariance matrix of $X$.\n\nSubstituting this back into our expression for $\\operatorname{Cov}(X_j, Y)$:\n$$\n\\operatorname{Cov}(X_j, Y) = \\sum_{i=1}^d \\alpha_i \\left( \\delta_{ij} \\sigma_j^2 + \\operatorname{Cov}(M_j, M_i) \\right) = \\alpha_j \\sigma_j^2 + \\sum_{i=1}^d \\alpha_i \\operatorname{Cov}(M_j, M_i)\n$$\nThis equation is key. It shows that the covariance between feature $X_j$ and target $Y$ has two components:\n1.  A direct component, $\\alpha_j \\sigma_j^2$, proportional to the feature's own coefficient $\\alpha_j$. This term is zero for non-predictive features.\n2.  An indirect component, $\\sum_{i=1}^d \\alpha_i \\operatorname{Cov}(M_j, M_i)$, which arises from the clustering structure. This component exists if the mean of feature $j$ covaries with the means of any predictive features across clusters.\n\nA feature selection rule is \"misled\" if a non-predictive feature (let's say feature $j$, where $\\alpha_j=0$) has a higher correlation with $Y$ than a genuinely predictive feature (feature $i$, where $\\alpha_i \\neq 0$). For a non-predictive feature $j$, the covariance simplifies to:\n$$\n\\operatorname{Cov}(X_j, Y) = \\sum_{i=1, \\alpha_i \\neq 0}^d \\alpha_i \\operatorname{Cov}(M_j, M_i) \\quad (\\text{since } \\alpha_j=0)\n$$\nThis covariance can be large and non-zero if the means of the non-predictive feature $j$ are strongly correlated with the means of a powerful predictive feature. This creates a spurious correlation between $X_j$ and $Y$ that is not due to a direct causal link (as defined by the linear model $Y = \\alpha^\\top X + \\epsilon$), but is instead mediated by the shared cluster structure. If this spurious correlation is stronger than the correlation of a weakly predictive feature, the selection rule will be misled.\n\nThe algorithmic procedure to test this is as follows:\n1.  For each test case, initialize a random number generator with the specified seed for reproducibility.\n2.  Generate $n$ samples. This involves two steps:\n    a. First, for each sample, draw a cluster assignment $k \\in \\{1, \\dots, K\\}$ from the categorical distribution defined by probabilities $\\{p_1, \\dots, p_K\\}$.\n    b. Second, for each sample assigned to cluster $k$, draw its feature vector $X$ from the multivariate normal distribution $\\mathcal{N}(\\mu_k, \\Sigma)$, where $\\Sigma$ is a diagonal matrix with the given per-feature variances $\\sigma_j^2$.\n3.  Generate the target variable $Y$. For each sample's feature vector $X^{(i)}$, compute $Y^{(i)} = \\alpha^\\top X^{(i)} + \\epsilon^{(i)}$, where $\\epsilon^{(i)}$ is drawn from $\\mathcal{N}(0, \\sigma_\\epsilon^2)$.\n4.  With the complete dataset $(X, Y)$ of $n$ samples, compute the sample Pearson correlation coefficient $\\rho_j$ between each feature column $X_j$ and the target vector $Y$. This is conventionally done using a function that computes the correlation matrix based on unbiased sample covariances and standard deviations.\n5.  Identify the set of predictive features $P = \\{j:\\alpha_j \\neq 0\\}$ and non-predictive features $N = \\{j:\\alpha_j = 0\\}$.\n6.  Calculate the maximum absolute correlation among non-predictive features, $\\max_{j \\in N} |\\rho_j|$, and the minimum absolute correlation among predictive features, $\\min_{i \\in P} |\\rho_i|$.\n7.  Check if the \"misled\" condition, $\\max_{j \\in N} |\\rho_j| > \\min_{i \\in P} |\\rho_i|$, is met. The result is a boolean value.\n\nThis procedure is repeated for all test cases, and the boolean results are collected. The test cases are designed to probe this mechanism:\n-   **Case 1**: Aligned cluster means should cause the non-predictive feature $X_2$ to be highly correlated with the strongly predictive feature $X_0$, leading to a large spurious correlation with $Y$. This is expected to mislead the selection rule.\n-   **Case 2**: Imbalanced clusters will reduce, but not eliminate, the covariance between cluster means, likely still resulting in a \"misled\" outcome.\n-   **Case 3**: No cluster separation ($\\mu_1 = \\mu_2$) means $\\operatorname{Cov}(M_j, M_i) = 0$ for all $j, i$. The indirect covariance term vanishes, and correlations will correctly reflect the direct influence $\\alpha_j$, so no misleading is expected.\n-   **Case 4**: The cluster mean shifts are constructed to be orthogonal to the signal vector $\\alpha$. Specifically, $\\sum_i \\alpha_i \\operatorname{Cov}(M_j, M_i)$ evaluates to zero. This demonstrates a specific configuration where clustering exists but does not induce a spurious correlation, hence no misleading is expected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates synthetic datasets and determines if a correlation-based\n    feature selection rule is misled by clustering structure.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, 0.1, 0.0]),\n            \"means\": np.array([[2.0, 0.0, 2.0], [-2.0, 0.0, -2.0]]),\n            \"stds\": np.array([0.5, 0.1, 0.5]),\n            \"cluster_probs\": np.array([0.5, 0.5]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 42\n        },\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, 0.1, 0.0]),\n            \"means\": np.array([[2.0, 0.0, 2.0], [-2.0, 0.0, -2.0]]),\n            \"stds\": np.array([0.5, 0.1, 0.5]),\n            \"cluster_probs\": np.array([0.9, 0.1]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 123\n        },\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, 0.1, 0.0]),\n            \"means\": np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            \"stds\": np.array([0.5, 0.1, 0.5]),\n            \"cluster_probs\": np.array([0.5, 0.5]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 7\n        },\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, -1.0, 0.0]),\n            \"means\": np.array([[2.0, 2.0, 2.0], [-2.0, -2.0, -2.0]]),\n            \"stds\": np.array([0.5, 0.5, 0.5]),\n            \"cluster_probs\": np.array([0.5, 0.5]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 2024\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        n, d, K = case[\"n\"], case[\"d\"], case[\"K\"]\n        alpha = case[\"alpha\"]\n        means = case[\"means\"]\n        stds = case[\"stds\"]\n        cluster_probs = case[\"cluster_probs\"]\n        sigma_epsilon = case[\"sigma_epsilon\"]\n        seed = case[\"seed\"]\n\n        # 1. Generate dataset\n        rng = np.random.default_rng(seed)\n\n        # Generate cluster assignments for n samples\n        cluster_indices = np.arange(K)\n        assignments = rng.choice(cluster_indices, size=n, p=cluster_probs)\n\n        # Generate X based on cluster assignments\n        X = np.zeros((n, d))\n        for i in range(n):\n            cluster_idx = assignments[i]\n            mean_vec = means[cluster_idx]\n            # Draw sample from multivariate normal with diagonal covariance\n            X[i, :] = rng.normal(loc=mean_vec, scale=stds)\n            \n        # Generate Y\n        epsilon = rng.normal(loc=0, scale=sigma_epsilon, size=n)\n        Y = X @ alpha + epsilon\n\n        # 2. Compute absolute sample Pearson correlations\n        correlations = []\n        for j in range(d):\n            # np.corrcoef returns a 2x2 matrix. The off-diagonal is the correlation.\n            corr_matrix = np.corrcoef(X[:, j], Y)\n            correlations.append(corr_matrix[0, 1])\n        \n        abs_correlations = np.abs(np.array(correlations))\n        \n        # 3. Determine if the case is \"misled\"\n        predictive_indices = np.where(alpha != 0)[0]\n        non_predictive_indices = np.where(alpha == 0)[0]\n\n        is_misled = False\n        # The condition requires at least one of each type of feature to exist.\n        if predictive_indices.size > 0 and non_predictive_indices.size > 0:\n            min_corr_predictive = np.min(abs_correlations[predictive_indices])\n            max_corr_non_predictive = np.max(abs_correlations[non_predictive_indices])\n            \n            if max_corr_non_predictive > min_corr_predictive:\n                is_misled = True\n        \n        results.append(is_misled)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3199415"}]}