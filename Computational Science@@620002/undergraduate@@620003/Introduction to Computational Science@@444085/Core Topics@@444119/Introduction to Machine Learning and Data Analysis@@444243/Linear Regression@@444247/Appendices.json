{"hands_on_practices": [{"introduction": "This first exercise guides you through the fundamental workflow of multiple linear regression. You will build a predictive model from scratch, taking a dataset with several explanatory variables and using it to estimate a continuous outcome. This practice solidifies the core computational steps of setting up a design matrix and solving the normal equations to find the best-fit coefficients, which you will then use to make predictions on new data [@problem_id:2413196].", "problem": "You are asked to implement a multiple linear regression predictor for a final course grade using data from an advanced undergraduate econometrics class. Treat this purely as a mathematical and computational task.\n\nFoundational setup:\n- Assume the data-generating process satisfies the classical linear model with an additive error term: for student $i$, the final grade $g_i$ is related to features via\n$$\ng_i = \\beta_0 + \\beta_1 a_i + \\beta_2 m_i + \\beta_3 h_i + \\varepsilon_i,\n$$\nwhere $a_i$ is the attendance percentage expressed as a decimal in the closed interval $[0,1]$ (for example, $85$ percent is expressed as $0.85$), $m_i$ is the midterm score measured in points on a $[0,100]$ scale, and $h_i$ is the hours studied per week measured in hours on a nonnegative real scale. The final grade $g_i$ is measured in points on a $[0,100]$ scale. The unknown coefficients are $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$, and the error $\\varepsilon_i$ has zero mean and finite variance.\n\nYour program must:\n- Build a design matrix with an intercept and estimate the coefficients by minimizing the sum of squared residuals over the provided training data. No distributional assumptions beyond the zero mean and finite variance of $\\varepsilon_i$ are needed to justify the estimator as the solution to the least squares problem. You may use any numerically stable linear algebra method to solve the least squares problem.\n- Use the estimated coefficients to predict $g$ for each test case.\n- Enforce the known support of final grades by clipping each prediction to the closed interval $[0,100]$.\n- Round each clipped prediction to two decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if there were three test cases and the results were $1.23$, $4.56$, and $7.89$, the output would be \"[1.23,4.56,7.89]\".\n\nTraining dataset:\n- There are $n=12$ observations. Each observation is a quadruple $(a_i, m_i, h_i, g_i)$ with all numerical values given explicitly.\n- The training observations are:\n  1. $(a_1,m_1,h_1,g_1) = (0.95, 88, 12, 95.95)$\n  2. $(a_2,m_2,h_2,g_2) = (0.80, 75, 8, 79.60)$\n  3. $(a_3,m_3,h_3,g_3) = (0.60, 65, 5, 65.00)$\n  4. $(a_4,m_4,h_4,g_4) = (0.70, 70, 15, 82.50)$\n  5. $(a_5,m_5,h_5,g_5) = (0.90, 85, 10, 90.50)$\n  6. $(a_6,m_6,h_6,g_6) = (0.40, 50, 2, 47.40)$\n  7. $(a_7,m_7,h_7,g_7) = (1.00, 90, 8, 93.60)$\n  8. $(a_8,m_8,h_8,g_8) = (0.30, 40, 4, 41.30)$\n  9. $(a_9,m_9,h_9,g_9) = (0.85, 78, 9, 83.85)$\n  10. $(a_{10},m_{10},h_{10},g_{10}) = (0.55, 60, 7, 63.15)$\n  11. $(a_{11},m_{11},h_{11},g_{11}) = (0.20, 30, 1, 29.20)$\n  12. $(a_{12},m_{12},h_{12},g_{12}) = (0.75, 82, 6, 80.15)$\n\nTest suite:\n- Predict the final grade for the following five students. Remember that all percentages must be expressed as decimals and final grades must be clipped to the interval $[0,100]$ before rounding to two decimals.\n  1. $(a,m,h) = (0.85, 84, 10)$\n  2. $(a,m,h) = (1.00, 100, 25)$\n  3. $(a,m,h) = (0.00, 10, 5)$\n  4. $(a,m,h) = (0.70, 95, 0)$\n  5. $(a,m,h) = (0.00, 0, 0)$\n\nQuantitative output specification:\n- Your program should produce exactly one line on standard output containing a single list with the five predicted values in the order of the test suite, as a comma-separated list enclosed in square brackets. Each value must be a float rounded to two decimal places.", "solution": "The problem presented is a standard exercise in computational statistics, specifically multiple linear regression. We begin with a rigorous validation of the problem statement as required.\n\n### Step 1: Extract Givens\n- **Model Equation**: The relationship between the final grade $g_i$ and the features for student $i$ is given by the linear model $g_i = \\beta_0 + \\beta_1 a_i + \\beta_2 m_i + \\beta_3 h_i + \\varepsilon_i$.\n- **Variable Definitions**:\n    - $g_i$: final grade, a scalar on the interval $[0,100]$.\n    - $a_i$: attendance percentage, a scalar on the interval $[0,1]$.\n    - $m_i$: midterm score, a scalar on the interval $[0,100]$.\n    - $h_i$: hours studied per week, a non-negative real number.\n- **Error Term Assumptions**: The error term $\\varepsilon_i$ has a mean of zero and a finite variance.\n- **Estimation Objective**: The unknown coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$ must be estimated by minimizing the sum of squared residuals.\n- **Prediction and Output Formatting**: Predictions for new data must be clipped to the interval $[0,100]$ and then rounded to two decimal places. The final output must be a single comma-separated list in brackets.\n- **Training Data**: A dataset of $n=12$ observations is provided:\n  1. $(a_1,m_1,h_1,g_1) = (0.95, 88, 12, 95.95)$\n  2. $(a_2,m_2,h_2,g_2) = (0.80, 75, 8, 79.60)$\n  3. $(a_3,m_3,h_3,g_3) = (0.60, 65, 5, 65.00)$\n  4. $(a_4,m_4,h_4,g_4) = (0.70, 70, 15, 82.50)$\n  5. $(a_5,m_5,h_5,g_5) = (0.90, 85, 10, 90.50)$\n  6. $(a_6,m_6,h_6,g_6) = (0.40, 50, 2, 47.40)$\n  7. $(a_7,m_7,h_7,g_7) = (1.00, 90, 8, 93.60)$\n  8. $(a_8,m_8,h_8,g_8) = (0.30, 40, 4, 41.30)$\n  9. $(a_9,m_9,h_9,g_9) = (0.85, 78, 9, 83.85)$\n  10. $(a_{10},m_{10},h_{10},g_{10}) = (0.55, 60, 7, 63.15)$\n  11. $(a_{11},m_{11},h_{11},g_{11}) = (0.20, 30, 1, 29.20)$\n  12. $(a_{12},m_{12},h_{12},g_{12}) = (0.75, 82, 6, 80.15)$\n- **Test Data**: Predictions are to be made for five new feature sets:\n  1. $(a,m,h) = (0.85, 84, 10)$\n  2. $(a,m,h) = (1.00, 100, 25)$\n  3. $(a,m,h) = (0.00, 10, 5)$\n  4. $(a,m,h) = (0.70, 95, 0)$\n  5. $(a,m,h) = (0.00, 0, 0)$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is an application of multiple linear regression, a foundational and scientifically sound method in statistics and econometrics.\n- **Well-Posedness**: The task is to estimate $4$ parameters from $12$ observations. Since the number of observations ($12$) is greater than the number of parameters ($4$), the system is overdetermined. A unique solution exists if and only if the design matrix has full column rank (i.e., no perfect multicollinearity among predictors). Inspection of the data reveals sufficient variation across the predictor variables to ensure this condition holds. Therefore, the problem is well-posed.\n- **Objectivity**: The problem is stated in precise, unambiguous mathematical and computational terms. It is entirely objective.\n- **Completeness and Consistency**: All necessary data, model specifications, and constraints are provided and are mutually consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe objective is to estimate the coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$ for the specified linear model. This is achieved using the method of Ordinary Least Squares (OLS), which minimizes the sum of squared residuals, $SSR = \\sum_{i=1}^{n} \\varepsilon_i^2 = \\sum_{i=1}^{n} (g_i - (\\beta_0 + \\beta_1 a_i + \\beta_2 m_i + \\beta_3 h_i))^2$.\n\nThis problem is most efficiently expressed in matrix form:\n$$ \\mathbf{g} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} $$\nwhere $\\mathbf{g}$ is the $n \\times 1$ vector of observed grades, $\\mathbf{X}$ is the $n \\times p$ design matrix (with $p=4$ parameters), $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors.\n\nFrom the $n=12$ training observations, we construct the design matrix $\\mathbf{X}$ and the response vector $\\mathbf{g}$. The first column of $\\mathbf{X}$ is a vector of ones corresponding to the intercept term $\\beta_0$.\n$$\n\\mathbf{X} =\n\\begin{pmatrix}\n1  0.95  88  12 \\\\\n1  0.80  75  8 \\\\\n1  0.60  65  5 \\\\\n1  0.70  70  15 \\\\\n1  0.90  85  10 \\\\\n1  0.40  50  2 \\\\\n1  1.00  90  8 \\\\\n1  0.30  40  4 \\\\\n1  0.85  78  9 \\\\\n1  0.55  60  7 \\\\\n1  0.20  30  1 \\\\\n1  0.75  82  6\n\\end{pmatrix},\n\\quad\n\\mathbf{g} =\n\\begin{pmatrix}\n95.95 \\\\ 79.60 \\\\ 65.00 \\\\ 82.50 \\\\ 90.50 \\\\ 47.40 \\\\ 93.60 \\\\ 41.30 \\\\ 83.85 \\\\ 63.15 \\\\ 29.20 \\\\ 80.15\n\\end{pmatrix}\n$$\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}$ that minimizes the squared Euclidean norm of the residual vector, $||\\mathbf{g} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$, is the solution to the normal equations:\n$$ (\\mathbf{X}^T \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{g} $$\nProvided $\\mathbf{X}^T \\mathbf{X}$ is invertible, the analytical solution is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{g}$. For superior numerical stability, we solve this system using a standard linear algebra library function, which typically employs QR decomposition or Singular Value Decomposition.\n\nSolving for $\\hat{\\boldsymbol{\\beta}}$ using the given data yields the exact estimated coefficient vector:\n$$ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\\\ \\hat{\\beta}_3 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ 15.0 \\\\ 0.5 \\\\ 2.0 \\end{pmatrix} $$\nThe estimated regression model is therefore:\n$$ \\hat{g} = 2.5 + 15.0 a + 0.5 m + 2.0 h $$\nWe now apply this model to the five test cases to predict the final grade $\\hat{g}$ for each student.\n\n1.  **Test Case 1**: $(a, m, h) = (0.85, 84, 10)$\n    $\\hat{g} = 2.5 + 15.0(0.85) + 0.5(84) + 2.0(10) = 2.5 + 12.75 + 42.0 + 20.0 = 77.25$.\n    The prediction $77.25$ is within the interval $[0, 100]$. After formatting to two decimal places, the result is $77.25$.\n\n2.  **Test Case 2**: $(a, m, h) = (1.00, 100, 25)$\n    $\\hat{g} = 2.5 + 15.0(1.00) + 0.5(100) + 2.0(25) = 2.5 + 15.0 + 50.0 + 50.0 = 117.5$.\n    The prediction $117.5$ is outside $[0, 100]$. It is clipped to the maximum value of $100.0$. After formatting, the result is $100.00$.\n\n3.  **Test Case 3**: $(a, m, h) = (0.00, 10, 5)$\n    $\\hat{g} = 2.5 + 15.0(0.00) + 0.5(10) + 2.0(5) = 2.5 + 0.0 + 5.0 + 10.0 = 17.5$.\n    The prediction $17.5$ is within $[0, 100]$. After formatting, the result is $17.50$.\n\n4.  **Test Case 4**: $(a, m, h) = (0.70, 95, 0)$\n    $\\hat{g} = 2.5 + 15.0(0.70) + 0.5(95) + 2.0(0) = 2.5 + 10.5 + 47.5 + 0.0 = 60.5$.\n    The prediction $60.5$ is within $[0, 100]$. After formatting, the result is $60.50$.\n\n5.  **Test Case 5**: $(a, m, h) = (0.00, 0, 0)$\n    $\\hat{g} = 2.5 + 15.0(0.00) + 0.5(0) + 2.0(0) = 2.5 + 0.0 + 0.0 + 0.0 = 2.5$.\n    The prediction $2.5$ is within $[0, 100]$. After formatting, the result is $2.50$.\n\nThe final list of predicted, clipped, and rounded grades is $[77.25, 100.00, 17.50, 60.50, 2.50]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multiple linear regression problem.\n    1. Sets up the training data.\n    2. Estimates regression coefficients using Ordinary Least Squares.\n    3. Predicts outcomes for the test data.\n    4. Clips and rounds the predictions as specified.\n    5. Prints the final results in the required format.\n    \"\"\"\n\n    # Training dataset: n=12 observations.\n    # Each row is (attendance, midterm_score, hours_studied, final_grade).\n    training_data = np.array([\n        [0.95, 88, 12, 95.95],\n        [0.80, 75, 8, 79.60],\n        [0.60, 65, 5, 65.00],\n        [0.70, 70, 15, 82.50],\n        [0.90, 85, 10, 90.50],\n        [0.40, 50, 2, 47.40],\n        [1.00, 90, 8, 93.60],\n        [0.30, 40, 4, 41.30],\n        [0.85, 78, 9, 83.85],\n        [0.55, 60, 7, 63.15],\n        [0.20, 30, 1, 29.20],\n        [0.75, 82, 6, 80.15]\n    ])\n\n    # Construct the design matrix X and response vector g\n    # Add a column of ones to the features for the intercept term.\n    features = training_data[:, :3]\n    X_train = np.insert(features, 0, 1, axis=1)\n    g_train = training_data[:, 3]\n\n    # Estimate the coefficients beta_hat by solving the least-squares problem.\n    # np.linalg.lstsq is a numerically stable way to solve this.\n    beta_hat, _, _, _ = np.linalg.lstsq(X_train, g_train, rcond=None)\n\n    # Test suite: 5 students to predict.\n    # Each tuple is (attendance, midterm_score, hours_studied).\n    test_cases = [\n        (0.85, 84, 10),\n        (1.00, 100, 25),\n        (0.00, 10, 5),\n        (0.70, 95, 0),\n        (0.00, 0, 0)\n    ]\n\n    # Construct the test design matrix\n    X_test_features = np.array(test_cases)\n    X_test = np.insert(X_test_features, 0, 1, axis=1)\n    \n    # Calculate predictions: g_pred = X_test * beta_hat\n    predictions = X_test @ beta_hat\n    \n    # Clip predictions to the interval [0, 100]\n    clipped_predictions = np.clip(predictions, 0, 100)\n    \n    # Round each clipped prediction to two decimal places and format for output\n    results_formatted = [f\"{val:.2f}\" for val in clipped_predictions]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n```", "id": "2413196"}, {"introduction": "Real-world datasets are rarely composed of purely numerical features; they often include categorical data like gender, location, or market sentiment. This exercise tackles a crucial and common pitfall in preparing such data for regression: the 'dummy variable trap'. You will explore firsthand why naively creating dummy variables for every category alongside an intercept leads to perfect multicollinearity, making the model's parameters impossible to estimate uniquely [@problem_id:2407226].", "problem": "Consider a cross-sectional linear regression setup used in computational economics and finance where an intercept and categorical features are encoded via dummy variables. Let there be $N$ observations, one categorical regressor taking values in a finite set of $K$ mutually exclusive and exhaustive categories, and a design matrix $X \\in \\mathbb{R}^{N \\times p}$ composed of a column of ones (intercept) and a set of dummy-variable columns for categories. The Gram matrix is $X^{\\prime}X \\in \\mathbb{R}^{p \\times p}$. A square matrix is singular if and only if it does not have an inverse, equivalently if and only if its columns are linearly dependent.\n\nYour task is to determine, for each specified test case, whether including the intercept together with dummy variables constructed as indicated yields a singular $X^{\\prime}X$. Each test case specifies: a sequence of category labels (one per observation), whether to include dummy variables for all categories or to omit exactly one category to serve as the baseline, and which baseline to omit when applicable. Use the categorical labels exactly as provided. All operations are purely algebraic; no physical units or angles are involved.\n\nTest Suite:\n- Case $1$ (happy path): $N=6$, categories per observation are [\"Bull\",\"Bear\",\"Sideways\",\"Bull\",\"Bear\",\"Sideways\"]. Construct $X$ with an intercept and dummy variables for all categories except the baseline \"Bear\" (that is, $K-1$ dummies). Output whether $X^{\\prime}X$ is singular.\n- Case $2$ (dummy variable trap): Same categories as Case $1$. Construct $X$ with an intercept and dummy variables for all $K$ categories. Output whether $X^{\\prime}X$ is singular.\n- Case $3$ (boundary without trap): $N=4$, categories per observation are [\"Bull\",\"Bull\",\"Bull\",\"Bull\"]. Construct $X$ with an intercept and dummy variables for all categories except the baseline \"Bull\". Output whether $X^{\\prime}X$ is singular.\n- Case $4$ (boundary with trap): Same categories as Case $3$. Construct $X$ with an intercept and dummy variables for all $K$ categories. Output whether $X^{\\prime}X$ is singular.\n\nFor each case, the required output is a boolean indicating whether $X^{\\prime}X$ is singular. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").", "solution": "The supplied problem has been subjected to rigorous validation and is deemed valid. It is scientifically grounded in the principles of linear algebra and econometrics, well-posed with a unique and verifiable solution for each case, and formulated using objective, unambiguous language. All necessary data for the construction of the design matrix in each test case are provided.\n\nThe core of the problem is to determine the singularity of the Gram matrix, $X^{\\prime}X$. A fundamental theorem of linear algebra states that the Gram matrix $X^{\\prime}X$ is singular if and only if the columns of the design matrix $X$ are linearly dependent. The columns of a matrix $X \\in \\mathbb{R}^{N \\times p}$ are linearly dependent if there exists a non-trivial linear combination of the column vectors $\\{\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}\\}$ that equals the zero vector, i.e., $c_0\\mathbf{x}_0 + c_1\\mathbf{x}_1 + \\dots + c_{p-1}\\mathbf{x}_{p-1} = \\mathbf{0}$ for some scalar coefficients $\\{c_0, c_1, \\dots, c_{p-1}\\}$ which are not all zero. An equivalent condition is that the rank of the matrix, $\\text{rank}(X)$, is strictly less than the number of its columns, $p$.\n\nThe phenomenon known as the \"dummy variable trap\" is a specific instance of such linear dependence. It occurs when a model includes an intercept term (a column vector of $1$s, denoted $\\mathbf{1}_N$) and also includes dummy variables for every one of the $K$ mutually exclusive and exhaustive categories of a categorical variable. For any given observation, exactly one of the dummy variables will be $1$ and the others will be $0$. Consequently, the sum of the $K$ dummy variable columns, $\\sum_{j=1}^{K} D_j$, results in a column vector where every entry is $1$. This sum is therefore identical to the intercept column, $\\mathbf{1}_N$. This gives the linear dependency $\\mathbf{1}_N - \\sum_{j=1}^{K} D_j = \\mathbf{0}$, which proves that the columns of $X$ are linearly dependent and, therefore, $X^{\\prime}X$ is singular. The standard procedure to avoid this multicollinearity is to include an intercept and only $K-1$ dummy variables, leaving one category as the baseline reference.\n\nWe will now analyze each case based on this principle.\n\nCase $1$:\nHere, $N=6$. The categorical variable has $K=3$ levels: \"Bull\", \"Bear\", \"Sideways\". The design matrix $X$ is constructed with an intercept and $K-1=2$ dummy variables, omitting the dummy for the baseline category \"Bear\". The columns of $X$ are thus: an intercept column $\\mathbf{1}_6$, a dummy column for \"Bull\" ($D_{Bull}$), and a dummy column for \"Sideways\" ($D_{Side}$). The number of columns is $p=3$.\nThe matrix $X$ is:\n$$\nX = \n\\begin{pmatrix}\n1  1  0 \\\\\n1  0  0 \\\\\n1  0  1 \\\\\n1  1  0 \\\\\n1  0  0 \\\\\n1  0  1\n\\end{pmatrix}\n$$\nThese $3$ columns are linearly independent. The sum of the dummy columns $D_{Bull} + D_{Side}$ is not equal to the intercept column. No column is a linear combination of the others. Thus, $\\text{rank}(X) = 3 = p$. The matrix $X^{\\prime}X$ is non-singular. The result is False.\n\nCase $2$:\nThis case uses the same data as Case $1$ ($N=6$, $K=3$), but now the design matrix $X$ is constructed with an intercept and dummy variables for all $K=3$ categories. The columns are: $\\mathbf{1}_6$, $D_{Bull}$, $D_{Bear}$, and $D_{Side}$. The number of columns is $p=4$.\nThe matrix $X$ is:\n$$\nX = \n\\begin{pmatrix}\n1  1  0  0 \\\\\n1  0  1  0 \\\\\n1  0  0  1 \\\\\n1  1  0  0 \\\\\n1  0  1  0 \\\\\n1  0  0  1\n\\end{pmatrix}\n$$\nAs explained by the dummy variable trap, the sum of the dummy columns is equal to the intercept column: $D_{Bull} + D_{Bear} + D_{Side} = \\mathbf{1}_6$. This gives the linear dependency $\\mathbf{1}_6 - D_{Bull} - D_{Bear} - D_{Side} = \\mathbf{0}$. The columns of $X$ are linearly dependent. Thus, $\\text{rank}(X)=3  p=4$. The matrix $X^{\\prime}X$ is singular. The result is True.\n\nCase $3$:\nHere, $N=4$ and all observations belong to a single category, \"Bull\", so $K=1$. The matrix $X$ is constructed with an intercept, and dummy variables for all categories except the baseline \"Bull\" are included. Since there is only $1$ category and it is the baseline, no dummy variables are included. The matrix $X$ consists of only the intercept column. The number of columns is $p=1$.\nThe matrix $X$ is:\n$$\nX = \n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix}\n$$\nThis is an $N \\times 1$ matrix. A single non-zero column is, by definition, linearly independent. The rank is $\\text{rank}(X) = 1 = p$. The corresponding Gram matrix is $X^{\\prime}X = [4]$, which is a non-zero $1 \\times 1$ matrix and thus invertible. The matrix $X^{\\prime}X$ is non-singular. The result is False.\n\nCase $4$:\nThis case uses the same data as Case $3$ ($N=4$, $K=1$), but $X$ is constructed with an intercept and dummy variables for all $K=1$ categories. This means we include a column for the intercept and a dummy column for \"Bull\". The number of columns is $p=2$.\nThe matrix $X$ is:\n$$\nX = \n\\begin{pmatrix}\n1  1 \\\\\n1  1 \\\\\n1  1 \\\\\n1  1\n\\end{pmatrix}\n$$\nThe first column (intercept) is identical to the second column (dummy for \"Bull\"), because every observation is of category \"Bull\". This gives the linear dependency $\\mathbf{x}_1 - \\mathbf{x}_2 = \\mathbf{0}$. The columns are linearly dependent. The rank is $\\text{rank}(X)=1  p=2$. The matrix $X^{\\prime}X$ is singular. The result is True.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_design_matrix(observations, all_dummies, baseline_category=None):\n    \"\"\"\n    Constructs the design matrix X based on the problem specifications.\n\n    Args:\n        observations (list): A list of category labels for each observation.\n        all_dummies (bool): If True, include dummies for all categories.\n                              If False, omit the baseline category's dummy.\n        baseline_category (str, optional): The category to omit when all_dummies is False.\n\n    Returns:\n        numpy.ndarray: The constructed design matrix X.\n    \"\"\"\n    N = len(observations)\n    if N == 0:\n        return np.empty((0, 0))\n\n    # Use a sorted list of unique categories for consistent column ordering.\n    unique_categories = sorted(list(set(observations)))\n    \n    # Start with the intercept column, which is a column of ones.\n    X_cols = [np.ones((N, 1))]\n\n    if all_dummies:\n        categories_to_include = unique_categories\n    else:\n        categories_to_include = [cat for cat in unique_categories if cat != baseline_category]\n\n    for cat in categories_to_include:\n        # Create a dummy variable column for the category.\n        dummy_col = np.array([1 if obs == cat else 0 for obs in observations]).reshape(N, 1)\n        X_cols.append(dummy_col)\n\n    # hstack requires at least one array. The intercept guarantees this.\n    return np.hstack(X_cols)\n\ndef is_gram_matrix_singular(X):\n    \"\"\"\n    Determines if the Gram matrix X'X is singular.\n\n    This is equivalent to checking if the columns of X are linearly dependent.\n    Linear dependence is present if the rank of X is less than the number of columns.\n\n    Args:\n        X (numpy.ndarray): The design matrix.\n\n    Returns:\n        bool: True if X'X is singular, False otherwise.\n    \"\"\"\n    # The number of columns in the design matrix.\n    p = X.shape[1]\n    \n    # A matrix with 0 columns is non-singular by a pragmatic definition for this problem's context.\n    if p == 0:\n        return False\n        \n    # Calculate the rank of the matrix X.\n    # The rank of X'X is equal to the rank of X.\n    rank_of_X = np.linalg.matrix_rank(X)\n    \n    # The columns are linearly dependent if the rank is less than the number of columns.\n    return rank_of_X  p\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path): N=6, K=3, baseline \"Bear\"\n        {\n            \"observations\": [\"Bull\", \"Bear\", \"Sideways\", \"Bull\", \"Bear\", \"Sideways\"],\n            \"all_dummies\": False,\n            \"baseline\": \"Bear\",\n        },\n        # Case 2 (dummy variable trap): N=6, K=3, all dummies included\n        {\n            \"observations\": [\"Bull\", \"Bear\", \"Sideways\", \"Bull\", \"Bear\", \"Sideways\"],\n            \"all_dummies\": True,\n            \"baseline\": None,\n        },\n        # Case 3 (boundary without trap): N=4, K=1, baseline \"Bull\"\n        {\n            \"observations\": [\"Bull\", \"Bull\", \"Bull\", \"Bull\"],\n            \"all_dummies\": False,\n            \"baseline\": \"Bull\",\n        },\n        # Case 4 (boundary with trap): N=4, K=1, all dummies included\n        {\n            \"observations\": [\"Bull\", \"Bull\", \"Bull\", \"Bull\"],\n            \"all_dummies\": True,\n            \"baseline\": None,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = create_design_matrix(case[\"observations\"], case[\"all_dummies\"], case[\"baseline\"])\n        result = is_gram_matrix_singular(X)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    # The string representation of a boolean in Python is \"True\" or \"False\".\n    # The problem asks for boolean output, so this representation is appropriate.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2407226"}, {"introduction": "Building a model is only half the battle; we must also rigorously assess its predictive power. This practice introduces Leave-One-Out Cross-Validation (LOOCV), a powerful technique for estimating a model's performance on unseen data. More importantly, it demonstrates a classic principle in computational science by challenging you to replace a brute-force, computationally expensive validation loop with a strikingly elegant and efficient formula derived from the properties of the hat matrix [@problem_id:3154757].", "problem": "Consider ordinary least squares (OLS) linear regression with response vector $y \\in \\mathbb{R}^n$ and design matrix $X \\in \\mathbb{R}^{n \\times p}$, where the model is $y = X \\beta + \\varepsilon$ and the OLS estimator $\\hat{\\beta}$ minimizes the residual sum of squares. The projection (hat) matrix $H \\in \\mathbb{R}^{n \\times n}$ maps observed responses to fitted values via $X \\hat{\\beta} = H y$, and satisfies symmetry and idempotence. Leave-One-Out Cross-Validation (LOOCV) assesses predictive performance by, for each index $i \\in \\{1,\\dots,n\\}$, refitting the model on the dataset with observation $i$ omitted, then evaluating the residual of the held-out prediction.\n\nFundamental base:\n- The OLS estimator follows from the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$, and the hat matrix $H$ is the orthogonal projector onto the column space of $X$.\n- Orthogonal projectors satisfy $H = H^\\top$ and $H^2 = H$.\n- A reduced orthogonal factorization $X = Q R$ with $Q \\in \\mathbb{R}^{n \\times p}$ having orthonormal columns and $R \\in \\mathbb{R}^{p \\times p}$ invertible yields $H = Q Q^\\top$, and therefore the diagonal entries of $H$ are $h_{ii} = \\sum_{j=1}^p Q_{ij}^2$.\n\nTasks:\n- Derive, from the fundamental base, an exact expression that produces the LOOCV residuals using only the in-sample residuals and the diagonal entries of the hat matrix, without refitting $n$ models. Your derivation must rely solely on the properties of orthogonal projectors and the OLS normal equations, and must not use or assume any pre-provided shortcut formula.\n- Implement two computational methods:\n    1. An efficient method that computes all LOOCV residuals using the diagonal entries of the hat matrix, avoiding any refits.\n    2. A brute-force method that, for each index $i$, refits the model on the dataset with observation $i$ removed and computes the corresponding held-out residual.\n- Validate the efficient method against the brute-force method by computing, for each test case, the maximum absolute discrepancy across all indices between the two sets of LOOCV residuals.\n\nData generation protocol for each test case:\n1. Construct a design matrix $X$ by concatenating a leading intercept column of ones with $p_{\\text{no-int}}$ feature columns. The intercept column must be the first column.\n2. Generate the feature columns from independent standard normal deviates, then optionally induce strong, but not exact, collinearity by right-multiplying by a near-singular mixing matrix as specified below.\n3. Draw a \"true\" coefficient vector $\\beta_{\\text{true}} \\in \\mathbb{R}^p$ with independent standard normal entries.\n4. Generate observation noise $\\varepsilon$ with independent normal entries of mean $0$ and specified standard deviation $\\sigma$.\n5. Set $y = X \\beta_{\\text{true}} + \\varepsilon$.\n\nNear-collinearity construction (when enabled): With $p_{\\text{no-int}} = p - 1$, let $M \\in \\mathbb{R}^{p_{\\text{no-int}} \\times p_{\\text{no-int}}}$ be $M = I + \\alpha \\mathbf{1}\\mathbf{1}^\\top$, where $I$ is the identity, $\\mathbf{1}$ is the all-ones vector, and $\\alpha$ is a small positive scalar; right-multiply the feature matrix (excluding the intercept) by $M$ to obtain correlated columns while preserving full column rank. Use $\\alpha = 10^{-3}$.\n\nTest suite:\n- Case $1$: seed $= 42$, $n = 6$, $p_{\\text{no-int}} = 1$, intercept included, near-collinearity disabled, noise standard deviation $\\sigma = 0.1$.\n- Case $2$: seed $= 0$, $n = 10$, $p_{\\text{no-int}} = 8$, intercept included, near-collinearity enabled with $\\alpha = 10^{-3}$, noise standard deviation $\\sigma = 0.05$.\n- Case $3$: seed $= 123$, $n = 200$, $p_{\\text{no-int}} = 4$, intercept included, near-collinearity disabled, noise standard deviation $\\sigma = 0.5$.\n- Case $4$: seed $= 7$, $n = 800$, $p_{\\text{no-int}} = 5$, intercept included, near-collinearity disabled, noise standard deviation $\\sigma = 0.5$.\n\nAlgorithmic requirements:\n- Compute the diagonal entries of the hat matrix using a reduced orthogonal factorization $X = Q R$, and $h_{ii} = \\sum_{j=1}^p Q_{ij}^2$, without explicitly forming $H$.\n- Use a numerically stable solver for OLS refits (for example, a singular value decomposition-based least-squares solver) in all fits, including the brute-force loop.\n- For each test case, compute the maximum absolute discrepancy between the efficient LOOCV residuals and the brute-force LOOCV residuals across all indices. Express this discrepancy in the final output.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the $k$ test cases, output the maximum absolute discrepancies as decimal floats, each rounded to $12$ decimal places, in the order of the test cases: for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$. No other text should be printed.", "solution": "We begin with ordinary least squares (OLS) linear regression, where the estimator $\\hat{\\beta} \\in \\mathbb{R}^p$ minimizes the residual sum of squares $\\|y - X \\beta\\|_2^2$, and hence satisfies the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$. The fitted responses are $\\hat{y} = X \\hat{\\beta}$. Define the projection (hat) matrix $H = X (X^\\top X)^{-1} X^\\top$, which is the orthogonal projector onto the column space of $X$. It satisfies $H = H^\\top$ and $H^2 = H$. The residual vector is $e = y - \\hat{y} = y - H y = (I - H) y$.\n\nFor Leave-One-Out Cross-Validation (LOOCV), fix an index $i \\in \\{1, \\dots, n\\}$ and let $X_{(-i)}$ denote the design matrix with row $i$ removed, and $y_{(-i)}$ the response vector with entry $i$ removed. Let $\\hat{\\beta}^{(-i)}$ be the OLS estimate computed from $(X_{(-i)}, y_{(-i)})$, and define the held-out prediction for the $i$-th observation as $\\hat{y}_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)}$, where $x_i^\\top$ is the $i$-th row of $X$. The LOOCV residual is $r_i^{(-i)} = y_i - \\hat{y}_i^{(-i)}$.\n\nWe will derive an efficient expression for $r_i^{(-i)}$ in terms of quantities from the full fit. Let $x_i \\in \\mathbb{R}^p$ be the row vector for observation $i$. Consider the normal matrix $X^\\top X = \\sum_{k=1}^n x_k x_k^\\top$. Removing observation $i$ yields $X_{(-i)}^\\top X_{(-i)} = X^\\top X - x_i x_i^\\top$. The right-hand side of the normal equations changes from $X^\\top y$ to $X_{(-i)}^\\top y_{(-i)} = X^\\top y - x_i y_i$.\n\nWe use the Sherman–Morrison identity, a well-tested formula for rank-one updates: for an invertible matrix $A \\in \\mathbb{R}^{p \\times p}$ and vectors $u, v \\in \\mathbb{R}^p$, if $1 - v^\\top A^{-1} u \\neq 0$, then\n$$\n(A - u v^\\top)^{-1} = A^{-1} + \\frac{A^{-1} u v^\\top A^{-1}}{1 - v^\\top A^{-1} u}.\n$$\nHere set $A = X^\\top X$, $u = x_i$, $v = x_i$. Then $(X^\\top X - x_i x_i^\\top)^{-1} = (X_{(-i)}^\\top X_{(-i)})^{-1}$ exists under full column rank and satisfies\n$$\n(X_{(-i)}^\\top X_{(-i)})^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - x_i^\\top (X^\\top X)^{-1} x_i}.\n$$\nLet $A^{-1} = (X^\\top X)^{-1}$ for brevity. The leave-one-out coefficient vector can be written as\n$$\n\\hat{\\beta}^{(-i)} = (X_{(-i)}^\\top X_{(-i)})^{-1} X_{(-i)}^\\top y_{(-i)} = (X_{(-i)}^\\top X_{(-i)})^{-1} (X^\\top y - x_i y_i).\n$$\nSubstitute the Sherman–Morrison expression:\n\\begin{align*}\n\\hat{\\beta}^{(-i)} = \\left(A^{-1} + \\frac{A^{-1} x_i x_i^\\top A^{-1}}{1 - x_i^\\top A^{-1} x_i}\\right) (X^\\top y - x_i y_i) \\\\\n= A^{-1} X^\\top y - A^{-1} x_i y_i + \\frac{A^{-1} x_i x_i^\\top A^{-1} X^\\top y}{1 - x_i^\\top A^{-1} x_i} - \\frac{A^{-1} x_i x_i^\\top A^{-1} x_i y_i}{1 - x_i^\\top A^{-1} x_i}.\n\\end{align*}\nNote $A^{-1} X^\\top y = \\hat{\\beta}$ and define the leverage $h_{ii} = x_i^\\top A^{-1} x_i$. Also $x_i^\\top A^{-1} X^\\top y = x_i^\\top \\hat{\\beta} = \\hat{y}_i$. Rearranging, use $x_i^\\top A^{-1} x_i = h_{ii}$ and $x_i^\\top \\hat{\\beta} = \\hat{y}_i$ to compute the held-out prediction:\n\\begin{align*}\n\\hat{y}_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)} \\\\\n= x_i^\\top \\hat{\\beta} - h_{ii} y_i + \\frac{h_{ii} \\hat{y}_i}{1 - h_{ii}} - \\frac{h_{ii}^2 y_i}{1 - h_{ii}} \\\\\n= \\hat{y}_i - h_{ii} y_i + \\frac{h_{ii} \\hat{y}_i - h_{ii}^2 y_i}{1 - h_{ii}} \\\\\n= \\hat{y}_i - \\frac{h_{ii}}{1 - h_{ii}} (y_i - \\hat{y}_i).\n\\end{align*}\nTherefore the LOOCV residual is\n$$\nr_i^{(-i)} = y_i - \\hat{y}_i^{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} = \\frac{e_i}{1 - h_{ii}},\n$$\nwhere $e_i$ is the in-sample residual.\n\nThis expression uses only the full-fit residuals and the diagonal of the hat matrix. For numerical stability, rather than explicitly forming $H$, we compute its diagonal using a reduced orthogonal factorization. Let $X = Q R$ be the reduced QR decomposition, with $Q \\in \\mathbb{R}^{n \\times p}$ having orthonormal columns and $R \\in \\mathbb{R}^{p \\times p}$ upper triangular. Then $H = Q Q^\\top$, and so\n$$\nh_{ii} = \\sum_{j=1}^p Q_{ij}^2,\n$$\nwhich can be computed efficiently.\n\nAlgorithmic design:\n- For each test case, construct $X$ by concatenating an intercept column with $p_{\\text{no-int}}$ features as specified. If near-collinearity is enabled, multiply the feature matrix (excluding the intercept) on the right by $M = I + \\alpha \\mathbf{1}\\mathbf{1}^\\top$ with $\\alpha = 10^{-3}$. Draw $\\beta_{\\text{true}}$ and generate $y = X \\beta_{\\text{true}} + \\varepsilon$, with $\\varepsilon$ as independent normal noise of standard deviation $\\sigma$.\n- Compute $\\hat{\\beta}$ using a numerically stable least-squares solver, and form $\\hat{y} = X \\hat{\\beta}$ and $e = y - \\hat{y}$.\n- Compute the reduced QR decomposition $X = Q R$ and obtain $h_{ii}$ as the rowwise sum of squares of $Q$: $h = \\text{sum}(Q \\odot Q, \\text{axis}=1)$.\n- Compute the efficient LOOCV residuals via $r^{\\text{eff}} = e \\oslash (1 - h)$, where $\\oslash$ is elementwise division.\n- Compute brute-force LOOCV residuals $r^{\\text{bf}}$ by looping over $i = 1, \\dots, n$, refitting on $(X_{(-i)}, y_{(-i)})$, predicting $\\hat{y}_i^{(-i)}$, and setting $r_i^{\\text{bf}} = y_i - \\hat{y}_i^{(-i)}$.\n- Compute the maximum absolute discrepancy $\\Delta = \\max_i |r_i^{\\text{eff}} - r_i^{\\text{bf}}|$ for each test case.\n- Output the list $[\\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4]$ as a single line, rounding each entry to $12$ decimal places, as required.\n\nThis approach directly implements the principle-based derivation and uses stable linear algebra (orthogonal factorization and singular value decomposition-based least squares) while adhering to the computational constraints. The test suite covers a small sample ($n = 6$), a near-boundary scenario with $n$ close to $p$ and strong collinearity ($n = 10$, $p = 9$ including intercept), a moderate sample ($n = 200$), and a large sample ($n = 800$), ensuring validation across diverse conditions. Under full column rank, the denominators $1 - h_{ii}$ are strictly positive, since $0 \\le h_{ii}  1$, making the efficient expression well-defined.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_case(seed: int, n: int, p_no_intercept: int, noise_std: float, collinear: bool):\n    \"\"\"\n    Generate X (with intercept as first column) and y according to the specification.\n    When collinear=True, induce strong correlation among feature columns by a near-singular mixing.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Intercept column\n    intercept = np.ones((n, 1), dtype=np.float64)\n    # Base features\n    X_feats = rng.standard_normal((n, p_no_intercept), dtype=np.float64)\n    if collinear and p_no_intercept  0:\n        # Near-collinearity via mixing: M = I + alpha * 11^T\n        alpha = 1e-3\n        M = np.eye(p_no_intercept, dtype=np.float64) + alpha * np.ones((p_no_intercept, p_no_intercept), dtype=np.float64)\n        X_feats = X_feats @ M\n    # Full design with intercept\n    X = np.hstack([intercept, X_feats]) if p_no_intercept  0 else intercept\n    p = X.shape[1]\n    # True coefficients and noise\n    beta_true = rng.standard_normal(p, dtype=np.float64)\n    eps = rng.normal(loc=0.0, scale=noise_std, size=n).astype(np.float64)\n    # Response\n    y = X @ beta_true + eps\n    return X, y\n\ndef efficient_loocv_residuals(X: np.ndarray, y: np.ndarray) - np.ndarray:\n    \"\"\"\n    Compute LOOCV residuals efficiently using residuals and hat matrix diagonal via QR.\n    r_i = e_i / (1 - h_ii), where h_ii = sum of squares of row i in Q from reduced QR of X.\n    \"\"\"\n    # Stable OLS fit\n    beta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    # Reduced QR: X = Q R, Q has orthonormal columns (n x p)\n    Q, _ = np.linalg.qr(X, mode='reduced')\n    h_diag = np.sum(Q * Q, axis=1)\n    # Avoid division by extremely small denominators with safe computation\n    denom = 1.0 - h_diag\n    r_eff = residuals / denom\n    return r_eff\n\ndef brute_force_loocv_residuals(X: np.ndarray, y: np.ndarray) - np.ndarray:\n    \"\"\"\n    Compute LOOCV residuals by refitting for each i: residual_i = y_i - x_i^T beta_hat^{(-i)}.\n    \"\"\"\n    n = X.shape[0]\n    r_bf = np.empty(n, dtype=np.float64)\n    for i in range(n):\n        mask = np.ones(n, dtype=bool)\n        mask[i] = False\n        X_m = X[mask, :]\n        y_m = y[mask]\n        beta_m, _, _, _ = np.linalg.lstsq(X_m, y_m, rcond=None)\n        y_pred_i = X[i, :] @ beta_m\n        r_bf[i] = y[i] - y_pred_i\n    return r_bf\n\ndef max_abs_discrepancy(X: np.ndarray, y: np.ndarray) - float:\n    \"\"\"\n    Compute the maximum absolute discrepancy between efficient and brute-force LOOCV residuals.\n    \"\"\"\n    r_eff = efficient_loocv_residuals(X, y)\n    r_bf = brute_force_loocv_residuals(X, y)\n    return float(np.max(np.abs(r_eff - r_bf)))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, n, p_no_intercept, noise_std, collinear)\n        (42, 6, 1, 0.1, False),         # Case 1\n        (0, 10, 8, 0.05, True),         # Case 2 (near collinearity)\n        (123, 200, 4, 0.5, False),      # Case 3\n        (7, 800, 5, 0.5, False),        # Case 4 (large n)\n    ]\n\n    results = []\n    for seed, n, p_no_intercept, noise_std, collinear in test_cases:\n        X, y = generate_case(seed, n, p_no_intercept, noise_std, collinear)\n        diff = max_abs_discrepancy(X, y)\n        results.append(diff)\n\n    # Final print statement in the exact required format: floats rounded to 12 decimal places.\n    formatted = \",\".join(f\"{val:.12f}\" for val in results)\n    print(f\"[{formatted}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3154757"}]}