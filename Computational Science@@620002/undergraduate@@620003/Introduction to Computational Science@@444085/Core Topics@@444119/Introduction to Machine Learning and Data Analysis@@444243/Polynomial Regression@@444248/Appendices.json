{"hands_on_practices": [{"introduction": "The power of polynomial regression lies in its flexibility, but this also presents a challenge: how do we choose the right degree for our model? Simply adding more terms until the fit looks good can lead to overfitting. This practice introduces a rigorous statistical approach, the nested model F-test, to determine if adding a higher-order term (like $x^3$ to a quadratic model) provides a statistically significant improvement, allowing you to make principled decisions about model complexity. [@problem_id:3158769]", "problem": "Consider a scalar response $y$ and a scalar predictor $x$ observed over $n$ samples. Assume the classical linear model setting under the Gaussâ€“Markov assumptions and normally distributed errors: the data are generated by $y = X \\beta + \\varepsilon$, where $X$ is a fixed design matrix, $\\beta$ is a fixed but unknown parameter vector, and the error vector $\\varepsilon$ has independent entries with distribution $\\mathcal{N}(0,\\sigma^2)$ for some unknown variance $\\sigma^2$. You will compare two nested polynomial regression models for $y$ as a function of $x$: a reduced model of degree $2$ (which includes the intercept, $x$, and $x^2$) and a full model of degree $3$ (which additionally includes $x^3$). The task is to decide, via a nested model test under the normal linear model, whether adding the term $x^3$ provides a statistically significant improvement in fit.\n\nWrite a program that, for each test case specified below, performs the following steps purely in mathematical terms:\n- Construct the reduced model with design matrix $X_{\\text{red}} = [\\mathbf{1}, x, x^2]$ and the full model with design matrix $X_{\\text{full}} = [\\mathbf{1}, x, x^2, x^3]$, where $\\mathbf{1}$ denotes a column of ones of length $n$.\n- Fit each model by minimizing the sum of squared residuals in the sense of Ordinary Least Squares (OLS), obtaining the residual sum of squares $RSS_{\\text{red}}$ and $RSS_{\\text{full}}$.\n- Use the standard nested-model testing framework for linear models with normally distributed errors to compute a test statistic comparing $RSS_{\\text{red}}$ and $RSS_{\\text{full}}$, evaluate its null distribution, and compute a $p$-value for the hypothesis that the cubic term has no effect conditional on the reduced model terms.\n- Decide if the improvement is statistically significant at the specified significance level $\\alpha$ for that test case, and return a boolean indicating whether the cubic term is significant.\n\nYour program must process the following test suite. Each test case is given as arrays of $x$ and $y$ values (listed in order) and a significance level $\\alpha$:\n1. Case A (clear cubic signal, no noise):\n   - $x = [-2, -1, 0, 1, 2, 3]$\n   - $y = 0.5 + 0.2 x - 0.1 x^2 + 1.0 x^3$ evaluated elementwise at the given $x$ values.\n   - $\\alpha = 0.05$.\n2. Case B (quadratic signal with small noise):\n   - $x = [-2, -1, 0, 1, 2, 3]$\n   - $y = 1.0 + 0.5 x - 0.3 x^2 + \\epsilon$, with $\\epsilon$ given elementwise as $\\epsilon = [0.005, -0.010, 0.0125, -0.0075, 0.0000, 0.0100]$.\n   - $\\alpha = 0.05$.\n3. Case C (small sample, quadratic signal with small noise):\n   - $x = [-2, -1, 0, 1, 2]$\n   - $y = 0.4 + 0.1 x - 0.2 x^2 + \\epsilon$, with $\\epsilon$ given elementwise as $\\epsilon = [0.005, -0.003, 0.002, -0.004, 0.001]$.\n   - $\\alpha = 0.05$.\n\nAll arrays must be treated as exact values without any randomization. There are no physical units; treat all quantities as dimensionless numbers. Angles do not appear, and no percentage-format outputs are required.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry corresponds to one test case in the same order as above and is a boolean indicating whether the inclusion of $x^3$ is statistically significant at the specified $\\alpha$. For example, an output might look like $[{\\rm True},{\\rm False},{\\rm False}]$ if the cubic term is significant for Case A and not for Cases B and C.", "solution": "The problem requires a comparison of two nested polynomial regression models using a statistical hypothesis test. The goal is to determine if adding a cubic term, $x^3$, to a quadratic model provides a statistically significant improvement in explaining the variation in a response variable $y$. This is a standard model selection problem addressed using an F-test for nested linear models. The solution proceeds by defining the models, calculating their respective goodness-of-fit, constructing the test statistic, and comparing its resulting p-value to a given significance level $\\alpha$.\n\nFirst, we define the two models. The reduced model, $\\mathcal{M}_{\\text{red}}$, is a polynomial of degree $2$. Its corresponding linear model equation is:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i $$\nFor $n$ observations, this can be written in matrix form as $y = X_{\\text{red}}\\beta_{\\text{red}} + \\varepsilon$. The design matrix $X_{\\text{red}}$ is an $n \\times 3$ matrix, with columns corresponding to the intercept, $x$, and $x^2$:\n$$ X_{\\text{red}} = [\\mathbf{1}, x, x^2] $$\nThe number of parameters in the reduced model is $p_{\\text{red}} = 3$.\n\nThe full model, $\\mathcal{M}_{\\text{full}}$, is a polynomial of degree $3$. Its equation is:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\varepsilon_i $$\nIn matrix form, $y = X_{\\text{full}}\\beta_{\\text{full}} + \\varepsilon$. The design matrix $X_{\\text{full}}$ is an $n \\times 4$ matrix:\n$$ X_{\\text{full}} = [\\mathbf{1}, x, x^2, x^3] $$\nThe number of parameters in the full model is $p_{\\text{full}} = 4$.\n\nBoth models are fit using Ordinary Least Squares (OLS), which finds the parameter estimates $\\hat{\\beta}$ that minimize the Residual Sum of Squares (RSS). The RSS for a generic model with design matrix $X$ is given by:\n$$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\| y - X\\hat{\\beta} \\|_2^2 $$\nThe OLS solution is $\\hat{\\beta} = (X^T X)^{-1}X^T y$. Let $RSS_{\\text{red}}$ and $RSS_{\\text{full}}$ be the minimized residual sums of squares for the reduced and full models, respectively. Since the full model is more complex (it contains all terms of the reduced model plus one more), it will always fit the data at least as well as the reduced model, meaning $RSS_{\\text{full}} \\le RSS_{\\text{red}}$.\n\nThe core of the problem is to test whether the improvement in fit, measured by the difference $RSS_{\\text{red}} - RSS_{\\text{full}}$, is large enough to be considered statistically significant, or if it's small enough to be plausibly due to random chance. This is formulated as a hypothesis test:\n- Null Hypothesis ($H_0$): The coefficient of the cubic term is zero ($\\beta_3 = 0$). The reduced model is sufficient.\n- Alternative Hypothesis ($H_1$): The coefficient of the cubic term is non-zero ($\\beta_3 \\ne 0$). The full model provides a significant improvement.\n\nTo test these hypotheses, we use the F-statistic for nested models. The statistic compares the reduction in RSS per extra parameter to the estimated error variance from the full model. It is defined as:\n$$ F = \\frac{(RSS_{\\text{red}} - RSS_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{red}})}{RSS_{\\text{full}} / (n - p_{\\text{full}})} $$\nThe numerator represents the average reduction in RSS per additional parameter. The number of additional parameters is the numerator degrees of freedom, $df_1 = p_{\\text{full}} - p_{\\text{red}} = 4 - 3 = 1$. The denominator, $MSE_{\\text{full}} = RSS_{\\text{full}} / (n - p_{\\text{full}})$, is the Mean Squared Error of the full model, which is an unbiased estimate of the error variance $\\sigma^2$ assuming the full model is correctly specified. The denominator degrees of freedom are $df_2 = n - p_{\\text{full}}$.\n\nUnder the null hypothesis $H_0$, this F-statistic follows an F-distribution with $df_1$ and $df_2$ degrees of freedom, written as $F \\sim F_{df_1, df_2}$.\n\nTo make a decision, we calculate the p-value, which is the probability of observing an F-statistic as extreme or more extreme than the one computed ($F_{\\text{obs}}$), assuming $H_0$ is true. For the F-test, this is a one-tailed probability:\n$$ p\\text{-value} = P(F_{df_1, df_2} \\ge F_{\\text{obs}}) $$\nThis probability is computed using the survival function (or complementary cumulative distribution function) of the F-distribution.\n\nThe final decision is made by comparing the p-value to the pre-specified significance level $\\alpha$:\n- If $p\\text{-value} < \\alpha$, we reject $H_0$. The evidence suggests that the cubic term is statistically significant.\n- If $p\\text{-value} \\ge \\alpha$, we fail to reject $H_0$. There is insufficient evidence to conclude that the cubic term is significant.\n\nA special case arises if the full model fits the data perfectly, resulting in $RSS_{\\text{full}} = 0$. This occurs in Case A where the data are generated without noise from a cubic polynomial. In this situation, the denominator of the F-statistic becomes $0$, leading to a mathematically infinite F-statistic. An infinite test statistic corresponds to a p-value of $0$. Consequently, $H_0$ is rejected for any positive $\\alpha$.\n\nThe implementation will compute these quantities for each test case. The design matrices $X_{\\text{red}}$ and $X_{\\text{full}}$ are constructed from the given $x$ values. The residual sums of squares, $RSS_{\\text{red}}$ and $RSS_{\\text{full}}$, are obtained efficiently using a numerical linear algebra routine for least squares, such as `numpy.linalg.lstsq`. The F-statistic and its corresponding p-value (using `scipy.stats.f.sf`) are then calculated to reach the final boolean conclusion for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Performs nested model F-tests for three polynomial regression cases.\n    \"\"\"\n\n    # Helper function to perform the nested F-test for a single case.\n    def perform_nested_model_test(x_vals, y_vals, alpha):\n        \"\"\"\n        Compares a degree 2 vs. degree 3 polynomial model via an F-test.\n\n        Args:\n            x_vals (np.ndarray): The predictor variable values.\n            y_vals (np.ndarray): The response variable values.\n            alpha (float): The significance level for the test.\n\n        Returns:\n            bool: True if the cubic term is significant, False otherwise.\n        \"\"\"\n        n = len(x_vals)\n        p_red = 3  # degree 2 model: intercept, x, x^2\n        p_full = 4 # degree 3 model: intercept, x, x^2, x^3\n\n        # Construct the design matrices for the reduced and full models.\n        # np.vander with increasing=True produces columns [x^0, x^1, x^2, ...]\n        X_red = np.vander(x_vals, N=p_red, increasing=True)\n        X_full = np.vander(x_vals, N=p_full, increasing=True)\n\n        # Calculate the Residual Sum of Squares (RSS) for both models.\n        # np.linalg.lstsq returns RSS as the second element of its output tuple.\n        # This is numerically stabler than inverting the matrix X.T @ X.\n        # The returned RSS is an array, so we extract the scalar value.\n        rss_red = np.linalg.lstsq(X_red, y_vals, rcond=None)[1][0]\n        \n        # When the full model provides a perfect fit, lstsq might return an empty\n        # array for the residuals. In this case, RSS is exactly 0.\n        lstsq_full_result = np.linalg.lstsq(X_full, y_vals, rcond=None)\n        rss_full = lstsq_full_result[1][0] if len(lstsq_full_result[1]) > 0 else 0.0\n\n        # Define degrees of freedom for the F-test\n        df1 = p_full - p_red\n        df2 = n - p_full\n        \n        # Handle the case where the denominator of the F-statistic is zero\n        if rss_full  np.finfo(float).eps:\n            # If RSS_full is effectively zero, the full model is a perfect fit.\n            # This implies an infinite F-statistic and a p-value of 0.\n            p_value = 0.0\n        elif df2 = 0:\n            # If there are no degrees of freedom for the error term, the test\n            # is not well-defined. By convention, we can consider it not significant.\n            # This case does not occur in the given problem set.\n            return False\n        else:\n            # Calculate the F-statistic\n            f_statistic = ((rss_red - rss_full) / df1) / (rss_full / df2)\n            \n            # Calculate the p-value using the survival function (1 - CDF) of the F-distribution\n            p_value = f.sf(f_statistic, df1, df2)\n\n        # The cubic term is significant if the p-value is less than the significance level alpha\n        return p_value  alpha\n\n    # Define the test cases from the problem statement.\n    case_a_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    case_a_y = 0.5 + 0.2*case_a_x - 0.1*case_a_x**2 + 1.0*case_a_x**3\n    case_a_alpha = 0.05\n    \n    case_b_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    case_b_noise = np.array([0.005, -0.010, 0.0125, -0.0075, 0.0000, 0.0100])\n    case_b_y = 1.0 + 0.5*case_b_x - 0.3*case_b_x**2 + case_b_noise\n    case_b_alpha = 0.05\n    \n    case_c_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    case_c_noise = np.array([0.005, -0.003, 0.002, -0.004, 0.001])\n    case_c_y = 0.4 + 0.1*case_c_x - 0.2*case_c_x**2 + case_c_noise\n    case_c_alpha = 0.05\n\n    test_cases = [\n        (case_a_x, case_a_y, case_a_alpha),\n        (case_b_x, case_b_y, case_b_alpha),\n        (case_c_x, case_c_y, case_c_alpha),\n    ]\n\n    results = []\n    for x_vals, y_vals, alpha in test_cases:\n        is_significant = perform_nested_model_test(x_vals, y_vals, alpha)\n        results.append(is_significant)\n\n    # Final print statement in the exact required format.\n    # The expected format is `[True,False,False]`.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158769"}, {"introduction": "After learning how to justify adding complexity, we now explore the perils of using a polynomial degree that is too high. This exercise provides a hands-on demonstration of Runge's phenomenon, a classic example of overfitting where a high-degree polynomial perfectly fits the training data but oscillates wildly between and beyond the data points. By training a model on one interval and testing it on an adjacent one, you will quantify the severe errors that can arise from polynomial extrapolation. [@problem_id:3175168]", "problem": "You will implement and analyze polynomial regression as a computational experiment to study extrapolation error. The core scenario is: train on $x \\in [-0.5, 0.5]$ and test on $x \\in [0.5, 1.0]$ for various polynomial degrees $d$, to empirically demonstrate instability of polynomial predictions outside the training range. Begin from the foundational definition of least squares regression and derive the discrete optimization formulation. Then design an algorithm that constructs the polynomial feature map and solves the regression using a numerically stable method.\n\nStart from the following fundamental base. Polynomial regression models an unknown function with a polynomial basis. For a degree $d$ polynomial model, the predictor has the form $f_{\\boldsymbol{w}}(x) = \\sum_{k=0}^{d} w_k x^k$. Given training samples $\\{(x_i, y_i)\\}_{i=1}^{n}$, the least squares objective is to minimize the empirical sum of squared residuals, namely $\\min_{\\boldsymbol{w} \\in \\mathbb{R}^{d+1}} \\sum_{i=1}^{n} \\left( y_i - \\sum_{k=0}^{d} w_k x_i^k \\right)^2$. The design matrix encodes the features $x_i^k$ in a Vandermonde structure, and the minimizer satisfies the normal equations when the design matrix has full column rank.\n\nYou must implement the following steps in a single program:\n- Define the ground-truth function $g(x)$, which provides target values. Use $g(x) = \\dfrac{1}{1 + 25 x^2}$ for all real $x$, a well-studied smooth function that is not a polynomial. This choice ensures that the polynomial model is an approximation and allows meaningful extrapolation evaluation.\n- Generate training inputs $x_i$ as $n$ equally spaced points in $[-0.5, 0.5]$ including the endpoints. For each $x_i$, set $y_i = g(x_i)$. No noise is added.\n- For a given polynomial degree $d$, construct the Vandermonde design matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (d+1)}$ with entries $A_{i,k} = x_i^k$ for $i \\in \\{1,\\dots,n\\}$ and $k \\in \\{0,\\dots,d\\}$, and solve the least squares problem to obtain $\\boldsymbol{w}$ using a numerically stable method that does not explicitly form $\\left(\\boldsymbol{A}^\\top \\boldsymbol{A}\\right)^{-1}$.\n- Evaluate extrapolation performance on a test grid of $m$ equally spaced points in $[0.5, 1.0]$ including the endpoints. Compute predictions $\\hat{y}_j = f_{\\boldsymbol{w}}(x_j^{\\text{test}})$ and compare against $g(x_j^{\\text{test}})$.\n- Quantify extrapolation error as the mean squared error (MSE) over the test grid, i.e., $\\text{MSE} = \\dfrac{1}{m} \\sum_{j=1}^{m} \\left( \\hat{y}_j - g(x_j^{\\text{test}}) \\right)^2$. Report this single scalar for each tested degree $d$.\n\nYour program must adhere to the following test suite and output specification:\n- Use $n = 11$ training points in $[-0.5, 0.5]$ and $m = 101$ test points in $[0.5, 1.0]$, both equally spaced and including endpoints.\n- Evaluate the test suite of polynomial degrees $d \\in \\{0, 1, 3, 5, 9, 10\\}$.\n- For each degree $d$, output the mean squared extrapolation error as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the degrees listed above, i.e., $[ \\text{MSE}(d{=}0), \\text{MSE}(d{=}1), \\text{MSE}(d{=}3), \\text{MSE}(d{=}5), \\text{MSE}(d{=}9), \\text{MSE}(d{=}10) ]$.\n\nScientific realism and derivation requirements:\n- Derive the algorithm from the least squares principle and the construction of the Vandermonde design matrix. Do not use shortcut formulas; the design matrix and least squares minimization must be part of the solution path.\n- Ensure numerical stability by using a solution method based on Singular Value Decomposition (SVD) or an equivalent numerically stable solver, avoiding explicit inversion of $\\boldsymbol{A}^\\top \\boldsymbol{A}$.\n- There are no physical units in this problem. Angles are not used. All outputs are floating-point scalars. The final output format is the single line described above.\n\nThe goal is to reason from first principles about why polynomial regression can become unstable for extrapolation, and to empirically demonstrate this effect via the specified test suite.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded computational task in the field of numerical analysis and machine learning. All parameters and objectives are clearly defined, and the taskâ€”to empirically demonstrate the instability of polynomial extrapolationâ€”is a classic and meaningful exercise based on established principles.\n\nThe core of the problem is to solve a series of polynomial regression tasks and evaluate their performance on an extrapolation domain. We begin by formalizing the problem from first principles.\n\nA polynomial regression model of degree $d$ seeks to approximate an unknown function using a predictor of the form:\n$$\nf_{\\boldsymbol{w}}(x) = w_0 + w_1x + w_2x^2 + \\dots + w_dx^d = \\sum_{k=0}^{d} w_k x^k\n$$\nHere, $\\boldsymbol{w} = [w_0, w_1, \\dots, w_d]^\\top$ is the vector of coefficients or weights that must be determined.\n\nGiven a set of $n$ training samples $\\{(x_i, y_i)\\}_{i=1}^{n}$, the principle of least squares dictates that we find the weights $\\boldsymbol{w}$ that minimize the sum of squared residuals (SSR) between the model's predictions $f_{\\boldsymbol{w}}(x_i)$ and the true target values $y_i$. The objective function to minimize is:\n$$\nE(\\boldsymbol{w}) = \\sum_{i=1}^{n} (y_i - f_{\\boldsymbol{w}}(x_i))^2 = \\sum_{i=1}^{n} \\left( y_i - \\sum_{k=0}^{d} w_k x_i^k \\right)^2\n$$\nTo facilitate a solution, we express this system in matrix-vector notation. Let $\\boldsymbol{y} \\in \\mathbb{R}^n$ be the vector of target values, $\\boldsymbol{y} = [y_1, y_2, \\dots, y_n]^\\top$. Let $\\boldsymbol{A}$ be the $n \\times (d+1)$ design matrix, where each row corresponds to a data point $x_i$ and each column corresponds to a polynomial basis function $x^k$. The entries of $\\boldsymbol{A}$ are given by $A_{i,k} = x_i^k$, for $i \\in \\{1,\\dots,n\\}$ and $k \\in \\{0,\\dots,d\\}$. This specific structure defines $\\boldsymbol{A}$ as a Vandermonde matrix.\n$$\n\\boldsymbol{A} = \\begin{pmatrix}\nx_1^0  x_1^1  \\dots  x_1^d \\\\\nx_2^0  x_2^1  \\dots  x_2^d \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nx_n^0  x_n^1  \\dots  x_n^d\n\\end{pmatrix}\n$$\nWith these definitions, the vector of predictions for all training points is $\\hat{\\boldsymbol{y}} = \\boldsymbol{A}\\boldsymbol{w}$. The least squares objective function becomes the squared Euclidean norm of the residual vector:\n$$\nE(\\boldsymbol{w}) = \\|\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{w}\\|_2^2\n$$\nThe minimization problem is $\\min_{\\boldsymbol{w} \\in \\mathbb{R}^{d+1}} \\|\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{w}\\|_2^2$. This is a standard linear least squares problem. A common analytical solution is derived from the normal equations, $\\boldsymbol{A}^\\top \\boldsymbol{A} \\boldsymbol{w} = \\boldsymbol{A}^\\top \\boldsymbol{y}$. If $\\boldsymbol{A}^\\top \\boldsymbol{A}$ is invertible, the solution is $\\boldsymbol{w} = (\\boldsymbol{A}^\\top \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\top \\boldsymbol{y}$.\n\nHowever, the problem correctly mandates avoiding this approach. Vandermonde matrices are notoriously ill-conditioned, especially for high degrees $d$. The condition number of $\\boldsymbol{A}^\\top\\boldsymbol{A}$ is the square of the condition number of $\\boldsymbol{A}$, which can lead to severe numerical instability and inaccurate results when forming and inverting $\\boldsymbol{A}^\\top\\boldsymbol{A}$ in finite-precision arithmetic.\n\nA numerically stable method for solving the least squares problem involves matrix decompositions, such as QR decomposition or Singular Value Decomposition (SVD). As SVD is generally the most robust, we base our method on it. Any matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (d+1)}$ has an SVD of the form $\\boldsymbol{A} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top$, where $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are orthogonal matrices and $\\boldsymbol{\\Sigma}$ is a rectangular diagonal matrix of singular values. The least squares solution is given by $\\boldsymbol{w} = \\boldsymbol{A}^+ \\boldsymbol{y}$, where $\\boldsymbol{A}^+ = \\boldsymbol{V}\\boldsymbol{\\Sigma}^+\\boldsymbol{U}^\\top$ is the Moore-Penrose pseudoinverse. Numerical libraries provide efficient and stable `lstsq` solvers that internally use such decompositions.\n\nThe complete algorithm is as follows:\n1.  Define the ground-truth function $g(x) = \\frac{1}{1 + 25x^2}$.\n2.  Generate training data: create a vector of $n=11$ equally spaced points $x_{\\text{train}}$ in the interval $[-0.5, 0.5]$ and compute the corresponding target values $y_{\\text{train},i} = g(x_{\\text{train},i})$.\n3.  Generate test data: create a vector of $m=101$ equally spaced points $x_{\\text{test}}$ in the extrapolation interval $[0.5, 1.0]$ and compute the ground-truth values $y_{\\text{test},j} = g(x_{\\text{test},j})$.\n4.  Initialize an empty list to store results.\n5.  For each polynomial degree $d$ in the test suite $\\{0, 1, 3, 5, 9, 10\\}$:\n    a. Construct the $n \\times (d+1)$ Vandermonde training matrix $\\boldsymbol{A}_{\\text{train}}$, where the entry at row $i$ and column $k$ (0-indexed) is $(x_{\\text{train},i})^k$.\n    b. Solve the linear least squares system $\\boldsymbol{A}_{\\text{train}}\\boldsymbol{w} \\approx \\boldsymbol{y}_{\\text{train}}$ for the weight vector $\\boldsymbol{w}$ using a numerically stable solver, such as `numpy.linalg.lstsq`.\n    c. Construct the $m \\times (d+1)$ Vandermonde test matrix $\\boldsymbol{A}_{\\text{test}}$, where the entry at row $j$ and column $k$ is $(x_{\\text{test},j})^k$.\n    d. Compute the model's predictions on the test set: $\\hat{\\boldsymbol{y}}_{\\text{test}} = \\boldsymbol{A}_{\\text{test}}\\boldsymbol{w}$.\n    e. Calculate the mean squared error (MSE) on the extrapolation domain: $\\text{MSE} = \\frac{1}{m} \\sum_{j=1}^{m} (\\hat{y}_{\\text{test},j} - y_{\\text{test},j})^2$.\n    f. Append the calculated MSE to the results list.\n6.  Output the list of MSE values.\n\nThis procedure will empirically demonstrate Runge's phenomenon. While higher-degree polynomials can achieve a better fit within the training interval, they tend to oscillate wildly outside of it. The extrapolation error is expected to be relatively small for low degrees, but to increase dramatically for high degrees ($d=9, 10$), showcasing the unreliability of high-degree polynomial extrapolation. Note that for $d=10$, the number of coefficients ($d+1=11$) equals the number of data points ($n=11$), so the model becomes an interpolating polynomial, which is particularly prone to severe oscillations between data points, including in the extrapolation region.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes polynomial regression to study extrapolation error.\n    \"\"\"\n    # Define the test suite of polynomial degrees.\n    test_cases_degrees = [0, 1, 3, 5, 9, 10]\n\n    # Define parameters for data generation.\n    n_train_points = 11\n    n_test_points = 101\n    \n    # Define the ground-truth function g(x).\n    g = lambda x: 1.0 / (1.0 + 25.0 * x**2)\n\n    # 1. Generate training data\n    # n points equally spaced in [-0.5, 0.5] including endpoints.\n    x_train = np.linspace(-0.5, 0.5, n_train_points)\n    y_train = g(x_train)\n\n    # 2. Generate test data for extrapolation evaluation\n    # m points equally spaced in [0.5, 1.0] including endpoints.\n    x_test = np.linspace(0.5, 1.0, n_test_points)\n    y_test_ground_truth = g(x_test)\n    \n    results = []\n    \n    # 3. Loop through each specified polynomial degree.\n    for d in test_cases_degrees:\n        # a. Construct the Vandermonde design matrix for training.\n        # The number of columns is d + 1 for a degree-d polynomial (powers 0 to d).\n        # np.vander with increasing=True gives columns [x^0, x^1, ..., x^d].\n        A_train = np.vander(x_train, d + 1, increasing=True)\n        \n        # b. Solve the least squares problem to find the weights w.\n        # np.linalg.lstsq is a numerically stable solver.\n        # It returns the solution, residuals, rank, and singular values.\n        # We only need the solution vector w.\n        w, _, _, _ = np.linalg.lstsq(A_train, y_train, rcond=None)\n        \n        # c. Construct the Vandermonde matrix for the test set.\n        A_test = np.vander(x_test, d + 1, increasing=True)\n        \n        # d. Compute predictions on the test set.\n        y_pred_test = A_test @ w\n        \n        # e. Quantify extrapolation error using Mean Squared Error (MSE).\n        mse = np.mean((y_pred_test - y_test_ground_truth)**2)\n        \n        results.append(mse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3175168"}, {"introduction": "Beyond the statistical challenges of choosing a degree, polynomial regression faces a crucial numerical hurdle, especially for higher-degree models. The standard monomial basis, $\\{1, x, x^2, \\dots, x^d\\}$, creates a design matrix that is often ill-conditioned, meaning small numerical errors can be greatly amplified in the solution. This practice demonstrates how to diagnose this problem using the matrix condition number and how to solve it by using a numerically superior basis of orthogonal polynomials. [@problem_id:2425191]", "problem": "You are tasked with quantitatively comparing the numerical conditioning of polynomial regression design matrices built from a standard monomial basis versus an orthogonal polynomial basis. In polynomial regression, one models data by choosing a set of basis functions and solving a linear least-squares problem. The numerical stability of the solution depends critically on the conditioning of the design matrix. The spectral condition number of a matrix is defined in terms of singular values computed by singular value decomposition and is widely used to assess numerical stability. Orthogonal polynomials are known to reduce multicollinearity and improve conditioning compared to naive monomials, especially when the input is scaled to a standard interval.\n\nStarting from the fundamental definitions of least-squares regression, orthogonal polynomials, and the spectral condition number, implement a program that, for each test case, constructs two design matrices using the same inputs:\n- A monomial Vandermonde-type design matrix built from the standard basis $\\{1, x, x^{2}, \\dots \\}$.\n- A design matrix built from the first kind Legendre polynomials evaluated on the same points after an affine normalization.\n\nFor each test case, follow these requirements:\n1. Given a set of input points $\\{x_{i}\\}_{i=0}^{n-1}$ in any real interval, first normalize them to $\\{t_{i}\\}$ in the interval $[-1,1]$ using the affine map\n$$\nt_{i} = \\frac{2\\,(x_{i} - x_{\\min})}{x_{\\max} - x_{\\min}} - 1,\n$$\nwhere $x_{\\min} = \\min_{i} x_{i}$ and $x_{\\max} = \\max_{i} x_{i}$.\n2. For a specified nonnegative integer degree $d$, construct two $n \\times (d+1)$ design matrices:\n   - The monomial design matrix $A$ with entries $A_{i,k} = t_{i}^{k}$ for $k = 0, 1, \\dots, d$.\n   - The Legendre design matrix $B$ with entries $B_{i,k} = P_{k}(t_{i})$ for $k = 0, 1, \\dots, d$, where $P_{k}$ denotes the $k$-th Legendre polynomial on $[-1,1]$.\n3. Compute the spectral condition numbers $\\kappa_{2}(A)$ and $\\kappa_{2}(B)$ using singular value decomposition. Use the definition\n$$\n\\kappa_{2}(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)},\n$$\nwhere $\\sigma_{\\max}(M)$ and $\\sigma_{\\min}(M)$ are, respectively, the largest and smallest singular values of $M$.\n4. For each test case, compute the improvement ratio\n$$\nr = \\frac{\\kappa_{2}(A)}{\\kappa_{2}(B)}.\n$$\nReport $r$ rounded to exactly six decimal places.\n\nAngle units: whenever the cosine function is used below to define points, the arguments are in radians.\n\nYour program must implement the following test suite and produce results in the specified final output format.\n\nTest suite:\n- Case $1$ (equispaced, moderate degree): $n = 50$, $d = 12$, with inputs $x_{i} = -1 + \\dfrac{2\\,i}{n-1}$ for $i = 0, 1, \\dots, n-1$.\n- Case $2$ (clustered near one endpoint): $n = 50$, $d = 12$, with inputs $x_{i} = 1 - \\exp\\!\\left(-\\dfrac{5\\,i}{n-1}\\right)$ for $i = 0, 1, \\dots, n-1$.\n- Case $3$ (Chebyshev-like interior concentration): $n = 30$, $d = 14$, with inputs $x_{i} = \\cos\\!\\left(\\dfrac{\\pi\\,(i+0.5)}{n}\\right)$ for $i = 0, 1, \\dots, n-1$.\n- Case $4$ (wide original scale, then normalized): $n = 40$, $d = 18$, with inputs $x_{i} = 0 + \\dfrac{1000\\,i}{n-1}$ for $i = 0, 1, \\dots, n-1$.\n\nAll computations must be done in double precision. For each case, ensure $d+1 \\le n$ so that the design matrices are full column rank for distinct $t_{i}$.\n\nFinal output format:\nYour program should produce a single line of output containing the four improvement ratios for the test cases as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, each rounded to exactly six decimal places, for example, \"[1.234567,8.901234,5.678901,2.345678]\". No other text should be printed. All values are unitless real numbers expressed as decimal floats with six digits after the decimal point.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded, well-posed, and objective. It presents a clear task in computational engineering, specifically in the numerical analysis of regression methods. All necessary data and definitions are provided, and there are no contradictions or ambiguities. I shall therefore proceed with a complete solution.\n\nThe task is to quantitatively evaluate the improvement in numerical conditioning when using an orthogonal polynomial basis (Legendre polynomials) instead of a standard monomial basis for a polynomial regression problem. The core of this analysis lies in the properties of the design matrix, which maps the regression coefficients to the predicted values.\n\nIn a polynomial regression problem, we seek to model a set of data points $\\{ (x_i, y_i) \\}_{i=0}^{n-1}$ with a polynomial of degree $d$. This polynomial can be expressed as a linear combination of basis functions $\\{ \\phi_k(x) \\}_{k=0}^{d}$:\n$$\nf(x) = \\sum_{k=0}^{d} c_k \\phi_k(x)\n$$\nThe goal is to find the coefficients $c_k$ that minimize the sum of squared errors between the model predictions $f(x_i)$ and the observed data $y_i$. This is a linear least-squares problem. The relationship between the vector of coefficients $C = [c_0, c_1, \\dots, c_d]^T$ and the vector of predicted values $Y_{pred} = [f(x_0), f(x_1), \\dots, f(x_{n-1})]^T$ is given by the matrix equation:\n$$\nY_{pred} = M C\n$$\nwhere $M$ is the $n \\times (d+1)$ design matrix with entries $M_{ik} = \\phi_k(x_i)$. The numerical stability of solving for the coefficient vector $C$ is critically dependent on the conditioning of this design matrix $M$. A common measure for this is the spectral condition number, $\\kappa_2(M)$, defined as the ratio of the largest to the smallest singular value of $M$:\n$$\n\\kappa_2(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)}\n$$\nA large condition number signifies an ill-conditioned problem, where small errors in the input data can lead to large errors in the computed coefficients, rendering the solution unreliable.\n\nThe problem requires a comparison of two choices for the basis functions:\n$1$. The standard monomial basis, $\\phi_k(x) = x^k$. The columns of the resulting design matrix, a Vandermonde-type matrix, become nearly linearly dependent for high degrees $d$ or for input points $x_i$ clustered away from the origin. This leads to a very large condition number.\n$2$. The Legendre polynomial basis, $\\phi_k(x) = P_k(x)$. Legendre polynomials $\\{ P_k(t) \\}_{k=0}^{\\infty}$ are orthogonal on the interval $[-1, 1]$ with respect to the $L^2$ inner product, i.e., $\\int_{-1}^{1} P_j(t) P_k(t) dt = 0$ for $j \\neq k$. This orthogonality property helps to ensures that the columns of the corresponding design matrix are far from being linearly dependent, resulting in a significantly smaller condition number and a more stable numerical problem.\n\nTo ensure the benefits of orthogonality are realized, the input data points must be mapped to the interval $[-1, 1]$ on which the Legendre polynomials are defined. The problem specifies the following affine transformation for a set of points $\\{x_i\\}$:\n$$\nt_i = \\frac{2(x_i - x_{\\min})}{x_{\\max} - x_{\\min}} - 1\n$$\nwhere $x_{\\min}$ and $x_{\\max}$ are the minimum and maximum values in the set $\\{x_i\\}$. This transformation is applied to all input points before constructing the design matrices. Note that if $x_{\\max} = x_{\\min}$, all points are identical, and this formula is ill-defined. However, the problem ensures distinct points are used, so $x_{\\max}  x_{\\min}$.\n\nThe algorithmic procedure to solve this problem is as follows:\nFor each test case, specified by the number of points $n$, the polynomial degree $d$, and a formula for generating points $\\{x_i\\}$:\n$1$. Generate the $n$ input points $\\{x_i\\}_{i=0}^{n-1}$.\n$2$. Compute $x_{\\min} = \\min_{i} x_i$ and $x_{\\max} = \\max_{i} x_i$.\n$3$. Normalize the points to obtain $\\{t_i\\}_{i=0}^{n-1}$ in the interval $[-1, 1]$ using the affine map.\n$4$. Construct the $n \\times (d+1)$ monomial design matrix $A$, where the entry in the $i$-th row and $k$-th column is $A_{ik} = t_i^k$, for $i \\in \\{0, \\dots, n-1\\}$ and $k \\in \\{0, \\dots, d\\}$.\n$5$. Construct the $n \\times (d+1)$ Legendre design matrix $B$, where $B_{ik} = P_k(t_i)$. The Legendre polynomials $P_k(t)$ are computed using the standard three-term recurrence relation:\n   $$ P_0(t) = 1 $$\n   $$ P_1(t) = t $$\n   $$ (k+1)P_{k+1}(t) = (2k+1)tP_k(t) - kP_{k-1}(t) \\quad \\text{for } k \\geq 1 $$\n   We will use a pre-existing library function for this evaluation, as is standard practice in computational science.\n$6$. For each matrix, $A$ and $B$, compute its singular values using Singular Value Decomposition (SVD). Let the singular values be $\\{\\sigma_j\\}$.\n$7$. Calculate the condition number for each matrix: $\\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$ and $\\kappa_2(B) = \\sigma_{\\max}(B) / \\sigma_{\\min}(B)$.\n$8$. Compute the improvement ratio $r = \\kappa_2(A) / \\kappa_2(B)$.\n$9$. Report the value of $r$ rounded to six decimal places, as required.\n\nThis procedure is implemented for each of the four specified test cases, and the results are aggregated into the specified output format. The use of double-precision floating-point arithmetic is standard and sufficient for this analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\n\ndef solve():\n    \"\"\"\n    Computes the improvement in design matrix conditioning by using Legendre\n    polynomials over monomials for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 50, \"d\": 12,\n            \"x_generator\": lambda n: -1.0 + 2.0 * np.arange(n) / (n - 1)\n        },\n        {\n            \"n\": 50, \"d\": 12,\n            \"x_generator\": lambda n: 1.0 - np.exp(-5.0 * np.arange(n) / (n - 1))\n        },\n        {\n            \"n\": 30, \"d\": 14,\n            \"x_generator\": lambda n: np.cos(np.pi * (np.arange(n) + 0.5) / n)\n        },\n        {\n            \"n\": 40, \"d\": 18,\n            \"x_generator\": lambda n: 1000.0 * np.arange(n) / (n - 1)\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        d = case[\"d\"]\n        \n        # Step 1: Generate input points\n        x = case[\"x_generator\"](n).astype(np.float64)\n\n        # Step 2: Normalize points to [-1, 1]\n        x_min, x_max = np.min(x), np.max(x)\n        \n        # Handle the case where all points are the same to avoid division by zero.\n        if np.isclose(x_max, x_min):\n            t = np.zeros_like(x)\n        else:\n            t = 2.0 * (x - x_min) / (x_max - x_min) - 1.0\n\n        # Step 3: Construct the monomial design matrix A (Vandermonde matrix)\n        # A_{ik} = t_i^k for k = 0, ..., d\n        # numpy.vander with increasing=True creates columns t^0, t^1, ..., t^d\n        A = np.vander(t, N=d + 1, increasing=True)\n\n        # Step 4: Construct the Legendre design matrix B\n        # B_{ik} = P_k(t_i) for k = 0, ..., d\n        B = np.zeros((n, d + 1), dtype=np.float64)\n        for k in range(d + 1):\n            B[:, k] = eval_legendre(k, t)\n\n        # Step 5: Compute the singular values for both matrices\n        # SVD returns singular values sorted in descending order.\n        # compute_uv=False is an optimization as we only need the singular values.\n        s_A = np.linalg.svd(A, compute_uv=False)\n        s_B = np.linalg.svd(B, compute_uv=False)\n\n        # Step 6: Compute the spectral condition numbers\n        # kappa(M) = sigma_max / sigma_min\n        kappa_A = s_A[0] / s_A[-1]\n        kappa_B = s_B[0] / s_B[-1]\n\n        # Step 7: Compute the improvement ratio\n        improvement_ratio = kappa_A / kappa_B\n        \n        # Step 8: Format the result and add to the list\n        results.append(f\"{improvement_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2425191"}]}