{"hands_on_practices": [{"introduction": "The engine of learning in most modern neural networks is gradient-based optimization. To truly understand how networks learn, we must first dissect the fundamental calculation that drives this process: the gradient of the loss function. This practice guides you through a first-principles exploration of the gradient for the softmax cross-entropy loss, the standard for multi-class classification [@problem_id:3134219]. By deriving and implementing these gradients, you will gain a concrete understanding of concepts like gradient saturation and the regularizing effect of label smoothing, providing a solid foundation for mastering backpropagation.", "problem": "You will analyze gradients of the Softmax Cross-Entropy (SCE) loss in the context of Artificial Neural Networks (ANN). Your tasks are to derive from first principles how the gradient depends on the logits, to relate the gradient magnitude to logit differences (margins), and to examine the impact of label smoothing on gradient saturation. You must then implement a program that computes quantitative gradient metrics for a provided test suite.\n\nFoundational base (definitions only):\n- Given a vector of logits $\\mathbf{z} \\in \\mathbb{R}^K$, the softmax function produces class probabilities $\\mathbf{p} \\in \\mathbb{R}^K$ with $p_i \\ge 0$ and $\\sum_{i=1}^K p_i = 1$.\n- For a target distribution $\\mathbf{y} \\in \\mathbb{R}^K$ with $y_i \\ge 0$ and $\\sum_{i=1}^K y_i = 1$, the cross-entropy loss is a scalar function of $\\mathbf{p}$.\n- Label smoothing modifies a one-hot target for the correct class index $c$ by assigning $y_c = 1 - \\varepsilon$ and $y_j = \\varepsilon/(K-1)$ for all $j \\ne c$, where $\\varepsilon \\in [0,1)$.\n\nRequired derivations and analysis:\n1) Starting strictly from the above definitions and fundamental calculus (quotient rule, chain rule, and properties of the exponential and logarithm), derive the gradient of the Softmax Cross-Entropy loss with respect to the logits $\\mathbf{z}$. Present the general result for an arbitrary target distribution $\\mathbf{y}$ and then specialize it to one-hot targets and to label-smoothed targets. Do not assume any pre-known gradient formula; derive it.\n2) Using your gradient expression and the fact that softmax probabilities depend only on differences of logits (shift invariance), analyze how the gradient magnitude relates to logit differences. Formalize the notion of the correct-class logit margin $\\Delta = z_c - \\max_{j \\ne c} z_j$ and explain, using your formulas, why large positive $\\Delta$ leads to small gradients (saturation) for one-hot targets, why large negative $\\Delta$ leads to large gradients, and how label smoothing with $\\varepsilon > 0$ alters these magnitudes even when the model is very confident.\n3) Implement a program that, for each test case, computes the following quantities:\n   - The softmax probabilities $\\mathbf{p}$ using a numerically stable method.\n   - The gradient vector $\\nabla_{\\mathbf{z}} L$ with respect to logits under label smoothing parameter $\\varepsilon$.\n   - The correct-class margin $\\Delta = z_c - \\max_{j \\ne c} z_j$.\n   - The Euclidean (also called $\\ell_2$) norm $\\lVert \\nabla_{\\mathbf{z}} L \\rVert_2$.\n   - The absolute gradient at the correct class $\\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert$.\n   - The maximum absolute gradient across classes $\\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert$.\n\nImplementation requirements:\n- Use natural logarithms.\n- Use a numerically stable softmax implementation (you may subtract $\\max_i z_i$ from all logits before exponentiation).\n- For each test case, return a list of four floats $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$, each rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. For example, the output format must look like $[[a\\_1,b\\_1,c\\_1,d\\_1],[a\\_2,b\\_2,c\\_2,d\\_2],\\dots]$ where each $a_i,b_i,c_i,d_i$ are decimal numbers shown with exactly $6$ digits after the decimal point.\n\nTest suite (each test case is a triple $(\\mathbf{z}, c, \\varepsilon)$):\n- Case $1$: $\\mathbf{z} = [0.1, 0.2, 0.15, 0.05]$, $c = 1$, $\\varepsilon = 0.0$.\n- Case $2$: $\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$, $c = 0$, $\\varepsilon = 0.0$.\n- Case $3$: $\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$, $c = 0$, $\\varepsilon = 0.0$.\n- Case $4$: $\\mathbf{z} = [1000.0, 1000.0, 1000.0, 1000.0]$, $c = 2$, $\\varepsilon = 0.0$.\n- Case $5$: $\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$, $c = 0$, $\\varepsilon = 0.1$.\n- Case $6$: $\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$, $c = 0$, $\\varepsilon = 0.1$.\n\nFinal output specification:\n- The program must print exactly one line: a single top-level list containing $6$ inner lists, one per test case, in the order listed above.\n- Each inner list must be $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$, with each float rounded to exactly $6$ decimal places.\n- There must be no spaces anywhere in the output line.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It consists of a standard, non-trivial derivation, a conceptual analysis rooted in that derivation, and a concrete implementation task, all central to the fundamentals of artificial neural networks.\n\n### 1. Derivation of the Gradient of Softmax Cross-Entropy Loss\n\nWe are tasked with deriving the gradient of the Softmax Cross-Entropy (SCE) loss with respect to the input logits, $\\mathbf{z}$.\n\n**Definitions:**\n- Logits: $\\mathbf{z} = [z_1, z_2, \\dots, z_K]^T \\in \\mathbb{R}^K$.\n- Softmax probability for class $i$: $p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$.\n- Target probability distribution: $\\mathbf{y} = [y_1, y_2, \\dots, y_K]^T$, where $y_i \\ge 0$ and $\\sum_{i=1}^K y_i = 1$.\n- Cross-Entropy Loss: $L(\\mathbf{p}, \\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i$. Note that the logarithm is the natural logarithm, $\\ln$.\n\nOur goal is to compute the gradient vector $\\nabla_{\\mathbf{z}} L$, whose components are $\\frac{\\partial L}{\\partial z_k}$ for $k \\in \\{1, \\dots, K\\}$. We apply the chain rule:\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\n$$\n\n**Step 1.1: Compute $\\frac{\\partial L}{\\partial p_i}$**\nThe loss is $L = -\\sum_{j=1}^K y_j \\log p_j$. The partial derivative with respect to a specific probability $p_i$ is straightforward:\n$$\n\\frac{\\partial L}{\\partial p_i} = \\frac{\\partial}{\\partial p_i} \\left( -y_i \\log p_i - \\sum_{j \\ne i} y_j \\log p_j \\right) = -\\frac{y_i}{p_i}\n$$\n\n**Step 1.2: Compute $\\frac{\\partial p_i}{\\partial z_k}$ (Jacobian of the Softmax function)**\nWe must consider two cases for the derivative of $p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ with respect to $z_k$. Let $D = \\sum_{j=1}^K e^{z_j}$.\n\nCase A: $i = k$. We use the quotient rule $\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}$.\n$$\n\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_k})}{\\partial z_k} D - e^{z_k} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{e^{z_k} D - e^{z_k} e^{z_k}}{D^2} = \\frac{e^{z_k}}{D} \\frac{D - e^{z_k}}{D} = p_k (1 - p_k)\n$$\n\nCase B: $i \\ne k$.\n$$\n\\frac{\\partial p_i}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_i})}{\\partial z_k} D - e^{z_i} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{0 \\cdot D - e^{z_i} e^{z_k}}{D^2} = -\\frac{e^{z_i}}{D} \\frac{e^{z_k}}{D} = -p_i p_k\n$$\n\n**Step 1.3: Combine the derivatives**\nNow we substitute these results back into the chain rule expression for $\\frac{\\partial L}{\\partial z_k}$:\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k} = \\left(\\frac{\\partial L}{\\partial p_k} \\frac{\\partial p_k}{\\partial z_k}\\right) + \\sum_{i \\ne k} \\left(\\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = \\left(-\\frac{y_k}{p_k}\\right) (p_k(1 - p_k)) + \\sum_{i \\ne k} \\left(-\\frac{y_i}{p_i}\\right) (-p_i p_k)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = -y_k(1 - p_k) + \\sum_{i \\ne k} y_i p_k = -y_k + y_k p_k + p_k \\sum_{i \\ne k} y_i\n$$\nFactoring out $p_k$:\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k \\left(y_k + \\sum_{i \\ne k} y_i \\right) - y_k\n$$\nSince $\\mathbf{y}$ is a probability distribution, $\\sum_{i=1}^K y_i = y_k + \\sum_{i \\ne k} y_i = 1$. This simplifies the expression to:\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k(1) - y_k = p_k - y_k\n$$\nThis elegant result states that the gradient of the Softmax Cross-Entropy loss with respect to a logit $z_k$ is the difference between the predicted probability $p_k$ and the target probability $y_k$. In vector form, the gradient is:\n$$\n\\nabla_{\\mathbf{z}} L = \\mathbf{p} - \\mathbf{y}\n$$\n\n**Specializations:**\n1.  **One-Hot Targets:** For a correct class index $c$, the target vector is $\\mathbf{y}$ with $y_c = 1$ and $y_j = 0$ for all $j \\ne c$. The gradient components are:\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - 0 = p_j \\quad (\\text{for } j \\ne c)$\n\n2.  **Label-Smoothed Targets:** With smoothing parameter $\\varepsilon \\in [0, 1)$, the target vector $\\mathbf{y}$ is defined as $y_c = 1 - \\varepsilon$ and $y_j = \\frac{\\varepsilon}{K-1}$ for all $j \\ne c$. The gradient components are:\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon)$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\quad (\\text{for } j \\ne c)$\n\n### 2. Analysis of Gradient Magnitude and Logit Margin\n\nThe softmax function is invariant to a constant shift in all logits, i.e., $\\text{softmax}(\\mathbf{z} + C) = \\text{softmax}(\\mathbf{z})$ for any scalar $C$. This means the probabilities $\\mathbf{p}$ depend only on the *differences* between logits.\n\nWe define the correct-class logit margin as $\\Delta = z_c - \\max_{j \\ne c} z_j$. This margin measures how much more \"confident\" the model is in the correct class $c$ compared to the most likely incorrect class.\n\n**Analysis for One-Hot Targets ($\\varepsilon = 0$):**\n- **Large Positive Margin ($\\Delta \\to \\infty$):** If $z_c$ is much larger than all other $z_j$, the model is very confident in the correct class.\n    - $p_c = \\frac{e^{z_c}}{e^{z_c} + \\sum_{j \\ne c} e^{z_j}} = \\frac{1}{1 + \\sum_{j \\ne c} e^{z_j - z_c}} \\to 1$ since $z_j - z_c \\to -\\infty$.\n    - Consequently, $p_j \\to 0$ for $j \\ne c$.\n    - The gradient components become:\n        - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to 1 - 1 = 0$.\n        - $(\\nabla_{\\mathbf{z}} L)_j = p_j \\to 0$.\n    - The entire gradient vector $\\nabla_{\\mathbf{z}} L$ approaches zero. This phenomenon is known as **gradient saturation**. The model receives a diminishing learning signal as it becomes more confident, effectively stopping learning for this example.\n\n- **Large Negative Margin ($\\Delta \\to -\\infty$):** If $z_c$ is much smaller than the largest incorrect logit, say $z_m = \\max_{j \\ne c} z_j$, the model is very confident in a wrong class.\n    - $p_c \\to 0$.\n    - The gradient for the correct class logit is $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to -1$. This is a strong signal to increase $z_c$.\n    - The probability for the \"winning\" incorrect class $m$ approaches $1$, i.e., $p_m \\to 1$.\n    - The gradient for this incorrect logit is $(\\nabla_{\\mathbf{z}} L)_m = p_m \\to 1$. This is a strong signal to decrease $z_m$.\n    - The gradient magnitude is large, providing a strong corrective signal to the model.\n\n**Impact of Label Smoothing ($\\varepsilon > 0$):**\nLabel smoothing changes the target distribution, which in turn alters the gradient behavior, particularly in the high-confidence regime.\n- **Large Positive Margin ($\\Delta \\to \\infty$):** As before, $p_c \\to 1$ and $p_j \\to 0$ for $j \\ne c$. The gradient components become:\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon) \\to 1 - (1 - \\varepsilon) = \\varepsilon$.\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\to 0 - \\frac{\\varepsilon}{K-1} = -\\frac{\\varepsilon}{K-1}$.\n- Unlike the one-hot case, the gradient components do not vanish.\n    - The gradient for the correct logit, $\\varepsilon$, is a small positive value. This penalizes overconfidence by encouraging $z_c$ to not grow infinitely larger than other logits. The target for $p_c$ is now $1-\\varepsilon$, not $1$.\n    - The gradients for incorrect logits are small negative values, encouraging their logits $z_j$ to increase slightly.\n- This persistent, non-zero gradient prevents the model from becoming excessively certain, acting as a regularizer that can improve generalization and model calibration.\n\n### 3. Implementation Details\n\nThe implementation computes the required metrics for each test case.\n1.  **Stable Softmax:** To prevent numerical overflow/underflow with large logits, we use the shift-invariance property by subtracting the maximum logit value from all logits before exponentiation: $p_i = \\frac{e^{z_i - \\max(\\mathbf{z})}}{\\sum_j e^{z_j - \\max(\\mathbf{z})}}$.\n2.  **Target Vector Construction:** A target vector $\\mathbf{y}$ is created based on the correct class index $c$, the number of classes $K$, and the smoothing parameter $\\varepsilon$.\n3.  **Gradient Calculation:** The gradient $\\nabla_{\\mathbf{z}} L$ is computed simply as $\\mathbf{p} - \\mathbf{y}$.\n4.  **Metric Computation:**\n    - The margin $\\Delta$ is calculated by finding the maximum logit among incorrect classes and subtracting it from the correct class logit.\n    - The Euclidean ($\\ell_2$) norm of the gradient vector is computed using `np.linalg.norm`.\n    - The absolute gradient at the correct class index and the maximum absolute gradient across all classes are found using `np.abs` and `np.max`.\n5.  **Output Formatting:** The results are formatted into a string that represents a list of lists, with each floating-point number formatted to exactly six decimal places, and no whitespace.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and computes gradient metrics for the Softmax Cross-Entropy loss\n    for a given suite of test cases.\n    \"\"\"\n    # Test suite: each case is a tuple (z, c, epsilon)\n    # z: logits vector (list of floats)\n    # c: correct class index (int)\n    # epsilon: label smoothing parameter (float)\n    test_cases = [\n        ([0.1, 0.2, 0.15, 0.05], 1, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.0),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.0),\n        ([1000.0, 1000.0, 1000.0, 1000.0], 2, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.1),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.1),\n    ]\n\n    all_results = []\n    \n    for z_list, c, epsilon in test_cases:\n        z = np.array(z_list, dtype=np.float64)\n        K = len(z)\n\n        # 1. Compute softmax probabilities (numerically stable)\n        z_max = np.max(z)\n        exp_z = np.exp(z - z_max)\n        p = exp_z / np.sum(exp_z)\n\n        # 2. Compute the gradient vector (grad_L = p - y)\n        if K > 1:\n            y = np.full(K, epsilon / (K - 1))\n        else: # Handle edge case of K=1, though not in tests\n            y = np.array([1.0])\n        y[c] = 1.0 - epsilon\n        \n        grad_L = p - y\n\n        # 3. Compute the correct-class margin Delta\n        if K > 1:\n            mask = np.ones(K, dtype=bool)\n            mask[c] = False\n            max_z_incorrect = np.max(z[mask])\n            delta = z[c] - max_z_incorrect\n        else: # Only one class, so margin is ill-defined, use 0\n            delta = 0.0\n\n        # 4. Compute the required gradient metrics\n        norm_grad_L = np.linalg.norm(grad_L)\n        abs_grad_c = np.abs(grad_L[c])\n        max_abs_grad = np.max(np.abs(grad_L))\n\n        # Store the four required floats\n        case_result = [delta, norm_grad_L, abs_grad_c, max_abs_grad]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[val1,val2,...],[...],...] with no spaces and 6 decimal places.\n    inner_results_str = []\n    for result_vector in all_results:\n        formatted_values = [f'{v:.6f}' for v in result_vector]\n        inner_results_str.append(f\"[{','.join(formatted_values)}]\")\n\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3134219"}, {"introduction": "Knowing how to compute a gradient is one thing; understanding what happens when we apply these updates over thousands of steps is another. This exercise delves into the dynamics of training, focusing on the crucial concept of permutation symmetry among hidden units [@problem_id:3134207]. You will discover why identically initialized neurons fail to learn distinct features under deterministic training and see firsthand how stochasticity from methods like SGD and dropout is essential for breaking this symmetry, enabling the network to develop a rich and diverse set of internal representations.", "problem": "You will study permutation symmetry among hidden units in a one-hidden-layer artificial neural network and quantify how different training dynamics preserve or break that symmetry. Consider a fully connected network with one hidden layer of width $n$, input dimension $d$, and a scalar output, defined by\n$$\nf_{\\theta}(x) \\;=\\; \\sum_{k=1}^{n} v_k \\,\\phi\\!\\left(w_k^{\\top} x + b_k\\right) + c,\n$$\nwhere $\\theta = \\{(w_k,b_k,v_k)_{k=1}^{n}, c\\}$ are the parameters, $w_k \\in \\mathbb{R}^{d}$, $b_k \\in \\mathbb{R}$, $v_k \\in \\mathbb{R}$, $c \\in \\mathbb{R}$, and the activation function is $\\phi(z) = \\tanh(z)$. The empirical risk on a dataset $\\{(x_i,y_i)\\}_{i=1}^{m}$ is the Mean Squared Error (MSE),\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2.\n$$\n\nFundamental base and definitions:\n- The permutation group $S_n$ acts on the hidden units by permuting their indices. A permutation $\\pi \\in S_n$ sends $(w_k,b_k,v_k)$ to $(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})$ and leaves $c$ unchanged.\n- Gradient descent updates parameters by $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ for a learning rate $\\eta > 0$. Stochastic Gradient Descent (SGD) uses unbiased estimators of the gradient computed on mini-batches rather than the full dataset.\n- Dropout multiplies hidden activations by independent Bernoulli masks. In inverted dropout, each hidden unit activation is multiplied by a random mask with keep probability $q \\in (0,1]$ and then rescaled by $1/q$ to keep its expectation unchanged.\n\nTasks:\n1) Using only the above core definitions, argue from first principles that for any $\\pi \\in S_n$ one has $\\mathcal{L}(\\theta) = \\mathcal{L}(\\theta^{\\pi})$ where $\\theta^{\\pi}$ is the parameter tuple after permuting hidden units by $\\pi$. Explain why if all hidden units are initialized identically, i.e., $(w_k,b_k,v_k) = (w_1,b_1,v_1)$ for all $k$, then full-batch gradient descent without noise preserves this equality at every step. Your argument must rely on the symmetry of the loss under $S_n$ and the chain rule, not on any shortcut formulas.\n\n2) Implement, from the definitions, a program that constructs a dataset and trains the network under four distinct training regimes to quantify feature diversification via a diversity metric. Use the following precise setup, which must be adhered to verbatim for reproducibility and testability.\n\n- Dataset: set $m=64$, $d=2$, generate inputs $x_i \\in \\mathbb{R}^2$ by $x_i \\sim \\mathcal{N}(0,I_2)$ with random seed $s_{\\text{data}}=2025$. Define targets by\n$$\ny_i \\;=\\; \\sin(x_{i1}) + 0.5 \\cos(2 x_{i2}),\n$$\nwhere $x_{i1}$ and $x_{i2}$ are the two coordinates of $x_i$.\n\n- Network initialization: set width $n=3$. Initialize $W \\in \\mathbb{R}^{n \\times d}$ so that each row is identical, $w_k^{\\top} = [0.1,\\,-0.2]$ for all $k \\in \\{1,2,3\\}$, set $b_k = 0$ for all $k$, set $v_k = 1/n$ for all $k$, and set $c = 0$.\n\n- Training objective and gradients: use the exact MSE loss above and compute all gradients via the chain rule,\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right) x_i,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right),\n$$\nwhere $B$ is the batch size. For dropout with keep probability $q$, replace $\\phi(z)$ by $\\tilde{\\phi}(z) = \\frac{m}{q}\\phi(z)$ where $m \\sim \\mathrm{Bernoulli}(q)$ independently per hidden unit and per sample, and use the same masked activations in backpropagation.\n\n- Feature diversity metric: after training, compute the mean pairwise angle among hidden weight vectors,\n$$\n\\Delta(W) \\;=\\; \\frac{1}{\\binom{n}{2}}\\sum_{1 \\leq i  j \\leq n} \\arccos\\!\\left(\\frac{w_i^{\\top} w_j}{\\|w_i\\|\\,\\|w_j\\|}\\right),\n$$\nwith the convention that if $\\|w_i\\|\\,\\|w_j\\|$ is below a numerical threshold (e.g., below $10^{-12}$), treat the corresponding angle as $0$. All angles must be expressed in radians.\n\n- Training regimes (test suite): run the following four cases, each with learning rate $\\eta = 0.05$ and the same dataset, but with different sources of stochasticity. Use the specified random seed $s_{\\text{case}}$ for any stochastic operation internal to each case (e.g., dropout masks, gradient noise, data shuffling).\n  - Case $1$ (symmetry preserved): full-batch gradient descent with batch size $B = m = 64$, no dropout (keep probability $q=1$), no noise, epochs $T=200$, random seed $s_{\\text{case}}=101$.\n  - Case $2$ (additive gradient noise): same as Case $1$ but at each update add independent zero-mean Gaussian noise with standard deviation $\\sigma=10^{-3}$ to every component of the parameter gradients before the update, epochs $T=200$, random seed $s_{\\text{case}}=102$.\n  - Case $3$ (stochastic gradient descent with dropout): mini-batch size $B=1$ (that is, pure Stochastic Gradient Descent (SGD)), apply inverted dropout on hidden activations with keep probability $q=0.5$ independently per sample and per hidden unit, epochs $T=200$, random seed $s_{\\text{case}}=103$.\n  - Case $4$ (explicit infinitesimal symmetry breaking): same as Case $1$ except perturb the first hidden weight vector at initialization by adding $\\varepsilon$ to its first coordinate, with $\\varepsilon = 10^{-3}$, epochs $T=200$, random seed $s_{\\text{case}}=104$.\n\nYour program must:\n- Implement the above training dynamics exactly as specified, including seeding.\n- After each case, compute $\\Delta(W)$ in radians.\n- Produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, where each $\\Delta_k$ is rounded to exactly $6$ decimal places. Angles must be in radians.\n\nFinal output format:\n- A single line with the list $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, where each entry is a float with exactly $6$ digits after the decimal point, representing the mean pairwise angle in radians for Cases $1$ through $4$, in that order.", "solution": "The problem is well-posed, scientifically grounded in the fundamentals of neural networks and optimization, and provides a complete, unambiguous specification for a numerical experiment. It is therefore deemed valid.\n\nThe solution consists of two parts as requested: a theoretical argument regarding permutation symmetry, and the implementation of a numerical experiment.\n\n### Part 1: Theoretical Argument on Permutation Symmetry\n\n**1.1. Invariance of the Loss Function under Permutation**\n\nThe network's output is defined as $f_{\\theta}(x) = \\sum_{k=1}^{n} v_k \\phi(w_k^{\\top} x + b_k) + c$, where $\\theta$ represents the full set of parameters $\\{(w_k, b_k, v_k)_{k=1}^{n}, c\\}$. The activation function is $\\phi(z) = \\tanh(z)$.\n\nA permutation $\\pi \\in S_n$ acts on the hidden units by re-indexing their parameters. The permuted parameter set is $\\theta^{\\pi} = \\{(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})_{k=1}^{n}, c\\}$. The network function with these permuted parameters is:\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{k=1}^{n} v_{\\pi(k)} \\phi\\left(w_{\\pi(k)}^{\\top} x + b_{\\pi(k)}\\right) + c $$\nLet $j = \\pi(k)$. As $k$ iterates from $1$ to $n$, $j$ also covers all indices from $1$ to $n$ because $\\pi$ is a permutation. Therefore, the summation is merely a reordering of its terms, which does not change the sum's value due to the commutativity of addition:\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{j \\in \\{\\pi(1), \\dots, \\pi(n)\\}} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = \\sum_{j=1}^{n} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = f_{\\theta}(x) $$\nSince the function's output $f_{\\theta}(x)$ is identical for any $\\theta$ and its permutation $\\theta^{\\pi}$, the Mean Squared Error (MSE) loss function $\\mathcal{L}(\\theta)$ is also invariant under this permutation:\n$$ \\mathcal{L}(\\theta^{\\pi}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta^{\\pi}}(x_i) - y_i\\right)^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2 = \\mathcal{L}(\\theta) $$\nThis confirms that the loss function possesses $S_n$ permutation symmetry.\n\n**1.2. Preservation of Symmetry by Full-Batch Gradient Descent**\n\nWe now argue that if all hidden units are initialized identically, this symmetry is preserved at every step of full-batch gradient descent. We use an inductive argument.\n\n*Base Case (Initialization):* The parameters are initialized such that $(w_k^{(0)}, b_k^{(0)}, v_k^{(0)}) = (\\mathbf{w}_0, \\mathbf{b}_0, \\mathbf{v}_0)$ for all $k \\in \\{1, \\dots, n\\}$. The symmetry holds at step $t=0$.\n\n*Inductive Step:* Assume at some step $t$, the parameters for all hidden units are identical: $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) = (\\mathbf{w}_t, \\mathbf{b}_t, \\mathbf{v}_t)$ for all $k$. We must show that the parameters remain identical after one gradient descent step, i.e., $(w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)})$ is the same for all $k$.\n\nThe gradient descent update rule for a parameter $\\vartheta$ is $\\vartheta^{(t+1)} = \\vartheta^{(t)} - \\eta \\nabla_{\\vartheta} \\mathcal{L}(\\theta^{(t)})$. We need to show that the gradients with respect to the parameters of each hidden unit are identical.\n\nLet's examine the gradient components for an arbitrary hidden unit $k$. For full-batch gradient descent, $B=m$.\nThe gradient with respect to the output weight $v_k$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right) $$\nUnder the inductive hypothesis, $w_k^{(t)} = \\mathbf{w}_t$ and $b_k^{(t)} = \\mathbf{b}_t$ for all $k$. This means the term $\\phi(\\dots)$ is identical for all units. Furthermore, the function value $f_{\\theta^{(t)}}(x_i) = \\sum_{j=1}^n v_j^{(t)} \\phi((w_j^{(t)})^{\\top} x_i + b_j^{(t)}) + c^{(t)}$ is also independent of the choice of unit index $k$, as all units contribute identically to the sum. Thus, the entire expression for $\\frac{\\partial \\mathcal{L}}{\\partial v_k}$ is the same for all $k$.\n\nThe gradient with respect to the input weights $w_k$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) x_i $$\nBy the inductive hypothesis, $v_k^{(t)} = \\mathbf{v}_t$, $w_k^{(t)} = \\mathbf{w}_t$, and $b_k^{(t)} = \\mathbf{b}_t$. As before, $f_{\\theta^{(t)}}(x_i)$ is common. All terms within the summation are therefore identical for any choice of $k$. Thus, $\\frac{\\partial \\mathcal{L}}{\\partial w_k}$ is the same for all $k$.\n\nA similar argument holds for the bias term gradient:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) $$\nThis gradient is also identical for all $k$.\n\nSince the gradients $(\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L})$ are identical for all units $k$, and the parameters $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)})$ are identical, the update step\n$$ (w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)}) = (w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) - \\eta (\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L}) $$\nproduces a new set of parameters that is also identical for all units.\n\nBy induction, starting with identical parameters, full-batch gradient descent maintains this symmetry across all training steps. This leads to all hidden units learning the exact same feature, and the weight vectors $w_k$ remain collinear (in this case, identical), resulting in a diversity metric $\\Delta(W) = 0$. Any deviation from this symmetric setup (e.g., stochastic gradients, gradient noise, dropout, or non-identical initialization) will break the symmetry and cause the hidden units to diversify.\n\n### Part 2: Implementation\n\nThe following program implements the four specified training regimes and calculates the feature diversity metric $\\Delta(W)$ for each.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from problem\nM = 64\nD = 2\nN = 3\nETA = 0.05\nEPOCHS = 200\nNUM_THRESHOLD = 1e-12\n\ndef phi(z):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(z)\n\ndef phi_prime(z):\n    \"\"\"Derivative of the tanh activation function.\"\"\"\n    return 1 - np.tanh(z)**2\n\ndef calculate_diversity(w_matrix):\n    \"\"\"\n    Computes the mean pairwise angle between rows of the weight matrix W.\n    \"\"\"\n    total_angle = 0.0\n    num_pairs = 0\n    for i in range(N):\n        for j in range(i + 1, N):\n            w_i = w_matrix[i]\n            w_j = w_matrix[j]\n            \n            norm_i = np.linalg.norm(w_i)\n            norm_j = np.linalg.norm(w_j)\n            \n            denominator = norm_i * norm_j\n            if denominator  NUM_THRESHOLD:\n                angle = 0.0\n            else:\n                dot_product = np.dot(w_i, w_j)\n                # Clip for numerical stability of arccos\n                cosine_sim = np.clip(dot_product / denominator, -1.0, 1.0)\n                angle = np.arccos(cosine_sim)\n            \n            total_angle += angle\n            num_pairs += 1\n            \n    return total_angle / num_pairs if num_pairs > 0 else 0.0\n\ndef run_case(case_num, seed, batch_size, q, noise_std, initial_perturbation, X, Y):\n    \"\"\"\n    Runs a single training experiment as specified by the case parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialize parameters\n    W = np.array([[0.1, -0.2]] * N, dtype=np.float64)\n    b = np.zeros(N, dtype=np.float64)\n    v = np.ones(N, dtype=np.float64) / N\n    c = 0.0\n\n    # Apply perturbation for Case 4\n    if initial_perturbation > 0:\n        W[0, 0] += initial_perturbation\n    \n    indices = np.arange(M)\n\n    for epoch in range(EPOCHS):\n        if batch_size  M:  # For SGD, shuffle data each epoch\n            rng.shuffle(indices)\n\n        for i in range(0, M, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            \n            B = X_batch.shape[0]\n\n            # Forward pass\n            Z = X_batch @ W.T + b\n            A_pre = phi(Z)\n            \n            dropout_mask = None\n            if q  1.0:\n                dropout_mask = rng.binomial(1, q, size=A_pre.shape)\n                A_post = A_pre * dropout_mask / q\n            else:\n                A_post = A_pre\n\n            Y_pred = A_post @ v + c\n            \n            # Backward pass (compute gradients)\n            error_term = (2 / B) * (Y_pred - Y_batch)  # shape (B,)\n            \n            grad_c = np.sum(error_term)\n            grad_v = A_post.T @ error_term  # shape (n,)\n            \n            delta_out = error_term[:, np.newaxis] * v  # shape (B, n)\n            \n            dZ = delta_out * (1 - A_pre**2) # phi_prime(Z) is 1-phi(Z)^2 = 1-A_pre^2\n            \n            if q  1.0:\n                dZ = dZ * dropout_mask / q\n            \n            grad_b = np.sum(dZ, axis=0)  # shape (n,)\n            grad_W = dZ.T @ X_batch      # shape (n, d)\n\n            # Add gradient noise for Case 2\n            if noise_std > 0:\n                grad_W += rng.normal(0, noise_std, size=grad_W.shape)\n                grad_b += rng.normal(0, noise_std, size=grad_b.shape)\n                grad_v += rng.normal(0, noise_std, size=grad_v.shape)\n                grad_c += rng.normal(0, noise_std)\n\n            # Update parameters\n            W -= ETA * grad_W\n            b -= ETA * grad_b\n            v -= ETA * grad_v\n            c -= ETA * grad_c\n\n    return calculate_diversity(W)\n\ndef solve():\n    # Dataset generation\n    data_rng = np.random.default_rng(2025)\n    X = data_rng.normal(0, 1, size=(M, D))\n    Y = np.sin(X[:, 0]) + 0.5 * np.cos(2 * X[:, 1])\n\n    # Test suite definition\n    test_cases = [\n        # (case_num, seed, batch_size, q, noise_std, initial_perturbation)\n        (1, 101, M, 1.0, 0.0, 0.0),            # Case 1: Full-batch GD\n        (2, 102, M, 1.0, 1e-3, 0.0),           # Case 2: Gradient noise\n        (3, 103, 1, 0.5, 0.0, 0.0),            # Case 3: SGD with dropout\n        (4, 104, M, 1.0, 0.0, 1e-3),           # Case 4: Initial perturbation\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params, X, Y)\n        results.append(result)\n\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3134207"}, {"introduction": "After examining the mechanics of how networks learn, it is equally important to understand their fundamental limitations. A model's architecture intrinsically defines its expressive power—the range of functions it can possibly represent. This practice explores this frontier by introducing Graph Neural Networks (GNNs) and their connection to the Weisfeiler-Lehman test of graph isomorphism [@problem_id:3134285]. By implementing a simple message-passing GNN, you will empirically verify that certain non-isomorphic graphs are indistinguishable, providing a powerful lesson on how architectural design imposes hard limits on what a neural network can learn.", "problem": "You are asked to empirically demonstrate the expressivity limits of message-passing Graph Neural Networks (GNNs) via the one-dimensional Weisfeiler-Lehman (WL) color refinement test. The goal is to construct a pair of non-isomorphic graphs that are indistinguishable by the WL test, implement a simple message-passing GNN, and verify that it also fails to distinguish the pair under uniform initial node features. You must produce a complete, runnable program that constructs the specified graphs, runs the specified GNN, and returns results for a small test suite.\n\nFundamental base and definitions to use:\n- A graph $G$ is a pair $(V,E)$ where $V$ is the set of vertices and $E \\subseteq V \\times V$ is the set of edges. We represent $G$ by its adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ for $n = |V|$, where $A_{ij} = 1$ if and only if $(i,j) \\in E$ and $A_{ij} = 0$ otherwise.\n- A Graph Neural Network (GNN) in the message-passing framework computes node states layer-by-layer by aggregating neighbor information. In its most basic form appropriate for this task, let the initial node features be $x_v^{(0)} \\in \\mathbb{R}^d$ for each vertex $v \\in V$. For layer $t \\in \\{1, \\dots, L\\}$, define the update\n$$\nh^{(t)} = \\sigma\\!\\left(A \\, h^{(t-1)}\\right),\n$$\nwhere $h^{(t)} \\in \\mathbb{R}^{n \\times d}$ stacks the node features row-wise, $A$ is the adjacency matrix of $G$, and $\\sigma$ is the Rectified Linear Unit (ReLU) nonlinearity applied element-wise. A graph-level readout is defined by\n$$\nr(G) \\;=\\; \\sum_{v \\in V} h_v^{(L)} \\;\\in\\; \\mathbb{R}^d,\n$$\nthat is, the sum of the final-layer node features.\n- The one-dimensional Weisfeiler-Lehman (WL) color refinement test iteratively refines vertex colors based on the multiset of colors of neighbors. It is known that standard message-passing GNNs that aggregate neighbor multisets cannot be more discriminative than the one-dimensional WL test under uniform initial features.\n\nProgram requirements:\n1. Implement the above GNN with the following fixed hyperparameters and initialization:\n   - Number of layers $L = 2$.\n   - Feature dimension $d = 3$.\n   - Initial node feature for every vertex $v$ is $x_v^{(0)} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}$.\n   - Update rule $h^{(t)} = \\sigma\\!\\left(A \\, h^{(t-1)}\\right)$ for $t = 1,2$, using the same adjacency matrix $A$ for the graph under evaluation.\n   - Readout $r(G) = \\sum_{v \\in V} h_v^{(2)}$.\n\n2. Construct the following graphs, using $1$-based vertex labels in the description below and undirected edges (the adjacency matrix must be symmetric with zeros on the diagonal):\n   - Graph $G_1$ ($6$ vertices), cycle $C_6$: edges $\\{(1,2),(2,3),(3,4),(4,5),(5,6),(6,1)\\}$.\n   - Graph $G_2$ ($6$ vertices), disjoint union of two triangles: edges $\\{(1,2),(2,3),(3,1),(4,5),(5,6),(6,4)\\}$.\n   - Graph $G_3$ ($4$ vertices), path $P_4$: edges $\\{(1,2),(2,3),(3,4)\\}$.\n   - Graph $G_4$ ($4$ vertices), cycle $C_4$: edges $\\{(1,2),(2,3),(3,4),(4,1)\\}$.\n   - Graph $G_5$ ($4$ vertices), a vertex permutation of $G_4$ via $\\pi(1)=3$, $\\pi(2)=1$, $\\pi(3)=4$, $\\pi(4)=2$; that is, $(i,j)$ is an edge in $G_5$ if and only if $(\\pi^{-1}(i), \\pi^{-1}(j))$ is an edge in $G_4$.\n\n3. For each of the following test pairs $(G_a,G_b)$, compute graph-level embeddings $r(G_a)$ and $r(G_b)$ via the specified GNN and return a boolean indicating whether the embeddings are equal within tolerance $\\varepsilon = 10^{-9}$. That is, return $\\mathrm{True}$ if $\\|r(G_a) - r(G_b)\\|_\\infty \\le \\varepsilon$, otherwise $\\mathrm{False}$.\n   - Test case $1$: $(G_1,G_2)$, a pair of non-isomorphic $2$-regular graphs that are indistinguishable by the one-dimensional WL test.\n   - Test case $2$: $(G_3,G_4)$, a pair where the one-dimensional WL test does distinguish, since degrees differ across vertices.\n   - Test case $3$: $(G_4,G_5)$, a pair of isomorphic graphs related by a vertex permutation.\n\n4. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$). Each $result_i$ must be a boolean in Python’s textual form, either $\\mathrm{True}$ or $\\mathrm{False}$.\n\nThe test suite is designed to cover different facets: a non-isomorphic but WL-indistinguishable pair (expressivity limit), a pair that is distinguishable under WL and message passing (happy path), and an isomorphic pair under permutation (boundary condition showing permutation invariance). No physical units, angle units, or percentages are involved in this task; all outputs are booleans as specified.", "solution": "The problem requires an empirical demonstration of the expressive limitations of a simple message-passing Graph Neural Network (GNN). Specifically, we are tasked to show that such a GNN, analogous to the one-dimensional Weisfeiler-Lehman (WL) test, cannot distinguish between a specific pair of non-isomorphic graphs. The solution involves constructing the specified graphs, implementing the GNN, and evaluating its output on three test cases.\n\nFirst, we formalize the GNN model and the graph representations. A graph $G$ with $n$ vertices is represented by its $n \\times n$ adjacency matrix $A$, where $A_{ij}=1$ if an edge exists between vertices $i$ and $j$, and $A_{ij}=0$ otherwise. For the undirected graphs in this problem, $A$ is symmetric with a zero diagonal.\n\nThe GNN architecture is defined by the following:\n- Initial node features for all $n$ vertices are uniform: $h^{(0)} \\in \\mathbb{R}^{n \\times d}$ is a matrix where each row is the vector $[1, 1, 1]^T$. The feature dimension is $d=3$.\n- The GNN has $L=2$ layers. The feature update rule for layer $t \\in \\{1, 2\\}$ is:\n$$\nh^{(t)} = \\sigma\\!\\left(A \\, h^{(t-1)}\\right)\n$$\nwhere $\\sigma$ is the element-wise Rectified Linear Unit (ReLU) activation function, $\\sigma(x) = \\max(0, x)$.\n- The graph-level embedding, or readout, is the sum of the final node features:\n$$\nr(G) = \\sum_{i=1}^{n} (h^{(2)})_i\n$$\nwhere $(h^{(2)})_i$ is the $i$-th row of the final feature matrix $h^{(2)}$.\n\nThe core of the implementation will be a function that takes an adjacency matrix $A$ and computes $r(G)$ according to these rules. We will then analyze the three test cases.\n\n**Test Case 1: $G_1$ (a $6$-cycle, $C_6$) vs. $G_2$ (two disjoint $3$-cycles, $2C_3$)**\n\nThis pair is the canonical example of non-isomorphic graphs that are indistinguishable by the $1$-WL test. Both graphs are $2$-regular, meaning every vertex has a degree of $2$. This structural property is key to why the GNN fails.\n\n1.  **Layer 0**: The initial feature matrix $h^{(0)}$ is a $6 \\times 3$ matrix of ones for both graphs.\n\n2.  **Layer 1**: The first layer's pre-activation features are computed as $A h^{(0)}$. The $i$-th row of this product is the sum of the initial feature vectors of the neighbors of vertex $i$. Since all initial feature vectors are $[1, 1, 1]^T$ and every vertex in both $G_1$ and $G_2$ has degree $2$, the $i$-th row for any vertex in either graph is $2 \\times [1, 1, 1]^T = [2, 2, 2]^T$. The ReLU activation has no effect as all values are positive. Thus, for both $G_1$ and $G_2$, the feature matrix $h^{(1)}$ is a $6 \\times 3$ matrix where every entry is $2$.\n\n3.  **Layer 2**: The second layer's pre-activation is $A h^{(1)}$. We repeat the process. The $i$-th row is the sum of the neighbors' feature vectors from $h^{(1)}$. Since all feature vectors in $h^{(1)}$ are $[2, 2, 2]^T$ and every vertex still has degree $2$, the $i$-th row becomes $2 \\times [2, 2, 2]^T = [4, 4, 4]^T$. Again, ReLU has no effect. Therefore, for both graphs, the final feature matrix $h^{(2)}$ is a $6 \\times 3$ matrix where every entry is $4$.\n\n4.  **Readout**: The graph embedding $r(G)$ is the sum of the rows of $h^{(2)}$. For both $G_1$ and $G_2$, this is $6 \\times [4, 4, 4]^T = [24, 24, 24]^T$.\n\nSince $r(G_1) = r(G_2)$, the comparison $\\|r(G_a) - r(G_b)\\|_\\infty \\le 10^{-9}$ will evaluate to $\\mathrm{True}$. This confirms the GNN's inability to distinguish these graphs, demonstrating its expressive limitations.\n\n**Test Case 2: $G_3$ (a path of length $4$, $P_4$) vs. $G_4$ (a $4$-cycle, $C_4$)**\n\nThese graphs are distinguishable by the $1$-WL test because their degree distributions differ.\n-   $G_3$ has $4$ vertices with degrees $\\{1, 2, 2, 1\\}$.\n-   $G_4$ has $4$ vertices and is $2$-regular, with degrees $\\{2, 2, 2, 2\\}$.\n\nThe GNN will distinguish them at the first layer.\n1.  **Layer 1**: For $G_3$, the two end vertices (degree $1$) will have feature vectors $[1, 1, 1]^T$ in $h^{(1)}$, while the two inner vertices (degree $2$) will have $[2, 2, 2]^T$. For $G_4$, all four vertices will have feature vectors $[2, 2, 2]^T$ in $h^{(1)}$. The node feature multisets are different, a difference that will be preserved and propagated through subsequent layers.\n\n2.  **Readout**: Due to the differing feature matrices from layer $1$ onwards, the final embeddings will not be equal.\n-   For $G_3$, $r(G_3)=[10, 10, 10]^T$.\n-   For $G_4$, $r(G_4)=[16, 16, 16]^T$.\nSince $r(G_3) \\neq r(G_4)$, the comparison will evaluate to $\\mathrm{False}$.\n\n**Test Case 3: $G_4$ (a $4$-cycle, $C_4$) vs. $G_5$ (a permuted $C_4$)**\n\n$G_5$ is isomorphic to $G_4$, meaning it has the same structure but with vertices relabeled according to the permutation $\\pi$. A GNN with a sum-based readout is permutation-invariant. This means that isomorphic graphs must yield identical embeddings.\n\nLet $A_4$ and $A_5$ be the adjacency matrices of $G_4$ and $G_5$. If $P$ is the permutation matrix corresponding to $\\pi$, then $A_5 = P A_4 P^T$. The initial features $h^{(0)}$ are uniform and thus invariant under permutation, i.e., $P h^{(0)} = h^{(0)}$. By induction, one can show that $h^{(t)}(G_5) = P h^{(t)}(G_4)$ for all layers $t$.\n\nThe final readout for $G_5$ is $r(G_5)^T = \\mathbf{1}^T h^{(L)}(G_5) = \\mathbf{1}^T P h^{(L)}(G_4)$, where $\\mathbf{1}$ is a vector of ones. Since permuting the elements of a vector of ones results in the same vector, $\\mathbf{1}^T P = \\mathbf{1}^T$. Therefore, $r(G_5)^T = \\mathbf{1}^T h^{(L)}(G_4) = r(G_4)^T$. The embeddings must be identical. The comparison will evaluate to $\\mathrm{True}$.\n\nThe implementation will construct the adjacency matrices for these five graphs and apply the GNN computation as described to verify these theoretical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GNN expressivity problem by constructing graphs, running a GNN,\n    and comparing embeddings.\n    \"\"\"\n\n    def build_adjacency_matrix(num_vertices: int, edges: list[tuple[int, int]]) -> np.ndarray:\n        \"\"\"\n        Builds a symmetric adjacency matrix with a zero diagonal from a 1-based edge list.\n\n        Args:\n            num_vertices: The number of vertices in the graph.\n            edges: A list of tuples, where each tuple represents an undirected edge\n                   with 1-based vertex labels.\n\n        Returns:\n            A numpy array representing the adjacency matrix.\n        \"\"\"\n        adj_matrix = np.zeros((num_vertices, num_vertices), dtype=np.float64)\n        for u, v in edges:\n            # Convert 1-based vertex labels to 0-based indices\n            u_idx, v_idx = u - 1, v - 1\n            adj_matrix[u_idx, v_idx] = 1.0\n            adj_matrix[v_idx, u_idx] = 1.0\n        return adj_matrix\n\n    def run_gnn(adj_matrix: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Runs the specified 2-layer GNN on a graph given its adjacency matrix.\n\n        Args:\n            adj_matrix: The adjacency matrix of the graph.\n\n        Returns:\n            A numpy array representing the graph-level embedding.\n        \"\"\"\n        num_vertices = adj_matrix.shape[0]\n        feature_dim = 3\n        \n        # ReLU activation function\n        relu = lambda x: np.maximum(0, x)\n        \n        # Layer 0: Initial features are all ones\n        h0 = np.ones((num_vertices, feature_dim), dtype=np.float64)\n        \n        # Layer 1 update: h^(1) = sigma(A * h^(0))\n        h1 = relu(adj_matrix @ h0)\n        \n        # Layer 2 update: h^(2) = sigma(A * h^(1))\n        h2 = relu(adj_matrix @ h1)\n        \n        # Readout: Sum of final-layer node features\n        graph_embedding = np.sum(h2, axis=0)\n        \n        return graph_embedding\n\n    # --- Graph Definitions ---\n    # G1: 6-cycle (C6)\n    g1_edges = [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 1)]\n    A1 = build_adjacency_matrix(6, g1_edges)\n    \n    # G2: Two disjoint 3-cycles (2*C3)\n    g2_edges = [(1, 2), (2, 3), (3, 1), (4, 5), (5, 6), (6, 4)]\n    A2 = build_adjacency_matrix(6, g2_edges)\n    \n    # G3: Path graph (P4)\n    g3_edges = [(1, 2), (2, 3), (3, 4)]\n    A3 = build_adjacency_matrix(4, g3_edges)\n    \n    # G4: Cycle graph (C4)\n    g4_edges = [(1, 2), (2, 3), (3, 4), (4, 1)]\n    A4 = build_adjacency_matrix(4, g4_edges)\n\n    # G5: Permuted C4\n    # The permutation pi is: pi(1)=3, pi(2)=1, pi(3)=4, pi(4)=2.\n    # An edge (u,v) in G4 becomes (pi(u), pi(v)) in G5.\n    g5_edges = [(3, 1), (1, 4), (4, 2), (2, 3)]\n    A5 = build_adjacency_matrix(4, g5_edges)\n\n    # --- Compute Graph Embeddings ---\n    r1 = run_gnn(A1)\n    r2 = run_gnn(A2)\n    r3 = run_gnn(A3)\n    r4 = run_gnn(A4)\n    r5 = run_gnn(A5)\n    \n    # --- Perform Test Cases ---\n    test_cases = [\n        (r1, r2),  # Test 1: (G1, G2)\n        (r3, r4),  # Test 2: (G3, G4)\n        (r4, r5)   # Test 3: (G4, G5)\n    ]\n    \n    tolerance = 1e-9\n    results = []\n    for r_a, r_b in test_cases:\n        # Check if embeddings are equal using the infinity norm within tolerance\n        diff_norm = np.linalg.norm(r_a - r_b, ord=np.inf)\n        results.append(diff_norm = tolerance)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3134285"}]}