{"hands_on_practices": [{"introduction": "The integrity of your training, validation, and test splits is paramount for trustworthy model evaluation. This first exercise tackles a critical and common pitfall known as data leakage, which occurs when data preprocessing steps like augmentation are applied before splitting the dataset. By implementing and comparing a correct and an incorrect pipeline, you will quantify how this error invalidates the independence of your test set and leads to misleadingly optimistic performance metrics. [@problem_id:3194804]", "problem": "You are given a scenario where independently augmented data samples are created before the dataset is split into training, validation, and test splits. This can cause augmented near-duplicates of the same original sample to be distributed across distinct splits, a form of data leakage that invalidates unbiased model evaluation. Your task is to implement and analyze two pipelines in a self-contained program: a naive pipeline that augments before splitting, and a corrected pipeline that splits original samples first and then augments within each split. You will compute leakage metrics for both pipelines under a fixed test suite.\n\nUse the following foundational base:\n- A training, validation, and test split must approximate independent and identically distributed samples for unbiased evaluation. Data leakage occurs if information about a test example is indirectly present in the training data. If augmented near-duplicates of the same original sample appear across splits, the independence assumption is violated.\n- Define an original sample identity as a group identifier. All augmented samples derived from the same original belong to one equivalence class. Two augmented samples are called augmented twins if they come from the same original identity.\n- Let $x \\in \\mathbb{R}^d$ denote a base feature vector. An augmentation produces $x' = a \\cdot x + \\epsilon$, where $a$ is a small multiplicative jitter and $\\epsilon$ is small additive noise. For near-duplicate detection using only features, define the cosine similarity of two vectors $u, v \\in \\mathbb{R}^d$ as $\\mathrm{cos}(u,v) = \\dfrac{u^\\top v}{\\lVert u \\rVert_2 \\lVert v \\rVert_2}$. Two samples are near-duplicates if $\\mathrm{cos}(u,v) \\ge \\theta$ for a fixed threshold $\\theta$.\n- A well-formed split means that all augmented twins of a given original identity must reside in exactly one split. A leakage event occurs if twins are found in different splits.\n\nYour program must:\n1. Generate synthetic base samples and augmentations for each test case deterministically using the provided random seed. For each base identity, draw a base vector from a standard normal distribution in $\\mathbb{R}^d$ and normalize it to unit length. For each of $k$ augmentations, draw $a$ uniformly from the interval $[-0.01, 0.01]$ and $\\epsilon$ from a zero-mean isotropic Gaussian with standard deviation $\\sigma$, then construct $x' = (1 + a)\\, x + \\epsilon$ and normalize $x'$ to unit length.\n2. Implement two pipelines:\n   - Naive pipeline: create all augmentations for all identities, then randomly split the augmented set by sample-level shuffling into training, validation, and test splits according to ratios $(r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}})$, with $r_{\\mathrm{train}} + r_{\\mathrm{val}} + r_{\\mathrm{test}} = 1$. Let the first $\\lfloor N r_{\\mathrm{train}} \\rfloor$ samples be training, the next $\\lfloor N r_{\\mathrm{val}} \\rfloor$ be validation, and the remainder be test, where $N$ is the total number of augmented samples.\n   - Corrected pipeline: first randomly shuffle the base identities and split them by group into training, validation, and test identity sets using the same ratio rounding rule at the identity level. Then generate the $k$ augmentations per identity directly into the assigned split.\n3. Compute for each pipeline:\n   - Group leakage count $G$: the number of base identities whose augmented samples appear in at least two distinct splits.\n   - Cross-split near-duplicate pair count $P$: the number of pairs of samples across distinct splits whose cosine similarity is strictly greater than $\\theta$. Count only pairs with samples in different splits; sum over the three split pairs (train–validation, train–test, validation–test). All vectors must be normalized before cosine similarity is computed to ensure the dot product equals the cosine similarity.\n\nUse the following test suite. Each test case supplies $(n_{\\mathrm{base}}, k, d, r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}}, \\sigma, \\theta, \\text{seed})$:\n- Case $1$: $n_{\\mathrm{base}} = 50$, $k = 3$, $d = 128$, $(r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}}) = (0.7, 0.15, 0.15)$, $\\sigma = 0.01$, $\\theta = 0.99$, $\\text{seed} = 42$.\n- Case $2$ (boundary: no augmentation multiplicity): $n_{\\mathrm{base}} = 40$, $k = 1$, $d = 128$, $(r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}}) = (0.6, 0.2, 0.2)$, $\\sigma = 0.01$, $\\theta = 0.99$, $\\text{seed} = 123$.\n- Case $3$ (heavier augmentation): $n_{\\mathrm{base}} = 30$, $k = 10$, $d = 128$, $(r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}}) = (0.6, 0.2, 0.2)$, $\\sigma = 0.01$, $\\theta = 0.99$, $\\text{seed} = 7$.\n- Case $4$ (imbalanced ratios with tiny validation): $n_{\\mathrm{base}} = 7$, $k = 4$, $d = 128$, $(r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}}) = (0.8, 0.01, 0.19)$, $\\sigma = 0.01$, $\\theta = 0.99$, $\\text{seed} = 2023$.\n\nFor each test case, produce the following six integers in order:\n- $I_{\\mathrm{naive}}$: indicator equal to $1$ if $G_{\\mathrm{naive}} > 0$, else $0$.\n- $I_{\\mathrm{corr}}$: indicator equal to $1$ if $G_{\\mathrm{corr}} > 0$, else $0$.\n- $G_{\\mathrm{naive}}$: the group leakage count for the naive pipeline.\n- $G_{\\mathrm{corr}}$: the group leakage count for the corrected pipeline.\n- $P_{\\mathrm{naive}}$: the cross-split near-duplicate pair count for the naive pipeline.\n- $P_{\\mathrm{corr}}$: the cross-split near-duplicate pair count for the corrected pipeline.\n\nFinal output format:\nYour program should produce a single line of output containing all results flattened across the four test cases and printed as a single comma-separated list of integers enclosed in square brackets. The order must be case $1$ values first, then case $2$, then case $3$, then case $4$, each contributing six integers in the exact order specified above. For example, an output with two hypothetical cases would be formatted as $[I_1,I_2,G_1,G_2,P_1,P_2,\\dots]$. No additional text should be printed.", "solution": "The user has provided a problem statement concerning data leakage in machine learning workflows due to improper ordering of data augmentation and dataset splitting. The task is to implement and contrast two pipelines: a \"naive\" pipeline that augments before splitting, and a \"corrected\" pipeline that splits first, then augments. The analysis involves computing specific leakage metrics for each pipeline across a series of test cases.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Core Concept**: Data leakage occurs when augmented near-duplicates of an original sample are distributed across train, validation, and test splits, violating the i.i.d. assumption for unbiased model evaluation.\n- **Definitions**:\n    - **Original Sample Identity**: A group identifier for an original data sample.\n    - **Augmented Twins**: Samples derived from the same original identity.\n    - **Well-formed Split**: A split where all augmented twins of a given identity reside in exactly one partition (train, validation, or test).\n    - **Leakage Event**: An occurrence of augmented twins from the same identity being found in different splits.\n- **Augmentation Process**:\n    - An augmented sample $x'$ is generated from a base feature vector $x \\in \\mathbb{R}^d$ using the formula $x' = (1 + a) \\cdot x + \\epsilon$.\n    - The base vector $x$ is normalized to unit length.\n    - $a$ is drawn from a uniform distribution $U[-0.01, 0.01]$.\n    - $\\epsilon$ is a noise vector drawn from a zero-mean isotropic Gaussian distribution with standard deviation $\\sigma$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d)$.\n    - The final augmented vector $x'$ is also normalized to unit length.\n- **Near-Duplicate Detection**:\n    - Two samples $u, v$ are near-duplicates if their cosine similarity, $\\mathrm{cos}(u,v) = \\frac{u^\\top v}{\\lVert u \\rVert_2 \\lVert v \\rVert_2}$, is greater than or equal to a threshold $\\theta$. Since all vectors are normalized to unit length, this simplifies to $u^\\top v \\ge \\theta$. The problem statement specifies strictly greater than: $\\mathrm{cos}(u,v) > \\theta$.\n- **Pipeline Implementations**:\n    - **Naive Pipeline**: Generate all augmentations, then randomly shuffle and split the entire set of augmented samples into train/validation/test based on ratios $(r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}})$. Split sizes are $\\lfloor N r_{\\mathrm{train}} \\rfloor$, $\\lfloor N r_{\\mathrm{val}} \\rfloor$, and the remainder for test, where $N$ is the total number of augmented samples.\n    - **Corrected Pipeline**: Randomly shuffle and split the base identities into train/validation/test sets. Then, generate augmentations for each identity within its assigned split.\n- **Metrics to Compute**:\n    - **Group Leakage Count ($G$)**: The number of base identities whose augmented samples appear in at least two different splits.\n    - **Cross-split Near-duplicate Pair Count ($P$)**: The number of pairs of samples $(u, v)$ such that $u$ and $v$ are in different splits and $\\mathrm{cos}(u,v) > \\theta$. The count is summed over train-val, train-test, and val-test pairs.\n- **Test Suite**: Four cases are provided, each with parameters $(n_{\\mathrm{base}}, k, d, r_{\\mathrm{train}}, r_{\\mathrm{val}}, r_{\\mathrm{test}}, \\sigma, \\theta, \\text{seed})$.\n    - Case 1: $(50, 3, 128, 0.7, 0.15, 0.15, 0.01, 0.99, 42)$\n    - Case 2: $(40, 1, 128, 0.6, 0.2, 0.2, 0.01, 0.99, 123)$\n    - Case 3: $(30, 10, 128, 0.6, 0.2, 0.2, 0.01, 0.99, 7)$\n    - Case 4: $(7, 4, 128, 0.8, 0.01, 0.19, 0.01, 0.99, 2023)$\n- **Output**: For each case, six integer values are required: $I_{\\mathrm{naive}} = (G_{\\mathrm{naive}} > 0)$, $I_{\\mathrm{corr}} = (G_{\\mathrm{corr}} > 0)$, $G_{\\mathrm{naive}}$, $G_{\\mathrm{corr}}$, $P_{\\mathrm{naive}}$, $P_{\\mathrm{corr}}$. The final output must be a single flattened list of these integers.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically and mathematically sound. It addresses a real-world, critical issue in applied machine learning, namely data leakage during preprocessing. The definitions of augmentation, splitting, and the leakage metrics ($G$ and $P$) are precise and formalizable. The use of a deterministic random seed for each test case ensures that the problem is well-posed and has a unique, verifiable solution. All necessary parameters are provided for each test case. The problem is free from ambiguity, subjectivity, and factual errors.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed to construct the solution.\n\n### Algorithmic Design and Principles\n\nThe solution requires a deterministic implementation of data generation, two distinct data splitting pipelines, and two leakage metrics. The core of the problem is to demonstrate the structural difference between sample-wise splitting and group-wise splitting and to quantify the resulting data leakage.\n\n**1. Deterministic Data Generation**\n\nFor each test case, a random number generator will be seeded with the provided `seed`. This ensures reproducibility.\n-   **Base Samples**: For each of the $n_{\\mathrm{base}}$ identities, a base vector $x \\in \\mathbb{R}^d$ is drawn from a standard normal distribution $\\mathcal{N}(0, I_d)$. This vector is then normalized to have a Euclidean norm of $1$, i.e., $x \\leftarrow x / \\lVert x \\rVert_2$.\n-   **Augmentations**: For each base sample $x$, $k$ augmented samples are created. For each augmentation $x'$, a multiplicative jitter term $a$ is drawn from $U[-0.01, 0.01]$ and an additive noise vector $\\epsilon$ is drawn from $\\mathcal{N}(0, \\sigma^2 I_d)$. The augmented vector is computed as $x' = (1+a)x + \\epsilon$. Crucially, $x'$ is also normalized to unit length, $x' \\leftarrow x' / \\lVert x' \\rVert_2$. This normalization simplifies the cosine similarity calculation to a simple dot product, as $\\mathrm{cos}(u,v) = u^\\top v$ for unit vectors $u$ and $v$.\n\n**2. Naive Pipeline: Augment-then-Split**\n\nThis pipeline mimics the incorrect but common practice of applying augmentations before splitting the dataset.\n-   First, a total of $N = n_{\\mathrm{base}} \\times k$ augmented samples are generated and stored, along with their corresponding original base identity index (from $0$ to $n_{\\mathrm{base}}-1$).\n-   The set of $N$ samples is then randomly shuffled.\n-   The shuffled set is partitioned. The number of training samples is $N_{\\mathrm{train}} = \\lfloor N \\cdot r_{\\mathrm{train}} \\rfloor$. The number of validation samples is $N_{\\mathrm{val}} = \\lfloor N \\cdot r_{\\mathrm{val}} \\rfloor$. The test set comprises the remaining $N_{\\mathrm{test}} = N - N_{\\mathrm{train}} - N_{\\mathrm{val}}$ samples.\n-   This random, sample-level assignment is very likely to distribute augmented twins of the same original sample across different splits, leading to leakage.\n\n**3. Corrected Pipeline: Split-then-Augment**\n\nThis pipeline follows the correct procedure to prevent leakage from augmentations.\n-   First, the set of $n_{\\mathrm{base}}$ *base identities* is randomly shuffled.\n-   These identities are partitioned into training, validation, and test sets using the same ratio logic. The number of training identities is $n_{\\mathrm{train}} = \\lfloor n_{\\mathrm{base}} \\cdot r_{\\mathrm{train}} \\rfloor$, validation identities is $n_{\\mathrm{val}} = \\lfloor n_{\\mathrm{base}} \\cdot r_{\\mathrm{val}} \\rfloor$, and test identities is $n_{\\mathrm{test}} = n_{\\mathrm{base}} - n_{\\mathrm{train}} - n_{\\mathrm{val}}$.\n-   After the identities are assigned to splits, the augmentation process is performed *within* each split. For each identity in the training identity set, its $k$ augmentations are generated and added to the training data. The same is done for validation and test identity sets.\n-   By construction, all augmented versions of a single original sample are guaranteed to be in the same split.\n\n**4. Leakage Metrics Calculation**\n\nFor each pipeline's resulting splits, we calculate the two metrics:\n-   **Group Leakage Count ($G$)**: We iterate through each base identity from $0$ to $n_{\\mathrm{base}}-1$. For each identity, we find which splits contain its augmented descendants. If the number of unique splits is greater than $1$, we increment $G$. For the corrected pipeline, $G$ will deterministically be $0$.\n-   **Near-Duplicate Pair Count ($P$)**: We must count pairs of samples across different splits with cosine similarity greater than $\\theta$. This is done by computing the full matrix of dot products between the data matrices of pairs of splits. For splits with data $X_1 \\in \\mathbb{R}^{N_1 \\times d}$ and $X_2 \\in \\mathbb{R}^{N_2 \\times d}$, the similarity matrix is $S = X_1 X_2^\\top \\in \\mathbb{R}^{N_1 \\times N_2}$. The number of pairs with similarity above $\\theta$ is the number of elements in $S$ that are greater than $\\theta$. This count is summed over the train-validation, train-test, and validation-test pairs.\n\nThe final output for each test case consists of the six integers: $(G_{\\mathrm{naive}} > 0)$, $(G_{\\mathrm{corr}} > 0)$, $G_{\\mathrm{naive}}$, $G_{\\mathrm{corr}}$, $P_{\\mathrm{naive}}$, and $P_{\\mathrm{corr}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n_base, k, d, r_train, r_val, r_test, sigma, theta, seed)\n        (50, 3, 128, 0.7, 0.15, 0.15, 0.01, 0.99, 42),\n        (40, 1, 128, 0.6, 0.2, 0.2, 0.01, 0.99, 123),\n        (30, 10, 128, 0.6, 0.2, 0.2, 0.01, 0.99, 7),\n        (7, 4, 128, 0.8, 0.01, 0.19, 0.01, 0.99, 2023),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        n_base, k, d, r_train, r_val, r_test, sigma, theta, seed = params\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate base data\n        base_vectors = rng.standard_normal(size=(n_base, d))\n        base_vectors /= np.linalg.norm(base_vectors, axis=1, keepdims=True)\n\n        # 2. Run both pipelines and compute metrics\n        g_naive, p_naive = run_naive_pipeline(\n            base_vectors, k, d, r_train, r_val, sigma, theta, rng\n        )\n        g_corr, p_corr = run_corrected_pipeline(\n            base_vectors, k, d, r_train, r_val, sigma, theta, rng\n        )\n        \n        i_naive = 1 if g_naive > 0 else 0\n        i_corr = 1 if g_corr > 0 else 0\n\n        all_results.extend([i_naive, i_corr, g_naive, g_corr, p_naive, p_corr])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef generate_augmentations(base_vector, k, d, sigma, rng):\n    \"\"\"Generates k augmentations for a single base vector.\"\"\"\n    if k == 0:\n        return np.empty((0, d)), np.array([])\n    \n    a_jitter = rng.uniform(-0.01, 0.01, size=k)\n    noise = rng.normal(0, sigma, size=(k, d))\n    \n    # Broadcasting base_vector and a_jitter\n    augmented_vectors = (1 + a_jitter)[:, np.newaxis] * base_vector + noise\n    \n    norms = np.linalg.norm(augmented_vectors, axis=1, keepdims=True)\n    # Avoid division by zero, although highly unlikely\n    safe_norms = np.where(norms == 0, 1e-9, norms)\n    augmented_vectors /= safe_norms\n    \n    return augmented_vectors\n\ndef compute_metrics(splits_data, splits_ids, n_base, theta):\n    \"\"\"Computes group leakage (G) and near-duplicate pairs (P).\"\"\"\n    # Compute Group Leakage (G)\n    group_split_map = {i: set() for i in range(n_base)}\n    for split_name, ids in splits_ids.items():\n        for group_id in ids:\n            group_split_map[group_id].add(split_name)\n    \n    g_leakage = sum(1 for splits in group_split_map.values() if len(splits) > 1)\n\n    # Compute Near-Duplicate Pairs (P)\n    p_pairs = 0\n    split_names = list(splits_data.keys())\n    \n    for i in range(len(split_names)):\n        for j in range(i + 1, len(split_names)):\n            name1, name2 = split_names[i], split_names[j]\n            data1, data2 = splits_data[name1], splits_data[name2]\n            \n            if data1.shape[0] == 0 or data2.shape[0] == 0:\n                continue\n\n            similarity_matrix = data1 @ data2.T\n            p_pairs += np.sum(similarity_matrix > theta)\n            \n    return g_leakage, p_pairs\n\ndef run_naive_pipeline(base_vectors, k, d, r_train, r_val, sigma, theta, rng):\n    \"\"\"Implements the naive augment-then-split pipeline.\"\"\"\n    n_base = base_vectors.shape[0]\n    total_samples = n_base * k\n\n    if total_samples == 0:\n        return 0, 0\n        \n    all_augmented_data = np.empty((total_samples, d))\n    all_group_ids = np.empty(total_samples, dtype=int)\n\n    for i in range(n_base):\n        start_idx = i * k\n        end_idx = (i + 1) * k\n        all_augmented_data[start_idx:end_idx] = generate_augmentations(base_vectors[i], k, d, sigma, rng)\n        all_group_ids[start_idx:end_idx] = i\n\n    indices = rng.permutation(total_samples)\n    shuffled_data = all_augmented_data[indices]\n    shuffled_ids = all_group_ids[indices]\n\n    train_size = int(np.floor(total_samples * r_train))\n    val_size = int(np.floor(total_samples * r_val))\n\n    train_data = shuffled_data[0:train_size]\n    train_ids = shuffled_ids[0:train_size]\n\n    val_data = shuffled_data[train_size : train_size + val_size]\n    val_ids = shuffled_ids[train_size : train_size + val_size]\n\n    test_data = shuffled_data[train_size + val_size:]\n    test_ids = shuffled_ids[train_size + val_size:]\n\n    splits_data = {'train': train_data, 'val': val_data, 'test': test_data}\n    splits_ids = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n    \n    return compute_metrics(splits_data, splits_ids, n_base, theta)\n\n\ndef run_corrected_pipeline(base_vectors, k, d, r_train, r_val, sigma, theta, rng):\n    \"\"\"Implements the corrected split-then-augment pipeline.\"\"\"\n    n_base = base_vectors.shape[0]\n    \n    if n_base == 0:\n        return 0, 0\n\n    identity_indices = rng.permutation(n_base)\n    \n    train_id_size = int(np.floor(n_base * r_train))\n    val_id_size = int(np.floor(n_base * r_val))\n\n    train_ids = identity_indices[0:train_id_size]\n    val_ids = identity_indices[train_id_size : train_id_size + val_id_size]\n    test_ids = identity_indices[train_id_size + val_id_size:]\n\n    train_data = np.vstack([generate_augmentations(base_vectors[i], k, d, sigma, rng) for i in train_ids]) if len(train_ids) > 0 else np.empty((0, d))\n    val_data = np.vstack([generate_augmentations(base_vectors[i], k, d, sigma, rng) for i in val_ids]) if len(val_ids) > 0 else np.empty((0, d))\n    test_data = np.vstack([generate_augmentations(base_vectors[i], k, d, sigma, rng) for i in test_ids]) if len(test_ids) > 0 else np.empty((0, d))\n    \n    train_group_ids = np.repeat(train_ids, k)\n    val_group_ids = np.repeat(val_ids, k)\n    test_group_ids = np.repeat(test_ids, k)\n    \n    splits_data = {'train': train_data, 'val': val_data, 'test': test_data}\n    splits_ids = {'train': train_group_ids, 'val': val_group_ids, 'test': test_group_ids}\n\n    # By construction, G must be 0, but we compute it for verification.\n    return compute_metrics(splits_data, splits_ids, n_base, theta)\n\nsolve()\n```", "id": "3194804"}, {"introduction": "Once your data splits are correctly established, the validation set's primary role is to guide hyperparameter tuning. In this practice, you will work on a common anomaly detection task, using the validation set to find the optimal decision threshold that maximizes the $F_1$ score. This process will also reveal a key concept: overfitting to the validation set, where the performance achieved during tuning is often an overestimation of the true performance on the final, unseen test set. [@problem_id:3200874]", "problem": "You are given a binary anomaly detection setting with three disjoint datasets: a training set $\\mathcal{D}_{\\text{train}}$, a validation set $\\mathcal{D}_{\\text{val}}$, and a testing set $\\mathcal{D}_{\\text{test}}$. Each dataset consists of real-valued anomaly scores and ground-truth binary labels, where label $0$ denotes a normal instance and label $1$ denotes an anomalous instance. The fundamental base you must use is comprised of the following widely accepted definitions and rules. First, use training-only statistics to normalize scores by standardization: for any score $x$, define the standardized score $z$ as\n$$\nz = \\frac{x - \\mu_{\\text{train}}}{\\sigma_{\\text{train}}},\n$$\nwhere $\\mu_{\\text{train}}$ is the mean of scores in $\\mathcal{D}_{\\text{train}}$ and $\\sigma_{\\text{train}}$ is the standard deviation of scores in $\\mathcal{D}_{\\text{train}}$. Second, adopt the decision rule parameterized by a threshold $\\tau$: predict anomalous if $z \\ge \\tau$ and normal otherwise. Third, define the confusion matrix counts on a labeled dataset for any fixed $\\tau$ as True Positive (TP), False Positive (FP), False Negative (FN), and True Negative (TN). From these counts, define precision $P$ and recall $R$ by\n$$\nP = \\frac{TP}{TP+FP} \\quad \\text{if } TP+FP>0, \\text{ else } P = 0, \\qquad\nR = \\frac{TP}{TP+FN} \\quad \\text{if } TP+FN>0, \\text{ else } R = 0.\n$$\nThen the $F_1$ score is\n$$\nF_1 = \\begin{cases}\n\\frac{2PR}{P+R}, & \\text{if } P+R>0,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nYour task is to investigate thresholding on the validation set for anomaly detection as follows. For each test case, compute $\\mu_{\\text{train}}$ and $\\sigma_{\\text{train}}$ from $\\mathcal{D}_{\\text{train}}$, standardize $\\mathcal{D}_{\\text{val}}$ and $\\mathcal{D}_{\\text{test}}$ accordingly, vary the decision threshold $\\tau$ over the set of candidates consisting of all distinct standardized validation scores, together with $-\\infty$ and $+\\infty$, and select the threshold $\\tau^\\star$ that maximizes $F_1$ on $\\mathcal{D}_{\\text{val}}$. In case of ties (multiple $\\tau$ achieving the same maximal $F_1$), choose the largest $\\tau$ among them. Finally, evaluate $F_1$ on $\\mathcal{D}_{\\text{test}}$ at $\\tau^\\star$ and report the difference\n$$\ng = F_1(\\mathcal{D}_{\\text{val}};\\tau^\\star) - F_1(\\mathcal{D}_{\\text{test}};\\tau^\\star).\n$$\nThis quantity $g$ is the validation-based overestimation of test performance expressed as a decimal.\n\nTest suite specification:\nFor each case, the training scores are assumed to be normal-only samples. All numbers below are real-valued and must be interpreted exactly as written.\n\nCase $1$ (distributional shift causing overestimation):\n- $\\mathcal{D}_{\\text{train}}$ scores: $[\\,0.9,\\,1.0,\\,1.1,\\,1.0,\\,0.95,\\,1.05\\,]$\n- $\\mathcal{D}_{\\text{val}}$ raw scores: $[\\,0.7,\\,0.85,\\,0.95,\\,1.2,\\,2.0,\\,2.1,\\,1.9,\\,0.8\\,]$, labels: $[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,0\\,]$\n- $\\mathcal{D}_{\\text{test}}$ raw scores: $[\\,0.9,\\,0.98,\\,1.3,\\,1.4,\\,1.5,\\,1.6,\\,1.7,\\,1.8\\,]$, labels: $[\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$\n\nCase $2$ (small validation set overfitting):\n- $\\mathcal{D}_{\\text{train}}$ scores: $[\\,0.0,\\,0.1,\\,-0.1,\\,0.15,\\,-0.05\\,]$\n- $\\mathcal{D}_{\\text{val}}$ raw scores: $[\\,0.0,\\,0.05,\\,-0.05,\\,0.9\\,]$, labels: $[\\,0,\\,0,\\,0,\\,1\\,]$\n- $\\mathcal{D}_{\\text{test}}$ raw scores: $[\\,0.0,\\,0.1,\\,0.2,\\,-0.1,\\,0.3,\\,0.4,\\,0.85,\\,0.75\\,]$, labels: $[\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,]$\n\nCase $3$ (boundary case with no anomalies in validation):\n- $\\mathcal{D}_{\\text{train}}$ scores: $[\\,5.0,\\,5.2,\\,4.8,\\,5.1,\\,4.9\\,]$\n- $\\mathcal{D}_{\\text{val}}$ raw scores: $[\\,5.0,\\,5.1,\\,5.2,\\,5.3\\,]$, labels: $[\\,0,\\,0,\\,0,\\,0\\,]$\n- $\\mathcal{D}_{\\text{test}}$ raw scores: $[\\,5.0,\\,5.1,\\,5.2,\\,5.5,\\,6.0,\\,6.5\\,]$, labels: $[\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$\n\nImplementation requirements:\n- For each case, compute $g$ exactly as specified above, using the standardized scores based solely on $\\mathcal{D}_{\\text{train}}$ statistics.\n- Express each $g$ as a decimal rounded to $4$ places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example `[g_1,g_2,g_3]`, with each $g$ formatted to $4$ decimal places and no additional text.", "solution": "The problem requires us to calculate a performance gap, denoted by $g$, between a model's performance on a validation set and its performance on a test set. This is a standard procedure in machine learning for assessing model overfitting and the generalizability of a hyperparameter tuning process. The specific task involves a simple threshold-based anomaly detector. The solution methodology is deterministic and comprises several steps, which will be detailed below.\n\nFirst, a rigorous definition of the component metrics is essential. The problem states that for a dataset with anomaly scores $z$ and ground-truth binary labels $y \\in \\{0, 1\\}$, where $1$ indicates an anomaly, the decision rule is to predict anomaly if $z \\ge \\tau$ for a given threshold $\\tau$. This partitions the dataset into four categories:\n- True Positives ($TP$): Instances where $z \\ge \\tau$ and $y=1$.\n- False Positives ($FP$): Instances where $z \\ge \\tau$ and $y=0$.\n- False Negatives ($FN$): Instances where $z < \\tau$ and $y=1$.\n- True Negatives ($TN$): Instances where $z < \\tau$ and $y=0$.\n\nFrom these counts, we define Precision ($P$) and Recall ($R$):\n$$\nP = \\begin{cases} \\frac{TP}{TP+FP} & \\text{if } TP+FP > 0 \\\\ 0 & \\text{if } TP+FP = 0 \\end{cases}\n$$\n$$\nR = \\begin{cases} \\frac{TP}{TP+FN} & \\text{if } TP+FN > 0 \\\\ 0 & \\text{if } TP+FN = 0 \\end{cases}\n$$\nThe $F_1$ score, which is the harmonic mean of precision and recall, is then defined as:\n$$\nF_1 = \\begin{cases} \\frac{2PR}{P+R} & \\text{if } P+R > 0 \\\\ 0 & \\text{if } P+R = 0 \\end{cases}\n$$\n\nThe overall algorithm proceeds as follows:\n\n1.  **Data Standardization**: The first step is to establish a common scale for the anomaly scores. We compute the mean $\\mu_{\\text{train}}$ and population standard deviation $\\sigma_{\\text{train}}$ of the scores in the training set $\\mathcal{D}_{\\text{train}}$. It is specified that $\\mathcal{D}_{\\text{train}}$ contains only normal samples, which is a common practice for training some types of anomaly detectors. Every raw score $x$ in the validation set $\\mathcal{D}_{\\text{val}}$ and the test set $\\mathcal{D}_{\\text{test}}$ is then transformed into a standardized score $z$ using the training statistics:\n    $$\n    z = \\frac{x - \\mu_{\\text{train}}}{\\sigma_{\\text{train}}}\n    $$\n    This transformation is critical as it uses information *only* from the training data to prevent information leakage from the validation or test sets into the feature engineering process.\n\n2.  **Threshold Optimization on the Validation Set**: The decision threshold $\\tau$ is a hyperparameter. We optimize it by evaluating the $F_1$ score on the standardized validation set, $\\mathcal{D}_{\\text{val}}$. The set of candidate thresholds, $\\mathcal{T}$, is constructed from all distinct standardized scores in $\\mathcal{D}_{\\text{val}}$, augmented with $-\\infty$ and $+\\infty$. For each $\\tau \\in \\mathcal{T}$, we calculate the corresponding $F_1$ score. The optimal threshold $\\tau^\\star$ is the one that maximizes this $F_1$ score. In the event that multiple thresholds yield the same maximum $F_1$ score, the problem specifies a tie-breaking rule: select the largest value of $\\tau$ among the contenders. This yields a unique $\\tau^\\star$. The maximum $F_1$ score achieved on the validation set is denoted $F_1(\\mathcal{D}_{\\text{val}};\\tau^\\star)$.\n\n3.  **Performance Evaluation on the Test Set**: The generalization performance of the chosen threshold $\\tau^\\star$ is assessed on the unseen test set, $\\mathcal{D}_{\\text{test}}$. We apply the decision rule $z_{\\text{test}} \\ge \\tau^\\star$ to the standardized test scores and calculate the resulting $F_1$ score, denoted $F_1(\\mathcal{D}_{\\text{test}};\\tau^\\star)$.\n\n4.  **Gap Calculation**: Finally, the problem asks for the difference $g$ between the validation F1 score and the test F1 score:\n    $$\n    g = F_1(\\mathcal{D}_{\\text{val}};\\tau^\\star) - F_1(\\mathcal{D}_{\\text{test}};\\tau^\\star)\n    $$\n    This value $g$ quantifies the optimism of the validation-based performance estimate. A large positive value indicates that the model and hyperparameter choice were overfitted to the validation set.\n\nLet us apply this procedure to **Case 1**:\n\n- $\\mathcal{D}_{\\text{train}}$ scores: $[\\,0.9,\\,1.0,\\,1.1,\\,1.0,\\,0.95,\\,1.05\\,]$\n- $\\mu_{\\text{train}} = 1.0$\n- $\\sigma_{\\text{train}} = \\sqrt{\\frac{1}{6}\\sum(x_i - \\mu_{\\text{train}})^2} = \\sqrt{\\frac{0.025}{6}} \\approx 0.06455$\n\n- Standardize $\\mathcal{D}_{\\text{val}}$ scores: $X_{\\text{val}} = [\\,0.7, \\dots, 0.8\\,]$, $Y_{\\text{val}} = [\\,0, \\dots, 0\\,]$\n  The standardized scores are $Z_{\\text{val}} \\approx [\\, -4.648, -2.324, -0.775, 3.098, 15.492, 17.041, 13.943, -3.098 \\,]$.\n  The corresponding labels are $[\\,0, 0, 0, 0, 1, 1, 1, 0\\,]$. In $\\mathcal{D}_{\\text{val}}$, there are $3$ anomalies and $5$ normal instances.\n\n- The set of candidate thresholds $\\mathcal{T}$ consists of the $8$ unique values in $Z_{\\text{val}}$ plus $\\{-\\infty, +\\infty\\}$. We iterate through these $10$ candidates. For instance, if we select $\\tau$ equal to the standardized score of raw score $1.9$ (which is $z(1.9) \\approx 13.943$), any instance with a score greater than or equal to this value is predicted anomalous. The instances in $\\mathcal{D}_{\\text{val}}$ with scores $z \\ge 13.943$ correspond to raw scores $1.9, 2.0, 2.1$. Their true labels are all $1$. Thus, for this $\\tau$:\n  - $TP=3$, $FP=0$, $FN=0$, $TN=5$.\n  - $P = 3/(3+0)=1$. $R = 3/(3+0)=1$.\n  - $F_1 = (2 \\cdot 1 \\cdot 1) / (1+1) = 1.0$.\n  A full search reveals this is the maximum possible $F_1$ score, and it is unique. Thus, $\\tau^\\star \\approx 13.943$ and $F_1(\\mathcal{D}_{\\text{val}};\\tau^\\star) = 1.0$.\n\n- We now evaluate using $\\tau^\\star$ on $\\mathcal{D}_{\\text{test}}$. The standardized scores for $\\mathcal{D}_{\\text{test}}$ range from approximately $-1.549$ to $12.394$. None of these scores are greater than or equal to $\\tau^\\star \\approx 13.943$. Therefore, all test instances are classified as normal.\n  - $Y_{\\text{test}}$ contains $3$ anomalies and $5$ normal instances.\n  - The predictions are all normal, so $TP=0$ and $FP=0$. This implies $FN=3$ and $TN=5$.\n  - $P = 0/(0+0)=0$. $R = 0/(0+3)=0$.\n  - $F_1(\\mathcal{D}_{\\text{test}};\\tau^\\star) = 0$.\n\n- The gap is $g_1 = 1.0 - 0.0 = 1.0$.\n\nA similar procedure is followed for the other cases. Notably, in **Case 3**, the validation set $\\mathcal{D}_{\\text{val}}$ contains no anomalies ($TP+FN=0$). By definition, this means $R=0$ for any choice of $\\tau$, which in turn forces $F_1=0$. The maximum $F_1$ score is $0$, which is achieved by all candidate thresholds. The tie-breaking rule (\"choose the largest $\\tau$\") dictates that we must select $\\tau^\\star = +\\infty$. This threshold, when applied to the test set, also results in an $F_1$ score of $0$, making the gap $g_3=0$.\n\nThe implementation will systematically execute these steps for each provided case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the calculation of the performance gap 'g' for each case\n    and prints the final results in the specified format.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"train_scores\": [0.9, 1.0, 1.1, 1.0, 0.95, 1.05],\n            \"val_data\": (\n                [0.7, 0.85, 0.95, 1.2, 2.0, 2.1, 1.9, 0.8],\n                [0, 0, 0, 0, 1, 1, 1, 0]\n            ),\n            \"test_data\": (\n                [0.9, 0.98, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n                [0, 0, 0, 0, 0, 1, 1, 1]\n            )\n        },\n        {\n            \"train_scores\": [0.0, 0.1, -0.1, 0.15, -0.05],\n            \"val_data\": (\n                [0.0, 0.05, -0.05, 0.9],\n                [0, 0, 0, 1]\n            ),\n            \"test_data\": (\n                [0.0, 0.1, 0.2, -0.1, 0.3, 0.4, 0.85, 0.75],\n                [0, 0, 0, 0, 0, 0, 1, 1]\n            )\n        },\n        {\n            \"train_scores\": [5.0, 5.2, 4.8, 5.1, 4.9],\n            \"val_data\": (\n                [5.0, 5.1, 5.2, 5.3],\n                [0, 0, 0, 0]\n            ),\n            \"test_data\": (\n                [5.0, 5.1, 5.2, 5.5, 6.0, 6.5],\n                [0, 0, 0, 1, 1, 1]\n            )\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        g = process_case(case[\"train_scores\"], case[\"val_data\"], case[\"test_data\"])\n        results.append(f\"{g:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_f1(scores_z, labels, tau):\n    \"\"\"\n    Calculates the F1 score for a given dataset and threshold.\n    \n    Args:\n        scores_z (np.array): Standardized anomaly scores.\n        labels (np.array): Ground-truth binary labels (0=normal, 1=anomaly).\n        tau (float): The decision threshold.\n\n    Returns:\n        float: The calculated F1 score.\n    \"\"\"\n    predictions = (scores_z >= tau).astype(int)\n    \n    tp = np.sum((predictions == 1) & (labels == 1))\n    fp = np.sum((predictions == 1) & (labels == 0))\n    fn = np.sum((predictions == 0) & (labels == 1))\n    \n    p_denom = tp + fp\n    r_denom = tp + fn\n    \n    p = tp / p_denom if p_denom > 0 else 0.0\n    r = tp / r_denom if r_denom > 0 else 0.0\n    \n    f1_denom = p + r\n    f1 = (2 * p * r) / f1_denom if f1_denom > 0 else 0.0\n    \n    return f1\n\ndef process_case(train_scores, val_data, test_data):\n    \"\"\"\n    Processes a single test case to compute the performance gap 'g'.\n    \n    Args:\n        train_scores (list): Scores from the training set.\n        val_data (tuple): A tuple of (scores, labels) for the validation set.\n        test_data (tuple): A tuple of (scores, labels) for the test set.\n\n    Returns:\n        float: The performance gap g.\n    \"\"\"\n    train_scores_np = np.array(train_scores, dtype=np.float64)\n    mu_train = np.mean(train_scores_np)\n    sigma_train = np.std(train_scores_np)\n\n    def standardize(scores, mu, sigma):\n        if sigma == 0:\n            # Handle the case where all training scores are identical.\n            # Scores equal to mu get z=0, others +/- inf.\n            scores_np = np.array(scores, dtype=np.float64)\n            z = np.zeros_like(scores_np)\n            z[scores_np > mu] = np.inf\n            z[scores_np < mu] = -np.inf\n            return z\n        return (np.array(scores, dtype=np.float64) - mu) / sigma\n\n    val_scores_raw, val_labels_raw = val_data\n    val_scores_z = standardize(val_scores_raw, mu_train, sigma_train)\n    val_labels = np.array(val_labels_raw)\n\n    test_scores_raw, test_labels_raw = test_data\n    test_scores_z = standardize(test_scores_raw, mu_train, sigma_train)\n    test_labels = np.array(test_labels_raw)\n\n    # Identify candidate thresholds\n    candidate_taus = np.unique(val_scores_z)\n    candidate_taus = np.concatenate((candidate_taus, [-np.inf, np.inf]))\n    \n    # Find the best threshold on the validation set\n    f1_tau_pairs = []\n    for tau in candidate_taus:\n        f1 = calculate_f1(val_scores_z, val_labels, tau)\n        f1_tau_pairs.append((f1, tau))\n    \n    # Sort by F1 (desc) and then tau (desc) to apply tie-breaking rule\n    f1_tau_pairs.sort(key=lambda x: (x[0], x[1]), reverse=True)\n    \n    best_f1_val, tau_star = f1_tau_pairs[0]\n    \n    # Evaluate on the test set with the chosen threshold\n    f1_test = calculate_f1(test_scores_z, test_labels, tau_star)\n    \n    # Compute the gap\n    g = best_f1_val - f1_test\n    \n    return g\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3200874"}, {"introduction": "Validation sets are powerful tools that extend beyond simple hyperparameter selection, proving invaluable in specialized tasks like model calibration and extrapolation. This exercise situates you in a High-Performance Computing (HPC) context, where you must predict algorithm runtimes for large problem sizes based on data from smaller ones. You will discover how to use the validation set not just to pick a model, but to systematically correct its predictions, significantly improving its extrapolation accuracy for the test set. [@problem_id:3200889]", "problem": "You are modeling High-Performance Computing (HPC) runtime scaling as a function of problem size using supervised learning with explicit training, validation, and testing sets. The goal is to quantify extrapolation errors when models trained on small problem sizes are used to predict runtimes at much larger sizes, and to apply a correction based on a log-linear fit.\n\nFundamental base:\n- Definition of training, validation, and testing sets: the training set is used to fit model parameters, the validation set is used to tune or calibrate the model without refitting to the test data, and the testing set is used only for final performance evaluation.\n- Ordinary least squares regression: model parameters are chosen to minimize the sum of squared residuals on the training set.\n- Logarithm laws for linearization: if a quantity follows a power law $T(n) = \\alpha n^p$, then $\\ln T = \\ln \\alpha + p \\ln n$, which is linear in $\\ln n$ and $\\ln T$ for $T(n) > 0$.\n\nTask:\n- You are given four independent test cases. Each case provides a deterministic ground-truth HPC runtime function $T(n)$ and three disjoint sets of problem sizes: training (small $n$), validation (medium $n$), and testing (large $n$). For each case:\n  1. Fit two models using only the training set:\n     - Linear-in-$n$ model: $T(n) \\approx \\beta_0 + \\beta_1 n$, where $(\\beta_0,\\beta_1)$ are obtained by ordinary least squares minimizing $\\sum (t_i - (\\beta_0 + \\beta_1 n_i))^2$ over the training pairs $(n_i,t_i)$.\n     - Power-law (log-linear) model: $T(n) \\approx \\alpha n^p$, where $(\\alpha,p)$ are obtained by ordinary least squares in the logarithmic domain using the linearized relation $\\ln T \\approx \\gamma_0 + \\gamma_1 \\ln n$ with $\\alpha = e^{\\gamma_0}$ and $p = \\gamma_1$.\n  2. Calibrate only the power-law model using the validation set by a multiplicative factor $c$ that corrects systematic bias without changing the exponent $p$. Choose $c$ to minimize the mean squared error in the logarithmic domain on the validation set. Show that the multiplicative correction that minimizes the mean squared logarithmic error is\n     $$ c = \\exp\\left(\\frac{1}{m}\\sum_{j=1}^{m} \\left(\\ln t_j^{\\text{(val)}} - \\ln \\hat{t}_j^{\\text{(val)}}\\right)\\right), $$\n     where $m$ is the number of validation points, $t_j^{\\text{(val)}}$ are the true validation runtimes, and $\\hat{t}_j^{\\text{(val)}}$ are the validation predictions from the uncalibrated power-law model.\n     Use $c$ to form the calibrated prediction $\\hat{T}_{\\text{pow,corr}}(n) = c \\,\\alpha n^p$.\n  3. Evaluate all three predictors on the testing set (large $n$) using the mean absolute relative error (MARE):\n     $$ \\text{MARE} = \\frac{1}{k}\\sum_{i=1}^{k} \\left|\\frac{\\hat{t}_i - t_i}{t_i}\\right|, $$\n     where $k$ is the number of test points, $t_i$ are the true test runtimes, and $\\hat{t}_i$ are the predicted test runtimes. Report MARE as a decimal (not a percentage).\n  4. Determine which of the three predictors has the smallest testing MARE. Encode the winner as an integer index: $0$ for the linear-in-$n$ model, $1$ for the uncalibrated power-law model, and $2$ for the calibrated power-law model.\n\nTest suite:\n- For all cases, use the same problem sizes:\n  - Training sizes (small $n$): $[128, 256, 512]$.\n  - Validation sizes (medium $n$): $[1024, 1536]$.\n  - Testing sizes (large $n$): $[4096, 8192, 12288]$.\n- Ground-truth runtime functions $T(n)$ for each case (all times are positive, in arbitrary time units; you do not need to output units since the requested metric is dimensionless):\n  1. Case A (almost linear with a quadratic component and fixed overhead): \n     $$ T(n) = 2\\times 10^{-7}\\, n + 5\\times 10^{-10}\\, n^2 + 2\\times 10^{-3}. $$\n  2. Case B (power-law with exponent $p = 1.8$ and small fixed overhead):\n     $$ T(n) = 10^{-9}\\, n^{1.8} + 10^{-4}. $$\n  3. Case C (near $n\\log_2 n$ scaling with fixed overhead):\n     $$ T(n) = 3\\times 10^{-9}\\, n\\, \\log_2 n + 10^{-4}, $$\n     where $\\log_2$ denotes the base-$2$ logarithm.\n  4. Case D (superlinear power-law with additional linear term and small overhead):\n     $$ T(n) = 5\\times 10^{-12}\\, n^{2.5} + 10^{-6}\\, n + 10^{-4}. $$\n\nRequired final output:\n- For each case, compute the testing MARE of:\n  - the linear-in-$n$ model ($\\text{MARE}_{\\text{lin}}$),\n  - the uncalibrated power-law model ($\\text{MARE}_{\\text{pow}}$),\n  - the calibrated power-law model ($\\text{MARE}_{\\text{pow,corr}}$),\n  in that order, each rounded to exactly $6$ digits after the decimal point.\n- Also compute the integer code indicating the best model according to the smallest testing MARE, as defined above.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each case is a list of the three floats and the integer, in the form\n  $$ [[\\text{MARE}_{\\text{lin}}, \\text{MARE}_{\\text{pow}}, \\text{MARE}_{\\text{pow,corr}}, \\text{best}], \\ldots]. $$\n  For example, the printed line should look like\n  $$ [[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]], $$\n  with each $a_i,b_i,c_i$ rounded to $6$ decimal places and each $d_i$ an integer.\n\nAngle units are not applicable. No physical unit conversion is required. All outputs are dimensionless decimals. Ensure the program is fully deterministic and requires no input.", "solution": "The problem has been validated and is deemed scientifically sound, well-posed, and complete. It provides a clear computational task rooted in the standard practices of model fitting and evaluation in computational science.\n\nThe objective is to compare the extrapolation performance of three predictive models for High-Performance Computing (HPC) runtime scaling. The models are constructed using disjoint training and validation datasets, and their final performance is assessed on a held-out testing dataset. This procedure is performed for four different ground-truth runtime functions, $T(n)$.\n\nThe problem sizes are partitioned into three sets:\n- Training set ($n_{\\text{train}}$): $\\{128, 256, 512\\}$. Used for initial model parameter fitting.\n- Validation set ($n_{\\text{val}}$): $\\{1024, 1536\\}$. Used for model calibration.\n- Testing set ($n_{\\text{test}}$): $\\{4096, 8192, 12288\\}$. Used for final, unbiased performance evaluation.\n\nFor each of the four test cases, we evaluate three predictive models for runtime $T$ as a function of problem size $n$.\n\n**Model 1: Linear-in-$n$ Model**\nThis model assumes a linear relationship:\n$$ T(n) \\approx \\beta_0 + \\beta_1 n $$\nThe parameters $(\\beta_0, \\beta_1)$ are determined by ordinary least squares (OLS) regression on the training data $(n_i, t_i)$, where $t_i = T(n_i)$ for $n_i \\in n_{\\text{train}}$. This involves finding the coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ that minimize the sum of squared residuals. Given a design matrix $X$ and a vector of observations $\\mathbf{t}$:\n$$ X = \\begin{pmatrix} 1 & n_1 \\\\ 1 & n_2 \\\\ \\vdots & \\vdots \\\\ 1 & n_N \\end{pmatrix}, \\quad \\mathbf{t} = \\begin{pmatrix} t_1 \\\\ t_2 \\\\ \\vdots \\\\ t_N \\end{pmatrix} $$\nThe OLS solution is $\\boldsymbol{\\beta} = (X^T X)^{-1} X^T \\mathbf{t}$.\n\n**Model 2: Uncalibrated Power-Law Model**\nThis model assumes a power-law relationship:\n$$ T(n) \\approx \\alpha n^p $$\nTo fit the parameters $(\\alpha, p)$, the equation is linearized by taking the natural logarithm of both sides, assuming $T(n) > 0$ and $n > 0$:\n$$ \\ln T(n) \\approx \\ln \\alpha + p \\ln n $$\nThis is a linear relationship between $\\ln T$ and $\\ln n$. We define new variables $y = \\ln T$ and $x = \\ln n$, and a new linear model $y \\approx \\gamma_0 + \\gamma_1 x$, where $\\gamma_0 = \\ln \\alpha$ and $\\gamma_1 = p$. The parameters $(\\gamma_0, \\gamma_1)$ are found via OLS on the log-transformed training data $(\\ln n_i, \\ln t_i)$. The power-law parameters are then recovered as $\\alpha = e^{\\gamma_0}$ and $p = \\gamma_1$.\n\n**Model 3: Calibrated Power-Law Model**\nThis model refines the uncalibrated power-law model by introducing a multiplicative correction factor, $c$, intended to reduce systematic bias observed on the validation set. The model form is:\n$$ \\hat{T}_{\\text{pow,corr}}(n) = c \\cdot (\\alpha n^p) $$\nThe factor $c$ is determined by minimizing the mean squared error in the logarithmic domain on the validation data. The error function to minimize is:\n$$ E(c) = \\frac{1}{m} \\sum_{j=1}^{m} \\left( \\ln t_j^{\\text{(val)}} - \\ln \\hat{t}_{\\text{pow,corr},j}^{\\text{(val)}} \\right)^2 $$\nwhere $m$ is the number of validation points, $t_j^{\\text{(val)}}$ are the true runtimes, and $\\hat{t}_{\\text{pow,corr},j}^{\\text{(val)}} = c \\cdot \\hat{t}_{\\text{pow},j}^{\\text{(val)}}$ are the calibrated predictions. Substituting the model form:\n$$ E(c) = \\frac{1}{m} \\sum_{j=1}^{m} \\left( \\ln t_j^{\\text{(val)}} - (\\ln c + \\ln \\hat{t}_{\\text{pow},j}^{\\text{(val)}}) \\right)^2 = \\frac{1}{m} \\sum_{j=1}^{m} \\left( (\\ln t_j^{\\text{(val)}} - \\ln \\hat{t}_{\\text{pow},j}^{\\text{(val)}}) - \\ln c \\right)^2 $$\nTo find the minimum, we set the derivative with respect to $\\ln c$ to zero:\n$$ \\frac{dE}{d(\\ln c)} = \\frac{-2}{m} \\sum_{j=1}^{m} \\left( (\\ln t_j^{\\text{(val)}} - \\ln \\hat{t}_{\\text{pow},j}^{\\text{(val)}}) - \\ln c \\right) = 0 $$\nSolving for $\\ln c$:\n$$ \\ln c = \\frac{1}{m} \\sum_{j=1}^{m} \\left(\\ln t_j^{\\text{(val)}} - \\ln \\hat{t}_j^{\\text{(val)}}\\right) $$\nThis shows that $\\ln c$ is the mean of the logarithmic residuals on the validation set. The optimal correction factor $c$ is therefore:\n$$ c = \\exp\\left(\\frac{1}{m}\\sum_{j=1}^{m} \\left(\\ln t_j^{\\text{(val)}} - \\ln \\hat{t}_j^{\\text{(val)}}\\right)\\right) $$\nThis confirms the formula provided.\n\n**Evaluation**\nAll three models are evaluated on the testing set ($n_{\\text{test}}$) using the Mean Absolute Relative Error (MARE):\n$$ \\text{MARE} = \\frac{1}{k}\\sum_{i=1}^{k} \\left|\\frac{\\hat{t}_i - t_i}{t_i}\\right| $$\nwhere $k$ is the number of test points. The model with the lowest MARE on the test set is deemed the best for that case, encoded as an integer index: $0$ for linear, $1$ for uncalibrated power-law, and $2$ for calibrated power-law.\n\nThe procedure is applied to four ground-truth functions:\n1.  Case A: $T(n) = 2 \\times 10^{-7} n + 5 \\times 10^{-10} n^2 + 2 \\times 10^{-3}$\n2.  Case B: $T(n) = 10^{-9} n^{1.8} + 10^{-4}$\n3.  Case C: $T(n) = 3 \\times 10^{-9} n \\log_2 n + 10^{-4}$\n4.  Case D: $T(n) = 5 \\times 10^{-12} n^{2.5} + 10^{-6} n + 10^{-4}$\n\nThe following algorithm performs these calculations for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Fits, calibrates, and evaluates models for HPC runtime scaling to quantify\n    extrapolation error, following the problem specification.\n    \"\"\"\n\n    # Define problem sizes for training, validation, and testing.\n    n_train = np.array([128, 256, 512], dtype=float)\n    n_val = np.array([1024, 1536], dtype=float)\n    n_test = np.array([4096, 8192, 12288], dtype=float)\n\n    # Define the four ground-truth runtime functions as test cases.\n    def T_A(n):\n        return 2e-7 * n + 5e-10 * n**2 + 2e-3\n\n    def T_B(n):\n        return 1e-9 * n**1.8 + 1e-4\n\n    def T_C(n):\n        return 3e-9 * n * np.log2(n) + 1e-4\n\n    def T_D(n):\n        return 5e-12 * n**2.5 + 1e-6 * n + 1e-4\n\n    test_cases = [T_A, T_B, T_C, T_D]\n\n    def fit_linear_ols(x_data, y_data):\n        \"\"\"\n        Performs ordinary least squares for a model y = c0 + c1*x.\n        Returns: (c0, c1) - intercept and slope.\n        \"\"\"\n        X = np.vstack([np.ones(len(x_data)), x_data]).T\n        coeffs, _, _, _ = np.linalg.lstsq(X, y_data, rcond=None)\n        return coeffs[0], coeffs[1]\n\n    def calculate_mare(t_pred, t_true):\n        \"\"\"Calculates Mean Absolute Relative Error.\"\"\"\n        return np.mean(np.abs((t_pred - t_true) / t_true))\n\n    all_case_results = []\n    for T_func in test_cases:\n        # 1. Generate ground-truth data for each set.\n        t_train = T_func(n_train)\n        t_val = T_func(n_val)\n        t_test = T_func(n_test)\n\n        # 2. Fit models using only the training data.\n        \n        # Model 1: Linear-in-n model, T(n) ~ beta0 + beta1*n\n        beta0, beta1 = fit_linear_ols(n_train, t_train)\n\n        # Model 2: Power-law model, T(n) ~ alpha * n^p\n        # Fit via log-linearization: ln(T) ~ ln(alpha) + p*ln(n)\n        log_n_train = np.log(n_train)\n        log_t_train = np.log(t_train)\n        gamma0, gamma1 = fit_linear_ols(log_n_train, log_t_train)\n        alpha = np.exp(gamma0)\n        p = gamma1\n\n        # 3. Calibrate the power-law model using the validation set.\n        \n        # Get predictions from the uncalibrated power-law model on validation data.\n        t_val_pred_pow = alpha * (n_val**p)\n        \n        # Calculate the log-domain residuals.\n        log_residuals = np.log(t_val) - np.log(t_val_pred_pow)\n        \n        # The optimal correction factor 'c' is exp(mean of log residuals).\n        c = np.exp(np.mean(log_residuals))\n        \n        # 4. Evaluate all three predictors on the testing set.\n        \n        # Predictions for the test set from each model.\n        t_test_pred_lin = beta0 + beta1 * n_test\n        t_test_pred_pow = alpha * (n_test**p)\n        t_test_pred_pow_corr = c * t_test_pred_pow\n        \n        # Calculate MARE for each model.\n        mare_lin = calculate_mare(t_test_pred_lin, t_test)\n        mare_pow = calculate_mare(t_test_pred_pow, t_test)\n        mare_pow_corr = calculate_mare(t_test_pred_pow_corr, t_test)\n\n        mares = [mare_lin, mare_pow, mare_pow_corr]\n\n        # 5. Determine the best model (smallest MARE).\n        best_model_idx = int(np.argmin(mares))\n        \n        # Append results for the current case.\n        all_case_results.append(mares + [best_model_idx])\n\n    # 6. Format the final output string as specified.\n    # Each MARE is formatted to exactly 6 decimal places.\n    # The final string is a list of lists with no spaces.\n    case_strings = []\n    for result_list in all_case_results:\n        mare_vals = result_list[:3]\n        best_idx_val = result_list[3]\n        \n        mare_strs = [f\"{m:.6f}\" for m in mare_vals]\n        \n        # Format: [mare1,mare2,mare3,best_idx]\n        inner_list_str = f\"[{','.join(mare_strs)},{best_idx_val}]\"\n        case_strings.append(inner_list_str)\n\n    # Format: [[...],[...],[...],[...]]\n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3200889"}]}