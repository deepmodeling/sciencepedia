{"hands_on_practices": [{"introduction": "Principal Component Analysis (PCA) is more than just a tool for creating 2D plots; its mathematical outputs have direct and powerful applications. This first practice demonstrates how the loadings from a principal component, which quantify the contribution of each original variable, can be used to engineer a new and meaningful feature. By calculating a \"metagene score,\" you will gain hands-on experience in translating the abstract results of PCA into a tangible biological insight [@problem_id:1428922].", "problem": "In systems biology, Principal Component Analysis (PCA) is a powerful technique for reducing the dimensionality of large datasets, such as gene expression profiles from cancer patients. A common application is to derive a \"metagene score\" that summarizes the activity of a biological pathway.\n\nA researcher is studying a specific metabolic pathway in cancer cells that involves four key genes: Gene A, Gene B, Gene C, and Gene D. They perform PCA on a large gene expression dataset and find that the first principal component (PC1) strongly correlates with the overall activity of this pathway. They decide to use the loadings of PC1 as weights to create a pathway activity score for new samples.\n\nThe PC1 loadings for the four genes are as follows:\n- Gene A: 0.65\n- Gene B: 0.55\n- Gene C: -0.40\n- Gene D: 0.30\n\nA new tumor sample is analyzed, and the standardized expression values (z-scores) for these four genes are measured:\n- Gene A: 2.20\n- Gene B: 1.80\n- Gene C: 1.50\n- Gene D: -1.10\n\nThe pathway activity score for this new sample is calculated as the sum of the products of each gene's expression value and its corresponding PC1 loading. Calculate the pathway activity score for this new tumor sample. Round your final answer to three significant figures.", "solution": "The problem asks us to calculate a \"pathway activity score\" for a new tumor sample. The score is defined as the sum of the products of each gene's standardized expression value and its corresponding Principal Component 1 (PC1) loading. This is equivalent to calculating the dot product of the expression vector and the loading vector.\n\nLet the vector of PC1 loadings be $\\mathbf{w}$ and the vector of gene expression values be $\\mathbf{g}$.\nThe loading vector $\\mathbf{w}$ for Genes A, B, C, and D is given as:\n$$ \\mathbf{w} = \\begin{pmatrix} 0.65 \\\\ 0.55 \\\\ -0.40 \\\\ 0.30 \\end{pmatrix} $$\n\nThe gene expression vector $\\mathbf{g}$ for the new sample is given as:\n$$ \\mathbf{g} = \\begin{pmatrix} 2.20 \\\\ 1.80 \\\\ 1.50 \\\\ -1.10 \\end{pmatrix} $$\n\nThe pathway activity score, $S$, is calculated as the dot product $\\mathbf{w} \\cdot \\mathbf{g}$:\n$$ S = \\sum_{i=1}^{4} w_i g_i = w_A g_A + w_B g_B + w_C g_C + w_D g_D $$\n\nNow, we substitute the given numerical values into this formula:\n$$ S = (0.65)(2.20) + (0.55)(1.80) + (-0.40)(1.50) + (0.30)(-1.10) $$\n\nWe calculate each product term by term:\n- For Gene A: $0.65 \\times 2.20 = 1.43$\n- For Gene B: $0.55 \\times 1.80 = 0.99$\n- For Gene C: $-0.40 \\times 1.50 = -0.60$\n- For Gene D: $0.30 \\times -1.10 = -0.33$\n\nNow, we sum these products to get the final score:\n$$ S = 1.43 + 0.99 - 0.60 - 0.33 $$\n$$ S = 2.42 - 0.60 - 0.33 $$\n$$ S = 1.82 - 0.33 $$\n$$ S = 1.49 $$\n\nThe problem asks for the answer to be rounded to three significant figures. The calculated value $1.49$ already has three significant figures.\n\nTherefore, the pathway activity score is 1.49.", "answer": "$$\\boxed{1.49}$$", "id": "1428922"}, {"introduction": "A crucial skill in computational science is not just applying an algorithm, but critically interpreting its output. PCA excels at identifying the largest sources of variance in a dataset, but these may not always correspond to the biological effect you wish to study. This exercise presents a realistic scenario where PCA uncovers a hidden \"batch effect\"—a systematic error introduced during data collection—teaching a vital lesson in data analysis and experimental awareness [@problem_id:1428916].", "problem": "A systems biology researcher is investigating the effect of a new drug, \"DrugX,\" on the gene expression profile of a specific type of cancer cell line. The experiment is designed as follows: 8 identical cultures of the cancer cells are prepared. 4 cultures are treated with DrugX (the \"Treated\" group), and the other 4 are treated with a placebo (the \"Control\" group).\n\nDue to time constraints in the lab, the researcher processes all 4 Control samples for RNA-sequencing on a Monday. The next day, on Tuesday, the researcher processes all 4 Treated samples. After obtaining the expression levels for 20,000 genes for each of the 8 samples, the researcher performs Principal Component Analysis (PCA), a dimensionality reduction technique that identifies the directions of maximum variance in a dataset.\n\nWhen plotting the first principal component (PC1) against the second principal component (PC2), the researcher observes that the 8 samples form two very distinct, non-overlapping clusters. Upon labeling the samples, it becomes clear that one cluster contains all 4 samples processed on Monday (the Control group), and the other cluster contains all 4 samples processed on Tuesday (the Treated group). The separation between these two clusters is the most prominent feature of the plot.\n\nBased on this PCA result, what is the most significant source of variation in this gene expression dataset?\n\nA. The biological effect of DrugX on the cancer cells' gene expression.\n\nB. A systematic, non-biological difference related to the day the samples were processed.\n\nC. Random statistical noise inherent in the RNA-sequencing measurement process.\n\nD. Natural genetic variation between the 8 different cell cultures.\n\nE. The variation captured by the second principal component (PC2).", "solution": "The goal of this problem is to correctly interpret the results of a Principal Component Analysis (PCA) in the context of a biological experiment.\n\nStep 1: Understand the purpose of PCA.\nPrincipal Component Analysis is a statistical procedure used to reduce the dimensionality of a dataset while retaining as much of the original variation as possible. It achieves this by transforming the data into a new set of variables, the principal components (PCs), which are a sequence of uncorrelated linear combinations of the original variables. The first principal component (PC1) is constructed to account for the largest possible variance in the data. The second principal component (PC2) accounts for the second-largest variance, and so on.\n\nStep 2: Analyze the described PCA result.\nThe problem states that when the data is plotted, the samples form two distinct clusters. This means there is a large, systematic difference between the samples in one cluster and the samples in the other. Furthermore, the problem explicitly states that this clustering perfectly separates the samples processed on Monday from those processed on Tuesday. Since PC1 and PC2 are used for the plot and show this clear separation, it means that the largest sources of variation in the dataset (captured primarily by PC1) are strongly correlated with the processing day.\n\nStep 3: Evaluate the experimental design and potential sources of variation.\nThe experiment was designed to measure the difference between the \"Control\" and \"Treated\" groups. However, the experimental procedure introduced a confounding variable: all Control samples were processed on one day (Monday), and all Treated samples were processed on another day (Tuesday). This creates a \"batch effect,\" where technical variations between processing runs (e.g., different reagent preparations, slight differences in ambient temperature, or machine calibration on different days) can introduce systematic, non-biological changes in the data.\n\nStep 4: Evaluate the given options based on the analysis.\n\nA. The biological effect of DrugX on the cancer cells' gene expression.\nIf this were the most significant source of variation, the PCA plot would show a clear separation between the \"Control\" and \"Treated\" groups. While the observed clusters do correspond to Control vs. Treated, they *also* correspond perfectly to Monday vs. Tuesday. Because the experimental design has confounded these two factors, we cannot be sure the separation is due to the drug. The fact that processing occurred on different days provides a strong alternative explanation for a systematic difference. This phenomenon, a batch effect, is a very common and powerful source of variation in high-throughput biology, often larger than the biological effect of interest. The phrasing \"systematic, non-biological difference\" is a more precise description of a batch effect.\n\nB. A systematic, non-biological difference related to the day the samples were processed.\nThis is the most likely explanation. The PCA found that the largest variance in the data aligns perfectly with the processing day. This indicates a strong batch effect. The differences between lab work done on Monday and Tuesday (reagents, machine state, environmental factors) have introduced a systematic signal that is larger than other sources of variation. The PCA has correctly identified this as the dominant pattern.\n\nC. Random statistical noise inherent in the RNA-sequencing measurement process.\nRandom noise would cause the data points to be scattered, but it would not create two distinct, tight clusters. Clustering is a sign of a systematic, not a random, effect.\n\nD. Natural genetic variation between the 8 different cell cultures.\nThe problem states that 8 *identical* cultures of the cancer cells were prepared. This implies they are from the same cell line and are as biologically identical as possible, so significant genetic variation is not expected to be the primary driver of differences. Even if there were minor variations, they would not be expected to correlate perfectly with the processing day.\n\nE. The variation captured by the second principal component (PC2).\nThis statement is fundamentally a misunderstanding of PCA. By definition, PC1 captures the greatest amount of variance in the data. PC2 captures the second greatest. Therefore, the variation captured by PC2 cannot be the *most significant* source of variation overall.\n\nConclusion: The most prominent feature of the PCA plot (the clustering by day) reflects the largest source of variance in the data. This variance is attributable to the systematic differences in how the two batches of samples were handled, which is a batch effect.", "answer": "$$\\boxed{B}$$", "id": "1428916"}, {"introduction": "While linear methods like PCA are robust, non-linear algorithms such as t-SNE are essential for visualizing more complex data manifolds. However, their power comes with the challenge of parameter tuning. This practice explores the `perplexity` parameter in t-SNE, which controls the algorithm's balance between focusing on local neighborhood structures versus the global data layout. By analyzing the trade-offs, you will develop the intuition needed to effectively apply t-SNE to uncover both rare subpopulations and large-scale patterns in complex datasets [@problem_id:1428872].", "problem": "A systems biologist is analyzing a high-dimensional single-cell RNA sequencing (scRNA-seq) dataset comprising 15,000 cells, with each cell characterized by the expression levels of 20,000 distinct genes. From prior biological studies, it is known that this dataset should contain a few large, phenotypically similar cell populations as well as several rare but distinct subpopulations that are critical for a specific biological function.\n\nTo visualize the cellular heterogeneity, the biologist uses the t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm. The perplexity parameter in t-SNE can be conceptualized as a soft measure of the number of effective nearest neighbors considered for each cell when learning the data structure. The biologist performs two separate analyses with different perplexity settings:\n\n1.  **Run 1 (Perplexity = 5):** The resulting 2D plot shows excellent separation of the known rare subpopulations, each forming a small, tight, and isolated cluster. However, the large, dominant cell populations appear to be fractured into multiple, seemingly disjointed smaller clusters, making it difficult to discern their overall relationship.\n2.  **Run 2 (Perplexity = 100):** The resulting 2D plot shows the large cell populations as cohesive and well-defined \"continents,\" providing a clear view of the global structure of the major cell types. However, in this visualization, the rare subpopulations are no longer identifiable as distinct clusters; they appear to be absorbed into the peripheries of the larger populations.\n\nBased on these observations and the principles of the t-SNE algorithm, which of the following statements provides the most accurate interpretation of the results?\n\nA. A low perplexity value makes the t-SNE algorithm prioritize preserving the local structure of the data, which is effective at resolving rare, dense clusters. A high perplexity value causes the algorithm to prioritize a more global structure, which is better for visualizing the relationships between large populations but can obscure small ones.\n\nB. A low perplexity value makes the t-SNE algorithm focus on the global structure, which is why the major populations are fractured. A high perplexity value makes the algorithm focus on the local structure, which is why the rare populations are merged into the larger ones.\n\nC. The high perplexity run is likely overfitting to noise in the dataset, creating artificial continents of cells. The low perplexity run is more robust and reveals the true, fragmented nature of all cell populations.\n\nD. The distances between the large \"continents\" in the high perplexity plot can be confidently interpreted as a measure of their transcriptional dissimilarity. The low perplexity plot distorts these large-scale distances, which is why it is not useful for global analysis.\n\nE. The difference in outcomes is because the perplexity parameter implicitly sets the number of clusters t-SNE expects to find. A low perplexity corresponds to a high number of expected clusters, while a high perplexity corresponds to a low number of expected clusters.", "solution": "t-SNE constructs high-dimensional conditional similarities $p_{j|i}$ using a Gaussian kernel $p_{j|i} \\propto \\exp\\!\\left(-\\frac{\\|x_{i}-x_{j}\\|^{2}}{2\\sigma_{i}^{2}}\\right)$, choosing $\\sigma_{i}$ so that the Shannon entropy $H(P_{i})$ of $P_{i}=\\{p_{j|i}\\}_{j}$ yields the user-chosen perplexity via $Perp(P_{i})=2^{H(P_{i})}$. A smaller perplexity forces smaller $\\sigma_{i}$, concentrating mass on very few nearest neighbors (emphasis on very local structure). A larger perplexity forces larger $\\sigma_{i}$, spreading mass over many neighbors (emphasis on broader, more mesoscopic structure). The low-dimensional similarities $q_{ij}$ are defined with a heavy-tailed Student kernel, and t-SNE minimizes the Kullback–Leibler divergence $\\mathrm{KL}(P\\|Q)$, which preferentially preserves local neighborhoods (it penalizes missing neighbors much more than introducing spurious distant neighbors), making t-SNE intrinsically local.\n\nInterpreting the runs:\n- Perplexity $=5$: With very few effective neighbors, the method excels at resolving small, tight, rare clusters (their strong local density is preserved) but can fragment large, heterogeneous populations into multiple pieces because only very local relationships are emphasized, preventing the cohesion of broader structures.\n- Perplexity $=100$: With many effective neighbors, broader structures are emphasized, enabling large populations to appear as cohesive continents. However, very small rare clusters can be absorbed into larger neighborhoods and lose distinct separation.\n\nAssessing the options:\n- A correctly states that low perplexity emphasizes local structure (good for rare, dense clusters) and high perplexity emphasizes a more global or mesoscopic structure (good for large-population relationships but can obscure small clusters).\n- B reverses the local/global emphasis and is inconsistent with t-SNE’s known behavior and the observations.\n- C incorrectly attributes the high perplexity result to overfitting and claims the low perplexity reveals true fragmentation; the behavior is driven by neighborhood scale, not overfitting noise.\n- D claims that inter-continent distances in t-SNE can be confidently interpreted as transcriptional dissimilarity; t-SNE does not preserve global distances reliably, so this is incorrect.\n- E incorrectly claims perplexity sets the expected number of clusters; perplexity controls the effective neighborhood size, not the number of clusters.\n\nTherefore, the correct interpretation is in A.", "answer": "$$\\boxed{A}$$", "id": "1428872"}]}