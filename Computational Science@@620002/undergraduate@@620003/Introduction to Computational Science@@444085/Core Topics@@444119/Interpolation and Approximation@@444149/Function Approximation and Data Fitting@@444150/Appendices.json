{"hands_on_practices": [{"introduction": "In many scientific experiments, we collect data that is corrupted by noise. A fundamental task is to estimate the rate of change, or derivative, from this noisy data, but naive methods like finite differences can drastically amplify the noise. This practice [@problem_id:3133528] introduces a much more robust method: the Savitzky-Golay filter, which fits local polynomials to the data to compute a smoothed derivative. Through a computational experiment, you will investigate the essential bias-variance trade-off, learning how the filter's window size affects the balance between smoothing away noise and preserving the true underlying signal.", "problem": "You are given a smooth scalar function sampled on an equally spaced grid and corrupted by additive independent identically distributed noise. Your task is to approximate the gradient, that is the first derivative, from the noisy samples using Savitzky-Golay polynomial smoothing, and to quantify the bias-variance trade-off induced by the window size choice. You must design a program that carries out a Monte Carlo assessment with a fixed random seed to ensure deterministic output. Base your derivation on the following foundational principles: the definition of the derivative, Taylor series expansion of smooth functions, least-squares polynomial fitting on a local window, and the definitions of statistical bias and variance.\n\nSetup. Let the true function be\n$$\nf(x) = e^{\\sin(2\\pi x)} + \\frac{1}{2} x^3,\n$$\nwith derivative\n$$\ng(x) = \\frac{df}{dx}(x) = e^{\\sin(2\\pi x)} \\cdot \\cos(2\\pi x) \\cdot 2\\pi + \\frac{3}{2} x^2,\n$$\nwhere the argument of the trigonometric functions is in radians. Consider a uniform grid of size $$N=201$$ on the interval $$[0,1]$$ given by $$x_i = \\frac{i}{N-1}$$ for $$i=0,1,\\dots,N-1$$, with spacing $$\\Delta = \\frac{1}{N-1}$$. For each Monte Carlo replication, generate noisy observations\n$$\ny_i = f(x_i) + \\varepsilon_i,\n$$\nwhere $$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$$ are independent across $$i$$. Use a fixed number of replications $$S=200$$ and a fixed random seed $$12345$$ for reproducibility.\n\nEstimator. For a given odd window length $$w$$ and polynomial order $$p$$ with $$w > p \\ge 0$$, define the Savitzky-Golay derivative estimator $$\\widehat{g}_i$$ at point $$x_i$$ as the first derivative at $$x_i$$ of the polynomial of degree $$p$$ that minimizes the sum of squared residuals over the window of length $$w$$ centered at $$x_i$$ (with appropriate handling near boundaries), divided by $$\\Delta$$ for correct scaling to the units of $$g(x)$$.\n\nBias-variance assessment. For each configuration $$(w,p,\\sigma)$$, run $$S$$ replications to obtain $$\\widehat{g}_i^{(s)}$$ for $$s=1,\\dots,S$$ and $$i=0,\\dots,N-1$$. Let\n$$\n\\overline{g}_i = \\frac{1}{S}\\sum_{s=1}^S \\widehat{g}_i^{(s)}\n$$\nbe the empirical mean of the estimator at grid location $$i$$, and let\n$$\n\\widehat{\\operatorname{Var}}_i = \\frac{1}{S-1}\\sum_{s=1}^S \\left(\\widehat{g}_i^{(s)} - \\overline{g}_i\\right)^2\n$$\nbe the sample variance at location $$i$$. Define the root-mean-square bias as\n$$\n\\operatorname{RMSBias} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left(\\overline{g}_i - g(x_i)\\right)^2}\n$$\nand the mean variance as\n$$\n\\operatorname{MeanVar} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\widehat{\\operatorname{Var}}_i.\n$$\n\nTest suite. Use the following four configurations to assess the bias-variance trade-off under different window sizes and noise levels, keeping the polynomial order fixed at $$p=3$$:\n- Case $$1$$ (small window): $$(w,p,\\sigma) = (5,3,0.05)$$.\n- Case $$2$$ (moderate window): $$(w,p,\\sigma) = (21,3,0.05)$$.\n- Case $$3$$ (large window): $$(w,p,\\sigma) = (61,3,0.05)$$.\n- Case $$4$$ (higher noise): $$(w,p,\\sigma) = (21,3,0.20)$$.\n\nYour program must:\n- Use the grid of size $$N=201$$ on $$[0,1]$$ with spacing $$\\Delta = \\frac{1}{200}$$.\n- For each case, generate $$S=200$$ independent noisy datasets with the same fixed seed $$12345$$ for reproducibility and estimate the derivative using Savitzky-Golay smoothing with the specified $$(w,p)$$ and derivative order $$1$$. Use radian measure for all trigonometric functions throughout.\n- Compute $$\\operatorname{RMSBias}$$ and $$\\operatorname{MeanVar}$$ as defined above.\n- Round each reported number to exactly $$6$$ decimal places.\n\nFinal output format. Your program should produce a single line of output containing a comma-separated list of the results for all test cases, in order, each case contributing two floats $$(\\operatorname{RMSBias}, \\operatorname{MeanVar})$$, yielding a flat list of $$8$$ numbers. The line must be enclosed in square brackets, for example\n$$\n[\\text{bias}_1,\\text{var}_1,\\text{bias}_2,\\text{var}_2,\\text{bias}_3,\\text{var}_3,\\text{bias}_4,\\text{var}_4].\n$$\nNo additional text should be printed. All numeric values must be unitless in this problem, and angles are to be interpreted in radians. Round all printed numbers to $$6$$ decimal places.", "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded exercise in computational science focused on function approximation and the bias-variance trade-off. All necessary parameters, functions, and definitions are provided, and there are no contradictions or ambiguities.\n\nThe solution will be a numerical experiment based on a Monte Carlo simulation. The core steps for each test case are:\n1.  **Grid and Function Definition**: A uniform grid of $N=201$ points is defined on the interval $[0,1]$. The true function $f(x) = e^{\\sin(2\\pi x)} + \\frac{1}{2} x^3$ and its analytical derivative $g(x) = \\frac{df}{dx}(x) = 2\\pi e^{\\sin(2\\pi x)} \\cos(2\\pi x) + \\frac{3}{2} x^2$ are evaluated at these grid points.\n\n2.  **Monte Carlo Simulation**: For each configuration of window size $w$, polynomial order $p$, and noise standard deviation $\\sigma$, a simulation of $S=200$ replications is performed. A fixed random seed of $12345$ is used at the start of the simulation for each configuration to ensure reproducibility and to allow for a direct comparison of the effect of changing parameters against an identical set of random noise samples.\n    - In each replication $s \\in \\{1, \\dots, S\\}$, noisy data $y_i^{(s)} = f(x_i) + \\varepsilon_i^{(s)}$ are generated, where $\\varepsilon_i^{(s)}$ are drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$.\n    - The Savitzky-Golay filter is applied to the noisy data $\\{y_i^{(s)}\\}$ to estimate the first derivative. This is accomplished using `scipy.signal.savgol_filter` with `deriv=1`. The filter locally fits a polynomial of degree $p$ to the data points within a window of size $w$ and computes the derivative of this polynomial. The `delta` argument of the function is set to the grid spacing $\\Delta = 1/(N-1)$ to ensure the derivative is correctly scaled. This process yields the estimated derivative $\\widehat{g}_i^{(s)}$ for each grid point $x_i$.\n\n3.  **Bias and Variance Calculation**: After $S$ replications, the collected estimates $\\{\\widehat{g}_i^{(s)}\\}_{s=1}^S$ for each grid point $i$ are used to compute performance metrics.\n    - The empirical mean of the estimator at each point is calculated as $\\overline{g}_i = \\frac{1}{S}\\sum_{s=1}^S \\widehat{g}_i^{(s)}$.\n    - The squared bias at each point is $(\\overline{g}_i - g(x_i))^2$. The overall figure of merit, the Root-Mean-Square Bias, is then computed as $\\operatorname{RMSBias} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} (\\overline{g}_i - g(x_i))^2}$.\n    - The sample variance of the estimator at each point is calculated using the unbiased estimator formula $\\widehat{\\operatorname{Var}}_i = \\frac{1}{S-1}\\sum_{s=1}^S (\\widehat{g}_i^{(s)} - \\overline{g}_i)^2$.\n    - The overall Mean Variance is the average of these sample variances over all grid points: $\\operatorname{MeanVar} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\widehat{\\operatorname{Var}}_i$.\n\n4.  **Analysis of Trade-off**: The pairs $(\\operatorname{RMSBias}, \\operatorname{MeanVar})$ are computed for four distinct test cases:\n    - Case 1: $(w,p,\\sigma) = (5,3,0.05)$ (small window)\n    - Case 2: $(w,p,\\sigma) = (21,3,0.05)$ (medium window)\n    - Case 3: $(w,p,\\sigma) = (61,3,0.05)$ (large window)\n    - Case 4: $(w,p,\\sigma) = (21,3,0.20)$ (high noise)\n\nThis procedure allows for a quantitative assessment of the bias-variance trade-off. Typically, increasing the window size $w$ leads to a decrease in variance (due to averaging over more noisy points) but an increase in bias (as the local polynomial has to fit a wider, potentially more curved, segment of the function, leading to systematic error). Increasing the noise level $\\sigma$ primarily increases the variance, with minimal effect on the bias. The final results are then formatted to the required precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import savgol_filter\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo assessment of the Savitzky-Golay derivative estimator\n    to quantify the bias-variance trade-off.\n    \"\"\"\n    # Define simulation parameters from the problem statement.\n    N = 201\n    S = 200\n    SEED = 12345\n\n    # Define the test cases.\n    test_cases = [\n        # (w, p, sigma)\n        (5, 3, 0.05),   # Case 1: small window\n        (21, 3, 0.05),  # Case 2: moderate window\n        (61, 3, 0.05),  # Case 3: large window\n        (21, 3, 0.20),  # Case 4: higher noise\n    ]\n\n    # Grid setup\n    x = np.linspace(0.0, 1.0, N)\n    delta = 1.0 / (N - 1)\n\n    # True function and its derivative\n    def f(t):\n        return np.exp(np.sin(2 * np.pi * t)) + 0.5 * t**3\n\n    def g(t):\n        return (np.exp(np.sin(2 * np.pi * t)) * np.cos(2 * np.pi * t) * 2 * np.pi\n                + 1.5 * t**2)\n\n    f_x = f(x)\n    g_x = g(x)\n\n    results = []\n\n    for w, p, sigma in test_cases:\n        # For each case, re-initialize the random number generator with the same seed\n        # for a fair comparison of parameters on identical noise patterns.\n        rng = np.random.default_rng(SEED)\n\n        # Array to store derivative estimates from all S replications\n        all_g_hats = np.zeros((S, N))\n\n        # Monte Carlo simulation loop\n        for s in range(S):\n            # Generate noisy observations\n            noise = rng.normal(loc=0.0, scale=sigma, size=N)\n            y = f_x + noise\n\n            # Estimate the derivative using Savitzky-Golay filter\n            g_hat = savgol_filter(y, window_length=w, polyorder=p,\n                                  deriv=1, delta=delta)\n            all_g_hats[s, :] = g_hat\n\n        # Calculate statistics after all replications are complete\n        \n        # Empirical mean of the estimator at each grid point\n        g_bar = np.mean(all_g_hats, axis=0)\n        \n        # Sample variance of the estimator at each grid point (ddof=1 for S-1 denominator)\n        var_hat = np.var(all_g_hats, axis=0, ddof=1)\n\n        # Compute overall RMS Bias and Mean Variance\n        squared_bias = (g_bar - g_x)**2\n        rms_bias = np.sqrt(np.mean(squared_bias))\n        mean_var = np.mean(var_hat)\n\n        results.append(rms_bias)\n        results.append(mean_var)\n\n    # Format the results for final output\n    # All numbers are rounded to exactly 6 decimal places.\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3133528"}, {"introduction": "Not all functions are smooth. Many important functions in modern science, particularly in machine learning, feature sharp \"kinks\" where their derivatives are discontinuous, a prime example being the Rectified Linear Unit (ReLU) function, $f(x) = \\max(0, x)$. This exercise [@problem_id:3133577] challenges you to approximate this non-differentiable function using two powerful techniques: global polynomial least-squares fitting and cubic smoothing splines. By comparing the results, you will gain insight into the strengths and weaknesses of each method when faced with a function that lacks smoothness, learning to quantify the error in both the function and its derivative.", "problem": "You are given the task of approximating the Rectified Linear Unit (ReLU) function, defined as $f(x) = \\max(0, x)$, on finite intervals using smooth approximations. The goal is to construct two families of approximants and to quantify their accuracy in terms of maximum function error and maximum derivative mismatch. The work must be grounded in fundamental principles of least-squares approximation and smoothing variational formulations.\n\nConstruct and evaluate the following approximants:\n- A polynomial least-squares approximant: For a chosen interval $[a,b]$ and polynomial degree $n$, select $N_{\\text{fit}}$ uniformly spaced sample points $\\{x_i\\}_{i=1}^{N_{\\text{fit}}}$ on $[a,b]$ and form an approximant $p_n(x)$ that minimizes the discrete least-squares objective $\\sum_{i=1}^{N_{\\text{fit}}} \\left(p_n(x_i) - f(x_i)\\right)^2$. Evaluate the resulting approximation and its derivative on a fine evaluation grid.\n- A cubic smoothing spline approximant: For the same interval $[a,b]$, sample the function at $N_{\\text{fit}}$ uniformly spaced points as above and construct a cubic smoothing spline $s(x)$ that balances fidelity to the data and smoothness. The spline is defined as the minimizer of a functional of the form $\\sum_{i=1}^{N_{\\text{fit}}} \\left(s(x_i) - f(x_i)\\right)^2 + \\lambda \\int_{a}^{b} \\left(s''(x)\\right)^2 \\, dx$, where $\\lambda \\ge 0$ denotes a smoothness penalty. Use a standard cubic spline ($k=3$) and a specified smoothing parameter. Evaluate the resulting approximation and its derivative on a fine evaluation grid.\n\nDefine the accuracy metrics to be reported:\n- The maximum function error over the interval $[a,b]$ computed on a fine grid of $N_{\\text{eval}}$ points, given by\n$$E_{\\max} = \\max_{x \\in [a,b]} \\left|g(x) - f(x)\\right|,$$\nwhere $g(x)$ denotes the approximant (either $p_n(x)$ or $s(x)$).\n- The maximum derivative mismatch over $[a,b] \\setminus \\{0\\}$ computed on a fine grid and excluding $x=0$ where $f'(x)$ is undefined, given by\n$$D_{\\max} = \\max_{x \\in [a,b] \\setminus \\{0\\}} \\left|g'(x) - f'(x)\\right|,$$\nwith $f'(x)=0$ for $x<0$ and $f'(x)=1$ for $x>0$.\n\nImplementation requirements:\n- Use uniform grids for both fitting and evaluation.\n- For polynomial least-squares approximation, derive $p_n(x)$ by solving the linear least-squares problem for the coefficients against the monomial basis $\\{1, x, x^2, \\dots, x^n\\}$ formed on the fitting grid.\n- For cubic smoothing splines, use a cubic spline ($k=3$) with the specified smoothing parameter and uniform fit grid; the spline must be differentiable and its derivative $s'(x)$ must be evaluated on the fine grid.\n- The derivative of the Rectified Linear Unit (ReLU) function $f'(x)$ is piecewise defined as $0$ for $x<0$ and $1$ for $x>0$, and is excluded at $x=0$.\n\nEvaluation grid and numerical details:\n- Use $N_{\\text{eval}} = 10001$ uniformly spaced points on each interval $[a,b]$ for evaluation.\n- When computing $D_{\\max}$, exclude the point $x=0$ from the evaluation grid by omitting points with $|x| \\le \\varepsilon$, where $\\varepsilon = 10^{-12}$.\n\nTest suite:\n- Polynomial case $1$: $[a,b]=[-1,1]$, degree $n=3$, $N_{\\text{fit}}=501$.\n- Polynomial case $2$: $[a,b]=[-3,3]$, degree $n=7$, $N_{\\text{fit}}=2001$.\n- Spline case $1$: $[a,b]=[-1,1]$, smoothing parameter $s=0.0$, $N_{\\text{fit}}=201$.\n- Spline case $2$: $[a,b]=[-1,1]$, smoothing parameter $s=0.01$, $N_{\\text{fit}}=201$.\n- Spline case $3$: $[a,b]=[-1,1]$, smoothing parameter $s=10.0$, $N_{\\text{fit}}=201$.\n\nAnswer specification:\n- For each test case, compute and return a list containing two floats $[E_{\\max}, D_{\\max}]$, each rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, specifically a list of these per-case lists. For example, an output with three cases would look like $[[e_1,d_1],[e_2,d_2],[e_3,d_3]]$, where each $e_i$ and $d_i$ are decimal floats rounded to six places.\n\nAll quantities are dimensionless. Angles are not involved in this problem. No external input or files should be used; the program must be self-contained.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the established principles of numerical analysis and function approximation, is well-posed with all necessary information provided, and is expressed in objective, formal language. The task is to approximate the Rectified Linear Unit (ReLU) function using polynomial least-squares and cubic smoothing splines and to quantify the approximation quality. The problem is a standard exercise in computational science.\n\nThe function to be approximated is the Rectified Linear Unit (ReLU), defined as:\n$$f(x) = \\max(0, x)$$\nThis function is continuous, but its first derivative is discontinuous at $x=0$. The derivative, where it exists, is the Heaviside step function:\n$$\nf'(x) = \\begin{cases}\n    0 & \\text{if } x < 0 \\\\\n    1 & \\text{if } x > 0\n\\end{cases}\n$$\nThe non-differentiability at $x=0$ poses a challenge for smooth approximants.\n\n**Method 1: Polynomial Least-Squares Approximation**\n\nThe first method seeks an approximation in the form of a polynomial of degree $n$:\n$$p_n(x) = \\sum_{j=0}^{n} c_j x^j$$\nThe coefficients $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$ are determined by minimizing the sum of squared errors over a set of $N_{\\text{fit}}$ discrete sample points $\\{x_i\\}_{i=1}^{N_{\\text{fit}}}$ on the interval $[a,b]$. The objective function to minimize is:\n$$S(\\mathbf{c}) = \\sum_{i=1}^{N_{\\text{fit}}} \\left( p_n(x_i) - f(x_i) \\right)^2$$\nSubstituting the expression for $p_n(x)$, we have a linear least-squares problem. This can be expressed in matrix form as minimizing $\\|\\mathbf{A}\\mathbf{c} - \\mathbf{y}\\|_2^2$, where $\\mathbf{y}$ is the vector of function values $f(x_i)$ and $\\mathbf{A}$ is the Vandermonde matrix with entries $A_{ij} = x_i^j$ for $i \\in \\{1, \\dots, N_{\\text{fit}}\\}$ and $j \\in \\{0, \\dots, n\\}$. The solution is found by solving the normal equations, $\\mathbf{A}^T\\mathbf{A}\\mathbf{c} = \\mathbf{A}^T\\mathbf{y}$. Numerically stable algorithms, such as those based on QR decomposition, are employed to solve for $\\mathbf{c}$. Once the coefficients are known, the polynomial $p_n(x)$ and its derivative, $p'_n(x) = \\sum_{j=1}^{n} j c_j x^{j-1}$, can be evaluated at any point.\n\n**Method 2: Cubic Smoothing Spline Approximation**\n\nThe second method employs a cubic smoothing spline, $s(x)$. Unlike a pure interpolation, a smoothing spline is the function that minimizes a penalized sum of squares. The objective functional being minimized is:\n$$ \\sum_{i=1}^{N_{\\text{fit}}} \\left(s(x_i) - f(x_i)\\right)^2 + \\lambda \\int_{a}^{b} \\left(s''(x)\\right)^2 \\, dx $$\nThe first term enforces fidelity to the data points, while the second term, weighted by a smoothing parameter $\\lambda \\ge 0$, penalizes roughness, measured by the integrated squared second derivative. This represents a classic trade-off between bias (lack of fit) and variance (excessive oscillation). Standard numerical libraries often use a related smoothing parameter $s$, which sets an upper bound on the sum of squared residuals, $\\sum_{i=1}^{N_{\\text{fit}}} (s(x_i) - f(x_i))^2 \\le s$. When $s=0$, the spline is constrained to pass through all data points, resulting in an interpolating spline. As $s$ increases, the spline becomes smoother, deviating more from the data points. A cubic spline ($k=3$) is a piecewise polynomial of degree $3$ that is continuous and has continuous first and second derivatives ($C^2$ continuity) across the entire interval. The resulting spline object can be evaluated for both its value $s(x)$ and its derivative $s'(x)$.\n\n**Error Metrics Calculation**\n\nThe quality of each approximation $g(x)$ (where $g$ is either $p_n$ or $s$) is assessed using two metrics on a fine evaluation grid of $N_{\\text{eval}}$ points on $[a,b]$.\n\n1.  **Maximum Function Error ($E_{\\max}$)**: This metric quantifies the largest deviation of the approximant from the true function value.\n    $$E_{\\max} = \\max_{x \\in [a,b]} \\left|g(x) - f(x)\\right|$$\n\n2.  **Maximum Derivative Mismatch ($D_{\\max}$)**: This metric measures the largest deviation of the approximant's derivative from the true function's derivative.\n    $$D_{\\max} = \\max_{x \\in [a,b] \\setminus \\{0\\}} \\left|g'(x) - f'(x)\\right|$$\n    The point $x=0$ is explicitly excluded from this calculation because $f'(x)$ is undefined there. Computationally, this is handled by ignoring points within a small tolerance $\\varepsilon$ of zero, i.e., where $|x| \\le \\varepsilon = 10^{-12}$.\n\nThe following algorithm will be implemented to compute the results for each test case as specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.interpolate import UnivariateSpline\nfrom numpy.polynomial import polynomial as P\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates polynomial and spline approximations of the ReLU function.\n    \"\"\"\n    \n    # Define the target function and its derivative\n    f_relu = lambda x: np.maximum(0, x)\n    f_relu_prime = lambda x: (x > 0).astype(float)\n    \n    # Define evaluation grid parameters\n    N_eval = 10001\n    epsilon = 1e-12\n    \n    # Test suite provided in the problem statement\n    test_cases = [\n        {'type': 'polynomial', 'interval': [-1, 1], 'degree': 3, 'N_fit': 501},\n        {'type': 'polynomial', 'interval': [-3, 3], 'degree': 7, 'N_fit': 2001},\n        {'type': 'spline', 'interval': [-1, 1], 's': 0.0, 'N_fit': 201},\n        {'type': 'spline', 'interval': [-1, 1], 's': 0.01, 'N_fit': 201},\n        {'type': 'spline', 'interval': [-1, 1], 's': 10.0, 'N_fit': 201},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a, b = case['interval']\n        N_fit = case['N_fit']\n        \n        # Create grids\n        x_fit = np.linspace(a, b, N_fit)\n        x_eval = np.linspace(a, b, N_eval)\n        \n        # Get function values on the fitting grid\n        y_fit = f_relu(x_fit)\n        \n        g_eval = None\n        g_prime_eval = None\n        \n        if case['type'] == 'polynomial':\n            n = case['degree']\n            \n            # Perform polynomial least-squares fit\n            poly_fit = P.Polynomial.fit(x_fit, y_fit, n)\n            \n            # Evaluate the polynomial and its derivative on the fine grid\n            g_eval = poly_fit(x_eval)\n            g_prime_eval = poly_fit.deriv()(x_eval)\n\n        elif case['type'] == 'spline':\n            s_param = case['s']\n            \n            # Construct the cubic smoothing spline\n            spline = UnivariateSpline(x_fit, y_fit, k=3, s=s_param)\n            \n            # Evaluate the spline and its derivative on the fine grid\n            g_eval = spline(x_eval)\n            g_prime_eval = spline.derivative(n=1)(x_eval)\n            \n        # Evaluate the true function and its derivative on the fine grid\n        f_eval = f_relu(x_eval)\n        f_prime_eval = f_relu_prime(x_eval)\n        \n        # Calculate maximum function error\n        E_max = np.max(np.abs(g_eval - f_eval))\n        \n        # Calculate maximum derivative mismatch, excluding the point x=0\n        mask = np.abs(x_eval) > epsilon\n        D_max = np.max(np.abs(g_prime_eval[mask] - f_prime_eval[mask]))\n        \n        all_results.append([round(E_max, 6), round(D_max, 6)])\n\n    # Format the final output string as a list of lists without spaces\n    outer_list_str = []\n    for res in all_results:\n        inner_list_str = f\"[{res[0]:.6f},{res[1]:.6f}]\"\n        outer_list_str.append(inner_list_str)\n    final_output_str = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3133577"}, {"introduction": "The effectiveness of a piecewise spline approximation is critically dependent on the placement of its knots—the points where the polynomial segments join. While uniform knot placement is a simple starting point, it is rarely optimal. This practice [@problem_id:3133600] elevates the approximation task into an optimization problem where you will search for the ideal locations for a given number of knots to best capture a function's features. By implementing an exhaustive search for the best knot placement and comparing its performance to a standard uniform configuration, you will discover how optimizing a model's internal structure can lead to significantly more accurate and efficient approximations.", "problem": "You are tasked with designing and implementing a program that fits a piecewise cubic spline to the function $f(x)=\\sin(5x)$ on the interval $[0,2\\pi]$, where all angles are in radians. The spline is constructed as a least squares (LS) fit from a cubic B-spline space with a specified set of interior knots. You must pose the interior knot placement as a discrete combinatorial optimization over a candidate grid and compare the best-found fit to a baseline using uniformly spaced interior knots. All quantities that involve angles must be treated in radians. The error measure is the mean squared error (MSE) between the fitted spline and the true function on a dense evaluation grid. Your outputs must be floats as specified below.\n\nFundamental base and constraints:\n- Start from the definition that least squares fitting minimizes the sum of squared residuals between observed values and a model over a chosen function space. The model space here is the finite-dimensional vector space of cubic splines defined by a fixed set of interior knots and degree $k=3$.\n- Use the interval $[a,b]=[0,2\\pi]$ with $x$ in radians and the target function $f(x)=\\sin(5x)$.\n- For each test case, produce a dataset $\\{(x_i,y_i)\\}_{i=1}^{N_{\\text{data}}}$ with $x_i$ uniformly spaced in $[0,2\\pi]$ and $y_i=f(x_i)$. No noise is added.\n- Define the mean squared error over an evaluation grid $\\{x_j^{\\text{eval}}\\}_{j=1}^{N_{\\text{eval}}}$ as\n$$\n\\text{MSE}=\\frac{1}{N_{\\text{eval}}}\\sum_{j=1}^{N_{\\text{eval}}}\\left(s(x_j^{\\text{eval}})-f(x_j^{\\text{eval}})\\right)^2,\n$$\nwhere $s(x)$ is the fitted spline and $f(x)=\\sin(5x)$.\n- Interior knots must lie strictly inside $(0,2\\pi)$ and be strictly increasing. The spline degree is $k=3$ (cubic), with no smoothing beyond least squares; the fit is purely by minimizing squared residuals in the spline space.\n\nOptimization formulation:\n- Let $m$ denote the number of interior knots to place, and let $N_{\\text{cand}}$ denote the number of candidate interior knot locations, which are to be generated as $N_{\\text{cand}}$ equally spaced points in the open interval $(0,2\\pi)$.\n- The free-knot placement problem is discrete: select $m$ distinct interior knot positions from the $N_{\\text{cand}}$ candidates to minimize the MSE on the evaluation grid. This selection is a combinatorial optimization over all $\\binom{N_{\\text{cand}}}{m}$ combinations. For the purposes of this problem, you must solve it by exhaustive enumeration over all combinations and choose the one yielding the smallest MSE. If any combination fails to produce a valid spline fit due to feasibility constraints, it should be skipped without terminating the program.\n- The baseline uses $m$ interior knots uniformly spaced inside $(0,2\\pi)$, i.e., partition $[0,2\\pi]$ into $m+1$ equal subintervals and place one interior knot at each interior division point.\n\nEvaluation and outputs:\n- Use $N_{\\text{eval}}=2001$ uniformly spaced evaluation points in $[0,2\\pi]$ to compute the MSEs.\n- For each test case, compute:\n  1. $\\text{MSE}_{\\text{free}}^\\star$: the minimum MSE achieved by exhaustive search over all $m$-sized combinations from the $N_{\\text{cand}}$ interior candidates.\n  2. $\\text{MSE}_{\\text{uniform}}$: the MSE using $m$ uniformly spaced interior knots.\n  3. The ratio $r=\\text{MSE}_{\\text{free}}^\\star / \\text{MSE}_{\\text{uniform}}$ as a float.\n\nTest suite:\nRun the program on the following three test cases, each represented as a triple $(m, N_{\\text{cand}}, N_{\\text{data}})$:\n- Case $1$: $(m, N_{\\text{cand}}, N_{\\text{data}})=(2, 10, 101)$.\n- Case $2$: $(m, N_{\\text{cand}}, N_{\\text{data}})=(3, 12, 121)$.\n- Case $3$: $(m, N_{\\text{cand}}, N_{\\text{data}})=(3, 13, 151)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the three ratios $[r_1,r_2,r_3]$, corresponding to the three test cases in the order listed above. Each ratio must be rounded to exactly $6$ decimal places, and the list must be comma-separated with no spaces, enclosed in square brackets. For example, an acceptable output format is like $[0.123456,0.234567,0.345678]$.", "solution": "The user has requested the design and implementation of a program to solve a knot placement optimization problem for a piecewise cubic spline. The problem is validated as sound, well-posed, and objective. It is a standard numerical analysis task involving function approximation.\n\n### Principle-Based Design\n\nThe solution is constructed based on the principles of function approximation using splines and numerical optimization.\n\n1.  **Spline Representation**: A piecewise polynomial function, known as a spline, is defined by its polynomial degree $k$ and a sequence of points called knots. For this problem, we use cubic splines, so the degree is $k=3$. The smoothness of the spline at the knots is determined by the knot multiplicity. The specific type of spline basis used here is the B-spline basis, which has desirable properties for numerical computation, such as local support and forming a partition of unity. A spline function $s(x)$ can be expressed as a linear combination of B-spline basis functions $B_{j,k,t}(x)$:\n    $$\n    s(x) = \\sum_{j=0}^{n-1} c_j B_{j, k, t}(x)\n    $$\n    where $\\{c_j\\}$ are the spline coefficients, $t$ is the knot vector, and $n$ is the number of basis functions. The number of basis functions is related to the number of interior knots $m$ and the spline degree $k$ by $n = m + k + 1$. The full knot vector $t$ for a spline on an interval $[a, b]$ with $m$ interior knots $\\{t_1, \\dots, t_m\\}$ is constructed by augmenting the interior knots with boundary knots of multiplicity $k+1$. This ensures the spline interpolates the boundary conditions appropriately for certain fitting types. The full knot vector is thus:\n    $$\n    t = [\\underbrace{a, \\dots, a}_{k+1 \\text{ times}}, t_1, \\dots, t_m, \\underbrace{b, \\dots, b}_{k+1 \\text{ times}}]\n    $$\n\n2.  **Least-Squares Fitting**: The problem requires fitting the spline to a set of data points $\\{(x_i, y_i)\\}_{i=1}^{N_{\\text{data}}}$ by minimizing the sum of squared residuals. The data is generated from the target function $f(x) = \\sin(5x)$ without noise. The objective is to find the coefficients $c = (c_0, \\dots, c_{n-1})$ that minimize:\n    $$\n    \\sum_{i=1}^{N_{\\text{data}}} \\left( s(x_i) - y_i \\right)^2 = \\sum_{i=1}^{N_{\\text{data}}} \\left( \\sum_{j=0}^{n-1} c_j B_{j, k, t}(x_i) - y_i \\right)^2\n    $$\n    This is a linear least-squares problem. If we define a design matrix $A$ with elements $A_{ij} = B_{j, k, t}(x_i)$, a vector of coefficients $c$, and a vector of data values $y$, the problem is to find $c$ that minimizes $\\|Ac - y\\|_2^2$. The `scipy.interpolate.make_lsq_spline` function is specifically designed to solve this problem, given the data points, the full knot vector, and the spline degree.\n\n3.  **Knot Placement Optimization**: The quality of a spline approximation is highly dependent on the placement of its knots. The optimal placement of knots is a challenging nonlinear optimization problem. This problem simplifies it to a discrete combinatorial optimization. A grid of $N_{\\text{cand}}$ candidate knot locations is established within the open interval $(a, b) = (0, 2\\pi)$. The task is to select the subset of $m$ knots from these candidates that results in the lowest Mean Squared Error (MSE) after the least-squares fit. The MSE is defined as:\n    $$\n    \\text{MSE} = \\frac{1}{N_{\\text{eval}}} \\sum_{j=1}^{N_{\\text{eval}}} \\left( s(x_j^{\\text{eval}}) - f(x_j^{\\text{eval}}) \\right)^2\n    $$\n    Since the number of combinations, $\\binom{N_{\\text{cand}}}{m}$, is small for the given test cases, the problem is solved by an exhaustive search. We iterate through every possible combination of $m$ knots, perform the least-squares fit for each, calculate the resulting MSE, and identify the combination that yields the minimum MSE, denoted $\\text{MSE}_{\\text{free}}^\\star$.\n\n4.  **Baseline Comparison**: To quantify the improvement gained by optimizing the knot locations, the result from the free-knot placement is compared against a baseline case. The baseline uses $m$ interior knots that are uniformly spaced within the interval $(0, 2\\pi)$. This provides a standard, non-adaptive reference for spline approximation. The MSE for this configuration, $\\text{MSE}_{\\text{uniform}}$, is calculated using the same least-squares fitting procedure.\n\n5.  **Algorithm Implementation**:\n    -   For each test case `(m, N_cand, N_data)`, the main logic is executed.\n    -   A helper function is defined to compute the MSE for a given set of interior knots. This function encapsulates spline creation, fitting, and evaluation, promoting code reuse and clarity. It includes error handling for combinations of knots and data that might not satisfy the conditions for a unique least-squares solution (e.g., the Schoenberg-Whitney conditions), ensuring the program's robustness.\n    -   The exhaustive search iterates through all combinations of candidate knots using `itertools.combinations`, calling the helper function for each to find `MSE_free_star`.\n    -   The baseline MSE, `MSE_uniform`, is calculated by calling the same helper function with uniformly spaced knots.\n    -   The final ratio $r = \\text{MSE}_{\\text{free}}^\\star / \\text{MSE}_{\\text{uniform}}$ is computed, formatted, and stored.\n    -   After processing all test cases, the collected ratios are printed in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import make_lsq_spline\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves a set of spline fitting problems with knot optimization.\n    For each test case, it computes the ratio of the Mean Squared Error (MSE)\n    from an optimized knot placement to the MSE from a uniform knot placement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2, 10, 101),\n        (3, 12, 121),\n        (3, 13, 151),\n    ]\n\n    results = []\n    \n    # Process each test case\n    for m, N_cand, N_data in test_cases:\n        # --- 1. Setup Phase: Define constants, grids, and target function ---\n        a, b = 0.0, 2.0 * np.pi\n        k = 3\n        N_eval = 2001\n        \n        # Target function to be approximated\n        f = lambda x: np.sin(5.0 * x)\n        \n        # Data points for least-squares fitting\n        x_data = np.linspace(a, b, N_data)\n        y_data = f(x_data)\n        \n        # Dense grid for evaluating the approximation error (MSE)\n        x_eval = np.linspace(a, b, N_eval)\n        y_true_eval = f(x_eval)\n\n        # --- 2. Helper Function for MSE Calculation ---\n        def compute_spline_mse(interior_knots):\n            \"\"\"\n            Computes the MSE for a spline fit with a given set of interior knots.\n            \n            Args:\n                interior_knots (list or np.ndarray): A list of interior knot locations.\n            \n            Returns:\n                float: The calculated Mean Squared Error. Returns np.inf if fitting fails.\n            \"\"\"\n            try:\n                # The full knot vector includes boundary knots with multiplicity k+1.\n                t = np.concatenate(([a] * (k + 1), sorted(interior_knots), [b] * (k + 1)))\n                \n                # Perform the least-squares spline fit.\n                # make_lsq_spline finds the coefficients c for the spline s(x)\n                # that minimizes the sum of squared errors ||s(x_data) - y_data||^2.\n                spl = make_lsq_spline(x_data, y_data, t, k, check_finite=False)\n                \n                # Evaluate the fitted spline on the dense evaluation grid.\n                y_spline_eval = spl(x_eval)\n                \n                # Calculate the Mean Squared Error.\n                mse = np.mean((y_spline_eval - y_true_eval) ** 2)\n                return mse\n                \n            except (ValueError, np.linalg.LinAlgError):\n                # The Schoenberg-Whitney conditions might not be met for certain\n                # pathological knot/data configurations, causing the fit to fail.\n                # As per the problem, such combinations should be skipped.\n                return np.inf\n\n        # --- 3. Free-Knot Optimization (Exhaustive Search) ---\n        # Generate N_cand candidate knots uniformly in the open interval (a, b).\n        cand_knots = np.linspace(a, b, N_cand + 2)[1:-1]\n        min_mse_free = np.inf\n        \n        # Iterate over all combinations of m knots from the candidate set.\n        for knot_combo in combinations(cand_knots, m):\n            mse = compute_spline_mse(list(knot_combo))\n            min_mse_free = min(min_mse_free, mse)\n        \n        # --- 4. Uniform-Knot Baseline ---\n        # Generate m uniformly spaced interior knots.\n        uniform_knots = np.linspace(a, b, m + 2)[1:-1]\n        mse_uniform = compute_spline_mse(uniform_knots)\n        \n        # --- 5. Compute and Store the Ratio ---\n        # If mse_uniform is zero or inf, the ratio is ill-defined.\n        # This is unlikely but handled for robustness.\n        if mse_uniform > 0 and np.isfinite(mse_uniform):\n            ratio = min_mse_free / mse_uniform\n        else:\n            ratio = 1.0  # Or another sensible default\n        \n        results.append(f\"{ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3133600"}]}