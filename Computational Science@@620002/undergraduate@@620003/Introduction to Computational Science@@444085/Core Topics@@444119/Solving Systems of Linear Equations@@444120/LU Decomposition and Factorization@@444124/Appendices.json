{"hands_on_practices": [{"introduction": "Before relying on automated library functions, it is essential to understand the mechanics of LU factorization from first principles. This exercise guides you through a manual, entry-by-entry calculation of the $L$ and $U$ factors for a given matrix $A$, reinforcing the direct connection between $A=LU$ and the rules of matrix multiplication [@problem_id:3156996]. By working through the process, you will also develop a deeper appreciation for the conditions that guarantee the existence and uniqueness of the factorization.", "problem": "Consider a square matrix $A \\in \\mathbb{R}^{3 \\times 3}$ to be factored as $A = L U$ in a Lower–Upper (LU) decomposition, where $L$ is lower triangular and $U$ is upper triangular. You are to enforce the normalization constraint that the diagonal entries of $U$ are all equal to $1$, i.e., $u_{11} = u_{22} = u_{33} = 1$. This normalization is common in computational practice because it transfers all pivot scaling into the diagonal of $L$. Work under the standard assumptions that no row interchanges are performed (no pivoting), and that the factorization exists for the given $A$.\n\nStarting from the core definitions of matrix multiplication and triangular structure, and using only well-tested facts from linear algebra (such as multiplicativity of determinants and the determinant of triangular matrices), proceed as follows:\n\n- Let \n$$A = \\begin{pmatrix}\n4 & -2 & 6 \\\\\n8 & -5 & 14 \\\\\n2 & 1 & 3\n\\end{pmatrix}, \\quad\nL = \\begin{pmatrix}\n\\ell_{11} & 0 & 0 \\\\\n\\ell_{21} & \\ell_{22} & 0 \\\\\n\\ell_{31} & \\ell_{32} & \\ell_{33}\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\n1 & u_{12} & u_{13} \\\\\n0 & 1 & u_{23} \\\\\n0 & 0 & 1\n\\end{pmatrix}.$$\nBy equating $A = L U$ entrywise and using only the definition of matrix multiplication together with the triangular structure, solve for the compatible entries $\\ell_{ij}$ and $u_{ij}$.\n\n- Explain, from first principles, in what sense the LU factorization is unique under the normalization $u_{ii} = 1$, and what constraint on $A$ (in terms of leading principal minors) is needed to ensure existence and uniqueness without pivoting. Your discussion should be grounded in the definitions and properties of triangular matrices and determinants, and should not rely on any pre-stated LU formulas.\n\n- As your final numerical deliverable, compute the determinant of $A$. Express your final answer as an exact integer; no rounding is required.", "solution": "The problem asks for three tasks to be completed based on the LU decomposition of a matrix $A$ into a lower triangular matrix $L$ and a unit upper triangular matrix $U$, such that $A = LU$.\n\nFirst, we must solve for the unknown entries of $L$ and $U$ for the given matrix $A$.\nThe given matrices are:\n$$A = \\begin{pmatrix} 4 & -2 & 6 \\\\ 8 & -5 & 14 \\\\ 2 & 1 & 3 \\end{pmatrix}, \\quad L = \\begin{pmatrix} \\ell_{11} & 0 & 0 \\\\ \\ell_{21} & \\ell_{22} & 0 \\\\ \\ell_{31} & \\ell_{32} & \\ell_{33} \\end{pmatrix}, \\quad U = \\begin{pmatrix} 1 & u_{12} & u_{13} \\\\ 0 & 1 & u_{23} \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n\nThe factorization $A=LU$ is expressed as:\n$$ \\begin{pmatrix} 4 & -2 & 6 \\\\ 8 & -5 & 14 \\\\ 2 & 1 & 3 \\end{pmatrix} = \\begin{pmatrix} \\ell_{11} & 0 & 0 \\\\ \\ell_{21} & \\ell_{22} & 0 \\\\ \\ell_{31} & \\ell_{32} & \\ell_{33} \\end{pmatrix} \\begin{pmatrix} 1 & u_{12} & u_{13} \\\\ 0 & 1 & u_{23} \\\\ 0 & 0 & 1 \\end{pmatrix} $$\nBy the definition of matrix multiplication, the entries of the product $LU$ are given by $(LU)_{ij} = \\sum_{k=1}^{3} L_{ik} U_{kj}$. We equate these entries with the corresponding entries $a_{ij}$ of $A$. The triangular structures of $L$ and $U$ simplify the sums significantly.\n\nWe proceed by sequentially determining the columns of $L$ and rows of $U$.\n\nColumn 1:\n$a_{11} = \\ell_{11} u_{11} = \\ell_{11} \\cdot 1 \\implies \\ell_{11} = 4$.\n$a_{21} = \\ell_{21} u_{11} = \\ell_{21} \\cdot 1 \\implies \\ell_{21} = 8$.\n$a_{31} = \\ell_{31} u_{11} = \\ell_{31} \\cdot 1 \\implies \\ell_{31} = 2$.\n\nRow 1:\n$a_{12} = \\ell_{11} u_{12} \\implies 4 u_{12} = -2 \\implies u_{12} = -\\frac{1}{2}$.\n$a_{13} = \\ell_{11} u_{13} \\implies 4 u_{13} = 6 \\implies u_{13} = \\frac{3}{2}$.\n\nColumn 2:\n$a_{22} = \\ell_{21} u_{12} + \\ell_{22} u_{22} = 8(-\\frac{1}{2}) + \\ell_{22}(1) = -4 + \\ell_{22}$.\nSo, $-5 = -4 + \\ell_{22} \\implies \\ell_{22} = -1$.\n$a_{32} = \\ell_{31} u_{12} + \\ell_{32} u_{22} = 2(-\\frac{1}{2}) + \\ell_{32}(1) = -1 + \\ell_{32}$.\nSo, $1 = -1 + \\ell_{32} \\implies \\ell_{32} = 2$.\n\nRow 2:\n$a_{23} = \\ell_{21} u_{13} + \\ell_{22} u_{23} = 8(\\frac{3}{2}) + (-1)u_{23} = 12 - u_{23}$.\nSo, $14 = 12 - u_{23} \\implies u_{23} = -2$.\n\nColumn 3:\n$a_{33} = \\ell_{31} u_{13} + \\ell_{32} u_{23} + \\ell_{33} u_{33} = 2(\\frac{3}{2}) + 2(-2) + \\ell_{33}(1) = 3 - 4 + \\ell_{33} = -1 + \\ell_{33}$.\nSo, $3 = -1 + \\ell_{33} \\implies \\ell_{33} = 4$.\n\nAll unknown entries have been determined. The resulting matrices are:\n$$ L = \\begin{pmatrix} 4 & 0 & 0 \\\\ 8 & -1 & 0 \\\\ 2 & 2 & 4 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 1 & -\\frac{1}{2} & \\frac{3}{2} \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\n\nSecond, we explain the uniqueness and existence conditions from first principles.\n\nFor uniqueness, assume two such factorizations exist for a non-singular matrix $A$: $A = L_1 U_1$ and $A = L_2 U_2$. Here, $L_1$ and $L_2$ are lower triangular, and $U_1$ and $U_2$ are unit upper triangular (upper triangular with 1s on the diagonal).\nFrom $L_1 U_1 = L_2 U_2$, we can write $L_2^{-1} L_1 = U_2 U_1^{-1}$.\nThe inverse of a lower triangular matrix is lower triangular, and the product of two lower triangular matrices is also lower triangular. Therefore, the left-hand side, $L_2^{-1} L_1$, is a lower triangular matrix.\nThe inverse of a unit upper triangular matrix is unit upper triangular, and the product of two unit upper triangular matrices is also unit upper triangular. Thus, the right-hand side, $U_2 U_1^{-1}$, is a unit upper triangular matrix.\nThe only matrix that is simultaneously lower triangular and unit upper triangular is the identity matrix $I$.\nTherefore, we must have $L_2^{-1} L_1 = I$ and $U_2 U_1^{-1} = I$.\nThese equations imply $L_1 = L_2$ and $U_1 = U_2$, which proves that the factorization, if it exists, is unique. This proof relies on the invertibility of $L_2$ and $U_1$. A unit triangular matrix like $U_1$ is always invertible since its determinant is $1$. The invertibility of $L_2$ depends on its diagonal elements being non-zero.\n\nFor existence, we must establish the condition that ensures the diagonal elements of $L$, $\\ell_{kk}$, are non-zero, as they are used as divisors in the calculation process. For instance, we calculated $u_{12} = a_{12}/\\ell_{11}$. In general, the formula for $u_{kj}$ for $j > k$ involves division by $\\ell_{kk}$.\nLet us relate $\\ell_{kk}$ to the properties of $A$. Consider the $k \\times k$ leading principal submatrix of $A$, denoted $A_k$. The factorization $A=LU$ implies a factorization of these submatrices. Let $L_k$ and $U_k$ be the $k \\times k$ leading principal submatrices of $L$ and $U$, respectively. The structure of matrix multiplication implies $A_k = L_k U_k$.\nUsing the property that the determinant of a product of matrices is the product of their determinants, we have $\\det(A_k) = \\det(L_k) \\det(U_k)$.\nThe determinant of a triangular matrix is the product of its diagonal elements.\nFor $U_k$, which is unit upper triangular, $\\det(U_k) = u_{11} u_{22} \\cdots u_{kk} = 1^k = 1$.\nFor $L_k$, $\\det(L_k) = \\ell_{11} \\ell_{22} \\cdots \\ell_{kk}$.\nThus, we find the critical relationship: $\\det(A_k) = \\ell_{11} \\ell_{22} \\cdots \\ell_{kk}$.\nFrom this, it follows that $\\ell_{kk} = \\frac{\\det(A_k)}{\\det(A_{k-1})}$ for $k > 1$, and $\\ell_{11} = \\det(A_1) = a_{11}$.\nThe LU decomposition algorithm (without pivoting) computes the $\\ell_{ij}$ and $u_{ij}$ sequentially. For the algorithm to be executable, we must be able to solve for each unknown. This requires that at each step $k$, the diagonal element $\\ell_{kk}$ must be non-zero, as it is used as a divisor to find the elements of the $k$-th row of $U$.\nThe condition $\\ell_{kk} \\neq 0$ for all $k = 1, \\dots, n$ is equivalent to the condition $\\det(A_k) \\neq 0$ for all $k = 1, \\dots, n$.\nTherefore, the necessary and sufficient condition for the existence and uniqueness of the LU factorization (with $u_{ii}=1$ and without pivoting) is that all leading principal minors of $A$ are non-zero.\n\nThird, we compute the determinant of $A$.\nUsing the property $\\det(A) = \\det(L) \\det(U)$, we can easily find the determinant from the factorization.\nThe determinant of $L$ is the product of its diagonal elements:\n$$ \\det(L) = \\ell_{11} \\ell_{22} \\ell_{33} = (4)(-1)(4) = -16 $$\nThe determinant of $U$ is the product of its diagonal elements:\n$$ \\det(U) = u_{11} u_{22} u_{33} = (1)(1)(1) = 1 $$\nTherefore, the determinant of $A$ is:\n$$ \\det(A) = \\det(L) \\det(U) = (-16)(1) = -16 $$\nThis provides the required numerical deliverable.", "answer": "$$\\boxed{-16}$$", "id": "3156996"}, {"introduction": "While partial pivoting is a general strategy for ensuring numerical stability, it is not always necessary. This practice introduces an important class of matrices—strictly diagonally dominant (SDD) matrices—for which Gaussian elimination without pivoting is provably stable [@problem_id:3156903]. You will first theoretically prove this stability and then confirm it through a numerical experiment, linking the abstract matrix property to the concrete behavior of the algorithm and its resistance to error growth.", "problem": "You are asked to construct a concrete sequence of square matrices and to reason about the existence, uniqueness, and numerical stability of Gaussian elimination without pivoting through first principles. Then, you will implement an algorithm to quantify stability numerically under controlled random perturbations. All mathematical symbols, variables, operators, and even numerals must be written using LaTeX delimiters.\n\nTask A (Construction). For each positive integer $n \\in \\{5,20,50\\}$, define a matrix $A_n \\in \\mathbb{R}^{n \\times n}$ by\n$$\n(A_n)_{ij} =\n\\begin{cases}\n3, & \\text{if } i=j, \\\\\n-1, & \\text{if } |i-j|=1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nProve, starting from core definitions and well-tested facts only, that each $A_n$ is strictly diagonally dominant by rows and is nonsingular. Then, for the class $\\{A_n\\}$, argue that Gaussian elimination without pivoting produces an $L U$ factorization for each $n$ and that the elimination is stable in the sense that the element growth factor is bounded by a modest constant independent of $n$.\n\nTask B (Derivation of stability from first principles). Using only core definitions and well-tested facts as the base:\n1) Begin from the definition of strict diagonal dominance by rows: a matrix $A \\in \\mathbb{R}^{n \\times n}$ is strictly diagonally dominant by rows if for each $i \\in \\{1,\\dots,n\\}$,\n$$\n|a_{ii}| \\;>\\; \\sum_{j \\neq i} |a_{ij}|.\n$$\n2) Use a well-tested fact to justify nonsingularity of $A_n$, such as the Gershgorin Circle Theorem or the Levy–Desplanques Theorem, both of which imply that strict diagonal dominance by rows guarantees nonsingularity.\n3) Consider the elementary step of Gaussian elimination without pivoting. Let $A^{(k)}$ be the matrix after $k-1$ elimination steps, with pivot $a^{(k)}_{kk} \\neq 0$, and multipliers $m_{ik} = a^{(k)}_{ik}/a^{(k)}_{kk}$ for $i>k$. The Schur complement update is\n$$\na^{(k+1)}_{ij} \\;=\\; a^{(k)}_{ij} \\;-\\; m_{ik}\\, a^{(k)}_{kj}, \\quad \\text{for } i,j > k.\n$$\nFrom these relations and the row strict diagonal dominance of $A^{(k)}$, derive that $|m_{ik}| < 1$ and that strict diagonal dominance is preserved in the trailing submatrix. Conclude inductively that no zero pivot occurs and that $L U$ without pivoting exists for $A_n$.\n4) Define the element growth factor\n$$\n\\gamma \\;=\\; \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $U$ is the upper-triangular factor produced by elimination without pivoting. Using the bounds on multipliers $|m_{ik}|<1$ coming from strict diagonal dominance, bound $\\gamma$ by a modest constant (for instance, show that $\\gamma \\leq 2$ for this class, up to effects of rounding in finite precision arithmetic).\n\nTask C (Numerical experiment under random perturbations). For each $n \\in \\{5,20,50\\}$ and each perturbation level $\\varepsilon \\in \\{10^{-12},10^{-8},10^{-6}\\}$, do the following:\n1) Construct $A_n$ as in Task A.\n2) Construct a random perturbation $E \\in \\mathbb{R}^{n \\times n}$ with entries drawn from a standard normal distribution, and scale $E$ so that its spectral norm satisfies\n$$\n\\|E\\|_2 \\;=\\; \\varepsilon \\, \\|A_n\\|_2.\n$$\n3) Form the perturbed matrix $\\widetilde{A} = A_n + E$. Generate a random right-hand side $b \\in \\mathbb{R}^n$ with independent standard normal entries.\n4) Compute an $L U$ factorization of $\\widetilde{A}$ without pivoting (implement Gaussian elimination yourself; do not call any library routine that performs pivoting), and solve $\\widetilde{A} x = b$.\n5) Compute the element growth factor\n$$\n\\gamma \\;=\\; \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |\\widetilde{A}_{ij}|}.\n$$\n6) Compute the scaled backward residual with respect to the unperturbed matrix $A_n$,\n$$\n\\eta \\;=\\; \\frac{\\|A_n x - b\\|_2}{\\|A_n\\|_2 \\, \\|x\\|_2 + \\|b\\|_2}.\n$$\n7) For each test case, regard the elimination as numerically stable if both conditions hold:\n   a) $\\gamma \\leq 2.1$,\n   b) $\\eta \\leq 1.5 \\, \\varepsilon$.\n\nTest Suite and Output. Use the test cases $(n,\\varepsilon,\\text{seed})$ given by\n$$\n(5,10^{-12},0), \\quad (5,10^{-8},1), \\quad (5,10^{-6},2), \\quad (20,10^{-12},3), \\quad (20,10^{-8},4), \\quad (20,10^{-6},5), \\quad (50,10^{-12},6), \\quad (50,10^{-8},7), \\quad (50,10^{-6},8).\n$$\nFor each triple, set the pseudorandom number generator using the given integer seed before creating $E$ and $b$. Your program must produce a single line of output containing the nine boolean results, one per test case in the listed order, where each boolean indicates whether both stability checks in step $7$ passed for that case. The required format is a comma-separated list enclosed in square brackets, for example $[{\\tt True},{\\tt False},\\dots]$. No other text should be printed.", "solution": "### Task A and B: Theoretical Analysis\n\nHere we merge the proofs required in Task A and B, as they are logically sequential. We begin by defining the matrix $A_n$ for $n \\in \\mathbb{Z}^+$ as specified:\n$$\n(A_n)_{ij} =\n\\begin{cases}\n3, & \\text{if } i=j, \\\\\n-1, & \\text{if } |i-j|=1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThis is a symmetric, tridiagonal matrix.\n\n**Strict Diagonal Dominance of $A_n$**\n\nA matrix $A$ is strictly diagonally dominant (SDD) by rows if for each row $i$, the magnitude of the diagonal element is strictly greater than the sum of the magnitudes of all other elements in that row: $|a_{ii}| > \\sum_{j \\neq i} |a_{ij}|$. Let's verify this for $A_n$.\n\n- For the first row ($i=1$), the diagonal element is $(A_n)_{11} = 3$. The only non-zero off-diagonal element is $(A_n)_{12} = -1$. The sum of magnitudes of off-diagonal elements is $\\sum_{j \\neq 1} |(A_n)_{1j}| = |(A_n)_{12}| = |-1| = 1$. We must check if $|(A_n)_{11}| > 1$. Since $|3| = 3 > 1$, the condition holds for $i=1$.\n- For the last row ($i=n$), the diagonal element is $(A_n)_{nn} = 3$. The only non-zero off-diagonal element is $(A_n)_{n,n-1} = -1$. The sum is $\\sum_{j \\neq n} |(A_n)_{nj}| = |(A_n)_{n,n-1}| = |-1| = 1$. We check if $|(A_n)_{nn}| > 1$. Since $|3| = 3 > 1$, the condition holds for $i=n$.\n- For any interior row ($1 < i < n$), the diagonal element is $(A_n)_{ii} = 3$. The non-zero off-diagonal elements are $(A_n)_{i,i-1} = -1$ and $(A_n)_{i,i+1} = -1$. The sum of magnitudes is $\\sum_{j \\neq i} |(A_n)_{ij}| = |(A_n)_{i,i-1}| + |(A_n)_{i,i+1}| = |-1| + |-1| = 2$. We check if $|(A_n)_{ii}| > 2$. Since $|3| = 3 > 2$, the condition holds.\n\nSince the condition holds for all rows $i \\in \\{1, \\dots, n\\}$, the matrix $A_n$ is strictly diagonally dominant by rows for any $n \\ge 2$.\n\n**Nonsingularity of $A_n$**\n\nThe Levy–Desplanques theorem states that any matrix that is strictly diagonally dominant by rows is nonsingular. Since we have proven that $A_n$ is SDD for any $n \\ge 2$, it follows directly from this theorem that $A_n$ is nonsingular. This also holds for $n=1$, where $A_1 = [3]$ is trivially nonsingular.\n\n**Existence of LU Factorization without Pivoting**\n\nGaussian elimination without pivoting succeeds if and only if all leading principal submatrices of the matrix are nonsingular. Let $A_k$ denote the $k \\times k$ leading principal submatrix of $A_n$. $A_k$ is formed by taking the first $k$ rows and columns of $A_n$. The structure of $A_k$ is identical to the matrix we defined as $A_n$ but of size $k \\times k$. As shown above, any such matrix $A_k$ (for $k \\ge 1$) is strictly diagonally dominant and therefore nonsingular. Since all leading principal submatrices of $A_n$ are nonsingular, Gaussian elimination without pivoting can be performed on $A_n$ to produce an $LU$ factorization.\n\n**Preservation of Strict Diagonal Dominance and Bounded Multipliers**\n\nWe prove by induction that the Schur complements generated during elimination remain SDD. Let $A^{(1)} = A_n$. At step $k$ of elimination, we have the matrix $A^{(k)}$. Assume the trailing submatrix of $A^{(k)}$ of size $(n-k+1) \\times (n-k+1)$ is SDD. Let's call this submatrix $S^{(k)}$. The pivot is $a_{kk}^{(k)}$. The multipliers are $m_{ik} = a_{ik}^{(k)}/a_{kk}^{(k)}$ for $i > k$. The entries of the next Schur complement $S^{(k+1)}$ are given by $a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)}$ for $i,j > k$.\n\nFor any row $i > k$ in $S^{(k+1)}$, the new diagonal element is $a_{ii}^{(k+1)}$ and the sum of off-diagonal magnitudes is $\\sum_{j>k, j \\neq i} |a_{ij}^{(k+1)}|$. We have:\n$$ |a_{ii}^{(k+1)}| = |a_{ii}^{(k)} - m_{ik}a_{ki}^{(k)}| \\ge |a_{ii}^{(k)}| - |m_{ik}||a_{ki}^{(k)}| $$\n$$ \\sum_{j>k, j \\neq i} |a_{ij}^{(k+1)}| = \\sum_{j>k, j \\neq i} |a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)}| \\le \\sum_{j>k, j \\neq i} |a_{ij}^{(k)}| + |m_{ik}|\\sum_{j>k, j \\neq i} |a_{kj}^{(k)}| $$\nSince $S^{(k)}$ is SDD, we have $|a_{ii}^{(k)}| > \\sum_{j \\ge k, j \\neq i} |a_{ij}^{(k)}|$ and $|a_{kk}^{(k)}| > \\sum_{j > k} |a_{kj}^{(k)}|$.\nThe proof that $|a_{ii}^{(k+1)}| > \\sum_{j>k, j \\neq i} |a_{ij}^{(k+1)}|$ relies on showing that\n$|a_{ii}^{(k)}| - |m_{ik}||a_{ki}^{(k)}| > \\sum_{j>k, j \\neq i} |a_{ij}^{(k)}| + |m_{ik}|\\sum_{j>k, j \\neq i} |a_{kj}^{(k)}|$.\nSubstituting the SDD property $|a_{ii}^{(k)}| > |a_{ik}^{(k)}| + \\sum_{j>k, j \\neq i} |a_{ij}^{(k)}|$, this is equivalent to showing:\n$|a_{ik}^{(k)}| - |m_{ik}||a_{ki}^{(k)}| > |m_{ik}|\\sum_{j>k, j \\neq i} |a_{kj}^{(k)}|$, which simplifies to $|a_{kk}^{(k)}| > \\sum_{j>k} |a_{kj}^{(k)}|$, the SDD property of the pivot row. Thus, SDD is preserved in all Schur complements.\n\nFor the specific matrix $A_n$, the structure simplifies the analysis. At step $k$, the active submatrix is tridiagonal. The only non-zero element in column $k$ below the diagonal is $a_{k+1,k}^{(k)}=-1$. All other $a_{ik}^{(k)}=0$ for $i > k+1$. Thus, the only non-zero multiplier is $m_{k+1,k} = a_{k+1,k}^{(k)}/a_{kk}^{(k)} = -1/a_{kk}^{(k)}$.\nThe pivots $d_k = a_{kk}^{(k)}$ follow a recurrence.\n$d_1 = (A_n)_{11} = 3$.\nAt step $1$, the update affects only row $2$: $a_{2j}^{(2)} = a_{2j}^{(1)} - m_{21}a_{1j}^{(1)}$.\n$d_2 = a_{22}^{(2)} = a_{22}^{(1)} - m_{21}a_{12}^{(1)} = 3 - (-1/3)(-1) = 3 - 1/3 = 8/3$.\nThe new off-diagonal element is $a_{23}^{(2)} = a_{23}^{(1)} - m_{21}a_{13}^{(1)} = -1 - (-1/3)(0) = -1$.\nThe process continues, and at step $k$, the pivot is $d_k = a_{kk}^{(k)}$ and the off-diagonal is $a_{k,k+1}^{(k)}=-1$. The next pivot is $d_{k+1} = a_{k+1,k+1}^{(k+1)} = a_{k+1,k+1}^{(k)} - m_{k+1,k}a_{k,k+1}^{(k)} = 3 - (-1/d_k)(-1) = 3 - 1/d_k$.\nThe sequence of pivots is $d_1=3$, $d_2=8/3 \\approx 2.667$, $d_3=3-3/8=21/8=2.625$. This sequence is monotonically decreasing and converges to the largest root of $d=3-1/d$, which is $d=(3+\\sqrt{5})/2 \\approx 2.618$.\nTherefore, for all $k$, we have $(3+\\sqrt{5})/2 \\le d_k \\le 3$.\nThe only non-zero multipliers are $m_{k+1,k} = -1/d_k$. Their magnitudes are $|m_{k+1,k}| = 1/d_k \\le 1/((3+\\sqrt{5})/2) = 2/(3+\\sqrt{5}) = (3-\\sqrt{5})/2 \\approx 0.382 < 1$.\nThis proves that all multipliers are bounded in magnitude by a constant strictly less than $1$.\n\n**Bound on the Element Growth Factor**\n\nThe growth factor is $\\gamma = \\max_{i,j} |u_{ij}| / \\max_{i,j} |a_{ij}|$. For $A_n$, $\\max_{i,j} |(A_n)_{ij}| = 3$.\nThe matrix $U$ resulting from elimination has entries $u_{ij} = a_{ij}^{(i)}$.\nThe diagonal entries are the pivots, $u_{ii} = d_i$. We found $(3+\\sqrt{5})/2 \\le d_i \\le 3$.\nThe off-diagonal entries are $u_{ij}$ for $j>i$. The only non-zero off-diagonals are on the first superdiagonal. Let's trace their values. At step $k$, $u_{k,k+1} = a_{k,k+1}^{(k)}$.\nFor $k=1$, $u_{12}=a_{12}^{(1)}=-1$.\nFor $k=2$, $u_{23}=a_{23}^{(2)} = a_{23}^{(1)} - m_{21}a_{13}^{(1)} = -1 - (-1/3)(0) = -1$.\nInductively, one can see that the elimination process for this tridiagonal matrix does not alter the superdiagonal elements: $u_{i,i+1} = -1$ for all $i$.\nThe matrix $U$ is an upper bidiagonal matrix:\n$$\nU =\n\\begin{pmatrix}\nd_1 & -1 & 0 & \\dots \\\\\n0 & d_2 & -1 & \\\\\n\\vdots & & \\ddots & \\ddots \\\\\n& & & d_{n-1} & -1 \\\\\n0 & \\dots & & 0 & d_n\n\\end{pmatrix}\n$$\nThe elements of $U$ are the pivots $d_i \\in [(3+\\sqrt{5})/2, 3]$ and $-1$.\nThus, $\\max_{i,j} |u_{ij}| = \\max(d_1, |-1|) = \\max(3,1) = 3$.\nThe growth factor is $\\gamma = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |(A_n)_{ij}|} = \\frac{3}{3} = 1$.\nThis is a very stable situation. The bound $\\gamma \\le 2$ requested in the problem is clearly satisfied, as $\\gamma=1$. For the perturbed matrix $\\widetilde{A}$, the growth factor is expected to be close to $1$.\n\n### Task C: Numerical Experiment Design\n\nThe implementation will follow these steps for each test case $(n, \\varepsilon, \\text{seed})$:\n1.  Set the random seed for reproducibility.\n2.  Construct the matrix $A_n \\in \\mathbb{R}^{n \\times n}$.\n3.  Generate a random matrix $E \\in \\mathbb{R}^{n \\times n}$ with entries from $\\mathcal{N}(0,1)$.\n4.  Calculate the spectral norms $\\|A_n\\|_2$ and $\\|E\\|_2$. Scale $E$ to get $E' = E \\cdot (\\varepsilon \\|A_n\\|_2 / \\|E\\|_2)$.\n5.  Form the perturbed matrix $\\widetilde{A} = A_n + E'$.\n6.  Generate a random right-hand side vector $b \\in \\mathbb{R}^n$ with entries from $\\mathcal{N}(0,1)$.\n7.  Implement a custom function `lu_no_pivot` to compute the $LU$ factorization of $\\widetilde{A}$ without pivoting, yielding matrices $L$ and $U$.\n8.  Implement custom functions for forward substitution (`forward_subst` to solve $Ly=b$) and backward substitution (`backward_subst` to solve $Ux=y$) to find the solution $x$.\n9.  Compute the growth factor $\\gamma = \\max_{i,j} |U_{ij}| / \\max_{i,j} |\\widetilde{A}_{ij}|$.\n10. Compute the scaled backward residual $\\eta = \\|A_n x - b\\|_2 / (\\|A_n\\|_2 \\|x\\|_2 + \\|b\\|_2)$.\n11. Check if both $\\gamma \\le 2.1$ and $\\eta \\le 1.5\\varepsilon$ are true. Store the boolean result.\n12. After all test cases, print the list of boolean results.", "answer": "```python\nimport numpy as np\n\ndef lu_no_pivot(A):\n    \"\"\"\n    Computes the LU factorization of a square matrix A without pivoting.\n    \n    Args:\n        A (np.ndarray): The input square matrix.\n        \n    Returns:\n        (np.ndarray, np.ndarray): A tuple containing the lower triangular\n                                  matrix L and the upper triangular matrix U.\n    \"\"\"\n    n = A.shape[0]\n    U = A.astype(np.float64, copy=True)\n    L = np.eye(n, dtype=np.float64)\n    for k in range(n - 1):\n        if U[k, k] == 0:\n            # Although theoretically impossible for SDD matrices,\n            # this is a safeguard for floating point arithmetic.\n            raise ValueError(\"Zero pivot encountered during LU factorization without pivoting.\")\n        for i in range(k + 1, n):\n            m = U[i, k] / U[k, k]\n            L[i, k] = m\n            U[i, k:] -= m * U[k, k:]\n            U[i, k] = 0.0 # Enforce exact zero for numerical stability\n    return L, U\n\ndef forward_subst(L, b):\n    \"\"\"\n    Solves the lower triangular system Ly = b.\n    \n    Args:\n        L (np.ndarray): A lower triangular matrix.\n        b (np.ndarray): The right-hand side vector.\n        \n    Returns:\n        np.ndarray: The solution vector y.\n    \"\"\"\n    n = L.shape[0]\n    y = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        y[i] = b[i] - np.dot(L[i, :i], y[:i])\n    return y\n\ndef backward_subst(U, y):\n    \"\"\"\n    Solves the upper triangular system Ux = y.\n    \n    Args:\n        U (np.ndarray): An upper triangular matrix.\n        y (np.ndarray): The right-hand side vector.\n        \n    Returns:\n        np.ndarray: The solution vector x.\n    \"\"\"\n    n = U.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]\n    return x\n\ndef run_test_case(n, epsilon, seed):\n    \"\"\"\n    Runs a single stability test case.\n    \n    Args:\n        n (int): The dimension of the matrix.\n        epsilon (float): The perturbation level.\n        seed (int): The seed for the random number generator.\n        \n    Returns:\n        bool: True if the stability checks pass, False otherwise.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # 1. Construct A_n\n    A_n = np.diag(np.full(n, 3.0, dtype=np.float64)) + \\\n          np.diag(np.full(n - 1, -1.0, dtype=np.float64), k=1) + \\\n          np.diag(np.full(n - 1, -1.0, dtype=np.float64), k=-1)\n\n    # 2. Construct random perturbation E\n    E = np.random.randn(n, n)\n    \n    # Scale E\n    norm_An = np.linalg.norm(A_n, ord=2)\n    norm_E = np.linalg.norm(E, ord=2)\n    if norm_E > 0:\n        E_scaled = E * (epsilon * norm_An / norm_E)\n    else: # Should not happen in practice\n        E_scaled = np.zeros((n,n), dtype=np.float64)\n        \n    # 3. Form perturbed matrix A_tilde and random vector b\n    A_tilde = A_n + E_scaled\n    b = np.random.randn(n)\n\n    # 4. Compute LU factorization and solve A_tilde * x = b\n    try:\n        L, U = lu_no_pivot(A_tilde)\n        y = forward_subst(L, b)\n        x = backward_subst(U, y)\n    except ValueError:\n        # If a zero pivot occurs, the process is unstable by definition\n        return False\n        \n    # 5. Compute element growth factor gamma\n    max_abs_U = np.max(np.abs(U))\n    max_abs_A_tilde = np.max(np.abs(A_tilde))\n    gamma = max_abs_U / max_abs_A_tilde if max_abs_A_tilde != 0 else float('inf')\n\n    # 6. Compute scaled backward residual eta\n    residual_norm = np.linalg.norm(A_n @ x - b, ord=2)\n    norm_x = np.linalg.norm(x, ord=2)\n    norm_b = np.linalg.norm(b, ord=2)\n    \n    denominator = norm_An * norm_x + norm_b\n    eta = residual_norm / denominator if denominator > 0 else float('inf')\n\n    # 7. Check stability conditions\n    gamma_stable = (gamma <= 2.1)\n    eta_stable = (eta <= 1.5 * epsilon)\n    \n    return gamma_stable and eta_stable\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, epsilon, seed)\n        (5, 10**-12, 0),\n        (5, 10**-8, 1),\n        (5, 10**-6, 2),\n        (20, 10**-12, 3),\n        (20, 10**-8, 4),\n        (20, 10**-6, 5),\n        (50, 10**-12, 6),\n        (50, 10**-8, 7),\n        (50, 10**-6, 8),\n    ]\n\n    results = []\n    for n, epsilon, seed in test_cases:\n        is_stable = run_test_case(n, epsilon, seed)\n        results.append(str(is_stable))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3156903"}, {"introduction": "Having explored the mechanics of LU factorization and a special case where pivoting is unnecessary, we now confront the critical role of pivoting in general-purpose solvers. This problem pits a robust LU decomposition algorithm with partial pivoting against the specialized, non-pivoting Thomas algorithm for tridiagonal systems [@problem_id:3156918]. By applying both methods to matrices designed to challenge them, you will directly observe how a small pivot can cause catastrophic failure in a non-pivoting scheme and how partial pivoting elegantly resolves the issue to maintain accuracy.", "problem": "You are given the task of implementing and comparing two fundamental approaches for solving linear systems with tridiagonal coefficient matrices in order to compute a matrix inverse: Lower–Upper (LU) factorization with partial pivoting and the Thomas algorithm. Your implementation must be a complete program that computes quantitative error measures for each approach on several test cases, and reports a single aggregated line of results.\n\nBackground and definitions:\n- Let $A \\in \\mathbb{R}^{n \\times n}$ be a nonsingular tridiagonal matrix. Solving $A x = b$ for multiple right-hand sides $b$ allows us to obtain $A^{-1}$ by solving with the columns of the identity matrix. The Thomas algorithm is a specialized form of Gaussian elimination for tridiagonal systems that does not perform pivoting.\n- In contrast, LU factorization with partial pivoting (PP) computes a permutation matrix $P$, a unit lower triangular matrix $L$, and an upper triangular matrix $U$ such that $P A = L U$. To solve $A x = b$, compute $y$ from $L y = P b$, then $x$ from $U x = y$.\n- The infinity norm of a matrix $M$, denoted $\\lVert M \\rVert_{\\infty}$, is defined as the maximum absolute row sum: $\\lVert M \\rVert_{\\infty} = \\max_{1 \\leq i \\leq n} \\sum_{j=1}^{n} |M_{i j}|$.\n- The growth factor for Gaussian elimination with partial pivoting is defined as $\\rho = \\lVert U \\rVert_{\\infty} / \\lVert A \\rVert_{\\infty}$ and is a measure related to error propagation during elimination.\n\nFundamental base:\n- Solving linear systems by Gaussian elimination follows from the equivalence of elementary row operations to left-multiplication by elementary matrices and the factorization of these operations into $P$, $L$, and $U$ so that $P A = L U$. Forward and backward substitution solve $L y = c$ and $U x = y$ for triangular $L$ and $U$.\n- The Thomas algorithm is derived by applying Gaussian elimination restricted to the tridiagonal structure, performing forward elimination to zero the subdiagonal and backward substitution to recover the solution; it forgoes pivoting and thus may suffer when a pivot is small in magnitude.\n- Residual-based error assessment uses the identity $A A^{-1} = I$; in finite precision, an approximate inverse $\\widetilde{A}^{-1}$ can be evaluated by computing the residual $R = A \\widetilde{A}^{-1} - I$ and its infinity norm $\\lVert R \\rVert_{\\infty}$.\n\nYour tasks:\n1. For each provided test matrix, construct the dense matrix $A$ from its tridiagonal components (main diagonal $a$, upper diagonal $b$, lower diagonal $c$).\n2. Compute an approximate inverse $\\widetilde{A}^{-1}_{\\mathrm{LU}}$ using LU factorization with partial pivoting, by solving $A X = I$ for all columns of the identity $I$.\n3. Compute an approximate inverse $\\widetilde{A}^{-1}_{\\mathrm{T}}$ using the Thomas algorithm without pivoting, again by solving $A X = I$.\n4. For each method, compute the residual infinity norm $\\lVert A \\widetilde{A}^{-1} - I \\rVert_{\\infty}$ as a scalar float.\n5. For LU with partial pivoting, compute:\n   - The growth factor $\\rho = \\lVert U \\rVert_{\\infty} / \\lVert A \\rVert_{\\infty}$.\n   - The total number of row swaps performed during factorization as a nonnegative integer.\n6. If the Thomas algorithm encounters a zero pivot during forward elimination or back substitution, treat the inverse as failing and report the residual as $+\\infty$ for that method. Otherwise, proceed normally.\n\nTest suite:\nUse the following tridiagonal matrices, specified by their diagonals:\n- Case 1 (well-conditioned, symmetric positive definite):\n  - Size $n = 5$\n  - Main diagonal $a = [2, 2, 2, 2, 2]$\n  - Upper diagonal $b = [-1, -1, -1, -1]$\n  - Lower diagonal $c = [-1, -1, -1, -1]$\n- Case 2 (leading tiny pivot that motivates pivoting):\n  - Size $n = 5$\n  - Main diagonal $a = [10^{-12}, 2, 2, 2, 2]$\n  - Upper diagonal $b = [1, 1, 1, 1]$\n  - Lower diagonal $c = [1, 1, 1, 1]$\n- Case 3 (interior tiny pivot with larger subdiagonal to trigger pivoting):\n  - Size $n = 8$\n  - Main diagonal $a = [2, 2, 10^{-10}, 2, 2, 2, 2, 2]$\n  - Upper diagonal $b = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$\n  - Lower diagonal $c = [0.5, 0.5, 3.0, 0.5, 0.5, 0.5, 0.5]$\n\nOutput specification:\n- For each case, compute and record four values in this exact order:\n  1. $r_{\\mathrm{LU}} = \\lVert A \\widetilde{A}^{-1}_{\\mathrm{LU}} - I \\rVert_{\\infty}$ (float).\n  2. $r_{\\mathrm{T}} = \\lVert A \\widetilde{A}^{-1}_{\\mathrm{T}} - I \\rVert_{\\infty}$ (float, or $+\\infty$ if failure).\n  3. $\\rho$ (float, the LU growth factor).\n  4. $s$ (integer, the number of row swaps in LU).\n- Your program should produce a single line of output containing the results for all three cases in a single flat list of twelve values, in case order, as a comma-separated list enclosed in square brackets. For example: \"[rLU1,rT1,rho1,s1,rLU2,rT2,rho2,s2,rLU3,rT3,rho3,s3]\".\n- All numbers are dimensionless; no physical units are involved.\n\nConstraints and expectations:\n- You must implement LU factorization with partial pivoting and forward/backward substitution based on first principles. You must implement the Thomas algorithm without pivoting and detect zero pivots as described.\n- Use $\\lVert \\cdot \\rVert_{\\infty}$ as defined above for all matrix norms.\n- Ensure scientific realism and numerical plausibility; the matrices are nonsingular and the metrics are well-defined under the given rules.", "solution": "The solution will be structured as follows:\n1.  Implementation of helper functions to construct the test matrices and compute the required matrix infinity norm.\n2.  Implementation of the LU factorization with partial pivoting (`lu_factor_pivoting`) and a corresponding solver (`solve_lu`) that uses forward and backward substitution.\n3.  Implementation of the Thomas algorithm (`thomas_algorithm`), a specialized non-pivoting solver for tridiagonal systems, including detection of zero pivots.\n4.  A main routine that iterates through each test case. For each case, it will:\n    a. Construct the dense matrix $A$.\n    b. Compute the inverse $\\widetilde{A}^{-1}_{\\mathrm{LU}}$ by repeatedly calling the LU solver with columns of the identity matrix.\n    c. Compute the inverse $\\widetilde{A}^{-1}_{\\mathrm{T}}$ by repeatedly calling the Thomas algorithm solver.\n    d. Calculate the specified metrics: residual norms ($r_{\\mathrm{LU}}$, $r_{\\mathrm{T}}$), growth factor ($\\rho$), and swap count ($s$).\n5.  The final results are aggregated and formatted into a single string as specified.\n\n**1. Helper Functions**\n\nA function `build_matrix` will be created to construct the full dense matrix $A \\in \\mathbb{R}^{n \\times n}$ from its main diagonal ($a$, length $n$), upper diagonal ($b$, length $n-1$), and lower diagonal ($c$, length $n-1$).\n\nA function `infinity_norm` will implement the matrix infinity norm, $\\lVert M \\rVert_{\\infty} = \\max_{i} \\sum_{j} |M_{ij}|$, which is the maximum absolute row sum. This function will be used for computing the residual norms and the growth factor.\n\n**2. LU Factorization with Partial Pivoting**\n\nThe core of this method is a function, `lu_factor_pivoting`, which computes the decomposition $PA = LU$.\n-   Input: a square matrix $A$.\n-   Output: a unit lower triangular matrix $L$, an upper triangular matrix $U$, a permutation matrix $P$, and the integer number of row swaps.\n\nThe algorithm proceeds column by column from $k=0$ to $n-2$:\n-   **Pivoting**: For column $k$, it identifies the element with the largest absolute value on or below the diagonal, i.e., in $A_{i,k}$ for $i \\ge k$. Let this be in row $p$. If $p \\neq k$, it performs a row swap between row $k$ and row $p$. This swap is recorded in the permutation matrix $P$ and the swap counter is incremented. To maintain the structure of $L$, the already computed portions of rows $k$ and $p$ in $L$ are also swapped.\n-   **Elimination**: After pivoting, for each row $j$ below the current row $k$ (i.e., $j > k$), it computes the multiplier $L_{jk} = U_{jk} / U_{kk}$ and updates row $j$ of $U$ by subtracting $L_{jk}$ times row $k$: $U_{j,:} \\leftarrow U_{j,:} - L_{jk} U_{k,:}$.\n\nA second function, `solve_lu`, uses this decomposition to solve $Ax=b$. From $PA=LU$, we get $LUx = Pb$. This is solved in two steps:\n1.  **Forward Substitution**: Solve $Ly=Pb$ for the intermediate vector $y$. Since $L$ is unit lower triangular, this is efficient: $y_i = (Pb)_i - \\sum_{j=0}^{i-1} L_{ij} y_j$.\n2.  **Backward Substitution**: Solve $Ux=y$ for the final solution $x$. Since $U$ is upper triangular, this is also efficient: $x_i = (y_i - \\sum_{j=i+1}^{n-1} U_{ij} x_j) / U_{ii}$.\n\nTo compute the inverse $\\widetilde{A}^{-1}_{\\mathrm{LU}}$, we solve $Ax_j = e_j$ for each column $e_j$ of the identity matrix $I$. The resulting solution vectors $x_j$ form the columns of $\\widetilde{A}^{-1}_{\\mathrm{LU}}$.\n\n**3. Thomas Algorithm**\n\nThe Thomas algorithm is a streamlined form of Gaussian elimination for tridiagonal systems. The implementation, `thomas_algorithm`, takes the three diagonals $a, b, c$ and a right-hand side vector $d$ as input.\n-   **Forward Elimination**: It iterates from the second row ($i=1$) to the last, eliminating the subdiagonal element $c_{i-1}$. This is done by calculating a multiplier $m = c_{i-1} / a'_{i-1}$ and updating the main diagonal entry $a'_i \\leftarrow a_i - m \\cdot b_{i-1}$ and the right-hand side $d'_i \\leftarrow d_i - m \\cdot d'_{i-1}$. At each step, a check is performed to ensure the pivot $a'_{i-1}$ is non-zero. If a zero pivot is encountered, the algorithm fails.\n-   **Backward Substitution**: After the forward sweep, the system is upper bidiagonal. The solution is found by iterating from the last unknown $x_{n-1}$ back to the first. $x_{n-1} = d'_{n-1} / a'_{n-1}$. Then, for $i$ from $n-2$ down to $0$, $x_i = (d'_i - b_i x_{i+1}) / a'_i$. Again, each diagonal element $a'_i$ used for division is checked for being non-zero.\n\nIf a zero pivot is found at any stage, the function returns a failure signal. Otherwise, it returns the solution vector $x$. The inverse $\\widetilde{A}^{-1}_{\\mathrm{T}}$ is found column by column, like with the LU method. If any column solution fails, the entire inverse computation is considered a failure, and the residual norm $r_{\\mathrm{T}}$ is reported as $+\\infty$.\n\n**4. Metrics and Execution**\n\nThe main program defines the three test cases. For each case:\n1.  The dense matrix $A$ is constructed. The infinity norm $\\lVert A \\rVert_{\\infty}$ is calculated.\n2.  The LU factorization of $A$ is computed. The resulting $U$ matrix is used to calculate the growth factor $\\rho = \\lVert U \\rVert_{\\infty} / \\lVert A \\rVert_{\\infty}$. The swap count $s$ is recorded.\n3.  The inverse $\\widetilde{A}^{-1}_{\\mathrm{LU}}$ is calculated using the LU solver. The residual $R_{\\mathrm{LU}} = A \\widetilde{A}^{-1}_{\\mathrm{LU}} - I$ is formed, and its infinity norm $r_{\\mathrm{LU}} = \\lVert R_{\\mathrm{LU}} \\rVert_{\\infty}$ is computed.\n4.  The inverse $\\widetilde{A}^{-1}_{\\mathrm{T}}$ is calculated using the Thomas algorithm. If successful, the residual $R_{\\mathrm{T}} = A \\widetilde{A}^{-1}_{\\mathrm{T}} - I$ and its norm $r_{\\mathrm{T}} = \\lVert R_{\\mathrm{T}} \\rVert_{\\infty}$ are computed. If the Thomas algorithm fails, $r_{\\mathrm{T}}$ is set to `inf`.\n5.  The four resulting values ($r_{\\mathrm{LU}}, r_{\\mathrm{T}}, \\rho, s$) are collected.\n\nAfter processing all cases, the list of results is flattened and printed in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def build_matrix(n, a, b, c):\n        \"\"\"Constructs a dense matrix from tridiagonal components.\"\"\"\n        mat = np.zeros((n, n), dtype=float)\n        mat += np.diag(a)\n        mat += np.diag(b, k=1)\n        mat += np.diag(c, k=-1)\n        return mat\n\n    def infinity_norm(M):\n        \"\"\"Computes the infinity norm of a matrix M.\"\"\"\n        return np.max(np.sum(np.abs(M), axis=1))\n\n    def lu_factor_pivoting(A_in):\n        \"\"\"\n        Performs LU factorization with partial pivoting: PA = LU.\n        \n        Args:\n            A_in (np.ndarray): The matrix to factorize.\n\n        Returns:\n            tuple: A tuple containing:\n                L (np.ndarray): Unit lower triangular matrix.\n                U (np.ndarray): Upper triangular matrix.\n                P (np.ndarray): Permutation matrix.\n                swaps (int): Number of row swaps.\n        \"\"\"\n        n = A_in.shape[0]\n        U = A_in.copy().astype(float)\n        L = np.eye(n, dtype=float)\n        P = np.eye(n, dtype=float)\n        swaps = 0\n        \n        for k in range(n - 1):\n            # Find pivot row\n            pivot_row = np.argmax(np.abs(U[k:, k])) + k\n            \n            if np.abs(U[pivot_row, k]) < 1e-15:\n                # In the context of this problem, non-singular matrices are assumed, \n                # but this is a safeguard.\n                raise ValueError(\"Matrix is singular or near-singular.\")\n\n            if pivot_row != k:\n                # Swap rows in U\n                U[[k, pivot_row], k:] = U[[pivot_row, k], k:]\n                # Swap rows in P\n                P[[k, pivot_row], :] = P[[pivot_row, k], :]\n                # Swap the computed part of L\n                L[[k, pivot_row], :k] = L[[pivot_row, k], :k]\n                swaps += 1\n\n            # Elimination\n            for j in range(k + 1, n):\n                L[j, k] = U[j, k] / U[k, k]\n                U[j, k:] -= L[j, k] * U[k, k:]\n        \n        # Zero out the lower triangle of U for a clean upper triangular matrix\n        U = np.triu(U)\n        return L, U, P, swaps\n\n    def solve_lu(L, U, P, b):\n        \"\"\"\n        Solves Ax=b using a pre-computed LUP factorization.\n        \"\"\"\n        n = L.shape[0]\n        # Step 1: Solve Ly = Pb for y (Forward substitution)\n        Pb = P @ b\n        y = np.zeros(n, dtype=float)\n        for i in range(n):\n            y[i] = Pb[i] - np.dot(L[i, :i], y[:i])\n        \n        # Step 2: Solve Ux = y for x (Backward substitution)\n        x = np.zeros(n, dtype=float)\n        for i in range(n - 1, -1, -1):\n            if np.abs(U[i, i]) < 1e-15:\n                raise ValueError(\"Matrix is singular.\")\n            x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]\n        return x\n\n    def thomas_algorithm(a_diag, b_diag, c_diag, d_vec):\n        \"\"\"\n        Solves a tridiagonal system Ax=d using the Thomas algorithm.\n        Returns the solution vector x, or None on failure (zero pivot).\n        \"\"\"\n        n = len(a_diag)\n        a_prime = np.copy(a_diag).astype(float)\n        d_prime = np.copy(d_vec).astype(float)\n        \n        # Forward elimination\n        for i in range(1, n):\n            if np.abs(a_prime[i-1]) < 1e-15:\n                return None  # Zero pivot failure\n            m = c_diag[i-1] / a_prime[i-1]\n            a_prime[i] -= m * b_diag[i-1]\n            d_prime[i] -= m * d_prime[i-1]\n        \n        # Backward substitution\n        if np.abs(a_prime[n-1]) < 1e-15:\n            return None # Zero pivot failure\n        \n        x = np.zeros(n, dtype=float)\n        x[n-1] = d_prime[n-1] / a_prime[n-1]\n        for i in range(n - 2, -1, -1):\n            if np.abs(a_prime[i]) < 1e-15:\n                return None # Zero pivot failure\n            x[i] = (d_prime[i] - b_diag[i] * x[i+1]) / a_prime[i]\n            \n        return x\n\n    test_cases = [\n        {\n            \"n\": 5,\n            \"a\": np.array([2, 2, 2, 2, 2]),\n            \"b\": np.array([-1, -1, -1, -1]),\n            \"c\": np.array([-1, -1, -1, -1]),\n        },\n        {\n            \"n\": 5,\n            \"a\": np.array([1e-12, 2, 2, 2, 2]),\n            \"b\": np.array([1, 1, 1, 1]),\n            \"c\": np.array([1, 1, 1, 1]),\n        },\n        {\n            \"n\": 8,\n            \"a\": np.array([2, 2, 1e-10, 2, 2, 2, 2, 2]),\n            \"b\": np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n            \"c\": np.array([0.5, 0.5, 3.0, 0.5, 0.5, 0.5, 0.5]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        a, b, c = case[\"a\"], case[\"b\"], case[\"c\"]\n        \n        A = build_matrix(n, a, b, c)\n        I = np.eye(n)\n\n        # --- 1. LU with Partial Pivoting ---\n        L, U, P, s = lu_factor_pivoting(A)\n        \n        A_inv_lu = np.zeros_like(A)\n        for j in range(n):\n            A_inv_lu[:, j] = solve_lu(L, U, P, I[:, j])\n            \n        R_lu = A @ A_inv_lu - I\n        r_lu = infinity_norm(R_lu)\n        \n        rho = infinity_norm(U) / infinity_norm(A)\n\n        # --- 2. Thomas Algorithm ---\n        A_inv_t = np.zeros_like(A)\n        thomas_failed = False\n        for j in range(n):\n            col = thomas_algorithm(a, b, c, I[:, j])\n            if col is None:\n                thomas_failed = True\n                break\n            A_inv_t[:, j] = col\n        \n        if thomas_failed:\n            r_t = np.inf\n        else:\n            R_t = A @ A_inv_t - I\n            r_t = infinity_norm(R_t)\n            \n        results.extend([r_lu, r_t, rho, s])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3156918"}]}