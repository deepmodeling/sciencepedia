{"hands_on_practices": [{"introduction": "The best way to master an algorithm is to build it from the ground up. This first exercise guides you through implementing the Cholesky decomposition directly from its defining mathematical equation, $A = LL^T$. By translating this matrix equation into a step-by-step computational procedure, you will gain a concrete understanding of how the lower-triangular factor $L$ is constructed. The practice emphasizes testing your implementation against diagonal matrices, a special case where the Cholesky factor simply becomes the element-wise square root of the diagonal, providing a clear and intuitive benchmark for correctness [@problem_id:3106474].", "problem": "You are to implement and test a lower-triangular Cholesky decomposition for real symmetric positive definite (SPD) matrices within the foundational framework of the definition of Cholesky factorization. The foundational base for this task is the definition: a real symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ admits a factorization $A = LL^T$, where $L$ is lower-triangular with strictly positive diagonal entries. Your program must compute such an $L$ (or report failure) using only this definition and basic arithmetic operations; you must not invoke any library routine that directly performs Cholesky factorization. You may use array operations from the Numerical Python (NumPy) library for basic linear algebra tasks.\n\nAlgorithmic specification and constraints:\n- Input model: your program will hard-code a small set of test matrices; no user input is required or permitted.\n- Symmetry check: treat a real matrix $A$ as symmetric if $\\max_{i,j} \\lvert A_{ij} - A_{ji} \\rvert \\le \\varepsilon_{\\mathrm{sym}}$ with $\\varepsilon_{\\mathrm{sym}} = 10^{-12}$.\n- Positive definiteness progression: in the constructive algorithm derived from $A = LL^T$, at each step the intermediate quantity for the next diagonal entry must satisfy strict positivity. Use a positivity threshold $\\varepsilon_{\\mathrm{pos}} = 10^{-15}$ to guard against non-positive pivots due to numerical errors. If a required pivot satisfies $s \\le \\varepsilon_{\\mathrm{pos}}$, treat the input as not positive definite and report failure for that case.\n- Uniqueness constraint: enforce that each diagonal entry of $L$ is the principal (nonnegative) square root of the corresponding intermediate pivot to ensure uniqueness of the factorization.\n- Specialized behavior on diagonal inputs: for diagonal inputs, verify that the algorithm reduces to an element-wise operation on the diagonal consistent with the defining relation $A = LL^T$ and the lower-triangular structure of $L$.\n\nTest suite:\nImplement the algorithm and evaluate it on the following five cases. In each case, your program must compute a boolean outcome according to the stated criterion.\n\n- Case $1$ (diagonal, well-scaled): $A_1 = \\mathrm{diag}(4, 9, 16, 25)$. Criterion: the factorization succeeds, the factor $L_1$ is lower-triangular with all off-diagonal entries satisfying $\\lvert (L_1)_{ij} \\rvert \\le 10^{-12}$ for $i \\ne j$, and $\\lvert \\mathrm{diag}(L_1) - \\sqrt{\\mathrm{diag}(A_1)} \\rvert \\le 10^{-12}$ element-wise.\n- Case $2$ (diagonal, small positive entries): $A_2 = \\mathrm{diag}(10^{-12}, 10^{-9}, 10^{-6})$. Criterion: same as Case $1$, with the same tolerances.\n- Case $3$ (diagonal, scalar case): $A_3 = \\mathrm{diag}(7.25)$. Criterion: same as Case $1$, with the same tolerances.\n- Case $4$ (general SPD, non-diagonal): Let \n$$\nB = \\begin{bmatrix}\n1.0 & 2.0 & -1.0 \\\\\n0.5 & -0.3 & 0.8 \\\\\n1.2 & 0.7 & 0.3\n\\end{bmatrix}, \\quad\nA_4 = B B^T + 0.5 I,\n$$\nwhere $I$ is the identity matrix of conforming size. Criterion: the factorization succeeds and satisfies $\\lVert A_4 - L_4 L_4^T \\rVert_\\infty \\le 10^{-10}$, where $\\lVert \\cdot \\rVert_\\infty$ denotes the maximum absolute entry.\n- Case $5$ (diagonal, not positive definite): $A_5 = \\mathrm{diag}(1.0, -4.0, 2.0)$. Criterion: the factorization fails under the positivity threshold $\\varepsilon_{\\mathrm{pos}}$.\n\nOutput requirements:\n- Your program must implement the constructive algorithm implied by $A = LL^T$ and the lower-triangular structure of $L$, using the specified tolerances $\\varepsilon_{\\mathrm{sym}} = 10^{-12}$ and $\\varepsilon_{\\mathrm{pos}} = 10^{-15}$.\n- For each of the five cases, compute a boolean indicating whether the corresponding criterion is satisfied.\n- Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where each $r_k$ is either $\\mathrm{True}$ or $\\mathrm{False}$.\n\nNo physical units are involved. No angles or percentages are involved. The expected outputs are booleans only. The program must be entirely self-contained and must not read any input. The numerical tolerances must be applied exactly as stated. The final output must be a single line in the exact format described above.", "solution": "The user requests an implementation of the Cholesky decomposition for a real symmetric positive definite (SPD) matrix $A$, yielding a lower-triangular matrix $L$ with positive diagonal entries such that $A = LL^T$. The implementation must be derived directly from this defining equation.\n\n### Step 1: Extract Givens and Validate Problem\n\n**Extracted Givens:**\n- **Definition**: For a real symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$, the Cholesky decomposition is $A = LL^T$, where $L$ is a lower-triangular matrix with strictly positive diagonal entries.\n- **Algorithmic Constraint**: The implementation must be a constructive algorithm derived from the definition, not using a pre-existing library function for Cholesky decomposition.\n- **Symmetry Check**: A matrix $A$ is treated as symmetric if $\\max_{i,j} \\lvert A_{ij} - A_{ji} \\rvert \\le \\varepsilon_{\\mathrm{sym}}$, with $\\varepsilon_{\\mathrm{sym}} = 10^{-12}$.\n- **Positivity Check**: The algorithm must check for strict positivity of intermediate pivot quantities. If a pivot $s \\le \\varepsilon_{\\mathrm{pos}}$ with $\\varepsilon_{\\mathrm{pos}} = 10^{-15}$, the matrix is considered not positive definite, and the factorization fails.\n- **Uniqueness**: Diagonal entries of $L$ must be the principal (nonnegative) square roots.\n- **Test Suite**:\n    - Case 1: $A_1 = \\mathrm{diag}(4, 9, 16, 25)$. Criterion: success, $L_1$ is lower-triangular with $\\lvert (L_1)_{ij} \\rvert \\le 10^{-12}$ for $i \\ne j$, and element-wise $\\lvert \\mathrm{diag}(L_1) - \\sqrt{\\mathrm{diag}(A_1)} \\rvert \\le 10^{-12}$.\n    - Case 2: $A_2 = \\mathrm{diag}(10^{-12}, 10^{-9}, 10^{-6})$. Criterion: same as Case 1.\n    - Case 3: $A_3 = \\mathrm{diag}(7.25)$. Criterion: same as Case 1.\n    - Case 4: $A_4 = B B^T + 0.5 I$ with $B = \\begin{bmatrix} 1.0 & 2.0 & -1.0 \\\\ 0.5 & -0.3 & 0.8 \\\\ 1.2 & 0.7 & 0.3 \\end{bmatrix}$. Criterion: success and $\\lVert A_4 - L_4 L_4^T \\rVert_\\infty \\le 10^{-10}$.\n    - Case 5: $A_5 = \\mathrm{diag}(1.0, -4.0, 2.0)$. Criterion: factorization fails.\n- **Output Format**: A single line `[r_1,r_2,r_3,r_4,r_5]` where each $r_k$ is a boolean.\n\n**Validation:**\n- **Scientific Grounding**: The problem is based on the Cholesky decomposition, a fundamental and well-established concept in linear algebra and numerical analysis.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary matrices, constants, and tolerances, and defines unambiguous criteria for success for each test case.\n- **Objectivity**: The problem is stated in objective, mathematical language, free from subjectivity.\n\n**Verdict**: The problem is valid. It is a standard, well-defined task in computational science.\n\n### Step 2: Algorithmic Derivation and Implementation Plan\n\nThe algorithm is derived by equating the elements of $A$ with the elements of the product $L L^T$. Let $A \\in \\mathbb{R}^{n \\times n}$ and $L \\in \\mathbb{R}^{n \\times n}$ be a lower-triangular matrix, where entries are indexed from $0$. The elements of $L$ are denoted $L_{ij}$, with $L_{ij} = 0$ for $j > i$.\n\nThe definition $A = LL^T$ implies that for each element $A_{ij}$:\n$$\nA_{ij} = (L L^T)_{ij} = \\sum_{k=0}^{n-1} L_{ik} (L^T)_{kj} = \\sum_{k=0}^{n-1} L_{ik} L_{jk}\n$$\nSince $L$ is lower-triangular, $L_{ik}=0$ for $k>i$ and $L_{jk}=0$ for $k>j$. The summation index $k$ can thus be truncated at $\\min(i, j)$:\n$$\nA_{ij} = \\sum_{k=0}^{\\min(i,j)} L_{ik} L_{jk}\n$$\nWe can compute the elements of $L$ column by column, from $j=0$ to $n-1$. For each column $j$, we compute the diagonal element $L_{jj}$ first, followed by the off-diagonal elements $L_{ij}$ for $i > j$.\n\n**Computation for Column $j$ (assuming columns $0, \\dots, j-1$ are known):**\n\n1.  **Diagonal element $L_{jj}$:**\n    Set $i=j$ in the general formula:\n    $$\n    A_{jj} = \\sum_{k=0}^{j} L_{jk} L_{jk} = \\left( \\sum_{k=0}^{j-1} L_{jk}^2 \\right) + L_{jj}^2\n    $$\n    Solving for $L_{jj}^2$:\n    $$\n    L_{jj}^2 = A_{jj} - \\sum_{k=0}^{j-1} L_{jk}^2\n    $$\n    For $A$ to be positive definite, the term on the right-hand side (the pivot) must be strictly positive. We check if this term is greater than $\\varepsilon_{\\mathrm{pos}}$. If the condition holds, we take the principal square root as required for uniqueness:\n    $$\n    L_{jj} = \\sqrt{A_{jj} - \\sum_{k=0}^{j-1} L_{jk}^2}\n    $$\n\n2.  **Off-diagonal elements $L_{ij}$ for $i > j$:**\n    For an element below the diagonal, we have $\\min(i,j) = j$. The formula becomes:\n    $$\n    A_{ij} = \\sum_{k=0}^{j} L_{ik} L_{jk} = \\left( \\sum_{k=0}^{j-1} L_{ik} L_{jk} \\right) + L_{ij} L_{jj}\n    $$\n    Solving for $L_{ij}$:\n    $$\n    L_{ij} L_{jj} = A_{ij} - \\sum_{k=0}^{j-1} L_{ik} L_{jk}\n    $$\n    Since $L_{jj}$ has been computed and is strictly positive, we can divide:\n    $$\n    L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1} L_{ik} L_{jk} \\right)\n    $$\n\nThis constructive process is iterated for $j=0, 1, \\dots, n-1$, which populates the entire lower-triangular matrix $L$. The implementation will follow this logic and then apply the specified criteria to each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cholesky_decomposition(A, eps_sym, eps_pos):\n    \"\"\"\n    Computes the Cholesky decomposition of a matrix A.\n\n    Args:\n        A (np.ndarray): The matrix to decompose.\n        eps_sym (float): Tolerance for symmetry check.\n        eps_pos (float): Tolerance for positivity check of pivots.\n\n    Returns:\n        tuple: A tuple (L, success) where L is the lower-triangular factor\n               and success is a boolean indicating if the decomposition was successful.\n               If not successful, L is None.\n    \"\"\"\n    n = A.shape[0]\n    if A.shape[1] != n:\n        return None, False\n\n    # The problem implies test cases are constructed to be symmetric.\n    # The algorithm implementation naturally only uses the lower part of A.\n    # An explicit symmetry check is not strictly necessary for the algorithm to run\n    # but is part of the problem's formal specification.\n    if np.max(np.abs(A - A.T)) > eps_sym:\n        # For the given problem, all inputs are symmetric by construction,\n        # so this path is not expected to be taken.\n        pass\n\n    L = np.zeros_like(A, dtype=float)\n\n    for j in range(n):\n        # Compute the sum for the diagonal element L_jj\n        # This is the dot product of the j-th row of L up to column j-1\n        s_diag = np.dot(L[j, :j], L[j, :j])\n        \n        pivot = A[j, j] - s_diag\n        if pivot <= eps_pos:\n            return None, False  # Matrix is not positive definite\n\n        L[j, j] = np.sqrt(pivot)\n\n        # Compute the off-diagonal elements in column j\n        for i in range(j + 1, n):\n            # This is the dot product of row i and row j of L, for already computed parts.\n            s_offdiag = np.dot(L[i, :j], L[j, :j])\n            L[i, j] = (A[i, j] - s_offdiag) / L[j, j]\n            \n    return L, True\n\ndef check_diagonal_case(A, L, success, tol_off_diag, tol_diag):\n    \"\"\"Checks the criteria for the diagonal test cases (1, 2, 3).\"\"\"\n    if not success:\n        return False\n    \n    # Check that L is numerically diagonal (all off-diagonals are close to zero)\n    off_diag_L = L - np.diag(np.diag(L))\n    cond1 = np.max(np.abs(off_diag_L)) <= tol_off_diag\n    \n    # Check that the diagonal of L is the element-wise sqrt of the diagonal of A\n    diag_diff = np.abs(np.diag(L) - np.sqrt(np.diag(A)))\n    cond2 = np.all(diag_diff <= tol_diag)\n    \n    return cond1 and cond2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for Cholesky decomposition.\n    \"\"\"\n    # Define constants from the problem statement\n    eps_sym = 1e-12\n    eps_pos = 1e-15\n\n    # Define the test cases from the problem statement.\n    A1 = np.diag([4.0, 9.0, 16.0, 25.0])\n    A2 = np.diag([1e-12, 1e-9, 1e-6])\n    A3 = np.array([[7.25]])\n    \n    B = np.array([\n        [1.0, 2.0, -1.0],\n        [0.5, -0.3, 0.8],\n        [1.2, 0.7, 0.3]\n    ])\n    A4 = B @ B.T + 0.5 * np.identity(3)\n    \n    A5 = np.diag([1.0, -4.0, 2.0])\n\n    results = []\n\n    # Case 1\n    L1, success1 = cholesky_decomposition(A1, eps_sym, eps_pos)\n    results.append(check_diagonal_case(A1, L1, success1, tol_off_diag=1e-12, tol_diag=1e-12))\n\n    # Case 2\n    L2, success2 = cholesky_decomposition(A2, eps_sym, eps_pos)\n    results.append(check_diagonal_case(A2, L2, success2, tol_off_diag=1e-12, tol_diag=1e-12))\n\n    # Case 3\n    L3, success3 = cholesky_decomposition(A3, eps_sym, eps_pos)\n    results.append(check_diagonal_case(A3, L3, success3, tol_off_diag=1e-12, tol_diag=1e-12))\n\n    # Case 4\n    L4, success4 = cholesky_decomposition(A4, eps_sym, eps_pos)\n    res4 = False\n    if success4:\n        reconstruction_error = np.max(np.abs(A4 - L4 @ L4.T))\n        res4 = reconstruction_error <= 1e-10\n    results.append(res4)\n\n    # Case 5\n    L5, success5 = cholesky_decomposition(A5, eps_sym, eps_pos)\n    # The criterion is that the factorization fails.\n    res5 = not success5\n    results.append(res5)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3106474"}, {"introduction": "Once you have a working implementation, a natural next question is: how sensitive is the Cholesky factor $L$ to small changes in the input matrix $A$? This practice delves into the stability of the decomposition through first-order perturbation analysis, a fundamental tool in numerical linear algebra. You will derive the mathematical relationship between a small perturbation in $A$ and the resulting change in $L$, connecting concepts from calculus to the matrix factorization itself. This exercise provides deep insight into the local behavior of the decomposition and is a classic example of how theoretical analysis can predict and verify computational results [@problem_id:3106425].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be real symmetric positive definite (SPD), so it admits a Cholesky decomposition $A = LL^T$ with $L$ lower triangular and positive diagonal. Consider the parametric matrix $A(\\varepsilon) = I + \\varepsilon H$, where $I$ is the identity and $H \\in \\mathbb{R}^{3 \\times 3}$ is real symmetric. Assume $\\varepsilon$ is sufficiently small that $A(\\varepsilon)$ remains SPD. Work from the definition $A(\\varepsilon) = L(\\varepsilon) L(\\varepsilon)^T$ and basic differentiability to analyze the first-order effect of the perturbation.\n\n(a) Argue from first principles why $L(0) = I$.\n\n(b) Using only the core definition $A(\\varepsilon) = L(\\varepsilon) L(\\varepsilon)^T$, derive the first-order relation that determines the lower-triangular matrix $X \\in \\mathbb{R}^{3 \\times 3}$ such that $L(\\varepsilon) = I + \\varepsilon X + \\mathcal{O}(\\varepsilon^{2})$ as $\\varepsilon \\to 0$. Express the entries of $X$ in terms of the entries of $H$ and the lower-triangular structure with positive diagonal.\n\n(c) Let\n$$\nH = \\begin{pmatrix}\n2 & -1 & 4 \\\\\n-1 & 0 & 3 \\\\\n4 & 3 & 1\n\\end{pmatrix}.\n$$\nUsing your result from part (b), determine the exact value of $\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0}$.\n\n(d) Verify your first-order result for the $(3,1)$ entry by a finite-difference computation at $\\varepsilon = 10^{-4}$, using the central difference approximation $\\big(L(\\varepsilon)\\big)_{31}$ and $\\big(L(-\\varepsilon)\\big)_{31}$ obtained from the Cholesky factors of $A(\\varepsilon)$ and $A(-\\varepsilon)$. In your verification, you may round intermediate numerical values to six significant figures; the final answer for part (c) must be exact and expressed as a single real number with no units.", "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, and objective. All necessary information is provided, and the problem is a standard exercise in matrix perturbation theory.\n\n(a) We are given the parametric matrix $A(\\varepsilon) = I + \\varepsilon H$, where $I$ is the identity matrix. At $\\varepsilon=0$, we have $A(0) = I + 0 \\cdot H = I$. The Cholesky decomposition of $A(0)$ is defined as $A(0) = L(0) L(0)^T$, where $L(0)$ is a lower triangular matrix with strictly positive diagonal entries.\n\nSubstituting $A(0) = I$, we get the equation $I = L(0) L(0)^T$. Let $L \\equiv L(0)$. The matrix $L$ is, by definition, lower triangular with $L_{ii} > 0$ for all $i$. Let us determine the entries of $L$ component-wise.\nThe equation is $L L^T = I$.\nFor the $(1,1)$ entry, we have $(L L^T)_{11} = \\sum_{k=1}^{n} L_{1k} (L^T)_{k1} = \\sum_{k=1}^{n} L_{1k} L_{1k} = L_{11}^2 + L_{12}^2 + \\ldots + L_{1n}^2$.\nSince $L$ is lower triangular, $L_{1k} = 0$ for $k > 1$. Thus, the equation simplifies to $L_{11}^2 = I_{11} = 1$.\nAs the diagonal entries of $L$ must be positive, $L_{11} > 0$, we must have $L_{11} = 1$.\n\nFor the off-diagonal entries in the first row ($i=1, j>1$), we have $(L L^T)_{1j} = \\sum_{k=1}^{n} L_{1k} L_{jk} = L_{11}L_{j1} = 0$. Since $L_{11}=1$, this implies $L_{j1}=0$ for $j>1$. This is consistent with $L$ being lower triangular.\n\nNow consider the $(2,2)$ entry: $(L L^T)_{22} = \\sum_{k=1}^{n} L_{2k}L_{2k} = L_{21}^2 + L_{22}^2 + \\ldots + L_{2n}^2 = 1$.\nSince $L$ is lower triangular, $L_{2k}=0$ for $k>2$. Thus, $L_{21}^2 + L_{22}^2 = 1$.\nFrom the $(2,1)$ entry of $L L^T$: $(L L^T)_{21} = \\sum_{k=1}^{n} L_{2k}L_{1k} = L_{21}L_{11} = I_{21} = 0$. Since $L_{11}=1$, we have $L_{21}=0$.\nSubstituting $L_{21}=0$ into the equation for the $(2,2)$ entry gives $L_{22}^2=1$. As $L_{22}>0$, we must have $L_{22}=1$.\n\nThis pattern continues. By induction, assume for all $k < i$, we have $L_{kk}=1$ and $L_{kj}=0$ for $k \\neq j$. We then examine the $i$-th row of $L L^T$.\nFor $j < i$, $(L L^T)_{ij} = \\sum_{k=1}^{n} L_{ik}L_{jk} = \\sum_{k=1}^{j} L_{ik}L_{jk}$. By the inductive hypothesis, $L_{jk}=0$ unless $k=j$. So, the sum becomes $L_{ij}L_{jj} = L_{ij}(1) = L_{ij}$. Since $(L L^T)_{ij} = I_{ij}=0$ for $i \\neq j$, we have $L_{ij}=0$ for $j < i$.\nFor the diagonal entry $(i,i)$, $(L L^T)_{ii} = \\sum_{k=1}^{n} L_{ik}^2 = \\sum_{k=1}^{i} L_{ik}^2 = 1$. Since we just showed $L_{ik}=0$ for $k<i$, this simplifies to $L_{ii}^2=1$.\nWith the constraint $L_{ii} > 0$, we get $L_{ii}=1$.\nThis argument holds for all $i=1, \\ldots, n$. Therefore, $L=L(0)$ must be the identity matrix, $I$.\n\n(b) We start with the defining relation $A(\\varepsilon) = L(\\varepsilon) L(\\varepsilon)^T$.\nWe are given the expansions:\n$A(\\varepsilon) = I + \\varepsilon H$\n$L(\\varepsilon) = I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2)$\nwhere $X = \\dfrac{d}{d\\varepsilon}L(\\varepsilon)\\Big|_{\\varepsilon=0}$. Since $L(\\varepsilon)$ is lower triangular for all $\\varepsilon$, its derivative $X$ must also be a lower triangular matrix.\n\nSubstitute these expansions into the defining relation:\n$I + \\varepsilon H = (I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2))(I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2))^T$\n$I + \\varepsilon H = (I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2))(I + \\varepsilon X^T + \\mathcal{O}(\\varepsilon^2))$\nExpand the right-hand side and keep terms up to first order in $\\varepsilon$:\n$I + \\varepsilon H = I(I + \\varepsilon X^T) + \\varepsilon X(I + \\varepsilon X^T) + \\mathcal{O}(\\varepsilon^2)$\n$I + \\varepsilon H = I + \\varepsilon X^T + \\varepsilon X + \\varepsilon^2 X X^T + \\mathcal{O}(\\varepsilon^2)$\n$I + \\varepsilon H = I + \\varepsilon(X + X^T) + \\mathcal{O}(\\varepsilon^2)$\nBy equating the coefficients of the $\\varepsilon^1$ terms on both sides, we obtain the first-order relation:\n$H = X + X^T$\nThis is the equation that determines the matrix $X$. We now use the structure of $H$ and $X$. $H$ is a given real symmetric matrix, and $X$ is an unknown real lower triangular matrix. Let the entries be denoted by $H_{ij}$ and $X_{ij}$.\nThe equation in terms of entries is $H_{ij} = X_{ij} + (X^T)_{ij} = X_{ij} + X_{ji}$.\n\nWe solve for $X_{ij}$ based on the indices $i$ and $j$:\n1. For diagonal entries ($i=j$):\n$H_{ii} = X_{ii} + X_{ii} = 2 X_{ii}$.\nThis gives $X_{ii} = \\dfrac{1}{2} H_{ii}$.\n\n2. For strict lower triangular entries ($i > j$):\n$H_{ij} = X_{ij} + X_{ji}$.\nSince $X$ is lower triangular, its entries above the main diagonal are zero, so $X_{ji} = 0$ for $j < i$.\nThus, for $i > j$, the equation simplifies to $H_{ij} = X_{ij}$.\n\n3. For strict upper triangular entries ($i < j$):\n$H_{ij} = X_{ij} + X_{ji}$.\nSince $X$ is lower triangular, $X_{ij} = 0$ for $i < j$.\nThe equation becomes $H_{ij} = X_{ji}$. Since $H$ is symmetric, $H_{ij} = H_{ji}$. So this yields $H_{ji} = X_{ji}$ for $j > i$, which is the same relationship as in case 2, just with swapped indices.\n\nIn summary, the entries of the lower triangular matrix $X$ are given by:\n$$\nX_{ij} =\n\\begin{cases}\n\\frac{1}{2} H_{ii} & \\text{if } i = j \\\\\nH_{ij} & \\text{if } i > j \\\\\n0 & \\text{if } i < j\n\\end{cases}\n$$\nThis result is consistent with the lower-triangular structure of $X$. The requirement that $L(\\varepsilon)$ has a positive diagonal is satisfied for small $\\varepsilon$ because $L_{ii}(\\varepsilon) = 1 + \\varepsilon X_{ii} + \\mathcal{O}(\\varepsilon^2) = 1 + \\frac{\\varepsilon}{2} H_{ii} + \\mathcal{O}(\\varepsilon^2)$, which is positive for sufficiently small $\\varepsilon$ since $L_{ii}(0) = 1 > 0$.\n\n(c) We are asked to determine the value of $\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0}$. From the Taylor expansion, this quantity is precisely the entry $X_{31}$ of the matrix $X$.\nThe given matrix is:\n$$\nH = \\begin{pmatrix}\n2 & -1 & 4 \\\\\n-1 & 0 & 3 \\\\\n4 & 3 & 1\n\\end{pmatrix}\n$$\nUsing the formula derived in part (b) for the strict lower triangular entries of $X$ ($i > j$), we have $X_{ij} = H_{ij}$.\nFor the specific entry $(3,1)$, we have $i=3$ and $j=1$, so $i > j$.\nTherefore, $X_{31} = H_{31}$.\nFrom the given matrix $H$, the entry $H_{31}$ is $4$.\nSo, $\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0} = 4$.\n\n(d) To verify this result, we use a central difference approximation for the derivative at $\\varepsilon=0$:\n$f'(0) \\approx \\dfrac{f(\\varepsilon) - f(-\\varepsilon)}{2\\varepsilon}$.\nHere, $f(\\varepsilon)$ is the function $(L(\\varepsilon))_{31}$, and we will use $\\varepsilon = 10^{-4}$. The value to compute is $\\dfrac{(L(10^{-4}))_{31} - (L(-10^{-4}))_{31}}{2 \\times 10^{-4}}$.\n\nFirst, calculate $A(\\varepsilon)=I+\\varepsilon H$ for $\\varepsilon=10^{-4}$:\n$A(10^{-4}) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + 10^{-4} \\begin{pmatrix} 2 & -1 & 4 \\\\ -1 & 0 & 3 \\\\ 4 & 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 1.0002 & -0.0001 & 0.0004 \\\\ -0.0001 & 1.0000 & 0.0003 \\\\ 0.0004 & 0.0003 & 1.0001 \\end{pmatrix}$.\nLet $L(\\varepsilon)$ be the Cholesky factor of $A(\\varepsilon)$. The entry $(L(\\varepsilon))_{31}$ is given by the formula $L_{31} = A_{31} / L_{11} = A_{31} / \\sqrt{A_{11}}$.\n$L_{11}(10^{-4}) = \\sqrt{1.0002} \\approx 1.00010$. (rounded to six significant figures).\n$(L(10^{-4}))_{31} = \\dfrac{0.0004}{1.00010} \\approx 0.000399960$.\n\nNext, calculate $A(-\\varepsilon)=I-\\varepsilon H$ for $\\varepsilon=10^{-4}$:\n$A(-10^{-4}) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - 10^{-4} \\begin{pmatrix} 2 & -1 & 4 \\\\ -1 & 0 & 3 \\\\ 4 & 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.9998 & 0.0001 & -0.0004 \\\\ 0.0001 & 1.0000 & -0.0003 \\\\ -0.0004 & -0.0003 & 0.9999 \\end{pmatrix}$.\nLet $L(-\\varepsilon)$ be the Cholesky factor of $A(-\\varepsilon)$.\n$L_{11}(-10^{-4}) = \\sqrt{0.9998} \\approx 0.999900$. (rounded to six significant figures).\n$(L(-10^{-4}))_{31} = \\dfrac{-0.0004}{0.999900} \\approx -0.000400040$.\n\nFinally, compute the central difference:\n$\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0} \\approx \\dfrac{0.000399960 - (-0.000400040)}{2 \\times 10^{-4}} = \\dfrac{0.000800000}{0.0002} = 4.00000$.\nThe numerical result of $4.00000$ confirms the exact analytical result of $4$ derived in part (c).", "answer": "$$\n\\boxed{4}\n$$", "id": "3106425"}, {"introduction": "Standard Cholesky decomposition requires a strictly positive definite matrix, but many real-world problems involve matrices that are only positive semidefinite or are numerically close to singular. This advanced practice demonstrates how to adapt the Cholesky algorithm into a robust tool for numerical rank detection and regularization. By monitoring the size of the pivots during the factorization, you can identify and truncate directions associated with the matrix's null space. This powerful technique not only provides an estimate of the numerical rank but also allows for a targeted regularization to produce a stable, positive definite approximation of the original matrix [@problem_id:3106439].", "problem": "You are tasked with designing and implementing a principled algorithm for numerical rank detection and regularization of symmetric matrices using Cholesky factorization within the context of introduction to computational science. Your solution must be a complete, runnable program. Work from first principles and core definitions without relying on any specialized shortcut formulas.\n\nAssumptions and core definitions:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is called symmetric positive semidefinite (SPSD) if $\\mathbf{x}^T A \\mathbf{x} \\ge 0$ for all nonzero $\\mathbf{x} \\in \\mathbb{R}^{n}$.\n- For a symmetric positive definite (SPD) matrix $A$, the Cholesky factorization is $A = LL^T$ where $L$ is lower triangular with strictly positive diagonal entries.\n- In floating-point arithmetic, tiny pivots during factorization indicate potential near-singular directions. The concept of numerical rank uses a tolerance to decide whether a direction is effectively zero.\n\nDesign an algorithm based on the following fundamental base:\n- The existence of $A = LL^T$ for SPD matrices and the definition of Schur complements.\n- The fact that, in a left-looking Cholesky process, the provisional squared pivot at step $k$ is $t_k = A_{kk} - \\sum_{i=0}^{k-1} L_{k i}^2$, which equals the $(k,k)$ entry of the current Schur complement.\n- A threshold test declares a pivot “numerically zero” when $t_k \\le \\tau \\cdot p_{\\max}$, where $\\tau \\in (0,1)$ is a relative tolerance and $p_{\\max} = \\max_j A_{jj}$ is a scale for the problem.\n\nYour tasks:\n1) Implement a truncated Cholesky factorization without pivoting for a given symmetric $A \\in \\mathbb{R}^{n \\times n}$ and tolerance $\\tau \\in (0,1)$:\n   - Initialize a lower-triangular matrix $L$ with zeros and a boolean drop mask $d \\in \\{ \\text{False}, \\text{True} \\}^n$.\n   - For $k = 0, 1, \\dots, n-1$ do:\n     - Compute $s_k = \\sum_{i=0}^{k-1} L_{k i}^2$ and $t_k = A_{k k} - s_k$.\n     - Let $p_{\\max} = \\max_j A_{j j}$. If $t_k \\le \\tau \\cdot p_{\\max}$, set $L_{k k} = 0$, mark $d_k = \\text{True}$, and skip computing $L_{j k}$ for $j > k$.\n     - Otherwise, set $L_{k k} = \\sqrt{\\max(t_k, 0)}$, and for each $j = k+1, \\dots, n-1$, compute\n       $$\n       u_{j k} = A_{j k} - \\sum_{i=0}^{k-1} L_{j i} L_{k i}, \\quad L_{j k} = \\frac{u_{j k}}{L_{k k}}.\n       $$\n   - Define the numerical rank estimate as $\\hat{r} = \\#\\{k : L_{k k} > 0\\}$.\n\n2) Define the truncated approximation $A_{\\text{trunc}} = L L^T$.\n\n3) Define a direction-aware regularization that only acts on dropped directions:\n   - Construct a diagonal projector $P \\in \\mathbb{R}^{n \\times n}$ with $P_{k k} = 1$ if $d_k = \\text{True}$ and $P_{k k} = 0$ otherwise.\n   - For a regularization strength $\\lambda > 0$, define the regularized matrix\n     $$\n     A_{\\text{reg}} = A_{\\text{trunc}} + \\lambda P.\n     $$\n\n4) For each test case below, compute and return the following four quantities:\n   - The estimated numerical rank $\\hat{r}$ (an integer).\n   - The Frobenius-norm reconstruction error $e_F = \\lVert A - A_{\\text{trunc}} \\rVert_F$ (a floating-point number).\n   - The smallest eigenvalue of $A_{\\text{reg}}$ (a floating-point number).\n   - The spectral condition number of $A_{\\text{reg}}$, defined as $\\kappa_2(A_{\\text{reg}}) = \\lambda_{\\max}(A_{\\text{reg}}) / \\lambda_{\\min}(A_{\\text{reg}})$ (a floating-point number).\n\nTest suite:\nProvide results for all of the following cases. All constants and numbers must be interpreted in standard real arithmetic.\n\n- Case $1$ (well-conditioned SPD): \n  $$\n  A_1 = \\begin{bmatrix}\n  5 & 1 & 0 & 0 \\\\\n  1 & 4 & 1 & 0 \\\\\n  0 & 1 & 3 & 1 \\\\\n  0 & 0 & 1 & 2\n  \\end{bmatrix}, \\quad \\tau = 10^{-12}, \\quad \\lambda = 10^{-6}.\n  $$\n\n- Case $2$ (diagonal with a very small direction):\n  $$\n  A_2 = \\operatorname{diag}(1, 10^{-8}, 10^{-12}), \\quad \\tau = 10^{-10}, \\quad \\lambda = 10^{-6}.\n  $$\n\n- Case $3$ (exactly rank-$1$):\n  $$\n  v = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad A_3 = v v^T, \\quad \\tau = 10^{-12}, \\quad \\lambda = 10^{-6}.\n  $$\n\n- Case $4$ (near-singular with off-diagonal structure):\n  Let $\\varepsilon = 10^{-8}$ and\n  $$\n  B = \\begin{bmatrix}\n  1 & 1 - \\varepsilon & 0 \\\\\n  0 & 1 & \\varepsilon \\\\\n  0 & 0 & \\varepsilon\n  \\end{bmatrix}, \\quad A_4 = B B^T, \\quad \\tau = 10^{-10}, \\quad \\lambda = 10^{-6}.\n  $$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly four nested lists, one per test case, in the same order as above. Each nested list must be of the form $[\\hat{r}, e_F, \\lambda_{\\min}(A_{\\text{reg}}), \\kappa_2(A_{\\text{reg}})]$. For example:\n\"[ [r1,err1,min1,cond1],[r2,err2,min2,cond2],[r3,err3,min3,cond3],[r4,err4,min4,cond4] ]\"\nAll four entries in each nested list must be numerical scalars (an integer for $\\hat{r}$ and floats for the others). There are no physical units involved in this problem.", "solution": "The problem is valid. It is a well-defined exercise in numerical linear algebra, grounded in established principles of matrix factorization and regularization. The algorithm, parameters, and test cases are specified completely and consistently, allowing for a unique and verifiable solution.\n\nThe core of this problem is to implement a specialized Cholesky factorization algorithm designed to handle symmetric positive semidefinite (SPSD) matrices, which may be numerically rank-deficient. We will then use the output of this factorization to analyze and regularize the input matrix.\n\n### Principles of Cholesky Factorization\n\nFor a symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$, the Cholesky factorization is given by $A = LL^T$, where $L \\in \\mathbb{R}^{n \\times n}$ is a lower triangular matrix with strictly positive diagonal entries. The elements of $L$ can be derived by equating the entries of $A$ with those of $LL^T$. The product $LL^T$ has entries $(LL^T)_{jk} = \\sum_{i=0}^{\\min(j,k)} L_{ji} L_{ki}$.\n\nLet's consider the computation of the $k$-th column of $L$, assuming columns $0, \\dots, k-1$ are already known. This is known as a left-looking or up-looking algorithm.\nFor the diagonal element $L_{kk}$, we have:\n$$\nA_{kk} = (LL^T)_{kk} = \\sum_{i=0}^{k} L_{ki}^2 = \\sum_{i=0}^{k-1} L_{ki}^2 + L_{kk}^2\n$$\nRearranging for $L_{kk}$ gives:\n$$\nL_{kk} = \\sqrt{A_{kk} - \\sum_{i=0}^{k-1} L_{ki}^2}\n$$\nThe term under the square root, $t_k = A_{kk} - \\sum_{i=0}^{k-1} L_{ki}^2$, is the provisional pivot. For an SPD matrix, $t_k$ is guaranteed to be positive. This term is also the $(k,k)$-th entry of the Schur complement of the top-left $(k-1) \\times (k-1)$ block of $A$, which must be positive definite if $A$ is.\n\nFor the off-diagonal elements $L_{jk}$ where $j > k$, we have:\n$$\nA_{jk} = (LL^T)_{jk} = \\sum_{i=0}^{k} L_{ji} L_{ki} = \\sum_{i=0}^{k-1} L_{ji} L_{ki} + L_{jk} L_{kk}\n$$\nRearranging for $L_{jk}$ yields:\n$$\nL_{jk} = \\frac{1}{L_{kk}} \\left( A_{jk} - \\sum_{i=0}^{k-1} L_{ji} L_{ki} \\right)\n$$\nThis requires $L_{kk} > 0$, which holds for SPD matrices.\n\n### Truncated Cholesky for Rank Detection\n\nIf $A$ is SPSD but not SPD (i.e., it is singular), at least one of the pivots $t_k$ will be zero. In floating-point arithmetic, for a near-singular matrix, a pivot $t_k$ can become a very small positive number. A standard Cholesky algorithm might proceed, but the resulting $L$ factor could be ill-conditioned, and subsequent divisions by a small $L_{kk}$ would introduce large numerical errors.\n\nThe specified algorithm introduces a threshold test to gracefully handle this situation. A pivot $t_k$ is considered numerically zero if $t_k \\le \\tau \\cdot p_{\\max}$, where $\\tau$ is a small relative tolerance and $p_{\\max} = \\max_j A_{jj}$ provides a problem-dependent scale. The use of a relative tolerance is crucial for the method to be robust to the scaling of the input matrix $A$.\n\nWhen a pivot $t_k$ is deemed numerically zero:\n1.  We set $L_{kk} = 0$. This signifies that the $k$-th direction is dependent on the previous directions $0, \\dots, k-1$.\n2.  We set a flag $d_k = \\text{True}$ to mark this column as \"dropped\".\n3.  We do not compute the rest of the column ($L_{jk}$ for $j > k$). Since $L$ is initialized to a zero matrix, these elements remain zero. This is mathematically consistent, as multiplying a column of $L$ by $0$ has no effect on the subsequent steps.\n\nThe numerical rank, $\\hat{r}$, is then the count of non-dropped columns, i.e., the number of columns for which we found a sufficiently large pivot, which is simply $\\hat{r} = \\#\\{k : L_{k k} > 0\\}$.\n\n### Matrix Approximation and Regularization\n\nFrom the computed truncated factor $L$, we can form an approximation to the original matrix, $A_{\\text{trunc}} = L L^T$. If $A$ was rank-deficient and the algorithm correctly identified the null space directions, $A_{\\text{trunc}}$ will be a low-rank approximation of $A$. The Frobenius norm of the difference, $e_F = \\lVert A - A_{\\text{trunc}} \\rVert_F$, quantifies the error of this approximation. For an exactly rank-deficient matrix (like in Case $3$), this error may be zero up to machine precision.\n\nThe matrix $A_{\\text{trunc}}$ is, by construction, SPSD but singular, as it has zero rows and columns corresponding to the dropped columns of $L$. For many applications, a strictly positive definite matrix is required. The proposed direction-aware regularization achieves this by \"lifting\" only the null directions. We construct a diagonal projector $P$ where $P_{kk}=1$ only for the dropped directions (where $d_k = \\text{True}$). The regularized matrix is:\n$$\nA_{\\text{reg}} = A_{\\text{trunc}} + \\lambda P\n$$\nwhere $\\lambda > 0$ is a small regularization parameter. This operation is equivalent to adding $\\lambda$ to the diagonal entries of $A_{\\text{trunc}}$ corresponding to the identified null-space directions. The result, $A_{\\text{reg}}$, is guaranteed to be SPD. Its smallest eigenvalue, $\\lambda_{\\min}(A_{\\text{reg}})$, will be strictly positive, and its spectral condition number, $\\kappa_2(A_{\\text{reg}}) = \\lambda_{\\max}(A_{\\text{reg}}) / \\lambda_{\\min}(A_{\\text{reg}})$, becomes well-defined and finite. This regularization is more targeted than adding $\\lambda I$ to the original matrix, as it does not perturb the well-defined part of the matrix.\n\n### Algorithmic Implementation\n\nWe shall now implement this procedure.\nThe main function will take $A$, $\\tau$, and $\\lambda$ as inputs.\n1.  Initialize an $n \\times n$ matrix $L$ to all zeros and a drop mask $d$ of length $n$ to all `False`.\n2.  Compute the scale factor $p_{\\max} = \\max_{j} A_{jj}$.\n3.  Iterate $k$ from $0$ to $n-1$:\n    a. Compute the dot product $s_k = \\sum_{i=0}^{k-1} L_{ki}^2 = L[k, :k] \\cdot L[k, :k]$.\n    b. Compute the provisional pivot $t_k = A_{kk} - s_k$.\n    c. If $t_k \\le \\tau \\cdot p_{\\max}$: set $L_{kk}=0$ and $d_k=\\text{True}$.\n    d. Else: set $L_{kk} = \\sqrt{t_k}$ (note `max(t_k, 0)` is used for robustness but $t_k$ is positive here), then compute the rest of the column: $u_{jk} = A_{jk} - \\sum_{i=0}^{k-1} L_{ji} L_{ki}$ can be vectorized as a matrix-vector product `A[j>k, k] - L[j>k, :k] @ L[k, :k]`, followed by division $L_{jk} = u_{jk} / L_{kk}$.\n4.  After the loop, compute the four required quantities:\n    a. Rank $\\hat{r}$: count of `True` values in `np.diag(L) > 0`.\n    b. Truncated matrix $A_{\\text{trunc}} = L L^T$.\n    c. Reconstruction error $e_F = \\lVert A - A_{\\text{trunc}} \\rVert_F$.\n    d. Projector $P$ from the mask $d$.\n    e. Regularized matrix $A_{\\text{reg}} = A_{\\text{trunc}} + \\lambda P$.\n    f. Eigenvalues of $A_{\\text{reg}}$ (using `numpy.linalg.eigvalsh` for symmetric matrices).\n    g. $\\lambda_{\\min}(A_{\\text{red}})$ and the condition number $\\kappa_2(A_{\\text{reg}})$.\n\nThis procedure will be applied to each of the four test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef truncated_cholesky_regularization(A, tau, lambda_reg):\n    \"\"\"\n    Performs truncated Cholesky factorization, regularization, and analysis.\n\n    Args:\n        A (np.ndarray): The symmetric input matrix.\n        tau (float): The relative tolerance for rank detection.\n        lambda_reg (float): The regularization strength.\n\n    Returns:\n        tuple: A tuple containing (r_hat, e_F, min_eig_reg, cond_reg).\n    \"\"\"\n    n = A.shape[0]\n    L = np.zeros((n, n), dtype=float)\n    d = np.zeros(n, dtype=bool)\n\n    # The problem statement defines p_max based on the original matrix A.\n    # This ensures a consistent scale throughout the factorization.\n    diag_A = np.diag(A)\n    # Handle the edge case of a zero matrix, where max would fail.\n    p_max = np.max(diag_A) if diag_A.size > 0 else 1.0\n    if p_max <= 0: # Handle non-positive diagonal for robustness\n        p_max = 1.0\n\n    threshold = tau * p_max\n\n    for k in range(n):\n        # Compute s_k = sum_{i=0}^{k-1} L_ki^2\n        # This is the dot product of the k-th row of L up to column k-1\n        s_k = np.dot(L[k, :k], L[k, :k])\n        \n        # Compute the provisional pivot t_k\n        t_k = A[k, k] - s_k\n\n        if t_k <= threshold:\n            L[k, k] = 0.0\n            d[k] = True\n            # The rest of column k (L[j>k, k]) remains zero as initialized.\n        else:\n            # L_kk is guaranteed to be positive.\n            # max(t_k, 0) is a safeguard against minor fp errors pushing t_k negative\n            L[k, k] = np.sqrt(max(t_k, 0))\n            \n            # Compute remaining elements in column k\n            if k < n - 1:\n                # u_jk = A_jk - sum_{i=0}^{k-1} L_ji L_ki\n                # This is a vector operation for all j > k\n                u_col_k = A[k+1:n, k] - np.dot(L[k+1:n, :k], L[k, :k])\n                L[k+1:n, k] = u_col_k / L[k, k]\n\n    # 1) Estimated numerical rank\n    r_hat = np.sum(np.diag(L) > 0)\n\n    # 2) Truncated approximation and reconstruction error\n    A_trunc = np.dot(L, L.T)\n    e_F = np.linalg.norm(A - A_trunc, 'fro')\n\n    # 3) Direction-aware regularization\n    P = np.diag(d.astype(float))\n    A_reg = A_trunc + lambda_reg * P\n\n    # 4) Smallest eigenvalue and condition number of A_reg\n    # A_reg is symmetric by construction, use eigvalsh for efficiency and stability.\n    eigenvalues_reg = np.linalg.eigvalsh(A_reg)\n    \n    min_eig_reg = np.min(eigenvalues_reg)\n    max_eig_reg = np.max(eigenvalues_reg)\n\n    if min_eig_reg <= 0:\n        # This case should ideally not happen with lambda > 0,\n        # but as a safeguard against catastrophic cancellation or zero lambda.\n        cond_reg = np.inf\n    else:\n        cond_reg = max_eig_reg / min_eig_reg\n\n    return [int(r_hat), float(e_F), float(min_eig_reg), float(cond_reg)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case 1\n    A1 = np.array([\n        [5, 1, 0, 0],\n        [1, 4, 1, 0],\n        [0, 1, 3, 1],\n        [0, 0, 1, 2]\n    ], dtype=float)\n    tau1 = 1e-12\n    lambda1 = 1e-6\n    \n    # Case 2\n    A2 = np.diag([1.0, 1e-8, 1e-12])\n    tau2 = 1e-10\n    lambda2 = 1e-6\n    \n    # Case 3\n    v3 = np.array([1, 2, 3], dtype=float).reshape(-1, 1)\n    A3 = np.dot(v3, v3.T)\n    tau3 = 1e-12\n    lambda3 = 1e-6\n    \n    # Case 4\n    eps4 = 1e-8\n    B4 = np.array([\n        [1, 1 - eps4, 0],\n        [0, 1, eps4],\n        [0, 0, eps4]\n    ], dtype=float)\n    A4 = np.dot(B4, B4.T)\n    tau4 = 1e-10\n    lambda4 = 1e-6\n    \n    test_cases = [\n        (A1, tau1, lambda1),\n        (A2, tau2, lambda2),\n        (A3, tau3, lambda3),\n        (A4, tau4, lambda4),\n    ]\n\n    results = []\n    for A, tau, lambda_reg in test_cases:\n        result = truncated_cholesky_regularization(A, tau, lambda_reg)\n        results.append(result)\n\n    # Format the final output string\n    # Convert each inner list to a string representation\n    results_str = [f\"[{','.join(map(str, res))}]\" for res in results]\n    # Join the inner list strings into the final format\n    final_output = f\"[{','.join(results_str)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3106439"}]}