{"hands_on_practices": [{"introduction": "The Secant method is a powerful open method that approximates roots by iteratively drawing lines through the two most recent points on a function's graph. This practice [@problem_id:2422713] provides fundamental experience in translating this algorithm into working code. By finding the non-trivial solutions to the equation $\\sin(x) = x/2$, you will implement an iterative process from scratch, manage stopping criteria, and observe how different initial guesses can lead to different roots.", "problem": "Consider the nonlinear equation in a single real variable, expressed in radians, given by $f(x) = \\sin(x) - \\dfrac{x}{2}$. A solution is any real number $x$ for which $f(x) = 0$. The trivial solution is $x = 0$. Your task is to compute numerical approximations to solutions of $f(x) = 0$ from specified initial seeds without using any external data. Angles must be treated in radians.\n\nUse the following test suite. Each test case provides an ordered pair of real-valued seeds $(x_0, x_1)$. For each test case, compute an approximation $\\hat{x}$ to a root of $f(x)$, using only the information implied by the problem statement and the seeds, and impose the following stopping criteria simultaneously: terminate when either the absolute value of the function at the current approximation satisfies $\\lvert f(\\hat{x}) \\rvert \\le \\tau$ or the change in successive approximations satisfies $\\lvert \\Delta \\hat{x} \\rvert \\le \\tau$, where $\\tau = 10^{-12}$. Impose a hard limit of $N_{\\max} = 100$ iterations per test case. If convergence is not achieved within $N_{\\max}$ iterations, return the last iterate as the approximation.\n\nTest suite (all seeds are in radians):\n- Case 1: $(x_0, x_1) = (1.0, 2.0)$\n- Case 2: $(x_0, x_1) = (-1.0, -2.0)$\n- Case 3: $(x_0, x_1) = (0.1, -0.1)$\n- Case 4: $(x_0, x_1) = (1.0, 1.5)$\n\nRequirements for numerical details:\n- Treat angles in radians.\n- Use the function $f(x) = \\sin(x) - \\dfrac{x}{2}$ exactly as stated.\n- Use the tolerance $\\tau = 10^{-12}$ and the iteration cap $N_{\\max} = 100$ as defined above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the approximated root $\\hat{x}$ rounded to $10$ decimal places. The final line must therefore have the form $[\\hat{x}_1,\\hat{x}_2,\\hat{x}_3,\\hat{x}_4]$, where each $\\hat{x}_k$ is a decimal-formatted real number rounded to $10$ decimal places. No other output is permitted.", "solution": "The problem presented requires the numerical approximation of roots for the nonlinear transcendental equation $f(x) = \\sin(x) - \\dfrac{x}{2} = 0$. The roots, aside from the trivial solution $x=0$, are the points of intersection between the functions $y = \\sin(x)$ and $y = \\dfrac{x}{2}$. A straightforward analysis of the graphs of these functions reveals one positive root, one negative root, and the trivial root at the origin.\n\nThe problem specifies that an open method for root finding must be used, and for each test case, two initial seeds, $(x_0, x_1)$, are provided. This setup uniquely determines the choice of method: the Secant method. This is a two-point iterative method that approximates the function $f(x)$ with a secant line passing through the two most recent approximations, $(x_{k-1}, f(x_{k-1}))$ and $(x_k, f(x_k))$. The next approximation, $x_{k+1}$, is the root of this secant line.\n\nThe recurrence relation for the Secant method is given by:\n$$ x_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} $$\nThe algorithm begins with the initial seeds $x_0$ and $x_1$ and iteratively generates a sequence of approximations $\\{x_k\\}$.\n\nThe implementation will proceed as follows for each test case $(x_0, x_1)$:\n$1$. Initialize the first two points in the sequence, $x_{k-1} = x_0$ and $x_k = x_1$.\n$2$. Enter a loop that iterates a maximum of $N_{\\max} = 100$ times.\n$3$. Inside the loop, compute the function values $f(x_k)$ and $f(x_{k-1})$.\n$4$. A critical check must be performed on the denominator, $f(x_k) - f(x_{k-1})$. If its absolute value is close to zero, the secant line is nearly horizontal, and the method becomes unstable or fails due to division by zero. In such a scenario, the iteration must be terminated.\n$5$. Compute the next approximation, $\\hat{x} \\equiv x_{k+1}$, using the secant formula.\n$6$. The iteration terminates if either of the two specified stopping criteria is met:\n    a) The residual error is sufficiently small: $|f(\\hat{x})| \\le \\tau$.\n    b) The change between successive approximations is sufficiently small: $|\\Delta \\hat{x}| = |x_{k+1} - x_k| \\le \\tau$.\nThe tolerance is specified as $\\tau = 10^{-12}$.\n$7$. If convergence is not achieved, the iterants are updated: $x_{k-1}$ is set to $x_k$, and $x_k$ is set to $x_{k+1}$. The loop then continues.\n$8$. If the loop completes $N_{\\max}$ iterations without convergence, the last computed approximation $x_k$ is returned as the result.\n\nBrief analysis of the test cases:\n- Cases $1$ and $2$ with initial seeds $(1.0, 2.0)$ and $(-1.0, -2.0)$ respectively, bracket the non-trivial positive and negative roots. The function $f(x)$ is odd, $f(-x) = -f(x)$, so the negative root is expected to be the additive inverse of the positive root. The Secant method is expected to converge rapidly.\n- Case $3$, with seeds $(0.1, -0.1)$, uses initial points symmetric about the origin. Due to the odd symmetry of the function $f(x)$, the first iteration of the secant method will yield $x_2 = 0$, the exact trivial root.\n- Case $4$, with seeds $(1.0, 1.5)$, presents a more challenging scenario. The interval $[1.0, 1.5]$ contains a local maximum of $f(x)$ where $f'(x) = \\cos(x) - \\frac{1}{2} = 0$, which occurs at $x = \\arccos(0.5) = \\frac{\\pi}{3} \\approx 1.047$ radians. The initial secant line will have a small slope, which can project the next iterate far from the root, but the algorithm is robust enough to eventually converge to the correct positive root.\n\nThe final program will implement this algorithm for each test case and format the resulting approximations as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerical approximations for roots of f(x) = sin(x) - x/2\n    using the Secant method for a given set of test cases.\n    \"\"\"\n\n    def f(x: float) -> float:\n        \"\"\"The nonlinear function f(x) = sin(x) - x/2.\"\"\"\n        return np.sin(x) - x / 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 2.0),\n        (-1.0, -2.0),\n        (0.1, -0.1),\n        (1.0, 1.5),\n    ]\n\n    # Parameters from the problem statement.\n    TAU = 1e-12\n    N_MAX = 100\n\n    results = []\n    for case in test_cases:\n        x_prev, x_curr = case\n        \n        for _ in range(N_MAX):\n            f_curr = f(x_curr)\n            f_prev = f(x_prev)\n\n            # Denominator for the Secant method formula\n            denominator = f_curr - f_prev\n\n            # If the denominator is very close to zero, the method may fail or stagnate.\n            # This can happen if f(x_curr) is very close to f(x_prev).\n            # The loop will naturally terminate if convergence is not achieved,\n            # so we only protect against division by zero.\n            if abs(denominator)  1e-15:  # Use a small epsilon to avoid division by zero\n                break\n\n            # Calculate the next approximation using the Secant method formula\n            x_next = x_curr - f_curr * (x_curr - x_prev) / denominator\n\n            # Check the stopping criteria:\n            # 1. The absolute value of the function at the new approximation.\n            # 2. The absolute change in successive approximations.\n            if abs(f(x_next)) = TAU or abs(x_next - x_curr) = TAU:\n                x_curr = x_next  # Update to the final converged value\n                break\n\n            # Update points for the next iteration\n            x_prev = x_curr\n            x_curr = x_next\n        \n        # Append the final approximation for this case to the results list.\n        # The problem requires rounding to 10 decimal places.\n        results.append(x_curr)\n\n    # Final print statement in the exact required format.\n    # The format specifier '.10f' ensures rounding to 10 decimal places.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "2422713"}, {"introduction": "Newton's method is famous for its rapid, quadratic convergence, but this speed is lost when finding a root with multiplicity $m \\gt 1$. This advanced practice [@problem_id:3260046] explores this critical limitation by having you first derive why the convergence degrades to linear. You will then implement powerful corrections that restore quadratic convergence, first by using a known multiplicity and then by estimating it adaptively from the function's derivatives.", "problem": "Consider the open root-finding method known as Newton’s method applied to the polynomial with a multiple root. Let the function be defined by $f(x) = (x - 2)^2(x + 1)$, which has a root at $x = 2$ of multiplicity $m = 2$ and a root at $x = -1$ of multiplicity $m = 1$. Use a rigorous analysis based on first principles to investigate the effect of root multiplicity on the local convergence behavior of Newton’s method, and then design and implement multiplicity corrections with two approaches: using known multiplicity $m$ and using an $m$ estimated from derivatives.\n\nYour derivation must begin from the following fundamental bases:\n- The definition of a root of multiplicity $m$: If $r$ is a root of $f$, then there exists a function $g$ with $g(r) \\neq 0$ such that $f(x) = (x - r)^m g(x)$.\n- The first-order Taylor expansion of a sufficiently smooth function $f$ about a point $x_k$: $f(x_{k+1}) \\approx f(x_k) + f'(x_k)(x_{k+1} - x_k)$.\n- The definition of Newton’s method as the solution update obtained by setting the first-order Taylor approximation to zero near a root, and the notion of local error $e_k = x_k - r$.\n\nFrom these bases:\n- Derive the local error recursion of Newton’s method near a root of multiplicity $m$ and determine the resulting convergence order and asymptotic error ratio.\n- Derive a multiplicity-corrected Newton update for the case where the multiplicity $m$ is known.\n- Derive a practical estimator for the multiplicity $m$ expressed in terms of $f$, $f'$, and $f''$ only, and use it to construct a multiplicity-corrected Newton update when $m$ is unknown.\n\nImplementation requirements:\n- Implement three methods: standard Newton’s method, multiplicity-corrected Newton’s method with known $m$, and multiplicity-corrected Newton’s method with $m$ estimated from derivatives.\n- For numerical experiments, use the explicit derivatives of $f(x) = (x - 2)^2(x + 1)$ and compute the empirical convergence order from iterates. Define the local error with respect to a chosen target root $r$ as $e_k = x_k - r$, and compute the empirical order by using three consecutive errors via the formula\n$$\np \\approx \\frac{\\ln\\left(\\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert\\right)}{\\ln\\left(\\lvert e_k \\rvert / \\lvert e_{k-1} \\rvert\\right)}.\n$$\nAlso report the last available asymptotic error ratio $\\rho \\approx \\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert$ at termination using the last two nonzero errors. If fewer than three nonzero errors are available for the order estimate, report $p = 0.0$; if fewer than two nonzero errors are available for the ratio, report $\\rho = 0.0$.\n\nStopping criteria for the iteration:\n- Terminate when $\\lvert f(x_k) \\rvert \\leq 10^{-16}$ or $\\lvert e_k \\rvert \\leq 10^{-16}$, or after $50$ iterations, whichever occurs first.\n\nTest suite:\n- Use the following $6$ test cases, each defined by a method label, initial guess $x_0$, and target root $r$:\n    1. Standard Newton’s method, $x_0 = 3.5$, $r = 2$.\n    2. Multiplicity-corrected Newton’s method with known $m = 2$, $x_0 = 3.5$, $r = 2$.\n    3. Multiplicity-corrected Newton’s method with $m$ estimated from derivatives, $x_0 = 3.5$, $r = 2$.\n    4. Standard Newton’s method, $x_0 = -0.8$, $r = -1$.\n    5. Multiplicity-corrected Newton’s method with $m$ estimated from derivatives, $x_0 = -0.8$, $r = -1$.\n    6. Standard Newton’s method, $x_0 = 2.1$, $r = 2$.\n\nFor each test case, report two floats rounded to six decimal places:\n- The empirical convergence order $p$ computed from the last three nonzero errors.\n- The last asymptotic error ratio $\\rho$ computed from the last two nonzero errors.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of pairs enclosed in square brackets, with each pair formatted as $[p,\\rho]$. The overall output should look like\n$[[p_1,\\rho_1],[p_2,\\rho_2],[p_3,\\rho_3],[p_4,\\rho_4],[p_5,\\rho_5],[p_6,\\rho_6]]$.", "solution": "The problem statement is a well-posed and scientifically grounded exercise in numerical analysis, specifically concerning the convergence properties of Newton's method for roots of varying multiplicity. All provided information is self-contained, mathematically consistent, and directly relevant to the topic of open root-finding methods. The problem is therefore deemed valid and a full solution follows.\n\nThe core of the problem is to analyze and remedy the degradation of Newton's method's convergence rate when applied to a root of multiplicity $m  1$. We will proceed by first deriving the convergence behavior of the standard method, then deriving two corrected methods, and finally implementing them to verify the theoretical results.\n\nThe function under consideration is $f(x) = (x - 2)^2(x + 1)$. This polynomial has a root $r=2$ of multiplicity $m=2$ and a simple root $r=-1$ of multiplicity $m=1$. For implementation, its derivatives are required:\n$f(x) = x^3 - 3x^2 + 4$\n$f'(x) = 3x^2 - 6x$\n$f''(x) = 6x - 6$\n\n### 1. Local Convergence of Standard Newton's Method\n\nLet $r$ be a root of multiplicity $m \\ge 1$. By definition, $f(x)$ can be written as $f(x) = (x-r)^m g(x)$, where $g(r) \\neq 0$.\nThe first derivative is $f'(x) = m(x-r)^{m-1}g(x) + (x-r)^m g'(x)$.\n\nThe standard Newton's iteration is given by:\n$$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$$\nSubstituting the expressions for $f(x_k)$ and $f'(x_k)$:\n$$x_{k+1} = x_k - \\frac{(x_k-r)^m g(x_k)}{m(x_k-r)^{m-1}g(x_k) + (x_k-r)^m g'(x_k)}$$\nLet the error at step $k$ be $e_k = x_k - r$. Then the error at the next step is $e_{k+1} = x_{k+1} - r$.\n$$e_{k+1} = e_k - \\frac{e_k^m g(x_k)}{m e_k^{m-1}g(x_k) + e_k^m g'(x_k)}$$\nFactoring out $e_k^{m-1}$ from the denominator's fraction:\n$$e_{k+1} = e_k - \\frac{e_k g(x_k)}{m g(x_k) + e_k g'(x_k)} = e_k \\left( 1 - \\frac{g(x_k)}{m g(x_k) + e_k g'(x_k)} \\right)$$\n$$e_{k+1} = e_k \\left( \\frac{m g(x_k) + e_k g'(x_k) - g(x_k)}{m g(x_k) + e_k g'(x_k)} \\right) = e_k \\left( \\frac{(m-1) g(x_k) + e_k g'(x_k)}{m g(x_k) + e_k g'(x_k)} \\right)$$\nAs $x_k \\to r$, we have $e_k \\to 0$ and $g(x_k) \\to g(r)$. The expression for the error becomes:\n$$e_{k+1} \\approx e_k \\left( \\frac{(m-1) g(r)}{m g(r)} \\right) = \\left( \\frac{m-1}{m} \\right) e_k$$\nThis demonstrates that for $m  1$, the error is reduced by a constant factor in each step. This is the definition of linear convergence (order $p=1$). The asymptotic error ratio is $\\rho = \\lim_{k\\to\\infty} \\frac{\\lvert e_{k+1} \\rvert}{\\lvert e_k \\rvert} = \\frac{m-1}{m}$. For $m=2$, as in the problem, we expect linear convergence with $\\rho = 1/2$.\n\nFor a simple root ($m=1$), the ratio is $(1-1)/1 = 0$, implying super-linear convergence. A more detailed Taylor series analysis shows that for $m=1$, the convergence is quadratic ($p=2$), with $e_{k+1} \\approx \\frac{f''(r)}{2f'(r)}e_k^2$.\n\n### 2. Multiplicity-Corrected Newton's Method (Known $m$)\n\nTo restore quadratic convergence, we can modify the iteration. Consider a new function $u(x) = f(x)^{1/m}$. If $f(x) = (x-r)^m g(x)$, then $u(x) = (x-r)g(x)^{1/m}$. Since $g(r) \\neq 0$, the function $u(x)$ has a simple root at $x=r$. Applying standard Newton's method to $u(x)$ yields quadratic convergence to its root $r$.\n\nThe iteration for $u(x)$ is $x_{k+1} = x_k - \\frac{u(x_k)}{u'(x_k)}$. We can express this in terms of $f(x_k)$ and $f'(x_k)$:\n$$u'(x) = \\frac{d}{dx} \\left(f(x)^{1/m}\\right) = \\frac{1}{m}f(x)^{\\frac{1}{m}-1}f'(x)$$\nThe step correction is:\n$$\\frac{u(x)}{u'(x)} = \\frac{f(x)^{1/m}}{\\frac{1}{m}f(x)^{\\frac{1}{m}-1}f'(x)} = \\frac{f(x)^{1/m}}{\\frac{f(x)^{1/m}}{m f(x)}f'(x)} = m \\frac{f(x)}{f'(x)}$$\nThis leads to the multiplicity-corrected Newton's method:\n$$x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$$\nSince this is equivalent to applying standard Newton's method to a function with a simple root, the convergence is quadratic ($p=2$).\n\n### 3. Multiplicity-Corrected Newton's Method (Estimated $m$)\n\nWhen $m$ is unknown, it can be estimated from the function and its derivatives. We can find a limit expression for $m$. Using the asymptotic forms $f(x) \\propto (x-r)^m$, $f'(x) \\propto m(x-r)^{m-1}$, and $f''(x) \\propto m(m-1)(x-r)^{m-2}$ near the root $r$, we can form a ratio where the unknown terms cancel.\n$$\\lim_{x\\to r} \\frac{f(x)f''(x)}{[f'(x)]^2} = \\frac{(x-r)^m g(r) \\cdot m(m-1)(x-r)^{m-2} g(r)}{[m(x-r)^{m-1} g(r)]^2} = \\frac{m(m-1)(x-r)^{2m-2}}{m^2(x-r)^{2m-2}} = \\frac{m-1}{m}$$\nWe can solve this for $m$:\n$$m = \\frac{1}{1 - \\frac{f(x)f''(x)}{[f'(x)]^2}} = \\frac{[f'(x)]^2}{[f'(x)]^2 - f(x)f''(x)}$$\nThis suggests an adaptive scheme where at each iteration $k$, we estimate $m$ as:\n$$m_k = \\frac{[f'(x_k)]^2}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$$\nAnd use this estimate in the corrected Newton update:\n$$x_{k+1} = x_k - m_k \\frac{f(x_k)}{f'(x_k)} = x_k - \\frac{f(x_k)f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$$\nThis is recognized as Halley's method. This method is known to restore quadratic convergence ($p=2$) for multiple roots and to achieve cubic convergence ($p=3$) for simple roots.\n\n### Summary of Implemented Methods\nThe implementation will analyze the following three iterative schemes:\n1.  **Standard Newton:** $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$\n2.  **Corrected (known $m$):** $x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$\n3.  **Corrected (estimated $m$):** $x_{k+1} = x_k - \\frac{f(x_k)f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$\n\nThese methods will be applied to the test cases to empirically calculate the convergence order $p$ and asymptotic error ratio $\\rho$, which are expected to align with the theoretical derivations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem concerning Newton's method and root multiplicity.\n    \"\"\"\n\n    def f(x_val):\n        \"\"\"The function f(x) = (x-2)^2(x+1).\"\"\"\n        # x**3 - 3*x**2 + 4\n        return x_val**3 - 3 * x_val**2 + 4\n\n    def df(x_val):\n        \"\"\"The first derivative f'(x).\"\"\"\n        # 3*x**2 - 6*x\n        return 3 * x_val**2 - 6 * x_val\n\n    def d2f(x_val):\n        \"\"\"The second derivative f''(x).\"\"\"\n        # 6*x - 6\n        return 6 * x_val - 6\n\n    def run_iteration(method, x_0, r, m_known=None):\n        \"\"\"\n        Runs a specified root-finding iteration and computes empirical convergence metrics.\n\n        Args:\n            method (str): The method to use ('standard', 'corrected_known_m', 'corrected_estimated_m').\n            x_0 (float): The initial guess.\n            r (float): The target root.\n            m_known (int, optional): The known multiplicity for the corrected method.\n\n        Returns:\n            A tuple (p, rho) containing the empirical order and asymptotic ratio.\n        \"\"\"\n        x = float(x_0)\n        errors = [x - r]\n        max_iter = 50\n        tol = 1e-16\n\n        for _ in range(max_iter):\n            fx = f(x)\n\n            if abs(fx) = tol or abs(x - r) = tol:\n                break\n\n            dfx = df(x)\n            if abs(dfx)  np.finfo(float).eps:\n                break\n\n            if method == 'standard':\n                x = x - fx / dfx\n            elif method == 'corrected_known_m':\n                x = x - m_known * fx / dfx\n            elif method == 'corrected_estimated_m':\n                d2fx = d2f(x)\n                denominator = dfx**2 - fx * d2fx\n                if abs(denominator)  np.finfo(float).eps:\n                    m_est = 1.0  # Fallback to standard Newton step\n                else:\n                    m_est = dfx**2 / denominator\n                x = x - m_est * fx / dfx\n            else:\n                raise ValueError(f\"Unknown method: {method}\")\n\n            errors.append(x - r)\n\n        # Using a small threshold to filter out errors that are effectively zero\n        nonzero_errors = [e for e in errors if abs(e) > 1e-18]\n\n        p = 0.0\n        if len(nonzero_errors) >= 3:\n            try:\n                e_k_plus_1 = abs(nonzero_errors[-1])\n                e_k = abs(nonzero_errors[-2])\n                e_k_minus_1 = abs(nonzero_errors[-3])\n\n                log_num = np.log(e_k_plus_1 / e_k)\n                log_den = np.log(e_k / e_k_minus_1)\n\n                if abs(log_den) > np.finfo(float).eps:\n                    p = log_num / log_den\n            except (ValueError, ZeroDivisionError):\n                p = 0.0 # Calculation failed\n\n        rho = 0.0\n        if len(nonzero_errors) >= 2:\n            try:\n                e_k_plus_1 = abs(nonzero_errors[-1])\n                e_k = abs(nonzero_errors[-2])\n                rho = e_k_plus_1 / e_k\n            except ZeroDivisionError:\n                rho = 0.0\n\n        return p, rho\n\n    test_cases = [\n        # (method, initial_guess_x0, target_root_r, known_multiplicity_m)\n        ('standard', 3.5, 2, None),                # Case 1\n        ('corrected_known_m', 3.5, 2, 2),         # Case 2\n        ('corrected_estimated_m', 3.5, 2, None),  # Case 3\n        ('standard', -0.8, -1, None),               # Case 4\n        ('corrected_estimated_m', -0.8, -1, None), # Case 5\n        ('standard', 2.1, 2, None)                 # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        method, x0, r, m_known = case\n        p, rho = run_iteration(method, x0, r, m_known)\n        results.append([round(p, 6), round(rho, 6)])\n\n    formatted_results = [f\"[{p_val:.6f},{rho_val:.6f}]\" for p_val, rho_val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3260046"}, {"introduction": "In practical scientific computing, algorithms are often combined to create more robust and efficient tools. This practice [@problem_id:3167328] moves from implementation to design, challenging you to engineer a hybrid solver. You will build an algorithm that begins with the inexpensive Secant method to approach a solution and then intelligently switches to the faster Newton's method once the local function behavior is favorable, a technique that mirrors strategies used in professional software libraries.", "problem": "Consider the scalar nonlinear equation $f(x)=x-e^{-x}$. The goal is to design and implement a hybrid open method for root finding that begins with a secant-style approach to move into a convergence basin and switches to a Newton-style update once the local derivative magnitude is sufficiently large, as determined by a user-specified threshold. Start from the fundamental base that an approximate root of a differentiable function $f(x)$ can be iteratively improved by using local linearization and finite-difference slope approximations. The method must satisfy the following principles:\n\n- Use a first-order local linear model of $f(x)$ to derive an update that seeks $f(x)=0$.\n- Use a finite-difference approximation to the slope when two distinct iterates are available.\n- Use the exact derivative once the magnitude of the derivative $|f'(x)|$ exceeds a specified threshold to accelerate convergence.\n\nImplementation specification:\n\n1. Implement a function that attempts to solve $f(x)=0$ for $f(x)=x-e^{-x}$ using a hybrid method that:\n   - Starts with a secant-style update based on two initial guesses $x_0$ and $x_1$.\n   - On each iteration, after forming the next iterate $x_{k+1}$, evaluate the magnitude of the derivative $|f'(x_{k+1})|$. If $|f'(x_{k+1})|$ exceeds a threshold $\\tau$, switch all subsequent iterations to a Newton-style update that uses the analytic derivative of $f(x)$.\n   - Terminate when either the absolute function value $|f(x_k)|$ is less than a tolerance $\\varepsilon$ or the absolute change $|x_{k}-x_{k-1}|$ is less than $\\varepsilon$, or when a maximum iteration count $N_{\\max}$ is reached. If $N_{\\max}$ is reached without satisfying the tolerance, return the last iterate.\n\n2. The derivative used for the switching decision and Newton-style update must be computed analytically for the given $f(x)$.\n\n3. Numerical parameters:\n   - Use a tolerance of $\\varepsilon=10^{-12}$.\n   - Use the specified iteration limits $N_{\\max}$ per test case.\n\n4. Output format:\n   - For each test case, return the final approximation to the root as a floating-point number rounded to $10$ decimal places.\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$).\n\nTest suite and coverage:\n\n- Case $1$ (happy path, moderate threshold, early switch near the basin): $x_0=0.0$, $x_1=1.0$, $\\tau=1.4$, $\\varepsilon=10^{-12}$, $N_{\\max}=50$.\n- Case $2$ (boundary: high threshold so Newton-style update is never used, secant-only convergence): $x_0=10.0$, $x_1=12.0$, $\\tau=1.6$, $\\varepsilon=10^{-12}$, $N_{\\max}=100$.\n- Case $3$ (edge: immediate switch because the derivative magnitude is large at the early iterates): $x_0=-3.0$, $x_1=-2.0$, $\\tau=1.1$, $\\varepsilon=10^{-12}$, $N_{\\max}=50$.\n- Case $4$ (edge: very small $N_{\\max}$ leading to premature termination with a coarse approximation): $x_0=0.0$, $x_1=2.0$, $\\tau=1.5$, $\\varepsilon=10^{-12}$, $N_{\\max}=2$.\n\nAll answers are pure numbers without physical units or angles. The function $f(x)$ is dimensionless. Ensure numerical stability in the presence of small finite-difference denominators by deferring to the Newton-style update only when the switching criterion is satisfied, and otherwise proceed with the secant-style update using the last two iterates.", "solution": "The problem requires the design and implementation of a hybrid numerical method to find the root of the scalar nonlinear equation $f(x) = x - e^{-x}$. The method begins with the secant method and transitions to Newton's method based on a specific criterion.\n\nThe function under consideration is $f(x) = x - e^{-x}$. To implement Newton's method and the switching criterion, the analytical derivative of $f(x)$ is required. The derivative, $f'(x)$, is computed as:\n$$\nf'(x) = \\frac{d}{dx}(x - e^{-x}) = 1 - (-e^{-x}) = 1 + e^{-x}\n$$\nThe function $f(x)$ is continuous and differentiable for all $x \\in \\mathbb{R}$. Since $e^{-x}  0$ for all real $x$, the derivative $f'(x) = 1 + e^{-x}  1$. This implies that $f(x)$ is a strictly monotonically increasing function. A strictly monotonic function can cross the x-axis at most once, guaranteeing that the equation $f(x)=0$ has a unique real root.\n\nThe hybrid algorithm is constructed from two fundamental open root-finding methods: the secant method and Newton's method. Both are based on finding the root of a local linear approximation of the function.\n\nThe secant method approximates the derivative $f'(x_k)$ using a backward finite difference based on the two preceding iterates, $x_k$ and $x_{k-1}$:\n$$\nf'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n$$\nSubstituting this approximation into the Newton's method formula yields the secant method update rule:\n$$\nx_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}\n$$\nThis method exhibits superlinear convergence (with an order of approximately $1.618$) but does not require the computation of the analytical derivative.\n\nNewton's method (also known as the Newton-Raphson method) uses the tangent line to the function at the current iterate $x_k$ to approximate the next iterate. This is derived from the first-order Taylor series expansion of $f(x)$ around $x_k$:\n$$\nf(x) \\approx f(x_k) + f'(x_k)(x - x_k)\n$$\nBy setting $f(x) = 0$ and designating the new approximation of the root as $x_{k+1}$, we solve for $x_{k+1}$:\n$$\n0 = f(x_k) + f'(x_k)(x_{k+1} - x_k) \\implies x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\nThis method typically demonstrates quadratic convergence near the root, provided the derivative is non-zero and well-behaved.\n\nThe specified hybrid algorithm synthesizes these two approaches. It begins with the secant method, using two initial guesses $x_0$ and $x_1$. At each iteration $k$, a new iterate $x_{k+1}$ is computed. The algorithm then evaluates the magnitude of the analytical derivative at this new point, $|f'(x_{k+1})|$. If this value exceeds a given threshold $\\tau$, the algorithm switches permanently to Newton's method for all subsequent iterations. This strategy leverages the computationally cheaper secant method to approach the basin of convergence, then switches to the faster-converging Newton's method once the local geometry of the function (as indicated by the derivative's magnitude) is deemed suitable.\n\nThe algorithm's implementation maintains a state variable, for instance, a boolean flag `use_newton`, initialized to false. The iterative process is as follows:\n1. Initialize with $x_{k-1} = x_0$ and $x_k = x_1$.\n2. In each iteration, check for termination based on step size: $|x_k - x_{k-1}|  \\varepsilon$.\n3. If `use_newton` is false, compute the next iterate $x_{k+1}$ using the secant rule. Otherwise, compute $x_{k+1}$ using the Newton's rule.\n4. Update the stored iterates: $x_{k-1} \\leftarrow x_k$ and $x_k \\leftarrow x_{k+1}$.\n5. Check for termination based on function value: $|f(x_k)|  \\varepsilon$.\n6. If `use_newton` is currently false, evaluate the switching criterion: if $|f'(x_k)|  \\tau$, set `use_newton` to true for all future iterations.\n7. This process continues until one of the termination criteria is met or the maximum number of iterations, $N_{\\max}$, is reached. If $N_{\\max}$ is reached, the last computed iterate is returned.\n\nThis design provides a robust procedure for finding the root, with the test cases designed to exercise the various logical paths of the algorithm: a standard switch, a secant-only path, an immediate switch, and premature termination.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    \n    def f(x: float) -> float:\n        \"\"\"The scalar nonlinear function f(x) = x - e^(-x).\"\"\"\n        return x - np.exp(-x)\n\n    def f_prime(x: float) -> float:\n        \"\"\"The analytical derivative f'(x) = 1 + e^(-x).\"\"\"\n        return 1.0 + np.exp(-x)\n\n    def hybrid_solver(x0: float, x1: float, tau: float, epsilon: float, n_max: int) -> float:\n        \"\"\"\n        Implements the hybrid secant-Newton root-finding method.\n\n        Args:\n            x0 (float): The first initial guess.\n            x1 (float): The second initial guess.\n            tau (float): The derivative magnitude threshold for switching to Newton's method.\n            epsilon (float): The tolerance for termination.\n            n_max (int): The maximum number of iterations.\n\n        Returns:\n            float: The final approximation of the root.\n        \"\"\"\n        xk_minus_1 = float(x0)\n        xk = float(x1)\n        use_newton = False\n\n        # Pre-check for convergence on the second initial guess\n        if abs(f(xk))  epsilon:\n            return xk\n\n        for _ in range(n_max):\n            # Termination condition 1: absolute change in iterates\n            if abs(xk - xk_minus_1)  epsilon:\n                return xk\n\n            # Select method and compute next iterate\n            if use_newton:\n                # Newton-Raphson update\n                fxk = f(xk)\n                fpxk = f_prime(xk)\n                # The derivative f'(x) for this function is always >= 1, so no division by zero\n                xk_plus_1 = xk - fxk / fpxk\n            else:\n                # Secant method update\n                fxk = f(xk)\n                fxk_minus_1 = f(xk_minus_1)\n                denominator = fxk - fxk_minus_1\n                # Check for stagnation, which would cause division by zero.\n                if denominator == 0:\n                    return xk\n                xk_plus_1 = xk - fxk * (xk - xk_minus_1) / denominator\n            \n            # Update iterates for the next step\n            xk_minus_1 = xk\n            xk = xk_plus_1\n            \n            # Termination condition 2: absolute function value\n            if abs(f(xk))  epsilon:\n                return xk\n                \n            # Switching condition: check if derivative magnitude exceeds threshold\n            if not use_newton:\n                if abs(f_prime(xk)) > tau:\n                    use_newton = True\n                    \n        # Termination condition 3: maximum iterations reached\n        return xk\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 1.4, 1e-12, 50),     # Case 1\n        (10.0, 12.0, 1.6, 1e-12, 100),   # Case 2\n        (-3.0, -2.0, 1.1, 1e-12, 50),   # Case 3\n        (0.0, 2.0, 1.5, 1e-12, 2)       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        x0, x1, tau, epsilon, n_max = case\n        root = hybrid_solver(x0, x1, tau, epsilon, n_max)\n        # Round the result to 10 decimal places as specified\n        results.append(round(root, 10))\n\n    # Final print statement in the exact required format.\n    # Using f-string formatting to avoid trailing zeros for integers.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3167328"}]}