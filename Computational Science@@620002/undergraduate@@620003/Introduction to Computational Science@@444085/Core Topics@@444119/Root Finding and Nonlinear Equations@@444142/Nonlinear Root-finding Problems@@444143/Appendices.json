{"hands_on_practices": [{"introduction": "Finding the roots of a nonlinear equation is a fundamental task in computational science. Before applying a powerful algorithm like Newton's method, it is crucial to first establish whether a root even exists and if it is unique. This practice guides you through this complete workflow, from using the Intermediate Value Theorem to prove existence to analyzing the function's derivative for uniqueness, culminating in the application of Newton's method to find the root of $f(x) = \\cos(x) - x^3$ [@problem_id:3164859].", "problem": "Consider the nonlinear equation $f(x) = \\cos(x) - x^{3}$ on the real line, where all angles are measured in radians. Using only foundational concepts from calculus and numerical analysis, address the following objectives to determine the real root structure and compute the unique real solution:\n1) Use the continuity of $f(x)$, together with the Intermediate Value Theorem (IVT), to establish the existence of at least one real root. Identify a closed interval $[a,b]$ within which a sign change occurs, and justify your choice of $a$ and $b$.\n2) Analyze the sign of the derivative $f^{\\prime}(x)$ and the range of $\\cos(x)$ to determine whether multiple real roots can exist. Provide a logically complete argument for the number of real roots of $f(x)$.\n3) Construct a practical initial guess $x_{0}$ for an iterative root-finding procedure by using qualitative graphical insight (based on the behavior of $\\cos(x)$ and $x^{3}$) and derivative sign checks. Justify your choice of $x_{0}$ using values of $f(x)$ at simple points and the monotonicity of $f(x)$ on a suitable interval.\n4) Derive an iterative root-finding method from the first-order Taylor polynomial of $f(x)$ centered at the current iterate, and apply it to compute the unique real root starting from your chosen $x_{0}$. Continue iterating until your approximation is consistent with reporting the root to $4$ significant figures.\n\nRound your final numerical answer to $4$ significant figures, and express it as a dimensionless real number.", "solution": "The problem requires an analysis of the real roots of the function $f(x) = \\cos(x) - x^{3}$. The solution is structured into four parts as requested.\n\n(1) Existence of a Real Root\n\nTo establish the existence of a real root, we will use the Intermediate Value Theorem (IVT). The function is given by $f(x) = \\cos(x) - x^{3}$. The function $\\cos(x)$ is continuous for all real numbers $x \\in \\mathbb{R}$, and the function $x^{3}$ is a polynomial, which is also continuous for all $x \\in \\mathbb{R}$. The difference of two continuous functions is continuous, so $f(x)$ is continuous on the entire real line $\\mathbb{R}$.\n\nThe IVT states that for a function $f$ continuous on a closed interval $[a, b]$, if $k$ is any number between $f(a)$ and $f(b)$, then there exists at least one number $c \\in [a, b]$ such that $f(c) = k$. To show a root exists, we need to find an interval $[a, b]$ where $f(a)$ and $f(b)$ have opposite signs, which implies that $0$ is between $f(a)$ and $f(b)$.\n\nLet us evaluate $f(x)$ at simple points.\nFor $a=0$:\n$$f(0) = \\cos(0) - 0^{3} = 1 - 0 = 1$$\nSo, $f(0) > 0$.\n\nFor $b=1$:\n$$f(1) = \\cos(1) - 1^{3} = \\cos(1) - 1$$\nSince angles are in radians, and $0  1  \\frac{\\pi}{2}$, we know that $0  \\cos(1)  1$. Therefore, $f(1) = \\cos(1) - 1  0$.\n\nSince $f(x)$ is continuous on the closed interval $[0, 1]$, and $f(0) = 1 > 0$ while $f(1)  0$, the IVT guarantees the existence of at least one root $c$ in the open interval $(0, 1)$ such that $f(c) = 0$.\n\n(2) Number of Real Roots\n\nTo determine the total number of real roots, we first analyze the potential range for any roots and then examine the function's monotonicity. A root of $f(x)$ corresponds to a solution of the equation $\\cos(x) = x^{3}$.\n\nThe range of the cosine function is $[-1, 1]$, so any real root $x$ must satisfy $-1 \\le x^{3} \\le 1$. This implies that $-1 \\le x \\le 1$. Therefore, all real roots of $f(x)$ must lie within the closed interval $[-1, 1]$.\n\nNext, we analyze the derivative of $f(x)$ to determine its monotonicity on this interval. The derivative is:\n$$f^{\\prime}(x) = \\frac{d}{dx}(\\cos(x) - x^{3}) = -\\sin(x) - 3x^{2}$$\nWe analyze the sign of $f^{\\prime}(x)$ on the interval $[-1, 1]$.\n\nCase i: $x \\in (0, 1]$.\nIn this interval, $x > 0$, so $3x^{2} > 0$. Also, for $x \\in (0, 1] \\subset (0, \\pi)$, we have $\\sin(x) > 0$. Therefore, both terms in the derivative are negative:\n$$f^{\\prime}(x) = \\underbrace{-\\sin(x)}_{0} + \\underbrace{(-3x^{2})}_{0}  0 \\quad \\text{for } x \\in (0, 1]$$\nSince $f^{\\prime}(x)  0$ on $(0, 1]$, the function $f(x)$ is strictly decreasing on the interval $[0, 1]$. A strictly monotonic function can have at most one root in an interval. Since we established in part (1) that a root exists in $(0, 1)$, there must be exactly one root in $[0, 1]$.\n\nCase ii: $x \\in [-1, 0)$.\nIn this interval, $x  0$. Let's examine the sign of $f(x)$ itself.\nThe term $\\cos(x)$ is positive for $x \\in [-1, 0) \\subset (-\\frac{\\pi}{2}, \\frac{\\pi}{2})$.\nThe term $-x^{3}$ is positive for $x  0$.\nThus, for $x \\in [-1, 0)$, $f(x)$ is the sum of two positive terms:\n$$f(x) = \\underbrace{\\cos(x)}_{>0} + \\underbrace{(-x^{3})}_{>0} > 0 \\quad \\text{for } x \\in [-1, 0)$$\nSince $f(x)$ is strictly positive on $[-1, 0)$, there are no roots in this interval.\n\nCombining our findings:\n- There are no roots for $|x| > 1$.\n- There are no roots for $x \\in [-1, 0)$.\n- There is exactly one root in $[0, 1]$.\nTherefore, the function $f(x) = \\cos(x) - x^{3}$ has exactly one real root.\n\n(3) Initial Guess for an Iterative Procedure\n\nFrom the analysis above, the unique root lies in the interval $(0, 1)$. A practical initial guess $x_{0}$ should be chosen within or near this interval. We seek a value $x_{0}$ where the iterative method is well-behaved. The derivative is $f'(x) = -\\sin(x) - 3x^2$. At $x=0$, $f'(0)=0$, which would cause division by zero in Newton's method. Thus, $x_0=0$ is a poor choice.\n\nLet's choose $x_{0}=1$ as our initial guess. This choice is justified because:\n- It is a simple endpoint of the interval $[0, 1]$ where the root is known to exist.\n- We have $f(1) = \\cos(1) - 1  0$.\n- The derivative at this point is $f'(1) = -\\sin(1) - 3 \\neq 0$, so the iterative step is well-defined.\n- The second derivative is $f''(x) = -\\cos(x) - 6x$. For $x \\in [0, 1]$, $\\cos(x) > 0$ and $6x \\ge 0$, so $f''(x)  0$. This means $f(x)$ is concave down on $[0,1]$. Since $f(1)  0$ and $f(x)$ is concave down, an initial guess of $x_0=1$ for Newton's method will produce a sequence of iterates that converges monotonically to the root.\n\n(4) Iterative Root-Finding and Solution\n\nThe problem asks to derive an iterative method from the first-order Taylor polynomial of $f(x)$ centered at an iterate $x_{n}$. The first-order Taylor expansion is:\n$$f(x) \\approx f(x_{n}) + f^{\\prime}(x_{n})(x - x_{n})$$\nTo find the root, we set the polynomial to zero, $f(x) = 0$, and solve for $x$, which we designate as the next iterate $x_{n+1}$:\n$$0 = f(x_{n}) + f^{\\prime}(x_{n})(x_{n+1} - x_{n})$$\n$$x_{n+1} - x_{n} = -\\frac{f(x_{n})}{f^{\\prime}(x_{n})}$$\n$$x_{n+1} = x_{n} - \\frac{f(x_{n})}{f^{\\prime}(x_{n})}$$\nThis is the well-known Newton-Raphson method. For our specific function:\n$$f(x) = \\cos(x) - x^{3}$$\n$$f^{\\prime}(x) = -\\sin(x) - 3x^{2}$$\nThe iteration formula is:\n$$x_{n+1} = x_{n} - \\frac{\\cos(x_{n}) - x_{n}^{3}}{-\\sin(x_{n}) - 3x_{n}^{2}} = x_{n} + \\frac{\\cos(x_{n}) - x_{n}^{3}}{\\sin(x_{n}) + 3x_{n}^{2}}$$\nWe start with $x_{0} = 1$ and iterate until the result is stable to $4$ significant figures. All calculations must use radians.\n\nIteration $0$: $x_{0} = 1$.\n$$x_{1} = 1 + \\frac{\\cos(1) - 1^{3}}{\\sin(1) + 3(1)^{2}} \\approx 1 + \\frac{0.540302 - 1}{0.841471 + 3} = 1 + \\frac{-0.459698}{3.841471} \\approx 1 - 0.119668 = 0.880332$$\n\nIteration $1$: $x_{1} \\approx 0.880332$.\n$$x_{2} = 0.880332 + \\frac{\\cos(0.880332) - (0.880332)^{3}}{\\sin(0.880332) + 3(0.880332)^{2}} \\approx 0.880332 + \\frac{0.636904 - 0.682220}{0.770956 + 2.325042} = 0.880332 + \\frac{-0.045316}{3.095998} \\approx 0.880332 - 0.014637 = 0.865695$$\n\nIteration $2$: $x_{2} \\approx 0.865695$.\n$$x_{3} = 0.865695 + \\frac{\\cos(0.865695) - (0.865695)^{3}}{\\sin(0.865695) + 3(0.865695)^{2}} \\approx 0.865695 + \\frac{0.647963 - 0.648834}{0.761066 + 2.248281} = 0.865695 + \\frac{-0.000871}{3.009347} \\approx 0.865695 - 0.000289 = 0.865406$$\nLet's use higher precision for the remaining steps to check for consistency.\n$x_2 \\approx 0.86569529$\n$x_3 \\approx 0.86547407$\n\nIteration $3$: $x_{3} \\approx 0.86547407$.\n$$x_{4} = 0.86547407 + \\frac{\\cos(0.86547407) - (0.86547407)^{3}}{\\sin(0.86547407) + 3(0.86547407)^{2}} \\approx 0.86547407 + \\frac{-2.05 \\times 10^{-7}}{3.008351} \\approx 0.86547407 - 6.81 \\times 10^{-8} = 0.86547400$$\n\nComparing the iterates rounded to $4$ significant figures:\n- $x_{2} \\approx 0.8657$\n- $x_{3} \\approx 0.8655$\n- $x_{4} \\approx 0.8655$\n\nThe value $0.8655$ is consistent between the third and fourth iterations. Therefore, the unique real root, reported to $4$ significant figures, is $0.8655$.", "answer": "$$\\boxed{0.8655}$$", "id": "3164859"}, {"introduction": "The success of any root-finding algorithm depends critically on the accurate evaluation of the function itself. This exercise highlights a common pitfall known as catastrophic cancellation, where subtracting nearly equal numbers can lead to a drastic loss of precision. By algebraically reformulating the function $f(x)=\\sqrt{1+x}-\\sqrt{1-x}-\\alpha$, you will learn a vital technique to ensure your calculations are numerically stable, a prerequisite for reliable root-finding [@problem_id:3164945].", "problem": "Consider the nonlinear function $f(x)=\\sqrt{1+x}-\\sqrt{1-x}-\\alpha$ defined for $x \\in (-1,1)$, where $\\alpha$ is a real parameter. In introductory computational science, nonlinear root-finding problems often require careful management of numerical cancellation when evaluating expressions near $x=0$. Your tasks are:\n\n1. Using only algebraic identities and the standard properties of real-valued square roots, derive an exact algebraic rearrangement of $f(x)$ that avoids catastrophic cancellation when $|x|$ is small. Explain why the rearranged form improves numerical stability by discussing how it changes the magnitudes of terms involved in the evaluation.\n\n2. Treating the equation $f(x)=0$ as a root-finding problem on the interval $(-1,1)$, use the rearranged form to solve for $x$ explicitly in terms of $\\alpha$. Justify the selection of the appropriate branch, including any necessary constraints on $\\alpha$ and monotonicity arguments, and ensure your derivation does not reintroduce cancellation-sensitive steps.\n\nProvide your final answer as a single closed-form analytic expression for the root $x^{\\star}(\\alpha)$, simplified as much as possible. No rounding is required, and no units are involved.", "solution": "### Part 1: Algebraic Rearrangement of $f(x)$\n\nThe function is given by $f(x)=\\sqrt{1+x}-\\sqrt{1-x}-\\alpha$. Catastrophic cancellation occurs when $x$ is very close to $0$. In this case, both $\\sqrt{1+x}$ and $\\sqrt{1-x}$ are very close to $1$, and their difference leads to a significant loss of precision in floating-point arithmetic.\n\nTo derive a numerically stable form, we multiply the difference of square roots by its conjugate expression, $\\sqrt{1+x}+\\sqrt{1-x}$, in both the numerator and denominator. This converts the problematic subtraction into an addition.\n\n$f(x) = \\left(\\sqrt{1+x}-\\sqrt{1-x}\\right) \\frac{\\sqrt{1+x}+\\sqrt{1-x}}{\\sqrt{1+x}+\\sqrt{1-x}} - \\alpha$\n\nUsing the identity $(a-b)(a+b) = a^2-b^2$, the numerator becomes:\n$(\\sqrt{1+x})^2 - (\\sqrt{1-x})^2 = (1+x) - (1-x) = 2x$\n\nSo, the rearranged function is:\n$f(x) = \\frac{2x}{\\sqrt{1+x}+\\sqrt{1-x}} - \\alpha$\n\nThis form is numerically superior for small $|x|$. The original expression involved subtracting two quantities that approach $1$. The rearranged expression's denominator involves the sum of these same two quantities. The sum approaches $1+1=2$ as $x \\to 0$. The addition of two positive numbers is a numerically stable operation, thus avoiding catastrophic cancellation and preserving relative accuracy.\n\n### Part 2: Explicit Solution of the Root-Finding Problem\n\nWe need to solve the equation $f(x)=0$ for the root $x$, which we denote as $x^{\\star}$. We can use both the original and rearranged forms of the equation.\n\nThe original form gives:\n$1) \\quad \\sqrt{1+x^{\\star}} - \\sqrt{1-x^{\\star}} = \\alpha$\n\nThe rearranged form (assuming $\\alpha \\neq 0$) gives:\n$\\frac{2x^{\\star}}{\\sqrt{1+x^{\\star}}+\\sqrt{1-x^{\\star}}} = \\alpha \\quad \\implies \\quad 2) \\quad \\sqrt{1+x^{\\star}}+\\sqrt{1-x^{\\star}} = \\frac{2x^{\\star}}{\\alpha}$\n\nWe now have a system of two linear equations in terms of the variables $\\sqrt{1+x^{\\star}}$ and $\\sqrt{1-x^{\\star}}$. This is a robust way to proceed as it uses the rearranged form to build the derivation.\n\nFirst, let's analyze the function $g(x) = \\sqrt{1+x}-\\sqrt{1-x}$ to determine the valid range for $\\alpha$. The derivative is $g'(x) = \\frac{1}{2\\sqrt{1+x}} + \\frac{1}{2\\sqrt{1-x}}$. For $x \\in (-1,1)$, both terms are positive, so $g'(x) > 0$. This means $g(x)$ is strictly increasing on its domain. A strictly monotonic function has a unique inverse, so for any $\\alpha$ in the range of $g(x)$, there will be a unique solution $x^{\\star}$.\n\nThe range is found by evaluating the limits at the boundaries of the domain:\n$\\lim_{x\\to -1^+} g(x) = \\sqrt{0} - \\sqrt{2} = -\\sqrt{2}$\n$\\lim_{x\\to 1^-} g(x) = \\sqrt{2} - \\sqrt{0} = \\sqrt{2}$\nSince $g(x)$ is continuous and strictly increasing, its range is $(-\\sqrt{2}, \\sqrt{2})$. Therefore, a unique solution $x^{\\star} \\in (-1,1)$ exists if and only if $\\alpha \\in (-\\sqrt{2}, \\sqrt{2})$.\n\nFurthermore, note that $g(0) = 0$. Since $g(x)$ is increasing, if $\\alpha > 0$, the solution $x^{\\star}$ must be greater than $0$. If $\\alpha  0$, the solution $x^{\\star}$ must be less than $0$. Thus, $\\text{sgn}(x^{\\star}) = \\text{sgn}(\\alpha)$.\n\nNow we solve the system of equations. Add equation $(1)$ and $(2)$:\n$( \\sqrt{1+x^{\\star}} - \\sqrt{1-x^{\\star}} ) + ( \\sqrt{1+x^{\\star}} + \\sqrt{1-x^{\\star}} ) = \\alpha + \\frac{2x^{\\star}}{\\alpha}$\n$2\\sqrt{1+x^{\\star}} = \\alpha + \\frac{2x^{\\star}}{\\alpha}$\n\nSquaring both sides:\n$4(1+x^{\\star}) = \\left(\\alpha + \\frac{2x^{\\star}}{\\alpha}\\right)^2 = \\alpha^2 + 2(\\alpha)\\left(\\frac{2x^{\\star}}{\\alpha}\\right) + \\left(\\frac{2x^{\\star}}{\\alpha}\\right)^2$\n$4+4x^{\\star} = \\alpha^2 + 4x^{\\star} + \\frac{4(x^{\\star})^2}{\\alpha^2}$\n\nThe $4x^{\\star}$ terms cancel on both sides:\n$4 = \\alpha^2 + \\frac{4(x^{\\star})^2}{\\alpha^2}$\n$4 - \\alpha^2 = \\frac{4(x^{\\star})^2}{\\alpha^2}$\n\nSolving for $(x^{\\star})^2$:\n$(x^{\\star})^2 = \\frac{\\alpha^2 (4-\\alpha^2)}{4}$\n\nNow, we take the square root:\n$x^{\\star} = \\pm \\sqrt{\\frac{\\alpha^2 (4-\\alpha^2)}{4}} = \\pm \\frac{|\\alpha|\\sqrt{4-\\alpha^2}}{2}$\n\nWe must select the correct sign. As established from our monotonicity argument, $\\text{sgn}(x^{\\star}) = \\text{sgn}(\\alpha)$.\n- If $\\alpha > 0$, then $|\\alpha|=\\alpha$. We need $x^{\\star}>0$, so we must choose the positive sign: $x^{\\star} = +\\frac{\\alpha\\sqrt{4-\\alpha^2}}{2}$.\n- If $\\alpha  0$, then $|\\alpha|=-\\alpha$. We need $x^{\\star}0$, so we must choose the negative sign: $x^{\\star} = -\\frac{(-\\alpha)\\sqrt{4-\\alpha^2}}{2} = \\frac{\\alpha\\sqrt{4-\\alpha^2}}{2}$.\n- If $\\alpha = 0$, the formula gives $x^{\\star}=0$, which is correct as $g(0)=0$.\n\nIn all cases, the solution is given by the same expression. The final, simplified, closed-form analytic expression for the root $x^{\\star}$ in terms of $\\alpha$ is:\n$x^{\\star}(\\alpha) = \\frac{\\alpha\\sqrt{4-\\alpha^2}}{2}$\n\nThis derivation satisfies the problem's constraints. It uses the rearranged form (via equation 2) and proceeds with valid algebraic steps to find the exact analytical solution. The expression is valid for $\\alpha \\in (-\\sqrt{2}, \\sqrt{2})$, which ensures $4-\\alpha^2 > 0$ and that the root $x^{\\star}$ lies strictly within $(-1,1)$.", "answer": "$$\\boxed{\\frac{\\alpha}{2}\\sqrt{4-\\alpha^2}}$$", "id": "3164945"}, {"introduction": "In many scientific and engineering applications, function evaluations are not exact but are corrupted by measurement noise. This practice moves beyond idealized functions and challenges you to find a root when the data is stochastic, using the bisection method as a foundation. You will implement and compare a naive approach with a statistically robust 'median-of-means' strategy, gaining insight into how to design algorithms that are resilient to real-world uncertainty [@problem_id:3164924].", "problem": "You will study robustness of root-finding under stochastic perturbations in function evaluations. Consider a deterministic target function $g(x)$ that is continuous and strictly increasing on an interval $[a,b]$, and suppose observations follow the noisy measurement model $f(x)=g(x)+\\epsilon$, where $\\epsilon$ denotes an independent and identically distributed random variable with zero mean and finite variance. A root is a point $x^{\\star}$ such that $g(x^{\\star})=0$. The bisection method relies on the Intermediate Value Theorem and sign tests to iteratively contract $[a,b]$ toward a root when $g(a)g(b)0$. In the presence of noise, sign tests become unreliable, motivating repeated sampling and robust aggregation.\n\nUsing only the Law of Large Numbers and core definitions, implement two bisection-based estimators that use sign tests derived from noisy evaluations:\n- Strategy A (single-sample sign): At each function evaluation point $x$, draw one sample of $\\epsilon$ and use the sign of the single noisy observation $f(x)=g(x)+\\epsilon$ to decide the bisection update.\n- Strategy B (median-of-means sign): At each function evaluation point $x$, form $K$ groups, each with $M$ independent samples of $\\epsilon$. Within each group, compute the average of the $M$ noisy observations of $f(x)$. Take the median across the $K$ group averages. Use the sign of this median-of-means estimator to decide the bisection update.\n\nWork with the specific target function $g(x)=\\tanh(x)-\\tfrac{1}{2}$, which is continuous and strictly increasing on $[-2,2]$ and has a unique root $x^{\\star}=\\operatorname{arctanh}\\!\\left(\\tfrac{1}{2}\\right)=\\tfrac{1}{2}\\ln\\!\\left(\\tfrac{1+\\tfrac{1}{2}}{1-\\tfrac{1}{2}}\\right)=\\tfrac{1}{2}\\ln(3)$. Throughout, use the initial bracket $[a,b]=[-2,2]$. Let the noise be Gaussian with $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})$. All random draws must be produced by a single pseudorandom number generator initialized once at the beginning with the fixed seed $20231105$, and never reseeded thereafter, to ensure deterministic reproducibility.\n\nImplement a bisection routine that performs exactly $L$ iterations. At each iteration, compute the midpoint $m=\\tfrac{a+b}{2}$. Estimate the sign of $f(m)$ using either Strategy A or Strategy B, and update the bracket according to the usual bisection logic: if the estimated signs at the left endpoint and the midpoint indicate opposite signs (their product is nonpositive), set $b\\leftarrow m$, otherwise set $a\\leftarrow m$ and update the stored sign of the left endpoint to the midpoint’s sign. After $L$ iterations, output the midpoint $\\hat{x}$ as the estimated root.\n\nUse the following test suite. For each case, run both strategies and report absolute errors $e=\\lvert\\hat{x}-x^{\\star}\\rvert$:\n- Case $1$ (baseline, noise-free): $\\sigma=0$, $L=30$, for Strategy B use $K=9$, $M=5$.\n- Case $2$ (happy path, small noise): $\\sigma=0.05$, $L=30$, for Strategy B use $K=9$, $M=5$.\n- Case $3$ (moderate noise): $\\sigma=0.2$, $L=35$, for Strategy B use $K=11$, $M=9$.\n- Case $4$ (larger noise, more robust aggregation): $\\sigma=0.4$, $L=40$, for Strategy B use $K=21$, $M=9$.\n\nYour program must:\n- Implement $g(x)=\\tanh(x)-\\tfrac{1}{2}$, noisy evaluations $f(x)=g(x)+\\epsilon$ with $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})$, and the two sign-estimation strategies described above.\n- Use the initial bracket $[-2,2]$ and perform exactly $L$ bisection iterations per run.\n- Use a single pseudorandom number generator initialized once with seed $20231105$ at the start of the program.\n- For each of the four cases, compute the absolute errors for Strategy A and Strategy B, denoted $e_{\\text{A}}$ and $e_{\\text{B}}$.\n- Produce a single line of output containing the $8$ numbers in the following order, each rounded to $6$ decimal places: $[e_{\\text{A},1},e_{\\text{B},1},e_{\\text{A},2},e_{\\text{B},2},e_{\\text{A},3},e_{\\text{B},3},e_{\\text{A},4},e_{\\text{B},4}]$.\n\nNo physical units are involved. Angles are not used. All outputs are real numbers. The final output format must be exactly one line that is a comma-separated list enclosed in square brackets, with each float rounded to $6$ decimal places as specified. Your solution must be a complete, runnable program in a modern programming language.", "solution": "### Problem Formulation\nWe are tasked with finding the root $x^{\\star}$ of a deterministic target function $g(x)$, defined as a point where $g(x^{\\star}) = 0$. However, we cannot evaluate $g(x)$ directly. Instead, we observe noisy measurements $f(x) = g(x) + \\epsilon$, where $\\epsilon$ is a random variable drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$ with mean $0$ and standard deviation $\\sigma$. The root-finding algorithm must operate on these noisy observations.\n\nThe specific target function is $g(x) = \\tanh(x) - \\frac{1}{2}$. This function is continuous and strictly increasing. The unique root $x^{\\star}$ is given by $\\tanh(x^{\\star}) = \\frac{1}{2}$, which solves to $x^{\\star} = \\operatorname{arctanh}(\\frac{1}{2}) = \\frac{1}{2}\\ln(3)$. The search for the root is conducted within the initial interval $[a_0, b_0] = [-2, 2]$. We can verify that $g(-2)  0$ and $g(2) > 0$, so by the Intermediate Value Theorem, a root is guaranteed to exist within this bracket.\n\n### The Bisection Method with Noisy Sign Tests\nThe standard bisection method relies on the sign of the function at the endpoints of an interval $[a, b]$ and its midpoint $m = \\frac{a+b}{2}$. If $g(a)$ and $g(m)$ have opposite signs, the root must lie in $[a, m]$, so we set $b \\leftarrow m$. Otherwise, the root must be in $[m, b]$, and we set $a \\leftarrow m$. This process halves the interval length at each iteration, guaranteeing convergence.\n\nIn a noisy setting, the sign of a single observation $f(x) = g(x) + \\epsilon$ may not equal the sign of the true function value $g(x)$, especially when $|g(x)|$ is small compared to the noise magnitude $\\sigma$. An incorrect sign test can lead the bisection algorithm to discard the half of the interval that contains the true root, compromising convergence.\n\nThe prescribed algorithm performs exactly $L$ iterations. At each step, a reference sign, initially corresponding to the left endpoint $a$, is maintained. Let this be $s_a$. The midpoint is $m = \\frac{a+b}{2}$, and its sign $s_m$ is estimated from noisy data. The update rule is:\n- If $s_a \\cdot s_m \\le 0$, the signs are opposite (or one is zero), so the new interval becomes $[a, m]$ by setting $b \\leftarrow m$.\n- Otherwise, the new interval becomes $[m, b]$ by setting $a \\leftarrow m$. In this case, the reference sign for the new left endpoint $m$ is updated to be $s_m$, so we set $s_a \\leftarrow s_m$.\n\nThe core of the problem lies in the design of two different strategies for estimating the sign of the function at a given point $x$.\n\n### Sign Estimation Strategies\n\n#### Strategy A: Single-Sample Sign\nThis is the most straightforward approach. To estimate the sign of $g(x)$, we take a single noisy measurement $f(x) = g(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, and use its sign.\n$$\n\\text{sign}_{\\text{A}}(x) = \\text{sign}(g(x) + \\epsilon)\n$$\nThis method is computationally cheap but highly susceptible to noise. A single large fluctuation $\\epsilon$ can easily flip the sign of the observation if $|g(x)|$ is not large.\n\n#### Strategy B: Median-of-Means Sign\nThis strategy employs statistical aggregation to obtain a more robust estimate of the sign. The procedure is as follows:\n1.  At a given point $x$, we draw $K \\times M$ independent noise samples, $\\epsilon_{ij}$ for $i=1, \\dots, K$ and $j=1, \\dots, M$.\n2.  These are used to form $K$ groups of $M$ noisy observations each: $f_{ij}(x) = g(x) + \\epsilon_{ij}$.\n3.  For each group $i$, we compute the sample mean of the observations:\n    $$\n    \\bar{f}_i(x) = \\frac{1}{M} \\sum_{j=1}^{M} f_{ij}(x) = g(x) + \\frac{1}{M} \\sum_{j=1}^{M} \\epsilon_{ij}\n    $$\n    By the Law of Large Numbers, as $M \\to \\infty$, this sample mean $\\bar{f}_i(x)$ converges to the expected value of the observation, which is $g(x)$ since $E[\\epsilon] = 0$. The variance of this mean is reduced by a factor of $M$: $\\text{Var}(\\bar{f}_i(x)) = \\sigma^2 / M$.\n4.  We then compute the median of these $K$ group means:\n    $$\n    \\hat{g}(x) = \\text{median}\\{\\bar{f}_1(x), \\dots, \\bar{f}_K(x)\\}\n    $$\n    The median provides robustness. Even if some group means are far from $g(x)$ due to random chance, the median is likely to be close to $g(x)$ as long as a majority of the group means are.\n5.  The final sign estimate is the sign of this median-of-means estimator:\n    $$\n    \\text{sign}_{\\text{B}}(x) = \\text{sign}(\\hat{g}(x))\n    $$\nThis strategy is more computationally intensive (requiring $K \\times M$ evaluations per sign test) but is expected to provide much greater reliability against noise.\n\n### Implementation and Execution\nThe solution is implemented in Python using the `numpy` library. A single pseudorandom number generator is initialized with the seed $20231105$ to ensure that the sequence of random numbers is deterministic and the results are reproducible.\n\nFor each of the four test cases specified:\n- A bisection solver function is called for each strategy. This function implements the iterative logic described above for a fixed number of iterations $L$.\n- The sign estimation is performed by dedicated functions for Strategy A and Strategy B, which are passed to the solver.\n- After $L$ iterations, the midpoint of the final interval is taken as the estimated root, $\\hat{x}$.\n- The absolute error $e = |\\hat{x} - x^{\\star}|$ is calculated, where $x^{\\star} = \\frac{1}{2}\\ln(3)$.\n\nThe parameters $K, M,$ and $L$ are increased for cases with higher noise $\\sigma$. this is a logical design, as more samples are needed for robust estimation (larger $K, M$) and more iterations are needed to refine the solution (larger $L$) when noise is more likely to cause incorrect steps. We expect Strategy B to yield significantly smaller errors than Strategy A, especially for larger values of $\\sigma$. For the noise-free case ($\\sigma=0$), both strategies reduce to the standard bisection method and should produce identical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g(x: float) - float:\n    \"\"\"The deterministic target function g(x) = tanh(x) - 1/2.\"\"\"\n    return np.tanh(x) - 0.5\n\ndef estimate_sign_A(x: float, sigma: float, rng: np.random.Generator, K: int = None, M: int = None) - int:\n    \"\"\"\n    Estimates the sign of g(x) using a single noisy sample (Strategy A).\n    K and M are unused but included for a consistent function signature.\n    \"\"\"\n    if sigma == 0.0:\n        f_x = g(x)\n    else:\n        noise = rng.normal(loc=0.0, scale=sigma)\n        f_x = g(x) + noise\n    return np.sign(f_x)\n\ndef estimate_sign_B(x: float, sigma: float, rng: np.random.Generator, K: int, M: int) - int:\n    \"\"\"\n    Estimates the sign of g(x) using the median-of-means estimator (Strategy B).\n    \"\"\"\n    if sigma == 0.0:\n        return np.sign(g(x))\n    \n    g_x = g(x)\n    \n    # Generate KxM noise samples efficiently in a single array\n    noises = rng.normal(loc=0.0, scale=sigma, size=(K, M))\n    \n    # Compute K group means\n    group_means = g_x + np.mean(noises, axis=1)\n    \n    # Compute the median of the K group means\n    median_of_means = np.median(group_means)\n    \n    return np.sign(median_of_means)\n\ndef bisection_solver(\n    L: int, \n    sigma: float, \n    sign_strategy, \n    rng: np.random.Generator, \n    K: int, \n    M: int\n) - float:\n    \"\"\"\n    Performs bisection for L iterations using a given sign estimation strategy.\n    \n    Args:\n        L: Number of iterations.\n        sigma: Standard deviation of the noise.\n        sign_strategy: The function to use for sign estimation (estimate_sign_A or estimate_sign_B).\n        rng: The pseudorandom number generator.\n        K: Number of groups for Strategy B.\n        M: Number of samples per group for Strategy B.\n\n    Returns:\n        The estimated root after L iterations.\n    \"\"\"\n    a, b = -2.0, 2.0\n    \n    # Estimate the sign at the initial left endpoint\n    sign_a = sign_strategy(a, sigma, rng, K=K, M=M)\n    \n    for _ in range(L):\n        m = (a + b) / 2.0\n        \n        # If midpoint is the root, stop early (unlikely with float arithmetic)\n        if m == a or m == b:\n            break\n            \n        sign_m = sign_strategy(m, sigma, rng, K=K, M=M)\n        \n        # Bisection update rule\n        if sign_a * sign_m = 0:\n            b = m\n        else:\n            a = m\n            sign_a = sign_m\n            \n    return (a + b) / 2.0\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Initialize a single random number generator for reproducibility\n    rng = np.random.default_rng(20231105)\n    \n    # Define the true root\n    x_star = 0.5 * np.log(3.0)\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'sigma': 0.0, 'L': 30, 'K': 9, 'M': 5},\n        {'sigma': 0.05, 'L': 30, 'K': 9, 'M': 5},\n        {'sigma': 0.2, 'L': 35, 'K': 11, 'M': 9},\n        {'sigma': 0.4, 'L': 40, 'K': 21, 'M': 9},\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma, L, K, M = case['sigma'], case['L'], case['K'], case['M']\n        \n        # Run Strategy A\n        x_hat_A = bisection_solver(L, sigma, estimate_sign_A, rng, K, M)\n        e_A = abs(x_hat_A - x_star)\n        \n        # Run Strategy B\n        x_hat_B = bisection_solver(L, sigma, estimate_sign_B, rng, K, M)\n        e_B = abs(x_hat_B - x_star)\n        \n        results.extend([e_A, e_B])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3164924"}]}