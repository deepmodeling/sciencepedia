{"hands_on_practices": [{"introduction": "Theory comes to life when applied to a concrete problem. This first practice explores the classic fixed-point equation $x = \\cos(x)$, a perfect illustration of the Contraction Mapping Theorem. By implementing the simple iteration $x_{k+1} = \\cos(x_k)$, you will not only find the numerical solution but also verify the conditions that guarantee its existence and uniqueness. This exercise will build a solid foundation by connecting the abstract theorem to the tangible behavior of a converging sequence [@problem_id:3231288].", "problem": "Consider the fixed-point equation $x=\\cos x$ with the fixed-point iteration $x_{k+1}=\\cos x_k$ defined on the closed interval $[0,1]$. Use the following fundamental bases: the definition of a fixed point $x^\\star$ satisfying $x^\\star=g(x^\\star)$ for a function $g$, the Mean Value Theorem (MVT), and the Contraction Mapping Theorem (CMT), also known as the Banach Fixed-Point Theorem. The iteration uses the angle unit in radians. Your program must implement the iteration and verify convergence and monotonicity properties for a specified test suite.\n\nTasks:\n- Starting from the fundamental definitions, justify why the function $g(x)=\\cos x$ maps $[0,1]$ into itself. Use the derivative $g'(x)=-\\sin x$ and the bound $\\sup_{x\\in[0,1]}|\\sin x|=\\sin(1)$ with $\\sin(1)1$ to argue that $g$ is a contraction on $[0,1]$ and therefore admits a unique fixed point $x^\\star\\in[0,1]$ to which the sequence $\\{x_k\\}$ converges for any $x_0\\in[0,1]$.\n- Analyze the monotone behavior of the iterates. Because $g$ is strictly decreasing on $[0,1]$, the full sequence $\\{x_k\\}$ is generally not monotone. However, demonstrate that the even subsequence $\\{x_{2k}\\}$ and the odd subsequence $\\{x_{2k+1}\\}$ are each monotone and converge to the same limit $x^\\star$.\n- Implement a program that, for each initial value $x_0$ in the provided test suite, performs fixed-point iteration $x_{k+1}=\\cos x_k$ until either the successive-iterate difference satisfies $|x_{k+1}-x_k|10^{-12}$ or a maximum of $1000$ iterations is reached. Store the full sequence of iterates for each test case. The angle unit must be radians.\n- For each test case, compute and report:\n  1. The initial value $x_0$.\n  2. The final approximation of the fixed point after stopping, denoted by $x_{\\text{approx}}$.\n  3. The number of iterations $N$ performed.\n  4. A boolean indicating whether the full sequence $\\{x_k\\}_{k=0}^N$ is monotone (nondecreasing or nonincreasing).\n  5. A boolean indicating whether the even-indexed subsequence $\\{x_{2k}\\}$ is monotone (nondecreasing or nonincreasing).\n  6. A boolean indicating whether the odd-indexed subsequence $\\{x_{2k+1}\\}$ is monotone (nondecreasing or nonincreasing).\n- Use the following test suite of initial values (all in radians): $x_0\\in\\{0,\\,\\tfrac{1}{2},\\,1,\\,0.7390851332151607\\}$. The last value is a high-precision approximation to the unique fixed point $x^\\star$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the exact form $[x_0,x_{\\text{approx}},N,\\text{is\\_monotone\\_full},\\text{is\\_monotone\\_even},\\text{is\\_monotone\\_odd}]$. For example, the overall output will look like $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ with four inner lists corresponding to the four test cases.\n\nAll numerical values mentioned above must be treated as scalars in your program. The program must not read any inputs and must print the single required line as its only output.", "solution": "We first recall the fundamental definitions and facts. A fixed point of a function $g$ is a value $x^\\star$ satisfying $x^\\star=g(x^\\star)$. A numerical fixed-point iteration constructs a sequence $\\{x_k\\}$ by $x_{k+1}=g(x_k)$ and seeks a limit $x^\\star$ such that $x_k\\to x^\\star$ and $x^\\star=g(x^\\star)$. The Contraction Mapping Theorem (CMT) states that if $g$ maps a complete metric space into itself and satisfies a Lipschitz bound $|g(x)-g(y)|\\le L|x-y|$ for all $x,y$ in the domain with a constant $L1$, then $g$ has a unique fixed point $x^\\star$ and for any initialization $x_0$ in the domain the iteration $x_{k+1}=g(x_k)$ converges to $x^\\star$.\n\nWe analyze $g(x)=\\cos x$ on the closed interval $[0,1]$. First, we show that $g$ maps $[0,1]$ into itself. For $x\\in[0,1]$, we have $\\cos x\\in[\\cos 1,\\,\\cos 0]=[\\cos 1,\\,1]$. Since $\\cos 10$, it follows that $g([0,1])\\subset[0,1]$. Next, we compute the derivative $g'(x)=-\\sin x$. On $[0,1]$, the function $\\sin x$ is nonnegative and bounded above by $\\sin 1$, so\n$$\n\\sup_{x\\in[0,1]}|g'(x)|=\\sup_{x\\in[0,1]}|\\sin x|=\\sin(1).\n$$\nBecause $\\sin(1)1$, we can use the Mean Value Theorem (MVT) to derive a Lipschitz bound: for any $x,y\\in[0,1]$,\n$$\n|g(x)-g(y)|=|\\cos x-\\cos y|=|g'(\\xi)|\\,|x-y|\\le \\sin(1)\\,|x-y|,\n$$\nfor some $\\xi$ between $x$ and $y$. Hence $g$ is a contraction on $[0,1]$ with contraction constant $L=\\sin(1)1$. By the Contraction Mapping Theorem, there exists a unique fixed point $x^\\star\\in[0,1]$, and for any $x_0\\in[0,1]$, the sequence defined by $x_{k+1}=\\cos x_k$ converges to $x^\\star$.\n\nWe next analyze monotone behavior. Note that $g$ is strictly decreasing on $[0,1]$ because $g'(x)=-\\sin x\\le 0$ and for $x\\in(0,1]$ we have $\\sin x0$. Let $x^\\star$ be the unique fixed point. Consider the error $e_k=x_k-x^\\star$. Using the MVT, there exists $\\xi_k$ between $x_k$ and $x^\\star$ such that\n$$\ne_{k+1}=x_{k+1}-x^\\star=g(x_k)-g(x^\\star)=g'(\\xi_k)\\,(x_k-x^\\star)=-\\sin(\\xi_k)\\,e_k.\n$$\nOn $[0,1]$, $\\sin(\\xi_k)\\in[0,\\sin(1)]$ and the sign of $e_{k+1}$ is the negative of the sign of $e_k$ whenever $\\sin(\\xi_k)0$. Therefore, unless $e_k=0$, the error alternates in sign, so the full sequence $\\{x_k\\}$ generally oscillates and is not monotone. However, we can infer monotone behavior of subsequences. Because $g$ is decreasing, we have for any $x\\in[0,1]$, $xx^\\star$ implies $g(x)g(x^\\star)=x^\\star$, and $xx^\\star$ implies $g(x)x^\\star$. Starting with any $x_0\\in[0,1]$, if $x_0x^\\star$ then $x_1 x^\\star$, and then $x_2=g(x_1)g(x^\\star)=x^\\star$, thus $x_2x^\\star$. The even subsequence $\\{x_{2k}\\}$ remains below $x^\\star$ and, using the contraction property,\n$$\n|x_{2k}-x^\\star|\\le L^2 |x_{2k-2}-x^\\star|,\n$$\nso $\\{x_{2k}\\}$ is monotone increasing towards $x^\\star$ (its terms get closer to $x^\\star$ from below). Symmetrically, the odd subsequence $\\{x_{2k+1}\\}$ remains above $x^\\star$ and is monotone decreasing towards $x^\\star$. If $x_0x^\\star$, the roles of the subsequences reverse, but the same alternating monotone convergence holds. In the special case $x_0=x^\\star$, the sequence is constant and trivially monotone. The contraction bound also yields a linear convergence rate:\n$$\n|x_{k}-x^\\star|\\le L^k |x_0-x^\\star|,\n$$\nwith $L=\\sin(1)$.\n\nAlgorithm design for the program:\n- For each test case initial value $x_0$, compute iterates $x_{k+1}=\\cos x_k$ (with angles in radians) until $|x_{k+1}-x_k|10^{-12}$ or a maximum of $1000$ iterations is reached. Store all iterates to check monotonicity.\n- Determine if the full sequence is monotone by checking whether all consecutive differences are nonnegative (nondecreasing) or all are nonpositive (nonincreasing), allowing equality. Determine monotonicity for the even subsequence $\\{x_{2k}\\}$ and the odd subsequence $\\{x_{2k+1}\\}$ similarly.\n- Report for each test case the list $[x_0,x_{\\text{approx}},N,\\text{is\\_monotone\\_full},\\text{is\\_monotone\\_even},\\text{is\\_monotone\\_odd}]$, where $x_{\\text{approx}}$ is the last iterate and $N$ the number of iteration steps performed.\n- Use the test suite $x_0\\in\\{0,\\,\\tfrac{1}{2},\\,1,\\,0.7390851332151607\\}$.\n\nThe contraction constant $L=\\sin(1)$ validates both the uniqueness of the fixed point and the convergence claim. The numerical results will reflect the theoretical monotone behavior: the full sequence generally oscillates but the even and odd subsequences are monotone and both converge to the same limit $x^\\star$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fixed_point_cos(x0, tol=1e-12, max_iter=1000):\n    \"\"\"\n    Perform fixed-point iteration x_{k+1} = cos(x_k) starting from x0.\n    Angles are in radians.\n    Returns:\n        x_approx: final approximation\n        iterations: number of iterations performed\n        seq: list of iterates including the initial value\n    \"\"\"\n    seq = [float(x0)]\n    x_prev = float(x0)\n    iterations = 0\n    for k in range(max_iter):\n        x_next = float(np.cos(x_prev))\n        seq.append(x_next)\n        iterations += 1\n        if abs(x_next - x_prev)  tol:\n            break\n        x_prev = x_next\n    return seq[-1], iterations, seq\n\ndef is_monotone(sequence, tol=0.0):\n    \"\"\"\n    Check if a sequence is monotone nondecreasing or monotone nonincreasing.\n    Equality is allowed.\n    tol can be used to soften comparisons, but defaults to strict.\n    \"\"\"\n    if len(sequence) = 1:\n        return True\n    diffs = [sequence[i+1] - sequence[i] for i in range(len(sequence)-1)]\n    nondecreasing = all(d = -tol for d in diffs)\n    nonincreasing = all(d = tol for d in diffs)\n    return nondecreasing or nonincreasing\n\ndef solve():\n    # Define the test cases from the problem statement (radians).\n    test_cases = [\n        0.0,\n        0.5,\n        1.0,\n        0.7390851332151607,  # high-precision approximation to the fixed point\n    ]\n\n    results = []\n    for x0 in test_cases:\n        x_approx, iters, seq = fixed_point_cos(x0, tol=1e-12, max_iter=1000)\n        # Full sequence monotonicity\n        mono_full = is_monotone(seq, tol=0.0)\n        # Even-indexed subsequence: indices 0,2,4,...\n        even_seq = seq[0::2]\n        mono_even = is_monotone(even_seq, tol=0.0)\n        # Odd-indexed subsequence: indices 1,3,5,...\n        odd_seq = seq[1::2]\n        mono_odd = is_monotone(odd_seq, tol=0.0)\n        # Assemble result for this test case\n        results.append([x0, x_approx, iters, mono_full, mono_even, mono_odd])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3231288"}, {"introduction": "An iterative method is only as reliable as its stopping criterion. While an iteration may converge in theory, a computer needs a clear signal to stop. In this practice, we investigate two mathematically equivalent criteria: the size of the residual, $|x_k - g(x_k)|$, and the size of the increment, $|x_{k+1} - x_k|$. You will discover through carefully designed test cases how the limitations of finite-precision arithmetic can cause these criteria to behave differently, sometimes leading to misleading results. This hands-on analysis [@problem_id:3130615] is crucial for writing robust and reliable numerical code.", "problem": "You will examine two common stopping criteria for fixed-point iterations and how finite-precision arithmetic can make them disagree. Consider the fixed-point iteration in one dimension defined by the recurrence $x_{k+1} = g(x_k)$ for $k = 0, 1, 2, \\dots$ and a given initial value $x_0$. In exact arithmetic, the residual-based quantity $r_k = \\lvert x_k - g(x_k) \\rvert$ and the increment-based quantity $s_k = \\lvert x_{k+1} - x_k \\rvert$ are equal by the defining relation $x_{k+1} = g(x_k)$. In floating-point arithmetic, however, these two computed quantities can disagree, sometimes severely, due to cancellation and rounding. Your task is to write a program that, for several test cases, computes both stopping criteria at each iteration and reports the first iteration index where each criterion falls below a prescribed tolerance. You must use the standard absolute value as the norm, and any angles must be in radians.\n\nFundamental base and definitions to be used:\n- A fixed point of a function $g$ is $x^{\\star}$ such that $x^{\\star} = g(x^{\\star})$.\n- A fixed-point iteration is defined by $x_{k+1} = g(x_k)$.\n- For one-dimensional problems, use the absolute value $\\lvert \\cdot \\rvert$ as the norm.\n- All angles must be interpreted in radians.\n\nStopping criteria to evaluate at each iteration index $k$:\n- Residual-based: compute $r_k = \\lvert x_k - g(x_k) \\rvert$ using the specified form of $g$ for each test case.\n- Increment-based: compute $s_k = \\lvert x_{k+1} - x_k \\rvert$ using the computed next iterate $x_{k+1}$.\n- Record $k_{\\mathrm{res}}$ as the smallest $k$ with $r_k  \\tau$ (the tolerance), or $-1$ if no such $k$ occurs before the maximum number of iterations $K$.\n- Record $k_{\\mathrm{inc}}$ as the smallest $k$ with $s_k  \\tau$, or $-1$ if no such $k$ occurs before the maximum number of iterations $K$.\n\nDesign your program to run the following test suite. In each case, implement $g$ exactly as specified, including any deliberate numerically stable or unstable formulations, and adhere to the given tolerance $\\tau$, maximum iteration count $K$, and initial value $x_0$.\n\nTest suite:\n1. Well-behaved mapping (happy path):\n   - Function: $g(x) = \\cos(x)$.\n   - Initial value: $x_0 = 1$.\n   - Tolerance: $\\tau = 10^{-10}$.\n   - Maximum iterations: $K = 100$.\n   - Note: Use the standard cosine function with angles in radians.\n\n2. Cancellation causes residual to mislead:\n   - Function: $g(x) = x - \\log(1 + x)$, where for the iteration update you must compute $x_{k+1} = x_k - \\mathrm{log1p}(x_k)$ using a numerically stable evaluation of $\\log(1+x)$, but for the residual you must compute $r_k = \\lvert \\log(1 + x_k) \\rvert$ using the naive composition $\\log(1 + x)$ (that is, compute $1 + x_k$ in floating point and then take the natural logarithm). This deliberately induces cancellation when $x_k$ is very small.\n   - Initial value: $x_0 = 10^{-8}$.\n   - Tolerance: $\\tau = 10^{-17}$.\n   - Maximum iterations: $K = 5$.\n   - Angles are not involved in this case.\n\n3. Rounding causes increment to mislead:\n   - Function: $g(x) = x - \\arctan(x)$.\n   - Initial value: $x_0 = 10^{16}$.\n   - Tolerance: $\\tau = 10^{-12}$.\n   - Maximum iterations: $K = 2$.\n   - Note: Use the standard arctangent function with angles in radians.\n\nFor each test case, your program should produce two integers: $k_{\\mathrm{res}}$ and $k_{\\mathrm{inc}}$. Aggregate the results for the three test cases into a single list in the following order:\n- Test case $1$: $k_{\\mathrm{res}}$, $k_{\\mathrm{inc}}$.\n- Test case $2$: $k_{\\mathrm{res}}$, $k_{\\mathrm{inc}}$.\n- Test case $3$: $k_{\\mathrm{res}}$, $k_{\\mathrm{inc}}$.\n\nFinal output format:\nYour program should produce a single line of output containing the six integers as a comma-separated list enclosed in square brackets, in the exact order specified above. For example, the format must be\n$[k_{\\mathrm{res},1},k_{\\mathrm{inc},1},k_{\\mathrm{res},2},k_{\\mathrm{inc},2},k_{\\mathrm{res},3},k_{\\mathrm{inc},3}]$.\n\nNo user input is allowed; all parameters must be hard-coded as specified above. All angles, where applicable, are in radians. All outputs must be integers.", "solution": "The problem requires an analysis of two distinct stopping criteria for a one-dimensional fixed-point iteration, $x_{k+1} = g(x_k)$, in the context of finite-precision floating-point arithmetic. The two criteria are based on the residual, $r_k = \\lvert x_k - g(x_k) \\rvert$, and the increment, $s_k = \\lvert x_{k+1} - x_k \\rvert$. In exact arithmetic, these quantities are identical since $x_{k+1}$ is defined as $g(x_k)$. However, due to rounding and cancellation errors in floating-point computations, their computed values can differ significantly, potentially leading to different conclusions about convergence.\n\nThe task is to implement a program that, for three specific test cases, performs the fixed-point iteration and tracks the first iteration index $k$ at which each criterion falls below a given tolerance $\\tau$. Let $k_{\\mathrm{res}}$ be the smallest index $k$ for which $r_k  \\tau$, and $k_{\\mathrm{inc}}$ be the smallest index $k$ for which $s_k  \\tau$. If the condition is not met within the maximum number of iterations $K$, the corresponding index is reported as $-1$. The iterations are indexed from $k=0, 1, 2, \\dots, K-1$.\n\nThe overall structure of the algorithm for each test case is as follows:\n1. Initialize the current iterate $x$ with the initial value $x_0$.\n2. Initialize $k_{\\mathrm{res}} = -1$ and $k_{\\mathrm{inc}} = -1$.\n3. Loop for $k$ from $0$ to $K-1$:\n    a. Compute $g(x_k)$ as specified for the test case. Note that for Case 2, the formula used for the residual differs from the one for the iteration update.\n    b. Calculate the residual $r_k = \\lvert x_k - g(x_k) \\rvert$ using the prescribed formula.\n    c. If $r_k  \\tau$ and $k_{\\mathrm{res}}$ has not yet been set (i.e., $k_{\\mathrm{res}} = -1$), set $k_{\\mathrm{res}} = k$.\n    d. Compute the next iterate $x_{k+1}$ using the prescribed update rule.\n    e. Calculate the increment $s_k = \\lvert x_{k+1} - x_k \\rvert$.\n    f. If $s_k  \\tau$ and $k_{\\mathrm{inc}}$ has not yet been set (i.e., $k_{\\mathrm{inc}} = -1$), set $k_{\\mathrm{inc}} = k$.\n    g. Update the iterate for the next step: $x_k \\leftarrow x_{k+1}$.\n    h. If both $k_{\\mathrm{res}}$ and $k_{\\mathrm{inc}}$ have been found, the loop can be terminated early.\n4. After the loop, the determined values of $k_{\\mathrm{res}}$ and $k_{\\mathrm{inc}}$ are reported.\n\nLet us analyze each test case.\n\n**Test Case 1: Well-behaved mapping**\n- Function: $g(x) = \\cos(x)$\n- Initial value: $x_0 = 1$\n- Tolerance: $\\tau = 10^{-10}$\n- Maximum iterations: $K = 100$\n\nFor this case, the iteration is $x_{k+1} = \\cos(x_k)$. The function $g(x) = \\cos(x)$ is a contraction mapping on the interval containing its fixed point $x^\\star \\approx 0.739085$, since $\\lvert g'(x) \\rvert = \\lvert -\\sin(x) \\rvert  1$ in the vicinity of $x^\\star$. The computation of $g(x_k)$ is numerically stable. The residual is $r_k = \\lvert x_k - \\cos(x_k) \\rvert$, and the next iterate is $x_{k+1} = \\cos(x_k)$. The increment is $s_k = \\lvert x_{k+1} - x_k \\rvert = \\lvert \\cos(x_k) - x_k \\rvert$. In floating-point arithmetic, if we compute `gxk = cos(xk)`, then `rk = abs(xk - gxk)` and `sk = abs(gxk - xk)` will yield identical floating-point values. Therefore, we expect $k_{\\mathrm{res}}$ and $k_{\\mathrm{inc}}$ to be equal. The iteration proceeds until the magnitude of the change drops below $\\tau$.\n\n**Test Case 2: Cancellation causes residual to mislead**\n- Function update: $x_{k+1} = x_k - \\mathrm{log1p}(x_k)$\n- Residual evaluation: $r_k = \\lvert \\log(1 + x_k) \\rvert$ (naively computed)\n- Initial value: $x_0 = 10^{-8}$\n- Tolerance: $\\tau = 10^{-17}$\n- Maximum iterations: $K = 5$\n\nThis case is designed to show how cancellation error can affect the residual calculation. The fixed point of $g(x) = x - \\log(1+x)$ is $x^\\star = 0$.\nThe update rule $x_{k+1} = x_k - \\mathrm{log1p}(x_k)$ uses the numerically stable function `log1p(x)` which computes $\\log(1+x)$ accurately for small $x$. The increment is calculated as $s_k = \\lvert x_{k+1} - x_k \\rvert = \\lvert (x_k - \\mathrm{log1p}(x_k)) - x_k \\rvert = \\lvert-\\mathrm{log1p}(x_k)\\rvert$, which is also accurate.\nThe residual, however, is calculated via $r_k = \\lvert \\log(1 + x_k) \\rvert$. When $x_k$ becomes very small, its value may be smaller than the machine epsilon relative to $1$. In standard double-precision floating-point arithmetic (IEEE 754), machine epsilon is approximately $2.22 \\times 10^{-16}$. If $\\lvert x_k \\rvert$ is sufficiently small (e.g., around $10^{-17}$), the floating-point sum $1 + x_k$ is rounded to exactly $1.0$. Consequently, $\\log(1.0)$ evaluates to $0$, and $r_k$ becomes $0$ prematurely.\nTracing the iteration:\n- $k=0$: $x_0 = 10^{-8}$. Iteration gives $x_1 = x_0 - \\mathrm{log1p}(x_0) \\approx x_0 - (x_0 - x_0^2/2) \\approx x_0^2/2 \\approx 0.5 \\times 10^{-16}$.\n- $k=1$: $x_1 \\approx 0.5 \\times 10^{-16}$. This value is smaller than machine epsilon. The computation of the residual $r_1$ involves $1+x_1$, which evaluates to $1.0$. Thus $r_1 = \\lvert \\log(1.0) \\rvert = 0$. This is less than $\\tau = 10^{-17}$, so $k_{\\mathrm{res}}=1$. However, the increment is $s_1 = \\lvert -\\mathrm{log1p}(x_1) \\rvert \\approx x_1 \\approx 0.5 \\times 10^{-16}$, which is greater than $\\tau$.\n- $k=2$: $x_2 = x_1 - \\mathrm{log1p}(x_1) \\approx x_1^2/2 \\approx 0.125 \\times 10^{-32}$. The increment $s_2 = \\lvert -\\mathrm{log1p}(x_2) \\rvert \\approx x_2 \\approx 1.25 \\times 10^{-33}$. This value is less than $\\tau$, so $k_{\\mathrm{inc}} = 2$.\nWe predict $k_{\\mathrm{res}}=1$ and $k_{\\mathrm{inc}}=2$.\n\n**Test Case 3: Rounding causes increment to mislead**\n- Function: $g(x) = x - \\arctan(x)$\n- Initial value: $x_0 = 10^{16}$\n- Tolerance: $\\tau = 10^{-12}$\n- Maximum iterations: $K = 2$\n\nThis case demonstrates how catastrophic cancellation in the update rule can mislead the increment-based stopping criterion. The fixed point of $g(x) = x - \\arctan(x)$ is $x^\\star = 0$.\nFor a large initial value such as $x_0 = 10^{16}$, the update is $x_1 = x_0 - \\arctan(x_0)$. For large $x$, $\\arctan(x) \\approx \\pi/2 \\approx 1.57$. The operation is $10^{16} - (\\pi/2)$. In double-precision arithmetic, the gap between consecutive representable numbers around $10^{16}$ is approximately $10^{16} \\times \\epsilon_{\\text{machine}} \\approx 10^{16} \\times 2.22 \\times 10^{-16} = 2.22$. Since the value to be subtracted, $\\pi/2$, is smaller than this gap, the result of the subtraction $10^{16} - \\pi/2$ is rounded to the nearest representable number, which is $10^{16}$ itself.\nThus, the computed next iterate is $x_1 = x_0$.\nLet's analyze the stopping criteria for $k=0$:\n- The increment is $s_0 = \\lvert x_1 - x_0 \\rvert = \\lvert 10^{16} - 10^{16} \\rvert = 0$. This is less than $\\tau=10^{-12}$, so $k_{\\mathrm{inc}}=0$.\n- The residual is $r_0 = \\lvert x_0 - g(x_0) \\rvert = \\lvert x_0 - (x_0 - \\arctan(x_0)) \\rvert = \\lvert \\arctan(x_0) \\rvert = \\arctan(10^{16}) \\approx \\pi/2$. This is much larger than $\\tau$, correctly indicating that the iterate is far from the fixed point.\nThe iteration stagnates at $x_k = 10^{16}$, so the residual criterion is never met.\nWe predict $k_{\\mathrm{inc}}=0$ and $k_{\\mathrm{res}}=-1$.\n\nThe program will implement these three test cases and collect the six resulting integers ($k_{\\mathrm{res},1}, k_{\\mathrm{inc},1}, k_{\\mathrm{res},2}, k_{\\mathrm{inc},2}, k_{\\mathrm{res},3}, k_{\\mathrm{inc},3}$) into a list for final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs three test cases for fixed-point iteration, comparing residual-based\n    and increment-based stopping criteria in finite-precision arithmetic.\n    \"\"\"\n    \n    # Define test cases: (g_update, g_residual, x0, tau, K)\n    # g_residual is a function to calculate g(x) for the residual, which might\n    # differ from the update rule due to numerical stability considerations.\n    test_cases = [\n        {\n            \"name\": \"Well-behaved\",\n            \"g_update\": lambda x: np.cos(x),\n            \"g_residual\": lambda x: np.cos(x),\n            \"x0\": 1.0,\n            \"tau\": 1e-10,\n            \"K\": 100,\n        },\n        {\n            \"name\": \"Cancellation in residual\",\n            # Update uses stable log1p\n            \"g_update\": lambda x: x - np.log1p(x),\n            # Residual uses naive log(1+x) to induce cancellation\n            \"g_residual\": lambda x: x - np.log(1 + x),\n            \"x0\": 1e-8,\n            \"tau\": 1e-17,\n            \"K\": 5,\n        },\n        {\n            \"name\": \"Rounding in increment\",\n            \"g_update\": lambda x: x - np.arctan(x),\n            \"g_residual\": lambda x: x - np.arctan(x),\n            \"x0\": 1e16,\n            \"tau\": 1e-12,\n            \"K\": 2,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x_k = case[\"x0\"]\n        tau = case[\"tau\"]\n        K = case[\"K\"]\n        g_update = case[\"g_update\"]\n        g_residual = case[\"g_residual\"]\n\n        k_res = -1\n        k_inc = -1\n\n        for k in range(K):\n            # 1. Compute residual-based criterion\n            # The residual is r_k = |x_k - g(x_k)|. The problem defines g(x)\n            # differently for update vs residual in case 2.\n            # In general, g(x) = x - h(x) - r_k = |h(x_k)|\n            # Case 1: h(x) = x - cos(x) = residual seems to be |x_k - cos(x_k)|\n            # Case 2: h(x) = log(1+x) = residual is |log(1+x_k)|\n            # Case 3: h(x) = arctan(x) = residual is |arctan(x_k)|\n            # The problem text can be interpreted as r_k = |x_k - g_residual(x_k)|. Let's follow this.\n            \n            # For Case 2, the residual is r_k = |log(1+x_k)|.\n            # The function is g(x) = x - log(1+x).\n            # The residual is |x_k - g(x_k)| = |x_k - (x_k - log(1+x_k))| = |log(1+x_k)|\n            if case[\"name\"] == \"Cancellation in residual\":\n                r_k = np.abs(np.log(1 + x_k))\n            else:\n                gx_for_res = g_residual(x_k)\n                r_k = np.abs(x_k - gx_for_res)\n\n            # Check residual stopping criterion\n            if r_k  tau and k_res == -1:\n                k_res = k\n\n            # 2. Compute next iterate and increment-based criterion\n            x_k_plus_1 = g_update(x_k)\n            s_k = np.abs(x_k_plus_1 - x_k)\n\n            # Check increment stopping criterion\n            if s_k  tau and k_inc == -1:\n                k_inc = k\n            \n            # 3. Update for next iteration\n            x_k = x_k_plus_1\n\n            # Optimization: stop if both criteria are met\n            if k_res != -1 and k_inc != -1:\n                break\n        \n        results.extend([k_res, k_inc])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3130615"}, {"introduction": "Guaranteed convergence is a great start, but the speed of convergence is often just as important in practice. Some iterations converge so slowly that they are impractical. This exercise tackles the issue of performance by first constructing a fixed-point problem with a contraction constant close to $1$, leading to very slow linear convergence. You will then implement a powerful technique known as Aitken's delta-squared process to accelerate the iteration, providing a dramatic demonstration of how a simple transformation can turn a crawl into a sprint [@problem_id:3130626].", "problem": "Consider one-dimensional fixed-point iteration on the real line defined by a mapping $g:\\mathbb{R}\\to\\mathbb{R}$ that is a contraction with Lipschitz constant $L\\in[0,1)$. Use the fundamental definition of a contraction mapping: for all $x,y\\in\\mathbb{R}$, $|g(x)-g(y)|\\le L|x-y|$, and the definition of fixed-point iteration $x_{k+1}=g(x_k)$. Your goal is to construct and analyze a family of mappings that exhibit slow linear convergence when the contraction constant $L$ is close to $1$, and then to apply a classical acceleration technique to quantify the improvement.\n\nWork with the affine family\n$g(x)=L\\,x+(1-L)\\,c,$\nwhere $c\\in\\mathbb{R}$ is a known constant. This family is a contraction whenever $L\\in[0,1)$, and its fixed point is the value $x^\\star$ satisfying $x^\\star=g(x^\\star)$.\n\nStarting from the core definitions only, perform the following tasks.\n\n- Task A (plain fixed-point iteration): For a given $L\\in[0,1)$, fixed point target $c\\in\\mathbb{R}$, initial guess $x_0\\in\\mathbb{R}$, and tolerance $\\varepsilon0$, determine the minimal number of evaluations of $g$ required by the unaccelerated fixed-point iteration $x_{k+1}=g(x_k)$ to produce an iterate $x_k$ such that $|x_k-c|\\le\\varepsilon$. Count one evaluation of $g$ per iteration. Use only quantities that are directly defined by the problem data.\n\n- Task B (acceleration): Apply a standard sequence acceleration to the fixed-point iteration, specifically Aitken’s delta-squared process (also known as Steffensen’s acceleration when applied to fixed-point iteration). Use the classical three-point transform to turn a triple $(x_k,x_{k+1},x_{k+2})$ produced by successive applications of $g$ into an accelerated estimate $\\hat{x}_k$. Count the number of evaluations of $g$ needed to obtain an accelerated estimate that satisfies $|\\hat{x}_k-c|\\le\\varepsilon$, where each accelerated estimate requires two evaluations of $g$ beyond the current $x_k$ to form $(x_{k+1},x_{k+2})$. If numerical degeneracy prevents applying the transform at some step, continue generating the next triple as needed.\n\n- Task C (quantify improvement): For each case, compute the speedup as the ratio of the number of evaluations required by plain fixed-point iteration to the number required by the accelerated method. Express this speedup as a decimal.\n\nDesign your program to solve the following test suite. In all cases, use the same tolerance $\\varepsilon=10^{-8}$, initial guess $x_0=c+1$, and fixed-point target $c=3$.\n\n- Case $1$: $L=0.9$.\n- Case $2$: $L=0.99$.\n- Case $3$: $L=0.0$ (boundary case).\n- Case $4$: $L=0.9999$ (near-boundary, very slow plain convergence).\n\nYour program must output a single line consisting of a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of the form $[N_{\\text{plain}},N_{\\text{acc}},S]$:\n- $N_{\\text{plain}}$ is the minimal number of evaluations of $g$ needed by plain fixed-point iteration to achieve $|x_k-c|\\le\\varepsilon$.\n- $N_{\\text{acc}}$ is the number of evaluations of $g$ needed by the Aitken-accelerated scheme to achieve $|\\hat{x}_k-c|\\le\\varepsilon$.\n- $S$ is the speedup $N_{\\text{plain}}/N_{\\text{acc}}$ expressed as a decimal.\n\nAs an example of the exact output format, the program should print a single line like $[[1,2,0.5],[\\dots],[\\dots],[\\dots]]$ with no spaces.\n\nNo physical units or angles are involved in this problem. All requested outputs are integers or decimals as specified.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the mathematical theory of fixed-point iteration and sequence acceleration, is well-posed with all necessary data provided, and is formulated using objective, unambiguous language. The problem is a standard exercise in introductory computational science and meets all criteria for a valid problem.\n\nWe proceed with the solution, which is divided into an analysis of the plain fixed-point iteration (Task A), an analysis of the accelerated iteration (Task B), and the calculation of the speedup (Task C).\n\nThe fixed-point iteration is defined by the mapping $g(x) = L x + (1-L)c$, where $L \\in [0,1)$, and the iterative process is $x_{k+1} = g(x_k)$. The fixed point $x^\\star$ of this iteration is the solution to $x^\\star = g(x^\\star)$.\n$$x^\\star = L x^\\star + (1-L)c$$\n$$x^\\star (1-L) = (1-L)c$$\nSince $L \\in [0,1)$, the term $(1-L)$ is non-zero, allowing us to divide by it.\n$$x^\\star = c$$\nThis confirms that the constant $c$ is the unique fixed point of the mapping $g(x)$.\n\n### Task A: Plain Fixed-Point Iteration\n\nWe analyze the convergence of the sequence $x_k$ to the fixed point $x^\\star = c$. Let the error at iteration $k$ be $e_k = x_k - c$.\n$$x_{k+1} - c = g(x_k) - c$$\n$$x_{k+1} - c = (L x_k + (1-L)c) - c$$\n$$x_{k+1} - c = L x_k - Lc$$\n$$x_{k+1} - c = L(x_k - c)$$\nThis gives the error recurrence relation $e_{k+1} = L e_k$. By induction, the error at iteration $k$ is related to the initial error $e_0 = x_0 - c$:\n$$e_k = L^k e_0$$\nThe condition for termination is $|x_k - c| \\le \\varepsilon$, which in terms of the error is $|e_k| \\le \\varepsilon$.\n$$|L^k e_0| \\le \\varepsilon$$\n$$|L|^k |x_0 - c| \\le \\varepsilon$$\nGiven that $L \\in [0,1)$, we have $|L| = L$. The problem specifies $x_0 = c+1$, so $|x_0 - c| = |(c+1) - c| = 1$. The condition simplifies to:\n$$L^k \\le \\varepsilon$$\nWe must find the minimal integer $k \\ge 1$ that satisfies this inequality. The number of evaluations of $g$ to obtain $x_k$ is exactly $k$. We denote this by $N_{\\text{plain}}$. The initial state $x_0$ is given, so no evaluations are needed for $k=0$. Since $|x_0-c|=1$ and $\\varepsilon=10^{-8}$, the condition is not met at $k=0$.\n\nWe must consider two sub-cases for $L$.\n\nCase $L=0$:\nThe mapping is $g(x) = (1-0)c = c$. The first iterate is $x_1 = g(x_0) = c$.\nThe error is $|x_1 - c| = |c-c|=0$. Since $0 \\le \\varepsilon$, the condition is met after one iteration.\nThus, for $L=0$, $N_{\\text{plain}} = 1$.\n\nCase $L \\in (0,1)$:\nWe solve the inequality $L^k \\le \\varepsilon$ by taking the natural logarithm of both sides.\n$$k \\ln(L) \\le \\ln(\\varepsilon)$$\nSince $L \\in (0,1)$, its logarithm $\\ln(L)$ is negative. Dividing by a negative number reverses the inequality sign:\n$$k \\ge \\frac{\\ln(\\varepsilon)}{\\ln(L)}$$\nThe minimal integer $k$ is the ceiling of this expression.\n$$N_{\\text{plain}} = \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln(L)} \\right\\rceil$$\n\n### Task B: Accelerated Iteration\n\nAitken's delta-squared process generates an accelerated sequence $\\hat{x}_k$ from a given sequence $x_k$. The formula for the accelerated estimate is:\n$$\\hat{x}_k = x_k - \\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k}$$\nTo compute the first accelerated term $\\hat{x}_0$, we need the triple $(x_0, x_1, x_2)$. This requires two evaluations of $g$: $x_1 = g(x_0)$ and $x_2 = g(x_1)$. To compute $\\hat{x}_k$ requires producing the sequence up to $x_{k+2}$, which takes $k+2$ evaluations of $g$. We seek the minimal number of total evaluations, $N_{\\text{acc}}$, to find an estimate $\\hat{x}_k$ such that $|\\hat{x}_k-c| \\le \\varepsilon$.\n\nLet us analyze the performance of Aitken's method on the affine sequence $x_k = c + L^k(x_0-c)$.\nThe differences are:\n$$x_{k+1} - x_k = (c + L^{k+1}(x_0-c)) - (c + L^k(x_0-c)) = (L^{k+1} - L^k)(x_0-c) = L^k(L-1)(x_0-c)$$\nThe denominator is the second-order difference:\n$$x_{k+2} - 2x_{k+1} + x_k = (x_{k+2}-x_{k+1}) - (x_{k+1}-x_k)$$\n$$= L^{k+1}(L-1)(x_0-c) - L^k(L-1)(x_0-c) = (L^{k+1}-L^k)(L-1)(x_0-c)$$\n$$= L^k(L-1)(L-1)(x_0-c) = L^k(L-1)^2(x_0-c)$$\nThe denominator is non-zero as long as $L \\neq 0$, $L \\neq 1$, and $x_0 \\neq c$. The problem constraints $L \\in [0,1)$ and $x_0 = c+1$ ensure this for $L \\in (0,1)$.\n\nThe correction term in Aitken's formula becomes:\n$$\\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k} = \\frac{\\left[L^k(L-1)(x_0-c)\\right]^2}{L^k(L-1)^2(x_0-c)} = \\frac{L^{2k}(L-1)^2(x_0-c)^2}{L^k(L-1)^2(x_0-c)} = L^k(x_0-c)$$\nSubstituting this back into the formula for $\\hat{x}_k$:\n$$\\hat{x}_k = x_k - L^k(x_0-c) = (c + L^k(x_0-c)) - L^k(x_0-c) = c$$\nThis remarkable result shows that for an affine iteration, Aitken's method produces the exact fixed point $c$ in a single step, provided the denominator is non-zero.\n\nFor all $L \\in (0,1)$, the denominator for $\\hat{x}_0$ is $(L-1)^2(x_0-c) \\neq 0$. Thus, the first accelerated estimate $\\hat{x}_0$ is equal to $c$. The error $|\\hat{x}_0 - c| = 0$, which is less than or equal to any positive $\\varepsilon$. To compute $\\hat{x}_0$, we need $x_1$ and $x_2$, which requires 2 evaluations of $g$. Therefore, for all $L \\in (0,1)$, $N_{\\text{acc}} = 2$.\n\nFor the boundary case $L=0$, the sequence is $x_0=c+1, x_1=c, x_2=c, \\dots$.\nLet's calculate $\\hat{x}_0$:\n$$x_1 - x_0 = c - (c+1) = -1$$\n$$x_2 - 2x_1 + x_0 = c - 2c + (c+1) = 1$$\nThe denominator is non-zero.\n$$\\hat{x}_0 = x_0 - \\frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} = (c+1) - \\frac{(-1)^2}{1} = c+1 - 1 = c$$\nAgain, the exact fixed point is found. This requires computing $x_1=g(x_0)$ and $x_2=g(x_1)$, for a total of $2$ evaluations. So, for $L=0$, $N_{\\text{acc}} = 2$.\n\nIn conclusion, for all test cases, $N_{\\text{acc}} = 2$.\n\n### Task C: Quantify Improvement\n\nThe speedup $S$ is the ratio of the number of evaluations required by the plain method to that required by the accelerated method:\n$$S = \\frac{N_{\\text{plain}}}{N_{\\text{acc}}}$$\nUsing our derived results, this becomes:\n$$S = \\frac{N_{\\text{plain}}}{2}$$\n\n### Calculations for Test Cases\n- Shared parameters: $c=3$, $x_0 = c+1 = 4$, $\\varepsilon=10^{-8}$.\n- $\\ln(\\varepsilon) = \\ln(10^{-8}) = -8 \\ln(10) \\approx -18.42068$.\n\nCase 1: $L=0.9$\n$N_{\\text{plain}} = \\lceil \\frac{\\ln(10^{-8})}{\\ln(0.9)} \\rceil = \\lceil \\frac{-18.42068}{-0.10536} \\rceil = \\lceil 174.83 \\rceil = 175$.\n$N_{\\text{acc}} = 2$.\n$S = 175 / 2 = 87.5$.\n\nCase 2: $L=0.99$\n$N_{\\text{plain}} = \\lceil \\frac{\\ln(10^{-8})}{\\ln(0.99)} \\rceil = \\lceil \\frac{-18.42068}{-0.01005} \\rceil = \\lceil 1832.85 \\rceil = 1833$.\n$N_{\\text{acc}} = 2$.\n$S = 1833 / 2 = 916.5$.\n\nCase 3: $L=0.0$\n$N_{\\text{plain}} = 1$.\n$N_{\\text{acc}} = 2$.\n$S = 1 / 2 = 0.5$.\n\nCase 4: $L=0.9999$\n$N_{\\text{plain}} = \\lceil \\frac{\\ln(10^{-8})}{\\ln(0.9999)} \\rceil = \\lceil \\frac{-18.42068}{-0.000100005} \\rceil = \\lceil 184201.3 \\rceil = 184202$.\n$N_{\\text{acc}} = 2$.\n$S = 184202 / 2 = 92101.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the fixed-point iteration problem for a given test suite.\n\n    The problem analyzes an affine fixed-point iteration g(x) = L*x + (1-L)*c\n    with fixed point c. It compares the number of function evaluations\n    required for convergence by a plain iteration versus an Aitken-accelerated\n    iteration.\n    \"\"\"\n\n    # Define the shared parameters from the problem statement.\n    c = 3.0\n    x0 = c + 1.0\n    eps = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.9,      # Case 1\n        0.99,     # Case 2\n        0.0,      # Case 3\n        0.9999,   # Case 4\n    ]\n\n    def get_n_plain(L: float, x0: float, c: float, eps: float) - int:\n        \"\"\"\n        Calculates the minimal number of evaluations for plain fixed-point iteration.\n\n        The error e_k = x_k - c follows e_k = L^k * e_0.\n        The condition is |e_k| = eps, which means L^k * |x0 - c| = eps.\n        Given |x0 - c| = 1, this simplifies to L^k = eps.\n        \"\"\"\n        # Check if already converged (not the case for the given problem data)\n        if abs(x0 - c) = eps:\n            return 0\n        \n        # Handle the boundary case L=0 separately.\n        # x_1 = g(x_0) = (1-0)*c = c. Convergence in 1 step.\n        if L == 0.0:\n            return 1\n        \n        # For L in (0, 1), solve L^k = eps for k.\n        # k * log(L) = log(eps) - k = log(eps) / log(L)\n        # We need the smallest integer k.\n        num_iterations = np.ceil(np.log(eps) / np.log(L))\n        return int(num_iterations)\n\n    def get_n_acc(L: float, x0: float, c: float, eps: float) - int:\n        \"\"\"\n        Calculates the minimal number of evaluations for Aitken-accelerated iteration.\n        \n        For an affine iteration g(x) = L*x + (1-L)*c, Aitken's method is known\n        to converge to the exact fixed point with the first accelerated estimate,\n        x_hat_0. Calculation of x_hat_0 requires the triple (x0, x1, x2).\n        \n        x1 = g(x0)  (1st evaluation)\n        x2 = g(x1)  (2nd evaluation)\n        \n        This holds for all L in [0, 1) as long as the denominator of Aitken's\n        formula is non-zero, which is true for the given test cases.\n        \"\"\"\n        # Check trivial convergence\n        if abs(x0 - c) = eps:\n            return 0\n            \n        # As derived in the solution, Aitken's method requires 2 evaluations\n        # to compute x_hat_0, which gives the exact solution c.\n        return 2\n\n    results = []\n    for L_val in test_cases:\n        N_plain = get_n_plain(L_val, x0, c, eps)\n        N_acc = get_n_acc(L_val, x0, c, eps)\n        \n        # Speedup is the ratio of evaluations.\n        S = N_plain / N_acc\n        \n        results.append([N_plain, N_acc, S])\n\n    # Format the output string exactly as specified in the problem,\n    # constructing it part by part to avoid extra spaces from str(list).\n    inner_parts = []\n    for res in results:\n        # For case L=0.9999, S is an integer value, but problem asks for decimal.\n        # Standard float formatting handles this.\n        inner_parts.append(f'[{res[0]},{res[1]},{res[2]}]')\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3130626"}]}