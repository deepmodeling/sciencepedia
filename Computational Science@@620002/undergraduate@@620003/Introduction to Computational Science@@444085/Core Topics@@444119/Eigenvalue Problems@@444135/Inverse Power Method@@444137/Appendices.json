{"hands_on_practices": [{"introduction": "The inverse power method is an iterative algorithm, and at its heart lies a single, repeated calculation. This first practice exercise isolates that fundamental step, asking you to perform one iteration of the shifted inverse power method. By focusing on solving the linear system $(A-\\sigma I)y_1 = x_0$, you will gain direct experience with the core operation that drives the entire algorithm. [@problem_id:1395843]", "problem": "In a numerical algorithm, a sequence of vectors is generated starting from an initial vector $x_0$. The first unnormalized vector in this sequence, denoted as $y_1$, is found by solving the linear system $(A-\\sigma I)y_1 = x_0$, where $A$ is a square matrix, $\\sigma$ is a scalar shift, and $I$ is the identity matrix of the same dimension as $A$.\n\nGiven the matrix $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$, the shift $\\sigma = 1.5$, and the initial vector $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, determine the components of the vector $y_1$. Present your answer as a row matrix where each component is an exact fraction or decimal.", "solution": "We are asked to solve the linear system $(A-\\sigma I) y_{1} = x_{0}$ for $y_{1}$, where $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$, $\\sigma = \\frac{3}{2}$, and $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFirst compute the shifted matrix:\n$$\nA - \\sigma I = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -1 \\\\ -1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nDenote $M = A - \\sigma I$. Then $y_{1} = M^{-1} x_{0}$. For a $2 \\times 2$ matrix $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, we use $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Here $a = d = \\frac{3}{2}$ and $b = c = -1$, so\n$$\n\\det(M) = \\left(\\frac{3}{2}\\right)\\left(\\frac{3}{2}\\right) - (-1)(-1) = \\frac{9}{4} - 1 = \\frac{5}{4},\n$$\nand\n$$\n\\operatorname{adj}(M) = \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nTherefore,\n$$\nM^{-1} = \\frac{1}{\\frac{5}{4}} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nMultiplying by $x_{0}$,\n$$\ny_{1} = M^{-1} x_{0} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ \\frac{4}{5} \\end{pmatrix}.\n$$\nThus the components of $y_{1}$ are $\\frac{6}{5}$ and $\\frac{4}{5}$, which we present as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{5} & \\frac{4}{5} \\end{pmatrix}}$$", "id": "1395843"}, {"introduction": "Now that you're familiar with the mechanics of a single step, let's explore the long-term behavior of the algorithm. This problem presents a thought experiment about the simplest version of the method: the unshifted inverse power method (where the shift $\\sigma=0$). This practice is designed to build your intuition for why the method works, revealing that it naturally converges to the eigenvector corresponding to the eigenvalue with the smallest magnitude. [@problem_id:1395849]", "problem": "A computational physicist is modeling the stability of a quantum system. The system's Hamiltonian is represented by a large, complex matrix $H$. While the full matrix is difficult to work with, a theoretical analysis has revealed that its four distinct energy eigenvalues are $\\{-6.5, -1.2, 0.9, 4.8\\}$, measured in some arbitrary energy units.\n\nTo find the ground state eigenvector of the system, which corresponds to the eigenvalue with the lowest energy, the physicist considers using an iterative numerical method. However, due to a misunderstanding, they implement the standard (unshifted) inverse power method on the matrix $H$. They use a randomly generated initial vector that has a non-zero component in the direction of each of the eigenvectors.\n\nAssuming the numerical method converges without issues of precision or floating-point errors, the eigenvector it finds will correspond to which of the given eigenvalues?\n\nA. $-6.5$\n\nB. $-1.2$\n\nC. $0.9$\n\nD. $4.8$", "solution": "Let $H$ be diagonalizable with distinct eigenvalues $\\{\\lambda_{i}\\}$ and corresponding eigenvectors $\\{v_{i}\\}$. Let the initial vector be $x_{0}=\\sum_{i} c_{i} v_{i}$ with $c_{i} \\neq 0$ for all $i$.\n\nThe standard power method applied to a matrix $A$ converges (under the usual nondefectiveness and separation assumptions) to the eigenvector associated with the eigenvalue of $A$ having largest magnitude. The inverse power method on $H$ is precisely the power method applied to $H^{-1}$.\n\nIf $\\lambda_{i}$ are the eigenvalues of $H$, then the eigenvalues of $H^{-1}$ are\n$$\n\\mu_{i}=\\frac{1}{\\lambda_{i}},\n$$\nwith the same eigenvectors $v_{i}$. After $k$ steps of inverse iteration (ignoring normalization for clarity), one has\n$$\n(H^{-1})^{k} x_{0}=(H^{-1})^{k} \\sum_{i} c_{i} v_{i}=\\sum_{i} c_{i} \\left(\\frac{1}{\\lambda_{i}}\\right)^{k} v_{i}.\n$$\nAs $k \\to \\infty$, the term with the largest $|\\mu_{i}|=|1/\\lambda_{i}|$ dominates, i.e., the term with the smallest $|\\lambda_{i}|$. Therefore, the unshifted inverse power method converges to the eigenvector corresponding to the eigenvalue of $H$ with minimal absolute value.\n\nGiven the eigenvalues $\\{-6.5, -1.2, 0.9, 4.8\\}$, their absolute values are $\\{6.5, 1.2, 0.9, 4.8\\}$. The smallest absolute value is $0.9$, hence the method converges to the eigenvector associated with $\\lambda=0.9$.\n\nTherefore, among the options provided, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1395849"}, {"introduction": "The true power of the inverse power method comes from using a non-zero shift, $\\sigma$, to target specific eigenvalues. However, the choice of the initial vector, $x_0$, also plays a critical role. This exercise explores the subtle but important interplay between the shift and the initial vector, demonstrating how an initial vector that lacks a component in a certain direction can cause the method to converge to an unexpected eigenvector. [@problem_id:1395866]", "problem": "Consider the real-valued matrix $A$ given by:\n$$A = \\begin{pmatrix} 2.5 & -1.5 & 0 \\\\ -1.5 & 2.5 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}$$\nThis matrix has eigenvalues $\\lambda_1 = 1$, $\\lambda_2 = 4$, and $\\lambda_3 = 5$. The corresponding un-normalized eigenvectors are $v_1 = [1, 1, 0]^T$, $v_2 = [1, -1, 0]^T$, and $v_3 = [0, 0, 1]^T$, respectively.\n\nThe inverse power method is used to find an eigenvalue-eigenvector pair of $A$. The method starts with an initial vector $x_0$ and uses a shift $\\sigma$. The iterative step is defined by solving $(A - \\sigma I) y_{k+1} = x_k$ for $y_{k+1}$, and then normalizing to get the next vector $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|$. As $k \\to \\infty$, the vector $x_k$ converges to an eigenvector of $A$.\n\nSuppose the method is executed with a shift of $\\sigma = 0.9$ and an initial vector of $x_0 = [1, -1, 1]^T$. The sequence of vectors $\\{x_k\\}$ will converge to an eigenvector of $A$. Determine the eigenvalue corresponding to this eigenvector.", "solution": "The inverse power method with shift $\\sigma$ applies the iteration\n$$\n(A-\\sigma I) y_{k+1} = x_{k}, \\quad x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|}.\n$$\nLet the eigenpairs of $A$ be $(\\lambda_{i}, v_{i})$ for $i \\in \\{1,2,3\\}$, with $\\lambda_{1}=1$, $\\lambda_{2}=4$, $\\lambda_{3}=5$ and $v_{1} = [1, 1, 0]^{T}$, $v_{2} = [1, -1, 0]^{T}$, $v_{3} = [0, 0, 1]^{T}$.\n\nDecompose the initial vector in the eigenbasis:\n$$\nx_{0} = \\alpha_{1} v_{1} + \\alpha_{2} v_{2} + \\alpha_{3} v_{3}.\n$$\nMatching components, for the first two entries we solve\n$$\n\\alpha_{1} + \\alpha_{2} = 1, \\quad \\alpha_{1} - \\alpha_{2} = -1,\n$$\nwhich yields $\\alpha_{1} = 0$, $\\alpha_{2} = 1$. For the third entry we have $\\alpha_{3} = 1$. Thus\n$$\nx_{0} = 0 \\cdot v_{1} + 1 \\cdot v_{2} + 1 \\cdot v_{3}.\n$$\n\nSince $(A-\\sigma I) v_{i} = (\\lambda_{i} - \\sigma) v_{i}$, it follows that\n$$\n(A-\\sigma I)^{-1} v_{i} = \\frac{1}{\\lambda_{i} - \\sigma} \\, v_{i}.\n$$\nAfter $k$ inverse iterations (before normalization), the vector is proportional to\n$$\n\\sum_{i=1}^{3} \\alpha_{i} (\\lambda_{i} - \\sigma)^{-k} v_{i}.\n$$\nHence, as $k \\to \\infty$, the component with the largest magnitude factor $\\left|(\\lambda_{i} - \\sigma)^{-1}\\right|$ among those with $\\alpha_{i} \\neq 0$ dominates.\n\nWith $\\sigma = 0.9$, we have\n$$\n\\left|\\frac{1}{\\lambda_{2} - \\sigma}\\right| = \\frac{1}{|4 - 0.9|} = \\frac{1}{3.1}, \\qquad\n\\left|\\frac{1}{\\lambda_{3} - \\sigma}\\right| = \\frac{1}{|5 - 0.9|} = \\frac{1}{4.1}.\n$$\nSince $\\frac{1}{3.1} > \\frac{1}{4.1}$ and $\\alpha_{2}, \\alpha_{3} \\neq 0$ but $\\alpha_{1} = 0$, the sequence $\\{x_{k}\\}$ converges to the eigenvector $v_{2}$ corresponding to $\\lambda_{2} = 4$.\n\nTherefore, the eigenvalue corresponding to the limiting eigenvector is $4$.", "answer": "$$\\boxed{4}$$", "id": "1395866"}]}