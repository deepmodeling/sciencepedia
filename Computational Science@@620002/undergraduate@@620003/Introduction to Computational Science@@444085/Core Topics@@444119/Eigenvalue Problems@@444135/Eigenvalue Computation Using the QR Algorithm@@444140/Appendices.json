{"hands_on_practices": [{"introduction": "The most direct way to find the eigenvalues of a matrix $A$ seems to be finding the roots of its characteristic polynomial, $\\det(\\lambda I - A) = 0$. However, this \"textbook\" method is notoriously unstable in practice; small errors in the computed polynomial coefficients can lead to large errors in the roots. This exercise provides a hands-on demonstration of this critical concept by comparing the accuracy of the polynomial method against the QR algorithm, which uses a sequence of numerically stable orthogonal transformations. By working through this problem [@problem_id:3121833], you will gain a profound appreciation for why the QR algorithm is the industry standard for eigenvalue computation.", "problem": "You are tasked with building a program that compares two numerical methods for computing eigenvalues of a small real symmetric matrix: the Orthogonal–Triangular (QR) algorithm and roots obtained from the characteristic polynomial coefficients. The goal is to quantify numerical stability differences between these two approaches for small matrix sizes by measuring how closely each method recovers a set of known ground-truth eigenvalues.\n\nStart from the following foundational base:\n- The eigenvalues of a square matrix are defined as the values of $\\lambda$ such that $\\det(\\lambda I - A) = 0$.\n- The characteristic polynomial of a matrix $A$ of size $n \\times n$ is a monic polynomial of degree $n$ whose roots are the eigenvalues of $A$.\n- Eigenvalues are invariant under similarity transformations; orthogonal similarity transformations preserve the spectrum and are numerically well-conditioned in floating-point arithmetic.\n- For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, the QR algorithm with orthogonal factorizations applies repeated orthogonal similarity transforms that tend to produce a diagonal matrix whose diagonal entries converge to the eigenvalues.\n\nYour program must implement the following steps in a principle-based manner:\n1. Implement a QR iteration specialized to real symmetric matrices of size $n = 3$ without calling any built-in eigenvalue solver. Given a real symmetric matrix $A$, perform iterations of the form $A_{k} = Q_{k}^{\\top} A_{k-1} Q_{k}$ where $A_{k-1} = Q_{k} R_{k}$ is the QR factorization at step $k$, until the off-diagonal Frobenius norm of $A_{k}$ is less than a tolerance. Extract the eigenvalue approximations from the diagonal entries of the resulting matrix.\n2. Construct the characteristic polynomial of the same matrix $A$ and compute its roots numerically. To do this, derive the coefficients of the cubic characteristic polynomial from invariant quantities of $A$ (for $n = 3$, this involves algebraic relations between the traces of powers of $A$ and the determinant of $A$), then compute the roots of this polynomial.\n3. For each test case below, the matrix $A$ must be constructed as $A = Q \\operatorname{diag}(d) Q^{\\top}$, where $Q$ is a fixed orthogonal matrix formed by composing two deterministic three-dimensional rotations, and $\\operatorname{diag}(d)$ is a diagonal matrix of the specified ground-truth eigenvalues. The orthogonal matrix $Q$ must be formed as $Q = R_{z}(\\theta_{z}) R_{y}(\\theta_{y})$, where $R_{z}(\\theta_{z})$ is a rotation around the $z$-axis by angle $\\theta_{z}$ and $R_{y}(\\theta_{y})$ is a rotation around the $y$-axis by angle $\\theta_{y}$. All angles must be interpreted in radians.\n\nDefine the relative error for a computed eigenvalue set $\\hat{\\lambda} \\in \\mathbb{C}^{3}$ against the ground-truth eigenvalues $\\lambda \\in \\mathbb{R}^{3}$ as\n$$\n\\mathrm{relerr}(\\hat{\\lambda}, \\lambda) = \\frac{\\left\\| \\operatorname{sort}(\\hat{\\lambda}) - \\operatorname{sort}(\\lambda) \\right\\|_{2}}{\\left\\|\\lambda\\right\\|_{2}},\n$$\nwhere sorting is done by ascending real part, and the norm is the Euclidean norm in $\\mathbb{C}^{3}$. If the denominator is numerically zero, your implementation must safeguard by adding a tiny positive number to avoid division by zero. For the characteristic polynomial method, it is permissible that numerical computation yields complex approximations; these must be included as complex numbers when evaluating the norm above.\n\nFor each test case, compute the base-$10$ logarithm of the ratio\n$$\nr = \\log_{10}\\left(\\frac{\\mathrm{relerr}(\\hat{\\lambda}_{\\mathrm{poly}}, \\lambda)}{\\max\\left(\\mathrm{relerr}(\\hat{\\lambda}_{\\mathrm{QR}}, \\lambda), \\varepsilon\\right)}\\right),\n$$\nwhere $\\hat{\\lambda}_{\\mathrm{poly}}$ are the eigenvalues computed from the characteristic polynomial coefficients and $\\hat{\\lambda}_{\\mathrm{QR}}$ are those computed by the QR iteration, and $\\varepsilon$ is a small positive safeguard such as $\\varepsilon = 10^{-16}$. A positive value of $r$ indicates that the QR method is more accurate than the polynomial-coefficient-root method for that test case, while a negative value indicates the opposite.\n\nUse the following test suite of parameter values to construct the matrices. Each case specifies the ground-truth eigenvalues $d = [d_{1}, d_{2}, d_{3}]$ and rotation angles $(\\theta_{z}, \\theta_{y})$ in radians:\n- Case $1$ (happy path, well-scaled): $d = [\\,1,\\,2,\\,3\\,]$, $\\theta_{z} = 0.3$, $\\theta_{y} = -0.5$.\n- Case $2$ (nearly multiple eigenvalues): $d = [\\,2,\\,2 + 10^{-10},\\,5\\,]$, $\\theta_{z} = 0.7$, $\\theta_{y} = -0.9$.\n- Case $3$ (large dynamic range): $d = [\\,10^{-8},\\,1,\\,10^{8}\\,]$, $\\theta_{z} = 0.1$, $\\theta_{y} = 0.2$.\n- Case $4$ (sign cancellation in coefficients): $d = [\\,1000,\\,-1000,\\,10^{-12}\\,]$, $\\theta_{z} = 0.4$, $\\theta_{y} = -0.4$.\n- Case $5$ (tightly clustered eigenvalues): $d = [\\,3 - 10^{-12},\\,3,\\,3 + 10^{-12}\\,]$, $\\theta_{z} = 0.2$, $\\theta_{y} = 0.6$.\n\nYour program should produce a single line of output containing the five computed values of $r$ in the order of the test cases above, formatted as a comma-separated list enclosed in square brackets, for example, $[r_{1},r_{2},r_{3},r_{4},r_{5}]$. No other text should be printed. There are no physical units involved in this computation. All angles must be in radians, and all outputs must be represented as floating-point numbers.", "solution": "The task is to quantitatively compare the numerical stability of two methods for computing the eigenvalues of a $3 \\times 3$ real symmetric matrix: the QR algorithm and finding the roots of the characteristic polynomial. The comparison is based on the relative error of the computed eigenvalues against a known ground truth.\n\nThe foundational principle is that while mathematical problems may have multiple equivalent analytical solutions, their computational implementations can exhibit vastly different behavior in the presence of floating-point arithmetic. This problem illustrates the superiority of numerically stable algorithms, like the QR iteration with orthogonal transformations, over methods that can be ill-conditioned, such as solving for polynomial roots from computed coefficients.\n\nFirst, we establish a verifiable ground truth. For each test case, a real symmetric matrix $A$ is constructed to have a predefined set of eigenvalues, $\\lambda = \\{d_1, d_2, d_3\\}$. This is achieved through a similarity transformation $A = Q D Q^\\top$, where $D = \\operatorname{diag}(d_1, d_2, d_3)$ is the diagonal matrix of the true eigenvalues and $Q$ is an orthogonal matrix. An orthogonal transformation $Q$ is chosen because it preserves the eigenvalues of $D$. The matrix $Q$ is constructed by composing two fundamental rotation matrices: a rotation $R_z(\\theta_z)$ about the $z$-axis by an angle $\\theta_z$, and a rotation $R_y(\\theta_y)$ about the $y$-axis by an angle $\\theta_y$. The resulting orthogonal matrix is $Q = R_z(\\theta_z) R_y(\\theta_y)$. The explicit forms of the rotation matrices are:\n$$\nR_z(\\theta_z) = \\begin{pmatrix} \\cos\\theta_z & -\\sin\\theta_z & 0 \\\\ \\sin\\theta_z & \\cos\\theta_z & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad\nR_y(\\theta_y) = \\begin{pmatrix} \\cos\\theta_y & 0 & \\sin\\theta_y \\\\ 0 & 1 & 0 \\\\ -\\sin\\theta_y & 0 & \\cos\\theta_y \\end{pmatrix}.\n$$\nThis construction yields a matrix $A$ whose eigenvalues are guaranteed to be the elements of $d$, allowing for a precise error analysis of the numerical methods.\n\nThe first method is the basic QR algorithm. Starting with $A_0 = A$, the algorithm generates a sequence of matrices $\\{A_k\\}$ via the iteration $A_{k-1} = Q_k R_k$ followed by $A_k = R_k Q_k$, for $k=1, 2, \\dots$. Here, $Q_k$ is an orthogonal matrix and $R_k$ is an upper triangular matrix factors of $A_{k-1}$. Each step is an orthogonal similarity transformation, $A_k = R_k Q_k = (Q_k^\\top A_{k-1}) Q_k = Q_k^\\top A_{k-1} Q_k$. Such transformations preserve the eigenvalues of the original matrix $A$. For a real symmetric matrix, the sequence $A_k$ converges to a diagonal matrix, $\\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\lambda_3)$. The diagonal entries of the limit matrix are the eigenvalues of $A$. This method is known for its excellent numerical stability. The iteration is terminated when the matrix $A_k$ is \"sufficiently diagonal,\" which is measured by the Frobenius norm of its off-diagonal elements, $\\sqrt{\\sum_{i \\neq j} |(A_k)_{ij}|^2}$, falling below a small tolerance, e.g., $\\tau = 10^{-14}$.\n\nThe second method involves the characteristic polynomial. The eigenvalues of $A$ are the roots of the characteristic polynomial $p(\\lambda) = \\det(\\lambda I - A)$. For a $3 \\times 3$ matrix $A$, this is a cubic polynomial which can be written as $p(\\lambda) = \\lambda^3 + c_2 \\lambda^2 + c_1 \\lambda + c_0$. The coefficients $c_i$ can be expressed in terms of matrix invariants, specifically the trace and determinant. Using the Faddeev–LeVerrier algorithm (or Newton's sums), the coefficients are given by:\n$$\nc_2 = -\\mathrm{tr}(A)\n$$\n$$\nc_1 = \\frac{1}{2}\\left((\\mathrm{tr}(A))^2 - \\mathrm{tr}(A^2)\\right)\n$$\n$$\nc_0 = -\\det(A)\n$$\nOnce these coefficients are computed from the matrix $A$, the eigenvalues $\\hat{\\lambda}_{\\mathrm{poly}}$ are found by computing the roots of the polynomial $\\lambda^3 + c_2 \\lambda^2 + c_1 \\lambda + c_0 = 0$ using a numerical root-finding routine. This approach, while mathematically direct, is often numerically unstable. Small errors in the computed coefficients (due to floating-point imprecision in calculating traces and determinants) can lead to large errors in the roots, a phenomenon famously illustrated by Wilkinson's polynomial.\n\nTo compare the two methods, we compute the relative error for each set of computed eigenvalues, $\\hat{\\lambda}_{\\mathrm{QR}}$ and $\\hat{\\lambda}_{\\mathrm{poly}}$, with respect to the ground-truth eigenvalues $\\lambda$. The error is defined as:\n$$\n\\mathrm{relerr}(\\hat{\\lambda}, \\lambda) = \\frac{\\left\\| \\operatorname{sort}(\\hat{\\lambda}) - \\operatorname{sort}(\\lambda) \\right\\|_{2}}{\\left\\|\\lambda\\right\\|_{2}}\n$$\nHere, $\\operatorname{sort}(\\cdot)$ arranges the eigenvalues in ascending order of their real parts, and $\\|\\cdot\\|_2$ is the Euclidean norm. A small positive constant is added to the denominator to prevent division by zero, although the test cases ensure this is not an issue.\n\nFinally, the relative performance of the two methods is quantified by the base-$10$ logarithm of an error ratio:\n$$\nr = \\log_{10}\\left(\\frac{\\mathrm{relerr}(\\hat{\\lambda}_{\\mathrm{poly}}, \\lambda)}{\\max\\left(\\mathrm{relerr}(\\hat{\\lambda}_{\\mathrm{QR}}, \\lambda), \\varepsilon\\right)}\\right)\n$$\nwhere $\\varepsilon = 10^{-16}$ is a small positive constant to avoid division by zero or by an extremely small number if the QR method achieves machine-precision accuracy. A positive value of $r$ indicates that the error from the polynomial method is larger than that of the QR method, signifying that the QR algorithm is more accurate for the given test case. The program will compute this value $r$ for each of the five specified test cases.", "answer": "```python\nimport numpy as np\n\ndef construct_matrix(d, theta_z, theta_y):\n    \"\"\"\n    Constructs a 3x3 real symmetric matrix A = Q D Q^T with known eigenvalues.\n\n    Args:\n        d (list or np.ndarray): The ground-truth eigenvalues.\n        theta_z (float): Rotation angle around the z-axis in radians.\n        theta_y (float): Rotation angle around the y-axis in radians.\n\n    Returns:\n        np.ndarray: The 3x3 symmetric matrix A.\n    \"\"\"\n    # Rotation matrix around z-axis\n    cz, sz = np.cos(theta_z), np.sin(theta_z)\n    Rz = np.array([\n        [cz, -sz, 0],\n        [sz, cz, 0],\n        [0, 0, 1]\n    ])\n\n    # Rotation matrix around y-axis\n    cy, sy = np.cos(theta_y), np.sin(theta_y)\n    Ry = np.array([\n        [cy, 0, sy],\n        [0, 1, 0],\n        [-sy, 0, cy]\n    ])\n\n    # Composite orthogonal matrix Q\n    Q = Rz @ Ry\n    # Diagonal matrix of eigenvalues\n    D = np.diag(d)\n\n    # Construct the symmetric matrix A\n    A = Q @ D @ Q.T\n    return A\n\ndef compute_eigs_qr(A, tol=1e-14, max_iter=1000):\n    \"\"\"\n    Computes eigenvalues of a symmetric matrix using the basic QR algorithm.\n\n    Args:\n        A (np.ndarray): The input 3x3 symmetric matrix.\n        tol (float): Tolerance for a off-diagonal norm to determine convergence.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The computed eigenvalues.\n    \"\"\"\n    Ak = A.copy()\n    for _ in range(max_iter):\n        # The Frobenius norm of the off-diagonal elements for a symmetric 3x3 matrix\n        off_diag_norm = np.sqrt(2 * (Ak[0, 1]**2 + Ak[0, 2]**2 + Ak[1, 2]**2))\n        if off_diag_norm < tol:\n            break\n        \n        # QR factorization and update step\n        Q, R = np.linalg.qr(Ak)\n        Ak = R @ Q\n        \n    return np.diag(Ak)\n\ndef compute_eigs_poly(A):\n    \"\"\"\n    Computes eigenvalues by finding roots of the characteristic polynomial.\n\n    Args:\n        A (npndarray): The input 3x3 matrix.\n\n    Returns:\n        np.ndarray: The computed eigenvalues (can be complex).\n    \"\"\"\n    tr_A = np.trace(A)\n    A_sq = A @ A\n    tr_A2 = np.trace(A_sq)\n    det_A = np.linalg.det(A)\n\n    # Coefficients for p(lambda) = lambda^3 + c2*lambda^2 + c1*lambda + c0 = 0\n    c2 = -tr_A\n    c1 = 0.5 * (tr_A**2 - tr_A2)\n    c0 = -det_A\n\n    coeffs = [1, c2, c1, c0]\n    return np.roots(coeffs)\n\ndef calculate_relative_error(lambda_hat, lambda_true, eps=1e-16):\n    \"\"\"\n    Calculates the relative error between computed and true eigenvalues.\n\n    Args:\n        lambda_hat (np.ndarray): Computed eigenvalues.\n        lambda_true (np.ndarray): Ground-truth eigenvalues.\n        eps (float): Small constant to safeguard against division by zero.\n\n    Returns:\n        float: The relative error.\n    \"\"\"\n    # Sort both eigenvalue sets. np.sort on complex arrays sorts by real part first.\n    sorted_lambda_hat = np.sort(lambda_hat)\n    sorted_lambda_true = np.sort(lambda_true)\n\n    numerator = np.linalg.norm(sorted_lambda_hat - sorted_lambda_true)\n    denominator = np.linalg.norm(sorted_lambda_true)\n    \n    # Safeguard against division by zero or a numerically zero value\n    return numerator / max(denominator, eps)\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison for all test cases.\n    \"\"\"\n    test_cases = [\n        {'d': [1., 2., 3.], 'theta_z': 0.3, 'theta_y': -0.5},\n        {'d': [2., 2. + 1e-10, 5.], 'theta_z': 0.7, 'theta_y': -0.9},\n        {'d': [1e-8, 1., 1e8], 'theta_z': 0.1, 'theta_y': 0.2},\n        {'d': [1000., -1000., 1e-12], 'theta_z': 0.4, 'theta_y': -0.4},\n        {'d': [3. - 1e-12, 3., 3. + 1e-12], 'theta_z': 0.2, 'theta_y': 0.6},\n    ]\n\n    results = []\n    comparison_epsilon = 1e-16\n\n    for case in test_cases:\n        d_true = np.array(case['d'])\n        A = construct_matrix(d_true, case['theta_z'], case['theta_y'])\n        \n        # Compute eigenvalues using both methods\n        lambda_qr = compute_eigs_qr(A)\n        lambda_poly = compute_eigs_poly(A)\n        \n        # Calculate relative errors\n        relerr_qr = calculate_relative_error(lambda_qr, d_true)\n        relerr_poly = calculate_relative_error(lambda_poly, d_true)\n        \n        # Compute the log-ratio of errors\n        r = np.log10(relerr_poly / max(relerr_qr, comparison_epsilon))\n        results.append(r)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3121833"}, {"introduction": "Now that we understand the QR algorithm's superior stability, we turn to its practical efficiency: how fast does it converge? The convergence rate is not uniform and is heavily influenced by the distribution of the eigenvalues themselves. This practice [@problem_id:3121818] guides you through an investigation of this relationship by implementing the QR algorithm with the powerful Wilkinson shift. By measuring the number of iterations required for matrices with different eigenvalue clustering, you will develop an intuition for the algorithm's performance characteristics.", "problem": "You are asked to investigate, by computation, how the separation of eigenvalues influences the convergence speed of the orthogonal-triangular (QR) algorithm when applied to real symmetric matrices. Your study must be performed in purely mathematical and algorithmic terms, and the result must be a complete, runnable program that outputs the number of QR iterations required for convergence on a small test suite of matrices. Your program must implement the classical shifted QR algorithm with a Wilkinson shift, use orthogonal similarity transforms, and terminate when the off-diagonal Frobenius norm falls below a prescribed tolerance.\n\nFoundational base for this task:\n- Orthogonal similarity preserves eigenvalues: if $Q$ is orthogonal (that is, $Q^{\\mathsf{T}} Q = I$), then $Q^{\\mathsf{T}} A Q$ and $A$ have the same spectrum.\n- The orthogonal-triangular (QR) factorization decomposes any square matrix $M$ as $M = Q R$, where $Q$ is orthogonal and $R$ is upper triangular.\n- For real symmetric matrices, the QR algorithm with appropriate shifts converges to a diagonal matrix whose diagonal entries are the eigenvalues.\n\nYour implementation requirements:\n- Construct each test matrix $A \\in \\mathbb{R}^{n \\times n}$ via an orthogonal similarity of a diagonal matrix of prescribed eigenvalues, namely $A = Q^{\\mathsf{T}} \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n) Q$, with $Q$ orthogonal.\n- Use the shifted QR iteration with a Wilkinson shift. In each iteration $k$, given the current iterate $A_k$, form a shift $\\mu_k$ from the trailing $2 \\times 2$ principal submatrix of $A_k$ via the real Wilkinson shift, then compute the orthogonal-triangular factorization of $A_k - \\mu_k I$ as $(A_k - \\mu_k I) = Q_k R_k$, and set $A_{k+1} = R_k Q_k + \\mu_k I$. Maintain symmetry at each step by explicitly symmetrizing $A_{k+1}$ as $(A_{k+1} + A_{k+1}^{\\mathsf{T}})/2$ to control roundoff drift.\n- Use the off-diagonal Frobenius norm as the stopping criterion. Define the strict lower-triangular part $L_k = \\operatorname{tril}(A_k,-1)$ and stop when $\\|L_k\\|_{\\mathrm{F}} \\le \\varepsilon$, with tolerance $\\varepsilon = 10^{-10}$. To avoid infinite loops in extreme cases, impose a maximum of $N_{\\max} = 5000$ iterations and stop if it is reached.\n- All computations are dimensionless; no physical units apply.\n\nTest suite specification:\n- Dimension: $n = 6$ for all cases.\n- For each case, construct $A$ as $A = Q^{\\mathsf{T}} \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_6) Q$, where $Q$ is generated by applying an orthogonal-triangular factorization to a real Gaussian random matrix seeded by a specified integer, except where noted. Use independent seeds per case to ensure reproducibility. Specifically:\n  1. Case $\\mathrm{C1}$ (moderately clustered eigenvalues, “happy path”): $\\lambda = [\\,1.0,\\,1.01,\\,1.02,\\,3.0,\\,5.0,\\,8.0\\,]$ with seed $0$.\n  2. Case $\\mathrm{C2}$ (well-separated eigenvalues): $\\lambda = [\\,1.0,\\,2.0,\\,4.0,\\,8.0,\\,16.0,\\,32.0\\,]$ with seed $1$.\n  3. Case $\\mathrm{C3}$ (tightly clustered subset): $\\lambda = [\\,1.0,\\,1.0001,\\,1.0002,\\,2.0,\\,3.0,\\,4.0\\,]$ with seed $2$.\n  4. Case $\\mathrm{C4}$ (already diagonal boundary case): $A = \\operatorname{diag}(1.0,\\,2.0,\\,2.0,\\,3.0,\\,5.0,\\,8.0)$, that is, use $Q = I$ so no similarity transform is applied.\n  5. Case $\\mathrm{C5}$ (repeated eigenvalues with mixing): $\\lambda = [\\,2.0,\\,2.0,\\,2.0,\\,3.0,\\,4.0,\\,5.0\\,]$ with seed $3$.\n\nWhat to compute:\n- For each case, run the shifted QR iteration until the stopping criterion is satisfied or until the maximum number of iterations is reached, and record the integer iteration count $k$ used.\n\nFinal output format:\n- Your program should produce a single line of output containing the five iteration counts, in the order $\\mathrm{C1}$ through $\\mathrm{C5}$, as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3,k_4,k_5]$, where each $k_i$ is an integer.\n\nNotes:\n- Use double-precision floating-point arithmetic (the default in the specified libraries).\n- Angles are not involved; no angle units are required.\n- There are no percentages involved; do not use percent signs anywhere.", "solution": "The user-provided problem is valid. It is scientifically sound, well-posed, objective, and contains all necessary information to construct a unique, computable solution. The task is to implement the shifted QR algorithm for real symmetric matrices to study the effect of eigenvalue separation on convergence speed.\n\nThe solution is constructed based on the following principles and algorithmic steps.\n\n**1. Matrix Construction via Orthogonal Similarity**\nThe foundation of the QR algorithm is that orthogonal similarity transforms preserve eigenvalues. To create test matrices with a predefined spectrum $\\lambda = \\{\\lambda_1, \\dots, \\lambda_n\\}$, we construct a matrix $A$ as:\n$$\nA = Q^{\\mathsf{T}} D Q\n$$\nwhere $D = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$ is the diagonal matrix of eigenvalues, and $Q$ is an orthogonal matrix (i.e., $Q^{\\mathsf{T}} Q = I$, where $I$ is the identity matrix). This construction ensures that $A$ is symmetric and has the specified eigenvalues. To ensure reproducibility while maintaining a non-trivial structure, the matrix $Q$ is generated from the QR factorization of a random matrix $M$ (i.e., $M=QR$), with a fixed seed for the random number generator for each test case.\n\n**2. The Shifted QR Iteration**\nThe QR algorithm iteratively computes a sequence of matrices $\\{A_k\\}_{k=0}^\\infty$ that converges to a diagonal matrix. Starting with $A_0 = A$, each iteration performs a \"shifted QR step\":\n1.  A shift, $\\mu_k$, is chosen.\n2.  The QR factorization of the shifted matrix is computed: $A_k - \\mu_k I = Q_k R_k$.\n3.  The next iterate is formed: $A_{k+1} = R_k Q_k + \\mu_k I$.\n\nThis process constitutes an orthogonal similarity transform, as $A_{k+1} = Q_k^{\\mathsf{T}}(A_k - \\mu_k I)Q_k + \\mu_k I = Q_k^{\\mathsf{T}}A_k Q_k$. Thus, each $A_k$ is symmetric and shares the same eigenvalues as the original matrix $A$.\n\n**3. The Wilkinson Shift for Accelerated Convergence**\nThe choice of shift $\\mu_k$ is critical for the speed of convergence. The problem specifies the **Wilkinson shift**, which is known to provide asymptotic cubic convergence for symmetric matrices. For each iterate $A_k$, the shift is derived from its trailing $2 \\times 2$ principal submatrix:\n$$\nT_k = \\begin{pmatrix} a_{n-1,n-1}^{(k)} & a_{n-1,n}^{(k)} \\\\ a_{n,n-1}^{(k)} & a_{n,n}^{(k)} \\end{pmatrix}\n$$\nSince $A_k$ is symmetric, $a_{n-1,n}^{(k)} = a_{n,n-1}^{(k)}$. The Wilkinson shift $\\mu_k$ is the eigenvalue of $T_k$ that is closer to the element $a_{n,n}^{(k)}$. Let $a = a_{n-1,n-1}^{(k)}$, $b = a_{n,n}^{(k)}$, and $c = a_{n-1,n}^{(k)}$. The shift can be computed using a numerically stable formula that avoids subtractive cancellation:\n$$\n\\mu_k = b - \\frac{\\operatorname{sgn}(\\delta) c^2}{|\\delta| + \\sqrt{\\delta^2 + c^2}}\n$$\nwhere $\\delta = (a-b)/2$ and $\\operatorname{sgn}(\\delta)$ is defined as $1$ if $\\delta \\ge 0$ and $-1$ if $\\delta < 0$. If the off-diagonal element $c$ is zero, the shift is simply $b$.\n\n**4. Symmetrization for Numerical Stability**\nIn floating-point arithmetic, the symmetry of $A_k$ can be gradually lost due to roundoff errors. To counteract this, after each iteration, the matrix $A_{k+1}$ is explicitly re-symmetrized:\n$$\nA_{k+1} \\leftarrow \\frac{A_{k+1} + A_{k+1}^{\\mathsf{T}}}{2}\n$$\n\n**5. Convergence Criterion**\nThe algorithm terminates when the iterate $A_k$ is sufficiently close to a diagonal matrix. This is quantified by measuring the magnitude of the off-diagonal elements. The stopping criterion specified is based on the Frobenius norm of the strictly lower-triangular part of $A_k$, denoted $L_k = \\operatorname{tril}(A_k, -1)$. The iteration stops when:\n$$\n\\|L_k\\|_{\\mathrm{F}} \\le \\varepsilon\n$$\nwhere the Frobenius norm is $\\|M\\|_{\\mathrm{F}} = \\sqrt{\\sum_{i,j} |m_{ij}|^2}$, and the tolerance is $\\varepsilon = 10^{-10}$. A maximum iteration limit of $N_{\\max} = 5000$ is also enforced to prevent infinite execution. The result of the computation for each test case is the total number of iterations, $k$, required to satisfy this criterion.", "answer": "```python\n# The complete and runnable Python 3 code that implements the shifted QR algorithm.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run the QR algorithm on a test suite of matrices\n    and report the number of iterations for convergence.\n    \"\"\"\n    \n    # Define problem constants\n    TOLERANCE = 1e-10\n    MAX_ITERATIONS = 5000\n    MATRIX_DIMENSION = 6\n\n    def create_matrix(eigenvalues, seed):\n        \"\"\"\n        Constructs a test matrix A = Q^T D Q for a given list of eigenvalues.\n        If the seed is None, it returns the diagonal matrix D itself.\n        \n        Args:\n            eigenvalues (list): A list of floats for the diagonal of D.\n            seed (int or None): The seed for the random number generator.\n\n        Returns:\n            np.ndarray: The constructed n x n symmetric matrix.\n        \"\"\"\n        n = len(eigenvalues)\n        D = np.diag(eigenvalues)\n\n        if seed is None:\n            # Handle Case C4, which is an already diagonal matrix.\n            return D\n\n        # Use a seeded random number generator for reproducible results.\n        rng = np.random.default_rng(seed)\n        \n        # Generate a random matrix M to derive an orthogonal matrix Q.\n        M = rng.standard_normal(size=(n, n))\n        \n        # The orthogonal matrix Q from the QR factorization of M.\n        Q, _ = np.linalg.qr(M)\n\n        # Construct the symmetric matrix A = Q^T D Q via similarity transform.\n        A = Q.T @ D @ Q\n        return A\n\n    def wilkinson_shift(A):\n        \"\"\"\n        Calculates the Wilkinson shift from the trailing 2x2 submatrix of A.\n        Uses a numerically stable formula and handles edge cases.\n\n        Args:\n            A (np.ndarray): The current symmetric matrix iterate.\n\n        Returns:\n            float: The Wilkinson shift value.\n        \"\"\"\n        # Get the trailing 2x2 submatrix elements.\n        a = A[-2, -2]\n        c = A[-2, -1] \n        b = A[-1, -1]\n\n        # If the off-diagonal element is close to zero, the trailing block is\n        # already diagonal. The eigenvalue is b, so that's the shift.\n        if np.isclose(c, 0.0):\n            return b\n\n        delta = (a - b) / 2.0\n        \n        # Define sgn(0) as 1 for the formula.\n        sign_delta = 1.0 if delta >= 0.0 else -1.0\n        \n        # Numerically stable formula for the eigenvalue of the 2x2 matrix\n        # that is closer to b. This avoids subtractive cancellation and 0/0.\n        shift = b - sign_delta * c**2 / (abs(delta) + np.sqrt(delta**2 + c**2))\n        \n        return shift\n\n    def shifted_qr_iteration(A_init, tol, max_iter):\n        \"\"\"\n        Performs the shifted QR algorithm with Wilkinson shift on a symmetric matrix.\n\n        Args:\n            A_init (np.ndarray): The initial symmetric matrix.\n            tol (float): The convergence tolerance for the off-diagonal norm.\n            max_iter (int): The maximum number of iterations allowed.\n\n        Returns:\n            int: The number of iterations until convergence or max_iter.\n        \"\"\"\n        A_k = np.copy(A_init)\n        n = A_k.shape[0]\n        \n        for k in range(max_iter):\n            # 1. Stopping Criterion: Check the Frobenius norm of the off-diagonal part.\n            # For a symmetric matrix, the norm of the strict lower triangle suffices.\n            off_diagonal_norm = np.linalg.norm(np.tril(A_k, -1), 'fro')\n            \n            if off_diagonal_norm <= tol:\n                return k\n            \n            # 2. Shift Calculation: Compute the Wilkinson shift.\n            mu_k = wilkinson_shift(A_k)\n            \n            # 3. QR Factorization: Decompose the shifted matrix.\n            I = np.identity(n)\n            Q_k, R_k = np.linalg.qr(A_k - mu_k * I)\n            \n            # 4. Matrix Update: Form the next iterate.\n            A_k = R_k @ Q_k + mu_k * I\n            \n            # 5. Symmetrization: Enforce symmetry to mitigate floating-point error.\n            A_k = (A_k + A_k.T) / 2.0\n            \n        return max_iter\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # C1 (moderately clustered)\n        {'eigenvalues': [1.0, 1.01, 1.02, 3.0, 5.0, 8.0], 'seed': 0},\n        # C2 (well-separated)\n        {'eigenvalues': [1.0, 2.0, 4.0, 8.0, 16.0, 32.0], 'seed': 1},\n        # C3 (tightly clustered subset)\n        {'eigenvalues': [1.0, 1.0001, 1.0002, 2.0, 3.0, 4.0], 'seed': 2},\n        # C4 (already diagonal)\n        {'eigenvalues': [1.0, 2.0, 2.0, 3.0, 5.0, 8.0], 'seed': None},\n        # C5 (repeated eigenvalues)\n        {'eigenvalues': [2.0, 2.0, 2.0, 3.0, 4.0, 5.0], 'seed': 3}\n    ]\n    \n    results = []\n    # Process each case and collect the iteration count.\n    for case in test_cases:\n        A = create_matrix(case['eigenvalues'], case['seed'])\n        \n        # Ensure the matrix dimension matches the problem spec.\n        if A.shape[0] != MATRIX_DIMENSION:\n            raise ValueError(f\"Matrix dimension mismatch for a test case.\")\n            \n        iterations = shifted_qr_iteration(A, tol=TOLERANCE, max_iter=MAX_ITERATIONS)\n        results.append(iterations)\n        \n    # Print the final results in the specified single-line format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3121818"}, {"introduction": "The most sophisticated versions of the QR algorithm employ an \"implicit shifting\" technique to enhance efficiency and handle complex eigenvalues using only real arithmetic. This method, known as the Francis QR step, works by introducing a small \"bulge\" into the matrix and then \"chasing\" it away with a series of carefully chosen rotations, all without ever explicitly forming the shifted matrix. This exercise [@problem_id:3121888] offers a unique opportunity to perform one of these implicit double-shift steps by hand, demystifying the elegant mechanics that make the modern QR algorithm so powerful and efficient.", "problem": "Consider the $4 \\times 4$ real upper Hessenberg matrix\n$$\nH \\;=\\;\n\\begin{pmatrix}\n6 & 5 & 0 & 0 \\\\\n4 & 6 & 5 & 0 \\\\\n0 & 4 & 6 & 5 \\\\\n0 & 0 & 4 & 6\n\\end{pmatrix}.\n$$\nAn implicit double-shift step of the orthogonal-triangular (QR) algorithm uses a real quadratic shift polynomial derived from the trailing $2 \\times 2$ block. Let the trailing block be denoted by\n$$\nT \\;=\\;\n\\begin{pmatrix}\n6 & 5 \\\\\n4 & 6\n\\end{pmatrix},\n$$\nand let its eigenvalues be $s_1$ and $s_2$ (possibly complex). Define the real coefficients $b = s_1 + s_2$ and $c = s_1 s_2$ so that the quadratic polynomial is $p(\\lambda) = \\lambda^2 - b \\lambda + c$. The implicit step begins by forming the vector\n$$\nv \\;=\\; (H^2 - b H + c I)e_1,\n$$\nwhere $e_1$ is the first standard basis vector and $I$ is the identity matrix. One then applies an orthogonal transformation acting on the leading $3 \\times 3$ block that maps $v$ to $\\alpha e_1$, with the convention that\n$$\n\\alpha \\;=\\; \\|v\\|_2.\n$$\nThis introduces a bulge strictly below the first subdiagonal, which is then chased rightward by successive $2 \\times 2$ orthogonal transformations (Givens rotations) until the upper Hessenberg form is restored.\n\nTasks:\n- Starting from first principles of similarity transformations and the Hessenberg structure preservation in the QR method, determine $b$ and $c$ from $T$, explicitly compute $v = (H^2 - b H + c I)e_1$, and construct an orthogonal reflector on the leading $3 \\times 3$ block that sends $v$ to $\\alpha e_1$.\n- Apply this reflector as a similarity on $H$ to expose the first bulge, and explicitly identify its matrix position $(i,j)$ in the updated matrix.\n- Explain the annihilation path of the bulge (the ordered sequence of positions $(i,j)$ that the bulge occupies as it is chased by Givens rotations) for this $4 \\times 4$ case, justifying why the path restores the upper Hessenberg form at the end of the step.\n\nReport the scalar $\\alpha$ as your final answer. No rounding is required. Your answer must be a single real number.", "solution": "The user wants me to solve a multi-part problem concerning an implicit double-shift QR step on a given $4 \\times 4$ upper Hessenberg matrix.\n\n### Problem Validation\n\nI will first validate the problem statement.\n\n**Step 1: Extract Givens**\n- The matrix is $H = \\begin{pmatrix} 6 & 5 & 0 & 0 \\\\ 4 & 6 & 5 & 0 \\\\ 0 & 4 & 6 & 5 \\\\ 0 & 0 & 4 & 6 \\end{pmatrix}$.\n- The trailing $2 \\times 2$ block is $T = \\begin{pmatrix} 6 & 5 \\\\ 4 & 6 \\end{pmatrix}$.\n- The eigenvalues of $T$ are $s_1$ and $s_2$.\n- The shift polynomial is $p(\\lambda) = \\lambda^2 - b \\lambda + c$, where $b = s_1 + s_2$ and $c = s_1 s_2$.\n- The first vector in the implicit step is $v = (H^2 - b H + c I)e_1$, where $e_1$ is the first standard basis vector.\n- An orthogonal transformation acts on the leading $3 \\times 3$ block to map $v$ to $\\alpha e_1$.\n- The convention for $\\alpha$ is $\\alpha = \\|v\\|_2$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, specifically about the Francis (double-shift) QR algorithm. All concepts are based on established matrix theory and numerical analysis principles. It is scientifically sound.\n- **Well-Posed:** The problem provides all necessary information to compute the requested quantities. The tasks are clearly defined and lead to a unique solution based on the provided conventions.\n- **Objective:** The language is precise and mathematical, free of any subjectivity or ambiguity.\n\nThe problem is self-contained, consistent, and well-posed. It falls squarely within the topic of computational science and eigenvalue algorithms.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n### Solution\n\nThe problem asks for three main tasks:\n1. Determine the coefficients $b$ and $c$, compute the vector $v$, and construct the initial reflector.\n2. Apply the reflector to $H$ and identify the position of the resulting bulge.\n3. Explain the annihilation path of the bulge.\nFinally, I must report the scalar $\\alpha$.\n\n**Part 1: Computation of $b$, $c$, $v$, and the reflector**\n\nThe coefficients $b$ and $c$ of the shift polynomial $p(\\lambda) = \\lambda^2 - b\\lambda + c$ are derived from the trailing $2 \\times 2$ block $T$. The polynomial $p(\\lambda)$ is the characteristic polynomial of $T$. The characteristic equation is $\\det(T - \\lambda I) = 0$.\n$$\n\\det\\left(\\begin{pmatrix} 6 & 5 \\\\ 4 & 6 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = (6 - \\lambda)(6 - \\lambda) - (5)(4) = \\lambda^2 - 12\\lambda + 36 - 20 = \\lambda^2 - 12\\lambda + 16.\n$$\nBy comparing this with $p(\\lambda) = \\lambda^2 - b\\lambda + c$, we identify the coefficients:\n- $b = s_1 + s_2 = \\text{trace}(T) = 6+6 = 12$.\n- $c = s_1 s_2 = \\det(T) = (6)(6) - (5)(4) = 36-20 = 16$.\n\nNext, we compute the vector $v = p(H)e_1 = (H^2 - b H + c I)e_1$. Since we are multiplying by $e_1$, we only need to compute the first column of the matrix $p(H)$.\nFirst, we find $He_1$, which is the first column of $H$:\n$$\nHe_1 = \\begin{pmatrix} 6 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nNext, we compute $H^2 e_1 = H(He_1)$:\n$$\nH^2 e_1 = \\begin{pmatrix} 6 & 5 & 0 & 0 \\\\ 4 & 6 & 5 & 0 \\\\ 0 & 4 & 6 & 5 \\\\ 0 & 0 & 4 & 6 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6(6)+5(4) \\\\ 4(6)+6(4) \\\\ 0(6)+4(4) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 36+20 \\\\ 24+24 \\\\ 16 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 56 \\\\ 48 \\\\ 16 \\\\ 0 \\end{pmatrix}.\n$$\nNow we assemble the vector $v$:\n$$\nv = H^2 e_1 - 12(He_1) + 16(Ie_1) = \\begin{pmatrix} 56 \\\\ 48 \\\\ 16 \\\\ 0 \\end{pmatrix} - 12 \\begin{pmatrix} 6 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix} + 16 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 56 - 72 + 16 \\\\ 48 - 48 + 0 \\\\ 16 - 0 + 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 16 \\\\ 0 \\end{pmatrix}.\n$$\nThe problem states that an orthogonal transformation maps $v$ to $\\alpha e_1$, with the convention $\\alpha = \\|v\\|_2$.\n$$\n\\alpha = \\|v\\|_2 = \\sqrt{0^2 + 0^2 + 16^2 + 0^2} = 16.\n$$\nThe problem specifies that the transformation is an orthogonal reflector acting on the leading $3 \\times 3$ block. Let this $4 \\times 4$ transformation be $Q_0 = \\begin{pmatrix} \\hat{Q}_0 & 0 \\\\ 0 & 1 \\end{pmatrix}$, where $\\hat{Q}_0$ is a $3 \\times 3$ reflector. We are mapping the first three components of $v$, which we denote $v' = (0, 0, 16)^T$, to $\\alpha e_1'$, where $e_1'=(1,0,0)^T$ is the first basis vector in $\\mathbb{R}^3$ and $\\alpha=\\|v'\\|_2=16$. Thus, we want $\\hat{Q}_0 v' = (16,0,0)^T$.\nA Householder reflector is given by $\\hat{Q}_0 = I - 2 \\frac{u u^T}{u^T u}$, where $u = v' - \\alpha e_1'$.\n$$\nu = \\begin{pmatrix} 0 \\\\ 0 \\\\ 16 \\end{pmatrix} - \\begin{pmatrix} 16 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -16 \\\\ 0 \\\\ 16 \\end{pmatrix}.\n$$\nWe can use a simpler, scaled version of $u$, such as $u_{s} = (-1, 0, 1)^T$.\n$u_s^T u_s = (-1)^2+0^2+1^2 = 2$.\n$u_s u_s^T = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} -1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}$.\nThe reflector is:\n$$\n\\hat{Q}_0 = I - 2 \\frac{u_s u_s^T}{u_s^T u_s} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{2}{2} \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}.\n$$\nThis is a simple permutation matrix because the vector $v'$ had only one non-zero component.\n\n**Part 2: Applying the Reflector and Identifying the Bulge**\n\nWe apply the reflector $Q_0$ as a similarity transformation to $H$. As $\\hat{Q}_0$ is symmetric, $Q_0$ is symmetric, so $Q_0^T = Q_0$. The new matrix is $H_1 = Q_0 H Q_0$.\n$Q_0 = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$.\n$Q_0$ swaps rows 1 and 3, and columns 1 and 3.\n$$\nH_1 = Q_0 H Q_0 = Q_0 \\left( \\begin{pmatrix} 6 & 5 & 0 & 0 \\\\ 4 & 6 & 5 & 0 \\\\ 0 & 4 & 6 & 5 \\\\ 0 & 0 & 4 & 6 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\right) = Q_0 \\begin{pmatrix} 0 & 5 & 6 & 0 \\\\ 5 & 6 & 4 & 0 \\\\ 6 & 4 & 0 & 5 \\\\ 4 & 0 & 0 & 6 \\end{pmatrix}.\n$$\n$$\nH_1 = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 5 & 6 & 0 \\\\ 5 & 6 & 4 & 0 \\\\ 6 & 4 & 0 & 5 \\\\ 4 & 0 & 0 & 6 \\end{pmatrix} = \\begin{pmatrix} 6 & 4 & 0 & 5 \\\\ 5 & 6 & 4 & 0 \\\\ 0 & 5 & 6 & 0 \\\\ 4 & 0 & 0 & 6 \\end{pmatrix}.\n$$\nAn upper Hessenberg matrix $A$ satisfies $A_{ij} = 0$ for all $i > j+1$. We inspect $H_1$ for elements that violate this condition.\n- $H_1(3,1) = 0$. ($i=3, j=1 \\implies i=j+2$). This is fine.\n- $H_1(4,1) = 4$. ($i=4, j=1 \\implies i=j+3$). This violates the Hessenberg structure.\n- $H_1(4,2) = 0$. ($i=4, j=2 \\implies i=j+2$). This is fine.\nThe \"bulge\" is the non-zero element strictly below the first subdiagonal. In this case, it is the single element $H_1(4,1) = 4$. Its matrix position is $(i,j) = (4,1)$.\n\n**Part 3: Annihilation Path of the Bulge**\n\nThe bulge must be \"chased\" down and to the right out of the matrix by a sequence of similarity transformations using Givens rotations, restoring the upper Hessenberg form. The general principle for chasing a bulge at position $(i,j)$ is to apply a Givens rotation $G(i-1, i)$ to annihilate it. This action then creates a new bulge at position $(i, j+1)$.\n\nFor this specific case, the path is as follows:\n1.  **Start:** The bulge is at position $(i,j) = (4,1)$.\n2.  **Step 1:** To annihilate the bulge at $(4,1)$, a Givens rotation $G_1 = G(3,4)$ is constructed. The similarity transformation $H_2 = G_1^T H_1 G_1$ is applied. Left multiplication by $G_1^T$ on rows 3 and 4 eliminates the element at $(4,1)$. Right multiplication by $G_1$ on columns 3 and 4 is required to maintain similarity. This column operation creates a new non-zero element (a new bulge) at position $(4,2)$. So, the bulge moves from $(4,1)$ to $(4,2)$.\n3.  **Step 2:** The new bulge is at $(4,2)$. To annihilate it, a Givens rotation $G_2 = G(3,4)$ is constructed. The similarity transformation $H_3 = G_2^T H_2 G_2$ is applied. Similar to the previous step, this transformation is designed to zero out the element at $(4,2)$, which in turn creates a new bulge at position $(4,3)$. The bulge moves from $(4,2)$ to $(4,3)$.\n4.  **Step 3:** The bulge is now at $(4,3)$. A final Givens rotation $G_3 = G(3,4)$ is used. The transformation $H_4 = G_3^T H_3 G_3$ eliminates the element at $(4,3)$. The right multiplication on columns 3 and 4 will modify the element at $(4,4)$, but since $i \\le j+1$ for this position ($4 \\le 4+1$), this does not violate the upper Hessenberg structure. The chase is complete, and the resulting matrix $H_4$ is in upper Hessenberg form.\n\nThe ordered sequence of positions occupied by the bulge is $(4,1) \\to (4,2) \\to (4,3)$.\n\nThe final answer required is the scalar $\\alpha$.\n$$\n\\alpha = 16.\n$$", "answer": "$$\\boxed{16}$$", "id": "3121888"}]}