{"hands_on_practices": [{"introduction": "The foundation of numerical differentiation lies in understanding not just the formulas, but also their inherent errors. This practice guides you through deriving the forward, backward, and central difference schemes from first principles using Taylor series expansions. By analyzing their truncation errors, you will gain a concrete understanding of their accuracy and see how it behaves in special cases, like at a function's inflection point. [@problem_id:3132346]", "problem": "You are given the scalar function $f(x) = \\dfrac{1}{1 + e^{-x}}$, which is smooth for all real $x$ and has an inflection point at $x = 0$. Your task is to estimate the slope $f'(0)$ using discrete difference quotients constructed on a uniform grid with spacing $h > 0$ and to quantify the bias of these estimates when the grid is coarse.\n\nUse the following as the fundamental base for your derivations and algorithm design:\n- The definition of the derivative of a smooth function, $f'(x) = \\lim_{h \\to 0} \\dfrac{f(x+h) - f(x)}{h}$.\n- The Taylor series expansion of a smooth function around a point, for example $f(x \\pm h)$ expanded about $x$.\n- Elementary rules of differentiation for the exponential function.\n\nDo not assume any difference formula as given. Instead, proceed from the principles above.\n\nTasks:\n1. Starting from Taylor series expansions about a point $x_0$, derive the leading-order truncation error for the three one-dimensional difference-quotient estimators of the derivative at $x_0$ commonly called the forward, backward, and central schemes. Express the truncation errors in terms of $f''(x_0)$, $f'''(x_0)$, and integer powers of $h$.\n2. Specialize your results to the logistic function $f(x) = \\dfrac{1}{1 + e^{-x}}$ at its inflection point $x_0 = 0$. Compute $f'(0)$ exactly, and also compute $f''(0)$ and $f'''(0)$ to determine the leading-order truncation error for the central scheme at $x_0 = 0$.\n3. Implement a program that, for each step size $h$ in the test suite $\\{2, 1, 0.5, 0.25\\}$, does all of the following at $x_0 = 0$:\n   - Computes the three numerical slope estimates using the forward, backward, and central schemes.\n   - Computes the empirical bias of each estimate as $\\text{bias} = \\text{estimate} - f'(0)$.\n   - Computes the predicted leading-order bias for the central scheme using your derived leading-order truncation term specialized to $x_0 = 0$.\n4. Final output format: Your program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, enclosed in square brackets. For each step size $h$, output the list $[h,\\text{bias}_{\\text{fwd}},\\text{bias}_{\\text{bwd}},\\text{bias}_{\\text{cen}},\\text{bias}_{\\text{cen,pred}}]$, where all entries are decimal numbers. The overall output is thus a single list of four such lists, in the order $h \\in \\{2,1,0.5,0.25\\}$.\n\nNotes:\n- There are no physical units in this problem; all quantities are dimensionless.\n- No angles are involved.\n- The final output must be exactly one line, formatted as a single list of lists as described above, for the specified test suite values only.", "solution": "The problem is valid as it is self-contained, scientifically grounded in the principles of calculus and numerical analysis, and unambiguously stated. We are asked to derive numerical differentiation formulas from first principles, analyze their truncation error, and apply this analysis to a specific function and point.\n\n### Part 1: Derivation of Difference Schemes and Truncation Errors\n\nWe begin with the Taylor series expansion of a smooth function $f(x)$ around a point $x_0$. For a step size $h > 0$, the expansions for $f(x_0+h)$ and $f(x_0-h)$ are:\n$$f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + O(h^4)$$\n$$f(x_0-h) = f(x_0) - hf'(x_0) + \\frac{h^2}{2!}f''(x_0) - \\frac{h^3}{3!}f'''(x_0) + O(h^4)$$\n\nThe bias of an estimator is defined as $\\text{bias} = \\text{estimator} - f'(x_0)$. We seek the leading-order term of this bias, which is the leading-order truncation error.\n\n**Forward Difference Scheme**\nThe forward difference quotient is defined as $\\frac{f(x_0+h) - f(x_0)}{h}$. Rearranging the Taylor expansion for $f(x_0+h)$:\n$$f(x_0+h) - f(x_0) = hf'(x_0) + \\frac{h^2}{2}f''(x_0) + O(h^3)$$\nDividing by $h$:\n$$\\frac{f(x_0+h) - f(x_0)}{h} = f'(x_0) + \\frac{h}{2}f''(x_0) + O(h^2)$$\nThe bias is therefore:\n$$\\text{bias}_{\\text{fwd}} = \\left(\\frac{f(x_0+h) - f(x_0)}{h}\\right) - f'(x_0) = \\frac{h}{2}f''(x_0) + O(h^2)$$\nThe leading-order truncation error is $\\frac{h}{2}f''(x_0)$.\n\n**Backward Difference Scheme**\nThe backward difference quotient is $\\frac{f(x_0) - f(x_0-h)}{h}$. Rearranging the Taylor expansion for $f(x_0-h)$:\n$$f(x_0) - f(x_0-h) = hf'(x_0) - \\frac{h^2}{2}f''(x_0) + O(h^3)$$\nDividing by $h$:\n$$\\frac{f(x_0) - f(x_0-h)}{h} = f'(x_0) - \\frac{h}{2}f''(x_0) + O(h^2)$$\nThe bias is:\n$$\\text{bias}_{\\text{bwd}} = \\left(\\frac{f(x_0) - f(x_0-h)}{h}\\right) - f'(x_0) = -\\frac{h}{2}f''(x_0) + O(h^2)$$\nThe leading-order truncation error is $-\\frac{h}{2}f''(x_0)$.\n\n**Central Difference Scheme**\nTo derive the central difference scheme, we subtract the expansion for $f(x_0-h)$ from the expansion for $f(x_0+h)$:\n$$f(x_0+h) - f(x_0-h) = (f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\frac{h^3}{6}f'''(x_0)) - (f(x_0) - hf'(x_0) + \\frac{h^2}{2}f''(x_0) - \\frac{h^3}{6}f'''(x_0)) + O(h^5)$$\nTerms with even powers of $h$ cancel:\n$$f(x_0+h) - f(x_0-h) = 2hf'(x_0) + \\frac{2h^3}{6}f'''(x_0) + O(h^5) = 2hf'(x_0) + \\frac{h^3}{3}f'''(x_0) + O(h^5)$$\nRearranging for the estimator $\\frac{f(x_0+h) - f(x_0-h)}{2h}$:\n$$\\frac{f(x_0+h) - f(x_0-h)}{2h} = f'(x_0) + \\frac{h^2}{6}f'''(x_0) + O(h^4)$$\nThe bias is:\n$$\\text{bias}_{\\text{cen}} = \\left(\\frac{f(x_0+h) - f(x_0-h)}{2h}\\right) - f'(x_0) = \\frac{h^2}{6}f'''(x_0) + O(h^4)$$\nThe leading-order truncation error is $\\frac{h^2}{6}f'''(x_0)$.\n\n### Part 2: Specialization to the Logistic Function\n\nWe are given the function $f(x) = \\frac{1}{1 + e^{-x}}$ and the point of interest $x_0 = 0$. We must compute the first three derivatives and evaluate them at $x_0 = 0$.\n\nA computationally efficient way to find the derivatives is to use the property $f'(x) = f(x)(1-f(x))$.\n$f'(x) = \\frac{d}{dx}(1+e^{-x})^{-1} = -(1+e^{-x})^{-2}(-e^{-x}) = \\frac{e^{-x}}{(1+e^{-x})^2}$.\n$f(x)(1-f(x)) = \\frac{1}{1+e^{-x}}\\left(1 - \\frac{1}{1+e^{-x}}\\right) = \\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}} = \\frac{e^{-x}}{(1+e^{-x})^2}$. The property holds.\n\nNow we compute higher derivatives:\n$f''(x) = \\frac{d}{dx}[f'(x)] = \\frac{d}{dx}[f(x)(1-f(x))] = f'(x)(1-f(x)) + f(x)(-f'(x)) = f'(x)(1-2f(x))$.\n$f'''(x) = \\frac{d}{dx}[f''(x)] = \\frac{d}{dx}[f'(x)(1-2f(x))] = f''(x)(1-2f(x)) + f'(x)(-2f'(x)) = f''(x)(1-2f(x)) - 2(f'(x))^2$.\n\nNext, we evaluate these at $x_0 = 0$:\nAt $x_0=0$, $e^{-0}=1$, so $f(0) = \\frac{1}{1+1} = \\frac{1}{2}$.\n$f'(0) = f(0)(1-f(0)) = \\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{4}$. This is the exact value of the slope.\n$f''(0) = f'(0)(1-2f(0)) = \\frac{1}{4}(1-2(\\frac{1}{2})) = \\frac{1}{4}(0) = 0$. This confirms that $x_0 = 0$ is an inflection point, as stated.\n$f'''(0) = f''(0)(1-2f(0)) - 2(f'(0))^2 = (0)(1-2(\\frac{1}{2})) - 2(\\frac{1}{4})^2 = 0 - 2(\\frac{1}{16}) = -\\frac{1}{8}$.\n\nThe problem states that $x_0=0$ is an inflection point, which means the leading-order $O(h)$ error terms for the forward and backward schemes, which depend on $f''(x_0)$, vanish. Their accuracy becomes $O(h^2)$ at this specific point. The leading-order truncation error for the central scheme is determined by $f'''(0)$.\n\nThe predicted leading-order bias for the central scheme is given by the leading term in its error expansion:\n$$\\text{bias}_{\\text{cen,pred}} = \\frac{h^2}{6}f'''(0) = \\frac{h^2}{6}\\left(-\\frac{1}{8}\\right) = -\\frac{h^2}{48}$$\n\n### Part 3: Algorithmic Implementation\n\nThe program will implement the following logic:\n1. Define the logistic function $f(x)$.\n2. Set the exact derivative $f'(0) = 0.25$ and $f'''(0) = -0.125$.\n3. For each step size $h$ in the set $\\{2, 1, 0.5, 0.25\\}$:\n   a. Compute the three numerical estimates at $x_0=0$:\n      - Forward: $\\text{est}_{\\text{fwd}} = \\frac{f(h) - f(0)}{h}$\n      - Backward: $\\text{est}_{\\text{bwd}} = \\frac{f(0) - f(-h)}{h}$\n      - Central: $\\text{est}_{\\text{cen}} = \\frac{f(h) - f(-h)}{2h}$\n   b. Compute the empirical bias for each as $\\text{bias} = \\text{estimate} - 0.25$.\n   c. Compute the predicted leading-order bias for the central scheme using the derived formula: $\\text{bias}_{\\text{cen,pred}} = -\\frac{h^2}{48}$.\n   d. Store the results $[h, \\text{bias}_{\\text{fwd}}, \\text{bias}_{\\text{bwd}}, \\text{bias}_{\\text{cen}}, \\text{bias}_{\\text{cen,pred}}]$ for the current $h$.\n4. Format the collected results into a single string representing a list of lists and print it.\n\nIt is worth noting that for the function $f(x)=(1+e^{-x})^{-1}$, we have the symmetry property $f(x)+f(-x)=1$. At the evaluation point $x_0=0$, where $f(0)=0.5$, this leads to the forward, backward, and central difference quotients being numerically identical. Consequently, their empirical biases will also be identical.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem for the logistic function.\n\n    This function performs the following tasks:\n    1. Defines the logistic function and its exact derivative details at x=0.\n    2. Iterates through a set of step sizes h.\n    3. For each h, calculates the forward, backward, and central difference estimates\n       of the derivative at x=0.\n    4. Computes the empirical bias for each estimate.\n    5. Calculates the theoretically predicted leading-order bias for the central\n       difference scheme.\n    6. Formats and prints the results as a list of lists.\n    \"\"\"\n    \n    # Define the scalar function f(x)\n    def f(x: float) -> float:\n        return 1.0 / (1.0 + np.exp(-x))\n\n    # Test suite of step sizes\n    h_values = [2.0, 1.0, 0.5, 0.25]\n    \n    # Point of evaluation\n    x0 = 0.0\n    \n    # Analytically derived constants\n    f_prime_exact_at_0 = 0.25  # f'(0) = 1/4\n    f_triple_prime_at_0 = -0.125  # f'''(0) = -1/8\n    \n    all_results = []\n\n    for h in h_values:\n        # Evaluate the function at the required points on the grid\n        f_plus_h = f(x0 + h)\n        f_minus_h = f(x0 - h)\n        f_at_x0 = f(x0)\n\n        # Compute the three numerical slope estimates\n        est_fwd = (f_plus_h - f_at_x0) / h\n        est_bwd = (f_at_x0 - f_minus_h) / h\n        est_cen = (f_plus_h - f_minus_h) / (2.0 * h)\n        \n        # Compute the empirical bias for each estimate\n        bias_fwd = est_fwd - f_prime_exact_at_0\n        bias_bwd = est_bwd - f_prime_exact_at_0\n        bias_cen = est_cen - f_prime_exact_at_0\n        \n        # Compute the predicted leading-order bias for the central scheme\n        # The formula is (h^2 / 6) * f'''(0)\n        bias_cen_pred = (h**2 / 6.0) * f_triple_prime_at_0\n        \n        # Append the list of results for this h\n        all_results.append([h, bias_fwd, bias_bwd, bias_cen, bias_cen_pred])\n        \n    # Format the final output string as a list of lists without spaces\n    # Example: [[2.0,-0.0596...,-0.0596...,-0.0596...,-0.0833...],...]\n    output_str = f\"[{','.join([f'[{v[0]},{v[1]},{v[2]},{v[3]},{v[4]}]' for v in all_results])}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3132346"}, {"introduction": "A deep understanding of truncation error unlocks powerful techniques for improving accuracy, a central theme in computational science. This exercise introduces Richardson extrapolation, a clever method that combines two lower-order estimates to cancel the leading error term and produce a higher-order approximation. By applying this to the forward difference formula, you will learn how to turn an $\\mathcal{O}(h)$ method into an $\\mathcal{O}(h^2)$ method, a valuable and widely applicable skill. [@problem_id:3132379]", "problem": "You are tasked with deriving and implementing an algorithm that uses Richardson extrapolation to improve the accuracy of a forward difference approximation of a first derivative. Begin from the following foundational base: the Taylor series expansion of a sufficiently smooth function around a point and the definition of the derivative as a limit. Specifically, use the Taylor series expansion of a smooth function $f$ at $x$ to express $f(x+h)$ in terms of $f(x)$, $f'(x)$, $f''(x)$, and higher-order derivatives, and use the forward difference approximation of the derivative given by $D_h^{+} f(x) = \\dfrac{f(x+h) - f(x)}{h}$. From this base, reason about the truncation error of $D_h^{+} f(x)$ and construct a linear combination of two forward differences at step sizes $h$ and $h/2$ that cancels the leading-order truncation error term, resulting in an approximation whose error is of order $O(h^2)$.\n\nYour tasks are:\n- Use the Taylor series to identify the leading truncation error term of $D_h^{+} f(x)$.\n- Determine coefficients $\\alpha$ and $\\beta$ such that $\\alpha D_{h/2}^{+} f(x) + \\beta D_h^{+} f(x)$ is a consistent approximation to $f'(x)$ and has truncation error $O(h^2)$.\n- Implement a program that, for the test suite specified below, computes the Richardson extrapolated derivative approximation at each $(x,h)$ pair, compares it to the exact derivative, and returns the absolute error as a floating-point number.\n\nUse the test function $f(x) = \\cos x$ with the exact derivative $f'(x) = -\\sin x$. All angles must be in radians.\n\nTest suite parameter sets $(x,h)$ (radians):\n- Case $1$ (general case): $x = \\dfrac{\\pi}{3}$, $h = 0.2$.\n- Case $2$ (boundary-like behavior where the exact derivative is zero): $x = 0$, $h = 0.1$.\n- Case $3$ (edge case with extremely small step to probe round-off): $x = 1.0$, $h = 10^{-8}$.\n- Case $4$ (coarse step size to observe error behavior): $x = \\dfrac{\\pi}{2}$, $h = 0.5$.\n\nYour program must:\n- Implement $D_h^{+} f(x) = \\dfrac{f(x+h) - f(x)}{h}$ and compute the Richardson extrapolated approximation using your derived coefficients.\n- For each test case, compute the absolute error $\\left|\\text{approx} - f'(x)\\right|$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each floating-point number rounded to $10$ decimal places. For example, the format must be like \"[r1,r2,r3,r4]\" where each $r_i$ is a decimal number.\n\nNo user input or external files are permitted. The program must be self-contained and executable as-is.", "solution": "The problem requires the derivation and implementation of a Richardson extrapolation algorithm to improve the accuracy of the forward difference approximation for a first derivative. The process begins with a formal validation of the problem statement.\n\n### Step 1: Extract Givens\n- **Function**: a sufficiently smooth function, denoted as $f$.\n- **Test Function**: $f(x) = \\cos x$.\n- **Exact Derivative**: $f'(x) = -\\sin x$.\n- **Base Approximation**: Forward difference formula, $D_h^{+} f(x) = \\dfrac{f(x+h) - f(x)}{h}$.\n- **Foundation**: Taylor series expansion of $f(x+h)$ around $x$.\n- **Goal**: Find coefficients $\\alpha$ and $\\beta$ such that the linear combination $\\alpha D_{h/2}^{+} f(x) + \\beta D_h^{+} f(x)$ yields a new approximation with a truncation error of order $O(h^2)$.\n- **Task**: Implement the derived formula and compute the absolute error, $|\\text{approx} - f'(x)|$, for a given test suite.\n- **Constraints**: All angles are in radians.\n- **Test Suite**:\n    - Case $1$: $x = \\dfrac{\\pi}{3}$, $h = 0.2$.\n    - Case $2$: $x = 0$, $h = 0.1$.\n    - Case $3$: $x = 1.0$, $h = 10^{-8}$.\n    - Case $4$: $x = \\dfrac{\\pi}{2}$, $h = 0.5$.\n- **Output Format**: A single line string `\"[r1,r2,r3,r4]\"` where each $r_i$ is the absolute error for the corresponding test case, rounded to $10$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is based on fundamental and standard concepts of numerical analysis: Taylor series, finite difference approximations, and Richardson extrapolation. These are core, well-established principles in computational science and mathematics. The choice of $f(x) = \\cos x$ is appropriate as it is an infinitely differentiable ($C^\\infty$) function, meeting the \"sufficiently smooth\" criterion. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined. It specifies the initial formula, the theoretical tool (Taylor series), the objective (derive a higher-order formula), the test function, the exact test parameters, and the precise output format. A unique and stable solution exists for the derivation of coefficients and the subsequent computation.\n- **Objective**: The problem is stated in precise, formal mathematical language. It is free of ambiguity, subjectivity, or opinion-based claims.\n\nThe problem does not exhibit any of the invalidity flaws. It is a standard, formal problem in numerical methods, complete, consistent, and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution will be provided.\n\n### Derivation of the Richardson Extrapolation Formula\n\nOur goal is to improve the accuracy of the forward difference approximation of the first derivative. We begin with the Taylor series expansion of a smooth function $f$ for a point $x+h$ around $x$.\n\nThe Taylor series expansion is given by:\n$$\nf(x+h) = f(x) + h f'(x) + \\frac{h^2}{2!} f''(x) + \\frac{h^3}{3!} f'''(x) + O(h^4)\n$$\nwhere $f'(x)$, $f''(x)$, etc., denote the derivatives of $f$ evaluated at $x$.\n\nWe rearrange this expansion to solve for $f'(x)$:\n$$\nh f'(x) = f(x+h) - f(x) - \\frac{h^2}{2} f''(x) - \\frac{h^3}{6} f'''(x) - O(h^4)\n$$\nDividing by $h$ gives an expression for the exact derivative:\n$$\nf'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{h}{2} f''(x) - \\frac{h^2}{6} f'''(x) - O(h^3)\n$$\nThe term $\\frac{f(x+h) - f(x)}{h}$ is the forward difference approximation, which we denote as $D_h^{+} f(x)$. Therefore, we can express the approximation in terms of the true value and an error series:\n$$\nD_h^{+} f(x) = f'(x) + \\frac{h}{2} f''(x) + \\frac{h^2}{6} f'''(x) + O(h^3)\n$$\nThis equation shows that the forward difference approximation $D_h^{+} f(x)$ has a truncation error whose leading term is $\\frac{h}{2} f''(x)$, making the approximation first-order accurate, or $O(h)$.\n\nLet's denote the approximation with step size $h$ as $A(h)$. The true value we seek is $L = f'(x)$. Our error expansion is of the form $A(h) = L + c_1 h + c_2 h^2 + c_3 h^3 + \\dots$.\nFrom our derivation, $c_1 = \\frac{f''(x)}{2}$ and $c_2 = \\frac{f'''(x)}{6}$.\n\nSo, we have:\n$$\nD_h^{+} f(x) = f'(x) + \\frac{h}{2} f''(x) + O(h^2)\n\\quad \\quad (1)\n$$\nNow, we write the same expression for a different step size, specifically $h/2$:\n$$\nD_{h/2}^{+} f(x) = f'(x) + \\frac{(h/2)}{2} f''(x) + O((h/2)^2)\n$$\n$$\nD_{h/2}^{+} f(x) = f'(x) + \\frac{h}{4} f''(x) + O(h^2)\n\\quad \\quad (2)\n$$\nOur objective is to find a linear combination of equations $(1)$ and $(2)$ that eliminates the leading error term, which is the term proportional to $h f''(x)$.\nLet's multiply equation $(2)$ by $2$:\n$$\n2 D_{h/2}^{+} f(x) = 2 f'(x) + \\frac{h}{2} f''(x) + O(h^2)\n\\quad \\quad (3)\n$$\nNow, we subtract equation $(1)$ from equation $(3)$:\n$$\n2 D_{h/2}^{+} f(x) - D_h^{+} f(x) = (2 f'(x) - f'(x)) + \\left(\\frac{h}{2} f''(x) - \\frac{h}{2} f''(x)\\right) + O(h^2)\n$$\n$$\n2 D_{h/2}^{+} f(x) - D_h^{+} f(x) = f'(x) + O(h^2)\n$$\nThis gives us a new, more accurate approximation for $f'(x)$, which we can call $D_{extrap} f(x)$:\n$$\nD_{extrap} f(x) = 2 D_{h/2}^{+} f(x) - D_h^{+} f(x)\n$$\nThe error of this new approximation is of order $O(h^2)$, as the $O(h)$ term has been cancelled.\n\nThe problem asks for coefficients $\\alpha$ and $\\beta$ such that the new approximation is $\\alpha D_{h/2}^{+} f(x) + \\beta D_h^{+} f(x)$. By comparing this with our derived formula, we identify the coefficients:\n$$\n\\alpha = 2\n$$\n$$\n\\beta = -1\n$$\nFor the approximation to be consistent, the sum of the coefficients must be $1$. Here, $\\alpha + \\beta = 2 + (-1) = 1$, which confirms consistency.\n\nThe final formula to be implemented is:\n$$\nD_{extrap} f(x) = 2 \\left( \\frac{f(x+h/2) - f(x)}{h/2} \\right) - \\left( \\frac{f(x+h) - f(x)}{h} \\right)\n$$\nThis will be used to compute the approximate derivative for each test case. The absolute error is then $|\\text{approx} - \\text{exact}| = |D_{extrap} f(x) - f'(x)|$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies Richardson extrapolation to a forward difference formula\n    to calculate the first derivative of f(x) = cos(x) and computes the absolute error.\n    \"\"\"\n\n    # Define the test function and its exact derivative.\n    # All angles are in radians, which is the default for numpy's trig functions.\n    def f(x):\n        return np.cos(x)\n\n    def df_dx_exact(x):\n        return -np.sin(x)\n\n    # Define the forward difference approximation D_h^{+} f(x).\n    def forward_difference(func, x, h):\n        if h == 0:\n            raise ValueError(\"Step size h cannot be zero.\")\n        return (func(x + h) - func(x)) / h\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.pi / 3, 0.2),       # Case 1: General case\n        (0.0, 0.1),             # Case 2: Boundary-like behavior (derivative is zero)\n        (1.0, 1e-8),            # Case 3: Edge case with small step (probes round-off)\n        (np.pi / 2, 0.5),       # Case 4: Coarse step size\n    ]\n\n    results = []\n    for x, h in test_cases:\n        # Calculate the forward difference for step sizes h and h/2.\n        # This is A(h) in the derivation.\n        approx_h = forward_difference(f, x, h)\n        \n        # This is A(h/2) in the derivation.\n        approx_h_half = forward_difference(f, x, h / 2.0)\n\n        # Apply the Richardson extrapolation formula: 2 * A(h/2) - A(h).\n        # The coefficients are alpha=2 and beta=-1.\n        richardson_approx = 2.0 * approx_h_half - 1.0 * approx_h\n\n        # Get the exact value of the derivative.\n        exact_value = df_dx_exact(x)\n\n        # Compute the absolute error between the extrapolated value and the exact value.\n        absolute_error = np.abs(richardson_approx - exact_value)\n        \n        results.append(absolute_error)\n\n    # Format the final output string according to the specification.\n    # Each result is formatted to 10 decimal places.\n    output_str = f\"[{','.join([f'{r:.10f}' for r in results])}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3132379"}, {"introduction": "Beyond correcting errors in existing formulas, we can design new methods with higher intrinsic accuracy from the ground up. This advanced practice challenges you to act as a numerical analyst, using Taylor series and linear algebra to systematically derive high-order finite difference stencils for both interior and boundary points. By constructing and validating a fifth-order central difference formula, you will demystify how complex numerical methods are created and verified. [@problem_id:3132411]", "problem": "You are to design and implement a small computational laboratory to derive and validate high-order finite difference formulas for the first derivative based on Taylor series expansions. The task must be completed from first principles using Taylor expansions and linear constraints, without relying on pre-memorized formulas. The target function for validation is the sine function, which uses radians. All angles in this problem must be expressed in radians.\n\nStart from the following foundational base:\n\n- The Taylor series expansion of a sufficiently smooth function about a point is given by\n$$\nf(x_0 + k h) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x_0)}{n!} (k h)^n,\n$$\nwhere $h$ is a small step size and $k$ is an integer indexing stencil nodes.\n\n- A linear finite difference approximation to the first derivative at $x_0$ may be written as\n$$\n\\frac{1}{h} \\sum_{k \\in \\mathcal{K}} w_k f(x_0 + k h),\n$$\nfor a chosen set of offsets $\\mathcal{K}$ and weights $w_k$, where the weights are determined by matching the Taylor expansion coefficients of $f'(x_0)$ and canceling lower-order truncation terms.\n\nYour tasks are:\n\n1. Using only the Taylor series expansion and linear algebraic consistency conditions, derive the five-point, order $\\mathcal{O}(h^4)$, symmetric central-difference stencil for $f'(x_0)$ built on the offsets $\\mathcal{K} = \\{-2,-1,0,1,2\\}$. Do not assume any weights; determine them from the series-matching conditions implied by the expansion and the requirement that the leading truncation error be proportional to $h^4$.\n\n2. Using the same approach, derive the three-point, order $\\mathcal{O}(h^2)$, one-sided forward-difference stencil for $f'(x_0)$ at the left boundary using offsets $\\mathcal{K} = \\{0,1,2\\}$, and the three-point, order $\\mathcal{O}(h^2)$, one-sided backward-difference stencil at the right boundary using offsets $\\mathcal{K} = \\{0,-1,-2\\}$.\n\n3. Implement the three derived formulas as functions in code. Validate them on the test function\n$$\nf(x) = \\sin(x),\n$$\nwith exact derivative\n$$\nf'(x) = \\cos(x),\n$$\nwhere $x$ is in radians.\n\n4. For each formula, estimate the observed order of accuracy by computing the absolute error over a diminishing sequence of step sizes $h$ and fitting a straight line to the data $(\\log h, \\log \\text{error})$ by ordinary least squares. The slope of this line is the observed order of accuracy $p$. Use the natural logarithm.\n\n5. Use the following test suite, which covers interior and boundary cases, as well as a range of step sizes that avoid out-of-domain accesses:\n- Interior point for the five-point central stencil: $x_0 = 1.3$. Use step sizes\n$$\nh \\in \\{0.2,\\; 0.1,\\; 0.05,\\; 0.025,\\; 0.0125,\\; 0.00625\\}.\n$$\n- Left boundary for the three-point forward stencil: $x_0 = 0$. Use the same set of $h$ values.\n- Right boundary for the three-point backward stencil: $x_0 = 2\\pi$. Use the same set of $h$ values.\n\nFor each case, compute the absolute error $|D_h f(x_0) - f'(x_0)|$ for every $h$ listed above, then estimate the observed order $p$ by the slope of the best-fit line in the $(\\log h, \\log \\text{error})$ plane.\n\nFinal Output Format:\nYour program should produce a single line of output containing the three observed orders, in the following order:\n- Five-point central stencil at $x_0 = 1.3$,\n- Three-point forward stencil at $x_0 = 0$,\n- Three-point backward stencil at $x_0 = 2\\pi$.\n\nThe output must be a comma-separated list enclosed in square brackets, with each value rounded to three decimal places, for example:\n$$\n[\\text{central},\\text{forward},\\text{backward}]\n$$\nAll computations must treat angles in radians. No user input should be required; all parameters are fixed as stated above.", "solution": "The problem requires the derivation of three finite difference formulas for the first derivative from first principles using Taylor series expansions, followed by a numerical validation of their theoretical orders of accuracy. The validation involves applying these formulas to the function $f(x) = \\sin(x)$ and determining the observed order of accuracy by a log-log analysis of the error as a function of the step size $h$.\n\nA general finite difference approximation for the first derivative $f'(x_0)$ is a linear combination of function values at stencil points $x_0 + k h$ for a set of integer offsets $\\mathcal{K}$:\n$$\nf'(x_0) \\approx \\frac{1}{h} \\sum_{k \\in \\mathcal{K}} w_k f(x_0 + k h)\n$$\nThe weights $w_k$ are determined by substituting the Taylor series expansion of $f(x_0 + k h)$ around $x_0$,\n$$\nf(x_0 + k h) = f(x_0) + (kh)f'(x_0) + \\frac{(kh)^2}{2!}f''(x_0) + \\frac{(kh)^3}{3!}f'''(x_0) + \\dots = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x_0)}{n!} (k h)^n\n$$\ninto the approximation formula. By collecting terms for each derivative $f^{(n)}(x_0)$, we obtain a polynomial in $h$. We then set up a system of linear equations for the weights $w_k$ by requiring that the coefficients of lower-order terms vanish and that the coefficient of $f'(x_0)$ is unity.\n\n**1. Derivation of the Five-Point Central-Difference Formula ($\\mathcal{O}(h^4)$)**\n\nFor this formula, the stencil is symmetric with offsets $\\mathcal{K} = \\{-2, -1, 0, 1, 2\\}$. The approximation is\n$$\nh f'(x_0) + \\mathcal{O}(h^5) \\approx \\sum_{k=-2}^{2} w_k f(x_0 + k h)\n$$\nSubstituting the Taylor series for each term $f(x_0 + k h)$ yields:\n$$\n\\sum_{k=-2}^{2} w_k f(x_0 + k h) = f(x_0) \\sum_{k=-2}^{2} w_k + h f'(x_0) \\sum_{k=-2}^{2} k w_k + \\frac{h^2 f''(x_0)}{2} \\sum_{k=-2}^{2} k^2 w_k + \\dots\n$$\nTo obtain an approximation for $f'(x_0)$ with a truncation error of $\\mathcal{O}(h^4)$, we require the formula to be exact for polynomials up to degree $4$. This leads to the following system of linear equations for the weights $w_k$:\n\\begin{enumerate}\n    \\item Coeff. of $f(x_0)$: $\\sum_{k=-2}^{2} w_k = w_{-2} + w_{-1} + w_0 + w_1 + w_2 = 0$\n    \\item Coeff. of $h f'(x_0)$: $\\sum_{k=-2}^{2} k w_k = -2w_{-2} - w_{-1} + w_1 + 2w_2 = 1$\n    \\item Coeff. of $h^2 f''(x_0)$: $\\sum_{k=-2}^{2} k^2 w_k = 4w_{-2} + w_{-1} + w_1 + 4w_2 = 0$\n    \\item Coeff. of $h^3 f'''(x_0)$: $\\sum_{k=-2}^{2} k^3 w_k = -8w_{-2} - w_{-1} + w_1 + 8w_2 = 0$\n    \\item Coeff. of $h^4 f^{(4)}(x_0)$: $\\sum_{k=-2}^{2} k^4 w_k = 16w_{-2} + w_{-1} + w_1 + 16w_2 = 0$\n\\end{enumerate}\nFor a symmetric stencil, we expect antisymmetric weights for the first derivative, i.e., $w_{-k} = -w_k$. This implies $w_0 = -w_0 \\implies w_0 = 0$.\nApplying this property simplifies the system:\n\\begin{itemize}\n    \\item Eq. 1: $(-w_2) + (-w_1) + 0 + w_1 + w_2 = 0$. This is automatically satisfied.\n    \\item Eq. 3: $4(-w_2) + (-w_1) + w_1 + 4w_2 = 0$. This is also automatically satisfied.\n    \\item Eq. 5: $16(-w_2) + (-w_1) + w_1 + 16w_2 = 0$. This is also automatically satisfied.\n\\end{itemize}\nThe remaining non-trivial equations are:\n\\begin{itemize}\n    \\item From Eq. 2: $-2(-w_2) - (-w_1) + w_1 + 2w_2 = 2w_2 + w_1 + w_1 + 2w_2 = 2w_1 + 4w_2 = 1$.\n    \\item From Eq. 4: $-8(-w_2) - (-w_1) + w_1 + 8w_2 = 8w_2 + w_1 + w_1 + 8w_2 = 2w_1 + 16w_2 = 0$.\n\\end{itemize}\nWe now solve this $2 \\times 2$ system:\n\\begin{align*}\n    w_1 + 2w_2 &= 1/2 \\\\\n    w_1 + 8w_2 &= 0\n\\end{align*}\nSubtracting the first equation from the second gives $6w_2 = -1/2$, so $w_2 = -1/12$.\nSubstituting this into the second equation gives $w_1 = -8w_2 = -8(-1/12) = 8/12 = 2/3$.\nThe weights are: $w_0=0$, $w_1=2/3$, $w_2=-1/12$, and by antisymmetry, $w_{-1}=-2/3$, $w_{-2}=1/12$.\nThe formula is:\n$$\nf'(x_0) \\approx \\frac{1}{12h} [f(x_0-2h) - 8f(x_0-h) + 8f(x_0+h) - f(x_0+2h)]\n$$\nThe leading error term involves $f^{(5)}(x_0)$ and is proportional to $h^4$.\n\n**2. Derivation of the Three-Point Forward-Difference Formula ($\\mathcal{O}(h^2)$)**\n\nFor this one-sided formula, the stencil offsets are $\\mathcal{K} = \\{0, 1, 2\\}$. We need to cancel terms up to $f''(x_0)$ to achieve an $\\mathcal{O}(h^2)$ error.\nThe approximation is $h f'(x_0) + \\mathcal{O}(h^3) \\approx w_0 f(x_0) + w_1 f(x_0+h) + w_2 f(x_0+2h)$.\nThis leads to the system:\n\\begin{enumerate}\n    \\item Coeff. of $f(x_0)$: $w_0 + w_1 + w_2 = 0$\n    \\item Coeff. of $h f'(x_0)$: $w_1 + 2w_2 = 1$\n    \\item Coeff. of $h^2 f''(x_0)$: $\\frac{1}{2}w_1 + \\frac{4}{2}w_2 = 0 \\implies w_1 + 4w_2 = 0$\n\\end{enumerate}\nFrom (3), $w_1 = -4w_2$. Substituting into (2) gives $-4w_2 + 2w_2 = 1 \\implies -2w_2=1 \\implies w_2 = -1/2$.\nThen, $w_1 = -4(-1/2) = 2$.\nFinally, from (1), $w_0 + 2 - 1/2 = 0 \\implies w_0 = -3/2$.\nThe weights are: $w_0=-3/2$, $w_1=2$, $w_2=-1/2$.\nThe formula is:\n$$\nf'(x_0) \\approx \\frac{1}{h} \\left[-\\frac{3}{2}f(x_0) + 2f(x_0+h) - \\frac{1}{2}f(x_0+2h)\\right] = \\frac{-3f(x_0) + 4f(x_0+h) - f(x_0+2h)}{2h}\n$$\n\n**3. Derivation of the Three-Point Backward-Difference Formula ($\\mathcal{O}(h^2)$)**\n\nFor this one-sided formula, the offsets are $\\mathcal{K} = \\{0, -1, -2\\}$. The derivation is analogous to the forward-difference case. The system of equations is:\n\\begin{enumerate}\n    \\item Coeff. of $f(x_0)$: $w_0 + w_{-1} + w_{-2} = 0$\n    \\item Coeff. of $h f'(x_0)$: $-w_{-1} - 2w_{-2} = 1$\n    \\item Coeff. of $h^2 f''(x_0)$: $\\frac{1}{2}w_{-1} + \\frac{4}{2}w_{-2} = 0 \\implies w_{-1} + 4w_{-2} = 0$\n\\end{enumerate}\nFrom (3), $w_{-1} = -4w_{-2}$. Substituting into (2) gives $-(-4w_{-2}) - 2w_{-2} = 1 \\implies 2w_{-2}=1 \\implies w_{-2} = 1/2$.\nThen, $w_{-1} = -4(1/2) = -2$.\nFinally, from (1), $w_0 - 2 + 1/2 = 0 \\implies w_0 = 3/2$.\nThe weights are: $w_0=3/2$, $w_{-1}=-2$, $w_{-2}=1/2$.\nThe formula is:\n$$\nf'(x_0) \\approx \\frac{1}{h} \\left[\\frac{3}{2}f(x_0) - 2f(x_0-h) + \\frac{1}{2}f(x_0-2h)\\right] = \\frac{3f(x_0) - 4f(x_0-h) + f(x_0-2h)}{2h}\n$$\n\n**4. Validation of Order of Accuracy**\n\nThe order of accuracy, $p$, of a numerical method describes how the error $E$ behaves as the step size $h$ approaches zero. For a method of order $p$, the error is expected to satisfy $E(h) \\approx C h^p$ for some constant $C$ and sufficiently small $h$. Taking the natural logarithm of this relationship gives:\n$$\n\\ln(E) \\approx \\ln(C) + p \\ln(h)\n$$\nThis equation is of the form $y = m x + c$, where $y = \\ln(E)$, $x = \\ln(h)$, the slope is $m=p$, and the intercept is $c=\\ln(C)$. To numerically estimate $p$, we compute the absolute error $E_i = |D_{h_i} f(x_0) - f'(x_0)|$ for a sequence of decreasing step sizes $h_i$. We then perform an ordinary least squares linear regression on the data points $(\\ln(h_i), \\ln(E_i))$. The slope of the resulting best-fit line is the observed order of accuracy $p$. This procedure will be implemented for the three derived formulas using the specified test function $f(x) = \\sin(x)$ and its exact derivative $f'(x) = \\cos(x)$, for the given points $x_0$ and step sizes $h$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Derives, implements, and validates finite difference formulas for the first derivative.\n    \"\"\"\n    \n    # Define the test function and its exact derivative (angles in radians)\n    f = np.sin\n    df_exact_func = np.cos\n    \n    # Define the sequence of step sizes\n    h_vals = np.array([0.2, 0.1, 0.05, 0.025, 0.0125, 0.00625])\n\n    # --- Finite Difference Formula Implementations ---\n    \n    def five_point_central(func, x0, h):\n        \"\"\"\n        Computes the O(h^4) five-point central difference approximation for f'(x0).\n        f'(x0) ~= (f(x-2h) - 8f(x-h) + 8f(x+h) - f(x+2h)) / (12h)\n        \"\"\"\n        f_m2 = func(x0 - 2 * h)\n        f_m1 = func(x0 - 1 * h)\n        f_p1 = func(x0 + 1 * h)\n        f_p2 = func(x0 + 2 * h)\n        \n        # Weights: w_-2=1/12, w_-1=-8/12, w_1=8/12, w_2=-1/12\n        return (f_m2 - 8 * f_m1 + 8 * f_p1 - f_p2) / (12 * h)\n\n    def three_point_forward(func, x0, h):\n        \"\"\"\n        Computes the O(h^2) three-point forward difference approximation for f'(x0).\n        f'(x0) ~= (-3f(x) + 4f(x+h) - f(x+2h)) / (2h)\n        \"\"\"\n        f0 = func(x0)\n        f1 = func(x0 + h)\n        f2 = func(x0 + 2 * h)\n        \n        # Weights: w_0=-3/2, w_1=4/2, w_2=-1/2\n        return (-3 * f0 + 4 * f1 - f2) / (2 * h)\n\n    def three_point_backward(func, x0, h):\n        \"\"\"\n        Computes the O(h^2) three-point backward difference approximation for f'(x0).\n        f'(x0) ~= (3f(x) - 4f(x-h) + f(x-2h)) / (2h)\n        \"\"\"\n        f0 = func(x0)\n        fm1 = func(x0 - h)\n        fm2 = func(x0 - 2 * h)\n        \n        # Weights: w_0=3/2, w_-1=-4/2, w_-2=1/2\n        return (3 * f0 - 4 * fm1 + fm2) / (2 * h)\n\n    # --- Test Suite Definition ---\n\n    test_cases = [\n        {\n            \"name\": \"Five-point central\",\n            \"x0\": 1.3,\n            \"formula\": five_point_central,\n        },\n        {\n            \"name\": \"Three-point forward\",\n            \"x0\": 0.0,\n            \"formula\": three_point_forward,\n        },\n        {\n            \"name\": \"Three-point backward\",\n            \"x0\": 2 * np.pi,\n            \"formula\": three_point_backward,\n        },\n    ]\n\n    observed_orders = []\n\n    for case in test_cases:\n        x0 = case[\"x0\"]\n        formula_func = case[\"formula\"]\n        \n        # Calculate the exact derivative at x0\n        df_true = df_exact_func(x0)\n        \n        # Compute absolute errors for the sequence of h values\n        errors = []\n        for h in h_vals:\n            df_approx = formula_func(f, x0, h)\n            error = np.abs(df_approx - df_true)\n            errors.append(error)\n        \n        errors = np.array(errors)\n        \n        # Prepare data for linear regression: (log(h), log(error))\n        log_h = np.log(h_vals)\n        log_error = np.log(errors)\n        \n        # Perform ordinary least squares regression\n        # The slope of the line is the observed order of accuracy\n        regression_result = linregress(log_h, log_error)\n        observed_order = regression_result.slope\n        \n        observed_orders.append(observed_order)\n\n    # Format the results for output\n    formatted_results = [\"{:.3f}\".format(p) for p in observed_orders]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3132411"}]}