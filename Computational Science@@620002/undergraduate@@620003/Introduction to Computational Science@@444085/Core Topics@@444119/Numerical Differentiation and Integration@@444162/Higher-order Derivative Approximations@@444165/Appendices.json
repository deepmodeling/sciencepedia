{"hands_on_practices": [{"introduction": "This practice goes to the heart of creating numerical stencils. Instead of using a pre-packaged formula, you will derive a high-order approximation for a third derivative from scratch using Taylor series expansions. This exercise reinforces the theoretical foundation of finite differences and demonstrates how accuracy is systematically improved by incorporating more stencil points to cancel out lower-order error terms [@problem_id:3238868].", "problem": "Let $f$ be a scalar function with at least $7$ continuous derivatives in a neighborhood of a point $x \\in \\mathbb{R}$. Consider a uniform grid with spacing $h > 0$. Starting only from Taylorâ€™s theorem with remainder about the point $x$, construct a central finite-difference approximation for the third derivative $f^{(3)}(x)$ whose truncation error is of order $h^{4}$. Use a symmetric stencil that samples $f$ at the points $\\{x \\pm h, x \\pm 2h, x \\pm 3h\\}$ and determine the coefficients by enforcing the required order conditions that follow from the Taylor expansions.\n\nYour final answer must be a single closed-form analytic expression for the stencil in terms of the samples $f(x \\pm h)$, $f(x \\pm 2h)$, and $f(x \\pm 3h)$ and the grid spacing $h$. Do not include an error term in the final answer. No rounding is required.", "solution": "The user wants to construct a central finite-difference approximation for the third derivative $f^{(3)}(x)$ with a truncation error of order $O(h^4)$, using a symmetric stencil on the points $\\{x \\pm h, x \\pm 2h, x \\pm 3h\\}$. The derivation must be based on Taylor's theorem.\n\nFirst, we perform a validation of the problem statement.\n\n### Step 1: Extract Givens\n- The function $f$ is a scalar function with at least $7$ continuous derivatives near $x \\in \\mathbb{R}$.\n- The grid is uniform with spacing $h > 0$.\n- The target is an approximation for the third derivative, $f^{(3)}(x)$.\n- The approximation method is a central finite-difference scheme.\n- The required truncation error is of order $O(h^4)$.\n- The stencil is symmetric and uses the sample points $\\{x \\pm h, x \\pm 2h, x \\pm 3h\\}$.\n- The derivation must start from Taylor's theorem.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard exercise in numerical analysis, concerning the derivation of finite-difference formulas. All concepts are well-established within mathematics and scientific computing. The requirement of $f$ having at least $7$ continuous derivatives is appropriate for an approximation with $O(h^4)$ error for the third derivative, as the leading error term is expected to involve $f^{(3+4)}(x) = f^{(7)}(x)$.\n- **Well-Posed**: The problem is well-posed. It specifies the function's smoothness, the points in the stencil, the desired derivative, the type of scheme (central), and the order of accuracy. This information is sufficient to uniquely determine the coefficients of the finite-difference formula.\n- **Objective**: The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, and objective. We may proceed with the solution.\n\nWe seek a linear combination of the function values at the specified stencil points that approximates $f^{(3)}(x)$. For a central difference approximation of an odd-order derivative, the stencil must be anti-symmetric. That is, the coefficient of $f(x+kh)$ must be the negative of the coefficient of $f(x-kh)$. The point $f(x)$ itself is not included. The general form of the approximation is:\n$$\nf^{(3)}(x) \\approx D_h[f](x) = c_1(f(x+h) - f(x-h)) + c_2(f(x+2h) - f(x-2h)) + c_3(f(x+3h) - f(x-3h))\n$$\nThe coefficients $c_1$, $c_2$, and $c_3$ must be determined. We use Taylor's theorem to expand the function values around the point $x$. The function $f$ is assumed to have at least $7$ continuous derivatives, so we can write the Taylor series with a remainder term. For an integer $k$, the expansions are:\n$$\nf(x+kh) = f(x) + (kh)f'(x) + \\frac{(kh)^2}{2!}f''(x) + \\frac{(kh)^3}{3!}f^{(3)}(x) + \\frac{(kh)^4}{4!}f^{(4)}(x) + \\frac{(kh)^5}{5!}f^{(5)}(x) + \\frac{(kh)^6}{6!}f^{(6)}(x) + \\frac{(kh)^7}{7!}f^{(7)}(x) + O(h^8)\n$$\n$$\nf(x-kh) = f(x) - (kh)f'(x) + \\frac{(kh)^2}{2!}f''(x) - \\frac{(kh)^3}{3!}f^{(3)}(x) + \\frac{(kh)^4}{4!}f^{(4)}(x) - \\frac{(kh)^5}{5!}f^{(5)}(x) + \\frac{(kh)^6}{6!}f^{(6)}(x) - \\frac{(kh)^7}{7!}f^{(7)}(x) + O(h^8)\n$$\nBy subtracting the second expansion from the first, we obtain an expression for the difference terms:\n$$\nf(x+kh) - f(x-kh) = 2(kh)f'(x) + 2\\frac{(kh)^3}{3!}f^{(3)}(x) + 2\\frac{(kh)^5}{5!}f^{(5)}(x) + 2\\frac{(kh)^7}{7!}f^{(7)}(x) + O(h^9)\n$$\nSubstituting this into the general form of our approximation $D_h[f](x)$:\n\\begin{align*}\nD_h[f](x) = & c_1 \\left( 2h f'(x) + \\frac{2h^3}{6}f^{(3)}(x) + \\frac{2h^5}{120}f^{(5)}(x) + \\frac{2h^7}{5040}f^{(7)}(x) + \\dots \\right) \\\\\n+ & c_2 \\left( 2(2h)f'(x) + \\frac{2(2h)^3}{6}f^{(3)}(x) + \\frac{2(2h)^5}{120}f^{(5)}(x) + \\frac{2(2h)^7}{5040}f^{(7)}(x) + \\dots \\right) \\\\\n+ & c_3 \\left( 2(3h)f'(x) + \\frac{2(3h)^3}{6}f^{(3)}(x) + \\frac{2(3h)^5}{120}f^{(5)}(x) + \\frac{2(3h)^7}{5040}f^{(7)}(x) + \\dots \\right)\n\\end{align*}\nNow, we group the terms by the derivatives of $f(x)$:\n\\begin{align*}\nD_h[f](x) = & f'(x) \\cdot \\left( 2h(c_1 + 2c_2 + 3c_3) \\right) \\\\\n+ & f^{(3)}(x) \\cdot \\left( \\frac{h^3}{3}(c_1 + 8c_2 + 27c_3) \\right) \\\\\n+ & f^{(5)}(x) \\cdot \\left( \\frac{h^5}{60}(c_1 + 32c_2 + 243c_3) \\right) \\\\\n+ & f^{(7)}(x) \\cdot \\left( \\frac{h^7}{2520}(c_1 + 128c_2 + 2187c_3) \\right) + \\dots\n\\end{align*}\nWe want the approximation $D_h[f](x)$ to be equal to $f^{(3)}(x)$ with a truncation error of order $O(h^4)$. This requires setting the coefficients of the derivative terms in the expansion of $D_h[f](x)$ to match those of $f^{(3)}(x)$. Let's define the approximation as\n$$\nD_h[f](x) = \\frac{a_1(f(x+h) - f(x-h)) + a_2(f(x+2h) - f(x-2h)) + a_3(f(x+3h) - f(x-3h))}{h^3}\n$$\nThen the expansion is:\n\\begin{align*}\nD_h[f](x) = & \\frac{1}{h^3} \\left[ f'(x) \\cdot 2h \\sum_{k=1}^3 k a_k + f^{(3)}(x) \\cdot \\frac{h^3}{3} \\sum_{k=1}^3 k^3 a_k + f^{(5)}(x) \\cdot \\frac{h^5}{60} \\sum_{k=1}^3 k^5 a_k + \\dots \\right] \\\\\n= & f'(x) \\cdot \\frac{2}{h^2} \\sum_{k=1}^3 k a_k + f^{(3)}(x) \\cdot \\frac{1}{3} \\sum_{k=1}^3 k^3 a_k + f^{(5)}(x) \\cdot \\frac{h^2}{60} \\sum_{k=1}^3 k^5 a_k + O(h^4)\n\\end{align*}\nFor this to approximate $f^{(3)}(x)$, we require:\n1. Coeff of $f'(x)$ must be $0$: $a_1 + 2a_2 + 3a_3 = 0$.\n2. Coeff of $f^{(3)}(x)$ must be $1$: $\\frac{1}{3}(a_1 + 8a_2 + 27a_3) = 1 \\implies a_1 + 8a_2 + 27a_3 = 3$.\n3. For an error of $O(h^4)$, the coefficient of the leading error term, which is $O(h^2)$, must be $0$: $a_1 + 32a_2 + 243a_3 = 0$.\n\nThis gives a system of three linear equations for the coefficients $a_1, a_2, a_3$:\n$$\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 8 & 27 \\\\ 1 & 32 & 243 \\end{pmatrix}\n\\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix}\n$$\nWe solve this system using row reduction on the augmented matrix:\n$$\n\\left[ \\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n1 & 8 & 27 & 3 \\\\\n1 & 32 & 243 & 0\n\\end{array} \\right]\n\\xrightarrow[R_3 \\to R_3 - R_1]{R_2 \\to R_2 - R_1}\n\\left[ \\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n0 & 6 & 24 & 3 \\\\\n0 & 30 & 240 & 0\n\\end{array} \\right]\n$$\nSimplifying the second row by dividing by $3$ and the third row by dividing by $30$:\n$$\n\\left[ \\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n0 & 2 & 8 & 1 \\\\\n0 & 1 & 8 & 0\n\\end{array} \\right]\n$$\nFrom the last row, we have $a_2 + 8a_3 = 0$, so $a_2 = -8a_3$.\nSubstituting this into the second row: $2(-8a_3) + 8a_3 = 1 \\implies -16a_3 + 8a_3 = 1 \\implies -8a_3 = 1$, which gives $a_3 = -1/8$.\nNow we find $a_2$: $a_2 = -8(-1/8) = 1$.\nFinally, from the first row: $a_1 + 2a_2 + 3a_3 = 0 \\implies a_1 = -2a_2 - 3a_3$.\n$a_1 = -2(1) - 3(-1/8) = -2 + 3/8 = -16/8 + 3/8 = -13/8$.\n\nThe coefficients are $a_1 = -13/8$, $a_2 = 1$, and $a_3 = -1/8$.\nSubstituting these back into the formula for $D_h[f](x)$:\n$$\nf^{(3)}(x) \\approx \\frac{1}{h^3} \\left[ -\\frac{13}{8}(f(x+h) - f(x-h)) + 1(f(x+2h) - f(x-2h)) - \\frac{1}{8}(f(x+3h) - f(x-3h)) \\right]\n$$\nTo obtain a cleaner expression, we factor out $1/8$:\n$$\nf^{(3)}(x) \\approx \\frac{1}{8h^3} \\left[ -13(f(x+h) - f(x-h)) + 8(f(x+2h) - f(x-2h)) - (f(x+3h) - f(x-3h)) \\right]\n$$\nExpanding and collecting terms according to their displacement from $x$:\n$$\nf^{(3)}(x) \\approx \\frac{1}{8h^3} [-f(x+3h) + 8f(x+2h) - 13f(x+h) + 13f(x-h) - 8f(x-2h) + f(x-3h)]\n$$\nThis is the desired finite-difference formula. The leading error term is given by the coefficient of $f^{(7)}(x)$ in the expansion of $D_h[f](x)$, which is $O(h^4)$, as required.\nThe error term is $E(h) = f^{(7)}(x) \\cdot \\frac{h^4}{2520} \\sum_{k=1}^3 k^7 a_k = \\frac{h^4 f^{(7)}(x)}{2520} (1^7 a_1 + 2^7 a_2 + 3^7 a_3) = \\frac{h^4 f^{(7)}(x)}{2520} (-\\frac{13}{8} + 128 - \\frac{2187}{8}) = \\frac{h^4 f^{(7)}(x)}{2520} (\\frac{-13+1024-2187}{8}) = \\frac{-1176}{20160}h^4 f^{(7)}(x) = -\\frac{7}{120}h^4 f^{(7)}(x)$. The error is indeed $O(h^4)$.\nThe final answer is the approximation formula itself.", "answer": "$$\\boxed{\\frac{-f(x+3h) + 8 f(x+2h) - 13 f(x+h) + 13 f(x-h) - 8 f(x-2h) + f(x-3h)}{8h^3}}$$", "id": "3238868"}, {"introduction": "Theory is essential, but verification is a cornerstone of computational science. This practice moves from analytical derivation to computational validation by asking you to implement several finite difference stencils and numerically measure their convergence rates. By comparing the observed order of accuracy with the theoretical prediction for a known function, you will gain a tangible understanding of what truncation error means and how to confirm your code is behaving as expected [@problem_id:3238903].", "problem": "Consider the function $f(x) = \\exp(-x^2)$. Your task is to approximate higher derivatives using centered finite difference stencils, derive the truncation error order from first principles, and verify the observed convergence rates against analytic derivatives.\n\nYou must proceed from the following foundational base:\n- Use Taylor series expansions of $f(x \\pm kh)$ about a point $x = x_0$, where $k \\in \\mathbb{Z}$ and $h > 0$ is small, to construct consistent centered finite difference approximations for derivatives. Specifically, use the Taylor expansion\n$$\nf(x_0 \\pm kh) = \\sum_{n=0}^{\\infty} \\frac{(\\pm kh)^n}{n!} f^{(n)}(x_0).\n$$\n- Use the definition of truncation error order: if an approximation $A_h$ to a quantity $Q$ satisfies $A_h = Q + C h^p + \\mathcal{O}(h^{p+1})$ for some constant $C \\neq 0$, then we say the approximation is of order $p$.\n- Use the definition of observed order between two step sizes $h_1$ and $h_2$ for errors $E(h_1)$ and $E(h_2)$ as\n$$\np_{\\mathrm{obs}} = \\frac{\\log\\big(E(h_1)/E(h_2)\\big)}{\\log(h_1/h_2)}.\n$$\n\nTasks:\n1. Derive symbolic expressions for the analytic second and fourth derivatives $f^{(2)}(x)$ and $f^{(4)}(x)$ for $f(x) = \\exp(-x^2)$ by direct differentiation.\n2. Using Taylor series, derive centered finite difference stencils and their leading truncation errors for:\n   - the standard centered $3$-point approximation to $f^{(2)}(x_0)$,\n   - the centered $5$-point approximation to $f^{(2)}(x_0)$ with higher accuracy than the $3$-point stencil,\n   - the centered $5$-point approximation to $f^{(4)}(x_0)$.\n   Do not assume any pre-known coefficients; derive them by matching Taylor series terms at $x_0$.\n3. Implement a program that evaluates the approximations at specific points and verifies the convergence order by computing the error against the analytic derivatives from Task $1$ and the observed orders from the error ratios as $h$ halves.\n\nTest suite:\n- Use the grid spacing sequence $h_k = 0.2 \\cdot 2^{-k}$ for $k \\in \\{0,1,2,3,4,5\\}$.\n- For each case below, compute the error $E(h_k)$ as the absolute difference between the numerical approximation and the analytic derivative at $x_0$, then compute the observed orders $p_k$ for $k \\in \\{1,2,3,4,5\\}$ using successive pairs $(h_{k-1}, h_k)$.\n- For robustness, define the reported observed order for each case as the median of the last three values $\\{p_3, p_4, p_5\\}$.\n- Verify the following four cases with their expected orders and points:\n  - Case A: centered $3$-point stencil for $f^{(2)}(x_0)$ at $x_0 = 0.7$, expected order $2$.\n  - Case B: centered $5$-point stencil for $f^{(2)}(x_0)$ at $x_0 = 1.3$, expected order $4$.\n  - Case C: centered $5$-point stencil for $f^{(4)}(x_0)$ at $x_0 = 0.0$, expected order $2$.\n  - Case D (edge magnitude case): centered $5$-point stencil for $f^{(2)}(x_0)$ at $x_0 = 2.0$, expected order $4$.\n\nPass-fail criterion for each case:\n- Let $\\tilde{p}$ be the reported observed order (median of $\\{p_3, p_4, p_5\\}$). The case passes if $|\\tilde{p} - p_{\\mathrm{expected}}| \\le 0.3$ and fails otherwise, where $p_{\\mathrm{expected}}$ is the expected order for that case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D], where each entry is the boolean result for that case, for example, \"[True,True,False,True]\".\n- No user input is required, and no physical units or angle units apply in this problem. All numerical values are dimensionless.", "solution": "The problem is assessed to be valid as it is scientifically grounded in numerical analysis, well-posed with a clear objective, and free from inconsistencies or ambiguities. We proceed with a complete solution.\n\nThe solution is presented in three parts, corresponding to the tasks outlined in the problem statement. First, we derive the analytic expressions for the required derivatives. Second, we derive the finite difference stencils and their truncation errors using Taylor series. Third, we outline the numerical verification procedure that will be implemented in the final code.\n\n### Part 1: Analytic Derivatives\n\nWe are given the function $f(x) = \\exp(-x^2)$. We will compute its second and fourth derivatives, $f^{(2)}(x)$ and $f^{(4)}(x)$, by direct differentiation. Let us denote derivatives with respect to $x$ using prime notation for brevity in the intermediate steps, i.e., $f'(x)$, $f''(x)$, etc.\n\nThe first derivative is:\n$$\nf'(x) = \\frac{d}{dx} \\exp(-x^2) = -2x \\exp(-x^2) = -2x f(x)\n$$\n\nThe second derivative is found using the product rule:\n$$\nf''(x) = \\frac{d}{dx} (-2x f(x)) = -2 f(x) - 2x f'(x)\n$$\nSubstituting $f'(x) = -2x f(x)$:\n$$\nf''(x) = -2 f(x) - 2x (-2x f(x)) = (-2 + 4x^2) f(x)\n$$\nSo, the analytic expression for the second derivative is:\n$$\nf^{(2)}(x) = (4x^2 - 2) \\exp(-x^2)\n$$\n\nTo find the fourth derivative, we first compute the third derivative:\n$$\nf'''(x) = \\frac{d}{dx} \\left( (4x^2 - 2) f(x) \\right) = (8x) f(x) + (4x^2 - 2) f'(x)\n$$\nSubstituting $f'(x) = -2x f(x)$:\n$$\nf'''(x) = 8x f(x) + (4x^2 - 2) (-2x f(x)) = (8x - 8x^3 + 4x) f(x) = (-8x^3 + 12x) f(x)\n$$\n\nFinally, the fourth derivative is computed by differentiating $f'''(x)$:\n$$\nf^{(4)}(x) = \\frac{d}{dx} \\left( (-8x^3 + 12x) f(x) \\right) = (-24x^2 + 12) f(x) + (-8x^3 + 12x) f'(x)\n$$\nSubstituting $f'(x) = -2x f(x)$:\n$$\nf^{(4)}(x) = (-24x^2 + 12) f(x) + (-8x^3 + 12x) (-2x f(x)) = (-24x^2 + 12 + 16x^4 - 24x^2) f(x)\n$$\nSo, the analytic expression for the fourth derivative is:\n$$\nf^{(4)}(x) = (16x^4 - 48x^2 + 12) \\exp(-x^2)\n$$\n\n### Part 2: Finite Difference Stencils and Truncation Errors\n\nWe derive the centered finite difference formulas and their leading truncation error terms by forming linear combinations of Taylor series expansions around a point $x_0$. Let $f_0^{(n)}$ denote $f^{(n)}(x_0)$. The relevant expansions are:\n$$\nf(x_0 \\pm h) = f_0 \\pm h f_0' + \\frac{h^2}{2} f_0'' + \\frac{\\pm h^3}{6} f_0''' + \\frac{h^4}{24} f_0^{(4)} + \\frac{\\pm h^5}{120} f_0^{(5)} + \\frac{h^6}{720} f_0^{(6)} + \\mathcal{O}(h^7)\n$$\n$$\nf(x_0 \\pm 2h) = f_0 \\pm 2h f_0' + \\frac{(2h)^2}{2} f_0'' + \\frac{\\pm (2h)^3}{6} f_0''' + \\frac{(2h)^4}{24} f_0^{(4)} + \\frac{\\pm (2h)^5}{120} f_0^{(5)} + \\frac{(2h)^6}{720} f_0^{(6)} + \\mathcal{O}(h^7)\n$$\n\n**2a. $3$-point centered stencil for $f^{(2)}(x_0)$**\nWe seek an approximation of the form $\\frac{c_{-1}f(x_0-h) + c_0 f(x_0) + c_1 f(x_0+h)}{h^2}$. For a centered stencil, symmetry requires $c_{-1}=c_1$.\nConsider the combination $f(x_0+h) - 2f(x_0) + f(x_0-h)$:\n$$\n(f_0 + h f_0' + \\frac{h^2}{2} f_0'' + \\frac{h^3}{6} f_0''' + \\frac{h^4}{24} f_0^{(4)} + \\mathcal{O}(h^6)) - 2f_0 + (f_0 - h f_0' + \\frac{h^2}{2} f_0'' - \\frac{h^3}{6} f_0''' + \\frac{h^4}{24} f_0^{(4)} + \\mathcal{O}(h^6))\n$$\nCombining terms, the odd-powered derivative terms cancel:\n$$\n= (1-2+1)f_0 + (1-1)h f_0' + (\\frac{1}{2}+\\frac{1}{2})h^2 f_0'' + (\\frac{1}{6}-\\frac{1}{6})h^3 f_0''' + (\\frac{1}{24}+\\frac{1}{24})h^4 f_0^{(4)} + \\mathcal{O}(h^6)\n$$\n$$\n= h^2 f_0'' + \\frac{h^4}{12} f_0^{(4)} + \\mathcal{O}(h^6)\n$$\nDividing by $h^2$, we get the approximation:\n$$\n\\frac{f(x_0-h) - 2f(x_0) + f(x_0+h)}{h^2} = f_0'' + \\frac{h^2}{12} f_0^{(4)} + \\mathcal{O}(h^4)\n$$\nThe stencil is $\\frac{1}{h^2}[f(x_0-h) - 2f(x_0) + f(x_0+h)]$. The leading truncation error is $E_T = \\frac{h^2}{12} f_0^{(4)}$, so the method is of order $p=2$.\n\n**2b. $5$-point centered stencil for $f^{(2)}(x_0)$**\nWe seek a more accurate approximation of the form $\\frac{1}{h^2} \\sum_{j=-2}^{2} c_j f(x_0+jh)$. Symmetry implies $c_{-j}=c_j$. The linear combination is $c_2(f(x_0-2h)+f(x_0+2h)) + c_1(f(x_0-h)+f(x_0+h)) + c_0 f(x_0)$. We use the summed expansions:\n$$\nf(x_0-h) + f(x_0+h) = 2f_0 + h^2 f_0'' + \\frac{h^4}{12} f_0^{(4)} + \\frac{h^6}{360}f_0^{(6)} + \\mathcal{O}(h^8)\n$$\n$$\nf(x_0-2h)+f(x_0+2h) = 2f_0 + 4h^2 f_0'' + \\frac{4h^4}{3} f_0^{(4)} + \\frac{8h^6}{45}f_0^{(6)} + \\mathcal{O}(h^8)\n$$\nWe form a system of equations to determine $c_0, c_1, c_2$ by matching coefficients of the derivatives:\n$$\n\\text{Numerator} = (2c_2 + 2c_1 + c_0)f_0 + (4c_2 + c_1)h^2 f_0'' + (\\frac{4}{3}c_2 + \\frac{1}{12}c_1)h^4 f_0^{(4)} + \\dots\n$$\nTo approximate $h^2 f_0''$, we require:\n\\begin{enumerate}\n    \\item Coeff of $f_0$: $2c_2 + 2c_1 + c_0 = 0$\n    \\item Coeff of $f_0''$: $4c_2 + c_1 = 1$\n    \\item To achieve higher order, we cancel the next error term ($f_0^{(4)}$): $\\frac{4}{3}c_2 + \\frac{1}{12}c_1 = 0$\n\\end{enumerate}\nFrom (3), $16c_2 + c_1 = 0 \\implies c_1 = -16c_2$.\nSubstituting into (2): $4c_2 + (-16c_2) = 1 \\implies -12c_2 = 1 \\implies c_2 = -1/12$.\nThen $c_1 = -16(-1/12) = 4/3$.\nFrom (1): $c_0 = -2c_1 - 2c_2 = -2(4/3) - 2(-1/12) = -8/3 + 1/6 = -16/6 + 1/6 = -15/6 = -5/2$.\nThe coefficients are $c_2=-1/12, c_1=4/3, c_0=-5/2$. The stencil is:\n$$\n\\frac{-\\frac{1}{12}f(x_0-2h) + \\frac{4}{3}f(x_0-h) - \\frac{5}{2}f(x_0) + \\frac{4}{3}f(x_0+h) - \\frac{1}{12}f(x_0+2h)}{h^2}\n$$\nThe next term in the numerator expansion involves $f_0^{(6)}$: $(\\frac{8}{45}c_2 + \\frac{1}{360}c_1)h^6 f_0^{(6)}$.\nSubstituting values: $(\\frac{8}{45}(-\\frac{1}{12}) + \\frac{1}{360}(\\frac{4}{3})) h^6 f_0^{(6)} = (-\\frac{2}{135} + \\frac{1}{270})h^6 f_0^{(6)} = -\\frac{3}{270}h^6 f_0^{(6)} = -\\frac{1}{90}h^6 f_0^{(6)}$.\nThe approximation is $f_0'' - \\frac{h^4}{90}f_0^{(6)} + \\mathcal{O}(h^6)$. The truncation error is $E_T = -\\frac{h^4}{90}f_0^{(6)}$, so the method is of order $p=4$.\n\n**2c. $5$-point centered stencil for $f^{(4)}(x_0)$**\nWe seek an approximation for $f_0^{(4)}$ of the form $\\frac{1}{h^4} \\sum_{j=-2}^{2} c_j f(x_0+jh)$. We use the same expansion for the numerator as in 2b, but match coefficients differently to isolate $f_0^{(4)}$:\n$$\n\\text{Numerator} = (2c_2 + 2c_1 + c_0)f_0 + (4c_2 + c_1)h^2 f_0'' + (\\frac{4}{3}c_2 + \\frac{1}{12}c_1)h^4 f_0^{(4)} + \\dots\n$$\nTo approximate $h^4 f_0^{(4)}$, we require:\n\\begin{enumerate}\n    \\item Coeff of $f_0$: $2c_2 + 2c_1 + c_0 = 0$\n    \\item Coeff of $f_0''$: $4c_2 + c_1 = 0 \\implies c_1 = -4c_2$\n    \\item Coeff of $f_0^{(4)}$: $\\frac{4}{3}c_2 + \\frac{1}{12}c_1 = 1$\n\\end{enumerate}\nFrom (2), substitute $c_1 = -4c_2$ into (3): $\\frac{4}{3}c_2 + \\frac{1}{12}(-4c_2) = 1 \\implies \\frac{4}{3}c_2 - \\frac{1}{3}c_2 = 1 \\implies c_2 = 1$.\nThen $c_1 = -4(1) = -4$.\nFrom (1): $c_0 = -2c_1 - 2c_2 = -2(-4) - 2(1) = 8 - 2 = 6$.\nThe coefficients are $c_2=1, c_1=-4, c_0=6$. The stencil is:\n$$\n\\frac{f(x_0-2h) - 4f(x_0-h) + 6f(x_0) - 4f(x_0+h) + f(x_0+2h)}{h^4}\n$$\nThe next term in the numerator expansion involves $f_0^{(6)}$: $(\\frac{8}{45}c_2 + \\frac{1}{360}c_1)h^6 f_0^{(6)}$.\nSubstituting values: $(\\frac{8}{45}(1) + \\frac{1}{360}(-4))h^6 f_0^{(6)} = (\\frac{64}{360} - \\frac{4}{360})h^6 f_0^{(6)} = \\frac{60}{360}h^6 f_0^{(6)} = \\frac{1}{6}h^6 f_0^{(6)}$.\nThe approximation is $f_0^{(4)} + \\frac{h^2}{6}f_0^{(6)} + \\mathcal{O}(h^4)$. The truncation error is $E_T = \\frac{h^2}{6}f_0^{(6)}$, so the method is of order $p=2$.\n\n### Part 3: Numerical Verification Strategy\n\nThe theoretical derivations will be verified computationally. For each of the four test cases specified:\n\\begin{enumerate}\n    \\item A sequence of step sizes $h_k = 0.2 \\cdot 2^{-k}$ for $k \\in \\{0, 1, 2, 3, 4, 5\\}$ is used.\n    \\item For each $h_k$, the appropriate finite difference approximation is computed at the specified point $x_0$.\n    \\item The true value of the derivative at $x_0$ is computed using the analytic formulas from Part 1.\n    \\item The absolute error $E(h_k)$ is calculated as the absolute difference between the numerical approximation and the analytic value.\n    \\item The observed order of convergence $p_k$ is computed for $k \\in \\{1, 2, 3, 4, 5\\}$ using pairs of successive errors:\n    $$\n    p_k = \\frac{\\log(E(h_{k-1})/E(h_k))}{\\log(h_{k-1}/h_k)} = \\frac{\\log(E(h_{k-1})/E(h_k))}{\\log(2)}\n    $$\n    \\item The reported observed order for each case, $\\tilde{p}$, is defined as the median of the last three computed orders, $\\{p_3, p_4, p_5\\}$. This provides a stable estimate of the asymptotic convergence rate as $h \\to 0$.\n    \\item Finally, each case is validated against its expected theoretical order $p_{\\mathrm{expected}}$ using the criterion $|\\tilde{p} - p_{\\mathrm{expected}}| \\le 0.3$.\n\\end{enumerate}\nThis procedure will be encapsulated in a Python program to generate the final boolean results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of approximating higher derivatives and verifying convergence orders.\n    \"\"\"\n\n    # Part 1: Analytic Functions\n    def f(x: float) -> float:\n        \"\"\"The base function f(x) = exp(-x^2).\"\"\"\n        return np.exp(-x**2)\n\n    def f_d2(x: float) -> float:\n        \"\"\"The analytic second derivative of f(x).\"\"\"\n        return (4 * x**2 - 2) * np.exp(-x**2)\n\n    def f_d4(x: float) -> float:\n        \"\"\"The analytic fourth derivative of f(x).\"\"\"\n        return (16 * x**4 - 48 * x**2 + 12) * np.exp(-x**2)\n\n    # Part 2: Finite Difference Stencils\n    def approx_d2_3pt(func, x0: float, h: float) -> float:\n        \"\"\"3-point centered difference approximation for the 2nd derivative.\"\"\"\n        return (func(x0 - h) - 2 * func(x0) + func(x0 + h)) / h**2\n\n    def approx_d2_5pt(func, x0: float, h: float) -> float:\n        \"\"\"5-point centered difference approximation for the 2nd derivative (order 4).\"\"\"\n        return (-func(x0 - 2 * h) + 16 * func(x0 - h) - 30 * func(x0) + 16 * func(x0 + h) - func(x0 + 2 * h)) / (12 * h**2)\n\n    def approx_d4_5pt(func, x0: float, h: float) -> float:\n        \"\"\"5-point centered difference approximation for the 4th derivative (order 2).\"\"\"\n        return (func(x0 - 2 * h) - 4 * func(x0 - h) + 6 * func(x0) - 4 * func(x0 + h) + func(x0 + 2 * h)) / h**4\n\n    # Part 3: Numerical Verification\n    h_values = [0.2 * (2**-k) for k in range(6)]\n    \n    # Test cases: (approximation_function, analytic_function, evaluation_point_x0, expected_order)\n    test_cases = [\n        # Case A: 3-point f''(0.7), expected order 2\n        (approx_d2_3pt, f_d2, 0.7, 2),\n        # Case B: 5-point f''(1.3), expected order 4\n        (approx_d2_5pt, f_d2, 1.3, 4),\n        # Case C: 5-point f^(4)(0.0), expected order 2\n        (approx_d4_5pt, f_d4, 0.0, 2),\n        # Case D: 5-point f''(2.0) edge case, expected order 4\n        (approx_d2_5pt, f_d2, 2.0, 4),\n    ]\n\n    final_results = []\n    for approx_func, analytic_func, x0, p_expected in test_cases:\n        errors = []\n        for h in h_values:\n            approx_val = approx_func(f, x0, h)\n            analytic_val = analytic_func(x0)\n            error = np.abs(approx_val - analytic_val)\n            errors.append(error)\n\n        observed_orders = []\n        # Calculate observed orders p_k for k in {1,2,3,4,5}\n        for k in range(1, len(h_values)):\n            # Ratio of step sizes is 2\n            h_ratio = h_values[k-1] / h_values[k]\n            \n            # Avoid division by zero if error becomes numerically zero\n            if errors[k] > 0 and errors[k-1] > 0:\n                error_ratio = errors[k-1] / errors[k]\n                order = np.log(error_ratio) / np.log(h_ratio)\n                observed_orders.append(order)\n            else:\n                # If error is zero, convergence is perfect/infinite.\n                # This case isn't expected to be hit, but we handle it.\n                observed_orders.append(np.inf)\n\n        # Per problem, use median of last three observed orders {p_3, p_4, p_5}\n        # These correspond to indices 2, 3, 4 of observed_orders list\n        # which has 5 elements (p_1 to p_5).\n        if len(observed_orders) >= 5:\n            last_three_orders = observed_orders[2:5]\n            reported_order = np.median(last_three_orders)\n            \n            # Apply pass-fail criterion\n            passed = np.abs(reported_order - p_expected) <= 0.3\n            final_results.append(passed)\n        else:\n            # This path should not be taken given the problem setup\n            final_results.append(False)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3238903"}, {"introduction": "Manually deriving stencils, as seen in the first practice, can be tedious and error-prone. This advanced exercise guides you to generalize the process by creating an automated routine that can generate the weights for any derivative order on any given stencil. By framing the problem as solving a linear system based on polynomial exactness, you will not only build a powerful computational tool but also gain insight into the elegant mathematical structure that underpins all finite difference methods [@problem_id:3238803].", "problem": "You are tasked with implementing and testing a general-purpose routine that generates exact finite difference weights for approximating a higher-order derivative using a given set of stencil nodes. The fundamental base for the derivation must be the Taylor series expansion of a smooth function and the induced moment conditions. Your implementation must solve a square linear system derived from the exactness conditions on polynomials, using exact rational arithmetic where possible.\n\nLet $f$ be a smooth function, $x \\in \\mathbb{R}$ a point, $h \\in \\mathbb{R}$ a step size, and let the stencil be given by distinct nodes $s_0, s_1, \\dots, s_{n-1} \\in \\mathbb{R}$. The goal is to construct weights $w_0, w_1, \\dots, w_{n-1}$ such that the finite difference formula\n$$\n\\frac{1}{h^p} \\sum_{j=0}^{n-1} w_j \\, f(x + h s_j)\n$$\nexactly reproduces the $p$-th derivative $f^{(p)}(x)$ for all polynomials up to degree $n-1$.\n\nStarting from the Taylor series expansion\n$$\nf(x + h s_j) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x)}{k!} (h s_j)^k,\n$$\nimpose the polynomial exactness (moment) conditions for degrees $k = 0, 1, \\dots, n-1$:\n$$\n\\sum_{j=0}^{n-1} w_j s_j^k =\n\\begin{cases}\n0, & \\text{if } k \\neq p, \\\\\np!, & \\text{if } k = p.\n\\end{cases}\n$$\nThese conditions yield a linear system of $n$ equations in the $n$ unknowns $w_j$. The matrix of the system is a square Vandermonde-type matrix with entries $s_j^k$.\n\nYour program must:\n- Construct and solve the linear system for general derivative order $p$ and general stencil nodes $s_j$ that are all distinct.\n- Use exact rational arithmetic when nodes are integers, by operating in the rational number field via standard library facilities, so that the resulting weights are exact rational numbers. If an exact symbolic solve is not applicable (e.g., non-integer nodes), a floating-point solve is acceptable, but for this task all provided stencil nodes are integers.\n- Validate inputs and handle invalid configurations by raising or signaling an error in a controlled manner. Invalid cases include repeated stencil nodes and $p$ outside the range $0 \\leq p \\leq n-1$.\n\nTest suite specification:\nImplement the following test cases inside your program and aggregate the results according to the final output format requirement.\n\n1. Central first derivative weights with $p = 1$ and nodes $[-1, 0, 1]$ (here $n = 3$ and $h = 1$ for weight generation). Compute the weights and compare them to the expected exact rational values $\\left[-\\frac{1}{2}, 0, \\frac{1}{2}\\right]$. Report the maximum absolute difference as a floating-point number.\n\n2. Central second derivative weights with $p = 2$ and nodes $[-1, 0, 1]$ (with $h = 1$ for weight generation). Compute the weights and compare them to the expected exact rational values $[1, -2, 1]$. Report the maximum absolute difference as a floating-point number.\n\n3. Forward fifth derivative weights with $p = 5$ and nodes $[0, 1, 2, 3, 4, 5]$ (with $h = 1$ for weight generation). Compare the computed weights to the expected exact integer sequence $[-1, 5, -10, 10, -5, 1]$. Report a boolean indicating exact equality.\n\n4. Numerical approximation accuracy for a smooth function using the forward fifth derivative weights. Using the weights from test case 3, set $x = 0$, $h = 10^{-3}$, and $f(x) = e^x$. Compute the absolute error of the approximation to $f^{(5)}(0)$, which equals $1$ for $f(x) = e^x$. Report the absolute error as a floating-point number.\n\n5. Polynomial exactness on a nonuniform stencil. Let $p = 2$, nodes $[-2, -1, 0, 1, 3, 4]$ (six distinct integer nodes), $x = 0$, and $h = 0.1$. Use the generated weights to approximate the second derivative of $f(x) = x^2$ at $x = 0$. Because the scheme is constructed to be exact for polynomials up to degree $n-1$, the result should be exactly $f^{(2)}(0) = 2$. Report a boolean indicating whether the absolute difference from $2$ is below $10^{-12}$.\n\n6. Invalid derivative order. Attempt to generate weights with nodes $[0, 1]$ and $p = 3$. Since $p \\geq n$ is invalid, the routine must signal an error. Report a boolean indicating whether the error was correctly detected.\n\n7. Duplicate nodes (singular system). Attempt to generate weights with nodes $[0, 0, 1]$ and $p = 1$. Since duplicate nodes yield a singular matrix, the routine must signal an error. Report a boolean indicating whether the error was correctly detected.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order corresponding to the test suite above:\n1. The floating-point maximum absolute difference for test case 1.\n2. The floating-point maximum absolute difference for test case 2.\n3. The boolean result for test case 3.\n4. The floating-point absolute error for test case 4.\n5. The boolean result for test case 5.\n6. The boolean result for test case 6.\n7. The boolean result for test case 7.\n\nFor example, the output should look like:\n\"[r1,r2,r3,r4,r5,r6,r7]\".", "solution": "The objective is to determine the weights $w_0, w_1, \\dots, w_{n-1}$ for a finite difference formula that approximates the $p$-th derivative of a sufficiently smooth function $f(x)$ at a point $x$. The formula is given by:\n$$\nf^{(p)}(x) \\approx \\frac{1}{h^p} \\sum_{j=0}^{n-1} w_j f(x + h s_j)\n$$\nwhere $\\{s_j\\}_{j=0}^{n-1}$ is a stencil of $n$ distinct, dimensionless nodes, and $h$ is the step size. The method for deriving the weights is to require the formula to be exact for all polynomials up to degree $n-1$.\n\nWe begin with the Taylor series expansion of $f(x + \\Delta x)$ around the point $x$:\n$$\nf(x + \\Delta x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x)}{k!} (\\Delta x)^k\n$$\nSubstituting $\\Delta x = h s_j$ for each node in the stencil, we have:\n$$\nf(x + h s_j) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x)}{k!} (h s_j)^k\n$$\nNow, we substitute this expansion into the summation part of the finite difference formula:\n$$\n\\sum_{j=0}^{n-1} w_j f(x + h s_j) = \\sum_{j=0}^{n-1} w_j \\left( \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x)}{k!} (h s_j)^k \\right)\n$$\nAssuming uniform convergence allows for the interchange of the order of summation:\n$$\n\\sum_{j=0}^{n-1} w_j f(x + h s_j) = \\sum_{k=0}^{\\infty} \\frac{h^k f^{(k)}(x)}{k!} \\left( \\sum_{j=0}^{n-1} w_j s_j^k \\right)\n$$\nThe full finite difference formula is intended to approximate $f^{(p)}(x)$, which means the right-hand side of the equation above, after being divided by $h^p$, should collapse to $f^{(p)}(x)$. This means we desire:\n$$\n\\frac{1}{h^p} \\sum_{k=0}^{\\infty} \\frac{h^k f^{(k)}(x)}{k!} \\left( \\sum_{j=0}^{n-1} w_j s_j^k \\right) = f^{(p)}(x)\n$$\nFor this equality to hold for an arbitrary function $f(x)$ (or more specifically, for its Taylor series representation), the coefficients of each derivative term $f^{(k)}(x)$ on the left-hand side must match the corresponding term on the right-hand side. The right-hand side only contains the $f^{(p)}(x)$ term. Therefore, the coefficient of $f^{(k)}(x)$ for $k \\neq p$ must be zero, and the coefficient of $f^{(p)}(x)$ must be $1$. This gives us the following conditions on the inner sum (the moments):\n- For $k = p$: The coefficient of $f^{(p)}(x)$ is $\\frac{1}{h^p} \\frac{h^p}{p!} \\left( \\sum_{j=0}^{n-1} w_j s_j^p \\right)$. We set this equal to $1$.\n$$\n\\frac{1}{p!} \\left( \\sum_{j=0}^{n-1} w_j s_j^p \\right) = 1 \\implies \\sum_{j=0}^{n-1} w_j s_j^p = p!\n$$\n- For $k \\neq p$: The coefficient of $f^{(k)}(x)$ is $\\frac{1}{h^p} \\frac{h^k}{k!} \\left( \\sum_{j=0}^{n-1} w_j s_j^k \\right)$. We set this equal to $0$.\n$$\n\\frac{h^k}{h^p k!} \\left( \\sum_{j=0}^{n-1} w_j s_j^k \\right) = 0 \\implies \\sum_{j=0}^{n-1} w_j s_j^k = 0\n$$\nThese conditions are imposed for $k = 0, 1, \\dots, n-1$. This provides a system of $n$ linear equations for the $n$ unknown weights $w_0, w_1, \\dots, w_{n-1}$. The system can be written in matrix form as $\\mathbf{A} \\mathbf{w} = \\mathbf{b}$, where:\n$$\n\\mathbf{A} =\n\\begin{pmatrix}\ns_0^0 & s_1^0 & \\dots & s_{n-1}^0 \\\\\ns_0^1 & s_1^1 & \\dots & s_{n-1}^1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_0^{n-1} & s_1^{n-1} & \\dots & s_{n-1}^{n-1}\n\\end{pmatrix}\n, \\quad\n\\mathbf{w} =\n\\begin{pmatrix}\nw_0 \\\\\nw_1 \\\\\n\\vdots \\\\\nw_{n-1}\n\\end{pmatrix}\n, \\quad\n\\mathbf{b} =\n\\begin{pmatrix}\n0 \\\\\n\\vdots \\\\\np! \\\\\n\\vdots \\\\\n0\n\\end{pmatrix}\n$$\nThe matrix $\\mathbf{A}$ is a Vandermonde matrix. Its determinant is non-zero if and only if all the nodes $s_j$ are distinct. The problem statement guarantees this for valid inputs, ensuring a unique solution for the weights $\\mathbf{w}$. The vector $\\mathbf{b}$ is a zero vector except for the entry at index $p$ (the $(p+1)$-th row), which is $p!$.\n\nThe problem states that the input stencil nodes are integers. To obtain exact weights, the linear system must be solved using rational arithmetic. This involves constructing the matrix $\\mathbf{A}$ and vector $\\mathbf{b}$ with rational numbers and using an algorithm like Gaussian elimination that operates on the field of rational numbers. Python's `fractions` module provides the necessary tools for this.\n\nThe implementation will consist of a primary routine that validates the inputs, constructs the linear system, and solves it.\nInput validation must ensure two conditions:\n1. All stencil nodes $s_j$ must be distinct. If `len(set(s))` is not equal to `len(s)`, the Vandermonde matrix $\\mathbf{A}$ is singular, and a unique solution does not exist.\n2. The order of the derivative $p$ must be in the range $0 \\leq p < n$, where $n$ is the number of stencil points. This is because $n$ points can uniquely define a polynomial of degree up to $n-1$, and these $n$ constraints are used to determine the $n$ weights. It is not possible to satisfy the moment conditions for a derivative order $p \\geq n$.\n\nFor valid inputs, the procedure is as follows:\n1. Define $n$ as the number of nodes in the stencil $s$.\n2. Construct the $n \\times n$ matrix $\\mathbf{A}$ where $A_{kj} = s_j^k$ for $k, j \\in \\{0, \\dots, n-1\\}$. Each element is represented as a rational number.\n3. Construct the $n \\times 1$ vector $\\mathbf{b}$ where $b_k = 0$ for $k \\neq p$ and $b_p = p!$. Each element is also a rational number.\n4. Solve the linear system $\\mathbf{A} \\mathbf{w} = \\mathbf{b}$ for $\\mathbf{w}$ using a rational arithmetic-based linear solver, such as Gaussian elimination with back substitution.\nThe resulting vector $\\mathbf{w}$ will contain the exact rational weights for the specified finite difference scheme. These weights are then used to perform the calculations required by the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom fractions import Fraction\nimport math\n\nclass FiniteDifferenceError(ValueError):\n    \"\"\"Custom exception for finite difference routine errors.\"\"\"\n    pass\n\ndef _solve_rational_system(A, b):\n    \"\"\"\n    Solves a linear system Ax=b where A and b contain Fraction objects.\n    Uses Gaussian elimination with back substitution.\n    \"\"\"\n    n = len(b)\n    # Create an augmented matrix\n    M = [row + [val] for row, val in zip(A, b)]\n\n    # Forward elimination\n    for i in range(n):\n        # Find a non-zero pivot\n        pivot_row = i\n        while pivot_row < n and M[pivot_row][i] == Fraction(0):\n            pivot_row += 1\n        \n        if pivot_row == n:\n            # This case is pre-empted by the duplicate node check,\n            # but is included for solver robustness.\n            raise np.linalg.LinAlgError(\"Singular matrix\")\n\n        # Swap the current row with the pivot row\n        M[i], M[pivot_row] = M[pivot_row], M[i]\n        \n        # Eliminate entries below the pivot\n        pivot_val = M[i][i]\n        for j in range(i + 1, n):\n            factor = M[j][i] / pivot_val\n            # M[j][i] = Fraction(0) # Not strictly necessary\n            for k in range(i + 1, n + 1):\n                M[j][k] -= factor * M[i][k]\n\n    # Backward substitution\n    x = [Fraction(0)] * n\n    for i in range(n - 1, -1, -1):\n        s = sum(M[i][j] * x[j] for j in range(i + 1, n))\n        x[i] = (M[i][n] - s) / M[i][i]\n        \n    return x\n\ndef generate_weights(s, p):\n    \"\"\"\n    Generates finite difference weights for a given stencil and derivative order.\n    \n    Args:\n        s (list or tuple of int): The stencil nodes.\n        p (int): The order of the derivative.\n        \n    Returns:\n        list of Fraction: The calculated weights.\n    \n    Raises:\n        FiniteDifferenceError: If inputs are invalid.\n    \"\"\"\n    n = len(s)\n    \n    # Input validation\n    if len(set(s)) != n:\n        raise FiniteDifferenceError(\"Stencil nodes must be distinct.\")\n    if not (0 <= p < n):\n        raise FiniteDifferenceError(f\"Derivative order p must be in [0, n-1]. Got p={p}, n={n}.\")\n\n    # Construct the Vandermonde matrix A\n    A = []\n    for k in range(n):\n        A.append([Fraction(node)**k for node in s])\n\n    # Construct the right-hand side vector b\n    b = [Fraction(0)] * n\n    b[p] = Fraction(math.factorial(p))\n    \n    # The system is A_transpose * w = b. Solve A.T w = b, which is w.T A = b.T\n    # This means we solve A^T x = b for x, where A is the matrix from the loop above.\n    # The matrix in the problem is A_kj = s_j^k. The system is sum_j A_kj w_j = b_k.\n    # So A is the coefficient matrix directly.\n    return _solve_rational_system(A, b)\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the final results.\n    \"\"\"\n    results = []\n\n    # Test Case 1: Central first derivative\n    p1 = 1\n    s1 = [-1, 0, 1]\n    expected_w1 = [Fraction(-1, 2), Fraction(0), Fraction(1, 2)]\n    computed_w1 = generate_weights(s1, p1)\n    diff1 = max(abs(float(c) - float(e)) for c, e in zip(computed_w1, expected_w1))\n    results.append(diff1)\n\n    # Test Case 2: Central second derivative\n    p2 = 2\n    s2 = [-1, 0, 1]\n    expected_w2 = [Fraction(1), Fraction(-2), Fraction(1)]\n    computed_w2 = generate_weights(s2, p2)\n    diff2 = max(abs(float(c) - float(e)) for c, e in zip(computed_w2, expected_w2))\n    results.append(diff2)\n\n    # Test Case 3: Forward fifth derivative\n    p3 = 5\n    s3 = [0, 1, 2, 3, 4, 5]\n    expected_w3 = [Fraction(v) for v in [-1, 5, -10, 10, -5, 1]]\n    computed_w3 = generate_weights(s3, p3)\n    exact_match3 = (computed_w3 == expected_w3)\n    results.append(exact_match3)\n\n    # Test Case 4: Numerical accuracy\n    p4 = 5\n    s4 = [0, 1, 2, 3, 4, 5]\n    h4 = 1e-3\n    x4 = 0.0\n    weights4 = generate_weights(s4, p4)\n    # f(x) = e^x, f^(5)(x) = e^x, f^(5)(0) = 1\n    approx4 = sum(float(w) * np.exp(x4 + h4 * node) for w, node in zip(weights4, s4)) / (h4**p4)\n    exact4 = 1.0\n    error4 = abs(approx4 - exact4)\n    results.append(error4)\n\n    # Test Case 5: Polynomial exactness on nonuniform stencil\n    p5 = 2\n    s5 = [-2, -1, 0, 1, 3, 4]\n    h5 = 0.1\n    x5 = 0.0\n    weights5 = generate_weights(s5, p5)\n    # f(x) = x^2, f''(x) = 2, f''(0) = 2\n    # The formula should be exact for polynomials of degree < n=6\n    f_vals5 = [(x5 + h5 * node)**2 for node in s5]\n    approx5 = sum(float(w) * fv for w, fv in zip(weights5, f_vals5)) / (h5**p5)\n    exact5 = 2.0\n    poly_exact5 = abs(approx5 - exact5) < 1e-12\n    results.append(poly_exact5)\n\n    # Test Case 6: Invalid derivative order\n    error_detected6 = False\n    try:\n        generate_weights(s=[0, 1], p=3)\n    except FiniteDifferenceError:\n        error_detected6 = True\n    results.append(error_detected6)\n\n    # Test Case 7: Duplicate nodes\n    error_detected7 = False\n    try:\n        generate_weights(s=[0, 0, 1], p=1)\n    except FiniteDifferenceError:\n        error_detected7 = True\n    results.append(error_detected7)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3238803"}]}