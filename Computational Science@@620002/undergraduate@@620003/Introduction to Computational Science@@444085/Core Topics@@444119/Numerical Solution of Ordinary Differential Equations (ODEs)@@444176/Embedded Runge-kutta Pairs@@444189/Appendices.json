{"hands_on_practices": [{"introduction": "The power of embedded Runge-Kutta methods lies in their efficiency, but their design is not arbitrary. A critical design choice is to use two methods of different orders, typically $p$ and $p-1$. This exercise guides you through a symbolic derivation to uncover why this choice is essential for a reliable error estimate. By exploring a pathological case where both methods have the same order, you will gain a deeper appreciation for the theoretical foundations of adaptive step-size control. [@problem_id:3123519]", "problem": "Consider a general two-stage explicit Runge–Kutta (RK) method with one embedded estimator. The stages are defined by\n$$\nk_{1} \\;=\\; f(y_{n}), \\qquad k_{2} \\;=\\; f\\!\\big(y_{n} + h\\,a_{21}\\,k_{1}\\big),\n$$\nand the main update and an embedded update are\n$$\ny_{n+1}^{(p)} \\;=\\; y_{n} + h\\big(b_{1}\\,k_{1} + b_{2}\\,k_{2}\\big), \\qquad y_{n+1}^{(\\hat{p})} \\;=\\; y_{n} + h\\big(\\hat{b}_{1}\\,k_{1} + \\hat{b}_{2}\\,k_{2}\\big),\n$$\nrespectively, where $h$ is the time step, $f$ is a sufficiently smooth right-hand side, and $a_{21}$, $b_{1}$, $b_{2}$, $\\hat{b}_{1}$, $\\hat{b}_{2}$ are method coefficients. The goal is to construct an embedded pair with main order $p=2$ and embedded order $\\hat{p}=1$, denoted $p=2(1)$.\n\nStarting from the Taylor expansion of the exact solution to the initial value problem $y'(t) = f(y(t))$ with $y(t_{n}) = y_{n}$, and the definitions of the method above, derive the order conditions imposed by matching the series coefficients up to the appropriate order for the main method and the embedded estimator. Solve the order conditions symbolically for the two-stage explicit case in terms of a free stage parameter $a_{21} = \\alpha \\neq 0$, and produce the one-parameter family of $p=2(1)$ embedded pairs $\\{b_{1}(\\alpha), b_{2}(\\alpha); \\hat{b}_{1}, \\hat{b}_{2}\\}$.\n\nThen, construct a pathological system to demonstrate why the embedded error must be of different order than the main method for reliable step-size control. Consider the linear autonomous test equation $y'(t) = \\lambda\\,y(t)$ with constant $\\lambda \\in \\mathbb{R}$ and show, by direct computation of the one-step updates, that if the embedded estimator were chosen to have the same order $p=2$ as the main method while sharing the same stage parameter $a_{21}=\\alpha$, then the embedded error estimate \n$$\nE \\;=\\; y_{n+1}^{(p)} - y_{n+1}^{(\\hat{p})}\n$$\nwould vanish for all $h$, $\\lambda$, and $y_{n}$, even though the true local truncation error of either method is nonzero for this test equation. Conclude with the explicit expression for $E$ for this pathological case.\n\nYour final answer must be a single row matrix containing the five coefficients $\\big(b_{1}(\\alpha), b_{2}(\\alpha), a_{21}(\\alpha), \\hat{b}_{1}, \\hat{b}_{2}\\big)$ for the $p=2(1)$ pair you derived, followed by the expression for $E$ in the pathological case described above. No numerical rounding is required. Express the final answer using exact symbolic forms.", "solution": "The user has provided a valid problem statement from the field of computational science, specifically concerning the derivation and analysis of embedded Runge-Kutta methods. The problem is scientifically grounded, well-posed, and objective. I will proceed with a full solution.\n\nThe problem is divided into two parts. First, we derive the order conditions for a two-stage explicit Runge-Kutta (RK) method to form a $p=2(1)$ embedded pair. Second, we analyze a pathological case where both the main method and the embedded estimator are of order $p=2$.\n\n**Part 1: Derivation of the $p=2(1)$ RK Pair**\n\nWe start by considering the Taylor series expansion of the exact solution to the initial value problem $y'(t) = f(y(t))$ with $y(t_n) = y_n$. The solution at time $t_{n+1} = t_n + h$ is given by:\n$$\ny(t_n + h) = y(t_n) + h y'(t_n) + \\frac{h^2}{2} y''(t_n) + O(h^3)\n$$\nUsing the chain rule and the definition of the ODE, we can express the derivatives of $y$ in terms of $f$ and its partial derivatives with respect to $y$, which we denote as $f_y$, $f_{yy}$, etc.\n$y'(t_n) = f(y(t_n)) = f(y_n)$\n$y''(t_n) = \\frac{d}{dt} f(y(t)) \\Big|_{t=t_n} = f_y(y(t_n)) \\cdot y'(t_n) = f_y f$\nSubstituting these into the Taylor expansion, we get:\n$$\ny(t_n + h) = y_n + h f + \\frac{h^2}{2} f_y f + O(h^3)\n$$\nNext, we expand the numerical solution provided by the RK method. The method is defined by:\n$$\nk_{1} = f(y_{n}) \\\\\nk_{2} = f(y_{n} + h a_{21} k_{1}) \\\\\ny_{n+1} = y_{n} + h (b_{1} k_{1} + b_{2} k_{2})\n$$\nWe expand the stage $k_2$ as a Taylor series around $y_n$:\n$k_1 = f(y_n) = f$\n$$\nk_2 = f(y_n + h a_{21} f) = f(y_n) + (h a_{21} f) f_y(y_n) + O(h^2) = f + h a_{21} f_y f + O(h^2)\n$$\nNow, we substitute the expansions of $k_1$ and $k_2$ into the update formula for the main method, $y_{n+1}^{(p)}$:\n$$\ny_{n+1}^{(p)} = y_n + h \\big( b_1 f + b_2 (f + h a_{21} f_y f + O(h^2)) \\big)\n$$\n$$\ny_{n+1}^{(p)} = y_n + h (b_1 + b_2) f + h^2 (b_2 a_{21}) f_y f + O(h^3)\n$$\nFor the main method to have order $p=2$, its expansion must match the exact solution's expansion up to the $O(h^2)$ term. By comparing the coefficients of $f$ and $f_y f$, we obtain the order conditions:\n\\begin{align*}\n\\text{Order } p=2 \\text{ conditions:} \\\\\nO(h): & \\quad b_1 + b_2 = 1 \\\\\nO(h^2): & \\quad b_2 a_{21} = \\frac{1}{2}\n\\end{align*}\nSimilarly, for the embedded method $y_{n+1}^{(\\hat{p})} = y_{n} + h(\\hat{b}_{1}k_{1} + \\hat{b}_{2}k_{2})$, we find its expansion:\n$$\ny_{n+1}^{(\\hat{p})} = y_n + h (\\hat{b}_1 + \\hat{b}_2) f + h^2 (\\hat{b}_2 a_{21}) f_y f + O(h^3)\n$$\nFor the embedded method to have order $\\hat{p}=1$, its expansion must match the exact solution's expansion up to the $O(h)$ term. This gives a single order condition:\n\\begin{align*}\n\\text{Order } \\hat{p}=1 \\text{ condition:} \\\\\nO(h): & \\quad \\hat{b}_1 + \\hat{b}_2 = 1\n\\end{align*}\nNow, we solve these systems of equations. The problem specifies a free parameter $a_{21} = \\alpha \\neq 0$.\nFrom the $p=2$ conditions:\n$b_2 \\alpha = \\frac{1}{2} \\implies b_2 = \\frac{1}{2\\alpha}$\n$b_1 = 1 - b_2 = 1 - \\frac{1}{2\\alpha}$\nFor the $\\hat{p}=1$ condition, there are infinite solutions. A common and simple choice is to set $\\hat{b}_2=0$, which implies $\\hat{b}_1=1$. This choice makes the embedded method equivalent to the Forward Euler method, $y_{n+1}^{(\\hat{p})} = y_n + h k_1 = y_n + h f(y_n)$, which is indeed a first-order method.\n\nThe resulting one-parameter family of coefficients for the $p=2(1)$ pair is:\n$b_1(\\alpha) = 1 - \\frac{1}{2\\alpha}$\n$b_2(\\alpha) = \\frac{1}{2\\alpha}$\n$a_{21}(\\alpha) = \\alpha$\n$\\hat{b}_1 = 1$\n$\\hat{b}_2 = 0$\n\n**Part 2: Analysis of the Pathological Case**\n\nWe now investigate the scenario where the embedded method is chosen to have the same order as the main method, i.e., $p=\\hat{p}=2$, while sharing the same stage values (same $a_{21}$). The test equation is the linear autonomous system $y'(t) = \\lambda y(t)$, for which $f(y) = \\lambda y$.\n\nThe order conditions for both methods are now identical:\nFor the main method ($p=2$): $b_1 + b_2 = 1$ and $b_2 a_{21} = \\frac{1}{2}$.\nFor the embedded method ($\\hat{p}=2$): $\\hat{b}_1 + \\hat{b}_2 = 1$ and $\\hat{b}_2 a_{21} = \\frac{1}{2}$.\n\nLet's compute the stages for this test problem:\n$k_1 = f(y_n) = \\lambda y_n$\n$k_2 = f(y_n + h a_{21} k_1) = \\lambda(y_n + h a_{21} (\\lambda y_n)) = \\lambda y_n (1 + h a_{21} \\lambda)$\n\nThe update for the main method is:\n$$\ny_{n+1}^{(p)} = y_n + h \\left( b_1(\\lambda y_n) + b_2(\\lambda y_n (1 + h a_{21} \\lambda)) \\right)\n$$\n$$\ny_{n+1}^{(p)} = y_n \\left( 1 + h\\lambda(b_1 + b_2) + (h\\lambda)^2 (b_2 a_{21}) \\right)\n$$\nSubstituting the order $p=2$ conditions, $b_1+b_2=1$ and $b_2 a_{21}=1/2$:\n$$\ny_{n+1}^{(p)} = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right)\n$$\nThe update for the embedded method is:\n$$\ny_{n+1}^{(\\hat{p})} = y_n + h \\left( \\hat{b}_1(\\lambda y_n) + \\hat{b}_2(\\lambda y_n (1 + h a_{21} \\lambda)) \\right)\n$$\n$$\ny_{n+1}^{(\\hat{p})} = y_n \\left( 1 + h\\lambda(\\hat{b}_1 + \\hat{b}_2) + (h\\lambda)^2 (\\hat{b}_2 a_{21}) \\right)\n$$\nSubstituting the (hypothetical) order $\\hat{p}=2$ conditions, $\\hat{b}_1+\\hat{b}_2=1$ and $\\hat{b}_2 a_{21}=1/2$:\n$$\ny_{n+1}^{(\\hat{p})} = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right)\n$$\nThe embedded error estimate is defined as $E = y_{n+1}^{(p)} - y_{n+1}^{(\\hat{p})}$. Computing this difference:\n$$\nE = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) - y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) = 0\n$$\nThe error estimate is identically zero for any choice of $h$, $\\lambda$, and $y_n$. However, the true local truncation error (LTE) of either method is non-zero. For the exact solution $y(t_{n+1}) = y_n \\exp(h\\lambda)$, the LTE is:\n$$\n\\text{LTE} = y_n \\exp(h\\lambda) - y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) = y_n \\left( \\frac{(h\\lambda)^3}{6} + O(h^4) \\right) \\neq 0\n$$\nThis demonstrates that if the embedded method has the same order as the main-method and shares its stage computations in a two-stage scheme, the error estimator fails for linear problems, as it incorrectly reports zero error. This is why embedded pairs are constructed with methods of different orders, such as $p(\\hat{p})$ with $\\hat{p}=p-1$.\n\nThe expression for $E$ in this pathological case is $E=0$. The five coefficients for the $p=2(1)$ pair are $b_{1}(\\alpha) = 1 - \\frac{1}{2\\alpha}$, $b_{2}(\\alpha) = \\frac{1}{2\\alpha}$, $a_{21}(\\alpha) = \\alpha$, $\\hat{b}_{1}=1$, and $\\hat{b}_{2}=0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\frac{1}{2\\alpha} & \\frac{1}{2\\alpha} & \\alpha & 1 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3123519"}, {"introduction": "Having established why embedded pairs must have different orders, a natural next question arises: what exactly does the error estimate $\\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert$ measure? This practice delves into the heart of the error estimator itself. You will first perform a theoretical derivation to determine its asymptotic order of accuracy and then confirm your findings through numerical experiments, a classic workflow in computational science that solidifies the link between theory and practice. [@problem_id:3123523]", "problem": "You are to derive and verify the asymptotic order of the embedded error estimator for explicit embedded Runge–Kutta pairs. Begin from fundamental definitions of the local truncation error and consistency of a one-step method, without assuming any special formulas beyond standard definitions.\n\nTask Part A (Derivation): Consider an explicit embedded Runge–Kutta pair that computes two approximations over one step: a higher-order approximation $y_{n+1}$ of order $p$ and a lower-order approximation $\\hat{y}_{n+1}$ of order $p-1$, both obtained from the same set of stage evaluations. Using the definition of local truncation error for one-step methods and its Taylor expansion in the step size $h$ at a fixed point $t_n$, derive the asymptotic order (as $h \\to 0$) of the embedded error estimator $\\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert$ for a general $p(p-1)$ pair. Your derivation must start from the definition that a method of order $r$ has local truncation error that vanishes up to terms of order $h^{r}$ and the leading error term behaves like $C h^{r+1}$ for some problem-dependent constant $C$, and must not assume any pre-known Runge–Kutta error coefficients.\n\nTask Part B (Numerical Test): Implement a program in a modern programming language that:\n- Integrates the scalar initial value problem $y'=\\sin t$ with initial value $y(0)=0$ over a finite interval using fixed step sizes. Angles must be treated in radians.\n- Uses two explicit embedded Runge–Kutta pairs: a $p=3$, $p-1=2$ Bogacki–Shampine $3(2)$ pair, and a $p=2$, $p-1=1$ Heun–Euler $2(1)$ pair.\n- For each fixed step size $h$, computes the maximum over all steps of the embedded error estimator $\\max_{n} \\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$.\n- Estimates the observed order by performing a least-squares linear fit on the logarithms of the measured maxima versus the logarithms of the step sizes, so that the slope approximates the asymptotic order with respect to $h$.\n\nTest Suite:\n- Test case $1$: Bogacki–Shampine $3(2)$ on $[0, 4.0]$ with $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$.\n- Test case $2$: Heun–Euler $2(1)$ on $[0, 4.0]$ with $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$.\n- Test case $3$: Bogacki–Shampine $3(2)$ on $[0, 0.5]$ with $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n- Test case $4$: Heun–Euler $2(1)$ on $[0, 0.5]$ with $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the four observed orders (the fitted slopes), in the order of the test cases $1$ through $4$, each rounded to three decimals, as a comma-separated list enclosed in square brackets. For example, the output must look like $[s_1,s_2,s_3,s_4]$ where each $s_i$ is a real number printed to three decimal places. No other text should be printed.", "solution": "This problem consists of two parts. Part A requires the derivation of the asymptotic order of the embedded error estimator for a general explicit embedded Runge–Kutta pair of order $p$ and $p-1$. Part B requires numerical verification of this derived order for the Bogacki–Shampine $3(2)$ pair and the Heun–Euler $2(1)$ pair.\n\n### Part A: Derivation of the Asymptotic Order\n\nLet the initial value problem (IVP) be given by $y'(t) = f(t, y(t))$ with an initial condition $y(t_0) = y_0$. We assume that the exact solution, denoted by $y(t)$, is sufficiently smooth.\n\nA one-step numerical method approximates the solution at $t_{n+1} = t_n + h$ using information at $t_n$. Let $y_n$ be the numerical approximation at $t_n$. To analyze the *local error*, we assume that the numerical solution is exact at the beginning of the step, i.e., $y_n = y(t_n)$.\n\nThe problem statement defines a method to be of order $r$ if the error of a single step starting from the exact solution, $y(t_{n+1}) - y_{n+1}$, has a leading term that behaves like $C h^{r+1}$ for some non-zero, problem-dependent constant $C$. This is the local truncation error.\n\nWe have an embedded Runge–Kutta pair consisting of two methods:\n1.  A higher-order method of order $p$, which produces the approximation $y_{n+1}$.\n2.  A lower-order method of order $p-1$, which produces the approximation $\\hat{y}_{n+1}$.\n\nAccording to the definition of order, the local errors for these two methods are:\n$$y(t_{n+1}) - y_{n+1} = \\mathcal{E}_{p,n} h^{p+1} + O(h^{p+2})$$\n$$y(t_{n+1}) - \\hat{y}_{n+1} = \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1})$$\n\nHere, $\\mathcal{E}_{p,n}$ and $\\mathcal{E}_{p-1,n}$ are the principal local truncation error coefficients at step $n$. For the methods to be of order $p$ and $p-1$ respectively (and not higher), these coefficients must be non-zero in general. The term $O(h^k)$ represents higher-order terms that vanish faster than $h^{k-1}$ as $h \\to 0$.\n\nThe embedded error estimator, which we denote as $\\Delta_{n+1}$, is the difference between the two numerical approximations:\n$$\\Delta_{n+1} = \\hat{y}_{n+1} - y_{n+1}$$\n\nTo find the asymptotic behavior of $\\Delta_{n+1}$, we can express $y_{n+1}$ and $\\hat{y}_{n+1}$ in terms of the exact solution $y(t_{n+1})$ and their respective local errors:\n$$y_{n+1} = y(t_{n+1}) - \\left( \\mathcal{E}_{p,n} h^{p+1} + O(h^{p+2}) \\right)$$\n$$\\hat{y}_{n+1} = y(t_{n+1}) - \\left( \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1}) \\right)$$\n\nNow, we compute the difference $\\Delta_{n+1} = \\hat{y}_{n+1} - y_{n+1}$ by substituting these expressions:\n$$\\Delta_{n+1} = \\left( y(t_{n+1}) - \\mathcal{E}_{p-1,n} h^{p} - O(h^{p+1}) \\right) - \\left( y(t_{n+1}) - \\mathcal{E}_{p,n} h^{p+1} - O(h^{p+2}) \\right)$$\n\nThe exact solution term $y(t_{n+1})$ cancels out:\n$$\\Delta_{n+1} = - \\mathcal{E}_{p-1,n} h^{p} - O(h^{p+1}) + \\mathcal{E}_{p,n} h^{p+1} + O(h^{p+2})$$\n\nWe can combine the higher-order terms. The term $\\mathcal{E}_{p,n} h^{p+1}$ is of order $h^{p+1}$, so it is absorbed into the $O(h^{p+1})$ term. This yields:\n$$\\Delta_{n+1} = - \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1})$$\n\nFor the lower-order method to be precisely of order $p-1$, the coefficient $\\mathcal{E}_{p-1,n}$ must be non-zero for a general problem. Therefore, the leading term in the expansion of the error estimator $\\Delta_{n+1}$ is $-\\mathcal{E}_{p-1,n} h^{p}$.\n\nThe norm of the error estimator is then:\n$$\\lVert \\Delta_{n+1} \\rVert = \\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert = \\lVert - \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1}) \\rVert$$\nAs $h \\to 0$, the behavior of this norm is dominated by the leading term:\n$$\\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert \\approx \\lVert \\mathcal{E}_{p-1,n} \\rVert h^{p}$$\n\nThis demonstrates that the asymptotic order of the embedded error estimator is $p$. This estimator is a proxy for the local error of the lower-order method, $y(t_{n+1}) - \\hat{y}_{n+1}$, which is of order $p$. An important consequence is that the error estimator has a higher order of accuracy than the error it is estimating.\n\n### Part B: Numerical Verification\n\nTo verify this result, we will implement the specified Runge–Kutta pairs and analyze the convergence of the embedded error estimator. Based on the derivation, we expect:\n- For the Bogacki–Shampine $3(2)$ pair ($p=3$), the error estimator $\\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$ should converge with order $p=3$.\n- For the Heun–Euler $2(1)$ pair ($p=2$), the error estimator $\\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$ should converge with order $p=2$.\n\nThe numerical procedure involves integrating the IVP $y'=\\sin t$, $y(0)=0$ over a given interval with a set of decreasing step sizes $h_i$. For each $h_i$, we compute the maximum error estimate over the interval, $E_i = \\max_{n} \\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$. Since we expect $E_i \\approx C h_i^p$, taking the logarithm gives $\\log(E_i) \\approx \\log(C) + p \\log(h_i)$. A linear least-squares fit of $\\log(E_i)$ versus $\\log(h_i)$ will yield a slope that approximates the order $p$. The following code performs this verification for the four specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and verifies the asymptotic order of embedded Runge-Kutta error estimators.\n    \"\"\"\n\n    # Define the differential equation for the IVP: y' = sin(t)\n    def f(t, y):\n        return np.sin(t)\n\n    # Implementation of the Heun-Euler 2(1) embedded pair step\n    def heun_euler_step(f_func, t, y, h):\n        \"\"\"\n        Performs one step of the Heun-Euler 2(1) method.\n        p=2 (Heun's method), p-1=1 (Forward Euler method).\n        \"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + h, y + h * k1)\n        \n        # Higher-order solution (p=2, Heun's method)\n        y_next = y + (h / 2.0) * (k1 + k2)\n        \n        # Lower-order solution (p-1=1, Forward Euler)\n        y_hat_next = y + h * k1\n        \n        return y_next, y_hat_next\n\n    # Implementation of the Bogacki-Shampine 3(2) embedded pair step\n    def bs32_step(f_func, t, y, h):\n        \"\"\"\n        Performs one step of the Bogacki-Shampine 3(2) method.\n        Uses the standard 4-stage explicit formulation.\n        p=3, p-1=2.\n        \"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f_func(t + 0.75 * h, y + 0.75 * h * k2)\n        \n        # Higher-order solution (p=3)\n        y_next = y + h * (2.0/9.0 * k1 + 1.0/3.0 * k2 + 4.0/9.0 * k3)\n        \n        k4 = f_func(t + h, y_next)\n        \n        # Lower-order solution (p-1=2)\n        y_hat_next = y + h * (7.0/24.0 * k1 + 1.0/4.0 * k2 + 1.0/3.0 * k3 + 1.0/8.0 * k4)\n        \n        return y_next, y_hat_next\n\n    def run_integration(step_func, t_span, y0, h):\n        \"\"\"\n        Integrates the ODE using a given step function and fixed step size h.\n        Returns the maximum embedded error estimate over the interval.\n        \"\"\"\n        t = t_span[0]\n        y = y0\n        error_estimates = []\n\n        while t < t_span[1]:\n            # Ensure the last step does not overshoot the interval end\n            current_h = min(h, t_span[1] - t)\n            \n            y_next, y_hat_next = step_func(f, t, y, current_h)\n            \n            error_estimates.append(np.abs(y_hat_next - y_next))\n            \n            y = y_next\n            t += current_h\n        \n        if not error_estimates:\n            return 0.0\n            \n        return np.max(error_estimates)\n\n    def estimate_order(h_values, max_errors):\n        \"\"\"\n        Estimates the convergence order by a linear fit on log-log data.\n        \"\"\"\n        log_h = np.log(h_values)\n        log_e = np.log(max_errors)\n        \n        # np.polyfit with degree 1 performs a linear least-squares fit.\n        # It returns [slope, intercept]. The slope is the estimated order.\n        slope, _ = np.polyfit(log_h, log_e, 1)\n        return slope\n\n    # Define the four test cases from the problem statement\n    test_cases = [\n        {'method': bs32_step, 't_span': [0.0, 4.0], 'h_values': np.array([0.5, 0.25, 0.125, 0.0625])},\n        {'method': heun_euler_step, 't_span': [0.0, 4.0], 'h_values': np.array([0.5, 0.25, 0.125, 0.0625])},\n        {'method': bs32_step, 't_span': [0.0, 0.5], 'h_values': np.array([0.1, 0.05, 0.025, 0.0125])},\n        {'method': heun_euler_step, 't_span': [0.0, 0.5], 'h_values': np.array([0.1, 0.05, 0.025, 0.0125])},\n    ]\n\n    results = []\n    y0 = 0.0  # Initial condition y(0) = 0\n\n    for case in test_cases:\n        max_errors = []\n        for h in case['h_values']:\n            max_err = run_integration(case['method'], case['t_span'], y0, h)\n            max_errors.append(max_err)\n        \n        observed_order = estimate_order(case['h_values'], np.array(max_errors))\n        results.append(observed_order)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join([f'{r:.3f}' for r in results])}]\")\n\nsolve()\n```", "id": "3123523"}, {"introduction": "With a solid theoretical understanding of the error estimator, we can now tackle more complex and realistic scenarios. This practice moves from scalar equations to systems of ODEs using the classic Lotka-Volterra predator-prey model. You will confront a crucial practical issue: how to define error tolerances when different components of the system, like prey and predator populations, have vastly different scales. By comparing scalar and vector tolerance strategies, you will develop a more nuanced approach to implementing robust adaptive solvers for real-world scientific problems. [@problem_id:3123484]", "problem": "You are to design and implement a complete, runnable program that compares scalar and vector absolute tolerances in an adaptive Ordinary Differential Equation (ODE) solver based on an embedded Runge–Kutta (RK) pair, and analyzes single-step acceptance maps. The problem uses the classical predator–prey (Lotka–Volterra) model, and your program must produce a single-line output that aggregates quantitative results from a small, specified test suite.\n\nThe starting point is the fundamental model given by the autonomous system\n$$\n\\frac{d\\mathbf{y}}{dt}=\\mathbf{f}(t,\\mathbf{y}), \\quad \\mathbf{y}(t)=\\begin{bmatrix}y_1(t)\\\\y_2(t)\\end{bmatrix},\n$$\nwhere, for the predator–prey model,\n$$\n\\mathbf{f}(t,\\mathbf{y})=\\begin{bmatrix}\n\\alpha y_1 - \\beta y_1 y_2\\\\\n\\delta y_1 y_2 - \\gamma y_2\n\\end{bmatrix},\n$$\nwith parameters $\\alpha>0$, $\\beta>0$, $\\gamma>0$, and $\\delta>0$. The steady state $\\mathbf{s}=\\begin{bmatrix}s_1\\\\s_2\\end{bmatrix}$ satisfies $\\mathbf{f}(t,\\mathbf{s})=\\mathbf{0}$, which for this system yields\n$$\ns_1=\\frac{\\gamma}{\\delta},\\quad s_2=\\frac{\\alpha}{\\beta}.\n$$\n\nAn embedded Runge–Kutta (ERK) pair of orders $(p,p-1)$ produces two approximations $\\mathbf{y}^{[p]}$ and $\\mathbf{y}^{[p-1]}$ at the end of a step of size $h$, and their difference estimates the local truncation error. You must use an ERK pair with $p=3$ and $p-1=2$ to drive adaptive step size control. The step accept/reject decision must be made by comparing a Root Mean Square (RMS) norm of the component-wise scaled error to the threshold value $1$ as follows. If $\\mathbf{e}$ is the error estimate vector for one step, then define per-component scaling\n$$\n\\tau_i = a_i + r\\cdot \\max\\!\\big(|y_i|,|y_i^{\\text{new}}|\\big),\n$$\nwhere $a_i$ are absolute tolerances and $r$ is a relative tolerance, and accept the step if\n$$\n\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{e_i}{\\tau_i}\\right)^2\\right)^{1/2}\\le 1,\n$$\nwith $n=2$ for this system.\n\nYou must implement and compare two absolute tolerance schemes:\n- Scalar absolute tolerance: $a_i=a_{\\text{sc}}$ for all components, with\n$$\na_{\\text{sc}}=\\eta\\cdot \\frac{s_1+s_2}{2},\n$$\nwhere $\\eta>0$ is a user-specified scale.\n- Vector absolute tolerance: $a_i=\\eta\\cdot s_i$ for each component $i\\in\\{1,2\\}$.\n\nUse a standard adaptive step size update rule of the form\n$$\nh_{\\text{new}}=h\\cdot \\theta\\cdot \\left(\\max\\!\\big(\\varepsilon, \\min(M,\\rho)\\big)\\right),\n$$\nwhere $\\rho$ is a function of the error measure that reduces $h$ when the error is too large and increases $h$ when the error is small, and $\\theta\\in(0,1)$ is a safety factor, and $\\varepsilon$ and $M$ are minimum and maximum growth limiters. You must choose parameters so that $h_{\\text{new}}$ appropriately responds to the measured error from the chosen ERK pair with $p=3$.\n\nDefine the “embedded acceptance map” at a given state $(t,\\mathbf{y})$ as the mapping $h\\mapsto \\{\\text{accept},\\text{reject}\\}$ that results when a single ERK step of size $h$ is tested with the above criterion, without actually advancing the state. This map can be summarized by the largest tested $h$ that would be accepted.\n\nImplementation requirements:\n- Use the predator–prey vector field $\\mathbf{f}$ defined above.\n- Use an embedded Runge–Kutta pair of orders $(3,2)$ for local error estimation and adaptive step size control.\n- Implement both absolute tolerance schemes defined above with the same relative tolerance $r$.\n- For each simulation, initialize the first step size to $h_0=(t_1-t_0)/2$.\n- For the acceptance map at the initial state $(t_0,\\mathbf{y}_0)$, evaluate a logarithmically spaced grid of candidate step sizes $h$ over $[h_{\\min},h_{\\max}]$ with $h_{\\min}=10^{-4}(t_1-t_0)$ and $h_{\\max}=0.9(t_1-t_0)$, and summarize by the maximum $h$ that is accepted.\n\nTest suite:\nFor each of the following three test cases, conduct two tasks: an adaptive integration over $[t_0,t_1]$ with both tolerance schemes and the acceptance map analysis at $(t_0,\\mathbf{y}_0)$ with both schemes. For the integration, record the number of rejected steps. For the acceptance map, report the ratio $h_{\\max}^{\\text{vec}}/h_{\\max}^{\\text{sc}}$, where $h_{\\max}^{\\text{vec}}$ is the maximum accepted step size under the vector absolute tolerance and $h_{\\max}^{\\text{sc}}$ is the maximum accepted step size under the scalar absolute tolerance. Use the following parameter sets:\n- Case A (baseline): $\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}10.0\\\\5.0\\end{bmatrix}$, $[t_0,t_1]=[0.0,20.0]$, $r=10^{-3}$, $\\eta=10^{-6}$.\n- Case B (unbalanced initial condition): $\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}1000.0\\\\0.1\\end{bmatrix}$, $[t_0,t_1]=[0.0,5.0]$, $r=10^{-4}$, $\\eta=10^{-8}$.\n- Case C (near steady state): $\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}20.0(1+10^{-4})\\\\10.0(1-10^{-4})\\end{bmatrix}$, $[t_0,t_1]=[0.0,10.0]$, $r=10^{-6}$, $\\eta=10^{-10}$.\n\nFor each case, output a triple of values in order:\n- The integer number of rejected steps using the scalar absolute tolerance.\n- The integer number of rejected steps using the vector absolute tolerance.\n- The floating-point ratio $h_{\\max}^{\\text{vec}}/h_{\\max}^{\\text{sc}}$ rounded to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases A, B, C, each contributing its triple, flattened into a single list. For example, the output must be of the form\n$$\n[\\text{A\\_rej\\_sc},\\text{A\\_rej\\_vec},\\text{A\\_ratio},\\text{B\\_rej\\_sc},\\text{B\\_rej\\_vec},\\text{B\\_ratio},\\text{C\\_rej\\_sc},\\text{C\\_rej\\_vec},\\text{C\\_ratio}],\n$$\nwhere the first two entries of each triple are integers and the third is a decimal rounded to six places. No additional text may be printed.", "solution": "The user wants to solve a numerical analysis problem involving Ordinary Differential Equations (ODEs).The problem requires the design and implementation of an adaptive ODE solver to analyze the behavior of the Lotka-Volterra predator-prey model. The analysis focuses on comparing two different absolute tolerance schemes—scalar versus vector—within the framework of an embedded Runge-Kutta method.\n\n**1. The Predator-Prey Model**\n\nThe system is described by the autonomous system of first-order ODEs:\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t, \\mathbf{y}), \\quad \\text{with } \\mathbf{y}(t) = \\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix}\n$$\nHere, $y_1(t)$ represents the prey population and $y_2(t)$ represents the predator population. The vector field $\\mathbf{f}$ for the Lotka-Volterra model is given by:\n$$\n\\mathbf{f}(t, \\mathbf{y}) = \\begin{bmatrix}\n\\alpha y_1 - \\beta y_1 y_2 \\\\\n\\delta y_1 y_2 - \\gamma y_2\n\\end{bmatrix}\n$$\nwhere $\\alpha, \\beta, \\gamma, \\delta$ are positive parameters governing the population dynamics. The non-trivial steady state (or equilibrium point) $\\mathbf{s} = \\begin{bmatrix} s_1 \\\\ s_2 \\end{bmatrix}$ is found by setting $\\mathbf{f}(t, \\mathbf{s}) = \\mathbf{0}$, which yields:\n$$\ns_1 = \\frac{\\gamma}{\\delta}, \\quad s_2 = \\frac{\\alpha}{\\beta}\n$$\nThis steady state is a key reference point for defining the absolute tolerance schemes.\n\n**2. The Embedded Runge-Kutta (3,2) Method**\n\nTo solve the ODE system numerically, we employ an adaptive step-size strategy driven by an embedded Runge-Kutta (ERK) pair of orders $p=3$ and $p-1=2$. Such a pair provides two approximations at each step, a higher-order one $\\mathbf{y}^{[3]}$ and a lower-order one $\\mathbf{y}^{[2]}$. Their difference serves as an estimate of the local truncation error.\n\nWe will use the Bogacki-Shampine 3(2) pair, which is a popular choice for its efficiency, including the First Same As Last (FSAL) property. For a step from $t_n$ to $t_{n+1} = t_n + h$, the stages are computed as follows:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 &= \\mathbf{f}(t_n, \\mathbf{y}_n) \\\\\n\\mathbf{k}_2 &= \\mathbf{f}(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_1) \\\\\n\\mathbf{k}_3 &= \\mathbf{f}(t_n + \\frac{3}{4}h, \\mathbf{y}_n + \\frac{3}{4}h\\mathbf{k}_2)\n\\end{aligned}\n$$\nThe third-order approximation, which is used to advance the solution, is:\n$$\n\\mathbf{y}_{n+1}^{[3]} = \\mathbf{y}_n + h \\left( \\frac{2}{9}\\mathbf{k}_1 + \\frac{1}{3}\\mathbf{k}_2 + \\frac{4}{9}\\mathbf{k}_3 \\right)\n$$\nTo estimate the error, a fourth stage is computed. Due to the FSAL property, this stage is an evaluation at the new point $(t_{n+1}, \\mathbf{y}_{n+1}^{[3]})$, meaning it can be reused as the first stage ($\\mathbf{k}_1$) of the subsequent step if the current step is accepted.\n$$\n\\mathbf{k}_4 = \\mathbf{f}(t_{n+1}, \\mathbf{y}_{n+1}^{[3]})\n$$\nThe local error estimate vector $\\mathbf{e}_{n+1}$ is the difference between the third-order solution and an embedded second-order solution. It can be computed directly using the stages:\n$$\n\\mathbf{e}_{n+1} = \\mathbf{y}_{n+1}^{[3]} - \\mathbf{y}_{n+1}^{[2]} = h \\left( -\\frac{5}{72}\\mathbf{k}_1 + \\frac{1}{12}\\mathbf{k}_2 + \\frac{1}{9}\\mathbf{k}_3 - \\frac{1}{8}\\mathbf{k}_4 \\right)\n$$\n\n**3. Adaptive Step-Size Control**\n\nThe decision to accept or reject a step is based on a scaled root-mean-square (RMS) norm of the error estimate. First, a per-component tolerance scale $\\tau_i$ is defined:\n$$\n\\tau_i = a_i + r \\cdot \\max(|y_{n,i}|, |y_{n+1,i}^{[3]}|)\n$$\nwhere $a_i$ is the absolute tolerance for component $i$, $r$ is the relative tolerance, $y_{n,i}$ is the $i$-th component of the solution at the start of the step, and $y_{n+1,i}^{[3]}$ is the proposed value at the end of the step.\n\nThe error norm $E$ is then calculated as:\n$$\nE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{e_i}{\\tau_i}\\right)^2}\n$$\nFor our 2D system, $n=2$. The step is accepted if $E \\le 1$.\n\nIf the step is accepted, the new step size is calculated. If it is rejected, the current step is re-tried with a smaller step size. The formula for the new step size $h_{\\text{new}}$ is:\n$$\nh_{\\text{new}} = h \\cdot \\theta \\cdot \\max(\\varepsilon, \\min(M, E^{-1/p}))\n$$\n- $h$ is the current step size.\n- $p=3$ is the order of the method. The factor $E^{-1/p}$ is the ideal scaling based on local error theory.\n- $\\theta \\in (0,1)$ is a safety factor to prevent overly optimistic step size increases. We choose $\\theta=0.9$.\n- $\\varepsilon$ and $M$ are factors to limit the minimum and maximum change in step size. We choose $\\varepsilon=0.2$ and $M=5.0$.\n\n**4. Absolute Tolerance Schemes**\n\nThe problem requires a comparison of two schemes for setting the absolute tolerances $a_i$:\n- **Scalar Tolerance:** All components share the same absolute tolerance, $a_i = a_{\\text{sc}}$, which is based on the average of the steady-state values:\n  $$\n  a_{\\text{sc}} = \\eta \\cdot \\frac{s_1 + s_2}{2}\n  $$\n- **Vector Tolerance:** Each component has its own absolute tolerance, scaled by its corresponding steady-state value:\n  $$\n  a_i = \\eta \\cdot s_i \\quad \\text{for } i \\in \\{1, 2\\}\n  $$\nIn both cases, $\\eta$ is a small, user-specified parameter. This comparison highlights how tailoring tolerances to the characteristic scales of each variable can affect solver performance.\n\n**5. Analysis Tasks and Implementation**\n\nFor each test case, two analyses are performed:\n1.  **Adaptive Integration:** The ODE is integrated over the specified interval $[t_0, t_1]$. The total number of rejected steps is counted for both the scalar and vector tolerance schemes. This metric provides insight into the efficiency of each scheme.\n2.  **Acceptance Map Analysis:** At the initial state $(t_0, \\mathbf{y}_0)$, we determine the maximum step size $h$ that would be accepted by the error control criterion. This is done by testing a logarithmically spaced grid of candidate step sizes $h$ from $h_{\\min}=10^{-4}(t_1-t_0)$ to $h_{\\max}=0.9(t_1-t_0)$. The ratio of the maximum accepted step for the vector tolerance ($h_{\\max}^{\\text{vec}}$) to that for the scalar tolerance ($h_{\\max}^{\\text{sc}}$) is computed. This ratio quantifies the immediate impact of the tolerance choice on the permissible step size.\n\nThe implementation will consist of functions to perform a single ERK step, run the full adaptive integration, and analyze the acceptance map. These functions will be called for each test case specified in the problem statement to generate the required output values.", "answer": "```python\nimport numpy as np\nfrom functools import partial\n\ndef solve():\n    \"\"\"\n    Main solver function to orchestrate the test suite and generate the final output.\n    \"\"\"\n    \n    # ERK 3(2) Bogacki-Shampine coefficients (FSAL variant)\n    # The method advances with a 3rd-order solution. The error is estimated\n    # by taking the difference with an embedded 2nd-order solution.\n    # c: nodes, A: matrix, b: 3rd-order weights, e: error weights (b_3 - b_2)\n    C = np.array([0, 1/2, 3/4, 1])\n    A = np.array([\n        [0, 0, 0, 0],\n        [1/2, 0, 0, 0],\n        [0, 3/4, 0, 0],\n        [2/9, 1/3, 4/9, 0]\n    ])\n    B3 = np.array([2/9, 1/3, 4/9, 0])\n    B2 = np.array([7/24, 1/4, 1/3, 1/8])\n    E = B3 - B2  # [-5/72, 1/12, 1/9, -1/8]\n\n    # Adaptive step-size controller parameters\n    SAFETY_FACTOR = 0.9\n    MIN_GROWTH = 0.2\n    MAX_GROWTH = 5.0\n    METHOD_ORDER = 3\n\n    def lotka_volterra_rhs(t, y, alpha, beta, gamma, delta):\n        \"\"\"Lotka-Volterra predator-prey model RHS function.\"\"\"\n        y1, y2 = y\n        dy1_dt = alpha * y1 - beta * y1 * y2\n        dy2_dt = delta * y1 * y2 - gamma * y2\n        return np.array([dy1_dt, dy2_dt])\n\n    def perform_single_step(rhs, t, y, h, k1_in=None):\n        \"\"\"\n        Performs a single step of the Bogacki-Shampine 3(2) method.\n        Implements the FSAL (First Same As Last) property by accepting k1.\n        \"\"\"\n        k = np.zeros((4, len(y)))\n        \n        if k1_in is None:\n            k[0] = rhs(t, y)\n        else:\n            k[0] = k1_in\n        \n        k[1] = rhs(t + C[1] * h, y + h * A[1,0] * k[0])\n        k[2] = rhs(t + C[2] * h, y + h * (A[2,0] * k[0] + A[2,1] * k[1]))\n\n        y_new = y + h * (B3[0]*k[0] + B3[1]*k[1] + B3[2]*k[2])\n        \n        # This k4 evaluation can be used as k1 of the next step (FSAL).\n        k[3] = rhs(t + h, y_new)\n\n        error_estimate = h * (E[0]*k[0] + E[1]*k[1] + E[2]*k[2] + E[3]*k[3])\n        \n        return y_new, error_estimate, k[3]\n\n    def run_integration(rhs, y0, t_span, r, a):\n        \"\"\"Runs the adaptive integration and returns the number of rejected steps.\"\"\"\n        t0, t1 = t_span\n        t = t0\n        y = np.copy(y0)\n        h = (t1 - t0) / 2.0  # Initial step size per problem spec\n        rejected_steps = 0\n        k1_next = None\n\n        while t < t1:\n            if t + h > t1:\n                h = t1 - t\n\n            y_new, err_est, k4 = perform_single_step(rhs, t, y, h, k1_in=k1_next)\n\n            # Calculate error norm E\n            y_max_abs = np.maximum(np.abs(y), np.abs(y_new))\n            tol_scale = a + r * y_max_abs\n            \n            # Guard against division by zero if tolerance is zero\n            err_ratio = np.divide(err_est, tol_scale, out=np.zeros_like(err_est), where=tol_scale!=0)\n            \n            error_norm = np.sqrt(np.mean(err_ratio**2))\n\n            if error_norm <= 1.0: # Step accepted\n                t += h\n                y = y_new\n                k1_next = k4 # FSAL property\n            else: # Step rejected\n                rejected_steps += 1\n                k1_next = None # Must recompute k1 for the new, smaller step\n\n            # Update step size\n            if error_norm == 0:\n                scale_factor = MAX_GROWTH\n            else:\n                scale_factor = (error_norm)**(-1.0 / METHOD_ORDER)\n\n            h_new_unlimited = h * SAFETY_FACTOR * scale_factor\n            h = np.clip(h_new_unlimited, h * MIN_GROWTH, h * MAX_GROWTH)\n            \n        return rejected_steps\n\n    def analyze_acceptance_map(rhs, y0, t_span, r, a):\n        \"\"\"Finds the maximum accepted step size h at the initial point.\"\"\"\n        t0, t1 = t_span\n        h_min = 1e-4 * (t1 - t0)\n        h_max = 0.9 * (t1 - t0)\n        h_candidates = np.logspace(np.log10(h_min), np.log10(h_max), 500)\n        \n        max_h_accepted = 0.0\n\n        for h in h_candidates:\n            y_new, err_est, _ = perform_single_step(rhs, t0, y0, h)\n            \n            y_max_abs = np.maximum(np.abs(y0), np.abs(y_new))\n            tol_scale = a + r * y_max_abs\n            \n            err_ratio = np.divide(err_est, tol_scale, out=np.full_like(err_est, np.inf), where=tol_scale!=0)\n            \n            error_norm = np.sqrt(np.mean(err_ratio**2))\n            \n            if error_norm <= 1.0:\n                max_h_accepted = h\n        \n        return max_h_accepted\n\n    test_cases = [\n        # Case A (baseline)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([10.0, 5.0]), 't_span': [0.0, 20.0], 'r': 1e-3, 'eta': 1e-6},\n        # Case B (unbalanced initial condition)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([1000.0, 0.1]), 't_span': [0.0, 5.0], 'r': 1e-4, 'eta': 1e-8},\n        # Case C (near steady state)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([20.0*(1+1e-4), 10.0*(1-1e-4)]), 't_span': [0.0, 10.0], 'r': 1e-6, 'eta': 1e-10},\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        p = {k: case[k] for k in ('alpha', 'beta', 'gamma', 'delta')}\n        y0, t_span, r, eta = case['y0'], case['t_span'], case['r'], case['eta']\n        \n        rhs = partial(lotka_volterra_rhs, **p)\n\n        # Steady state\n        s1 = p['gamma'] / p['delta']\n        s2 = p['alpha'] / p['beta']\n\n        # Scalar absolute tolerance\n        a_sc_val = eta * (s1 + s2) / 2\n        a_sc = np.array([a_sc_val, a_sc_val])\n        \n        # Vector absolute tolerance\n        a_vec = eta * np.array([s1, s2])\n\n        # Task 1: Adaptive integration\n        rej_sc = run_integration(rhs, y0, t_span, r, a_sc)\n        rej_vec = run_integration(rhs, y0, t_span, r, a_vec)\n\n        # Task 2: Acceptance map analysis\n        h_max_sc = analyze_acceptance_map(rhs, y0, t_span, r, a_sc)\n        h_max_vec = analyze_acceptance_map(rhs, y0, t_span, r, a_vec)\n        \n        ratio = h_max_vec / h_max_sc if h_max_sc > 0 else 0.0\n\n        results.extend([rej_sc, rej_vec, ratio])\n\n    # Format the results for the final output string\n    formatted_results = []\n    for i, res in enumerate(results):\n        if (i + 1) % 3 == 0: # It's a ratio\n            formatted_results.append(f\"{res:.6f}\")\n        else: # It's a rejection count\n            formatted_results.append(str(res))\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3123484"}]}