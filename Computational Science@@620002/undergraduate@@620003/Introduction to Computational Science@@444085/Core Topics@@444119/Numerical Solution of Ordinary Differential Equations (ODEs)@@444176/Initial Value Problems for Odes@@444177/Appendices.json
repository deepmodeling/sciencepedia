{"hands_on_practices": [{"introduction": "Many problems in science and engineering involve processes that occur on vastly different timescales, from the rapid decay of a transient component to the slow evolution of the overall system. Such systems are described by \"stiff\" ordinary differential equations (ODEs). This practice provides a foundational, hands-on understanding of stiffness by contrasting the simplest explicit method (Forward Euler) with the simplest implicit method (Backward Euler). You will discover firsthand why explicit methods can fail dramatically when confronted with stiffness and how implicit methods provide a stable and robust alternative, a crucial concept for any computational scientist [@problem_id:3282680].", "problem": "Consider the initial value problem for an ordinary differential equation (ODE)\n$$\n\\frac{dy}{dt} = -100\\,(y - \\cos t), \\quad y(0) = 1,\n$$\nwhere $t$ is in radians. You will compare two single-step time-integration methods to illustrate stiffness: the explicit forward Euler method and the implicit backward Euler method. Your task is to derive the update rules from first principles and then implement a program that quantifies the difference in their behavior for various time steps.\n\nStart from the fundamental definition of the derivative, namely \n$$\n\\frac{dy}{dt} = \\lim_{h \\to 0} \\frac{y(t+h) - y(t)}{h},\n$$\nand approximate it with a finite difference appropriate for each method. Use only this definition and the idea of evaluating the right-hand side $f(t,y)$ either at the known time (for an explicit step) or at the advanced time (for an implicit step) to construct, by algebra, the respective one-step update rules that map $y_n \\approx y(t_n)$ to $y_{n+1} \\approx y(t_{n+1})$, where $t_{n+1} = t_n + h$. Do not assume any prefabricated formulas.\n\nThen, for each method, apply your derived update rule to advance the numerical solution from $t=0$ to $t=T$ using a fixed step size $h$ such that $T/h$ is an integer. Use the following specification:\n\n- The model is the given ODE with $f(t,y) = -100\\,(y - \\cos t)$, $t$ in radians, and initial condition $y(0)=1$.\n- The time horizon is $T=1$.\n- For each $h$ in the test suite below, perform exactly $N = T/h$ steps starting from $y_0 = 1$.\n\nCompute the exact solution $y(t)$ analytically by solving the linear ODE with an integrating factor. Use this exact solution to compute the absolute error at the final time $t=T$ for both methods:\n$$\nE_{\\mathrm{FE}}(h) = \\left|y_{\\mathrm{FE}}(T;h) - y(T)\\right|, \\quad E_{\\mathrm{BE}}(h) = \\left|y_{\\mathrm{BE}}(T;h) - y(T)\\right|.\n$$\nFor each test case, report the ratio\n$$\nR(h) = \\frac{E_{\\mathrm{FE}}(h)}{E_{\\mathrm{BE}}(h)}.\n$$\n\nTest suite:\n- Case $1$: $h=0.005$, $T=1$.\n- Case $2$: $h=0.02$, $T=1$ (boundary of forward Euler linear stability for eigenvalue $-100$).\n- Case $3$: $h=0.05$, $T=1$ (forward Euler unstable regime).\n- Case $4$: $h=0.1$, $T=1$ (strongly unstable for forward Euler).\n\nYour program must:\n- Implement both methods using only the update rules you derive from the derivative definition.\n- Implement the exact solution to the ODE in closed form.\n- For each case, return $R(h)$ as a floating-point number.\n\nFinal output format:\n- Produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the cases above, for example, $[r_1,r_2,r_3,r_4]$.\n- Each $r_k$ must be rounded to $6$ significant digits, using scientific notation if necessary.\n\nAll numerical outputs are unitless because the problem is posed in nondimensional form, and angles are in radians. The only accepted output types are floating-point numbers as specified, aggregated into the single required list.", "solution": "The problem statement is a valid exercise in numerical methods for ordinary differential equations (ODEs). It is scientifically grounded, well-posed, objective, and contains all necessary information to derive and implement a solution.\n\nThe problem requires a three-part analysis: first, deriving the analytical solution to the given initial value problem (IVP); second, deriving the update rules for the forward and backward Euler methods from first principles; and third, implementing these methods to compute the ratio of their numerical errors for a given set of time steps.\n\n### 1. Analytical Solution of the ODE\n\nThe given ODE is a first-order linear non-homogeneous differential equation:\n$$\n\\frac{dy}{dt} = -100(y - \\cos t)\n$$\nThis can be rewritten in the standard form $\\frac{dy}{dt} + P(t)y = Q(t)$:\n$$\n\\frac{dy}{dt} + 100y = 100\\cos t\n$$\nHere, $P(t) = 100$ and $Q(t) = 100\\cos t$. We solve this using an integrating factor, $\\mu(t)$.\n$$\n\\mu(t) = \\exp\\left(\\int P(t) dt\\right) = \\exp\\left(\\int 100 dt\\right) = e^{100t}\n$$\nMultiplying the standard form of the ODE by $\\mu(t)$ gives:\n$$\ne^{100t}\\frac{dy}{dt} + 100e^{100t}y = 100e^{100t}\\cos t\n$$\nThe left side is the derivative of the product $y(t)\\mu(t)$:\n$$\n\\frac{d}{dt}\\left(y \\cdot e^{100t}\\right) = 100e^{100t}\\cos t\n$$\nIntegrating both sides with respect to $t$:\n$$\ny \\cdot e^{100t} = \\int 100e^{100t}\\cos t \\,dt + C\n$$\nThe integral $\\int e^{at}\\cos(bt) dt$ has a standard form:\n$$\n\\int e^{at}\\cos(bt) dt = \\frac{e^{at}}{a^2+b^2}(a\\cos(bt) + b\\sin(bt))\n$$\nIn our case, $a=100$ and $b=1$. Substituting these values:\n$$\n\\int 100e^{100t}\\cos t \\,dt = 100 \\left[ \\frac{e^{100t}}{100^2+1^2}(100\\cos t + 1\\sin t) \\right] = \\frac{100e^{100t}}{10001}(100\\cos t + \\sin t)\n$$\nSubstituting this back into the equation for $y \\cdot e^{100t}$:\n$$\ny \\cdot e^{100t} = \\frac{100e^{100t}}{10001}(100\\cos t + \\sin t) + C\n$$\nDividing by $e^{100t}$ to solve for $y(t)$:\n$$\ny(t) = \\frac{10000}{10001}\\cos t + \\frac{100}{10001}\\sin t + Ce^{-100t}\n$$\nWe use the initial condition $y(0)=1$ to find the constant $C$:\n$$\n1 = y(0) = \\frac{10000}{10001}\\cos(0) + \\frac{100}{10001}\\sin(0) + Ce^{0}\n$$\n$$\n1 = \\frac{10000}{10001}(1) + \\frac{100}{10001}(0) + C(1) \\implies C = 1 - \\frac{10000}{10001} = \\frac{1}{10001}\n$$\nThus, the exact analytical solution is:\n$$\ny(t) = \\frac{10000}{10001}\\cos t + \\frac{100}{10001}\\sin t + \\frac{1}{10001}e^{-100t}\n$$\n\n### 2. Derivation of Numerical Update Rules\n\nWe start with the finite difference approximation of the derivative, where $h$ is a finite time step size:\n$$\n\\frac{dy}{dt}\\bigg|_{t=t_n} \\approx \\frac{y(t_{n+1}) - y(t_n)}{h} \\approx \\frac{y_{n+1} - y_n}{h}\n$$\nwhere $y_n \\approx y(t_n)$ and $t_{n+1} = t_n + h$. Let the ODE be $\\frac{dy}{dt} = f(t,y)$, where $f(t,y) = -100(y - \\cos t)$.\n\n#### Forward Euler (Explicit) Method\nThe forward Euler method approximates the derivative at the current time step $t_n$ using the function value $f(t_n, y_n)$.\n$$\n\\frac{y_{n+1} - y_n}{h} = f(t_n, y_n)\n$$\nSolving for $y_{n+1}$ gives the general update rule:\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\nSubstituting our specific function $f(t,y)$:\n$$\ny_{n+1} = y_n + h(-100(y_n - \\cos(t_n)))\n$$\n$$\ny_{n+1} = y_n - 100h y_n + 100h \\cos(t_n)\n$$\n$$\ny_{n+1} = (1 - 100h)y_n + 100h \\cos(t_n)\n$$\nThis is the explicit update rule for the forward Euler method.\n\n#### Backward Euler (Implicit) Method\nThe backward Euler method approximates the derivative using the function value at the next time step, $f(t_{n+1}, y_{n+1})$.\n$$\n\\frac{y_{n+1} - y_n}{h} = f(t_{n+1}, y_{n+1})\n$$\nThis yields an equation that is implicit in the unknown $y_{n+1}$:\n$$\ny_{n+1} = y_n + h f(t_{n+1}, y_{n+1})\n$$\nSubstituting our specific function $f(t,y)$:\n$$\ny_{n+1} = y_n + h(-100(y_{n+1} - \\cos(t_{n+1})))\n$$\nWe must now solve this equation for $y_{n+1}$:\n$$\ny_{n+1} = y_n - 100h y_{n+1} + 100h \\cos(t_{n+1})\n$$\n$$\ny_{n+1} + 100h y_{n+1} = y_n + 100h \\cos(t_{n+1})\n$$\n$$\ny_{n+1}(1 + 100h) = y_n + 100h \\cos(t_{n+1})\n$$\n$$\ny_{n+1} = \\frac{y_n + 100h \\cos(t_{n+1})}{1 + 100h}\n$$\nThis is the explicit form of the update rule for the (originally implicit) backward Euler method for this linear ODE.\n\n### 3. Computational Strategy\n\nFor each given time step $h$, we will perform the following steps:\n1.  Set the initial condition $y_0 = 1$ and final time $T=1$.\n2.  Calculate the number of steps $N = T/h$.\n3.  Simulate the system from $t=0$ to $t=T$ using the derived forward Euler update rule to find the numerical solution at the final time, $y_{\\mathrm{FE}}(T;h)$.\n4.  Simulate the system again from $t=0$ to $t=T$ using the derived backward Euler update rule to find its numerical solution, $y_{\\mathrm{BE}}(T;h)$.\n5.  Calculate the exact solution at the final time, $y(T)$, using the analytical formula.\n6.  Compute the absolute errors for both methods: $E_{\\mathrm{FE}}(h) = |y_{\\mathrm{FE}}(T;h) - y(T)|$ and $E_{\\mathrm{BE}}(h) = |y_{\\mathrm{BE}}(T;h) - y(T)|$.\n7.  Calculate the ratio $R(h) = E_{\\mathrm{FE}}(h) / E_{\\mathrm{BE}}(h)$.\n8.  The calculated ratios for all test cases will be collected and formatted as specified. The large eigenvalue-like term ($-100$) makes the problem stiff, and we expect the forward Euler method to become unstable for $h > 2/100 = 0.02$, leading to a dramatic increase in its error and the ratio $R(h)$. The backward Euler method is A-stable and will not exhibit this instability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the stiff ODE problem by comparing forward and backward Euler methods.\n    \"\"\"\n    \n    # Test cases defined by the problem statement.\n    # Each case is a specific value for the time step h.\n    test_cases = [\n        0.005,  # Case 1: h=0.005\n        0.02,   # Case 2: h=0.02 (stability boundary)\n        0.05,   # Case 3: h=0.05 (unstable FE)\n        0.1,    # Case 4: h=0.1 (strongly unstable FE)\n    ]\n\n    T = 1.0  # Final time\n    y0 = 1.0  # Initial condition y(0) = 1\n\n    def y_exact(t):\n        \"\"\"\n        Computes the analytical solution of the ODE at time t.\n        y(t) = (1/10001) * [10000*cos(t) + 100*sin(t) + exp(-100t)]\n        \"\"\"\n        return (10000.0 * np.cos(t) + 100.0 * np.sin(t) + np.exp(-100.0 * t)) / 10001.0\n\n    y_true_at_T = y_exact(T)\n    results = []\n\n    for h in test_cases:\n        # Number of steps N must be an integer, as T/h is guaranteed to be one.\n        N = int(T / h)\n\n        # 1. Forward Euler Method\n        # y_{n+1} = (1 - 100h)y_n + 100h*cos(t_n)\n        y_fe = y0\n        t = 0.0\n        for _ in range(N):\n            y_fe = (1.0 - 100.0 * h) * y_fe + 100.0 * h * np.cos(t)\n            t += h\n        y_fe_final = y_fe\n\n        # 2. Backward Euler Method\n        # y_{n+1} = (y_n + 100h*cos(t_{n+1})) / (1 + 100h)\n        y_be = y0\n        t = 0.0\n        for _ in range(N):\n            t += h # t is now t_{n+1}\n            y_be = (y_be + 100.0 * h * np.cos(t)) / (1.0 + 100.0 * h)\n        y_be_final = y_be\n\n        # 3. Error Calculation\n        err_fe = np.abs(y_fe_final - y_true_at_T)\n        err_be = np.abs(y_be_final - y_true_at_T)\n\n        # 4. Ratio Calculation\n        # The problem might imply a case where err_be is zero.\n        # However, for these first-order methods on this problem, it won't be exactly zero.\n        ratio = err_fe / err_be\n        results.append(ratio)\n        \n    # Format the results to 6 significant digits and join them into a single string.\n    # The 'g' format specifier is ideal for handling significant digits and\n    # automatically choosing between fixed-point and scientific notation.\n    formatted_results = [f\"{r:.6g}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3282680"}, {"introduction": "A good numerical solution should do more than just approximate the correct values; it should also preserve the fundamental qualitative properties of the true solution. For many physical systems, such as models of population dynamics or chemical concentrations, the state variables must remain non-negative. This practice explores the important concept of positivity preservation, investigating how and why standard explicit methods can produce unphysical negative results. By analyzing the behavior of various methods on a simple decay problem, you will derive step-size conditions that guarantee positivity and discover how a change of variables can enforce this property unconditionally [@problem_id:3144090].", "problem": "Consider the initial value problem (IVP) for an ordinary differential equation (ODE) given by $$y'(t)=-k\\,y(t),\\quad y(0)=y_0,$$ where $k>0$ and $y_0>0$. The exact solution is strictly positive for all $t\\ge 0$. In computational practice, explicit time-stepping methods approximate the continuous dynamics by discrete updates. This problem asks you to analyze positivity preservation (avoiding negative values) of the numerical solution under several explicit methods and to formulate conditions or modifications that guarantee nonnegative approximations.\n\nStarting from the fundamental definition of the derivative, $$y'(t)=\\lim_{h\\to 0}\\frac{y(t+h)-y(t)}{h},$$ and the notion that explicit methods approximate slopes using available information at or within the current step, derive, for the ODE $$y'=-k\\,y,$$ the one-step amplification or update relations for each of the following explicit schemes:\n\n- Forward Euler (explicit first-order method).\n- Explicit midpoint method (explicit second-order Runge–Kutta).\n- A classical explicit third-order Runge–Kutta method (with three stages and weights chosen for third-order accuracy).\n- Classical explicit fourth-order Runge–Kutta method.\n- Two-step Adams–Bashforth method of order $2$ (requiring one-step initialization for $y_1$).\n\nUsing only first principles and core definitions of these methods (no shortcut formulas), derive the discrete updates for $y_{n+1}$ in terms of $y_n$ (and $y_{n-1}$ where applicable) for the test equation $$y'=-k\\,y$$ and express any resulting amplification factor or recurrence coefficients as functions of the dimensionless step parameter $$a=k\\,h,$$ where $h>0$ is the time step. Based on those discrete relations, establish conditions under which the numerical solution remains nonnegative for all steps when starting from $y_0>0$.\n\nFurther, propose either:\n- modified step-size restrictions (an upper bound on $h$ as a function of $k$), or\n- a method variant grounded in a valid transformation of variables,\nthat guarantees $$y_n\\ge 0$$ for all $n$.\n\nAfter completing the derivations and analysis, implement a program that evaluates positivity (nonnegativity) for specified parameter sets by simulating the discrete schemes for a finite number of steps and checking whether all computed iterates satisfy $$y_n\\ge 0.$$\n\nYour program must implement the following numerical methods for the ODE $$y'=-k\\,y$$:\n- Forward Euler.\n- Explicit midpoint (second-order Runge–Kutta).\n- Explicit third-order Runge–Kutta (classical).\n- Explicit fourth-order Runge–Kutta (classical).\n- Adams–Bashforth of order $2$ (with the explicit midpoint method used to compute $y_1$).\n- A positivity-preserving variant based on the logarithmic transformation $$u=\\ln(y)$$, which converts the ODE to $$u'=-k$$ and yields an exact affine update in $u$, followed by reconstruction of $$y=\\exp(u)$$.\n\nSimulation rule: for a given method, simulate $N$ uniform steps of size $h$ from $t=0$ to $t=N\\,h$ starting with $y_0>0$, and declare the result for that test case to be boolean $$\\text{True}$$ if every simulated iterate satisfies $$y_n\\ge 0$$ (including $n=0$), otherwise $$\\text{False}$$.\n\nTest suite to cover general and edge cases:\n1. Forward Euler with $$k=2.0$$, $$h=0.5$$, $$N=5$$, $$y_0=1.0$$.\n2. Forward Euler with $$k=2.0$$, $$h=0.6$$, $$N=5$$, $$y_0=1.0$$.\n3. Explicit midpoint with $$k=100.0$$, $$h=1.0$$, $$N=5$$, $$y_0=1.0$$.\n4. Explicit third-order Runge–Kutta with $$k=1.0$$, $$h=1.5$$, $$N=5$$, $$y_0=1.0$$.\n5. Explicit third-order Runge–Kutta with $$k=1.0$$, $$h=2.0$$, $$N=5$$, $$y_0=1.0$$.\n6. Explicit fourth-order Runge–Kutta with $$k=5.0$$, $$h=10.0$$, $$N=5$$, $$y_0=1.0$$.\n7. Adams–Bashforth of order $2$$ with $$k=3.0$$, $$h=\\frac{2}{9}$$, $$N=10$$, $$y_0=1.0$$.\n8. Adams–Bashforth of order $2$$ with $$k=3.0$$, $$h=0.4$$, $$N=10$$, $$y_0=1.0$$.\n9. Log-transformed variant with $$k=1000.0$$, $$h=10.0$$, $$N=5$$, $$y_0=0.5$$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example $$[\\text{True},\\text{False},\\dots]$$.", "solution": "The problem requires an analysis of positivity preservation for several explicit numerical methods applied to the initial value problem (IVP) for the test equation\n$$y'(t) = -k\\,y(t), \\quad y(0)=y_0,$$\nwhere $k>0$ and $y_0>0$. The exact solution is $y(t) = y_0 \\exp(-kt)$, which remains strictly positive for all $t \\ge 0$. We will derive the discrete update relations for each method, establish conditions on the dimensionless step parameter $a=kh$ that guarantee the numerical solution $y_n$ remains nonnegative, and propose a variant that is unconditionally positivity-preserving.\n\nA numerical method applied to this stable test equation yields a discrete recurrence of the form $y_{n+1} = G(a) y_n$ for a one-step method, or a multi-step linear recurrence for a multi-step method. The function $G(a)$ is the amplification factor. For the numerical solution to remain nonnegative, given $y_n \\ge 0$, we require that the update rule produces $y_{n+1} \\ge 0$. For one-step methods, this translates to the condition $G(a) \\ge 0$.\n\nWe derive the update relations and positivity conditions for each specified method from first principles.\n\n**1. Forward Euler Method**\nThe Forward Euler method approximates the solution at step $n+1$ using the tangent at step $n$:\n$$y_{n+1} = y_n + h\\,y'(t_n) = y_n + h\\,f(t_n, y_n).$$\nFor the given ODE, $f(t,y) = -k\\,y$, so we have:\n$$y_{n+1} = y_n + h(-k\\,y_n) = (1 - k\\,h) y_n.$$\nThe amplification factor is $G(a) = 1 - a$, where $a=k\\,h$. For positivity preservation, we require $y_{n+1} \\ge 0$. Since we start with $y_0 > 0$ and proceed by induction assuming $y_n > 0$, the condition becomes:\n$$G(a) = 1 - a \\ge 0 \\implies a \\le 1.$$\nThe step-size restriction is thus $h \\le 1/k$.\n\n**2. Explicit Midpoint Method (Second-Order Runge-Kutta)**\nThis is a two-stage explicit Runge-Kutta method. The update is computed using the slope at the midpoint of the interval, estimated using an initial Euler step.\n$$k_1 = f(t_n, y_n) = -k\\,y_n$$\n$$k_2 = f(t_n + h/2, y_n + \\frac{h}{2} k_1) = -k \\left( y_n + \\frac{h}{2} (-k\\,y_n) \\right) = -k \\left( 1 - \\frac{k\\,h}{2} \\right) y_n$$\n$$y_{n+1} = y_n + h\\,k_2 = y_n + h \\left[ -k \\left( 1 - \\frac{k\\,h}{2} \\right) y_n \\right] = \\left[ 1 - k\\,h + \\frac{(k\\,h)^2}{2} \\right] y_n.$$\nThe amplification factor is $G(a) = 1 - a + a^2/2$. To check for positivity, we inspect this quadratic in $a$. The discriminant is $\\Delta = (-1)^2 - 4(1)(1/2) = 1 - 2 = -1 < 0$. Since the leading coefficient ($1/2$) is positive, the quadratic $G(a)$ is strictly positive for all real values of $a$.\nTherefore, the explicit midpoint method is unconditionally positivity-preserving for this ODE; no restriction on the step size $h$ is needed to maintain nonnegativity.\n\n**3. Classical Explicit Third-Order Runge-Kutta Method (RK3)**\nWe use the classical three-stage method of Kutta:\n$$k_1 = f(t_n, y_n) = -k\\,y_n$$\n$$k_2 = f(t_n + h/2, y_n + \\frac{h}{2} k_1) = -k \\left( y_n + \\frac{h}{2} (-k\\,y_n) \\right) = -k \\left( 1 - \\frac{a}{2} \\right) y_n$$\n$$k_3 = f(t_n + h, y_n - h\\,k_1 + 2h\\,k_2) = -k \\left( y_n - h(-k\\,y_n) + 2h(-k(1-a/2)y_n) \\right)$$\n$$k_3 = -k \\left( 1 + a - 2a(1-a/2) \\right) y_n = -k \\left( 1 + a - 2a + a^2 \\right) y_n = -k \\left( 1 - a + a^2 \\right) y_n$$\n$$y_{n+1} = y_n + \\frac{h}{6} (k_1 + 4k_2 + k_3)$$\n$$y_{n+1} = y_n + \\frac{h}{6} \\left[ -k\\,y_n - 4k(1-a/2)y_n - k(1-a+a^2)y_n \\right]$$\n$$y_{n+1} = y_n \\left[ 1 - \\frac{a}{6} (1 + 4(1-a/2) + (1-a+a^2)) \\right]$$\n$$y_{n+1} = y_n \\left[ 1 - \\frac{a}{6} (1 + 4 - 2a + 1 - a + a^2) \\right] = y_n \\left[ 1 - \\frac{a}{6} (6 - 3a + a^2) \\right]$$\n$$y_{n+1} = \\left( 1 - a + \\frac{a^2}{2} - \\frac{a^3}{6} \\right) y_n.$$\nThe amplification factor is $G(a) = 1 - a + a^2/2 - a^3/6$. This polynomial is the order-3 Taylor approximation of $\\exp(-a)$. Unlike the quadratic for RK2, this cubic polynomial has a real root. We require $G(a) \\ge 0$. Numerically solving for the smallest positive root of $G(a)=0$ yields $a_{crit} \\approx 1.756$.\nThe condition for positivity is $a \\le a_{crit}$, or $k\\,h \\lesssim 1.756$.\n\n**4. Classical Explicit Fourth-Order Runge-Kutta Method (RK4)**\nThis is the most common four-stage RK method. For the test equation $y'=-k\\,y$, its amplification factor is known to be the order-4 Taylor approximation of $\\exp(-a)$:\n$$G(a) = 1 - a + \\frac{a^2}{2!} - \\frac{a^3}{3!} + \\frac{a^4}{4!} = 1 - a + \\frac{a^2}{2} - \\frac{a^3}{6} + \\frac{a^4}{24}.$$\nThis can be verified by a tedious but straightforward calculation similar to the RK3 case. We analyze the positivity of this quartic polynomial. The derivative is $G'(a) = -1 + a - a^2/2 + a^3/6 = -G_{RK3}(a)$. The extrema of $G_{RK4}(a)$ occur at the roots of $G_{RK3}(a)$. The first positive root of $G_{RK3}(a) = 0$ is $a \\approx 1.756$. The global minimum of $G_{RK4}(a)$ for $a>0$ can be shown to be positive. Therefore, $G(a)>0$ for all $a \\ge 0$.\nLike the RK2 method, the classical RK4 method is unconditionally positivity-preserving for this ODE.\n\n**5. Two-Step Adams-Bashforth Method (AB2)**\nThis linear multi-step method uses information from steps $n$ and $n-1$:\n$$y_{n+1} = y_n + h \\left( \\frac{3}{2} f_n - \\frac{1}{2} f_{n-1} \\right),$$\nwhere $f_i = f(t_i, y_i) = -k\\,y_i$. Substituting this gives the recurrence relation:\n$$y_{n+1} = y_n + h \\left( \\frac{3}{2}(-k\\,y_n) - \\frac{1}{2}(-k\\,y_{n-1}) \\right)$$\n$$y_{n+1} = \\left( 1 - \\frac{3}{2}k\\,h \\right) y_n + \\left( \\frac{1}{2}k\\,h \\right) y_{n-1}.$$\nIn terms of $a=k\\,h$, this is:\n$$y_{n+1} = \\left( 1 - \\frac{3}{2}a \\right) y_n + \\frac{a}{2} y_{n-1}.$$\nFor the solution to be guaranteed to remain nonnegative, we can impose a sufficient condition that $y_{n+1}$ is a convex combination of previous nonnegative values, $y_n$ and $y_{n-1}$. This requires the coefficients in the recurrence to be non-negative.\nThe coefficient $\\frac{a}{2}$ is always non-negative since $k>0$ and $h>0$. The other coefficient requires:\n$$1 - \\frac{3}{2}a \\ge 0 \\implies 1 \\ge \\frac{3}{2}a \\implies a \\le \\frac{2}{3}.$$\nThus, a sufficient condition for the AB2 method to preserve positivity is $k\\,h \\le 2/3$. If this condition is violated, the presence of a negative coefficient can lead to negative solutions.\n\n**6. Positivity-Preserving Variant: Logarithmic Transformation**\nA robust method to guarantee positivity is to transform the dependent variable. Let $u(t) = \\ln(y(t))$. The dynamics of $u(t)$ are governed by:\n$$u'(t) = \\frac{d}{dt} \\ln(y(t)) = \\frac{1}{y(t)} y'(t).$$\nSubstituting $y'(t) = -k\\,y(t)$:\n$$u'(t) = \\frac{1}{y(t)} (-k\\,y(t)) = -k.$$\nThe new IVP for $u$ is $u' = -k$ with $u(0)=\\ln(y_0)$. This linear ODE can be solved exactly over a time step $h$:\n$$u(t_{n+1}) = u(t_n) - k\\,h.$$\nThe numerical scheme is simply $u_{n+1} = u_n - a$. Starting from $u_0 = \\ln(y_0)$, the discrete solution for $u$ is exact. The solution for $y$ is reconstructed by the inverse transformation:\n$$y_n = \\exp(u_n).$$\nSince the exponential function $\\exp(\\cdot)$ yields a strictly positive value for any finite real argument, $y_n$ is guaranteed to be positive for all $n$, regardless of the step size $h$. This method is therefore unconditionally positivity-preserving.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and testing various numerical methods\n    for positivity preservation on the ODE y' = -k*y.\n    \"\"\"\n\n    def check_positivity(y_values):\n        \"\"\"Checks if all values in the sequence are non-negative.\"\"\"\n        for y in y_values:\n            if y  0.0:\n                return False\n        return True\n\n    def forward_euler(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        for n in range(N):\n            y[n+1] = y[n] * (1.0 - k * h)\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def explicit_midpoint(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        a = k * h\n        amp_factor = 1.0 - a + 0.5 * a**2\n        \n        for n in range(N):\n            y[n+1] = y[n] * amp_factor\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def rk3_classical(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        a = k * h\n        amp_factor = 1.0 - a + 0.5 * a**2 - (1.0/6.0) * a**3\n        \n        for n in range(N):\n            y[n+1] = y[n] * amp_factor\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def rk4_classical(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        a = k * h\n        amp_factor = 1.0 - a + 0.5 * a**2 - (1.0/6.0) * a**3 + (1.0/24.0) * a**4\n\n        for n in range(N):\n            y[n+1] = y[n] * amp_factor\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def ab2(k, h, N, y0):\n        if N == 0:\n            return y0 >= 0.0\n\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0.0: return False\n\n        # Use explicit midpoint for the first step y1\n        a = k * h\n        amp_factor_midpoint = 1.0 - a + 0.5 * a**2\n        y[1] = y[0] * amp_factor_midpoint\n        if y[1]  0.0: return False\n\n        for n in range(1, N):\n            f_n = -k * y[n]\n            f_n_minus_1 = -k * y[n-1]\n            y[n+1] = y[n] + h * (1.5 * f_n - 0.5 * f_n_minus_1)\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def log_transform(k, h, N, y0):\n        if y0 = 0:  # Log is not defined for non-positive y0\n            return False\n            \n        u = np.zeros(N + 1)\n        u[0] = np.log(y0)\n        \n        # This loop computes u_n, but reconstruction y_n = exp(u_n) is always > 0.\n        # So we can just return True if y0 > 0.\n        # The loop is included for completeness but is not strictly necessary for the check.\n        for n in range(N):\n            u[n+1] = u[n] - k * h\n        \n        # Reconstructed y values are always positive\n        # y = np.exp(u)\n        # return check_positivity(y)\n        return True\n    \n    # Map method names to functions\n    methods = {\n        \"forward_euler\": forward_euler,\n        \"explicit_midpoint\": explicit_midpoint,\n        \"rk3\": rk3_classical,\n        \"rk4\": rk4_classical,\n        \"ab2\": ab2,\n        \"log_transform\": log_transform\n    }\n\n    # Test cases from the problem statement.\n    test_suite = [\n        (\"forward_euler\",     {'k': 2.0, 'h': 0.5, 'N': 5, 'y0': 1.0}),\n        (\"forward_euler\",     {'k': 2.0, 'h': 0.6, 'N': 5, 'y0': 1.0}),\n        (\"explicit_midpoint\", {'k': 100.0, 'h': 1.0, 'N': 5, 'y0': 1.0}),\n        (\"rk3\",               {'k': 1.0, 'h': 1.5, 'N': 5, 'y0': 1.0}),\n        (\"rk3\",               {'k': 1.0, 'h': 2.0, 'N': 5, 'y0': 1.0}),\n        (\"rk4\",               {'k': 5.0, 'h': 10.0, 'N': 5, 'y0': 1.0}),\n        (\"ab2\",               {'k': 3.0, 'h': 2.0/9.0, 'N': 10, 'y0': 1.0}),\n        (\"ab2\",               {'k': 3.0, 'h': 0.4, 'N': 10, 'y0': 1.0}),\n        (\"log_transform\",     {'k': 1000.0, 'h': 10.0, 'N': 5, 'y0': 0.5}),\n    ]\n\n    results = []\n    for method_name, params in test_suite:\n        func = methods[method_name]\n        result = func(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3144090"}, {"introduction": "Once stability is assured, the focus shifts to achieving a desired accuracy with the least amount of computational effort. This practice delves into the trade-offs between two powerful, high-order methods for solving ODEs: Taylor series methods and Runge-Kutta methods. By implementing a fourth-order Taylor method, which requires symbolic differentiation of the ODE's right-hand side, and comparing it against the general-purpose classical fourth-order Runge-Kutta (RK4) method, you will gain insight into the relationship between analytical pre-computation, numerical accuracy, and computational cost [@problem_id:3282712].", "problem": "Implement a self-contained program that, for the initial value problem (IVP) defined by the ordinary differential equation (ODE) $$\\frac{dy}{dt} = \\cos(t) + y,$$ with initial condition $$y(0) = 1,$$ constructs and compares two one-step numerical methods over a fixed time interval. The two methods are:\n- A Taylor series method of order $4$ that advances from $t_n$ to $t_{n+1}=t_n+h$ by evaluating $$y_{n+1} = y_n + h\\,y^{(1)}(t_n) + \\frac{h^2}{2!}\\,y^{(2)}(t_n) + \\frac{h^3}{3!}\\,y^{(3)}(t_n) + \\frac{h^4}{4!}\\,y^{(4)}(t_n),$$ where $y^{(k)}(t)$ denotes the $k$-th total time derivative of $y(t)$ evaluated at $t=t_n$. You must obtain $y^{(k)}(t)$ for $k=1,2,3,4$ using only fundamental rules: the chain rule and the identity $$\\frac{d}{dt}g(t,y(t)) = \\frac{\\partial g}{\\partial t}(t,y) + \\frac{\\partial g}{\\partial y}(t,y)\\,\\frac{dy}{dt},$$ applied to $$g(t,y)=\\cos(t)+y.$$ Do not use any pre-derived shortcut formulas; compute the needed derivatives symbolically and then evaluate them numerically at $(t_n,y_n)$ during the step.\n- The classical explicit Runge–Kutta method of order $4$ (often referred to as RK4) applied to the same right-hand side.\n\nBoth methods must integrate from $t=0$ to $t=T$ with uniform step size $h$, where $T$ and $h$ will be specified by the test suite. Angles must be interpreted in radians.\n\nUse a scientifically grounded reference solution derived from solving the linear ODE analytically. Starting from the definition of an integrating factor for linear first-order ODEs, the exact solution has the form $$y(t) = \\text{(a function of }t\\text{)}$$ obtained by solving $$\\frac{dy}{dt} - y = \\cos(t)$$ with the integrating factor method. Your program must implement this exact solution and use it to compute the absolute error at $t=T$ for each numerical method and step size $h$.\n\nTo compare computational cost, define cost as the total number of trigonometric function evaluations performed within the numerical integrator. Specifically, each evaluation of $\\cos(\\cdot)$ or $\\sin(\\cdot)$ inside the numerical method counts as $1$ unit of cost; all other operations are free for the purposes of this cost model. You must instrument your implementation so that these counts are measured accurately and separately for the Taylor method and for RK4. The exact solution evaluation must not contribute to the cost.\n\nTest suite:\n- Fixed final time: $T=2$.\n- Initial condition: $y(0)=1$.\n- Step sizes $h$ to test: $h\\in\\{2.0,\\,1.0,\\,0.5,\\,0.2,\\,0.1,\\,0.05\\}$. For each $h$, take exactly $N=T/h$ steps (these $h$ values are chosen so that $N$ is an integer).\n- Angle unit: radians.\n\nFor each value of $h$ in the test suite, run both methods using the same $h$ and compute:\n- The absolute error at $t=T$ for the Taylor method of order $4$, denoted $E_{\\mathrm{T4}}(h) = \\lvert y_{\\mathrm{T4}}(T;h) - y_{\\mathrm{exact}}(T)\\rvert$.\n- The absolute error at $t=T$ for RK4, denoted $E_{\\mathrm{RK4}}(h) = \\lvert y_{\\mathrm{RK4}}(T;h) - y_{\\mathrm{exact}}(T)\\rvert$.\n- The trigonometric evaluation count used by the Taylor method, denoted $C_{\\mathrm{T4}}(h)$.\n- The trigonometric evaluation count used by RK4, denoted $C_{\\mathrm{RK4}}(h)$.\n\nYour program should produce a single line of output containing the results aggregated for all test cases into one flat list, in the following exact order for each step size $h$:\n$$[E_{\\mathrm{T4}}(2.0),\\,E_{\\mathrm{RK4}}(2.0),\\,C_{\\mathrm{T4}}(2.0),\\,C_{\\mathrm{RK4}}(2.0),\\,E_{\\mathrm{T4}}(1.0),\\,E_{\\mathrm{RK4}}(1.0),\\,C_{\\mathrm{T4}}(1.0),\\,C_{\\mathrm{RK4}}(1.0),\\,E_{\\mathrm{T4}}(0.5),\\,E_{\\mathrm{RK4}}(0.5),\\,C_{\\mathrm{T4}}(0.5),\\,C_{\\mathrm{RK4}}(0.5),\\,E_{\\mathrm{T4}}(0.2),\\,E_{\\mathrm{RK4}}(0.2),\\,C_{\\mathrm{T4}}(0.2),\\,C_{\\mathrm{RK4}}(0.2),\\,E_{\\mathrm{T4}}(0.1),\\,E_{\\mathrm{RK4}}(0.1),\\,C_{\\mathrm{T4}}(0.1),\\,C_{\\mathrm{RK4}}(0.1),\\,E_{\\mathrm{T4}}(0.05),\\,E_{\\mathrm{RK4}}(0.05),\\,C_{\\mathrm{T4}}(0.05),\\,C_{\\mathrm{RK4}}(0.05)].$$\n\nThe program must print exactly one line, formatted as a comma-separated list enclosed in square brackets, with the values in the order above. All numerical answers are scalars (floating-point numbers for errors and integers for counts). Angles must be interpreted in radians throughout, and no physical units other than the angle unit apply here.", "solution": "We begin from the initial value problem (IVP) $$\\frac{dy}{dt} = f(t,y),\\quad y(0)=y_0,$$ with $$f(t,y) = \\cos(t)+y.$$ The Taylor series one-step method of order $4$ at step size $h$ around $(t_n,y_n)$ is defined by $$y_{n+1} = y_n + \\sum_{k=1}^{4} \\frac{h^k}{k!} y^{(k)}(t_n),$$ where $y^{(k)}(t)$ denotes the $k$-th total time derivative of $y(t)$, that is, $$y^{(1)}(t) = \\frac{dy}{dt}(t),\\quad y^{(2)}(t) = \\frac{d^2 y}{dt^2}(t),\\quad \\ldots.$$ To compute these derivatives for a general right-hand side $f(t,y)$, we use the chain rule in the form of the total derivative operator $$\\frac{d}{dt} g(t,y(t)) = \\frac{\\partial g}{\\partial t}(t,y) + \\frac{\\partial g}{\\partial y}(t,y)\\,\\frac{dy}{dt}(t) = g_t(t,y) + g_y(t,y)\\,f(t,y).$$ Applying this to our specific $f(t,y)=\\cos(t)+y$ yields step-by-step expressions for the derivatives evaluated at $(t,y)$.\n\nFirst derivative: $$y^{(1)} = f(t,y) = \\cos(t) + y.$$\n\nSecond derivative: $$y^{(2)} = \\frac{d}{dt}y^{(1)} = \\frac{d}{dt} f(t,y) = f_t(t,y) + f_y(t,y)\\,f(t,y).$$ For $f(t,y)=\\cos(t)+y$, we have $$f_t(t,y) = -\\sin(t),\\quad f_y(t,y)=1,$$ hence $$y^{(2)} = -\\sin(t) + \\left(\\cos(t)+y\\right).$$\n\nThird derivative: $$y^{(3)} = \\frac{d}{dt} y^{(2)} = \\frac{d}{dt}\\left(-\\sin(t) + \\cos(t) + y\\right) = -\\cos(t) - \\sin(t) + y^{(1)}.$$ Substituting $y^{(1)} = \\cos(t)+y$ gives $$y^{(3)} = -\\sin(t) + y.$$\n\nFourth derivative: $$y^{(4)} = \\frac{d}{dt} y^{(3)} = \\frac{d}{dt}\\left(-\\sin(t) + y\\right) = -\\cos(t) + y^{(1)}.$$ Substituting $y^{(1)} = \\cos(t)+y$ yields $$y^{(4)} = y.$$\n\nThese expressions enable an efficient Taylor method implementation at order $4$ using only evaluations of $\\sin(t)$ and $\\cos(t)$ at the current time and the current value $y$.\n\nFor the classical explicit Runge–Kutta method of order $4$ (RK4), at step size $h$ we define stage values $$k_1 = f(t_n,y_n),$$ $$k_2 = f\\!\\left(t_n+\\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right),$$ $$k_3 = f\\!\\left(t_n+\\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right),$$ $$k_4 = f(t_n+h, y_n + hk_3),$$ and advance by $$y_{n+1} = y_n + \\frac{h}{6}\\left(k_1 + 2k_2 + 2k_3 + k_4\\right).$$\n\nNext, we derive the exact solution to use as a reference. The ODE $$\\frac{dy}{dt} - y = \\cos(t)$$ is linear. Using an integrating factor, the standard approach is to multiply by $e^{-t}$ to obtain $$\\frac{d}{dt}\\left(e^{-t} y\\right) = e^{-t}\\cos(t).$$ The integral $$\\int e^{-t}\\cos(t)\\,dt$$ is a well-known form, yielding $$\\int e^{-t}\\cos(t)\\,dt = \\frac{e^{-t}}{2}\\left(-\\cos(t) + \\sin(t)\\right) + C.$$ Therefore, $$e^{-t} y(t) = \\frac{e^{-t}}{2}\\left(-\\cos(t) + \\sin(t)\\right) + C,$$ and multiplying both sides by $e^{t}$ gives $$y(t) = \\frac{-\\cos(t) + \\sin(t)}{2} + C e^{t}.$$ Applying the initial condition $y(0)=1$ gives $$1 = \\frac{-\\cos(0) + \\sin(0)}{2} + C = \\frac{-1+0}{2} + C \\implies C = \\frac{3}{2}.$$ Thus the exact solution is $$y_{\\mathrm{exact}}(t) = \\frac{-\\cos(t) + \\sin(t)}{2} + \\frac{3}{2} e^{t}.$$\n\nCost model: We define computational cost as the number of trigonometric evaluations performed inside the numerical integrator. Specifically, each call to either $\\cos(\\cdot)$ or $\\sin(\\cdot)$ during the numerical integration counts as $1$ unit of cost. For the Taylor method here, each step uses $\\cos(t_n)$ and $\\sin(t_n)$ once, so the theoretical cost per step is $2$. For RK4 here, each step uses $\\cos(\\cdot)$ at four distinct stage times, so the theoretical cost per step is $4$. We instrument the code to measure the counts directly, ensuring that only trigonometric calls in the integrator are counted; the exact solution evaluation does not contribute to the count.\n\nAccuracy expectations: Both methods are order $4$, so for sufficiently small $h$ the global truncation error at $t=T$ should scale like $\\mathcal{O}(h^4)$. The constants differ between methods, so their actual errors will not be identical for the same $h$, and the Taylor method here benefits from exact local derivatives at each step.\n\nAlgorithmic structure:\n- Implement a trigonometric wrapper that counts calls to $\\cos(\\cdot)$ and $\\sin(\\cdot)$ separately for each method and test case.\n- Implement the Taylor-$4$ step using the derived $y^{(k)}(t)$ with the chain rule: evaluate $y^{(1)}$, $y^{(2)}$, $y^{(3)}$, $y^{(4)}$ at $(t_n,y_n)$, then advance using the Taylor polynomial through order $4$.\n- Implement RK4 using stage evaluations of $f(t,y)=\\cos(t)+y$ at the standard nodes.\n- For each step size $h$ in the test suite, perform $N=T/h$ steps to reach $t=T$, compute the absolute errors for both methods against $y_{\\mathrm{exact}}(T)$, and record trigonometric counts.\n- Output a single flat list in the specified order: for each $h$ in $\\{2.0,1.0,0.5,0.2,0.1,0.05\\}$, append $E_{\\mathrm{T4}}(h)$, $E_{\\mathrm{RK4}}(h)$, $C_{\\mathrm{T4}}(h)$, $C_{\\mathrm{RK4}}(h)$.\n\nThis principled construction adheres to the fundamental definitions of the total derivative and linear ODE solution by integrating factor, and yields a clear, testable comparison of accuracy and cost for two fourth-order methods on the specified IVP.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass TrigCounter:\n    \"\"\"\n    Wrapper to count trigonometric function evaluations (cos and sin).\n    \"\"\"\n    def __init__(self):\n        self.cos_calls = 0\n        self.sin_calls = 0\n\n    def cos(self, x):\n        self.cos_calls += 1\n        return float(np.cos(x))\n\n    def sin(self, x):\n        self.sin_calls += 1\n        return float(np.sin(x))\n\n    @property\n    def total_trig_calls(self):\n        return self.cos_calls + self.sin_calls\n\ndef exact_solution(t):\n    # y(t) = (-cos t + sin t)/2 + (3/2) e^t\n    return (-np.cos(t) + np.sin(t)) / 2.0 + 1.5 * np.exp(t)\n\ndef f_rhs(t, y, trig: TrigCounter):\n    # f(t, y) = cos(t) + y; counts only cos in this RHS\n    return trig.cos(t) + y\n\ndef taylor4_step(t, y, h, trig: TrigCounter):\n    \"\"\"\n    One step of the fourth-order Taylor method for y' = cos(t) + y.\n    Uses derivatives:\n      y'   = cos(t) + y\n      y''  = -sin(t) + cos(t) + y\n      y''' = -sin(t) + y\n      y''''= y\n    \"\"\"\n    ct = trig.cos(t)\n    st = trig.sin(t)\n    y1 = ct + y\n    y2 = -st + ct + y\n    y3 = -st + y\n    y4 = y\n    return y + h * y1 + (h**2) * y2 / 2.0 + (h**3) * y3 / 6.0 + (h**4) * y4 / 24.0\n\ndef rk4_step(t, y, h, trig: TrigCounter):\n    \"\"\"\n    One step of classical RK4 for y' = cos(t) + y.\n    Each f evaluation counts a cos() call through the trig wrapper.\n    \"\"\"\n    k1 = f_rhs(t, y, trig)\n    k2 = f_rhs(t + 0.5*h, y + 0.5*h*k1, trig)\n    k3 = f_rhs(t + 0.5*h, y + 0.5*h*k2, trig)\n    k4 = f_rhs(t + h, y + h*k3, trig)\n    return y + (h/6.0) * (k1 + 2.0*k2 + 2.0*k3 + k4)\n\ndef integrate_taylor4(h, T, y0):\n    trig = TrigCounter()\n    N = int(round(T / h))\n    t = 0.0\n    y = y0\n    for _ in range(N):\n        y = taylor4_step(t, y, h, trig)\n        t += h\n    return y, trig.total_trig_calls\n\ndef integrate_rk4(h, T, y0):\n    trig = TrigCounter()\n    N = int(round(T / h))\n    t = 0.0\n    y = y0\n    for _ in range(N):\n        y = rk4_step(t, y, h, trig)\n        t += h\n    return y, trig.total_trig_calls\n\ndef solve():\n    # Define the test cases from the problem statement.\n    T = 2.0\n    y0 = 1.0\n    test_steps = [2.0, 1.0, 0.5, 0.2, 0.1, 0.05]  # h values\n\n    exact_T = float(exact_solution(T))\n\n    results = []\n    for h in test_steps:\n        # Taylor-4 integration\n        yT_taylor, trig_taylor = integrate_taylor4(h, T, y0)\n        err_taylor = abs(yT_taylor - exact_T)\n\n        # RK4 integration\n        yT_rk4, trig_rk4 = integrate_rk4(h, T, y0)\n        err_rk4 = abs(yT_rk4 - exact_T)\n\n        # Append in the specified order:\n        # E_T4(h), E_RK4(h), C_T4(h), C_RK4(h)\n        results.append(err_taylor)\n        results.append(err_rk4)\n        results.append(int(trig_taylor))\n        results.append(int(trig_rk4))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3282712"}]}