{"hands_on_practices": [{"introduction": "The relationship between consistency, stability, and convergence is captured by the celebrated Lax-Richtmyer equivalence theorem, which, in the context of ODEs, states that a consistent numerical method converges if and only if it is stable. This exercise provides a direct, hands-on demonstration of this fundamental principle. You will implement and contrast a stable method with a custom-built method that is consistent but deliberately designed to be unstable, allowing you to observe firsthand why consistency alone is insufficient to guarantee a meaningful numerical solution [@problem_id:3156045].", "problem": "Consider the initial value problem for an ordinary differential equation (ODE) given by $y'(t) = f(t,y(t))$ with $y(0) = y_0$, where $y(t)$ is a sufficiently smooth scalar function and $f$ is a sufficiently smooth function. Define the local truncation error (LTE) as the defect produced by substituting the exact solution into a proposed numerical recurrence for a single step, and define the global truncation error (GTE) as the difference between the numerical approximation and the exact solution at a specified final time. In the context of linear multistep methods, zero-stability is the property that the homogeneous part of the recurrence does not amplify perturbations unboundedly as the number of steps increases; for one-step methods, stability refers to the boundedness of error propagation governed by the linearized recurrence factor.\n\nDesign and implement a numerical experiment to separate the concept of consistency (LTE tends to zero as the step size tends to zero) from convergence (GTE tends to zero as the step size tends to zero). Use the ODE $y'(t) = -y(t)$ on the interval $t \\in [0,1]$ with the exact solution $y(t) = e^{-t}$ and $y_0 = 1$. Construct and analyze a linear multistep method that is consistent (its LTE tends to $0$ as $h \\to 0$) but not stable, and thus not convergent, in contrast with a stable one-step method.\n\nTasks:\n1. Implement the two-step recurrence $y_{n+1} - 2 y_n + y_{n-1} = h f(t_n, y_n)$ for $f(t,y) = -y$. Initialize the method with $y_0 = y(0)$ and $y_1 = y(h)$ taken from the exact solution to isolate propagation effects rather than start-up approximation effects. For $n = 1,2,\\dots,N-1$ with $Nh = 1$, evolve the recurrence to $t_N = 1$ and compute the final-time absolute global truncation error $|y_N - e^{-1}|$.\n2. For the same method, compute the maximum-in-time absolute local truncation error by evaluating the one-step defect when substituting the exact solution into the recurrence, i.e., $\\max_{n=1,\\dots,N-1} \\left| y(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) - h f(t_n, y(t_n)) \\right|$, where $t_n = n h$.\n3. Implement the forward Euler one-step method $y_{n+1} = y_n + h f(t_n, y_n)$ for $f(t,y) = -y$ with $y_0 = 1$, and similarly compute its maximum-in-time absolute local truncation error $\\max_{n=0,\\dots,N-1} \\left| y(t_{n+1}) - y(t_n) - h f(t_n, y(t_n)) \\right|$ and its final-time absolute global truncation error $|y_N - e^{-1}|$.\n4. Use the following test suite of step sizes, which explores a general case and progressively refined steps: $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$. For each $h$, let $N = 1/h$ be the number of steps so that $t_N = 1$ exactly.\n5. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each $h$ in the order listed, append four floating-point numbers in this order: the maximum-in-time absolute local truncation error for the unstable two-step method, the final-time absolute global truncation error for the unstable two-step method, the maximum-in-time absolute local truncation error for the forward Euler method, and the final-time absolute global truncation error for the forward Euler method. The final output, therefore, contains $16$ floats corresponding to the four values for each of the four step sizes, all in a single flat list. No physical units are involved in this problem, and angles are not used.", "solution": "We start from the initial value problem $y'(t) = f(t,y(t))$ with $f(t,y) = -y$, $y(0) = 1$, and exact solution $y(t) = e^{-t}$ on $t \\in [0,1]$. The local truncation error (LTE) at a time $t_n$ for a given scheme is defined as the algebraic defect obtained when substituting the exact values into the numerical recurrence over a single step. The global truncation error (GTE) at time $t_N$ is defined as the difference $|y_N - y(t_N)|$ where $y_N$ is the numerical solution and $y(t_N)$ is the exact solution.\n\nTo contrast consistency from convergence, we consider two schemes.\n\nFirst scheme (two-step, consistent but not stable):\nThe method is $y_{n+1} - 2 y_n + y_{n-1} = h f(t_n,y_n)$, which for $f(t,y) = -y$ becomes $y_{n+1} = (2 - h) y_n - y_{n-1}$. Its homogeneous recurrence is $y_{n+1} - 2 y_n + y_{n-1} = 0$ with characteristic polynomial $\\rho(\\zeta) = \\zeta^2 - 2 \\zeta + 1 = (\\zeta - 1)^2$. The double root at $\\zeta = 1$ violates the zero-stability condition for linear multistep methods, which requires that all roots of $\\rho(\\zeta)$ satisfy $|\\zeta| \\le 1$ and that any root with $|\\zeta| = 1$ be simple. Therefore, this method is not zero-stable and cannot be convergent, regardless of consistency.\n\nTo see consistency, we derive the local truncation error. By definition, the LTE at $t_n$ is\n$$\n\\tau_n = y(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) - h f(t_n, y(t_n)).\n$$\nUsing Taylor expansions around $t_n$ for a sufficiently smooth $y$, the left-hand side of the recurrence for the exact solution becomes:\n$$\ny(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) = (y(t_n)+hy'(t_n)+\\frac{h^2}{2}y''(t_n)) - 2y(t_n) + (y(t_n)-hy'(t_n)+\\frac{h^2}{2}y''(t_n)) + \\mathcal{O}(h^4) = h^2 y''(t_n) + \\mathcal{O}(h^4).\n$$\nSubstituting this into the LTE definition along with $f(t_n, y(t_n)) = -y(t_n)$ gives:\n$$\n\\tau_n = (h^2 y''(t_n) + \\mathcal{O}(h^4)) - h(-y(t_n)) = h^2 y''(t_n) + h y(t_n) + \\mathcal{O}(h^4).\n$$\nFor our specific ODE, $y'(t) = -y(t)$ implies $y''(t) = -y'(t) = y(t)$, so\n$$\n\\tau_n = h^2 y(t_n) + h y(t_n) + \\mathcal{O}(h^4) = h y(t_n) + \\mathcal{O}(h^2).\n$$\nTherefore, as $h \\to 0$, $\\tau_n \\to 0$, demonstrating consistency.\n\nHowever, convergence (GTE $\\to 0$ as $h \\to 0$) requires stability in addition to consistency. To see why global truncation error does not vanish here, consider the error $e_n = y_n - y(t_n)$. Subtracting the exact recurrence (which the exact solution satisfies up to the LTE), the error satisfies\n$$\ne_{n+1} - 2 e_n + e_{n-1} = -h e_n + \\tau_n,\n$$\nwhere $\\tau_n$ is the LTE defect as above. The homogeneous part of this error recurrence,\n$$\ne_{n+1} - 2 e_n + e_{n-1} = 0,\n$$\nhas solutions of the form $e_n = C_1 + C_2 n$. A repeated root at $\\zeta = 1$ implies polynomial growth in $n$ for perturbations, even when the forcing term $-h e_n$ is small. Including the forcing $\\tau_n$ with magnitude $\\mathcal{O}(h)$, a discrete variation-of-constants argument shows that $e_n$ accumulates linearly with the number of steps. Over $N = 1/h$ steps (since $Nh = 1$), the accumulated amplification yields\n$$\ne_N \\sim \\mathcal{O}(N \\cdot h) = \\mathcal{O}(1),\n$$\nwhich does not tend to zero as $h \\to 0$. Thus, despite LTE $\\to 0$ (consistency), the method is not convergent because it is not zero-stable; its GTE does not vanish.\n\nSecond scheme (one-step, stable and convergent):\nThe forward Euler method is $y_{n+1} = y_n + h f(t_n,y_n)$, here $y_{n+1} = (1 - h) y_n$. Its LTE, by substitution of the exact solution, is\n$$\n\\tau_n = y(t_{n+1}) - y(t_n) - h f(t_n, y(t_n)) = y(t_{n+1}) - y(t_n) + h y(t_n).\n$$\nUsing Taylor expansion,\n$$\ny(t_{n+1}) - y(t_n) = h y'(t_n) + \\frac{h^2}{2} y''(t_n) + \\mathcal{O}(h^3) = -h y(t_n) + \\frac{h^2}{2} y''(t_n) + \\mathcal{O}(h^3),\n$$\nand with $y''(t_n) = y(t_n)$, we get\n$$\n\\tau_n = \\frac{h^2}{2} y(t_n) + \\mathcal{O}(h^3),\n$$\nso LTE $\\to 0$ quadratically. The error recurrence is\n$$\ne_{n+1} = (1 - h) e_n + \\tau_n.\n$$\nFor $h \\in (0,2)$, we have $|1 - h|  1$, implying that errors are damped. Summing the forced response shows\n$$\ne_N = \\sum_{k=0}^{N-1} (1 - h)^{N-1-k} \\tau_k,\n$$\nwhich is bounded by a geometric series scaling with $\\max_k |\\tau_k| = \\mathcal{O}(h^2)$, yielding $e_N = \\mathcal{O}(h)$ as $h \\to 0$. Hence, forward Euler is both consistent and stable, and therefore convergent, with GTE $\\to 0$.\n\nExperimental design and computation:\n- We fix $T = 1$ and step sizes $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$ so that $N = 1/h$ is an integer.\n- For the two-step method, we use exact initialization $y_0 = y(0)$ and $y_1 = y(h)$ to focus on propagation stability. We compute the maximum-in-time absolute LTE by substituting $y(t_n)$ into the recurrence and taking the maximum over $n = 1,\\dots,N-1$. We compute the final-time absolute GTE $|y_N - e^{-1}|$ after evolving the recurrence to $t_N = 1$.\n- For forward Euler, we compute its maximum-in-time absolute LTE similarly via substitution and its final-time absolute GTE by evolving the numerical recurrence from $y_0 = 1$.\n\nExpected qualitative outcomes:\n- For the two-step method, the maximum-in-time absolute LTE decreases with $h$ (consistency), but the final-time absolute GTE does not tend to zero due to lack of zero-stability; it remains of order $\\mathcal{O}(1)$ as $h \\to 0$.\n- For forward Euler, both the maximum-in-time absolute LTE and the final-time absolute GTE decrease with $h$; the final-time GTE tends to zero linearly, reflecting convergence.\n\nThe program outputs, in a single flat list and in the order of the test suite, for each $h$:\n1. the two-step method maximum-in-time absolute LTE,\n2. the two-step method final-time absolute GTE,\n3. the forward Euler maximum-in-time absolute LTE,\n4. the forward Euler final-time absolute GTE.\n\nThis directly demonstrates that LTE $\\to 0$ does not guarantee GTE $\\to 0$ without stability, thereby distinguishing consistency from convergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef y_exact(t: float) - float:\n    # Exact solution for y' = -y with y(0) = 1\n    return np.exp(-t)\n\ndef f_rhs(t: float, y: float) - float:\n    # Right-hand side f(t,y) = -y\n    return -y\n\ndef lte_two_step(h: float, T: float) - float:\n    # Compute maximum-in-time absolute LTE for two-step method:\n    # tau_n = y(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) - h f(t_n, y(t_n))\n    N = int(round(T / h))\n    # times t_n = n h, n = 0..N\n    max_tau = 0.0\n    for n in range(1, N):  # n=1..N-1 has neighbors n-1 and n+1 within [0,N]\n        t_n = n * h\n        tau = y_exact(t_n + h) - 2.0 * y_exact(t_n) + y_exact(t_n - h) - h * f_rhs(t_n, y_exact(t_n))\n        max_tau = max(max_tau, abs(tau))\n    return max_tau\n\ndef gte_two_step(h: float, T: float) - float:\n    # Compute final-time absolute GTE for two-step method evolution with exact starts\n    N = int(round(T / h))\n    y = np.zeros(N + 1, dtype=float)\n    # exact starts\n    y[0] = y_exact(0.0)\n    if N = 1:\n        y[1] = y_exact(h)\n    # evolve: y_{n+1} = (2 - h) y_n - y_{n-1}\n    for n in range(1, N):\n        y[n + 1] = (2.0 - h) * y[n] - y[n - 1]\n    return abs(y[N] - y_exact(T))\n\ndef lte_forward_euler(h: float, T: float) - float:\n    # Compute maximum-in-time absolute LTE for forward Euler:\n    # tau_n = y(t_{n+1}) - y(t_n) - h f(t_n, y(t_n)), for n=0..N-1\n    N = int(round(T / h))\n    max_tau = 0.0\n    for n in range(0, N):\n        t_n = n * h\n        tau = y_exact(t_n + h) - y_exact(t_n) - h * f_rhs(t_n, y_exact(t_n))\n        max_tau = max(max_tau, abs(tau))\n    return max_tau\n\ndef gte_forward_euler(h: float, T: float) - float:\n    # Compute final-time absolute GTE for forward Euler evolution\n    N = int(round(T / h))\n    y = 1.0  # y0\n    for _ in range(N):\n        y = y + h * f_rhs(0.0, y)  # f depends only on y here; t not needed\n        # Using t argument consistently, but for f = -y, t is irrelevant.\n    return abs(y - y_exact(T))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    T = 1.0\n    test_steps = [0.5, 0.25, 0.125, 0.0625]\n\n    results = []\n    for h in test_steps:\n        # Two-step method (consistent but not zero-stable)\n        lte_ts = lte_two_step(h, T)\n        gte_ts = gte_two_step(h, T)\n        # Forward Euler (stable for h in (0,2), convergent)\n        lte_fe = lte_forward_euler(h, T)\n        gte_fe = gte_forward_euler(h, T)\n        results.extend([lte_ts, gte_ts, lte_fe, gte_fe])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3156045"}, {"introduction": "Our standard theory of convergence presumes that the initial value problem has a unique, well-defined solution to which our numerical approximation can converge. This practice explores the fascinating complications that arise when this core assumption is violated. By numerically solving the ODE $y'(t) = \\sqrt{|y(t)|}$, which famously lacks a unique solution at $y(0)=0$, you will investigate how different numerical schemes can \"select\" and converge to different analytical solutions, revealing a deeper connection between the stability of the method and the stability of the underlying equilibria of the ODE itself [@problem_id:3111982].", "problem": "Consider the autonomous initial value problem (IVP) given by the ordinary differential equation (ODE) $y'(t) = \\sqrt{|y(t)|}$ with initial condition $y(0) = 0$. In the classical existence and uniqueness theory for ODEs, uniqueness is guaranteed if the right-hand side function is locally Lipschitz continuous in the dependent variable near the initial condition. Here, the function $f(y) = \\sqrt{|y|}$ is continuous but not locally Lipschitz near $y = 0$. As a result, the IVP admits multiple solutions, and common numerical methods may select different solution trajectories in the limit as the step size is refined. This raises questions about consistency, stability, and convergence in the presence of non-uniqueness.\n\nYour task is to write a complete, runnable program that, for the specified test suite, numerically integrates the IVP over the time horizon $T = 1$ using uniform time steps and reports the terminal value $y(T)$ for each case. You must implement the following numerical methods from first principles:\n\n- Forward (explicit) Euler: For a uniform step size $h$, update $y_{n+1} = y_n + h\\sqrt{|y_n|}$.\n- Backward (implicit) Euler: For a uniform step size $h$, update $y_{n+1}$ by solving the implicit equation $y_{n+1} = y_n + h\\sqrt{|y_{n+1}|}$. At each step, enforce $y_{n+1} \\ge 0$. For $y_n > 0$, choose the unique nonnegative solution. For $y_n = 0$, there are two nonnegative solutions; in that special case, your program must support selecting either $y_{n+1} = 0$ (the zero branch) or $y_{n+1} = h^2$ (the nonzero branch), and then proceed with the unique nonnegative solution thereafter.\n- Classical fourth-order explicit Runge–Kutta method (RK4): For a uniform step size $h$, update using the standard four-stage formula with the right-hand side $f(y) = \\sqrt{|y|}$, ensuring $y_n \\ge 0$ is maintained throughout.\n\nFundamental base for reasoning:\n- Initial value problem definition for ODEs: Given $y'(t) = f(y(t))$ and $y(0) = y_0$, with $f$ continuous, there exists at least one solution on a neighborhood of $t = 0$.\n- Local Lipschitz continuity near a point ensures uniqueness of solutions and underpins standard convergence theorems for consistent, stable numerical methods.\n- Numerical consistency means the local truncation error tends to $0$ as $h \\to 0$.\n- Numerical stability means perturbations (including those introduced by discretization choices or rounding) do not grow uncontrollably under the method.\n- Numerical convergence means the numerical solution approaches a true solution of the IVP as $h \\to 0$.\n\nImplementation details:\n- Use the right-hand side $f(y) = \\sqrt{|y|}$ everywhere in your code.\n- Use $y_0 = \\varepsilon$ where $\\varepsilon$ is a specified initial perturbation; this allows investigation of sensitivity near $y = 0$.\n- Use a uniform step size $h$ such that $N = T/h$ is an integer. The program must integrate $N$ steps to reach $t = T$.\n- When solving the implicit equation for backward Euler, reduce it to a scalar quadratic in $s = \\sqrt{y_{n+1}}$ and select the appropriate nonnegative root as specified above. When $y_n = 0$, the program must support both initial branches at the first step and then continue with the unique nonnegative root afterwards.\n\nTest suite:\nCompute and report $y(T)$ for the following six cases. In every case, take $T = 1$:\n1. Forward Euler with $h = 0.1$, $\\varepsilon = 0$, no special branch selection.\n2. Forward Euler with $h = 0.1$, $\\varepsilon = 10^{-12}$, no special branch selection.\n3. Backward Euler (nonzero branch at the first step) with $h = 0.1$, $\\varepsilon = 0$.\n4. Backward Euler (zero branch at the first step) with $h = 0.1$, $\\varepsilon = 0$.\n5. Runge–Kutta $4$ (RK4) with $h = 0.02$, $\\varepsilon = 0$.\n6. Runge–Kutta $4$ (RK4) with $h = 0.02$, $\\varepsilon = 10^{-12}$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be the terminal value $y(T)$ for the corresponding test case in the exact order listed above, represented as a floating-point number. For example, the output must look like $[r_1,r_2,r_3,r_4,r_5,r_6]$ where each $r_i$ is a float.", "solution": "The problem is valid. It presents a well-defined computational task to explore the behavior of standard numerical methods for an initial value problem (IVP) that is known to exhibit non-unique solutions. The IVP is $y'(t) = \\sqrt{|y(t)|}$ with $y(0)=0$. The function $f(y) = \\sqrt{|y|}$ is continuous but not locally Lipschitz continuous at $y=0$, which is the theoretical reason for the non-uniqueness. The problem specifies the numerical methods (Forward Euler, Backward Euler, RK4), provides all necessary parameters (time horizon $T$, step size $h$, initial perturbation $\\varepsilon$), and gives clear instructions for implementation, including how to resolve the ambiguity in the implicit Backward Euler method. The task is scientifically sound, objective, and well-posed from a computational standpoint.\n\nThe analytical solutions to this IVP starting from $y(0)=0$ are known. One solution is the trivial solution, $y_A(t) = 0$ for all $t \\ge 0$. Another is the parabolic solution, $y_B(t) = t^2/4$ for all $t \\ge 0$. Any solution of the form $y(t) = 0$ for $0 \\le t  c$ and $y(t) = (t-c)^2/4$ for $t \\ge c$ is also a valid solution for any constant $c \\ge 0$. The problem investigates which of these solutions, if any, the numerical methods converge to under different conditions.\n\nWe will implement the three specified methods to find the numerical approximation of $y(T)$ for $T=1$.\n\n**1. Forward (Explicit) Euler Method**\nThe update rule is given by $y_{n+1} = y_n + h f(y_n)$. For this problem, $f(y) = \\sqrt{|y|}$, so the formula is:\n$$y_{n+1} = y_n + h \\sqrt{|y_n|}$$\nStarting with $y_0 = \\varepsilon \\ge 0$, all subsequent values $y_n$ will be non-negative, so $|y_n| = y_n$.\nIf the initial condition is $y_0 = 0$, then $y_1 = 0 + h\\sqrt{0} = 0$. By induction, $y_n = 0$ for all $n$. The method remains on the trivial solution branch.\nIf the initial condition is perturbed to $y_0 = \\varepsilon > 0$, the solution will immediately become non-zero and begin to approximate the parabolic solution $y_B(t) = t^2/4$.\n\n**2. Backward (Implicit) Euler Method**\nThe update rule is given by the implicit equation $y_{n+1} = y_n + h f(y_{n+1})$, which for this problem is:\n$$y_{n+1} = y_n + h \\sqrt{|y_{n+1}|}$$\nAssuming $y_n \\ge 0$ and seeking a solution $y_{n+1} \\ge 0$, we can drop the absolute value. Let $s = \\sqrt{y_{n+1}}$, where $s \\ge 0$. The equation becomes $s^2 = y_n + hs$, which can be rearranged into a standard quadratic equation for $s$:\n$$s^2 - hs - y_n = 0$$\nThe solutions for $s$ are given by the quadratic formula:\n$$s = \\frac{h \\pm \\sqrt{(-h)^2 - 4(1)(-y_n)}}{2} = \\frac{h \\pm \\sqrt{h^2 + 4y_n}}{2}$$\nSince we require $s = \\sqrt{y_{n+1}} \\ge 0$, we must choose the positive root:\n$$s = \\frac{h + \\sqrt{h^2 + 4y_n}}{2}$$\nThus, the general update rule for $y_{n+1}$ is:\n$$y_{n+1} = s^2 = \\left( \\frac{h + \\sqrt{h^2 + 4y_n}}{2} \\right)^2$$\nThis formula gives a unique non-negative solution for $y_{n+1}$ as long as $y_n > 0$.\n\nA special case arises when $y_n = 0$. The quadratic equation for $s$ becomes $s^2 - hs = 0$, or $s(s-h) = 0$. This yields two non-negative solutions for $s$: $s=0$ and $s=h$. These correspond to $y_{n+1}=0$ (the \"zero branch\") and $y_{n+1}=h^2$ (the \"nonzero branch\").\n\nThe problem specifies how to handle this ambiguity based on the test case:\n- For the test cases with $\\varepsilon = 0$, $y_0 = 0$. The choice between the zero and nonzero branch is made explicitly for the first step ($n=0$).\n- For all subsequent steps ($n > 0$), if $y_n$ were to become $0$, the problem directs to \"proceed with the unique nonnegative solution thereafter\". This is interpreted to mean using the general formula, which for $y_n=0$ yields $y_{n+1} = (\\frac{h+h}{2})^2 = h^2$. This constitutes a default selection of the nonzero branch after the first step.\n\n**3. Classical Fourth-Order Runge-Kutta (RK4) Method**\nThe update rule is given by the standard four-stage formula:\n$$y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$\nwhere the stages are computed as:\n$$k_1 = f(y_n) = \\sqrt{|y_n|}$$\n$$k_2 = f(y_n + \\frac{h}{2}k_1) = \\sqrt{|y_n + \\frac{h}{2}k_1|}$$\n$$k_3 = f(y_n + \\frac{h}{2}k_2) = \\sqrt{|y_n + \\frac{h}{2}k_2|}$$\n$$k_4 = f(y_n + h k_3) = \\sqrt{|y_n + h k_3|}$$\nSimilar to the Forward Euler method, if $y_0 = 0$, then all stages $k_1, k_2, k_3, k_4$ will be $0$. Consequently, $y_1=0$, and by induction, $y_n = 0$ for all $n$. The method stays on the trivial solution.\nIf $y_0 = \\varepsilon > 0$, the method will immediately produce a non-zero trajectory that approximates the parabolic solution $y_B(t) = t^2/4$. Due to its higher order, the RK4 approximation is expected to be more accurate than that of the first-order methods.\n\nThe program will now be constructed to execute these three methods for the six specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Numerically integrates the IVP y'(t) = sqrt(|y(t)|) with y(0) = eps\n    over T=1 for a suite of test cases and reports the terminal value y(T).\n    \"\"\"\n\n    test_cases = [\n        {'method': 'forward_euler', 'h': 0.1, 'epsilon': 0.0, 'branch': None},\n        {'method': 'forward_euler', 'h': 0.1, 'epsilon': 1e-12, 'branch': None},\n        {'method': 'backward_euler', 'h': 0.1, 'epsilon': 0.0, 'branch': 'nonzero'},\n        {'method': 'backward_euler', 'h': 0.1, 'epsilon': 0.0, 'branch': 'zero'},\n        {'method': 'rk4', 'h': 0.02, 'epsilon': 0.0, 'branch': None},\n        {'method': 'rk4', 'h': 0.02, 'epsilon': 1e-12, 'branch': None},\n    ]\n\n    T = 1.0\n    results = []\n\n    def ode_rhs(y):\n        return np.sqrt(np.abs(y))\n\n    for case in test_cases:\n        h = case['h']\n        eps = case['epsilon']\n        method = case['method']\n        branch = case['branch']\n\n        # Number of steps\n        N = int(round(T / h))\n        if not np.isclose(T, N * h):\n            raise ValueError(\"T must be an integer multiple of h.\")\n\n        y = eps\n\n        if method == 'forward_euler':\n            for _ in range(N):\n                y = y + h * ode_rhs(y)\n            results.append(y)\n\n        elif method == 'backward_euler':\n            # Handle the first step where y0=eps might be 0\n            if np.isclose(y, 0.0):\n                if branch == 'zero':\n                    y = 0.0\n                elif branch == 'nonzero':\n                    y = h**2\n                else: # This case occurs if eps  0\n                    s = (h + np.sqrt(h**2 + 4 * y)) / 2.0\n                    y = s**2\n            else:\n                s = (h + np.sqrt(h**2 + 4 * y)) / 2.0\n                y = s**2\n\n            # Handle remaining N-1 steps\n            for _ in range(1, N):\n                if np.isclose(y, 0.0):\n                    # As per problem interpretation, if y_n=0 for n0,\n                    # the general formula picks the nonzero branch.\n                    y = h**2\n                else:\n                    # General update for y  0\n                    s = (h + np.sqrt(h**2 + 4 * y)) / 2.0\n                    y = s**2\n            results.append(y)\n            \n        elif method == 'rk4':\n            for _ in range(N):\n                k1 = ode_rhs(y)\n                k2 = ode_rhs(y + 0.5 * h * k1)\n                k3 = ode_rhs(y + 0.5 * h * k2)\n                k4 = ode_rhs(y + h * k3)\n                y = y + (h / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n            results.append(y)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3111982"}, {"introduction": "This final practice challenges you to apply the concepts of consistency, stability, and convergence in a more realistic setting typical of computational science. You will conduct a full step-halving convergence study on a stiff predator-prey model, a system where different processes evolve on vastly different timescales. By implementing and comparing a stability-limited explicit method against a more robust implicit method, you will see precisely how a method's stability region determines its ability to achieve its theoretical order of convergence on challenging, real-world problems [@problem_id:3112018].", "problem": "Create a complete, runnable program that performs a step-halving convergence study on a predator–prey system with a saturating functional response and uses it to reason about consistency, stability, and convergence of ordinary differential equation (ODE) solvers. The study must be grounded in first principles: the initial value problem (IVP) definition, the notions of consistency, stability, and convergence, and the construction of fixed-step one-step methods from Taylor expansions and integral formulations. Use these ideas to design a procedure that detects when stiffness introduced by prey saturation forces implicit methods to realize their formal order of accuracy while explicit methods are stability-limited on coarser steps.\n\nYou must work with the predator–prey system\n$$\n\\begin{aligned}\n\\frac{dx}{dt} = r\\,x\\left(1-\\frac{x}{K}\\right)\\;-\\;\\frac{c\\,x\\,y}{1+\\alpha x},\\\\\n\\frac{dy}{dt} = \\varepsilon\\left(\\eta\\,\\frac{c\\,x\\,y}{1+\\alpha x}\\;-\\;m\\,y\\right),\n\\end{aligned}\n$$\nwhere $x$ is the prey density and $y$ is the predator density. The saturating functional response is encoded by the factor $(1+\\alpha x)$ in the denominator, and stiffness can arise when the prey dynamics rapidly saturate (large $r$ relative to other rates and/or small $\\varepsilon$ makes the predator–prey timescales disparate).\n\nBase your reasoning and algorithmic design on the following foundational elements only:\n- The initial value problem (IVP) concept for an ODE: given $d\\mathbf{u}/dt=\\mathbf{f}(t,\\mathbf{u})$ and $\\mathbf{u}(0)=\\mathbf{u}_0$, seek $\\mathbf{u}(t)$.\n- Definitions: A method is consistent if its local truncation error tends to $0$ as the step size $h\\to 0$. A method is stable if errors do not grow uncontrollably under perturbations; for linear scalar tests $du/dt=\\lambda u$ with $\\mathrm{Re}(\\lambda)\\le 0$, this is characterized by a stability region in the complex plane for $z=h\\lambda$. A method is convergent if its global error tends to $0$ as $h\\to 0$. Consistency plus stability implies convergence.\n- Fixed-step one-step methods may be constructed by matching Taylor series (for explicit Runge–Kutta-type schemes) or by applying the fundamental theorem of calculus to average the vector field over the step (for implicit trapezoidal-type schemes).\n\nImplement two methods:\n- An explicit second-order Runge–Kutta method (explicit midpoint), a consistent explicit method of formal order $2$.\n- The implicit trapezoidal rule (also of formal order $2$), solved at each step by Newton’s method using the exact Jacobian.\n\nUse these methods on a fixed interval $[0,T]$, with $T$ specified below. For each method, perform a step-halving study with uniform steps. For each step size $h_k$, compute the numerical solution at $t=T$ and compare against a high-accuracy reference solution to estimate the global error. Estimate the experimental order of convergence (EOC) from a linear fit of $\\log(\\text{error})$ versus $\\log(h)$ using a decreasing-error suffix of the smallest available step sizes. A method “realizes its formal order” in this study if the estimated EOC is at least $1.7$ using at least $3$ strictly decreasing error points at the smallest step sizes.\n\nDefine “stiffness from prey saturation forces implicit methods to realize their formal order” for a given parameter set to mean: the implicit trapezoidal method realizes its formal order as defined above and simultaneously the explicit midpoint method is stability-limited on coarse steps, evidenced by failing to produce finite, non-NaN terminal values on at least one of the coarser step sizes in the halving sequence (i.e., it produces fewer than all $5$ finite terminal approximations across the prescribed step sizes). Under this condition, report a value of $1$ for that parameter set; otherwise, report $0$.\n\nNumerical details and test suite:\n- Use final time $T=2$.\n- Use initial condition $(x(0),y(0))=(x_0,y_0)=(0.5,0.3)$.\n- Use $N_k \\in \\{20,40,80,160,320\\}$ uniform steps, corresponding to $h_k=T/N_k$ for $k=0,1,2,3,4$. This gives $5$ step sizes with halving.\n- Compute a high-accuracy reference solution at $t=T$ using a stiff-capable integrator with strict tolerances; this reference is used only to measure terminal global errors for the fixed-step methods.\n- For the Newton solve in the implicit trapezoidal method, use the exact Jacobian of $\\mathbf{f}$, an absolute Newton tolerance of $10^{-12}$, and a maximum of $20$ iterations per step.\n\nProvide results for the following three parameter sets (test suite), each expressed as $(r,K,\\alpha,c,\\eta,m,\\varepsilon)$:\n- Case $\\mathsf{A}$ (non-stiff baseline): $(2,\\,1,\\,2,\\,3,\\,0.7,\\,0.4,\\,1)$.\n- Case $\\mathsf{B}$ (moderately stiff via prey saturation and timescale split): $(50,\\,1,\\,2,\\,4,\\,0.7,\\,0.4,\\,0.1)$.\n- Case $\\mathsf{C}$ (highly stiff): $(150,\\,1,\\,2,\\,4,\\,0.7,\\,0.4,\\,0.05)$.\n\nOutput specification:\n- For each case in the order $(\\mathsf{A},\\mathsf{B},\\mathsf{C})$, output an integer $0$ or $1$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[1,0,1]$.\n\nAll computations are dimensionless; no physical units are required. Angles are not used. The final output must be computable as booleans or integers as specified, and the single-line format is mandatory.", "solution": "The central task is to investigate the numerical properties of ODE solvers by performing a step-halving convergence study on a predator-prey system. The system's dynamics can be adjusted to be stiff, which allows for a clear demonstration of the differing behaviors of explicit and implicit numerical methods. We will implement two second-order methods, one explicit and one implicit, and use their performance to classify the stiffness of the system under different parameter sets, strictly adhering to the definitions of consistency, stability, and convergence.\n\nThe system is a two-dimensional autonomous ordinary differential equation (ODE) given by:\n$$\n\\begin{aligned}\n\\frac{dx}{dt} = r\\,x\\left(1-\\frac{x}{K}\\right)\\;-\\;\\frac{c\\,x\\,y}{1+\\alpha x} \\\\\n\\frac{dy}{dt} = \\varepsilon\\left(\\eta\\,\\frac{c\\,x\\,y}{1+\\alpha x}\\;-\\;m\\,y\\right)\n\\end{aligned}\n$$\nThis can be written as an initial value problem (IVP) in the standard vector form $d\\mathbf{u}/dt = \\mathbf{f}(t, \\mathbf{u})$ with an initial condition $\\mathbf{u}(t_0) = \\mathbf{u}_0$. Here, the state vector is $\\mathbf{u}(t) = [x(t), y(t)]^T$, and the vector field is $\\mathbf{f}(\\mathbf{u}) = [f_x(x,y), f_y(x,y)]^T$. The initial condition is given as $\\mathbf{u}(0) = [x_0, y_0]^T = [0.5, 0.3]^T$. The study will be conducted on the time interval $[0, T]$ with $T=2$.\n\nThe core concepts are:\n- **Consistency**: A method is consistent if its local truncation error (the error made in a single step) approaches zero as the step size $h \\to 0$.\n- **Stability**: A method is stable if errors introduced at one step do not grow unboundedly in subsequent steps. For stiff problems, this is analyzed with the model equation $du/dt = \\lambda u$ for complex $\\lambda$ with $\\mathrm{Re}(\\lambda) \\le 0$. A method's stability region is the set of complex values $z=h\\lambda$ for which the numerical solution remains bounded.\n- **Convergence**: A method is convergent if the global error (the cumulative error at a fixed final time $T$) approaches zero as $h \\to 0$. The Lax-Richtmyer equivalence theorem states that for a consistent method, stability is equivalent to convergence. The rate of convergence is typically measured by the experimental order of convergence (EOC).\n\nWe will implement two fixed-step, one-step methods of formal order $2$.\n\n**1. Explicit Second-Order Runge-Kutta Method (Explicit Midpoint)**\n\nThis method is derived by matching terms in the Taylor series expansion of the solution. It advances the solution from time $t_n$ to $t_{n+1} = t_n + h$ using two function evaluations:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = \\mathbf{f}(t_n, \\mathbf{u}_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}\\left(t_n + \\frac{h}{2}, \\mathbf{u}_n + \\frac{h}{2}\\mathbf{k}_1\\right) \\\\\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + h \\mathbf{k}_2\n\\end{aligned}\n$$\nThis method is explicit because $\\mathbf{u}_{n+1}$ is computed directly from known quantities. It is consistent with a local truncation error of order $\\mathcal{O}(h^3)$, which leads to a global error of order $\\mathcal{O}(h^2)$. However, its stability region is a finite area in the complex plane. If a problem is stiff, the eigenvalues of its Jacobian can have large negative real parts, forcing the product $h\\lambda$ outside the stability region unless the step size $h$ is severely restricted. This is a stability limitation.\n\n**2. Implicit Trapezoidal Rule**\n\nThis method is derived from the fundamental theorem of calculus, $\\mathbf{u}(t_{n+1}) - \\mathbf{u}(t_n) = \\int_{t_n}^{t_{n+1}} \\mathbf{f}(t, \\mathbf{u}(t)) dt$, by approximating the integral using the trapezoidal rule:\n$$\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + \\frac{h}{2} \\left[ \\mathbf{f}(t_n, \\mathbf{u}_n) + \\mathbf{f}(t_{n+1}, \\mathbf{u}_{n+1}) \\right]\n$$\nThis method is implicit because the unknown $\\mathbf{u}_{n+1}$ appears on both sides of the equation. It is also a second-order method. Critically, it is A-stable, meaning its stability region contains the entire left half of the complex plane. This property allows it to remain stable for stiff problems even with large step sizes, making its accuracy, not stability, the limiting factor.\n\nTo use the trapezoidal rule, we must solve a nonlinear system of equations for $\\mathbf{u}_{n+1}$ at each step. We define a function $\\mathbf{G}(\\mathbf{w}) = \\mathbf{0}$ where we seek the root $\\mathbf{w} = \\mathbf{u}_{n+1}$:\n$$\n\\mathbf{G}(\\mathbf{w}) = \\mathbf{w} - \\mathbf{u}_n - \\frac{h}{2} \\left[ \\mathbf{f}(t_n, \\mathbf{u}_n) + \\mathbf{f}(t_{n+1}, \\mathbf{w}) \\right] = \\mathbf{0}\n$$\nThis is solved using Newton's method. Starting with an initial guess $\\mathbf{w}^{(0)}$ (e.g., $\\mathbf{w}^{(0)} = \\mathbf{u}_n$), we iterate:\n$$\n\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - [J_G(\\mathbf{w}^{(k)})]^{-1} \\mathbf{G}(\\mathbf{w}^{(k)})\n$$\nThe Jacobian of $\\mathbf{G}$ with respect to $\\mathbf{w}$ is $J_G(\\mathbf{w}) = I - \\frac{h}{2} J_f(t_{n+1}, \\mathbf{w})$, where $I$ is the identity matrix and $J_f$ is the Jacobian of the ODE vector field $\\mathbf{f}$. For our system, the components of $J_f$ are:\n$$\nJ_f(x, y) = \\begin{pmatrix} \\frac{\\partial f_x}{\\partial x}  \\frac{\\partial f_x}{\\partial y} \\\\ \\frac{\\partial f_y}{\\partial x}  \\frac{\\partial f_y}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} r\\left(1-\\frac{2x}{K}\\right) - \\frac{cy}{(1+\\alpha x)^2}  -\\frac{cx}{1+\\alpha x} \\\\ \\frac{\\varepsilon\\eta cy}{(1+\\alpha x)^2}  \\varepsilon\\left(\\frac{\\eta cx}{1+\\alpha x} - m\\right) \\end{pmatrix}\n$$\nThe Newton iteration involves solving the linear system $J_G(\\mathbf{w}^{(k)}) \\Delta\\mathbf{w}^{(k)} = -\\mathbf{G}(\\mathbf{w}^{(k)})$ for the update $\\Delta\\mathbf{w}^{(k)}$, then setting $\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} + \\Delta\\mathbf{w}^{(k)}$. We stop when the norm of the residual $\\mathbf{G}(\\mathbf{w}^{(k)})$ is below a tolerance of $10^{-12}$.\n\n**Algorithmic Design for the Study**\n\nFor each parameter set $(\\mathsf{A}, \\mathsf{B}, \\mathsf{C})$, the following procedure is executed:\n1.  **Reference Solution**: A high-accuracy reference solution $\\mathbf{u}_{\\text{ref}}$ at $t=T=2$ is computed using a robust stiff solver (`Radau` from `SciPy`) with very strict tolerances (`atol`$=10^{-13}$, `rtol`$=10^{-13}$).\n2.  **Step-Halving Loop**: A loop is run over the step counts $N_k \\in \\{20, 40, 80, 160, 320\\}$, corresponding to step sizes $h_k = T/N_k$.\n3.  **Method Evaluation**: For each $h_k$:\n    *   The explicit midpoint method is used to integrate the IVP from $t=0$ to $t=T$. The final state $\\mathbf{u}_{\\text{EM}}(T)$ is recorded. We check if the result is finite. The $L_2$ norm of the global error, $\\|\\mathbf{u}_{\\text{EM}}(T) - \\mathbf{u}_{\\text{ref}}\\|_2$, is stored.\n    *   The implicit trapezoidal method is used similarly. The final state $\\mathbf{u}_{\\text{IT}}(T)$ and its global error norm, $\\|\\mathbf{u}_{\\text{IT}}(T) - \\mathbf{u}_{\\text{ref}}\\|_2$, are stored.\n4.  **Analysis and Classification**:\n    *   **Explicit Method Stability**: We check if the explicit method produced any non-finite (e.g., `inf` or `NaN`) results for $\\mathbf{u}_{\\text{EM}}(T)$ across the $5$ step sizes. If so, a flag `explicit_is_unstable` is set to true.\n    *   **Implicit Method Convergence**: We analyze the errors from the implicit method. We find the longest suffix of the error list (corresponding to the smallest step sizes) that is strictly decreasing. If this suffix contains at least $3$ points, we perform a linear regression on $\\log(\\text{error})$ vs. $\\log(h)$ for these points. The slope of this fit is the estimated EOC. If EOC is at least $1.7$, a flag `implicit_realizes_order` is set to true.\n    *   **Final Verdict**: For the given parameter set, the result is $1$ if `implicit_realizes_order` is true AND `explicit_is_unstable` is true. Otherwise, the result is $0$.\n\nThis procedure effectively contrasts the methods. In non-stiff cases, both methods should converge with an EOC near $2$. In stiff cases, the explicit method will fail for larger step sizes due to stability constraints, while the A-stable implicit method will remain stable and demonstrate its theoretical convergence order, thus satisfying the conditions for a result of $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Performs a step-halving convergence study on a predator-prey system to \n    detect stiffness based on the performance of explicit and implicit solvers.\n    \"\"\"\n\n    # --- Problem Definition ---\n    # Initial Value Problem (IVP) settings\n    T_FINAL = 2.0\n    U0 = np.array([0.5, 0.3])\n\n    # Step-halving study parameters\n    N_STEPS_LIST = [20, 40, 80, 160, 320]\n    H_LIST = [T_FINAL / n for n in N_STEPS_LIST]\n\n    # Newton's method parameters for the implicit solver\n    NEWTON_TOL = 1e-12\n    NEWTON_MAX_ITER = 20\n\n    # Test cases: (r, K, alpha, c, eta, m, epsilon)\n    test_cases = [\n        # Case A: Non-stiff baseline\n        (2.0, 1.0, 2.0, 3.0, 0.7, 0.4, 1.0),\n        # Case B: Moderately stiff\n        (50.0, 1.0, 2.0, 4.0, 0.7, 0.4, 0.1),\n        # Case C: Highly stiff\n        (150.0, 1.0, 2.0, 4.0, 0.7, 0.4, 0.05),\n    ]\n\n    # --- ODE System Definition ---\n    def f_ode(t, u, params):\n        r, K, alpha, c, eta, m, epsilon = params\n        x, y = u\n        \n        # Avoid potential division by zero or negative populations, though unlikely with the chosen IC\n        if x  0: x = 0\n        if y  0: y = 0\n            \n        common_term = (c * x * y) / (1.0 + alpha * x)\n        \n        dxdt = r * x * (1.0 - x / K) - common_term\n        dydt = epsilon * (eta * common_term - m * y)\n        \n        return np.array([dxdt, dydt])\n\n    def jacobian_f(t, u, params):\n        r, K, alpha, c, eta, m, epsilon = params\n        x, y = u\n        \n        # Avoid negative populations in Jacobian calculation\n        if x  0: x = 0\n        if y  0: y = 0\n\n        denom = 1.0 + alpha * x\n        denom_sq = denom * denom\n\n        df1_dx = r * (1.0 - 2.0 * x / K) - (c * y) / denom_sq\n        df1_dy = - (c * x) / denom\n        df2_dx = epsilon * eta * (c * y) / denom_sq\n        df2_dy = epsilon * (eta * (c * x) / denom - m)\n        \n        return np.array([[df1_dx, df1_dy], [df2_dx, df2_dy]])\n\n    # --- Numerical Methods ---\n    def explicit_midpoint_solver(f, u0, t_final, n_steps, params):\n        h = t_final / n_steps\n        u = u0.copy()\n        t = 0.0\n        for _ in range(n_steps):\n            k1 = f(t, u, params)\n            k2 = f(t + h / 2.0, u + h / 2.0 * k1, params)\n            u += h * k2\n            t += h\n            if not np.all(np.isfinite(u)):\n                return u # Propagate non-finite value immediately\n        return u\n\n    def implicit_trapezoidal_solver(f, jac, u0, t_final, n_steps, params):\n        h = t_final / n_steps\n        u_n = u0.copy()\n        t_n = 0.0\n        I = np.identity(len(u0))\n\n        for _ in range(n_steps):\n            f_n = f(t_n, u_n, params)\n            t_np1 = t_n + h\n            \n            # Newton's method to solve for u_{n+1}\n            w = u_n.copy() # Initial guess for u_{n+1}\n            for _ in range(NEWTON_MAX_ITER):\n                f_np1 = f(t_np1, w, params)\n                G = w - u_n - (h / 2.0) * (f_n + f_np1)\n                \n                if np.linalg.norm(G)  NEWTON_TOL:\n                    break\n\n                J_G = I - (h / 2.0) * jac(t_np1, w, params)\n                delta_w = solve_linear_system(J_G, -G)\n                w += delta_w\n            \n            u_n = w\n            t_n = t_np1\n            \n            if not np.all(np.isfinite(u_n)):\n                return u_n\n\n        return u_n\n\n    results = []\n    for params in test_cases:\n        # 1. Compute high-accuracy reference solution\n        ref_sol = solve_ivp(\n            f_ode, [0, T_FINAL], U0, method='Radau', \n            args=(params,), atol=1e-13, rtol=1e-13\n        )\n        u_ref = ref_sol.y[:, -1]\n\n        em_errors = []\n        it_errors = []\n        em_unstable = False\n\n        # 2. Run step-halving study for both methods\n        for i, N in enumerate(N_STEPS_LIST):\n            h = H_LIST[i]\n\n            # Explicit Midpoint\n            u_em = explicit_midpoint_solver(f_ode, U0, T_FINAL, N, params)\n            if not np.all(np.isfinite(u_em)):\n                em_unstable = True\n                em_errors.append(np.inf)\n            else:\n                em_errors.append(np.linalg.norm(u_em - u_ref))\n\n            # Implicit Trapezoidal\n            u_it = implicit_trapezoidal_solver(f_ode, jacobian_f, U0, T_FINAL, N, params)\n            if not np.all(np.isfinite(u_it)):\n                 it_errors.append(np.inf)\n            else:\n                it_errors.append(np.linalg.norm(u_it - u_ref))\n        \n        # 3. Analyze results and classify\n        \n        # Check if implicit method realizes its formal order\n        it_realizes_order = False\n        \n        # Find the longest strictly decreasing suffix of errors\n        fit_indices = []\n        if len(it_errors)  0:\n            fit_indices.append(len(it_errors) - 1)\n            for i in range(len(it_errors) - 2, -1, -1):\n                if it_errors[i+1]  it_errors[i] and it_errors[i+1]  0:\n                    fit_indices.insert(0, i)\n                else:\n                    break\n        \n        if len(fit_indices) = 3:\n            h_fit = np.log([H_LIST[i] for i in fit_indices])\n            err_fit = np.log([it_errors[i] for i in fit_indices])\n            \n            # Use polyfit to find the slope (Experimental Order of Convergence)\n            eoc = np.polyfit(h_fit, err_fit, 1)[0]\n            \n            if eoc = 1.7:\n                it_realizes_order = True\n        \n        # Final classification\n        if it_realizes_order and em_unstable:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112018"}]}