{"hands_on_practices": [{"introduction": "This first exercise provides a concrete starting point by applying Heun's method and the closely related midpoint method to compute a single step for an ordinary differential equation. Since both are second-order Runge-Kutta methods, this direct comparison will familiarize you with the mechanics of the predictor-corrector sequence and highlight the subtle differences that distinguish various numerical schemes.", "problem": "Consider the initial value problem (IVP) given by the ordinary differential equation $y'(t) = ty$ with the initial condition $y(1) = 2$.\n\nWe wish to approximate the value of $y(1.2)$ using a single step of size $h = 0.2$. Two different second-order Runge-Kutta methods will be used: the midpoint method and Heun's method (also known as the improved Euler method or the explicit trapezoidal rule).\n\nLet $y_M$ be the numerical approximation of $y(1.2)$ obtained using the midpoint method, and let $y_H$ be the numerical approximation of $y(1.2)$ obtained using Heun's method.\n\nCalculate the absolute difference $|y_H - y_M|$. Provide the exact numerical value of this difference.", "solution": "We have the IVP $y'(t)=f(t,y)=ty$ with $y(1)=2$, step size $h=0.2=\\frac{1}{5}$, and we seek approximations at $t_{1}=1+h=1.2=\\frac{6}{5}$.\n\nFor the midpoint (explicit midpoint) method, the update is\n$$\ny_{n+1}=y_{n}+h\\,f\\!\\left(t_{n}+\\frac{h}{2},\\,y_{n}+\\frac{h}{2}f(t_{n},y_{n})\\right).\n$$\nWith $t_{0}=1$, $y_{0}=2$:\n- $k_{1}=f(1,2)=1\\cdot 2=2$.\n- Midpoint arguments: $t_{0}+\\frac{h}{2}=1+\\frac{0.2}{2}=1.1=\\frac{11}{10}$ and $y_{0}+\\frac{h}{2}k_{1}=2+\\frac{0.2}{2}\\cdot 2=2.2=\\frac{11}{5}$.\n- $k_{2}=f\\!\\left(\\frac{11}{10},\\frac{11}{5}\\right)=\\frac{11}{10}\\cdot\\frac{11}{5}=\\frac{121}{50}$.\nThus\n$$\ny_{M}=y_{1}=2+h\\,k_{2}=2+\\frac{1}{5}\\cdot\\frac{121}{50}=2+\\frac{121}{250}=\\frac{500+121}{250}=\\frac{621}{250}.\n$$\n\nFor Heun’s method (explicit trapezoidal rule), the update is\n$$\ny_{n+1}=y_{n}+\\frac{h}{2}\\left(f(t_{n},y_{n})+f\\!\\left(t_{n}+h,\\,y_{n}+h f(t_{n},y_{n})\\right)\\right).\n$$\nWith $t_{0}=1$, $y_{0}=2$:\n- $k_{1}=f(1,2)=2$.\n- Predictor at the end: $t_{0}+h=1+0.2=1.2=\\frac{6}{5}$ and $y_{0}+h k_{1}=2+0.2\\cdot 2=2.4=\\frac{12}{5}$.\n- $k_{2}=f\\!\\left(\\frac{6}{5},\\frac{12}{5}\\right)=\\frac{6}{5}\\cdot\\frac{12}{5}=\\frac{72}{25}$.\nThus\n$$\ny_{H}=y_{1}=2+\\frac{h}{2}(k_{1}+k_{2})=2+\\frac{0.2}{2}\\left(2+\\frac{72}{25}\\right)\n=2+\\frac{1}{10}\\cdot\\frac{50+72}{25}=2+\\frac{1}{10}\\cdot\\frac{122}{25}=2+\\frac{122}{250}=\\frac{500+122}{250}=\\frac{622}{250}=\\frac{311}{125}.\n$$\n\nCompute the absolute difference:\n$$\n|y_{H}-y_{M}|=\\left|\\frac{311}{125}-\\frac{621}{250}\\right|=\\left|\\frac{622}{250}-\\frac{621}{250}\\right|=\\frac{1}{250}.\n$$", "answer": "$$\\boxed{\\frac{1}{250}}$$", "id": "2200959"}, {"introduction": "When is a numerical method not an approximation, but perfectly exact? This thought-provoking problem challenges you to discover the specific class of differential equations for which Heun's method yields the analytical solution, regardless of the step size $h$. Solving this reveals a fundamental connection between Heun's method and the trapezoidal rule of integration, offering a deeper insight into why the method works as it does.", "problem": "Consider the initial value problem for an ordinary differential equation (ODE) given by $y^{\\prime}(t)=f(t,y(t))$ with an arbitrary initial condition $y(0)=y_{0}$. Construct a specific function $f(t,y)$ such that, when this system is advanced in time using Heun’s method (the explicit trapezoidal predictor–corrector scheme) with any step size $h>0$, the numerical values at the grid points coincide exactly with the analytical solution for all choices of $y_{0}$ and for all $h>0$. \n\nProvide your answer as a single explicit expression for $f(t,y)$ (for example, an expression in $t$ and $y$), not as an equation. No rounding is required.", "solution": "The problem requires the numerical solution from Heun's method to be identical to the analytical solution for any step size $h>0$ and any initial condition $y_0$. This means that if we start a step from a point $(t_n, y(t_n))$ on the true analytical curve, the method must yield the exact value at the next point, $y(t_{n+1})$.\n\nThe analytical solution is given by the integral of the ODE:\n$$ y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s, y(s)) ds $$\nHeun's method for one step is:\n$$ y_{n+1} = y_n + \\frac{h}{2} [f(t_n, y_n) + f(t_{n+1}, y_n + h f(t_n, y_n))] $$\nFor the method to be exact, assuming $y_n = y(t_n)$, we must have $y_{n+1} = y(t_{n+1})$. This leads to the identity:\n$$ \\int_{t_n}^{t_{n+1}} f(s, y(s)) ds = \\frac{h}{2} [f(t_n, y(t_n)) + f(t_{n+1}, y(t_n) + h f(t_n, y(t_n)))] $$\nThis must hold for any valid solution curve $y(s)$ and for all $h > 0$.\n\nLet's consider the case where $f(t,y)$ is independent of $y$, i.e., $f(t,y) = g(t)$. In this case, the identity simplifies to:\n$$ \\int_{t_n}^{t_{n+1}} g(s) ds = \\frac{h}{2} [g(t_n) + g(t_{n+1})] $$\nThis equation states that the trapezoidal rule for integrating $g(s)$ is exact for any interval. The error of the trapezoidal rule is proportional to the second derivative of the integrand. For the error to be zero for any interval, the second derivative must be identically zero, i.e., $g''(s) = 0$. This implies that $g(s)$ must be a linear function of $s$:\n$$ g(s) = as + c $$\nwhere $a$ and $c$ are arbitrary constants. Thus, any function of the form $f(t,y) = at+c$ is a solution.\n\nTo show that functions depending on $y$ are generally not solutions, consider a general linear form $f(t,y) = at + by + c$. For Heun's method to be exact for all $h>0$, the Taylor series of the numerical solution and the analytical solution must match for all powers of $h$. This requires all higher-order derivatives of the analytical solution, starting from $y'''(t)$, to be zero for all $t$ and for any solution $y(t)$.\nWe have:\n$y' = at+by+c$\n$y'' = a+by'$\n$y''' = by''$\nThe condition $y'''(t)=0$ implies either $b=0$ or $y''(t)=0$. If $b \\neq 0$ and $y''(t)=0$, then $y'(t)$ must be a constant, which leads to a contradiction when substituted back into the ODE unless $b=0$. Thus, we must have $b=0$.\n\nThis shows that $f(t,y)$ cannot depend on $y$. Therefore, the function must be of the form $f(t,y) = at+c$. The problem asks for a specific function, so we can choose any constants. A simple, non-trivial choice is $a=1$ and $c=0$, which gives $f(t,y) = t$.", "answer": "$$\n\\boxed{t}\n$$", "id": "2428172"}, {"introduction": "Theory tells us that the global error $E(h)$ of Heun's method scales with the square of the step size, a property known as second-order accuracy, or $\\mathcal{O}(h^2)$. In this final practice, you will write a program to verify this theoretical result by solving a set of test problems with decreasing step sizes. This exercise introduces a standard technique in computational science for measuring a method's order of convergence and confirming its expected behavior.", "problem": "Consider the initial value problem (IVP) $y'(t) = f(t,y(t))$ with $y(t_0) = y_0$ where $f$ is continuously differentiable with respect to both arguments on a closed interval $[t_0,T]$. The global discretization error at time $T$ for a one-step method with uniform step size $h$ and $N = (T - t_0)/h$ steps is defined as $E(h) = \\lvert y_N - y(T) \\rvert$, where $y_N$ is the numerical approximation to $y(T)$ produced by the method.\n\nHeun's method (also known as the explicit trapezoidal method) is the one-step method obtained by applying the trapezoidal quadrature rule to the integral representation $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s,y(s)) \\, ds$ and approximating the second function evaluation using a forward Euler predictor. The resulting step from $(t_n,y_n)$ to $(t_{n+1},y_{n+1})$ with uniform step size $h$ is\n$$\ny_{n+1} = y_n + \\frac{h}{2}\\left(f(t_n,y_n) + f\\!\\left(t_{n+1}, y_n + h\\,f(t_n,y_n)\\right)\\right).\n$$\nAssume $f$ and the exact solution $y$ are smooth enough so that the local truncation error of Heun's method is well-defined.\n\nYour task is to numerically demonstrate, across a small test suite of smooth problems, that the global error $E(h)$ of Heun's method at the final time $T$ scales like $\\mathcal{O}(h^2)$. To do this, for each test case below:\n- Implement Heun's method on $[t_0,T]$ with uniform step sizes $h = (T - t_0)/N$ for a list of $N$ values.\n- Compute the global error $E(h)$ at $T$ using the known exact solution $y(T)$.\n- Estimate the observed order $p$ as the slope of the best-fit line in the least-squares sense for the pairs $\\big(\\log(h), \\log(E(h))\\big)$ over the provided set of step sizes. Concretely, if $(x_i,y_i) = \\big(\\log(h_i),\\log(E(h_i))\\big)$, find the line $y \\approx \\alpha + p\\,x$ that minimizes $\\sum_i (y_i - \\alpha - p\\,x_i)^2$ and return the slope $p$.\n\nUse the following test suite. When trigonometric functions appear, interpret all angles in radians.\n\n- Test Case $1$ (linear, exponentially decaying): $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, exact solution $y(t) = e^{-t}$, step counts $N \\in \\{10, 20, 40, 80\\}$.\n- Test Case $2$ (multiplicative oscillation): $f(t,y) = y\\cos(t)$, $t_0 = 0$, $y_0 = 1$, $T = \\pi$, exact solution $y(t) = \\exp(\\sin(t))$, step counts $N \\in \\{20, 40, 80, 160\\}$.\n- Test Case $3$ (nonlinear growth on a short interval): $f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $T = 0.5$, exact solution $y(t) = \\frac{1}{1 - t}$, step counts $N \\in \\{20, 40, 80, 160\\}$.\n- Test Case $4$ (pure forcing): $f(t,y) = \\sin(t)$, $t_0 = 0$, $y_0 = 0$, $T = 2$, exact solution $y(t) = 1 - \\cos(t)$, step counts $N \\in \\{20, 40, 80, 160\\}$.\n\nYour program must:\n- Implement Heun's method as described.\n- For each test case, compute the list of $(h, E(h))$ values using the specified $N$ values, estimate the slope $p$ as described, and collect the four slopes.\n- Produce a single line of output containing these four slopes as a comma-separated list enclosed in square brackets, for example, $[p_1,p_2,p_3,p_4]$. Each $p_i$ must be a floating-point number.\n\nThere are no physical units involved. All trigonometric evaluations must use angles in radians. The program must be self-contained and must not require any input. The final outputs are the four floating-point slopes corresponding to the four test cases.", "solution": "The problem requires a numerical demonstration of the convergence order of Heun's method for several initial value problems (IVPs). The theoretical global error for a one-step method of order $p$, at a fixed time $T$ and using a step size $h$, is expected to behave as $E(h) \\approx C h^p$ for some constant $C$ as $h \\to 0$. Heun's method is known to be a second-order method, so we expect to find $p \\approx 2$.\n\nTo verify this numerically, we can analyze the relationship between the error $E(h)$ and the step size $h$. Taking the natural logarithm of the error expression gives:\n$$ \\log(E(h)) \\approx \\log(C) + p \\log(h) $$\nThis equation has the form of a line, $y = \\alpha + p x$, where $y = \\log(E(h))$, $x = \\log(h)$, and the intercept is $\\alpha = \\log(C)$. The slope of this line is the order of convergence, $p$. The problem specifies that we must estimate this slope $p$ by performing a linear least-squares regression on the set of data points $(\\log(h_i), \\log(E(h_i)))$ generated from a sequence of step sizes $h_i$.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Define the IVP parameters: the function $f(t,y)$, initial time $t_0$, initial value $y_0$, final time $T$, and the exact solution $y(t)$.\n2.  For a given list of step counts $\\{N_1, N_2, \\dots, N_m\\}$:\n    a. For each $N_i$, calculate the step size $h_i = (T - t_0) / N_i$.\n    b. Solve the IVP from $t_0$ to $T$ using Heun's method with $N_i$ steps to obtain the numerical approximation $y_{N_i}$. The iteration formula for Heun's method is:\n    $$ y_{n+1} = y_n + \\frac{h}{2}\\left(f(t_n,y_n) + f\\!\\left(t_{n+1}, y_n + h f(t_n,y_n)\\right)\\right) $$\n    where $t_{n+1} = t_n + h$. This is applied $N_i$ times, starting from $(t_0, y_0)$.\n    c. Calculate the true solution at the final time, $y(T)$.\n    d. Compute the global error $E(h_i) = |y_{N_i} - y(T)|$.\n    e. Store the pair $(h_i, E(h_i))$.\n3.  After generating the set of $m$ data points $\\{(h_i, E(h_i))\\}$, transform them by taking logarithms to get $\\{(\\log(h_i), \\log(E(h_i)))\\}$. Let $x_i = \\log(h_i)$ and $y_i = \\log(E(h_i))$.\n4.  Estimate the slope $p$ of the best-fit line through the points $(x_i, y_i)$. For a simple linear regression model $y = \\alpha + px$, the slope $p$ which minimizes the sum of squared errors $\\sum_{i=1}^m (y_i - (\\alpha + px_i))^2$ is given by:\n$$ p = \\frac{\\sum_{i=1}^m (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^m (x_i - \\bar{x})^2} $$\nwhere $\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x_i$ and $\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y_i$ are the sample means of the $x$ and $y$ data, respectively. This formula is equivalent to computing the covariance of $x$ and $y$ divided by the variance of $x$.\n\nThis procedure is implemented for each of the four test cases provided. The implementation uses Python and the `numpy` library. `numpy` provides implementations for the necessary mathematical functions (e.g., `exp`, `sin`, `cos`, `log`, `pi`) and facilitates array operations for the least-squares calculation. A function is created to implement Heun's method, and another to compute the least-squares slope. The main part of the program iterates through the test cases, computes the errors for the specified step counts, estimates the convergence order $p$ for each case, and collects these values for the final output. The estimated values for $p$ are expected to be close to $2$, confirming the second-order accuracy of Heun's method for these smooth problems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing Heun's method, calculating global errors,\n    and estimating the order of convergence for four different IVPs.\n    \"\"\"\n\n    def heun_solver(f, t0, y0, T, N):\n        \"\"\"\n        Solves an IVP y'(t) = f(t, y) using Heun's method.\n\n        Args:\n            f: The function f(t, y).\n            t0: Initial time.\n            y0: Initial value.\n            T: Final time.\n            N: Number of steps.\n\n        Returns:\n            The numerical approximation of y(T).\n        \"\"\"\n        h = (T - t0) / N\n        t = float(t0)\n        y = float(y0)\n        for _ in range(N):\n            k1 = f(t, y)\n            k2 = f(t + h, y + h * k1)\n            y = y + (h / 2.0) * (k1 + k2)\n            t = t + h\n        return y\n\n    def estimate_order(h_values, E_values):\n        \"\"\"\n        Estimates the order of convergence p from step sizes h and errors E.\n        This is done by finding the slope of the best-fit line for log(E) vs. log(h).\n\n        Args:\n            h_values: A numpy array of step sizes.\n            E_values: A numpy array of corresponding global errors.\n\n        Returns:\n            The estimated order of convergence p.\n        \"\"\"\n        # Take the natural logarithm of step sizes and errors\n        log_h = np.log(h_values)\n        log_E = np.log(E_values)\n        \n        # We want to find the slope p of the line y = alpha + p*x that best fits\n        # the data (x, y) = (log_h, log_E) in the least-squares sense.\n        # The formula for the slope is p = Cov(x, y) / Var(x).\n        x = log_h\n        y = log_E\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        p = numerator / denominator\n        return p\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -y,\n            \"y_exact\": lambda t: np.exp(-t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N_values\": [10, 20, 40, 80]\n        },\n        {\n            \"f\": lambda t, y: y * np.cos(t),\n            \"y_exact\": lambda t: np.exp(np.sin(t)),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": np.pi,\n            \"N_values\": [20, 40, 80, 160]\n        },\n        {\n            \"f\": lambda t, y: y**2,\n            \"y_exact\": lambda t: 1.0 / (1.0 - t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 0.5,\n            \"N_values\": [20, 40, 80, 160]\n        },\n        {\n            \"f\": lambda t, y: np.sin(t),\n            \"y_exact\": lambda t: 1.0 - np.cos(t),\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 2.0,\n            \"N_values\": [20, 40, 80, 160]\n        }\n    ]\n\n    estimated_orders = []\n    \n    # Process each test case\n    for case in test_cases:\n        h_values = []\n        errors = []\n        \n        # Calculate the exact solution at the final time T once\n        y_exact_at_T = case[\"y_exact\"](case[\"T\"])\n        \n        # Run the simulation for each specified number of steps N\n        for N in case[\"N_values\"]:\n            t0, y0, T = case[\"t0\"], case[\"y0\"], case[\"T\"]\n            \n            # Calculate step size\n            h = (T - t0) / N\n            \n            # Get numerical solution using Heun's method\n            y_numerical_at_T = heun_solver(case[\"f\"], t0, y0, T, N)\n            \n            # Calculate global error\n            error = np.abs(y_numerical_at_T - y_exact_at_T)\n            \n            h_values.append(h)\n            errors.append(error)\n        \n        # Estimate the order of convergence p\n        p = estimate_order(np.array(h_values), np.array(errors))\n        estimated_orders.append(p)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, estimated_orders))}]\")\n\nsolve()\n```", "id": "3259641"}]}