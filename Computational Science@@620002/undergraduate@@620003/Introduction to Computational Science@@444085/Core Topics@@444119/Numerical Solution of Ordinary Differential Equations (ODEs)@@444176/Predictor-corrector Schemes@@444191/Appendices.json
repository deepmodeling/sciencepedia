{"hands_on_practices": [{"introduction": "The core idea of a predictor-corrector scheme can be understood through a simple geometric lens. When optimizing a function over a constrained set, like a manifold, the easiest path—following the negative gradient—often leads you off the feasible region. This practice [@problem_id:3163808] provides a concrete example of this scenario, where you first take an unconstrained \"predictor\" step to decrease the objective, and then apply a \"corrector\" step that projects the resulting point back onto the constraint manifold, thereby restoring feasibility. Working through this exercise builds a strong visual and mathematical intuition for how these two fundamental stages operate.", "problem": "Consider the equality-constrained optimization problem of minimizing the quadratic objective subject to a smooth manifold constraint. Let the objective be given by $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$ with $x \\in \\mathbb{R}^{2}$ and $c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, and let the constraint be $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$, which defines the unit circle. Suppose you are at the feasible point $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n\nA predictor-corrector scheme proceeds by first taking an unconstrained predictor step based on the gradient of the objective, and then correcting to restore feasibility by projecting onto the constraint manifold using a first-order local approximation. Specifically:\n\n1. Use an unconstrained gradient step to form the predictor $x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)})$ with step size $\\alpha = \\frac{1}{4}$.\n2. Using the first-order Taylor expansion of the constraint about $x^{\\mathrm{pred}}$, $h(x^{\\mathrm{pred}} + \\delta) \\approx h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta$, determine the minimal-norm correction $\\delta$ that restores feasibility, where $J_{h}(x)$ denotes the Jacobian of $h$ at $x$. Then form the corrected iterate $x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta$.\n3. Compute the objective value $f(x^{\\mathrm{corr}})$.\n\nExpress your final numerical answer for $f(x^{\\mathrm{corr}})$ exactly as a rational number. Do not round.", "solution": "The user has provided a well-posed optimization problem that requires the application of a single predictor-corrector step. The problem is scientifically grounded, self-contained, and objective. We will proceed with the solution by following the specified steps.\n\nThe objective function is $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$ where $x = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} \\in \\mathbb{R}^{2}$ and $c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$. The constraint is $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$. The initial point is $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n\n**Step 1: Predictor Step**\n\nFirst, we compute the gradient of the objective function, $\\nabla f(x)$.\nThe objective function can be written as $f(x) = \\frac{1}{2}((x_1 - 1)^2 + (x_2 - 0)^2)$.\nThe gradient is given by:\n$$ \\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix} = x - c $$\nWe evaluate the gradient at the initial point $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$ \\nabla f(x^{(0)}) = x^{(0)} - c = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} $$\nThe predictor step is an unconstrained gradient step with step size $\\alpha = \\frac{1}{4}$:\n$$ x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)}) $$\nSubstituting the values:\n$$ x^{\\mathrm{pred}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 + \\frac{1}{4} \\\\ 1 - \\frac{1}{4}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{4} \\\\ \\frac{3}{4}\\end{pmatrix} $$\n\n**Step 2: Corrector Step**\n\nThe corrector step aims to restore feasibility by moving from $x^{\\mathrm{pred}}$ by a minimal-norm correction vector $\\delta$ such that the first-order approximation of the constraint is satisfied.\nThe linearized constraint equation is $h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta = 0$.\n\nFirst, we evaluate the constraint function at $x^{\\mathrm{pred}}$:\n$$ h(x^{\\mathrm{pred}}) = \\left(\\frac{1}{4}\\right)^{2} + \\left(\\frac{3}{4}\\right)^{2} - 1 = \\frac{1}{16} + \\frac{9}{16} - 1 = \\frac{10}{16} - 1 = \\frac{5}{8} - \\frac{8}{8} = -\\frac{3}{8} $$\nNext, we compute the Jacobian of the constraint function $h(x)$. The Jacobian is a $1 \\times 2$ matrix:\n$$ J_h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1} & \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 & 2x_2 \\end{pmatrix} $$\nWe evaluate the Jacobian at $x^{\\mathrm{pred}}$:\n$$ J_h(x^{\\mathrm{pred}}) = \\begin{pmatrix} 2\\left(\\frac{1}{4}\\right) & 2\\left(\\frac{3}{4}\\right) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{3}{2} \\end{pmatrix} $$\nThe minimal-norm correction $\\delta$ that solves the linear system $J_h(x^{\\mathrm{pred}})\\,\\delta = -h(x^{\\mathrm{pred}})$ is given by the formula:\n$$ \\delta = J_h(x^{\\mathrm{pred}})^T \\left( J_h(x^{\\mathrm{pred}}) J_h(x^{\\mathrm{pred}})^T \\right)^{-1} (-h(x^{\\mathrm{pred}})) $$\nLet $J = J_h(x^{\\mathrm{pred}})$. We compute the term $JJ^T$:\n$$ JJ^T = \\begin{pmatrix} \\frac{1}{2} & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{1}{4} + \\frac{9}{4} = \\frac{10}{4} = \\frac{5}{2} $$\nThe inverse is $(JJ^T)^{-1} = \\frac{2}{5}$.\nNow we can calculate $\\delta$:\n$$ \\delta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\right) \\left( -(-\\frac{3}{8}) \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\cdot \\frac{3}{8} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{6}{40} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{3}{20} \\right) $$\n$$ \\delta = \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\nThe corrected iterate $x^{\\mathrm{corr}}$ is found by adding $\\delta$ to $x^{\\mathrm{pred}}$:\n$$ x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\nTo perform the addition, we use a common denominator of $40$:\n$$ x^{\\mathrm{corr}} = \\begin{pmatrix} \\frac{10}{40} + \\frac{3}{40} \\\\ \\frac{30}{40} + \\frac{9}{40} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\n\n**Step 3: Compute the Objective Value**\n\nFinally, we compute the objective value at the corrected point $x^{\\mathrm{corr}}$.\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\|x^{\\mathrm{corr}} - c\\|^2 $$\nFirst, calculate the vector difference $x^{\\mathrm{corr}} - c$:\n$$ x^{\\mathrm{corr}} - c = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} - \\frac{40}{40} \\\\ \\frac{39}{40} - 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{27}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\nNext, compute the squared Euclidean norm of this vector:\n$$ \\|x^{\\mathrm{corr}} - c\\|^2 = \\left(-\\frac{27}{40}\\right)^2 + \\left(\\frac{39}{40}\\right)^2 = \\frac{27^2}{40^2} + \\frac{39^2}{40^2} = \\frac{729}{1600} + \\frac{1521}{1600} = \\frac{729 + 1521}{1600} = \\frac{2250}{1600} $$\nThe objective value is half of this quantity:\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\left( \\frac{2250}{1600} \\right) = \\frac{2250}{3200} $$\nWe simplify the fraction by dividing the numerator and denominator by their greatest common divisor.\n$$ f(x^{\\mathrm{corr}}) = \\frac{225}{320} = \\frac{45}{64} $$\nThe final answer is required as an exact rational number.", "answer": "$$ \\boxed{\\frac{45}{64}} $$", "id": "3163808"}, {"introduction": "Predictor-corrector mechanisms are not just for accelerating convergence; they are also critical for ensuring an algorithm's stability and feasibility. In Interior-Point Methods (IPMs), a powerful class of algorithms for constrained optimization, iterates must remain strictly inside the feasible region. This exercise [@problem_id:3163783] demonstrates a vital safeguarding technique where a \"predictor\" step might propose a move that violates these bounds, and a \"corrector\" step, known as the fraction-to-the-boundary rule, scales it back to maintain strict feasibility. This practice highlights how the predictor-corrector pattern serves as a fundamental building block for robust, real-world solvers.", "problem": "Consider a bound-constrained convex optimization problem with simple box constraints, where the variable vector $x \\in \\mathbb{R}^{n}$ is required to satisfy $0 < x < u$ componentwise, and the method uses a logarithmic barrier of the form $\\phi(x) = f(x) - \\mu \\sum_{i=1}^{n} \\left( \\ln(x_{i}) + \\ln(u_{i} - x_{i}) \\right)$ for some barrier parameter $\\mu > 0$. In the Interior-Point Method (IPM), a Mehrotra-style predictor-corrector step proposes a tentative displacement $d \\in \\mathbb{R}^{n}$ from the current strictly feasible point $x$, but the predictor may heavily violate the bounds if taken with unit step length. To preserve strict feasibility of both the primal variables $x$ and the associated upper-bound slacks $s = u - x$ at the next iterate $x^{+} = x + \\alpha d$ and $s^{+} = s - \\alpha d$, the acceptance step length $\\alpha$ must be chosen to keep $x^{+} > 0$ and $s^{+} > 0$ componentwise.\n\nStarting from the fundamental requirement that the logarithmic barrier is only defined on the interior, namely $x_{i} > 0$ and $s_{i} = u_{i} - x_{i} > 0$ for all $i$, derive the largest step length $\\alpha_{\\max}$ that maintains strict positivity for both $x^{+}$ and $s^{+}$ along the given direction $d$. Then, apply the fraction-to-the-boundary safeguard with parameter $\\tau \\in (0,1)$ to obtain a corrected step length $\\alpha_{\\text{safe}}$ suitable for use in the predictor-corrector scheme.\n\nUse the following data for a three-dimensional instance:\n- $x = (4,\\, 1.5,\\, 6)$,\n- $u = (5,\\, 2,\\, 7)$,\n- $d = (-3,\\, -1,\\, 2)$,\n- $\\tau = 0.8$.\n\nCompute the safeguarded step length $\\alpha_{\\text{safe}}$. Provide your final answer as a single real number. If any rounding were needed, you would round to four significant figures; however, if an exact value is obtained, report it exactly.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Problem Context**: A bound-constrained convex optimization problem.\n- **Constraints**: $0 < x < u$ componentwise, for $x \\in \\mathbb{R}^{n}$.\n- **Barrier Function**: $\\phi(x) = f(x) - \\mu \\sum_{i=1}^{n} \\left( \\ln(x_{i}) + \\ln(u_{i} - x_{i}) \\right)$ for $\\mu > 0$.\n- **Method**: Interior-Point Method (IPM) with a predictor-corrector step.\n- **Current Primal Iterate**: A strictly feasible point $x$.\n- **Search Direction**: $d \\in \\mathbb{R}^{n}$.\n- **Upper-Bound Slacks**: $s = u - x$.\n- **Next Iterates**: $x^{+} = x + \\alpha d$ and $s^{+} = s - \\alpha d$.\n- **Feasibility Requirement**: $x^{+} > 0$ and $s^{+} > 0$, componentwise.\n- **Step Lengths**: $\\alpha_{\\max}$ is the largest step length maintaining strict positivity, and $\\alpha_{\\text{safe}}$ is a safeguarded step.\n- **Safeguard Rule**: $\\alpha_{\\text{safe}} = \\tau \\alpha_{\\max}$ with $\\tau \\in (0,1)$.\n- **Instance Data ($n=3$):**\n  - $x = (4,\\, 1.5,\\, 6)$\n  - $u = (5,\\, 2,\\, 7)$\n  - $d = (-3,\\, -1,\\, 2)$\n  - $\\tau = 0.8$\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, describing a standard \"fraction-to-the-boundary\" rule used in Interior-Point Methods for optimization. It is well-posed, providing all necessary data to compute a unique solution. The data is consistent; the initial point $x$ is strictly feasible as required, since $0 < 4 < 5$, $0 < 1.5 < 2$, and $0 < 6 < 7$. There are no ambiguities, contradictions, or violations of scientific principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\n### Solution Derivation\n\nThe goal is to compute the safeguarded step length $\\alpha_{\\text{safe}}$. This requires first determining $\\alpha_{\\max}$, the maximum step length $\\alpha > 0$ that maintains strict feasibility for the next iterate. Strict feasibility means that for every component $i \\in \\{1, 2, \\dots, n\\}$, the conditions $x_i^{+} > 0$ and $s_i^{+} > 0$ must hold.\n\nThe next primal iterate is $x^{+} = x + \\alpha d$. The condition $x^{+} > 0$ implies $x_i + \\alpha d_i > 0$ for each component $i$.\n- If $d_i \\ge 0$, since $x_i > 0$ (by initial feasibility) and we seek $\\alpha > 0$, this inequality $x_i + \\alpha d_i > 0$ is always satisfied and imposes no upper bound on $\\alpha$.\n- If $d_i < 0$, the inequality can be rearranged to $\\alpha (-d_i) < x_i$. Since $-d_i > 0$, we can divide to obtain an upper bound on the step length: $\\alpha < \\frac{x_i}{-d_i}$.\n\nThe next slack iterate is $s^{+} = s - \\alpha d$, where $s = u - x$. The condition $s^{+} > 0$ implies $s_i - \\alpha d_i > 0$ for each component $i$.\n- If $d_i \\le 0$, since $s_i > 0$ (by initial feasibility) and $\\alpha > 0$, the term $-\\alpha d_i$ is non-negative, so the inequality $s_i - \\alpha d_i > 0$ is always satisfied and imposes no upper bound on $\\alpha$.\n- If $d_i > 0$, the inequality can be rearranged to $\\alpha d_i < s_i$. This gives an upper bound on the step length: $\\alpha < \\frac{s_i}{d_i}$.\n\nTo satisfy all these conditions simultaneously for all $i$, $\\alpha$ must be smaller than the minimum of all derived upper bounds. The term $\\alpha_{\\max}$ in this context refers to the step length that takes one component of either $x^{+}$ or $s^{+}$ to its boundary value of $0$. Therefore, $\\alpha_{\\max}$ is the minimum of all the limiting step ratios.\n\n$$\n\\alpha_{\\max} = \\min \\left( \\min_{i: d_i < 0} \\left\\{ \\frac{x_i}{-d_i} \\right\\}, \\min_{i: d_i > 0} \\left\\{ \\frac{s_i}{d_i} \\right\\} \\right)\n$$\n\nNow, we apply this formula to the given three-dimensional data:\n- $x = (4, 1.5, 6)$, so $x_1=4$, $x_2=1.5$, $x_3=6$.\n- $u = (5, 2, 7)$, so $u_1=5$, $u_2=2$, $u_3=7$.\n- $d = (-3, -1, 2)$, so $d_1=-3$, $d_2=-1$, $d_3=2$.\n\nFirst, we compute the slack variables $s = u - x$:\n- $s_1 = u_1 - x_1 = 5 - 4 = 1$.\n- $s_2 = u_2 - x_2 = 2 - 1.5 = 0.5$.\n- $s_3 = u_3 - x_3 = 7 - 6 = 1$.\n\nNext, we calculate the step limits for each component that provides a bound.\nThe set of indices where $d_i < 0$ is $\\{1, 2\\}$.\n- For $i=1$: The limit is $\\frac{x_1}{-d_1} = \\frac{4}{-(-3)} = \\frac{4}{3}$.\n- For $i=2$: The limit is $\\frac{x_2}{-d_2} = \\frac{1.5}{-(-1)} = 1.5 = \\frac{3}{2}$.\n\nThe set of indices where $d_i > 0$ is $\\{3\\}$.\n- For $i=3$: The limit is $\\frac{s_3}{d_3} = \\frac{1}{2}$.\n\nNow, we find $\\alpha_{\\max}$ by taking the minimum of these computed limits:\n$$\n\\alpha_{\\max} = \\min\\left(\\frac{4}{3}, \\frac{3}{2}, \\frac{1}{2}\\right)\n$$\nComparing the values: $\\frac{4}{3} \\approx 1.333\\dots$, $\\frac{3}{2} = 1.5$, and $\\frac{1}{2} = 0.5$. The minimum is $\\frac{1}{2}$.\n$$\n\\alpha_{\\max} = 0.5\n$$\n\nFinally, we apply the fraction-to-the-boundary safeguard using the given parameter $\\tau = 0.8$:\n$$\n\\alpha_{\\text{safe}} = \\tau \\alpha_{\\max}\n$$\nSubstituting the values:\n$$\n\\alpha_{\\text{safe}} = 0.8 \\times 0.5 = 0.4\n$$\nThe result is an exact decimal value.", "answer": "$$\\boxed{0.4}$$", "id": "3163783"}, {"introduction": "The true power of the predictor-corrector pattern lies in its flexibility as an algorithmic design template. This advanced practice [@problem_id:3163759] challenges you to move beyond applying existing methods and instead design and test a novel predictor-corrector scheme for the Lasso problem, a cornerstone of modern machine learning and statistics. By combining the simple Iterative Shrinkage-Thresholding Algorithm (ISTA) as a predictor and an acceleration step inspired by FISTA as a corrector, you will explore the conditions under which such a hybrid approach can successfully accelerate convergence without overshooting the solution. This computational exercise provides hands-on experience in algorithmic experimentation and evaluation.", "problem": "Consider the convex optimization problem known as the Least Absolute Shrinkage and Selection Operator (Lasso), defined by the objective function\n$$\nF(x) \\;=\\; \\phi(x) \\;+\\; \\lambda \\,\\|x\\|_1,\\qquad \\phi(x) \\;=\\; \\frac{1}{2}\\,\\|A x - b\\|_2^2,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $\\lambda > 0$, and $x \\in \\mathbb{R}^n$. The smooth part $\\phi(x)$ is differentiable with gradient\n$$\n\\nabla \\phi(x) \\;=\\; A^\\top (A x - b),\n$$\nwhose gradient is Lipschitz continuous with constant $L \\ge \\|A^\\top A\\|_2$ in the spectral norm. The non-smooth part is the scaled $\\ell_1$-norm, whose proximal operator is the soft-thresholding (shrinkage) mapping\n$$\n\\operatorname{shrink}(z, \\tau)_i \\;=\\; \\operatorname{sign}(z_i)\\,\\max\\!\\big(|z_i| - \\tau,\\, 0\\big), \\quad \\text{for each coordinate } i,\n$$\nwhich equals the proximal operator of $\\tau \\,\\|x\\|_1$ at point $z$.\n\nYou will design a predictor-corrector scheme for minimizing $F(x)$ starting from a current iterate $x_k$ and a previous iterate $x_{k-1}$. The predictor is one Iterative Shrinkage-Thresholding Algorithm (ISTA) step, and the corrector is one Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) step. Your tasks:\n\n1) Predictor (ISTA): Using a stepsize $s \\in (0, 1/L]$, compute\n$$\nx_{\\mathrm{p}} \\;=\\; \\operatorname{shrink}\\!\\big(x_k - s\\,\\nabla \\phi(x_k),\\; s\\,\\lambda\\big).\n$$\n\n2) Corrector (one FISTA step): With the classical FISTA momentum parameters defined by the sequence $t_0 = 1$ and\n$$\nt_{j} \\;=\\; \\frac{1 + \\sqrt{1 + 4\\,t_{j-1}^2}}{2},\\quad \\text{for } j \\ge 1,\n$$\nform the one-step acceleration coefficient\n$$\n\\beta \\;=\\; \\frac{t_{1} - 1}{t_{2}},\n$$\nand compute the extrapolated point\n$$\ny \\;=\\; x_{\\mathrm{p}} + \\beta\\,(x_{\\mathrm{p}} - x_{k-1}),\n$$\nfollowed by one proximal-gradient update at $y$:\n$$\nx_{\\mathrm{c}} \\;=\\; \\operatorname{shrink}\\!\\big(y - s\\,\\nabla \\phi(y),\\; s\\,\\lambda\\big).\n$$\n\n3) Conditions to test: We say the corrector accelerates without overshooting when both of the following hold:\n- Acceleration: $F(x_{\\mathrm{c}}) < F(x_{\\mathrm{p}})$.\n- No overshoot: $F(x_{\\mathrm{c}}) \\le F(x_k)$.\nBecause of finite precision, adopt a non-negativity tolerance $\\varepsilon = 10^{-10}$ and interpret $a \\le b$ as $a \\le b + \\varepsilon$ and $a < b$ as $a \\le b - \\varepsilon$.\n\n4) Initialization protocol: For a given test case $(A, b, \\lambda, \\alpha)$, estimate $L$ by $L = \\|A\\|_2^2$ using the spectral norm of $A$, and set $s = \\alpha/L$ with $\\alpha \\in (0, 1]$. Initialize at $x_0 = 0$. Compute two warm-up ISTA steps\n$$\nx_1 \\;=\\; \\operatorname{shrink}\\!\\big(x_0 - s\\,\\nabla \\phi(x_0),\\; s\\,\\lambda\\big),\\qquad\nx_2 \\;=\\; \\operatorname{shrink}\\!\\big(x_1 - s\\,\\nabla \\phi(x_1),\\; s\\,\\lambda\\big).\n$$\nThen apply the predictor-corrector at $k = 2$ using $(x_{k-1}, x_k) = (x_1, x_2)$ to obtain $x_{\\mathrm{p}}$ and $x_{\\mathrm{c}}$, and evaluate the two inequalities in item $3$.\n\n5) Test suite: Use the following four test cases. Each case uses $m = 2$ and $n = 2$, and $b = [1, 0]^\\top$. The matrix $A$ is symmetric with correlation parameter $\\rho$ via\n$$\nA \\;=\\; \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}.\n$$\nEvaluate the boolean indicating “accelerates without overshooting” for each case.\n- Case $1$ (happy path): $\\rho = 0.3$, $\\lambda = 0.1$, $\\alpha = 1.0$.\n- Case $2$ (ill-conditioned challenge): $\\rho = 0.99$, $\\lambda = 0.001$, $\\alpha = 1.0$.\n- Case $3$ (strong regularization): $\\rho = 0.3$, $\\lambda = 0.6$, $\\alpha = 1.0$.\n- Case $4$ (boundary decoupled case): $\\rho = 0$, $\\lambda = 0.1$, $\\alpha = 1.0$.\n\n6) Required final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain four boolean values corresponding to the four cases in the order listed above. For example, the output format must look like\n$$\n[\\text{true}_1,\\text{true}_2,\\text{true}_3,\\text{true}_4]\n$$\nbut with Python boolean capitalization, that is, exactly like $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}]$ (this example is only illustrative; you must output the actual computed results for the specified test cases).", "solution": "The user has provided a well-defined computational problem in the field of numerical optimization. I will first validate the problem statement and then provide a comprehensive solution.\n\n### Problem Validation\n\n1.  **Extract Givens**:\n    *   **Objective Function**: $F(x) = \\phi(x) + \\lambda \\|x\\|_1$, with $\\phi(x) = \\frac{1}{2}\\|Ax - b\\|_2^2$.\n    *   **Data**: $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $\\lambda > 0$, $x \\in \\mathbb{R}^n$.\n    *   **Gradient**: $\\nabla \\phi(x) = A^\\top (Ax - b)$.\n    *   **Lipschitz Constant of $\\nabla\\phi$**: $L \\ge \\|A^\\top A\\|_2$.\n    *   **Proximal Operator**: $\\operatorname{shrink}(z, \\tau)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\tau, 0)$.\n    *   **Predictor Step (ISTA)**: $x_{\\mathrm{p}} = \\operatorname{shrink}(x_k - s\\nabla \\phi(x_k), s\\lambda)$ with stepsize $s \\in (0, 1/L]$.\n    *   **Corrector Step (FISTA-like)**:\n        *   Momentum parameters: $t_0 = 1$, $t_{j} = (1 + \\sqrt{1 + 4t_{j-1}^2})/2$ for $j \\ge 1$.\n        *   Acceleration coefficient: $\\beta = (t_1 - 1)/t_2$.\n        *   Extrapolation: $y = x_{\\mathrm{p}} + \\beta(x_{\\mathrm{p}} - x_{k-1})$.\n        *   Correction: $x_{\\mathrm{c}} = \\operatorname{shrink}(y - s\\nabla \\phi(y), s\\lambda)$.\n    *   **Evaluation Conditions**: \"Accelerates without overshooting\" holds if both $F(x_{\\mathrm{c}}) < F(x_{\\mathrm{p}})$ and $F(x_{\\mathrm{c}}) \\le F(x_k)$ are true, interpreted with a tolerance $\\varepsilon = 10^{-10}$ such that $a < b$ is $a \\le b - \\varepsilon$ and $a \\le b$ is $a \\le b + \\varepsilon$.\n    *   **Initialization**: For a case $(A, b, \\lambda, \\alpha)$, set $L = \\|A\\|_2^2$, $s = \\alpha/L$. Start with $x_0 = 0$. Compute $2$ warm-up ISTA steps to get $x_1$ and $x_2$.\n    *   **Test Execution**: Apply the predictor-corrector scheme at iteration $k=2$, with current iterate $x_k=x_2$ and previous iterate $x_{k-1}=x_1$.\n    *   **Test Suite**: $m=2$, $n=2$, $b = [1, 0]^\\top$, $A = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}$. Four cases are specified with different values for $(\\rho, \\lambda, \\alpha)$.\n\n2.  **Validate Using Extracted Givens**:\n    *   **Scientific Grounding**: The problem is well-grounded in the theory of convex optimization and first-order methods. The Lasso objective, ISTA, and FISTA are standard topics. The specific predictor-corrector scheme is a valid, though custom, construction based on these established algorithms. The choice of the Lipschitz constant $L = \\|A\\|_2^2$ is correct, as for a symmetric matrix $A$, $\\|A^\\top A\\|_2 = \\|A^2\\|_2 = \\|A\\|_2^2$. For a general matrix, it is also true that $\\|A^\\top A\\|_2 = \\|A\\|_2^2$.\n    *   **Well-Posedness and Completeness**: The problem is deterministic and provides all necessary information: functions, algorithmic steps, initial conditions, parameter values for all test cases, and precise evaluation criteria including numerical tolerances. It is unambiguous and self-contained.\n    *   **Objectivity and Feasibility**: The problem is stated using objective mathematical language. The test cases involve small ($2 \\times 2$) matrices, making the computations entirely feasible.\n\n3.  **Verdict and Action**: The problem statement is valid. It is a clear, consistent, and computationally sound task. I will proceed with the solution.\n\n### Solution Derivation\n\nThe problem requires the implementation and evaluation of a specific predictor-corrector algorithm for solving the Lasso optimization problem. The solution will be constructed by following the provided steps precisely.\n\n**1. The Lasso Problem and Proximal Gradient Methods**\n\nThe objective is to minimize the function $F(x) = \\phi(x) + g(x)$, where $\\phi(x) = \\frac{1}{2}\\|Ax - b\\|_2^2$ is a smooth, convex, differentiable function, and $g(x) = \\lambda \\|x\\|_1$ is a convex but non-differentiable regularization term.\n\nProblems of this structure are effectively solved by proximal gradient methods. The core idea is to iteratively perform a gradient descent step on the smooth part $\\phi(x)$ and then apply the proximal operator of the non-smooth part $g(x)$ to correct the step. The general update rule is:\n$$\nx_{k+1} = \\operatorname{prox}_{sg}(x_k - s\\nabla \\phi(x_k))\n$$\nwhere $s > 0$ is the stepsize. For $g(x) = \\lambda\\|x\\|_1$, the proximal operator $\\operatorname{prox}_{s\\lambda\\|\\cdot\\|_1}(z)$ is the soft-thresholding function, $\\operatorname{shrink}(z, s\\lambda)$, defined element-wise as:\n$$\n\\operatorname{shrink}(z, \\tau)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\tau, 0)\n$$\nThis iterative scheme is known as the Iterative Shrinkage-Thresholding Algorithm (ISTA). For convergence, the stepsize $s$ must be in the interval $(0, 1/L]$, where $L$ is the Lipschitz constant of $\\nabla\\phi(x)$.\n\n**2. The Specified Predictor-Corrector Scheme**\n\nThe algorithm we must implement is a hybrid scheme. It starts from a current iterate $x_k$ and a previous iterate $x_{k-1}$.\n\n*   **Initialization**: We start at $x_0=0$. To obtain a reasonable starting pair $(x_1, x_2)$, we perform $2$ \"warm-up\" steps of ISTA. The stepsize $s$ is fixed as $s = \\alpha/L$, where $L = \\|A\\|_2^2$ is the Lipschitz constant of $\\nabla\\phi$. For the given test cases, $\\alpha=1.0$, so we use the full step $s=1/L$.\n    $$\n    x_1 = \\operatorname{shrink}(x_0 - s\\nabla \\phi(x_0), s\\lambda)\n    $$\n    $$\n    x_2 = \\operatorname{shrink}(x_1 - s\\nabla \\phi(x_1), s\\lambda)\n    $$\n    These iterates, $x_1$ and $x_2$, will serve as the initial state $(x_{k-1}, x_k)$ for the main algorithm, with $k=2$.\n\n*   **Predictor**: The predictor step is a single application of ISTA starting from $x_k = x_2$:\n    $$\n    x_{\\mathrm{p}} = \\operatorname{shrink}(x_k - s\\nabla \\phi(x_k), s\\lambda)\n    $$\n    This produces a candidate solution $x_{\\mathrm{p}}$, which would be the next iterate, $x_3$, in a standard ISTA sequence. By construction, proximal gradient descent guarantees a decrease in the objective function value, i.e., $F(x_{\\mathrm{p}}) \\le F(x_k)$.\n\n*   **Corrector**: The corrector step aims to improve upon the predictor by incorporating momentum, in the spirit of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). FISTA accelerates convergence by taking the proximal gradient step at an extrapolated point. Here, the extrapolation is defined as:\n    $$\n    y = x_{\\mathrm{p}} + \\beta(x_{\\mathrm{p}} - x_{k-1})\n    $$\n    where $x_{k-1} = x_1$. The coefficient $\\beta$ is a fixed, one-step momentum parameter derived from the FISTA recurrence relations:\n    $$\n    t_0 = 1, \\quad t_1 = \\frac{1 + \\sqrt{1 + 4t_0^2}}{2} = \\frac{1 + \\sqrt{5}}{2}, \\quad t_2 = \\frac{1 + \\sqrt{1 + 4t_1^2}}{2}\n    $$\n    $$\n    \\beta = \\frac{t_1 - 1}{t_2}\n    $$\n    The corrector iterate $x_{\\mathrm{c}}$ is then obtained by applying the proximal gradient update at this extrapolated point $y$:\n    $$\n    x_{\\mathrm{c}} = \\operatorname{shrink}(y - s\\nabla \\phi(y), s\\lambda)\n    $$\n\n**3. Evaluation Logic**\n\nThe performance of the corrector is judged by two conditions, which must both be met:\n1.  **Acceleration**: $F(x_{\\mathrm{c}}) < F(x_{\\mathrm{p}})$. This condition checks if the corrector step actually improved upon the simple predictor step. The strict inequality is important.\n2.  **No Overshoot**: $F(x_{\\mathrm{c}}) \\le F(x_k)$. The momentum term in accelerated methods can cause oscillations, where an iterate might temporarily increase the objective function value. This condition ensures that the corrector step does not produce a worse result than the starting iterate $x_k$.\n\nThe numerical implementation will use a tolerance of $\\varepsilon = 10^{-10}$ to make these comparisons robust against floating-point inaccuracies.\n\n**4. Execution on Test Cases**\n\nThe procedure is applied to each of the $4$ specified test cases. For each case, characterized by parameters $(\\rho, \\lambda, \\alpha)$:\n1.  The matrix $A$ and vector $b$ are constructed.\n2.  The Lipschitz constant $L = \\|A\\|_2^2$ and stepsize $s = \\alpha/L$ are computed.\n3.  The warm-up iterates $x_1$ and $x_2$ are calculated from $x_0=0$.\n4.  The predictor $x_{\\mathrm{p}}$ is calculated from $x_k=x_2$.\n5.  The momentum coefficient $\\beta$ is calculated, and the corrector $x_{\\mathrm{c}}$ is computed using the extrapolation from $(x_{\\mathrm{p}}, x_1)$.\n6.  The objective function values $F(x_2)$, $F(x_{\\mathrm{p}})$, and $F(x_{\\mathrm{c}})$ are calculated.\n7.  The two conditions for \"accelerates without overshooting\" are evaluated, and the logical conjunction of their results determines the final boolean output for that case.\n\nThis complete, deterministic procedure will be encoded in the provided Python script.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a predictor-corrector scheme for the Lasso problem\n    on a suite of four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (rho, lambda, alpha)\n    test_cases = [\n        (0.3, 0.1, 1.0),    # Case 1: happy path\n        (0.99, 0.001, 1.0), # Case 2: ill-conditioned challenge\n        (0.3, 0.6, 1.0),    # Case 3: strong regularization\n        (0.0, 0.1, 1.0),    # Case 4: boundary decoupled case\n    ]\n\n    results = []\n    \n    # Define helper functions based on the problem statement\n    \n    def objective_F(x, A, b, lambda_val):\n        \"\"\"Computes the Lasso objective function F(x).\"\"\"\n        smooth_part = 0.5 * np.linalg.norm(A @ x - b)**2\n        nonsmooth_part = lambda_val * np.linalg.norm(x, 1)\n        return smooth_part + nonsmooth_part\n\n    def grad_phi(x, A, b):\n        \"\"\"Computes the gradient of the smooth part phi(x).\"\"\"\n        return A.T @ (A @ x - b)\n\n    def shrink(z, tau):\n        \"\"\"Applies the soft-thresholding operator element-wise.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    for case_params in test_cases:\n        rho, lambda_val, alpha = case_params\n        \n        # Setup for the test case\n        m, n = 2, 2\n        b = np.array([1.0, 0.0])\n        A = np.array([[1.0, rho], [rho, 1.0]])\n        \n        # Initialization protocol\n        # L must be >= ||A^T A||_2. We use L = ||A||_2^2 which is equal to ||A^T A||_2\n        L = np.linalg.norm(A, 2)**2\n        s = alpha / L\n        epsilon = 1e-10\n\n        # Initial point\n        x0 = np.zeros(n)\n\n        # Two warm-up ISTA steps\n        x1 = shrink(x0 - s * grad_phi(x0, A, b), s * lambda_val)\n        x2 = shrink(x1 - s * grad_phi(x1, A, b), s * lambda_val)\n        \n        # Set iterates for the main step (k=2)\n        xk_minus_1 = x1\n        xk = x2\n\n        # 1) Predictor (ISTA step)\n        xp = shrink(xk - s * grad_phi(xk, A, b), s * lambda_val)\n\n        # 2) Corrector (one FISTA step)\n        # Compute momentum coefficient beta\n        t0 = 1.0\n        t1 = (1.0 + np.sqrt(1.0 + 4.0 * t0**2)) / 2.0\n        t2 = (1.0 + np.sqrt(1.0 + 4.0 * t1**2)) / 2.0\n        beta = (t1 - 1.0) / t2\n        \n        # Extrapolate\n        y = xp + beta * (xp - xk_minus_1)\n        \n        # Proximal-gradient update at y\n        xc = shrink(y - s * grad_phi(y, A, b), s * lambda_val)\n        \n        # 3) Evaluate conditions\n        F_xk = objective_F(xk, A, b, lambda_val)\n        F_xp = objective_F(xp, A, b, lambda_val)\n        F_xc = objective_F(xc, A, b, lambda_val)\n\n        # Condition: Acceleration\n        accelerates = (F_xc = F_xp - epsilon)\n        \n        # Condition: No overshoot\n        no_overshoot = (F_xc = F_xk + epsilon)\n\n        # Final boolean result for the case\n        case_result = accelerates and no_overshoot\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3163759"}]}