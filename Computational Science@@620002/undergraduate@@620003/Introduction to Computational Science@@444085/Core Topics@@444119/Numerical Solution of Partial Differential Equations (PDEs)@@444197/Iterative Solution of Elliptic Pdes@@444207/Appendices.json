{"hands_on_practices": [{"introduction": "To build effective iterative solvers, we must first understand their convergence properties. This practice delves into the theoretical foundation of stationary iterative methods by using Local Fourier Analysis (LFA) to analyze how basic schemes like the Jacobi and Gauss-Seidel methods handle errors. By examining how these methods dampen error components of different frequencies, you will not only see why Gauss-Seidel often outperforms Jacobi but also derive the optimal relaxation parameter, $\\omega$, for the Successive Over-Relaxation (SOR) method—a key technique for accelerating convergence [@problem_id:3148203]. This exercise provides a crucial theoretical lens for evaluating and improving iterative solvers.", "problem": "Consider the one-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$ with Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$. Discretize the interval into $N$ interior points with uniform spacing $h = \\frac{1}{N+1}$, and approximate $-u''(x)$ using the second-order centered finite difference scheme to obtain the linear system $A \\mathbf{u} = \\mathbf{b}$, where $A$ is the tridiagonal matrix with diagonal entries $\\frac{2}{h^2}$ and sub- and super-diagonal entries $-\\frac{1}{h^2}$. Consider stationary iterative methods defined by standard matrix splittings: the Jacobi method, the Gauss–Seidel method with forward lexicographic ordering, and the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega \\in (0,2)$.\n\nUsing Local Fourier Analysis (LFA) of error modes $e^{\\mathrm{i} j \\theta}$ (with integer grid index $j$ and discrete frequencies $\\theta \\in (0,\\pi)$ consistent with Dirichlet boundary conditions), derive the error amplification factors for the Jacobi and Gauss–Seidel iterations on this grid. From these, obtain the spectral radii of their iteration operators in terms of $N$, and explain which method damps high-frequency error components more effectively.\n\nThen, using the same LFA framework applied to the SOR error update, determine the value of the relaxation parameter $\\omega$ that minimizes the spectral radius of the SOR iteration operator for this problem. Express your final answer as a closed-form analytic expression depending on $N$ only. No rounding is required. The relaxation parameter is dimensionless; do not include units.", "solution": "The user wants a detailed analysis of stationary iterative methods for a 1D Poisson problem. I will first validate the problem statement and then proceed with the derivations as requested.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Equation:** One-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$.\n-   **Boundary Conditions:** Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$.\n-   **Discretization:** The interval is discretized into $N$ interior points with uniform spacing $h = \\frac{1}{N+1}$.\n-   **Numerical Scheme:** Second-order centered finite difference for $-u''(x)$.\n-   **Linear System:** $A \\mathbf{u} = \\mathbf{b}$, where $A$ is a tridiagonal matrix with diagonal entries $\\frac{2}{h^2}$ and sub- and super-diagonal entries $-\\frac{1}{h^2}$.\n-   **Iterative Methods:** Jacobi, Gauss–Seidel (forward lexicographic ordering), and Successive Over-Relaxation (SOR) with parameter $\\omega \\in (0,2)$.\n-   **Analysis Method:** Local Fourier Analysis (LFA) of error modes $e^{\\mathrm{i} j \\theta}$ with integer grid index $j$ and discrete frequencies $\\theta \\in (0,\\pi)$ consistent with Dirichlet boundary conditions.\n-   **Tasks:**\n    1.  Derive error amplification factors for Jacobi and Gauss–Seidel using LFA.\n    2.  Obtain the spectral radii of their iteration operators in terms of $N$.\n    3.  Explain which method damps high-frequency error components more effectively.\n    4.  Using the LFA framework for the SOR error update, determine the optimal relaxation parameter $\\omega$ that minimizes the SOR spectral radius.\n    5.  Express the final answer for $\\omega$ as a closed-form expression in $N$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a classic textbook example in numerical analysis and computational science. It deals with the finite difference discretization of the Poisson equation and the analysis of standard iterative solvers (Jacobi, Gauss-Seidel, SOR). The use of Local Fourier Analysis (also known as von Neumann stability analysis) is a standard technique for analyzing such methods. All concepts are well-established and scientifically sound.\n-   **Well-Posed:** The problem is well-defined. It asks for specific derivations (amplification factors, spectral radii) and an optimization problem (finding optimal $\\omega$). All necessary information and context are provided. The existence and uniqueness of the results are guaranteed by the theory of iterative methods.\n-   **Objective:** The problem is stated in precise, formal mathematical language, free of any subjectivity, ambiguity, or opinion.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard, well-posed problem in the field of computational science. I will now proceed to the solution.\n\n### Solution Derivation\n\nThe given partial differential equation is $-u''(x) = f(x)$ with $u(0)=u(1)=0$.\nLet the grid points be $x_j = jh$ for $j=0, 1, \\dots, N+1$, where $h = \\frac{1}{N+1}$. The discretized equation at an interior grid point $x_j$ using the second-order centered finite difference scheme is:\n$$-\\frac{u(x_{j-1}) - 2u(x_j) + u(x_{j+1})}{h^2} \\approx f(x_j)$$\nLet $u_j$ be the numerical approximation to $u(x_j)$. The system of linear equations is $A \\mathbf{u} = \\mathbf{b}$, where the equation at row $j$ is:\n$$\\frac{1}{h^2}(2u_j - u_{j-1} - u_{j+1}) = f_j$$\nThe error at iteration $k$ is defined as $e_j^{(k)} = u_j - u_j^{(k)}$, where $u_j$ is the exact solution to the discrete system. The error propagates according to the homogeneous version of the iterative scheme.\n\n**Jacobi Method**\nThe Jacobi iteration updates all components of the solution vector simultaneously, using values from the previous iteration $k$:\n$$u_j^{(k+1)} = \\frac{1}{2}(u_{j-1}^{(k)} + u_{j+1}^{(k)}) + \\frac{h^2}{2}f_j$$\nThe exact solution $u_j$ satisfies $u_j = \\frac{1}{2}(u_{j-1} + u_{j+1}) + \\frac{h^2}{2}f_j$. Subtracting the iteration equation from this gives the error propagation equation:\n$$e_j^{(k+1)} = \\frac{1}{2}(e_{j-1}^{(k)} + e_{j+1}^{(k)})$$\nWe perform Local Fourier Analysis by substituting a single error mode $e_j^{(k)} = (\\lambda_J(\\theta))^k e^{\\mathrm{i} j \\theta}$, where $\\lambda_J(\\theta)$ is the amplification factor.\n$$(\\lambda_J(\\theta))^{k+1} e^{\\mathrm{i} j \\theta} = \\frac{1}{2} \\left( (\\lambda_J(\\theta))^k e^{\\mathrm{i} (j-1) \\theta} + (\\lambda_J(\\theta))^k e^{\\mathrm{i} (j+1) \\theta} \\right)$$\nDividing by $(\\lambda_J(\\theta))^k e^{\\mathrm{i} j \\theta}$ gives the amplification factor:\n$$\\lambda_J(\\theta) = \\frac{1}{2} (e^{-\\mathrm{i} \\theta} + e^{\\mathrm{i} \\theta}) = \\cos(\\theta)$$\n\n**Gauss-Seidel Method**\nThe Gauss-Seidel method with forward lexicographic ordering uses the most up-to-date values available within the current iteration:\n$$u_j^{(k+1)} = \\frac{1}{2}(u_{j-1}^{(k+1)} + u_{j+1}^{(k)}) + \\frac{h^2}{2}f_j$$\nThe error propagation equation is:\n$$e_j^{(k+1)} = \\frac{1}{2}(e_{j-1}^{(k+1)} + e_{j+1}^{(k)})$$\nFor methods like Gauss-Seidel with a preferential sweep direction, a single Fourier mode $e^{\\mathrm{i} j\\theta}$ is not an eigenvector of the iteration operator. A rigorous LFA is more complex. However, LFA can still be used to derive a \"symbol\" of the operator that describes how a single Fourier mode is transformed, which is crucial for analyzing smoothing properties. Substituting $e_j^{(k)} = (\\lambda_{GS}(\\theta))^k e^{\\mathrm{i}j\\theta}$ into the error update:\n$$(\\lambda_{GS}(\\theta))^{k+1} e^{\\mathrm{i} j \\theta} = \\frac{1}{2} \\left( (\\lambda_{GS}(\\theta))^{k+1} e^{\\mathrm{i} (j-1) \\theta} + (\\lambda_{GS}(\\theta))^k e^{\\mathrm{i} (j+1) \\theta} \\right)$$\n$$\\lambda_{GS}(\\theta) e^{\\mathrm{i} j \\theta} = \\frac{1}{2} \\left( \\lambda_{GS}(\\theta) e^{\\mathrm{i} (j-1) \\theta} + e^{\\mathrm{i} (j+1) \\theta} \\right)$$\n$$\\lambda_{GS}(\\theta) (1 - \\frac{1}{2} e^{-\\mathrm{i} \\theta}) = \\frac{1}{2} e^{\\mathrm{i} \\theta}$$\n$$\\lambda_{GS}(\\theta) = \\frac{e^{\\mathrm{i} \\theta}}{2 - e^{-\\mathrm{i} \\theta}}$$\nThis complex quantity is the LFA symbol for Gauss-Seidel. While not a true eigenvalue, its magnitude determines the damping of the mode $\\theta$.\n\n**Spectral Radii and High-Frequency Damping**\nThe Dirichlet boundary conditions $u_0 = u_{N+1} = 0$ imply that the error is also zero at the boundaries. The corresponding discrete Fourier modes are sine functions, which are compositions of complex exponentials $e^{\\mathrm{i}j\\theta}$. The allowed discrete frequencies are $\\theta_m = \\frac{m\\pi}{N+1}$ for $m = 1, 2, \\dots, N$.\n\nThe spectral radius of an iteration operator is the maximum of the magnitudes of its eigenvalues. For the Jacobi method applied to this problem, the Fourier modes are the eigenvectors, so the eigenvalues of the iteration matrix $T_J$ are $\\mu_m = \\lambda_J(\\theta_m) = \\cos(\\frac{m\\pi}{N+1})$. The spectral radius is:\n$$\\rho(T_J) = \\max_{m=1, \\dots, N} \\left| \\cos\\left(\\frac{m\\pi}{N+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{N+1}\\right)$$\n\nFor the Gauss-Seidel spectral radius, we cannot simply take the maximum of $|\\lambda_{GS}(\\theta_m)|$. Instead, we use a fundamental theorem for matrices that are \"consistently ordered\", such as the matrix $A$ in this problem. The theorem states that the spectral radius of the Gauss-Seidel operator is the square of the spectral radius of the Jacobi operator:\n$$\\rho(T_{GS}) = (\\rho(T_J))^2 = \\cos^2\\left(\\frac{\\pi}{N+1}\\right)$$\n\nTo compare the damping of high-frequency error components, we examine the magnitude of the amplification factors for $\\theta \\in [\\pi/2, \\pi]$.\nFor Jacobi: $|\\lambda_J(\\theta)| = |\\cos(\\theta)|$. As $\\theta \\to \\pi$, $|\\lambda_J(\\theta)| \\to 1$. Thus, Jacobi is very inefficient at damping high-frequency errors (it is a poor smoother).\nFor Gauss-Seidel, we use the LFA symbol's magnitude:\n$$|\\lambda_{GS}(\\theta)| = \\left| \\frac{e^{\\mathrm{i} \\theta}}{2 - e^{-\\mathrm{i} \\theta}} \\right| = \\frac{1}{|2 - \\cos\\theta + \\mathrm{i}\\sin\\theta|} = \\frac{1}{\\sqrt{(2-\\cos\\theta)^2 + \\sin^2\\theta}} = \\frac{1}{\\sqrt{5-4\\cos\\theta}}$$\nFor high frequencies $\\theta \\in [\\pi/2, \\pi]$, $\\cos\\theta$ ranges from $0$ to $-1$. The maximum of $|\\lambda_{GS}(\\theta)|$ in this range occurs at $\\theta=\\pi/2$, where its value is $1/\\sqrt{5} \\approx 0.447$. At $\\theta=\\pi$, the value is $1/3$. Since the amplification factor for high frequencies is bounded well below $1$, Gauss-Seidel is effective at damping high-frequency errors and is a good smoother. Clearly, Gauss-Seidel is more effective than Jacobi for this purpose.\n\n**Optimal SOR Parameter**\nThe SOR method is an extrapolation of the Gauss-Seidel method: $u_j^{(k+1)} = (1-\\omega)u_j^{(k)} + \\omega u_j^{GS}$. The error propagation equation is:\n$$e_j^{(k+1)} = (1-\\omega)e_j^{(k)} + \\frac{\\omega}{2}(e_{j-1}^{(k+1)} + e_{j+1}^{(k)})$$\nApplying a similar LFA procedure is possible but algebraically complex. A more robust approach relies on the established theory for consistently ordered matrices, which connects the eigenvalues $\\lambda$ of the SOR iteration matrix $T_{SOR}$ to the eigenvalues $\\mu$ of the Jacobi matrix $T_J$:\n$$(\\lambda + \\omega - 1)^2 = \\lambda \\omega^2 \\mu^2$$\nThe eigenvalues $\\mu$ of $T_J$ are real and lie in $(-\\rho_J, \\rho_J)$, where $\\rho_J = \\cos(\\frac{\\pi}{N+1})$. We seek to choose $\\omega$ to minimize $\\rho(T_{SOR}) = \\max_{\\mu} |\\lambda(\\mu, \\omega)|$.\n\nThe analysis of the above relation yields two regimes for the magnitude of $\\lambda$:\n1. If $\\omega^2\\mu^2 < 4(\\omega-1)$ (which requires $\\omega > 1$), the roots for $\\sqrt{\\lambda}$ are complex conjugates, and $|\\lambda| = \\omega-1$.\n2. If $\\omega^2\\mu^2 \\ge 4(\\omega-1)$, the roots are real, and the magnitude is given by $|\\lambda| = \\left( \\frac{\\omega|\\mu| + \\sqrt{\\omega^2\\mu^2 - 4(\\omega-1)}}{2} \\right)^2$. This is an increasing function of $|\\mu|$.\n\nThe optimal choice of $\\omega$, denoted $\\omega_{opt}$, occurs when the maximum eigenvalue magnitude is the same in both regimes. This happens when the critical value of $\\mu$ for which the discriminant is zero coincides with the largest Jacobi eigenvalue, $\\rho_J$.\n$$\\omega_{opt}^2 \\rho_J^2 = 4(\\omega_{opt} - 1)$$\nRearranging gives a quadratic equation for $\\omega_{opt}$:\n$$\\rho_J^2 \\omega_{opt}^2 - 4\\omega_{opt} + 4 = 0$$\nThe solutions are:\n$$\\omega_{opt} = \\frac{4 \\pm \\sqrt{16 - 16\\rho_J^2}}{2\\rho_J^2} = \\frac{2(1 \\pm \\sqrt{1-\\rho_J^2})}{\\rho_J^2}$$\nSubstituting $\\rho_J = \\cos(\\frac{\\pi}{N+1})$, and noting that $\\sqrt{1-\\rho_J^2} = \\sin(\\frac{\\pi}{N+1})$ for the given range of arguments:\n$$\\omega_{opt} = \\frac{2 \\left( 1 \\pm \\sin\\left(\\frac{\\pi}{N+1}\\right) \\right)}{\\cos^2\\left(\\frac{\\pi}{N+1}\\right)} = \\frac{2 \\left( 1 \\pm \\sin\\left(\\frac{\\pi}{N+1}\\right) \\right)}{1 - \\sin^2\\left(\\frac{\\pi}{N+1}\\right)} = \\frac{2 \\left( 1 \\pm \\sin\\left(\\frac{\\pi}{N+1}\\right) \\right)}{\\left(1 - \\sin\\left(\\frac{\\pi}{N+1}\\right)\\right)\\left(1 + \\sin\\left(\\frac{\\pi}{N+1}\\right)\\right)}$$\nThis gives two possible solutions:\n$$\\omega_1 = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{N+1}\\right)} \\quad \\text{and} \\quad \\omega_2 = \\frac{2}{1 - \\sin\\left(\\frac{\\pi}{N+1}\\right)}$$\nSince convergence of SOR is only guaranteed for $\\omega \\in (0,2)$, and for $N \\ge 1$, $\\sin(\\frac{\\pi}{N+1}) > 0$, we have $\\omega_1 \\in (1,2)$ and $\\omega_2 > 2$. Therefore, the optimal relaxation parameter is the first solution.\n$$\\omega_{opt} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{N+1}\\right)}$$", "answer": "$$ \\boxed{ \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{N+1}\\right)} } $$", "id": "3148203"}, {"introduction": "Standard point-wise iterative methods can converge very slowly when the underlying physical problem exhibits strong directional dependencies, a property known as anisotropy. This hands-on coding exercise challenges you to implement a more robust solver: line relaxation. By solving implicitly for entire lines of unknowns at once, this method addresses the strong couplings that hinder simpler schemes [@problem_id:3148150]. Comparing the convergence of line relaxation along different axes will provide a clear, practical demonstration of how tailoring a numerical method to the structure of the PDE is critical for efficient computation.", "problem": "Implement and compare line relaxation along the $x$-direction versus the $y$-direction for the anisotropic diffusion model problem given by the linear, second-order, elliptic partial differential equation (PDE): \n$$\n-\\partial_x\\big(\\alpha \\, \\partial_x u\\big) - \\partial_y\\big(\\beta \\, \\partial_y u\\big) = f \\quad \\text{in} \\quad \\Omega=(0,1)\\times(0,1),\n$$\nwith homogeneous Dirichlet boundary condition $u=0$ on $\\partial\\Omega$. Assume constant coefficients with $\\alpha = r$ and $\\beta = 1$, where $r \\gg 1$ models anisotropy aligned with the $x$-direction. Discretize $\\Omega$ on a uniform Cartesian grid with $n_x$ and $n_y$ interior points in the $x$ and $y$ directions, respectively, and grid spacings $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$. Use the standard second-order central difference discretization at each interior node $(i,j)$ for $1 \\le i \\le n_x$ and $1 \\le j \\le n_y$:\n$$\n-\\frac{\\alpha}{h_x^2}\\big(u_{i+1,j}-2u_{i,j}+u_{i-1,j}\\big) \\;-\\; \\frac{\\beta}{h_y^2}\\big(u_{i,j+1}-2u_{i,j}+u_{i,j-1}\\big) \\;=\\; f_{i,j},\n$$\nwith $u=0$ on the boundary ghost layers. Let $f(x,y)=\\sin(\\pi x)\\sin(\\pi y)$. Initialize with $u=0$ in the interior.\n\nStarting from the fundamental base consisting of (i) the definition of the elliptic operator as a divergence form diffusion operator, (ii) the finite difference approximation of second derivatives, and (iii) the Gauss–Seidel iteration principle, derive two line-relaxation schemes:\n\n- $x$-line relaxation (solve tridiagonal systems along fixed $y$): For each fixed $j$, solve the tridiagonal system in $i$ that treats the $x$-direction couplings implicitly and the $y$-direction couplings using the most recently available values, in lexicographic forward order in $j$.\n\n- $y$-line relaxation (solve tridiagonal systems along fixed $x$): For each fixed $i$, solve the tridiagonal system in $j$ that treats the $y$-direction couplings implicitly and the $x$-direction couplings using the most recently available values, in lexicographic forward order in $i$.\n\nFor each scheme, measure the residual $r = f - A u$ in the discrete $L^2$-norm \n$$\n\\|r\\|_2 \\;=\\; \\Big(h_x h_y \\sum_{i=1}^{n_x}\\sum_{j=1}^{n_y} r_{i,j}^2\\Big)^{1/2}.\n$$\nDefine the empirical average per-sweep convergence factor over $N$ line-relaxation sweeps by the geometric mean\n$$\n\\rho \\;=\\; \\Bigg(\\frac{\\|r^{(N)}\\|_2}{\\|r^{(0)}\\|_2}\\Bigg)^{1/N},\n$$\nwhere $\\|r^{(k)}\\|_2$ is the residual norm after $k$ complete line-relaxation sweeps. Compute $\\rho_x$ for the $x$-line relaxation and $\\rho_y$ for the $y$-line relaxation, then report the ratio \n$$\n\\gamma \\;=\\; \\frac{\\rho_y}{\\rho_x}.\n$$\nA value $\\gamma > 1$ indicates that $x$-line relaxation reduces the residual faster per sweep than $y$-line relaxation.\n\nYour program must implement both line-relaxation schemes with exact tridiagonal solves along lines (Thomas algorithm), apply them independently from the same initial guess $u=0$, and compute $\\gamma$ as specified. There are no physical units in this problem. Angles in the source term use radians due to the standard mathematical definition of $\\sin$.\n\nTest Suite:\n- Case $1$: $n_x=64$, $n_y=64$, $r=10^3$, $N=8$.\n- Case $2$: $n_x=64$, $n_y=64$, $r=10$, $N=8$.\n- Case $3$: $n_x=64$, $n_y=64$, $r=1$, $N=8$.\n\nFor each case, compute the single float $\\gamma$ and round it to $6$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., [result1,result2,result3]). In this problem, the output must be the list $[\\gamma_1,\\gamma_2,\\gamma_3]$ corresponding to the three test cases in the order listed above, each rounded to $6$ decimal places.", "solution": "The problem is valid. It is a well-posed numerical analysis problem grounded in the standard theory of iterative methods for elliptic partial differential equations. All parameters, conditions, and required outputs are specified unambiguously.\n\nThe problem requires the implementation and comparison of two line-relaxation schemes for solving the anisotropic diffusion equation. The core of the task lies in deriving the iterative update rules for each scheme from fundamental principles.\n\nThe governing partial differential equation (PDE) is:\n$$\n-\\partial_x\\big(\\alpha \\, \\partial_x u\\big) - \\partial_y\\big(\\beta \\, \\partial_y u\\big) = f\n$$\non the unit square $\\Omega=(0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions $u=0$ on $\\partial\\Omega$. The coefficients are constant, $\\alpha = r$ and $\\beta = 1$, with $r \\ge 1$. The source term is $f(x,y)=\\sin(\\pi x)\\sin(\\pi y)$.\n\nWe first discretize the domain and the PDE. Using a uniform grid with $n_x$ and $n_y$ interior points, the grid spacings are $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$. A second-order central difference approximation for the derivatives at an interior grid node $(i,j)$, where $1 \\le i \\le n_x$ and $1 \\le j \\le n_y$, yields the discrete equation:\n$$\n-\\frac{\\alpha}{h_x^2}\\big(u_{i+1,j}-2u_{i,j}+u_{i-1,j}\\big) - \\frac{\\beta}{h_y^2}\\big(u_{i,j+1}-2u_{i,j}+u_{i,j-1}\\big) = f_{i,j}\n$$\nThe boundary condition $u=0$ implies that for any boundary-adjacent node, the value of $u$ at the corresponding \"ghost\" point outside the domain is zero. For example, for $i=1$, $u_{0,j}=0$, and for $j=n_y$, $u_{i,n_y+1}=0$.\n\nLet's rearrange the discrete equation to isolate the term $u_{i,j}$ and identify the couplings. Let $C_x = \\alpha/h_x^2$ and $C_y = \\beta/h_y^2$. The equation becomes:\n$$\n(2C_x + 2C_y)u_{i,j} - C_x(u_{i-1,j} + u_{i+1,j}) - C_y(u_{i,j-1} + u_{i,j+1}) = f_{i,j}\n$$\nThis represents one equation in the large system of linear equations $Au=f$ for all the unknown values $u_{i,j}$ in the grid interior. Line relaxation is a particular strategy for structuring a Gauss-Seidel-type iteration to solve this system.\n\n**1. $x$-Line Relaxation Scheme**\n\nIn $x$-line relaxation, we iterate by sweeping through the grid line by line, in this case, along horizontal lines (constant $j$). The sweep proceeds in lexicographic forward order, from $j=1$ to $n_y$. For each line $j$, we solve for all the unknowns $u_{i,j}$ for $i=1, \\dots, n_x$ simultaneously. This means the couplings in the $x$-direction (to $u_{i-1,j}$ and $u_{i+1,j}$) are treated implicitly. The couplings in the $y$-direction (to $u_{i,j-1}$ and $u_{i,j+1}$) are treated explicitly, using the most recently updated values, which is the essence of the Gauss-Seidel method.\n\nLet $k$ be the iteration index for the full sweep. For a fixed line $j$, we are computing the vector of unknowns $u_{*,j}^{(k+1)}$. The values from the line below, $u_{*,j-1}$, have already been computed in the current sweep, so we use $u_{*,j-1}^{(k+1)}$. The values from the line above, $u_{*,j+1}$, are from the previous sweep, so we use $u_{*,j+1}^{(k)}$. The system of equations for the unknowns on line $j$ is:\n$$\n(2C_x + 2C_y)u_{i,j}^{(k+1)} - C_x u_{i-1,j}^{(k+1)} - C_x u_{i+1,j}^{(k+1)} = f_{i,j} + C_y u_{i,j-1}^{(k+1)} + C_y u_{i,j+1}^{(k)}\n$$\nfor $i=1, \\dots, n_x$. This is a tridiagonal system of $n_x$ equations for the $n_x$ unknowns $u_{1,j}^{(k+1)}, \\dots, u_{n_x,j}^{(k+1)}$. The system can be written as $T_x U_j^{(k+1)} = d_j$, where $U_j^{(k+1)}$ is the vector of unknowns on line $j$. The tridiagonal matrix $T_x$ has:\n- Diagonal elements: $2(C_x + C_y) = 2(\\alpha/h_x^2 + \\beta/h_y^2)$\n- Sub- and super-diagonal elements: $-C_x = -\\alpha/h_x^2$\n\nThe right-hand side vector $d_j$ has elements:\n$$\n(d_j)_i = f_{i,j} + C_y\\left(u_{i,j-1}^{(k+1)} + u_{i,j+1}^{(k)}\\right)\n$$\nThe boundary conditions $u_{0,j}=0$ and $u_{n_x+1,j}=0$ are incorporated implicitly. This tridiagonal system is solved for each $j$ from $1$ to $n_y$ using an exact solver like the Thomas algorithm.\n\n**2. $y$-Line Relaxation Scheme**\n\nThe derivation is symmetric to the $x$-line case. We sweep through the grid along vertical lines (constant $i$) from $i=1$ to $n_x$. For each line $i$, we solve for the unknowns $u_{i,j}$ for $j=1, \\dots, n_y$ implicitly. Couplings in the $x$-direction are treated explicitly using the most recent values based on the Gauss-Seidel principle.\n\nFor a fixed line $i$, the system for unknowns $u_{i,*}^{(k+1)}$ is:\n$$\n(2C_x + 2C_y)u_{i,j}^{(k+1)} - C_y u_{i,j-1}^{(k+1)} - C_y u_{i,j+1}^{(k+1)} = f_{i,j} + C_x u_{i-1,j}^{(k+1)} + C_x u_{i+1,j}^{(k)}\n$$\nfor $j=1, \\dots, n_y$. This constitutes a tridiagonal system $T_y U_i^{(k+1)} = d_i$ for the vector of unknowns $U_i^{(k+1)}$ on line $i$. The matrix $T_y$ has:\n- Diagonal elements: $2(C_x + C_y) = 2(\\alpha/h_x^2 + \\beta/h_y^2)$\n- Sub- and super-diagonal elements: $-C_y = -\\beta/h_y^2$\n\nThe right-hand side vector $d_i$ has elements:\n$$\n(d_i)_j = f_{i,j} + C_x\\left(u_{i-1,j}^{(k+1)} + u_{i+1,j}^{(k)}\\right)\n$$\nThis system is solved for each $i$ from $1$ to $n_x$.\n\n**3. Performance Analysis**\n\nThe problem features anisotropy controlled by $r = \\alpha/\\beta$. When $r \\gg 1$, the coupling strength in the $x$-direction, proportional to $C_x = r/h_x^2$, is much stronger than in the $y$-direction, proportional to $C_y = 1/h_y^2$. Line relaxation methods are most effective when they treat the strongest couplings implicitly. Therefore, for $r \\gg 1$, $x$-line relaxation, which solves implicitly along the direction of strong coupling, is expected to converge much faster than $y$-line relaxation. As $r \\to 1$, the problem becomes isotropic, and the performance of both methods should become identical.\n\nThe convergence rate is quantified by the empirical average convergence factor $\\rho$, defined as:\n$$\n\\rho = \\left(\\frac{\\|r^{(N)}\\|_2}{\\|r^{(0)}\\|_2}\\right)^{1/N}\n$$\nHere, $r^{(k)} = f - Au^{(k)}$ is the residual after $k$ sweeps, and the norm is the discrete $L^2$-norm:\n$$\n\\|r\\|_2 = \\left(h_x h_y \\sum_{i=1}^{n_x}\\sum_{j=1}^{n_y} r_{i,j}^2\\right)^{1/2}\n$$\nA smaller value of $\\rho$ indicates faster convergence. We compute $\\rho_x$ for $x$-line relaxation and $\\rho_y$ for $y$-line relaxation, and then their ratio $\\gamma = \\rho_y / \\rho_x$. Based on the reasoning above, we expect $\\gamma > 1$ for $r > 1$ and $\\gamma \\approx 1$ for $r=1$.\n\nThe algorithm consists of initializing the solution $u^{(0)}=0$, calculating the initial residual norm $\\|r^{(0)}\\|_2$, and then running each relaxation scheme for $N$ iterations to find $u^{(N)}$. The final residual norm $\\|r^{(N)}\\|_2$ is then used to compute $\\rho$ and subsequently $\\gamma$. The tridiagonal systems within each sweep are solved using the `solve_banded` function from `scipy.linalg`, which is an efficient and numerically stable implementation equivalent to the Thomas algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef calculate_residual_norm(u_padded, f_interior, alpha, beta, hx, hy):\n    \"\"\"\n    Calculates the discrete L2-norm of the residual r = f - A*u.\n    \n    Args:\n        u_padded (np.ndarray): Solution array of size (nx+2, ny+2) with boundaries.\n        f_interior (np.ndarray): Source term array of size (nx, ny).\n        alpha, beta (float): Diffusion coefficients.\n        hx, hy (float): Grid spacings.\n\n    Returns:\n        float: The discrete L2-norm of the residual.\n    \"\"\"\n    nx, ny = f_interior.shape\n    u_int = u_padded[1:-1, 1:-1]\n    \n    # Central difference approximation of the operator A*u using slicing on the padded array\n    u_ip1 = u_padded[2:, 1:-1]\n    u_im1 = u_padded[:-2, 1:-1]\n    u_jp1 = u_padded[1:-1, 2:]\n    u_jm1 = u_padded[1:-1, :-2]\n\n    # Note the negative sign in the PDE definition\n    Au = -alpha * (u_ip1 - 2 * u_int + u_im1) / hx**2 \\\n         -beta  * (u_jp1 - 2 * u_int + u_jm1) / hy**2\n    \n    r_interior = f_interior - Au\n    \n    # Discrete L2 norm: sqrt(hx*hy * sum(r_ij^2))\n    norm = np.linalg.norm(r_interior) * np.sqrt(hx * hy)\n    return norm\n\n\ndef solve_x_line(nx, ny, hx, hy, alpha, beta, f_interior, N):\n    \"\"\"\n    Solves the system using x-line relaxation for N sweeps.\n    \"\"\"\n    u = np.zeros((nx + 2, ny + 2))\n    \n    Cx = alpha / hx**2\n    Cy = beta / hy**2\n    \n    # Pre-assemble the constant tridiagonal matrix for all x-lines\n    diag_x = np.full(nx, 2 * Cx + 2 * Cy)\n    off_diag_x = np.full(nx - 1, -Cx)\n    A_x_banded = np.zeros((3, nx))\n    A_x_banded[0, 1:] = off_diag_x\n    A_x_banded[1, :] = diag_x\n    A_x_banded[2, :-1] = off_diag_x\n    \n    for _ in range(N):\n        u_prev_sweep = u.copy()\n        # Sweep along y-direction (fixed j), solving for lines in x\n        for j_pad in range(1, ny + 1):\n            j_py = j_pad - 1\n            # RHS: f_ij + C_y * (u_{i,j-1}^{(k+1)} + u_{i,j+1}^{(k)})\n            # u values from line j-1 are from the current sweep (u).\n            # u values from line j+1 are from the previous sweep (u_prev_sweep).\n            rhs = f_interior[:, j_py] + Cy * (u[1:-1, j_pad - 1] + u_prev_sweep[1:-1, j_pad + 1])\n            \n            line_solution = solve_banded((1, 1), A_x_banded, rhs)\n            u[1:-1, j_pad] = line_solution\n            \n    return u\n\n\ndef solve_y_line(nx, ny, hx, hy, alpha, beta, f_interior, N):\n    \"\"\"\n    Solves the system using y-line relaxation for N sweeps.\n    \"\"\"\n    u = np.zeros((nx + 2, ny + 2))\n    \n    Cx = alpha / hx**2\n    Cy = beta / hy**2\n    \n    # Pre-assemble the constant tridiagonal matrix for all y-lines\n    diag_y = np.full(ny, 2 * Cx + 2 * Cy)\n    off_diag_y = np.full(ny-1, -Cy)\n    A_y_banded = np.zeros((3, ny))\n    A_y_banded[0, 1:] = off_diag_y\n    A_y_banded[1, :] = diag_y\n    A_y_banded[2, :-1] = off_diag_y\n\n    for _ in range(N):\n        u_prev_sweep = u.copy()\n        # Sweep along x-direction (fixed i), solving for lines in y\n        for i_pad in range(1, nx + 1):\n            i_py = i_pad - 1\n            # RHS: f_ij + C_x * (u_{i-1,j}^{(k+1)} + u_{i+1,j}^{(k)})\n            # u values from line i-1 are from the current sweep (u).\n            # u values from line i+1 are from the previous sweep (u_prev_sweep).\n            rhs = f_interior[i_py, :] + Cx * (u[i_pad - 1, 1:-1] + u_prev_sweep[i_pad + 1, 1:-1])\n\n            line_solution = solve_banded((1, 1), A_y_banded, rhs)\n            u[i_pad, 1:-1] = line_solution\n            \n    return u\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (64, 64, 1000.0, 8),\n        (64, 64, 10.0, 8),\n        (64, 64, 1.0, 8),\n    ]\n\n    results = []\n    for case in test_cases:\n        nx, ny, r, N = case\n        \n        # Setup grid and problem parameters\n        alpha = r\n        beta = 1.0\n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        \n        # Create grid coordinates for interior points\n        x_coords = np.linspace(hx, 1.0 - hx, nx)\n        y_coords = np.linspace(hy, 1.0 - hy, ny)\n        X, Y = np.meshgrid(x_coords, y_coords, indexing='ij')\n        \n        # Source term f(x,y)\n        f_interior = np.sin(np.pi * X) * np.sin(np.pi * Y)\n        \n        # Initial condition u=0 and initial residual\n        u0_padded = np.zeros((nx + 2, ny + 2))\n        r0_norm = calculate_residual_norm(u0_padded, f_interior, alpha, beta, hx, hy)\n\n        # X-line relaxation\n        uN_x = solve_x_line(nx, ny, hx, hy, alpha, beta, f_interior, N)\n        rN_x_norm = calculate_residual_norm(uN_x, f_interior, alpha, beta, hx, hy)\n        rho_x = (rN_x_norm / r0_norm)**(1.0 / N)\n        \n        # Y-line relaxation\n        uN_y = solve_y_line(nx, ny, hx, hy, alpha, beta, f_interior, N)\n        rN_y_norm = calculate_residual_norm(uN_y, f_interior, alpha, beta, hx, hy)\n        rho_y = (rN_y_norm / r0_norm)**(1.0 / N)\n        \n        # Compute the ratio gamma\n        gamma = rho_y / rho_x\n        results.append(round(gamma, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3148150"}, {"introduction": "Beyond stationary methods with fixed parameters, a more powerful class of techniques uses a sequence of carefully chosen parameters to accelerate convergence. This practice introduces Chebyshev semi-iteration, a method rooted in the powerful minimax property of Chebyshev polynomials. You will derive the optimal sequence of step sizes based on estimates of the matrix's eigenvalues and implement a matrix-free solver to apply the method [@problem_id:3148184]. This exercise provides a fantastic bridge from simpler iterative schemes to the powerful concepts of polynomial acceleration that underpin advanced Krylov subspace methods like the Conjugate Gradient algorithm.", "problem": "You are asked to design and implement a complete numerical experiment that applies Chebyshev semi-iteration to a symmetric positive-definite (SPD) linear system arising from a standard finite-difference discretization of an elliptic partial differential equation (PDE). The SPD system is $A u = b$ obtained by discretizing the Poisson equation $-\\nabla^2 u = f$ with homogeneous Dirichlet boundary conditions on the unit square $[0,1]\\times[0,1]$. Your task is to derive, implement, and evaluate a Chebyshev semi-iteration method that uses estimated extremal eigenvalue bounds of $A$ to select step sizes. The derivation must begin from the fundamental finite-difference approximation of the Laplace operator and the minimax property of Chebyshev polynomials, and should not assume any iteration formulas a priori.\n\nBackground and setup:\n- Discretize $-\\nabla^2 u = f$ on a uniform grid with $N$ interior points per coordinate direction, grid spacing $h = 1/(N+1)$, and homogeneous Dirichlet boundary data $u=0$ on the boundary. Use the standard $5$-point stencil to approximate $-\\nabla^2$: at an interior grid node $(i,j)$,\n  $$\n  \\left(-\\nabla^2 u\\right)_{i,j} \\approx \\frac{4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}}{h^2}.\n  $$\n  Assemble the associated SPD matrix $A$ implicitly via a matrix-free operator and form $b$ by sampling $f$ at the interior grid points. Use $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, so that the corresponding smooth analytical solution of the continuous problem is $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$.\n\n- Implement Chebyshev semi-iteration as a stationary iteration of the form $x^{(k+1)} = x^{(k)} + \\omega_k r^{(k)}$ with $r^{(k)} = b - A x^{(k)}$, where the step sizes $\\omega_k$ are chosen using the extremal eigenvalue bounds of $A$. You must:\n  1. Start from the definition of Chebyshev polynomials and their minimax property on $[-1,1]$.\n  2. Map the spectral interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ to $[-1,1]$ and derive the sequence of step sizes $\\{\\omega_k\\}_{k=1}^m$ that minimizes the maximum residual amplification over the interval after a prescribed polynomial degree $m$.\n  3. Implement the $m$-step Chebyshev semi-iteration that uses these $\\omega_k$, without forming $A$ explicitly.\n\n- Spectral bounds: For the $5$-point Laplacian with homogeneous Dirichlet boundary on an $N \\times N$ interior grid, it is a well-tested fact that the eigenvalues of $A$ lie in an interval $[\\lambda_{\\min}^{\\mathrm{ex}}, \\lambda_{\\max}^{\\mathrm{ex}}]$ that can be computed from the one-dimensional eigenvalues. Specifically, for $h = 1/(N+1)$ and indices $p,q \\in \\{1,2,\\dots,N\\}$,\n  $$\n  \\lambda_{p,q} = \\frac{4}{h^2}\\sin^2\\!\\left(\\frac{\\pi p}{2(N+1)}\\right) + \\frac{4}{h^2}\\sin^2\\!\\left(\\frac{\\pi q}{2(N+1)}\\right),\n  $$\n  and hence $\\lambda_{\\min}^{\\mathrm{ex}}$ and $\\lambda_{\\max}^{\\mathrm{ex}}$ occur at $(p,q)=(1,1)$ and $(p,q)=(N,N)$, respectively. In this assignment, you must use estimates\n  $$\n  \\lambda_{\\min} = s_{\\ell}\\,\\lambda_{\\min}^{\\mathrm{ex}}, \\quad \\lambda_{\\max} = s_{u}\\,\\lambda_{\\max}^{\\mathrm{ex}},\n  $$\n  where $s_{\\ell}>0$ and $s_u>0$ are given scale factors per test case to model imperfect prior knowledge. The quality of these bounds will influence the effectiveness of the iteration. All trigonometric computations must use angles in radians.\n\n- Your program must:\n  1. Construct $b$ by sampling $f$ at interior grid points $(x_i,y_j)=(ih,jh)$ for $i,j\\in\\{1,\\dots,N\\}$.\n  2. Initialize with $x^{(0)}=0$.\n  3. Run exactly $m$ steps of Chebyshev semi-iteration with the step sizes derived from your analysis for the interval $[\\lambda_{\\min},\\lambda_{\\max}]$.\n  4. Return the relative residual norm $\\|r^{(m)}\\|_2/\\|b\\|_2$ as a floating-point number for each test case, where $r^{(m)}=b-Ax^{(m)}$.\n  5. Do not assemble $A$ explicitly; apply $A$ to vectors via the $5$-point stencil.\n\nTest suite:\n- Use the following four test cases, each specified by $(N, m, s_{\\ell}, s_u)$:\n  1. $(N, m, s_{\\ell}, s_u) = (\\,31,\\,10,\\,1.0,\\,1.0\\,)$.\n  2. $(N, m, s_{\\ell}, s_u) = (\\,31,\\,10,\\,0.8,\\,1.2\\,)$.\n  3. $(N, m, s_{\\ell}, s_u) = (\\,63,\\,20,\\,1.0,\\,1.0\\,)$.\n  4. $(N, m, s_{\\ell}, s_u) = (\\,7,\\,4,\\,0.6,\\,1.4\\,)$.\n\nQuantifiable outputs:\n- For each test case, compute the scalar value $q = \\|r^{(m)}\\|_2/\\|b\\|_2$ and round it to $8$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases shown above, for example, [q_1,q_2,q_3,q_4].\n\nConstraints and notes:\n- The implementation must be matrix-free and must not form dense matrices. Use the $5$-point stencil operator to apply $A$.\n- Ensure numerical stability by using the step sizes obtained from your Chebyshev derivation.\n- All computations must be in non-dimensional units, and all angles must be in radians.", "solution": "The task is to design, derive, and implement a Chebyshev semi-iteration method to solve a symmetric positive-definite (SPD) linear system $Ax=b$. This system arises from the finite-difference discretization of the Poisson equation on a unit square. The derivation must start from fundamental principles and the implementation must be matrix-free.\n\n### 1. The Discretized Problem\n\nThe Poisson equation $-\\nabla^2 u = f$ is discretized on a uniform grid on the domain $[0,1]\\times[0,1]$ with homogeneous Dirichlet boundary conditions ($u=0$ on the boundary). The grid has $N$ interior points in each coordinate direction, resulting in a grid spacing of $h = 1/(N+1)$. The standard $5$-point finite-difference stencil for the negative Laplacian at an interior grid node $(i,j)$ is\n$$\n(-\\nabla^2 u)_{i,j} \\approx \\frac{4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}}{h^2}\n$$\nThis discretization leads to a sparse, SPD linear system of equations $Ax=b$ of size $N^2 \\times N^2$, where $x$ is a vector representing the values of $u$ at the $N^2$ interior grid points. The matrix $A$ is the discrete Laplacian operator, and the vector $b$ is formed by evaluating the source function $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$ at the interior grid points. We are required to apply the operator $A$ to a vector without explicitly assembling the matrix $A$.\n\n### 2. Chebyshev Semi-Iteration\n\nThe problem specifies an iterative method of the form:\n$$\nx^{(k+1)} = x^{(k)} + \\omega_k r^{(k)}, \\quad \\text{with } r^{(k)} = b - Ax^{(k)}\n$$\nThis is a non-stationary Richardson iteration. Let us analyze the propagation of the residual.\n$$\nr^{(k+1)} = b - Ax^{(k+1)} = b - A(x^{(k)} + \\omega_k r^{(k)}) = (b - Ax^{(k)}) - \\omega_k A r^{(k)} = r^{(k)} - \\omega_k A r^{(k)} = (I - \\omega_k A)r^{(k)}\n$$\nAfter $m$ steps, starting with an initial guess $x^{(0)}$ and its residual $r^{(0)} = b - Ax^{(0)}$, the residual becomes:\n$$\nr^{(m)} = \\left( \\prod_{k=0}^{m-1} (I - \\omega_k A) \\right) r^{(0)}\n$$\nLet $R_m(\\lambda) = \\prod_{k=0}^{m-1} (1-\\omega_k \\lambda)$. This is a polynomial of degree $m$ in $\\lambda$. We can write the residual relation as $r^{(m)} = R_m(A)r^{(0)}$. A key property of this polynomial is $R_m(0) = \\prod_{k=0}^{m-1} (1-0)=1$.\n\nOur goal is to minimize the error after $m$ steps. Since $A$ is SPD, its eigenvectors form a basis. Minimizing the norm of the residual $r^{(m)}$ is achieved by finding a polynomial $R_m(\\lambda)$ that is minimal in magnitude over the spectrum of $A$, subject to the constraint $R_m(0)=1$. The spectrum of $A$, denoted $\\sigma(A)$, lies within the interval $[\\lambda_{\\min}^{\\mathrm{ex}}, \\lambda_{\\max}^{\\mathrm{ex}}]$. We are given estimates for these bounds, $[\\lambda_{\\min}, \\lambda_{\\max}]$. The problem thus becomes a constrained minimax problem:\n$$\n\\min_{R_m} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |R_m(\\lambda)| \\quad \\text{subject to } R_m(0)=1 \\text{ and } \\deg(R_m) \\le m\n$$\n\n### 3. Derivation of Optimal Parameters\n\nThis minimax problem is solved by Chebyshev polynomials. The Chebyshev polynomial of the first kind, $T_m(z)$, is the unique polynomial of degree $m$ that minimizes the maximum absolute value on $[-1,1]$ among all monic polynomials of that degree. A related property is that for any point $z_0 \\notin [-1,1]$, the polynomial $P_m(z)$ of degree $m$ satisfying $P_m(z_0)=1$ which minimizes $\\max_{z \\in [-1,1]} |P_m(z)|$ is given by $P_m(z) = T_m(z)/T_m(z_0)$.\n\nWe first map the spectral interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ to the interval $[-1,1]$ using a linear transformation:\n$$\nz(\\lambda) = \\frac{2\\lambda - (\\lambda_{\\max}+\\lambda_{\\min})}{\\lambda_{\\max}-\\lambda_{\\min}}\n$$\nOur constraint is $R_m(0)=1$. We find the point corresponding to $\\lambda=0$ in the $z$-domain:\n$$\nz_0 = z(0) = -\\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}}\n$$\nSince $A$ is SPD, $0 < \\lambda_{\\min} \\le \\lambda_{\\max}$, which implies that $z_0 < -1$.\nThe optimal residual polynomial $R_m(\\lambda)$ is then given by:\n$$\nR_m(\\lambda) = \\frac{T_m(z(\\lambda))}{T_m(z_0)} = \\frac{T_m\\left(\\frac{2\\lambda - \\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)}{T_m\\left(-\\frac{\\lambda_{\\max}+\\lambda_{\\min}}{\\lambda_{\\max}-\\lambda_{\\min}}\\right)}\n$$\nThe iteration parameters $\\omega_k$ are determined by the roots of $R_m(\\lambda)$. The roots of $R_m(\\lambda)$ are the values of $\\lambda$ for which $T_m(z(\\lambda))=0$. The roots of $T_m(z)$ are given by:\n$$\n\\hat{z}_k = \\cos\\left(\\frac{(2k-1)\\pi}{2m}\\right), \\quad k=1, 2, \\dots, m\n$$\nWe find the corresponding roots $\\lambda_k^*$ of $R_m(\\lambda)$ by inverting the linear map $z(\\lambda)$:\n$$\n\\lambda_k^* = \\frac{\\lambda_{\\max}+\\lambda_{\\min}}{2} + \\frac{\\lambda_{\\max}-\\lambda_{\\min}}{2} \\hat{z}_k = \\frac{\\lambda_{\\max}+\\lambda_{\\min}}{2} + \\frac{\\lambda_{\\max}-\\lambda_{\\min}}{2}\\cos\\left(\\frac{(2k-1)\\pi}{2m}\\right)\n$$\nThe residual polynomial is $R_m(\\lambda) = \\prod (1-\\omega_k \\lambda)$, so its roots are $1/\\omega_k$. Therefore, the optimal step sizes are the reciprocals of the roots $\\lambda_k^*$:\n$$\n\\omega_k = \\frac{1}{\\lambda_k^*} = \\left( \\frac{\\lambda_{\\max}+\\lambda_{\\min}}{2} + \\frac{\\lambda_{\\max}-\\lambda_{\\min}}{2}\\cos\\left(\\frac{(2k-1)\\pi}{2m}\\right) \\right)^{-1}, \\quad k=1, 2, \\dots, m\n$$\nThe product in $R_m(A)$ is commutative, so the order in which these $m$ step sizes are used does not affect the final result after $m$ iterations.\n\n### 4. Algorithm Summary\n\nFor each test case $(N, m, s_{\\ell}, s_u)$:\n1.  Compute grid spacing $h=1/(N+1)$.\n2.  Construct the right-hand side vector $b$ of size $N^2$ by sampling $f(x,y)$ at the $N^2$ interior grid points.\n3.  Calculate the exact eigenvalue bounds of the discrete operator $A$:\n    $$\n    \\lambda_{\\min}^{\\mathrm{ex}} = \\frac{8}{h^2}\\sin^2\\left(\\frac{\\pi}{2(N+1)}\\right), \\quad \\lambda_{\\max}^{\\mathrm{ex}} = \\frac{8}{h^2}\\cos^2\\left(\\frac{\\pi}{2(N+1)}\\right)\n    $$\n4.  Apply the scale factors to get the estimated bounds for the iteration: $\\lambda_{\\min} = s_{\\ell} \\lambda_{\\min}^{\\mathrm{ex}}$ and $\\lambda_{\\max} = s_{u} \\lambda_{\\max}^{\\mathrm{ex}}$.\n5.  Compute the $m$ Chebyshev step sizes $\\{\\omega_k\\}_{k=1}^m$ using the formula derived above.\n6.  Initialize the solution vector $x^{(0)} = 0$.\n7.  Iterate for $k$ from $0$ to $m-1$:\n    a. Compute the residual $r^{(k)} = b - Ax^{(k)}$ using a matrix-free function for the action of $A$.\n    b. Update the solution $x^{(k+1)} = x^{(k)} + \\omega_{k+1} r^{(k)}$.\n8.  After $m$ steps, compute the final residual $r^{(m)} = b - Ax^{(m)}$.\n9.  The final output is the relative residual norm $q = \\|r^{(m)}\\|_2 / \\|b\\|_2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of applying Chebyshev semi-iteration to the discretized \n    Poisson equation for a set of predefined test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (31, 10, 1.0, 1.0),\n        (31, 10, 0.8, 1.2),\n        (63, 20, 1.0, 1.0),\n        (7, 4, 0.6, 1.4),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, m, s_ell, s_u = case\n        \n        # 1. Setup\n        h = 1.0 / (N + 1)\n        \n        # Grid points for evaluating the source function f(x,y)\n        grid_pts = np.linspace(h, 1.0 - h, N)\n        X, Y = np.meshgrid(grid_pts, grid_pts)\n        \n        # Construct the right-hand side vector b by sampling f\n        f_vals = 2 * np.pi**2 * np.sin(np.pi * X) * np.sin(np.pi * Y)\n        b = f_vals.flatten()\n        b_norm = np.linalg.norm(b)\n\n        # 2. Spectral bounds calculation\n        # Exact extremal eigenvalues of the 5-point discrete Laplacian\n        lambda_min_ex = (8.0 / h**2) * (np.sin(np.pi / (2.0 * (N + 1))))**2\n        lambda_max_ex = (8.0 / h**2) * (np.cos(np.pi / (2.0 * (N + 1))))**2\n        \n        # Estimated bounds for Chebyshev iteration\n        lambda_min = s_ell * lambda_min_ex\n        lambda_max = s_u * lambda_max_ex\n\n        # 3. Chebyshev step sizes (omegas) derivation\n        omegas = np.zeros(m)\n        center = (lambda_max + lambda_min) / 2.0\n        radius = (lambda_max - lambda_min) / 2.0\n        \n        for k in range(1, m + 1):\n            chebyshev_root = np.cos((2.0 * k - 1.0) * np.pi / (2.0 * m))\n            polynomial_root = center + radius * chebyshev_root\n            omegas[k-1] = 1.0 / polynomial_root\n\n        # 4. Matrix-free operator A (discrete Laplacian)\n        def apply_A(x_vec):\n            \"\"\"Applies the 5-point stencil operator A to a vector x_vec.\"\"\"\n            x_grid = x_vec.reshape((N, N))\n            \n            # Start with 4*u_ij\n            Ax_grid = 4.0 * x_grid\n            \n            # Subtract neighbors (u_i-1,j, u_i+1,j, u_i,j-1, u_i,j+1)\n            # These slicing operations correctly handle boundary conditions,\n            # as missing neighbors are implicitly zero (not subtracted).\n            if N > 1:\n                Ax_grid[1:, :] -= x_grid[:-1, :]  # u_i-1,j\n                Ax_grid[:-1, :] -= x_grid[1:, :]  # u_i+1,j\n                Ax_grid[:, 1:] -= x_grid[:, :-1]  # u_i,j-1\n                Ax_grid[:, :-1] -= x_grid[:, 1:]  # u_i,j+1\n\n            return Ax_grid.flatten() / h**2\n\n        # 5. Chebyshev Iteration\n        # Initialize solution vector x^(0) = 0\n        x = np.zeros(N * N)\n        \n        # Run m steps of the iteration: x^(k+1) = x^(k) + omega_k * r^(k)\n        for k in range(m):\n            residual = b - apply_A(x)\n            x = x + omegas[k] * residual\n\n        # 6. Final Evaluation\n        # Compute the final residual r^(m) = b - A*x^(m)\n        final_residual = b - apply_A(x)\n        \n        # Compute the relative residual norm and round to 8 decimal places\n        q = np.linalg.norm(final_residual) / b_norm\n        results.append(round(q, 8))\n\n    # Final print statement in the exact required format.\n    # The format specifier in f-string handles cases like '0.1' instead of '0.10000000'\n    # so we need to format it properly. Using map(str,...) is fine since round() already fixed precision.\n    formatted_results = [f\"{r:.8f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3148184"}]}