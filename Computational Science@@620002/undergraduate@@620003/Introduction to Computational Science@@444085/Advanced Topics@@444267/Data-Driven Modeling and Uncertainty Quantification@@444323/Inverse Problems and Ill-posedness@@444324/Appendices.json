{"hands_on_practices": [{"introduction": "A core feature of many ill-posed problems is instability: small changes or noise in the measurements can lead to enormous changes in the solution. This exercise provides a hands-on demonstration of this fundamental instability using the Singular Value Decomposition (SVD). By constructing a matrix $A$ with known, rapidly decaying singular values, you will see exactly how a small, controlled perturbation in the data space $b$ gets amplified in the solution space $x$, providing a concrete understanding of ill-conditioning and its relationship to the matrix spectrum [@problem_id:3147053].", "problem": "You will investigate instability in linear inverse problems via singular value amplification, starting from first principles of linear algebra. Use the following fundamental base: the Singular Value Decomposition (SVD) and the Moore–Penrose pseudoinverse (defined precisely below), Euclidean norm properties, and linearity of matrix–vector multiplication.\n\nTask. Construct a family of matrices with rapidly decaying singular values, define controlled perturbations in the data space, and verify the predicted amplification in the solution space. Your program must implement the specified construction and checks, and output a single line aggregating boolean results for a fixed test suite.\n\nDefinitions to use as the base. For any real matrix $A \\in \\mathbb{R}^{m \\times n}$, there exist orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$, and a diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ with nonnegative diagonal entries ordered as $ \\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$, such that $A = U \\Sigma V^{\\mathsf{T}}$. The columns of $U$ are the left singular vectors $\\{u_i\\}_{i=1}^m$, and the columns of $V$ are the right singular vectors $\\{v_i\\}_{i=1}^n$. The Moore–Penrose pseudoinverse $A^{+}$ is defined by $A^{+} = V \\Sigma^{+} U^{\\mathsf{T}}$, where $\\Sigma^{+}$ is formed by replacing each nonzero diagonal entry $\\sigma_i$ with $1/\\sigma_i$ and transposing the shape.\n\nProblem requirements. For each specified test case:\n- Construct $U$ and $V$ as orthogonal matrices and choose singular values $\\{\\sigma_k\\}_{k=1}^n$ that decay geometrically according to $\\sigma_k = \\alpha^{k-1}$ for a given scalar $\\alpha$ with $0  \\alpha  1$. Form $A = U \\Sigma V^{\\mathsf{T}}$.\n- Define a perturbation direction in data space as the left singular vector $u_i$ for a specified index $i \\in \\{1,\\dots,n\\}$. Let the right-hand side be $b = \\epsilon \\, u_i$ with a specified perturbation magnitude $\\epsilon > 0$.\n- Compute the minimum-norm least-squares solution $x = A^{+} b$.\n- Compute the amplification factor $r = \\|x\\|_2 / \\epsilon$ and compare it, using your derivation from the stated base, to the theoretical value that follows from the SVD structure. Use a relative tolerance $\\mathrm{rtol} = 10^{-6}$ and an absolute tolerance $\\mathrm{atol} = 10^{-8}$ in the comparison, i.e., decide “pass” if $|r - r_{\\mathrm{theory}}| \\le \\max(\\mathrm{atol}, \\mathrm{rtol} \\cdot |r_{\\mathrm{theory}}|)$, otherwise “fail”. No physical units are involved.\n\nMatrix construction details. Use any reproducible method to generate orthogonal $U$ and $V$ (for example, by orthonormalizing random Gaussian matrices). Ensure $\\Sigma$ has diagonal entries $\\sigma_k = \\alpha^{k-1}$ for $k = 1,\\dots,n$ and zeros elsewhere, so that $A$ has rapidly decaying singular values.\n\nTest suite. Implement exactly the following five test cases, each given as a tuple $(m,n,\\alpha,i,\\epsilon)$:\n- $(\\,6,\\,6,\\,0.2,\\,1,\\,10^{-6}\\,)$\n- $(\\,6,\\,6,\\,0.2,\\,6,\\,10^{-6}\\,)$\n- $(\\,10,\\,6,\\,0.3,\\,3,\\,10^{-5}\\,)$\n- $(\\,10,\\,6,\\,0.3,\\,6,\\,10^{-8}\\,)$\n- $(\\,8,\\,5,\\,0.5,\\,5,\\,10^{-4}\\,)$\n\nWhat to output. For each test case, output a boolean indicating whether the numerically observed amplification $r$ matches the theoretical prediction derived from the SVD to within the specified tolerances. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[True,False,True,True,False]”).\n\nConstraints and notes.\n- Angles are not involved.\n- All computations are purely numerical linear algebra in real arithmetic.\n- The program must be fully deterministic and must not require any external input.", "solution": "The problem is valid as it is scientifically grounded in linear algebra, well-posed with a clear and objective set of instructions, and its components are fully specified. The task is to verify numerically a theoretical result concerning the instability of linear inverse problems, which is a standard topic in computational science.\n\nThe core of the problem lies in understanding the solution to the linear system $Ax = b$ when the matrix $A$ is ill-conditioned. The minimum-norm least-squares solution is given by $x = A^{+}b$, where $A^{+}$ is the Moore-Penrose pseudoinverse. We will first derive the theoretical amplification factor and then implement a numerical verification.\n\n### Theoretical Derivation\n\nWe are given the Singular Value Decomposition (SVD) of a matrix $A \\in \\mathbb{R}^{m \\times n}$ as $A = U\\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$ on its main diagonal. The columns of $U$ are the left singular vectors $\\{u_i\\}$ and the columns of $V$ are the right singular vectors $\\{v_i\\}$.\n\nThe Moore-Penrose pseudoinverse is defined as $A^{+} = V\\Sigma^{+}U^{\\mathsf{T}}$, where $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ is obtained by taking the reciprocal of the non-zero entries of $\\Sigma$ and then transposing the matrix. For this problem, the singular values are $\\sigma_k = \\alpha^{k-1}$ for $k=1, \\dots, n$, with $0  \\alpha  1$. Since $\\alpha \\neq 0$, all $\\sigma_k$ are non-zero. The diagonal entries of $\\Sigma^{+}$ are therefore $1/\\sigma_k$.\n\nThe right-hand side vector $b$ is defined as a perturbation of magnitude $\\epsilon$ along the direction of the $i$-th left singular vector, $u_i$:\n$$b = \\epsilon u_i$$\nwhere $u_i$ is the $i$-th column of $U$.\n\nThe solution $x$ is then computed as:\n$$x = A^{+}b = (V\\Sigma^{+}U^{\\mathsf{T}})(\\epsilon u_i)$$\nUsing linearity, we can pull out the scalar $\\epsilon$:\n$$x = \\epsilon (V\\Sigma^{+}U^{\\mathsf{T}}u_i)$$\nThe term $U^{\\mathsf{T}}u_i$ is the product of the transpose of $U$ with its own $i$-th column. Since $U$ is an orthogonal matrix, its columns form an orthonormal basis. The product $u_j^{\\mathsf{T}}u_i = \\delta_{ji}$ (the Kronecker delta). Therefore, $U^{\\mathsf{T}}u_i$ results in a vector with $1$ in the $i$-th position and $0$ elsewhere. This is the $i$-th standard basis vector in $\\mathbb{R}^m$, which we denote as $e_i$. Note that the problem indices are 1-based.\n$$x = \\epsilon (V\\Sigma^{+}e_i)$$\nNext, we evaluate $\\Sigma^{+}e_i$. The matrix $\\Sigma^{+}$ is an $n \\times m$ matrix with values $1/\\sigma_k$ on its diagonal up to the rank of A (which is $n$). Multiplying $\\Sigma^{+}$ by $e_i$ selects the $i$-th column of $\\Sigma^{+}$. This column has only one non-zero entry, which is $1/\\sigma_i$ at the $i$-th position. This product results in the vector $(1/\\sigma_i)e_i'$, where $e_i'$ is the $i$-th standard basis vector in $\\mathbb{R}^n$.\n$$x = \\epsilon V \\left(\\frac{1}{\\sigma_i} e_i'\\right) = \\frac{\\epsilon}{\\sigma_i} (V e_i')$$\nThe product $Ve_i'$ selects the $i$-th column of the matrix $V$, which is the right singular vector $v_i$.\n$$x = \\frac{\\epsilon}{\\sigma_i} v_i$$\nWe can now compute the Euclidean norm of the solution $x$:\n$$\\|x\\|_2 = \\left\\|\\frac{\\epsilon}{\\sigma_i} v_i\\right\\|_2 = \\left|\\frac{\\epsilon}{\\sigma_i}\\right| \\|v_i\\|_2$$\nSince $\\epsilon > 0$ and $\\sigma_i = \\alpha^{i-1} > 0$, the absolute value is redundant. Furthermore, since $V$ is an orthogonal matrix, its columns are unit vectors, so $\\|v_i\\|_2 = 1$.\n$$\\|x\\|_2 = \\frac{\\epsilon}{\\sigma_i}$$\nThe problem asks for the amplification factor $r$, defined as the ratio of the norm of the solution to the magnitude of the perturbation:\n$$r = \\frac{\\|x\\|_2}{\\epsilon} = \\frac{\\epsilon/\\sigma_i}{\\epsilon} = \\frac{1}{\\sigma_i}$$\nGiven that $\\sigma_i = \\alpha^{i-1}$, the theoretical amplification factor is:\n$$r_{\\mathrm{theory}} = \\frac{1}{\\alpha^{i-1}} = \\alpha^{1-i}$$\nThis derivation shows that a perturbation in the direction of the $i$-th left singular vector $u_i$ is amplified in the solution by a factor equal to the reciprocal of the corresponding singular value $\\sigma_i$. For small $\\sigma_i$ (i.e., large $i$), this amplification is significant, demonstrating the ill-conditioned nature of the problem.\n\n### Numerical Implementation\n\nThe program will perform the following steps for each test case $(m, n, \\alpha, i, \\epsilon)$:\n1.  Set a fixed random seed for reproducibility.\n2.  Generate orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ using the QR decomposition of random matrices.\n3.  Construct the diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ with entries $\\sigma_k = \\alpha^{k-1}$ for $k=1, \\dots, n$.\n4.  Define the perturbation vector $b = \\epsilon u_i$, where $u_i$ is the $i$-th column of $U$ (using 1-based indexing for $i$).\n5.  Construct the pseudoinverse $A^{+} = V\\Sigma^{+}U^{\\mathsf{T}}$. $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ is built with diagonal entries $1/\\sigma_k$.\n6.  Calculate the solution $x = A^{+}b$.\n7.  Compute the numerical amplification factor $r = \\|x\\|_2 / \\epsilon$.\n8.  Compute the theoretical amplification factor $r_{\\mathrm{theory}} = 1/\\sigma_i = \\alpha^{1-i}$.\n9.  Compare $r$ and $r_{\\mathrm{theory}}$ using the specified condition: $|r - r_{\\mathrm{theory}}| \\le \\max(\\mathrm{atol}, \\mathrm{rtol} \\cdot |r_{\\mathrm{theory}}|)$, with $\\mathrm{rtol} = 10^{-6}$ and $\\mathrm{atol} = 10^{-8}$.\n10. Store the boolean result of the comparison.\n11. After processing all test cases, format the list of boolean results into the required string format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs matrices with specified singular value decay, applies a\n    controlled perturbation, and verifies the theoretical amplification\n    of the solution norm for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (m, n, alpha, i, epsilon)\n    test_cases = [\n        (6, 6, 0.2, 1, 10**-6),\n        (6, 6, 0.2, 6, 10**-6),\n        (10, 6, 0.3, 3, 10**-5),\n        (10, 6, 0.3, 6, 10**-8),\n        (8, 5, 0.5, 5, 10**-4),\n    ]\n\n    results = []\n    \n    # Use a fixed seed for reproducible generation of orthogonal matrices.\n    np.random.seed(0)\n\n    # Tolerances for comparison as specified in the problem statement.\n    rtol = 1e-6\n    atol = 1e-8\n\n    for case in test_cases:\n        m, n, alpha, i_one_based, epsilon = case\n\n        # Step 1: Construct orthogonal matrices U and V.\n        # A reproducible method is to use QR decomposition of a random matrix.\n        U, _ = np.linalg.qr(np.random.randn(m, m))\n        V, _ = np.linalg.qr(np.random.randn(n, n))\n\n        # Step 2: Construct the Sigma matrix with geometrically decaying singular values.\n        # sigma_k = alpha**(k-1) for k = 1,...,n.\n        # Python uses 0-based indexing, so s_vals[k] is alpha**k, for k=0...n-1\n        s_vals = np.array([alpha**k for k in range(n)])\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, s_vals)\n\n        # Step 3: Define the perturbation in the data space.\n        # b = epsilon * u_i, where u_i is the i-th left singular vector.\n        # Note: The problem uses a 1-based index `i`.\n        u_i = U[:, i_one_based - 1]\n        b = epsilon * u_i\n\n        # Step 4: Compute the minimum-norm least-squares solution x = A^+ b.\n        # We construct A^+ from SVD components as per definitions.\n        # A^+ = V * Sigma^+ * U^T\n        \n        # Construct Sigma_plus, the n x m pseudoinverse of Sigma.\n        # The reciprocals of s_vals are numerically stable as s_vals are  0.\n        s_plus_vals = 1.0 / s_vals\n        Sigma_plus = np.zeros((n, m))\n        np.fill_diagonal(Sigma_plus, s_plus_vals)\n        \n        A_plus = V @ Sigma_plus @ U.T\n        x = A_plus @ b\n\n        # Step 5: Compute the numerical amplification factor.\n        r_numeric = np.linalg.norm(x) / epsilon\n\n        # Step 6: Compute the theoretical amplification factor.\n        # r_theory = 1 / sigma_i = 1 / alpha**(i-1).\n        # We access the i-th singular value using 0-based index `i_one_based - 1`.\n        sigma_i = s_vals[i_one_based - 1]\n        r_theory = 1.0 / sigma_i\n\n        # Step 7: Compare r_numeric to r_theory using specified tolerances.\n        # The comparison checks if |r - r_theory| = max(atol, rtol * |r_theory|).\n        # We use abs(r_theory) for robustness, though it's positive here.\n        is_close = abs(r_numeric - r_theory) = max(atol, rtol * abs(r_theory))\n        results.append(is_close)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3147053"}, {"introduction": "Besides instability, another key reason a problem may be ill-posed is the lack of a unique solution. For a linear operator $A$, if there exists a non-zero vector $z$ such that $Az=0$ (i.e., the nullspace of $A$ is non-trivial), then any solution $x$ is not unique, because $A(x+z)=Ax+Az=Ax$. This practice explores a blur-and-downsample operator, a common model in signal processing, that possesses such a nullspace. You will computationally verify that distinct input signals can produce identical measurements, solidifying the connection between the algebraic concept of a nullspace and the practical failure to recover a single, unambiguous answer [@problem_id:3147025].", "problem": "Consider a discrete one-dimensional signal represented as a vector $x \\in \\mathbb{R}^n$ with $n$ even. Define a blur-and-downsample linear operator $A : \\mathbb{R}^n \\to \\mathbb{R}^{n/2}$ that averages adjacent entries and keeps one value per pair. Specifically, for a weight vector $w = (w_1, w_2)$ with $w_1 + w_2 = 1$, the operator acts by\n$$\n(Ax)_k = w_1 x_{2k-1} + w_2 x_{2k}, \\quad k = 1, 2, \\dots, \\frac{n}{2}.\n$$\nThis operator models a blur (weighted average within each pair) followed by decimation (keeping one value per pair). The inverse problem is to recover $x$ from measurements $y = A x$. Start from the core definitions of linear operators and nullspace: the nullspace of $A$ is $\\{z \\in \\mathbb{R}^n : A z = 0\\}$. An inverse problem is well-posed only if a solution exists, is unique, and depends continuously on the data; in this problem, focus on the uniqueness aspect. Using only these fundamental bases, reason about how the existence of a nontrivial nullspace leads to non-uniqueness of solutions of $A x = y$.\n\nYour task is to construct two distinct signals $x_1, x_2 \\in \\mathbb{R}^n$ such that $A x_1 = A x_2$ for the given $A$ and demonstrate computationally that they produce identical measurements. To do this, for each test case:\n- Construct $x_1$ deterministically as the ramp $x_{1,i} = i$ for $i = 1, 2, \\dots, n$.\n- Construct a nonzero vector $z \\in \\mathbb{R}^n$ that lies in the nullspace of $A$ by setting, for each pair,\n$$\nz_{2k-1} = w_2, \\quad z_{2k} = -w_1, \\quad k = 1, 2, \\dots, \\frac{n}{2}.\n$$\n- Form $x_2 = x_1 + \\alpha z$ with a specified scalar $\\alpha \\neq 0$.\n- Compute $y_1 = A x_1$ and $y_2 = A x_2$ and verify $y_1 = y_2$ within a numerical tolerance $\\tau$ by checking $\\|y_1 - y_2\\|_2 \\le \\tau$.\n- Estimate the nullspace dimension (nullity) of $A$ by computing the rank via the Singular Value Decomposition (SVD). Using the singular values $\\sigma_1, \\dots, \\sigma_{\\min(n/2, n)}$, define the numerical rank as the count of singular values greater than $\\tau$, and report the nullity as $n - \\text{rank}$.\n\nDesign your program to process the following test suite of parameter values $(n, w_1, w_2, \\alpha, \\tau)$:\n- Test case $1$: $(n, w_1, w_2, \\alpha, \\tau) = (8, 1/2, 1/2, 1, 10^{-12})$.\n- Test case $2$: $(n, w_1, w_2, \\alpha, \\tau) = (10, 3/4, 1/4, 2, 10^{-12})$.\n- Test case $3$: $(n, w_1, w_2, \\alpha, \\tau) = (2, 1, 0, -1, 10^{-12})$.\n- Test case $4$: $(n, w_1, w_2, \\alpha, \\tau) = (8, 3/5, 2/5, 3, 10^{-12})$.\n\nFor each test case, produce two outputs:\n1. A boolean indicating whether $A x_1$ and $A x_2$ are equal within tolerance, i.e., whether $\\|y_1 - y_2\\|_2 \\le \\tau$.\n2. An integer equal to the estimated nullspace dimension $n - \\text{rank}$.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases with the two values per test case flattened into one list. For example, the output should be of the form\n`[`bool_1`, `int_1`, `bool_2`, `int_2`, `bool_3`, `int_3`, `bool_4`, `int_4`]`.\nNo physical units are involved in this problem; all quantities are dimensionless. Angles are not used.", "solution": "The problem requires a computational demonstration of the non-uniqueness of solutions for a linear inverse problem, which arises from the existence of a non-trivial nullspace of the forward operator. The forward operator $A: \\mathbb{R}^n \\to \\mathbb{R}^{n/2}$ models a blur-and-downsample process on a one-dimensional signal $x \\in \\mathbb{R}^n$, where $n$ is even. The problem is to recover $x$ from the measurements $y = Ax$.\n\nThe operator $A$ is defined by its action on a vector $x$:\n$$\n(Ax)_k = w_1 x_{2k-1} + w_2 x_{2k}, \\quad k = 1, 2, \\dots, \\frac{n}{2}\n$$\nwhere $w_1$ and $w_2$ are weights such that $w_1 + w_2 = 1$. This is a linear operator. To facilitate computation, we represent $A$ as a matrix $A_{\\text{mat}} \\in \\mathbb{R}^{(n/2) \\times n}$. Using 0-based indexing for matrix rows and columns, the $k$-th row of $A_{\\text{mat}}$ (where $k$ ranges from $0$ to $\\frac{n}{2}-1$) corresponds to the $k+1$-th component of the output vector $y$. This row will have non-zero entries only in columns $2k$ and $2k+1$. Specifically, the matrix elements are:\n$$\n(A_{\\text{mat}})_{k,j} = \n\\begin{cases} \nw_1  \\text{if } j = 2k \\\\\nw_2  \\text{if } j = 2k+1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nfor $k = 0, \\dots, \\frac{n}{2}-1$ and $j = 0, \\dots, n-1$.\n\nThe uniqueness of the solution to the inverse problem $Ax=y$ is directly related to the nullspace (or kernel) of $A$, which is the set of all vectors $z$ such that $Az=0$. If the nullspace contains only the zero vector, i.e., $\\text{null}(A) = \\{0\\}$, then for any two solutions $x_1$ and $x_2$ such that $Ax_1 = y$ and $Ax_2=y$, we have $A(x_1-x_2) = Ax_1 - Ax_2 = y - y = 0$. This implies $x_1-x_2 \\in \\text{null}(A)$, so $x_1-x_2=0$, meaning $x_1=x_2$. The solution is unique.\n\nConversely, if there exists a non-zero vector $z \\in \\text{null}(A)$, the solution is not unique. Let $x_1$ be any solution to $Ax=y$. We can construct a second, distinct signal $x_2 = x_1 + \\alpha z$ for any non-zero scalar $\\alpha$. Applying the operator $A$ to $x_2$ yields:\n$$\nAx_2 = A(x_1 + \\alpha z) = Ax_1 + A(\\alpha z) = y + \\alpha (Az) = y + \\alpha \\cdot 0 = y\n$$\nSince $\\alpha z \\neq 0$, $x_1$ and $x_2$ are distinct vectors that both map to the same measurement vector $y$.\n\nThe problem asks us to demonstrate this principle by constructing two such signals, $x_1$ and $x_2$, and verifying that they produce the same output $y$.\nFirst, a signal $x_1$ is defined as a simple ramp function: $x_{1,i} = i$ for $i=1, \\dots, n$.\nSecond, a non-zero vector $z \\in \\text{null}(A)$ is constructed. The condition $Az=0$ means that for each $k \\in \\{1, \\dots, n/2\\}$, we must have $w_1 z_{2k-1} + w_2 z_{2k} = 0$. The problem provides a specific construction for such a $z$ by setting $z_{2k-1} = w_2$ and $z_{2k} = -w_1$ for each pair. This choice indeed satisfies the nullspace condition: $w_1(w_2) + w_2(-w_1) = w_1 w_2 - w_2 w_1 = 0$.\nWith these, we form $x_2 = x_1 + \\alpha z$. By the reasoning above, we expect $Ax_1 = Ax_2$. We verify this numerically by computing $y_1 = Ax_1$ and $y_2 = Ax_2$ and checking if the Euclidean norm of their difference, $\\|y_1 - y_2\\|_2$, is within a small numerical tolerance $\\tau$.\n\nThe second part of the task is to estimate the dimension of the nullspace, known as the nullity of $A$. The rank-nullity theorem states that for a linear map $A: \\mathbb{R}^n \\to \\mathbb{R}^m$, $\\text{rank}(A) + \\text{nullity}(A) = n$. We can estimate the rank of the matrix $A_{\\text{mat}}$ using its Singular Value Decomposition (SVD). The number of singular values $\\sigma_i$ that are greater than a small tolerance $\\tau$ provides a numerically stable estimate of the rank. The nullity is then calculated as $n - \\text{rank}(A)$. For the given operator $A$, as long as $w_1$ and $w_2$ are not both zero (ensured by $w_1+w_2=1$), the $n/2$ rows of the matrix $A_{\\text{mat}}$ are linearly independent. This is because each row $k$ has non-zero entries in a unique pair of columns ($2k$ and $2k+1$) not shared by any other row. Therefore, the rank of $A$ is $n/2$, and its nullity is $n - n/2 = n/2$. This implies that the solution space for any given $y$ is an affine subspace of dimension $n/2$.\n\nThe implementation will perform these constructions and calculations for each test case.\nFor each given set of parameters $(n, w_1, w_2, \\alpha, \\tau)$:\n1. Construct the $(n/2) \\times n$ matrix $A_{\\text{mat}}$.\n2. Construct vectors $x_1$, $z$, and $x_2$.\n3. Compute $y_1 = A_{\\text{mat}} x_1$ and $y_2 = A_{\\text{mat}} x_2$.\n4. Check if $\\|y_1 - y_2\\|_2 \\le \\tau$.\n5. Compute the SVD of $A_{\\text{mat}}$, count singular values $>\\tau$ to find the rank, and calculate nullity as $n - \\text{rank}$.\nThe results (a boolean and an integer) for all test cases are then collected and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a suite of test cases, demonstrating non-uniqueness\n    in an inverse problem due to a non-trivial nullspace.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, w1, w2, alpha, tau)\n        (8, 1/2, 1/2, 1, 1e-12),\n        (10, 3/4, 1/4, 2, 1e-12),\n        (2, 1, 0, -1, 1e-12),\n        (8, 3/5, 2/5, 3, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, w1, w2, alpha, tau = case\n        \n        # The dimension of the output space is m = n/2.\n        m = n // 2\n        \n        # 1. Construct the matrix representation of the linear operator A.\n        # The matrix A_mat has dimensions (m x n).\n        A_mat = np.zeros((m, n))\n        for k in range(m):\n            # Using 0-based indexing for rows (k) and columns.\n            # (Ax)_{k+1} corresponds to row k of the matrix product.\n            # x_{2(k+1)-1} = x_{2k+1} - column 2k\n            # x_{2(k+1)}   = x_{2k+2} - column 2k+1\n            # (A_mat @ x)[k] = w1 * x[2*k] + w2 * x[2*k+1]\n            A_mat[k, 2*k] = w1\n            A_mat[k, 2*k+1] = w2\n\n        # 2. Construct the signal x1 as a ramp.\n        # x1_i = i, for i=1,...,n. numpy's arange is 0-indexed, so we go from 1 to n+1.\n        x1 = np.arange(1, n + 1, dtype=float)\n\n        # 3. Construct a non-zero vector z in the nullspace of A.\n        # z_{2k-1} = w2, z_{2k} = -w1 for k=1,...,m\n        z = np.zeros(n)\n        for k in range(m):\n            # 0-based indexing: z_{2k} = w2, z_{2k+1} = -w1\n            z[2*k] = w2\n            z[2*k+1] = -w1\n\n        # 4. Construct the second signal x2 = x1 + alpha * z.\n        x2 = x1 + alpha * z\n\n        # 5. Compute the measurements y1 = A*x1 and y2 = A*x2.\n        y1 = A_mat @ x1\n        y2 = A_mat @ x2\n\n        # 6. Verify that y1 and y2 are equal within the given tolerance.\n        # This is the first result for the test case.\n        norm_diff = np.linalg.norm(y1 - y2)\n        are_equal = norm_diff = tau\n        results.append(are_equal)\n\n        # 7. Estimate the nullspace dimension (nullity) of A.\n        # The rank-nullity theorem states: rank(A) + nullity(A) = n (number of columns).\n        # We compute the rank from the Singular Value Decomposition (SVD).\n        singular_values = np.linalg.svd(A_mat, compute_uv=False)\n        \n        # The numerical rank is the number of singular values greater than the tolerance.\n        rank = np.sum(singular_values  tau)\n        \n        # The nullity is n - rank.\n        nullity = n - rank\n        \n        # This is the second result for the test case.\n        results.append(nullity)\n\n    # Final print statement in the exact required format.\n    # map(str,...) converts boolean True to 'True', False to 'False', and integers to strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3147025"}, {"introduction": "Having diagnosed the core issues of ill-posedness, we now turn to a powerful method for finding stable and meaningful solutions: regularization. Regularization works by incorporating prior knowledge about the solution into a modified objective function, typically balancing data fidelity with a penalty term. This exercise connects the choice of the data fidelity term to statistical models of noise, comparing the standard squared $\\ell_2$-norm misfit, which arises from a Gaussian noise assumption, with the robust $\\ell_1$-norm misfit derived from a Laplace noise assumption. By implementing and comparing estimators on data with and without large outliers, you will gain a practical appreciation for how statistical modeling guides the design of robust algorithms for solving inverse problems [@problem_id:3147000].", "problem": "You are given a linear inverse model with observations modeled by $b = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{m}$. The data noise $\\varepsilon$ may be either Laplace-distributed or Gaussian-distributed, and the reconstruction is regularized by a linear operator $L \\in \\mathbb{R}^{p \\times n}$ that promotes smoothness through its action on $x$. The task is to design and implement a program that demonstrates robustness to outliers by comparing a Laplace-data model to a Gaussian-data model in a maximum a posteriori (MAP) estimation setting, without using any pre-specified shortcut formulas. Begin from core definitions: probability density functions, maximum likelihood, and the concept of a Gaussian prior leading to a quadratic penalty. Derive the appropriate objective functions for the two noise models and formulate computational strategies to obtain the corresponding MAP estimates. Ensure scientific realism by using reproducible synthetic data with a clearly defined test suite.\n\nConstruct $A$, $L$, and a ground-truth $x^\\star$ deterministically as follows:\n- Let $m = 60$ and $n = 40$.\n- Generate $A$ by sampling independent standard normal entries with a fixed seed $s_A = 7$ and normalizing each column to have unit $\\ell_2$ norm.\n- Define $L$ as the first-order forward difference operator: for $k \\in \\{0, 1, \\dots, n-2\\}$, the $k$-th row of $L$ has entries $-1$ at column $k$ and $+1$ at column $k+1$, and zeros elsewhere. Thus $L \\in \\mathbb{R}^{(n-1) \\times n}$.\n- Define a piecewise-constant ground truth $x^\\star \\in \\mathbb{R}^{n}$ with zero-based indexing: set $x^\\star_k = 1$ for $k \\in \\{5,6,\\dots,15\\}$, set $x^\\star_k = -0.6$ for $k \\in \\{25,26,\\dots,35\\}$, and set $x^\\star_k = 0$ otherwise.\n\nConstruct observations $b$ for each test case by $b = A x^\\star + \\varepsilon$, where $\\varepsilon$ is generated according to the specified noise model and outlier configuration. Use fixed seeds to ensure reproducibility across cases. For Laplace noise, draw independent noise entries from a Laplace distribution with zero mean and scale parameter $s_L = 0.05$. For Gaussian noise, draw independent noise entries from a normal distribution with zero mean and standard deviation $\\sigma = 0.05$. In outlier cases, add two large perturbations (outliers) of amplitude $a_o = 5.0$ to entries of $b$ at measurement indices $i = 3$ and $i = 17$ (with zero-based indexing). All angles, if any arise, must be considered in radians, but this construction does not require angles.\n\nFor the Laplace noise model, derive the appropriate data misfit from the Laplace probability density function and combine it with the quadratic smoothness penalty induced by $L$ to obtain the MAP estimator. For the Gaussian noise model, derive the corresponding data misfit from the Gaussian probability density function and combine it with the same quadratic smoothness penalty to obtain the MAP estimator. Implement computationally sound solvers for both estimators, ensuring numerical stability. For the Laplace-data MAP estimator, use a robust iterative scheme that converges to a minimizer of a convex objective. For the Gaussian-data MAP estimator, use a direct linear algebraic solver based on normal equations.\n\nDefine the test suite consisting of the following five cases, each specifying the noise type, outlier configuration, and regularization strength $\\lambda$:\n- Case $1$: Laplace noise with no outliers, $\\lambda = 0.1$, seed $s_1 = 101$.\n- Case $2$: Laplace noise with outliers, $\\lambda = 0.1$, seed $s_2 = 102$.\n- Case $3$: Gaussian noise with outliers, $\\lambda = 0.1$, seed $s_3 = 103$.\n- Case $4$: Gaussian noise with no outliers, $\\lambda = 0.1$, seed $s_4 = 104$.\n- Case $5$: Laplace noise with outliers, boundary case $\\lambda = 0$, seed $s_5 = 105$.\n\nFor each case, compute:\n- The estimator under the Laplace-data model with quadratic regularization using $L$ (MAP for Laplace noise).\n- The estimator under the Gaussian-data model with the same quadratic regularization using $L$ (MAP for Gaussian noise).\n\nQuantify performance for each case by computing the Euclidean reconstruction error $\\|x - x^\\star\\|_2$ for both estimators. The result for each case must be a boolean indicating whether the Laplace-data estimator yields strictly smaller Euclidean reconstruction error than the Gaussian-data estimator, i.e., return $\\text{True}$ if $\\|x_{\\text{Laplace}} - x^\\star\\|_2  \\|x_{\\text{Gaussian}} - x^\\star\\|_2$, otherwise return $\\text{False}$.\n\nYour program must produce a single line of output containing the results for the five cases as a comma-separated list enclosed in square brackets, with booleans in the order of the cases described above. For example, the output must be of the form $[r_1,r_2,r_3,r_4,r_5]$, where each $r_j$ is either $\\text{True}$ or $\\text{False}$.\n\nNo physical units are involved in this problem. Percentages must not be used anywhere in the output or computation.", "solution": "The task is to compare the robustness of two estimators for a linear inverse problem, derived from a Maximum A Posteriori (MAP) framework. The first estimator is based on a Gaussian noise model, and the second on a Laplace noise model. The comparison is performed on synthetic data with and without outliers.\n\nWe begin from the Bayesian perspective. The MAP estimate $\\hat{x}_{\\text{MAP}}$ of a parameter vector $x$ given measurement data $b$ maximizes the posterior probability $p(x|b)$. By Bayes' theorem, this is equivalent to minimizing the negative log-posterior:\n$$ \\hat{x}_{\\text{MAP}} = \\arg\\max_x p(x|b) = \\arg\\max_x p(b|x)p(x) = \\arg\\min_x \\left\\{ -\\log(p(b|x)) - \\log(p(x)) \\right\\} $$\nHere, $p(b|x)$ is the likelihood of the data given the parameters, and $p(x)$ is the prior probability of the parameters. The term $-\\log(p(b|x))$ is the data-fidelity term, and $-\\log(p(x))$ is the regularization term.\n\nThe model for the observations is $b = Ax + \\varepsilon$, where $\\varepsilon \\in \\mathbb{R}^m$ is a vector of i.i.d. noise components. The likelihood $p(b|x)$ is determined by the probability distribution of the noise $\\varepsilon$.\n\nFor the **Gaussian-data model**, each noise component $\\varepsilon_i$ is assumed to follow a Gaussian distribution with mean $0$ and variance $\\sigma^2$, denoted $\\mathcal{N}(0, \\sigma^2)$. The probability density function (PDF) is $p(\\varepsilon_i) = (\\sqrt{2\\pi}\\sigma)^{-1} \\exp(-\\varepsilon_i^2 / (2\\sigma^2))$. The likelihood for the entire measurement vector $b$ is the product of the individual densities, evaluated at the residuals $\\varepsilon_i = (Ax)_i - b_i$:\n$$ p(b|x) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{((Ax)_i - b_i)^2}{2\\sigma^2}\\right) = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right)^m \\exp\\left(-\\frac{\\|Ax-b\\|_2^2}{2\\sigma^2}\\right) $$\nThe negative log-likelihood, ignoring constant terms that do not influence the minimization with respect to $x$, is proportional to the squared Euclidean norm ($\\ell_2$-norm) of the residual:\n$$ -\\log(p(b|x)) \\propto \\|Ax-b\\|_2^2 $$\n\nFor the **Laplace-data model**, each noise component $\\varepsilon_i$ is assumed to follow a Laplace distribution with mean $0$ and scale parameter $s_L$, denoted $\\text{Laplace}(0, s_L)$. The PDF is $p(\\varepsilon_i) = (2s_L)^{-1} \\exp(-|\\varepsilon_i|/s_L)$. The likelihood is:\n$$ p(b|x) = \\prod_{i=1}^{m} \\frac{1}{2s_L} \\exp\\left(-\\frac{|(Ax)_i - b_i|}{s_L}\\right) = \\left(\\frac{1}{2s_L}\\right)^m \\exp\\left(-\\frac{\\|Ax-b\\|_1}{s_L}\\right) $$\nThe corresponding negative log-likelihood is proportional to the $\\ell_1$-norm of the residual:\n$$ -\\log(p(b|x)) \\propto \\|Ax-b\\|_1 $$\nThe $\\ell_1$-norm data fidelity is known to be more robust to large-magnitude outliers than the $\\ell_2$-norm.\n\nThe regularization is introduced via the prior term $p(x)$. The problem specifies a quadratic smoothness penalty induced by a linear operator $L$. This corresponds to a Gaussian prior on the transformed vector $Lx$, i.e., $p(Lx) \\propto \\exp(-\\alpha \\|Lx\\|_2^2)$ for some constant $\\alpha > 0$. The negative log-prior is thus proportional to a quadratic penalty on $x$:\n$$ -\\log(p(x)) \\propto \\|Lx\\|_2^2 $$\n\nCombining the data-fidelity and regularization terms with a regularization parameter $\\lambda$ yields the final objective functions to be minimized:\n1.  **Gaussian-data MAP estimator ($\\hat{x}_{\\text{G}}$)**: This corresponds to Tikhonov regularization.\n    $$ \\hat{x}_{\\text{G}} = \\arg\\min_x J_G(x) = \\arg\\min_x \\left\\{ \\|Ax-b\\|_2^2 + \\lambda \\|Lx\\|_2^2 \\right\\} $$\n2.  **Laplace-data MAP estimator ($\\hat{x}_{\\text{L}}$)**: This is a form of $\\ell_1$-regularized regression, but with the $\\ell_1$-norm on the data term.\n    $$ \\hat{x}_{\\text{L}} = \\arg\\min_x J_L(x) = \\arg\\min_x \\left\\{ \\|Ax-b\\|_1 + \\lambda \\|Lx\\|_2^2 \\right\\} $$\n\nTo find the solution for the Gaussian model, we note that $J_G(x)$ is a differentiable convex function. Its minimum is found by setting its gradient with respect to $x$ to zero:\n$$ \\nabla_x J_G(x) = \\nabla_x \\left( (Ax-b)^T(Ax-b) + \\lambda(Lx)^T(Lx) \\right) = 2A^T(Ax-b) + 2\\lambda L^T(Lx) = 0 $$\nRearranging gives the normal equations, a system of linear equations:\n$$ (A^T A + \\lambda L^T L)x = A^T b $$\nFor $\\lambda > 0$, the matrix $(A^T A + \\lambda L^T L)$ is symmetric positive-definite, ensuring a unique solution that can be computed efficiently using a direct linear solver.\n\nThe objective function $J_L(x)$ for the Laplace model is convex but non-differentiable due to the $\\ell_1$-norm. It cannot be solved by directly setting a gradient to zero. An iterative method is required. We use the Iteratively Reweighted Least Squares (IRLS) algorithm. The key idea is to approximate the $\\ell_1$-norm term by a weighted $\\ell_2$-norm. Using the identity $|r_i| = r_i^2 / |r_i|$, we can express the $\\ell_1$-norm term as $\\sum_i w_i ((Ax)_i - b_i)^2$, where the weights $w_i = 1 / |(Ax)_i - b_i|$ depend on $x$. In IRLS, these weights are updated at each iteration using the solution from the previous iteration, $x^{(k)}$. This leads to solving a sequence of weighted least-squares problems.\nAt iteration $k+1$, we solve for $x^{(k+1)}$ by minimizing:\n$$ \\min_x \\left\\{ \\sum_{i=1}^m w_i^{(k)} ((Ax)_i - b_i)^2 + \\lambda \\|Lx\\|_2^2 \\right\\} = \\min_x \\left\\{ (Ax-b)^T W^{(k)} (Ax-b) + \\lambda \\|Lx\\|_2^2 \\right\\} $$\nwhere $W^{(k)}$ is a diagonal matrix with entries $W_{ii}^{(k)} = (\\max(|(Ax^{(k)}-b)_i|, \\epsilon))^{-1}$. The small parameter $\\epsilon > 0$ ensures numerical stability by preventing division by zero. Setting the gradient of this quadratic objective to zero yields the linear system for the next iterate $x^{(k+1)}$:\n$$ (A^T W^{(k)} A + \\lambda L^T L) x^{(k+1)} = A^T W^{(k)} b $$\nThis system is solved repeatedly until the solution $x^{(k)}$ converges. The initial guess $x^{(0)}$ can be set to the Gaussian-data solution $\\hat{x}_{\\text{G}}$.\n\nThe implementation will follow these derivations to construct the estimators. The parameters ($m=60, n=40$), matrices ($A, L$), ground truth $x^\\star$, and test cases are defined as specified in the problem statement. The performance of each estimator is evaluated by computing the Euclidean reconstruction error $\\| \\hat{x} - x^\\star \\|_2$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the inverse problem by comparing Laplace and Gaussian data models.\n\n    Derives and implements MAP estimators for both models and evaluates their\n    robustness to outliers across a suite of test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    M, N = 60, 40\n    S_A_SEED = 7\n    S_L_SCALE = 0.05\n    SIGMA_STD = 0.05\n    OUTLIER_AMP = 5.0\n    OUTLIER_INDICES = [3, 17]\n\n    # --- Test Case Suite ---\n    test_cases = [\n        {'case': 1, 'noise_type': 'laplace', 'outliers': False, 'lambda_val': 0.1, 'seed': 101},\n        {'case': 2, 'noise_type': 'laplace', 'outliers': True, 'lambda_val': 0.1, 'seed': 102},\n        {'case': 3, 'noise_type': 'gaussian', 'outliers': True, 'lambda_val': 0.1, 'seed': 103},\n        {'case': 4, 'noise_type': 'gaussian', 'outliers': False, 'lambda_val': 0.1, 'seed': 104},\n        {'case': 5, 'noise_type': 'laplace', 'outliers': True, 'lambda_val': 0.0, 'seed': 105},\n    ]\n\n    # --- Deterministic Setup ---\n    # Generate matrix A\n    rng_A = np.random.default_rng(seed=S_A_SEED)\n    A = rng_A.standard_normal((M, N))\n    A /= np.linalg.norm(A, axis=0) # Normalize columns to unit l2 norm\n\n    # Define L, the first-order forward difference operator\n    L = np.zeros((N - 1, N))\n    rows = np.arange(N - 1)\n    L[rows, rows] = -1.0\n    L[rows, rows + 1] = 1.0\n\n    # Define ground truth x_star\n    x_star = np.zeros(N)\n    x_star[5:16] = 1.0\n    x_star[25:36] = -0.6\n\n    # Pre-compute matrix products for efficiency\n    AtA = A.T @ A\n    LtL = L.T @ L\n\n    # --- Solver Implementations ---\n\n    def compute_gaussian_estimator(A, b, L, lambda_val, AtA_p, LtL_p):\n        \"\"\"\n        Computes the MAP estimate for the Gaussian-data model.\n        Solves (A^T A + lambda * L^T L) x = A^T b.\n        \"\"\"\n        H = AtA_p + lambda_val * LtL_p\n        rhs = A.T @ b\n        try:\n            x_g = np.linalg.solve(H, rhs)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular cases (e.g., lambda=0 with ill-conditioned A^T A)\n            x_g = np.linalg.pinv(H) @ rhs\n        return x_g\n\n    def compute_laplace_estimator(A, b, L, lambda_val, AtA_p, LtL_p, x_initial):\n        \"\"\"\n        Computes the MAP estimate for the Laplace-data model using IRLS.\n        Solves argmin ||Ax-b||_1 + lambda * ||Lx||_2^2.\n        \"\"\"\n        num_iter = 20\n        epsilon = 1e-6\n        x_l = np.copy(x_initial)\n\n        for _ in range(num_iter):\n            residuals = np.abs(A @ x_l - b)\n            weights = 1.0 / np.maximum(residuals, epsilon)\n            W = np.diag(weights)\n\n            H = A.T @ W @ A + lambda_val * LtL_p\n            rhs = A.T @ W @ b\n\n            try:\n                x_l = np.linalg.solve(H, rhs)\n            except np.linalg.LinAlgError:\n                x_l = np.linalg.pinv(H) @ rhs\n        \n        return x_l\n\n    # --- Main Loop for Test Cases ---\n    results = []\n    for case in test_cases:\n        # Generate noise based on the case\n        rng_case = np.random.default_rng(seed=case['seed'])\n        if case['noise_type'] == 'laplace':\n            epsilon_noise = rng_case.laplace(0, S_L_SCALE, size=M)\n        else: # gaussian\n            epsilon_noise = rng_case.normal(0, SIGMA_STD, size=M)\n\n        # Construct observations b\n        b = A @ x_star + epsilon_noise\n        if case['outliers']:\n            b[OUTLIER_INDICES[0]] += OUTLIER_AMP\n            b[OUTLIER_INDICES[1]] += OUTLIER_AMP\n\n        # --- Compute Estimators ---\n        lambda_val = case['lambda_val']\n\n        # Gaussian-data model estimator\n        x_gaussian = compute_gaussian_estimator(A, b, L, lambda_val, AtA, LtL)\n        \n        # Laplace-data model estimator (initialize with Gaussian solution)\n        x_laplace = compute_laplace_estimator(A, b, L, lambda_val, AtA, LtL, x_initial=x_gaussian)\n\n        # --- Quantify Performance ---\n        err_gaussian = np.linalg.norm(x_gaussian - x_star)\n        err_laplace = np.linalg.norm(x_laplace - x_star)\n\n        # Compare errors and store the boolean result\n        results.append(err_laplace  err_gaussian)\n\n    # --- Final Output ---\n    # Format the list of booleans as a string \"[True,False,...]\"\n    print(f\"[{','.join(map(lambda x: str(x), results))}]\")\n\nsolve()\n```", "id": "3147000"}]}