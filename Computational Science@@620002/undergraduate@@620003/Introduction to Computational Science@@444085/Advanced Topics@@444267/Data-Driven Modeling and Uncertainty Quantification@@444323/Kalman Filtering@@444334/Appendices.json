{"hands_on_practices": [{"introduction": "To truly understand the Kalman filter, we must first grasp its central component: the Kalman gain. This exercise strips the filter down to its essentials—a single-state system—to derive the gain from first principles. By doing so, you will see how the filter optimally balances the uncertainty of your prediction with the uncertainty of your measurement, providing a powerful intuition that scales to more complex problems. [@problem_id:3149123]", "problem": "A scalar state $x_{k}$ evolves in discrete time according to a random-walk model $x_{k} = x_{k-1} + w_{k-1}$, where the process noise $w_{k-1}$ is zero-mean Gaussian with variance $Q > 0$. At time $k$, a measurement $z_{k}$ of the state is obtained via $z_{k} = x_{k} + v_{k}$, where the measurement noise $v_{k}$ is zero-mean Gaussian with variance $R > 0$. Assume $w_{k-1}$ and $v_{k}$ are mutually independent and independent of past states and measurements. Let the prior estimate be $\\hat{x}_{k|k-1}$ with prior error variance $P_{k|k-1}$, and consider the Minimum Mean Square Error (MMSE) estimator for the scalar linear Gaussian case.\n\nTasks:\n- Starting from the definitions of the model and the MMSE principle, derive the analytic expression for the scalar Kalman gain $K_{k}$ that minimizes the expected squared estimation error at time $k$. Then show that the posterior estimate $\\hat{x}_{k|k}$ can be written as a weighted average of the prior estimate $\\hat{x}_{k|k-1}$ and the measurement $z_{k}$.\n- For the random-walk model described above, analyze the variance recursion and derive the steady-state gain $K_{\\infty}$ that arises as $k \\to \\infty$ under constant $Q$ and $R$. Express this steady-state gain solely as a function of the ratio $r = Q/R$.\n\nAnswer specification:\n- Provide your final answer as a single closed-form analytic expression for $K_{\\infty}(r)$.\n- No rounding is required, and no physical units are needed.", "solution": "The setting is a scalar linear Gaussian estimation problem. We leverage the Minimum Mean Square Error (MMSE) principle, which states that among unbiased estimators, the conditional mean is the estimator that minimizes the expected squared error. For linear Gaussian systems, the optimal MMSE estimator takes a linear innovation form.\n\nWe start from the model definitions:\n- State evolution: $x_{k} = x_{k-1} + w_{k-1}$ with $w_{k-1} \\sim \\mathcal{N}(0, Q)$.\n- Measurement: $z_{k} = x_{k} + v_{k}$ with $v_{k} \\sim \\mathcal{N}(0, R)$.\n- Independence: $w_{k-1}$ and $v_{k}$ are independent of each other and of past states and measurements.\n\nLet $\\hat{x}_{k|k-1}$ be the prior estimate and $P_{k|k-1}$ the prior error variance at time $k$. Consider estimators of the form\n$$\n\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k} \\left( z_{k} - \\hat{x}_{k|k-1} \\right),\n$$\nwhere $K_{k}$ is a scalar gain chosen to minimize the expected squared estimation error. Define the estimation error after update\n$$\ne_{k|k} = x_{k} - \\hat{x}_{k|k} = x_{k} - \\hat{x}_{k|k-1} - K_{k} \\left( z_{k} - \\hat{x}_{k|k-1} \\right).\n$$\nUsing the measurement model $z_{k} = x_{k} + v_{k}$, the innovation becomes\n$$\nz_{k} - \\hat{x}_{k|k-1} = \\left( x_{k} - \\hat{x}_{k|k-1} \\right) + v_{k}.\n$$\nTherefore,\n$$\ne_{k|k} = \\left( x_{k} - \\hat{x}_{k|k-1} \\right) - K_{k} \\left( \\left( x_{k} - \\hat{x}_{k|k-1} \\right) + v_{k} \\right) = (1 - K_{k}) \\left( x_{k} - \\hat{x}_{k|k-1} \\right) - K_{k} v_{k}.\n$$\nBy independence and zero mean of the error terms, the posterior mean squared error is\n$$\n\\mathbb{E}\\!\\left[ e_{k|k}^{2} \\right] = (1 - K_{k})^{2} \\, \\mathbb{V}\\mathrm{ar}\\!\\left( x_{k} - \\hat{x}_{k|k-1} \\right) + K_{k}^{2} \\, \\mathbb{V}\\mathrm{ar}(v_{k}) = (1 - K_{k})^{2} P_{k|k-1} + K_{k}^{2} R.\n$$\nWe minimize this quadratic function in $K_{k}$ by differentiation:\n$$\n\\frac{d}{dK_{k}} \\left[ (1 - K_{k})^{2} P_{k|k-1} + K_{k}^{2} R \\right] = -2(1 - K_{k}) P_{k|k-1} + 2 K_{k} R.\n$$\nSetting the derivative to zero yields\n$$\n-2(1 - K_{k}) P_{k|k-1} + 2 K_{k} R = 0 \\quad \\Rightarrow \\quad K_{k} R = (1 - K_{k}) P_{k|k-1}.\n$$\nSolving for $K_{k}$ gives the scalar optimal Kalman gain\n$$\nK_{k} = \\frac{P_{k|k-1}}{P_{k|k-1} + R}.\n$$\nSubstituting this into the estimator form shows the weighted-average interpretation:\n$$\n\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k} \\left( z_{k} - \\hat{x}_{k|k-1} \\right) = (1 - K_{k}) \\, \\hat{x}_{k|k-1} + K_{k} \\, z_{k},\n$$\nwhere $K_{k} \\in (0, 1)$ when $P_{k|k-1} > 0$ and $R > 0$, so the posterior estimate is a convex combination of the prior estimate and the measurement. The weights adapt according to the relative confidence: larger $P_{k|k-1}$ (less confidence in the prior) increases $K_{k}$, while larger $R$ (less confidence in the measurement) decreases $K_{k}$.\n\nNext, we analyze the variance recursion for the random-walk model to obtain the steady-state gain as $k \\to \\infty$. The prediction step for the variance is\n$$\nP_{k|k-1} = P_{k-1|k-1} + Q.\n$$\nThe posterior variance after update, using the minimized mean squared error expression above, is\n$$\nP_{k|k} = (1 - K_{k})^{2} P_{k|k-1} + K_{k}^{2} R.\n$$\nSubstituting the optimal $K_{k} = \\frac{P_{k|k-1}}{P_{k|k-1} + R}$ yields a well-known equivalent closed form, which we can compute directly:\n\n$$\nP_{k|k} = (1 - K_{k}) P_{k|k-1} = \\left( 1 - \\frac{P_{k|k-1}}{P_{k|k-1} + R} \\right) P_{k|k-1} = \\frac{R \\, P_{k|k-1}}{P_{k|k-1} + R}.\n$$\n\nIn steady state, denote $P_{k|k} \\to P$ and $P_{k|k-1} \\to P + Q$. The steady-state algebraic equation for $P$ becomes\n$$\nP = \\frac{R \\, (P + Q)}{(P + Q) + R}.\n$$\nMultiplying both sides by $(P + Q + R)$ and simplifying:\n\n$$\nP (P + Q + R) = R (P + Q) \\quad \\Rightarrow \\quad P^{2} + P Q + P R = R P + R Q.\n$$\n\nCancel $R P$ on both sides:\n\n$$\nP^{2} + P Q - R Q = 0.\n$$\n\nThis is a quadratic equation in $P$ with positive coefficients $Q > 0$, $R > 0$. The physically meaningful (positive) solution is\n\n$$\nP = \\frac{-Q + \\sqrt{Q^{2} + 4 R Q}}{2}.\n$$\n\nThe corresponding steady-state predicted variance is\n\n$$\nP_{k|k-1} \\to P + Q = \\frac{Q + \\sqrt{Q^{2} + 4 R Q}}{2}.\n$$\n\nTherefore the steady-state gain $K_{\\infty}$ is\n\n$$\nK_{\\infty} = \\frac{P + Q}{(P + Q) + R} = \\frac{\\frac{Q + \\sqrt{Q^{2} + 4 R Q}}{2}}{\\frac{Q + \\sqrt{Q^{2} + 4 R Q}}{2} + R} = \\frac{Q + \\sqrt{Q^{2} + 4 R Q}}{Q + \\sqrt{Q^{2} + 4 R Q} + 2 R}.\n$$\n\nExpress $K_{\\infty}$ solely as a function of the ratio $r = Q / R$. Substitute $Q = r R$ to obtain\n\n$$\nK_{\\infty}(r) = \\frac{r R + \\sqrt{r^{2} R^{2} + 4 r R^{2}}}{r R + \\sqrt{r^{2} R^{2} + 4 r R^{2}} + 2 R} = \\frac{r + \\sqrt{r^{2} + 4 r}}{r + \\sqrt{r^{2} + 4 r} + 2}.\n$$\n\nThis closed-form expression reveals the dependence on $r = Q / R$ and matches intuition across ratios:\n- As $r \\to 0$ (negligible process noise relative to measurement noise), $\\sqrt{r^{2} + 4 r} \\to 0$, hence $K_{\\infty}(r) \\to 0$, placing nearly full weight on the prior.\n- As $r \\to \\infty$ (dominant process noise relative to measurement noise), $\\sqrt{r^{2} + 4 r} \\sim r$, hence $K_{\\infty}(r) \\to 1$, placing nearly full weight on the measurement.\n- For intermediate $r$, $K_{\\infty}(r)$ increases monotonically with $r$, smoothly balancing the prior and measurement according to their relative uncertainties.\n\nThe required final answer is the closed-form function $K_{\\infty}(r)$.", "answer": "$$\\boxed{\\frac{r+\\sqrt{r^{2}+4r}}{r+\\sqrt{r^{2}+4r}+2}}$$", "id": "3149123"}, {"introduction": "A theoretical model is only as good as its real-world performance. This hands-on coding exercise introduces a powerful diagnostic tool: the innovations whiteness test. You will implement a Kalman filter and then test if its prediction errors are truly random, as theory predicts they should be for an optimal filter. This practice is essential for learning how to validate your filter and diagnose mismatches between your model and reality. [@problem_id:3149135]", "problem": "You will implement and use the Kalman filter (KF) to test the whiteness of the innovations sequence by computing its sample autocorrelation. Work in a purely discrete-time, linear, time-invariant, Gaussian state-space setting. The fundamental base to use is: linear Gaussian models, conditional expectation as the minimum mean squared error estimator, and the orthogonality principle of least squares. Do not use any shortcut formulas that are not derivable from these bases; instead, design the KF algorithm from the definitions of prediction and correction for Gaussian random variables and linear maps.\n\nModel assumptions and definitions:\n- The hidden state at time step $k$ is $x_k \\in \\mathbb{R}^2$, representing position and velocity as $x_k = \\begin{bmatrix}x^{(p)}_k \\\\ x^{(v)}_k\\end{bmatrix}$.\n- The true dynamics are linear: $x_k = F x_{k-1} + B u + w_{k-1}$, with $F \\in \\mathbb{R}^{2 \\times 2}$, $B \\in \\mathbb{R}^{2 \\times 1}$, constant input $u \\in \\mathbb{R}$, and process noise $w_{k-1} \\sim \\mathcal{N}(0, Q_{\\text{true}})$ that are independent and identically distributed (i.i.d.) over $k$.\n- The observation is scalar: $z_k = H x_k + v_k$, with $H \\in \\mathbb{R}^{1 \\times 2}$ and measurement noise $v_k \\sim \\mathcal{N}(0, R_{\\text{true}})$, i.i.d. over $k$, independent from $\\{w_j\\}_{j}$ and from the initial state $x_0$.\n- The innovations sequence is defined by $e_k = z_k - H \\hat{x}_{k|k-1}$, where $\\hat{x}_{k|k-1}$ is the KF one-step-ahead prediction of the state given data up to time $k-1$. Under a correctly specified model and optimal KF, the innovations $\\{e_k\\}$ are zero-mean, uncorrelated Gaussian random variables (white).\n\nWhiteness test to implement:\n- Given a finite sequence $\\{e_k\\}_{k=1}^{N}$, compute the sample autocorrelation at lags $\\ell = 1, 2, \\dots, L$:\n  1. Compute the sample mean $\\bar{e} = \\frac{1}{N} \\sum_{k=1}^{N} e_k$.\n  2. Define the demeaned sequence $d_k = e_k - \\bar{e}$.\n  3. For each lag $\\ell$, compute the normalized sample autocorrelation\n     $$r[\\ell] = \\frac{\\sum_{k=\\ell+1}^{N} d_k \\, d_{k-\\ell}}{\\sum_{k=1}^{N} d_k^2}.$$\n- Under the null hypothesis of whiteness for a large sample, each $r[\\ell]$ is approximately distributed as a zero-mean variable with standard deviation $1/\\sqrt{N}$. Use the two-sided $95\\%$ criterion $|r[\\ell]| \\leq \\frac{1.96}{\\sqrt{N}}$ for all $\\ell \\in \\{1,\\dots,L\\}$ to accept whiteness. Implement the test that returns a boolean: true if whiteness is accepted, false otherwise.\n\nImplementation details to respect:\n- Use the KF designed from linear-Gaussian prediction and correction: at each step perform a prediction using the assumed $(F, Q)$ and an update with $(H, R)$, forming $e_k$ before the update.\n- Use a an initial Gaussian prior $\\hat{x}_{0|0} \\in \\mathbb{R}^2$ and covariance $P_{0|0} \\in \\mathbb{R}^{2 \\times 2}$.\n- Use a fixed random seed so results are reproducible.\n\nTest suite and parameters:\nUse time step $\\Delta t = 1.0$, horizon $N = 500$, and lags $L = 10$. Use\n$$F = \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0.5 \\\\ 1.0 \\end{bmatrix}.$$\nUse the true process noise covariance and measurement noise variance\n$$Q_{\\text{true}} = \\mathrm{diag}(10^{-4}, 10^{-5}), \\quad R_{\\text{true}} = 10^{-2}.$$\nUse the prior\n$$\\hat{x}_{0|0} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad P_{0|0} = I_{2}.$$\nSet the random seed to $0$.\n\nProvide three test cases that exercise different behaviors:\n\n1) Matched model (happy path):\n- True input $u = 0$.\n- Assumed model in the KF: $(F, Q, H, R)$ equal to $(F, Q_{\\text{true}}, H, R_{\\text{true}})$ and no input modeled.\n- Expected outcome: the innovations should be white.\n\n2) Model mismatch via unmodeled constant acceleration (colored innovations):\n- True input $u = a_0$ with $a_0 = 0.2$ (constant for all time).\n- The true state evolves with the nonzero input via $B u$.\n- The KF assumes no input (uses the same $(F, H)$ as above but with no $B u$ term), and uses $(Q, R) = (Q_{\\text{true}}, R_{\\text{true}})$.\n- Expected outcome: the innovations should be colored and the whiteness test should fail.\n\n3) Model mismatch via severely underestimated process noise (colored innovations):\n- True input $u = 0$.\n- KF uses $(F, H)$ as above, and $(Q, R) = (Q_{\\text{assumed}}, R_{\\text{true}})$ with\n  $$Q_{\\text{assumed}} = \\mathrm{diag}(10^{-8}, 10^{-9}).$$\n- Expected outcome: the innovations should be colored and the whiteness test should fail.\n\nProgramming task and required output:\n- Implement a program that simulates the true system for each test case, runs the KF with the specified assumed parameters, computes the innovations sequence $\\{e_k\\}$, evaluates the whiteness test using $L$ and the $|r[\\ell]| \\le \\frac{1.96}{\\sqrt{N}}$ criterion, and records a boolean result for each case.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the three test cases specified above. For example, an output like $[ \\text{true}, \\text{false}, \\text{false} ]$ but with exact Python boolean literals and no spaces, i.e., $[\\text{True},\\text{False},\\text{False}]$.", "solution": "The problem requires the implementation of a Kalman filter (KF) from its foundational principles to test the whiteness of its innovations sequence. This test serves as a diagnostic tool for the filter's performance and the correctness of the underlying state-space model. The solution will first establish the theoretical basis for the KF within a linear-Gaussian framework, then detail the whiteness test, and finally apply these to the specified test cases.\n\n### The Linear-Gaussian State-Space Model\n\nThe system is described by a discrete-time, linear, time-invariant state-space model. The evolution of the hidden state and the generation of observations are governed by two equations:\n\n1.  **State Equation**: The true state of the system at time step $k$, denoted by the vector $x_k \\in \\mathbb{R}^2$, evolves according to a linear stochastic difference equation:\n    $$x_k = F x_{k-1} + B u + w_{k-1}$$\n    Here, $F \\in \\mathbb{R}^{2 \\times 2}$ is the state transition matrix, $B \\in \\mathbb{R}^{2 \\times 1}$ is the input matrix, $u \\in \\mathbb{R}$ is a constant control input, and $w_{k-1}$ is the process noise. The process noise is assumed to be a sequence of independent and identically distributed (i.i.d.) Gaussian random vectors with zero mean and covariance matrix $Q_{\\text{true}}$, i.e., $w_{k-1} \\sim \\mathcal{N}(0, Q_{\\text{true}})$.\n\n2.  **Measurement Equation**: The observation at time step $k$, a scalar $z_k \\in \\mathbb{R}$, is a linear projection of the true state corrupted by noise:\n    $$z_k = H x_k + v_k$$\n    Here, $H \\in \\mathbb{R}^{1 \\times 2}$ is the observation matrix, and $v_k$ is the measurement noise. The measurement noise is assumed to be a sequence of i.i.d. Gaussian random variables with zero mean and variance $R_{\\text{true}}$, i.e., $v_k \\sim \\mathcal{N}(0, R_{\\text{true}})$. The noise sequences $\\{w_k\\}$ and $\\{v_k\\}$ are mutually independent.\n\nThe core of the estimation problem is to infer the sequence of hidden states $\\{x_k\\}$ given the sequence of noisy measurements $\\{z_k\\}$.\n\n### The Kalman Filter from First Principles\n\nThe Kalman filter is the optimal estimator for linear-Gaussian systems in the sense that it minimizes the mean squared error (MMSE) of the state estimate. The filter operates recursively, processing one measurement at a time. Its derivation relies on the property that for jointly Gaussian random variables, the conditional distribution remains Gaussian.\n\nLet the conditional probability density of the state $x_k$ given all measurements up to time $j$ be denoted by $p(x_k | z_{1:j})$. The Kalman filter computes the exact posterior distribution $p(x_k | z_{1:k})$ at each step, which is fully characterized by its mean (the state estimate) and its covariance. The recursive process consists of two stages: prediction and correction.\n\nAssume at the end of step $k-1$, we have the posterior (filtered) distribution of the state $x_{k-1}$:\n$$x_{k-1} | z_{1:k-1} \\sim \\mathcal{N}(\\hat{x}_{k-1|k-1}, P_{k-1|k-1})$$\nwhere $\\hat{x}_{k-1|k-1}$ is the state estimate and $P_{k-1|k-1}$ is the error covariance matrix.\n\n#### 1. Prediction (Time Update)\n\nThe prediction step projects the state and covariance estimates forward from time $k-1$ to time $k$, before the measurement $z_k$ is incorporated. The goal is to compute the prior distribution for step $k$, $p(x_k | z_{1:k-1})$.\n\n-   **Predicted State Mean**: Using the linearity of the expectation operator on the state equation (where the KF's model of dynamics is used, which might differ from the true dynamics):\n    $$\\hat{x}_{k|k-1} = \\mathbb{E}[F x_{k-1} + w_{k-1} | z_{1:k-1}] = F \\mathbb{E}[x_{k-1} | z_{1:k-1}] + \\mathbb{E}[w_{k-1}]$$\n    Given $\\mathbb{E}[x_{k-1} | z_{1:k-1}] = \\hat{x}_{k-1|k-1}$ and $\\mathbb{E}[w_{k-1}] = 0$, the predicted state mean is:\n    $$\\hat{x}_{k|k-1} = F \\hat{x}_{k-1|k-1}$$\n    Note that the control input term $B u$ is omitted here, as per the problem specification for the KF's assumed model in all test cases.\n\n-   **Predicted Error Covariance**: The covariance of the predicted state is found by propagating the covariance from the previous step through the dynamics model. The state prediction error is $x_k - \\hat{x}_{k|k-1} = F(x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1}$.\n    $$P_{k|k-1} = \\text{Cov}(x_k | z_{1:k-1}) = \\mathbb{E}[(x_k - \\hat{x}_{k|k-1})(x_k - \\hat{x}_{k|k-1})^T | z_{1:k-1}]$$\n    Since the error in the previous estimate $(x_{k-1} - \\hat{x}_{k-1|k-1})$ is independent of the process noise $w_{k-1}$, the covariance is:\n    $$P_{k|k-1} = \\text{Cov}(F(x_{k-1} - \\hat{x}_{k-1|k-1})) + \\text{Cov}(w_{k-1}) = F P_{k-1|k-1} F^T + Q$$\n    where $Q$ is the process noise covariance matrix assumed by the filter.\n\nThe predicted distribution is thus $x_k | z_{1:k-1} \\sim \\mathcal{N}(\\hat{x}_{k|k-1}, P_{k|k-1})$.\n\n#### 2. Correction (Measurement Update)\n\nThe correction step refines the predicted estimate by incorporating the new measurement $z_k$. This is a Bayesian update. We have the prior on $x_k$ and a likelihood from the measurement model.\n\n-   **Innovation**: The innovation, or measurement residual, $e_k$, is the difference between the actual measurement $z_k$ and its prediction based on the model and prior state estimate:\n    $$e_k = z_k - \\mathbb{E}[z_k | z_{1:k-1}] = z_k - \\mathbb{E}[H x_k + v_k | z_{1:k-1}] = z_k - H \\hat{x}_{k|k-1}$$\n\n-   **Innovation Covariance**: The covariance of the innovation, $S_k$, is:\n    $$S_k = \\text{Cov}(e_k) = \\text{Cov}(H(x_k - \\hat{x}_{k|k-1}) + v_k)$$\n    Since the state prediction error and measurement noise $v_k$ are independent:\n    $$S_k = H \\text{Cov}(x_k - \\hat{x}_{k|k-1}) H^T + \\text{Cov}(v_k) = H P_{k|k-1} H^T + R$$\n    where $R$ is the measurement noise covariance assumed by the filter.\n\n-   **Optimal Kalman Gain**: The updated state estimate is a linear combination of the predicted estimate and the innovation. The weighting factor that minimizes the posterior error covariance is the Kalman gain, $K_k$. It is derived from the cross-covariance between the state and the measurement.\n    $$K_k = \\text{Cov}(x_k, z_k | z_{1:k-1}) S_k^{-1} = \\text{Cov}(x_k, H x_k + v_k) S_k^{-1}$$\n    $$K_k = \\text{Cov}(x_k, x_k H^T) S_k^{-1} = P_{k|k-1} H^T S_k^{-1}$$\n\n-   **Updated State Mean**: The posterior state estimate is the prior estimate corrected by the innovation, weighted by the Kalman gain:\n    $$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k e_k$$\n\n-   **Updated Error Covariance**: The posterior error covariance is reduced from the prior covariance:\n    $$P_{k|k} = P_{k|k-1} - K_k S_k K_k^T = (I - K_k H) P_{k|k-1}$$\n    The latter form, known as the Joseph form, is often preferred for its numerical stability.\n\nThe corrected (filtered) distribution is $x_k | z_{1:k} \\sim \\mathcal{N}(\\hat{x}_{k|k}, P_{k|k})$, completing the recursive cycle.\n\n### The Innovations Whiteness Test\n\nA fundamental property of the optimal Kalman filter is that if the model parameters $(F, H, Q, R)$ are correctly specified and the noise processes are indeed Gaussian and white, the resulting innovations sequence $\\{e_k\\}$ is a zero-mean, uncorrelated (white) Gaussian sequence. Any deviation from this property, such as non-zero mean or temporal correlation, indicates a mismatch between the filter's model and the true underlying process.\n\nThe problem requires a statistical test for the whiteness of a finite innovations sequence $\\{e_k\\}_{k=1}^{N}$. This is done by examining its sample autocorrelation function.\n\n1.  First, the sample mean is computed: $\\bar{e} = \\frac{1}{N} \\sum_{k=1}^{N} e_k$.\n2.  The sequence is demeaned: $d_k = e_k - \\bar{e}$.\n3.  The normalized sample autocorrelation for a time lag $\\ell \\in \\{1, 2, \\dots, L\\}$ is computed as:\n    $$r[\\ell] = \\frac{\\sum_{k=\\ell+1}^{N} d_k \\, d_{k-\\ell}}{\\sum_{k=1}^{N} d_k^2}$$\n4.  Under the null hypothesis that the sequence is white noise, for a large sample size $N$, each $r[\\ell]$ is approximately distributed as $\\mathcal{N}(0, 1/N)$. A common statistical test rejects the whiteness hypothesis if any autocorrelation coefficient falls outside a confidence interval. Using a $95\\%$ confidence level, the test is:\n    Accept whiteness if $|r[\\ell]| \\le \\frac{1.96}{\\sqrt{N}}$ for all $\\ell = 1, \\dots, L$. Otherwise, reject whiteness.\n\n### Analysis of Test Cases\n\nThe three test cases are designed to probe the filter's performance under different conditions:\n\n1.  **Matched Model**: The KF uses the true model parameters $(F, Q_{\\text{true}}, H, R_{\\text{true}})$ and the true system has no input ($u=0$). In this \"happy path\" scenario, the KF is theoretically optimal. The innovations sequence $\\{e_k\\}$ is expected to be white, and the whiteness test should pass.\n\n2.  **Unmodeled Constant Acceleration**: The true system is subject to a constant input $u=a_0=0.2$, representing a constant acceleration. The KF, however, is not configured to account for this input. This model mismatch will introduce a systematic bias in the state prediction. The error will grow over time, causing the innovations to be non-zero mean and highly correlated. The whiteness test is expected to fail.\n\n3.  **Severely Underestimated Process Noise**: The KF uses a process noise covariance $Q_{\\text{assumed}}$ that is orders of magnitude smaller than the true one, $Q_{\\text{true}}$. This makes the filter overly confident in its own predictions based on the model dynamics. It will be slow to react to the random perturbations of the true state, as it will attribute most of the innovation to measurement noise rather than state error. This sluggish response leads to errors that persist over several time steps, resulting in a strongly autocorrelated innovations sequence. The whiteness test is expected to fail.\n\nThe program will execute these three scenarios, collecting the innovations from each and applying the whiteness test to produce a boolean result.", "answer": "```python\nimport numpy as np\n\ndef simulate_system(F, B, H, Q_true, R_true, x0_true, u_true, N):\n    \"\"\"\n    Simulates the true linear state-space system.\n    Model: x_k = F @ x_{k-1} + B @ u + w_{k-1}\n           z_k = H @ x_k + v_k\n    \"\"\"\n    n_states = F.shape[0]\n    x_true = np.zeros((N + 1, n_states, 1))\n    z_obs = np.zeros((N, 1))\n    x_true[0] = x0_true\n\n    process_noise = np.random.multivariate_normal(np.zeros(n_states), Q_true, size=N)\n    measurement_noise = np.random.normal(0, np.sqrt(R_true), size=N)\n\n    for k in range(N):\n        # State evolution uses noise w_{k-1} (indexed as k here) to get x_k\n        w_km1 = process_noise[k].reshape(n_states, 1)\n        x_true[k + 1] = F @ x_true[k] + B * u_true + w_km1\n        \n        # Observation z_k corresponds to state x_k\n        v_k = measurement_noise[k]\n        z_obs[k] = H @ x_true[k+1] + v_k\n\n    return z_obs\n\ndef kalman_filter(z_obs, F, H, Q_kf, R_kf, x0_hat, P0):\n    \"\"\"\n    Runs the Kalman Filter and returns the innovations sequence.\n    \"\"\"\n    N = len(z_obs)\n    n_states = F.shape[0]\n    \n    x_hat = x0_hat\n    P = P0\n    \n    innovations = np.zeros(N)\n\n    for k in range(N):\n        # --- Prediction (Time Update) ---\n        # x_hat_{k|k-1} = F @ x_hat_{k-1|k-1}\n        x_hat_pred = F @ x_hat\n        # P_{k|k-1} = F @ P_{k-1|k-1} @ F.T + Q\n        P_pred = F @ P @ F.T + Q_kf\n        \n        # --- Innovation Calculation ---\n        # e_k = z_k - H @ x_hat_{k|k-1}\n        z_k = z_obs[k]\n        e_k = z_k - (H @ x_hat_pred)\n        innovations[k] = e_k.item()\n\n        # --- Correction (Measurement Update) ---\n        # S_k = H @ P_{k|k-1} @ H.T + R\n        S_k = H @ P_pred @ H.T + R_kf\n        # K_k = P_{k|k-1} @ H.T @ S_k^-1\n        K_k = P_pred @ H.T / S_k # S_k is scalar\n        \n        # x_hat_{k|k} = x_hat_{k|k-1} + K_k @ e_k\n        x_hat = x_hat_pred + K_k * e_k\n        # P_{k|k} = (I - K_k @ H) @ P_{k|k-1}\n        I = np.eye(n_states)\n        P = (I - K_k @ H) @ P_pred\n        \n    return innovations\n\ndef whiteness_test(innovations, L, N):\n    \"\"\"\n    Performs the whiteness test on the innovations sequence.\n    \"\"\"\n    # 1. Compute sample mean\n    e_bar = np.mean(innovations)\n    \n    # 2. Demean the sequence\n    d = innovations - e_bar\n    \n    # Denominator for normalization\n    denom = np.sum(d**2)\n    if denom == 0:\n        return True # Avoid division by zero; no variance means white\n\n    # 3. Compute normalized sample autocorrelation for lags l = 1..L\n    threshold = 1.96 / np.sqrt(N)\n    \n    for l in range(1, L + 1):\n        # Numerator: sum_{k=l+1 to N} d_k * d_{k-l}\n        # Using numpy slicing for efficiency: d[l:] corresponds to d_k for k>l\n        # d[:-l] corresponds to d_{k-l}\n        num = np.sum(d[l:] * d[:-l])\n        \n        r_l = num / denom\n        \n        # 4. Check against threshold\n        if np.abs(r_l) > threshold:\n            return False # Whiteness test fails\n            \n    return True # Whiteness test passes for all lags\n\ndef solve():\n    \"\"\"\n    Main function to run the three test cases and print results.\n    \"\"\"\n    # --- Global Parameters ---\n    np.random.seed(0)\n    \n    dt = 1.0\n    N = 500\n    L = 10\n    \n    F = np.array([[1.0, dt], [0.0, 1.0]])\n    H = np.array([[1.0, 0.0]])\n    B = np.array([[0.5 * dt**2], [dt]])\n    \n    Q_true = np.diag([1e-4, 1e-5])\n    R_true = 1e-2\n    \n    x0_hat = np.array([[0.0], [1.0]])\n    P0 = np.eye(2)\n    x0_true = np.array([[0.0], [1.0]])\n    \n    # --- Test Cases Configuration ---\n    test_cases = [\n        # Case 1: Matched model\n        {\n            \"name\": \"Matched\",\n            \"u_true\": 0.0,\n            \"Q_kf\": Q_true,\n            \"R_kf\": R_true\n        },\n        # Case 2: Unmodeled constant acceleration\n        {\n            \"name\": \"Unmodeled Input\",\n            \"u_true\": 0.2,\n            \"Q_kf\": Q_true,\n            \"R_kf\": R_true\n        },\n        # Case 3: Underestimated process noise\n        {\n            \"name\": \"Underestimated Noise\",\n            \"u_true\": 0.0,\n            \"Q_kf\": np.diag([1e-8, 1e-9]),\n            \"R_kf\": R_true\n        }\n    ]\n    \n    results = []\n    \n    # Simulate true observations for Case 1  3 (u_true=0)\n    z_obs_u0 = simulate_system(F, B, H, Q_true, R_true, x0_true, u_true=0.0, N=N)\n    \n    # Simulate true observations for Case 2 (u_true=0.2)\n    z_obs_u_const = simulate_system(F, B, H, Q_true, R_true, x0_true, u_true=0.2, N=N)\n\n    # --- Run KF and Whiteness Test for each case ---\n    \n    # Case 1\n    case1_params = test_cases[0]\n    innovations_1 = kalman_filter(z_obs_u0, F, H, case1_params[\"Q_kf\"], case1_params[\"R_kf\"], x0_hat, P0)\n    result_1 = whiteness_test(innovations_1, L, N)\n    results.append(result_1)\n\n    # Case 2\n    case2_params = test_cases[1]\n    innovations_2 = kalman_filter(z_obs_u_const, F, H, case2_params[\"Q_kf\"], case2_params[\"R_kf\"], x0_hat, P0)\n    result_2 = whiteness_test(innovations_2, L, N)\n    results.append(result_2)\n\n    # Case 3\n    case3_params = test_cases[2]\n    innovations_3 = kalman_filter(z_obs_u0, F, H, case3_params[\"Q_kf\"], case3_params[\"R_kf\"], x0_hat, P0)\n    result_3 = whiteness_test(innovations_3, L, N)\n    results.append(result_3)\n    \n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3149135"}, {"introduction": "Can a Kalman filter estimate something it cannot see? This advanced practice explores the crucial concept of observability. You will investigate how the filter's uncertainty evolves for state components that are not directly measured. Through simulation, you will discover how the system's underlying dynamics can either allow uncertainty to grow infinitely or, remarkably, constrain it by coupling unmeasured states to observed ones. [@problem_id:3149199]", "problem": "Consider a discrete-time, linear, time-invariant, Gaussian state-space model with state vector $x_k \\in \\mathbb{R}^n$ and measurement $z_k \\in \\mathbb{R}^m$ given by the relations\n$$\nx_{k} = A x_{k-1} + w_{k-1}, \\quad z_k = H x_k + v_k,\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is the state transition matrix, $H \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, $w_{k-1} \\sim \\mathcal{N}(0,Q)$ is the process noise with covariance $Q \\in \\mathbb{R}^{n \\times n}$, and $v_k \\sim \\mathcal{N}(0,R)$ is the measurement noise with covariance $R \\in \\mathbb{R}^{m \\times m}$. Assume $w_{k-1}$ and $v_k$ are independent and independent across time, and the initial state has Gaussian prior $x_0 \\sim \\mathcal{N}(\\mu_0,P_0)$ with mean $\\mu_0 \\in \\mathbb{R}^n$ and positive semi-definite covariance $P_0 \\in \\mathbb{R}^{n \\times n}$.\n\nYour task is to analyze how the uncertainty evolves in subspaces that are not directly measured when the measurement matrix $H$ is rank-deficient (non-invertible), especially when the dynamics matrix $A$ is marginally stable (all eigenvalues on the unit circle with no eigenvalue of magnitude strictly greater than one). Specifically:\n\n- Starting from the fundamental definition of a linear Gaussian model and the rule for conditioning a joint Gaussian distribution, derive the recursive update for the state mean and covariance when processing measurements sequentially. Do not assume or quote any pre-derived recursion; instead, construct it from first principles of multivariate Gaussian conditioning and the law of total expectation and variance.\n- Define the unmeasured subspace at time index $k$ as the null space of $H$, that is, the set of all $u \\in \\mathbb{R}^n$ such that $H u = 0$. Use an orthonormal basis $W \\in \\mathbb{R}^{n \\times r}$ for this subspace, where $r = n - \\mathrm{rank}(H)$. Quantify the posterior uncertainty “confined” to the unmeasured subspace after each measurement as the scalar\n$$\n\\sigma_{k}^2 = \\mathrm{trace}\\!\\left(W^\\top P_{k|k} W\\right),\n$$\nwhere $P_{k|k}$ is the state covariance after incorporating the measurement at time $k$.\n- Using the derived recursion, implement a program that, for each provided test case, iterates the filter for a specified number of steps and returns the final subspace uncertainty $\\sigma_{N}^2$ at step $N$.\n\nAll quantities are dimensionless; no physical units are involved.\n\nTest suite to implement and evaluate:\n\n- Case $1$ (happy path with rank-deficient measurement and identity dynamics):\n  - $n = 2$, $m = 1$,\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $H = \\begin{bmatrix} 1  0 \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 0.04 \\end{bmatrix}$,\n  - $\\mu_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $N = 50$.\n- Case $2$ (rank-deficient measurement with marginally stable rotation dynamics that couple measured and unmeasured directions over time):\n  - $n = 2$, $m = 1$,\n  - $A = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$,\n  - $H = \\begin{bmatrix} 1  0 \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 0.04 \\end{bmatrix}$,\n  - $\\mu_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $N = 200$.\n- Case $3$ (boundary case with no process noise in the unmeasured direction):\n  - $n = 2$, $m = 1$,\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $H = \\begin{bmatrix} 1  0 \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 0.01  0 \\\\ 0  0 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 0.04 \\end{bmatrix}$,\n  - $\\mu_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $N = 50$.\n\nProgram requirements:\n\n- Implement the sequential filtering recursion derived from the fundamental Gaussian conditioning principle. Use numerically stable linear algebra where appropriate.\n- For each test case, compute the final posterior covariance $P_{N|N}$ and then compute $\\sigma_{N}^2 = \\mathrm{trace}\\!\\left(W^\\top P_{N|N} W\\right)$, where $W$ is any orthonormal basis for $\\mathrm{null}(H)$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the test cases, where each element is $\\sigma_{N}^2$ rounded to six decimal places, that is, $[\\sigma_{N,1}^2,\\sigma_{N,2}^2,\\sigma_{N,3}^2]$.", "solution": "This problem requires deriving the Kalman filter's recursive update equations from the fundamental properties of multivariate Gaussian distributions and then applying them to study the evolution of uncertainty in an unmeasured subspace.\n\n### Derivation of the Recursive Estimation Equations\n\nThe Kalman filter operates in a two-step cycle: prediction and update. Let the posterior distribution of the state at time $k-1$, given all measurements up to that time ($Z_{k-1}$), be Gaussian: $x_{k-1} | Z_{k-1} \\sim \\mathcal{N}(\\mu_{k-1|k-1}, P_{k-1|k-1})$.\n\n**1. Prediction Step**\nThe goal is to determine the distribution of the state $x_k$ at time $k$, given measurements up to $k-1$. This is the prior distribution for step $k$. Using the state evolution model $x_k = A x_{k-1} + w_{k-1}$:\n- The predicted mean $\\mu_{k|k-1}$ is found by the law of total expectation:\n  $$ \\mu_{k|k-1} = \\mathbb{E}[x_k | Z_{k-1}] = \\mathbb{E}[A x_{k-1} + w_{k-1} | Z_{k-1}] = A \\mu_{k-1|k-1} $$\n- The predicted covariance $P_{k|k-1}$ is found by the law of total variance. Since $x_{k-1}$ and $w_{k-1}$ are independent:\n  $$ P_{k|k-1} = \\mathrm{Cov}(x_k | Z_{k-1}) = A \\mathrm{Cov}(x_{k-1} | Z_{k-1}) A^\\top + \\mathrm{Cov}(w_{k-1}) = A P_{k-1|k-1} A^\\top + Q $$\nThe predicted distribution is thus $x_k | Z_{k-1} \\sim \\mathcal{N}(\\mu_{k|k-1}, P_{k|k-1})$.\n\n**2. Update Step**\nThe update step incorporates the new measurement $z_k = H x_k + v_k$ to refine the prediction. This is achieved by conditioning on $z_k$. The key is that for a linear Gaussian system, the joint distribution of the state $x_k$ and measurement $z_k$ (given past data $Z_{k-1}$) is also Gaussian. We find its parameters:\n- The joint mean is $\\begin{pmatrix} \\mu_{k|k-1} \\\\ H \\mu_{k|k-1} \\end{pmatrix}$.\n- The joint covariance matrix is $\\begin{pmatrix} P_{k|k-1}  P_{k|k-1} H^\\top \\\\ H P_{k|k-1}  H P_{k|k-1} H^\\top + R \\end{pmatrix}$. The bottom-right term, $S_k = H P_{k|k-1} H^\\top + R$, is the innovation covariance.\n\nUsing the standard formula for conditioning a partitioned Gaussian distribution, the posterior distribution of $x_k$ given $z_k$ (and $Z_{k-1}$) is $\\mathcal{N}(\\mu_{k|k}, P_{k|k})$, where:\n- The posterior mean is $\\mu_{k|k} = \\mu_{k|k-1} + K_k (z_k - H \\mu_{k|k-1})$.\n- The posterior covariance is $P_{k|k} = P_{k|k-1} - K_k S_k K_k^\\top = (I - K_k H) P_{k|k-1}$.\nThe term $K_k = P_{k|k-1} H^\\top S_k^{-1}$ is the optimal Kalman gain.\n\n### Subspace Uncertainty and Test Case Analysis\n\nThe problem asks for the evolution of the posterior covariance $P_{k|k}$ itself. Since the covariance update equations do not depend on the specific measurement values $z_k$, we can iterate them deterministically starting from $P_0$. The uncertainty in the unmeasured subspace is $\\sigma_k^2 = \\mathrm{trace}(W^\\top P_{k|k} W)$, where the columns of $W$ form an orthonormal basis for the null space of $H$.\n\n**Analysis of Test Cases:**\n- **Case 1**: $A=I$ and $H=[1, 0]$. The two state components evolve independently. The first component is measured, so its uncertainty will decrease. The second component is unmeasured and resides in the null space of $H$. Its uncertainty is not reduced by measurements. Since it is driven by process noise ($Q_{22}=0.01$), its variance will grow with each step. We expect $\\sigma_N^2 \\approx (P_0)_{22} + N \\cdot Q_{22} = 1 + 50 \\times 0.01 = 1.5$.\n- **Case 2**: $A$ is a rotation matrix. Although $H$ only measures the first component at any given time, the dynamics rotate the state. The unmeasured second component at step $k$ becomes the measured first component at step $k+1$. This coupling makes the system fully observable. Information from measurements flows into all state components, so we expect the total uncertainty, including that in any fixed subspace, to converge to a finite steady-state value.\n- **Case 3**: $A=I$ and $H=[1, 0]$, but the process noise for the unmeasured second component is zero ($Q_{22}=0$). The uncertainty in the second component is not reduced by measurement (as in Case 1) but is also not increased by process noise. Therefore, its variance should remain constant at its initial value, $(P_0)_{22} = 1$. We expect $\\sigma_N^2 = 1.0$.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It iterates through each case, simulates the Kalman filter covariance evolution,\n    and computes the final uncertainty in the unmeasured subspace.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            # Case 1: Identity dynamics, rank-deficient measurement\n            \"n\": 2, \"m\": 1,\n            \"A\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"Q\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"R\": np.array([[0.04]]),\n            \"P0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"N\": 50,\n        },\n        {\n            # Case 2: Rotation dynamics, coupling measured/unmeasured states\n            \"n\": 2, \"m\": 1,\n            \"A\": np.array([[0.0, -1.0], [1.0, 0.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"Q\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"R\": np.array([[0.04]]),\n            \"P0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"N\": 200,\n        },\n        {\n            # Case 3: Identity dynamics, no process noise in unmeasured direction\n            \"n\": 2, \"m\": 1,\n            \"A\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"Q\": np.array([[0.01, 0.0], [0.0, 0.0]]),\n            \"R\": np.array([[0.04]]),\n            \"P0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"N\": 50,\n        }\n    ]\n\n    def run_filter_covariance(A, H, Q, R, P0, N):\n        \"\"\"\n        Iterates the Kalman filter covariance equations for N steps.\n\n        Args:\n            A (np.ndarray): State transition matrix.\n            H (np.ndarray): Measurement matrix.\n            Q (np.ndarray): Process noise covariance.\n            R (np.ndarray): Measurement noise covariance.\n            P0 (np.ndarray): Initial state covariance.\n            N (int): Number of steps to iterate.\n\n        Returns:\n            np.ndarray: The final posterior covariance P_N|N.\n        \"\"\"\n        n = A.shape[0]\n        I = np.eye(n)\n        P_kk = P0\n        \n        for _ in range(N):\n            # Prediction step for covariance\n            P_k_km1 = A @ P_kk @ A.T + Q\n            \n            # Update step for covariance\n            # Innovation covariance\n            S_k = H @ P_k_km1 @ H.T + R\n            # Kalman gain\n            K_k = P_k_km1 @ H.T @ np.linalg.inv(S_k)\n            # Posterior covariance update (Joseph form for stability)\n            P_kk = (I - K_k @ H) @ P_k_km1\n            \n        return P_kk\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current case\n        A, H, Q, R, P0, N = case[\"A\"], case[\"H\"], case[\"Q\"], case[\"R\"], case[\"P0\"], case[\"N\"]\n        \n        # 1. Run the covariance filter to get the final posterior covariance P_N|N\n        P_NN = run_filter_covariance(A, H, Q, R, P0, N)\n        \n        # 2. Find an orthonormal basis for the null space of H\n        # The columns of W form the basis.\n        W = linalg.null_space(H)\n        \n        # 3. Compute the uncertainty in the unmeasured subspace\n        # This is trace(W^T * P_NN * W)\n        if W.shape[1] == 0: # Check if null space is empty\n             sigma_N_sq = 0.0\n        else:\n             sigma_N_sq = np.trace(W.T @ P_NN @ W)\n        \n        results.append(sigma_N_sq)\n    \n    # Format the final output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3149199"}]}