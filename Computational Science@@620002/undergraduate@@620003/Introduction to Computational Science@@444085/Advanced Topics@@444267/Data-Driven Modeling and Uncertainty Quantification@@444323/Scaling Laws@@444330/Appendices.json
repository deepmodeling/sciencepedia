{"hands_on_practices": [{"introduction": "Understanding the performance of different algorithms is a cornerstone of computational science. This practice explores the concept of algorithmic scaling by comparing three methods for multiplying large integers. You will move beyond simple asymptotic notation to a more realistic model that includes constant factors, allowing you to determine the practical \"crossover points\" where a more complex but asymptotically faster algorithm, like one based on the Fast Fourier Transform (FFT), actually becomes more efficient than simpler methods [@problem_id:3190117].", "problem": "You are comparing three algorithms for multiplying big integers: a digit-by-digit schoolbook method, a divide-and-conquer Karatsuba method, and a Fast Fourier Transform (FFT)-based method. The goal is to determine the smallest integer size at which the FFT-based method becomes faster than both competitors, under realistic scaling-law models with explicit constant factors.\n\nFundamental base for modeling:\n- The asymptotic rate of the schoolbook method scales like $n^2$.\n- The asymptotic rate of the Karatsuba method scales like $n^{\\log_2 3}$.\n- The asymptotic rate of the FFT-based method scales like $n \\log_2 n$.\n- For empirical realism, constant factors and lower-order linear terms must be included.\n\nAssume the following time models measured in arbitrary but consistent cost units:\n- Schoolbook: $$T_{\\mathrm{S}}(n) = a_{\\mathrm{S}} \\, n^2.$$\n- Karatsuba: $$T_{\\mathrm{K}}(n) = a_{\\mathrm{K}} \\, n^{\\alpha} + b_{\\mathrm{K}} \\, n,$$ where $\\alpha = \\log_2 3$.\n- FFT-based: $$T_{\\mathrm{F}}(n) = a_{\\mathrm{F}} \\, n \\log_2 n + b_{\\mathrm{F}} \\, n.$$\n\nTask:\n- For each parameter set in the test suite, compute the smallest integer $n$ in the inclusive range $[n_{\\min}, n_{\\max}]$ such that both inequalities hold:\n  $$T_{\\mathrm{F}}(n) \\le T_{\\mathrm{S}}(n) \\quad \\text{and} \\quad T_{\\mathrm{F}}(n) \\le T_{\\mathrm{K}}(n).$$\n- If no such integer exists in the range for a given parameter set, output $-1$ for that set.\n\nRequirements and conventions:\n- Use base-$2$ logarithms for every $\\log_2$.\n- Treat $n$ as a dimensionless integer representing operand length (for example, digits or machine limbs). No physical units are involved.\n- At $n = 1$, define $\\log_2 1 = 0$ and evaluate the models directly.\n- The output for each test case is a single integer.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $$[n_1,n_2,n_3,n_4,n_5].$$\n\nTest suite:\nProvide results for the following parameter sets, each given as $(a_{\\mathrm{S}}, a_{\\mathrm{K}}, a_{\\mathrm{F}}, b_{\\mathrm{K}}, b_{\\mathrm{F}}, n_{\\min}, n_{\\max})$.\n\n1. $ (\\,1,\\,5,\\,200,\\,0,\\,1000,\\,1,\\,10^9\\,) $\n2. $ (\\,1000,\\,1000,\\,1,\\,0,\\,0,\\,1,\\,10^6\\,) $\n3. $ (\\,1,\\,1,\\,10^7,\\,0,\\,0,\\,1,\\,10^6\\,) $\n4. Choose constants to create a boundary tie at $n = 64$:\n   - $ a_{\\mathrm{F}} = 1 $, $ b_{\\mathrm{F}} = 0 $,\n   - $ a_{\\mathrm{S}} = \\dfrac{\\log_2 64}{64} $,\n   - $ a_{\\mathrm{K}} = 64^{\\,1 - \\log_2 3} \\cdot \\log_2 64 $,\n   - $ b_{\\mathrm{K}} = 0 $,\n   - $ n_{\\min} = 1 $, $ n_{\\max} = 10^6 $.\n   Thus the tuple is $ \\left(\\,\\dfrac{\\log_2 64}{64},\\,64^{\\,1 - \\log_2 3} \\cdot \\log_2 64,\\,1,\\,0,\\,0,\\,1,\\,10^6\\,\\right) $.\n5. $ (\\,0.5,\\,0.2,\\,10,\\,10000,\\,5000,\\,1,\\,10^7\\,) $\n\nYour program must implement a robust method to find the smallest integer $n$ satisfying the two inequalities or return $-1$ if none exist within the specified range. The final output must be a single line in the format described above, aggregating the results for all five cases.", "solution": "The problem presented is a well-posed exercise in computational science, specifically concerning the analysis of algorithmic scaling laws. It is scientifically grounded, objective, and contains all necessary information for a rigorous solution. The problem is deemed valid.\n\nThe central task is to determine the crossover point at which an FFT-based integer multiplication algorithm becomes more efficient than both a schoolbook multiplication algorithm and a Karatsuba multiplication algorithm. This point is defined as the smallest integer operand size, $n$, within a given range $[n_{\\min}, n_{\\max}]$, where the computational cost of the FFT-based method is less than or equal to the costs of its competitors.\n\nThe time complexity for each algorithm is modeled by the following functions, where $n$ represents the operand size (e.g., number of digits or limbs):\n\n1.  Schoolbook method: $T_{\\mathrm{S}}(n) = a_{\\mathrm{S}} \\, n^2$\n2.  Karatsuba method: $T_{\\mathrm{K}}(n) = a_{\\mathrm{K}} \\, n^{\\alpha} + b_{\\mathrm{K}} \\, n$, where the exponent $\\alpha = \\log_2 3 \\approx 1.585$.\n3.  FFT-based method: $T_{\\mathrm{F}}(n) = a_{\\mathrm{F}} \\, n \\log_2 n + b_{\\mathrm{F}} \\, n$. Note that for $n=1$, we adhere to the convention $\\log_2 1 = 0$.\n\nWe seek the smallest integer $n \\in [n_{\\min}, n_{\\max}]$ that satisfies the following system of inequalities:\n$$\n\\begin{cases} \nT_{\\mathrm{F}}(n) \\le T_{\\mathrm{S}}(n) \\\\\nT_{\\mathrm{F}}(n) \\le T_{\\mathrm{K}}(n) \n\\end{cases}\n$$\nIf no such integer exists within the specified range, the result for that parameter set is defined to be $-1$.\n\nTo analyze this system, we can define two difference functions:\n$$D_{\\mathrm{S}}(n) = T_{\\mathrm{S}}(n) - T_{\\mathrm{F}}(n) = a_{\\mathrm{S}} n^2 - (a_{\\mathrm{F}} n \\log_2 n + b_{\\mathrm{F}} n)$$\n$$D_{\\mathrm{K}}(n) = T_{\\mathrm{K}}(n) - T_{\\mathrm{F}}(n) = (a_{\\mathrm{K}} n^{\\alpha} + b_{\\mathrm{K}} n) - (a_{\\mathrm{F}} n \\log_2 n + b_{\\mathrm{F}} n)$$\nThe problem is now to find the smallest integer $n \\in [n_{\\min}, n_{\\max}]$ for which both $D_{\\mathrm{S}}(n) \\ge 0$ and $D_{\\mathrm{K}}(n) \\ge 0$.\n\nLet's examine the asymptotic behavior of these functions. The dominant terms are $a_{\\mathrm{S}} n^2$ in $D_{\\mathrm{S}}(n)$ and $a_{\\mathrm{K}} n^{\\alpha}$ in $D_{\\mathrm{K}}(n)$. Since the exponents $2$ and $\\alpha \\approx 1.585$ are both greater than $1$, the terms $n^2$ and $n^{\\alpha}$ grow asymptotically faster than the $n \\log_2 n$ term from $T_{\\mathrm{F}}(n)$. Assuming the leading coefficients $a_{\\mathrm{S}}$ and $a_{\\mathrm{K}}$ are positive, both difference functions, $D_{\\mathrm{S}}(n)$ and $D_{\\mathrm{K}}(n)$, will eventually become positive and remain positive for all larger $n$.\n\nThis behavior implies that the boolean condition $C(n) \\equiv (T_{\\mathrm{F}}(n) \\le T_{\\mathrm{S}}(n) \\land T_{\\mathrm{F}}(n) \\le T_{\\mathrm{K}}(n))$ will typically transition from false to true at a specific crossover point. For $n$ smaller than this point, the large constant factors or linear terms associated with the FFT-based method make it less efficient. For $n$ at or beyond this point, its superior asymptotic scaling prevails.\n\nThe search for this smallest integer $n$ must be efficient, as the range $[n_{\\min}, n_{\\max}]$ can be as large as $[1, 10^9]$. A linear search that evaluates $C(n)$ for each $n$ starting from $n_{\\min}$ is computationally infeasible.\n\nGiven the expected monotonic nature of the solution space (i.e., if $C(k)$ is true, $C(j)$ is likely true for all $j > k$), a binary search algorithm is the appropriate and efficient method to find the first integer $n$ for which $C(n)$ is true.\n\nThe algorithm proceeds as follows for each parameter set:\n1.  Define a boolean function `check(n)` that evaluates $C(n)$. All model calculations are performed using floating-point arithmetic to ensure precision with real-valued coefficients and function outputs.\n2.  An initial check is performed at $n=n_{\\max}$. If `check(n_max)` is false, it implies no solution exists in the range (assuming monotonicity), and the result is $-1$.\n3.  A second check is performed at $n=n_{\\min}$. If `check(n_min)` is true, then $n_{\\min}$ is the smallest integer satisfying the condition, and it is the answer.\n4.  If neither of the above conditions is met, a binary search is initiated on the integer range $[n_{\\min}, n_{\\max}]$. Let the search interval be $[L, R]$.\n5.  In each step of the search, a midpoint $M = L + \\lfloor(R-L)/2\\rfloor$ is chosen.\n    - If `check(M)` is true, it means $M$ is a potential solution. We record $M$ as the current best answer and continue searching for a smaller solution in the lower half of the interval, by setting $R = M-1$.\n    - If `check(M)` is false, $M$ is too small for the FFT-based method to be superior. The search must continue in the upper half of the interval, by setting $L = M+1$.\n6.  The search terminates when $L > R$. The last recorded answer is the smallest integer $n$ in the range that satisfies the conditions. If the loop completes without ever finding a valid $n$ (which is precluded by the initial check at $n_{\\max}$), the result would remain $-1$.\n\nThis binary search approach robustly and efficiently locates the precise integer crossover point, reducing the search space logarithmically.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the smallest integer n where an FFT-based multiplication algorithm\n    outperforms Schoolbook and Karatsuba methods for several parameter sets.\n    \"\"\"\n\n    # Define the exponent for the Karatsuba algorithm\n    alpha = np.log2(3)\n\n    # Test suite: (a_S, a_K, a_F, b_K, b_F, n_min, n_max)\n    test_cases = [\n        (1, 5, 200, 0, 1000, 1, 10**9),\n        (1000, 1000, 1, 0, 0, 1, 10**6),\n        (1, 1, 10**7, 0, 0, 1, 10**6),\n        (\n            np.log2(64) / 64,\n            64**(1 - np.log2(3)) * np.log2(64),\n            1,\n            0,\n            0,\n            1,\n            10**6\n        ),\n        (0.5, 0.2, 10, 10000, 5000, 1, 10**7),\n    ]\n\n    def find_crossover(params):\n        \"\"\"\n        Finds the smallest integer n in the given range [n_min, n_max]\n        that satisfies the crossover conditions, using binary search.\n        \"\"\"\n        a_S, a_K, a_F, b_K, b_F, n_min, n_max = params\n\n        def check(n):\n            \"\"\"\n            Checks if T_F(n) <= T_S(n) and T_F(n) <= T_K(n).\n            n is an integer, but calculations use floats for precision.\n            \"\"\"\n            if n <= 0:\n                return False\n            \n            n_float = float(n)\n            \n            # Handle log2(1) = 0 case\n            log2_n = 0.0 if n == 1 else np.log2(n_float)\n\n            # Calculate the cost for each algorithm\n            t_s = a_S * n_float**2\n            t_k = a_K * n_float**alpha + b_K * n_float\n            t_f = a_F * n_float * log2_n + b_F * n_float\n            \n            return t_f <= t_s and t_f <= t_k\n\n        # Optimization: If the condition is met at n_min, it's the answer.\n        # This also handles the n_min=1 case correctly.\n        if check(n_min):\n            return n_min\n            \n        # Optimization: If the condition is not met at n_max and the crossover\n        # function is monotonic, no solution exists in the range.\n        if not check(n_max):\n            return -1\n\n        low = n_min\n        high = n_max\n        ans = -1\n\n        while low <= high:\n            mid = low + (high - low) // 2\n            \n            # This check is for safety, though n_min >= 1 should prevent mid=0.\n            if mid == 0:\n                low = 1\n                continue\n\n            if check(mid):\n                # mid is a potential answer, try to find a smaller one\n                ans = mid\n                high = mid - 1\n            else:\n                # mid is too small, need to search in the upper half\n                low = mid + 1\n        \n        return ans\n\n    results = []\n    for case in test_cases:\n        result = find_crossover(case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3190117"}, {"introduction": "As we move from a single processor to parallel computing, performance scaling becomes more complex. This exercise introduces a foundational performance model for a parallel 2D heat equation solver to explore how efficiency changes with the number of processors. By calculating the communication-to-computation ratio, you will gain insight into the critical trade-offs between latency, bandwidth, and computation, and distinguish between the two fundamental paradigms of strong and weak scaling [@problem_id:3190082].", "problem": "You are given a simplified performance model for a two-dimensional explicit finite-difference heat equation solver that performs one time step on a uniform grid using a five-point stencil. The domain is partitioned among processes on a two-dimensional process grid. The goal is to derive and compute the communication-to-computation time ratio for one time step under both strong scaling and weak scaling scenarios, using the average per-process cost. The final result for each test case must be a unitless ratio, rounded to six decimal places.\n\nModeling assumptions and fundamental base:\n- The global grid has size $N_x \\times N_y$ grid points for strong scaling. For weak scaling, the local subdomain per process has fixed size $n_x^{\\mathrm{loc}} \\times n_y^{\\mathrm{loc}}$ independent of the process count.\n- The number of processes is $P$, and processes are arranged on a square grid with $p_x=p_y=\\sqrt{P}$, where $P$ is a perfect square. There is no overlap of communication and computation.\n- Each process updates all of its local grid points with a five-point stencil. The compute time per grid point update is $\\gamma$ seconds, so the compute time per process is $T_{\\mathrm{comp}}=\\gamma \\, n_x^{\\mathrm{loc}} n_y^{\\mathrm{loc}}$.\n- Halo exchange occurs with up to four face neighbors. Each interface exchange is one cell wide and carries exactly the border grid points along that face. There are no periodic boundaries; if a neighbor does not exist, no message is sent on that face.\n- Communication follows the latency-bandwidth (also known as the Hockney) model: each message incurs a latency cost $\\alpha$ seconds and a per-element transfer cost $\\beta$ seconds per grid point. All messages in one time step are of size equal to the length of the exchanged edge in grid points.\n- The communication time per process for one time step is modeled as the average over all processes. Let the total number of point-to-point messages exchanged across all processes in one time step be $M_{\\mathrm{tot}}$, and the total number of grid points communicated across all messages be $E_{\\mathrm{tot}}$. Then the average per-process communication time is $T_{\\mathrm{comm}}=\\alpha \\, \\frac{M_{\\mathrm{tot}}}{P} + \\beta \\, \\frac{E_{\\mathrm{tot}}}{P}$. For a $p_x \\times p_y$ process grid with non-periodic boundaries:\n  - The number of vertical neighbor pairs is $p_x (p_y-1)$; the number of horizontal neighbor pairs is $(p_x-1) p_y$. Each pair exchanges two messages per time step (one in each direction).\n  - Each vertical neighbor message carries $n_x^{\\mathrm{loc}}$ elements; each horizontal neighbor message carries $n_y^{\\mathrm{loc}}$ elements.\n- Under strong scaling, $n_x^{\\mathrm{loc}}=N_x/p_x$ and $n_y^{\\mathrm{loc}}=N_y/p_y$. Under weak scaling, $n_x^{\\mathrm{loc}}$ and $n_y^{\\mathrm{loc}}$ are given and fixed as $P$ changes.\n\nTarget quantity:\n- The communication-to-compute ratio is $R = \\dfrac{T_{\\mathrm{comm}}}{T_{\\mathrm{comp}}}$, a unitless number.\n\nYour task:\n- For each test case below, compute $R$ using the model above. Assume all divisions evenly divide as specified. Use $p_x=p_y=\\sqrt{P}$ exactly. Angles do not appear in this problem.\n- Output each ratio rounded to six decimal places.\n\nTest suite (six cases):\n1. Strong scaling baseline:\n   - Mode: strong\n   - $N_x=8192$, $N_y=8192$, $P=16$\n   - $\\alpha=2\\times 10^{-6}$ s, $\\beta=5\\times 10^{-9}$ s per element, $\\gamma=3\\times 10^{-9}$ s per grid point update\n2. Strong scaling boundary case (single process):\n   - Mode: strong\n   - $N_x=8192$, $N_y=8192$, $P=1$\n   - $\\alpha=2\\times 10^{-6}$ s, $\\beta=5\\times 10^{-9}$ s per element, $\\gamma=3\\times 10^{-9}$ s per grid point update\n3. Strong scaling many processes:\n   - Mode: strong\n   - $N_x=8192$, $N_y=8192$, $P=256$\n   - $\\alpha=2\\times 10^{-6}$ s, $\\beta=5\\times 10^{-9}$ s per element, $\\gamma=3\\times 10^{-9}$ s per grid point update\n4. Weak scaling baseline:\n   - Mode: weak\n   - $n_x^{\\mathrm{loc}}=1024$, $n_y^{\\mathrm{loc}}=1024$, $P=16$\n   - $\\alpha=2\\times 10^{-6}$ s, $\\beta=5\\times 10^{-9}$ s per element, $\\gamma=3\\times 10^{-9}$ s per grid point update\n5. Weak scaling, latency-dominated small subdomains:\n   - Mode: weak\n   - $n_x^{\\mathrm{loc}}=64$, $n_y^{\\mathrm{loc}}=64$, $P=256$\n   - $\\alpha=5\\times 10^{-6}$ s, $\\beta=5\\times 10^{-9}$ s per element, $\\gamma=3\\times 10^{-9}$ s per grid point update\n6. Weak scaling, bandwidth-sensitive anisotropic local size:\n   - Mode: weak\n   - $n_x^{\\mathrm{loc}}=1024$, $n_y^{\\mathrm{loc}}=256$, $P=64$\n   - $\\alpha=1\\times 10^{-6}$ s, $\\beta=2\\times 10^{-8}$ s per element, $\\gamma=2\\times 10^{-9}$ s per grid point update\n\nFinal output format:\n- Your program must produce a single line of output containing the six ratios, in the same order as the test cases above, as a comma-separated list enclosed in square brackets. Each ratio must be rendered with exactly six digits after the decimal point. For example, the format is like [result1,result2,result3,...] where each result is a decimal with six places.", "solution": "The problem is assessed as valid. It is scientifically grounded in standard performance modeling for parallel computing, is well-posed with all necessary information provided, and is stated using objective, formal language. The model, while simplified, is a standard tool (the Hockney model) for analyzing communication costs in parallel algorithms. All parameters are physically plausible and the setup is internally consistent.\n\nThe objective is to compute the communication-to-computation time ratio, $R = \\frac{T_{\\mathrm{comm}}}{T_{\\mathrm{comp}}}$, for a single time step of a two-dimensional finite-difference solver.\n\nFirst, we formalize the expressions for compute time, $T_{\\mathrm{comp}}$, and communication time, $T_{\\mathrm{comm}}$.\n\nThe compute time per process is defined as the time to update all local grid points. With a local subdomain of size $n_x^{\\mathrm{loc}} \\times n_y^{\\mathrm{loc}}$ and a computation cost of $\\gamma$ per point, the compute time is:\n$$T_{\\mathrm{comp}} = \\gamma \\, n_x^{\\mathrm{loc}} n_y^{\\mathrm{loc}}$$\n\nThe communication time per process, $T_{\\mathrm{comm}}$, is modeled as the average cost over all $P$ processes.\n$$T_{\\mathrm{comm}} = \\alpha \\frac{M_{\\mathrm{tot}}}{P} + \\beta \\frac{E_{\\mathrm{tot}}}{P}$$\nwhere $\\alpha$ is the message latency, $\\beta$ is the per-element transfer cost, $M_{\\mathrm{tot}}$ is the total number of messages exchanged, and $E_{\\mathrm{tot}}$ is the total number of grid points transferred.\n\nWe must determine $M_{\\mathrm{tot}}$ and $E_{\\mathrm{tot}}$. The processes are arranged on a $p_x \\times p_y$ grid, where $p_x = p_y = \\sqrt{P}$. Boundaries are non-periodic.\nThe number of internal horizontal interfaces is $p_y(p_x-1)$.\nThe number of internal vertical interfaces is $p_x(p_y-1)$.\nEach interface involves an exchange of two messages (one in each direction).\nTherefore, the total number of messages, $M_{\\mathrm{tot}}$, is:\n$$M_{\\mathrm{tot}} = 2 \\cdot p_x(p_y-1) + 2 \\cdot p_y(p_x-1)$$\nSubstituting $p_x = p_y = \\sqrt{P}$:\n$$M_{\\mathrm{tot}} = 2\\sqrt{P}(\\sqrt{P}-1) + 2\\sqrt{P}(\\sqrt{P}-1) = 4\\sqrt{P}(\\sqrt{P}-1)$$\nThe average number of messages per process is:\n$$\\frac{M_{\\mathrm{tot}}}{P} = \\frac{4\\sqrt{P}(\\sqrt{P}-1)}{P} = \\frac{4(\\sqrt{P}-1)}{\\sqrt{P}} = 4\\left(1 - \\frac{1}{\\sqrt{P}}\\right)$$\nNote that if $P=1$, $M_{\\mathrm{tot}}=0$, which is correct as there is no communication.\n\nNext, we determine the total number of elements communicated, $E_{\\mathrm{tot}}$.\nA message across a vertical interface (between process columns) contains a row of $n_x^{\\mathrm{loc}}$ points.\nA message across a horizontal interface (between process rows) contains a column of $n_y^{\\mathrm{loc}}$ points.\nThe total number of elements communicated is:\n$$E_{\\mathrm{tot}} = 2 \\cdot p_x(p_y-1) \\cdot n_x^{\\mathrm{loc}} + 2 \\cdot p_y(p_x-1) \\cdot n_y^{\\mathrm{loc}}$$\nSubstituting $p_x = p_y = \\sqrt{P}$:\n$$E_{\\mathrm{tot}} = 2\\sqrt{P}(\\sqrt{P}-1)n_x^{\\mathrm{loc}} + 2\\sqrt{P}(\\sqrt{P}-1)n_y^{\\mathrm{loc}} = 2\\sqrt{P}(\\sqrt{P}-1)(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}})$$\nThe average number of elements communicated per process is:\n$$\\frac{E_{\\mathrm{tot}}}{P} = \\frac{2\\sqrt{P}(\\sqrt{P}-1)(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}})}{P} = \\frac{2(\\sqrt{P}-1)(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}})}{\\sqrt{P}} = 2\\left(1 - \\frac{1}{\\sqrt{P}}\\right)(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}})$$\n\nNow we can write the full expression for $T_{\\mathrm{comm}}$:\n$$T_{\\mathrm{comm}} = \\alpha \\cdot 4\\left(1 - \\frac{1}{\\sqrt{P}}\\right) + \\beta \\cdot 2\\left(1 - \\frac{1}{\\sqrt{P}}\\right)(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}})$$\n$$T_{\\mathrm{comm}} = 2\\left(1 - \\frac{1}{\\sqrt{P}}\\right) \\left[ 2\\alpha + \\beta(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}}) \\right]$$\n\nThe communication-to-computation ratio $R$ is therefore:\n$$R = \\frac{T_{\\mathrm{comm}}}{T_{\\mathrm{comp}}} = \\frac{2\\left(1 - \\frac{1}{\\sqrt{P}}\\right) \\left[ 2\\alpha + \\beta(n_x^{\\mathrm{loc}} + n_y^{\\mathrm{loc}}) \\right]}{\\gamma \\, n_x^{\\mathrm{loc}} n_y^{\\mathrm{loc}}}$$\nThis general formula applies to both scaling scenarios. The distinction lies in how $n_x^{\\mathrm{loc}}$ and $n_y^{\\mathrm{loc}}$ are determined.\n\nFor strong scaling, the global problem size $N_x \\times N_y$ is fixed. The local size per process shrinks as $P$ increases:\n$n_x^{\\mathrm{loc}} = N_x / p_x = N_x / \\sqrt{P}$\n$n_y^{\\mathrm{loc}} = N_y / p_y = N_y / \\sqrt{P}$\n\nFor weak scaling, the local problem size $n_x^{\\mathrm{loc}} \\times n_y^{\\mathrm{loc}}$ is fixed. The global problem size grows with $P$.\n\nWe now apply this framework to each test case.\n\n**Case 1: Strong scaling baseline**\n- Mode: strong\n- $N_x=8192$, $N_y=8192$, $P=16$ ($\\sqrt{P}=4$)\n- $\\alpha=2 \\times 10^{-6}$ s, $\\beta=5 \\times 10^{-9}$ s/element, $\\gamma=3 \\times 10^{-9}$ s/update\n- $n_x^{\\mathrm{loc}} = 8192/4 = 2048$, $n_y^{\\mathrm{loc}} = 8192/4 = 2048$\n- $R = \\frac{2(1 - 1/4)[2(2 \\times 10^{-6}) + (5 \\times 10^{-9})(2048+2048)]}{(3 \\times 10^{-9})(2048)(2048)} = \\frac{1.5[4 \\times 10^{-6} + 2.048 \\times 10^{-5}]}{0.012582912} = \\frac{3.672 \\times 10^{-5}}{0.012582912} \\approx 0.002918$\n\n**Case 2: Strong scaling boundary case (single process)**\n- Mode: strong\n- $N_x=8192$, $N_y=8192$, $P=1$ ($\\sqrt{P}=1$)\n- $\\alpha, \\beta, \\gamma$ as in Case 1.\n- The term $(1 - 1/\\sqrt{P}) = (1-1/1) = 0$. Thus, $T_{\\mathrm{comm}}=0$ and $R=0$.\n- $R=0.000000$\n\n**Case 3: Strong scaling many processes**\n- Mode: strong\n- $N_x=8192$, $N_y=8192$, $P=256$ ($\\sqrt{P}=16$)\n- $\\alpha=2 \\times 10^{-6}$ s, $\\beta=5 \\times 10^{-9}$ s/element, $\\gamma=3 \\times 10^{-9}$ s/update\n- $n_x^{\\mathrm{loc}} = 8192/16 = 512$, $n_y^{\\mathrm{loc}} = 8192/16 = 512$\n- $R = \\frac{2(1 - 1/16)[2(2 \\times 10^{-6}) + (5 \\times 10^{-9})(512+512)]}{(3 \\times 10^{-9})(512)(512)} = \\frac{1.875[4 \\times 10^{-6} + 5.12 \\times 10^{-6}]}{0.000786432} = \\frac{1.71 \\times 10^{-5}}{0.000786432} \\approx 0.021742$\n\n**Case 4: Weak scaling baseline**\n- Mode: weak\n- $n_x^{\\mathrm{loc}}=1024$, $n_y^{\\mathrm{loc}}=1024$, $P=16$ ($\\sqrt{P}=4$)\n- $\\alpha=2 \\times 10^{-6}$ s, $\\beta=5 \\times 10^{-9}$ s/element, $\\gamma=3 \\times 10^{-9}$ s/update\n- $R = \\frac{2(1 - 1/4)[2(2 \\times 10^{-6}) + (5 \\times 10^{-9})(1024+1024)]}{(3 \\times 10^{-9})(1024)(1024)} = \\frac{1.5[4 \\times 10^{-6} + 1.024 \\times 10^{-5}]}{0.003145728} = \\frac{2.136 \\times 10^{-5}}{0.003145728} \\approx 0.006790$\n\n**Case 5: Weak scaling, latency-dominated small subdomains**\n- Mode: weak\n- $n_x^{\\mathrm{loc}}=64$, $n_y^{\\mathrm{loc}}=64$, $P=256$ ($\\sqrt{P}=16$)\n- $\\alpha=5 \\times 10^{-6}$ s, $\\beta=5 \\times 10^{-9}$ s/element, $\\gamma=3 \\times 10^{-9}$ s/update\n- $R = \\frac{2(1 - 1/16)[2(5 \\times 10^{-6}) + (5 \\times 10^{-9})(64+64)]}{(3 \\times 10^{-9})(64)(64)} = \\frac{1.875[10^{-5} + 6.4 \\times 10^{-7}]}{1.2288 \\times 10^{-5}} = \\frac{1.995 \\times 10^{-5}}{1.2288 \\times 10^{-5}} \\approx 1.623535$\n\n**Case 6: Weak scaling, bandwidth-sensitive anisotropic local size**\n- Mode: weak\n- $n_x^{\\mathrm{loc}}=1024$, $n_y^{\\mathrm{loc}}=256$, $P=64$ ($\\sqrt{P}=8$)\n- $\\alpha=1 \\times 10^{-6}$ s, $\\beta=2 \\times 10^{-8}$ s/element, $\\gamma=2 \\times 10^{-9}$ s/update\n- $R = \\frac{2(1 - 1/8)[2(1 \\times 10^{-6}) + (2 \\times 10^{-8})(1024+256)]}{(2 \\times 10^{-9})(1024)(256)} = \\frac{1.75[2 \\times 10^{-6} + 2.56 \\times 10^{-5}]}{0.000524288} = \\frac{4.83 \\times 10^{-5}}{0.000524288} \\approx 0.092125$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the communication-to-computation time ratio for a\n    2D finite-difference solver under different scaling scenarios.\n    \"\"\"\n    test_cases = [\n        # Case 1: Strong scaling baseline\n        {\n            \"mode\": \"strong\",\n            \"Nx\": 8192, \"Ny\": 8192, \"P\": 16,\n            \"alpha\": 2e-6, \"beta\": 5e-9, \"gamma\": 3e-9\n        },\n        # Case 2: Strong scaling boundary case (single process)\n        {\n            \"mode\": \"strong\",\n            \"Nx\": 8192, \"Ny\": 8192, \"P\": 1,\n            \"alpha\": 2e-6, \"beta\": 5e-9, \"gamma\": 3e-9\n        },\n        # Case 3: Strong scaling many processes\n        {\n            \"mode\": \"strong\",\n            \"Nx\": 8192, \"Ny\": 8192, \"P\": 256,\n            \"alpha\": 2e-6, \"beta\": 5e-9, \"gamma\": 3e-9\n        },\n        # Case 4: Weak scaling baseline\n        {\n            \"mode\": \"weak\",\n            \"nx_loc\": 1024, \"ny_loc\": 1024, \"P\": 16,\n            \"alpha\": 2e-6, \"beta\": 5e-9, \"gamma\": 3e-9\n        },\n        # Case 5: Weak scaling, latency-dominated small subdomains\n        {\n            \"mode\": \"weak\",\n            \"nx_loc\": 64, \"ny_loc\": 64, \"P\": 256,\n            \"alpha\": 5e-6, \"beta\": 5e-9, \"gamma\": 3e-9\n        },\n        # Case 6: Weak scaling, bandwidth-sensitive anisotropic local size\n        {\n            \"mode\": \"weak\",\n            \"nx_loc\": 1024, \"ny_loc\": 256, \"P\": 64,\n            \"alpha\": 1e-6, \"beta\": 2e-8, \"gamma\": 2e-9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p = case[\"P\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        gamma = case[\"gamma\"]\n\n        if p == 1:\n            # A single process has no communication.\n            results.append(0.0)\n            continue\n\n        sqrt_p = np.sqrt(p)\n        \n        if case[\"mode\"] == \"strong\":\n            nx_loc = case[\"Nx\"] / sqrt_p\n            ny_loc = case[\"Ny\"] / sqrt_p\n        elif case[\"mode\"] == \"weak\":\n            nx_loc = case[\"nx_loc\"]\n            ny_loc = case[\"ny_loc\"]\n        \n        # Compute time per process\n        t_comp = gamma * nx_loc * ny_loc\n\n        # Average communication time per process\n        # T_comm = 2 * (1 - 1/sqrt(P)) * [2*alpha + beta*(nx_loc + ny_loc)]\n        t_comm = 2.0 * (1.0 - 1.0 / sqrt_p) * (2.0 * alpha + beta * (nx_loc + ny_loc))\n        \n        # Communication-to-computation ratio\n        ratio = t_comm / t_comp\n        results.append(ratio)\n\n    # Format the final output as specified\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3190082"}, {"introduction": "Beyond measuring speed, scaling laws are also essential for understanding the accuracy of numerical methods. This practice delves into the world of stochastic differential equations, where you will empirically validate the convergence rates of the Euler-Maruyama method. Through Monte Carlo simulations and log-log regression analysis, you will verify the distinct scaling behaviors of strong error, which measures pathwise accuracy, and weak error, which measures accuracy in expectation [@problem_id:3190151].", "problem": "Consider the stochastic differential equation (SDE) in one dimension defined by $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ with initial condition $X_0 = x_0$, where $W_t$ is a standard Wiener process with independent increments and $a(\\cdot)$ and $b(\\cdot)$ are globally Lipschitz and of linear growth. The Euler–Maruyama time-stepping scheme with step size $\\Delta t$ over a terminal time $T$ updates a discrete approximation $X_n^{\\mathrm{EM}}$ by $X_{n+1}^{\\mathrm{EM}} = X_n^{\\mathrm{EM}} + a(X_n^{\\mathrm{EM}})\\,\\Delta t + b(X_n^{\\mathrm{EM}})\\,\\Delta W_n$, where $\\Delta W_n$ are independent normal increments with distribution $\\mathcal{N}(0,\\Delta t)$ and $n=0,1,\\dots,N-1$ for $N = T/\\Delta t$. Two error notions are considered at the terminal time: the strong error defined as the expectation $\\mathbb{E}\\left[\\lvert X_T - X_T^{\\mathrm{EM}}\\rvert\\right]$ and the weak error defined for a scalar observable function $f$ as $\\lvert \\mathbb{E}[f(X_T)] - \\mathbb{E}[f(X_T^{\\mathrm{EM}})]\\rvert$. Your task is to empirically determine scaling laws for these errors with respect to the time step $\\Delta t$ via systematic refinement.\n\nThe validation approach must use a sequence of refinements with halved step sizes $\\Delta t$ while keeping the terminal time $T$ fixed. For each $\\Delta t$, estimate the strong and weak errors by Monte Carlo sampling with a fixed number of paths, then compute empirical scaling exponents by linear regression of $\\log(\\text{error})$ against $\\log(\\Delta t)$. The computational experiment must be based on processes with known exact terminal solutions, so that reference values can be computed without bias. Use the following processes and parameter sets, and adopt $f(x)=x$ for weak error evaluation. All numbers must be treated exactly as specified.\n\n- Geometric Brownian Motion (GBM): $dX_t = \\mu X_t\\,dt + \\sigma X_t\\,dW_t$, with $X_0 = x_0$.\n- Deterministic limit of GBM: $dX_t = \\mu X_t\\,dt$ (that is, $\\sigma = 0$), with $X_0 = x_0$.\n\nFor GBM, simulations must be performed by generating independent Gaussian increments and applying the Euler–Maruyama update. Strong error must be computed by coupling the numerical approximation with the exact solution along the same Brownian path, using the terminal aggregate increment. Weak error must be computed by comparing the Monte Carlo estimator $\\mathbb{E}[X_T^{\\mathrm{EM}}]$ to the exact terminal expectation for the given process.\n\nUse the following globally defined parameters for all tests unless explicitly stated:\n- Terminal time $T = 1.0$.\n- Refinement levels $N \\in \\{32,64,128,256\\}$, so that $\\Delta t = T/N$.\n- Number of independent Monte Carlo paths per level $M_{\\text{paths}} = 16000$.\n- Random number generator must be seeded deterministically so the results are reproducible.\n\nDefine the following test suite with explicit parameter values:\n\n- Test $1\\mathrm{W}$ (GBM weak scaling): $x_0 = 1.0$, $\\mu = 0.2$, $\\sigma = 0.5$. Determine the weak scaling exponent $p_{\\mathrm{w}}$ from the regression; the program must return a boolean indicating whether $p_{\\mathrm{w}}$ lies within a tolerance of $\\pm 0.25$ around $1.0$.\n\n- Test $1\\mathrm{S}$ (GBM strong scaling): $x_0 = 1.0$, $\\mu = 0.2$, $\\sigma = 0.5$. Determine the strong scaling exponent $p_{\\mathrm{s}}$ from the regression; the program must return a boolean indicating whether $p_{\\mathrm{s}}$ lies within a tolerance of $\\pm 0.25$ around $0.5$.\n\n- Test $2\\mathrm{W}$ (Deterministic weak scaling): $x_0 = 1.0$, $\\mu = 0.2$, $\\sigma = 0.0$. Determine $p_{\\mathrm{w}}$; the program must return a boolean indicating whether $p_{\\mathrm{w}}$ lies within a tolerance of $\\pm 0.25$ around $1.0$.\n\n- Test $2\\mathrm{S}$ (Deterministic strong scaling): $x_0 = 1.0$, $\\mu = 0.2$, $\\sigma = 0.0$. Determine $p_{\\mathrm{s}}$; the program must return a boolean indicating whether $p_{\\mathrm{s}}$ lies within a tolerance of $\\pm 0.25$ around $1.0$.\n\n- Test $3$ (Degenerate GBM): $x_0 = 0.0$, $\\mu = 0.2$, $\\sigma = 0.5$. In this case, the exact and numerical terminal values are identically zero for any $\\Delta t$. The program must return a boolean indicating whether both the strong error and the weak error estimates are numerically below a threshold of $10^{-12}$ for all refinement levels.\n\nExact values required for reference calculations must be obtained from the closed-form terminal solutions for the specified processes. For GBM with parameters $(x_0,\\mu,\\sigma)$ and terminal time $T$, the exact terminal value along a Brownian path with terminal increment $W_T$ is $X_T = x_0 \\exp\\left((\\mu - \\tfrac{1}{2}\\sigma^2)T + \\sigma W_T\\right)$, and the exact terminal expectation is $\\mathbb{E}[X_T] = x_0 \\exp(\\mu T)$. For the deterministic limit with $\\sigma = 0$, the exact terminal value is $X_T = x_0 \\exp(\\mu T)$ and the exact terminal expectation coincides with this value.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[1\\mathrm{W},1\\mathrm{S},2\\mathrm{W},2\\mathrm{S},3]$, where each entry is a boolean evaluated according to the specifications above. For example, a valid output is $[{\\tt True},{\\tt True},{\\tt True},{\\tt True},{\\tt True}]$ if all validations succeed.", "solution": "The user-provided problem is a valid, well-posed computational task in the field of stochastic numerical methods. It requires the empirical validation of convergence rates for the Euler-Maruyama scheme applied to specific stochastic differential equations (SDEs). The problem is scientifically grounded, self-contained, and all parameters and procedures are clearly specified.\n\n### Theoretical Framework\n\nThe problem centers on the numerical approximation of a one-dimensional Itô SDE:\n$$dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t, \\quad X_0 = x_0$$\nwhere $W_t$ is a standard Wiener process. The Euler-Maruyama scheme provides a discrete-time approximation $X^{\\mathrm{EM}}$ at time points $t_n = n \\Delta t$ via the recurrence relation:\n$$X_{n+1}^{\\mathrm{EM}} = X_n^{\\mathrm{EM}} + a(X_n^{\\mathrm{EM}})\\,\\Delta t + b(X_n^{\\mathrm{EM}})\\,\\Delta W_n$$\nHere, $\\Delta t = T/N$ is the time step size for a terminal time $T$ and $N$ steps, and $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ are independent and identically distributed normal random variables with mean $0$ and variance $\\Delta t$, i.e., $\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)$.\n\nTwo primary notions of convergence are relevant:\n1.  **Strong Convergence**: This measures the pathwise-closeness of the numerical approximation to the true solution. The strong error at terminal time $T$ is defined as $\\mathbb{E}\\left[\\lvert X_T - X_T^{\\mathrm{EM}}\\rvert\\right]$. For SDEs with globally Lipschitz coefficients, the Euler-Maruyama scheme has a strong order of convergence of $p_{\\mathrm{s}} = 0.5$. This implies the error scales as $\\mathcal{O}((\\Delta t)^{0.5})$.\n2.  **Weak Convergence**: This measures the closeness of the expectations of functions of the solution. For a given observable function $f$, the weak error is $\\lvert \\mathbb{E}[f(X_T)] - \\mathbb{E}[f(X_T^{\\mathrm{EM}})]\\rvert$. The Euler-Maruyama scheme typically has a weak order of convergence of $p_{\\mathrm{w}} = 1.0$, meaning the error scales as $\\mathcal{O}(\\Delta t)$.\n\nThe problem asks to empirically determine the scaling exponents $p_{\\mathrm{s}}$ and $p_{\\mathrm{w}}$ by observing that if the error $\\epsilon$ scales as $\\epsilon(\\Delta t) \\approx C (\\Delta t)^p$, then a plot of $\\log(\\epsilon)$ against $\\log(\\Delta t)$ will be approximately linear with a slope of $p$:\n$$\\log(\\epsilon(\\Delta t)) \\approx \\log(C) + p \\log(\\Delta t)$$\nThe exponent $p$ can thus be estimated by performing a linear regression on the log-log data obtained from simulations at various step sizes $\\Delta t$.\n\n### Methodology and Implementation Strategy\n\nThe solution involves a series of computational experiments for the specified test cases. The core of the implementation will be a vectorized Monte Carlo simulation of the Euler-Maruyama scheme using `numpy`.\n\n**1. Simulation of Geometric Brownian Motion (GBM):**\nThe GBM process is given by $dX_t = \\mu X_t\\,dt + \\sigma X_t\\,dW_t$. This corresponds to $a(x) = \\mu x$ and $b(x) = \\sigma x$. The Euler-Maruyama update is:\n$$X_{n+1}^{\\mathrm{EM}} = X_n^{\\mathrm{EM}} + \\mu X_n^{\\mathrm{EM}}\\,\\Delta t + \\sigma X_n^{\\mathrm{EM}}\\,\\Delta W_n = X_n^{\\mathrm{EM}}(1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n)$$\nFor each refinement level $N \\in \\{32, 64, 128, 256\\}$, we will perform the following steps:\n- Discretize time with $\\Delta t = T/N$.\n- Simulate $M_{\\text{paths}} = 16000$ independent paths from $t=0$ to $t=T$. For each path, we generate $N$ random increments $\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)$ and apply the update rule iteratively.\n- For each path, we keep track of the cumulative Wiener increment $W_T = \\sum_{n=0}^{N-1} \\Delta W_n$.\n- At the terminal time $T$, we will have a set of numerical solutions $\\{X_{T,i}^{\\mathrm{EM}}\\}_{i=1}^{M_{\\text{paths}}}$.\n\n**2. Error Calculation for GBM:**\n- **Strong Error**: The exact solution for GBM at time $T$ for a path driven by the total increment $W_T$ is $X_T = x_0 \\exp\\left((\\mu - \\frac{1}{2}\\sigma^2)T + \\sigma W_T\\right)$. We compute this for each of the $M_{\\text{paths}}$ simulated Wiener paths. The strong error is then estimated by the sample mean:\n$$\\epsilon_{\\mathrm{strong}} \\approx \\frac{1}{M_{\\text{paths}}} \\sum_{i=1}^{M_{\\text{paths}}} \\lvert X_{T,i} - X_{T,i}^{\\mathrm{EM}}\\rvert$$\n- **Weak Error**: The observable is $f(x)=x$. The exact terminal expectation is $\\mathbb{E}[X_T] = x_0 \\exp(\\mu T)$. The numerical expectation is estimated by the sample mean $\\mathbb{E}[X_T^{\\mathrm{EM}}] \\approx \\frac{1}{M_{\\text{paths}}} \\sum_{i=1}^{M_{\\text{paths}}} X_{T,i}^{\\mathrm{EM}}$. The weak error is thus:\n$$\\epsilon_{\\mathrm{weak}} \\approx \\left\\lvert x_0 e^{\\mu T} - \\frac{1}{M_{\\text{paths}}} \\sum_{i=1}^{M_{\\text{paths}}} X_{T,i}^{\\mathrm{EM}} \\right\\rvert$$\n\n**3. Deterministic Case ($\\sigma=0$):**\nWhen $\\sigma=0$, the SDE reduces to the ordinary differential equation (ODE) $dX_t = \\mu X_t\\,dt$. The Euler-Maruyama scheme becomes the forward Euler method: $X_{n+1}^{\\mathrm{EM}} = X_n^{\\mathrm{EM}}(1 + \\mu \\Delta t)$. The numerical solution at time $T$ is deterministic: $X_T^{\\mathrm{EM}} = x_0(1 + \\mu \\Delta t)^N$. The exact solution is $X_T = x_0 \\exp(\\mu T)$. As there is no randomness, a Monte Carlo simulation is unnecessary ($M_{\\text{paths}}=1$). The strong and weak errors are identical:\n$$\\epsilon_{\\mathrm{strong}} = \\epsilon_{\\mathrm{weak}} = \\lvert X_T - X_T^{\\mathrm{EM}}\\rvert = \\lvert x_0 e^{\\mu T} - x_0(1 + \\mu \\frac{T}{N})^N \\rvert$$\nThe theoretical convergence rate for the forward Euler method is $p=1.0$.\n\n**4. Degenerate Case ($x_0=0$):**\nFor GBM with $x_0=0$, the initial condition is an absorbing state. The update rule $X_{n+1}^{\\mathrm{EM}} = X_n^{\\mathrm{EM}}(1 + \\mu \\Delta t + \\sigma \\Delta W_n)$ shows that if $X_n^{\\mathrm{EM}}=0$, then $X_{n+1}^{\\mathrm{EM}}=0$. By induction, $X_n^{\\mathrm{EM}}=0$ for all $n$. Similarly, the exact solution is $X_t=0$ for all $t$. Therefore, both strong and weak errors are theoretically zero. The test verifies that the numerical implementation yields errors below a small floating-point tolerance ($10^{-12}$).\n\n**5. Scaling Exponent Estimation:**\nFor each test suite requiring a scaling exponent (Tests $1\\mathrm{S}, 1\\mathrm{W}, 2\\mathrm{S}, 2\\mathrm{W}$), we compute the errors for each $\\Delta t = T/N$. We then use `numpy.polyfit` to find the slope of the best-fit line for $\\log(\\text{error})$ versus $\\log(\\Delta t)$. This slope is our empirical estimate for the convergence order $p$.\n\nThe final Python implementation will encapsulate this logic, executing each test case sequentially, and producing the required boolean output based on the specified criteria. A deterministic seed for the random number generator ensures reproducibility.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically determines the scaling laws for the Euler-Maruyama scheme\n    applied to Geometric Brownian Motion and its deterministic limit.\n    \"\"\"\n    \n    # Global parameters\n    T = 1.0\n    N_VALUES = [32, 64, 128, 256]\n    M_PATHS = 16000\n    RNG_SEED = 42\n\n    def get_scaling_exponent(delta_ts, errors):\n        \"\"\"\n        Performs a linear regression on log-log data to find the scaling exponent.\n        \"\"\"\n        # Filter out zero or negative errors before taking log\n        valid_indices = errors > 0\n        if np.sum(valid_indices) < 2:\n            return np.nan # Not enough data for regression\n\n        log_delta_ts = np.log(delta_ts[valid_indices])\n        log_errors = np.log(errors[valid_indices])\n        \n        # Fit a line: log(error) = p * log(dt) + C\n        p, _ = np.polyfit(log_delta_ts, log_errors, 1)\n        return p\n\n    def run_stochastic_simulation(params, n_values, T_val, m_paths, seed):\n        \"\"\"\n        Runs the Euler-Maruyama simulation for a stochastic process.\n        \"\"\"\n        x0, mu, sigma = params\n        \n        delta_ts = np.array([T_val / N for N in n_values])\n        strong_errors = []\n        weak_errors = []\n        \n        rng = np.random.default_rng(seed)\n\n        for N in n_values:\n            dt = T_val / N\n            sqrt_dt = np.sqrt(dt)\n\n            # Initialize paths\n            X_em = np.full(m_paths, x0, dtype=float)\n            \n            # If x0 is 0, the solution is always 0.\n            if x0 == 0.0:\n                X_T_em = np.zeros(m_paths)\n                total_dW = np.zeros(m_paths)\n            else:\n                total_dW = np.zeros(m_paths)\n                for _ in range(N):\n                    dW = rng.normal(0.0, sqrt_dt, m_paths)\n                    X_em += mu * X_em * dt + sigma * X_em * dW\n                    total_dW += dW\n                X_T_em = X_em\n\n            # Weak Error Calculation\n            E_X_T_exact = x0 * np.exp(mu * T_val)\n            E_X_T_em = np.mean(X_T_em)\n            weak_err = np.abs(E_X_T_exact - E_X_T_em)\n            weak_errors.append(weak_err)\n\n            # Strong Error Calculation\n            W_T = total_dW\n            X_T_exact = x0 * np.exp((mu - 0.5 * sigma**2) * T_val + sigma * W_T)\n            strong_err = np.mean(np.abs(X_T_exact - X_T_em))\n            strong_errors.append(strong_err)\n            \n        return delta_ts, np.array(strong_errors), np.array(weak_errors)\n        \n    def run_deterministic_simulation(params, n_values, T_val):\n        \"\"\"\n        Computes the error for the deterministic ODE case.\n        \"\"\"\n        x0, mu, _ = params\n        \n        delta_ts = np.array([T_val / N for N in n_values])\n        errors = []\n        \n        X_T_exact = x0 * np.exp(mu * T_val)\n        \n        for N in n_values:\n            X_T_em = x0 * (1 + mu * T_val / N)**N\n            errors.append(np.abs(X_T_exact - X_T_em))\n            \n        return delta_ts, np.array(errors)\n\n    results = []\n\n    # Test 1W & 1S: GBM weak and strong scaling\n    params_1 = (1.0, 0.2, 0.5)\n    dts_1, s_err_1, w_err_1 = run_stochastic_simulation(params_1, N_VALUES, T, M_PATHS, RNG_SEED)\n    p_w1 = get_scaling_exponent(dts_1, w_err_1)\n    p_s1 = get_scaling_exponent(dts_1, s_err_1)\n    \n    # Test 1W\n    results.append(abs(p_w1 - 1.0) <= 0.25)\n    # Test 1S\n    results.append(abs(p_s1 - 0.5) <= 0.25)\n    \n    # Test 2W & 2S: Deterministic weak and strong scaling\n    params_2 = (1.0, 0.2, 0.0)\n    dts_2, err_2 = run_deterministic_simulation(params_2, N_VALUES, T)\n    p_2 = get_scaling_exponent(dts_2, err_2)\n    \n    # Test 2W\n    results.append(abs(p_2 - 1.0) <= 0.25)\n    # Test 2S\n    results.append(abs(p_2 - 1.0) <= 0.25)\n    \n    # Test 3: Degenerate GBM\n    params_3 = (0.0, 0.2, 0.5)\n    _, s_err_3, w_err_3 = run_stochastic_simulation(params_3, N_VALUES, T, M_PATHS, RNG_SEED)\n    \n    threshold = 1e-12\n    all_errors_small = np.all(s_err_3 < threshold) and np.all(w_err_3 < threshold)\n    results.append(all_errors_small)\n    \n    # Format and print final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3190151"}]}