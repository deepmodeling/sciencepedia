{"hands_on_practices": [{"introduction": "To truly understand Proper Orthogonal Decomposition (POD), we will first derive the POD modes analytically for a simple, known signal. This exercise [@problem_id:3265890] strips away numerical complexities and reveals the fundamental connection between POD modes and the eigenfunctions of the spatial correlation operator. By working through this foundational example, you will gain a concrete understanding of how POD identifies the most dominant spatial structures within a dataset.", "problem": "Consider the spatial domain $x \\in [0,1]$ with the spatial inner product given by $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$, and the temporal ensemble average defined by the long-time mean $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} (\\cdot) \\, dt$. Let the space-time field be\n$$\nu(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t),\n$$\nwhere $\\omega_{1} > 0$ and $\\omega_{2} > 0$ are distinct real constants. Using the foundational definition of Proper Orthogonal Decomposition (POD), where the spatial POD modes are the eigenfunctions of the spatial correlation operator\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy,\n$$\nconstructed from the correlation kernel\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt,\n$$\nderive the analytical spatial POD modes associated with $u(x,t)$. Express the modes as $L^{2}([0,1])$-normalized functions. Your final answer must list the two normalized spatial POD modes in a single row matrix. No units are required, and no rounding is needed.", "solution": "The problem requires the derivation of the spatial Proper Orthogonal Decomposition (POD) modes for a given space-time field $u(x,t)$. By definition, the spatial POD modes, denoted by $\\phi(x)$, are the eigenfunctions of the spatial two-point correlation operator $\\mathcal{C}$, which is an integral operator with kernel $C(x,y)$. The eigenproblem is given by:\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy = \\lambda \\phi(x)\n$$\nThe kernel $C(x,y)$ is defined as the time-average of the product of the field at two spatial locations, $x$ and $y$.\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt\n$$\nThe first step is to compute this kernel for the given field $u(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t)$.\n\nLet's expand the product $u(x,t) \\, u(y,t)$:\n\\begin{align*}\nu(x,t) \\, u(y,t) = & \\left[ \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t) \\right] \\left[ \\sin(2\\pi y) \\cos(\\omega_{1} t) + \\sin(3\\pi y) \\cos(\\omega_{2} t) \\right] \\\\\n= & \\sin(2\\pi x) \\sin(2\\pi y) \\cos^2(\\omega_{1} t) \\\\\n& + \\sin(3\\pi x) \\sin(3\\pi y) \\cos^2(\\omega_{2} t) \\\\\n& + \\sin(2\\pi x) \\sin(3\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t) \\\\\n& + \\sin(3\\pi x) \\sin(2\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t)\n\\end{align*}\nTo find $C(x,y)$, we must compute the long-time average of the temporal components. We need the following standard time-average results for harmonic functions:\n$1$. For any non-zero frequency $\\omega > 0$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\frac{1 + \\cos(2\\omega t)}{2} \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\left[ \\frac{t}{2} + \\frac{\\sin(2\\omega t)}{4\\omega} \\right]_{0}^{T} = \\frac{1}{2}\n$$\n$2$. For two distinct positive frequencies $\\omega_1 \\neq \\omega_2$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos(\\omega_1 t) \\cos(\\omega_2 t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{0}^{T} \\left[ \\cos((\\omega_1 - \\omega_2)t) + \\cos((\\omega_1 + \\omega_2)t) \\right] dt = 0\n$$\nThe second result holds because the integral of a cosine function over a period is zero, and its indefinite integral is a sine function, which is bounded. Dividing by $T \\to \\infty$ makes the average tend to $0$.\n\nApplying these time averages to the expanded product, the cross-terms involving $\\cos(\\omega_{1} t) \\cos(\\omega_{2} t)$ average to zero. We are left with:\n\\begin{align*}\nC(x,y) &= \\sin(2\\pi x) \\sin(2\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{1} t) \\, dt \\right) + \\sin(3\\pi x) \\sin(3\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{2} t) \\, dt \\right) \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y)\n\\end{align*}\nThis is a separable kernel of rank $2$. The eigenfunctions of an operator with such a kernel must lie in the span of the functions that constitute the kernel, i.e., $\\text{span}\\{\\sin(2\\pi x), \\sin(3\\pi x)\\}$.\n\nLet's check if the basis functions $\\psi_1(x) = \\sin(2\\pi x)$ and $\\psi_2(x) = \\sin(3\\pi x)$ are orthogonal under the given inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$:\n$$\n\\langle \\psi_1, \\psi_2 \\rangle = \\int_{0}^{1} \\sin(2\\pi x) \\sin(3\\pi x) \\, dx = \\frac{1}{2} \\int_{0}^{1} \\left[ \\cos(\\pi x) - \\cos(5\\pi x) \\right] dx = \\frac{1}{2} \\left[ \\frac{\\sin(\\pi x)}{\\pi} - \\frac{\\sin(5\\pi x)}{5\\pi} \\right]_{0}^{1} = 0\n$$\nSince the spatial functions are orthogonal, they are indeed the eigenfunctions of the correlation operator $\\mathcal{C}$. We can verify this by substituting them into the eigenproblem.\n\nFor the first eigenfunction candidate $\\phi(x) = \\psi_1(x) = \\sin(2\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_1](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(2\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin^2(2\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin(3\\pi y) \\sin(2\\pi y) \\, dy\n\\end{align*}\nThe second integral is zero due to orthogonality. The first integral is:\n$$\n\\int_{0}^{1} \\sin^2(2\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(4\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(4\\pi y)}{8\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_1](x) = \\frac{1}{2} \\sin(2\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(2\\pi x)$.\nSo, $\\phi_1(x) = \\sin(2\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_1 = \\frac{1}{4}$.\n\nFor the second eigenfunction candidate $\\phi(x) = \\psi_2(x) = \\sin(3\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_2](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(3\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin(2\\pi y) \\sin(3\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin^2(3\\pi y) \\, dy\n\\end{align*}\nThe first integral is zero. The second integral is:\n$$\n\\int_{0}^{1} \\sin^2(3\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(6\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(6\\pi y)}{12\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_2](x) = \\frac{1}{2} \\sin(3\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(3\\pi x)$.\nSo, $\\phi_2(x) = \\sin(3\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_2 = \\frac{1}{4}$.\n\nThe final step is to normalize these eigenfunctions to have unit $L^2$-norm. The squared norm of an eigenfunction $\\phi$ is $\\|\\phi\\|^2 = \\langle \\phi, \\phi \\rangle = \\int_{0}^{1} \\phi(x)^2 \\, dx$.\nFor the first mode, $\\sin(2\\pi x)$:\n$$\n\\|\\sin(2\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(2\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is $1 / \\sqrt{1/2} = \\sqrt{2}$. The first normalized POD mode is $\\hat{\\phi_1}(x) = \\sqrt{2} \\sin(2\\pi x)$.\n\nFor the second mode, $\\sin(3\\pi x)$:\n$$\n\\|\\sin(3\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(3\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is also $\\sqrt{2}$. The second normalized POD mode is $\\hat{\\phi_2}(x) = \\sqrt{2} \\sin(3\\pi x)$.\n\nThe two non-trivial spatial POD modes are therefore $\\sqrt{2} \\sin(2\\pi x)$ and $\\sqrt{2} \\sin(3\\pi x)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{2}\\sin(2\\pi x) & \\sqrt{2}\\sin(3\\pi x)\n\\end{pmatrix}\n}\n$$", "id": "3265890"}, {"introduction": "One of the great strengths of reduced-order modeling is the ability to create a single, efficient model that is accurate over a range of system parameters. In this practice [@problem_id:3184782], you will construct a POD basis from solutions to the heat equation with varying thermal diffusivity, $\\kappa$. You will then test this basis on unseen parameter values, exploring how well this \"generalized\" basis can represent system behavior it was not explicitly trained on.", "problem": "You are asked to implement a complete, runnable program that constructs a Proper Orthogonal Decomposition (POD) basis from training snapshots of the one-dimensional heat equation and then evaluates how the orthogonal projection error decays as the number of retained POD modes increases for several unseen thermal diffusivity parameters within a specified range. All quantities must be treated in purely mathematical terms, without physical units.\n\nConsider the one-dimensional heat equation with zero Dirichlet boundary conditions on a unit interval:\n- Governing law: $u_t = \\kappa\\,u_{xx}$ for $x \\in [0,1]$ and $t \\ge 0$.\n- Boundary conditions: $u(0,t) = 0$ and $u(1,t) = 0$ for all $t \\ge 0$.\n- Initial condition: $u(x,0) = \\sum_{n=1}^{8} a_n \\sin(n\\pi x)$, where the coefficients are given by $a_1=1.0$, $a_2=0.5$, $a_3=-0.3$, $a_4=0.2$, $a_5=0.1$, $a_6=-0.05$, $a_7=0.04$, $a_8=-0.02$.\n\nFundamental base for solution construction: It is a well-tested fact that the solution to the above initial-boundary-value problem, under these boundary conditions, admits a Fourier sine series representation with time-dependent modal amplitudes that decay exponentially in time. You must use this fact to compute snapshots analytically, without numerically discretizing the partial differential equation.\n\nDefinitions and required computations:\n- Define the analytical solution as $u(x,t;\\kappa) = \\sum_{n=1}^{8} a_n \\exp\\!\\big(-\\kappa (n\\pi)^2 t\\big)\\,\\sin(n\\pi x)$.\n- Let the spatial grid be uniform on $[0,1]$ with $N_x = 200$ points, including the endpoints.\n- Let the training parameter values be $\\kappa_{\\text{train}} \\in \\{0.06, 0.14, 0.22\\}$ and the training snapshot times be $t \\in \\{0.00, 0.05, 0.10, 0.20, 0.50\\}$.\n- Construct the training snapshot matrix $M_{\\text{train}} \\in \\mathbb{R}^{N_x \\times N_{\\text{snap}}}$ by concatenating, as columns, all spatial snapshots $u(\\cdot, t; \\kappa)$ for every $t$ in the specified set and every $\\kappa$ in the specified training set, in any fixed deterministic order.\n- Compute a POD basis as the columns of $U$ obtained from the Singular Value Decomposition (SVD) $M_{\\text{train}} = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{N_x \\times r_{\\max}}$ has orthonormal columns and $r_{\\max} \\le \\min(N_x, N_{\\text{snap}})$. Use these $U$ columns in order as the POD modes; do not subtract a mean (i.e., perform POD directly on $M_{\\text{train}}$).\n- For evaluation, use unseen test parameters $\\kappa_{\\text{test}} \\in \\{0.05, 0.10, 0.17, 0.25\\}$ and the same time set $t \\in \\{0.00, 0.05, 0.10, 0.20, 0.50\\}$ to build $M_{\\text{test}}(\\kappa) \\in \\mathbb{R}^{N_x \\times N_{\\text{test}}}$ for each test parameter.\n- For a given number of retained modes $r$, define the orthogonal projection operator $P_r = U_r U_r^\\top$, where $U_r \\in \\mathbb{R}^{N_x \\times r}$ contains the first $r$ POD modes. For each test parameter $\\kappa$, compute the relative projection error\n$$\nE(\\kappa,r) = \\frac{\\lVert M_{\\text{test}}(\\kappa) - P_r M_{\\text{test}}(\\kappa) \\rVert_F}{\\lVert M_{\\text{test}}(\\kappa) \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. Take $E(\\kappa,0)$ to be the relative error of the zero approximation (i.e., $P_0 = 0$).\n- Evaluate $E(\\kappa,r)$ for $r \\in \\{0,1,2,3,5,8,10\\}$ and for each $\\kappa \\in \\{0.05, 0.10, 0.17, 0.25\\}$.\n\nTest suite and coverage rationale:\n- The choice $r=0$ tests the boundary case of no retained modes.\n- The choice $r=10$ exceeds the number of nonzero initial Fourier components, probing the expected saturation to zero error.\n- The test parameters include both interval endpoints $0.05$ and $0.25$ and interior values $0.10$ and $0.17$, none of which are used in training, thus exercising generalization across the parameter range.\n\nFinal output requirements:\n- Your program should produce a single line of output containing the results as a nested list of floats with no spaces. The outer list must be ordered by the test parameters in ascending order $\\kappa \\in [0.05, 0.10, 0.17, 0.25]$. For each test parameter, the inner list must contain the errors $[E(\\kappa,0), E(\\kappa,1), E(\\kappa,2), E(\\kappa,3), E(\\kappa,5), E(\\kappa,8), E(\\kappa,10)]$ in this exact $r$ order. Express each float rounded to eight decimal places. For example, the printed shape must look like $[[e_{11},e_{12},\\dots],[e_{21},\\dots],\\dots]$ with numeric entries in place of the $e_{ij}$.", "solution": "The problem requires the implementation of a Proper Orthogonal Decomposition (POD) based reduced-order model for the one-dimensional heat equation. The process involves generating training data, constructing a low-dimensional basis, and evaluating the basis's effectiveness on unseen test data. The entire procedure is based on an analytical solution, avoiding discretization errors of the governing partial differential equation (PDE).\n\nStep 1: Analytical Solution and Data Generation\n\nThe problem concerns the heat equation $u_t = \\kappa\\,u_{xx}$ on the domain $x \\in [0,1]$ with zero Dirichlet boundary conditions, $u(0,t) = u(1,t) = 0$. The initial condition is a finite Fourier series, $u(x,0) = \\sum_{n=1}^{8} a_n \\sin(n\\pi x)$, with given coefficients $\\{a_n\\}$. The analytical solution, which is derived from the method of separation of variables, is provided as:\n$$\nu(x,t;\\kappa) = \\sum_{n=1}^{8} a_n \\exp\\!\\big(-\\kappa (n\\pi)^2 t\\big)\\,\\sin(n\\pi x)\n$$\nThis formula allows us to generate \"snapshots\" of the solution field, which are vectors representing the temperature distribution $u(\\cdot, t; \\kappa)$ at a specific time $t$ and for a given thermal diffusivity $\\kappa$. These snapshots are evaluated on a uniform spatial grid of $N_x = 200$ points on $[0,1]$.\n\nStep 2: Training Snapshot Matrix Construction\n\nTo build a reduced-order model, we first collect a set of representative solutions. The training data is generated using the training parameter set $\\kappa_{\\text{train}} \\in \\{0.06, 0.14, 0.22\\}$ and snapshot times $t \\in \\{0.00, 0.05, 0.10, 0.20, 0.50\\}$. For each pair $(\\kappa, t)$ in the training set, we compute the corresponding solution snapshot vector. These snapshot vectors are then concatenated as columns to form the training snapshot matrix $M_{\\text{train}} \\in \\mathbb{R}^{N_x \\times N_{\\text{snap}}}$. Here, $N_x = 200$ and the total number of snapshots is $N_{\\text{snap}} = |\\kappa_{\\text{train}}| \\times |t| = 3 \\times 5 = 15$.\n\nStep 3: Proper Orthogonal Decomposition (POD)\n\nThe core of POD is to find an orthonormal basis that is optimal for representing the training data in a least-squares sense. This is achieved by performing a Singular Value Decomposition (SVD) on the snapshot matrix:\n$$\nM_{\\text{train}} = U \\Sigma V^\\top\n$$\nThe columns of the matrix $U \\in \\mathbb{R}^{N_x \\times N_{\\text{snap}}}$ (from a thin SVD, since $N_x > N_{\\text{snap}}$) are the left singular vectors. These vectors form the POD basis. They are ordered by the magnitude of their corresponding singular values in the diagonal matrix $\\Sigma \\in \\mathbb{R}^{N_{\\text{snap}} \\times N_{\\text{snap}}}$, with the first column of $U$ being the most energetic mode.\nAn important observation is that all snapshots, for any $t$ and $\\kappa$, lie within the $8$-dimensional subspace spanned by the functions $\\{\\sin(n\\pi x)\\}_{n=1}^8$. Consequently, the rank of the snapshot matrix $M_{\\text{train}}$ cannot exceed $8$. This implies that only the first $8$ singular values will be significantly non-zero, and the POD basis will effectively have a dimension of $8$.\n\nStep 4: Projection and Error Evaluation\n\nThe POD basis allows for the approximation of any solution snapshot (both within and outside the training set) by its orthogonal projection onto the subspace spanned by the first $r$ POD modes. We denote the truncated basis matrix as $U_r \\in \\mathbb{R}^{N_x \\times r}$, which contains the first $r$ columns of $U$. The orthogonal projection operator onto this subspace is $P_r = U_r U_r^\\top$.\n\nTo evaluate the quality of the basis, we generate test data for a set of unseen parameters $\\kappa_{\\text{test}} \\in \\{0.05, 0.10, 0.17, 0.25\\}$ using the same set of time instances. For each $\\kappa \\in \\kappa_{\\text{test}}$, we form a test snapshot matrix $M_{\\text{test}}(\\kappa)$. The approximation of this test matrix using $r$ modes is given by $P_r M_{\\text{test}}(\\kappa)$. The error of this approximation is quantified using the relative Frobenius norm:\n$$\nE(\\kappa,r) = \\frac{\\lVert M_{\\text{test}}(\\kappa) - P_r M_{\\text{test}}(\\kappa) \\rVert_F}{\\lVert M_{\\text{test}}(\\kappa) \\rVert_F}\n$$\nFor computational efficiency, the projection $P_r M_{\\text{test}}(\\kappa)$ is calculated as $U_r (U_r^\\top M_{\\text{test}}(\\kappa))$. The case $r=0$ corresponds to approximating the solution by zero, yielding an error $E(\\kappa,0)=1$. We compute this error for each $\\kappa \\in \\kappa_{\\text{test}}$ and for a specified set of retained modes $r \\in \\{0, 1, 2, 3, 5, 8, 10\\}$. The results are aggregated into a nested list for the final output. Since the underlying solution space is $8$-dimensional, we expect the error to drop to nearly zero for $r \\ge 8$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Constructs a POD basis for the 1D heat equation and evaluates projection error.\n    \"\"\"\n    # Define problem parameters\n    A_COEFFS = np.array([1.0, 0.5, -0.3, 0.2, 0.1, -0.05, 0.04, -0.02])\n    NX = 200\n    KAPPA_TRAIN = [0.06, 0.14, 0.22]\n    T_POINTS = [0.00, 0.05, 0.10, 0.20, 0.50]\n    KAPPA_TEST = [0.05, 0.10, 0.17, 0.25]\n    R_VALUES = [0, 1, 2, 3, 5, 8, 10]\n\n    # Spatial grid\n    x = np.linspace(0, 1, NX)\n\n    def analytical_u(x_grid, t, kappa, coeffs):\n        \"\"\"Computes the analytical solution u(x, t; kappa).\"\"\"\n        sol = np.zeros_like(x_grid)\n        for n_idx, a_n in enumerate(coeffs):\n            n = n_idx + 1\n            mode_component = a_n * np.exp(-kappa * (n * np.pi)**2 * t) * np.sin(n * np.pi * x_grid)\n            sol += mode_component\n        return sol\n\n    # --- Step 1: Construct Training Snapshot Matrix ---\n    train_snapshots = []\n    for kappa_train_val in KAPPA_TRAIN:\n        for t_val in T_POINTS:\n            snapshot = analytical_u(x, t_val, kappa_train_val, A_COEFFS)\n            train_snapshots.append(snapshot)\n    \n    M_train = np.stack(train_snapshots, axis=1)\n\n    # --- Step 2: Compute POD Basis via SVD ---\n    # Use thin SVD since Nx > N_snap\n    U, s, vh = linalg.svd(M_train, full_matrices=False)\n\n    # --- Step 3: Evaluate Projection Error for Test Cases ---\n    all_results = []\n    for kappa_test_val in KAPPA_TEST:\n        # Build the test snapshot matrix for the current kappa\n        test_snapshots = []\n        for t_val in T_POINTS:\n            snapshot = analytical_u(x, t_val, kappa_test_val, A_COEFFS)\n            test_snapshots.append(snapshot)\n        \n        M_test = np.stack(test_snapshots, axis=1)\n\n        norm_M_test = linalg.norm(M_test, 'fro')\n        \n        kappa_errors = []\n        for r in R_VALUES:\n            if norm_M_test == 0:\n                 # Should not happen in this problem, but for robustness\n                 # If M_test is zero, projection is perfect (error 0), except for r=0\n                 error = 1.0 if r == 0 else 0.0\n            elif r == 0:\n                # Per problem spec, P_0 = 0. Error is ||M - 0|| / ||M|| = 1.\n                error = 1.0\n            else:\n                # Truncate the POD basis\n                Ur = U[:, :r]\n                \n                # Project M_test onto the r-dimensional subspace\n                # P_r * M_test = Ur * Ur.T * M_test\n                projected_M_test = Ur @ (Ur.T @ M_test)\n                \n                # Compute the relative Frobenius norm of the error\n                error_matrix = M_test - projected_M_test\n                norm_error = linalg.norm(error_matrix, 'fro')\n                error = norm_error / norm_M_test\n\n            kappa_errors.append(error)\n        \n        all_results.append(kappa_errors)\n        \n    # --- Step 4: Format output string ---\n    outer_parts = []\n    for inner_list in all_results:\n        inner_parts = [f\"{x:.8f}\" for x in inner_list]\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    final_string = f\"[{','.join(outer_parts)}]\"\n\n    print(final_string)\n\nsolve()\n```", "id": "3184782"}, {"introduction": "While POD is a powerful tool, it is not a universal solution, and understanding its limitations is key to using it effectively. This exercise [@problem_id:3265968] investigates a classic challenge for POD: systems dominated by pure translation, such as a moving wave or pulse. By analyzing the reconstruction error, you will see firsthand why a basis composed of stationary spatial modes can be inefficient at capturing advective phenomena.", "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n$[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]$,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.", "solution": "The objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3265968"}]}