{"hands_on_practices": [{"introduction": "Before tackling complex computational models, it's essential to master the fundamental mechanics of Bayesian inference. This first practice provides a complete workout of the core concepts using a classic statistical model. By working through the Poisson-Gamma conjugate pair, you will derive the posterior distribution from first principles, observe the 'shrinkage' effect that balances prior knowledge with observed data, and compute the posterior predictive distribution to make forecasts about new observations. This exercise [@problem_id:3101554] builds the foundational skillset required for all Bayesian analysis.", "problem": "A research team models the count of discrete events per fixed time window using a Poisson process. Let $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$, where $\\lambda$ is the Poisson rate for the window, and let the prior on $\\lambda$ be $\\lambda \\sim \\mathrm{Gamma}(a,b)$ with shape $a$ and rate $b$, with density $p(\\lambda) = \\frac{b^{a}}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b \\lambda)$ for $\\lambda > 0$. The team collects $n$ independent and identically distributed (IID) observations $y_1, \\dots, y_n$ given $\\lambda$, with the joint likelihood based on the Poisson model.\n\nStarting from Bayesâ€™ rule and the definitions above, derive the posterior distribution $p(\\lambda \\mid y_1,\\dots,y_n)$ and its mean, and explain briefly how the posterior mean exhibits shrinkage between the prior mean and the sample mean. Then, by integrating out $\\lambda$, derive the posterior predictive distribution for a future observation $y_{\\mathrm{new}}$ given the observed data.\n\nFinally, for the case $a = 3$, $b = 4$, $n = 5$, and $\\sum_{i=1}^{n} y_i = 12$, compute the posterior predictive probability that $y_{\\mathrm{new}} = 7$. Express the final probability as a decimal and round your answer to four significant figures. There are no physical units; report a unitless probability.", "solution": "The solution proceeds in four stages:\n1.  Derivation of the posterior distribution for the Poisson rate $\\lambda$.\n2.  Derivation of the posterior mean and explanation of the shrinkage effect.\n3.  Derivation of the posterior predictive distribution for a new observation $y_{\\mathrm{new}}$.\n4.  Numerical computation of the required probability using the derived distributions.\n\n**1. Posterior Distribution of $\\lambda$**\n\nAccording to Bayes' rule, the posterior distribution of the parameter $\\lambda$ given the data $y_1, \\dots, y_n$ (denoted as $\\mathbf{y}$) is proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda) \\, p(\\lambda)$$\nThe observations $y_1, \\dots, y_n$ are independent and identically distributed (IID) given $\\lambda$, following a Poisson distribution, $y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. The joint likelihood function is the product of the individual probability mass functions:\n$$p(\\mathbf{y} \\mid \\lambda) = \\prod_{i=1}^{n} p(y_i \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} = \\frac{\\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n\\lambda)}{\\prod_{i=1}^{n} y_i!}$$\nIn the context of finding the posterior for $\\lambda$, we can treat any term that does not depend on $\\lambda$ as part of the proportionality constant. Thus, the likelihood is proportional to:\n$$p(\\mathbf{y} \\mid \\lambda) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n\\lambda)$$\nThe prior distribution for $\\lambda$ is given as a Gamma distribution, $\\lambda \\sim \\mathrm{Gamma}(a,b)$, with the probability density function (PDF):\n$$p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda) \\propto \\lambda^{a-1} \\exp(-b\\lambda)$$\nNow, we multiply the likelihood and the prior:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto \\left( \\lambda^{\\sum y_i} \\exp(-n\\lambda) \\right) \\cdot \\left( \\lambda^{a-1} \\exp(-b\\lambda) \\right)$$\nCombining the terms with $\\lambda$:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{a + \\sum y_i - 1} \\exp(-(b+n)\\lambda)$$\nThis is the kernel of a Gamma distribution. We can identify the parameters of this posterior distribution. The posterior distribution is $\\lambda \\mid \\mathbf{y} \\sim \\mathrm{Gamma}(a', b')$, where the updated parameters are:\n$$a' = a + \\sum_{i=1}^{n} y_i$$\n$$b' = b + n$$\nThe full posterior PDF is $p(\\lambda \\mid \\mathbf{y}) = \\frac{(b')^{a'}}{\\Gamma(a')} \\lambda^{a'-1} \\exp(-b'\\lambda)$.\n\n**2. Posterior Mean and Shrinkage**\n\nThe mean of a Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$ is $E[X] = \\frac{\\alpha}{\\beta}$.\nThe posterior mean of $\\lambda$ is therefore the mean of the posterior distribution $\\mathrm{Gamma}(a', b')$:\n$$E[\\lambda \\mid \\mathbf{y}] = \\frac{a'}{b'} = \\frac{a + \\sum_{i=1}^{n} y_i}{b + n}$$\nTo explain the shrinkage effect, we can rewrite this expression. The prior mean of $\\lambda$ is $E[\\lambda] = \\frac{a}{b}$. The sample mean of the observations is $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$, which is the maximum likelihood estimate for $\\lambda$.\nThe posterior mean can be expressed as a weighted average of the prior mean and the sample mean:\n$$E[\\lambda \\mid \\mathbf{y}] = \\frac{a + n\\bar{y}}{b + n} = \\frac{b(a/b) + n\\bar{y}}{b+n} = \\frac{b}{b+n} E[\\lambda] + \\frac{n}{b+n} \\bar{y}$$\nLet $w = \\frac{b}{b+n}$. Then $1-w = \\frac{n}{b+n}$, and the posterior mean is:\n$$E[\\lambda \\mid \\mathbf{y}] = w E[\\lambda] + (1-w) \\bar{y}$$\nSince $b > 0$ and $n > 0$, the weight $w$ is between $0$ and $1$. The posterior mean is a weighted average of the prior mean and the sample mean. This demonstrates shrinkage: the posterior estimate is \"pulled away\" from the sample mean $\\bar{y}$ towards the prior mean $E[\\lambda]$. The amount of shrinkage is determined by the relative values of $b$ (representing the \"strength\" or \"pseudo-sample size\" of the prior) and $n$ (the sample size of the data).\n\n**3. Posterior Predictive Distribution**\n\nThe posterior predictive distribution for a new observation $y_{\\mathrm{new}}$ is obtained by marginalizing the likelihood of the new observation over the posterior distribution of the parameter $\\lambda$:\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\int_{0}^{\\infty} p(y_{\\mathrm{new}} \\mid \\lambda) \\, p(\\lambda \\mid \\mathbf{y}) \\, d\\lambda$$\nWe substitute the Poisson likelihood for $y_{\\mathrm{new}}$ and the Gamma posterior for $\\lambda$:\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\int_{0}^{\\infty} \\left( \\frac{\\lambda^{y_{\\mathrm{new}}} \\exp(-\\lambda)}{y_{\\mathrm{new}}!} \\right) \\left( \\frac{(b')^{a'}}{\\Gamma(a')} \\lambda^{a'-1} \\exp(-b'\\lambda) \\right) d\\lambda$$\nWe group the terms that do not depend on $\\lambda$ outside the integral:\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\frac{(b')^{a'}}{y_{\\mathrm{new}}! \\, \\Gamma(a')} \\int_{0}^{\\infty} \\lambda^{y_{\\mathrm{new}} + a' - 1} \\exp(-(b'+1)\\lambda) \\, d\\lambda$$\nThe integral is of the form $\\int_{0}^{\\infty} x^{\\alpha-1} \\exp(-\\beta x) dx$, which evaluates to $\\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}}$. For our integral, $\\alpha = a' + y_{\\mathrm{new}}$ and $\\beta = b' + 1$.\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\frac{(b')^{a'}}{y_{\\mathrm{new}}! \\, \\Gamma(a')} \\frac{\\Gamma(a' + y_{\\mathrm{new}})}{(b'+1)^{a' + y_{\\mathrm{new}}}}$$\nRearranging the terms gives:\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\frac{\\Gamma(a' + y_{\\mathrm{new}})}{y_{\\mathrm{new}}! \\, \\Gamma(a')} \\frac{(b')^{a'}}{(b'+1)^{a'}} \\frac{1}{(b'+1)^{y_{\\mathrm{new}}}}$$\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\frac{\\Gamma(a' + y_{\\mathrm{new}})}{y_{\\mathrm{new}}! \\, \\Gamma(a')} \\left( \\frac{b'}{b'+1} \\right)^{a'} \\left( \\frac{1}{b'+1} \\right)^{y_{\\mathrm{new}}}$$\nUsing the identity for the generalized binomial coefficient, $\\binom{n}{k} = \\frac{\\Gamma(n+1)}{k! \\Gamma(n-k+1)}$, we can write $\\frac{\\Gamma(r+k)}{k! \\Gamma(r)} = \\binom{r+k-1}{k}$. Here, $r=a'$ and $k=y_{\\mathrm{new}}$:\n$$p(y_{\\mathrm{new}} \\mid \\mathbf{y}) = \\binom{a' + y_{\\mathrm{new}} - 1}{y_{\\mathrm{new}}} \\left( \\frac{b'}{b'+1} \\right)^{a'} \\left( \\frac{1}{b'+1} \\right)^{y_{\\mathrm{new}}}$$\nThis is the probability mass function of a Negative Binomial distribution, $y_{\\mathrm{new}} \\mid \\mathbf{y} \\sim \\mathrm{NB}(r=a', p=\\frac{b'}{b'+1})$.\n\n**4. Numerical Calculation**\n\nWe are given the following values:\n- Prior parameters: $a=3$, $b=4$\n- Data summary: $n=5$, $\\sum_{i=1}^{n} y_i = 12$\nFirst, we compute the parameters of the posterior distribution, $a'$ and $b'$:\n$$a' = a + \\sum_{i=1}^{n} y_i = 3 + 12 = 15$$\n$$b' = b + n = 4 + 5 = 9$$\nWe need to compute the posterior predictive probability for a new observation $y_{\\mathrm{new}} = 7$. Using the derived PMF:\n$$P(y_{\\mathrm{new}} = 7 \\mid \\mathbf{y}) = \\binom{15 + 7 - 1}{7} \\left( \\frac{9}{9+1} \\right)^{15} \\left( \\frac{1}{9+1} \\right)^{7}$$\n$$P(y_{\\mathrm{new}} = 7 \\mid \\mathbf{y}) = \\binom{21}{7} \\left( \\frac{9}{10} \\right)^{15} \\left( \\frac{1}{10} \\right)^{7}$$\nFirst, calculate the binomial coefficient:\n$$\\binom{21}{7} = \\frac{21!}{7!(21-7)!} = \\frac{21!}{7!14!} = \\frac{21 \\times 20 \\times 19 \\times 18 \\times 17 \\times 16 \\times 15}{7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1} = 116280$$\nNow, substitute this value into the probability expression:\n$$P(y_{\\mathrm{new}} = 7 \\mid \\mathbf{y}) = 116280 \\times (0.9)^{15} \\times (0.1)^7$$\nUsing a calculator for the powers:\n$$(0.9)^{15} \\approx 0.205891132$$\n$$(0.1)^7 = 10^{-7}$$\nSo, the probability is:\n$$P(y_{\\mathrm{new}} = 7 \\mid \\mathbf{y}) = 116280 \\times 0.205891132 \\times 10^{-7} \\approx 23941.59 \\times 10^{-7} \\approx 0.002394159$$\nRounding the result to four significant figures gives $0.002394$.", "answer": "$$\\boxed{0.002394}$$", "id": "3101554"}, {"introduction": "Having mastered the basics, we now apply Bayesian inference to a more realistic scenario: calibrating a physics-based computational model. In this practice, you will infer the thermal diffusivity of a material by fitting the solution of the one-dimensional heat equation to synthetic data. This exercise [@problem_id:3101556] introduces the critical challenge of model error, specifically the numerical error arising from different time-stepping schemes. You will investigate how the certainty of your inference is affected by the interplay between observational noise and the accuracy of your computational model, a central theme in scientific computing.", "problem": "You are asked to implement Bayesian calibration for a one-dimensional heat conduction model to infer the thermal diffusivity parameter $k$ using a Gaussian observation model and to study how the posterior distribution changes with the observation noise and with different time discretizations of the computational model. All quantities are non-dimensional, so no physical units are used.\n\nSpecification of the forward model:\n- The one-dimensional heat equation is\n$$\n\\frac{\\partial u}{\\partial t}(x,t) = k \\frac{\\partial^2 u}{\\partial x^2}(x,t), \\quad x \\in (0,1), \\ t \\ge 0,\n$$\nwith homogeneous Dirichlet boundary conditions $u(0,t)=0$ and $u(1,t)=0$, and the initial condition $u(x,0)=\\sin(\\pi x)$.\n- For this initial-boundary value problem, separation of variables implies that the solution remains in the first eigenmode. Define the amplitude $a(t)$ by $u(x,t)=a(t)\\sin(\\pi x)$. The amplitude satisfies the linear ordinary differential equation\n$$\n\\frac{da}{dt}(t) = -\\lambda a(t), \\quad \\text{with} \\ \\lambda = k\\pi^2, \\quad a(0)=1.\n$$\n- The exact solution is $a_{\\text{exact}}(t) = \\exp(-\\lambda t)$, so $u(\\tfrac{1}{2},t)=a_{\\text{exact}}(t)$ because $\\sin(\\pi/2)=1$.\n\nObservation model:\n- We observe the temperature at a single spatial location $x=\\tfrac{1}{2}$ and at three times $t \\in \\{0.02, 0.05, 0.1\\}$. Stack these into a vector of length $3$ as $y=[u(\\tfrac{1}{2},0.02), u(\\tfrac{1}{2},0.05), u(\\tfrac{1}{2},0.1)]^\\top$.\n- The observation model is Gaussian: $y \\sim \\mathcal{N}(u(k), \\sigma^2 I)$, where $u(k)$ is the model-predicted vector for a given $k$, $\\sigma>0$ is the observational noise standard deviation, and $I$ is the identity matrix of size $3$.\n- Synthetic data are generated from the exact model at the true parameter $k_{\\text{true}}=0.12$ without added noise, that is $y = [\\exp(-\\pi^2 k_{\\text{true}} \\cdot 0.02), \\exp(-\\pi^2 k_{\\text{true}} \\cdot 0.05), \\exp(-\\pi^2 k_{\\text{true}} \\cdot 0.1)]^\\top$.\n\nComputational model discretizations:\n- Instead of using the exact amplitude, approximate $a(t)$ by time-stepping schemes applied to the linear ordinary differential equation $da/dt = -\\lambda a$ with $\\lambda = k\\pi^2$, integrating from $t=0$ to the times of interest using a base step size $\\Delta t = T/N_{\\text{base}}$ with $T=0.1$ and $N_{\\text{base}} \\in \\mathbb{N}$. For each observation time $t_j \\in \\{0.02,0.05,0.1\\}$, use $N_j = t_j/\\Delta t$ steps (these are integers for the values specified here).\n- Consider two time discretizations:\n    - Discretization $\\mathcal{D}_{\\text{EE},10}$: explicit Euler with $N_{\\text{base}}=10$ and update $a_{n+1} = a_n - \\Delta t \\lambda a_n$.\n    - Discretization $\\mathcal{D}_{\\text{CN},100}$: Crankâ€“Nicolson with $N_{\\text{base}}=100$ and update $a_{n+1} = a_n \\frac{1 - \\tfrac{1}{2}\\Delta t \\lambda}{1 + \\tfrac{1}{2}\\Delta t \\lambda}$.\n- For each discretization and for each $k$, the predicted observation vector $u(k)$ is obtained by stepping from $a(0)=1$ to $a(t_j)$ for each $t_j \\in \\{0.02,0.05,0.1\\}$ and evaluating $u(\\tfrac{1}{2},t_j)=a(t_j)$.\n\nBayesian calibration:\n- Use a uniform prior for $k$ on the interval $[k_{\\min}, k_{\\max}] = [0.05, 0.25]$ and zero outside it.\n- Given data $y$ and a chosen $\\sigma>0$, the posterior density (up to a normalizing constant) is $p(k\\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - u(k)\\|_2^2\\right)$ for $k \\in [0.05, 0.25]$, and $0$ otherwise.\n- Approximate posterior expectations by numerical quadrature over a uniform grid of $k$ values with $N_k = 2001$ equally spaced points on $[0.05,0.25]$. Specifically, compute the posterior mean $\\mathbb{E}[k\\mid y]$ and posterior standard deviation $\\sqrt{\\mathbb{V}[k\\mid y]}$ using normalized quadrature weights derived from the unnormalized posterior.\n\nTest suite:\n- Use the following six test cases to study posterior sensitivity to the observation noise level $\\sigma$ and the computational discretization:\n    1. Case $1$: discretization $\\mathcal{D}_{\\text{EE},10}$, $\\sigma=0.005$.\n    2. Case $2$: discretization $\\mathcal{D}_{\\text{EE},10}$, $\\sigma=0.02$.\n    3. Case $3$: discretization $\\mathcal{D}_{\\text{EE},10}$, $\\sigma=0.1$.\n    4. Case $4$: discretization $\\mathcal{D}_{\\text{CN},100}$, $\\sigma=0.005$.\n    5. Case $5$: discretization $\\mathcal{D}_{\\text{CN},100}$, $\\sigma=0.02$.\n    6. Case $6$: discretization $\\mathcal{D}_{\\text{CN},100}$, $\\sigma=0.1$.\n\nRequired outputs:\n- For each test case, compute the posterior mean and posterior standard deviation of $k$ using the specified prior, data, discretization, and $\\sigma$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should be ordered as $[\\mu_1, s_1, \\mu_2, s_2, \\mu_3, s_3, \\mu_4, s_4, \\mu_5, s_5, \\mu_6, s_6]$, where $\\mu_i$ and $s_i$ are the posterior mean and posterior standard deviation for Case $i$, respectively, each rounded to $6$ decimal places.", "solution": "We begin from the one-dimensional heat equation $\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2}$ on the unit interval with homogeneous Dirichlet boundary conditions and initial condition $u(x,0)=\\sin(\\pi x)$. By the method of separation of variables and eigenfunction expansions for the Laplacian with Dirichlet conditions, the solution can be expressed as a sum over spatial sine modes. Given the specific initial condition, only the first eigenmode is present, so\n$$\nu(x,t) = a(t)\\sin(\\pi x),\n$$\nwith amplitude $a(t)$ satisfying\n$$\n\\frac{da}{dt}(t) = -\\lambda a(t), \\quad \\lambda = k\\pi^2, \\quad a(0)=1.\n$$\nThis is a linear ordinary differential equation whose exact solution is $a_{\\text{exact}}(t) = \\exp(-\\lambda t)$. At $x=\\tfrac{1}{2}$, $\\sin(\\pi/2)=1$, therefore $u(\\tfrac{1}{2},t)=a(t)$.\n\nThe observation model stipulates that the observation vector $y \\in \\mathbb{R}^3$ at times $t \\in \\{0.02,0.05,0.1\\}$ satisfies the Gaussian likelihood\n$$\ny \\sim \\mathcal{N}(u(k), \\sigma^2 I),\n$$\nwhere $u(k) \\in \\mathbb{R}^3$ stacks the model outputs $u(\\tfrac{1}{2}, t_j)$ for $t_j \\in \\{0.02,0.05,0.1\\}$. The synthetic data are generated from the exact model at $k_{\\text{true}}=0.12$ without noise:\n$$\ny = \\left[\\exp(-\\pi^2 \\cdot 0.12 \\cdot 0.02), \\ \\exp(-\\pi^2 \\cdot 0.12 \\cdot 0.05), \\ \\exp(-\\pi^2 \\cdot 0.12 \\cdot 0.1)\\right]^\\top.\n$$\n\nFor computational modeling under different discretizations, we integrate the amplitude ordinary differential equation using single-step schemes with a base step size $\\Delta t = T/N_{\\text{base}}$ and $T=0.1$. For each observation time $t_j$, the number of steps is $N_j = t_j/\\Delta t$ (these are integers for the given times and base step sizes). The schemes are:\n- Explicit Euler (denoted $\\mathcal{D}_{\\text{EE},10}$): $a_{n+1} = a_n - \\Delta t \\lambda a_n$. After $N_j$ steps, $a(t_j)$ is approximated by repeated application of the factor $(1-\\Delta t \\lambda)$ starting from $a(0)=1$.\n- Crankâ€“Nicolson (denoted $\\mathcal{D}_{\\text{CN},100}$): $a_{n+1} = a_n \\frac{1 - \\tfrac{1}{2}\\Delta t \\lambda}{1 + \\tfrac{1}{2}\\Delta t \\lambda}$. After $N_j$ steps, $a(t_j)$ is approximated by repeated application of the factor $\\frac{1 - \\tfrac{1}{2}\\Delta t \\lambda}{1 + \\tfrac{1}{2}\\Delta t \\lambda}$ starting from $a(0)=1$.\n\nBayesian inference proceeds from Bayes' theorem. With a uniform prior on $[k_{\\min}, k_{\\max}] = [0.05, 0.25]$, the posterior density is\n$$\np(k\\mid y) = \\frac{p(y\\mid k) p(k)}{\\int_{k_{\\min}}^{k_{\\max}} p(y\\mid \\kappa) p(\\kappa)\\, d\\kappa}\n\\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - u(k)\\|_2^2\\right) \\ \\mathbf{1}_{[0.05,0.25]}(k),\n$$\nwhere $\\mathbf{1}_{[0.05,0.25]}$ is the indicator function. To compute posterior expectations such as the mean and variance, we approximate the integrals with numerical quadrature over a uniform grid of $N_k=2001$ points on $[0.05,0.25]$. Let the grid be $\\{k_i\\}_{i=1}^{N_k}$ and define unnormalized weights\n$$\nw_i = \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - u(k_i)\\|_2^2\\right).\n$$\nTo avoid numerical underflow when $\\sigma$ is small, we perform a log-sum-exp stabilization by subtracting $\\max_i \\log w_i$ before exponentiation; this does not change the normalized posterior weights since it multiplies all weights by the same constant. Denote normalized weights by $\\tilde{w}_i = w_i / \\sum_{j=1}^{N_k} w_j$. Then the posterior mean and variance are approximated as\n$$\n\\mathbb{E}[k\\mid y] \\approx \\sum_{i=1}^{N_k} k_i \\tilde{w}_i, \\quad\n\\mathbb{V}[k\\mid y] \\approx \\sum_{i=1}^{N_k} (k_i - \\mathbb{E}[k\\mid y])^2 \\tilde{w}_i,\n$$\nand the posterior standard deviation is $\\sqrt{\\mathbb{V}[k\\mid y]}$.\n\nAlgorithmic steps for each test case:\n1. Fix the discretization scheme and $N_{\\text{base}}$ to determine $\\Delta t = 0.1/N_{\\text{base}}$ and the step counts $N_j$ for $t_j \\in \\{0.02,0.05,0.1\\}$.\n2. Construct the grid $\\{k_i\\}_{i=1}^{2001}$ on $[0.05,0.25]$.\n3. For each $k_i$, compute $\\lambda_i = \\pi^2 k_i$ and the model predictions $u(k_i) \\in \\mathbb{R}^3$ by stepping the amplitude from $a(0)=1$ to $a(t_j)$ for each $t_j$ using the chosen scheme.\n4. Form residuals $r_i = y - u(k_i)$, compute $e_i = \\|r_i\\|_2^2$, then compute stabilized unnormalized log-weights $\\ell_i = -\\frac{1}{2\\sigma^2} e_i$, shift by subtracting $\\max_i \\ell_i$, exponentiate to get $w_i$, and normalize to obtain $\\tilde{w}_i$.\n5. Compute the posterior mean and standard deviation using the normalized weights.\n\nSensitivity expectations:\n- For small $\\sigma$ (e.g., $\\sigma=0.005$), the likelihood is sharply peaked, and with a highly accurate discretization (Crankâ€“Nicolson with $N_{\\text{base}}=100$), the posterior mean should be very close to $k_{\\text{true}}=0.12$ with a small posterior standard deviation. With a coarser, more biased discretization (explicit Euler with $N_{\\text{base}}=10$), the model discrepancy will bias the posterior mean away from $0.12$, and the posterior standard deviation will still be small due to the sharp likelihood.\n- For larger $\\sigma$ (e.g., $\\sigma=0.1$), the likelihood is diffuse, so the posterior approaches the prior; the posterior mean moves towards the prior center $(0.05+0.25)/2 = 0.15$, and the posterior standard deviation increases toward that of the uniform prior on $[0.05,0.25]$.", "answer": "```python\nimport numpy as np\n\ndef generate_synthetic_data(k_true, times):\n    lam_true = (np.pi ** 2) * k_true\n    return np.exp(-lam_true * times)\n\ndef forward_amplitudes(k_vals, times, scheme, n_base):\n    \"\"\"\n    Compute model predictions u(1/2, t_j) = a(t_j) for each k in k_vals and each t in times,\n    using time-stepping schemes for the ODE a' = -lambda a, a(0)=1, lambda = pi^2 * k.\n    \n    Parameters:\n        k_vals: array of shape (nk,)\n        times: array of shape (nt,)\n        scheme: 'EE' for explicit Euler, 'CN' for Crank-Nicolson\n        n_base: number of base steps to reach T = max(times)\n    Returns:\n        preds: array of shape (nk, nt)\n    \"\"\"\n    T = np.max(times)\n    dt = T / n_base\n    # Ensure times are integer multiples of dt per problem setup\n    steps_per_time = np.round(times / dt).astype(int)\n    lam = (np.pi ** 2) * k_vals[:, None]  # shape (nk,1) for broadcasting\n    preds = np.empty((k_vals.shape[0], times.shape[0]), dtype=float)\n    if scheme == 'EE':\n        # factor = (1 - dt * lambda)\n        factor = (1.0 - dt * lam)\n        # For each time, raise to the power N_j\n        for j, Nj in enumerate(steps_per_time):\n            preds[:, j] = np.power(factor[:, 0], Nj)\n    elif scheme == 'CN':\n        # factor = (1 - 0.5 dt lambda) / (1 + 0.5 dt lambda)\n        num = (1.0 - 0.5 * dt * lam)\n        den = (1.0 + 0.5 * dt * lam)\n        factor = num / den\n        for j, Nj in enumerate(steps_per_time):\n            preds[:, j] = np.power(factor[:, 0], Nj)\n    else:\n        raise ValueError(\"Unknown scheme. Use 'EE' or 'CN'.\")\n    return preds\n\ndef posterior_stats(y, times, scheme, n_base, sigma, k_min=0.05, k_max=0.25, nk=2001):\n    \"\"\"\n    Compute posterior mean and std of k given data y, model discretization, and sigma.\n    Uniform prior on [k_min, k_max], quadrature over equally spaced grid nk.\n    \"\"\"\n    k_grid = np.linspace(k_min, k_max, nk)\n    preds = forward_amplitudes(k_grid, times, scheme, n_base)  # (nk, nobs)\n    # residuals: (nk, nobs)\n    residuals = preds - y[None, :]\n    err2 = np.sum(residuals ** 2, axis=1)  # (nk,)\n    # Stabilized log-weights\n    logw = -0.5 * err2 / (sigma ** 2)\n    logw -= np.max(logw)\n    w = np.exp(logw)\n    w_sum = np.sum(w)\n    if w_sum == 0.0 or not np.isfinite(w_sum):\n        # Fallback to near-uniform if under/overflow occurs\n        w = np.ones_like(w) / w.size\n        w_sum = 1.0\n    # Normalize\n    w /= w_sum\n    mean = np.sum(k_grid * w)\n    var = np.sum(((k_grid - mean) ** 2) * w)\n    std = np.sqrt(max(var, 0.0))\n    return mean, std\n\ndef solve():\n    # Problem parameters\n    times = np.array([0.02, 0.05, 0.1], dtype=float)\n    k_true = 0.12\n    y = generate_synthetic_data(k_true, times)\n\n    # Test cases: (scheme, n_base, sigma)\n    test_cases = [\n        ('EE', 10, 0.005),\n        ('EE', 10, 0.02),\n        ('EE', 10, 0.1),\n        ('CN', 100, 0.005),\n        ('CN', 100, 0.02),\n        ('CN', 100, 0.1),\n    ]\n\n    results = []\n    for scheme, n_base, sigma in test_cases:\n        mean, std = posterior_stats(y, times, scheme, n_base, sigma)\n        results.append(f\"{mean:.6f}\")\n        results.append(f\"{std:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3101556"}, {"introduction": "Our final practice addresses one of the most important and subtle aspects of modern computational modeling: accounting for model inadequacy. Real-world computational models are rarely perfect and often exhibit systematic biases. This advanced exercise [@problem_id:3101558] introduces a powerful framework where the model discrepancy itself is treated as a latent function with a Gaussian Process (GP) prior. By learning to infer both the model parameters and the discrepancy function, you will see how to de-couple model bias from random noise, leading to more robust and honest uncertainty quantification.", "problem": "Consider a computational model with parametric output and systematic model discrepancy. You observe inputs $x_i \\in [0,1]$ and outputs $y_i \\in \\mathbb{R}$ for $i \\in \\{1,\\dots,n\\}$. The observation model is\n$$\ny_i \\;=\\; u(\\theta, x_i) \\;+\\; \\delta(x_i) \\;+\\; \\varepsilon_i,\n$$\nwhere $u(\\theta, x)$ is a known parametric computational model, $\\theta \\in \\mathbb{R}$ is an unknown parameter to be inferred, $\\delta(\\cdot)$ is a latent discrepancy function that captures model bias, and $\\varepsilon_i$ is independent observational noise. Assume the following foundations as the starting point:\n- Bayes' rule for posterior inference,\n- The definition of a Gaussian Process (GP) prior: $\\delta(\\cdot)$ has a zero-mean Gaussian Process (GP) prior with covariance kernel $k(x,x')$,\n- The law of the Gaussian distribution for the noise: $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independently.\n\nUse the parametric computational model $u(\\theta, x) = \\theta x$. The Gaussian Process (GP) prior on $\\delta(\\cdot)$ uses the squared exponential (also called Radial Basis Function) kernel\n$$\nk(x,x') \\;=\\; \\alpha^2 \\exp\\!\\Big(-\\frac{(x-x')^2}{2\\ell^2}\\Big),\n$$\nwith amplitude $\\alpha > 0$ and length scale $\\ell > 0$. Let the prior on $\\theta$ be Gaussian $\\theta \\sim \\mathcal{N}(0,\\tau^2)$ with known $\\tau > 0$. Treat $\\alpha$, $\\ell$, $\\sigma$, and $\\tau$ as fixed and known hyperparameters.\n\nTask:\n1. Starting only from Bayes' rule and the properties of jointly Gaussian random variables, derive the marginal likelihood of the data conditioned on $\\theta$ by integrating out the GP discrepancy $\\delta(\\cdot)$, and then derive the posterior distribution of $\\theta$ given the data.\n2. Derive the posterior distribution of the discrepancy function $\\delta(\\cdot)$ given the data and $\\theta$, and then derive how to obtain the posterior mean and posterior variance of $\\delta(x^\\star)$ at a specified input $x^\\star$ when $\\theta$ is itself uncertain.\n3. Design and implement an algorithm that:\n   - Computes the posterior mean and variance of $\\theta$ using a one-dimensional numerical integration over a grid for $\\theta$.\n   - Computes the posterior mean and variance of $\\delta(x^\\star)$ by appropriately combining the conditional GP posterior with the uncertainty in $\\theta$.\n   - Uses numerically stable linear algebra for the required Gaussian computations.\n\nAssume the following test suite of cases. For each case, use the provided $(x,y)$ data, hyperparameters $(\\alpha,\\ell,\\sigma,\\tau)$, and evaluation point $x^\\star$. All numbers are given explicitly:\n- Case A (general, moderate noise):\n  - $x = [\\,0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\,]$\n  - $y = [\\,0.05,\\,0.65,\\,1.0,\\,1.33,\\,1.98\\,]$\n  - $\\alpha = 0.4$, $\\ell = 0.3$, $\\sigma = 0.1$, $\\tau = 1.0$, $x^\\star = 0.5$\n- Case B (near noise-free boundary):\n  - $x = [\\,0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\,]$\n  - $y = [\\,0.05,\\,0.65,\\,1.0,\\,1.33,\\,1.98\\,]$\n  - $\\alpha = 0.4$, $\\ell = 0.3$, $\\sigma = 0.000001$, $\\tau = 1.0$, $x^\\star = 0.5$\n- Case C (few points, high noise, different hyperparameters):\n  - $x = [\\,0.1,\\,0.4,\\,0.9\\,]$\n  - $y = [\\,0.4588,\\,0.2088,\\,0.9412\\,]$\n  - $\\alpha = 0.3$, $\\ell = 0.5$, $\\sigma = 0.5$, $\\tau = 2.0$, $x^\\star = 0.5$\n\nYour program must:\n- Implement the derivations to compute, for each case, the following four quantities:\n  1. Posterior mean of $\\theta$ (a float),\n  2. Posterior standard deviation of $\\theta$ (a float),\n  3. Posterior mean of $\\delta(x^\\star)$ (a float),\n  4. Posterior standard deviation of $\\delta(x^\\star)$ (a float).\n- Round each float to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test caseâ€™s result must itself be a list in the order described above. For example:\n[[theta_mean, theta_std, delta_mean, delta_std], [...], [...]]\nUse $x^\\star = 0.5$ for all cases. No additional text should be printed; only the single-line bracketed list is allowed.", "solution": "The problem requires the derivation and implementation of a Bayesian inference framework for a computational model featuring a parametric term and a non-parametric Gaussian Process (GP) discrepancy term. We are given an observation model, priors for all unknown quantities, and specific datasets for computation.\n\nThe observation model for a set of $n$ data points $D = \\{ (x_i, y_i) \\}_{i=1}^n$ is given by\n$$y_i = u(\\theta, x_i) + \\delta(x_i) + \\varepsilon_i, \\quad i \\in \\{1, \\dots, n\\}$$\nwhere $u(\\theta, x) = \\theta x$ is the computational model with a single parameter $\\theta$. The term $\\delta(\\cdot)$ represents the model discrepancy, and $\\varepsilon_i$ is observational noise.\n\nThe probabilistic specification is as follows:\n- The prior on the parameter $\\theta$ is Gaussian: $\\theta \\sim \\mathcal{N}(0, \\tau^2)$.\n- The prior on the discrepancy function $\\delta(\\cdot)$ is a zero-mean Gaussian Process: $\\delta(\\cdot) \\sim \\mathcal{GP}(0, k(x, x'))$, with the squared exponential kernel $k(x,x') = \\alpha^2 \\exp(-\\frac{(x-x')^2}{2\\ell^2})$.\n- The observational noise is i.i.d. Gaussian: $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- The hyperparameters $\\alpha$, $\\ell$, $\\sigma$, and $\\tau$ are treated as known constants.\n\nLet us define the data in vector form: $\\mathbf{y} = [y_1, \\dots, y_n]^T$, $\\mathbf{x} = [x_1, \\dots, x_n]^T$. The model predictions are $\\mathbf{u}(\\theta) = u(\\theta, \\mathbf{x}) = \\theta\\mathbf{x}$. The latent discrepancy values at the observation points are $\\boldsymbol{\\delta} = [\\delta(x_1), \\dots, \\delta(x_n)]^T$, and the noise vector is $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_n]^T$. The model is then $\\mathbf{y} = \\theta\\mathbf{x} + \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}$.\n\nFrom the GP prior, the vector $\\boldsymbol{\\delta}$ follows a multivariate normal distribution: $\\boldsymbol{\\delta} \\sim \\mathcal{N}(\\mathbf{0}, K)$, where $K$ is the $n \\times n$ covariance matrix with entries $K_{ij} = k(x_i, x_j)$. The noise vector follows $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$, where $I$ is the $n \\times n$ identity matrix.\n\n### Part 1: Posterior Distribution of $\\theta$\n\nTo find the posterior distribution of $\\theta$, $p(\\theta|\\mathbf{y})$, we first apply Bayes' rule:\n$$p(\\theta|\\mathbf{y}) \\propto p(\\mathbf{y}|\\theta) p(\\theta)$$\nThe prior $p(\\theta)$ is given as $\\mathcal{N}(\\theta|0, \\tau^2)$. We need to derive the marginal likelihood $p(\\mathbf{y}|\\theta)$, which involves integrating out the latent discrepancy vector $\\boldsymbol{\\delta}$.\n\nConditioned on a fixed value of $\\theta$, the model can be written as $\\mathbf{y} - \\theta\\mathbf{x} = \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}$. Let $\\mathbf{z}(\\theta) = \\mathbf{y} - \\theta\\mathbf{x}$. The vector $\\mathbf{z}(\\theta)$ is a sum of two independent, zero-mean Gaussian vectors $\\boldsymbol{\\delta}$ and $\\boldsymbol{\\varepsilon}$. Therefore, $\\mathbf{z}(\\theta)$ is also a Gaussian random vector.\nIts mean is $E[\\mathbf{z}(\\theta)] = E[\\boldsymbol{\\delta}] + E[\\boldsymbol{\\varepsilon}] = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}$.\nIts covariance is $\\text{Cov}(\\mathbf{z}(\\theta)) = \\text{Cov}(\\boldsymbol{\\delta}) + \\text{Cov}(\\boldsymbol{\\varepsilon}) = K + \\sigma^2 I$. Let us denote this covariance matrix by $\\Sigma_y = K + \\sigma^2 I$.\n\nThus, the distribution of $\\mathbf{z}(\\theta)$ is $\\mathbf{z}(\\theta) \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_y)$. This implies that the distribution of $\\mathbf{y}$ conditioned on $\\theta$ is $\\mathbf{y}|\\theta \\sim \\mathcal{N}(\\theta\\mathbf{x}, \\Sigma_y)$. The marginal likelihood is the probability density function of this distribution:\n$$p(\\mathbf{y}|\\theta) = \\frac{1}{(2\\pi)^{n/2}|\\det(\\Sigma_y)|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\theta\\mathbf{x})^T \\Sigma_y^{-1}(\\mathbf{y} - \\theta\\mathbf{x})\\right)$$\nThe unnormalized posterior for $\\theta$ is the product of the likelihood and the prior:\n$$p(\\theta|\\mathbf{y}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\theta\\mathbf{x})^T \\Sigma_y^{-1}(\\mathbf{y} - \\theta\\mathbf{x})\\right) \\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$$\nThis posterior is analytically a Gaussian distribution. However, the problem specifies computing its moments via numerical integration. We define a grid of values $\\{\\theta_j\\}$. For each $\\theta_j$, we compute the unnormalized posterior value $\\tilde{p}_j = p(\\mathbf{y}|\\theta_j)p(\\theta_j)$. The posterior probability mass at each grid point is approximated as $p(\\theta_j|\\mathbf{y}) \\approx \\tilde{p}_j / \\sum_k \\tilde{p}_k \\Delta\\theta$, where $\\Delta\\theta$ is the grid spacing. The posterior mean and variance are then computed by numerical integration (e.g., trapezoidal rule):\n$$E[\\theta|\\mathbf{y}] = \\int \\theta p(\\theta|\\mathbf{y}) d\\theta \\approx \\frac{\\sum_j \\theta_j \\tilde{p}_j \\Delta\\theta}{\\sum_k \\tilde{p}_k \\Delta\\theta}$$\n$$\\text{Var}(\\theta|\\mathbf{y}) = E[\\theta^2|\\mathbf{y}] - (E[\\theta|\\mathbf{y}])^2 \\approx \\frac{\\sum_j \\theta_j^2 \\tilde{p}_j \\Delta\\theta}{\\sum_k \\tilde{p}_k \\Delta\\theta} - (E[\\theta|\\mathbf{y}])^2$$\n\n### Part 2: Posterior Distribution of $\\delta(x^\\star)$\n\nFirst, we derive the posterior distribution of the discrepancy $\\delta^\\star = \\delta(x^\\star)$ at a new point $x^\\star$, conditioned on both the data $\\mathbf{y}$ and a fixed parameter value $\\theta$. From the properties of Gaussian Processes, the joint distribution of the latent values $\\boldsymbol{\\delta}$ at the training points and $\\delta^\\star$ at the test point is a zero-mean Gaussian:\n$$\n\\begin{pmatrix} \\boldsymbol{\\delta} \\\\ \\delta^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}^\\star \\\\ (\\mathbf{k}^\\star)^T & k^{\\star\\star} \\end{pmatrix} \\right)\n$$\nwhere $\\mathbf{k}^\\star$ is an $n \\times 1$ vector with entries $(\\mathbf{k}^\\star)_i = k(x_i, x^\\star)$ and $k^{\\star\\star} = k(x^\\star, x^\\star) = \\alpha^2$.\n\nWe have the observation $\\mathbf{z}(\\theta) = \\mathbf{y} - \\theta\\mathbf{x} = \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}$. We need the conditional distribution of $\\delta^\\star$ given $\\mathbf{z}(\\theta)$. The joint distribution of $(\\delta^\\star, \\mathbf{z}(\\theta))$ is also Gaussian. Its covariance components are:\n- $\\text{Cov}(\\delta^\\star, \\delta^\\star) = k^{\\star\\star}$\n- $\\text{Cov}(\\mathbf{z}(\\theta), \\mathbf{z}(\\theta)) = K + \\sigma^2 I = \\Sigma_y$\n- $\\text{Cov}(\\delta^\\star, \\mathbf{z}(\\theta)) = \\text{Cov}(\\delta^\\star, \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}) = \\text{Cov}(\\delta^\\star, \\boldsymbol{\\delta}) = (\\mathbf{k}^\\star)^T$\n\nUsing the standard formula for conditional Gaussian distributions, the posterior for $\\delta^\\star$ given $\\mathbf{y}$ and $\\theta$ is a Gaussian distribution $\\delta^\\star|\\mathbf{y}, \\theta \\sim \\mathcal{N}(\\mu_{\\delta^\\star|\\theta}, \\sigma^2_{\\delta^\\star|\\theta})$ with:\n$$E[\\delta^\\star|\\mathbf{y}, \\theta] = \\mu_{\\delta^\\star|\\theta} = (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} (\\mathbf{y} - \\theta\\mathbf{x})$$\n$$\\text{Var}(\\delta^\\star|\\mathbf{y}, \\theta) = \\sigma^2_{\\delta^\\star|\\theta} = k^{\\star\\star} - (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{k}^\\star$$\nNote that the conditional variance $\\sigma^2_{\\delta^\\star|\\theta}$ is independent of $\\theta$ and $\\mathbf{y}$.\n\nTo account for the uncertainty in $\\theta$, we marginalize over its posterior distribution $p(\\theta|\\mathbf{y})$. We use the law of total expectation and law of total variance.\nThe posterior mean of $\\delta^\\star$ is:\n$$E[\\delta^\\star|\\mathbf{y}] = E_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]] = E_{p(\\theta|\\mathbf{y})}[(\\mathbf{k}^\\star)^T \\Sigma_y^{-1} (\\mathbf{y} - \\theta\\mathbf{x})]$$\n$$E[\\delta^\\star|\\mathbf{y}] = (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{y} - ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x}) E_{p(\\theta|\\mathbf{y})}[\\theta]$$\n$$E[\\delta^\\star|\\mathbf{y}] = (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{y} - ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x}) E[\\theta|\\mathbf{y}]$$\nThe posterior variance of $\\delta^\\star$ is:\n$$\\text{Var}(\\delta^\\star|\\mathbf{y}) = E_{p(\\theta|\\mathbf{y})}[\\text{Var}(\\delta^\\star|\\mathbf{y}, \\theta)] + \\text{Var}_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]]$$\nThe first term is:\n$$E_{p(\\theta|\\mathbf{y})}[\\sigma^2_{\\delta^\\star|\\theta}] = \\sigma^2_{\\delta^\\star|\\theta} = k^{\\star\\star} - (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{k}^\\star, \\text{ as it is constant w.r.t. } \\theta$$\nThe second term is:\n$$\\text{Var}_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]] = \\text{Var}_{p(\\theta|\\mathbf{y})}[(\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{y} - \\theta((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x})]$$\n$$\\text{Var}_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]] = ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x})^2 \\text{Var}_{p(\\theta|\\mathbf{y})}[\\theta]$$\nCombining these gives the total posterior variance:\n$$\\text{Var}(\\delta^\\star|\\mathbf{y}) = (k^{\\star\\star} - (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{k}^\\star) + ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x})^2 \\text{Var}(\\theta|\\mathbf{y})$$\n\n### Part 3: Algorithmic Implementation\n\nThe algorithm proceeds as follows for each test case:\n1.  Construct the kernel matrix $K$ from the input points $\\mathbf{x}$ and hyperparameters $\\alpha, \\ell$. Form the data covariance $\\Sigma_y = K + \\sigma^2 I$.\n2.  To ensure numerical stability, compute the Cholesky decomposition of $\\Sigma_y = LL^T$. Use this decomposition with a specialized solver (like `scipy.linalg.cho_solve`) to efficiently and stably compute products with $\\Sigma_y^{-1}$, such as $\\Sigma_y^{-1}\\mathbf{y}$ and $\\Sigma_y^{-1}\\mathbf{x}$.\n3.  Establish a numerical grid for $\\theta$. To ensure the grid covers the high-probability region of the posterior, an analytical approximation for the posterior mean and variance of $\\theta$ is used to center the grid and set its range (e.g., $\\mu_\\theta \\pm 6\\sigma_\\theta$).\n4.  For each point $\\theta_j$ on the grid, evaluate the unnormalized log-posterior $\\log p(\\mathbf{y}|\\theta_j) + \\log p(\\theta_j)$. Exponentiate these values (using the log-sum-exp trick to prevent numerical overflow/underflow) to get unnormalized posterior probabilities $\\tilde{p}_j$.\n5.  Use numerical integration (trapezoidal rule) over the grid to compute the normalizing constant, and then the posterior mean $E[\\theta|\\mathbf{y}]$ and variance $\\text{Var}(\\theta|\\mathbf{y})$.\n6.  Using the computed posterior mean and variance of $\\theta$, calculate the final posterior mean and variance of $\\delta(x^\\star)$ using the formulas derived in Part 2. This requires computing additional terms like $\\mathbf{k}^\\star$, $k^{\\star\\star}$, and products involving them with $\\Sigma_y^{-1}$.\n7.  The required quantities are the posterior mean and standard deviation for $\\theta$ and $\\delta(x^\\star)$. The standard deviations are the square roots of the computed variances.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general, moderate noise)\n        (\n            [0.0, 0.25, 0.5, 0.75, 1.0],\n            [0.05, 0.65, 1.0, 1.33, 1.98],\n            0.4, 0.3, 0.1, 1.0, 0.5\n        ),\n        # Case B (near noise-free boundary)\n        (\n            [0.0, 0.25, 0.5, 0.75, 1.0],\n            [0.05, 0.65, 1.0, 1.33, 1.98],\n            0.4, 0.3, 0.000001, 1.0, 0.5\n        ),\n        # Case C (few points, high noise, different hyperparameters)\n        (\n            [0.1, 0.4, 0.9],\n            [0.4588, 0.2088, 0.9412],\n            0.3, 0.5, 0.5, 2.0, 0.5\n        )\n    ]\n\n    all_results_lists = []\n    for case in test_cases:\n        x, y, alpha, l, sigma, tau, x_star = case\n        result_list = solve_one_case(x, y, alpha, l, sigma, tau, x_star)\n        all_results_lists.append(result_list)\n\n    # Format the final output string exactly as required.\n    string_parts = []\n    for res_list in all_results_lists:\n        inner_str = f\"[{res_list[0]:.6f},{res_list[1]:.6f},{res_list[2]:.6f},{res_list[3]:.6f}]\"\n        string_parts.append(inner_str)\n    final_output = f\"[{','.join(string_parts)}]\"\n    print(final_output)\n\ndef solve_one_case(x, y, alpha, l, sigma, tau, x_star):\n    \"\"\"\n    Computes posterior statistics for one test case.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = len(x)\n\n    def kernel(x1, x2, alpha_k, l_k):\n        \"\"\"Squared exponential kernel for 1D inputs.\"\"\"\n        x1 = np.asarray(x1).reshape(-1, 1)\n        x2 = np.asarray(x2).reshape(-1, 1)\n        sqdist = (x1 - x2.T)**2\n        return alpha_k**2 * np.exp(-0.5 / l_k**2 * sqdist)\n\n    # 1. Pre-computation using numerically stable linear algebra\n    X_train = x[:, np.newaxis]\n    K = kernel(X_train, X_train, alpha, l)\n    Sigma_y = K + sigma**2 * np.eye(n)\n\n    # Cholesky decomposition for stable solves: Sigma_y = L L^T\n    try:\n        L, lower = cho_factor(Sigma_y, lower=True)\n    except np.linalg.LinAlgError:\n        # Failsafe for non-positive definite matrix, though not expected here.\n        return [float('nan')] * 4\n\n    # Solve linear systems for vectors needed later.\n    # `cho_solve` computes x in `A x = b` given Cholesky factor of A.\n    v_y = cho_solve((L, lower), y)  # Represents Sigma_y^-1 * y\n    v_x = cho_solve((L, lower), x)  # Represents Sigma_y^-1 * x\n\n    # 2. Compute posterior of theta via numerical integration\n    # Use analytical posterior moments to define an effective integration grid.\n    x_T_Sigma_inv_x = np.dot(x, v_x)\n    y_T_Sigma_inv_x = np.dot(y, v_x)\n    \n    post_var_theta_analytical = 1.0 / (x_T_Sigma_inv_x + 1.0 / tau**2)\n    post_mean_theta_analytical = post_var_theta_analytical * y_T_Sigma_inv_x\n    post_std_theta_analytical = np.sqrt(post_var_theta_analytical)\n\n    # Set up numerical grid for theta\n    n_grid = 2001\n    grid_half_width = 8 * post_std_theta_analytical # A sufficiently wide grid\n    theta_grid = np.linspace(post_mean_theta_analytical - grid_half_width,\n                              post_mean_theta_analytical + grid_half_width, n_grid)\n\n    # Calculate unnormalized log posterior on the grid\n    # log p(y|theta) + log p(theta)\n    y_T_Sigma_inv_y = np.dot(y, v_y)\n    log_likelihood_term = -0.5 * (y_T_Sigma_inv_y - 2 * theta_grid * y_T_Sigma_inv_x + theta_grid**2 * x_T_Sigma_inv_x)\n    log_prior_theta = -0.5 * (theta_grid**2) / tau**2\n    unnorm_log_post_theta = log_likelihood_term + log_prior_theta\n\n    # Normalize posterior using log-sum-exp trick for numerical stability\n    log_p_max = np.max(unnorm_log_post_theta)\n    unnorm_post_theta = np.exp(unnorm_log_post_theta - log_p_max)\n    \n    # Numerical integration using trapezoidal rule\n    normalizing_constant = np.trapz(unnorm_post_theta, theta_grid)\n    post_theta_pdf = unnorm_post_theta / normalizing_constant\n    \n    # Compute posterior moments for theta\n    post_mean_theta = np.trapz(theta_grid * post_theta_pdf, theta_grid)\n    post_var_theta = np.trapz(theta_grid**2 * post_theta_pdf, theta_grid) - post_mean_theta**2\n    post_var_theta = max(0, post_var_theta) # Ensure non-negativity\n    post_std_theta = np.sqrt(post_var_theta)\n\n    # 3. Compute posterior of delta(x_star)\n    X_star = np.array([[x_star]])\n    k_star_vec = kernel(X_train, X_star, alpha, l).flatten() # k(x_i, x_star)\n    k_star_star = alpha**2 # k(x_star, x_star)\n\n    # Solve for v_k_star = Sigma_y^-1 * k_star_vec\n    v_k_star = cho_solve((L, lower), k_star_vec)\n    \n    k_star_T_Sigma_inv_y = np.dot(k_star_vec, v_y)\n    k_star_T_Sigma_inv_x = np.dot(k_star_vec, v_x)\n    k_star_T_Sigma_inv_k_star = np.dot(k_star_vec, v_k_star)\n    \n    # Posterior mean of delta(x_star)\n    post_mean_delta = k_star_T_Sigma_inv_y - k_star_T_Sigma_inv_x * post_mean_theta\n\n    # Posterior variance of delta(x_star)\n    var_delta_given_theta = k_star_star - k_star_T_Sigma_inv_k_star\n    var_of_mean_delta = (k_star_T_Sigma_inv_x**2) * post_var_theta\n    post_var_delta = var_delta_given_theta + var_of_mean_delta\n    post_var_delta = max(0, post_var_delta) # Ensure non-negativity\n    post_std_delta = np.sqrt(post_var_delta)\n    \n    # 4. Format and return results rounded to six decimal places\n    return [\n        round(post_mean_theta, 6),\n        round(post_std_theta, 6),\n        round(post_mean_delta, 6),\n        round(post_std_delta, 6)\n    ]\n\nsolve()\n```", "id": "3101558"}]}