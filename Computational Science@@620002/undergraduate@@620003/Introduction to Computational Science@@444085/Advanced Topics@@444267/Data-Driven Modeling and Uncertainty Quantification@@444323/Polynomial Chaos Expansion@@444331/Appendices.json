{"hands_on_practices": [{"introduction": "This first practice exercise establishes the foundational link between the Polynomial Chaos Expansion (PCE) framework and the statistical moments of a system's response. You will explore the approximation of a simple nonlinear function of a random variable, $\\sin(a\\xi)$. By deriving the expression for the zeroth-order PCE coefficient, you will discover a profound property: this single coefficient is identical to the exact mean (or expected value) of the quantity of interest. This exercise provides a concrete demonstration of how the structure of PCE elegantly captures fundamental statistical information.", "problem": "Consider a scalar input random variable $\\xi$ with probability density function $p_{\\xi}(x)=1$ on the interval $[0,1]$ and $p_{\\xi}(x)=0$ otherwise, so that $\\xi$ is uniformly distributed on $[0,1]$. Define the nonlinear transform $f(\\xi)=\\sin(a\\,\\xi)$, where $a>0$ is a fixed parameter. Using the framework of Polynomial Chaos Expansion (PCE), with basis functions given by the orthonormal shifted Legendre polynomials $\\{\\phi_{n}(x)\\}_{n\\geq 0}$ on $[0,1]$ under the weight $p_{\\xi}(x)=1$, do the following:\n\n1. Starting from the definition of PCE and the orthonormality of the basis $\\{\\phi_{n}\\}$ with respect to the inner product induced by $p_{\\xi}$, derive the expression for the coefficient associated with the zero-order mode (the constant basis function) when expanding $f(\\xi)$.\n2. Use this coefficient to approximate the expectation $\\mathbb{E}[\\sin(a\\,\\xi)]$ via PCE truncated at polynomial degree $3$.\n3. Independently, derive the exact analytic expression for $\\mathbb{E}[\\sin(a\\,\\xi)]$ from first principles using the definition of expectation as an integral with respect to $p_{\\xi}$.\n4. Evaluate the resulting expectation for $a=\\pi$ and provide the final numerical value in pure decimal form.\n\nRound your final numerical answer to four significant figures. No physical units are involved.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. We may proceed with the solution.\n\nThe problem asks for an analysis of the function $f(\\xi) = \\sin(a\\,\\xi)$ where $\\xi$ is a random variable uniformly distributed on the interval $[0, 1]$. The probability density function (PDF) for $\\xi$ is thus $p_{\\xi}(x) = 1$ for $x \\in [0, 1]$ and $p_{\\xi}(x) = 0$ otherwise. We are using a Polynomial Chaos Expansion (PCE) with a basis of orthonormal shifted Legendre polynomials $\\{\\phi_n(x)\\}_{n \\ge 0}$ on $[0,1]$.\n\nThe inner product with respect to the weighting function $w(x) = p_{\\xi}(x) = 1$ is defined as:\n$$ \\langle g, h \\rangle = \\int_{0}^{1} g(x) h(x) w(x) \\,dx = \\int_{0}^{1} g(x) h(x) \\,dx $$\nThe orthonormality of the basis functions implies $\\langle \\phi_n, \\phi_m \\rangle = \\delta_{nm}$, where $\\delta_{nm}$ is the Kronecker delta.\n\nThe PCE of the function $f(\\xi)$ is given by the series expansion:\n$$ f(\\xi) = \\sum_{n=0}^{\\infty} c_n \\phi_n(\\xi) $$\nwhere the coefficients $c_n$ are determined by projecting the function $f$ onto the basis functions:\n$$ c_n = \\langle f, \\phi_n \\rangle = \\int_{0}^{1} f(x) \\phi_n(x) \\,dx $$\n\n**1. Derivation of the Zero-Order Coefficient ($c_0$)**\n\nThe first task is to find the expression for the coefficient $c_0$, which corresponds to the zero-order basis function $\\phi_0(x)$. The zeroth-order shifted Legendre polynomial is a constant, let's denote the standard (non-normalized) polynomial as $P_0^*(x) = 1$. To find the orthonormal basis function $\\phi_0(x)$, we must normalize $P_0^*(x)$. Let $\\phi_0(x) = k$ for some constant $k$. The normalization condition is $\\langle \\phi_0, \\phi_0 \\rangle = 1$.\n$$ \\int_{0}^{1} (k)^2 \\,dx = 1 $$\n$$ k^2 \\int_{0}^{1} 1 \\,dx = 1 $$\n$$ k^2 [x]_{0}^{1} = 1 \\implies k^2 (1 - 0) = 1 \\implies k^2 = 1 $$\nWe choose the positive root, so $k=1$. Thus, the zero-order orthonormal basis function is $\\phi_0(x) = 1$.\n\nNow, we can compute the coefficient $c_0$ using its definition:\n$$ c_0 = \\langle f, \\phi_0 \\rangle = \\int_{0}^{1} f(x) \\phi_0(x) \\,dx = \\int_{0}^{1} \\sin(a\\,x) \\cdot 1 \\,dx $$\nThis integral is evaluated as:\n$$ c_0 = \\left[ -\\frac{1}{a} \\cos(a\\,x) \\right]_{0}^{1} = -\\frac{1}{a} (\\cos(a \\cdot 1) - \\cos(a \\cdot 0)) $$\nSince $\\cos(0) = 1$, we get:\n$$ c_0 = -\\frac{1}{a} (\\cos(a) - 1) = \\frac{1 - \\cos(a)}{a} $$\nThis is the expression for the zero-order coefficient.\n\n**2. PCE Approximation of the Expectation $\\mathbb{E}[\\sin(a\\,\\xi)]$**\n\nThe PCE truncated at a polynomial degree $P$ provides an approximation of the function $f(\\xi)$, denoted as $f_P(\\xi)$:\n$$ f_P(\\xi) = \\sum_{n=0}^{P} c_n \\phi_n(\\xi) $$\nThe expectation of this approximation is given by:\n$$ \\mathbb{E}[f_P(\\xi)] = \\mathbb{E}\\left[\\sum_{n=0}^{P} c_n \\phi_n(\\xi)\\right] $$\nBy the linearity of the expectation operator:\n$$ \\mathbb{E}[f_P(\\xi)] = \\sum_{n=0}^{P} c_n \\mathbb{E}[\\phi_n(\\xi)] $$\nThe expectation of a basis function $\\phi_n(\\xi)$ is:\n$$ \\mathbb{E}[\\phi_n(\\xi)] = \\int_{-\\infty}^{\\infty} \\phi_n(x) p_{\\xi}(x) \\,dx = \\int_{0}^{1} \\phi_n(x) \\cdot 1 \\,dx $$\nRecalling that $\\phi_0(x) = 1$, we can write this integral as an inner product:\n$$ \\mathbb{E}[\\phi_n(\\xi)] = \\int_{0}^{1} \\phi_n(x) \\phi_0(x) \\,dx = \\langle \\phi_n, \\phi_0 \\rangle $$\nDue to the orthonormality of the basis, $\\langle \\phi_n, \\phi_0 \\rangle = \\delta_{n0}$. This means $\\mathbb{E}[\\phi_0(\\xi)] = 1$ and $\\mathbb{E}[\\phi_n(\\xi)] = 0$ for all $n > 0$.\n\nSubstituting this result back into the expression for the expectation of the PCE:\n$$ \\mathbb{E}[f_P(\\xi)] = c_0 \\cdot \\mathbb{E}[\\phi_0(\\xi)] + \\sum_{n=1}^{P} c_n \\cdot \\mathbb{E}[\\phi_n(\\xi)] = c_0 \\cdot 1 + \\sum_{n=1}^{P} c_n \\cdot 0 = c_0 $$\nThis demonstrates a fundamental property of PCE: the expectation of the PCE approximation of any order $P \\ge 0$ is exactly equal to the zero-order coefficient $c_0$. The specification of truncation at polynomial degree $3$ is therefore immaterial for the calculation of the mean.\nThe PCE approximation for the expectation is thus:\n$$ \\mathbb{E}[\\sin(a\\,\\xi)] \\approx c_0 = \\frac{1 - \\cos(a)}{a} $$\n\n**3. Exact Analytic Expression for $\\mathbb{E}[\\sin(a\\,\\xi)]$.**\n\nWe can derive the exact expectation from first principles using the definition of expectation for a function of a continuous random variable:\n$$ \\mathbb{E}[g(\\xi)] = \\int_{-\\infty}^{\\infty} g(x) p_{\\xi}(x) \\,dx $$\nFor our problem, $g(\\xi) = \\sin(a\\,\\xi)$ and $p_{\\xi}(x) = 1$ on $[0,1]$.\n$$ \\mathbb{E}[\\sin(a\\,\\xi)] = \\int_{0}^{1} \\sin(a\\,x) \\cdot 1 \\,dx $$\nThis is precisely the same integral we evaluated for $c_0$.\n$$ \\mathbb{E}[\\sin(a\\,\\xi)] = \\left[ -\\frac{1}{a} \\cos(a\\,x) \\right]_{0}^{1} = -\\frac{1}{a} (\\cos(a) - \\cos(0)) = \\frac{1 - \\cos(a)}{a} $$\nAs established in Part 2, the zero-order PCE coefficient is not an approximation but is identical to the exact mean of the random quantity.\n\n**4. Numerical Evaluation for $a = \\pi$.**\n\nFinally, we evaluate the derived expression for the expectation when $a=\\pi$:\n$$ \\mathbb{E}[\\sin(\\pi\\,\\xi)] = \\frac{1 - \\cos(\\pi)}{\\pi} $$\nUsing the fact that $\\cos(\\pi) = -1$, we have:\n$$ \\mathbb{E}[\\sin(\\pi\\,\\xi)] = \\frac{1 - (-1)}{\\pi} = \\frac{2}{\\pi} $$\nTo provide the numerical value, we compute this quantity:\n$$ \\frac{2}{\\pi} \\approx 0.63661977... $$\nRounding to four significant figures as requested, we obtain $0.6366$.", "answer": "$$\\boxed{0.6366}$$", "id": "3174309"}, {"introduction": "Building on the foundational concept of the mean, this next practice guides you through the construction and validation of a complete PCE surrogate model. You will implement a computational benchmark using quadratic response functions, for which a low-order PCE is theoretically exact. This hands-on coding exercise reinforces the crucial relationships between the full set of PCE coefficients and the primary statistical momentsâ€”the mean and the variance. You will also gain practical experience with essential numerical techniques, such as Gaussian quadrature, which are used to accurately compute the expansion coefficients via orthonormal projection.", "problem": "You are asked to construct and validate a synthetic benchmark for generalized Polynomial Chaos (gPC) that yields exact mean and variance for quadratic response functions. The benchmark shall be designed so that the truncated orthonormal polynomial chaos of total degree two represents the response exactly. You will implement coefficient extraction by orthonormal projection using exact numerical quadrature and verify that the mean and variance computed from the gPC coefficients match the analytical values derived from distribution moments.\n\nFundamental base and assumptions: consider independent random variables with known orthonormal polynomial families and exact quadrature rules. For a standard normal random variable $X$ with probability density function $\\phi(x)$ and a uniform random variable $U$ on $[-1,1]$, use their associated orthonormal polynomial bases and Gaussian quadrature rules. The generalized Polynomial Chaos (gPC) basis functions are tensor products of orthonormal univariate polynomials appropriate to each marginal distribution. Coefficients are defined by orthonormal projection with respect to the underlying probability measure.\n\nYour tasks:\n1. Construct orthonormal univariate bases:\n   - For $X \\sim \\mathcal{N}(0,1)$, use the orthonormalized probabilists' Hermite polynomials $\\{\\psi_n(x)\\}_{n \\ge 0}$ such that $E[\\psi_m(X)\\psi_n(X)] = \\delta_{mn}$, where $\\delta_{mn}$ is the Kronecker delta.\n   - For $U \\sim \\text{Uniform}([-1,1])$, use the orthonormalized Legendre polynomials $\\{\\ell_n(u)\\}_{n \\ge 0}$ such that $E[\\ell_m(U)\\ell_n(U)] = \\delta_{mn}$.\n   - In multiple dimensions, use tensor products of the univariate orthonormal bases, indexed by multi-indices of total degree up to two.\n2. Extract gPC coefficients by orthonormal projection. For a response $f(\\boldsymbol{\\xi})$ and an orthonormal basis function $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$, the coefficient $c_{\\boldsymbol{\\alpha}}$ is the inner product\n   $$c_{\\boldsymbol{\\alpha}} = E\\left[f(\\boldsymbol{\\xi}) \\, \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\right],$$\n   where $E[\\cdot]$ denotes expectation with respect to the joint distribution of $\\boldsymbol{\\xi}$.\n3. Compute the mean and variance from the extracted coefficients using orthonormality of the basis. Then, compute the exact analytical mean and variance using well-tested distribution moment formulas for quadratic polynomials and compare the two.\n4. Use exact numerical quadrature rules suitable for the corresponding measures and polynomial degrees:\n   - For $X \\sim \\mathcal{N}(0,1)$, use three-point Gauss-Hermite quadrature with the change of variables $x = \\sqrt{2}\\,t$ to evaluate expectations of polynomials of degree up to four exactly.\n   - For $U \\sim \\text{Uniform}([-1,1])$, use three-point Gauss-Legendre quadrature scaled by the probability density to evaluate expectations of polynomials of degree up to four exactly.\n5. Implement the above for one-dimensional and two-dimensional independent random inputs and quadratic responses. For two dimensions, use tensor-product quadrature to maintain exactness for bivariate polynomials of appropriate degree.\n\nDefine the response functions and their parameters as follows. For one-dimensional cases, let\n$$f(x) = a_0 + a_1 x + a_2 x^2,$$\nand for two-dimensional cases with independent inputs $(x,y)$, let\n$$f(x,y) = c_0 + c_1 x + c_2 y + c_3 x^2 + c_4 y^2 + c_5 x y.$$\n\nUse the following test suite of parameter values:\n- Case 1 (standard normal, one-dimensional): $a_0 = 1.3$, $a_1 = -0.7$, $a_2 = 0.5$.\n- Case 2 (uniform on $[-1,1]$, one-dimensional): $a_0 = 0.2$, $a_1 = 1.1$, $a_2 = -0.5$.\n- Case 3 (standard normal, two-dimensional): $c_0 = 0.9$, $c_1 = 0.2$, $c_2 = -0.4$, $c_3 = 0.3$, $c_4 = 0.1$, $c_5 = 0.25$.\n- Case 4 (uniform on $[-1,1]$, two-dimensional): $c_0 = -0.5$, $c_1 = 0.6$, $c_2 = 0.1$, $c_3 = -0.2$, $c_4 = 0.7$, $c_5 = 0.3$.\n- Case 5 (standard normal, one-dimensional, constant edge case): $a_0 = 2.0$, $a_1 = 0.0$, $a_2 = 0.0$.\n- Case 6 (uniform on $[-1,1]$, one-dimensional, linear edge case): $a_0 = 0.0$, $a_1 = 1.0$, $a_2 = 0.0$.\n\nAnalytical reference moments to be used:\n- For $X \\sim \\mathcal{N}(0,1)$: $E[X] = 0$, $E[X^2] = 1$, $E[X^4] = 3$.\n- For $U \\sim \\text{Uniform}([-1,1])$: $E[U] = 0$, $E[U^2] = \\frac{1}{3}$, $E[U^4] = \\frac{1}{5}$.\n\nYour program must:\n- Implement orthonormal polynomial evaluation for both distributions.\n- Implement three-point Gaussian quadrature rules with appropriate scaling to compute expectations exactly for the required polynomial degrees.\n- Extract gPC coefficients for total degree two.\n- Compute gPC-based mean and variance and compare them to analytical results for each case.\n- Produce a single line of output containing the validation results as a comma-separated list enclosed in square brackets. For each case, output a boolean indicating whether both mean and variance match the analytical values within an absolute tolerance of $10^{-12}$.\n\nThe final output format must be exactly:\n\"Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., [True,False,True,False,True,True]).\"", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the theory of generalized Polynomial Chaos (gPC) expansions, a standard method in uncertainty quantification. The problem is well-posed, with all necessary parameters, distributions, and response functions explicitly defined. The objective is clear and falsifiable: to verify the correctness of a numerical implementation against analytical results. The underlying mathematical assumptions, such as the exactness of a degree-$2$ gPC expansion for a quadratic response and the sufficiency of $3$-point Gaussian quadrature, are correct.\n\nThe solution proceeds as follows. First, we define the necessary orthonormal polynomial bases for the standard normal and uniform distributions. Second, we derive the analytical expressions for the mean and variance of the quadratic response functions. Third, we express the response function in the gPC basis to find the theoretical coefficients and the corresponding mean and variance. The results are shown to be identical to the analytical ones. Fourth, we describe the numerical procedure for extracting the gPC coefficients using orthonormal projection, evaluated via exact numerical quadrature. Finally, we implement this procedure for each test case and verify that the numerically obtained mean and variance match the analytical values within the specified tolerance.\n\nThe gPC expansion of a response function $f(\\boldsymbol{\\xi})$ with random inputs $\\boldsymbol{\\xi}$ is given by\n$$f(\\boldsymbol{\\xi}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$$\nwhere $\\{\\Psi_{\\boldsymbol{\\alpha}}\\}$ is a basis of polynomials orthonormal with respect to the probability measure of $\\boldsymbol{\\xi}$, and $c_{\\boldsymbol{\\alpha}}$ are the gPC coefficients. For a quadratic response function, the expansion is exact when truncated at total polynomial degree $P=2$.\n\nThe coefficients are found by orthonormal projection:\n$$c_{\\boldsymbol{\\alpha}} = E\\left[f(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\right]$$\nwhere $E[\\cdot]$ denotes the expectation. Due to orthonormality, $E[\\Psi_{\\boldsymbol{\\alpha}}] = \\delta_{\\boldsymbol{\\alpha}\\mathbf{0}}$ and $E[\\Psi_{\\boldsymbol{\\alpha}}\\Psi_{\\boldsymbol{\\beta}}] = \\delta_{\\boldsymbol{\\alpha}\\boldsymbol{\\beta}}$.\nThe mean and variance of $f$ are then computed directly from the coefficients:\n$$\\mu_f = E[f] = E\\left[\\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}\\right] = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} E[\\Psi_{\\boldsymbol{\\alpha}}] = c_{\\mathbf{0}}$$\n$$\\sigma_f^2 = E[(f - \\mu_f)^2] = E\\left[\\left(\\sum_{\\boldsymbol{\\alpha} \\ne \\mathbf{0}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}\\right)^2\\right] = \\sum_{\\boldsymbol{\\alpha} \\ne \\mathbf{0}} \\sum_{\\boldsymbol{\\beta} \\ne \\mathbf{0}} c_{\\boldsymbol{\\alpha}}c_{\\boldsymbol{\\beta}} E[\\Psi_{\\boldsymbol{\\alpha}}\\Psi_{\\boldsymbol{\\beta}}] = \\sum_{\\boldsymbol{\\alpha} \\ne \\mathbf{0}} c_{\\boldsymbol{\\alpha}}^2$$\n\nThe univariate orthonormal polynomial bases for total degree up to $2$ are:\n1.  For $X \\sim \\mathcal{N}(0,1)$: Orthonormal probabilists' Hermite polynomials.\n    $$\\psi_0(x) = 1, \\quad \\psi_1(x) = x, \\quad \\psi_2(x) = \\frac{1}{\\sqrt{2}}(x^2 - 1)$$\n    The inverse relations are: $1 = \\psi_0(x)$, $x = \\psi_1(x)$, and $x^2 = \\sqrt{2}\\psi_2(x) + \\psi_0(x)$.\n\n2.  For $U \\sim \\text{Uniform}([-1,1])$: Orthonormal Legendre polynomials.\n    $$\\ell_0(u) = 1, \\quad \\ell_1(u) = \\sqrt{3}u, \\quad \\ell_2(u) = \\frac{\\sqrt{5}}{2}(3u^2 - 1)$$\n    The inverse relations are: $1 = \\ell_0(u)$, $u = \\frac{1}{\\sqrt{3}}\\ell_1(u)$, and $u^2 = \\frac{2}{3\\sqrt{5}}\\ell_2(u) + \\frac{1}{3} = \\frac{2\\sqrt{5}}{15}\\ell_2(u) + \\frac{1}{3}\\ell_0(u)$.\n\nThe analytical mean and variance are derived from the moments of the distributions.\nFor a one-dimensional response $f(z) = a_0 + a_1 z + a_2 z^2$, where $z$ is $X$ or $U$:\n$$\\mu_f = E[f(z)] = a_0 + a_1 E[z] + a_2 E[z^2]$$\nSince $E[X]=0$ and $E[U]=0$, this simplifies to $\\mu_f = a_0 + a_2 E[z^2]$.\n$$\\sigma_f^2 = E[(f(z) - \\mu_f)^2] = E[(a_1 z + a_2 (z^2-E[z^2]))^2]$$\nSince odd moments are zero, the cross term vanishes:\n$$\\sigma_f^2 = a_1^2 E[z^2] + a_2^2 E[(z^2-E[z^2])^2] = a_1^2 E[z^2] + a_2^2 (E[z^4] - (E[z^2])^2)$$\n\nFor a two-dimensional response $f(x,y) = c_0 + c_1 x + c_2 y + c_3 x^2 + c_4 y^2 + c_5 xy$, where $x, y$ are independent variables:\n$$\\mu_f = E[f(x,y)] = c_0 + c_3 E[x^2] + c_4 E[y^2]$$\n$$\\sigma_f^2 = \\text{Var}(f(x,y)) = c_1^2 \\text{Var}(x) + c_2^2 \\text{Var}(y) + c_3^2 \\text{Var}(x^2) + c_4^2 \\text{Var}(y^2) + c_5^2 \\text{Var}(xy)$$\nUsing $\\text{Var}(z) = E[z^2]$ for zero-mean $z$, and independence of $x,y$, we get:\n$$\\sigma_f^2 = c_1^2 E[x^2] + c_2^2 E[y^2] + c_3^2(E[x^4] - (E[x^2])^2) + c_4^2(E[y^4] - (E[y^2])^2) + c_5^2 E[x^2]E[y^2]$$\n\nThe coefficient extraction integrals $c_{\\boldsymbol{\\alpha}} = E[f(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})]$ are evaluated using numerical quadrature. Since the integrand $f(\\boldsymbol{\\xi})\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$ for $P=2$ is a polynomial of at most degree $4=2+2$, a $3$-point Gaussian quadrature rule, which is exact for polynomials of degree up to $2 \\times 3 - 1 = 5$, yields the exact coefficient values.\n\nFor $X \\sim \\mathcal{N}(0,1)$, we use Gauss-Hermite quadrature. The expectation $E[g(X)]$ is computed as:\n$$E[g(X)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} g(x) e^{-x^2/2} dx = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} g(\\sqrt{2}t) e^{-t^2} dt \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{3} w_i g(\\sqrt{2}t_i)$$\nwhere $(t_i, w_i)$ are the standard $3$-point Gauss-Hermite quadrature points and weights.\n\nFor $U \\sim \\text{Uniform}([-1,1])$, we use Gauss-Legendre quadrature. The expectation $E[g(U)]$ is computed as:\n$$E[g(U)] = \\frac{1}{2} \\int_{-1}^{1} g(u) du \\approx \\frac{1}{2} \\sum_{i=1}^{3} w_i g(u_i)$$\nwhere $(u_i, w_i)$ are the standard $3$-point Gauss-Legendre quadrature points and weights.\n\nIn two dimensions, a tensor product of the univariate quadrature rules is used to maintain exactness. The implementation will calculate the analytical and gPC-based statistics for each case and verify their agreement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and validates a synthetic benchmark for generalized Polynomial Chaos (gPC)\n    for quadratic response functions. It verifies that mean and variance from gPC\n    coefficients (extracted via exact quadrature) match analytical values.\n    \"\"\"\n\n    TOLERANCE = 1e-12\n\n    # --- Orthonormal Polynomial Definitions ---\n    def hermite_psi(n, x):\n        \"\"\"Orthonormal probabilists' Hermite polynomials psi_n(x).\"\"\"\n        if n == 0:\n            return np.ones_like(x)\n        if n == 1:\n            return x\n        if n == 2:\n            return (x**2 - 1) / np.sqrt(2)\n        raise ValueError(\"Only degrees 0, 1, 2 are implemented.\")\n\n    def legendre_ell(n, u):\n        \"\"\"Orthonormal Legendre polynomials ell_n(u).\"\"\"\n        if n == 0:\n            return np.ones_like(u)\n        if n == 1:\n            return np.sqrt(3) * u\n        if n == 2:\n            return np.sqrt(5) / 2 * (3 * u**2 - 1)\n        raise ValueError(\"Only degrees 0, 1, 2 are implemented.\")\n\n    # --- Response Function Definitions ---\n    def response_1d(x, coeffs):\n        return coeffs['a0'] + coeffs['a1'] * x + coeffs['a2'] * x**2\n\n    def response_2d(xy, coeffs):\n        x, y = xy[..., 0], xy[..., 1]\n        return (coeffs['c0'] + coeffs['c1'] * x + coeffs['c2'] * y +\n                coeffs['c3'] * x**2 + coeffs['c4'] * y**2 + coeffs['c5'] * x * y)\n\n    # --- Analytical Statistics ---\n    def get_analytical_stats(case):\n        coeffs = case['coeffs']\n        dist_type = case['dist']\n        dim = case['dim']\n\n        # Moments\n        if dist_type == 'normal':\n            moments = {'E_z1': 0, 'E_z2': 1, 'E_z4': 3}\n        else: # uniform\n            moments = {'E_z1': 0, 'E_z2': 1/3, 'E_z4': 1/5}\n\n        if dim == 1:\n            mean = coeffs['a0'] + coeffs['a2'] * moments['E_z2']\n            var_z2 = moments['E_z4'] - moments['E_z2']**2\n            variance = coeffs['a1']**2 * moments['E_z2'] + coeffs['a2']**2 * var_z2\n        else: # dim == 2\n            mean = coeffs['c0'] + coeffs['c3'] * moments['E_z2'] + coeffs['c4'] * moments['E_z2']\n            var_z2 = moments['E_z4'] - moments['E_z2']**2\n            variance = (coeffs['c1']**2 * moments['E_z2'] +\n                        coeffs['c2']**2 * moments['E_z2'] +\n                        coeffs['c3']**2 * var_z2 +\n                        coeffs['c4']**2 * var_z2 +\n                        coeffs['c5']**2 * moments['E_z2'] * moments['E_z2'])\n        return mean, variance\n\n    # --- gPC Coefficient Extraction and Statistics ---\n    def get_gpc_stats(case):\n        dist_type = case['dist']\n        dim = case['dim']\n        coeffs = case['coeffs']\n\n        if dist_type == 'normal':\n            poly_func = hermite_psi\n            # 3-point Gauss-Hermite quadrature\n            gh_pts, gh_w = np.polynomial.hermite.hermgauss(3)\n            quad_pts = gh_pts * np.sqrt(2)\n            quad_w = gh_w / np.sqrt(np.pi)\n        else: # uniform\n            poly_func = legendre_ell\n            # 3-point Gauss-Legendre quadrature\n            gl_pts, gl_w = np.polynomial.legendre.leggauss(3)\n            quad_pts = gl_pts\n            quad_w = gl_w / 2.0\n\n        if dim == 1:\n            f_vals = response_1d(quad_pts, coeffs)\n            \n            multi_indices = [0, 1, 2]\n            pce_coeffs = []\n            for alpha in multi_indices:\n                psi_vals = poly_func(alpha, quad_pts)\n                integrand = f_vals * psi_vals\n                c_alpha = np.sum(integrand * quad_w)\n                pce_coeffs.append(c_alpha)\n            \n            mean_gpc = pce_coeffs[0]\n            var_gpc = np.sum(np.array(pce_coeffs[1:])**2)\n\n        else: # dim == 2\n            quad_pts_x, quad_pts_y = np.meshgrid(quad_pts, quad_pts)\n            quad_pts_2d = np.stack([quad_pts_x.ravel(), quad_pts_y.ravel()], axis=-1)\n            f_vals = response_2d(quad_pts_2d, coeffs)\n            \n            quad_w_x, quad_w_y = np.meshgrid(quad_w, quad_w)\n            quad_w_2d = (quad_w_x * quad_w_y).ravel()\n\n            multi_indices = [(0, 0), (1, 0), (0, 1), (2, 0), (0, 2), (1, 1)]\n            pce_coeffs = {}\n            for alpha in multi_indices:\n                psi_vals = poly_func(alpha[0], quad_pts_2d[:, 0]) * poly_func(alpha[1], quad_pts_2d[:, 1])\n                integrand = f_vals * psi_vals\n                c_alpha = np.sum(integrand * quad_w_2d)\n                pce_coeffs[alpha] = c_alpha\n\n            mean_gpc = pce_coeffs[(0, 0)]\n            var_gpc = sum(c**2 for k, c in pce_coeffs.items() if k != (0, 0))\n        \n        return mean_gpc, var_gpc\n\n    # --- Main Loop ---\n    test_cases = [\n        {'id': 1, 'dim': 1, 'dist': 'normal', 'coeffs': {'a0': 1.3, 'a1': -0.7, 'a2': 0.5}},\n        {'id': 2, 'dim': 1, 'dist': 'uniform', 'coeffs': {'a0': 0.2, 'a1': 1.1, 'a2': -0.5}},\n        {'id': 3, 'dim': 2, 'dist': 'normal', 'coeffs': {'c0': 0.9, 'c1': 0.2, 'c2': -0.4, 'c3': 0.3, 'c4': 0.1, 'c5': 0.25}},\n        {'id': 4, 'dim': 2, 'dist': 'uniform', 'coeffs': {'c0': -0.5, 'c1': 0.6, 'c2': 0.1, 'c3': -0.2, 'c4': 0.7, 'c5': 0.3}},\n        {'id': 5, 'dim': 1, 'dist': 'normal', 'coeffs': {'a0': 2.0, 'a1': 0.0, 'a2': 0.0}},\n        {'id': 6, 'dim': 1, 'dist': 'uniform', 'coeffs': {'a0': 0.0, 'a1': 1.0, 'a2': 0.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        mean_analytical, var_analytical = get_analytical_stats(case)\n        mean_gpc, var_gpc = get_gpc_stats(case)\n\n        mean_match = np.isclose(mean_analytical, mean_gpc, atol=TOLERANCE, rtol=0)\n        var_match = np.isclose(var_analytical, var_gpc, atol=TOLERANCE, rtol=0)\n        \n        results.append(mean_match and var_match)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3174279"}, {"introduction": "After learning to construct a valid PCE model, it is vital to understand the assumptions that underpin it and the consequences of their violation. This advanced practice investigates a common and subtle pitfall in applying PCE: the orthogonality weight mismatch. You will empirically demonstrate the systematic bias that arises when using a standard polynomial basis (e.g., Legendre polynomials) with a regression-based approach on data that does not follow the corresponding canonical distribution (e.g., uniform). This exercise highlights the critical importance of the 'orthogonality' in Polynomial Chaos and provides insight into the behavior of non-intrusive PCE methods in real-world scenarios.", "problem": "You will write a complete program to empirically demonstrate how an orthogonality weight mismatch affects the coefficients of a Polynomial Chaos Expansion (PCE). Consider the Legendre polynomial basis, which is orthogonal on the interval $[-1,1]$ with respect to the uniform weight. You will compare two coefficient vectors for a truncated expansion of order $p$: one obtained by orthogonal projection under the uniform weight and one obtained by unweighted least squares regression using samples drawn from a non-uniform input distribution.\n\nUse the following fundamental base:\n- The Legendre polynomials $\\{P_n(x)\\}_{n=0}^{\\infty}$ are orthogonal on $[-1,1]$ with respect to the inner product $\\langle f,g\\rangle = \\int_{-1}^{1} f(x) g(x)\\,dx$, that is,\n$$\\int_{-1}^{1} P_m(x)\\,P_n(x)\\,dx = \\frac{2}{2n+1}\\,\\delta_{mn}.$$\n- A truncated Polynomial Chaos Expansion (PCE) of order $p$ approximates a scalar response $y(x)$ as\n$$y(x)\\approx \\sum_{n=0}^{p} c_n\\,P_n(x),$$\nwhere the coefficients $\\{c_n\\}_{n=0}^{p}$ are determined by an appropriate optimality criterion.\n\nThe task is to quantify the systematic coefficient bias introduced when the data are generated from a non-uniform input distribution but the Legendre basis (appropriate for the uniform weight) is still used with unweighted least squares. The program must implement the following steps for each test case:\n1. Define the function $y(x)$ as specified by the test case.\n2. Compute the coefficient vector $\\boldsymbol{c}^{\\star} = (c_0^{\\star},\\ldots,c_p^{\\star})$ corresponding to the orthogonal projection of $y(x)$ onto $\\{P_n(x)\\}_{n=0}^{p}$ under the uniform weight on $[-1,1]$. To do this, approximate the required integrals numerically using Gaussianâ€“Legendre quadrature with at least $M=500$ nodes over $[-1,1]$.\n3. Independently, generate $N$ independent and identically distributed samples from a Beta distribution on $[0,1]$ with shape parameters $(a,b)$, then transform them to $[-1,1]$ via $X = 2U - 1$. Use these inputs to compute output samples $Y = y(X)$, form the design matrix with columns $P_n(X)$ for $n=0,\\ldots,p$, and compute the unweighted least squares estimate $\\widehat{\\boldsymbol{c}}$ solving the normal equations implicitly via a numerically stable solver. Use a fixed random seed $s=12345$ for reproducibility.\n4. Report the scalar bias magnitude defined as the Euclidean norm\n$$\\|\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}\\|_2 = \\left(\\sum_{n=0}^{p} \\left(\\widehat{c}_n - c_n^{\\star}\\right)^2\\right)^{1/2}.$$\n\nImplement the following test suite of parameter values to cover different facets:\n- Case $1$ (happy path, pronounced mismatch): $y(x)=\\exp(x)$, $p=3$, $N=400$, $(a,b)=(2,5)$.\n- Case $2$ (no mismatch in distribution): $y(x)=\\exp(x)$, $p=3$, $N=400$, $(a,b)=(1,1)$ (uniform on $[-1,1]$ after transformation).\n- Case $3$ (different nonlinearity and mismatch): $y(x)=\\sin(x)$ with $x$ in radians, $p=5$, $N=800$, $(a,b)=(5,2)$.\n- Case $4$ (model well-specified within span): $y(x)=1 + 0.2\\,x + 0.3\\,x^2$, $p=5$, $N=200$, $(a,b)=(2,5)$.\n\nYour program should produce a single line of output containing the bias magnitudes for the four cases as a comma-separated list of floats rounded to six decimal places, enclosed in square brackets, in the same order as above, for example\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$.\nNo user input is required. All angles are in radians. No physical units are involved.", "solution": "The problem requires an empirical investigation into the bias of Polynomial Chaos Expansion (PCE) coefficients that arises when the sampling distribution of the input data does not match the orthogonality weight function of the chosen polynomial basis. We will compare coefficients derived from two distinct methods: a theoretically optimal orthogonal projection and a data-driven least squares regression.\n\nThe basis functions are the Legendre polynomials $\\{P_n(x)\\}_{n=0}^{\\infty}$, which are orthogonal on the interval $[-1, 1]$ with respect to the uniform weight function, $w(x) = 1$. The orthogonality condition is given by:\n$$\n\\int_{-1}^{1} P_m(x) P_n(x) \\,dx = \\frac{2}{2n+1}\\delta_{mn}\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta.\n\nA function $y(x)$ can be approximated by a truncated PCE of order $p$:\n$$\ny(x) \\approx y_p(x) = \\sum_{n=0}^{p} c_n P_n(x)\n$$\n\nWe will compute two sets of coefficients for this expansion, $\\boldsymbol{c}^{\\star}$ and $\\widehat{\\boldsymbol{c}}$.\n\n**1. Orthogonal Projection Coefficients ($\\boldsymbol{c}^{\\star}$)**\n\nThe coefficients $\\boldsymbol{c}^{\\star} = (c_0^{\\star}, \\dots, c_p^{\\star})$ are determined by projecting the function $y(x)$ onto the basis $\\{P_n(x)\\}_{n=0}^{p}$ in the function space $L^2([-1,1])$. The coefficients that minimize the weighted squared error $\\int_{-1}^{1} [y(x) - y_p(x)]^2 w(x) \\,dx$ with $w(x)=1$ are given by the orthogonal projection formula:\n$$\nc_n^{\\star} = \\frac{\\langle y, P_n \\rangle}{\\langle P_n, P_n \\rangle} = \\frac{\\int_{-1}^{1} y(x) P_n(x) \\,dx}{\\int_{-1}^{1} P_n(x)^2 \\,dx}\n$$\nSubstituting the known value of the inner product $\\langle P_n, P_n \\rangle = \\frac{2}{2n+1}$, we obtain the precise formula for each coefficient:\n$$\nc_n^{\\star} = \\frac{2n+1}{2} \\int_{-1}^{1} y(x) P_n(x) \\,dx\n$$\nTo compute these coefficients, we must numerically approximate the integral. The problem specifies using Gaussian-Legendre quadrature with $M \\geq 500$ nodes. Let $\\{x_i\\}_{i=1}^M$ be the quadrature nodes and $\\{w_i\\}_{i=1}^M$ be the corresponding weights on the interval $[-1, 1]$. The integral is then approximated as a weighted sum:\n$$\n\\int_{-1}^{1} f(x) \\,dx \\approx \\sum_{i=1}^{M} w_i f(x_i)\n$$\nApplying this to our coefficient formula, we get the numerical estimate:\n$$\nc_n^{\\star} \\approx \\frac{2n+1}{2} \\sum_{i=1}^{M} w_i y(x_i) P_n(x_i)\n$$\nThese coefficients serve as the \"true\" or ideal coefficients for a PCE based on Legendre polynomials.\n\n**2. Least Squares Regression Coefficients ($\\widehat{\\boldsymbol{c}}$)**\n\nThe second set of coefficients, $\\widehat{\\boldsymbol{c}} = (\\widehat{c}_0, \\dots, \\widehat{c}_p)$, is estimated from a finite set of $N$ input-output samples, $\\{(X_j, Y_j)\\}_{j=1}^N$. The input samples $X_j$ are not drawn from a uniform distribution on $[-1, 1]$. Instead, they are generated by drawing $U_j$ from a Beta distribution with parameters $(a,b)$ on $[0,1]$ and then applying an affine transformation $X_j = 2U_j - 1$. The output samples are $Y_j = y(X_j)$.\n\nThe coefficients $\\widehat{\\boldsymbol{c}}$ are found by solving an unweighted linear least squares problem. We seek to minimize the sum of squared residuals:\n$$\n\\min_{\\widehat{\\boldsymbol{c}}} \\sum_{j=1}^{N} \\left( Y_j - \\sum_{n=0}^{p} \\widehat{c}_n P_n(X_j) \\right)^2\n$$\nThis can be expressed in matrix form as minimizing $\\|\\boldsymbol{Y} - \\boldsymbol{\\Psi}\\widehat{\\boldsymbol{c}}\\|_2^2$, where:\n- $\\boldsymbol{Y} = [Y_1, Y_2, \\dots, Y_N]^T$ is the vector of output samples.\n- $\\boldsymbol{\\Psi}$ is the $N \\times (p+1)$ design matrix with entries $\\Psi_{jn} = P_n(X_j)$.\n- $\\widehat{\\boldsymbol{c}} = [\\widehat{c}_0, \\widehat{c}_1, \\dots, \\widehat{c}_p]^T$ is the vector of coefficients to be determined.\n\nThe solution is found by solving the normal equations, $(\\boldsymbol{\\Psi}^T \\boldsymbol{\\Psi}) \\widehat{\\boldsymbol{c}} = \\boldsymbol{\\Psi}^T \\boldsymbol{Y}$. For numerical stability, this is best solved using methods based on QR decomposition or singular value decomposition, as implemented in standard numerical libraries.\n\n**3. The Source of Bias**\n\nThe least squares estimate $\\widehat{\\boldsymbol{c}}$ converges to the coefficients that are optimal with respect to the actual sampling distribution of $X$. If the probability density function of $X$ is $f_X(x)$, the least squares procedure is a discrete approximation of minimizing the error in a weighted $L^2$-norm with weight $f_X(x)$.\n\nA systematic bias $\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}$ arises precisely because the weight function of the sampling distribution, $f_X(x)$, does not match the weight function for which the Legendre basis is orthogonal, $w(x) = 1$. The only exception is Case $2$, where the Beta distribution parameters are $(a,b)=(1,1)$, which corresponds to a uniform distribution on $[0,1]$. After transformation, this yields a uniform distribution on $[-1,1]$, so $f_X(x) \\propto w(x)$, and the bias is expected to be minimal, attributable only to finite sample variance. For other cases where $(a,b) \\neq (1,1)$, a non-uniform sampling density $f_X(x)$ is produced, leading to a demonstrable systematic bias.\n\nThe magnitude of this bias is quantified by the Euclidean norm of the difference vector:\n$$\n\\|\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}\\|_2 = \\sqrt{\\sum_{n=0}^{p} (\\widehat{c}_n - c_n^{\\star})^2}\n$$\n\n**Algorithm Summary**\nThe program will execute the following steps for each of the four test cases:\n1.  Define the target function $y(x)$ and parameters $p, N, a, b$.\n2.  Compute the vector $\\boldsymbol{c}^{\\star}$ by applying the Gaussian-Legendre quadrature formula with $M=500$ nodes.\n3.  Set the random seed to $s=12345$ for reproducibility.\n4.  Generate $N$ samples from the Beta$(a,b)$ distribution, transform them to the interval $[-1,1]$, and evaluate $y(x)$ to get the sample pairs.\n5.  Construct the design matrix $\\boldsymbol{\\Psi}$ using the generated input samples and evaluations of Legendre polynomials up to order $p$.\n6.  Solve the linear least squares problem $\\boldsymbol{Y} \\approx \\boldsymbol{\\Psi}\\widehat{\\boldsymbol{c}}$ to find the coefficient vector $\\widehat{\\boldsymbol{c}}$.\n7.  Calculate the Euclidean norm $\\|\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}\\|_2$.\n8.  Collect the results and format the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\nfrom numpy.polynomial.legendre import leggauss\n\ndef compute_c_star(y_func, p: int, M: int) -> np.ndarray:\n    \"\"\"\n    Computes PCE coefficients via orthogonal projection using Gaussian-Legendre quadrature.\n\n    Args:\n        y_func: The function y(x) to be expanded.\n        p: The maximum order of the PCE.\n        M: The number of quadrature nodes to use.\n\n    Returns:\n        A numpy array containing the coefficients c_star.\n    \"\"\"\n    nodes, weights = leggauss(M)\n    c_star = np.zeros(p + 1)\n    \n    # Evaluate the function y at all quadrature nodes once\n    y_vals = y_func(nodes)\n    \n    for n in range(p + 1):\n        # Evaluate the nth Legendre polynomial at the nodes\n        p_n_vals = eval_legendre(n, nodes)\n        \n        # Approximate the integral using quadrature\n        integrand_values = y_vals * p_n_vals\n        integral = np.sum(weights * integrand_values)\n        \n        # Apply the projection formula\n        c_star[n] = (2 * n + 1) / 2.0 * integral\n        \n    return c_star\n\ndef compute_c_hat(y_func, p: int, N: int, a: float, b: float, seed: int) -> np.ndarray:\n    \"\"\"\n    Computes PCE coefficients via unweighted least squares from samples.\n\n    Args:\n        y_func: The function y(x) to generate samples.\n        p: The maximum order of the PCE.\n        N: The number of samples.\n        a: The alpha parameter of the Beta distribution.\n        b: The beta parameter of the Beta distribution.\n        seed: The random seed for reproducibility.\n\n    Returns:\n        A numpy array containing the estimated coefficients c_hat.\n    \"\"\"\n    # Generate samples from the specified distribution\n    rng = np.random.default_rng(seed)\n    U = rng.beta(a, b, size=N)\n    X = 2.0 * U - 1.0  # Transform samples from [0, 1] to [-1, 1]\n    Y = y_func(X)\n    \n    # Construct the design matrix Psi\n    Psi = np.zeros((N, p + 1))\n    for n in range(p + 1):\n        Psi[:, n] = eval_legendre(n, X)\n        \n    # Solve the least squares problem\n    c_hat, _, _, _ = np.linalg.lstsq(Psi, Y, rcond=None)\n    \n    return c_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define constants\n    QUADRATURE_NODES = 500\n    RANDOM_SEED = 12345\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (y_function, order_p, num_samples_N, beta_a, beta_b)\n    test_cases = [\n        (lambda x: np.exp(x), 3, 400, 2, 5),\n        (lambda x: np.exp(x), 3, 400, 1, 1),\n        (lambda x: np.sin(x), 5, 800, 5, 2),\n        (lambda x: 1.0 + 0.2 * x + 0.3 * x**2, 5, 200, 2, 5)\n    ]\n\n    results = []\n    for case in test_cases:\n        y_func, p, N, a, b = case\n        \n        # 1. Compute the theoretical coefficients c_star via quadrature\n        c_star = compute_c_star(y_func, p, QUADRATURE_NODES)\n        \n        # 2. Compute the estimated coefficients c_hat via least squares\n        c_hat = compute_c_hat(y_func, p, N, a, b, RANDOM_SEED)\n        \n        # 3. Calculate the bias magnitude (Euclidean norm of the difference)\n        bias = np.linalg.norm(c_hat - c_star)\n        \n        results.append(bias)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3174362"}]}