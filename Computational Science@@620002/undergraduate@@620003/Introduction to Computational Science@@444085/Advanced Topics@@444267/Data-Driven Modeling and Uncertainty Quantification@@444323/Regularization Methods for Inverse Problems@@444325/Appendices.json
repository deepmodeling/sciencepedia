{"hands_on_practices": [{"introduction": "A primary goal of regularization in many inverse problems is to enforce simplicity. The celebrated $\\ell_1$-norm, or LASSO penalty, achieves this by promoting sparsity, driving many individual coefficients of a solution to exactly zero. However, what if our problem has a known structure where variables naturally belong to groups? This hands-on exercise challenges you to compare the element-wise sparsity of the $\\ell_1$-norm with the group-wise sparsity induced by the group LASSO penalty, revealing how a structured regularizer can better respect the underlying problem physics. [@problem_id:3185666]", "problem": "Consider a linear inverse problem with a known forward operator and unknown coefficients partitioned into predefined, disjoint groups. The goal is to compare two regularization choices for estimating the unknown coefficients: the plain $\\ell_{1}$ norm, which promotes entrywise sparsity, and the group sparsity penalty, which promotes groupwise selection. You must write a complete program that, for the given test suite, solves both regularized inverse problems and reports which method correctly selects entire active groups.\n\nBase problem setup:\n- Forward model: given a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and data $\\mathbf{y} \\in \\mathbb{R}^{m}$, recover $\\mathbf{x} \\in \\mathbb{R}^{n}$ by minimizing an objective of the form\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\, \\mathcal{R}(\\mathbf{x}).\n$$\n- Two choices of regularizer $\\mathcal{R}$:\n  1. Plain $\\ell_{1}$: $\\mathcal{R}(\\mathbf{x}) = \\|\\mathbf{x}\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\n  2. Group sparsity (group lasso): suppose indices are partitioned into $G$ disjoint groups $\\{g_{1},\\dots,g_{G}\\}$; define $\\mathcal{R}(\\mathbf{x}) = \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$, where $\\mathbf{x}_{g}$ is the subvector of $\\mathbf{x}$ restricted to group $g$.\n\nFundamental base and assumptions you must use:\n- Use the least-squares data fidelity $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}$ and the two convex regularizers defined above.\n- In all test cases below, take $\\mathbf{A} = \\mathbf{I}_{n}$, the identity matrix of size $n \\times n$, so that the data are $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$ with no noise. This is a standard and well-tested design that isolates the effect of the regularizer on selection without conflating forward model conditioning.\n- Groups are disjoint and specified explicitly. The success criterion concerns recovery of entire active groups.\n\nDefinitions for evaluation:\n- Let the ground-truth vector be $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^{n}$, partitioned into groups $\\{g_{1},\\dots,g_{G}\\}$. A group $g$ is called active if $\\|\\mathbf{x}_{\\text{true},g}\\|_{2} > 0$, and inactive otherwise.\n- Given an estimate $\\widehat{\\mathbf{x}}$, declare that it selects entire groups correctly if and only if:\n  - For every active group $g$, all entries of $\\widehat{\\mathbf{x}}_{g}$ are nonzero.\n  - For every inactive group $g$, all entries of $\\widehat{\\mathbf{x}}_{g}$ are exactly zero.\n- In numerical computations, treat any entry with absolute value less than a tolerance $\\varepsilon = 10^{-8}$ as zero, and any entry with absolute value greater than or equal to $\\varepsilon$ as nonzero.\n\nYour program must:\n- For each test case, solve both problems\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}$,\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$,\n  using any correct numerical method of your choice, to within a numerical tolerance that ensures stable identification according to the above rule.\n- Decide, for each method and each test case, whether entire active groups are correctly selected.\n- Map the outcome of each test case to an integer code $c \\in \\{0,1,2,3\\}$ as follows:\n  - $c = 0$: neither method selects entire active groups correctly,\n  - $c = 1$: only the plain $\\ell_{1}$ method does,\n  - $c = 2$: only the group sparsity method does,\n  - $c = 3$: both methods do.\n\nTest suite:\n- All cases use $n = 6$, groups $g_{1} = [0,1,2]$, $g_{2} = [3,4,5]$, $\\mathbf{A} = \\mathbf{I}_{6}$, and $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$.\n- Case $1$ (happy path where $\\ell_{1}$ fails but group sparsity succeeds): $\\mathbf{x}_{\\text{true}} = [0.6, 0.6, 0.6, 0, 0, 0]$, $\\lambda = 0.8$.\n- Case $2$ (boundary with very strong regularization): $\\mathbf{x}_{\\text{true}} = [1, 1, 1, 0, 0, 0]$, $\\lambda = 5$.\n- Case $3$ (correlated magnitudes within a group where $\\ell_{1}$ keeps only a subset): $\\mathbf{x}_{\\text{true}} = [1.2, 0.3, 0.2, 0, 0, 0]$, $\\lambda = 0.5$.\n- Case $4$ (weak regularization where both succeed): $\\mathbf{x}_{\\text{true}} = [2, 2, 2, 0, 0, 0]$, $\\lambda = 0.1$.\n\nNumerical and implementation requirements:\n- Angles are not involved; no physical units are involved.\n- Use the numerical tolerance $\\varepsilon = 10^{-8}$ for zero/nonzero decisions.\n- Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, ordered as $[c_{1}, c_{2}, c_{3}, c_{4}]$ for the four test cases.\n\nYour task:\n- Implement the solver, run it on the four specified cases, apply the selection rule, and print the single-line result in the exact format specified. No user input is required and no external data files are allowed.", "solution": "The problem requires a comparison of two regularization methods, the plain $\\ell_1$ norm and the group sparsity norm, for a linear inverse problem. The goal is to determine which method correctly identifies predefined groups of active coefficients. The problem is simplified by setting the forward operator $\\mathbf{A}$ to the identity matrix $\\mathbf{I}$ and providing noise-free data $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$. This setup isolates the behavior of the regularizers' associated proximal operators.\n\nThe two optimization problems to be solved for each test case are:\n1.  **Plain $\\ell_1$ Regularization (LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}\n    $$\n2.  **Group Sparsity Regularization (Group LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g \\in G} \\|\\mathbf{x}_{g}\\|_{2}\n    $$\nwhere $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$, $\\lambda$ is the regularization parameter, and $G$ is the set of predefined, disjoint groups of indices. For all test cases, the vector dimension is $n=6$, and the groups are $g_1$ (indices $\\{0, 1, 2\\}$) and $g_2$ (indices $\\{3, 4, 5\\}$).\n\nThe solutions to these optimization problems are found by applying the proximal operator of the respective regularization term to the data $\\mathbf{y}$.\n\n**Solution for Plain $\\ell_1$ Regularization**\n\nThe objective function is separable with respect to the individual components of $\\mathbf{x}$. The problem can be written as:\n$$\n\\sum_{i=1}^{n} \\min_{x_i \\in \\mathbb{R}} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda |x_i| \\right)\n$$\nThe solution for each component $\\widehat{x}_i$ is given by the soft-thresholding operator:\n$$\n\\widehat{x}_i = \\text{prox}_{\\lambda|\\cdot|}(y_i) = \\text{sign}(y_i) \\max(|y_i| - \\lambda, 0)\n$$\nThis operator thresholds each component individually. A component $y_i$ with magnitude less than $\\lambda$ is set to zero in the solution $\\widehat{x}_i$. This promotes entry-wise sparsity.\n\n**Solution for Group Sparsity Regularization**\n\nThe objective function is separable with respect to the coefficient groups. The problem can be written as:\n$$\n\\sum_{g \\in G} \\min_{\\mathbf{x}_g \\in \\mathbb{R}^{|g|}} \\left( \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{y}_g\\|_{2}^2 + \\lambda \\|\\mathbf{x}_g\\|_{2} \\right)\n$$\nThe solution for each subvector $\\widehat{\\mathbf{x}}_g$ is given by the group (or block) soft-thresholding operator:\n$$\n\\widehat{\\mathbf{x}}_g = \\text{prox}_{\\lambda\\|\\cdot\\|_{2}}(\\mathbf{y}_g) = \\left( 1 - \\frac{\\lambda}{\\|\\mathbf{y}_g\\|_{2}} \\right)_+ \\mathbf{y}_g = \\frac{\\mathbf{y}_g}{\\|\\mathbf{y}_g\\|_{2}} \\max(\\|\\mathbf{y}_g\\|_{2} - \\lambda, 0)\n$$\nwhere $(z)_+ = \\max(z, 0)$. This operator acts on entire groups. If the Euclidean norm of a data subvector, $\\|\\mathbf{y}_g\\|_{2}$, is less than $\\lambda$, the entire corresponding solution subvector $\\widehat{\\mathbf{x}}_g$ is set to the zero vector. Otherwise, the subvector is scaled but its direction is preserved. This promotes group-wise sparsity, meaning either all coefficients in a group are zero, or all can be non-zero.\n\n**Evaluation and Implementation**\n\nFor each test case, we compute the estimates $\\widehat{\\mathbf{x}}_{\\ell_1}$ and $\\widehat{\\mathbf{x}}_{\\text{group}}$. We then evaluate their success based on the provided criterion. A method is successful if, for an estimate $\\widehat{\\mathbf{x}}$:\n1.  For every active group $g$ (where $\\|\\mathbf{x}_{\\text{true},g}\\|_{2} > 0$), all entries of $\\widehat{\\mathbf{x}}_g$ are non-zero.\n2.  For every inactive group $g$ (where $\\|\\mathbf{x}_{\\text{true},g}\\|_{2} = 0$), all entries of $\\widehat{\\mathbf{x}}_g$ are zero.\n\nNumerically, \"zero\" is defined as having an absolute value less than $\\varepsilon = 10^{-8}$.\n\nFor all test cases, $\\mathbf{x}_{\\text{true}, g_1}$ has at least one non-zero component, making $g_1$ the active group. $\\mathbf{x}_{\\text{true}, g_2}$ is the zero vector, making $g_2$ the inactive group.\n- The $\\ell_1$ method will fail the success criterion if $\\lambda$ is large enough to zero-out some, but not all, components of an active group (e.g., if components of $\\mathbf{y}_{g_1}$ have different magnitudes).\n- The group sparsity method is designed to avoid this: it treats each group as a single unit, either zeroing it out completely or keeping it (scaled). Thus, it will either set all of $\\widehat{\\mathbf{x}}_{g_1}$ to zero or keep all its components non-zero (assuming $\\mathbf{y}_{g_1}$ has no zero entries, which is true in the relevant test cases).\n\nThe program will implement these two proximal operators and the evaluation logic, compute the results for the four test cases, and map them to the specified integer codes. The final output will be a list of these codes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularization comparison problem for a given test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x_true_list, lambda_val)\n    test_cases = [\n        # Case 1\n        (np.array([0.6, 0.6, 0.6, 0.0, 0.0, 0.0]), 0.8),\n        # Case 2\n        (np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0]), 5.0),\n        # Case 3\n        (np.array([1.2, 0.3, 0.2, 0.0, 0.0, 0.0]), 0.5),\n        # Case 4\n        (np.array([2.0, 2.0, 2.0, 0.0, 0.0, 0.0]), 0.1),\n    ]\n\n    # Global problem parameters\n    groups = [[0, 1, 2], [3, 4, 5]]\n    epsilon = 1e-8\n\n    def solve_l1(y, lam):\n        \"\"\"\n        Solves the l1-regularized problem using soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * ||x||_1\n        \"\"\"\n        return np.sign(y) * np.maximum(np.abs(y) - lam, 0)\n\n    def solve_group_sparsity(y, lam, groups_list):\n        \"\"\"\n        Solves the group sparsity regularized problem using block soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * sum_g ||x_g||_2\n        \"\"\"\n        x_hat = np.zeros_like(y)\n        for g_indices in groups_list:\n            y_g = y[g_indices]\n            norm_y_g = np.linalg.norm(y_g)\n            \n            if norm_y_g > lam:\n                scaler = (1 - lam / norm_y_g)\n                x_hat[g_indices] = scaler * y_g\n            else:\n                x_hat[g_indices] = 0.0\n        return x_hat\n\n    def check_success(x_hat, x_true, groups_list, tol):\n        \"\"\"\n        Checks if an estimate x_hat correctly selects entire active groups.\n        Success criterion:\n        1. For active groups, all entries of x_hat_g must be nonzero.\n        2. For inactive groups, all entries of x_hat_g must be zero.\n        \"\"\"\n        for g_indices in groups_list:\n            x_true_g = x_true[g_indices]\n            x_hat_g = x_hat[g_indices]\n            \n            # Determine if the ground-truth group is active\n            is_active = np.linalg.norm(x_true_g) > 0\n            \n            if is_active:\n                # All entries must be nonzero\n                if not np.all(np.abs(x_hat_g) >= tol):\n                    return False\n            else: # inactive\n                # All entries must be zero\n                if not np.all(np.abs(x_hat_g) < tol):\n                    return False\n        return True\n\n    results = []\n    for x_true, lam in test_cases:\n        y = x_true  # Data is noise-free\n\n        # Solve and evaluate for plain l1\n        x_hat_l1 = solve_l1(y, lam)\n        l1_succeeds = check_success(x_hat_l1, x_true, groups, epsilon)\n\n        # Solve and evaluate for group sparsity\n        x_hat_group = solve_group_sparsity(y, lam, groups)\n        group_succeeds = check_success(x_hat_group, x_true, groups, epsilon)\n        \n        # Determine the integer code based on success\n        code = 0\n        if l1_succeeds and group_succeeds:\n            code = 3\n        elif group_succeeds:\n            code = 2\n        elif l1_succeeds:\n            code = 1\n        \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3185666"}, {"introduction": "In signal and image processing, a powerful prior is that signals are often piecewise-constant or piecewise-smooth. Total Variation (TV) regularization is exceptionally effective at enforcing this, but often at the cost of introducing \"staircasing\" artifacts, where smooth ramps are converted into a series of flat steps. This practice provides a direct comparison between TV and more sophisticated edge-preserving penalties, such as the Huber and Perona-Malik regularizers, allowing you to quantify their ability to reduce staircasing while faithfully preserving both sharp edges and gentle slopes. [@problem_id:3185682]", "problem": "You are given the task of designing and implementing a numerical experiment to compare regularization penalties for a one-dimensional inverse problem of denoising. The fundamental base is the variational formulation of inverse problems: given noisy observations $y \\in \\mathbb{R}^{N}$ of an unknown clean signal $x^{\\star} \\in \\mathbb{R}^{N}$, a common approach is to compute an estimate $x \\in \\mathbb{R}^{N}$ by minimizing an energy of the form\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\; \\frac{1}{2}\\| x - y \\|_{2}^{2} + \\lambda \\, R(x),\n$$\nwhere $\\lambda > 0$ controls the strength of regularization and $R(x)$ is a penalty encoding prior knowledge on $x$. In this problem, you will compare three penalties that are widely used in the context of regularization methods for inverse problems:\n\n- Total Variation (TV): $R_{\\mathrm{TV}}(x) = \\sum_{i=1}^{N-1} \\lvert x_{i+1} - x_{i} \\rvert$.\n- Huberized Total Variation: $R_{\\mathrm{Huber},\\delta}(x) = \\sum_{i=1}^{N-1} \\phi_{\\delta}(x_{i+1} - x_{i})$ with the Huber function\n$$\n\\phi_{\\delta}(d) =\n\\begin{cases}\n\\frac{1}{2\\delta} d^{2}, & \\text{if } \\lvert d \\rvert \\le \\delta, \\\\\n\\lvert d \\rvert - \\frac{\\delta}{2}, & \\text{if } \\lvert d \\rvert > \\delta,\n\\end{cases}\n$$\nfor parameter $\\delta > 0$.\n- Perona–Malik-type penalty: $R_{\\mathrm{PM},k}(x) = \\sum_{i=1}^{N-1} \\psi_{k}(x_{i+1} - x_{i})$ with\n$$\n\\psi_{k}(d) = \\frac{k^{2}}{2} \\log\\!\\big(1 + (d/k)^{2}\\big),\n$$\nfor parameter $k > 0$.\n\nThe goal is to empirically demonstrate that edge-preserving penalties, such as the Huberized Total Variation and the Perona–Malik-type penalty, can reduce the “staircasing” effect commonly observed with standard Total Variation while avoiding oversmoothing of small gradients.\n\nConstruct a synthetic ground-truth signal $x^{\\star} \\in \\mathbb{R}^{N}$ that contains both a small, nonzero gradient region and a sharp edge:\n- Let $N = 256$.\n- Define a ramp with constant slope $m > 0$ starting at index $i_{0}$ and ending at index $i_{1}$, and a sharp step of amplitude $A > 0$ at an index $j$.\n- Precisely, for indices $i \\in \\{0,1,\\dots,N-1\\}$,\n  - $x^{\\star}_{i} = 0$ for $i < i_{0}$,\n  - $x^{\\star}_{i} = m\\,(i - i_{0})$ for $i_{0} \\le i < i_{1}$,\n  - $x^{\\star}_{i} = m\\,(i_{1} - i_{0})$ for $i_{1} \\le i < j$,\n  - $x^{\\star}_{i} = m\\,(i_{1} - i_{0}) + A$ for $i \\ge j$.\n\nGenerate noisy data $y = x^{\\star} + \\eta$, where $\\eta$ is independent and identically distributed Gaussian noise with zero mean and variance $\\sigma^{2}$.\n\nFor each penalty, compute the numerical minimizer of the corresponding energy with the same data $y$ and regularization parameter $\\lambda$. Use a first-order iterative method appropriate to each penalty:\n- For the non-differentiable Total Variation, use a provably convergent first-order method based on convex duality and projections onto the dual feasible set.\n- For the differentiable penalties $R_{\\mathrm{Huber},\\delta}$ and $R_{\\mathrm{PM},k}$, use a gradient-based descent method with a monotone line search that guarantees energy decrease.\n\nDesign quantitative measures to compare the reconstructions in the ramp region $\\{i_{0}, i_{0}+1, \\dots, i_{1}-1\\}$:\n- Define the discrete gradient $g_{i} = x_{i+1} - x_{i}$ for $i \\in \\{0,1,\\dots,N-2\\}$.\n- In the ramp region, compute the “plateau ratio” $P(x)$ as the fraction of indices $i \\in \\{i_{0},\\dots,i_{1}-2\\}$ for which $\\lvert g_{i} \\rvert < \\tau$, where $\\tau$ is a fixed threshold chosen as a fraction of the true slope $m$.\n- Estimate the ramp slope as the mean of $g_{i}$ over the ramp region and report the absolute slope error $\\lvert \\widehat{m}(x) - m \\rvert$.\n\nYour program must construct the signal, add noise, solve the three minimization problems, and evaluate whether each edge-preserving penalty reduces staircasing without oversmoothing the small gradient, when compared to standard Total Variation, according to the following logical criteria:\n- Let $x^{\\mathrm{TV}}$ denote the Total Variation solution and $x^{\\mathrm{E}}$ denote an edge-preserving solution (either Huberized Total Variation or Perona–Malik-type).\n- “Reduced staircasing” is declared if $P(x^{\\mathrm{E}}) < P(x^{\\mathrm{TV}}) - \\varepsilon_{P}$ for a fixed margin $\\varepsilon_{P} > 0$.\n- “No oversmoothing of small gradients” is declared if $\\lvert \\widehat{m}(x^{\\mathrm{E}}) - m \\rvert < \\lvert \\widehat{m}(x^{\\mathrm{TV}}) - m \\rvert - \\varepsilon_{m}$ for a fixed margin $\\varepsilon_{m} > 0$.\n\nYour program should run the following test suite, where each test defines all parameters and a fixed random seed for reproducibility:\n- Test $1$: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.08$, $\\lambda=0.35$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2026$.\n- Test $2$: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.08$, $\\lambda=0.60$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2027$.\n- Test $3$: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.12$, $\\lambda=0.35$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2028$.\n\nFor each test, produce two boolean results:\n- One boolean for Huberized Total Variation indicating whether both conditions hold relative to Total Variation.\n- One boolean for the Perona–Malik-type penalty indicating whether both conditions hold relative to Total Variation.\n\nFinal output format: Your program should produce a single line of output containing a list of the six boolean results in order of the tests and methods, as a comma-separated list enclosed in square brackets, namely $[\\text{Huber}_{1}, \\text{PM}_{1}, \\text{Huber}_{2}, \\text{PM}_{2}, \\text{Huber}_{3}, \\text{PM}_{3}]$, where each entry is either $\\text{True}$ or $\\text{False}$.", "solution": "The problem presents a well-posed numerical experiment designed to compare the performance of different regularization penalties for one-dimensional signal denoising. The problem is scientifically grounded in the principles of variational inverse problems, self-contained with all necessary parameters and definitions, and its objective is formally verifiable through the specified computational procedure. The problem is therefore deemed valid.\n\nThe core task is to find an estimate $x \\in \\mathbb{R}^{N}$ of a true signal $x^{\\star} \\in \\mathbb{R}^{N}$ from noisy observations $y \\in \\mathbb{R}^{N}$ by minimizing a functional of the form:\n$$\n\\min_{x \\in \\mathbb{R}^{N}} E(x) = \\frac{1}{2}\\| x - y \\|_{2}^{2} + \\lambda \\, R(x)\n$$\nHere, $\\frac{1}{2}\\| x - y \\|_{2}^{2}$ is the data fidelity term, which promotes closeness of the solution $x$ to the measurements $y$. The term $R(x)$ is a regularization penalty, weighted by a parameter $\\lambda > 0$, that encodes prior knowledge about the structure of the signal $x^{\\star}$. We will compare three specific regularizers.\n\nLet $D: \\mathbb{R}^{N} \\to \\mathbb{R}^{N-1}$ be the discrete forward difference operator, such that $(Dx)_i = x_{i+1} - x_i$ for $i \\in \\{0, \\dots, N-2\\}$. The three regularizers are expressed in terms of the discrete gradient $Dx$:\n\n1.  **Total Variation (TV)**: $R_{\\mathrm{TV}}(x) = \\sum_{i=0}^{N-2} \\lvert (Dx)_i \\rvert = \\| Dx \\|_{1}$. This penalty is convex and promotes piecewise-constant solutions, which can lead to an undesirable artifact known as \"staircasing\" in regions of smooth gradients.\n\n2.  **Huberized Total Variation (Huber-TV)**: $R_{\\mathrm{Huber},\\delta}(x) = \\sum_{i=0}^{N-2} \\phi_{\\delta}((Dx)_i)$, where the Huber function $\\phi_{\\delta}$ is defined as:\n    $$\n    \\phi_{\\delta}(d) =\n    \\begin{cases}\n    \\frac{1}{2\\delta} d^{2}, & \\text{if } \\lvert d \\rvert \\le \\delta \\\\\n    \\lvert d \\rvert - \\frac{\\delta}{2}, & \\text{if } \\lvert d \\rvert > \\delta\n    \\end{cases}\n    $$\n    This penalty is also convex. It behaves quadratically for small gradients ($\\lvert d \\rvert \\le \\delta$) and linearly for large gradients, providing a smooth approximation to the $\\ell_1$-norm and aiming to reduce staircasing.\n\n3.  **Perona–Malik-type (PM)**: $R_{\\mathrm{PM},k}(x) = \\sum_{i=0}^{N-2} \\psi_{k}((Dx)_i)$, with the potential function:\n    $$\n    \\psi_{k}(d) = \\frac{k^{2}}{2} \\log\\!\\left(1 + \\left(\\frac{d}{k}\\right)^{2}\\right)\n    $$\n    This penalty is non-convex and is known as an edge-preserving potential. It penalizes large gradients less severely than small ones, which can preserve sharp edges while smoothing noise.\n\n**Numerical Minimization Algorithms**\n\nTo find the minimizer $x$ for each regularizer, we employ appropriate first-order iterative methods.\n\n**Total Variation Minimization**: The energy functional $E(x)$ with the TV regularizer is convex but non-differentiable due to the $\\ell_1$-norm. As suggested, we use a method based on convex duality. The primal problem is $\\min_{x} \\frac{1}{2}\\| x - y \\|_{2}^{2} + \\lambda \\| Dx \\|_{1}$. Its Fenchel-Rockafellar dual problem is to minimize the following with respect to the dual variable $p \\in \\mathbb{R}^{N-1}$:\n$$\n\\min_{p: \\| p \\|_{\\infty} \\le 1} \\frac{1}{2} \\| y - \\lambda D^{*}p \\|_{2}^{2}\n$$\nwhere $D^{*}: \\mathbb{R}^{N-1} \\to \\mathbb{R}^{N}$ is the adjoint of the forward difference operator (negative divergence). This is a differentiable quadratic optimization problem over a simple convex set (an infinity-norm ball). We solve it using projected gradient descent. The iterative update for $p$ is:\n$$\np^{k+1} = \\text{proj}_{\\| \\cdot \\|_{\\infty} \\le 1} \\left( p^{k} - \\alpha_{k} \\nabla_{p} f(p^k) \\right)\n$$\nwhere $f(p) = \\frac{1}{2} \\| y - \\lambda D^{*}p \\|_{2}^{2}$ and its gradient is $\\nabla_{p}f(p) = -\\lambda D(y - \\lambda D^{*}p)$. The projection $\\text{proj}_{\\| \\cdot \\|_{\\infty} \\le 1}(q)$ is a simple element-wise clipping of the vector $q$ to the interval $[-1, 1]$. The step size $\\alpha_k$ must be chosen to ensure convergence; a constant step size $\\alpha < 2/L$ is sufficient, where $L$ is the Lipschitz constant of $\\nabla_p f(p)$, which is $L = \\lambda^2 \\| DD^{*} \\|_{2}$. For the 1D case, $\\| DD^{*} \\|_{2} \\le 4$. Once the optimal dual variable $p^{\\star}$ is found, the primal solution is recovered via the relation $x^{\\star} = y - \\lambda D^{*}p^{\\star}$.\n\n**Huber-TV and PM Minimization**: The energy functionals for Huber-TV and PM penalties are differentiable. Therefore, we can use a gradient descent method. The gradient of the energy functional $E(x)$ is:\n$$\n\\nabla E(x) = (x - y) + \\lambda \\nabla R(x)\n$$\nThe gradient of the regularizer, $\\nabla R(x)$, can be expressed compactly as $\\nabla R(x) = D^{*}\\left( \\rho'(Dx) \\right)$, where $\\rho'$ is the derivative of the potential function ($\\phi'_{\\delta}$ or $\\psi'_k$) applied element-wise to the vector of differences $Dx$. The derivatives are:\n- For Huber-TV: $\\phi'_{\\delta}(d) = \\text{clip}(d/\\delta, -1, 1)$.\n- For PM: $\\psi'_{k}(d) = \\frac{d}{1 + (d/k)^2}$.\nThe gradient descent update is $x^{k+1} = x^{k} - \\alpha_{k} \\nabla E(x^{k})$. To satisfy the requirement of a monotone line search, the step size $\\alpha_k$ is determined at each iteration using a backtracking line search. We start with a trial step size $\\alpha$ and reduce it by a factor $\\beta \\in (0,1)$ until the Armijo condition is met:\n$$\nE(x^k - \\alpha \\nabla E(x^k)) \\le E(x^k) - c \\alpha \\| \\nabla E(x^k) \\|_{2}^{2}\n$$\nfor a constant $c \\in (0,1)$, typically $c=10^{-4}$. This guarantees that the energy decreases at each step, leading to convergence to a (local) minimum.\n\n**Evaluation and Comparison**\n\nA synthetic signal $x^{\\star}$ is constructed with a ramp (small, constant gradient) and a sharp step. Gaussian noise is added to form the observation $y$. After computing the reconstructions $x^{\\mathrm{TV}}$, $x^{\\mathrm{Huber}}$, and $x^{\\mathrm{PM}}$, we evaluate them based on two quantitative metrics computed over the ramp region:\n1.  **Plateau Ratio $P(x)$**: The fraction of gradients in the ramp segment whose magnitude is close to zero, specifically smaller than a threshold $\\tau$. A high value indicates significant staircasing.\n    $P(x) = \\frac{|\\{ i \\in \\{i_{0}, \\dots, i_{1}-2\\} \\,:\\, |x_{i+1}-x_i| < \\tau \\}|}{i_1 - i_0 - 1}$\n2.  **Absolute Slope Error**: The absolute difference between the true ramp slope $m$ and the mean estimated slope $\\widehat{m}(x)$ from the reconstruction in the ramp region.\n    $\\widehat{m}(x) = \\frac{1}{i_1 - i_0 - 1}\\sum_{i=i_0}^{i_1-2} (x_{i+1}-x_i)$\n\nAn edge-preserving penalty (Huber or PM) is deemed to exhibit superior performance relative to TV if it simultaneously achieves:\n- **Reduced Staircasing**: $P(x^{\\mathrm{E}}) < P(x^{\\mathrm{TV}}) - \\varepsilon_{P}$\n- **No Oversmoothing of Small Gradients**: $\\lvert \\widehat{m}(x^{\\mathrm{E}}) - m \\rvert < \\lvert \\widehat{m}(x^{\\mathrm{TV}}) - m \\rvert - \\varepsilon_{m}$\nwhere $x^{\\mathrm{E}}$ is the reconstruction from the edge-preserving penalty, and $\\varepsilon_P, \\varepsilon_m$ are small positive margins. The following implementation carries out this comparison for the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy used\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment comparing regularization penalties.\n    \"\"\"\n\n    test_cases = [\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.08, 'lambda_reg': 0.35, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2026},\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.08, 'lambda_reg': 0.60, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2027},\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.12, 'lambda_reg': 0.35, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2028},\n    ]\n\n    # --- Finite Difference Operators ---\n    def D_op(x):\n        \"\"\"Forward difference operator D.\"\"\"\n        return x[1:] - x[:-1]\n\n    def DT_op(p):\n        \"\"\"Adjoint of D (negative divergence).\"\"\"\n        N = p.shape[0] + 1\n        res = np.zeros(N, dtype=p.dtype)\n        res[1:] += p\n        res[:-1] -= p\n        return res\n\n    # --- Signal Generation ---\n    def generate_signal(N, i0, i1, j, m, A):\n        x_star = np.zeros(N)\n        indices = np.arange(N)\n        ramp_val = m * (i1 - i0)\n        \n        # Ramp region\n        mask_ramp = (indices >= i0) & (indices < i1)\n        x_star[mask_ramp] = m * (indices[mask_ramp] - i0)\n        \n        # Plateau after ramp\n        mask_plateau = (indices >= i1) & (indices < j)\n        x_star[mask_plateau] = ramp_val\n        \n        # Step region\n        mask_step = (indices >= j)\n        x_star[mask_step] = ramp_val + A\n        \n        return x_star\n\n    # --- Solvers ---\n    MAX_ITER = 1000\n\n    def solve_tv(y, lambda_reg):\n        \"\"\"Solves the TV denoising problem using dual projected gradient ascent.\"\"\"\n        N = len(y)\n        p = np.zeros(N - 1)\n        # Spectral norm of D is ~2, so ||DD*|| is ~4.\n        # Step size < 2 / (lambda^2 * ||DD*||) = 1/(2*lambda^2)\n        step_size = 0.4 / (lambda_reg**2) \n\n        for _ in range(MAX_ITER):\n            grad = -lambda_reg * D_op(y - lambda_reg * DT_op(p))\n            p_new = p - step_size * grad\n            p = np.clip(p_new, -1.0, 1.0)\n            \n        x_rec = y - lambda_reg * DT_op(p)\n        return x_rec\n\n    def solve_differentiable(y, lambda_reg, reg_type, param):\n        \"\"\"Solves denoising for differentiable regularizers (Huber, PM) using gradient descent.\"\"\"\n        x = y.copy()\n        N = len(y)\n\n        # Backtracking line search parameters\n        alpha_init = 1.0\n        beta = 0.5\n        c_armijo = 1e-4\n\n        for _ in range(MAX_ITER):\n            d = D_op(x)\n            \n            # Compute gradient of regularizer\n            if reg_type == 'huber':\n                grad_R_term = np.clip(d / param, -1.0, 1.0)\n            elif reg_type == 'pm':\n                grad_R_term = d / (1 + (d / param)**2)\n            else:\n                raise ValueError(\"Unknown regularization type\")\n\n            grad_E = (x - y) + lambda_reg * DT_op(grad_R_term)\n\n            # Compute current energy\n            if reg_type == 'huber':\n                mask = np.abs(d) <= param\n                R_val = np.sum(0.5 / param * d[mask]**2) + np.sum(np.abs(d[~mask]) - 0.5 * param)\n            elif reg_type == 'pm':\n                R_val = np.sum(0.5 * param**2 * np.log(1 + (d / param)**2))\n            \n            E_current = 0.5 * np.sum((x - y)**2) + lambda_reg * R_val\n            \n            # Backtracking line search\n            alpha = alpha_init\n            grad_E_norm_sq = np.sum(grad_E**2)\n            \n            while True:\n                x_new = x - alpha * grad_E\n                d_new = D_op(x_new)\n\n                if reg_type == 'huber':\n                    mask_new = np.abs(d_new) <= param\n                    R_val_new = np.sum(0.5 / param * d_new[mask_new]**2) + np.sum(np.abs(d_new[~mask_new]) - 0.5 * param)\n                elif reg_type == 'pm':\n                    R_val_new = np.sum(0.5 * param**2 * np.log(1 + (d_new / param)**2))\n                \n                E_new = 0.5 * np.sum((x_new - y)**2) + lambda_reg * R_val_new\n                \n                if E_new <= E_current - c_armijo * alpha * grad_E_norm_sq:\n                    break\n                alpha *= beta\n                if alpha < 1e-9: # Failsafe\n                    break\n\n            x = x - alpha * grad_E\n\n        return x\n\n    # --- Evaluation Metrics ---\n    def evaluate_reconstruction(x, i0, i1, m, tau):\n        \"\"\"Calculates plateau ratio and slope error.\"\"\"\n        ramp_grads = D_op(x[i0:i1])\n        num_grads = len(ramp_grads)\n        \n        # Plateau ratio\n        plateau_count = np.sum(np.abs(ramp_grads) < tau)\n        plateau_ratio = plateau_count / num_grads if num_grads > 0 else 0.0\n        \n        # Slope error\n        m_hat = np.mean(ramp_grads) if num_grads > 0 else 0.0\n        slope_error = np.abs(m_hat - m)\n        \n        return plateau_ratio, slope_error\n\n    # --- Main Loop ---\n    final_results = []\n    for params in test_cases:\n        # Set parameters for the current test\n        N, i0, i1, j = params['N'], params['i0'], params['i1'], params['j']\n        m, A, sigma = params['m'], params['A'], params['sigma']\n        lambda_reg, delta, k = params['lambda_reg'], params['delta'], params['k']\n        tau = params['tau_frac'] * m\n        eps_p, eps_m = params['eps_p'], params['eps_m']\n        seed = params['seed']\n        \n        # Generate signal and noise\n        np.random.seed(seed)\n        x_star = generate_signal(N, i0, i1, j, m, A)\n        noise = np.random.normal(0, sigma, N)\n        y = x_star + noise\n        \n        # Solve for all three regularizers\n        x_tv = solve_tv(y, lambda_reg)\n        x_hub = solve_differentiable(y, lambda_reg, 'huber', delta)\n        x_pm = solve_differentiable(y, lambda_reg, 'pm', k)\n        \n        # Evaluate metrics\n        p_tv, m_err_tv = evaluate_reconstruction(x_tv, i0, i1, m, tau)\n        p_hub, m_err_hub = evaluate_reconstruction(x_hub, i0, i1, m, tau)\n        p_pm, m_err_pm = evaluate_reconstruction(x_pm, i0, i1, m, tau)\n        \n        # Apply comparison criteria\n        hub_better = (p_hub < p_tv - eps_p) and (m_err_hub < m_err_tv - eps_m)\n        pm_better = (p_pm < p_tv - eps_p) and (m_err_pm < m_err_tv - eps_m)\n        \n        final_results.extend([hub_better, pm_better])\n        \n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3185682"}, {"introduction": "After developing a regularized method, how can we reliably assess its performance? A common but dangerous pitfall is the 'inverse crime': using the exact same numerical model to generate synthetic test data and to perform the inversion, which often leads to deceptively optimistic results. This final practice illuminates this critical methodological issue, demonstrating quantitatively how committing an inverse crime can inflate performance and how introducing a deliberate mesh mismatch provides a more honest and robust evaluation of your algorithm. [@problem_id:3185734]", "problem": "Consider a one-dimensional linear inverse problem on the periodic domain $[0,1)$ where the unknown signal $x(t)$ is blurred by a known kernel and observed with additive noise. The forward model is linear, given by $y = A x + \\varepsilon$, where $A$ is a discretized convolution operator, $y$ is the observed data, and $\\varepsilon$ is additive noise. You will implement zero-order Tikhonov regularization to reconstruct $x$ and quantitatively evaluate how using the same discretization grid for both data generation and inversion (an inverse crime) artificially inflates performance, and how a mesh mismatch mitigates this effect.\n\nUse the following fundamental definitions and modeling assumptions:\n\n- Discretization: For an integer $n$, define a uniform grid with spacing $\\Delta t = 1/n$ and sample $x$ at points $t_i = i/n$ for $i \\in \\{0,1,\\dots,n-1\\}$. Denote the resulting vector by $x_n \\in \\mathbb{R}^n$.\n- True signal: Define the continuous function\n  $$x_{\\mathrm{true}}(t) = 0.5 \\exp\\!\\left(-\\frac{(t-0.3)^2}{2 \\cdot 0.05^2}\\right) + 1.0 \\exp\\!\\left(-\\frac{(t-0.72)^2}{2 \\cdot 0.03^2}\\right) + 0.2 \\sin(2\\pi \\cdot 7 \\cdot t),$$\n  and let $x_n$ be its pointwise samples on the grid of size $n$.\n- Periodic Gaussian blur kernel: For a grid of size $n$, define a discrete, normalized, periodic Gaussian kernel $h_n \\in \\mathbb{R}^n$ by\n  $$h_n[k] = \\frac{\\exp\\!\\left(-\\frac{d(k)^2}{2 \\sigma^2}\\right)}{\\sum_{j=0}^{n-1} \\exp\\!\\left(-\\frac{d(j)^2}{2 \\sigma^2}\\right)}, \\quad d(k) = \\min\\!\\left(\\frac{k}{n}, 1 - \\frac{k}{n}\\right), \\quad k \\in \\{0,1,\\dots,n-1\\},$$\n  with $\\sigma = 0.03$. This yields a circular convolution operator $A_n \\in \\mathbb{R}^{n \\times n}$ that applies $h_n$ by circular convolution.\n- Noise model: For noise-free data $y \\in \\mathbb{R}^m$, add noise $\\varepsilon$ so that the relative Euclidean norm of the noise equals a prescribed level $\\eta$, that is, $\\|\\varepsilon\\|_{2} = \\eta \\|y\\|_{2}$. Use $\\eta = 0.02$.\n- Zero-order Tikhonov regularization: Given a model matrix $B \\in \\mathbb{R}^{m \\times n}$ and data $z \\in \\mathbb{R}^{m}$, the reconstruction $x^\\ast \\in \\mathbb{R}^{n}$ is the minimizer of\n  $$\\min_{x \\in \\mathbb{R}^{n}} \\|B x - z\\|_{2}^{2} + \\lambda^2 \\|x\\|_{2}^{2},$$\n  where $\\lambda = 0.01$.\n\nMesh mismatch to mitigate inverse crime:\n\n- Inverse crime occurs if the same discretization and the same forward operator used to generate synthetic data are reused in the inverse step. To mitigate this, generate data on a finer grid $n_f$ and form coarse measurements by block-averaging onto a coarse grid $n_c$ with $n_f/n_c \\in \\mathbb{N}$. Then invert on the coarse grid $n_c$ with its own coarse operator $A_{n_c}$, not with an exactly projected fine operator.\n- Block-averaging restriction: For $n_f = r \\cdot n_c$ with integer $r$, map $y_{n_f} \\in \\mathbb{R}^{n_f}$ to $y_{n_c} \\in \\mathbb{R}^{n_c}$ by averaging contiguous blocks:\n  $$(R y_{n_f})[i] = \\frac{1}{r} \\sum_{j=0}^{r-1} y_{n_f}[r \\cdot i + j], \\quad i \\in \\{0,1,\\dots,n_c-1\\}.$$\n\nTasks:\n\n- Implement the forward operator $A_n$ as a circulant matrix that produces circular convolution by $h_n$.\n- Implement the noise addition to achieve relative noise level $\\eta$ in the Euclidean norm.\n- Implement the zero-order Tikhonov solution by solving the associated normal equations.\n\nEvaluation protocol:\n\n- Case $1$ (inverse crime): Use $n_f = 64$, $n_c = 64$. Generate $x_{64}$ from $x_{\\mathrm{true}}$, synthesize noise-free data $y = A_{64} x_{64}$, add noise at level $\\eta$, and reconstruct on the same grid using $A_{64}$ with the same discretization.\n- Case $2$ (mesh mismatch): Use $n_f = 128$, $n_c = 64$. Generate $x_{128}$ and $y_{128} = A_{128} x_{128}$, add noise at level $\\eta$, block-average to $y_{64}$ using $r = 2$, and reconstruct on the coarse grid using $A_{64}$.\n- Case $3$ (stronger mesh mismatch): Use $n_f = 256$, $n_c = 64$. Generate $x_{256}$ and $y_{256} = A_{256} x_{256}$, add noise at level $\\eta$, block-average to $y_{64}$ using $r = 4$, and reconstruct on the coarse grid using $A_{64}$.\n\nFor each case, compute the relative reconstruction error\n$$e = \\frac{\\|x^\\ast - x_{n_c}\\|_{2}}{\\|x_{n_c}\\|_{2}},$$\nwhere $x_{n_c}$ is the true signal sampled on the coarse grid of size $n_c$.\n\nTest suite and output:\n\n- Use the three cases specified above with $\\sigma = 0.03$, $\\eta = 0.02$, $\\lambda = 0.01$, and a fixed pseudorandom seed of $1729$ for noise generation.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[e_{\\text{crime}}, e_{128 \\to 64}, e_{256 \\to 64}]$, with each float rounded to exactly $6$ decimal places.", "solution": "The problem requires an investigation into the \"inverse crime\" phenomenon in the context of a one-dimensional deconvolution problem solved with Tikhonov regularization. An inverse crime occurs when the numerical model used to generate synthetic data for testing an inversion algorithm is identical to the model used within the inversion algorithm itself. This can lead to an artificially optimistic assessment of the algorithm's performance. We will quantify this effect by comparing a case where an inverse crime is committed against two cases where a mesh mismatch is introduced to mitigate it.\n\nThe forward problem is modeled by the linear equation $y = A x + \\varepsilon$. Here, $x \\in \\mathbb{R}^n$ represents the discretized unknown signal, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $\\varepsilon \\in \\mathbb{R}^m$ is additive noise, and $y \\in \\mathbb{R}^m$ is the observed data.\n\nFirst, we define the components of the discrete forward model. The continuous true signal is given by:\n$$x_{\\mathrm{true}}(t) = 0.5 \\exp\\!\\left(-\\frac{(t-0.3)^2}{2 \\cdot 0.05^2}\\right) + 1.0 \\exp\\!\\left(-\\frac{(t-0.72)^2}{2 \\cdot 0.03^2}\\right) + 0.2 \\sin(2\\pi \\cdot 7 \\cdot t)$$\nThis signal is sampled on a uniform grid of size $n$ over the domain $[0,1)$ at points $t_i = i/n$ for $i \\in \\{0, 1, \\dots, n-1\\}$, yielding the vector $x_n \\in \\mathbb{R}^n$.\n\nThe forward operator $A_n$ represents a circular convolution with a discrete, periodic Gaussian blur kernel $h_n \\in \\mathbb{R}^n$. The kernel is defined for a given grid size $n$ and standard deviation $\\sigma = 0.03$ as:\n$$h_n[k] = \\frac{\\exp\\!\\left(-\\frac{d(k)^2}{2 \\sigma^2}\\right)}{Z}, \\quad \\text{where} \\quad Z = \\sum_{j=0}^{n-1} \\exp\\!\\left(-\\frac{d(j)^2}{2 \\sigma^2}\\right)$$\nThe function $d(k) = \\min\\!\\left(\\frac{k}{n}, 1 - \\frac{k}{n}\\right)$ represents the periodic distance on the grid. The normalization factor $Z$ ensures that the kernel sums to one. The operator $A_n \\in \\mathbb{R}^{n \\times n}$ is constructed as a circulant matrix whose first column is the kernel vector $h_n$. The action $A_n x_n$ is equivalent to the circular convolution $h_n * x_n$.\n\nTo simulate realistic measurements, additive noise $\\varepsilon$ is introduced. A random noise vector is generated, and its magnitude is scaled to match a specific relative noise level $\\eta = 0.02$. For noise-free data $y_{\\text{clean}}$, the noise vector $\\varepsilon$ is constructed such that its Euclidean norm satisfies $\\|\\varepsilon\\|_{2} = \\eta \\|y_{\\text{clean}}\\|_{2}$. A fixed pseudorandom seed of $1729$ ensures reproducibility.\n\nThe inverse problem is to recover an estimate of the signal $x$ from the noisy data $y$. Since convolution is a smoothing operation, its inverse (deconvolution) is ill-posed, meaning small errors in the data $y$ can lead to large errors in the reconstructed signal. To stabilize the solution, we employ zero-order Tikhonov regularization. The regularized solution $x^\\ast$ is the vector that minimizes the objective function:\n$$J(x) = \\|B x - z\\|_{2}^{2} + \\lambda^2 \\|x\\|_{2}^{2}$$\nHere, $z$ is the measurement vector, $B$ is the forward operator used for the inversion, and $\\lambda = 0.01$ is the regularization parameter that balances data fidelity (the first term) and solution smoothness (the second term, which penalizes the norm of $x$). The minimizer of $J(x)$ is found by solving the associated normal equations:\n$$(B^T B + \\lambda^2 I) x = B^T z$$\nwhere $I$ is the identity matrix. The solution is given by $x^\\ast = (B^T B + \\lambda^2 I)^{-1} B^T z$. We solve this system of linear equations for $x^\\ast$.\n\nWe examine three cases to study the inverse crime. In all cases, the reconstruction is performed on a coarse grid of size $n_c=64$ using the operator $A_{64}$. The \"true\" signal for comparison is the continuous signal sampled on this grid, $x_{64}$.\n\nCase 1 (Inverse Crime): Data is generated and inverted on the same grid.\n- $n_f = 64$, $n_c = 64$.\n- Generate $x_{64}$ and compute $y_{64} = A_{64} x_{64}$.\n- Add noise to get $z = y_{64,\\text{noisy}}$.\n- Reconstruct $x^\\ast$ using $B=A_{64}$ and data $z$.\n\nCase 2 (Mesh Mismatch): Data is generated on a finer grid ($n_f=128$) and then downsampled to the coarse grid ($n_c=64$) before inversion.\n- $n_f = 128$, $n_c = 64$. The ratio is $r = n_f/n_c = 2$.\n- Generate $x_{128}$ and $y_{128} = A_{128} x_{128}$.\n- Add noise to get $y_{128,\\text{noisy}}$.\n- Generate coarse data $z$ by block-averaging: $z[i] = \\frac{1}{2} \\sum_{j=0}^{1} y_{128, \\text{noisy}}[2i + j]$.\n- Reconstruct $x^\\ast$ using $B=A_{64}$ and data $z$. This avoids the inverse crime as the data generation model ($A_{128}$ and block averaging) differs from the inversion model ($A_{64}$).\n\nCase 3 (Stronger Mesh Mismatch): Similar to Case 2, but with an even finer grid for data generation ($n_f=256$).\n- $n_f = 256$, $n_c = 64$. The ratio is $r = n_f/n_c = 4$.\n- Generate $x_{256}$ and $y_{256} = A_{256} x_{256}$.\n- Add noise to get $y_{256,\\text{noisy}}$.\n- Generate coarse data $z$ by block-averaging with $r=4$.\n- Reconstruct $x^\\ast$ using $B=A_{64}$ and data $z$.\n\nFor each case, the performance is evaluated by the relative reconstruction error:\n$$e = \\frac{\\|x^\\ast - x_{n_c}\\|_{2}}{\\|x_{n_c}\\|_{2}}$$\nwhere $x_{n_c} = x_{64}$ is the true signal sampled on the coarse grid. A lower error in Case 1 compared to Cases 2 and 3 would indicate an artificially optimistic result due to the inverse crime.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import circulant\n\ndef solve():\n    \"\"\"\n    Solves the inverse problem for three cases to demonstrate the 'inverse crime'.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    SIGMA = 0.03\n    ETA = 0.02\n    LAMBDA = 0.01\n    SEED = 1729\n    \n    # --- Helper Functions ---\n    def get_true_signal(n):\n        \"\"\"Generates the true signal x_true(t) sampled on a grid of size n.\"\"\"\n        t = np.linspace(0.0, 1.0, n, endpoint=False)\n        term1 = 0.5 * np.exp(-((t - 0.3)**2) / (2 * 0.05**2))\n        term2 = 1.0 * np.exp(-((t - 0.72)**2) / (2 * 0.03**2))\n        term3 = 0.2 * np.sin(2 * np.pi * 7 * t)\n        return term1 + term2 + term3\n\n    def get_kernel(n, sigma):\n        \"\"\"Computes the discrete periodic Gaussian kernel h_n.\"\"\"\n        k = np.arange(n)\n        d = np.minimum(k / n, 1 - k / n)\n        h_unnormalized = np.exp(-(d**2) / (2 * sigma**2))\n        h = h_unnormalized / np.sum(h_unnormalized)\n        return h\n\n    def block_average(y_fine, r):\n        \"\"\"Performs block-averaging from a fine grid to a coarse grid.\"\"\"\n        n_fine = len(y_fine)\n        n_coarse = n_fine // r\n        # Reshape into (n_coarse, r) and take the mean along axis 1\n        return y_fine.reshape(n_coarse, r).mean(axis=1)\n\n    # --- Main Logic ---\n    rng = np.random.default_rng(seed=SEED)\n    \n    test_cases = [\n        (64, 64),  # Case 1: Inverse crime\n        (128, 64), # Case 2: Mesh mismatch\n        (256, 64), # Case 3: Stronger mesh mismatch\n    ]\n    \n    results = []\n\n    # Get the common coarse grid operator and true signal once\n    n_c_fixed = 64\n    x_true_coarse = get_true_signal(n_c_fixed)\n    h_coarse = get_kernel(n_c_fixed, SIGMA)\n    A_coarse = circulant(h_coarse)\n\n    for n_f, n_c in test_cases:\n        # Step 1: Generate data on the fine grid (n_f)\n        x_true_fine = get_true_signal(n_f)\n        h_fine = get_kernel(n_f, SIGMA)\n        A_fine = circulant(h_fine)\n        y_fine_clean = A_fine @ x_true_fine\n        \n        # Step 2: Add noise\n        noise_vec = rng.standard_normal(size=n_f)\n        noise_norm = np.linalg.norm(noise_vec)\n        y_clean_norm = np.linalg.norm(y_fine_clean)\n        \n        # Scale noise to the desired relative level\n        epsilon = ETA * (y_clean_norm / noise_norm) * noise_vec\n        y_fine_noisy = y_fine_clean + epsilon\n\n        # Step 3: Prepare data for inversion\n        if n_f == n_c:\n            # Case 1: No downsampling needed\n            z = y_fine_noisy\n        else:\n            # Cases 2 & 3: Apply block-averaging\n            r = n_f // n_c\n            z = block_average(y_fine_noisy, r)\n\n        # Step 4: Perform Tikhonov regularization on the coarse grid (n_c)\n        B = A_coarse\n        # Solve the normal equations: (B^T B + lambda^2 I) x = B^T z\n        lhs = B.T @ B + (LAMBDA**2) * np.identity(n_c)\n        rhs = B.T @ z\n        x_reconstructed = np.linalg.solve(lhs, rhs)\n\n        # Step 5: Calculate relative reconstruction error\n        error_norm = np.linalg.norm(x_reconstructed - x_true_coarse)\n        true_norm = np.linalg.norm(x_true_coarse)\n        relative_error = error_norm / true_norm\n        results.append(relative_error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "3185734"}]}