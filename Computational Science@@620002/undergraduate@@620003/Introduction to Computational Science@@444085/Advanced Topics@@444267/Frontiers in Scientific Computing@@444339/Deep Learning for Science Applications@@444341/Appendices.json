{"hands_on_practices": [{"introduction": "Standard deep learning models are powerful function approximators but are often agnostic to the underlying physical laws governing the systems they model. A key technique in scientific machine learning is to design custom loss functions that penalize violations of these principles. This exercise guides you through implementing a \"physics-informed\" loss to enforce monotonicity, a critical property for functions like Cumulative Distribution Functions (CDFs), ensuring your model's predictions are not just accurate but also mathematically sound. [@problem_id:3116982]", "problem": "You are modeling a Cumulative Distribution Function (CDF) as a function $\\hat{F}_{\\theta}(x)$ using a simple differentiable parametric form that mimics an unconstrained neural network. In scientific applications, the learned CDF must satisfy the fundamental property of monotonicity: for a real-valued random variable $X$, the Cumulative Distribution Function (CDF) $F(x) = \\mathbb{P}(X \\le x)$ is non-decreasing, and when differentiable its derivative $F'(x)$ equals the Probability Density Function (PDF) $f(x)$, which obeys $f(x) \\ge 0$. This implies the monotonicity constraint $F'(x) \\ge 0$ almost everywhere. In many deep learning architectures used for science applications, unconstrained models may violate this monotonicity, so we seek a principled loss that penalizes violations.\n\nStarting from the core definitions above, design a loss that comprises:\n- A data fidelity term between the learned CDF $\\hat{F}_{\\theta}(x)$ and an empirical CDF $\\tilde{F}(x)$ computed from sample data.\n- A monotonicity penalty that enforces the constraint $F'(x) \\ge 0$ by penalizing negative discrete slopes of $\\hat{F}_{\\theta}(x)$ on a grid.\n\nUse the following foundations and definitions:\n- The empirical CDF $\\tilde{F}(x)$ for a sample $\\{s_j\\}_{j=1}^m$ is defined by $\\tilde{F}(x) = \\frac{1}{m}\\sum_{j=1}^{m} \\mathbf{1}\\{s_j \\le x\\}$.\n- Let a grid $\\{x_i\\}_{i=1}^{n}$ with $x_1 < x_2 < \\cdots < x_n$ be given. Define the discrete slope $D_i = \\frac{\\hat{F}_{\\theta}(x_{i+1}) - \\hat{F}_{\\theta}(x_i)}{x_{i+1} - x_i}$ for $i \\in \\{1,\\dots,n-1\\}$.\n- Define the Rectified Linear Unit (ReLU) by $\\operatorname{ReLU}(t) = \\max(0,t)$.\n\nYour task:\n- Implement a program that, for each provided test case, computes the total loss\n$$\nL(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{F}_{\\theta}(x_i) - \\tilde{F}(x_i)\\right)^2 \\;+\\; \\lambda \\sum_{i=1}^{n-1}\\left(\\operatorname{ReLU}\\!\\left(-D_i\\right)\\right)^2,\n$$\nwhere $\\lambda$ is a nonnegative penalty weight.\n- The parametric CDF model is specified as\n$$\n\\hat{F}_{\\theta}(x) = \\sigma\\!\\left(\\alpha \\sin(2\\pi x) + \\beta x + \\gamma\\right),\n$$\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid, and $\\theta = (\\alpha,\\beta,\\gamma)$ are real parameters. This form is bounded in $[0,1]$ but not guaranteed to be monotone, making it suitable for testing the penalty.\n\nAlgorithmic specifications:\n- Compute $\\tilde{F}(x_i)$ on the grid directly from the given sample set by counting the proportion of sample points less than or equal to $x_i$.\n- Compute the mean-squared error term and the monotonicity penalty as specified above.\n- No physical units are involved.\n- Angles in trigonometric functions must be in radians.\n- Each test case specifies $(\\theta, \\lambda)$, a grid $\\{x_i\\}_{i=1}^{n}$, and a sample set for computing $\\tilde{F}(x)$.\n\nTest suite:\n- Use grid points $\\{x_i\\}_{i=1}^{n}$ uniformly spaced on $[0,1]$ unless otherwise stated.\n- Case one (general monotone trend, happy path):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$ for $i \\in \\{1,\\dots,21\\}$.\n  - Sample set (uniform-like): $\\{0.05, 0.12, 0.18, 0.22, 0.31, 0.37, 0.41, 0.48, 0.53, 0.60, 0.66, 0.74, 0.80, 0.86, 0.92\\}$.\n  - Parameters: $\\alpha = 0.0$, $\\beta = 8.0$, $\\gamma = 0.0$.\n  - Penalty weight: $\\lambda = 10.0$.\n- Case two (non-monotone oscillations):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$.\n  - Sample set: same as case one.\n  - Parameters: $\\alpha = 1.0$, $\\beta = 0.5$, $\\gamma = 0.0$.\n  - Penalty weight: $\\lambda = 10.0$.\n- Case three (negative local slopes with stronger penalty):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$.\n  - Sample set (more concentrated near zero): $\\{0.01, 0.03, 0.07, 0.10, 0.14, 0.18, 0.25, 0.35, 0.50, 0.70, 0.85, 0.95\\}$.\n  - Parameters: $\\alpha = 0.6$, $\\beta = -0.1$, $\\gamma = -0.2$.\n  - Penalty weight: $\\lambda = 50.0$.\n- Case four (constant model, boundary scenario for monotonicity):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$.\n  - Sample set: same as case three.\n  - Parameters: $\\alpha = 0.0$, $\\beta = 0.0$, $\\gamma = 0.0$.\n  - Penalty weight: $\\lambda = 10.0$.\n- Case five (minimal grid, duplicates in empirical sample):\n  - Grid: $n = 2$ points $\\{0.0, 1.0\\}$.\n  - Sample set: $\\{0.2, 0.2, 0.2, 0.8, 0.8\\}$.\n  - Parameters: $\\alpha = -0.8$, $\\beta = 0.7$, $\\gamma = 0.5$.\n  - Penalty weight: $\\lambda = 5.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the five cases in order, rounded to six decimal places (for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$).", "solution": "The problem is valid. It is scientifically grounded in the principles of probability theory and numerical optimization, well-posed with all necessary information provided, and objective in its formulation. It presents a standard task in scientific machine learning: designing a loss function to enforce a physical or mathematical constraint (monotonicity of a Cumulative Distribution Function) on a parametric model.\n\nThe task is to compute the total loss $L(\\theta)$ for five distinct test cases. The loss function is composed of two parts: a data fidelity term and a monotonicity penalty.\n$$\nL(\\theta) = \\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{F}_{\\theta}(x_i) - \\tilde{F}(x_i)\\right)^2}_{\\text{Mean Squared Error (MSE)}} \\;+\\; \\underbrace{\\lambda \\sum_{i=1}^{n-1}\\left(\\operatorname{ReLU}\\!\\left(-D_i\\right)\\right)^2}_{\\text{Monotonicity Penalty}}\n$$\nWe will systematically compute each component for every test case.\n\nThe parametric model for the CDF is given by $\\hat{F}_{\\theta}(x) = \\sigma\\!\\left(\\alpha \\sin(2\\pi x) + \\beta x + \\gamma\\right)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid function and $\\theta = (\\alpha, \\beta, \\gamma)$.\n\nThe process for each test case is as follows:\n1.  Define the grid of points $\\{x_i\\}_{i=1}^{n}$ and the sample set $\\{s_j\\}_{j=1}^{m}$.\n2.  Compute the values of the empirical CDF, $\\tilde{F}(x_i)$, at each grid point $x_i$. By definition, $\\tilde{F}(x_i) = \\frac{1}{m}\\sum_{j=1}^{m} \\mathbf{1}\\{s_j \\le x_i\\}$, which is the fraction of samples less than or equal to $x_i$.\n3.  Compute the values of the parametric model CDF, $\\hat{F}_{\\theta}(x_i)$, at each grid point $x_i$ using the given parameters $\\theta = (\\alpha, \\beta, \\gamma)$.\n4.  Calculate the MSE term: This is the mean of the squared differences between $\\hat{F}_{\\theta}(x_i)$ and $\\tilde{F}(x_i)$ over all grid points.\n$$\n\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{F}_{\\theta}(x_i) - \\tilde{F}(x_i)\\right)^2\n$$\n5.  Calculate the monotonicity penalty term:\n    a. First, compute the discrete slopes $D_i$ for $i \\in \\{1, \\dots, n-1\\}$:\n    $$\n    D_i = \\frac{\\hat{F}_{\\theta}(x_{i+1}) - \\hat{F}_{\\theta}(x_i)}{x_{i+1} - x_i}\n    $$\n    b. Next, penalize any negative slopes. The penalty for each interval is $\\left(\\operatorname{ReLU}(-D_i)\\right)^2$, where $\\operatorname{ReLU}(t) = \\max(0, t)$. This term is non-zero only if $D_i < 0$.\n    c. The total penalty is the weighted sum over all intervals:\n    $$\n    \\text{Penalty} = \\lambda \\sum_{i=1}^{n-1}\\left(\\operatorname{ReLU}(-D_i)\\right)^2\n    $$\n6.  The total loss $L(\\theta)$ is the sum of the MSE and the Penalty.\n\nWe now apply this procedure to each test case.\n\n**Case 1:**\n- Grid: $n=21$ points on $[0,1]$, $x_i = \\frac{i-1}{20}$.\n- Sample set: $m=15$ points $\\{0.05, \\dots, 0.92\\}$.\n- Parameters: $\\theta = (\\alpha=0.0, \\beta=8.0, \\gamma=0.0)$.\n- Penalty weight: $\\lambda = 10.0$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(8x)$. The derivative of the argument $8x$ is $8 > 0$. Since $\\sigma$ is a strictly increasing function, $\\hat{F}_{\\theta}(x)$ is guaranteed to be monotonic. Therefore, all discrete slopes $D_i$ will be positive. The term $\\operatorname{ReLU}(-D_i)$ will be $0$ for all $i$, resulting in a monotonicity penalty of $0$. The total loss will be solely the MSE between $\\sigma(8x_i)$ and the computed $\\tilde{F}(x_i)$.\n\n**Case 2:**\n- Grid, Sample set, $\\lambda$ are the same as in Case 1.\n- Parameters: $\\theta = (\\alpha=1.0, \\beta=0.5, \\gamma=0.0)$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(\\sin(2\\pi x) + 0.5x)$. The sinusoidal term $\\sin(2\\pi x)$ introduces oscillations. The derivative of the argument is $2\\pi \\cos(2\\pi x) + 0.5$, which can be negative (e.g., near $x=0.5$). This will lead to intervals with negative slopes $D_i$, resulting in a non-zero monotonicity penalty. The total loss will be the sum of the MSE and this penalty term.\n\n**Case 3:**\n- Grid: $n=21$ points on $[0,1]$.\n- Sample set: $m=12$ points $\\{0.01, \\dots, 0.95\\}$.\n- Parameters: $\\theta = (\\alpha=0.6, \\beta=-0.1, \\gamma=-0.2)$.\n- Penalty weight: $\\lambda = 50.0$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(0.6\\sin(2\\pi x) - 0.1x - 0.2)$. The negative coefficient $\\beta=-0.1$ contributes to a decreasing trend, making negative slopes more likely and more pronounced. Consequently, we expect a significant non-zero monotonicity penalty, which is amplified by the larger weight $\\lambda=50.0$.\n\n**Case 4:**\n- Grid, Sample set are the same as in Case 3.\n- Parameters: $\\theta = (\\alpha=0.0, \\beta=0.0, \\gamma=0.0)$.\n- Penalty weight: $\\lambda = 10.0$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(0) = 0.5$. This is a constant function. The discrete slopes $D_i$ are all identically $0$. The term $\\operatorname{ReLU}(-D_i) = \\operatorname{ReLU}(0) = 0$. The monotonicity penalty will be $0$. The total loss will be the MSE between the constant value $0.5$ and the empirical CDF $\\tilde{F}(x_i)$.\n\n**Case 5:**\n- Grid: $n=2$ points, $x_1=0.0, x_2=1.0$.\n- Sample set: $m=5$ points $\\{0.2, 0.2, 0.2, 0.8, 0.8\\}$.\n- Parameters: $\\theta = (\\alpha=-0.8, \\beta=0.7, \\gamma=0.5)$.\n- Penalty weight: $\\lambda = 5.0$.\nThere is only one interval, from $x_1=0.0$ to $x_2=1.0$.\nThe empirical CDF values are $\\tilde{F}(0.0) = \\frac{0}{5} = 0.0$ and $\\tilde{F}(1.0) = \\frac{5}{5} = 1.0$.\nThe model values are $\\hat{F}_{\\theta}(0.0) = \\sigma(-0.8\\sin(0) + 0.7(0) + 0.5) = \\sigma(0.5)$ and $\\hat{F}_{\\theta}(1.0) = \\sigma(-0.8\\sin(2\\pi) + 0.7(1) + 0.5) = \\sigma(1.2)$.\nThe MSE term is $\\frac{1}{2}\\left((\\sigma(0.5) - 0.0)^2 + (\\sigma(1.2) - 1.0)^2\\right)$.\nThere is only one discrete slope, $D_1 = \\frac{\\hat{F}_{\\theta}(1.0) - \\hat{F}_{\\theta}(0.0)}{1.0-0.0} = \\sigma(1.2) - \\sigma(0.5)$. Since $\\sigma$ is increasing and $1.2 > 0.5$, $D_1$ is positive.\nTherefore, $\\operatorname{ReLU}(-D_1) = 0$, and the monotonicity penalty is $0$. The total loss is just the MSE term.\n\nThe implementation will follow this logic for each case, performing the numerical calculations as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total loss for a parametric CDF model across five test cases.\n    The loss includes a data fidelity term and a monotonicity penalty.\n    \"\"\"\n\n    test_cases = [\n        # Case one\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.05, 0.12, 0.18, 0.22, 0.31, 0.37, 0.41, 0.48, 0.53, 0.60, 0.66, 0.74, 0.80, 0.86, 0.92]),\n            \"params\": {\"alpha\": 0.0, \"beta\": 8.0, \"gamma\": 0.0},\n            \"lambda_val\": 10.0\n        },\n        # Case two\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.05, 0.12, 0.18, 0.22, 0.31, 0.37, 0.41, 0.48, 0.53, 0.60, 0.66, 0.74, 0.80, 0.86, 0.92]),\n            \"params\": {\"alpha\": 1.0, \"beta\": 0.5, \"gamma\": 0.0},\n            \"lambda_val\": 10.0\n        },\n        # Case three\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.01, 0.03, 0.07, 0.10, 0.14, 0.18, 0.25, 0.35, 0.50, 0.70, 0.85, 0.95]),\n            \"params\": {\"alpha\": 0.6, \"beta\": -0.1, \"gamma\": -0.2},\n            \"lambda_val\": 50.0\n        },\n        # Case four\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.01, 0.03, 0.07, 0.10, 0.14, 0.18, 0.25, 0.35, 0.50, 0.70, 0.85, 0.95]),\n            \"params\": {\"alpha\": 0.0, \"beta\": 0.0, \"gamma\": 0.0},\n            \"lambda_val\": 10.0\n        },\n        # Case five\n        {\n            \"grid_points\": np.array([0.0, 1.0]),\n            \"samples\": np.array([0.2, 0.2, 0.2, 0.8, 0.8]),\n            \"params\": {\"alpha\": -0.8, \"beta\": 0.7, \"gamma\": 0.5},\n            \"lambda_val\": 5.0\n        }\n    ]\n\n    results = []\n    \n    # Define helper functions based on the problem statement\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def f_hat(x, alpha, beta, gamma):\n        z = alpha * np.sin(2.0 * np.pi * x) + beta * x + gamma\n        return sigmoid(z)\n\n    def relu(t):\n        return np.maximum(0, t)\n\n    for case in test_cases:\n        # Step 1: Define grid and samples\n        if \"grid_points\" in case:\n            x_grid = case[\"grid_points\"]\n        else:\n            x_grid = np.linspace(case[\"grid_lims\"][0], case[\"grid_lims\"][1], case[\"grid_n\"])\n        \n        samples = case[\"samples\"]\n        n = len(x_grid)\n        m = len(samples)\n        \n        params = case[\"params\"]\n        alpha, beta, gamma = params[\"alpha\"], params[\"beta\"], params[\"gamma\"]\n        lambda_val = case[\"lambda_val\"]\n\n        # Step 2: Compute empirical CDF F_tilde\n        # For each x_i in x_grid, count how many samples are <= x_i\n        counts = np.sum(samples <= x_grid[:, np.newaxis], axis=1)\n        f_tilde_values = counts / m\n        \n        # Step 3: Compute parametric CDF F_hat\n        f_hat_values = f_hat(x_grid, alpha, beta, gamma)\n\n        # Step 4: Calculate the MSE term\n        mse_term = np.mean((f_hat_values - f_tilde_values)**2)\n\n        # Step 5: Calculate the monotonicity penalty term\n        # a. Compute discrete slopes D_i\n        if n > 1:\n            delta_f_hat = np.diff(f_hat_values)\n            delta_x = np.diff(x_grid)\n            # Avoid division by zero, though not expected in these cases\n            # A small tolerance epsilon could be added for robustness\n            discrete_slopes = delta_f_hat / delta_x\n            \n            # b. Apply ReLU to negative slopes and square\n            penalty_per_interval = relu(-discrete_slopes)**2\n            \n            # c. Sum and weight by lambda\n            monotonicity_penalty = lambda_val * np.sum(penalty_per_interval)\n        else:\n            monotonicity_penalty = 0.0\n\n        # Step 6: Compute total loss\n        total_loss = mse_term + monotonicity_penalty\n        results.append(total_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3116982"}, {"introduction": "Beyond modifying the loss function, we can design neural network architectures that inherently respect physical symmetries, a concept known as building in an \"inductive bias.\" This practice explores equivariance, a cornerstone of geometric deep learning, by constructing a simple message-passing network for molecular property prediction. You will verify that the model's output is invariant to rotations, demonstrating how the architecture correctly understands that a molecule's energy does not change when it is rotated in space. [@problem_id:3117017]", "problem": "You are asked to design and implement a minimal, rotation-equivariant message passing scheme that produces a rotation-invariant scalar prediction for molecular configurations. The goal is to encode the scientific requirement that scalar observables like energy are invariant under any rotation from the Special Orthogonal group (SO) in three dimensions, denoted as $\\mathrm{SO}(3)$.\n\nStarting from fundamental definitions, a rotation matrix $\\mathbf{R} \\in \\mathrm{SO}(3)$ satisfies $\\mathbf{R}^\\top \\mathbf{R} = \\mathbf{I}$ and $\\det(\\mathbf{R}) = 1$. A scalar physical property such as energy $E$ must satisfy $E(\\mathbf{R}\\mathbf{r}) = E(\\mathbf{r})$ for all $\\mathbf{R} \\in \\mathrm{SO}(3)$, where $\\mathbf{r} = (\\mathbf{r}_1,\\dots,\\mathbf{r}_N)$ are atom coordinates in $\\mathbb{R}^3$.\n\nYou will implement a single-layer equivariant message passing mechanism and a readout that produces an invariant energy-like scalar. The program must follow this specification:\n\n1) Input representation. A molecule is a set of $N$ atoms with positions $\\mathbf{r}_i \\in \\mathbb{R}^3$ and integer types $Z_i \\in \\mathbb{Z}_{\\ge 1}$ (use atomic numbers). Define pairwise relative vectors $\\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$, distances $d_{ij} = \\|\\mathbf{r}_{ij}\\|_2$, and unit directions $\\hat{\\mathbf{r}}_{ij} = \\mathbf{r}_{ij}/d_{ij}$. To avoid division by zero, if $d_{ij} \\le \\delta$ for a very small $\\delta > 0$, set $\\hat{\\mathbf{r}}_{ij} = \\mathbf{0}$ by convention.\n\n2) Message passing (vector-valued, rotation equivariant). Define a scalar, distance- and type-dependent coefficient\n$$\ns_{ij} = \\exp(-\\beta\\, d_{ij})\\left(a_0 + a_1 Z_i + a_2 Z_j + a_3 Z_i Z_j\\right),\n$$\nand compute a node-wise vector message\n$$\n\\mathbf{m}_i = \\sum_{j \\ne i} s_{ij}\\, \\hat{\\mathbf{r}}_{ij}.\n$$\nHere $\\beta, a_0, a_1, a_2, a_3$ are fixed real constants.\n\n3) Invariant readout (scalar energy-like prediction). Define a per-node scalar\n$$\ne_i = c_1 \\|\\mathbf{m}_i\\|_2^2 + c_2 \\sum_{j \\ne i} \\exp(-\\gamma\\, d_{ij}),\n$$\nand the total prediction\n$$\nE(\\mathbf{r}, \\mathbf{Z}) = \\sum_{i=1}^N e_i + c_3 \\sum_{i=1}^N Z_i,\n$$\nwith fixed real constants $c_1, c_2, c_3, \\gamma$.\n\n4) Equivariance/invariance requirement. The message $\\mathbf{m}_i$ must transform as a vector under $\\mathbf{R} \\in \\mathrm{SO}(3)$, and the final $E(\\mathbf{r}, \\mathbf{Z})$ must satisfy $E(\\mathbf{R}\\mathbf{r}, \\mathbf{Z}) = E(\\mathbf{r}, \\mathbf{Z})$.\n\n5) Rotation generation. Construct $\\mathbf{R}$ from a given axis-angle $(\\hat{\\mathbf{a}}, \\theta)$ using Rodrigues’ formula\n$$\n\\mathbf{R} = \\cos\\theta\\, \\mathbf{I} + \\sin\\theta\\, [\\hat{\\mathbf{a}}]_\\times + (1-\\cos\\theta)\\, \\hat{\\mathbf{a}}\\hat{\\mathbf{a}}^\\top,\n$$\nwhere $[\\hat{\\mathbf{a}}]_\\times$ is the skew-symmetric matrix of the unit axis $\\hat{\\mathbf{a}}$ and $\\theta$ is an angle in radians.\n\nConstants to use in your program:\n- $\\delta = 10^{-12}$,\n- $a_0 = 0.5$, $a_1 = 0.1$, $a_2 = -0.05$, $a_3 = 0.02$,\n- $\\beta = 1.3$, $\\gamma = 0.9$,\n- $c_1 = 0.7$, $c_2 = 0.2$, $c_3 = 0.05$.\n\nNumerical tolerance for equality:\n- Two floating-point predictions $x$ and $y$ are considered equal if $|x-y| \\le \\tau$ with $\\tau = 10^{-10}$.\n\nAngle unit:\n- All angles $\\theta$ are specified in radians.\n\nTest suite. Your program must hard-code the following four test cases, each consisting of positions $\\mathbf{r}$ (in any consistent length unit), integer types $\\mathbf{Z}$, and a rotation axis-angle $(\\hat{\\mathbf{a}}, \\theta)$:\n\n- Case $1$ (general, non-degenerate triangle): $\\mathbf{Z} = [8, 1, 1]$, positions\n  $$\n  \\mathbf{r}_1 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_2 = (0.9572, 0.0, 0.0),\\;\n  \\mathbf{r}_3 = (-0.2399872, 0.927297, 0.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} \\propto (1.0, 2.0, 3.0)$ normalized to unit length, angle $\\theta = 1.234$.\n\n- Case $2$ (linear triatomic, $180^\\circ$ rotation): $\\mathbf{Z} = [6, 1, 1]$, positions\n  $$\n  \\mathbf{r}_1 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_2 = (1.1, 0.0, 0.0),\\;\n  \\mathbf{r}_3 = (-1.1, 0.0, 0.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} = (0.0, 0.0, 1.0)$, angle $\\theta = \\pi$.\n\n- Case $3$ (single atom): $\\mathbf{Z} = [8]$, position\n  $$\n  \\mathbf{r}_1 = (1.0, 2.0, 3.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} = (0.0, 0.0, 1.0)$, angle $\\theta = 0.7$.\n\n- Case $4$ (degenerate, coincident positions to stress-test $\\delta$ handling): $\\mathbf{Z} = [1, 1, 1]$, positions\n  $$\n  \\mathbf{r}_1 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_2 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_3 = (0.0, 0.0, 0.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} = (0.0, 1.0, 0.0)$, angle $\\theta = 2.2$.\n\nFor each case, compute $E(\\mathbf{r}, \\mathbf{Z})$ and $E(\\mathbf{R}\\mathbf{r}, \\mathbf{Z})$, then compare them using the tolerance $\\tau$. The expected outcome is a boolean per case indicating whether rotational invariance holds within tolerance.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example\n$[b_1,b_2,b_3,b_4]$\nwhere each $b_k$ is a boolean value for case $k$.", "solution": "The problem requires the design and verification of a computational model for a scalar molecular property that is, by construction, invariant under three-dimensional rotations. This is a foundational concept in physics-informed machine learning, where incorporating known physical symmetries into a model's architecture improves its accuracy, data efficiency, and generalizability. The symmetry group of interest is the Special Orthogonal group in three dimensions, $\\mathrm{SO}(3)$, which represents all proper rotations.\n\nA scalar property $E$ is said to be rotation-invariant if for any rotation matrix $\\mathbf{R} \\in \\mathrm{SO}(3)$ and any configuration of atomic positions $\\mathbf{r} = (\\mathbf{r}_1, \\dots, \\mathbf{r}_N)$, the property remains unchanged: $E(\\mathbf{R}\\mathbf{r}, \\mathbf{Z}) = E(\\mathbf{r}, \\mathbf{Z})$, where $\\mathbf{Z}$ represents the invariant atomic types. We will demonstrate that the prescribed model architecture guarantees this invariance by analyzing the transformation properties of each component.\n\nThe design relies on the concepts of equivariance and invariance. A vector-valued function $f(\\mathbf{x})$ is equivariant if $f(\\mathbf{R}\\mathbf{x}) = \\mathbf{R}f(\\mathbf{x})$, meaning its output transforms like a vector. A scalar-valued function $g(\\mathbf{x})$ is invariant if $g(\\mathbf{R}\\mathbf{x}) = g(\\mathbf{x})$. The model is constructed by a sequence of operations that maintain these properties.\n\n**1. Input Features and Transformation Properties**\n\nThe model begins with fundamental geometric quantities derived from the atomic positions $\\mathbf{r}_i \\in \\mathbb{R}^3$. Under a rotation $\\mathbf{R}$, a position vector transforms as $\\mathbf{r}'_i = \\mathbf{R}\\mathbf{r}_i$.\n\n- **Relative Position Vectors**: For any pair of atoms $i$ and $j$, the relative position vector is $\\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$. These vectors are equivariant because $\\mathbf{r}'_{ij} = \\mathbf{r}'_j - \\mathbf{r}'_i = \\mathbf{R}\\mathbf{r}_j - \\mathbf{R}\\mathbf{r}_i = \\mathbf{R}(\\mathbf{r}_j - \\mathbf{r}_i) = \\mathbf{R}\\mathbf{r}_{ij}$.\n\n- **Pairwise Distances**: The distance $d_{ij} = \\|\\mathbf{r}_{ij}\\|_2$ is the Euclidean norm. Since rotations are isometries (length-preserving), distances are invariant: $d'_{ij} = \\|\\mathbf{r}'_{ij}\\|_2 = \\|\\mathbf{R}\\mathbf{r}_{ij}\\|_2 = \\|\\mathbf{r}_{ij}\\|_2 = d_{ij}$.\n\n- **Unit Direction Vectors**: Defined as $\\hat{\\mathbf{r}}_{ij} = \\mathbf{r}_{ij} / d_{ij}$ (for $d_{ij} > \\delta$), these vectors are equivariant: $\\hat{\\mathbf{r}}'_{ij} = \\mathbf{r}'_{ij} / d'_{ij} = (\\mathbf{R}\\mathbf{r}_{ij}) / d_{ij} = \\mathbf{R}(\\mathbf{r}_{ij} / d_{ij}) = \\mathbf{R}\\hat{\\mathbf{r}}_{ij}$. The special case where $d_{ij} \\le \\delta$ sets $\\hat{\\mathbf{r}}_{ij} = \\mathbf{0}$, which is a fixed point of rotation ($\\mathbf{R}\\mathbf{0} = \\mathbf{0}$).\n\nThe atomic types $Z_i$ are scalars that do not depend on position and are thus inherently invariant.\n\n**2. Equivariant Message Passing**\n\nThe model computes an intermediate vector representation $\\mathbf{m}_i$ for each atom $i$ through a message passing mechanism. This stage is designed to be equivariant.\n\n- **Invariant Message Coefficient ($s_{ij}$)**: The strength of the interaction between atoms $i$ and $j$ is modulated by a scalar coefficient $s_{ij} = \\exp(-\\beta\\, d_{ij})\\left(a_0 + a_1 Z_i + a_2 Z_j + a_3 Z_i Z_j\\right)$. This coefficient is a function of only invariant quantities ($d_{ij}$, $Z_i$, $Z_j$) and fixed constants ($a_k$, $\\beta$), making $s_{ij}$ itself an invariant scalar.\n\n- **Equivariant Vector Message ($\\mathbf{m}_i$)**: The message for atom $i$ is a weighted sum of the equivariant direction vectors pointing to its neighbors:\n$$\n\\mathbf{m}_i = \\sum_{j \\ne i} s_{ij}\\, \\hat{\\mathbf{r}}_{ij}\n$$\nSince this is a linear combination of equivariant vectors ($\\hat{\\mathbf{r}}_{ij}$) with invariant scalar weights ($s_{ij}$), the resulting vector $\\mathbf{m}_i$ is also equivariant. Its transformation under rotation is:\n$$\n\\mathbf{m}'_i = \\sum_{j \\ne i} s'_{ij}\\, \\hat{\\mathbf{r}}'_{ij} = \\sum_{j \\ne i} s_{ij}\\, (\\mathbf{R}\\hat{\\mathbf{r}}_{ij}) = \\mathbf{R} \\left(\\sum_{j \\ne i} s_{ij}\\, \\hat{\\mathbf{r}}_{ij}\\right) = \\mathbf{R}\\mathbf{m}_i\n$$\n\n**3. Invariant Readout and Total Prediction**\n\nThe final stage maps the set of equivariant vectors $\\{\\mathbf{m}_i\\}$ to a single, total invariant scalar $E$.\n\n- **Per-Node Invariant Scalar ($e_i$)**: An invariant scalar is first computed for each node.\n$$\ne_i = c_1 \\|\\mathbf{m}_i\\|_2^2 + c_2 \\sum_{j \\ne i} \\exp(-\\gamma\\, d_{ij})\n$$\nThe term $\\|\\mathbf{m}_i\\|_2^2$ is the squared norm of the equivariant vector $\\mathbf{m}_i$. The norm of an equivariant vector is always invariant, as $\\|\\mathbf{R}\\mathbf{v}\\|_2^2 = (\\mathbf{R}\\mathbf{v})^\\top(\\mathbf{R}\\mathbf{v}) = \\mathbf{v}^\\top\\mathbf{R}^\\top\\mathbf{R}\\mathbf{v} = \\mathbf{v}^\\top\\mathbf{I}\\mathbf{v} = \\|\\mathbf{v}\\|_2^2$. The second term is a sum of functions of the invariant distance $d_{ij}$, and is thus also invariant. Consequently, $e_i$, being a linear combination of invariant terms, is invariant.\n\n- **Total Invariant Prediction ($E$)**: The final prediction is the sum of these per-node invariants and an invariant atomic-type contribution.\n$$\nE(\\mathbf{r}, \\mathbf{Z}) = \\sum_{i=1}^N e_i + c_3 \\sum_{i=1}^N Z_i\n$$\nAs a sum of invariant quantities, the total prediction $E$ is guaranteed to be rotation-invariant, fulfilling the problem's central requirement.\n\n**4. Implementation and Verification**\n\nThe algorithm is implemented using `numpy` for numerical computations. A rotation matrix $\\mathbf{R}$ is generated from a specified axis-angle pair $(\\hat{\\mathbf{a}}, \\theta)$ via Rodrigues' formula: $\\mathbf{R} = \\cos\\theta\\, \\mathbf{I} + \\sin\\theta\\, [\\hat{\\mathbf{a}}]_\\times + (1-\\cos\\theta)\\, \\hat{\\mathbf{a}}\\hat{\\mathbf{a}}^\\top$, where $[\\hat{\\mathbf{a}}]_\\times$ is the skew-symmetric matrix corresponding to the cross-product with $\\hat{\\mathbf{a}}$. The logic is applied to the provided test cases to numerically confirm that, within a defined tolerance $\\tau = 10^{-10}$, the computed value of $E$ is identical for both the original and the rotated atomic coordinates.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and verifies a rotation-invariant scalar prediction for molecular configurations.\n    \"\"\"\n    # Define constants from the problem statement.\n    DELTA = 1e-12\n    A0, A1, A2, A3 = 0.5, 0.1, -0.05, 0.02\n    BETA = 1.3\n    GAMMA = 0.9\n    C1, C2, C3 = 0.7, 0.2, 0.05\n    TOLERANCE_TAU = 1e-10\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": np.array([8, 1, 1], dtype=int),\n            \"r\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.9572, 0.0, 0.0],\n                [-0.2399872, 0.927297, 0.0]\n            ]),\n            \"a\": np.array([1.0, 2.0, 3.0]),\n            \"theta\": 1.234\n        },\n        {\n            \"Z\": np.array([6, 1, 1], dtype=int),\n            \"r\": np.array([\n                [0.0, 0.0, 0.0],\n                [1.1, 0.0, 0.0],\n                [-1.1, 0.0, 0.0]\n            ]),\n            \"a\": np.array([0.0, 0.0, 1.0]),\n            \"theta\": np.pi\n        },\n        {\n            \"Z\": np.array([8], dtype=int),\n            \"r\": np.array([\n                [1.0, 2.0, 3.0]\n            ]),\n            \"a\": np.array([0.0, 0.0, 1.0]),\n            \"theta\": 0.7\n        },\n        {\n            \"Z\": np.array([1, 1, 1], dtype=int),\n            \"r\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ]),\n            \"a\": np.array([0.0, 1.0, 0.0]),\n            \"theta\": 2.2\n        }\n    ]\n    \n    def compute_E(positions, types):\n        \"\"\"\n        Computes the total scalar prediction E for a given molecular configuration.\n        \"\"\"\n        num_atoms = len(types)\n        \n        # Handle single-atom and empty cases.\n        if num_atoms <= 1:\n            return C3 * np.sum(types)\n\n        total_e_sum = 0.0\n        for i in range(num_atoms):\n            m_i = np.zeros(3)\n            e_i_dist_term = 0.0\n            \n            for j in range(num_atoms):\n                if i == j:\n                    continue\n                \n                # Calculate relative vector, distance, and unit vector\n                r_ij = positions[j] - positions[i]\n                d_ij = np.linalg.norm(r_ij)\n                \n                if d_ij <= DELTA:\n                    r_hat_ij = np.zeros(3)\n                else:\n                    r_hat_ij = r_ij / d_ij\n                \n                # Calculate scalar coefficient s_ij\n                s_ij = np.exp(-BETA * d_ij) * (A0 + A1 * types[i] + A2 * types[j] + A3 * types[i] * types[j])\n                \n                # Update message vector m_i\n                m_i += s_ij * r_hat_ij\n                \n                # Update distance-dependent term for e_i\n                e_i_dist_term += np.exp(-GAMMA * d_ij)\n            \n            # Calculate per-node scalar e_i\n            m_i_norm_sq = np.dot(m_i, m_i) # More efficient than np.linalg.norm()**2\n            e_i = C1 * m_i_norm_sq + C2 * e_i_dist_term\n            total_e_sum += e_i\n            \n        # Calculate total prediction E\n        total_E = total_e_sum + C3 * np.sum(types)\n        return total_E\n\n    results = []\n    for case in test_cases:\n        Z, r, a, theta = case[\"Z\"], case[\"r\"], case[\"a\"], case[\"theta\"]\n        \n        # 1. Compute E for the original configuration\n        E_original = compute_E(r, Z)\n        \n        # 2. Generate the rotation matrix R using Rodrigues' formula\n        a_norm = np.linalg.norm(a)\n        if a_norm < DELTA: # Handles zero vector for axis\n             a_hat = np.zeros(3)\n        else:\n             a_hat = a / a_norm\n        \n        I = np.identity(3)\n        a_cross_matrix = np.array([\n            [0, -a_hat[2], a_hat[1]],\n            [a_hat[2], 0, -a_hat[0]],\n            [-a_hat[1], a_hat[0], 0]\n        ])\n        a_outer_product = np.outer(a_hat, a_hat)\n        \n        R = np.cos(theta) * I + np.sin(theta) * a_cross_matrix + (1 - np.cos(theta)) * a_outer_product\n\n        # 3. Apply rotation to coordinates\n        # r is (N, 3). R is (3, 3). We need (r @ R.T) or (R @ r.T).T\n        r_rotated = r @ R.T\n\n        # 4. Compute E for the rotated configuration\n        E_rotated = compute_E(r_rotated, Z)\n        \n        # 5. Check for invariance within the given tolerance\n        is_invariant = np.abs(E_original - E_rotated) <= TOLERANCE_TAU\n        results.append(is_invariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3117017"}, {"introduction": "Deep learning models are increasingly used as \"surrogates\" to emulate or accelerate traditional scientific simulations, but how can we trust their predictions? This exercise reveals a fascinating link between a modern architecture, the Residual Network (ResNet), and classical numerical methods for solving partial differential equations (PDEs). You will frame a ResNet as a time-stepping solver for the Heat Equation and analyze its stability using spectral methods, a critical skill for developing robust deep learning models for dynamical systems. [@problem_id:3116956]", "problem": "Consider a one-dimensional diffusion governed by the Heat Equation, a Partial Differential Equation (PDE), with periodic boundary conditions. The continuous model is $u_t = \\alpha u_{xx}$ on a uniform spatial grid. In a standard finite-difference explicit time-stepping scheme with time step $dt$ and spatial grid spacing $dx$, a single solver step can be written as a discrete operator $\\mathcal{S}$ applied to the state $u^n$ to produce $u^{n+1} = \\mathcal{S}(u^n)$. In this problem, you will emulate one solver step $\\mathcal{S}$ using a residual network (a skip-connection block) and assess the stability of the resulting operator by computing a tight bound $K$ such that the inequality $\\|u^{n+1}\\|_2 \\le K \\|u^n\\|_2$ holds for all inputs $u^n$ on the given grid. Here, $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nFundamental base:\n- Finite differences approximate second derivatives by local stencils. The discrete Laplacian in one dimension with periodic boundary conditions uses the standard three-point stencil.\n- Linear, shift-invariant discrete operators on a periodic grid are circulant and are diagonalized by the Discrete Fourier Transform (DFT). The operator norm induced by the Euclidean norm equals the maximum magnitude of the operator’s eigenvalues in the Fourier basis.\n\nYour tasks are:\n- Implement a residual network that emulates one explicit Euler solver step for the Heat Equation on a periodic grid. Let the residual block be $\\mathcal{R}(u) = u + f(u)$, where $f(u)$ applies the discrete Laplacian via periodic convolution and scales it by $\\alpha dt$. Use the canonical three-point stencil for the Laplacian and circular (periodic) indexing. Concretely, for a state vector $u \\in \\mathbb{R}^N$, define\n$$\n\\mathrm{Lap}(u)_i = \\frac{u_{i+1} - 2u_i + u_{i-1}}{dx^2},\n$$\nwith periodic indexing, and set\n$$\n\\mathcal{R}(u) = u + \\alpha\\, dt\\, \\mathrm{Lap}(u).\n$$\nTreat $\\mathcal{R}$ as a residual network with one linear convolutional layer in the residual branch and an identity skip-connection.\n\n- Using the circulant structure of $\\mathcal{R}$, determine the tightest bound $K$ such that for all $u$, $\\|\\mathcal{R}(u)\\|_2 \\le K \\|u\\|_2$. The bound $K$ must be computed from first principles via the spectral characterization of circulant operators on a periodic grid of size $N$.\n\n- Validate numerically that the inequality $\\|\\mathcal{R}(u)\\|_2 \\le K \\|u\\|_2$ holds by checking it on a small set of randomly generated input states $u$ for each test case.\n\nUnits:\n- Let $\\alpha$ be a diffusion coefficient with units $\\mathrm{m}^2/\\mathrm{s}$, $dx$ be in $\\mathrm{m}$, and $dt$ be in $\\mathrm{s}$. The bound $K$ is dimensionless.\n\nAngle units are not involved. Percentages are not involved.\n\nTest suite:\nUse the following parameter sets, each specified by $(N, dx, \\alpha, dt)$:\n- Case $1$: $(N=\\;64, dx=\\;0.01\\;\\mathrm{m}, \\alpha=\\;10^{-4}\\;\\mathrm{m}^2/\\mathrm{s}, dt=\\;0.01\\;\\mathrm{s})$.\n- Case $2$: $(N=\\;64, dx=\\;0.01\\;\\mathrm{m}, \\alpha=\\;10^{-4}\\;\\mathrm{m}^2/\\mathrm{s}, dt=\\;0.50\\;\\mathrm{s})$.\n- Case $3$: $(N=\\;64, dx=\\;0.01\\;\\mathrm{m}, \\alpha=\\;10^{-4}\\;\\mathrm{m}^2/\\mathrm{s}, dt=\\;0.75\\;\\mathrm{s})$.\n- Case $4$: $(N=\\;65, dx=\\;0.01\\;\\mathrm{m}, \\alpha=\\;10^{-4}\\;\\mathrm{m}^2/\\mathrm{s}, dt=\\;0.75\\;\\mathrm{s})$.\n\nAnswer specification:\n- For each test case, compute the tightest bound $K$ for the operator $\\mathcal{R}$ on the given grid.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the $K$ values rounded to three decimal places and ordered by the cases above, for example, $[K_1,K_2,K_3,K_4]$.\n\nDesign for coverage:\n- Case $1$ is a happy-path stable regime (small $dt$).\n- Case $2$ is a boundary regime.\n- Case $3$ is an unstable regime on an even grid.\n- Case $4$ probes the impact of an odd grid size on the spectral maximum for the same parameters as Case $3$.\n\nAll numerical values must be computed by your program. No external inputs or files are allowed. The output must be a single line exactly as specified.", "solution": "The task is to find the tightest bound $K$ for a numerical operator $\\mathcal{R}$ that emulates one step of an explicit Euler solver for the one-dimensional Heat Equation, $u_t = \\alpha u_{xx}$. The bound $K$ must satisfy the inequality $\\|\\mathcal{R}(u)\\|_2 \\le K \\|u\\|_2$ for any state vector $u \\in \\mathbb{R}^N$ on a periodic grid of size $N$.\n\nThe operator $\\mathcal{R}$ is defined as a residual block:\n$$\n\\mathcal{R}(u) = u + \\alpha \\, dt \\, \\mathrm{Lap}(u)\n$$\nwhere $u$ is the state vector of size $N$, $\\alpha$ is the diffusion coefficient, $dt$ is the time step, and $\\mathrm{Lap}(u)$ is the discrete Laplacian operator. On a uniform grid with spacing $dx$ and periodic boundary conditions, the $i$-th component of the Laplacian is given by the three-point stencil:\n$$\n(\\mathrm{Lap}(u))_i = \\frac{u_{(i+1) \\pmod N} - 2u_i + u_{(i-1) \\pmod N}}{dx^2}\n$$\nThe operator $\\mathcal{R}$ is linear and can be represented by a matrix-vector product $\\mathcal{R}(u) = A u$, where $A$ is an $N \\times N$ matrix. Let the dimensionless parameter $\\gamma$ be defined as $\\gamma = \\frac{\\alpha \\, dt}{dx^2}$. The matrix $A$ can be written as:\n$$\nA = I + \\gamma L_d\n$$\nwhere $I$ is the $N \\times N$ identity matrix and $L_d$ is the matrix representing the unscaled discrete Laplacian with the stencil $(1, -2, 1)$ on a periodic grid. Due to the shift-invariant stencil and periodic boundary conditions, both $L_d$ and $A$ are circulant matrices. The first row of $A$ is $(1 - 2\\gamma, \\gamma, 0, \\dots, 0, \\gamma)$.\n\nThe tightest bound $K$ is the induced $2$-norm of the operator, which corresponds to the spectral norm of the matrix $A$, denoted $\\|A\\|_2$. For a normal matrix, which includes any circulant matrix, the spectral norm is equal to its spectral radius, $\\rho(A)$, defined as the maximum absolute value of its eigenvalues.\n$$\nK = \\|A\\|_2 = \\rho(A) = \\max_k |\\lambda_k(A)|\n$$\nThe eigenvalues $\\lambda_k$ of a circulant matrix are given by the Discrete Fourier Transform (DFT) of its first row. For a circulant matrix with first row $(c_0, c_1, \\dots, c_{N-1})$, the eigenvalues are $\\lambda_k = \\sum_{j=0}^{N-1} c_j e^{-2\\pi i k j / N}$ for $k = 0, 1, \\dots, N-1$.\n\nFor the matrix $A$, the only non-zero elements in the first row are $c_0 = 1 - 2\\gamma$, $c_1 = \\gamma$, and $c_{N-1} = \\gamma$. The eigenvalues are:\n$$\n\\lambda_k = (1 - 2\\gamma) e^0 + \\gamma e^{-2\\pi i k(1)/N} + \\gamma e^{-2\\pi i k(N-1)/N}\n$$\nUsing the property $e^{-2\\pi i k(N-1)/N} = e^{-2\\pi i k} e^{2\\pi i k/N} = e^{2\\pi i k/N}$, the expression simplifies:\n$$\n\\lambda_k = 1 - 2\\gamma + \\gamma (e^{-2\\pi i k/N} + e^{2\\pi i k/N})\n$$\nApplying Euler's formula, $2\\cos\\theta = e^{i\\theta} + e^{-i\\theta}$, we obtain:\n$$\n\\lambda_k = 1 - 2\\gamma + 2\\gamma \\cos\\left(\\frac{2\\pi k}{N}\\right) = 1 + 2\\gamma \\left(\\cos\\left(\\frac{2\\pi k}{N}\\right) - 1\\right)\n$$\nUsing the trigonometric identity $\\cos(2\\theta) - 1 = -2\\sin^2(\\theta)$, the eigenvalues can be written in their final real-valued form:\n$$\n\\lambda_k = 1 - 4\\gamma \\sin^2\\left(\\frac{\\pi k}{N}\\right), \\quad \\text{for } k = 0, 1, \\dots, N-1\n$$\nThe bound $K$ is the maximum of the absolute values of these eigenvalues over all possible mode numbers $k$:\n$$\nK = \\max_{k \\in \\{0, \\dots, N-1\\}} \\left| 1 - 4\\gamma \\sin^2\\left(\\frac{\\pi k}{N}\\right) \\right|\n$$\nLet $S_k = \\sin^2\\left(\\frac{\\pi k}{N}\\right)$. The expression for the eigenvalues is a linear function of $S_k$. The values of $S_k$ range from $S_0 = 0$ to a maximum value $S_{max} = \\sin^2\\left(\\frac{\\pi \\lfloor N/2 \\rfloor}{N}\\right)$. The maximum of $|\\lambda_k|$ must occur at one of the extremal values of $S_k$.\nFor $k=0$, $S_0=0$ and $\\lambda_0 = 1$.\nFor $k=\\lfloor N/2 \\rfloor$, $S_k = S_{max}$ and the eigenvalue is $\\lambda_{ext} = 1 - 4\\gamma S_{max}$.\nTherefore, the bound $K$ is determined by the greater of the absolute values of these two eigenvalues:\n$$\nK = \\max(|\\lambda_0|, |\\lambda_{\\lfloor N/2 \\rfloor}|) = \\max\\left(1, \\left|1 - 4\\gamma S_{max}\\right|\\right)\n$$\nThe numerical scheme is stable if $K \\le 1$. This occurs when $|1 - 4\\gamma S_{max}| \\le 1$, which simplifies to the well-known Courant-Friedrichs-Lewy (CFL) condition for this scheme: $4\\gamma S_{max} \\le 2$, or $\\gamma S_{max} \\le 0.5$.\n- If $\\gamma S_{max} \\le 0.5$, the scheme is stable. The expression $1 - 4\\gamma S_{max}$ is between $-1$ and $1$, so its absolute value is less than or equal to $1$. In this case, $K=1$.\n- If $\\gamma S_{max} > 0.5$, the scheme is unstable. The expression $1 - 4\\gamma S_{max}$ is less than $-1$. Its absolute value is $|1 - 4\\gamma S_{max}| = 4\\gamma S_{max} - 1$, which is greater than $1$. In this case, $K = 4\\gamma S_{max} - 1$.\n\nThis logic is implemented for each test case to compute the corresponding bound $K$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the tightest stability bound K for a finite-difference scheme\n    for the Heat Equation, interpreted as a residual network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, dx, alpha, dt)\n    test_cases = [\n        (64, 0.01, 1e-4, 0.01),  # Case 1: Stable\n        (64, 0.01, 1e-4, 0.50),  # Case 2: Boundary of stability\n        (64, 0.01, 1e-4, 0.75),  # Case 3: Unstable on an even grid\n        (65, 0.01, 1e-4, 0.75),  # Case 4: Unstable on an odd grid\n    ]\n\n    results = []\n    for case in test_cases:\n        N, dx, alpha, dt = case\n\n        # The operator is R(u) = u + alpha * dt * Lap(u).\n        # We can write this as R(u) = (I + gamma * L_d) * u,\n        # where gamma = alpha * dt / dx^2 and L_d is the discrete Laplacian matrix.\n        # The eigenvalues of the operator matrix A = I + gamma * L_d are:\n        # lambda_k = 1 - 4 * gamma * sin^2(pi * k / N) for k=0, ..., N-1.\n\n        # The bound K is the spectral radius of A, which is max(|lambda_k|).\n        # Let's compute gamma.\n        gamma = alpha * dt / (dx**2)\n\n        # The maximum of sin^2(pi*k/N) occurs at k = floor(N/2).\n        # This term determines the most negative eigenvalue, which dictates stability.\n        k_for_max_sin = np.floor(N / 2)\n        s_max_sq = np.sin(np.pi * k_for_max_sin / N)**2\n\n        # The eigenvalues are all real. The maximum absolute value is either at\n        # k=0 (lambda_0 = 1) or at k where sin^2 is maximized.\n        # The eigenvalue for the latter case is:\n        lambda_at_s_max = 1 - 4 * gamma * s_max_sq\n        \n        # The bound K is the maximum of |1| and |lambda_at_s_max|.\n        K = np.maximum(1.0, np.abs(lambda_at_s_max))\n\n        # A more direct formula based on stability condition gamma * s_max_sq <= 0.5:\n        # if gamma * s_max_sq <= 0.5:\n        #     K = 1.0\n        # else:\n        #     # In this case, lambda_at_s_max will be < -1, so its abs value is\n        #     # -(lambda_at_s_max) = 4 * gamma * s_max_sq - 1.\n        #     K = 4 * gamma * s_max_sq - 1\n\n        results.append(K)\n\n    # Format the results for output, rounding to three decimal places.\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3116956"}]}