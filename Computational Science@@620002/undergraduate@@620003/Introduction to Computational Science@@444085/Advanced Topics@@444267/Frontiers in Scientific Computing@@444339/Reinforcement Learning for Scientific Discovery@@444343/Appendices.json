{"hands_on_practices": [{"introduction": "This first exercise provides a practical foundation in applying reinforcement learning to strategic scientific planning. You will implement a Q-learning agent to manage a portfolio of scientific hypotheses, deciding which lines of inquiry to retire to best allocate limited resources. By designing a reward function based on the Expected Value of Information (EVI), you will directly connect the principles of decision theory to the mechanics of a core, value-based RL algorithm. This practice will build your skills in modeling a sequential decision process as a Markov Decision Process (MDP) and solving it using a classic tabular method. [@problem_id:3186163]", "problem": "You are given a formal decision scenario for Reinforcement Learning (RL) in the context of selecting which scientific hypotheses to retire to maximize the expected value of information for the remaining hypotheses. The environment is modeled as a Markov Decision Process (MDP) where each state encodes the subset of hypotheses that remain under consideration. Actions retire exactly one currently active hypothesis. Rewards at each step are based on the Expected Value of Information (EVI) of the remaining hypotheses after the action, scaled by a simple resource-sharing model.\n\nFundamental base:\n- Markov Decision Process (MDP): a tuple $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$ with states $\\mathcal{S}$, actions $\\mathcal{A}$, transition probabilities $P$, reward function $r$, and discount factor $\\gamma$.\n- Expected value and information: for a binary hypothesis with prior probability $p \\in (0,1)$, its information content measured by Shannon entropy (in natural units, nats) is $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$.\n- Resource sharing: a fixed budget $B$ is shared equally among all currently active hypotheses. If $n$ hypotheses remain, each receives resource share $B/n$.\n\nModel specification:\n- Let there be $N$ hypotheses. Each hypothesis $i \\in \\{0,1,\\dots,N-1\\}$ has prior $p_i \\in (0,1)$ and information weight $w_i > 0$ representing the decision-theoretic value of learning about hypothesis $i$.\n- A state $s$ is any subset of indices of the currently active hypotheses. An action $a \\in s$ retires hypothesis $a$, yielding the new state $s' = s \\setminus \\{a\\}$.\n- The instantaneous reward for taking action $a$ in state $s$ is defined as\n$$\nr(s,a) = \\frac{B}{|s'|} \\sum_{j \\in s'} w_j H(p_j),\n$$\nwith $H(p_j) = -p_j \\ln(p_j) - (1-p_j)\\ln(1-p_j)$ and $|s'|$ the number of remaining hypotheses after the action. The logarithm is the natural logarithm; the unit of information is nats.\n- The episode terminates once exactly $R_{\\max}$ retirements have been performed. We require $R_{\\max} \\le N-1$ so at least one hypothesis remains at termination. If $R_{\\max} = 0$, no actions are taken and the baseline reward is defined as\n$$\nr_{\\mathrm{baseline}}(s) = \\frac{B}{|s|} \\sum_{j \\in s} w_j H(p_j).\n$$\n\nTask:\n- Implement Q-learning to learn a policy for retirement decisions. Use an $\\varepsilon$-greedy exploration schedule and a learning rate $\\alpha$. The state can be represented as a bitmask over $N$ hypotheses. Only actions corresponding to currently active hypotheses are allowed at a given state. The Q-learning update is\n$$\nQ(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q(s',a') \\right),\n$$\nwhere $s'$ is the next state after taking action $a$ in state $s$.\n- After training, evaluate the greedy policy (choose the action with maximal $Q$ value among allowed actions) starting from the full set of hypotheses until $R_{\\max}$ retirements have been performed. Report, for each test case, the list of retired hypothesis indices (use zero-based indexing) and the cumulative undiscounted sum of rewards obtained during evaluation. For the case $R_{\\max} = 0$, report an empty retirement list and the baseline reward defined above.\n- The logarithm base must be the natural logarithm.\n\nTest suite:\nProvide results for the following parameter sets. In each case, use the given parameters $(N, p, w, B, R_{\\max}, \\gamma, \\text{episodes}, \\varepsilon_{\\text{init}}, \\varepsilon_{\\text{final}}, \\alpha)$, with all vectors given in the same order as hypotheses indexed by $i \\in \\{0,1,\\dots,N-1\\}$.\n\n- Case $1$ (general case):\n  - $N = 4$\n  - $p = [0.10, 0.70, 0.50, 0.05]$\n  - $w = [1.00, 0.90, 0.60, 0.20]$\n  - $B = 1.00$\n  - $R_{\\max} = 2$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 12000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- Case $2$ (edge probabilities close to $0$ or $1$ but strictly inside $(0,1)$):\n  - $N = 5$\n  - $p = [0.01, 0.85, 0.40, 0.95, 0.20]$\n  - $w = [0.50, 1.20, 0.80, 0.10, 0.70]$\n  - $B = 1.00$\n  - $R_{\\max} = 2$\n  - $\\gamma = 0.90$\n  - $\\text{episodes} = 15000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- Case $3$ (ties: identical priors and weights):\n  - $N = 4$\n  - $p = [0.50, 0.50, 0.50, 0.50]$\n  - $w = [1.00, 1.00, 1.00, 1.00]$\n  - $B = 1.00$\n  - $R_{\\max} = 3$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 10000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- Case $4$ (no retirement budget):\n  - $N = 3$\n  - $p = [0.40, 0.60, 0.55]$\n  - $w = [0.90, 0.80, 0.70]$\n  - $B = 1.00$\n  - $R_{\\max} = 0$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 1$\n  - $\\varepsilon_{\\text{init}} = 0.00$\n  - $\\varepsilon_{\\text{final}} = 0.00$\n  - $\\alpha = 0.30$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each per-case result is itself a list containing two elements: the retirement sequence (a list of zero-based indices) and the cumulative undiscounted reward (a float). For example, an output for two cases would look like\n$[[[0,2],1.234],[[],0.567]]$.", "solution": "We model the decision problem as a Markov Decision Process (MDP) and solve it using Reinforcement Learning (RL) via Q-learning. The goal is to select which hypotheses to retire so that the expected value of information (EVI) of the remaining hypotheses, under a simple resource-sharing model, is maximized over a fixed retirement horizon.\n\nPrinciples and definitions:\n- Reinforcement Learning (RL) optimizes decision-making in an environment modeled by an MDP $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$, where $\\gamma \\in [0,1)$ is the discount factor. The agent selects actions to maximize expected discounted cumulative reward.\n- Expected value of information, grounded in decision theory and information theory: for a binary hypothesis with prior $p \\in (0,1)$, the Shannon entropy $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$ (with natural logarithm) measures uncertainty. If we proxy the utility of learning by $w > 0$ and assume resources are shared equally among active hypotheses, the instantaneous reward is proportional to the sum of $w H(p)$ over remaining hypotheses, scaled by the resource share.\n\nEnvironment construction:\n- Let there be $N$ hypotheses with priors $p_i \\in (0,1)$ and weights $w_i > 0$. A state $s \\subseteq \\{0,1,\\dots,N-1\\}$ represents the active hypotheses. An action $a \\in s$ retires hypothesis $a$ yielding $s' = s \\setminus \\{a\\}$.\n- If $n' = |s'|$, then the resource share per remaining hypothesis is $B/n'$. The instantaneous reward is\n$$\nr(s,a) = \\frac{B}{n'} \\sum_{j \\in s'} w_j H(p_j), \\quad H(p_j) = -p_j \\ln(p_j) - (1-p_j)\\ln(1-p_j).\n$$\nThis is a direct application of the definition of expected value and the Shannon entropy as a well-tested measure of information.\n- The episode terminates once $R_{\\max}$ retirements have been made. This yields a finite-horizon decision problem with a discrete and deterministic transition dynamic.\n\nAlgorithmic derivation:\n- Q-learning approximates the optimal action-value function $Q^*(s,a)$ satisfying the Bellman optimality equation\n$$\nQ^*(s,a) = r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s',a'),\n$$\nwhere $s'$ is the state after taking action $a$ in state $s$.\n- The learning update is\n$$\nQ(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q(s',a') \\right),\n$$\nwith learning rate $\\alpha \\in (0,1]$. Exploration is implemented via an $\\varepsilon$-greedy policy: with probability $\\varepsilon$ choose a random allowed action; otherwise choose the action maximizing $Q(s,a)$.\n- States are represented as bitmasks of length $N$. In a state $s$, the allowed actions are the indices $i$ such that the bitmask has a $1$ at position $i$. An episode begins at the full mask (all hypotheses active) and ends after $R_{\\max}$ retirements. For $R_{\\max} = 0$, no actions are taken and the baseline reward is\n$$\nr_{\\mathrm{baseline}}(s) = \\frac{B}{|s|} \\sum_{j \\in s} w_j H(p_j).\n$$\n\nScientific justification:\n- The reward design follows from the principle that expected value is the sum over outcomes times their value; using $H(p)$ as the expected information under uncertainty and a weight $w$ as the utility scaling for learning about a hypothesis. This combines fundamental decision-theoretic constructs and the well-tested Shannon entropy.\n- Retiring a hypothesis frees its resource share to be redistributed equally among remaining hypotheses, increasing the per-hypothesis expected information capture rate while reducing the number of hypotheses contributing to the sum. The agent must balance removing low-information hypotheses against maintaining sufficient breadth, which is a realistic trade-off in scientific portfolio management.\n\nImplementation details:\n- For each test case, the Q-learning agent is trained for the specified number of episodes, with an exploration parameter $\\varepsilon$ annealed linearly from $\\varepsilon_{\\text{init}}$ to $\\varepsilon_{\\text{final}}$. After training, the greedy policy is executed to produce the retirement sequence and the cumulative undiscounted reward.\n- Hypotheses are reported using zero-based indices. The output is a single line string containing a list of per-case results, where each result is itself a list $[\\text{sequence}, \\text{reward}]$.\n\nEdge-case coverage:\n- Case $2$ uses $p$ values very close to $0$ or $1$ (but strictly inside), testing numerical stability of $H(p)$; the implementation clamps $p$ away from $0$ and $1$ at machine-safe margins to avoid $\\ln(0)$, consistent with the domain requirement $p \\in (0,1)$.\n- Case $3$ creates ties with identical $p$ and $w$, demonstrating that any symmetry-breaking from learning still yields coherent sequences.\n- Case $4$ sets $R_{\\max} = 0$, ensuring the implementation correctly returns no retirements and a baseline reward as defined.\n\nFinal output:\n- The program prints exactly one line: a bracketed comma-separated list with no spaces. Each element is a pair represented as a list: the retirement sequence (list of integers) and the cumulative undiscounted reward (float), following the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef shannon_entropy_nats(p: float) -> float:\n    \"\"\"Compute Shannon entropy H(p) in nats for a binary variable.\"\"\"\n    # Clamp to avoid log(0); the problem states p in (0,1), but clamp for numerical safety.\n    eps = 1e-12\n    p = float(np.clip(p, eps, 1.0 - eps))\n    return -(p * np.log(p) + (1.0 - p) * np.log(1.0 - p))\n\nclass HypothesisEnv:\n    \"\"\"Finite deterministic environment for retiring hypotheses.\"\"\"\n    def __init__(self, p_vec, w_vec, B, Rmax):\n        self.p = np.array(p_vec, dtype=float)\n        self.w = np.array(w_vec, dtype=float)\n        self.B = float(B)\n        self.N = len(self.p)\n        self.Rmax = int(Rmax)\n        # Precompute entropies\n        self.H = np.array([shannon_entropy_nats(pi) for pi in self.p], dtype=float)\n        # Full state bitmask: all ones for N hypotheses\n        self.full_state = (1 << self.N) - 1\n\n    def remaining_indices(self, state_mask: int):\n        return [i for i in range(self.N) if (state_mask >> i) & 1]\n\n    def count_remaining(self, state_mask: int) -> int:\n        return int(np.sum([(state_mask >> i) & 1 for i in range(self.N)]))\n\n    def retired_count(self, state_mask: int) -> int:\n        return self.N - self.count_remaining(state_mask)\n\n    def is_terminal(self, state_mask: int) -> bool:\n        return self.retired_count(state_mask) >= self.Rmax\n\n    def allowed_actions(self, state_mask: int):\n        if self.is_terminal(state_mask):\n            return []\n        return self.remaining_indices(state_mask)\n\n    def step(self, state_mask: int, action: int):\n        \"\"\"Take action (retire hypothesis 'action'), return (next_state, reward).\"\"\"\n        if ((state_mask >> action) & 1) == 0:\n            raise ValueError(\"Action must retire an active hypothesis.\")\n        next_state = state_mask & ~(1 << action)\n        rem = self.remaining_indices(next_state)\n        n_rem = len(rem)\n        if n_rem <= 0:\n            # Should not occur because Rmax <= N-1 per problem spec\n            reward = 0.0\n        else:\n            share = self.B / n_rem\n            reward = share * float(np.sum(self.w[rem] * self.H[rem]))\n        return next_state, reward\n\n    def baseline_reward(self, state_mask: int) -> float:\n        \"\"\"Reward with no retirement: resource equally split among current hypotheses.\"\"\"\n        rem = self.remaining_indices(state_mask)\n        n_rem = len(rem)\n        if n_rem <= 0:\n            return 0.0\n        share = self.B / n_rem\n        return share * float(np.sum(self.w[rem] * self.H[rem]))\n\ndef q_learning(env: HypothesisEnv, gamma: float, episodes: int,\n               eps_init: float, eps_final: float, alpha: float, rng: np.random.Generator):\n    \"\"\"Train Q-learning on the environment.\"\"\"\n    num_states = 1 << env.N\n    Q = np.zeros((num_states, env.N), dtype=float)\n\n    def epsilon_for_episode(ep):\n        # Linear annealing from eps_init to eps_final\n        if episodes <= 1:\n            return eps_final\n        return eps_init + (eps_final - eps_init) * (ep / (episodes - 1))\n\n    for ep in range(episodes):\n        state = env.full_state\n        while not env.is_terminal(state):\n            actions = env.allowed_actions(state)\n            eps = epsilon_for_episode(ep)\n            if actions:\n                if rng.random() < eps:\n                    action = int(rng.choice(actions))\n                else:\n                    # Greedy: argmax Q among allowed actions\n                    q_vals = np.array([Q[state, a] for a in actions], dtype=float)\n                    # Break ties consistently by choosing smallest index among best\n                    best_idx = int(np.argmax(q_vals))\n                    action = int(actions[best_idx])\n            else:\n                break\n            next_state, reward = env.step(state, action)\n            next_actions = env.allowed_actions(next_state)\n            if next_actions:\n                max_next_q = np.max([Q[next_state, a] for a in next_actions])\n            else:\n                max_next_q = 0.0\n            # Q-learning update\n            Q[state, action] = (1.0 - alpha) * Q[state, action] + alpha * (reward + gamma * max_next_q)\n            state = next_state\n    return Q\n\ndef evaluate_greedy(env: HypothesisEnv, Q: np.ndarray):\n    \"\"\"Evaluate greedy policy derived from Q from full state.\"\"\"\n    state = env.full_state\n    seq = []\n    total_reward = 0.0\n    if env.Rmax == 0:\n        return seq, env.baseline_reward(state)\n    steps = 0\n    while not env.is_terminal(state):\n        actions = env.allowed_actions(state)\n        if not actions:\n            break\n        # Greedy among allowed\n        q_vals = np.array([Q[state, a] for a in actions], dtype=float)\n        best_idx = int(np.argmax(q_vals))\n        action = int(actions[best_idx])\n        next_state, reward = env.step(state, action)\n        seq.append(action)\n        total_reward += reward\n        state = next_state\n        steps += 1\n        if steps > env.Rmax:\n            # Safety check; should not happen\n            break\n    return seq, total_reward\n\ndef serialize(obj):\n    \"\"\"Serialize lists and primitive numbers without spaces.\"\"\"\n    if isinstance(obj, (int, np.integer)):\n        return str(int(obj))\n    if isinstance(obj, (float, np.floating)):\n        # Use repr to keep reasonable precision\n        return repr(float(obj))\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        return \"[\" + \",\".join(serialize(x) for x in obj) + \"]\"\n    raise TypeError(f\"Unsupported type for serialization: {type(obj)}\")\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"N\": 4,\n            \"p\": [0.10, 0.70, 0.50, 0.05],\n            \"w\": [1.00, 0.90, 0.60, 0.20],\n            \"B\": 1.00,\n            \"Rmax\": 2,\n            \"gamma\": 0.95,\n            \"episodes\": 12000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 2\n        {\n            \"N\": 5,\n            \"p\": [0.01, 0.85, 0.40, 0.95, 0.20],\n            \"w\": [0.50, 1.20, 0.80, 0.10, 0.70],\n            \"B\": 1.00,\n            \"Rmax\": 2,\n            \"gamma\": 0.90,\n            \"episodes\": 15000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 3\n        {\n            \"N\": 4,\n            \"p\": [0.50, 0.50, 0.50, 0.50],\n            \"w\": [1.00, 1.00, 1.00, 1.00],\n            \"B\": 1.00,\n            \"Rmax\": 3,\n            \"gamma\": 0.95,\n            \"episodes\": 10000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 4\n        {\n            \"N\": 3,\n            \"p\": [0.40, 0.60, 0.55],\n            \"w\": [0.90, 0.80, 0.70],\n            \"B\": 1.00,\n            \"Rmax\": 0,\n            \"gamma\": 0.95,\n            \"episodes\": 1,\n            \"eps_init\": 0.00,\n            \"eps_final\": 0.00,\n            \"alpha\": 0.30,\n        },\n    ]\n\n    rng = np.random.default_rng(42)\n    results = []\n    for case in test_cases:\n        env = HypothesisEnv(case[\"p\"], case[\"w\"], case[\"B\"], case[\"Rmax\"])\n        Q = q_learning(\n            env=env,\n            gamma=case[\"gamma\"],\n            episodes=case[\"episodes\"],\n            eps_init=case[\"eps_init\"],\n            eps_final=case[\"eps_final\"],\n            alpha=case[\"alpha\"],\n            rng=rng,\n        )\n        seq, total_reward = evaluate_greedy(env, Q)\n        results.append([seq, total_reward])\n\n    # Final print statement in the exact required format (no spaces).\n    print(serialize(results))\n\nsolve()\n```", "id": "3186163"}, {"introduction": "Moving from tabular methods to function approximation, this practice explores the power of contextual bandits for optimizing real-time experimental design. You will build an agent that learns to select the most informative questions to ask participants in a simulated citizen science project. The exercise requires implementing the Linear Upper-Confidence-Bound (LinUCB) algorithm, a highly effective strategy for balancing the exploration-exploitation tradeoff. This hands-on problem demonstrates how RL can personalize interventions and efficiently learn in environments with continuous context spaces and unknown reward dynamics. [@problem_id:3186203]", "problem": "You are tasked with designing and evaluating a Reinforcement Learning (RL) policy that sequentially selects the next question to ask a participant on a citizen science platform. Each question belongs to a discrete set of question types, and the goal is to maximize cumulative classification improvement that contributes to scientific discovery. Model this interaction as a contextual bandit that arises from a one-step Markov Decision Process (MDP) approximation, where the immediate reward captures the classification gain due to the participant’s response. The following definitions and constraints must be used to construct a scientifically sound and self-consistent simulation testbed:\n\n- Reinforcement Learning (RL) operates on discrete time steps $t \\in \\{1,2,\\dots,T\\}$ with an action set $\\mathcal{A} = \\{1,2,\\dots,K\\}$ representing question types. The environment provides a context vector $c_t \\in \\mathbb{R}^d$ sampled independently at each step, and maintains a scalar classification accuracy $a_t \\in [0,1]$ that evolves according to participant contributions.\n- At each step, the agent observes $c_t$, selects an action $i \\in \\mathcal{A}$, and receives a reward $r_t \\in \\mathbb{R}_{\\ge 0}$ representing the incremental classification accuracy gain. Define the slack $s_t = 1 - a_t$ capturing diminishing returns, and define the augmented feature vector $x_t \\in \\mathbb{R}^{d+2}$ by concatenating a bias term, the context, and the slack:\n$$\nx_t = \\begin{bmatrix} 1 \\\\ c_t \\\\ s_t \\end{bmatrix}.\n$$\n- The true expected reward for action $i$ is a linear function of the features with unknown parameter $w^{(i)} \\in \\mathbb{R}^{d+2}$, corrupted by noise, and truncated to reflect nonnegative gains and diminishing returns. Specifically, the observed reward is:\n$$\nr_t = \\min\\left\\{ s_t,\\ \\max\\left\\{ 0,\\ x_t^\\top w^{(i)} + \\epsilon_t \\right\\}\\right\\},\n$$\nwhere $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ is independent Gaussian noise. The classification accuracy then updates as:\n$$\na_{t+1} = \\min\\{1,\\ a_t + r_t\\}.\n$$\n- Your RL policy must select actions based only on past observed contexts and rewards, trading off exploration and exploitation in a principled way. The problem expects a contextual strategy that uses the observed $x_t$ to infer which action will maximize expected cumulative reward.\n\nFundamental base and assumptions:\n- Treat the interaction as a contextual bandit derived from a one-step MDP reduction using the principle that immediate reward encapsulates the value of a participant’s contribution.\n- Use well-tested statistical modeling assumptions: independent Gaussian noise, linear expected reward in features, and diminishing returns via slack $s_t$.\n- Do not assume any a priori knowledge of the true parameters $w^{(i)}$ during action selection; the agent must learn from data.\n\nYour program must implement the above environment and an RL policy that learns to select actions. It must run the following test suite with specified parameters and produce the required output format.\n\nTest suite specification:\n- Common context distribution for all tests: $c_t \\sim \\mathcal{N}(\\mu, \\Sigma)$ with $\\mu = [0, 0]$ and $\\Sigma = I_2$, where $I_2$ is the $2 \\times 2$ identity matrix. Thus $d = 2$ and $x_t \\in \\mathbb{R}^{4}$.\n- For all tests, features are ordered as $x_t = [1,\\ c_{t,1},\\ c_{t,2},\\ s_t]^\\top$.\n\nTest case $1$ (happy path):\n- Number of actions $K = 3$.\n- Horizon $T = 200$.\n- Initial accuracy $a_0 = 0.5$.\n- Noise standard deviation $\\sigma = 0.05$.\n- True parameters for actions:\n$$\nw^{(1)} = [0.05,\\ 0.8,\\ 0.0,\\ 0.5],\\quad\nw^{(2)} = [0.05,\\ -0.2,\\ 0.9,\\ 0.3],\\quad\nw^{(3)} = [0.05,\\ 0.4,\\ 0.4,\\ 0.7].\n$$\n\nTest case $2$ (boundary condition: near-saturated accuracy):\n- Number of actions $K = 2$.\n- Horizon $T = 100$.\n- Initial accuracy $a_0 = 0.95$.\n- Noise standard deviation $\\sigma = 0.05$.\n- True parameters for actions:\n$$\nw^{(1)} = [0.02,\\ 0.5,\\ 0.5,\\ 0.2],\\quad\nw^{(2)} = [0.02,\\ 0.6,\\ -0.1,\\ 0.2].\n$$\n\nTest case $3$ (edge case: high-noise environment):\n- Number of actions $K = 3$.\n- Horizon $T = 300$.\n- Initial accuracy $a_0 = 0.4$.\n- Noise standard deviation $\\sigma = 0.3$.\n- True parameters for actions:\n$$\nw^{(1)} = [0.10,\\ 0.3,\\ 0.3,\\ 0.4],\\quad\nw^{(2)} = [0.10,\\ 0.35,\\ 0.25,\\ 0.35],\\quad\nw^{(3)} = [0.10,\\ 0.2,\\ 0.5,\\ 0.3].\n$$\n\nOutput requirements:\n- For each test case, compute two quantities: the final accuracy $a_T$ and the cumulative reward $R = \\sum_{t=1}^T r_t$.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case represented as a two-element list. The exact format is:\n$$\n\\text{[}[a_T^{(1)}, R^{(1)}],[a_T^{(2)}, R^{(2)}],[a_T^{(3)}, R^{(3)}]\\text{]},\n$$\nwhere all numbers must be printed as decimals rounded to six places and no spaces are allowed anywhere in the line.\n- Set a fixed random seed internally to ensure reproducibility. Do not read any input and do not write any files.\n\nScientific realism and universality:\n- The setup expects the agent to improve accuracy through selected questions, with gains reflecting statistical dependence on the participant context and diminishing returns as the model saturates.\n- All random variables and parameters are well-defined and consistent with standard statistical modeling in computational science.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded, well-posed, and objective. All necessary parameters and definitions for a reproducible simulation are provided. The problem asks for the design of a Reinforcement Learning (RL) policy to solve a contextual bandit problem formulated to model a citizen science scenario. The following solution provides a principled algorithmic approach and its implementation.\n\nThe problem is an instance of a linear contextual bandit. At each discrete time step $t \\in \\{1, 2, \\dots, T\\}$, an agent must select an action $i$ from a set of $K$ available actions, $\\mathcal{A} = \\{1, 2, \\dots, K\\}$. The decision is informed by a context vector $c_t \\in \\mathbb{R}^d$. The problem defines an augmented feature vector $x_t \\in \\mathbb{R}^{d+2}$ as the concatenation of a bias term, the context, and a state-dependent slack term $s_t = 1 - a_t$, where $a_t \\in [0, 1]$ is the current classification accuracy.\n$$\nx_t = \\begin{bmatrix} 1 \\\\ c_t \\\\ s_t \\end{bmatrix}\n$$\nThe expected reward for taking action $i$ is linear in this feature vector, parameterized by an unknown weight vector $w^{(i)} \\in \\mathbb{R}^{d+2}$. The observed reward $r_t$ is a noisy realization of this linear function, truncated to ensure non-negativity and that the accuracy gain does not exceed the available slack $s_t$.\n$$\nr_t = \\min\\left\\{ s_t,\\ \\max\\left\\{ 0,\\ x_t^\\top w^{(i)} + \\epsilon_t \\right\\}\\right\\}, \\quad \\text{where } \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe system state, represented by the accuracy $a_t$, evolves according to the received reward:\n$$\na_{t+1} = \\min\\{1,\\ a_t + r_t\\}\n$$\nThe agent does not know the true parameters $w^{(i)}$ and must learn them from observations of contexts, actions, and rewards to maximize the cumulative reward $R = \\sum_{t=1}^T r_t$. This necessitates a strategy that balances exploration (trying actions to learn their parameters) and exploitation (choosing the action believed to be best).\n\nA highly effective and theoretically-grounded algorithm for this class of problems is the Linear Upper Confidence Bound (LinUCB) algorithm. LinUCB models the expected reward for each action $i$ as a linear function of the context, $\\mathbb{E}[r_t | x_t, i] \\approx x_t^\\top w^{(i)}$, and uses online ridge regression to estimate the unknown weight vectors $\\hat{w}^{(i)}$.\n\nFor each action $i \\in \\mathcal{A}$, the algorithm maintains a design matrix $A_i \\in \\mathbb{R}^{(d+2) \\times (d+2)}$ and a reward vector $b_i \\in \\mathbb{R}^{d+2}$. These are initialized as $A_i = I_{d+2}$ (the identity matrix, corresponding to a regularizing prior) and $b_i = \\mathbf{0}$. After observing a feature-reward pair $(x_t, r_t)$ for a chosen action $i_t$, the statistics are updated as:\n$$\nA_{i_t} \\leftarrow A_{i_t} + x_t x_t^\\top\n$$\n$$\nb_{i_t} \\leftarrow b_{i_t} + r_t x_t\n$$\nAt each time step $t$, the agent computes an estimate of the weight vector for each action $i$:\n$$\n\\hat{w}^{(i)} = A_i^{-1} b_i\n$$\nThe core of LinUCB lies in its action selection strategy. Instead of greedily choosing the action with the highest predicted reward $x_t^\\top \\hat{w}^{(i)}$, it adds an exploration bonus. This bonus is proportional to the uncertainty in the estimate for the current context $x_t$. The action $i_t$ is selected according to:\n$$\ni_t = \\arg\\max_{i \\in \\mathcal{A}} \\left( x_t^\\top \\hat{w}^{(i)} + \\alpha \\sqrt{x_t^\\top A_i^{-1} x_t} \\right)\n$$\nThe term $x_t^\\top \\hat{w}^{(i)}$ is the predicted reward (exploitation), and the term $\\alpha \\sqrt{x_t^\\top A_i^{-1} x_t}$ is the upper confidence bound on that estimate (exploration). The hyperparameter $\\alpha \\ge 0$ controls the trade-off; for this implementation, a standard value of $\\alpha = 1.0$ is chosen.\n\nThe simulation for each test case proceeds as follows:\n$1$. Set a fixed random seed for reproducibility.\n$2$. Initialize simulation parameters: horizon $T$, number of actions $K$, initial accuracy $a_0$, noise $\\sigma$, and true weights $w^{(i)}$.\n$3$. Initialize the LinUCB agent with $K$ arms and feature dimension $d+2=4$.\n$4$. Initialize the current accuracy $a_t = a_0$ and cumulative reward $R = 0$.\n$5$. For each time step $t$ from $1$ to $T$:\n    a. Calculate the current slack $s_t = 1 - a_t$.\n    b. Sample a context vector $c_t \\sim \\mathcal{N}(\\mu, \\Sigma)$, where $\\mu=[0,0]$ and $\\Sigma=I_2$.\n    c. Construct the augmented feature vector $x_t = [1, c_{t,1}, c_{t,2}, s_t]^\\top$.\n    d. The LinUCB agent selects an action $i_t$ using the UCB criterion.\n    e. The environment generates a reward $r_t$ based on the true weight $w^{(i_t)}$, the feature vector $x_t$, a random noise sample $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$, and applies the specified truncations.\n    f. Update the classification accuracy $a_{t+1} = \\min\\{1, a_t + r_t\\}$ and the cumulative reward $R = R + r_t$.\n    g. The agent's internal model is updated with the observation $(x_t, i_t, r_t)$.\n$6$. After $T$ steps, the final accuracy $a_T$ and total cumulative reward $R$ are recorded. This process is repeated for all three test cases specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    test_cases = [\n        {\n            \"K\": 3, \"T\": 200, \"a0\": 0.5, \"sigma\": 0.05,\n            \"w\": np.array([\n                [0.05, 0.8, 0.0, 0.5],\n                [0.05, -0.2, 0.9, 0.3],\n                [0.05, 0.4, 0.4, 0.7]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        },\n        {\n            \"K\": 2, \"T\": 100, \"a0\": 0.95, \"sigma\": 0.05,\n            \"w\": np.array([\n                [0.02, 0.5, 0.5, 0.2],\n                [0.02, 0.6, -0.1, 0.2]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        },\n        {\n            \"K\": 3, \"T\": 300, \"a0\": 0.4, \"sigma\": 0.3,\n            \"w\": np.array([\n                [0.10, 0.3, 0.3, 0.4],\n                [0.10, 0.35, 0.25, 0.35],\n                [0.10, 0.2, 0.5, 0.3]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        final_a, total_r = run_simulation(params)\n        all_results.append(f\"[{final_a:.6f},{total_r:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nclass LinUCBAgent:\n    \"\"\"\n    Implements the Linear Upper Confidence Bound (LinUCB) algorithm.\n    \"\"\"\n    def __init__(self, K, d, alpha=1.0):\n        self.K = K\n        self.dim = d + 2  # bias, context_dims, slack\n        self.alpha = alpha\n        # Initialize A as identity matrix and b as zero vector for each arm\n        self.A = [np.identity(self.dim) for _ in range(K)]\n        self.b = [np.zeros(self.dim) for _ in range(K)]\n\n    def select_action(self, x_t):\n        \"\"\"\n        Selects an action based on the UCB criterion for the given context.\n        \"\"\"\n        scores = []\n        for i in range(self.K):\n            A_inv = np.linalg.inv(self.A[i])\n            w_hat = A_inv @ self.b[i]\n            p_t_i = x_t.T @ w_hat\n            ucb = self.alpha * np.sqrt(x_t.T @ A_inv @ x_t)\n            scores.append(p_t_i + ucb)\n        \n        return np.argmax(scores)\n\n    def update(self, action_idx, x_t, r_t):\n        \"\"\"\n        Updates the model for the chosen action with the observed reward.\n        \"\"\"\n        self.A[action_idx] += np.outer(x_t, x_t)\n        self.b[action_idx] += r_t * x_t\n\ndef run_simulation(params):\n    \"\"\"\n    Runs a single simulation episode for a given set of parameters.\n    \"\"\"\n    K = params[\"K\"]\n    T = params[\"T\"]\n    a_t = params[\"a0\"]\n    sigma = params[\"sigma\"]\n    true_w = params[\"w\"]\n    mu = params[\"mu\"]\n    d = params[\"d\"]\n    \n    agent = LinUCBAgent(K, d, alpha=1.0)\n    \n    cumulative_reward = 0.0\n    context_cov = np.identity(d)\n\n    for _ in range(T):\n        s_t = 1.0 - a_t\n        \n        # Sample context and form feature vector\n        c_t = np.random.multivariate_normal(mu, context_cov)\n        x_t = np.concatenate(([1.0], c_t, [s_t]))\n\n        # Agent selects an action\n        action_idx = agent.select_action(x_t)\n\n        # Environment generates reward\n        noise = np.random.normal(0, sigma)\n        raw_reward = x_t.T @ true_w[action_idx] + noise\n        r_t = np.min([s_t, np.max([0, raw_reward])])\n\n        # Update system state\n        a_t = np.min([1.0, a_t + r_t])\n        cumulative_reward += r_t\n\n        # Agent updates its model\n        agent.update(action_idx, x_t, r_t)\n\n    return a_t, cumulative_reward\n\nsolve()\n```", "id": "3186203"}, {"introduction": "This final practice advances to policy gradient methods, a cornerstone of modern reinforcement learning, to automate a critical decision in computational science. Your task is to train an agent that learns a policy to choose between a fast, biased approximation and a slow, accurate simulation, balancing the tradeoff between computational cost and scientific accuracy. You will implement a parameterized softmax policy and optimize it directly using gradient ascent, providing a first-principles understanding of how policy-based agents learn. This exercise showcases how RL can optimize the scientific workflow itself, leading to more efficient discovery. [@problem_id:3186206]", "problem": "You are asked to formalize and implement a minimal Reinforcement Learning (RL) setup for a scientifically grounded decision task: selecting which computational approximation to use for evaluating a set of scientific hypotheses, balancing expected accuracy against computational cost. The scenario is a contextual bandit, where each context corresponds to one hypothesis with a given latent parameter, and each action corresponds to one of two computational approximations.\n\nYou must construct and solve the problem starting only from foundational definitions. The agent must learn a policy that maps a numeric context to an action by maximizing expected reward. The reward must encode the accuracy-cost tradeoff. You must not hard-code any oracle that directly outputs the optimal action; instead, solve the optimization by learning a parameterized policy from first principles using gradient ascent on the expected return.\n\nFundamental setup:\n\n- There is a finite set of hypotheses indexed by $h \\in \\mathcal{H}$. Each hypothesis has a true scalar latent value $ \\theta_h \\in \\mathbb{R} $.\n- On each decision, the agent observes a fixed feature vector $ x_h \\in \\mathbb{R}^d $ constructed from $ \\theta_h $ and then chooses action $ a \\in \\{0,1\\} $. Action $ a=0 $ denotes a mean-field approximation (deterministic, biased), and action $ a=1 $ denotes a Monte Carlo approximation (stochastic, unbiased with variance).\n- The agent receives a scalar reward $ r $ defined by the negative squared error between the chosen approximation’s estimate and the true value, penalized by a linear computational cost. Formally, with an instantaneous estimate $ \\widehat{\\theta}_{h,a} $, the reward is\n$$\nr(h,a) \\;=\\; -\\bigl(\\widehat{\\theta}_{h,a} - \\theta_h\\bigr)^2 \\;-\\; \\lambda \\, c_a,\n$$\nwhere $ \\lambda \\ge 0 $ is a tradeoff coefficient, and $ c_a \\ge 0 $ is the action-dependent cost. No physical units are involved.\n\n- For the mean-field approximation ($ a=0 $), the estimate is a deterministic biased shrinkage\n$$\n\\widehat{\\theta}_{h,0} \\;=\\; \\alpha \\, \\theta_h,\n$$\nwith $ \\alpha \\in (0,1) $.\n- For the Monte Carlo approximation ($ a=1 $), the estimate is an unbiased noisy observation\n$$\n\\widehat{\\theta}_{h,1} \\;=\\; \\theta_h \\;+\\; \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith variance parameter $ \\sigma^2 \\ge 0 $. Therefore, when $ a=1 $, the reward is a random variable due to $ \\varepsilon $.\n\nPolicy and objective:\n\n- Use a softmax policy $ \\pi_\\mathbf{W}(a \\mid x_h) $ over two actions with linear logits. Let $ d=2 $ and $ x_h = \\begin{bmatrix}1 \\\\ \\theta_h^2\\end{bmatrix} \\in \\mathbb{R}^2 $. Let $ \\mathbf{W} \\in \\mathbb{R}^{2 \\times 2} $ collect the action-specific parameter vectors in its rows. The logits are $ z_a = \\mathbf{w}_a^{\\top} x_h $ and the action probabilities are\n$$\n\\pi_\\mathbf{W}(a \\mid x_h) \\;=\\; \\frac{\\exp(z_a)}{\\exp(z_0) + \\exp(z_1)}.\n$$\n- The objective is the average expected reward over the finite hypothesis set,\n$$\nJ(\\mathbf{W}) \\;=\\; \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\,\\mathbb{E}_{a \\sim \\pi_\\mathbf{W}(\\cdot \\mid x_h)} \\bigl[\\, \\mathbb{E}[\\, r(h,a) \\mid h, a \\,] \\,\\bigr],\n$$\nwhich follows from the definition of expected return for a contextual bandit and the law of total expectation. You must maximize $ J(\\mathbf{W}) $ by gradient ascent using only fundamental definitions. You must not assume the gradient formula; derive it from the softmax and the expectation definitions.\n\nEvaluation metric:\n\n- After training, for each $ h \\in \\mathcal{H} $, convert the stochastic policy into a deterministic choice by selecting the action with maximum probability $ \\arg\\max_a \\pi_\\mathbf{W}(a \\mid x_h) $.\n- Let $ Q(h,a) = \\mathbb{E}[\\,r(h,a) \\mid h,a\\,] $ denote the expected reward under action $ a $ in context $ h $. Evaluate:\n    - The learned policy value $ V_{\\text{learn}} = \\frac{1}{|\\mathcal{H}|} \\sum_h Q(h, \\arg\\max_a \\pi_\\mathbf{W}(a \\mid x_h)) $.\n    - The optimal policy value $ V_{\\text{opt}} = \\frac{1}{|\\mathcal{H}|} \\sum_h \\max_a Q(h,a) $.\n    - The regret $ \\Delta = V_{\\text{opt}} - V_{\\text{learn}} $.\n- Report the regret $ \\Delta $ as a floating-point number for each test case. No physical units apply. Round each regret to exactly six decimal places.\n\nExplicit expressions available from first principles for $ Q(h,a) $:\n\n- Because $ \\widehat{\\theta}_{h,0} = \\alpha \\theta_h $ is deterministic,\n$$\nQ(h,0) \\;=\\; -(\\alpha\\theta_h - \\theta_h)^2 \\;-\\; \\lambda c_0 \\;=\\; -(\\alpha - 1)^2\\theta_h^2 \\;-\\; \\lambda c_0.\n$$\n- Because $ \\widehat{\\theta}_{h,1} = \\theta_h + \\varepsilon $ with $ \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2) $ and $ \\mathbb{E}[\\varepsilon^2] = \\sigma^2 $,\n$$\nQ(h,1) \\;=\\; -\\mathbb{E}[(\\varepsilon)^2] \\;-\\; \\lambda c_1 \\;=\\; -\\sigma^2 \\;-\\; \\lambda c_1.\n$$\n\nTest suite:\n\nUse the fixed hypothesis set\n$$\n\\mathcal{H} \\;=\\; \\{\\,-2.0,\\,-1.0,\\,-0.5,\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0\\,\\},\n$$\nso $ |\\mathcal{H}| = 8 $, with features $ x_h = \\begin{bmatrix}1 \\\\ \\theta_h^2\\end{bmatrix} $. For each test case, define parameters $ (\\alpha,\\sigma,\\lambda,c_0,c_1) $ as:\n\n- Case $1$ (typical mixed regime): $ \\alpha=0.5, \\ \\sigma=0.2, \\ \\lambda=0.5, \\ c_0=1.0, \\ c_1=1.0 $.\n- Case $2$ (cost-dominated with expensive Monte Carlo): $ \\alpha=0.7, \\ \\sigma=0.1, \\ \\lambda=2.0, \\ c_0=1.0, \\ c_1=5.0 $.\n- Case $3$ (accuracy-only, $ \\lambda=0.0 $): $ \\alpha=0.6, \\ \\sigma=0.05, \\ \\lambda=0.0, \\ c_0=0.0, \\ c_1=0.0 $.\n- Case $4$ (threshold behavior with moderate cost): $ \\alpha=0.8, \\ \\sigma=0.1, \\ \\lambda=0.3, \\ c_0=1.0, \\ c_1=1.0 $.\n- Case $5$ (edge tie near $ |\\theta|=1 $): $ \\alpha=0.75, \\ \\sigma=0.25, \\ \\lambda=0.4, \\ c_0=1.0, \\ c_1=1.0 $.\n\nImplementation requirements:\n\n- Implement gradient ascent on $ J(\\mathbf{W}) $ using the softmax policy and the expected rewards $ Q(h,a) $ defined above, derived from the fundamental laws of expectation and the softmax definition. Use a stopping criterion based on small gradient norm or a maximum number of iterations to ensure convergence. Initialize $ \\mathbf{W} $ to zeros.\n- For reproducibility, no stochasticity is required in training since $ Q(h,a) $ is known in closed form. You must not sample $ \\varepsilon $ during training or evaluation; only $ Q(h,a) $ is used.\n- For each test case, output the regret $ \\Delta $ rounded to six decimals.\n\nFinal output format:\n\nYour program should produce a single line of output containing the regrets for Cases $1$ through $5$ as a comma-separated list enclosed in square brackets (e.g., \"[0.000000,0.123456,0.000100,0.000000,0.000000]\"). No extra text should be printed.", "solution": "The posed problem is a contextual bandit scenario designed to model a common decision in computational science: the trade-off between the accuracy of a computational approximation and its associated cost. The agent must learn a policy, parameterized by a weight matrix $ \\mathbf{W} $, to select an action (an approximation method) based on a context (features of a scientific hypothesis) to maximize the expected reward. The reward function encodes both accuracy, via negative squared error, and computational cost. The problem is valid, well-posed, and requires a solution derived from first principles.\n\nWe begin by formalizing the objective and then deriving the gradient required for its optimization via gradient ascent.\n\nThe set of hypotheses is $ \\mathcal{H} $, where each hypothesis $ h \\in \\mathcal{H} $ is associated with a true latent value $ \\theta_h $. The agent observes a feature vector $ x_h \\in \\mathbb{R}^2 $ for each hypothesis, where $ x_h = [1, \\theta_h^2]^\\top $. The available actions are $ a \\in \\{0,1\\} $. The expected reward for taking action $ a $ on hypothesis $ h $, denoted by the quality function $ Q(h,a) $, is given.\nFor action $ a=0 $ (mean-field):\n$$\nQ(h,0) = -(\\alpha - 1)^2\\theta_h^2 - \\lambda c_0\n$$\nFor action $ a=1 $ (Monte Carlo):\n$$\nQ(h,1) = -\\sigma^2 - \\lambda c_1\n$$\n\nThe agent's policy $ \\pi_\\mathbf{W}(a \\mid x_h) $ is a softmax function over linear logits. The logits are $ z_a = \\mathbf{w}_a^\\top x_h $, where $ \\mathbf{w}_a^\\top $ is the $ a $-th row of the weight matrix $ \\mathbf{W} \\in \\mathbb{R}^{2 \\times 2} $. The policy is:\n$$\n\\pi_\\mathbf{W}(a \\mid x_h) = \\frac{\\exp(\\mathbf{w}_a^\\top x_h)}{\\sum_{k \\in \\{0,1\\}} \\exp(\\mathbf{w}_k^\\top x_h)}\n$$\nThe objective is to find the parameters $ \\mathbf{W} $ that maximize the average expected reward over all hypotheses:\n$$\nJ(\\mathbf{W}) = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\mathbb{E}_{a \\sim \\pi_\\mathbf{W}(\\cdot \\mid x_h)} [Q(h,a)] = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\sum_{a \\in \\{0,1\\}} \\pi_\\mathbf{W}(a \\mid x_h) Q(h,a)\n$$\nTo maximize $ J(\\mathbf{W}) $, we employ gradient ascent. This requires computing the gradient of $ J(\\mathbf{W}) $ with respect to the parameters $ \\mathbf{W} $, which are the vectors $ \\mathbf{w}_0 $ and $ \\mathbf{w}_1 $. Due to the linearity of the gradient operator and the summation, we can focus on the gradient of the expected reward for a single hypothesis $ h $, which we denote as $ J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} \\pi_\\mathbf{W}(a \\mid x_h) Q(h,a) $.\n\nThe gradient of $ J_h(\\mathbf{W}) $ with respect to an arbitrary weight vector $ \\mathbf{w}_k $ ($ k \\in \\{0,1\\} $) is:\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\nabla_{\\mathbf{w}_k} \\pi_\\mathbf{W}(a \\mid x_h)\n$$\nTo compute the gradient of the softmax policy, we use the log-derivative identity, $ \\nabla \\pi = \\pi \\nabla \\log \\pi $. The gradient of the log-policy probability is:\n$$\n\\nabla_{\\mathbf{w}_k} \\log \\pi_\\mathbf{W}(a \\mid x_h) = \\nabla_{\\mathbf{w}_k} \\left( \\mathbf{w}_a^\\top x_h - \\log\\sum_{j \\in \\{0,1\\}} e^{\\mathbf{w}_j^\\top x_h} \\right)\n$$\n$$\n= (\\nabla_{\\mathbf{w}_k} \\mathbf{w}_a^\\top x_h) - \\frac{1}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} (\\nabla_{\\mathbf{w}_k} \\sum_j e^{\\mathbf{w}_j^\\top x_h})\n$$\nUsing the Kronecker delta $ \\delta_{ak} $, we have $ \\nabla_{\\mathbf{w}_k} \\mathbf{w}_a^\\top x_h = \\delta_{ak} x_h $. The second term becomes:\n$$\n\\frac{1}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} (e^{\\mathbf{w}_k^\\top x_h} \\nabla_{\\mathbf{w}_k} \\mathbf{w}_k^\\top x_h) = \\frac{e^{\\mathbf{w}_k^\\top x_h}}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} x_h = \\pi_\\mathbf{W}(k \\mid x_h) x_h\n$$\nCombining these results, we get:\n$$\n\\nabla_{\\mathbf{w}_k} \\log \\pi_\\mathbf{W}(a \\mid x_h) = (\\delta_{ak} - \\pi_\\mathbf{W}(k \\mid x_h)) x_h\n$$\nSubstituting this back into the gradient of $ J_h(\\mathbf{W}) $:\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) (\\delta_{ak} - \\pi_\\mathbf{W}(k \\mid x_h)) x_h\n$$\nWe can rearrange the sum:\n$$\n= \\left( Q(h,k)\\pi_\\mathbf{W}(k \\mid x_h) - \\pi_\\mathbf{W}(k \\mid x_h) \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) \\right) x_h\n$$\nLet $ V_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) $ be the expected value for hypothesis $ h $ under policy $ \\pi_\\mathbf{W} $. The expression simplifies to the standard policy gradient form with a baseline:\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\pi_\\mathbf{W}(k \\mid x_h) (Q(h,k) - V_h(\\mathbf{W})) x_h\n$$\nThis is a general and fundamental result. For our two-action case, we can further simplify the baseline term $ Q(h,k) - V_h(\\mathbf{W}) $:\nFor $ k=0 $: $ Q(h,0) - (\\pi_0 Q_0 + \\pi_1 Q_1) = (1-\\pi_0)Q_0 - \\pi_1 Q_1 = \\pi_1 Q_0 - \\pi_1 Q_1 = \\pi_1(Q_0 - Q_1) $.\nFor $ k=1 $: $ Q(h,1) - (\\pi_0 Q_0 + \\pi_1 Q_1) = Q_1 - \\pi_0 Q_0 - (1-\\pi_0)Q_1 = \\pi_0 Q_1 - \\pi_0 Q_0 = \\pi_0(Q_1 - Q_0) $.\nThe gradients with respect to $ \\mathbf{w}_0 $ and $ \\mathbf{w}_1 $ for a single hypothesis $ h $ are:\n$$\n\\nabla_{\\mathbf{w}_0} J_h(\\mathbf{W}) = \\pi_0 (\\pi_1(Q_0 - Q_1)) x_h = \\pi_0\\pi_1(Q(h,0) - Q(h,1))x_h\n$$\n$$\n\\nabla_{\\mathbf{w}_1} J_h(\\mathbf{W}) = \\pi_1 (\\pi_0(Q_1 - Q_0)) x_h = \\pi_0\\pi_1(Q(h,1) - Q(h,0))x_h\n$$\nNote that $ \\nabla_{\\mathbf{w}_0} J_h(\\mathbf{W}) = -\\nabla_{\\mathbf{w}_1} J_h(\\mathbf{W}) $, which reflects the fact that the policy is determined by the difference in logits $ z_1 - z_0 = (\\mathbf{w}_1 - \\mathbf{w}_0)^\\top x_h $.\n\nThe total gradient for the objective $ J(\\mathbf{W}) $ is the average of these single-hypothesis gradients over the set $ \\mathcal{H} $:\n$$\n\\nabla_\\mathbf{W} J(\\mathbf{W}) = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\nabla_\\mathbf{W} J_h(\\mathbf{W})\n$$\nThe learning algorithm is then a simple gradient ascent procedure. Starting with $ \\mathbf{W} $ as a zero matrix, we iteratively update the weights using a learning rate $ \\eta $:\n$$\n\\mathbf{W}_{t+1} = \\mathbf{W}_t + \\eta \\nabla_\\mathbf{W} J(\\mathbf{W}_t)\n$$\nThis process is continued for a fixed number of iterations or until the norm of the gradient is below a tolerance, ensuring convergence to a local maximum of the objective function.\n\nAfter training, the regret $ \\Delta $ is calculated. The optimal policy's value $ V_{\\text{opt}} $ is determined by taking the best possible action for each hypothesis: $ V_{\\text{opt}} = \\frac{1}{|\\mathcal{H}|} \\sum_h \\max_{a} Q(h,a) $. The learned policy's value $ V_{\\text{learn}} $ is found by first converting the stochastic softmax policy to a deterministic one by choosing the action with the highest probability, $ a^*_h = \\arg\\max_a \\pi_{\\mathbf{W}}(a \\mid x_h) $, and then calculating the average reward: $ V_{\\text{learn}} = \\frac{1}{|\\mathcal{H}|} \\sum_h Q(h, a^*_h) $. The regret is the difference $ \\Delta = V_{\\text{opt}} - V_{\\text{learn}} $. A non-zero regret indicates that for at least one hypothesis, the learned deterministic policy did not select an action that yields the maximum expected reward.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the contextual bandit problem for a series of test cases using\n    gradient ascent on the expected reward.\n    \"\"\"\n    \n    # Fixed hypothesis set\n    thetas = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0])\n    H_size = len(thetas)\n    # Feature vectors x_h = [1, theta_h^2]^T\n    features = np.vstack((np.ones(H_size), thetas**2)).T\n\n    # Test cases: (alpha, sigma, lambda, c0, c1)\n    test_cases = [\n        (0.5, 0.2, 0.5, 1.0, 1.0),\n        (0.7, 0.1, 2.0, 1.0, 5.0),\n        (0.6, 0.05, 0.0, 0.0, 0.0),\n        (0.8, 0.1, 0.3, 1.0, 1.0),\n        (0.75, 0.25, 0.4, 1.0, 1.0)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        alpha, sigma, lam, c0, c1 = case\n        \n        # --- Pre-calculate Q-values for all hypotheses ---\n        # Q_values matrix of shape (H_size, 2)\n        Q_values = np.zeros((H_size, 2))\n        \n        # Q(h, 0) for all h\n        Q_values[:, 0] = -((alpha - 1)**2) * (thetas**2) - lam * c0\n        # Q(h, 1) for all h\n        Q_values[:, 1] = -(sigma**2) - lam * c1\n        \n        # --- Gradient Ascent Training ---\n        W = np.zeros((2, 2))  # Policy weights: W[0] for w0, W[1] for w1\n        learning_rate = 0.1\n        num_iterations = 20000\n\n        for _ in range(num_iterations):\n            grad_W = np.zeros((2, 2))\n            \n            # Compute logits for all hypotheses at once\n            # logits shape: (H_size, 2)\n            logits = features @ W.T\n            \n            # Stable softmax\n            logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n            exps = np.exp(logits_stable)\n            probs = exps / np.sum(exps, axis=1, keepdims=True) # shape (H_size, 2)\n            \n            # Calculate gradient contributions\n            # pi_0 * pi_1 * (Q0 - Q1) for each hypothesis\n            factor = probs[:, 0] * probs[:, 1] * (Q_values[:, 0] - Q_values[:, 1]) # shape (H_size,)\n\n            # Reshape factor to (H_size, 1) to broadcast with features (H_size, 2)\n            factor = factor[:, np.newaxis]\n            \n            # Gradient for w0 for all h\n            grad_w0 = np.sum(factor * features, axis=0)\n            # Gradient for w1 is the negative of grad_w0\n            grad_w1 = -grad_w0\n            \n            # Average gradient over all hypotheses\n            grad_W[0, :] = grad_w0 / H_size\n            grad_W[1, :] = grad_w1 / H_size\n\n            # Update weights\n            W += learning_rate * grad_W\n\n        # --- Evaluation ---\n        # Optimal policy value\n        V_opt_sum = np.sum(np.max(Q_values, axis=1))\n        V_opt = V_opt_sum / H_size\n        \n        # Learned policy value\n        # Recalculate final probabilities with trained W\n        final_logits = features @ W.T\n        final_logits_stable = final_logits - np.max(final_logits, axis=1, keepdims=True)\n        final_exps = np.exp(final_logits_stable)\n        final_probs = final_exps / np.sum(final_exps, axis=1, keepdims=True)\n        \n        # Deterministic learned actions\n        learned_actions = np.argmax(final_probs, axis=1)\n        \n        # Calculate value of learned policy\n        V_learn_sum = np.sum(Q_values[np.arange(H_size), learned_actions])\n        V_learn = V_learn_sum / H_size\n        \n        # Regret\n        regret = V_opt - V_learn\n        \n        # Format and append result\n        results.append(f\"{regret:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3186206"}]}