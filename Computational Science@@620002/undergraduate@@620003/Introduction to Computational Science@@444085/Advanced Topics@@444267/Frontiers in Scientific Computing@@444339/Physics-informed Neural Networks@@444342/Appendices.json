{"hands_on_practices": [{"introduction": "The heart of a Physics-Informed Neural Network lies in its loss function, which translates the governing laws of a physical system into a quantifiable objective for the network to minimize. This first exercise [@problem_id:2126324] provides foundational practice in this critical step. You will construct the loss function for a classic electrostatic problem, learning how to combine the residual of the governing partial differential equation with the constraints imposed by boundary conditions.", "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.", "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "Many physical phenomena evolve over time, requiring us to account not only for spatial boundary conditions but also for the system's state at an initial moment. This practice [@problem_id:2126340] builds upon the previous exercise by introducing the time dimension, using the 1D heat equation as a model system. You will learn to formulate a complete loss function that incorporates the governing PDE, initial conditions, and time-dependent boundary conditions, a crucial skill for modeling dynamic systems.", "problem": "An engineer is developing a simulation for heat transfer in a novel one-dimensional composite rod of length $L$. The temperature distribution along the rod, $u(x, t)$, is governed by the 1D heat equation:\n$$\n\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}\n$$\nfor $x \\in [0, L]$ and $t \\in [0, T]$, where $\\alpha$ is the thermal diffusivity of the material.\n\nThe initial temperature distribution at $t=0$ is given by $u(x, 0) = \\sin\\left(\\frac{\\pi x}{L}\\right)$.\nThe boundary conditions are as follows:\n1. The end at $x=0$ is held at a constant zero temperature: $u(0, t) = 0$.\n2. The end at $x=L$ is subjected to a periodic heat source, resulting in a time-varying temperature: $u(L, t) = A \\cos(\\omega t)$, where $A$ is an amplitude and $\\omega$ is an angular frequency.\n\nThe engineer decides to use a Physics-Informed Neural Network (PINN) to approximate the solution $u(x, t)$. The PINN is represented by a neural network $\\hat{u}(x, t; \\theta)$ with trainable parameters $\\theta$. To train the network, a total loss function, $\\mathcal{L}_{\\text{total}}$, is minimized. This loss function is constructed by sampling points and enforcing the governing PDE, the initial condition, and the boundary conditions.\n\nThe sampling points are defined as:\n- A set of $N_p$ collocation points $\\{ (x_i^{(p)}, t_i^{(p)}) \\}_{i=1}^{N_p}$ randomly sampled from the interior of the domain $(0, L) \\times (0, T]$.\n- A set of $N_{ic}$ initial points $\\{ x_j^{(ic)} \\}_{j=1}^{N_{ic}}$ randomly sampled from the spatial domain $[0, L]$ at $t=0$.\n- A set of $N_{bc}$ boundary time points $\\{ t_k^{(bc)} \\}_{k=1}^{N_{bc}}$ randomly sampled from the time domain $[0, T]$ for each boundary.\n\nAssuming the total loss is a sum of the mean squared errors for the PDE residual, the initial condition, and the boundary conditions (with equal weighting), which of the following expressions correctly represents the total loss function $\\mathcal{L}_{\\text{total}}$?\n\nFor conciseness, we define the following terms based on the network's output $\\hat{u} = \\hat{u}(x, t; \\theta)$:\n- $\\mathcal{L}_{\\text{PDE}} = \\frac{1}{N_p} \\sum_{i=1}^{N_p} \\left| \\frac{\\partial \\hat{u}}{\\partial t}(x_i^{(p)}, t_i^{(p)}) - \\alpha \\frac{\\partial^2 \\hat{u}}{\\partial x^2}(x_i^{(p)}, t_i^{(p)}) \\right|^2$\n- $\\mathcal{L}_{\\text{IC}} = \\frac{1}{N_{ic}} \\sum_{j=1}^{N_{ic}} \\left| \\hat{u}(x_j^{(ic)}, 0) - \\sin\\left(\\frac{\\pi x_j^{(ic)}}{L}\\right) \\right|^2$\n- $\\mathcal{L}_{\\text{BC,0}} = \\frac{1}{N_{bc}} \\sum_{k=1}^{N_{bc}} \\left| \\hat{u}(0, t_k^{(bc)}) \\right|^2$\n- $\\mathcal{L}_{\\text{BC,L}} = \\frac{1}{N_{bc}} \\sum_{k=1}^{N_{bc}} \\left| \\hat{u}(L, t_k^{(bc)}) - A \\cos(\\omega t_k^{(bc)}) \\right|^2$\n\nA. $\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{IC}} + \\mathcal{L}_{\\text{BC,0}} + \\frac{1}{N_{bc}} \\sum_{k=1}^{N_{bc}} \\left| \\hat{u}(L, t_k^{(bc)}) - A \\right|^2$\n\nB. $\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{BC,0}} + \\mathcal{L}_{\\text{BC,L}}$\n\nC. $\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{IC}} + \\mathcal{L}_{\\text{BC,0}} + \\mathcal{L}_{\\text{BC,L}}$\n\nD. $\\mathcal{L}_{\\text{total}} = \\frac{1}{N_{ic}} \\sum_{j=1}^{N_{ic}} \\left| \\frac{\\partial \\hat{u}}{\\partial t}(x_j^{(ic)}, 0) - \\alpha \\frac{\\partial^2 \\hat{u}}{\\partial x^2}(x_j^{(ic)}, 0) \\right|^2 + \\mathcal{L}_{\\text{IC}} + \\mathcal{L}_{\\text{BC,0}} + \\mathcal{L}_{\\text{BC,L}}$\n\nE. $\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{PDE}} + \\frac{1}{N_{ic}} \\sum_{j=1}^{N_{ic}} \\left| \\hat{u}(x_j^{(ic)}, T) - \\sin\\left(\\frac{\\pi x_j^{(ic)}}{L}\\right) \\right|^2 + \\mathcal{L}_{\\text{BC,0}} + \\mathcal{L}_{\\text{BC,L}}$", "solution": "We require a Physics-Informed Neural Network loss that enforces, with equal weighting, the governing PDE inside the space-time interior, the initial condition at $t=0$ over $x \\in [0,L]$, and both boundary conditions at $x=0$ and $x=L$ over $t \\in [0,T]$. The standard mean squared error construction for a PINN is the sum of these components:\n- PDE residual enforced at collocation points $(x_{i}^{(p)}, t_{i}^{(p)})$ in the interior $(0,L) \\times (0,T]$, yielding $\\mathcal{L}_{\\text{PDE}}$ as defined.\n- Initial condition enforced at $t=0$ for sampled spatial points $x_{j}^{(ic)} \\in [0,L]$, yielding $\\mathcal{L}_{\\text{IC}}$ as defined.\n- Boundary at $x=0$ enforced for sampled times $t_{k}^{(bc)} \\in [0,T]$, giving $\\mathcal{L}_{\\text{BC,0}}$ as defined.\n- Boundary at $x=L$ enforced for sampled times $t_{k}^{(bc)} \\in [0,T]$, giving $\\mathcal{L}_{\\text{BC,L}}$ as defined.\n\nWith equal weighting, the correct total loss is the sum:\n$$\n\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{PDE}}+\\mathcal{L}_{\\text{IC}}+\\mathcal{L}_{\\text{BC,0}}+\\mathcal{L}_{\\text{BC,L}}.\n$$\nNow assess the options:\n- Option A replaces the boundary target at $x=L$ by $A$ rather than $A\\cos(\\omega t)$, which fails to match the specified time-dependent boundary condition, so it is incorrect.\n- Option B omits the initial condition term entirely, which violates the requirement to enforce the initial condition, so it is incorrect.\n- Option C is precisely the sum of the four correctly defined components with equal weighting, so it is correct.\n- Option D computes the PDE residual on the initial-condition sample set at $t=0$ rather than on the interior collocation points; it contradicts the stated sampling of PDE residual points and under-enforces the PDE in the interior, so it is incorrect.\n- Option E enforces a final-time condition $t=T$ equal to the initial profile, which is not specified by the problem and is generally false for the heat equation unless specially constructed; hence it is incorrect.\n\nTherefore, the correct expression is the simple sum of the four given components, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "2126340"}, {"introduction": "While adding terms to the loss function is a flexible way to enforce physical constraints, it is not the only method. An alternative and often more robust approach is to design the neural network's architecture to satisfy certain conditions by construction. This exercise [@problem_id:2126300] introduces this powerful technique of \"hard-coding\" constraints, demonstrating how to create a transformation that guarantees the network's output will always satisfy the required Dirichlet boundary conditions, often leading to faster and more stable training.", "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.", "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.", "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$", "id": "2126300"}]}