{"hands_on_practices": [{"introduction": "The simplex method progresses from one vertex of the feasible region to a better one in a series of steps called pivots. Mastering the pivot operation is the first step toward understanding the entire algorithm. This exercise [@problem_id:2221017] provides a concrete simplex tableau and asks you to perform a single, complete pivot, giving you hands-on practice with the core mechanics of selecting an entering and leaving variable to improve the objective function.", "problem": "A marketing analytics firm is using linear programming to determine the optimal allocation of a client's advertising budget. The goal is to maximize customer reach. The decision variables are $x_1$, the number of ad units purchased on social media, and $x_2$, the number of ad units purchased on search engines. The problem has been formulated as a standard maximization problem, and the initial simplex tableau is given below. The variables $s_1$ and $s_2$ are slack variables representing unused budget and staff hours, respectively. The variable $Z$ represents the total customer reach to be maximized.\n\nThe rows of the tableau correspond to the objective function $Z$, and the two constraints involving $s_1$ and $s_2$ as basic variables. The columns correspond to the variables $Z, x_1, x_2, s_1, s_2$, and the Right-Hand Side (RHS) values.\n\nThe initial tableau is:\n$$\n\\begin{array}{c|cccccc}\n\\text{Basic Variable} & Z & x_1 & x_2 & s_1 & s_2 & \\text{RHS} \\\\\n\\hline\nZ & 1 & -5 & -4 & 0 & 0 & 0 \\\\\ns_1 & 0 & 10 & 8 & 1 & 0 & 80 \\\\\ns_2 & 0 & 2 & 3 & 0 & 1 & 24 \\\\\n\\end{array}\n$$\n\nPerform exactly one complete pivot operation according to the standard simplex algorithm for maximization problems. Determine the updated values for the coefficient of $x_2$ in the Z-row, the coefficient of $s_1$ in the Z-row, and the new value of the objective function $Z$ (the RHS value of the Z-row).\n\nYour answer should be a set of three numerical values presented in a row matrix in the specified order: (coefficient of $x_2$, coefficient of $s_1$, value of $Z$).", "solution": "We apply the standard simplex pivot rules for a maximization problem.\n\n1) Identify the entering variable: in the Z-row, the most negative coefficient among decision variables is $-5$ for $x_{1}$, so $x_{1}$ enters.\n\n2) Perform the minimum ratio test on the $x_{1}$ column to choose the leaving variable:\n$$\n\\frac{80}{10}=8 \\quad \\text{for } s_{1}, \\qquad \\frac{24}{2}=12 \\quad \\text{for } s_{2}.\n$$\nThe minimum ratio is $8$, so $s_{1}$ leaves. The pivot element is $10$ in the $s_{1}$ row and $x_{1}$ column.\n\n3) Pivot to make the pivot element unity and eliminate $x_{1}$ from other rows.\n\nNormalize the pivot row ($s_{1}$ row) by dividing by $10$:\n$$\ns_{1}\\text{-row}: \\quad [0,\\;10,\\;8,\\;1,\\;0\\;|\\;80] \\;\\to\\; [0,\\;1,\\;\\tfrac{4}{5},\\;\\tfrac{1}{10},\\;0\\;|\\;8].\n$$\n\nEliminate $x_{1}$ from the Z-row by adding $5$ times the new $s_{1}$ row to the Z-row:\n$$\nZ\\text{-row}: \\quad [1,\\;-5,\\;-4,\\;0,\\;0\\;|\\;0] + 5\\cdot[0,\\;1,\\;\\tfrac{4}{5},\\;\\tfrac{1}{10},\\;0\\;|\\;8]\n= [1,\\;0,\\;0,\\;\\tfrac{1}{2},\\;0\\;|\\;40].\n$$\n\n(Eliminating $x_{1}$ from the $s_{2}$ row is part of the complete pivot but is not needed for the requested Z-row coefficients.)\n\nTherefore, after this single pivot, the coefficient of $x_{2}$ in the Z-row is $0$, the coefficient of $s_{1}$ in the Z-row is $\\tfrac{1}{2}$, and the new objective value $Z$ (RHS of the Z-row) is $40$.", "answer": "$$\\boxed{\\begin{pmatrix}0 & \\frac{1}{2} & 40\\end{pmatrix}}$$", "id": "2221017"}, {"introduction": "Once you understand how to pivot, it's crucial to grasp the geometric intuition behind the algorithm's behavior. A common misconception is that if the feasible region is infinitely large, the objective function must also be able to increase infinitely. This exercise [@problem_id:2443959] challenges you to construct a direct counterexample, demonstrating the subtle but critical relationship between a feasible region's directions of unboundedness and the gradient of the objective function.", "problem": "In linear programming, a common misconception is that an unbounded feasible region forces the objective function to be unbounded. Using only first principles, construct and analyze a two-dimensional linear program that serves as a counterexample to this claim.\n\nStart from the following foundational definitions:\n- A feasible region is the set of all points that satisfy the linear constraints of the problem.\n- A set is unbounded if there exists a direction vector along which one can move arbitrarily far while remaining in the set.\n- A linear objective function is a linear form $c^{\\top}x$ in the decision vector $x$.\n\nConsider the linear program in two variables $x_{1}$ and $x_{2}$:\n- Decision variables: $x_{1} \\geq 0$, $x_{2} \\geq 0$.\n- Constraint: $x_{1} \\leq 1$.\n- Objective: maximize $z = x_{1}$.\n\nTasks:\n1. Using the definitions above, argue directly from the constraint set that the feasible region is unbounded.\n2. Using only linearity of the objective and the constraint $x_{1} \\leq 1$, determine the optimal value of the objective function.\n3. Explain why the unboundedness of the feasible region does not translate into unboundedness of the objective in this instance, referring to the existence of a feasible direction in which the objective does not increase.\n\nProvide the maximal objective value as your final answer. Express your answer as an exact number with no rounding.", "solution": "The problem presents a linear program and asks for an analysis to demonstrate that an unbounded feasible region does not necessarily imply an unbounded objective function. We shall proceed by systematically addressing each of the posed tasks, adhering strictly to first principles and the provided definitions.\n\nThe linear program is defined over the decision variables $x_1$ and $x_2$ as follows:\nMaximize $z = x_1$\nSubject to:\n$x_1 \\leq 1$\n$x_1 \\geq 0$\n$x_2 \\geq 0$\n\nLet the feasible region be denoted by the set $S$. A point $p = (x_1, x_2)$ is in $S$ if and only if its coordinates satisfy all three constraints simultaneously. Thus, $S = \\{ (x_1, x_2) \\in \\mathbb{R}^2 \\mid 0 \\leq x_1 \\leq 1, x_2 \\geq 0 \\}$. This set represents a semi-infinite strip in the first quadrant of the Cartesian plane.\n\n**Task 1: Unboundedness of the Feasible Region**\n\nTo prove that the feasible region $S$ is unbounded, we must show, according to the provided definition, that there exists a direction vector along which one can move arbitrarily far while remaining in the set.\n\nLet us choose a point $p_0 \\in S$. A simple choice is $p_0 = (0, 0)$, which clearly satisfies $0 \\leq 0 \\leq 1$ and $0 \\geq 0$.\nNow, let us define a direction vector $d = (0, 1)^T$. We construct a ray starting from $p_0$ in the direction of $d$. Any point on this ray can be parameterized by $\\lambda \\geq 0$ as:\n$$ p(\\lambda) = p_0 + \\lambda d = (0, 0)^T + \\lambda (0, 1)^T = (0, \\lambda)^T $$\nThe coordinates of any such point are $(x_1, x_2) = (0, \\lambda)$. We must verify if $p(\\lambda)$ remains in the feasible region $S$ for any non-negative value of $\\lambda$. We check the constraints:\n1.  $x_1 \\leq 1$: The condition is $0 \\leq 1$, which is true.\n2.  $x_1 \\geq 0$: The condition is $0 \\geq 0$, which is true.\n3.  $x_2 \\geq 0$: The condition is $\\lambda \\geq 0$, which is true by our definition of the parameter $\\lambda$.\n\nSince all constraints are satisfied for any $\\lambda \\geq 0$, any point on the ray $(0, \\lambda)$ is in the feasible set $S$. As we can choose $\\lambda$ to be arbitrarily large, we can move an arbitrary distance from the origin along the direction $d=(0,1)^T$ and remain within $S$. Therefore, by the given definition, the feasible region $S$ is unbounded.\n\n**Task 2: Optimal Value of the Objective Function**\n\nThe objective is to maximize the function $z = x_1$ for all points $(x_1, x_2)$ in the feasible region $S$. The definition of $S$ includes the constraint $x_1 \\leq 1$. This constraint, by its very nature, imposes an upper bound on the possible values of $x_1$.\n\nThe objective function $z$ is a linear function that depends solely on the variable $x_1$. To maximize $z$, we must find the maximum possible value for $x_1$ that is permitted within the feasible region $S$. The constraints defining $S$ are $0 \\leq x_1 \\leq 1$ and $x_2 \\geq 0$. From the constraint $x_1 \\leq 1$, it is clear that the value of $x_1$ can never exceed $1$.\n\nDoes this maximum value exist within the feasible region? Yes. Consider the point $p^* = (1, 0)$.\n- Is $p^*$ feasible? We check the constraints: $x_1 = 1$ satisfies $0 \\leq 1 \\leq 1$, and $x_2 = 0$ satisfies $0 \\geq 0$. Thus, $p^* \\in S$.\n- At this point, the objective function value is $z = x_1 = 1$.\n\nSince for any point $(x_1, x_2) \\in S$, we have $x_1 \\leq 1$, it follows that the objective function $z = x_1$ is bounded above by $1$. Because we have found a feasible point where this value is achieved, the maximal value of the objective function is precisely $1$.\n\n**Task 3: Explanation of Bounded Objective with Unbounded Region**\n\nThe unboundedness of the feasible region does not translate to the unboundedness of the objective function in this case. The reason lies in the relationship between the gradient of the objective function and the direction(s) of unboundedness of the feasible region.\n\nThe objective function is $z = x_1$. This can be written in vector form as $z = c^T x$, where $x = (x_1, x_2)^T$ and the cost vector (which is the gradient of $z$) is $c = (1, 0)^T$.\n\nIn Task 1, we identified a direction of unboundedness, $d = (0, 1)^T$. This vector indicates that we can move infinitely in the positive $x_2$ direction. For the objective function to be unbounded, its value must increase indefinitely as we move along such a feasible direction. Let's analyze the change in the objective function along this direction $d$.\n\nThe rate of change of the objective function $z$ in the direction $d$ is given by the directional derivative, which for a linear function is simply the dot product $c^T d$. Let us compute this value:\n$$ c^T d = (1, 0) \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (1)(0) + (0)(1) = 0 $$\nThe result $c^T d = 0$ shows that moving along the direction of unboundedness $d$ does not change the value of the objective function at all. The direction of unboundedness is orthogonal to the direction of increase of the objective function.\n\nTo illustrate, consider any feasible point $p_0 = (x_{1,0}, x_{2,0}) \\in S$. Let's move from this point along the direction $d$. A new point is $p(\\lambda) = p_0 + \\lambda d = (x_{1,0}, x_{2,0} + \\lambda)$ for $\\lambda \\geq 0$. As long as $0 \\leq x_{1,0} \\leq 1$, this new point remains feasible for all $\\lambda \\geq 0$. The objective value at this new point is:\n$$ z(p(\\lambda)) = c^T p(\\lambda) = c^T(p_0 + \\lambda d) = c^T p_0 + \\lambda(c^T d) $$\nSince we found $c^T d = 0$, this simplifies to:\n$$ z(p(\\lambda)) = c^T p_0 = x_{1,0} $$\nThis confirms that the objective function value remains constant along any ray parallel to the $x_2$-axis within the feasible set. The unboundedness of the region exists in a direction in which the objective function does not increase. The objective function is bounded because its value depends only on $x_1$, and $x_1$ itself is bounded by the constraints.\n\nIn summary, for a linear program with an unbounded feasible region to have an unbounded objective, there must exist a feasible direction of unboundedness $d$ for which $c^T d > 0$. In this counterexample, no such direction exists; for the only direction of unboundedness, we have $c^T d = 0$.", "answer": "$$\\boxed{1}$$", "id": "2443959"}, {"introduction": "The simplex iterations we've seen so far conveniently start from a basic feasible solution, often at the origin. But what happens when the origin is not feasible, or when we have equality constraints? This advanced practice [@problem_id:3192717] introduces the two-phase method, a powerful technique to handle these situations. You will implement Phase I, which uses the simplex algorithm itself to find an initial feasible vertex or, if none exists, to prove that the problem is infeasible.", "problem": "Consider the formal feasibility question in Linear Programming (LP), where one must decide whether there exists a vector of decision variables satisfying a finite set of linear equality and inequality constraints together with nonnegativity restrictions. The simplex method resolves feasibility constructively by seeking a Basic Feasible Solution (BFS) formed from a set of basic variables such that all variables are nonnegative and the constraints are satisfied. When a BFS cannot be formed directly, a standard two-phase approach is used. In Phase I, one augments the constraints with artificial variables and minimizes the sum of these artificial variables. This is justified by the definition of feasibility: if the minimum of the sum of artificial variables is strictly positive, then no vector of original variables can satisfy all constraints with nonnegative slack or surplus, hence the original LP is infeasible. Conversely, a minimum of zero certifies that a BFS exists for the original constraints and all artificial variables can be driven to zero.\n\nYour task is to implement Phase I of the simplex method using artificial variables, treating it as a primal simplex with the objective to maximize the negative of the sum of artificial variables (equivalently, to minimize their sum). Begin from the following fundamental base:\n\n- The LP feasibility definition: find $x \\in \\mathbb{R}^n$ such that $A_{\\text{eq}} x = b_{\\text{eq}}$, $A_{\\text{le}} x \\le b_{\\text{le}}$, $A_{\\text{ge}} x \\ge b_{\\text{ge}}$, and $x \\ge 0$.\n- Slack and surplus construction: for constraints $a_i^\\top x \\le b_i$, introduce slack variables $s_i \\ge 0$ to write $a_i^\\top x + s_i = b_i$; for constraints $a_i^\\top x \\ge b_i$, introduce surplus variables $s_i \\ge 0$ to write $a_i^\\top x - s_i = b_i$.\n- Artificial variables construction: for constraints that cannot provide an immediate BFS (all equalities and all $\\ge$ constraints after surplus introduction), add an artificial variable $a_i \\ge 0$ to write $a_i^\\top x - s_i + a_i = b_i$ (for $\\ge$) or $a_i^\\top x + a_i = b_i$ (for $=$).\n- Phase I objective: minimize $\\sum_i a_i$, equivalently maximize $-\\sum_i a_i$. If the optimal value of $\\sum_i a_i$ is strictly positive, the original LP is infeasible.\n\nAlgorithmic requirements:\n\n- Implement the primal simplex for Phase I using the reduced cost principle. Let $M \\in \\mathbb{R}^{m \\times p}$ be the full constraint matrix after adding slack, surplus, and artificial variables, where $p$ is the total number of variables. Let $b \\in \\mathbb{R}^m$ be the right-hand side vector after any necessary sign adjustments to ensure nonnegative right-hand sides. Maintain a basis index set $B$ such that $B$ selects $m$ columns of $M$ giving the basis matrix $M_B$. For the Phase I objective $c \\in \\mathbb{R}^p$ with entries $-1$ for artificial variables and $0$ otherwise, compute the reduced costs for each nonbasic column $j$ as $r_j = c_j - c_B^\\top M_B^{-1} M_{\\cdot j}$. Select the entering variable with $r_j > 0$ (use Bland’s rule by choosing the smallest index to avoid cycling). Compute the direction $d = -M_B^{-1} M_{\\cdot j}$ for basic variables, use the minimum ratio test $t_i = x_{B,i} / (-d_i)$ for indices with $d_i < 0$ (equivalently, $x_{B,i} / (M_B^{-1} M_{\\cdot j})_i$ for those with positive components in $M_B^{-1} M_{\\cdot j}$), and pivot. Iterate until no entering variable with positive reduced cost exists. The Phase I optimal value is $-\\;c_B^\\top x_B$, which equals the minimized sum of artificial variables.\n\n- Row sign normalization: for any constraint with $b_i < 0$, multiply the entire row by $-1$ and flip the inequality direction ($\\le$ becomes $\\ge$ and vice versa) to maintain $b_i \\ge 0$. Equality constraints remain equalities when multiplied by $-1$.\n\n- Nonnegativity: all variables, including original, slack, surplus, and artificial, must satisfy $x \\ge 0$.\n\nTest suite:\n\nFor each case, variables are $x_1, x_2 \\ge 0$. Provide the constraints as $(A, b, \\text{senses})$ where $A \\in \\mathbb{R}^{m \\times 2}$, $b \\in \\mathbb{R}^m$, and senses is a list using entries from $\\{\\text{le}, \\text{ge}, \\text{eq}\\}$ meaning $\\le$, $\\ge$, $=$ respectively.\n\n- Case $1$ (infeasible): $A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, senses $= [\\text{le}, \\text{ge}]$. Constraints: $x_1 + x_2 \\le 1$, $x_1 \\ge 2$.\n- Case $2$ (feasible, slack-only): $A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}$, senses $= [\\text{le}, \\text{le}]$. Constraints: $x_1 + 2 x_2 \\le 4$, $2 x_1 + x_2 \\le 5$.\n- Case $3$ (feasible, equality requires artificial): $A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}$, senses $= [\\text{eq}, \\text{le}]$. Constraints: $x_1 + x_2 = 3$, $x_1 + 2 x_2 \\le 5$.\n- Case $4$ (infeasible, inconsistent equalities): $A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, senses $= [\\text{eq}, \\text{eq}]$. Constraints: $x_1 + x_2 = 1$, $x_1 + x_2 = 2$.\n- Case $5$ (feasible, row sign flip and mixed senses): $A = \\begin{bmatrix} -1 & 1 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$, senses $= [\\text{ge}, \\text{le}]$. Constraints: $-x_1 + x_2 \\ge -1$, $x_2 \\le 2$.\n\nOutput specification:\n\n- Your program must produce a single line containing a list of floating-point numbers, one per test case, each equal to the optimal value of the Phase I objective $\\sum a_i$ minimized (equivalently $-\\max (-\\sum a_i)$), rounded to six decimal places. Interpret values less than $10^{-8}$ in magnitude as zero in the final reported numbers.\n- The exact output format must be a single line in the form $[v_1,v_2,v_3,v_4,v_5]$ where each $v_k$ is a decimal number with six digits after the decimal point.\n\nNo physical units or angles are involved in this problem, so no unit specification is required.", "solution": "The user's request is to implement Phase I of the simplex method for linear programming feasibility. The problem is to determine if a set of linear constraints has a non-negative solution. This is achieved by minimizing the sum of auxiliary artificial variables. If the minimal sum is zero, the original problem is feasible; if it is strictly positive, it is infeasible.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Problem Definition**: Find $x \\in \\mathbb{R}^n$ such that $A_{\\text{eq}} x = b_{\\text{eq}}$, $A_{\\text{le}} x \\le b_{\\text{le}}$, $A_{\\text{ge}} x \\ge b_{\\text{ge}}$, and $x \\ge 0$.\n- **Standard Form Conversion**:\n    - For $a_i^\\top x \\le b_i$, use a slack variable $s_i \\ge 0$: $a_i^\\top x + s_i = b_i$.\n    - For $a_i^\\top x \\ge b_i$, use a surplus variable $s_i \\ge 0$: $a_i^\\top x - s_i = b_i$.\n- **Artificial Variables**: For equality constraints and $\\ge$ constraints, add an artificial variable $a_i \\ge 0$ to serve in the initial basis.\n- **Phase I Objective**: Minimize $\\sum_i a_i$, equivalent to maximizing $z = -\\sum_i a_i$. A strictly positive minimum for $\\sum_i a_i$ implies infeasibility.\n- **Algorithmic Specification**: Primal simplex method using reduced costs.\n    - Full constraint matrix $M \\in \\mathbb{R}^{m \\times p}$.\n    - Right-hand side vector $b \\in \\mathbb{R}^m$, with all entries non-negative.\n    - Basis index set $B$ of size $m$.\n    - Phase I cost vector $c$: $c_j = -1$ for artificial variables, $0$ otherwise.\n    - Reduced cost: $r_j = c_j - c_B^\\top M_B^{-1} M_{\\cdot j}$.\n    - Entering variable: Smallest index $j$ with $r_j > 0$ (Bland's rule).\n    - Leaving variable: Minimum ratio test, using Bland's rule for tie-breaking.\n- **Row Normalization**: If $b_i < 0$, multiply the constraint by $-1$ and reverse the inequality.\n- **Test Cases**: Five distinct cases are provided, each defined by a matrix $A$, vector $b$, and a list of senses (`le`, `ge`, `eq`).\n- **Output Format**: A single-line list of the minimal sum of artificial variables for each test case, rounded to six decimal places. Values with magnitude less than $10^{-8}$ are to be treated as zero.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is evaluated against the validation criteria:\n\n1.  **Scientifically Grounded**: The problem is fundamentally sound. It describes the standard, textbook formulation of Phase I of the simplex algorithm, a cornerstone of operations research and computational science.\n2.  **Well-Posed**: The problem is well-posed. The objective is clear, the algorithm is explicitly defined (primal simplex with Bland's rule to prevent cycling), and the input for each test case is fully specified. This ensures that a unique, meaningful solution (the minimum sum of artificial variables) exists and is computable.\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free of any subjectivity or ambiguity.\n4.  **Completeness and Consistency**: The setup is complete and internally consistent. It provides all necessary components: constraint conversion rules, objective function definition, algorithmic steps, and specific test data.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. It is a clear, sound, and formalizable task in the field of computational science. The solution will proceed with the implementation of the specified algorithm.\n\n### Algorithmic Design and Solution\n\nThe core of the solution is a Python function that implements the primal simplex algorithm for the Phase I objective. The steps are as follows:\n\n1.  **Preprocessing and Tableau Construction**:\n    - The input constraints $(A, b, \\text{senses})$ are processed. Each constraint with a negative right-hand side $b_i$ is multiplied by $-1$, and its sense is flipped accordingly. This ensures all $b_i \\ge 0$.\n    - A full matrix $M$ for the Phase I problem is constructed. Its columns correspond to the original variables ($x_j$), slack variables ($s_k$), surplus variables ($sl_l$), and artificial variables ($a_p$).\n    - The Phase I cost vector $c$ is created. It has entries of $-1$ for each artificial variable and $0$ for all other variables, aligning with the objective of maximizing $z = -\\sum_i a_i$.\n\n2.  **Initial Basis Identification**:\n    - An initial Basic Feasible Solution (BFS) for the auxiliary problem is required.\n    - For each constraint of type $\\le$ (after normalization), the corresponding slack variable can serve as the basic variable for that row.\n    - For each constraint of type $\\ge$ or $=$, an artificial variable is introduced and serves as the basic variable for that row.\n    - The set of indices of these basic variables forms the initial basis $B$. If no artificial variables are needed, the original problem is feasible, and the minimal sum is trivially $0$.\n\n3.  **Simplex Iterations**:\n    - The algorithm iterates until an optimal solution for the Phase I problem is found. Each iteration involves the following steps:\n    - **Compute Reduced Costs**: The vector of simplex multipliers (or dual variables) $\\pi = c_B^\\top M_B^{-1}$ is calculated, where $M_B$ is the basis matrix formed by the columns of $M$ corresponding to the variables in the basis $B$, and $c_B$ are their costs. The reduced cost for each non-basic variable $j$ is then $r_j = c_j - \\pi M_{\\cdot j}$.\n    - **Select Entering Variable**: According to Bland's rule, the non-basic variable with the smallest index $j$ for which $r_j > 0$ is chosen to enter the basis. If no such variable exists, the current solution is optimal, and the loop terminates.\n    - **Select Leaving Variable (Minimum Ratio Test)**: The direction of change in the basic variables, $d = M_B^{-1} M_{\\cdot j}$, is computed for the entering variable $j$. The maximum step size $\\theta$ is found by the minimum ratio test: $\\theta = \\min \\{ x_{B,i} / d_i \\mid d_i > 0 \\}$, where $x_B = M_B^{-1} b$ is the current vector of basic variable values. The variable leaving the basis is the one corresponding to the row that limits $\\theta$. Bland's rule is used to break ties: among all rows yielding the minimum ratio, the one corresponding to the basic variable with the smallest index is chosen.\n    - **Pivot**: The basis $B$ is updated by replacing the leaving variable's index with the entering variable's index.\n\n4.  **Termination and Result**:\n    - The loop terminates when all reduced costs are non-positive. The optimal value of the Phase I objective is $z_{\\text{opt}} = c_B^\\top x_B$.\n    - The minimum sum of the artificial variables is $-z_{\\text{opt}}$.\n    - This value is returned. A value of $0$ (within a small tolerance) indicates feasibility; a strictly positive value indicates infeasibility.\n\nThis computational procedure is implemented in Python using the `NumPy` library for matrix operations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_phase1(A, b, senses, tol=1e-9):\n    \"\"\"\n    Implements Phase I of the Simplex method to determine LP feasibility.\n\n    This function sets up and solves the Phase I linear program, which seeks to\n    minimize the sum of artificial variables.\n\n    Args:\n        A (np.ndarray): The coefficient matrix of the original constraints.\n        b (np.ndarray): The right-hand side vector of the original constraints.\n        senses (list): A list of strings ('le', 'ge', 'eq') indicating the sense\n                       of each constraint.\n        tol (float): A tolerance for floating-point comparisons.\n\n    Returns:\n        float: The minimum sum of artificial variables. A value of 0 indicates\n               feasibility, while a positive value indicates infeasibility. Returns\n               inf if the algorithm fails to converge or encounters an issue.\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Preprocessing and Tableau Setup\n    A_p = A.copy().astype(float)\n    b_p = b.copy().astype(float)\n    senses_p = list(senses)\n\n    # Row normalization to ensure b_p >= 0\n    for i in range(m):\n        if b_p[i] < 0:\n            A_p[i, :] *= -1\n            b_p[i] *= -1\n            if senses_p[i] == 'le': senses_p[i] = 'ge'\n            elif senses_p[i] == 'ge': senses_p[i] = 'le'\n\n    # Build variable map to track original, slack, surplus, and artificial variables\n    var_map = []\n    # Original variables\n    for i in range(n):\n        var_map.append(('orig', i))\n    \n    # Add slack, surplus, and artificial variables based on constraint senses\n    for i in range(m):\n        if senses_p[i] == 'le':\n            var_map.append(('slack', i))\n        elif senses_p[i] == 'ge':\n            var_map.append(('surplus', i))\n            var_map.append(('artif', i))\n        elif senses_p[i] == 'eq':\n            var_map.append(('artif', i))\n\n    # If no artificial variables are needed, the problem is trivially feasible\n    if not any(v_type == 'artif' for v_type, _ in var_map):\n      return 0.0\n\n    # Build the full constraint matrix M for the Phase I problem\n    num_total_vars = len(var_map)\n    M = np.zeros((m, num_total_vars))\n    M[:, :n] = A_p\n    for k in range(n, num_total_vars):\n        v_type, v_orig_idx = var_map[k]\n        if v_type == 'slack': M[v_orig_idx, k] = 1.0\n        elif v_type == 'surplus': M[v_orig_idx, k] = -1.0\n        elif v_type == 'artif': M[v_orig_idx, k] = 1.0\n\n    # Build the cost vector c for Phase I (maximize -sum(a_i))\n    c = np.zeros(num_total_vars)\n    for k, (v_type, _) in enumerate(var_map):\n        if v_type == 'artif': c[k] = -1.0\n\n    # 2. Initial Basis Identification\n    basis = [-1] * m\n    for i in range(m):\n        if senses_p[i] == 'le':\n            # Find the slack variable for row i to be basic\n            for k, (v_type, v_orig_idx) in enumerate(var_map):\n                if v_type == 'slack' and v_orig_idx == i:\n                    basis[i] = k\n                    break\n        else:  # 'ge' or 'eq'\n            # Find the artificial variable for row i to be basic\n            for k, (v_type, v_orig_idx) in enumerate(var_map):\n                if v_type == 'artif' and v_orig_idx == i:\n                    basis[i] = k\n                    break\n    \n    # 3. Simplex Loop\n    max_iter = 5 * m # Set a practical iteration limit\n    for _ in range(max_iter):\n        M_B = M[:, basis]\n        c_B = c[basis]\n\n        try:\n            M_B_inv = np.linalg.inv(M_B)\n        except np.linalg.LinAlgError:\n            # Singular basis matrix, indicates a problem like linearly dependent constraints.\n            # In Phase I, this suggests the system is ill-defined or infeasible.\n            return np.inf\n\n        non_basis_indices = sorted([i for i in range(num_total_vars) if i not in basis])\n        pi = c_B @ M_B_inv # Simplex multipliers\n\n        # Bland's rule for entering variable: choose smallest index with positive reduced cost\n        entering_var = -1\n        for j in non_basis_indices:\n            rc = c[j] - pi @ M[:, j]\n            if rc > tol:\n                entering_var = j\n                break\n\n        if entering_var == -1: # Optimality reached\n            x_B = M_B_inv @ b_p\n            obj_val = c_B @ x_B\n            return -obj_val\n\n        # Minimum Ratio Test to find leaving variable\n        d = M_B_inv @ M[:, entering_var]\n        \n        if np.all(d <= tol):\n            # Unbounded problem. This should not occur in a well-formed Phase I.\n            return np.inf\n\n        x_B = M_B_inv @ b_p\n        \n        min_ratio = np.inf\n        potential_leaves = []\n        for i in range(m):\n            if d[i] > tol:\n                ratio = x_B[i] / d[i]\n                if ratio < min_ratio - tol:\n                    min_ratio = ratio\n                    potential_leaves = [(ratio, basis[i])]\n                elif abs(ratio - min_ratio) < tol:\n                    potential_leaves.append((ratio, basis[i]))\n        \n        if not potential_leaves:\n           # This case should be prevented by the check for d > tol.\n           return np.inf\n            \n        # Bland's rule for leaving variable: break ties with smallest variable index\n        leaving_var = min(var_idx for _, var_idx in potential_leaves)\n        \n        # Pivot: update the basis\n        leaving_row_idx = basis.index(leaving_var)\n        basis[leaving_row_idx] = entering_var\n\n    return np.inf # Failed to converge within iteration limit\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the Phase I simplex solver.\n    \"\"\"\n    test_cases = [\n        # Case 1 (infeasible)\n        (np.array([[1, 1], [1, 0]]), np.array([1, 2]), ['le', 'ge']),\n        # Case 2 (feasible, slack-only)\n        (np.array([[1, 2], [2, 1]]), np.array([4, 5]), ['le', 'le']),\n        # Case 3 (feasible, equality requires artificial)\n        (np.array([[1, 1], [1, 2]]), np.array([3, 5]), ['eq', 'le']),\n        # Case 4 (infeasible, inconsistent equalities)\n        (np.array([[1, 1], [1, 1]]), np.array([1, 2]), ['eq', 'eq']),\n        # Case 5 (feasible, row sign flip)\n        (np.array([[-1, 1], [0, 1]]), np.array([-1, 2]), ['ge', 'le']),\n    ]\n\n    results = []\n    for A, b, senses in test_cases:\n        raw_result = solve_phase1(A, b, senses)\n        # Interpret values smaller than 1e-8 as zero, as per problem spec\n        if abs(raw_result) < 1e-8:\n            final_result = 0.0\n        else:\n            final_result = raw_result\n        results.append(final_result)\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3192717"}]}